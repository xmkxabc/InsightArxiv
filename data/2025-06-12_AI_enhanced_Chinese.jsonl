{"id": "2506.09148", "title": "Adversarial Text Generation with Dynamic Contextual Perturbation", "authors": ["Hetvi Waghela", "Jaydip Sen", "Sneha Rakshit", "Subhasis Dasgupta"], "summary": "Adversarial attacks on Natural Language Processing (NLP) models expose\nvulnerabilities by introducing subtle perturbations to input text, often\nleading to misclassification while maintaining human readability. Existing\nmethods typically focus on word-level or local text segment alterations,\noverlooking the broader context, which results in detectable or semantically\ninconsistent perturbations. We propose a novel adversarial text attack scheme\nnamed Dynamic Contextual Perturbation (DCP). DCP dynamically generates\ncontext-aware perturbations across sentences, paragraphs, and documents,\nensuring semantic fidelity and fluency. Leveraging the capabilities of\npre-trained language models, DCP iteratively refines perturbations through an\nadversarial objective function that balances the dual objectives of inducing\nmodel misclassification and preserving the naturalness of the text. This\ncomprehensive approach allows DCP to produce more sophisticated and effective\nadversarial examples that better mimic natural language patterns. Our\nexperimental results, conducted on various NLP models and datasets, demonstrate\nthe efficacy of DCP in challenging the robustness of state-of-the-art NLP\nsystems. By integrating dynamic contextual analysis, DCP significantly enhances\nthe subtlety and impact of adversarial attacks. This study highlights the\ncritical role of context in adversarial attacks and lays the groundwork for\ncreating more robust NLP systems capable of withstanding sophisticated\nadversarial strategies.", "comment": "This is the accepted version of the paper, which was presented at\n  IEEE CALCON. The conference was organized at Jadavpur University, Kolkata,\n  from December 14 to 15, 2025. The paper is six pages long, and it consists of\n  six tables and six figures. This is not the final camera-ready version of the\n  paper", "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.09148v1", "AI": {"title_translation": "具有动态上下文扰动的对抗性文本生成", "tldr": "本文提出了一种名为DCP的新型对抗性文本攻击方案，通过动态生成上下文感知的扰动来挑战NLP模型的鲁棒性，从而产生更复杂和有效的对抗性样本。", "motivation": "现有对抗性文本攻击方法通常只关注词级或局部文本段落的修改，忽略了更广泛的上下文，这导致了可检测或语义不一致的扰动。为了暴露NLP模型的脆弱性并创建更鲁棒的系统，需要一种能够生成更自然、上下文感知的对抗性文本的方法。", "method": "本文提出了一种名为动态上下文扰动（DCP）的新型对抗性文本攻击方案。DCP利用预训练语言模型的能力，动态地在句子、段落和文档级别生成上下文感知的扰动，以确保语义保真度和流畅性。它通过一个对抗性目标函数迭代地优化扰动，该函数平衡了诱导模型错误分类和保持文本自然度的双重目标。", "result": "在各种NLP模型和数据集上进行的实验结果表明，DCP在挑战最先进NLP系统的鲁棒性方面是有效的。通过整合动态上下文分析，DCP显著增强了对抗性攻击的微妙性和影响力。", "conclusion": "DCP的研究强调了上下文在对抗性攻击中的关键作用，并为创建能够抵御复杂对抗性策略的更鲁棒的NLP系统奠定了基础。", "translation": "自然语言处理（NLP）模型的对抗性攻击通过对输入文本引入微小的扰动来暴露其脆弱性，这通常会导致误分类，同时保持人类可读性。现有方法通常侧重于词级或局部文本段落的改变，忽略了更广泛的上下文，这导致了可检测或语义不一致的扰动。我们提出了一种名为动态上下文扰动（DCP）的新型对抗性文本攻击方案。DCP动态地在句子、段落和文档中生成上下文感知的扰动，确保语义保真度和流畅性。利用预训练语言模型的能力，DCP通过一个对抗性目标函数迭代地优化扰动，该函数平衡了诱导模型误分类和保持文本自然度的双重目标。这种综合方法使DCP能够产生更复杂和有效的对抗性样本，更好地模仿自然语言模式。我们在各种NLP模型和数据集上进行的实验结果表明，DCP在挑战最先进NLP系统的鲁棒性方面是有效的。通过整合动态上下文分析，DCP显著增强了对抗性攻击的微妙性和影响力。这项研究强调了上下文在对抗性攻击中的关键作用，并为创建能够抵御复杂对抗性策略的更鲁棒的NLP系统奠定了基础。", "summary": "本文提出了一种名为动态上下文扰动（DCP）的新型对抗性文本攻击方案，旨在解决现有方法在生成上下文感知扰动方面的不足。DCP利用预训练语言模型，动态地在不同文本粒度上生成语义一致且流畅的扰动，以诱导NLP模型错误分类同时保持文本自然度。实验证明，DCP能够生成更有效和复杂的对抗性样本，显著提升了攻击的微妙性和影响力，并为构建更强大的NLP系统提供了基础。", "keywords": "对抗性攻击, 文本生成, 上下文扰动, NLP模型, 鲁棒性", "comments": "DCP的创新之处在于其动态生成上下文感知扰动的能力，这解决了现有对抗性文本攻击方法忽略广泛上下文的局限性。通过利用预训练语言模型并平衡攻击有效性和文本自然度，DCP能够生成更难以检测且语义连贯的对抗性样本。这项工作的重要性在于它揭示了NLP模型在上下文理解方面的潜在脆弱性，并为开发更鲁棒的NLP系统指明了方向。"}}
{"id": "2506.09312", "title": "What is the Cost of Differential Privacy for Deep Learning-Based Trajectory Generation?", "authors": ["Erik Buchholz", "Natasha Fernandes", "David D. Nguyen", "Alsharif Abuadbba", "Surya Nepal", "Salil S. Kanhere"], "summary": "While location trajectories offer valuable insights, they also reveal\nsensitive personal information. Differential Privacy (DP) offers formal\nprotection, but achieving a favourable utility-privacy trade-off remains\nchallenging. Recent works explore deep learning-based generative models to\nproduce synthetic trajectories. However, current models lack formal privacy\nguarantees and rely on conditional information derived from real data during\ngeneration. This work investigates the utility cost of enforcing DP in such\nmodels, addressing three research questions across two datasets and eleven\nutility metrics. (1) We evaluate how DP-SGD, the standard DP training method\nfor deep learning, affects the utility of state-of-the-art generative models.\n(2) Since DP-SGD is limited to unconditional models, we propose a novel DP\nmechanism for conditional generation that provides formal guarantees and assess\nits impact on utility. (3) We analyse how model types - Diffusion, VAE, and GAN\n- affect the utility-privacy trade-off. Our results show that DP-SGD\nsignificantly impacts performance, although some utility remains if the\ndatasets is sufficiently large. The proposed DP mechanism improves training\nstability, particularly when combined with DP-SGD, for unstable models such as\nGANs and on smaller datasets. Diffusion models yield the best utility without\nguarantees, but with DP-SGD, GANs perform best, indicating that the best\nnon-private model is not necessarily optimal when targeting formal guarantees.\nIn conclusion, DP trajectory generation remains a challenging task, and formal\nguarantees are currently only feasible with large datasets and in constrained\nuse cases.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.09312v1", "AI": {"title_translation": "深度学习轨迹生成中差分隐私的成本是什么？", "tldr": "本文研究了在基于深度学习的轨迹生成模型中实施差分隐私的效用成本，评估了DP-SGD和提出了一种新的条件生成DP机制，并发现DP显著影响性能，且在某些情况下，GANs在差分隐私下表现最佳。", "motivation": "位置轨迹虽然提供有价值的见解，但也会泄露敏感的个人信息。差分隐私（DP）提供了正式的保护，但实现有利的效用-隐私权衡仍然具有挑战性。当前基于深度学习的生成模型缺乏正式的隐私保证，并且在生成过程中依赖于来自真实数据的条件信息。本研究旨在调查在此类模型中强制执行DP的效用成本。", "method": "本研究通过解决三个研究问题，在两个数据集和十一个效用指标上进行了调查：1. 评估DP-SGD（深度学习的标准DP训练方法）如何影响最先进生成模型的效用。2. 提出一种新的条件生成DP机制，以提供正式保证并评估其对效用的影响，因为DP-SGD仅限于无条件模型。3. 分析模型类型（Diffusion、VAE和GAN）如何影响效用-隐私权衡。", "result": "研究结果表明，DP-SGD显著影响性能，但如果数据集足够大，仍然保留一定的效用。所提出的DP机制提高了训练稳定性，尤其是在与DP-SGD结合时，对于GANs等不稳定模型和较小数据集。在没有保证的情况下，Diffusion模型产生了最佳效用，但在DP-SGD下，GANs表现最佳，这表明在追求正式保证时，最佳的非私有模型不一定是最佳的。", "conclusion": "差分隐私轨迹生成仍然是一项具有挑战性的任务，目前正式的隐私保证仅在大数据集和受限用例中可行。", "translation": "虽然位置轨迹提供了有价值的见解，但它们也揭示了敏感的个人信息。差分隐私（DP）提供了正式的保护，但实现有利的效用-隐私权衡仍然具有挑战性。最近的工作探索了基于深度学习的生成模型来生成合成轨迹。然而，当前模型缺乏正式的隐私保证，并且在生成过程中依赖于来自真实数据的条件信息。这项工作调查了在此类模型中强制执行DP的效用成本，解决了两个数据集和十一个效用指标上的三个研究问题。(1) 我们评估了DP-SGD（深度学习的标准DP训练方法）如何影响最先进生成模型的效用。(2) 由于DP-SGD仅限于无条件模型，我们提出了一种新的条件生成DP机制，该机制提供正式保证并评估其对效用影响。(3) 我们分析了模型类型——Diffusion、VAE和GAN——如何影响效用-隐私权衡。我们的结果表明，DP-SGD显著影响性能，尽管如果数据集足够大，仍然保留一些效用。所提出的DP机制提高了训练稳定性，特别是在与DP-SGD结合时，对于GANs等不稳定模型和较小数据集。Diffusion模型在没有保证的情况下产生了最佳效用，但在DP-SGD下，GANs表现最佳，这表明在追求正式保证时，最佳的非私有模型不一定是最佳的。总之，差分隐私轨迹生成仍然是一项具有挑战性的任务，目前正式的隐私保证仅在大数据集和受限用例中可行。", "summary": "本文探讨了在基于深度学习的轨迹生成中实施差分隐私（DP）的效用成本。研究评估了标准DP训练方法DP-SGD对最先进生成模型的影响，并提出了一种新的条件生成DP机制。通过对Diffusion、VAE和GAN模型在两个数据集上的分析，发现DP-SGD显著降低了模型性能，但如果数据集足够大，仍能保持一定效用。此外，新提出的DP机制提高了训练稳定性，并且在DP-SGD应用下，GANs在私有设置中表现优于其他模型，表明非私有下的最佳模型在应用DP后可能并非最优。研究总结，带正式保证的DP轨迹生成目前仅适用于大数据集和受限场景。", "keywords": "差分隐私, 深度学习, 轨迹生成, 效用-隐私权衡, DP-SGD", "comments": "这篇论文解决了在敏感轨迹数据生成中隐私保护的关键挑战。它的创新之处在于深入探讨了深度学习生成模型中差分隐私的效用成本，并提出了一种针对条件生成的、具有正式保证的新型DP机制。研究结果揭示了在实现强隐私保证时所面临的实际限制，特别是强调了数据集大小和模型选择对隐私-效用权衡的影响。它为未来在隐私保护生成模型领域的研究提供了重要的见解和方向。"}}
{"id": "2506.09353", "title": "DAVSP: Safety Alignment for Large Vision-Language Models via Deep Aligned Visual Safety Prompt", "authors": ["Yitong Zhang", "Jia Li", "Liyi Cai", "Ge Li"], "summary": "Large Vision-Language Models (LVLMs) have achieved impressive progress across\nvarious applications but remain vulnerable to malicious queries that exploit\nthe visual modality. Existing alignment approaches typically fail to resist\nmalicious queries while preserving utility on benign ones effectively. To\naddress these challenges, we propose Deep Aligned Visual Safety Prompt (DAVSP),\nwhich is built upon two key innovations. First, we introduce the Visual Safety\nPrompt, which appends a trainable padding region around the input image. It\npreserves visual features and expands the optimization space. Second, we\npropose Deep Alignment, a novel approach to train the visual safety prompt\nthrough supervision in the model's activation space. It enhances the inherent\nability of LVLMs to perceive malicious queries, achieving deeper alignment than\nprior works. Extensive experiments across five benchmarks on two representative\nLVLMs demonstrate that DAVSP effectively resists malicious queries while\npreserving benign input utility. Furthermore, DAVSP exhibits great cross-model\ngeneration ability. Ablation studies further reveal that both the Visual Safety\nPrompt and Deep Alignment are essential components, jointly contributing to its\noverall effectiveness. The code is publicly available at\nhttps://github.com/zhangyitonggg/DAVSP.", "comment": "16 pages", "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.09353v1", "AI": {"title_translation": "DAVSP：通过深度对齐视觉安全提示实现大型视觉语言模型的安全对齐", "tldr": "DAVSP通过在输入图像周围添加可训练填充区域并利用模型激活空间中的深度对齐训练，有效提升大型视觉语言模型对恶意查询的抵抗力，同时保持良性输入的效用。", "motivation": "大型视觉语言模型（LVLMs）易受利用视觉模态的恶意查询攻击，现有对齐方法在抵抗恶意查询同时保持良性查询效用方面效果不佳。", "method": "本文提出深度对齐视觉安全提示（DAVSP）。首先，引入视觉安全提示（Visual Safety Prompt），在输入图像周围附加可训练的填充区域，以保留视觉特征并扩展优化空间。其次，提出深度对齐（Deep Alignment），通过在模型激活空间中进行监督训练视觉安全提示，增强LVLMs感知恶意查询的内在能力，实现比以往工作更深层次的对齐。", "result": "在两个代表性大型视觉语言模型上的五个基准测试中，DAVSP有效抵抗恶意查询，同时保持了良性输入的效用。此外，DAVSP表现出良好的跨模型生成能力。消融研究表明，视觉安全提示和深度对齐都是其整体有效性的重要组成部分。", "conclusion": "DAVSP通过其创新的视觉安全提示和深度对齐方法，有效解决了大型视觉语言模型面临的恶意查询安全问题，同时保持了模型处理良性输入的效用，并展现出优异的跨模型泛化能力。", "translation": "大型视觉语言模型（LVLMs）在各种应用中取得了令人瞩目的进展，但仍然容易受到利用视觉模态的恶意查询的攻击。现有的对齐方法通常无法在抵抗恶意查询的同时有效地保持良性查询的效用。为了解决这些挑战，我们提出了深度对齐视觉安全提示（DAVSP），它基于两项关键创新。首先，我们引入了视觉安全提示，它在输入图像周围附加一个可训练的填充区域。它保留了视觉特征并扩展了优化空间。其次，我们提出了深度对齐，这是一种通过在模型的激活空间中进行监督训练视觉安全提示的新方法。它增强了LVLMs感知恶意查询的内在能力，实现了比以往工作更深层次的对齐。在两个代表性LVLMs上的五个基准测试中进行的广泛实验表明，DAVSP有效地抵抗了恶意查询，同时保持了良性输入的效用。此外，DAVSP表现出强大的跨模型生成能力。消融研究进一步表明，视觉安全提示和深度对齐都是其整体有效性的重要组成部分，共同发挥作用。代码已在https://github.com/zhangyitonggg/DAVSP 公开。", "summary": "DAVSP旨在解决大型视觉语言模型（LVLMs）面临的恶意查询安全问题。该方法引入了视觉安全提示，通过在输入图像周围添加可训练填充区域来扩展优化空间，并采用深度对齐技术在模型的激活空间中训练此提示，从而增强LVLMs识别和抵抗恶意查询的能力。实验证明，DAVSP在抵抗恶意查询的同时有效保持了模型处理良性输入的性能，并展现出良好的跨模型泛化能力。", "keywords": "大型视觉语言模型, 安全对齐, 视觉安全提示, 深度对齐, 恶意查询", "comments": "DAVSP的创新之处在于结合了可训练的视觉安全提示和在激活空间进行的深度对齐训练，这提供了一种新颖且有效的安全对齐方法，超越了以往仅在输出层进行对齐的工作，对提升LVLMs的鲁棒性具有重要意义。"}}
{"id": "2506.09365", "title": "ContextBuddy: AI-Enhanced Contextual Insights for Security Alert Investigation (Applied to Intrusion Detection)", "authors": ["Ronal Singh", "Mohan Baruwal Chhetri", "Surya Nepal", "Cecile Paris"], "summary": "Modern Security Operations Centres (SOCs) integrate diverse tools, such as\nSIEM, IDS, and XDR systems, offering rich contextual data, including alert\nenrichments, flow features, and similar case histories. Yet, analysts must\nstill manually determine which of these contextual cues are most relevant when\nvalidating specific alerts. We introduce ContextBuddy, an AI assistant that\nlearns from analysts' prior investigations to help them identify the most\nrelevant context for new alerts. Rather than providing enrichments,\nContextBuddy models how analysts have previously selected context and suggests\ntailored cues based on the characteristics of each alert. We formulate context\nselection as a sequential decision-making problem and apply imitation learning\n(IL) to capture analysts' strategies, evaluating multiple IL approaches.\nThrough staged evaluation, we validate ContextBuddy using two intrusion\ndetection datasets (HIKARI-2021, UNSW-NB15). In simulation-based experiments,\nContextBuddy helped simulated reinforcement learning analysts improve\nclassification accuracy (p < 0.001) (increasing F1 by 2.5% for HIKARI and 9%\nfor UNSW), reducing false negatives (1.5% for HIKARI and 10% for UNSW), and\nkeeping false positives below 1%. Decision confidence among agents also\nimproved by 2-3% (p < 0.001). In a within-subject user study (N=13; power =\n0.8), non-experts using ContextBuddy improved classification accuracy by 21.1%\n(p = 0.008) and reduced alert validation time by 24% (p = 0.01). These results\ndemonstrate that by learning context-selection patterns from analysts,\nContextBuddy can yield notable improvements in investigation effectiveness and\nefficiency.", "comment": "27 pages, 33 figures, 7 tables, under review", "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.09365v1", "AI": {"title_translation": "ContextBuddy：AI增强型安全警报调查上下文洞察（应用于入侵检测）", "tldr": "ContextBuddy是一个AI助手，它通过模仿学习分析师的调查策略，帮助安全分析师识别最相关的上下文，从而显著提高安全警报调查的效率和准确性。", "motivation": "现代安全运营中心（SOC）整合了多种工具，提供丰富的上下文数据，但分析师仍需手动确定哪些上下文线索与特定警报最相关，这导致效率低下。", "method": "论文提出了ContextBuddy，一个AI助手，它通过将上下文选择建模为序列决策问题，并应用模仿学习（IL）来捕捉分析师的策略。通过模拟实验和用户研究，在两个入侵检测数据集（HIKARI-2021, UNSW-NB15）上进行了评估。", "result": "在模拟实验中，ContextBuddy帮助模拟强化学习分析师提高了分类准确性（F1值HIKARI增加2.5%，UNSW增加9%），减少了误报（HIKARI减少1.5%，UNSW减少10%），并将误报率控制在1%以下，决策信心提高了2-3%。在用户研究中（N=13），非专家使用ContextBuddy后，分类准确率提高了21.1%，警报验证时间缩短了24%。", "conclusion": "通过学习分析师的上下文选择模式，ContextBuddy能够显著提高安全警报调查的有效性和效率。", "translation": "现代安全运营中心（SOC）整合了SIEM、IDS和XDR系统等多种工具，提供丰富的上下文数据，包括警报丰富信息、流特征和类似案例历史。然而，分析师在验证特定警报时，仍需手动确定哪些上下文线索最相关。我们引入了ContextBuddy，一个AI助手，它通过学习分析师先前的调查经验，帮助他们为新警报识别最相关的上下文。ContextBuddy不是提供丰富信息，而是建模分析师之前如何选择上下文，并根据每个警报的特征建议定制的线索。我们将上下文选择表述为一个序列决策问题，并应用模仿学习（IL）来捕捉分析师的策略，评估了多种IL方法。通过分阶段评估，我们使用两个入侵检测数据集（HIKARI-2021，UNSW-NB15）验证了ContextBuddy。在基于模拟的实验中，ContextBuddy帮助模拟强化学习分析师提高了分类准确性（p < 0.001）（HIKARI的F1提高了2.5%，UNSW的F1提高了9%），减少了误报（HIKARI为1.5%，UNSW为10%），并将误报率保持在1%以下。代理之间的决策信心也提高了2-3%（p < 0.001）。在一次受试者内部用户研究中（N=13；功效=0.8），非专家使用ContextBuddy后，分类准确率提高了21.1%（p = 0.008），警报验证时间缩短了24%（p = 0.01）。这些结果表明，通过学习分析师的上下文选择模式，ContextBuddy可以在调查有效性和效率方面产生显著改进。", "summary": "ContextBuddy是一个AI助手，旨在解决安全分析师在调查警报时手动识别相关上下文的挑战。该系统通过将上下文选择视为序列决策问题，并利用模仿学习从分析师过去的调查中学习，从而为新警报提供定制的上下文线索。在模拟实验和用户研究中，ContextBuddy显著提高了入侵检测警报的分类准确率，减少了误报，并缩短了警报验证时间，证明了其在提高安全调查效率和有效性方面的潜力。", "keywords": "ContextBuddy, 安全运营, 模仿学习, 入侵检测, 上下文洞察", "comments": "该论文的创新点在于将安全警报的上下文选择问题转化为序列决策和模仿学习任务，这为AI辅助安全运营提供了一个新颖且实用的方法。通过学习人类分析师的隐性知识，ContextBuddy能够提供更精准、更个性化的上下文建议，从而显著提升安全分析师的工作效率和准确性。其在真实数据集和用户研究中的积极结果表明了该方法的强大潜力，未来有望推广到更广泛的安全分析场景。"}}
{"id": "2506.09480", "title": "Reliability of Capacitive Read in Arrays of Ferroelectric Capacitors", "authors": ["Luca Fehlings", "Muhtasim Alam Chowdhury", "Banafsheh Saber Latibari", "Soheil Salehi", "Erika Covi"], "summary": "The non-destructive capacitance read-out of ferroelectric capacitors (FeCaps)\nbased on doped HfO$_2$ metal-ferroelectric-metal (MFM) structures offers the\npotential for low-power and highly scalable crossbar arrays. This is due to a\nnumber of factors, including the selector-less design, the absence of sneak\npaths, the power-efficient charge-based read operation, and the reduced IR\ndrop. Nevertheless, a reliable capacitive readout presents certain challenges,\nparticularly in regard to device variability and the trade-off between read\nyield and read disturbances, which can ultimately result in bit-flips. This\npaper presents a digital read macro for HfO$_2$ FeCaps and provides design\nguidelines for capacitive readout of HfO$_2$ FeCaps, taking device-centric\nreliability and yield challenges into account. An experimentally calibrated\nphysics-based compact model of HfO$_2$ FeCaps is employed to investigate the\nreliability of the read-out operation of the FeCap macro through Monte Carlo\nsimulations. Based on this analysis, we identify limitations posed by the\ndevice variability and propose potential mitigation strategies through\ndesign-technology co-optimization (DTCO) of the FeCap device characteristics\nand the CMOS circuit design. Finally, we examine the potential applications of\nthe FeCap macro in the context of secure hardware. We identify potential\nsecurity threats and propose strategies to enhance the robustness of the\nsystem.", "comment": "4 pages, 6 figures, submitted and presented at ISCAS 2025, London", "cate": "cs.ET", "url": "http://arxiv.org/abs/2506.09480v1", "AI": {"title_translation": "铁电电容器阵列中电容读出的可靠性", "tldr": "该研究提出了一种数字读出宏和设计指南，并利用蒙特卡洛模拟和DTCO方法，解决了HfO2铁电电容器阵列中电容读出受器件变异性影响的可靠性挑战，并探讨了其在安全硬件中的应用。", "motivation": "掺杂HfO2金属-铁电-金属（MFM）结构的铁电电容器（FeCaps）的非破坏性电容读出在低功耗和高可扩展性交叉阵列方面具有潜力。然而，可靠的电容读出面临挑战，特别是器件变异性以及读出良率和读出干扰之间的权衡，这可能导致位翻转。", "method": "1. 提出了一种用于HfO2 FeCaps的数字读出宏和电容读出的设计指南，考虑了以器件为中心的可靠性和良率挑战。\n2. 采用经过实验校准的基于物理的HfO2 FeCaps紧凑模型。\n3. 通过蒙特卡洛模拟研究FeCap宏读出操作的可靠性。\n4. 根据分析，识别器件变异性带来的限制，并通过FeCap器件特性和CMOS电路设计的“设计-技术协同优化”（DTCO）提出潜在的缓解策略。\n5. 探讨了FeCap宏在安全硬件中的潜在应用，识别潜在的安全威胁并提出增强系统鲁棒性的策略。", "result": "1. 识别了器件变异性对电容读出操作的限制。\n2. 提出了通过FeCap器件特性和CMOS电路设计的“设计-技术协同优化”（DTCO）来缓解这些限制的策略。\n3. 探讨了FeCap宏在安全硬件中的潜在应用，并提出了增强系统鲁棒性的策略。", "conclusion": "该研究识别了HfO2铁电电容器阵列中电容读出操作面临的器件变异性限制，并提出了通过设计-技术协同优化（DTCO）来缓解这些限制的设计指南和策略，同时还探讨了其在安全硬件中的应用及相关安全增强措施。", "translation": "掺杂HfO2金属-铁电-金属（MFM）结构铁电电容器（FeCaps）的非破坏性电容读出，为低功耗和高可扩展性交叉阵列提供了潜力。这归因于多种因素，包括无选择器设计、无潜通路、基于电荷的节能读出操作以及降低的IR压降。然而，可靠的电容读出带来了一定的挑战，特别是在器件变异性以及读出良率和读出干扰之间的权衡方面，这最终可能导致位翻转。本文提出了一种用于HfO2 FeCaps的数字读出宏，并为HfO2 FeCaps的电容读出提供了设计指南，同时考虑了以器件为中心的可靠性和良率挑战。本文采用经过实验校准的基于物理的HfO2 FeCaps紧凑模型，通过蒙特卡洛模拟研究FeCap宏读出操作的可靠性。基于此分析，我们识别了器件变异性带来的限制，并通过FeCap器件特性和CMOS电路设计的“设计-技术协同优化”（DTCO）提出了潜在的缓解策略。最后，我们探讨了FeCap宏在安全硬件背景下的潜在应用。我们识别了潜在的安全威胁并提出了增强系统鲁棒性的策略。", "summary": "本文针对掺杂HfO2铁电电容器（FeCaps）阵列中电容读出的可靠性问题进行了研究。鉴于器件变异性可能导致读出良率和干扰之间的权衡以及位翻转，研究提出了一种数字读出宏和设计指南。通过实验校准的物理模型和蒙特卡洛模拟，论文识别了器件变异性带来的限制，并提出了通过器件特性与CMOS电路设计的“设计-技术协同优化”（DTCO）来缓解这些问题。此外，论文还探讨了FeCap宏在安全硬件中的应用潜力，并提出了增强系统安全性的策略。", "keywords": "铁电电容器, 电容读出, 可靠性, HfO2, 设计-技术协同优化, 安全硬件", "comments": "这篇论文解决了铁电电容器（FeCaps）在实际应用中，特别是作为交叉阵列存储器时，电容读出可靠性这一关键挑战。其创新之处在于提出了结合器件物理模型、电路设计（数字读出宏）和设计-技术协同优化（DTCO）的全面解决方案，以应对器件变异性带来的影响。此外，论文还前瞻性地探讨了FeCap在安全硬件领域的应用，并考虑了相关的安全威胁与增强策略，这显示了该研究的实用价值和前瞻性。"}}
{"id": "2506.09098", "title": "WD-DETR: Wavelet Denoising-Enhanced Real-Time Object Detection Transformer for Robot Perception with Event Cameras", "authors": ["Yangjie Cui", "Boyang Gao", "Yiwei Zhang", "Xin Dong", "Jinwu Xiang", "Daochun Li", "Zhan Tu"], "summary": "Previous studies on event camera sensing have demonstrated certain detection\nperformance using dense event representations. However, the accumulated noise\nin such dense representations has received insufficient attention, which\ndegrades the representation quality and increases the likelihood of missed\ndetections. To address this challenge, we propose the Wavelet\nDenoising-enhanced DEtection TRansformer, i.e., WD-DETR network, for event\ncameras. In particular, a dense event representation is presented first, which\nenables real-time reconstruction of events as tensors. Then, a wavelet\ntransform method is designed to filter noise in the event representations. Such\na method is integrated into the backbone for feature extraction. The extracted\nfeatures are subsequently fed into a transformer-based network for object\nprediction. To further reduce inference time, we incorporate the Dynamic\nReorganization Convolution Block (DRCB) as a fusion module within the hybrid\nencoder. The proposed method has been evaluated on three event-based object\ndetection datasets, i.e., DSEC, Gen1, and 1Mpx. The results demonstrate that\nWD-DETR outperforms tested state-of-the-art methods. Additionally, we implement\nour approach on a common onboard computer for robots, the NVIDIA Jetson Orin\nNX, achieving a high frame rate of approximately 35 FPS using TensorRT FP16,\nwhich is exceptionally well-suited for real-time perception of onboard robotic\nsystems.", "comment": "https://youtu.be/AQAgVdrx1DE", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.09098v1", "AI": {"title_translation": "WD-DETR：用于事件相机的机器人感知小波去噪增强实时目标检测Transformer", "tldr": "WD-DETR提出了一种基于小波去噪的Transformer网络，用于事件相机实时目标检测，有效减少噪声并提高检测性能和帧率。", "motivation": "现有事件相机感知方法在密集事件表示中存在累积噪声问题，这会降低表示质量并增加漏检的可能性。", "method": "本文提出了WD-DETR网络，首先生成密集事件表示以实现实时张量重建。然后，设计了一种小波变换方法来过滤事件表示中的噪声，并将其集成到骨干网络中进行特征提取。提取的特征随后被送入基于Transformer的网络进行目标预测。为了进一步减少推理时间，将动态重组卷积块（DRCB）作为融合模块整合到混合编码器中。", "result": "WD-DETR在DSEC、Gen1和1Mpx三个事件目标检测数据集上优于现有SOTA方法。此外，该方法在NVIDIA Jetson Orin NX上使用TensorRT FP16实现了大约35 FPS的高帧率，非常适合板载机器人系统的实时感知。", "conclusion": "WD-DETR通过有效去噪和实时处理，显著提升了事件相机在机器人感知中的目标检测性能。", "translation": "事件相机感知方面的先前研究已经证明了使用密集事件表示的特定检测性能。然而，这种密集表示中累积的噪声尚未受到足够的关注，这会降低表示质量并增加漏检的可能性。为了解决这一挑战，我们提出了用于事件相机的Wavelet Denoising-enhanced DEtection TRansformer，即WD-DETR网络。具体而言，首先提出了一个密集事件表示，它能够实时重建事件为张量。然后，设计了一种小波变换方法来过滤事件表示中的噪声。这种方法被集成到骨干网络中用于特征提取。提取的特征随后被送入基于Transformer的网络进行目标预测。为了进一步减少推理时间，我们将动态重组卷积块（DRCB）作为融合模块整合到混合编码器中。所提出的方法已在DSEC、Gen1和1Mpx三个基于事件的目标检测数据集上进行了评估。结果表明，WD-DETR优于经过测试的最新方法。此外，我们将我们的方法实现在机器人常用的板载计算机NVIDIA Jetson Orin NX上，使用TensorRT FP16实现了大约35 FPS的高帧率，这非常适合板载机器人系统的实时感知。", "summary": "本文提出了WD-DETR，一种针对事件相机的实时目标检测Transformer网络，旨在解决密集事件表示中的噪声问题。该网络通过小波变换去噪和DRCB融合模块，有效提高了检测性能和推理速度，使其在DSEC、Gen1和1Mpx数据集上超越现有SOTA，并在NVIDIA Jetson Orin NX上实现35 FPS的实时感知。", "keywords": "事件相机, 目标检测, 小波去噪, Transformer, 实时感知", "comments": "这项研究创新性地将小波去噪技术引入到事件相机的密集事件表示中，有效解决了噪声累积导致性能下降的问题。结合Transformer架构和实时优化（如DRCB），使其在机器人实时感知应用中具有重要的实用价值和优越性。"}}
{"id": "2506.09066", "title": "ReStNet: A Reusable & Stitchable Network for Dynamic Adaptation on IoT Devices", "authors": ["Maoyu Wang", "Yao Lu", "Jiaqi Nie", "Zeyu Wang", "Yun Lin", "Qi Xuan", "Guan Gui"], "summary": "With the rapid development of deep learning, a growing number of pre-trained\nmodels have been publicly available. However, deploying these fixed models in\nreal-world IoT applications is challenging because different devices possess\nheterogeneous computational and memory resources, making it impossible to\ndeploy a single model across all platforms. Although traditional compression\nmethods, such as pruning, quantization, and knowledge distillation, can improve\nefficiency, they become inflexible once applied and cannot adapt to changing\nresource constraints. To address these issues, we propose ReStNet, a Reusable\nand Stitchable Network that dynamically constructs a hybrid network by\nstitching two pre-trained models together. Implementing ReStNet requires\naddressing several key challenges, including how to select the optimal\nstitching points, determine the stitching order of the two pre-trained models,\nand choose an effective fine-tuning strategy. To systematically address these\nchallenges and adapt to varying resource constraints, ReStNet determines the\nstitching point by calculating layer-wise similarity via Centered Kernel\nAlignment (CKA). It then constructs the hybrid model by retaining early layers\nfrom a larger-capacity model and appending deeper layers from a smaller one. To\nfacilitate efficient deployment, only the stitching layer is fine-tuned. This\ndesign enables rapid adaptation to changing budgets while fully leveraging\navailable resources. Moreover, ReStNet supports both homogeneous (CNN-CNN,\nTransformer-Transformer) and heterogeneous (CNN-Transformer) stitching,\nallowing to combine different model families flexibly. Extensive experiments on\nmultiple benchmarks demonstrate that ReStNet achieve flexible\naccuracy-efficiency trade-offs at runtime while significantly reducing training\ncost.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09066v1", "AI": {"title_translation": "ReStNet: 一种用于物联网设备动态适应的可重用和可拼接网络", "tldr": "ReStNet是一种可重用和可拼接的网络，通过拼接两个预训练模型，实现物联网设备上深度学习模型的动态适应，以解决资源异构性和固定模型部署的挑战。", "motivation": "现有预训练模型在物联网设备上部署面临挑战，因为设备资源异构，单一模型无法适应所有平台。传统压缩方法不灵活，无法适应变化的资源限制。", "method": "提出ReStNet，通过拼接两个预训练模型动态构建混合网络。通过计算层级相似性（CKA）确定最优拼接点，保留大容量模型的早期层并追加小容量模型的深层。仅微调拼接层以实现高效部署。支持同构和异构模型拼接。", "result": "在多个基准测试中，ReStNet在运行时实现了灵活的精度-效率权衡，并显著降低了训练成本。", "conclusion": "ReStNet通过其可重用和可拼接的设计，有效地解决了物联网设备上深度学习模型部署的资源异构性和动态适应性问题，实现了高效的性能和成本效益。", "translation": "随着深度学习的快速发展，越来越多的预训练模型已经公开可用。然而，在实际物联网应用中部署这些固定模型面临挑战，因为不同的设备拥有异构的计算和内存资源，使得单一模型无法部署到所有平台。尽管剪枝、量化和知识蒸馏等传统压缩方法可以提高效率，但它们一旦应用就变得不灵活，无法适应变化的资源限制。为了解决这些问题，我们提出了ReStNet，一种可重用和可拼接的网络，通过拼接两个预训练模型来动态构建混合网络。实现ReStNet需要解决几个关键挑战，包括如何选择最优拼接点、确定两个预训练模型的拼接顺序以及选择有效的微调策略。为了系统地解决这些挑战并适应不同的资源限制，ReStNet通过计算中心核对齐（CKA）来计算层级相似性，从而确定拼接点。然后，它通过保留较大容量模型的早期层并附加较小容量模型的更深层来构建混合模型。为了促进高效部署，只对拼接层进行微调。这种设计能够快速适应不断变化的预算，同时充分利用可用资源。此外，ReStNet支持同构（CNN-CNN、Transformer-Transformer）和异构（CNN-Transformer）拼接，允许灵活组合不同的模型家族。在多个基准测试中进行的广泛实验表明，ReStNet在运行时实现了灵活的精度-效率权衡，同时显著降低了训练成本。", "summary": "ReStNet是一种创新性的深度学习模型部署框架，旨在解决物联网设备上模型部署的资源异构性和动态适应性问题。它通过计算层级相似性（CKA）智能地拼接两个预训练模型，构建混合网络，并仅微调拼接层，从而实现高效的动态适应性。ReStNet支持同构和异构模型拼接，并在实验中展现出优异的精度-效率权衡和训练成本降低。", "keywords": "可重用网络, 可拼接网络, 物联网设备, 动态适应, 模型压缩", "comments": "ReStNet的创新之处在于其“可重用和可拼接”的网络设计理念，通过动态拼接预训练模型来适应异构的物联网设备资源，这比传统的静态压缩方法更灵活。利用CKA进行拼接点选择是一种新颖且有效的方法。该方法的重要性在于它为在资源受限且多变的物联网环境中部署和管理深度学习模型提供了一个高效且适应性强的解决方案，有望降低部署成本并提高模型利用率。"}}
{"id": "2506.09176", "title": "Robot-Gated Interactive Imitation Learning with Adaptive Intervention Mechanism", "authors": ["Haoyuan Cai", "Zhenghao Peng", "Bolei Zhou"], "summary": "Interactive Imitation Learning (IIL) allows agents to acquire desired\nbehaviors through human interventions, but current methods impose high\ncognitive demands on human supervisors. We propose the Adaptive Intervention\nMechanism (AIM), a novel robot-gated IIL algorithm that learns an adaptive\ncriterion for requesting human demonstrations. AIM utilizes a proxy Q-function\nto mimic the human intervention rule and adjusts intervention requests based on\nthe alignment between agent and human actions. By assigning high Q-values when\nthe agent deviates from the expert and decreasing these values as the agent\nbecomes proficient, the proxy Q-function enables the agent to assess the\nreal-time alignment with the expert and request assistance when needed. Our\nexpert-in-the-loop experiments reveal that AIM significantly reduces expert\nmonitoring efforts in both continuous and discrete control tasks. Compared to\nthe uncertainty-based baseline Thrifty-DAgger, our method achieves a 40%\nimprovement in terms of human take-over cost and learning efficiency.\nFurthermore, AIM effectively identifies safety-critical states for expert\nassistance, thereby collecting higher-quality expert demonstrations and\nreducing overall expert data and environment interactions needed. Code and demo\nvideo are available at https://github.com/metadriverse/AIM.", "comment": "ICML 2025 Poster", "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.09176v1", "AI": {"title_translation": "机器人门控交互式模仿学习与自适应干预机制", "tldr": "提出了一种名为AIM的机器人门控交互式模仿学习算法，通过自适应干预机制显著减少了人类监督者的认知负担和干预成本，同时提高了学习效率和数据质量。", "motivation": "现有的交互式模仿学习（IIL）方法对人类监督者施加了高认知负荷。", "method": "提出了自适应干预机制（AIM），这是一种新型的机器人门控交互式模仿学习算法。AIM使用代理Q函数来模仿人类干预规则，并根据智能体与人类动作之间的一致性调整干预请求。通过在智能体偏离专家时分配高Q值，并在智能体熟练时降低这些值，代理Q函数使智能体能够评估与专家的实时一致性并在需要时请求帮助。", "result": "专家在环实验表明，AIM显著减少了连续和离散控制任务中的专家监控工作。与基于不确定性的基线Thrifty-DAgger相比，该方法在人类接管成本和学习效率方面提高了40%。此外，AIM有效地识别了需要专家协助的安全关键状态，从而收集了更高质量的专家演示，并减少了所需的总体专家数据和环境交互。", "conclusion": "AIM通过自适应干预机制显著降低了人类监督者的认知负担和干预成本，同时提高了交互式模仿学习的效率和数据质量。", "translation": "交互式模仿学习（IIL）允许智能体通过人类干预获得期望的行为，但目前的方法对人类监督者施加了高认知负荷。我们提出了自适应干预机制（AIM），一种新颖的机器人门控IIL算法，它学习一种自适应的准则来请求人类演示。AIM利用代理Q函数来模仿人类干预规则，并根据智能体与人类动作之间的一致性调整干预请求。通过在智能体偏离专家时分配高Q值并在智能体熟练时降低这些值，代理Q函数使智能体能够评估与专家的实时一致性并在需要时请求帮助。我们的专家在环实验表明，AIM显著减少了连续和离散控制任务中的专家监控工作。与基于不确定性的基线Thrifty-DAgger相比，我们的方法在人类接管成本和学习效率方面提高了40%。此外，AIM有效地识别了需要专家协助的安全关键状态，从而收集了更高质量的专家演示并减少了所需的总体专家数据和环境交互。代码和演示视频可在https://github.com/metadriverse/AIM获取。", "summary": "本文提出了一种名为自适应干预机制（AIM）的新型机器人门控交互式模仿学习算法，旨在解决现有方法对人类监督者认知负担过高的问题。AIM利用代理Q函数模拟人类干预规则，并根据智能体与专家行为的一致性自适应地请求人类演示。实验结果表明，AIM显著减少了人类监控工作，在人类接管成本和学习效率方面比基线方法提高了40%，并且能够识别安全关键状态以收集高质量的专家数据，从而减少了总体数据需求。", "keywords": "交互式模仿学习, 自适应干预机制, 机器人门控, 代理Q函数, 人类监督", "comments": "这篇论文的创新点在于提出了自适应干预机制（AIM），通过引入代理Q函数实现了机器人门控的自适应干预请求，有效降低了人类监督者的认知负担，这是交互式模仿学习领域的一个重要进展。其方法不仅提高了学习效率，还通过识别安全关键状态提升了专家数据的质量，具有很强的实用价值和应用前景。"}}
{"id": "2506.09052", "title": "Llama-Affinity: A Predictive Antibody Antigen Binding Model Integrating Antibody Sequences with Llama3 Backbone Architecture", "authors": ["Delower Hossain", "Ehsan Saghapour", "Kevin Song", "Jake Y. Chen"], "summary": "Antibody-facilitated immune responses are central to the body's defense\nagainst pathogens, viruses, and other foreign invaders. The ability of\nantibodies to specifically bind and neutralize antigens is vital for\nmaintaining immunity. Over the past few decades, bioengineering advancements\nhave significantly accelerated therapeutic antibody development. These\nantibody-derived drugs have shown remarkable efficacy, particularly in treating\ncancer, SARS-CoV-2, autoimmune disorders, and infectious diseases.\nTraditionally, experimental methods for affinity measurement have been\ntime-consuming and expensive. With the advent of artificial intelligence, in\nsilico medicine has been revolutionized; recent developments in machine\nlearning, particularly the use of large language models (LLMs) for representing\nantibodies, have opened up new avenues for AI-based design and improved\naffinity prediction. Herein, we present an advanced antibody-antigen binding\naffinity prediction model (LlamaAffinity), leveraging an open-source Llama 3\nbackbone and antibody sequence data sourced from the Observed Antibody Space\n(OAS) database. The proposed approach shows significant improvement over\nexisting state-of-the-art (SOTA) methods (AntiFormer, AntiBERTa, AntiBERTy)\nacross multiple evaluation metrics. Specifically, the model achieved an\naccuracy of 0.9640, an F1-score of 0.9643, a precision of 0.9702, a recall of\n0.9586, and an AUC-ROC of 0.9936. Moreover, this strategy unveiled higher\ncomputational efficiency, with a five-fold average cumulative training time of\nonly 0.46 hours, significantly lower than in previous studies.", "comment": "7 Pages", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09052v1", "AI": {"title_translation": "Llama-Affinity：一种集成抗体序列与Llama3骨干架构的预测性抗体抗原结合模型", "tldr": "本文提出了LlamaAffinity模型，该模型利用Llama3骨干架构和抗体序列数据，用于预测抗体-抗原结合亲和力。该模型在多项评估指标上显著优于现有SOTA方法，并展现出更高的计算效率。", "motivation": "传统抗体亲和力测量方法耗时且昂贵。随着人工智能，特别是大语言模型（LLMs）在抗体表征方面的应用，为AI驱动的设计和改进亲和力预测开辟了新途径，但仍需要更先进、高效的模型。", "method": "本文提出了一个先进的抗体-抗原结合亲和力预测模型LlamaAffinity。该模型利用开源的Llama 3骨干架构，并整合了来自Observed Antibody Space (OAS) 数据库的抗体序列数据。", "result": "LlamaAffinity模型在多项评估指标上显著优于现有最先进方法（AntiFormer、AntiBERTa、AntiBERTy）。具体表现为：准确率0.9640，F1-分数0.9643，精确率0.9702，召回率0.9586，AUC-ROC 0.9936。此外，该策略展现出更高的计算效率，平均累计训练时间仅为0.46小时，比以往研究显著降低。", "conclusion": "LlamaAffinity模型在预测抗体-抗原结合亲和力方面展现出卓越的性能和计算效率，为治疗性抗体开发提供了更优的AI驱动方法，有望加速抗体药物的发现和优化过程。", "translation": "抗体介导的免疫反应是身体对抗病原体、病毒和其他外来入侵者防御的核心。抗体特异性结合和中和抗原的能力对于维持免疫力至关重要。在过去的几十年里，生物工程的进步显著加速了治疗性抗体的开发。这些抗体衍生药物在治疗癌症、SARS-CoV-2、自身免疫性疾病和传染病方面显示出卓越的疗效。传统上，亲和力测量的实验方法耗时且昂贵。随着人工智能的出现，体外计算医学（in silico medicine）发生了革命性变化；机器学习的最新发展，特别是使用大型语言模型（LLMs）来表示抗体，为基于AI的设计和改进亲和力预测开辟了新途径。在此，我们提出了一个先进的抗体-抗原结合亲和力预测模型（LlamaAffinity），该模型利用开源的Llama 3骨干架构和来自Observed Antibody Space（OAS）数据库的抗体序列数据。所提出的方法在多项评估指标上显示出比现有最先进（SOTA）方法（AntiFormer、AntiBERTa、AntiBERTy）显著的改进。具体来说，该模型实现了0.9640的准确率、0.9643的F1分数、0.9702的精确率、0.9586的召回率和0.9936的AUC-ROC。此外，该策略揭示了更高的计算效率，平均累计训练时间仅为0.46小时，比以往研究显著降低。", "summary": "本文提出了LlamaAffinity模型，这是一个先进的抗体-抗原结合亲和力预测模型，它创新性地结合了开源Llama 3骨干架构和来自OAS数据库的抗体序列数据。该模型在预测性能上显著超越了现有SOTA方法，并大幅提高了计算效率，为加速治疗性抗体药物的开发提供了强大的AI驱动工具。", "keywords": "抗体-抗原结合, 亲和力预测, Llama3, 大语言模型, 计算生物学", "comments": "该研究创新性地将大型语言模型Llama3应用于抗体-抗原结合亲和力预测，通过整合抗体序列数据，不仅在预测精度上取得了显著的SOTA性能，还在计算效率上实现了突破性提升。这对于高通量筛选和优化治疗性抗体具有重要意义，有望大幅降低药物研发的时间和成本，推动生物制药领域的发展。"}}
{"id": "2506.09239", "title": "Rejection-Sampled Linear Codes for Lossy Compression and Channel Simulation", "authors": ["Cheuk Ting Li", "Jianguo Zhao"], "summary": "We show that a linear code combined with rejection sampling can give a\ncapacity-achieving scheme for simulating channels with additive noises with\nexchangeable distributions. Hence, it can be used in lossy source coding to\nachieve the rate-distortion function. Interestingly, unlike conventional linear\ncovering codes for lossy compression which concerns the trade-off between the\nrate and the covering radius, our construction only requires the linear code to\nhave a large distance (not a large covering radius), and is not sensitive to\nthe rate of the linear code. Experiments reveal that our construction can\noutperform conventional covering codes for lossy source coding with Hamming\ndistortion for a certain range of distortion levels, and performs well even\nwhen the blocklength is small (e.g., 24).", "comment": "12 pages, 5 figures", "cate": "cs.IT", "url": "http://arxiv.org/abs/2506.09239v1", "AI": {"title_translation": "拒绝采样线性码用于有损压缩和信道仿真", "tldr": "结合拒绝采样的线性码可以实现信道仿真和有损压缩的容量上限，且性能优于传统覆盖码。", "motivation": "传统线性覆盖码在有损压缩中关注码率和覆盖半径的权衡，而本文提出了一种不依赖大覆盖半径且对码率不敏感的新方法，旨在改进有损压缩和信道仿真。", "method": "提出了一种结合拒绝采样的线性码方案，用于模拟具有可交换分布的加性噪声信道，并应用于有损信源编码。该方法仅要求线性码具有大距离，而不要求大覆盖半径。", "result": "该构造方案能够实现信道仿真中的容量上限，并在有损信源编码中达到速率-失真函数。实验表明，对于某些失真水平，它优于传统的汉明失真有损信源编码覆盖码，即使在码块长度较小（例如24）时也表现良好。", "conclusion": "结合拒绝采样的线性码能够为有损压缩和信道仿真提供一种容量达到方案，且在性能上优于传统方法，尤其是在对覆盖半径和码率不敏感方面。", "translation": "我们展示了线性码与拒绝采样相结合可以提供一种容量达到方案，用于模拟具有可交换分布的加性噪声信道。因此，它可以在有损信源编码中用于实现速率-失真函数。有趣的是，与传统关注码率和覆盖半径之间权衡的有损压缩线性覆盖码不同，我们的构造方案仅要求线性码具有大距离（而非大覆盖半径），并且对线性码的码率不敏感。实验表明，我们的构造方案在某些失真水平下，对于汉明失真有损信源编码，可以优于传统的覆盖码，并且即使在码块长度较小（例如24）时也表现良好。", "summary": "本文提出了一种结合拒绝采样的线性码新方法，用于实现有损压缩和信道仿真的容量上限。与传统线性覆盖码不同，该方法不依赖于大覆盖半径，且对码率不敏感。实验证明，该方案在特定失真水平下性能优于传统覆盖码，即使在小码块长度下也表现出色。", "keywords": "拒绝采样, 线性码, 有损压缩, 信道仿真, 速率-失真函数", "comments": "本文的创新点在于将拒绝采样与线性码结合，提供了一种无需关注覆盖半径即可实现容量达到性能的新范式。这种方法对传统有损压缩的覆盖码理论提出了挑战，并展示了其在小码块长度下的实用性，具有重要的理论和实践意义。"}}
{"id": "2506.09230", "title": "Formal Methods Meets Readability: Auto-Documenting JML Java Code", "authors": ["Juan Carlos Recio Abad", "Ruben Saborido", "Francisco Chicano"], "summary": "This paper investigates whether formal specifications using Java Modeling\nLanguage (JML) can enhance the quality of Large Language Model (LLM)-generated\nJavadocs. While LLMs excel at producing documentation from code alone, we\nhypothesize that incorporating formally verified invariants yields more\ncomplete and accurate results. We present a systematic comparison of\ndocumentation generated from JML-annotated and non-annotated Java classes,\nevaluating quality through both automated metrics and expert analysis. Our\nfindings demonstrate that JML significantly improves class-level documentation\ncompleteness, with more moderate gains at the method level. Formal\nspecifications prove particularly effective in capturing complex class\ninvariants and design contracts that are frequently overlooked in code-only\ndocumentation. A threshold effect emerges, where the benefits of JML become\nmore pronounced for classes with richer sets of invariants. While JML enhances\nspecification coverage, its impact on core descriptive quality is limited,\nsuggesting that formal specifications primarily ensure comprehensive coverage\nrather than fundamentally altering implementation descriptions. These results\noffer actionable insights for software teams adopting formal methods in\ndocumentation workflows, highlighting scenarios where JML provides clear\nadvantages. The study contributes to AI-assisted software documentation\nresearch by demonstrating how formal methods and LLMs can synergistically\nimprove documentation quality.", "comment": null, "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.09230v1", "AI": {"title_translation": "形式化方法遇上可读性：JML Java代码的自动文档化", "tldr": "本研究表明，使用JML形式化规范可以显著提高大型语言模型（LLM）生成的Java文档的完整性，尤其是在类级别。", "motivation": "研究旨在探讨Java建模语言（JML）形式化规范是否能提升大型语言模型（LLM）生成的Javadocs的质量，并假设结合形式化验证的不变量能产生更完整和准确的结果。", "method": "研究通过系统比较JML标注和非标注Java类生成的文档质量，并利用自动化指标和专家分析进行评估。", "result": "研究发现，JML显著提高了类级别文档的完整性，对方法级别文档的提升则较为温和。形式化规范在捕获复杂类不变量和设计契约方面特别有效，这些在纯代码文档中常被忽略。JML的益处在不变量集合更丰富的类中更为明显。JML增强了规范覆盖率，但对核心描述质量的影响有限。", "conclusion": "研究结果为在文档工作流程中采用形式化方法的软件团队提供了可操作的见解，突出了JML的明显优势场景。该研究通过展示形式化方法和LLM如何协同提高文档质量，为AI辅助软件文档研究做出了贡献。", "translation": "本文研究了使用Java建模语言（JML）的形式化规范是否能提高大型语言模型（LLM）生成的Javadocs的质量。尽管LLM擅长仅从代码生成文档，但我们假设结合形式化验证的不变量会产生更完整和准确的结果。我们系统地比较了从JML标注和非标注Java类生成的文档，并通过自动化指标和专家分析评估了质量。我们的研究结果表明，JML显著提高了类级别文档的完整性，在方法级别则有更温和的提升。形式化规范在捕获复杂类不变量和设计契约方面被证明特别有效，这些在纯代码文档中经常被忽视。出现了一个阈值效应，即JML的益处在不变量集合更丰富的类中变得更加明显。虽然JML增强了规范覆盖率，但其对核心描述质量的影响有限，这表明形式化规范主要确保全面覆盖，而不是从根本上改变实现描述。这些结果为在文档工作流程中采用形式化方法的软件团队提供了可操作的见解，突出了JML的明显优势场景。该研究通过展示形式化方法和LLM如何协同提高文档质量，为AI辅助软件文档研究做出了贡献。", "summary": "本研究探讨了将Java建模语言（JML）形式化规范整合到大型语言模型（LLM）驱动的Java文档生成中的有效性。通过对JML标注和非标注Java类的系统比较，研究发现JML显著提升了类级别文档的完整性，尤其是在捕获复杂类不变量和设计契约方面。尽管JML对核心描述质量影响有限，但它有效地提高了规范覆盖率，证明了形式化方法与LLM协同提升软件文档质量的潜力。", "keywords": "JML, LLM, 形式化方法, 文档生成, Java", "comments": "该论文创新性地结合了形式化方法（JML）和大型语言模型（LLM）来提升软件文档的质量。其重要性在于为AI辅助文档生成提供了新的视角和实践指导，尤其是在确保文档完整性和准确性方面。研究发现JML在类级别文档完整性上的显著提升，以及对复杂不变量的有效捕获，是其主要亮点。局限性可能在于JML对核心描述质量的有限影响，以及实际应用中JML标注的成本和复杂性。"}}
{"id": "2506.09089", "title": "Designing conflict-based communicative tasks in Teaching Chinese as a Foreign Language with ChatGPT", "authors": ["Xia Li"], "summary": "In developing the teaching program for a course in Oral Expression in\nTeaching Chinese as a Foreign Language at the university level, the teacher\ndesigns communicative tasks based on conflicts to encourage learners to engage\nin interactive dynamics and develop their oral interaction skills. During the\ndesign of these tasks, the teacher uses ChatGPT to assist in finalizing the\nprogram. This article aims to present the key characteristics of the\ninteractions between the teacher and ChatGPT during this program development\nprocess, as well as to examine the use of ChatGPT and its impacts in this\nspecific context.", "comment": "in French language", "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.09089v1", "AI": {"title_translation": "在对外汉语教学中使用ChatGPT设计基于冲突的交际任务", "tldr": "本文探讨了在大学对外汉语口语表达课程中，教师如何使用ChatGPT辅助设计基于冲突的交际任务，并分析了教师与ChatGPT互动的特点及其影响。", "motivation": "教师在大学对外汉语口语表达课程中，设计基于冲突的交际任务以促进学生互动和口语技能发展，并在此过程中使用ChatGPT辅助。本文旨在呈现教师与ChatGPT互动的主要特点，并探讨ChatGPT在该特定语境下的使用及其影响。", "method": "教师在大学对外汉语口语表达课程的项目开发过程中，设计了基于冲突的交际任务，并利用ChatGPT辅助任务的最终确定。文章将呈现教师与ChatGPT互动的关键特征，并审视ChatGPT在此特定语境下的使用及其影响。", "result": "文章旨在呈现在此项目开发过程中教师与ChatGPT之间互动的主要特征，并审视ChatGPT在该特定语境下的使用及其影响。", "conclusion": "Not mentioned in abstract", "translation": "在大学对外汉语口语表达课程的教学项目开发过程中，教师设计了基于冲突的交际任务，以鼓励学习者参与互动动态并发展他们的口语互动技能。在这些任务的设计过程中，教师使用ChatGPT辅助最终确定该项目。本文旨在呈现在此项目开发过程中教师与ChatGPT之间互动的主要特征，并审视ChatGPT在该特定语境下的使用及其影响。", "summary": "本文探讨了在大学对外汉语口语表达课程中，教师如何设计基于冲突的交际任务以促进学生口语互动技能的发展。研究重点是教师在任务设计过程中利用ChatGPT进行辅助，并分析了教师与ChatGPT互动的特点及其在该特定教学背景下的影响。", "keywords": "对外汉语教学, ChatGPT, 交际任务, 冲突, 教师与AI互动", "comments": "本文的创新之处在于将ChatGPT引入到对外汉语教学任务设计中，特别是结合了“基于冲突的交际任务”这一教学策略。这为语言教学中人工智能辅助教学提供了新的视角和实践案例，对于探索AI在教学设计和教师角色转变中的潜力具有重要意义。"}}
{"id": "2506.09234", "title": "Transaction Categorization with Relational Deep Learning in QuickBooks", "authors": ["Kaiwen Dong", "Padmaja Jonnalagedda", "Xiang Gao", "Ayan Acharya", "Maria Kissa", "Mauricio Flores", "Nitesh V. Chawla", "Kamalika Das"], "summary": "Automatic transaction categorization is crucial for enhancing the customer\nexperience in QuickBooks by providing accurate accounting and bookkeeping. The\ndistinct challenges in this domain stem from the unique formatting of\ntransaction descriptions, the wide variety of transaction categories, and the\nvast scale of the data involved. Furthermore, organizing transaction data in a\nrelational database creates difficulties in developing a unified model that\ncovers the entire database. In this work, we develop a novel graph-based model,\nnamed Rel-Cat, which is built directly over the relational database. We\nintroduce a new formulation of transaction categorization as a link prediction\ntask within this graph structure. By integrating techniques from natural\nlanguage processing and graph machine learning, our model not only outperforms\nthe existing production model in QuickBooks but also scales effectively to a\ngrowing customer base with a simpler, more effective architecture without\ncompromising on accuracy. This design also helps tackle a key challenge of the\ncold start problem by adapting to minimal data.", "comment": "Accepted to ECML-PKDD 2025", "cate": "cs.CE", "url": "http://arxiv.org/abs/2506.09234v1", "AI": {"title_translation": "QuickBooks中基于关系深度学习的交易分类", "tldr": "Rel-Cat是一种新的图基模型，用于QuickBooks中的交易分类，它通过将NLP和图机器学习结合，解决了现有模型的挑战并超越了其性能。", "motivation": "自动交易分类对于通过提供准确的会计和簿记来增强QuickBooks中的客户体验至关重要。然而，该领域面临交易描述格式独特、交易类别广泛、数据规模庞大以及在关系数据库中组织数据给开发统一模型带来困难等挑战。", "method": "开发了一种名为Rel-Cat的新型图基模型，该模型直接构建在关系数据库之上。将交易分类重新定义为图结构中的链接预测任务，并集成了自然语言处理（NLP）和图机器学习技术。", "result": "Rel-Cat模型不仅在QuickBooks中超越了现有的生产模型，而且能够以更简单、更有效的架构有效地扩展到不断增长的客户群，同时不影响准确性。该设计还有助于通过适应最少数据来解决冷启动问题。", "conclusion": "该研究成功开发了一种创新的图基模型Rel-Cat，它显著提升了QuickBooks中的交易分类能力，解决了现有挑战，并为未来的可扩展性和冷启动问题提供了有效解决方案。", "translation": "自动交易分类对于通过提供准确的会计和簿记来增强QuickBooks中的客户体验至关重要。该领域的独特挑战源于交易描述的独特格式、交易类别的广泛多样性以及所涉及数据的巨大规模。此外，在关系数据库中组织交易数据给开发一个覆盖整个数据库的统一模型带来了困难。在这项工作中，我们开发了一种新颖的图基模型，名为Rel-Cat，它直接构建在关系数据库之上。我们将交易分类引入为这种图结构中的链接预测任务的新表述。通过整合自然语言处理和图机器学习的技术，我们的模型不仅在QuickBooks中超越了现有的生产模型，而且能够以更简单、更有效的架构有效地扩展到不断增长的客户群，而不会影响准确性。这种设计还有助于通过适应最少数据来解决冷启动问题。", "summary": "本文提出了一种名为Rel-Cat的新型图基模型，用于解决QuickBooks中自动交易分类的挑战。该模型直接构建在关系数据库上，将交易分类建模为图结构中的链接预测任务，并结合了自然语言处理和图机器学习技术。实验结果表明，Rel-Cat模型在性能上优于现有生产模型，并展现出良好的可扩展性和冷启动问题处理能力。", "keywords": "交易分类, 关系深度学习, 图神经网络, 链接预测, QuickBooks", "comments": "该论文的创新之处在于将交易分类问题转化为图结构中的链接预测任务，并成功地将自然语言处理与图机器学习相结合，直接在关系数据库上构建模型。这不仅解决了传统关系数据库建模的复杂性，还显著提升了系统性能，尤其在处理大规模数据和冷启动问题上表现出色，对企业级应用具有重要价值。"}}
{"id": "2506.09764", "title": "Alice and the Caterpillar: A more descriptive null model for assessing data mining results", "authors": ["Giulia Preti", "Gianmarco De Francisci Morales", "Matteo Riondato"], "summary": "We introduce novel null models for assessing the results obtained from\nobserved binary transactional and sequence datasets, using statistical\nhypothesis testing. Our null models maintain more properties of the observed\ndataset than existing ones. Specifically, they preserve the Bipartite Joint\nDegree Matrix of the bipartite (multi-)graph corresponding to the dataset,\nwhich ensures that the number of caterpillars, i.e., paths of length three, is\npreserved, in addition to other properties considered by other models. We\ndescribe Alice, a suite of Markov chain Monte Carlo algorithms for sampling\ndatasets from our null models, based on a carefully defined set of states and\nefficient operations to move between them. The results of our experimental\nevaluation show that Alice mixes fast and scales well, and that our null model\nfinds different significant results than ones previously considered in the\nliterature.", "comment": null, "cate": "cs.SI", "url": "http://arxiv.org/abs/2506.09764v1", "AI": {"title_translation": "Alice 和毛毛虫：一种用于评估数据挖掘结果的更具描述性的零模型", "tldr": "本文提出了一种新颖的零模型和马尔可夫链蒙特卡洛（MCMC）算法（Alice），用于评估二元事务和序列数据挖掘结果。这些模型保留了更多数据集属性，并发现了与现有模型不同的显著结果。", "motivation": "现有用于评估数据挖掘结果的零模型未能充分保留观察数据集的属性，可能导致不准确的显著性评估。因此，需要开发更具描述性的零模型。", "method": "本文引入了新颖的零模型，这些模型在现有模型的基础上，额外保留了二分（多）图的二分联合度矩阵，从而确保了“毛毛虫”（即长度为三的路径）的数量得以保留。此外，论文还描述了名为“Alice”的马尔可夫链蒙特卡洛（MCMC）算法套件，用于从这些新零模型中高效采样数据集。", "result": "实验评估表明，Alice 算法混合速度快且扩展性好。更重要的是，与文献中先前考虑的模型相比，本文提出的零模型能够发现不同的显著结果。", "conclusion": "通过保留更多的观测数据集属性，特别是二分联合度矩阵，本文提出的新型零模型及其配套的 Alice MCMC 算法为数据挖掘结果的评估提供了一种更稳健和准确的方法，从而可能导致发现不同且更具意义的显著性结果。", "translation": "我们引入了新颖的零模型，用于使用统计假设检验评估从观察到的二元事务和序列数据集中获得的结果。我们的零模型比现有模型保留了更多观察数据集的属性。具体来说，它们保留了对应于数据集的二分（多）图的二分联合度矩阵，这确保了除了其他模型考虑的属性之外，毛毛虫（即长度为三的路径）的数量也得以保留。我们描述了 Alice，这是一套马尔可夫链蒙特卡洛算法，用于从我们的零模型中采样数据集，它基于一组精心定义的状态和在它们之间高效移动的操作。我们的实验评估结果表明，Alice 混合速度快且扩展性好，并且我们的零模型发现了与文献中先前考虑的模型不同的显著结果。", "summary": "本文提出了一种用于评估二元事务和序列数据集数据挖掘结果的新型零模型，并利用统计假设检验进行评估。这些新模型旨在比现有模型保留更多的观测数据集属性，特别是二分联合度矩阵，从而保持“毛毛虫”（即长度为三的路径）的数量。文章还介绍了“Alice”，这是一套用于从这些零模型中采样的马尔可夫链蒙特卡洛算法。实验结果表明，Alice 在混合和可扩展性方面表现高效，并且所提出的零模型能够识别出与现有模型不同的显著结果。", "keywords": "零模型, 数据挖掘, 假设检验, 马尔可夫链蒙特卡洛, 二分图", "comments": "该论文通过引入保留更多数据结构属性（特别是二分联合度矩阵）的零模型，显著提高了数据挖掘结果统计假设检验的鲁棒性。配套的“Alice”MCMC 算法为实际应用这些模型提供了可行途径。发现其产生了不同的显著结果，这表明之前的评估可能不够准确，从而凸显了这项工作对于更可靠数据挖掘分析的重要性。"}}
{"id": "2506.09335", "title": "Intelligent System of Emergent Knowledge: A Coordination Fabric for Billions of Minds", "authors": ["Moshi Wei", "Sparks Li"], "summary": "The Intelligent System of Emergent Knowledge (ISEK) establishes a\ndecentralized network where human and artificial intelligence agents\ncollaborate as peers, forming a self-organizing cognitive ecosystem. Built on\nWeb3 infrastructure, ISEK combines three fundamental principles: (1) a\ndecentralized multi-agent architecture resistant to censorship, (2) symbiotic\nAI-human collaboration with equal participation rights, and (3) resilient\nself-adaptation through distributed consensus mechanisms.\n  The system implements an innovative coordination protocol featuring a\nsix-phase workflow (Publish, Discover, Recruit, Execute, Settle, Feedback) for\ndynamic task allocation, supported by robust fault tolerance and a\nmultidimensional reputation system. Economic incentives are governed by the\nnative $ISEK token, facilitating micropayments, governance participation, and\nreputation tracking, while agent sovereignty is maintained through NFT-based\nidentity management.\n  This synthesis of blockchain technology, artificial intelligence, and\nincentive engineering creates an infrastructure that actively facilitates\nemergent intelligence. ISEK represents a paradigm shift from conventional\nplatforms, enabling the organic development of large-scale, decentralized\ncognitive systems where autonomous agents collectively evolve beyond\ncentralized constraints.", "comment": "11 pages, 1 figures,", "cate": "cs.MA", "url": "http://arxiv.org/abs/2506.09335v1", "AI": {"title_translation": "涌现知识智能系统：数十亿思维的协调结构", "tldr": "ISEK是一个基于Web3的去中心化AI-人类协作网络，通过多阶段工作流、激励代币和NFT实现大规模涌现智能。", "motivation": "需要一种新范式来促进大规模、去中心化认知系统的有机发展，以超越传统中心化平台的限制。", "method": "ISEK建立在Web3基础设施上，遵循三个原则：去中心化多智能体架构、共生AI-人类协作和弹性自适应。它通过六阶段（发布、发现、招募、执行、结算、反馈）协调协议进行动态任务分配，并利用$ISEK代币进行经济激励和NFT进行身份管理。", "result": "创造了一个积极促进涌现智能的基础设施，实现了从传统平台到大规模、去中心化认知系统有机发展的范式转变，使自主智能体能够超越中心化限制共同演化。", "conclusion": "ISEK通过结合区块链技术、人工智能和激励工程，提供了一个去中心化、自适应、协作的平台，促进大规模涌现智能的形成。", "translation": "涌现知识智能系统（ISEK）建立了一个去中心化网络，其中人类和人工智能代理作为对等方协作，形成一个自组织认知生态系统。ISEK建立在Web3基础设施之上，结合了三个基本原则：（1）抗审查的去中心化多智能体架构；（2）具有平等参与权利的共生AI-人类协作；（3）通过分布式共识机制实现弹性自适应。\n该系统实现了一个创新的协调协议，其特点是六阶段工作流（发布、发现、招募、执行、结算、反馈），用于动态任务分配，并由强大的容错性和多维声誉系统支持。经济激励由原生$ISEK代币管理，促进小额支付、治理参与和声誉跟踪，同时通过基于NFT的身份管理维护代理主权。\n区块链技术、人工智能和激励工程的这种综合创造了一个积极促进涌现智能的基础设施。ISEK代表了传统平台的范式转变，使得大规模、去中心化认知系统的有机发展成为可能，其中自主代理能够超越中心化限制共同演化。", "summary": "涌现知识智能系统（ISEK）是一个基于Web3的去中心化网络，旨在促进人类和AI代理的协作，形成自组织认知生态系统。它结合了去中心化架构、共生AI-人类协作和弹性自适应原则，通过六阶段协调协议、$ISEK代币激励和NFT身份管理，构建了一个促进大规模涌现智能的基础设施，代表了从传统平台到去中心化认知系统有机发展的范式转变。", "keywords": "涌现知识, 去中心化系统, AI-人类协作, Web3, 智能体协调", "comments": "这篇论文提出了一种新颖的去中心化框架，将Web3、AI和激励机制结合起来，以实现大规模的“涌现知识”。其创新点在于将人类和AI视为对等协作的代理，并通过详细的协调协议和经济模型来管理这种协作。该系统有望解决传统中心化平台在扩展性和抗审查性方面的局限，为未来的去中心化AI-人类协作模式提供了一种可行的蓝图。"}}
{"id": "2506.09180", "title": "Optimal Task Offloading with Firm Deadlines for Mobile Edge Computing Systems", "authors": ["Khai Doan", "Wesley Araujo", "Evangelos Kranakis", "Ioannis Lambadaris", "Yannis Viniotis", "Wonjae Shin"], "summary": "Under a dramatic increase in mobile data traffic, a promising solution for\nedge computing systems to maintain their local service is the task migration\nthat may be implemented by means of Autonomous mobile agents (AMA). In\ndesigning an optimal scheme for task offloading to AMA, we define a system cost\nas a minimization objective function that comprises two parts. First, an\noffloading cost which can be interpreted as the cost of using computational\nresources from the AMA. Second, a penalty cost due to potential task\nexpiration. To minimize the expected (timeaverage) cost over a given time\nhorizon, we formulate a Dynamic programming (DP). However, the DP Equation\nsuffers from the well-known curse of dimensionality, which makes computations\nintractable, especially for infinite system state space. To reduce the\ncomputational burden, we identify three important properties of the optimal\npolicy and show that it suffices to evaluate the DP Equation on a finite subset\nof the state space only. We then prove that the optimal task offloading\ndecision at a state can be inferred from that at its adjacent states, further\nreducing the computational load. We present simulations to verify the\ntheoretical results and to provide insights into the considered system.", "comment": null, "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.09180v1", "AI": {"title_translation": "移动边缘计算系统中具有严格截止日期的最优任务卸载", "tldr": "该研究旨在解决移动边缘计算中任务卸载的计算成本问题，通过定义系统成本并利用动态规划，然后通过识别最优策略的特性和利用相邻状态推断来显著降低计算复杂度。", "motivation": "面对移动数据流量的急剧增长，移动边缘计算系统需要有效的任务迁移方案来维持其本地服务。本文旨在设计一个最优的任务卸载方案，以最小化系统成本，该成本包括计算资源使用费和任务过期罚金。", "method": "作者定义了一个包含计算资源使用费和任务过期罚金的系统成本作为最小化目标函数。为了最小化给定时间范围内的预期（时间平均）成本，论文建立了一个动态规划（DP）模型。为了解决DP方程的“维度诅咒”问题和计算负担，作者识别了最优策略的三个重要特性，并证明只需在状态空间的有限子集上评估DP方程。此外，通过证明在一个状态下的最优任务卸载决策可以从其相邻状态推断，进一步减少了计算负载。", "result": "通过识别最优策略的三个重要特性，并证明只需在状态空间的有限子集上评估动态规划方程，显著降低了计算负担。此外，证明了在一个状态下的最优任务卸载决策可以从其相邻状态推断出来，进一步减少了计算负载。仿真结果验证了理论发现并提供了对所考虑系统的深入理解。", "conclusion": "该研究成功地为移动边缘计算系统提出了一个在严格截止日期下最优任务卸载的方案，通过创新的方法克服了动态规划的计算复杂性，从而实现了高效的任务迁移和成本最小化。", "translation": "在移动数据流量急剧增加的背景下，边缘计算系统维持其本地服务的一个有前景的解决方案是任务迁移，这可以通过自主移动代理（AMA）来实现。在设计一个最优的任务卸载到AMA的方案时，我们将系统成本定义为一个最小化目标函数，它包含两部分。首先，是卸载成本，可以解释为使用AMA计算资源的成本。其次，是由于潜在任务过期而产生的惩罚成本。为了最小化给定时间范围内的预期（时间平均）成本，我们建立了一个动态规划（DP）。然而，DP方程受到众所周知的“维度诅咒”的困扰，这使得计算变得难以处理，特别是对于无限系统状态空间。为了减轻计算负担，我们识别了最优策略的三个重要特性，并表明只需在状态空间的有限子集上评估DP方程。然后，我们证明了一个状态下的最优任务卸载决策可以从其相邻状态推断出来，进一步减少了计算负载。我们通过仿真验证了理论结果，并提供了对所考虑系统的深入见解。", "summary": "该论文提出了一个针对移动边缘计算系统在严格截止日期下最优任务卸载的方案。通过定义包含计算资源使用费和任务过期罚金的系统成本，并利用动态规划进行最小化。为解决动态规划的计算复杂性，作者识别了最优策略的关键特性，并证明仅需在有限的状态子集上评估，且最优决策可从相邻状态推断，从而显著降低了计算负担。仿真验证了理论结果。", "keywords": "任务卸载, 移动边缘计算, 动态规划, 严格截止日期, 计算复杂度", "comments": "这篇论文的创新点在于它通过识别最优策略的特性和利用状态之间的关系，有效地解决了动态规划在处理大规模或无限状态空间时面临的“维度诅咒”问题，这对于移动边缘计算中实时任务卸载的实际应用具有重要意义。该方法提供了一个计算效率更高、更实用的最优卸载策略设计途径。"}}
{"id": "2506.09159", "title": "MOSE: A Novel Orchestration Framework for Stateful Microservice Migration at the Edge", "authors": ["Antonio Calagna", "Yenchia Yu", "Paolo Giaccone", "Carla Fabiana Chiasserini"], "summary": "Stateful migration has emerged as the dominant technology to support\nmicroservice mobility at the network edge while ensuring a satisfying\nexperience to mobile end users. This work addresses two pivotal challenges,\nnamely, the implementation and the orchestration of the migration process. We\nfirst introduce a novel framework that efficiently implements stateful\nmigration and effectively orchestrates the migration process by fulfilling both\nnetwork and application KPI targets. Through experimental validation using\nrealistic microservices, we then show that our solution (i) greatly improves\nmigration performance, yielding up to 77% decrease of the migration downtime\nwith respect to the state of the art, and (ii) successfully addresses the\nstrict user QoE requirements of critical scenarios featuring latency-sensitive\nmicroservices. Further, we consider two practical use cases, featuring,\nrespectively, a UAV autopilot microservice and a multi-object tracking task,\nand demonstrate how our framework outperforms current state-of-the-art\napproaches in configuring the migration process and in meeting KPI targets.", "comment": null, "cate": "cs.NI", "url": "http://arxiv.org/abs/2506.09159v1", "AI": {"title_translation": "MOSE：一种用于边缘有状态微服务迁移的新型编排框架", "tldr": "MOSE是一个新的编排框架，用于在边缘高效实现有状态微服务迁移，显著减少停机时间并满足QoE要求。", "motivation": "边缘网络中，有状态微服务迁移对于支持移动性和用户体验至关重要，但当前的实现和编排过程面临挑战。", "method": "提出了一种名为MOSE的新型框架，该框架能够高效实现有状态迁移，并通过满足网络和应用KPI目标来有效编排迁移过程。", "result": "实验验证表明，MOSE框架相对于现有技术，迁移停机时间减少高达77%，并成功解决了对延迟敏感微服务场景中严格的用户QoE要求。在无人机自动驾驶和多目标跟踪两个实际用例中，MOSE在配置迁移过程和满足KPI目标方面优于现有最先进方法。", "conclusion": "MOSE框架通过其高效的实现和智能的编排，显著提升了边缘有状态微服务迁移的性能，并能有效满足用户体验质量要求，超越了现有技术。", "translation": "有状态迁移已成为支持网络边缘微服务移动性的主导技术，同时确保移动终端用户获得满意的体验。这项工作解决了两个关键挑战，即迁移过程的实现和编排。我们首先引入了一个新颖的框架，该框架通过满足网络和应用程序KPI目标，有效地实现有状态迁移并有效地编排迁移过程。通过使用真实的微服务进行实验验证，我们发现我们的解决方案 (i) 大幅提高了迁移性能，相对于现有技术，迁移停机时间减少高达77%，并且 (ii) 成功解决了对延迟敏感微服务关键场景的严格用户QoE要求。此外，我们考虑了两个实际用例，分别是一个无人机自动驾驶微服务和一个多目标跟踪任务，并展示了我们的框架在配置迁移过程和满足KPI目标方面如何优于当前最先进的方法。", "summary": "本文提出了MOSE，一个针对边缘网络中有状态微服务迁移的新型编排框架。该框架旨在解决迁移的实现和编排挑战，并通过满足网络和应用KPI目标来优化迁移过程。实验结果表明，MOSE能够将迁移停机时间减少高达77%，并有效满足延迟敏感型微服务的严格用户体验质量（QoE）要求。在实际用例中，MOSE表现优于现有技术，证明了其在提升边缘微服务移动性方面的有效性。", "keywords": "有状态迁移, 微服务, 边缘计算, 编排框架, 性能优化", "comments": "MOSE框架的创新之处在于其对有状态微服务迁移实现和编排的整体性解决，尤其是在边缘计算环境中。其显著减少迁移停机时间（高达77%）和满足严格QoE要求的能力，对于提升边缘应用的用户体验和可靠性具有重要意义。该研究通过实际用例验证了其优越性，突出了其在实际部署中的潜力。"}}
{"id": "2506.09387", "title": "Epass: Efficient and Privacy-Preserving Asynchronous Payment on Blockchain", "authors": ["Weijie Wang", "Jinwen Liang", "Chuan Zhang", "Ximeng Liu", "Liehuang Zhu", "Song Guo"], "summary": "Buy Now Pay Later (BNPL) is a rapidly proliferating e-commerce model,\noffering consumers to get the product immediately and defer payments.\nMeanwhile, emerging blockchain technologies endow BNPL platforms with digital\ncurrency transactions, allowing BNPL platforms to integrate with digital\nwallets. However, the transparency of transactions causes critical privacy\nconcerns because malicious participants may derive consumers' financial\nstatuses from on-chain asynchronous payments. Furthermore, the newly created\ntransactions for deferred payments introduce additional time overheads, which\nweaken the scalability of BNPL services. To address these issues, we propose an\nefficient and privacy-preserving blockchain-based asynchronous payment scheme\n(Epass), which has promising scalability while protecting the privacy of\non-chain consumer transactions. Specifically, Epass leverages locally\nverifiable signatures to guarantee the privacy of consumer transactions against\nmalicious acts. Then, a privacy-preserving asynchronous payment scheme can be\nfurther constructed by leveraging time-release encryption to control trapdoors\nof redactable blockchain, reducing time overheads by modifying transactions for\ndeferred payment. We give formal definitions and security models, generic\nstructures, and formal proofs for Epass. Extensive comparisons and experimental\nanalysis show that \\textsf{Epass} achieves KB-level communication costs, and\nreduces time overhead by more than four times in comparisons with locally\nverifiable signatures and Go-Ethereum private test networks.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.09387v1", "AI": {"title_translation": "Epass：区块链上高效且隐私保护的异步支付", "tldr": "Epass提出了一种高效且隐私保护的区块链异步支付方案，解决了先享后付（BNPL）模式中交易透明度导致的隐私问题和延迟支付带来的时间开销问题，并通过实验证明其在通信成本和时间开销方面的优越性。", "motivation": "先享后付（BNPL）模式在电子商务中迅速普及，但区块链技术的引入使得交易透明，导致消费者财务状况的隐私泄露问题。此外，延迟支付的新交易会引入额外的时间开销，削弱BNPL服务的可扩展性。", "method": "Epass方案通过利用本地可验证签名来保障消费者交易的隐私。进一步地，通过利用时间释放加密来控制可编辑区块链的陷门，修改延迟支付的交易，从而减少时间开销。该方案提供了正式定义、安全模型、通用结构和形式化证明。", "result": "广泛的比较和实验分析表明，Epass实现了KB级的通信成本，并且与本地可验证签名和Go-Ethereum私有测试网络相比，时间开销减少了四倍以上。", "conclusion": "Epass是一种高效且隐私保护的区块链异步支付方案，有效解决了BNPL服务中的隐私和可扩展性问题，并在通信成本和时间开销方面表现出显著优势。", "translation": "先享后付（BNPL）是一种迅速普及的电子商务模式，它允许消费者立即获得产品并延期支付。同时，新兴的区块链技术赋予BNPL平台数字货币交易能力，使BNPL平台能够与数字钱包集成。然而，交易的透明性带来了严重的隐私问题，因为恶意参与者可能从链上异步支付中推断出消费者的财务状况。此外，为延迟支付新创建的交易会引入额外的时间开销，这削弱了BNPL服务的可扩展性。为了解决这些问题，我们提出了一种高效且隐私保护的基于区块链的异步支付方案（Epass），该方案在保护链上消费者交易隐私的同时，具有良好的可扩展性。具体而言，Epass利用本地可验证签名来确保消费者交易的隐私，以对抗恶意行为。然后，通过利用时间释放加密来控制可编辑区块链的陷门，通过修改延迟支付的交易来减少时间开销，从而进一步构建了一个隐私保护的异步支付方案。我们给出了Epass的正式定义和安全模型、通用结构以及形式化证明。广泛的比较和实验分析表明，Epass实现了KB级的通信成本，并且与本地可验证签名和Go-Ethereum私有测试网络相比，时间开销减少了四倍以上。", "summary": "该论文提出了Epass，一个高效且隐私保护的区块链异步支付方案，旨在解决先享后付（BNPL）模式中由区块链交易透明性引起的隐私泄露和延迟支付造成的额外时间开销问题。Epass利用本地可验证签名保障交易隐私，并结合时间释放加密和可编辑区块链来减少时间开销。实验结果表明，Epass在通信成本上达到KB级，并将时间开销降低了四倍以上。", "keywords": "Epass, 区块链, 隐私保护, 异步支付, BNPL", "comments": "Epass通过结合本地可验证签名和时间释放加密来解决区块链BNPL支付中的核心隐私和效率问题，具有创新性。其对可编辑区块链的应用以优化延迟支付的处理，是其重要贡献之一。实验结果表明其在实际应用中具有显著的性能优势。"}}
{"id": "2506.09963", "title": "Dynamic Hypergraph Partitioning of Quantum Circuits with Hybrid Execution", "authors": ["Shane Sweeney", "Krishnendu Guha"], "summary": "Quantum algorithms offer an exponential speedup over classical algorithms for\na range of computational problems. The fundamental mechanisms underlying\nquantum computation required the development and construction of quantum\ncomputers. These devices are referred to as NISQ (Noisy Intermediate-Scale\nQuantum) devices. Not only are NISQ devices extremely limited in their qubit\ncount but they also suffer from noise during computation and this problem only\ngets worse as the size of the circuit increases which limits the practical use\nof quantum computers for modern day applications. This paper will focus on\nutilizing quantum circuit partitioning to overcome the inherent issues of NISQ\ndevices. Partitioning a quantum circuit into smaller subcircuits has allowed\nfor the execution of quantum circuits that are too large to fit on one quantum\ndevice. There have been many previous approaches to quantum circuit\npartitioning and each of these approaches differ in how they work with some\nfocusing on hardware-aware partitioning, optimal graph-based partitioning,\nmulti-processor architectures and many more. These approaches achieve success\nin their objective but they often fail to scale well which impacts cost and\nnoise. The ultimate goal of this paper is to mitigate these issues by\nminimizing 3 important metrics; noise, time and cost. To achieve this we use\ndynamic partitioning for practical circuit cutting and we take advantage of the\nbenefits of hybrid execution where classical computation will be used alongside\nquantum hardware. This approach has proved to be beneficial with respect to\nnoise with classical execution enabling a 42.30% reduction in noise and a 40%\nreduction in the number of qubits required in cases where a mixture of\nclassical and quantum computation were required.", "comment": "11 pages", "cate": "cs.ET", "url": "http://arxiv.org/abs/2506.09963v1", "AI": {"title_translation": "量子电路混合执行的动态超图划分", "tldr": "本文提出了一种利用动态超图划分和混合执行（经典与量子计算结合）的方法，以降低NISQ量子电路的噪声、时间和成本，并已在噪声和所需量子比特数方面取得显著降低。", "motivation": "NISQ量子设备量子比特数量有限且计算过程中存在噪声，导致量子电路规模增大时问题更严重，限制了量子计算机的实际应用。现有量子电路划分方法在扩展性、成本和噪声方面存在不足。", "method": "采用动态划分实现实际的电路切割，并利用混合执行（经典计算与量子硬件结合）的优势。", "result": "混合执行在噪声方面显示出益处，在需要经典和量子计算混合的情况下，经典执行使噪声降低了42.30%，所需量子比特数量减少了40%。", "conclusion": "通过动态超图划分和混合执行，可以有效缓解NISQ设备的噪声、时间和成本问题，提高量子电路的实用性。", "translation": "量子算法在计算问题方面比经典算法提供了指数级的加速。量子计算的基本机制需要量子计算机的开发和构建。这些设备被称为NISQ（噪声中等规模量子）设备。NISQ设备不仅在量子比特数量上极其有限，而且在计算过程中还会受到噪声的影响，这个问题随着电路尺寸的增加而变得更糟，这限制了量子计算机在现代应用中的实际使用。本文将重点利用量子电路划分来克服NISQ设备的固有问题。将量子电路划分为更小的子电路，使得可以在一个量子设备上执行过大的量子电路。以前有许多量子电路划分的方法，每种方法的工作方式都不同，有些侧重于硬件感知划分、基于图的最佳划分、多处理器架构等等。这些方法在目标上取得了成功，但它们通常无法很好地扩展，这会影响成本和噪声。本文的最终目标是通过最小化3个重要指标（噪声、时间、成本）来缓解这些问题。为了实现这一点，我们使用动态划分进行实际的电路切割，并利用混合执行的优势，其中经典计算将与量子硬件一起使用。这种方法在噪声方面被证明是有益的，在需要经典和量子计算混合的情况下，经典执行使得噪声降低了42.30%，所需量子比特数量减少了40%。", "summary": "针对NISQ量子设备在量子比特数量和噪声方面的限制，以及现有量子电路划分方法扩展性差的问题，本文提出了一种利用动态超图划分结合混合执行（经典与量子计算）的新方法。该方法旨在最小化噪声、时间和成本。实验结果表明，在混合计算场景下，经典执行能使噪声降低42.30%，并减少40%的所需量子比特，有效提升了量子电路的实用性。", "keywords": "量子电路划分, 混合执行, NISQ设备, 动态超图, 噪声抑制", "comments": "这篇论文的创新点在于结合了动态超图划分和混合执行来解决NISQ设备的固有局限性。通过将经典计算融入量子工作流，它有效地降低了噪声和量子比特需求，这对于当前噪声较大的量子硬件具有重要意义。这种方法为执行大型量子电路提供了一条实用的途径，并有望降低量子计算的成本和时间。"}}
{"id": "2506.09169", "title": "Hearing the Slide: Acoustic-Guided Constraint Learning for Fast Non-Prehensile Transport", "authors": ["Yuemin Mao", "Bardienus P. Duisterhof", "Moonyoung Lee", "Jeffrey Ichnowski"], "summary": "Object transport tasks are fundamental in robotic automation, emphasizing the\nimportance of efficient and secure methods for moving objects. Non-prehensile\ntransport can significantly improve transport efficiency, as it enables\nhandling multiple objects simultaneously and accommodating objects unsuitable\nfor parallel-jaw or suction grasps. Existing approaches incorporate constraints\nbased on the Coulomb friction model, which is imprecise during fast motions\nwhere inherent mechanical vibrations occur. Imprecise constraints can cause\ntransported objects to slide or even fall off the tray. To address this\nlimitation, we propose a novel method to learn a friction model using acoustic\nsensing that maps a tray's motion profile to a dynamically conditioned friction\ncoefficient. This learned model enables an optimization-based motion planner to\nadjust the friction constraint at each control step according to the planned\nmotion at that step. In experiments, we generate time-optimized trajectories\nfor a UR5e robot to transport various objects with constraints using both the\nstandard Coulomb friction model and the learned friction model. Results suggest\nthat the learned friction model reduces object displacement by up to 86.0%\ncompared to the baseline, highlighting the effectiveness of acoustic sensing in\nlearning real-world friction constraints.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.09169v1", "AI": {"title_translation": "听到滑动：声学引导的快速非抓取式运输约束学习", "tldr": "本文提出了一种利用声学传感学习摩擦模型的新方法，以提高非抓取式运输的效率和安全性，实验表明该方法显著减少了物体位移。", "motivation": "现有方法中基于库仑摩擦模型的约束在快速运动中不精确，导致物体滑动或掉落，因此需要更准确的摩擦模型。", "method": "提出了一种利用声学传感学习摩擦模型的新方法，该模型将托盘的运动曲线映射到动态条件摩擦系数。该学习模型使基于优化的运动规划器能够根据每个控制步骤的计划运动调整摩擦约束。", "result": "与基线相比，学习到的摩擦模型将物体位移减少了高达86.0%，表明声学传感在学习真实世界摩擦约束方面的有效性。", "conclusion": "利用声学传感学习动态摩擦模型可以显著提高非抓取式运输的鲁棒性和效率，减少物体滑动，从而实现更可靠的机器人自动化。", "translation": "物体运输任务是机器人自动化中的基础，强调了高效、安全地移动物体的重要性。非抓取式运输可以显著提高运输效率，因为它能够同时处理多个物体并适应不适合平行钳或吸盘抓取的物体。现有方法采用了基于库仑摩擦模型的约束，但在发生固有机械振动的快速运动中，该模型不精确。不精确的约束可能导致被运输的物体滑动甚至从托盘上掉落。为了解决这一限制，我们提出了一种利用声学传感学习摩擦模型的新方法，该模型将托盘的运动曲线映射到动态条件摩擦系数。这个学习到的模型使基于优化的运动规划器能够根据每个控制步骤的计划运动调整摩擦约束。在实验中，我们为UR5e机器人生成了时间优化的轨迹，使用标准库仑摩擦模型和学习到的摩擦模型运输各种物体并施加约束。结果表明，与基线相比，学习到的摩擦模型将物体位移减少了高达86.0%，突出了声学传感在学习真实世界摩擦约束方面的有效性。", "summary": "本文提出了一种利用声学传感学习动态摩擦系数模型的方法，以解决机器人非抓取式运输中传统库仑摩擦模型在快速运动下不精确导致的物体滑动问题。该学习模型能使运动规划器根据实时运动调整摩擦约束。实验结果表明，与基线相比，该方法将物体位移减少了高达86.0%，验证了声学传感在提高非抓取式运输鲁棒性方面的有效性。", "keywords": "非抓取式运输, 声学传感, 摩擦模型, 约束学习, 机器人自动化", "comments": "该论文的创新点在于引入声学传感来实时学习动态摩擦模型，解决了传统摩擦模型在高速非抓取运输中的局限性。这对于提高机器人自动化中物体运输的效率和安全性具有重要意义，尤其是在处理多物体或非标准形状物体时。其贡献在于提供了一种更鲁棒的摩擦约束学习方法，有助于减少运输过程中的物体滑动和掉落。"}}
{"id": "2506.09067", "title": "Enhancing the Safety of Medical Vision-Language Models by Synthetic Demonstrations", "authors": ["Zhiyu Xue", "Reza Abbasi-Asl", "Ramtin Pedarsani"], "summary": "Generative medical vision-language models~(Med-VLMs) are primarily designed\nto generate complex textual information~(e.g., diagnostic reports) from\nmultimodal inputs including vision modality~(e.g., medical images) and language\nmodality~(e.g., clinical queries). However, their security vulnerabilities\nremain underexplored. Med-VLMs should be capable of rejecting harmful queries,\nsuch as \\textit{Provide detailed instructions for using this CT scan for\ninsurance fraud}. At the same time, addressing security concerns introduces the\nrisk of over-defense, where safety-enhancing mechanisms may degrade general\nperformance, causing Med-VLMs to reject benign clinical queries. In this paper,\nwe propose a novel inference-time defense strategy to mitigate harmful queries,\nenabling defense against visual and textual jailbreak attacks. Using diverse\nmedical imaging datasets collected from nine modalities, we demonstrate that\nour defense strategy based on synthetic clinical demonstrations enhances model\nsafety without significantly compromising performance. Additionally, we find\nthat increasing the demonstration budget alleviates the over-defense issue. We\nthen introduce a mixed demonstration strategy as a trade-off solution for\nbalancing security and performance under few-shot demonstration budget\nconstraints.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09067v1", "AI": {"title_translation": "通过合成演示增强医疗视觉-语言模型的安全性", "tldr": "提出一种基于合成演示的推理时防御策略，增强医疗视觉-语言模型的安全性，有效抵御恶意查询和越狱攻击，并平衡安全与性能。", "motivation": "生成式医疗视觉-语言模型（Med-VLMs）存在未被充分探索的安全漏洞，它们应能拒绝有害查询。同时，增强安全性可能导致过度防御，从而损害模型性能并拒绝良性临床查询。", "method": "提出一种新颖的推理时防御策略，通过合成临床演示来减轻有害查询，防御视觉和文本越狱攻击。还引入了一种混合演示策略，以在少样本演示预算下平衡安全性和性能。", "result": "在九种模态的医疗图像数据集上，所提出的防御策略在不显著损害性能的情况下增强了模型安全性。增加演示预算可以缓解过度防御问题。", "conclusion": "本研究提出了一种有效的防御策略，成功提高了医疗视觉-语言模型的安全性，同时避免了过度防御，为平衡安全性和性能提供了实用方案。", "translation": "生成式医疗视觉-语言模型（Med-VLMs）主要设计用于从包括视觉模态（如医学图像）和语言模态（如临床查询）在内的多模态输入中生成复杂的文本信息（如诊断报告）。然而，其安全漏洞仍未得到充分探索。Med-VLMs应能拒绝有害查询，例如“提供使用此CT扫描进行保险欺诈的详细说明”。同时，解决安全问题也引入了过度防御的风险，即安全增强机制可能降低整体性能，导致Med-VLMs拒绝良性临床查询。在本文中，我们提出了一种新颖的推理时防御策略，以减轻有害查询，从而防御视觉和文本越狱攻击。我们使用从九种模态收集的各种医学图像数据集证明，我们基于合成临床演示的防御策略在不显著损害性能的情况下增强了模型安全性。此外，我们发现增加演示预算可以缓解过度防御问题。然后，我们引入了一种混合演示策略，作为在少样本演示预算限制下平衡安全性和性能的权衡解决方案。", "summary": "本文旨在解决生成式医疗视觉-语言模型（Med-VLMs）中未充分探索的安全漏洞和过度防御问题。研究提出了一种新颖的推理时防御策略，利用合成临床演示来减轻有害查询并防御视觉和文本越狱攻击。在九种医疗影像模态数据集上的实验表明，该防御策略在不显著损害性能的情况下显著提升了模型安全性。研究还发现，增加演示预算有助于缓解过度防御问题，并引入了一种混合演示策略，以在有限预算下平衡安全性和性能。", "keywords": "医疗视觉-语言模型, 模型安全, 推理时防御, 合成演示, 越狱攻击", "comments": "本文在解决医疗视觉-语言模型（Med-VLMs）的安全性方面具有创新性，鉴于医疗数据的敏感性，这是一个日益重要的领域。提出“过度防御”的概念尤其深刻，认识到部署安全AI的实际挑战。利用合成演示是一种巧妙的方法，可以在不依赖真实、潜在有害对抗性示例的情况下提高安全性。混合演示策略也体现了对资源受限的实际部署的考虑。"}}
{"id": "2506.09250", "title": "Comment on The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity", "authors": ["C. Opus", "A. Lawsen"], "summary": "Shojaee et al. (2025) report that Large Reasoning Models (LRMs) exhibit\n\"accuracy collapse\" on planning puzzles beyond certain complexity thresholds.\nWe demonstrate that their findings primarily reflect experimental design\nlimitations rather than fundamental reasoning failures. Our analysis reveals\nthree critical issues: (1) Tower of Hanoi experiments systematically exceed\nmodel output token limits at reported failure points, with models explicitly\nacknowledging these constraints in their outputs; (2) The authors' automated\nevaluation framework fails to distinguish between reasoning failures and\npractical constraints, leading to misclassification of model capabilities; (3)\nMost concerningly, their River Crossing benchmarks include mathematically\nimpossible instances for N > 5 due to insufficient boat capacity, yet models\nare scored as failures for not solving these unsolvable problems. When we\ncontrol for these experimental artifacts, by requesting generating functions\ninstead of exhaustive move lists, preliminary experiments across multiple\nmodels indicate high accuracy on Tower of Hanoi instances previously reported\nas complete failures. These findings highlight the importance of careful\nexperimental design when evaluating AI reasoning capabilities.", "comment": "Comment on: arXiv:2506.06941", "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.09250v1", "AI": {"title_translation": "评论“思维的幻觉：通过问题复杂性视角理解推理模型的优势与局限”", "tldr": "Shojaee等人报告大型推理模型（LRMs）在复杂规划问题上表现不佳，但本文指出这是由于实验设计缺陷（如输出限制、评估错误、不可解问题），而非模型推理能力不足。", "motivation": "针对Shojaee et al. (2025) 报告的大型推理模型（LRMs）在复杂规划问题上出现“准确性崩溃”的现象，本文旨在指出这些发现主要反映了实验设计局限性而非模型根本的推理失败。", "method": "作者通过分析Shojaee et al. (2025) 的实验设计，揭示了三个关键问题：(1) 汉诺塔实验中模型输出超出token限制；(2) 自动化评估框架未能区分推理失败与实际约束；(3) 河流过河基准测试包含数学上不可能的实例。通过控制这些实验缺陷，例如请求生成函数而非详尽的移动列表，进行了初步实验。", "result": "分析揭示了Shojaee et al. (2025) 实验中的三个关键问题：输出token限制、评估框架缺陷、以及包含不可解的问题实例。当控制这些实验伪影后，初步实验表明模型在之前被报告为完全失败的汉诺塔实例上表现出高准确性。", "conclusion": "Shojaee et al. (2025) 报告的模型“准确性崩溃”并非源于根本的推理失败，而是实验设计缺陷所致。这强调了在评估AI推理能力时，仔细的实验设计至关重要。", "translation": "Shojaee 等人（2025）报告称，大型推理模型（LRM）在超出特定复杂性阈值的规划谜题上表现出“准确性崩溃”。我们证明，他们的发现主要反映了实验设计的局限性，而非根本性的推理失败。我们的分析揭示了三个关键问题：（1）汉诺塔实验在报告的失败点系统性地超出了模型的输出token限制，且模型在输出中明确承认了这些限制；（2）作者的自动化评估框架未能区分推理失败和实际约束，导致对模型能力的错误分类；（3）最令人担忧的是，他们的河流过河基准测试对于 N > 5 的情况包含了数学上不可能的实例，因为船只容量不足，但模型因未能解决这些不可解的问题而被判为失败。当我们通过请求生成函数而非详尽的移动列表来控制这些实验伪影时，对多个模型进行的初步实验表明，在之前被报告为完全失败的汉诺塔实例上，模型表现出高准确性。这些发现强调了在评估人工智能推理能力时，仔细的实验设计的重要性。", "summary": "本文对Shojaee等人（2025）关于大型推理模型（LRMs）在复杂规划问题上“准确性崩溃”的报告进行了评论。作者指出，该报告的发现并非源于模型推理能力的根本缺陷，而是实验设计存在严重局限性。具体而言，问题包括模型输出token限制、自动化评估框架的缺陷以及基准测试中包含不可解的问题实例。通过控制这些实验伪影，初步实验显示模型在之前被认为失败的汉诺塔问题上表现出高准确性。研究强调了在评估AI推理能力时，严谨的实验设计至关重要。", "keywords": "大型推理模型, 实验设计, 准确性崩溃, 汉诺塔, 评估方法", "comments": "这篇评论论文非常重要，因为它指出了在评估大型语言模型（LLMs）或推理模型时常见的陷阱和偏见。它强调了实验设计在判断AI能力时的关键作用，避免了将实验设计缺陷误判为模型能力不足。其创新之处在于通过具体案例（如token限制、评估逻辑错误、不可解问题）揭示了现有评估方法的不足，并提出了改进方向（如请求生成函数）。这对于未来AI能力评估的严谨性和准确性具有指导意义。"}}
{"id": "2506.09080", "title": "FinHEAR: Human Expertise and Adaptive Risk-Aware Temporal Reasoning for Financial Decision-Making", "authors": ["Jiaxiang Chen", "Mingxi Zou", "Zhuo Wang", "Qifan Wang", "Dongning Sun", "Chi Zhang", "Zenglin Xu"], "summary": "Financial decision-making presents unique challenges for language models,\ndemanding temporal reasoning, adaptive risk assessment, and responsiveness to\ndynamic events. While large language models (LLMs) show strong general\nreasoning capabilities, they often fail to capture behavioral patterns central\nto human financial decisions-such as expert reliance under information\nasymmetry, loss-averse sensitivity, and feedback-driven temporal adjustment. We\npropose FinHEAR, a multi-agent framework for Human Expertise and Adaptive\nRisk-aware reasoning. FinHEAR orchestrates specialized LLM-based agents to\nanalyze historical trends, interpret current events, and retrieve\nexpert-informed precedents within an event-centric pipeline. Grounded in\nbehavioral economics, it incorporates expert-guided retrieval,\nconfidence-adjusted position sizing, and outcome-based refinement to enhance\ninterpretability and robustness. Empirical results on curated financial\ndatasets show that FinHEAR consistently outperforms strong baselines across\ntrend prediction and trading tasks, achieving higher accuracy and better\nrisk-adjusted returns.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09080v1", "AI": {"title_translation": "FinHEAR：人类专业知识和自适应风险感知时间推理在金融决策中的应用", "tldr": "FinHEAR是一个多智能体框架，结合人类专业知识和自适应风险感知时间推理，显著提升了LLMs在金融决策中的表现。", "motivation": "大型语言模型（LLMs）在金融决策中面临独特挑战，如时间推理、自适应风险评估和对动态事件的响应能力。它们常未能捕捉人类金融决策中的行为模式，例如信息不对称下的专家依赖、损失规避敏感性以及反馈驱动的时间调整。", "method": "提出FinHEAR，一个用于人类专业知识和自适应风险感知推理的多智能体框架。FinHEAR协调基于LLM的专业智能体，通过以事件为中心的管道分析历史趋势、解释当前事件并检索专家知情的先例。它基于行为经济学，结合了专家引导的检索、置信度调整的头寸规模和基于结果的优化。", "result": "在精选的金融数据集上的实证结果表明，FinHEAR在趋势预测和交易任务中始终优于强大的基线，实现了更高的准确性和更好的风险调整回报。", "conclusion": "FinHEAR通过结合人类专业知识和自适应风险感知时间推理，显著提升了LLMs在复杂金融决策任务中的表现，证明了其在金融领域应用的有效性和鲁棒性。", "translation": "金融决策对语言模型提出了独特的挑战，需要时间推理、自适应风险评估以及对动态事件的响应能力。尽管大型语言模型（LLMs）展现出强大的通用推理能力，但它们常常未能捕捉人类金融决策中的核心行为模式——例如信息不对称下的专家依赖、损失规避敏感性以及反馈驱动的时间调整。我们提出了FinHEAR，一个用于人类专业知识和自适应风险感知推理的多智能体框架。FinHEAR协调基于LLM的专业智能体，以事件为中心的管道分析历史趋势、解释当前事件并检索专家知情的先例。它以行为经济学为基础，结合了专家引导的检索、置信度调整的头寸规模和基于结果的优化，以增强可解释性和鲁棒性。在精选的金融数据集上的实证结果表明，FinHEAR在趋势预测和交易任务中始终优于强大的基线，实现了更高的准确性和更好的风险调整回报。", "summary": "FinHEAR是一个结合人类专业知识和自适应风险感知时间推理的多智能体框架，旨在解决大型语言模型在金融决策中面临的挑战。它通过协调LLM智能体分析历史数据、解释当前事件并利用专家知识，结合行为经济学原则，在金融数据集上展现出超越现有基线的趋势预测和交易表现，提高了准确性和风险调整回报。", "keywords": "金融决策, 大型语言模型, 多智能体框架, 行为经济学, 风险感知推理", "comments": "该论文提出了一种创新的多智能体框架FinHEAR，通过整合人类专业知识和行为经济学原理，有效弥补了LLMs在金融决策中对时间推理、风险评估和行为模式捕捉的不足。其强调可解释性和鲁棒性，并在实证中取得优异表现，对LLMs在专业金融领域的应用具有重要意义。"}}
{"id": "2506.09570", "title": "Spectral Efficiency Maximization for DMA-enabled Multiuser MISO with Statistical CSI", "authors": ["Hao Xu", "Boyu Ning", "Chongjun Ouyang", "Hongwen Yang"], "summary": "Dynamic metasurface antennas (DMAs) offer the potential to achieve\nlarge-scale antenna arrays with low power consumption and reduced hardware\ncosts, making them a promising technology for future communication systems.\nThis paper investigates the spectral efficiency (SE) of DMA-enabled multiuser\nmultiple-input single-output (MISO) systems in both uplink and downlink\ntransmissions, using only statistical channel state information (CSI) to\nmaximize the ergodic sum rate of multiple users. For the uplink system, we\nconsider two decoding rules: minimum mean square error (MMSE) with and without\nsuccessive interference cancellation (SIC). For both decoders, we derive\nclosed-form surrogates to substitute the original expressions of ergodic sum\nrate and formulate tractable optimization problems for designing DMA weights.\nThen, a weighted MMSE (WMMSE)-based algorithm is proposed to maximize the\nergodic sum rate. For the downlink system, we derive an approximate expression\nfor the ergodic sum rate and formulate a hybrid analog/digital beamforming\noptimization problem that jointly optimizes the digital precoder and DMA\nweights. A penalty dual decomposition (PDD)-based algorithm is proposed by\nleveraging the fractional programming framework. Numerical results validate the\naccuracy of the derived surrogates and highlight the superiority of the\nproposed algorithms over baseline schemes. It is shown that these algorithms\nare effective across various DMA settings and are particularly well-suited for\nsystem design in fast time-varying channels.", "comment": null, "cate": "cs.IT", "url": "http://arxiv.org/abs/2506.09570v1", "AI": {"title_translation": "基于统计CSI的DMA多用户MISO系统频谱效率最大化", "tldr": "该研究旨在最大化动态超表面天线(DMA)多用户MISO系统在仅有统计CSI的情况下，上行和下行传输的频谱效率，并提出了基于加权MMSE和惩罚对偶分解的新算法，数值结果验证了其优越性。", "motivation": "动态超表面天线(DMAs)在实现低功耗、低硬件成本的大规模天线阵列方面具有潜力，使其成为未来通信系统的有前景技术。本文旨在研究DMA多用户MISO系统的频谱效率。", "method": "本文研究了DMA多用户MISO系统在上行和下行传输中的频谱效率，仅使用统计信道状态信息(CSI)来最大化多用户的遍历和速率。对于上行系统，考虑了两种解码规则：带和不带连续干扰消除(SIC)的最小均方误差(MMSE)。对于这两种解码器，推导了遍历和速率的闭式替代表达式，并提出了基于加权MMSE(WMMSE)的算法来最大化遍历和速率。对于下行系统，推导了遍历和速率的近似表达式，并提出了基于惩罚对偶分解(PDD)的算法，通过分数规划框架联合优化数字预编码器和DMA权重。", "result": "数值结果验证了所推导替代表达式的准确性，并突出了所提出算法优于基线方案的优越性。结果表明，这些算法在各种DMA设置下均有效，并且特别适用于快速时变信道中的系统设计。", "conclusion": "所提出的算法在各种DMA设置下均有效，并且特别适用于快速时变信道中的系统设计，且优于基线方案。", "translation": "动态超表面天线（DMA）有望实现低功耗和低硬件成本的大规模天线阵列，使其成为未来通信系统的一项有前景的技术。本文研究了在仅使用统计信道状态信息（CSI）来最大化多用户遍历和速率的情况下，DMA多用户多输入单输出（MISO）系统在上行和下行传输中的频谱效率（SE）。对于上行系统，我们考虑了两种解码规则：带和不带连续干扰消除（SIC）的最小均方误差（MMSE）。对于这两种解码器，我们推导了闭式替代表达式以替代遍历和速率的原始表达式，并制定了可处理的优化问题用于设计DMA权重。然后，提出了一种基于加权MMSE（WMMSE）的算法来最大化遍历和速率。对于下行系统，我们推导了遍历和速率的近似表达式，并制定了一个混合模拟/数字波束成形优化问题，该问题联合优化数字预编码器和DMA权重。通过利用分数规划框架，提出了一种基于惩罚对偶分解（PDD）的算法。数值结果验证了所推导替代表达式的准确性，并突出了所提出算法优于基线方案的优越性。结果表明，这些算法在各种DMA设置下均有效，并且特别适用于快速时变信道中的系统设计。", "summary": "本文研究了基于动态超表面天线(DMA)的多用户多输入单输出(MISO)系统在仅有统计信道状态信息(CSI)下的频谱效率最大化问题。针对上行链路，提出了基于加权MMSE(WMMSE)的算法，通过推导遍历和速率的闭式替代表达式来优化DMA权重。针对下行链路，提出了一种基于惩罚对偶分解(PDD)的算法，联合优化数字预编码器和DMA权重。数值结果验证了所提出算法的有效性和优越性，尤其是在快速时变信道中表现良好。", "keywords": "动态超表面天线, 频谱效率, MISO, 统计CSI, 遍历和速率", "comments": "该论文的创新点在于针对DMA多用户MISO系统，在仅有统计CSI的条件下，分别对上行和下行链路提出了频谱效率最大化的优化算法。特别是，论文推导了闭式替代表达式和近似表达式，并提出了WMMSE和PDD算法，这对于实际系统中有限CSI反馈的情况具有重要意义。其在快速时变信道中的适用性也增加了其实用价值。"}}
{"id": "2506.09289", "title": "UTBoost: Rigorous Evaluation of Coding Agents on SWE-Bench", "authors": ["Boxi Yu", "Yuxuan Zhu", "Pinjia He", "Daniel Kang"], "summary": "The advent of Large Language Models (LLMs) has spurred the development of\ncoding agents for real-world code generation. As a widely used benchmark for\nevaluating the code generation capabilities of these agents, SWE-Bench uses\nreal-world problems based on GitHub issues and their corresponding pull\nrequests. However, the manually written test cases included in these pull\nrequests are often insufficient, allowing generated patches to pass the tests\nwithout resolving the underlying issue. To address this challenge, we introduce\nUTGenerator, an LLM-driven test case generator that automatically analyzes\ncodebases and dependencies to generate test cases for real-world Python\nprojects. Building on UTGenerator, we propose UTBoost, a comprehensive\nframework for test case augmentation. In our evaluation, we identified 36 task\ninstances with insufficient test cases and uncovered 345 erroneous patches\nincorrectly labeled as passed in the original SWE Bench. These corrections,\nimpacting 40.9% of SWE-Bench Lite and 24.4% of SWE-Bench Verified leaderboard\nentries, yield 18 and 11 ranking changes, respectively.", "comment": null, "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.09289v1", "AI": {"title_translation": "UTBoost：对SWE-Bench上编码智能体的严格评估", "tldr": "针对SWE-Bench基准测试中测试用例不足的问题，本文提出了UTGenerator自动生成测试用例，并在此基础上构建了UTBoost框架，揭示了SWE-Bench中大量错误的通过补丁，显著改变了排行榜排名，从而实现了对编码智能体的更严格评估。", "motivation": "现有的SWE-Bench基准测试中，用于评估代码生成智能体的手动编写测试用例往往不足，导致生成的补丁即使未能解决实际问题也能通过测试。", "method": "引入了UTGenerator，一个由LLM驱动的测试用例生成器，能自动分析代码库和依赖项，为真实世界的Python项目生成测试用例。在此基础上，提出了UTBoost，一个全面的测试用例增强框架。", "result": "识别出36个测试用例不足的任务实例，并揭露了SWE-Bench原始数据中345个被错误标记为“通过”的错误补丁。这些修正影响了SWE-Bench Lite 40.9%和SWE-Bench Verified 24.4%的排行榜条目，分别导致18和11个排名变化。", "conclusion": "通过引入UTBoost框架和增强测试用例，本文揭示了SWE-Bench中现有评估的不足，并提供了更严格、更准确的编码智能体评估方法，从而提升了代码生成能力的评估质量。", "translation": "大型语言模型（LLMs）的出现推动了用于真实世界代码生成的编码智能体的开发。作为评估这些智能体代码生成能力广泛使用的基准测试，SWE-Bench使用基于GitHub问题及其相应拉取请求的真实世界问题。然而，这些拉取请求中包含的手动编写的测试用例通常不足，导致生成的补丁在未解决根本问题的情况下也能通过测试。为了解决这一挑战，我们引入了UTGenerator，一个由LLM驱动的测试用例生成器，它能自动分析代码库和依赖项，为真实世界的Python项目生成测试用例。在UTGenerator的基础上，我们提出了UTBoost，一个全面的测试用例增强框架。在我们的评估中，我们识别出36个测试用例不足的任务实例，并揭露了SWE-Bench原始数据中345个被错误标记为“通过”的错误补丁。这些修正影响了SWE-Bench Lite 40.9%和SWE-Bench Verified 24.4%的排行榜条目，分别导致18和11个排名变化。", "summary": "本文针对SWE-Bench基准测试中现有测试用例不足导致编码智能体评估不准确的问题，提出了LLM驱动的UTGenerator来自动生成测试用例，并在此基础上构建了UTBoost测试用例增强框架。通过应用UTBoost，研究发现了SWE-Bench中大量被错误标记为“通过”的补丁，并显著改变了多个排行榜的排名，证明了其在提高编码智能体评估严格性方面的有效性。", "keywords": "编码智能体, SWE-Bench, 测试用例生成, UTBoost, LLM", "comments": "本文的创新之处在于提出了一个LLM驱动的自动化测试用例生成器UTGenerator，并在此基础上构建了UTBoost框架来增强现有代码生成基准测试的评估能力。其重要性在于揭示了像SWE-Bench这样广泛使用的基准测试中存在的评估漏洞，并通过量化分析展示了这些漏洞对智能体排名的显著影响，对于未来编码智能体的准确评估和发展具有重要指导意义。"}}
{"id": "2506.09153", "title": "Real-Time Confidence Detection through Facial Expressions and Hand Gestures", "authors": ["Tanjil Hasan Sakib", "Samia Jahan Mojumder", "Rajan Das Gupta", "Md Imrul Hasan Showmick", "Md. Yeasin Rahat", "Md. Jakir Hossen"], "summary": "Real-time face orientation recognition is a cutting-edge technology meant to\ntrack and analyze facial movements in virtual environments such as online\ninterviews, remote meetings, and virtual classrooms. As the demand for virtual\ninteractions grows, it becomes increasingly important to measure participant\nengagement, attention, and overall interaction. This research presents a novel\nsolution that leverages the Media Pipe Face Mesh framework to identify facial\nlandmarks and extract geometric data for calculating Euler angles, which\ndetermine head orientation in real time. The system tracks 3D facial landmarks\nand uses this data to compute head movements with a focus on accuracy and\nresponsiveness. By studying Euler angles, the system can identify a user's head\norientation with an accuracy of 90\\%, even at a distance of up to four feet.\nThis capability offers significant enhancements for monitoring user\ninteraction, allowing for more immersive and interactive virtual ex-periences.\nThe proposed method shows its reliability in evaluating participant\nattentiveness during online assessments and meetings. Its application goes\nbeyond engagement analysis, potentially providing a means for improving the\nquality of virtual communication, fostering better understanding between\nparticipants, and ensuring a higher level of interaction in digital spaces.\nThis study offers a basis for future developments in enhancing virtual user\nexperiences by integrating real-time facial tracking technologies, paving the\nway for more adaptive and interactive web-based platform.", "comment": "Accepted in MECON 2025", "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.09153v1", "AI": {"title_translation": "通过面部表情和手势进行实时置信度检测", "tldr": "该研究提出了一种基于Media Pipe Face Mesh框架的实时头部姿态识别系统，通过计算欧拉角来高精度识别用户头部方向，以增强虚拟环境中的用户互动和参与度监控。", "motivation": "随着虚拟交互需求的增长，衡量参与者的投入度、注意力和整体互动变得越来越重要。", "method": "该研究利用Media Pipe Face Mesh框架识别面部地标，提取几何数据来计算欧拉角，从而实时确定头部方向。系统跟踪3D面部地标并计算头部运动，重点关注准确性和响应性。", "result": "该系统通过研究欧拉角，即使在四英尺的距离内也能以90%的准确率识别用户的头部方向。", "conclusion": "这项能力显著增强了用户互动监控，实现了更沉浸和交互式的虚拟体验。所提出的方法在评估在线评估和会议期间参与者的注意力方面显示出其可靠性。其应用超越了参与度分析，可能为改善虚拟通信质量、促进参与者之间更好的理解以及确保数字空间中更高水平的互动提供了一种手段。", "translation": "实时面部姿态识别是一项前沿技术，旨在跟踪和分析在线面试、远程会议和虚拟教室等虚拟环境中的面部运动。随着虚拟交互需求的增长，衡量参与者的投入度、注意力和整体互动变得越来越重要。本研究提出了一种新颖的解决方案，该方案利用Media Pipe Face Mesh框架识别面部地标并提取几何数据以计算欧拉角，从而实时确定头部方向。该系统跟踪3D面部地标并利用这些数据计算头部运动，重点关注准确性和响应性。通过研究欧拉角，该系统即使在四英尺的距离内也能以90%的准确率识别用户的头部方向。这项能力显著增强了用户互动监控，实现了更沉浸和交互式的虚拟体验。所提出的方法在评估在线评估和会议期间参与者的注意力方面显示出其可靠性。其应用超越了参与度分析，可能为改善虚拟通信质量、促进参与者之间更好的理解以及确保数字空间中更高水平的互动提供了一种手段。这项研究为未来通过集成实时面部跟踪技术来增强虚拟用户体验的发展提供了基础，为更具适应性和交互性的网络平台铺平了道路。", "summary": "本研究提出了一种利用Media Pipe Face Mesh框架进行实时头部姿态识别的新方法，旨在解决虚拟环境中用户参与度监控的需求。该系统通过分析面部地标和计算欧拉角，能够以90%的准确率实时识别用户头部方向，即使在一定距离外也能有效工作。这显著增强了虚拟互动体验，并为在线评估和会议中的注意力评估提供了可靠手段，有望提升虚拟通信的质量和互动水平。", "keywords": "实时头部姿态识别, Media Pipe Face Mesh, 欧拉角, 虚拟交互, 参与度监控", "comments": "论文标题提及“置信度检测”，但摘要内容主要集中在“头部姿态识别”和“参与度/注意力监控”，并未详细说明如何从头部姿态推断“置信度”。这可能是标题与摘要内容之间的一个小范围不一致。然而，该研究提出的实时头部姿态识别技术，基于Media Pipe Face Mesh框架，具有较高的准确性和实用性，对于增强虚拟交互体验、评估用户参与度具有重要意义。"}}
{"id": "2506.09749", "title": "Large Language Models for Design Structure Matrix Optimization", "authors": ["Shuo Jiang", "Min Xie", "Jianxi Luo"], "summary": "In complex engineering systems, the interdependencies among components or\ndevelopment activities are often modeled and analyzed using Design Structure\nMatrix (DSM). Reorganizing elements within a DSM to minimize feedback loops and\nenhance modularity or process efficiency constitutes a challenging\ncombinatorial optimization (CO) problem in engineering design and operations.\nAs problem sizes increase and dependency networks become more intricate,\ntraditional optimization methods that solely use mathematical heuristics often\nfail to capture the contextual nuances and struggle to deliver effective\nsolutions. In this study, we explore the potential of Large Language Models\n(LLMs) for helping solve such CO problems by leveraging their capabilities for\nadvanced reasoning and contextual understanding. We propose a novel LLM-based\nframework that integrates network topology with contextual domain knowledge for\niterative optimization of DSM element sequencing - a common CO problem.\nExperiments on various DSM cases show that our method consistently achieves\nfaster convergence and superior solution quality compared to both stochastic\nand deterministic baselines. Notably, we find that incorporating contextual\ndomain knowledge significantly enhances optimization performance regardless of\nthe chosen LLM backbone. These findings highlight the potential of LLMs to\nsolve complex engineering CO problems by combining semantic and mathematical\nreasoning. This approach paves the way towards a new paradigm in LLM-based\nengineering design optimization.", "comment": null, "cate": "cs.CE", "url": "http://arxiv.org/abs/2506.09749v1", "AI": {"title_translation": "大型语言模型在设计结构矩阵优化中的应用", "tldr": "本研究探索了大型语言模型（LLMs）在解决工程设计结构矩阵（DSM）优化这一复杂组合优化问题中的潜力，通过结合网络拓扑和上下文领域知识，实现了更快的收敛和更好的解决方案质量。", "motivation": "在复杂的工程系统中，设计结构矩阵（DSM）的元素重组以最小化反馈回路和增强模块化或过程效率是一个具有挑战性的组合优化（CO）问题。随着问题规模增大和依赖网络变得复杂，传统仅使用数学启发式方法的优化方法往往无法捕捉上下文细微差别，难以提供有效的解决方案。", "method": "本研究提出了一种新颖的基于LLM的框架，该框架将网络拓扑与上下文领域知识相结合，用于迭代优化DSM元素排序。", "result": "在各种DSM案例上的实验表明，我们的方法始终实现更快的收敛和卓越的解决方案质量。值得注意的是，无论选择何种LLM骨干，结合上下文领域知识都显著增强了优化性能。", "conclusion": "研究结果强调了LLMs通过结合语义和数学推理来解决复杂工程组合优化问题的潜力。这种方法为基于LLM的工程设计优化开辟了新范式。", "translation": "在复杂的工程系统中，组件或开发活动之间的相互依赖关系通常使用设计结构矩阵（DSM）进行建模和分析。在DSM中重新组织元素以最小化反馈回路并增强模块化或过程效率是工程设计和操作中一个具有挑战性的组合优化（CO）问题。随着问题规模的增加和依赖网络的日益复杂，传统仅使用数学启发式方法的优化方法往往无法捕捉上下文的细微差别，并且难以提供有效的解决方案。在本研究中，我们探索了大型语言模型（LLMs）通过利用其高级推理和上下文理解能力来帮助解决此类CO问题的潜力。我们提出了一种新颖的基于LLM的框架，该框架将网络拓扑与上下文领域知识相结合，用于迭代优化DSM元素排序——一个常见的CO问题。在各种DSM案例上的实验表明，我们的方法与随机和确定性基线相比，始终实现了更快的收敛和卓越的解决方案质量。值得注意的是，我们发现无论选择何种LLM骨干，结合上下文领域知识都显著增强了优化性能。这些发现突出了LLMs通过结合语义和数学推理来解决复杂工程CO问题的潜力。这种方法为基于LLM的工程设计优化开辟了新范式。", "summary": "本研究探讨了大型语言模型（LLMs）在解决工程设计结构矩阵（DSM）优化这一复杂组合优化问题中的应用。针对传统方法在处理大规模复杂依赖网络时遇到的挑战，论文提出了一种结合网络拓扑和上下文领域知识的LLM-based框架，用于迭代优化DSM元素排序。实验结果表明，该方法在收敛速度和解决方案质量上均优于现有基线，并且强调了结合领域知识对优化性能的关键提升作用。这为LLMs在工程设计优化中开辟了新的研究方向。", "keywords": "大型语言模型, 设计结构矩阵, 组合优化, 工程设计, 上下文理解", "comments": "这项研究的创新之处在于将大型语言模型的先进推理和上下文理解能力应用于工程领域的组合优化问题，特别是设计结构矩阵的优化。它弥补了传统数学启发式方法在处理复杂上下文信息方面的不足，并展示了LLM结合领域知识解决实际工程挑战的巨大潜力，预示着工程设计优化领域的新范式。"}}
{"id": "2506.09866", "title": "ELRUHNA: Elimination Rule-basedHypergraph Alignment", "authors": ["Cameron Ibrahim", "S M Ferdous", "Ilya Safro", "Marco Minutoli", "Mahantesh Halappanavar"], "summary": "Hypergraph alignment is a well-known NP-hard problem with numerous practical\napplications across domains such as bioinformatics, social network analysis,\nand computer vision. Despite its computational complexity, practical and\nscalable solutions are urgently needed to enable pattern discovery and entity\ncorrespondence in high-order relational data. The problem remains understudied\nin contrast to its graph based counterpart. In this paper, we propose ELRUHNA,\nan elimination rule-based framework for unsupervised hypergraph alignment that\noperates on the bipartite representation of hypergraphs. We introduce the\nincidence alignment formulation, a binary quadratic optimization approach that\njointly aligns vertices and hyperedges. ELRUHNA employs a novel similarity\npropagation scheme using local matching and cooling rules, supported by an\ninitialization strategy based on generalized eigenvector centrality for\nincidence matrices. Through extensive experiments on real-world datasets, we\ndemonstrate that ELRUHNA achieves higher alignment accuracy compared to\nstate-of-the-art algorithms, while scaling effectively to large hypergraphs.", "comment": null, "cate": "cs.SI", "url": "http://arxiv.org/abs/2506.09866v1", "AI": {"title_translation": "ELRUHNA：基于消除规则的超图对齐", "tldr": "ELRUHNA是一个基于消除规则的无监督超图对齐框架，在真实数据集上表现出更高的对齐精度和可扩展性。", "motivation": "超图对齐是一个NP-hard问题，在生物信息学、社交网络分析和计算机视觉等领域有广泛应用，但与图对齐相比，其研究不足，迫切需要可扩展的解决方案来发现高阶关系数据中的模式和实体对应关系。", "method": "本文提出了ELRUHNA，一个基于消除规则的无监督超图对齐框架，它在超图的二分表示上操作。该框架引入了关联对齐公式，这是一种共同对齐顶点和超边的二元二次优化方法。ELRUHNA采用了一种新颖的相似性传播方案，使用局部匹配和冷却规则，并由基于关联矩阵广义特征向量中心性的初始化策略支持。", "result": "在真实世界数据集上的广泛实验表明，ELRUHNA比最先进的算法实现了更高的对齐精度，同时有效地扩展到大型超图。", "conclusion": "ELRUHNA为超图对齐问题提供了一个有效且可扩展的解决方案，其性能优于现有技术。", "translation": "超图对齐是一个众所周知的NP-hard问题，在生物信息学、社交网络分析和计算机视觉等领域有大量的实际应用。尽管其计算复杂性很高，但迫切需要实用且可扩展的解决方案，以实现高阶关系数据中的模式发现和实体对应。与基于图的对应方法相比，该问题仍未得到充分研究。在本文中，我们提出了ELRUHNA，一个基于消除规则的无监督超图对齐框架，它在超图的二分表示上操作。我们引入了关联对齐公式，这是一种共同对齐顶点和超边的二元二次优化方法。ELRUHNA采用了一种新颖的相似性传播方案，使用局部匹配和冷却规则，并由基于关联矩阵广义特征向量中心性的初始化策略支持。通过对真实世界数据集的广泛实验，我们证明了ELRUHNA比最先进的算法实现了更高的对齐精度，同时有效地扩展到大型超图。", "summary": "本文提出了ELRUHNA，一个基于消除规则的无监督超图对齐框架，旨在解决超图对齐这一NP-hard问题。ELRUHNA通过在超图的二分表示上操作，并引入关联对齐公式，实现顶点和超边的联合对齐。它采用新颖的相似性传播方案和基于广义特征向量中心性的初始化策略。实验结果表明，ELRUHNA在对齐精度和可扩展性方面均优于现有先进算法。", "keywords": "超图对齐, 无监督学习, 二分表示, 相似性传播, NP-hard问题", "comments": "这篇论文解决了超图对齐这一重要的NP-hard问题，其创新点在于提出了基于消除规则的框架ELRUHNA，并引入了关联对齐公式和新颖的相似性传播机制。该方法在实际应用中表现出更高的精度和良好的可扩展性，对于处理高阶关系数据具有重要意义。"}}
{"id": "2506.09434", "title": "When Is Diversity Rewarded in Cooperative Multi-Agent Learning?", "authors": ["Michael Amir", "Matteo Bettini", "Amanda Prorok"], "summary": "The success of teams in robotics, nature, and society often depends on the\ndivision of labor among diverse specialists; however, a principled explanation\nfor when such diversity surpasses a homogeneous team is still missing. Focusing\non multi-agent task allocation problems, our goal is to study this question\nfrom the perspective of reward design: what kinds of objectives are best suited\nfor heterogeneous teams? We first consider an instantaneous, non-spatial\nsetting where the global reward is built by two generalized aggregation\noperators: an inner operator that maps the $N$ agents' effort allocations on\nindividual tasks to a task score, and an outer operator that merges the $M$\ntask scores into the global team reward. We prove that the curvature of these\noperators determines whether heterogeneity can increase reward, and that for\nbroad reward families this collapses to a simple convexity test. Next, we ask\nwhat incentivizes heterogeneity to emerge when embodied, time-extended agents\nmust learn an effort allocation policy. To study heterogeneity in such\nsettings, we use multi-agent reinforcement learning (MARL) as our computational\nparadigm, and introduce Heterogeneous Environment Design (HED), a\ngradient-based algorithm that optimizes the parameter space of underspecified\nMARL environments to find scenarios where heterogeneity is advantageous.\nExperiments in matrix games and an embodied Multi-Goal-Capture environment show\nthat, despite the difference in settings, HED rediscovers the reward regimes\npredicted by our theory to maximize the advantage of heterogeneity, both\nvalidating HED and connecting our theoretical insights to reward design in\nMARL. Together, these results help us understand when behavioral diversity\ndelivers a measurable benefit.", "comment": null, "cate": "cs.MA", "url": "http://arxiv.org/abs/2506.09434v1", "AI": {"title_translation": "合作多智能体学习中何时奖励多样性？", "tldr": "本文研究了多智能体团队中多样性何时有利，通过奖励函数特性（凸性）来识别，并提出了一个算法（HED）来发现异构性具有优势的MARL环境。", "motivation": "机器、自然界和社会中团队的成功往往依赖于不同专业的分工；然而，对于这种多样性何时能超越同质团队，目前仍缺乏一个有原则的解释。本研究旨在从奖励设计的角度探讨这个问题。", "method": "本研究首先在一个瞬时、非空间的环境中，通过分析构建全局奖励的两个广义聚合算子（内部和外部）的曲率，证明了凸性决定了异构性是否能增加奖励。其次，为了研究具身、时间扩展智能体学习努力分配策略时异构性的出现，引入了异构环境设计（HED），这是一种基于梯度的算法，用于优化未充分指定的MARL环境参数空间，以找到异构性有利的场景。", "result": "理论上，研究证明了聚合算子的曲率（特别是凸性）决定了异构性是否能增加奖励。实验上，在矩阵博弈和多目标捕获环境中，HED重新发现了理论预测的能最大化异构性优势的奖励机制，从而验证了HED并连接了理论见解与MARL中的奖励设计。", "conclusion": "这些结果有助于我们理解行为多样性何时能带来可衡量的收益，并将理论洞察与多智能体强化学习（MARL）中的奖励设计联系起来。", "translation": "机器、自然界和社会中团队的成功往往取决于不同专业分工；然而，对于这种多样性何时能超越同质团队的原理性解释仍然缺失。本文着重于多智能体任务分配问题，旨在从奖励设计的角度研究这个问题：什么样的目标最适合异构团队？我们首先考虑一个瞬时、非空间的环境，其中全局奖励由两个广义聚合算子构建：一个内部算子将N个智能体在个体任务上的努力分配映射到任务得分，一个外部算子将M个任务得分合并到全局团队奖励中。我们证明了这些算子的曲率决定了异构性是否能增加奖励，并且对于广泛的奖励族，这简化为一个简单的凸性测试。接下来，我们探讨当具身、时间扩展的智能体必须学习努力分配策略时，是什么激励了异构性的出现。为了在这样的设置中研究异构性，我们使用多智能体强化学习（MARL）作为计算范式，并引入了异构环境设计（HED），这是一种基于梯度的算法，它优化了未充分指定MARL环境的参数空间，以找到异构性具有优势的场景。在矩阵博弈和具身多目标捕获环境中的实验表明，尽管设置不同，HED重新发现了我们理论预测的能够最大化异构性优势的奖励机制，这既验证了HED，又将我们的理论见解与MARL中的奖励设计联系起来。总的来说，这些结果有助于我们理解行为多样性何时能带来可衡量的收益。", "summary": "本论文探讨了合作多智能体学习中多样性何时有利，重点关注奖励设计。理论上，研究证明了奖励函数中聚合算子的曲率（凸性）决定了异构性是否能增加奖励。此外，论文引入了异构环境设计（HED），一种基于梯度的多智能体强化学习（MARL）算法，通过实验重新发现了这些理论预测的奖励机制，验证了该方法，并将理论见解与实际MARL奖励设计相结合，以实现有利的多样性。", "keywords": "多样性, 多智能体学习, 奖励设计, 异构性, 凸性", "comments": "该论文通过将多样性的益处与奖励函数的数学特性（曲率/凸性）直接关联，提供了一个新颖的视角，为多样性何时有利提供了有原则的解释。HED作为一种计算方法，能够识别奖励异构性的环境，创新性地弥合了理论洞察与实际MARL应用之间的鸿沟。它解决了多智能体系统中的一个基本问题，有望指导更有效的合作AI团队设计。"}}
{"id": "2506.09187", "title": "A Data-driven Predictive Control Architecture for Train Thermal Energy Management", "authors": ["Ahmed Aboudonia", "Johannes Estermann", "Keith Moffat", "Manfred Morari", "John Lygeros"], "summary": "We aim to improve the energy efficiency of train climate control\narchitectures, with a focus on a specific class of regional trains operating\nthroughout Switzerland, especially in Zurich and Geneva. Heating, Ventilation,\nand Air Conditioning (HVAC) systems represent the second largest energy\nconsumer in these trains after traction. The current architecture comprises a\nhigh-level rule-based controller and a low-level tracking controller. To\nimprove train energy efficiency, we propose adding a middle data-driven\npredictive control layer aimed at minimizing HVAC energy consumption while\nmaintaining passenger comfort. The scheme incorporates a multistep prediction\nmodel developed using real-world data collected from a limited number of train\ncoaches. To validate the effectiveness of the proposed architecture, we conduct\nmultiple experiments on a separate set of train coaches; our results suggest\nenergy savings between 10% and 35% with respect to the current architecture.", "comment": null, "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.09187v1", "AI": {"title_translation": "列车热能管理的数据驱动预测控制架构", "tldr": "本文提出一种数据驱动预测控制架构，旨在优化列车HVAC系统能耗，实验结果显示可实现10%至35%的能源节省，同时保持乘客舒适度。", "motivation": "旨在提高列车气候控制架构的能源效率，特别是针对瑞士区域列车，因为HVAC系统是这些列车中仅次于牵引的第二大能源消耗者。", "method": "提出在现有规则控制和跟踪控制架构中增加一个中间数据驱动预测控制层，以最小化HVAC能耗并保持乘客舒适度。该方案利用从真实列车数据中开发的多步预测模型，并在独立的列车车厢上进行了实验验证。", "result": "实验结果表明，与当前架构相比，能源节省在10%到35%之间。", "conclusion": "通过引入数据驱动预测控制层，可以显著提高列车HVAC系统的能源效率，同时不牺牲乘客舒适度。", "translation": "我们旨在提高列车气候控制架构的能源效率，重点关注在瑞士各地（尤其是在苏黎世和日内瓦）运营的特定区域列车。在这些列车中，供暖、通风和空调（HVAC）系统是仅次于牵引的第二大能源消耗者。当前的架构包括一个高级别的基于规则的控制器和一个低级别的跟踪控制器。为了提高列车能源效率，我们建议增加一个中间数据驱动预测控制层，旨在最小化HVAC能耗，同时保持乘客舒适度。该方案包含一个使用从有限数量列车车厢收集的真实世界数据开发的多步预测模型。为了验证所提出架构的有效性，我们在独立的列车车厢组上进行了多次实验；我们的结果表明，相对于当前架构，能源节省在10%到35%之间。", "summary": "本文提出了一种用于列车热能管理的数据驱动预测控制架构，旨在提高HVAC系统的能源效率。通过在现有控制架构中引入一个中间预测控制层，并利用真实数据建立多步预测模型，该方法在保持乘客舒适度的前提下，实现了10%至35%的能源节省。", "keywords": "列车热能管理, 数据驱动控制, 预测控制, HVAC系统, 能源效率", "comments": "这篇论文的创新点在于引入了数据驱动的预测控制层来优化列车HVAC系统的能耗，而非仅仅依赖传统的规则控制。其重要性在于HVAC系统是列车的主要能耗来源之一，该方法能带来显著的节能效果（10%-35%），具有实际应用价值。局限性可能在于模型对数据质量和覆盖范围的依赖，以及在不同列车类型和运行环境下的泛化能力。"}}
{"id": "2506.09197", "title": "Adaptive Bandwidth Sharing for Optimizing QoE of Real-Time Video", "authors": ["Sushi Anna George", "Vinay Joseph"], "summary": "The concept of spectrum or bandwidth sharing has gained significant global\nattention as a means to enhance the efficiency of real-time traffic management\nin wireless networks. Effective bandwidth sharing enables optimal utilization\nof available resources, reducing congestion and improving QoE for\ndelay-sensitive applications such as real-time video transmission. In this\npaper, we propose a novel iterative semi-static bandwidth sharing policy that\nbalances the advantages of both static and dynamic sharing approaches. Our\napproach minimizes the frequency of coordination between network operators\nwhile ensuring efficient resource allocation and meeting the stringent QoE\ndemands of real-time traffic. The proposed policy iteratively optimizes both\nthe spectrum sharing between operators and the resource allocation for\nindividual clients. We establish strong theoretical guarantees for the\noptimality of the proposed policy and prove that it converges to the optimal\nstatic sharing policy irrespective of initial conditions or fluctuations in\ntraffic arrival rates. Additionally, we conduct extensive simulations to\nevaluate the impact of key system parameters - including step size, hyperperiod\nlength, and arrival process dynamics - on the performance of our policy. Our\nresults demonstrate the effectiveness of the proposed approach in achieving\nnear-optimal bandwidth allocation with reduced overhead, making it a practical\nsolution for real-time wireless applications.", "comment": "arXiv admin note: text overlap with arXiv:2401.10681", "cate": "cs.NI", "url": "http://arxiv.org/abs/2506.09197v1", "AI": {"title_translation": "优化实时视频QoE的自适应带宽共享", "tldr": "提出一种迭代半静态带宽共享策略，在减少协调开销的同时，实现实时视频的近最优带宽分配和QoE优化。", "motivation": "无线网络中实时流量管理需要提高效率，有效的带宽共享能优化资源利用，减少拥塞，提升实时视频等时延敏感应用的QoE。", "method": "提出一种新颖的迭代半静态带宽共享策略，平衡静态和动态共享的优点。该策略迭代优化运营商间的频谱共享和客户端的资源分配，旨在最小化网络运营商间的协调频率。", "result": "理论上，该策略具有最优性保证，并能收敛到最优静态共享策略，不受初始条件和流量到达率波动影响。仿真结果表明，该方法能以更低的开销实现近最优带宽分配。", "conclusion": "所提出的自适应带宽共享策略是一种实用的实时无线应用解决方案，能够有效实现近最优带宽分配并降低开销。", "translation": "频谱或带宽共享的概念作为提高无线网络中实时流量管理效率的手段，已获得全球性的广泛关注。有效的带宽共享能够优化可用资源的利用，减少拥塞，并提高实时视频传输等时延敏感应用的QoE。在本文中，我们提出了一种新颖的迭代半静态带宽共享策略，它平衡了静态和动态共享方法的优点。我们的方法在确保高效资源分配和满足实时流量严格的QoE需求的同时，最小化了网络运营商之间的协调频率。所提出的策略迭代地优化了运营商之间的频谱共享和单个客户端的资源分配。我们为所提出策略的最优性建立了强有力的理论保证，并证明它无论初始条件或流量到达率波动如何，都能收敛到最优静态共享策略。此外，我们进行了广泛的仿真，以评估关键系统参数——包括步长、超周期长度和到达过程动态——对我们策略性能的影响。我们的结果证明了所提出方法在实现近最优带宽分配和降低开销方面的有效性，使其成为实时无线应用的实用解决方案。", "summary": "本文提出一种迭代半静态带宽共享策略，旨在优化无线网络中实时视频的QoE。该策略结合了静态和动态共享的优势，通过迭代优化运营商间的频谱共享和客户端资源分配，减少了运营商协调频率，同时保证高效资源分配和满足实时QoE需求。理论分析证明了其最优性和收敛性，仿真结果也验证了其在实现近最优带宽分配和降低开销方面的有效性，为实时无线应用提供了一个实用方案。", "keywords": "自适应带宽共享, 实时视频, QoE优化, 半静态策略, 资源分配", "comments": "这篇论文的创新点在于提出了一个平衡静态和动态共享优点的迭代半静态带宽共享策略，并提供了强有力的理论最优性保证和收敛性证明。其重要性在于为实时视频等时延敏感应用提供了一种实用且高效的带宽分配方案，能够有效提升QoE并降低网络开销。"}}
{"id": "2506.09426", "title": "Exploiting Control-flow Enforcement Technology for Sound and Precise Static Binary Disassembly", "authors": ["Brian Zhao", "Yiwei Yang", "Yusheng Zheng", "Andi Quinn"], "summary": "Rewriting x86_64 binaries-whether for security hardening, dynamic\ninstrumentation, or performance profiling is notoriously difficult due to\nvariable-length instructions, interleaved code and data, and indirect jumps to\narbitrary byte offsets. Existing solutions (e.g., \"superset disassembly\")\nensure soundness but incur significant overhead and produce large rewritten\nbinaries, especially for on-the-fly instrumentation. This paper addresses these\nchallenges by introducing the Time Variance Authority (TVA), which leverages\nIntel's Control-Flow Enforcement Technology (CET). By recognizing endbr64 as\nthe only valid indirect jump target, TVA prunes spurious disassembly paths\nwhile preserving soundness and emulates CET constraints on processors lacking\nnative CET support, effectively mitigating ROP/JOP exploits without new\nhardware. We implement TVA by modernizing the Multiverse rewriter for 64-bit\nLinux. Our evaluation on SPEC CPU2017 and real-world applications shows that\nTVA-guided rewriting achieves up to 1.3x faster instrumentation time. These\nresults underscore TVA's feasibility as a high-performance, uprobes-free\nalternative for robust x86_64 binary analysis and rewriting.", "comment": null, "cate": "cs.AR", "url": "http://arxiv.org/abs/2506.09426v1", "AI": {"title_translation": "利用控制流强制技术实现稳健精确的静态二进制反汇编", "tldr": "本文介绍了TVA，一种利用Intel CET技术，通过识别endbr64来修剪反汇编路径，从而实现对x86_64二进制文件的快速、稳健且精确的静态反汇编和重写，无需新硬件即可有效缓解ROP/JOP攻击。", "motivation": "x86_64二进制文件重写由于变长指令、代码数据交错以及间接跳转等问题而非常困难。现有解决方案（如“超集反汇编”）虽然保证稳健性，但会产生显著的开销和庞大的重写二进制文件，尤其是在即时插桩时。", "method": "本文引入了时间方差权限（Time Variance Authority, TVA），利用Intel的控制流强制技术（CET）。TVA通过识别endbr64作为唯一有效的间接跳转目标，修剪虚假的反汇编路径，同时保持稳健性，并模拟CET约束以在缺乏原生CET支持的处理器上运行，从而无需新硬件即可有效缓解ROP/JOP攻击。通过现代化Multiverse重写器以支持64位Linux来实现了TVA。", "result": "对SPEC CPU2017和实际应用的评估表明，TVA引导的重写可以使插桩时间加快高达1.3倍。", "conclusion": "这些结果强调了TVA作为一种高性能、无uprobes的替代方案，在稳健的x86_64二进制分析和重写方面的可行性。", "translation": "重写x86_64二进制文件——无论是为了安全加固、动态插桩还是性能分析——都因变长指令、代码数据交错以及指向任意字节偏移的间接跳转而臭名昭著地困难。现有解决方案（例如“超集反汇编”）虽然能确保稳健性，但会产生显著的开销并生成庞大的重写二进制文件，尤其是在即时插桩时。本文通过引入时间方差权限（Time Variance Authority, TVA）来解决这些挑战，该权限利用了Intel的控制流强制技术（CET）。通过识别endbr64作为唯一有效的间接跳转目标，TVA在保持稳健性的同时修剪了虚假的反汇编路径，并模拟了在缺乏原生CET支持的处理器上的CET约束，从而无需新硬件即可有效缓解ROP/JOP攻击。我们通过现代化Multiverse重写器以支持64位Linux来实现TVA。我们对SPEC CPU2017和实际应用的评估表明，TVA引导的重写可以使插桩时间加快高达1.3倍。这些结果强调了TVA作为一种高性能、无uprobes的替代方案，在稳健的x86_64二进制分析和重写方面的可行性。", "summary": "本文提出了一种名为时间方差权限（TVA）的新方法，旨在解决x86_64二进制文件重写中存在的挑战，如变长指令和间接跳转。TVA利用Intel的控制流强制技术（CET），通过识别endbr64来精简反汇编路径，同时保持稳健性并有效缓解ROP/JOP攻击，即使在没有原生CET硬件的情况下也能实现。通过对Multiverse重写器进行现代化改造实现TVA，实验结果表明，TVA显著提高了插桩时间，证明了其在x86_64二进制分析和重写方面的高性能和实用性。", "keywords": "二进制反汇编, 控制流强制技术, x86_64, 二进制重写, TVA", "comments": "这篇论文通过利用Intel的现有硬件技术（CET）来解决二进制反汇编和重写中的长期难题，展现了创新性。它巧妙地将硬件辅助的安全特性转化为提高反汇编效率和准确性的手段，同时还能在没有新硬件的情况下提供安全缓解。其重要性在于提供了一个比现有“超集反汇编”更高效、更轻量级的替代方案，对于安全加固、动态插桩和性能分析等领域具有重要意义。"}}
{"id": "2506.09418", "title": "Securing Open RAN: A Survey of Cryptographic Challenges and Emerging Solutions for 5G", "authors": ["Ryan Barker", "Fatemeh Afghah"], "summary": "The advent of Open Radio Access Networks (O-RAN) introduces modularity and\nflexibility into 5G deployments but also surfaces novel security challenges\nacross disaggregated interfaces. This literature review synthesizes recent\nresearch across thirteen academic and industry sources, examining\nvulnerabilities such as cipher bidding-down attacks, partial encryption\nexposure on control/user planes, and performance trade-offs in securing O-RAN\ninterfaces like E2 and O1. The paper surveys key cryptographic tools -- SNOW-V,\nAES-256, and ZUC-256 -- evaluating their throughput, side-channel resilience,\nand adaptability to heterogeneous slices (eMBB, URLLC, mMTC). Emphasis is\nplaced on emerging testbeds and AI-driven controllers that facilitate dynamic\norchestration, anomaly detection, and secure configuration. We conclude by\noutlining future research directions, including hardware offloading,\ncross-layer cipher adaptation, and alignment with 3GPP TS 33.501 and O-RAN\nAlliance security mandates, all of which point toward the need for integrated,\nzero-trust architectures in 6G.", "comment": "4 pages, 1 figure", "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.09418v1", "AI": {"title_translation": "保护开放无线接入网络：5G加密挑战与新兴解决方案综述", "tldr": "本综述探讨了开放无线接入网络（O-RAN）在5G部署中引入的加密挑战，并提出了新兴解决方案，包括对加密工具的评估和AI驱动的控制器。", "motivation": "开放无线接入网络（O-RAN）的出现为5G部署带来了模块化和灵活性，但同时也暴露出跨非聚合接口的新型安全挑战，因此需要对这些挑战及其新兴解决方案进行综述。", "method": "本文通过文献综述的形式，综合了来自十三家学术和行业来源的最新研究，审查了O-RAN接口（如E2和O1）上的漏洞（如密码降级攻击、控制/用户平面上的部分加密暴露以及性能权衡），并调查了关键的加密工具（SNOW-V、AES-256和ZUC-256），评估了它们的吞吐量、侧信道弹性以及对异构切片（eMBB、URLLC、mMTC）的适应性。研究还强调了新兴的测试平台和AI驱动控制器。", "result": "研究发现了O-RAN接口上的多种漏洞，并评估了SNOW-V、AES-256和ZUC-256等加密工具的性能和适应性。此外，新兴的测试平台和AI驱动的控制器被强调为促进动态编排、异常检测和安全配置的有效解决方案。", "conclusion": "论文总结了未来的研究方向，包括硬件卸载、跨层密码适应以及与3GPP TS 33.501和O-RAN联盟安全规范的对齐，并指出在6G中需要集成化的零信任架构。", "translation": "开放无线接入网络（O-RAN）的出现为5G部署带来了模块化和灵活性，但同时也暴露出跨非聚合接口的新型安全挑战。本文献综述综合了来自十三家学术和行业来源的最新研究，审查了O-RAN接口（如E2和O1）上的漏洞，例如密码降级攻击、控制/用户平面上的部分加密暴露以及性能权衡。论文调查了关键的加密工具——SNOW-V、AES-256和ZUC-256——评估了它们的吞吐量、侧信道弹性以及对异构切片（eMBB、URLLC、mMTC）的适应性。重点放在了新兴的测试平台和AI驱动控制器上，它们有助于动态编排、异常检测和安全配置。我们最后概述了未来的研究方向，包括硬件卸载、跨层密码适应以及与3GPP TS 33.501和O-RAN联盟安全规范的对齐，所有这些都指向了6G中对集成化零信任架构的需求。", "summary": "本综述探讨了开放无线接入网络（O-RAN）在5G部署中带来的安全挑战，并审查了相关的加密漏洞和性能权衡。论文评估了SNOW-V、AES-256和ZUC-256等加密工具的适用性，并强调了新兴的测试平台和AI驱动控制器在提升O-RAN安全中的作用。文章最后提出了未来的研究方向，包括硬件卸载和跨层密码适应，并指出6G需要集成化的零信任架构。", "keywords": "Open RAN安全, 5G, 加密挑战, 零信任架构, AI控制器", "comments": "这篇论文通过全面的文献综述，系统地识别了Open RAN在5G部署中的关键加密安全挑战，并对现有及新兴的解决方案进行了评估。其创新之处在于将传统的密码学工具与AI驱动的智能控制器相结合，为Open RAN的安全防护提供了多维度视角。论文对未来研究方向的展望，特别是集成化零信任架构的提出，具有前瞻性，对6G安全架构设计具有指导意义。"}}
{"id": "2506.09160", "title": "Understanding Human-AI Trust in Education", "authors": ["Griffin Pitts", "Sanaz Motamedi"], "summary": "As AI chatbots become increasingly integrated in education, students are\nturning to these systems for guidance, feedback, and information. However, the\nanthropomorphic characteristics of these chatbots create ambiguity regarding\nwhether students develop trust toward them as they would a human peer or\ninstructor, based in interpersonal trust, or as they would any other piece of\ntechnology, based in technology trust. This ambiguity presents theoretical\nchallenges, as interpersonal trust models may inappropriately ascribe human\nintentionality and morality to AI, while technology trust models were developed\nfor non-social technologies, leaving their applicability to anthropomorphic\nsystems unclear. To address this gap, we investigate how human-like and\nsystem-like trusting beliefs comparatively influence students' perceived\nenjoyment, trusting intention, behavioral intention to use, and perceived\nusefulness of an AI chatbot - factors associated with students' engagement and\nlearning outcomes. Through partial least squares structural equation modeling,\nwe found that human-like and system-like trust significantly influenced student\nperceptions, with varied effects. Human-like trust more strongly predicted\ntrusting intention, while system-like trust better predicted behavioral\nintention and perceived usefulness. Both had similar effects on perceived\nenjoyment. Given the partial explanatory power of each type of trust, we\npropose that students develop a distinct form of trust with AI chatbots\n(human-AI trust) that differs from human-human and human-technology models of\ntrust. Our findings highlight the need for new theoretical frameworks specific\nto human-AI trust and offer practical insights for fostering appropriately\ncalibrated trust, which is critical for the effective adoption and pedagogical\nimpact of AI in education.", "comment": null, "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.09160v1", "AI": {"title_translation": "理解教育领域中的人机信任", "tldr": "本研究探讨了学生对教育AI聊天机器人的信任，发现学生对AI的信任是一种独特形式，不同于人际信任和技术信任，并强调了开发专门针对人机信任的理论框架的重要性。", "motivation": "随着AI聊天机器人在教育中的日益普及，学生开始依赖这些系统。然而，AI聊天机器人的拟人化特征导致了一个模糊性：学生对其建立的信任是基于人际信任（如同对人类同伴或教师），还是基于技术信任（如同对其他技术）。这种模糊性带来了理论挑战，因为现有模型可能不适用。为了填补这一空白，研究旨在调查类人信任和类系统信任信仰如何影响学生对AI聊天机器人的感知享受、信任意图、行为使用意图和感知有用性。", "method": "研究通过偏最小二乘结构方程模型（PLS-SEM）来调查类人信任和类系统信任信仰如何影响学生对AI聊天机器人的感知享受、信任意图、行为使用意图和感知有用性。", "result": "研究发现，类人信任和类系统信任显著影响了学生的感知，但效果各异。类人信任更能预测信任意图，而类系统信任更能预测行为意图和感知有用性。两者对感知享受的影响相似。", "conclusion": "鉴于每种信任类型都有部分解释力，研究提出学生与AI聊天机器人之间发展出一种独特形式的信任（人机信任），这种信任不同于人际信任和人机技术信任模型。研究结果强调了需要针对人机信任开发新的理论框架，并为培养适当校准的信任提供了实践见解，这对于AI在教育中的有效采用和教学影响至关重要。", "translation": "随着AI聊天机器人日益融入教育，学生们正转向这些系统寻求指导、反馈和信息。然而，这些聊天机器人的拟人化特征造成了模糊性，即学生对它们建立的信任是基于人际信任（如同对人类同伴或教师），还是基于技术信任（如同对任何其他技术）。这种模糊性带来了理论挑战，因为人际信任模型可能会不恰当地将人类意图和道德归因于AI，而技术信任模型是为非社交技术开发的，其对拟人化系统的适用性尚不明确。为了解决这一空白，我们调查了类人信任和类系统信任信仰如何比较性地影响学生对AI聊天机器人的感知享受、信任意图、行为使用意图和感知有用性——这些因素与学生的参与度和学习成果相关。通过偏最小二乘结构方程模型，我们发现类人信任和类系统信任显著影响了学生的感知，且效果各异。类人信任更强烈地预测了信任意图，而类系统信任更好地预测了行为意图和感知有用性。两者对感知享受的影响相似。鉴于每种信任类型的部分解释力，我们提出学生与AI聊天机器人之间发展出一种独特形式的信任（人机信任），这种信任不同于人际信任和人机技术信任模型。我们的发现强调了需要针对人机信任开发新的理论框架，并为培养适当校准的信任提供了实践见解，这对于AI在教育中的有效采用和教学影响至关重要。", "summary": "本研究探讨了在教育领域中，学生如何信任拟人化AI聊天机器人。针对现有信任模型无法解释AI拟人化特性的问题，研究通过偏最小二乘结构方程模型分析了类人信任和类系统信任对学生感知（享受、信任意图、行为意图、有用性）的影响。结果显示，类人信任更影响信任意图，类系统信任更影响行为意图和有用性。研究提出存在一种独特的人机信任形式，并呼吁建立专门的理论框架以促进AI在教育中的有效应用。", "keywords": "人机信任, AI聊天机器人, 教育技术, 信任模型, 结构方程模型", "comments": "本文创新性地提出了“人机信任”这一概念，填补了现有信任理论在解释人与拟人化AI互动中的空白。其重要性在于，随着AI在教育及其他领域的广泛应用，理解这种新型信任关系对于设计更有效、更值得信赖的AI系统至关重要。研究结果为AI伦理和教育技术设计提供了实践指导。"}}
{"id": "2506.09182", "title": "Towards Full-Scenario Safety Evaluation of Automated Vehicles: A Volume-Based Method", "authors": ["Hang Zhou", "Chengyuan Ma", "Shiyu Shen", "Xiaopeng Li"], "summary": "With the rapid development of automated vehicles (AVs) in recent years,\ncommercially available AVs are increasingly demonstrating high-level automation\ncapabilities. However, most existing AV safety evaluation methods are primarily\ndesigned for simple maneuvers such as car-following and lane-changing. While\nsuitable for basic tests, these methods are insufficient for assessing\nhigh-level automation functions deployed in more complex environments. First,\nthese methods typically use crash rate as the evaluation metric, whose accuracy\nheavily depends on the quality and completeness of naturalistic driving\nenvironment data used to estimate scenario probabilities. Such data is often\ndifficult and expensive to collect. Second, when applied to diverse scenarios,\nthese methods suffer from the curse of dimensionality, making large-scale\nevaluation computationally intractable. To address these challenges, this paper\nproposes a novel framework for full-scenario AV safety evaluation. A unified\nmodel is first introduced to standardize the representation of diverse driving\nscenarios. This modeling approach constrains the dimension of most scenarios to\na regular highway setting with three lanes and six surrounding background\nvehicles, significantly reducing dimensionality. To further avoid the\nlimitations of probability-based method, we propose a volume-based evaluation\nmethod that quantifies the proportion of risky scenarios within the entire\nscenario space. For car-following scenarios, we prove that the set of safe\nscenarios is convex under specific settings, enabling exact volume computation.\nExperimental results validate the effectiveness of the proposed volume-based\nmethod using both AV behavior models from existing literature and six\nproduction AV models calibrated from field-test trajectory data in the Ultra-AV\ndataset. Code and data will be made publicly available upon acceptance of this\npaper.", "comment": "NA", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.09182v1", "AI": {"title_translation": "面向全场景自动驾驶汽车安全评估：一种基于体积的方法", "tldr": "本文提出了一种新的基于体积的框架，用于自动驾驶汽车的全场景安全评估，通过统一建模和避免概率依赖来解决现有方法的局限性。", "motivation": "现有的自动驾驶汽车安全评估方法主要针对简单的驾驶操作，不足以评估复杂环境中的高级自动化功能。这些方法依赖于难以收集和成本高昂的自然驾驶环境数据来估计场景概率，并且在应用于多样化场景时面临维度灾难问题，导致大规模评估的计算困难。", "method": "本文提出了一种新颖的全场景自动驾驶汽车安全评估框架。首先，引入了一个统一模型来标准化多样化驾驶场景的表示，将大多数场景的维度限制在常规高速公路设置（三车道和六辆周围背景车辆）中，从而显著降低了维度。其次，提出了一种基于体积的评估方法，量化整个场景空间中危险场景的比例，以避免基于概率方法的局局限性。对于跟车场景，证明了在特定设置下安全场景的集合是凸的，从而可以精确计算体积。", "result": "实验结果验证了所提出的基于体积的方法的有效性，使用了现有文献中的自动驾驶汽车行为模型以及从Ultra-AV数据集中现场测试轨迹数据校准的六个量产自动驾驶汽车模型。", "conclusion": "本文提出的基于体积的全场景自动驾驶汽车安全评估方法克服了现有方法的局限性，实现了对复杂驾驶场景下自动驾驶汽车的有效安全评估。", "translation": "近年来，随着自动驾驶汽车（AVs）的快速发展，商用自动驾驶汽车正日益展现出高水平的自动化能力。然而，大多数现有的自动驾驶汽车安全评估方法主要针对简单的驾驶操作，如跟车和变道。虽然适用于基本测试，但这些方法不足以评估部署在更复杂环境中的高级自动化功能。首先，这些方法通常使用碰撞率作为评估指标，其准确性严重依赖于用于估计场景概率的自然驾驶环境数据的质量和完整性。此类数据通常难以收集且成本高昂。其次，当应用于多样化场景时，这些方法会受到维度灾难的影响，使得大规模评估在计算上变得难以处理。为了解决这些挑战，本文提出了一种新颖的全场景自动驾驶汽车安全评估框架。首先引入一个统一模型来标准化多样化驾驶场景的表示。这种建模方法将大多数场景的维度限制在常规高速公路设置中，包括三车道和六辆周围背景车辆，从而显著降低了维度。为了进一步避免基于概率方法的局限性，我们提出了一种基于体积的评估方法，该方法量化了整个场景空间中危险场景的比例。对于跟车场景，我们证明了在特定设置下，安全场景的集合是凸的，从而可以精确计算体积。实验结果使用现有文献中的自动驾驶汽车行为模型和从Ultra-AV数据集中现场测试轨迹数据校准的六个量产自动驾驶汽车模型，验证了所提出的基于体积的方法的有效性。代码和数据将在论文被接受后公开发布。", "summary": "本文针对现有自动驾驶汽车安全评估方法在复杂场景下评估能力不足、依赖昂贵数据和面临维度灾难等问题，提出了一种新颖的全场景评估框架。该框架通过引入统一模型来标准化场景表示并降低维度，并提出一种基于体积的评估方法来量化危险场景比例，避免了对概率的依赖。实验结果验证了该方法的有效性，为自动驾驶汽车的全场景安全评估提供了新的途径。", "keywords": "自动驾驶汽车, 安全评估, 全场景, 基于体积, 维度降低", "comments": "本文提出的基于体积的评估方法是其主要创新点，它有效避免了传统评估方法对自然驾驶数据质量和完整性的高度依赖，并解决了维度灾难问题，这对于自动驾驶汽车在复杂真实世界场景中的大规模安全评估具有重要意义。统一建模和维度降低策略也显著提高了评估的可行性。"}}
{"id": "2506.09068", "title": "BG-HOP: A Bimanual Generative Hand-Object Prior", "authors": ["Sriram Krishna", "Sravan Chittupalli", "Sungjae Park"], "summary": "In this work, we present BG-HOP, a generative prior that seeks to model\nbimanual hand-object interactions in 3D. We address the challenge of limited\nbimanual interaction data by extending existing single-hand generative priors,\ndemonstrating preliminary results in capturing the joint distribution of hands\nand objects. Our experiments showcase the model's capability to generate\nbimanual interactions and synthesize grasps for given objects. We make code and\nmodels publicly available.", "comment": "Presented at Agents in Interaction, from Humans to Robots, CVPR 2025", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09068v1", "AI": {"title_translation": "BG-HOP：一种双手动态手物先验模型", "tldr": "BG-HOP是一个生成式先验模型，用于建模3D双手动态手物交互，通过扩展现有单手先验来解决数据限制问题，并能生成双手动交互和合成抓取。", "motivation": "该研究旨在解决3D双手动态手物交互数据有限的挑战，并希望建模双手动态手物交互的联合分布。", "method": "研究团队提出了BG-HOP，一个生成式先验模型，通过扩展现有的单手生成式先验来建模3D双手动态手物交互。", "result": "实验结果表明，该模型能够生成双手动态交互并为给定物体合成抓取。", "conclusion": "BG-HOP模型能够初步捕获手和物体的联合分布，证明了其在建模3D双手动态手物交互方面的潜力。", "translation": "在这项工作中，我们提出了BG-HOP，一个旨在建模3D双手动态手物交互的生成式先验模型。我们通过扩展现有的单手生成式先验来解决双手动态交互数据有限的挑战，并展示了捕获手和物体联合分布的初步结果。我们的实验展示了模型生成双手动态交互和为给定物体合成抓取的能力。我们公开发布了代码和模型。", "summary": "本研究介绍了BG-HOP，一个用于建模3D双手动态手物交互的生成式先验模型。为克服双手动态交互数据稀缺的问题，该模型通过扩展现有单手生成式先验来实现。初步结果表明，BG-HOP能够捕获手和物体的联合分布，并成功生成双手动态交互和合成物体抓取。", "keywords": "双手动态交互, 生成式先验, 手物建模, 3D交互, 抓取合成", "comments": "BG-HOP的创新之处在于通过扩展单手先验来解决双手动态手物交互数据稀缺的挑战，这为3D手物交互建模提供了一个新的视角。其能够生成双手动态交互和合成抓取的能力，对于虚拟现实、机器人操作等领域具有重要意义。公开发布代码和模型也促进了该领域的研究进展。"}}
{"id": "2506.09344", "title": "Ming-Omni: A Unified Multimodal Model for Perception and Generation", "authors": ["Inclusion AI", "Biao Gong", "Cheng Zou", "Chuanyang Zheng", "Chunluan Zhou", "Canxiang Yan", "Chunxiang Jin", "Chunjie Shen", "Dandan Zheng", "Fudong Wang", "Furong Xu", "GuangMing Yao", "Jun Zhou", "Jingdong Chen", "Jianxin Sun", "Jiajia Liu", "Jianjiang Zhu", "Jun Peng", "Kaixiang Ji", "Kaiyou Song", "Kaimeng Ren", "Libin Wang", "Lixiang Ru", "Lele Xie", "Longhua Tan", "Lyuxin Xue", "Lan Wang", "Mochen Bai", "Ning Gao", "Pei Chen", "Qingpei Guo", "Qinglong Zhang", "Qiang Xu", "Rui Liu", "Ruijie Xiong", "Sirui Gao", "Tinghao Liu", "Taisong Li", "Weilong Chai", "Xinyu Xiao", "Xiaomei Wang", "Xiaoxue Chen", "Xiao Lu", "Xiaoyu Li", "Xingning Dong", "Xuzheng Yu", "Yi Yuan", "Yuting Gao", "Yunxiao Sun", "Yipeng Chen", "Yifei Wu", "Yongjie Lyu", "Ziping Ma", "Zipeng Feng", "Zhijiang Fang", "Zhihao Qiu", "Ziyuan Huang", "Zhengyu He"], "summary": "We propose Ming-Omni, a unified multimodal model capable of processing\nimages, text, audio, and video, while demonstrating strong proficiency in both\nspeech and image generation. Ming-Omni employs dedicated encoders to extract\ntokens from different modalities, which are then processed by Ling, an MoE\narchitecture equipped with newly proposed modality-specific routers. This\ndesign enables a single model to efficiently process and fuse multimodal inputs\nwithin a unified framework, thereby facilitating diverse tasks without\nrequiring separate models, task-specific fine-tuning, or structural redesign.\nImportantly, Ming-Omni extends beyond conventional multimodal models by\nsupporting audio and image generation. This is achieved through the integration\nof an advanced audio decoder for natural-sounding speech and Ming-Lite-Uni for\nhigh-quality image generation, which also allow the model to engage in\ncontext-aware chatting, perform text-to-speech conversion, and conduct\nversatile image editing. Our experimental results showcase Ming-Omni offers a\npowerful solution for unified perception and generation across all modalities.\nNotably, our proposed Ming-Omni is the first open-source model we are aware of\nto match GPT-4o in modality support, and we release all code and model weights\nto encourage further research and development in the community.", "comment": "18 pages,8 figures", "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.09344v1", "AI": {"title_translation": "Ming-Omni: 一个用于感知与生成的统一多模态模型", "tldr": "Ming-Omni是一个统一的多模态模型，能够处理图像、文本、音频和视频，并擅长语音和图像生成。它采用专用编码器和MoE架构，实现了高效的统一感知与生成，且是首个与GPT-4o模态支持匹配的开源模型。", "motivation": "现有模型通常需要单独的模型、任务特定的微调或结构重新设计来处理不同模态和任务。Ming-Omni旨在提供一个统一的框架，高效处理和融合多模态输入，并支持感知和生成任务，特别是扩展到音频和图像生成，以解决这些局限性。", "method": "Ming-Omni采用专用编码器从不同模态（图像、文本、音频、视频）中提取tokens。这些tokens随后由Ling处理，Ling是一个配备新提出的模态特定路由器的MoE（Mixture-of-Experts）架构。该模型还集成了先进的音频解码器用于自然语音生成，以及Ming-Lite-Uni用于高质量图像生成，从而支持上下文感知聊天、文本到语音转换和多功能图像编辑。", "result": "实验结果表明，Ming-Omni为所有模态的统一感知和生成提供了强大的解决方案。值得注意的是，它是已知首个在模态支持方面与GPT-4o匹配的开源模型。", "conclusion": "Ming-Omni成功地构建了一个统一的多模态模型，能够高效地处理和生成多种模态数据，无需单独模型或任务特定微调，并在模态支持方面达到了与GPT-4o相当的水平，且作为开源项目发布以促进社区研究。", "translation": "我们提出了Ming-Omni，一个统一的多模态模型，能够处理图像、文本、音频和视频，同时在语音和图像生成方面表现出强大的能力。Ming-Omni采用专用编码器从不同模态中提取tokens，然后由Ling处理，Ling是一个配备新提出的模态特定路由器的MoE架构。这种设计使得单个模型能够在统一框架内高效处理和融合多模态输入，从而促进各种任务而无需单独的模型、任务特定的微调或结构重新设计。重要的是，Ming-Omni通过支持音频和图像生成，超越了传统的多模态模型。这通过集成先进的音频解码器用于自然语音和Ming-Lite-Uni用于高质量图像生成来实现，这也使模型能够进行上下文感知聊天、执行文本到语音转换以及进行多功能图像编辑。我们的实验结果表明，Ming-Omni为所有模态的统一感知和生成提供了强大的解决方案。值得注意的是，我们提出的Ming-Omni是已知首个在模态支持方面与GPT-4o匹配的开源模型，我们发布了所有代码和模型权重，以鼓励社区的进一步研究和开发。", "summary": "Ming-Omni是一个创新的统一多模态模型，能够处理图像、文本、音频和视频，并支持语音和图像生成。它通过专用编码器和基于MoE的Ling架构（包含模态特定路由器）高效地融合多模态输入。该模型无需任务特定微调，即可实现统一感知和生成，并通过集成先进的解码器扩展了音频和图像生成能力。实验证明其在所有模态上的强大性能，并且是首个与GPT-4o在模态支持上匹配的开源模型。", "keywords": "统一多模态模型, 感知与生成, MoE架构, 开源模型, 多模态融合", "comments": "Ming-Omni的创新之处在于其统一的多模态处理框架，通过MoE架构和模态特定路由器实现了高效的多模态融合。其重要性在于首次实现并开源了一个在模态支持上能与GPT-4o比肩的模型，这对于推动多模态AI研究具有重大意义。它通过单一模型处理多种任务，并支持生成能力，展现了未来AI模型的发展方向。"}}
{"id": "2506.09084", "title": "Enhanced Whole Page Optimization via Mixed-Grained Reward Mechanism-Adapted Language Models", "authors": ["Xinyuan Wang", "Liang Wu", "Yanjie Fu"], "summary": "Optimizing the presentation of search and recommendation results is crucial\nto enhancing user experience and engagement. Whole Page Optimization (WPO)\nplays a pivotal role in this process, as it directly influences how information\nis surfaced to users. While Pre-trained Large Language Models (LLMs) have\ndemonstrated remarkable capabilities in generating coherent and contextually\nrelevant content, fine-tuning these models for complex tasks like WPO presents\nchallenges. Specifically, the need for extensive human-annotated data to\nmitigate issues such as hallucinations and model instability can be\nprohibitively expensive, especially in large-scale systems that interact with\nmillions of items daily. In this work, we address the challenge of fine-tuning\nLLMs for WPO by using user feedback as the supervision. Unlike manually labeled\ndatasets, user feedback is inherently noisy and less precise. To overcome this,\nwe propose a reward-based fine-tuning approach, PageLLM, which employs a\nmixed-grained reward mechanism that combines page-level and item-level rewards.\nThe page-level reward evaluates the overall quality and coherence, while the\nitem-level reward focuses on the accuracy and relevance of key recommendations.\nThis dual-reward structure ensures that both the holistic presentation and the\ncritical individual components are optimized. We validate PageLLM on both\npublic and industrial datasets. PageLLM outperforms baselines and achieves a\n0.44\\% GMV increase in an online A/B test with over 10 million users,\ndemonstrating its real-world impact.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09084v1", "AI": {"title_translation": "通过混合粒度奖励机制适应语言模型增强全页面优化", "tldr": "提出PageLLM，一种利用混合粒度用户反馈奖励机制微调LLM进行全页面优化，显著提升用户体验和GMV。", "motivation": "优化搜索和推荐结果的呈现对提升用户体验至关重要。全页面优化（WPO）在此过程中扮演关键角色。然而，使用预训练LLM进行WPO微调面临挑战，尤其是需要大量人工标注数据来解决幻觉和模型不稳定性问题，这在大型系统中成本高昂。", "method": "针对LLM在WPO微调中对人工标注数据的依赖问题，本研究提出PageLLM，一种基于奖励的微调方法。PageLLM采用混合粒度奖励机制，结合页面级奖励（评估整体质量和连贯性）和项目级奖励（关注关键推荐的准确性和相关性），以克服用户反馈的噪声和不精确性。", "result": "PageLLM在公共和工业数据集上均表现优异，超越了基线模型。在针对超过1000万用户的在线A/B测试中，PageLLM实现了0.44%的GMV增长，证明了其在实际应用中的有效性。", "conclusion": "PageLLM通过创新的混合粒度奖励机制，成功利用噪声用户反馈有效微调LLM进行全页面优化，显著提升了用户体验和商业指标，为大规模推荐系统中的LLM应用提供了可行且高效的解决方案。", "translation": "优化搜索和推荐结果的呈现对于提升用户体验和参与度至关重要。全页面优化（WPO）在此过程中发挥着关键作用，因为它直接影响信息如何呈现给用户。尽管预训练大型语言模型（LLMs）在生成连贯且上下文相关的内容方面表现出卓越的能力，但针对WPO等复杂任务对这些模型进行微调仍面临挑战。具体而言，为了减轻幻觉和模型不稳定性等问题，需要大量人工标注数据，这在每天与数百万项目交互的大规模系统中可能成本过高。在这项工作中，我们通过使用用户反馈作为监督来解决微调LLM进行WPO的挑战。与手动标注的数据集不同，用户反馈本身是嘈杂且不精确的。为了克服这一点，我们提出了一种基于奖励的微调方法PageLLM，它采用混合粒度奖励机制，结合了页面级和项目级奖励。页面级奖励评估整体质量和连贯性，而项目级奖励则侧重于关键推荐的准确性和相关性。这种双重奖励结构确保了整体呈现和关键个体组件都得到优化。我们在公共和工业数据集上验证了PageLLM。PageLLM优于基线模型，并在超过1000万用户的在线A/B测试中实现了0.44%的GMV增长，展示了其在现实世界中的影响力。", "summary": "本研究提出PageLLM，一种新颖的基于奖励的微调方法，旨在解决大型语言模型（LLMs）在全页面优化（WPO）中对昂贵人工标注数据的依赖问题。PageLLM利用嘈杂的用户反馈，通过创新的混合粒度奖励机制（结合页面级和项目级奖励）来优化搜索和推荐结果的整体呈现和关键组件。实验结果表明，PageLLM在公共和工业数据集上均超越基线，并在大规模在线A/B测试中实现了显著的GMV增长，证明了其在提升用户体验和商业效益方面的有效性。", "keywords": "全页面优化, 语言模型, 奖励机制, 用户反馈, 推荐系统", "comments": "该论文的创新点在于提出了一种无需大量人工标注数据，而是利用噪声用户反馈来微调LLM进行全页面优化的方法。混合粒度奖励机制是其核心，有效平衡了整体页面质量和个体推荐精度。其在真实世界大规模系统中的GMV增长证明了该方法的实际应用价值和商业影响力，为LLM在推荐系统中的落地提供了新的思路。"}}
{"id": "2506.09651", "title": "On the Ding and Helleseth's 8th open problem about optimal ternary cyclic codes", "authors": ["Dong He", "Peipei Zheng", "Qunying Liao"], "summary": "The cyclic code is a subclass of linear codes and has applications in\nconsumer electronics, data storage systems and communication systems due to the\nefficient encoding and decoding algorithms. In 2013, Ding, et al. presented\nnine open problems about optimal ternary cyclic codes. Till now, the 1st, 2nd,\n6th and 7th problems were completely solved, the 3rd, 8th and 9th problems were\nincompletely solved. In this manuscript, we focus on the 8th problem. By\ndetermining the root set of some special polynomials over finite fields, we\npresent a counterexample and a sufficient condition for the ternary cyclic code\n$\\mathcal{C}_{(1, e)}$ optimal. Furthermore, basing on the properties of finite\nfields, we construct a class of optimal ternary cyclic codes with respect to\nthe Sphere Packing Bound, and show that these codes are not equivalent to any\nknown codes.", "comment": "17 pages", "cate": "cs.IT", "url": "http://arxiv.org/abs/2506.09651v1", "AI": {"title_translation": "关于Ding和Helleseth关于最优三元循环码的第八个开放问题", "tldr": "本文解决了Ding和Helleseth关于最优三元循环码的第八个开放问题，通过确定特殊多项式的根集，给出了一个反例和充分条件，并构建了一类新的最优三元循环码。", "motivation": "循环码在消费电子、数据存储和通信系统中应用广泛。Ding等人于2013年提出了九个关于最优三元循环码的开放问题，其中第八个问题尚未完全解决。本文旨在解决这一问题。", "method": "通过确定有限域上一些特殊多项式的根集来解决问题。此外，还利用有限域的性质来构建新的最优三元循环码。", "result": "提出了一个反例和最优三元循环码$\\\\mathcal{C}_{(1, e)}$的充分条件。构建了一类新的、相对于球堆积界最优的三元循环码，并且证明这些码不与任何已知码等价。", "conclusion": "本文通过确定特殊多项式的根集和利用有限域性质，成功地部分解决了Ding和Helleseth的第八个开放问题，并发现了一类新的最优三元循环码，对循环码理论和应用做出了贡献。", "translation": "循环码是线性码的一个子类，由于其高效的编码和解码算法，在消费电子、数据存储系统和通信系统中都有应用。2013年，Ding等人提出了九个关于最优三元循环码的开放问题。到目前为止，第1、2、6、7个问题已完全解决，第3、8、9个问题尚未完全解决。在本手稿中，我们重点关注第8个问题。通过确定有限域上一些特殊多项式的根集，我们提出了一个反例和三元循环码$\\\\mathcal{C}_{(1, e)}$最优的充分条件。此外，基于有限域的性质，我们构建了一类相对于球堆积界最优的三元循环码，并表明这些码不与任何已知码等价。", "summary": "本文致力于解决Ding和Helleseth提出的关于最优三元循环码的第八个开放问题。研究人员通过分析有限域上特殊多项式的根集，提供了一个反例和关于三元循环码$\\\\mathcal{C}_{(1, e)}$最优的充分条件。此外，基于有限域的特性，研究还构造了一类新的、相对于球堆积界最优的三元循环码，并证明了这些新构造的码与现有已知码不等价。", "keywords": "循环码, 最优三元循环码, 开放问题, 有限域, 球堆积界", "comments": "这项研究通过解决一个著名的开放问题，为循环码理论做出了重要贡献。通过提供反例和充分条件，以及构造新的最优码，它不仅推进了理论知识，也可能为未来的编码实践提供新的选择。特别是证明新构造的码与已知码不等价，突出了其原创性和重要性。"}}
{"id": "2506.09370", "title": "Assessing the Impact of Refactoring Energy-Inefficient Code Patterns on Software Sustainability: An Industry Case Study", "authors": ["Rohit Mehra", "Priyavanshi Pathania", "Vibhu Saujanya Sharma", "Vikrant Kaulgud", "Sanjay Podder", "Adam P. Burden"], "summary": "Advances in technologies like artificial intelligence and metaverse have led\nto a proliferation of software systems in business and everyday life. With this\nwidespread penetration, the carbon emissions of software are rapidly growing as\nwell, thereby negatively impacting the long-term sustainability of our\nenvironment. Hence, optimizing software from a sustainability standpoint\nbecomes more crucial than ever. We believe that the adoption of automated tools\nthat can identify energy-inefficient patterns in the code and guide appropriate\nrefactoring can significantly assist in this optimization. In this extended\nabstract, we present an industry case study that evaluates the sustainability\nimpact of refactoring energy-inefficient code patterns identified by automated\nsoftware sustainability assessment tools for a large application. Preliminary\nresults highlight a positive impact on the application's sustainability\npost-refactoring, leading to a 29% decrease in per-user per-month energy\nconsumption.", "comment": "3 pages. To be published in the proceedings of 38th IEEE/ACM\n  International Conference on Automated Software Engineering (ASE 2023),\n  Kirchberg, Luxembourg", "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.09370v1", "AI": {"title_translation": "评估重构低能效代码模式对软件可持续性的影响：一项行业案例研究", "tldr": "一项行业案例研究表明，重构低能效代码模式能显著降低软件能耗，提升可持续性。", "motivation": "随着人工智能和元宇宙等技术的发展，软件系统日益普及，导致软件碳排放快速增长，对环境的长期可持续性产生负面影响。因此，从可持续性的角度优化软件变得比以往任何时候都更加关键。", "method": "本文通过一项行业案例研究，评估了对大型应用程序中使用自动化软件可持续性评估工具识别出的低能效代码模式进行重构后，其对可持续性的影响。", "result": "初步结果显示，重构对应用程序的可持续性产生了积极影响，实现了每用户每月能耗降低29%。", "conclusion": "抽象中未明确给出最终结论，仅提及“初步结果”。可以推断重构低能效代码模式对软件可持续性有积极作用。", "translation": "人工智能和元宇宙等技术的进步导致软件系统在商业和日常生活中激增。随着这种广泛的普及，软件的碳排放也在迅速增长，从而对我们环境的长期可持续性产生负面影响。因此，从可持续性的角度优化软件变得比以往任何时候都更加关键。我们相信，采用能够识别代码中低能效模式并指导适当重构的自动化工具可以极大地帮助实现这种优化。在这份扩展摘要中，我们 प्रस्तुत 了一项行业案例研究，该研究评估了对大型应用程序中通过自动化软件可持续性评估工具识别出的低能效代码模式进行重构后，其对可持续性的影响。初步结果突出表明，重构后应用程序的可持续性产生了积极影响，导致每用户每月能耗降低了29%。", "summary": "本研究通过一项行业案例研究，探讨了重构低能效代码模式对软件可持续性的影响。鉴于软件碳排放日益增长对环境造成的负面影响，研究旨在评估使用自动化工具识别并重构大型应用程序中的低能效代码模式后，其对可持续性的改善效果。初步结果表明，此类重构对软件可持续性有积极作用，显著降低了每用户每月能耗29%。", "keywords": "软件可持续性, 代码重构, 能源效率, 碳排放, 案例研究", "comments": "这项研究的创新之处在于其专注于软件能耗优化，并将其与环境可持续性联系起来，这在当前碳排放日益受到关注的背景下显得尤为重要。通过实际的行业案例研究，为自动化工具识别和重构低能效代码模式提供了初步的实证支持。其局限性在于结果仅为“初步”，可能需要更深入、更广泛的研究来验证其普适性和长期效果。"}}
{"id": "2506.09212", "title": "Show Me Your Best Side: Characteristics of User-Preferred Perspectives for 3D Graph Drawings", "authors": ["Lucas Joos", "Gavin J. Mooney", "Maximilian T. Fischer", "Daniel A. Keim", "Falk Schreiber", "Helen C. Purchase", "Karsten Klein"], "summary": "The visual analysis of graphs in 3D has become increasingly popular,\naccelerated by the rise of immersive technology, such as augmented and virtual\nreality. Unlike 2D drawings, 3D graph layouts are highly viewpoint-dependent,\nmaking perspective selection critical for revealing structural and relational\npatterns. Despite its importance, there is limited empirical evidence guiding\nwhat constitutes an effective or preferred viewpoint from the user's\nperspective. In this paper, we present a systematic investigation into\nuser-preferred viewpoints in 3D graph visualisations. We conducted a controlled\nstudy with 23 participants in a virtual reality environment, where users\nselected their most and least preferred viewpoints for 36 different graphs\nvarying in size and layout. From this data, enriched by qualitative feedback,\nwe distil common strategies underlying viewpoint choice. We further analyse the\nalignment of user preferences with classical 2D aesthetic criteria (e.g.,\nCrossings), 3D-specific measures (e.g., Node-Node Occlusion), and introduce a\nnovel measure capturing the perceivability of a graph's principal axes\n(Isometric Viewpoint Deviation). Our data-driven analysis indicates that\nStress, Crossings, Gabriel Ratio, Edge-Node Overlap, and Isometric Viewpoint\nDeviation are key indicators of viewpoint preference. Beyond our findings, we\ncontribute a publicly available dataset consisting of the graphs and computed\naesthetic measures, supporting further research and the development of\nviewpoint evaluation measures for 3D graph drawing.", "comment": null, "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.09212v1", "AI": {"title_translation": "给我看你最好的一面：3D图表绘制中用户偏好视角的特征", "tldr": "一项关于3D图表可视化中用户偏好视角的实证研究，确定了关键的美学指标并提供了一个公开数据集。", "motivation": "尽管3D图表可视化日益流行且视角选择至关重要，但关于用户角度下有效或偏好视角的实证证据有限。", "method": "本研究对3D图表可视化中的用户偏好视角进行了系统调查。通过在虚拟现实环境中对23名参与者进行受控研究，收集了他们对36个不同大小和布局图表最偏好和最不偏好视角的选择。研究分析了视角选择的常见策略，并将用户偏好与经典2D美学标准、3D特定度量以及新引入的衡量图表主轴可感知性的“等距视角偏差”进行对齐分析。", "result": "数据驱动分析表明，应力（Stress）、交叉（Crossings）、Gabriel比率（Gabriel Ratio）、边节点重叠（Edge-Node Overlap）和等距视角偏差（Isometric Viewpoint Deviation）是视角偏好的关键指标。", "conclusion": "本研究确定了3D图表绘制中用户偏好视角的关键指标，并提供了一个公开可用的图表和计算美学度量数据集，以支持进一步的研究和视角评估度量的开发。", "translation": "3D图表的视觉分析已变得越来越受欢迎，沉浸式技术如增强现实和虚拟现实的兴起加速了这一趋势。与2D绘图不同，3D图表布局高度依赖于视角，这使得视角选择对于揭示结构和关系模式至关重要。尽管其重要性，但关于用户视角下什么是有效或偏好的视角的经验证据有限。在本文中，我们对3D图表可视化中的用户偏好视角进行了系统调查。我们在虚拟现实环境中对23名参与者进行了一项受控研究，用户为36个不同大小和布局的图表选择了他们最偏好和最不偏好的视角。从这些数据中，通过定性反馈的丰富，我们提炼出视角选择背后的常见策略。我们进一步分析了用户偏好与经典2D美学标准（例如，交叉）、3D特定度量（例如，节点-节点遮挡）的一致性，并引入了一种新颖的度量方法，捕捉图表主轴的可感知性（等距视角偏差）。我们的数据驱动分析表明，应力、交叉、Gabriel比率、边节点重叠和等距视角偏差是视角偏好的关键指标。除了我们的发现之外，我们还贡献了一个公开可用的数据集，其中包含图表和计算出的美学度量，支持进一步的研究和3D图表绘制中视角评估度量的开发。", "summary": "本研究系统调查了3D图表可视化中用户偏好的视角。通过在虚拟现实环境中进行一项有23名参与者参与的受控研究，收集了用户对36个不同图表的视角偏好数据。分析结果表明，应力、交叉、Gabriel比率、边节点重叠和新引入的等距视角偏差是影响用户视角偏好的关键指标。此外，本研究还贡献了一个公开数据集，以促进3D图表绘制领域未来研究和视角评估方法的发展。", "keywords": "3D图表绘制, 视角偏好, 虚拟现实, 美学标准, 用户研究", "comments": "这项研究通过实证方法填补了3D图表可视化中用户偏好视角理解的空白，其创新之处在于引入了“等距视角偏差”这一新颖度量，并提供了宝贵的公开数据集，这对于推动3D图表绘制和沉浸式技术领域的发展具有重要意义。研究结果对于指导未来3D图表设计和自动视角选择具有实践价值。"}}
{"id": "2506.09755", "title": "Intelligent Design 4.0: Paradigm Evolution Toward the Agentic AI Era", "authors": ["Shuo Jiang", "Min Xie", "Frank Youhua Chen", "Jian Ma", "Jianxi Luo"], "summary": "Research and practice in Intelligent Design (ID) have significantly enhanced\nengineering innovation, efficiency, quality, and productivity over recent\ndecades, fundamentally reshaping how engineering designers think, behave, and\ninteract with design processes. The recent emergence of Foundation Models\n(FMs), particularly Large Language Models (LLMs), has demonstrated general\nknowledge-based reasoning capabilities, and open new paths and avenues for\nfurther transformation in engineering design. In this context, this paper\nintroduces Intelligent Design 4.0 (ID 4.0) as an emerging paradigm empowered by\nagentic AI systems. We review the historical evolution of ID across four\ndistinct stages: rule-based expert systems, task-specific machine learning\nmodels, large-scale foundation AI models, and the recent emerging paradigm of\nmulti-agent collaboration. We propose a conceptual framework for ID 4.0 and\ndiscuss its potential to support end-to-end automation of engineering design\nprocesses through coordinated, autonomous multi-agent-based systems.\nFurthermore, we discuss future perspectives to enhance and fully realize ID\n4.0's potential, including more complex design scenarios, more practical design\nimplementations, novel agent coordination mechanisms, and autonomous design\ngoal-setting with better human value alignment. In sum, these insights lay a\nfoundation for advancing Intelligent Design toward greater adaptivity,\nautonomy, and effectiveness in addressing increasingly complex design\nchallenges.", "comment": null, "cate": "cs.CE", "url": "http://arxiv.org/abs/2506.09755v1", "AI": {"title_translation": "智能设计4.0：迈向代理AI时代的范式演进", "tldr": "论文提出了智能设计4.0（ID 4.0），这是一个由代理AI系统驱动的新范式，旨在通过多智能体协作实现工程设计的端到端自动化。", "motivation": "智能设计（ID）已显著提升工程创新和效率。随着基础模型（特别是大型语言模型）的出现，工程设计面临进一步转型的机会。本文旨在介绍和发展适应这一新时代的智能设计范式。", "method": "论文回顾了智能设计（ID）的四个演进阶段：基于规则的专家系统、特定任务机器学习模型、大规模基础AI模型和多智能体协作。在此基础上，提出了一个ID 4.0的概念框架，并讨论了其通过协调的、自主的多智能体系统支持工程设计流程端到端自动化的潜力。", "result": "论文引入了智能设计4.0（ID 4.0）作为一种由代理AI系统赋能的新兴范式，并提出了其概念框架。", "conclusion": "这些见解为推动智能设计在应对日益复杂的设计挑战方面实现更大的适应性、自主性和有效性奠定了基础。", "translation": "智能设计（ID）的研究与实践在近几十年来显著提升了工程创新、效率、质量和生产力，从根本上重塑了工程设计师的思维、行为以及与设计过程的互动方式。最近基础模型（特别是大型语言模型）的出现，展现了基于通用知识的推理能力，为工程设计的进一步转型开辟了新的途径和方向。在此背景下，本文引入了智能设计4.0（ID 4.0）作为一种由代理AI系统赋能的新兴范式。我们回顾了ID在四个不同阶段的历史演变：基于规则的专家系统、特定任务机器学习模型、大规模基础AI模型以及最近新兴的多智能体协作范式。我们提出了一个ID 4.0的概念框架，并讨论了其通过协调的、自主的多智能体系统支持工程设计流程端到端自动化的潜力。此外，我们还讨论了未来展望，以增强和充分实现ID 4.0的潜力，包括更复杂的设计场景、更实际的设计实现、新颖的智能体协调机制以及具有更好人类价值对齐的自主设计目标设定。总而言之，这些见解为推动智能设计在应对日益复杂的设计挑战方面实现更大的适应性、自主性和有效性奠定了基础。", "summary": "本文介绍了智能设计4.0（ID 4.0），一个由代理AI系统驱动的新兴范式，旨在利用基础模型（特别是LLM）的进步和多智能体协作实现工程设计流程的端到端自动化。论文回顾了ID的演进历程，提出了ID 4.0的概念框架，并探讨了其未来发展方向，以期提升设计过程的适应性、自主性和效率。", "keywords": "智能设计4.0, 代理AI, 多智能体系统, 工程设计, 范式演进", "comments": "这篇论文提出了一个前瞻性的智能设计范式，将当前AI领域最热门的代理AI概念引入工程设计，具有重要的理论和实践意义。它不仅回顾了ID的历史，更展望了未来发展，指明了多智能体协作在自动化设计中的巨大潜力。"}}
{"id": "2506.09221", "title": "In Crowd Veritas: Leveraging Human Intelligence To Fight Misinformation", "authors": ["Michael Soprano"], "summary": "The spread of online misinformation poses serious threats to democratic\nsocieties. Traditionally, expert fact-checkers verify the truthfulness of\ninformation through investigative processes. However, the volume and immediacy\nof online content present major scalability challenges. Crowdsourcing offers a\npromising alternative by leveraging non-expert judgments, but it introduces\nconcerns about bias, accuracy, and interpretability. This thesis investigates\nhow human intelligence can be harnessed to assess the truthfulness of online\ninformation, focusing on three areas: misinformation assessment, cognitive\nbiases, and automated fact-checking systems. Through large-scale crowdsourcing\nexperiments and statistical modeling, it identifies key factors influencing\nhuman judgments and introduces a model for the joint prediction and explanation\nof truthfulness. The findings show that non-expert judgments often align with\nexpert assessments, particularly when factors such as timing and experience are\nconsidered. By deepening our understanding of human judgment and bias in\ntruthfulness assessment, this thesis contributes to the development of more\ntransparent, trustworthy, and interpretable systems for combating\nmisinformation.", "comment": "PhD thesis, University of Udine, defended May 2023, 458 pages", "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.09221v1", "AI": {"title_translation": "群众之中有真理：利用人类智慧对抗虚假信息", "tldr": "本文研究如何利用众包（非专家判断）来对抗网络虚假信息，发现非专家判断在考虑特定因素时与专家判断一致，并提出了一种预测和解释真实性的模型，以期开发更透明、可信和可解释的打假系统。", "motivation": "线上虚假信息的传播对民主社会构成严重威胁。传统专家事实核查面临内容量大和即时性的可扩展性挑战。众包虽有潜力，但也引入了偏见、准确性和可解释性问题。本论文旨在探讨如何利用人类智慧来评估在线信息的真实性。", "method": "通过大规模众包实验和统计建模，研究虚假信息评估、认知偏见和自动化事实核查系统三个领域，识别影响人类判断的关键因素，并引入了一个用于真实性联合预测和解释的模型。", "result": "研究发现，非专家判断通常与专家评估一致，尤其是在考虑了时间、经验等因素时。", "conclusion": "本论文通过加深对真实性评估中人类判断和偏见的理解，有助于开发更透明、可信和可解释的虚假信息打击系统。", "translation": "线上虚假信息的传播对民主社会构成严重威胁。传统上，专家事实核查员通过调查过程验证信息的真实性。然而，在线内容的数量和即时性带来了重大的可扩展性挑战。众包通过利用非专家判断提供了一个有前景的替代方案，但它引入了关于偏见、准确性和可解释性的担忧。本论文研究了如何利用人类智慧来评估在线信息的真实性，重点关注三个领域：虚假信息评估、认知偏见和自动化事实核查系统。通过大规模众包实验和统计建模，它识别了影响人类判断的关键因素，并引入了一个用于真实性联合预测和解释的模型。研究结果表明，非专家判断通常与专家评估一致，尤其是在考虑了时间、经验等因素时。通过加深我们对真实性评估中人类判断和偏见的理解，本论文有助于开发更透明、可信和可解释的虚假信息打击系统。", "summary": "本文探讨了利用人类智慧（尤其是众包）对抗网络虚假信息的可行性。针对传统专家事实核查的可扩展性问题及众包的固有挑战，作者通过大规模众包实验和统计建模，研究了虚假信息评估、认知偏见和自动化事实核查系统。研究发现，在考虑时间、经验等因素后，非专家判断与专家评估高度一致。这项工作深化了对人类判断和偏见的理解，为构建更透明、可信和可解释的虚假信息打击系统奠定了基础。", "keywords": "虚假信息, 众包, 事实核查, 人类智慧, 认知偏见", "comments": "本文的创新之处在于其大规模的众包实验设计，以及对非专家判断在特定条件下与专家判断一致性的验证。这为解决传统事实核查的扩展性问题提供了新的视角，并强调了在设计自动化系统时理解人类认知偏见的重要性。"}}
{"id": "2506.09600", "title": "Effective Red-Teaming of Policy-Adherent Agents", "authors": ["Itay Nakash", "George Kour", "Koren Lazar", "Matan Vetzler", "Guy Uziel", "Ateret Anaby-Tavor"], "summary": "Task-oriented LLM-based agents are increasingly used in domains with strict\npolicies, such as refund eligibility or cancellation rules. The challenge lies\nin ensuring that the agent consistently adheres to these rules and policies,\nappropriately refusing any request that would violate them, while still\nmaintaining a helpful and natural interaction. This calls for the development\nof tailored design and evaluation methodologies to ensure agent resilience\nagainst malicious user behavior. We propose a novel threat model that focuses\non adversarial users aiming to exploit policy-adherent agents for personal\nbenefit. To address this, we present CRAFT, a multi-agent red-teaming system\nthat leverages policy-aware persuasive strategies to undermine a\npolicy-adherent agent in a customer-service scenario, outperforming\nconventional jailbreak methods such as DAN prompts, emotional manipulation, and\ncoercive. Building upon the existing tau-bench benchmark, we introduce\ntau-break, a complementary benchmark designed to rigorously assess the agent's\nrobustness against manipulative user behavior. Finally, we evaluate several\nstraightforward yet effective defense strategies. While these measures provide\nsome protection, they fall short, highlighting the need for stronger,\nresearch-driven safeguards to protect policy-adherent agents from adversarial\nattacks", "comment": null, "cate": "cs.MA", "url": "http://arxiv.org/abs/2506.09600v1", "AI": {"title_translation": "政策遵循型代理的有效红队测试", "tldr": "本文提出了一个新颖的威胁模型和多智能体红队系统CRAFT，用于有效测试政策遵循型LLM代理的鲁棒性，发现现有防御措施不足，强调需要更强的保障。", "motivation": "确保任务导向型LLM代理在诸如退款资格或取消规则等具有严格政策的领域中，能够始终遵守规则，并能适当拒绝违反政策的请求，同时保持有用和自然的交互，并抵御恶意用户的利用。", "method": "提出了一个新颖的威胁模型，专注于旨在利用政策遵循型代理以谋取个人利益的对抗性用户。设计并提出了CRAFT，一个多智能体红队系统，它利用政策感知说服策略来破坏客户服务场景中的政策遵循型代理。在现有tau-bench基准的基础上，引入了tau-break，一个补充基准，旨在严格评估代理对操纵性用户行为的鲁棒性。评估了几种直接但有效的防御策略。", "result": "CRAFT在客户服务场景中优于传统的越狱方法，如DAN提示、情感操纵和胁迫。现有防御措施虽然提供了一些保护，但不足以充分抵御对抗性攻击。", "conclusion": "需要更强大、研究驱动的保障措施来保护政策遵循型代理免受对抗性攻击。", "translation": "基于LLM的任务导向型代理越来越多地应用于具有严格政策的领域，例如退款资格或取消规则。挑战在于确保代理始终遵守这些规则和政策，适当地拒绝任何违反请求，同时仍然保持有用和自然的交互。这需要开发量身定制的设计和评估方法，以确保代理对恶意用户行为的弹性。我们提出了一种新颖的威胁模型，专注于旨在利用政策遵循型代理谋取个人利益的对抗性用户。为了解决这个问题，我们提出了CRAFT，一个多代理红队系统，它利用政策感知说服策略来破坏客户服务场景中的政策遵循型代理，其性能优于传统的越狱方法，如DAN提示、情感操纵和胁迫。在现有tau-bench基准的基础上，我们引入了tau-break，一个补充基准，旨在严格评估代理对操纵性用户行为的鲁棒性。最后，我们评估了几种直接但有效的防御策略。虽然这些措施提供了一些保护，但它们不足，这凸显了需要更强大、研究驱动的保障措施来保护政策遵循型代理免受对抗性攻击。", "summary": "本论文关注任务导向型LLM代理在严格政策领域中面临的挑战，即如何确保代理遵守政策并抵御恶意利用。为此，作者提出了一个新颖的威胁模型，并开发了多智能体红队系统CRAFT，该系统通过政策感知说服策略有效攻击政策遵循型代理，并优于传统越狱方法。同时，引入了tau-break基准来评估代理的鲁棒性。研究发现，尽管评估了一些防御策略，但现有措施仍不足以提供充分保护，强调了未来需要更强大的研究驱动型安全保障。", "keywords": "LLM代理, 红队测试, 政策遵循, 对抗性攻击, 鲁棒性", "comments": "这篇论文创新性地提出了一个针对政策遵循型LLM代理的威胁模型和红队测试系统CRAFT，填补了现有越狱方法在特定场景下的不足。引入tau-break基准对于评估代理鲁棒性具有重要意义。研究结果揭示了当前防御策略的局限性，对未来构建更安全的AI代理具有指导价值。"}}
{"id": "2506.09273", "title": "Data-Driven Nonlinear Regulation: Gaussian Process Learning", "authors": ["Telema Harry", "Martin Guay", "Shimin Wang", "Richard D. Braatz"], "summary": "This article addresses the output regulation problem for a class of nonlinear\nsystems using a data-driven approach. An output feedback controller is proposed\nthat integrates a traditional control component with a data-driven learning\nalgorithm based on Gaussian Process (GP) regression to learn the nonlinear\ninternal model. Specifically, a data-driven technique is employed to directly\napproximate the unknown internal model steady-state map from observed\ninput-output data online. Our method does not rely on model-based observers\nutilized in previous studies, making it robust and suitable for systems with\nmodelling errors and model uncertainties. Finally, we demonstrate through\nnumerical examples and detailed stability analysis that, under suitable\nconditions, the closed-loop system remains bounded and converges to a compact\nset, with the size of this set decreasing as the accuracy of the data-driven\nmodel improves over time.", "comment": null, "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.09273v1", "AI": {"title_translation": "数据驱动的非线性调节：高斯过程学习", "tldr": "本文提出了一种结合传统控制和高斯过程学习的数据驱动输出反馈控制器，用于非线性系统的输出调节问题，该方法不依赖模型观测器，并通过数据驱动模型提高精度，确保系统稳定性和收敛性。", "motivation": "解决非线性系统的输出调节问题，并克服传统方法对模型观测器的依赖，提高对建模误差和模型不确定性的鲁棒性。", "method": "提出了一种输出反馈控制器，该控制器将传统控制组件与基于高斯过程（GP）回归的数据驱动学习算法相结合，以在线学习非线性内部模型，直接从观测到的输入输出数据中近似未知内部模型稳态映射。", "result": "通过数值例子和详细的稳定性分析表明，在适当条件下，闭环系统保持有界并收敛到一个紧集，并且随着数据驱动模型精度的提高，该紧集的大小会随时间减小。", "conclusion": "该数据驱动的非线性调节方法能够有效解决非线性系统的输出调节问题，并通过在线学习和数据驱动模型精度提升，确保系统的鲁棒稳定性与收敛性。", "translation": "本文采用数据驱动方法解决了非线性系统的一类输出调节问题。提出了一种输出反馈控制器，该控制器将传统控制组件与基于高斯过程（GP）回归的数据驱动学习算法相结合，以学习非线性内部模型。具体来说，采用一种数据驱动技术，在线从观测到的输入输出数据中直接近似未知内部模型的稳态映射。我们的方法不依赖于以往研究中使用的基于模型的观测器，这使得它对具有建模误差和模型不确定性的系统具有鲁棒性和适用性。最后，我们通过数值例子和详细的稳定性分析表明，在适当条件下，闭环系统保持有界并收敛到一个紧集，并且随着数据驱动模型精度的提高，该紧集的大小会随时间减小。", "summary": "本文提出了一种新颖的数据驱动输出反馈控制器，用于解决非线性系统的输出调节问题。该控制器将传统控制与基于高斯过程回归的数据驱动学习相结合，实现在线近似未知非线性内部模型的稳态映射，从而避免了对模型观测器的依赖，增强了对建模误差和不确定性的鲁棒性。研究通过理论分析和数值示例证明，闭环系统能够保持有界并收敛至一个紧集，且该集合的大小随数据驱动模型精度的提升而减小。", "keywords": "数据驱动, 非线性调节, 高斯过程, 输出反馈控制, 内部模型", "comments": "这篇论文的创新点在于将高斯过程学习引入到非线性系统的输出调节问题中，特别是通过数据驱动的方式在线学习内部模型，从而摆脱了对传统模型观测器的依赖。这使得该方法对实际系统中常见的模型不确定性和误差具有更强的鲁棒性，具有重要的实际应用价值。其理论分析也清晰地阐明了系统的稳定性和收敛性。"}}
{"id": "2506.09245", "title": "Age of Information in Unreliable Tandem Queues", "authors": ["Muthukrishnan Senthilkumar", "Aresh Dadlani", "Hina Tabassum"], "summary": "Stringent demands for timely information delivery, driven by the widespread\nadoption of real-time applications and the Internet of Things, have established\nthe age of information (AoI) as a critical metric for quantifying data\nfreshness. Existing AoI models often assume multi-hop communication networks\nwith fully reliable nodes, which may not accurately capture scenarios involving\nnode transmission failures. This paper presents an analytical framework for two\nconfigurations of tandem queue systems, where status updates generated by a\nsingle sensor are relayed to a destination monitor through unreliable\nintermediate nodes. Using the probability generating function, we first derive\nthe sojourn time distribution for an infinite-buffer M/M/1 tandem system with\ntwo unreliable nodes. We then extend our analysis to an M/G/1 tandem system\nwith an arbitrary number of unreliable nodes, employing the supplementary\nvariable technique while assuming that only the first node has an infinite\nbuffer. Numerical results demonstrate the impact of key system parameters on\nthe average AoI in unreliable tandem queues with Markovian and non-Markovian\nservice times.", "comment": null, "cate": "cs.NI", "url": "http://arxiv.org/abs/2506.09245v1", "AI": {"title_translation": "不可靠串联队列中的信息年龄", "tldr": "本文提出了一个分析框架，用于研究不可靠多跳串联队列系统中的信息年龄（AoI），并分析了M/M/1和M/G/1配置。", "motivation": "实时应用和物联网的广泛普及对及时信息交付提出了严格要求，使得信息年龄（AoI）成为量化数据新鲜度的关键指标。然而，现有AoI模型通常假设多跳通信网络中的节点完全可靠，这无法准确捕捉涉及节点传输失败的实际场景。", "method": "本文提出了两种串联队列系统的分析框架：1. 对于具有两个不可靠节点的无限缓冲区M/M/1串联系统，使用概率生成函数推导了停留时间分布。2. 对于具有任意数量不可靠节点的M/G/1串联系统（假设只有第一个节点具有无限缓冲区），采用补充变量技术进行分析。此外，通过数值结果演示了关键系统参数的影响。", "result": "推导出了具有两个不可靠节点的无限缓冲区M/M/1串联系统的停留时间分布。将分析扩展到具有任意数量不可靠节点的M/G/1串联系统。数值结果表明了关键系统参数对具有马尔可夫和非马尔可夫服务时间的不可靠串联队列中平均AoI的影响。", "conclusion": "本文为不可靠多跳串联队列系统中的信息年龄（AoI）提供了一个分析框架，并展示了系统参数对不同服务时间分布下平均AoI的影响。", "translation": "对实时应用和物联网的广泛采用对及时信息交付的严格要求，使得信息年龄（AoI）成为衡量数据新鲜度的关键指标。现有的AoI模型通常假设多跳通信网络中的节点完全可靠，这可能无法准确捕捉涉及节点传输失败的场景。本文提出了两种串联队列系统的分析框架，其中由单个传感器生成的状态更新通过不可靠的中间节点中继到目的地监视器。我们首先使用概率生成函数推导了具有两个不可靠节点的无限缓冲区M/M/1串联系统的停留时间分布。然后，我们将分析扩展到具有任意数量不可靠节点的M/G/1串联系统，采用补充变量技术，同时假设只有第一个节点具有无限缓冲区。数值结果表明了关键系统参数对具有马尔可夫和非马尔可夫服务时间的不可靠串联队列中平均AoI的影响。", "summary": "本文针对现有信息年龄（AoI）模型未考虑节点传输失败的问题，提出了一个分析框架，用于研究不可靠串联队列系统中的信息年龄。文章分析了M/M/1和M/G/1两种配置，分别采用概率生成函数和补充变量技术推导了关键性能指标，并通过数值结果展示了系统参数对平均AoI的影响。", "keywords": "信息年龄, 串联队列, 不可靠节点, 停留时间, 数据新鲜度", "comments": "该论文通过考虑节点不可靠性，填补了现有AoI模型在多跳网络中与现实场景不符的空白，具有创新性。其采用概率生成函数和补充变量技术分析不同系统配置，显示了全面的分析方法。"}}
{"id": "2506.09596", "title": "FPGA-Based Multiplier with a New Approximate Full Adder for Error-Resilient Applications", "authors": ["Ali Ranjbar", "Elham Esmaeili", "Roghayeh Rafieisangari", "Nabiollah Shiri"], "summary": "Electronic devices primarily aim to offer low power consumption, high speed,\nand a compact area. The performance of very large-scale integration (VLSI)\ndevices is influenced by arithmetic operations, where multiplication is a\ncrucial operation. Therefore, a high-speed multiplier is essential for\ndeveloping any signal-processing module. Numerous multipliers have been\nreviewed in existing literature, and their speed is largely determined by how\npartial products (PPs) are accumulated. To enhance the speed of multiplication\nbeyond current methods, an approximate adder-based multiplier is introduced.\nThis approach allows for the simultaneous addition of PPs from two consecutive\nbits using a novel approximate adder. The proposed multiplier is utilized in a\nmean filter structure and implemented in ISE Design Suite 14.7 using VHDL and\nsynthesized on the Xilinx Spartan3-XC3S400 FPGA board. Compared to the\nliterature, the proposed multiplier achieves power and power-delay product\n(PDP) improvements of 56.09% and 73.02%, respectively. The validity of the\nexpressed multiplier is demonstrated through the mean filter system. Results\nshow that it achieves power savings of 33.33%. Additionally, the proposed\nmultiplier provides more accurate results than other approximate multipliers by\nexpressing higher values of peak signal-to-noise ratio (PSNR), (30.58%), and\nstructural similarity index metric (SSIM), (22.22%), while power consumption is\nin a low range.", "comment": null, "cate": "cs.AR", "url": "http://arxiv.org/abs/2506.09596v1", "AI": {"title_translation": "基于FPGA的带有新型近似全加器的容错应用乘法器", "tldr": "提出了一种基于新型近似加法器的FPGA乘法器，在功耗、PDP和精度方面优于现有方法，适用于容错应用。", "motivation": "电子设备追求低功耗、高速和紧凑面积。超大规模集成（VLSI）设备的性能受算术操作影响，其中乘法是关键操作。现有乘法器速度受部分积累积方式限制，因此需要开发高速乘法器以提升信号处理模块的性能。", "method": "引入了一种基于新型近似加法器的乘法器，该加法器能够同时处理来自两个连续位的偏积。该乘法器在均值滤波器结构中得到应用，并使用VHDL语言在ISE Design Suite 14.7中实现，最终在Xilinx Spartan3-XC3S400 FPGA板上进行了综合验证。", "result": "与现有文献相比，所提出的乘法器在功耗和功耗-延迟积（PDP）方面分别实现了56.09%和73.02%的改进。在均值滤波器系统中，它实现了33.33%的功耗节省。此外，该乘法器通过提高PSNR（30.58%）和SSIM（22.22%）值，提供了比其他近似乘法器更准确的结果，同时保持低功耗。", "conclusion": "所提出的基于新型近似全加器的FPGA乘法器在功耗、速度和精度方面表现出显著优势，使其成为容错应用的有效解决方案。", "translation": "电子设备主要目标是提供低功耗、高速和紧凑面积。超大规模集成（VLSI）设备的性能受到算术运算的影响，其中乘法是一项关键操作。因此，高速乘法器对于开发任何信号处理模块都至关重要。现有文献中已经回顾了大量的乘法器，它们的速度很大程度上取决于部分积（PPs）的累积方式。为了超越现有方法提高乘法速度，引入了一种基于近似加法器的乘法器。这种方法允许使用一种新型近似加法器同时添加来自两个连续位的PPs。所提出的乘法器被用于均值滤波器结构中，并使用VHDL在ISE Design Suite 14.7中实现，并在Xilinx Spartan3-XC3S400 FPGA板上进行综合。与现有文献相比，所提出的乘法器在功耗和功耗-延迟积（PDP）方面分别实现了56.09%和73.02%的改进。通过均值滤波器系统证明了所表达乘法器的有效性。结果表明，它实现了33.33%的功耗节省。此外，所提出的乘法器通过表达更高的峰值信噪比（PSNR）（30.58%）和结构相似性指数（SSIM）（22.22%）值，提供了比其他近似乘法器更准确的结果，同时功耗处于低范围。", "summary": "该论文介绍了一种基于新型近似全加器的FPGA乘法器，旨在解决电子设备对低功耗、高速和紧凑面积的需求，特别是在乘法运算方面。该乘法器通过允许同时处理两个连续位的偏积来提高速度。在均值滤波器结构中实现并验证后，实验结果表明，与现有方法相比，该乘法器在功耗和功耗-延迟积上显著优化，同时提供了更高的精度和更低的功耗，使其适用于容错应用。", "keywords": "FPGA, 近似乘法器, 近似全加器, 功耗优化, 容错应用", "comments": "该论文的创新点在于引入了一种新型近似全加器来优化FPGA上的乘法运算，这对于需要兼顾性能和能效的容错应用具有重要意义。通过具体的硬件实现和量化指标（如功耗、PDP、PSNR和SSIM）的验证，论文有效展示了其方法在精度损失可控的前提下，在功耗和速度方面带来的显著提升。这种近似计算方法在资源受限或对实时性要求高的场景中，如边缘计算和物联网设备，具有广阔的应用前景。"}}
{"id": "2506.09231", "title": "Enhancing Acoustic-to-Articulatory Speech Inversion by Incorporating Nasality", "authors": ["Saba Tabatabaee", "Suzanne Boyce", "Liran Oren", "Mark Tiede", "Carol Espy-Wilson"], "summary": "Speech is produced through the coordination of vocal tract constricting\norgans: lips, tongue, velum, and glottis. Previous works developed Speech\nInversion (SI) systems to recover acoustic-to-articulatory mappings for lip and\ntongue constrictions, called oral tract variables (TVs), which were later\nenhanced by including source information (periodic and aperiodic energies, and\nF0 frequency) as proxies for glottal control. Comparison of the nasometric\nmeasures with high-speed nasopharyngoscopy showed that nasalance can serve as\nground truth, and that an SI system trained with it reliably recovers velum\nmovement patterns for American English speakers. Here, two SI training\napproaches are compared: baseline models that estimate oral TVs and nasalance\nindependently, and a synergistic model that combines oral TVs and source\nfeatures with nasalance. The synergistic model shows relative improvements of\n5% in oral TVs estimation and 9% in nasalance estimation compared to the\nbaseline models.", "comment": "Accepted to be presented at Interspeech 2025", "cate": "eess.AS", "url": "http://arxiv.org/abs/2506.09231v1", "AI": {"title_translation": "通过引入鼻音特征增强声学-发音语音反演", "tldr": "本文通过将鼻音特征（鼻音度）与口腔声道变量和声源特征相结合，显著提高了声学-发音语音反演系统的性能。", "motivation": "先前的语音反演（SI）系统主要关注唇部和舌部等口腔声道变量的恢复，并通过引入声源信息来增强声门控制的代理。然而，这些系统缺乏对鼻音特征（如软腭运动）的有效建模，而鼻音在语音产生中扮演着重要角色。因此，本文的动机是探索通过整合鼻音特征来进一步提升声学-发音语音反演系统的准确性和全面性。", "method": "研究比较了两种语音反演（SI）训练方法：基线模型和协同模型。基线模型独立估计口腔声道变量和鼻音度。协同模型则将口腔声道变量和声源特征与鼻音度相结合进行训练。通过与高速鼻咽镜检查结果的比较，确认了鼻音度可以作为软腭运动的真实参考。", "result": "与基线模型相比，协同模型在口腔声道变量估计方面显示出5%的相对改善，在鼻音度估计方面显示出9%的相对改善。", "conclusion": "将鼻音特征（鼻音度）与口腔声道变量和声源特征协同整合到语音反演系统中，可以显著提高声学-发音映射的恢复精度，尤其是在软腭运动和整体语音产生建模方面。", "translation": "语音是通过声道的收缩器官：嘴唇、舌头、软腭和声门的协同作用产生的。以前的工作开发了语音反演（SI）系统来恢复唇部和舌部收缩的声学-发音映射，这些被称为口腔声道变量（TVs），后来通过包含声源信息（周期性和非周期性能量，以及F0频率）作为声门控制的代理得到了增强。鼻音测量与高速鼻咽镜检查的比较表明，鼻音度可以作为真实参考，并且使用其训练的SI系统能够可靠地恢复美式英语说话者的软腭运动模式。本文比较了两种SI训练方法：独立估计口腔声道变量和鼻音度的基线模型，以及将口腔声道变量和声源特征与鼻音度相结合的协同模型。与基线模型相比，协同模型在口腔声道变量估计方面显示出5%的相对改善，在鼻音度估计方面显示出9%的相对改善。", "summary": "本研究旨在通过引入鼻音特征来增强声学-发音语音反演（SI）系统。文章比较了两种SI训练方法：一种是独立估计口腔声道变量和鼻音度的基线模型，另一种是将口腔声道变量、声源特征与鼻音度相结合的协同模型。结果表明，协同模型在口腔声道变量估计方面有5%的相对改善，在鼻音度估计方面有9%的相对改善，证明了结合鼻音特征对SI系统性能的显著提升。", "keywords": "语音反演, 鼻音度, 声学-发音, 协同模型, 软腭运动", "comments": "这篇论文的创新点在于明确提出并验证了将鼻音特征（尤其是鼻音度作为软腭运动的代理）整合到声学-发音语音反演系统中的有效性。以往的研究多关注口腔声道和声门控制，而对鼻音的考虑较少。通过引入协同模型，论文不仅提升了对口腔声道变量的估计精度，更显著改善了对软腭运动的恢复，这对于更全面、准确地理解和建模语音产生过程具有重要意义。该方法为未来更精细的语音合成、语音障碍诊断等应用提供了新的思路。"}}
{"id": "2506.09061", "title": "EdgeProfiler: A Fast Profiling Framework for Lightweight LLMs on Edge Using Analytical Model", "authors": ["Alyssa Pinnock", "Shakya Jayakody", "Kawsher A Roxy", "Md Rubel Ahmed"], "summary": "This paper introduces EdgeProfiler, a fast profiling framework designed for\nevaluating lightweight Large Language Models (LLMs) on edge systems. While LLMs\noffer remarkable capabilities in natural language understanding and generation,\ntheir high computational, memory, and power requirements often confine them to\ncloud environments. EdgeProfiler addresses these challenges by providing a\nsystematic methodology for assessing LLM performance in resource-constrained\nedge settings. The framework profiles compact LLMs, including TinyLLaMA,\nGemma3.1B, Llama3.2-1B, and DeepSeek-r1-1.5B, using aggressive quantization\ntechniques and strict memory constraints. Analytical modeling is used to\nestimate latency, FLOPs, and energy consumption. The profiling reveals that\n4-bit quantization reduces model memory usage by approximately 60-70%, while\nmaintaining accuracy within 2-5% of full-precision baselines. Inference speeds\nare observed to improve by 2-3x compared to FP16 baselines across various edge\ndevices. Power modeling estimates a 35-50% reduction in energy consumption for\nINT4 configurations, enabling practical deployment on hardware such as\nRaspberry Pi 4/5 and Jetson Orin Nano Super. Our findings emphasize the\nimportance of efficient profiling tailored to lightweight LLMs in edge\nenvironments, balancing accuracy, energy efficiency, and computational\nfeasibility.", "comment": "4 figures, 7 pages, IEEE conference template", "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.09061v1", "AI": {"title_translation": "EdgeProfiler：一种基于分析模型的边缘轻量级LLM快速性能分析框架", "tldr": "EdgeProfiler是一个快速性能分析框架，用于评估边缘设备上的轻量级LLM，通过分析模型和量化技术，显著降低内存和能耗，并提高推理速度，使其能在资源受限的边缘设备上部署。", "motivation": "大型语言模型（LLM）的高计算、内存和功耗要求限制了它们在云环境中的部署。本文旨在解决在资源受限的边缘环境中评估LLM性能的挑战。", "method": "本文介绍了EdgeProfiler，一个快速性能分析框架。它采用分析模型来估计LLM的延迟、FLOPs和能耗。该框架通过激进的量化技术和严格的内存限制，对包括TinyLLaMA、Gemma3.1B、Llama3.2-1B和DeepSeek-r1-1.5B在内的紧凑型LLM进行性能分析。", "result": "4位量化将模型内存使用量减少了约60-70%，同时将精度保持在全精度基线的2-5%以内。与FP16基线相比，推理速度提高了2-3倍。功耗模型估计INT4配置的能耗降低了35-50%，从而实现在Raspberry Pi 4/5和Jetson Orin Nano Super等硬件上的实际部署。", "conclusion": "针对边缘环境中轻量级LLM的高效性能分析至关重要，它能平衡准确性、能效和计算可行性。", "translation": "本文介绍了EdgeProfiler，一个为评估边缘系统上轻量级大型语言模型（LLM）而设计的快速性能分析框架。尽管LLM在自然语言理解和生成方面提供了卓越的能力，但其高计算、内存和功耗要求常常将其限制在云环境中。EdgeProfiler通过提供一种系统方法来评估资源受限边缘设置中的LLM性能，从而解决了这些挑战。该框架使用激进的量化技术和严格的内存限制，对包括TinyLLaMA、Gemma3.1B、Llama3.2-1B和DeepSeek-r1-1.5B在内的紧凑型LLM进行性能分析。分析模型用于估计延迟、FLOPs和能耗。性能分析显示，4位量化将模型内存使用量减少了大约60-70%，同时将精度保持在全精度基线的2-5%以内。与FP16基线相比，在各种边缘设备上，推理速度提高了2-3倍。功耗模型估计INT4配置的能耗降低了35-50%，从而实现在Raspberry Pi 4/5和Jetson Orin Nano Super等硬件上的实际部署。我们的发现强调了针对边缘环境中轻量级LLM的高效性能分析的重要性，以平衡准确性、能效和计算可行性。", "summary": "EdgeProfiler是一个快速性能分析框架，用于评估边缘设备上的轻量级LLM。它通过分析模型估算量化LLM（如TinyLLaMA、Gemma3.1B等）的延迟、FLOPs和能耗。研究表明，4位量化可将内存使用减少60-70%，同时保持2-5%的精度损失；推理速度提高2-3倍；能耗降低35-50%，使得LLM能在Raspberry Pi和Jetson等边缘硬件上部署。这强调了在边缘环境中对轻量级LLM进行高效性能分析以平衡性能、能效和可行性的重要性。", "keywords": "EdgeProfiler, 轻量级LLM, 边缘计算, 量化, 分析模型", "comments": "本文解决了在资源受限的边缘设备上部署LLM的关键挑战。其创新之处在于使用分析模型进行快速性能分析，并通过激进的量化技术显著提升了内存、速度和能效。在树莓派和Jetson等常见边缘硬件上的实际部署展示了其强大的实际应用价值，对于推动LLM在云环境之外的普及具有重要意义。"}}
{"id": "2506.09167", "title": "Estimating Visceral Adiposity from Wrist-Worn Accelerometry", "authors": ["James R. Williamson", "Andrew Alini", "Brian A. Telfer", "Adam W. Potter", "Karl E. Friedl"], "summary": "Visceral adipose tissue (VAT) is a key marker of both metabolic health and\nhabitual physical activity (PA). Excess VAT is highly correlated with type 2\ndiabetes and insulin resistance. The mechanistic basis for this pathophysiology\nrelates to overloading the liver with fatty acids. VAT is also a highly labile\nfat depot, with increased turnover stimulated by catecholamines during\nexercise. VAT can be measured with sophisticated imaging technologies, but can\nalso be inferred directly from PA. We tested this relationship using National\nHealth and Nutrition Examination Survey (NHANES) data from 2011-2014, for\nindividuals aged 20-60 years with 7 days of accelerometry data (n=2,456 men;\n2,427 women) [1]. Two approaches were used for estimating VAT from activity.\nThe first used engineered features based on movements during gait and sleep,\nand then ridge regression to map summary statistics of these features into a\nVAT estimate. The second approach used deep neural networks trained on 24 hours\nof continuous accelerometry. A foundation model first mapped each 10s frame\ninto a high-dimensional feature vector. A transformer model then mapped each\nday's feature vector time series into a VAT estimate, which were averaged over\nmultiple days. For both approaches, the most accurate estimates were obtained\nwith the addition of covariate information about subject demographics and body\nmeasurements. The best performance was obtained by combining the two\napproaches, resulting in VAT estimates with correlations of r=0.86. These\nfindings demonstrate a strong relationship between PA and VAT and, by\nextension, between PA and metabolic health risks.", "comment": "13 pages", "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.09167v1", "AI": {"title_translation": "从腕戴式加速度计估算内脏脂肪", "tldr": "本研究使用腕戴式加速度计数据，通过两种机器学习方法（特征工程+岭回归和深度神经网络）成功估算内脏脂肪，并发现结合两种方法和协变量信息可获得高精度（r=0.86）的估算结果。", "motivation": "内脏脂肪组织（VAT）是代谢健康和日常体育活动的关键指标，过量的VAT与2型糖尿病和胰岛素抵抗高度相关。虽然可以通过复杂的影像技术测量VAT，但也可以从体育活动中推断，本研究旨在验证这种推断关系。", "method": "研究使用了2011-2014年国家健康和营养检查调查（NHANES）数据，包括20-60岁个体7天的加速度计数据（2,456名男性；2,427名女性）。采用了两种方法从活动中估算VAT：1. 基于步态和睡眠期间运动的工程特征，然后使用岭回归进行估算。2. 使用在24小时连续加速度计数据上训练的深度神经网络，包括一个基础模型将10秒帧映射到高维特征向量，以及一个Transformer模型将每日特征向量时间序列映射到VAT估算值。两种方法均通过添加受试者人口统计学和身体测量协变量信息获得了最准确的估算。", "result": "结合两种方法可以获得最佳性能，估算的VAT与真实值之间的相关性达到r=0.86。", "conclusion": "这些发现表明体育活动与内脏脂肪之间存在强烈的关系，进而也表明体育活动与代谢健康风险之间存在关系。", "translation": "内脏脂肪组织（VAT）是代谢健康和日常体育活动（PA）的关键标志物。过量的VAT与2型糖尿病和胰岛素抵抗高度相关。这种病理生理学机制基础与肝脏被脂肪酸过载有关。VAT也是一个高度不稳定的脂肪储存库，运动时儿茶酚胺刺激其周转增加。VAT可以通过复杂的影像技术测量，但也可以直接从PA中推断出来。我们使用2011-2014年国家健康和营养检查调查（NHANES）数据，对20-60岁且拥有7天加速度计数据的个体（2,456名男性；2,427名女性）测试了这种关系[1]。采用了两种方法从活动中估算VAT。第一种方法使用基于步态和睡眠期间运动的工程特征，然后使用岭回归将这些特征的汇总统计数据映射到VAT估算值。第二种方法使用在24小时连续加速度计数据上训练的深度神经网络。一个基础模型首先将每个10秒帧映射到高维特征向量。然后一个Transformer模型将每天的特征向量时间序列映射到VAT估算值，并对多天的结果取平均。对于这两种方法，通过添加关于受试者人口统计学和身体测量协变量信息可以获得最准确的估算。通过结合这两种方法获得了最佳性能，VAT估算值与真实值之间的相关性达到r=0.86。这些发现证明了PA与VAT之间存在强烈的关系，并由此推断PA与代谢健康风险之间也存在关系。", "summary": "本研究旨在探索利用腕戴式加速度计数据估算内脏脂肪组织（VAT）的可行性，因为VAT是代谢健康和体育活动的重要生物标志物。研究利用了大型健康调查数据，开发并比较了两种机器学习方法：一种基于特征工程和岭回归，另一种基于深度神经网络。结果显示，结合两种方法并加入人口统计学和身体测量协变量信息后，可以实现高精度的VAT估算（相关性r=0.86）。这表明体育活动与内脏脂肪及代谢健康风险之间存在强关联。", "keywords": "内脏脂肪组织, 加速度计, 机器学习, 代谢健康, 体育活动", "comments": "这篇论文展示了一种非侵入性、可穿戴设备估算内脏脂肪的创新方法，这对于大规模健康监测和早期代谢疾病风险筛查具有重要意义。结合传统机器学习和深度学习的混合方法，并利用协变量信息来提高准确性，是其方法学上的亮点。高达0.86的相关性表明了该方法的巨大潜力。"}}
{"id": "2506.09065", "title": "Exploring Image Transforms derived from Eye Gaze Variables for Progressive Autism Diagnosis", "authors": ["Abigail Copiaco", "Christian Ritz", "Yassine Himeur", "Valsamma Eapen", "Ammar Albanna", "Wathiq Mansoor"], "summary": "The prevalence of Autism Spectrum Disorder (ASD) has surged rapidly over the\npast decade, posing significant challenges in communication, behavior, and\nfocus for affected individuals. Current diagnostic techniques, though\neffective, are time-intensive, leading to high social and economic costs. This\nwork introduces an AI-powered assistive technology designed to streamline ASD\ndiagnosis and management, enhancing convenience for individuals with ASD and\nefficiency for caregivers and therapists. The system integrates transfer\nlearning with image transforms derived from eye gaze variables to diagnose ASD.\nThis facilitates and opens opportunities for in-home periodical diagnosis,\nreducing stress for individuals and caregivers, while also preserving user\nprivacy through the use of image transforms. The accessibility of the proposed\nmethod also offers opportunities for improved communication between guardians\nand therapists, ensuring regular updates on progress and evolving support\nneeds. Overall, the approach proposed in this work ensures timely, accessible\ndiagnosis while protecting the subjects' privacy, improving outcomes for\nindividuals with ASD.", "comment": "6 pages, 8 figures, and 1 table", "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.09065v1", "AI": {"title_translation": "探索源自眼动变量的图像变换在渐进式自闭症诊断中的应用", "tldr": "该研究提出了一种AI辅助技术，利用眼动变量衍生的图像变换和迁移学习，实现快速、便捷的居家自闭症诊断，同时保护用户隐私。", "motivation": "在过去十年中，自闭症谱系障碍（ASD）患病率迅速上升，而当前诊断技术耗时且成本高昂。因此，需要一种更便捷、高效、可及且保护隐私的ASD诊断和管理方法，尤其适用于居家定期诊断。", "method": "该工作引入了一种由人工智能驱动的辅助技术，该系统将迁移学习与源自眼动变量的图像变换相结合，用于诊断自闭症谱系障碍（ASD）。", "result": "所提出的方法促进了居家定期诊断的可能性，减轻了患者和护理人员的压力，通过使用图像变换保护了用户隐私，并改善了监护人与治疗师之间的沟通。", "conclusion": "该方法确保了及时、可及的诊断，同时保护了受试者的隐私，从而改善了自闭症谱系障碍患者的预后。", "translation": "在过去十年中，自闭症谱系障碍（ASD）的患病率迅速上升，给受影响个体的沟通、行为和专注力带来了重大挑战。当前的诊断技术虽然有效，但耗时较长，导致高昂的社会和经济成本。这项工作引入了一种由人工智能驱动的辅助技术，旨在简化ASD的诊断和管理，提高ASD患者的便利性以及护理人员和治疗师的效率。该系统将迁移学习与源自眼动变量的图像变换相结合，用于诊断ASD。这促进并开启了居家定期诊断的机会，减轻了个体和护理人员的压力，同时通过使用图像变换保护了用户隐私。所提出方法的可及性还为监护人和治疗师之间改善沟通提供了机会，确保定期更新进展和不断变化的支持需求。总的来说，这项工作中提出的方法确保了及时、可及的诊断，同时保护了受试者的隐私，改善了ASD患者的预后。", "summary": "本论文提出了一种AI驱动的辅助技术，旨在简化自闭症谱系障碍（ASD）的诊断和管理。该系统结合了迁移学习和从眼动变量中提取的图像变换，以实现及时、可及且保护隐私的居家定期诊断。此方法旨在减少诊断时间和成本，提高ASD患者的便利性，并加强护理人员与治疗师之间的沟通。", "keywords": "自闭症谱系障碍, 眼动变量, 图像变换, 迁移学习, AI诊断", "comments": "该论文的创新之处在于利用从眼动变量转换而来的图像结合迁移学习进行ASD诊断。这种方法提供了一种非侵入性、保护隐私且可居家使用的解决方案，有望显著减轻传统诊断方法的负担。"}}
{"id": "2506.09073", "title": "Understanding and Improving Data Repurposing", "authors": ["J. Parsons", "R. Lukyanenko", "B. Greenwood", "C. Cooper"], "summary": "We live in an age of unprecedented opportunities to use existing data for\ntasks not anticipated when those data were collected, resulting in widespread\ndata repurposing. This commentary defines and maps the scope of data\nrepurposing to highlight its importance for organizations and society and the\nneed to study data repurposing as a frontier of data management. We explain how\nrepurposing differs from original data use and data reuse and then develop a\nframework for data repurposing consisting of concepts and activities for\nadapting existing data to new tasks. The framework and its implications are\nillustrated using two examples of repurposing, one in healthcare and one in\ncitizen science. We conclude by suggesting opportunities for research to better\nunderstand data repurposing and enable more effective data repurposing\npractices.", "comment": null, "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.09073v1", "AI": {"title_translation": "理解与改进数据再利用", "tldr": "本文定义并探讨了数据再利用的范围和重要性，提出了一个数据再利用框架，并指出未来的研究方向。", "motivation": "在当前时代，利用现有数据来完成收集时未曾预料到的任务带来了前所未有的机遇，这导致了数据再利用的广泛应用。本文旨在强调数据再利用对组织和社会的重要性，以及将其作为数据管理前沿领域进行研究的必要性。", "method": "本文首先定义并描绘了数据再利用的范围，以突显其重要性。接着，解释了数据再利用与原始数据使用和数据重用之间的区别。然后，开发了一个包含概念和活动的数据再利用框架，用于使现有数据适应新任务。最后，通过医疗保健和公民科学领域的两个再利用示例来说明该框架及其影响。", "result": "开发了一个数据再利用框架，其中包含将现有数据适应新任务的概念和活动。该框架及其影响通过医疗保健和公民科学领域的两个再利用示例得到了阐释。", "conclusion": "研究人员总结了未来研究的机会，以更好地理解数据再利用，并实现更有效的数据再利用实践。", "translation": "我们生活在一个前所未有的时代，可以利用现有数据来完成数据收集时未曾预料到的任务，这导致了数据再利用的广泛应用。本文定义并描绘了数据再利用的范围，以突显其对组织和社会的重要性，以及将其作为数据管理前沿领域进行研究的必要性。我们解释了再利用与原始数据使用和数据重用有何不同，然后开发了一个数据再利用框架，其中包括使现有数据适应新任务的概念和活动。该框架及其影响通过医疗保健和公民科学领域的两个再利用示例进行了说明。最后，我们提出了研究机会，以更好地理解数据再利用并实现更有效的数据再利用实践。", "summary": "本文探讨了数据再利用现象，将其定义并描绘了其范围，强调了其对组织和社会的重要性。文章阐释了数据再利用与原始数据使用和数据重用之间的区别，并提出了一个数据再利用框架，该框架包含使现有数据适应新任务的概念和活动。通过医疗保健和公民科学领域的案例，该框架得到了说明。文章最后提出了未来研究方向，以促进对数据再利用的理解和实践。", "keywords": "数据再利用, 数据管理, 框架, 数据重用, 现有数据", "comments": "本文创新性地提出了数据再利用的概念，并将其与数据重用区分开来，为数据管理领域开辟了新的研究视角。其提出的框架为理解和实践数据再利用提供了理论基础和指导，对于充分利用现有数据资产具有重要意义。通过具体的案例说明，增强了理论的实用性。"}}
{"id": "2506.09443", "title": "LLMs Cannot Reliably Judge (Yet?): A Comprehensive Assessment on the Robustness of LLM-as-a-Judge", "authors": ["Songze Li", "Chuokun Xu", "Jiaying Wang", "Xueluan Gong", "Chen Chen", "Jirui Zhang", "Jun Wang", "Kwok-Yan Lam", "Shouling Ji"], "summary": "Large Language Models (LLMs) have demonstrated remarkable intelligence across\nvarious tasks, which has inspired the development and widespread adoption of\nLLM-as-a-Judge systems for automated model testing, such as red teaming and\nbenchmarking. However, these systems are susceptible to adversarial attacks\nthat can manipulate evaluation outcomes, raising concerns about their\nrobustness and, consequently, their trustworthiness. Existing evaluation\nmethods adopted by LLM-based judges are often piecemeal and lack a unified\nframework for comprehensive assessment. Furthermore, prompt template and model\nselections for improving judge robustness have been rarely explored, and their\nperformance in real-world settings remains largely unverified. To address these\ngaps, we introduce RobustJudge, a fully automated and scalable framework\ndesigned to systematically evaluate the robustness of LLM-as-a-Judge systems.\nRobustJudge investigates the impact of attack methods and defense strategies\n(RQ1), explores the influence of prompt template and model selection (RQ2), and\nassesses the robustness of real-world LLM-as-a-Judge applications (RQ3).Our\nmain findings are: (1) LLM-as-a-Judge systems are still vulnerable to a range\nof adversarial attacks, including Combined Attack and PAIR, while defense\nmechanisms such as Re-tokenization and LLM-based Detectors offer improved\nprotection; (2) Robustness is highly sensitive to the choice of prompt template\nand judge models. Our proposed prompt template optimization method can improve\nrobustness, and JudgeLM-13B demonstrates strong performance as a robust\nopen-source judge; (3) Applying RobustJudge to Alibaba's PAI platform reveals\npreviously unreported vulnerabilities. The source code of RobustJudge is\nprovided at https://github.com/S3IC-Lab/RobustJudge.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.09443v1", "AI": {"title_translation": "大型语言模型（LLM）尚不能可靠地评判（？）: 对LLM作为评判者的鲁棒性进行全面评估", "tldr": "LLM-as-a-Judge系统容易受到对抗性攻击，本研究引入RobustJudge框架全面评估并提升其鲁棒性。", "motivation": "大型语言模型（LLM）作为评判者（LLM-as-a-Judge）的系统在自动化模型测试中被广泛采用，但它们易受对抗性攻击，影响评估结果的可靠性。现有评估方法零散且缺乏统一框架，对提示模板和模型选择如何提升鲁棒性的探索不足，在实际应用中的表现也未经验证。", "method": "为解决上述问题，本研究引入了RobustJudge，一个全自动化且可扩展的框架，旨在系统性地评估LLM-as-a-Judge系统的鲁棒性。RobustJudge通过三个研究问题进行探讨：调查攻击方法和防御策略的影响（RQ1），探索提示模板和模型选择的影响（RQ2），以及评估实际LLM-as-a-Judge应用的鲁棒性（RQ3）。", "result": "主要发现包括：(1) LLM-as-a-Judge系统仍易受多种对抗性攻击（如联合攻击和PAIR），但重分词（Re-tokenization）和基于LLM的检测器等防御机制能提供改进保护；(2) 鲁棒性对提示模板和评判模型选择高度敏感，本研究提出的提示模板优化方法可以提升鲁棒性，并且JudgeLM-13B作为鲁棒的开源评判者展现出强大性能；(3) 将RobustJudge应用于阿里巴巴的PAI平台，揭示了此前未报告的漏洞。", "conclusion": "LLM-as-a-Judge系统虽然强大，但其鲁棒性不足，易受对抗性攻击。为了提升其可靠性和可信度，需要系统性的评估框架（如RobustJudge）以及对提示模板和评判模型的优化选择。", "translation": "大型语言模型（LLM）在各种任务中展现出卓越的智能，这激发了LLM-as-a-Judge系统在自动化模型测试（如红队测试和基准测试）中的开发和广泛应用。然而，这些系统容易受到对抗性攻击，这些攻击可以操纵评估结果，从而引发对其鲁棒性以及随之而来的可信度的担忧。LLM评判者采用的现有评估方法通常零散且缺乏统一的框架进行全面评估。此外，用于提高评判者鲁棒性的提示模板和模型选择很少被探索，其在实际环境中的性能也基本未经验证。\n为解决这些空白，我们引入了RobustJudge，一个完全自动化且可扩展的框架，旨在系统性地评估LLM-as-a-Judge系统的鲁棒性。RobustJudge调查了攻击方法和防御策略的影响（RQ1），探索了提示模板和模型选择的影响（RQ2），并评估了实际LLM-as-a-Judge应用的鲁棒性（RQ3）。\n我们的主要发现是：(1) LLM-as-a-Judge系统仍然容易受到一系列对抗性攻击，包括联合攻击（Combined Attack）和PAIR，而重分词（Re-tokenization）和基于LLM的检测器等防御机制提供了改进的保护；(2) 鲁棒性对提示模板和评判模型的选择高度敏感。我们提出的提示模板优化方法可以提高鲁棒性，并且JudgeLM-13B作为鲁棒的开源评判者展现出强大的性能；(3) 将RobustJudge应用于阿里巴巴的PAI平台揭示了此前未报告的漏洞。RobustJudge的源代码可在https://github.com/S3IC-Lab/RobustJudge获取。", "summary": "本论文旨在解决LLM作为评判者系统（LLM-as-a-Judge）的鲁棒性问题，该系统易受对抗性攻击。为此，研究引入了RobustJudge，一个系统化、自动化的框架，用于全面评估这些系统的鲁棒性。研究发现，LLM-as-a-Judge系统确实易受多种攻击，但通过有效的防御策略以及优化提示模板和选择合适的评判模型（如JudgeLM-13B）可以显著提高其鲁棒性。此外，RobustJudge还在实际平台中发现了未报告的漏洞，验证了其在提升LLM-based评估可信度方面的实用价值。", "keywords": "LLM-as-a-Judge, 鲁棒性, 对抗性攻击, 评估框架, 提示工程", "comments": "该论文具有重要意义，因为它系统性地解决了LLM-as-a-Judge系统的一个关键局限性：易受对抗性攻击。RobustJudge框架的引入填补了先前缺乏统一鲁棒性评估方法的空白。研究中关于攻击漏洞、有效防御策略以及提示/模型选择影响的发现，对于开发更值得信赖的LLM评估工具具有极高价值。将其应用于实际平台进一步验证了其实用性。"}}
{"id": "2506.09217", "title": "Perception Characteristics Distance: Measuring Stability and Robustness of Perception System in Dynamic Conditions under a Certain Decision Rule", "authors": ["Boyu Jiang", "Liang Shi", "Zhengzhi Lin", "Loren Stowe", "Feng Guo"], "summary": "The performance of perception systems in autonomous driving systems (ADS) is\nstrongly influenced by object distance, scene dynamics, and environmental\nconditions such as weather. AI-based perception outputs are inherently\nstochastic, with variability driven by these external factors, while\ntraditional evaluation metrics remain static and event-independent, failing to\ncapture fluctuations in confidence over time. In this work, we introduce the\nPerception Characteristics Distance (PCD) -- a novel evaluation metric that\nquantifies the farthest distance at which an object can be reliably detected,\nincorporating uncertainty in model outputs. To support this, we present the\nSensorRainFall dataset, collected on the Virginia Smart Road using a\nsensor-equipped vehicle (cameras, radar, LiDAR) under controlled daylight-clear\nand daylight-rain scenarios, with precise ground-truth distances to the target\nobjects. Statistical analysis reveals the presence of change points in the\nvariance of detection confidence score with distance. By averaging the PCD\nvalues across a range of detection quality thresholds and probabilistic\nthresholds, we compute the mean PCD (mPCD), which captures the overall\nperception characteristics of a system with respect to detection distance.\nApplying state-of-the-art perception models shows that mPCD captures meaningful\nreliability differences under varying weather conditions -- differences that\nstatic metrics overlook. PCD provides a principled, distribution-aware measure\nof perception performance, supporting safer and more robust ADS operation,\nwhile the SensorRainFall dataset offers a valuable benchmark for evaluation.\nThe SensorRainFall dataset is publicly available at\nhttps://www.kaggle.com/datasets/datadrivenwheels/sensorrainfall, and the\nevaluation code is open-sourced at\nhttps://github.com/datadrivenwheels/PCD_Python.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.09217v1", "AI": {"title_translation": "感知特性距离：在特定决策规则下测量动态条件下感知系统的稳定性和鲁棒性", "tldr": "本文提出了感知特性距离（PCD）和平均感知特性距离（mPCD）作为自动驾驶系统感知性能的新评估指标，特别是在动态和不确定条件下，并引入了SensorRainFall数据集进行验证。", "motivation": "自动驾驶系统（ADS）中感知系统的性能受物体距离、场景动态和环境条件（如天气）的强烈影响。基于AI的感知输出本质上是随机的，其变异性受这些外部因素驱动，而传统的评估指标是静态且与事件无关的，无法捕捉随时间变化的置信度波动。", "method": "本文引入了感知特性距离（PCD），这是一种量化物体可以被可靠检测到的最远距离的新评估指标，该指标包含了模型输出的不确定性。为了支持这一点，本文提出了SensorRainFall数据集，该数据集使用配备传感器的车辆（摄像头、雷达、激光雷达）在受控的白天晴朗和白天降雨场景下收集，并具有精确的目标物体地面真实距离。通过对一系列检测质量阈值和概率阈值上的PCD值进行平均，计算出平均PCD（mPCD），它捕捉了系统在检测距离方面的整体感知特性。", "result": "统计分析揭示了检测置信度分数方差随距离变化存在变化点。应用最先进的感知模型表明，mPCD在不同天气条件下捕捉到了有意义的可靠性差异——这些差异是静态指标所忽略的。", "conclusion": "PCD提供了一种基于原理的、分布感知的感知性能度量，支持更安全、更鲁棒的ADS操作，而SensorRainFall数据集为评估提供了有价值的基准。", "translation": "自动驾驶系统（ADS）中感知系统的性能受到物体距离、场景动态以及天气等环境条件的强烈影响。基于AI的感知输出本质上是随机的，其变异性受这些外部因素驱动，而传统的评估指标是静态且与事件无关的，无法捕捉随时间变化的置信度波动。在这项工作中，我们引入了感知特性距离（PCD）——一种新颖的评估指标，它量化了物体可以被可靠检测到的最远距离，并包含了模型输出的不确定性。为了支持这一点，我们提出了SensorRainFall数据集，该数据集是在弗吉尼亚智能道路上使用配备传感器的车辆（摄像头、雷达、激光雷达）在受控的白天晴朗和白天降雨场景下收集的，并具有精确的目标物体地面真实距离。统计分析揭示了检测置信度分数方差随距离变化存在变化点。通过对一系列检测质量阈值和概率阈值上的PCD值进行平均，我们计算出平均PCD（mPCD），它捕捉了系统在检测距离方面的整体感知特性。应用最先进的感知模型表明，mPCD在不同天气条件下捕捉到了有意义的可靠性差异——这些差异是静态指标所忽略的。PCD提供了一种基于原理的、分布感知的感知性能度量，支持更安全、更鲁棒的ADS操作，而SensorRainFall数据集为评估提供了有价值的基准。SensorRainFall数据集可在https://www.kaggle.com/datasets/datadrivenwheels/sensorrainfall公开获取，评估代码已在https://github.com/datadrivenwheels/PCD_Python开源。", "summary": "本文提出了一种名为感知特性距离（PCD）的新型评估指标，用于量化自动驾驶系统中感知系统的稳定性和鲁棒性，尤其是在动态和不确定条件下。PCD能够捕捉模型输出的不确定性，并被进一步平均以得到平均感知特性距离（mPCD），以表征系统整体的感知特性。为了支持PCD的验证，本文引入了SensorRainFall数据集，该数据集包含在不同天气条件下收集的传感器数据和精确的地面真实距离。研究结果表明，mPCD能够有效揭示静态指标无法捕捉到的、在不同天气条件下的感知可靠性差异，从而促进更安全、更鲁棒的自动驾驶系统运行。", "keywords": "感知系统, 自动驾驶, 感知特性距离, 鲁棒性, 数据集", "comments": "本文的创新之处在于提出了PCD和mPCD这两个新的感知性能评估指标，它们克服了传统静态指标无法捕捉动态条件下置信度波动的问题，并考虑了AI感知输出的随机性。引入的SensorRainFall数据集为感知系统在不同天气条件下的评估提供了宝贵的公共资源，具有很高的实用价值。这种分布感知的度量方法对于提升自动驾驶系统的安全性和鲁棒性至关重要。"}}
{"id": "2506.09071", "title": "Segment Any Architectural Facades (SAAF):An automatic segmentation model for building facades, walls and windows based on multimodal semantics guidance", "authors": ["Peilin Li", "Jun Yin", "Jing Zhong", "Ran Luo", "Pengyu Zeng", "Miao Zhang"], "summary": "In the context of the digital development of architecture, the automatic\nsegmentation of walls and windows is a key step in improving the efficiency of\nbuilding information models and computer-aided design. This study proposes an\nautomatic segmentation model for building facade walls and windows based on\nmultimodal semantic guidance, called Segment Any Architectural Facades (SAAF).\nFirst, SAAF has a multimodal semantic collaborative feature extraction\nmechanism. By combining natural language processing technology, it can fuse the\nsemantic information in text descriptions with image features, enhancing the\nsemantic understanding of building facade components. Second, we developed an\nend-to-end training framework that enables the model to autonomously learn the\nmapping relationship from text descriptions to image segmentation, reducing the\ninfluence of manual intervention on the segmentation results and improving the\nautomation and robustness of the model. Finally, we conducted extensive\nexperiments on multiple facade datasets. The segmentation results of SAAF\noutperformed existing methods in the mIoU metric, indicating that the SAAF\nmodel can maintain high-precision segmentation ability when faced with diverse\ndatasets. Our model has made certain progress in improving the accuracy and\ngeneralization ability of the wall and window segmentation task. It is expected\nto provide a reference for the development of architectural computer vision\ntechnology and also explore new ideas and technical paths for the application\nof multimodal learning in the architectural field.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09071v1", "AI": {"title_translation": "分割任何建筑立面（SAAF）：一种基于多模态语义引导的建筑立面、墙体和窗户自动分割模型", "tldr": "SAAF是一种基于多模态语义引导的建筑立面、墙体和窗户自动分割模型，通过融合文本和图像特征，实现了高精度、高鲁棒性的分割，并在mIoU指标上超越了现有方法。", "motivation": "在建筑数字化发展背景下，墙体和窗户的自动分割是提高建筑信息模型和计算机辅助设计效率的关键步骤。", "method": "本研究提出了一种名为SAAF的建筑立面墙体和窗户自动分割模型。SAAF具有多模态语义协同特征提取机制，通过结合自然语言处理技术，融合文本描述中的语义信息与图像特征，增强对建筑立面组件的语义理解。同时，开发了一个端到端训练框架，使模型能够自主学习文本描述到图像分割的映射关系，减少人工干预，提高模型的自动化和鲁棒性。", "result": "SAAF模型在多个立面数据集上进行了广泛实验，其分割结果在mIoU指标上优于现有方法，表明SAAF模型在面对多样化数据集时能保持高精度分割能力。", "conclusion": "SAAF模型在提高墙体和窗户分割任务的准确性和泛化能力方面取得了进展，有望为建筑计算机视觉技术发展提供参考，并为多模态学习在建筑领域的应用探索新思路和技术路径。", "translation": "在建筑数字化发展的背景下，墙体和窗户的自动分割是提高建筑信息模型和计算机辅助设计效率的关键步骤。本研究提出了一种基于多模态语义引导的建筑立面墙体和窗户自动分割模型，名为Segment Any Architectural Facades（SAAF）。首先，SAAF具有多模态语义协同特征提取机制。通过结合自然语言处理技术，它可以将文本描述中的语义信息与图像特征融合，增强对建筑立面组件的语义理解。其次，我们开发了一个端到端训练框架，使模型能够自主学习从文本描述到图像分割的映射关系，减少人工干预对分割结果的影响，提高模型的自动化和鲁棒性。最后，我们在多个立面数据集上进行了广泛实验。SAAF的分割结果在mIoU指标上优于现有方法，表明SAAF模型在面对多样化数据集时能够保持高精度分割能力。我们的模型在提高墙体和窗户分割任务的准确性和泛化能力方面取得了进展。它有望为建筑计算机视觉技术的发展提供参考，并为多模态学习在建筑领域的应用探索新的思路和技术路径。", "summary": "SAAF（Segment Any Architectural Facades）是一种针对建筑立面、墙体和窗户的自动分割模型，旨在提高建筑信息模型和计算机辅助设计的效率。该模型创新性地融合了自然语言处理技术，通过多模态语义协同特征提取机制，将文本描述的语义信息与图像特征相结合，从而增强了对建筑组件的语义理解。SAAF采用端到端训练框架，实现了文本到图像分割的自主学习，显著减少了人工干预并提升了模型的自动化和鲁棒性。实验证明，SAAF在多个立面数据集上的mIoU指标表现优于现有方法，展现出高精度和良好的泛化能力，为建筑计算机视觉和多模态学习应用提供了新的方向。", "keywords": "建筑立面分割, 多模态语义, 自动分割, 墙体窗户, SAAF", "comments": "该论文提出的SAAF模型通过引入多模态语义引导，特别是结合自然语言处理技术来增强图像分割的语义理解，具有显著的创新性。这种方法不仅提高了建筑立面分割的精度和自动化程度，还为将多模态学习应用于建筑领域开辟了新的技术路径，对于提升BIM和CAD的效率具有重要意义。其端到端训练框架也保证了模型的鲁棒性。"}}
{"id": "2506.09390", "title": "Beyond Nash Equilibrium: Bounded Rationality of LLMs and humans in Strategic Decision-making", "authors": ["Kehan Zheng", "Jinfeng Zhou", "Hongning Wang"], "summary": "Large language models are increasingly used in strategic decision-making\nsettings, yet evidence shows that, like humans, they often deviate from full\nrationality. In this study, we compare LLMs and humans using experimental\nparadigms directly adapted from behavioral game-theory research. We focus on\ntwo well-studied strategic games, Rock-Paper-Scissors and the Prisoner's\nDilemma, which are well known for revealing systematic departures from rational\nplay in human subjects. By placing LLMs in identical experimental conditions,\nwe evaluate whether their behaviors exhibit the bounded rationality\ncharacteristic of humans. Our findings show that LLMs reproduce familiar human\nheuristics, such as outcome-based strategy switching and increased cooperation\nwhen future interaction is possible, but they apply these rules more rigidly\nand demonstrate weaker sensitivity to the dynamic changes in the game\nenvironment. Model-level analyses reveal distinctive architectural signatures\nin strategic behavior, and even reasoning models sometimes struggle to find\neffective strategies in adaptive situations. These results indicate that\ncurrent LLMs capture only a partial form of human-like bounded rationality and\nhighlight the need for training methods that encourage flexible opponent\nmodeling and stronger context awareness.", "comment": null, "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.09390v1", "AI": {"title_translation": "超越纳什均衡：大型语言模型和人类在战略决策中的有限理性", "tldr": "大型语言模型（LLMs）和人类在战略决策中都表现出有限理性，但LLMs在应用启发式规则时更僵化，对动态环境变化的敏感性较弱。", "motivation": "大型语言模型越来越多地应用于战略决策场景，但有证据表明它们像人类一样，经常偏离完全理性。本研究旨在比较LLMs和人类的有限理性表现。", "method": "本研究采用行为博弈论研究中直接改编的实验范式，重点关注剪刀石头布和囚徒困境这两种经典战略游戏，将LLMs置于与人类受试者相同的实验条件下进行评估。", "result": "研究发现，LLMs再现了人类熟悉的启发式行为，例如基于结果的策略转换以及在未来可能互动时增加合作，但它们应用这些规则更僵化，对游戏环境的动态变化敏感性较弱。模型层面的分析揭示了战略行为中独特的架构特征，即使是推理模型在适应性情境中也难以找到有效策略。", "conclusion": "目前的大型语言模型只捕捉到人类有限理性的一部分形式，这突出表明需要鼓励灵活的对手建模和更强的上下文感知能力的训练方法。", "translation": "大型语言模型越来越多地应用于战略决策场景，然而有证据表明，它们像人类一样，经常偏离完全理性。在本研究中，我们使用直接改编自行为博弈论研究的实验范式来比较LLMs和人类。我们重点关注两种被充分研究的战略游戏：剪刀石头布和囚徒困境，这两种游戏以揭示人类受试者系统性偏离理性行为而闻名。通过将LLMs置于相同的实验条件下，我们评估它们的行为是否表现出人类特有的有限理性。我们的发现表明，LLMs再现了人类熟悉的启发式行为，例如基于结果的策略转换以及在未来可能互动时增加合作，但它们应用这些规则更僵化，对游戏环境的动态变化敏感性较弱。模型层面的分析揭示了战略行为中独特的架构特征，甚至推理模型在适应性情境中也难以找到有效策略。这些结果表明，当前的大型语言模型只捕捉到人类有限理性的一部分形式，并强调需要鼓励灵活的对手建模和更强的上下文感知能力的训练方法。", "summary": "本研究通过行为博弈论实验（剪刀石头布和囚徒困境）比较了大型语言模型（LLMs）和人类在战略决策中的有限理性。结果显示，LLMs能复制人类的启发式行为，但在应用上更为僵化，且对环境动态变化的敏感性不足。这表明当前LLMs仅部分具备人类的有限理性，亟需改进训练方法以提升其灵活的对手建模和上下文感知能力。", "keywords": "LLMs, 有限理性, 战略决策, 博弈论, 人类启发式", "comments": "这篇论文创新性地将行为博弈论的实验范式应用于LLMs，深入探讨了LLMs在战略决策中的有限理性。其重要性在于揭示了当前LLMs在处理动态、适应性情境时的局限性，指出了LLMs在模拟人类复杂认知方面仍有提升空间，并为未来LLM的训练和发展提供了明确的方向，即增强灵活性和上下文感知能力。"}}
{"id": "2506.09085", "title": "LLM-ML Teaming: Integrated Symbolic Decoding and Gradient Search for Valid and Stable Generative Feature Transformation", "authors": ["Xinyuan Wang", "Haoyue Bai", "Nanxu Gong", "Wangyang Ying", "Sixun Dong", "Xiquan Cui", "Yanjie Fu"], "summary": "Feature transformation enhances data representation by deriving new features\nfrom the original data. Generative AI offers potential for this task, but faces\nchallenges in stable generation (consistent outputs) and valid generation\n(error-free sequences). Existing methods--traditional MLs' low validity and\nLLMs' instability--fail to resolve both. We find that LLMs ensure valid syntax,\nwhile ML's gradient-steered search stabilizes performance. To bridge this gap,\nwe propose a teaming framework combining LLMs' symbolic generation with ML's\ngradient optimization. This framework includes four steps: (1) golden examples\ngeneration, aiming to prepare high-quality samples with the ground knowledge of\nthe teacher LLM; (2) feature transformation sequence embedding and search,\nintending to uncover potentially superior embeddings within the latent space;\n(3) student LLM feature transformation, aiming to distill knowledge from the\nteacher LLM; (4) LLM-ML decoder teaming, dedicating to combine ML and the\nstudent LLM probabilities for valid and stable generation. The experiments on\nvarious datasets show that the teaming policy can achieve 5\\% improvement in\ndownstream performance while reducing nearly half of the error cases. The\nresults also demonstrate the efficiency and robustness of the teaming policy.\nAdditionally, we also have exciting findings on LLMs' capacity to understand\nthe original data.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09085v1", "AI": {"title_translation": "大型语言模型与机器学习协作：集成符号解码与梯度搜索用于有效和稳定的生成式特征转换", "tldr": "本文提出了一种结合大型语言模型（LLM）和机器学习（ML）的协作框架，旨在解决生成式特征转换中有效性和稳定性的挑战，实验结果显示在下游任务性能和错误率上均有显著提升。", "motivation": "特征转换通过从原始数据中导出新特征来增强数据表示。生成式AI在此任务中具有潜力，但面临稳定生成（输出一致性）和有效生成（无错误序列）的挑战。现有方法（传统机器学习的有效性低和大型语言模型的不稳定性）未能同时解决这两个问题。研究发现大型语言模型能确保有效语法，而机器学习的梯度引导搜索能稳定性能，因此需要弥合两者之间的差距。", "method": "本文提出了一个结合大型语言模型（LLM）符号生成与机器学习（ML）梯度优化的协作框架。该框架包括四个步骤：(1) 黄金示例生成，旨在利用教师大型语言模型的先验知识准备高质量样本；(2) 特征转换序列嵌入与搜索，旨在在潜在空间中发现潜在的优越嵌入；(3) 学生大型语言模型特征转换，旨在从教师大型语言模型中提炼知识；(4) 大型语言模型-机器学习解码器协作，致力于结合机器学习和学生大型语言模型的概率以实现有效和稳定的生成。", "result": "实验结果表明，该协作策略可以在下游性能上实现5%的提升，同时将错误案例减少近一半。结果还证明了该协作策略的效率和鲁棒性。此外，研究还发现了大型语言模型理解原始数据的能力令人兴奋的能力。", "conclusion": "通过结合大型语言模型的符号生成能力和机器学习的梯度优化能力，所提出的LLM-ML协作框架有效地解决了生成式特征转换中有效性和稳定性的挑战，显著提升了下游任务性能并减少了错误。", "translation": "特征转换通过从原始数据中导出新特征来增强数据表示。生成式AI在此任务中具有潜力，但面临稳定生成（输出一致性）和有效生成（无错误序列）的挑战。现有方法——传统机器学习的低有效性和大型语言模型的不稳定性——未能同时解决这两个问题。我们发现大型语言模型能确保有效语法，而机器学习的梯度引导搜索能稳定性能。为了弥合这一差距，我们提出了一个结合大型语言模型符号生成与机器学习梯度优化的协作框架。该框架包括四个步骤：(1) 黄金示例生成，旨在利用教师大型语言模型的先验知识准备高质量样本；(2) 特征转换序列嵌入与搜索，旨在在潜在空间中发现潜在的优越嵌入；(3) 学生大型语言模型特征转换，旨在从教师大型语言模型中提炼知识；(4) 大型语言模型-机器学习解码器协作，致力于结合机器学习和学生大型语言模型的概率以实现有效和稳定的生成。在各种数据集上的实验表明，该协作策略可以在下游性能上实现5%的提升，同时将错误案例减少近一半。结果还证明了该协作策略的效率和鲁棒性。此外，我们还在大型语言模型理解原始数据的能力方面有令人兴奋的发现。", "summary": "本文提出了一种名为“LLM-ML协作”的框架，旨在解决生成式特征转换中有效性（无错误序列）和稳定性（输出一致性）的挑战。鉴于大型语言模型（LLM）在语法有效性方面的优势以及机器学习（ML）在性能稳定性方面的能力，该框架将LLM的符号生成与ML的梯度优化相结合。其核心流程包括生成高质量的黄金示例、在潜在空间中搜索优越的特征转换序列嵌入、学生LLM的知识蒸馏，以及LLM与ML概率的联合解码。实验结果表明，该协作策略在下游任务性能上提升了5%，并将错误案例减少了近一半，同时展现出高效性和鲁棒性，并揭示了LLM理解原始数据的潜力。", "keywords": "特征转换, 生成式AI, 大型语言模型-机器学习协作, 符号解码, 梯度搜索", "comments": "这项研究的创新之处在于其将大型语言模型的符号生成能力与机器学习的梯度优化相结合，以解决生成式特征转换中长期存在的有效性和稳定性问题。通过这种独特的协作模式，该方法不仅在下游任务性能上取得了显著提升，而且大幅减少了错误率，这对于实际应用具有重要意义。该框架的模块化设计（黄金示例生成、序列嵌入与搜索、知识蒸馏和联合解码）也展现了其系统性和可扩展性。论文还提及了LLM理解原始数据的潜力，这为未来的研究提供了新的方向。"}}
{"id": "2506.09689", "title": "BF-Max: an Efficient Bit Flipping Decoder with Predictable Decoding Failure Rate", "authors": ["Alessio Baldelli", "Marco Baldi", "Franco Chiaraluce", "Paolo Santini"], "summary": "The Bit-Flipping (BF) decoder, thanks to its very low computational\ncomplexity, is widely employed in post-quantum cryptographic schemes based on\nModerate Density Parity Check codes in which, ultimately, decryption boils down\nto syndrome decoding. In such a setting, for security concerns, one must\nguarantee that the Decoding Failure Rate (DFR) is negligible. Such a condition,\nhowever, is very difficult to guarantee, because simulations are of little help\nand the decoder performance is difficult to model theoretically. In this paper,\nwe introduce a new version of the BF decoder, that we call BF-Max,\ncharacterized by the fact that in each iteration only one bit (the least\nreliable) is flipped. When the number of iterations is equal to the number of\nerrors to be corrected, we are able to develop a theoretical characterization\nof the DFR that tightly matches with numerical simulations. We also show how\nBF-Max can be implemented efficiently, achieving low complexity and making it\ninherently constant time. With our modeling, we are able to accurately predict\nvalues of DFR that are remarkably lower than those estimated by applying other\napproaches.", "comment": "5 pages plus 1 page that contains only bibliography, 2 figures", "cate": "cs.IT", "url": "http://arxiv.org/abs/2506.09689v1", "AI": {"title_translation": "BF-Max：一种具有可预测解码失败率的高效比特翻转解码器", "tldr": "提出了一种名为BF-Max的新型比特翻转解码器，它能准确预测极低的解码失败率，并且高效且恒定时间。", "motivation": "比特翻转（BF）解码器在后量子密码方案中广泛应用，但其解码失败率（DFR）难以保证，因为模拟帮助不大且理论建模困难，而安全性要求DFR可忽略不计。", "method": "引入了一种名为BF-Max的新型BF解码器，其特点是在每次迭代中只翻转一个比特（可靠性最低的比特）。当迭代次数等于要纠正的错误数量时，能够对DFR进行理论表征。", "result": "理论表征的DFR与数值模拟高度吻合。BF-Max可以高效实现，具有低复杂度和固有恒定时间特性。该模型能够准确预测比其他方法估计值显著更低的DFR。", "conclusion": "BF-Max通过准确预测极低的解码失败率，解决了BF解码器在安全应用中DFR难以保证的问题，并且实现了高效和恒定时间的解码。", "translation": "比特翻转（BF）解码器因其极低的计算复杂度，在基于中等密度奇偶校验码的后量子密码方案中被广泛采用，在这些方案中，最终的解密归结为伴随式解码。在这种背景下，出于安全考虑，必须保证解码失败率（DFR）可以忽略不计。然而，这种条件很难保证，因为模拟帮助不大，并且解码器性能很难进行理论建模。在本文中，我们引入了一种新版本的BF解码器，我们称之为BF-Max，其特点是在每次迭代中只翻转一个比特（可靠性最低的比特）。当迭代次数等于要纠正的错误数量时，我们能够开发出与数值模拟高度吻合的DFR理论表征。我们还展示了BF-Max如何高效实现，实现低复杂度和固有恒定时间。通过我们的建模，我们能够准确预测出比应用其他方法估计值显著更低的DFR值。", "summary": "本文提出了一种名为BF-Max的新型比特翻转（BF）解码器，旨在解决现有BF解码器在后量子密码方案中难以保证可忽略解码失败率（DFR）的问题。BF-Max的特点是每次迭代只翻转最不可靠的比特，并在迭代次数等于错误数量时，能够提供与数值模拟高度吻合的DFR理论表征。研究表明，BF-Max可以高效实现，具有低计算复杂度和恒定时间特性，并且其模型能够准确预测出显著低于其他方法的DFR值。", "keywords": "比特翻转解码器, 解码失败率, 后量子密码, BF-Max, 理论建模", "comments": "这篇论文的创新点在于为比特翻转解码器提供了一种可预测的解码失败率理论模型，这对于需要严格安全保证的后量子密码方案至关重要。通过每次只翻转一个比特的策略，BF-Max不仅提高了预测准确性，还保持了低复杂度，使其在实际应用中更具吸引力。"}}
{"id": "2506.09396", "title": "Reasoning as a Resource: Optimizing Fast and Slow Thinking in Code Generation Models", "authors": ["Zongjie Li", "Shuai Wang"], "summary": "This position paper proposes a fundamental shift in designing code generation\nmodels: treating reasoning depth as a controllable resource. Rather than being\nan incidental byproduct of prompting, we argue that the trade-off between\nrapid, direct answers (\"fast thinking\") and elaborate, chain-of-thought\ndeliberation (\"slow thinking\") must be explicitly managed. We contend that\noptimizing reasoning budgets across the entire model lifecycle - from synthetic\ndata creation and benchmarking to real-world deploymen - can unlock superior\ntrade-offs among accuracy, latency, and cost. This paper outlines how adaptive\ncontrol over reasoning can enrich supervision signals, motivate new\nmulti-dimensional benchmarks, and inform cost-aware, security-conscious\ndeployment policies. By viewing fast and slow thinking as complementary modes\nto be scheduled, we envision coding agents that think deep when necessary and\nact fast when possible.", "comment": null, "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.09396v1", "AI": {"title_translation": "推理即资源：优化代码生成模型中的快思考与慢思考", "tldr": "本文提出将推理深度视为一种可控资源，以优化代码生成模型在准确性、延迟和成本之间的权衡，通过自适应控制快思考和慢思考，实现更高效的编码智能体。", "motivation": "传统的代码生成模型将推理深度视为提示的附带产物，未能显式管理快思考（快速直接答案）和慢思考（链式思考推敲）之间的权衡。本文旨在提出一种根本性转变，将推理深度视为可控资源，以解锁准确性、延迟和成本之间的更优权衡。", "method": "本文提出将推理深度视为一种可控资源，并在整个模型生命周期（从合成数据创建、基准测试到实际部署）中优化推理预算。通过自适应控制推理，可以丰富监督信号、激励新的多维基准，并指导注重成本和安全的部署策略。", "result": "Not mentioned in abstract", "conclusion": "通过将快思考和慢思考视为可互补且可调度的模式，可以构建出在必要时深入思考、在可能时快速行动的编码智能体。", "translation": "这篇立场论文提出了一种设计代码生成模型的根本性转变：将推理深度视为一种可控资源。我们认为，快速、直接的答案（“快思考”）与精细、链式思考的深思熟虑（“慢思考”）之间的权衡，必须被明确管理，而非仅仅是提示的附带产物。我们主张，在整个模型生命周期——从合成数据创建和基准测试到实际部署——优化推理预算，可以解锁准确性、延迟和成本之间的卓越权衡。本文概述了对推理的自适应控制如何丰富监督信号，激励新的多维基准，并指导注重成本和安全的部署策略。通过将快思考和慢思考视为可调度的互补模式，我们设想编码智能体在必要时进行深入思考，在可能时快速行动。", "summary": "本文提出将代码生成模型中的推理深度视为一种可控资源，而非提示的副产品。作者认为，应显式管理快思考和慢思考之间的权衡，并在模型生命周期中优化推理预算，以提升准确性、降低延迟和成本。论文探讨了自适应控制推理如何改进监督、基准测试和部署策略，旨在创建能根据需求调整思考深度的智能编码代理。", "keywords": "推理深度, 代码生成模型, 快思考, 慢思考, 资源优化", "comments": "这篇立场论文提出了一个新颖且重要的视角，将模型的“思考”过程（快思考和慢思考）提升为可管理的资源，而非被动结果。这一理念对于未来构建更高效、更具成本效益且性能优越的AI模型具有指导意义，尤其是在资源受限或对实时性有高要求的场景下。它强调了在设计和部署模型时，不仅要关注准确性，还要综合考虑计算成本和延迟。"}}
{"id": "2506.09216", "title": "\"How do you even know that stuff?\": Barriers to expertise sharing among spreadsheet users", "authors": ["Qing", "Xia", "Advait Sarkar", "Duncan Brumby", "Anna Cox"], "summary": "Spreadsheet collaboration provides valuable opportunities for learning and\nexpertise sharing between colleagues. Sharing expertise is essential for the\nretention of important technical skillsets within organisations, but previous\nstudies suggest that spreadsheet experts often fail to disseminate their\nknowledge to others. We suggest that social norms and beliefs surrounding the\nvalue of spreadsheet use significantly influence user engagement in sharing\nbehaviours. To explore this, we conducted 31 semi-structured interviews with\nprofessional spreadsheet users from two separate samples. We found that\nspreadsheet providers face challenges in adapting highly personalised\nstrategies to often subjective standards and evaluating the appropriate social\ntiming of sharing. In addition, conflicted self-evaluations of one's\nspreadsheet expertise, dismissive normative beliefs about the value of this\nknowledge, and concerns about the potential disruptions associated with\ncollaboration can further deter sharing. We suggest these observations reflect\nthe challenges of long-term learning in feature-rich software designed\nprimarily with initial learnability in mind. We therefore provide implications\nfor design to navigate this tension. Overall, our findings demonstrate how the\ncomplex interaction between technology design and social dynamics can shape\ncollaborative learning behaviours in the context of feature-rich software.", "comment": "Accepted at CSCW 2025", "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.09216v1", "AI": {"title_translation": "“你怎么会知道这些？”：电子表格用户之间专业知识共享的障碍", "tldr": "本研究探讨了电子表格用户在分享专业知识时面临的障碍，发现社会规范、个人评价和对协作的担忧会阻碍知识传播。", "motivation": "尽管电子表格协作提供了学习和专业知识共享的机会，但先前的研究表明，电子表格专家往往未能传播他们的知识。本研究旨在探讨社会规范和信念如何影响用户参与知识共享行为，以理解这些障碍。", "method": "研究人员对来自两个不同样本的31名专业电子表格用户进行了半结构化访谈。", "result": "研究发现，电子表格知识提供者在调整高度个性化的策略以适应主观标准和评估共享的适当社会时机方面面临挑战。此外，对自身电子表格专业知识的矛盾自我评价、对知识价值的轻视规范性信念以及对协作可能带来干扰的担忧会进一步阻碍共享。", "conclusion": "这些观察结果反映了在主要为初始可学习性而设计的、功能丰富的软件中长期学习的挑战。研究表明技术设计和社会动态之间的复杂互动如何塑造功能丰富的软件环境中的协作学习行为，并为设计提供了启示。", "translation": "电子表格协作提供了同事之间学习和专业知识共享的宝贵机会。共享专业知识对于组织内重要技术技能的保留至关重要，但先前的研究表明，电子表格专家往往未能将他们的知识传播给他人。我们认为，围绕电子表格使用价值的社会规范和信念显著影响用户参与共享行为。为了探索这一点，我们对来自两个不同样本的专业电子表格用户进行了31次半结构化访谈。我们发现，电子表格提供者在将高度个性化的策略适应通常主观的标准以及评估共享的适当社会时机方面面临挑战。此外，对自身电子表格专业知识的矛盾自我评价、对这种知识价值的轻视规范性信念以及对协作可能带来的潜在干扰的担忧会进一步阻碍共享。我们认为这些观察结果反映了主要以初始可学习性为设计目的的功能丰富软件中长期学习的挑战。因此，我们为解决这种张力提供了设计上的启示。总的来说，我们的发现表明了技术设计和社会动态之间复杂的互动如何塑造功能丰富的软件背景下的协作学习行为。", "summary": "本研究通过对31名专业电子表格用户的半结构化访谈，探讨了电子表格用户在分享专业知识时遇到的障碍。研究发现，个人在调整分享策略、评估分享时机、自我评价以及对知识价值的规范性信念和协作干扰的担忧，都会阻碍知识传播。这些障碍反映了功能丰富的软件中长期学习的挑战，并强调了技术设计和社会动态对协作学习行为的影响，为未来的设计提供了启示。", "keywords": "电子表格, 专业知识共享, 协作, 社会规范, 学习障碍", "comments": "这项研究深入探讨了电子表格用户在知识共享中面临的复杂社会和心理障碍，而不仅仅是技术障碍。它揭示了即使在看似简单的工具中，人际互动和认知偏差如何影响组织内的知识流动。研究强调了软件设计需要考虑长期学习和协作的社会动态，而不仅仅是初始易用性，这对于设计更有效的协作工具具有重要意义。"}}
{"id": "2506.09822", "title": "Superstudent intelligence in thermodynamics", "authors": ["Rebecca Loubet", "Pascal Zittlau", "Marco Hoffmann", "Luisa Vollmer", "Sophie Fellenz", "Heike Leitte", "Fabian Jirasek", "Johannes Lenhard", "Hans Hasse"], "summary": "In this short note, we report and analyze a striking event: OpenAI's large\nlanguage model o3 has outwitted all students in a university exam on\nthermodynamics. The thermodynamics exam is a difficult hurdle for most\nstudents, where they must show that they have mastered the fundamentals of this\nimportant topic. Consequently, the failure rates are very high, A-grades are\nrare - and they are considered proof of the students' exceptional intellectual\nabilities. This is because pattern learning does not help in the exam. The\nproblems can only be solved by knowledgeably and creatively combining\nprinciples of thermodynamics. We have given our latest thermodynamics exam not\nonly to the students but also to OpenAI's most powerful reasoning model, o3,\nand have assessed the answers of o3 exactly the same way as those of the\nstudents. In zero-shot mode, the model o3 solved all problems correctly, better\nthan all students who took the exam; its overall score was in the range of the\nbest scores we have seen in more than 10,000 similar exams since 1985. This is\na turning point: machines now excel in complex tasks, usually taken as proof of\nhuman intellectual capabilities. We discuss the consequences this has for the\nwork of engineers and the education of future engineers.", "comment": "This document is the unedited Author's version of a yet to be\n  Submitted Work to Physical Review Physics Education Research. 15 pages, 2\n  figures, Graphical Abstract, Highlights and SI available (12 pages)", "cate": "cs.CE", "url": "http://arxiv.org/abs/2506.09822v1", "AI": {"title_translation": "热力学中的超学生智能", "tldr": "OpenAI的大型语言模型o3在热力学大学考试中表现出色，超越了所有学生，其分数达到历史最高水平，表明机器在复杂任务上已超越人类智能。", "motivation": "热力学考试对大多数学生来说是一个很难的障碍，失败率很高，A级成绩罕见，被认为是学生卓越智力能力的证明，因为解决问题需要创造性地结合热力学原理，而非模式学习。本研究旨在报告并分析OpenAI大型语言模型o3在热力学考试中超越所有学生的显著事件。", "method": "研究人员将最新的热力学考试同时提供给学生和OpenAI最强大的推理模型o3。在零样本模式下，模型o3的答案以与学生完全相同的方式进行评估。", "result": "模型o3在零样本模式下正确解决了所有问题，表现优于所有参加考试的学生；其总分达到了自1985年以来超过10,000次类似考试中所见的最佳分数范围。", "conclusion": "这是一个转折点：机器现在在通常被视为人类智力能力证明的复杂任务中表现出色。研究讨论了这对于工程师的工作和未来工程师的教育所产生的影响。", "translation": "在这篇短文中，我们报告并分析了一个惊人的事件：OpenAI的大型语言模型o3在一次大学热力学考试中击败了所有学生。热力学考试对大多数学生来说是一个很难的障碍，他们必须证明自己已经掌握了这个重要主题的基础知识。因此，失败率非常高，A级成绩很少见——它们被认为是学生卓越智力能力的证明。这是因为模式学习对考试没有帮助。问题只能通过知识渊博和创造性地结合热力学原理来解决。我们不仅将我们最新的热力学考试给了学生，也给了OpenAI最强大的推理模型o3，并以与评估学生答案完全相同的方式评估了o3的答案。在零样本模式下，模型o3正确解决了所有问题，比所有参加考试的学生都好；其总分达到了我们自1985年以来在超过10,000次类似考试中所见的最佳分数范围。这是一个转折点：机器现在在通常被视为人类智力能力证明的复杂任务中表现出色。我们讨论了这对于工程师的工作和未来工程师的教育所产生的影响。", "summary": "本研究报告了OpenAI的大型语言模型o3在一次大学热力学考试中超越所有学生的显著事件。热力学考试因其需要创造性应用原理而非模式学习而难度极高。研究人员将考试同时提供给学生和o3模型，并在零样本模式下评估了o3的答案。结果显示，o3模型正确解决了所有问题，表现优于所有学生，并取得了历史最高分。这一发现表明机器现在能够在传统上被视为人类智力证明的复杂任务中表现出色，对未来的工程工作和教育具有深远影响。", "keywords": "热力学, 大型语言模型, OpenAI, o3, 考试", "comments": "这篇论文揭示了大型语言模型在复杂推理和问题解决方面的惊人能力，尤其是在传统上被认为是人类独有领域的热力学等学科。其创新之处在于证明了LLM不仅能进行模式识别，还能进行深层次的知识整合和创造性应用。这对于未来工程师的教育和行业工作方式无疑是一个重要的里程碑，预示着人机协作和教育范式可能发生重大转变。论文的局限性可能在于其“短文”性质，可能缺乏对模型内部机制或更广泛数据集的深入分析。"}}
{"id": "2506.09726", "title": "Don't be Afraid of Cell Complexes! An Introduction from an Applied Perspective", "authors": ["Josef Hoppe", "Vincent P. Grande", "Michael T. Schaub"], "summary": "Cell complexes (CCs) are a higher-order network model deeply rooted in\nalgebraic topology that has gained interest in signal processing and network\nscience recently. However, while the processing of signals supported on CCs can\nbe described in terms of easily-accessible algebraic or combinatorial notions,\nthe commonly presented definition of CCs is grounded in abstract concepts from\ntopology and remains disconnected from the signal processing methods developed\nfor CCs. In this paper, we aim to bridge this gap by providing a simplified\ndefinition of CCs that is accessible to a wider audience and can be used in\npractical applications. Specifically, we first introduce a simplified notion of\nabstract regular cell complexes (ARCCs). These ARCCs only rely on notions from\nalgebra and can be shown to be equivalent to regular cell complexes for most\npractical applications. Second, using this new definition we provide an\naccessible introduction to (abstract) cell complexes from a perspective of\nnetwork science and signal processing. Furthermore, as many practical\napplications work with CCs of dimension 2 and below, we provide an even simpler\ndefinition for this case that significantly simplifies understanding and\nworking with CCs in practice.", "comment": "Preprint version, comments welcome!", "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.09726v1", "AI": {"title_translation": "不用害怕细胞复形！一个应用视角的介绍", "tldr": "本文为信号处理和网络科学领域简化了细胞复形（CCs）的定义，引入了抽象正则细胞复形（ARCCs），使其更易于理解和实际应用，特别是对于低维情况。", "motivation": "细胞复形（CCs）作为一种高阶网络模型，其传统定义过于抽象且与实际信号处理方法脱节，阻碍了其在信号处理和网络科学领域的广泛应用。本文旨在通过提供一个简化且易于理解的CCs定义来弥合这一理论与实践之间的鸿沟。", "method": "论文首先引入了抽象正则细胞复形（ARCCs）的简化概念，该概念仅基于代数，并被证明在大多数实际应用中与正则细胞复形等价。其次，利用这一新定义，论文从网络科学和信号处理的角度对（抽象）细胞复形进行了易于理解的介绍。此外，针对维度为2及以下的CCs，论文提供了一个更简化的定义。", "result": "本文成功提供了一个简化且易于理解的细胞复形（特别是ARCCs）定义，该定义以代数为基础而非抽象拓扑学，使其在信号处理和网络科学领域更具实用性，尤其适用于低维情况。", "conclusion": "本文通过提供简化且易于理解的细胞复形定义，成功弥合了其抽象拓扑学定义与信号处理和网络科学实际应用之间的差距，从而促进了其更广泛的采纳和理解。", "translation": "细胞复形（CCs）是一种植根于代数拓扑学的高阶网络模型，最近在信号处理和网络科学领域引起了兴趣。然而，虽然CCs上支持的信号处理可以用易于理解的代数或组合概念来描述，但CCs常见的定义却植根于拓扑学的抽象概念，并与为CCs开发的信号处理方法脱节。在本文中，我们旨在通过提供一个简化的CCs定义来弥合这一差距，使其更广泛的受众能够理解并在实际应用中使用。具体来说，我们首先引入了抽象正则细胞复形（ARCCs）的简化概念。这些ARCCs仅依赖于代数概念，并且在大多数实际应用中可以证明与正则细胞复形是等价的。其次，利用这个新定义，我们从网络科学和信号处理的角度对（抽象）细胞复形进行了易于理解的介绍。此外，由于许多实际应用涉及维度为2及以下的CCs，我们为这种情况提供了一个更简单的定义，这显著简化了在实践中理解和使用CCs。", "summary": "本文针对细胞复形（CCs）定义复杂性阻碍其在信号处理和网络科学中应用的问题，提出了抽象正则细胞复形（ARCCs）这一简化、基于代数的定义，该定义在实际应用中与正则CCs等价。论文从应用角度对CCs进行了易于理解的介绍，并为低维CCs提供了更简化的定义，旨在弥合抽象理论与实际应用之间的鸿沟。", "keywords": "细胞复形, 高阶网络, 代数拓扑, 信号处理, 网络科学", "comments": "这篇论文对于使细胞复形等高级拓扑概念更容易被信号处理和网络科学等应用领域所接受具有重要意义。其创新之处在于通过将CCs（ARCCs）的定义建立在代数基础上（比拓扑学更具体），并提供实用指导，特别是针对低维情况，从而简化了概念。这有望显著扩大高阶网络模型的应用范围。"}}
{"id": "2506.09195", "title": "Graph Attention-based Decentralized Actor-Critic for Dual-Objective Control of Multi-UAV Swarms", "authors": ["Haoran Peng", "Ying-Jun Angela Zhang"], "summary": "This research focuses on optimizing multi-UAV systems with dual objectives:\nmaximizing service coverage as the primary goal while extending battery\nlifetime as the secondary objective. We propose a Graph Attention-based\nDecentralized Actor-Critic (GADC) to optimize the dual objectives. The proposed\napproach leverages a graph attention network to process UAVs' limited local\nobservation and reduce the dimension of the environment states. Subsequently,\nan actor-double-critic network is developed to manage dual policies for joint\nobjective optimization. The proposed GADC uses a Kullback-Leibler (KL)\ndivergence factor to balance the tradeoff between coverage performance and\nbattery lifetime in the multi-UAV system. We assess the scalability and\nefficiency of GADC through comprehensive benchmarking against state-of-the-art\nmethods, considering both theory and experimental aspects. Extensive testing in\nboth ideal settings and NVIDIA Sionna's realistic ray tracing environment\ndemonstrates GADC's superior performance.", "comment": null, "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.09195v1", "AI": {"title_translation": "基于图注意力机制的去中心化Actor-Critic多无人机集群双目标控制", "tldr": "该研究提出了一种基于图注意力的去中心化Actor-Critic（GADC）方法，用于优化多无人机系统在最大化服务覆盖和延长电池寿命方面的双目标控制，并在理论和实验中表现出优越性能。", "motivation": "优化多无人机系统，实现服务覆盖最大化和电池寿命延长这两个双目标。", "method": "提出了一种基于图注意力机制的去中心化Actor-Critic (GADC) 方法。该方法利用图注意力网络处理无人机的有限局部观测并降低环境状态维度。随后，开发了一个Actor-Double-Critic网络来管理双策略以进行联合目标优化。GADC使用Kullback-Leibler (KL) 散度因子来平衡覆盖性能和电池寿命之间的权衡。", "result": "GADC在理论和实验方面，通过与最新方法的综合基准测试，评估了其可扩展性和效率。在理想设置和NVIDIA Sionna的真实光线追踪环境中进行的广泛测试表明GADC具有卓越的性能。", "conclusion": "GADC能够有效地优化多无人机系统的双目标控制，并在复杂环境中表现出优越的性能和可扩展性。", "translation": "本研究侧重于优化具有双目标的多无人机系统：以最大化服务覆盖作为主要目标，同时延长电池寿命作为次要目标。我们提出了一种基于图注意力机制的去中心化Actor-Critic（GADC）来优化双目标。所提出的方法利用图注意力网络处理无人机的有限局部观测并降低环境状态的维度。随后，开发了一个Actor-Double-Critic网络来管理用于联合目标优化的双策略。所提出的GADC使用Kullback-Leibler（KL）散度因子来平衡多无人机系统中覆盖性能和电池寿命之间的权衡。我们通过与最先进方法的综合基准测试，同时考虑理论和实验方面，评估了GADC的可扩展性和效率。在理想设置和NVIDIA Sionna的真实光线追踪环境中进行的广泛测试表明GADC具有卓越的性能。", "summary": "本研究提出了一种名为GADC的基于图注意力的去中心化Actor-Critic方法，旨在解决多无人机系统中的双目标优化问题，即最大化服务覆盖和延长电池寿命。GADC利用图注意力网络处理局部观测并降低状态维度，并通过Actor-Double-Critic网络管理双策略以实现联合优化。该方法引入KL散度因子以平衡两个目标间的权衡。实验结果表明，GADC在可扩展性和效率方面均优于现有方法，并在不同环境下表现出卓越性能。", "keywords": "多无人机系统, 图注意力网络, 去中心化Actor-Critic, 双目标优化, 目标优化, 优化", "comments": "该论文创新性地将图注意力网络与去中心化Actor-Critic框架结合，有效解决了多无人机系统中的双目标优化问题，特别是在处理有限局部观测和降低状态维度方面具有优势。引入KL散度因子来平衡多目标间的权衡，提升了方法的实用性。在真实环境中的测试进一步验证了其鲁棒性和性能。"}}
{"id": "2506.09388", "title": "Integer-Clustering Optimization of Hydrogen and Battery EV Fleets Considering DERs", "authors": ["Sijia Geng", "Thomas Lee", "Dharik Mallapragada", "Audun Botterud"], "summary": "Electrified transportation leads to a tighter integration between\ntransportation and energy distribution systems. In this work, we develop\nscalable optimization models to co-design hydrogen and battery electric vehicle\n(EV) fleets, distributed energy resources, and fast-charging and\nhydrogen-fueling infrastructure to efficiently meet transportation demands. A\nnovel integer-clustering formulation is used for optimizing fleet-level EV\noperation while maintaining accurate individual vehicle dispatch, which\nsignificantly improves the computation efficiency with guaranteed performance.\nWe apply the optimization model to Boston's public transit bus network using\nreal geospatial data and cost parameters. Realistic insights are provided into\nthe future evolution of coupled electricity-transportation-hydrogen systems,\nincluding the effects of electricity price structure, hydrogen fuel cost,\ncarbon emission constraint, temperature effects on EV range, and distribution\nsystem upgrade cost.", "comment": "10 pages, 9 figures", "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.09388v1", "AI": {"title_translation": "考虑分布式能源的氢燃料和电池电动汽车车队整数聚类优化", "tldr": "该研究开发了可扩展的优化模型，利用新颖的整数聚类公式，共同设计氢燃料和电池电动汽车车队、分布式能源以及充电基础设施，以高效满足交通需求，并应用于波士顿公交网络进行案例研究。", "motivation": "电气化交通导致交通和能源分配系统之间更紧密的整合。本工作旨在开发可扩展的优化模型，以高效满足交通需求，并共同设计氢燃料和电池电动汽车车队、分布式能源以及快速充电和加氢基础设施。", "method": "开发了可扩展的优化模型，用于共同设计氢燃料和电池电动汽车车队、分布式能源以及快速充电和加氢基础设施。采用了一种新颖的整数聚类公式来优化车队级别的电动汽车运营，同时保持准确的单个车辆调度，显著提高了计算效率并保证了性能。该优化模型应用于波士顿的公共交通巴士网络，使用了真实的地理空间数据和成本参数。", "result": "新颖的整数聚类公式显著提高了计算效率并保证了性能。研究为耦合的电力-交通-氢能系统的未来演变提供了实际见解，包括电价结构、氢燃料成本、碳排放限制、温度对电动汽车续航里程的影响以及配电系统升级成本的影响。", "conclusion": "该模型为理解耦合的电力-交通-氢能系统的未来演变提供了有价值的见解，揭示了各种经济和环境因素的影响。", "translation": "电气化交通导致交通和能源分配系统之间更紧密的整合。在这项工作中，我们开发了可扩展的优化模型，以共同设计氢燃料和电池电动汽车（EV）车队、分布式能源以及快速充电和加氢基础设施，从而高效满足交通需求。我们采用了一种新颖的整数聚类公式来优化车队级别的电动汽车运营，同时保持准确的单个车辆调度，这显著提高了计算效率并保证了性能。我们将该优化模型应用于波士顿的公共交通巴士网络，使用了真实的地理空间数据和成本参数。研究为耦合的电力-交通-氢能系统的未来演变提供了实际见解，包括电价结构、氢燃料成本、碳排放限制、温度对电动汽车续航里程的影响以及配电系统升级成本。", "summary": "本研究针对电气化交通中交通与能源系统紧密融合的趋势，开发了可扩展的优化模型。该模型采用新颖的整数聚类公式，旨在高效共同设计氢燃料和电池电动汽车车队、分布式能源以及配套的充电/加氢基础设施。该方法在保证性能的同时显著提升了计算效率，并通过在波士顿公交网络的实际应用，揭示了电价、氢燃料成本、碳排放、温度及配电系统升级成本等因素对未来耦合系统演变的影响。", "keywords": "整数聚类, 电动汽车车队, 氢燃料, 分布式能源, 优化", "comments": "该论文的创新点在于提出了新颖的整数聚类公式，解决了车队级电动汽车运营优化中的计算效率问题，同时保持了单个车辆调度的准确性。其重要性在于为未来交通、能源和氢能系统的整合提供了全面的优化框架和实际见解，对于推动可持续交通发展具有指导意义。"}}
{"id": "2506.09268", "title": "A Multi-Armed Bandit Framework for Online Optimisation in Green Integrated Terrestrial and Non-Terrestrial Networks", "authors": ["Henri Alam", "Antonio de Domenico", "Tareq Si Salem", "Florian Kaltenberger"], "summary": "Integrated terrestrial and non-terrestrial network (TN-NTN) architectures\noffer a promising solution for expanding coverage and improving capacity for\nthe network. While non-terrestrial networks (NTNs) are primarily exploited for\nthese specific reasons, their role in alleviating terrestrial network (TN) load\nand enabling energy-efficient operation has received comparatively less\nattention. In light of growing concerns associated with the densification of\nterrestrial deployments, this work aims to explore the potential of NTNs in\nsupporting a more sustainable network. In this paper, we propose a novel online\noptimisation framework for integrated TN-NTN architectures, built on a\nmulti-armed bandit (MAB) formulation and leveraging the Bandit-feedback\nConstrained Online Mirror Descent (BCOMD) algorithm. Our approach adaptively\noptimises key system parameters--including bandwidth allocation, user equipment\n(UE) association, and macro base station (MBS) shutdown--to balance network\ncapacity and energy efficiency in real time. Extensive system-level simulations\nover a 24-hour period show that our framework significantly reduces the\nproportion of unsatisfied UEs during peak hours and achieves up to 19%\nthroughput gains and 5% energy savings in low-traffic periods, outperforming\nstandard network settings following 3GPP recommendations.", "comment": "To be published in 2025 IEEE International Workshop on Signal\n  Processing and Artificial Intelligence in Wireless Communications (IEEE SPAWC\n  2025)", "cate": "cs.NI", "url": "http://arxiv.org/abs/2506.09268v1", "AI": {"title_translation": "用于绿色综合地面和非地面网络在线优化的多臂老虎机框架", "tldr": "本文提出了一种基于多臂老虎机（MAB）和BCOMD算法的在线优化框架，用于综合地面和非地面网络（TN-NTN），以实时平衡网络容量和能源效率，并在模拟中显示出显著的性能提升。", "motivation": "尽管非地面网络（NTNs）主要用于扩展覆盖和提高容量，但其在减轻地面网络（TN）负载和实现节能运行方面的作用受到的关注相对较少。鉴于地面部署日益密集带来的担忧，本研究旨在探索NTNs在支持更可持续网络方面的潜力。", "method": "本文提出了一种新颖的在线优化框架，用于综合TN-NTN架构。该框架基于多臂老虎机（MAB）公式，并利用了Bandit-feedback Constrained Online Mirror Descent (BCOMD) 算法。该方法自适应地优化关键系统参数，包括带宽分配、用户设备（UE）关联和宏基站（MBS）关闭，以实时平衡网络容量和能源效率。", "result": "系统级模拟结果表明，该框架在高峰时段显著降低了未满足UE的比例，并在低流量时期实现了高达19%的吞吐量增益和5%的能源节省，优于遵循3GPP建议的标准网络设置。", "conclusion": "该研究提出的基于多臂老虎机和BCOMD算法的在线优化框架，能够有效平衡综合地面和非地面网络的容量与能源效率，显著改善网络性能，并支持更可持续的网络运行。", "translation": "综合地面和非地面网络（TN-NTN）架构为扩展网络覆盖和提高容量提供了有前景的解决方案。虽然非地面网络（NTNs）主要因这些特定原因而被利用，但其在减轻地面网络（TN）负载和实现节能运行方面的作用受到的关注相对较少。鉴于地面部署日益密集带来的担忧，本工作旨在探索NTNs在支持更可持续网络方面的潜力。在本文中，我们提出了一种新颖的在线优化框架，用于综合TN-NTN架构，该框架建立在多臂老虎机（MAB）公式之上，并利用了Bandit-feedback Constrained Online Mirror Descent (BCOMD) 算法。我们的方法自适应地优化关键系统参数——包括带宽分配、用户设备（UE）关联和宏基站（MBS）关闭——以实时平衡网络容量和能源效率。对24小时周期进行的广泛系统级模拟表明，我们的框架在高峰时段显著降低了未满足UE的比例，并在低流量时期实现了高达19%的吞吐量增益和5%的能源节省，优于遵循3GPP建议的标准网络设置。", "summary": "本研究提出了一种基于多臂老虎机（MAB）和Bandit-feedback Constrained Online Mirror Descent (BCOMD) 算法的在线优化框架，用于绿色综合地面和非地面网络（TN-NTN）。该框架旨在通过自适应优化带宽分配、UE关联和MBS关闭等关键系统参数，以实时平衡网络容量和能源效率。模拟结果表明，与标准网络设置相比，该框架在高峰时段显著减少了未满足的用户数量，并在低流量时段实现了高达19%的吞吐量增益和5%的能源节省，证明了其在构建可持续网络方面的潜力。", "keywords": "多臂老虎机, 在线优化, 绿色网络, 地面-非地面网络, 能源效率", "comments": "该论文创新性地将多臂老虎机框架和BCOMD算法应用于综合地面与非地面网络的在线优化问题，以实现容量与能效的平衡。这种方法对于解决日益增长的地面网络部署密度问题，并推动绿色通信具有重要意义。其自适应优化关键参数的能力是该框架的亮点，且仿真结果也验证了其有效性。"}}
{"id": "2506.09198", "title": "Low-Level and NUMA-Aware Optimization for High-Performance Quantum Simulation", "authors": ["Ali Rezaei", "Luc Jaulmes", "Maria Bahna", "Oliver Thomson Brown", "Antonio Barbalace"], "summary": "Scalable classical simulation of quantum circuits is crucial for advancing\nboth quantum algorithm development and hardware validation. In this work, we\nfocus on performance enhancements through meticulous low-level tuning on a\nsingle-node system, thereby not only advancing the performance of classical\nquantum simulations but also laying the groundwork for scalable, heterogeneous\nimplementations that may eventually bridge the gap toward noiseless quantum\ncomputing. Although similar efforts in low-level tuning have been reported in\nthe literature, such implementations have not been released as open-source\nsoftware, thereby impeding independent evaluation and further development. We\nintroduce an open-source, high-performance extension to the QuEST simulator\nthat brings state-of-the-art low-level and NUMA optimizations to modern\ncomputers. Our approach emphasizes locality-aware computation and incorporates\nhardware-specific optimizations such as NUMA-aware memory allocation, thread\npinning, AVX-512 vectorization, aggressive loop unrolling, and explicit memory\nprefetching. Experiments demonstrate significant speedups - 5.5-6.5x for\nsingle-qubit gate operations, 4.5x for two-qubit gates, 4x for Random Quantum\nCircuits (RQC), and 1.8x for Quantum Fourier Transform (QFT), demonstrating\nthat rigorous performance tuning can substantially extend the practical\nsimulation capacity of classical quantum simulators on current hardware.", "comment": "12 pages, 9 figures, 2 tables, 2 pseudocodes", "cate": "quant-ph", "url": "http://arxiv.org/abs/2506.09198v1", "AI": {"title_translation": "高性能量子模拟的底层和NUMA感知优化", "tldr": "通过底层和NUMA优化，显著提升了单节点量子模拟器的性能，并发布了开源扩展。", "motivation": "可扩展的经典量子电路模拟对于量子算法开发和硬件验证至关重要。现有文献中报告的类似底层优化实现未开源，阻碍了独立评估和进一步开发。", "method": "引入QuEST模拟器的开源高性能扩展，强调局部性感知计算，并结合硬件特定优化，如NUMA感知内存分配、线程绑定、AVX-512向量化、激进循环展开和显式内存预取。", "result": "实验表明，单量子比特门操作加速5.5-6.5倍，双量子比特门加速4.5倍，随机量子电路（RQC）加速4倍，量子傅里叶变换（QFT）加速1.8倍。", "conclusion": "严格的性能调优可以显著扩展经典量子模拟器在当前硬件上的实际模拟能力。", "translation": "可扩展的量子电路经典模拟对于量子算法开发和硬件验证都至关重要。在这项工作中，我们专注于通过在单节点系统上进行细致的底层调优来提升性能，从而不仅提高了经典量子模拟的性能，而且为可扩展的异构实现奠定了基础，这些实现最终可能弥合与无噪声量子计算之间的差距。尽管文献中报告了类似的底层调优工作，但此类实现尚未作为开源软件发布，从而阻碍了独立评估和进一步开发。我们引入了QuEST模拟器的一个开源高性能扩展，为现代计算机带来了最先进的底层和NUMA优化。我们的方法强调局部性感知计算，并结合了硬件特定的优化，如NUMA感知内存分配、线程绑定、AVX-512向量化、激进循环展开和显式内存预取。实验表明，单量子比特门操作加速5.5-6.5倍，双量子比特门加速4.5倍，随机量子电路（RQC）加速4倍，量子傅里叶变换（QFT）加速1.8倍，这表明严格的性能调优可以显著扩展经典量子模拟器在当前硬件上的实际模拟能力。", "summary": "本文介绍了一种针对单节点经典量子模拟器QuEST的开源高性能扩展，通过细致的底层和NUMA感知优化，显著提升了模拟性能。该方法强调局部性感知计算，并结合了NUMA内存分配、线程绑定、AVX-512向量化、循环展开和内存预取等硬件优化。实验结果表明，与现有方法相比，各种量子门和电路的模拟速度均有显著提升，证明了严格性能调优在扩展经典量子模拟能力方面的重要性。", "keywords": "量子模拟, 性能优化, NUMA, QuEST, 低层优化", "comments": "这项工作通过发布开源代码，解决了现有低层优化方案缺乏独立评估和进一步开发的问题，具有重要意义。其详细的硬件特定优化策略为提升经典量子模拟性能提供了宝贵的实践经验，并为未来可扩展异构实现奠定了基础。"}}
{"id": "2506.09521", "title": "You Are What You Say: Exploiting Linguistic Content for VoicePrivacy Attacks", "authors": ["Ünal Ege Gaznepoglu", "Anna Leschanowsky", "Ahmad Aloradi", "Prachi Singh", "Daniel Tenbrinck", "Emanuël A. P. Habets", "Nils Peters"], "summary": "Speaker anonymization systems hide the identity of speakers while preserving\nother information such as linguistic content and emotions. To evaluate their\nprivacy benefits, attacks in the form of automatic speaker verification (ASV)\nsystems are employed. In this study, we assess the impact of intra-speaker\nlinguistic content similarity in the attacker training and evaluation datasets,\nby adapting BERT, a language model, as an ASV system. On the VoicePrivacy\nAttacker Challenge datasets, our method achieves a mean equal error rate (EER)\nof 35%, with certain speakers attaining EERs as low as 2%, based solely on the\ntextual content of their utterances. Our explainability study reveals that the\nsystem decisions are linked to semantically similar keywords within utterances,\nstemming from how LibriSpeech is curated. Our study suggests reworking the\nVoicePrivacy datasets to ensure a fair and unbiased evaluation and challenge\nthe reliance on global EER for privacy evaluations.", "comment": "5 pages, 6 figures, 1 table, accepted at INTERSPEECH 2025", "cate": "eess.AS", "url": "http://arxiv.org/abs/2506.09521v1", "AI": {"title_translation": "言语即身份：利用语言内容进行语音隐私攻击", "tldr": "本研究评估了语音隐私攻击中，攻击者训练和评估数据集中说话人内部语言内容相似性的影响，通过将BERT模型应用于自动说话人验证系统，发现即使仅基于文本内容，也能实现较低的等错误率，并指出当前数据集和评估方法存在问题。", "motivation": "为了评估说话人匿名化系统的隐私效益，研究人员使用自动说话人验证（ASV）系统进行攻击。本研究的动机是评估说话人内部语言内容相似性在攻击者训练和评估数据集中对隐私攻击的影响。", "method": "本研究通过将语言模型BERT改造为自动说话人验证（ASV）系统来评估攻击效果。该方法在VoicePrivacy Attacker Challenge数据集上进行，并分析了系统决策与语义相似关键词的关联。", "result": "该方法在VoicePrivacy Attacker Challenge数据集上实现了35%的平均等错误率（EER），某些说话人仅基于其话语的文本内容就能达到低至2%的EER。可解释性研究表明，系统决策与话语中语义相似的关键词有关，这源于LibriSpeech数据集的策展方式。", "conclusion": "研究表明，需要重新设计VoicePrivacy数据集以确保公平无偏的评估，并挑战了在隐私评估中对全局等错误率（EER）的依赖。", "translation": "说话人匿名化系统在隐藏说话人身份的同时，保留了语言内容和情感等其他信息。为了评估其隐私效益，研究人员采用了自动说话人验证（ASV）系统形式的攻击。在本研究中，我们通过将语言模型BERT改造为ASV系统，评估了攻击者训练和评估数据集中说话人内部语言内容相似性的影响。在VoicePrivacy Attacker Challenge数据集上，我们的方法实现了35%的平均等错误率（EER），某些说话人仅基于其话语的文本内容就能达到低至2%的EER。我们的可解释性研究表明，系统决策与话语中语义相似的关键词有关，这源于LibriSpeech的策展方式。我们的研究建议重新设计VoicePrivacy数据集，以确保公平无偏的评估，并挑战了在隐私评估中对全局EER的依赖。", "summary": "本研究探讨了在语音隐私攻击中，攻击者训练和评估数据集中说话人内部语言内容相似性的影响。研究人员将BERT语言模型适配为自动说话人验证（ASV）系统，并在VoicePrivacy Attacker Challenge数据集上进行了测试。结果显示，仅基于文本内容，该系统就能达到35%的平均EER，甚至对某些说话人能达到2%的低EER。研究还发现系统决策与语义相似的关键词相关。鉴于此，论文建议重新审视VoicePrivacy数据集的构建，并质疑仅依赖全局EER进行隐私评估的合理性。", "keywords": "语音隐私, 语言内容, 自动说话人验证, BERT, 等错误率", "comments": "本文创新性地指出，即使在语音匿名化场景下，纯粹的语言内容也能被利用进行说话人身份识别攻击，这对于语音隐私保护领域是一个重要的警示。它揭示了现有数据集（如LibriSpeech）可能存在的偏见，以及当前隐私评估指标（全局EER）的局限性，对未来的研究和数据集设计提出了重要建议。"}}
{"id": "2506.09242", "title": "Multi-GPU Acceleration of PALABOS Fluid Solver using C++ Standard Parallelism", "authors": ["Jonas Latt", "Christophe Coreixas"], "summary": "This article presents the principles, software architecture, and performance\nanalysis of the GPU port of the lattice Boltzmann software library Palabos (J.\nLatt et al., \"Palabos: Parallel lattice Boltzmann solver\", Comput. Math. Appl.\n81, 334-350, (2021)). A hybrid CPU-GPU execution model is adopted, in which\nnumerical components are selectively assigned to either the CPU or the GPU,\ndepending on considerations of performance or convenience. This design enables\na progressive porting strategy, allowing most features of the original\nCPU-based codebase to be gradually and seamlessly adapted to GPU execution. The\nnew architecture builds upon two complementary paradigms: a classical\nobject-oriented structure for CPU execution, and a data-oriented counterpart\nfor GPUs, which reproduces the modularity of the original code while\neliminating object-oriented overhead detrimental to GPU performance. Central to\nthis approach is the use of modern C++, including standard parallel algorithms\nand template metaprogramming techniques, which permit the generation of\nhardware-agnostic computational kernels. This facilitates the development of\nuser-defined, GPU-accelerated components such as collision operators or\nboundary conditions, while preserving compatibility with the existing codebase\nand avoiding the need for external libraries or non-standard language\nextensions. The correctness and performance of the GPU-enabled Palabos are\ndemonstrated through a series of three-dimensional multiphysics benchmarks,\nincluding the laminar-turbulent transition in a Taylor-Green vortex, lid-driven\ncavity flow, and pore-scale flow in Berea sandstone. Despite the high-level\nabstraction of the implementation, the single-GPU performance is similar to\nCUDA-native solvers, and multi-GPU tests exhibit good weak and strong scaling\nacross all test cases.", "comment": null, "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.09242v1", "AI": {"title_translation": "使用C++标准并行性对PALABOS流体求解器进行多GPU加速", "tldr": "本文介绍了使用C++标准并行性，将Lattice Boltzmann软件库Palabos移植到GPU，并展示了其在单GPU上与CUDA原生求解器相似的性能，以及在多GPU上的良好扩展性。", "motivation": "本文旨在将Lattice Boltzmann软件库Palabos移植到GPU以利用GPU加速进行流体模拟，同时保持兼容性并避免使用外部库。其动机是提高性能并实现现有基于CPU的功能的渐进式移植。", "method": "文章采用了混合CPU-GPU执行模型，根据性能或便利性选择性地将数值组件分配给CPU或GPU。设计基于两种互补范式：CPU执行的经典面向对象结构和GPU的面向数据对应结构。核心方法是使用现代C++，包括标准并行算法和模板元编程技术，以生成硬件无关的计算内核，从而促进用户定义GPU加速组件的开发，同时保持与现有代码库的兼容性，并避免对外部库或非标准语言扩展的需求。", "result": "GPU加速的Palabos的正确性和性能通过一系列三维多物理场基准测试得到验证，包括Taylor-Green涡流中的层流-湍流过渡、盖驱动腔流和Berea砂岩中的孔隙尺度流。尽管实现具有高层次抽象，但单GPU性能与CUDA原生求解器相似，并且多GPU测试在所有测试用例中都表现出良好的弱扩展性和强扩展性。", "conclusion": "本文总结，所提出的Palabos的GPU移植，利用C++标准并行性和混合架构，成功实现了与CUDA原生求解器相当的高性能和良好的多GPU扩展性，同时保持了兼容性并避免了外部依赖。", "translation": "本文介绍了Lattice Boltzmann软件库Palabos（J. Latt et al., \"Palabos: Parallel lattice Boltzmann solver\", Comput. Math. Appl. 81, 334-350, (2021)）的GPU移植的原理、软件架构和性能分析。采用了一种混合CPU-GPU执行模型，其中数值组件根据性能或便利性的考虑，选择性地分配给CPU或GPU。这种设计实现了渐进式移植策略，允许原始基于CPU的代码库的大多数功能逐渐且无缝地适应GPU执行。新架构建立在两种互补范式之上：用于CPU执行的经典面向对象结构，以及用于GPU的面向数据对应结构，后者在消除对GPU性能不利的面向对象开销的同时，再现了原始代码的模块化。这种方法的核心是使用现代C++，包括标准并行算法和模板元编程技术，这允许生成硬件无关的计算内核。这促进了用户定义GPU加速组件（如碰撞算子或边界条件）的开发，同时保留了与现有代码库的兼容性，并避免了对外部库或非标准语言扩展的需求。通过一系列三维多物理场基准测试，包括Taylor-Green涡流中的层流-湍流过渡、盖驱动腔流和Berea砂岩中的孔隙尺度流，证明了启用GPU的Palabos的正确性和性能。尽管实现具有高层次抽象，但单GPU性能与CUDA原生求解器相似，并且多GPU测试在所有测试用例中都表现出良好的弱扩展性和强扩展性。", "summary": "本文详细介绍了Lattice Boltzmann软件库Palabos的GPU移植，该移植采用了混合CPU-GPU执行模型，并结合了面向对象和面向数据的编程范式。通过利用现代C++标准并行算法和模板元编程技术，该方法实现了硬件无关的计算内核，便于用户自定义GPU加速组件，同时保持与现有代码库的兼容性。性能测试表明，在三维多物理场基准测试中，其单GPU性能与CUDA原生求解器相当，多GPU测试也展现出良好的弱扩展性和强扩展性。", "keywords": "多GPU, Palabos, C++标准并行性, Lattice Boltzmann, 流体求解器", "comments": "该论文具有创新性，因为它展示了一种实用且有效的方法，使用标准C++特性将复杂的科学软件（如Palabos）移植到GPU，避免了供应商特定的API（如CUDA），同时实现了可比的性能。这种方法增强了可移植性和可维护性，这在科学计算中是一个显著的优势。"}}
{"id": "2506.09181", "title": "Energy efficiency of DMAs vs. conventional MIMO: a sensitivity analysis", "authors": ["Pablo Ramírez-Espinosa", "David Morales-Jiménez", "Beatriz Soret"], "summary": "Motivated by the stringent and challenging need for `greener communications'\nin increasingly power-hungry 5G networks, this paper presents a detailed energy\nefficiency analysis for three different multi-antenna architectures, namely\nfully-digital arrays, hybrid arrays, and dynamic metasurface antennas (DMAs).\nBy leveraging a circuital model, which captures mutual coupling, insertion\nlosses, propagation through the waveguides in DMAs and other electromagnetic\nphenomena, we design a transmit Wiener filter solution for the three systems.\nWe then use these results to analyze the energy efficiency, considering\ndifferent consumption models and supplied power, and with particular focus on\nthe impact of the physical phenomena. DMAs emerge as an efficient alternative\nto classical arrays across diverse tested scenarios, most notably under low\ntransmission power, strong coupling, and scalability requirements.", "comment": null, "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.09181v1", "AI": {"title_translation": "DMA与传统MIMO的能效对比：敏感性分析", "tldr": "本文分析了DMA、全数字和混合阵列的能效，发现DMA在低发射功率和强耦合下表现出高效率。", "motivation": "鉴于功耗日益增长的5G网络对“绿色通信”提出了严格且具有挑战性的需求。", "method": "利用包含互耦、插入损耗和DMA波导传播等电磁现象的电路模型，为三种系统设计了发送维纳滤波器解决方案，然后分析了不同功耗模型和供电下的能效，并特别关注物理现象的影响。", "result": "在各种测试场景中，DMA被证明是传统阵列的有效替代方案，尤其是在低发射功率、强耦合和可扩展性要求下。", "conclusion": "DMA在多种测试场景下，尤其是在低发射功率、强耦合和可扩展性要求下，是传统阵列的有效能效替代方案。", "translation": "鉴于功耗日益增长的5G网络对“绿色通信”提出了严格且具有挑战性的需求，本文详细分析了三种不同的多天线架构的能效：全数字阵列、混合阵列和动态超表面天线（DMA）。通过利用一个捕获互耦、插入损耗、DMA中波导传播及其他电磁现象的电路模型，我们为这三种系统设计了发送维纳滤波器解决方案。然后，我们利用这些结果分析能效，考虑了不同的功耗模型和供电，并特别关注物理现象的影响。在各种测试场景中，DMA被证明是传统阵列的有效替代方案，尤其是在低发射功率、强耦合和可扩展性要求下。", "summary": "本文旨在解决5G网络中对“绿色通信”的迫切需求，详细分析了全数字阵列、混合阵列和动态超表面天线（DMA）这三种多天线架构的能效。研究通过构建一个考虑互耦、插入损耗和波导传播等电磁现象的电路模型，并设计发送维纳滤波器解决方案，进而分析了不同功耗模型和供电下的能效，并强调了物理现象的影响。结果表明，在多种测试场景下，特别是低发射功率、强耦合和可扩展性要求下，DMA是传统阵列的一种高效替代方案。", "keywords": "能效, DMA, MIMO, 5G网络, 电路模型", "comments": "该论文解决了5G网络中能耗的关键问题，并将DMA作为一种有前景的解决方案。利用详细的电路模型来捕捉物理现象是其一大优势。研究结果突出了DMA在特定条件下的独特优势。"}}
{"id": "2506.09095", "title": "Foundation Models in Medical Imaging -- A Review and Outlook", "authors": ["Vivien van Veldhuizen", "Vanessa Botha", "Chunyao Lu", "Melis Erdal Cesur", "Kevin Groot Lipman", "Edwin D. de Jong", "Hugo Horlings", "Clárisa Sanchez", "Cees Snoek", "Ritse Mann", "Eric Marcus", "Jonas Teuwen"], "summary": "Foundation models (FMs) are changing the way medical images are analyzed by\nlearning from large collections of unlabeled data. Instead of relying on\nmanually annotated examples, FMs are pre-trained to learn general-purpose\nvisual features that can later be adapted to specific clinical tasks with\nlittle additional supervision. In this review, we examine how FMs are being\ndeveloped and applied in pathology, radiology, and ophthalmology, drawing on\nevidence from over 150 studies. We explain the core components of FM pipelines,\nincluding model architectures, self-supervised learning methods, and strategies\nfor downstream adaptation. We also review how FMs are being used in each\nimaging domain and compare design choices across applications. Finally, we\ndiscuss key challenges and open questions to guide future research.", "comment": null, "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.09095v1", "AI": {"title_translation": "医疗影像中的基础模型——综述与展望", "tldr": "本文综述了基础模型在医疗影像分析中的发展和应用，涵盖其核心组件、在不同影像领域的应用以及未来的挑战。", "motivation": "基础模型正在改变医疗影像分析方式，通过学习大量未标注数据来克服对人工标注的依赖，因此有必要对它们在医疗影像领域的开发和应用进行系统性回顾。", "method": "本文通过审查150多项研究，综述了基础模型在病理学、放射学和眼科学中的开发和应用。具体解释了基础模型管道的核心组成部分，包括模型架构、自监督学习方法和下游适应策略，并比较了不同应用中的设计选择。", "result": "综述展示了基础模型在病理学、放射学和眼科学等医疗影像领域的开发和应用情况，阐明了基础模型管道的核心组成部分，并比较了不同影像领域中基础模型的设计选择和使用方式。", "conclusion": "基础模型在医疗影像领域面临关键挑战和开放问题，这些问题将指导未来的研究方向。", "translation": "基础模型（FMs）通过从未标注的大量数据中学习，正在改变医疗图像的分析方式。它们不再依赖手动标注的示例，而是通过预训练来学习通用视觉特征，这些特征随后可以以很少的额外监督适应特定的临床任务。在本综述中，我们借鉴了150多项研究的证据，审视了基础模型在病理学、放射学和眼科学中的开发和应用。我们解释了基础模型管道的核心组件，包括模型架构、自监督学习方法和下游适应策略。我们还回顾了基础模型在每个成像领域的使用方式，并比较了跨应用的设计选择。最后，我们讨论了关键挑战和开放问题，以指导未来的研究。", "summary": "本文对基础模型在医疗影像分析领域的应用进行了全面综述，探讨了它们如何通过从未标注数据中学习来改变传统分析方式。综述涵盖了基础模型在病理学、放射学和眼科学中的发展与应用，详细阐述了其核心组件、学习方法和适应策略，并比较了不同领域的设计选择。文章最后指出了当前面临的挑战和未来的研究方向。", "keywords": "基础模型, 医疗影像, 综述, 自监督学习, 临床任务", "comments": "这篇综述的重要性在于它系统地梳理了基础模型在医疗影像这一关键应用领域的发展现状，为研究者提供了全面的视角，并指明了未来的研究方向，对推动该领域的发展具有重要指导意义。"}}
{"id": "2506.09102", "title": "Revolutionizing Clinical Trials: A Manifesto for AI-Driven Transformation", "authors": ["Mihaela van der Schaar", "Richard Peck", "Eoin McKinney", "Jim Weatherall", "Stuart Bailey", "Justine Rochon", "Chris Anagnostopoulos", "Pierre Marquet", "Anthony Wood", "Nicky Best", "Harry Amad", "Julianna Piskorz", "Krzysztof Kacprzyk", "Rafik Salama", "Christina Gunther", "Francesca Frau", "Antoine Pugeat", "Ramon Hernandez"], "summary": "This manifesto represents a collaborative vision forged by leaders in\npharmaceuticals, consulting firms, clinical research, and AI. It outlines a\nroadmap for two AI technologies - causal inference and digital twins - to\ntransform clinical trials, delivering faster, safer, and more personalized\noutcomes for patients. By focusing on actionable integration within existing\nregulatory frameworks, we propose a way forward to revolutionize clinical\nresearch and redefine the gold standard for clinical trials using AI.", "comment": null, "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.09102v1", "AI": {"title_translation": "革新临床试验：AI驱动转型的宣言", "tldr": "本宣言提出利用因果推断和数字孪生两种AI技术，在现有监管框架内，加速、安全、个性化地改造临床试验。", "motivation": "本宣言旨在通过整合AI技术，解决当前临床试验的效率、安全性和个性化不足的问题，从而革新临床研究并重新定义临床试验的黄金标准。", "method": "本宣言提出利用因果推断和数字孪生两种AI技术。通过在现有监管框架内进行可操作的整合，旨在革新临床研究。", "result": "通过AI驱动的转型，临床试验将实现更快、更安全、更个性化的患者结果。", "conclusion": "本宣言呼吁利用因果推断和数字孪生等AI技术，在现有监管框架内，彻底革新临床试验，以提供更快、更安全、更个性化的结果，并重新定义临床试验的黄金标准。", "translation": "本宣言代表了制药、咨询公司、临床研究和人工智能领域领导者共同形成的愿景。它概述了两种人工智能技术——因果推断和数字孪生——如何改造临床试验的路线图，为患者提供更快、更安全、更个性化的结果。通过专注于在现有监管框架内进行可操作的整合，我们提出了一条前进的道路，以革新临床研究，并利用人工智能重新定义临床试验的黄金标准。", "summary": "本宣言由制药、咨询、临床研究和AI领域的领导者共同制定，旨在利用因果推断和数字孪生两种AI技术，在现有监管框架内，加速、安全、个性化地改革临床试验，从而革新临床研究并重新定义行业标准。", "keywords": "AI, 临床试验, 因果推断, 数字孪生, 转型", "comments": "这份宣言的创新之处在于其协作性质和对两种特定AI技术（因果推断和数字孪生）在临床试验中实际应用的关注，同时强调在现有监管框架内进行整合，这为AI在受监管行业中的应用提供了可行的路径。其重要性在于旨在提升临床试验的效率、安全性和个性化，对患者和医药行业都具有重大意义。"}}
{"id": "2506.09966", "title": "Tight Paths and Tight Pairs in Weighted Directed Graphs", "authors": ["José Luis Balcázar"], "summary": "We state the graph-theoretic computational problem of finding tight paths in\na directed, edge-weighted graph, as well as its simplification of finding tight\npairs. These problems are motivated by the need of algorithms that find\nso-called basic antecedents in closure spaces, in one specific approach to data\nanalysis. We discuss and compare several algorithms to approach these problems.", "comment": null, "cate": "cs.DS", "url": "http://arxiv.org/abs/2506.09966v1", "AI": {"title_translation": "加权有向图中的紧路径和紧对", "tldr": "论文提出了在加权有向图中寻找紧路径和紧对的计算问题，并讨论了解决这些问题的几种算法，其动机是为了数据分析中寻找基本前件。", "motivation": "这些问题源于在数据分析的特定方法中，需要算法来寻找闭包空间中所谓的“基本前件”。", "method": "本文讨论并比较了几种解决寻找紧路径和紧对问题的算法。", "result": "Not mentioned in abstract", "conclusion": "Not mentioned in abstract", "translation": "我们阐述了在有向、边加权图中寻找紧路径的图论计算问题，以及其简化形式——寻找紧对。这些问题的提出，是出于在数据分析的特定方法中，需要算法来寻找闭包空间中所谓的“基本前件”。我们讨论并比较了几种解决这些问题的算法。", "summary": "本文提出了在加权有向图中寻找“紧路径”及其简化形式“紧对”的图论计算问题。这些问题源于数据分析中在闭包空间内寻找基本前件的算法需求。文中讨论并比较了几种解决这些问题的算法。", "keywords": "紧路径, 紧对, 加权有向图, 数据分析, 基本前件", "comments": "该摘要提出了一个由特定数据分析需求驱动的计算问题，表明了其潜在的实际应用价值。然而，摘要中没有详细说明所讨论的具体算法或比较结果，因此除了问题提出和一般方法论讨论之外，难以评估论文的具体贡献。"}}
{"id": "2506.09070", "title": "STREAMINGGS: Voxel-Based Streaming 3D Gaussian Splatting with Memory Optimization and Architectural Support", "authors": ["Chenqi Zhang", "Yu Feng", "Jieru Zhao", "Guangda Liu", "Wenchao Ding", "Chentao Wu", "Minyi Guo"], "summary": "3D Gaussian Splatting (3DGS) has gained popularity for its efficiency and\nsparse Gaussian-based representation. However, 3DGS struggles to meet the\nreal-time requirement of 90 frames per second (FPS) on resource-constrained\nmobile devices, achieving only 2 to 9 FPS.Existing accelerators focus on\ncompute efficiency but overlook memory efficiency, leading to redundant DRAM\ntraffic. We introduce STREAMINGGS, a fully streaming 3DGS\nalgorithm-architecture co-design that achieves fine-grained pipelining and\nreduces DRAM traffic by transforming from a tile-centric rendering to a\nmemory-centric rendering. Results show that our design achieves up to 45.7\n$\\times$ speedup and 62.9 $\\times$ energy savings over mobile Ampere GPUs.", "comment": null, "cate": "cs.GR", "url": "http://arxiv.org/abs/2506.09070v1", "AI": {"title_translation": "STREAMINGGS: 基于体素的流式3D高斯泼溅，具有内存优化和架构支持", "tldr": "STREAMINGGS通过算法-架构协同设计，将3DGS从瓦片中心渲染转换为内存中心渲染，显著提高了移动设备上的性能和能效。", "motivation": "3D高斯泼溅（3DGS）在资源受限的移动设备上难以达到实时性能（90 FPS），现有加速器忽视内存效率导致DRAM流量冗余。", "method": "引入STREAMINGGS，一种完全流式的3DGS算法-架构协同设计。它通过从瓦片中心渲染转换为内存中心渲染，实现了细粒度流水线化并减少了DRAM流量。", "result": "STREAMINGGS在移动Ampere GPU上实现了高达45.7倍的加速和62.9倍的能效提升。", "conclusion": "STREAMINGGS通过其创新的算法-架构协同设计和内存优化方法，有效解决了3DGS在移动设备上的性能瓶颈，实现了显著的加速和能效提升。", "translation": "3D高斯泼溅（3DGS）因其效率和基于稀疏高斯的表示而广受欢迎。然而，3DGS难以满足资源受限的移动设备上每秒90帧（FPS）的实时要求，只能达到2到9 FPS。现有加速器侧重于计算效率，但忽视了内存效率，导致DRAM流量冗余。我们引入了STREAMINGGS，一个完全流式的3DGS算法-架构协同设计，它通过从瓦片中心渲染转换为内存中心渲染，实现了细粒度流水线化并减少了DRAM流量。结果表明，我们的设计比移动Ampere GPU实现了高达45.7倍的加速和62.9倍的能效提升。", "summary": "本文提出了STREAMINGGS，一种针对3D高斯泼溅（3DGS）的算法-架构协同设计，旨在解决其在资源受限移动设备上实时性能不足的问题。通过从瓦片中心渲染转换为内存中心渲染，STREAMINGGS实现了细粒度流水线化并显著减少了DRAM流量。实验结果表明，STREAMINGGS在移动Ampere GPU上实现了高达45.7倍的加速和62.9倍的能效提升。", "keywords": "3D高斯泼溅, 流式渲染, 内存优化, 移动设备, 算法-架构协同设计", "comments": "STREAMINGGS的创新之处在于其算法-架构协同设计以及从瓦片中心渲染到内存中心渲染的范式转变，这有效解决了3DGS在移动设备上因内存效率低下导致的性能瓶颈。这项工作对于推动3DGS在边缘设备和移动应用中的实际部署具有重要意义，因为它不仅提高了性能，还显著降低了能耗。"}}
{"id": "2506.09464", "title": "Efficient Modular Multiplier over GF (2^m) for ECPM", "authors": ["Ruby Kumari", "Gaurav Purohit", "Abhijit Karmakar"], "summary": "Elliptic curve cryptography (ECC) has emerged as the dominant public-key\nprotocol, with NIST standardizing parameters for binary field GF(2^m) ECC\nsystems. This work presents a hardware implementation of a Hybrid\nMultiplication technique for modular multiplication over binary field GF(2m),\ntargeting NIST B-163, 233, 283, and 571 parameters. The design optimizes the\ncombination of conventional multiplication (CM) and Karatsuba multiplication\n(KM) to enhance elliptic curve point multiplication (ECPM). The key innovation\nuses CM for smaller operands (up to 41 bits for m=163) and KM for larger ones,\nreducing computational complexity and enhancing efficiency. The design is\nevaluated in three areas: Resource Utilization For m=163, the hybrid design\nuses 6,812 LUTs, a 39.82% reduction compared to conventional methods. For\nm=233, LUT usage reduces by 45.53% and 70.70% compared to overlap-free and\nbit-parallel implementations. Delay Performance For m=163, achieves 13.31ns\ndelay, improving by 37.60% over bit-parallel implementations. For m=233,\nmaintains 13.39ns delay. Area-Delay Product For m=163, achieves ADP of 90,860,\noutperforming bit-parallel (75,337) and digit-serial (43,179) implementations.\nFor m=233, demonstrates 16.86% improvement over overlap-free and 96.10% over\nbit-parallel designs. Results show the hybrid technique significantly improves\nspeed, hardware efficiency, and resource utilization for ECC cryptographic\nsystems.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.09464v1", "AI": {"title_translation": "高效的GF(2^m)域模乘器用于ECPM", "tldr": "本论文提出了一种针对GF(2^m)上模乘的混合乘法技术硬件实现，用于椭圆曲线密码学（ECC），结合了传统乘法和Karatsuba乘法，在NIST B-163/233/283/571参数下，显著提升了速度、硬件效率和资源利用率。", "motivation": "椭圆曲线密码学（ECC）已成为主导的公钥协议，NIST对二元域GF(2^m) ECC系统进行了参数标准化，因此需要高效的硬件实现来增强椭圆曲线点乘（ECPM）。", "method": "该工作提出了一种针对二元域GF(2^m)上模乘的混合乘法技术的硬件实现。该设计优化了传统乘法（CM）和Karatsuba乘法（KM）的组合，具体为对较小操作数使用CM，对较大操作数使用KM，以降低计算复杂性并提高效率。目标是NIST B-163、233、283和571参数。", "result": "资源利用率方面：对于m=163，混合设计使用6,812个LUT，比传统方法减少39.82%。对于m=233，LUT使用量比无重叠和位并行实现分别减少45.53%和70.70%。延迟性能方面：对于m=163，实现了13.31ns延迟，比位并行实现提高37.60%。对于m=233，保持13.39ns延迟。面积-延迟积（ADP）方面：对于m=163，实现了90,860的ADP，优于位并行（75,337）和数字串行（43,179）实现。对于m=233，比无重叠设计提高16.86%，比位并行设计提高96.10%。总体而言，混合技术显著提高了ECC密码系统的速度、硬件效率和资源利用率。", "conclusion": "所提出的混合模乘技术在速度、硬件效率和资源利用率方面显著提升了ECC密码系统的效率和性能，尤其适用于NIST标准化参数。", "translation": "椭圆曲线密码学（ECC）已成为主导的公钥协议，NIST对二元域GF(2^m) ECC系统进行了参数标准化。这项工作提出了一种针对二元域GF(2^m)上模乘的混合乘法技术的硬件实现，目标是NIST B-163、233、283和571参数。该设计优化了传统乘法（CM）和Karatsuba乘法（KM）的组合，以增强椭圆曲线点乘（ECPM）。关键创新在于对较小操作数（m=163时高达41位）使用CM，对较大操作数使用KM，从而降低了计算复杂性并提高了效率。该设计在三个方面进行了评估：资源利用率 对于m=163，混合设计使用6,812个LUT，与传统方法相比减少了39.82%。对于m=233，LUT使用量与无重叠和位并行实现相比分别减少了45.53%和70.70%。延迟性能 对于m=163，实现了13.31ns的延迟，比位并行实现提高了37.60%。对于m=233，保持13.39ns的延迟。面积-延迟积 对于m=163，实现了90,860的ADP，优于位并行（75,337）和数字串行（43,179）实现。对于m=233，比无重叠设计提高了16.86%，比位并行设计提高了96.10%。结果表明，混合技术显著提高了ECC密码系统的速度、硬件效率和资源利用率。", "summary": "本论文介绍了一种高效的混合模乘技术硬件实现，用于GF(2^m)上的椭圆曲线密码学（ECC），目标是NIST B-163、233、283和571参数。通过策略性地结合对较小操作数使用传统乘法和对较大操作数使用Karatsuba乘法，该设计显著降低了计算复杂性。评估结果表明，在资源利用率（LUT减少高达70.70%）、延迟性能（提升高达37.60%）和面积-延迟积方面均有显著改善，最终提升了ECC系统的速度和硬件效率。", "keywords": "椭圆曲线密码学, 模乘, 混合乘法, GF(2^m), 硬件实现", "comments": "这篇论文提出了一种实用且创新的方法，通过智能地结合两种现有的乘法技术来优化ECC中的模乘。针对各种NIST参数的详细性能指标突出了在硬件效率和速度方面的显著优势，这对于实际密码学应用至关重要。混合策略有效地平衡了复杂性和性能。"}}
{"id": "2506.09505", "title": "On the Performance of Cloud-based ARM SVE for Zero-Knowledge Proving Systems", "authors": ["Dumitrel Loghin", "Shuang Liang", "Shengwei Liu", "Xiong Liu", "Pingcheng Ruan", "Zhigang Ye"], "summary": "Zero-knowledge proofs (ZKP) are becoming a gold standard in scaling\nblockchains and bringing Web3 to life. At the same time, ZKP for transactions\nrunning on the Ethereum Virtual Machine require powerful servers with hundreds\nof CPU cores. The current zkProver implementation from Polygon is optimized for\nx86-64 CPUs by vectorizing key operations, such as Merkle tree building with\nPoseidon hashes over the Goldilocks field, with Advanced Vector Extensions (AVX\nand AVX512). With these optimizations, a ZKP for a batch of transactions is\ngenerated in less than two minutes. With the advent of cloud servers with ARM\nwhich are at least 10% cheaper than x86-64 servers and the implementation of\nARM Scalable Vector Extension (SVE), we wonder if ARM servers can take over\ntheir x86-64 counterparts. Unfortunately, our analysis shows that current ARM\nCPUs are not a match for their x86-64 competitors. Graviton4 from Amazon Web\nServices (AWS) and Axion from Google Cloud Platform (GCP) are 1.6X and 1.4X\nslower compared to the latest AMD EPYC and Intel Xeon servers from AWS with AVX\nand AVX512, respectively, when building a Merkle tree with over four million\nleaves. This low performance is due to (1) smaller vector size in these ARM\nCPUs (128 bits versus 512 bits in AVX512) and (2) lower clock frequency. On the\nother hand, ARM SVE/SVE2 Instruction Set Architecture (ISA) is at least as\npowerful as AVX/AVX512 but more flexible. Moreover, we estimate that increasing\nthe vector size to 512 bits will enable higher performance in ARM CPUs compared\nto their x86-64 counterparts while maintaining their price advantage.", "comment": null, "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.09505v1", "AI": {"title_translation": "基于云的ARM SVE在零知识证明系统中的性能", "tldr": "基于云的ARM CPU目前在零知识证明方面比x86-64慢，原因在于硬件限制，但ARM SVE在向量大小增加后具有潜力。", "motivation": "零知识证明（ZKP）对扩展区块链至关重要，但这需要强大的x86-64服务器。云端ARM服务器价格更低，且ARM SVE的出现引发了其能否取代x86-64处理ZKP工作负载的疑问。", "method": "研究人员分析了当前ARM CPU（来自AWS的Graviton4和来自GCP的Axion）与AWS上最新的AMD EPYC和Intel Xeon服务器（支持AVX和AVX512）在构建包含超过四百万个叶子的Merkle树这一关键ZKP操作中的性能。", "result": "当前ARM CPU（Graviton4和Axion）在Merkle树构建方面分别比x86-64对应产品慢1.6倍和1.4倍。这归因于ARM CPU中较小的向量大小（128位对比AVX512中的512位）和较低的时钟频率。然而，ARM SVE/SVE2 ISA与AVX/AVX512同样强大且更灵活，并且估计将ARM向量大小增加到512位将使其性能超越x86-64，同时保持价格优势。", "conclusion": "当前的云端ARM CPU在零知识证明系统方面无法与x86-64竞争，原因在于硬件限制。然而，随着未来的改进，特别是向量大小增加到512位，ARM SVE有望在性能上超越x86-64，同时保持其成本优势。", "translation": "零知识证明 (ZKP) 正在成为扩展区块链和实现 Web3 的黄金标准。同时，在以太坊虚拟机上运行的交易的 ZKP 需要配备数百个 CPU 核心的强大服务器。Polygon 当前的 zkProver 实现针对 x86-64 CPU 进行了优化，通过向量化关键操作（例如使用 Poseidon 哈希在 Goldilocks 域上构建 Merkle 树），利用了高级向量扩展 (AVX 和 AVX512)。通过这些优化，一批交易的 ZKP 可以在不到两分钟内生成。随着比 x86-64 服务器至少便宜 10% 的基于 ARM 的云服务器的出现以及 ARM 可伸缩向量扩展 (SVE) 的实现，我们想知道 ARM 服务器是否能取代其 x86-64 对应产品。不幸的是，我们的分析表明，当前的 ARM CPU 无法与它们的 x86-64 竞争对手匹敌。在构建超过四百万个叶子的 Merkle 树时，亚马逊网络服务 (AWS) 的 Graviton4 和谷歌云平台 (GCP) 的 Axion 分别比 AWS 上最新的配备 AVX 和 AVX512 的 AMD EPYC 和 Intel Xeon 服务器慢 1.6 倍和 1.4 倍。这种低性能是由于 (1) 这些 ARM CPU 中较小的向量大小（128 位对比 AVX512 中的 512 位）和 (2) 较低的时钟频率。另一方面，ARM SVE/SVE2 指令集架构 (ISA) 至少与 AVX/AVX512 一样强大，但更灵活。此外，我们估计将向量大小增加到 512 位将使 ARM CPU 获得比其 x86-64 对应产品更高的性能，同时保持其价格优势。", "summary": "本文评估了基于云的 ARM SVE 在零知识证明 (ZKP) 系统中的性能，特别是与 x86-64 服务器的对比。研究发现，尽管 ARM 服务器价格更低，但当前的 ARM CPU（如 Graviton4 和 Axion）在执行 Merkle 树构建等关键 ZKP 操作时，由于向量大小和时钟频率的限制，性能明显低于 x86-64 服务器。然而，作者指出 ARM SVE/SVE2 ISA 具有强大且灵活的潜力，并预测如果 ARM CPU 的向量大小增加到 512 位，其性能将超越 x86-64 且仍能保持价格优势。", "keywords": "零知识证明, ARM SVE, x86-64, Merkle 树, 云计算性能", "comments": "这篇论文对在快速发展的零知识证明领域中，不同硬件架构的性能进行了及时且重要的比较。它揭示了当前 ARM 服务器在处理 ZKP 工作负载方面的局限性，特别是在向量处理能力上的差距。然而，论文也指出了 ARM SVE 的巨大潜力，为未来ARM架构在高性能计算（尤其是区块链和Web3基础设施）中的发展方向提供了指导，强调了硬件设计中向量大小的关键作用。"}}
{"id": "2506.09284", "title": "UAD: Unsupervised Affordance Distillation for Generalization in Robotic Manipulation", "authors": ["Yihe Tang", "Wenlong Huang", "Yingke Wang", "Chengshu Li", "Roy Yuan", "Ruohan Zhang", "Jiajun Wu", "Li Fei-Fei"], "summary": "Understanding fine-grained object affordances is imperative for robots to\nmanipulate objects in unstructured environments given open-ended task\ninstructions. However, existing methods of visual affordance predictions often\nrely on manually annotated data or conditions only on a predefined set of\ntasks. We introduce UAD (Unsupervised Affordance Distillation), a method for\ndistilling affordance knowledge from foundation models into a task-conditioned\naffordance model without any manual annotations. By leveraging the\ncomplementary strengths of large vision models and vision-language models, UAD\nautomatically annotates a large-scale dataset with detailed $<$instruction,\nvisual affordance$>$ pairs. Training only a lightweight task-conditioned\ndecoder atop frozen features, UAD exhibits notable generalization to\nin-the-wild robotic scenes and to various human activities, despite only being\ntrained on rendered objects in simulation. Using affordance provided by UAD as\nthe observation space, we show an imitation learning policy that demonstrates\npromising generalization to unseen object instances, object categories, and\neven variations in task instructions after training on as few as 10\ndemonstrations. Project website: https://unsup-affordance.github.io/", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.09284v1", "AI": {"title_translation": "UAD：用于机器人操作泛化的无监督示能蒸馏", "tldr": "UAD是一种无需手动标注，从基础模型中蒸馏示能知识的方法，它能自动标注大规模数据集，并展示了在机器人操作中对未知场景和任务的显著泛化能力。", "motivation": "现有的视觉示能预测方法依赖手动标注数据或仅限于预定义任务集，这限制了机器人在非结构化环境中处理开放式任务的能力。", "method": "UAD（无监督示能蒸馏）通过利用大型视觉模型和视觉-语言模型的互补优势，将示能知识从基础模型蒸馏到任务条件示能模型中。它自动标注大规模数据集，生成详细的<指令，视觉示能>对。UAD仅在冻结特征之上训练一个轻量级任务条件解码器。", "result": "尽管仅在模拟渲染对象上训练，UAD在真实机器人场景和各种人类活动中表现出显著的泛化能力。使用UAD提供的示能作为观察空间，模仿学习策略在仅10次演示后，对未见过的物体实例、物体类别甚至任务指令变体都表现出有希望的泛化能力。", "conclusion": "UAD通过无监督示能蒸馏，有效解决了机器人操作中示能预测对标注数据的依赖问题，显著提升了机器人在复杂环境和开放式任务中的泛化能力。", "translation": "理解细粒度物体示能对于机器人在非结构化环境中根据开放式任务指令操纵物体至关重要。然而，现有的视觉示能预测方法通常依赖手动标注数据或仅限于预定义任务集。我们引入了UAD（无监督示能蒸馏），一种无需任何手动标注即可从基础模型中蒸馏示能知识到任务条件示能模型的方法。通过利用大型视觉模型和视觉-语言模型的互补优势，UAD自动标注了一个包含详细<指令，视觉示能>对的大规模数据集。UAD仅在冻结特征之上训练一个轻量级任务条件解码器，尽管只在模拟中的渲染对象上训练，但它在真实机器人场景和各种人类活动中表现出显著的泛化能力。我们展示了使用UAD提供的示能作为观察空间的模仿学习策略，在仅10次演示后，对未见过的物体实例、物体类别甚至任务指令变体都表现出有希望的泛化能力。项目网站：https://unsup-affordance.github.io/", "summary": "UAD（无监督示能蒸馏）是一种创新方法，旨在解决机器人操作中视觉示能预测对手动标注数据的依赖问题。它通过结合大型视觉模型和视觉-语言模型的优势，自动生成大规模的指令-视觉示能数据集。UAD仅训练一个轻量级解码器，便能从基础模型中蒸馏出示能知识，并在模拟训练后展现出对真实世界机器人场景和人类活动的强大泛化能力。此外，将UAD的示能作为观察空间，模仿学习策略在少量演示下，也能对新物体和任务指令表现出良好的泛化性。", "keywords": "无监督学习, 示能蒸馏, 机器人操作, 泛化, 模仿学习", "comments": "UAD的创新点在于其无监督的示能蒸馏方法，这极大地减少了对昂贵手动标注数据的依赖，从而为机器人操作的泛化性开辟了新途径。它利用了当前基础模型的强大能力，并将其转化为实际机器人应用中的有效示能表示，这对于推动机器人技术在非结构化环境中的发展具有重要意义。该方法仅需少量演示即可实现泛化，显示出其在实际部署中的巨大潜力。"}}
{"id": "2506.09079", "title": "VersaVid-R1: A Versatile Video Understanding and Reasoning Model from Question Answering to Captioning Tasks", "authors": ["Xinlong Chen", "Yuanxing Zhang", "Yushuo Guan", "Bohan Zeng", "Yang Shi", "Sihan Yang", "Pengfei Wan", "Qiang Liu", "Liang Wang", "Tieniu Tan"], "summary": "Recent advancements in multimodal large language models have successfully\nextended the Reason-Then-Respond paradigm to image-based reasoning, yet\nvideo-based reasoning remains an underdeveloped frontier, primarily due to the\nscarcity of high-quality reasoning-oriented data and effective training\nmethodologies. To bridge this gap, we introduce DarkEventInfer and MixVidQA,\ntwo novel datasets specifically designed to stimulate the model's advanced\nvideo understanding and reasoning abilities. DarkEventinfer presents videos\nwith masked event segments, requiring models to infer the obscured content\nbased on contextual video cues. MixVidQA, on the other hand, presents\ninterleaved video sequences composed of two distinct clips, challenging models\nto isolate and reason about one while disregarding the other. Leveraging these\ncarefully curated training samples together with reinforcement learning guided\nby diverse reward functions, we develop VersaVid-R1, the first versatile video\nunderstanding and reasoning model under the Reason-Then-Respond paradigm\ncapable of handling multiple-choice and open-ended question answering, as well\nas video captioning tasks. Extensive experiments demonstrate that VersaVid-R1\nsignificantly outperforms existing models across a broad spectrum of\nbenchmarks, covering video general understanding, cognitive reasoning, and\ncaptioning tasks.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09079v1", "AI": {"title_translation": "VersaVid-R1：一种从问答到字幕任务的多功能视频理解和推理模型", "tldr": "VersaVid-R1模型通过引入DarkEventInfer和MixVidQA两个新数据集，解决了视频推理中数据稀缺和训练方法不足的问题，并在多项视频理解和推理任务中表现出色。", "motivation": "现有的多模态大语言模型在图像推理方面取得了进展，但视频推理仍不发达，主要原因是缺乏高质量的推理导向数据和有效的训练方法。", "method": "提出了两个新的数据集：DarkEventInfer（要求模型根据上下文推断被遮蔽的事件内容）和MixVidQA（要求模型从交错视频序列中隔离并推理一个片段）。利用这些数据集和强化学习，开发了VersaVid-R1模型，该模型是首个在Reason-Then-Respond范式下处理多项选择、开放式问答和视频字幕任务的多功能视频理解和推理模型。", "result": "VersaVid-R1在视频通用理解、认知推理和字幕任务的广泛基准测试中，显著优于现有模型。", "conclusion": "VersaVid-R1通过创新的数据集和训练方法，成功提升了视频理解和推理能力，并在多项任务中展现出卓越性能。", "translation": "多模态大型语言模型的最新进展已成功将“先推理后响应”范式扩展到基于图像的推理，但基于视频的推理仍然是一个未充分发展的领域，这主要是由于高质量推理导向数据的稀缺和有效训练方法的不足。为了弥补这一空白，我们引入了DarkEventInfer和MixVidQA，这两个新颖的数据集专门设计用于激发模型的先进视频理解和推理能力。DarkEventInfer呈现的是包含被遮蔽事件片段的视频，要求模型根据上下文视频线索推断被遮蔽的内容。另一方面，MixVidQA呈现的是由两个不同片段组成的交错视频序列，挑战模型去分离并推理其中一个片段而忽略另一个。利用这些精心策划的训练样本以及由不同奖励函数指导的强化学习，我们开发了VersaVid-R1，这是在“先推理后响应”范式下首个多功能视频理解和推理模型，能够处理多项选择和开放式问答以及视频字幕任务。广泛的实验表明，VersaVid-R1在涵盖视频通用理解、认知推理和字幕任务的广泛基准测试中，显著优于现有模型。", "summary": "本文针对视频理解和推理领域数据和训练方法不足的问题，提出了两个创新数据集DarkEventInfer和MixVidQA，旨在提升模型的视频推理能力。在此基础上，结合强化学习，开发了VersaVid-R1模型。该模型是首个在“先推理后响应”范式下，能处理多项选择和开放式问答以及视频字幕任务的多功能视频理解和推理模型。实验结果表明，VersaVid-R1在多种视频理解、认知推理和字幕任务基准上均显著优于现有模型。", "keywords": "视频理解, 视频推理, 大型语言模型, 数据集, 强化学习", "comments": "该研究通过引入两个新颖且设计巧妙的数据集（DarkEventInfer和MixVidQA），有效解决了视频推理领域数据稀缺的痛点，这是其主要创新点。结合强化学习训练出的VersaVid-R1模型，实现了从问答到字幕任务的多功能视频理解和推理，展现了其在复杂视频理解任务上的潜力，为未来视频AI发展奠定了基础。"}}
{"id": "2506.09420", "title": "A Call for Collaborative Intelligence: Why Human-Agent Systems Should Precede AI Autonomy", "authors": ["Henry Peng Zou", "Wei-Chieh Huang", "Yaozu Wu", "Chunyu Miao", "Dongyuan Li", "Aiwei Liu", "Yue Zhou", "Yankai Chen", "Weizhi Zhang", "Yangning Li", "Liancheng Fang", "Renhe Jiang", "Philip S. Yu"], "summary": "Recent improvements in large language models (LLMs) have led many researchers\nto focus on building fully autonomous AI agents. This position paper questions\nwhether this approach is the right path forward, as these autonomous systems\nstill have problems with reliability, transparency, and understanding the\nactual requirements of human. We suggest a different approach: LLM-based\nHuman-Agent Systems (LLM-HAS), where AI works with humans rather than replacing\nthem. By keeping human involved to provide guidance, answer questions, and\nmaintain control, these systems can be more trustworthy and adaptable. Looking\nat examples from healthcare, finance, and software development, we show how\nhuman-AI teamwork can handle complex tasks better than AI working alone. We\nalso discuss the challenges of building these collaborative systems and offer\npractical solutions. This paper argues that progress in AI should not be\nmeasured by how independent systems become, but by how well they can work with\nhumans. The most promising future for AI is not in systems that take over human\nroles, but in those that enhance human capabilities through meaningful\npartnership.", "comment": null, "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.09420v1", "AI": {"title_translation": "协作智能的呼唤：为什么人机协作系统应先于AI自主性", "tldr": "本文认为，与其追求完全自主的AI，不如优先发展人机协作系统（LLM-HAS），因为它们更可靠、透明且能更好地理解人类需求，通过人类指导和控制来增强人类能力。", "motivation": "当前许多研究侧重于构建完全自主的AI代理，但这些系统在可靠性、透明度和理解人类实际需求方面存在问题。本文质疑这一路径是否正确。", "method": "本文提出了一种不同的方法：基于LLM的人机协作系统（LLM-HAS），强调AI与人类协作而非取代人类。通过保持人类参与提供指导、回答问题和保持控制，这些系统可以更值得信赖和适应性强。通过医疗保健、金融和软件开发领域的例子，展示人机协作如何比AI独立工作更好地处理复杂任务。讨论了构建这些协作系统的挑战并提供了实用解决方案。", "result": "本文展示了人机协作系统在处理复杂任务方面优于AI独立工作，并通过具体领域的例子进行了说明。它提出了AI进步的衡量标准不应是系统的独立性，而是与人类协作的程度。", "conclusion": "AI最有前途的未来不在于取代人类角色的系统，而在于通过有意义的伙伴关系增强人类能力的系统。AI的进步应衡量其与人类协作的程度，而非独立性。", "translation": "大型语言模型（LLMs）的最新改进促使许多研究人员专注于构建完全自主的AI代理。这篇立场论文质疑这种方法是否是前进的正确道路，因为这些自主系统在可靠性、透明度和理解人类实际需求方面仍然存在问题。我们提出了一种不同的方法：基于LLM的人机协作系统（LLM-HAS），其中AI与人类协同工作而不是取代他们。通过让人类参与提供指导、回答问题和保持控制，这些系统可以更值得信赖和更具适应性。通过医疗保健、金融和软件开发领域的例子，我们展示了人机协作如何比AI独立工作更好地处理复杂任务。我们还讨论了构建这些协作系统的挑战并提供了实用解决方案。本文认为，AI的进步不应以系统变得多么独立来衡量，而应以它们与人类协作的程度来衡量。AI最有前途的未来不在于取代人类角色的系统，而在于通过有意义的伙伴关系增强人类能力的系统。", "summary": "这篇立场论文质疑当前构建完全自主AI代理的趋势，指出其在可靠性、透明度和理解人类需求上的不足。作者提出并倡导基于LLM的人机协作系统（LLM-HAS），强调AI应与人类协作以增强人类能力，而非取代人类。通过具体领域案例，论文论证了人机协作在处理复杂任务上的优势，并讨论了实施挑战及解决方案。核心论点是AI的进步应以其与人类协作的程度而非自主性来衡量，认为AI的未来在于提升人类能力而非替代人类角色。", "keywords": "人机协作系统, AI自主性, 大型语言模型, 协作智能, 人类增强", "comments": "这篇论文的创新之处在于它挑战了AI领域当前普遍追求完全自主AI的范式，并提出了一个以人类为中心的替代方案。其重要性在于强调了AI系统在实际应用中，尤其是在高风险领域，人类监督和协作的关键作用，这对于AI的负责任发展具有指导意义。论文的局限性可能在于其作为立场论文，更多是概念性论证而非实验验证，但其提出的协作智能理念为未来AI研究和应用提供了新的视角。"}}
{"id": "2506.09087", "title": "Spiking Neural Models for Decision-Making Tasks with Learning", "authors": ["Sophie Jaffard", "Giulia Mezzadri", "Patricia Reynaud-Bouret", "Etienne Tanré"], "summary": "In cognition, response times and choices in decision-making tasks are\ncommonly modeled using Drift Diffusion Models (DDMs), which describe the\naccumulation of evidence for a decision as a stochastic process, specifically a\nBrownian motion, with the drift rate reflecting the strength of the evidence.\nIn the same vein, the Poisson counter model describes the accumulation of\nevidence as discrete events whose counts over time are modeled as Poisson\nprocesses, and has a spiking neurons interpretation as these processes are used\nto model neuronal activities. However, these models lack a learning mechanism\nand are limited to tasks where participants have prior knowledge of the\ncategories. To bridge the gap between cognitive and biological models, we\npropose a biologically plausible Spiking Neural Network (SNN) model for\ndecision-making that incorporates a learning mechanism and whose neurons\nactivities are modeled by a multivariate Hawkes process. First, we show a\ncoupling result between the DDM and the Poisson counter model, establishing\nthat these two models provide similar categorizations and reaction times and\nthat the DDM can be approximated by spiking Poisson neurons. To go further, we\nshow that a particular DDM with correlated noise can be derived from a Hawkes\nnetwork of spiking neurons governed by a local learning rule. In addition, we\ndesigned an online categorization task to evaluate the model predictions. This\nwork provides a significant step toward integrating biologically relevant\nneural mechanisms into cognitive models, fostering a deeper understanding of\nthe relationship between neural activity and behavior.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09087v1", "AI": {"title_translation": "用于学习决策任务的尖峰神经模型", "tldr": "本文提出了一种具有学习机制的生物学上合理的尖峰神经网络（SNN）模型，用于决策任务，并展示了其与现有认知模型的联系，从而促进了生物学相关神经机制与认知模型的整合。", "motivation": "现有的决策模型，如漂移扩散模型（DDM）和泊松计数模型，缺乏学习机制，且仅限于参与者对类别有先验知识的任务。为了弥合认知模型和生物学模型之间的差距，需要引入一个结合学习机制的生物学上合理的模型。", "method": "本文提出了一种生物学上合理的尖峰神经网络（SNN）模型，其神经元活动由多元霍克斯过程建模，并包含一个学习机制。首先，建立了DDM和泊松计数模型之间的耦合关系。其次，展示了具有相关噪声的特定DDM可以从由局部学习规则控制的霍克斯尖峰神经元网络中导出。此外，设计了一个在线分类任务来评估模型预测。", "result": "研究结果表明，DDM和泊松计数模型提供了相似的分类和反应时间，并且DDM可以用尖峰泊松神经元近似。此外，证明了具有相关噪声的特定DDM可以从由局部学习规则控制的霍克斯尖峰神经元网络中导出。", "conclusion": "这项工作为将生物学相关的神经机制整合到认知模型中迈出了重要一步，促进了对神经活动与行为之间关系的更深理解。", "translation": "在认知领域，决策任务中的反应时间和选择通常使用漂移扩散模型（DDM）进行建模，该模型将决策证据的积累描述为一个随机过程，特别是布朗运动，其中漂移率反映了证据的强度。同样，泊松计数模型将证据的积累描述为离散事件，其随时间变化的计数被建模为泊松过程，并且具有尖峰神经元解释，因为这些过程用于建模神经元活动。然而，这些模型缺乏学习机制，并且仅限于参与者对类别具有先验知识的任务。为了弥合认知模型和生物学模型之间的差距，我们提出了一种生物学上合理的尖峰神经网络（SNN）模型用于决策，该模型结合了学习机制，并且其神经元活动由多元霍克斯过程建模。首先，我们展示了DDM和泊松计数模型之间的耦合结果，证实这两个模型提供了相似的分类和反应时间，并且DDM可以通过尖峰泊松神经元进行近似。更进一步，我们展示了具有相关噪声的特定DDM可以从由局部学习规则控制的霍克斯尖峰神经元网络中导出。此外，我们设计了一个在线分类任务来评估模型预测。这项工作为将生物学相关的神经机制整合到认知模型中迈出了重要一步，促进了对神经活动与行为之间关系的更深理解。", "summary": "本文针对现有决策模型缺乏学习机制的局限性，提出了一种生物学上合理的尖峰神经网络（SNN）模型。该模型将学习机制纳入其中，并使用多元霍克斯过程模拟神经元活动。研究首先证明了漂移扩散模型（DDM）与泊松计数模型之间的耦合关系，表明两者在分类和反应时间上具有相似性，且DDM可由尖峰泊松神经元近似。进一步，论文展示了特定DDM可由具有局部学习规则的霍克斯尖峰神经元网络推导而来。通过在线分类任务评估了模型预测，旨在将生物学神经机制融入认知模型，以加深对神经活动与行为关系的理解。", "keywords": "尖峰神经网络, 决策, 学习机制, 漂移扩散模型, 霍克斯过程", "comments": "这篇论文的创新之处在于提出了一个结合学习机制的生物学上合理的尖峰神经网络模型，弥补了现有认知模型在学习能力上的不足。通过将神经活动建模为多元霍克斯过程，并建立与经典漂移扩散模型和泊松计数模型的联系，该研究为理解神经活动如何产生决策行为提供了新的视角。其重要性在于促进了认知科学与计算神经科学的交叉融合，为未来开发更全面的大脑功能模型奠定了基础。"}}
{"id": "2506.09931", "title": "Faster-than-Nyquist Signaling is Good for Single-Carrier ISAC: An Analytical Study", "authors": ["Shuangyang Li", "Fan Liu", "Yifeng Xiong", "Weijie Yuan", "Baoming Bai", "Christos Masouros", "Giuseppe Caire"], "summary": "In this paper, we provide an analytical study of single-carrier\nfaster-than-Nyquist (FTN) signaling for integrated sensing and communications\n(ISAC). Our derivations show that FTN is advantageous for ISAC, and reveal new\ninsights that these advantages come from the fact that FTN signaling can\neffectively avoid the spectral aliasing due to the mismatch between the symbol\nrate and the bandwidth of the shaping pulse. Specifically, the communication\nspectral efficiency advantages of FTN signaling over time-invariant multipath\nchannels are analytically shown, where both upper- and lower-bounds on the\nspectral efficiency are derived. We show that the gap between these two bounds\ncorresponds to the potential signal-to-noise ratio (SNR) variation due to the\npresence of multipath delay and spectral aliasing, which diminishes as the\nsymbol rate grows higher. Particularly, in the limiting case, this SNR\nvariation disappears while the degree of freedom (DoF) of the system attain the\nmaximum. Furthermore, the sensing advantages for FTN signals are verified in\nterms of the expected normalized squared ambiguity function. We show that FTN\nsignals generally enjoy a more robust ranging performance. More importantly, we\nprove that FTN signaling can effectively avoid the undesired peaks in the\nconsidered ambiguity function along the Doppler dimension, thereby reducing the\nambiguities in velocity estimation. All these conclusions are explicitly\nverified by numerical results.", "comment": null, "cate": "cs.IT", "url": "http://arxiv.org/abs/2506.09931v1", "AI": {"title_translation": "超奈奎斯特信号有利于单载波ISAC：一项分析研究", "tldr": "本文通过分析研究表明，超奈奎斯特（FTN）信号对于集成感知与通信（ISAC）系统具有优势，因为它能有效避免符号速率与脉冲带宽不匹配导致的频谱混叠，从而提高通信频谱效率并改善感知性能。", "motivation": "本文旨在通过分析研究，探讨单载波超奈奎斯特（FTN）信号在集成感知与通信（ISAC）中的优势。", "method": "本文采用分析研究方法，通过推导通信频谱效率的上下界，并分析期望归一化平方模糊函数，来验证FTN信号的优势。", "result": "研究表明，FTN信号有利于ISAC，其优势在于能有效避免符号速率与成形脉冲带宽不匹配导致的频谱混叠。在时不变多径信道下，FTN信号的通信频谱效率具有优势，且上下界之间的差距随符号速率增加而减小，在极限情况下信噪比（SNR）变化消失，系统自由度（DoF）达到最大。感知方面，FTN信号通常具有更鲁棒的测距性能，并能有效避免多普勒维度上不期望的峰值，从而减少速度估计的模糊性。", "conclusion": "超奈奎斯特（FTN）信号对于单载波集成感知与通信（ISAC）系统具有显著优势，主要体现在其能有效避免频谱混叠，从而提高通信频谱效率并改善感知性能（包括测距鲁棒性和减少速度估计模糊性）。", "translation": "本文对用于集成感知与通信（ISAC）的单载波超奈奎斯特（FTN）信号进行了分析研究。我们的推导表明，FTN对ISAC有利，并揭示了这些优势源于FTN信号能有效避免由于符号速率与成形脉冲带宽不匹配而引起的频谱混叠。具体而言，本文分析展示了FTN信号在时不变多径信道下对通信频谱效率的优势，并推导了频谱效率的上下界。我们发现，这两个界限之间的差距对应于多径延迟和频谱混叠引起的潜在信噪比（SNR）变化，该变化随符号速率的增加而减小。特别是在极限情况下，这种SNR变化消失，而系统的自由度（DoF）达到最大。此外，本文通过期望归一化平方模糊函数验证了FTN信号的感知优势。我们表明FTN信号通常具有更鲁棒的测距性能。更重要的是，我们证明了FTN信号可以有效避免所考虑的模糊函数在多普勒维度上的不期望峰值，从而减少速度估计中的模糊性。所有这些结论都通过数值结果得到了明确验证。", "summary": "本文对单载波超奈奎斯特（FTN）信号在集成感知与通信（ISAC）中的应用进行了深入分析。研究发现，FTN信号能有效避免因符号速率与脉冲带宽不匹配导致的频谱混叠，从而在通信频谱效率和感知性能两方面展现出显著优势。具体而言，FTN在多径信道下能提高通信频谱效率，其信噪比变化随符号速率增加而减小；在感知方面，FTN能提供更鲁棒的测距性能，并有效减少速度估计中的模糊性。数值结果验证了这些分析结论。", "keywords": "超奈奎斯特信号, 集成感知与通信, 频谱效率, 模糊函数, 频谱混叠", "comments": "这篇论文的创新点在于它提供了一个关于FTN信号在ISAC中优势的分析性研究，并揭示了其优势来源于有效避免频谱混叠这一新见解。这对于理解FTN在未来通信和感知一体化系统中的应用潜力具有重要意义。"}}
{"id": "2506.09550", "title": "Automated Synthesis of Formally Verified Multi-Abstraction Function Summaries", "authors": ["Fanpeng Yang", "Xu Ma", "Shuling Wang", "Xiong Xu", "Qinxiang Cao", "Naijun Zhan", "Xiaofeng Li", "Bin Gu"], "summary": "Function summaries, which characterize the behavior of code segments\n(typically functions) through preconditions and postconditions, are essential\nfor understanding, reusing, and verifying software, particularly in\nsafety-critical domains like aerospace embedded systems. However, these\nmission-critical legacy code serving as a valuable reused asset often lacks\nformal specifications. It is challenging to automatically generate function\nsummaries for C programs, due to the existence of complex features such as\nloops, nested function calls, pointer aliasing, and so on. Moreover, function\nsummaries should support multiple abstraction levels to meet diverse\nrequirements, e.g. precise summaries capturing full functionality for formal\nverification and intuitive summaries for human understanding.\n  To address these challenges, we first propose a novel framework that combines\nsymbolic execution, large language models (LLMs), and formal verification to\ngenerate Relatively Strongest Postconditions (RSPs) and build function\nsummaries that fully capture program behavior. Our approach leverages VST-A's\nsymbolic execution to precisely track program execution paths and state\ntransitions, employs LLMs to infer loop invariants based on predefined\ntemplates, and uses Frama-C to guarantee soundness of generated summaries in an\niterative refinement loop. Furthermore, from generated RSPs, we automatically\nsynthesize strongest non-redundant postconditions expressed within given domain\nspecific language. We compare our approach with existing work through extensive\nexperiments.", "comment": null, "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.09550v1", "AI": {"title_translation": "形式化验证多抽象函数摘要的自动化综合", "tldr": "本文提出一个结合符号执行、大型语言模型和形式化验证的新框架，自动生成C程序的形式化验证多抽象函数摘要，以解决现有代码缺乏形式化规约和摘要生成困难的问题。", "motivation": "软件（特别是安全关键领域）需要函数摘要来理解、重用和验证，但现有任务关键型遗留代码缺乏形式化规约，且自动生成C程序函数摘要面临循环、嵌套调用、指针别名等复杂特性挑战。此外，函数摘要需要支持多抽象级别。", "method": "提出一个结合符号执行（VST-A）、大型语言模型（LLMs）和形式化验证（Frama-C）的新框架。该框架利用VST-A的符号执行精确跟踪程序执行路径和状态转换；利用LLMs基于预定义模板推断循环不变量；利用Frama-C在迭代细化循环中保证生成摘要的正确性。此外，从生成的RSPs中自动综合出在给定领域特定语言中表达的最强非冗余后置条件。", "result": "通过广泛实验与现有工作进行了比较。", "conclusion": "本文提出了一种自动化综合形式化验证多抽象函数摘要的新框架，有效解决了现有代码缺乏形式化规约和摘要生成困难的问题，并支持多抽象级别。", "translation": "函数摘要通过前置条件和后置条件来描述代码段（通常是函数）的行为，对于理解、重用和验证软件至关重要，尤其是在航空航天嵌入式系统等安全关键领域。然而，这些作为宝贵可重用资产的关键任务遗留代码通常缺乏形式化规约。由于存在循环、嵌套函数调用、指针别名等复杂特性，自动为C程序生成函数摘要具有挑战性。此外，函数摘要应支持多抽象级别以满足不同需求，例如捕获完整功能的精确摘要用于形式化验证，以及用于人类理解的直观摘要。\n为了应对这些挑战，我们首先提出了一个新颖的框架，该框架结合了符号执行、大型语言模型（LLMs）和形式化验证，以生成相对最强后置条件（RSPs）并构建完全捕获程序行为的函数摘要。我们的方法利用VST-A的符号执行来精确跟踪程序执行路径和状态转换，采用LLMs基于预定义模板推断循环不变量，并使用Frama-C在迭代细化循环中保证生成摘要的正确性。此外，从生成的RSPs中，我们自动综合出在给定领域特定语言中表达的最强非冗余后置条件。我们通过广泛的实验将我们的方法与现有工作进行了比较。", "summary": "本文针对安全关键领域软件函数摘要生成面临的挑战，提出了一种新颖的自动化框架。该框架结合符号执行、大型语言模型和形式化验证，能够为C程序自动生成形式化验证的多抽象函数摘要，特别是相对最强后置条件（RSPs）。通过利用VST-A进行精确路径跟踪、LLMs推断循环不变量以及Frama-C保证健全性，该方法旨在解决遗留代码缺乏形式化规约、复杂程序特性以及多抽象级别支持的需求。", "keywords": "函数摘要, 形式化验证, 符号执行, 大型语言模型, 自动化综合", "comments": "本文的创新点在于将符号执行、LLMs和形式化验证结合起来，形成一个端到端的自动化框架，用于生成形式化验证的函数摘要，特别是对循环不变量的推断和多抽象级别的支持，这对于理解和验证复杂遗留代码具有重要意义。"}}
{"id": "2506.09220", "title": "Beyond the Hype: Mapping Uncertainty and Gratification in AI Assistant Use", "authors": ["Karen Joy", "Tawfiq Ammari", "Alyssa Sheehan"], "summary": "This paper examines the gap between the promises and real-world performance\nof emerging AI personal assistants. Drawing on interviews with early adopters\nof devices like Rabbit R1 and Humane AI Pin, as well as services like Ohai and\nDocus, we map user experiences through the lens of Uses and Gratifications and\nUncertainty Reduction Theory. We identify three core types of user uncertainty,\nfunctional, interactional, and social, and explore how each disrupts different\nuser gratifications. We show that while marketing hype fuels initial adoption,\nunmet expectations often result in frustration or abandonment. Our findings\nhighlight the importance of transparency, task-specific design, and user\ncontrol over contextual memory and personalization. We provide design and\npolicy recommendations, including user-facing explainability tools and calls\nfor regulatory benchmarks such as CI Bench, to guide ethical and interpretable\nAI integration. Our study offers actionable insights for creating more usable,\ntrustworthy, and socially aligned AI assistants.", "comment": null, "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.09220v1", "AI": {"title_translation": "超越炒作：绘制人工智能助手使用中的不确定性和满足感", "tldr": "本文通过访谈早期用户，探讨了AI助手在营销宣传与实际表现之间的差距，识别了功能性、互动性和社会性三种不确定性，并提出了设计和政策建议以提升AI助手的可用性和信任度。", "motivation": "探讨新兴AI个人助手在营销宣传与实际表现之间的差距。", "method": "采访Rabbit R1和Humane AI Pin等设备的早期采用者，以及Ohai和Docus等服务的用户，并运用使用与满足理论和不确定性降低理论来分析用户体验。", "result": "识别出功能性、互动性和社会性三种核心用户不确定性，这些不确定性会破坏不同的用户满足感。研究表明，营销炒作推动了初步采用，但未满足的期望常导致用户沮丧或放弃。", "conclusion": "强调透明度、任务特定设计以及用户对上下文记忆和个性化控制的重要性。提供了设计和政策建议，包括用户可见的可解释性工具和监管基准（如CI Bench），以指导道德和可解释的AI集成，从而创建更可用、值得信赖和符合社会规范的AI助手。", "translation": "本文探讨了新兴AI个人助手在营销宣传与实际表现之间的差距。通过访谈Rabbit R1和Humane AI Pin等设备的早期采用者，以及Ohai和Docus等服务的使用者，我们运用使用与满足理论和不确定性降低理论来绘制用户体验。我们识别出功能性、互动性和社会性三种核心用户不确定性，并探讨了每种不确定性如何扰乱不同的用户满足感。我们发现，虽然营销炒作推动了初步采用，但未能满足的期望常常导致用户沮丧或放弃。我们的研究结果强调了透明度、任务特定设计以及用户对上下文记忆和个性化控制的重要性。我们提供了设计和政策建议，包括面向用户的可解释性工具和对监管基准（如CI Bench）的呼吁，以指导道德和可解释的AI集成。我们的研究为创建更可用、值得信赖和符合社会规范的AI助手提供了可操作的见解。", "summary": "本研究通过访谈AI助手早期用户，揭示了营销宣传与实际用户体验之间的差距。研究识别了功能性、互动性和社会性三种用户不确定性，发现它们阻碍了用户满足感，导致用户因期望落空而沮丧或放弃。论文强调了透明度、任务特定设计和用户控制的重要性，并提出了设计和政策建议，旨在促进AI助手的道德、可解释性、可用性和信任度。", "keywords": "AI助手, 用户体验, 不确定性, 使用与满足, 设计建议", "comments": "这篇论文的创新之处在于它将“使用与满足理论”和“不确定性降低理论”应用于新兴AI助手的用户体验分析，并首次提出了功能性、互动性和社会性这三种具体的用户不确定性类型。其重要性在于，它不仅揭示了AI助手“炒作”背后的实际用户痛点，还提供了具体的设计和政策建议，如强调透明度、任务特定设计和用户控制，以及引入监管基准，这对于指导AI产品的未来开发和伦理治理具有重要的实践意义。"}}
{"id": "2506.09332", "title": "Natural Language Guided Ligand-Binding Protein Design", "authors": ["Zhenqiao Song", "Ramith Hettiarachchi", "Chuan Li", "Jianwen Xie", "Lei Li"], "summary": "Can AI protein models follow human language instructions and design proteins\nwith desired functions (e.g. binding to a ligand)? Designing proteins that bind\nto a given ligand is crucial in a wide range of applications in biology and\nchemistry. Most prior AI models are trained on protein-ligand complex data,\nwhich is scarce due to the high cost and time requirements of laboratory\nexperiments. In contrast, there is a substantial body of human-curated text\ndescriptions about protein-ligand interactions and ligand formula. In this\npaper, we propose InstructPro, a family of protein generative models that\nfollow natural language instructions to design ligand-binding proteins. Given a\ntextual description of the desired function and a ligand formula in SMILES,\nInstructPro generates protein sequences that are functionally consistent with\nthe specified instructions. We develop the model architecture, training\nstrategy, and a large-scale dataset, InstructProBench, to support both training\nand evaluation. InstructProBench consists of 9,592,829 triples of (function\ndescription, ligand formula, protein sequence). We train two model variants:\nInstructPro-1B (with 1 billion parameters) and InstructPro-3B~(with 3 billion\nparameters). Both variants consistently outperform strong baselines, including\nProGen2, ESM3, and Pinal. Notably, InstructPro-1B achieves the highest docking\nsuccess rate (81.52% at moderate confidence) and the lowest average root mean\nsquare deviation (RMSD) compared to ground truth structures (4.026{\\AA}).\nInstructPro-3B further descreases the average RMSD to 2.527{\\AA}, demonstrating\nInstructPro's ability to generate ligand-binding proteins that align with the\nfunctional specifications.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09332v1", "AI": {"title_translation": "自然语言引导的配体结合蛋白设计", "tldr": "AI蛋白模型可以根据自然语言指令和配体公式设计配体结合蛋白。", "motivation": "设计结合特定配体的蛋白质在生物和化学领域应用广泛，但现有AI模型依赖稀缺的蛋白质-配体复合物数据。然而，存在大量关于蛋白质-配体相互作用和配体公式的人工整理文本描述。", "method": "提出InstructPro，一个蛋白质生成模型家族，它遵循自然语言指令来设计配体结合蛋白。给定所需功能的文本描述和SMILES格式的配体公式，InstructPro生成与指定指令功能一致的蛋白质序列。开发了模型架构、训练策略和一个大规模数据集InstructProBench（包含9,592,829个（功能描述，配体公式，蛋白质序列）三元组）。训练了InstructPro-1B和InstructPro-3B两个模型变体。", "result": "InstructPro的两个变体都持续优于强大的基线模型，包括ProGen2、ESM3和Pinal。InstructPro-1B在适度置信度下实现了最高的对接成功率（81.52%），并且与真实结构相比，平均均方根偏差（RMSD）最低（4.026Å）。InstructPro-3B进一步将平均RMSD降低到2.527Å。", "conclusion": "InstructPro展示了生成与功能规范相符的配体结合蛋白的能力。", "translation": "本论文题目为“自然语言引导的配体结合蛋白设计”。\n\n摘要：\nAI蛋白模型能否遵循人类语言指令，设计具有所需功能（例如，与配体结合）的蛋白质？设计与给定配体结合的蛋白质在生物学和化学领域的广泛应用中至关重要。大多数先前的AI模型都在蛋白质-配体复合物数据上进行训练，由于实验室实验的高成本和时间要求，这些数据非常稀缺。相反，存在大量关于蛋白质-配体相互作用和配体公式的人工整理文本描述。在本文中，我们提出了InstructPro，一个蛋白质生成模型家族，它遵循自然语言指令来设计配体结合蛋白。给定所需功能的文本描述和SMILES格式的配体公式，InstructPro生成与指定指令功能一致的蛋白质序列。我们开发了模型架构、训练策略和一个大规模数据集InstructProBench，以支持训练和评估。InstructProBench包含9,592,829个（功能描述，配体公式，蛋白质序列）三元组。我们训练了两个模型变体：InstructPro-1B（10亿参数）和InstructPro-3B（30亿参数）。两个变体都持续优于强大的基线模型，包括ProGen2、ESM3和Pinal。值得注意的是，InstructPro-1B在适度置信度下实现了最高的对接成功率（81.52%），并且与真实结构相比，平均均方根偏差（RMSD）最低（4.026Å）。InstructPro-3B进一步将平均RMSD降低到2.527Å，展示了InstructPro生成与功能规范相符的配体结合蛋白的能力。", "summary": "本论文提出了InstructPro，一个利用自然语言指令和配体SMILES公式设计配体结合蛋白的生成模型家族。为了解决蛋白质-配体复合物数据稀缺的问题，InstructPro利用了大量人工整理的蛋白质-配体相互作用文本描述。通过开发新的模型架构、训练策略和大规模数据集InstructProBench，InstructPro-1B和InstructPro-3B两个变体在对接成功率和RMSD方面均显著优于现有基线模型，证明了其生成功能一致性蛋白质序列的能力。", "keywords": "蛋白质设计, 自然语言, 配体结合, 生成模型, InstructPro", "comments": "本文的创新之处在于利用自然语言指令和丰富的文本数据来指导蛋白质设计，克服了传统方法对稀缺复合物数据的依赖。InstructProBench作为大规模数据集的贡献也十分重要。模型的性能指标（高对接成功率和低RMSD）表明其在生成功能性配体结合蛋白方面的强大能力。"}}
{"id": "2506.09947", "title": "KI4Demokratie: An AI-Based Platform for Monitoring and Fostering Democratic Discourse", "authors": ["Rudy Alexandro Garrido Veliz", "Till Nikolaus Schaland", "Simon Bergmoser", "Florian Horwege", "Somya Bansal", "Ritesh Nahar", "Martin Semmann", "Jörg Forthmann", "Seid Muhie Yimam"], "summary": "Social media increasingly fuel extremism, especially right-wing extremism,\nand enable the rapid spread of antidemocratic narratives. Although AI and data\nscience are often leveraged to manipulate political opinion, there is a\ncritical need for tools that support effective monitoring without infringing on\nfreedom of expression. We present KI4Demokratie, an AI-based platform that\nassists journalists, researchers, and policymakers in monitoring right-wing\ndiscourse that may undermine democratic values. KI4Demokratie applies machine\nlearning models to a large-scale German online data gathered on a daily basis,\nproviding a comprehensive view of trends in the German digital sphere. Early\nanalysis reveals both the complexity of tracking organized extremist behavior\nand the promise of our integrated approach, especially during key events.", "comment": null, "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.09947v1", "AI": {"title_translation": "KI4Demokratie：一个用于监测和促进民主话语的AI平台", "tldr": "KI4Demokratie是一个AI平台，旨在帮助记者、研究人员和政策制定者监测德国数字领域可能破坏民主价值观的右翼言论。", "motivation": "社交媒体日益助长极端主义，特别是右翼极端主义，并使反民主叙事迅速传播。尽管人工智能和数据科学常被用于操纵政治观点，但迫切需要能够在不侵犯言论自由的情况下支持有效监控的工具。", "method": "KI4Demokratie平台将机器学习模型应用于每日收集的大规模德国在线数据。", "result": "早期分析揭示了追踪有组织极端主义行为的复杂性，以及该集成方法的潜力，尤其是在关键事件期间。", "conclusion": "该平台在监测和促进民主话语方面显示出潜力，通过追踪极端主义叙事来帮助保护民主价值观。", "translation": "社交媒体日益助长极端主义，尤其是右翼极端主义，并使反民主叙事迅速传播。尽管人工智能和数据科学常被用于操纵政治观点，但迫切需要能够在不侵犯言论自由的情况下支持有效监控的工具。我们提出了KI4Demokratie，一个基于人工智能的平台，旨在协助记者、研究人员和政策制定者监测可能破坏民主价值观的右翼言论。KI4Demokratie将机器学习模型应用于每日收集的大规模德国在线数据，提供了德国数字领域趋势的全面视图。早期分析揭示了追踪有组织极端主义行为的复杂性以及我们集成方法的潜力，尤其是在关键事件期间。", "summary": "本文介绍了KI4Demokratie，一个基于AI的平台，旨在帮助记者、研究人员和政策制定者监测德国在线数据中的右翼极端主义话语，以保护民主价值观。该平台利用机器学习模型分析每日收集的大规模德国在线数据，旨在提供德国数字领域趋势的全面视图。初步分析表明，尽管追踪有组织极端主义行为复杂，但该集成方法在监测关键事件方面显示出潜力。", "keywords": "AI平台, 民主话语, 极端主义, 社交媒体监控, 机器学习", "comments": "该论文提出了一种创新的AI平台，旨在解决社交媒体上极端主义言论扩散的问题，同时兼顾言论自由。其重要性在于提供了一个工具，赋能专业人士有效监测反民主叙事。该平台的价值在于其对大规模在线数据的机器学习应用，以及在保护民主价值观方面的潜在作用。"}}
{"id": "2506.09331", "title": "Multi-Agent Language Models: Advancing Cooperation, Coordination, and Adaptation", "authors": ["Arjun Vaithilingam Sudhakar"], "summary": "Modern Large Language Models (LLMs) exhibit impressive zero-shot and few-shot\ngeneralization capabilities across complex natural language tasks, enabling\ntheir widespread use as virtual assistants for diverse applications such as\ntranslation and summarization. Despite being trained solely on large corpora of\ntext without explicit supervision on author intent, LLMs appear to infer the\nunderlying meaning of textual interactions. This raises a fundamental question:\ncan LLMs model and reason about the intentions of others, i.e., do they possess\na form of theory of mind? Understanding other's intentions is crucial for\neffective collaboration, which underpins human societal success and is\nessential for cooperative interactions among multiple agents, including humans\nand autonomous systems. In this work, we investigate the theory of mind in LLMs\nthrough the lens of cooperative multi-agent reinforcement learning (MARL),\nwhere agents learn to collaborate via repeated interactions, mirroring human\nsocial reasoning. Our approach aims to enhance artificial agent's ability to\nadapt and cooperate with both artificial and human partners. By leveraging\nLLM-based agents capable of natural language interaction, we move towards\ncreating hybrid human-AI systems that can foster seamless collaboration, with\nbroad implications for the future of human-artificial interaction.", "comment": "arXiv admin note: substantial text overlap with arXiv:2311.07687", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.09331v1", "AI": {"title_translation": "多智能体语言模型：推进合作、协调与适应", "tldr": "本文探讨了大型语言模型（LLMs）是否具备心智理论，即理解他人意图的能力，并通过合作式多智能体强化学习（MARL）来提升AI智能体与人类及其他AI的合作与适应能力，旨在构建无缝的人机协作系统。", "motivation": "现代大型语言模型（LLMs）在自然语言任务中表现出色，但其是否能像人类一样理解并推理他人的意图（即具备心智理论）是一个基本问题。理解他人意图对于有效的协作至关重要，而协作是人类社会成功的基石，也是多智能体（包括人类和自主系统）之间互动所必需的。因此，研究LLMs的这一能力对于提升人工智能与人类的协作至关重要。", "method": "本研究通过合作式多智能体强化学习（MARL）的视角来探究大型语言模型（LLMs）的心智理论，其中智能体通过重复交互学习协作，这与人类的社会推理过程相似。该方法旨在增强人工智能体与人工和人类伙伴的适应和合作能力，并利用基于LLM的智能体进行自然语言交互。", "result": "Not mentioned in abstract", "conclusion": "本研究旨在通过利用基于大型语言模型（LLMs）的智能体进行自然语言交互，创建能够促进无缝协作的混合人机系统，这将对未来的人机交互产生广泛影响。", "translation": "现代大型语言模型（LLMs）在复杂的自然语言任务中表现出令人印象深刻的零样本和少样本泛化能力，使其作为虚拟助手广泛应用于翻译和摘要等多种场景。尽管仅通过大量文本语料库进行训练，没有明确监督作者意图，LLMs似乎能够推断文本交互的潜在含义。这引发了一个根本性问题：LLMs能否建模和推理他人的意图，即它们是否拥有一种心智理论？理解他人的意图对于有效的协作至关重要，而协作是人类社会成功的基石，也是包括人类和自主系统在内的多智能体之间合作互动的必要条件。在这项工作中，我们通过合作式多智能体强化学习（MARL）的视角来研究LLMs中的心智理论，其中智能体通过重复交互学习协作，这反映了人类的社会推理。我们的方法旨在增强人工智能体与人工和人类伙伴的适应和合作能力。通过利用能够进行自然语言交互的基于LLM的智能体，我们正朝着创建能够促进无缝协作的混合人机系统迈进，这对未来的人工智能交互具有广泛影响。", "summary": "本文探讨了大型语言模型（LLMs）是否具备理解他人意图的“心智理论”能力，这对于人机及多智能体协作至关重要。研究采用合作式多智能体强化学习（MARL）方法，旨在提升LLM驱动的智能体在自然语言交互中与人类及其他AI的合作与适应能力，以期构建更高效的混合人机协作系统。", "keywords": "多智能体语言模型, 心智理论, 合作, 强化学习, 人机协作", "comments": "本文提出将大型语言模型（LLMs）与多智能体强化学习（MARL）结合，以探究LLMs的心智理论能力，并提升其在复杂多智能体环境中的合作与适应性。其创新点在于将LLMs的自然语言理解能力与MARL的协作学习机制相结合，旨在弥合LLMs在理解“意图”方面的不足，从而为构建更高级、更智能的人机协作系统奠定基础。这种研究方向对于实现真正无缝、高效的人工智能与人类社会融合具有重要意义。"}}
{"id": "2506.09392", "title": "Voltage-Controlled Oscillator and Memristor-Based Analog Computing for Solving Systems of Linear Equations", "authors": ["Hao Li", "Rizwan S. Peerla", "Frank Barrows", "Francesco Caravelli", "Bibhu Datta Sahoo"], "summary": "Matrix computations have become increasingly significant in many data-driven\napplications. However, Moores law for digital computers has been gradually\napproaching its limit in recent years. Moreover, digital computers encounter\nsubstantial complexity when performing matrix computations and need a long time\nto finish the computations, and existing analog matrix computation schemes\nrequire a large chip area and power consumption. This paper proposes a linear\nalgebra system of equations based on integrators, which features low power\nconsumption, compact area, and fast computation time. Due to the simple\nstructure of the ring oscillator, the ring oscillator-based integrator exhibits\na compact area and low power consumption. Therefore, ring oscillator-based\nintegrators are introduced into the linear algebra system of equations, and\nthis system can be used to compute the linear algebra equations of the matrix\nwith either positive or negative values. This paper provides a detailed\nanalysis and verification of the proposed circuit structure. Compared to\nsimilar circuits, this work has significant advantages in terms of area, power\nconsumption, and computation speed.", "comment": "11 pages, 22 figures, Journal", "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.09392v1", "AI": {"title_translation": "基于压控振荡器和忆阻器的模拟计算求解线性方程组", "tldr": "本文提出了一种基于环形振荡器积分器的线性代数方程系统，该系统具有低功耗、紧凑面积和快速计算的特点，并可用于求解正负值的矩阵线性代数方程。", "motivation": "数字计算机在矩阵计算方面面临摩尔定律限制、复杂性高、计算时间长的问题；现有模拟矩阵计算方案则需要较大的芯片面积和功耗。", "method": "本文提出了一种基于积分器的线性代数方程系统，引入了基于环形振荡器的积分器，利用其结构简单、面积紧凑和低功耗的特点。该系统可以处理正负值的矩阵线性代数方程，并对所提出的电路结构进行了详细分析和验证。", "result": "与同类电路相比，该工作在面积、功耗和计算速度方面具有显著优势。", "conclusion": "本文提出的基于环形振荡器积分器的线性代数方程系统在模拟计算中实现了矩阵计算的低功耗、紧凑面积和高速率，有效解决了现有方法的局限性。", "translation": "矩阵计算在许多数据驱动应用中变得越来越重要。然而，近年来数字计算机的摩尔定律逐渐接近其极限。此外，数字计算机在执行矩阵计算时会遇到巨大的复杂性，需要很长时间才能完成计算，而现有的模拟矩阵计算方案则需要较大的芯片面积和功耗。本文提出了一种基于积分器的线性代数方程系统，该系统具有低功耗、紧凑面积和快速计算时间的特点。由于环形振荡器结构简单，基于环形振荡器的积分器具有紧凑的面积和低功耗。因此，将基于环形振荡器的积分器引入线性代数方程系统，该系统可用于计算具有正值或负值的矩阵的线性代数方程。本文对所提出的电路结构进行了详细分析和验证。与同类电路相比，这项工作在面积、功耗和计算速度方面具有显著优势。", "summary": "针对数字计算机在矩阵计算中遇到的局限性以及现有模拟方案的不足，本文提出了一种基于环形振荡器积分器的线性代数方程系统。该系统利用环形振荡器的简单结构，实现了低功耗、紧凑的芯片面积和快速的计算速度，并能够处理含正负值的矩阵线性方程组。实验验证表明，该方案在性能上优于现有同类电路。", "keywords": "模拟计算, 线性方程组, 忆阻器, 压控振荡器, 矩阵计算", "comments": "本文创新性地将环形振荡器积分器应用于线性代数方程系统求解，有效解决了模拟矩阵计算中面积和功耗过大的问题，并提升了计算速度，为未来低功耗、高性能模拟计算提供了新的思路。"}}
{"id": "2506.09647", "title": "Real-Time Network Traffic Forecasting with Missing Data: A Generative Model Approach", "authors": ["Lei Deng", "Wenhan Xu", "Jingwei Li", "Danny H. K. Tsang"], "summary": "Real-time network traffic forecasting is crucial for network management and\nearly resource allocation. Existing network traffic forecasting approaches\noperate under the assumption that the network traffic data is fully observed.\nHowever, in practical scenarios, the collected data are often incomplete due to\nvarious human and natural factors. In this paper, we propose a generative model\napproach for real-time network traffic forecasting with missing data. Firstly,\nwe model the network traffic forecasting task as a tensor completion problem.\nSecondly, we incorporate a pre-trained generative model to achieve the low-rank\nstructure commonly associated with tensor completion. The generative model\neffectively captures the intrinsic low-rank structure of network traffic data\nduring pre-training and enables the mapping from a compact latent\nrepresentation to the tensor space. Thirdly, rather than directly optimizing\nthe high-dimensional tensor, we optimize its latent representation, which\nsimplifies the optimization process and enables real-time forecasting. We also\nestablish a theoretical recovery guarantee that quantifies the error bound of\nthe proposed approach. Experiments on real-world datasets demonstrate that our\napproach achieves accurate network traffic forecasting within 100 ms, with a\nmean absolute error (MAE) below 0.002, as validated on the Abilene dataset.", "comment": null, "cate": "cs.NI", "url": "http://arxiv.org/abs/2506.09647v1", "AI": {"title_translation": "实时网络流量预测与缺失数据：一种生成模型方法", "tldr": "本文提出了一种基于生成模型的实时网络流量预测方法，能够有效处理缺失数据，并通过优化潜在表示实现快速准确的预测。", "motivation": "实时网络流量预测对于网络管理和早期资源分配至关重要。然而，现有方法假设数据完整，这与实际场景中因各种人为和自然因素导致的数据不完整性不符。", "method": "本文将网络流量预测任务建模为张量补全问题。首先，引入一个预训练的生成模型来捕获网络流量数据的内在低秩结构。其次，通过优化张量的紧凑潜在表示而非直接优化高维张量，简化了优化过程并实现了实时预测。此外，还建立了理论恢复保证以量化方法的误差界限。", "result": "在真实世界数据集上的实验表明，该方法可以在100毫秒内实现准确的网络流量预测，并且在Abilene数据集上的平均绝对误差（MAE）低于0.002。", "conclusion": "本文提出了一种新颖的生成模型方法，有效解决了实时网络流量预测中缺失数据的问题，实现了快速且准确的预测，并提供了理论保证。", "translation": "实时网络流量预测对于网络管理和早期资源分配至关重要。现有的网络流量预测方法都假设网络流量数据是完全观测到的。然而，在实际场景中，由于各种人为和自然因素，收集到的数据往往是不完整的。在本文中，我们提出了一种用于处理缺失数据的实时网络流量预测的生成模型方法。首先，我们将网络流量预测任务建模为张量补全问题。其次，我们引入了一个预训练的生成模型，以实现与张量补全通常相关的低秩结构。生成模型在预训练期间有效地捕获了网络流量数据的内在低秩结构，并实现了从紧凑的潜在表示到张量空间的映射。第三，我们不是直接优化高维张量，而是优化其潜在表示，这简化了优化过程并实现了实时预测。我们还建立了量化所提出方法误差界限的理论恢复保证。在真实世界数据集上的实验表明，我们的方法在100毫秒内实现了准确的网络流量预测，平均绝对误差（MAE）低于0.002，这在Abilene数据集上得到了验证。", "summary": "本文针对实时网络流量预测中常见的缺失数据问题，提出了一种基于生成模型的解决方案。该方法将预测任务视为张量补全问题，利用预训练生成模型捕捉数据低秩特性，并通过优化紧凑的潜在表示实现高效且实时的预测。实验证明，该方法在真实数据集上能实现快速（100毫秒内）和高精度（MAE低于0.002）的预测，并提供了理论误差界限。", "keywords": "网络流量预测, 缺失数据, 生成模型, 张量补全, 实时", "comments": "这项工作通过引入生成模型来处理网络流量预测中的缺失数据问题，并将其转化为张量补全任务，具有创新性。特别值得注意的是，它通过优化潜在表示而非高维张量来确保实时性能，这对于实际网络管理至关重要。此外，提供理论恢复保证增强了其可靠性。该方法在速度和准确性上的表现使其在实际应用中具有重要价值。"}}
{"id": "2506.09549", "title": "A Study on Speech Assessment with Visual Cues", "authors": ["Shafique Ahmed", "Ryandhimas E. Zezario", "Nasir Saleem", "Amir Hussain", "Hsin-Min Wang", "Yu Tsao"], "summary": "Non-intrusive assessment of speech quality and intelligibility is essential\nwhen clean reference signals are unavailable. In this work, we propose a\nmultimodal framework that integrates audio features and visual cues to predict\nPESQ and STOI scores. It employs a dual-branch architecture, where spectral\nfeatures are extracted using STFT, and visual embeddings are obtained via a\nvisual encoder. These features are then fused and processed by a CNN-BLSTM with\nattention, followed by multi-task learning to simultaneously predict PESQ and\nSTOI. Evaluations on the LRS3-TED dataset, augmented with noise from the DEMAND\ncorpus, show that our model outperforms the audio-only baseline. Under seen\nnoise conditions, it improves LCC by 9.61% (0.8397->0.9205) for PESQ and 11.47%\n(0.7403->0.8253) for STOI. These results highlight the effectiveness of\nincorporating visual cues in enhancing the accuracy of non-intrusive speech\nassessment.", "comment": "Accepted to Interspeech 2025", "cate": "eess.AS", "url": "http://arxiv.org/abs/2506.09549v1", "AI": {"title_translation": "基于视觉线索的语音评估研究", "tldr": "该研究提出一个结合音频和视觉线索的多模态框架，用于非侵入式语音质量和可懂度评估，在LRS3-TED数据集上表现优于纯音频基线。", "motivation": "当没有干净的参考信号时，非侵入式语音质量和可懂度评估至关重要。", "method": "提出一个多模态框架，整合音频特征和视觉线索来预测PESQ和STOI分数。该框架采用双分支架构，通过STFT提取频谱特征，通过视觉编码器获取视觉嵌入。这些特征随后被融合，并通过带注意力机制的CNN-BLSTM进行处理，最后通过多任务学习同时预测PESQ和STOI。", "result": "在LRS3-TED数据集（通过DEMAND语料库中的噪声增强）上的评估显示，该模型优于纯音频基线。在已知噪声条件下，PESQ的LCC提高了9.61%（从0.8397到0.9205），STOI的LCC提高了11.47%（从0.7403到0.8253）。", "conclusion": "这些结果强调了结合视觉线索在提高非侵入式语音评估准确性方面的有效性。", "translation": "当没有干净的参考信号时，非侵入式语音质量和可懂度评估至关重要。在这项工作中，我们提出了一个多模态框架，它整合了音频特征和视觉线索来预测PESQ和STOI分数。它采用双分支架构，其中使用STFT提取频谱特征，并通过视觉编码器获取视觉嵌入。这些特征随后被融合，并通过带注意力机制的CNN-BLSTM进行处理，最后通过多任务学习同时预测PESQ和STOI。在LRS3-TED数据集（通过DEMAND语料库中的噪声增强）上的评估显示，我们的模型优于纯音频基线。在已知噪声条件下，PESQ的LCC提高了9.61%（0.8397->0.9205），STOI的LCC提高了11.47%（0.7403->0.8253）。这些结果强调了结合视觉线索在提高非侵入式语音评估准确性方面的有效性。", "summary": "本研究提出一种新颖的多模态框架，通过结合音频频谱特征和视觉嵌入，实现对语音质量（PESQ）和可懂度（STOI）的非侵入式评估。该框架采用双分支CNN-BLSTM架构，并利用多任务学习同时预测两项指标。在LRS3-TED数据集上的实验结果表明，与纯音频基线相比，该模型在噪声环境下显著提高了评估准确性，验证了视觉线索在语音评估中的重要作用。", "keywords": "语音评估, 视觉线索, 多模态, 非侵入式, PESQ, STOI", "comments": "该研究的创新之处在于将视觉线索引入非侵入式语音评估，解决了在无干净参考信号情况下的评估难题。通过多模态融合和双分支架构，模型能够更准确地预测语音质量和可懂度，显著优于传统的纯音频方法。这为未来语音处理领域，尤其是在嘈杂或复杂环境下的语音评估提供了新的思路和技术支持。"}}
{"id": "2506.09275", "title": "A Survey of End-to-End Modeling for Distributed DNN Training: Workloads, Simulators, and TCO", "authors": ["Jonas Svedas", "Hannah Watson", "Nathan Laubeuf", "Diksha Moolchandani", "Abubakr Nada", "Arjun Singh", "Dwaipayan Biswas", "James Myers", "Debjyoti Bhattacharjee"], "summary": "Distributed deep neural networks (DNNs) have become a cornerstone for scaling\nmachine learning to meet the demands of increasingly complex applications.\nHowever, the rapid growth in model complexity far outpaces CMOS technology\nscaling, making sustainable and efficient system design a critical challenge.\nAddressing this requires coordinated co-design across software, hardware, and\ntechnology layers. Due to the prohibitive cost and complexity of deploying\nfull-scale training systems, simulators play a pivotal role in enabling this\ndesign exploration. This survey reviews the landscape of distributed DNN\ntraining simulators, focusing on three major dimensions: workload\nrepresentation, simulation infrastructure, and models for total cost of\nownership (TCO) including carbon emissions. It covers how workloads are\nabstracted and used in simulation, outlines common workload representation\nmethods, and includes comprehensive comparison tables covering both simulation\nframeworks and TCO/emissions models, detailing their capabilities, assumptions,\nand areas of focus. In addition to synthesizing existing tools, the survey\nhighlights emerging trends, common limitations, and open research challenges\nacross the stack. By providing a structured overview, this work supports\ninformed decision-making in the design and evaluation of distributed training\nsystems.", "comment": null, "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.09275v1", "AI": {"title_translation": "分布式深度神经网络训练端到端建模综述：工作负载、模拟器和总拥有成本", "tldr": "本文综述了分布式DNN训练模拟器，涵盖工作负载表示、模拟基础设施和总拥有成本模型，旨在支持分布式训练系统的设计和评估。", "motivation": "随着深度神经网络模型复杂度的快速增长远超CMOS技术扩展速度，使得可持续高效的系统设计成为一个关键挑战。由于部署全尺寸训练系统的成本高昂且复杂，模拟器在实现这种设计探索中发挥着关键作用。", "method": "本文通过综述现有分布式DNN训练模拟器，重点关注工作负载表示、模拟基础设施和总拥有成本（包括碳排放）模型三个主要维度。它涵盖了工作负载如何抽象和用于模拟，概述了常见的工作负载表示方法，并提供了包含模拟框架和TCO/排放模型的综合比较表，详细说明了它们的功能、假设和关注领域。", "result": "综述了分布式DNN训练模拟器的现状，涵盖了工作负载的抽象和使用、常见的工作负载表示方法，并提供了模拟框架和TCO/排放模型的综合比较表。同时，强调了新兴趋势、常见局限性和开放研究挑战。", "conclusion": "本文通过提供结构化概述，支持分布式训练系统设计和评估中的知情决策。", "translation": "分布式深度神经网络（DNN）已成为扩展机器学习以满足日益复杂应用需求的基础。然而，模型复杂度的快速增长远超CMOS技术扩展速度，使得可持续高效的系统设计成为一个关键挑战。解决这一问题需要软件、硬件和技术层面的协同设计。由于部署全尺寸训练系统的成本高昂且复杂，模拟器在实现这种设计探索中发挥着关键作用。本综述回顾了分布式DNN训练模拟器的现状，重点关注三个主要维度：工作负载表示、模拟基础设施以及包括碳排放的总拥有成本（TCO）模型。它涵盖了工作负载如何被抽象并用于模拟，概述了常见的工作负载表示方法，并包含了涵盖模拟框架和TCO/排放模型的综合比较表，详细说明了它们的功能、假设和关注领域。除了综合现有工具之外，本综述还强调了整个堆栈中新兴趋势、常见局限性和开放研究挑战。通过提供结构化的概述，这项工作支持分布式训练系统设计和评估中的知情决策。", "summary": "这篇综述深入探讨了分布式深度神经网络（DNN）训练的端到端建模，重点关注工作负载、模拟器和总拥有成本（TCO）。鉴于DNN模型复杂性快速增长与CMOS技术扩展之间的差距，以及部署实际训练系统的高昂成本，模拟器在系统设计中变得至关重要。该论文审查了分布式DNN训练模拟器的现状，涵盖了工作负载表示、模拟基础设施和TCO（包括碳排放）模型。它提供了现有工具的结构化概述、比较表，并指出了新兴趋势、局限性和开放研究挑战，旨在为分布式训练系统的设计和评估提供信息支持。", "keywords": "分布式DNN训练, 模拟器, 工作负载, 总拥有成本, 碳排放", "comments": "这篇综述论文具有重要的实际意义，因为它直接解决了当前在扩展分布式DNN训练中面临的效率和可持续性挑战。通过系统地梳理和比较现有模拟器，并引入TCO和碳排放考量，它为研究人员和工程师提供了一个全面的设计和评估框架。其价值在于整合了跨软件、硬件和技术栈的视角，并指出未来的研究方向，有助于推动更节能和经济的AI系统发展。"}}
{"id": "2506.09186", "title": "Not all those who drift are lost: Drift correction and calibration scheduling for the IoT", "authors": ["Aaron Hurst", "Andrey V. Kalinichev", "Klaus Koren", "Daniel E. Lucani"], "summary": "Sensors provide a vital source of data that link digital systems with the\nphysical world. However, as sensors age, the relationship between what they\nmeasure and what they output changes. This is known as sensor drift and poses a\nsignificant challenge that, combined with limited opportunity for\nre-calibration, can severely limit data quality over time. Previous approaches\nto drift correction typically require large volumes of ground truth data and do\nnot consider measurement or prediction uncertainty. In this paper, we propose a\nprobabilistic sensor drift correction method that takes a fundamental approach\nto modelling the sensor response using Gaussian Process Regression. Tested\nusing dissolved oxygen sensors, our method delivers mean squared error (MSE)\nreductions of up to 90% and more than 20% on average. We also propose a novel\nuncertainty-driven calibration schedule optimisation approach that builds on\ntop of drift correction and further reduces MSE by up to 15.7%.", "comment": null, "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.09186v1", "AI": {"title_translation": "并非所有漂移者都迷失：物联网中的漂移校正与校准调度", "tldr": "传感器会随时间漂移，降低数据质量。本文提出了一种使用高斯过程回归的概率漂移校正方法，显著降低了均方误差（MSE），并提出了一种不确定性驱动的校准调度优化方法以进一步改进。", "motivation": "传感器随着老化会发生漂移，导致测量值与输出值之间的关系发生变化，这严重限制了数据质量，尤其是在重新校准机会有限的情况下。以往的漂移校正方法通常需要大量真实数据，并且不考虑测量或预测的不确定性。", "method": "本文提出了一种概率传感器漂移校正方法，该方法使用高斯过程回归对传感器响应进行建模。此外，还提出了一种新颖的、基于不确定性驱动的校准调度优化方法，该方法建立在漂移校正之上。", "result": "漂移校正方法在溶解氧传感器测试中，使均方误差（MSE）平均降低了20%以上，最高可达90%。校准调度优化方法在此基础上进一步将MSE降低了15.7%。", "conclusion": "所提出的概率漂移校正和不确定性驱动的校准调度方法，通过有效缓解传感器漂移和优化重新校准，显著提高了物联网中的传感器数据质量。", "translation": "传感器是连接数字系统与物理世界的关键数据来源。然而，随着传感器老化，其测量值与输出值之间的关系会发生变化。这被称为传感器漂移，构成了一个重大挑战，再加上重新校准机会有限，会严重限制数据随时间推移的质量。以往的漂移校正方法通常需要大量的真实数据，并且不考虑测量或预测的不确定性。在本文中，我们提出了一种概率传感器漂移校正方法，该方法采用高斯过程回归对传感器响应进行建模。经溶解氧传感器测试，我们的方法使均方误差（MSE）平均降低了20%以上，最高可达90%。我们还提出了一种新颖的、基于不确定性驱动的校准调度优化方法，该方法建立在漂移校正之上，并进一步将MSE降低了15.7%。", "summary": "本文旨在解决物联网中因传感器老化和校准机会有限导致的传感器漂移及数据质量下降问题。论文提出了一种基于高斯过程回归的概率传感器漂移校正方法，该方法能够将均方误差（MSE）降低高达90%。此外，还引入了一种不确定性驱动的校准调度优化方法，能在漂移校正的基础上进一步将MSE降低15.7%，从而有效维护长期传感器部署的数据质量。", "keywords": "传感器漂移, 物联网, 高斯过程回归, 校准, 数据质量", "comments": "该论文通过提出一种基于高斯过程回归的概率方法来解决物联网中传感器漂移的关键问题，具有创新性。它不仅从根本上模拟了传感器响应并考虑了不确定性，还通过不确定性驱动的校准调度优化进一步增强了方法的实用性，为长期传感器部署的数据质量维护提供了一个全面的解决方案。"}}
{"id": "2506.09100", "title": "Low-Rank Augmented Implicit Neural Representation for Unsupervised High-Dimensional Quantitative MRI Reconstruction", "authors": ["Haonan Zhang", "Guoyan Lao", "Yuyao Zhang", "Hongjiang Wei"], "summary": "Quantitative magnetic resonance imaging (qMRI) provides tissue-specific\nparameters vital for clinical diagnosis. Although simultaneous multi-parametric\nqMRI (MP-qMRI) technologies enhance imaging efficiency, robustly reconstructing\nqMRI from highly undersampled, high-dimensional measurements remains a\nsignificant challenge. This difficulty arises primarily because current\nreconstruction methods that rely solely on a single prior or physics-informed\nmodel to solve the highly ill-posed inverse problem, which often leads to\nsuboptimal results. To overcome this limitation, we propose LoREIN, a novel\nunsupervised and dual-prior-integrated framework for accelerated 3D MP-qMRI\nreconstruction. Technically, LoREIN incorporates both low-rank prior and\ncontinuity prior via low-rank representation (LRR) and implicit neural\nrepresentation (INR), respectively, to enhance reconstruction fidelity. The\npowerful continuous representation of INR enables the estimation of optimal\nspatial bases within the low-rank subspace, facilitating high-fidelity\nreconstruction of weighted images. Simultaneously, the predicted multi-contrast\nweighted images provide essential structural and quantitative guidance, further\nenhancing the reconstruction accuracy of quantitative parameter maps.\nFurthermore, our work introduces a zero-shot learning paradigm with broad\npotential in complex spatiotemporal and high-dimensional image reconstruction\ntasks, further advancing the field of medical imaging.", "comment": null, "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.09100v1", "AI": {"title_translation": "低秩增强隐式神经表示用于无监督高维定量MRI重建", "tldr": "LoREIN是一种结合低秩和隐式神经表示的新框架，用于无监督加速高维定量MRI重建，解决了传统方法在欠采样高维数据重建中的不足。", "motivation": "尽管同步多参数qMRI提高了成像效率，但从高度欠采样的、高维测量中稳健重建qMRI仍然是一个重大挑战。现有方法仅依赖单一先验或物理信息模型，导致次优结果。", "method": "本文提出了LoREIN，一个无监督、双先验集成框架，用于加速3D MP-qMRI重建。LoREIN通过低秩表示（LRR）和隐式神经表示（INR）分别整合了低秩先验和连续性先验，以增强重建保真度。INR的连续表示能够估计低秩子空间内的最佳空间基，促进加权图像的高保真重建。预测的多对比度加权图像提供结构和定量指导，进一步提高定量参数图的重建精度。此外，本文引入了一种零样本学习范式。", "result": "实现了加权图像的高保真重建和定量参数图的重建精度提高。", "conclusion": "LoREIN通过整合双先验和引入零样本学习范式，显著提升了高维qMRI重建的性能，并具有在复杂时空和高维图像重建任务中的广泛潜力，进一步推动了医学影像领域的发展。", "translation": "定量磁共振成像（qMRI）提供对临床诊断至关重要的组织特异性参数。尽管同步多参数qMRI（MP-qMRI）技术提高了成像效率，但从高度欠采样的、高维测量中稳健重建qMRI仍然是一个重大挑战。这一困难主要源于当前的重建方法仅依赖单一先验或物理信息模型来解决高度不适定的逆问题，这通常导致次优结果。为了克服这一限制，我们提出了LoREIN，一个新颖的无监督、双先验集成框架，用于加速3D MP-qMRI重建。从技术上讲，LoREIN通过低秩表示（LRR）和隐式神经表示（INR）分别整合了低秩先验和连续性先验，以增强重建保真度。INR强大的连续表示使得在低秩子空间内估计最佳空间基成为可能，从而促进加权图像的高保真重建。同时，预测的多对比度加权图像提供必要的结构和定量指导，进一步提高定量参数图的重建精度。此外，我们的工作引入了一种零样本学习范式，在复杂时空和高维图像重建任务中具有广阔潜力，进一步推动了医学影像领域的发展。", "summary": "本文提出了LoREIN，一种无监督、双先验集成框架，用于加速高维定量磁共振成像（qMRI）重建。该框架通过结合低秩表示（LRR）和隐式神经表示（INR）来利用低秩先验和连续性先验，以解决传统方法在高度欠采样数据重建中的局限性。LoREIN能够实现高保真加权图像重建和提高定量参数图的精度，并引入了零样本学习范式，有望应用于更复杂的医学图像重建任务。", "keywords": "定量MRI重建, 低秩表示, 隐式神经表示, 无监督学习, 零样本学习", "comments": "LoREIN的创新之处在于其双先验集成框架，结合了低秩表示和隐式神经表示，有效解决了高维欠采样qMRI重建的挑战。特别是引入零样本学习范式，为医学影像领域的高维和时空图像重建提供了新的思路和广阔的应用潜力，提升了模型在数据稀缺场景下的泛化能力。"}}
{"id": "2506.09107", "title": "FAIRTOPIA: Envisioning Multi-Agent Guardianship for Disrupting Unfair AI Pipelines", "authors": ["Athena Vakali", "Ilias Dimitriadis"], "summary": "AI models have become active decision makers, often acting without human\nsupervision. The rapid advancement of AI technology has already caused harmful\nincidents that have hurt individuals and societies and AI unfairness in heavily\ncriticized. It is urgent to disrupt AI pipelines which largely neglect human\nprinciples and focus on computational biases exploration at the data (pre),\nmodel(in), and deployment (post) processing stages. We claim that by exploiting\nthe advances of agents technology, we will introduce cautious, prompt, and\nongoing fairness watch schemes, under realistic, systematic, and human-centric\nfairness expectations. We envision agents as fairness guardians, since agents\nlearn from their environment, adapt to new information, and solve complex\nproblems by interacting with external tools and other systems. To set the\nproper fairness guardrails in the overall AI pipeline, we introduce a\nfairness-by-design approach which embeds multi-role agents in an end-to-end\n(human to AI) synergetic scheme. Our position is that we may design adaptive\nand realistic AI fairness frameworks, and we introduce a generalized algorithm\nwhich can be customized to the requirements and goals of each AI decision\nmaking scenario. Our proposed, so called FAIRTOPIA framework, is structured\nover a three-layered architecture, which encapsulates the AI pipeline inside an\nagentic guardian and a knowledge-based, self-refining layered scheme. Based on\nour proposition, we enact fairness watch in all of the AI pipeline stages,\nunder robust multi-agent workflows, which will inspire new fairness research\nhypothesis, heuristics, and methods grounded in human-centric, systematic,\ninterdisciplinary, socio-technical principles.", "comment": "11 pages, 4 figures", "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.09107v1", "AI": {"title_translation": "FAIRTOPIA：设想多智能体守护以颠覆不公平的AI管道", "tldr": "该论文提出了FAIRTOPIA，一个多智能体框架，旨在通过在AI管道的各个阶段引入谨慎、及时和持续的公平性监控，来解决AI的不公平问题，并遵循以人为本的原则。", "motivation": "AI模型已成为缺乏人类监督的活跃决策者，导致有害事件并受到广泛批评。当前AI管道忽视人类原则，仅关注数据（预处理）、模型（处理中）和部署（后处理）阶段的计算偏差，因此迫切需要颠覆这些不公平的AI管道。", "method": "论文提出了FAIRTOPIA框架，这是一种公平性嵌入式设计方法，将多角色智能体嵌入到端到端（人到AI）的协同方案中。它引入了一个可定制的通用算法，并构建了一个三层架构，将AI管道封装在智能体守护和基于知识的自精炼分层方案中。该框架通过强大的多智能体工作流，在AI管道的所有阶段实施公平性监控。", "result": "FAIRTOPIA框架允许设计自适应和现实的AI公平性框架，并且可以根据每个AI决策场景的需求和目标进行定制。它将启发新的公平性研究假设、启发式方法和方法，这些都植根于以人为本、系统性、跨学科的社会技术原则。", "conclusion": "通过引入FAIRTOPIA框架，可以在AI管道的所有阶段实施公平性监控，并通过强大的多智能体工作流，启发基于以人为本、系统性、跨学科的社会技术原则的公平性研究。", "translation": "AI模型已成为活跃的决策者，通常在没有人为监督的情况下运行。AI技术的快速发展已经导致了伤害个人和社会的有害事件，AI的不公平性受到了严厉批评。迫切需要颠覆那些在很大程度上忽视人类原则、只关注数据（预处理）、模型（处理中）和部署（后处理）阶段计算偏差的AI管道。我们声称，通过利用智能体技术的进步，我们将在现实、系统和以人为本的公平性期望下，引入谨慎、及时和持续的公平性监控方案。我们将智能体设想为公平性守护者，因为智能体可以从环境中学习，适应新信息，并通过与外部工具和其他系统交互来解决复杂问题。为了在整个AI管道中设置适当的公平性护栏，我们引入了一种公平性嵌入式设计方法，该方法将多角色智能体嵌入到端到端（人到AI）的协同方案中。我们的立场是，我们可以设计自适应和现实的AI公平性框架，并且我们引入了一个可以根据每个AI决策场景的要求和目标进行定制的通用算法。我们提出的所谓FAIRTOPIA框架，构建于一个三层架构之上，该架构将AI管道封装在智能体守护者和基于知识的自精炼分层方案中。基于我们的提议，我们在AI管道的所有阶段，在强大的多智能体工作流下，实施公平性监控，这将启发新的公平性研究假设、启发式方法和方法，这些都植根于以人为本、系统性、跨学科的社会技术原则。", "summary": "FAIRTOPIA提出一个多智能体框架，通过嵌入多角色智能体和引入通用算法，在AI管道的各个阶段实现以人为本的公平性监控，旨在解决当前AI系统中的不公平问题并促进新的公平性研究。该框架构建于一个三层架构之上，并强调公平性嵌入式设计。", "keywords": "多智能体系统, AI公平性, AI管道, 守护, 以人为本AI", "comments": "这篇论文创新性地提出了一种基于多智能体守护的AI公平性框架，超越了传统仅关注计算偏差的方法。其“公平性嵌入式设计”理念和可定制的通用算法，为实现端到端、以人为本的AI公平性提供了新的视角和实用方法。这种方法有望在实际AI应用中发挥重要作用，并为未来的公平性研究奠定基础。"}}
{"id": "2506.09959", "title": "Almost-Optimal Local-Search Methods for Sparse Tensor PCA", "authors": ["Max Lovig", "Conor Sheehan", "Konstantinos Tsirkas", "Ilias Zadik"], "summary": "Local-search methods are widely employed in statistical applications, yet\ninterestingly, their theoretical foundations remain rather underexplored,\ncompared to other classes of estimators such as low-degree polynomials and\nspectral methods. Of note, among the few existing results recent studies have\nrevealed a significant \"local-computational\" gap in the context of a\nwell-studied sparse tensor principal component analysis (PCA), where a broad\nclass of local Markov chain methods exhibits a notable underperformance\nrelative to other polynomial-time algorithms. In this work, we propose a series\nof local-search methods that provably \"close\" this gap to the best known\npolynomial-time procedures in multiple regimes of the model, including and\ngoing beyond the previously studied regimes in which the broad family of local\nMarkov chain methods underperforms. Our framework includes: (1) standard greedy\nand randomized greedy algorithms applied to the (regularized) posterior of the\nmodel; and (2) novel random-threshold variants, in which the randomized greedy\nalgorithm accepts a proposed transition if and only if the corresponding change\nin the Hamiltonian exceeds a random Gaussian threshold-rather that if and only\nif it is positive, as is customary. The introduction of the random thresholds\nenables a tight mathematical analysis of the randomized greedy algorithm's\ntrajectory by crucially breaking the dependencies between the iterations, and\ncould be of independent interest to the community.", "comment": null, "cate": "math.ST", "url": "http://arxiv.org/abs/2506.09959v1", "AI": {"title_translation": "稀疏张量PCA的近似最优局部搜索方法", "tldr": "本文提出了一系列新的局部搜索方法，可以在稀疏张量PCA中，在多个模型机制下，弥补现有局部马尔可夫链方法与已知最佳多项式时间算法之间的计算差距。", "motivation": "局部搜索方法在统计应用中广泛使用，但其理论基础与谱方法等其他估计器相比研究不足。现有研究发现，在稀疏张量PCA中，局部马尔可夫链方法存在显著的“局部计算”差距，表现不如其他多项式时间算法。", "method": "提出了一系列局部搜索方法，包括：1. 应用于模型（正则化）后验的标准贪婪算法和随机贪婪算法；2. 新颖的随机阈值变体，其中随机贪婪算法仅当哈密顿量相应变化超过随机高斯阈值时才接受提议的转换，而不是像传统方法那样仅当变化为正时才接受。随机阈值的引入通过打破迭代间的依赖性，使得随机贪婪算法的轨迹能够进行严密的数学分析。", "result": "所提出的局部搜索方法被证明能够“弥补”在稀疏张量PCA中局部马尔可夫链方法与已知最佳多项式时间算法之间的计算差距，包括并超越了先前研究的、局部马尔可夫链方法表现不佳的机制。", "conclusion": "本文提出的局部搜索方法，特别是带有随机阈值的新变体，能够有效解决稀疏张量PCA中的“局部计算”差距问题，并可能为社区提供独立的理论分析工具。", "translation": "局部搜索方法在统计应用中被广泛采用，但有趣的是，与其他类别的估计器（如低度多项式和谱方法）相比，它们的理论基础仍未得到充分探索。值得注意的是，在少数现有结果中，最近的研究揭示了在深入研究的稀疏张量主成分分析（PCA）背景下存在显著的“局部计算”差距，其中一类广泛的局部马尔可夫链方法相对于其他多项式时间算法表现出显著的劣势。在这项工作中，我们提出了一系列局部搜索方法，这些方法被证明能够“弥补”在模型多个机制下（包括并超越了先前研究的、局部马尔可夫链方法表现不佳的机制）与已知最佳多项式时间过程之间的这一差距。我们的框架包括：(1) 应用于模型（正则化）后验的标准贪婪算法和随机贪婪算法；以及 (2) 新颖的随机阈值变体，其中随机贪婪算法仅当哈密顿量相应变化超过随机高斯阈值时才接受提议的转换——而不是像传统方法那样仅当变化为正时才接受。随机阈值的引入通过关键性地打破迭代间的依赖性，使得随机贪婪算法的轨迹能够进行严密的数学分析，这可能对社区具有独立的意义。", "summary": "本文针对稀疏张量主成分分析（PCA）中局部马尔可夫链方法存在的“局部计算”差距问题，提出了一系列近似最优的局部搜索方法。这些方法包括应用于正则化后验的标准贪婪和随机贪婪算法，以及一种新颖的随机阈值变体。通过引入随机高斯阈值来决定状态接受，该方法能够打破迭代间的依赖性，从而实现严格的数学分析，并被证明在多个模型机制下弥补了与最佳多项式时间算法的性能差距。", "keywords": "局部搜索, 稀疏张量PCA, 随机阈值, 贪婪算法, 计算差距", "comments": "本文的创新点在于提出了带有随机阈值的新型局部搜索算法，这不仅解决了稀疏张量PCA中的一个重要计算差距，而且通过引入随机阈值来打破迭代依赖性，为局部搜索算法的理论分析提供了新的工具和思路，这对于更广泛的优化和统计领域都可能具有重要的独立兴趣。"}}
{"id": "2506.09075", "title": "SILK: Smooth InterpoLation frameworK for motion in-betweening A Simplified Computational Approach", "authors": ["Elly Akhoundi", "Hung Yu Ling", "Anup Anand Deshmukh", "Judith Butepage"], "summary": "Motion in-betweening is a crucial tool for animators, enabling intricate\ncontrol over pose-level details in each keyframe. Recent machine learning\nsolutions for motion in-betweening rely on complex models, incorporating\nskeleton-aware architectures or requiring multiple modules and training steps.\nIn this work, we introduce a simple yet effective Transformer-based framework,\nemploying a single Transformer encoder to synthesize realistic motions for\nmotion in-betweening tasks. We find that data modeling choices play a\nsignificant role in improving in-betweening performance. Among others, we show\nthat increasing data volume can yield equivalent or improved motion\ntransitions, that the choice of pose representation is vital for achieving\nhigh-quality results, and that incorporating velocity input features enhances\nanimation performance. These findings challenge the assumption that model\ncomplexity is the primary determinant of animation quality and provide insights\ninto a more data-centric approach to motion interpolation. Additional videos\nand supplementary material are available at https://silk-paper.github.io.", "comment": "Accepted to CVPR 2025 Human Motion Generation Workshop. 10 pages, 3\n  figures, 5 Tables, and 40 References", "cate": "cs.GR", "url": "http://arxiv.org/abs/2506.09075v1", "AI": {"title_translation": "SILK: 运动中间帧平滑插值框架——一种简化的计算方法", "tldr": "SILK是一个基于Transformer的简单框架，用于运动中间帧生成，强调数据建模而非模型复杂性对性能的影响。", "motivation": "现有的运动中间帧机器学习解决方案依赖于复杂的模型，融合骨骼感知架构或需要多个模块和训练步骤，这增加了复杂性。", "method": "引入一个名为SILK的简单而有效的基于Transformer的框架，采用单个Transformer编码器来合成逼真的运动，用于运动中间帧任务。", "result": "研究发现数据建模选择在改善中间帧性能方面起着重要作用。具体表现为：增加数据量可以产生同等或改进的运动过渡；姿态表示的选择对于获得高质量结果至关重要；结合速度输入特征可以增强动画性能。这些发现挑战了模型复杂性是动画质量主要决定因素的假设。", "conclusion": "数据建模选择在运动插值中起着关键作用，并且一个更以数据为中心的方法可以有效提升动画质量，挑战了模型复杂性是主要决定因素的假设。", "translation": "运动中间帧是动画师的关键工具，能够精细控制每个关键帧的姿态级细节。最近的运动中间帧机器学习解决方案依赖于复杂的模型，融合了骨骼感知架构或需要多个模块和训练步骤。在这项工作中，我们引入了一个简单而有效的基于Transformer的框架，采用单个Transformer编码器来合成逼真的运动，用于运动中间帧任务。我们发现数据建模选择在改善中间帧性能方面起着重要作用。其中包括，我们展示了增加数据量可以产生同等或改进的运动过渡，姿态表示的选择对于获得高质量结果至关重要，以及结合速度输入特征可以增强动画性能。这些发现挑战了模型复杂性是动画质量主要决定因素的假设，并为运动插值提供了一种更以数据为中心的方法。更多视频和补充材料可在 https://silk-paper.github.io 获取。", "summary": "本文提出了SILK，一个基于单个Transformer编码器的运动中间帧生成框架。该研究强调数据建模选择（如数据量、姿态表示和速度输入）对动画质量的决定性作用，而非模型复杂性，为运动插值提供了一种更简化的数据驱动方法。", "keywords": "运动中间帧, Transformer, 数据建模, 动画, 姿态表示", "comments": "这篇论文的创新在于挑战了当前机器学习在动画领域中过度追求模型复杂度的趋势，转而强调数据建模的重要性。其提出的SILK框架以简洁高效的方式实现了高质量的运动中间帧生成，为未来的研究提供了新的视角，即简化模型并优化数据处理可能更具效率和效果。"}}
{"id": "2506.09204", "title": "A Topological Improvement of the Overall Performance of Sparse Evolutionary Training: Motif-Based Structural Optimization of Sparse MLPs Project", "authors": ["Xiaotian Chen", "Hongyun Liu", "Seyed Sahand Mohammadi Ziabari"], "summary": "Deep Neural Networks (DNNs) have been proven to be exceptionally effective\nand have been applied across diverse domains within deep learning. However, as\nDNN models increase in complexity, the demand for reduced computational costs\nand memory overheads has become increasingly urgent. Sparsity has emerged as a\nleading approach in this area. The robustness of sparse Multi-layer Perceptrons\n(MLPs) for supervised feature selection, along with the application of Sparse\nEvolutionary Training (SET), illustrates the feasibility of reducing\ncomputational costs without compromising accuracy. Moreover, it is believed\nthat the SET algorithm can still be improved through a structural optimization\nmethod called motif-based optimization, with potential efficiency gains\nexceeding 40% and a performance decline of under 4%. This research investigates\nwhether the structural optimization of Sparse Evolutionary Training applied to\nMulti-layer Perceptrons (SET-MLP) can enhance performance and to what extent\nthis improvement can be achieved.", "comment": null, "cate": "cs.NE", "url": "http://arxiv.org/abs/2506.09204v1", "AI": {"title_translation": "稀疏演化训练整体性能的拓扑改进：基于主题的稀疏MLP结构优化项目", "tldr": "本研究旨在通过基于主题的结构优化方法，提升稀疏演化训练（SET）在稀疏多层感知器（MLP）中的性能，以降低计算成本和内存开销。", "motivation": "随着深度神经网络（DNNs）模型复杂度的增加，降低计算成本和内存开销的需求日益紧迫。稀疏性是解决此问题的主要方法。研究认为稀疏演化训练（SET）算法可以通过基于主题的结构优化方法进一步改进，有望实现超过40%的效率提升和低于4%的性能下降。", "method": "本研究调查了应用于多层感知器（SET-MLP）的稀疏演化训练的结构优化是否可以提高性能以及可以达到何种程度的改进。具体方法是“基于主题的结构优化”。", "result": "抽象中未提及具体研究结果，但指出通过基于主题的优化，SET算法有望实现超过40%的效率提升和低于4%的性能下降。", "conclusion": "Not mentioned in abstract", "translation": "深度神经网络（DNNs）已被证明非常有效，并已应用于深度学习的各个领域。然而，随着DNN模型复杂度的增加，降低计算成本和内存开销的需求变得日益紧迫。稀疏性已成为该领域的一种主要方法。稀疏多层感知器（MLPs）在监督特征选择方面的鲁棒性，以及稀疏演化训练（SET）的应用，都表明在不影响准确性的前提下降低计算成本的可行性。此外，人们认为SET算法仍然可以通过一种称为基于主题优化的结构优化方法进行改进，潜在效率提升可超过40%，性能下降可低于4%。本研究调查了应用于多层感知器（SET-MLP）的稀疏演化训练的结构优化是否可以提高性能以及这种改进可以达到何种程度。", "summary": "本文探讨了通过基于主题的结构优化方法来提升稀疏演化训练（SET）在稀疏多层感知器（MLPs）中的整体性能。鉴于深度神经网络日益增长的计算成本和内存开销，稀疏性已成为关键解决方案。研究旨在验证这种优化方法能否显著提高SET-MLP的效率，并有望实现超过40%的效率提升和低于4%的性能下降，从而在不牺牲准确性的前提下降低计算负担。", "keywords": "稀疏演化训练, 多层感知器, 结构优化, 基于主题优化, 深度神经网络", "comments": "这项研究旨在通过引入基于主题的结构优化来改进稀疏演化训练（SET）算法，以解决深度神经网络日益增长的计算成本和内存开销问题。如果能实现其宣称的潜在效率提升（超过40%），这将对稀疏神经网络的实际部署和能耗优化具有重要意义。该方法通过拓扑改进来优化稀疏MLP，具有一定的创新性。"}}
{"id": "2506.09529", "title": "Gradient-Weighted, Data-Driven Normalization for Approximate Border Bases -- Concept and Computation", "authors": ["Hiroshi Kera", "Achim Kehrein"], "summary": "This paper studies the concept and the computation of approximately vanishing\nideals of a finite set of data points. By data points, we mean that the points\ncontain some uncertainty, which is a key motivation for the approximate\ntreatment. A careful review of the existing border basis concept for an exact\ntreatment motivates a new adaptation of the border basis concept for an\napproximate treatment. In the study of approximately vanishing polynomials, the\nnormalization of polynomials plays a vital role. So far, the most common\nnormalization in computational commutative algebra uses the coefficient norm of\na polynomial. Inspired by recent developments in machine learning, the present\npaper proposes and studies the use of gradient-weighted normalization. The\ngradient-weighted semi-norm evaluates the gradient of a polynomial at the data\npoints. This data-driven nature of gradient-weighted normalization produces, on\nthe one hand, better stability against perturbation and, on the other hand,\nvery significantly, invariance of border bases with respect to scaling the data\npoints. Neither property is achieved with coefficient normalization. In\nparticular, we present an example of the lack of scaling invariance with\nrespect to coefficient normalization, which can cause an approximate border\nbasis computation to fail. This is extremely relevant because scaling of the\npoint set is often recommended for preprocessing the data. Further, we use an\nexisting algorithm with coefficient normalization to show that it is easily\nadapted to gradient-weighted normalization. The analysis of the adapted\nalgorithm only requires tiny changes, and the time complexity remains the same.\nFinally, we present numerical experiments on three affine varieties to\ndemonstrate the superior stability of our data-driven normalization over\ncoefficient normalization. We obtain robustness to perturbations and invariance\nto scaling.", "comment": "39 pages, 3 figures, 2 tables. Extended version of \"Border basis\n  computation with gradient-weighted normalization\" from ISSAC'22", "cate": "cs.SC", "url": "http://arxiv.org/abs/2506.09529v1", "AI": {"title_translation": "梯度加权、数据驱动的近似边界基范数归一化——概念与计算", "tldr": "本文提出了一种新的梯度加权归一化方法，用于近似边界基计算，解决了传统系数归一化在数据扰动和缩放下的不稳定性问题。", "motivation": "现有边界基概念主要用于精确处理，但实际数据点常包含不确定性，需要近似处理。传统的系数归一化在近似消失理想的研究中存在对数据缩放不具有不变性的问题，可能导致计算失败。", "method": "论文提出并研究了受机器学习启发的梯度加权数据驱动归一化方法。该方法通过在数据点处评估多项式的梯度来定义半范数。此外，论文展示了现有使用系数归一化的算法可以轻松地适应梯度加权归一化，且时间复杂度保持不变。", "result": "梯度加权归一化在扰动下具有更好的稳定性，并且对于数据点的缩放具有边界基不变性，而系数归一化不具备这些特性。数值实验证明了该数据驱动归一化在三种仿射簇上的优越稳定性，表现出对扰动的鲁棒性和对缩放的不变性。", "conclusion": "梯度加权、数据驱动的归一化方法能显著提高近似边界基计算的稳定性和鲁棒性，特别是解决了传统系数归一化在数据缩放时可能导致计算失败的问题，使其更适用于实际具有不确定性的数据。", "translation": "本文研究了有限数据点集的近似消失理想的概念和计算。数据点意味着这些点包含一些不确定性，这是近似处理的关键动机。对现有精确处理的边界基概念的仔细回顾，促使我们对近似处理的边界基概念进行新的适应。在近似消失多项式的研究中，多项式的归一化起着至关重要的作用。到目前为止，计算交换代数中最常见的归一化是使用多项式的系数范数。受机器学习最新发展的启发，本文提出并研究了梯度加权归一化的使用。梯度加权半范数在数据点处评估多项式的梯度。这种梯度加权归一化的数据驱动特性一方面产生了更好的抗扰动稳定性，另一方面，非常显著地，产生了边界基对于数据点缩放的不变性。这两种特性都无法通过系数归一化实现。特别是，我们举例说明了系数归一化在缩放不变性方面的不足，这可能导致近似边界基计算失败。这极其重要，因为通常建议对点集进行缩放以进行数据预处理。此外，我们使用现有的系数归一化算法来表明它很容易适应梯度加权归一化。对适应算法的分析只需要微小的改动，并且时间复杂度保持不变。最后，我们对三种仿射簇进行了数值实验，以证明我们的数据驱动归一化相对于系数归一化的优越稳定性。我们获得了对扰动的鲁棒性和对缩放的不变性。", "summary": "本文针对包含不确定性的数据点集，研究了近似消失理想的概念和计算。鉴于传统系数归一化在近似边界基计算中存在的稳定性问题（尤其在数据缩放时缺乏不变性），论文提出了一种受机器学习启发的梯度加权数据驱动归一化方法。该方法通过在数据点评估多项式梯度实现，实验证明其在扰动下具有更优越的稳定性和对数据缩放的不变性。此外，现有算法可轻松适应此新归一化方法，且不增加时间复杂度。", "keywords": "梯度加权归一化, 近似边界基, 数据驱动, 消失理想, 稳定性, 缩放不变性", "comments": "本文的创新点在于引入了数据驱动的梯度加权归一化方法，这在计算交换代数中是一个新颖的思路，尤其受到机器学习的启发。它有效地解决了传统系数归一化在处理不确定数据时缺乏鲁棒性和缩放不变性的关键问题，使得近似边界基的计算更加稳定和实用。对于需要预处理（如缩放）数据的实际应用来说，这项工作具有重要意义。"}}
{"id": "2506.09502", "title": "The Secure Overview and Analysis OF 3GPP MAC CE", "authors": ["Jin Cao", "Yuanyuan Yang", "Ruhui Ma", "Sheng Li", "Hui Li"], "summary": "To more effectively control and allocate network resources, MAC CE has been\nintroduced into the network protocol, which is a type of control signaling\nlocated in the MAC layer. Since MAC CE lacks encryption and integrity\nprotection mechanisms provided by PDCP, the control signaling carried by MAC CE\nis vulnerable to interception or tampering by attackers during resource\nscheduling and allocation. Currently, the 3GPP has analyzed the security risks\nof Layer 1/Layer 2 Triggered Mobility (LTM), where handover signaling sent to\nthe UE via MAC CE by the network can lead to privacy leaks and network attacks.\nHowever, in addition to LTM, there may be other potential security\nvulnerabilities in other protocol procedures. Therefore, this paper explores\nthe security threats to MAC CE and the corresponding protection mechanisms. The\nresearch is expected to support the 3GPP's study of MAC CE and be integrated\nwith the security research of lower-layer protocols, thereby enhancing the\nsecurity and reliability of the entire communication system.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.09502v1", "AI": {"title_translation": "3GPP MAC CE的安全概述与分析", "tldr": "本文探讨了3GPP MAC CE由于缺乏加密和完整性保护机制而面临的安全威胁，并提出了相应的保护机制，旨在增强通信系统的安全性。", "motivation": "MAC CE在资源调度和分配过程中易受攻击，现有研究（如3GPP对LTM的分析）只涵盖了部分漏洞，可能存在其他潜在安全漏洞。因此，需要全面探讨MAC CE的安全威胁及保护机制。", "method": "本文探索了MAC CE面临的安全威胁和相应的保护机制。", "result": "Not mentioned in abstract", "conclusion": "本研究旨在支持3GPP对MAC CE的研究，并与下层协议的安全研究相结合，从而增强整个通信系统的安全性和可靠性。", "translation": "为了更有效地控制和分配网络资源，MAC CE被引入网络协议中，它是一种位于MAC层的控制信令。由于MAC CE缺乏PDCP提供的加密和完整性保护机制，MAC CE承载的控制信令在资源调度和分配过程中容易被攻击者截获或篡改。目前，3GPP已经分析了第一层/第二层触发移动性（LTM）的安全风险，其中网络通过MAC CE发送给UE的切换信令可能导致隐私泄露和网络攻击。然而，除了LTM，其他协议过程中可能还存在其他潜在的安全漏洞。因此，本文探讨了MAC CE面临的安全威胁以及相应的保护机制。该研究有望支持3GPP对MAC CE的研究，并与下层协议的安全研究相结合，从而增强整个通信系统的安全性和可靠性。", "summary": "本文旨在解决3GPP MAC CE在资源调度和分配中存在的安全漏洞问题。由于MAC CE缺乏加密和完整性保护，其承载的控制信令易受攻击。现有研究已发现LTM中的安全风险，但可能存在其他未被识别的漏洞。因此，本研究深入探讨了MAC CE的安全威胁，并提出了相应的保护机制，以期提升整个通信系统的安全性和可靠性，并为3GPP的相关研究提供支持。", "keywords": "3GPP, MAC CE, 网络安全, 漏洞分析, 保护机制", "comments": "该论文关注3GPP MAC CE这一关键网络组件的安全问题，其重要性在于MAC CE在网络资源控制和分配中的核心作用。缺乏加密和完整性保护是一个严重的缺陷，可能导致隐私泄露和网络攻击。这项研究有望为3GPP的安全标准制定提供有价值的输入，并促进下层协议的整体安全性。其创新点在于对MAC CE潜在的、除LTM之外的其他安全漏洞的探索。"}}
{"id": "2506.09758", "title": "Mainframe-style channel controllers for modern disaggregated memory systems", "authors": ["Zikai Liu", "Jasmin Schult", "Pengcheng Xu", "Timothy Roscoe"], "summary": "Despite the promise of alleviating the main memory bottleneck, and the\nexistence of commercial hardware implementations, techniques for Near-Data\nProcessing have seen relatively little real-world deployment. The idea has\nreceived renewed interest with the appearance of disaggregated or \"far\" memory,\nfor example in the use of CXL memory pools.\n  However, we argue that the lack of a clear OS-centric abstraction of\nNear-Data Processing is a major barrier to adoption of the technology. Inspired\nby the channel controllers which interface the CPU to disk drives in mainframe\nsystems, we propose memory channel controllers as a convenient, portable, and\nvirtualizable abstraction of Near-Data Processing for modern disaggregated\nmemory systems.\n  In addition to providing a clean abstraction that enables OS integration\nwhile requiring no changes to CPU architecture, memory channel controllers\nincorporate another key innovation: they exploit the cache coherence provided\nby emerging interconnects to provide a much richer programming model, with more\nfine-grained interaction, than has been possible with existing designs.", "comment": null, "cate": "cs.OS", "url": "http://arxiv.org/abs/2506.09758v1", "AI": {"title_translation": "用于现代解耦内存系统的大型机式通道控制器", "tldr": "本文提出内存通道控制器作为一种方便、可移植且可虚拟化的近数据处理抽象，以解决其在现代解耦内存系统中缺乏操作系统中心抽象的问题，并利用缓存一致性提供更丰富的编程模型。", "motivation": "尽管近数据处理（NDP）有望缓解主内存瓶颈且存在商业硬件实现，但其在实际部署中相对较少。主要障碍在于缺乏清晰的以操作系统为中心的NDP抽象。", "method": "受大型机系统中连接CPU与磁盘驱动器的通道控制器启发，本文提出内存通道控制器作为一种方便、可移植且可虚拟化的近数据处理抽象，用于现代解耦内存系统。", "result": "Not mentioned in abstract", "conclusion": "内存通道控制器提供了一种干净的抽象，实现了操作系统集成且无需改变CPU架构，并通过利用新兴互连提供的缓存一致性，提供了比现有设计更丰富、更细粒度的编程模型。", "translation": "尽管有望缓解主内存瓶颈，并且存在商业硬件实现，但近数据处理技术在实际部署中相对较少。随着解耦或“远端”内存的出现，例如在CXL内存池中的使用，这一理念重新获得了关注。\n然而，我们认为缺乏清晰的以操作系统为中心的近数据处理抽象是该技术普及的主要障碍。受大型机系统中连接CPU与磁盘驱动器的通道控制器启发，我们提出内存通道控制器作为一种方便、可移植且可虚拟化的近数据处理抽象，用于现代解耦内存系统。\n除了提供一个干净的抽象，使得操作系统集成成为可能且无需改变CPU架构之外，内存通道控制器还融合了另一个关键创新：它们利用新兴互连提供的缓存一致性，提供比现有设计更丰富、更细粒度的编程模型。", "summary": "本文提出了一种名为“内存通道控制器”的新型抽象，旨在解决近数据处理（NDP）在解耦内存系统中实际部署所面临的操作系统抽象缺失问题。该控制器借鉴了大型机通道设计，提供了一种方便、可移植且可虚拟化的NDP接口，无需修改CPU架构即可实现操作系统集成。此外，它还创新性地利用了新兴互连的缓存一致性，从而提供了比现有方案更精细和丰富的编程模型。", "keywords": "内存通道控制器, 近数据处理, 解耦内存, 缓存一致性, 操作系统抽象", "comments": "本文提出的内存通道控制器为近数据处理（NDP）的实际应用提供了一个有前景的解决方案。其创新点在于借鉴了大型机通道的概念，为NDP提供了一个OS友好的、可虚拟化的抽象，这对于克服当前NDP普及的障碍至关重要。同时，利用缓存一致性来丰富编程模型也极大地提升了NDP的可用性和效率。该方法有望推动NDP在现代解耦内存系统中的广泛部署。"}}
{"id": "2506.09366", "title": "SkillBlender: Towards Versatile Humanoid Whole-Body Loco-Manipulation via Skill Blending", "authors": ["Yuxuan Kuang", "Haoran Geng", "Amine Elhafsi", "Tan-Dzung Do", "Pieter Abbeel", "Jitendra Malik", "Marco Pavone", "Yue Wang"], "summary": "Humanoid robots hold significant potential in accomplishing daily tasks\nacross diverse environments thanks to their flexibility and human-like\nmorphology. Recent works have made significant progress in humanoid whole-body\ncontrol and loco-manipulation leveraging optimal control or reinforcement\nlearning. However, these methods require tedious task-specific tuning for each\ntask to achieve satisfactory behaviors, limiting their versatility and\nscalability to diverse tasks in daily scenarios. To that end, we introduce\nSkillBlender, a novel hierarchical reinforcement learning framework for\nversatile humanoid loco-manipulation. SkillBlender first pretrains\ngoal-conditioned task-agnostic primitive skills, and then dynamically blends\nthese skills to accomplish complex loco-manipulation tasks with minimal\ntask-specific reward engineering. We also introduce SkillBench, a parallel,\ncross-embodiment, and diverse simulated benchmark containing three embodiments,\nfour primitive skills, and eight challenging loco-manipulation tasks,\naccompanied by a set of scientific evaluation metrics balancing accuracy and\nfeasibility. Extensive simulated experiments show that our method significantly\noutperforms all baselines, while naturally regularizing behaviors to avoid\nreward hacking, resulting in more accurate and feasible movements for diverse\nloco-manipulation tasks in our daily scenarios. Our code and benchmark will be\nopen-sourced to the community to facilitate future research. Project page:\nhttps://usc-gvl.github.io/SkillBlender-web/.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.09366v1", "AI": {"title_translation": "SkillBlender：通过技能融合实现多功能拟人机器人全身运动-操作", "tldr": "SkillBlender是一个分层强化学习框架，通过预训练的原始技能动态融合，实现了多功能拟人机器人全身运动-操作，解决了现有方法任务特定调优的问题，并在新基准SkillBench上表现出色。", "motivation": "现有的人形机器人全身控制和运动-操作方法需要针对每个任务进行繁琐的特定任务调优，这限制了它们在日常场景中处理多样化任务的通用性和可扩展性。", "method": "我们引入了SkillBlender，一个新颖的分层强化学习框架，用于多功能人形机器人运动-操作。SkillBlender首先预训练目标条件下的任务无关原始技能，然后动态融合这些技能以完成复杂的运动-操作任务，同时最大限度地减少特定任务的奖励工程。我们还引入了SkillBench，一个并行、跨实体和多样化的模拟基准。", "result": "广泛的模拟实验表明，我们的方法显著优于所有基线，同时自然地规范行为以避免奖励操纵，从而在日常场景中为多样化的运动-操作任务产生更准确和可行的运动。", "conclusion": "SkillBlender通过其分层强化学习框架和技能融合方法，为多功能人形机器人运动-操作提供了一种有效且通用的解决方案，克服了传统方法的局限性，并在新基准上展现了优越的性能。", "translation": "人形机器人在各种环境中完成日常任务方面具有巨大的潜力，这得益于它们的灵活性和类人形态。最近的工作在利用最优控制或强化学习方面，在人形机器人全身控制和运动-操作方面取得了重大进展。然而，这些方法需要针对每个任务进行繁琐的特定任务调优才能实现令人满意的行为，这限制了它们在日常场景中处理多样化任务的通用性和可扩展性。为此，我们引入了SkillBlender，一个新颖的分层强化学习框架，用于多功能人形机器人运动-操作。SkillBlender首先预训练目标条件下的任务无关原始技能，然后动态融合这些技能以完成复杂的运动-操作任务，同时最大限度地减少特定任务的奖励工程。我们还引入了SkillBench，一个并行、跨实体和多样化的模拟基准，包含三个实体、四种原始技能和八个具有挑战性的运动-操作任务，并附带一套平衡准确性和可行性的科学评估指标。广泛的模拟实验表明，我们的方法显著优于所有基线，同时自然地规范行为以避免奖励操纵，从而在日常场景中为多样化的运动-操作任务产生更准确和可行的运动。我们的代码和基准将开源给社区，以促进未来的研究。项目页面：https://usc-gvl.github.io/SkillBlender-web/。", "summary": "本文提出了SkillBlender，一个分层强化学习框架，旨在通过预训练的原始技能动态融合，解决人形机器人全身运动-操作中任务特定调优的挑战，从而实现多功能性。该框架通过预训练任务无关的原始技能并动态融合它们来完成复杂任务，同时减少奖励工程。研究还引入了SkillBench基准进行评估，实验结果表明SkillBlender在准确性和可行性方面均优于现有基线。", "keywords": "人形机器人, 运动-操作, 技能融合, 分层强化学习, SkillBench", "comments": "SkillBlender的创新之处在于其分层强化学习方法和技能融合机制，它有效地解决了人形机器人多任务学习中普遍存在的任务特定调优问题，显著提高了通用性和可扩展性。同时，引入SkillBench基准为未来相关研究提供了宝贵的评估工具，促进了领域的发展。其避免奖励操纵的特性也增强了行为的鲁棒性。"}}
{"id": "2506.09081", "title": "FlagEvalMM: A Flexible Framework for Comprehensive Multimodal Model Evaluation", "authors": ["Zheqi He", "Yesheng Liu", "Jing-shu Zheng", "Xuejing Li", "Richeng Xuan", "Jin-Ge Yao", "Xi Yang"], "summary": "We present FlagEvalMM, an open-source evaluation framework designed to\ncomprehensively assess multimodal models across a diverse range of\nvision-language understanding and generation tasks, such as visual question\nanswering, text-to-image/video generation, and image-text retrieval. We\ndecouple model inference from evaluation through an independent evaluation\nservice, thus enabling flexible resource allocation and seamless integration of\nnew tasks and models. Moreover, FlagEvalMM utilizes advanced inference\nacceleration tools (e.g., vLLM, SGLang) and asynchronous data loading to\nsignificantly enhance evaluation efficiency. Extensive experiments show that\nFlagEvalMM offers accurate and efficient insights into model strengths and\nlimitations, making it a valuable tool for advancing multimodal research. The\nframework is publicly accessible athttps://github.com/flageval-baai/FlagEvalMM.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09081v1", "AI": {"title_translation": "FlagEvalMM：一个用于综合多模态模型评估的灵活框架", "tldr": "FlagEvalMM是一个开源的评估框架，用于全面评估多模态模型在视觉-语言理解和生成任务上的表现，具有高效、灵活的特点。", "motivation": "为了全面评估多模态模型在各种视觉-语言理解和生成任务上的表现。", "method": "FlagEvalMM通过独立的评估服务将模型推理与评估解耦，实现灵活的资源分配和新任务模型的无缝集成。它还利用先进的推理加速工具（如vLLM、SGLang）和异步数据加载来显著提高评估效率。", "result": "广泛的实验表明，FlagEvalMM能准确高效地提供关于模型优势和局限性的见解。", "conclusion": "FlagEvalMM是一个有价值的工具，有助于推动多模态研究的进展。", "translation": "我们提出了FlagEvalMM，一个开源评估框架，旨在全面评估多模态模型在各种视觉-语言理解和生成任务上的表现，例如视觉问答、文本到图像/视频生成以及图像-文本检索。我们通过独立的评估服务将模型推理与评估解耦，从而实现灵活的资源分配和新任务及模型的无缝集成。此外，FlagEvalMM利用先进的推理加速工具（例如vLLM、SGLang）和异步数据加载来显著提高评估效率。广泛的实验表明，FlagEvalMM能准确高效地提供关于模型优势和局限性的见解，使其成为推动多模态研究的宝贵工具。该框架可在https://github.com/flageval-baai/FlagEvalMM 公开访问。", "summary": "FlagEvalMM是一个开源的、灵活的多模态模型评估框架。它通过解耦推理和评估、利用推理加速工具和异步数据加载，实现了对视觉-语言理解和生成任务的全面、高效评估，为多模态研究提供了有价值的见解。", "keywords": "多模态模型, 评估框架, 视觉-语言, FlagEvalMM, 开源", "comments": "FlagEvalMM的创新之处在于其解耦推理和评估的设计，以及对先进推理加速工具的利用，这显著提高了评估的效率和灵活性。作为一个开源框架，它对多模态研究社区具有重要价值，能够促进模型开发和基准测试的标准化。"}}
{"id": "2506.09498", "title": "Fast Monte Carlo Tree Diffusion: 100x Speedup via Parallel Sparse Planning", "authors": ["Jaesik Yoon", "Hyeonseo Cho", "Yoshua Bengio", "Sungjin Ahn"], "summary": "Diffusion models have recently emerged as a powerful approach for trajectory\nplanning. However, their inherently non-sequential nature limits their\neffectiveness in long-horizon reasoning tasks at test time. The recently\nproposed Monte Carlo Tree Diffusion (MCTD) offers a promising solution by\ncombining diffusion with tree-based search, achieving state-of-the-art\nperformance on complex planning problems. Despite its strengths, our analysis\nshows that MCTD incurs substantial computational overhead due to the sequential\nnature of tree search and the cost of iterative denoising. To address this, we\npropose Fast-MCTD, a more efficient variant that preserves the strengths of\nMCTD while significantly improving its speed and scalability. Fast-MCTD\nintegrates two techniques: Parallel MCTD, which enables parallel rollouts via\ndelayed tree updates and redundancy-aware selection; and Sparse MCTD, which\nreduces rollout length through trajectory coarsening. Experiments show that\nFast-MCTD achieves up to 100x speedup over standard MCTD while maintaining or\nimproving planning performance. Remarkably, it even outperforms Diffuser in\ninference speed on some tasks, despite Diffuser requiring no search and\nyielding weaker solutions. These results position Fast-MCTD as a practical and\nscalable solution for diffusion-based inference-time reasoning.", "comment": null, "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.09498v1", "AI": {"title_translation": "快速蒙特卡洛树扩散：通过并行稀疏规划实现100倍加速", "tldr": "扩散模型在轨迹规划中表现出色，但MCTD因其计算开销而受限。Fast-MCTD通过并行和稀疏规划，将MCTD提速100倍，同时保持甚至提升性能，使其成为实用的扩散推理解决方案。", "motivation": "扩散模型在轨迹规划中虽强大，但其非顺序性限制了长时序推理任务的有效性。近期提出的蒙特卡洛树扩散（MCTD）结合扩散与树搜索，在复杂规划问题上表现出色，但由于树搜索的顺序性和迭代去噪的成本，MCTD带来了巨大的计算开销。本文的动机在于解决MCTD的计算开销问题。", "method": "本文提出了Fast-MCTD，一种更高效的MCTD变体，它集成了两种技术：1. 并行MCTD：通过延迟树更新和冗余感知选择实现并行rollout。2. 稀疏MCTD：通过轨迹粗化减少rollout长度。", "result": "实验表明，Fast-MCTD比标准MCTD提速高达100倍，同时保持或提高了规划性能。值得注意的是，尽管Diffuser不需要搜索且解决方案较弱，Fast-MCTD在某些任务上的推理速度甚至超越了Diffuser。", "conclusion": "Fast-MCTD是扩散模型在推理时进行推理的一种实用且可扩展的解决方案。", "translation": "扩散模型最近已成为轨迹规划的强大方法。然而，它们固有的非顺序性限制了它们在测试时在长时序推理任务中的有效性。最近提出的蒙特卡洛树扩散（MCTD）通过将扩散与基于树的搜索相结合，提供了一个有前景的解决方案，在复杂的规划问题上取得了最先进的性能。尽管MCTD具有优势，但我们的分析表明，由于树搜索的顺序性和迭代去噪的成本，MCTD会产生大量的计算开销。为了解决这个问题，我们提出了Fast-MCTD，一个更高效的变体，它保留了MCTD的优点，同时显著提高了其速度和可扩展性。Fast-MCTD集成了两种技术：并行MCTD，它通过延迟树更新和冗余感知选择实现并行rollout；以及稀疏MCTD，它通过轨迹粗化减少rollout长度。实验表明，Fast-MCTD比标准MCTD提速高达100倍，同时保持或提高了规划性能。值得注意的是，尽管Diffuser不需要搜索且解决方案较弱，它甚至在某些任务上的推理速度超越了Diffuser。这些结果将Fast-MCTD定位为一种实用且可扩展的基于扩散的推理时推理解决方案。", "summary": "扩散模型在轨迹规划中展现出强大能力，但其非顺序性限制了长时序推理。蒙特卡洛树扩散（MCTD）结合扩散与树搜索，在复杂规划问题上表现出色，但存在计算开销。为解决此问题，本文提出了Fast-MCTD，通过并行MCTD（延迟树更新和冗余感知选择）和稀疏MCTD（轨迹粗化）两种技术，显著提高了速度和可扩展性。实验证明，Fast-MCTD比标准MCTD提速100倍，同时保持或提高规划性能，甚至在某些任务上超越了无需搜索且性能较弱的Diffuser，使其成为一种实用且可扩展的扩散推理解决方案。", "keywords": "扩散模型, 蒙特卡洛树搜索, 轨迹规划, 并行规划, 稀疏规划", "comments": "这篇论文的创新点在于通过引入并行和稀疏规划技术，显著提升了蒙特卡洛树扩散（MCTD）模型在轨迹规划任务中的推理速度，实现了高达100倍的加速。其重要性在于将原本计算成本高昂的MCTD变得更加实用和可扩展，甚至在速度上超越了一些不进行搜索的基线模型，同时保持或提升了规划性能，这对于扩散模型在实际长时序推理任务中的应用具有重要意义。"}}
{"id": "2506.09090", "title": "Integrating Asynchronous AdaBoost into Federated Learning: Five Real World Applications", "authors": ["Arthur Oghlukyan", "Nuria Gomez Blas"], "summary": "This paper presents a comprehensive analysis of an enhanced asynchronous\nAdaBoost framework for federated learning (FL), focusing on its application\nacross five distinct domains: computer vision on edge devices, blockchain-based\nmodel transparency, on-device mobile personalization, IoT anomaly detection,\nand federated healthcare diagnostics. The proposed algorithm incorporates\nadaptive communication scheduling and delayed weight compensation to reduce\nsynchronization frequency and communication overhead while preserving or\nimproving model accuracy. We examine how these innovations improve\ncommunication efficiency, scalability, convergence, and robustness in each\ndomain. Comparative metrics including training time, communication overhead,\nconvergence iterations, and classification accuracy are evaluated using data\nand estimates derived from Oghlukyan's enhanced AdaBoost framework. Empirical\nresults show, for example, training time reductions on the order of 20-35% and\ncommunication overhead reductions of 30-40% compared to baseline AdaBoost, with\nconvergence achieved in significantly fewer boosting rounds. Tables and charts\nsummarize these improvements by domain. Mathematical formulations of the\nadaptive scheduling rule and error-driven synchronization thresholds are\nprovided. Overall, the enhanced AdaBoost exhibits markedly improved efficiency\nand robustness across diverse FL scenarios, suggesting broad applicability of\nthe approach.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09090v1", "AI": {"title_translation": "将异步AdaBoost集成到联邦学习中：五个现实世界应用", "tldr": "本文提出了一种增强的异步AdaBoost联邦学习框架，通过自适应通信调度和延迟权重补偿，显著降低了通信开销和训练时间，同时保持或提高了模型精度，并在五个现实世界应用中展示了其效率和鲁棒性。", "motivation": "本文旨在解决联邦学习中的同步频率和通信开销问题，同时保持或提高模型精度，特别是在多样化的现实世界应用中。", "method": "本文提出了一种增强的异步AdaBoost联邦学习框架。该算法结合了自适应通信调度和延迟权重补偿，以减少同步频率和通信开销，同时保持或提高模型精度。其评估基于Oghlukyan的增强AdaBoost框架的数据和估计。", "result": "实证结果表明，与基线AdaBoost相比，训练时间减少了20-35%，通信开销减少了30-40%，并且在显著更少的提升轮次中实现了收敛。这些改进在每个领域都有体现。", "conclusion": "增强的AdaBoost在各种联邦学习场景中表现出显著提高的效率和鲁棒性，表明该方法具有广泛的适用性。", "translation": "本文对联邦学习（FL）中增强的异步AdaBoost框架进行了全面分析，重点关注其在五个不同领域的应用：边缘设备上的计算机视觉、基于区块链的模型透明度、设备上的移动个性化、物联网异常检测和联邦医疗诊断。所提出的算法结合了自适应通信调度和延迟权重补偿，以减少同步频率和通信开销，同时保持或提高模型精度。我们研究了这些创新如何在每个领域提高通信效率、可扩展性、收敛性和鲁棒性。使用来自Oghlukyan增强AdaBoost框架的数据和估计，评估了包括训练时间、通信开销、收敛迭代次数和分类精度在内的比较指标。实证结果显示，例如，与基线AdaBoost相比，训练时间减少了大约20-35%，通信开销减少了30-40%，并且在显著更少的提升轮次中实现收敛。表格和图表按领域总结了这些改进。提供了自适应调度规则和误差驱动同步阈值的数学公式。总的来说，增强的AdaBoost在各种FL场景中表现出显著提高的效率和鲁棒性，表明该方法具有广泛的适用性。", "summary": "本文介绍了一种用于联邦学习的增强型异步AdaBoost框架，该框架通过自适应通信调度和延迟权重补偿，旨在减少同步和通信开销，同时保持模型精度。该方法在边缘设备计算机视觉、区块链模型透明度、移动个性化、物联网异常检测和联邦医疗诊断这五个现实世界应用中进行了测试。结果显示，与基线AdaBoost相比，训练时间减少20-35%，通信开销减少30-40%，并能更快收敛，显著提高了效率和鲁棒性。", "keywords": "异步AdaBoost, 联邦学习, 通信效率, 现实世界应用, 可扩展性", "comments": "该论文的创新之处在于将异步AdaBoost与自适应通信和延迟权重补偿集成到联邦学习中，解决了通信效率和可扩展性等关键问题。其在五个多样化现实世界领域的应用，突显了其重要的实践价值和广泛的适用性。"}}
{"id": "2506.09091", "title": "Variational Inference Optimized Using the Curved Geometry of Coupled Free Energy", "authors": ["Kenric Nelson", "Igor Oliveira", "Amenah Al-Najafi", "Fode Zhang", "Hon Keung Tony Ng"], "summary": "We introduce an optimization framework for variational inference based on the\ncoupled free energy, extending variational inference techniques to account for\nthe curved geometry of the coupled exponential family. This family includes\nimportant heavy-tailed distributions such as the generalized Pareto and the\nStudent's t. By leveraging the coupled free energy, which is equal to the\ncoupled evidence lower bound (ELBO) of the inverted probabilities, we improve\nthe accuracy and robustness of the learned model. The coupled generalization of\nFisher Information metric and the affine connection. The method is applied to\nthe design of a coupled variational autoencoder (CVAE). By using the coupling\nfor both the distributions and cost functions, the reconstruction metric is\nderived to still be the mean-square average loss with modified constants. The\nnovelty comes from sampling the heavy-tailed latent distribution with its\nassociated coupled probability, which has faster decaying tails. The result is\nthe ability to train a model with high penalties in the tails, while assuring\nthat the training samples have a reduced number of outliers. The Wasserstein-2\nor Fr\\'echet Inception Distance of the reconstructed CelebA images shows the\nCVAE has a 3\\% improvement over the VAE after 5 epochs of training.", "comment": "11 pages, 2 figures, AGI-25", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09091v1", "AI": {"title_translation": "利用耦合自由能的曲率几何优化变分推断", "tldr": "本文提出了一种基于耦合自由能的变分推断优化框架，旨在处理重尾分布，提高模型准确性和鲁棒性，并通过耦合变分自编码器（CVAE）在图像重建上取得了3%的改进。", "motivation": "本研究的动机是扩展变分推断技术，以考虑耦合指数族的曲率几何，该族包含广义帕累托分布和学生t分布等重要的重尾分布，从而提高学习模型的准确性和鲁棒性。", "method": "本文引入了一种基于耦合自由能的变分推断优化框架，该框架利用了等于反转概率的耦合证据下界（ELBO）的耦合自由能。它使用了Fisher信息度量和仿射连接的耦合推广。该方法应用于设计耦合变分自编码器（CVAE），通过对分布和成本函数都使用耦合，并使用具有更快衰减尾部的相关耦合概率对重尾潜在分布进行采样。", "result": "结果表明，该方法能够训练一个在尾部具有高惩罚的模型，同时确保训练样本中的异常值数量减少。在CelebA图像重构上，CVAE在5个训练周期后，其Wasserstein-2或Fréchet Inception距离比VAE有3%的改进。", "conclusion": "本文提出的利用耦合自由能及其曲率几何的变分推断框架，能有效处理重尾分布，提高了模型的准确性和鲁棒性。通过CVAE在图像重建上的应用，验证了该方法的有效性。", "translation": "我们引入了一种基于耦合自由能的变分推断优化框架，扩展了变分推断技术以考虑耦合指数族的曲率几何。该族包括重要的重尾分布，如广义帕累托分布和学生t分布。通过利用耦合自由能（其等于反转概率的耦合证据下界（ELBO）），我们提高了学习模型的准确性和鲁棒性。Fisher信息度量和仿射连接的耦合推广。该方法应用于耦合变分自编码器（CVAE）的设计。通过对分布和成本函数都使用耦合，重构度量被推导为仍然是具有修改常数的均方平均损失。新颖之处在于使用其相关的耦合概率对重尾潜在分布进行采样，该概率具有更快的衰减尾部。结果是能够训练一个在尾部具有高惩罚的模型，同时确保训练样本中的异常值数量减少。CelebA图像重构的Wasserstein-2或Fréchet Inception距离显示，经过5个训练周期后，CVAE比VAE有3%的改进。", "summary": "本文提出了一种新颖的变分推断优化框架，该框架融合了耦合指数族的曲率几何，特别适用于重尾分布。通过利用耦合自由能，该框架显著提升了模型的准确性和鲁棒性。作者将此方法应用于开发耦合变分自编码器（CVAE），并证明了其处理重尾潜在分布、减少异常值的能力，并在CelebA图像重建上相对于标准VAE取得了3%的性能提升。", "keywords": "变分推断, 耦合自由能, 重尾分布, 耦合变分自编码器, 曲率几何", "comments": "本文的创新之处在于将耦合指数族的曲率几何融入变分推断，尤其针对重尾分布的建模，这解决了复杂数据建模中的一个重要挑战。将此方法应用于CVAE并在图像重建中展现的性能提升，突显了其在实际应用中的潜力和重要性。训练样本中异常值的减少是一个显著的优势。"}}
{"id": "2506.09601", "title": "ASTAGEN: Empirical Evaluation of Automated SATD Taxonomy Generation with LLMs", "authors": ["Sota Nakashima", "Yuta Ishimoto", "Masanari Kondo", "Tao Xiao", "Yasutaka Kamei"], "summary": "Technical debt refers to suboptimal code that degrades software quality. When\ndevelopers intentionally introduce such debt, it is called self-admitted\ntechnical debt (SATD). Since SATD hinders maintenance, identifying its\ncategories is key to uncovering quality issues. Traditionally, constructing\nsuch taxonomies requires manually inspecting SATD comments and surrounding\ncode, which is time-consuming, labor-intensive, and often inconsistent due to\nannotator subjectivity. This study presents ASTAGEN, an initial step toward\nautomating SATD taxonomy generation using large language models (LLMs). Given a\ncomment and its surrounding code, ASTAGEN first generates a concise explanation\nfor each SATD comment, then incrementally generates and updates categories to\nconstruct a taxonomy. We evaluate ASTAGEN on SATD datasets from three domains:\nquantum software, smart contracts, and machine learning. It successfully\nrecovers domain-specific categories reported in prior work, such as Layer\nConfiguration in machine learning. Compared to a naive use of an LLM, ASTAGEN\nproduces more consistent category assignments due to its explanation-driven,\niterative design. It also completes taxonomy generation in under two hours and\nfor less than one USD, even on the largest dataset. These results suggest that\nwhile full automation remains challenging, ASTAGEN is able to support\nsemi-automated taxonomy construction. Furthermore, our work opens up avenues\nfor future work, such as automatic taxonomy generation in other areas.", "comment": null, "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.09601v1", "AI": {"title_translation": "ASTAGEN：使用大型语言模型自动生成SATD分类法的实证评估", "tldr": "ASTAGEN利用LLM自动生成SATD分类法，效率高且结果一致，支持半自动化。", "motivation": "传统上，构建自承认技术债务（SATD）分类法需要手动检查评论和代码，这耗时、费力且因注释者主观性而导致不一致。", "method": "ASTAGEN利用大型语言模型（LLMs）自动化SATD分类法生成。它首先为每个SATD注释及其周围代码生成简洁的解释，然后通过增量生成和更新类别来构建分类法。", "result": "ASTAGEN成功恢复了先前工作中报告的特定领域类别（如机器学习中的层配置）。与简单使用LLM相比，ASTAGEN由于其解释驱动的迭代设计，产生了更一致的类别分配。即使在最大的数据集上，它也能在两小时内且成本不到一美元的情况下完成分类法生成。", "conclusion": "尽管完全自动化仍具挑战性，但ASTAGEN能够支持半自动化的分类法构建，并为其他领域的自动化分类法生成开辟了新途径。", "translation": "技术债务是指降低软件质量的次优代码。当开发人员有意引入此类债务时，它被称为自承认技术债务（SATD）。由于SATD阻碍维护，识别其类别是发现质量问题的关键。传统上，构建此类分类法需要手动检查SATD注释和周围代码，这耗时、费力，并且由于注释者主观性而常常不一致。本研究提出了ASTAGEN，这是使用大型语言模型（LLMs）自动化SATD分类法生成的第一步。给定一个注释及其周围代码，ASTAGEN首先为每个SATD注释生成简洁的解释，然后增量生成并更新类别以构建分类法。我们在来自三个领域（量子软件、智能合约和机器学习）的SATD数据集上评估了ASTAGEN。它成功恢复了先前工作中报告的特定领域类别，例如机器学习中的层配置。与简单使用LLM相比，ASTAGEN由于其解释驱动的迭代设计，产生了更一致的类别分配。即使在最大的数据集上，它也能在两小时内且成本不到一美元的情况下完成分类法生成。这些结果表明，尽管完全自动化仍具挑战性，但ASTAGEN能够支持半自动化的分类法构建。此外，我们的工作为未来的工作开辟了道路，例如其他领域的自动化分类法生成。", "summary": "本研究提出了ASTAGEN，一个利用大型语言模型（LLMs）自动化自承认技术债务（SATD）分类法生成的系统。针对传统手动分类耗时且不一致的问题，ASTAGEN通过生成解释并迭代更新类别来构建分类法。实验表明，ASTAGEN能有效识别特定领域类别，并比简单LLM产生更一致的结果，同时具有高效率和低成本。这表明ASTAGEN可用于半自动化分类法构建，并为未来全自动化研究奠定基础。", "keywords": "自承认技术债务, 分类法生成, 大型语言模型, 自动化, 软件质量", "comments": "ASTAGEN的创新之处在于其利用LLM的解释生成能力和迭代设计来提高SATD分类的一致性和效率，克服了传统手动方法的弊端。其经济性也增加了其实用价值。该工作为将LLMs应用于软件工程中的自动化分类任务提供了有前景的方向。"}}
{"id": "2506.09236", "title": "Augmented Reality User Interfaces for First Responders: A Scoping Literature Review", "authors": ["Erin Argo", "Tanim Ahmed", "Sarah Gable", "Callie Hampton", "Jeronimo Grandi", "Regis Kopper"], "summary": "During the past decade, there has been a significant increase in research\nfocused on integrating AR User Interfaces into public safety applications,\nparticularly for first responders in the domains of Emergency Medical Services,\nFirefighting, and Law Enforcement. This paper presents the results of a scoping\nreview involving the application of AR user interfaces in the public safety\ndomain and applies an established systematic review methodology to provide a\ncomprehensive analysis of the current research landscape, identifying key\ntrends, challenges, and gaps in the literature. This review includes\npeer-reviewed publications indexed by the major scientific databases up to\nApril 2025. A basic keyword search retrieved 1,751 papers, of which 90 were\ndeemed relevant for this review. An in-depth analysis of the literature allowed\nthe development of a faceted taxonomy that categorizes AR user interfaces for\npublic safety. This classification lays a solid foundation for future research,\nwhile also highlighting key design considerations, challenges, and gaps in the\nliterature. This review serves as a valuable resource for researchers and\ndevelopers, offering insights that can drive further advances in the field.", "comment": "19 pages, 4 figures, 8 tables", "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.09236v1", "AI": {"title_translation": "增强现实用户界面在急救人员中的应用：范围文献综述", "tldr": "对过去十年增强现实用户界面在公共安全领域（特别是急救人员）应用的研究进行了范围综述，识别了趋势、挑战和空白，并开发了一个分类法。", "motivation": "过去十年，将增强现实用户界面整合到公共安全应用（特别是急救人员领域）的研究显著增加。本研究旨在对该领域进行全面分析，识别关键趋势、挑战和文献空白。", "method": "采用既定的系统综述方法进行了一项范围综述。通过关键词搜索从主要科学数据库中检索到1,751篇论文，筛选出90篇相关论文进行深入分析。基于此分析，开发了一个用于公共安全领域增强现实用户界面的分面分类法。", "result": "识别了公共安全领域增强现实用户界面的关键趋势、挑战和文献空白。筛选出90篇相关论文。开发了一个分面分类法，该分类法对公共安全领域的增强现实用户界面进行了分类。", "conclusion": "该分类法为未来的研究奠定了坚实基础，并强调了关键的设计考虑因素、挑战和文献空白。本综述为研究人员和开发者提供了宝贵资源，为该领域的进一步发展提供了见解。", "translation": "过去十年，将增强现实用户界面整合到公共安全应用（特别是急救医疗服务、消防和执法等急救人员领域）的研究显著增加。本文介绍了对公共安全领域增强现实用户界面应用进行范围综述的结果，并采用既定的系统综述方法对当前研究现状进行了全面分析，识别了文献中的关键趋势、挑战和空白。本次综述包含了截至2025年4月主要科学数据库收录的同行评审出版物。一项基本的关键词搜索检索到1,751篇论文，其中90篇被认为与本次综述相关。对文献的深入分析使得能够开发出一个分面分类法，对公共安全领域的增强现实用户界面进行分类。该分类法为未来的研究奠定了坚实基础，同时也强调了文献中的关键设计考虑因素、挑战和空白。本次综述为研究人员和开发者提供了宝贵资源，提供了可以推动该领域进一步发展的见解。", "summary": "本文对过去十年增强现实用户界面在公共安全领域，尤其是急救人员应用的研究进行了范围综述。通过系统的文献检索和分析，从1,751篇论文中筛选出90篇相关文献，识别了该领域的关键趋势、挑战和研究空白。研究还开发了一个分面分类法，对公共安全领域的增强现实用户界面进行了分类，为未来的研究提供了基础和设计指导。", "keywords": "增强现实用户界面, 急救人员, 公共安全, 范围综述, 分面分类法", "comments": "这篇综述通过系统的方法对AR在公共安全领域的应用进行了全面的梳理，其创新之处在于开发了一个分面分类法，这对于该领域的后续研究具有重要的指导意义。它不仅揭示了当前的研究热点和挑战，也为未来的设计和开发指明了方向。"}}
{"id": "2506.09891", "title": "Causal Climate Emulation with Bayesian Filtering", "authors": ["Sebastian Hickman", "Ilija Trajkovic", "Julia Kaltenborn", "Francis Pelletier", "Alex Archibald", "Yaniv Gurwicz", "Peer Nowack", "David Rolnick", "Julien Boussard"], "summary": "Traditional models of climate change use complex systems of coupled equations\nto simulate physical processes across the Earth system. These simulations are\nhighly computationally expensive, limiting our predictions of climate change\nand analyses of its causes and effects. Machine learning has the potential to\nquickly emulate data from climate models, but current approaches are not able\nto incorporate physics-informed causal relationships. Here, we develop an\ninterpretable climate model emulator based on causal representation learning.\nWe derive a physics-informed approach including a Bayesian filter for stable\nlong-term autoregressive emulation. We demonstrate that our emulator learns\naccurate climate dynamics, and we show the importance of each one of its\ncomponents on a realistic synthetic dataset and data from two widely deployed\nclimate models.", "comment": "32 pages, 21 figures", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09891v1", "AI": {"title_translation": "因果气候模拟与贝叶斯滤波", "tldr": "本文开发了一个基于因果表示学习和贝叶斯滤波的气候模型模拟器，旨在克服传统气候模型计算成本高和现有机器学习方法缺乏物理因果关系的问题。该模拟器能够快速准确地模拟气候动力学。", "motivation": "传统气候模型计算成本高昂，限制了气候变化预测和原因分析。现有机器学习方法无法有效纳入物理信息化的因果关系。", "method": "开发了一个基于因果表示学习的可解释气候模型模拟器，并推导了一种包含贝叶斯滤波的物理信息方法，用于稳定的长期自回归模拟。", "result": "该模拟器能够学习准确的气候动力学，并在真实的合成数据集和两个广泛部署的气候模型数据上展示了其每个组件的重要性。", "conclusion": "该因果气候模拟器能够有效且准确地模拟气候动力学，克服了传统模型的局限性，并为气候预测和分析提供了新的高效且可解释的工具。", "translation": "传统的气候变化模型使用复杂的耦合方程系统来模拟地球系统中的物理过程。这些模拟计算成本极高，限制了我们对气候变化的预测及其原因和影响的分析。机器学习有潜力快速模拟气候模型的数据，但当前的方法无法融入物理信息化的因果关系。在此，我们开发了一个基于因果表示学习的可解释气候模型模拟器。我们推导了一种包含贝叶斯滤波的物理信息方法，用于稳定的长期自回归模拟。我们证明了我们的模拟器能够学习准确的气候动力学，并在一个真实的合成数据集和来自两个广泛部署的气候模型的数据上展示了其每个组件的重要性。", "summary": "本文提出了一种结合因果表示学习和贝士滤波的气候模型模拟器，以解决传统气候模型计算成本高和现有机器学习方法无法融入物理因果关系的问题。该模拟器能够准确学习气候动力学，并在实际数据上验证了其有效性和各组件的重要性，为高效且可解释的气候预测和因果分析提供了新途径。", "keywords": "气候模拟, 因果表示学习, 贝叶斯滤波, 机器学习, 气候动力学", "comments": "该研究创新性地将因果表示学习与贝叶斯滤波相结合，构建了一个可解释且高效的气候模型模拟器。这不仅解决了传统气候模型计算成本高昂的问题，还弥补了现有机器学习方法在融入物理因果关系方面的不足，对于推动气候变化预测和理解具有重要意义。"}}
{"id": "2506.09410", "title": "Large-scale LH2 pipeline infrastructure concept for airports", "authors": ["H. A. Krog", "Y. Jooss", "H. Fyhn", "P. Nekså", "I. Hjorth"], "summary": "Infrastructure and processes for handling of liquid hydrogen (LH2) is needed\nto enable large-scale decarbonization of aviation with hydrogen aircraft. At\nlarge airports, pipeline and hydrant systems will be important for a mature\nhydrogen-powered air travel market. As the vaporization of LH2 is a challenge\nin fuel handling, the pipeline infrastructure must be designed and operated\nsuch that the fuel is subcooled. Through modelling and simulation of aircraft\ntanks refuelling by a pipeline infrastructure concept, it is found that\ncontinuous recycling of LH2 within the system is needed to maintain subcooling,\nand the pump operation is important for preventing flashing. With the proposed\nconcept, some hydrogen vapor is formed in the aircraft tank, but the vapor can\nbe utilised by hydrogen-powered ground support equipment.", "comment": null, "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.09410v1", "AI": {"title_translation": "机场大规模液氢管道基础设施概念", "tldr": "为实现航空业脱碳，研究了机场大规模液氢管道加注系统，发现需通过连续循环和泵操作保持液氢过冷，产生的蒸汽可供地面设备使用。", "motivation": "为了通过氢动力飞机实现航空业大规模脱碳，需要液氢（LH2）处理基础设施和流程。在大型机场，管道和加注系统对于成熟的氢动力航空旅行市场将非常重要。由于液氢的汽化是燃料处理中的一个挑战，因此管道基础设施必须设计和操作成使燃料保持过冷状态。", "method": "通过对飞机油箱通过管道基础设施概念进行加注的建模和模拟。", "result": "发现系统内需要连续循环液氢以保持过冷状态，并且泵操作对于防止闪蒸至关重要。在提出的概念下，飞机油箱中会形成一些氢蒸汽，但这些蒸汽可以被氢动力地面支持设备利用。", "conclusion": "为了支持航空业大规模脱碳，机场液氢管道基础设施必须通过连续循环和优化泵操作来维持液氢的过冷状态，并且产生的氢蒸汽可以有效利用。", "translation": "为了通过氢动力飞机实现航空业大规模脱碳，需要液氢（LH2）处理基础设施和流程。在大型机场，管道和加注系统对于成熟的氢动力航空旅行市场将非常重要。由于液氢的汽化是燃料处理中的一个挑战，因此管道基础设施必须设计和操作成使燃料保持过冷状态。通过对飞机油箱通过管道基础设施概念进行加注的建模和模拟，发现系统内需要连续循环液氢以保持过冷状态，并且泵操作对于防止闪蒸至关重要。在提出的概念下，飞机油箱中会形成一些氢蒸汽，但这些蒸汽可以被氢动力地面支持设备利用。", "summary": "本文提出了机场大规模液氢管道基础设施的概念，以支持航空业的脱碳。针对液氢汽化难题，研究通过建模和模拟发现，系统内需连续循环液氢并优化泵操作以保持燃料过冷，防止闪蒸。此外，飞机油箱中产生的氢蒸汽可被氢动力地面支持设备有效利用。", "keywords": "液氢, 管道基础设施, 航空脱碳, 机场, 加注系统", "comments": "这项研究对于推动航空业的氢能转型具有重要意义，它解决了液氢加注过程中保持燃料稳定性的关键技术挑战。提出的连续循环和蒸汽利用概念具有创新性，为机场未来液氢基础设施的设计提供了实用的解决方案。"}}
{"id": "2506.09703", "title": "Multi-Level Damage-Aware Graph Learning for Resilient UAV Swarm Networks", "authors": ["Huan Lin", "Chenguang Zhu", "Lianghui Ding", "Feng Yang"], "summary": "Unmanned aerial vehicle (UAV) swarm networks leverage resilient algorithms to\naddress communication network split issues and restore connectivity. However,\nexisting graph learning-based resilient algorithms face over-aggregation and\nnon-convergence problems caused by uneven and sparse topology under massive\ndamage scenarios. To alleviate these problems, we propose a novel Multi-Level\nDamage-Aware Graph Learning (ML-DAGL) algorithm, which generates recovery\ntrajectories by mining information from destroyed UAVs. We first introduce a\nMulti-Branch Damage Attention (MBDA) module, which forms a sequence of\nmulti-hop Damage Attentive Graphs (mDAG) with different ranges of receptive\nfields. Each mDAG links only remaining and damaged nodes to ensure a more even\ndegree distribution for mitigating over-aggregation, and utilizes multi-hop\ndilation to establish more links for sparse topology enhancement. To resort to\nthe mDAG, we propose a Dilated Graph Convolution Network (DGCN), which\ngenerates the optimal recovery trajectories with theoretically proven\nconvergence under massive damage cases. Simulation results show that the\nproposed algorithm can guarantee the connectivity restoration under large swarm\nand damage scales, while significantly expediting the recovery time by 75.94%\nand improving the topology uniformity after recovery.", "comment": "15 pages. arXiv admin note: text overlap with arXiv:2411.11342", "cate": "cs.NI", "url": "http://arxiv.org/abs/2506.09703v1", "AI": {"title_translation": "弹性无人机群网络的层级损伤感知图学习", "tldr": "提出ML-DAGL算法，通过多级损伤感知图学习解决无人机群网络在大规模损伤下的连接恢复问题，显著提高恢复速度和拓扑均匀性。", "motivation": "现有的基于图学习的弹性算法在无人机群网络面临大规模损伤时，由于拓扑结构不均匀和稀疏，存在过聚合和不收敛问题，导致通信网络分裂和连接中断。", "method": "提出多级损伤感知图学习（ML-DAGL）算法。该算法通过挖掘受损无人机信息生成恢复轨迹。它引入了多分支损伤注意力（MBDA）模块，形成一系列具有不同感受野范围的多跳损伤注意力图（mDAG）。每个mDAG仅连接剩余和受损节点，以确保更均匀的度分布来缓解过聚合，并利用多跳膨胀来建立更多链接以增强稀疏拓扑。ML-DAGL还提出了一个膨胀图卷积网络（DGCN），利用mDAG生成最优恢复轨迹，并在大规模损伤情况下具有理论证明的收敛性。", "result": "仿真结果表明，所提出的算法在大规模无人机群和损伤情况下能保证连接恢复，同时显著加快恢复时间75.94%，并提高了恢复后的拓扑均匀性。", "conclusion": "该研究成功开发了一种新颖的ML-DAGL算法，有效解决了无人机群网络在大规模损伤下的连接恢复挑战，显著提升了恢复效率和网络拓扑质量。", "translation": "无人机（UAV）群网络利用弹性算法来解决通信网络分裂问题并恢复连接性。然而，现有的基于图学习的弹性算法在大规模损伤情景下，由于拓扑结构不均匀和稀疏，面临过聚合和不收敛问题。为了缓解这些问题，我们提出了一种新颖的多级损伤感知图学习（ML-DAGL）算法，该算法通过挖掘受损无人机的信息来生成恢复轨迹。我们首先引入了一个多分支损伤注意力（MBDA）模块，它形成了一系列具有不同感受野范围的多跳损伤注意力图（mDAG）。每个mDAG仅连接剩余和受损节点，以确保更均匀的度分布以缓解过聚合，并利用多跳膨胀来建立更多链接以增强稀疏拓扑。为了利用mDAG，我们提出了一种膨胀图卷积网络（DGCN），它在理论上证明了在大规模损伤情况下的收敛性，从而生成最优恢复轨迹。仿真结果表明，所提出的算法在大规模群和损伤情况下能保证连接恢复，同时显著加快恢复时间75.94%，并提高了恢复后的拓扑均匀性。", "summary": "本文提出了一种新颖的多级损伤感知图学习（ML-DAGL）算法，旨在解决无人机群网络在遭受大规模损伤时通信连接恢复中的过聚合和不收敛问题。ML-DAGL通过引入多分支损伤注意力（MBDA）模块生成多跳损伤注意力图（mDAG），并结合膨胀图卷积网络（DGCN）来生成最优恢复轨迹。实验结果表明，该算法能有效恢复大规模受损无人机群网络的连接性，显著缩短恢复时间并提高拓扑均匀性。", "keywords": "无人机群网络, 图学习, 损伤感知, 连接恢复, 弹性算法", "comments": "这篇论文的创新点在于提出了ML-DAGL算法，特别是引入了MBDA模块和mDAG的概念，以更有效地处理大规模损伤下不均匀和稀疏的无人机群网络拓扑。通过仅连接剩余和受损节点以及使用多跳膨胀，它有效地缓解了图学习中常见的过聚合问题，并增强了稀疏连接。理论上证明的收敛性也增加了其方法的可靠性。该研究对于提升无人机群网络的鲁棒性和弹性具有重要意义。"}}
{"id": "2506.09606", "title": "Unmasking real-world audio deepfakes: A data-centric approach", "authors": ["David Combei", "Adriana Stan", "Dan Oneata", "Nicolas Müller", "Horia Cucu"], "summary": "The growing prevalence of real-world deepfakes presents a critical challenge\nfor existing detection systems, which are often evaluated on datasets collected\njust for scientific purposes. To address this gap, we introduce a novel dataset\nof real-world audio deepfakes. Our analysis reveals that these real-world\nexamples pose significant challenges, even for the most performant detection\nmodels. Rather than increasing model complexity or exhaustively search for a\nbetter alternative, in this work we focus on a data-centric paradigm, employing\nstrategies like dataset curation, pruning, and augmentation to improve model\nrobustness and generalization.\n  Through these methods, we achieve a 55% relative reduction in EER on the\nIn-the-Wild dataset, reaching an absolute EER of 1.7%, and a 63% reduction on\nour newly proposed real-world deepfakes dataset, AI4T. These results highlight\nthe transformative potential of data-centric approaches in enhancing deepfake\ndetection for real-world applications. Code and data available at:\nhttps://github.com/davidcombei/AI4T.", "comment": "Accepted at Interspeech 2025", "cate": "eess.AS", "url": "http://arxiv.org/abs/2506.09606v1", "AI": {"title_translation": "揭露真实世界音频深度伪造：一种以数据为中心的方法", "tldr": "本研究引入了一个新的真实世界音频深度伪造数据集，并采用以数据为中心的方法显著提升了深度伪造检测在真实世界应用中的性能。", "motivation": "现有深度伪造检测系统在真实世界深度伪造面前面临严峻挑战，因为它们通常在仅用于科学目的的数据集上进行评估，导致与真实世界应用的差距。", "method": "引入了一个名为AI4T的真实世界音频深度伪造新数据集。研究采用以数据为中心的方法，通过数据集整理、修剪和增强等策略来提高模型的鲁棒性和泛化能力，而非增加模型复杂性。", "result": "在In-the-Wild数据集上，等错误率（EER）相对降低了55%，达到1.7%的绝对EER。在AI4T新数据集上，EER降低了63%。", "conclusion": "以数据为中心的方法在增强真实世界应用的深度伪造检测方面具有变革性潜力。", "translation": "真实世界深度伪造日益普及，对现有检测系统构成了严峻挑战，这些系统通常仅在科学目的收集的数据集上进行评估。为了弥补这一差距，我们引入了一个新的真实世界音频深度伪造数据集。我们的分析表明，即使对于性能最佳的检测模型，这些真实世界的示例也带来了重大挑战。在这项工作中，我们没有增加模型复杂性或穷尽搜索更好的替代方案，而是专注于以数据为中心的范式，采用数据集整理、修剪和增强等策略来提高模型的鲁棒性和泛化能力。\n通过这些方法，我们在In-the-Wild数据集上实现了55%的EER相对降低，达到了1.7%的绝对EER，并在我们新提出的真实世界深度伪造数据集AI4T上实现了63%的降低。这些结果突显了以数据为中心的方法在增强真实世界应用的深度伪造检测方面的变革性潜力。代码和数据可在https://github.com/davidcombei/AI4T获取。", "summary": "该论文旨在解决现有深度伪造检测系统在真实世界音频深度伪造面前表现不佳的问题，原因在于其评估数据集与真实世界场景存在脱节。为此，研究引入了一个新的真实世界音频深度伪造数据集AI4T。论文提出了一种以数据为中心的方法，通过数据集的整理、修剪和增强来提升检测模型的鲁棒性和泛化能力。实验结果表明，该方法在In-the-Wild数据集上实现了55%的EER相对降低（绝对EER为1.7%），并在AI4T数据集上实现了63%的EER降低，有力证明了以数据为中心的方法在提升真实世界深度伪造检测方面的巨大潜力。", "keywords": "真实世界深度伪造, 音频深度伪造, 深度伪造检测, 以数据为中心, 数据集", "comments": "该论文的创新之处在于其以数据为中心的方法，而非传统地侧重于模型复杂性。通过创建并利用一个专门的真实世界音频深度伪造数据集AI4T，它有效地弥合了学术研究与实际应用之间的差距，为深度伪造检测领域提供了一个重要的新范式和实用资源。"}}
{"id": "2506.09280", "title": "TTrace: Lightweight Error Checking and Diagnosis for Distributed Training", "authors": ["Haitian Jiang", "Shaowei Zhu", "Zhen Zhang", "Zhenyu Song", "Xinwei Fu", "Zhen Jia", "Yida Wang", "Jinyang Li"], "summary": "Distributed training is essential for scaling the training of large neural\nnetwork models, such as large language models (LLMs), across thousands of GPUs.\nHowever, the complexity of distributed training programs makes them\nparticularly prone to silent bugs, which do not produce explicit error signal\nbut lead to incorrect training outcome. Effectively detecting and localizing\nsuch silent bugs in distributed training is challenging. Common debugging\npractice using metrics like training loss or gradient norm curves can be\ninefficient and ineffective. Additionally, obtaining intermediate tensor values\nand determining whether they are correct during silent bug localization is\ndifficult, particularly in the context of low-precision training.\n  To address those challenges, we design and implement TTrace, the first system\ncapable of detecting and localizing silent bugs in distributed training. TTrace\ncollects intermediate tensors from distributing training in a fine-grained\nmanner and compares them against those from a trusted single-device reference\nimplementation. To properly compare the floating-point values in the tensors,\nwe propose novel mathematical analysis that provides a guideline for setting\nthresholds, enabling TTrace to distinguish bug-induced errors from\nfloating-point round-off errors. Experimental results demonstrate that TTrace\neffectively detects 11 existing bugs and 3 new bugs in the widely used\nMegatron-LM framework, while requiring fewer than 10 lines of code change.\nTTrace is effective in various training recipes, including low-precision\nrecipes involving BF16 and FP8.", "comment": null, "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.09280v1", "AI": {"title_translation": "TTrace：分布式训练的轻量级错误检查与诊断", "tldr": "TTrace是首个能够通过比较中间张量并利用数学分析区分错误和舍入误差，从而有效检测和定位分布式训练中静默错误的系统。", "motivation": "分布式训练对于扩展大型神经网络模型至关重要，但其复杂性使其容易产生静默错误，这些错误不会产生明确的错误信号但会导致训练结果不正确。现有调试方法（如训练损失或梯度范数曲线）效率低下且无效，并且在低精度训练中，获取和确定中间张量值的正确性非常困难。", "method": "TTrace通过细粒度地收集分布式训练中的中间张量，并将其与可信的单设备参考实现进行比较。为了正确比较张量中的浮点值，TTrace提出了一种新颖的数学分析方法，为设置阈值提供了指导，从而能够区分由错误引起的误差和浮点舍入误差。", "result": "实验结果表明，TTrace有效地检测了广泛使用的Megatron-LM框架中的11个现有错误和3个新错误，并且只需不到10行代码修改。TTrace在各种训练方案中都有效，包括涉及BF16和FP8的低精度方案。", "conclusion": "TTrace是首个能够有效检测和定位分布式训练中静默错误的系统，它通过创新的张量比较和数学分析方法解决了传统调试的挑战，并在实际应用中展现出高效率和广泛的适用性，尤其是在低精度训练场景下。", "translation": "分布式训练对于扩展大型神经网络模型（如大型语言模型（LLM））至关关重要，可跨数千个GPU进行训练。然而，分布式训练程序的复杂性使其特别容易出现静默错误，这些错误不会产生明确的错误信号，但会导致不正确的训练结果。有效检测和定位分布式训练中的此类静默错误具有挑战性。使用训练损失或梯度范数曲线等指标的常见调试实践可能效率低下且无效。此外，在静默错误定位期间获取中间张量值并确定它们是否正确是困难的，特别是在低精度训练的背景下。\n为了解决这些挑战，我们设计并实现了TTrace，这是第一个能够检测和定位分布式训练中静默错误的系统。TTrace以细粒度方式从分布式训练中收集中间张量，并将其与可信的单设备参考实现进行比较。为了正确比较张量中的浮点值，我们提出了新颖的数学分析，为设置阈值提供了指导，使TTrace能够区分由错误引起的误差和浮点舍入误差。实验结果表明，TTrace有效地检测了广泛使用的Megatron-LM框架中的11个现有错误和3个新错误，同时只需不到10行代码修改。TTrace在各种训练方案中都有效，包括涉及BF16和FP8的低精度方案。", "summary": "本文介绍了TTrace，一个用于分布式训练的轻量级错误检查和诊断系统。TTrace旨在解决大型神经网络模型分布式训练中常见的静默错误难以检测和定位的问题。该系统通过细粒度地收集并比较分布式训练的中间张量与单设备参考实现，并利用新颖的数学分析来区分真正的错误和浮点舍入误差。实验证明，TTrace能够有效检测Megatron-LM框架中的多项现有和新错误，且仅需少量代码修改，并适用于包括低精度在内的多种训练场景。", "keywords": "分布式训练, 静默错误, 错误诊断, 张量比较, TTrace", "comments": "TTrace的创新性在于它是首个专门针对分布式训练中静默错误检测和定位的系统，其通过将中间张量与参考实现进行比较，并结合独特的数学分析来处理浮点精度问题，解决了现有调试方法的痛点。该系统的重要性体现在它能够显著提高大规模分布式训练的可靠性，尤其是在低精度训练日益普及的背景下，其轻量级和高效的特性使其具有很高的实用价值。"}}
{"id": "2506.09194", "title": "Integration of Contrastive Predictive Coding and Spiking Neural Networks", "authors": ["Emirhan Bilgiç", "Neslihan Serap Şengör", "Namık Berk Yalabık", "Yavuz Selim İşler", "Aykut Görkem Gelen", "Rahmi Elibol"], "summary": "This study examines the integration of Contrastive Predictive Coding (CPC)\nwith Spiking Neural Networks (SNN). While CPC learns the predictive structure\nof data to generate meaningful representations, SNN mimics the computational\nprocesses of biological neural systems over time. In this study, the goal is to\ndevelop a predictive coding model with greater biological plausibility by\nprocessing inputs and outputs in a spike-based system. The proposed model was\ntested on the MNIST dataset and achieved a high classification rate in\ndistinguishing positive sequential samples from non-sequential negative\nsamples. The study demonstrates that CPC can be effectively combined with SNN,\nshowing that an SNN trained for classification tasks can also function as an\nencoding mechanism. Project codes and detailed results can be accessed on our\nGitHub page: https://github.com/vnd-ogrenme/ongorusel-kodlama/tree/main/CPC_SNN", "comment": "4 pages, 5 figures, 1 table. Accepted at the 2025 33rd Signal\n  Processing and Communications Applications Conference (SIU)", "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.09194v1", "AI": {"title_translation": "对比预测编码与脉冲神经网络的集成", "tldr": "本研究探讨了对比预测编码（CPC）与脉冲神经网络（SNN）的集成，旨在开发一个更具生物合理性的预测编码模型，并在MNIST数据集上实现了高分类率。", "motivation": "本研究旨在通过在基于脉冲的系统中处理输入和输出，开发一个更具生物合理性的预测编码模型。", "method": "本研究将对比预测编码（CPC）与脉冲神经网络（SNN）进行集成，构建了一个预测编码模型，并在MNIST数据集上进行了测试。", "result": "所提出的模型在区分正向序列样本和非序列负向样本时，在MNIST数据集上实现了高分类率。", "conclusion": "本研究表明，对比预测编码（CPC）可以与脉冲神经网络（SNN）有效结合，并且经过分类任务训练的SNN也可以作为一种编码机制发挥作用。", "translation": "本研究探讨了对比预测编码（CPC）与脉冲神经网络（SNN）的集成。CPC学习数据的预测结构以生成有意义的表示，而SNN则模仿生物神经系统随时间变化的计算过程。本研究的目标是通过在基于脉冲的系统中处理输入和输出，开发一个更具生物合理性的预测编码模型。所提出的模型在MNIST数据集上进行了测试，并在区分正向序列样本和非序列负向样本方面取得了高分类率。本研究表明，CPC可以与SNN有效结合，这表明经过分类任务训练的SNN也可以作为一种编码机制发挥作用。项目代码和详细结果可在我们的GitHub页面访问：https://github.com/vnd-ogrenme/ongorusel-kodlama/tree/main/CPC_SNN", "summary": "本研究致力于将对比预测编码（CPC）与脉冲神经网络（SNN）相结合，以构建一个更具生物合理性的预测编码模型。该模型通过在基于脉冲的系统上处理数据，并在MNIST数据集上进行了验证，成功实现了对序列样本的高效分类。研究结果证实了CPC与SNN集成的可行性，并指出SNN不仅能用于分类，还能作为有效的编码机制。", "keywords": "对比预测编码, 脉冲神经网络, 生物合理性, 分类, 编码机制", "comments": "这项研究的创新之处在于将对比预测编码（CPC）与脉冲神经网络（SNN）结合，旨在提高预测编码模型的生物合理性。这种集成有望在更接近生物大脑计算方式的框架下，实现高效的数据表示学习和分类任务。其在MNIST数据集上的成功应用，展示了该方法在实际应用中的潜力，并为未来基于SNN的自监督学习提供了新的方向。"}}
{"id": "2506.09161", "title": "An Explainable Deep Learning Framework for Brain Stroke and Tumor Progression via MRI Interpretation", "authors": ["Rajan Das Gupta", "Md Imrul Hasan Showmick", "Mushfiqur Rahman Abir", "Shanjida Akter", "Md. Yeasin Rahat", "Md. Jakir Hossen"], "summary": "Early and accurate detection of brain abnormalities, such as tumors and\nstrokes, is essential for timely intervention and improved patient outcomes. In\nthis study, we present a deep learning-based system capable of identifying both\nbrain tumors and strokes from MRI images, along with their respective stages.\nWe have executed two groundbreaking strategies involving convolutional neural\nnetworks, MobileNet V2 and ResNet-50-optimized through transfer learning to\nclassify MRI scans into five diagnostic categories. Our dataset, aggregated and\naugmented from various publicly available MRI sources, was carefully curated to\nensure class balance and image diversity. To enhance model generalization and\nprevent overfitting, we applied dropout layers and extensive data augmentation.\nThe models achieved strong performance, with training accuracy reaching 93\\%\nand validation accuracy up to 88\\%. While ResNet-50 demonstrated slightly\nbetter results, Mobile Net V2 remains a promising option for real-time\ndiagnosis in low resource settings due to its lightweight architecture. This\nresearch offers a practical AI-driven solution for early brain abnormality\ndetection, with potential for clinical deployment and future enhancement\nthrough larger datasets and multi modal inputs.", "comment": "Accepted in MECON 2025", "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.09161v1", "AI": {"title_translation": "一种基于MRI解释的可解释深度学习框架，用于脑卒中和肿瘤进展", "tldr": "本研究开发了一个基于深度学习的系统，利用MobileNet V2和ResNet-50通过MRI图像识别脑肿瘤和中风及其阶段，实现了高准确率，并为早期临床诊断提供了实用的AI解决方案。", "motivation": "早期准确检测脑部异常（如肿瘤和中风）对于及时干预和改善患者预后至关重要。", "method": "本研究提出了一个基于深度学习的系统，能够从MRI图像中识别脑肿瘤和中风及其各自阶段。采用了两种基于卷积神经网络（MobileNet V2和ResNet-50）的策略，通过迁移学习进行优化，将MRI扫描分类为五种诊断类别。数据集从公开可用的MRI来源聚合和扩充，并进行精心策划以确保类别平衡和图像多样性。为增强模型泛化能力和防止过拟合，应用了dropout层和广泛的数据增强。", "result": "模型表现出强大的性能，训练准确率达到93%，验证准确率高达88%。ResNet-50表现略好，但MobileNet V2由于其轻量级架构，在资源受限环境下仍是实时诊断的有前景选择。", "conclusion": "这项研究为早期脑部异常检测提供了一个实用的AI驱动解决方案，具有临床部署的潜力，并可通过更大的数据集和多模态输入在未来得到增强。", "translation": "早期准确检测脑部异常，例如肿瘤和中风，对于及时干预和改善患者预后至关重要。在本研究中，我们提出了一个基于深度学习的系统，能够从MRI图像中识别脑肿瘤和中风及其各自阶段。我们采用了两种开创性的策略，涉及卷积神经网络MobileNet V2和ResNet-50，通过迁移学习进行优化，将MRI扫描分类为五种诊断类别。我们的数据集从各种公开可用的MRI来源聚合和扩充，并经过精心策划，以确保类别平衡和图像多样性。为了增强模型泛化能力并防止过拟合，我们应用了dropout层和广泛的数据增强。模型表现出强大的性能，训练准确率达到93%，验证准确率高达88%。虽然ResNet-50表现略好，但MobileNet V2由于其轻量级架构，在资源受限环境下仍是实时诊断的有前景选择。这项研究为早期脑部异常检测提供了一个实用的AI驱动解决方案，具有临床部署的潜力，并可通过更大的数据集和多模态输入在未来得到增强。", "summary": "本研究开发了一个基于深度学习的框架，利用MobileNet V2和ResNet-50两种卷积神经网络，通过迁移学习对MRI图像进行分类，以实现脑肿瘤和中风的早期检测及其阶段识别。该系统在分类精度上表现出色，训练准确率达93%，验证准确率达88%。研究强调了AI在临床诊断中的应用潜力，尤其是在资源受限环境下的实时诊断。", "keywords": "深度学习, 脑卒中, 脑肿瘤, MRI, 图像解释", "comments": "该论文提出了一种实用的AI驱动解决方案，用于早期脑部异常检测，具有重要的临床应用前景。其创新点在于结合了MobileNet V2和ResNet-50两种模型，并通过迁移学习和数据增强提升了模型的泛化能力。特别指出MobileNet V2在低资源环境下的潜力，增加了其实用价值。未来可进一步探索多模态输入以提升性能。"}}
{"id": "2506.09665", "title": "VideoMat: Extracting PBR Materials from Video Diffusion Models", "authors": ["Jacob Munkberg", "Zian Wang", "Ruofan Liang", "Tianchang Shen", "Jon Hasselgren"], "summary": "We leverage finetuned video diffusion models, intrinsic decomposition of\nvideos, and physically-based differentiable rendering to generate high quality\nmaterials for 3D models given a text prompt or a single image. We condition a\nvideo diffusion model to respect the input geometry and lighting condition.\nThis model produces multiple views of a given 3D model with coherent material\nproperties. Secondly, we use a recent model to extract intrinsics (base color,\nroughness, metallic) from the generated video. Finally, we use the intrinsics\nalongside the generated video in a differentiable path tracer to robustly\nextract PBR materials directly compatible with common content creation tools.", "comment": null, "cate": "cs.GR", "url": "http://arxiv.org/abs/2506.09665v1", "AI": {"title_translation": "VideoMat：从视频扩散模型中提取PBR材质", "tldr": "VideoMat利用微调的视频扩散模型、视频内在分解和可微分渲染，根据文本提示或单张图像为3D模型生成高质量的PBR材质。", "motivation": "为3D模型生成高质量的PBR（基于物理的渲染）材质，可以通过文本提示或单张图像作为输入。", "method": "该方法首先调整视频扩散模型，使其符合输入的几何和光照条件，从而生成具有一致材质属性的给定3D模型的多个视图。其次，利用一个近期模型从生成的视频中提取内在属性（基色、粗糙度、金属度）。最后，将这些内在属性和生成的视频一起用于可微分路径追踪器中，以稳健地提取与常用内容创建工具直接兼容的PBR材质。", "result": "该方法能够为3D模型生成高质量的材质，并稳健地提取与常用内容创建工具直接兼容的PBR材质。", "conclusion": "通过结合视频扩散模型、内在分解和可微分渲染，VideoMat成功开发了一种从文本/图像输入生成高质量、兼容性强的PBR材质的方法。", "translation": "我们利用微调的视频扩散模型、视频的内在分解以及基于物理的可微分渲染，根据文本提示或单张图像为3D模型生成高质量的材质。我们对视频扩散模型进行条件化处理，使其符合输入的几何和光照条件。该模型能生成给定3D模型的多个视图，并具有一致的材质属性。其次，我们使用一个近期模型从生成的视频中提取内在属性（基色、粗糙度、金属度）。最后，我们将这些内在属性与生成的视频一起用于可微分路径追踪器中，以稳健地提取与常用内容创建工具直接兼容的PBR材质。", "summary": "VideoMat提出了一种新颖的方法，通过结合微调的视频扩散模型、视频内在分解和可微分渲染，从文本提示或单张图像中自动生成高质量的3D模型PBR材质。该方法首先通过条件化视频扩散模型生成具有一致材质属性的多视图视频，然后从中提取内在属性，最后通过可微分路径追踪器提取出与现有内容创建工具兼容的PBR材质。", "keywords": "PBR材质, 视频扩散模型, 内在分解, 可微分渲染, 3D模型", "comments": "这项研究的创新之处在于将视频扩散模型应用于PBR材质提取，提供了一种从文本或图像输入生成3D模型高质量材质的自动化流程。其重要性在于简化了3D内容创作中材质生成的复杂性，提高了工作效率和材质质量。该方法生成的材质与常见工具兼容，具有很强的实用性。"}}
{"id": "2506.09599", "title": "Energy Aware Development of Neuromorphic Implantables: From Metrics to Action", "authors": ["Enrique Barba Roque", "Luis Cruz"], "summary": "Spiking Neural Networks (SNNs) and neuromorphic computing present a promising\nalternative to traditional Artificial Neural Networks (ANNs) by significantly\nimproving energy efficiency, particularly in edge and implantable devices.\nHowever, assessing the energy performance of SNN models remains a challenge due\nto the lack of standardized and actionable metrics and the difficulty of\nmeasuring energy consumption in experimental neuromorphic hardware. In this\npaper, we conduct a preliminary exploratory study of energy efficiency metrics\nproposed in the SNN benchmarking literature. We classify 13 commonly used\nmetrics based on four key properties: Accessibility, Fidelity, Actionability,\nand Trend-Based analysis. Our findings indicate that while many existing\nmetrics provide useful comparisons between architectures, they often lack\npractical insights for SNN developers. Notably, we identify a gap between\naccessible and high-fidelity metrics, limiting early-stage energy assessment.\nAdditionally, we emphasize the lack of metrics that provide practitioners with\nactionable insights, making it difficult to guide energy-efficient SNN\ndevelopment. To address these challenges, we outline research directions for\nbridging accessibility and fidelity and finding new Actionable metrics for\nimplantable neuromorphic devices, introducing more Trend-Based metrics, metrics\nthat reflect changes in power requirements, battery-aware metrics, and\nimproving energy-performance tradeoff assessments. The results from this paper\npave the way for future research on enhancing energy metrics and their\nActionability for SNNs.", "comment": "ICT45 2025 submission", "cate": "cs.NE", "url": "http://arxiv.org/abs/2506.09599v1", "AI": {"title_translation": "神经形态植入物中的能量感知开发：从度量到行动", "tldr": "该研究探讨了SNN能量效率度量方法的不足，指出现有度量缺乏实用性和可操作性，并提出了未来研究方向以开发更有效的度量方法，以促进神经形态植入设备的能效开发。", "motivation": "尽管脉冲神经网络（SNNs）和神经形态计算在边缘和植入设备中具有显著的能效优势，但由于缺乏标准化、可操作的度量标准以及实验性神经形态硬件能量测量的困难，评估SNN模型的能量性能仍然是一个挑战。", "method": "本文对SNN基准测试文献中提出的能效度量标准进行了初步探索性研究。研究人员根据可访问性、保真度、可操作性和基于趋势的分析这四个关键属性，对13个常用度量标准进行了分类。", "result": "研究发现，尽管许多现有度量标准为架构提供了有用的比较，但它们往往缺乏对SNN开发人员的实用见解。特别地，可访问性高但保真度低的度量标准与保真度高但可访问性低的度量标准之间存在差距，这限制了早期阶段的能量评估。此外，还缺乏为从业者提供可操作性见解的度量标准，这使得指导能效SNN开发变得困难。", "conclusion": "为了解决这些挑战，本文提出了未来的研究方向，包括弥合可访问性和保真度之间的差距，为植入式神经形态设备寻找新的可操作性度量标准，引入更多基于趋势的度量标准、反映功率需求变化的度量标准、电池感知度量标准，并改进能量-性能权衡评估。本文的研究结果为未来增强SNN能效度量标准及其可操作性的研究铺平了道路。", "translation": "脉冲神经网络（SNNs）和神经形态计算通过显著提高能效，特别是对于边缘和植入设备，为传统人工神经网络（ANNs）提供了一种有前景的替代方案。然而，由于缺乏标准化和可操作的度量标准，以及在实验性神经形态硬件中测量能耗的困难，评估SNN模型的能量性能仍然是一个挑战。在本文中，我们对SNN基准测试文献中提出的能效度量标准进行了初步探索性研究。我们根据四个关键属性对13个常用度量标准进行了分类：可访问性、保真度、可操作性和基于趋势的分析。我们的研究结果表明，尽管许多现有度量标准在架构之间提供了有用的比较，但它们往往缺乏对SNN开发人员的实用见解。值得注意的是，我们发现可访问性高但保真度低的度量标准与保真度高但可访问性低的度量标准之间存在差距，这限制了早期阶段的能量评估。此外，我们强调缺乏能为从业者提供可操作性见解的度量标准，这使得指导能效SNN开发变得困难。为了应对这些挑战，我们概述了弥合可访问性和保真度差距、为植入式神经形态设备寻找新的可操作性度量标准、引入更多基于趋势的度量标准、反映功率需求变化的度量标准、电池感知度量标准以及改进能量-性能权衡评估的研究方向。本文的研究结果为未来增强SNN能效度量标准及其可操作性的研究铺平了道路。", "summary": "本研究旨在解决脉冲神经网络（SNNs）和神经形态计算在边缘和植入设备中评估能量性能的挑战。文章对SNN基准测试文献中的13种能效度量标准进行了分类和探索性研究，发现现有度量标准在实用性、可访问性与保真度之间存在差距，且缺乏可操作性，难以指导SNN的能效开发。为此，论文提出了未来研究方向，包括开发更具可操作性、基于趋势、电池感知以及能反映功率变化的度量标准，以期为神经形态植入设备的能效开发提供更有效的指导。", "keywords": "脉冲神经网络, 神经形态计算, 能量效率, 植入设备, 度量标准", "comments": "本文识别并分类了SNN能效评估中现有度量标准的不足，特别强调了实用性、可操作性以及可访问性与保真度之间的矛盾。其创新之处在于系统性地梳理了当前面临的挑战，并为未来的研究指明了清晰的方向，尤其关注了植入式设备的特殊需求，这对于推动神经形态计算在实际应用中的落地具有重要意义。"}}
{"id": "2506.09950", "title": "Oracle-Based Multistep Strategy for Solving Polynomial Systems Over Finite Fields and Algebraic Cryptanalysis of the Aradi Cipher", "authors": ["La Scala Roberto", "Sharwan Kumar Tiwari"], "summary": "The multistep solving strategy consists in a divide-and-conquer approach:\nwhen a multivariate polynomial system is computationally infeasible to solve\ndirectly, one variable is assigned over the elements of the base finite field,\nand the procedure is recursively applied to the resulting simplified systems.\nIn a previous work by the same authors (among others), this approach proved\neffective in the algebraic cryptanalysis of the Trivium cipher. In this paper,\nwe present a new implementation of the corresponding algorithm based on a\nDepth-First Search strategy, along with a novel complexity analysis leveraging\ntree structures. We further introduce the notion of an \"oracle function\" as a\ngeneral predictive tool for deciding whether the evaluation of a new variable\nis necessary to simplify the current polynomial system. This notion allows us\nto unify all previously proposed variants of the multistep strategy, including\nthe classical hybrid approach, by appropriately selecting the oracle function.\nFinally, we apply the multistep solving strategy to the cryptanalysis of the\nlow-latency block cipher Aradi, recently introduced by the NSA. We present the\nfirst full round algebraic attack, raising concerns about the cipher's actual\nsecurity with respect to its key length.", "comment": "19 pages", "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.09950v1", "AI": {"title_translation": "基于Oracle的多步策略用于有限域多项式系统求解与Aradi密码的代数密码分析", "tldr": "本文提出了一种基于深度优先搜索和“oracle函数”的多步策略新实现，用于解决有限域上的多项式系统，并将其应用于Aradi密码的代数密码分析，首次实现了对Aradi的完整轮次攻击。", "motivation": "当多元多项式系统难以直接计算求解时，需要一种有效的策略。作者之前的多步求解策略在Trivium密码的代数密码分析中证明有效，因此本文旨在提出该策略的新实现，并引入“oracle函数”以统一和改进现有方法，最终应用于对新密码Aradi的分析。", "method": "本文采用多步求解策略，该策略是一种分治法，通过递归地对简化系统应用过程，将一个变量分配给基有限域的元素。具体方法包括：1. 提出基于深度优先搜索策略的新算法实现。2. 进行利用树结构的新颖复杂性分析。3. 引入“oracle函数”作为通用预测工具，用于判断是否需要评估新变量来简化当前多项式系统，从而统一了之前所有提出的多步策略变体。4. 将该策略应用于低延迟分组密码Aradi的密码分析。", "result": "本文成功对Aradi密码进行了首次完整轮次的代数攻击，这引发了对其密钥长度实际安全性的担忧。", "conclusion": "多步求解策略，特别是结合“oracle函数”的新实现，能够有效地对有限域上的多项式系统进行求解，并成功应用于对Aradi密码的代数密码分析，揭示了其潜在的安全漏洞。", "translation": "多步求解策略是一种分治方法：当多元多项式系统计算上难以直接求解时，将一个变量分配给基有限域的元素，并递归地将该过程应用于由此产生的简化系统。在同一作者（及其他作者）之前的工作中，这种方法在Trivium密码的代数密码分析中被证明是有效的。在本文中，我们提出了基于深度优先搜索策略的相应算法的新实现，以及利用树结构的新颖复杂性分析。我们进一步引入了“oracle函数”的概念，作为一种通用的预测工具，用于决定是否需要评估一个新变量来简化当前的多项式系统。通过适当选择oracle函数，这个概念使我们能够统一所有先前提出的多步策略变体，包括经典的混合方法。最后，我们将多步求解策略应用于美国国家安全局最近引入的低延迟分组密码Aradi的密码分析。我们提出了首次完整轮次的代数攻击，引发了对该密码相对于其密钥长度的实际安全性的担忧。", "summary": "本文提出了一种基于深度优先搜索的多步求解策略新实现，并引入了“oracle函数”以统一和优化现有变体。该策略旨在解决有限域上计算复杂的多项式系统。作者将此方法应用于美国国家安全局（NSA）新引入的Aradi密码的代数密码分析，成功实施了首次完整轮次的代数攻击，从而对Aradi密码的实际安全性提出了质疑。", "keywords": "多步策略, 有限域多项式系统, 代数密码分析, Oracle函数, Aradi密码", "comments": "本文的创新点在于引入了“oracle函数”的概念，该函数能够统一并优化现有的多步求解策略变体，这大大提高了该方法的通用性和效率。此外，将该策略成功应用于Aradi密码的代数密码分析，并实现了首次完整轮次攻击，这不仅验证了方法的有效性，也对Aradi密码的实际安全性提出了重要警告，具有显著的实践意义。"}}
{"id": "2506.09209", "title": "Revisiting Graph Projections for Effective Complementary Product Recommendation", "authors": ["Leandro Anghinoni", "Pablo Zivic", "Jorge Adrian Sanchez"], "summary": "Complementary product recommendation is a powerful strategy to improve\ncustomer experience and retail sales. However, recommending the right product\nis not a simple task because of the noisy and sparse nature of user-item\ninteractions. In this work, we propose a simple yet effective method to predict\na list of complementary products given a query item, based on the structure of\na directed weighted graph projected from the user-item bipartite graph. We\nrevisit bipartite graph projections for recommender systems and propose a novel\napproach for inferring complementarity relationships from historical user-item\ninteractions. We compare our model with recent methods from the literature and\nshow, despite the simplicity of our approach, an average improvement of +43%\nand +38% over sequential and graph-based recommenders, respectively, over\ndifferent benchmarks.", "comment": null, "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.09209v1", "AI": {"title_translation": "重新审视图投影以实现有效的互补产品推荐", "tldr": "本文提出了一种简单而有效的图投影方法，用于互补产品推荐，其性能优于现有的序列和基于图的推荐器。", "motivation": "互补产品推荐是提升客户体验和零售销售的有效策略，但由于用户-物品交互的噪声和稀疏性，准确推荐具有挑战性。", "method": "本文提出了一种基于从用户-物品二分图投影得到的有向加权图结构来预测互补产品列表的方法。该方法重新审视了推荐系统中的二分图投影，并提出了一种从历史用户-物品交互中推断互补关系的新方法。", "result": "尽管方法简单，但与序列推荐器和基于图的推荐器相比，在不同基准测试中，平均性能分别提高了+43%和+38%。", "conclusion": "提出的简单图投影方法在互补产品推荐方面非常有效，显著优于现有的复杂方法。", "translation": "互补产品推荐是提高客户体验和零售销售的强大策略。然而，由于用户-物品交互的噪声和稀疏性，推荐正确的产品并非一项简单的任务。在这项工作中，我们提出了一种简单而有效的方法，基于从用户-物品二分图投影得到的有向加权图的结构，预测给定查询物品的互补产品列表。我们重新审视了推荐系统中的二分图投影，并提出了一种从历史用户-物品交互中推断互补关系的新方法。我们将我们的模型与文献中的最新方法进行了比较，结果表明，尽管我们的方法很简单，但在不同基准测试中，与序列推荐器和基于图的推荐器相比，分别平均提高了+43%和+38%。", "summary": "本文针对用户-物品交互稀疏和噪声环境下互补产品推荐的挑战，提出了一种新颖、简单且有效的方法。该方法利用从用户-物品二分图投影得到的有向加权图来推断互补关系。实验结果表明，其性能优越，与当前的序列和基于图的推荐系统相比，取得了显著改进。", "keywords": "互补产品推荐, 图投影, 二分图, 推荐系统, 用户-物品交互", "comments": "该论文的创新之处在于其方法的简洁性和高效性，表明重新审视基础图投影技术可以在互补产品推荐领域取得实质性进展，甚至超越更复杂的现代方法，突显了良好应用基础方法的强大潜力。"}}
{"id": "2506.09211", "title": "An Introduction to Solving the Least-Squares Problem in Variational Data Assimilation", "authors": ["I. Daužickaitė", "M. A. Freitag", "S. Gürol", "A. S. Lawless", "A. Ramage", "J. A. Scott", "J. M. Tabeart"], "summary": "Variational data assimilation is a technique for combining measured data with\ndynamical models. It is a key component of Earth system state estimation and is\ncommonly used in weather and ocean forecasting. The approach involves a\nlarge-scale generalized nonlinear least-squares problem. Solving the resulting\nsequence of sparse linear subproblems requires the use of sophisticated\nnumerical linear algebra methods. In practical applications, the computational\ndemands severely limit the number of iterations of a Krylov subspace solver\nthat can be performed and so high-quality preconditioners are vital. In this\npaper, we introduce variational data assimilation from a numerical linear\nalgebra perspective and review current solution techniques, with a focus on the\nchallenges that arise in large-scale geophysical systems.", "comment": null, "cate": "math.NA", "url": "http://arxiv.org/abs/2506.09211v1", "AI": {"title_translation": "变分数据同化中最小二乘问题的求解介绍", "tldr": "本文从数值线性代数角度介绍了变分数据同化，重点关注其大规模最小二乘问题的求解，特别是地球物理系统中计算限制下高质量预处理器的重要性。", "motivation": "变分数据同化是地球系统状态估计（如天气和海洋预报）的关键技术，但其涉及的大规模非线性最小二乘问题计算量巨大，限制了求解器的迭代次数，因此高质量的预处理器至关重要。", "method": "本文从数值线性代数角度介绍了变分数据同化，并回顾了当前的求解技术，重点关注大规模地球物理系统中出现的挑战。", "result": "本文作为一篇介绍性文章，概述了变分数据同化在数值线性代数方面的挑战和现有解决方案，并未提出新的研究结果。", "conclusion": "在大型地球物理系统中，解决变分数据同化中的最小二乘问题需要先进的数值线性代数方法和高质量的预处理器来克服计算限制。", "translation": "变分数据同化是一种将测量数据与动力学模型相结合的技术。它是地球系统状态估计的关键组成部分，常用于天气和海洋预报。该方法涉及一个大规模的广义非线性最小二乘问题。求解由此产生的稀疏线性子问题序列需要使用复杂的数值线性代数方法。在实际应用中，计算需求严重限制了Krylov子空间求解器可执行的迭代次数，因此高质量的预处理器至关重要。在本文中，我们从数值线性代数的角度介绍了变分数据同化，并回顾了当前的求解技术，重点关注大规模地球物理系统中出现的挑战。", "summary": "本文从数值线性代数的视角介绍了变分数据同化，该技术是地球系统状态估计（如天气和海洋预报）的关键组成部分。文章阐述了变分数据同化如何涉及求解大规模广义非线性最小二乘问题，并强调了在实际应用中，由于计算限制，需要复杂的数值线性代数方法和高质量的预处理器来有效解决由此产生的稀疏线性子问题，尤其是在大型地球物理系统中的挑战。", "keywords": "变分数据同化, 最小二乘问题, 数值线性代数, 预处理器, 地球物理系统", "comments": "本文为理解变分数据同化中的数值线性代数挑战提供了一个有价值的入门。它清晰地概述了核心计算问题，特别是预处理器在迭代求解器和地球物理系统大规模应用中的关键作用。其对实际限制的关注使其与计算科学和地球系统建模领域的研究人员高度相关。"}}
{"id": "2506.09189", "title": "Fractional Fourier Sound Synthesis", "authors": ["Esteban Gutiérrez", "Rodrigo Cádiz", "Carlos Sing Long", "Frederic Font", "Xavier Serra"], "summary": "This paper explores the innovative application of the Fractional Fourier\nTransform (FrFT) in sound synthesis, highlighting its potential to redefine\ntime-frequency analysis in audio processing. As an extension of the classical\nFourier Transform, the FrFT introduces fractional order parameters, enabling a\ncontinuous interpolation between time and frequency domains and unlocking\nunprecedented flexibility in signal manipulation. Crucially, the FrFT also\nopens the possibility of directly synthesizing sounds in the alpha-domain,\nproviding a unique framework for creating timbral and dynamic characteristics\nunattainable through conventional methods. This work delves into the\nmathematical principles of the FrFT, its historical evolution, and its\ncapabilities for synthesizing complex audio textures. Through experimental\nanalyses, we showcase novel sound design techniques, such as alpha-synthesis\nand alpha-filtering, which leverage the FrFT's time-frequency rotation\nproperties to produce innovative sonic results. The findings affirm the FrFT's\nvalue as a transformative tool for composers, sound designers, and researchers\nseeking to push the boundaries of auditory creativity.", "comment": "Accepted to the International Computer Music Conference (ICMC) 2025\n  held in Boston, USA. 6 pages and 2 figures", "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.09189v1", "AI": {"title_translation": "分数傅里叶声音合成", "tldr": "本文探讨分数傅里叶变换（FrFT）在声音合成中的创新应用，展示其在音频处理中重新定义时频分析的潜力，并能实现传统方法无法达到的音色和动态特性。", "motivation": "探索分数傅里叶变换（FrFT）在声音合成中的创新应用，旨在重新定义音频处理中的时频分析，并提供传统方法无法实现的音色和动态特性。", "method": "深入研究FrFT的数学原理、历史演变及其合成复杂音频纹理的能力。通过实验分析，展示了利用FrFT时频旋转特性（如alpha-synthesis和alpha-filtering）的新颖声音设计技术。", "result": "实验分析展示了新颖的声音设计技术，如alpha-synthesis和alpha-filtering，利用FrFT的时频旋转特性产生了创新的声音结果。", "conclusion": "研究结果证实了FrFT作为一种变革性工具的价值，对于寻求突破听觉创意界限的作曲家、声音设计师和研究人员具有重要意义。", "translation": "本文探讨了分数傅里叶变换（FrFT）在声音合成中的创新应用，强调其在音频处理中重新定义时频分析的潜力。作为经典傅里叶变换的扩展，FrFT引入了分数阶参数，实现了时间域和频率域之间的连续插值，并解锁了前所未有的信号操作灵活性。至关重要的是，FrFT还开辟了直接在alpha域合成声音的可能性，为创建传统方法无法达到的音色和动态特性提供了独特的框架。这项工作深入探讨了FrFT的数学原理、其历史演变及其合成复杂音频纹理的能力。通过实验分析，我们展示了新颖的声音设计技术，如alpha-synthesis和alpha-filtering，这些技术利用FrFT的时频旋转特性来产生创新的声音结果。研究结果证实了FrFT作为一种变革性工具的价值，对于寻求突破听觉创意界限的作曲家、声音设计师和研究人员具有重要意义。", "summary": "本文探讨分数傅里叶变换（FrFT）在声音合成中的创新应用。FrFT作为经典傅里叶变换的扩展，通过引入分数阶参数，实现了时频域的连续插值，并能直接在alpha域合成声音，从而创造传统方法无法实现的音色和动态特性。研究深入分析了FrFT的数学原理和合成能力，并通过实验展示了alpha-synthesis和alpha-filtering等新颖声音设计技术，证明FrFT是音频创意领域的变革性工具。", "keywords": "分数傅里叶变换, 声音合成, 时频分析, 音频处理, 信号操作", "comments": "该论文的创新点在于将分数傅里叶变换（FrFT）应用于声音合成，这不仅扩展了傅里叶变换在音频处理中的应用范围，更重要的是，它提供了一种全新的时频分析和声音合成框架，能够实现传统方法难以达到的独特音色和动态效果。这为作曲家、声音设计师和研究人员带来了突破性的工具，有望推动听觉创意和声音设计的边界。"}}
{"id": "2506.09525", "title": "Beyond Personalization: Federated Recommendation with Calibration via Low-rank Decomposition", "authors": ["Jundong Chen", "Honglei Zhang", "Haoxuan Li", "Chunxu Zhang", "Zhiwei Li", "Yidong Li"], "summary": "Federated recommendation (FR) is a promising paradigm to protect user privacy\nin recommender systems. Distinct from general federated scenarios, FR\ninherently needs to preserve client-specific parameters, i.e., user embeddings,\nfor privacy and personalization. However, we empirically find that globally\naggregated item embeddings can induce skew in user embeddings, resulting in\nsuboptimal performance. To this end, we theoretically analyze the user\nembedding skew issue and propose Personalized Federated recommendation with\nCalibration via Low-Rank decomposition (PFedCLR). Specifically, PFedCLR\nintroduces an integrated dual-function mechanism, implemented with a buffer\nmatrix, to jointly calibrate local user embedding and personalize global item\nembeddings. To ensure efficiency, we employ a low-rank decomposition of the\nbuffer matrix to reduce the model overhead. Furthermore, for privacy, we train\nand upload the local model before personalization, preventing the server from\naccessing sensitive information. Extensive experiments demonstrate that PFedCLR\neffectively mitigates user embedding skew and achieves a desirable trade-off\namong performance, efficiency, and privacy, outperforming state-of-the-art\n(SOTA) methods.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.09525v1", "AI": {"title_translation": "超越个性化：通过低秩分解校准的联邦推荐", "tldr": "本文提出了PFedCLR，一种新的联邦推荐方法，通过低秩分解校准局部用户嵌入并个性化全局项目嵌入，以解决用户嵌入偏差问题，同时兼顾性能、效率和隐私。", "motivation": "联邦推荐（FR）作为一种保护用户隐私的推荐系统范式很有前景。然而，研究发现全局聚合的项目嵌入会导致用户嵌入出现偏差，从而影响性能。因此，需要一种方法来解决用户嵌入偏差问题并提升联邦推荐的性能。", "method": "本文提出了PFedCLR（个性化联邦推荐与低秩分解校准）方法。PFedCLR引入了一个集成的双功能机制，通过一个缓冲矩阵来实现，用于联合校准本地用户嵌入和个性化全局项目嵌入。为提高效率，对缓冲矩阵进行低秩分解。为保护隐私，在个性化之前训练并上传本地模型，防止服务器访问敏感信息。", "result": "实验结果表明，PFedCLR有效缓解了用户嵌入偏差问题，并在性能、效率和隐私之间取得了理想的权衡，优于最先进的方法。", "conclusion": "PFedCLR成功地解决了联邦推荐中用户嵌入偏差的问题，通过创新的校准和个性化机制，在保护隐私的同时显著提升了系统性能和效率。", "translation": "联邦推荐（FR）是一种很有前景的保护推荐系统用户隐私的范式。与一般联邦场景不同，FR本质上需要保留客户端特有的参数，即用户嵌入，以实现隐私和个性化。然而，我们凭经验发现，全局聚合的项目嵌入可能会导致用户嵌入出现偏差，从而导致次优的性能。为此，我们从理论上分析了用户嵌入偏差问题，并提出了通过低秩分解进行校准的个性化联邦推荐（PFedCLR）。具体来说，PFedCLR引入了一个集成的双功能机制，通过一个缓冲矩阵实现，用于联合校准本地用户嵌入和个性化全局项目嵌入。为确保效率，我们采用了缓冲矩阵的低秩分解来减少模型开销。此外，为了隐私，我们在个性化之前训练并上传本地模型，防止服务器访问敏感信息。广泛的实验表明，PFedCLR有效缓解了用户嵌入偏差，并在性能、效率和隐私之间取得了理想的权衡，优于最先进（SOTA）的方法。", "summary": "该论文提出了一种名为PFedCLR的联邦推荐系统，旨在解决现有联邦推荐中全局聚合导致的用户嵌入偏差问题。PFedCLR通过引入一个带有缓冲矩阵的双功能机制，利用低秩分解来校准本地用户嵌入并个性化全局项目嵌入，同时在个性化前上传本地模型以保护隐私。实验证明，PFedCLR在性能、效率和隐私之间实现了良好平衡，并超越了现有方法。", "keywords": "联邦推荐, 用户嵌入偏差, 低秩分解, 隐私保护, 个性化", "comments": "PFedCLR的创新之处在于其双功能机制，能够同时校准用户嵌入和个性化项目嵌入，并通过低秩分解有效降低了计算开销。它解决了联邦推荐中一个核心的性能瓶颈——用户嵌入偏差问题，同时兼顾了隐私保护，这对于实际应用具有重要意义。"}}
{"id": "2506.09383", "title": "Bipedal Balance Control with Whole-body Musculoskeletal Standing and Falling Simulations", "authors": ["Chengtian Ma", "Yunyue Wei", "Chenhui Zuo", "Chen Zhang", "Yanan Sui"], "summary": "Balance control is important for human and bipedal robotic systems. While\ndynamic balance during locomotion has received considerable attention,\nquantitative understanding of static balance and falling remains limited. This\nwork presents a hierarchical control pipeline for simulating human balance via\na comprehensive whole-body musculoskeletal system. We identified spatiotemporal\ndynamics of balancing during stable standing, revealed the impact of muscle\ninjury on balancing behavior, and generated fall contact patterns that aligned\nwith clinical data. Furthermore, our simulated hip exoskeleton assistance\ndemonstrated improvement in balance maintenance and reduced muscle effort under\nperturbation. This work offers unique muscle-level insights into human balance\ndynamics that are challenging to capture experimentally. It could provide a\nfoundation for developing targeted interventions for individuals with balance\nimpairments and support the advancement of humanoid robotic systems.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.09383v1", "AI": {"title_translation": "双足平衡控制与全身肌肉骨骼站立和跌倒模拟", "tldr": "该研究利用全身肌肉骨骼系统模拟了人类平衡的层次控制，揭示了平衡的时空动力学、肌肉损伤的影响和跌倒模式，并展示了髋部外骨骼辅助的有效性，为平衡障碍干预和类人机器人发展提供了独特的肌肉层面见解。", "motivation": "尽管步态中的动态平衡已受到广泛关注，但对静态平衡和跌倒的定量理解仍然有限。该研究旨在通过模拟来深入理解人类平衡的肌肉层面动力学。", "method": "本研究提出了一种分层控制流程，通过一个全面的全身肌肉骨骼系统来模拟人类平衡。", "result": "研究识别了稳定站立期间平衡的时空动力学；揭示了肌肉损伤对平衡行为的影响；生成了与临床数据一致的跌倒接触模式；模拟的髋部外骨骼辅助在扰动下改善了平衡维持并减少了肌肉劳损。", "conclusion": "这项工作为人类平衡动力学提供了独特的肌肉层面见解，这些见解通过实验难以捕捉。它可能为开发针对平衡障碍个体的干预措施提供基础，并支持类人机器人系统的发展。", "translation": "平衡控制对于人类和双足机器人系统至关重要。尽管步态中的动态平衡已受到相当大的关注，但对静态平衡和跌倒的定量理解仍然有限。这项工作提出了一种分层控制流程，用于通过全面的全身肌肉骨骼系统模拟人类平衡。我们识别了稳定站立期间平衡的时空动力学，揭示了肌肉损伤对平衡行为的影响，并生成了与临床数据一致的跌倒接触模式。此外，我们模拟的髋部外骨骼辅助在扰动下表现出平衡维持的改善和肌肉劳损的减少。这项工作为人类平衡动力学提供了独特的肌肉层面见解，这些见解通过实验难以捕捉。它可能为开发针对平衡障碍个体的干预措施提供基础，并支持类人机器人系统的发展。", "summary": "本研究通过构建一个分层控制流程和全面的全身肌肉骨骼系统，模拟了人类的平衡控制、站立和跌倒过程。研究揭示了稳定站立的平衡动力学、肌肉损伤对平衡的影响以及与临床数据相符的跌倒模式。此外，模拟结果表明髋部外骨骼辅助能有效改善平衡并减轻肌肉负担。这项工作为理解人类平衡提供了宝贵的肌肉层面视角，并有望推动平衡障碍干预和类人机器人技术的发展。", "keywords": "平衡控制, 肌肉骨骼系统, 模拟, 跌倒, 外骨骼", "comments": "该论文的创新之处在于其利用全身肌肉骨骼模拟系统，首次提供了难以通过实验直接获取的、独特的肌肉层面的平衡动力学见解。其重要性体现在不仅加深了对人类平衡机制的理解，还为开发针对平衡障碍的个性化干预措施和优化类人机器人系统提供了理论基础和技术支持。模拟外骨骼辅助的有效性也展示了该方法的潜在应用价值。"}}
{"id": "2506.09082", "title": "AVA-Bench: Atomic Visual Ability Benchmark for Vision Foundation Models", "authors": ["Zheda Mai", "Arpita Chowdhury", "Zihe Wang", "Sooyoung Jeon", "Lemeng Wang", "Jiacheng Hou", "Jihyung Kil", "Wei-Lun Chao"], "summary": "The rise of vision foundation models (VFMs) calls for systematic evaluation.\nA common approach pairs VFMs with large language models (LLMs) as\ngeneral-purpose heads, followed by evaluation on broad Visual Question\nAnswering (VQA) benchmarks. However, this protocol has two key blind spots: (i)\nthe instruction tuning data may not align with VQA test distributions, meaning\na wrong prediction can stem from such data mismatch rather than a VFM' visual\nshortcomings; (ii) VQA benchmarks often require multiple visual abilities,\nmaking it hard to tell whether errors stem from lacking all required abilities\nor just a single critical one. To address these gaps, we introduce AVA-Bench,\nthe first benchmark that explicitly disentangles 14 Atomic Visual Abilities\n(AVAs) -- foundational skills like localization, depth estimation, and spatial\nunderstanding that collectively support complex visual reasoning tasks. By\ndecoupling AVAs and matching training and test distributions within each,\nAVA-Bench pinpoints exactly where a VFM excels or falters. Applying AVA-Bench\nto leading VFMs thus reveals distinctive \"ability fingerprints,\" turning VFM\nselection from educated guesswork into principled engineering. Notably, we find\nthat a 0.5B LLM yields similar VFM rankings as a 7B LLM while cutting GPU hours\nby 8x, enabling more efficient evaluation. By offering a comprehensive and\ntransparent benchmark, we hope AVA-Bench lays the foundation for the next\ngeneration of VFMs.", "comment": "First two authors contribute equally", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09082v1", "AI": {"title_translation": "AVA-Bench：视觉基础模型的原子视觉能力基准", "tldr": "提出了AVA-Bench，一个用于系统评估视觉基础模型的基准，它通过解耦原子视觉能力来精确定位模型的优缺点，并发现小型LLM也能高效评估。", "motivation": "视觉基础模型（VFMs）的兴起需要系统性评估。现有VQA基准存在两个盲点：(i) 指令微调数据与VQA测试分布不匹配，可能导致错误归因；(ii) VQA任务常需多种视觉能力，难以判断错误是源于缺乏所有能力还是某个关键能力。", "method": "引入了AVA-Bench，这是第一个明确解耦14种原子视觉能力（AVAs）的基准，这些能力是复杂视觉推理任务的基础技能。通过解耦AVAs并在每个能力内部匹配训练和测试分布，AVA-Bench能精确定位VFM的优缺点。", "result": "AVA-Bench揭示了领先VFM独特的“能力指纹”，将VFM选择从猜测转变为工程原理。值得注意的是，发现0.5B的LLM能产生与7B LLM相似的VFM排名，同时减少8倍GPU小时，实现了更高效的评估。", "conclusion": "AVA-Bench通过提供一个全面透明的基准，有望为下一代VFM奠定基础。", "translation": "视觉基础模型（VFMs）的兴起要求进行系统性评估。一种常见的方法是将VFMs与大型语言模型（LLMs）配对作为通用头部，然后在广泛的视觉问答（VQA）基准上进行评估。然而，这种协议存在两个关键盲点：(i) 指令微调数据可能与VQA测试分布不一致，这意味着错误的预测可能源于此类数据不匹配，而非VFM的视觉缺陷；(ii) VQA基准通常需要多种视觉能力，因此很难判断错误是源于缺乏所有所需能力，还是仅缺乏某个关键能力。为了弥补这些空白，我们引入了AVA-Bench，这是第一个明确解耦14种原子视觉能力（AVAs）的基准——这些基础技能如定位、深度估计和空间理解共同支持复杂的视觉推理任务。通过解耦AVAs并在每个能力内部匹配训练和测试分布，AVA-Bench能精确定位VFM的优缺点。将AVA-Bench应用于领先的VFMs，揭示了独特的“能力指纹”，将VFM的选择从凭经验猜测转变为有原则的工程。值得注意的是，我们发现一个0.5B的LLM能产生与7B LLM相似的VFM排名，同时将GPU小时减少8倍，从而实现更高效的评估。通过提供一个全面透明的基准，我们希望AVA-Bench能为下一代VFMs奠定基础。", "summary": "本文提出了AVA-Bench，一个针对视觉基础模型（VFMs）的系统评估基准。它解决了现有VQA基准中数据分布不匹配和多能力混淆的问题，通过解耦14种原子视觉能力（AVAs），精确定位VFMs的优缺点。实验表明，AVA-Bench能揭示VFM的“能力指纹”，并能利用小型LLM进行高效评估，为VFM的选型和未来发展提供了透明且高效的工具。", "keywords": "视觉基础模型, 原子视觉能力, 基准测试, VQA, 模型评估", "comments": "AVA-Bench的创新之处在于其首次明确解耦了原子视觉能力（AVAs），这使得对视觉基础模型的评估更加精细和透明，能够准确诊断模型的强项和弱项。此外，发现小型LLM也能有效进行VFM排名，显著降低了评估成本，具有重要的实际应用价值。这将有助于推动VFM的系统性发展和优化。"}}
{"id": "2506.09655", "title": "DipLLM: Fine-Tuning LLM for Strategic Decision-making in Diplomacy", "authors": ["Kaixuan Xu", "Jiajun Chai", "Sicheng Li", "Yuqian Fu", "Yuanheng Zhu", "Dongbin Zhao"], "summary": "Diplomacy is a complex multiplayer game that requires both cooperation and\ncompetition, posing significant challenges for AI systems. Traditional methods\nrely on equilibrium search to generate extensive game data for training, which\ndemands substantial computational resources. Large Language Models (LLMs) offer\na promising alternative, leveraging pre-trained knowledge to achieve strong\nperformance with relatively small-scale fine-tuning. However, applying LLMs to\nDiplomacy remains challenging due to the exponential growth of possible action\ncombinations and the intricate strategic interactions among players. To address\nthis challenge, we propose DipLLM, a fine-tuned LLM-based agent that learns\nequilibrium policies for Diplomacy. DipLLM employs an autoregressive\nfactorization framework to simplify the complex task of multi-unit action\nassignment into a sequence of unit-level decisions. By defining an equilibrium\npolicy within this framework as the learning objective, we fine-tune the model\nusing only 1.5% of the data required by the state-of-the-art Cicero model,\nsurpassing its performance. Our results demonstrate the potential of fine-tuned\nLLMs for tackling complex strategic decision-making in multiplayer games.", "comment": "Accepted to the 42nd International Conference on Machine Learning\n  (ICML 2025)", "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.09655v1", "AI": {"title_translation": "DipLLM：在外交游戏中微调大型语言模型以进行战略决策", "tldr": "DipLLM是一个微调的LLM，通过自回归分解框架学习外交游戏的均衡策略，使用少量数据超越了SOTA模型Cicero。", "motivation": "传统方法在复杂的多人游戏（如外交）中训练AI需要大量计算资源。尽管LLM有潜力，但由于行动组合的指数级增长和玩家间复杂的战略互动，将其应用于外交游戏仍具挑战。本研究旨在解决这些挑战，提出一个基于LLM的解决方案。", "method": "本文提出了DipLLM，一个微调的LLM代理，用于学习外交游戏的均衡策略。DipLLM采用自回归分解框架，将多单位行动分配的复杂任务简化为一系列单位级别的决策。通过将均衡策略定义为学习目标，模型仅使用Cicero模型所需数据的1.5%进行微调。", "result": "DipLLM使用仅为最先进模型Cicero所需数据量1.5%的数据，超越了Cicero的性能。", "conclusion": "微调的大型语言模型在解决多人游戏中的复杂战略决策方面具有巨大潜力。", "translation": "外交是一个复杂的多人游戏，需要合作与竞争，这对AI系统构成了重大挑战。传统方法依赖于均衡搜索来生成大量的游戏数据进行训练，这需要大量的计算资源。大型语言模型（LLM）提供了一种有前景的替代方案，利用预训练知识通过相对小规模的微调即可实现强大性能。然而，由于可能行动组合的指数级增长以及玩家之间复杂的战略互动，将LLM应用于外交游戏仍然具有挑战性。为了解决这一挑战，我们提出了DipLLM，一个经过微调的基于LLM的智能体，用于学习外交游戏的均衡策略。DipLLM采用自回归分解框架，将多单位行动分配的复杂任务简化为一系列单位级别的决策。通过将该框架内的均衡策略定义为学习目标，我们仅使用最先进的Cicero模型所需数据量的1.5%来微调模型，并超越了其性能。我们的结果证明了微调LLM在解决多人游戏中复杂战略决策方面的潜力。", "summary": "本文提出了DipLLM，一个为复杂多人游戏《外交》设计的微调LLM代理。针对传统方法计算成本高昂以及LLM直接应用于该游戏所面临的挑战，DipLLM通过自回归分解框架将复杂的多单位行动决策简化为单位级序列决策，并以均衡策略作为学习目标进行微调。实验结果表明，DipLLM仅用极少量数据（Cicero模型所需数据的1.5%）就超越了现有最先进模型Cicero的性能，展示了微调LLM在处理复杂战略决策任务上的强大潜力。", "keywords": "大型语言模型, 外交游戏, 战略决策, 微调, 自回归分解", "comments": "DipLLM的创新之处在于其将LLM应用于复杂多人博弈《外交》中的战略决策，并通过自回归分解框架有效处理了行动空间巨大的问题。其最显著的优势是数据效率极高，仅用SOTA模型1.5%的数据量就实现了性能超越，这对于资源受限的AI训练具有重要意义。该研究为LLM在复杂战略决策领域的应用开辟了新的路径。"}}
{"id": "2506.09452", "title": "Learning Obfuscations Of LLM Embedding Sequences: Stained Glass Transform", "authors": ["Jay Roberts", "Kyle Mylonakis", "Sidhartha Roy", "Kaan Kale"], "summary": "The high cost of ownership of AI compute infrastructure and challenges of\nrobust serving of large language models (LLMs) has led to a surge in managed\nModel-as-a-service deployments. Even when enterprises choose on-premises\ndeployments, the compute infrastructure is typically shared across many teams\nin order to maximize the return on investment. In both scenarios the deployed\nmodels operate only on plaintext data, and so enterprise data owners must allow\ntheir data to appear in plaintext on a shared or multi-tenant compute\ninfrastructure. This results in data owners with private or sensitive data\nbeing hesitant or restricted in what data they use with these types of\ndeployments. In this work we introduce the Stained Glass Transform, a learned,\nstochastic, and sequence dependent transformation of the word embeddings of an\nLLM which information theoretically provides privacy to the input of the LLM\nwhile preserving the utility of model. We theoretically connect a particular\nclass of Stained Glass Transforms to the theory of mutual information of\nGaussian Mixture Models. We then calculate a-postiori privacy estimates, based\non mutual information, and verify the privacy and utility of instances of\ntransformed embeddings through token level metrics of privacy and standard LLM\nperformance benchmarks.", "comment": "Submitted to IEEE S&P 2026", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09452v1", "AI": {"title_translation": "学习LLM嵌入序列的混淆：花窗玻璃变换", "tldr": "提出“花窗玻璃变换” (Stained Glass Transform)，一种学习型、随机变换，用于混淆LLM输入嵌入序列，在共享计算基础设施上保护数据隐私的同时保持模型效用。", "motivation": "大型语言模型（LLM）在共享或多租户计算基础设施上处理明文数据时，导致拥有敏感或私人数据的企业因隐私担忧而限制其数据使用。", "method": "引入“花窗玻璃变换”(Stained Glass Transform)，这是一种学习型、随机且依赖序列的LLM词嵌入变换，旨在信息理论上为LLM输入提供隐私，同时保持模型效用。该方法理论上与高斯混合模型的互信息理论相关联。", "result": "基于互信息计算了后验隐私估计，并通过token级别的隐私指标和标准LLM性能基准验证了变换后嵌入的隐私性和效用。", "conclusion": "花窗玻璃变换能够为LLM输入提供隐私保护，同时有效保持模型的效用。", "translation": "AI计算基础设施的高昂拥有成本以及大型语言模型（LLM）健壮服务的挑战，导致托管模型即服务部署激增。即使企业选择本地部署，计算基础设施通常也会在多个团队之间共享，以最大限度地提高投资回报。在这两种情况下，部署的模型都只在明文数据上运行，因此企业数据所有者必须允许他们的数据以明文形式出现在共享或多租户计算基础设施上。这导致拥有私人或敏感数据的企业数据所有者在使用这些类型的部署时犹豫不决或受到限制。在这项工作中，我们引入了花窗玻璃变换（Stained Glass Transform），这是一种学习型、随机且依赖序列的LLM词嵌入变换，它在信息理论上为LLM的输入提供了隐私，同时保持了模型的效用。我们理论上将特定类别的花窗玻璃变换与高斯混合模型的互信息理论联系起来。然后，我们基于互信息计算了后验隐私估计，并通过token级别的隐私指标和标准LLM性能基准验证了变换后嵌入实例的隐私性和效用。", "summary": "本文针对大型语言模型（LLM）在共享或多租户计算环境中处理明文数据时存在的隐私问题，提出了一种名为“花窗玻璃变换”(Stained Glass Transform)的新方法。该变换是一种学习型、随机且依赖序列的LLM词嵌入转换，旨在从信息理论上为LLM的输入提供隐私保护，同时确保模型效用不受影响。研究将该变换与高斯混合模型的互信息理论联系起来，并通过计算后验隐私估计以及利用token级别隐私指标和标准LLM性能基准，验证了其在保护隐私和保持效用方面的有效性。", "keywords": "LLM隐私, 嵌入混淆, 花窗玻璃变换, 数据隐私, 互信息", "comments": "这项工作通过引入“花窗玻璃变换”为LLM数据隐私提供了一个新颖的解决方案，特别是在共享或多租户AI基础设施中。其创新之处在于提出了一种学习型、随机且依赖序列的嵌入变换，能在信息理论层面实现隐私保护而不牺牲模型效用。理论上与互信息和高斯混合模型的结合也增加了其严谨性。这对于推动LLM在处理敏感数据场景下的应用具有重要意义。"}}
{"id": "2506.09636", "title": "Translating a VDM Model of a Medical Device into Kapture", "authors": ["Joe Hare", "Leo Freitas", "Ken Pierce"], "summary": "As the complexity of safety-critical medical devices increases, so does the\nneed for clear, verifiable, software requirements. This paper explores the use\nof Kapture, a formal modelling tool developed by D-RisQ, to translate an\nexisting formal VDM model of a medical implant for treating focal epilepsy\ncalled CANDO. The work was undertaken without prior experience in formal\nmethods. The paper assess Kapture's usability, the challenges of formal\nmodelling, and the effectiveness of the translated model. The result is a model\nin Kapture which covers over 90% of the original VDM model, and produces\nmatching traces of results. While several issues were encountered during design\nand implementation, mainly due to the initial learning curve, this paper\ndemonstrates that complex systems can be effectively modelled in Kapture by\ninexperienced users and highlights some difficulties in translating VDM\nspecifications to Kapture.", "comment": "Presented at the 23rd Overture workshop, June 2025\n  (arXiv:cs/2506.08680)", "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.09636v1", "AI": {"title_translation": "将医疗器械的VDM模型转换为Kapture", "tldr": "本文探讨了如何将一个现有的医疗设备VDM形式化模型（CANDO）转换为Kapture工具中的模型，评估了Kapture的可用性及形式化建模的挑战，结果表明该方法对非专业用户有效，模型覆盖率高且结果一致。", "motivation": "随着安全关键型医疗设备的复杂性增加，对清晰、可验证的软件需求的需求也随之增加。", "method": "本文探索使用D-RisQ开发的Kapture形式化建模工具，将一个现有治疗局灶性癫痫的医疗植入物CANDO的VDM形式化模型进行转换。该工作是在没有形式化方法经验的情况下进行的，并评估了Kapture的可用性、形式化建模的挑战以及转换模型的有效性。", "result": "结果是在Kapture中生成了一个模型，该模型覆盖了原始VDM模型的90%以上，并产生了匹配的结果轨迹。", "conclusion": "尽管在设计和实现过程中遇到了一些问题（主要是由于初始学习曲线），但本文证明了即使是没有经验的用户也能在Kapture中有效地对复杂系统进行建模，并指出了一些将VDM规范转换为Kapture的困难。", "translation": "随着安全关键型医疗设备的复杂性增加，对清晰、可验证的软件需求的需求也随之增加。本文探讨了使用D-RisQ开发的形式化建模工具Kapture，来转换一个现有的用于治疗局灶性癫痫的医疗植入物CANDO的VDM形式化模型。这项工作是在没有形式化方法经验的情况下进行的。本文评估了Kapture的可用性、形式化建模的挑战以及转换模型的有效性。结果是在Kapture中生成了一个模型，该模型覆盖了原始VDM模型的90%以上，并产生了匹配的结果轨迹。尽管在设计和实现过程中遇到了一些问题，主要是由于初始学习曲线，但本文证明了没有经验的用户也能在Kapture中有效地对复杂系统进行建模，并强调了将VDM规范转换为Kapture的一些困难。", "summary": "本文研究了如何将现有医疗设备CANDO的VDM形式化模型转换为Kapture工具中的模型，以应对复杂医疗设备日益增长的软件需求验证挑战。研究评估了Kapture的可用性，并指出即使是无经验用户也能在该工具中有效建模复杂系统，尽管在转换过程中遇到了一些学习曲线和特定困难。最终，转换后的Kapture模型成功覆盖了原VDM模型的90%以上，并产生了匹配的结果轨迹。", "keywords": "VDM, Kapture, 医疗设备, 形式化建模, 模型转换", "comments": "本文的创新点在于证明了Kapture这一形式化建模工具对于没有形式化方法经验的用户也具有较高的可用性，能够有效处理复杂医疗设备的建模任务。其重要性在于为医疗设备等安全关键领域的软件需求验证提供了一种可行的、易于上手的方法。局限性在于，转换过程中仍存在学习曲线和VDM到Kapture转换的特定困难，这可能需要进一步的工具改进或更详细的指导。"}}
{"id": "2506.09292", "title": "AI Tutors vs. Tenacious Myths: Evidence from Personalised Dialogue Interventions in Education", "authors": ["Brooklyn J. Corbett", "Jason M. Tangen"], "summary": "Misconceptions in psychology and education persist despite clear\ncontradictory evidence, resisting traditional correction methods. This study\ninvestigated whether personalised AI dialogue could effectively correct these\nstubborn beliefs. In a preregistered experiment (N = 375), participants holding\nstrong psychology misconceptions engaged in one of three interventions: (1)\npersonalised AI dialogue targeting their specific misconception, (2) generic\ntextbook-style refutation, or (3) neutral AI dialogue (control). Results showed\nthat personalised AI dialogue produced significantly larger immediate belief\nreductions compared to both textbook reading and neutral dialogue. This\nadvantage persisted at 10-day follow-up but diminished by 2 months, where AI\ndialogue and textbook conditions converged while both remained superior to\ncontrol. Both AI conditions generated significantly higher engagement and\nconfidence than textbook reading, demonstrating the motivational benefits of\nconversational interaction. These findings demonstrate that AI dialogue can\naccelerate initial belief correction through personalised, interactive\nengagement that disrupts the cognitive processes maintaining misconceptions.\nHowever, the convergence of effects over time suggests brief interventions\nrequire reinforcement for lasting change. Future applications should integrate\nAI tutoring into structured educational programs with spaced reinforcement to\nsustain the initial advantages of personalised dialogue.", "comment": "Originally posted as https://doi.org/10.31234/osf.io/x4wqh_v1", "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.09292v1", "AI": {"title_translation": "AI导师 vs. 顽固迷思：个性化对话干预在教育中的证据", "tldr": "个性化AI对话比传统方法更能有效纠正心理学和教育中的顽固错误观念，但长期效果需要强化。", "motivation": "心理学和教育中的错误观念难以通过传统方法纠正，本研究旨在探讨个性化AI对话是否能有效纠正这些顽固信念。", "method": "一项预注册实验(N=375)，参与者持有强烈的心理学错误观念，被分为三组：个性化AI对话、通用教科书式反驳、中性AI对话（对照组）。", "result": "个性化AI对话在即时信念纠正方面显著优于教科书阅读和中性对话。这种优势在10天后仍存在，但在2个月后AI对话和教科书条件的效果趋于一致，且两者均优于对照组。两种AI条件都比教科书阅读产生了更高的参与度和信心。", "conclusion": "AI对话通过个性化、互动式参与可以加速初始信念纠正，但短期干预需要强化才能实现持久改变。未来应用应将AI辅导整合到结构化教育项目中，并进行间隔强化。", "translation": "心理学和教育领域的错误观念尽管有明确的相反证据，但仍持续存在，并抵制传统的纠正方法。本研究调查了个性化AI对话是否能有效纠正这些顽固的信念。在一项预注册实验中（N = 375），持有强烈心理学错误观念的参与者参与了三种干预措施中的一种：(1) 针对其特定错误观念的个性化AI对话，(2) 通用教科书式反驳，或 (3) 中性AI对话（对照组）。结果显示，与教科书阅读和中性对话相比，个性化AI对话产生了显著更大的即时信念减少。这种优势在10天的后续跟踪中仍然存在，但在2个月后有所减弱，此时AI对话和教科书条件的效果趋于一致，而两者都优于对照组。两种AI条件都比教科书阅读产生了显著更高的参与度和信心，这表明对话式互动的激励益处。这些发现表明，AI对话可以通过个性化、互动式参与来加速初始信念纠正，从而干扰维持错误观念的认知过程。然而，随着时间推移效果的趋同表明，简短的干预需要强化才能实现持久改变。未来的应用应将AI辅导整合到结构化的教育项目中，并进行间隔强化，以维持个性化对话的初始优势。", "summary": "本研究通过一项包含375名参与者的实验，探讨了个性化AI对话在纠正心理学和教育中顽固错误观念方面的有效性。结果表明，个性化AI对话在即时和短期内（10天）比传统教科书和中性对话更能显著减少错误信念，并提高了参与度和信心。尽管长期效果（2个月）趋于与教科书效果一致，但AI对话通过个性化互动加速了初始信念纠正。研究建议未来应将AI辅导与间隔强化相结合，以维持其长期优势。", "keywords": "AI对话, 错误观念纠正, 个性化学习, 教育技术, 认知干预", "comments": "该研究创新性地探讨了AI对话在纠正顽固错误观念方面的潜力，强调了个性化互动的重要性。其发现揭示了AI在教育领域作为补充工具的价值，尤其是在初始信念纠正和提高学习者参与度方面。然而，研究也指出短期干预的局限性，即需要持续强化以实现持久改变，这为未来AI教育系统的设计提供了重要启示。"}}
{"id": "2506.09913", "title": "A Note on the Reliability of Goal-Oriented Error Estimates for Galerkin Finite Element Methods with Nonlinear Functionals", "authors": ["Brian N. Granzow", "Stephen D. Bond", "D. Thomas Seidl", "Bernhard Endtmayer"], "summary": "We consider estimating the discretization error in a nonlinear functional\n$J(u)$ in the setting of an abstract variational problem: find $u \\in\n\\mathcal{V}$ such that $B(u,\\varphi) = L(\\varphi) \\; \\forall \\varphi \\in\n\\mathcal{V}$, as approximated by a Galerkin finite element method. Here,\n$\\mathcal{V}$ is a Hilbert space, $B(\\cdot,\\cdot)$ is a bilinear form, and\n$L(\\cdot)$ is a linear functional. We consider well-known error estimates\n$\\eta$ of the form $J(u) - J(u_h) \\approx \\eta = L(z) - B(u_h, z)$, where $u_h$\ndenotes a finite element approximation to $u$, and $z$ denotes the solution to\nan auxiliary adjoint variational problem. We show that there exist nonlinear\nfunctionals for which error estimates of this form are not reliable, even in\nthe presence of an exact adjoint solution solution $z$. An estimate $\\eta$ is\nsaid to be reliable if there exists a constant $C \\in \\mathbb{R}_{>0}$\nindependent of $u_h$ such that $|J(u) - J(u_h)| \\leq C|\\eta|$. We present\nseveral example pairs of bilinear forms and nonlinear functionals where\nreliability of $\\eta$ is not achieved.", "comment": "6 pages", "cate": "math.NA", "url": "http://arxiv.org/abs/2506.09913v1", "AI": {"title_translation": "关于Galerkin有限元方法中非线性泛函目标导向误差估计可靠性的一点说明", "tldr": "对于Galerkin有限元方法中非线性泛函的目标导向误差估计，即使存在精确的伴随解，也可能不可靠。", "motivation": "本研究旨在探讨Galerkin有限元方法中非线性泛函目标导向误差估计的可靠性问题，指出现有估计方法可能存在的局限性。", "method": "论文考虑了一个抽象变分问题，并分析了众所周知的目标导向误差估计形式。通过理论分析和具体示例，证明了在特定非线性泛函情况下，这些估计是不可靠的。", "result": "研究表明，对于某些非线性泛函，即使存在精确的伴随解，形式为$J(u) - J(u_h) \\approx \\eta = L(z) - B(u_h, z)$的误差估计也是不可靠的。论文提供了几个双线性形式和非线性泛函的示例对，证明了估计的不可靠性。", "conclusion": "论文得出结论，对于Galerkin有限元方法中的非线性泛函，常见的目标导向误差估计并不总是可靠的，这揭示了这些估计的局限性。", "translation": "我们考虑在抽象变分问题背景下，估算非线性泛函$J(u)$中的离散误差：找到$u \\in \\mathcal{V}$使得$B(u,\\varphi) = L(\\varphi) \\; \\forall \\varphi \\in \\mathcal{V}$，并通过Galerkin有限元方法近似。这里，$\\mathcal{V}$是一个Hilbert空间，$B(\\cdot,\\cdot)$是一个双线性形式，$L(\\cdot)$是一个线性泛函。我们考虑了众所周知的误差估计$\\eta$，其形式为$J(u) - J(u_h) \\approx \\eta = L(z) - B(u_h, z)$，其中$u_h$表示$u$的有限元近似，$z$表示辅助伴随变分问题的解。我们表明，存在一些非线性泛函，对于这些泛函，即使存在精确的伴随解$z$，这种形式的误差估计也是不可靠的。如果存在一个独立于$u_h$的常数$C \\in \\mathbb{R}_{>0}$使得$|J(u) - J(u_h)| \\leq C|\\eta|$，则称估计$\\eta$是可靠的。我们给出了几个双线性形式和非线性泛函的示例对，其中$\\eta$的可靠性未能实现。", "summary": "本文探讨了Galerkin有限元方法中非线性泛函目标导向误差估计的可靠性。研究发现，即使有精确的伴随解，常用的误差估计形式（$J(u) - J(u_h) \\approx \\eta = L(z) - B(u_h, z)$）对于某些非线性泛函而言仍可能不可靠。论文通过具体示例验证了这一不可靠性，挑战了这些估计的普遍适用性。", "keywords": "Galerkin有限元方法, 目标导向误差估计, 非线性泛函, 可靠性, 伴随问题", "comments": "这篇论文揭示了Galerkin有限元方法中非线性泛函目标导向误差估计的一个重要局限性。它通过理论分析和具体示例，严谨地证明了现有方法的潜在不可靠性，为依赖这些估计的数值分析和工程实践提供了重要的警示。其创新点在于对传统认知进行了批判性审视。"}}
{"id": "2506.09513", "title": "ReasonMed: A 370K Multi-Agent Generated Dataset for Advancing Medical Reasoning", "authors": ["Yu Sun", "Xingyu Qian", "Weiwen Xu", "Hao Zhang", "Chenghao Xiao", "Long Li", "Yu Rong", "Wenbing Huang", "Qifeng Bai", "Tingyang Xu"], "summary": "Though reasoning-based large language models (LLMs) have excelled in\nmathematics and programming, their capabilities in knowledge-intensive medical\nquestion answering remain underexplored. To address this, we introduce\nReasonMed, the largest medical reasoning dataset, comprising 370k high-quality\nexamples distilled from 1.7 million initial reasoning paths generated by\nvarious LLMs. ReasonMed is constructed through a \\textit{multi-agent\nverification and refinement process}, where we design an \\textit{Error Refiner}\nto enhance the reasoning paths by identifying and correcting error-prone steps\nflagged by a verifier. Leveraging ReasonMed, we systematically investigate best\npractices for training medical reasoning models and find that combining\ndetailed Chain-of-Thought (CoT) reasoning with concise answer summaries yields\nthe most effective fine-tuning strategy. Based on this strategy, we train\nReasonMed-7B, which sets a new benchmark for sub-10B models, outperforming the\nprior best by 4.17\\% and even exceeding LLaMA3.1-70B on PubMedQA by 4.60\\%.", "comment": "24 pages, 6 figures, 7 tables", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.09513v1", "AI": {"title_translation": "ReasonMed：一个用于推进医学推理的37万多智能体生成数据集", "tldr": "引入ReasonMed，一个37万条高质量的医学推理数据集，通过多智能体验证和细化过程构建，显著提升了LLMs在医学问答中的推理能力，ReasonMed-7B超越了现有最佳模型。", "motivation": "尽管基于推理的大语言模型（LLM）在数学和编程方面表现出色，但它们在知识密集型医学问答中的能力尚未得到充分探索。", "method": "1. 构建了ReasonMed数据集，包含37万个高质量示例，这些示例是从170万个由不同LLM生成的初始推理路径中提取的。2. 数据集通过“多智能体验证和细化过程”构建，其中设计了一个“错误修正器”来识别并纠正验证器标记的易错步骤，从而增强推理路径。3. 系统地研究了训练医学推理模型的最佳实践，发现将详细的思维链（CoT）推理与简洁的答案摘要相结合，能产生最有效的微调策略。", "result": "1. ReasonMed-7B（基于上述策略训练）为小于10B的模型设立了新基准。2. 在PubMedQA上，ReasonMed-7B超越了之前的最佳模型4.17%。3. 在PubMedQA上，ReasonMed-7B甚至超越了LLaMA3.1-70B 4.60%。", "conclusion": "ReasonMed数据集和结合CoT推理与简洁答案摘要的微调策略，显著提升了LLMs在医学推理任务上的表现，为医学AI领域树立了新标杆。", "translation": "尽管基于推理的大语言模型（LLM）在数学和编程方面表现出色，但它们在知识密集型医学问答中的能力仍未得到充分探索。为了解决这个问题，我们引入了ReasonMed，这是最大的医学推理数据集，包含37万个高质量示例，这些示例是从各种LLM生成的170万个初始推理路径中提炼出来的。ReasonMed通过一个“多智能体验证和细化过程”构建，我们设计了一个“错误修正器”来识别和纠正由验证器标记的易错步骤，从而增强推理路径。利用ReasonMed，我们系统地研究了训练医学推理模型的最佳实践，发现将详细的思维链（CoT）推理与简洁的答案摘要相结合，能产生最有效的微调策略。基于这一策略，我们训练了ReasonMed-7B，它为小于10B的模型设定了新基准，超越了之前最佳模型4.17%，甚至在PubMedQA上超越了LLaMA3.1-70B 4.60%。", "summary": "本研究介绍了ReasonMed，一个包含37万高质量示例的医学推理数据集，旨在弥补当前大语言模型在医学问答推理能力上的不足。该数据集通过独特的多智能体验证和错误修正过程构建，确保了推理路径的准确性。研究发现，结合详细的思维链推理和简洁答案摘要的微调策略最为有效。基于此策略训练的ReasonMed-7B模型在小于10B的模型中表现最佳，并在PubMedQA上显著超越了现有顶级模型，包括LLaMA3.1-70B。", "keywords": "医学推理, 大语言模型, 数据集, 多智能体, 思维链", "comments": "该论文的创新之处在于构建了一个大规模、高质量的医学推理数据集ReasonMed，并通过多智能体验证和细化过程确保了数据的准确性。其提出的结合CoT和答案摘要的微调策略，为提升LLMs在专业领域（尤其是医学）的推理能力提供了有效途径。ReasonMed-7B在小参数模型中取得的显著性能提升，证明了高质量数据和优化训练策略的重要性，对推动医学AI的发展具有重要意义。"}}
{"id": "2506.09447", "title": "Optimization and Control Technologies for Renewable-Dominated Hydrogen-Blended Integrated Gas-Electricity System: A Review", "authors": ["Wenxin Liu", "Jiakun Fang", "Shichang Cui", "Iskandar Abdullaev", "Suyang Zhou", "Xiaomeng Ai", "Jinyu Wen"], "summary": "The growing coupling among electricity, gas, and hydrogen systems is driven\nby green hydrogen blending into existing natural gas pipelines, paving the way\ntoward a renewable-dominated energy future. However, the integration poses\nsignificant challenges, particularly ensuring efficient and safe operation\nunder varying hydrogen penetration and infrastructure adaptability. This paper\nreviews progress in optimization and control technologies for hydrogen-blended\nintegrated gas-electricity system. First, key technologies and international\ndemonstration projects are introduced to provide an overview of current\ndevelopments. Besides, advances in gas-electricity system integration,\nincluding modeling, scheduling, planning and market design, are reviewed\nrespectively. Then, the potential for cross-system fault propagation is\nhighlighted, and practical methods for safety analysis and control are\nproposed. Finally, several possible research directions are introduced, aiming\nto ensure efficient renewable integration and reliable operation.", "comment": null, "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.09447v1", "AI": {"title_translation": "可再生能源主导的氢气混合综合燃气-电力系统的优化与控制技术：综述", "tldr": "综述了可再生能源主导的氢气混合燃气-电力系统在优化与控制技术方面的进展，涵盖建模、调度、规划、市场设计、安全分析和控制，并提出未来研究方向。", "motivation": "绿色氢气掺入现有天然气管道推动电力、燃气和氢系统耦合，但这种集成在不同氢渗透率和基础设施适应性下，高效安全运行面临挑战。", "method": "本文通过综述的方式，介绍了氢气混合综合燃气-电力系统的优化与控制技术进展，包括关键技术、国际示范项目、燃气-电力系统集成的建模、调度、规划和市场设计，以及跨系统故障传播的安全分析和控制方法。", "result": "综述了氢气混合燃气-电力系统的优化与控制技术，包括关键技术、国际示范项目、燃气-电力系统集成的建模、调度、规划、市场设计方面的进展，并提出了跨系统故障传播的安全分析和控制方法。", "conclusion": "论文总结了可再生能源主导的氢气混合燃气-电力系统的优化与控制进展，并提出了未来研究方向，旨在确保可再生能源的高效集成和可靠运行。", "translation": "随着绿色氢气掺入现有天然气管道，电力、燃气和氢系统之间的耦合日益增长，为可再生能源主导的能源未来铺平了道路。然而，这种集成带来了重大挑战，特别是在不同的氢渗透率和基础设施适应性下，如何确保高效和安全运行。本文综述了氢气混合综合燃气-电力系统的优化与控制技术进展。首先，介绍了关键技术和国际示范项目，以概述当前的发展情况。此外，还分别综述了燃气-电力系统集成方面的进展，包括建模、调度、规划和市场设计。然后，强调了跨系统故障传播的可能性，并提出了实用的安全分析和控制方法。最后，介绍了几个可能的研究方向，旨在确保可再生能源的高效集成和可靠运行。", "summary": "本文综述了可再生能源主导的氢气混合综合燃气-电力系统的优化与控制技术进展。论文首先概述了关键技术和国际示范项目，随后深入探讨了燃气-电力系统集成在建模、调度、规划和市场设计方面的最新进展。此外，文章还强调了跨系统故障传播的风险，并提出了相应的安全分析和控制方法。最后，展望了未来研究方向，以促进可再生能源的高效整合和系统的可靠运行。", "keywords": "氢气混合系统, 燃气-电力系统集成, 优化与控制, 可再生能源, 能源系统安全", "comments": "这篇综述论文系统地梳理了氢气混合燃气-电力系统在优化与控制方面的技术进展，为领域内的研究人员提供了全面的参考。其创新点在于将氢能与传统燃气-电力系统结合，探讨了未来能源系统的集成挑战与解决方案。论文强调了安全运行的重要性，并提出了具体的安全分析与控制方法，具有较强的实用价值。"}}
{"id": "2506.09878", "title": "Virtualizing RAN: Science, Strategy, and Architecture of Software-Defined Mobile Networks", "authors": ["Ryan Barker"], "summary": "Virtualising the Radio Access Network (RAN) is widely touted as the\ncorner-stone of affordable 5G and a prerequisite for AI-native 6G. Yet current\ndiscourse often isolates spectrum policy, cloud engineering and organisational\nreadiness into silos. This paper delivers an integrated analysis that spans\nscience, technology, business strategy and culture. I first review\nspectrum-auction economics and show-via a comparative study of T-Mobile US and\nVerizon-that mid-band contiguity leveraged through software-defined carrier\naggregation outperforms mmWave-centric deployments in both coverage and churn\nmetrics. I then formalise the technical foundations of virtualised and open\nRAN, deriving capacity limits from contiguous and dis-contiguous spectrum maths\nand quantifying hardware ceilings for 400 MHz mmWave channels. Edge compute\nplatforms (NVIDIA EGX, Samsung vRAN 3.0) and SDN-controlled RAN Intelligent\nControllers are examined alongside AI ML pipelines that enable\ndigital-twin-driven optimisation. A security cost model extends recent O-RAN\nmeasurements to show how 256-bit cipher enforcement adds 35-60 us latency\nunless mitigated by inline crypto off-load. Finally, a national automation case\nstudy of live vRAN sites -- demonstrates an 81 to 13 day cycle-time reduction\nonce cultural change errors are corrected. I conclude with open research\nchallenges for sub-THz 6G, energy-neutral AI accelerators and zero-trust\norchestration, offering actionable recommendations for operators, vendors and\nresearchers.", "comment": "12 pages, 4 figures, 8 tables", "cate": "cs.NI", "url": "http://arxiv.org/abs/2506.09878v1", "AI": {"title_translation": "虚拟化RAN：软件定义移动网络的科学、战略与架构", "tldr": "本文对无线接入网（RAN）虚拟化进行了跨科学、技术、商业战略和文化的综合分析，通过案例研究和技术建模，展示了其在5G和6G中的潜力，并提出了未来研究挑战和建议。", "motivation": "虚拟化无线接入网（RAN）被广泛认为是经济实惠的5G和AI原生6G的关键基石，但当前讨论常将频谱政策、云工程和组织准备孤立看待。本文旨在提供一个涵盖科学、技术、商业战略和文化的综合分析，以解决这一问题。", "method": "回顾频谱拍卖经济学，并对T-Mobile US和Verizon进行比较研究；形式化虚拟化和开放RAN的技术基础，推导容量限制并量化硬件上限；考察边缘计算平台和SDN控制的RAN智能控制器，以及AI/ML管道；扩展O-RAN测量，提出安全成本模型；进行实时vRAN站点的国家自动化案例研究。", "result": "通过软件定义载波聚合利用中频段连续性，在覆盖范围和用户流失指标上优于毫米波中心部署；从连续和非连续频谱数学中推导出容量限制，并量化了400 MHz毫米波信道的硬件上限；256位密码强制执行会增加35-60微秒的延迟，除非通过内联加密卸载来缓解；国家自动化案例研究表明，一旦纠正文化变革错误，周期时间可从81天缩短至13天。", "conclusion": "本文总结了Sub-THz 6G、能源中性AI加速器和零信任编排方面的开放研究挑战，并为运营商、供应商和研究人员提供了可行的建议。", "translation": "虚拟化无线接入网（RAN）被广泛认为是经济实惠的5G的基石，也是AI原生6G的先决条件。然而，当前的讨论常常将频谱政策、云工程和组织准备孤立起来。本文提供了一个跨越科学、技术、商业战略和文化的综合分析。我首先回顾了频谱拍卖经济学，并通过对T-Mobile US和Verizon的比较研究表明，通过软件定义载波聚合利用中频段连续性在覆盖范围和用户流失指标上都优于以毫米波为中心的部署。然后，我形式化了虚拟化和开放RAN的技术基础，从连续和非连续频谱数学中推导出容量限制，并量化了400 MHz毫米波信道的硬件上限。同时考察了边缘计算平台（NVIDIA EGX、Samsung vRAN 3.0）和SDN控制的RAN智能控制器，以及支持数字孪生驱动优化的AI/ML管道。一个安全成本模型扩展了最近的O-RAN测量结果，表明除非通过内联加密卸载来缓解，否则256位密码强制执行会增加35-60微秒的延迟。最后，一项针对实时vRAN站点的国家自动化案例研究——展示了在纠正文化变革错误后，周期时间从81天缩短到13天。我以Sub-THz 6G、能源中性AI加速器和零信任编排方面的开放研究挑战作为结论，并为运营商、供应商和研究人员提供了可行的建议。", "summary": "本文对无线接入网（RAN）虚拟化进行了深入的综合分析，涵盖了科学、技术、商业战略和文化层面。通过对比研究，文章指出中频段连续性结合软件定义载波聚合在5G部署中优于毫米波方案，并从技术上阐述了虚拟化RAN的容量限制和硬件要求。文中还探讨了边缘计算、SDN控制的RAN智能控制器以及AI/ML在优化中的应用，并提出了安全成本模型，指出加密对延迟的影响。此外，一项自动化案例研究表明，纠正文化错误能显著缩短部署周期。文章最后提出了6G、AI加速和零信任方面的研究挑战，并提供了实用建议。", "keywords": "虚拟化RAN, 软件定义移动网络, 5G, 6G, 频谱政策", "comments": "本文的创新之处在于其对RAN虚拟化进行了多维度、跨学科的综合分析，打破了传统讨论中各领域孤立的局面。通过结合经济学、技术建模、案例研究和安全分析，提供了对vRAN潜力及其实现路径的全面洞察。其提出的实践性建议对于运营商和研究人员具有重要参考价值，尤其是在推动5G演进和6G发展方面。"}}
{"id": "2506.09825", "title": "On the Impossibility of a Perfect Hypervisor", "authors": ["Mordechai Guri"], "summary": "We establish a fundamental impossibility result for a `perfect hypervisor',\none that (1) preserves every observable behavior of any program exactly as on\nbare metal and (2) adds zero timing or resource overhead.\n  Within this model we prove two theorems. (1) Indetectability Theorem. If such\na hypervisor existed, no guest-level program, measurement, or timing test could\ndistinguish it from native execution; all traces, outputs, and timings would be\nidentical.\n  (2) Impossibility Theorem. Despite that theoretical indetectability, a\nperfect hypervisor cannot exist on any machine with finite computational\nresources.\n  These results are architecture-agnostic and extend beyond hypervisors to any\nvirtualization layer emulators, sandboxes, containers, or\nruntime-instrumentation frameworks. Together they provide a formal foundation\nfor future work on the principles and limits of virtualization.", "comment": null, "cate": "cs.OS", "url": "http://arxiv.org/abs/2506.09825v1", "AI": {"title_translation": "论完美虚拟机监控程序的不可行性", "tldr": "本文证明了在有限计算资源下，一个既能精确保留程序行为又无开销的“完美虚拟机监控程序”是不可能存在的。", "motivation": "论文旨在探讨并建立关于“完美虚拟机监控程序”的根本性不可能结果。一个完美虚拟机监控程序被定义为能精确保留任何程序的裸机可观察行为，且不增加任何时间或资源开销。", "method": "作者通过建立一个模型，并在该模型内证明了两个定理：不可检测性定理和不可行性定理，从而得出结论。", "result": "1) 不可检测性定理：如果存在这样的虚拟机监控程序，任何客户机程序、测量或时间测试都无法将其与原生执行区分开来；所有跟踪、输出和时间都将相同。2) 不可行性定理：尽管存在理论上的不可检测性，但在任何具有有限计算资源的机器上，完美的虚拟机监控程序不可能存在。这些结果与架构无关，并适用于任何虚拟化层。", "conclusion": "论文证明了完美的虚拟机监控程序在有限计算资源下是不可能存在的，并为未来关于虚拟化原理和限制的研究提供了正式基础。", "translation": "我们为“完美虚拟机监控程序”建立了一个根本性的不可能结果，这种监控程序能够(1)精确地保留任何程序在裸机上的所有可观察行为，并且(2)不增加任何时间或资源开销。在此模型中，我们证明了两个定理。(1)不可检测性定理：如果存在这样的虚拟机监控程序，任何客户机级程序、测量或时间测试都无法将其与原生执行区分开来；所有跟踪、输出和时间都将相同。(2)不可行性定理：尽管存在理论上的不可检测性，但在任何具有有限计算资源的机器上，完美的虚拟机监控程序不可能存在。这些结果与架构无关，并超越了虚拟机监控程序，延伸到任何虚拟化层——模拟器、沙盒、容器或运行时插桩框架。它们共同为未来关于虚拟化原理和限制的研究提供了正式基础。", "summary": "本文探讨了“完美虚拟机监控程序”的存在性，该程序被定义为能精确模拟裸机行为且无任何开销。作者通过理论证明了两个关键定理：首先，如果完美虚拟机监控程序存在，则其与原生执行无法区分；其次，在任何有限计算资源的机器上，完美虚拟机监控程序是不可能存在的。这些结论具有普适性，适用于所有形式的虚拟化技术，为虚拟化领域的未来研究奠定了理论基础。", "keywords": "虚拟机监控程序, 虚拟化, 不可能性定理, 有限资源, 不可检测性", "comments": "这篇论文提出了一个关于虚拟化根本限制的重要理论结果。其创新之处在于通过严格的数学证明，揭示了“完美”虚拟化在资源有限的现实世界中是不可能实现的。这对于理解虚拟化技术的本质、设计更实际的虚拟化解决方案以及设定对其性能和隔离能力的预期具有深远意义。该研究不仅限于虚拟机监控程序，还涵盖了广泛的虚拟化技术，使其影响力更为广泛。"}}
{"id": "2506.09653", "title": "Recognizing Every Voice: Towards Inclusive ASR for Rural Bhojpuri Women", "authors": ["Sakshi Joshi", "Eldho Ittan George", "Tahir Javed", "Kaushal Bhogale", "Nikhil Narasimhan", "Mitesh M. Khapra"], "summary": "Digital inclusion remains a challenge for marginalized communities,\nespecially rural women in low-resource language regions like Bhojpuri.\nVoice-based access to agricultural services, financial transactions, government\nschemes, and healthcare is vital for their empowerment, yet existing ASR\nsystems for this group remain largely untested. To address this gap, we create\nSRUTI ,a benchmark consisting of rural Bhojpuri women speakers. Evaluation of\ncurrent ASR models on SRUTI shows poor performance due to data scarcity, which\nis difficult to overcome due to social and cultural barriers that hinder\nlarge-scale data collection. To overcome this, we propose generating synthetic\nspeech using just 25-30 seconds of audio per speaker from approximately 100\nrural women. Augmenting existing datasets with this synthetic data achieves an\nimprovement of 4.7 WER, providing a scalable, minimally intrusive solution to\nenhance ASR and promote digital inclusion in low-resource language.", "comment": "Accepted at Interspeech 2025", "cate": "eess.AS", "url": "http://arxiv.org/abs/2506.09653v1", "AI": {"title_translation": "识别每一个声音：迈向农村博杰普尔语女性的包容性ASR", "tldr": "现有ASR系统在低资源博杰普尔语农村女性群体中表现不佳，本文通过合成语音数据增强现有数据集，显著提升了ASR性能，提供了一种可扩展的数字包容解决方案。", "motivation": "边缘化社区，特别是低资源语言地区（如博杰普尔语）的农村女性，面临数字鸿沟。语音访问服务对她们的赋权至关重要，但现有ASR系统对该群体未充分测试且表现不佳，主要原因是数据稀缺。", "method": "创建了SRUTI基准数据集，包含农村博杰普尔语女性说话者。评估了现有ASR模型在SRUTI上的性能。提出并实施了使用每位说话者25-30秒音频生成合成语音的方法，并用这些合成数据增强现有数据集。", "result": "现有ASR模型在SRUTI上的性能很差，原因在于数据稀缺。使用合成数据增强现有数据集，使词错率（WER）提高了4.7。", "conclusion": "生成合成语音提供了一种可扩展、侵入性最小的解决方案，可以增强低资源语言中的ASR系统，并促进数字包容。", "translation": "数字包容对于边缘化社区，特别是博杰普尔语等低资源语言地区的农村女性来说，仍然是一个挑战。语音访问农业服务、金融交易、政府计划和医疗保健对于她们的赋权至关重要，然而针对这一群体的现有ASR系统在很大程度上未经测试。为了弥补这一差距，我们创建了SRUTI，一个由农村博杰普尔语女性说话者组成的基准测试。现有ASR模型在SRUTI上的评估显示出较差的性能，原因是数据稀缺，而由于阻碍大规模数据收集的社会和文化障碍，数据稀缺难以克服。为了克服这一点，我们建议使用每位说话者仅25-30秒的音频（来自大约100名农村女性）生成合成语音。用这种合成数据增强现有数据集，实现了4.7的词错率（WER）改进，为增强ASR和促进低资源语言中的数字包容提供了一种可扩展、侵入性最小的解决方案。", "summary": "本文关注低资源博杰普尔语农村女性的数字包容问题，指出现有ASR系统在该群体中表现不佳，主要源于数据稀缺。为解决此问题，研究团队构建了SRUTI基准，并提出了一种创新方法：利用少量（每人25-30秒）真实音频生成合成语音，以此扩充数据集。实验结果显示，通过合成数据增强，ASR模型的词错率显著降低了4.7，证明了该方法在提升低资源语言ASR性能和促进数字包容方面的有效性和可扩展性。", "keywords": "农村博杰普尔语女性, ASR, 数字包容, 合成语音, 低资源语言", "comments": "这篇论文的创新点在于提出了一种解决低资源语言ASR数据稀缺问题的有效且侵入性极小的方法——通过少量真实语音生成合成数据。这对于促进边缘化群体的数字包容具有重要意义，尤其是在面临社会文化障碍导致数据收集困难的地区。其方法的可扩展性也使其在其他低资源语言场景中具有应用潜力。"}}
{"id": "2506.09282", "title": "ScalableHD: Scalable and High-Throughput Hyperdimensional Computing Inference on Multi-Core CPUs", "authors": ["Dhruv Parikh", "Viktor Prasanna"], "summary": "Hyperdimensional Computing (HDC) is a brain-inspired computing paradigm that\nrepresents and manipulates information using high-dimensional vectors, called\nhypervectors (HV). Traditional HDC methods, while robust to noise and\ninherently parallel, rely on single-pass, non-parametric training and often\nsuffer from low accuracy. To address this, recent approaches adopt iterative\ntraining of base and class HVs, typically accelerated on GPUs. Inference,\nhowever, remains lightweight and well-suited for real-time execution. Yet,\nefficient HDC inference has been studied almost exclusively on specialized\nhardware such as FPGAs and GPUs, with limited attention to general-purpose\nmulti-core CPUs. To address this gap, we propose ScalableHD for scalable and\nhigh-throughput HDC inference on multi-core CPUs. ScalableHD employs a\ntwo-stage pipelined execution model, where each stage is parallelized across\ncores and processes chunks of base and class HVs. Intermediate results are\nstreamed between stages using a producer-consumer mechanism, enabling\non-the-fly consumption and improving cache locality. To maximize performance,\nScalableHD integrates memory tiling and NUMA-aware worker-to-core binding.\nFurther, it features two execution variants tailored for small and large batch\nsizes, each designed to exploit compute parallelism based on workload\ncharacteristics while mitigating the memory-bound compute pattern that limits\nHDC inference performance on modern multi-core CPUs. ScalableHD achieves up to\n10x speedup in throughput (samples per second) over state-of-the-art baselines\nsuch as TorchHD, across a diverse set of tasks ranging from human activity\nrecognition to image classification, while preserving task accuracy.\nFurthermore, ScalableHD exhibits robust scalability: increasing the number of\ncores yields near-proportional throughput improvements.", "comment": "IC3", "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.09282v1", "AI": {"title_translation": "ScalableHD: 可扩展和高吞吐量的多核CPU超维度计算推理", "tldr": "ScalableHD 是一种在多核CPU上实现可扩展、高吞吐量超维度计算推理的新方法，通过流水线执行、内存分块和NUMA感知绑定，相较于现有技术实现了显著的性能提升。", "motivation": "现有高效的超维度计算（HDC）推理研究主要集中在FPGA和GPU等专用硬件上，而对通用多核CPU的关注有限。本文旨在填补这一空白，解决在多核CPU上进行HDC推理的效率问题。", "method": "提出ScalableHD，采用两阶段流水线执行模型，每个阶段在多核上并行处理，并使用生产者-消费者机制流式传输中间结果。为最大化性能，ScalableHD集成了内存分块和NUMA感知的核-工作绑定。此外，它还提供了针对小批量和大批量的两种执行变体，以根据工作负载特性利用计算并行性并缓解内存限制。", "result": "ScalableHD在吞吐量（每秒样本数）方面比TorchHD等现有基线提高了10倍，同时保持了任务准确性。它还表现出强大的可扩展性，增加核心数量能带来接近线性的吞吐量提升。", "conclusion": "ScalableHD成功地在多核CPU上实现了可扩展和高吞吐量的超维度计算推理，显著提升了性能并展现出良好的可扩展性，填补了通用CPU上HDC推理效率研究的空白。", "translation": "超维度计算（HDC）是一种受大脑启发的计算范式，它使用高维向量（称为超向量，HV）来表示和操作信息。传统的HDC方法虽然对噪声具有鲁棒性且本质上是并行的，但依赖于单次、非参数训练，并且通常精度较低。为了解决这个问题，最近的方法采用基向量和类别超向量的迭代训练，通常在GPU上加速。然而，推理仍然是轻量级的，非常适合实时执行。然而，高效的HDC推理几乎只在FPGA和GPU等专用硬件上进行过研究，对通用多核CPU的关注有限。为了弥补这一空白，我们提出了ScalableHD，用于在多核CPU上实现可扩展和高吞吐量的HDC推理。ScalableHD采用两阶段流水线执行模型，每个阶段在多个核心上并行处理，并分块处理基向量和类别超向量。中间结果通过生产者-消费者机制在阶段之间流式传输，从而实现即时消耗并改善缓存局部性。为了最大限度地提高性能，ScalableHD集成了内存分块和NUMA感知的核-工作绑定。此外，它还具有两种执行变体，分别针对小批量和大批量进行了优化，每种变体都旨在根据工作负载特性利用计算并行性，同时减轻限制现代多核CPU上HDC推理性能的内存密集型计算模式。ScalableHD在从人类活动识别到图像分类等各种任务中，实现了吞吐量（每秒样本数）比TorchHD等最先进的基线提高了10倍，同时保持了任务准确性。此外，ScalableHD表现出强大的可扩展性：增加核心数量可以带来接近线性的吞吐量改进。", "summary": "本论文提出了ScalableHD，一种专为多核CPU设计的可扩展且高吞吐量的超维度计算（HDC）推理框架。针对传统HDC推理在通用CPU上效率低下的问题，ScalableHD采用两阶段流水线执行模型，结合内存分块、NUMA感知绑定以及针对不同批量大小的优化策略，有效利用多核并行性并缓解内存瓶颈。实验结果表明，ScalableHD在保持准确性的前提下，吞吐量比现有技术提升高达10倍，并展现出优异的线性可扩展性。", "keywords": "Hyperdimensional Computing, Multi-Core CPUs, Scalability, High-Throughput, Inference", "comments": "这项工作通过将超维度计算推理有效部署到通用多核CPU上，填补了HDC领域的一个重要空白，这对于HDC的广泛应用至关重要，因为它降低了对专用硬件的依赖。其创新之处在于结合了流水线执行、内存优化（分块、NUMA感知）和工作负载适应性（大小批量变体），系统性地解决了CPU上的性能瓶颈。所实现的10倍吞吐量提升和接近线性的可扩展性证明了其设计的有效性，这对于实时HDC应用具有重要意义。"}}
{"id": "2506.09162", "title": "The RSNA Lumbar Degenerative Imaging Spine Classification (LumbarDISC) Dataset", "authors": ["Tyler J. Richards", "Adam E. Flanders", "Errol Colak", "Luciano M. Prevedello", "Robyn L. Ball", "Felipe Kitamura", "John Mongan", "Maryam Vazirabad", "Hui-Ming Lin", "Anne Kendell", "Thanat Kanthawang", "Salita Angkurawaranon", "Emre Altinmakas", "Hakan Dogan", "Paulo Eduardo de Aguiar Kuriki", "Arjuna Somasundaram", "Christopher Ruston", "Deniz Bulja", "Naida Spahovic", "Jennifer Sommer", "Sirui Jiang", "Eduardo Moreno Judice de Mattos Farina", "Eduardo Caminha Nunes", "Michael Brassil", "Megan McNamara", "Johanna Ortiz", "Jacob Peoples", "Vinson L. Uytana", "Anthony Kam", "Venkata N. S. Dola", "Daniel Murphy", "David Vu", "Dataset Contributor Group", "Dataset Annotator Group", "Competition Data Notebook Group", "Jason F. Talbott"], "summary": "The Radiological Society of North America (RSNA) Lumbar Degenerative Imaging\nSpine Classification (LumbarDISC) dataset is the largest publicly available\ndataset of adult MRI lumbar spine examinations annotated for degenerative\nchanges. The dataset includes 2,697 patients with a total of 8,593 image series\nfrom 8 institutions across 6 countries and 5 continents. The dataset is\navailable for free for non-commercial use via Kaggle and RSNA Medical Imaging\nResource of AI (MIRA). The dataset was created for the RSNA 2024 Lumbar Spine\nDegenerative Classification competition where competitors developed deep\nlearning models to grade degenerative changes in the lumbar spine. The degree\nof spinal canal, subarticular recess, and neural foraminal stenosis was graded\nat each intervertebral disc level in the lumbar spine. The images were\nannotated by expert volunteer neuroradiologists and musculoskeletal\nradiologists from the RSNA, American Society of Neuroradiology, and the\nAmerican Society of Spine Radiology. This dataset aims to facilitate research\nand development in machine learning and lumbar spine imaging to lead to\nimproved patient care and clinical efficiency.", "comment": null, "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.09162v1", "AI": {"title_translation": "RSNA腰椎退行性影像学脊柱分类（LumbarDISC）数据集", "tldr": "LumbarDISC是RSNA发布的最大的公开可用腰椎MRI退行性变数据集，旨在推动机器学习在腰椎影像学研究中的应用。", "motivation": "该数据集旨在为RSNA 2024腰椎退行性分类竞赛提供数据，并促进机器学习和腰椎影像学领域的研究与开发，以改善患者护理和临床效率。", "method": "LumbarDISC数据集包含2,697名患者的8,593个图像系列，由来自8个机构的RSNA、美国神经放射学会和美国脊柱放射学会的专家神经放射科医生和肌肉骨骼放射科医生对腰椎每个椎间盘水平的椎管、关节下隐窝和神经孔狭窄程度进行分级注释。该数据集通过Kaggle和RSNA医学影像AI资源（MIRA）免费提供非商业用途。", "result": "RSNA Lumbar Degenerative Imaging Spine Classification (LumbarDISC) 数据集是目前最大的公开可用的成人腰椎MRI退行性变标注数据集。它包含来自6个国家和5个大陆的8个机构的2,697名患者的8,593个图像系列。", "conclusion": "该数据集的发布旨在促进机器学习在腰椎影像学研究与开发中的应用，最终目标是改善患者护理和提高临床效率。", "translation": "北美放射学会（RSNA）腰椎退行性影像学脊柱分类（LumbarDISC）数据集是目前最大的公开可用的成人MRI腰椎检查退行性变注释数据集。该数据集包括来自6个国家和5大洲的8个机构的2,697名患者，共计8,593个图像系列。该数据集可通过Kaggle和RSNA医学影像AI资源（MIRA）免费用于非商业用途。该数据集是为RSNA 2024腰椎退行性分类竞赛创建的，竞赛选手开发深度学习模型来对腰椎的退行性变进行分级。在腰椎的每个椎间盘水平，对椎管、关节下隐窝和神经孔狭窄的程度进行了分级。图像由来自RSNA、美国神经放射学会和美国脊柱放射学会的专家志愿神经放射科医生和肌肉骨骼放射科医生进行注释。该数据集旨在促进机器学习和腰椎影像学领域的研究和开发，以改善患者护理和临床效率。", "summary": "RSNA LumbarDISC数据集是最大的公开腰椎MRI退行性变数据集，包含2,697名患者的8,593个图像系列。该数据集由专家放射科医生注释，旨在支持RSNA 2024竞赛，并推动机器学习在腰椎影像学中的研究与应用，以期改善患者护理和提升临床效率。", "keywords": "腰椎退行性变, MRI, 数据集, 机器学习, 放射学", "comments": "该数据集的创新之处在于其规模庞大且经过专家注释，填补了该领域高质量公开数据集的空白。其重要性在于为深度学习模型在腰椎退行性变分类方面的开发和验证提供了宝贵的资源，有望显著推动医学影像AI在临床诊断和治疗中的应用，从而提高诊断准确性和效率。"}}
{"id": "2506.09178", "title": "Understanding Self-Regulated Learning Behavior Among High and Low Dropout Risk Students During CS1: Combining Trace Logs, Dropout Prediction and Self-Reports", "authors": ["Denis Zhidkikh", "Ville Isomöttönen", "Toni Taipalus"], "summary": "The introductory programming course (CS1) at the university level is often\nperceived as particularly challenging, contributing to high dropout rates among\nComputer Science students. Identifying when and how students encounter\ndifficulties in this course is critical for providing targeted support. This\nstudy explores the behavioral patterns of CS1 students at varying dropout risks\nusing self-regulated learning (SRL) as the theoretical framework. Using\nlearning analytics, we analyzed trace logs and task performance data from a\nvirtual learning environment to map resource usage patterns and used student\ndropout prediction to distinguish between low and high dropout risk behaviors.\nData from 47 consenting students were used to carry out the analysis.\nAdditionally, self-report questionnaires from 29 participants enriched the\ninterpretation of observed patterns. The findings reveal distinct weekly\nlearning strategy types and categorize course behavior. Among low dropout risk\nstudents, three learning strategies were identified that different in how\nstudents prioritized completing tasks and reading course materials. High\ndropout risk students exhibited nine different strategies, some representing\ntemporary unsuccessful strategies that can be recovered from, while others\nindicating behaviors of students on the verge of dropping out. This study\nhighlights the value of combining student behavior profiling with predictive\nlearning analytics to explain dropout predictions and devise targeted\ninterventions. Practical findings of the study can in turn be used to help\nteachers, teaching assistants and other practitioners to better recognize and\naddress students at the verge of dropping out.", "comment": "29 pages, 8 figures, 3 tables, submitted to ACM Transactions on\n  Computing Education", "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.09178v1", "AI": {"title_translation": "理解CS1课程中高低辍学风险学生的自我调节学习行为：结合追踪日志、辍学预测和自我报告", "tldr": "本研究结合追踪日志、辍学预测和自我报告，分析了CS1课程中高低辍学风险学生的自我调节学习行为，以识别并支持有辍学风险的学生。", "motivation": "大学阶段的入门级编程课程（CS1）通常被认为极具挑战性，导致计算机科学专业学生的辍学率很高。识别学生何时以及如何在该课程中遇到困难对于提供有针对性的支持至关重要。", "method": "本研究利用学习分析方法，分析了来自虚拟学习环境的追踪日志和任务表现数据，以描绘资源使用模式，并利用学生辍学预测来区分低风险和高风险辍学行为。分析使用了来自47名同意学生的S数据。此外，29名参与者的自我报告问卷丰富了对观察到模式的解释。", "result": "研究结果揭示了不同的每周学习策略类型并对课程行为进行了分类。在低辍学风险学生中，识别出三种学习策略，它们在学生如何优先完成任务和阅读课程材料方面有所不同。高辍学风险学生表现出九种不同的策略，其中一些代表了可以恢复的暂时性不成功策略，而另一些则表明学生处于辍学边缘的行为。", "conclusion": "这项研究强调了将学生行为画像与预测性学习分析相结合的价值，以解释辍学预测并制定有针对性的干预措施。研究的实际发现反过来可以帮助教师、助教和其他实践者更好地识别和解决处于辍学边缘的学生。", "translation": "大学阶段的入门级编程课程（CS1）通常被认为极具挑战性，导致计算机科学专业学生的辍学率很高。识别学生何时以及如何在该课程中遇到困难对于提供有针对性的支持至关重要。本研究以自我调节学习（SRL）为理论框架，探讨了CS1课程中不同辍学风险学生的行为模式。我们利用学习分析方法，分析了来自虚拟学习环境的追踪日志和任务表现数据，以描绘资源使用模式，并利用学生辍学预测来区分低风险和高风险辍学行为。分析使用了来自47名同意学生的S数据。此外，29名参与者的自我报告问卷丰富了对观察到模式的解释。研究结果揭示了不同的每周学习策略类型并对课程行为进行了分类。在低辍学风险学生中，识别出三种学习策略，它们在学生如何优先完成任务和阅读课程材料方面有所不同。高辍学风险学生表现出九种不同的策略，其中一些代表了可以恢复的暂时性不成功策略，而另一些则表明学生处于辍学边缘的行为。这项研究强调了将学生行为画像与预测性学习分析相结合的价值，以解释辍学预测并制定有针对性的干预措施。研究的实际发现反过来可以帮助教师、助教和其他实践者更好地识别和解决处于辍学边缘的学生。", "summary": "本研究旨在通过结合追踪日志、辍学预测和自我报告，深入理解CS1课程中高低辍学风险学生的自我调节学习行为。研究分析了47名学生的学习分析数据和29名学生的自我报告，揭示了不同辍学风险学生独特的学习策略类型。结果显示，低风险学生有三种学习策略，而高风险学生则有九种，包括可恢复的暂时性失败策略和预示辍学的行为。本研究强调了结合学生行为画像与预测性学习分析的重要性，以期为CS1课程中处于辍学边缘的学生提供有针对性的干预和支持。", "keywords": "自我调节学习, 辍学预测, CS1课程, 学习分析, 追踪日志", "comments": "这项研究的创新之处在于其结合了多种数据源（追踪日志、辍学预测和自我报告）来全面理解学生的自我调节学习行为，并将其与辍学风险关联起来。其重要性在于为CS1课程中识别和干预高风险学生提供了实用的洞察和工具，有助于降低辍学率。"}}
{"id": "2506.09909", "title": "TransGI: Real-Time Dynamic Global Illumination With Object-Centric Neural Transfer Model", "authors": ["Yijie Deng", "Lei Han", "Lu Fang"], "summary": "Neural rendering algorithms have revolutionized computer graphics, yet their\nimpact on real-time rendering under arbitrary lighting conditions remains\nlimited due to strict latency constraints in practical applications. The key\nchallenge lies in formulating a compact yet expressive material representation.\nTo address this, we propose TransGI, a novel neural rendering method for\nreal-time, high-fidelity global illumination. It comprises an object-centric\nneural transfer model for material representation and a radiance-sharing\nlighting system for efficient illumination. Traditional BSDF representations\nand spatial neural material representations lack expressiveness, requiring\nthousands of ray evaluations to converge to noise-free colors. Conversely,\nreal-time methods trade quality for efficiency by supporting only diffuse\nmaterials. In contrast, our object-centric neural transfer model achieves\ncompactness and expressiveness through an MLP-based decoder and vertex-attached\nlatent features, supporting glossy effects with low memory overhead. For\ndynamic, varying lighting conditions, we introduce local light probes capturing\nscene radiance, coupled with an across-probe radiance-sharing strategy for\nefficient probe generation. We implemented our method in a real-time rendering\nengine, combining compute shaders and CUDA-based neural networks. Experimental\nresults demonstrate that our method achieves real-time performance of less than\n10 ms to render a frame and significantly improved rendering quality compared\nto baseline methods.", "comment": null, "cate": "cs.GR", "url": "http://arxiv.org/abs/2506.09909v1", "AI": {"title_translation": "TransGI：基于目标中心神经传输模型的实时动态全局光照", "tldr": "TransGI是一种新的神经渲染方法，通过目标中心神经传输模型和辐射共享光照系统，实现了实时、高保真动态全局光照，帧渲染时间小于10毫秒。", "motivation": "现有的神经渲染算法在任意光照条件下的实时渲染方面受限于严格的延迟约束，主要挑战在于如何构建紧凑而富有表现力的材质表示。传统BSDF和空间神经材质表示缺乏表现力，需要大量光线评估；实时方法则牺牲质量，仅支持漫反射材质。", "method": "提出TransGI，它包含一个用于材质表示的目标中心神经传输模型和一个用于高效照明的辐射共享光照系统。目标中心神经传输模型通过基于MLP的解码器和顶点附加的潜在特征实现紧凑性和表现力，支持光泽效果且内存开销低。对于动态、变化的光照条件，引入了捕获场景辐射的局部光照探头，并结合跨探头辐射共享策略以实现高效的探头生成。该方法在实时渲染引擎中实现，结合了计算着色器和基于CUDA的神经网络。", "result": "实验结果表明，该方法实现了小于10毫秒的实时帧渲染性能，并且与基线方法相比，渲染质量显著提高。", "conclusion": "TransGI通过创新的目标中心神经传输模型和高效的辐射共享光照系统，成功解决了实时动态全局光照中的关键挑战，实现了高性能和高保真的渲染效果。", "translation": "神经渲染算法彻底改变了计算机图形学，但由于实际应用中严格的延迟限制，它们在任意光照条件下的实时渲染方面影响有限。关键挑战在于如何构建紧凑而富有表现力的材质表示。为了解决这个问题，我们提出了TransGI，一种用于实时、高保真全局光照的新型神经渲染方法。它包括一个用于材质表示的目标中心神经传输模型和一个用于高效照明的辐射共享光照系统。传统的BSDF表示和空间神经材质表示缺乏表现力，需要数千次光线评估才能收敛到无噪声的颜色。相反，实时方法通过仅支持漫反射材质来牺牲质量以换取效率。相比之下，我们的目标中心神经传输模型通过基于MLP的解码器和顶点附加的潜在特征实现了紧凑性和表现力，以低内存开销支持光泽效果。对于动态、变化的光照条件，我们引入了捕获场景辐射的局部光照探头，并结合跨探头辐射共享策略以实现高效的探头生成。我们在实时渲染引擎中实现了我们的方法，结合了计算着色器和基于CUDA的神经网络。实验结果表明，我们的方法实现了小于10毫秒的实时性能以渲染一帧，并且与基线方法相比，渲染质量显著提高。", "summary": "TransGI是一种新颖的神经渲染方法，旨在解决现有算法在实时动态全局光照中面临的延迟和材质表示挑战。该方法提出了一种目标中心神经传输模型，利用基于MLP的解码器和顶点附加的潜在特征，以紧凑且富有表现力的方式表示材质，支持光泽效果并降低内存开销。为处理动态光照，TransGI引入了局部光照探头和跨探头辐射共享策略。实验结果表明，TransGI在实时渲染引擎中实现了小于10毫秒的帧渲染时间，并显著提升了渲染质量。", "keywords": "实时渲染, 全局光照, 神经渲染, 材质表示, 光照系统", "comments": "该论文提出了一种创新的目标中心神经传输模型，有效解决了实时全局光照中材质表示的紧凑性和表现力问题。通过结合辐射共享光照系统，它优化了动态光照处理，使得神经渲染在实时动态全局光照场景中具有重要的实践意义，有望推动该领域的发展。"}}
{"id": "2506.09205", "title": "Genetic Transformer-Assisted Quantum Neural Networks for Optimal Circuit Design", "authors": ["Haiyan Wang"], "summary": "We introduce Genetic Transformer Assisted Quantum Neural Networks (GTQNNs), a\nhybrid learning framework that combines a transformer encoder with a shallow\nvariational quantum circuit and automatically fine tunes the circuit via the\nNSGA-II multi objective genetic algorithm. The transformer reduces\nhigh-dimensional classical data to a compact, qubit sized representation, while\nNSGA-II searches for Pareto optimal circuits that (i) maximize classification\naccuracy and (ii) minimize primitive gate count an essential constraint for\nnoisy intermediate-scale quantum (NISQ) hardware. Experiments on four\nbenchmarks (Iris, Breast Cancer, MNIST, and Heart Disease) show that GTQNNs\nmatch or exceed state of the art quantum models while requiring much fewer\ngates for most cases. A hybrid Fisher information analysis further reveals that\nthe trained networks operate far from barren plateaus; the leading curvature\ndirections increasingly align with the quantum subspace as the qubit budget\ngrows, confirming that the transformer front end has effectively condensed the\ndata. Together, these results demonstrate that GTQNNs deliver competitive\nperformance with a quantum resource budget well suited to present-day NISQ\ndevices.", "comment": null, "cate": "quant-ph", "url": "http://arxiv.org/abs/2506.09205v1", "AI": {"title_translation": "遗传变压器辅助量子神经网络用于优化电路设计", "tldr": "引入GTQNNs，一种结合变压器和量子电路，通过NSGA-II优化量子电路设计，实现高性能且低门数的量子模型，适用于NISQ设备。", "motivation": "针对噪声中等规模量子（NISQ）硬件的限制，寻找一种能同时最大化分类精度和最小化原始门数的量子电路设计方法。", "method": "提出遗传变压器辅助量子神经网络（GTQNNs），一个混合学习框架。该框架结合了变压器编码器和浅层变分量子电路，并通过NSGA-II多目标遗传算法自动微调电路。变压器用于将高维经典数据降维为紧凑的、量子比特大小的表示，而NSGA-II则寻找帕累托最优电路，以最大化分类精度和最小化原始门数。", "result": "在Iris、乳腺癌、MNIST和心脏病四个基准测试中，GTQNNs的性能与最先进的量子模型相当或超越。在大多数情况下，GTQNNs所需的门数大大减少。混合Fisher信息分析表明，训练后的网络远离贫瘠高原；随着量子比特预算的增加，主曲率方向越来越与量子子空间对齐，证实了变压器前端有效地压缩了数据。", "conclusion": "GTQNNs以适合当前NISQ设备的量子资源预算，提供了具有竞争力的性能。", "translation": "我们引入了遗传变压器辅助量子神经网络（GTQNNs），这是一个混合学习框架，它将变压器编码器与浅层变分量子电路相结合，并通过NSGA-II多目标遗传算法自动微调电路。变压器将高维经典数据减少为紧凑的、量子比特大小的表示，而NSGA-II则搜索帕累托最优电路，这些电路（i）最大化分类精度和（ii）最小化原始门数——这是噪声中等规模量子（NISQ）硬件的基本约束。在四个基准测试（Iris、乳腺癌、MNIST和心脏病）上的实验表明，GTQNNs与最先进的量子模型性能相当或超越，同时在大多数情况下所需的门数少得多。混合Fisher信息分析进一步揭示，训练后的网络远离贫瘠高原；随着量子比特预算的增长，主曲率方向越来越与量子子空间对齐，证实了变压器前端有效地压缩了数据。总而言之，这些结果表明GTQNNs以适合当前NISQ设备的量子资源预算，提供了具有竞争力的性能。", "summary": "本文提出了一种名为遗传变压器辅助量子神经网络（GTQNNs）的混合学习框架。该框架结合了变压器编码器和浅层变分量子电路，并利用NSGA-II多目标遗传算法进行电路优化。GTQNNs通过变压器对经典数据进行降维，并由NSGA-II寻找在分类精度和门数之间达到帕累托最优的量子电路。实验结果表明，GTQNNs在多个基准测试上取得了与现有最先进量子模型相当或更优的性能，同时显著减少了所需的量子门数，使其非常适合资源受限的NISQ设备。", "keywords": "遗传变压器，量子神经网络，电路设计，NSGA-II，NISQ", "comments": "这篇论文的创新点在于将经典机器学习中的变压器模型与量子神经网络和多目标遗传算法相结合，有效地解决了NISQ设备中量子资源（特别是门数）受限的问题。通过变压器进行数据压缩，再通过NSGA-II优化电路，实现了在保持高性能的同时显著降低量子电路复杂度的目标，这对于当前量子计算硬件的发展具有重要意义。"}}
{"id": "2506.09266", "title": "Improved error bounds for Koopman operator and reconstructed trajectories approximations with kernel-based methods", "authors": ["Diego Olguín", "Axel Osses", "Héctor Ramírez"], "summary": "In this article, we propose a new error bound for Koopman operator\napproximation using Kernel Extended Dynamic Mode Decomposition. The new\nestimate is $O(N^{-1/2})$, with a constant related to the probability of\nsuccess of the bound, given by Hoeffding's inequality, similar to other\nmethodologies, such as Philipp et al. Furthermore, we propose a \\textit{lifting\nback} operator to obtain trajectories generated by embedding the initial state\nand iterating a linear system in a higher dimension. This naturally yields an\n$O(N^{-1/2})$ error bound for mean trajectories. Finally, we show numerical\nresults including an example of nonlinear system, exhibiting successful\napproximation with exponential decay faster than $-1/2$, as suggested by the\ntheoretical results.", "comment": "24 pages, 6 figures", "cate": "math.NA", "url": "http://arxiv.org/abs/2506.09266v1", "AI": {"title_translation": "Koopman算子和基于核方法的重构轨迹近似的改进误差界", "tldr": "本文提出了一种使用核扩展动态模态分解的Koopman算子近似的新误差界限，并引入了“回溯”算子以获得轨迹，两者均达到了O(N^-1/2)的误差界限，并通过数值结果验证了其有效性。", "motivation": "改进Koopman算子近似的误差界限，并提供一种方法来重构轨迹。", "method": "1. 提出了一种新的基于核扩展动态模态分解（Kernel Extended Dynamic Mode Decomposition）的Koopman算子近似误差界限。\n2. 提出了一种“回溯”（lifting back）算子，通过嵌入初始状态并在更高维度迭代线性系统来获得生成的轨迹。", "result": "1. 新的Koopman算子近似误差估计为 $O(N^{-1/2})$。\n2. “回溯”算子自然地产生了平均轨迹的 $O(N^{-1/2})$ 误差界限。\n3. 数值结果（包括非线性系统示例）显示成功的近似，其指数衰减速度快于 $-1/2$，与理论结果一致。", "conclusion": "论文成功地提出了改进的Koopman算子和重构轨迹近似的误差界限，并通过数值实验验证了其有效性。", "translation": "在本文中，我们提出了一种使用核扩展动态模态分解（Kernel Extended Dynamic Mode Decomposition）进行Koopman算子近似的新误差界限。新的估计是 $O(N^{-1/2})$，其常数与界限的成功概率相关，由霍夫丁不等式给出，类似于其他方法，例如 Philipp 等人的工作。此外，我们提出了一个“回溯”（lifting back）算子，通过嵌入初始状态并在更高维度迭代线性系统来获得生成的轨迹。这自然地产生了平均轨迹的 $O(N^{-1/2})$ 误差界限。最后，我们展示了数值结果，包括一个非线性系统的例子，该例子显示出成功的近似，其指数衰减速度快于 $-1/2$，正如理论结果所暗示的。", "summary": "本文提出了一种基于核扩展动态模态分解的Koopman算子近似的新误差界限，其估计为 $O(N^{-1/2})$。同时，引入了一个“回溯”算子来重构轨迹，并证明其平均轨迹的误差界限也为 $O(N^{-1/2})$。通过数值实验验证了这些理论结果在非线性系统中的有效性，显示出快速的指数衰减。", "keywords": "Koopman算子, 核方法, 误差界限, 轨迹近似, 动态模态分解", "comments": "该论文在Koopman算子近似的误差分析方面做出了贡献，特别是引入了新的误差界限和“回溯”算子，为基于核方法的动态系统分析提供了更严格的理论基础和实际应用潜力。其理论结果通过数值实验得到了验证，增加了研究的可信度。"}}
{"id": "2506.09206", "title": "SimClass: A Classroom Speech Dataset Generated via Game Engine Simulation For Automatic Speech Recognition Research", "authors": ["Ahmed Adel Attia", "Jing Liu", "Carl Espy-Wilson"], "summary": "The scarcity of large-scale classroom speech data has hindered the\ndevelopment of AI-driven speech models for education. Public classroom datasets\nremain limited, and the lack of a dedicated classroom noise corpus prevents the\nuse of standard data augmentation techniques.\n  In this paper, we introduce a scalable methodology for synthesizing classroom\nnoise using game engines, a framework that extends to other domains. Using this\nmethodology, we present SimClass, a dataset that includes both a synthesized\nclassroom noise corpus and a simulated classroom speech dataset. The speech\ndata is generated by pairing a public children's speech corpus with YouTube\nlecture videos to approximate real classroom interactions in clean conditions.\nOur experiments on clean and noisy speech demonstrate that SimClass closely\napproximates real classroom speech, making it a valuable resource for\ndeveloping robust speech recognition and enhancement models.", "comment": null, "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.09206v1", "AI": {"title_translation": "SimClass：一个通过游戏引擎模拟生成的用于自动语音识别研究的课堂语音数据集", "tldr": "本文介绍了SimClass，一个通过游戏引擎模拟生成的课堂语音数据集，旨在解决教育领域AI语音模型开发中课堂语音数据稀缺的问题。", "motivation": "缺乏大规模的课堂语音数据阻碍了教育领域AI驱动的语音模型发展，现有公开数据集有限，且缺少专门的课堂噪声语料库，无法使用标准数据增强技术。", "method": "提出了一种使用游戏引擎合成课堂噪声的可扩展方法。基于此方法，创建了SimClass数据集，包含合成的课堂噪声语料库和模拟的课堂语音数据集。语音数据通过将儿童语音语料库与YouTube讲座视频配对生成，以模拟真实的课堂交互。", "result": "实验表明，SimClass数据集在干净和嘈杂语音条件下都能很好地模拟真实的课堂语音。", "conclusion": "SimClass数据集是开发鲁棒语音识别和增强模型的宝贵资源，能够有效缓解课堂语音数据稀缺的问题。", "translation": "大规模课堂语音数据的稀缺阻碍了教育领域AI驱动语音模型的发展。公开的课堂数据集仍然有限，并且缺乏专门的课堂噪声语料库，这使得标准数据增强技术无法使用。\n在本文中，我们介绍了一种使用游戏引擎合成课堂噪声的可扩展方法，该框架可以扩展到其他领域。使用这种方法，我们提出了SimClass，一个包含合成课堂噪声语料库和模拟课堂语音数据集的数据集。语音数据通过将公共儿童语音语料库与YouTube讲座视频配对生成，以模拟干净条件下的真实课堂交互。我们对干净和嘈杂语音的实验表明，SimClass密切地近似于真实的课堂语音，使其成为开发鲁棒语音识别和增强模型的宝贵资源。", "summary": "针对教育领域AI语音模型开发中课堂语音数据稀缺的挑战，本文提出了一种利用游戏引擎合成课堂噪声的可扩展方法，并基于此构建了SimClass数据集。该数据集包含合成的课堂噪声和模拟的课堂语音（通过结合儿童语音和YouTube讲座视频生成），实验证明SimClass能有效模拟真实课堂语音，为语音识别和增强模型的开发提供了重要资源。", "keywords": "课堂语音数据集, 游戏引擎模拟, 自动语音识别, 数据增强, SimClass", "comments": "本文创新性地利用游戏引擎模拟生成课堂语音数据，有效解决了真实课堂数据采集困难和数据量不足的问题。这种模拟方法具有可扩展性，未来可应用于其他领域的数据生成，为特定领域语音模型的开发提供了新的思路和宝贵资源。"}}
{"id": "2506.09559", "title": "Identity and Access Management for the Computing Continuum", "authors": ["Chalima Dimitra Nassar Kyriakidou", "Athanasia Maria Papathanasiou", "Vasilios A. Siris", "Nikos Fotiou", "George C. Polyzos", "Eduardo Cánovas Martínez", "Antonio Skarmeta"], "summary": "The computing continuum introduces new challenges for access control due to\nits dynamic, distributed, and heterogeneous nature. In this paper, we propose a\nZero-Trust (ZT) access control solution that leverages decentralized\nidentification and authentication mechanisms based on Decentralized Identifiers\n(DIDs) and Verifiable Credentials (VCs). Additionally, we employ\nRelationship-Based Access Control (ReBAC) to define policies that capture the\nevolving trust relationships inherent in the continuum. Through a\nproof-of-concept implementation, we demonstrate the feasibility and efficiency\nof our solution, highlighting its potential to enhance security and trust in\ndecentralized environments.", "comment": "Proceedings of the 2nd International Workshop on MetaOS for the\n  Cloud-Edge-IoT Continuum, pp 33-39. 2025", "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.09559v1", "AI": {"title_translation": "计算连续体中的身份与访问管理", "tldr": "针对计算连续体中的访问控制挑战，本文提出了一种结合零信任、去中心化身份（DIDs/VCs）和基于关系访问控制（ReBAC）的解决方案，并通过概念验证展示了其可行性和效率。", "motivation": "计算连续体由于其动态、分布式和异构的特性，为访问控制带来了新的挑战。", "method": "提出了一种零信任（ZT）访问控制解决方案，该方案利用基于去中心化标识符（DIDs）和可验证凭证（VCs）的去中心化识别和认证机制。此外，采用基于关系的访问控制（ReBAC）来定义策略，以捕获连续体中固有的不断演变的信任关系。", "result": "通过概念验证实施，证明了该解决方案的可行性和效率。", "conclusion": "该解决方案有望增强去中心化环境中的安全性和信任。", "translation": "计算连续体因其动态、分布式和异构的特性，为访问控制带来了新的挑战。在本文中，我们提出了一种零信任（ZT）访问控制解决方案，该方案利用基于去中心化标识符（DIDs）和可验证凭证（VCs）的去中心化识别和认证机制。此外，我们采用基于关系的访问控制（ReBAC）来定义策略，以捕获连续体中固有的不断演变的信任关系。通过概念验证实施，我们展示了我们解决方案的可行性和效率，突出了其在去中心化环境中增强安全性和信任的潜力。", "summary": "本文针对计算连续体中动态、分布式和异构特性带来的访问控制挑战，提出了一种创新的零信任（ZT）访问控制解决方案。该方案结合了基于去中心化标识符（DIDs）和可验证凭证（VCs）的去中心化身份验证机制，并融入了基于关系的访问控制（ReBAC）来管理不断变化的信任关系。通过概念验证，研究证明了该方案的有效性和效率，有望显著提升去中心化环境下的安全性和信任度。", "keywords": "计算连续体, 身份与访问管理, 零信任, 去中心化标识符, 基于关系访问控制", "comments": "该论文提出了一种结合零信任、去中心化身份和基于关系访问控制的综合性解决方案，以应对计算连续体中复杂的访问控制挑战。其创新性在于将多种先进的安全范式融合，旨在解决传统集中式模型在动态异构环境下的局限性。通过概念验证，展示了其实用潜力，对于提升未来去中心化环境的安全性和信任具有重要意义。"}}
{"id": "2506.09384", "title": "Analyzing Key Objectives in Human-to-Robot Retargeting for Dexterous Manipulation", "authors": ["Chendong Xin", "Mingrui Yu", "Yongpeng Jiang", "Zhefeng Zhang", "Xiang Li"], "summary": "Kinematic retargeting from human hands to robot hands is essential for\ntransferring dexterity from humans to robots in manipulation teleoperation and\nimitation learning. However, due to mechanical differences between human and\nrobot hands, completely reproducing human motions on robot hands is impossible.\nExisting works on retargeting incorporate various optimization objectives,\nfocusing on different aspects of hand configuration. However, the lack of\nexperimental comparative studies leaves the significance and effectiveness of\nthese objectives unclear. This work aims to analyze these retargeting\nobjectives for dexterous manipulation through extensive real-world comparative\nexperiments. Specifically, we propose a comprehensive retargeting objective\nformulation that integrates intuitively crucial factors appearing in recent\napproaches. The significance of each factor is evaluated through experimental\nablation studies on the full objective in kinematic posture retargeting and\nreal-world teleoperated manipulation tasks. Experimental results and\nconclusions provide valuable insights for designing more accurate and effective\nretargeting algorithms for real-world dexterous manipulation.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.09384v1", "AI": {"title_translation": "人机灵巧操作重定向中的关键目标分析", "tldr": "本文通过广泛的真实世界实验，分析了人手到机器人手运动重定向中各种优化目标的有效性，以弥补现有研究中缺乏实验比较的空白，并为设计更准确的重定向算法提供见解。", "motivation": "由于人手和机器人手之间的机械差异，将人类动作完全复制到机器人手上是不可能的。现有重定向工作整合了各种优化目标，但缺乏实验比较研究，导致这些目标的重要性与有效性不明确。", "method": "本文提出了一种综合性的重定向目标公式，整合了现有方法中直观上重要的因素。通过对完整目标进行运动学姿态重定向和真实世界遥操作任务的实验消融研究，评估了每个因素的重要性。", "result": "实验结果和结论为人机灵巧操作中设计更准确、更有效的重定向算法提供了宝贵的见解。", "conclusion": "本研究的实验结果和结论为人机灵巧操作设计更准确、有效的重定向算法提供了宝贵的见解。", "translation": "将人手运动学重定向到机器人手对于在操作遥操作和模仿学习中将人类灵巧性转移到机器人至关重要。然而，由于人手和机器人手之间的机械差异，完全在机器人手上重现人类动作是不可能的。现有的重定向工作整合了各种优化目标，侧重于手部配置的不同方面。然而，缺乏实验比较研究使得这些目标的重要性与有效性不明确。本工作旨在通过广泛的真实世界比较实验，分析这些用于灵巧操作的重定向目标。具体而言，我们提出了一种综合性的重定向目标公式，该公式整合了最近方法中直观上重要的因素。通过对运动学姿态重定向和真实世界遥操作任务中完整目标的实验消融研究，评估了每个因素的重要性。实验结果和结论为设计更准确、有效的真实世界灵巧操作重定向算法提供了宝贵的见解。", "summary": "本研究旨在通过广泛的真实世界实验，分析人手到机器人手运动重定向中的关键优化目标，以解决现有研究中缺乏实验比较的问题。文章提出了一种综合性的重定向目标公式，并利用消融研究评估了其中各因素的重要性。研究结果为开发更准确、有效的机器人灵巧操作重定向算法提供了有价值的指导。", "keywords": "运动学重定向, 灵巧操作, 机器人手, 优化目标, 遥操作", "comments": "本文通过填补现有研究中缺乏对人手到机器人手重定向优化目标的实验比较这一空白，具有重要的实践意义。其提出的综合性目标公式和严格的消融实验方法，为未来重定向算法的设计提供了坚实的实验依据和宝贵的见解。"}}
{"id": "2506.09083", "title": "BakuFlow: A Streamlining Semi-Automatic Label Generation Tool", "authors": ["Jerry Lin", "Partick P. W. Chen"], "summary": "Accurately labeling (or annotation) data is still a bottleneck in computer\nvision, especially for large-scale tasks where manual labeling is\ntime-consuming and error-prone. While tools like LabelImg can handle the\nlabeling task, some of them still require annotators to manually label each\nimage. In this paper, we introduce BakuFlow, a streamlining semi-automatic\nlabel generation tool. Key features include (1) a live adjustable magnifier for\npixel-precise manual corrections, improving user experience; (2) an interactive\ndata augmentation module to diversify training datasets; (3) label propagation\nfor rapidly copying labeled objects between consecutive frames, greatly\naccelerating annotation of video data; and (4) an automatic labeling module\npowered by a modified YOLOE framework. Unlike the original YOLOE, our extension\nsupports adding new object classes and any number of visual prompts per class\nduring annotation, enabling flexible and scalable labeling for dynamic,\nreal-world datasets. These innovations make BakuFlow especially effective for\nobject detection and tracking, substantially reducing labeling workload and\nimproving efficiency in practical computer vision and industrial scenarios.", "comment": "4 pages, 3 figures, 1 Table", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09083v1", "AI": {"title_translation": "BakuFlow：一种流线型半自动标签生成工具", "tldr": "BakuFlow是一个半自动标签生成工具，通过结合手动修正、数据增强、标签传播和改进的YOLOE自动标注，显著提高了计算机视觉任务的标注效率和灵活性。", "motivation": "计算机视觉中准确的数据标注仍然是瓶颈，尤其对于大规模任务，手动标注耗时且易出错。现有工具如LabelImg仍需大量手动操作。", "method": "BakuFlow是一个流线型半自动标签生成工具，其关键特性包括：1. 可实时调节的放大镜，用于像素级精确手动修正。2. 交互式数据增强模块，用于多样化训练数据集。3. 标签传播功能，用于在连续帧之间快速复制已标注对象。4. 由改进的YOLOE框架驱动的自动标注模块，支持添加新对象类别和每个类别任意数量的视觉提示。", "result": "这些创新使BakuFlow在对象检测和跟踪方面特别有效，显著减少了标注工作量，提高了在实际计算机视觉和工业场景中的效率。", "conclusion": "BakuFlow通过其半自动化的特性和对YOLOE框架的改进，有效解决了大规模数据标注的效率和灵活性问题，特别适用于对象检测和跟踪任务。", "translation": "标题：BakuFlow：一种流线型半自动标签生成工具\n摘要：在计算机视觉领域，准确的数据标注（或注释）仍然是一个瓶颈，特别是对于大规模任务，手动标注既耗时又容易出错。虽然像LabelImg这样的工具可以处理标注任务，但其中一些仍然要求标注者手动标注每张图像。在本文中，我们引入了BakuFlow，一个流线型半自动标签生成工具。其主要特点包括：（1）一个实时可调节的放大镜，用于像素级精确手动修正，改善用户体验；（2）一个交互式数据增强模块，用于多样化训练数据集；（3）标签传播功能，用于在连续帧之间快速复制已标注对象，极大地加速了视频数据的标注；（4）一个由改进的YOLOE框架驱动的自动标注模块。与原始的YOLOE不同，我们的扩展支持在标注过程中添加新的对象类别和每个类别任意数量的视觉提示，从而为动态、真实世界的数据集实现灵活和可扩展的标注。这些创新使得BakuFlow在对象检测和跟踪方面特别有效，大大减少了实际计算机视觉和工业场景中的标注工作量并提高了效率。", "summary": "BakuFlow是一个创新的半自动标签生成工具，旨在解决计算机视觉领域大规模数据标注的效率和准确性问题。它结合了像素级手动修正、交互式数据增强、视频标签传播以及基于改进YOLOE框架的自动标注功能。特别是，其自动标注模块支持灵活添加新类别和视觉提示，使其能够高效处理动态、真实世界的数据集，显著提升了对象检测和跟踪任务的标注效率并降低了工作量。", "keywords": "数据标注, 半自动工具, 计算机视觉, YOLOE, 对象检测, 标签传播", "comments": "BakuFlow的创新之处在于其结合了多种功能来创建一个流线型的半自动标注流程。特别是对YOLOE框架的改进，使其能够支持动态添加新类别和视觉提示，这对于处理不断变化和复杂的数据集至关重要，大大增强了工具的实用性和可扩展性。它解决了计算机视觉领域长期存在的数据标注瓶颈。"}}
{"id": "2506.09656", "title": "Application-Driven Value Alignment in Agentic AI Systems: Survey and Perspectives", "authors": ["Wei Zeng", "Hengshu Zhu", "Chuan Qin", "Han Wu", "Yihang Cheng", "Sirui Zhang", "Xiaowei Jin", "Yinuo Shen", "Zhenxing Wang", "Feimin Zhong", "Hui Xiong"], "summary": "The ongoing evolution of AI paradigms has propelled AI research into the\nAgentic AI stage. Consequently, the focus of research has shifted from single\nagents and simple applications towards multi-agent autonomous decision-making\nand task collaboration in complex environments. As Large Language Models (LLMs)\nadvance, their applications become more diverse and complex, leading to\nincreasingly situational and systemic risks. This has brought significant\nattention to value alignment for AI agents, which aims to ensure that an\nagent's goals, preferences, and behaviors align with human values and societal\nnorms. This paper reviews value alignment in agent systems within specific\napplication scenarios. It integrates the advancements in AI driven by large\nmodels with the demands of social governance. Our review covers value\nprinciples, agent system application scenarios, and agent value alignment\nevaluation. Specifically, value principles are organized hierarchically from a\ntop-down perspective, encompassing macro, meso, and micro levels. Agent system\napplication scenarios are categorized and reviewed from a general-to-specific\nviewpoint. Agent value alignment evaluation systematically examines datasets\nfor value alignment assessment and relevant value alignment methods.\nAdditionally, we delve into value coordination among multiple agents within\nagent systems. Finally, we propose several potential research directions in\nthis field.", "comment": null, "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.09656v1", "AI": {"title_translation": "智能体AI系统中应用驱动的价值对齐：综述与展望", "tldr": "本文综述了智能体AI系统中应用驱动的价值对齐问题，涵盖了价值原则、应用场景、评估方法和多智能体协调，并提出了未来研究方向。", "motivation": "随着AI范式演进到智能体AI阶段，研究焦点转向复杂环境中的多智能体自主决策和任务协作。大型语言模型的应用日益多样和复杂，导致情境性和系统性风险增加，因此确保AI智能体的目标、偏好和行为与人类价值观及社会规范对齐变得至关重要。", "method": "本文通过综述的方式，审查了特定应用场景下智能体系统中的价值对齐问题。它将大型模型驱动的AI进展与社会治理需求相结合，并从价值原则（宏观、中观、微观）、智能体系统应用场景（从一般到具体）和智能体价值对齐评估（数据集、方法）三个方面进行了系统性梳理和回顾。此外，还探讨了多智能体间的价值协调。", "result": "本文系统地组织了价值原则的层次结构（宏观、中观、微观），对智能体系统应用场景进行了分类和回顾，并审查了价值对齐评估的数据集和方法。同时，探讨了多智能体间的价值协调问题。", "conclusion": "本文对智能体AI系统中的价值对齐进行了全面的综述，并提出了该领域的几个潜在研究方向。", "translation": "随着AI范式的持续演进，AI研究已进入智能体AI阶段。因此，研究重点已从单一智能体和简单应用转向复杂环境中的多智能体自主决策和任务协作。随着大型语言模型（LLMs）的进步，其应用变得更加多样和复杂，导致情境性和系统性风险日益增加。这使得AI智能体的价值对齐受到极大关注，其旨在确保智能体的目标、偏好和行为与人类价值观及社会规范相符。本文综述了特定应用场景下智能体系统中的价值对齐问题。它将大型模型驱动的AI进展与社会治理需求相结合。我们的综述涵盖了价值原则、智能体系统应用场景以及智能体价值对齐评估。具体而言，价值原则从自上而下的角度分层组织，包括宏观、中观和微观层面。智能体系统应用场景从一般到具体的视角进行分类和审查。智能体价值对齐评估系统地考察了用于价值对齐评估的数据集和相关价值对齐方法。此外，我们深入探讨了智能体系统中多个智能体之间的价值协调。最后，我们提出了该领域的几个潜在研究方向。", "summary": "本文对智能体AI系统中的应用驱动价值对齐问题进行了全面综述。鉴于大型语言模型推动AI进入多智能体、复杂环境应用阶段，带来了潜在风险，价值对齐成为关键。该综述系统地梳理了价值原则的层次结构、智能体系统应用场景分类、价值对齐评估方法及数据集，并探讨了多智能体间的价值协调，最终提出了未来研究方向。", "keywords": "智能体AI系统, 价值对齐, 大型语言模型, 多智能体系统, 综述", "comments": "这篇综述论文的重要性在于它及时地关注了当前AI发展前沿——智能体AI系统中的核心挑战之一：价值对齐。随着LLM的应用日益复杂，其潜在风险也随之增加，因此确保AI行为与人类价值观一致至关重要。该论文的创新之处在于其结构化的综述方法，将价值原则、应用场景和评估方法进行了系统性整合，并特别关注了多智能体协调，为未来研究提供了清晰的路线图。"}}
{"id": "2506.09092", "title": "CUDA-LLM: LLMs Can Write Efficient CUDA Kernels", "authors": ["Wentao Chen", "Jiace Zhu", "Qi Fan", "Yehan Ma", "An Zou"], "summary": "Large Language Models (LLMs) have demonstrated strong capabilities in\ngeneral-purpose code generation. However, generating the code which is deeply\nhardware-specific, architecture-aware, and performance-critical, especially for\nmassively parallel GPUs, remains a complex challenge. In this work, we explore\nthe use of LLMs for the automated generation and optimization of CUDA programs,\nwith the goal of producing high-performance GPU kernels that fully exploit the\nunderlying hardware. To address this challenge, we propose a novel framework\ncalled \\textbf{Feature Search and Reinforcement (FSR)}. FSR jointly optimizes\ncompilation and functional correctness, as well as the runtime performance,\nwhich are validated through extensive and diverse test cases, and measured by\nactual kernel execution latency on the target GPU, respectively. This approach\nenables LLMs not only to generate syntactically and semantically correct CUDA\ncode but also to iteratively refine it for efficiency, tailored to the\ncharacteristics of the GPU architecture. We evaluate FSR on representative CUDA\nkernels, covering AI workloads and computational intensive algorithms. Our\nresults show that LLMs augmented with FSR consistently guarantee correctness\nrates. Meanwhile, the automatically generated kernels can outperform general\nhuman-written code by a factor of up to 179$\\times$ in execution speeds. These\nfindings highlight the potential of combining LLMs with performance\nreinforcement to automate GPU programming for hardware-specific,\narchitecture-sensitive, and performance-critical applications.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09092v1", "AI": {"title_translation": "CUDA-LLM：LLM 可以编写高效的 CUDA 内核", "tldr": "CUDA-LLM 提出 FSR 框架，使 LLM 能够生成并优化高效的 CUDA 内核，在 GPU 上实现高达 179 倍的性能提升。", "motivation": "大型语言模型在通用代码生成方面表现出色，但在生成深度硬件特定、架构感知且性能关键的代码（尤其是针对大规模并行 GPU 的代码）方面仍面临复杂挑战。", "method": "本文提出了一个名为“特征搜索和强化”（FSR）的新颖框架。FSR 联合优化编译和功能正确性以及运行时性能。通过广泛多样的测试用例验证，并分别通过目标 GPU 上的实际内核执行延迟进行测量。该方法使 LLM 不仅能够生成语法和语义正确的 CUDA 代码，还能根据 GPU 架构的特性迭代优化其效率。", "result": "FSR 增强的 LLM 始终保证了正确率。同时，自动生成的内核在执行速度上比通用的人工编写代码快高达 179 倍。", "conclusion": "这些发现突出了将 LLM 与性能强化相结合，以自动化硬件特定、架构敏感和性能关键应用程序的 GPU 编程的潜力。", "translation": "大型语言模型（LLM）在通用代码生成方面表现出强大的能力。然而，生成深度硬件特定、架构感知且性能关键的代码，特别是针对大规模并行 GPU 的代码，仍然是一个复杂的挑战。在这项工作中，我们探索使用 LLM 自动生成和优化 CUDA 程序，目标是生成能够充分利用底层硬件的高性能 GPU 内核。为了解决这一挑战，我们提出了一个名为“特征搜索和强化”（FSR）的新颖框架。FSR 联合优化编译和功能正确性以及运行时性能，这些分别通过广泛多样的测试用例进行验证，并通过目标 GPU 上的实际内核执行延迟进行测量。这种方法使 LLM 不仅能够生成语法和语义正确的 CUDA 代码，还能根据 GPU 架构的特性迭代优化其效率。我们在代表性的 CUDA 内核上评估了 FSR，涵盖了 AI 工作负载和计算密集型算法。我们的结果表明，FSR 增强的 LLM 始终保证了正确率。同时，自动生成的内核在执行速度上比通用的人工编写代码快高达 179 倍。这些发现突出了将 LLM 与性能强化相结合，以自动化硬件特定、架构敏感和性能关键应用程序的 GPU 编程的潜力。", "summary": "本文提出了 CUDA-LLM 框架，旨在利用大型语言模型（LLM）自动生成和优化高性能 CUDA 内核。针对 GPU 编程中硬件特异性和性能优化的挑战，该研究引入了“特征搜索和强化”（FSR）框架。FSR 能够联合优化代码的编译、功能正确性和运行时性能，并根据 GPU 架构特性迭代改进代码效率。实验结果表明，结合 FSR 的 LLM 能够确保代码的正确性，并且自动生成的内核在执行速度上比人工编写的代码快高达 179 倍，展示了 LLM 在自动化 GPU 编程方面的巨大潜力。", "keywords": "大型语言模型, CUDA, GPU 编程, 代码生成, 性能优化", "comments": "该论文的创新之处在于提出了 FSR 框架，有效地将 LLM 的代码生成能力与硬件特定的性能优化相结合，解决了 LLM 在生成高性能、硬件感知代码方面的固有挑战。其重要性在于为自动化 GPU 编程开辟了新途径，显著提升了开发效率和代码性能，对于 AI 和计算密集型应用领域具有重要意义。高达 179 倍的性能提升是一个非常亮眼的成果，预示着未来硬件编程的范式转变。"}}
{"id": "2506.09641", "title": "Modeling Probabilistic Reduction using Information Theory and Naive Discriminative Learning", "authors": ["Anna Stein", "Kevin Tang"], "summary": "This study compares probabilistic predictors based on information theory with\nNaive Discriminative Learning (NDL) predictors in modeling acoustic word\nduration, focusing on probabilistic reduction. We examine three models using\nthe Buckeye corpus: one with NDL-derived predictors using information-theoretic\nformulas, one with traditional NDL predictors, and one with N-gram\nprobabilistic predictors. Results show that the N-gram model outperforms both\nNDL models, challenging the assumption that NDL is more effective due to its\ncognitive motivation. However, incorporating information-theoretic formulas\ninto NDL improves model performance over the traditional model. This research\nhighlights a) the need to incorporate not only frequency and contextual\npredictability but also average contextual predictability, and b) the\nimportance of combining information-theoretic metrics of predictability and\ninformation derived from discriminative learning in modeling acoustic\nreduction.", "comment": "Submitted to Interspeech 2025", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.09641v1", "AI": {"title_translation": "使用信息论和朴素判别学习建模概率性缩减", "tldr": "本研究比较了基于信息论的概率预测器与朴素判别学习（NDL）预测器在声学词语时长建模中的表现，发现N-gram模型优于NDL模型，但将信息论公式整合到NDL中可以提高其性能。", "motivation": "本研究旨在比较基于信息论的概率预测器与朴性判别学习（NDL）预测器在建模声学词语时长中的表现，并探究概率性缩减的有效模型。", "method": "研究使用了Buckeye语料库，比较了三种模型：1) 使用信息论公式推导的NDL预测器模型；2) 传统NDL预测器模型；3) N-gram概率预测器模型。", "result": "N-gram模型表现优于两种NDL模型，挑战了NDL因认知动机而更有效的假设。然而，将信息论公式整合到NDL中可以改善其性能。", "conclusion": "在建模声学缩减时，不仅需要考虑频率和语境可预测性，还需要考虑平均语境可预测性。结合可预测性的信息论度量和判别学习得出的信息至关重要。", "translation": "本研究比较了基于信息论的概率预测器与朴素判别学习（NDL）预测器在建模声学词语时长中的表现，重点关注概率性缩减。我们使用Buckeye语料库检验了三种模型：一种是使用信息论公式推导的NDL预测器模型，一种是传统NDL预测器模型，以及一种是N-gram概率预测器模型。结果表明，N-gram模型表现优于两种NDL模型，这挑战了NDL因其认知动机而更有效的假设。然而，将信息论公式整合到NDL中可以改善其相对于传统模型的性能。这项研究强调了a) 不仅需要纳入频率和语境可预测性，还需要纳入平均语境可预测性，以及b) 在建模声学缩减时，结合可预测性的信息论度量和从判别学习中得出的信息的重要性。", "summary": "本研究比较了信息论和朴素判别学习（NDL）在建模声学词语时长中的概率性缩减。通过在Buckeye语料库上测试三种模型（信息论-NDL、传统NDL和N-gram），结果显示N-gram模型表现最佳，对NDL的认知优势提出挑战。但研究也发现，将信息论公式融入NDL可提升其性能。这表明在建模声学缩减时，需结合频率、语境可预测性、平均语境可预测性，并整合信息论和判别学习的优点。", "keywords": "信息论, 朴素判别学习, 概率性缩减, 声学词语时长, N-gram模型", "comments": "本研究通过比较不同模型对声学词语时长中概率性缩减的建模能力，对朴素判别学习（NDL）的有效性提出了新的见解。其创新之处在于将信息论公式引入NDL，并证明了这种结合能够提升模型性能。研究强调了在声学缩减建模中，不仅要考虑频率和语境可预测性，还要关注平均语境可预测性以及信息论与判别学习相结合的重要性，为未来相关研究提供了宝贵的方向。"}}
{"id": "2506.09683", "title": "Calculating Software's Energy Use and Carbon Emissions: A Survey of the State of Art, Challenges, and the Way Ahead", "authors": ["Priyavanshi Pathania", "Nikhil Bamby", "Rohit Mehra", "Samarth Sikand", "Vibhu Saujanya Sharma", "Vikrant Kaulgud", "Sanjay Podder", "Adam P. Burden"], "summary": "The proliferation of software and AI comes with a hidden risk: its growing\nenergy and carbon footprint. As concerns regarding environmental sustainability\ncome to the forefront, understanding and optimizing how software impacts the\nenvironment becomes paramount. In this paper, we present a state-of-the-art\nreview of methods and tools that enable the measurement of software and\nAI-related energy and/or carbon emissions. We introduce a taxonomy to\ncategorize the existing work as Monitoring, Estimation, or Black-Box\napproaches. We delve deeper into the tools and compare them across different\ndimensions and granularity - for example, whether their measurement encompasses\nenergy and carbon emissions and the components considered (like CPU, GPU, RAM,\netc.). We present our observations on the practical use (component wise\nconsolidation of approaches) as well as the challenges that we have identified\nacross the current state-of-the-art. As we start an initiative to address these\nchallenges, we emphasize active collaboration across the community in this\nimportant field.", "comment": "8 pages. To be published in the proceedings of 9th International\n  Workshop on Green and Sustainable Software (GREENS '25), April 29, 2025,\n  Ottawa, Canada (Co-located with ICSE 2025)", "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.09683v1", "AI": {"title_translation": "计算软件的能耗和碳排放：现状、挑战和未来之路调查", "tldr": "软件和AI的能耗及碳排放日益增长，本文综述了现有测量方法和工具，提出了分类法，并讨论了挑战和未来方向。", "motivation": "软件和AI的普及带来了日益增长的能耗和碳足迹，环境可持续性问题日益突出，因此理解和优化软件对环境的影响变得至关重要。", "method": "本文对测量软件和AI相关能耗及碳排放的现有方法和工具进行了综述，引入了一种分类法（监控、估计、黑盒方法），并深入比较了工具在不同维度和粒度上的差异。", "result": "提出了一个将现有工作分类为监控、估计或黑盒方法的分类法；比较了不同工具在能耗和碳排放测量范围以及考虑的组件（如CPU、GPU、RAM等）方面的差异；提出了对实际使用的观察结果以及识别出的当前技术挑战。", "conclusion": "软件和AI的能耗和碳排放测量是一个重要的领域，存在挑战，需要社区积极协作来解决这些挑战。", "translation": "软件和人工智能的普及带来了一个隐藏的风险：其日益增长的能源和碳足迹。随着对环境可持续性的关注日益突出，理解和优化软件如何影响环境变得至关重要。在本文中，我们对测量软件和人工智能相关能源和/或碳排放的方法和工具进行了最新综述。我们引入了一个分类法，将现有工作分为监控、估计或黑盒方法。我们深入探讨了这些工具，并在不同维度和粒度上进行了比较——例如，它们的测量是否涵盖能源和碳排放以及所考虑的组件（如CPU、GPU、RAM等）。我们提出了对实际使用（方法组件级整合）的观察结果以及我们识别出的当前技术面临的挑战。当我们开始一项旨在应对这些挑战的倡议时，我们强调社区在这个重要领域进行积极协作。", "summary": "本文对软件和AI的能耗及碳排放测量方法和工具进行了全面的综述。作者提出了一个分类法来组织现有工作，并详细比较了各种工具。研究揭示了当前技术的实际应用观察和面临的挑战，并强调了社区协作以应对这些挑战的重要性。", "keywords": "软件能耗, 碳排放, 能源效率, AI碳足迹, 测量工具", "comments": "这篇综述文章及时且重要，因为它解决了日益增长的软件和AI能耗及其环境影响问题。通过提供一个分类法和对现有工具的详细比较，它为研究人员和实践者理解该领域的现状提供了宝贵的框架。文章还明确指出了未来的挑战和协作的必要性，为未来的研究指明了方向。"}}
{"id": "2506.09354", "title": "\"Is This Really a Human Peer Supporter?\": Misalignments Between Peer Supporters and Experts in LLM-Supported Interactions", "authors": ["Kellie Yu Hui Sim", "Roy Ka-Wei Lee", "Kenny Tsu Wei Choo"], "summary": "Mental health is a growing global concern, prompting interest in AI-driven\nsolutions to expand access to psychosocial support. Peer support, grounded in\nlived experience, offers a valuable complement to professional care. However,\nvariability in training, effectiveness, and definitions raises concerns about\nquality, consistency, and safety. Large Language Models (LLMs) present new\nopportunities to enhance peer support interactions, particularly in real-time,\ntext-based interactions. We present and evaluate an AI-supported system with an\nLLM-simulated distressed client, context-sensitive LLM-generated suggestions,\nand real-time emotion visualisations. 2 mixed-methods studies with 12 peer\nsupporters and 5 mental health professionals (i.e., experts) examined the\nsystem's effectiveness and implications for practice. Both groups recognised\nits potential to enhance training and improve interaction quality. However, we\nfound a key tension emerged: while peer supporters engaged meaningfully,\nexperts consistently flagged critical issues in peer supporter responses, such\nas missed distress cues and premature advice-giving. This misalignment\nhighlights potential limitations in current peer support training, especially\nin emotionally charged contexts where safety and fidelity to best practices are\nessential. Our findings underscore the need for standardised, psychologically\ngrounded training, especially as peer support scales globally. They also\ndemonstrate how LLM-supported systems can scaffold this development--if\ndesigned with care and guided by expert oversight. This work contributes to\nemerging conversations on responsible AI integration in mental health and the\nevolving role of LLMs in augmenting peer-delivered care.", "comment": null, "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.09354v1", "AI": {"title_translation": "“这真的是人类同伴支持者吗？”：大型语言模型支持的互动中同伴支持者与专家之间的错位", "tldr": "研究发现，在LLM辅助的心理健康同伴支持中，同伴支持者和专家对互动质量的看法存在显著差异，强调了标准化培训和专家监督的重要性。", "motivation": "心理健康问题日益增长，AI驱动解决方案（特别是LLM）有望扩展心理社会支持。同伴支持虽然有价值，但存在培训、有效性及定义方面的变异性，引发对质量和安全的担忧。本研究旨在探讨LLM如何增强同伴支持，并评估其潜在问题。", "method": "研究开发并评估了一个AI支持系统，该系统包含一个LLM模拟的受困客户、情境敏感的LLM生成建议和实时情绪可视化。通过2项混合方法研究，招募了12名同伴支持者和5名心理健康专业人员（专家）来检查系统的有效性和实践意义。", "result": "两组（同伴支持者和专家）都认可系统在增强培训和提高互动质量方面的潜力。然而，一个关键的矛盾出现了：尽管同伴支持者进行了有意义的参与，但专家们持续指出同伴支持者回应中的关键问题，例如错过了痛苦线索和过早给出建议。这突显了两者之间的错位。", "conclusion": "研究结果强调了对标准化、以心理学为基础的同伴支持培训的需求，尤其是在全球推广同伴支持时。同时，LLM支持系统可以促进这种发展，但前提是设计需谨慎并有专家监督。这项工作有助于探讨AI在心理健康领域负责任的整合以及LLM在增强同伴护理中的作用。", "translation": "心理健康是一个日益增长的全球性问题，这促使人们对人工智能驱动的解决方案产生兴趣，以扩大心理社会支持的获取。同伴支持以其亲身经历为基础，为专业护理提供了宝贵的补充。然而，培训、有效性和定义方面的差异引发了对质量、一致性和安全性的担忧。大型语言模型（LLMs）为增强同伴支持互动，特别是在实时、基于文本的互动中，带来了新的机会。我们提出并评估了一个由人工智能支持的系统，该系统包含一个LLM模拟的受困客户、情境敏感的LLM生成建议和实时情绪可视化。通过对12名同伴支持者和5名心理健康专业人员（即专家）进行的2项混合方法研究，检查了该系统的有效性及其对实践的影响。两组都认可其在增强培训和提高互动质量方面的潜力。然而，我们发现一个关键的矛盾出现了：尽管同伴支持者进行了有意义的参与，但专家们持续指出同伴支持者回应中的关键问题，例如错过了痛苦线索和过早给出建议。这种错位突显了当前同伴支持培训的潜在局限性，特别是在情感激烈、安全性和遵循最佳实践至关重要的情境中。我们的研究结果强调了对标准化、以心理学为基础的培训的需求，尤其是在全球范围内推广同伴支持时。它们还表明，如果设计得当并由专家监督，LLM支持的系统如何能够促进这种发展。这项工作为新兴的关于人工智能在心理健康领域负责任整合以及LLMs在增强同伴提供护理中不断演变的作用的讨论做出了贡献。", "summary": "本研究探讨了在大型语言模型（LLM）辅助下进行心理健康同伴支持的潜力与挑战。研究开发了一个AI支持系统，并对同伴支持者和心理健康专家进行了评估。结果显示，尽管双方都认同LLM辅助系统的潜在益处，但专家们发现同伴支持者在互动中存在关键问题，如未能识别痛苦线索或过早提供建议，这揭示了同伴支持培训的不足。论文强调了标准化、心理学基础培训的重要性，并指出LLM系统在专家监督下可作为提升同伴支持质量的工具。", "keywords": "心理健康, 同伴支持, 大型语言模型 (LLM), AI辅助系统, 培训错位", "comments": "这篇论文的创新之处在于它直接探讨了LLM在心理健康同伴支持中的应用，并揭示了同伴支持者与专家之间在互动质量判断上的关键“错位”。这指出了LLM辅助系统在实际应用中可能面临的挑战，即如何确保由非专业人士提供的支持能够达到专业标准。论文的重要性在于它不仅指出了问题，也提出了解决方案的方向——标准化培训和专家监督下的LLM辅助。这对于负责任地将AI整合到敏感的心理健康领域具有重要的指导意义。"}}
{"id": "2506.09789", "title": "Delegations as Adaptive Representation Patterns: Rethinking Influence in Liquid Democracy", "authors": ["Davide Grossi", "Andreas Nitsche"], "summary": "Liquid democracy is a mechanism for the division of labor in decision-making\nthrough the transitive delegation of influence. In essence, all individuals\npossess the autonomy to determine the issues with which they will engage\ndirectly, while for other matters, they may appoint a representative of their\nchoosing. So far, the literature has studied the delegation structures emerging\nin liquid democracy as static. As a result, transitivity defined as the\ncapacity to transfer acquired authority to another entity, has been identified\nas a concern as it would be conducive to unrestrained accumulation of power.\n  Focusing on the implementation of liquid democracy supported by the\nLiquidFeedback software, we propose a novel approach to assessing the influence\nof voting nodes in a transitive delegation graph, taking into account the\nprocess nature of real-world liquid democracy in which delegation and voting\nare distinct and increasingly independent activities. By introducing a novel\nmodel of delegations in liquid democracy, we show how transitivity may in fact\ncontribute to an effective regulation of deliberation influence and\ndecision-making power. While maintaining the one-person, one-vote paradigm for\nall votes cast, the anticipated influence of an agent, to the extent it is\nstemming from transitivity, experiences a precipitous decline following an\nexponential trajectory.\n  In general, it is our objective to move the first steps towards a rigorous\nanalysis of liquid democracy as an adaptive democratic representation process.\nThe adaptivity aspect of liquid democracy has not yet been explored within the\nexisting academic literature despite it being, we believe, one of its most\nimportant features. We therefore also outline a research agenda focusing on\nthis aspect of liquid democracy.", "comment": null, "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.09789v1", "AI": {"title_translation": "委托作为自适应代表模式：重新思考流动民主中的影响力", "tldr": "流动民主中，委托的传递性常被视为权力累积的风险，但本文提出其实际上有助于有效调节影响力，且代理人的预期影响力会呈指数级下降。", "motivation": "现有文献将流动民主中的委托结构视为静态的，并认为传递性会导致权力不受限制的累积。本文旨在对流动民主进行严格分析，将其视为一个自适应的民主代表过程，而自适应性是现有文献尚未充分探索的重要特征。", "method": "本文提出了一种评估流动民主中投票节点影响力的新方法，该方法考虑了委托和投票是独立活动的实际过程性质，并引入了一个新颖的委托模型。研究基于LiquidFeedback软件。", "result": "研究表明，传递性实际上可能有助于有效调节审议影响力和决策权力。尽管保持一人一票的原则，但代理人因传递性而产生的预期影响力会呈指数级下降。", "conclusion": "本文旨在迈出将流动民主严格分析为自适应民主代表过程的第一步。作者认为自适应性是流动民主最重要的特征之一，并为此方面勾勒了未来的研究议程。", "translation": "流动民主是一种通过影响力的传递性委托实现决策分工的机制。本质上，所有个体都拥有自主权来决定他们将直接参与的问题，而对于其他事务，他们可以选择指定一位代表。迄今为止，文献将流动民主中出现的委托结构视为静态的。因此，传递性（定义为将获得的权力转移给另一个实体的能力）被认为是一个问题，因为它可能导致权力不受限制的累积。\n本研究侧重于由LiquidFeedback软件支持的流动民主的实施，提出了一种评估传递性委托图中投票节点影响力的新方法，该方法考虑了现实世界流动民主的过程性质，即委托和投票是不同且日益独立的活动。通过引入一个新颖的流动民主委托模型，我们展示了传递性实际上如何有助于有效调节审议影响力和决策权力。在所有投票都保持一人一票范式的同时，代理人因传递性而产生的预期影响力会呈指数级下降。\n总的来说，我们的目标是迈出将流动民主作为一种自适应民主代表过程进行严格分析的第一步。流动民主的自适应性方面尚未在现有学术文献中得到探索，尽管我们认为这是其最重要的特征之一。因此，我们还勾勒了一个侧重于流动民主这一方面的研究议程。", "summary": "本研究重新审视了流动民主中的委托机制，特别关注其传递性。与现有文献将委托结构视为静态不同，本文提出了一种新方法来评估流动民主中投票节点的影响力，该方法考虑了委托和投票作为独立活动的过程性质。通过引入一个新颖的委托模型，研究表明传递性实际上可以有效调节审议影响力和决策权力，且代理人的预期影响力会呈指数级下降。本文旨在将流动民主分析为一个自适应的民主代表过程，并提出了未来的研究议程。", "keywords": "流动民主, 委托, 传递性, 自适应代表, 影响力", "comments": "本文的创新之处在于将流动民主的委托机制视为一个动态的、自适应的过程，挑战了以往文献中关于传递性必然导致权力过度集中的观点。它提出了一个量化影响力衰减的模型，为理解流动民主的实际运作提供了新的视角。强调自适应性及其在未来研究中的重要性，为该领域开辟了新的研究方向。"}}
{"id": "2506.09512", "title": "A Survey on the Role of Artificial Intelligence and Machine Learning in 6G-V2X Applications", "authors": ["Donglin Wang", "Anjie Qiu", "Qiuheng Zhou", "Hans D. Schotten"], "summary": "The rapid advancement of Vehicle-to-Everything (V2X) communication is\ntransforming Intelligent Transportation Systems (ITS), with 6G networks\nexpected to provide ultra-reliable, low-latency, and high-capacity connectivity\nfor Connected and Autonomous Vehicles (CAVs). Artificial Intelligence (AI) and\nMachine Learning (ML) have emerged as key enablers in optimizing V2X\ncommunication by enhancing network management, predictive analytics, security,\nand cooperative driving due to their outstanding performance across various\ndomains, such as natural language processing and computer vision. This survey\ncomprehensively reviews recent advances in AI and ML models applied to 6G-V2X\ncommunication. It focuses on state-of-the-art techniques, including Deep\nLearning (DL), Reinforcement Learning (RL), Generative Learning (GL), and\nFederated Learning (FL), with particular emphasis on developments from the past\ntwo years. Notably, AI, especially GL, has shown remarkable progress and\nemerging potential in enhancing the performance, adaptability, and intelligence\nof 6G-V2X systems. Despite these advances, a systematic summary of recent\nresearch efforts in this area remains lacking, which this survey aims to\naddress. We analyze their roles in 6G-V2X applications, such as intelligent\nresource allocation, beamforming, intelligent traffic management, and security\nmanagement. Furthermore, we explore the technical challenges, including\ncomputational complexity, data privacy, and real-time decision-making\nconstraints, while identifying future research directions for AI-driven 6G-V2X\ndevelopment. This study aims to provide valuable insights for researchers,\nengineers, and policymakers working towards realizing intelligent, AI-powered\nV2X ecosystems in 6G communication.", "comment": "7 pages, 1 figure", "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.09512v1", "AI": {"title_translation": "人工智能和机器学习在6G-V2X应用中作用的综述", "tldr": "本综述全面回顾了人工智能和机器学习模型在6G-V2X通信中的最新进展，重点关注深度学习、强化学习、生成学习和联邦学习等技术，并分析了它们在智能资源分配、波束成形、智能交通管理和安全管理等应用中的作用，探讨了技术挑战并指明了未来的研究方向。", "motivation": "尽管人工智能和机器学习在6G-V2X通信中取得了显著进展，但目前缺乏对该领域最新研究成果的系统性总结，本综述旨在填补这一空白。", "method": "本综述全面审查了应用于6G-V2X通信的人工智能和机器学习模型的最新进展，重点关注深度学习、强化学习、生成学习和联邦学习等最先进技术，特别是过去两年内的发展。分析了它们在6G-V2X应用中的作用，并探讨了技术挑战。", "result": "综述分析了人工智能和机器学习在6G-V2X应用中的作用，如智能资源分配、波束成形、智能交通管理和安全管理。同时，探讨了计算复杂度、数据隐私和实时决策限制等技术挑战，并确定了AI驱动的6G-V2X发展的未来研究方向。", "conclusion": "本研究旨在为致力于实现6G通信中智能、AI驱动的V2X生态系统的研究人员、工程师和政策制定者提供有价值的见解。", "translation": "车联网（V2X）通信的快速发展正在改变智能交通系统（ITS），6G网络有望为联网和自动驾驶车辆（CAVs）提供超可靠、低延迟和高容量的连接。人工智能（AI）和机器学习（ML）凭借其在自然语言处理和计算机视觉等各个领域的出色表现，已成为优化V2X通信的关键推动者，通过增强网络管理、预测分析、安全性以及协同驾驶。本综述全面回顾了应用于6G-V2X通信的人工智能和机器学习模型的最新进展。它重点关注最先进的技术，包括深度学习（DL）、强化学习（RL）、生成学习（GL）和联邦学习（FL），并特别强调了过去两年内的发展。值得注意的是，人工智能，特别是生成学习，在增强6G-V2X系统的性能、适应性和智能性方面显示出显著的进展和新兴潜力。尽管取得了这些进展，但该领域最新研究成果的系统性总结仍然缺乏，本综述旨在解决这一问题。我们分析了它们在6G-V2X应用中的作用，例如智能资源分配、波束成形、智能交通管理和安全管理。此外，我们探讨了技术挑战，包括计算复杂度、数据隐私和实时决策限制，同时确定了AI驱动的6G-V2X发展的未来研究方向。本研究旨在为致力于实现6G通信中智能、AI驱动的V2X生态系统的研究人员、工程师和政策制定者提供有价值的见解。", "summary": "本综述探讨了人工智能和机器学习在6G-V2X通信中的关键作用，旨在填补现有研究系统性总结的空白。文章全面回顾了深度学习、强化学习、生成学习和联邦学习等前沿技术在提升6G-V2X系统性能、适应性和智能性方面的应用，并分析了它们在智能资源分配、波束成形、智能交通管理和安全管理等方面的具体应用。同时，指出了计算复杂度、数据隐私和实时决策等技术挑战，并提出了未来的研究方向，为相关领域的专业人士提供了有价值的见解。", "keywords": "6G-V2X, 人工智能, 机器学习, 深度学习, 联邦学习", "comments": "本综述的重要价值在于系统性地梳理了人工智能和机器学习在6G-V2X领域的前沿应用和技术挑战，填补了该领域缺乏系统性总结的空白。文章不仅涵盖了多种先进的机器学习范式，还深入分析了它们在具体应用场景中的潜力与局限，并对未来研究方向进行了展望，对于推动6G-V2X生态系统的发展具有重要的指导意义。"}}
{"id": "2506.09397", "title": "SLED: A Speculative LLM Decoding Framework for Efficient Edge Serving", "authors": ["Xiangchen Li", "Dimitrios Spatharakis", "Saeid Ghafouri", "Jiakun Fan", "Dimitrios Nikolopoulos"], "summary": "Regardless the advancements in device capabilities, efficient inferencing\nadvanced large language models (LLMs) at the edge remains challenging due to\nlimited device memory and power constraints. Existing strategies, such as\naggressive quantization, pruning, or remote inference, trade accuracy for\nefficiency or lead to substantial cost burdens. This position paper introduces\na new approach that leverages speculative decoding, previously viewed primarily\nas a decoding acceleration technique for autoregressive generation of LLMs, as\na promising approach specifically adapted for edge computing by orchestrating\ncomputation across heterogeneous devices. We propose SLED, a method that allows\nlightweight edge devices to draft multiple candidate tokens locally using\ndiverse draft models, while a single, shared edge server efficiently batches\nand verifies the tokens utilizing a more precise target model. This approach\nsupports device heterogeneity and reduces server-side memory footprint by\navoiding the need to deploy multiple target models. Our initial experiments\nwith Jetson Orin Nano, Raspberry Pi 5, and an RTX 6000 edge server indicate\nsubstantial benefits: significantly reduced latency, improved energy\nefficiency, and increased concurrent inference sessions, all without\nsacrificing model accuracy.", "comment": "6 pages, 9 figures, 2 tables", "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.09397v1", "AI": {"title_translation": "SLED：一种用于高效边缘服务的推测性LLM解码框架", "tldr": "SLED是一种新的LLM边缘推理框架，通过在轻量级边缘设备上草拟令牌并在共享边缘服务器上验证，显著降低延迟并提高效率，同时保持准确性。", "motivation": "尽管设备能力有所进步，但在边缘设备上高效推理大型语言模型（LLMs）仍然面临挑战，原因在于设备内存和功耗受限。现有策略（如量化、剪枝或远程推理）要么牺牲准确性，要么导致高成本。", "method": "本文提出SLED框架，利用推测性解码。轻量级边缘设备使用不同的草稿模型在本地草拟多个候选令牌，而单个共享边缘服务器利用更精确的目标模型高效地批量处理和验证这些令牌。该方法支持设备异构性并减少服务器端内存占用。", "result": "初步实验表明SLED显著降低了延迟、提高了能源效率、增加了并发推理会话，且未牺牲模型准确性。", "conclusion": "SLED通过在异构设备间协调计算，为边缘LLM推理提供了一种高效且准确的解决方案。", "translation": "尽管设备能力有所进步，但在边缘设备上高效推理先进的大型语言模型（LLMs）仍然面临挑战，原因在于设备内存和功耗受限。现有策略，如激进的量化、剪枝或远程推理，要么以牺牲准确性换取效率，要么导致巨大的成本负担。这篇立场论文介绍了一种新方法，该方法利用推测性解码——以前主要被视为LLM自回归生成的一种解码加速技术——作为一种有前景的、专门适用于边缘计算的方法，通过在异构设备之间协调计算。我们提出了SLED，一种允许轻量级边缘设备使用不同的草稿模型在本地草拟多个候选令牌的方法，而单个共享边缘服务器则利用更精确的目标模型高效地批量处理和验证这些令牌。这种方法支持设备异构性，并通过避免部署多个目标模型的需求来减少服务器端的内存占用。我们使用Jetson Orin Nano、Raspberry Pi 5和RTX 6000边缘服务器进行的初步实验表明了显著的益处：显著降低延迟、提高能源效率以及增加并发推理会话，所有这些都未牺牲模型准确性。", "summary": "本文提出SLED框架，旨在解决边缘设备上LLM推理的内存和功耗限制问题。SLED利用推测性解码，允许轻量级边缘设备在本地生成候选令牌，并由一个共享边缘服务器使用更精确的模型进行高效验证。这种分布式方法支持设备异构性，并能减少服务器内存占用。初步实验验证了SLED在降低延迟、提高能效和增加并发会话方面的显著优势，同时保持了模型准确性。", "keywords": "推测性解码, 边缘计算, 大型语言模型, LLM推理, SLED", "comments": "SLED的创新之处在于将推测性解码应用于边缘计算场景，并设计了异构设备间的协同工作机制，有效地解决了边缘LLM部署的资源限制问题。其重要性在于提供了一种无需牺牲准确性即可提高边缘LLM推理效率的实用方案。"}}
{"id": "2506.09707", "title": "Fine-Tuning Large Audio-Language Models with LoRA for Precise Temporal Localization of Prolonged Exposure Therapy Elements", "authors": ["Suhas BN", "Andrew M. Sherrill", "Jyoti Alaparthi", "Dominik Mattioli", "Rosa I. Arriaga", "Chris W. Wiese", "Saeed Abdullah"], "summary": "Prolonged Exposure (PE) therapy is an effective treatment for post-traumatic\nstress disorder (PTSD), but evaluating therapist fidelity remains\nlabor-intensive due to the need for manual review of session recordings. We\npresent a method for the automatic temporal localization of key PE fidelity\nelements -- identifying their start and stop times -- directly from session\naudio and transcripts. Our approach fine-tunes a large pre-trained\naudio-language model, Qwen2-Audio, using Low-Rank Adaptation (LoRA) to process\nfocused 30-second windows of audio-transcript input. Fidelity labels for three\ncore protocol phases -- therapist orientation (P1), imaginal exposure (P2), and\npost-imaginal processing (P3) -- are generated via LLM-based prompting and\nverified by trained raters. The model is trained to predict normalized boundary\noffsets using soft supervision guided by task-specific prompts. On a dataset of\n313 real PE sessions, our best configuration (LoRA rank 8, 30s windows)\nachieves a mean absolute error (MAE) of 5.3 seconds across tasks. We further\nanalyze the effects of window size and LoRA rank, highlighting the importance\nof context granularity and model adaptation. This work introduces a scalable\nframework for fidelity tracking in PE therapy, with potential to support\nclinician training, supervision, and quality assurance.", "comment": "5 pages, 2 figures", "cate": "eess.AS", "url": "http://arxiv.org/abs/2506.09707v1", "AI": {"title_translation": "使用 LoRA 微调大型音频-语言模型以精确时间定位延长暴露疗法要素", "tldr": "论文提出一种利用LoRA微调大型音频-语言模型（Qwen2-Audio）的方法，用于自动识别和定位延长暴露疗法（PE）会话中的关键治疗元素，以实现治疗师忠实度的自动化评估，其MAE为5.3秒。", "motivation": "延长暴露（PE）疗法对PTSD有效，但评估治疗师忠实度因需要手动审查会话录音而耗时费力。", "method": "提出一种自动时间定位PE忠实度元素的方法。该方法使用LoRA微调大型预训练音频-语言模型Qwen2-Audio，处理30秒的音频-转录输入窗口。通过基于LLM的提示生成忠实度标签并由训练有素的评估员验证。模型训练预测归一化边界偏移量，并采用任务特定提示指导的软监督。", "result": "在313个真实PE会话数据集上，最佳配置（LoRA rank 8, 30秒窗口）在所有任务上实现了5.3秒的平均绝对误差（MAE）。还分析了窗口大小和LoRA rank的影响，强调了上下文粒度和模型适应的重要性。", "conclusion": "这项工作引入了一个可扩展的PE疗法忠实度跟踪框架，有望支持临床医生培训、监督和质量保证。", "translation": "延长暴露（PE）疗法是治疗创伤后应激障碍（PTSD）的有效方法，但由于需要手动审查会话录音，评估治疗师忠实度仍然费力。我们提出了一种直接从会话音频和转录中自动时间定位关键PE忠实度元素（识别其开始和停止时间）的方法。我们的方法使用低秩适应（LoRA）微调一个大型预训练的音频-语言模型Qwen2-Audio，以处理聚焦的30秒音频-转录输入窗口。治疗师导向（P1）、想象暴露（P2）和想象后处理（P3）这三个核心协议阶段的忠实度标签通过基于大型语言模型（LLM）的提示生成并由训练有素的评估员验证。该模型被训练用于预测归一化边界偏移量，并由任务特定提示引导的软监督。在包含313个真实PE会话的数据集上，我们最佳配置（LoRA秩8，30秒窗口）在所有任务上实现了5.3秒的平均绝对误差（MAE）。我们进一步分析了窗口大小和LoRA秩的影响，强调了上下文粒度和模型适应的重要性。这项工作引入了一个可扩展的PE疗法忠实度跟踪框架，有望支持临床医生培训、监督和质量保证。", "summary": "本文提出一种新颖的方法，利用LoRA微调大型音频-语言模型Qwen2-Audio，以实现延长暴露（PE）疗法中关键治疗元素（如治疗师导向、想象暴露、想象后处理）的自动时间定位。该方法通过处理30秒的音频-转录输入窗口，并结合LLM生成的标签和软监督训练模型。在313个PE会话数据集上，模型实现了5.3秒的平均绝对误差，为PE疗法的忠实度评估提供了一个可扩展的自动化框架，有望提升临床培训和质量管理效率。", "keywords": "延长暴露疗法, PTSD, 音频-语言模型, LoRA, 时间定位, 治疗忠实度", "comments": "这项工作创新性地将大型音频-语言模型与LoRA技术结合应用于心理治疗领域，实现了PE疗法中关键元素的时间定位自动化，极大地减轻了手动评估的负担。其可扩展性框架有望在临床实践中发挥重要作用，提升治疗质量和培训效率。"}}
{"id": "2506.09225", "title": "Near-Field Sensing Enabled Predictive Beamforming: Fundamentals, Framework, and Opportunities", "authors": ["Hao Jiang", "Zhaolin Wang", "Yue Liu", "Hyundong Shin", "Arumugam Nallanathan", "Yuanwei Liu"], "summary": "The article proposes a novel near-field predictive beamforming framework for\nhigh-mobility wireless networks. Specifically, due to the spherical waves and\nnon-uniform Doppler frequencies brought by the near-field region, the new\nability of full-dimensional location and velocity sensing is characterized.\nBuilding on this foundation, the near-field predictive beamforming framework is\nproposed to proactively design beamformers for mobility users following\narbitrary trajectories. Compared to the conventional far-field counterpart, the\nnear-field predictive beamforming stands out due to: i) Prior-Knowledge-Free\nPrediction, and ii) Low-Complexity and Generalizable System Design. To realize\nthese advantages, the implementation methods are discussed, followed by a case\nstudy confirming the benefits of the proposed framework. Finally, the article\nhighlights promising research opportunities inspired by the proposed framework.", "comment": "Submitted to IEEE", "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.09225v1", "AI": {"title_translation": "近场感知赋能的预测波束赋形：基本原理、框架与机遇", "tldr": "本文提出了一种用于高移动性无线网络的新型近场预测波束赋形框架，利用近场独特的感知能力实现无先验知识预测和低复杂度设计，并通过案例研究验证了其优势，同时展望了未来研究机会。", "motivation": "针对高移动性无线网络的需求，以及传统远场波束赋形在近场区域因球形波和非均匀多普勒频率而面临的挑战，文章旨在利用近场区域特有的全维度位置和速度感知能力，提出一种能够主动为任意轨迹移动用户设计波束赋形器的方法。", "method": "文章首先描述了近场区域带来的球形波和非均匀多普勒频率所赋予的全维度位置和速度感知新能力。在此基础上，提出了近场预测波束赋形框架，旨在主动为遵循任意轨迹的移动用户设计波束赋形器。为了实现框架的优势，文中讨论了具体的实现方法，并通过案例研究验证了所提出框架的益处。", "result": "所提出的近场预测波束赋形框架相比传统远场对应方案具有显著优势，包括：i) 无先验知识预测能力，以及 ii) 低复杂度且可泛化的系统设计。这些益处已通过案例研究得到证实。", "conclusion": "文章最后强调了受所提出框架启发而产生的有前景的研究机会。", "translation": "本文提出了一种用于高移动性无线网络的新型近场预测波束赋形框架。具体而言，由于近场区域带来的球形波和非均匀多普勒频率，文中描述了其全维度位置和速度感知的新能力。在此基础上，提出了近场预测波束赋形框架，旨在主动为遵循任意轨迹的移动用户设计波束赋形器。与传统的远场对应方案相比，近场预测波束赋形因以下特点而脱颖而出：i) 无先验知识预测，以及 ii) 低复杂度且可泛化的系统设计。为了实现这些优势，文中讨论了实现方法，并通过案例研究证实了所提出框架的益处。最后，本文强调了受所提出框架启发而产生的有前景的研究机会。", "summary": "本文提出了一种创新的近场预测波束赋形框架，专为高移动性无线网络设计。该框架利用近场区域特有的球形波和非均匀多普勒频率所赋予的全维度位置和速度感知能力。与传统远场方案不同，该框架实现了无先验知识预测和低复杂度、可泛化的系统设计，能主动为任意轨迹移动用户设计波束赋形器。文章讨论了实现方法并通过案例研究证实了其优势，并展望了未来的研究方向。", "keywords": "近场感知, 预测波束赋形, 高移动性, 球形波, 无线网络", "comments": "这篇论文在近场通信领域引入了预测波束赋形的概念，具有创新性。其核心价值在于利用近场固有的感知能力克服了传统远场波束赋形在高速移动场景下的局限性，并提出了无先验知识预测和低复杂度设计的优势，这对于未来6G及更高频率的移动通信系统具有重要意义。"}}
{"id": "2506.09377", "title": "An Interpretable Two-Stage Feature Decomposition Method for Deep Learning-based SAR ATR", "authors": ["Chenwei Wang", "Renjie Xu", "Congwen Wu", "Cunyi Yin", "Ziyun Liao", "Deqing Mao", "Sitong Zhang", "Hong Yan"], "summary": "Synthetic aperture radar automatic target recognition (SAR ATR) has seen\nsignificant performance improvements with deep learning. However, the black-box\nnature of deep SAR ATR introduces low confidence and high risks in\ndecision-critical SAR applications, hindering practical deployment. To address\nthis issue, deep SAR ATR should provide an interpretable reasoning basis $r_b$\nand logic $\\lambda_w$, forming the reasoning logic $\\sum_{i} {{r_b^i} \\times\n{\\lambda_w^i}} =pred$ behind the decisions. Therefore, this paper proposes a\nphysics-based two-stage feature decomposition method for interpretable deep SAR\nATR, which transforms uninterpretable deep features into attribute scattering\ncenter components (ASCC) with clear physical meanings. First, ASCCs are\nobtained through a clustering algorithm. To extract independent physical\ncomponents from deep features, we propose a two-stage decomposition method. In\nthe first stage, a feature decoupling and discrimination module separates deep\nfeatures into approximate ASCCs with global discriminability. In the second\nstage, a multilayer orthogonal non-negative matrix tri-factorization (MLO-NMTF)\nfurther decomposes the ASCCs into independent components with distinct physical\nmeanings. The MLO-NMTF elegantly aligns with the clustering algorithms to\nobtain ASCCs. Finally, this method ensures both an interpretable reasoning\nprocess and accurate recognition results. Extensive experiments on four\nbenchmark datasets confirm its effectiveness, showcasing the method's\ninterpretability, robust recognition performance, and strong generalization\ncapability.", "comment": null, "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.09377v1", "AI": {"title_translation": "一种基于深度学习的SAR ATR可解释两阶段特征分解方法", "tldr": "本文提出了一种物理驱动的两阶段特征分解方法，使深度学习SAR ATR模型具有可解释性，并通过实验证明了其有效性、鲁棒性和泛化能力。", "motivation": "深度学习在合成孔径雷达自动目标识别（SAR ATR）中表现出色，但其“黑箱”特性导致决策关键型SAR应用中的低置信度和高风险，阻碍了实际部署。因此，深度SAR ATR需要提供可解释的推理基础和逻辑。", "method": "本文提出了一种基于物理的两阶段特征分解方法，用于可解释的深度SAR ATR。该方法将不可解释的深度特征转换为具有明确物理意义的属性散射中心分量（ASCC）。首先，通过聚类算法获得ASCCs。为了从深度特征中提取独立的物理分量，提出了一种两阶段分解方法：第一阶段，特征解耦和判别模块将深度特征分离为具有全局判别能力的近似ASCCs；第二阶段，多层正交非负矩阵三因子分解（MLO-NMTF）进一步将ASCCs分解为具有不同物理意义的独立分量。MLO-NMTF与聚类算法优雅地对齐以获得ASCCs。", "result": "在四个基准数据集上进行了广泛的实验，证实了该方法的有效性，展示了其可解释性、鲁棒的识别性能和强大的泛化能力。", "conclusion": "该方法确保了可解释的推理过程和准确的识别结果，有效解决了深度SAR ATR的黑箱问题，并提升了其在实际应用中的部署潜力。", "translation": "合成孔径雷达自动目标识别（SAR ATR）在深度学习的推动下取得了显著的性能提升。然而，深度SAR ATR的“黑箱”特性在决策关键型SAR应用中引入了低置信度和高风险，阻碍了实际部署。为了解决这个问题，深度SAR ATR应该提供可解释的推理基础$r_b$和逻辑$\\lambda_w$，形成决策背后的推理逻辑$\\sum_{i} {{r_b^i} \\times {\\lambda_w^i}} =pred$。因此，本文提出了一种基于物理的两阶段特征分解方法，用于可解释的深度SAR ATR，该方法将不可解释的深度特征转换为具有明确物理意义的属性散射中心分量（ASCC）。首先，通过聚类算法获得ASCCs。为了从深度特征中提取独立的物理分量，我们提出了一种两阶段分解方法。在第一阶段，特征解耦和判别模块将深度特征分离为具有全局判别能力的近似ASCCs。在第二阶段，多层正交非负矩阵三因子分解（MLO-NMTF）进一步将ASCCs分解为具有不同物理意义的独立分量。MLO-NMTF与聚类算法优雅地对齐以获得ASCCs。最后，该方法确保了可解释的推理过程和准确的识别结果。在四个基准数据集上进行了广泛的实验，证实了该方法的有效性，展示了该方法的可解释性、鲁棒的识别性能和强大的泛化能力。", "summary": "本文提出了一种针对深度学习SAR ATR的可解释两阶段特征分解方法，旨在解决其“黑箱”问题。该方法将深度特征分解为具有物理意义的属性散射中心分量（ASCC）。第一阶段通过特征解耦和判别模块提取近似ASCCs，第二阶段则利用多层正交非负矩阵三因子分解（MLO-NMTF）进一步分解为独立的物理分量。实验证明，该方法不仅提供了可解释的推理过程，还保持了准确的识别性能，并展现出良好的泛化能力。", "keywords": "SAR ATR, 可解释性, 特征分解, 深度学习, 属性散射中心分量", "comments": "该论文通过引入物理驱动的两阶段特征分解方法，有效地提升了深度学习SAR ATR模型的可解释性，这在军事和安全等决策关键型应用中具有重要意义。其创新之处在于将不可解释的深度特征转化为具有明确物理意义的组件，并结合了特征解耦和矩阵分解技术。这种方法有望提高用户对模型决策的信任度，促进深度学习技术在SAR ATR领域的实际部署。"}}
{"id": "2506.09185", "title": "Whole-Person Education for AI Engineers", "authors": ["Rubaina Khan", "Tammy Mackenzie", "Sreyoshi Bhaduri", "Animesh Paul", "Branislav Radeljić", "Joshua Owusu Ansah", "Beyza Nur Guler", "Indrani Bhaduri", "Rodney Kimbangu", "Nils Ever Murrugarra Llerena", "Hayoung Shin", "Lilianny Virguez", "Rosa Paccotacya Yanque", "Thomas Mekhaël", "Allen Munoriyarwa", "Leslie Salgado", "Debarati Basu", "Curwyn Mapaling", "Natalie Perez", "Yves Gaudet", "Paula Larrondo"], "summary": "This autoethnographic study explores the need for interdisciplinary education\nspanning both technical and philosophical skills - as such, this study\nleverages whole-person education as a theoretical approach needed in AI\nengineering education to address the limitations of current paradigms that\nprioritize technical expertise over ethical and societal considerations.\nDrawing on a collaborative autoethnography approach of fourteen diverse\nstakeholders, the study identifies key motivations driving the call for change,\nincluding the need for global perspectives, bridging the gap between academia\nand industry, integrating ethics and societal impact, and fostering\ninterdisciplinary collaboration. The findings challenge the myths of\ntechnological neutrality and technosaviourism, advocating for a future where AI\nengineers are equipped not only with technical skills but also with the ethical\nawareness, social responsibility, and interdisciplinary understanding necessary\nto navigate the complex challenges of AI development. The study provides\nvaluable insights and recommendations for transforming AI engineering education\nto ensure the responsible development of AI technologies.", "comment": "conference pre-print, position paper. 21 pages", "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.09185v1", "AI": {"title_translation": "AI工程师的全人教育", "tldr": "本研究通过自民族志方法，探讨了AI工程师全人教育的必要性，以弥补当前教育模式中技术优先、忽视伦理和社会责任的不足。", "motivation": "当前AI工程教育模式过分强调技术专业知识，忽视伦理和社会考量，导致AI工程师缺乏全球视野、跨学科协作能力、伦理意识和社会责任感。因此，需要引入全人教育来解决这些限制，以应对AI发展的复杂挑战。", "method": "采用协作式自民族志研究方法，与十四位不同的利益相关者合作。", "result": "识别了推动变革的关键动机，包括对全球视角的需求、弥合学术界与工业界之间的鸿沟、整合伦理与社会影响以及促进跨学科协作。研究结果挑战了技术中立性和技术救世主的神话。", "conclusion": "AI工程师不仅需要技术技能，还需要伦理意识、社会责任和跨学科理解，以应对AI开发的复杂挑战，并确保AI技术的负责任发展。", "translation": "这项自民族志研究探讨了跨越技术和哲学技能的跨学科教育的必要性——因此，本研究利用全人教育作为人工智能工程教育中所需的理论方法，以解决当前优先考虑技术专长而非伦理和社会考量的范式的局限性。该研究借鉴了十四位不同利益相关者的协作式自民族志方法，确定了推动变革的关键动机，包括对全球视角的需求、弥合学术界与工业界之间的鸿沟、整合伦理与社会影响以及促进跨学科协作。研究结果挑战了技术中立性和技术救世主的神话，倡导人工智能工程师未来不仅要具备技术技能，还要具备导航人工智能开发复杂挑战所需的伦理意识、社会责任和跨学科理解。该研究为改革人工智能工程教育提供了宝贵的见解和建议，以确保人工智能技术的负责任发展。", "summary": "本自民族志研究强调了AI工程教育中全人教育的重要性，旨在纠正当前过分侧重技术而忽视伦理和社会责任的弊端。通过与多方利益相关者合作，研究揭示了全球视野、产学融合、伦理整合及跨学科协作的必要性，并挑战了技术中立等观念。最终倡导培养具备全面技能、伦理意识和社会责任感的AI工程师，以促进AI的负责任发展。", "keywords": "全人教育, AI工程教育, 伦理意识, 跨学科教育, 自民族志", "comments": "这篇论文创新性地将“全人教育”理念引入AI工程领域，挑战了传统上对技术能力单一的强调，为AI工程师的培养提供了更全面、更具社会责任感的视角。其采用的自民族志研究方法也为深入理解AI教育现状提供了独特的视角。"}}
{"id": "2506.09997", "title": "DGS-LRM: Real-Time Deformable 3D Gaussian Reconstruction From Monocular Videos", "authors": ["Chieh Hubert Lin", "Zhaoyang Lv", "Songyin Wu", "Zhen Xu", "Thu Nguyen-Phuoc", "Hung-Yu Tseng", "Julian Straub", "Numair Khan", "Lei Xiao", "Ming-Hsuan Yang", "Yuheng Ren", "Richard Newcombe", "Zhao Dong", "Zhengqin Li"], "summary": "We introduce the Deformable Gaussian Splats Large Reconstruction Model\n(DGS-LRM), the first feed-forward method predicting deformable 3D Gaussian\nsplats from a monocular posed video of any dynamic scene. Feed-forward scene\nreconstruction has gained significant attention for its ability to rapidly\ncreate digital replicas of real-world environments. However, most existing\nmodels are limited to static scenes and fail to reconstruct the motion of\nmoving objects. Developing a feed-forward model for dynamic scene\nreconstruction poses significant challenges, including the scarcity of training\ndata and the need for appropriate 3D representations and training paradigms. To\naddress these challenges, we introduce several key technical contributions: an\nenhanced large-scale synthetic dataset with ground-truth multi-view videos and\ndense 3D scene flow supervision; a per-pixel deformable 3D Gaussian\nrepresentation that is easy to learn, supports high-quality dynamic view\nsynthesis, and enables long-range 3D tracking; and a large transformer network\nthat achieves real-time, generalizable dynamic scene reconstruction. Extensive\nqualitative and quantitative experiments demonstrate that DGS-LRM achieves\ndynamic scene reconstruction quality comparable to optimization-based methods,\nwhile significantly outperforming the state-of-the-art predictive dynamic\nreconstruction method on real-world examples. Its predicted physically grounded\n3D deformation is accurate and can readily adapt for long-range 3D tracking\ntasks, achieving performance on par with state-of-the-art monocular video 3D\ntracking methods.", "comment": "Project page: https://hubert0527.github.io/dgslrm/", "cate": "cs.GR", "url": "http://arxiv.org/abs/2506.09997v1", "AI": {"title_translation": "DGS-LRM：从单目视频实时重建可变形3D高斯", "tldr": "DGS-LRM是首个从单目视频预测可变形3D高斯的前馈方法，实现动态场景实时重建，性能优于现有方法。", "motivation": "现有前馈场景重建模型大多限于静态场景，无法重建移动物体的运动。动态场景重建面临训练数据稀缺、缺乏合适的3D表示和训练范式等挑战。", "method": "引入DGS-LRM，一个前馈方法，通过：增强的大规模合成数据集，包含多视角视频和密集3D场景流监督；逐像素可变形3D高斯表示，易于学习，支持高质量动态视图合成，并实现长距离3D跟踪；大型Transformer网络，实现实时、可泛化的动态场景重建。", "result": "DGS-LRM在动态场景重建质量上与基于优化的方法相当，并且在真实世界示例上显著优于最先进的预测性动态重建方法。其预测的物理接地3D变形准确，可用于长距离3D跟踪任务，性能与最先进的单目视频3D跟踪方法持平。", "conclusion": "DGS-LRM是首个从单目视频预测可变形3D高斯的前馈方法，有效解决了动态场景重建的挑战，实现了实时、高质量的重建和跟踪性能。", "translation": "我们介绍了可变形高斯泼溅大重建模型（DGS-LRM），这是第一个从任何动态场景的单目姿态视频预测可变形3D高斯泼溅的前馈方法。前馈场景重建因其能够快速创建真实世界环境的数字副本而受到广泛关注。然而，大多数现有模型仅限于静态场景，无法重建移动物体的运动。开发用于动态场景重建的前馈模型带来了重大挑战，包括训练数据稀缺以及需要适当的3D表示和训练范式。为了应对这些挑战，我们引入了几个关键的技术贡献：一个增强的大规模合成数据集，包含地面真实多视角视频和密集的3D场景流监督；一个易于学习、支持高质量动态视图合成并实现长距离3D跟踪的逐像素可变形3D高斯表示；以及一个实现实时、可泛化动态场景重建的大型Transformer网络。广泛的定性和定量实验表明，DGS-LRM在动态场景重建质量上与基于优化的方法相当，同时在真实世界示例上显著优于最先进的预测性动态重建方法。其预测的物理接地3D变形准确，并且可以轻松适应长距离3D跟踪任务，实现与最先进的单目视频3D跟踪方法相当的性能。", "summary": "本文提出了DGS-LRM，一种创新的前馈方法，首次实现了从单目视频实时重建可变形3D高斯，用于动态场景。该模型通过大规模合成数据集、新型可变形3D高斯表示和大型Transformer网络，有效解决了现有方法在处理动态场景时的局限性。实验证明，DGS-LRM在动态场景重建质量上媲美优化方法，并在真实世界数据上超越现有预测方法，同时其3D变形预测在长距离跟踪任务中表现出色。", "keywords": "可变形3D高斯, 单目视频, 实时重建, 动态场景, 前馈模型", "comments": "DGS-LRM的创新之处在于它是首个实现从单目视频进行实时可变形3D高斯重建的前馈模型，解决了动态场景重建中的关键挑战。其引入的增强数据集、新的3D表示和Transformer网络是重要的技术贡献，有望推动动态场景理解和重建领域的发展。"}}
{"id": "2506.09404", "title": "Synergizing Reinforcement Learning and Genetic Algorithms for Neural Combinatorial Optimization", "authors": ["Shengda Gu", "Kai Li", "Junliang Xing", "Yifan Zhang", "Jian Cheng"], "summary": "Combinatorial optimization problems are notoriously challenging due to their\ndiscrete structure and exponentially large solution space. Recent advances in\ndeep reinforcement learning (DRL) have enabled the learning heuristics directly\nfrom data. However, DRL methods often suffer from limited exploration and\nsusceptibility to local optima. On the other hand, evolutionary algorithms such\nas Genetic Algorithms (GAs) exhibit strong global exploration capabilities but\nare typically sample inefficient and computationally intensive. In this work,\nwe propose the Evolutionary Augmentation Mechanism (EAM), a general and\nplug-and-play framework that synergizes the learning efficiency of DRL with the\nglobal search power of GAs. EAM operates by generating solutions from a learned\npolicy and refining them through domain-specific genetic operations such as\ncrossover and mutation. These evolved solutions are then selectively reinjected\ninto the policy training loop, thereby enhancing exploration and accelerating\nconvergence. We further provide a theoretical analysis that establishes an\nupper bound on the KL divergence between the evolved solution distribution and\nthe policy distribution, ensuring stable and effective policy updates. EAM is\nmodel-agnostic and can be seamlessly integrated with state-of-the-art DRL\nsolvers such as the Attention Model, POMO, and SymNCO. Extensive results on\nbenchmark problems (e.g., TSP, CVRP, PCTSP, and OP) demonstrate that EAM\nsignificantly improves both solution quality and training efficiency over\ncompetitive baselines.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09404v1", "AI": {"title_translation": "协同强化学习和遗传算法用于神经组合优化", "tldr": "本文提出了进化增强机制（EAM），一个结合深度强化学习（DRL）和遗传算法（GA）优势的通用框架，用于解决组合优化问题，显著提升了解决方案质量和训练效率。", "motivation": "组合优化问题因其离散结构和指数级大的解空间而极具挑战性。深度强化学习（DRL）方法通常面临探索受限和易受局部最优影响的问题。遗传算法（GA）等进化算法虽具有强大的全局探索能力，但通常样本效率低下且计算密集。", "method": "本文提出了进化增强机制（EAM），一个通用且即插即用的框架，协同了DRL的学习效率和GA的全局搜索能力。EAM通过从学习策略生成解决方案，并通过交叉和变异等领域特定遗传操作对其进行精炼。这些进化的解决方案被选择性地重新注入策略训练循环，以增强探索并加速收敛。此外，提供了理论分析，建立了进化解分布和策略分布之间KL散度的上限，确保稳定有效的策略更新。EAM是模型无关的，可与Attention Model、POMO和SymNCO等DRL求解器无缝集成。", "result": "在TSP、CVRP、PCTSP和OP等基准问题上的广泛实验结果表明，EAM在解决方案质量和训练效率方面都显著优于竞争基线。", "conclusion": "EAM成功地将DRL的学习效率与GA的全局搜索能力相结合，为解决组合优化问题提供了一个有效且通用的框架，显著提高了解决方案质量和训练效率。", "translation": "组合优化问题因其离散结构和指数级大的解空间而极具挑战性。深度强化学习（DRL）的最新进展使得直接从数据中学习启发式方法成为可能。然而，DRL方法通常面临探索受限和易受局部最优影响的问题。另一方面，遗传算法（GA）等进化算法展现出强大的全局探索能力，但通常样本效率低下且计算密集。在这项工作中，我们提出了进化增强机制（EAM），这是一个通用且即插即用的框架，它协同了DRL的学习效率和GA的全局搜索能力。EAM通过学习策略生成解决方案，并通过交叉和变异等领域特定遗传操作对其进行精炼。然后，这些进化的解决方案被选择性地重新注入策略训练循环，从而增强探索并加速收敛。我们进一步提供了理论分析，建立了进化解分布和策略分布之间KL散度的上限，确保稳定有效的策略更新。EAM是模型无关的，可以与Attention Model、POMO和SymNCO等最先进的DRL求解器无缝集成。在基准问题（例如，TSP、CVRP、PCTSP和OP）上的广泛结果表明，EAM在解决方案质量和训练效率方面都显著优于竞争基线。", "summary": "本文提出了一种名为进化增强机制（EAM）的通用且即插即用框架，旨在通过结合深度强化学习（DRL）的学习效率和遗传算法（GA）的全局搜索能力来解决具有挑战性的组合优化问题。EAM通过从学习策略生成并利用遗传操作精炼解决方案，然后将这些进化的解选择性地重新注入策略训练循环，从而增强探索并加速收敛。该框架还包含理论分析以确保稳定的策略更新。实验证明，EAM在多个基准问题上显著提升了解决方案质量和训练效率。", "keywords": "组合优化, 强化学习, 遗传算法, 神经组合优化, 进化增强机制", "comments": "EAM的创新之处在于其“即插即用”的通用性，以及通过将GA的全局探索能力有效融入DRL训练循环中，克服了DRL在组合优化中常见的局部最优和探索不足问题。理论分析也为其稳定性提供了保证，使其成为解决复杂组合优化问题的一个重要进展。"}}
{"id": "2506.09260", "title": "ThinkQE: Query Expansion via an Evolving Thinking Process", "authors": ["Yibin Lei", "Tao Shen", "Andrew Yates"], "summary": "Effective query expansion for web search benefits from promoting both\nexploration and result diversity to capture multiple interpretations and facets\nof a query. While recent LLM-based methods have improved retrieval performance\nand demonstrate strong domain generalization without additional training, they\noften generate narrowly focused expansions that overlook these desiderata. We\npropose ThinkQE, a test-time query expansion framework addressing this\nlimitation through two key components: a thinking-based expansion process that\nencourages deeper and comprehensive semantic exploration, and a\ncorpus-interaction strategy that iteratively refines expansions using retrieval\nfeedback from the corpus. Experiments on diverse web search benchmarks (DL19,\nDL20, and BRIGHT) show ThinkQE consistently outperforms prior approaches,\nincluding training-intensive dense retrievers and rerankers.", "comment": null, "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.09260v1", "AI": {"title_translation": "ThinkQE：通过演化思维过程进行查询扩展", "tldr": "ThinkQE是一个新的查询扩展框架，它通过思维过程和语料库交互来解决现有LLM方法在探索和多样性方面的局限性，并在网络搜索基准测试中表现出色。", "motivation": "当前的基于LLM的查询扩展方法虽然提高了检索性能并具有很强的领域泛化能力，但它们通常会生成狭隘的扩展，忽略了查询解释和结果多样性的重要性。", "method": "本文提出了ThinkQE，一个测试时查询扩展框架。它包含两个关键组件：一个基于思维的扩展过程，鼓励更深入和全面的语义探索；以及一个语料库交互策略，利用来自语料库的检索反馈迭代地优化扩展。", "result": "在多个网络搜索基准测试（DL19、DL20和BRIGHT）上的实验表明，ThinkQE始终优于包括训练密集型密集检索器和重排器在内的现有方法。", "conclusion": "ThinkQE通过其独特的思维过程和语料库交互策略，有效地解决了现有LLM查询扩展方法在探索和多样性方面的不足，并在实际网络搜索任务中取得了显著的性能提升。", "translation": "**论文题目：** ThinkQE：通过演化思维过程进行查询扩展\n\n**摘要：**\n有效的网络搜索查询扩展受益于同时促进探索和结果多样性，以捕捉查询的多种解释和方面。虽然最近基于大型语言模型（LLM）的方法提高了检索性能，并在无需额外训练的情况下展现出强大的领域泛化能力，但它们通常生成狭隘的扩展，忽略了这些期望。我们提出了ThinkQE，一个测试时查询扩展框架，通过两个关键组件来解决这一限制：一个基于思维的扩展过程，鼓励更深入和全面的语义探索；以及一个语料库交互策略，利用来自语料库的检索反馈迭代地优化扩展。在各种网络搜索基准测试（DL19、DL20和BRIGHT）上的实验表明，ThinkQE始终优于现有方法，包括训练密集型密集检索器和重排器。", "summary": "ThinkQE是一种新的测试时查询扩展框架，旨在解决现有基于LLM的方法在生成查询扩展时缺乏探索性和多样性的问题。它通过引入一个鼓励深度语义探索的思维过程和一个利用检索反馈迭代优化扩展的语料库交互策略来实现。实验证明，ThinkQE在多个网络搜索基准上均优于包括训练密集型模型在内的现有方法。", "keywords": "查询扩展, LLM, 网络搜索, 语义探索, 检索反馈", "comments": "ThinkQE的创新之处在于其结合了“思维过程”和“语料库交互”来动态优化查询扩展，这对于提高LLM在实际检索任务中的表现具有重要意义。它在无需额外训练的情况下超越了训练密集型模型，显示出其强大的泛化能力和实用价值。"}}
{"id": "2506.09309", "title": "A discontinuous Galerkin plane wave neural network method for Helmholtz equation and Maxwell's equations", "authors": ["Long Yuan", "Menghui Wu", "Qiya Hu"], "summary": "In this paper we propose a discontinuous Galerkin plane wave neural network\n(DGPWNN) method for approximately solving Helmholtz equation and Maxwell's\nequations. In this method, we define an elliptic-type variational problem as in\nthe plane wave least square method with $h-$refinement and introduce the\nadaptive construction of recursively augmented discontinuous Galerkin subspaces\nwhose basis functions are realizations of element-wise neural network functions\nwith $hp-$refinement, where the activation function is chosen as a\ncomplex-valued exponential function like the plane wave function.\n  A sequence of basis functions approaching the unit residuals are recursively\ngenerated by iteratively solving quasi-maximization problems associated with\nthe underlying residual functionals and the intersection of the closed unit\nball and discontinuous plane wave neural network spaces. The convergence\nresults of the DGPWNN method are established without the assumption on the\nboundedness of the neural network parameters. Numerical experiments confirm the\neffectiveness of the proposed method.", "comment": "31 pages", "cate": "math.NA", "url": "http://arxiv.org/abs/2506.09309v1", "AI": {"title_translation": "Helmholtz方程和麦克斯韦方程的间断Galerkin平面波神经网络方法", "tldr": "本文提出了一种间断Galerkin平面波神经网络(DGPWNN)方法，用于近似求解亥姆霍兹方程和麦克斯韦方程。该方法利用神经网络函数作为基函数，自适应地构建子空间，并在理论上证明了收敛性。", "motivation": "解决亥姆霍兹方程和麦克斯韦方程的近似求解问题。", "method": "本文提出了一种间断Galerkin平面波神经网络(DGPWNN)方法。该方法定义了一个椭圆型变分问题，并引入了自适应构建的递归增强间断Galerkin子空间，其基函数是单元级神经网络函数的实现，激活函数选择为复值指数函数。通过迭代求解与残差泛函和间断平面波神经网络空间交集相关的准最大化问题，递归生成逼近单位残差的基函数序列。", "result": "建立了DGPWNN方法的收敛性结果，且无需假设神经网络参数的有界性。数值实验证实了所提方法的有效性。", "conclusion": "DGPWNN方法是一种有效且收敛的求解亥姆霍兹方程和麦克斯韦方程的近似方法。", "translation": "在本文中，我们提出了一种用于近似求解亥姆霍兹方程和麦克斯韦方程的间断Galerkin平面波神经网络 (DGPWNN) 方法。在该方法中，我们像平面波最小二乘法一样定义了一个椭圆型变分问题，并引入了递归增强间断Galerkin子空间的自适应构建，其基函数是单元级神经网络函数的实现，其中激活函数选择为像平面波函数一样的复值指数函数。通过迭代求解与底层残差泛函以及闭合单位球和间断平面波神经网络空间的交集相关的准最大化问题，递归生成一系列逼近单位残差的基函数。DGPWNN方法的收敛性结果在不假设神经网络参数有界性的情况下得以建立。数值实验证实了所提出方法的有效性。", "summary": "本文提出了一种间断Galerkin平面波神经网络（DGPWNN）方法，用于近似求解亥姆霍兹方程和麦克斯韦方程。该方法通过构建基于复值指数激活函数的单元级神经网络函数作为基函数，自适应地生成递归增强的间断Galerkin子空间。理论上，该方法在不要求神经网络参数有界性的前提下证明了收敛性，并通过数值实验验证了其有效性。", "keywords": "间断Galerkin方法, 平面波神经网络, 亥姆霍兹方程, 麦克斯韦方程, 数值方法", "comments": "该论文将间断Galerkin方法与神经网络相结合，提出了一种新颖的数值求解方法。其创新之处在于利用神经网络函数的自适应特性来构建基函数，并解决了传统方法中可能存在的参数有界性假设问题，为复杂电磁场问题的求解提供了新的思路和工具。"}}
{"id": "2506.09448", "title": "OWSM-Biasing: Contextualizing Open Whisper-Style Speech Models for Automatic Speech Recognition with Dynamic Vocabulary", "authors": ["Yui Sudo", "Yusuke Fujita", "Atsushi Kojima", "Tomoya Mizumoto", "Lianbo Liu"], "summary": "Speech foundation models (SFMs), such as Open Whisper-Style Speech Models\n(OWSM), are trained on massive datasets to achieve accurate automatic speech\nrecognition. However, even SFMs struggle to accurately recognize rare and\nunseen words. While contextual biasing (CB) is a promising approach to improve\nrecognition of such words, most CB methods are trained from scratch, resulting\nin lower performance than SFMs due to the lack of pre-trained knowledge. This\npaper integrates an existing CB method with OWSM v3.1 while freezing its\npre-trained parameters. By leveraging the knowledge embedded in SFMs, the\nproposed method enables effective CB while preserving the advantages of SFMs,\neven with a small dataset. Experimental results show that the proposed method\nimproves the biasing word error rate (B-WER) by 11.6 points, resulting in a 0.9\npoint improvement in the overall WER while reducing the real-time factor by\n7.5% compared to the non-biasing baseline on the LibriSpeech 100 test-clean\nset.", "comment": "Accepted to Interspeech 2025", "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.09448v1", "AI": {"title_translation": "OWSM偏置：利用动态词汇上下文化开放式耳语风格语音模型进行自动语音识别", "tldr": "本文提出了一种将上下文偏置方法与OWSM集成的新方法，旨在提高自动语音识别中对稀有和未见词汇的识别精度，即使在小数据集上也能有效提升性能并降低实时因子。", "motivation": "即使是像OWSM这样的语音基础模型（SFMs）在识别稀有和未见词汇时也存在困难。大多数上下文偏置（CB）方法需要从头开始训练，这导致其性能低于SFMs，因为它们缺乏预训练知识。因此，需要一种能够有效整合CB且同时保留SFMs优势的方法。", "method": "本文将一种现有的上下文偏置（CB）方法与OWSM v3.1集成，同时冻结其预训练参数。通过利用SFMs中嵌入的知识，即使在小数据集上，该方法也能实现有效的CB，并保留SFMs的优势。", "result": "实验结果显示，与LibriSpeech 100 test-clean集上的非偏置基线相比，所提出的方法将偏置词错误率（B-WER）提高了11.6点，整体词错误率（WER）提高了0.9点，同时将实时因子降低了7.5%。", "conclusion": "所提出的方法通过将现有上下文偏置方法与OWSM集成并冻结其预训练参数，成功地实现了有效的上下文偏置，同时保留了语音基础模型的优势，即使在小数据集上也能显著提高自动语音识别性能。", "translation": "语音基础模型（SFMs），例如开放式耳语风格语音模型（OWSM），通过大规模数据集训练，以实现准确的自动语音识别。然而，即使是SFMs也难以准确识别稀有和未见词汇。尽管上下文偏置（CB）是改善此类词汇识别的一种有前景的方法，但大多数CB方法需要从头开始训练，由于缺乏预训练知识，导致其性能低于SFMs。本文将一种现有的CB方法与OWSM v3.1集成，同时冻结其预训练参数。通过利用SFMs中嵌入的知识，所提出的方法即使在小数据集上也能实现有效的CB，同时保留SFMs的优势。实验结果显示，与LibriSpeech 100 test-clean集上的非偏置基线相比，所提出的方法将偏置词错误率（B-WER）提高了11.6点，整体词错误率（WER）提高了0.9点，同时将实时因子降低了7.5%。", "summary": "本文旨在解决语音基础模型（SFMs）在自动语音识别（ASR）中识别稀有和未见词汇的挑战。研究人员将一种现有的上下文偏置（CB）方法与Open Whisper-Style Speech Models (OWSM v3.1) 集成，并冻结了OWSM的预训练参数，从而利用了SFMs的深层知识。这种方法即使在小数据集上也能实现有效的CB，同时保持SFMs的优势。实验证明，该方法在LibriSpeech 100 test-clean集上显著提高了偏置词错误率（B-WER）11.6点，整体词错误率（WER）0.9点，并降低了7.5%的实时因子，优于非偏置基线。", "keywords": "自动语音识别, 上下文偏置, 语音基础模型, OWSM, 动态词汇", "comments": "本文的创新之处在于将上下文偏置技术与冻结的预训练语音基础模型（如OWSM）相结合，有效地解决了传统CB方法需要从头训练而性能受限的问题。这种方法能够利用SFMs的强大知识，即使在数据量有限的情况下也能提高对稀有词汇的识别能力，对于实际ASR系统的部署具有重要意义。整体WER提升0.9点虽然不是巨大飞跃，但在复杂ASR任务中仍是值得关注的改进。"}}
{"id": "2506.09562", "title": "TooBadRL: Trigger Optimization to Boost Effectiveness of Backdoor Attacks on Deep Reinforcement Learning", "authors": ["Songze Li", "Mingxuan Zhang", "Oubo Ma", "Kang Wei", "Shouling Ji"], "summary": "Deep reinforcement learning (DRL) has achieved remarkable success in a wide\nrange of sequential decision-making domains, including robotics, healthcare,\nsmart grids, and finance. Recent research demonstrates that attackers can\nefficiently exploit system vulnerabilities during the training phase to execute\nbackdoor attacks, producing malicious actions when specific trigger patterns\nare present in the state observations. However, most existing backdoor attacks\nrely primarily on simplistic and heuristic trigger configurations, overlooking\nthe potential efficacy of trigger optimization. To address this gap, we\nintroduce TooBadRL (Trigger Optimization to Boost Effectiveness of Backdoor\nAttacks on DRL), the first framework to systematically optimize DRL backdoor\ntriggers along three critical axes, i.e., temporal, spatial, and magnitude.\nSpecifically, we first introduce a performance-aware adaptive freezing\nmechanism for injection timing. Then, we formulate dimension selection as a\ncooperative game, utilizing Shapley value analysis to identify the most\ninfluential state variable for the injection dimension. Furthermore, we propose\na gradient-based adversarial procedure to optimize the injection magnitude\nunder environment constraints. Evaluations on three mainstream DRL algorithms\nand nine benchmark tasks show that TooBadRL significantly improves attack\nsuccess rates, while ensuring minimal degradation of normal task performance.\nThese results highlight the previously underappreciated importance of\nprincipled trigger optimization in DRL backdoor attacks. The source code of\nTooBadRL can be found at https://github.com/S3IC-Lab/TooBadRL.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.09562v1", "AI": {"title_translation": "TooBadRL: 触发优化以提升深度强化学习后门攻击的有效性", "tldr": "TooBadRL是一个新框架，通过系统地优化时间、空间和幅度三个关键维度的触发器，显著提高了深度强化学习后门攻击的成功率，同时保持正常任务性能。", "motivation": "现有的深度强化学习后门攻击主要依赖于简单且启发式的触发器配置，忽视了触发器优化的潜在效力。为了解决这一空白，本研究引入了TooBadRL框架。", "method": "TooBadRL是首个系统优化DRL后门触发器的框架，沿着时间、空间和幅度三个关键轴进行。具体来说，首先引入了性能感知自适应冻结机制用于注入时机；然后将维度选择公式化为合作博弈，利用Shapley值分析来识别最具影响力的状态变量作为注入维度；此外，提出了基于梯度的对抗过程，在环境约束下优化注入幅度。", "result": "在三种主流DRL算法和九个基准任务上的评估表明，TooBadRL显著提高了攻击成功率，同时确保了正常任务性能的最小退化。", "conclusion": "这些结果强调了在DRL后门攻击中，原则性触发器优化的重要性，而这在以前并未得到充分重视。", "translation": "深度强化学习（DRL）在机器人、医疗保健、智能电网和金融等广泛的序贯决策领域取得了显著成功。最近的研究表明，攻击者可以在训练阶段有效利用系统漏洞来执行后门攻击，当状态观察中存在特定触发模式时，会产生恶意行为。然而，大多数现有的后门攻击主要依赖于简单且启发式的触发器配置，忽视了触发器优化的潜在效力。为了解决这一空白，我们引入了TooBadRL（触发优化以提升DRL后门攻击的有效性），这是第一个系统地沿着时间、空间和幅度三个关键轴优化DRL后门触发器的框架。具体来说，我们首先引入了一种性能感知自适应冻结机制用于注入时机。然后，我们将维度选择公式化为合作博弈，利用Shapley值分析来识别最具影响力的状态变量作为注入维度。此外，我们提出了一种基于梯度的对抗过程，在环境约束下优化注入幅度。在三种主流DRL算法和九个基准任务上的评估表明，TooBadRL显著提高了攻击成功率，同时确保了正常任务性能的最小退化。这些结果强调了在DRL后门攻击中，原则性触发器优化的重要性，而这在以前并未得到充分重视。TooBadRL的源代码可在https://github.com/S3IC-Lab/TooBadRL找到。", "summary": "TooBadRL是一个针对深度强化学习（DRL）后门攻击的新型框架，旨在通过系统优化触发器来提高攻击效率。该框架解决了现有攻击中触发器配置过于简单的问题，首次在时间、空间和幅度三个维度上对DRL后门触发器进行优化。具体方法包括：采用性能感知自适应冻结机制确定注入时机，利用Shapley值分析选择关键注入维度，以及通过梯度对抗过程优化注入幅度。实验结果表明，TooBadRL显著提升了攻击成功率，同时对正常任务性能影响甚微，强调了触发器优化在DRL后门攻击中的关键作用。", "keywords": "深度强化学习, 后门攻击, 触发优化, TooBadRL, 网络安全", "comments": "TooBadRL的创新之处在于其首次系统地优化了深度强化学习后门攻击的触发器，从时间、空间和幅度三个维度进行了精细化设计。这解决了以往攻击中触发器配置粗糙、效率不高的问题，显著提升了攻击的隐蔽性和成功率。该研究揭示了触发器优化在DRL安全领域的重要性，为未来防御策略的开发提供了新的视角。"}}
{"id": "2506.09406", "title": "Scoop-and-Toss: Dynamic Object Collection for Quadrupedal Systems", "authors": ["Minji Kang", "Chanwoo Baek", "Yoonsang Lee"], "summary": "Quadruped robots have made significant advances in locomotion, extending\ntheir capabilities from controlled environments to real-world applications.\nBeyond movement, recent work has explored loco-manipulation using the legs to\nperform tasks such as pressing buttons or opening doors. While these efforts\ndemonstrate the feasibility of leg-based manipulation, most have focused on\nrelatively static tasks. In this work, we propose a framework that enables\nquadruped robots to collect objects without additional actuators by leveraging\nthe agility of their legs. By attaching a simple scoop-like add-on to one leg,\nthe robot can scoop objects and toss them into a collection tray mounted on its\nback. Our method employs a hierarchical policy structure comprising two expert\npolicies-one for scooping and tossing, and one for approaching object\npositions-and a meta-policy that dynamically switches between them. The expert\npolicies are trained separately, followed by meta-policy training for\ncoordinated multi-object collection. This approach demonstrates how quadruped\nlegs can be effectively utilized for dynamic object manipulation, expanding\ntheir role beyond locomotion.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.09406v1", "AI": {"title_translation": "铲取抛掷：四足系统动态物体收集", "tldr": "该研究提出了一种利用四足机器人腿部敏捷性进行动态物体收集的方法，通过简单的附加装置和分层策略实现物体铲取和抛掷。", "motivation": "尽管四足机器人在运动方面取得了显著进展，并且最近的工作探索了腿部操作，但大多数都集中在相对静态的任务上。本研究的动机是使四足机器人能够利用其腿部进行动态物体收集，而无需额外的执行器。", "method": "该方法通过在一条腿上安装一个简单的铲状附加装置，使机器人能够铲取物体并将其抛入背部的收集盘中。它采用分层策略结构，包括两个专家策略（一个用于铲取和抛掷，一个用于接近物体位置）和一个动态切换它们的元策略。专家策略单独训练，然后进行元策略训练以实现协调的多物体收集。", "result": "该方法展示了四足机器人腿部可以有效地用于动态物体操作。", "conclusion": "四足机器人腿部可以有效地用于动态物体操作，扩展了它们除了运动之外的作用。", "translation": "四足机器人在运动方面取得了显著进展，将其能力从受控环境扩展到实际应用。除了运动，最近的工作探索了利用腿部进行运动操作，以执行诸如按下按钮或打开门之类的任务。尽管这些努力证明了基于腿部操作的可行性，但大多数都集中在相对静态的任务上。在这项工作中，我们提出了一种框架，该框架使四足机器人能够通过利用其腿部的敏捷性来收集物体，而无需额外的执行器。通过在一条腿上连接一个简单的铲状附加装置，机器人可以铲取物体并将其抛入其背部安装的收集盘中。我们的方法采用分层策略结构，包括两个专家策略——一个用于铲取和抛掷，一个用于接近物体位置——以及一个在它们之间动态切换的元策略。专家策略单独训练，然后进行元策略训练以实现协调的多物体收集。这种方法展示了如何有效利用四足机器人腿部进行动态物体操作，扩展了它们除了运动之外的作用。", "summary": "本研究提出了一种“铲取抛掷”框架，使四足机器人能够通过利用其腿部敏捷性进行动态物体收集，无需额外执行器。通过在一条腿上附加一个铲状装置，机器人可以铲取物体并将其抛入背部的收集盘。该方法采用分层策略结构，包含用于铲取/抛掷和接近物体的专家策略，以及一个动态切换的元策略，从而实现多物体收集并扩展了四足机器人在动态操作中的作用。", "keywords": "四足机器人, 动态物体收集, 铲取抛掷, 腿部操作, 分层策略", "comments": "这项研究的创新之处在于，它通过利用四足机器人腿部的固有敏捷性，并结合简单的附加装置和分层策略，实现了动态物体收集，而无需复杂的额外执行器。这扩展了四足机器人在非静态任务中的应用潜力，并为未来多功能机器人设计提供了新思路。该方法展示了对现有机器人硬件的巧妙利用，提高了其任务执行的灵活性。"}}
{"id": "2506.09106", "title": "Bias Analysis in Unconditional Image Generative Models", "authors": ["Xiaofeng Zhang", "Michelle Lin", "Simon Lacoste-Julien", "Aaron Courville", "Yash Goyal"], "summary": "The widespread adoption of generative AI models has raised growing concerns\nabout representational harm and potential discriminatory outcomes. Yet, despite\ngrowing literature on this topic, the mechanisms by which bias emerges -\nespecially in unconditional generation - remain disentangled. We define the\nbias of an attribute as the difference between the probability of its presence\nin the observed distribution and its expected proportion in an ideal reference\ndistribution. In our analysis, we train a set of unconditional image generative\nmodels and adopt a commonly used bias evaluation framework to study bias shift\nbetween training and generated distributions. Our experiments reveal that the\ndetected attribute shifts are small. We find that the attribute shifts are\nsensitive to the attribute classifier used to label generated images in the\nevaluation framework, particularly when its decision boundaries fall in\nhigh-density regions. Our empirical analysis indicates that this classifier\nsensitivity is often observed in attributes values that lie on a spectrum, as\nopposed to exhibiting a binary nature. This highlights the need for more\nrepresentative labeling practices, understanding the shortcomings through\ngreater scrutiny of evaluation frameworks, and recognizing the socially complex\nnature of attributes when evaluating bias.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09106v1", "AI": {"title_translation": "无条件图像生成模型中的偏差分析", "tldr": "研究发现无条件图像生成模型中的属性偏差转移很小，但评估结果对属性分类器敏感，尤其是在处理连续性属性时，这强调了改进标注和评估框架的必要性。", "motivation": "广泛采用的生成式AI模型引发了对表征性危害和潜在歧视结果的担忧。尽管已有大量相关文献，但偏差产生的机制（尤其是在无条件生成中）仍未被充分理解。", "method": "定义属性偏差为观察分布中属性存在概率与理想参考分布中预期比例的差异。训练了一组无条件图像生成模型，并采用常用偏差评估框架研究训练分布和生成分布之间的偏差转移。", "result": "检测到的属性转移很小。属性转移对评估框架中用于标注生成图像的属性分类器敏感，尤其当其决策边界落在高密度区域时。这种分类器敏感性常出现在连续性而非二元性属性值上。", "conclusion": "强调需要更具代表性的标注实践，通过更严格审查评估框架来理解其缺陷，并在评估偏差时认识到属性的社会复杂性。", "translation": "生成式AI模型的广泛应用引发了对表征性危害和潜在歧视结果日益增长的担忧。然而，尽管关于这一主题的文献不断增加，但偏差产生的机制——尤其是在无条件生成中——仍未被充分理解。我们将属性偏差定义为观察分布中属性存在概率与其在理想参考分布中预期比例之间的差异。在我们的分析中，我们训练了一组无条件图像生成模型，并采用常用的偏差评估框架来研究训练分布和生成分布之间的偏差转移。我们的实验表明，检测到的属性转移很小。我们发现属性转移对评估框架中用于标注生成图像的属性分类器敏感，尤其当其决策边界落在高密度区域时。我们的实证分析表明，这种分类器敏感性通常在处于光谱上的属性值中观察到，而不是表现出二元性质。这突出了需要更具代表性的标注实践，通过更严格审查评估框架来理解其缺陷，并在评估偏差时认识到属性的社会复杂性。", "summary": "本文研究了无条件图像生成模型中的偏差。作者定义了属性偏差，并训练模型以评估训练和生成分布之间的偏差转移。研究发现，属性转移虽然不大，但对用于评估的属性分类器高度敏感，特别是在处理连续性属性时。这表明在评估生成模型偏差时，需要改进标注方法、严格审查评估框架并考虑属性的社会复杂性。", "keywords": "图像生成模型, 偏差分析, 无条件生成, 属性分类器, 表征性危害", "comments": "这篇论文在生成式AI模型偏差分析方面具有重要意义，尤其关注了无条件生成中的偏差机制。其创新之处在于揭示了评估框架中属性分类器的敏感性，特别是对于连续性属性。这为未来偏差评估方法的改进提供了关键洞察，强调了在实践中需要更细致和全面的方法来处理属性标注和评估。"}}
{"id": "2506.09659", "title": "Intent Factored Generation: Unleashing the Diversity in Your Language Model", "authors": ["Eltayeb Ahmed", "Uljad Berdica", "Martha Elliott", "Danijela Horak", "Jakob N. Foerster"], "summary": "Obtaining multiple meaningfully diverse, high quality samples from Large\nLanguage Models for a fixed prompt remains an open challenge. Current methods\nfor increasing diversity often only operate at the token-level, paraphrasing\nthe same response. This is problematic because it leads to poor exploration on\nreasoning problems and to unengaging, repetitive conversational agents. To\naddress this we propose Intent Factored Generation (IFG), factorising the\nsampling process into two stages. First, we sample a semantically dense intent,\ne.g., a summary or keywords. Second, we sample the final response conditioning\non both the original prompt and the intent from the first stage. This allows us\nto use a higher temperature during the intent step to promote conceptual\ndiversity, and a lower temperature during the final generation to ensure the\noutputs are coherent and self-consistent. Additionally, we find that prompting\nthe model to explicitly state its intent for each step of the chain-of-thought\nbefore generating the step is beneficial for reasoning tasks. We demonstrate\nour method's effectiveness across a diverse set of tasks. We show this method\nimproves both pass@k and Reinforcement Learning from Verifier Feedback on maths\nand code tasks. For instruction-tuning, we combine IFG with Direct Preference\nOptimisation to increase conversational diversity without sacrificing reward.\nFinally, we achieve higher diversity while maintaining the quality of\ngenerations on a general language modelling task, using a new dataset of reader\ncomments and news articles that we collect and open-source. In summary, we\npresent a simple method of increasing the sample diversity of LLMs while\nmaintaining performance. This method can be implemented by changing the prompt\nand varying the temperature during generation, making it easy to integrate into\nmany algorithms for gains across various applications.", "comment": null, "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.09659v1", "AI": {"title_translation": "意图分解生成：释放语言模型的多样性", "tldr": "本文提出了一种名为意图分解生成（IFG）的新方法，通过将采样过程分解为两个阶段（首先采样语义意图，然后生成最终响应）来提高大型语言模型（LLM）输出的多样性，同时保持质量，并在推理、对话和通用语言建模等多种任务中展现了有效性。", "motivation": "从大型语言模型（LLM）中为固定提示获取多个有意义的、多样化的高质量样本仍然是一个开放的挑战。当前增加多样性的方法通常只在token级别操作，复述相同的响应，这导致在推理问题上探索不足，并产生无趣、重复的对话代理。", "method": "1. 提出意图分解生成（IFG），将采样过程分解为两个阶段：第一阶段，使用较高温度采样语义密集的意图（如摘要或关键词），以促进概念多样性；第二阶段，在原始提示和第一阶段意图的条件下采样最终响应，使用较低温度确保输出连贯自洽。2. 对于推理任务，在生成思维链的每个步骤之前，提示模型明确说明其意图。3. 在指令微调中，将IFG与直接偏好优化（DPO）相结合，以在不牺牲奖励的情况下增加对话多样性。4. 收集并开源了一个包含读者评论和新闻文章的新数据集，用于通用语言建模任务。", "result": "1. 在数学和代码任务上改进了pass@k和来自验证器反馈的强化学习。2. 在指令微调中，与DPO结合后，增加了对话多样性而未牺牲奖励。3. 在通用语言建模任务上，使用新数据集实现了更高的多样性，同时保持了生成质量。4. 在各种任务中都证明了该方法的有效性。", "conclusion": "本文提出了一种简单的方法，即意图分解生成（IFG），可以在保持性能的同时增加LLM的样本多样性。该方法可以通过改变提示和在生成过程中改变温度来实现，使其易于集成到许多算法中，从而在各种应用中获得收益。", "translation": "从大型语言模型中获取针对固定提示的多个有意义的、多样化的高质量样本仍然是一个开放的挑战。目前增加多样性的方法通常只在token级别操作，导致对相同响应的复述。这带来了问题，因为它导致在推理问题上探索不足，并产生无趣、重复的对话代理。为了解决这个问题，我们提出了意图分解生成（IFG），将采样过程分解为两个阶段。首先，我们采样一个语义密集的意图，例如摘要或关键词。其次，我们在原始提示和第一阶段意图的基础上采样最终响应。这使我们能够在意图阶段使用更高的温度来促进概念多样性，并在最终生成阶段使用更低的温度来确保输出连贯和自洽。此外，我们发现，在生成思维链的每个步骤之前，提示模型明确说明其意图对推理任务是有益的。我们证明了我们方法在各种任务中的有效性。我们展示了该方法在数学和代码任务上改进了pass@k和来自验证器反馈的强化学习。对于指令微调，我们将IFG与直接偏好优化相结合，以在不牺牲奖励的情况下增加对话多样性。最后，我们使用我们收集并开源的新数据集（读者评论和新闻文章），在通用语言建模任务上实现了更高的多样性，同时保持了生成质量。总而言之，我们提出了一种简单的方法，可以在保持性能的同时增加LLM的样本多样性。这种方法可以通过改变提示和在生成过程中改变温度来实现，使其易于集成到许多算法中，从而在各种应用中获得收益。", "summary": "本文提出了一种名为意图分解生成（IFG）的新方法，旨在解决大型语言模型（LLM）在生成多样化、高质量输出方面的挑战。IFG将采样过程分为两个阶段：首先，使用较高温度采样语义密集的意图（如摘要或关键词），以促进概念多样性；其次，在原始提示和意图的条件下，使用较低温度采样最终响应，以确保输出的连贯性。实验证明，该方法在数学和代码等推理任务以及对话多样性方面均表现出有效性，同时在包括通用语言建模在内的各种应用中保持了生成质量。该研究还收集并开源了一个新的数据集。IFG是一种简单且易于实现的技巧，通过调整提示和温度即可集成到现有算法中。", "keywords": "大型语言模型, 多样性, 意图分解生成, 采样, 概念多样性", "comments": "这篇论文提出了一种新颖且直观的方法来解决LLM输出缺乏多样性的普遍问题，这对于创意写作、对话系统和复杂推理等应用至关重要。两阶段的“意图分解”是一种优雅的解决方案，它将概念探索与连贯生成分离开来。该方法可以通过简单地改变提示和温度来实现，这使其具有高度的实用性和易于采纳性。在多种任务（数学、代码、对话、通用语言模型）中展示的有效性突出了其广泛的适用性。新数据集的开源也为社区做出了宝贵的贡献。"}}
{"id": "2506.09093", "title": "Merging Smarter, Generalizing Better: Enhancing Model Merging on OOD Data", "authors": ["Bingjie Zhang", "Hongkang Li", "Changlong Shi", "Guowei Rong", "He Zhao", "Dongsheng Wang", "Dandan Guo", "Meng Wang"], "summary": "Multi-task learning (MTL) concurrently trains a model on diverse task\ndatasets to exploit common features, thereby improving overall performance\nacross the tasks. Recent studies have dedicated efforts to merging multiple\nindependent model parameters into a unified model for MTL, thus circumventing\nthe need for training data and expanding the scope of applicable scenarios of\nMTL. However, current approaches to model merging predominantly concentrate on\nenhancing performance within in-domain (ID) datasets, often overlooking their\nefficacy on out-of-domain (OOD) datasets. In this work, we proposed LwPTV\n(Layer-wise Pruning Task Vector) by building a saliency score, measuring the\nredundancy of parameters in task vectors. Designed in this way ours can achieve\nmask vector for each task and thus perform layer-wise pruning on the task\nvectors, only keeping the pre-trained model parameters at the corresponding\nlayer in merged model. Owing to its flexibility, our method can be seamlessly\nintegrated with most of existing model merging methods to improve their\nperformance on OOD tasks. Extensive experiments demonstrate that the\napplication of our method results in substantial enhancements in OOD\nperformance while preserving the ability on ID tasks.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09093v1", "AI": {"title_translation": "更智能地合并，更好地泛化：增强OOD数据上的模型合并", "tldr": "当前模型合并方法主要关注域内性能，但忽略了域外 (OOD) 数据。本文提出了LwPTV (逐层剪枝任务向量)，通过逐层剪枝任务向量来提高OOD性能，且可与现有合并方法无缝集成。", "motivation": "当前模型合并方法主要集中于提高域内 (ID) 数据集的性能，但往往忽视了它们在域外 (OOD) 数据集上的有效性。", "method": "本文提出了LwPTV (Layer-wise Pruning Task Vector)，通过构建一个显著性分数来衡量任务向量中参数的冗余性。该方法为每个任务生成掩码向量，并对任务向量进行逐层剪枝，只保留合并模型中相应层的预训练模型参数。LwPTV可以与大多数现有模型合并方法无缝集成。", "result": "大量的实验表明，应用LwPTV显著增强了模型在OOD任务上的性能，同时保留了在ID任务上的能力。", "conclusion": "本文提出的LwPTV方法通过显著提高模型在域外数据上的泛化能力，有效解决了当前模型合并方法的局限性，使得模型合并更加鲁棒和适用。", "translation": "多任务学习 (MTL) 同时在不同任务数据集上训练一个模型，以利用共同特征，从而提高跨任务的整体性能。最近的研究致力于将多个独立的模型参数合并到一个统一模型中用于 MTL，从而规避了训练数据的需求并扩展了 MTL 的适用场景。然而，当前的模型合并方法主要集中于提高域内 (ID) 数据集的性能，往往忽视了它们在域外 (OOD) 数据集上的有效性。在这项工作中，我们提出了 LwPTV (Layer-wise Pruning Task Vector)，通过构建显著性分数来衡量任务向量中参数的冗余性。通过这种设计，我们的方法可以为每个任务实现掩码向量，从而对任务向量进行逐层剪枝，只保留合并模型中相应层的预训练模型参数。由于其灵活性，我们的方法可以与大多数现有模型合并方法无缝集成，以提高它们在 OOD 任务上的性能。大量的实验表明，我们的方法的应用显著增强了 OOD 性能，同时保留了在 ID 任务上的能力。", "summary": "本文提出了一种名为LwPTV (Layer-wise Pruning Task Vector) 的新方法，旨在提升合并模型在域外 (OOD) 数据集上的泛化能力。鉴于现有模型合并技术主要侧重于域内性能，LwPTV通过构建显著性分数并对任务向量进行逐层剪枝来解决OOD挑战，确保仅保留必要的预训练模型参数。该方法可与现有合并方法无缝集成，并在不损害域内性能的前提下，显著改善OOD性能。", "keywords": "模型合并, 域外数据, 多任务学习, 任务向量, 剪枝", "comments": "该论文通过关注域外泛化能力，解决了模型合并研究中的一个关键空白，这对于数据分布变化常见的实际应用至关重要。所提出的逐层剪枝机制是优化任务向量的创新方式，有望带来更鲁棒和高效的合并模型。其与现有方法集成的灵活性是一个显著优势。"}}
{"id": "2506.09732", "title": "End-to-End Dynamic Metasurface Antenna Wireless System: Prototype, Opportunities, and Challenges", "authors": ["François Yven", "Jean Tapie", "Jérôme Sol", "Philipp del Hougne"], "summary": "Dynamic metasurface antennas (DMAs) are a promising hybrid analog/digital\nbeamforming technology to realize next-generation wireless systems with low\ncost, footprint, and power consumption. The research on DMA-empowered wireless\nsystems is still at an early stage, mostly limited to theoretical studies under\nsimplifying assumptions on the one hand and a few antenna-level experiments on\nthe other hand. Substantial knowledge gaps arise from the lack of complete\nend-to-end DMA-empowered wireless system prototypes. In addition, recently\nunveiled benefits of strong inter-element mutual coupling (MC) in DMAs remain\nuntapped. Here, we demonstrate a K-band prototype of an end-to-end wireless\nsystem based on a DMA with strong inter-element MC. To showcase the flexible\ncontrol over the DMA's radiation pattern, we present an experimental case study\nof simultaneously steering a beam to a desired transmitter and a null to an\nundesired jammer, achieving up to 43~dB discrimination. Using software-defined\nradios, we transmit and receive QPSK OFDM waveforms to evaluate the bit error\nrate. We also discuss algorithmic and technological challenges associated with\nenvisioned future evolutions of our end-to-end testbed and real-life DMA-based\nwireless systems.", "comment": "7 pages, 4 figures, submitted to an IEEE Journal", "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.09732v1", "AI": {"title_translation": "端到端动态超表面天线无线系统：原型、机遇与挑战", "tldr": "本文展示了一个基于动态超表面天线（DMA）的K波段端到端无线系统原型，解决了现有研究中缺乏完整系统原型的问题，并通过实验展示了其灵活的波束控制能力，并讨论了未来的挑战。", "motivation": "动态超表面天线（DMAs）是实现低成本、小尺寸、低功耗下一代无线系统的有前景技术。然而，目前对DMA赋能无线系统的研究仍处于早期阶段，多限于理论研究或少数天线级实验，缺乏完整的端到端DMA赋能无线系统原型，且DMA中强互耦的潜在优势尚未被充分利用。", "method": "本文展示了一个基于具有强单元间互耦的动态超表面天线的K波段端到端无线系统原型。为展示DMA辐射方向图的灵活控制，通过实验案例研究同时将波束指向期望发射器并将零点指向不期望的干扰器。使用软件定义无线电传输和接收QPSK OFDM波形以评估误码率。", "result": "实验案例研究表明，该系统能同时将波束指向期望发射器并将零点指向不期望的干扰器，实现了高达43 dB的区分度。通过软件定义无线电评估了误码率。", "conclusion": "本文展示了一个端到端动态超表面天线无线系统原型，验证了其灵活的波束控制能力。研究还讨论了与该端到端测试平台以及未来实际DMA无线系统发展相关的算法和技术挑战。", "translation": "动态超表面天线（DMAs）是一种有前景的混合模拟/数字波束成形技术，可实现低成本、小尺寸和低功耗的下一代无线系统。对DMA赋能无线系统的研究仍处于早期阶段，一方面大多局限于简化假设下的理论研究，另一方面仅限于少数天线级实验。由于缺乏完整的端到端DMA赋能无线系统原型，导致了大量的知识空白。此外，最近揭示的DMA中强单元间互耦的优势尚未被利用。本文展示了一个基于具有强单元间互耦的DMA的K波段端到端无线系统原型。为了展示对DMA辐射方向图的灵活控制，我们提出了一个实验案例研究，同时将波束转向期望的发射器并将零点转向不期望的干扰器，实现了高达43 dB的区分度。使用软件定义无线电，我们传输和接收QPSK OFDM波形以评估误码率。我们还讨论了与我们设想的端到端测试平台和实际DMA无线系统未来演进相关的算法和技术挑战。", "summary": "本文介绍了一个K波段的端到端动态超表面天线（DMA）无线系统原型，该系统集成了强单元间互耦的DMA，旨在弥补现有研究中缺乏完整系统原型的空白。研究通过实验展示了DMA在波束控制上的灵活性，例如同时实现波束指向和零点压制，并使用软件定义无线电评估了系统性能。文章最后讨论了未来DMA无线系统发展所面临的算法和技术挑战。", "keywords": "动态超表面天线, 端到端系统, 波束成形, 互耦, 无线系统", "comments": "本文通过构建并演示首个K波段端到端动态超表面天线无线系统原型，填补了该领域在完整系统层面研究的空白，尤其强调了强互耦的利用。其创新性在于将理论研究与实际系统相结合，并通过实验验证了DMA在灵活波束控制方面的巨大潜力，为下一代无线通信系统的发展提供了重要的实践基础和方向。"}}
{"id": "2506.09702", "title": "Mapping NVD Records to Their VFCs: How Hard is it?", "authors": ["Huu Hung Nguyen", "Duc Manh Tran", "Yiran Cheng", "Thanh Le-Cong", "Hong Jin Kang", "Ratnadira Widyasari", "Shar Lwin Khin", "Ouh Eng Lieh", "Ting Zhang", "David Lo"], "summary": "Mapping National Vulnerability Database (NVD) records to vulnerability-fixing\ncommits (VFCs) is crucial for vulnerability analysis but challenging due to\nsparse explicit links in NVD references.This study explores this mapping's\nfeasibility through an empirical approach. Manual analysis of NVD references\nshowed Git references enable over 86% success, while non-Git references achieve\nunder 14%. Using these findings, we built an automated pipeline extracting\n31,942 VFCs from 20,360 NVD records (8.7% of 235,341) with 87% precision,\nmainly from Git references. To fill gaps, we mined six external security\ndatabases, yielding 29,254 VFCs for 18,985 records (8.1%) at 88.4% precision,\nand GitHub repositories, adding 3,686 VFCs for 2,795 records (1.2%) at 73%\nprecision. Combining these, we mapped 26,710 unique records (11.3% coverage)\nfrom 7,634 projects, with overlap between NVD and external databases, plus\nunique GitHub contributions. Despite success with Git references, 88.7% of\nrecords remain unmapped, highlighting the difficulty without Git links. This\nstudy offers insights for enhancing vulnerability datasets and guiding future\nautomated security research.", "comment": null, "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.09702v1", "AI": {"title_translation": "将NVD记录映射到其VFCs：有多难？", "tldr": "本研究探讨了将国家漏洞数据库（NVD）记录映射到漏洞修复提交（VFCs）的可行性，发现Git引用是成功的关键，但总体覆盖率仍然很低。", "motivation": "将国家漏洞数据库（NVD）记录映射到漏洞修复提交（VFCs）对于漏洞分析至关重要，但由于NVD引用中显式链接稀疏而具有挑战性。", "method": "本研究采用经验方法探索映射的可行性。首先手动分析NVD引用，然后构建了一个自动化管道，从NVD记录中提取VFCs。为弥补空白，还挖掘了六个外部安全数据库和GitHub仓库。", "result": "手动分析显示Git引用成功率超过86%，非Git引用成功率低于14%。自动化管道从20,360个NVD记录中提取了31,942个VFCs（占235,341个的8.7%），精确度为87%。挖掘外部数据库为18,985个记录（8.1%）提供了29,254个VFCs，精确度为88.4%；GitHub提供了2,795个记录（1.2%）的3,686个VFCs，精确度为73%。综合来看，共映射了26,710个唯一记录（覆盖率11.3%），但仍有88.7%的记录未被映射。", "conclusion": "尽管Git引用取得了成功，但如果没有Git链接，将NVD记录映射到VFCs仍然非常困难，88.7%的记录仍未被映射。本研究为增强漏洞数据集和指导未来自动化安全研究提供了见解。", "translation": "将国家漏洞数据库（NVD）记录映射到漏洞修复提交（VFCs）对于漏洞分析至关重要，但由于NVD引用中显式链接稀疏而具有挑战性。本研究通过经验方法探讨了这种映射的可行性。对NVD引用的手动分析表明，Git引用实现了超过86%的成功率，而非Git引用的成功率低于14%。利用这些发现，我们构建了一个自动化管道，从20,360个NVD记录（占235,341个的8.7%）中提取了31,942个VFCs，精确度为87%，主要来自Git引用。为了弥补空白，我们挖掘了六个外部安全数据库，为18,985个记录（8.1%）产生了29,254个VFCs，精确度为88.4%，并挖掘了GitHub仓库，为2,795个记录（1.2%）额外添加了3,686个VFCs，精确度为73%。结合这些结果，我们从7,634个项目中映射了26,710个唯一记录（覆盖率11.3%），其中NVD和外部数据库之间存在重叠，并有独特的GitHub贡献。尽管Git引用取得了成功，但仍有88.7%的记录未被映射，突显了在没有Git链接的情况下的难度。本研究为增强漏洞数据集和指导未来自动化安全研究提供了见解。", "summary": "本研究探讨了将国家漏洞数据库（NVD）记录映射到漏洞修复提交（VFCs）的挑战与可行性。通过手动分析发现Git引用在映射中发挥关键作用，成功率远高于非Git引用。基于此，研究团队开发了一个自动化管道，并结合外部安全数据库和GitHub数据，成功映射了约11.3%的NVD记录。尽管在Git引用方面取得了进展，但仍有高达88.7%的记录因缺乏Git链接而未能映射，这表明在没有明确Git信息的情况下，映射工作仍然面临巨大挑战。本研究为提升漏洞数据集质量和未来的自动化安全研究提供了重要启示。", "keywords": "NVD, VFCs, 漏洞分析, Git引用, 自动化映射", "comments": "该研究解决了漏洞分析领域的一个重要挑战，即NVD记录与漏洞修复提交的映射问题。其创新点在于通过实证分析量化了Git引用在映射过程中的重要性，并构建了自动化管道以提高效率。然而，研究结果也明确指出，在缺乏Git链接的情况下，映射的难度依然很高，这为未来的研究指明了方向，即如何处理那些没有明确代码仓库引用的漏洞记录。该工作对于提升漏洞数据的可用性和促进自动化安全分析具有重要意义，同时也揭示了当前方法的局限性。"}}
{"id": "2506.09362", "title": "\"I Said Things I Needed to Hear Myself\": Peer Support as an Emotional, Organisational, and Sociotechnical Practice in Singapore", "authors": ["Kellie Yu Hui Sim", "Kenny Tsu Wei Choo"], "summary": "Peer support plays a vital role in expanding access to mental health care by\nproviding empathetic, community-based support outside formal clinical systems.\nAs digital platforms increasingly mediate such support, the design and impact\nof these technologies remain under-examined, particularly in Asian contexts.\nThis paper presents findings from an interview study with 20 peer supporters in\nSingapore, who operate across diverse online, offline, and hybrid environments.\nThrough a thematic analysis, we unpack how participants start, conduct, and\nsustain peer support, highlighting their motivations, emotional labour, and the\nsociocultural dimensions shaping their practices. Building on this grounded\nunderstanding, we surface design directions for culturally responsive digital\ntools that scaffold rather than supplant relational care. Drawing insights from\nqualitative accounts, we offer a situated perspective on how AI might\nresponsibly augment peer support. This research contributes to human-centred\ncomputing by articulating the lived realities of peer supporters and proposing\ndesign implications for trustworthy and context-sensitive AI in mental health.", "comment": null, "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.09362v1", "AI": {"title_translation": "“我说了我需要听的话”：新加坡的同伴支持作为一种情感、组织和社会技术实践", "tldr": "本文研究了新加坡同伴支持者的实践，以指导为心理健康领域设计文化响应型数字工具和负责任的AI。", "motivation": "尽管同伴支持在心理健康护理中很重要，但数字平台介导的同伴支持的设计和影响，尤其是在亚洲背景下，仍未得到充分研究。", "method": "对新加坡20名在不同线上、线下和混合环境中工作的同伴支持者进行了访谈研究，并进行了主题分析。", "result": "揭示了参与者如何开始、进行和维持同伴支持，强调了他们的动机、情感劳动以及塑造其实践的社会文化维度。基于此，提出了文化响应型数字工具的设计方向，并就AI如何负责任地增强同伴支持提供了见解。", "conclusion": "该研究通过阐明同伴支持者的真实生活体验，并为心理健康领域中值得信赖且情境敏感的AI提出设计启示，从而促进了以人为本的计算发展。", "translation": "同伴支持通过在正式临床系统之外提供同理的、基于社区的支持，在扩大心理健康护理的可及性方面发挥着至关重要的作用。随着数字平台越来越多地介导此类支持，这些技术的设计和影响仍未得到充分研究，尤其是在亚洲背景下。本文介绍了对新加坡20名同伴支持者的访谈研究结果，这些支持者在各种线上、线下和混合环境中运作。通过主题分析，我们揭示了参与者如何开始、进行和维持同伴支持，强调了他们的动机、情感劳动以及塑造其实践的社会文化维度。在此扎实理解的基础上，我们提出了文化响应型数字工具的设计方向，这些工具应起到支架作用而非取代关系护理。从定性叙述中汲取见解，我们提供了一个情境化的视角，探讨AI如何负责任地增强同伴支持。这项研究通过阐明同伴支持者的真实生活体验，并为心理健康领域中值得信赖且情境敏感的AI提出设计启示，从而促进了以人为本的计算发展。", "summary": "本文通过对新加坡20名同伴支持者的访谈研究，深入探讨了同伴支持者在数字和混合环境中的实践、动机、情感劳动和社会文化影响。研究旨在为心理健康领域设计文化响应型数字工具和负责任的AI提供指导，以增强而非取代人际关系护理，从而促进以人为本的计算发展。", "keywords": "同伴支持, 心理健康, 数字平台, 新加坡, 人工智能设计", "comments": "这篇论文通过深入的定性研究，揭示了同伴支持在数字时代和特定文化背景下的复杂性。其创新之处在于不仅分析了同伴支持者的实践，更进一步提出了针对心理健康领域AI和数字工具的设计方向，强调了文化敏感性和以人为本的重要性，为未来技术与人类关怀的结合提供了宝贵见解。"}}
{"id": "2506.09523", "title": "Adaptive event-triggered robust tracking control of soft robots", "authors": ["Renjie Ma", "Ziyao Qu", "Zhijian Hu", "Dong Zhao", "Marios M. Polycarpou"], "summary": "Soft robots manufactured with flexible materials can be highly compliant and\nadaptive to their surroundings, which facilitates their application in areas\nsuch as dexterous manipulation and environmental exploration. This paper aims\nat investigating the tracking control problem for soft robots under uncertainty\nsuch as unmodeled dynamics and external disturbance. First, we establish a\nnovel switching function and design the compensated tracking error dynamics by\nvirtue of the command filter. Then, based on the backstepping methodology, the\nvirtual controllers and the adaptive logic estimating the supremum of\nuncertainty impacts are developed for synthesizing an event-triggered control\nstrategy. In addition, the uniformed finite-time stability certification is\nderived for different scenarios of the switching function. Finally, we perform\na case study of a soft robot to illustrate the effectiveness of the proposed\ncontrol algorithm.", "comment": "8 pages, 7 figures", "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.09523v1", "AI": {"title_translation": "软机器人的自适应事件触发鲁棒跟踪控制", "tldr": "本文提出了一种针对软机器人不确定性下的自适应事件触发鲁棒跟踪控制算法，并通过案例研究验证了其有效性。", "motivation": "软机器人因其柔顺性和环境适应性在灵巧操作和环境探索等领域具有应用潜力。然而，在存在未建模动力学和外部干扰等不确定性下，软机器人的跟踪控制问题是一个挑战。", "method": "首先，建立了新颖的切换函数并通过指令滤波器设计了补偿跟踪误差动力学。然后，基于反步法，开发了虚拟控制器和用于估计不确定性影响上界的自适应逻辑，以合成事件触发控制策略。此外，推导了不同切换函数场景下的统一有限时间稳定性证明。", "result": "通过一个软机器人案例研究，证明了所提出的控制算法的有效性。", "conclusion": "本文提出的自适应事件触发鲁棒跟踪控制算法能够有效解决软机器人在不确定性下的跟踪控制问题。", "translation": "用柔性材料制造的软机器人具有高度柔顺性和对环境的适应性，这促进了它们在灵巧操作和环境探索等领域的应用。本文旨在研究软机器人在未建模动力学和外部干扰等不确定性下的跟踪控制问题。首先，我们建立了一个新颖的切换函数，并通过指令滤波器设计了补偿跟踪误差动力学。然后，基于反步法，开发了虚拟控制器和用于估计不确定性影响上界的自适应逻辑，以合成事件触发控制策略。此外，推导了不同切换函数场景下的统一有限时间稳定性证明。最后，我们进行了一个软机器人案例研究，以说明所提出的控制算法的有效性。", "summary": "本文研究了软机器人在未建模动力学和外部干扰等不确定性下的跟踪控制问题。研究人员提出了一种自适应事件触发鲁棒跟踪控制算法，该算法结合了新颖的切换函数、指令滤波器、反步法以及自适应逻辑来估计不确定性影响。该方法还提供了统一的有限时间稳定性证明。通过一个软机器人案例研究，验证了所提出控制算法的有效性。", "keywords": "软机器人, 跟踪控制, 事件触发, 自适应控制, 鲁棒控制", "comments": "该论文创新性地将事件触发控制与自适应反步法结合，解决了软机器人在复杂不确定性环境下的跟踪控制问题，并提供了严格的稳定性证明，对于提升软机器人的实际应用性能具有重要意义。"}}
{"id": "2506.09804", "title": "Regularizing Learnable Feature Extraction for Automatic Speech Recognition", "authors": ["Peter Vieting", "Maximilian Kannen", "Benedikt Hilmes", "Ralf Schlüter", "Hermann Ney"], "summary": "Neural front-ends are an appealing alternative to traditional, fixed feature\nextraction pipelines for automatic speech recognition (ASR) systems since they\ncan be directly trained to fit the acoustic model. However, their performance\noften falls short compared to classical methods, which we show is largely due\nto their increased susceptibility to overfitting. This work therefore\ninvestigates regularization methods for training ASR models with learnable\nfeature extraction front-ends. First, we examine audio perturbation methods and\nshow that larger relative improvements can be obtained for learnable features.\nAdditionally, we identify two limitations in the standard use of SpecAugment\nfor these front-ends and propose masking in the short time Fourier transform\n(STFT)-domain as a simple but effective modification to address these\nchallenges. Finally, integrating both regularization approaches effectively\ncloses the performance gap between traditional and learnable features.", "comment": "Accepted at Interspeech 2025", "cate": "eess.AS", "url": "http://arxiv.org/abs/2506.09804v1", "AI": {"title_translation": "自动语音识别中可学习特征提取的正则化", "tldr": "本文通过正则化方法解决了自动语音识别中可学习特征提取易过拟合的问题，并成功缩小了其与传统方法之间的性能差距。", "motivation": "自动语音识别（ASR）中的神经网络前端特征提取虽然有吸引力，但其性能常不如传统方法，主要原因是更容易过拟合。", "method": "研究了用于训练带可学习特征提取前端的ASR模型的正则化方法，包括：1. 检查音频扰动方法；2. 识别并解决SpecAugment在可学习前端应用中的局限性，提出了短时傅里叶变换（STFT）域的掩蔽方法。", "result": "音频扰动方法对可学习特征带来了更大的相对改进；提出STFT域的掩蔽方法简单有效；整合两种正则化方法后，有效缩小了可学习特征与传统特征之间的性能差距。", "conclusion": "通过应用音频扰动和STFT域掩蔽等正则化方法，可以显著提高自动语音识别中可学习特征提取的性能，使其达到与传统特征提取方法相当的水平。", "translation": "神经网络前端是自动语音识别（ASR）系统中传统固定特征提取管道的一种有吸引力的替代方案，因为它们可以直接训练以适应声学模型。然而，它们的性能通常不如经典方法，我们发现这很大程度上是由于它们更容易过拟合。因此，这项工作研究了用于训练具有可学习特征提取前端的ASR模型的正则化方法。首先，我们检查了音频扰动方法，并表明对于可学习特征可以获得更大的相对改进。此外，我们确定了SpecAugment在这些前端标准使用中的两个局限性，并提出在短时傅里叶变换（STFT）域进行掩蔽作为一种简单但有效的修改来解决这些挑战。最后，整合这两种正则化方法有效地弥补了传统特征和可学习特征之间的性能差距。", "summary": "本文探讨了自动语音识别（ASR）中可学习特征提取前端的正则化方法，以解决其性能不如传统方法且易于过拟合的问题。研究发现，音频扰动方法对可学习特征有显著改进，并提出了在短时傅里叶变换（STFT）域进行掩蔽作为SpecAugment的有效改进。通过整合这些正则化方法，成功地缩小了可学习特征与传统特征之间的性能差距。", "keywords": "自动语音识别, 可学习特征提取, 正则化, 过拟合, 音频扰动", "comments": "本文解决了可学习特征提取在ASR中过拟合的痛点，通过引入和改进正则化技术，成功提升了其性能，使其能够与传统方法媲美。这对于推动ASR领域中端到端学习模型的实际应用具有重要意义，尤其是在特征提取环节。"}}
{"id": "2506.09463", "title": "Efficient Task Graph Scheduling for Parallel QR Factorization in SLSQP", "authors": ["Soumyajit Chatterjee", "Rahul Utkoor", "Uppu Eshwar", "Sathya Peri", "V. Krishna Nandivada"], "summary": "Efficient task scheduling is paramount in parallel programming on multi-core\narchitectures, where tasks are fundamental computational units. QR\nfactorization is a critical sub-routine in Sequential Least Squares Quadratic\nProgramming (SLSQP) for solving non-linear programming (NLP) problems. QR\nfactorization decomposes a matrix into an orthogonal matrix Q and an upper\ntriangular matrix R, which are essential for solving systems of linear\nequations arising from optimization problems. SLSQP uses an in-place version of\nQR factorization, which requires storing intermediate results for the next\nsteps of the algorithm. Although DAG-based approaches for QR factorization are\nprevalent in the literature, they often lack control over the intermediate\nkernel results, providing only the final output matrices Q and R. This\nlimitation is particularly challenging in SLSQP, where intermediate results of\nQR factorization are crucial for back-substitution logic at each iteration. Our\nwork introduces novel scheduling techniques using a two-queue approach to\nexecute the QR factorization kernel effectively. This approach, implemented in\nhigh-level C++ programming language, facilitates compiler optimizations and\nallows storing intermediate results required by back-substitution logic.\nEmpirical evaluations demonstrate substantial performance gains, including a\n10x improvement over the sequential QR version of the SLSQP algorithm.", "comment": null, "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.09463v1", "AI": {"title_translation": "SLSQP中并行QR分解的有效任务图调度", "tldr": "该研究提出了一种新的双队列调度技术，用于SLSQP中并行QR分解，以有效管理中间结果并显著提高性能，实现了10倍的加速。", "motivation": "在多核架构上的并行编程中，高效的任务调度至关重要。QR分解是序列最小二乘二次规划（SLSQP）中解决非线性规划（NLP）问题的关键子程序。现有的基于DAG的QR分解方法通常缺乏对中间核结果的控制，而这些中间结果对于SLSQP中每次迭代的反向替换逻辑至关重要。", "method": "本研究引入了使用双队列方法的新颖调度技术，以有效地执行QR分解内核。该方法使用高级C++编程语言实现，有助于编译器优化并允许存储反向替换逻辑所需的中间结果。", "result": "经验评估表明，性能获得了显著提升，包括比SLSQP算法的顺序QR版本提高了10倍。", "conclusion": "通过采用新颖的双队列调度技术，该研究成功解决了SLSQP中并行QR分解对中间结果控制的挑战，从而显著提升了算法性能，证明了该方法的有效性和重要性。", "translation": "在多核架构上的并行编程中，高效的任务调度至关重要，其中任务是基本的计算单元。QR分解是序列最小二乘二次规划（SLSQP）中解决非线性规划（NLP）问题的关键子程序。QR分解将矩阵分解为正交矩阵Q和上三角矩阵R，这对于解决优化问题中出现的线性方程组至关重要。SLSQP使用QR分解的原位版本，这需要存储中间结果以供算法的后续步骤使用。尽管基于DAG的QR分解方法在文献中很普遍，但它们通常缺乏对中间核结果的控制，仅提供最终输出矩阵Q和R。这种限制在SLSQP中尤其具有挑战性，因为QR分解的中间结果对于每次迭代的反向替换逻辑至关重要。我们的工作引入了使用双队列方法的新颖调度技术，以有效地执行QR分解内核。这种方法以高级C++编程语言实现，有助于编译器优化并允许存储反向替换逻辑所需的中间结果。经验评估表明，性能获得了显著提升，包括比SLSQP算法的顺序QR版本提高了10倍。", "summary": "该论文解决了SLSQP中并行QR分解的效率问题，特别是传统基于DAG方法无法有效管理中间结果的局限性。研究提出了一种新颖的双队列调度方法，以C++实现，能够存储反向替换逻辑所需的中间结果。实验结果表明，该方法显著提高了性能，相比SLSQP算法的顺序QR版本实现了10倍的加速。", "keywords": "QR分解, SLSQP, 并行编程, 任务调度, 多核架构", "comments": "该论文的创新之处在于其针对SLSQP中QR分解对中间结果特定需求的关注，这解决了现有DAG方法的一个关键局限性。提出的双队列调度技术是其核心贡献，它不仅提高了效率，还通过C++实现促进了编译器优化。这项工作对于提升非线性规划求解器的性能具有重要意义。"}}
{"id": "2506.09253", "title": "Development of a Photon-Counting Deadtime Noise Model that Extends Dynamic Range and Resolution in Atmospheric Lidar", "authors": ["Grant J. Kirchhoff", "Matthew Hayman", "Willem J. Marais", "Jeffrey P. Thayer", "Rory A. Barton-Grimley"], "summary": "This work derives and validates a noise model that encapsulates deadtime of\nnon-paralyzable detectors with random photon arrivals to enable advanced\nprocessing, like maximum-likelihood estimation, of high resolution atmospheric\nlidar profiles while accounting for deadtime bias. This estimator was validated\nacross a wide dynamic range at high resolution (4 millimeters in range, 17\nmilliseconds in time). Experiments demonstrate that the noise model outperforms\nthe current state-of-the-art for very short time-of-flight (2 nanoseconds) and\nextended targets (1 microsecond). The proposed noise model also produces\naccurate deadtime correction for very short integration times. This work sets\nthe foundation for further study into accurate retrievals of high flux and\ndynamic atmospheric features, e.g., clouds and aerosol layers.", "comment": null, "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.09253v1", "AI": {"title_translation": "开发了一种光子计数死时间噪声模型，可扩展大气激光雷达的动态范围和分辨率", "tldr": "本文开发并验证了一种新的光子计数死时间噪声模型，显著提高了大气激光雷达在高分辨率和高通量条件下的性能，解决了死时间偏差问题。", "motivation": "当前大气激光雷达在处理高分辨率剖面时，存在死时间偏差问题，影响了数据准确性和动态范围。需要一种能有效处理死时间并扩展系统性能的噪声模型。", "method": "该工作推导并验证了一种噪声模型，该模型封装了非瘫痪探测器随机光子到达的死时间。通过将此模型应用于最大似然估计等高级处理方法，实现了对高分辨率大气激光雷达剖面的死时间偏差校正。", "result": "该模型在宽动态范围和高分辨率（4毫米距离，17毫秒时间）下得到验证。实验表明，该噪声模型在极短的飞行时间（2纳秒）和扩展目标（1微秒）方面优于现有技术，并能对极短的积分时间进行准确的死时间校正。", "conclusion": "这项工作为未来准确反演高通量和动态大气特征（如云和气溶胶层）奠定了基础。", "translation": "这项工作推导并验证了一种噪声模型，该模型封装了非瘫痪探测器随机光子到达的死时间，以实现对高分辨率大气激光雷达剖面的高级处理，例如最大似然估计，同时考虑了死时间偏差。该估计器在宽动态范围和高分辨率（4毫米距离，17毫秒时间）下得到了验证。实验表明，该噪声模型在极短的飞行时间（2纳秒）和扩展目标（1微秒）方面优于现有技术水平。所提出的噪声模型还能对极短的积分时间进行准确的死时间校正。这项工作为进一步准确地检索高通量和动态大气特征（例如云和气溶胶层）奠定了基础。", "summary": "本文推导并验证了一种新的光子计数死时间噪声模型，旨在提高大气激光雷达在高分辨率剖面处理中的性能。该模型能够有效处理非瘫痪探测器的死时间偏差，并已在宽动态范围和高分辨率下得到验证。实验结果显示，该模型在极短飞行时间和扩展目标方面优于现有技术，并能准确校正极短积分时间的死时间，为高通量大气特征的准确反演提供了基础。", "keywords": "光子计数, 死时间, 噪声模型, 大气激光雷达, 动态范围", "comments": "该研究通过提出一种新的死时间噪声模型，有效解决了大气激光雷达在高通量和高分辨率测量中面临的死时间偏差问题，显著提升了数据处理的准确性和系统动态范围。其创新性在于将死时间效应精确建模并应用于先进的数据处理方法，为未来精确探测复杂大气结构提供了重要的技术支撑。"}}
{"id": "2506.09661", "title": "A Cytology Dataset for Early Detection of Oral Squamous Cell Carcinoma", "authors": ["Garima Jain", "Sanghamitra Pati", "Mona Duggal", "Amit Sethi", "Abhijeet Patil", "Gururaj Malekar", "Nilesh Kowe", "Jitender Kumar", "Jatin Kashyap", "Divyajeet Rout", "Deepali", "Hitesh", "Nishi Halduniya", "Sharat Kumar", "Heena Tabassum", "Rupinder Singh Dhaliwal", "Sucheta Devi Khuraijam", "Sushma Khuraijam", "Sharmila Laishram", "Simmi Kharb", "Sunita Singh", "K. Swaminadtan", "Ranjana Solanki", "Deepika Hemranjani", "Shashank Nath Singh", "Uma Handa", "Manveen Kaur", "Surinder Singhal", "Shivani Kalhan", "Rakesh Kumar Gupta", "Ravi. S", "D. Pavithra", "Sunil Kumar Mahto", "Arvind Kumar", "Deepali Tirkey", "Saurav Banerjee", "L. Sreelakshmi"], "summary": "Oral squamous cell carcinoma OSCC is a major global health burden,\nparticularly in several regions across Asia, Africa, and South America, where\nit accounts for a significant proportion of cancer cases. Early detection\ndramatically improves outcomes, with stage I cancers achieving up to 90 percent\nsurvival. However, traditional diagnosis based on histopathology has limited\naccessibility in low-resource settings because it is invasive,\nresource-intensive, and reliant on expert pathologists. On the other hand, oral\ncytology of brush biopsy offers a minimally invasive and lower cost\nalternative, provided that the remaining challenges, inter observer variability\nand unavailability of expert pathologists can be addressed using artificial\nintelligence. Development and validation of robust AI solutions requires access\nto large, labeled, and multi-source datasets to train high capacity models that\ngeneralize across domain shifts. We introduce the first large and multicenter\noral cytology dataset, comprising annotated slides stained with\nPapanicolaou(PAP) and May-Grunwald-Giemsa(MGG) protocols, collected from ten\ntertiary medical centers in India. The dataset is labeled and annotated by\nexpert pathologists for cellular anomaly classification and detection, is\ndesigned to advance AI driven diagnostic methods. By filling the gap in\npublicly available oral cytology datasets, this resource aims to enhance\nautomated detection, reduce diagnostic errors, and improve early OSCC diagnosis\nin resource-constrained settings, ultimately contributing to reduced mortality\nand better patient outcomes worldwide.", "comment": "7 pages, 2 figurs", "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.09661v1", "AI": {"title_translation": "口腔鳞状细胞癌早期检测的细胞学数据集", "tldr": "为了解决口腔鳞状细胞癌早期诊断在资源匮乏地区面临的挑战，本文介绍了首个大型多中心口腔细胞学数据集，旨在推动基于AI的诊断方法。", "motivation": "口腔鳞状细胞癌（OSCC）是全球性的健康负担，早期检测能显著提高生存率。然而，传统的组织病理学诊断方法在资源匮乏地区可及性有限，因为它具有侵入性、资源密集且依赖专家病理学家。口腔细胞学虽然微创且成本较低，但存在观察者间差异和专家病理学家不足的问题。开发和验证强大的AI解决方案需要大型、带标签和多来源的数据集来训练高容量模型，以解决这些挑战并改进诊断。", "method": "本文介绍了首个大型多中心口腔细胞学数据集。该数据集包含经过巴氏染色（PAP）和迈-格伦瓦尔德-吉姆萨染色（MGG）协议染色的注释玻片，这些玻片是从印度十个三级医疗中心收集的。数据集由专家病理学家进行细胞异常分类和检测的标注。", "result": "结果是创建并发布了首个大型多中心口腔细胞学数据集，其中包含来自印度十个三级医疗中心的PAP和MGG染色的注释玻片，并由专家病理学家对细胞异常进行分类和检测。", "conclusion": "该数据集填补了公开可用的口腔细胞学数据集的空白，旨在增强自动化检测、减少诊断错误，并改善资源受限环境下的早期OSCC诊断，最终有助于降低全球死亡率和改善患者预后。", "translation": "口腔鳞状细胞癌（OSCC）是全球主要的健康负担，特别是在亚洲、非洲和南美洲的几个地区，它占癌症病例的很大一部分。早期检测能显著改善结果，I期癌症的生存率高达90%。然而，基于组织病理学的传统诊断方法在资源匮乏地区的可及性有限，因为它具有侵入性、资源密集且依赖专家病理学家。另一方面，刷检活检的口腔细胞学提供了一种微创且成本较低的替代方案，前提是可以通过人工智能解决剩余的挑战，即观察者间变异性和专家病理学家的缺乏。开发和验证强大的AI解决方案需要访问大型、带标签和多来源的数据集，以训练能够泛化跨领域转移的高容量模型。我们引入了第一个大型多中心口腔细胞学数据集，该数据集包含来自印度十个三级医疗中心的经巴氏染色（PAP）和迈-格伦瓦尔德-吉姆萨染色（MGG）协议染色的注释玻片。该数据集由专家病理学家进行细胞异常分类和检测的标注，旨在推进人工智能驱动的诊断方法。通过填补公开可用的口腔细胞学数据集的空白，这一资源旨在增强自动化检测、减少诊断错误，并改善资源受限环境下的早期OSCC诊断，最终有助于降低全球死亡率和改善患者预后。", "summary": "本文介绍了一个针对口腔鳞状细胞癌（OSCC）早期检测的大型多中心口腔细胞学数据集。鉴于OSCC的全球健康负担以及传统诊断方法的局限性，该数据集旨在通过提供AI模型训练所需的标注数据，弥补现有公共数据集的不足。该数据集包含来自印度十个医疗中心的巴氏和MGG染色的注释玻片，由专家病理学家进行细胞异常分类和检测，以期推动AI驱动的诊断方法，从而提高OSCC的早期检测率，减少诊断错误，并最终改善全球患者预后。", "keywords": "口腔鳞状细胞癌, 细胞学, 数据集, 早期检测, 人工智能", "comments": "这篇论文通过发布首个大型多中心口腔细胞学数据集，为AI在OSCC早期诊断领域的应用奠定了重要基础。其创新性在于解决了AI模型开发中数据稀缺的关键瓶颈，特别是从多个中心收集数据有助于提高模型的泛化能力。该数据集对于推动自动化诊断工具的开发至关重要，有望显著改善资源受限地区的医疗可及性和诊断准确性。"}}
{"id": "2506.09472", "title": "Situated Bayes -- Feminist and Pluriversal Perspectives on Bayesian Knowledge", "authors": ["Juni Schindler", "Goda Klumbytė", "Matthew Fuller"], "summary": "This is the introduction and lead article to the Situated Bayes special issue\nof Computational Culture. The article introduces Bayes' Theorem and aspects of\nits contemporary uses, for instance in machine learning. A mathematical\ndiscussion is developed alongside a consideration of Bayes Theorem in relation\nto critical theories of knowledge, specifically the discussion of situated\nknowledge in feminist theories of science, pluriversal knowledge in decolonial\ntheory, and critical approaches to mathematics. We discuss whether there are\npossible resonances between Bayesian mapping of multiple functions and the idea\nof the subjective on the one hand and these theoretical propositions on the\nother and propose further lines of enquiry for future research. In closing the\nintroduction, the contributions to the special issue are briefly described.", "comment": null, "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.09472v1", "AI": {"title_translation": "情境贝叶斯——贝叶斯知识的女性主义和多元宇宙视角", "tldr": "本文介绍了贝叶斯定理，并探讨了其与女性主义情境知识理论和去殖民多元宇宙知识理论的联系，提出了未来的研究方向。", "motivation": "本文旨在探讨贝叶斯定理（尤其是在机器学习等当代应用中）与批判性知识理论之间的关系，特别是科学女性主义理论中的情境知识、去殖民理论中的多元宇宙知识以及对数学的批判性方法。", "method": "文章在数学讨论的基础上，将贝叶斯定理与批判性知识理论联系起来进行探讨。它讨论了贝叶斯对多重函数的映射和主观性概念与这些理论主张之间可能存在的共鸣。", "result": "文章讨论了贝叶斯方法和主观视角与女性主义/去殖民理论主张之间潜在的联系（共鸣）。它还为未来的研究提出了进一步的探究方向。", "conclusion": "本文作为特刊的导言，旨在探索贝叶斯定理与批判性知识理论的交叉点，并概述了这一跨学科领域的未来研究方向。", "translation": "这是《计算文化》特刊“情境贝叶斯”的导言和主打文章。文章介绍了贝叶斯定理及其当代用途的各个方面，例如在机器学习中。在数学讨论的同时，还考虑了贝叶斯定理与批判性知识理论的关系，特别是科学女性主义理论中情境知识的讨论、去殖民理论中多元宇宙知识的讨论以及对数学的批判性方法。我们讨论了贝叶斯对多重函数的映射和主观性概念之间是否存在可能的共鸣，并提出了未来研究的进一步探究方向。在结束导言时，简要描述了特刊的贡献。", "summary": "这篇为“情境贝叶斯”特刊撰写的导言文章，探讨了贝叶斯定理及其在机器学习等领域的现代应用。文章批判性地将贝叶斯定理与女性主义情境知识理论、去殖民多元宇宙知识理论以及批判性数学方法联系起来。它探讨了贝叶斯映射和主观思想与这些批判性理论之间潜在的联系，并为这一跨学科领域提出了未来的研究方向。", "keywords": "贝叶斯定理, 情境知识, 女性主义理论, 去殖民理论, 知识生产", "comments": "本文的创新之处在于其跨学科方法，弥合了贝叶斯定理等数学概念与批判性社会理论（女性主义和去殖民知识理论）之间的鸿沟。这为审视数据驱动领域中广泛使用的计算方法的哲学和社会影响开辟了新途径。其重要性在于促进对知识生产的更批判和细致的理解。"}}
{"id": "2506.09485", "title": "Adv-BMT: Bidirectional Motion Transformer for Safety-Critical Traffic Scenario Generation", "authors": ["Yuxin Liu", "Zhenghao Peng", "Xuanhao Cui", "Bolei Zhou"], "summary": "Scenario-based testing is essential for validating the performance of\nautonomous driving (AD) systems. However, such testing is limited by the\nscarcity of long-tailed, safety-critical scenarios in existing datasets\ncollected in the real world. To tackle the data issue, we propose the Adv-BMT\nframework, which augments real-world scenarios with diverse and realistic\nadversarial interactions. The core component of Adv-BMT is a bidirectional\nmotion transformer (BMT) model to perform inverse traffic motion predictions,\nwhich takes agent information in the last time step of the scenario as input,\nand reconstruct the traffic in the inverse of chronological order until the\ninitial time step. The Adv-BMT framework is a two-staged pipeline: it first\nconducts adversarial initializations and then inverse motion predictions.\nDifferent from previous work, we do not need any collision data for\npretraining, and are able to generate realistic and diverse collision\ninteractions. Our experimental results validate the quality of generated\ncollision scenarios by Adv-BMT: training in our augmented dataset would reduce\nepisode collision rates by 20\\% compared to previous work.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.09485v1", "AI": {"title_translation": "Adv-BMT：用于安全关键交通场景生成的双向运动变换器", "tldr": "Adv-BMT利用双向运动变换器生成多样化、逼真的安全关键交通场景，以解决自动驾驶系统测试中真实世界数据稀缺的问题，无需碰撞数据预训练，能有效降低自动驾驶系统碰撞率。", "motivation": "自动驾驶系统性能验证依赖于场景测试，但现有数据集中缺乏长尾、安全关键的真实世界场景。", "method": "提出Adv-BMT框架，通过双向运动变换器（BMT）模型执行逆向交通运动预测，以最后一个时间步的智能体信息为输入，逆向重建交通场景至初始时间步。Adv-BMT是一个两阶段的流程：首先进行对抗性初始化，然后进行逆向运动预测。", "result": "Adv-BMT生成的碰撞场景质量得到验证：在增强数据集上进行训练可以将碰撞率相比现有方法降低20%。", "conclusion": "Adv-BMT框架能够有效地生成高质量、多样化且逼真的安全关键交通场景，显著提升自动驾驶系统的测试和验证能力，且无需预训练的碰撞数据。", "translation": "场景测试对于验证自动驾驶（AD）系统的性能至关重要。然而，此类测试受到现有真实世界数据集中长尾、安全关键场景稀缺的限制。为了解决数据问题，我们提出了Adv-BMT框架，该框架通过多样化且逼真的对抗性交互来增强真实世界场景。Adv-BMT的核心组件是双向运动变换器（BMT）模型，用于执行逆向交通运动预测，该模型以场景中最后一个时间步的智能体信息作为输入，并以时间倒序重建交通直到初始时间步。Adv-BMT框架是一个两阶段的流程：它首先进行对抗性初始化，然后进行逆向运动预测。与以往工作不同的是，我们不需要任何碰撞数据进行预训练，并且能够生成逼真且多样化的碰撞交互。我们的实验结果验证了Adv-BMT生成碰撞场景的质量：与以往工作相比，在我们的增强数据集上进行训练可以将碰撞率降低20%。", "summary": "Adv-BMT是一个用于生成安全关键交通场景的框架，旨在解决自动驾驶系统测试中真实世界数据稀缺的问题。它通过双向运动变换器（BMT）模型执行逆向交通运动预测，能够从场景的最后一步逆向重建交通。该框架不需要碰撞数据进行预训练，即可生成多样化且逼真的碰撞场景，实验证明在Adv-BMT增强数据集上训练可将碰撞率降低20%。", "keywords": "自动驾驶, 场景生成, 双向运动变换器, 安全关键, 对抗性交互", "comments": "这篇论文的创新点在于提出了Adv-BMT框架，利用双向运动变换器进行逆向交通运动预测，从而生成多样化、逼真的安全关键交通场景，有效地解决了自动驾驶系统测试中长尾场景数据稀缺的问题。其重要性在于无需预训练的碰撞数据即可生成高质量的碰撞场景，这极大地降低了数据采集和标注的成本，并提高了自动驾驶系统测试的效率和安全性。"}}
{"id": "2506.09409", "title": "MAGMaR Shared Task System Description: Video Retrieval with OmniEmbed", "authors": ["Jiaqi Samantha Zhan", "Crystina Zhang", "Shengyao Zhuang", "Xueguang Ma", "Jimmy Lin"], "summary": "Effective video retrieval remains challenging due to the complexity of\nintegrating visual, auditory, and textual modalities. In this paper, we explore\nunified retrieval methods using OmniEmbed, a powerful multimodal embedding\nmodel from the Tevatron 2.0 toolkit, in the context of the MAGMaR shared task.\nEvaluated on the comprehensive MultiVENT 2.0 dataset, OmniEmbed generates\nunified embeddings for text, images, audio, and video, enabling robust\nmultimodal retrieval. By finetuning OmniEmbed with the combined multimodal\ndata--visual frames, audio tracks, and textual descriptions provided in\nMultiVENT 2.0, we achieve substantial improvements in complex, multilingual\nvideo retrieval tasks. Our submission achieved the highest score on the MAGMaR\nshared task leaderboard among public submissions as of May 20th, 2025,\nhighlighting the practical effectiveness of our unified multimodal retrieval\napproach. Model checkpoint in this work is opensourced.", "comment": null, "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.09409v1", "AI": {"title_translation": "MAGMaR 共享任务系统描述：使用 OmniEmbed 进行视频检索", "tldr": "本文介绍了在 MAGMaR 共享任务中，使用 OmniEmbed 模型通过多模态数据微调，实现了目前最佳的视频检索性能。", "motivation": "由于整合视觉、听觉和文本模态的复杂性，有效的视频检索仍然具有挑战性。", "method": "探索使用 Tevatron 2.0 工具包中的多模态嵌入模型 OmniEmbed 进行统一检索。通过 MultiVENT 2.0 数据集，对 OmniEmbed 进行微调，结合视觉帧、音轨和文本描述等多模态数据。", "result": "实现了复杂多语言视频检索任务的显著改进。在 MAGMaR 共享任务排行榜上，在截至 2025 年 5 月 20 日的公开提交中获得了最高分。", "conclusion": "统一多模态检索方法在实际视频检索任务中表现出有效性。", "translation": "有效的视频检索由于整合视觉、听觉和文本模态的复杂性而仍然具有挑战性。在本文中，我们在 MAGMaR 共享任务的背景下，探索使用 Tevatron 2.0 工具包中强大的多模态嵌入模型 OmniEmbed 进行统一检索方法。通过全面的 MultiVENT 2.0 数据集进行评估，OmniEmbed 为文本、图像、音频和视频生成统一的嵌入，从而实现强大的多模态检索。通过使用 MultiVENT 2.0 中提供的组合多模态数据——视觉帧、音轨和文本描述——对 OmniEmbed 进行微调，我们在复杂的多语言视频检索任务中取得了显著改进。截至 2025 年 5 月 20 日，我们的提交在 MAGMaR 共享任务排行榜上获得了所有公开提交中的最高分，突显了我们统一多模态检索方法的实际有效性。这项工作的模型检查点是开源的。", "summary": "本文旨在解决视频检索中多模态数据整合的挑战。作者在 MAGMaR 共享任务中，利用 Tevatron 2.0 工具包中的 OmniEmbed 多模态嵌入模型，通过对 MultiVENT 2.0 数据集中的视觉、听觉和文本数据进行微调，实现了统一的视频检索。实验结果表明，该方法在复杂多语言视频检索任务中取得了显著提升，并在 MAGMaR 共享任务中取得了当前公开提交的最高分，证明了其在实际应用中的有效性。该模型检查点已开源。", "keywords": "视频检索, 多模态嵌入, OmniEmbed, MAGMaR 共享任务, MultiVENT 2.0", "comments": "这项工作展示了在视频检索领域，通过有效整合多模态数据和使用强大的预训练模型（如 OmniEmbed）进行微调，可以显著提升性能。其在共享任务中取得最高分，验证了方法的实用性和竞争力。模型的开源有助于社区进一步研究和应用。"}}
{"id": "2506.09361", "title": "Overcoming logarithmic singularities in the Cahn-Hilliard equation with Flory-Huggins potential: An unconditionally convergent ADMM approach", "authors": ["Ruo Li", "Shengtong Liang", "Zhonghua Qiao"], "summary": "The Cahn-Hilliard equation with Flory-Huggins potential serves as a\nfundamental phase field model for describing phase separation phenomena. Due to\nthe presence of logarithmic singularities at $u=\\pm 1$, the solution $u$ is\nconstrained within the interval $(-1,1)$. While convex splitting schemes are\ncommonly employed to preserve this bound and guarantee unconditional unique\nsolvability, their practical implementation requires solving nonlinear systems\ncontaining singular logarithmic terms at each time step. This introduces\nsignificant challenges in both ensuring convergence of iterative solvers and\nmaintaining the solution bounds throughout the iterations. Existing solvers\noften rely on restrictive conditions -- such as the strict separation property\nor small time step sizes -- to ensure convergence, which can limit their\napplicability. In this work, we introduce a novel iterative solver that is\nspecifically designed for singular nonlinear systems, with the use of a variant\nof the alternating direction method of multipliers (ADMM). By developing a\ntailored variable splitting strategy within the ADMM framework, our method\nefficiently decouples the challenging logarithmic nonlinearity, enabling\neffective handling of singularities. Crucially, we rigorously prove the\nunconditional convergence of our ADMM-based solver, which removes the need for\ntime step constraints or strict separation conditions. This allows us to fully\nleverage the unconditional solvability offered by convex splitting schemes.\nComprehensive numerical experiments demonstrate the superior efficiency and\nrobustness of our ADMM variant, strongly validating both our algorithmic design\nand theoretical results.", "comment": null, "cate": "math.NA", "url": "http://arxiv.org/abs/2506.09361v1", "AI": {"title_translation": "克服Cahn-Hilliard方程中Flory-Huggins势的对数奇异性：一种无条件收敛的ADMM方法", "tldr": "针对Cahn-Hilliard方程中由Flory-Huggins势引起的对数奇异性，本文提出了一种基于ADMM的迭代求解器，通过定制的变量分裂策略实现无条件收敛，提高了计算效率和鲁棒性。", "motivation": "现有求解Cahn-Hilliard方程（含Flory-Huggins势）的方法，在处理对数奇异性时面临挑战，因为需要解决包含奇异对数项的非线性系统，导致迭代求解器难以收敛且难以保持解的界限。此外，现有求解器常依赖于严格条件（如严格分离性质或小时间步长）来确保收敛，这限制了其适用性。", "method": "本文引入了一种专门为奇异非线性系统设计的新型迭代求解器，该求解器是交替方向乘子法（ADMM）的一种变体。通过在ADMM框架内开发定制的变量分裂策略，该方法有效地解耦了具有挑战性的对数非线性项，从而有效处理了奇异性。", "result": "本文严格证明了所提出的基于ADMM的求解器的无条件收敛性，这消除了对时间步长限制或严格分离条件的需要。全面的数值实验证明了该ADMM变体具有优越的效率和鲁棒性。", "conclusion": "本文提出的ADMM求解器能够充分利用凸分裂方案提供的无条件可解性，并且其算法设计和理论结果通过数值实验得到了有力验证。", "translation": "带有Flory-Huggins势的Cahn-Hilliard方程是描述相分离现象的基本相场模型。由于在$u=\\pm 1$处存在对数奇异性，解$u$被限制在区间$(-1,1)$内。虽然凸分裂方案通常用于保持此界限并保证无条件唯一可解性，但其实际实现需要在每个时间步求解包含奇异对数项的非线性系统。这在确保迭代求解器的收敛性和在整个迭代过程中保持解的界限方面引入了重大挑战。现有求解器通常依赖于限制性条件——例如严格分离性质或小时间步长——来确保收敛，这会限制其适用性。在这项工作中，我们引入了一种专门为奇异非线性系统设计的新型迭代求解器，该求解器使用了交替方向乘子法（ADMM）的一种变体。通过在ADMM框架内开发定制的变量分裂策略，我们的方法有效地解耦了具有挑战性的对数非线性，从而有效处理了奇异性。至关重要的是，我们严格证明了我们基于ADMM的求解器的无条件收敛性，这消除了对时间步长限制或严格分离条件的需要。这使我们能够充分利用凸分裂方案提供的无条件可解性。全面的数值实验证明了我们ADMM变体的卓越效率和鲁棒性，有力地验证了我们的算法设计和理论结果。", "summary": "本文针对Cahn-Hilliard方程中Flory-Huggins势引起的对数奇异性问题，提出了一种基于ADMM的无条件收敛迭代求解器。该方法通过定制的变量分裂策略，有效解耦了奇异对数非线性项，从而避免了现有方法对小时间步长或严格分离条件的依赖。理论上，该求解器被证明是无条件收敛的，数值实验也验证了其高效性和鲁棒性，使其能够充分利用凸分裂方案的无条件可解性。", "keywords": "Cahn-Hilliard方程, Flory-Huggins势, 对数奇异性, ADMM, 无条件收敛", "comments": "该论文的创新点在于提出了一种新型的ADMM变体，能够有效地处理Cahn-Hilliard方程中由Flory-Huggins势引起的对数奇异性。其核心贡献在于通过巧妙的变量分裂策略，实现了求解器的无条件收敛，从而克服了现有方法在收敛性和时间步长限制方面的挑战。这对于相场模拟中的数值计算具有重要意义，因为它允许使用更大的时间步长，提高计算效率和适用性。"}}
{"id": "2506.09487", "title": "BemaGANv2: A Tutorial and Comparative Survey of GAN-based Vocoders for Long-Term Audio Generation", "authors": ["Taesoo Park", "Mungwi Jeong", "Mingyu Park", "Narae Kim", "Junyoung Kim", "Mujung Kim", "Jisang Yoo", "Hoyun Lee", "Sanghoon Kim", "Soonchul Kwon"], "summary": "This paper presents a tutorial-style survey and implementation guide of\nBemaGANv2, an advanced GAN-based vocoder designed for high-fidelity and\nlong-term audio generation. Built upon the original BemaGAN architecture,\nBemaGANv2 incorporates major architectural innovations by replacing traditional\nResBlocks in the generator with the Anti-aliased Multi-Periodicity composition\n(AMP) module, which internally applies the Snake activation function to better\nmodel periodic structures. In the discriminator framework, we integrate the\nMulti-Envelope Discriminator (MED), a novel architecture we originally\nproposed, to extract rich temporal envelope features crucial for periodicity\ndetection. Coupled with the Multi-Resolution Discriminator (MRD), this\ncombination enables more accurate modeling of long-range dependencies in audio.\nWe systematically evaluate various discriminator configurations, including MSD\n+ MED, MSD + MRD, and MPD + MED + MRD, using objective metrics (FAD, SSIM,\nPLCC, MCD) and subjective evaluations (MOS, SMOS). This paper also provides a\ncomprehensive tutorial on the model architecture, training methodology, and\nimplementation to promote reproducibility. The code and pre-trained models are\navailable at: https://github.com/dinhoitt/BemaGANv2.", "comment": "11 pages, 7 figures. Survey and tutorial paper. Currently under\n  review at ICT Express as an extended version of our ICAIIC 2025 paper", "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.09487v1", "AI": {"title_translation": "BemaGANv2：一种用于长期音频生成的基于GAN的声码器教程和比较性调查", "tldr": "BemaGANv2是一种先进的基于GAN的声码器，通过引入AMP模块、Snake激活函数和MED判别器等创新，旨在实现高保真和长期音频生成。论文提供了详细的教程和实现指南，并系统评估了不同的判别器配置。", "motivation": "本研究旨在介绍并改进基于GAN的声码器，以实现高保真和长期音频生成。具体来说，它提出了BemaGANv2，一个在原有BemaGAN基础上进行了重大架构创新的模型，旨在更好地建模周期性结构和提取时间包络特征，从而更准确地建模音频中的长距离依赖性。", "method": "本文介绍了BemaGANv2，它在BemaGANv1的基础上进行了改进。生成器中，传统的ResBlocks被替换为抗混叠多周期性组合（AMP）模块，该模块内部应用Snake激活函数以更好地建模周期性结构。判别器框架中，集成了多包络判别器（MED）和多分辨率判别器（MRD），以提取丰富的时域包络特征并更准确地建模音频中的长距离依赖性。论文还提供了模型架构、训练方法和实现的全面教程。", "result": "论文系统评估了各种判别器配置，包括MSD + MED、MSD + MRD和MPD + MED + MRD，使用了客观指标（FAD, SSIM, PLCC, MCD）和主观评估（MOS, SMOS）。具体的评估结果（如哪个配置表现最佳）未在摘要中明确提及，但表明进行了全面的评估以验证其有效性。", "conclusion": "摘要中未明确提及研究结论，但论文旨在提供BemaGANv2的详细介绍、实现指南和系统评估，以促进高保真和长期音频生成。", "translation": "本文提供了BemaGANv2的教程式调查和实现指南，BemaGANv2是一种先进的基于GAN的声码器，专为高保真和长期音频生成而设计。BemaGANv2建立在原始BemaGAN架构之上，通过用抗混叠多周期性组合（AMP）模块取代生成器中的传统ResBlocks，引入了主要的架构创新，该模块内部应用Snake激活函数以更好地建模周期性结构。在判别器框架中，我们集成了我们最初提出的新型架构——多包络判别器（MED），以提取对周期性检测至关重要的丰富时间包络特征。结合多分辨率判别器（MRD），这种组合能够更准确地建模音频中的长距离依赖性。我们使用客观指标（FAD、SSIM、PLCC、MCD）和主观评估（MOS、SMOS）系统评估了各种判别器配置，包括MSD + MED、MSD + MRD和MPD + MED + MRD。本文还提供了关于模型架构、训练方法和实现的全面教程，以促进可重现性。代码和预训练模型可在以下网址获取：https://github.com/dinhoitt/BemaGANv2。", "summary": "BemaGANv2是一种针对高保真和长期音频生成而设计的先进GAN声码器。它通过在生成器中引入抗混叠多周期性组合（AMP）模块（使用Snake激活函数）来更好地建模周期性结构，并在判别器中整合了多包络判别器（MED）和多分辨率判别器（MRD），以捕获时间包络特征并建模长距离依赖性。论文提供了详细的教程和实现指南，并系统评估了不同判别器配置的性能。", "keywords": "GAN声码器, 音频生成, BemaGANv2, AMP模块, MED判别器", "comments": "BemaGANv2的创新点在于其生成器中引入的AMP模块和Snake激活函数，以及判别器中集成MED和MRD，这些都旨在更有效地处理音频的周期性结构和长距离依赖性，这对于高保真和长期音频生成至关重要。论文还提供了详细的教程和实现指南，对于促进研究的可复现性非常有价值。"}}
{"id": "2506.09569", "title": "The Rabin cryptosystem over number fields", "authors": ["Alessandro Cobbe", "Andreas Nickel", "Akay Schuster"], "summary": "We extend Rabin's cryptosystem to general number fields. We show that\ndecryption of a random plaintext is as hard as the integer factorisation\nproblem, provided the modulus in our scheme has been chosen carefully. We\ninvestigate the performance of our new cryptosystem in comparison with the\nclassical Rabin scheme and a more recent version over the Gaussian integers.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.09569v1", "AI": {"title_translation": "数域上的Rabin密码系统", "tldr": "本文将Rabin密码系统推广到一般数域，并证明在特定条件下，其解密难度与整数分解问题相当，同时与现有Rabin方案进行了性能比较。", "motivation": "将Rabin密码系统推广到更一般的数域，以探索其在更广泛数学结构上的应用和安全性。", "method": "通过将Rabin密码系统扩展到一般数域，并选择合适的模数，来构建新的密码方案。同时，对新方案的性能与经典Rabin方案以及高斯整数上的Rabin方案进行了比较研究。", "result": "在仔细选择模数的情况下，新方案中随机明文的解密难度与整数分解问题一样困难。研究还对新密码系统与经典Rabin方案以及高斯整数上的Rabin方案的性能进行了比较。", "conclusion": "本文成功将Rabin密码系统扩展到一般数域，并证明了其安全性（解密难度等同于整数分解问题），同时对其性能进行了评估和比较。", "translation": "我们将Rabin密码系统扩展到一般数域。我们证明，如果我们的方案中的模数选择得当，随机明文的解密难度与整数分解问题一样困难。我们研究了我们新的密码系统与经典Rabin方案以及高斯整数上的一个更近期版本的性能比较。", "summary": "本文将Rabin密码系统推广到一般数域，并指出在模数选择得当的情况下，解密难度等同于整数分解问题，从而确保了其安全性。研究还对该新方案与传统Rabin方案以及高斯整数上的Rabin方案的性能进行了对比分析。", "keywords": "Rabin密码系统, 数域, 整数分解, 公钥密码学, 密码安全", "comments": "本文的创新点在于将Rabin密码系统推广到更广阔的数域上，这为基于数域的密码学研究提供了新的视角。其重要性在于证明了新方案在特定条件下的安全性，等同于整数分解的难度，这为新的公钥密码系统设计提供了理论基础。"}}
{"id": "2506.09422", "title": "Time-Unified Diffusion Policy with Action Discrimination for Robotic Manipulation", "authors": ["Ye Niu", "Sanping Zhou", "Yizhe Li", "Ye Den", "Le Wang"], "summary": "In many complex scenarios, robotic manipulation relies on generative models\nto estimate the distribution of multiple successful actions. As the diffusion\nmodel has better training robustness than other generative models, it performs\nwell in imitation learning through successful robot demonstrations. However,\nthe diffusion-based policy methods typically require significant time to\niteratively denoise robot actions, which hinders real-time responses in robotic\nmanipulation. Moreover, existing diffusion policies model a time-varying action\ndenoising process, whose temporal complexity increases the difficulty of model\ntraining and leads to suboptimal action accuracy. To generate robot actions\nefficiently and accurately, we present the Time-Unified Diffusion Policy\n(TUDP), which utilizes action recognition capabilities to build a time-unified\ndenoising process. On the one hand, we build a time-unified velocity field in\naction space with additional action discrimination information. By unifying all\ntimesteps of action denoising, our velocity field reduces the difficulty of\npolicy learning and speeds up action generation. On the other hand, we propose\nan action-wise training method, which introduces an action discrimination\nbranch to supply additional action discrimination information. Through\naction-wise training, the TUDP implicitly learns the ability to discern\nsuccessful actions to better denoising accuracy. Our method achieves\nstate-of-the-art performance on RLBench with the highest success rate of 82.6%\non a multi-view setup and 83.8% on a single-view setup. In particular, when\nusing fewer denoising iterations, TUDP achieves a more significant improvement\nin success rate. Additionally, TUDP can produce accurate actions for a wide\nrange of real-world tasks.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.09422v1", "AI": {"title_translation": "用于机器人操作的时间统一扩散策略与动作判别", "tldr": "本文提出了时间统一扩散策略（TUDP），通过引入动作判别能力来统一去噪过程，从而提高扩散模型在机器人操作中生成动作的效率和准确性。", "motivation": "现有的基于扩散的策略通常需要大量时间迭代去噪机器人动作，这阻碍了实时响应。此外，它们建模的是时变动作去噪过程，其时间复杂性增加了模型训练难度并导致次优的动作精度。", "method": "本文提出了时间统一扩散策略（TUDP）。一方面，它在动作空间中构建了一个时间统一的速度场，并加入了动作判别信息，通过统一动作去噪的所有时间步，降低了策略学习难度并加速了动作生成。另一方面，它提出了一种动作级训练方法，引入了一个动作判别分支以提供额外的动作判别信息，通过动作级训练，TUDP隐式学习了识别成功动作的能力，从而提高了去噪精度。", "result": "TUDP在RLBench上取得了最先进的性能，在多视角设置下成功率最高达82.6%，在单视角设置下成功率最高达83.8%。特别是在使用更少去噪迭代次数时，TUDP在成功率方面取得了更显著的提升。此外，TUDP可以为各种现实世界任务生成精确的动作。", "conclusion": "时间统一扩散策略（TUDP）通过引入动作判别和时间统一的去噪过程，有效解决了传统扩散策略在机器人操作中效率和准确性方面的问题，并实现了最先进的性能。", "translation": "在许多复杂的场景中，机器人操作依赖于生成模型来估计多个成功动作的分布。由于扩散模型比其他生成模型具有更好的训练鲁棒性，因此它通过成功的机器人演示在模仿学习中表现良好。然而，基于扩散的策略方法通常需要大量时间迭代去噪机器人动作，这阻碍了机器人操作中的实时响应。此外，现有的扩散策略建模的是时变动作去噪过程，其时间复杂性增加了模型训练的难度并导致次优的动作精度。为了高效准确地生成机器人动作，我们提出了时间统一扩散策略（TUDP），它利用动作识别能力来构建时间统一的去噪过程。一方面，我们在动作空间中构建了一个时间统一的速度场，并加入了额外的动作判别信息。通过统一动作去噪的所有时间步，我们的速度场降低了策略学习的难度并加速了动作生成。另一方面，我们提出了一种动作级训练方法，引入了一个动作判别分支以提供额外的动作判别信息。通过动作级训练，TUDP隐式学习了识别成功动作的能力，以获得更好的去噪精度。我们的方法在RLBench上取得了最先进的性能，在多视角设置下成功率最高达82.6%，在单视角设置下成功率最高达83.8%。特别是，当使用更少的去噪迭代次数时，TUDP在成功率方面取得了更显著的提升。此外，TUDP可以为各种现实世界任务生成精确的动作。", "summary": "本文针对机器人操作中扩散模型去噪效率低和精度次优的问题，提出了时间统一扩散策略（TUDP）。TUDP通过构建时间统一的速度场和引入动作判别分支进行动作级训练，显著降低了策略学习难度，加速了动作生成，并提高了去噪精度。实验结果表明，TUDP在RLBench上取得了最先进的成功率，并且在减少去噪迭代次数时表现出更显著的性能提升，能够为多种真实世界任务生成准确动作。", "keywords": "机器人操作, 扩散模型, 时间统一, 动作判别, 模仿学习", "comments": "本文通过提出时间统一的去噪过程和动作判别机制，创新性地解决了扩散模型在机器人操作中实时性和准确性方面的挑战。其核心贡献在于将时间维度统一化，并利用判别信息辅助生成，这为高效、精确的机器人动作生成提供了新的思路，具有重要的实践意义。"}}
{"id": "2506.09109", "title": "CAIRe: Cultural Attribution of Images by Retrieval-Augmented Evaluation", "authors": ["Arnav Yayavaram", "Siddharth Yayavaram", "Simran Khanuja", "Michael Saxon", "Graham Neubig"], "summary": "As text-to-image models become increasingly prevalent, ensuring their\nequitable performance across diverse cultural contexts is critical. Efforts to\nmitigate cross-cultural biases have been hampered by trade-offs, including a\nloss in performance, factual inaccuracies, or offensive outputs. Despite\nwidespread recognition of these challenges, an inability to reliably measure\nthese biases has stalled progress. To address this gap, we introduce CAIRe, a\nnovel evaluation metric that assesses the degree of cultural relevance of an\nimage, given a user-defined set of labels. Our framework grounds entities and\nconcepts in the image to a knowledge base and uses factual information to give\nindependent graded judgments for each culture label. On a manually curated\ndataset of culturally salient but rare items built using language models, CAIRe\nsurpasses all baselines by 28% F1 points. Additionally, we construct two\ndatasets for culturally universal concept, one comprising of T2I-generated\noutputs and another retrieved from naturally occurring data. CAIRe achieves\nPearson's correlations of 0.56 and 0.66 with human ratings on these sets, based\non a 5-point Likert scale of cultural relevance. This demonstrates its strong\nalignment with human judgment across diverse image sources.", "comment": "Preprint, under review", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09109v1", "AI": {"title_translation": "CAIRe：通过检索增强评估进行图像文化归因", "tldr": "本文介绍了CAIRe，一种新的评估指标，旨在解决文本到图像模型中跨文化偏见难以衡量的问题。CAIRe通过将图像实体与知识库关联并进行事实判断来评估图像的文化相关性，在文化敏感数据集上显著优于基线，并与人类判断高度一致。", "motivation": "随着文本到图像模型日益普及，确保其在不同文化背景下的公平表现至关重要。然而，由于缺乏可靠的偏见衡量方法，缓解跨文化偏见的工作（如性能下降、事实不准确或冒犯性输出）一直受阻。本文旨在解决这一测量空白。", "method": "本文引入了CAIRe，一种新颖的评估指标，用于评估给定用户定义标签集的情况下图像的文化相关程度。CAIRe框架将图像中的实体和概念与知识库关联起来，并使用事实信息对每个文化标签给出独立的评分判断。", "result": "1. 在一个使用语言模型构建的手动策划的、具有文化显著性但罕见的物品数据集上，CAIRe超越所有基线28%的F1点。2. 在两个文化通用概念数据集（一个包含T2I生成的输出，另一个从自然发生的数据中检索）上，CAIRe与人类对文化相关性的5点Likert量表评分分别达到0.56和0.66的皮尔逊相关性。", "conclusion": "CAIRe是一种有效且可靠的图像文化相关性评估指标，它能够解决文本到图像模型中跨文化偏见难以衡量的问题，并与人类判断高度一致，从而有助于确保AI生成内容在不同文化背景下的公平性。", "translation": "随着文本到图像模型越来越普及，确保它们在不同文化背景下表现公平至关重要。缓解跨文化偏见的工作受到权衡的阻碍，包括性能下降、事实不准确或产生冒犯性输出。尽管这些挑战已得到广泛认可，但无法可靠地衡量这些偏见阻碍了进展。为了解决这一空白，我们引入了CAIRe，一种新颖的评估指标，用于评估给定用户定义标签集的情况下图像的文化相关程度。我们的框架将图像中的实体和概念与知识库关联起来，并使用事实信息对每个文化标签给出独立的评分判断。在一个使用语言模型构建的手动策划的、具有文化显著性但罕见的物品数据集上，CAIRe超越所有基线28%的F1点。此外，我们构建了两个文化通用概念数据集，一个包含T2I生成的输出，另一个从自然发生的数据中检索。CAIRe在这些数据集上与人类对文化相关性的5点Likert量表评分分别达到0.56和0.66的皮尔逊相关性。这表明它在不同图像来源上与人类判断高度一致。", "summary": "本文提出了CAIRe，一种用于评估图像文化相关性的新指标，旨在解决文本到图像模型中跨文化偏见难以衡量的问题。CAIRe通过将图像内容与知识库关联并进行事实判断来量化文化相关性。实验结果表明，CAIRe在文化敏感数据集上显著优于现有基线，并且在文化通用概念数据集上与人类判断高度一致，证明了其在多种图像来源上的有效性。", "keywords": "文化归因, 图像评估, 文本到图像模型, 跨文化偏见, 检索增强评估", "comments": "CAIRe的创新之处在于其通过检索增强评估来量化图像文化相关性的方法，特别是将图像内容与知识库关联以进行事实判断。这解决了文本到图像模型公平性评估中的一个关键障碍，即缺乏可靠的偏见测量工具。其在文化敏感和通用概念数据集上的出色表现，以及与人类判断的高度一致性，表明了其在促进更公平、更具文化意识的AI生成内容方面的潜力。"}}
{"id": "2506.09977", "title": "How Do People Revise Inconsistent Beliefs? Examining Belief Revision in Humans with User Studies", "authors": ["Stylianos Loukas Vasileiou", "Antonio Rago", "Maria Vanina Martinez", "William Yeoh"], "summary": "Understanding how humans revise their beliefs in light of new information is\ncrucial for developing AI systems which can effectively model, and thus align\nwith, human reasoning. While theoretical belief revision frameworks rely on a\nset of principles that establish how these operations are performed, empirical\nevidence from cognitive psychology suggests that people may follow different\npatterns when presented with conflicting information. In this paper, we present\nthree comprehensive user studies showing that people consistently prefer\nexplanation-based revisions, i.e., those which are guided by explanations, that\nresult in changes to their belief systems that are not necessarily captured by\nclassical belief change theory. Our experiments systematically investigate how\npeople revise their beliefs with explanations for inconsistencies, whether they\nare provided with them or left to formulate them themselves, demonstrating a\nrobust preference for what may seem non-minimal revisions across different\ntypes of scenarios. These findings have implications for AI systems designed to\nmodel human reasoning or interact with humans, suggesting that such systems\nshould accommodate explanation-based, potentially non-minimal belief revision\noperators to better align with human cognitive processes.", "comment": null, "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.09977v1", "AI": {"title_translation": "人们如何修正不一致的信念？通过用户研究检验人类的信念修正", "tldr": "人类在面对不一致信息时，倾向于基于解释来修正信念，这种修正方式可能与经典理论不同，对AI系统建模人类推理有重要意义。", "motivation": "理解人类如何根据新信息修正信念对于开发能够有效模拟并与人类推理对齐的AI系统至关重要。虽然理论信念修正框架依赖于一套原则，但认知心理学的实证证据表明，人们在面对冲突信息时可能遵循不同的模式。", "method": "本文通过三项全面的用户研究，系统地调查了人们如何根据对不一致性的解释来修正信念，无论是提供解释还是让他们自己形成解释。", "result": "研究表明，人们始终偏好基于解释的信念修正，即由解释引导的修正，这些修正导致其信念系统的变化不一定被经典信念改变理论所捕捉。实验证明了在不同情景下，人们对看似非最小化修正的强烈偏好。", "conclusion": "这些发现对旨在模拟人类推理或与人类交互的AI系统具有重要意义，表明此类系统应适应基于解释的、可能是非最小化的信念修正操作，以更好地与人类认知过程对齐。", "translation": "理解人类如何根据新信息修正信念对于开发能够有效模拟并与人类推理对齐的AI系统至关重要。虽然理论信念修正框架依赖于一套原则来确定这些操作的执行方式，但认知心理学的实证证据表明，人们在面对冲突信息时可能遵循不同的模式。在本文中，我们提出了三项全面的用户研究，表明人们始终偏好基于解释的修正，即由解释引导的修正，这些修正导致其信念系统的变化不一定被经典信念改变理论所捕捉。我们的实验系统地调查了人们如何根据对不一致性的解释来修正信念，无论是提供解释还是让他们自己形成解释，这表明在不同类型的情景中，人们对看似非最小化修正的强烈偏好。这些发现对旨在模拟人类推理或与人类交互的AI系统具有重要意义，表明此类系统应适应基于解释的、可能是非最小化的信念修正操作，以更好地与人类认知过程对齐。", "summary": "本研究通过三项用户研究，探讨了人类在面对不一致信息时如何修正信念。结果发现，人们倾向于基于解释进行信念修正，即使这导致的变化可能不符合经典的信念改变理论，并且表现出对非最小化修正的偏好。这些发现对开发更符合人类认知过程的AI系统具有重要启示。", "keywords": "信念修正, 用户研究, 解释性修正, 人类推理, AI系统", "comments": "该研究通过实证用户研究，揭示了人类信念修正的独特模式，即对解释的偏好和对非最小化修正的接受，这挑战了传统的信念修正理论。其重要性在于为AI系统设计提供了新的视角，强调了在模拟人类推理时需要考虑解释性机制，这对于构建更智能、更具人机交互能力的AI系统至关重要。"}}
{"id": "2506.09096", "title": "Intra-Trajectory Consistency for Reward Modeling", "authors": ["Chaoyang Zhou", "Shunyu Liu", "Zengmao Wang", "Di Wang", "Rong-Cheng Tu", "Bo Du", "Dacheng Tao"], "summary": "Reward models are critical for improving large language models (LLMs),\nparticularly in reinforcement learning from human feedback (RLHF) or\ninference-time verification. Current reward modeling typically relies on scores\nof overall responses to learn the outcome rewards for the responses. However,\nsince the response-level scores are coarse-grained supervision signals, the\nreward model struggles to identify the specific components within a response\ntrajectory that truly correlate with the scores, leading to poor generalization\non unseen responses. In this paper, we propose to leverage generation\nprobabilities to establish reward consistency between processes in the response\ntrajectory, which allows the response-level supervisory signal to propagate\nacross processes, thereby providing additional fine-grained signals for reward\nlearning. Building on analysis under the Bayesian framework, we develop an\nintra-trajectory consistency regularization to enforce that adjacent processes\nwith higher next-token generation probability maintain more consistent rewards.\nWe apply the proposed regularization to the advanced outcome reward model,\nimproving its performance on RewardBench. Besides, we show that the reward\nmodel trained with the proposed regularization induces better DPO-aligned\npolicies and achieves better best-of-N (BON) inference-time verification\nresults. Our code is provided in https://github.com/chaoyang101/ICRM.", "comment": "Under review", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09096v1", "AI": {"title_translation": "奖励建模中的轨迹内一致性", "tldr": "本文提出了一种轨迹内一致性正则化方法，通过利用生成概率来建立响应轨迹中过程之间的奖励一致性，从而为奖励模型提供细粒度信号，以提高大型语言模型的泛化能力。", "motivation": "当前的奖励建模通常依赖于对整体响应的评分来学习结果奖励，但这种粗粒度的监督信号导致奖励模型难以识别响应轨迹中与分数真正相关的特定组件，从而在未见过的响应上泛化能力差。", "method": "本文提出利用生成概率来建立响应轨迹中过程之间的奖励一致性，使响应级别的监督信号能够跨过程传播，从而为奖励学习提供额外的细粒度信号。基于贝叶斯框架分析，开发了一种轨迹内一致性正则化方法，以确保具有更高下一令牌生成概率的相邻过程保持更一致的奖励。", "result": "将所提出的正则化应用于先进的结果奖励模型，提高了其在RewardBench上的性能。此外，研究表明，使用所提出的正则化训练的奖励模型能诱导更好的DPO对齐策略，并实现更好的N选一（BON）推理时间验证结果。", "conclusion": "通过引入轨迹内一致性正则化，可以为奖励模型提供细粒度信号，有效解决现有奖励模型泛化能力差的问题，并显著提高大型语言模型在RLHF和推理时间验证中的性能。", "translation": "奖励模型对于改进大型语言模型（LLM）至关重要，尤其是在人类反馈强化学习（RLHF）或推理时间验证中。当前的奖励建模通常依赖于对整体响应的评分来学习响应的结果奖励。然而，由于响应级别的评分是粗粒度的监督信号，奖励模型难以识别响应轨迹中真正与评分相关的特定组件，导致在未见过的响应上泛化能力差。在本文中，我们提出利用生成概率来建立响应轨迹中过程之间的奖励一致性，这使得响应级别的监督信号能够跨过程传播，从而为奖励学习提供额外的细粒度信号。基于贝叶斯框架下的分析，我们开发了一种轨迹内一致性正则化方法，以强制相邻过程在具有更高下一令牌生成概率时保持更一致的奖励。我们将所提出的正则化应用于先进的结果奖励模型，提高了其在RewardBench上的性能。此外，我们还表明，使用所提出的正则化训练的奖励模型能诱导更好的DPO对齐策略，并实现更好的N选一（BON）推理时间验证结果。我们的代码可在https://github.com/chaoyang101/ICRM中获取。", "summary": "本文针对大型语言模型奖励建模中粗粒度监督信号导致泛化能力差的问题，提出了一种轨迹内一致性正则化方法。该方法利用生成概率在响应轨迹中建立过程间的奖励一致性，从而提供细粒度信号。实验结果表明，该方法提高了奖励模型在RewardBench上的性能，并能诱导更好的DPO对齐策略和BON推理时间验证结果。", "keywords": "奖励建模, 轨迹内一致性, 大型语言模型, RLHF, 泛化能力", "comments": "这项工作提出了一种新颖的方法来解决奖励模型中粗粒度监督信号的局限性，通过引入轨迹内一致性正则化，有效地利用了生成概率来提供更细粒度的奖励学习信号。其创新点在于将贝叶斯框架下的生成概率与奖励一致性相结合，为提高LLM的泛化能力提供了一条有前景的途径。这项研究对于RLHF和推理时间验证中的奖励建模具有重要意义，有望提升LLM的性能和可靠性。"}}
{"id": "2506.09810", "title": "Generalizing Supervised Contrastive learning: A Projection Perspective", "authors": ["Minoh Jeong", "Alfred Hero"], "summary": "Self-supervised contrastive learning (SSCL) has emerged as a powerful\nparadigm for representation learning and has been studied from multiple\nperspectives, including mutual information and geometric viewpoints. However,\nsupervised contrastive (SupCon) approaches have received comparatively little\nattention in this context: for instance, while InfoNCE used in SSCL is known to\nform a lower bound on mutual information (MI), the relationship between SupCon\nand MI remains unexplored. To address this gap, we introduce ProjNCE, a\ngeneralization of the InfoNCE loss that unifies supervised and self-supervised\ncontrastive objectives by incorporating projection functions and an adjustment\nterm for negative pairs. We prove that ProjNCE constitutes a valid MI bound and\naffords greater flexibility in selecting projection strategies for class\nembeddings. Building on this flexibility, we further explore the centroid-based\nclass embeddings in SupCon by exploring a variety of projection methods.\nExtensive experiments on multiple datasets and settings demonstrate that\nProjNCE consistently outperforms both SupCon and standard cross-entropy\ntraining. Our work thus refines SupCon along two complementary\nperspective--mutual information interpretation and projection design--and\noffers broadly applicable improvements whenever SupCon serves as the\nfoundational contrastive objective.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09810v1", "AI": {"title_translation": "泛化监督对比学习：一种投影视角", "tldr": "本文提出了ProjNCE，一种泛化InfoNCE损失的方法，它统一了监督和自监督对比学习，并从互信息和投影设计角度改进了监督对比学习，实验证明其性能优于现有方法。", "motivation": "自监督对比学习（SSCL）已成为表示学习的强大范式，但监督对比学习（SupCon）方法受到的关注相对较少。特别地，SSCL中使用的InfoNCE损失已知是互信息（MI）的下界，但SupCon与MI之间的关系仍未被探索。", "method": "为解决现有问题，本文引入了ProjNCE，它泛化了InfoNCE损失，通过结合投影函数和负样本调整项来统一监督和自监督对比目标。作者证明了ProjNCE是一个有效的MI下界，并为类嵌入的投影策略选择提供了更大的灵活性。在此基础上，进一步探索了SupCon中基于质心的类嵌入的各种投影方法。", "result": "在多个数据集和设置上进行的大量实验表明，ProjNCE持续优于SupCon和标准的交叉熵训练方法。", "conclusion": "本文的工作从互信息解释和投影设计两个互补的视角完善了监督对比学习（SupCon），并在SupCon作为基础对比目标时提供了广泛适用的改进。", "translation": "自监督对比学习（SSCL）已成为表示学习的强大范式，并已从互信息和几何视角等多个角度进行了研究。然而，监督对比（SupCon）方法在此背景下受到的关注相对较少：例如，虽然SSCL中使用的InfoNCE已知构成互信息（MI）的下界，但SupCon与MI之间的关系仍未被探索。为了弥补这一空白，我们引入了ProjNCE，它是InfoNCE损失的一种泛化，通过结合投影函数和负样本调整项来统一监督和自监督对比目标。我们证明ProjNCE构成了一个有效的MI下界，并为类嵌入的投影策略选择提供了更大的灵活性。基于这种灵活性，我们通过探索各种投影方法，进一步探讨了SupCon中基于质心的类嵌入。在多个数据集和设置上进行的大量实验表明，ProjNCE持续优于SupCon和标准的交叉熵训练。因此，我们的工作从互信息解释和投影设计两个互补的视角完善了SupCon，并在SupCon作为基础对比目标时提供了广泛适用的改进。", "summary": "本文提出了一种名为ProjNCE的新型损失函数，旨在泛化InfoNCE并统一监督与自监督对比学习目标。ProjNCE通过引入投影函数和负样本调整项，解决了监督对比学习（SupCon）与互信息（MI）关系未被探索的问题。研究证明ProjNCE是有效的MI下界，并提高了类嵌入投影策略的灵活性。实验结果显示，ProjNCE在多个数据集上均优于SupCon和传统交叉熵训练，从而从互信息解释和投影设计两方面改进了SupCon。", "keywords": "监督对比学习, 互信息, 投影, ProjNCE, 表示学习", "comments": "本文的创新点在于提出了ProjNCE损失函数，它巧妙地将监督和自监督对比学习统一起来，并首次明确了监督对比学习与互信息之间的关系。通过引入投影设计和负样本调整项，该方法不仅在理论上为SupCon提供了更深的理解，也在实践中显著提升了其性能。这项工作为未来的对比学习研究，特别是其在不同应用场景下的泛化能力，提供了有价值的参考。"}}
{"id": "2506.09713", "title": "A First Look at Bugs in LLM Inference Engines", "authors": ["Mugeng Liu", "Siqi Zhong", "Weichen Bi", "Yixuan Zhang", "Zhiyang Chen", "Zhenpeng Chen", "Xuanzhe Liu", "Yun Ma"], "summary": "Large language model-specific inference engines (in short as \\emph{LLM\ninference engines}) have become a fundamental component of modern AI\ninfrastructure, enabling the deployment of LLM-powered applications (LLM apps)\nacross cloud and local devices. Despite their critical role, LLM inference\nengines are prone to bugs due to the immense resource demands of LLMs and the\ncomplexities of cross-platform compatibility. However, a systematic\nunderstanding of these bugs remains lacking. To bridge this gap, we present the\nfirst empirical study on bugs in LLM inference engines. We mine official\nrepositories of 5 widely adopted LLM inference engines, constructing a\ncomprehensive dataset of 929 real-world bugs. Through a rigorous open coding\nprocess, we analyze these bugs to uncover their symptoms, root causes, and\ncommonality. Our findings reveal six major bug symptoms and a taxonomy of 28\nroot causes, shedding light on the key challenges in bug detection and location\nwithin LLM inference engines. Based on these insights, we propose a series of\nactionable implications for researchers, inference engine vendors, and LLM app\ndevelopers.", "comment": "Under review", "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.09713v1", "AI": {"title_translation": "LLM推理引擎中的错误初探", "tldr": "首次对LLM推理引擎中的错误进行了实证研究，发现了6种主要症状和28种根本原因，并提出了可行的建议。", "motivation": "LLM推理引擎是现代AI基础设施的关键组成部分，但由于其巨大的资源需求和跨平台兼容性的复杂性，它们容易出现错误。然而，目前对这些错误的系统性理解仍然缺乏，本研究旨在填补这一空白。", "method": "本研究对5个广泛采用的LLM推理引擎的官方仓库进行了挖掘，构建了一个包含929个真实世界错误的综合数据集。随后，通过严格的开放编码过程，分析了这些错误，以揭示其症状、根本原因和普遍性。", "result": "研究结果揭示了六种主要的错误症状和28种根本原因的分类，阐明了LLM推理引擎中错误检测和定位的关键挑战。", "conclusion": "基于研究发现，论文为研究人员、推理引擎供应商和LLM应用程序开发人员提出了一系列可操作的建议。", "translation": "大型语言模型专用推理引擎（简称LLM推理引擎）已成为现代AI基础设施的基本组成部分，支持在云端和本地设备上部署由LLM驱动的应用程序（LLM应用）。尽管它们扮演着关键角色，但由于LLM巨大的资源需求和跨平台兼容性的复杂性，LLM推理引擎容易出现错误。然而，目前对这些错误的系统性理解仍然缺乏。为了弥补这一差距，我们首次对LLM推理引擎中的错误进行了实证研究。我们挖掘了5个广泛采用的LLM推理引擎的官方仓库，构建了一个包含929个真实世界错误的综合数据集。通过严格的开放编码过程，我们分析了这些错误，以揭示它们的症状、根本原因和普遍性。我们的研究结果揭示了六种主要的错误症状和28种根本原因的分类，阐明了LLM推理引擎中错误检测和定位的关键挑战。基于这些见解，我们为研究人员、推理引擎供应商和LLM应用程序开发人员提出了一系列可操作的建议。", "summary": "本文首次对大型语言模型（LLM）推理引擎中的错误进行了实证研究。通过分析5个主流LLM推理引擎的929个真实错误，研究揭示了6种主要错误症状和28种根本原因分类。这些发现有助于理解LLM推理引擎中错误检测和定位的挑战，并为相关方提供了改进建议。", "keywords": "LLM推理引擎, 错误分析, 实证研究, 根本原因, 错误症状", "comments": "这是一项开创性的研究，首次系统地分析了LLM推理引擎中的错误。其贡献在于构建了大规模的真实错误数据集，并对错误症状和根本原因进行了分类，为未来LLM推理引擎的可靠性提升提供了宝贵的经验和指导。该研究的重要性在于其揭示了LLM应用部署中底层基础设施的潜在脆弱性，并为开发者和研究者提供了明确的改进方向。"}}
{"id": "2506.09696", "title": "Patterns of Patterns III", "authors": ["Joseph Corneli", "Charles J. Danoff", "Raymond S. Puzio", "Sridevi Ayloo", "Serge Belich", "Mary Tedeschi"], "summary": "Building on earlier installments, this paper re-examines the PLACARD pattern.\nWe report on a series of workshops where PLACARD was used to scaffold\ncollaborative reflection, speculative inquiry, and stimulate design pattern\ngeneration. These accounts are enriched by a comparison case: virtual workshops\ncarried out with simple AI-based chatbots. We discuss limitations and lessons\nlearned from both the human and multi-agent settings. We conclude by outlining\na future development strategy at the intersection of AI agents, design\npatterns, and institutional governance.", "comment": "18 pages; submitted to Pattern Languages of Programs 2025", "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.09696v1", "AI": {"title_translation": "模式的模式 III", "tldr": "本文重新审视了PLACARD模式，报告了使用该模式促进协作反思、推测性探究和设计模式生成的系列研讨会，并与AI聊天机器人虚拟研讨会进行了比较，讨论了局限性并提出了未来发展策略。", "motivation": "本文旨在重新审视PLACARD模式，并探讨其在促进协作反思、推测性探究和设计模式生成方面的应用。", "method": "研究通过一系列使用PLACARD模式的研讨会进行，以促进协作反思、推测性探究和设计模式生成。此外，还进行了一个比较案例，即使用简单AI聊天机器人进行的虚拟研讨会。", "result": "报告了PLACARD模式在研讨会中用于搭建协作反思、推测性探究和激发设计模式生成支架的情况。讨论了在人类和多智能体设置中遇到的局限性及经验教训。", "conclusion": "论文通过概述AI智能体、设计模式和制度治理交叉领域的未来发展战略作为结论。", "translation": "本文在前几期的基础上，重新审视了PLACARD模式。我们报告了一系列研讨会，其中PLACARD被用于搭建协作反思、推测性探究的支架，并激发设计模式的生成。这些记录通过一个比较案例得到丰富：使用简单的基于AI的聊天机器人进行的虚拟研讨会。我们讨论了人类和多智能体设置的局限性和经验教训。最后，我们概述了AI智能体、设计模式和制度治理交叉领域的未来发展战略。", "summary": "本文重新审视了PLACARD模式，并报告了在系列研讨会中使用该模式促进协作反思、推测性探究和设计模式生成的经验。研究还与AI聊天机器人进行的虚拟研讨会进行了比较，并讨论了人机交互环境下的局限性及经验教训。最后，论文提出了AI智能体、设计模式和制度治理相结合的未来发展策略。", "keywords": "PLACARD模式, 协作反思, 设计模式生成, AI聊天机器人, 多智能体设置", "comments": "这篇论文的创新点在于将传统研讨会中的模式应用与AI聊天机器人辅助的虚拟研讨会进行对比分析，这为协作设计和知识生成提供了新的视角。其重要性在于探索了AI在辅助人类协作和模式生成中的潜力，并指出了未来AI智能体、设计模式和制度治理融合的发展方向。"}}
{"id": "2506.09573", "title": "Probability-One Optimization of Generalized Rayleigh Quotient Sum For Multi-Source Generalized Total Least-Squares", "authors": ["Dominik Friml", "Pavel Václavek"], "summary": "This paper addresses the global optimization of the sum of the Rayleigh\nquotient and the generalized Rayleigh quotient on the unit sphere. While\nvarious methods have been proposed for this problem, they do not guarantee\nconvergence to the global maximizer. To overcome this limitation, we introduce\na probability-one homotopy optimization method that, under certain conditions,\nguarantees convergence to the global maximizer. The proposed method is analyzed\nalongside state-of-the-art approaches through numerical experiments, evaluating\ntheir performance in terms of convergence speed and ability to reach the global\nmaximizer. Furthermore, we demonstrate how this ties in with the multi-source\nBayesian Generalized Total Least-Squares (B-GTLS) problem, illustrating its\napplicability.", "comment": "This is the preprint version prior to peer review", "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.09573v1", "AI": {"title_translation": "多源广义全最小二乘的广义瑞利商和的概率一优化", "tldr": "本文提出了一种概率一同伦优化方法，用于解决瑞利商和广义瑞利商之和的全局优化问题，该方法能保证收敛到全局最大值，并适用于多源贝叶斯广义全最小二乘问题。", "motivation": "现有方法无法保证收敛到全局最大值，因此需要一种能保证收敛到全局最大值的方法。", "method": "引入了一种概率一同伦优化方法。", "result": "所提出的方法通过数值实验与现有方法进行了比较，评估了其收敛速度和达到全局最大值的性能。此外，还展示了该方法如何与多源贝叶斯广义全最小二乘（B-GTLS）问题相结合，证明了其适用性。", "conclusion": "提出的概率一同伦优化方法在特定条件下能保证收敛到全局最大值，并适用于多源贝叶斯广义全最小二乘问题。", "translation": "本文解决了单位球面上瑞利商和广义瑞利商之和的全局优化问题。尽管针对此问题已提出了各种方法，但它们不能保证收敛到全局最大值。为了克服这一限制，我们引入了一种概率一同伦优化方法，该方法在特定条件下能保证收敛到全局最大值。通过数值实验，将所提出的方法与最先进的方法进行了分析，评估了它们在收敛速度和达到全局最大值方面的性能。此外，我们还展示了这如何与多源贝叶斯广义全最小二乘（B-GTLS）问题相关联，说明了其适用性。", "summary": "本文提出了一种新颖的概率一同伦优化方法，旨在解决单位球面上瑞利商和广义瑞利商之和的全局优化问题。与现有方法不同，该方法在特定条件下能保证收敛到全局最大值。通过数值实验验证了其性能，并展示了其在多源贝叶斯广义全最小二乘问题中的应用潜力。", "keywords": "瑞利商, 广义瑞利商, 全局优化, 概率一, 同伦优化", "comments": "该论文的主要创新点在于提出了一种能够保证收敛到全局最大值的概率一同伦优化方法，解决了现有方法无法保证全局收敛的局限性。这对于需要精确全局最优解的优化问题具有重要意义，尤其是在其与多源贝叶斯广义全最小二乘问题的结合中体现了实用价值。"}}
{"id": "2506.09175", "title": "PHRASED: Phrase Dictionary Biasing for Speech Translation", "authors": ["Peidong Wang", "Jian Xue", "Rui Zhao", "Junkun Chen", "Aswin Shanmugam Subramanian", "Jinyu Li"], "summary": "Phrases are essential to understand the core concepts in conversations.\nHowever, due to their rare occurrence in training data, correct translation of\nphrases is challenging in speech translation tasks. In this paper, we propose a\nphrase dictionary biasing method to leverage pairs of phrases mapping from the\nsource language to the target language. We apply the phrase dictionary biasing\nmethod to two types of widely adopted models, a transducer-based streaming\nspeech translation model and a multimodal large language model. Experimental\nresults show that the phrase dictionary biasing method outperforms phrase list\nbiasing by 21% relatively for the streaming speech translation model. In\naddition, phrase dictionary biasing enables multimodal large language models to\nuse external phrase information, achieving 85% relative improvement in phrase\nrecall.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.09175v1", "AI": {"title_translation": "PHRASED：语音翻译中的短语词典偏置", "tldr": "本文提出了一种短语词典偏置方法，旨在解决语音翻译中短语翻译不准确的问题，并在流式语音翻译模型和多模态大型语言模型上取得了显著效果。", "motivation": "在语音翻译任务中，由于短语在训练数据中出现频率低，导致短语的正确翻译具有挑战性。", "method": "提出了一种短语词典偏置方法，利用源语言到目标语言的短语对映射。该方法应用于基于 transducer 的流式语音翻译模型和多模态大型语言模型。", "result": "实验结果显示，短语词典偏置方法在流式语音翻译模型上相对短语列表偏置提高了21%。此外，该方法使多模态大型语言模型能够使用外部短语信息，实现85%的短语召回率相对提升。", "conclusion": "短语词典偏置方法能够有效提高语音翻译中短语的翻译准确性，并增强多模态大型语言模型利用外部短语信息的能力。", "translation": "短语对于理解对话的核心概念至关重要。然而，由于它们在训练数据中很少出现，短语的正确翻译在语音翻译任务中具有挑战性。在本文中，我们提出了一种短语词典偏置方法，以利用从源语言到目标语言的短语对映射。我们将短语词典偏置方法应用于两种广泛采用的模型：基于 transducer 的流式语音翻译模型和多模态大型语言模型。实验结果表明，短语词典偏置方法在流式语音翻译模型上相对短语列表偏置提高了21%。此外，短语词典偏置使多模态大型语言模型能够使用外部短语信息，短语召回率相对提高了85%。", "summary": "本文提出了一种名为PHRASED的短语词典偏置方法，旨在解决语音翻译中短语因稀疏性而难以准确翻译的问题。该方法通过利用源语言到目标语言的短语对映射，增强了模型对短语的理解和翻译能力。实验证明，该方法在流式语音翻译模型上比传统短语列表偏置效果提升21%，并且使多模态大型语言模型在短语召回率上实现85%的显著提升，表明其在提高语音翻译质量方面的有效性。", "keywords": "语音翻译, 短语词典偏置, 流式语音翻译, 大型语言模型, 短语召回", "comments": "这篇论文通过引入短语词典偏置，有效解决了语音翻译中短语稀疏性导致的翻译难题。其创新点在于将外部短语知识以偏置的形式融入到两种不同类型的模型中，并取得了显著的性能提升，尤其是在短语召回率方面。这为未来语音翻译系统利用外部知识提供了新的思路，具有重要的实际应用价值。"}}
{"id": "2506.09255", "title": "AI-Driven SEEG Channel Ranking for Epileptogenic Zone Localization", "authors": ["Saeed Hashemi", "Genchang Peng", "Mehrdad Nourani", "Omar Nofal", "Jay Harvey"], "summary": "Stereo-electroencephalography (SEEG) is an invasive technique to implant\ndepth electrodes and collect data for pre-surgery evaluation. Visual inspection\nof signals recorded from hundreds of channels is time consuming and\ninefficient. We propose a machine learning approach to rank the impactful\nchannels by incorporating clinician's selection and computational finding. A\nclassification model using XGBoost is trained to learn the discriminative\nfeatures of each channel during ictal periods. Then, the SHapley Additive\nexPlanations (SHAP) scoring is utilized to rank SEEG channels based on their\ncontribution to seizures. A channel extension strategy is also incorporated to\nexpand the search space and identify suspicious epileptogenic zones beyond\nthose selected by clinicians. For validation, SEEG data for five patients were\nanalyzed showing promising results in terms of accuracy, consistency, and\nexplainability.", "comment": "Accepted to be presented at the 47th Annual International Conference\n  of the IEEE Engineering in Medicine and Biology Society (EMBC 2025). This\n  version is submitted to arXiv prior to final IEEE formatting and publication", "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.09255v1", "AI": {"title_translation": "AI驱动的SEEG通道排序用于癫痫灶定位", "tldr": "提出一种机器学习方法，结合临床医生选择和计算结果，利用XGBoost和SHAP对SEEG通道进行排序，以高效定位癫痫灶。", "motivation": "视觉检查数百个SEEG通道信号耗时且低效，需要更高效的方法来定位癫痫灶。", "method": "提出一种机器学习方法，结合临床医生选择和计算结果。使用XGBoost训练分类模型学习发作期间通道的判别特征。利用SHAP评分根据通道对癫痫发作的贡献进行排序。采用通道扩展策略扩大搜索空间。", "result": "对五名患者的SEEG数据进行分析，在准确性、一致性和可解释性方面显示出有希望的结果。", "conclusion": "该AI驱动的SEEG通道排序方法能有效且可解释地帮助定位癫痫灶，提高术前评估效率。", "translation": "立体脑电图（SEEG）是一种侵入性技术，用于植入深部电极并收集数据进行术前评估。目视检查数百个通道记录的信号既耗时又低效。我们提出了一种机器学习方法，通过结合临床医生的选择和计算结果来对有影响力的通道进行排序。使用XGBoost训练了一个分类模型，以学习发作期间每个通道的判别特征。然后，利用SHapley Additive exPlanations (SHAP) 评分根据SEEG通道对癫痫发作的贡献进行排序。还结合了通道扩展策略，以扩大搜索空间并识别临床医生选择范围之外的可疑癫痫灶。为了验证，分析了五名患者的SEEG数据，在准确性、一致性和可解释性方面显示出有希望的结果。", "summary": "本文提出一种AI驱动的SEEG通道排序方法，旨在解决传统视觉检查耗时低效的问题。该方法结合临床医生选择和计算发现，利用XGBoost模型学习发作期特征，并使用SHAP评分对通道进行排序。此外，还引入通道扩展策略以识别潜在癫痫灶。在五名患者数据上的验证显示出良好的准确性、一致性和可解释性，为癫痫灶定位提供了高效、可解释的辅助手段。", "keywords": "SEEG, 癫痫灶定位, 机器学习, XGBoost, SHAP", "comments": "这项研究的创新之处在于将机器学习（XGBoost）和可解释性AI（SHAP）应用于SEEG数据分析，以自动化和优化癫痫灶定位过程。结合临床知识和计算结果，并引入通道扩展策略，提高了定位的效率和潜在的准确性。其重要性在于能够显著减轻临床医生的工作负担，并可能提高癫痫术前评估的精准度。"}}
{"id": "2506.09949", "title": "Sampling Theory for Super-Resolution with Implicit Neural Representations", "authors": ["Mahrokh Najaf", "Gregory Ongie"], "summary": "Implicit neural representations (INRs) have emerged as a powerful tool for\nsolving inverse problems in computer vision and computational imaging. INRs\nrepresent images as continuous domain functions realized by a neural network\ntaking spatial coordinates as inputs. However, unlike traditional pixel\nrepresentations, little is known about the sample complexity of estimating\nimages using INRs in the context of linear inverse problems. Towards this end,\nwe study the sampling requirements for recovery of a continuous domain image\nfrom its low-pass Fourier samples by fitting a single hidden-layer INR with\nReLU activation and a Fourier features layer using a generalized form of weight\ndecay regularization. Our key insight is to relate minimizers of this\nnon-convex parameter space optimization problem to minimizers of a convex\npenalty defined over an infinite-dimensional space of measures. We identify a\nsufficient number of Fourier samples for which an image realized by an INR is\nexactly recoverable by solving the INR training problem. To validate our\ntheory, we empirically assess the probability of achieving exact recovery of\nimages realized by low-width single hidden-layer INRs, and illustrate the\nperformance of INRs on super-resolution recovery of continuous domain phantom\nimages.", "comment": "arXiv admin note: text overlap with arXiv:2405.18410", "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.09949v1", "AI": {"title_translation": "隐式神经表示超分辨率的采样理论", "tldr": "本研究探讨了隐式神经表示（INRs）在超分辨率中图像估计的采样复杂性，并提出了一个理论框架来确定精确恢复所需的傅里叶采样数。", "motivation": "尽管隐式神经表示（INRs）在计算机视觉和计算成像中是强大的工具，但对于使用INRs在线性逆问题背景下估计图像的采样复杂性知之甚少。", "method": "研究通过拟合带有ReLU激活和傅里叶特征层的单隐藏层INR，并使用广义权重衰减正则化，从其低通傅里叶样本中恢复连续域图像的采样要求。关键在于将非凸参数空间优化问题的最小值与无限维测度空间上定义的凸惩罚的最小值联系起来。", "result": "确定了实现INR图像精确恢复所需的足够傅里叶样本数。通过实证评估了低宽度单隐藏层INR实现精确恢复的概率，并展示了INR在连续域图像超分辨率恢复中的性能。", "conclusion": "论文提出了一个理论框架，用于理解和确定隐式神经表示在超分辨率任务中精确图像恢复所需的采样条件，并通过实验进行了验证。", "translation": "隐式神经表示（INRs）已成为解决计算机视觉和计算成像中逆问题的强大工具。INRs将图像表示为由神经网络实现的连续域函数，该网络将空间坐标作为输入。然而，与传统的像素表示不同，对于在线性逆问题背景下使用INRs估计图像的采样复杂性知之甚少。为此，我们研究了通过拟合一个带有ReLU激活和傅里叶特征层的单隐藏层INR，并使用广义形式的权重衰减正则化，从其低通傅里叶样本中恢复连续域图像的采样要求。我们的关键见解是将这个非凸参数空间优化问题的最小值与在无限维测度空间上定义的凸惩罚的最小值联系起来。我们确定了实现INR图像精确恢复所需的足够傅里叶样本数，通过解决INR训练问题即可实现。为了验证我们的理论，我们凭经验评估了低宽度单隐藏层INR实现图像精确恢复的概率，并展示了INR在连续域幻影图像超分辨率恢复中的性能。", "summary": "这篇论文探讨了隐式神经表示（INRs）在超分辨率任务中的采样理论。它研究了使用单隐藏层INR从低通傅里叶样本中恢复连续域图像所需的采样条件，并提出了一个理论框架，将非凸优化问题与凸惩罚联系起来。研究确定了精确恢复INR图像所需的傅里叶样本数，并通过实验验证了理论，展示了INRs在超分辨率中的有效性。", "keywords": "隐式神经表示, 超分辨率, 采样理论, 逆问题, 傅里叶样本", "comments": "这篇论文为理解隐式神经表示的理论基础提供了重要的贡献，特别是在采样复杂性和逆问题解决方面。将非凸优化问题与凸惩罚联系起来的见解是其创新点。这有助于INRs在实际应用中更可靠地进行图像重建，尤其是在数据受限的场景下。"}}
{"id": "2506.09632", "title": "Ties of Trust: a bowtie model to uncover trustor-trustee relationships in LLMs", "authors": ["Eva Paraschou", "Maria Michali", "Sofia Yfantidou", "Stelios Karamanidis", "Stefanos Rafail Kalogeros", "Athena Vakali"], "summary": "The rapid and unprecedented dominance of Artificial Intelligence (AI),\nparticularly through Large Language Models (LLMs), has raised critical trust\nchallenges in high-stakes domains like politics. Biased LLMs' decisions and\nmisinformation undermine democratic processes, and existing trust models fail\nto address the intricacies of trust in LLMs. Currently, oversimplified,\none-directional approaches have largely overlooked the many relationships\nbetween trustor (user) contextual factors (e.g. ideology, perceptions) and\ntrustee (LLMs) systemic elements (e.g. scientists, tool's features). In this\nwork, we introduce a bowtie model for holistically conceptualizing and\nformulating trust in LLMs, with a core component comprehensively exploring\ntrust by tying its two sides, namely the trustor and the trustee, as well as\ntheir intricate relationships. We uncover these relationships within the\nproposed bowtie model and beyond to its sociotechnical ecosystem, through a\nmixed-methods explanatory study, that exploits a political discourse analysis\ntool (integrating ChatGPT), by exploring and responding to the next critical\nquestions: 1) How do trustor's contextual factors influence trust-related\nactions? 2) How do these factors influence and interact with trustee systemic\nelements? 3) How does trust itself vary across trustee systemic elements? Our\nbowtie-based explanatory analysis reveals that past experiences and familiarity\nsignificantly shape trustor's trust-related actions; not all trustor contextual\nfactors equally influence trustee systemic elements; and trustee's\nhuman-in-the-loop features enhance trust, while lack of transparency decreases\nit. Finally, this solid evidence is exploited to deliver recommendations,\ninsights and pathways towards building robust trusting ecosystems in LLM-based\nsolutions.", "comment": "Accepted for publication at The 2025 ACM Conference on Fairness,\n  Accountability, and Transparency (FAccT '25). This version corresponds to the\n  camera-ready manuscript submitted to the conference proceedings", "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.09632v1", "AI": {"title_translation": "信任的纽带：一种揭示大型语言模型中信任者-受托者关系的蝴蝶结模型", "tldr": "本文提出了一个蝴蝶结模型，用于分析大型语言模型（LLMs）中复杂的信任者-受托者关系，揭示了用户情境和LLM特性如何影响信任，并提供了构建信任的建议。", "motivation": "大型语言模型（LLMs）的快速普及在政治等高风险领域带来了严峻的信任挑战。现有信任模型过于简化，未能解决LLMs中信任的复杂性，忽视了信任者（用户）情境因素与受托者（LLMs）系统元素之间的多重关系，导致偏见和错误信息可能破坏民主进程。", "method": "研究引入了一个蝴蝶结模型，以整体性地概念化和构建LLMs中的信任，其核心组件通过连接信任者和受托者及其复杂关系来全面探索信任。通过一项结合了政治话语分析工具（集成ChatGPT）的混合方法解释性研究，揭示了所提出的蝴蝶结模型及其社会技术生态系统中的这些关系，并回答了信任者因素如何影响信任行为、这些因素如何影响和与受托者元素互动以及信任本身如何随受托者元素变化等关键问题。", "result": "基于蝴蝶结的解释性分析揭示，过去的经验和熟悉度显著塑造了信任者的信任相关行为；并非所有信任者情境因素都平等地影响受托者系统元素；受托者的“人在回路”特性增强信任，而缺乏透明度则会降低信任。", "conclusion": "这些确凿的证据被用于为构建基于LLM解决方案中稳健的信任生态系统提供建议、见解和途径。", "translation": "人工智能（AI），特别是大型语言模型（LLMs）的快速且前所未有的主导地位，在政治等高风险领域引发了严峻的信任挑战。有偏见的LLMs决策和错误信息破坏了民主进程，而现有信任模型未能解决LLMs中信任的复杂性。目前，过于简化、单向的方法在很大程度上忽视了信任者（用户）情境因素（例如意识形态、感知）与受托者（LLMs）系统元素（例如科学家、工具特性）之间的诸多关系。在这项工作中，我们引入了一个蝴蝶结模型，用于整体性地概念化和构建LLMs中的信任，其核心组件通过连接信任的两个方面，即信任者和受托者，以及它们之间错综复杂的关系，全面探索信任。我们通过一项混合方法解释性研究，利用政治话语分析工具（集成ChatGPT），通过探索和回答以下关键问题，揭示了所提出的蝴蝶结模型及其社会技术生态系统中的这些关系：1）信任者的情境因素如何影响信任相关行为？2）这些因素如何影响并与受托者系统元素互动？3）信任本身如何随受托者系统元素变化？我们基于蝴蝶结的解释性分析揭示，过去的经验和熟悉度显著塑造了信任者的信任相关行为；并非所有信任者情境因素都平等地影响受托者系统元素；受托者的“人在回路”特性增强信任，而缺乏透明度则会降低信任。最后，这些确凿的证据被用于为构建基于LLM解决方案中稳健的信任生态系统提供建议、见解和途径。", "summary": "本文针对大型语言模型（LLMs）在政治等高风险领域面临的信任挑战，提出了一种新颖的蝴蝶结模型。该模型通过连接信任者（用户）的情境因素与受托者（LLM）的系统元素，整体性地概念化并构建了LLMs中的信任。通过一项利用政治话语分析工具的混合方法研究，研究揭示了信任者的过往经验和熟悉度、受托者的“人在回路”特性以及透明度对信任的显著影响。这些发现为构建稳健的LLM信任生态系统提供了宝贵的建议和途径。", "keywords": "大型语言模型, 信任, 蝴蝶结模型, 信任者-受托者关系, 人工智能伦理", "comments": "本文的创新之处在于引入了一种新颖的“蝴蝶结模型”来整体性地分析大型语言模型中的信任，超越了以往简化的单向视角。其重要性体现在解决了政治等关键领域中的信任挑战，提供了一种结构化的方法来理解用户与AI系统之间复杂的相互作用，并为构建更值得信赖的LLM解决方案提供了可操作的建议。"}}
{"id": "2506.09898", "title": "Discrete Scale-invariant Metric Learning for Efficient Collaborative Filtering", "authors": ["Yan Zhang", "Li Deng", "Lixin Duan", "Sami Azam"], "summary": "Metric learning has attracted extensive interest for its ability to provide\npersonalized recommendations based on the importance of observed user-item\ninteractions. Current metric learning methods aim to push negative items away\nfrom the corresponding users and positive items by an absolute geometrical\ndistance margin. However, items may come from imbalanced categories with\ndifferent intra-class variations. Thus, the absolute distance margin may not be\nideal for estimating the difference between user preferences over imbalanced\nitems. To this end, we propose a new method, named discrete scale-invariant\nmetric learning (DSIML), by adding binary constraints to users and items, which\nmaps users and items into binary codes of a shared Hamming subspace to speed up\nthe online recommendation. Specifically, we firstly propose a scale-invariant\nmargin based on angles at the negative item points in the shared Hamming\nsubspace. Then, we derive a scale-invariant triple hinge loss based on the\nmargin. To capture more preference difference information, we integrate a\npairwise ranking loss into the scale-invariant loss in the proposed model. Due\nto the difficulty of directly optimizing the mixed integer optimization problem\nformulated with \\textit{log-sum-exp} functions, we seek to optimize its\nvariational quadratic upper bound and learn hash codes with an alternating\noptimization strategy. Experiments on benchmark datasets clearly show that our\nproposed method is superior to competitive metric learning and hashing-based\nbaselines for recommender systems. The implementation code is available at\nhttps://github.com/AnonyFeb/dsml.", "comment": null, "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.09898v1", "AI": {"title_translation": "用于高效协同过滤的离散尺度不变度量学习", "tldr": "本文提出了一种新的离散尺度不变度量学习（DSIML）方法，通过将用户和物品映射到共享汉明子空间中的二进制代码，并设计了尺度不变的损失函数，以解决现有度量学习方法在处理不平衡类别物品时的局限性，从而提高推荐系统的性能。", "motivation": "当前度量学习方法使用绝对几何距离来推开负面物品，但当物品来自具有不同类内变化的不平衡类别时，这种绝对距离可能不适合估计用户偏好差异。因此，需要一种新的方法来解决这个问题。", "method": "本文提出了一种名为离散尺度不变度量学习（DSIML）的新方法。该方法通过向用户和物品添加二进制约束，将它们映射到共享汉明子空间中的二进制代码，以加速在线推荐。具体来说，首先提出了一种基于共享汉明子空间中负面物品点角度的尺度不变边距。然后，基于该边距推导出尺度不变的三重铰链损失。为了捕获更多的偏好差异信息，将成对排序损失整合到模型中的尺度不变损失中。由于直接优化包含log-sum-exp函数的混合整数优化问题存在困难，因此寻求优化其变分二次上界，并采用交替优化策略学习哈希码。", "result": "在基准数据集上的实验清楚地表明，所提出的方法优于竞争性度量学习和基于哈希的推荐系统基线方法。", "conclusion": "DSIML通过引入尺度不变性并结合二进制哈希，有效解决了传统度量学习在处理不平衡物品类别时的局限性，显著提升了推荐系统的性能和效率。", "translation": "度量学习因其能够根据观察到的用户-物品交互的重要性提供个性化推荐而引起了广泛关注。当前的度量学习方法旨在通过绝对几何距离将负面物品推离相应的用户和正面物品。然而，物品可能来自具有不同类内变化的不平衡类别。因此，绝对距离边距可能不适合估计用户对不平衡物品的偏好差异。为此，我们提出了一种名为离散尺度不变度量学习（DSIML）的新方法，通过向用户和物品添加二进制约束，将用户和物品映射到共享汉明子空间中的二进制代码，以加速在线推荐。具体来说，我们首先提出了一种基于共享汉明子空间中负面物品点角度的尺度不变边距。然后，我们基于该边距推导出尺度不变的三重铰链损失。为了捕获更多的偏好差异信息，我们将成对排序损失整合到所提出模型中的尺度不变损失中。由于直接优化用log-sum-exp函数表述的混合整数优化问题存在困难，我们寻求优化其变分二次上界，并采用交替优化策略学习哈希码。在基准数据集上的实验清楚地表明，我们提出的方法优于竞争性度量学习和基于哈希的推荐系统基线方法。实施代码可在https://github.com/AnonyFeb/dsml获取。", "summary": "本文提出了一种名为离散尺度不变度量学习（DSIML）的新方法，旨在解决现有度量学习在处理不平衡类别物品时，绝对距离度量不理想的问题。DSIML通过将用户和物品映射到共享汉明子空间中的二进制代码，并引入基于角度的尺度不变边距和尺度不变三重铰链损失，同时整合成对排序损失，以捕获更丰富的偏好信息。该方法采用交替优化策略学习哈希码。实验结果表明，DSIML在推荐系统性能上优于现有度量学习和哈希基线方法。", "keywords": "度量学习, 协同过滤, 尺度不变性, 哈希, 推荐系统", "comments": "该论文的创新点在于提出了离散尺度不变度量学习（DSIML），它通过引入尺度不变性来解决传统度量学习在处理不平衡数据时的局限性。同时，将用户和物品映射到二进制代码的汉明子空间，不仅解决了推荐效率问题，还为度量学习提供了一种新的视角。方法论结合了尺度不变损失和哈希技术，具有较高的实用价值和理论意义。"}}
{"id": "2506.09394", "title": "Subspace-constrained randomized coordinate descent for linear systems with good low-rank matrix approximations", "authors": ["Jackie Lok", "Elizaveta Rebrova"], "summary": "The randomized coordinate descent (RCD) method is a classical algorithm with\nsimple, lightweight iterations that is widely used for various optimization\nproblems, including the solution of positive semidefinite linear systems. As a\nlinear solver, RCD is particularly effective when the matrix is\nwell-conditioned; however, its convergence rate deteriorates rapidly in the\npresence of large spectral outliers. In this paper, we introduce the\nsubspace-constrained randomized coordinate descent (SC-RCD) method, in which\nthe dynamics of RCD are restricted to an affine subspace corresponding to a\ncolumn Nystr\\\"{o}m approximation, efficiently computed using the recently\nanalyzed RPCholesky algorithm. We prove that SC-RCD converges at a rate that is\nunaffected by large spectral outliers, making it an effective and\nmemory-efficient solver for large-scale, dense linear systems with rapidly\ndecaying spectra, such as those encountered in kernel ridge regression.\nExperimental validation and comparisons with related solvers based on\ncoordinate descent and the conjugate gradient method demonstrate the efficiency\nof SC-RCD. Our theoretical results are derived by developing a more general\nsubspace-constrained framework for the sketch-and-project method. This\nframework generalizes popular algorithms such as randomized Kaczmarz and\ncoordinate descent, and provides a flexible, implicit preconditioning strategy\nfor a variety of iterative solvers, which may be of independent interest.", "comment": null, "cate": "math.NA", "url": "http://arxiv.org/abs/2506.09394v1", "AI": {"title_translation": "用于具有良好低秩矩阵近似的线性系统的子空间约束随机坐标下降法", "tldr": "随机坐标下降（RCD）在存在大谱异常值时收敛速度会恶化。本文提出子空间约束随机坐标下降（SC-RCD）方法，通过将RCD动态限制在Nyström近似对应的子空间中，解决了这个问题。SC-RCD的收敛速度不受谱异常值影响，是一种有效且内存高效的求解器，适用于具有快速衰减谱的大规模线性系统。", "motivation": "经典的随机坐标下降（RCD）方法虽然简单轻量，但在处理存在大谱异常值的线性系统时，其收敛速度会迅速恶化，尽管它对条件良好的矩阵很有效。", "method": "本文引入了子空间约束随机坐标下降（SC-RCD）方法，该方法将RCD的动态限制在一个仿射子空间中，该子空间对应于通过RPCholesky算法高效计算的列Nyström近似。此外，还为sketch-and-project方法开发了一个更通用的子空间约束框架，该框架推广了随机Kaczmarz和坐标下降等算法。", "result": "SC-RCD的收敛速度不受大谱异常值的影响。它是一种有效且内存高效的求解器，适用于求解具有快速衰减谱的大规模密集线性系统（例如核岭回归中遇到的系统）。实验验证和与其他求解器的比较证明了SC-RCD的效率。", "conclusion": "SC-RCD为具有谱异常值的线性系统提供了一种鲁棒且高效的解决方案，克服了传统RCD的局限性。所开发的通用子空间约束框架为各种迭代求解器提供了一种灵活的隐式预处理策略。", "translation": "随机坐标下降 (RCD) 方法是一种经典的算法，具有简单、轻量级的迭代，广泛用于各种优化问题，包括正半定线性系统的求解。作为一种线性求解器，RCD 在矩阵条件良好时特别有效；然而，在存在大谱异常值时，其收敛速度会迅速恶化。在本文中，我们引入了子空间约束随机坐标下降 (SC-RCD) 方法，其中 RCD 的动态被限制在一个仿射子空间中，该子空间对应于列 Nyström 近似，并使用最近分析的 RPCholesky 算法高效计算。我们证明 SC-RCD 的收敛速度不受大谱异常值的影响，使其成为解决具有快速衰减谱的大规模密集线性系统（例如核岭回归中遇到的系统）的有效且内存高效的求解器。实验验证以及与基于坐标下降和共轭梯度法的相关求解器的比较证明了 SC-RCD 的效率。我们的理论结果是通过为 sketch-and-project 方法开发一个更通用的子空间约束框架而得出的。这个框架推广了流行的算法，例如随机 Kaczmarz 和坐标下降，并为各种迭代求解器提供了一种灵活的隐式预处理策略，这可能具有独立的意义。", "summary": "本文提出了一种改进的线性系统求解算法——子空间约束随机坐标下降（SC-RCD）。与受谱异常值影响的传统RCD不同，SC-RCD将其动态限制在Nyström近似的子空间中，从而实现了对这些异常值鲁棒的收敛速度。经验证，SC-RCD是一种有效且内存高效的求解器，适用于具有快速衰减谱的大规模密集系统。该工作还为迭代求解器引入了一个通用的子空间约束框架。", "keywords": "子空间约束随机坐标下降, 线性系统, 低秩近似, 谱异常值, Nyström近似", "comments": "该论文的创新之处在于通过引入源自Nyström近似的子空间约束，使RCD能够处理谱异常值，这是一种巧妙的隐式预处理方法。将该方法推广到更广泛的sketch-and-project方法的子空间约束框架也是一项重要的理论贡献，有望为其他迭代求解器提供灵活的预处理策略。这可能在机器学习和数值线性代数的大规模应用中具有广泛的意义。"}}
{"id": "2506.09709", "title": "Training-Free Voice Conversion with Factorized Optimal Transport", "authors": ["Alexander Lobashev", "Assel Yermekova", "Maria Larchenko"], "summary": "This paper introduces Factorized MKL-VC, a training-free modification for\nkNN-VC pipeline. In contrast with original pipeline, our algorithm performs\nhigh quality any-to-any cross-lingual voice conversion with only 5 second of\nreference audio. MKL-VC replaces kNN regression with a factorized optimal\ntransport map in WavLM embedding subspaces, derived from Monge-Kantorovich\nLinear solution. Factorization addresses non-uniform variance across\ndimensions, ensuring effective feature transformation. Experiments on\nLibriSpeech and FLEURS datasets show MKL-VC significantly improves content\npreservation and robustness with short reference audio, outperforming kNN-VC.\nMKL-VC achieves performance comparable to FACodec, especially in cross-lingual\nvoice conversion domain.", "comment": "Interspeech 2025", "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.09709v1", "AI": {"title_translation": "基于分解最优传输的免训练语音转换", "tldr": "本文介绍了Factorized MKL-VC，一种免训练的语音转换方法，它使用分解最优传输，即使在短参考音频和跨语言转换下也能表现良好。", "motivation": "本研究旨在改进kNN-VC管道，实现高质量的任意到任意的跨语言语音转换，且仅需5秒参考音频。它旨在解决现有kNN-VC管道的局限性。", "method": "本文提出的Factorized MKL-VC方法是对kNN-VC管道的免训练修改。它用Monge-Kantorovich线性解导出的WavLM嵌入子空间中的分解最优传输映射取代了kNN回归。分解解决了维度间非均匀方差的问题，确保了有效的特征转换。", "result": "在LibriSpeech和FLEURS数据集上的实验表明，MKL-VC显著提高了内容保留和短参考音频的鲁棒性，优于kNN-VC。MKL-VC实现了与FACodec相当的性能，尤其是在跨语言语音转换领域。", "conclusion": "MKL-VC是一种有效的免训练语音转换方法，与现有的kNN-VC等方法相比，它提供了改进的性能，尤其适用于跨语言任务和有限参考音频的情况，并且与FACodec的性能相当。", "translation": "本文介绍了Factorized MKL-VC，一种对kNN-VC管道的免训练修改。与原始管道相比，我们的算法仅需5秒参考音频即可执行高质量的任意到任意跨语言语音转换。MKL-VC用WavLM嵌入子空间中由Monge-Kantorovich线性解导出的分解最优传输映射取代了kNN回归。分解解决了维度间非均匀方差的问题，确保了有效的特征转换。在LibriSpeech和FLEURS数据集上的实验表明，MKL-VC显著提高了内容保留和短参考音频的鲁棒性，优于kNN-VC。MKL-VC实现了与FACodec相当的性能，尤其是在跨语言语音转换领域。", "summary": "本文介绍了Factorized MKL-VC，一种免训练的语音转换方法，它通过在WavLM嵌入子空间中使用分解最优传输映射来修改kNN-VC。这种方法仅需5秒参考音频即可实现高质量、任意到任意的跨语言语音转换。实验证明，与kNN-VC相比，MKL-VC在内容保留和鲁棒性方面表现更优，并且在跨语言任务上与FACodec性能相当。", "keywords": "免训练, 语音转换, 最优传输, kNN-VC, 跨语言", "comments": "该论文的创新之处在于将分解最优传输应用于kNN-VC管道，使其免训练且仅需少量参考音频即可高效运行。其跨语言能力是一个显著的优势。"}}
{"id": "2506.09580", "title": "The Everyday Security of Living with Conflict", "authors": ["Jessica McClearn", "Reem Talhouk", "Rikke Bjerg Jensen"], "summary": "When `cyber' is used as a prefix, attention is typically drawn to the\ntechnological and spectacular aspects of war and conflict -- and, by extension,\nsecurity. We offer a different approach to engaging with and understanding\nsecurity in such contexts, by foregrounding the everyday -- mundane --\nexperiences of security within communities living with and fleeing from war. We\ndo so through three vignettes from our field research in Colombia, Lebanon and\nSweden, respectively, and by highlighting the significance of ethnography for\nsecurity research with communities living in regions afflicted by war. We\nconclude by setting out a call to action for security researchers and\npractitioners to consider such lived experiences in the design of security\ntechnology that aims to cater to the needs of communities in `global conflict\nand disaster regions'.", "comment": "Published in IEEE Security and Privacy Magazine", "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.09580v1", "AI": {"title_translation": "冲突生活中的日常安全", "tldr": "本文通过哥伦比亚、黎巴嫩和瑞典的案例研究，强调了在冲突地区，日常安全体验和人种学对安全研究的重要性，并呼吁安全研究人员和实践者在设计安全技术时考虑这些生活经验。", "motivation": "传统的“网络”安全研究倾向于关注战争和冲突的技术和引人注目的方面，而本文旨在提供一种不同的方法，通过关注生活在冲突中或逃离冲突的社区的日常安全体验来理解安全。", "method": "研究通过在哥伦比亚、黎巴巴嫩和瑞典进行的实地研究中的三个小插曲（vignettes）来展开，并强调了人种学对于与受战争影响社区进行安全研究的重要性。", "result": "本文通过案例研究展示了日常安全体验的重要性，并强调了人种学在理解冲突地区社区安全方面的关键作用。", "conclusion": "论文呼吁安全研究人员和实践者在为“全球冲突和灾难地区”的社区设计安全技术时，应考虑这些实际的生活经验。", "translation": "当“网络”被用作前缀时，人们的注意力通常会集中在战争和冲突的技术和引人注目的方面——以及由此延伸的安全。我们通过突出生活在战争中和逃离战争的社区中日常的、世俗的安全经验，提供了一种不同的方法来参与和理解这些背景下的安全。我们通过在哥伦比亚、黎巴巴嫩和瑞典的实地研究中的三个小插曲来做到这一点，并强调了人种学对于与受战争影响社区进行安全研究的重要性。最后，我们呼吁安全研究人员和实践者在设计旨在满足“全球冲突和灾难地区”社区需求的安全技术时，考虑这些生活经验。", "summary": "本文提出了一种不同于传统关注技术和引人注目的视角，来理解冲突背景下的安全问题。通过哥伦比亚、黎巴嫩和瑞典的实地研究案例，作者强调了生活在冲突地区或逃离冲突的社区中日常安全体验的重要性，并指出人种学在安全研究中的关键作用。论文最后呼吁安全技术的设计应充分考虑受冲突影响社区的实际生活经验。", "keywords": "日常安全, 冲突, 人种学, 社区, 生活经验", "comments": "这篇论文通过将焦点从宏大的“网络”安全转向冲突地区社区的日常安全体验，提供了一个新颖且重要的人本视角。它强调了人种学研究方法在理解复杂社会背景下安全需求的价值，对于设计更贴近用户实际需求的安全技术具有指导意义。"}}
{"id": "2506.09444", "title": "Design of an innovative robotic surgical instrument for circular stapling", "authors": ["Paul Tucan", "Nadim Al Hajjar", "Calin Vaida", "Alexandru Pusca", "Tiberiu Antal", "Corina Radu", "Daniel Jucan", "Adrian Pisla", "Damien Chablat", "Doina Pisla"], "summary": "Esophageal cancer remains a highly aggressive malignancy with low survival\nrates, requiring advanced surgical interventions like esophagectomy.\nTraditional manual techniques, including circular staplers, face challenges\nsuch as limited precision, prolonged recovery times, and complications like\nleaks and tissue misalignment. This paper presents a novel robotic circular\nstapler designed to enhance the dexterity in confined spaces, improve tissue\nalignment, and reduce post-operative risks. Integrated with a cognitive robot\nthat serves as a surgeon's assistant, the surgical stapler uses three actuators\nto perform anvil motion, cutter/stapler motion and allows a 75-degree bending\nof the cartridge (distal tip). Kinematic analysis is used to compute the\nstapler tip's position, ensuring synchronization with a robotic system.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.09444v1", "AI": {"title_translation": "创新型机器人环形吻合器械的设计", "tldr": "本文提出了一种新型机器人环形吻合器械，旨在解决食管癌手术中传统手动吻合器精度和并发症问题，提高手术效果。", "motivation": "食管癌是一种侵袭性强的恶性肿瘤，生存率低，需要食管切除术等先进外科干预。传统手动环形吻合器存在精度有限、恢复时间长以及渗漏和组织错位等并发症的挑战。", "method": "本文提出了一种新型机器人环形吻合器，旨在增强狭窄空间内的灵活性，改善组织对齐，并降低术后风险。该器械与认知机器人集成，作为外科医生助手，使用三个执行器控制砧座运动、切割/吻合器运动，并允许弹药筒（远端尖端）75度弯曲。同时使用运动学分析计算吻合器尖端位置，确保与机器人系统同步。", "result": "该设计旨在增强狭窄空间内的灵活性，改善组织对齐，并降低术后风险。", "conclusion": "Not mentioned in abstract", "translation": "食管癌仍然是一种高度侵袭性的恶性肿瘤，生存率低，需要食管切除术等先进的外科干预。传统的手动技术，包括环形吻合器，面临着精度有限、恢复时间长以及渗漏和组织错位等并发症的挑战。本文提出了一种新型机器人环形吻合器，旨在增强狭窄空间内的灵活性，改善组织对齐，并降低术后风险。该手术吻合器与作为外科医生助手的认知机器人集成，使用三个执行器执行砧座运动、切割/吻合器运动，并允许弹药筒（远端尖端）75度弯曲。运动学分析用于计算吻合器尖端的位置，确保与机器人系统同步。", "summary": "本文介绍了一种新型机器人环形吻合器，旨在解决传统手动吻合器在食管癌手术中面临的精度和并发症问题。该器械集成了认知机器人，通过三个执行器控制砧座、切割/吻合，并实现75度弯曲，同时利用运动学分析确保与机器人系统同步，以期提高手术精度、改善组织对齐并减少术后风险。", "keywords": "机器人手术器械, 环形吻合器, 食管癌, 运动学分析, 认知机器人", "comments": "这篇论文提出了一种创新的机器人辅助手术器械，旨在解决传统手动环形吻合器在食管癌手术中的固有挑战。其创新点在于将机器人技术与环形吻合器相结合，通过精密的执行器控制和运动学分析，有望显著提高手术精度和安全性。这项研究对于提高食管癌患者的预后具有重要意义。"}}
{"id": "2506.09113", "title": "Seedance 1.0: Exploring the Boundaries of Video Generation Models", "authors": ["Yu Gao", "Haoyuan Guo", "Tuyen Hoang", "Weilin Huang", "Lu Jiang", "Fangyuan Kong", "Huixia Li", "Jiashi Li", "Liang Li", "Xiaojie Li", "Xunsong Li", "Yifu Li", "Shanchuan Lin", "Zhijie Lin", "Jiawei Liu", "Shu Liu", "Xiaonan Nie", "Zhiwu Qing", "Yuxi Ren", "Li Sun", "Zhi Tian", "Rui Wang", "Sen Wang", "Guoqiang Wei", "Guohong Wu", "Jie Wu", "Ruiqi Xia", "Fei Xiao", "Xuefeng Xiao", "Jiangqiao Yan", "Ceyuan Yang", "Jianchao Yang", "Runkai Yang", "Tao Yang", "Yihang Yang", "Zilyu Ye", "Xuejiao Zeng", "Yan Zeng", "Heng Zhang", "Yang Zhao", "Xiaozheng Zheng", "Peihao Zhu", "Jiaxin Zou", "Feilong Zuo"], "summary": "Notable breakthroughs in diffusion modeling have propelled rapid improvements\nin video generation, yet current foundational model still face critical\nchallenges in simultaneously balancing prompt following, motion plausibility,\nand visual quality. In this report, we introduce Seedance 1.0, a\nhigh-performance and inference-efficient video foundation generation model that\nintegrates several core technical improvements: (i) multi-source data curation\naugmented with precision and meaningful video captioning, enabling\ncomprehensive learning across diverse scenarios; (ii) an efficient architecture\ndesign with proposed training paradigm, which allows for natively supporting\nmulti-shot generation and jointly learning of both text-to-video and\nimage-to-video tasks. (iii) carefully-optimized post-training approaches\nleveraging fine-grained supervised fine-tuning, and video-specific RLHF with\nmulti-dimensional reward mechanisms for comprehensive performance improvements;\n(iv) excellent model acceleration achieving ~10x inference speedup through\nmulti-stage distillation strategies and system-level optimizations. Seedance\n1.0 can generate a 5-second video at 1080p resolution only with 41.4 seconds\n(NVIDIA-L20). Compared to state-of-the-art video generation models, Seedance\n1.0 stands out with high-quality and fast video generation having superior\nspatiotemporal fluidity with structural stability, precise instruction\nadherence in complex multi-subject contexts, native multi-shot narrative\ncoherence with consistent subject representation.", "comment": "Seedance 1.0 Technical Report", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09113v1", "AI": {"title_translation": "Seedance 1.0: 探索视频生成模型的边界", "tldr": "Seedance 1.0是一个高性能、推理高效的视频生成模型，通过多源数据、高效架构、优化后训练和模型加速，实现了高质量、快速的视频生成，解决了现有模型在提示遵循、运动合理性和视觉质量方面的挑战。", "motivation": "现有视频生成基础模型在同时平衡提示遵循、运动合理性和视觉质量方面面临严峻挑战。", "method": "Seedance 1.0集成了多项核心技术改进：(i) 增强了精度和有意义视频字幕的多源数据整理，实现了跨多样场景的全面学习；(ii) 具有提议训练范式的高效架构设计，原生支持多镜头生成，并共同学习文本到视频和图像到视频任务；(iii) 仔细优化的后训练方法，利用细粒度监督微调和带有多维度奖励机制的视频特定RLHF，实现全面的性能改进；(iv) 通过多阶段蒸馏策略和系统级优化实现约10倍推理加速。", "result": "Seedance 1.0可以在41.4秒内生成一个5秒的1080p分辨率视频（NVIDIA-L20）。与最先进的视频生成模型相比，Seedance 1.0在高质量和快速视频生成方面表现突出，具有卓越的时空流畅性和结构稳定性，在复杂多主题背景下精确遵循指令，以及原生多镜头叙事连贯性和一致的主题表示。", "conclusion": "Seedance 1.0显著提升了视频生成模型的性能和效率，在质量、速度和复杂场景处理方面超越了现有技术。", "translation": "扩散模型方面的显著突破推动了视频生成的快速改进，然而当前的基础模型在同时平衡提示遵循、运动合理性和视觉质量方面仍面临严峻挑战。在本报告中，我们介绍了Seedance 1.0，一个高性能、推理高效的视频基础生成模型，它集成了几项核心技术改进：(i) 增强了精度和有意义视频字幕的多源数据整理，实现了跨多样场景的全面学习；(ii) 具有提议训练范式的高效架构设计，原生支持多镜头生成，并共同学习文本到视频和图像到视频任务；(iii) 仔细优化的后训练方法，利用细粒度监督微调和带有多维度奖励机制的视频特定RLHF，实现全面的性能改进；(iv) 通过多阶段蒸馏策略和系统级优化实现约10倍推理加速的卓越模型加速。Seedance 1.0仅需41.4秒（NVIDIA-L20）即可生成一个5秒的1080p分辨率视频。与最先进的视频生成模型相比，Seedance 1.0在高质量和快速视频生成方面表现突出，具有卓越的时空流畅性和结构稳定性，在复杂多主题背景下精确遵循指令，以及原生多镜头叙事连贯性和一致的主题表示。", "summary": "本文介绍了Seedance 1.0，一个旨在解决当前视频生成模型在提示遵循、运动合理性和视觉质量方面挑战的高性能、推理高效的基础模型。它通过多源数据整理、高效架构设计（支持多镜头生成和文本/图像到视频任务）、优化的后训练方法（包括RLHF）以及模型加速（10倍推理速度提升）等核心技术改进，实现了在NVIDIA-L20上41.4秒生成5秒1080p视频的能力。Seedance 1.0在视频质量、速度、时空流畅性、结构稳定性、指令遵循和多镜头叙事连贯性方面超越了现有SOTA模型。", "keywords": "视频生成, 扩散模型, Seedance 1.0, 推理效率, 多镜头生成", "comments": "Seedance 1.0的创新之处在于其整合了多项技术改进，包括多源数据处理、高效架构、高级后训练策略（如RLHF）以及显著的模型加速。这使其不仅提升了视频生成的质量和速度，还解决了复杂场景下的指令遵循和多镜头叙事连贯性问题，这些都是当前视频生成领域的关键挑战。其在推理效率上的显著提升（10倍加速）对于实际应用具有重要意义。"}}
{"id": "2506.09985", "title": "V-JEPA 2: Self-Supervised Video Models Enable Understanding, Prediction and Planning", "authors": ["Mido Assran", "Adrien Bardes", "David Fan", "Quentin Garrido", "Russell Howes", "Mojtaba", "Komeili", "Matthew Muckley", "Ammar Rizvi", "Claire Roberts", "Koustuv Sinha", "Artem Zholus", "Sergio Arnaud", "Abha Gejji", "Ada Martin", "Francois Robert Hogan", "Daniel Dugas", "Piotr Bojanowski", "Vasil Khalidov", "Patrick Labatut", "Francisco Massa", "Marc Szafraniec", "Kapil Krishnakumar", "Yong Li", "Xiaodong Ma", "Sarath Chandar", "Franziska Meier", "Yann LeCun", "Michael Rabbat", "Nicolas Ballas"], "summary": "A major challenge for modern AI is to learn to understand the world and learn\nto act largely by observation. This paper explores a self-supervised approach\nthat combines internet-scale video data with a small amount of interaction data\n(robot trajectories), to develop models capable of understanding, predicting,\nand planning in the physical world. We first pre-train an action-free\njoint-embedding-predictive architecture, V-JEPA 2, on a video and image dataset\ncomprising over 1 million hours of internet video. V-JEPA 2 achieves strong\nperformance on motion understanding (77.3 top-1 accuracy on Something-Something\nv2) and state-of-the-art performance on human action anticipation (39.7\nrecall-at-5 on Epic-Kitchens-100) surpassing previous task-specific models.\nAdditionally, after aligning V-JEPA 2 with a large language model, we\ndemonstrate state-of-the-art performance on multiple video question-answering\ntasks at the 8 billion parameter scale (e.g., 84.0 on PerceptionTest, 76.9 on\nTempCompass). Finally, we show how self-supervised learning can be applied to\nrobotic planning tasks by post-training a latent action-conditioned world\nmodel, V-JEPA 2-AC, using less than 62 hours of unlabeled robot videos from the\nDroid dataset. We deploy V-JEPA 2-AC zero-shot on Franka arms in two different\nlabs and enable picking and placing of objects using planning with image goals.\nNotably, this is achieved without collecting any data from the robots in these\nenvironments, and without any task-specific training or reward. This work\ndemonstrates how self-supervised learning from web-scale data and a small\namount of robot interaction data can yield a world model capable of planning in\nthe physical world.", "comment": "48 pages, 19 figures", "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.09985v1", "AI": {"title_translation": "V-JEPA 2：自监督视频模型实现理解、预测和规划", "tldr": "V-JEPA 2是一个自监督视频模型，通过大规模网络视频和少量机器人交互数据，实现了对物理世界的理解、预测和零样本规划。", "motivation": "现代AI面临的主要挑战是如何主要通过观察来学习理解世界和学习行动。本文旨在探索一种自监督方法，结合互联网规模的视频数据和少量交互数据（机器人轨迹），开发能够理解、预测和规划物理世界的模型。", "method": "首先，使用包含超过100万小时互联网视频的视频和图像数据集，预训练一个无动作的联合嵌入预测架构V-JEPA 2。将V-JEPA 2与大型语言模型对齐。通过对V-JEPA 2进行后训练，使用来自Droid数据集的少于62小时的未标记机器人视频，构建一个潜在动作条件世界模型V-JEPA 2-AC，以应用于机器人规划任务。在两个不同实验室的Franka机械臂上零样本部署V-JEPA 2-AC，实现基于图像目标的抓取和放置。", "result": "V-JEPA 2在运动理解方面表现出色（Something-Something v2上77.3%的top-1准确率）。在人类动作预测方面达到最先进性能（Epic-Kitchens-100上39.7%的recall-at-5），超越了之前的任务特定模型。在80亿参数规模下，与大型语言模型对齐后，在多个视频问答任务上展示了最先进的性能（例如，PerceptionTest上84.0%，TempCompass上76.9%）。V-JEPA 2-AC在没有收集任何环境数据、没有任务特定训练或奖励的情况下，实现了零样本的机器人抓取和放置。", "conclusion": "这项工作表明，从网络规模数据和少量机器人交互数据中进行的自监督学习可以产生一个能够在物理世界中进行规划的世界模型。", "translation": "现代AI面临的一个主要挑战是如何主要通过观察来学习理解世界和学习行动。本文探索了一种自监督方法，该方法将互联网规模的视频数据与少量交互数据（机器人轨迹）相结合，以开发能够在物理世界中理解、预测和规划的模型。我们首先在一个包含超过100万小时互联网视频的视频和图像数据集上，预训练一个无动作的联合嵌入预测架构V-JEPA 2。V-JEPA 2在运动理解方面取得了强大的性能（Something-Something v2上77.3%的top-1准确率），并在人类动作预测方面达到了最先进的性能（Epic-Kitchens-100上39.7%的recall-at-5），超越了之前的任务特定模型。此外，在将V-JEPA 2与大型语言模型对齐后，我们展示了在80亿参数规模下，在多个视频问答任务上（例如，PerceptionTest上84.0%，TempCompass上76.9%）的最先进性能。最后，我们展示了如何通过使用来自Droid数据集的少于62小时的未标记机器人视频，对潜在动作条件世界模型V-JEPA 2-AC进行后训练，将自监督学习应用于机器人规划任务。我们在两个不同实验室的Franka机械臂上零样本部署了V-JEPA 2-AC，并能够使用图像目标规划来实现物体的抓取和放置。值得注意的是，这在没有收集这些环境中任何机器人数据、没有进行任何任务特定训练或奖励的情况下实现。这项工作表明，从网络规模数据和少量机器人交互数据中进行的自监督学习可以产生一个能够在物理世界中进行规划的世界模型。", "summary": "本文提出了V-JEPA 2，一个自监督视频模型，通过预训练在大量网络视频数据上，并在少量机器人交互数据上进行后训练，使其能够理解、预测和规划物理世界。V-JEPA 2在运动理解、人类动作预测和视频问答方面表现出色。通过其变体V-JEPA 2-AC，该模型实现了零样本的机器人抓取和放置任务，展示了自监督学习在构建通用世界模型方面的潜力，无需大量任务特定数据。", "keywords": "自监督学习, 视频模型, 世界模型, 机器人规划, V-JEPA 2", "comments": "本文的创新之处在于结合了互联网规模的视频数据和少量机器人交互数据进行自监督学习，从而构建了一个能够跨越理解、预测和规划多模态任务的通用世界模型。其零样本机器人规划能力尤其引人注目，表明了该方法在减少机器人领域数据依赖方面的巨大潜力。这对于实现更通用、更自主的AI系统具有重要意义。"}}
{"id": "2506.09099", "title": "Too Big to Think: Capacity, Memorization, and Generalization in Pre-Trained Transformers", "authors": ["Joshua Barron", "Devin White"], "summary": "The relationship between memorization and generalization in large language\nmodels (LLMs) remains an open area of research, with growing evidence that the\ntwo are deeply intertwined. In this work, we investigate this relationship by\npre-training a series of capacity-limited Transformer models from scratch on\ntwo synthetic character-level tasks designed to separately probe generalization\n(via arithmetic extrapolation) and memorization (via factual recall). We\nobserve a consistent trade-off: small models extrapolate to unseen arithmetic\ncases but fail to memorize facts, while larger models memorize but fail to\nextrapolate. An intermediate-capacity model exhibits a similar shift toward\nmemorization. When trained on both tasks jointly, no model (regardless of size)\nsucceeds at extrapolation. These findings suggest that pre-training may\nintrinsically favor one learning mode over the other. By isolating these\ndynamics in a controlled setting, our study offers insight into how model\ncapacity shapes learning behavior and offers broader implications for the\ndesign and deployment of small language models.", "comment": "Accepted for oral presentation to Tiny Titans: The next wave of\n  On-Device Learning for Foundational Models Workshop at the 42nd International\n  Conference on Machine Learning", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09099v1", "AI": {"title_translation": "太大而无法思考：预训练Transformer中的容量、记忆化和泛化", "tldr": "本研究通过在合成任务上预训练容量受限的Transformer模型，发现大型语言模型在记忆化和泛化之间存在权衡：小模型善于泛化但不善于记忆，大模型善于记忆但不善于泛化。联合训练时，模型在泛化方面表现不佳。", "motivation": "大型语言模型（LLMs）中记忆化和泛化之间的关系仍是未解之谜，且有证据表明两者密切相关。本研究旨在深入探讨这种关系。", "method": "研究人员从头开始预训练了一系列容量受限的Transformer模型，并在两个合成的字符级任务上进行训练：一个用于探测泛化能力（通过算术外推），另一个用于探测记忆能力（通过事实回忆）。", "result": "研究发现了一个持续的权衡：小模型能外推到未见的算术情况但无法记住事实，而大模型能记住事实但无法外推。中等容量的模型也表现出向记忆化的类似转变。当同时在两项任务上进行训练时，无论模型大小，都没有模型能在外推任务上取得成功。", "conclusion": "这些发现表明预训练可能内在偏向于一种学习模式而非另一种。本研究通过在受控环境中分离这些动态，揭示了模型容量如何塑造学习行为，并对小型语言模型的设计和部署具有更广泛的启示。", "translation": "大型语言模型（LLMs）中记忆化和泛化之间的关系仍然是一个开放的研究领域，并且有越来越多的证据表明两者之间存在着深刻的相互关联。在这项工作中，我们通过从头开始在一系列容量受限的Transformer模型上进行预训练来研究这种关系。这些模型在两个合成的字符级任务上进行训练，这些任务旨在分别探测泛化能力（通过算术外推）和记忆能力（通过事实回忆）。我们观察到一种持续的权衡：小模型能够外推到未见的算术情况但无法记住事实，而大模型能够记住事实但无法外推。一个中等容量的模型也表现出向记忆化的类似转变。当同时在两项任务上进行训练时，无论模型大小，都没有模型能在外推任务上取得成功。这些发现表明预训练可能内在偏向于一种学习模式而非另一种。通过在受控环境中分离这些动态，我们的研究为模型容量如何塑造学习行为提供了见解，并对小型语言模型的设计和部署具有更广泛的启示。", "summary": "本研究探讨了预训练Transformer模型中容量、记忆化和泛化之间的关系。通过在合成任务上训练不同容量的模型，研究发现小模型倾向于泛化而记忆能力弱，大模型则倾向于记忆而泛化能力差，存在一个明显的权衡。当模型同时训练泛化和记忆任务时，无论大小，都无法成功泛化。这表明预训练过程可能内在偏向于某种学习模式，为理解模型容量如何影响学习行为以及未来小型语言模型的设计提供了重要启示。", "keywords": "容量, 记忆化, 泛化, Transformer, 预训练", "comments": "这项研究通过受控的合成任务，清晰地揭示了预训练Transformer模型在记忆化和泛化能力之间存在的内在权衡，这对于理解大型语言模型的学习机制具有重要意义。其发现预训练可能偏向某种学习模式的观点，对未来模型架构设计和训练策略的优化提供了宝贵的指导，尤其是在平衡模型大小和性能方面。"}}
{"id": "2506.09870", "title": "Private Aggregation for Byzantine-Resilient Heterogeneous Federated Learning", "authors": ["Maximilian Egger", "Rawad Bitar"], "summary": "Ensuring resilience to Byzantine clients while maintaining the privacy of the\nclients' data is a fundamental challenge in federated learning (FL). When the\nclients' data is homogeneous, suitable countermeasures were studied from an\ninformation-theoretic perspective utilizing secure aggregation techniques while\nensuring robust aggregation of the clients' gradients. However, the\ncountermeasures used fail when the clients' data is heterogeneous. Suitable\npre-processing techniques, such as nearest neighbor mixing, were recently shown\nto enhance the performance of those countermeasures in the heterogeneous\nsetting. Nevertheless, those pre-processing techniques cannot be applied with\nthe introduced privacy-preserving mechanisms.\n  We propose a multi-stage method encompassing a careful co-design of\nverifiable secret sharing, secure aggregation, and a tailored symmetric private\ninformation retrieval scheme to achieve information-theoretic privacy\nguarantees and Byzantine resilience under data heterogeneity. We evaluate the\neffectiveness of our scheme on a variety of attacks and show how it outperforms\nthe previously known techniques. Since the communication overhead of secure\naggregation is non-negligible, we investigate the interplay with zero-order\nestimation methods that reduce the communication cost in state-of-the-art FL\ntasks and thereby make private aggregation scalable.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09870v1", "AI": {"title_translation": "拜占庭容错异构联邦学习中的私有聚合", "tldr": "本文提出了一种多阶段方法，结合了可验证秘密共享、安全聚合和定制的对称私有信息检索，以在数据异构性下实现信息理论隐私保证和拜占庭容错，并优于现有技术。", "motivation": "在联邦学习（FL）中，确保对拜占庭客户端的弹性同时维护客户端数据的隐私是一个基本挑战。现有的针对同构数据的对策在异构数据下会失效，且适合的预处理技术无法与隐私保护机制结合使用。", "method": "本文提出了一种多阶段方法，精心协同设计了可验证秘密共享、安全聚合和定制的对称私有信息检索方案，以在数据异构性下实现信息理论隐私保证和拜占庭容错。此外，还研究了与零阶估计方法的相互作用，以降低通信成本并使私有聚合具有可扩展性。", "result": "该方案在各种攻击下进行了有效性评估，结果表明它优于以前已知的方法。", "conclusion": "该研究成功地提出了一种在数据异构性下实现信息理论隐私保证和拜占庭容错的私有聚合方案，并展示了其优越的性能和可扩展性。", "translation": "确保对拜占庭客户端的弹性同时维护客户端数据的隐私是联邦学习（FL）中的一个基本挑战。当客户端数据是同构时，已经从信息论角度研究了利用安全聚合技术同时确保客户端梯度鲁棒聚合的合适对策。然而，当客户端数据是异构时，所使用的对策会失效。最近的研究表明，合适的预处理技术，如最近邻混合，可以提高这些对策在异构设置下的性能。然而，这些预处理技术不能与所引入的隐私保护机制一起应用。\n我们提出了一种多阶段方法，包括可验证秘密共享、安全聚合和定制的对称私有信息检索方案的精心协同设计，以在数据异构性下实现信息理论隐私保证和拜占庭容错。我们评估了我们方案在各种攻击下的有效性，并展示了它如何优于以前已知的方法。由于安全聚合的通信开销不可忽略，我们研究了其与零阶估计方法的相互作用，这些方法可以降低最先进FL任务中的通信成本，从而使私有聚合具有可扩展性。", "summary": "本文针对联邦学习中数据异构性下拜占庭客户端的弹性与隐私保护的挑战，提出了一种多阶段私有聚合方案。该方案结合了可验证秘密共享、安全聚合和定制的对称私有信息检索，实现了信息理论隐私保证和拜占庭容错。实验结果表明，该方法优于现有技术，并且通过与零阶估计方法的结合，有效降低了通信开销，提高了可扩展性。", "keywords": "联邦学习, 拜占庭容错, 异构数据, 私有聚合, 安全聚合", "comments": "Not mentioned in abstract"}}
{"id": "2506.09759", "title": "Towards Bridging Formal Methods and Human Interpretability", "authors": ["Abhijit Paul", "Proma Chowdhury", "Kazi Sakib"], "summary": "Labeled Transition Systems (LTS) are integral to model checking and design\nrepair tools. System engineers frequently examine LTS designs during model\nchecking or design repair to debug, identify inconsistencies, and validate\nsystem behavior. Despite LTS's significance, no prior research has examined\nhuman comprehension of these designs. To address this, we draw on traditional\nsoftware engineering and graph theory, identifying 7 key metrics: cyclomatic\ncomplexity, state space size, average branching factor, maximum depth, Albin\ncomplexity, modularity, and redundancy. We created a dataset of 148 LTS\ndesigns, sampling 48 for 324 paired comparisons, and ranked them using the\nBradley-Terry model. Through Kendall's Tau correlation analysis, we found that\nAlbin complexity ($\\tau = 0.444$), state space size ($\\tau = 0.420$),\ncyclomatic complexity ($\\tau = 0.366$), and redundancy ($\\tau = 0.315$) most\naccurately reflect human comprehension of LTS designs. To showcase the metrics'\nutility, we applied the Albin complexity metric within the Fortis design repair\ntool, ranking system redesigns. This ranking reduced annotators' comprehension\ntime by 39\\%, suggesting that metrics emphasizing human factors can enhance\nformal design interpretability.", "comment": "Need to improve data annotation process in methodology section", "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.09759v1", "AI": {"title_translation": "弥合形式化方法与人类可解释性之间的鸿沟", "tldr": "研究人员首次探索LTS设计的人类理解度，通过识别七个关键指标并进行实证分析，发现Albin复杂度、状态空间大小、圈复杂度及冗余度与人类理解度高度相关，并将Albin复杂度应用于设计修复工具，显著提高了理解效率。", "motivation": "尽管LTS在模型检测和设计修复中至关重要，但此前没有研究探讨人类对这些设计的理解，这阻碍了系统工程师在调试、识别不一致性和验证系统行为时的效率。", "method": "1. 从传统软件工程和图论中识别出7个关键指标：圈复杂度、状态空间大小、平均分支因子、最大深度、Albin复杂度、模块化和冗余。2. 创建了一个包含148个LTS设计的数据集，并对其中48个进行了324次配对比较，使用Bradley-Terry模型进行排名。3. 通过Kendall's Tau相关性分析，评估这些指标与人类理解度的相关性。4. 将Albin复杂度指标应用于Fortis设计修复工具，对系统重新设计进行排名。", "result": "1. Albin复杂度（$\\tau = 0.444$）、状态空间大小（$\\tau = 0.420$）、圈复杂度（$\\tau = 0.366$）和冗余度（$\\tau = 0.315$）最准确地反映了人类对LTS设计的理解。2. 将Albin复杂度指标应用于Fortis设计修复工具后，将注释者的理解时间减少了39%。", "conclusion": "强调人类因素的指标可以增强形式化设计的可解释性，并有效提高工程师对LTS设计的理解效率。", "translation": "标记转换系统（LTS）是模型检测和设计修复工具不可或缺的一部分。系统工程师在模型检测或设计修复过程中经常检查LTS设计，以进行调试、识别不一致性并验证系统行为。尽管LTS具有重要意义，但此前没有研究检查人类对这些设计的理解。为了解决这个问题，我们借鉴了传统软件工程和图论，确定了7个关键指标：圈复杂度、状态空间大小、平均分支因子、最大深度、Albin复杂度、模块化和冗余。我们创建了一个包含148个LTS设计的数据集，对其中48个进行了324次配对比较，并使用Bradley-Terry模型对它们进行排名。通过Kendall's Tau相关性分析，我们发现Albin复杂度（$\\tau = 0.444$）、状态空间大小（$\\tau = 0.420$）、圈复杂度（$\\tau = 0.366$）和冗余度（$\\tau = 0.315$）最准确地反映了人类对LTS设计的理解。为了展示这些指标的实用性，我们将Albin复杂度指标应用于Fortis设计修复工具，对系统重新设计进行排名。这种排名将注释者的理解时间减少了39%，表明强调人类因素的指标可以增强形式化设计的可解释性。", "summary": "本文首次探讨了标记转换系统（LTS）设计的人类理解度问题。研究者借鉴软件工程和图论，识别出圈复杂度、状态空间大小、平均分支因子、最大深度、Albin复杂度、模块化和冗余等7个关键指标。通过对148个LTS设计数据集的分析和配对比较，发现Albin复杂度、状态空间大小、圈复杂度和冗余度与人类理解LTS设计的程度高度相关。将Albin复杂度应用于设计修复工具后，显著提高了注释者的理解效率，证实了以人为本的指标能有效提升形式化设计的可解释性。", "keywords": "标记转换系统, 人类可解释性, 形式化方法, Albin复杂度, 设计修复", "comments": "这项研究具有创新性，因为它首次将人类理解度引入到形式化方法领域，填补了LTS设计可理解性研究的空白。通过量化指标并进行实证验证，为提升形式化设计的可用性和工程师的工作效率提供了具体的方法和工具。其将理论指标与实际应用相结合的验证方式，也增强了研究的实用价值。"}}
{"id": "2506.09801", "title": "Investigating the Perception of Translational Shape-Changing Haptic Interfaces", "authors": ["Qihan Yang", "Xin Zhou", "Adam J. Spiers"], "summary": "Shape-changing haptic interfaces (SCHIs) are a promising and emerging field.\nHowever, compared to more established stimulus modalities, such as vibration,\nthere is sparse literature on the perception of dynamic shapes. Furthermore,\nthe influence of properties such as grasp types and displacement\nmagnitude/direction has not been formally evaluated. This work attempts to\ninitiate a formal perceptual evaluation of SCHIs via a psychophysical user\nstudy involving a 1-DOF translational shape-changing interface that can move\nits body with 1.25-micrometer resolution. Participants completed a Method of\nConstant Stimulus study while holding the device with three different grasps.\nStimuli direction occurred both toward and away from the thumb, while the\nstandard stimuli varied between small (0.48 mm) and large (6 mm). Our results\nindicate that translational SCHIs should maximize the translation magnitude\nrather than the number of fingers in contact. We also demonstrated how to apply\nour findings to real-world applications via a simple 'paddle game', where we\ncompared conventional linear mapping with non-linear mapping derived from our\nperceptual experiment outcomes between the device position and its represented\nvalue. Results indicate that the non-linear mapping was more effective, with\nimproved error distribution. We hope this work inspires further formal\nperceptual investigation into other SCHI morphologies.", "comment": "7 pages, 8 figures. Accepted version to appear in: Proceedings of the\n  IEEE World Haptics Conference (WHC), 2025", "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.09801v1", "AI": {"title_translation": "探究平移式形变触觉界面的感知", "tldr": "本研究通过用户心理物理学实验，评估了平移式形变触觉界面的感知，发现应最大化平移幅度而非接触手指数量，并提出非线性映射在实际应用中更有效。", "motivation": "当前关于动态形状感知的文献稀少，且握持类型、位移幅度/方向等属性的影响尚未得到正式评估。", "method": "通过一项心理物理学用户研究，使用一个1-DOF平移式形变界面进行恒定刺激法研究。参与者在三种不同握持方式下进行实验，刺激方向包括朝向和远离拇指，标准刺激在小（0.48毫米）和大（6毫米）之间变化。研究还通过一个“打桨游戏”应用了研究发现，比较了传统线性映射和基于感知实验结果的非线性映射。", "result": "结果表明，平移式形变触觉界面应最大化平移幅度而非接触手指数量。在“打桨游戏”中，非线性映射比传统线性映射更有效，并改善了错误分布。", "conclusion": "本研究希望能启发对其他形变触觉界面形态的进一步正式感知研究。", "translation": "形变触觉界面（SCHIs）是一个有前景且新兴的领域。然而，与振动等更成熟的刺激模式相比，关于动态形状感知的文献稀少。此外，握持类型和位移幅度/方向等属性的影响尚未得到正式评估。本工作试图通过一项心理物理学用户研究来启动对SCHIs的正式感知评估，该研究涉及一个1-DOF平移式形变界面，其主体可以以1.25微米的分辨率移动。参与者在三种不同握持方式下完成了恒定刺激法研究。刺激方向包括朝向和远离拇指，而标准刺激在小（0.48毫米）和大（6毫米）之间变化。我们的结果表明，平移式SCHIs应最大化平移幅度而不是接触手指的数量。我们还通过一个简单的“打桨游戏”演示了如何将我们的发现应用于实际应用，在该游戏中，我们比较了传统的线性映射和从我们的感知实验结果中导出的设备位置与其表示值之间的非线性映射。结果表明，非线性映射更有效，错误分布得到改善。我们希望这项工作能启发对其他SCHI形态的进一步正式感知研究。", "summary": "本研究旨在填补动态形状感知领域研究空白，对平移式形变触觉界面（SCHIs）进行首次正式感知评估。通过一项心理物理学用户研究，实验发现平移式SCHIs应着重于最大化平移幅度而非增加手指接触面积。此外，研究还通过一个“打桨游戏”展示了如何将感知实验结果应用于实际，证明基于感知结果的非线性映射在改善错误分布方面优于传统线性映射。这项工作为未来的SCHI感知研究奠定了基础。", "keywords": "形变触觉界面, 感知, 心理物理学, 平移式触觉, 非线性映射", "comments": "这项研究具有创新性，因为它首次对平移式形变触觉界面的感知进行了正式的心理物理学评估，填补了该领域的一个重要空白。其发现，特别是关于最大化平移幅度以及非线性映射在实际应用中的有效性，为未来形变触觉界面的设计和优化提供了宝贵的指导。通过将感知研究与实际应用相结合，该工作展示了理论研究的实用价值。"}}
{"id": "2506.09685", "title": "Bridging Continuous-time LQR and Reinforcement Learning via Gradient Flow of the Bellman Error", "authors": ["Armin Gießler", "Albertus Johannes Malan", "Sören Hohmann"], "summary": "In this paper, we present a novel method for computing the optimal feedback\ngain of the infinite-horizon Linear Quadratic Regulator (LQR) problem via an\nordinary differential equation. We introduce a novel continuous-time Bellman\nerror, derived from the Hamilton-Jacobi-Bellman (HJB) equation, which\nquantifies the suboptimality of stabilizing policies and is parametrized in\nterms of the feedback gain. We analyze its properties, including its effective\ndomain, smoothness, coerciveness and show the existence of a unique stationary\npoint within the stability region. Furthermore, we derive a closed-form\ngradient expression of the Bellman error that induces a gradient flow. This\nconverges to the optimal feedback and generates a unique trajectory which\nexclusively comprises stabilizing feedback policies. Additionally, this work\nadvances interesting connections between LQR theory and Reinforcement Learning\n(RL) by redefining suboptimality of the Algebraic Riccati Equation (ARE) as a\nBellman error, adapting a state-independent formulation, and leveraging\nLyapunov equations to overcome the infinite-horizon challenge. We validate our\nmethod in a simulation and compare it to the state of the art.", "comment": "submitted to Conference on Decision and Control", "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.09685v1", "AI": {"title_translation": "通过贝尔曼误差的梯度流连接连续时间LQR和强化学习", "tldr": "本文提出了一种基于常微分方程的新方法来解决LQR问题，通过最小化连续时间贝尔曼误差，从而连接了LQR和强化学习。", "motivation": "本文旨在提出一种通过常微分方程计算无限 horizon 线性二次调节器 (LQR) 问题最优反馈增益的新方法，并进一步推进LQR理论与强化学习 (RL) 之间的联系。", "method": "研究引入了一种新的连续时间贝尔曼误差，该误差来源于Hamilton-Jacobi-Bellman (HJB) 方程，并以反馈增益为参数。通过推导贝尔曼误差的闭合形式梯度表达式，从而产生一个梯度流。该方法通过将代数Riccati方程 (ARE) 的次优性重新定义为贝尔曼误差，采用状态无关的公式，并利用Lyapunov方程来克服无限 horizon 的挑战。", "result": "所提出的梯度流收敛于最优反馈，并生成一个仅包含稳定反馈策略的唯一轨迹。该方法已在仿真中得到验证，并与现有技术进行了比较。", "conclusion": "本文提出了一种新颖的基于常微分方程的LQR方法，通过引入连续时间贝尔曼误差和梯度流，实现了最优反馈的收敛，并建立了LQR与强化学习之间的重要联系。", "translation": "本文提出了一种通过常微分方程计算无限 horizon 线性二次调节器 (LQR) 问题最优反馈增益的新方法。我们引入了一种新的连续时间贝尔曼误差，它来源于 Hamilton-Jacobi-Bellman (HJB) 方程，用于量化稳定策略的次优性，并以反馈增益为参数。我们分析了它的性质，包括其有效域、平滑性、强制性，并证明了在稳定区域内存在一个唯一的驻点。此外，我们推导了贝尔曼误差的闭合形式梯度表达式，该表达式产生了一个梯度流。这收敛于最优反馈，并生成一个唯一轨迹，该轨迹仅包含稳定反馈策略。此外，这项工作通过将代数Riccati方程 (ARE) 的次优性重新定义为贝尔曼误差，采用状态无关的公式，并利用Lyapunov方程克服无限 horizon 的挑战，从而推进了LQR理论与强化学习 (RL) 之间有趣的联系。我们在仿真中验证了我们的方法，并将其与现有技术进行了比较。", "summary": "本文提出了一种基于常微分方程（ODE）的新方法，用于计算无限 horizon 线性二次调节器（LQR）问题的最优反馈增益。研究引入了源自Hamilton-Jacobi-Bellman（HJB）方程的连续时间贝尔曼误差，该误差量化了稳定策略的次优性，并以反馈增益为参数。论文分析了该误差的性质，并推导了其闭合形式的梯度表达式，从而产生一个收敛于最优反馈并生成唯一稳定反馈策略轨迹的梯度流。此外，该工作通过将代数Riccati方程（ARE）的次优性重新定义为贝尔曼误差，采用状态无关的公式，并利用Lyapunov方程克服无限 horizon 的挑战，从而建立了LQR理论与强化学习（RL）之间的联系。该方法已通过仿真验证。", "keywords": "LQR, 强化学习, 贝尔曼误差, 梯度流, 最优控制", "comments": "该论文通过将LQR问题置于连续时间贝尔曼误差和梯度流的框架内，提供了一种创新的方法，有效连接了LQR和强化学习。这种连接对于推动这两个领域的发展具有重要意义，尤其是在利用Lyapunov方程解决无限 horizon 问题方面。梯度流所生成策略的收敛性和稳定性证明是其关键优势。"}}
{"id": "2506.09554", "title": "Understanding the Performance and Power of LLM Inferencing on Edge Accelerators", "authors": ["Mayank Arya", "Yogesh Simmhan"], "summary": "Large Language Models (LLMs) have demonstrated exceptional benefits to a wide\nrange of domains, for tasks as diverse as code generation and robot navigation.\nWhile LLMs are usually served from cloud data centers, mission-critical and\nprivacy-sensitive applications may require local hosting of open LLM models.\nGiven the large GPU memory footprint needed for LLMs, edge accelerators such as\nNvidia Jetson Orin AGX with 64GB of shared GPU-CPU RAM are a compelling choice.\nHowever, the feasibility and performance of LLM inference on edge accelerators\nis under-explored. This study presents a detailed evaluation of LLM inference\non the NVIDIA Jetson Orin AGX, on four SOTA models ranging from 2.7B to 32.8B\nparameters, such as Meta Llama3.1, Microsoft-Phi2, Deepseek-R1-Qwen.We\ninvestigate the impact of varying batch sizes, sequence lengths, and\nquantization levels on latency, throughput, and perplexity, and also explore\nvarious custom power modes on the Orin AGX to perform power and energy\nconsumption analysis. Our findings offer interesting insights on the trade-offs\nbetween efficiency, inference speed and resource use, e.g., increasing the\nsequence length causes a decrease in token throughput and quantization causes\nsmaller LLMs to be slower. These results can help optimize LLM serving on edge\naccelerators for practical applications.", "comment": null, "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.09554v1", "AI": {"title_translation": "理解边缘加速器上LLM推理的性能和功耗", "tldr": "本研究评估了LLM在NVIDIA Jetson Orin AGX边缘加速器上的推理性能和功耗，探讨了批处理大小、序列长度和量化对延迟、吞吐量和困惑度的影响，并提供了优化边缘LLM部署的见解。", "motivation": "尽管大型语言模型（LLMs）通常在云数据中心提供服务，但任务关键型和隐私敏感型应用可能需要本地托管开放的LLM模型。鉴于LLM需要较大的GPU内存，Nvidia Jetson Orin AGX等边缘加速器是一个引人注目的选择。然而，LLM在边缘加速器上进行推理的可行性和性能尚未得到充分探索。", "method": "本研究详细评估了LLM在NVIDIA Jetson Orin AGX上的推理性能。使用了四种最先进的模型（SOTA），参数范围从2.7B到32.8B，包括Meta Llama3.1、Microsoft-Phi2和Deepseek-R1-Qwen。研究调查了不同批处理大小、序列长度和量化水平对延迟、吞吐量和困惑度的影响，并探索了Orin AGX上的各种自定义电源模式以进行功耗和能耗分析。", "result": "研究结果提供了关于效率、推理速度和资源使用之间权衡的有趣见解。例如，增加序列长度会导致token吞吐量下降，而量化会导致较小的LLM变慢。", "conclusion": "这些结果可以帮助优化LLM在边缘加速器上的服务，以应用于实际场景。", "translation": "大型语言模型 (LLM) 已在广泛的领域展现出卓越的优势，涵盖了代码生成和机器人导航等多样化任务。虽然 LLM 通常从云数据中心提供服务，但任务关键型和隐私敏感型应用可能需要本地托管开放的 LLM 模型。考虑到 LLM 所需的巨大 GPU 内存占用，Nvidia Jetson Orin AGX 等具有 64GB 共享 GPU-CPU RAM 的边缘加速器是一个引人注目的选择。然而，LLM 在边缘加速器上进行推理的可行性和性能尚未得到充分探索。本研究详细评估了 LLM 在 NVIDIA Jetson Orin AGX 上的推理性能，使用了四种 SOTA 模型，参数范围从 2.7B 到 32.8B，例如 Meta Llama3.1、Microsoft-Phi2、Deepseek-R1-Qwen。我们研究了不同批处理大小、序列长度和量化水平对延迟、吞吐量和困惑度的影响，并探讨了 Orin AGX 上的各种自定义电源模式以进行功耗和能耗分析。我们的研究结果提供了关于效率、推理速度和资源使用之间权衡的有趣见解，例如，增加序列长度会导致 token 吞吐量下降，而量化会导致较小的 LLM 变慢。这些结果有助于优化 LLM 在边缘加速器上的服务，以应用于实际场景。", "summary": "本研究评估了大型语言模型（LLMs）在NVIDIA Jetson Orin AGX边缘加速器上的推理性能和功耗。通过测试多款SOTA模型并调整批处理大小、序列长度和量化水平，论文揭示了效率、推理速度和资源使用之间的权衡，旨在为边缘设备上的LLM部署提供优化指导。", "keywords": "LLM推理, 边缘加速器, 性能评估, 功耗分析, NVIDIA Jetson Orin AGX", "comments": "这篇论文解决了LLM在边缘设备部署中的关键挑战，即性能和功耗。其创新之处在于对真实边缘加速器进行了详细的实证评估，并提供了具体的性能权衡数据，对于推动LLM在任务关键型和隐私敏感型边缘应用中的实际落地具有重要意义。"}}
{"id": "2506.09439", "title": "Eigenvalue-Based Detection in MIMO Systems for Integrated Sensing and Communication", "authors": ["Alex Obando", "Saman Atapattu", "Prathapasinghe Dharmawansa", "Akram Hourani", "Kandeepan Sithamparanathan"], "summary": "This paper considers a MIMO Integrated Sensing and Communication (ISAC)\nsystem, where a base station simultaneously serves a MIMO communication user\nand a remote MIMO sensing receiver, without channel state information (CSI) at\nthe transmitter. Existing MIMO ISAC literature often prioritizes communication\nrate or detection probability, typically under constant false-alarm rate (CFAR)\nassumptions, without jointly analyzing detection reliability and communication\nconstraints. To address this gap, we adopt an eigenvalue-based detector for\nrobust sensing and use a performance metric, the total detection error, that\njointly captures false-alarm and missed-detection probabilities. We derive\nnovel closed-form expressions for both probabilities under the eigenvalue\ndetector, enabling rigorous sensing analysis. Using these expressions, we\nformulate and solve a joint power allocation and threshold optimization problem\nthat minimizes total detection error while meeting a minimum communication rate\nrequirement. Simulation results demonstrate that the proposed joint design\nsubstantially outperforms conventional CFAR-based schemes, highlighting the\nbenefits of power- and threshold-aware optimization in MIMO ISAC systems.", "comment": "10 pages, IEEE 102nd Vehicular Technology Conference (VTC2025-Fall),\n  Chengdu, China", "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.09439v1", "AI": {"title_translation": "基于特征值的MIMO系统集成感知与通信检测", "tldr": "本文提出了一种基于特征值的检测器，用于MIMO ISAC系统，通过联合优化功率分配和阈值来最小化总检测误差，同时满足通信速率要求，优于传统CFAR方案。", "motivation": "现有MIMO ISAC文献通常优先考虑通信速率或检测概率，通常在恒定虚警率（CFAR）假设下，但没有联合分析检测可靠性和通信约束。本文旨在解决这一空白。", "method": "采用基于特征值的检测器进行鲁棒感知，并使用总检测误差作为性能指标（联合捕获虚警和漏检概率）。推导了特征值检测器下这两种概率的闭式表达式。然后，利用这些表达式，建立并解决了一个联合功率分配和阈值优化问题，旨在最小化总检测误差，同时满足最低通信速率要求。", "result": "仿真结果表明，所提出的联合设计显著优于传统的基于CFAR的方案，突出了MIMO ISAC系统中功率和阈值感知优化的优势。", "conclusion": "本文成功地通过联合优化功率分配和阈值，在MIMO ISAC系统中实现了鲁棒的基于特征值的检测，有效解决了检测可靠性和通信约束的联合分析问题，并展示了其优越性。", "translation": "本文考虑了一个MIMO集成感知与通信（ISAC）系统，其中基站同时服务一个MIMO通信用户和一个远程MIMO感知接收器，且发射端没有信道状态信息（CSI）。现有的MIMO ISAC文献通常优先考虑通信速率或检测概率，通常在恒定虚警率（CFAR）假设下，但没有联合分析检测可靠性和通信约束。为了解决这一空白，我们采用了一个基于特征值的检测器进行鲁棒感知，并使用一个性能指标——总检测误差，它联合捕获了虚警和漏检概率。我们推导了特征值检测器下这两种概率的新颖闭式表达式，从而实现了严格的感知分析。利用这些表达式，我们建立并解决了一个联合功率分配和阈值优化问题，旨在最小化总检测误差，同时满足最低通信速率要求。仿真结果表明，所提出的联合设计显著优于传统的基于CFAR的方案，突出了MIMO ISAC系统中功率和阈值感知优化的优势。", "summary": "本文研究MIMO集成感知与通信（ISAC）系统在发射端无CSI的情况下的检测问题。针对现有研究未联合分析检测可靠性和通信约束的不足，提出一种基于特征值的检测器。该方法以最小化总检测误差（包含虚警和漏检）为目标，推导了相关概率的闭式表达式，并在此基础上建立了联合功率分配和阈值优化问题，以满足通信速率要求。仿真结果验证了该联合设计方案相比传统CFAR方案的显著性能提升。", "keywords": "MIMO ISAC, 特征值检测, 功率分配, 阈值优化, 总检测误差", "comments": "该论文的创新点在于首次联合分析了MIMO ISAC系统中检测可靠性和通信约束，并通过引入总检测误差作为性能指标，解决了传统CFAR方案的局限性。其提出的基于特征值的检测器和联合优化策略，为MIMO ISAC系统的设计提供了新的思路，具有重要的理论和实践意义。未来研究可以考虑在更复杂的信道环境或多用户场景下的应用。"}}
{"id": "2506.09063", "title": "Reconstructing Heterogeneous Biomolecules via Hierarchical Gaussian Mixtures and Part Discovery", "authors": ["Shayan Shekarforoush", "David B. Lindell", "Marcus A. Brubaker", "David J. Fleet"], "summary": "Cryo-EM is a transformational paradigm in molecular biology where\ncomputational methods are used to infer 3D molecular structure at atomic\nresolution from extremely noisy 2D electron microscope images. At the forefront\nof research is how to model the structure when the imaged particles exhibit\nnon-rigid conformational flexibility and compositional variation where parts\nare sometimes missing. We introduce a novel 3D reconstruction framework with a\nhierarchical Gaussian mixture model, inspired in part by Gaussian Splatting for\n4D scene reconstruction. In particular, the structure of the model is grounded\nin an initial process that infers a part-based segmentation of the particle,\nproviding essential inductive bias in order to handle both conformational and\ncompositional variability. The framework, called CryoSPIRE, is shown to reveal\nbiologically meaningful structures on complex experimental datasets, and\nestablishes a new state-of-the-art on CryoBench, a benchmark for cryo-EM\nheterogeneity methods.", "comment": "21 pages, 14 figures, Project Webpage:\n  https://shekshaa.github.io/CryoSPIRE", "cate": "q-bio.QM", "url": "http://arxiv.org/abs/2506.09063v1", "AI": {"title_translation": "通过分层高斯混合和部分发现重建异质生物分子", "tldr": "CryoSPIRE是一个新的3D重建框架，利用分层高斯混合和部分发现来处理冷冻电镜中生物分子的构象灵活性和组成变异性，并在基准测试中达到了最先进的水平。", "motivation": "在分子生物学中，冷冻电镜（Cryo-EM）在推断分子结构时，面临着成像颗粒表现出非刚性构象灵活性和组成变异性（部分有时缺失）的挑战，亟需新的计算方法来建模这些结构。", "method": "本文引入了一个名为CryoSPIRE的新型3D重建框架，该框架采用分层高斯混合模型，并受到4D场景重建中高斯散布的启发。模型的结构基于一个初始过程，该过程推断出颗粒的基于部分的分割，提供了处理构象和组成变异性所需的基本归纳偏差。", "result": "CryoSPIRE框架被证明能够揭示复杂实验数据集上具有生物学意义的结构，并在冷冻电镜异质性方法的基准测试CryoBench上建立了新的最先进水平。", "conclusion": "CryoSPIRE框架通过其创新的分层高斯混合和部分发现方法，有效解决了冷冻电镜中异质生物分子的重建挑战，并在实际应用和基准测试中展现出卓越性能。", "translation": "冷冻电镜是分子生物学中一个变革性的范式，其中计算方法用于从极度嘈杂的2D电子显微镜图像中以原子分辨率推断3D分子结构。研究的前沿是如何在成像颗粒表现出非刚性构象灵活性和组成变异性（部分有时缺失）时对结构进行建模。我们引入了一个新颖的3D重建框架，该框架采用分层高斯混合模型，部分灵感来源于4D场景重建中的高斯散布。特别是，模型的结构基于一个初始过程，该过程推断出颗粒的基于部分的分割，提供了处理构象和组成变异性所需的基本归纳偏差。该框架名为CryoSPIRE，被证明能够在复杂的实验数据集上揭示具有生物学意义的结构，并在冷冻电镜异质性方法的基准测试CryoBench上建立了新的最先进水平。", "summary": "本文提出了一种名为CryoSPIRE的新型3D重建框架，旨在解决冷冻电镜中异质生物分子（具有构象灵活性和组成变异性）的结构建模问题。该框架利用分层高斯混合模型，并结合了基于部分的分割方法，以提供处理变异性的归纳偏差。实验结果表明，CryoSPIRE能够从复杂数据集中重建出有生物学意义的结构，并在CryoBench基准测试中达到了最先进的性能。", "keywords": "冷冻电镜, 3D重建, 异质生物分子, 分层高斯混合, 部分发现", "comments": "这篇论文的创新点在于将分层高斯混合模型与受高斯散布启发的思想相结合，并引入了部分发现机制来处理冷冻电镜中生物分子的高度异质性。这种方法为理解和重建具有复杂动态和组成变化的分子结构提供了强大的工具，对于推动分子生物学研究具有重要意义。"}}
{"id": "2506.09731", "title": "The Path is the Goal: a Study on the Nature and Effects of Shortest-Path Stability Under Perturbation of Destination", "authors": ["Giuliano Cornacchia", "Mirco Nanni"], "summary": "This work examines the phenomenon of path variability in urban navigation,\nwhere small changes in destination might lead to significantly different\nsuggested routes. Starting from an observation of this variability over the\ncity of Barcelona, we explore whether this is a localized or widespread\noccurrence and identify factors influencing path variability. We introduce the\nconcept of \"path stability\", a measure of how robust a suggested route is to\nminor destination adjustments, define a detailed experimentation process and\napply it across multiple cities worldwide. Our analysis shows that path\nstability is shaped by city-specific factors and trip characteristics, also\nidentifying some common patterns. Results reveal significant heterogeneity in\npath stability across cities, allowing for categorization into \"stable\" and\n\"unstable\" cities. These findings offer new insights for urban planning and\ntraffic management, highlighting opportunities for optimizing navigation\nsystems to enhance route consistency and urban mobility.", "comment": null, "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.09731v1", "AI": {"title_translation": "路径即目标：目的地扰动下最短路径稳定性的性质与影响研究", "tldr": "研究了城市导航中目的地微小变化导致路线显著不同的现象，引入“路径稳定性”概念，发现其受城市和行程特征影响，并对城市进行了分类。", "motivation": "观察到城市导航中目的地微小变化可能导致建议路线显著不同的现象，并想探究其是局部还是普遍现象，以及影响因素。", "method": "引入“路径稳定性”概念，定义了详细的实验过程，并将其应用于全球多个城市进行分析。", "result": "分析表明路径稳定性受城市特定因素和行程特征影响，并存在一些共同模式。结果揭示了城市间路径稳定性存在显著异质性，可将城市分为“稳定”和“不稳定”两类。", "conclusion": "这些发现为城市规划和交通管理提供了新见解，强调了优化导航系统以提高路线一致性和城市流动性的机会。", "translation": "这项工作研究了城市导航中路径可变性现象，即目的地微小变化可能导致建议路线显著不同。从对巴塞罗那市这种可变性的观察出发，我们探讨了这是一种局部还是普遍现象，并识别了影响路径可变性的因素。我们引入了“路径稳定性”的概念，它衡量了建议路线对微小目的地调整的鲁棒性，定义了详细的实验过程并将其应用于全球多个城市。我们的分析表明，路径稳定性受城市特定因素和行程特征的影响，同时也识别出一些共同模式。结果揭示了城市间路径稳定性存在显著异质性，允许将城市分为“稳定”和“不稳定”两类。这些发现为城市规划和交通管理提供了新见解，强调了优化导航系统以提高路线一致性和城市流动性的机会。", "summary": "本文研究了城市导航中目的地微小变化导致建议路线显著不同的“路径可变性”现象。通过引入“路径稳定性”概念并进行全球多城市实验，发现路径稳定性受城市特定因素和行程特征影响，且城市间存在显著异质性，可分为“稳定”和“不稳定”城市。研究结果为城市规划和交通管理提供了优化导航系统以提升路线一致性和城市流动性的新视角。", "keywords": "路径稳定性, 城市导航, 路径可变性, 最短路径, 交通管理", "comments": "这项研究通过引入“路径稳定性”这一新颖概念，量化并分析了城市导航中路径选择对目的地微小扰动的敏感性，为城市规划和导航系统优化提供了重要的理论基础和实践指导。"}}
{"id": "2506.09279", "title": "A Topic Modeling Analysis of Stigma Dimensions, Social, and Related Behavioral Circumstances in Clinical Notes Among Patients with HIV", "authors": ["Ziyi Chen", "Yiyang Liu", "Mattia Prosperi", "Krishna Vaddiparti", "Robert L Cook", "Jiang Bian", "Yi Guo", "Yonghui Wu"], "summary": "Objective: To characterize stigma dimensions, social, and related behavioral\ncircumstances in people living with HIV (PLWHs) seeking care, using natural\nlanguage processing methods applied to a large collection of electronic health\nrecord (EHR) clinical notes from a large integrated health system in the\nsoutheast United States. Methods: We identified 9,140 cohort of PLWHs from the\nUF Health IDR and performed topic modeling analysis using Latent Dirichlet\nAllocation (LDA) to uncover stigma dimensions, social, and related behavioral\ncircumstances. Domain experts created a seed list of HIV-related stigma\nkeywords, then applied a snowball strategy to iteratively review notes for\nadditional terms until saturation was reached. To identify more target topics,\nwe tested three keyword-based filtering strategies. Domain experts manually\nreviewed the detected topics using the prevalent terms and key discussion\ntopics. Word frequency analysis was used to highlight the prevalent terms\nassociated with each topic. In addition, we conducted topic variation analysis\namong subgroups to examine differences across age and sex-specific\ndemographics. Results and Conclusion: Topic modeling on sentences containing at\nleast one keyword uncovered a wide range of topic themes associated with\nHIV-related stigma, social, and related behaviors circumstances, including\n\"Mental Health Concern and Stigma\", \"Social Support and Engagement\", \"Limited\nHealthcare Access and Severe Illness\", \"Treatment Refusal and Isolation\" and so\non. Topic variation analysis across age subgroups revealed differences.\nExtracting and understanding the HIV-related stigma dimensions, social, and\nrelated behavioral circumstances from EHR clinical notes enables scalable,\ntime-efficient assessment, overcoming the limitations of traditional\nquestionnaires and improving patient outcomes.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09279v1", "AI": {"title_translation": "艾滋病患者临床记录中耻辱维度、社会和相关行为情况的主题建模分析", "tldr": "本研究利用主题建模分析电子健康记录中的临床笔记，以识别和表征艾滋病病毒感染者（PLWHs）所经历的耻辱维度、社会和相关行为情况，结果揭示了多种相关主题，并为克服传统问卷的局限性提供了可扩展的评估方法。", "motivation": "本研究旨在利用自然语言处理方法，分析大量电子健康记录（EHR）临床笔记，以表征艾滋病病毒感染者（PLWHs）在寻求护理过程中所经历的耻辱维度、社会和相关行为情况。", "method": "研究识别了来自UF Health IDR的9,140名艾滋病病毒感染者队列，并使用潜在狄利克雷分配（LDA）进行主题建模分析。领域专家创建了艾滋病相关耻辱关键词的种子列表，并采用滚雪球策略迭代审查笔记以获取额外术语，直至达到饱和。为识别更多目标主题，研究测试了三种基于关键词的过滤策略。领域专家手动审查了检测到的主题。此外，还进行了主题变异分析，以检查不同年龄和性别亚组之间的差异。", "result": "主题建模揭示了与艾滋病相关耻辱、社会和相关行为情况相关的广泛主题，包括“心理健康问题和耻辱”、“社会支持和参与”、“有限的医疗保健获取和严重疾病”、“拒绝治疗和隔离”等。跨年龄亚组的主题变异分析也揭示了差异。", "conclusion": "从电子健康记录临床笔记中提取和理解艾滋病相关耻辱维度、社会和相关行为情况，能够实现可扩展、省时的评估，克服了传统问卷的局限性，并改善了患者预后。", "translation": "目的：利用自然语言处理方法，对美国东南部大型综合医疗系统的大量电子健康记录（EHR）临床笔记进行分析，以表征艾滋病病毒感染者（PLWHs）在寻求护理过程中所经历的耻辱维度、社会和相关行为情况。方法：我们从UF Health IDR中识别了9,140名艾滋病病毒感染者队列，并使用潜在狄利克雷分配（LDA）进行主题建模分析，以揭示耻辱维度、社会和相关行为情况。领域专家创建了艾滋病相关耻辱关键词的种子列表，然后应用滚雪球策略迭代审查笔记以获取额外术语，直至达到饱和。为了识别更多目标主题，我们测试了三种基于关键词的过滤策略。领域专家使用普遍术语和关键讨论主题手动审查了检测到的主题。词频分析用于突出与每个主题相关的普遍术语。此外，我们还对亚组进行了主题变异分析，以检查不同年龄和性别特定人口统计学之间的差异。结果和结论：对包含至少一个关键词的句子进行主题建模，揭示了与艾滋病相关耻辱、社会和相关行为情况相关的广泛主题，包括“心理健康问题和耻辱”、“社会支持和参与”、“有限的医疗保健获取和严重疾病”、“拒绝治疗和隔离”等。跨年龄亚组的主题变异分析揭示了差异。从电子健康记录临床笔记中提取和理解艾滋病相关耻辱维度、社会和相关行为情况，能够实现可扩展、省时的评估，克服了传统问卷的局限性，并改善了患者预后。", "summary": "本研究利用自然语言处理和主题建模（LDA）技术，分析了9,140名艾滋病病毒感染者（PLWHs）的电子健康记录（EHR）临床笔记，旨在识别和表征艾滋病相关的耻辱维度、社会和行为情况。研究通过关键词过滤和专家审查，成功揭示了如“心理健康和耻辱”、“社会支持”及“有限医疗获取”等多样化主题。结果表明，这种方法能够实现对艾滋病相关耻辱的可扩展、高效评估，从而克服传统问卷的局限性并有望改善患者预后。", "keywords": "主题建模, 艾滋病, 耻辱, 电子健康记录, 自然语言处理", "comments": "该研究的创新之处在于利用自然语言处理和主题建模技术，从非结构化的临床笔记中大规模、系统地识别和分析艾滋病患者所面临的耻辱、社会及行为挑战。这种方法克服了传统问卷调查在效率和覆盖面上的局限性，为理解患者真实经历提供了更深入、更全面的视角。其重要性在于为医疗系统提供了一种可扩展的工具，用于识别和解决艾滋病患者的复杂需求，从而可能改善护理质量和患者结局。"}}
{"id": "2506.09435", "title": "FNPF-SEM: A parallel spectral element model in Firedrake for fully nonlinear water wave simulations", "authors": ["Jens Visbech", "Anders Melander", "Allan Peter Engsig-Karup"], "summary": "We present a new parallel spectral element solver, FNPF-SEM, for simulating\nlinear and fully nonlinear potential flow-based water waves and their\ninteraction with offshore structures. The tool is designed as a general-purpose\nwave model for offshore engineering applications. Built within the open-source\nframework Firedrake, the new FNPF-SEM model is designed as a computational tool\ncapable of capturing both linear and nonlinear wave phenomena with high\naccuracy and efficiency, with support for high-order (spectral) finite\nelements. Additionally, Firedrake provides native support for MPI-based\nparallelism, allowing for efficient multi-CPU distributed computations needed\nfor large-scale simulations. We demonstrate the capabilities of the high-order\nspectral element model through h- and p-convergence studies, and weak and\nstrong scaling tests. Validation is performed against analytical solutions and\nexperimental data for several benchmark cases, including nonlinear high-order\nharmonic generation and linear and nonlinear wave interactions with a cylinder\nand a breakwater. The new FNPF-SEM model offers a numerical framework for\nsimulating wave propagation and wave-structure interactions, with the following\nkey features: i) the ability to represent complex geometries through flexible,\nunstructured finite element meshes; ii) reduced numerical diffusion and\ndispersion by using high-order polynomial expansions; and iii) scalability to\nfull- and large-scale simulations over long time periods through a parallel\nimplementation.", "comment": "23 pages, 19 figures", "cate": "math.NA", "url": "http://arxiv.org/abs/2506.09435v1", "AI": {"title_translation": "FNPF-SEM：一个用于全非线性水波模拟的Firedrake并行谱元模型", "tldr": "FNPF-SEM是一个基于Firedrake的并行谱元求解器，用于高效准确地模拟线性和全非线性水波及其与海上结构物的相互作用，支持高阶有限元和大规模并行计算。", "motivation": "该研究旨在开发一个通用型水波模型，用于海上工程应用，能够高精度、高效率地捕捉线性和非线性波浪现象。", "method": "本文提出了FNPF-SEM模型，这是一个基于开源框架Firedrake构建的并行谱元求解器。它利用高阶（谱）有限元方法和MPI并行计算，以支持大规模模拟。模型通过h-和p-收敛性研究以及弱和强扩展性测试来验证其能力。", "result": "FNPF-SEM模型通过h-和p-收敛性研究、弱和强扩展性测试展示了其能力。通过与解析解和实验数据进行验证，包括非线性高阶谐波生成以及波浪与圆柱体和防波堤的线性和非线性相互作用，证明了其有效性。", "conclusion": "新的FNPF-SEM模型提供了一个用于模拟波浪传播和波浪-结构相互作用的数值框架，其关键特性包括：能够通过灵活的非结构化有限元网格表示复杂几何形状；通过使用高阶多项式展开减少数值扩散和色散；以及通过并行实现，可扩展到全尺寸和大规模长时间模拟。", "translation": "我们提出了一个新的并行谱元求解器FNPF-SEM，用于模拟线性和全非线性势流基水波及其与海上结构物的相互作用。该工具被设计为用于海上工程应用的通用型波浪模型。新的FNPF-SEM模型构建于开源框架Firedrake内，旨在成为一个能够高精度、高效率地捕捉线性和非线性波浪现象的计算工具，并支持高阶（谱）有限元。此外，Firedrake原生支持基于MPI的并行计算，从而实现大规模模拟所需的分布式多CPU高效计算。我们通过h-和p-收敛性研究以及弱和强扩展性测试来展示高阶谱元模型的能力。针对多个基准案例，包括非线性高阶谐波生成以及波浪与圆柱体和防波堤的线性和非线性相互作用，模型与解析解和实验数据进行了验证。新的FNPF-SEM模型提供了一个用于模拟波浪传播和波浪-结构相互作用的数值框架，具有以下关键特性：i) 能够通过灵活的非结构化有限元网格表示复杂几何形状；ii) 通过使用高阶多项式展开减少数值扩散和色散；以及iii) 通过并行实现，可扩展到全尺寸和大规模长时间模拟。", "summary": "本文介绍了一个名为FNPF-SEM的并行谱元求解器，该模型基于Firedrake框架开发，旨在高效准确地模拟线性和全非线性水波及其与海上结构物的相互作用。它支持高阶有限元和MPI并行计算，能够处理复杂几何形状，减少数值扩散和色散，并可扩展到大规模长时间模拟。模型通过收敛性研究和与解析解及实验数据的验证，展示了其在各种基准案例中的能力。", "keywords": "水波模拟, 谱元方法, Firedrake, 并行计算, 非线性波浪", "comments": "FNPF-SEM模型在海上工程领域具有重要意义，它结合了高阶谱元方法和并行计算的优势，能够高精度、高效率地模拟复杂的非线性水波现象及波浪与结构物的相互作用。其在Firedrake框架下的实现，使其具备了良好的可扩展性和处理复杂几何的能力，为大规模水波模拟提供了强大的数值工具。"}}
{"id": "2506.09792", "title": "Incorporating Linguistic Constraints from External Knowledge Source for Audio-Visual Target Speech Extraction", "authors": ["Wenxuan Wu", "Shuai Wang", "Xixin Wu", "Helen Meng", "Haizhou Li"], "summary": "Audio-visual target speaker extraction (AV-TSE) models primarily rely on\ntarget visual cues to isolate the target speaker's voice from others. We know\nthat humans leverage linguistic knowledge, such as syntax and semantics, to\nsupport speech perception. Inspired by this, we explore the potential of\npre-trained speech-language models (PSLMs) and pre-trained language models\n(PLMs) as auxiliary knowledge sources for AV-TSE. In this study, we propose\nincorporating the linguistic constraints from PSLMs or PLMs for the AV-TSE\nmodel as additional supervision signals. Without introducing any extra\ncomputational cost during inference, the proposed approach consistently\nimproves speech quality and intelligibility. Furthermore, we evaluate our\nmethod in multi-language settings and visual cue-impaired scenarios and show\nrobust performance gains.", "comment": "Accepted by Interspeech 2025", "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.09792v1", "AI": {"title_translation": "融合外部知识源的语言约束以进行音视频目标语音提取", "tldr": "本文提出了一种将预训练语音语言模型（PSLMs）或预训练语言模型（PLMs）的语言约束作为额外监督信号融入音视频目标语音提取（AV-TSE）模型的方法，在不增加推理计算成本的情况下，持续提升了语音质量和可懂度，并在多语言和视觉线索受损场景中表现出鲁棒的性能提升。", "motivation": "现有的音视频目标语音提取（AV-TSE）模型主要依赖视觉线索来分离目标说话人的声音。受人类利用语言知识（如语法和语义）辅助语音感知的启发，本文旨在探索预训练语音语言模型（PSLMs）和预训练语言模型（PLMs）作为AV-TSE辅助知识源的潜力。", "method": "本文提出将来自预训练语音语言模型（PSLMs）或预训练语言模型（PLMs）的语言约束作为额外的监督信号，融入到音视频目标语音提取（AV-TSE）模型中。该方法在推理过程中不引入额外的计算成本。", "result": "所提出的方法在不增加推理计算成本的情况下，持续改善了语音质量和可懂度。此外，该方法在多语言设置和视觉线索受损场景中也表现出鲁棒的性能提升。", "conclusion": "通过将预训练语音语言模型（PSLMs）或预训练语言模型（PLMs）的语言约束作为额外监督信号融入音视频目标语音提取模型，可以有效提升语音质量和可懂度，并在多种复杂场景下保持鲁棒性。", "translation": "音视频目标说话人提取（AV-TSE）模型主要依赖目标视觉线索来从其他人中分离出目标说话人的声音。我们知道人类利用语言知识，如语法和语义，来支持语音感知。受此启发，我们探索了预训练语音语言模型（PSLMs）和预训练语言模型（PLMs）作为AV-TSE辅助知识源的潜力。在本研究中，我们提出将来自PSLMs或PLMs的语言约束作为额外的监督信号，融入到AV-TSE模型中。在推理过程中不引入任何额外计算成本的情况下，所提出的方法持续改善了语音质量和可懂度。此外，我们在多语言设置和视觉线索受损场景中评估了我们的方法，并显示出鲁棒的性能提升。", "summary": "本文提出了一种创新的音视频目标语音提取（AV-TSE）方法，该方法借鉴人类利用语言知识的机制，将预训练语音语言模型（PSLMs）或预训练语言模型（PLMs）的语言约束作为额外的监督信号引入AV-TSE模型。实验结果表明，该方法在不增加推理计算成本的前提下，显著提升了语音质量和可懂度，并在多语言及视觉线索受损等复杂场景下展现出优异的鲁棒性能。", "keywords": "音视频目标语音提取, 语言约束, 预训练语音语言模型, 语音质量, 鲁棒性", "comments": "该论文的创新点在于将外部语言知识（来自PSLMs或PLMs）以监督信号的形式引入到音视频目标语音提取任务中，突破了传统AV-TSE模型仅依赖视觉线索的局限。这种方法不仅提升了模型性能，尤其是在语音质量和可懂度方面，而且在推理时不增加计算负担，这对于实际应用具有重要意义。在多语言和视觉受损场景下的鲁棒性评估进一步证明了其广泛适用性。"}}
{"id": "2506.09662", "title": "Empirical Quantification of Spurious Correlations in Malware Detection", "authors": ["Bianca Perasso", "Ludovico Lozza", "Andrea Ponte", "Luca Demetrio", "Luca Oneto", "Fabio Roli"], "summary": "End-to-end deep learning exhibits unmatched performance for detecting\nmalware, but such an achievement is reached by exploiting spurious correlations\n-- features with high relevance at inference time, but known to be useless\nthrough domain knowledge. While previous work highlighted that deep networks\nmainly focus on metadata, none investigated the phenomenon further, without\nquantifying their impact on the decision. In this work, we deepen our\nunderstanding of how spurious correlation affects deep learning for malware\ndetection by highlighting how much models rely on empty spaces left by the\ncompiler, which diminishes the relevance of the compiled code. Through our\nseminal analysis on a small-scale balanced dataset, we introduce a ranking of\ntwo end-to-end models to better understand which is more suitable to be put in\nproduction.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.09662v1", "AI": {"title_translation": "恶意软件检测中虚假关联的实证量化", "tldr": "深度学习在恶意软件检测中利用虚假关联，本研究量化了这种关联的影响，发现模型主要依赖编译器留下的空白空间，并对两种模型进行了排名以评估其生产适用性。", "motivation": "深度学习在恶意软件检测中表现出无与伦比的性能，但这种成就的实现是利用了虚假关联——即在推断时高度相关但通过领域知识已知无用的特征。尽管先前的研究强调深度网络主要关注元数据，但它们并未进一步探究这种现象，也未量化虚假关联对模型决策的影响。", "method": "本文通过突出模型在多大程度上依赖编译器留下的空白空间来深化对虚假关联如何影响恶意软件检测深度学习的理解，这降低了编译代码的相关性。通过在一个小型均衡数据集上的开创性分析，研究引入了两种端到端模型的排名，以更好地理解哪种模型更适合投入生产。", "result": "研究结果表明，深度学习模型在恶意软件检测中很大程度上依赖于编译器留下的空白空间，这显著降低了编译代码的实际相关性。此外，通过对两种端到端模型进行排名，为评估其在生产环境中的适用性提供了依据。", "conclusion": "通过对两种端到端模型的排名分析，本研究有助于更好地理解哪种模型更适合投入生产环境，从而为恶意软件检测系统的实际部署提供实证指导。", "translation": "端到端深度学习在检测恶意软件方面表现出无与伦比的性能，但这种成就的实现是利用了虚假关联——在推断时具有高度相关性，但通过领域知识已知无用的特征。虽然之前的工作强调深度网络主要关注元数据，但没有进一步研究这种现象，也没有量化它们对决策的影响。在这项工作中，我们通过突出模型在多大程度上依赖于编译器留下的空白空间来深化我们对虚假关联如何影响恶意软件检测深度学习的理解，这降低了编译代码的相关性。通过我们在一个小型均衡数据集上的开创性分析，我们引入了两种端到端模型的排名，以更好地理解哪种模型更适合投入生产。", "summary": "本文探讨了深度学习在恶意软件检测中利用虚假关联的问题，并首次对其影响进行了量化分析。研究发现，模型过度依赖编译器留下的空白空间而非实际编译代码，从而削弱了模型的鲁棒性。通过对两种端到端模型进行排名，本研究为选择更适合生产环境的恶意软件检测模型提供了实证依据。", "keywords": "恶意软件检测, 深度学习, 虚假关联, 实证量化, 模型排名", "comments": "这篇论文具有重要的意义，因为它揭示了深度学习模型在恶意软件检测中可能存在的“捷径”问题，即模型可能并非真正学习到恶意行为的本质特征，而是利用了数据中存在的虚假关联。这种对虚假关联进行量化分析的方法，对于理解深度学习模型在安全领域的局限性及其可靠性至关重要，有助于指导未来模型的改进和实际部署。"}}
{"id": "2506.09229", "title": "Cross-Frame Representation Alignment for Fine-Tuning Video Diffusion Models", "authors": ["Sungwon Hwang", "Hyojin Jang", "Kinam Kim", "Minho Park", "Jaegul choo"], "summary": "Fine-tuning Video Diffusion Models (VDMs) at the user level to generate\nvideos that reflect specific attributes of training data presents notable\nchallenges, yet remains underexplored despite its practical importance.\nMeanwhile, recent work such as Representation Alignment (REPA) has shown\npromise in improving the convergence and quality of DiT-based image diffusion\nmodels by aligning, or assimilating, its internal hidden states with external\npretrained visual features, suggesting its potential for VDM fine-tuning. In\nthis work, we first propose a straightforward adaptation of REPA for VDMs and\nempirically show that, while effective for convergence, it is suboptimal in\npreserving semantic consistency across frames. To address this limitation, we\nintroduce Cross-frame Representation Alignment (CREPA), a novel regularization\ntechnique that aligns hidden states of a frame with external features from\nneighboring frames. Empirical evaluations on large-scale VDMs, including\nCogVideoX-5B and Hunyuan Video, demonstrate that CREPA improves both visual\nfidelity and cross-frame semantic coherence when fine-tuned with\nparameter-efficient methods such as LoRA. We further validate CREPA across\ndiverse datasets with varying attributes, confirming its broad applicability.\nProject page: https://crepavideo.github.io", "comment": "24 pages, 25 figures", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09229v1", "AI": {"title_translation": "视频扩散模型微调的跨帧表示对齐", "tldr": "本文提出了一种名为跨帧表示对齐（CREPA）的新型正则化技术，通过将单帧的隐藏状态与相邻帧的外部特征对齐，显著改善了视频扩散模型（VDMs）在微调时的视觉保真度和跨帧语义连贯性。", "motivation": "用户级微调视频扩散模型以生成反映训练数据特定属性的视频面临挑战且未被充分探索。尽管现有的表示对齐（REPA）方法在图像扩散模型中表现出潜力，但直接应用于视频扩散模型时，虽能加速收敛，却无法有效保持跨帧语义一致性，这促使研究者寻求新的解决方案。", "method": "研究者首先将表示对齐（REPA）方法直接应用于视频扩散模型（VDMs），发现其对模型收敛有效，但在保持跨帧语义一致性方面表现不佳。为解决此限制，他们引入了跨帧表示对齐（CREPA）技术，这是一种新颖的正则化方法，通过将某一帧的内部隐藏状态与来自其相邻帧的外部预训练视觉特征进行对齐。该方法与LoRA等参数高效的微调技术结合使用。", "result": "在CogVideoX-5B和Hunyuan Video等大型视频扩散模型（VDMs）上的实证评估表明，当结合LoRA等参数高效方法进行微调时，跨帧表示对齐（CREPA）显著改善了视频的视觉保真度和跨帧语义连贯性。此外，CREPA在具有不同属性的多种数据集上得到了进一步验证，证实了其广泛适用性。", "conclusion": "跨帧表示对齐（CREPA）是一种有效且具有广泛适用性的正则化技术，能够显著提升视频扩散模型在用户级微调时的视频质量和跨帧语义一致性，解决了现有方法在视频生成连贯性方面的局限性。", "translation": "用户级微调视频扩散模型（VDMs）以生成反映训练数据特定属性的视频面临显著挑战，尽管其具有实际重要性，但仍未得到充分探索。同时，最近的工作，如表示对齐（REPA），已通过对齐或同化其内部隐藏状态与外部预训练视觉特征，在改进基于DiT的图像扩散模型的收敛性和质量方面显示出前景，这表明其在VDM微调中的潜力。在这项工作中，我们首先提出了REPA在VDM中的直接适应，并凭经验表明，虽然对收敛有效，但在保持跨帧语义一致性方面表现不佳。为了解决这一限制，我们引入了跨帧表示对齐（CREPA），这是一种新颖的正则化技术，它将一帧的隐藏状态与来自相邻帧的外部特征进行对齐。在CogVideoX-5B和Hunyuan Video等大型VDM上的实证评估表明，当使用LoRA等参数高效方法进行微调时，CREPA同时改善了视觉保真度和跨帧语义连贯性。我们进一步在具有不同属性的各种数据集上验证了CREPA，证实了其广泛适用性。项目页面：https://crepavideo.github.io", "summary": "本文提出了一种名为跨帧表示对齐（CREPA）的新型正则化技术，旨在解决视频扩散模型（VDMs）用户级微调中跨帧语义一致性差的问题。通过将单帧的隐藏状态与相邻帧的外部特征对齐，CREPA显著提高了微调后VDMs的视觉保真度和跨帧语义连贯性，并在CogVideoX-5B和Hunyuan Video等大型VDM以及多种数据集上验证了其有效性和广泛适用性，尤其是在与LoRA等参数高效方法结合使用时。", "keywords": "视频扩散模型, 微调, 跨帧表示对齐, 正则化, 语义一致性", "comments": "这项工作通过引入创新的跨帧表示对齐技术，解决了视频扩散模型微调中的一个关键挑战——保持跨帧语义一致性。其在大型VDM上的成功应用，特别是与参数高效方法如LoRA的结合，凸显了其在实际应用中的重要性和潜力，为高质量视频生成提供了新的思路，对于用户级定制化视频生成具有重要的实践意义。"}}
{"id": "2410.16222", "title": "An Interpretable N-gram Perplexity Threat Model for Large Language Model Jailbreaks", "authors": ["Valentyn Boreiko", "Alexander Panfilov", "Vaclav Voracek", "Matthias Hein", "Jonas Geiping"], "summary": "A plethora of jailbreaking attacks have been proposed to obtain harmful\nresponses from safety-tuned LLMs. These methods largely succeed in coercing the\ntarget output in their original settings, but their attacks vary substantially\nin fluency and computational effort. In this work, we propose a unified threat\nmodel for the principled comparison of these methods. Our threat model checks\nif a given jailbreak is likely to occur in the distribution of text. For this,\nwe build an N-gram language model on 1T tokens, which, unlike model-based\nperplexity, allows for an LLM-agnostic, nonparametric, and inherently\ninterpretable evaluation. We adapt popular attacks to this threat model, and,\nfor the first time, benchmark these attacks on equal footing with it. After an\nextensive comparison, we find attack success rates against safety-tuned modern\nmodels to be lower than previously presented and that attacks based on discrete\noptimization significantly outperform recent LLM-based attacks. Being\ninherently interpretable, our threat model allows for a comprehensive analysis\nand comparison of jailbreak attacks. We find that effective attacks exploit and\nabuse infrequent bigrams, either selecting the ones absent from real-world text\nor rare ones, e.g., specific to Reddit or code datasets.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2410.16222v2", "AI": {"title_translation": "大语言模型越狱的可解释N-gram困惑度威胁模型", "tldr": "提出一种基于N-gram困惑度的可解释威胁模型，用于统一比较大语言模型越狱攻击，发现攻击成功率低于预期，且离散优化攻击优于基于LLM的攻击。", "motivation": "现有的越狱攻击方法在流畅性和计算效率上差异很大，且缺乏一个统一的威胁模型来公平比较这些方法。", "method": "提出一个N-gram困惑度威胁模型，该模型基于1T token构建N-gram语言模型，实现LLM无关、非参数和可解释的评估。使用此模型统一基准测试了流行的越狱攻击。", "result": "发现针对安全调优的现代模型的攻击成功率低于先前报告；基于离散优化的攻击显著优于近期基于LLM的攻击；有效的攻击利用并滥用不常见的二元组，例如真实文本中不存在的或特定于Reddit或代码数据集的稀有二元组。", "conclusion": "所提出的可解释N-gram困惑度威胁模型能够对越狱攻击进行全面分析和比较，揭示了不同攻击方法的真实效果和利用的漏洞。", "translation": "越狱攻击层出不穷，旨在从经过安全微调的大语言模型中获取有害响应。这些方法在原始设置中很大程度上成功地迫使目标输出，但其攻击在流畅性和计算工作量方面差异很大。在这项工作中，我们提出了一种统一的威胁模型，用于对这些方法进行原则性比较。我们的威胁模型检查给定越狱是否可能发生在文本分布中。为此，我们构建了一个基于1T token的N-gram语言模型，这与基于模型的困惑度不同，它允许进行与LLM无关、非参数且本质上可解释的评估。我们将流行的攻击方法应用于此威胁模型，并首次在此基础上公平地对这些攻击进行基准测试。经过广泛比较，我们发现针对安全微调的现代模型的攻击成功率低于先前报告的水平，并且基于离散优化的攻击显著优于近期基于LLM的攻击。由于其固有的可解释性，我们的威胁模型允许对越狱攻击进行全面分析和比较。我们发现，有效的攻击利用并滥用不常见的二元组，要么选择真实世界文本中不存在的二元组，要么选择稀有的二元组，例如特定于Reddit或代码数据集的二元组。", "summary": "本文提出一种可解释的N-gram困惑度威胁模型，用于统一评估大语言模型越狱攻击。该模型通过构建基于1T token的N-gram语言模型，提供LLM无关、非参数且可解释的评估方式。研究发现，现有越狱攻击对安全调优模型的成功率低于预期，且离散优化攻击表现优于基于LLM的攻击。该模型还揭示了有效攻击通常利用不常见的二元组。", "keywords": "大语言模型, 越狱攻击, N-gram困惑度, 威胁模型, 可解释性", "comments": "该研究通过引入一个LLM无关且可解释的N-gram困惑度威胁模型，为大语言模型越狱攻击的公平比较提供了一个新颖且有原则的框架。其创新点在于摆脱了对特定LLM的依赖，并提供了更直观的攻击效果分析。研究结果对于理解和防御越狱攻击具有重要意义，特别是揭示了离散优化攻击的有效性以及攻击利用不常见二元组的策略，这有助于未来更精准地设计防御措施。"}}
{"id": "2506.09101", "title": "Feature Shift Localization Network", "authors": ["Míriam Barrabés", "Daniel Mas Montserrat", "Kapal Dev", "Alexander G. Ioannidis"], "summary": "Feature shifts between data sources are present in many applications\ninvolving healthcare, biomedical, socioeconomic, financial, survey, and\nmulti-sensor data, among others, where unharmonized heterogeneous data sources,\nnoisy data measurements, or inconsistent processing and standardization\npipelines can lead to erroneous features. Localizing shifted features is\nimportant to address the underlying cause of the shift and correct or filter\nthe data to avoid degrading downstream analysis. While many techniques can\ndetect distribution shifts, localizing the features originating them is still\nchallenging, with current solutions being either inaccurate or not scalable to\nlarge and high-dimensional datasets. In this work, we introduce the Feature\nShift Localization Network (FSL-Net), a neural network that can localize\nfeature shifts in large and high-dimensional datasets in a fast and accurate\nmanner. The network, trained with a large number of datasets, learns to extract\nthe statistical properties of the datasets and can localize feature shifts from\npreviously unseen datasets and shifts without the need for re-training. The\ncode and ready-to-use trained model are available at\nhttps://github.com/AI-sandbox/FSL-Net.", "comment": "9 pages, 2 figures, 4 tables", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09101v1", "AI": {"title_translation": "特征漂移定位网络", "tldr": "FSL-Net是一个神经网络，能快速准确地定位大型高维数据集中特征漂移，且无需对未见过的数据集和漂移进行再训练。", "motivation": "数据源之间的特征漂移在医疗、生物医学、社会经济、金融、调查和多传感器数据等许多应用中普遍存在，这些漂移可能由不协调的异构数据源、噪声数据测量或不一致的处理和标准化流程引起。定位漂移特征对于解决漂移的根本原因、纠正或过滤数据以避免降低下游分析的质量至关重要。尽管许多技术可以检测分布漂移，但定位产生漂移的特征仍然具有挑战性，现有解决方案要么不准确，要么无法扩展到大型和高维数据集。", "method": "本文引入了特征漂移定位网络（FSL-Net），这是一个神经网络，能够快速准确地定位大型和高维数据集中的特征漂移。该网络通过大量数据集进行训练，学习提取数据集的统计特性，并且无需再训练即可定位来自以前未见过的数据集和漂移的特征漂移。", "result": "FSL-Net能够快速准确地定位大型和高维数据集中的特征漂移。该网络可以定位来自以前未见过的数据集和漂移的特征漂移，而无需再训练。", "conclusion": "本文提出的FSL-Net有效解决了大型高维数据集中特征漂移定位的挑战，提高了定位的速度和准确性，并展现了对新数据和新漂移的泛化能力。", "translation": "数据源之间的特征漂移在医疗、生物医学、社会经济、金融、调查和多传感器数据等许多应用中普遍存在，其中不协调的异构数据源、噪声数据测量或不一致的处理和标准化流程可能导致错误的特征。定位漂移特征对于解决漂移的根本原因、纠正或过滤数据以避免降低下游分析的质量至关重要。尽管许多技术可以检测分布漂移，但定位产生漂移的特征仍然具有挑战性，现有解决方案要么不准确，要么无法扩展到大型和高维数据集。在这项工作中，我们引入了特征漂移定位网络（FSL-Net），这是一个神经网络，能够快速准确地定位大型和高维数据集中的特征漂移。该网络通过大量数据集进行训练，学习提取数据集的统计特性，并且无需再训练即可定位来自以前未见过的数据集和漂移的特征漂移。代码和现成的训练模型可在https://github.com/AI-sandbox/FSL-Net获取。", "summary": "本文提出了特征漂移定位网络（FSL-Net），旨在解决大型高维数据集中特征漂移定位的挑战。FSL-Net是一个神经网络，通过学习数据集的统计特性，能够快速准确地识别特征漂移，并且无需对新的或未见过的数据集进行再训练。该网络解决了现有解决方案在准确性和可扩展性方面的不足，对于纠正数据以避免影响下游分析至关重要。", "keywords": "特征漂移, 定位网络, 神经网络, 高维数据, 数据质量", "comments": "该论文的创新之处在于提出了FSL-Net，一个能够快速、准确地在大规模高维数据中定位特征漂移的神经网络，且无需对新数据或新漂移进行再训练，这显著提升了现有方法的效率和泛化能力。其重要性体现在解决了数据质量问题对下游分析的负面影响。此外，作者提供了代码和预训练模型，极大地促进了该方法的应用和进一步研究。"}}
{"id": "2506.09900", "title": "Corrections to Friis noise factor formulas for cascade networks", "authors": ["Ankitha E Bangera"], "summary": "The signal-to-noise ratio of a multistage cascade network is often estimated\nusing the well-known Friis' formulas for noise factors (or the noise figures in\ndecibels). However, this article addresses the major errors in Friis' noise\nfactor formulas for higher stages. Additionally, we re-derive the correct\nformulas to calculate the stage-wise noise factors for cascade networks from\nthe basic definition of noise factors. We then present a comparison of our\nderived formulas with Friis' noise factor formulas. Contrary to Friis' formula,\nwe define the total noise factor of an n-stage cascade network as the product\nof its stage-wise noise factors. We further validate our derived formulas for a\ncascade network by correlating them with the expressions for a staircase\navalanche photodiode.", "comment": "Friis' noise factor formulas for cascade networks are often used to\n  derive other related formulas for devices involving cascade mechanism (eg.\n  staircase APDs). However, Friis' equations for higher stages are themselves\n  incorrect. This article points out the existing mistakes and re-derives the\n  correct formulas for a cascade network's noise factor (10 pages, 3 figures,\n  preprint under submission)", "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.09900v1", "AI": {"title_translation": "级联网络中Friis噪声系数公式的修正", "tldr": "本文修正了高阶级联网络中Friis噪声系数公式的主要错误，并推导了正确的公式。", "motivation": "针对高阶级联网络中Friis噪声系数公式存在的重大错误。", "method": "从噪声系数的基本定义重新推导了级联网络中各级噪声系数的正确公式；将推导出的公式与Friis公式进行比较；通过与阶梯式雪崩光电二极管的表达式相关联，验证了推导出的公式。", "result": "与Friis公式相反，本文将n级级联网络的总噪声系数定义为其各级噪声系数的乘积。", "conclusion": "本文提供了修正后的级联网络噪声系数计算公式，并通过与雪崩光电二极管的关联进行了验证。", "translation": "多级级联网络的信噪比通常使用著名的Friis噪声系数公式（或以分贝表示的噪声系数）进行估算。然而，本文指出Friis噪声系数公式在高阶级联网络中存在主要错误。此外，我们从噪声系数的基本定义重新推导了计算级联网络中各级噪声系数的正确公式。然后，我们将我们推导出的公式与Friis噪声系数公式进行了比较。与Friis公式相反，我们将n级级联网络的总噪声系数定义为其各级噪声系数的乘积。我们通过将我们推导出的级联网络公式与阶梯式雪崩光电二极管的表达式相关联，进一步验证了我们的公式。", "summary": "本文旨在修正高阶级联网络中Friis噪声系数公式的重大错误。作者从噪声系数的基本定义出发，重新推导了计算级联网络各级噪声系数的正确公式，并将其与Friis公式进行了对比。研究提出，n级级联网络的总噪声系数应定义为各级噪声系数的乘积，这与Friis公式不同。推导出的公式通过与阶梯式雪崩光电二极管的表达式相关联得到了进一步验证。", "keywords": "Friis噪声系数, 级联网络, 噪声系数, 雪崩光电二极管", "comments": "这篇论文的重要性在于它纠正了长期以来在多级级联网络噪声分析中可能被广泛误用或不准确的Friis噪声系数公式。通过重新推导并提供经过验证的修正公式，它为工程师和研究人员提供了更精确的工具来估算级联系统的信噪比，这对于射频、光学通信等领域的设计至关重要。其创新点在于挑战并改进了经典理论。"}}
{"id": "2506.09845", "title": "variability.dev: Towards an Online Toolbox for Feature Modeling", "authors": ["Tobias Heß", "Lukas Ostheimer", "Tobias Betz", "Simon Karrer", "Tim Jannik Schmidt", "Pierre Coquet", "Sean Semmler", "Thomas Thüm"], "summary": "The emergence of feature models as the default to model the variability in\nconfigurable systems fosters a rich diversity in applications, application\ndomains, and perspectives. Independent of their domain, modelers require to\nopen, view, edit, transform, save, and configure models as well as to\ncollaborate with others. However, at the time of writing, the top five results\nwhen googling ``Online Editor Feature Model'' point to editors that either have\nminimal functionality, are unmaintained or defunct, or require an offline\ninstallation, such as FeatureIDE. In this work we present a preview of our\nin-development online toolbox for feature modeling, variability.dev. In\nparticular, we showcase our collaborative feature-model editor and our online\nconfigurator both of which are built on top of the FeatureIDE library.", "comment": "Presented at 6th International Workshop on Languages for Modelling\n  Variability (MODEVAR'24) (arXiv:cs/2402.15511). 5 pages, 3 figures", "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.09845v1", "AI": {"title_translation": "variability.dev：迈向特征建模的在线工具箱", "tldr": "本文介绍了正在开发的在线特征建模工具箱 variability.dev 的预览，其中包括一个协作式特征模型编辑器和在线配置器，旨在解决现有在线工具功能不足或不可用的问题。", "motivation": "现有在线特征模型编辑器功能不足、维护不善、已停用或需要离线安装（如 FeatureIDE），无法满足模型构建者对打开、查看、编辑、转换、保存和配置模型以及协作的需求。", "method": "开发了一个名为 variability.dev 的在线特征建模工具箱的预览版。该工具箱包含一个协作式特征模型编辑器和一个在线配置器，两者都基于 FeatureIDE 库构建。", "result": "展示了正在开发的在线特征建模工具箱 variability.dev 的预览，特别是其协作式特征模型编辑器和在线配置器。", "conclusion": "Not mentioned in abstract", "translation": "可配置系统中特征模型作为建模可变性的默认方式的出现，促进了应用程序、应用领域和视角的丰富多样性。无论其领域如何，模型构建者都需要打开、查看、编辑、转换、保存和配置模型，并与他人协作。然而，在撰写本文时，谷歌搜索“在线编辑器特征模型”的前五个结果指向的编辑器要么功能极少，要么未维护或已停用，要么需要离线安装，例如 FeatureIDE。在这项工作中，我们展示了我们正在开发的在线特征建模工具箱 variability.dev 的预览。特别是，我们展示了我们的协作式特征模型编辑器和在线配置器，两者都构建在 FeatureIDE 库之上。", "summary": "本文介绍了正在开发的在线特征建模工具箱 variability.dev 的预览，旨在解决当前在线特征模型编辑器功能有限或不可用的问题。该工具箱包括一个协作式特征模型编辑器和在线配置器，两者均基于 FeatureIDE 库构建，旨在为模型构建者提供一个全面的在线解决方案。", "keywords": "特征建模, 在线工具, 可变性, 协作编辑器, FeatureIDE", "comments": "该论文提出了一个解决现有在线特征建模工具痛点的重要方向，即开发一个功能全面、协作性强的在线工具箱。基于 FeatureIDE 库构建能够利用其成熟的功能，而在线和协作的特性则提升了易用性和团队效率。其创新性在于填补了在线特征建模工具的市场空白。"}}
{"id": "2506.09968", "title": "SRLAgent: Enhancing Self-Regulated Learning Skills through Gamification and LLM Assistance", "authors": ["Wentao Ge", "Yuqing Sun", "Ziyan Wang", "Haoyue Zheng", "Weiyang He", "Piaohong Wang", "Qianyu Zhu", "Benyou Wang"], "summary": "Self-regulated learning (SRL) is crucial for college students navigating\nincreased academic demands and independence. Insufficient SRL skills can lead\nto disorganized study habits, low motivation, and poor time management,\nundermining learners ability to thrive in challenging environments. Through a\nformative study involving 59 college students, we identified key challenges\nstudents face in developing SRL skills, including difficulties with\ngoal-setting, time management, and reflective learning. To address these\nchallenges, we introduce SRLAgent, an LLM-assisted system that fosters SRL\nskills through gamification and adaptive support from large language models\n(LLMs). Grounded in Zimmermans three-phase SRL framework, SRLAgent enables\nstudents to engage in goal-setting, strategy execution, and self-reflection\nwithin an interactive game-based environment. The system offers real-time\nfeedback and scaffolding powered by LLMs to support students independent study\nefforts. We evaluated SRLAgent using a between-subjects design, comparing it to\na baseline system (SRL without Agent features) and a traditional multimedia\nlearning condition. Results showed significant improvements in SRL skills\nwithin the SRLAgent group (p < .001, Cohens d = 0.234) and higher engagement\ncompared to the baselines. This work highlights the value of embedding SRL\nscaffolding and real-time AI support within gamified environments, offering\ndesign implications for educational technologies that aim to promote deeper\nlearning and metacognitive skill development.", "comment": "14 pages", "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.09968v1", "AI": {"title_translation": "SRLAgent：通过游戏化和大型语言模型辅助增强自主学习技能", "tldr": "SRLAgent是一个结合游戏化和LLM辅助的系统，旨在帮助大学生提升自主学习技能，实验结果显示其显著提高了学生的自主学习能力和参与度。", "motivation": "大学生在应对日益增长的学业要求和独立性时，自主学习（SRL）至关重要。自主学习技能不足会导致学习习惯混乱、动力不足和时间管理不善，从而影响学生在挑战性环境中茁壮成长。本研究旨在通过引入SRLAgent系统来解决这些挑战，以提升学生的自主学习技能。", "method": "通过一项包含59名大学生的形成性研究，确定了学生在发展自主学习技能方面面临的关键挑战，包括目标设定、时间管理和反思性学习的困难。为解决这些挑战，研究引入了SRLAgent系统，该系统基于Zimmerman的三阶段自主学习框架，通过游戏化和大型语言模型（LLM）的自适应支持来培养自主学习技能。系统在一个交互式游戏环境中支持学生进行目标设定、策略执行和自我反思，并提供由LLM驱动的实时反馈和支架。研究采用组间设计，将SRLAgent与基线系统（无Agent功能的SRL）和传统多媒体学习条件进行比较评估。", "result": "结果显示，SRLAgent组的自主学习技能（p < .001，Cohens d = 0.234）和参与度均显著高于基线组。", "conclusion": "这项工作强调了在游戏化环境中嵌入自主学习支架和实时AI支持的价值，为旨在促进深度学习和元认知技能发展的教育技术提供了设计启示。", "translation": "自主学习（SRL）对于应对日益增长的学业要求和独立性的大学生至关重要。自主学习技能不足会导致学习习惯混乱、动力不足和时间管理不善，从而影响学习者在挑战性环境中茁壮成长。通过一项包含59名大学生的形成性研究，我们确定了学生在发展自主学习技能方面面临的关键挑战，包括目标设定、时间管理和反思性学习的困难。为解决这些挑战，我们引入了SRLAgent，一个由大型语言模型（LLM）辅助的系统，通过游戏化和LLM的自适应支持来培养自主学习技能。SRLAgent以Zimmerman的三阶段自主学习框架为基础，使学生能够在交互式游戏环境中进行目标设定、策略执行和自我反思。该系统提供由LLM驱动的实时反馈和支架，以支持学生的独立学习努力。我们采用组间设计评估了SRLAgent，将其与基线系统（无Agent功能的SRL）和传统多媒体学习条件进行了比较。结果显示，SRLAgent组的自主学习技能显著提高（p < .001，Cohens d = 0.234），并且与基线组相比具有更高的参与度。这项工作强调了在游戏化环境中嵌入自主学习支架和实时AI支持的价值，为旨在促进深度学习和元认知技能发展的教育技术提供了设计启示。", "summary": "本研究介绍了SRLAgent，一个结合游戏化和大型语言模型（LLM）辅助的系统，旨在提升大学生的自主学习（SRL）技能。通过对59名学生的形成性研究，识别出学生在目标设定、时间管理和反思性学习方面的挑战。SRLAgent基于Zimmerman的SRL框架，在一个交互式游戏环境中提供LLM驱动的实时反馈和支架。实验结果表明，与基线系统和传统多媒体学习相比，SRLAgent显著提高了学生的SRL技能和参与度，突出了在游戏化环境中整合AI支持对教育技术的价值。", "keywords": "自主学习, 游戏化, 大型语言模型, 教育技术, 学生参与度", "comments": "SRLAgent的创新之处在于将游戏化与大型语言模型（LLM）的自适应支持相结合，为自主学习技能的培养提供了新颖且有效的途径。该系统能够提供实时反馈和个性化支架，有效地解决了学生在自主学习中面临的实际挑战。这项研究为未来教育技术的设计提供了重要启示，特别是在利用AI促进深度学习和元认知发展方面具有显著潜力。"}}
{"id": "2506.09817", "title": "Enhanced V2X Communication Using Game-Theory Based Adaptive MAC Protocols", "authors": ["Dhrumil Bhatt", "Nirbhay Singhal"], "summary": "This paper presents an enhanced Vehicle-to-Everything (V2X) communication\nsystem featuring adaptive Medium Access Control (MAC) using game theory. Our\napproach integrates dynamic transmission power control, dynamic beacon rates,\ncontention window adaptation, and implicit acknowledgment mechanisms within a\nManhattan-like grid-based mobility scenario. Simulations are conducted in a\ncircular coverage area, incorporating refined signal propagation models and\nprobabilistic vehicle mobility with boundary reflection. The results\ndemonstrate effective beacon delivery with average delays under 0.35 s and\npacket loss rates less than 1% in high-density conditions specifically, with up\nto 80 vehicles operating within a 250 m radius. Key innovations include game\ntheory-based environment-aware transmission parameter adaptation and a scalable\ndesign suited for interference-prone V2X deployments.", "comment": "Accepted at the 16th ICCCNT", "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.09817v1", "AI": {"title_translation": "采用博弈论自适应MAC协议的增强型V2X通信", "tldr": "该论文提出了一种基于博弈论的自适应MAC协议，以增强V2X通信，在高密度环境下实现了低延迟和低丢包率。", "motivation": "增强V2X通信，解决其在复杂环境下的介质访问控制问题，尤其是在干扰严重的V2X部署中。", "method": "提出了一种基于博弈论的自适应介质访问控制（MAC）系统，该系统集成了动态传输功率控制、动态信标速率、竞争窗口自适应以及隐式确认机制。在类似曼哈顿网格的移动场景中，结合了精细的信号传播模型和带边界反射的概率车辆移动模型，在圆形覆盖区域内进行了仿真。", "result": "仿真结果表明，在高达80辆车在250米半径范围内运行的高密度条件下，平均信标传输延迟低于0.35秒，丢包率低于1%。", "conclusion": "该研究成功地通过基于博弈论的自适应MAC协议增强了V2X通信，在高密度、高干扰环境下实现了高效的信标传输，证明了其可扩展性和环境感知传输参数自适应能力。", "translation": "本文提出了一种采用博弈论自适应介质访问控制（MAC）的增强型车联网（V2X）通信系统。我们的方法在类似曼哈顿网格的移动场景中，集成了动态传输功率控制、动态信标速率、竞争窗口自适应和隐式确认机制。仿真在圆形覆盖区域内进行，并结合了精细的信号传播模型和带边界反射的概率车辆移动模型。结果表明，在特别是高达80辆车在250米半径范围内运行的高密度条件下，信标传输有效，平均延迟低于0.35秒，丢包率低于1%。主要创新包括基于博弈论的环境感知传输参数自适应和适用于易受干扰V2X部署的可扩展设计。", "summary": "本文提出了一种基于博弈论的自适应MAC协议，旨在增强V2X通信性能。该协议通过动态调整传输功率、信标速率、竞争窗口和引入隐式确认机制，在高密度车辆环境中进行了仿真验证。结果显示，在250米半径内多达80辆车的高密度场景下，系统能将平均信标延迟控制在0.35秒以内，丢包率低于1%，有效提升了V2X通信的可靠性和效率。", "keywords": "V2X通信, 博弈论, 自适应MAC, 动态传输控制, 信标传输", "comments": "这篇论文的创新点在于将博弈论应用于V2X通信的MAC协议设计，实现了环境感知的传输参数自适应，这对于解决高密度、高干扰V2X部署中的介质访问问题具有重要意义。其可扩展性设计也增强了实际应用潜力。"}}
{"id": "2506.09823", "title": "Frosty for partial synchrony", "authors": ["Stephen Buttolph", "Andrew Lewis-Pye", "Kevin Sekniqi"], "summary": "Snowman is the consensus protocol used by blockchains on Avalanche. Recent\nwork has shown both how to augment Snowman with a `liveness' module called\n`Frosty' that protects against liveness attacks, and also how to modify Snowman\nso as to be consistent in partial synchrony. Since Frosty assumes (a strong\nform of) synchrony, the aim of this note is to show how to modify Frosty to\ndeal with the partially synchronous version of Snowman.", "comment": "arXiv admin note: substantial text overlap with arXiv:2404.14250,\n  arXiv:2501.15904", "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.09823v1", "AI": {"title_translation": "Frosty 适用于部分同步环境", "tldr": "本文旨在展示如何修改共识协议Frosty，使其能在部分同步的Snowman版本中运行。", "motivation": "由于Frosty模块假设强同步性，而Snowman协议已修改为在部分同步环境中保持一致性，因此需要修改Frosty以适应部分同步的Snowman版本。", "method": "本文旨在展示如何修改Frosty模块，使其能够处理部分同步版本的Snowman协议。", "result": "本文展示了如何修改Frosty以处理部分同步的Snowman版本。", "conclusion": "Not mentioned in abstract", "translation": "Snowman 是 Avalanche 区块链上使用的共识协议。最近的工作表明，如何用一个名为“Frosty”的“活性”模块来增强 Snowman，以防止活性攻击，以及如何修改 Snowman 以在部分同步中保持一致。由于 Frosty 假设（一种强形式的）同步性，因此本文的目的是展示如何修改 Frosty 以处理 Snowman 的部分同步版本。", "summary": "Snowman 是 Avalanche 区块链的共识协议。虽然已有的工作通过 Frosty 模块增强了 Snowman 的活性并使其在部分同步中保持一致，但 Frosty 本身假设强同步性。本文旨在展示如何修改 Frosty，使其能够适应并处理 Snowman 的部分同步版本。", "keywords": "Frosty, Snowman, 部分同步, 共识协议, 活性", "comments": "本文的创新点在于解决了共识协议中活性模块在同步假设减弱（从强同步到部分同步）时的兼容性问题，这对于提高区块链协议在更实际网络环境下的鲁棒性具有重要意义。"}}
{"id": "2506.09649", "title": "Mathematical proof of errors in Capasso's excess noise factor formula for an n-step staircase multiplier", "authors": ["Ankitha E Bangera"], "summary": "Solid-state devices such as multistep staircase avalanche photodiodes (APDs)\nare analogues to the photomultiplier tubes and are considered as a\ncascade-amplifier. The major source of internal noise in these APDs is due to\nthe randomness in their stepwise impact ionization. Recent literature on\nstaircase APDs by research groups such as Campbell and co-workers have reported\nthe theoretical estimates of total excess noise factors using Capasso's excess\nnoise factor formula. This formula is based on Friis' total noise factor\nformula for cascade networks. This article proves that Capasso's formula for a\nstaircase APD, erroneously considers the power gains in Friis' total noise\nfactor formula as the gains.", "comment": "Recent research articles that have used Capasso's formula have been\n  published in famous research journals, which include 'Nature Photonics\n  (2021)' and 'Optics (2023).' It is important that the research community is\n  aware that Capasso's formula and its equivalent forms are incorrect. This\n  article points out the errors in the formulas (5 pages, No figures, preprint\n  is under submission)", "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.09649v1", "AI": {"title_translation": "n步阶梯倍增器中Capasso过剩噪声因子公式错误的数学证明", "tldr": "本文数学证明了Capasso用于阶梯APD的过剩噪声因子公式存在错误，因为它错误地理解了Friis公式中的增益。", "motivation": "现有研究团队在阶梯APD的理论估计中广泛使用了Capasso的过剩噪声因子公式，而该公式基于Friis的级联网络总噪声因子公式。本文的动机是指出并纠正Capasso公式中的一个根本性错误。", "method": "本文通过数学证明的方式，指出并验证了Capasso用于阶梯APD的过剩噪声因子公式中存在的错误。", "result": "结果表明，Capasso的公式错误地将Friis总噪声因子公式中的功率增益视为增益。", "conclusion": "Capasso针对阶梯雪崩光电二极管（APD）的过剩噪声因子公式是错误的，因为它在应用Friis总噪声因子公式时，错误地将功率增益当作了增益。", "translation": "固态器件，如多级阶梯雪崩光电二极管（APD），类似于光电倍增管，被认为是级联放大器。这些APD内部噪声的主要来源是其逐步碰撞电离的随机性。最近，Campbell及其同事等研究团队在阶梯APD的文献中，报告了使用Capasso过剩噪声因子公式对总过剩噪声因子进行的理论估计。该公式基于Friis的级联网络总噪声因子公式。本文证明，Capasso用于阶梯APD的公式错误地将Friis总噪声因子公式中的功率增益视为增益。", "summary": "本文通过数学证明，揭示了Capasso用于多级阶梯雪崩光电二极管（APD）的过剩噪声因子公式存在错误。该公式被现有研究广泛应用，但其在解释Friis级联网络总噪声因子公式时，错误地将功率增益与增益混淆，导致了不准确的噪声因子估计。", "keywords": "阶梯雪崩光电二极管, 过剩噪声因子, Capasso公式, Friis公式, 数学证明", "comments": "本文的创新之处在于通过严格的数学证明，纠正了一个在多级阶梯雪崩光电二极管（APD）领域可能被广泛接受和使用的重要公式中的错误。这对于准确评估APD的内部噪声具有重要意义，有助于避免基于错误公式进行理论估计和器件设计。"}}
{"id": "2506.09510", "title": "Generalized Gaussian Entropy Model for Point Cloud Attribute Compression with Dynamic Likelihood Intervals", "authors": ["Changhao Peng", "Yuqi Ye", "Wei Gao"], "summary": "Gaussian and Laplacian entropy models are proved effective in learned point\ncloud attribute compression, as they assist in arithmetic coding of latents.\nHowever, we demonstrate through experiments that there is still unutilized\ninformation in entropy parameters estimated by neural networks in current\nmethods, which can be used for more accurate probability estimation. Thus we\nintroduce generalized Gaussian entropy model, which controls the tail shape\nthrough shape parameter to more accurately estimate the probability of latents.\nMeanwhile, to the best of our knowledge, existing methods use fixed likelihood\nintervals for each integer during arithmetic coding, which limits model\nperformance. We propose Mean Error Discriminator (MED) to determine whether the\nentropy parameter estimation is accurate and then dynamically adjust likelihood\nintervals. Experiments show that our method significantly improves\nrate-distortion (RD) performance on three VAE-based models for point cloud\nattribute compression, and our method can be applied to other compression\ntasks, such as image and video compression.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09510v1", "AI": {"title_translation": "用于点云属性压缩的广义高斯熵模型与动态似然区间", "tldr": "本文提出了一种广义高斯熵模型和均值误差判别器（MED），通过更准确的概率估计和动态调整似然区间，显著提高了点云属性压缩的率失真性能。", "motivation": "现有方法在神经网络估计的熵参数中仍存在未利用的信息，且算术编码中使用固定似然区间限制了模型性能，导致概率估计不够准确。", "method": "引入广义高斯熵模型，通过形状参数控制尾部形状，更准确地估计潜在变量的概率。同时，提出均值误差判别器（MED）来判断熵参数估计是否准确，并动态调整算术编码中的似然区间。", "result": "实验表明，在三种基于VAE的点云属性压缩模型上，本文方法显著提高了率失真（RD）性能。", "conclusion": "本文提出的方法可以应用于其他压缩任务，如图像和视频压缩。", "translation": "高斯和拉普拉斯熵模型已被证明在学习点云属性压缩中是有效的，因为它们有助于潜在变量的算术编码。然而，我们通过实验证明，当前方法中神经网络估计的熵参数中仍存在未利用的信息，可以用于更准确的概率估计。因此，我们引入了广义高斯熵模型，它通过形状参数控制尾部形状，以更准确地估计潜在变量的概率。同时，据我们所知，现有方法在算术编码期间对每个整数使用固定的似然区间，这限制了模型性能。我们提出了均值误差判别器（MED）来确定熵参数估计是否准确，然后动态调整似然区间。实验表明，我们的方法显著提高了三种基于VAE的点云属性压缩模型的率失真（RD）性能，并且我们的方法可以应用于其他压缩任务，例如图像和视频压缩。", "summary": "本文针对点云属性压缩中熵模型和算术编码的局限性，提出了广义高斯熵模型和均值误差判别器（MED）。广义高斯熵模型通过形状参数更准确地估计潜在变量概率，而MED则根据熵参数估计的准确性动态调整似然区间。实验证明，该方法显著提升了点云属性压缩的率失真性能，并具有推广到其他压缩任务的潜力。", "keywords": "点云属性压缩, 广义高斯熵模型, 动态似然区间, 率失真性能, 均值误差判别器", "comments": "该论文的创新点在于提出了广义高斯熵模型和动态似然区间调整机制，有效解决了现有方法中熵参数信息未充分利用和固定似然区间限制性能的问题。通过更精细的概率建模和自适应的编码策略，显著提升了压缩效率。其普适性也表明了该方法在其他数据压缩领域的潜力。"}}
{"id": "2506.09746", "title": "TikTok's Research API: Problems Without Explanations", "authors": ["Carlos Entrena-Serrano", "Martin Degeling", "Salvatore Romano", "Raziye Buse Çetin"], "summary": "Following the Digital Services Act of 2023, which requires Very Large Online\nPlatforms (VLOPs) and Very Large Online Search Engines (VLOSEs) to facilitate\ndata accessibility for independent research, TikTok augmented its Research API\naccess within Europe in July 2023. This action was intended to ensure\ncompliance with the DSA, bolster transparency, and address systemic risks.\nNonetheless, research findings reveal that despite this expansion, notable\nlimitations and inconsistencies persist within the data provided. Our\nexperiment reveals that the API fails to provide metadata for one in eight\nvideos provided through data donations, including official TikTok videos,\nadvertisements, videos from China, and content from specific accounts, without\nan apparent reason. The API data is incomplete, making it unreliable when\nworking with data donations, a prominent methodology for algorithm audits and\nresearch on platform accountability. To monitor the functionality of the API\nand eventual fixes implemented by TikTok, we publish a dashboard with a daily\ncheck of the availability of 10 videos that were not retrievable in the last\nmonth. The video list includes very well-known accounts, notably that of Taylor\nSwift. The current API lacks the necessary capabilities for thorough\nindependent research and scrutiny. It is crucial to support and safeguard\nresearchers who utilize data scraping to independently validate the platform's\ndata quality.", "comment": null, "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.09746v1", "AI": {"title_translation": "TikTok的研究API：问题重重，却无解释", "tldr": "TikTok的欧洲研究API在数据完整性和一致性方面存在严重缺陷，导致独立研究受阻，且无法解释缺失数据的原因。", "motivation": "2023年《数字服务法案》要求大型在线平台向独立研究人员提供数据访问权限，TikTok为此增强了其欧洲研究API。然而，研究人员旨在评估该API是否真正符合规定，并支持透明度和解决系统性风险。", "method": "研究人员通过实验发现API的问题，具体是利用数据捐赠（一种算法审计和平台责任研究的常用方法）来测试API，并监测其提供数据的完整性。他们还发布了一个仪表板，每日检查10个上个月无法检索到的视频的可用性。", "result": "实验显示，TikTok的API未能提供八分之一通过数据捐赠获得的视频的元数据，包括官方TikTok视频、广告、来自中国的视频和特定账户的内容，且没有给出原因。API数据不完整，使其在使用数据捐赠时不可靠。", "conclusion": "当前的TikTok研究API缺乏进行彻底独立研究和审查所需的能力。作者强调支持和保护利用数据抓取来独立验证平台数据质量的研究人员至关重要。", "translation": "在2023年《数字服务法案》要求超大型在线平台（VLOPs）和超大型在线搜索引擎（VLOSEs）便利独立研究的数据可访问性之后，TikTok于2023年7月在欧洲增强了其研究API访问权限。此举旨在确保符合DSA，增强透明度，并解决系统性风险。然而，研究结果表明，尽管进行了这种扩展，所提供的数据中仍存在显著的局限性和不一致性。我们的实验显示，API未能提供八分之一通过数据捐赠获得的视频的元数据，包括官方TikTok视频、广告、来自中国的视频以及特定账户的内容，且没有给出明显原因。API数据不完整，导致在使用数据捐赠时不可靠，而数据捐赠是算法审计和平台责任研究的重要方法。为了监测API的功能和TikTok最终实施的修复措施，我们发布了一个仪表板，每日检查上个月无法检索到的10个视频的可用性。视频列表包括非常知名的账户，特别是泰勒·斯威夫特的账户。当前的API缺乏进行彻底独立研究和审查所需的能力。支持和保护利用数据抓取独立验证平台数据质量的研究人员至关重要。", "summary": "本研究评估了TikTok为遵守《数字服务法案》而在欧洲推出的研究API。尽管API旨在提高透明度，但研究发现其在数据完整性和一致性方面存在严重缺陷，例如未能提供大量视频的元数据，且无合理解释。这使得API数据在进行算法审计和平台责任研究时不可靠。文章指出，当前API不足以支持独立的深入研究，并强调了支持通过数据抓取验证平台数据质量的研究人员的重要性。", "keywords": "TikTok研究API, 数据完整性, 数字服务法案, 平台透明度, 算法审计", "comments": "该论文揭示了大型平台在遵守法规和提供数据透明度方面面临的实际挑战。其创新之处在于通过实验验证了API的局限性，并提供了一个持续监测API功能的仪表板，为后续研究提供了基础。论文的重要性在于其直接影响到对平台算法和内容治理的独立审查，对数字服务法案的有效实施提出了质疑。"}}
{"id": "2506.09414", "title": "PGDA-KGQA: A Prompt-Guided Generative Framework with Multiple Data Augmentation Strategies for Knowledge Graph Question Answering", "authors": ["Xiujun Zhou", "Pingjian Zhang", "Deyou Tang"], "summary": "Knowledge Graph Question Answering (KGQA) is a crucial task in natural\nlanguage processing that requires reasoning over knowledge graphs (KGs) to\nanswer natural language questions. Recent methods utilizing large language\nmodels (LLMs) have shown remarkable semantic parsing capabilities but are\nlimited by the scarcity of diverse annotated data and multi-hop reasoning\nsamples. Traditional data augmentation approaches are focus mainly on\nsingle-hop questions and prone to semantic distortion, while LLM-based methods\nprimarily address semantic distortion but usually neglect multi-hop reasoning,\nthus limiting data diversity. The scarcity of multi-hop samples further weakens\nmodels' generalization. To address these issues, we propose PGDA-KGQA, a\nprompt-guided generative framework with multiple data augmentation strategies\nfor KGQA. At its core, PGDA-KGQA employs a unified prompt-design paradigm: by\ncrafting meticulously engineered prompts that integrate the provided textual\ncontent, it leverages LLMs to generate large-scale (question, logical form)\npairs for model training. Specifically, PGDA-KGQA enriches its training set by:\n(1) generating single-hop pseudo questions to improve the alignment of question\nsemantics with KG relations; (2) applying semantic-preserving question\nrewriting to improve robustness against linguistic variations; (3) employing\nanswer-guided reverse path exploration to create realistic multi-hop questions.\nBy adopting an augment-generate-retrieve semantic parsing pipeline, PGDA-KGQA\nutilizes the augmented data to enhance the accuracy of logical form generation\nand thus improve answer retrieval performance. Experiments demonstrate that\noutperforms state-of-the-art methods on standard KGQA datasets, achieving\nimprovements on WebQSP by 2.8%, 1.2%, and 3.1% and on ComplexWebQuestions by\n1.8%, 1.1%, and 2.4% in F1, Hits@1, and Accuracy, respectively.", "comment": "13 pages, 7 figures, 5 tables", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.09414v1", "AI": {"title_translation": "PGDA-KGQA: 一种面向知识图谱问答的提示引导生成框架及多数据增强策略", "tldr": "PGDA-KGQA提出了一种新的提示引导生成框架，结合多种数据增强策略，通过LLM生成大规模问答对，有效解决了知识图谱问答中数据稀缺和多跳推理不足的问题，并在标准数据集上超越了SOTA方法。", "motivation": "现有的基于大型语言模型（LLM）的知识图谱问答（KGQA）方法受限于多样化标注数据和多跳推理样本的稀缺性。传统数据增强方法主要关注单跳问题且易产生语义扭曲，而基于LLM的方法虽能解决语义扭曲但常忽略多跳推理，从而限制了数据多样性，进一步削弱了模型的泛化能力。", "method": "PGDA-KGQA是一种提示引导的生成框架，采用统一的提示设计范式，利用LLM生成大规模（问题，逻辑形式）对用于模型训练。它通过以下方式丰富训练集：1) 生成单跳伪问题以改善问题语义与知识图谱关系的对齐；2) 应用语义保留的问题重写以提高对语言变异的鲁棒性；3) 采用答案引导的逆向路径探索来创建真实的多跳问题。PGDA-KGQA采用“增强-生成-检索”的语义解析流程，利用增强数据提高逻辑形式生成精度和答案检索性能。", "result": "实验表明，PGDA-KGQA在标准KGQA数据集上优于现有SOTA方法，在WebQSP数据集上，F1、Hits@1和Accuracy分别提升了2.8%、1.2%和3.1%；在ComplexWebQuestions数据集上，F1、Hits@1和Accuracy分别提升了1.8%、1.1%和2.4%。", "conclusion": "PGDA-KGQA通过其创新的提示引导生成框架和多数据增强策略，成功解决了知识图谱问答中数据稀缺和多跳推理的挑战，显著提升了模型性能，并在多个标准数据集上超越了现有最佳方法，证明了其有效性和优越性。", "translation": "知识图谱问答（KGQA）是自然语言处理中的一项关键任务，需要对知识图谱（KG）进行推理以回答自然语言问题。最近利用大型语言模型（LLM）的方法展现出卓越的语义解析能力，但受限于多样化标注数据和多跳推理样本的稀缺性。传统数据增强方法主要关注单跳问题且易产生语义扭曲，而基于LLM的方法主要解决语义扭曲但通常忽略多跳推理，从而限制了数据多样性。多跳样本的稀缺性进一步削弱了模型的泛化能力。为了解决这些问题，我们提出了PGDA-KGQA，一个用于KGQA的提示引导生成框架，具有多种数据增强策略。PGDA-KGQA的核心是采用统一的提示设计范式：通过精心设计的提示，整合所提供的文本内容，利用LLM生成大规模的（问题，逻辑形式）对用于模型训练。具体而言，PGDA-KGQA通过以下方式丰富其训练集：(1) 生成单跳伪问题以改善问题语义与KG关系的对齐；(2) 应用语义保留的问题重写以提高对语言变异的鲁棒性；(3) 采用答案引导的逆向路径探索来创建真实的多跳问题。通过采用“增强-生成-检索”的语义解析流程，PGDA-KGQA利用增强数据来提高逻辑形式生成的准确性，从而提高答案检索性能。实验表明，PGDA-KGQA在标准KGQA数据集上优于现有SOTA方法，在WebQSP上F1、Hits@1和Accuracy分别提升了2.8%、1.2%和3.1%，在ComplexWebQuestions上分别提升了1.8%、1.1%和2.4%。", "summary": "PGDA-KGQA提出了一种新颖的提示引导生成框架，旨在解决知识图谱问答（KGQA）中数据稀缺和多跳推理不足的问题。该框架利用大型语言模型（LLM）通过精心设计的提示生成大规模（问题，逻辑形式）训练对，并通过三种数据增强策略（生成单跳伪问题、语义保留的问题重写、答案引导的逆向路径探索）丰富训练数据，尤其关注多跳推理。实验结果表明，PGDA-KGQA在标准KGQA数据集上显著优于现有SOTA方法，提升了逻辑形式生成和答案检索的准确性。", "keywords": "知识图谱问答, 数据增强, 大型语言模型, 提示工程, 多跳推理", "comments": "该论文的创新点在于结合了LLM的生成能力与多策略数据增强，特别是在解决多跳推理样本稀缺性方面的努力。其“增强-生成-检索”的语义解析流程是一个全面的解决方案。通过细致的提示工程，PGDA-KGQA能够有效地利用LLM生成高质量的训练数据，克服了传统方法和单一LLM方法的局限性。其在标准数据集上的显著性能提升证明了该框架的有效性和实用性。"}}
{"id": "2506.09519", "title": "Segregated Runge-Kutta schemes for the time integration of the incompressible Navier-Stokes equations in presence of pressure stabilization", "authors": ["Pavel Bakhvalov"], "summary": "Segregated Runge-Kutta (SRK) schemes are time integration methods for the\nincompressible Navier-Stokes equations. In this approach, convection and\ndiffusion can be independently treated either explicitly or implicitly, which\nin particular allows to construct implicit-explicit (IMEX) methods. Original\nSRK schemes (Colomes, Badia, IJNME, 2015) are designed for finite-element\nmethods that satisfy the inf-sup condition. In this paper, the idea of SRK\nschemes is generalized to spatial discretizations with pressure stabilization.\nIn the numerical experiments, SRK schemes are demonstrated with both\nfinite-difference and finite element spatial discretizations. Numerical results\nshow that one of the SRK schemes outperforms the third-order multistep\nprojection-based method in terms of accuracy while preserving the computational\ncosts.", "comment": null, "cate": "math.NA", "url": "http://arxiv.org/abs/2506.09519v1", "AI": {"title_translation": "用于存在压力稳定化的情况下不可压缩纳维-斯托克斯方程时间积分的分离式龙格-库塔方案", "tldr": "本文将分离式龙格-库塔（SRK）方案推广到具有压力稳定化的空间离散化方法，并在数值实验中展示其在精度上优于现有方法，同时保持计算成本。", "motivation": "原始的分离式龙格-库塔（SRK）方案是为满足inf-sup条件的有限元方法设计的。本文的动机是将其推广到具有压力稳定化的空间离散化方法，以扩大其适用范围。", "method": "本文将分离式龙格-库塔（SRK）方案推广到具有压力稳定化的空间离散化方法。通过有限差分和有限元空间离散化方法进行了数值实验。", "result": "数值结果表明，其中一个SRK方案在精度方面优于三阶多步基于投影的方法，同时保持了计算成本。", "conclusion": "推广后的分离式龙格-库塔（SRK）方案在存在压力稳定化的情况下，能够有效且高效地对不可压缩纳维-斯托克斯方程进行时间积分，并在精度上表现出色。", "translation": "分离式龙格-库塔（SRK）方案是不可压缩纳维-斯托克斯方程的时间积分方法。在这种方法中，对流和扩散可以独立地进行显式或隐式处理，这尤其允许构建隐式-显式（IMEX）方法。原始的SRK方案（Colomes, Badia, IJNME, 2015）是为满足inf-sup条件的有限元方法设计的。在本文中，SRK方案的思想被推广到具有压力稳定化的空间离散化方法。在数值实验中，SRK方案通过有限差分和有限元空间离散化方法进行了演示。数值结果表明，其中一个SRK方案在精度方面优于三阶多步基于投影的方法，同时保持了计算成本。", "summary": "本文将用于不可压缩纳维-斯托克斯方程时间积分的分离式龙格-库塔（SRK）方案进行了推广，使其适用于具有压力稳定化的空间离散化方法。该方案允许对流和扩散进行独立的显式或隐式处理，从而构建IMEX方法。数值实验证实，推广后的SRK方案在保持计算成本的同时，其精度优于现有的三阶多步基于投影的方法。", "keywords": "分离式龙格-库塔方案, 不可压缩纳维-斯托克斯方程, 压力稳定化, 时间积分, 有限元", "comments": "本文的主要创新在于将分离式龙格-库塔（SRK）方案推广到适用于具有压力稳定化的空间离散化方法，这显著扩大了其应用范围。其重要性在于提供了一种在保持计算效率的同时提高不可压缩纳维-斯托克斯方程时间积分精度的新方法。"}}
{"id": "2506.09874", "title": "UmbraTTS: Adapting Text-to-Speech to Environmental Contexts with Flow Matching", "authors": ["Neta Glazer", "Aviv Navon", "Yael Segal", "Aviv Shamsian", "Hilit Segev", "Asaf Buchnick", "Menachem Pirchi", "Gil Hetz", "Joseph Keshet"], "summary": "Recent advances in Text-to-Speech (TTS) have enabled highly natural speech\nsynthesis, yet integrating speech with complex background environments remains\nchallenging. We introduce UmbraTTS, a flow-matching based TTS model that\njointly generates both speech and environmental audio, conditioned on text and\nacoustic context. Our model allows fine-grained control over background volume\nand produces diverse, coherent, and context-aware audio scenes. A key challenge\nis the lack of data with speech and background audio aligned in natural\ncontext. To overcome the lack of paired training data, we propose a\nself-supervised framework that extracts speech, background audio, and\ntranscripts from unannotated recordings. Extensive evaluations demonstrate that\nUmbraTTS significantly outperformed existing baselines, producing natural,\nhigh-quality, environmentally aware audios.", "comment": null, "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.09874v1", "AI": {"title_translation": "UmbraTTS：使用流匹配技术使文本转语音适应环境背景", "tldr": "UmbraTTS是一个基于流匹配的文本转语音模型，可以同时生成语音和环境音频，解决了在复杂背景下语音合成的挑战，并提出了自监督框架来解决数据缺乏问题，效果优于现有基线。", "motivation": "现有文本转语音技术难以将语音与复杂的背景环境整合，并且缺乏自然环境中语音和背景音频对齐的数据。", "method": "提出UmbraTTS，一个基于流匹配的TTS模型，能够根据文本和声学上下文共同生成语音和环境音频。为克服数据缺乏，提出了一个自监督框架，从无标注录音中提取语音、背景音频和文本。", "result": "UmbraTTS显著优于现有基线，生成了自然、高质量且具有环境感知能力的音频。", "conclusion": "UmbraTTS成功解决了文本转语音在复杂环境下的集成挑战，并能生成高质量、环境感知的音频。", "translation": "文本转语音（TTS）的最新进展已实现了高度自然的语音合成，但将语音与复杂的背景环境相结合仍然具有挑战性。我们引入了UmbraTTS，一个基于流匹配的TTS模型，它根据文本和声学上下文共同生成语音和环境音频。我们的模型允许对背景音量进行细粒度控制，并产生多样化、连贯且具有上下文感知能力的音频场景。一个关键挑战是缺乏在自然语境中语音和背景音频对齐的数据。为了克服配对训练数据不足的问题，我们提出了一种自监督框架，可以从无标注录音中提取语音、背景音频和文本。广泛的评估表明，UmbraTTS显著优于现有基线，生成了自然、高质量且具有环境感知能力的音频。", "summary": "UmbraTTS是一个创新的文本转语音模型，它利用流匹配技术，能够同时生成语音和相关的环境背景音。该模型解决了在复杂环境中整合语音的难题，并能对背景音量进行精细控制。为了应对缺乏配对训练数据的问题，研究人员开发了一个自监督框架，可以从无标注录音中提取所需数据。实验结果表明，UmbraTTS在生成自然、高质量且具有环境感知能力的音频方面，表现优于现有模型。", "keywords": "文本转语音, 环境感知, 流匹配, 自监督学习, 音频合成", "comments": "这篇论文的创新点在于提出了一个基于流匹配的TTS模型，不仅能生成语音，还能同时生成环境音频，从而更好地适应环境上下文。此外，为解决数据稀缺问题而设计的自监督框架也具有重要意义，使其能够在无标注数据上进行训练。"}}
{"id": "2506.09719", "title": "On the Virtues of Information Security in the UK Climate Movement", "authors": ["Mikaela Brough", "Rikke Bjerg Jensen", "Martin R. Albrecht"], "summary": "We report on an ethnographic study with members of the climate movement in\nthe United Kingdom (UK). We conducted participant observation and interviews at\nprotests and in various activist settings. Reporting on the findings as they\nrelate to information security, we show that members of the UK climate movement\nwrestled with (i) a fundamental tension between openness and secrecy; (ii)\ntensions between autonomy and collective interdependence in\ninformation-security decision-making; (iii) conflicting activist ideals that\nshape security discourses; and (iv) pressures from different social gazes --\nfrom each other, from people outside the movement and from their adversaries.\nOverall, our findings shed light on the social complexities of\ninformation-security research in activist settings and provoke methodological\nquestions about programmes that aim to design for activists.", "comment": "To appear at the USENIX Security Symposium 2025", "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.09719v1", "AI": {"title_translation": "英国气候运动中信息安全的优点", "tldr": "一项对英国气候运动成员的民族志研究揭示了他们在信息安全方面面临的复杂挑战，包括开放与保密、自主与集体依赖、理想冲突以及社会压力。", "motivation": "该研究旨在通过民族志方法，深入理解英国气候运动成员在信息安全实践中所面临的复杂社会维度和挑战。", "method": "研究采用民族志方法，具体包括在抗议活动和各种积极分子环境中进行参与式观察和访谈。", "result": "英国气候运动成员在信息安全方面面临以下挑战：(i) 开放与保密之间的根本张力；(ii) 信息安全决策中自主性与集体相互依赖性之间的张力；(iii) 塑造安全话语的冲突的积极分子理想；以及 (iv) 来自彼此、运动外部人员及其对手的不同社会目光的压力。", "conclusion": "研究结果揭示了积极分子环境中信息安全研究的社会复杂性，并引发了关于旨在为积极分子进行设计的项目的方法论问题。", "translation": "我们报告了一项对英国气候运动成员的民族志研究。我们在抗议活动和各种积极分子环境中进行了参与式观察和访谈。在报告与信息安全相关的发现时，我们展示了英国气候运动的成员在以下方面挣扎：(i) 开放与保密之间的根本张力；(ii) 信息安全决策中自主性与集体相互依赖性之间的张力；(iii) 塑造安全话语的冲突的积极分子理想；以及 (iv) 来自不同社会凝视——彼此之间、运动外部人员及其对手——的压力。总的来说，我们的发现揭示了积极分子环境中信息安全研究的社会复杂性，并引发了关于旨在为积极分子进行设计的项目的方法论问题。", "summary": "这项民族志研究深入探讨了英国气候运动成员在信息安全方面的实践与挑战。研究通过参与式观察和访谈发现，这些成员在信息安全决策中面临开放与保密、个人自主与集体依赖、内在理想冲突以及外部社会压力等多重复杂性。研究结果不仅揭示了积极分子环境中信息安全的社会复杂性，也对未来为积极分子设计的安全方案提出了方法论上的思考。", "keywords": "信息安全, 气候运动, 民族志, 积极分子, 英国", "comments": "这项研究通过民族志方法深入分析了信息安全在积极分子群体中的实际运作，揭示了其固有的社会和伦理复杂性。它超越了技术层面，强调了文化、理想和外部压力如何塑造安全实践，为信息安全研究和相关设计提供了宝贵的社会学视角，尤其对为边缘群体设计安全工具具有重要启示。"}}
{"id": "2506.09491", "title": "DCIRNet: Depth Completion with Iterative Refinement for Dexterous Grasping of Transparent and Reflective Objects", "authors": ["Guanghu Xie", "Zhiduo Jiang", "Yonglong Zhang", "Yang Liu", "Zongwu Xie", "Baoshi Cao", "Hong Liu"], "summary": "Transparent and reflective objects in everyday environments pose significant\nchallenges for depth sensors due to their unique visual properties, such as\nspecular reflections and light transmission. These characteristics often lead\nto incomplete or inaccurate depth estimation, which severely impacts downstream\ngeometry-based vision tasks, including object recognition, scene\nreconstruction, and robotic manipulation. To address the issue of missing depth\ninformation in transparent and reflective objects, we propose DCIRNet, a novel\nmultimodal depth completion network that effectively integrates RGB images and\ndepth maps to enhance depth estimation quality. Our approach incorporates an\ninnovative multimodal feature fusion module designed to extract complementary\ninformation between RGB images and incomplete depth maps. Furthermore, we\nintroduce a multi-stage supervision and depth refinement strategy that\nprogressively improves depth completion and effectively mitigates the issue of\nblurred object boundaries. We integrate our depth completion model into\ndexterous grasping frameworks and achieve a $44\\%$ improvement in the grasp\nsuccess rate for transparent and reflective objects. We conduct extensive\nexperiments on public datasets, where DCIRNet demonstrates superior\nperformance. The experimental results validate the effectiveness of our\napproach and confirm its strong generalization capability across various\ntransparent and reflective objects.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.09491v1", "AI": {"title_translation": "DCIRNet：用于透明和反射物体灵巧抓取的迭代细化深度补全", "tldr": "DCIRNet是一种多模态深度补全网络，通过融合RGB图像和深度图，并采用多阶段监督和细化策略，显著提高了透明和反射物体的深度估计质量和机器人抓取成功率。", "motivation": "透明和反射物体由于其独特的光学特性（如镜面反射和光线传输），对深度传感器构成重大挑战，导致深度估计不完整或不准确，严重影响了物体识别、场景重建和机器人操作等下游基于几何的视觉任务。", "method": "本文提出了DCIRNet，一个新颖的多模态深度补全网络，有效整合RGB图像和深度图以提高深度估计质量。该方法包含一个创新的多模态特征融合模块，用于提取RGB图像和不完整深度图之间的互补信息。此外，引入了多阶段监督和深度细化策略，以逐步改善深度补全并有效缓解物体边界模糊问题。", "result": "DCIRNet使透明和反射物体的抓取成功率提高了44%。在公共数据集上的广泛实验表明，DCIRNet表现出卓越的性能，验证了其方法的有效性和对各种透明和反射物体的强大泛化能力。", "conclusion": "DCIRNet成功解决了透明和反射物体的深度信息缺失问题，显著提高了深度估计质量和机器人灵巧抓取成功率，并展示了强大的泛化能力。", "translation": "日常生活环境中的透明和反射物体由于其独特的视觉特性，如镜面反射和光线传输，对深度传感器构成了重大挑战。这些特性常常导致深度估计不完整或不准确，这严重影响了下游基于几何的视觉任务，包括物体识别、场景重建和机器人操作。为了解决透明和反射物体深度信息缺失的问题，我们提出了DCIRNet，一个新颖的多模态深度补全网络，它有效整合了RGB图像和深度图以提高深度估计质量。我们的方法包含一个创新的多模态特征融合模块，旨在提取RGB图像和不完整深度图之间的互补信息。此外，我们引入了一种多阶段监督和深度细化策略，该策略逐步改善深度补全并有效缓解物体边界模糊的问题。我们将我们的深度补全模型整合到灵巧抓取框架中，并将透明和反射物体的抓取成功率提高了44%。我们在公共数据集上进行了广泛的实验，DCIRNet展示了卓越的性能。实验结果验证了我们方法的有效性并证实了其对各种透明和反射物体的强大泛化能力。", "summary": "DCIRNet是一个针对透明和反射物体深度补全的新型多模态网络。它通过融合RGB图像和深度图，并采用创新的特征融合模块和多阶段深度细化策略，有效解决了透明和反射物体深度信息缺失和边界模糊问题。该模型在机器人灵巧抓取任务中表现出色，将透明和反射物体的抓取成功率提高了44%，并在公共数据集上展示了卓越的性能和强大的泛化能力。", "keywords": "深度补全, 透明物体, 反射物体, 灵巧抓取, 多模态", "comments": "本文提出了一种新颖的多模态深度补全网络DCIRNet，其创新点在于结合了RGB图像和深度图的多模态特征融合模块，以及分阶段的监督和深度细化策略，有效解决了透明和反射物体深度估计的难题。该研究的重要性体现在其显著提高了机器人对这类复杂物体的抓取成功率，为机器人操作在现实世界中的应用提供了重要支持。DCIRNet的泛化能力也值得肯定。"}}
{"id": "2506.09237", "title": "PatchGuard: Adversarially Robust Anomaly Detection and Localization through Vision Transformers and Pseudo Anomalies", "authors": ["Mojtaba Nafez", "Amirhossein Koochakian", "Arad Maleki", "Jafar Habibi", "Mohammad Hossein Rohban"], "summary": "Anomaly Detection (AD) and Anomaly Localization (AL) are crucial in fields\nthat demand high reliability, such as medical imaging and industrial\nmonitoring. However, current AD and AL approaches are often susceptible to\nadversarial attacks due to limitations in training data, which typically\ninclude only normal, unlabeled samples. This study introduces PatchGuard, an\nadversarially robust AD and AL method that incorporates pseudo anomalies with\nlocalization masks within a Vision Transformer (ViT)-based architecture to\naddress these vulnerabilities. We begin by examining the essential properties\nof pseudo anomalies, and follow it by providing theoretical insights into the\nattention mechanisms required to enhance the adversarial robustness of AD and\nAL systems. We then present our approach, which leverages Foreground-Aware\nPseudo-Anomalies to overcome the deficiencies of previous anomaly-aware\nmethods. Our method incorporates these crafted pseudo-anomaly samples into a\nViT-based framework, with adversarial training guided by a novel loss function\ndesigned to improve model robustness, as supported by our theoretical analysis.\nExperimental results on well-established industrial and medical datasets\ndemonstrate that PatchGuard significantly outperforms previous methods in\nadversarial settings, achieving performance gains of $53.2\\%$ in AD and\n$68.5\\%$ in AL, while also maintaining competitive accuracy in non-adversarial\nsettings. The code repository is available at\nhttps://github.com/rohban-lab/PatchGuard .", "comment": "Accepted to the Conference on Computer Vision and Pattern Recognition\n  (CVPR) 2025", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09237v1", "AI": {"title_translation": "PatchGuard：通过视觉Transformer和伪异常实现对抗性鲁棒异常检测与定位", "tldr": "PatchGuard是一种基于Vision Transformer的异常检测和定位方法，通过引入伪异常和对抗训练，显著提高了在对抗环境下的鲁棒性，同时保持了非对抗环境下的准确性。", "motivation": "当前的异常检测（AD）和异常定位（AL）方法通常容易受到对抗性攻击，因为训练数据通常只包含正常的、未标记的样本，导致了鲁棒性不足。", "method": "本研究提出了PatchGuard，一种对抗性鲁棒的AD和AL方法。它将带有定位掩码的伪异常整合到基于Vision Transformer（ViT）的架构中。该方法首先研究了伪异常的基本属性，并提供了关于注意力机制的理论见解，以增强AD和AL系统的对抗鲁棒性。然后，利用前景感知伪异常来克服现有方法的缺陷。PatchGuard将这些精心制作的伪异常样本整合到ViT框架中，并通过一种新的、旨在提高模型鲁棒性的损失函数进行对抗性训练。", "result": "PatchGuard在对抗性设置下，异常检测（AD）性能提升了53.2%，异常定位（AL）性能提升了68.5%，显著优于现有方法。同时，在非对抗性设置下也保持了有竞争力的准确性。", "conclusion": "PatchGuard通过结合Vision Transformer、伪异常和对抗性训练，有效解决了现有异常检测和定位方法在对抗性攻击下的脆弱性问题，显著提升了系统的鲁棒性和性能。", "translation": "异常检测（AD）和异常定位（AL）在要求高可靠性的领域至关重要，例如医学成像和工业监测。然而，由于训练数据通常只包含正常的、未标记的样本，当前的AD和AL方法往往容易受到对抗性攻击。本研究引入了PatchGuard，这是一种对抗性鲁棒的AD和AL方法，它在基于视觉Transformer（ViT）的架构中结合了带有定位掩码的伪异常，以解决这些漏洞。我们首先检查了伪异常的基本属性，然后提供了关于增强AD和AL系统对抗鲁棒性所需的注意力机制的理论见解。随后，我们提出了我们的方法，该方法利用前景感知伪异常来克服以前异常感知方法的不足。我们的方法将这些精心制作的伪异常样本整合到基于ViT的框架中，并通过一种新的损失函数指导对抗性训练，该损失函数旨在提高模型鲁棒性，这得到了我们的理论分析的支持。在成熟的工业和医学数据集上的实验结果表明，PatchGuard在对抗性设置下显著优于以前的方法，在AD方面实现了53.2%的性能提升，在AL方面实现了68.5%的性能提升，同时在非对抗性设置下也保持了有竞争力的准确性。代码库可在https://github.com/rohban-lab/PatchGuard 获取。", "summary": "PatchGuard是一种新颖的、对抗性鲁棒的异常检测和定位方法，旨在解决现有方法在对抗性攻击下的脆弱性。它利用基于Vision Transformer的架构，并整合了带有定位掩码的伪异常以及前景感知伪异常。通过理论分析和新颖的损失函数引导的对抗性训练，PatchGuard在工业和医学数据集上表现出色，尤其在对抗性环境下，AD和AL性能分别提升了53.2%和68.5%，同时保持了非对抗性下的高准确性。", "keywords": "异常检测, 异常定位, 对抗性鲁棒性, Vision Transformer, 伪异常", "comments": "PatchGuard的创新之处在于其将伪异常（特别是前景感知伪异常）与Vision Transformer架构相结合，并通过理论指导的对抗性训练来增强模型的对抗性鲁棒性。这对于医疗成像和工业监测等高可靠性要求的领域至关重要，因为这些领域中的异常检测系统对对抗性攻击的脆弱性是一个重大安全隐患。该研究不仅提出了一个有效的方法，还提供了理论见解和显著的性能提升，这使其成为异常检测和对抗性机器学习领域的重要贡献。"}}
{"id": "2506.08672", "title": "RuleReasoner: Reinforced Rule-based Reasoning via Domain-aware Dynamic Sampling", "authors": ["Yang Liu", "Jiaqi Li", "Zilong Zheng"], "summary": "Rule-based reasoning has been acknowledged as one of the fundamental problems\nin reasoning, while deviations in rule formats, types, and complexity in\nreal-world applications pose severe challenges. Recent studies have shown that\nlarge reasoning models (LRMs) have remarkable reasoning capabilities, and their\nperformance is substantially enhanced by reinforcement learning (RL). However,\nit remains an open question whether small reasoning models (SRMs) can learn\nrule-based reasoning effectively with robust generalization across diverse\ntasks and domains. To address this, we introduce Reinforced Rule-based\nReasoning, a.k.a. RuleReasoner, a simple yet effective method to conduct\nrule-based reasoning via a wide collection of curated tasks and a novel\ndomain-aware dynamic sampling approach. Specifically, RuleReasoner resamples\neach training batch by updating the sampling weights of different domains based\non historical rewards. This facilitates domain augmentation and flexible online\nlearning schedules for RL, obviating the need for pre-hoc human-engineered\nmix-training recipes used in existing methods. Empirical evaluations on\nin-distribution (ID) and out-of-distribution (OOD) benchmarks reveal that\nRuleReasoner outperforms frontier LRMs by a significant margin ($\\Delta$4.1%\naverage points on eight ID tasks and $\\Delta$10.4% average points on three OOD\ntasks over OpenAI-o1). Notably, our approach also exhibits higher computational\nefficiency compared to prior dynamic sampling methods for RL.", "comment": "22 pages, 10 figures, 8 tables", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.08672v1", "AI": {"title_translation": "RuleReasoner: 基于领域感知动态采样的强化规则推理", "tldr": "RuleReasoner通过领域感知动态采样，显著提升了小推理模型在规则推理任务上的泛化能力，超越了大型推理模型并提高了计算效率。", "motivation": "尽管大型推理模型（LRM）在规则推理方面表现出色，但现有方法在应对现实世界中规则格式、类型和复杂性变化时面临挑战，且小型推理模型（SRM）能否有效学习规则推理并具有强大的泛化能力仍是未解之谜。", "method": "本文提出了RuleReasoner，一种通过收集大量精选任务和新颖的领域感知动态采样方法进行规则推理的简单有效方法。RuleReasoner通过根据历史奖励更新不同领域的采样权重来重新采样每个训练批次，从而促进领域增强和灵活的在线学习调度，无需预先设计的人工混合训练方案。", "result": "RuleReasoner在分布内（ID）和分布外（OOD）基准测试中表现显著优于前沿大型推理模型，在八个ID任务上平均提升4.1%，在三个OOD任务上平均提升10.4%（相对于OpenAI-o1）。此外，RuleReasoner相比现有强化学习动态采样方法展现出更高的计算效率。", "conclusion": "RuleReasoner通过其新颖的领域感知动态采样方法，成功地使小型推理模型在规则推理任务上实现了强大的泛化能力和卓越的性能，同时提高了计算效率，解决了现有方法的局限性。", "translation": "基于规则的推理被认为是推理领域的基本问题之一，而现实应用中规则格式、类型和复杂性的偏差带来了严峻挑战。最近的研究表明，大型推理模型（LRM）具有卓越的推理能力，并且其性能通过强化学习（RL）得到了显著增强。然而，小型推理模型（SRM）是否能够有效学习基于规则的推理并在不同任务和领域中实现强大的泛化能力仍然是一个悬而未决的问题。为了解决这个问题，我们引入了强化规则推理，即RuleReasoner，这是一种通过大量精选任务和新颖的领域感知动态采样方法进行规则推理的简单而有效的方法。具体而言，RuleReasoner通过根据历史奖励更新不同领域的采样权重来重新采样每个训练批次。这促进了领域增强和强化学习的灵活在线学习调度，避免了现有方法中需要预先进行人工设计的混合训练方案。对分布内（ID）和分布外（OOD）基准测试的实证评估表明，RuleReasoner以显著的优势超越了前沿大型推理模型（在八个ID任务上平均提升4.1%，在三个OOD任务上平均提升10.4%（相对于OpenAI-o1））。值得注意的是，与之前的强化学习动态采样方法相比，我们的方法还表现出更高的计算效率。", "summary": "本文介绍了RuleReasoner，一种旨在增强小型推理模型在规则推理任务上泛化能力的方法。针对现有方法在规则多样性和复杂性上的挑战，RuleReasoner利用领域感知动态采样，根据历史奖励调整训练批次的采样权重，从而实现领域增强和灵活的在线学习。实验结果表明，RuleReasoner在分布内和分布外任务上均显著优于现有大型推理模型，并展现出更高的计算效率。", "keywords": "规则推理, 强化学习, 动态采样, 领域感知, 小型推理模型", "comments": "RuleReasoner的创新点在于其领域感知动态采样机制，该机制有效地解决了现有方法中需要人工设计混合训练方案的痛点，并通过强化学习实现了对小型推理模型泛化能力的显著提升。其在OOD任务上的优异表现和更高的计算效率也凸显了其在实际应用中的潜力。"}}
{"id": "2506.09104", "title": "Unifying Block-wise PTQ and Distillation-based QAT for Progressive Quantization toward 2-bit Instruction-Tuned LLMs", "authors": ["Jung Hyun Lee", "Seungjae Shin", "Vinnam Kim", "Jaeseong You", "An Chen"], "summary": "As the rapid scaling of large language models (LLMs) poses significant\nchallenges for deployment on resource-constrained devices, there is growing\ninterest in extremely low-bit quantization, such as 2-bit. Although prior works\nhave shown that 2-bit large models are pareto-optimal over their 4-bit smaller\ncounterparts in both accuracy and latency, these advancements have been limited\nto pre-trained LLMs and have not yet been extended to instruction-tuned models.\nTo bridge this gap, we propose Unified Progressive Quantization (UPQ)$-$a novel\nprogressive quantization framework (FP16$\\rightarrow$INT4$\\rightarrow$INT2)\nthat unifies block-wise post-training quantization (PTQ) with\ndistillation-based quantization-aware training (Distill-QAT) for INT2\ninstruction-tuned LLM quantization. UPQ first quantizes FP16 instruction-tuned\nmodels to INT4 using block-wise PTQ to significantly reduce the quantization\nerror introduced by subsequent INT2 quantization. Next, UPQ applies Distill-QAT\nto enable INT2 instruction-tuned LLMs to generate responses consistent with\ntheir original FP16 counterparts by minimizing the generalized Jensen-Shannon\ndivergence (JSD) between the two. To the best of our knowledge, we are the\nfirst to demonstrate that UPQ can quantize open-source instruction-tuned LLMs\nto INT2 without relying on proprietary post-training data, while achieving\nstate-of-the-art performances on MMLU and IFEval$-$two of the most\nrepresentative benchmarks for evaluating instruction-tuned LLMs.", "comment": "Preprint", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09104v1", "AI": {"title_translation": "将块级PTQ和基于蒸馏的QAT统一用于2比特指令微调LLM的渐进式量化", "tldr": "提出UPQ框架，将块级PTQ和蒸馏QAT结合，实现2比特指令微调LLM的渐进式量化，无需专有数据，性能优异。", "motivation": "大型语言模型（LLM）的快速扩展给资源受限设备的部署带来挑战。尽管2比特极低位量化受到关注并在预训练LLM上取得进展，但这些进展尚未扩展到指令微调模型。本文旨在弥合这一差距，实现指令微调LLM的2比特量化。", "method": "本文提出统一渐进量化（UPQ）框架，这是一种新颖的渐进式量化方法（FP16→INT4→INT2），它将块级训练后量化（PTQ）与基于蒸馏的量化感知训练（Distill-QAT）相结合，用于INT2指令微调LLM量化。UPQ首先使用块级PTQ将FP16指令微调模型量化为INT4，以显著减少后续INT2量化引入的误差。接着，UPQ应用Distill-QAT，通过最小化广义Jensen-Shannon散度（JSD），使INT2指令微调LLM能够生成与原始FP16模型一致的响应。", "result": "据作者所知，这是首次证明UPQ可以在不依赖专有训练后数据的情况下，将开源指令微调LLM量化到INT2。同时，UPQ在MMLU和IFEval（两个评估指令微调LLM的代表性基准）上实现了最先进的性能。", "conclusion": "UPQ框架成功地将开源指令微调LLMs量化到2比特，并在MMLU和IFEval等重要基准上取得了最先进的性能，填补了该领域在指令微调模型2比特量化方面的空白，且无需依赖专有数据。", "translation": "随着大型语言模型（LLM）的快速扩展给资源受限设备的部署带来了巨大挑战，人们对2比特等极低位量化越来越感兴趣。尽管先前的研究表明，2比特大型模型在准确性和延迟方面优于4比特较小模型，但这些进展仅限于预训练LLM，尚未扩展到指令微调模型。为了弥合这一差距，我们提出了统一渐进量化（UPQ）——一种新颖的渐进量化框架（FP16→INT4→INT2），它将块级训练后量化（PTQ）与基于蒸馏的量化感知训练（Distill-QAT）相结合，用于INT2指令微调LLM量化。UPQ首先使用块级PTQ将FP16指令微调模型量化为INT4，以显著减少后续INT2量化引入的误差。接下来，UPQ应用Distill-QAT，通过最小化两者之间的广义Jensen-Shannon散度（JSD），使INT2指令微调LLM能够生成与其原始FP16模型一致的响应。据我们所知，我们是第一个证明UPQ可以在不依赖专有训练后数据的情况下，将开源指令微调LLM量化到INT2，同时在MMLU和IFEval（评估指令微调LLM的两个最具代表性的基准）上实现最先进的性能。", "summary": "本文提出了统一渐进量化（UPQ）框架，旨在解决资源受限设备上部署LLM的挑战，特别是将2比特量化扩展到指令微调模型。UPQ采用FP16→INT4→INT2的渐进式量化路径，并创新性地结合了块级PTQ和基于蒸馏的QAT。其中，PTQ用于将模型初步量化到INT4以减少误差，Distill-QAT则通过最小化JSD确保INT2模型与FP16模型行为一致。研究首次证明UPQ能将开源指令微调LLM量化到INT2，无需专有数据，并在MMLU和IFEval基准上取得了最先进的性能。", "keywords": "量化, 指令微调LLM, 2比特量化, PTQ, QAT", "comments": "该论文的创新点在于首次成功将2比特量化应用于指令微调的LLM，这在之前是一个空白。它通过结合块级PTQ和Distill-QAT的渐进式量化方法，有效解决了极低位量化带来的精度损失问题。无需专有数据即可实现SOTA性能，极大地降低了实际部署的门槛，对于在边缘设备上运行大型语言模型具有重要意义。"}}
{"id": "2506.09873", "title": "Stakeholder Participation for Responsible AI Development: Disconnects Between Guidance and Current Practice", "authors": ["Emma Kallina", "Thomas Bohné", "Jat Singh"], "summary": "Responsible AI (rAI) guidance increasingly promotes stakeholder involvement\n(SHI) during AI development. At the same time, SHI is already common in\ncommercial software development, but with potentially different foci. This\nstudy clarifies the extent to which established SHI practices are able to\ncontribute to rAI efforts as well as potential disconnects -- essential\ninsights to inform and tailor future interventions that further shift industry\npractice towards rAI efforts. First, we analysed 56 rAI guidance documents to\nidentify why SHI is recommended (i.e. its expected benefits for rAI) and\nuncovered goals such as redistributing power, improving socio-technical\nunderstandings, anticipating risks, and enhancing public oversight. To\nunderstand why and how SHI is currently practised in commercial settings, we\nthen conducted an online survey (n=130) and semi-structured interviews (n=10)\nwith AI practitioners. Our findings reveal that SHI in practice is primarily\ndriven by commercial priorities (e.g. customer value, compliance) and several\nfactors currently discourage more rAI-aligned SHI practices. This suggests that\nestablished SHI practices are largely not contributing to rAI efforts. To\naddress this disconnect, we propose interventions and research opportunities to\nadvance rAI development in practice.", "comment": "Published at the 2025 ACM Conference on Fairness, Accountability, and\n  Transparency FAccT'25", "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.09873v1", "AI": {"title_translation": "负责任人工智能开发中的利益相关者参与：指导与当前实践之间的脱节", "tldr": "负责任AI的指导意见提倡利益相关者参与，但实际操作中，利益相关者参与主要受商业驱动，与负责任AI的目标存在显著脱节。", "motivation": "旨在阐明既定的利益相关者参与实践在何种程度上能促进负责任AI的努力，以及可能存在的脱节，从而为未来干预措施提供信息，以推动行业实践转向负责任AI。", "method": "首先，分析了56份负责任AI指导文件，以确定利益相关者参与被推荐的原因（即其对负责任AI的预期益处）。其次，对AI从业者进行了在线调查（n=130）和半结构化访谈（n=10），以了解在商业环境中利益相关者参与的当前实践方式。", "result": "研究发现，实践中的利益相关者参与主要受商业优先级（如客户价值、合规性）驱动，并且有几个因素目前阻碍了更多与负责任AI相符的利益相关者参与实践。", "conclusion": "这表明既定的利益相关者参与实践在很大程度上并未促进负责任AI的努力。为解决这种脱节，本文提出了干预措施和研究机会，以促进负责任AI在实践中的发展。", "translation": "负责任人工智能（rAI）的指导方针日益提倡在AI开发过程中让利益相关者参与。同时，利益相关者参与在商业软件开发中已经很常见，但可能侧重点不同。本研究旨在阐明既定的利益相关者参与实践在何种程度上能促进负责任AI的努力，以及可能存在的脱节——这些都是为未来干预措施提供信息和指导的关键见解，以进一步推动行业实践转向负责任AI。首先，我们分析了56份负责任AI指导文件，以确定利益相关者参与被推荐的原因（即其对负责任AI的预期益处），并发现了诸如重新分配权力、改善社会技术理解、预测风险和加强公众监督等目标。为了了解在商业环境中利益相关者参与的当前实践方式和原因，我们随后对AI从业者进行了在线调查（n=130）和半结构化访谈（n=10）。我们的研究结果表明，实践中的利益相关者参与主要受商业优先级（例如客户价值、合规性）驱动，并且有几个因素目前阻碍了更多与负责任AI相符的利益相关者参与实践。这表明既定的利益相关者参与实践在很大程度上并未促进负责任AI的努力。为解决这种脱节，我们提出了干预措施和研究机会，以促进负责任AI在实践中的发展。", "summary": "本研究探讨了负责任AI指导方针中提倡的利益相关者参与与当前商业实践之间的脱节。通过分析负责任AI指导文件和对AI从业者的调查与访谈，研究发现，尽管指导方针强调利益相关者参与对负责任AI的重要性（如重新分配权力、改善理解、预测风险），但实际操作中，利益相关者参与主要由商业目标驱动，且存在阻碍其与负责任AI目标对齐的因素。这表明现有实践未能有效促进负责任AI。为弥补这一差距，本文提出了相应的干预措施和研究方向。", "keywords": "负责任AI, 利益相关者参与, AI开发, 指导方针, 实践", "comments": "这篇论文揭示了负责任AI领域一个关键的实践与理论脱节问题。其创新之处在于通过结合指导文件分析和实证调查，系统性地揭示了这种脱节的具体表现和原因。其重要性在于为未来制定更有效的负责任AI实施策略提供了扎实的证据基础，并提出了具体的干预方向，对推动行业实践具有指导意义。"}}
{"id": "2506.09054", "title": "Particle Builder -- Learn about the Standard Model while playing against an AI", "authors": ["Mohammad Attar", "Andrew Carse", "Yeming Chen", "Thomas Green", "Jeong-Yeon Ha", "Yanbai Jin", "Amy McWilliams", "Theirry Panggabean", "Zhengyu Peng", "Lujin Sun", "Jing Ru", "Jiacheng She", "Jialin Wang", "Zilun Wei", "Jiayuan Zhu", "Lachlan McGinness"], "summary": "Particle Builder Online is a web-based education game designed for high\nschool physics students. Students can play against an AI opponent or peers to\nfamiliarise themselves with the Standard Model of Particle Physics. The game is\naimed at a high school level and tailored to the International Baccalaureate\nand the Australian Curriculum. Students from four schools in Canberra took\npre/post-tests and a survey while completing a lesson where they played\nParticle Builder. Students' understanding of particle physics concepts improved\nsignificantly. Students found the game more enjoyable and effective than\nregular classroom lessons.", "comment": "This demo has been accepted for presentation at the AIED 2025\n  Interactive Events Track", "cate": "physics.ed-ph", "url": "http://arxiv.org/abs/2506.09054v1", "AI": {"title_translation": "粒子建造者——在与AI对战中学习标准模型", "tldr": "Particle Builder是一款针对高中生物理学生的在线教育游戏，旨在帮助他们通过与AI或同伴对战来熟悉粒子物理标准模型。实验证明，该游戏能显著提高学生对粒子物理概念的理解，并比传统课堂教学更受学生欢迎和有效。", "motivation": "该游戏旨在帮助高中物理学生以一种引人入胜的方式熟悉粒子物理标准模型，作为传统课堂教学的替代方案。", "method": "堪培拉四所学校的学生在完成包含粒子建造者游戏的课程时，进行了前测/后测和一项调查。", "result": "学生对粒子物理概念的理解显著提高。学生们发现该游戏比常规课堂教学更具趣味性和有效性。", "conclusion": "粒子建造者是一款有效且有趣的教育工具，适用于向高中生教授标准模型。", "translation": "Particle Builder Online是一款专为高中物理学生设计的网络教育游戏。学生可以与AI对手或同伴对战，以熟悉粒子物理标准模型。该游戏面向高中生，并根据国际文凭和澳大利亚课程进行了调整。来自堪培拉四所学校的学生在完成包含粒子建造者游戏的课程时，进行了前测/后测和一项调查。学生对粒子物理概念的理解显著提高。学生们发现该游戏比常规课堂教学更具趣味性和有效性。", "summary": "Particle Builder Online是一款为高中物理学生设计的基于网络的教育游戏，旨在通过与AI或同伴对战，帮助学生熟悉粒子物理标准模型。该游戏适用于国际文凭和澳大利亚课程，并在堪培拉的四所学校进行了测试。结果显示，学生对粒子物理概念的理解显著提高，并且他们认为该游戏比传统课堂教学更具趣味性和有效性。", "keywords": "粒子建造者, 标准模型, 教育游戏, 粒子物理, 高中物理", "comments": "本文提出了一种通过游戏化教授复杂物理概念的创新方法，展示了其在提高学生参与度和学习成果方面超越传统方法的潜力。"}}
{"id": "2506.09164", "title": "On Polynomial Stochastic Barrier Functions: Bernstein Versus Sum-of-Squares", "authors": ["Peter Amorese", "Morteza Lahijanian"], "summary": "Stochastic Barrier Functions (SBFs) certify the safety of stochastic systems\nby formulating a functional optimization problem, which state-of-the-art\nmethods solve using Sum-of-Squares (SoS) polynomials. This work focuses on\npolynomial SBFs and introduces a new formulation based on Bernstein polynomials\nand provides a comparative analysis of its theoretical and empirical\nperformance against SoS methods. We show that the Bernstein formulation leads\nto a linear program (LP), in contrast to the semi-definite program (SDP)\nrequired for SoS, and that its relaxations exhibit favorable theoretical\nconvergence properties. However, our empirical results reveal that the\nBernstein approach struggles to match SoS in practical performance, exposing an\nintriguing gap between theoretical advantages and real-world feasibility.", "comment": "To appear in IEEE Control Systems Letters (L-CSS) 2025", "cate": "math.OC", "url": "http://arxiv.org/abs/2506.09164v1", "AI": {"title_translation": "论多项式随机障碍函数：伯恩斯坦与平方和", "tldr": "论文引入了一种基于伯恩斯坦多项式的随机障碍函数新方法，与平方和方法相比，它在理论上具有优势（线性规划、收敛性），但在实际性能上表现不佳，揭示了理论与实践之间的差距。", "motivation": "随机障碍函数（SBFs）用于认证随机系统的安全性，当前主流方法使用平方和（SoS）多项式解决。本研究的动机是探索并引入一种新的多项式SBFs方法，即基于伯恩斯坦多项式的方法，并对其进行理论和实证分析，以期找到更优或具有不同特性的解决方案。", "method": "本文针对多项式随机障碍函数，引入了一种基于伯恩斯坦多项式的新公式。该方法将其表述为一个线性规划（LP）问题，并与需要半正定规划（SDP）的平方和（SoS）方法进行了理论和实证性能的比较分析。", "result": "伯恩斯坦公式导致线性规划（LP），而平方和（SoS）需要半正定规划（SDP）。伯恩斯坦方法的松弛表现出有利的理论收敛特性。实证结果显示，伯恩斯坦方法在实际性能上难以与平方和方法匹敌。", "conclusion": "伯恩斯坦方法在随机障碍函数认证方面存在理论优势（如线性规划、收敛性）与实际性能之间的显著差距，表明理论上的优越性不一定能转化为现实世界的实用性。", "translation": "随机障碍函数（SBFs）通过建立一个泛函优化问题来认证随机系统的安全性，现有最先进的方法使用平方和（SoS）多项式来解决这个问题。这项工作侧重于多项式SBFs，并引入了一种基于伯恩斯坦多项式的新公式，对其理论和经验性能与SoS方法进行了比较分析。我们表明，伯恩斯坦公式导致线性规划（LP），与SoS所需的半正定规划（SDP）形成对比，并且其松弛表现出有利的理论收敛特性。然而，我们的实证结果表明，伯恩斯坦方法在实际性能上难以与SoS匹敌，揭示了理论优势与现实可行性之间一个有趣的差距。", "summary": "本文研究多项式随机障碍函数（SBFs）的认证问题，提出了一种基于伯恩斯坦多项式的新方法，并与主流的平方和（SoS）方法进行对比。理论分析表明，伯恩斯坦公式可转化为线性规划（LP），且具有良好的收敛性，优于SoS所需的半正定规划（SDP）。然而，实证结果却发现伯恩斯坦方法在实际应用中表现不如SoS，凸显了理论优势与实际性能之间的差异。", "keywords": "随机障碍函数, 伯恩斯坦多项式, 平方和, 线性规划, 半正定规划", "comments": "本文的创新之处在于引入了伯恩斯坦多项式来构建随机障碍函数，并将其转化为线性规划问题，这在理论上比传统的平方和方法更具优势。然而，其重要性也体现在揭示了理论优势与实际性能之间可能存在的差距，这对于未来的研究具有指导意义，即在追求理论优化的同时，也需要充分考虑实际应用的效率和效果。"}}
{"id": "2506.09218", "title": "A Technique for Isolating Lexically-Independent Phonetic Dependencies in Generative CNNs", "authors": ["Bruno Ferenc Šegedin"], "summary": "The ability of deep neural networks (DNNs) to represent phonotactic\ngeneralizations derived from lexical learning remains an open question. This\nstudy (1) investigates the lexically-invariant generalization capacity of\ngenerative convolutional neural networks (CNNs) trained on raw audio waveforms\nof lexical items and (2) explores the consequences of shrinking the\nfully-connected layer (FC) bottleneck from 1024 channels to 8 before training.\nUltimately, a novel technique for probing a model's lexically-independent\ngeneralizations is proposed that works only under the narrow FC bottleneck:\ngenerating audio outputs by bypassing the FC and inputting randomized feature\nmaps into the convolutional block. These outputs are equally biased by a\nphonotactic restriction in training as are outputs generated with the FC. This\nresult shows that the convolutional layers can dynamically generalize phonetic\ndependencies beyond lexically-constrained configurations learned by the FC.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.09218v1", "AI": {"title_translation": "生成式CNN中隔离词汇独立语音依赖的技术", "tldr": "本文提出了一种新技术，用于探究生成式CNN中卷积层对词汇独立语音依赖的泛化能力，发现卷积层能动态泛化语音依赖。", "motivation": "深度神经网络（DNNs）表示源自词汇学习的音位概括能力仍是一个未解决的问题。", "method": "研究调查了在词汇项原始音频波形上训练的生成式CNN的词汇不变泛化能力，并探索了在训练前将全连接层（FC）瓶颈从1024通道缩小到8的影响。提出了一种新的探究模型词汇独立泛化能力的技术，该技术仅在窄FC瓶颈下有效：通过绕过FC并将随机特征图输入到卷积块来生成音频输出。", "result": "通过绕过FC生成的音频输出与通过FC生成的输出一样，同样受到训练中音位限制的偏置。", "conclusion": "卷积层可以动态地泛化超出FC学习到的词汇约束配置的语音依赖。", "translation": "深度神经网络（DNNs）表示源自词汇学习的音位概括能力仍然是一个悬而未决的问题。本研究（1）调查了在词汇项原始音频波形上训练的生成式卷积神经网络（CNNs）的词汇不变泛化能力，并（2）探讨了在训练前将全连接层（FC）瓶颈从1024通道缩小到8所带来的影响。最终，提出了一种探究模型词汇独立泛化能力的新技术，该技术仅在窄FC瓶颈下有效：通过绕过全连接层并向卷积块输入随机特征图来生成音频输出。这些输出与使用全连接层生成的输出一样，同样受到训练中音位限制的偏置。这一结果表明，卷积层能够动态地泛化超出全连接层学习到的词汇约束配置的语音依赖。", "summary": "本文研究了生成式CNNs的词汇不变泛化能力，并提出了一种新技术来探究其词汇独立语音依赖。通过缩小全连接层瓶颈并绕过该层输入随机特征图到卷积块，研究发现生成的音频输出仍受音位限制，表明卷积层能够动态地泛化超出词汇约束的语音依赖。", "keywords": "生成式CNNs, 语音依赖, 词汇独立, 音位泛化, 全连接层瓶颈", "comments": "本文提出了一种新颖的技术，通过操纵神经网络的瓶颈层来揭示其深层结构（卷积层）学习到的独立于词汇的语音泛化能力。这一发现对于理解生成式CNNs如何处理和泛化语音信息具有重要意义，尤其是在语音合成和识别领域。该方法通过直接测试卷积层的泛化能力，为解释深度学习模型内部机制提供了一个新的视角。"}}
{"id": "2506.09199", "title": "FLoRIST: Singular Value Thresholding for Efficient and Accurate Federated Fine-Tuning of Large Language Models", "authors": ["Hariharan Ramesh", "Jyotikrishna Dass"], "summary": "Integrating Low-Rank Adaptation (LoRA) into federated learning offers a\npromising solution for parameter-efficient fine-tuning of Large Language Models\n(LLMs) without sharing local data. However, several methods designed for\nfederated LoRA present significant challenges in balancing communication\nefficiency, model accuracy, and computational cost, particularly among\nheterogeneous clients. These methods either rely on simplistic averaging of\nlocal adapters, which introduces aggregation noise, require transmitting large\nstacked local adapters, leading to poor communication efficiency, or\nnecessitate reconstructing memory-dense global weight-update matrix and\nperforming computationally expensive decomposition to design client-specific\nlow-rank adapters. In this work, we propose FLoRIST, a federated fine-tuning\nframework that achieves mathematically accurate aggregation without incurring\nhigh communication or computational overhead. Instead of constructing the full\nglobal weight-update matrix at the server, FLoRIST employs an efficient\ndecomposition pipeline by performing singular value decomposition on stacked\nlocal adapters separately. This approach operates within a compact intermediate\nspace to represent the accumulated information from local LoRAs. We introduce\ntunable singular value thresholding for server-side optimal rank selection to\nconstruct a pair of global low-rank adapters shared by all clients. Extensive\nempirical evaluations across multiple datasets and LLMs demonstrate that\nFLoRIST consistently strikes the best balance between superior communication\nefficiency and competitive performance in both homogeneous and heterogeneous\nsetups.", "comment": "21 pages, 12 figures", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09199v1", "AI": {"title_translation": "FLoRIST：用于大型语言模型高效准确联邦微调的奇异值阈值法", "tldr": "FLoRIST 是一种新的联邦微调框架，通过在堆叠的本地适配器上进行奇异值分解和可调奇异值阈值处理，实现了高效准确的联邦LoRA聚合，解决了现有方法在通信效率、模型精度和计算成本之间的平衡问题。", "motivation": "现有的联邦LoRA方法在平衡通信效率、模型精度和计算成本方面存在显著挑战，尤其是在异构客户端环境下。这些方法要么引入聚合噪声，要么通信效率低下，要么计算成本高昂。", "method": "本文提出了FLoRIST框架，通过对堆叠的本地适配器独立执行奇异值分解，并在紧凑的中间空间中操作，避免在服务器端构建完整的全局权重更新矩阵。此外，该方法引入可调奇异值阈值法进行服务器端最优秩选择，以构建共享的全局低秩适配器。", "result": "在多个数据集和大型语言模型上的广泛实证评估表明，FLoRIST在同构和异构设置下，始终在卓越的通信效率和具有竞争力的性能之间取得最佳平衡。", "conclusion": "FLoRIST 提供了一种在联邦学习环境中对大型语言模型进行参数高效微调的有效解决方案，它在不牺牲精度的情况下显著提高了通信效率并降低了计算成本。", "translation": "将低秩适应（LoRA）集成到联邦学习中，为在不共享本地数据的情况下对大型语言模型（LLMs）进行参数高效微调提供了一种有前途的解决方案。然而，为联邦LoRA设计的几种方法在平衡通信效率、模型精度和计算成本方面提出了显著挑战，尤其是在异构客户端之间。这些方法要么依赖于本地适配器的简单平均，这会引入聚合噪声；要么需要传输大量堆叠的本地适配器，导致通信效率低下；要么需要重建内存密集型的全局权重更新矩阵并执行计算成本高昂的分解来设计客户端特定的低秩适配器。在这项工作中，我们提出了FLoRIST，一个联邦微调框架，它在不产生高通信或计算开销的情况下实现了数学上准确的聚合。FLoRIST不构建服务器上的完整全局权重更新矩阵，而是通过分别对堆叠的本地适配器执行奇异值分解来采用高效的分解管道。这种方法在紧凑的中间空间中操作，以表示来自本地LoRA的累积信息。我们引入了可调奇异值阈值法，用于服务器端的最优秩选择，以构建一对所有客户端共享的全局低秩适配器。在多个数据集和LLMs上的广泛实证评估表明，FLoRIST在同构和异构设置中，始终在卓越的通信效率和具有竞争力的性能之间取得最佳平衡。", "summary": "本文提出了FLoRIST，一个针对大型语言模型联邦微调的新框架，旨在解决现有联邦LoRA方法在通信效率、模型精度和计算成本方面的挑战。FLoRIST通过对堆叠的本地LoRA适配器独立执行奇异值分解，并在紧凑的中间空间中进行聚合，避免了构建完整的全局权重更新矩阵。此外，它引入了可调奇异值阈值法进行最优秩选择。实验证明，FLoRIST在同构和异构环境中均能实现卓越的通信效率和具有竞争力的性能。", "keywords": "联邦学习, LoRA, 大型语言模型, 奇异值分解, 通信效率", "comments": "FLoRIST的创新之处在于其独特的奇异值分解聚合管道和可调奇异值阈值技术，这使其能够在联邦学习中实现LoRA的准确高效聚合，有效解决了现有方法的痛点。该工作对于在资源受限和数据隐私敏感的联邦环境中部署大型语言模型具有重要意义。"}}
{"id": "2506.09650", "title": "HopaDIFF: Holistic-Partial Aware Fourier Conditioned Diffusion for Referring Human Action Segmentation in Multi-Person Scenarios", "authors": ["Kunyu Peng", "Junchao Huang", "Xiangsheng Huang", "Di Wen", "Junwei Zheng", "Yufan Chen", "Kailun Yang", "Jiamin Wu", "Chongqing Hao", "Rainer Stiefelhagen"], "summary": "Action segmentation is a core challenge in high-level video understanding,\naiming to partition untrimmed videos into segments and assign each a label from\na predefined action set. Existing methods primarily address single-person\nactivities with fixed action sequences, overlooking multi-person scenarios. In\nthis work, we pioneer textual reference-guided human action segmentation in\nmulti-person settings, where a textual description specifies the target person\nfor segmentation. We introduce the first dataset for Referring Human Action\nSegmentation, i.e., RHAS133, built from 133 movies and annotated with 137\nfine-grained actions with 33h video data, together with textual descriptions\nfor this new task. Benchmarking existing action recognition methods on RHAS133\nusing VLM-based feature extractors reveals limited performance and poor\naggregation of visual cues for the target person. To address this, we propose a\nholistic-partial aware Fourier-conditioned diffusion framework, i.e., HopaDIFF,\nleveraging a novel cross-input gate attentional xLSTM to enhance\nholistic-partial long-range reasoning and a novel Fourier condition to\nintroduce more fine-grained control to improve the action segmentation\ngeneration. HopaDIFF achieves state-of-the-art results on RHAS133 in diverse\nevaluation settings. The code is available at\nhttps://github.com/KPeng9510/HopaDIFF.git.", "comment": "The code is available at https://github.com/KPeng9510/HopaDIFF.git", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09650v1", "AI": {"title_translation": "HopaDIFF：基于整体-局部感知傅里叶条件扩散的多人场景指代人体动作分割", "tldr": "引入了HopaDIFF，一种新颖的扩散框架，以及RHAS133数据集，用于多人视频中的文本引导人体动作分割，性能优于现有方法。", "motivation": "现有动作分割方法主要关注单人活动和固定动作序列，忽视了多人场景以及通过文本描述指定目标人物的需求。", "method": "提出HopaDIFF，一种整体-局部感知傅里叶条件扩散框架。它利用新颖的跨输入门控注意力xLSTM来增强整体-局部长程推理，并利用新颖的傅里叶条件引入更细粒度的控制，以改进动作分割生成。同时，还推出了RHAS133，第一个用于指代人体动作分割的数据集。", "result": "HopaDIFF在RHAS133的各种评估设置中取得了最先进的结果。现有动作识别方法在RHAS133上的性能有限，对目标人物的视觉线索聚合能力差。", "conclusion": "HopaDIFF有效地解决了多人场景中指代人体动作分割的挑战，并树立了新的基准。", "translation": "动作分割是高级视频理解中的一个核心挑战，旨在将未剪辑的视频分割成片段并为每个片段分配一个预定义动作集中的标签。现有方法主要解决单人活动与固定动作序列的问题，忽略了多人场景。在这项工作中，我们开创了在多人设置中基于文本参考的人体动作分割，其中文本描述指定了要分割的目标人物。我们推出了第一个用于指代人体动作分割的数据集，即RHAS133，该数据集从133部电影中构建，并标注了137个细粒度动作，包含33小时的视频数据，以及针对这项新任务的文本描述。使用基于VLM的特征提取器在RHAS133上对现有动作识别方法进行基准测试，结果显示其性能有限，并且对目标人物的视觉线索聚合能力差。为了解决这个问题，我们提出了一种整体-局部感知傅里叶条件扩散框架，即HopaDIFF，它利用一种新颖的跨输入门控注意力xLSTM来增强整体-局部长程推理，并利用一种新颖的傅里叶条件来引入更细粒度的控制，以改进动作分割生成。HopaDIFF在RHAS133的各种评估设置中取得了最先进的结果。代码可在https://github.com/KPeng9510/HopaDIFF.git获取。", "summary": "该论文引入了HopaDIFF，一种新颖的整体-局部感知傅里叶条件扩散框架，用于多人场景中基于文本参考的人体动作分割这一新任务。它还提出了RHAS133，第一个用于该任务的数据集，该数据集从133部电影中构建，包含33小时的视频数据和细粒度的动作标注。HopaDIFF利用跨输入门控注意力xLSTM和傅里叶条件在RHAS133上取得了最先进的结果，解决了现有方法在多人设置中的局限性。", "keywords": "人体动作分割, 多人场景, 扩散模型, 指代表达, 视频理解", "comments": "该论文在多人场景中开创性地提出了文本参考引导的人体动作分割，这是一个以前被忽视的领域，具有创新性。RHAS133数据集的引入非常重要，它为这项新颖且具有挑战性的任务提供了急需的基准。HopaDIFF新颖的架构，结合了整体-局部感知和傅里叶条件，展示出强大的性能，为复杂的视频理解指明了有前景的方向。"}}
{"id": "2506.09771", "title": "Where Journalism Silenced Voices: Exploring Discrimination in the Representation of Indigenous Communities in Bangladesh", "authors": ["Abhijit Paul", "Adity Khisa", "Zarif Masud", "Sharif Md. Abdullah", "Ahmedul Kabir", "Shebuti Rayana"], "summary": "In this paper, we examine the intersections of indigeneity and media\nrepresentation in shaping perceptions of indigenous communities in Bangladesh.\nUsing a mixed-methods approach, we combine quantitative analysis of media data\nwith qualitative insights from focus group discussions (FGD). First, we\nidentify a total of 4,893 indigenous-related articles from our initial dataset\nof 2.2 million newspaper articles, using a combination of keyword-based\nfiltering and LLM, achieving 77% accuracy and an F1-score of 81.9\\%. From\nmanually inspecting 3 prominent Bangla newspapers, we identify 15 genres that\nwe use as our topics for semi-supervised topic modeling using CorEx. Results\nshow indigenous news articles have higher representation of culture and\nentertainment (19%, 10% higher than general news articles), and a\ndisproportionate focus on conflict and protest (9%, 7% higher than general\nnews). On the other hand, sentiment analysis reveals that 57% of articles on\nindigenous topics carry a negative tone, compared to 27% for non-indigenous\nrelated news. Drawing from communication studies, we further analyze framing,\npriming, and agenda-setting (frequency of themes) to support the case for\ndiscrimination in representation of indigenous news coverage. For the\nqualitative part of our analysis, we facilitated FGD, where participants\nfurther validated these findings. Participants unanimously expressed their\nfeeling of being under-represented, and that critical issues affecting their\ncommunities (such as education, healthcare, and land rights) are systematically\nmarginalized in news media coverage. By highlighting 8 cases of discrimination\nand media misrepresentation that were frequently mentioned by participants in\nthe FGD, this study emphasizes the urgent need for more equitable media\npractices that accurately reflect the experiences and struggles of marginalized\ncommunities.", "comment": null, "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.09771v1", "AI": {"title_translation": "新闻业何以让声音沉寂：探究孟加拉国原住民社区代表中的歧视", "tldr": "孟加拉国的新闻业通过错误呈现、过度关注负面/琐碎话题并忽视关键问题，歧视原住民社区。本研究通过对新闻文章和焦点小组的混合方法分析证实了这一点。", "motivation": "本文旨在探讨原住民身份与媒体表征之间的交叉点，以及它们如何塑造孟加拉国原住民社区的认知，并探究新闻报道中对原住民社区代表的歧视。", "method": "本研究采用混合方法。定量分析包括从220万篇报纸文章中识别出4,893篇与原住民相关的文章，并使用关键词过滤和LLM进行筛选（准确率77%，F1分数81.9%）。通过对3家孟加拉语报纸的手动检查，确定了15种新闻类型，并使用CorEx进行半监督主题建模。此外，还进行了情感分析，并分析了框架、启动和议程设置。定性部分则通过焦点小组讨论（FGD）进行。", "result": "定量结果显示，原住民新闻文章中文化和娱乐的代表性更高（19%，比一般新闻高10%），且过度关注冲突和抗议（9%，比一般新闻高7%）。情感分析表明，57%的原住民主题文章带有负面基调，而非原住民相关新闻仅为27%。焦点小组讨论验证了这些发现，参与者普遍感到被低估，并认为影响其社区的关键问题（如教育、医疗保健和土地权利）在新闻媒体报道中被系统性边缘化。研究还突出了8个常见的歧视和媒体错误表征案例。", "conclusion": "本研究强调，孟加拉国新闻业在原住民社区的代表中存在歧视，迫切需要更公平的媒体实践，以准确反映边缘化社区的经历和 struggles。", "translation": "在本文中，我们探讨了原住民身份和媒体表征在塑造孟加拉国原住民社区认知方面的交叉点。我们采用混合方法，结合了媒体数据的定量分析和焦点小组讨论（FGD）的定性见解。首先，我们使用关键词过滤和LLM的组合，从220万篇报纸文章的初始数据集中识别出4,893篇与原住民相关的文章，准确率达到77%，F1分数达到81.9%。通过手动检查3家著名的孟加拉语报纸，我们识别出15种类型，将其作为使用CorEx进行半监督主题建模的主题。结果显示，原住民新闻文章中文化和娱乐的代表性更高（19%，比一般新闻文章高10%），并且过度关注冲突和抗议（9%，比一般新闻文章高7%）。另一方面，情感分析显示，57%的原住民主题文章带有负面基调，而与非原住民相关的新闻文章只有27%。借鉴传播学，我们进一步分析了框架、启动和议程设置（主题频率），以支持原住民新闻报道中存在歧视的论点。在定性分析部分，我们组织了焦点小组讨论，参与者进一步验证了这些发现。参与者一致表示他们感到被低估，并且影响他们社区的关键问题（如教育、医疗保健和土地权利）在新闻媒体报道中被系统性边缘化。通过突出焦点小组讨论中参与者频繁提及的8个歧视和媒体错误表征案例，本研究强调迫切需要更公平的媒体实践，以准确反映边缘化社区的经历和 struggles。", "summary": "本文采用混合方法，调查了孟加拉国媒体对原住民社区代表中的歧视。通过对4,893篇原住民相关新闻文章的定量分析，发现新闻内容过度关注文化/娱乐和冲突/抗议，并呈现出负面情绪。定性焦点小组讨论进一步证实了这些发现，参与者报告称其社区被低估，关键问题被边缘化。研究强调了媒体需要采取更公平的实践，以准确反映边缘化社区的经历。", "keywords": "原住民社区, 媒体歧视, 孟加拉国, 新闻表征, 混合方法", "comments": "本研究系统性地调查了媒体对原住民社区的歧视，具有重要的社会意义。其混合方法，结合了大规模定量分析和受影响社区的定性验证，增强了研究结果的说服力。利用LLM进行初步筛选是AI在媒体分析中创新应用的一个体现。研究结果提供了关于错误表征的具体证据，强调了媒体改革的必要性。"}}
{"id": "2506.09645", "title": "Learning Efficient and Generalizable Graph Retriever for Knowledge-Graph Question Answering", "authors": ["Tianjun Yao", "Haoxuan Li", "Zhiqiang Shen", "Pan Li", "Tongliang Liu", "Kun Zhang"], "summary": "Large Language Models (LLMs) have shown strong inductive reasoning ability\nacross various domains, but their reliability is hindered by the outdated\nknowledge and hallucinations. Retrieval-Augmented Generation mitigates these\nissues by grounding LLMs with external knowledge; however, most existing RAG\npipelines rely on unstructured text, limiting interpretability and structured\nreasoning. Knowledge graphs, which represent facts as relational triples, offer\na more structured and compact alternative. Recent studies have explored\nintegrating knowledge graphs with LLMs for knowledge graph question answering\n(KGQA), with a significant proportion adopting the retrieve-then-reasoning\nparadigm. In this framework, graph-based retrievers have demonstrated strong\nempirical performance, yet they still face challenges in generalization\nability. In this work, we propose RAPL, a novel framework for efficient and\neffective graph retrieval in KGQA. RAPL addresses these limitations through\nthree aspects: (1) a two-stage labeling strategy that combines heuristic\nsignals with parametric models to provide causally grounded supervision; (2) a\nmodel-agnostic graph transformation approach to capture both intra- and\ninter-triple interactions, thereby enhancing representational capacity; and (3)\na path-based reasoning strategy that facilitates learning from the injected\nrational knowledge, and supports downstream reasoner through structured inputs.\nEmpirically, RAPL outperforms state-of-the-art methods by $2.66\\%-20.34\\%$, and\nsignificantly reduces the performance gap between smaller and more powerful\nLLM-based reasoners, as well as the gap under cross-dataset settings,\nhighlighting its superior retrieval capability and generalizability. Codes are\navailable at: https://github.com/tianyao-aka/RAPL.", "comment": "32 pages, 28 figures", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.09645v1", "AI": {"title_translation": "学习高效且可泛化的图检索器用于知识图谱问答", "tldr": "本文提出了RAPL，一个高效且可泛化的图检索框架，通过两阶段标注、模型无关图转换和基于路径的推理策略，显著提升了知识图谱问答中的检索性能和泛化能力。", "motivation": "大型语言模型(LLMs)存在知识过时和幻觉问题，现有检索增强生成(RAG)方法依赖非结构化文本，限制了解释性和结构化推理。知识图谱提供了结构化替代方案，但现有图检索器在泛化能力上仍面临挑战。", "method": "本文提出了RAPL框架，通过三个方面解决现有问题：1) 两阶段标注策略，结合启发式信号和参数模型提供因果基础监督；2) 模型无关的图转换方法，捕获三元组内部和之间交互以增强表示能力；3) 基于路径的推理策略，促进从注入的理性知识中学习并支持下游推理器。", "result": "RAPL在经验上优于现有SOTA方法2.66%-20.34%，显著缩小了小型与强大LLM推理器之间的性能差距，以及跨数据集设置下的差距，突出了其卓越的检索能力和泛化能力。", "conclusion": "RAPL通过其创新的两阶段标注、图转换和路径推理策略，显著提高了知识图谱问答中图检索的效率、有效性和泛化能力，为LLM与知识图谱的结合提供了更可靠的解决方案。", "translation": "大型语言模型（LLMs）在各种领域展示出强大的归纳推理能力，但其可靠性受到过时知识和幻觉的阻碍。检索增强生成（RAG）通过将LLMs与外部知识结合来缓解这些问题；然而，大多数现有RAG流程依赖于非结构化文本，限制了解释性和结构化推理。知识图谱以关系三元组的形式表示事实，提供了一种更结构化和紧凑的替代方案。最近的研究探索了将知识图谱与LLMs集成用于知识图谱问答（KGQA），其中很大一部分采用了检索-然后-推理范式。在此框架中，基于图的检索器已显示出强大的经验性能，但它们在泛化能力上仍面临挑战。在这项工作中，我们提出了RAPL，一个用于KGQA中高效且有效的图检索的新框架。RAPL通过三个方面解决了这些限制：（1）一种两阶段标注策略，结合启发式信号和参数模型以提供因果基础监督；（2）一种模型无关的图转换方法，以捕获三元组内部和之间的交互，从而增强表示能力；以及（3）一种基于路径的推理策略，促进从注入的理性知识中学习，并通过结构化输入支持下游推理器。从经验上看，RAPL的性能优于现有最先进的方法2.66%-20.34%，并显著缩小了较小和更强大基于LLM的推理器之间的性能差距，以及跨数据集设置下的差距，突出了其卓越的检索能力和泛化能力。代码可在以下网址获取：https://github.com/tianyao-aka/RAPL。", "summary": "本文提出了一种名为RAPL的新型框架，旨在解决知识图谱问答（KGQA）中图检索器在效率和泛化能力方面的挑战。RAPL通过引入两阶段标注策略、模型无关的图转换方法以及基于路径的推理策略来增强检索性能。实验结果表明，RAPL在多个方面显著优于现有最先进的方法，并在跨数据集设置下表现出更强的泛化能力，有效提升了LLM在KGQA任务中的可靠性。", "keywords": "知识图谱问答, 图检索, 泛化能力, 大型语言模型, 检索增强生成", "comments": "RAPL的创新之处在于其结合了启发式信号和参数模型的两阶段标注策略，以及模型无关的图转换方法，有效提升了图检索的监督信号质量和表示能力。其强调泛化能力和对不同规模LLM的性能提升，使其在知识图谱问答领域具有重要意义。该工作为将结构化知识图谱与LLM结合提供了有效途径，并有望缓解LLM的知识过时和幻觉问题。"}}
{"id": "2506.09584", "title": "Extensive Database of Spatial Ballistic Captures with Application to Lunar Trailblazer", "authors": ["Lorenzo Anoè", "Roberto Armellin", "Gregory Lantoine", "Claudio Bombardelli"], "summary": "For low-energy missions to the Moon and beyond, Ballistic Capture has proven\nto be a valuable technique for enabling orbital insertion while alleviating\npropulsion system requirements. This approach offers two key advantages. First,\nit extends the insertion window, allowing multiple maneuver opportunities to\nmitigate potential failures at the nominal insertion point. Second, it enables\nthe required insertion maneuver to be distributed across multiple revolutions,\nreducing propulsion system constraints in terms of single-burn thrust. Prior\nresearch introduced the concept of Energy Transition Domain to support the\ncreation of a comprehensive database of Ballistic Captures in the planar\nCircular Restricted Three-Body Problem. However, to apply these trajectories to\na real mission scenario, a three-dimensional, spatial analysis and transition\nto an ephemeris model are necessary. This paper first extends the Energy\nTransition Domain framework to the spatial case, constructing an extensive\ndatabase of spatial Ballistic Captures. Then, using Lunar Trailblazer as a case\nstudy, a subset of the trajectories is filtered using a mission-specific\ndistance metric, and transitioned into an ephemeris model. Finally, interesting\nfeatures of this subset are analyzed, and sample high-fidelity trajectories are\nselected as potential backup options for Lunar Trailblazer.", "comment": null, "cate": "math.NA", "url": "http://arxiv.org/abs/2506.09584v1", "AI": {"title_translation": "空间弹道捕获的广泛数据库及其在“月球开拓者”任务中的应用", "tldr": "本文扩展了能量转换域框架以构建空间弹道捕获数据库，并将其应用于“月球开拓者”任务，筛选并分析了高保真轨迹作为潜在备份方案。", "motivation": "低能量月球任务中，弹道捕获是一种有价值的技术，可减轻推进系统要求、延长插入窗口并分散机动。然而，现有研究的能量转换域概念仅限于平面圆形限制性三体问题，缺乏三维空间分析和星历模型转换，这对于实际任务应用是必需的。", "method": "本文首先将能量转换域框架扩展到三维空间，构建了一个广泛的空间弹道捕获数据库。然后，以“月球开拓者”任务为例，使用任务特定的距离度量筛选出一部分轨迹，并将其转换为星历模型。最后，分析了这部分轨迹的有趣特征，并选择样本高保真轨迹作为“月球开拓者”的潜在备份选项。", "result": "成功构建了广泛的空间弹道捕获数据库。为“月球开拓者”任务筛选并分析了在星历模型中的高保真轨迹子集，并识别出可作为潜在备份选项的轨迹。", "conclusion": "通过扩展能量转换域框架并构建空间弹道捕获数据库，本研究为实际月球任务（如“月球开拓者”）提供了宝贵的低能量轨迹选项和潜在的备份方案，验证了其在复杂三维环境下的应用潜力。", "translation": "对于前往月球及更远区域的低能量任务而言，弹道捕获已被证明是一种有价值的技术，它能够在减轻推进系统要求的同时实现轨道插入。这种方法提供了两个关键优势。首先，它延长了插入窗口，允许多次机动机会以减轻名义插入点可能发生的故障。其次，它使得所需的插入机动能够分布在多个公转周期中完成，从而降低了单次点火推力方面的推进系统限制。先前的研究引入了能量转换域的概念，以支持在平面圆形限制性三体问题中创建全面的弹道捕获数据库。然而，为了将这些轨迹应用于真实任务场景，需要进行三维空间分析并转换为星历模型。本文首先将能量转换域框架扩展到空间情况，构建了一个广泛的空间弹道捕获数据库。然后，以“月球开拓者”任务为例，使用任务特定的距离度量筛选出一部分轨迹，并将其转换为星历模型。最后，分析了这部分轨迹的有趣特征，并选择样本高保真轨迹作为“月球开拓者”的潜在备份选项。", "summary": "本文扩展了能量转换域框架以创建三维空间弹道捕获数据库，并以“月球开拓者”任务为例，筛选并分析了适用于真实星历模型的高保真轨迹。研究旨在为低能量月球任务提供减轻推进系统限制的替代轨道插入方案，并为实际任务提供潜在的备份轨迹。", "keywords": "弹道捕获, 月球开拓者, 空间轨迹, 能量转换域, 低能量任务", "comments": "本文的创新之处在于将能量转换域框架从平面扩展到三维空间，并成功构建了实用的空间弹道捕获数据库。其重要性体现在为实际深空任务（如“月球开拓者”）提供了更灵活、更节省燃料的轨道插入方案，尤其是在应对潜在故障和系统限制方面具有重大应用价值。"}}
{"id": "2506.09807", "title": "Physical Layer-Based Device Fingerprinting for Wireless Security: From Theory to Practice", "authors": ["Junqing Zhang", "Francesco Ardizzon", "Mattia Piana", "Guanxiong Shen", "Stefano Tomasin"], "summary": "The identification of the devices from which a message is received is part of\nsecurity mechanisms to ensure authentication in wireless communications.\nConventional authentication approaches are cryptography-based, which, however,\nare usually computationally expensive and not adequate in the Internet of\nThings (IoT), where devices tend to be low-cost and with limited resources.\nThis paper provides a comprehensive survey of physical layer-based device\nfingerprinting, which is an emerging device authentication for wireless\nsecurity. In particular, this article focuses on hardware impairment-based\nidentity authentication and channel features-based authentication. They are\npassive techniques that are readily applicable to legacy IoT devices. Their\nintrinsic hardware and channel features, algorithm design methodologies,\napplication scenarios, and key research questions are extensively reviewed\nhere. The remaining research challenges are discussed, and future work is\nsuggested that can further enhance the physical layer-based device\nfingerprinting.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.09807v1", "AI": {"title_translation": "物理层设备指纹识别用于无线安全：从理论到实践", "tldr": "本文全面综述了基于物理层的设备指纹识别技术，作为一种新兴的无线安全认证方法，特别关注基于硬件损伤和信道特征的认证，并讨论了挑战和未来工作。", "motivation": "传统的基于密码学的无线通信认证方法计算成本高昂，不适用于资源受限的物联网（IoT）设备。因此，需要一种更高效、适用于低成本设备的认证机制。", "method": "本文对基于物理层的设备指纹识别技术进行了全面综述，重点介绍了基于硬件损伤的身份认证和基于信道特征的认证方法。文章详细回顾了其固有的硬件和信道特征、算法设计方法、应用场景和关键研究问题。", "result": "综述了基于硬件损伤和信道特征的物理层设备指纹识别技术，这些技术是被动且适用于传统物联网设备的。文章详细审查了其固有的硬件和信道特征、算法设计方法、应用场景和关键研究问题。", "conclusion": "物理层设备指纹识别是一种有前景的无线安全认证方法，尤其适用于物联网设备。文章讨论了当前的研究挑战并提出了未来的研究方向，以进一步增强该技术。", "translation": "消息接收设备的识别是无线通信中确保认证的安全机制的一部分。传统的认证方法基于密码学，然而，它们通常计算成本高昂，不适用于物联网（IoT），因为物联网设备往往是低成本且资源有限的。本文全面综述了基于物理层的设备指纹识别，这是一种新兴的无线安全设备认证方法。特别是，本文重点关注基于硬件损伤的身份认证和基于信道特征的认证。它们是被动技术，易于应用于传统物联网设备。本文广泛回顾了它们的内在硬件和信道特征、算法设计方法、应用场景和关键研究问题。讨论了剩余的研究挑战，并提出了可以进一步增强基于物理层设备指纹识别的未来工作。", "summary": "本文对物理层设备指纹识别技术进行了全面综述，该技术作为一种新兴的无线安全认证方法，旨在解决传统加密认证在资源受限物联网设备中的局限性。文章重点关注基于硬件损伤和信道特征的被动认证技术，详细回顾了其原理、设计、应用及研究挑战，并展望了未来发展方向。", "keywords": "物理层安全, 设备指纹识别, 无线安全, 物联网, 认证", "comments": "这篇综述论文对于无线安全领域的物理层认证方法提供了全面的概述，特别是强调了其在物联网环境中的适用性。其创新点在于将硬件固有特征和信道特性作为认证依据，避免了传统加密方法的计算开销。论文对现有技术进行了系统梳理，并指出了未来的研究方向，对该领域的研究人员具有重要的指导意义。"}}
{"id": "2506.09494", "title": "Advances on Affordable Hardware Platforms for Human Demonstration Acquisition in Agricultural Applications", "authors": ["Alberto San-Miguel-Tello", "Gennaro Scarati", "Alejandro Hernández", "Mario Cavero-Vidal", "Aakash Maroti", "Néstor García"], "summary": "This paper presents advances on the Universal Manipulation Interface (UMI), a\nlow-cost hand-held gripper for robot Learning from Demonstration (LfD), for\ncomplex in-the-wild scenarios found in agricultural settings. The focus is on\nimproving the acquisition of suitable samples with minimal additional setup.\nFirstly, idle times and user's cognitive load are reduced through the\nextraction of individual samples from a continuous demonstration considering\ntask events. Secondly, reliability on the generation of task sample's\ntrajectories is increased through the combination on-board inertial\nmeasurements and external visual marker localization usage using Extended\nKalman Filtering (EKF). Results are presented for a fruit harvesting task,\noutperforming the default pipeline.", "comment": "7 pages, 2 figures", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.09494v1", "AI": {"title_translation": "农业应用中人类示教获取的经济型硬件平台进展", "tldr": "本文介绍了UMI手持夹具在农业LfD中数据采集的改进，通过事件提取和EKF结合惯性/视觉数据提高了效率和可靠性，并在水果采摘任务中表现优异。", "motivation": "解决农业场景中机器人示教学习（LfD）数据采集的挑战，特别是如何通过低成本硬件平台，以最小的额外设置，提高适合样本的获取效率和可靠性。", "method": "1. 通过考虑任务事件，从连续示教中提取独立样本，减少空闲时间和用户认知负荷。2. 结合板载惯性测量和外部视觉标记定位，使用扩展卡尔曼滤波（EKF）提高任务样本轨迹生成的可靠性。", "result": "在水果采摘任务中展示了结果，性能优于默认流程。", "conclusion": "Not mentioned in abstract", "translation": "本文介绍了通用操作接口（UMI）的进展，这是一种用于机器人示教学习（LfD）的低成本手持夹具，适用于农业环境中复杂的野外场景。重点是通过最少的额外设置来改进合适样本的获取。首先，通过考虑任务事件从连续示教中提取独立样本，减少了空闲时间和用户的认知负荷。其次，通过结合板载惯性测量和使用扩展卡尔曼滤波（EKF）的外部视觉标记定位，提高了任务样本轨迹生成的可靠性。在水果采摘任务中展示了结果，其性能优于默认流程。", "summary": "本文研究了通用操作接口（UMI）在农业机器人示教学习（LfD）中的应用，旨在提高复杂野外场景下数据采集的效率和可靠性。通过优化样本提取流程，减少用户负担，并结合惯性测量与视觉定位（采用EKF）来提高轨迹生成的准确性，该方法在水果采摘任务中取得了超越传统方法的表现。", "keywords": "农业机器人, 示教学习, 低成本硬件, 数据采集, 扩展卡尔曼滤波", "comments": "这项工作通过改进低成本硬件平台UMI的数据采集方法，为农业机器人学习提供了实用且高效的解决方案。其创新点在于结合任务事件进行样本提取以降低用户负担，并融合多传感器数据（惯性与视觉）提升轨迹精度，这对于推动LfD在实际农业场景中的应用具有重要意义。"}}
{"id": "2506.09278", "title": "UFM: A Simple Path towards Unified Dense Correspondence with Flow", "authors": ["Yuchen Zhang", "Nikhil Keetha", "Chenwei Lyu", "Bhuvan Jhamb", "Yutian Chen", "Yuheng Qiu", "Jay Karhade", "Shreyas Jha", "Yaoyu Hu", "Deva Ramanan", "Sebastian Scherer", "Wenshan Wang"], "summary": "Dense image correspondence is central to many applications, such as visual\nodometry, 3D reconstruction, object association, and re-identification.\nHistorically, dense correspondence has been tackled separately for\nwide-baseline scenarios and optical flow estimation, despite the common goal of\nmatching content between two images. In this paper, we develop a Unified Flow &\nMatching model (UFM), which is trained on unified data for pixels that are\nco-visible in both source and target images. UFM uses a simple, generic\ntransformer architecture that directly regresses the (u,v) flow. It is easier\nto train and more accurate for large flows compared to the typical\ncoarse-to-fine cost volumes in prior work. UFM is 28% more accurate than\nstate-of-the-art flow methods (Unimatch), while also having 62% less error and\n6.7x faster than dense wide-baseline matchers (RoMa). UFM is the first to\ndemonstrate that unified training can outperform specialized approaches across\nboth domains. This result enables fast, general-purpose correspondence and\nopens new directions for multi-modal, long-range, and real-time correspondence\ntasks.", "comment": "Project Page: https://uniflowmatch.github.io/", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09278v1", "AI": {"title_translation": "UFM：一种实现统一稠密对应与光流的简单路径", "tldr": "UFM是一种统一的光流和匹配模型，使用简单的Transformer架构，在稠密对应任务上比现有方法更准确、更快，证明了统一训练的优越性。", "motivation": "稠密图像对应在许多应用中至关重要，但宽基线场景和光流估计通常被单独处理，尽管它们有共同的目标。", "method": "开发了一种统一的光流与匹配模型（UFM），在源图像和目标图像中共同可见的像素上进行统一数据训练。UFM使用简单的通用Transformer架构，直接回归(u,v)流。", "result": "UFM比最先进的光流方法（Unimatch）准确度高28%，同时比稠密宽基线匹配器（RoMa）错误率低62%且速度快6.7倍。UFM首次证明统一训练在两个领域都能超越专门方法。", "conclusion": "统一训练可以超越专门方法，实现快速、通用的对应，并为多模态、长距离和实时对应任务开辟新方向。", "translation": "稠密图像对应是许多应用的核心，例如视觉里程计、3D重建、目标关联和重识别。历史上，尽管在两幅图像之间匹配内容的目标相同，但稠密对应在宽基线场景和光流估计方面一直被单独处理。在本文中，我们开发了一种统一的光流与匹配模型（UFM），它针对源图像和目标图像中共同可见的像素在统一数据上进行训练。UFM采用一种简单通用的Transformer架构，直接回归(u,v)流。与以往工作中典型的从粗到精的成本体相比，它更容易训练，并且对于大流量更准确。UFM比最先进的光流方法（Unimatch）准确度高28%，同时比稠密宽基线匹配器（RoMa）错误率低62%且速度快6.7倍。UFM首次证明统一训练可以超越跨两个领域的专门方法。这一结果实现了快速、通用的对应，并为多模态、长距离和实时对应任务开辟了新方向。", "summary": "本文提出了一种名为UFM的统一光流与匹配模型，旨在解决传统上将宽基线场景和光流估计分开处理的问题。UFM采用简单的Transformer架构，通过在统一数据上训练共同可见像素的(u,v)流，实现了更高的准确性和更快的速度。实验结果表明，UFM在光流和稠密宽基线匹配任务上均优于现有专业方法，证明了统一训练的有效性，为通用对应任务提供了新方向。", "keywords": "稠密对应, 光流, Transformer, 统一模型, 图像匹配", "comments": "这篇论文的创新点在于提出了一个统一的模型UFM来处理光流和稠密对应这两个传统上分开的领域。它证明了通过统一训练一个简单的Transformer架构，可以同时在两个领域取得超越专业方法的性能，这为未来的通用对应任务开辟了新的研究方向。其在速度和准确性上的显著提升显示了其重要性。"}}
{"id": "2506.09105", "title": "MetaTT: A Global Tensor-Train Adapter for Parameter-Efficient Fine-Tuning", "authors": ["Javier Lopez-Piqueres", "Pranav Deshpande", "Archan Ray", "Mattia J. Villani", "Marco Pistoia", "Niraj Kumar"], "summary": "We present MetaTT, a unified Tensor Train (TT) adapter framework for global\nlow-rank fine-tuning of pre-trained transformers. Unlike LoRA, which fine-tunes\neach weight matrix independently, MetaTT uses a single shared TT to factorize\nall transformer sub-modules -- query, key, value, projection, and feed-forward\nlayers -- by indexing the structural axes like layer and matrix type, and\noptionally heads and tasks. For a given rank, while LoRA adds parameters\nproportional to the product across modes, MetaTT only adds parameters\nproportional to the sum across modes leading to a significantly compressed\nfinal adapter. Our benchmarks compare MetaTT with LoRA along with recent\nstate-of-the-art matrix and tensor decomposition based fine-tuning schemes. We\nobserve that when tested on standard language modeling benchmarks, MetaTT leads\nto the most reduction in the parameters while maintaining similar accuracy to\nLoRA and even outperforming other tensor-based methods. Unlike CP or other\nrank-factorizations, the TT ansatz benefits from mature optimization routines\n-- e.g., DMRG-style rank adaptive minimization in addition to Adam, which we\nfind simplifies training. Because new modes can be appended cheaply, MetaTT\nnaturally extends to shared adapters across many tasks without redesigning the\ncore tensor.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09105v1", "AI": {"title_translation": "MetaTT：一种用于参数高效微调的全局张量-训练适配器", "tldr": "MetaTT是一种新的全局张量-训练(TT)适配器框架，用于预训练transformer的低秩微调，比LoRA更节省参数且保持相似精度。", "motivation": "解决预训练transformer微调中参数效率低的问题，并改进现有低秩微调方法的局限性（如LoRA独立微调每个权重矩阵，以及其他张量分解方法优化复杂）。", "method": "提出MetaTT，一个统一的张量训练(TT)适配器框架。它使用一个共享的TT来分解所有transformer子模块（query, key, value, projection, and feed-forward layers），通过索引结构轴（如层、矩阵类型，可选地还有heads和tasks）。与LoRA不同，MetaTT的参数量与模式之和成比例，而非乘积，从而显著压缩了最终适配器。它还受益于成熟的优化程序，如DMRG式秩自适应最小化。", "result": "在标准语言建模基准测试中，MetaTT在显著减少参数的同时，保持了与LoRA相似的准确性，甚至优于其他基于张量的微调方法。", "conclusion": "MetaTT提供了一种参数效率更高、性能与LoRA相当甚至优于其他张量方法的预训练transformer微调方案，并且具有训练简化和易于扩展到多任务的优势。", "translation": "我们提出了MetaTT，一个用于预训练transformer全局低秩微调的统一张量训练（TT）适配器框架。与LoRA独立微调每个权重矩阵不同，MetaTT使用一个共享的TT来分解所有transformer子模块——查询、键、值、投影和前馈层——通过索引层和矩阵类型等结构轴，以及可选的头部和任务。对于给定的秩，LoRA增加的参数与模式的乘积成比例，而MetaTT增加的参数仅与模式之和成比例，从而显著压缩了最终的适配器。我们的基准测试将MetaTT与LoRA以及最近最先进的基于矩阵和张量分解的微调方案进行了比较。我们观察到，在标准语言建模基准测试中，MetaTT在参数减少方面表现最佳，同时保持与LoRA相似的准确性，甚至优于其他基于张量的方法。与CP或其他秩分解不同，TT ansatz受益于成熟的优化例程——例如，除了Adam之外的DMRG式秩自适应最小化，我们发现这简化了训练。由于可以廉价地附加新模式，MetaTT自然地扩展到跨许多任务的共享适配器，而无需重新设计核心张量。", "summary": "本文提出了MetaTT，一种新颖的全局张量训练（TT）适配器框架，用于预训练transformer的参数高效低秩微调。MetaTT通过一个共享TT分解所有transformer子模块，其参数量与模式之和成比例，显著少于LoRA等方法。实验表明，MetaTT在大幅减少参数的同时，在语言建模任务上保持了与LoRA相当的精度，并优于其他张量方法。此外，MetaTT的训练过程更简单，且易于扩展到多任务共享适配器。", "keywords": "张量训练, 参数高效微调, Transformer, 低秩分解, 适配器", "comments": "MetaTT的创新之处在于其采用全局共享的张量训练适配器，而非独立微调每个模块，从而实现了显著的参数压缩。其与LoRA的参数量比较（和 vs 乘积）是关键优势。此外，利用TT的成熟优化例程简化训练，以及易于扩展到多任务的能力，也增加了其实用性。"}}
{"id": "2506.09929", "title": "Assessing a Safety Case: Bottom-up Guidance for Claims and Evidence Evaluation", "authors": ["Scott Schnelle", "Francesca Favaro", "Laura Fraade-Blanar", "David Wichner", "Holland Broce", "Justin Miranda"], "summary": "As Automated Driving Systems (ADS) technology advances, ensuring safety and\npublic trust requires robust assurance frameworks, with safety cases emerging\nas a critical tool toward such a goal. This paper explores an approach to\nassess how a safety case is supported by its claims and evidence, toward\nestablishing credibility for the overall case. Starting from a description of\nthe building blocks of a safety case (claims, evidence, and optional\nformat-dependent entries), this paper delves into the assessment of support of\neach claim through the provided evidence. Two domains of assessment are\noutlined for each claim: procedural support (formalizing process specification)\nand implementation support (demonstrating process application). Additionally,\nan assessment of evidence status is also undertaken, independently from the\nclaims support. Scoring strategies and evaluation guidelines are provided,\nincluding detailed scoring tables for claim support and evidence status\nassessment. The paper further discusses governance, continual improvement, and\ntiming considerations for safety case assessments. Reporting of results and\nfindings is contextualized within its primary use for internal decision-making\non continual improvement efforts. The presented approach builds on state of the\nart auditing practices, but specifically tackles the question of judging the\ncredibility of a safety case. While not conclusive on its own, it provides a\nstarting point toward a comprehensive \"Case Credibility Assessment\" (CCA),\nstarting from the evaluation of the support for each claim (individually and in\naggregate), as well as every piece of evidence provided. By delving into the\ntechnical intricacies of ADS safety cases, this work contributes to the ongoing\ndiscourse on safety assurance and aims to facilitate the responsible\nintegration of ADS technology into society.", "comment": null, "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.09929v1", "AI": {"title_translation": "评估安全案例：对主张和证据评估的自下而上指导", "tldr": "本文提出了一种评估自动驾驶系统安全案例可信度的方法，通过自下而上地评估其主张和证据的支持度。", "motivation": "随着自动驾驶系统（ADS）技术的发展，确保安全和公众信任需要强大的保障框架，而安全案例是实现这一目标的关键工具。本文旨在探讨如何评估安全案例由其主张和证据支持的程度，以建立整体案例的可信度。", "method": "本文首先描述了安全案例的基本组成部分（主张、证据），然后深入探讨了通过所提供的证据评估每个主张的支持度。为每个主张概述了两个评估领域：程序支持和实施支持。此外，还独立于主张支持对证据状态进行了评估。论文提供了评分策略和评估指南，包括用于主张支持和证据状态评估的详细评分表。该方法建立在最先进的审计实践基础上。", "result": "提出的方法提供了一个迈向全面“案例可信度评估”（CCA）的起点，从评估每个主张（单独和总计）的支持度以及提供的每一项证据开始。", "conclusion": "这项工作深入探讨了ADS安全案例的技术复杂性，有助于正在进行的安全保障讨论，并旨在促进ADS技术在社会中的负责任整合。", "translation": "随着自动驾驶系统（ADS）技术的发展，确保安全和公众信任需要强大的保障框架，而安全案例是实现这一目标的关键工具。本文探讨了一种评估安全案例如何由其主张和证据支持的方法，以建立整体案例的可信度。本文从安全案例的基本组成部分（主张、证据和可选的格式依赖条目）的描述开始，深入研究了通过所提供的证据评估每个主张的支持度。为每个主张概述了两个评估领域：程序支持（规范流程规范）和实施支持（演示流程应用）。此外，还独立于主张支持对证据状态进行了评估。论文提供了评分策略和评估指南，包括用于主张支持和证据状态评估的详细评分表。论文进一步讨论了安全案例评估的治理、持续改进和时间考虑。结果和发现的报告在其主要用于内部决策以进行持续改进工作的背景下进行了说明。所提出的方法建立在最先进的审计实践基础上，但专门解决了判断安全案例可信度的问题。虽然其本身并非决定性的，但它提供了一个迈向全面“案例可信度评估”（CCA）的起点，从评估每个主张（单独和总计）的支持度以及提供的每一项证据开始。通过深入探讨ADS安全案例的技术复杂性，这项工作有助于正在进行的安全保障讨论，并旨在促进ADS技术在社会中的负责任整合。", "summary": "本文提出了一种评估自动驾驶系统（ADS）安全案例可信度的方法。该方法通过自下而上地评估安全案例中主张和证据的支持度来实现，包括程序支持和实施支持两个维度，并独立评估证据状态。论文提供了详细的评分策略和指南，旨在为全面的“案例可信度评估”（CCA）奠定基础，从而促进ADS技术的安全集成。", "keywords": "安全案例, 自动驾驶系统, 主张, 证据评估, 可信度评估", "comments": "这篇论文的创新点在于提出了一个系统化的、自下而上的安全案例评估方法，特别关注了主张和证据的可信度评估，这对于自动驾驶系统等高风险领域的安全保障至关重要。其详细的评分策略和指南具有很强的实践指导意义，填补了现有审计实践在安全案例可信度判断方面的空白。"}}
{"id": "2506.09226", "title": "Terabyte-Scale Analytics in the Blink of an Eye", "authors": ["Bowen Wu", "Wei Cui", "Carlo Curino", "Matteo Interlandi", "Rathijit Sen"], "summary": "For the past two decades, the DB community has devoted substantial research\nto take advantage of cheap clusters of machines for distributed data analytics\n-- we believe that we are at the beginning of a paradigm shift. The scaling\nlaws and popularity of AI models lead to the deployment of incredibly powerful\nGPU clusters in commercial data centers. Compared to CPU-only solutions, these\nclusters deliver impressive improvements in per-node compute, memory bandwidth,\nand inter-node interconnect performance. In this paper, we study the problem of\nscaling analytical SQL queries on distributed clusters of GPUs, with the stated\ngoal of establishing an upper bound on the likely performance gains. To do so,\nwe build a prototype designed to maximize performance by leveraging ML/HPC best\npractices, such as group communication primitives for cross-device data\nmovements. This allows us to conduct thorough performance experimentation to\npoint our community towards a massive performance opportunity of at least\n60$\\times$. To make these gains more relatable, before you can blink twice, our\nsystem can run all 22 queries of TPC-H at a 1TB scale factor!", "comment": null, "cate": "cs.DB", "url": "http://arxiv.org/abs/2506.09226v1", "AI": {"title_translation": "眨眼间完成太字节级分析", "tldr": "本文研究了在分布式GPU集群上扩展分析性SQL查询的问题，旨在利用ML/HPC最佳实践，实现至少60倍的性能提升，使系统能在极短时间内处理1TB规模的TPC-H所有查询。", "motivation": "过去二十年，数据库社区致力于利用廉价机器集群进行分布式数据分析，但作者认为范式正在转变。AI模型的扩展定律和普及导致了商用数据中心部署了功能强大的GPU集群，与仅使用CPU的解决方案相比，这些集群在每节点计算、内存带宽和节点间互连性能方面都有显著提升。因此，本文的动机是研究如何在分布式GPU集群上扩展分析性SQL查询，并建立潜在性能增益的上限。", "method": "研究人员构建了一个原型系统，旨在通过利用ML/HPC的最佳实践（例如用于跨设备数据移动的组通信原语）来最大化性能。他们通过该原型进行彻底的性能实验。", "result": "实验结果表明，存在至少60倍的巨大性能提升机会。该系统能在极短时间内完成1TB规模TPC-H基准测试中的所有22个查询。", "conclusion": "本文指出了在分布式GPU集群上进行分析性SQL查询存在巨大的性能提升机会（至少60倍），并成功展示了在GPU集群上实现太字节级分析的可行性，预示着数据分析领域可能迎来新的范式转变。", "translation": "在过去的二十年里，数据库社区投入了大量研究来利用廉价机器集群进行分布式数据分析——我们相信我们正处于范式转变的开端。AI模型的扩展定律和普及导致了商用数据中心部署了功能强大的GPU集群。与仅使用CPU的解决方案相比，这些集群在每节点计算、内存带宽和节点间互连性能方面都有显著提升。在本文中，我们研究了在分布式GPU集群上扩展分析性SQL查询的问题，旨在建立可能的性能增益上限。为此，我们构建了一个原型系统，旨在通过利用ML/HPC的最佳实践（例如用于跨设备数据移动的组通信原语）来最大化性能。这使我们能够进行彻底的性能实验，以向我们的社区指出至少60倍的巨大性能机会。为了让这些收益更具关联性，在您眨两次眼之前，我们的系统就能以1TB的规模因子运行TPC-H的所有22个查询！", "summary": "该论文探讨了在分布式GPU集群上扩展分析性SQL查询的潜力，旨在利用GPU相比CPU解决方案在计算、内存带宽和互连方面的显著优势。通过构建一个利用ML/HPC最佳实践的原型系统，研究人员展示了至少60倍的性能提升，并成功地在极短时间内完成了1TB规模TPC-H基准测试中的所有查询，预示着数据分析领域可能迎来新的范式转变。", "keywords": "GPU集群, 分布式数据分析, SQL查询, 性能优化, TPC-H", "comments": "本文极具创新性，将AI/HPC领域GPU集群的强大计算能力引入到传统的数据分析（SQL查询）领域，提出了一种新的范式。其通过利用ML/HPC最佳实践来构建原型系统，并量化出至少60倍的性能提升，这对于大数据分析而言是里程碑式的进展。它指明了未来分布式数据分析的一个重要方向，即充分利用GPU集群的潜力，有望大幅缩短分析时间，从而实现更快速的决策。"}}
{"id": "2506.09679", "title": "Geometric flow regularization in latent spaces for smooth dynamics with the efficient variations of curvature", "authors": ["Andrew Gracyk"], "summary": "We design strategies in nonlinear geometric analysis to temper the effects of\nadversarial learning for sufficiently smooth data of numerical method-type\ndynamics in encoder-decoder methods, variational and deterministic, through the\nuse of geometric flow regularization. We augment latent spaces with geometric\nflows to control structure. Our techniques rely on adaptations of curvature and\nRicci flow. We invent new geometric flows or discover them neurally and\nnon-parametrically. All of our flows are solved using physics-informed\nlearning. Traditional geometric meaning is traded for computing ability, but we\nmaintain key geometric invariants, the primary of which are maintained,\nintrinsically-low structure, canonicity or a lack of irregularity,\nnontriviality due to sufficient lower bounds on curvature, and distortion of\nvolume element, that develop quality in the inference stage. Our primary\ncontributions are fourfold. We develop a loss based on Gaussian curvature using\nclosed path circulation integration for surfaces, bypassing automatic\ndifferentiation of the Christoffel symbols through use of Stokes' theorem. We\ninvent a new parametric flow derived from a linear version of the Gauss\nequation and a Riemannian decomposition for a custom tensor defined with a\nnormal Hessian and Weyl tensor proxies. We develop two strategies based on time\ndifferentiation of functionals, one with a special case of scalar curvature for\nconformally-changed metrics, and another with harmonic maps, their energy, and\ninduced metrics. Our methods, while diminished analytically, maintain overall\nintegral latent structure. We showcase that curvature flows and the formulation\nof geometric structure in intermediary encoded settings enhance learning and\noverall zero-shot and adversarial fidelity.", "comment": "First version", "cate": "math.NA", "url": "http://arxiv.org/abs/2506.09679v1", "AI": {"title_translation": "潜在空间中的几何流正则化，用于具有高效曲率变化的平滑动力学", "tldr": "本文提出在编码器-解码器模型的潜在空间中使用几何流正则化，以缓和对抗性学习的影响，并增强数据平滑性和对抗性鲁棒性。", "motivation": "旨在通过非线性几何分析策略和几何流正则化，来缓和对抗性学习对编码器-解码器方法中数值方法类型动力学的足够平滑数据的影响。", "method": "1. 通过几何流增强潜在空间以控制结构。2. 技术依赖于曲率和Ricci流的改编。3. 发明新的几何流或通过神经网络和非参数方式发现它们。4. 所有流都通过物理信息学习求解。5. 开发了一种基于高斯曲率的损失函数，利用闭合路径循环积分，并通过斯托克斯定理绕过Christoffel符号的自动微分。6. 发明了一种新的参数流，该流源自高斯方程的线性版本和为自定义张量设计的黎曼分解。7. 开发了两种基于泛函时间微分的策略：一种是针对共形变换度量的标量曲率特例，另一种是针对调和映射、其能量和诱导度量。", "result": "曲率流和中间编码设置中的几何结构表述增强了学习能力以及整体的零样本和对抗性保真度。", "conclusion": "尽管分析上有所不足，但本文提出的方法保持了整体的积分潜在结构，并能有效增强学习和对抗性保真度。", "translation": "我们设计了非线性几何分析策略，通过使用几何流正则化来缓和对抗性学习对编码器-解码器方法（变分和确定性）中数值方法类型动力学足够平滑数据的影响。我们用几何流增强潜在空间以控制结构。我们的技术依赖于曲率和Ricci流的改编。我们发明了新的几何流，或者通过神经网络和非参数方式发现它们。我们所有的流都通过物理信息学习求解。传统几何意义被计算能力所取代，但我们保持了关键的几何不变量，其中主要包括内在的低结构、规范性或缺乏不规则性、由于曲率的充分下界而导致的非平凡性，以及在推理阶段发展质量的体积元畸变。我们的主要贡献有四方面。我们开发了一种基于高斯曲率的损失函数，该函数利用闭合路径循环积分来处理曲面，通过使用斯托克斯定理绕过了Christoffel符号的自动微分。我们发明了一种新的参数流，该流源自高斯方程的线性版本和为自定义张量设计的黎曼分解，该张量使用法向Hessian和Weyl张量代理定义。我们开发了两种基于泛函时间微分的策略，一种是针对共形变换度量的标量曲率特例，另一种是针对调和映射、其能量和诱导度量。我们的方法虽然在分析上有所不足，但保持了整体的积分潜在结构。我们展示了曲率流和中间编码设置中几何结构的表述增强了学习能力以及整体的零样本和对抗性保真度。", "summary": "本文通过在编码器-解码器方法的潜在空间中引入几何流正则化，来应对对抗性学习的影响。研究人员利用非线性几何分析，改编曲率和Ricci流，并开发了新的几何流，这些流通过物理信息学习求解。主要贡献包括基于高斯曲率的损失函数、一种新的参数流以及两种基于泛函时间微分的策略。实验结果表明，这些几何流和结构表述显著提升了学习效果、零样本能力和对抗性保真度，同时保持了潜在结构的完整性。", "keywords": "几何流, 潜在空间, 对抗性学习, 曲率, 物理信息学习", "comments": "这篇论文的创新点在于将复杂的非线性几何分析（如曲率流和Ricci流）引入到机器学习的潜在空间正则化中，以解决对抗性学习带来的数据不平滑问题。通过牺牲部分传统几何意义来换取计算能力，同时保留关键几何不变量，这是一种新颖的思路。特别是利用物理信息学习来求解几何流，以及开发基于高斯曲率和斯托克斯定理的损失函数，都体现了跨学科的深度融合和技术创新。该方法在增强对抗性鲁棒性和零样本学习方面的潜力巨大。"}}
{"id": "2506.09548", "title": "Tightly-Coupled LiDAR-IMU-Leg Odometry with Online Learned Leg Kinematics Incorporating Foot Tactile Information", "authors": ["Taku Okawara", "Kenji Koide", "Aoki Takanose", "Shuji Oishi", "Masashi Yokozuka", "Kentaro Uno", "Kazuya Yoshida"], "summary": "In this letter, we present tightly coupled LiDAR-IMU-leg odometry, which is\nrobust to challenging conditions such as featureless environments and\ndeformable terrains. We developed an online learning-based leg kinematics model\nnamed the neural leg kinematics model, which incorporates tactile information\n(foot reaction force) to implicitly express the nonlinear dynamics between\nrobot feet and the ground. Online training of this model enhances its\nadaptability to weight load changes of a robot (e.g., assuming delivery or\ntransportation tasks) and terrain conditions. According to the \\textit{neural\nadaptive leg odometry factor} and online uncertainty estimation of the leg\nkinematics model-based motion predictions, we jointly solve online training of\nthis kinematics model and odometry estimation on a unified factor graph to\nretain the consistency of both. The proposed method was verified through real\nexperiments using a quadruped robot in two challenging situations: 1) a sandy\nbeach, representing an extremely featureless area with a deformable terrain,\nand 2) a campus, including multiple featureless areas and terrain types of\nasphalt, gravel (deformable terrain), and grass. Experimental results showed\nthat our odometry estimation incorporating the \\textit{neural leg kinematics\nmodel} outperforms state-of-the-art works. Our project page is available for\nfurther details: https://takuokawara.github.io/RAL2025_project_page/", "comment": "Robotics and Automation Letters", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.09548v1", "AI": {"title_translation": "紧密耦合的LiDAR-IMU-腿部里程计，结合在线学习的腿部运动学和足部触觉信息", "tldr": "一种鲁棒的LiDAR-IMU-腿部里程计系统，针对四足机器人设计，通过在线学习的神经腿部运动学模型并结合足部触觉信息，以应对无特征和可变形地形等挑战性环境。", "motivation": "开发一种鲁obust的里程计系统，使四足机器人能够在无特征环境和可变形地形等挑战性条件下运行，并适应机器人负重变化和地形条件。", "method": "开发了一个在线学习的神经腿部运动学模型，该模型融合了足部触觉信息（足部反作用力），以隐式表达机器人足部与地面之间的非线性动力学。通过“神经自适应腿部里程计因子”和基于腿部运动学模型的运动预测的在线不确定性估计，在统一的因子图上联合求解该运动学模型的在线训练和里程计估计，以保持两者的 E2一致性。", "result": "在沙滩和校园（包括沥青、碎石、草地等）等挑战性环境下，使用四足机器人进行的真实实验表明，该方法结合神经腿部运动学模型的里程计估计性能优于现有最先进的工作。", "conclusion": "所提出的结合在线学习的神经腿部运动学模型和足部触觉信息的紧密耦合LiDAR-IMU-腿部里程计系统，显著提高了四足机器人在挑战性环境中的鲁棒性和性能，优于现有方法。", "translation": "在本文中，我们提出了一种紧密耦合的LiDAR-IMU-腿部里程计，它对无特征环境和可变形地形等挑战性条件具有鲁棒性。我们开发了一种基于在线学习的腿部运动学模型，名为神经腿部运动学模型，该模型结合了触觉信息（足部反作用力）以隐式表达机器人足部与地面之间的非线性动力学。该模型的在线训练增强了其对机器人负重变化（例如，假设执行递送或运输任务）和地形条件的适应性。根据神经自适应腿部里程计因子和基于腿部运动学模型的运动预测的在线不确定性估计，我们在统一的因子图上联合求解该运动学模型的在线训练和里程计估计，以保持两者的 E2一致性。所提出的方法通过在两种挑战性情况下使用四足机器人进行的真实实验得到了验证：1）沙滩，代表一个具有可变形地形的极度无特征区域；2）校园，包括多个无特征区域和沥青、碎石（可变形地形）和草地等地形类型。实验结果表明，我们结合神经腿部运动学模型的里程计估计优于现有最先进的工作。我们的项目页面可提供更多详细信息：https://takuokawara.github.io/RAL2025_project_page/", "summary": "本论文提出了一种针对四足机器人的紧密耦合LiDAR-IMU-腿部里程计系统，该系统在无特征环境和可变形地形等复杂条件下表现出强大的鲁棒性。核心创新在于引入了一个在线学习的神经腿部运动学模型，该模型通过整合足部触觉信息来捕捉机器人足部与地面之间的非线性动力学。通过在统一的因子图上联合优化里程计估计和运动学模型的在线训练，系统实现了高度的适应性和一致性。实验结果表明，该方法在沙滩和校园等实际复杂环境中，其里程计估计性能优于现有最先进的技术。", "keywords": "LiDAR-IMU-leg里程计, 神经腿部运动学, 触觉信息, 因子图, 四足机器人", "comments": "本文通过创新性地将在线学习的腿部运动学与足部触觉信息相结合，在四足机器人里程计领域取得了重大进展。利用统一因子图进行联合优化是保持一致性和鲁棒性的巧妙方法。对不同负荷和地形条件（特别是可变形地形）的适应性，解决了机器人导航中的一个关键挑战。在真实世界、挑战性环境中的实验验证进一步巩固了该论文的贡献。"}}
{"id": "2506.09299", "title": "Lightweight Object Detection Using Quantized YOLOv4-Tiny for Emergency Response in Aerial Imagery", "authors": ["Sindhu Boddu", "Arindam Mukherjee"], "summary": "This paper presents a lightweight and energy-efficient object detection\nsolution for aerial imagery captured during emergency response situations. We\nfocus on deploying the YOLOv4-Tiny model, a compact convolutional neural\nnetwork, optimized through post-training quantization to INT8 precision. The\nmodel is trained on a custom-curated aerial emergency dataset, consisting of\n10,820 annotated images covering critical emergency scenarios. Unlike prior\nworks that rely on publicly available datasets, we created this dataset\nourselves due to the lack of publicly available drone-view emergency imagery,\nmaking the dataset itself a key contribution of this work. The quantized model\nis evaluated against YOLOv5-small across multiple metrics, including mean\nAverage Precision (mAP), F1 score, inference time, and model size. Experimental\nresults demonstrate that the quantized YOLOv4-Tiny achieves comparable\ndetection performance while reducing the model size from 22.5 MB to 6.4 MB and\nimproving inference speed by 44\\%. With a 71\\% reduction in model size and a\n44\\% increase in inference speed, the quantized YOLOv4-Tiny model proves highly\nsuitable for real-time emergency detection on low-power edge devices.", "comment": "6 Pages, 3 figures", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09299v1", "AI": {"title_translation": "采用量化YOLOv4-Tiny的轻量级目标检测在航空图像应急响应中的应用", "tldr": "本文提出了一种用于航空图像应急响应的轻量级、能量高效的目标检测方案，通过对YOLOv4-Tiny模型进行INT8量化，显著减小模型尺寸并提高推理速度，适用于低功耗边缘设备。", "motivation": "现有公开数据集中缺乏无人机视角应急图像，且需要在低功耗边缘设备上实现实时应急检测，因此需要一个轻量级、能量高效的目标检测解决方案。", "method": "采用YOLOv4-Tiny模型，通过训练后量化（post-training quantization）将其优化到INT8精度。模型在自定义的航空应急数据集（包含10,820张带标注图像）上进行训练。将量化模型与YOLOv5-small在mAP、F1分数、推理时间和模型大小等多个指标上进行评估。", "result": "量化后的YOLOv4-Tiny在检测性能上与YOLOv5-small相当，同时模型尺寸从22.5 MB减少到6.4 MB（减少71%），推理速度提高44%。", "conclusion": "量化后的YOLOv4-Tiny模型非常适合在低功耗边缘设备上进行实时应急检测。", "translation": "本文提出了一种轻量级、能量高效的目标检测解决方案，用于在应急响应情况下捕获的航空图像。我们专注于部署YOLOv4-Tiny模型，这是一种紧凑的卷积神经网络，通过训练后量化优化到INT8精度。该模型在一个自定义策划的航空应急数据集上进行训练，该数据集包含10,820张标注图像，涵盖了关键的应急场景。与依赖公开可用数据集的先前工作不同，由于缺乏公开可用的无人机视角应急图像，我们自己创建了这个数据集，使得该数据集本身成为这项工作的一个关键贡献。量化模型与YOLOv5-small在多个指标上进行了评估，包括平均精度均值（mAP）、F1分数、推理时间和模型大小。实验结果表明，量化后的YOLOv4-Tiny实现了可比的检测性能，同时将模型尺寸从22.5 MB减少到6.4 MB，并将推理速度提高了44%。模型尺寸减少71%，推理速度提高44%，量化后的YOLOv4-Tiny模型被证明非常适用于低功耗边缘设备上的实时应急检测。", "summary": "本文提出了一种基于INT8量化YOLOv4-Tiny的轻量级目标检测方案，专为航空应急图像设计。研究团队创建了一个包含10,820张图像的自定义数据集，以弥补公开应急数据的不足。实验结果显示，量化后的YOLOv4-Tiny在保持可比检测性能的同时，模型尺寸减小了71%，推理速度提升了44%，使其非常适合在低功耗边缘设备上进行实时应急响应。", "keywords": "量化YOLOv4-Tiny, 目标检测, 航空图像, 应急响应, 边缘设备", "comments": "该论文的创新点在于针对航空应急场景定制了数据集，并成功将YOLOv4-Tiny模型进行量化以适应边缘设备，解决了实际应用中资源受限的问题。其在模型尺寸和推理速度上的显著改进，对于实时应急响应具有重要意义。"}}
{"id": "2506.09108", "title": "SensorLM: Learning the Language of Wearable Sensors", "authors": ["Yuwei Zhang", "Kumar Ayush", "Siyuan Qiao", "A. Ali Heydari", "Girish Narayanswamy", "Maxwell A. Xu", "Ahmed A. Metwally", "Shawn Xu", "Jake Garrison", "Xuhai Xu", "Tim Althoff", "Yun Liu", "Pushmeet Kohli", "Jiening Zhan", "Mark Malhotra", "Shwetak Patel", "Cecilia Mascolo", "Xin Liu", "Daniel McDuff", "Yuzhe Yang"], "summary": "We present SensorLM, a family of sensor-language foundation models that\nenable wearable sensor data understanding with natural language. Despite its\npervasive nature, aligning and interpreting sensor data with language remains\nchallenging due to the lack of paired, richly annotated sensor-text\ndescriptions in uncurated, real-world wearable data. We introduce a\nhierarchical caption generation pipeline designed to capture statistical,\nstructural, and semantic information from sensor data. This approach enabled\nthe curation of the largest sensor-language dataset to date, comprising over\n59.7 million hours of data from more than 103,000 people. Furthermore, SensorLM\nextends prominent multimodal pretraining architectures (e.g., CLIP, CoCa) and\nrecovers them as specific variants within a generic architecture. Extensive\nexperiments on real-world tasks in human activity analysis and healthcare\nverify the superior performance of SensorLM over state-of-the-art in zero-shot\nrecognition, few-shot learning, and cross-modal retrieval. SensorLM also\ndemonstrates intriguing capabilities including scaling behaviors, label\nefficiency, sensor captioning, and zero-shot generalization to unseen tasks.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09108v1", "AI": {"title_translation": "SensorLM：学习可穿戴传感器的语言", "tldr": "SensorLM是一个新的基础模型家族，旨在通过自然语言理解可穿戴传感器数据，解决了数据注释不足的挑战，并在各种任务中超越了现有技术水平。", "motivation": "由于缺乏未整理的真实世界可穿戴数据中配对的、丰富注释的传感器-文本描述，将传感器数据与语言对齐和解释仍然具有挑战性。", "method": "引入了一个分层的字幕生成管道，旨在从传感器数据中捕获统计、结构和语义信息。此方法促成了迄今为止最大的传感器-语言数据集的整理。SensorLM还扩展了著名的多模态预训练架构（例如CLIP、CoCa），并在通用架构中将它们恢复为特定变体。", "result": "整理了迄今为止最大的传感器-语言数据集，包含超过5970万小时的数据，来自超过103,000人。在人体活动分析和医疗保健的真实世界任务中，SensorLM在零样本识别、少样本学习和跨模态检索方面验证了其优于现有技术的性能。SensorLM还展示了包括扩展行为、标签效率、传感器字幕生成和对未见任务的零样本泛化等有趣的能力。", "conclusion": "SensorLM成功地通过自然语言实现了可穿戴传感器数据理解，并在各种任务和泛化场景中展示了卓越的性能和引人入胜的能力。", "translation": "我们提出了SensorLM，一个传感器-语言基础模型家族，它能够通过自然语言理解可穿戴传感器数据。尽管其普遍存在，但由于在未整理的真实世界可穿戴数据中缺乏配对的、丰富注释的传感器-文本描述，将传感器数据与语言对齐和解释仍然具有挑战性。我们引入了一个分层的字幕生成管道，旨在从传感器数据中捕获统计、结构和语义信息。这种方法促成了迄今为止最大的传感器-语言数据集的整理，包含来自超过103,000人的5970万小时数据。此外，SensorLM扩展了著名的多模态预训练架构（例如CLIP、CoCa），并在通用架构中将它们恢复为特定变体。在人体活动分析和医疗保健的真实世界任务中进行的广泛实验验证了SensorLM在零样本识别、少样本学习和跨模态检索方面优于现有技术的性能。SensorLM还展示了包括扩展行为、标签效率、传感器字幕生成和对未见任务的零样本泛化等有趣的能力。", "summary": "SensorLM是一个新的基础模型家族，旨在通过自然语言理解可穿戴传感器数据。为解决缺乏配对注释数据的挑战，研究人员开发了一个分层字幕生成管道，并创建了迄今为止最大的传感器-语言数据集。SensorLM扩展了现有的多模态预训练架构，并在人体活动分析和医疗保健等实际任务中，在零样本识别、少样本学习和跨模态检索方面表现出优于现有技术水平的性能，并展示了可扩展性、标签效率、传感器字幕生成和对未见任务的零样本泛化能力。", "keywords": "SensorLM, 可穿戴传感器, 自然语言理解, 多模态预训练, 数据集", "comments": "SensorLM的创新在于其通过分层字幕生成管道解决了可穿戴传感器数据与自然语言对齐的挑战，并构建了迄今为止最大的传感器-语言数据集。其能够将现有主流多模态预训练架构作为特定变体恢复，并展示出在多种实际任务中的优越性能和零样本泛化能力，这对于可穿戴设备数据分析和医疗保健领域具有重要意义。"}}
{"id": "2506.09938", "title": "Microservices and Real-Time Processing in Retail IT: A Review of Open-Source Toolchains and Deployment Strategies", "authors": ["Aaditaa Vashisht", "Rekha B S"], "summary": "With the rapid pace of digital transformation, the retail industry is\nincreasingly depending on real-time, scalable, and resilient systems to manage\nfinancial transactions, analyze customer behavior, and streamline order\nprocessing. This literature review explores how modern event-driven and\nmicroservices-based architectures, particularly those leveraging Apache Kafka,\nSpring Boot, MongoDB, and Kubernetes are transforming retail and financial\nsystems. By systematically reviewing academic publications, technical white\npapers, and industry reports from recent years, this study synthesizes key\nthemes and implementation strategies. The analysis reveals that technologies\nlike Kafka and Spring Boot are instrumental in building low-latency,\nevent-driven applications that support real-time analytics and fraud detection,\nwhile MongoDB, when deployed on Kubernetes, ensures fault tolerance and high\navailability in inventory and transaction systems. Kubernetes itself plays a\ncrucial role in automating deployment and scaling of microservices. These\nfindings provide valuable insights for industry practitioners aiming to design\nscalable infrastructures, identify research opportunities in hybrid deployment\nmodels, and offer educators a foundation to integrate modern system\narchitectures into professional and technical communication training.", "comment": null, "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.09938v1", "AI": {"title_translation": "零售IT中的微服务与实时处理：开源工具链与部署策略综述", "tldr": "本论文综述了零售和金融系统中如何利用微服务和实时处理技术（Kafka, Spring Boot, MongoDB, Kubernetes）来实现可扩展性、弹性及实时能力。", "motivation": "随着数字化转型的快速发展，零售业需要实时、可扩展和弹性系统来管理金融交易、分析客户行为和简化订单处理。", "method": "通过系统性地审查近年来的学术出版物、技术白皮书和行业报告，综合关键主题和实施策略。", "result": "Kafka和Spring Boot对于构建低延迟、事件驱动的应用程序（支持实时分析和欺诈检测）至关重要。当MongoDB部署在Kubernetes上时，可确保库存和交易系统的容错性和高可用性。Kubernetes在自动化微服务部署和扩展方面发挥关键作用。", "conclusion": "研究结果为行业从业者设计可扩展基础设施提供了宝贵见解，指明了混合部署模型中的研究机会，并为教育工作者提供了将现代系统架构整合到专业和技术交流培训中的基础。", "translation": "随着数字化转型的快速发展，零售业越来越依赖实时、可扩展和弹性系统来管理金融交易、分析客户行为和简化订单处理。本文献综述探讨了现代事件驱动和基于微服务的架构，特别是那些利用Apache Kafka、Spring Boot、MongoDB和Kubernetes的架构，如何改变零售和金融系统。通过系统性地审查近年来的学术出版物、技术白皮书和行业报告，本研究综合了关键主题和实施策略。分析显示，Kafka和Spring Boot等技术对于构建低延迟、事件驱动的应用程序（支持实时分析和欺诈检测）至关重要，而当MongoDB部署在Kubernetes上时，则确保了库存和交易系统的容错性和高可用性。Kubernetes本身在自动化微服务部署和扩展方面发挥着关键作用。这些发现为旨在设计可扩展基础设施的行业从业者提供了宝贵见解，为混合部署模型中的研究机会提供了方向，并为教育工作者提供了将现代系统架构整合到专业和技术交流培训中的基础。", "summary": "本文献综述探讨了现代事件驱动和基于微服务的架构，特别是利用Apache Kafka、Spring Boot、MongoDB和Kubernetes的架构，如何在零售和金融IT中实现转型。通过系统回顾学术和行业资料，研究综合了关键主题，揭示了这些技术如何支持实时处理、容错性、高可用性和自动化扩展，为从业者和教育工作者提供了宝贵见解。", "keywords": "微服务, 实时处理, 零售IT, Apache Kafka, Kubernetes", "comments": "本论文对现代零售IT中关键技术的综述及时且具有现实意义。其对开源工具链和实际部署策略的关注，使其对行业从业者具有高度价值。此外，对混合部署模型中研究机会的识别也为未来的工作指明了方向。"}}
{"id": "2506.09766", "title": "Vulnerability-Based Optimal Grid Defense Strategies for Enhancing Cyber-Physical Energy System Resilience", "authors": ["Eric Tönges", "Philipp Härtel", "Martin Braun"], "summary": "An approach is proposed to identify optimal asset protection strategies based\non vulnerability assessment outcomes. Traditional bilevel attacker-defender\nmodels emphasize worstcase scenarios but offer limited defensive guidance. In\ncontrast, trilevel models introduce high computational complexity and rely on\nfixed network configurations. The proposed critical-components method leverages\nvulnerability assessment results to determine protection strategies,\neffectively outsourcing the upper-level defense decision. This enables\nadaptability to diverse network topologies, assessment techniques, and\ncyber-physical energy systems without the overhead of multi-level optimization.\nCase studies demonstrate the potential for improved system resilience across\nvarying operational conditions.", "comment": null, "cate": "math.OC", "url": "http://arxiv.org/abs/2506.09766v1", "AI": {"title_translation": "基于脆弱性的最优电网防御策略，以增强信息物理能源系统弹性", "tldr": "该研究提出了一种基于脆弱性评估结果的关键组件方法，用于识别最优资产保护策略，以提高信息物理能源系统的弹性，克服了传统多层优化模型的局限性。", "motivation": "传统的双层攻击者-防御者模型侧重于最坏情况，但防御指导有限；三层模型计算复杂且依赖固定网络配置。因此，需要一种更灵活、高效的方法来确定最优资产保护策略。", "method": "提出了一种名为“关键组件方法”的方法。该方法利用脆弱性评估结果来确定保护策略，有效地将上层防御决策外包，从而避免了多层优化的开销。", "result": "案例研究表明，该方法能够提高系统在不同运行条件下的弹性。", "conclusion": "该研究提出了一种基于脆弱性评估的有效资产保护策略识别方法，能够增强信息物理能源系统的弹性，并且具有良好的适应性和计算效率。", "translation": "提出了一种基于脆弱性评估结果识别最优资产保护策略的方法。传统的双层攻击者-防御者模型强调最坏情况，但提供的防御指导有限。相比之下，三层模型引入了高计算复杂性并依赖于固定的网络配置。所提出的关键组件方法利用脆弱性评估结果来确定保护策略，有效地将上层防御决策外包。这使得该方法能够适应不同的网络拓扑、评估技术和信息物理能源系统，而无需多层优化的开销。案例研究表明，在不同的运行条件下，该方法具有提高系统弹性的潜力。", "summary": "本文提出了一种创新的关键组件方法，利用脆弱性评估结果来制定信息物理能源系统的最优资产保护策略。该方法通过将上层防御决策外包，克服了传统双层和三层攻击者-防御者模型在防御指导、计算复杂性和网络配置依赖性方面的局限性，从而提高了系统弹性并适应多种网络环境。", "keywords": "脆弱性评估, 资产保护策略, 信息物理能源系统, 系统弹性, 关键组件方法", "comments": "该论文提出了一种新颖且实用的方法，通过利用脆弱性评估结果简化了复杂的防御决策过程，避免了多层优化的高计算开销。这种“外包”上层决策的思路是其创新点，使其在不同网络拓扑和评估技术下更具适应性，对于增强信息物理能源系统的网络弹性具有重要意义。"}}
{"id": "2506.09375", "title": "CoLMbo: Speaker Language Model for Descriptive Profiling", "authors": ["Massa Baali", "Shuo Han", "Syed Abdul Hannan", "Purusottam Samal", "Karanveer Singh", "Soham Deshmukh", "Rita Singh", "Bhiksha Raj"], "summary": "Speaker recognition systems are often limited to classification tasks and\nstruggle to generate detailed speaker characteristics or provide context-rich\ndescriptions. These models primarily extract embeddings for speaker\nidentification but fail to capture demographic attributes such as dialect,\ngender, and age in a structured manner. This paper introduces CoLMbo, a Speaker\nLanguage Model (SLM) that addresses these limitations by integrating a speaker\nencoder with prompt-based conditioning. This allows for the creation of\ndetailed captions based on speaker embeddings. CoLMbo utilizes user-defined\nprompts to adapt dynamically to new speaker characteristics and provides\ncustomized descriptions, including regional dialect variations and age-related\ntraits. This innovative approach not only enhances traditional speaker\nprofiling but also excels in zero-shot scenarios across diverse datasets,\nmarking a significant advancement in the field of speaker recognition.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.09375v1", "AI": {"title_translation": "CoLMbo: 用于描述性分析的说话人语言模型", "tldr": "CoLMbo是一个通过结合说话人编码器和基于提示的条件作用来生成详细说话人描述的说话人语言模型，解决了传统说话人识别系统在生成详细特征方面的局限性，并在零样本场景中表现出色。", "motivation": "传统的说话人识别系统通常局限于分类任务，难以生成详细的说话人特征或提供上下文丰富的描述。这些模型主要提取用于说话人识别的嵌入，但未能以结构化的方式捕获方言、性别和年龄等人口统计属性。", "method": "本文介绍了CoLMbo，一个说话人语言模型（SLM），通过将说话人编码器与基于提示的条件作用相结合来解决这些限制。这允许基于说话人嵌入创建详细的描述。CoLMbo利用用户定义的提示动态适应新的说话人特征，并提供定制的描述，包括区域方言变体和与年龄相关的特征。", "result": "CoLMbo能够创建基于说话人嵌入的详细描述，提供定制的描述，包括区域方言变体和与年龄相关的特征。它不仅增强了传统的说话人分析，而且在不同数据集的零样本场景中表现出色。", "conclusion": "CoLMbo的创新方法不仅增强了传统的说话人分析，而且在零样本场景中表现出色，标志着说话人识别领域的一个重大进步。", "translation": "说话人识别系统通常局限于分类任务，难以生成详细的说话人特征或提供上下文丰富的描述。这些模型主要提取用于说话人识别的嵌入，但未能以结构化的方式捕获方言、性别和年龄等人口统计属性。本文介绍了CoLMbo，一个说话人语言模型（SLM），通过将说话人编码器与基于提示的条件作用相结合来解决这些限制。这允许基于说话人嵌入创建详细的描述。CoLMbo利用用户定义的提示动态适应新的说话人特征，并提供定制的描述，包括区域方言变体和与年龄相关的特征。这种创新方法不仅增强了传统的说话人分析，而且在不同数据集的零样本场景中表现出色，标志着说话人识别领域的一个重大进步。", "summary": "CoLMbo是一个新颖的说话人语言模型（SLM），旨在克服现有说话人识别系统在生成详细和上下文丰富描述方面的局限性。它通过整合说话人编码器与基于提示的条件作用，能够根据说话人嵌入生成详细的描述，并利用用户定义的提示进行动态适应和定制化描述，包括方言和年龄特征。该模型在零样本场景下表现出色，代表了说话人识别领域的重要进展。", "keywords": "说话人语言模型, 说话人识别, 描述性分析, 零样本学习, 提示工程", "comments": "CoLMbo的创新之处在于其结合了说话人编码器和基于提示的条件作用，使得模型能够生成更详细、更具描述性的说话人特征，而不仅仅是进行分类。其在零样本场景下的出色表现，预示着该技术在实际应用中具有广阔的潜力，尤其是在需要精细化说话人画像的场景。"}}
{"id": "2506.09438", "title": "Generalization Error Analysis for Attack-Free and Byzantine-Resilient Decentralized Learning with Data Heterogeneity", "authors": ["Haoxiang Ye", "Tao Sun", "Qing Ling"], "summary": "Decentralized learning, which facilitates joint model training across\ngeographically scattered agents, has gained significant attention in the field\nof signal and information processing in recent years. While the optimization\nerrors of decentralized learning algorithms have been extensively studied,\ntheir generalization errors remain relatively under-explored. As the\ngeneralization errors reflect the scalability of trained models on unseen data\nand are crucial in determining the performance of trained models in real-world\napplications, understanding the generalization errors of decentralized learning\nis of paramount importance. In this paper, we present fine-grained\ngeneralization error analysis for both attack-free and Byzantine-resilient\ndecentralized learning with heterogeneous data as well as under mild\nassumptions, in contrast to prior studies that consider homogeneous data and/or\nrely on a stringent bounded stochastic gradient assumption. Our results shed\nlight on the impact of data heterogeneity, model initialization and stochastic\ngradient noise -- factors that have not been closely investigated before -- on\nthe generalization error of decentralized learning. We also reveal that\nByzantine attacks performed by malicious agents largely affect the\ngeneralization error, and their negative impact is inherently linked to the\ndata heterogeneity while remaining independent on the sample size. Numerical\nexperiments on both convex and non-convex tasks are conducted to validate our\ntheoretical findings.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09438v1", "AI": {"title_translation": "具有数据异构性的无攻击和拜占庭鲁棒去中心化学习的泛化误差分析", "tldr": "本文对存在数据异构性的无攻击和拜占庭鲁棒去中心化学习的泛化误差进行了细致分析，揭示了数据异构性、模型初始化和随机梯度噪声的影响，并发现拜占庭攻击对泛化误差的负面影响与数据异构性相关。", "motivation": "去中心化学习的优化误差已得到广泛研究，但其泛化误差相对探索不足。由于泛化误差反映了训练模型在新数据上的可扩展性，并且对模型在实际应用中的性能至关重要，因此理解去中心化学习的泛化误差至关重要。", "method": "本文对无攻击和拜占庭鲁棒去中心化学习在数据异构性和温和假设下进行了细致的泛化误差分析，这与以往考虑同质数据或依赖严格有界随机梯度假设的研究不同。通过数值实验验证了理论发现。", "result": "研究结果揭示了数据异构性、模型初始化和随机梯度噪声对去中心化学习泛化误差的影响。此外，恶意代理执行的拜占庭攻击会严重影响泛化误差，其负面影响与数据异构性固有相关，但与样本量无关。", "conclusion": "本文对去中心化学习的泛化误差进行了深入分析，特别是在数据异构性和拜占庭攻击存在的情况下，揭示了关键因素对其泛化性能的影响，强调了在实际应用中考虑这些因素的重要性。", "translation": "去中心化学习促进了地理上分散的代理之间的联合模型训练，近年来在信号和信息处理领域获得了广泛关注。尽管去中心化学习算法的优化误差已得到广泛研究，但其泛化误差仍相对探索不足。由于泛化误差反映了训练模型在新数据上的可扩展性，并且对训练模型在实际应用中的性能至关重要，因此理解去中心化学习的泛化误差至关重要。在本文中，我们对具有异构数据以及在温和假设下的无攻击和拜占庭鲁棒去中心化学习进行了细致的泛化误差分析，这与以往考虑同质数据和/或依赖严格有界随机梯度假设的研究形成对比。我们的结果揭示了数据异构性、模型初始化和随机梯度噪声（这些因素以前未被密切研究）对去中心化学习泛化误差的影响。我们还发现恶意代理执行的拜占庭攻击在很大程度上影响泛化误差，并且它们的负面影响与数据异构性固有相关，但与样本量无关。对凸和非凸任务进行的数值实验验证了我们的理论发现。", "summary": "本文对具有数据异构性的去中心化学习的泛化误差进行了细致分析，涵盖了无攻击和拜占庭鲁棒两种情况。研究揭示了数据异构性、模型初始化和随机梯度噪声对泛化误差的影响，并指出拜占庭攻击对泛化误差的负面影响与数据异构性密切相关。通过理论分析和数值实验，验证了这些发现，为理解去中心化学习在实际应用中的性能提供了重要见解。", "keywords": "去中心化学习, 泛化误差, 数据异构性, 拜占庭鲁棒, 攻击分析", "comments": "该论文的创新之处在于其对去中心化学习泛化误差的细致分析，特别是在数据异构性和拜占庭攻击存在的情况下，这弥补了以往研究主要关注优化误差或假设同质数据的不足。其结果揭示了数据异构性、模型初始化和随机梯度噪声等未被充分研究的因素对泛化误差的关键影响，并深入探讨了拜占庭攻击的影响机制，具有重要的理论和实践意义。"}}
{"id": "2506.09773", "title": "Cross-Channel Unlabeled Sensing over a Union of Signal Subspaces", "authors": ["Taulant Koka", "Manolis C. Tsakiris", "Benjamín Béjar Haro", "Michael Muma"], "summary": "Cross-channel unlabeled sensing addresses the problem of recovering a\nmulti-channel signal from measurements that were shuffled across channels. This\nwork expands the cross-channel unlabeled sensing framework to signals that lie\nin a union of subspaces. The extension allows for handling more complex signal\nstructures and broadens the framework to tasks like compressed sensing. These\nmismatches between samples and channels often arise in applications such as\nwhole-brain calcium imaging of freely moving organisms or multi-target\ntracking. We improve over previous models by deriving tighter bounds on the\nrequired number of samples for unique reconstruction, while supporting more\ngeneral signal types. The approach is validated through an application in\nwhole-brain calcium imaging, where organism movements disrupt sample-to-neuron\nmappings. This demonstrates the utility of our framework in real-world settings\nwith imprecise sample-channel associations, achieving accurate signal\nreconstruction.", "comment": "Accepted to ICASSP 2025. \\copyright 2025 IEEE. Personal use of this\n  material is permitted", "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.09773v1", "AI": {"title_translation": "跨通道无标签感知在信号子空间并集上的应用", "tldr": "本文将跨通道无标签感知扩展到信号子空间并集，改进了唯一重建的样本界限，并在全脑钙成像等应用中展现了实用性。", "motivation": "跨通道无标签感知旨在从通道混洗的测量中恢复多通道信号。现有框架需要扩展以处理更复杂的信号结构和更广泛的任务（如压缩感知），因为样本和通道之间的不匹配在全脑钙成像或多目标跟踪等实际应用中经常出现。", "method": "这项工作将跨通道无标签感知框架扩展到位于子空间并集中的信号。它推导了唯一重建所需样本数量的更严格界限，并支持更通用的信号类型。该方法通过在全脑钙成像中的应用进行了验证。", "result": "研究人员推导了唯一重建所需样本数量的更严格界限，支持了更通用的信号类型，并在样本-通道关联不精确的实际环境中（如全脑钙成像）实现了精确的信号重建。", "conclusion": "将跨通道无标签感知框架扩展到信号子空间并集，为处理具有样本-通道不匹配的应用（如全脑钙成像）提供了改进的重建保证和实际效用。", "translation": "跨通道无标签感知解决了从跨通道混洗测量中恢复多通道信号的问题。这项工作将跨通道无标签感知框架扩展到位于子空间并集中的信号。这种扩展允许处理更复杂的信号结构，并将该框架拓宽到诸如压缩感知等任务。样本和通道之间的这些不匹配常常出现在自由移动生物的全脑钙成像或多目标跟踪等应用中。我们通过推导出唯一重建所需样本数量的更严格界限，同时支持更通用的信号类型，从而改进了以前的模型。该方法通过在全脑钙成像中的应用得到验证，其中生物体的运动会扰乱样本到神经元的映射。这证明了我们框架在样本-通道关联不精确的实际环境中（实现精确信号重建）的实用性。", "summary": "本文将跨通道无标签感知扩展到位于子空间并集中的信号，从而能够处理更复杂的信号结构和诸如压缩感知等应用。它通过推导更严格的唯一重建样本界限并支持更广泛的信号类型，改进了现有模型。该框架的有效性通过其在全脑钙成像中的应用得到证明，即使生物体运动导致样本-通道不匹配，它也能精确重建信号。", "keywords": "跨通道无标签感知, 子空间并集, 信号重建, 钙成像, 更严格界限", "comments": "这项工作的创新之处在于将跨通道无标签感知框架扩展到子空间并集，这显著拓宽了其在更复杂信号结构和存在样本-通道不匹配的实际场景中的适用性。推导更严格的样本界限也代表了显著的理论改进。其在全脑钙成像中的验证凸显了其实际重要性。"}}
{"id": "2506.09687", "title": "Matrix best approximation in the spectral norm", "authors": ["Vance Faber", "Jörg Liesen", "Petr Tichý"], "summary": "We derive, similar to Lau and Riha, a matrix formulation of a general best\napproximation theorem of Singer for the special case of spectral approximations\nof a given matrix from a given subspace. Using our matrix formulation we\ndescribe the relation of the spectral approximation problem to semidefinite\nprogramming, and we present a simple MATLAB code to solve the problem\nnumerically. We then obtain geometric characterizations of spectral\napproximations that are based on the $k$-dimensional field of $k$ matrices,\nwhich we illustrate with several numerical examples. The general spectral\napproximation problem is a min-max problem, whose value is bounded from below\nby the corresponding max-min problem. Using our geometric characterizations of\nspectral approximations, we derive several necessary and sufficient as well as\nsufficient conditions for equality of the max-min and min-max values. Finally,\nwe prove that the max-min and min-max values are always equal when we\n``double'' the problem. Several results in this paper generalize results that\nhave been obtained in the convergence analysis of the GMRES method for solving\nlinear algebraic systems.", "comment": "24 pages, 3 figures", "cate": "math.NA", "url": "http://arxiv.org/abs/2506.09687v1", "AI": {"title_translation": "谱范数下的矩阵最佳逼近", "tldr": "本文推导了谱逼近问题的矩阵公式，将其与半定规划关联，提供了MATLAB代码，并基于k矩阵的k维场获得了几何特征，同时推导了最大最小和最小最大值相等的条件，并证明了“加倍”问题时它们总是相等，泛化了GMRES方法的一些结果。", "motivation": "为了推导Singer通用最佳逼近定理在给定子空间对给定矩阵进行谱逼近的特殊情况下的矩阵公式，并分析其性质。", "method": "本文推导了谱逼近问题的矩阵公式，将其与半定规划相关联，并使用MATLAB代码进行数值求解。通过k矩阵的k维场获得了几何特征，并分析了最大最小和最小最大问题。", "result": "推导了谱逼近问题的矩阵公式，描述了其与半定规划的关系，并提供了MATLAB代码。获得了基于k矩阵的k维场的谱逼近几何特征。推导了最大最小和最小最大值相等的必要和充分条件以及充分条件。证明了“加倍”问题时最大最小和最小最大值总是相等。", "conclusion": "本文中的几个结果推广了GMRES方法在求解线性代数系统收敛性分析中获得的结果。", "translation": "我们类似于Lau和Riha，推导了Singer通用最佳逼近定理在给定子空间对给定矩阵进行谱逼近的特殊情况下的矩阵公式。利用我们的矩阵公式，我们描述了谱逼近问题与半定规划的关系，并提供了一个简单的MATLAB代码来数值求解该问题。然后，我们获得了基于k矩阵的k维场的谱逼近的几何特征，并通过几个数值例子进行了说明。一般的谱逼近问题是一个最小最大问题，其值由相应的最大最小问题从下方界定。利用我们对谱逼近的几何特征，我们推导了最大最小和最小最大值相等的几个必要和充分条件以及充分条件。最后，我们证明了当我们将问题“加倍”时，最大最小和最小最大值总是相等。本文中的几个结果推广了在线性代数系统求解的GMRES方法的收敛性分析中获得的结果。", "summary": "本文推导了Singer通用最佳逼近定理在谱逼近特殊情况下的矩阵公式。该公式揭示了谱逼近问题与半定规划的关系，并提供了MATLAB数值求解代码。研究还基于k矩阵的k维场获得了谱逼近的几何特征，并分析了最大最小问题，推导了最大最小和最小最大值相等的条件，并证明了在“加倍”问题时它们总是相等。本文的某些结果推广了GMRES方法在线性代数系统收敛性分析中的发现。", "keywords": "矩阵最佳逼近, 谱范数, 半定规划, 几何特征, GMRES方法", "comments": "本文的创新之处在于为谱逼近问题提供了新的矩阵公式和几何特征，并将其与半定规划联系起来。其重要性在于，部分结果能够推广GMRES方法在解决线性代数系统收敛性分析中的应用，这对于数值线性代数领域具有实际意义。"}}
{"id": "2506.09956", "title": "LLMail-Inject: A Dataset from a Realistic Adaptive Prompt Injection Challenge", "authors": ["Sahar Abdelnabi", "Aideen Fay", "Ahmed Salem", "Egor Zverev", "Kai-Chieh Liao", "Chi-Huang Liu", "Chun-Chih Kuo", "Jannis Weigend", "Danyael Manlangit", "Alex Apostolov", "Haris Umair", "João Donato", "Masayuki Kawakita", "Athar Mahboob", "Tran Huu Bach", "Tsun-Han Chiang", "Myeongjin Cho", "Hajin Choi", "Byeonghyeon Kim", "Hyeonjin Lee", "Benjamin Pannell", "Conor McCauley", "Mark Russinovich", "Andrew Paverd", "Giovanni Cherubin"], "summary": "Indirect Prompt Injection attacks exploit the inherent limitation of Large\nLanguage Models (LLMs) to distinguish between instructions and data in their\ninputs. Despite numerous defense proposals, the systematic evaluation against\nadaptive adversaries remains limited, even when successful attacks can have\nwide security and privacy implications, and many real-world LLM-based\napplications remain vulnerable. We present the results of LLMail-Inject, a\npublic challenge simulating a realistic scenario in which participants\nadaptively attempted to inject malicious instructions into emails in order to\ntrigger unauthorized tool calls in an LLM-based email assistant. The challenge\nspanned multiple defense strategies, LLM architectures, and retrieval\nconfigurations, resulting in a dataset of 208,095 unique attack submissions\nfrom 839 participants. We release the challenge code, the full dataset of\nsubmissions, and our analysis demonstrating how this data can provide new\ninsights into the instruction-data separation problem. We hope this will serve\nas a foundation for future research towards practical structural solutions to\nprompt injection.", "comment": "Dataset at:\n  https://huggingface.co/datasets/microsoft/llmail-inject-challenge", "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.09956v1", "AI": {"title_translation": "LLMail-Inject：一个来自真实自适应提示注入挑战的数据集", "tldr": "本文介绍了LLMail-Inject，一个模拟真实场景的公开挑战，旨在通过自适应提示注入攻击LLM邮件助手，并发布了包含20万次攻击提交的数据集，以促进对指令-数据分离问题的研究。", "motivation": "间接提示注入攻击利用大型语言模型（LLM）无法区分输入中指令和数据的固有局限性，这可能导致广泛的安全和隐私风险。尽管已提出多种防御方案，但针对自适应对手的系统评估仍不足，许多现实世界的LLM应用依然脆弱。因此，本研究旨在通过构建一个大规模数据集，深入理解指令-数据分离问题，并为未来的结构性防御方案奠定基础。", "method": "研究人员设计并实施了一项名为LLMail-Inject的公开挑战，模拟了参与者自适应地向电子邮件注入恶意指令以触发LLM邮件助手中未经授权的工具调用的真实场景。该挑战涵盖了多种防御策略、LLM架构和检索配置。", "result": "该挑战产生了包含839名参与者提交的208,095个独特攻击的数据集。研究人员发布了挑战代码、完整的提交数据集及其分析，表明这些数据能够为指令-数据分离问题提供新见解。", "conclusion": "该数据集将为未来针对提示注入的实用结构性解决方案研究奠定基础，以解决指令-数据分离问题。", "translation": "间接提示注入攻击利用大型语言模型（LLM）在输入中区分指令和数据的固有局限性。尽管有许多防御方案，但针对自适应对手的系统评估仍然有限，即使成功的攻击可能带来广泛的安全和隐私影响，并且许多现实世界的基于LLM的应用程序仍然易受攻击。我们展示了LLMail-Inject的成果，这是一项公开挑战，模拟了参与者自适应地尝试将恶意指令注入电子邮件以触发基于LLM的电子邮件助手中未经授权的工具调用的真实场景。该挑战涵盖了多种防御策略、LLM架构和检索配置，最终形成了包含839名参与者提交的208,095个独特攻击的数据集。我们发布了挑战代码、完整的提交数据集以及我们的分析，证明了这些数据如何为指令-数据分离问题提供新见解。我们希望这将为未来针对提示注入的实用结构性解决方案研究奠定基础。", "summary": "本文介绍了LLMail-Inject，一个旨在解决大型语言模型（LLM）中指令-数据分离问题的公共挑战和数据集。该挑战模拟了一个真实的自适应提示注入场景，其中参与者尝试通过电子邮件注入恶意指令以触发LLM邮件助手中未经授权的工具调用。该研究收集了来自839名参与者的208,095个独特攻击提交，并发布了相关代码和数据集。作者强调，这些数据将为深入理解指令-数据分离问题提供新见解，并为未来的提示注入防御研究奠定基础。", "keywords": "提示注入, 大型语言模型, 数据集, 网络安全, LLMail-Inject", "comments": "这篇论文通过创建一个大规模的、模拟真实场景的挑战数据集，为LLM提示注入攻击的研究提供了宝贵的资源。其创新之处在于模拟了自适应对手的行为，这比静态评估更具现实意义。该数据集的发布对于推动LLM安全领域，特别是指令-数据分离问题的结构性解决方案研究具有重要意义。"}}
{"id": "2506.09552", "title": "Enhancing Human-Robot Collaboration: A Sim2Real Domain Adaptation Algorithm for Point Cloud Segmentation in Industrial Environments", "authors": ["Fatemeh Mohammadi Amin", "Darwin G. Caldwell", "Hans Wernher van de Venn"], "summary": "The robust interpretation of 3D environments is crucial for human-robot\ncollaboration (HRC) applications, where safety and operational efficiency are\nparamount. Semantic segmentation plays a key role in this context by enabling a\nprecise and detailed understanding of the environment. Considering the intense\ndata hunger for real-world industrial annotated data essential for effective\nsemantic segmentation, this paper introduces a pioneering approach in the\nSim2Real domain adaptation for semantic segmentation of 3D point cloud data,\nspecifically tailored for HRC. Our focus is on developing a network that\nrobustly transitions from simulated environments to real-world applications,\nthereby enhancing its practical utility and impact on a safe HRC.\n  In this work, we propose a dual-stream network architecture (FUSION)\ncombining Dynamic Graph Convolutional Neural Networks (DGCNN) and Convolutional\nNeural Networks (CNN) augmented with residual layers as a Sim2Real domain\nadaptation algorithm for an industrial environment. The proposed model was\nevaluated on real-world HRC setups and simulation industrial point clouds, it\nshowed increased state-of-the-art performance, achieving a segmentation\naccuracy of 97.76%, and superior robustness compared to existing methods.", "comment": "Preprint, Journal of Intelligent & Robotic Systems", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.09552v1", "AI": {"title_translation": "增强人机协作：一种用于工业环境中点云分割的Sim2Real域适应算法", "tldr": "本文提出了一种名为FUSION的双流网络架构，结合DGCNN和CNN，用于3D点云语义分割的Sim2Real域适应，旨在提高工业环境中人机协作的安全性和效率。该方法在真实世界和模拟数据上表现出最先进的性能，分割精度达到97.76%。", "motivation": "在人机协作（HRC）应用中，对3D环境的鲁棒解释至关重要，因为安全性和操作效率是首要考虑因素。然而，有效的语义分割需要大量的真实世界工业标注数据，这导致了数据匮乏的问题。因此，需要一种能够将模拟环境训练的模型成功应用于真实世界的Sim2Real域适应方法。", "method": "本文提出了一种名为FUSION的双流网络架构，结合了动态图卷积神经网络（DGCNN）和增强了残差层的卷积神经网络（CNN），作为一种用于工业环境的Sim2Real域适应算法。", "result": "该模型在真实世界的人机协作设置和模拟工业点云上进行了评估，结果显示其性能超越了现有最先进的方法，实现了97.76%的分割精度，并且比现有方法具有更强的鲁棒性。", "conclusion": "本文提出的FUSION双流网络架构在Sim2Real域适应方面表现出色，显著提高了工业环境中3D点云语义分割的准确性和鲁棒性，从而增强了人机协作的安全性和实用性。", "translation": "对3D环境的鲁棒解释对于人机协作（HRC）应用至关重要，在这些应用中，安全性和操作效率至高无上。语义分割通过实现对环境的精确和详细理解，在此背景下发挥着关键作用。考虑到有效语义分割对真实世界工业标注数据的强烈数据需求，本文引入了一种在Sim2Real域适应方面的开创性方法，用于3D点云数据的语义分割，专门为人机协作量身定制。我们的重点是开发一个能够从模拟环境稳健地过渡到真实世界应用的神经网络，从而增强其实用性和对安全人机协作的影响。\n在这项工作中，我们提出了一种双流网络架构（FUSION），结合了动态图卷积神经网络（DGCNN）和增强了残差层的卷积神经网络（CNN），作为一种用于工业环境的Sim2Real域适应算法。所提出的模型在真实世界的人机协作设置和模拟工业点云上进行了评估，结果显示其性能超越了现有最先进的方法，实现了97.76%的分割精度，并且比现有方法具有更强的鲁棒性。", "summary": "本文针对人机协作（HRC）中3D环境理解对大量真实世界标注数据的需求，提出了一种名为FUSION的双流网络架构，用于3D点云语义分割的Sim2Real域适应。该网络结合了动态图卷积神经网络（DGCNN）和卷积神经网络（CNN），旨在实现从模拟环境到真实世界的鲁棒过渡。实验结果表明，该模型在真实世界HRC设置和模拟工业点云上，达到了97.76%的分割精度，并显示出优于现有方法的先进性能和卓越鲁棒性，从而提高了HRC的安全性和效率。", "keywords": "Sim2Real, 域适应, 点云分割, 人机协作, 语义分割", "comments": "本文提出了一种结合DGCNN和CNN的双流网络（FUSION），用于解决工业环境中3D点云语义分割的Sim2Real域适应问题，这在人机协作（HRC）领域具有重要意义。其创新之处在于针对性地解决了真实世界数据标注困难的问题，通过域适应提升了模型的实用性。97.76%的分割精度和优越的鲁棒性表明该方法在实际应用中具有很高的潜力，有助于提高HRC的安全性和效率。"}}
{"id": "2506.09300", "title": "Efficient Edge Deployment of Quantized YOLOv4-Tiny for Aerial Emergency Object Detection on Raspberry Pi 5", "authors": ["Sindhu Boddu", "Arindam Mukherjee"], "summary": "This paper presents the deployment and performance evaluation of a quantized\nYOLOv4-Tiny model for real-time object detection in aerial emergency imagery on\na resource-constrained edge device the Raspberry Pi 5. The YOLOv4-Tiny model\nwas quantized to INT8 precision using TensorFlow Lite post-training\nquantization techniques and evaluated for detection speed, power consumption,\nand thermal feasibility under embedded deployment conditions. The quantized\nmodel achieved an inference time of 28.2 ms per image with an average power\nconsumption of 13.85 W, demonstrating a significant reduction in power usage\ncompared to its FP32 counterpart. Detection accuracy remained robust across key\nemergency classes such as Ambulance, Police, Fire Engine, and Car Crash. These\nresults highlight the potential of low-power embedded AI systems for real-time\ndeployment in safety-critical emergency response applications.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09300v1", "AI": {"title_translation": "量化YOLOv4-Tiny在树莓派5上高效边缘部署用于空中紧急目标检测", "tldr": "本文研究了在资源受限的边缘设备树莓派5上部署量化YOLOv4-Tiny模型，用于实时空中紧急目标检测的性能。", "motivation": "在资源受限的边缘设备上实现实时、低功耗的空中紧急图像目标检测是一个挑战，这对于安全关键的紧急响应应用至关重要。", "method": "研究人员将YOLOv4-Tiny模型使用TensorFlow Lite后训练量化技术量化到INT8精度，并在嵌入式部署条件下评估了其检测速度、功耗和热可行性。", "result": "量化后的模型实现了每张图像28.2毫秒的推理时间，平均功耗为13.85瓦，与FP32模型相比显著降低了功耗。在救护车、警车、消防车和车祸等关键紧急类别上的检测精度保持稳定。", "conclusion": "低功耗嵌入式AI系统在安全关键的紧急响应应用中具有实时部署的巨大潜力。", "translation": "本文介绍了量化YOLOv4-Tiny模型在资源受限的边缘设备树莓派5上部署和性能评估，用于空中紧急图像中的实时目标检测。YOLOv4-Tiny模型使用TensorFlow Lite后训练量化技术量化到INT8精度，并在嵌入式部署条件下评估了其检测速度、功耗和热可行性。量化后的模型实现了每张图像28.2毫秒的推理时间，平均功耗为13.85瓦，与FP32模型相比显著降低了功耗。在救护车、警车、消防车和车祸等关键紧急类别上的检测精度保持稳定。这些结果突出了低功耗嵌入式AI系统在安全关键的紧急响应应用中实时部署的潜力。", "summary": "本研究探讨了在树莓派5等资源受限的边缘设备上部署量化YOLOv4-Tiny模型，以实现空中紧急目标检测的效率。通过INT8量化，该模型在保持检测精度的同时，显著降低了功耗和推理时间，证明了其在实时紧急响应应用中的可行性。", "keywords": "YOLOv4-Tiny, 边缘部署, 量化, 树莓派5, 目标检测", "comments": "该研究展示了在边缘设备上部署AI模型的实用方法，通过量化技术有效解决了资源受限环境下的性能和功耗问题，对于未来低功耗嵌入式AI系统的发展具有重要意义。"}}
{"id": "2506.09110", "title": "CodeBrain: Bridging Decoupled Tokenizer and Multi-Scale Architecture for EEG Foundation Model", "authors": ["Jingying Ma", "Feng Wu", "Qika Lin", "Yucheng Xing", "Chenyu Liu", "Ziyu Jia", "Mengling Feng"], "summary": "Electroencephalography (EEG) provides real-time insights into brain activity\nand is widely used in neuroscience. However, variations in channel\nconfigurations, sequence lengths, and task objectives limit the transferability\nof traditional task-specific models. Although recent EEG foundation models\n(EFMs) aim to learn generalizable representations, they struggle with limited\nheterogeneous representation capacity and inefficiency in capturing multi-scale\nbrain dependencies. To address these challenges, we propose CodeBrain, an\nefficient EFM structurally aligned with brain organization, trained in two\nstages. (1) We introduce a TFDual-Tokenizer that independently tokenizes\nheterogeneous temporal and frequency components, enabling a quadratic expansion\nof the discrete representation space. This also offers a degree of\ninterpretability through cross-domain token analysis. (2) We propose the\nEEGSSM, which combines a structured global convolution architecture and a\nsliding window attention mechanism to jointly model sparse long-range and local\ndependencies. Unlike fully connected Transformer models, EEGSSM better reflects\nthe brain's small-world topology and efficiently captures EEG's inherent\nmulti-scale structure. EEGSSM is trained with a masked self-supervised learning\nobjective to predict token indices obtained in TFDual-Tokenizer. Comprehensive\nexperiments on 10 public EEG datasets demonstrate the generalizability of\nCodeBrain with linear probing. By offering biologically informed and\ninterpretable EEG modeling, CodeBrain lays the foundation for future\nneuroscience research. Both code and pretraining weights will be released in\nthe future version.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09110v1", "AI": {"title_translation": "CodeBrain：弥合解耦分词器与多尺度架构，构建脑电图基础模型", "tldr": "CodeBrain 是一种高效的脑电图基础模型 (EFM)，通过解耦分词器和多尺度架构解决了传统模型和现有 EFM 在异构表示和多尺度依赖捕获方面的局限性，并在10个公共数据集上展示了良好的泛化能力。", "motivation": "传统的任务特定脑电图模型因通道配置、序列长度和任务目标的变化而限制了可迁移性。尽管现有的脑电图基础模型 (EFM) 旨在学习可泛化表示，但它们在异构表示能力有限以及捕获多尺度脑依赖关系方面效率低下。", "method": "提出 CodeBrain，一个与大脑组织结构对齐的高效 EFM，分两阶段训练。(1) 引入 TFDual-Tokenizer，独立地对异构时域和频域成分进行分词，实现离散表示空间的二次扩展，并提供跨域分词分析的可解释性。(2) 提出 EEGSSM，结合结构化全局卷积架构和滑动窗口注意力机制，共同建模稀疏长程和局部依赖关系，更好地反映大脑的小世界拓扑结构并高效捕获脑电图固有的多尺度结构。EEGSSM 通过掩蔽自监督学习目标进行训练，以预测 TFDual-Tokenizer 获得的分词索引。", "result": "在 10 个公共脑电图数据集上进行的综合实验表明，CodeBrain 具有良好的泛化能力（通过线性探测验证）。", "conclusion": "CodeBrain 提供了生物学上合理且可解释的脑电图建模，为未来的神经科学研究奠定了基础。", "translation": "脑电图 (EEG) 提供对大脑活动的实时洞察，并广泛应用于神经科学。然而，通道配置、序列长度和任务目标的变化限制了传统任务特定模型的可迁移性。尽管最近的脑电图基础模型 (EFM) 旨在学习可泛化表示，但它们在异构表示能力有限以及捕获多尺度脑依赖关系方面效率低下。为了解决这些挑战，我们提出了 CodeBrain，一个高效的 EFM，其结构与大脑组织对齐，并分两阶段进行训练。(1) 我们引入了一个 TFDual-Tokenizer，它独立地对异构时域和频域成分进行分词，从而实现离散表示空间的二次扩展。这还通过跨域分词分析提供了一定程度的可解释性。(2) 我们提出了 EEGSSM，它结合了结构化全局卷积架构和滑动窗口注意力机制，以共同建模稀疏长程和局部依赖关系。与全连接的 Transformer 模型不同，EEGSSM 更好地反映了大脑的小世界拓扑结构，并有效地捕获了脑电图固有的多尺度结构。EEGSSM 通过掩蔽自监督学习目标进行训练，以预测 TFDual-Tokenizer 中获得的分词索引。在 10 个公共脑电图数据集上进行的综合实验表明，CodeBrain 具有良好的泛化能力（通过线性探测验证）。通过提供生物学上合理且可解释的脑电图建模，CodeBrain 为未来的神经科学研究奠定了基础。代码和预训练权重将在未来版本中发布。", "summary": "本文提出 CodeBrain，一个高效的脑电图基础模型 (EFM)，旨在解决传统脑电图模型和现有 EFM 在处理异构数据和捕获多尺度脑依赖方面的局限性。CodeBrain 采用两阶段训练：首先，TFDual-Tokenizer 独立对时域和频域成分进行分词，扩展表示空间并提高可解释性；其次，EEGSSM 结合全局卷积和滑动窗口注意力机制，有效建模脑电图的多尺度和稀疏长程依赖，同时反映大脑的小世界拓扑。通过在 10 个公共数据集上的实验，CodeBrain 展示了其出色的泛化能力，为神经科学研究提供了生物学上合理且可解释的建模方法。", "keywords": "脑电图基础模型, 多尺度架构, 分词器, 自监督学习, 泛化能力", "comments": "CodeBrain 的创新性在于其结合了独立的时频分词器 (TFDual-Tokenizer) 和结合了全局卷积与滑动窗口注意力的多尺度架构 (EEGSSM)，这种设计不仅提高了模型处理异构脑电图数据的能力和效率，更重要的是，它在结构上与大脑的拓扑结构相契合，并提供了可解释性，这对于神经科学研究具有重要意义。其分词器实现离散表示空间的二次扩展，以及 EEGSSM 对小世界拓扑的反映，都是其独特之处。"}}
{"id": "2506.09790", "title": "ComfyUI-R1: Exploring Reasoning Models for Workflow Generation", "authors": ["Zhenran Xu", "Yiyu Wang", "Xue Yang", "Longyue Wang", "Weihua Luo", "Kaifu Zhang", "Baotian Hu", "Min Zhang"], "summary": "AI-generated content has evolved from monolithic models to modular workflows,\nparticularly on platforms like ComfyUI, enabling customization in creative\npipelines. However, crafting effective workflows requires great expertise to\norchestrate numerous specialized components, presenting a steep learning curve\nfor users. To address this challenge, we introduce ComfyUI-R1, the first large\nreasoning model for automated workflow generation. Starting with our curated\ndataset of 4K workflows, we construct long chain-of-thought (CoT) reasoning\ndata, including node selection, workflow planning, and code-level workflow\nrepresentation. ComfyUI-R1 is trained through a two-stage framework: (1) CoT\nfine-tuning for cold start, adapting models to the ComfyUI domain; (2)\nreinforcement learning for incentivizing reasoning capability, guided by a\nfine-grained rule-metric hybrid reward, ensuring format validity, structural\nintegrity, and node-level fidelity. Experiments show that our 7B-parameter\nmodel achieves a 97\\% format validity rate, along with high pass rate,\nnode-level and graph-level F1 scores, significantly surpassing prior\nstate-of-the-art methods that employ leading closed-source models such as\nGPT-4o and Claude series. Further analysis highlights the critical role of the\nreasoning process and the advantage of transforming workflows into code.\nQualitative comparison reveals our strength in synthesizing intricate workflows\nwith diverse nodes, underscoring the potential of long CoT reasoning in AI art\ncreation.", "comment": "Work in progress. Try it out in ComfyUI-Copilot\n  https://github.com/AIDC-AI/ComfyUI-Copilot", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.09790v1", "AI": {"title_translation": "ComfyUI-R1：探索工作流生成的推理模型", "tldr": "ComfyUI-R1是一个大型推理模型，通过两阶段训练（CoT微调和强化学习）自动生成复杂的ComfyUI工作流，显著优于现有方法，降低了用户门槛。", "motivation": "在ComfyUI等平台上，创建有效的模块化AI生成内容工作流需要专业知识来协调大量组件，这对用户来说学习曲线陡峭。", "method": "引入ComfyUI-R1，首个用于自动化工作流生成的大型推理模型。利用4K工作流数据集构建长链式思考(CoT)推理数据（包括节点选择、工作流规划、代码级工作流表示）。模型通过两阶段框架训练：1) CoT微调进行冷启动以适应ComfyUI领域；2) 强化学习，通过细粒度规则-度量混合奖励激励推理能力，确保格式有效性、结构完整性和节点级保真度。", "result": "7B参数模型实现了97%的格式有效性率，以及高通过率、节点级和图级F1分数，显著优于使用GPT-4o和Claude系列等领先闭源模型的现有SOTA方法。进一步分析强调了推理过程的关键作用以及将工作流转换为代码的优势。定性比较显示其在合成包含多样节点的复杂工作流方面的优势。", "conclusion": "ComfyUI-R1通过长链式思考推理，有效地自动化了ComfyUI工作流的生成，显著降低了用户门槛，并展示了其在AI艺术创作中的巨大潜力。将工作流转换为代码对推理过程至关重要。", "translation": "AI生成内容已从单一模型演变为模块化工作流，尤其是在ComfyUI等平台上，实现了创意流程的定制化。然而，制作有效的工作流需要丰富的专业知识来协调众多专业组件，这给用户带来了陡峭的学习曲线。为了解决这一挑战，我们推出了ComfyUI-R1，这是首个用于自动化工作流生成的大型推理模型。我们从整理的4K工作流数据集中着手，构建了长链式思考（CoT）推理数据，包括节点选择、工作流规划和代码级工作流表示。ComfyUI-R1通过两阶段框架进行训练：(1) CoT微调用于冷启动，使模型适应ComfyUI领域；(2) 强化学习用于激励推理能力，由细粒度规则-度量混合奖励引导，确保格式有效性、结构完整性和节点级保真度。实验表明，我们的70亿参数模型实现了97%的格式有效性率，以及高通过率、节点级和图级F1分数，显著超越了采用GPT-4o和Claude系列等领先闭源模型的现有SOTA方法。进一步分析强调了推理过程的关键作用以及将工作流转换为代码的优势。定性比较揭示了我们在合成包含多样节点的复杂工作流方面的优势，突显了长CoT推理在AI艺术创作中的潜力。", "summary": "ComfyUI-R1是一个创新性的大型推理模型，旨在解决ComfyUI平台上手动创建复杂AI生成内容工作流的挑战。该模型通过利用4K工作流数据集构建长链式思考数据，并采用两阶段训练框架（CoT微调和强化学习），实现了工作流的自动化生成。实验证明，ComfyUI-R1在格式有效性、通过率和F1分数上均显著优于现有SOTA方法，并突出了推理过程和代码化工作流的重要性，展现了其在AI艺术创作中的巨大潜力。", "keywords": "工作流生成, 推理模型, ComfyUI, 链式思考, 强化学习", "comments": "这篇论文通过引入ComfyUI-R1，解决了ComfyUI等平台工作流创建的复杂性问题，其创新之处在于首次将大型推理模型应用于自动化工作流生成，并结合了长链式思考(CoT)和强化学习。模型表现显著优于GPT-4o等闭源模型，证明了其在特定领域推理能力上的优势。将工作流表示为代码是一个重要的创新点，有助于模型的推理和生成。这对于降低AI艺术创作的门槛和加速内容生产具有重要意义。"}}
{"id": "2506.09776", "title": "A Saddle Point Algorithm for Robust Data-Driven Factor Model Problems", "authors": ["Shabnam Khodakaramzadeh", "Soroosh Shafiee", "Gabriel de Albuquerque Gleizer", "Peyman Mohajerin Esfahani"], "summary": "We study the factor model problem, which aims to uncover low-dimensional\nstructures in high-dimensional datasets. Adopting a robust data-driven\napproach, we formulate the problem as a saddle-point optimization. Our primary\ncontribution is a general first-order algorithm that solves this reformulation\nby leveraging a linear minimization oracle (LMO). We further develop\nsemi-closed form solutions (up to a scalar) for three specific LMOs,\ncorresponding to the Frobenius norm, Kullback-Leibler divergence, and Gelbrich\n(aka Wasserstein) distance. The analysis includes explicit quantification of\nthese LMOs' regularity conditions, notably the Lipschitz constants of the dual\nfunction, whthich govern the algorithm's convergence performance. Numerical\nexperiments confirm our meod's effectiveness in high-dimensional settings,\noutperforming standard off-the-shelf optimization solvers.", "comment": "Submitted to Automatica", "cate": "math.OC", "url": "http://arxiv.org/abs/2506.09776v1", "AI": {"title_translation": "鲁棒数据驱动因子模型问题的鞍点算法", "tldr": "本文提出了一种用于鲁棒数据驱动因子模型问题的通用一阶鞍点算法，通过线性最小化预言机（LMO）解决，并在数值实验中表现优于现有优化求解器。", "motivation": "旨在揭示高维数据集中的低维结构，解决因子模型问题。", "method": "采用鲁棒数据驱动方法，将问题表述为鞍点优化问题。提出了一种利用线性最小化预言机（LMO）的通用一阶算法来解决此重构问题。进一步为Frobenius范数、Kullback-Leibler散度和Gelbrich距离这三种特定LMO开发了半封闭形式的解。分析中还明确量化了这些LMO的正则性条件，特别是对偶函数的Lipschitz常数。", "result": "数值实验证实了该方法在高维设置中的有效性，并且性能优于标准的现成优化求解器。", "conclusion": "该算法为鲁棒数据驱动因子模型问题提供了一个有效且性能优越的解决方案，尤其适用于高维数据。", "translation": "我们研究因子模型问题，旨在揭示高维数据集中的低维结构。采用鲁棒数据驱动方法，我们将问题表述为鞍点优化。我们的主要贡献是一种通用的一阶算法，该算法通过利用线性最小化预言机（LMO）来解决此重构问题。我们进一步为Frobenius范数、Kullback-Leibler散度和Gelbrich（即Wasserstein）距离这三种特定LMO开发了半封闭形式的解（精确到标量）。分析包括明确量化这些LMO的正则性条件，特别是对偶函数的Lipschitz常数，这些常数决定了算法的收敛性能。数值实验证实了我们方法在高维设置中的有效性，优于标准的现成优化求解器。", "summary": "本文研究了鲁棒数据驱动的因子模型问题，旨在从高维数据中发现低维结构。通过将问题重新表述为鞍点优化，作者提出了一种通用的、基于线性最小化预言机（LMO）的一阶算法。该研究进一步为Frobenius范数、Kullback-Leibler散度和Gelbrich距离对应的LMOs提供了半封闭形式的解，并分析了其正则性条件。数值实验证明了该算法在高维环境下的优越性能。", "keywords": "因子模型, 鞍点算法, 线性最小化预言机, 鲁棒数据驱动, 高维数据", "comments": "该论文的创新之处在于将鲁棒数据驱动的因子模型问题转化为鞍点优化问题，并开发了一种通用的基于LMO的一阶算法。其贡献在于为多种常见距离度量提供了LMO的半封闭解，并对算法的收敛性进行了理论分析。数值实验结果显示其在高维数据处理上的优越性，这对于实际应用具有重要意义。"}}
{"id": "2506.09660", "title": "SyncFed: Time-Aware Federated Learning through Explicit Timestamping and Synchronization", "authors": ["Baran Can Gül", "Stefanos Tziampazis", "Nasser Jazdi", "Michael Weyrich"], "summary": "As Federated Learning (FL) expands to larger and more distributed\nenvironments, consistency in training is challenged by network-induced delays,\nclock unsynchronicity, and variability in client updates. This combination of\nfactors may contribute to misaligned contributions that undermine model\nreliability and convergence. Existing methods like staleness-aware aggregation\nand model versioning address lagging updates heuristically, yet lack mechanisms\nto quantify staleness, especially in latency-sensitive and cross-regional\ndeployments. In light of these considerations, we introduce \\emph{SyncFed}, a\ntime-aware FL framework that employs explicit synchronization and timestamping\nto establish a common temporal reference across the system. Staleness is\nquantified numerically based on exchanged timestamps under the Network Time\nProtocol (NTP), enabling the server to reason about the relative freshness of\nclient updates and apply temporally informed weighting during aggregation. Our\nempirical evaluation on a geographically distributed testbed shows that, under\n\\emph{SyncFed}, the global model evolves within a stable temporal context,\nresulting in improved accuracy and information freshness compared to\nround-based baselines devoid of temporal semantics.", "comment": "Preprint version. Accepted for publication at IEEE ETFA 2025", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09660v1", "AI": {"title_translation": "SyncFed：通过显式时间戳和同步实现时间感知联邦学习", "tldr": "SyncFed 是一种时间感知联邦学习框架，通过时间戳和同步量化更新的“陈旧度”，并在聚合时进行时间加权，从而提高模型准确性和信息新鲜度。", "motivation": "联邦学习在大型分布式环境中面临网络延迟、时钟不同步和客户端更新可变性带来的训练一致性挑战，导致贡献错位，损害模型可靠性和收敛性。现有方法未能有效量化“陈旧度”，尤其是在延迟敏感和跨区域部署中。", "method": "引入 SyncFed 框架，通过显式同步和时间戳（基于网络时间协议 NTP）建立系统范围的共同时间参考。根据交换的时间戳数值化量化“陈旧度”，使服务器能够判断客户端更新的相对新鲜度，并在聚合时应用时间感知的加权。", "result": "在地理分布式测试平台上的实证评估表明，SyncFed 下的全局模型在稳定的时间背景下演进，与缺乏时间语义的基于轮次的基线相比，提高了准确性和信息新鲜度。", "conclusion": "SyncFed 通过引入时间感知机制，有效解决了联邦学习中客户端更新一致性问题，提高了模型性能和数据新鲜度。", "translation": "随着联邦学习（FL）扩展到更大、更分布式的环境，训练的一致性受到网络引起的延迟、时钟不同步和客户端更新可变性的挑战。这些因素的结合可能导致贡献错位，从而损害模型可靠性和收敛性。现有的方法，如陈旧度感知聚合和模型版本控制，启发式地解决了滞后更新问题，但缺乏量化陈旧度的机制，尤其是在延迟敏感和跨区域部署中。鉴于这些考虑，我们引入了 SyncFed，一个时间感知 FL 框架，它采用显式同步和时间戳来在整个系统中建立一个共同的时间参考。陈旧度是根据网络时间协议（NTP）下交换的时间戳进行数值量化的，这使得服务器能够判断客户端更新的相对新鲜度，并在聚合期间应用时间感知的加权。我们在地理分布式测试平台上的实证评估表明，在 SyncFed 下，全局模型在稳定的时间背景下演进，与缺乏时间语义的基于轮次的基线相比，提高了准确性和信息新鲜度。", "summary": "本文提出了 SyncFed，一个时间感知的联邦学习框架，旨在解决大型分布式FL环境中因网络延迟和时钟不同步导致的训练一致性问题。SyncFed 利用显式时间戳和同步（基于NTP）来量化客户端更新的“陈旧度”，并据此进行时间加权聚合。实验结果表明，SyncFed 能使全局模型在稳定的时间上下文中演进，从而提高模型的准确性和信息新鲜度。", "keywords": "联邦学习, 时间感知, 同步, 时间戳, 陈旧度", "comments": "SyncFed 的创新之处在于引入了显式的时间戳和同步机制，将时间维度量化并融入联邦学习的聚合过程，而非仅仅依赖启发式方法。这对于解决大规模、异构FL部署中的数据陈旧性和一致性问题具有重要意义，尤其是在对延迟敏感的场景下。"}}
{"id": "2506.09855", "title": "Foundation Model-Aided Deep Reinforcement Learning for RIS-Assisted Wireless Communication", "authors": ["Mohammad Ghassemi", "Sara Farrag Mobarak", "Han Zhang", "Ali Afana", "Akram Bin Sediq", "Melike Erol-Kantarci"], "summary": "Reconfigurable intelligent surfaces (RIS) have emerged as a promising\ntechnology for enhancing wireless communication by dynamically controlling\nsignal propagation in the environment. However, their efficient deployment\nrelies on accurate channel state information (CSI), which leads to high channel\nestimation overhead due to their passive nature and the large number of\nreflective elements. In this work, we solve this challenge by proposing a novel\nframework that leverages a pre-trained open-source foundation model (FM) named\nlarge wireless model (LWM) to process wireless channels and generate versatile\nand contextualized channel embeddings. These embeddings are then used for the\njoint optimization of the BS beamforming and RIS configurations. To be more\nspecific, for joint optimization, we design a deep reinforcement learning (DRL)\nmodel to automatically select the BS beamforming vector and RIS phase-shift\nmatrix, aiming to maximize the spectral efficiency (SE). This work shows that a\npre-trained FM for radio signal understanding can be fine-tuned and integrated\nwith DRL for effective decision-making in wireless networks. It highlights the\npotential of modality-specific FMs in real-world network optimization.\nAccording to the simulation results, the proposed method outperforms the\nDRL-based approach and beam sweeping-based approach, achieving 9.89% and 43.66%\nhigher SE, respectively.", "comment": "6 pages, 5 figures, PIMRC conference", "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.09855v1", "AI": {"title_translation": "基础模型辅助的深度强化学习用于RIS辅助的无线通信", "tldr": "该研究提出一个利用预训练基础模型（LWM）和深度强化学习（DRL）的框架，用于优化RIS辅助无线通信中的基站波束成形和RIS配置，以解决CSI开销大的问题，并显著提高频谱效率。", "motivation": "RIS部署面临挑战，即其高效部署依赖于精确的信道状态信息（CSI），但其无源特性和大量反射单元导致信道估计开销高。", "method": "提出一个新颖的框架，利用预训练的开源基础模型（LWM）处理无线信道并生成信道嵌入。这些嵌入用于基站波束成形和RIS配置的联合优化。具体来说，设计了一个深度强化学习（DRL）模型来自动选择基站波束成形向量和RIS相移矩阵，目标是最大化频谱效率（SE）。", "result": "仿真结果显示，所提方法优于基于DRL的方法和基于波束扫描的方法，频谱效率分别高出9.89%和43.66%。", "conclusion": "该工作表明，用于无线电信号理解的预训练基础模型可以与DRL进行微调和集成，以实现无线网络中的有效决策，并突出了特定模态基础模型在实际网络优化中的潜力。", "translation": "可重构智能表面（RIS）已成为一种很有前途的技术，通过动态控制环境中的信号传播来增强无线通信。然而，其高效部署依赖于精确的信道状态信息（CSI），由于其无源特性和大量的反射单元，这导致了高昂的信道估计开销。在这项工作中，我们通过提出一个新颖的框架来解决这一挑战，该框架利用预训练的开源基础模型（FM），即大型无线模型（LWM）来处理无线信道并生成多功能和情境化的信道嵌入。这些嵌入随后用于基站波束成形和RIS配置的联合优化。更具体地说，为了联合优化，我们设计了一个深度强化学习（DRL）模型，以自动选择基站波束成形向量和RIS相移矩阵，旨在最大化频谱效率（SE）。这项工作表明，用于无线电信号理解的预训练FM可以与DRL进行微调和集成，以实现无线网络中的有效决策。它突出了特定模态FM在实际网络优化中的潜力。根据仿真结果，所提出的方法优于基于DRL的方法和基于波束扫描的方法，分别实现了9.89%和43.66%的更高频谱效率。", "summary": "本文提出了一个利用预训练基础模型（LWM）和深度强化学习（DRL）的创新框架，旨在解决可重构智能表面（RIS）辅助无线通信中高昂的信道估计开销问题。该框架首先利用LWM生成信道嵌入，然后通过DRL模型联合优化基站波束成形和RIS配置，以最大化频谱效率。仿真结果表明，该方法在频谱效率方面显著优于现有DRL和波束扫描方法，突出了基础模型在无线网络优化中的巨大潜力。", "keywords": "可重构智能表面, 基础模型, 深度强化学习, 信道状态信息, 频谱效率", "comments": "这项工作创新性地将预训练的基础模型（LWM）引入到RIS辅助无线通信的优化中，通过生成信道嵌入有效降低了信道估计开销，并结合DRL实现了高效的联合优化。这为未来无线网络中利用大规模预训练模型进行智能决策提供了新的范式和重要启示。"}}
{"id": "2506.09721", "title": "Generative Models for Parameter Space Reduction applied to Reduced Order Modelling", "authors": ["Guglielmo Padula", "Gianluigi Rozza"], "summary": "Solving and optimising Partial Differential Equations (PDEs) in geometrically\nparameterised domains often requires iterative methods, leading to high\ncomputational and time complexities. One potential solution is to learn a\ndirect mapping from the parameters to the PDE solution. Two prominent methods\nfor this are Data-driven Non-Intrusive Reduced Order Models (DROMs) and\nParametrised Physics Informed Neural Networks (PPINNs). However, their accuracy\ntends to degrade as the number of geometric parameters increases. To address\nthis, we propose adopting Generative Models to create new geometries,\neffectively reducing the number of parameters, and improving the performance of\nDROMs and PPINNs. The first section briefly reviews the general theory of\nGenerative Models and provides some examples, whereas the second focusses on\ntheir application to geometries with fixed or variable points, emphasising\ntheir integration with DROMs and PPINNs. DROMs trained on geometries generated\nby these models demonstrate enhanced accuracy due to reduced parameter\ndimensionality. For PPINNs, we introduce a methodology that leverages\nGenerative Models to reduce the parameter dimensions and improve convergence.\nThis approach is tested on a Poisson equation defined over deformed Stanford\nBunny domains.", "comment": null, "cate": "math.NA", "url": "http://arxiv.org/abs/2506.09721v1", "AI": {"title_translation": "应用于降阶建模的参数空间降维生成模型", "tldr": "现有偏微分方程求解方法在参数多时表现不佳，本文提出使用生成模型减少参数以提高其性能。", "motivation": "在几何参数化域中求解和优化偏微分方程计算成本高；数据驱动非侵入式降阶模型（DROMs）和参数化物理信息神经网络（PPINNs）的精度随几何参数数量增加而降低。", "method": "提出采用生成模型来创建新的几何形状，从而有效减少参数数量，并提高DROMs和PPINNs的性能。对于PPINNs，引入了一种利用生成模型减少参数维度并改善收敛性的方法。", "result": "经过这些模型生成的几何形状训练的DROMs由于参数维度降低而提高了精度。对于PPINNs，所提出的方法改善了收敛性。该方法在变形的斯坦福兔子域上定义的泊松方程上进行了测试。", "conclusion": "生成模型可以有效地减少几何参数化偏微分方程的参数空间，从而提高DROMs和PPINNs的性能。", "translation": "在几何参数化域中求解和优化偏微分方程（PDEs）通常需要迭代方法，这导致计算和时间复杂度很高。一种潜在的解决方案是学习从参数到PDE解的直接映射。为此，两种主要方法是数据驱动的非侵入式降阶模型（DROMs）和参数化物理信息神经网络（PPINNs）。然而，它们的精度往往随着几何参数数量的增加而降低。为了解决这个问题，我们提出采用生成模型来创建新的几何形状，从而有效减少参数数量，并提高DROMs和PPINNs的性能。第一部分简要回顾了生成模型的一般理论并提供了一些示例，而第二部分则重点介绍了它们在具有固定或可变点的几何形状中的应用，强调了它们与DROMs和PPINNs的集成。经过这些模型生成的几何形状训练的DROMs由于参数维度降低而显示出更高的精度。对于PPINNs，我们引入了一种利用生成模型减少参数维度并改善收敛性的方法。该方法在变形的斯坦福兔子域上定义的泊松方程上进行了测试。", "summary": "本文旨在解决使用数据驱动非侵入式降阶模型（DROMs）和参数化物理信息神经网络（PPINNs）求解几何参数化偏微分方程时，计算成本高和精度随参数数量增加而降低的问题。文章提出利用生成模型创建新几何形状，从而有效减少参数空间。这种方法提高了DROMs的精度（因维度降低），并改善了PPINNs的收敛性，并在泊松方程上进行了验证。", "keywords": "生成模型, 参数空间降维, 降阶建模, 偏微分方程, 物理信息神经网络", "comments": "这篇论文为参数化偏微分方程问题中维度灾难的缓解提供了一种创新方法，这对于DROMs和PPINNs等数据驱动方法是一个重要挑战。通过利用生成模型进行参数空间降维，它为提高这些技术在复杂工程和科学模拟中的效率和准确性提供了一个有前景的方向。对几何参数化的关注尤其具有相关性。"}}
{"id": "2506.09581", "title": "Integrating Quantized LLMs into Robotics Systems as Edge AI to Leverage their Natural Language Processing Capabilities", "authors": ["Miguel Á. González-Santamarta", "Francisco J. Rodríguez-Lera", "David Sobrín-Hidalgo", "Ángel Manuel Guerrero-Higueras", "Vicente MatellÁn-Olivera"], "summary": "Large Language Models (LLMs) have experienced great advancements in the last\nyear resulting in an increase of these models in several fields to face natural\nlanguage tasks. The integration of these models in robotics can also help to\nimprove several aspects such as human-robot interaction, navigation, planning\nand decision-making. Therefore, this paper introduces llama\\_ros, a tool\ndesigned to integrate quantized Large Language Models (LLMs) into robotic\nsystems using ROS 2. Leveraging llama.cpp, a highly optimized runtime engine,\nllama\\_ros enables the efficient execution of quantized LLMs as edge artificial\nintelligence (AI) in robotics systems with resource-constrained environments,\naddressing the challenges of computational efficiency and memory limitations.\nBy deploying quantized LLMs, llama\\_ros empowers robots to leverage the natural\nlanguage understanding and generation for enhanced decision-making and\ninteraction which can be paired with prompt engineering, knowledge graphs,\nontologies or other tools to improve the capabilities of autonomous robots.\nAdditionally, this paper provides insights into some use cases of using\nllama\\_ros for planning and explainability in robotics.", "comment": "10 pages, 4 figures, Submitted to 3rd edition of the Workshop on\n  Ontologies and Standards for Robotics and Automation (WOSRA) at ICRA 2024", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.09581v1", "AI": {"title_translation": "将量化大型语言模型作为边缘AI集成到机器人系统中以利用其自然语言处理能力", "tldr": "本文介绍了llama_ros，一个将量化大型语言模型（LLM）作为边缘AI集成到机器人系统中的工具，利用其自然语言处理能力，解决了资源受限环境下的计算效率和内存限制问题。", "motivation": "大型语言模型（LLM）在自然语言任务方面取得了巨大进步，将其集成到机器人中可以改善人机交互、导航、规划和决策等方面。然而，在资源受限的机器人系统中高效集成这些模型面临计算效率和内存限制的挑战。", "method": "本文引入了llama_ros，一个利用ROS 2和高度优化的llama.cpp运行时引擎，将量化大型语言模型（LLM）集成到机器人系统中的工具。它使得LLM能够在资源受限的机器人系统中作为边缘AI高效执行。", "result": "llama_ros使机器人能够利用自然语言理解和生成能力，以增强决策和交互。它可以与提示工程、知识图谱、本体或其他工具结合使用，以提高自主机器人的能力。此外，本文还提供了使用llama_ros进行机器人规划和可解释性的一些用例。", "conclusion": "本文通过llama_ros工具展示了将量化大型语言模型作为边缘AI集成到机器人系统中的方法，从而在资源受限的环境下，利用LLM的自然语言处理能力，显著提升了机器人的决策和交互能力。", "translation": "大型语言模型（LLM）在过去一年取得了巨大进步，导致这些模型在多个领域中越来越多地应用于自然语言任务。将这些模型集成到机器人中也有助于改善人机交互、导航、规划和决策等多个方面。因此，本文引入了llama_ros，一个旨在利用ROS 2将量化大型语言模型（LLM）集成到机器人系统中的工具。llama_ros利用高度优化的运行时引擎llama.cpp，使得量化LLM能够在资源受限的机器人系统中作为边缘人工智能（AI）高效执行，解决了计算效率和内存限制的挑战。通过部署量化LLM，llama_ros使机器人能够利用自然语言理解和生成能力，以增强决策和交互，这可以与提示工程、知识图谱、本体或其他工具结合使用，以提高自主机器人的能力。此外，本文还提供了使用llama_ros进行机器人规划和可解释性的一些用例。", "summary": "本文介绍了llama_ros，一个旨在将量化大型语言模型（LLM）作为边缘AI集成到机器人系统中的工具。该工具利用ROS 2和llama.cpp，解决了在资源受限的机器人环境中集成LLM所面临的计算效率和内存限制问题。通过部署量化LLM，llama_ros使机器人能够利用自然语言处理能力来增强决策和交互，并可与其他技术结合使用，以提高自主机器人的能力。文章还探讨了llama_ros在机器人规划和可解释性方面的应用案例。", "keywords": "量化LLM, 机器人系统, 边缘AI, 自然语言处理, ROS 2", "comments": "本文的创新点在于将量化LLM集成到机器人系统中作为边缘AI，有效地解决了资源受限环境下的部署难题。这对于推动LLM在实际机器人应用中的普及具有重要意义，尤其是在人机交互、自主决策和规划方面。通过提供llama_ros这一具体工具，该研究为研究人员和开发者提供了一个实用的解决方案，加速了LLM与机器人技术的融合。"}}
{"id": "2506.09327", "title": "MSSDF: Modality-Shared Self-supervised Distillation for High-Resolution Multi-modal Remote Sensing Image Learning", "authors": ["Tong Wang", "Guanzhou Chen", "Xiaodong Zhang", "Chenxi Liu", "Jiaqi Wang", "Xiaoliang Tan", "Wenchao Guo", "Qingyuan Yang", "Kaiqi Zhang"], "summary": "Remote sensing image interpretation plays a critical role in environmental\nmonitoring, urban planning, and disaster assessment. However, acquiring\nhigh-quality labeled data is often costly and time-consuming. To address this\nchallenge, we proposes a multi-modal self-supervised learning framework that\nleverages high-resolution RGB images, multi-spectral data, and digital surface\nmodels (DSM) for pre-training. By designing an information-aware adaptive\nmasking strategy, cross-modal masking mechanism, and multi-task self-supervised\nobjectives, the framework effectively captures both the correlations across\ndifferent modalities and the unique feature structures within each modality. We\nevaluated the proposed method on multiple downstream tasks, covering typical\nremote sensing applications such as scene classification, semantic\nsegmentation, change detection, object detection, and depth estimation.\nExperiments are conducted on 15 remote sensing datasets, encompassing 26 tasks.\nThe results demonstrate that the proposed method outperforms existing\npretraining approaches in most tasks. Specifically, on the Potsdam and\nVaihingen semantic segmentation tasks, our method achieved mIoU scores of\n78.30\\% and 76.50\\%, with only 50\\% train-set. For the US3D depth estimation\ntask, the RMSE error is reduced to 0.182, and for the binary change detection\ntask in SECOND dataset, our method achieved mIoU scores of 47.51\\%, surpassing\nthe second CS-MAE by 3 percentage points. Our pretrain code, checkpoints, and\nHR-Pairs dataset can be found in https://github.com/CVEO/MSSDF.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09327v1", "AI": {"title_translation": "MSSDF: 高分辨率多模态遥感图像学习的模态共享自监督蒸馏", "tldr": "MSSDF提出了一种多模态自监督学习框架，利用RGB、多光谱和DSM数据进行预训练，通过信息感知掩蔽策略、跨模态掩蔽机制和多任务目标，在多种遥感下游任务上表现优异。", "motivation": "遥感图像解译在环境监测、城市规划和灾害评估中至关重要，但获取高质量的标注数据成本高昂且耗时。", "method": "本研究提出了一个多模态自监督学习框架，利用高分辨率RGB图像、多光谱数据和数字表面模型（DSM）进行预训练。该框架设计了信息感知自适应掩蔽策略、跨模态掩蔽机制和多任务自监督目标，有效捕获不同模态间的相关性以及各模态内独特的特征结构。", "result": "该方法在15个遥感数据集、涵盖26个任务的评估中，在大多数任务中优于现有预训练方法。具体来说，在Potsdam和Vaihingen语义分割任务中，仅使用50%的训练集就达到了78.30%和76.50%的mIoU分数；在US3D深度估计任务中，RMSE误差降至0.182；在SECOND数据集的二元变化检测任务中，mIoU分数达到47.51%，超过第二名CS-MAE 3个百分点。", "conclusion": "所提出的MSSDF框架通过有效整合多模态数据并进行自监督预训练，显著提升了遥感图像在多种下游任务上的性能，证明了其在解决标注数据稀缺问题上的有效性和优越性。", "translation": "遥感图像解译在环境监测、城市规划和灾害评估中扮演着关键角色。然而，获取高质量的标注数据通常成本高昂且耗时。为了应对这一挑战，我们提出了一种多模态自监督学习框架，该框架利用高分辨率RGB图像、多光谱数据和数字表面模型（DSM）进行预训练。通过设计信息感知自适应掩蔽策略、跨模态掩蔽机制和多任务自监督目标，该框架有效地捕获了不同模态间的相关性以及各模态内独特的特征结构。我们在多个下游任务上评估了所提出的方法，涵盖了场景分类、语义分割、变化检测、目标检测和深度估计等典型的遥感应用。实验在15个遥感数据集上进行，涵盖26个任务。结果表明，所提出的方法在大多数任务中优于现有预训练方法。具体来说，在Potsdam和Vaihingen语义分割任务中，我们的方法仅使用50%的训练集就达到了78.30%和76.50%的mIoU分数。对于US3D深度估计任务，RMSE误差降至0.182；对于SECOND数据集中的二元变化检测任务，我们的方法达到了47.51%的mIoU分数，超过第二名CS-MAE 3个百分点。我们的预训练代码、检查点和HR-Pairs数据集可在https://github.com/CVEO/MSSDF找到。", "summary": "本文提出了一种名为MSSDF的多模态自监督学习框架，旨在解决遥感图像解译中高质量标注数据获取困难的问题。该框架利用高分辨率RGB、多光谱和DSM数据进行预训练，并设计了信息感知自适应掩蔽策略、跨模态掩蔽机制和多任务自监督目标，以捕获模态间相关性和模态内特征结构。在场景分类、语义分割、变化检测、目标检测和深度估计等多种遥感下游任务的广泛评估中，MSSDF在多数任务上超越了现有预训练方法，取得了显著的性能提升，例如在语义分割和变化检测任务中表现突出。", "keywords": "多模态, 自监督学习, 遥感图像, 预训练, 模态共享", "comments": "MSSDF的创新性在于其模态共享自监督蒸馏框架，特别是信息感知自适应掩蔽策略和跨模态掩蔽机制，这使其能够有效利用多模态遥感数据进行预训练，并提升在多种下游任务上的泛化能力。其在仅使用50%训练集的情况下取得高分，凸显了其数据效率和实用价值。该方法为解决遥感领域标注数据稀缺问题提供了有效途径。"}}
{"id": "2506.09114", "title": "TRACE: Grounding Time Series in Context for Multimodal Embedding and Retrieval", "authors": ["Jialin Chen", "Ziyu Zhao", "Gaukhar Nurbek", "Aosong Feng", "Ali Maatouk", "Leandros Tassiulas", "Yifeng Gao", "Rex Ying"], "summary": "The ubiquity of dynamic data in domains such as weather, healthcare, and\nenergy underscores a growing need for effective interpretation and retrieval of\ntime-series data. These data are inherently tied to domain-specific contexts,\nsuch as clinical notes or weather narratives, making cross-modal retrieval\nessential not only for downstream tasks but also for developing robust\ntime-series foundation models by retrieval-augmented generation (RAG). Despite\nthe increasing demand, time-series retrieval remains largely underexplored.\nExisting methods often lack semantic grounding, struggle to align heterogeneous\nmodalities, and have limited capacity for handling multi-channel signals. To\naddress this gap, we propose TRACE, a generic multimodal retriever that grounds\ntime-series embeddings in aligned textual context. TRACE enables fine-grained\nchannel-level alignment and employs hard negative mining to facilitate\nsemantically meaningful retrieval. It supports flexible cross-modal retrieval\nmodes, including Text-to-Timeseries and Timeseries-to-Text, effectively linking\nlinguistic descriptions with complex temporal patterns. By retrieving\nsemantically relevant pairs, TRACE enriches downstream models with informative\ncontext, leading to improved predictive accuracy and interpretability. Beyond a\nstatic retrieval engine, TRACE also serves as a powerful standalone encoder,\nwith lightweight task-specific tuning that refines context-aware\nrepresentations while maintaining strong cross-modal alignment. These\nrepresentations achieve state-of-the-art performance on downstream forecasting\nand classification tasks. Extensive experiments across multiple domains\nhighlight its dual utility, as both an effective encoder for downstream\napplications and a general-purpose retriever to enhance time-series models.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09114v1", "AI": {"title_translation": "TRACE：在上下文中关联时间序列以实现多模态嵌入和检索", "tldr": "TRACE是一个多模态检索器，通过将时间序列嵌入与文本上下文对齐，实现时间序列的有效检索和表示，并在下游任务中表现出色。", "motivation": "动态数据（如天气、医疗、能源）的普遍存在，需要有效的时间序列数据解释和检索。现有方法缺乏语义关联，难以对齐异构模态，且处理多通道信号的能力有限，阻碍了时间序列基础模型的发展。", "method": "本文提出了TRACE，一个通用的多模态检索器，它将时间序列嵌入与对齐的文本上下文关联。TRACE实现了细粒度的通道级对齐，并采用困难负样本挖掘来促进语义有意义的检索。它支持文本到时间序列和时间序列到文本的灵活跨模态检索模式。此外，TRACE也可作为强大的独立编码器，通过轻量级的任务特定调优来改进上下文感知表示。", "result": "TRACE在下游预测和分类任务中取得了最先进的性能。广泛的实验突显了其作为下游应用的有效编码器和增强时间序列模型的通用检索器的双重效用。", "conclusion": "TRACE通过语义关联的跨模态检索和强大的编码能力，有效解决了时间序列数据解释和检索的挑战，为时间序列基础模型和下游应用提供了有力的工具。", "translation": "天气、医疗保健和能源等领域动态数据的普遍存在，突显了对时间序列数据有效解释和检索日益增长的需求。这些数据本质上与特定领域上下文（例如临床笔记或天气叙述）相关联，使得跨模态检索不仅对下游任务至关重要，而且对于通过检索增强生成（RAG）开发强大的时间序列基础模型也至关重要。尽管需求日益增长，时间序列检索在很大程度上仍未得到充分探索。现有方法通常缺乏语义关联，难以对齐异构模态，并且处理多通道信号的能力有限。为了解决这一差距，我们提出了TRACE，一个通用的多模态检索器，它将时间序列嵌入与对齐的文本上下文关联起来。TRACE实现了细粒度的通道级对齐，并采用困难负样本挖掘来促进语义有意义的检索。它支持灵活的跨模态检索模式，包括文本到时间序列和时间序列到文本，有效地将语言描述与复杂的时间模式联系起来。通过检索语义相关的配对，TRACE用信息丰富的上下文丰富了下游模型，从而提高了预测准确性和可解释性。除了作为一个静态检索引擎，TRACE还可作为一个强大的独立编码器，通过轻量级的任务特定调优来改进上下文感知表示，同时保持强大的跨模态对齐。这些表示在下游预测和分类任务中取得了最先进的性能。在多个领域进行的广泛实验突显了其双重效用，既是下游应用的有效编码器，又是增强时间序列模型的通用检索器。", "summary": "本文提出了TRACE，一个通用的多模态检索器，旨在解决时间序列数据解释和检索的挑战。TRACE通过将时间序列嵌入与文本上下文对齐，实现细粒度的通道级对齐和跨模态检索（Text-to-Timeseries和Timeseries-to-Text）。它采用困难负样本挖掘来增强语义关联。TRACE不仅作为检索器，还能作为强大的独立编码器，生成上下文感知表示，并在预测和分类等下游任务中达到最先进的性能，突显其在时间序列分析中的双重价值。", "keywords": "时间序列, 多模态嵌入, 跨模态检索, 上下文关联, 检索增强生成检索增强生成, 深度学习", "comments": "TRACE的创新之处在于其将时间序列与文本上下文进行语义关联和对齐的能力，解决了现有方法在异构模态对齐和多通道信号处理方面的不足。其作为通用检索器和独立编码器的双重功能，以及在检索增强生成（RAG）和下游任务中的潜力，使其在时间序列基础模型和应用领域具有重要意义。"}}
{"id": "2506.09876", "title": "Aucamp: An Underwater Camera-Based Multi-Robot Platform with Low-Cost, Distributed, and Robust Localization", "authors": ["Jisheng Xu", "Ding Lin", "Pangkit Fong", "Chongrong Fang", "Xiaoming Duan", "Jianping He"], "summary": "This paper introduces an underwater multi-robot platform, named Aucamp,\ncharacterized by cost-effective monocular-camera-based sensing, distributed\nprotocol and robust orientation control for localization. We utilize the\nclarity feature to measure the distance, present the monocular imaging model,\nand estimate the position of the target object. We achieve global positioning\nin our platform by designing a distributed update protocol. The distributed\nalgorithm enables the perception process to simultaneously cover a broader\nrange, and greatly improves the accuracy and robustness of the positioning.\nMoreover, the explicit dynamics model of the robot in our platform is obtained,\nbased on which, we propose a robust orientation control framework. The control\nsystem ensures that the platform maintains a balanced posture for each robot,\nthereby ensuring the stability of the localization system. The platform can\nswiftly recover from an forced unstable state to a stable horizontal posture.\nAdditionally, we conduct extensive experiments and application scenarios to\nevaluate the performance of our platform. The proposed new platform may provide\nsupport for extensive marine exploration by underwater sensor networks.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.09876v1", "AI": {"title_translation": "Aucamp：一种低成本、分布式、鲁棒定位的水下相机多机器人平台", "tldr": "本文介绍了一种名为Aucamp的水下多机器人平台，它利用低成本单目相机、分布式协议和鲁棒姿态控制实现定位，提高了定位的准确性和鲁棒性，并能快速恢复稳定姿态，有望支持海洋探索。", "motivation": "为了支持广泛的海洋探索，需要一种低成本、分布式且鲁棒的水下多机器人平台。", "method": "该平台名为Aucamp，采用成本效益高的单目相机进行感知，利用清晰度特征测量距离，并基于单目成像模型估计目标物体位置。通过设计分布式更新协议实现全局定位，并利用机器人明确的动力学模型提出鲁棒的姿态控制框架，以确保平台稳定性。", "result": "该分布式算法显著提高了定位的准确性和鲁棒性，同时扩大了感知范围。控制系统确保每个机器人保持平衡姿态，保障定位系统的稳定性，并能从强制不稳定状态迅速恢复到稳定的水平姿态。平台性能已通过大量实验和应用场景进行评估。", "conclusion": "所提出的新平台有望通过水下传感器网络为广泛的海洋探索提供支持。", "translation": "本文介绍了一种名为Aucamp的水下多机器人平台，其特点是基于成本效益高的单目相机感知、分布式协议和鲁棒的姿态控制，用于定位。我们利用清晰度特征测量距离，提出了单目成像模型，并估计目标物体的位置。我们通过设计分布式更新协议，在我们的平台中实现了全局定位。分布式算法使感知过程能够同时覆盖更广的范围，并大大提高了定位的准确性和鲁棒性。此外，我们获得了平台中机器人的明确动力学模型，并在此基础上提出了一个鲁棒的姿态控制框架。控制系统确保平台为每个机器人保持平衡姿态，从而确保定位系统的稳定性。该平台可以从强制不稳定状态迅速恢复到稳定的水平姿态。此外，我们还进行了大量的实验和应用场景来评估我们平台的性能。所提出的新平台有望通过水下传感器网络为广泛的海洋探索提供支持。", "summary": "本文介绍了一种名为Aucamp的水下多机器人平台，该平台通过低成本单目相机实现感知，并结合分布式协议和鲁棒姿态控制进行定位。它利用清晰度特征进行距离测量和目标定位，并通过分布式更新协议实现全局定位，显著提升了定位的准确性和鲁棒性。此外，平台设计了鲁棒的姿态控制系统，确保机器人能保持平衡并快速恢复稳定。该平台已通过实验验证，旨在为海洋探索提供支持。", "keywords": "水下多机器人平台, 定位, 单目相机, 分布式协议, 姿态控制", "comments": "该论文的创新点在于提出了一个低成本、分布式且鲁棒的水下多机器人定位平台。通过结合单目相机、分布式算法和鲁棒姿态控制，解决了水下环境定位的挑战，特别是在成本和鲁棒性方面表现突出，对于水下传感器网络和海洋探索具有重要意义。"}}
{"id": "2506.09830", "title": "Machine Learning-based quadratic closures for non-intrusive Reduced Order Models", "authors": ["Gabriele Codega", "Anna Ivagnes", "Nicola Demo", "Gianluigi Rozza"], "summary": "In the present work, we introduce a data-driven approach to enhance the\naccuracy of non-intrusive Reduced Order Models (ROMs). In particular, we focus\non ROMs built using Proper Orthogonal Decomposition (POD) in an under-resolved\nand marginally-resolved regime, i.e. when the number of modes employed is not\nenough to capture the system dynamics. We propose a method to re-introduce the\ncontribution of neglected modes through a quadratic correction term, given by\nthe action of a quadratic operator on the POD coefficients. Differently from\nthe state-of-the-art methodologies, where the operator is learned via\nleast-squares optimisation, we propose to parametrise the operator by a\nMulti-Input Operators Network (MIONet). This way, we are able to build models\nwith higher generalisation capabilities, where the operator itself is\ncontinuous in space -- thus agnostic of the domain discretisation -- and\nparameter-dependent. We test our model on two standard benchmarks in fluid\ndynamics and show that the correction term improves the accuracy of standard\nPOD-based ROMs.", "comment": "18 pages, 8 figures", "cate": "math.NA", "url": "http://arxiv.org/abs/2506.09830v1", "AI": {"title_translation": "基于机器学习的非侵入式降阶模型二次闭合", "tldr": "该研究提出了一种基于机器学习的二次闭合方法，通过多输入算子网络（MIONet）为非侵入式降阶模型（ROMs）引入二次校正项，以提高在模态不足情况下的准确性，并在流体动力学基准测试中验证了其有效性。", "motivation": "当本征正交分解（POD）构建的降阶模型（ROMs）所使用的模态数量不足以捕捉系统动力学时（即欠解析或边缘解析状态），模型的准确性会下降。本研究旨在通过引入一种数据驱动方法来提高非侵入式ROMs的准确性。", "method": "该研究提出了一种数据驱动方法，通过引入一个二次校正项来重新引入被忽略模态的贡献。该校正项由一个二次算子作用于POD系数产生。与现有通过最小二乘优化学习算子的方法不同，该研究通过多输入算子网络（MIONet）来参数化该二次算子，从而构建出具有更高泛化能力、空间连续性且与域离散化无关的参数依赖模型。", "result": "该模型在流体动力学的两个标准基准测试中进行了验证，结果表明所提出的二次校正项显著提高了标准基于POD的降阶模型（ROMs）的准确性。", "conclusion": "基于机器学习的二次闭合方法，特别是使用MIONet参数化校正算子，能有效提高非侵入式POD降阶模型的准确性和泛化能力，尤其是在欠解析场景下。", "translation": "在当前的工作中，我们引入了一种数据驱动方法来提高非侵入式降阶模型（ROMs）的准确性。特别是，我们关注在欠解析和边缘解析状态下使用本征正交分解（POD）构建的ROMs，即当所使用的模态数量不足以捕捉系统动力学时。我们提出了一种方法，通过二次校正项重新引入被忽略模态的贡献，该校正项由二次算子作用于POD系数产生。与通过最小二乘优化学习算子的现有方法不同，我们提出通过多输入算子网络（MIONet）来参数化该算子。通过这种方式，我们能够构建具有更高泛化能力的模型，其中算子本身在空间上是连续的——因此与域离散化无关——并且是参数依赖的。我们在流体动力学的两个标准基准上测试了我们的模型，结果表明校正项提高了标准基于POD的ROMs的准确性。", "summary": "本文提出了一种数据驱动方法，旨在提高非侵入式降阶模型（ROMs）的准确性，尤其是在模态不足的欠解析状态下基于本征正交分解（POD）构建的模型。核心创新在于引入一个二次校正项来弥补被忽略模态的贡献。与传统最小二乘优化不同，该方法使用多输入算子网络（MIONet）来参数化二次算子，从而使模型具有更强的泛化能力、空间连续性以及与域离散化无关的特性。在流体动力学基准测试中，该方法成功地提升了标准POD-ROMs的准确性。", "keywords": "降阶模型, 机器学习, 本征正交分解, 二次闭合, MIONet", "comments": "该论文提出了一种创新方法，通过引入基于机器学习的二次闭合来提高降阶模型（ROMs）的准确性，尤其是在具有挑战性的欠解析场景中。使用多输入算子网络（MIONet）来参数化二次闭合算子是相对于传统最小二乘方法的一项重大进步，它提供了更好的泛化能力和空间连续性。这对于在计算成本高昂的复杂系统中进行高效建模具有广泛的潜在影响。"}}
{"id": "2506.09227", "title": "SoK: Machine Unlearning for Large Language Models", "authors": ["Jie Ren", "Yue Xing", "Yingqian Cui", "Charu C. Aggarwal", "Hui Liu"], "summary": "Large language model (LLM) unlearning has become a critical topic in machine\nlearning, aiming to eliminate the influence of specific training data or\nknowledge without retraining the model from scratch. A variety of techniques\nhave been proposed, including Gradient Ascent, model editing, and re-steering\nhidden representations. While existing surveys often organize these methods by\ntheir technical characteristics, such classifications tend to overlook a more\nfundamental dimension: the underlying intention of unlearning--whether it seeks\nto truly remove internal knowledge or merely suppress its behavioral effects.\nIn this SoK paper, we propose a new taxonomy based on this intention-oriented\nperspective. Building on this taxonomy, we make three key contributions. First,\nwe revisit recent findings suggesting that many removal methods may\nfunctionally behave like suppression, and explore whether true removal is\nnecessary or achievable. Second, we survey existing evaluation strategies,\nidentify limitations in current metrics and benchmarks, and suggest directions\nfor developing more reliable and intention-aligned evaluations. Third, we\nhighlight practical challenges--such as scalability and support for sequential\nunlearning--that currently hinder the broader deployment of unlearning methods.\nIn summary, this work offers a comprehensive framework for understanding and\nadvancing unlearning in generative AI, aiming to support future research and\nguide policy decisions around data removal and privacy.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09227v1", "AI": {"title_translation": "SoK: 大语言模型机器遗忘", "tldr": "这篇SoK论文为大语言模型（LLM）的机器遗忘提出了一种新的意图导向分类法，探讨了真正移除与抑制的争论，调查了评估策略，并强调了实际挑战，为未来的研究和政策提供了全面的框架。", "motivation": "大语言模型（LLM）的遗忘是消除特定训练数据影响而无需从头训练的关键课题。现有研究忽略了遗忘的根本意图（真正移除与抑制）。本论文旨在填补这一空白。", "method": "本论文提出了一种新的意图导向的遗忘方法分类法。在此分类法的基础上，重新审视了移除与抑制的功能行为，调查了现有的评估策略，并强调了实际部署中的挑战。这是一篇“知识系统化”（SoK）论文。", "result": "1. 提出了一种新的大语言模型遗忘的意图导向分类法。2. 重新审视了移除方法是否表现为抑制的发现，并探讨了真正移除的必要性或可实现性。3. 调查了现有评估策略，指出了当前度量和基准的局限性，并提出了开发更可靠、更符合意图的评估方向。4. 强调了目前阻碍遗忘方法更广泛部署的实际挑战（如可扩展性和对顺序遗忘的支持）。", "conclusion": "这项工作为理解和推进生成式AI中的遗忘提供了一个全面的框架，旨在支持未来的研究并指导围绕数据移除和隐私的政策决策。", "translation": "大语言模型（LLM）的遗忘已成为机器学习中的一个关键课题，旨在在不从头开始重新训练模型的情况下，消除特定训练数据或知识的影响。人们提出了各种技术，包括梯度上升、模型编辑和重定向隐藏表示。尽管现有调查通常根据技术特征组织这些方法，但此类分类往往忽略了一个更基本的维度：遗忘的潜在意图——它是寻求真正移除内部知识，还是仅仅抑制其行为效果。在这篇SoK论文中，我们基于这种意图导向的视角提出了一种新的分类法。在此分类法的基础上，我们做出了三个关键贡献。首先，我们重新审视了最近的研究发现，这些发现表明许多移除方法在功能上可能表现为抑制，并探讨了真正的移除是否必要或可实现。其次，我们调查了现有的评估策略，识别了当前度量和基准的局限性，并提出了开发更可靠、更符合意图的评估方向。第三，我们强调了目前阻碍遗忘方法更广泛部署的实际挑战——例如可扩展性和对顺序遗忘的支持。总而言之，这项工作为理解和推进生成式AI中的遗忘提供了一个全面的框架，旨在支持未来的研究并指导围绕数据移除和隐私的政策决策。", "summary": "这篇SoK论文探讨了大语言模型（LLM）机器遗忘的关键课题。它引入了一种新颖的意图导向分类法，区分了真正的知识移除和行为抑制。在此基础上，论文提出了三项主要贡献：重新审视了移除方法的功能行为，调查了当前的评估策略并指出了其局限性，并强调了可扩展性等实际部署挑战。这项工作为推进生成式AI中的遗忘提供了一个全面的框架，旨在指导未来的研究和数据隐私政策。", "keywords": "机器遗忘, 大语言模型, 数据隐私, 分类法, 生成式AI", "comments": "这篇论文的创新之处在于它为LLM遗忘引入了一种新的“意图导向”分类法，将重点从技术特征转向遗忘的根本目标。这种视角对于开发更有效、更真正保护隐私的遗忘方法至关重要。其重要性在于为快速发展且关键的领域提供了一个基础性框架，解决了理论问题（真正的移除与抑制）和实际挑战，这将极大地指导生成式AI和数据隐私领域的未来研究和政策制定。"}}
{"id": "2506.09583", "title": "VAULT: A Mobile Mapping System for ROS 2-based Autonomous Robots", "authors": ["Miguel Á. González-Santamarta", "Francisco J. Rodríguez-Lera", "Vicente Matellán-Olivera"], "summary": "Localization plays a crucial role in the navigation capabilities of\nautonomous robots, and while indoor environments can rely on wheel odometry and\n2D LiDAR-based mapping, outdoor settings such as agriculture and forestry,\npresent unique challenges that necessitate real-time localization and\nconsistent mapping. Addressing this need, this paper introduces the VAULT\nprototype, a ROS 2-based mobile mapping system (MMS) that combines various\nsensors to enable robust outdoor and indoor localization. The proposed solution\nharnesses the power of Global Navigation Satellite System (GNSS) data,\nvisual-inertial odometry (VIO), inertial measurement unit (IMU) data, and the\nExtended Kalman Filter (EKF) to generate reliable 3D odometry. To further\nenhance the localization accuracy, Visual SLAM (VSLAM) is employed, resulting\nin the creation of a comprehensive 3D point cloud map. By leveraging these\nsensor technologies and advanced algorithms, the prototype offers a\ncomprehensive solution for outdoor localization in autonomous mobile robots,\nenabling them to navigate and map their surroundings with confidence and\nprecision.", "comment": "15 pages, 5 figures, Submitted to WAF 2023: Workshop de Agentes\n  Fisicos", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.09583v1", "AI": {"title_translation": "VAULT：一个基于ROS 2的自主机器人移动测绘系统", "tldr": "本文介绍了VAULT，一个基于ROS 2的移动测绘系统，它利用GNSS、VIO、IMU、EKF和VSLAM实现自主机器人在室外和室内环境中的鲁棒3D定位和测绘。", "motivation": "自主机器人的导航能力中，定位至关重要。虽然室内环境可以依赖传统方法，但农业和林业等室外环境对实时定位和一致性测绘提出了独特挑战，需要更强大的解决方案。", "method": "VAULT原型是一个基于ROS 2的移动测绘系统（MMS），它结合了GNSS数据、视觉惯性里程计（VIO）、惯性测量单元（IMU）数据和扩展卡尔曼滤波器（EKF）来生成可靠的3D里程计。此外，还采用了视觉SLAM（VSLAM）来提高定位精度并创建全面的3D点云地图。", "result": "该原型为自主移动机器人提供了全面的室外定位解决方案，使其能够自信而精确地导航和测绘周围环境。它能生成可靠的3D里程计和全面的3D点云地图。", "conclusion": "VAULT原型通过结合多种传感器技术和先进算法，为自主移动机器人的室外定位和测绘提供了一个强大的解决方案。", "translation": "自主机器人的导航能力中，定位扮演着至关重要的角色。虽然室内环境可以依赖轮式里程计和基于2D LiDAR的测绘，但农业和林业等室外环境带来了独特的挑战，需要实时定位和一致的测绘。为满足这一需求，本文介绍了VAULT原型，一个基于ROS 2的移动测绘系统（MMS），它结合了各种传感器，以实现强大的室外和室内定位。所提出的解决方案利用全球导航卫星系统（GNSS）数据、视觉惯性里程计（VIO）、惯性测量单元（IMU）数据以及扩展卡尔曼滤波器（EKF）的功能来生成可靠的3D里程计。为了进一步提高定位精度，还采用了视觉SLAM（VSLAM），从而创建了一个全面的3D点云地图。通过利用这些传感器技术和先进算法，该原型为自主移动机器人的室外定位提供了全面的解决方案，使它们能够自信而精确地导航和测绘周围环境。", "summary": "本文介绍了VAULT，一个基于ROS 2的移动测绘系统，旨在为自主机器人在复杂室外和室内环境中提供鲁棒定位。该系统整合了GNSS、VIO、IMU和EKF以生成可靠的3D里程计，并通过VSLAM进一步增强定位精度和创建3D点云地图，从而解决了实时定位和一致性测绘的挑战。", "keywords": "移动测绘系统, ROS 2, 自主机器人, 定位, SLAM, GNSS, VIO", "comments": "该论文解决了自主机器人在室外鲁棒定位的关键需求，这是一个具有挑战性的问题。结合多种传感器模态（GNSS、VIO、IMU）和先进滤波（EKF）以及VSLAM进行3D测绘是一种可靠的方法。使用ROS 2表明该系统具有实用性和可扩展性。"}}
{"id": "2506.09343", "title": "CheckManual: A New Challenge and Benchmark for Manual-based Appliance Manipulation", "authors": ["Yuxing Long", "Jiyao Zhang", "Mingjie Pan", "Tianshu Wu", "Taewhan Kim", "Hao Dong"], "summary": "Correct use of electrical appliances has significantly improved human life\nquality. Unlike simple tools that can be manipulated with common sense,\ndifferent parts of electrical appliances have specific functions defined by\nmanufacturers. If we want the robot to heat bread by microwave, we should\nenable them to review the microwave manual first. From the manual, it can learn\nabout component functions, interaction methods, and representative task steps\nabout appliances. However, previous manual-related works remain limited to\nquestion-answering tasks while existing manipulation researchers ignore the\nmanual's important role and fail to comprehend multi-page manuals. In this\npaper, we propose the first manual-based appliance manipulation benchmark\nCheckManual. Specifically, we design a large model-assisted human-revised data\ngeneration pipeline to create manuals based on CAD appliance models. With these\nmanuals, we establish novel manual-based manipulation challenges, metrics, and\nsimulator environments for model performance evaluation. Furthermore, we\npropose the first manual-based manipulation planning model ManualPlan to set up\na group of baselines for the CheckManual benchmark.", "comment": "CVPR 2025 Highlight", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09343v1", "AI": {"title_translation": "CheckManual：一项基于说明书的家电操作新挑战与基准", "tldr": "本文提出了CheckManual，这是首个基于说明书的家电操作基准，旨在使机器人能够通过理解说明书来操作复杂电器。同时，还提出了ManualPlan作为基线模型。", "motivation": "机器人操作电器需要理解其复杂功能和操作步骤，而这些信息通常在说明书中详细定义。现有研究要么局限于说明书问答，要么忽略说明书在操作中的重要作用，且未能处理多页说明书，这限制了机器人自主操作复杂电器的能力。", "method": "本文提出了首个基于说明书的家电操作基准CheckManual。具体方法包括：1) 设计一个大型模型辅助人工修订的数据生成流程，基于CAD家电模型创建说明书。2) 利用这些说明书，建立新颖的基于说明书的操作挑战、评估指标和模拟器环境。3) 提出了首个基于说明书的操作规划模型ManualPlan，为CheckManual基准设置了一组基线。", "result": "成功建立了CheckManual基准，其中包含基于CAD模型生成的电器说明书、创新的操作挑战、评估指标和模拟器环境。同时，提出了ManualPlan模型，为CheckManual基准提供了一组有效的基线。", "conclusion": "本文成功提出了CheckManual，这是首个基于说明书的家电操作基准，并通过引入ManualPlan模型为该基准设定了基线，有效解决了机器人通过理解多页说明书进行复杂电器操作的挑战。", "translation": "正确使用电器显著提高了人类生活质量。与可以用常识操作的简单工具不同，电器的不同部分具有制造商定义的特定功能。如果我们要让机器人用微波炉加热面包，我们应该让它们首先查阅微波炉说明书。从说明书中，机器人可以了解部件功能、交互方法以及关于电器的代表性任务步骤。然而，以前与说明书相关的工作仍然局限于问答任务，而现有的操作研究人员则忽略了说明书的重要作用，未能理解多页说明书。在本文中，我们提出了第一个基于说明书的家电操作基准CheckManual。具体来说，我们设计了一个大型模型辅助人工修订的数据生成流程，以基于CAD家电模型创建说明书。利用这些说明书，我们建立了新颖的基于说明书的操作挑战、指标和模拟器环境，用于模型性能评估。此外，我们提出了第一个基于说明书的操作规划模型ManualPlan，为CheckManual基准设置了一组基线。", "summary": "本文提出了CheckManual，这是首个专注于机器人通过理解多页说明书进行家电操作的基准。为构建此基准，研究团队开发了一个大型模型辅助的数据生成流程，基于CAD模型创建电器说明书，并在此基础上设计了新的操作挑战、评估指标和模拟器环境。此外，本文还引入了ManualPlan，作为首个基于说明书的操作规划模型，为CheckManual基准提供了基线，旨在推动机器人利用说明书知识执行复杂任务的能力。", "keywords": "家电操作, 说明书理解, 机器人基准, CheckManual, ManualPlan", "comments": "这项工作具有重要的创新性和实用价值。它首次将复杂的说明书理解与机器人操作任务相结合，填补了现有机器人操作研究中对多页说明书理解的空白。CheckManual基准和ManualPlan模型的提出，为未来开发更智能、更自主的机器人提供了坚实的基础，使其能够像人类一样从指导手册中学习并执行复杂任务。这项工作有望极大地推动机器人领域在真实世界应用中的发展。"}}
{"id": "2506.09163", "title": "Scalable Spatiotemporal Inference with Biased Scan Attention Transformer Neural Processes", "authors": ["Daniel Jenson", "Jhonathan Navott", "Piotr Grynfelder", "Mengyan Zhang", "Makkunda Sharma", "Elizaveta Semenova", "Seth Flaxman"], "summary": "Neural Processes (NPs) are a rapidly evolving class of models designed to\ndirectly model the posterior predictive distribution of stochastic processes.\nWhile early architectures were developed primarily as a scalable alternative to\nGaussian Processes (GPs), modern NPs tackle far more complex and data hungry\napplications spanning geology, epidemiology, climate, and robotics. These\napplications have placed increasing pressure on the scalability of these\nmodels, with many architectures compromising accuracy for scalability. In this\npaper, we demonstrate that this tradeoff is often unnecessary, particularly\nwhen modeling fully or partially translation invariant processes. We propose a\nversatile new architecture, the Biased Scan Attention Transformer Neural\nProcess (BSA-TNP), which introduces Kernel Regression Blocks (KRBlocks),\ngroup-invariant attention biases, and memory-efficient Biased Scan Attention\n(BSA). BSA-TNP is able to: (1) match or exceed the accuracy of the best models\nwhile often training in a fraction of the time, (2) exhibit translation\ninvariance, enabling learning at multiple resolutions simultaneously, (3)\ntransparently model processes that evolve in both space and time, (4) support\nhigh dimensional fixed effects, and (5) scale gracefully -- running inference\nwith over 1M test points with 100K context points in under a minute on a single\n24GB GPU.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09163v1", "AI": {"title_translation": "基于偏置扫描注意力Transformer神经过程的可扩展时空推断", "tldr": "神经过程（NPs）在可扩展性方面面临挑战，通常以牺牲准确性为代价。本文提出了偏置扫描注意力Transformer神经过程（BSA-TNP），这是一种新架构，可在保持高准确性的同时实现卓越的可扩展性，尤其适用于时空推断和翻译不变过程。", "motivation": "早期的神经过程（NPs）模型虽然是高斯过程（GPs）的可扩展替代品，但现代应用（如地质学、流行病学、气候和机器人学）对NPs的可扩展性提出了更高要求，许多现有架构为了可扩展性而牺牲了准确性。本文旨在证明这种权衡通常是不必要的，特别是在建模完全或部分翻译不变过程时。", "method": "本文提出了一种多功能的新架构：偏置扫描注意力Transformer神经过程（BSA-TNP）。该模型引入了核回归块（KRBlocks）、群不变注意力偏置以及内存高效的偏置扫描注意力（BSA）机制。", "result": "BSA-TNP能够：(1) 匹配或超越最佳模型的准确性，同时通常只需很短的训练时间；(2) 展现出翻译不变性，支持同时在多个分辨率下进行学习；(3) 透明地建模在空间和时间上都演变的过程；(4) 支持高维固定效应；(5) 优雅地进行扩展——在单个24GB GPU上，以不到一分钟的时间完成对超过100万个测试点和10万个上下文点的推断。", "conclusion": "本文提出的偏置扫描注意力Transformer神经过程（BSA-TNP）有效解决了神经过程在可扩展性和准确性之间的权衡问题，特别是在处理翻译不变的时空推断任务时，展现出卓越的性能和效率。", "translation": "神经过程（NPs）是一类快速发展的模型，旨在直接建模随机过程的后验预测分布。虽然早期的架构主要是作为高斯过程（GPs）的可扩展替代品而开发的，但现代的NPs应对着更复杂、数据需求更大的应用，涵盖地质学、流行病学、气候和机器人学等领域。这些应用对这些模型的可扩展性提出了越来越大的压力，许多架构为了可扩展性而牺牲了准确性。在本文中，我们证明这种权衡通常是不必要的，特别是在建模完全或部分翻译不变过程时。我们提出了一种多功能的新架构——偏置扫描注意力Transformer神经过程（BSA-TNP），它引入了核回归块（KRBlocks）、群不变注意力偏置和内存高效的偏置扫描注意力（BSA）。BSA-TNP能够：(1) 匹配或超越最佳模型的准确性，同时通常只需很短的训练时间；(2) 展现出翻译不变性，支持同时在多个分辨率下进行学习；(3) 透明地建模在空间和时间上都演变的过程；(4) 支持高维固定效应；(5) 优雅地进行扩展——在单个24GB GPU上，以不到一分钟的时间完成对超过100万个测试点和10万个上下文点的推断。", "summary": "神经过程（NPs）在处理大规模应用时面临可扩展性挑战，往往需要牺牲准确性。本文提出了一种新颖的架构——偏置扫描注意力Transformer神经过程（BSA-TNP），旨在克服这一限制，特别适用于翻译不变过程。BSA-TNP通过集成核回归块、群不变注意力偏置和偏置扫描注意力，实现了高准确度、更快的训练速度、翻译不变性、透明的时空建模能力，并支持高维固定效应。该模型展现了卓越的可扩展性，能够高效地处理大规模数据集。", "keywords": "神经过程, 时空推断, 可扩展性, Transformer, 注意力", "comments": "本文的创新之处在于通过引入新颖的注意力机制（偏置扫描注意力）和回归块（核回归块），有效解决了神经过程（NPs）在处理大规模时空数据时面临的准确性与可扩展性之间的关键权衡问题。其在单个GPU上实现百万级测试点和十万级上下文点的高效推断能力，凸显了该模型在实际应用中的重要性和潜力。"}}
{"id": "2506.09363", "title": "SAGE: Exploring the Boundaries of Unsafe Concept Domain with Semantic-Augment Erasing", "authors": ["Hongguang Zhu", "Yunchao Wei", "Mengyu Wang", "Siyu Jiao", "Yan Fang", "Jiannan Huang", "Yao Zhao"], "summary": "Diffusion models (DMs) have achieved significant progress in text-to-image\ngeneration. However, the inevitable inclusion of sensitive information during\npre-training poses safety risks, such as unsafe content generation and\ncopyright infringement. Concept erasing finetunes weights to unlearn\nundesirable concepts, and has emerged as a promising solution. However,\nexisting methods treat unsafe concept as a fixed word and repeatedly erase it,\ntrapping DMs in ``word concept abyss'', which prevents generalized\nconcept-related erasing. To escape this abyss, we introduce semantic-augment\nerasing which transforms concept word erasure into concept domain erasure by\nthe cyclic self-check and self-erasure. It efficiently explores and unlearns\nthe boundary representation of concept domain through semantic spatial\nrelationships between original and training DMs, without requiring additional\npreprocessed data. Meanwhile, to mitigate the retention degradation of\nirrelevant concepts while erasing unsafe concepts, we further propose the\nglobal-local collaborative retention mechanism that combines global semantic\nrelationship alignment with local predicted noise preservation, effectively\nexpanding the retentive receptive field for irrelevant concepts. We name our\nmethod SAGE, and extensive experiments demonstrate the comprehensive\nsuperiority of SAGE compared with other methods in the safe generation of DMs.\nThe code and weights will be open-sourced at\nhttps://github.com/KevinLight831/SAGE.", "comment": "Under review", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09363v1", "AI": {"title_translation": "SAGE：利用语义增强擦除探索不安全概念领域的边界", "tldr": "SAGE通过语义增强擦除和全局局部协作保留机制，解决了扩散模型中不安全内容生成的问题，提高了安全生成能力。", "motivation": "扩散模型在文生图方面取得了显著进展，但预训练中不可避免地包含敏感信息，导致不安全内容生成和版权侵犯等安全风险。现有概念擦除方法将不安全概念视为固定词汇并重复擦除，陷入“词汇概念深渊”，阻碍了广义概念相关擦除。", "method": "本文提出SAGE方法，核心是“语义增强擦除”，通过循环自检和自擦除将概念词擦除转化为概念领域擦除，高效探索并解学概念领域的边界表示，无需额外预处理数据。同时，为减轻擦除不安全概念时无关概念的保留退化，提出“全局局部协作保留机制”，结合全局语义关系对齐和局部预测噪声保留，有效扩展无关概念的保留感受野。", "result": "广泛的实验证明SAGE在扩散模型的安全生成方面相比其他方法具有全面优势。", "conclusion": "SAGE方法能够有效解决扩散模型中不安全内容生成的问题，显著提高其安全生成能力。", "translation": "扩散模型（DMs）在文本到图像生成方面取得了显著进展。然而，预训练过程中不可避免地包含敏感信息带来了安全风险，例如不安全内容生成和版权侵犯。概念擦除通过微调权重来“忘却”不期望的概念，已成为一种有前景的解决方案。然而，现有方法将不安全概念视为固定词汇并重复擦除，使DMs陷入“词汇概念深渊”，这阻碍了广义的概念相关擦除。为了摆脱这个深渊，我们引入了语义增强擦除，它通过循环自检和自擦除将概念词擦除转化为概念领域擦除。它通过原始DM和训练DM之间的语义空间关系，高效地探索并解学概念领域的边界表示，而无需额外的预处理数据。同时，为了减轻在擦除不安全概念时无关概念的保留退化，我们进一步提出了全局局部协作保留机制，它结合了全局语义关系对齐和局部预测噪声保留，有效扩展了无关概念的保留感受野。我们将我们的方法命名为SAGE，广泛的实验证明SAGE在DMs的安全生成方面相比其他方法具有全面的优越性。代码和权重将在https://github.com/KevinLight831/SAGE 开源。", "summary": "扩散模型在文本到图像生成中面临安全风险，现有概念擦除方法存在局限性。本文提出SAGE，通过“语义增强擦除”将概念词擦除扩展到概念领域擦除，并引入“全局局部协作保留机制”以维持无关概念的质量。实验证明SAGE在扩散模型的安全生成方面表现出全面优势。", "keywords": "扩散模型, 概念擦除, 安全生成, 语义增强, 全局局部协作", "comments": "本文的创新点在于提出了“语义增强擦除”来解决现有概念擦除方法在处理不安全概念时面临的“词汇概念深渊”问题，实现了更广义的概念领域擦除。同时，“全局局部协作保留机制”有效缓解了擦除过程中无关概念的退化，提高了模型的实用性。SAGE为扩散模型的安全应用提供了新的思路和有效解决方案。"}}
{"id": "2506.09588", "title": "Attention-Based Map Encoding for Learning Generalized Legged Locomotion", "authors": ["Junzhe He", "Chong Zhang", "Fabian Jenelten", "Ruben Grandia", "Moritz BÄcher", "Marco Hutter"], "summary": "Dynamic locomotion of legged robots is a critical yet challenging topic in\nexpanding the operational range of mobile robots. It requires precise planning\nwhen possible footholds are sparse, robustness against uncertainties and\ndisturbances, and generalizability across diverse terrains. While traditional\nmodel-based controllers excel at planning on complex terrains, they struggle\nwith real-world uncertainties. Learning-based controllers offer robustness to\nsuch uncertainties but often lack precision on terrains with sparse steppable\nareas. Hybrid methods achieve enhanced robustness on sparse terrains by\ncombining both methods but are computationally demanding and constrained by the\ninherent limitations of model-based planners. To achieve generalized legged\nlocomotion on diverse terrains while preserving the robustness of\nlearning-based controllers, this paper proposes to learn an attention-based map\nencoding conditioned on robot proprioception, which is trained as part of the\nend-to-end controller using reinforcement learning. We show that the network\nlearns to focus on steppable areas for future footholds when the robot\ndynamically navigates diverse and challenging terrains. We synthesize behaviors\nthat exhibit robustness against uncertainties while enabling precise and agile\ntraversal of sparse terrains. Additionally, our method offers a way to\ninterpret the topographical perception of a neural network. We have trained two\ncontrollers for a 12-DoF quadrupedal robot and a 23-DoF humanoid robot\nrespectively and tested the resulting controllers in the real world under\nvarious challenging indoor and outdoor scenarios, including ones unseen during\ntraining.", "comment": "Original draft prior to peer review. Significant revisions and new\n  materials are expected after formal publication release", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.09588v1", "AI": {"title_translation": "基于注意力的地图编码，用于学习广义腿式运动", "tldr": "本文提出了一种基于注意力的地图编码方法，通过强化学习训练，旨在实现腿式机器人在多样化和稀疏地形上的鲁棒且精确的广义运动，并在四足和人形机器人上进行了真实世界测试。", "motivation": "腿式机器人的动态运动在扩展其操作范围方面至关重要但具有挑战性，需要精确规划、鲁棒性以及在不同地形上的泛化能力。传统的基于模型控制器在复杂地形规划方面表现出色，但在现实世界的不确定性中表现不佳。基于学习的控制器对不确定性具有鲁棒性，但在可踩踏区域稀疏的地形上缺乏精度。混合方法虽然增强了鲁棒性，但计算成本高且受限于模型规划器的固有局限性。因此，需要一种方法在保持学习控制器鲁棒性的同时，实现广义腿式运动。", "method": "本文提出学习一种基于注意力的地图编码，该编码以机器人本体感知为条件，并作为端到端控制器的一部分使用强化学习进行训练。该网络学习在机器人动态导航各种具有挑战性的地形时，专注于未来落脚点的可踩踏区域。", "result": "该方法合成了对不确定性表现出鲁棒性的行为，同时实现了稀疏地形的精确灵活穿越。此外，我们的方法提供了一种解释神经网络地形感知的方式。我们分别为一个12自由度四足机器人和一个23自由度人形机器人训练了两个控制器，并在各种具有挑战性的室内和室外场景中（包括训练期间未见的场景）在现实世界中测试了所得控制器。", "conclusion": "本文成功展示了一种基于注意力地图编码的学习方法，该方法实现了在多样化和挑战性地形上鲁棒且精确的广义腿式运动，克服了现有方法的局限性，并提供了可解释性。", "translation": "腿式机器人的动态运动是扩展移动机器人操作范围的关键但具有挑战性的话题。当可能的落脚点稀疏时，它需要精确的规划，对不确定性和干扰具有鲁棒性，并且能够泛化到各种地形。虽然传统的基于模型的控制器擅长在复杂地形上进行规划，但它们难以应对现实世界中的不确定性。基于学习的控制器对这些不确定性具有鲁棒性，但通常在可踩踏区域稀疏的地形上缺乏精度。混合方法通过结合两种方法在稀疏地形上实现了增强的鲁棒性，但计算量大，并且受到基于模型规划器固有局限性的限制。为了在保持基于学习的控制器鲁棒性的同时，在各种地形上实现广义腿式运动，本文提出学习一种基于注意力的地图编码，该编码以机器人本体感知为条件，并作为端到端控制器的一部分使用强化学习进行训练。我们表明，当机器人在动态导航各种具有挑战性的地形时，网络会学习关注未来落脚点的可踩踏区域。我们合成的行为对不确定性表现出鲁棒性，同时能够精确灵活地穿越稀疏地形。此外，我们的方法提供了一种解释神经网络地形感知的方式。我们分别为一个12自由度四足机器人和一个23自由度人形机器人训练了两个控制器，并在各种具有挑战性的室内和室外场景中（包括训练期间未见的场景）在现实世界中测试了所得控制器。", "summary": "本文介绍了一种基于注意力的地图编码方法，用于学习广义腿式运动。通过将这种编码集成到端到端强化学习控制器中，系统学会了在多样化和挑战性地形上识别最佳落脚点。这种方法克服了传统基于模型和纯粹基于学习方法的局限性，实现了在稀疏地形上的鲁棒和精确导航。该方法还提供了对神经网络地形感知的见解。在四足和人形机器人上的真实世界测试验证了其在各种场景（包括未见场景）中的有效性。", "keywords": "腿式运动, 注意力机制, 地图编码, 强化学习, 机器人导航", "comments": "该论文的创新之处在于在端到端强化学习框架内使用注意力机制进行地图编码，有效地结合了规划的精确性和学习的鲁棒性。这解决了腿式运动研究中的一个关键空白，实现了对多样化和挑战性环境的泛化。可解释性方面也是一个重要的贡献。"}}
{"id": "2506.09345", "title": "An Effective End-to-End Solution for Multimodal Action Recognition", "authors": ["Songping Wang", "Xiantao Hu", "Yueming Lyu", "Caifeng Shan"], "summary": "Recently, multimodal tasks have strongly advanced the field of action\nrecognition with their rich multimodal information. However, due to the\nscarcity of tri-modal data, research on tri-modal action recognition tasks\nfaces many challenges. To this end, we have proposed a comprehensive multimodal\naction recognition solution that effectively utilizes multimodal information.\nFirst, the existing data are transformed and expanded by optimizing data\nenhancement techniques to enlarge the training scale. At the same time, more\nRGB datasets are used to pre-train the backbone network, which is better\nadapted to the new task by means of transfer learning. Secondly, multimodal\nspatial features are extracted with the help of 2D CNNs and combined with the\nTemporal Shift Module (TSM) to achieve multimodal spatial-temporal feature\nextraction comparable to 3D CNNs and improve the computational efficiency. In\naddition, common prediction enhancement methods, such as Stochastic Weight\nAveraging (SWA), Ensemble and Test-Time augmentation (TTA), are used to\nintegrate the knowledge of models from different training periods of the same\narchitecture and different architectures, so as to predict the actions from\ndifferent perspectives and fully exploit the target information. Ultimately, we\nachieved the Top-1 accuracy of 99% and the Top-5 accuracy of 100% on the\ncompetition leaderboard, demonstrating the superiority of our solution.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09345v1", "AI": {"title_translation": "一种有效的多模态动作识别端到端解决方案", "tldr": "本文提出了一种全面的多模态动作识别解决方案，通过数据增强、迁移学习、2D CNN结合TSM进行高效特征提取以及预测增强方法，解决了三模态数据稀缺带来的挑战，并在竞赛排行榜上取得了99%的Top-1准确率和100%的Top-5准确率。", "motivation": "由于三模态数据的稀缺性，三模态动作识别任务的研究面临诸多挑战。", "method": "首先，通过优化数据增强技术对现有数据进行转换和扩展，以扩大训练规模。同时，利用更多RGB数据集对骨干网络进行预训练，并通过迁移学习更好地适应新任务。其次，借助2D CNNs提取多模态空间特征，并结合时序位移模块（TSM）实现与3D CNNs媲美的多模态时空特征提取，同时提高计算效率。此外，还使用了随机权重平均（SWA）、集成（Ensemble）和测试时增强（TTA）等通用预测增强方法，以整合来自同一架构不同训练周期和不同架构的模型知识。", "result": "在竞赛排行榜上取得了99%的Top-1准确率和100%的Top-5准确率。", "conclusion": "本解决方案展示了其优越性。", "translation": "最近，多模态任务凭借其丰富的多模态信息极大地推动了动作识别领域的发展。然而，由于三模态数据的稀缺性，三模态动作识别任务的研究面临诸多挑战。为此，我们提出了一种全面的多模态动作识别解决方案，有效地利用了多模态信息。首先，通过优化数据增强技术对现有数据进行转换和扩展，以扩大训练规模。同时，利用更多RGB数据集对骨干网络进行预训练，并通过迁移学习更好地适应新任务。其次，借助2D CNNs提取多模态空间特征，并结合时序位移模块（TSM）实现与3D CNNs媲美的多模态时空特征提取，同时提高计算效率。此外，还使用了随机权重平均（SWA）、集成和测试时增强（TTA）等通用预测增强方法，以整合来自同一架构不同训练周期和不同架构的模型知识，从而从不同角度预测动作并充分利用目标信息。最终，我们在竞赛排行榜上取得了99%的Top-1准确率和100%的Top-5准确率，证明了我们解决方案的优越性。", "summary": "本文提出了一种针对多模态动作识别的端到端解决方案，旨在解决三模态数据稀缺的挑战。该方案通过数据增强和利用RGB数据集进行迁移学习来扩大训练规模和适应新任务。它还利用2D CNNs结合时序位移模块（TSM）进行高效的多模态时空特征提取，并采用随机权重平均（SWA）、集成和测试时增强（TTA）等预测增强方法来整合模型知识。实验结果显示，该方案在竞赛排行榜上取得了99%的Top-1准确率和100%的Top-5准确率，证明了其优越性。", "keywords": "多模态动作识别, 数据增强, 迁移学习, 时序位移模块, 预测增强", "comments": "该论文的创新之处在于提出了一个综合性的端到端解决方案，有效应对了三模态动作识别中数据稀缺和计算效率的挑战。通过结合数据增强、迁移学习、2D CNN与TSM的特征提取以及多种预测增强技术，该方案不仅提高了模型的准确性，还兼顾了计算效率，在实际应用中具有重要意义。"}}
{"id": "2506.09171", "title": "Improving LLM Agent Planning with In-Context Learning via Atomic Fact Augmentation and Lookahead Search", "authors": ["Samuel Holt", "Max Ruiz Luyten", "Thomas Pouplin", "Mihaela van der Schaar"], "summary": "Large Language Models (LLMs) are increasingly capable but often require\nsignificant guidance or extensive interaction history to perform effectively in\ncomplex, interactive environments. Existing methods may struggle with adapting\nto new information or efficiently utilizing past experiences for multi-step\nreasoning without fine-tuning. We introduce a novel LLM agent framework that\nenhances planning capabilities through in-context learning, facilitated by\natomic fact augmentation and a recursive lookahead search. Our agent learns to\nextract task-critical ``atomic facts'' from its interaction trajectories. These\nfacts dynamically augment the prompts provided to LLM-based components\nresponsible for action proposal, latent world model simulation, and state-value\nestimation. Planning is performed via a depth-limited lookahead search, where\nthe LLM simulates potential trajectories and evaluates their outcomes, guided\nby the accumulated facts and interaction history. This approach allows the\nagent to improve its understanding and decision-making online, leveraging its\nexperience to refine its behavior without weight updates. We provide a\ntheoretical motivation linking performance to the quality of fact-based\nabstraction and LLM simulation accuracy. Empirically, our agent demonstrates\nimproved performance and adaptability on challenging interactive tasks,\nachieving more optimal behavior as it accumulates experience, showcased in\ntasks such as TextFrozenLake and ALFWorld.", "comment": "9-page main paper, 1 figure. Accepted for an Oral presentation at the\n  First Workshop on Computer Use Agents (ICML 2025), Vancouver, Canada", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09171v1", "AI": {"title_translation": "通过原子事实增强和前瞻搜索改进LLM智能体的上下文学习规划", "tldr": "该论文提出了一种新的LLM智能体框架，通过原子事实增强和递归前瞻搜索，利用上下文学习来提升LLM在复杂交互环境中的规划能力，无需权重更新即可在线改进。", "motivation": "大型语言模型（LLMs）在复杂交互环境中，需要大量指导或交互历史才能有效表现，且现有方法难以在不进行微调的情况下适应新信息或有效利用过往经验进行多步推理。", "method": "本文引入了一种新的LLM智能体框架，通过原子事实增强和递归前瞻搜索来增强规划能力。该智能体从交互轨迹中提取“原子事实”，这些事实动态地增强用于行动提议、潜在世界模型模拟和状态价值估计的LLM组件的提示。规划通过深度受限的前瞻搜索进行，LLM模拟潜在轨迹并评估结果，由累积的事实和交互历史引导。", "result": "实验证明，该智能体在具有挑战性的交互任务（如TextFrozenLake和ALFWorld）上表现出改进的性能和适应性，并随着经验的积累实现更优化的行为。", "conclusion": "该方法使智能体能够在线改进其理解和决策，利用经验优化行为而无需更新权重。理论上，性能与基于事实的抽象质量和LLM模拟精度相关。", "translation": "大型语言模型（LLMs）的能力日益增强，但在复杂、交互式环境中有效执行通常需要大量的指导或广泛的交互历史。现有方法可能难以适应新信息，或在不进行微调的情况下有效利用过往经验进行多步推理。我们引入了一种新颖的LLM智能体框架，通过上下文学习，并借助原子事实增强和递归前瞻搜索，来提升规划能力。我们的智能体学习从其交互轨迹中提取任务关键的“原子事实”。这些事实动态地增强了提供给负责行动提议、潜在世界模型模拟和状态价值估计的基于LLM组件的提示。规划通过深度受限的前瞻搜索进行，其中LLM模拟潜在轨迹并评估其结果，并由累积的事实和交互历史引导。这种方法允许智能体在线改进其理解和决策，利用其经验来完善其行为而无需更新权重。我们提供了理论依据，将性能与基于事实的抽象质量和LLM模拟精度联系起来。经验上，我们的智能体在具有挑战性的交互任务中表现出改进的性能和适应性，随着经验的积累实现更优化的行为，这在TextFrozenLake和ALFWorld等任务中得到了展示。", "summary": "本文提出一种新颖的LLM智能体框架，旨在通过上下文学习提升其在复杂交互环境中的规划能力。该框架核心在于引入“原子事实增强”和“递归前瞻搜索”。智能体从交互中提取关键的原子事实，并用这些事实动态地增强LLM的提示，以辅助行动提议、世界模型模拟和价值估计。通过深度受限的前瞻搜索，LLM模拟潜在轨迹并评估结果，从而实现在线行为改进而无需模型权重更新。实验证明，该方法在交互式任务中显著提升了性能和适应性。", "keywords": "LLM智能体, 上下文学习, 规划, 原子事实增强, 前瞻搜索", "comments": "该论文提出了一种创新的方法来提高LLM智能体的在线学习和规划能力，其亮点在于无需微调即可通过上下文学习实现行为优化。原子事实增强和前瞻搜索的结合，有效地利用了LLM的推理能力进行自我改进。这种方法对于开发更自主、适应性强的LLM智能体具有重要意义，尤其是在动态和部分可观察的环境中。"}}
{"id": "2506.09259", "title": "Self-Anchored Attention Model for Sample-Efficient Classification of Prosocial Text Chat", "authors": ["Zhuofang Li", "Rafal Kocielnik", "Fereshteh Soltani", "Penphob", "Boonyarungsrit", "Animashree Anandkumar", "R. Michael Alvarez"], "summary": "Millions of players engage daily in competitive online games, communicating\nthrough in-game chat. Prior research has focused on detecting relatively small\nvolumes of toxic content using various Natural Language Processing (NLP)\ntechniques for the purpose of moderation. However, recent studies emphasize the\nimportance of detecting prosocial communication, which can be as crucial as\nidentifying toxic interactions. Recognizing prosocial behavior allows for its\nanalysis, rewarding, and promotion. Unlike toxicity, there are limited\ndatasets, models, and resources for identifying prosocial behaviors in\ngame-chat text. In this work, we employed unsupervised discovery combined with\ngame domain expert collaboration to identify and categorize prosocial player\nbehaviors from game chat. We further propose a novel Self-Anchored Attention\nModel (SAAM) which gives 7.9% improvement compared to the best existing\ntechnique. The approach utilizes the entire training set as \"anchors\" to help\nimprove model performance under the scarcity of training data. This approach\nled to the development of the first automated system for classifying prosocial\nbehaviors in in-game chats, particularly given the low-resource settings where\nlarge-scale labeled data is not available. Our methodology was applied to one\nof the most popular online gaming titles - Call of Duty(R): Modern\nWarfare(R)II, showcasing its effectiveness. This research is novel in applying\nNLP techniques to discover and classify prosocial behaviors in player in-game\nchat communication. It can help shift the focus of moderation from solely\npenalizing toxicity to actively encouraging positive interactions on online\nplatforms.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.09259v1", "AI": {"title_translation": "用于亲社会文本聊天样本高效分类的自锚定注意力模型", "tldr": "本文提出了一种新型的自锚定注意力模型（SAAM），旨在解决游戏聊天中亲社会文本分类的数据稀缺问题，并在低资源设置下实现了显著的性能提升。", "motivation": "现有研究主要关注检测有毒内容，但检测亲社会交流同样重要，有助于分析、奖励和推广积极行为。然而，与毒性检测不同，用于识别游戏聊天中亲社会行为的数据集、模型和资源非常有限。", "method": "本研究结合无监督发现和游戏领域专家协作来识别和分类游戏聊天中的亲社会玩家行为。进一步提出了一种新颖的自锚定注意力模型（SAAM），该模型利用整个训练集作为“锚点”来帮助在训练数据稀缺的情况下提高模型性能。", "result": "SAAM 比现有最佳技术提高了 7.9%。该方法开发了第一个用于分类游戏内聊天中亲社会行为的自动化系统，尤其适用于缺乏大规模标记数据的低资源设置。该方法已成功应用于《使命召唤：现代战争II》，并展示了其有效性。", "conclusion": "这项研究首次将NLP技术应用于发现和分类玩家游戏内聊天中的亲社会行为。它有助于将在线平台的审核重点从单纯惩罚毒性转向积极鼓励正向互动。", "translation": "数百万玩家每天参与竞技性在线游戏，并通过游戏内聊天进行交流。先前的研究主要集中于使用各种自然语言处理（NLP）技术检测相对少量有害内容以进行审核。然而，最近的研究强调了检测亲社会交流的重要性，这与识别有害互动同样关键。识别亲社会行为有助于对其进行分析、奖励和推广。与有害性不同，用于识别游戏聊天文本中亲社会行为的数据集、模型和资源有限。在这项工作中，我们结合无监督发现和游戏领域专家协作，识别并分类了游戏聊天中的亲社会玩家行为。我们进一步提出了一种新颖的自锚定注意力模型（SAAM），与现有最佳技术相比，该模型性能提高了7.9%。该方法利用整个训练集作为“锚点”来帮助在训练数据稀缺的情况下提高模型性能。这种方法促成了第一个用于分类游戏内聊天中亲社会行为的自动化系统的开发，特别是在缺乏大规模标记数据的低资源设置中。我们的方法应用于最受欢迎的在线游戏之一——《使命召唤®：现代战争®II》，展示了其有效性。这项研究在应用NLP技术发现和分类玩家游戏内聊天中的亲社会行为方面具有新颖性。它有助于将审核的重点从单纯惩罚有害行为转向积极鼓励在线平台上的积极互动。", "summary": "本文提出了一种新颖的自锚定注意力模型（SAAM），用于在游戏聊天中对亲社会文本进行高效分类，尤其是在标记数据稀缺的低资源环境下。该模型结合无监督发现和领域专家知识，利用整个训练集作为“锚点”来提升性能，并在《使命召唤：现代战争II》中表现出比现有最佳技术7.9%的改进。这项工作首次将NLP应用于游戏内亲社会行为的检测，旨在推动在线平台审核从惩罚毒性转向鼓励积极互动。", "keywords": "亲社会文本分类, 自锚定注意力模型, 样本高效, 游戏聊天, 自然语言处理", "comments": "本文的创新点在于提出了自锚定注意力模型（SAAM），有效解决了在亲社会行为检测中数据稀缺的问题，通过利用整个训练集作为“锚点”显著提升了模型性能。其重要性在于首次将NLP技术应用于游戏内亲社会行为的发现和分类，为在线平台从单一的负面内容审核转向积极鼓励正面互动提供了技术支持，具有重要的实践意义和潜在的社会价值。"}}
{"id": "2506.09933", "title": "Efficient multigrid solvers for mixed-degree local discontinuous Galerkin multiphase Stokes problems", "authors": ["Robert I. Saye"], "summary": "We design and investigate efficient multigrid solvers for multiphase Stokes\nproblems discretised via mixed-degree local discontinuous Galerkin methods.\nUsing the template of a standard multigrid V-cycle, we develop a smoother\nanalogous to element-wise block Gauss-Seidel, except the diagonal block\ninverses are replaced with an approximation that balances the smoothing of the\nvelocity and pressure variables, factoring in the unequal scaling of the\nvarious Stokes system operators, and optimised via two-grid local Fourier\nanalysis. We evaluate the performance of the multigrid solver across an\nextensive range of two- and three-dimensional test problems, including\nsteady-state and unsteady, standard-form and stress-form, single-phase and\nhigh-contrast multiphase Stokes problems, with multiple kinds of boundary\nconditions and various choices of polynomial degree. In the lowest-degree case,\ni.e., that of piecewise constant pressure fields, we observe reliable multigrid\nconvergence rates, though not especially fast. However, in every other case, we\nsee rapid convergence rates matching those of classical Poisson-style geometric\nmultigrid methods; e.g., 5 iterations reduce the Stokes system residual by 5 to\n10 orders of magnitude.", "comment": "25 pages, 10 figures, 4 algorithms, 1 table", "cate": "math.NA", "url": "http://arxiv.org/abs/2506.09933v1", "AI": {"title_translation": "混合度局部间断伽辽金多相Stokes问题的有效多重网格求解器", "tldr": "本文设计并研究了一种用于通过混合度局部间断伽辽金方法离散化的多相Stokes问题的有效多重网格求解器，在大多数情况下实现了快速收敛。", "motivation": "论文旨在设计并研究高效的多重网格求解器，以解决通过混合度局部间断伽辽金方法离散化的多相Stokes问题，这表明对现有方法的效率或鲁棒性存在改进需求。", "method": "采用标准多重网格V循环模板，开发了一种类似于单元块高斯-赛德尔的平滑器，其中对角块逆被替换为一种近似值，该近似值平衡了速度和压力变量的平滑，并考虑了不同Stokes系统算子的不相等缩放，通过两网格局部傅里叶分析进行了优化。", "result": "对多重网格求解器在广泛的二维和三维测试问题上进行了性能评估，包括稳态和非稳态、标准形式和应力形式、单相和高对比度多相Stokes问题。在最低次数（分段常数压力场）情况下，收敛率可靠但不特别快。但在所有其他情况下，收敛率与经典泊松型几何多重网格方法相匹配，例如，5次迭代将Stokes系统残差降低5到10个数量级。", "conclusion": "除了最低次数情况外，所开发的多重网格求解器能够为混合度局部间断伽辽金离散化的多相Stokes问题提供快速且与经典方法相当的收敛速度。", "translation": "我们设计并研究了通过混合度局部间断伽辽金方法离散化的多相Stokes问题的有效多重网格求解器。使用标准多重网格V循环的模板，我们开发了一种类似于单元块高斯-赛德尔的平滑器，但其对角块逆被替换为一种近似值，该近似值平衡了速度和压力变量的平滑，考虑了各种Stokes系统算子的不相等缩放，并通过两网格局部傅里叶分析进行了优化。我们评估了多重网格求解器在广泛的二维和三维测试问题上的性能，包括稳态和非稳态、标准形式和应力形式、单相和高对比度多相Stokes问题，以及多种边界条件和各种多项式次数选择。在最低次数情况下，即分段常数压力场，我们观察到可靠的多重网格收敛率，尽管速度不特别快。然而，在所有其他情况下，我们看到了与经典泊松型几何多重网格方法相匹配的快速收敛率；例如，5次迭代将Stokes系统残差降低5到10个数量级。", "summary": "本文设计并研究了一种用于通过混合度局部间断伽辽金方法离散化的多相Stokes问题的有效多重网格求解器。该求解器采用改进的平滑器，通过优化近似对角块逆来平衡速度和压力变量的平滑。在广泛的测试问题上评估了其性能，结果表明，除了最低次数情况外，该求解器在解决多相Stokes问题时表现出快速且高效的收敛特性。", "keywords": "多重网格, 局部间断伽辽金, 多相Stokes问题, 平滑器, 收敛率", "comments": "这篇论文通过开发一种优化的多重网格平滑器，有效解决了多相Stokes问题离散化后的求解效率问题，特别是在高阶离散情况下表现出卓越的收敛性能，对于计算流体力学领域具有重要意义。"}}
{"id": "2506.09984", "title": "InterActHuman: Multi-Concept Human Animation with Layout-Aligned Audio Conditions", "authors": ["Zhenzhi Wang", "Jiaqi Yang", "Jianwen Jiang", "Chao Liang", "Gaojie Lin", "Zerong Zheng", "Ceyuan Yang", "Dahua Lin"], "summary": "End-to-end human animation with rich multi-modal conditions, e.g., text,\nimage and audio has achieved remarkable advancements in recent years. However,\nmost existing methods could only animate a single subject and inject conditions\nin a global manner, ignoring scenarios that multiple concepts could appears in\nthe same video with rich human-human interactions and human-object\ninteractions. Such global assumption prevents precise and per-identity control\nof multiple concepts including humans and objects, therefore hinders\napplications. In this work, we discard the single-entity assumption and\nintroduce a novel framework that enforces strong, region-specific binding of\nconditions from modalities to each identity's spatiotemporal footprint. Given\nreference images of multiple concepts, our method could automatically infer\nlayout information by leveraging a mask predictor to match appearance cues\nbetween the denoised video and each reference appearance. Furthermore, we\ninject local audio condition into its corresponding region to ensure\nlayout-aligned modality matching in a iterative manner. This design enables the\nhigh-quality generation of controllable multi-concept human-centric videos.\nEmpirical results and ablation studies validate the effectiveness of our\nexplicit layout control for multi-modal conditions compared to implicit\ncounterparts and other existing methods.", "comment": "TL;DR: The first multi-person dialogue video generation method from\n  pairs of reference image and audio via explicit layout-aligned condition\n  injection. See project page https://zhenzhiwang.github.io/interacthuman/ for\n  more details", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09984v1", "AI": {"title_translation": "InterActHuman：基于布局对齐音频条件的多概念人体动画", "tldr": "该论文提出InterActHuman，一个新颖的框架，通过区域特定的条件绑定和布局对齐的音频条件，实现高质量、可控的多概念以人为中心的视频生成，克服了现有方法单主体和全局条件的问题。", "motivation": "现有的人体动画方法大多只能动画化单个主体，并以全局方式注入条件，忽略了视频中可能出现多概念（人与人、人与物互动）的场景。这种全局假设阻碍了对包括人与物在内的多个概念的精确和逐身份控制，从而限制了应用。", "method": "提出了一种新颖的框架，放弃了单实体假设，强制将多模态条件与每个身份的时空足迹进行强烈的区域特定绑定。方法通过掩码预测器利用参考图像自动推断布局信息，并在迭代过程中将局部音频条件注入到其相应区域，以确保布局对齐的模态匹配。", "result": "经验结果和消融研究验证了其显式布局控制对于多模态条件的有效性，优于隐式对应物和其他现有方法，实现了可控的多概念以人为中心的视频的高质量生成。", "conclusion": "该研究成功开发了一个能够处理多概念和复杂交互的以人为中心视频动画系统，通过其新颖的布局对齐和区域特定条件绑定方法，显著提升了动画的控制性和质量。", "translation": "近年来，结合文本、图像和音频等丰富多模态条件的端到端人体动画取得了显著进展。然而，大多数现有方法只能动画化单个主体，并以全局方式注入条件，忽略了视频中可能出现多个概念（包括丰富的人与人互动和人与物互动）的场景。这种全局假设阻碍了对包括人与物在内的多个概念的精确和逐身份控制，从而限制了应用。在这项工作中，我们放弃了单实体假设，引入了一个新颖的框架，该框架强制将模态条件与每个身份的时空足迹进行强大的、区域特定的绑定。给定多个概念的参考图像，我们的方法可以利用掩码预测器在去噪视频和每个参考外观之间匹配外观线索，从而自动推断布局信息。此外，我们以迭代方式将局部音频条件注入到其相应区域，以确保布局对齐的模态匹配。这种设计使得能够高质量地生成可控的多概念以人为中心的视频。经验结果和消融研究验证了我们的显式布局控制在多模态条件下相对于隐式对应物和其他现有方法的有效性。", "summary": "InterActHuman是一个新颖的框架，旨在解决现有方法在多概念人体动画中单主体和全局条件的问题。它通过引入区域特定的条件绑定和布局对齐的音频条件，实现了对视频中多个概念（包括人与人、人与物互动）的精确控制。该方法利用掩码预测器推断布局，并迭代地将局部音频条件注入相应区域，从而生成高质量、可控的多概念以人为中心的视频。实验结果验证了其显式布局控制的有效性。", "keywords": "人体动画, 多概念, 布局对齐, 音频条件, 多模态", "comments": "这篇论文通过放弃单实体动画的假设，并引入区域特定和布局对齐的多模态条件绑定，显著提升了人体动画的复杂性和控制能力。其创新点在于对多概念交互的处理，特别是通过掩码预测器推断布局和局部音频条件注入，为生成更真实、多样的以人为中心视频奠定了基础，具有重要的应用潜力。"}}
{"id": "2506.09623", "title": "Analytic Task Scheduler: Recursive Least Squares Based Method for Continual Learning in Embodied Foundation Models", "authors": ["Lipei Xie", "Yingxin Li", "Huiping Zhuang"], "summary": "Embodied foundation models are crucial for Artificial Intelligence (AI)\ninteracting with the physical world by integrating multi-modal inputs, such as\nproprioception, vision and language, to understand human intentions and\ngenerate actions to control robots. While these models demonstrate strong\ngeneralization and few-shot learning capabilities, they face significant\nchallenges in continually acquiring new skills without forgetting previously\nlearned skills, a problem known as catastrophic forgetting. To address this\nissue, we propose the Analytic Task Scheduler (ATS), a novel framework for\ncontinual learning in embodied foundation models. ATS consists of a\ntask-specific model library, where each model is fine-tuned independently on a\nsingle task, and an analytic scheduler trained using recursive least squares\n(RLS) to learn the mapping between language instructions and task-specific\nmodels. This architecture enables accurate task recognition and dynamic model\nselection while fundamentally avoiding parameter interference across tasks. The\nscheduler updates its parameters incrementally using only statistics\n(autocorrelation and cross-correlation matrices), enabling forgetting-resistant\nlearning without the need to revisit historical data. We validate ATS on a\nreal-world robot platform (RM65B), demonstrating superior resistance to\nforgetting and strong adaptability to task variations. The results highlight\nATS as an effective, scalable, and deployable solution for continual learning\nin embodied foundation models operating in complex, dynamic environments. Our\ncode will be available at\nhttps://github.com/MIAA-Embodied-AI/AnalyticTaskScheduler", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.09623v1", "AI": {"title_translation": "分析任务调度器：基于递归最小二乘法的具身基础模型持续学习方法", "tldr": "具身基础模型在持续学习中面临灾难性遗忘。本文提出了分析任务调度器（ATS），一个利用任务特定模型库和基于递归最小二乘法（RLS）训练的调度器的新框架。ATS通过动态模型选择和增量更新避免了遗忘，并在真实机器人上得到了验证。", "motivation": "具身基础模型在持续获取新技能时面临“灾难性遗忘”的重大挑战，即它们会遗忘先前学习到的技能。", "method": "本文提出了分析任务调度器（ATS），一个用于具身基础模型持续学习的新颖框架。ATS包含一个任务特定模型库，其中每个模型针对单个任务独立微调。一个使用递归最小二乘法（RLS）训练的分析调度器学习语言指令与任务特定模型之间的映射。这种架构实现了准确的任务识别和动态模型选择，从根本上避免了跨任务的参数干扰。调度器仅使用统计数据（自相关和互相关矩阵）增量更新参数，无需重新访问历史数据，从而实现了抗遗忘学习。", "result": "ATS 在真实世界机器人平台（RM65B）上进行了验证，结果表明其具有卓越的抗遗忘能力和对任务变化的强大适应性。", "conclusion": "ATS 是一个有效、可扩展且可部署的解决方案，适用于在复杂动态环境中运行的具身基础模型进行持续学习。", "translation": "具身基础模型对于人工智能（AI）通过整合多模态输入（如本体感受、视觉和语言）来理解人类意图并生成控制机器人的动作，从而与物理世界进行交互至关重要。尽管这些模型展现出强大的泛化能力和少样本学习能力，但在持续获取新技能而不忘记先前学习的技能方面面临重大挑战，这一问题被称为灾难性遗忘。为了解决这个问题，我们提出了分析任务调度器（ATS），这是一个用于具身基础模型持续学习的新颖框架。ATS 由一个任务特定模型库组成，其中每个模型都在单个任务上独立微调，以及一个使用递归最小二乘法（RLS）训练的分析调度器，用于学习语言指令与任务特定模型之间的映射。这种架构能够实现准确的任务识别和动态模型选择，同时从根本上避免了跨任务的参数干扰。调度器仅使用统计数据（自相关和互相关矩阵）增量更新其参数，从而实现了抗遗忘学习，而无需重新访问历史数据。我们在真实世界机器人平台（RM65B）上验证了 ATS，展示了其卓越的抗遗忘能力和对任务变化的强大适应性。结果表明，ATS 是一个在复杂动态环境中运行的具身基础模型进行持续学习的有效、可扩展和可部署的解决方案。我们的代码将可在 https://github.com/MIAA-Embodied-AI/AnalyticTaskScheduler 获取。", "summary": "本文提出了一种名为分析任务调度器（ATS）的新型框架，旨在解决具身基础模型在持续学习中面临的灾难性遗忘问题。ATS 包含一个任务特定模型库，每个模型独立微调，以及一个基于递归最小二乘法（RLS）训练的分析调度器，用于将语言指令映射到相应的任务模型。这种模块化设计通过动态模型选择，从根本上避免了跨任务的参数干扰，并允许调度器仅利用统计数据进行增量更新，无需历史数据。在真实机器人平台上的验证表明，ATS 具有卓越的抗遗忘能力和强大的任务适应性，证明其是具身基础模型持续学习的有效、可扩展且可部署的解决方案。", "keywords": "持续学习, 具身基础模型, 灾难性遗忘, 递归最小二乘法, 任务调度", "comments": "该论文的创新之处在于其模块化设计，通过任务特定模型库和基于RLS的调度器，明确地避免了灾难性遗忘，而非在单一模型内部进行缓解。利用RLS进行增量更新且无需历史数据，是实现高效和抗遗忘学习的关键技术贡献。在真实机器人平台上的验证进一步增强了其可信度和实际应用价值。"}}
{"id": "2506.09350", "title": "Autoregressive Adversarial Post-Training for Real-Time Interactive Video Generation", "authors": ["Shanchuan Lin", "Ceyuan Yang", "Hao He", "Jianwen Jiang", "Yuxi Ren", "Xin Xia", "Yang Zhao", "Xuefeng Xiao", "Lu Jiang"], "summary": "Existing large-scale video generation models are computationally intensive,\npreventing adoption in real-time and interactive applications. In this work, we\npropose autoregressive adversarial post-training (AAPT) to transform a\npre-trained latent video diffusion model into a real-time, interactive video\ngenerator. Our model autoregressively generates a latent frame at a time using\na single neural function evaluation (1NFE). The model can stream the result to\nthe user in real time and receive interactive responses as controls to generate\nthe next latent frame. Unlike existing approaches, our method explores\nadversarial training as an effective paradigm for autoregressive generation.\nThis not only allows us to design an architecture that is more efficient for\none-step generation while fully utilizing the KV cache, but also enables\ntraining the model in a student-forcing manner that proves to be effective in\nreducing error accumulation during long video generation. Our experiments\ndemonstrate that our 8B model achieves real-time, 24fps, streaming video\ngeneration at 736x416 resolution on a single H100, or 1280x720 on 8xH100 up to\na minute long (1440 frames). Visit our research website at\nhttps://seaweed-apt.com/2", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09350v1", "AI": {"title_translation": "用于实时交互式视频生成的自回归对抗后训练", "tldr": "本文提出自回归对抗后训练 (AAPT)，将预训练的潜在视频扩散模型转化为实时交互式视频生成器，通过一次神经网络函数评估 (1NFE) 逐帧生成潜在帧，实现了实时、24fps 的视频流生成。", "motivation": "现有的大规模视频生成模型计算量巨大，阻碍了它们在实时和交互式应用中的采用。", "method": "提出自回归对抗后训练 (AAPT) 方法，将预训练的潜在视频扩散模型转换为实时交互式视频生成器。该模型通过一次神经网络函数评估 (1NFE) 自回归地逐帧生成潜在帧。该方法探索了对抗训练作为自回归生成的一种有效范式，设计了更高效的单步生成架构并充分利用 KV 缓存，同时采用学生强制训练方式有效减少长视频生成中的误差累积。", "result": "该 8B 模型在单块 H100 GPU 上实现了 736x416 分辨率的实时 24fps 流式视频生成，或在 8 块 H100 GPU 上实现 1280x720 分辨率的长达一分钟（1440 帧）的视频生成。", "conclusion": "本文提出的自回归对抗后训练 (AAPT) 方法成功地将计算密集型视频生成模型转化为高效的实时交互式系统，显著提升了视频生成在实际应用中的可用性。", "translation": "现有的大规模视频生成模型计算量巨大，阻碍了它们在实时和交互式应用中的采用。在这项工作中，我们提出了自回归对抗后训练 (AAPT)，将预训练的潜在视频扩散模型转化为实时交互式视频生成器。我们的模型通过一次神经网络函数评估 (1NFE) 自回归地逐帧生成潜在帧。该模型可以实时向用户传输结果，并接收交互式响应作为控制来生成下一个潜在帧。与现有方法不同，我们的方法探索了对抗训练作为自回归生成的一种有效范式。这不仅使我们能够设计一个更高效的单步生成架构，同时充分利用 KV 缓存，而且还能够以学生强制的方式训练模型，这被证明在长视频生成过程中有效减少误差累积。我们的实验表明，我们的 8B 模型在单个 H100 上实现了 736x416 分辨率的实时 24fps 流式视频生成，或在 8 个 H100 上实现 1280x720 分辨率的长达一分钟（1440 帧）的视频生成。请访问我们的研究网站：https://seaweed-apt.com/2", "summary": "本文提出一种名为自回归对抗后训练 (AAPT) 的新方法，旨在解决现有大规模视频生成模型计算量大、无法实时交互的问题。AAPT 通过对预训练的潜在视频扩散模型进行后训练，使其能够通过一次神经网络函数评估 (1NFE) 自回归地逐帧生成潜在帧，从而实现实时视频流。该方法创新性地将对抗训练应用于自回归生成，优化了架构以提高效率并利用 KV 缓存，同时采用学生强制训练减少误差累积。实验证明，该 8B 模型能在单块 H100 上实现 24fps、736x416 分辨率的实时视频生成，或在 8 块 H100 上生成 1280x720 分辨率的长达一分钟的视频。", "keywords": "自回归, 对抗训练, 视频生成, 实时, 交互式", "comments": "该论文的创新点在于提出了自回归对抗后训练 (AAPT) 方法，尤其是在自回归生成中引入对抗训练范式，并结合学生强制训练策略以减少误差累积。这使得大型视频生成模型能够实现前所未有的实时交互性能，极大地拓宽了其在实际应用中的潜力。其在单张 H100 GPU 上实现实时 24fps 视频生成的能力，展示了显著的效率提升和工程实现上的突破。"}}
{"id": "2506.09172", "title": "MultiNet: An Open-Source Software Toolkit \\& Benchmark Suite for the Evaluation and Adaptation of Multimodal Action Models", "authors": ["Pranav Guruprasad", "Yangyue Wang", "Harshvardhan Sikka"], "summary": "Recent innovations in multimodal action models represent a promising\ndirection for developing general-purpose agentic systems, combining visual\nunderstanding, language comprehension, and action generation. We introduce\nMultiNet - a novel, fully open-source benchmark and surrounding software\necosystem designed to rigorously evaluate and adapt models across vision,\nlanguage, and action domains. We establish standardized evaluation protocols\nfor assessing vision-language models (VLMs) and vision-language-action models\n(VLAs), and provide open source software to download relevant data, models, and\nevaluations. Additionally, we provide a composite dataset with over 1.3\ntrillion tokens of image captioning, visual question answering, commonsense\nreasoning, robotic control, digital game-play, simulated\nlocomotion/manipulation, and many more tasks. The MultiNet benchmark,\nframework, toolkit, and evaluation harness have been used in downstream\nresearch on the limitations of VLA generalization.", "comment": "ICML CodeML Workshop, 13 Pages, 6 Figures, 2 Tables", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09172v1", "AI": {"title_translation": "MultiNet：用于评估和适应多模态动作模型的开源软件工具包和基准套件", "tldr": "介绍了一个名为MultiNet的开源基准和软件生态系统，用于严格评估和适应多模态动作模型，并提供标准化协议和大型数据集。", "motivation": "多模态动作模型在开发通用代理系统方面显示出巨大潜力，但需要一个统一的平台来严格评估和适应这些模型。", "method": "引入了MultiNet，一个开源的基准和软件生态系统，旨在评估和适应跨视觉、语言和动作领域的多模态模型。它建立了评估视觉-语言模型（VLM）和视觉-语言-动作模型（VLA）的标准化协议，并提供开源软件以下载数据、模型和评估。此外，它还提供了一个包含超过1.3万亿tokens的复合数据集，涵盖多种任务。", "result": "MultiNet提供了一个用于评估和适应多模态动作模型的开源基准、框架、工具包和评估工具。它已被用于下游研究，以探索VLA泛化能力的局限性。", "conclusion": "MultiNet是一个全面且开放的资源，旨在促进多模态动作模型的研究和开发，特别是在评估其泛化能力方面。", "translation": "近期多模态动作模型的创新为开发通用代理系统指明了有前景的方向，这些系统结合了视觉理解、语言理解和动作生成。我们引入了MultiNet——一个新颖的、完全开源的基准和周围的软件生态系统，旨在严格评估和适应跨视觉、语言和动作领域的模型。我们建立了评估视觉-语言模型（VLM）和视觉-语言-动作模型（VLA）的标准化评估协议，并提供开源软件以下载相关数据、模型和评估。此外，我们提供了一个复合数据集，包含超过1.3万亿tokens的图像字幕、视觉问答、常识推理、机器人控制、数字游戏、模拟运动/操作以及更多任务。MultiNet基准、框架、工具包和评估工具已被用于关于VLA泛化局限性的下游研究。", "summary": "MultiNet是一个新颖的开源软件工具包和基准套件，旨在严格评估和适应多模态动作模型。它为视觉-语言模型（VLM）和视觉-语言-动作模型（VLA）建立了标准化评估协议，并提供下载数据、模型和评估的开源软件。MultiNet还包含一个超过1.3万亿tokens的复合数据集，涵盖多种任务，并已被用于研究VLA泛化能力的局限性。", "keywords": "多模态动作模型, 开源工具包, 基准, 评估, 视觉-语言-动作模型", "comments": "MultiNet的创新在于其作为完全开源的综合性基准和软件生态系统，为多模态动作模型的评估和适应提供了标准化的平台。其包含的庞大复合数据集以及对VLA泛化能力局限性的关注，使其在推动通用代理系统发展方面具有重要意义。"}}
{"id": "2506.09207", "title": "mLaSDI: Multi-stage latent space dynamics identification", "authors": ["William Anderson", "Kevin Chung", "Youngsoo Choi"], "summary": "Determining accurate numerical solutions of partial differential equations\n(PDEs) is an important task in many scientific disciplines. However, solvers\ncan be computationally expensive, leading to the development of reduced-order\nmodels (ROMs). Recently, Latent Space Dynamics Identification (LaSDI) was\nproposed as a data-driven, non-intrusive ROM framework. LaSDI compresses the\ntraining data using an autoencoder and learns a system of user-chosen ordinary\ndifferential equations (ODEs), which govern the latent space dynamics. This\nallows for rapid predictions by interpolating and evolving the low-dimensional\nODEs in the latent space. While LaSDI has produced effective ROMs for numerous\nproblems, the autoencoder can have difficulty accurately reconstructing\ntraining data while also satisfying the imposed dynamics in the latent space,\nparticularly in complex or high-frequency regimes. To address this, we propose\nmulti-stage Latent Space Dynamics Identification (mLaSDI). With mLaSDI, several\nautoencoders are trained sequentially in stages, where each autoencoder learns\nto correct the error of the previous stages. We find that applying mLaSDI with\nsmall autoencoders results in lower prediction and reconstruction errors, while\nalso reducing training time compared to LaSDI.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09207v1", "AI": {"title_translation": "mLaSDI：多阶段潜空间动力学识别", "tldr": "mLaSDI是一种改进的降阶模型框架，通过顺序训练多个自编码器来纠正误差，从而在复杂或高频情况下降低预测和重建误差，并缩短训练时间。", "motivation": "偏微分方程（PDEs）的精确数值解计算成本高昂，导致降阶模型（ROMs）的发展。现有数据驱动的降阶模型LaSDI在复杂或高频情况下难以准确重建训练数据并满足潜在空间动力学。", "method": "我们提出了多阶段潜空间动力学识别（mLaSDI）方法。mLaSDI通过顺序训练多个自编码器，每个自编码器学习纠正前一阶段的误差。", "result": "与LaSDI相比，将mLaSDI与小型自编码器结合使用，可以降低预测和重建误差，同时减少训练时间。", "conclusion": "mLaSDI通过多阶段误差纠正机制，有效解决了LaSDI在复杂高频情境下的重建和预测精度问题，并提升了训练效率，是一种更优的降阶模型方法。", "translation": "确定偏微分方程（PDEs）的精确数值解是许多科学领域的重要任务。然而，求解器计算成本可能很高，导致了降阶模型（ROMs）的开发。最近，潜空间动力学识别（LaSDI）被提出作为一种数据驱动的非侵入式ROM框架。LaSDI使用自编码器压缩训练数据，并学习用户选择的常微分方程（ODEs）系统，该系统控制潜空间动力学。这允许通过插值和演化潜空间中的低维ODE进行快速预测。虽然LaSDI已为许多问题生成了有效的ROM，但自编码器在准确重建训练数据同时满足潜空间中施加的动力学方面可能存在困难，尤其是在复杂或高频情况下。为了解决这个问题，我们提出了多阶段潜空间动力学识别（mLaSDI）。通过mLaSDI，多个自编码器分阶段顺序训练，每个自编码器学习纠正前一阶段的误差。我们发现，与LaSDI相比，将mLaSDI与小型自编码器结合使用，可以降低预测和重建误差，同时减少训练时间。", "summary": "本文提出mLaSDI，一种改进的潜空间动力学识别（LaSDI）框架，旨在解决LaSDI在复杂或高频情境下重建精度不足的问题。mLaSDI通过顺序训练多个自编码器，每个自编码器负责纠正前一阶段的误差。实验结果表明，mLaSDI能有效降低预测和重建误差，并缩短训练时间，从而提高降阶模型的性能。", "keywords": "mLaSDI, 潜空间动力学识别, 降阶模型, 自编码器, 偏微分方程", "comments": "mLaSDI的创新之处在于其多阶段误差纠正机制，通过顺序训练多个小型自编码器来逐步优化模型性能，有效解决了传统LaSDI在复杂系统建模中的精度瓶颈。这种方法不仅提升了预测和重建的准确性，还实现了训练时间的缩减，对于需要高效处理PDEs的科学计算领域具有重要意义。"}}
{"id": "2506.09474", "title": "Covert Entanglement Generation over Bosonic Channels", "authors": ["Evan J. D. Anderson", "Michael S. Bullock", "Ohad Kimelfeld", "Christopher K. Eyre", "Filip Rozpędek", "Uzi Pereg", "Boulat A. Bash"], "summary": "We explore covert entanglement generation over the lossy thermal-noise\nbosonic channel, which is a quantum-mechanical model of many practical\nsettings, including optical, microwave, and radio-frequency (RF) channels.\nCovert communication ensures that an adversary is unable to detect the presence\nof transmissions, which are concealed in channel noise. We show that a\n$\\textit{square root law}$ (SRL) for covert entanglement generation similar to\nthat for classical: $L_{\\rm EG}\\sqrt{n}$ entangled bits (ebits) can be\ngenerated covertly and reliably over $n$ uses of a bosonic channel. We report a\nsingle-letter expression for optimal $L_{\\rm EG}$ as well as an achievable\nmethod. We additionally analyze the performance of covert entanglement\ngeneration using single- and dual-rail photonic qubits, which may be more\npractical for physical implementation.", "comment": null, "cate": "quant-ph", "url": "http://arxiv.org/abs/2506.09474v1", "AI": {"title_translation": "玻色子信道上的隐蔽纠缠生成", "tldr": "研究了在玻色子信道上隐蔽生成纠缠，发现存在一个类似于经典的平方根定律。", "motivation": "本文旨在探索在量子通信中实现隐蔽性，即确保对抗者无法检测到传输的存在，并将传输隐藏在信道噪声中。这对于光学、微波和射频等多种实际通信设置具有重要意义。", "method": "通过探索有损热噪声玻色子信道，推导了最优 $L_{\\rm EG}$ 的单字母表达式和可实现方法。此外，研究分析了使用单轨和双轨光子量子比特进行隐蔽纠缠生成的性能，以评估其实际物理实现的可行性。", "result": "研究表明，隐蔽纠缠生成存在一个类似于经典的平方根定律：在 $n$ 次玻色子信道使用中，可以隐蔽且可靠地生成 $L_{\\rm EG}\\sqrt{n}$ 个纠缠比特。本文报告了最优 $L_{\\rm EG}$ 的单字母表达式以及一种可实现的方法。", "conclusion": "本文证明了在玻色子信道上可以实现隐蔽且可靠的纠缠生成，并遵循平方根定律。这些发现为实际物理实现提供了理论基础和可行的方案，特别是在使用单轨和双轨光子量子比特的情况下。", "translation": "我们探索了在有损热噪声玻色子信道上的隐蔽纠缠生成，这是一种许多实际设置的量子力学模型，包括光学、微波和射频（RF）信道。隐蔽通信确保对抗者无法检测到传输的存在，传输被隐藏在信道噪声中。我们证明了隐蔽纠缠生成存在一个类似于经典的平方根定律：在 $n$ 次玻色子信道使用中，可以隐蔽且可靠地生成 $L_{\\rm EG}\\sqrt{n}$ 个纠缠比特。我们报告了最优 $L_{\\rm EG}$ 的单字母表达式以及一种可实现的方法。我们还分析了使用单轨和双轨光子量子比特进行隐蔽纠缠生成的性能，这对于物理实现可能更具实用性。", "summary": "本文研究了在有损热噪声玻色子信道（包括光学、微波和射频信道）上隐蔽生成量子纠缠的问题。研究发现，隐蔽纠缠生成遵循一个平方根定律，即在 $n$ 次信道使用中可隐蔽生成 $L_{\\rm EG}\\sqrt{n}$ 个纠缠比特。文章给出了最优 $L_{\\rm EG}$ 的单字母表达式和可实现方法，并分析了使用单轨和双轨光子量子比特进行隐蔽纠缠生成的性能，这对于实际物理实现具有重要意义。", "keywords": "隐蔽纠缠生成, 玻色子信道, 平方根定律, 量子通信, 光子量子比特", "comments": "该研究在量子通信领域具有创新性，将隐蔽通信的概念扩展到量子纠缠生成，并提出了量化其性能的平方根定律。这对于开发新型安全量子通信协议具有重要意义，尤其是在存在窃听者且传输需保持不可检测的场景中。"}}
{"id": "2506.09629", "title": "R-CARLA: High-Fidelity Sensor Simulations with Interchangeable Dynamics for Autonomous Racing", "authors": ["Maurice Brunner", "Edoardo Ghignone", "Nicolas Baumann", "Michele Magno"], "summary": "Autonomous racing has emerged as a crucial testbed for autonomous driving\nalgorithms, necessitating a simulation environment for both vehicle dynamics\nand sensor behavior. Striking the right balance between vehicle dynamics and\nsensor accuracy is crucial for pushing vehicles to their performance limits.\nHowever, autonomous racing developers often face a trade-off between accurate\nvehicle dynamics and high-fidelity sensor simulations. This paper introduces\nR-CARLA, an enhancement of the CARLA simulator that supports holistic\nfull-stack testing, from perception to control, using a single system. By\nseamlessly integrating accurate vehicle dynamics with sensor simulations,\nopponents simulation as NPCs, and a pipeline for creating digital twins from\nreal-world robotic data, R-CARLA empowers researchers to push the boundaries of\nautonomous racing development. Furthermore, it is developed using CARLA's rich\nsuite of sensor simulations. Our results indicate that incorporating the\nproposed digital-twin framework into R-CARLA enables more realistic full-stack\ntesting, demonstrating a significant reduction in the Sim-to-Real gap of car\ndynamics simulation by 42% and by 82% in the case of sensor simulation across\nvarious testing scenarios.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.09629v1", "AI": {"title_translation": "R-CARLA：具有可互换动态的高保真传感器模拟用于自动驾驶竞赛", "tldr": "R-CARLA是CARLA模拟器的增强版，旨在解决自动驾驶竞赛中车辆动力学和高保真传感器模拟之间的权衡问题，显著缩小了仿真与现实之间的差距。", "motivation": "自动驾驶竞赛开发者在准确的车辆动力学和高保真传感器模拟之间面临权衡，这阻碍了对自动驾驶算法进行全面的全栈测试。", "method": "R-CARLA是CARLA模拟器的增强版，它通过无缝集成精确的车辆动力学、高保真传感器模拟、NPC对手模拟以及从真实机器人数据创建数字孪生的管道，支持从感知到控制的全面全栈测试。", "result": "R-CARLA，特别是结合其数字孪生框架，实现了更真实的全面全栈测试，在各种测试场景中，将汽车动力学模拟的仿真与现实差距减少了42%，传感器模拟的差距减少了82%。", "conclusion": "R-CARLA为自动驾驶竞赛提供了一个全面且逼真的模拟环境，有效弥合了车辆动力学和传感器行为的仿真与现实差距，从而推动了自动驾驶算法的开发。", "translation": "自动驾驶竞赛已成为自动驾驶算法的关键试验台，需要一个同时支持车辆动力学和传感器行为的仿真环境。在车辆动力学和传感器精度之间取得适当的平衡对于将车辆推向其性能极限至关重要。然而，自动驾驶竞赛开发者经常面临准确车辆动力学和高保真传感器模拟之间的权衡。本文介绍了R-CARLA，它是CARLA模拟器的一个增强版本，支持使用单一系统进行从感知到控制的整体全栈测试。通过将准确的车辆动力学与传感器模拟、作为NPC的对手模拟以及从真实世界机器人数据创建数字孪生的管道无缝集成，R-CARLA使研究人员能够突破自动驾驶竞赛开发的界限。此外，它利用CARLA丰富的传感器模拟套件进行开发。我们的结果表明，将所提出的数字孪生框架整合到R-CARLA中可以实现更真实的全面全栈测试，在各种测试场景中，显著减少了汽车动力学模拟的仿真与现实差距42%，传感器模拟的差距82%。", "summary": "R-CARLA是一款基于CARLA模拟器增强的系统，旨在解决自动驾驶竞赛中车辆动力学和高保真传感器模拟之间的权衡。它通过无缝集成精确的车辆动力学、高保真传感器模拟、NPC对手以及从真实机器人数据创建数字孪生的管道，实现了从感知到控制的全面全栈测试。实验结果表明，R-CARLA显著缩小了仿真与现实之间的差距，在车辆动力学模拟方面减少了42%，在传感器模拟方面减少了82%，从而提供了更真实的测试环境。", "keywords": "自动驾驶竞赛, 传感器模拟, 车辆动力学, 仿真与现实差距, 数字孪生", "comments": "R-CARLA的创新在于其全面解决了自动驾驶模拟中的核心挑战——车辆动力学与传感器模拟的平衡，并通过引入数字孪生框架显著提升了仿真与现实的一致性。这对于加速自动驾驶算法的开发和测试具有重要意义，尤其是在高风险的自动驾驶竞赛场景中。"}}
{"id": "2506.09357", "title": "A new approach for image segmentation based on diffeomorphic registration and gradient fields", "authors": ["Junchao Zhou"], "summary": "Image segmentation is a fundamental task in computer vision aimed at\ndelineating object boundaries within images. Traditional approaches, such as\nedge detection and variational methods, have been widely explored, while recent\nadvances in deep learning have shown promising results but often require\nextensive training data. In this work, we propose a novel variational framework\nfor 2D image segmentation that integrates concepts from shape analysis and\ndiffeomorphic transformations. Our method models segmentation as the\ndeformation of a template curve via a diffeomorphic transformation of the image\ndomain, using the Large Deformation Diffeomorphic Metric Mapping (LDDMM)\nframework. The curve evolution is guided by a loss function that compares the\ndeformed curve to the image gradient field, formulated through the varifold\nrepresentation of geometric shapes. The approach is implemented in Python with\nGPU acceleration using the PyKeops library. This framework allows for accurate\nsegmentation with a flexible and theoretically grounded methodology that does\nnot rely on large datasets.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09357v1", "AI": {"title_translation": "基于微分同胚配准和梯度场的新图像分割方法", "tldr": "本文提出了一种基于微分同胚配准和梯度场的新型变分框架，用于2D图像分割，该方法能实现精确分割，且不依赖于大型数据集。", "motivation": "传统的图像分割方法已被广泛探索，而深度学习方法虽然前景广阔，但通常需要大量的训练数据。本研究旨在提出一种不需要大量数据集的图像分割方法。", "method": "本文提出了一种新颖的2D图像分割变分框架，该框架结合了形状分析和微分同胚变换的概念。该方法通过图像域的微分同胚变换（使用大变形微分同胚度量映射LDDMM框架）将分割建模为模板曲线的变形。曲线演化由一个损失函数引导，该函数通过几何形状的变分表示来比较变形曲线与图像梯度场。该方法使用PyKeops库在Python中实现，并支持GPU加速。", "result": "该框架能够实现精确的图像分割，并且提供了一种灵活且有理论基础的方法，不依赖于大型数据集。", "conclusion": "本研究提出的基于微分同胚配准和梯度场的新型变分框架，能够实现精确的图像分割，且具有灵活性和坚实的理论基础，同时解决了对大型数据集的依赖问题。", "translation": "图像分割是计算机视觉中的一项基本任务，旨在描绘图像中的对象边界。传统的边缘检测和变分方法已被广泛探索，而深度学习的最新进展显示出有希望的结果，但通常需要大量的训练数据。在这项工作中，我们提出了一种新颖的2D图像分割变分框架，该框架整合了形状分析和微分同胚变换的概念。我们的方法使用大变形微分同胚度量映射（LDDMM）框架，将分割建模为通过图像域的微分同胚变换对模板曲线进行变形。曲线演化由一个损失函数引导，该函数通过几何形状的变分表示来比较变形曲线与图像梯度场。该方法使用PyKeops库在Python中实现，并支持GPU加速。该框架允许进行精确分割，并具有灵活且有理论基础的方法，不依赖于大型数据集。", "summary": "本文提出了一种新颖的2D图像分割变分框架，旨在解决深度学习方法对大量训练数据的依赖。该方法将分割视为通过大变形微分同胚度量映射（LDDMM）对模板曲线进行变形，并利用基于图像梯度场和几何形状变分表示的损失函数来引导曲线演化。该框架在Python中实现并支持GPU加速，能够实现精确且灵活的分割，且无需大量数据集，提供了一种理论上严谨的替代方案。", "keywords": "图像分割, 微分同胚配准, 梯度场, 变分框架, LDDMM", "comments": "该论文通过利用微分同胚配准（一种强大的形状分析工具）并将其与梯度场相结合，为图像分割提供了一种创新方法。其主要优势在于提供了一个有理论基础的框架，规避了深度学习方法常见的局限性——对大型数据集的需求。这使得它在数据获取具有挑战性的应用中尤为宝贵。LDDMM和变分表示的使用表明其具有复杂的数学基础。"}}
{"id": "2506.09173", "title": "The Curious Language Model: Strategic Test-Time Information Acquisition", "authors": ["Michael Cooper", "Rohan Wadhawan", "John Michael Giorgi", "Chenhao Tan", "Davis Liang"], "summary": "Decision-makers often possess insufficient information to render a confident\ndecision. In these cases, the decision-maker can often undertake actions to\nacquire the necessary information about the problem at hand, e.g., by\nconsulting knowledgeable authorities or by conducting experiments. Importantly,\ndifferent levers of information acquisition come with different costs, posing\nthe challenge of selecting the actions that are both informative and\ncost-effective. In this work, we propose CuriosiTree, a heuristic-based,\ntest-time policy for zero-shot information acquisition in large language models\n(LLMs). CuriosiTree employs a greedy tree search to estimate the expected\ninformation gain of each action and strategically chooses actions based on a\nbalance of anticipated information gain and associated cost. Empirical\nvalidation in a clinical diagnosis simulation shows that CuriosiTree enables\ncost-effective integration of heterogenous sources of information, and\noutperforms baseline action selection strategies in selecting action sequences\nthat enable accurate diagnosis.", "comment": "39 pages", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09173v1", "AI": {"title_translation": "探索性语言模型：战略性测试时信息获取", "tldr": "提出CuriosiTree，一种LLM在测试时零样本信息获取的策略，通过贪婪树搜索平衡信息增益和成本，并在临床诊断模拟中表现出成本效益和更高的诊断准确性。", "motivation": "决策者常常缺乏足够的信息来做出自信的决定。信息获取行为（如咨询专家、进行实验）伴随着不同的成本，因此选择既有信息量又经济高效的行动是一个挑战。", "method": "本文提出了CuriosiTree，一种基于启发式的测试时零样本信息获取策略，用于大型语言模型（LLMs）。CuriosiTree采用贪婪树搜索来估计每个行动的预期信息增益，并根据预期信息增益和相关成本之间的平衡来战略性地选择行动。", "result": "在临床诊断模拟中的实证验证表明，CuriosiTree能够实现异构信息的成本效益整合，并且在选择能够实现准确诊断的行动序列方面优于基线行动选择策略。", "conclusion": "CuriosiTree通过在测试时战略性地、成本效益地获取信息，提高了大型语言模型在信息不足情况下的决策准确性，尤其是在临床诊断等复杂场景中。", "translation": "决策者常常缺乏足够的信息来做出自信的决定。在这种情况下，决策者通常可以采取行动来获取有关当前问题所需的信息，例如通过咨询知识渊博的权威机构或进行实验。重要的是，不同的信息获取方式伴随着不同的成本，这带来了选择既有信息量又经济高效的行动的挑战。在这项工作中，我们提出了CuriosiTree，一种基于启发式的测试时零样本信息获取策略，用于大型语言模型（LLMs）。CuriosiTree采用贪婪树搜索来估计每个行动的预期信息增益，并根据预期信息增益和相关成本之间的平衡来战略性地选择行动。在临床诊断模拟中的实证验证表明，CuriosiTree能够实现异构信息的成本效益整合，并且在选择能够实现准确诊断的行动序列方面优于基线行动选择策略。", "summary": "本文提出了CuriosiTree，一种针对大型语言模型（LLMs）的启发式测试时零样本信息获取策略。鉴于决策者在信息不足时面临的挑战和信息获取的成本差异，CuriosiTree利用贪婪树搜索来评估和选择能够平衡预期信息增益与相关成本的行动。在临床诊断模拟中的实验证明，CuriosiTree能够有效地整合不同来源的信息，并显著优于现有基线策略，从而实现更准确的诊断。", "keywords": "大型语言模型, 信息获取, 成本效益, 贪婪树搜索, 临床诊断", "comments": "本文的创新点在于提出了CuriosiTree，一个用于LLM的测试时零样本信息获取策略，解决了传统上信息不足导致决策不确定性的问题。其通过平衡信息增益和成本的贪婪树搜索方法，为LLM在复杂场景（如临床诊断）中进行更智能、更经济的决策提供了新途径。这项工作对于提高LLM在实际应用中的效用和可靠性具有重要意义。"}}
{"id": "2506.09847", "title": "Dataset of News Articles with Provenance Metadata for Media Relevance Assessment", "authors": ["Tomas Peterka", "Matyas Bohacek"], "summary": "Out-of-context and misattributed imagery is the leading form of media\nmanipulation in today's misinformation and disinformation landscape. The\nexisting methods attempting to detect this practice often only consider whether\nthe semantics of the imagery corresponds to the text narrative, missing\nmanipulation so long as the depicted objects or scenes somewhat correspond to\nthe narrative at hand. To tackle this, we introduce News Media Provenance\nDataset, a dataset of news articles with provenance-tagged images. We formulate\ntwo tasks on this dataset, location of origin relevance (LOR) and date and time\nof origin relevance (DTOR), and present baseline results on six large language\nmodels (LLMs). We identify that, while the zero-shot performance on LOR is\npromising, the performance on DTOR hinders, leaving room for specialized\narchitectures and future work.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.09847v1", "AI": {"title_translation": "带有出处元数据的新闻文章数据集，用于媒体相关性评估", "tldr": "本文引入了一个新的新闻文章数据集（News Media Provenance Dataset），其中包含带有出处标签的图像，以检测脱离上下文和错误归属的媒体操纵。该数据集定义了来源地点相关性（LOR）和来源日期时间相关性（DTOR）两个任务，并提供了六个大型语言模型（LLM）的基线结果。研究发现，LOR的零样本性能良好，但DTOR的性能仍有提升空间，为未来的专业架构研究提供了方向。", "motivation": "当今虚假信息和错误信息环境中，脱离上下文和错误归属的图像是媒体操纵的主要形式。现有检测方法仅考虑图像语义与文本叙述的对应性，容易遗漏描绘对象或场景与叙述大致对应但实际已被操纵的情况，因此需要更有效的方法来识别此类媒体操纵。", "method": "研究引入了“新闻媒体来源数据集”（News Media Provenance Dataset），该数据集包含带有出处标签图像的新闻文章。在此数据集上，研究者提出了两个任务：来源地点相关性（LOR）和来源日期时间相关性（DTOR）。研究者使用六个大型语言模型（LLM）对这些任务进行了基线测试。", "result": "在来源地点相关性（LOR）任务上，大型语言模型的零样本性能表现出良好潜力。然而，在来源日期时间相关性（DTOR）任务上的表现不佳，存在显著的性能瓶颈。", "conclusion": "尽管在来源地点相关性（LOR）任务上取得了有希望的零样本性能，但来源日期时间相关性（DTOR）任务的性能仍需提升。这为开发专门的架构和未来的研究工作留下了广阔空间，以更有效地检测脱离上下文和错误归属的图像。", "translation": "脱离上下文和错误归属的图像是当今虚假信息和错误信息环境中媒体操纵的主要形式。现有试图检测这种行为的方法通常只考虑图像的语义是否与文本叙述对应，只要描绘的物体或场景与当前叙述大致对应，就会遗漏操纵。为了解决这个问题，我们引入了新闻媒体来源数据集，这是一个包含带有来源标签图像的新闻文章数据集。我们在这个数据集上提出了两个任务：来源地点相关性（LOR）和来源日期时间相关性（DTOR），并展示了六个大型语言模型（LLM）的基线结果。我们发现，虽然LOR的零样本性能令人鼓舞，但DTOR的性能受阻，这为专门的架构和未来的工作留下了空间。", "summary": "本文旨在解决检测脱离上下文和错误归属图像这一媒体操纵形式的挑战。为此，研究者构建了“新闻媒体来源数据集”，其中包含带有出处元数据的新闻文章图像。基于此数据集，论文提出了两个核心任务：来源地点相关性（LOR）和来源日期时间相关性（DTOR），并利用六个大型语言模型进行了基线性能评估。结果显示，LOR任务的零样本性能表现出积极前景，但DTOR任务的性能存在明显不足，这表明未来需要开发更专业的模型和架构来提升此类媒体操查的检测能力。", "keywords": "出处元数据, 媒体操纵, 新闻文章, 数据集, 大型语言模型", "comments": "该论文通过创建带有出处元数据的专门数据集，超越了仅凭语义对应进行媒体操纵检测的局限性，在解决当前虚假信息挑战方面具有创新性和重要性。所提出的LOR和DTOR任务为评估媒体相关性提供了新的维度。DTOR性能的局限性指出了未来研究的关键方向，即开发更鲁棒的模型来验证图像的时间和空间出处。"}}
{"id": "2506.09697", "title": "Human-robot collaborative transport personalization via Dynamic Movement Primitives and velocity scaling", "authors": ["Paolo Franceschi", "Andrea Bussolan", "Vincenzo Pomponi", "Oliver Avram", "Stefano Baraldo", "Anna Valente"], "summary": "Nowadays, industries are showing a growing interest in human-robot\ncollaboration, particularly for shared tasks. This requires intelligent\nstrategies to plan a robot's motions, considering both task constraints and\nhuman-specific factors such as height and movement preferences. This work\nintroduces a novel approach to generate personalized trajectories using Dynamic\nMovement Primitives (DMPs), enhanced with real-time velocity scaling based on\nhuman feedback. The method was rigorously tested in industrial-grade\nexperiments, focusing on the collaborative transport of an engine cowl lip\nsection. Comparative analysis between DMP-generated trajectories and a\nstate-of-the-art motion planner (BiTRRT) highlights their adaptability combined\nwith velocity scaling. Subjective user feedback further demonstrates a clear\npreference for DMP- based interactions. Objective evaluations, including\nphysiological measurements from brain and skin activity, reinforce these\nfindings, showcasing the advantages of DMPs in enhancing human-robot\ninteraction and improving user experience.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.09697v1", "AI": {"title_translation": "人机协作运输个性化：基于动态运动基元和速度标定", "tldr": "通过动态运动基元(DMPs)和速度标定，实现人机协作运输的个性化，提高了用户体验。", "motivation": "工业界对人机协作共享任务的兴趣日益增长，需要智能策略来规划机器人运动，同时考虑任务约束和人类特定因素（如身高和运动偏好）。", "method": "本文引入了一种新颖的方法，利用动态运动基元（DMPs）生成个性化轨迹，并通过基于人类反馈的实时速度标定进行增强。该方法在工业级实验中进行了严格测试，并与最先进的运动规划器（BiTRRT）进行了比较分析。", "result": "DMP生成的轨迹结合速度标定表现出良好的适应性。主观用户反馈显示对DMP交互有明显偏好。客观评估（包括大脑和皮肤活动的生理测量）也证实了DMPs在增强人机交互和改善用户体验方面的优势。", "conclusion": "DMPs结合速度标定能有效实现人机协作运输的个性化，显著提升人机交互和用户体验。", "translation": "如今，工业界对人机协作，特别是共享任务，表现出越来越浓厚的兴趣。这需要智能策略来规划机器人的运动，同时考虑任务约束和人类特有的因素，如身高和运动偏好。这项工作引入了一种新颖的方法，利用动态运动基元（DMPs）生成个性化轨迹，并通过基于人类反馈的实时速度标定进行增强。该方法在工业级实验中进行了严格测试，重点关注发动机整流罩唇缘部分的协作运输。DMP生成的轨迹与最先进的运动规划器（BiTRRT）之间的比较分析突出了它们结合速度标定的适应性。主观用户反馈进一步表明对基于DMP的交互有明显的偏好。包括大脑和皮肤活动生理测量在内的客观评估强化了这些发现，展示了DMPs在增强人机交互和改善用户体验方面的优势。", "summary": "本文提出一种新颖的人机协作运输个性化方法，利用动态运动基元（DMPs）生成轨迹，并结合实时速度标定。该方法在工业级实验中表现出优异的适应性，且用户反馈和生理测量均表明DMPs能显著提升人机交互和用户体验。", "keywords": "人机协作, 动态运动基元, 速度标定, 个性化, 用户体验", "comments": "该研究的创新点在于将DMPs与实时速度标定相结合，实现人机协作的个性化，并通过多维度（主观、客观生理）验证了其有效性，对提升工业人机协作效率和用户满意度具有重要意义。"}}
{"id": "2506.09174", "title": "Multivariate Long-term Time Series Forecasting with Fourier Neural Filter", "authors": ["Chenheng Xu", "Dan Wu", "Yixin Zhu", "Ying Nian Wu"], "summary": "Multivariate long-term time series forecasting has been suffering from the\nchallenge of capturing both temporal dependencies within variables and spatial\ncorrelations across variables simultaneously. Current approaches predominantly\nrepurpose backbones from natural language processing or computer vision (e.g.,\nTransformers), which fail to adequately address the unique properties of time\nseries (e.g., periodicity). The research community lacks a dedicated backbone\nwith temporal-specific inductive biases, instead relying on domain-agnostic\nbackbones supplemented with auxiliary techniques (e.g., signal decomposition).\nWe introduce FNF as the backbone and DBD as the architecture to provide\nexcellent learning capabilities and optimal learning pathways for\nspatio-temporal modeling, respectively. Our theoretical analysis proves that\nFNF unifies local time-domain and global frequency-domain information\nprocessing within a single backbone that extends naturally to spatial modeling,\nwhile information bottleneck theory demonstrates that DBD provides superior\ngradient flow and representation capacity compared to existing unified or\nsequential architectures. Our empirical evaluation across 11 public benchmark\ndatasets spanning five domains (energy, meteorology, transportation,\nenvironment, and nature) confirms state-of-the-art performance with consistent\nhyperparameter settings. Notably, our approach achieves these results without\nany auxiliary techniques, suggesting that properly designed neural\narchitectures can capture the inherent properties of time series, potentially\ntransforming time series modeling in scientific and industrial applications.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09174v1", "AI": {"title_translation": "基于傅里叶神经滤波器的多元长期时间序列预测", "tldr": "该论文引入了FNF作为骨干网络和DBD作为架构，用于多元长期时间序列预测，在不使用辅助技术的情况下实现了最先进的性能，有效捕获了时间和空间依赖性。", "motivation": "当前的多元长期时间序列预测方法主要重用自然语言处理或计算机视觉领域的骨干网络（如Transformer），这些网络未能充分解决时间序列的独特属性（如周期性），且缺乏时间特定归纳偏置，无法同时捕获变量内部时间依赖性和变量间空间相关性。", "method": "本文引入了FNF（傅里叶神经滤波器）作为骨干网络，它在一个单一骨干网络中统一了局部时域和全局频域信息处理，并自然地扩展到空间建模。同时引入了DBD（域特定骨干设计）作为架构，通过信息瓶颈理论证明其提供优越的梯度流和表示能力。", "result": "在涵盖能源、气象、交通、环境和自然五个领域的11个公共基准数据集上进行了实证评估，证实了在一致超参数设置下的最先进性能。值得注意的是，该方法在没有任何辅助技术的情况下取得了这些结果。", "conclusion": "设计得当的神经网络架构（如FNF和DBD）能够捕获时间序列的固有属性，这可能改变科学和工业应用中的时间序列建模，解决了现有领域无关骨干网络的局限性。", "translation": "多元长期时间序列预测一直面临同时捕获变量内部时间依赖性和变量间空间相关性的挑战。当前方法主要重用自然语言处理或计算机视觉领域的骨干网络（例如，Transformer），这些网络未能充分解决时间序列的独特属性（例如，周期性）。研究界缺乏具有时间特定归纳偏置的专用骨干网络，而是依赖于领域无关的骨干网络辅以辅助技术（例如，信号分解）。我们引入FNF作为骨干网络，DBD作为架构，分别提供卓越的学习能力和最优的学习路径，用于时空建模。我们的理论分析证明，FNF在一个单一骨干网络中统一了局部时域和全局频域信息处理，并自然地扩展到空间建模，而信息瓶颈理论表明，与现有统一或顺序架构相比，DBD提供了卓越的梯度流和表示能力。我们在涵盖五个领域（能源、气象、交通、环境和自然）的11个公共基准数据集上的实证评估证实了在一致超参数设置下的最先进性能。值得注意的是，我们的方法在没有任何辅助技术的情况下取得了这些结果，这表明设计得当的神经网络架构可以捕获时间序列的固有属性，这可能改变科学和工业应用中的时间序列建模。", "summary": "本文提出了一种新颖的架构，包括FNF（傅里叶神经滤波器）作为骨干网络和DBD用于最优学习路径，以解决多元长期时间序列预测的挑战。FNF统一了时域和频域处理，而DBD增强了梯度流和表示能力。理论分析和在11个基准数据集上的实证评估表明，该方法在不依赖辅助技术的情况下实现了最先进的性能，突出了专用神经网络架构在时间序列建模中的潜力。", "keywords": "多元时间序列预测, 傅里叶神经滤波器, 时空建模, 深度学习, 时间序列", "comments": "该论文引入了一种专门为时间序列设计的创新型骨干网络（FNF）和架构（DBD），解决了重用其他领域模型所面临的局限性。其在不使用辅助技术的情况下实现最先进性能的能力是一项重大创新，表明了一种更基础的方法来捕获时间序列属性。这可能在各种应用中带来更高效和准确的模型。"}}
{"id": "2506.09297", "title": "A thorough study of Riemannian Newton's Method", "authors": ["Caio O. da Silva", "Yuri A. Aoto", "Felipe F. G. S. Costa", "Márcio F. da Silva"], "summary": "This work presents a thorough numerical study of Riemannian Newton's Method\n(RNM) for optimization problems, with a focus on the Grassmannian and on the\nStiefel manifold. We compare the Riemannian formulation of Newton's Method with\nits classical Euclidean counterpart based on Lagrange multipliers by applying\nboth approaches to the important and challenging Hartree--Fock energy\nminimization problem from Quantum Chemistry. Experiments on a dataset of 125\nmolecules show that the Riemannian approaches achieve higher convergence rates,\nrequire fewer iterations, and exhibit greater robustness to the choice of\ninitial guess. In this work we also analyze the numerical issues that arise\nfrom using Newton's Method on the total manifold when the cost function is\ndefined on the quotient manifold. We investigate the performance of a modified\nRNM in which we ignore the small eigenvalues of the Hessian and the results\nindicate that this modified method is stable and performs on par with the RNM\non the quotient manifold.", "comment": null, "cate": "math.OC", "url": "http://arxiv.org/abs/2506.09297v1", "AI": {"title_translation": "黎曼牛顿法的一次彻底研究", "tldr": "本文对黎曼牛顿法进行了全面的数值研究，并将其与欧几里得方法进行比较，结果表明黎曼方法在收敛速度、迭代次数和鲁棒性方面表现更优，且改进后的黎曼牛顿法稳定有效。", "motivation": "旨在对优化问题中的黎曼牛顿法（RNM）进行彻底的数值研究，并将其与基于拉格朗日乘子的经典欧几里得牛顿法进行比较，特别是在格拉斯曼流形和斯蒂弗尔流形上，以及应用于具有挑战性的哈特里-福克能量最小化问题。", "method": "本文比较了黎曼牛顿法与经典欧几里得牛顿法在哈特里-福克能量最小化问题上的应用。通过对125个分子的数据集进行实验。同时，分析了在总流形上使用牛顿法时出现的数值问题，并研究了忽略海森矩阵小特征值的改进黎曼牛顿法的性能。", "result": "黎曼方法实现了更高的收敛速度，需要更少的迭代次数，并且对初始猜测的选择表现出更大的鲁棒性。改进后的黎曼牛顿法是稳定的，并且与在商流形上的黎曼牛顿法表现相当。", "conclusion": "黎曼方法在优化问题中表现优于经典的欧几里得方法，特别是在收敛速度、迭代次数和鲁棒性方面。此外，改进的黎曼牛顿法在处理数值问题时是稳定且有效的。", "translation": "本工作对优化问题中的黎曼牛顿法（RNM）进行了彻底的数值研究，重点关注格拉斯曼流形和斯蒂弗尔流形。通过将这两种方法应用于量子化学中重要且具有挑战性的哈特里-福克能量最小化问题，我们将黎曼牛顿法与基于拉格朗日乘子的经典欧几里得对应方法进行了比较。对125个分子数据集的实验表明，黎曼方法实现了更高的收敛速度，需要更少的迭代次数，并且对初始猜测的选择表现出更大的鲁棒性。在这项工作中，我们还分析了当成本函数定义在商流形上时，在总流形上使用牛顿法所产生的数值问题。我们研究了忽略海森矩阵小特征值的改进RNM的性能，结果表明这种改进方法是稳定的，并且与在商流形上的RNM表现相当。", "summary": "本文对优化问题中的黎曼牛顿法（RNM）进行了全面的数值研究，并将其与欧几里得牛顿法进行比较。研究发现，黎曼方法在收敛速度、迭代次数和初始猜测鲁棒性方面均优于欧几里得方法，尤其适用于量子化学中的哈特里-福克能量最小化等复杂问题。此外，论文还分析了在总流形上使用牛顿法时可能出现的数值问题，并提出了一种改进的黎曼牛顿法，该方法通过忽略海森矩阵的小特征值来保持稳定性和性能。", "keywords": "黎曼牛顿法, 优化, 格拉斯曼流形, 斯蒂弗尔流形, 哈特里-福克", "comments": "这项工作通过对黎曼牛顿法与经典欧几里得牛顿法在复杂优化问题（如哈特里-福克能量最小化）上的深入数值比较，展示了黎曼方法的显著优势，尤其是在收敛性、效率和鲁棒性方面。其创新之处在于提供了充分的实验证据来支持黎曼优化方法的优越性，并解决了在特定流形上应用牛顿法时可能遇到的数值稳定性问题，提出了一种有效的改进方案，这对于实际应用具有重要指导意义。"}}
{"id": "2506.09765", "title": "Learning to Optimize Package Picking for Large-Scale, Real-World Robot Induction", "authors": ["Shuai Li", "Azarakhsh Keipour", "Sicong Zhao", "Srinath Rajagopalan", "Charles Swan", "Kostas E. Bekris"], "summary": "Warehouse automation plays a pivotal role in enhancing operational\nefficiency, minimizing costs, and improving resilience to workforce\nvariability. While prior research has demonstrated the potential of machine\nlearning (ML) models to increase picking success rates in large-scale robotic\nfleets by prioritizing high-probability picks and packages, these efforts\nprimarily focused on predicting success probabilities for picks sampled using\nheuristic methods. Limited attention has been given, however, to leveraging\ndata-driven approaches to directly optimize sampled picks for better\nperformance at scale. In this study, we propose an ML-based framework that\npredicts transform adjustments as well as improving the selection of suction\ncups for multi-suction end effectors for sampled picks to enhance their success\nprobabilities. The framework was integrated and evaluated in test workcells\nthat resemble the operations of Amazon Robotics' Robot Induction (Robin) fleet,\nwhich is used for package manipulation. Evaluated on over 2 million picks, the\nproposed method achieves a 20\\% reduction in pick failure rates compared to a\nheuristic-based pick sampling baseline, demonstrating its effectiveness in\nlarge-scale warehouse automation scenarios.", "comment": "The 19th International Symposium on Experimental Robotics (ISER\n  2025); 6-10 July 2025, Santa Fe, New Mexico, USA; 10 pages", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.09765v1", "AI": {"title_translation": "学习优化大规模真实世界机器人感应中的包裹拣选", "tldr": "本文提出了一种基于机器学习的框架，用于优化大规模仓库自动化中的包裹拣选，通过预测变换调整和改进吸盘选择，将拣选失败率降低了20%。", "motivation": "仓库自动化对于提高运营效率、降低成本和增强劳动力变动弹性至关重要。尽管之前的研究利用机器学习提高了机器人拣选成功率，但它们主要侧重于预测启发式采样拣选的成功概率。目前对直接利用数据驱动方法优化采样拣选以提高大规模性能的关注有限。", "method": "本研究提出了一种基于机器学习的框架，该框架预测变换调整并改进多吸盘末端执行器的吸盘选择，以提高采样拣选的成功概率。该框架已集成并在模拟Amazon Robotics Robin车队操作的测试工作单元中进行评估。", "result": "在超过200万次拣选的评估中，所提出的方法与基于启发式的拣选采样基线相比，拣选失败率降低了20%。", "conclusion": "所提出的机器学习框架在Amazon Robotics Robin车队的大规模仓库自动化场景中，通过优化包裹拣选显著降低了拣选失败率，证明了其有效性。", "translation": "仓库自动化在提高运营效率、最小化成本和提高对劳动力变动弹性的方面发挥着关键作用。虽然先前的研究已经证明了机器学习（ML）模型通过优先选择高概率的拣选和包裹来提高大规模机器人车队拣选成功率的潜力，但这些努力主要集中于预测使用启发式方法采样的拣选的成功概率。然而，很少有研究关注利用数据驱动方法直接优化采样拣选以在大规模上获得更好的性能。在本研究中，我们提出了一个基于ML的框架，该框架预测变换调整并改进多吸盘末端执行器的吸盘选择，以提高采样拣选的成功概率。该框架已集成并在模拟Amazon Robotics Robot Induction (Robin) 车队操作的测试工作单元中进行评估，该车队用于包裹操作。在超过200万次拣选的评估中，所提出的方法与基于启发式的拣选采样基线相比，拣选失败率降低了20%，证明了其在大规模仓库自动化场景中的有效性。", "summary": "本研究提出了一种基于机器学习的框架，旨在优化大规模仓库自动化中的包裹拣选。该框架通过预测变换调整和改进多吸盘末端执行器的吸盘选择来提高拣选成功率。在对Amazon Robotics Robin车队操作进行模拟的测试环境中，对超过200万次拣选的评估显示，与启发式基线相比，该方法将拣选失败率降低了20%，证明了其在实际大规模应用中的有效性。", "keywords": "机器人拣选, 仓库自动化, 机器学习, 优化, 拣选失败率", "comments": "该研究通过提出一种数据驱动的机器学习框架，直接优化了大规模机器人拣选过程中的采样拣选，而非仅仅预测成功率，这是其创新之处。在真实世界场景中实现的20%拣选失败率降低，凸显了其在提高仓库自动化效率方面的重要性和实用价值。"}}
{"id": "2506.09369", "title": "ScaleLSD: Scalable Deep Line Segment Detection Streamlined", "authors": ["Zeran Ke", "Bin Tan", "Xianwei Zheng", "Yujun Shen", "Tianfu Wu", "Nan Xue"], "summary": "This paper studies the problem of Line Segment Detection (LSD) for the\ncharacterization of line geometry in images, with the aim of learning a\ndomain-agnostic robust LSD model that works well for any natural images. With\nthe focus of scalable self-supervised learning of LSD, we revisit and\nstreamline the fundamental designs of (deep and non-deep) LSD approaches to\nhave a high-performing and efficient LSD learner, dubbed as ScaleLSD, for the\ncuration of line geometry at scale from over 10M unlabeled real-world images.\nOur ScaleLSD works very well to detect much more number of line segments from\nany natural images even than the pioneered non-deep LSD approach, having a more\ncomplete and accurate geometric characterization of images using line segments.\nExperimentally, our proposed ScaleLSD is comprehensively testified under\nzero-shot protocols in detection performance, single-view 3D geometry\nestimation, two-view line segment matching, and multiview 3D line mapping, all\nwith excellent performance obtained. Based on the thorough evaluation, our\nScaleLSD is observed to be the first deep approach that outperforms the\npioneered non-deep LSD in all aspects we have tested, significantly expanding\nand reinforcing the versatility of the line geometry of images. Code and Models\nare available at https://github.com/ant-research/scalelsd", "comment": "accepted to CVPR 2025; 17 pages, appendices included", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09369v1", "AI": {"title_translation": "ScaleLSD: 流线型可扩展深度线段检测", "tldr": "ScaleLSD是一个可扩展的深度线段检测模型，它在超过1000万张未标记图像上进行自监督学习，在检测性能和各种几何任务中超越了传统的非深度方法，能够更完整和准确地表征图像的线几何。", "motivation": "旨在学习一个领域无关的鲁棒线段检测模型，该模型适用于任何自然图像，并能大规模地从图像中提取线几何特征。", "method": "通过重新审视和简化深度及非深度线段检测方法的基本设计，开发了一个名为ScaleLSD的高性能高效学习器，用于从超过1000万张未标记的真实世界图像中大规模地提取线几何特征，并采用可扩展的自监督学习。", "result": "ScaleLSD能够从任何自然图像中检测出比现有非深度LSD方法更多的线段，提供更完整和准确的图像几何表征。在零样本协议下，ScaleLSD在检测性能、单视图3D几何估计、双视图线段匹配和多视图3D线映射方面均表现出色，是第一个在所有测试方面都超越了传统非深度LSD方法的深度方法。", "conclusion": "ScaleLSD显著扩展并增强了图像线几何的通用性，是第一个在所有测试方面都超越了传统非深度LSD方法的深度方法。", "translation": "本文研究了图像中用于表征线几何的线段检测（LSD）问题，旨在学习一个领域无关的鲁棒LSD模型，该模型能很好地适用于任何自然图像。本文着重于可扩展的线段检测自监督学习，我们重新审视并简化了（深度和非深度）LSD方法的基本设计，以获得一个高性能且高效的LSD学习器，命名为ScaleLSD，用于从超过1000万张未标记的真实世界图像中大规模地提取线几何特征。我们的ScaleLSD在从任何自然图像中检测线段数量方面表现出色，甚至超过了开创性的非深度LSD方法，通过线段对图像进行了更完整和准确的几何表征。实验证明，我们提出的ScaleLSD在零样本协议下，在检测性能、单视图3D几何估计、双视图线段匹配和多视图3D线映射方面都得到了全面验证，所有方面都取得了优异的性能。基于彻底的评估，我们的ScaleLSD被认为是第一个在所有测试方面都超越了开创性非深度LSD的深度方法，显著扩展并增强了图像线几何的通用性。代码和模型可在https://github.com/ant-research/scalelsd 获取。", "summary": "本文提出了ScaleLSD，一个可扩展的深度线段检测模型，旨在学习领域无关的鲁棒LSD模型。它通过重新设计深度和非深度LSD方法，利用超过1000万张未标记图像进行自监督学习。实验证明，ScaleLSD在检测性能、单视图3D几何估计、双视图线段匹配和多视图3D线映射等零样本任务中表现出色，超越了传统的非深度LSD方法，提供了更完整和准确的线段检测，显著增强了图像线几何的通用性。", "keywords": "线段检测, 深度学习, 自监督学习, 图像几何, ScaleLSD", "comments": "ScaleLSD是第一个在所有测试方面均超越传统非深度线段检测方法的深度方法，其创新在于大规模自监督学习和对现有LSD设计的流线化。其重要性在于提供了一个领域无关、鲁棒且高效的线段检测模型，极大地扩展了图像线几何的应用范围。"}}
{"id": "2506.09183", "title": "Multi-Task Reward Learning from Human Ratings", "authors": ["Mingkang Wu", "Devin White", "Evelyn Rose", "Vernon Lawhern", "Nicholas R Waytowich", "Yongcan Cao"], "summary": "Reinforcement learning from human feeback (RLHF) has become a key factor in\naligning model behavior with users' goals. However, while humans integrate\nmultiple strategies when making decisions, current RLHF approaches often\nsimplify this process by modeling human reasoning through isolated tasks such\nas classification or regression. In this paper, we propose a novel\nreinforcement learning (RL) method that mimics human decision-making by jointly\nconsidering multiple tasks. Specifically, we leverage human ratings in\nreward-free environments to infer a reward function, introducing learnable\nweights that balance the contributions of both classification and regression\nmodels. This design captures the inherent uncertainty in human decision-making\nand allows the model to adaptively emphasize different strategies. We conduct\nseveral experiments using synthetic human ratings to validate the effectiveness\nof the proposed approach. Results show that our method consistently outperforms\nexisting rating-based RL methods, and in some cases, even surpasses traditional\nRL approaches.", "comment": "Accepted to the workshop on Models of Human Feedback for AI Alignment\n  at the 42nd International Conference on Machine Learning", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09183v1", "AI": {"title_translation": "基于人类评分的多任务奖励学习", "tldr": "本文提出一种新颖的多任务强化学习方法，通过整合分类和回归任务，从人类评分中学习奖励函数，解决了现有RLHF简化人类决策的问题，并取得了优于现有方法的性能。", "motivation": "当前的强化学习从人类反馈（RLHF）方法在建模人类推理时，倾向于通过孤立的任务（如分类或回归）来简化人类决策过程，而人类在实际决策中会整合多种策略，这导致模型无法充分捕捉人类决策的复杂性和不确定性。", "method": "本文提出了一种新颖的强化学习（RL）方法，通过联合考虑分类和回归任务来模仿人类决策。具体来说，它利用无奖励环境中的人类评分来推断奖励函数，并引入可学习的权重来平衡分类和回归模型对奖励函数的贡献。这种设计旨在捕捉人类决策中固有的不确定性，并允许模型自适应地强调不同的策略。", "result": "通过使用合成人类评分进行的实验表明，所提出的方法始终优于现有的基于评分的RL方法，并且在某些情况下，甚至超越了传统的RL方法。", "conclusion": "本研究提出的多任务奖励学习方法能够有效地模仿人类决策过程，通过整合分类和回归模型的贡献，显著提升了从人类评分中学习奖励函数的性能，优于现有RL方法。", "translation": "强化学习从人类反馈（RLHF）已成为使模型行为与用户目标对齐的关键因素。然而，尽管人类在做决策时会整合多种策略，但当前的RLHF方法通常通过分类或回归等孤立任务来建模人类推理，从而简化了这一过程。在本文中，我们提出了一种新颖的强化学习（RL）方法，通过联合考虑多任务来模仿人类决策。具体来说，我们利用无奖励环境中的人类评分来推断奖励函数，引入可学习的权重来平衡分类和回归模型的贡献。这种设计捕捉了人类决策中固有的不确定性，并允许模型自适应地强调不同的策略。我们使用合成人类评分进行了多项实验，以验证所提出方法的有效性。结果表明，我们的方法始终优于现有的基于评分的RL方法，并且在某些情况下，甚至超越了传统的RL方法。", "summary": "本文提出了一种新颖的多任务强化学习方法，旨在解决现有RLHF在建模人类决策时过度简化的局限性。该方法通过联合考虑分类和回归任务，并引入可学习权重来平衡两者贡献，从人类评分中推断奖励函数。实验结果表明，该方法在模拟人类决策方面表现出色，并显著优于现有基于评分的RL方法，甚至在某些情况下超越了传统RL方法。", "keywords": "强化学习, 人类反馈, 多任务学习, 奖励函数, 分类回归", "comments": "这项研究的创新之处在于其多任务学习框架，它更真实地模拟了人类决策的复杂性，通过结合分类和回归任务并引入可学习权重来提高奖励学习的准确性。这对于提升RLHF模型的对齐能力具有重要意义。"}}
{"id": "2506.09996", "title": "From Judgment to Interference: Early Stopping LLM Harmful Outputs via Streaming Content Monitoring", "authors": ["Yang Li", "Qiang Sheng", "Yehan Yang", "Xueyao Zhang", "Juan Cao"], "summary": "Though safety alignment has been applied to most large language models\n(LLMs), LLM service providers generally deploy a subsequent moderation as the\nexternal safety guardrail in real-world products. Existing moderators mainly\npractice a conventional full detection, which determines the harmfulness based\non the complete LLM output, causing high service latency. Recent works pay more\nattention to partial detection where moderators oversee the generation midway\nand early stop the output if harmfulness is detected, but they directly apply\nmoderators trained with the full detection paradigm to incomplete outputs,\nintroducing a training-inference gap that lowers the performance. In this\npaper, we explore how to form a data-and-model solution that natively supports\npartial detection. For the data, we construct FineHarm, a dataset consisting of\n29K prompt-response pairs with fine-grained annotations to provide reasonable\nsupervision for token-level training. Then, we propose the streaming content\nmonitor, which is trained with dual supervision of response- and token-level\nlabels and can follow the output stream of LLM to make a timely judgment of\nharmfulness. Experiments show that SCM gains 0.95+ in macro F1 score that is\ncomparable to full detection, by only seeing the first 18% of tokens in\nresponses on average. Moreover, the SCM can serve as a pseudo-harmfulness\nannotator for improving safety alignment and lead to a higher harmlessness\nscore than DPO.", "comment": "22 pages, 7 figures, and 9 tables", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.09996v1", "AI": {"title_translation": "从判断到干预：通过流式内容监控提前阻止大型语言模型有害输出", "tldr": "本研究提出了一种名为流式内容监控（SCM）的新方法，通过构建FineHarm数据集和双重监督训练，实现在LLM生成有害内容早期进行检测和阻止，显著降低延迟并保持高性能，并可辅助安全对齐。", "motivation": "现有的大型语言模型（LLM）安全审核主要依赖于对完整输出的检测，导致服务延迟较高。尽管近期工作尝试进行部分检测，但它们直接将为完整检测训练的审核器应用于不完整输出，导致训练与推理之间的差距，从而降低了性能。本研究旨在解决现有方法的延迟问题和性能差距，探索原生支持部分检测的数据和模型解决方案。", "method": "本研究提出了一种数据与模型相结合的解决方案，以原生支持部分检测。在数据方面，构建了FineHarm数据集，包含2.9万个带有细粒度标注的提示-响应对，为令牌级别的训练提供监督。在模型方面，提出了流式内容监控（SCM），该模型通过响应级别和令牌级别的双重监督进行训练，能够实时跟踪LLM的输出流并及时判断有害性。", "result": "实验结果表明，流式内容监控（SCM）在宏观F1分数上达到0.95+，与完整检测方法相当，但平均只需查看响应中前18%的令牌。此外，SCM还可以作为伪有害性标注器，用于改进安全对齐，并产生比DPO更高的无害性分数。", "conclusion": "本论文成功探索了一种原生支持部分检测的数据和模型解决方案，即流式内容监控（SCM），它能有效且高效地在大型语言模型生成有害输出的早期进行检测和阻止。SCM不仅提高了检测效率，还能辅助LLM的安全对齐。", "translation": "尽管安全对齐已应用于大多数大型语言模型（LLM），但LLM服务提供商通常在实际产品中部署后续审核作为外部安全护栏。现有审核器主要采用传统的完整检测，即根据完整的LLM输出判断有害性，这导致了较高的服务延迟。近期工作更关注部分检测，即审核器在生成过程中进行监督，如果检测到有害性则提前停止输出，但它们直接将使用完整检测范式训练的审核器应用于不完整输出，引入了训练-推理差距，从而降低了性能。在本文中，我们探讨了如何形成一种原生支持部分检测的数据和模型解决方案。在数据方面，我们构建了FineHarm，一个包含2.9万个带有细粒度标注的提示-响应对的数据集，为令牌级别训练提供合理的监督。然后，我们提出了流式内容监控器（SCM），该模型通过响应级别和令牌级别的双重监督进行训练，可以跟踪LLM的输出流，及时判断有害性。实验表明，SCM在宏观F1分数上达到0.95+，与完整检测相当，平均只需查看响应中前18%的令牌。此外，SCM可以作为伪有害性标注器，用于改进安全对齐，并产生比DPO更高的无害性分数。", "summary": "本论文提出了一种名为流式内容监控（SCM）的新方法，旨在解决大型语言模型（LLM）有害内容检测中的高延迟和训练-推理差距问题。通过构建包含细粒度标注的FineHarm数据集，并采用响应级别和令牌级别的双重监督训练SCM，该模型能够实时监控LLM的输出流并在有害内容生成早期进行判断和阻止。实验证明，SCM在仅查看响应中少量令牌的情况下，其检测性能与完整检测相当，并且能够辅助提升LLM的安全对齐效果。", "keywords": "大型语言模型, 有害内容检测, 早期停止, 流式内容监控, 安全对齐", "comments": "该论文的创新点在于提出了原生支持部分检测的数据和模型解决方案，有效地解决了现有LLM有害内容检测中存在的延迟高和训练-推理差距问题。通过引入细粒度的FineHarm数据集和双重监督训练的SCM模型，实现了在内容生成早期进行判断和阻止，极大地提升了检测效率和用户体验。此外，SCM还能作为伪标注器辅助安全对齐，具有重要的实际应用价值。"}}
{"id": "2506.09711", "title": "Non-Euclidean dual gradient ascent for entropically regularized linear and semidefinite programming", "authors": ["Yuhang Cai", "Michael Lindsey"], "summary": "We present an optimization framework that exhibits dimension-independent\nconvergence on a broad class of semidefinite programs (SDPs). Our approach\nfirst regularizes the primal problem with the von Neumann entropy, then solve\nthe regularized problem using dual gradient ascent with respect to a\nproblem-adapted norm. In particular, we show that the dual gradient norm\nconverges to zero at a rate independent of the ambient dimension and, via\nrounding arguments, construct primal-feasible solutions in certain special\ncases. We also derive explicit convergence rates for the objective. In order to\nachieve optimal computational scaling, we must accommodate the use of\nstochastic gradients constructed via randomized trace estimators. Throughout we\nillustrate the generality of our framework via three important special cases --\nthe Goemans-Williamson SDP relaxation of the Max-Cut problem, the optimal\ntransport linear program, and several SDP relaxations of the permutation\nsynchronization problem. Numerical experiments confirm that our methods achieve\ndimension-independent convergence in practice.", "comment": null, "cate": "math.OC", "url": "http://arxiv.org/abs/2506.09711v1", "AI": {"title_translation": "非欧几里得双梯度上升法用于熵正则化线性和半定规划", "tldr": "提出一种基于非欧几里得双梯度上升法的优化框架，解决了广义半定规划问题，实现了维度无关的收敛速度，并通过数值实验验证。", "motivation": "解决半定规划（SDPs）的优化问题，旨在实现维度无关的收敛性。", "method": "该方法首先使用冯·诺依曼熵对原始问题进行正则化，然后使用针对问题自适应范数的双梯度上升法求解正则化问题。为了实现最佳计算规模，还考虑了使用随机迹估计器构建的随机梯度。通过Max-Cut问题、最优传输线性规划和排列同步问题的SDP松弛等特殊情况说明了该框架的通用性。", "result": "该框架在广义半定规划（SDPs）上表现出维度无关的收敛性。双梯度范数以独立于环境维度的速度收敛到零，在某些特殊情况下通过舍入论证构建了原始可行解，并导出了目标函数的显式收敛率。数值实验证实了该方法在实践中实现了维度无关的收敛。", "conclusion": "该研究提出了一种有效的优化框架，通过非欧几里得双梯度上升法和熵正则化，解决了广义半定规划问题，实现了理论和实践上的维度无关收敛。", "translation": "我们提出了一个优化框架，该框架在广泛的半定规划（SDPs）类别上表现出与维度无关的收敛性。我们的方法首先使用冯·诺依曼熵对原始问题进行正则化，然后使用针对问题自适应范数的双梯度上升法求解正则化问题。特别是，我们表明双梯度范数以独立于环境维度的速度收敛到零，并且通过舍入论证，在某些特殊情况下构建了原始可行解。我们还导出了目标函数的显式收敛率。为了实现最佳计算规模，我们必须适应使用通过随机迹估计器构建的随机梯度。我们通过三个重要的特殊情况——Max-Cut问题的Goemans-Williamson SDP松弛、最优传输线性规划和排列同步问题的几个SDP松弛——说明了我们框架的通用性。数值实验证实我们的方法在实践中实现了维度无关的收敛。", "summary": "这篇论文提出了一种新的优化框架，利用非欧几里得双梯度上升法和冯·诺依曼熵正则化来解决广义半定规划（SDPs）。该方法在理论上证明了双梯度范数和目标函数的收敛速度与问题维度无关，并在特定情况下能构建原始可行解。为优化计算效率，引入了随机梯度。通过Max-Cut、最优传输和排列同步等问题，验证了该框架的通用性，数值实验也证实了其维度无关的收敛特性。", "keywords": "半定规划, 双梯度上升, 熵正则化, 维度无关收敛, 非欧几里得优化", "comments": "这项研究的创新之处在于结合了非欧几里得几何、熵正则化和双梯度上升法，为半定规划提供了一种新颖且高效的求解方法。其核心贡献是实现了维度无关的收敛速度，这对于处理大规模SDPs具有重要意义，因为它解决了传统方法在处理高维问题时面临的计算瓶颈。该框架的通用性通过多个经典优化问题的应用得到了验证，表明其在理论和实践上都具有广阔的应用前景。"}}
{"id": "2506.09800", "title": "Reinforced Refinement with Self-Aware Expansion for End-to-End Autonomous Driving", "authors": ["Haochen Liu", "Tianyu Li", "Haohan Yang", "Li Chen", "Caojun Wang", "Ke Guo", "Haochen Tian", "Hongchen Li", "Hongyang Li", "Chen Lv"], "summary": "End-to-end autonomous driving has emerged as a promising paradigm for\ndirectly mapping sensor inputs to planning maneuvers using learning-based\nmodular integrations. However, existing imitation learning (IL)-based models\nsuffer from generalization to hard cases, and a lack of corrective feedback\nloop under post-deployment. While reinforcement learning (RL) offers a\npotential solution to tackle hard cases with optimality, it is often hindered\nby overfitting to specific driving cases, resulting in catastrophic forgetting\nof generalizable knowledge and sample inefficiency. To overcome these\nchallenges, we propose Reinforced Refinement with Self-aware Expansion (R2SE),\na novel learning pipeline that constantly refines hard domain while keeping\ngeneralizable driving policy for model-agnostic end-to-end driving systems.\nThrough reinforcement fine-tuning and policy expansion that facilitates\ncontinuous improvement, R2SE features three key components: 1) Generalist\nPretraining with hard-case allocation trains a generalist imitation learning\n(IL) driving system while dynamically identifying failure-prone cases for\ntargeted refinement; 2) Residual Reinforced Specialist Fine-tuning optimizes\nresidual corrections using reinforcement learning (RL) to improve performance\nin hard case domain while preserving global driving knowledge; 3) Self-aware\nAdapter Expansion dynamically integrates specialist policies back into the\ngeneralist model, enhancing continuous performance improvement. Experimental\nresults in closed-loop simulation and real-world datasets demonstrate\nimprovements in generalization, safety, and long-horizon policy robustness over\nstate-of-the-art E2E systems, highlighting the effectiveness of reinforce\nrefinement for scalable autonomous driving.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.09800v1", "AI": {"title_translation": "端到端自动驾驶中基于自感知扩展的强化修正", "tldr": "R2SE是一个新的学习流程，通过强化微调和策略扩展，持续改进端到端自动驾驶系统在困难情况下的泛化能力和安全性。", "motivation": "现有的基于模仿学习（IL）的端到端自动驾驶模型在处理困难情况时泛化能力不足，并且在部署后缺乏纠正性反馈。而强化学习（RL）虽然能解决困难情况，但容易过拟合，导致泛化知识的灾难性遗忘和样本效率低下。", "method": "本文提出了R2SE（Reinforced Refinement with Self-aware Expansion）学习流程，通过强化微调和策略扩展实现持续改进，包含三个关键组件：1) 泛化预训练与困难案例分配：训练泛化模仿学习驾驶系统，同时动态识别易失败案例进行有针对性的修正；2) 残差强化专家微调：利用强化学习优化残差修正，提高困难案例领域的性能，同时保留全局驾驶知识；3) 自感知适配器扩展：动态将专家策略整合回泛化模型，增强持续性能提升。", "result": "在闭环仿真和真实世界数据集中，实验结果表明R2SE在泛化能力、安全性和长时程策略鲁棒性方面优于最先进的端到端系统。", "conclusion": "R2SE通过强化修正和自感知扩展，有效克服了现有端到端自动驾驶方法的局限性，显著提高了系统在困难情况下的泛化能力、安全性和鲁棒性，为可扩展的自动驾驶提供了有效途径。", "translation": "端到端自动驾驶已成为一种有前景的范式，它利用基于学习的模块化集成，直接将传感器输入映射到规划操作。然而，现有的基于模仿学习（IL）的模型在处理困难情况时泛化能力不足，并且在部署后缺乏纠正性反馈循环。尽管强化学习（RL）提供了解决困难情况并达到最优的潜在解决方案，但它常常受限于对特定驾驶案例的过拟合，导致通用知识的灾难性遗忘和样本效率低下。为了克服这些挑战，我们提出了R2SE（Reinforced Refinement with Self-aware Expansion），一个新颖的学习流程，它持续修正困难领域，同时为模型无关的端到端驾驶系统保留通用驾驶策略。通过促进持续改进的强化微调和策略扩展，R2SE具有三个关键组件：1）带有困难案例分配的泛化预训练，训练一个泛化模仿学习（IL）驾驶系统，同时动态识别易失败的案例以进行有针对性的修正；2）残差强化专家微调，使用强化学习（RL）优化残差修正，以提高困难案例领域的性能，同时保留全局驾驶知识；3）自感知适配器扩展，动态地将专家策略重新整合到泛化模型中，增强持续性能改进。在闭环仿真和真实世界数据集中的实验结果表明，R2SE在泛化能力、安全性和长时程策略鲁棒性方面优于最先进的端到端系统，突出了强化修正对于可扩展自动驾驶的有效性。", "summary": "本文提出了R2SE（Reinforced Refinement with Self-aware Expansion）框架，旨在解决现有端到端自动驾驶系统在泛化能力和样本效率方面的挑战。R2SE结合了模仿学习和强化学习的优点，通过泛化预训练识别困难案例、残差强化专家微调优化困难情况性能以及自感知适配器扩展整合专家策略，实现了对困难领域的持续修正，同时保持了通用驾驶策略。实验证明，R2SE在泛化、安全性和鲁棒性方面优于现有技术。", "keywords": "端到端自动驾驶, 强化学习, 模仿学习, 泛化, 策略修正", "comments": "本文提出了一种新颖的强化修正与自感知扩展的端到端自动驾驶学习框架R2SE，巧妙地结合了模仿学习和强化学习的优势，解决了传统方法在泛化能力和样本效率上的痛点。其核心创新在于动态识别困难案例并进行有针对性的强化修正，同时通过自感知扩展机制保证了通用知识的保留和持续改进。这对于提升自动驾驶系统在复杂现实世界场景中的可靠性和安全性具有重要意义。"}}
{"id": "2506.09378", "title": "UniForward: Unified 3D Scene and Semantic Field Reconstruction via Feed-Forward Gaussian Splatting from Only Sparse-View Images", "authors": ["Qijian Tian", "Xin Tan", "Jingyu Gong", "Yuan Xie", "Lizhuang Ma"], "summary": "We propose a feed-forward Gaussian Splatting model that unifies 3D scene and\nsemantic field reconstruction. Combining 3D scenes with semantic fields\nfacilitates the perception and understanding of the surrounding environment.\nHowever, key challenges include embedding semantics into 3D representations,\nachieving generalizable real-time reconstruction, and ensuring practical\napplicability by using only images as input without camera parameters or ground\ntruth depth. To this end, we propose UniForward, a feed-forward model to\npredict 3D Gaussians with anisotropic semantic features from only uncalibrated\nand unposed sparse-view images. To enable the unified representation of the 3D\nscene and semantic field, we embed semantic features into 3D Gaussians and\npredict them through a dual-branch decoupled decoder. During training, we\npropose a loss-guided view sampler to sample views from easy to hard,\neliminating the need for ground truth depth or masks required by previous\nmethods and stabilizing the training process. The whole model can be trained\nend-to-end using a photometric loss and a distillation loss that leverages\nsemantic features from a pre-trained 2D semantic model. At the inference stage,\nour UniForward can reconstruct 3D scenes and the corresponding semantic fields\nin real time from only sparse-view images. The reconstructed 3D scenes achieve\nhigh-quality rendering, and the reconstructed 3D semantic field enables the\nrendering of view-consistent semantic features from arbitrary views, which can\nbe further decoded into dense segmentation masks in an open-vocabulary manner.\nExperiments on novel view synthesis and novel view segmentation demonstrate\nthat our method achieves state-of-the-art performances for unifying 3D scene\nand semantic field reconstruction.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09378v1", "AI": {"title_translation": "UniForward：仅通过稀疏视图图像前馈高斯溅射统一三维场景和语义场重建", "tldr": "UniForward提出了一种前馈高斯溅射模型，仅利用稀疏视图图像，无需相机参数或深度真值，即可实时统一重建高质量三维场景和视图一致的语义场，并在新视角合成和分割任务上达到SOTA性能。", "motivation": "将三维场景与语义场结合有助于感知和理解周围环境。然而，主要挑战在于将语义嵌入三维表示、实现可泛化的实时重建，以及仅使用图像作为输入（无需相机参数或深度真值）来确保实际适用性。", "method": "本文提出了UniForward，一个前馈模型，可以仅从未校准和未定位的稀疏视图图像预测具有各向异性语义特征的三维高斯。为实现三维场景和语义场的统一表示，我们将语义特征嵌入三维高斯并通过双分支解耦解码器进行预测。训练过程中，提出了一种损失引导的视图采样器，从易到难采样视图，消除了以往方法所需的深度真值或掩码，并稳定了训练过程。整个模型可以通过光度损失和利用预训练二维语义模型语义特征的蒸馏损失进行端到端训练。", "result": "在推理阶段，UniForward可以仅从稀疏视图图像实时重建三维场景和相应的语义场。重建的三维场景实现了高质量渲染，重建的三维语义场可以从任意视角渲染视图一致的语义特征，这些特征可以进一步解码为开放词汇的密集分割掩码。在新视角合成和新视角分割实验中，我们的方法在统一三维场景和语义场重建方面达到了最先进的性能。", "conclusion": "UniForward通过前馈高斯溅射实现了三维场景和语义场的统一重建，仅依赖稀疏视图图像，无需相机参数和深度真值，达到了实时、高质量的重建效果，并在相关任务中表现出最先进的性能。", "translation": "我们提出了一种前馈高斯溅射模型，它统一了三维场景和语义场重建。将三维场景与语义场结合有助于感知和理解周围环境。然而，主要挑战包括将语义嵌入三维表示、实现可泛化的实时重建，以及通过仅使用图像作为输入（无需相机参数或深度真值）来确保实际适用性。为此，我们提出了UniForward，一个前馈模型，可以仅从非校准和未定位的稀疏视图图像预测具有各向异性语义特征的三维高斯。为了实现三维场景和语义场的统一表示，我们将语义特征嵌入三维高斯并通过双分支解耦解码器进行预测。在训练过程中，我们提出了一种损失引导的视图采样器，从易到难采样视图，消除了以往方法所需的深度真值或掩码，并稳定了训练过程。整个模型可以利用光度损失和利用预训练二维语义模型语义特征的蒸馏损失进行端到端训练。在推理阶段，我们的UniForward可以仅从稀疏视图图像实时重建三维场景和相应的语义场。重建的三维场景实现了高质量渲染，重建的三维语义场可以从任意视角渲染视图一致的语义特征，这些特征可以进一步解码为开放词汇的密集分割掩码。在新视角合成和新视角分割实验中，我们的方法在统一三维场景和语义场重建方面达到了最先进的性能。", "summary": "本文提出了UniForward，一个创新的前馈高斯溅射模型，旨在统一三维场景和语义场的重建。该模型克服了现有方法的局限性，仅利用未校准和未定位的稀疏视图图像作为输入，即可实时生成高质量的三维场景和视图一致的语义特征。UniForward通过将语义特征嵌入三维高斯并采用双分支解耦解码器实现统一表示，同时引入损失引导的视图采样器稳定训练。实验证明，UniForward在新视角合成和语义分割任务上均达到了最先进的性能，为环境感知和理解提供了高效实用的解决方案。", "keywords": "三维场景重建, 语义场, 高斯溅射, 稀疏视图图像, 前馈模型", "comments": "UniForward的创新点在于其统一的三维场景和语义场重建方法，特别是在仅使用稀疏、未校准图像作为输入的情况下实现实时且高质量的重建。其前馈高斯溅射模型结合语义特征嵌入和双分支解码器，有效解决了语义与三维表示结合的挑战。损失引导的视图采样器也提升了训练的鲁棒性。该方法在实际应用中具有重要意义，因为它降低了对输入数据（如相机参数和深度真值）的要求，同时在新视角合成和语义分割方面达到了SOTA，展现了强大的泛化能力和实用性。"}}
{"id": "2506.09193", "title": "LaDCast: A Latent Diffusion Model for Medium-Range Ensemble Weather Forecasting", "authors": ["Yilin Zhuang", "Karthik Duraisamy"], "summary": "Accurate probabilistic weather forecasting demands both high accuracy and\nefficient uncertainty quantification, challenges that overburden both ensemble\nnumerical weather prediction (NWP) and recent machine-learning methods. We\nintroduce LaDCast, the first global latent-diffusion framework for medium-range\nensemble forecasting, which generates hourly ensemble forecasts entirely in a\nlearned latent space. An autoencoder compresses high-dimensional ERA5\nreanalysis fields into a compact representation, and a transformer-based\ndiffusion model produces sequential latent updates with arbitrary hour\ninitialization. The model incorporates Geometric Rotary Position Embedding\n(GeoRoPE) to account for the Earth's spherical geometry, a dual-stream\nattention mechanism for efficient conditioning, and sinusoidal temporal\nembeddings to capture seasonal patterns. LaDCast achieves deterministic and\nprobabilistic skill close to that of the European Centre for Medium-Range\nForecast IFS-ENS, without any explicit perturbations. Notably, LaDCast\ndemonstrates superior performance in tracking rare extreme events such as\ncyclones, capturing their trajectories more accurately than established models.\nBy operating in latent space, LaDCast reduces storage and compute by orders of\nmagnitude, demonstrating a practical path toward forecasting at kilometer-scale\nresolution in real time. We open-source our code and models and provide the\ntraining and evaluation pipelines at: https://github.com/tonyzyl/ladcast.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09193v1", "AI": {"title_translation": "LaDCast：一种用于中期集合天气预报的潜在扩散模型", "tldr": "LaDCast是一个新的潜在扩散模型，能高效地进行中期集合天气预报，其准确性接近现有顶尖模型，并在追踪极端事件方面表现更优，同时大幅降低计算和存储需求。", "motivation": "准确的概率天气预报需要高精度和高效的不确定性量化，而现有的集合数值天气预报（NWP）和机器学习方法都面临挑战。", "method": "本文引入了LaDCast，这是第一个用于中期集合预报的全球潜在扩散框架。它通过一个自编码器将高维ERA5再分析场压缩到紧凑的潜在表示空间，然后一个基于Transformer的扩散模型在学习到的潜在空间中生成每小时的集合预报。模型融入了几何旋转位置嵌入（GeoRoPE）来处理地球的球面几何，一个双流注意力机制用于高效条件化，以及正弦时间嵌入来捕捉季节模式。", "result": "LaDCast在确定性和概率技能方面接近欧洲中期天气预报中心（ECMWF）的IFS-ENS模型，且无需显式扰动。值得注意的是，LaDCast在追踪气旋等罕见极端事件方面表现出卓越的性能，比现有模型更准确地捕捉其轨迹。通过在潜在空间中操作，LaDCast将存储和计算量减少了几个数量级。", "conclusion": "LaDCast为实现实时千米级分辨率的预报提供了一条可行的路径。", "translation": "准确的概率天气预报既要求高精度又要求高效的不确定性量化，这些挑战使得集合数值天气预报（NWP）和近期机器学习方法都负担过重。我们引入了LaDCast，这是第一个用于中期集合预报的全球潜在扩散框架，它完全在学习到的潜在空间中生成每小时的集合预报。一个自编码器将高维ERA5再分析场压缩成紧凑的表示，一个基于Transformer的扩散模型生成具有任意小时初始化的序列潜在更新。该模型结合了几何旋转位置嵌入（GeoRoPE）以考虑地球的球面几何，一个用于高效条件化的双流注意力机制，以及正弦时间嵌入以捕捉季节模式。LaDCast在确定性和概率技能方面接近欧洲中期天气预报中心IFS-ENS，且无需任何显式扰动。值得注意的是，LaDCast在追踪气旋等罕见极端事件方面表现出卓越的性能，比现有模型更准确地捕捉其轨迹。通过在潜在空间中操作，LaDCast将存储和计算量减少了几个数量级，展示了实时实现千米级分辨率预报的实用路径。我们开源了代码和模型，并提供了训练和评估流程，网址为：https://github.com/tonyzyl/ladcast。", "summary": "本文提出了LaDCast，一种基于潜在扩散模型的全球中期集合天气预报框架。该模型通过自编码器将ERA5数据压缩到潜在空间，并利用基于Transformer的扩散模型生成预报，同时结合了GeoRoPE和双流注意力机制。LaDCast在预报准确性上接近ECMWF的IFS-ENS，尤其在追踪极端事件方面表现更优，并显著降低了计算和存储成本，为实现高分辨率实时预报提供了新途径。", "keywords": "天气预报, 潜在扩散模型, 集合预报, 机器学习, 极端事件", "comments": "LaDCast的创新之处在于首次将潜在扩散模型应用于全球中期集合天气预报，并在潜在空间中进行操作，这显著降低了计算和存储需求。其在极端事件追踪上的优异表现也值得关注。该研究为未来高分辨率、实时天气预报提供了重要的技术突破，并开源了代码和模型，有利于社区的进一步研究和应用。"}}
{"id": "2506.09775", "title": "The Intrinsic Riemannian Proximal Gradient Method for Nonconvex Optimization", "authors": ["Ronny Bergmann", "Hajg Jasa", "Paula John", "Max Pfeffer"], "summary": "We consider the proximal gradient method on Riemannian manifolds for\nfunctions that are possibly not geodesically convex. Starting from the\nforward-backward-splitting, we define an intrinsic variant of the proximal\ngradient method that uses proximal maps defined on the manifold and therefore\ndoes not require or work in the embedding. We investigate its convergence\nproperties and illustrate its numerical performance, particularly for nonconvex\nor nonembedded problems that are hence out of reach for other methods.", "comment": null, "cate": "math.OC", "url": "http://arxiv.org/abs/2506.09775v1", "AI": {"title_translation": "非凸优化的内在黎曼近端梯度法", "tldr": "本文提出了一种内在黎曼近端梯度法，用于解决黎曼流形上非测地凸函数的优化问题，特别适用于其他方法无法处理的非凸或非嵌入问题。", "motivation": "现有黎曼流形上的近端梯度法可能不适用于非测地凸函数，且通常需要在嵌入空间中操作。本文的动机是开发一种不依赖于嵌入且能处理非凸问题的内在方法。", "method": "该方法从前向-后向分裂出发，定义了一种内在的近端梯度变体，其近端映射直接定义在流形上，因此不需要或不依赖于嵌入空间。", "result": "研究了该方法的收敛性，并展示了其数值性能，特别是在其他方法无法处理的非凸或非嵌入问题上的表现。", "conclusion": "本文提出的内在黎曼近端梯度法能够有效地解决黎曼流形上非测地凸的非凸或非嵌入优化问题，填补了现有方法的空白。", "translation": "我们考虑了黎曼流形上可能非测地凸函数的近端梯度法。从前向-后向分裂开始，我们定义了一种近端梯度法的内在变体，它使用在流形上定义的近端映射，因此不需要或不作用于嵌入空间。我们研究了它的收敛性，并阐明了它的数值性能，特别是对于其他方法无法触及的非凸或非嵌入问题。", "summary": "本文提出了一种用于黎曼流形上非测地凸函数的内在黎曼近端梯度法。该方法基于流形上定义的近端映射，无需嵌入空间。研究结果表明，该方法具有良好的收敛性和数值性能，尤其适用于传统方法难以处理的非凸或非嵌入优化问题。", "keywords": "黎曼优化, 近端梯度法, 非凸优化, 流形, 收敛性", "comments": "该论文的创新之处在于提出了一种完全内在的黎曼近端梯度法，避免了在嵌入空间中操作的需要，从而能够处理更广泛的非凸和非嵌入问题，拓展了黎曼优化方法的应用范围。"}}
{"id": "2506.09803", "title": "Devil's Hand: Data Poisoning Attacks to Locally Private Graph Learning Protocols", "authors": ["Longzhu He", "Chaozhuo Li", "Peng Tang", "Litian Zhang", "Sen Su"], "summary": "Graph neural networks (GNNs) have achieved significant success in graph\nrepresentation learning and have been applied to various domains. However, many\nreal-world graphs contain sensitive personal information, such as user profiles\nin social networks, raising serious privacy concerns when graph learning is\nperformed using GNNs. To address this issue, locally private graph learning\nprotocols have gained considerable attention. These protocols leverage the\nprivacy advantages of local differential privacy (LDP) and the effectiveness of\nGNN's message-passing in calibrating noisy data, offering strict privacy\nguarantees for users' local data while maintaining high utility (e.g., node\nclassification accuracy) for graph learning. Despite these advantages, such\nprotocols may be vulnerable to data poisoning attacks, a threat that has not\nbeen considered in previous research. Identifying and addressing these threats\nis crucial for ensuring the robustness and security of privacy-preserving graph\nlearning frameworks. This work introduces the first data poisoning attack\ntargeting locally private graph learning protocols. The attacker injects fake\nusers into the protocol, manipulates these fake users to establish links with\ngenuine users, and sends carefully crafted data to the server, ultimately\ncompromising the utility of private graph learning. The effectiveness of the\nattack is demonstrated both theoretically and empirically. In addition, several\ndefense strategies have also been explored, but their limited effectiveness\nhighlights the need for more robust defenses.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09803v1", "AI": {"title_translation": "魔鬼之手：针对局部私有图学习协议的数据投毒攻击", "tldr": "本文首次提出了针对局部私有图学习协议的数据投毒攻击，通过注入虚假用户和精心构造的数据来损害其效用，并指出需要更强大的防御措施。", "motivation": "图神经网络（GNNs）在图表示学习中取得了显著成功，但在涉及敏感个人信息的真实世界图中，其隐私问题日益突出。为了解决这一问题，局部私有图学习协议被提出，它结合了局部差分隐私（LDP）和GNNs的消息传递机制，在提供严格隐私保障的同时保持了高实用性。然而，现有研究尚未考虑这些协议可能面临数据投毒攻击的威胁，识别和解决这些威胁对于确保保护隐私的图学习框架的鲁棒性和安全性至关重要。", "method": "攻击者通过向协议中注入虚假用户，操纵这些虚假用户与真实用户建立链接，并向服务器发送精心构造的数据，最终损害私有图学习的效用。", "result": "攻击的有效性得到了理论和经验的证明。此外，本文还探索了几种防御策略，但其有限的有效性凸显了对更强大防御措施的需求。", "conclusion": "针对局部私有图学习协议的数据投毒攻击是可行且有效的，现有防御措施的有效性有限，因此迫切需要开发更强大的防御策略来确保隐私保护图学习框架的鲁棒性和安全性。", "translation": "图神经网络（GNNs）在图表示学习中取得了显著成功，并已应用于各个领域。然而，许多真实世界的图包含敏感的个人信息，例如社交网络中的用户资料，这在使用GNNs进行图学习时引发了严重的隐私问题。为了解决这个问题，局部私有图学习协议受到了广泛关注。这些协议利用了局部差分隐私（LDP）的隐私优势和GNN消息传递在校准噪声数据方面的有效性，为用户的本地数据提供了严格的隐私保障，同时保持了图学习的高实用性（例如，节点分类准确性）。尽管有这些优点，此类协议可能容易受到数据投毒攻击，这是以前研究中未曾考虑的威胁。识别和解决这些威胁对于确保保护隐私的图学习框架的鲁棒性和安全性至关重要。这项工作首次引入了针对局部私有图学习协议的数据投毒攻击。攻击者向协议中注入虚假用户，操纵这些虚假用户与真实用户建立链接，并向服务器发送精心构造的数据，最终损害私有图学习的效用。攻击的有效性得到了理论和经验的证明。此外，本文还探索了几种防御策略，但其有限的有效性凸显了对更强大防御措施的需求。", "summary": "图神经网络在处理敏感图数据时面临隐私挑战，局部私有图学习协议应运而生，旨在提供隐私保护。然而，本研究首次揭示了这些协议易受数据投毒攻击。攻击者通过注入虚假用户并精心构造数据来破坏私有图学习的效用。研究通过理论和实验证明了攻击的有效性，并指出现有防御措施的局限性，强调了开发更鲁棒防御策略的必要性。", "keywords": "数据投毒攻击, 局部私有图学习, 图神经网络, 隐私保护, 对抗性机器学习", "comments": "本文首次提出了针对局部私有图学习协议的数据投毒攻击，填补了该领域的一个重要空白，揭示了隐私保护图学习框架中潜在的安全漏洞。这项工作对于推动安全和隐私保护的图学习研究具有重要意义，提醒研究人员在设计此类协议时需充分考虑对抗性攻击。"}}
{"id": "2506.09859", "title": "Hierarchical Learning-Enhanced MPC for Safe Crowd Navigation with Heterogeneous Constraints", "authors": ["Huajian Liu", "Yixuan Feng", "Wei Dong", "Kunpeng Fan", "Chao Wang", "Yongzhuo Gao"], "summary": "In this paper, we propose a novel hierarchical framework for robot navigation\nin dynamic environments with heterogeneous constraints. Our approach leverages\na graph neural network trained via reinforcement learning (RL) to efficiently\nestimate the robot's cost-to-go, formulated as local goal recommendations. A\nspatio-temporal path-searching module, which accounts for kinematic\nconstraints, is then employed to generate a reference trajectory to facilitate\nsolving the non-convex optimization problem used for explicit constraint\nenforcement. More importantly, we introduce an incremental action-masking\nmechanism and a privileged learning strategy, enabling end-to-end training of\nthe proposed planner. Both simulation and real-world experiments demonstrate\nthat the proposed method effectively addresses local planning in complex\ndynamic environments, achieving state-of-the-art (SOTA) performance. Compared\nwith existing learning-optimization hybrid methods, our approach eliminates the\ndependency on high-fidelity simulation environments, offering significant\nadvantages in computational efficiency and training scalability. The code will\nbe released as open-source upon acceptance of the paper.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.09859v1", "AI": {"title_translation": "分层学习增强型MPC用于异构约束下的安全人群导航", "tldr": "本文提出一种分层学习增强型MPC框架，通过GNN和RL实现机器人异构约束下的安全导航，无需高保真模拟，效率高且性能SOTA。", "motivation": "在具有异构约束的动态环境中实现机器人安全导航，并有效解决复杂动态环境中的局部规划问题。", "method": "提出一种新颖的分层框架，结合了通过强化学习（RL）训练的图神经网络（GNN）来有效估计机器人的“成本到目标”并提供局部目标推荐；一个考虑运动学约束的时空路径搜索模块来生成参考轨迹，以辅助解决用于显式约束执行的非凸优化问题；以及引入增量动作掩蔽机制和特权学习策略，实现端到端训练。", "result": "在复杂动态环境中有效解决了局部规划问题，实现了最先进（SOTA）的性能。与现有学习优化混合方法相比，消除了对高保真模拟环境的依赖，并在计算效率和训练可扩展性方面具有显著优势。", "conclusion": "该方法通过分层学习增强型MPC框架，结合GNN和RL，成功实现了在异构约束动态环境中机器人安全导航，并表现出SOTA性能，同时解决了对高保真模拟环境的依赖问题。", "translation": "在本文中，我们提出了一种新颖的分层框架，用于机器人在具有异构约束的动态环境中进行导航。我们的方法利用通过强化学习（RL）训练的图神经网络（GNN）来有效估计机器人的“成本到目标”，并将其表述为局部目标推荐。随后，采用一个考虑运动学约束的时空路径搜索模块来生成参考轨迹，以促进解决用于显式约束执行的非凸优化问题。更重要的是，我们引入了一种增量动作掩蔽机制和特权学习策略，从而实现了所提出规划器的端到端训练。仿真和真实世界实验都表明，所提出的方法有效解决了复杂动态环境中的局部规划问题，实现了最先进（SOTA）的性能。与现有的学习优化混合方法相比，我们的方法消除了对高保真模拟环境的依赖，在计算效率和训练可扩展性方面提供了显著优势。代码将在论文被接受后开源。", "summary": "本文提出了一种用于机器人安全导航的分层框架，该框架在动态环境中处理异构约束。它结合了通过强化学习训练的图神经网络来估计成本和提供局部目标，以及一个时空路径搜索模块来生成参考轨迹。通过引入增量动作掩蔽和特权学习策略，实现了端到端训练。实验证明，该方法在复杂动态环境中表现出最先进的性能，并且与现有方法相比，其优势在于无需高保真模拟，显著提高了计算效率和训练可扩展性。", "keywords": "分层学习, 模型预测控制 (MPC), 机器人导航, 异构约束, 强化学习, 图神经网络", "comments": "这篇论文的创新点在于其分层学习增强型MPC框架，特别是结合GNN和RL来估计成本和生成局部目标推荐。此外，引入增量动作掩蔽和特权学习策略实现了端到端训练，并显著降低了对高保真模拟环境的依赖，这对于实际部署和提高训练效率具有重要意义。在复杂动态环境下的SOTA性能也证明了其有效性。"}}
{"id": "2506.09385", "title": "ReID5o: Achieving Omni Multi-modal Person Re-identification in a Single Model", "authors": ["Jialong Zuo", "Yongtai Deng", "Mengdan Tan", "Rui Jin", "Dongyue Wu", "Nong Sang", "Liang Pan", "Changxin Gao"], "summary": "In real-word scenarios, person re-identification (ReID) expects to identify a\nperson-of-interest via the descriptive query, regardless of whether the query\nis a single modality or a combination of multiple modalities. However, existing\nmethods and datasets remain constrained to limited modalities, failing to meet\nthis requirement. Therefore, we investigate a new challenging problem called\nOmni Multi-modal Person Re-identification (OM-ReID), which aims to achieve\neffective retrieval with varying multi-modal queries. To address dataset\nscarcity, we construct ORBench, the first high-quality multi-modal dataset\ncomprising 1,000 unique identities across five modalities: RGB, infrared, color\npencil, sketch, and textual description. This dataset also has significant\nsuperiority in terms of diversity, such as the painting perspectives and\ntextual information. It could serve as an ideal platform for follow-up\ninvestigations in OM-ReID. Moreover, we propose ReID5o, a novel multi-modal\nlearning framework for person ReID. It enables synergistic fusion and\ncross-modal alignment of arbitrary modality combinations in a single model,\nwith a unified encoding and multi-expert routing mechanism proposed. Extensive\nexperiments verify the advancement and practicality of our ORBench. A wide\nrange of possible models have been evaluated and compared on it, and our\nproposed ReID5o model gives the best performance. The dataset and code will be\nmade publicly available at https://github.com/Zplusdragon/ReID5o_ORBench.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09385v1", "AI": {"title_translation": "ReID5o：在单一模型中实现全方位多模态行人重识别", "tldr": "本文提出了一种名为全方位多模态行人重识别（OM-ReID）的新问题，构建了首个高质量的多模态数据集ORBench，并提出了ReID5o框架，以在单一模型中有效处理任意多模态查询。", "motivation": "在真实场景中，行人重识别（ReID）需要通过描述性查询来识别目标人物，无论查询是单一模态还是多模态组合。然而，现有方法和数据集受限于有限的模态，未能满足这一要求。", "method": "为解决数据集稀缺问题，作者构建了ORBench，这是第一个包含RGB、红外、彩色铅笔画、草图和文本描述五种模态的1000个独特身份的高质量多模态数据集。此外，作者提出了ReID5o，一个新颖的多模态行人重识别学习框架，该框架通过统一编码和多专家路由机制，在单一模型中实现任意模态组合的协同融合和跨模态对齐。", "result": "广泛的实验验证了ORBench的先进性和实用性。在ORBench上评估了各种可能的模型，作者提出的ReID5o模型表现最佳。", "conclusion": "本文提出了全方位多模态行人重识别（OM-ReID）这一新挑战性问题，构建了首个高质量多模态数据集ORBench，并提出了新颖的ReID5o多模态学习框架。实验证明了所提数据集和模型的优越性。", "translation": "在真实世界场景中，行人重识别（ReID）期望通过描述性查询来识别目标人物，无论查询是单一模态还是多种模态的组合。然而，现有方法和数据集仍然受限于有限的模态，未能满足这一要求。因此，我们研究了一个新的挑战性问题，称为全方位多模态行人重识别（OM-ReID），旨在实现对不同多模态查询的有效检索。为了解决数据集稀缺问题，我们构建了ORBench，这是第一个高质量的多模态数据集，包含1000个独特身份，涵盖五种模态：RGB、红外、彩色铅笔画、草图和文本描述。该数据集在多样性方面也具有显著优势，例如绘画视角和文本信息。它可以作为OM-ReID后续研究的理想平台。此外，我们提出了ReID5o，一个新颖的行人重识别多模态学习框架。它通过统一编码和多专家路由机制，在一个单一模型中实现任意模态组合的协同融合和跨模态对齐。大量的实验验证了我们ORBench的先进性和实用性。在ORBench上评估了各种可能的模型，我们提出的ReID5o模型表现最佳。数据集和代码将在https://github.com/Zplusdragon/ReID5o_ORBench公开。", "summary": "本文提出并研究了全方位多模态行人重识别（OM-ReID）这一新问题，旨在通过任意多模态查询实现有效的行人检索。为解决现有方法和数据集的局限性，作者构建了ORBench，这是首个包含五种模态（RGB、红外、彩色铅笔画、草图和文本描述）的高质量多模态数据集。此外，作者提出了ReID5o，一个新颖的多模态学习框架，它通过统一编码和多专家路由机制，在单一模型中实现任意模态组合的协同融合和跨模态对齐。实验结果表明，ORBench具有先进性和实用性，且ReID5o模型在其中表现最佳。", "keywords": "行人重识别, 多模态学习, 数据集, 模态融合, ReID5o", "comments": "该论文的创新点在于提出了全方位多模态行人重识别（OM-ReID）这一新颖且具有挑战性的问题，并首次构建了一个包含五种丰富模态的高质量数据集ORBench。同时，提出的ReID5o框架能够在一个单一模型中处理任意模态组合的融合与对齐，这对于实际应用中复杂多变的查询场景具有重要意义。这为未来的多模态ReID研究奠定了坚实的基础，并有望推动该领域的发展。"}}
{"id": "2506.09399", "title": "Improving Out-of-Distribution Detection via Dynamic Covariance Calibration", "authors": ["Kaiyu Guo", "Zijian Wang", "Brian C. Lovell", "Mahsa Baktashmotlagh"], "summary": "Out-of-Distribution (OOD) detection is essential for the trustworthiness of\nAI systems. Methods using prior information (i.e., subspace-based methods) have\nshown effective performance by extracting information geometry to detect OOD\ndata with a more appropriate distance metric. However, these methods fail to\naddress the geometry distorted by ill-distributed samples, due to the\nlimitation of statically extracting information geometry from the training\ndistribution. In this paper, we argue that the influence of ill-distributed\nsamples can be corrected by dynamically adjusting the prior geometry in\nresponse to new data. Based on this insight, we propose a novel approach that\ndynamically updates the prior covariance matrix using real-time input features,\nrefining its information. Specifically, we reduce the covariance along the\ndirection of real-time input features and constrain adjustments to the residual\nspace, thus preserving essential data characteristics and avoiding effects on\nunintended directions in the principal space. We evaluate our method on two\npre-trained models for the CIFAR dataset and five pre-trained models for\nImageNet-1k, including the self-supervised DINO model. Extensive experiments\ndemonstrate that our approach significantly enhances OOD detection across\nvarious models. The code is released at https://github.com/workerbcd/ooddcc.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09399v1", "AI": {"title_translation": "通过动态协方差校准改进分布外检测", "tldr": "本文提出了一种通过动态调整先验协方差矩阵来改进分布外检测的新方法，有效解决了现有方法在处理病态分布样本时几何失真的问题。", "motivation": "现有的基于子空间的分布外检测方法通过提取信息几何来检测OOD数据，但由于静态提取信息几何的限制，无法解决由病态分布样本引起的几何失真问题。", "method": "提出了一种新方法，通过使用实时输入特征动态更新先验协方差矩阵，从而优化其信息。具体来说，沿实时输入特征的方向减小协方差，并将调整限制在残差空间中，以保留数据特征并避免影响主空间中的非预期方向。", "result": "在CIFAR数据集的两个预训练模型和ImageNet-1k的五个预训练模型（包括自监督DINO模型）上进行了评估，实验证明该方法显著增强了各种模型的OOD检测性能。", "conclusion": "通过动态调整先验几何，可以有效纠正病态分布样本的影响，从而显著提高分布外检测的性能。", "translation": "分布外检测（OOD）对于AI系统的可信度至关重要。使用先验信息的方法（即基于子空间的方法）通过提取信息几何来检测OOD数据，并使用更合适的距离度量，显示出有效的性能。然而，由于从训练分布中静态提取信息几何的限制，这些方法未能解决由病态分布样本引起的几何失真问题。在本文中，我们认为可以通过动态调整先验几何以响应新数据来纠正病态分布样本的影响。基于这一见解，我们提出了一种新颖的方法，该方法使用实时输入特征动态更新先验协方差矩阵，从而优化其信息。具体来说，我们沿实时输入特征的方向减小协方差，并将调整限制在残差空间中，从而保留了基本数据特征并避免了对主空间中非预期方向的影响。我们在CIFAR数据集的两个预训练模型和ImageNet-1k的五个预训练模型（包括自监督DINO模型）上评估了我们的方法。广泛的实验表明，我们的方法显著增强了各种模型的OOD检测能力。代码已在https://github.com/workerbcd/ooddcc 发布。", "summary": "本文提出了一种新颖的动态协方差校准方法来改进分布外检测（OOD）。针对现有基于子空间方法无法处理病态分布样本导致的几何失真问题，该方法通过实时输入特征动态更新先验协方差矩阵，并通过沿特征方向减小协方差并限制残差空间调整来保留数据特性。实验证明，该方法在多个数据集和模型上显著提升了OOD检测性能。", "keywords": "分布外检测, 协方差校准, 信息几何, 实时输入, 可信赖AI", "comments": "这篇论文通过引入动态协方差校准机制，创新性地解决了现有基于子空间的OOD检测方法在处理病态分布样本时遇到的几何失真问题。其核心思想是根据实时输入数据动态调整先验信息几何，这比静态方法更具鲁棒性和适应性。该方法对于提升AI系统在实际应用中的可信赖性具有重要意义。"}}
{"id": "2506.09200", "title": "FedRAG: A Framework for Fine-Tuning Retrieval-Augmented Generation Systems", "authors": ["Val Andrei Fajardo", "David B. Emerson", "Amandeep Singh", "Veronica Chatrath", "Marcelo Lotif", "Ravi Theja", "Alex Cheung", "Izuki Matsubi"], "summary": "Retrieval-augmented generation (RAG) systems have been shown to be effective\nin addressing many of the drawbacks of relying solely on the parametric memory\nof large language models. Recent work has demonstrated that RAG systems can be\nimproved via fine-tuning of their retriever and generator models. In this work,\nwe introduce FedRAG, a framework for fine-tuning RAG systems across centralized\nand federated architectures. FedRAG supports state-of-the-art fine-tuning\nmethods, offering a simple and intuitive interface and a seamless conversion\nfrom centralized to federated training tasks. FedRAG is also deeply integrated\nwith the modern RAG ecosystem, filling a critical gap in available tools.", "comment": "9 pages, 4 figures, 2 tables. Accepted for the CODEML Workshop at\n  ICML 2025. Framework code available at\n  https://github.com/VectorInstitute/fed-rag", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09200v1", "AI": {"title_translation": "FedRAG: 一个用于微调检索增强生成系统的框架", "tldr": "FedRAG是一个用于在集中式和联邦式架构下微调检索增强生成（RAG）系统的框架，它支持最先进的微调方法，并填补了现有工具的空白。", "motivation": "现有的检索增强生成（RAG）系统虽然能有效解决大型语言模型参数记忆的局限性，但仍可通过微调检索器和生成器模型来改进。目前缺乏一个能够支持在集中式和联邦式架构下对RAG系统进行微调的集成框架。", "method": "本文引入了FedRAG，这是一个用于在集中式和联邦式架构下微调RAG系统的框架。FedRAG支持最先进的微调方法，提供简单直观的界面，并能实现从集中式到联邦式训练任务的无缝转换。此外，FedRAG与现代RAG生态系统深度整合。", "result": "Not mentioned in abstract", "conclusion": "FedRAG框架通过支持集中式和联邦式架构下的RAG系统微调，并与现代RAG生态系统深度整合，填补了现有工具的关键空白，从而为RAG系统的改进提供了重要的支持。", "translation": "检索增强生成（RAG）系统已被证明能有效解决仅依赖大型语言模型参数记忆所带来的许多缺点。最近的研究表明，通过微调其检索器和生成器模型，RAG系统可以得到改进。在这项工作中，我们引入了FedRAG，这是一个用于在集中式和联邦式架构下微调RAG系统的框架。FedRAG支持最先进的微调方法，提供简单直观的界面，并能实现从集中式到联邦式训练任务的无缝转换。FedRAG还与现代RAG生态系统深度整合，填补了现有工具中的关键空白。", "summary": "本文介绍了FedRAG，一个旨在微调检索增强生成（RAG）系统的框架。FedRAG支持在集中式和联邦式架构下对RAG系统的检索器和生成器进行微调，它集成了最先进的微调方法，提供用户友好的界面，并能无缝转换训练任务。该框架填补了当前RAG生态系统在工具方面的关键空白。", "keywords": "FedRAG, 检索增强生成, 微调, 联邦学习, 大语言模型", "comments": "FedRAG的创新之处在于其将RAG系统微调的能力扩展到联邦学习架构，这对于保护数据隐私和在分布式环境中利用异构数据进行模型优化具有重要意义。它填补了现有工具链的空白，为RAG系统的进一步发展和应用提供了坚实的基础。"}}
{"id": "2506.09914", "title": "From Theory to Practice: Advancing Multi-Robot Path Planning Algorithms and Applications", "authors": ["Teng Guo"], "summary": "The labeled MRPP (Multi-Robot Path Planning) problem involves routing robots\nfrom start to goal configurations efficiently while avoiding collisions.\nDespite progress in solution quality and runtime, its complexity and industrial\nrelevance continue to drive research.\n  This dissertation introduces scalable MRPP methods with provable guarantees\nand practical heuristics. First, we study dense MRPP on 2D grids, relevant to\nwarehouse and parcel systems. We propose the Rubik Table method, achieving $(1\n+ \\delta)$-optimal makespan (with $\\delta \\in (0, 0.5]$) for up to $\\frac{m_1\nm_2}{2}$ robots, solving large instances efficiently and setting a new\ntheoretical benchmark.\n  Next, we address real-world MRPP. We design optimal layouts for structured\nenvironments (e.g., warehouses, parking systems) and propose a puzzle-based\nsystem for dense, deadlock-free autonomous vehicle parking. We also extend MRPP\nto Reeds-Shepp robots, introducing motion primitives and smoothing techniques\nto ensure feasible, efficient paths under nonholonomic constraints. Simulations\nand real-world tests validate the approach in urban driving and robotic\ntransport scenarios.", "comment": "Ph.D. thesis", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.09914v1", "AI": {"title_translation": "从理论到实践：多机器人路径规划算法与应用的进展", "tldr": "本论文旨在解决多机器人路径规划（MRPP）问题，提出了针对密集2D网格（魔方表方法）的可扩展算法，以及针对仓库、停车系统和Reeds-Shepp机器人等现实世界应用的方法，并通过仿真和实际测试进行了验证。", "motivation": "多机器人路径规划（MRPP）问题尽管在解决方案质量和运行时间方面取得了进展，但其固有的复杂性和工业相关性持续推动着研究。", "method": "1. 提出了魔方表（Rubik Table）方法，用于解决2D网格上的密集MRPP问题，实现了$(1 + \text{delta})$-最优完工时间。2. 为结构化环境（如仓库、停车系统）设计了最优布局，并提出了一种基于拼图的系统，用于实现密集、无死锁的自动驾驶汽车停车。3. 将MRPP扩展到Reeds-Shepp机器人，引入了运动原语和平滑技术，以确保在非完整约束下生成可行、高效的路径。", "result": "1. 魔方表方法有效地解决了2D网格上的大型密集MRPP实例，并建立了新的理论基准。2. 为结构化环境设计的最佳布局和基于拼图的系统实现了密集、无死锁的自动驾驶汽车停车。3. 扩展到Reeds-Shepp机器人的MRPP方法生成了可行且高效的路径。4. 仿真和实际测试验证了该方法在城市驾驶和机器人运输场景中的有效性。", "conclusion": "本论文介绍了具有可证明保证和实用启发式方法的可扩展多机器人路径规划（MRPP）方法，成功地将该领域从理论基准推进到现实世界应用。", "translation": "带标签的多机器人路径规划（MRPP）问题涉及在避免碰撞的同时，有效地将机器人从起始配置路由到目标配置。尽管在解决方案质量和运行时间方面取得了进展，但其复杂性和工业相关性持续推动着研究。\n本论文介绍了具有可证明保证和实用启发式方法的可扩展MRPP方法。首先，我们研究了2D网格上的密集MRPP，这与仓库和包裹系统相关。我们提出了魔方表（Rubik Table）方法，对于多达 $\\frac{m_1 m_2}{2}$ 个机器人，实现了 $(1 + \\delta)$-最优完工时间（其中 $\\delta \\in (0, 0.5]$），有效地解决了大型实例并建立了新的理论基准。\n接下来，我们解决了现实世界中的MRPP。我们为结构化环境（例如仓库、停车系统）设计了最佳布局，并提出了一种基于拼图的系统，用于密集、无死锁的自动驾驶汽车停车。我们还将MRPP扩展到Reeds-Shepp机器人，引入了运动原语和平滑技术，以确保在非完整约束下生成可行、高效的路径。仿真和实际测试验证了该方法在城市驾驶和机器人运输场景中的有效性。", "summary": "本论文致力于推进多机器人路径规划（MRPP），引入了具有可证明保证和实用启发式方法的可扩展算法。它提出了魔方表方法，用于密集2D网格MRPP，实现了接近最优的完工时间，并建立了新的理论基准。该工作还通过为结构化环境（如仓库、停车场）设计最优布局和基于拼图的系统，以及将MRPP扩展到具有运动原语和平滑技术的Reeds-Shepp机器人，解决了现实世界中的MRPP问题。所提出的方法通过在各种机器人场景中的仿真和实际测试得到了验证。", "keywords": "多机器人路径规划, 魔方表, Reeds-Shepp机器人, 仓库自动化, 自动泊车", "comments": "该论文通过弥合理论MRPP与实际应用之间的差距做出了重要贡献。魔方表方法为密集环境提供了强大的理论保证，而对仓库、停车和非完整机器人（Reeds-Shepp）等现实世界场景的扩展则展示了所提出技术的多功能性和适用性。通过仿真和实际测试进行的验证增强了研究的可信度。"}}
{"id": "2506.09403", "title": "SRPL-SFDA: SAM-Guided Reliable Pseudo-Labels for Source-Free Domain Adaptation in Medical Image Segmentation", "authors": ["Xinya Liu", "Jianghao Wu", "Tao Lu", "Shaoting Zhang", "Guotai Wang"], "summary": "Domain Adaptation (DA) is crucial for robust deployment of medical image\nsegmentation models when applied to new clinical centers with significant\ndomain shifts. Source-Free Domain Adaptation (SFDA) is appealing as it can deal\nwith privacy concerns and access constraints on source-domain data during\nadaptation to target-domain data. However, SFDA faces challenges such as\ninsufficient supervision in the target domain with unlabeled images. In this\nwork, we propose a Segment Anything Model (SAM)-guided Reliable Pseudo-Labels\nmethod for SFDA (SRPL-SFDA) with three key components: 1) Test-Time Tri-branch\nIntensity Enhancement (T3IE) that not only improves quality of raw\npseudo-labels in the target domain, but also leads to SAM-compatible inputs\nwith three channels to better leverage SAM's zero-shot inference ability for\nrefining the pseudo-labels; 2) A reliable pseudo-label selection module that\nrejects low-quality pseudo-labels based on Consistency of Multiple SAM Outputs\n(CMSO) under input perturbations with T3IE; and 3) A reliability-aware training\nprocedure in the unlabeled target domain where reliable pseudo-labels are used\nfor supervision and unreliable parts are regularized by entropy minimization.\nExperiments conducted on two multi-domain medical image segmentation datasets\nfor fetal brain and the prostate respectively demonstrate that: 1) SRPL-SFDA\neffectively enhances pseudo-label quality in the unlabeled target domain, and\nimproves SFDA performance by leveraging the reliability-aware training; 2)\nSRPL-SFDA outperformed state-of-the-art SFDA methods, and its performance is\nclose to that of supervised training in the target domain. The code of this\nwork is available online: https://github.com/HiLab-git/SRPL-SFDA.", "comment": "18 pages, 4 figures. Accepted for publication in Neurocomputing", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09403v1", "AI": {"title_translation": "SRPL-SFDA: SAM引导的医学图像分割无源域适应可靠伪标签方法", "tldr": "SRPL-SFDA利用SAM生成并优化可靠伪标签，用于医学图像分割的无源域适应，性能优于现有方法。", "motivation": "医学图像分割模型在面对显著域偏移的新临床中心部署时，域适应（DA）至关重要。无源域适应（SFDA）因能解决源域数据隐私和访问限制问题而具有吸引力。然而，SFDA面临目标域无标签图像监督不足的挑战。", "method": "本文提出了一种由Segment Anything Model (SAM) 引导的无源域适应可靠伪标签方法（SRPL-SFDA），包含三个关键组件：1) 测试时三分支强度增强（T3IE），用于提高目标域原始伪标签质量，并生成与SAM兼容的三通道输入，以更好地利用SAM的零样本推理能力来优化伪标签；2) 一个可靠伪标签选择模块，基于T3IE输入扰动下的多SAM输出一致性（CMSO）来拒绝低质量伪标签；3) 一个无标签目标域的可靠性感知训练过程，其中可靠伪标签用于监督，不可靠部分通过熵最小化进行正则化。", "result": "在胎儿大脑和前列腺的两个多域医学图像分割数据集上进行的实验表明：1) SRPL-SFDA有效提高了无标签目标域的伪标签质量，并通过可靠性感知训练提升了SFDA性能；2) SRPL-SFDA优于现有最先进的SFDA方法，其性能接近目标域的监督训练。", "conclusion": "SRPL-SFDA有效提高了无标签目标域的伪标签质量，并通过可靠性感知训练提升了SFDA性能；SRPL-SFDA优于现有最先进的SFDA方法，其性能接近目标域的监督训练。", "translation": "域适应（DA）对于医学图像分割模型在应用于具有显著域偏移的新临床中心时的鲁棒部署至关重要。无源域适应（SFDA）具有吸引力，因为它可以处理在适应目标域数据时源域数据的隐私问题和访问限制。然而，SFDA面临着诸如目标域中无标签图像监督不足等挑战。在这项工作中，我们提出了一种由Segment Anything Model (SAM) 引导的SFDA可靠伪标签方法（SRPL-SFDA），包含三个关键组件：1) 测试时三分支强度增强（T3IE），它不仅提高了目标域原始伪标签的质量，而且生成了与SAM兼容的三通道输入，以更好地利用SAM的零样本推理能力来优化伪标签；2) 一个可靠伪标签选择模块，根据T3IE输入扰动下的多SAM输出一致性（CMSO）来拒绝低质量伪标签；3) 一个无标签目标域的可靠性感知训练过程，其中可靠伪标签用于监督，不可靠部分通过熵最小化进行正则化。在分别针对胎儿大脑和前列腺的两个多域医学图像分割数据集上进行的实验表明：1) SRPL-SFDA有效提高了无标签目标域的伪标签质量，并通过利用可靠性感知训练提高了SFDA性能；2) SRPL-SFDA优于现有最先进的SFDA方法，其性能接近目标域的监督训练。本工作的代码已在线提供：https://github.com/HiLab-git/SRPL-SFDA。", "summary": "本文提出SRPL-SFDA，一种SAM引导的医学图像分割无源域适应方法。该方法通过提升伪标签质量和可靠性，解决了SFDA中监督不足的问题。其包含测试时三分支强度增强（T3IE）以生成SAM兼容输入并优化伪标签，一个基于多SAM输出一致性（CMSO）的可靠伪标签选择模块，以及一个可靠性感知训练过程。实验证明，SRPL-SFDA有效提升了伪标签质量，改善了SFDA性能，并超越了现有最先进方法，性能接近监督训练。", "keywords": "域适应, 无源域适应, 医学图像分割, 伪标签, SAM", "comments": "本文创新性地利用Segment Anything Model (SAM) 解决医学图像分割中的无源域适应问题，该问题因隐私顾虑和源数据缺乏而极具挑战。所提出的SRPL-SFDA通过引入新颖的SAM引导的伪标签优化和选择机制，解决了SFDA中不可靠伪标签的关键问题。该方法能够实现接近监督训练的性能，这对于在不同临床环境中实际部署具有重要意义。"}}
{"id": "2506.09202", "title": "Policy-Based Trajectory Clustering in Offline Reinforcement Learning", "authors": ["Hao Hu", "Xinqi Wang", "Simon Shaolei Du"], "summary": "We introduce a novel task of clustering trajectories from offline\nreinforcement learning (RL) datasets, where each cluster center represents the\npolicy that generated its trajectories. By leveraging the connection between\nthe KL-divergence of offline trajectory distributions and a mixture of\npolicy-induced distributions, we formulate a natural clustering objective. To\nsolve this, we propose Policy-Guided K-means (PG-Kmeans) and Centroid-Attracted\nAutoencoder (CAAE). PG-Kmeans iteratively trains behavior cloning (BC) policies\nand assigns trajectories based on policy generation probabilities, while CAAE\nresembles the VQ-VAE framework by guiding the latent representations of\ntrajectories toward the vicinity of specific codebook entries to achieve\nclustering. Theoretically, we prove the finite-step convergence of PG-Kmeans\nand identify a key challenge in offline trajectory clustering: the inherent\nambiguity of optimal solutions due to policy-induced conflicts, which can\nresult in multiple equally valid but structurally distinct clusterings.\nExperimentally, we validate our methods on the widely used D4RL dataset and\ncustom GridWorld environments. Our results show that both PG-Kmeans and CAAE\neffectively partition trajectories into meaningful clusters. They offer a\npromising framework for policy-based trajectory clustering, with broad\napplications in offline RL and beyond.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09202v1", "AI": {"title_translation": "基于策略的离线强化学习轨迹聚类", "tldr": "该论文引入了离线强化学习中基于策略的轨迹聚类新任务。提出了两种方法：策略引导K均值（PG-Kmeans）和质心吸引自编码器（CAAE）。理论上证明了PG-Kmeans的收敛性，并指出了最优解的固有模糊性。实验在D4RL和GridWorld数据集上验证了方法的有效性。", "motivation": "该论文引入了一项新颖的任务，即对离线强化学习（RL）数据集中的轨迹进行聚类，其中每个聚类中心代表生成其轨迹的策略。其动机在于实现这种基于策略的轨迹聚类。", "method": "该论文通过利用离线轨迹分布的KL散度与策略诱导分布混合之间的联系，制定了一个自然的聚类目标。为解决此问题，提出了两种方法：1. 策略引导K均值（PG-Kmeans）：迭代训练行为克隆（BC）策略并根据策略生成概率分配轨迹。2. 质心吸引自编码器（CAAE）：类似于VQ-VAE框架，通过引导轨迹的潜在表示趋向特定码本条目来实现聚类。理论上，证明了PG-Kmeans的有限步收敛性。", "result": "实验结果显示，在广泛使用的D4RL数据集和自定义GridWorld环境中，PG-Kmeans和CAAE都能有效地将轨迹划分为有意义的簇。研究还发现，离线轨迹聚类存在一个关键挑战：由于策略引起的冲突导致最优解固有的模糊性，这可能导致多个同样有效但结构上不同的聚类。", "conclusion": "所提出的PG-Kmeans和CAAE方法为基于策略的轨迹聚类提供了一个有前景的框架，在离线强化学习及其他领域具有广泛的应用。同时，研究也指出了该任务中解的模糊性挑战。", "translation": "我们引入了一项新颖的任务，即对离线强化学习（RL）数据集中的轨迹进行聚类，其中每个聚类中心代表生成其轨迹的策略。通过利用离线轨迹分布的KL散度与策略诱导分布混合之间的联系，我们制定了一个自然的聚类目标。为了解决这个问题，我们提出了策略引导K均值（PG-Kmeans）和质心吸引自编码器（CAAE）。PG-Kmeans迭代地训练行为克隆（BC）策略并根据策略生成概率分配轨迹，而CAAE则类似于VQ-VAE框架，通过引导轨迹的潜在表示趋向特定码本条目来实现聚类。理论上，我们证明了PG-Kmeans的有限步收敛性，并指出了离线轨迹聚类中的一个关键挑战：由于策略引起的冲突导致最优解固有的模糊性，这可能导致多个同样有效但结构上不同的聚类。实验上，我们在广泛使用的D4RL数据集和自定义GridWorld环境中验证了我们的方法。我们的结果表明，PG-Kmeans和CAAE都能有效地将轨迹划分为有意义的簇。它们为基于策略的轨迹聚类提供了一个有前景的框架，在离线RL及其他领域具有广泛的应用。", "summary": "该论文引入了离线强化学习中一项新颖的轨迹聚类任务，其中聚类中心由生成轨迹的潜在策略定义。为此，提出了两种新方法：策略引导K均值（PG-Kmeans）和质心吸引自编码器（CAAE）。PG-Kmeans采用迭代策略训练和分配，而CAAE则采用类似VQ-VAE的方法。论文理论上证明了PG-Kmeans的收敛性，并强调了最优解模糊性的挑战。在D4RL和GridWorld数据集上的实验结果表明，这两种方法都能有效地创建有意义的轨迹簇，具有广泛的应用前景。", "keywords": "离线强化学习, 轨迹聚类, 策略引导K均值, 质心吸引自编码器, 策略模糊性", "comments": "这篇论文解决了离线强化学习中一个重要且新颖的问题，即根据生成策略对轨迹进行聚类。引入两种不同的方法（PG-Kmeans和CAAE）提供了一种全面的解决方案。PG-Kmeans收敛性的理论分析很有价值，而对最优解模糊性的识别是对基于策略的轨迹聚类固有复杂性的重要见解，可以指导未来的研究。"}}
{"id": "2506.09930", "title": "From Intention to Execution: Probing the Generalization Boundaries of Vision-Language-Action Models", "authors": ["Irving Fang", "Juexiao Zhang", "Shengbang Tong", "Chen Feng"], "summary": "One promise that Vision-Language-Action (VLA) models hold over traditional\nimitation learning for robotics is to leverage the broad generalization\ncapabilities of large Vision-Language Models (VLMs) to produce versatile,\n\"generalist\" robot policies. However, current evaluations of VLAs remain\ninsufficient. Traditional imitation learning benchmarks are unsuitable due to\nthe lack of language instructions. Emerging benchmarks for VLAs that\nincorporate language often come with limited evaluation tasks and do not intend\nto investigate how much VLM pretraining truly contributes to the generalization\ncapabilities of the downstream robotic policy. Meanwhile, much research relies\non real-world robot setups designed in isolation by different institutions,\nwhich creates a barrier for reproducibility and accessibility. To address this\ngap, we introduce a unified probing suite of 50 simulation-based tasks across\n10 subcategories spanning language instruction, vision, and objects. We\nsystematically evaluate several state-of-the-art VLA architectures on this\nsuite to understand their generalization capability. Our results show that\nwhile VLM backbones endow VLAs with robust perceptual understanding and high\nlevel planning, which we refer to as good intentions, this does not reliably\ntranslate into precise motor execution: when faced with out-of-distribution\nobservations, policies often exhibit coherent intentions, but falter in action\nexecution. Moreover, finetuning on action data can erode the original VLM's\ngeneralist reasoning abilities. We release our task suite and evaluation code\nto serve as a standardized benchmark for future VLAs and to drive research on\nclosing the perception-to-action gap. More information, including the source\ncode, can be found at https://ai4ce.github.io/INT-ACT/", "comment": "Under review", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.09930v1", "AI": {"title_translation": "从意图到执行：探究视觉-语言-动作模型的泛化边界", "tldr": "VLA模型有泛化潜力但评估不足。本文提出50个模拟任务的统一基准，发现VLA模型在感知理解和高层规划上表现良好，但在精确动作执行（特别是OOD情况）上表现不佳，且动作数据微调可能损害VLM的通用推理能力。", "motivation": "视觉-语言-动作（VLA）模型有望利用大型视觉-语言模型（VLM）的泛化能力实现通用机器人策略，但现有评估不足。传统模仿学习基准缺乏语言指令，新兴VLA基准任务有限且未深入探究VLM预训练对泛化能力的贡献。此外，真实世界机器人设置的独立设计阻碍了可复现性和可及性。", "method": "引入了一个包含10个子类别（涵盖语言指令、视觉和物体）的50个基于模拟任务的统一探测套件，并系统地评估了多个最先进的VLA架构在该套件上的泛化能力。", "result": "VLM骨干赋予VLA模型强大的感知理解和高层规划能力（“良好意图”），但这并不能可靠地转化为精确的运动执行；当面临分布外观测时，策略常表现出连贯的意图，但在动作执行上却步履维艰。此外，在动作数据上进行微调可能会损害原始VLM的通用推理能力。", "conclusion": "VLA模型在感知理解和高层规划（意图）与精确动作执行之间存在显著差距。尽管VLM骨干提供了强大的感知能力，但这种能力并不能可靠地转化为精确的运动执行，尤其是在分布外观测情况下。此外，在动作数据上进行微调可能损害原始VLM的通用推理能力。本文发布的任务套件和评估代码旨在作为未来VLA的标准化基准，以推动弥合感知到动作之间差距的研究。", "translation": "视觉-语言-动作（VLA）模型相对于传统机器人模仿学习的一个前景是，利用大型视觉-语言模型（VLM）的广泛泛化能力来生成多功能、“通用型”机器人策略。然而，当前对VLA的评估仍然不足。传统模仿学习基准因缺乏语言指令而不适用。新兴的VLA基准虽然结合了语言，但通常评估任务有限，并未旨在探究VLM预训练对下游机器人策略泛化能力究竟贡献了多少。同时，许多研究依赖于由不同机构独立设计的真实世界机器人设置，这为可复现性和可及性制造了障碍。为了解决这一空白，我们引入了一个统一的探测套件，包含50个基于模拟的任务，涵盖语言指令、视觉和物体10个子类别。我们系统地评估了该套件上的几种最先进的VLA架构，以理解它们的泛化能力。我们的结果表明，尽管VLM骨干赋予VLA模型强大的感知理解和高层规划能力（我们称之为良好意图），但这并不能可靠地转化为精确的运动执行：当面临分布外观测时，策略通常表现出连贯的意图，但在动作执行上却步履维艰。此外，在动作数据上进行微调可能会损害原始VLM的通用推理能力。我们发布了任务套件和评估代码，以作为未来VLA的标准化基准，并推动弥合感知到动作之间差距的研究。更多信息，包括源代码，可在https://ai4ce.github.io/INT-ACT/找到。", "summary": "本文旨在解决视觉-语言-动作（VLA）模型评估不足的问题，这些模型旨在利用视觉-语言模型（VLM）实现通用机器人策略。为此，作者引入了一个名为INT-ACT的统一模拟探测套件，包含10个类别共50个任务，用于系统评估VLA的泛化能力。他们对最先进VLA架构的评估表明，尽管VLM骨干提供了强大的感知理解和高层规划能力，但这种“良好意图”往往未能转化为精确的运动执行，尤其是在分布外观测情况下。此外，在动作数据上进行微调可能会损害VLM固有的推理能力。作者发布了他们的基准，以促进弥合VLA模型中感知到动作之间差距的研究。", "keywords": "视觉-语言-动作模型, 泛化, 机器人学, 基准测试, 模拟, 感知-动作差距", "comments": "这篇论文的创新之处在于提出了一个统一的、基于模拟的VLA模型评估基准，填补了现有评估方法不足、可复现性差的空白。它通过50个多样化任务系统地探究了VLA模型的泛化边界，尤其揭示了VLM骨干赋予的“良好意图”（感知理解和高层规划）与实际精确动作执行之间的差距，以及动作数据微调可能损害VLM通用推理能力的问题。该基准的发布对于推动通用机器人领域的发展至关重要，它提供了一个标准化的工具来识别VLA模型的局限性，并引导未来研究集中于弥合感知到动作之间的关键鸿沟。"}}
{"id": "2506.09411", "title": "Synthetic Human Action Video Data Generation with Pose Transfer", "authors": ["Vaclav Knapp", "Matyas Bohacek"], "summary": "In video understanding tasks, particularly those involving human motion,\nsynthetic data generation often suffers from uncanny features, diminishing its\neffectiveness for training. Tasks such as sign language translation, gesture\nrecognition, and human motion understanding in autonomous driving have thus\nbeen unable to exploit the full potential of synthetic data. This paper\nproposes a method for generating synthetic human action video data using pose\ntransfer (specifically, controllable 3D Gaussian avatar models). We evaluate\nthis method on the Toyota Smarthome and NTU RGB+D datasets and show that it\nimproves performance in action recognition tasks. Moreover, we demonstrate that\nthe method can effectively scale few-shot datasets, making up for groups\nunderrepresented in the real training data and adding diverse backgrounds. We\nopen-source the method along with RANDOM People, a dataset with videos and\navatars of novel human identities for pose transfer crowd-sourced from the\ninternet.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09411v1", "AI": {"title_translation": "合成人类动作视频数据生成与姿态迁移", "tldr": "提出了一种使用姿态迁移生成合成人类动作视频数据的方法，解决了现有合成数据的不自然问题，并在动作识别任务中表现出色，还能扩展少样本数据集。", "motivation": "现有的合成数据生成在人类动作视频理解任务中存在不自然特征，降低了其训练效果，导致手语翻译、手势识别和自动驾驶中的人体动作理解等任务无法充分利用合成数据的潜力。", "method": "提出了一种使用姿态迁移（特别是可控的3D高斯头像模型）来生成合成人类动作视频数据的方法。", "result": "该方法在Toyota Smarthome和NTU RGB+D数据集上进行了评估，并显示其在动作识别任务中提高了性能。此外，该方法还能有效扩展少样本数据集，弥补真实训练数据中代表性不足的群体，并增加多样化的背景。", "conclusion": "该方法通过姿态迁移有效解决了合成人类动作视频数据不自然的问题，提高了动作识别任务的性能，并能扩展少样本数据集，为视频理解任务提供了有价值的合成数据生成方案。", "translation": "在视频理解任务中，特别是涉及人体动作的任务，合成数据生成常常存在不自然特征，从而降低了其训练效果。因此，手语翻译、手势识别和自动驾驶中的人体动作理解等任务一直未能充分发挥合成数据的潜力。本文提出了一种使用姿态迁移（特别是可控的3D高斯头像模型）来生成合成人类动作视频数据的方法。我们在Toyota Smarthome和NTU RGB+D数据集上评估了该方法，并表明它在动作识别任务中提高了性能。此外，我们证明该方法可以有效扩展少样本数据集，弥补真实训练数据中代表性不足的群体，并增加多样化的背景。我们开源了该方法以及RANDOM People数据集，该数据集包含从互联网众包的用于姿态迁移的新颖人类身份的视频和头像。", "summary": "本文针对现有合成人类动作视频数据在视频理解任务中存在的不自然问题，提出了一种基于姿态迁移（利用可控3D高斯头像模型）的合成数据生成方法。实验结果表明，该方法不仅在动作识别任务中显著提升了性能，还能有效扩展少样本数据集，增加数据多样性。研究者还开源了该方法和配套的RANDOM People数据集，以促进相关研究。", "keywords": "合成数据, 姿态迁移, 动作识别, 3D高斯头像, 少样本学习", "comments": "这项工作通过引入姿态迁移和3D高斯头像模型，有效地解决了合成人类动作视频数据“不自然”的核心问题，这对于依赖大量高质量数据的视频理解任务（如手语翻译、手势识别）具有重要意义。其能够扩展少样本数据集并增加背景多样性的能力，进一步提升了其实用价值。开源代码和数据集的举措也极大地促进了社区的进一步研究和应用。"}}
{"id": "2506.09934", "title": "Fluoroscopic Shape and Pose Tracking of Catheters with Custom Radiopaque Markers", "authors": ["Jared Lawson", "Rohan Chitale", "Nabil Simaan"], "summary": "Safe navigation of steerable and robotic catheters in the cerebral\nvasculature requires awareness of the catheters shape and pose. Currently, a\nsignificant perception burden is placed on interventionalists to mentally\nreconstruct and predict catheter motions from biplane fluoroscopy images.\nEfforts to track these catheters are limited to planar segmentation or bulky\nsensing instrumentation, which are incompatible with microcatheters used in\nneurointervention. In this work, a catheter is equipped with custom radiopaque\nmarkers arranged to enable simultaneous shape and pose estimation under biplane\nfluoroscopy. A design measure is proposed to guide the arrangement of these\nmarkers to minimize sensitivity to marker tracking uncertainty. This approach\nwas deployed for microcatheters smaller than 2mm OD navigating phantom\nvasculature with shape tracking errors less than 1mm and catheter roll errors\nbelow 40 degrees. This work can enable steerable catheters to autonomously\nnavigate under biplane imaging.", "comment": "8 pages, 5 figures, accepted in Robotics and Automation Letters", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.09934v1", "AI": {"title_translation": "带有定制不透射线标记的导管的荧光透视形状和姿态跟踪", "tldr": "该研究开发了一种使用定制不透射线标记的荧光透视方法，用于同时跟踪导管的形状和姿态，以提高神经介入手术中的安全性。", "motivation": "在脑血管中安全导航可转向和机器人导管需要了解导管的形状和姿态。目前，介入医师需要从双平面荧光透视图像中进行心理重建和预测导管运动，感知负担很重。现有的导管跟踪方法（平面分割或笨重传感设备）与神经介入中使用的微导管不兼容。", "method": "一种导管配备了定制的不透射线标记，这些标记经过排列，可以在双平面荧光透视下同时估计形状和姿态。提出了一种设计度量来指导标记的排列，以最大限度地减少对标记跟踪不确定性的敏感性。", "result": "该方法已应用于外径小于2mm的微导管，在模拟血管中导航时，形状跟踪误差小于1mm，导管滚动误差低于40度。", "conclusion": "这项工作可以使可转向导管在双平面成像下自主导航。", "translation": "导管在脑血管中安全导航需要了解其形状和姿态。目前，介入医师在从双平面荧光透视图像中进行心理重建和预测导管运动时，面临着巨大的感知负担。跟踪这些导管的努力仅限于平面分割或笨重的传感设备，这些与神经介入中使用的微导管不兼容。在这项工作中，导管配备了定制的不透射线标记，这些标记的排列使得能够在双平面荧光透视下同时估计形状和姿态。提出了一种设计度量来指导这些标记的排列，以最大限度地减少对标记跟踪不确定性的敏感性。该方法已应用于外径小于2毫米的微导管，在模拟血管中导航时，形状跟踪误差小于1毫米，导管滚动误差低于40度。这项工作可以使可转向导管在双平面成像下自主导航。", "summary": "本文提出了一种创新的荧光透视方法，通过在导管上部署定制的不透射线标记，实现了在双平面荧光透视下同时跟踪导管的形状和姿态。该方法旨在解决当前神经介入中导管跟踪的局限性，并减轻介入医师的感知负担。实验证明，该方法在微导管上表现出高精度，有望实现可转向导管的自主导航。", "keywords": "导管跟踪, 荧光透视, 形状估计, 姿态跟踪, 不透射线标记", "comments": "这项工作通过创新的标记设计和荧光透视技术，有效地解决了微导管在神经介入中形状和姿态跟踪的挑战。其核心创新在于定制的放射不透明标记排列和最小化跟踪不确定性的设计度量，这使得该方法与现有笨重或不兼容的解决方案相比具有显著优势。该研究的重要性在于它能够减轻介入医师的认知负担，并为未来可转向导管的自主导航奠定基础，从而提高神经介入手术的安全性和效率。"}}
{"id": "2506.09416", "title": "Noise Conditional Variational Score Distillation", "authors": ["Xinyu Peng", "Ziyang Zheng", "Yaoming Wang", "Han Li", "Nuowen Kan", "Wenrui Dai", "Chenglin Li", "Junni Zou", "Hongkai Xiong"], "summary": "We propose Noise Conditional Variational Score Distillation (NCVSD), a novel\nmethod for distilling pretrained diffusion models into generative denoisers. We\nachieve this by revealing that the unconditional score function implicitly\ncharacterizes the score function of denoising posterior distributions. By\nintegrating this insight into the Variational Score Distillation (VSD)\nframework, we enable scalable learning of generative denoisers capable of\napproximating samples from the denoising posterior distribution across a wide\nrange of noise levels. The proposed generative denoisers exhibit desirable\nproperties that allow fast generation while preserve the benefit of iterative\nrefinement: (1) fast one-step generation through sampling from pure Gaussian\nnoise at high noise levels; (2) improved sample quality by scaling the\ntest-time compute with multi-step sampling; and (3) zero-shot probabilistic\ninference for flexible and controllable sampling. We evaluate NCVSD through\nextensive experiments, including class-conditional image generation and inverse\nproblem solving. By scaling the test-time compute, our method outperforms\nteacher diffusion models and is on par with consistency models of larger sizes.\nAdditionally, with significantly fewer NFEs than diffusion-based methods, we\nachieve record-breaking LPIPS on inverse problems.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09416v1", "AI": {"title_translation": "噪声条件变分分数蒸馏", "tldr": "NCVSD是一种新方法，通过利用无条件分数函数来蒸馏预训练扩散模型，生成能够快速生成并保持迭代细化优势的生成去噪器，在图像生成和逆问题解决中表现出色。", "motivation": "旨在解决现有扩散模型在生成速度和保持迭代细化优势方面的挑战，通过蒸馏预训练扩散模型来创建高效的生成去噪器。", "method": "提出了噪声条件变分分数蒸馏 (NCVSD)。该方法揭示了无条件分数函数隐式地表征去噪后验分布的分数函数。通过将此见解整合到变分分数蒸馏 (VSD) 框架中，实现了生成去噪器的可扩展学习，这些去噪器能够在各种噪声水平下近似去噪后验分布的样本。其特性包括：1) 通过从高噪声水平的纯高斯噪声中采样实现快速一步生成；2) 通过多步采样扩展测试时间计算来提高样本质量；3) 零样本概率推理实现灵活和可控的采样。", "result": "1. 通过扩展测试时间计算，NCVSD 优于教师扩散模型，并与更大尺寸的一致性模型相当。\n2. 与基于扩散的方法相比，NCVSD 使用显著更少的 NFEs，在逆问题上实现了破纪录的 LPIPS。\n3. 实现了快速一步生成和通过多步采样提高样本质量。\n4. 支持零样本概率推理。", "conclusion": "NCVSD 是一种有效且高效的方法，能够将预训练扩散模型蒸馏成高性能的生成去噪器，在图像生成和逆问题解决方面表现出色，并提供了快速生成和高质量采样的优势。", "translation": "我们提出了噪声条件变分分数蒸馏（NCVSD），这是一种将预训练扩散模型蒸馏成生成去噪器的新方法。我们通过揭示无条件分数函数隐式地表征去噪后验分布的分数函数来实现这一点。通过将这一见解整合到变分分数蒸馏（VSD）框架中，我们能够可扩展地学习生成去噪器，这些去噪器能够在大范围噪声水平下近似去噪后验分布的样本。所提出的生成去噪器具有理想的特性，可以实现快速生成同时保留迭代细化的优势：(1) 通过在高噪声水平下从纯高斯噪声中采样实现快速一步生成；(2) 通过多步采样扩展测试时间计算来提高样本质量；以及 (3) 用于灵活和可控采样的零样本概率推理。我们通过广泛的实验评估了 NCVSD，包括类别条件图像生成和逆问题解决。通过扩展测试时间计算，我们的方法优于教师扩散模型，并与更大尺寸的一致性模型相当。此外，与基于扩散的方法相比，我们使用显著更少的 NFEs，在逆问题上实现了破纪录的 LPIPS。", "summary": "本文提出了一种名为噪声条件变分分数蒸馏（NCVSD）的新方法，用于将预训练的扩散模型蒸馏为高效的生成去噪器。NCVSD 的核心在于发现无条件分数函数能隐式表征去噪后验分布的分数函数，并将其融入变分分数蒸馏框架。该方法实现了快速单步生成、通过多步采样提升样本质量以及零样本概率推理。实验证明，NCVSD 在图像生成和逆问题解决中表现卓越，性能优于教师扩散模型，与大型一致性模型媲美，并在逆问题上以更少的计算量取得了最佳LPIPS。", "keywords": "扩散模型, 分数蒸馏, 生成去噪器, 图像生成, 逆问题", "comments": "这篇论文的创新点在于揭示了无条件分数函数与去噪后验分布分数函数之间的关系，并将其应用于扩散模型的蒸馏，显著提高了生成速度和效率，同时保持了高质量的生成能力。其在逆问题上的表现尤其突出，展现了该方法在实际应用中的潜力。"}}
{"id": "2506.09215", "title": "Robust Noise Attenuation via Adaptive Pooling of Transformer Outputs", "authors": ["Greyson Brothers"], "summary": "We investigate the design of pooling methods used to summarize the outputs of\ntransformer embedding models, primarily motivated by reinforcement learning and\nvision applications. This work considers problems where a subset of the input\nvectors contains requisite information for a downstream task (signal) while the\nrest are distractors (noise). By framing pooling as vector quantization with\nthe goal of minimizing signal loss, we demonstrate that the standard methods\nused to aggregate transformer outputs, AvgPool, MaxPool, and ClsToken, are\nvulnerable to performance collapse as the signal-to-noise ratio (SNR) of inputs\nfluctuates. We then show that an attention-based adaptive pooling method can\napproximate the signal-optimal vector quantizer within derived error bounds for\nany SNR. Our theoretical results are first validated by supervised experiments\non a synthetic dataset designed to isolate the SNR problem, then generalized to\nstandard relational reasoning, multi-agent reinforcement learning, and vision\nbenchmarks with noisy observations, where transformers with adaptive pooling\ndisplay superior robustness across tasks.", "comment": "[ICML 2025 Spotlight Poster] To be published in the Forty-Second\n  International Conference on Machine Learning (ICML) Proceedings", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09215v1", "AI": {"title_translation": "通过Transformer输出的自适应池化实现鲁棒噪声衰减", "tldr": "该论文提出了一种针对Transformer输出的自适应池化方法，该方法对噪声具有鲁棒性，在各种任务中优于标准池化方法。", "motivation": "该研究旨在设计有效的Transformer嵌入模型池化方法，尤其是在强化学习和视觉应用中，其中部分输入向量是信号，其余是噪声。标准的池化方法（AvgPool、MaxPool、ClsToken）在输入信噪比（SNR）波动时容易出现性能崩溃。", "method": "该论文将池化建模为向量量化，旨在最小化信号损失。它提出了一种基于注意力的自适应池化方法，该方法可以在任何SNR下，在推导出的误差范围内近似信号最优的向量量化器。该方法通过理论分析和实验验证。", "result": "标准池化方法（AvgPool、MaxPool、ClsToken）在信噪比波动时容易出现性能崩溃。所提出的基于注意力的自适应池化方法可以在任何SNR下，在推导出的误差范围内近似信号最优的向量量化器。该理论结果在合成数据集上的监督实验中得到验证，并推广到关系推理、多智能体强化学习和视觉基准测试中。带有自适应池化的Transformer在各项任务中显示出卓越的鲁棒性。", "conclusion": "该论文揭示了标准池化方法在信噪比波动下的脆弱性，并提出了一种基于注意力的自适应池化方法，该方法通过有效处理噪声，在各种任务中表现出卓越的鲁棒性。", "translation": "我们研究了用于总结Transformer嵌入模型输出的池化方法设计，这主要受强化学习和视觉应用的启发。这项工作考虑的问题是，输入向量的子集包含下游任务所需的必要信息（信号），而其余部分是干扰项（噪声）。通过将池化视为向量量化，目标是最小化信号损失，我们证明了用于聚合Transformer输出的标准方法，即AvgPool、MaxPool和ClsToken，在输入信噪比（SNR）波动时容易出现性能崩溃。然后我们表明，一种基于注意力的自适应池化方法可以在任何SNR下，在推导出的误差范围内近似信号最优的向量量化器。我们的理论结果首先通过在旨在隔离SNR问题的合成数据集上的监督实验得到验证，然后推广到具有噪声观测的标准关系推理、多智能体强化学习和视觉基准，其中带有自适应池化的Transformer在各项任务中显示出卓越的鲁棒性。", "summary": "该论文研究了Transformer输出的池化方法，旨在解决强化学习和视觉应用中输入数据包含信号和噪声的问题。研究发现，传统的池化方法（AvgPool、MaxPool、ClsToken）在信噪比（SNR）波动时性能会急剧下降。为此，论文提出了一种基于注意力的自适应池化方法，该方法能有效近似信号最优的向量量化器，并在理论上和实验中（包括合成数据集、关系推理、多智能体强化学习和视觉基准）证明了其在噪声环境下的卓越鲁棒性。", "keywords": "自适应池化, Transformer, 噪声衰减, 信噪比, 鲁棒性", "comments": "该论文解决了Transformer应用中，尤其是在噪声环境下，一个关键问题。识别出标准池化方法对信噪比波动的脆弱性具有重要意义。所提出的基于注意力的自适应池化方法，得到了理论保证和跨不同领域的实证验证支持，提供了一个鲁棒的解决方案，增强了Transformer在真实世界噪声输入场景中的实际适用性。其创新之处在于将池化视为向量量化，并开发了一种自适应机制来有效减轻噪声。"}}
{"id": "2506.09937", "title": "SAFE: Multitask Failure Detection for Vision-Language-Action Models", "authors": ["Qiao Gu", "Yuanliang Ju", "Shengxiang Sun", "Igor Gilitschenski", "Haruki Nishimura", "Masha Itkina", "Florian Shkurti"], "summary": "While vision-language-action models (VLAs) have shown promising robotic\nbehaviors across a diverse set of manipulation tasks, they achieve limited\nsuccess rates when deployed on novel tasks out-of-the-box. To allow these\npolicies to safely interact with their environments, we need a failure detector\nthat gives a timely alert such that the robot can stop, backtrack, or ask for\nhelp. However, existing failure detectors are trained and tested only on one or\na few specific tasks, while VLAs require the detector to generalize and detect\nfailures also in unseen tasks and novel environments. In this paper, we\nintroduce the multitask failure detection problem and propose SAFE, a failure\ndetector for generalist robot policies such as VLAs. We analyze the VLA feature\nspace and find that VLAs have sufficient high-level knowledge about task\nsuccess and failure, which is generic across different tasks. Based on this\ninsight, we design SAFE to learn from VLA internal features and predict a\nsingle scalar indicating the likelihood of task failure. SAFE is trained on\nboth successful and failed rollouts, and is evaluated on unseen tasks. SAFE is\ncompatible with different policy architectures. We test it on OpenVLA, $\\pi_0$,\nand $\\pi_0$-FAST in both simulated and real-world environments extensively. We\ncompare SAFE with diverse baselines and show that SAFE achieves\nstate-of-the-art failure detection performance and the best trade-off between\naccuracy and detection time using conformal prediction. More qualitative\nresults can be found at https://vla-safe.github.io/.", "comment": "Project Page: https://vla-safe.github.io/", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.09937v1", "AI": {"title_translation": "SAFE：多任务视觉-语言-动作模型故障检测", "tldr": "本文提出了SAFE，一个通用的故障检测器，用于视觉-语言-动作模型，能在未见任务中有效检测故障，并达到最先进的性能。", "motivation": "视觉-语言-动作模型（VLAs）在部署到新任务时成功率有限，且现有故障检测器仅限于特定任务，无法泛化到未见任务和新环境。为了让机器人安全地与环境交互，需要一个能及时预警并在通用策略上泛化的故障检测器。", "method": "本文引入了多任务故障检测问题，并提出了SAFE，一个用于通用机器人策略（如VLAs）的故障检测器。研究人员分析了VLA的特征空间，发现VLA对任务的成功和失败具有通用的高层知识。基于此，SAFE被设计为从VLA的内部特征中学习，并预测一个单一的标量，指示任务失败的可能性。SAFE在成功和失败的推演中进行训练，并在未见任务中进行评估，兼容不同的策略架构。", "result": "SAFE在模拟和真实环境中，对OpenVLA、$\\pi_0$和$\\pi_0$-FAST进行了广泛测试。与多种基线相比，SAFE实现了最先进的故障检测性能，并使用共形预测在准确性和检测时间之间取得了最佳权衡。", "conclusion": "SAFE是一个针对通用机器人策略（如VLA）的多任务故障检测器，能够有效检测未见任务中的故障，并在准确性和检测时间之间取得最佳平衡，从而提高了VLA在实际部署中的安全性和鲁棒性。", "translation": "尽管视觉-语言-动作模型（VLAs）在各种操作任务中展现出有前景的机器人行为，但当它们被直接部署到新任务时，成功率有限。为了让这些策略能够安全地与环境交互，我们需要一个故障检测器，能够及时发出警报，以便机器人可以停止、回溯或寻求帮助。然而，现有故障检测器仅在少数特定任务上进行训练和测试，而VLAs需要检测器能够泛化并在未见任务和新环境中检测故障。在本文中，我们引入了多任务故障检测问题，并提出了SAFE，一个用于通用机器人策略（如VLAs）的故障检测器。我们分析了VLA的特征空间，发现VLAs对任务的成功和失败具有足够的高层知识，这些知识在不同任务中是通用的。基于这一见解，我们设计了SAFE，使其从VLA内部特征中学习，并预测一个单一的标量，指示任务失败的可能性。SAFE在成功和失败的推演中进行训练，并在未见任务中进行评估。SAFE兼容不同的策略架构。我们在模拟和真实环境中，对OpenVLA、$\\pi_0$和$\\pi_0$-FAST进行了广泛测试。我们将SAFE与各种基线进行比较，结果表明SAFE在故障检测性能上达到了最先进水平，并使用共形预测在准确性和检测时间之间取得了最佳权衡。更多定性结果可在 https://vla-safe.github.io/ 找到。", "summary": "本文针对视觉-语言-动作模型（VLAs）在陌生任务中成功率低且现有故障检测器泛化能力差的问题，提出了一个通用的多任务故障检测器SAFE。SAFE通过分析VLA的内部特征，利用其对任务成功/失败的通用高层知识，学习预测任务失败的可能性。该模型在成功和失败的推演中进行训练，并在未见任务中进行评估，兼容多种策略架构。实验结果表明，SAFE在模拟和真实环境中，对多种VLA模型均能实现最先进的故障检测性能，并在准确性和检测时间之间取得最佳平衡，显著提升了通用机器人策略的安全性。", "keywords": "故障检测, 视觉-语言-动作模型, 多任务学习, 机器人, 通用策略", "comments": "这项工作通过提出一个通用的多任务故障检测器SAFE，解决了视觉-语言-动作模型在实际部署中面临的关键安全问题。其创新点在于利用VLA模型自身的内部特征进行故障预测，并证明了这些特征在不同任务间的通用性，这对于提高通用机器人策略的鲁棒性和部署安全性具有重要意义。"}}
{"id": "2506.09417", "title": "ODG: Occupancy Prediction Using Dual Gaussians", "authors": ["Yunxiao Shi", "Yinhao Zhu", "Shizhong Han", "Jisoo Jeong", "Amin Ansari", "Hong Cai", "Fatih Porikli"], "summary": "3D occupancy provides fine-grained 3D geometry and semantics for scene\nunderstanding which is critical for autonomous driving. Most existing methods,\nhowever, carry high compute costs, requiring dense 3D feature volume and\ncross-attention to effectively aggregate information. More recent works have\nadopted Bird's Eye View (BEV) or sparse points as scene representation with\nmuch reduced cost, but still suffer from their respective shortcomings. More\nconcretely, BEV struggles with small objects that often experience significant\ninformation loss after being projected to the ground plane. On the other hand,\npoints can flexibly model little objects in 3D, but is inefficient at capturing\nflat surfaces or large objects. To address these challenges, in this paper, we\npresent a novel 3D occupancy prediction approach, ODG, which combines BEV and\nsparse points based representations. We propose a dual-branch design: a\nquery-based sparse points branch and a BEV branch. The 3D information learned\nin the sparse points branch is shared with the BEV stream via cross-attention,\nwhich enriches the weakened signals of difficult objects on the BEV plane. The\noutputs of both branches are finally fused to generate predicted 3D occupancy.\nWe conduct extensive experiments on the Occ3D-nuScenes and Occ3D-Waymo\nbenchmarks that demonstrate the superiority of our proposed ODG. Moreover, ODG\nalso delivers competitive inference speed when compared to the latest efficient\napproaches.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09417v1", "AI": {"title_translation": "ODG：使用双高斯进行占用预测", "tldr": "本文提出了一种名为ODG的新型3D占用预测方法，结合了BEV和稀疏点表示，以解决现有方法的局限性，并在性能和推理速度上表现出色。", "motivation": "现有的3D占用预测方法计算成本高昂，需要密集的3D特征体和交叉注意力。虽然BEV和稀疏点表示降低了成本，但BEV难以处理小物体，而稀疏点则难以捕捉平面或大型物体。", "method": "本文提出了一种名为ODG的新型3D占用预测方法，结合了BEV和稀疏点表示。它采用双分支设计：一个基于查询的稀疏点分支和一个BEV分支。稀疏点分支学习到的3D信息通过交叉注意力与BEV流共享，以增强BEV平面上困难对象的弱化信号。两个分支的输出最终融合以生成预测的3D占用。", "result": "在Occ3D-nuScenes和Occ3D-Waymo基准测试中进行了广泛的实验，结果表明所提出的ODG方法具有优越性。此外，ODG与最新的高效方法相比，也提供了具有竞争力的推理速度。", "conclusion": "ODG通过结合BEV和稀疏点表示的优势，有效解决了现有3D占用预测方法的局限性，在提供卓越性能的同时保持了高效的推理速度。", "translation": "3D占用提供了用于场景理解的细粒度3D几何和语义信息，这对自动驾驶至关重要。然而，大多数现有方法计算成本高昂，需要密集的3D特征体和交叉注意力来有效聚合信息。最近的工作采用了鸟瞰图（BEV）或稀疏点作为场景表示，大大降低了成本，但仍受各自缺点的影响。更具体地说，BEV在投射到地面后经常会丢失大量信息，难以处理小物体。另一方面，点可以灵活地建模3D中的小物体，但在捕获平面或大型物体时效率低下。为了解决这些挑战，本文提出了一种新颖的3D占用预测方法ODG，它结合了BEV和基于稀疏点的表示。我们提出了一种双分支设计：一个基于查询的稀疏点分支和一个BEV分支。稀疏点分支中学习到的3D信息通过交叉注意力与BEV流共享，这丰富了BEV平面上困难对象的弱化信号。两个分支的输出最终融合以生成预测的3D占用。我们在Occ3D-nuScenes和Occ3D-Waymo基准测试上进行了广泛的实验，证明了我们提出的ODG的优越性。此外，与最新的高效方法相比，ODG还提供了具有竞争力的推理速度。", "summary": "本文提出了一种名为ODG的新型3D占用预测方法，旨在解决现有BEV和稀疏点表示的局限性。ODG采用双分支设计，结合了基于查询的稀疏点分支和BEV分支。通过交叉注意力机制，稀疏点分支的3D信息被共享给BEV流，以增强对小物体和困难对象的感知。最终，两个分支的输出被融合以生成3D占用预测。实验结果表明，ODG在Occ3D-nuScenes和Occ3D-Waymo基准测试上表现出卓越的性能，并具备高效的推理速度。", "keywords": "3D占用预测, 双高斯, BEV, 稀疏点, 自动驾驶", "comments": "ODG的创新之处在于其双分支设计，巧妙地结合了BEV和稀疏点表示的优势，并通过信息共享机制弥补了各自的缺陷。这种融合方法在处理自动驾驶中对大小物体和复杂表面进行3D占用预测时，展现出重要的潜力。其在性能和效率上的平衡，使其成为自动驾驶领域一个有前景的解决方案。"}}
{"id": "2506.09979", "title": "Locomotion on Constrained Footholds via Layered Architectures and Model Predictive Control", "authors": ["Zachary Olkin", "Aaron D. Ames"], "summary": "Computing stabilizing and optimal control actions for legged locomotion in\nreal time is difficult due to the nonlinear, hybrid, and high dimensional\nnature of these robots. The hybrid nature of the system introduces a\ncombination of discrete and continuous variables which causes issues for\nnumerical optimal control. To address these challenges, we propose a layered\narchitecture that separates the choice of discrete variables and a smooth Model\nPredictive Controller (MPC). The layered formulation allows for online\nflexibility and optimality without sacrificing real-time performance through a\ncombination of gradient-free and gradient-based methods. The architecture\nleverages a sampling-based method for determining discrete variables, and a\nclassical smooth MPC formulation using these fixed discrete variables. We\ndemonstrate the results on a quadrupedal robot stepping over gaps and onto\nterrain with varying heights. In simulation, we demonstrate the controller on a\nhumanoid robot for gap traversal. The layered approach is shown to be more\noptimal and reliable than common heuristic-based approaches and faster to\ncompute than pure sampling methods.", "comment": "Submitted to Humanoids 2025", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.09979v1", "AI": {"title_translation": "通过分层架构和模型预测控制实现受限落足点的运动", "tldr": "本文提出了一种分层架构，结合无梯度和基于梯度的方法，以实时为足式机器人计算稳定和最优的运动控制动作，并在四足机器人和人形机器人上进行了验证，证明其比传统方法更优、更可靠、计算更快。", "motivation": "由于足式机器人的非线性、混合和高维特性，实时计算其稳定和最优的运动控制动作非常困难。系统固有的混合特性引入了离散和连续变量的组合，给数值最优控制带来了问题。", "method": "提出了一种分层架构，将离散变量的选择与平滑的模型预测控制器（MPC）分离。该架构利用基于采样的方法确定离散变量，并使用这些固定离散变量进行经典的平滑MPC公式化。通过结合无梯度和基于梯度的方法，实现在线灵活性和最优性，同时不牺牲实时性能。", "result": "在四足机器人上演示了跨越间隙和在不同高度地形上行走的控制器效果。在仿真中，在人形机器人上演示了间隙穿越的控制器。该分层方法被证明比常见的基于启发式的方法更优、更可靠，并且比纯采样方法计算更快。", "conclusion": "分层方法能够有效地解决足式机器人运动控制中的挑战，实现了在线灵活性和最优性，同时保持实时性能，并优于现有的一些方法。", "translation": "计算足式机器人在实时环境中的稳定和最优控制动作是困难的，因为这些机器人具有非线性、混合和高维的特性。系统的混合特性引入了离散和连续变量的组合，这导致了数值最优控制的问题。为了解决这些挑战，我们提出了一种分层架构，将离散变量的选择和平滑的模型预测控制器（MPC）分离。这种分层公式通过结合无梯度和基于梯度的方法，实现了在线灵活性和最优性，而不会牺牲实时性能。该架构利用基于采样的方法确定离散变量，并使用这些固定的离散变量进行经典的平滑MPC公式化。我们在四足机器人跨越间隙和在不同高度地形上行走时演示了结果。在仿真中，我们在人形机器人上演示了间隙穿越的控制器。这种分层方法被证明比常见的基于启发式的方法更优、更可靠，并且比纯采样方法计算更快。", "summary": "本文针对足式机器人运动控制中实时计算稳定最优动作的挑战，提出了一种分层架构。该架构将离散变量选择与平滑MPC分离，通过结合采样和梯度方法，在保证实时性能的同时实现了在线灵活性和最优性。实验证明，该方法在四足和人形机器人上表现出比启发式方法更优、更可靠，且计算速度快于纯采样方法。", "keywords": "足式机器人, 模型预测控制, 分层架构, 实时控制, 运动控制", "comments": "这篇论文的创新点在于其提出的分层架构，它巧妙地结合了无梯度和基于梯度的方法来处理足式机器人运动控制中的离散和连续变量，从而在保证实时性的前提下实现了更高的最优性和可靠性。这种方法为高维、混合系统实时最优控制提供了一个有前景的解决方案，对于机器人领域的实际应用具有重要意义。"}}
{"id": "2506.09427", "title": "A High-Quality Dataset and Reliable Evaluation for Interleaved Image-Text Generation", "authors": ["Yukang Feng", "Jianwen Sun", "Chuanhao Li", "Zizhen Li", "Jiaxin Ai", "Fanrui Zhang", "Yifan Chang", "Sizhuo Zhou", "Shenglin Zhang", "Yu Dai", "Kaipeng Zhang"], "summary": "Recent advancements in Large Multimodal Models (LMMs) have significantly\nimproved multimodal understanding and generation. However, these models still\nstruggle to generate tightly interleaved image-text outputs, primarily due to\nthe limited scale, quality and instructional richness of current training\ndatasets. To address this, we introduce InterSyn, a large-scale multimodal\ndataset constructed using our Self-Evaluation with Iterative Refinement (SEIR)\nmethod. InterSyn features multi-turn, instruction-driven dialogues with tightly\ninterleaved imagetext responses, providing rich object diversity and rigorous\nautomated quality refinement, making it well-suited for training\nnext-generation instruction-following LMMs. Furthermore, to address the lack of\nreliable evaluation tools capable of assessing interleaved multimodal outputs,\nwe introduce SynJudge, an automatic evaluation model designed to quantitatively\nassess multimodal outputs along four dimensions: text content, image content,\nimage quality, and image-text synergy.\n  Experimental studies show that the SEIR method leads to substantially higher\ndataset quality compared to an otherwise identical process without refinement.\n  Moreover, LMMs trained on InterSyn achieve uniform performance gains across\nall evaluation metrics, confirming InterSyn's utility for advancing multimodal\nsystems.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09427v1", "AI": {"title_translation": "高质量交错图像-文本生成数据集与可靠评估方法", "tldr": "该研究介绍了InterSyn，一个使用SEIR方法构建的大规模高质量交错图像-文本数据集，以及SynJudge，一个用于评估交错多模态输出的自动评估模型，旨在解决现有LMM在生成交错内容时面临的数据集和评估工具不足的问题。", "motivation": "当前的大型多模态模型（LMMs）在生成紧密交错的图像-文本输出方面仍面临挑战，主要原因是现有训练数据集的规模、质量和指令丰富性有限。此外，缺乏可靠的评估工具来评估交错多模态输出。", "method": "研究引入了InterSyn，一个大规模多模态数据集，该数据集使用自评估与迭代细化（SEIR）方法构建，具有多轮、指令驱动的对话和紧密交错的图像-文本响应。同时，引入了SynJudge，一个自动评估模型，用于定量评估多模态输出的四个维度：文本内容、图像内容、图像质量和图像-文本协同。", "result": "实验研究表明，与没有细化的相同过程相比，SEIR方法显著提高了数据集质量。此外，在InterSyn上训练的LMMs在所有评估指标上都实现了统一的性能提升。", "conclusion": "InterSyn数据集和SynJudge评估模型有效解决了LMM在交错图像-文本生成方面的挑战，并被证实对提升多模态系统性能具有实用性。", "translation": "大型多模态模型（LMMs）的最新进展显著提升了多模态理解和生成能力。然而，这些模型在生成紧密交错的图像-文本输出方面仍面临挑战，这主要归因于当前训练数据集的规模、质量和指令丰富性有限。为了解决这个问题，我们引入了InterSyn，一个使用自评估与迭代细化（SEIR）方法构建的大规模多模态数据集。InterSyn具有多轮、指令驱动的对话和紧密交错的图像-文本响应，提供了丰富的对象多样性和严格的自动化质量细化，使其非常适合训练下一代指令遵循LMMs。此外，为了解决缺乏能够评估交错多模态输出的可靠评估工具的问题，我们引入了SynJudge，一个自动评估模型，旨在从文本内容、图像内容、图像质量和图像-文本协同四个维度定量评估多模态输出。实验研究表明，与没有细化的相同过程相比，SEIR方法显著提高了数据集质量。此外，在InterSyn上训练的LMMs在所有评估指标上都实现了统一的性能提升，证实了InterSyn在推进多模态系统方面的实用性。", "summary": "本文介绍了InterSyn，一个通过自评估与迭代细化（SEIR）方法构建的大规模、高质量、指令驱动的交错图像-文本数据集，旨在解决大型多模态模型（LMMs）在生成此类内容时面临的数据集限制。同时，提出了SynJudge，一个自动评估模型，用于多维度评估交错多模态输出的质量。实验证明SEIR方法能显著提高数据集质量，并且在InterSyn上训练的LMMs在各项评估指标上均表现出性能提升，验证了InterSyn对推进多模态系统的有效性。", "keywords": "交错图像-文本生成, 大规模多模态模型, 数据集, 评估, InterSyn, SynJudge", "comments": "该论文通过引入高质量数据集构建方法（SEIR）和专门的评估工具（SynJudge），创新性地解决了当前LMM在生成复杂交错多模态内容方面的核心挑战。InterSyn数据集的规模和质量，以及SynJudge的量化评估能力，对于推动下一代指令遵循LMM的发展具有重要意义。"}}
{"id": "2506.09247", "title": "Agent-based Condition Monitoring Assistance with Multimodal Industrial Database Retrieval Augmented Generation", "authors": ["Karl Löwenmark", "Daniel Strömbergsson", "Chang Liu", "Marcus Liwicki", "Fredrik Sandin"], "summary": "Condition monitoring (CM) plays a crucial role in ensuring reliability and\nefficiency in the process industry. Although computerised maintenance systems\neffectively detect and classify faults, tasks like fault severity estimation,\nand maintenance decisions still largely depend on human expert analysis. The\nanalysis and decision making automatically performed by current systems\ntypically exhibit considerable uncertainty and high false alarm rates, leading\nto increased workload and reduced efficiency.\n  This work integrates large language model (LLM)-based reasoning agents with\nCM workflows to address analyst and industry needs, namely reducing false\nalarms, enhancing fault severity estimation, improving decision support, and\noffering explainable interfaces. We propose MindRAG, a modular framework\ncombining multimodal retrieval-augmented generation (RAG) with novel vector\nstore structures designed specifically for CM data. The framework leverages\nexisting annotations and maintenance work orders as surrogates for labels in a\nsupervised learning protocol, addressing the common challenge of training\npredictive models on unlabelled and noisy real-world datasets.\n  The primary contributions include: (1) an approach for structuring industry\nCM data into a semi-structured multimodal vector store compatible with\nLLM-driven workflows; (2) developing multimodal RAG techniques tailored for CM\ndata; (3) developing practical reasoning agents capable of addressing\nreal-world CM queries; and (4) presenting an experimental framework for\nintegrating and evaluating such agents in realistic industrial scenarios.\nPreliminary results, evaluated with the help of an experienced analyst,\nindicate that MindRAG provide meaningful decision support for more efficient\nmanagement of alarms, thereby improving the interpretability of CM systems.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09247v1", "AI": {"title_translation": "基于Agent的多模态工业数据库检索增强生成辅助状态监测", "tldr": "MindRAG是一个结合LLM和多模态RAG的框架，旨在通过提供更好的决策支持和可解释性，减少工业状态监测中的误报并提高效率。", "motivation": "现有计算机化维护系统在故障严重性估计和维护决策方面仍依赖人工，且存在高不确定性和高误报率，导致工作量增加和效率降低。", "method": "提出MindRAG框架，一个将LLM推理代理与多模态检索增强生成(RAG)技术相结合的模块化框架，并为状态监测数据设计了新颖的向量存储结构，利用现有标注和维护工单作为监督学习的替代标签。", "result": "MindRAG能为更高效的警报管理提供有意义的决策支持，并提高状态监测系统的可解释性。", "conclusion": "MindRAG框架通过集成LLM代理和多模态RAG技术，有效解决了工业状态监测中的挑战，显著提升了决策支持能力和系统可解释性，从而提高了效率。", "translation": "状态监测（CM）在确保过程工业的可靠性和效率方面发挥着关键作用。尽管计算机化维护系统能有效检测和分类故障，但故障严重性估计和维护决策等任务仍主要依赖于人类专家分析。当前系统自动执行的分析和决策通常表现出相当大的不确定性和高误报率，导致工作量增加和效率降低。\n这项工作将基于大型语言模型（LLM）的推理代理与CM工作流程集成，以满足分析师和行业需求，即减少误报、增强故障严重性估计、改进决策支持并提供可解释的界面。我们提出了MindRAG，一个模块化框架，它将多模态检索增强生成（RAG）与专为CM数据设计的新颖向量存储结构相结合。该框架利用现有注释和维护工单作为监督学习协议中标签的替代品，解决了在未标记和嘈杂的真实世界数据集上训练预测模型的常见挑战。\n主要贡献包括：（1）一种将工业CM数据构建成与LLM驱动工作流程兼容的半结构化多模态向量存储的方法；（2）开发针对CM数据的多模态RAG技术；（3）开发能够处理真实世界CM查询的实用推理代理；以及（4）提出一个用于在实际工业场景中集成和评估此类代理的实验框架。在经验丰富的分析师的帮助下评估的初步结果表明，MindRAG为更高效的警报管理提供了有意义的决策支持，从而提高了CM系统的可解释性。", "summary": "本文提出了MindRAG框架，它将大型语言模型（LLM）推理代理与多模态检索增强生成（RAG）技术相结合，以解决工业状态监测中故障诊断不确定性高、误报多、决策依赖人工等问题。MindRAG通过构建多模态向量存储并利用现有数据作为标签，旨在提供更准确的故障严重性估计、改进决策支持和可解释的接口。初步结果显示，MindRAG能有效提高警报管理效率和系统可解释性。", "keywords": "状态监测, 大型语言模型, 检索增强生成, 工业数据库, 决策支持", "comments": "MindRAG创新性地将LLM与多模态RAG应用于工业状态监测领域，特别是在处理未标记和嘈杂的真实世界数据方面，通过利用现有维护工单作为标签，提供了一个实用的解决方案。其关注点在于减少误报、提高决策支持和可解释性，对提升工业维护效率具有重要意义。"}}
{"id": "2506.09990", "title": "Chain-of-Action: Trajectory Autoregressive Modeling for Robotic Manipulation", "authors": ["Wenbo Zhang", "Tianrun Hu", "Yanyuan Qiao", "Hanbo Zhang", "Yuchu Qin", "Yang Li", "Jiajun Liu", "Tao Kong", "Lingqiao Liu", "Xiao Ma"], "summary": "We present Chain-of-Action (CoA), a novel visuo-motor policy paradigm built\nupon Trajectory Autoregressive Modeling. Unlike conventional approaches that\npredict next step action(s) forward, CoA generates an entire trajectory by\nexplicit backward reasoning with task-specific goals through an action-level\nChain-of-Thought (CoT) process. This process is unified within a single\nautoregressive structure: (1) the first token corresponds to a stable keyframe\naction that encodes the task-specific goals; and (2) subsequent action tokens\nare generated autoregressively, conditioned on the initial keyframe and\npreviously predicted actions. This backward action reasoning enforces a\nglobal-to-local structure, allowing each local action to be tightly constrained\nby the final goal. To further realize the action reasoning structure, CoA\nincorporates four complementary designs: continuous action token\nrepresentation; dynamic stopping for variable-length trajectory generation;\nreverse temporal ensemble; and multi-token prediction to balance action chunk\nmodeling with global structure. As a result, CoA gives strong spatial\ngeneralization capabilities while preserving the flexibility and simplicity of\na visuo-motor policy. Empirically, we observe CoA achieves the state-of-the-art\nperformance across 60 RLBench tasks and 8 real-world manipulation tasks.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.09990v1", "AI": {"title_translation": "Chain-of-Action: 机器人操纵的轨迹自回归建模", "tldr": "CoA是一种新的视觉-运动策略，通过动作级的思维链过程以自回归方式向后推理生成完整轨迹，在机器人操纵任务上实现了最先进的性能。", "motivation": "传统方法预测下一步动作，而CoA旨在通过对任务特定目标进行显式向后推理来生成整个轨迹，以解决传统方法可能缺乏全局约束的问题。", "method": "本文提出了Chain-of-Action (CoA)，一种基于轨迹自回归建模的视觉-运动策略范式。它通过动作级思维链（CoT）过程，对任务特定目标进行显式向后推理，生成整个轨迹。该过程统一在一个单一的自回归结构中：(1) 第一个token对应一个编码任务特定目标的稳定关键帧动作；(2) 后续动作token在初始关键帧和先前预测动作的条件下自回归生成。这种向后动作推理强制执行一种从全局到局部的结构。CoA还融合了四种互补设计：连续动作token表示；用于变长轨迹生成的动态停止；逆向时间集成；以及多token预测。", "result": "CoA在60个RLBench任务和8个真实世界操纵任务上取得了最先进的性能，并展现出强大的空间泛化能力，同时保持了视觉-运动策略的灵活性和简单性。", "conclusion": "CoA通过其独特的向后动作推理和自回归结构，显著提升了机器人操纵任务的性能和泛化能力。", "translation": "我们提出了Chain-of-Action (CoA)，一种基于轨迹自回归建模的新型视觉-运动策略范式。与预测下一步动作的传统方法不同，CoA通过动作级思维链（CoT）过程，对任务特定目标进行显式向后推理，从而生成整个轨迹。这个过程统一在一个单一的自回归结构中：(1) 第一个token对应一个编码任务特定目标的稳定关键帧动作；(2) 后续动作token在初始关键帧和先前预测动作的条件下自回归生成。这种向后动作推理强制执行一种从全局到局部的结构，使得每个局部动作都受到最终目标的严格约束。为了进一步实现动作推理结构，CoA融合了四种互补设计：连续动作token表示；用于变长轨迹生成的动态停止；逆向时间集成；以及多token预测，以平衡动作块建模与全局结构。结果，CoA在保持视觉-运动策略的灵活性和简单性的同时，提供了强大的空间泛化能力。经验上，我们观察到CoA在60个RLBench任务和8个真实世界操纵任务上取得了最先进的性能。", "summary": "本文提出了Chain-of-Action (CoA)，一种新颖的视觉-运动策略，它采用轨迹自回归建模。与传统的前向预测不同，CoA通过动作级的思维链过程，对任务目标进行显式向后推理来生成完整的动作轨迹。该方法通过将初始关键帧动作与后续自回归生成的动作相结合，实现了全局到局部的约束。CoA还整合了连续动作token表示、动态停止、逆向时间集成和多token预测等设计。实验证明，CoA在多项RLBench和真实世界机器人操纵任务中实现了最先进的性能，并展现出强大的空间泛化能力。", "keywords": "轨迹自回归建模, 机器人操纵, Chain-of-Action, 视觉-运动策略, 向后推理", "comments": "CoA的创新点在于其独特的“向后推理”和“动作级思维链”机制，这与传统的“前向预测”形成鲜明对比，有效地将全局目标约束到局部动作。这种全局-局部结构对于复杂机器人操纵任务的轨迹生成具有重要意义，增强了策略的泛化能力。"}}
{"id": "2506.09429", "title": "A Novel Lightweight Transformer with Edge-Aware Fusion for Remote Sensing Image Captioning", "authors": ["Swadhin Das", "Divyansh Mundra", "Priyanshu Dayal", "Raksha Sharma"], "summary": "Transformer-based models have achieved strong performance in remote sensing\nimage captioning by capturing long-range dependencies and contextual\ninformation. However, their practical deployment is hindered by high\ncomputational costs, especially in multi-modal frameworks that employ separate\ntransformer-based encoders and decoders. In addition, existing remote sensing\nimage captioning models primarily focus on high-level semantic extraction while\noften overlooking fine-grained structural features such as edges, contours, and\nobject boundaries. To address these challenges, a lightweight transformer\narchitecture is proposed by reducing the dimensionality of the encoder layers\nand employing a distilled version of GPT-2 as the decoder. A knowledge\ndistillation strategy is used to transfer knowledge from a more complex teacher\nmodel to improve the performance of the lightweight network. Furthermore, an\nedge-aware enhancement strategy is incorporated to enhance image representation\nand object boundary understanding, enabling the model to capture fine-grained\nspatial details in remote sensing images. Experimental results demonstrate that\nthe proposed approach significantly improves caption quality compared to\nstate-of-the-art methods.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09429v1", "AI": {"title_translation": "结合边缘感知的遥感图像字幕生成新型轻量级Transformer", "tldr": "提出一种轻量级Transformer模型，结合边缘感知融合策略，用于遥感图像字幕生成，有效降低计算成本并提升字幕质量。", "motivation": "现有Transformer模型在遥感图像字幕生成中计算成本高；多模态框架部署困难；现有模型忽视边缘、轮廓等细粒度结构特征。", "method": "提出轻量级Transformer架构，通过降低编码器层维度和采用蒸馏版GPT-2作为解码器。采用知识蒸馏策略从复杂教师模型向轻量级网络迁移知识。引入边缘感知增强策略，增强图像表示和对象边界理解，捕获细粒度空间细节。", "result": "实验结果表明，所提方法与现有最先进方法相比，显著提高了字幕质量。", "conclusion": "该研究提出的轻量级Transformer模型结合边缘感知融合策略，有效解决了遥感图像字幕生成中的计算成本高昂和细粒度特征捕捉不足的问题，显著提升了字幕生成质量。", "translation": "基于Transformer的模型通过捕获长距离依赖和上下文信息，在遥感图像字幕生成方面取得了强大的性能。然而，其高计算成本阻碍了实际部署，尤其是在采用独立基于Transformer的编码器和解码器的多模态框架中。此外，现有的遥感图像字幕生成模型主要关注高级语义提取，却常常忽略了边缘、轮廓和对象边界等细粒度结构特征。为了解决这些挑战，本文提出了一种轻量级Transformer架构，通过降低编码器层的维度并采用GPT-2的蒸馏版本作为解码器。采用知识蒸馏策略将知识从更复杂的教师模型迁移到轻量级网络以提高其性能。此外，还引入了边缘感知增强策略，以增强图像表示和对象边界理解，使模型能够捕获遥感图像中的细粒度空间细节。实验结果表明，所提出的方法与现有最先进的方法相比，显著提高了字幕质量。", "summary": "本文提出了一种用于遥感图像字幕生成的轻量级Transformer模型，旨在解决现有模型计算成本高昂和忽略细粒度结构特征的问题。该模型通过降低编码器维度和使用蒸馏版GPT-2作为解码器实现轻量化，并结合知识蒸馏和边缘感知增强策略来提升性能和捕获图像细节。实验证明，该方法显著提升了字幕生成质量。", "keywords": "遥感图像字幕生成, 轻量级Transformer, 边缘感知融合, 知识蒸馏, 细粒度特征", "comments": "该研究创新性地结合了轻量化Transformer设计、知识蒸馏和边缘感知机制，有效解决了遥感图像字幕生成中计算效率与细节捕捉的平衡问题。其降低部署成本并提升细粒度特征理解的能力，对于实际应用具有重要意义。"}}
{"id": "2506.09258", "title": "CFMI: Flow Matching for Missing Data Imputation", "authors": ["Vaidotas Simkus", "Michael U. Gutmann"], "summary": "We introduce conditional flow matching for imputation (CFMI), a new\ngeneral-purpose method to impute missing data. The method combines continuous\nnormalising flows, flow-matching, and shared conditional modelling to deal with\nintractabilities of traditional multiple imputation. Our comparison with nine\nclassical and state-of-the-art imputation methods on 24 small to\nmoderate-dimensional tabular data sets shows that CFMI matches or outperforms\nboth traditional and modern techniques across a wide range of metrics. Applying\nthe method to zero-shot imputation of time-series data, we find that it matches\nthe accuracy of a related diffusion-based method while outperforming it in\nterms of computational efficiency. Overall, CFMI performs at least as well as\ntraditional methods on lower-dimensional data while remaining scalable to\nhigh-dimensional settings, matching or exceeding the performance of other deep\nlearning-based approaches, making it a go-to imputation method for a wide range\nof data types and dimensionalities.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09258v1", "AI": {"title_translation": "CFMI：缺失数据插补的流匹配", "tldr": "CFMI是一种新的通用缺失数据插补方法，它结合了连续归一化流、流匹配和共享条件建模，在各种数据集和维度上表现出与现有方法相当或更优的性能，并且计算效率更高。", "motivation": "解决传统多重插补的难处理性。", "method": "引入条件流匹配插补（CFMI），该方法结合了连续归一化流、流匹配和共享条件建模。", "result": "在24个小到中等维度的表格数据集上，CFMI在各种指标上匹配或优于传统和现代插补技术。应用于时间序列数据的零样本插补时，其精度与相关的基于扩散的方法相匹配，但在计算效率方面表现更优。在低维数据上，CFMI至少与传统方法表现一样好，同时可扩展到高维设置，匹配或超过其他基于深度学习的方法的性能。", "conclusion": "CFMI是一种适用于广泛数据类型和维度，可作为首选的通用缺失数据插补方法。", "translation": "我们引入了条件流匹配插补（CFMI），这是一种新的通用缺失数据插补方法。该方法结合了连续归一化流、流匹配和共享条件建模，以解决传统多重插补的难处理性。我们与24个小到中等维度表格数据集上的九种经典和最先进的插补方法进行了比较，结果表明CFMI在各种指标上与传统和现代技术相当或更优。将该方法应用于时间序列数据的零样本插补时，我们发现其精度与相关的基于扩散的方法相匹配，但在计算效率方面表现更优。总的来说，CFMI在低维数据上的表现至少与传统方法一样好，同时仍可扩展到高维设置，匹配或超过其他基于深度学习的方法的性能，使其成为适用于各种数据类型和维度的一种首选插补方法。", "summary": "本文提出了一种名为条件流匹配插补（CFMI）的新型通用缺失数据插补方法。CFMI结合了连续归一化流、流匹配和共享条件建模，旨在克服传统多重插补的局限性。实验结果表明，在多种表格数据集和时间序列数据上，CFMI在性能上与现有经典和最先进的插补方法相当或更优，并且在高维设置下具有良好的可扩展性和计算效率，使其成为一种多功能且高效的缺失数据插补工具。", "keywords": "缺失数据插补, 流匹配, 条件建模, 归一化流, 深度学习", "comments": "CFMI的创新之处在于将流匹配技术应用于缺失数据插补，并结合了共享条件建模，有效地解决了传统方法在大规模和高维数据上的挑战。其在计算效率上的优势，尤其是在时间序列数据上的表现，使其成为一个有前景的通用插补方法，对于实际应用具有重要意义。"}}
{"id": "2506.09994", "title": "eFlesh: Highly customizable Magnetic Touch Sensing using Cut-Cell Microstructures", "authors": ["Venkatesh Pattabiraman", "Zizhou Huang", "Daniele Panozzo", "Denis Zorin", "Lerrel Pinto", "Raunaq Bhirangi"], "summary": "If human experience is any guide, operating effectively in unstructured\nenvironments -- like homes and offices -- requires robots to sense the forces\nduring physical interaction. Yet, the lack of a versatile, accessible, and\neasily customizable tactile sensor has led to fragmented, sensor-specific\nsolutions in robotic manipulation -- and in many cases, to force-unaware,\nsensorless approaches. With eFlesh, we bridge this gap by introducing a\nmagnetic tactile sensor that is low-cost, easy to fabricate, and highly\ncustomizable. Building an eFlesh sensor requires only four components: a\nhobbyist 3D printer, off-the-shelf magnets (<$5), a CAD model of the desired\nshape, and a magnetometer circuit board. The sensor is constructed from tiled,\nparameterized microstructures, which allow for tuning the sensor's geometry and\nits mechanical response. We provide an open-source design tool that converts\nconvex OBJ/STL files into 3D-printable STLs for fabrication. This modular\ndesign framework enables users to create application-specific sensors, and to\nadjust sensitivity depending on the task. Our sensor characterization\nexperiments demonstrate the capabilities of eFlesh: contact localization RMSE\nof 0.5 mm, and force prediction RMSE of 0.27 N for normal force and 0.12 N for\nshear force. We also present a learned slip detection model that generalizes to\nunseen objects with 95% accuracy, and visuotactile control policies that\nimprove manipulation performance by 40% over vision-only baselines -- achieving\n91% average success rate for four precise tasks that require sub-mm accuracy\nfor successful completion. All design files, code and the CAD-to-eFlesh STL\nconversion tool are open-sourced and available on https://e-flesh.com.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.09994v1", "AI": {"title_translation": "eFlesh: 使用切割单元微结构的高度可定制磁性触觉传感", "tldr": "eFlesh是一种低成本、易于制造且高度可定制的磁性触觉传感器，旨在解决机器人操作中缺乏通用触觉传感器的挑战，显著提高机器人操作性能。", "motivation": "机器人需要在非结构化环境中感知物理交互力，但目前缺乏通用、易于获取和定制的触觉传感器，导致机器人操作中出现了碎片化、特定于传感器的解决方案，甚至采用缺乏力感知能力的无传感器方法。", "method": "eFlesh是一种磁性触觉传感器，仅需四个组件即可构建：一个业余爱好者3D打印机、现成磁铁、所需形状的CAD模型和一块磁力计电路板。传感器由平铺的参数化微结构构成，允许调整传感器的几何形状及其机械响应。该项目提供了一个开源设计工具，可以将凸面OBJ/STL文件转换为可3D打印的STL文件，从而实现模块化设计框架，使用户能够创建特定应用的传感器并调整灵敏度。", "result": "eFlesh的传感器特性实验显示：接触定位RMSE为0.5毫米；法向力预测RMSE为0.27牛顿，剪切力预测RMSE为0.12牛顿。此外，提出的学习型滑移检测模型对未知物体具有95%的泛化准确率。视触觉控制策略使操作性能比仅视觉基线提高了40%，在需要亚毫米精度的四项精确任务中，平均成功率达到91%。所有设计文件、代码和转换工具均已开源。", "conclusion": "eFlesh通过提供一种低成本、易于制造且高度可定制的磁性触觉传感器，成功弥补了机器人触觉传感领域的空白，显著提升了机器人在非结构化环境中的操作性能和成功率。", "translation": "如果人类经验是任何指导，那么在非结构化环境（如家庭和办公室）中有效操作，需要机器人感知物理交互过程中的力。然而，由于缺乏多功能、易于获取和易于定制的触觉传感器，导致机器人操作中出现了碎片化、特定于传感器的解决方案——在许多情况下，甚至采用缺乏力感知能力的无传感器方法。通过eFlesh，我们弥补了这一空白，引入了一种低成本、易于制造且高度可定制的磁性触觉传感器。构建eFlesh传感器仅需四个组件：一个业余爱好者3D打印机、现成磁铁（<5美元）、所需形状的CAD模型和一块磁力计电路板。该传感器由平铺的参数化微结构构成，这允许调整传感器的几何形状及其机械响应。我们提供了一个开源设计工具，可以将凸面OBJ/STL文件转换为可3D打印的STL文件进行制造。这种模块化设计框架使用户能够创建特定应用的传感器，并根据任务调整灵敏度。我们的传感器特性实验证明了eFlesh的能力：接触定位RMSE为0.5毫米，法向力预测RMSE为0.27牛顿，剪切力预测RMSE为0.12牛顿。我们还提出了一种学习型滑移检测模型，该模型对未见过的物体具有95%的泛化准确率，以及视触觉控制策略，与仅视觉基线相比，操作性能提高了40%——在需要亚毫米精度才能成功完成的四项精确任务中，平均成功率达到91%。所有设计文件、代码和CAD-to-eFlesh STL转换工具均已开源，并可在https://e-flesh.com上获取。", "summary": "本文介绍了eFlesh，一种低成本、易于制造且高度可定制的磁性触觉传感器，旨在解决机器人操作中缺乏通用触觉传感器的挑战。eFlesh传感器仅需四种组件即可构建，并利用参数化微结构实现几何和机械响应的调整。通过开源设计工具，用户可以创建特定应用的传感器并调整灵敏度。实验结果表明，eFlesh在接触定位、力预测、滑移检测和视触觉控制方面表现出色，显著提高了机器人操作的成功率。", "keywords": "磁性触觉传感, 可定制, 微结构, 机器人操作, 开源", "comments": "eFlesh的创新之处在于其高度可定制性、低成本和易于制造的特性，通过开源设计工具和模块化框架，极大地降低了触觉传感器的开发和应用门槛。这对于推动机器人技术在非结构化环境中的实际应用具有重要意义，尤其是在需要精细操作和力感知的领域。"}}
{"id": "2506.09445", "title": "TOGA: Temporally Grounded Open-Ended Video QA with Weak Supervision", "authors": ["Ayush Gupta", "Anirban Roy", "Rama Chellappa", "Nathaniel D. Bastian", "Alvaro Velasquez", "Susmit Jha"], "summary": "We address the problem of video question answering (video QA) with temporal\ngrounding in a weakly supervised setup, without any temporal annotations. Given\na video and a question, we generate an open-ended answer grounded with the\nstart and end time. For this task, we propose TOGA: a vision-language model for\nTemporally Grounded Open-Ended Video QA with Weak Supervision. We instruct-tune\nTOGA to jointly generate the answer and the temporal grounding. We operate in a\nweakly supervised setup where the temporal grounding annotations are not\navailable. We generate pseudo labels for temporal grounding and ensure the\nvalidity of these labels by imposing a consistency constraint between the\nquestion of a grounding response and the response generated by a question\nreferring to the same temporal segment. We notice that jointly generating the\nanswers with the grounding improves performance on question answering as well\nas grounding. We evaluate TOGA on grounded QA and open-ended QA tasks. For\ngrounded QA, we consider the NExT-GQA benchmark which is designed to evaluate\nweakly supervised grounded question answering. For open-ended QA, we consider\nthe MSVD-QA and ActivityNet-QA benchmarks. We achieve state-of-the-art\nperformance for both tasks on these benchmarks.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09445v1", "AI": {"title_translation": "TOGA：弱监督下时序定位开放式视频问答", "tldr": "提出TOGA模型，在没有时序标注的情况下，通过弱监督生成视频问答的时序定位和开放式答案，并在多项基准测试中达到SOTA。", "motivation": "解决在缺乏时序标注的弱监督设置下，视频问答（Video QA）中时序定位和开放式答案生成的问题。", "method": "提出TOGA视觉-语言模型，通过指令微调联合生成答案和时序定位。在弱监督设置下，生成时序定位的伪标签，并通过在问答和同一时序片段的问答响应之间施加一致性约束来确保伪标签的有效性。", "result": "在Grounded QA任务（NExT-GQA基准）和Open-Ended QA任务（MSVD-QA和ActivityNet-QA基准）上均实现了最先进的性能。", "conclusion": "联合生成答案和时序定位可以提高问答和定位的性能。", "translation": "我们解决了在弱监督设置下（没有任何时序标注）进行时序定位视频问答（video QA）的问题。给定一个视频和一个问题，我们生成一个以开始和结束时间为基础的开放式答案。为此，我们提出了TOGA：一个用于弱监督下时序定位开放式视频问答的视觉-语言模型。我们对TOGA进行指令微调，以联合生成答案和时序定位。我们在没有时序定位标注的弱监督设置下运行。我们生成时序定位的伪标签，并通过在定位响应的问题与指代相同时间段的问题所生成的响应之间施加一致性约束来确保这些标签的有效性。我们注意到，联合生成答案和定位可以提高问答和定位的性能。我们在定位问答和开放式问答任务上评估了TOGA。对于定位问答，我们考虑了NExT-GQA基准，该基准旨在评估弱监督定位问答。对于开放式问答，我们考虑了MSVD-QA和ActivityNet-QA基准。在这两个任务的这些基准上，我们都取得了最先进的性能。", "summary": "本文提出了一种名为TOGA的视觉-语言模型，旨在解决弱监督下时序定位开放式视频问答（Video QA）问题，即在没有时序标注的情况下，为视频问题生成带有开始和结束时间的开放式答案。TOGA通过指令微调联合生成答案和时序定位，并利用伪标签和一致性约束来处理弱监督设置。实验结果表明，联合生成答案和定位能提升性能，并且TOGA在NExT-GQA、MSVD-QA和ActivityNet-QA等多个基准测试中均达到了最先进水平。", "keywords": "视频问答, 时序定位, 弱监督, 开放式问答, 视觉-语言模型", "comments": "这篇论文的创新点在于在弱监督环境下，通过生成伪标签和施加一致性约束，成功地实现了视频问答的时序定位和开放式答案的联合生成。这种方法避免了对昂贵的时序标注的依赖，大大降低了数据标注成本，对于实际应用具有重要意义。联合生成答案和定位的策略也提升了模型在两个任务上的表现，显示了其方法的有效性。"}}
{"id": "2506.09270", "title": "Uncertainty Prioritized Experience Replay", "authors": ["Rodrigo Carrasco-Davis", "Sebastian Lee", "Claudia Clopath", "Will Dabney"], "summary": "Prioritized experience replay, which improves sample efficiency by selecting\nrelevant transitions to update parameter estimates, is a crucial component of\ncontemporary value-based deep reinforcement learning models. Typically,\ntransitions are prioritized based on their temporal difference error. However,\nthis approach is prone to favoring noisy transitions, even when the value\nestimation closely approximates the target mean. This phenomenon resembles the\nnoisy TV problem postulated in the exploration literature, in which\nexploration-guided agents get stuck by mistaking noise for novelty. To mitigate\nthe disruptive effects of noise in value estimation, we propose using epistemic\nuncertainty estimation to guide the prioritization of transitions from the\nreplay buffer. Epistemic uncertainty quantifies the uncertainty that can be\nreduced by learning, hence reducing transitions sampled from the buffer\ngenerated by unpredictable random processes. We first illustrate the benefits\nof epistemic uncertainty prioritized replay in two tabular toy models: a simple\nmulti-arm bandit task, and a noisy gridworld. Subsequently, we evaluate our\nprioritization scheme on the Atari suite, outperforming quantile regression\ndeep Q-learning benchmarks; thus forging a path for the use of uncertainty\nprioritized replay in reinforcement learning agents.", "comment": "Accepted at Reinforcement Learning Conference", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09270v1", "AI": {"title_translation": "不确定性优先经验回放", "tldr": "本文提出一种新的经验回放机制，利用认知不确定性而非时间差分误差来优先选择经验，以解决传统优先经验回放易受噪声影响的问题，并在Atari等任务上取得了优异表现。", "motivation": "传统的优先经验回放（PER）通过时间差分误差来优先选择经验，但这种方法容易偏爱噪声过渡，即使价值估计接近目标均值。这类似于探索文献中提出的“嘈杂电视问题”，即探索引导的智能体错误地将噪声视为新颖性而陷入困境。", "method": "提出使用认知不确定性估计来指导经验回放缓冲区中的过渡优先级。认知不确定性量化了可以通过学习来减少的不确定性，从而减少了从不可预测的随机过程生成的缓冲区中采样的过渡。", "result": "首先在两个表格玩具模型（简单的多臂老虎机任务和嘈杂的网格世界）中说明了认知不确定性优先回放的益处。随后，在Atari套件上评估了该优先级方案，性能优于分位数回归深度Q学习基准。", "conclusion": "本文提出的不确定性优先经验回放为强化学习智能体中的应用开辟了道路。", "translation": "优先经验回放通过选择相关的过渡来更新参数估计，从而提高样本效率，是当代基于价值的深度强化学习模型的关键组成部分。通常，过渡是根据其时间差分误差进行优先排序的。然而，这种方法容易偏爱噪声过渡，即使价值估计接近目标均值。这种现象类似于探索文献中提出的嘈杂电视问题，其中探索引导的智能体错误地将噪声视为新颖性而陷入困境。为了减轻价值估计中噪声的破坏性影响，我们建议使用认知不确定性估计来指导经验回放缓冲区中过渡的优先级。认知不确定性量化了可以通过学习来减少的不确定性，从而减少了从不可预测的随机过程生成的缓冲区中采样的过渡。我们首先在两个表格玩具模型中说明了认知不确定性优先回放的益处：一个简单的多臂老虎机任务和一个嘈杂的网格世界。随后，我们在Atari套件上评估了我们的优先级方案，其性能优于分位数回归深度Q学习基准；从而为不确定性优先回放应用于强化学习智能体开辟了道路。", "summary": "本文针对现有优先经验回放（PER）在深度强化学习中易受噪声影响的问题，提出了一种新的不确定性优先经验回放（UPE）。UPE利用认知不确定性来指导经验的优先级选择，旨在减少因噪声导致的错误优先排序。实验在多臂老虎机、嘈杂网格世界和Atari套件上进行，结果表明UPE优于现有的基准方法，为强化学习中利用不确定性进行经验回放提供了新方向。", "keywords": "优先经验回放, 认知不确定性, 深度强化学习, 样本效率, 噪声", "comments": "这篇论文通过引入认知不确定性来改进传统的优先经验回放机制，有效解决了“嘈杂电视问题”，即避免智能体将噪声误认为是新颖性。其创新点在于将不确定性量化并应用于经验优先级排序，这对于提高深度强化学习的样本效率和稳定性具有重要意义。在Atari套件上的优异表现也证明了其在实际应用中的潜力。"}}
{"id": "2506.09446", "title": "Harmonizing and Merging Source Models for CLIP-based Domain Generalization", "authors": ["Yuhe Ding", "Jian Liang", "Bo Jiang", "Zi Wang", "Aihua Zheng", "Bin Luo"], "summary": "CLIP-based domain generalization aims to improve model generalization to\nunseen domains by leveraging the powerful zero-shot classification capabilities\nof CLIP and multiple source datasets. Existing methods typically train a single\nmodel across multiple source domains to capture domain-shared information.\nHowever, this paradigm inherently suffers from two types of conflicts: 1)\nsample conflicts, arising from noisy samples and extreme domain shifts among\nsources; and 2) optimization conflicts, stemming from competition and\ntrade-offs during multi-source training. Both hinder the generalization and\nlead to suboptimal solutions. Recent studies have shown that model merging can\neffectively mitigate the competition of multi-objective optimization and\nimprove generalization performance. Inspired by these findings, we propose\nHarmonizing and Merging (HAM), a novel source model merging framework for\nCLIP-based domain generalization. During the training process of the source\nmodels, HAM enriches the source samples without conflicting samples, and\nharmonizes the update directions of all models. Then, a redundancy-aware\nhistorical model merging method is introduced to effectively integrate\nknowledge across all source models. HAM comprehensively consolidates source\ndomain information while enabling mutual enhancement among source models,\nultimately yielding a final model with optimal generalization capabilities.\nExtensive experiments on five widely used benchmark datasets demonstrate the\neffectiveness of our approach, achieving state-of-the-art performance.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09446v1", "AI": {"title_translation": "用于CLIP域泛化的源模型协调与合并", "tldr": "本文提出了HAM框架，通过协调源模型训练和冗余感知模型合并来解决CLIP域泛化中的冲突，实现最先进的性能。", "motivation": "现有的CLIP域泛化方法通过在多个源域上训练单一模型来捕获域共享信息，但这种范式存在两种冲突：1）样本冲突，源于噪声样本和极端域偏移；2）优化冲突，源于多源训练中的竞争和权衡。这两种冲突都阻碍了泛化并导致次优解决方案。", "method": "本文提出了一个新颖的源模型合并框架——协调与合并（HAM）。在源模型训练过程中，HAM在不引入冲突样本的情况下丰富源样本，并协调所有模型的更新方向。然后，引入了一种冗余感知历史模型合并方法，以有效地整合所有源模型中的知识。", "result": "在五个广泛使用的基准数据集上的大量实验证明了我们方法的有效性，实现了最先进的性能。", "conclusion": "HAM全面整合了源域信息，同时实现了源模型之间的相互增强，最终产生了一个具有最佳泛化能力的模型。", "translation": "基于CLIP的域泛化旨在通过利用CLIP强大的零样本分类能力和多个源数据集来提高模型对未见域的泛化能力。现有方法通常在多个源域上训练单个模型以捕获域共享信息。然而，这种范式本身存在两种冲突：1）样本冲突，源于噪声样本和源之间极端的域偏移；2）优化冲突，源于多源训练中的竞争和权衡。两者都阻碍了泛化并导致次优解决方案。最近的研究表明，模型合并可以有效缓解多目标优化的竞争并提高泛化性能。受这些发现的启发，我们提出了协调与合并（HAM），一个用于基于CLIP的域泛化的新颖源模型合并框架。在源模型训练过程中，HAM在不引入冲突样本的情况下丰富源样本，并协调所有模型的更新方向。然后，引入了一种冗余感知历史模型合并方法，以有效地整合所有源模型中的知识。HAM全面整合了源域信息，同时实现了源模型之间的相互增强，最终产生了一个具有最佳泛化能力的模型。在五个广泛使用的基准数据集上的大量实验证明了我们方法的有效性，实现了最先进的性能。", "summary": "本文提出了一种名为HAM（Harmonizing and Merging）的新型源模型合并框架，用于解决CLIP域泛化中现有方法存在的样本冲突和优化冲突问题。HAM通过在训练阶段丰富无冲突样本并协调模型更新方向，并在后续阶段采用冗余感知历史模型合并来有效整合知识。实验结果表明，HAM在多个基准数据集上实现了最先进的泛化性能。", "keywords": "CLIP域泛化, 模型合并, 源模型协调, 样本冲突, 优化冲突", "comments": "该论文的创新点在于将模型合并的思想引入到CLIP域泛化领域，有效地解决了多源训练中的样本和优化冲突。通过协调源模型训练和冗余感知合并，HAM能够全面整合知识并提高泛化能力，为CLIP域泛化提供了一个新颖且有效的解决方案。"}}
{"id": "2506.09272", "title": "G-Sim: Generative Simulations with Large Language Models and Gradient-Free Calibration", "authors": ["Samuel Holt", "Max Ruiz Luyten", "Antonin Berthon", "Mihaela van der Schaar"], "summary": "Constructing robust simulators is essential for asking \"what if?\" questions\nand guiding policy in critical domains like healthcare and logistics. However,\nexisting methods often struggle, either failing to generalize beyond historical\ndata or, when using Large Language Models (LLMs), suffering from inaccuracies\nand poor empirical alignment. We introduce G-Sim, a hybrid framework that\nautomates simulator construction by synergizing LLM-driven structural design\nwith rigorous empirical calibration. G-Sim employs an LLM in an iterative loop\nto propose and refine a simulator's core components and causal relationships,\nguided by domain knowledge. This structure is then grounded in reality by\nestimating its parameters using flexible calibration techniques. Specifically,\nG-Sim can leverage methods that are both likelihood-free and gradient-free with\nrespect to the simulator, such as gradient-free optimization for direct\nparameter estimation or simulation-based inference for obtaining a posterior\ndistribution over parameters. This allows it to handle non-differentiable and\nstochastic simulators. By integrating domain priors with empirical evidence,\nG-Sim produces reliable, causally-informed simulators, mitigating\ndata-inefficiency and enabling robust system-level interventions for complex\ndecision-making.", "comment": "Accepted at the 42nd International Conference on Machine Learning\n  (ICML 2025). 9 pages, 3 figures", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09272v1", "AI": {"title_translation": "G-Sim：基于大型语言模型和无梯度校准的生成式模拟", "tldr": "G-Sim是一个混合框架，它结合了大型语言模型（LLM）驱动的结构设计和无梯度校准技术，以自动化构建鲁棒、因果知情的模拟器，适用于处理复杂决策问题。", "motivation": "在医疗和物流等关键领域，构建鲁棒的模拟器对于提出“假设”问题和指导政策至关重要。然而，现有方法要么难以泛化超出历史数据，要么在使用大型语言模型（LLM）时存在不准确和经验对齐不佳的问题。", "method": "本文提出G-Sim，一个混合框架，通过结合LLM驱动的结构设计和严格的经验校准来自动化模拟器构建。G-Sim在一个迭代循环中使用LLM来提出和完善模拟器的核心组件和因果关系，并由领域知识指导。然后，通过使用灵活的校准技术估计其参数，将该结构与现实相结合。具体来说，G-Sim可以利用对模拟器而言是无似然和无梯度的方法，例如用于直接参数估计的无梯度优化或用于获取参数后验分布的基于模拟的推理。这使其能够处理不可微和随机的模拟器。", "result": "通过将领域先验与经验证据相结合，G-Sim生成了可靠、因果知情的模拟器，减轻了数据效率低下的问题。", "conclusion": "G-Sim能够实现鲁棒的系统级干预，以应对复杂的决策制定。", "translation": "构建鲁棒的模拟器对于提出“假设”问题和指导医疗和物流等关键领域的政策至关重要。然而，现有方法往往存在困难，要么无法泛化超出历史数据，要么在使用大型语言模型（LLM）时存在不准确和经验对齐不佳的问题。我们引入G-Sim，一个混合框架，通过结合LLM驱动的结构设计与严格的经验校准来自动化模拟器构建。G-Sim在一个迭代循环中使用LLM来提出和完善模拟器的核心组件和因果关系，并由领域知识指导。然后，通过使用灵活的校准技术估计其参数，将该结构与现实相结合。具体来说，G-Sim可以利用对模拟器而言是无似然和无梯度的方法，例如用于直接参数估计的无梯度优化或用于获取参数后验分布的基于模拟的推理。这使其能够处理不可微和随机的模拟器。通过将领域先验与经验证据相结合，G-Sim生成了可靠、因果知情的模拟器，减轻了数据效率低下的问题，并为复杂决策制定提供了鲁棒的系统级干预。", "summary": "G-Sim是一个新颖的混合框架，旨在克服现有模拟器构建方法的局限性。它结合了大型语言模型（LLM）的结构设计能力和无梯度、无似然的经验校准技术。通过LLM迭代生成和细化模拟器结构，并利用例如无梯度优化等方法进行参数估计，G-Sim能够处理不可微和随机的模拟器。该方法旨在提供可靠、因果知情的模拟器，以支持关键领域的复杂决策和系统级干预。", "keywords": "生成式模拟, 大型语言模型, 无梯度校准, 因果推理, 模拟器构建", "comments": "G-Sim的创新之处在于其混合方法，将LLM的强大生成能力与梯度无关的校准技术相结合。这使其能够克服传统模拟器在泛化能力和处理复杂、不可微系统方面的挑战。其对因果关系和数据效率的关注，使其在需要鲁棒决策支持的领域具有重要意义。"}}
{"id": "2506.09460", "title": "Evidential Deep Learning with Spectral-Spatial Uncertainty Disentanglement for Open-Set Hyperspectral Domain Generalization", "authors": ["Amirreza Khoshbakht", "Erchan Aptoula"], "summary": "Open-set domain generalization(OSDG) for hyperspectral image classification\npresents significant challenges due to the presence of unknown classes in\ntarget domains and the need for models to generalize across multiple unseen\ndomains without target-specific adaptation. Existing domain adaptation methods\nassume access to target domain data during training and fail to address the\nfundamental issue of domain shift when unknown classes are present, leading to\nnegative transfer and reduced classification performance. To address these\nlimitations, we propose a novel open-set domain generalization framework that\ncombines four key components: Spectrum-Invariant Frequency Disentanglement\n(SIFD) for domain-agnostic feature extraction, Dual-Channel Residual Network\n(DCRN) for robust spectral-spatial feature learning, Evidential Deep Learning\n(EDL) for uncertainty quantification, and Spectral-Spatial Uncertainty\nDisentanglement (SSUD) for reliable open-set classification. The SIFD module\nextracts domain-invariant spectral features in the frequency domain through\nattention-weighted frequency analysis and domain-agnostic regularization, while\nDCRN captures complementary spectral and spatial information via parallel\npathways with adaptive fusion. EDL provides principled uncertainty estimation\nusing Dirichlet distributions, enabling the SSUD module to make reliable\nopen-set decisions through uncertainty-aware pathway weighting and adaptive\nrejection thresholding. Experimental results on three cross-scene hyperspectral\nclassification tasks show that our approach achieves performance comparable to\nstate-of-the-art domain adaptation methods while requiring no access to the\ntarget domain during training. The implementation will be made available at\nhttps://github.com/amir-khb/SSUDOSDG upon acceptance.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09460v1", "AI": {"title_translation": "基于谱空间不确定性解耦的证据深度学习用于开放集高光谱域泛化", "tldr": "该论文提出了一种新的开放集高光谱域泛化框架，通过谱空间不确定性解耦，在训练阶段无需目标域数据即可处理未知类别和未见域。", "motivation": "开放集域泛化（OSDG）在高光谱图像分类中面临巨大挑战，因为目标域中存在未知类别，且模型需要泛化到多个未见域而无需目标特定适应。现有域适应方法假设训练期间可访问目标域数据，但当存在未知类别时，无法解决域漂移的根本问题，导致负迁移和分类性能下降。", "method": "本研究提出了一种新颖的开放集域泛化框架，结合了四个关键组件：1. 谱不变频率解耦（SIFD）用于域无关特征提取；2. 双通道残差网络（DCRN）用于鲁棒的谱空间特征学习；3. 证据深度学习（EDL）用于不确定性量化；4. 谱空间不确定性解耦（SSUD）用于可靠的开放集分类。SIFD通过注意力加权频率分析和域无关正则化在频域中提取域不变谱特征；DCRN通过并行通路自适应融合捕获互补的谱和空间信息；EDL使用Dirichlet分布提供不确定性估计；SSUD通过不确定性感知通路加权和自适应拒绝阈值进行开放集决策。", "result": "在三个跨场景高光谱分类任务上的实验结果表明，该方法实现了与最先进的域适应方法相当的性能，同时在训练期间无需访问目标域数据。", "conclusion": "所提出的框架通过谱空间不确定性解耦，有效解决了开放集高光谱域泛化的挑战，并在训练阶段无需目标域数据的情况下取得了与现有先进方法相当的性能。", "translation": "开放集域泛化（OSDG）在高光谱图像分类中面临巨大挑战，原因在于目标域中存在未知类别，以及模型需要在没有目标特定适应的情况下泛化到多个未见域。现有的域适应方法假设在训练期间可以访问目标域数据，并且在存在未知类别时未能解决域漂移的根本问题，导致负迁移和分类性能下降。为了解决这些限制，我们提出了一种新颖的开放集域泛化框架，该框架结合了四个关键组件：用于域无关特征提取的谱不变频率解耦（SIFD）、用于鲁棒谱空间特征学习的双通道残差网络（DCRN）、用于不确定性量化的证据深度学习（EDL），以及用于可靠开放集分类的谱空间不确定性解耦（SSUD）。SIFD模块通过注意力加权频率分析和域无关正则化在频域中提取域不变谱特征，而DCRN通过并行通路自适应融合捕获互补的谱和空间信息。EDL使用Dirichlet分布提供有原则的不确定性估计，使SSUD模块能够通过不确定性感知通路加权和自适应拒绝阈值做出可靠的开放集决策。在三个跨场景高光谱分类任务上的实验结果表明，我们的方法达到了与最先进的域适应方法相当的性能，同时在训练期间无需访问目标域。该实现将在接收后在https://github.com/amir-khb/SSUDOSDG 提供。", "summary": "本论文提出了一种新颖的开放集高光谱域泛化框架，旨在解决目标域中未知类别和未见域泛化的问题。该框架整合了谱不变频率解耦、双通道残差网络、证据深度学习和谱空间不确定性解耦。它能够提取域不变特征、学习鲁棒的谱空间信息、量化不确定性并做出可靠的开放集决策。实验结果表明，该方法在无需目标域数据训练的情况下，性能可与现有最先进的域适应方法媲美。", "keywords": "开放集域泛化, 高光谱图像分类, 证据深度学习, 不确定性解耦, 域泛化", "comments": "该论文的创新点在于将证据深度学习与谱空间不确定性解耦相结合，用于开放集域泛化。其重要性体现在无需目标域数据即可实现域泛化，这在实际应用中具有显著优势，克服了传统域适应方法的关键限制。"}}
{"id": "2506.09276", "title": "Learning The Minimum Action Distance", "authors": ["Lorenzo Steccanella", "Joshua B. Evans", "Özgür Şimşek", "Anders Jonsson"], "summary": "This paper presents a state representation framework for Markov decision\nprocesses (MDPs) that can be learned solely from state trajectories, requiring\nneither reward signals nor the actions executed by the agent. We propose\nlearning the minimum action distance (MAD), defined as the minimum number of\nactions required to transition between states, as a fundamental metric that\ncaptures the underlying structure of an environment. MAD naturally enables\ncritical downstream tasks such as goal-conditioned reinforcement learning and\nreward shaping by providing a dense, geometrically meaningful measure of\nprogress. Our self-supervised learning approach constructs an embedding space\nwhere the distances between embedded state pairs correspond to their MAD,\naccommodating both symmetric and asymmetric approximations. We evaluate the\nframework on a comprehensive suite of environments with known MAD values,\nencompassing both deterministic and stochastic dynamics, as well as discrete\nand continuous state spaces, and environments with noisy observations.\nEmpirical results demonstrate that the proposed approach not only efficiently\nlearns accurate MAD representations across these diverse settings but also\nsignificantly outperforms existing state representation methods in terms of\nrepresentation quality.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09276v1", "AI": {"title_translation": "学习最小行动距离", "tldr": "本文提出了一种在MDPs中仅从状态轨迹学习最小行动距离（MAD）的状态表示框架，该框架在各种环境下表现出色并优于现有方法。", "motivation": "现有的状态表示方法可能需要奖励信号或代理执行的动作。本文旨在提出一种新的状态表示框架，可以仅从状态轨迹中学习，捕获环境的底层结构，并支持下游任务如目标条件强化学习和奖励塑造。", "method": "提出学习最小行动距离（MAD），将其定义为状态之间转换所需的最小行动数。采用自监督学习方法构建一个嵌入空间，其中嵌入状态对之间的距离对应于其MAD，可适应对称和非对称近似。", "result": "经验结果表明，所提出的方法不仅在各种设置（包括确定性、随机动力学、离散/连续状态空间和噪声观测环境）中有效地学习了准确的MAD表示，而且在表示质量方面显著优于现有状态表示方法。", "conclusion": "该框架能够高效地学习准确的MAD表示，并在表示质量上超越现有方法，为MDPs提供了一种强大的、无需奖励或动作信号的状态表示。", "translation": "本文提出了一种用于马尔可夫决策过程（MDPs）的状态表示框架，该框架可以仅从状态轨迹中学习，既不需要奖励信号，也不需要代理执行的动作。我们提出学习最小行动距离（MAD），将其定义为状态之间转换所需的最小行动数，作为捕获环境底层结构的基本度量。MAD通过提供密集且具有几何意义的进展度量，自然地支持关键的下游任务，如目标条件强化学习和奖励塑造。我们的自监督学习方法构建了一个嵌入空间，其中嵌入状态对之间的距离对应于它们的MAD，同时适应对称和非对称近似。我们在具有已知MAD值的综合环境中评估了该框架，这些环境涵盖了确定性和随机动力学、离散和连续状态空间以及具有噪声观测的环境。经验结果表明，所提出的方法不仅在这些多样化的设置中有效地学习了准确的MAD表示，而且在表示质量方面显著优于现有状态表示方法。", "summary": "本文介绍了一种用于马尔可夫决策过程（MDPs）的状态表示框架，该框架仅通过状态轨迹进行自监督学习，无需奖励或动作信息。核心思想是学习最小行动距离（MAD），即状态间转换所需的最小行动数，作为环境结构的基本度量。该方法构建一个嵌入空间，使状态对间的距离反映其MAD。实验证明，该方法在多种复杂环境下能高效学习准确的MAD表示，并显著优于现有状态表示方法，为目标条件强化学习和奖励塑造等下游任务提供了有效支持。", "keywords": "最小行动距离, 状态表示, 马尔可夫决策过程, 自监督学习, 强化学习", "comments": "这篇论文提出了一种新颖且实用的状态表示学习方法，其创新之处在于能够仅从状态轨迹中学习，无需奖励和动作信号，这大大降低了数据收集的门槛。MAD作为一种几何意义上的进展度量，对目标条件强化学习和奖励塑造等任务具有重要价值。该方法的自监督学习范式和在多样化环境中的出色表现，显示了其在强化学习领域广阔的应用前景。"}}
{"id": "2506.09469", "title": "Optimizing Cooperative Multi-Object Tracking using Graph Signal Processing", "authors": ["Maria Damanaki", "Nikos Piperigkos", "Alexandros Gkillas", "Aris S. Lalos"], "summary": "Multi-Object Tracking (MOT) plays a crucial role in autonomous driving\nsystems, as it lays the foundations for advanced perception and precise path\nplanning modules. Nonetheless, single agent based MOT lacks in sensing\nsurroundings due to occlusions, sensors failures, etc. Hence, the integration\nof multiagent information is essential for comprehensive understanding of the\nenvironment. This paper proposes a novel Cooperative MOT framework for tracking\nobjects in 3D LiDAR scene by formulating and solving a graph topology-aware\noptimization problem so as to fuse information coming from multiple vehicles.\nBy exploiting a fully connected graph topology defined by the detected bounding\nboxes, we employ the Graph Laplacian processing optimization technique to\nsmooth the position error of bounding boxes and effectively combine them. In\nthat manner, we reveal and leverage inherent coherences of diverse multi-agent\ndetections, and associate the refined bounding boxes to tracked objects at two\nstages, optimizing localization and tracking accuracies. An extensive\nevaluation study has been conducted, using the real-world V2V4Real dataset,\nwhere the proposed method significantly outperforms the baseline frameworks,\nincluding the state-of-the-art deep-learning DMSTrack and V2V4Real, in various\ntesting sequences.", "comment": "2025 IEEE International Conference on Multimedia and Expo Workshops,\n  3DMM - 3D Multimedia Analytics, Search and Generation", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09469v1", "AI": {"title_translation": "使用图信号处理优化协同多目标跟踪", "tldr": "本文提出了一种新的协同多目标跟踪框架，通过图拓扑感知优化来融合多车辆信息，显著优于现有基线方法。", "motivation": "单智能体多目标跟踪（MOT）由于遮挡和传感器故障等原因，在感知周围环境方面存在不足。因此，整合多智能体信息对于全面理解环境至关重要。", "method": "本文提出了一种新颖的协同多目标跟踪（MOT）框架，用于在3D LiDAR场景中跟踪物体。该框架通过构建和解决一个图拓扑感知优化问题来融合来自多辆车的信息。通过利用由检测到的边界框定义的完全连接图拓扑，采用图拉普拉斯处理优化技术来平滑边界框的位置误差并有效地组合它们。该方法分两个阶段将精炼后的边界框与被跟踪对象关联起来，从而优化定位和跟踪精度。", "result": "使用真实世界的V2V4Real数据集进行了广泛的评估研究，结果表明所提出的方法在各种测试序列中显著优于基线框架，包括最先进的深度学习DMSTrack和V2V4Real。", "conclusion": "本文提出的基于图信号处理的协同多目标跟踪框架，通过有效融合多智能体信息，显著提升了3D LiDAR场景下的物体跟踪性能，超越了现有的先进方法。", "translation": "多目标跟踪（MOT）在自动驾驶系统中扮演着至关重要的角色，因为它为高级感知和精确路径规划模块奠定了基础。然而，基于单一智能体的MOT由于遮挡、传感器故障等原因，在感知周围环境方面存在不足。因此，整合多智能体信息对于全面理解环境至关重要。本文提出了一种新颖的协同MOT框架，通过构建和解决一个图拓扑感知优化问题来融合来自多辆车的信息，从而在3D LiDAR场景中跟踪物体。通过利用由检测到的边界框定义的完全连接图拓扑，我们采用图拉普拉斯处理优化技术来平滑边界框的位置误差并有效地组合它们。通过这种方式，我们揭示并利用了多样化多智能体检测固有的连贯性，并在两个阶段将精炼后的边界框与被跟踪对象关联起来，从而优化定位和跟踪精度。使用真实世界的V2V4Real数据集进行了广泛的评估研究，结果表明所提出的方法在各种测试序列中显著优于基线框架，包括最先进的深度学习DMSTrack和V2V4Real。", "summary": "本文提出了一种新颖的协同多目标跟踪（MOT）框架，旨在解决单智能体MOT在自动驾驶中因遮挡和传感器故障导致的环境感知不足问题。该方法通过构建和解决一个图拓扑感知优化问题，利用图拉普拉斯处理技术融合多车辆信息，以平滑边界框位置误差并关联跟踪对象，从而优化定位和跟踪精度。在V2V4Real数据集上的实验表明，所提出的方法在性能上显著优于包括DMSTrack和V2V4Real在内的现有基线方法。", "keywords": "协同多目标跟踪, 图信号处理, 自动驾驶, 3D LiDAR, 信息融合", "comments": "该论文的创新点在于将图信号处理应用于协同多目标跟踪，通过优化图拓扑来有效融合多源信息，从而提升了自动驾驶系统中对复杂环境的感知能力和跟踪精度。"}}
{"id": "2506.09473", "title": "Provoking Multi-modal Few-Shot LVLM via Exploration-Exploitation In-Context Learning", "authors": ["Cheng Chen", "Yunpeng Zhai", "Yifan Zhao", "Jinyang Gao", "Bolin Ding", "Jia Li"], "summary": "In-context learning (ICL), a predominant trend in instruction learning, aims\nat enhancing the performance of large language models by providing clear task\nguidance and examples, improving their capability in task understanding and\nexecution. This paper investigates ICL on Large Vision-Language Models (LVLMs)\nand explores the policies of multi-modal demonstration selection. Existing\nresearch efforts in ICL face significant challenges: First, they rely on\npre-defined demonstrations or heuristic selecting strategies based on human\nintuition, which are usually inadequate for covering diverse task requirements,\nleading to sub-optimal solutions; Second, individually selecting each\ndemonstration fails in modeling the interactions between them, resulting in\ninformation redundancy. Unlike these prevailing efforts, we propose a new\nexploration-exploitation reinforcement learning framework, which explores\npolicies to fuse multi-modal information and adaptively select adequate\ndemonstrations as an integrated whole. The framework allows LVLMs to optimize\nthemselves by continually refining their demonstrations through\nself-exploration, enabling the ability to autonomously identify and generate\nthe most effective selection policies for in-context learning. Experimental\nresults verify the superior performance of our approach on four Visual\nQuestion-Answering (VQA) datasets, demonstrating its effectiveness in enhancing\nthe generalization capability of few-shot LVLMs.", "comment": "10 pages, 6 figures, CVPR 2025", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09473v1", "AI": {"title_translation": "通过探索-利用语境学习激发多模态少样本LVLM", "tldr": "本文提出了一种新的探索-利用强化学习框架，用于在上下文学习中自适应选择多模态演示，以提高少样本LVLM的泛化能力。", "motivation": "现有上下文学习（ICL）方法在多模态演示选择上存在挑战：首先，它们依赖预定义或启发式选择策略，这些策略通常不足以覆盖多样化的任务需求，导致次优解决方案；其次，单独选择每个演示无法模拟它们之间的交互，导致信息冗余。", "method": "本文提出了一种新的探索-利用强化学习框架。该框架探索融合多模态信息的策略，并作为一个整体自适应地选择足够的演示。它通过自我探索不断完善演示，使LVLM能够自主识别和生成最有效的上下文学习选择策略。", "result": "实验结果验证了该方法在四个视觉问答（VQA）数据集上的卓越性能。", "conclusion": "该研究提出的探索-利用强化学习框架通过自适应选择多模态演示，有效增强了少样本大型视觉-语言模型（LVLM）的泛化能力。", "translation": "上下文学习（ICL）是指令学习中的一个主要趋势，旨在通过提供清晰的任务指导和示例来提高大型语言模型的性能，从而增强其任务理解和执行能力。本文研究了大型视觉-语言模型（LVLM）上的ICL，并探索了多模态演示选择的策略。ICL的现有研究面临重大挑战：首先，它们依赖于预定义的演示或基于人类直觉的启发式选择策略，这些策略通常不足以覆盖多样化的任务需求，导致次优解决方案；其次，单独选择每个演示未能模拟它们之间的交互，导致信息冗余。与这些流行的方法不同，我们提出了一种新的探索-利用强化学习框架，该框架探索融合多模态信息的策略，并作为一个整体自适应地选择足够的演示。该框架允许LVLM通过自我探索不断完善其演示来自我优化，从而能够自主识别和生成最有效的上下文学习选择策略。实验结果验证了我们的方法在四个视觉问答（VQA）数据集上的卓越性能，证明了其在增强少样本LVLM泛化能力方面的有效性。", "summary": "本文针对大型视觉-语言模型（LVLMs）中的上下文学习（ICL）问题，提出了一种新颖的探索-利用强化学习框架。该框架旨在克服现有ICL方法在多模态演示选择上的局限性，即依赖预定义策略和忽略演示间交互导致的次优解和信息冗余。通过自适应地选择和融合多模态演示，并允许LVLMs通过自我探索优化自身，该方法能够自主生成有效的选择策略。实验证明，该方法在多个视觉问答数据集上表现优异，显著提升了少样本LVLMs的泛化能力。", "keywords": "上下文学习, 强化学习, 多模态, 少样本, 视觉-语言模型", "comments": "该论文的创新点在于将强化学习引入到多模态上下文学习的演示选择中，通过探索-利用机制克服了传统启发式或预定义策略的局限性。这种自适应、整体性的演示选择方法，尤其是在处理多模态信息时，有望显著提升少样本LVLMs的性能和泛化能力。其自我优化机制也为未来的ICL研究提供了新的思路。"}}
{"id": "2506.09286", "title": "Causal Graph Recovery in Neuroimaging through Answer Set Programming", "authors": ["Mohammadsajad Abavisani", "Kseniya Solovyeva", "David Danks", "Vince Calhoun", "Sergey Plis"], "summary": "Learning graphical causal structures from time series data presents\nsignificant challenges, especially when the measurement frequency does not\nmatch the causal timescale of the system. This often leads to a set of equally\npossible underlying causal graphs due to information loss from sub-sampling\n(i.e., not observing all possible states of the system throughout time). Our\nresearch addresses this challenge by incorporating the effects of sub-sampling\nin the derivation of causal graphs, resulting in more accurate and intuitive\noutcomes. We use a constraint optimization approach, specifically answer set\nprogramming (ASP), to find the optimal set of answers. ASP not only identifies\nthe most probable underlying graph, but also provides an equivalence class of\npossible graphs for expert selection. In addition, using ASP allows us to\nleverage graph theory to further prune the set of possible solutions, yielding\na smaller, more accurate answer set significantly faster than traditional\napproaches. We validate our approach on both simulated data and empirical\nstructural brain connectivity, and demonstrate its superiority over established\nmethods in these experiments. We further show how our method can be used as a\nmeta-approach on top of established methods to obtain, on average, 12%\nimprovement in F1 score. In addition, we achieved state of the art results in\nterms of precision and recall of reconstructing causal graph from sub-sampled\ntime series data. Finally, our method shows robustness to varying degrees of\nsub-sampling on realistic simulations, whereas other methods perform worse for\nhigher rates of sub-sampling.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09286v1", "AI": {"title_translation": "通过答案集编程在神经影像中恢复因果图", "tldr": "该研究通过答案集编程（ASP）解决了神经影像中因果图恢复的挑战，尤其是在测量频率与因果时间尺度不匹配导致信息丢失的情况下，提高了因果图恢复的准确性和鲁棒性。", "motivation": "从时间序列数据中学习图形因果结构具有挑战性，尤其当测量频率与系统因果时间尺度不匹配时，会导致信息丢失和多个同样可能的底层因果图。传统方法在处理亚采样数据时表现不佳。", "method": "研究采用约束优化方法，特别是答案集编程（ASP），来找到最优答案集。ASP不仅识别最可能的底层图，还提供了一个可能的图的等价类供专家选择。此外，ASP利用图论进一步修剪解决方案集，从而比传统方法更快地得到更小、更准确的答案集。该方法还可以作为元方法叠加在现有方法之上。", "result": "在模拟数据和经验结构脑连接数据上验证了该方法，并证明其优于现有方法。作为元方法使用时，F1分数平均提高了12%。在从亚采样时间序列数据重建因果图的精确度和召回率方面取得了最先进的结果。该方法对不同程度的亚采样表现出鲁棒性，而其他方法在较高亚采样率下表现更差。", "conclusion": "通过答案集编程（ASP）和整合亚采样效应，本研究提出了一种在神经影像中恢复因果图的有效方法，该方法在准确性、鲁棒性和处理亚采样数据方面优于传统方法，并取得了最先进的成果。", "translation": "从时间序列数据中学习图形因果结构带来了重大挑战，特别是当测量频率与系统的因果时间尺度不匹配时。由于亚采样导致的信息丢失（即，在整个时间段内未观察到系统的所有可能状态），这通常会导致一组同样可能的底层因果图。我们的研究通过在因果图的推导中纳入亚采样的影响来解决这一挑战，从而获得更准确和直观的结果。我们使用约束优化方法，特别是答案集编程（ASP），来找到最优的答案集。ASP不仅识别最可能的底层图，还提供了一个可能的图的等价类供专家选择。此外，使用ASP使我们能够利用图论进一步修剪可能的解决方案集，从而比传统方法更快地产生更小、更准确的答案集。我们在模拟数据和经验结构脑连接数据上验证了我们的方法，并证明了其在这些实验中优于现有方法。我们进一步展示了我们的方法如何作为一种元方法叠加在现有方法之上，平均可将F1分数提高12%。此外，我们在从亚采样时间序列数据重建因果图的精确度和召回率方面取得了最先进的结果。最后，我们的方法在真实模拟中对不同程度的亚采样表现出鲁棒性，而其他方法在较高亚采样率下表现更差。", "summary": "本研究提出了一种基于答案集编程（ASP）的新方法，用于在神经影像中恢复因果图，尤其解决了测量频率与因果时间尺度不匹配导致的亚采样问题。该方法通过整合亚采样效应和利用图论进行剪枝，能够识别最可能的因果图及其等价类。实验证明，该方法在处理亚采样数据时，相比传统方法在准确性、F1分数、精确度和召回率方面均有显著提升，并表现出更强的鲁棒性。", "keywords": "因果图恢复, 神经影像, 答案集编程, 亚采样, 时间序列数据", "comments": "该论文的创新点在于将答案集编程（ASP）应用于神经影像中的因果图恢复，并特别关注了亚采样带来的信息丢失问题。通过ASP的约束优化能力和结合图论，该方法能够有效地处理不完整或不匹配的时间序列数据，这对于实际应用中的复杂生物系统建模具有重要意义。其作为元方法提升现有方法性能的潜力也值得关注。局限性可能在于ASP的计算复杂性和对专家知识的依赖（在等价类选择方面）。"}}
{"id": "2506.09508", "title": "Efficient Preference-Based Reinforcement Learning: Randomized Exploration Meets Experimental Design", "authors": ["Andreas Schlaginhaufen", "Reda Ouhamma", "Maryam Kamgarpour"], "summary": "We study reinforcement learning from human feedback in general Markov\ndecision processes, where agents learn from trajectory-level preference\ncomparisons. A central challenge in this setting is to design algorithms that\nselect informative preference queries to identify the underlying reward while\nensuring theoretical guarantees. We propose a meta-algorithm based on\nrandomized exploration, which avoids the computational challenges associated\nwith optimistic approaches and remains tractable. We establish both regret and\nlast-iterate guarantees under mild reinforcement learning oracle assumptions.\nTo improve query complexity, we introduce and analyze an improved algorithm\nthat collects batches of trajectory pairs and applies optimal experimental\ndesign to select informative comparison queries. The batch structure also\nenables parallelization of preference queries, which is relevant in practical\ndeployment as feedback can be gathered concurrently. Empirical evaluation\nconfirms that the proposed method is competitive with reward-based\nreinforcement learning while requiring a small number of preference queries.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09508v1", "AI": {"title_translation": "高效基于偏好的强化学习：随机探索与实验设计相结合", "tldr": "本文提出了一种基于随机探索的元算法，用于从人类偏好反馈中进行强化学习，并结合实验设计来提高查询效率和可并行性，在理论和实践中均表现出竞争力。", "motivation": "在从人类反馈（轨迹级偏好比较）中进行强化学习时，核心挑战在于设计算法以选择信息丰富的偏好查询来识别潜在奖励，同时确保理论保证并避免乐观方法带来的计算挑战。", "method": "本文提出了一种基于随机探索的元算法，以避免计算挑战并保持可处理性。为提高查询复杂度，引入并分析了一种改进算法，该算法收集成批的轨迹对，并应用最优实验设计来选择信息丰富的比较查询。批处理结构也支持偏好查询的并行化。", "result": "在温和的强化学习预言机假设下，建立了遗憾界和最终迭代保证。实证评估证实，所提出的方法与基于奖励的强化学习具有竞争力，同时只需要少量偏好查询。", "conclusion": "所提出的方法为基于偏好的强化学习提供了一种高效且有竞争力的方法，具有理论保证和查询效率、并行化等实际优势。", "translation": "我们研究在通用马尔可夫决策过程中，从人类反馈中进行强化学习，其中智能体从轨迹级别的偏好比较中学习。在这种环境下，一个核心挑战是设计算法，选择信息丰富的偏好查询来识别潜在奖励，同时确保理论保证。我们提出了一种基于随机探索的元算法，它避免了与乐观方法相关的计算挑战，并保持了可处理性。我们在温和的强化学习预言机假设下，建立了遗憾界和最终迭代保证。为了提高查询复杂度，我们引入并分析了一种改进算法，该算法收集成批的轨迹对，并应用最优实验设计来选择信息丰富的比较查询。批处理结构还支持偏好查询的并行化，这在实际部署中非常重要，因为反馈可以同时收集。实证评估证实，所提出的方法与基于奖励的强化学习具有竞争力，同时只需要少量偏好查询。", "summary": "本文针对从人类偏好反馈中学习的强化学习问题，提出了一种高效的元算法。该算法基于随机探索，避免了传统方法的计算难题，并提供了理论保证。为进一步提升查询效率，研究引入了结合最优实验设计的批处理方法，实现了偏好查询的并行化。实验结果表明，该方法在少量查询下能与基于奖励的强化学习相媲美，具有显著的实用价值。", "keywords": "强化学习, 人类反馈, 偏好学习, 随机探索, 实验设计", "comments": "该论文解决了强化学习中一个关键的实际挑战，使其更适用于难以明确定义奖励函数的真实世界场景。随机探索与实验设计的结合在提高效率和可处理性方面具有创新性。对并行化的关注是其重要的实际优势。"}}
{"id": "2506.09476", "title": "Urban1960SatSeg: Unsupervised Semantic Segmentation of Mid-20$^{th}$ century Urban Landscapes with Satellite Imageries", "authors": ["Tianxiang Hao", "Lixian Zhang", "Yingjia Zhang", "Mengxuan Chen", "Jinxiao Zhang", "Haohuan Fu"], "summary": "Historical satellite imagery, such as mid-20$^{th}$ century Keyhole data,\noffers rare insights into understanding early urban development and long-term\ntransformation. However, severe quality degradation (e.g., distortion,\nmisalignment, and spectral scarcity) and annotation absence have long hindered\nsemantic segmentation on such historical RS imagery. To bridge this gap and\nenhance understanding of urban development, we introduce\n$\\textbf{Urban1960SatBench}$, an annotated segmentation dataset based on\nhistorical satellite imagery with the earliest observation time among all\nexisting segmentation datasets, along with a benchmark framework for\nunsupervised segmentation tasks, $\\textbf{Urban1960SatUSM}$. First,\n$\\textbf{Urban1960SatBench}$ serves as a novel, expertly annotated semantic\nsegmentation dataset built on mid-20$^{th}$ century Keyhole imagery, covering\n1,240 km$^2$ and key urban classes (buildings, roads, farmland, water). As the\nearliest segmentation dataset of its kind, it provides a pioneering benchmark\nfor historical urban understanding. Second,\n$\\textbf{Urban1960SatUSM}$(Unsupervised Segmentation Model) is a novel\nunsupervised semantic segmentation framework for historical RS imagery. It\nemploys a confidence-aware alignment mechanism and focal-confidence loss based\non a self-supervised learning architecture, which generates robust\npseudo-labels and adaptively prioritizes prediction difficulty and label\nreliability to improve unsupervised segmentation on noisy historical data\nwithout manual supervision. Experiments show Urban1960SatUSM significantly\noutperforms existing unsupervised segmentation methods on Urban1960SatSeg for\nsegmenting historical urban scenes, promising in paving the way for\nquantitative studies of long-term urban change using modern computer vision.\nOur benchmark and supplementary material are available at\nhttps://github.com/Tianxiang-Hao/Urban1960SatSeg.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09476v1", "AI": {"title_translation": "Urban1960SatSeg：基于卫星图像的20世纪中期城市景观无监督语义分割", "tldr": "该研究引入了Urban1960SatBench，一个历史卫星图像数据集，以及Urban1960SatUSM，一个无监督语义分割框架，旨在分析20世纪中期的城市景观，以克服数据质量问题和标注缺失的挑战。", "motivation": "利用历史卫星图像理解早期城市发展和长期演变面临严峻挑战，主要原因在于图像质量严重退化（如失真、错位、光谱稀缺）以及缺乏人工标注，这严重阻碍了语义分割任务的进行。", "method": "本研究提出了两项核心贡献：1. Urban1960SatBench：一个新颖的、经过专家标注的语义分割数据集，基于20世纪中期的Keyhole图像，覆盖1,240平方公里，包含建筑物、道路、农田、水域等关键城市类别。它是同类中最早的分割数据集，为历史城市研究提供了开创性基准。2. Urban1960SatUSM（无监督分割模型）：一个用于历史遥感图像的新型无监督语义分割框架。它采用基于自监督学习架构的置信度感知对齐机制和焦点置信度损失，能够生成鲁棒的伪标签，并自适应地优先处理预测难度和标签可靠性，从而在没有人工监督的情况下改进对噪声历史数据的无监督分割。", "result": "实验结果表明，Urban1960SatUSM在Urban1960SatSeg数据集上对历史城市场景的分割性能显著优于现有的无监督分割方法。", "conclusion": "本研究提出的基准数据集和框架为利用现代计算机视觉技术对长期城市变化进行定量研究铺平了道路，有效解决了历史卫星图像带来的挑战。", "translation": "历史卫星图像，例如20世纪中期的Keyhole数据，为理解早期城市发展和长期演变提供了难得的见解。然而，严重的质量退化（例如，失真、错位和光谱稀缺）以及缺乏标注长期以来一直阻碍着此类历史遥感图像上的语义分割。为了弥补这一差距并增强对城市发展的理解，我们引入了Urban1960SatBench，这是一个基于历史卫星图像的标注分割数据集，其观测时间在所有现有分割数据集中最早，同时还引入了一个用于无监督分割任务的基准框架Urban1960SatUSM。首先，Urban1960SatBench作为一个新颖的、经过专家标注的语义分割数据集，建立在20世纪中期的Keyhole图像上，覆盖1,240平方公里，包含关键的城市类别（建筑物、道路、农田、水域）。作为同类中最早的分割数据集，它为历史城市理解提供了一个开创性的基准。其次，Urban1960SatUSM（无监督分割模型）是一个用于历史遥感图像的新型无监督语义分割框架。它采用基于自监督学习架构的置信度感知对齐机制和焦点置信度损失，生成鲁棒的伪标签，并自适应地优先处理预测难度和标签可靠性，以在没有人工监督的情况下改进对噪声历史数据的无监督分割。实验表明，Urban1960SatUSM在Urban1960SatSeg上对历史城市场景的分割性能显著优于现有无监督分割方法，有望为使用现代计算机视觉进行长期城市变化的定量研究铺平道路。我们的基准和补充材料可在https://github.com/Tianxiang-Hao/Urban1960SatSeg获取。", "summary": "该论文旨在解决20世纪中期历史卫星图像语义分割面临的质量差和缺乏标注的挑战。为此，它引入了Urban1960SatBench，一个新颖的、经过专家标注的早期城市景观数据集，以及Urban1960SatUSM，一个无监督语义分割框架。Urban1960SatUSM利用自监督学习架构中的置信度感知对齐机制和焦点置信度损失，生成鲁棒的伪标签，并在无需人工监督的情况下改进对噪声历史数据的分割。实验结果表明，Urban1960SatUSM显著优于现有方法，从而能够对长期城市变化进行定量研究。", "keywords": "历史卫星图像, 无监督语义分割, 城市景观, Keyhole数据, 自监督学习", "comments": "该论文通过提供独特的历史数据集（Urban1960SatBench）和有效的无监督分割框架（Urban1960SatUSM）做出了重大贡献。该数据集的早期观测时间和专家标注对于历史城市研究特别有价值。Urban1960SatUSM的无监督方法是创新的，特别是它通过置信度感知机制处理噪声历史数据，这对于此类数据的手动标注挑战至关重要。这项工作有望为长期城市发展模式的定量研究提供新的见解。"}}
{"id": "2506.09316", "title": "On-the-Fly Adaptive Distillation of Transformer to Dual-State Linear Attention", "authors": ["Yeonju Ro", "Zhenyu Zhang", "Souvik Kundu", "Zhangyang Wang", "Aditya Akella"], "summary": "Large language models (LLMs) excel at capturing global token dependencies via\nself-attention but face prohibitive compute and memory costs on lengthy inputs.\nWhile sub-quadratic methods (e.g., linear attention) can reduce these costs,\nthey often degrade accuracy due to overemphasizing recent tokens. In this work,\nwe first propose \\textit{dual-state linear attention} (\\textbf{\\dsla}), a novel\ndesign that maintains two specialized hidden states-one for preserving\nhistorical context and one for tracking recency-thereby mitigating the\nshort-range bias typical of linear-attention architectures. To further balance\nefficiency and accuracy under dynamic workload conditions, we introduce\n\\textbf{\\serve}, an online \\textit{adaptive distillation} framework that\nprogressively replaces Transformer layers with DSLA layers at inference time,\nguided by a sensitivity-based layer ordering. \\serve\\ uses a chained\nfine-tuning strategy to ensure that each newly converted DSLA layer remains\nconsistent with previously replaced layers, preserving the overall quality.\nExtensive evaluations on commonsense reasoning, long-context QA, and text\nsummarization demonstrate that \\serve\\ yields \\textbf{2.3x} faster inference\nthan Llama2-7B and \\textbf{3.0x} faster than the hybrid Zamba-7B, while\nretaining comparable performance across downstream tasks. Our ablation studies\nshow that DSLA's dual states capture both global and local dependencies,\naddressing the historical-token underrepresentation seen in prior linear\nattentions. Codes are available at https://github.com/utnslab/DSLA-Serve.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09316v1", "AI": {"title_translation": "Transformer到双态线性注意力的即时自适应蒸馏", "tldr": "本文提出双态线性注意力（DSLA）以解决线性注意力对近期token的偏重问题，并引入在线自适应蒸馏框架SERVE，在推理时将Transformer层逐步替换为DSLA层，显著提升推理速度同时保持性能。", "motivation": "大型语言模型（LLMs）在处理长输入时面临高昂的计算和内存成本。虽然次二次方方法（如线性注意力）能降低成本，但由于过度强调近期token，常导致准确性下降。", "method": "1. 提出双态线性注意力（DSLA）：设计两个专门的隐藏状态，一个用于保留历史上下文，一个用于跟踪近期信息，以缓解线性注意力的短程偏差。\n2. 引入SERVE在线自适应蒸馏框架：在推理时，根据基于敏感度的层排序，逐步将Transformer层替换为DSLA层。\n3. 采用链式微调策略：确保每个新转换的DSLA层与之前替换的层保持一致，以保持整体质量。", "result": "SERVE在下游任务中保持可比性能的同时，推理速度比Llama2-7B快2.3倍，比混合型Zamba-7B快3.0倍。消融研究表明DSLA的双态机制能捕获全局和局部依赖，解决了先前线性注意力中历史token表示不足的问题。", "conclusion": "本文提出的双态线性注意力（DSLA）和在线自适应蒸馏框架SERVE有效平衡了大型语言模型的效率和准确性，显著提升了推理速度，同时保持了任务性能，解决了现有线性注意力架构的短程偏差问题。", "translation": "大型语言模型（LLMs）通过自注意力机制擅长捕获全局token依赖，但在处理冗长输入时面临高昂的计算和内存成本。虽然次二次方方法（如线性注意力）可以降低这些成本，但由于过度强调近期token，常常导致准确性下降。在这项工作中，我们首先提出了双态线性注意力（DSLA），这是一种新颖的设计，它维持两个专门的隐藏状态——一个用于保留历史上下文，一个用于跟踪近期信息——从而减轻了线性注意力架构典型的短程偏差。为了在动态工作负载条件下进一步平衡效率和准确性，我们引入了SERVE，一个在线自适应蒸馏框架，它在推理时根据基于敏感度的层排序，逐步用DSLA层替换Transformer层。SERVE使用链式微调策略，以确保每个新转换的DSLA层与先前替换的层保持一致，从而保持整体质量。在常识推理、长上下文问答和文本摘要任务上的广泛评估表明，SERVE比Llama2-7B的推理速度快2.3倍，比混合型Zamba-7B快3.0倍，同时在下游任务中保持了可比的性能。我们的消融研究表明，DSLA的双态机制捕获了全局和局部依赖，解决了先前线性注意力中历史token表示不足的问题。代码已在https://github.com/utnslab/DSLA-Serve 提供。", "summary": "本文提出了一种名为双态线性注意力（DSLA）的新型架构，通过维护历史上下文和近期信息的双隐藏状态来缓解线性注意力对近期token的偏重问题。在此基础上，引入了SERVE在线自适应蒸馏框架，该框架在推理时根据敏感度排序逐步将Transformer层替换为DSLA层，并采用链式微调策略以保持性能。实验结果表明，SERVE在保持与Llama2-7B和Zamba-7B相当性能的同时，显著提高了推理速度（分别快2.3倍和3.0倍），有效平衡了大型语言模型的效率和准确性。", "keywords": "双态线性注意力, 自适应蒸馏, Transformer, 线性注意力, 大语言模型", "comments": "该论文的创新点在于提出了双态线性注意力（DSLA）来解决传统线性注意力在长序列处理中对历史信息捕获不足的问题，并通过SERVE框架实现了Transformer模型在推理阶段的动态、自适应蒸值，从而在保持性能的同时大幅提升了效率。这种即时蒸馏的策略非常实用，特别是在需要动态调整模型复杂度的场景下具有重要意义。链式微调策略保证了模型质量，是实现无缝转换的关键。"}}
{"id": "2506.09520", "title": "How attention simplifies mental representations for planning", "authors": ["Jason da Silva Castanheira", "Nicholas Shea", "Stephen M. Fleming"], "summary": "Human planning is efficient -- it frugally deploys limited cognitive\nresources to accomplish difficult tasks -- and flexible -- adapting to novel\nproblems and environments. Computational approaches suggest that people\nconstruct simplified mental representations of their environment, balancing the\ncomplexity of a task representation with its utility. These models imply a\nnested optimisation in which planning shapes perception, and perception shapes\nplanning -- but the perceptual and attentional mechanisms governing how this\ninteraction unfolds remain unknown. Here, we harness virtual maze navigation to\ncharacterise how spatial attention controls which aspects of a task\nrepresentation enter subjective awareness and are available for planning. We\nfind that spatial proximity governs which aspects of a maze are available for\nplanning, and that when task-relevant information follows natural (lateralised)\ncontours of attention, people can more easily construct simplified and useful\nmaze representations. This influence of attention varies considerably across\nindividuals, explaining differences in people's task representations and\nbehaviour. Inspired by the 'spotlight of attention' analogy, we incorporate the\neffects of visuospatial attention into existing computational accounts of\nvalue-guided construal. Together, our work bridges computational perspectives\non perception and decision-making to better understand how individuals\nrepresent their environments in aid of planning.", "comment": null, "cate": "q-bio.NC", "url": "http://arxiv.org/abs/2506.09520v1", "AI": {"title_translation": "注意力如何简化规划的心理表征", "tldr": "注意力通过简化心理表征来促进规划，其中空间接近性和注意力轮廓是关键因素，且个体差异显著。", "motivation": "计算方法表明，规划和感知之间存在一种嵌套优化关系，但控制这种互动的具体感知和注意机制尚不明确。本研究旨在探究空间注意力如何控制任务表征的哪些方面进入主观意识并可用于规划。", "method": "研究利用虚拟迷宫导航来表征空间注意力如何控制任务表征，并探究其对规划的影响。此外，研究还将视觉空间注意力的影响融入现有的价值引导建构的计算模型中。", "result": "研究发现，空间接近性决定了迷宫的哪些方面可用于规划。当任务相关信息遵循自然的（偏侧的）注意力轮廓时，人们能更容易地构建简化且有用的迷宫表征。这种注意力的影响因个体而异，解释了人们任务表征和行为的差异。", "conclusion": "本研究连接了感知和决策的计算视角，以更好地理解个体如何表征其环境以辅助规划。注意力通过简化心理表征，使人类规划更加高效。", "translation": "人类的规划是高效的——它节俭地部署有限的认知资源来完成困难任务——并且是灵活的——适应新颖的问题和环境。计算方法表明，人们构建了环境的简化心理表征，平衡了任务表征的复杂性及其效用。这些模型暗示了一种嵌套优化，其中规划塑造感知，感知塑造规划——但控制这种互动的感知和注意机制仍然未知。在这里，我们利用虚拟迷宫导航来描述空间注意力如何控制任务表征的哪些方面进入主观意识并可用于规划。我们发现空间接近性决定了迷宫的哪些方面可用于规划，并且当任务相关信息遵循自然的（偏侧的）注意力轮廓时，人们可以更容易地构建简化且有用的迷宫表征。这种注意力的影响因个体而异，解释了人们任务表征和行为的差异。受“注意力聚光灯”类比的启发，我们将视觉空间注意力的影响纳入现有的价值引导建构的计算解释中。总之，我们的工作连接了感知和决策的计算视角，以更好地理解个体如何表征其环境以辅助规划。", "summary": "本研究探讨了空间注意力如何简化心理表征以实现高效的人类规划。通过虚拟迷宫导航实验，研究发现空间接近性和自然的注意力轮廓会影响哪些任务信息可用于规划，从而形成简化而有用的表征。研究还揭示了这种注意力影响存在显著的个体差异，并将视觉空间注意力纳入计算模型，从而连接了感知和决策领域，以更好地理解个体在规划中如何表征其环境。", "keywords": "注意力, 心理表征, 规划, 空间认知, 虚拟迷宫导航", "comments": "本文的创新之处在于直接探究了规划与感知之间互动的具体感知和注意机制，填补了现有计算模型中的空白。它通过新颖的虚拟迷宫导航范式提供了实证证据，并将研究结果整合到计算解释中，从而推进了我们对规划中认知资源分配的理解。此外，发现个体差异也具有重要意义。"}}
{"id": "2506.09479", "title": "TinySplat: Feedforward Approach for Generating Compact 3D Scene Representation", "authors": ["Zetian Song", "Jiaye Fu", "Jiaqi Zhang", "Xiaohan Lu", "Chuanmin Jia", "Siwei Ma", "Wen Gao"], "summary": "The recent development of feedforward 3D Gaussian Splatting (3DGS) presents a\nnew paradigm to reconstruct 3D scenes. Using neural networks trained on\nlarge-scale multi-view datasets, it can directly infer 3DGS representations\nfrom sparse input views. Although the feedforward approach achieves high\nreconstruction speed, it still suffers from the substantial storage cost of 3D\nGaussians. Existing 3DGS compression methods relying on scene-wise optimization\nare not applicable due to architectural incompatibilities. To overcome this\nlimitation, we propose TinySplat, a complete feedforward approach for\ngenerating compact 3D scene representations. Built upon standard feedforward\n3DGS methods, TinySplat integrates a training-free compression framework that\nsystematically eliminates key sources of redundancy. Specifically, we introduce\nView-Projection Transformation (VPT) to reduce geometric redundancy by\nprojecting geometric parameters into a more compact space. We further present\nVisibility-Aware Basis Reduction (VABR), which mitigates perceptual redundancy\nby aligning feature energy along dominant viewing directions via basis\ntransformation. Lastly, spatial redundancy is addressed through an\noff-the-shelf video codec. Comprehensive experimental results on multiple\nbenchmark datasets demonstrate that TinySplat achieves over 100x compression\nfor 3D Gaussian data generated by feedforward methods. Compared to the\nstate-of-the-art compression approach, we achieve comparable quality with only\n6% of the storage size. Meanwhile, our compression framework requires only 25%\nof the encoding time and 1% of the decoding time.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09479v1", "AI": {"title_translation": "TinySplat: 生成紧凑3D场景表示的前馈方法", "tldr": "TinySplat 提出了一种完整的前馈方法，用于压缩前馈3D高斯飞溅（3DGS）生成的3D场景表示，通过系统地消除几何、感知和空间冗余，实现了超过100倍的压缩率和卓越的效率。", "motivation": "尽管前馈3D高斯飞溅（3DGS）方法在3D场景重建方面实现了高速重建，但它仍然面临3D高斯数据存储成本高昂的问题。现有的3DGS压缩方法依赖于场景级优化，由于架构不兼容，不适用于前馈方法。", "method": "TinySplat 是一种完整的前馈方法，用于生成紧凑的3D场景表示。它建立在标准前馈3DGS方法之上，并集成了一个免训练的压缩框架，系统地消除了冗余。具体来说，它引入了视图投影变换（VPT）来减少几何冗余，通过基础变换引入了可见性感知基础约简（VABR）来减轻感知冗余，并通过现成的视频编解码器解决了空间冗余。", "result": "TinySplat 在多个基准数据集上实现了对前馈方法生成的3D高斯数据超过100倍的压缩率。与最先进的压缩方法相比，TinySplat 以仅6%的存储大小实现了可比的质量。此外，其压缩框架仅需25%的编码时间和1%的解码时间。", "conclusion": "TinySplat 成功地克服了前馈3DGS方法中3D高斯数据存储成本高昂的限制，通过其创新的免训练压缩框架，实现了显著的压缩率和效率提升，同时保持了高质量的场景表示。", "translation": "前馈3D高斯飞溅（3DGS）的最新发展为重建3D场景提供了一种新范式。利用在大型多视图数据集上训练的神经网络，它可以直接从稀疏输入视图推断3DGS表示。尽管前馈方法实现了高速重建，但它仍然面临3D高斯数据存储成本高昂的问题。现有依赖于场景级优化的3DGS压缩方法由于架构不兼容而无法适用。为了克服这一限制，我们提出了TinySplat，一种用于生成紧凑3D场景表示的完整前馈方法。TinySplat 基于标准前馈3DGS方法，集成了一个免训练的压缩框架，系统地消除了冗余的关键来源。具体来说，我们引入了视图投影变换（VPT）来通过将几何参数投影到更紧凑的空间来减少几何冗余。我们进一步提出了可见性感知基础约简（VABR），通过基础变换将特征能量沿着主导视角方向对齐来减轻感知冗余。最后，空间冗余通过现成的视频编解码器得到解决。在多个基准数据集上的综合实验结果表明，TinySplat 对前馈方法生成的3D高斯数据实现了超过100倍的压缩。与最先进的压缩方法相比，我们在仅6%的存储大小下实现了可比的质量。同时，我们的压缩框架仅需25%的编码时间和1%的解码时间。", "summary": "TinySplat 提出了一种针对前馈3D高斯飞溅（3DGS）的完整前馈压缩方法，旨在解决其高昂的3D高斯数据存储成本问题。该方法集成了一个免训练的压缩框架，通过视图投影变换（VPT）减少几何冗余，通过可见性感知基础约简（VABR）减轻感知冗余，并通过视频编解码器处理空间冗余。实验结果表明，TinySplat 实现了对前馈3DGS数据超过100倍的压缩，并在存储大小仅为6%的情况下达到与现有最先进方法相当的质量，同时显著缩短了编码和解码时间。", "keywords": "3D高斯飞溅, 场景表示, 数据压缩, 前馈方法, 冗余消除", "comments": "TinySplat 的创新在于其针对前馈3DGS架构设计的免训练压缩框架，这解决了现有压缩方法不兼容前馈模型的问题。通过系统地消除几何、感知和空间冗余，它在实现极高压缩率的同时，保持了高质量，并且在效率上表现出色，这对于未来3D场景表示的实际应用具有重要意义。"}}
{"id": "2506.09482", "title": "Marrying Autoregressive Transformer and Diffusion with Multi-Reference Autoregression", "authors": ["Dingcheng Zhen", "Qian Qiao", "Tan Yu", "Kangxi Wu", "Ziwei Zhang", "Siyuan Liu", "Shunshun Yin", "Ming Tao"], "summary": "We introduce TransDiff, the first image generation model that marries\nAutoregressive (AR) Transformer with diffusion models. In this joint modeling\nframework, TransDiff encodes labels and images into high-level semantic\nfeatures and employs a diffusion model to estimate the distribution of image\nsamples. On the ImageNet 256x256 benchmark, TransDiff significantly outperforms\nother image generation models based on standalone AR Transformer or diffusion\nmodels. Specifically, TransDiff achieves a Fr\\'echet Inception Distance (FID)\nof 1.61 and an Inception Score (IS) of 293.4, and further provides x2 faster\ninference latency compared to state-of-the-art methods based on AR Transformer\nand x112 faster inference compared to diffusion-only models. Furthermore,\nbuilding on the TransDiff model, we introduce a novel image generation paradigm\ncalled Multi-Reference Autoregression (MRAR), which performs autoregressive\ngeneration by predicting the next image. MRAR enables the model to reference\nmultiple previously generated images, thereby facilitating the learning of more\ndiverse representations and improving the quality of generated images in\nsubsequent iterations. By applying MRAR, the performance of TransDiff is\nimproved, with the FID reduced from 1.61 to 1.42. We expect TransDiff to open\nup a new frontier in the field of image generation.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09482v1", "AI": {"title_translation": "将自回归Transformer与扩散模型结合，并引入多参考自回归", "tldr": "TransDiff是首个结合自回归Transformer和扩散模型的图像生成模型，在ImageNet 256x256基准上表现卓越，实现了更快的推理速度。此外，引入多参考自回归（MRAR）进一步提升了性能。", "motivation": "旨在结合自回归（AR）Transformer和扩散模型的优势，以克服单一模型在图像生成方面的局限性，并提升生成质量和效率。", "method": "提出了TransDiff模型，它将自回归Transformer与扩散模型结合，用于图像生成。TransDiff将标签和图像编码为高级语义特征，并利用扩散模型估计图像样本的分布。在此基础上，引入了多参考自回归（MRAR）范式，通过预测下一张图像并引用多张先前生成的图像，以学习更多样化的表示并提高图像质量。", "result": "TransDiff在ImageNet 256x256基准上显著优于独立的自回归Transformer或扩散模型。具体而言，TransDiff实现了1.61的FID和293.4的IS。推理速度比最先进的自回归Transformer方法快2倍，比仅使用扩散模型的方法快112倍。应用MRAR后，TransDiff的性能进一步提升，FID从1.61降低到1.42。", "conclusion": "TransDiff有望在图像生成领域开辟新天地，证明了结合自回归Transformer和扩散模型（特别是通过MRAR）的有效性。", "translation": "我们引入了TransDiff，这是第一个将自回归（AR）Transformer与扩散模型结合的图像生成模型。在这个联合建模框架中，TransDiff将标签和图像编码为高级语义特征，并采用扩散模型来估计图像样本的分布。在ImageNet 256x256基准测试中，TransDiff显著优于基于独立AR Transformer或扩散模型的其他图像生成模型。具体而言，TransDiff实现了1.61的Fréchet Inception Distance (FID) 和293.4的Inception Score (IS)，并且与基于AR Transformer的最先进方法相比，推理延迟快2倍，与仅使用扩散模型的方法相比，推理延迟快112倍。此外，在TransDiff模型的基础上，我们引入了一种新颖的图像生成范式，称为多参考自回归（MRAR），它通过预测下一张图像来执行自回归生成。MRAR使模型能够引用多个先前生成的图像，从而促进学习更多样化的表示并提高后续迭代中生成图像的质量。通过应用MRAR，TransDiff的性能得到改善，FID从1.61降低到1.42。我们期望TransDiff能在图像生成领域开辟新天地。", "summary": "本文介绍了TransDiff，一种首次结合自回归（AR）Transformer和扩散模型的图像生成模型。TransDiff通过将标签和图像编码为高级语义特征，并利用扩散模型估计图像分布。在ImageNet 256x256基准测试中，TransDiff显著超越了独立的AR Transformer或扩散模型，实现了1.61的FID和293.4的IS，并且推理速度分别快2倍和112倍。此外，论文引入了多参考自回归（MRAR）范式，通过引用多个先前生成的图像来预测下一张图像，进一步将TransDiff的FID从1.61降低到1.42。该模型有望在图像生成领域开辟新篇章。", "keywords": "自回归Transformer, 扩散模型, 图像生成, 多参考自回归, TransDiff", "comments": "该论文的创新之处在于首次将自回归Transformer与扩散模型相结合，创建了TransDiff模型，并在图像生成任务中取得了显著的性能提升和推理速度优势。引入多参考自回归（MRAR）范式是另一个亮点，它通过利用历史信息进一步提高了生成图像的质量和多样性。这项工作为未来的图像生成研究提供了一个新的、高效的混合模型范式。"}}
{"id": "2506.09147", "title": "LLM-as-a-qualitative-judge: automating error analysis in natural language generation", "authors": ["Nadezhda Chirkova", "Tunde Oluwaseyi Ajayi", "Seth Aycock", "Zain Muhammad Mujahid", "Vladana Perlić", "Ekaterina Borisova", "Markarit Vartampetian"], "summary": "Prompting large language models (LLMs) to evaluate generated text, known as\nLLM-as-a-judge, has become a standard evaluation approach in natural language\ngeneration (NLG), but is primarily used as a quantitative tool, i.e. with\nnumerical scores as main outputs. In this work, we propose\nLLM-as-a-qualitative-judge, an LLM-based evaluation approach with the main\noutput being a structured report of common issue types in the NLG system\noutputs. Our approach is targeted at providing developers with meaningful\ninsights on what improvements can be done to a given NLG system and consists of\ntwo main steps, namely open-ended per-instance issue analysis and clustering of\nthe discovered issues using an intuitive cumulative algorithm. We also\nintroduce a strategy for evaluating the proposed approach, coupled with ~300\nannotations of issues in instances from 12 NLG datasets. Our results show that\nLLM-as-a-qualitative-judge correctly recognizes instance-specific issues in 2/3\ncases and is capable of producing error type reports resembling the reports\ncomposed by human annotators. Our code and data are publicly available at\nhttps://github.com/tunde-ajayi/llm-as-a-qualitative-judge.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.09147v1", "AI": {"title_translation": "LLM作为定性判断者：自动化自然语言生成中的错误分析", "tldr": "提出LLM作为定性判断者，一种LLM评估方法，用于自动生成NLG系统输出的结构化错误报告，以提供改进见解，并显示其能识别问题并生成类似人类报告。", "motivation": "LLM-as-a-judge主要作为定量工具使用，但缺少对NLG系统输出中常见问题类型的定性洞察，导致开发者难以了解如何改进系统。", "method": "提出LLM-as-a-qualitative-judge方法，主要输出是NLG系统输出中常见问题类型的结构化报告。该方法包括两个步骤：开放式实例级问题分析和使用直观累积算法对发现的问题进行聚类。还引入了一种评估所提方法策略，并使用了来自12个NLG数据集的约300个实例问题注释。", "result": "LLM-as-a-qualitative-judge在2/3的案例中正确识别了实例特定问题，并且能够生成与人类标注者编写的报告相似的错误类型报告。", "conclusion": "LLM-as-a-qualitative-judge是一种有效且可靠的自动化NLG错误分析工具，能够为开发者提供有意义的改进洞察。", "translation": "提示大型语言模型（LLM）来评估生成文本，即LLM-as-a-judge，已成为自然语言生成（NLG）中一种标准的评估方法，但它主要被用作一种定量工具，即以数值分数作为主要输出。在这项工作中，我们提出了LLM-as-a-qualitative-judge，一种基于LLM的评估方法，其主要输出是NLG系统输出中常见问题类型的结构化报告。我们的方法旨在为开发者提供关于如何改进给定NLG系统的有意义的见解，并包括两个主要步骤，即开放式实例级问题分析和使用直观累积算法对发现的问题进行聚类。我们还引入了一种评估所提方法策略，并结合了来自12个NLG数据集的约300个实例中的问题注释。我们的结果表明，LLM-as-a-qualitative-judge在2/3的案例中正确识别了实例特定问题，并且能够生成与人类标注者编写的报告相似的错误类型报告。我们的代码和数据在https://github.com/tunde-ajayi/llm-as-a-qualitative-judge 公开可用。", "summary": "本文提出了一种名为LLM-as-a-qualitative-judge的LLM评估方法，旨在自动化自然语言生成（NLG）中的错误分析。与传统LLM-as-a-judge主要提供数值分数不同，该方法生成结构化报告，详细说明NLG系统输出中的常见问题类型，从而为开发者提供可操作的改进见解。该方法包括实例级问题分析和问题聚类。实验结果表明，该方法能够有效地识别实例特定问题，并生成与人类专家相似的错误类型报告。", "keywords": "LLM-as-a-qualitative-judge, 错误分析, 自然语言生成, LLM评估, 定性分析", "comments": "这项工作创新性地将LLM的应用从定量评估扩展到定性错误分析，为NLG系统的改进提供了更具体、可操作的反馈。它解决了现有LLM-as-a-judge方法在提供详细错误类型方面不足的问题，对于NLG开发和调试具有重要意义。"}}
{"id": "2506.09347", "title": "ErrorEraser: Unlearning Data Bias for Improved Continual Learning", "authors": ["Xuemei Cao", "Hanlin Gu", "Xin Yang", "Bingjun Wei", "Haoyang Liang", "Xiangkun Wang", "Tianrui Li"], "summary": "Continual Learning (CL) primarily aims to retain knowledge to prevent\ncatastrophic forgetting and transfer knowledge to facilitate learning new\ntasks. Unlike traditional methods, we propose a novel perspective: CL not only\nneeds to prevent forgetting, but also requires intentional forgetting.This\narises from existing CL methods ignoring biases in real-world data, leading the\nmodel to learn spurious correlations that transfer and amplify across tasks.\nFrom feature extraction and prediction results, we find that data biases\nsimultaneously reduce CL's ability to retain and transfer knowledge. To address\nthis, we propose ErrorEraser, a universal plugin that removes erroneous\nmemories caused by biases in CL, enhancing performance in both new and old\ntasks. ErrorEraser consists of two modules: Error Identification and Error\nErasure. The former learns the probability density distribution of task data in\nthe feature space without prior knowledge, enabling accurate identification of\npotentially biased samples. The latter ensures only erroneous knowledge is\nerased by shifting the decision space of representative outlier samples.\nAdditionally, an incremental feature distribution learning strategy is designed\nto reduce the resource overhead during error identification in downstream\ntasks. Extensive experimental results show that ErrorEraser significantly\nmitigates the negative impact of data biases, achieving higher accuracy and\nlower forgetting rates across three types of CL methods. The code is available\nat https://github.com/diadai/ErrorEraser.", "comment": "12 pages", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09347v1", "AI": {"title_translation": "ErrorEraser：通过遗忘数据偏差改进持续学习", "tldr": "提出ErrorEraser，一个通用插件，通过识别和擦除持续学习中由数据偏差导致的错误记忆，以提高新旧任务的性能。", "motivation": "现有持续学习方法忽略了真实世界数据中的偏差，导致模型学习到虚假关联，这些关联会在任务间传递和放大，从而降低持续学习保留和迁移知识的能力。", "method": "提出ErrorEraser，包含两个模块：错误识别和错误擦除。错误识别模块学习特征空间中任务数据的概率密度分布以识别潜在偏差样本。错误擦除模块通过改变代表性异常样本的决策空间来擦除错误知识。此外，还设计了增量特征分布学习策略以降低资源开销。", "result": "实验结果表明，ErrorEraser显著减轻了数据偏差的负面影响，在三种类型的持续学习方法中均实现了更高的准确性和更低的遗忘率。", "conclusion": "ErrorEraser通过有意识地遗忘数据偏差导致的错误记忆，有效解决了持续学习中的偏差问题，提升了模型的泛化能力和知识保留能力。", "translation": "持续学习（CL）主要旨在保留知识以防止灾难性遗忘，并迁移知识以促进新任务的学习。与传统方法不同，我们提出了一个新颖的视角：CL不仅需要防止遗忘，还需要有意识地遗忘。这源于现有CL方法忽略了真实世界数据中的偏差，导致模型学习到虚假关联，这些关联会在任务间传递和放大。从特征提取和预测结果来看，我们发现数据偏差同时降低了CL保留和迁移知识的能力。为了解决这个问题，我们提出了ErrorEraser，一个通用的插件，它能消除CL中由偏差引起的错误记忆，从而提高新旧任务的性能。ErrorEraser由两个模块组成：错误识别和错误擦除。前者在没有先验知识的情况下学习特征空间中任务数据的概率密度分布，从而准确识别潜在的偏差样本。后者通过改变代表性异常样本的决策空间，确保只擦除错误知识。此外，还设计了一种增量特征分布学习策略，以减少下游任务中错误识别的资源开销。大量的实验结果表明，ErrorEraser显著减轻了数据偏差的负面影响，在三种类型的CL方法中均实现了更高的准确性和更低的遗忘率。代码可在https://github.com/diadai/ErrorEraser获取。", "summary": "本文提出了一种名为ErrorEraser的通用插件，旨在解决持续学习（CL）中由数据偏差引起的性能下降问题。传统CL方法主要关注防止遗忘，但作者指出，数据偏差会导致模型学习到虚假关联，损害知识保留和迁移能力。ErrorEraser通过其错误识别和错误擦除模块，无先验地识别并清除这些错误记忆。实验证明，ErrorEraser显著提高了多种CL方法的准确性并降低了遗忘率。", "keywords": "持续学习, 数据偏差, 错误擦除, 知识遗忘, 通用插件", "comments": "这篇论文提出了一种新颖的视角，即持续学习不仅需要防止遗忘，还需要“有意遗忘”由数据偏差引起的错误记忆，这对于提升持续学习的鲁棒性和泛化能力具有重要意义。ErrorEraser作为一个通用插件，易于集成到现有CL方法中，具有较高的实用价值。"}}
{"id": "2506.09348", "title": "Adversarial Surrogate Risk Bounds for Binary Classification", "authors": ["Natalie S. Frank"], "summary": "A central concern in classification is the vulnerability of machine learning\nmodels to adversarial attacks. Adversarial training is one of the most popular\ntechniques for training robust classifiers, which involves minimizing an\nadversarial surrogate risk. Recent work characterized when a minimizing\nsequence of an adversarial surrogate risk is also a minimizing sequence of the\nadversarial classification risk for binary classification -- a property known\nas adversarial consistency. However, these results do not address the rate at\nwhich the adversarial classification risk converges to its optimal value for\nsuch a sequence of functions that minimize the adversarial surrogate. This\npaper provides surrogate risk bounds that quantify that convergence rate.\nAdditionally, we derive distribution-dependent surrogate risk bounds in the\nstandard (non-adversarial) learning setting, that may be of independent\ninterest.", "comment": "37 pages, 2 figures", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09348v1", "AI": {"title_translation": "对二元分类的对抗性替代风险界限", "tldr": "本文通过提供替代风险界限，量化了对抗性训练中对抗性分类风险的收敛速度，并推导了标准学习设置下的分布依赖性替代风险界限。", "motivation": "机器学习模型易受对抗性攻击是分类中的一个核心问题。对抗性训练是训练鲁棒分类器最流行的技术之一，它涉及最小化对抗性替代风险。然而，现有研究未能解决对抗性分类风险收敛到其最优值的速率问题。", "method": "本文通过提供替代风险界限来量化对抗性分类风险的收敛速度。", "result": "本文量化了对抗性分类风险的收敛速度，并推导出了标准（非对抗性）学习设置下的分布依赖性替代风险界限。", "conclusion": "本文成功提供了量化对抗性分类风险收敛速度的替代风险界限，并额外推导了标准学习设置中的分布依赖性替代风险界限。", "translation": "分类中的一个核心问题是机器学习模型易受对抗性攻击。对抗性训练是训练鲁棒分类器最流行的技术之一，它涉及最小化对抗性替代风险。最近的工作描述了对抗性替代风险的最小化序列何时也是二元分类对抗性分类风险的最小化序列——这是一种被称为对抗性一致性的属性。然而，这些结果没有解决对抗性分类风险对于最小化对抗性替代的函数序列收敛到其最优值的速率问题。本文提供了量化该收敛速度的替代风险界限。此外，我们在标准（非对抗性）学习设置中推导了与分布相关的替代风险界限，这可能具有独立的意义。", "summary": "本文旨在解决对抗性训练中对抗性分类风险收敛速度未被量化的问题。通过提供替代风险界限，该研究量化了对抗性分类风险收敛到最优值的速度。此外，论文还推导了在标准非对抗性学习设置中的分布依赖性替代风险界限。", "keywords": "对抗性攻击, 对抗性训练, 替代风险, 二元分类, 收敛速度", "comments": "这篇论文的创新点在于首次量化了对抗性训练中对抗性分类风险的收敛速度，填补了现有研究的空白。其提出的替代风险界限对于理解和改进鲁棒分类器的训练效率具有重要意义。"}}
{"id": "2506.09748", "title": "Hierarchical Image Matching for UAV Absolute Visual Localization via Semantic and Structural Constraints", "authors": ["Xiangkai Zhang", "Xiang Zhou", "Mao Chen", "Yuchen Lu", "Xu Yang", "Zhiyong Liu"], "summary": "Absolute localization, aiming to determine an agent's location with respect\nto a global reference, is crucial for unmanned aerial vehicles (UAVs) in\nvarious applications, but it becomes challenging when global navigation\nsatellite system (GNSS) signals are unavailable. Vision-based absolute\nlocalization methods, which locate the current view of the UAV in a reference\nsatellite map to estimate its position, have become popular in GNSS-denied\nscenarios. However, existing methods mostly rely on traditional and low-level\nimage matching, suffering from difficulties due to significant differences\nintroduced by cross-source discrepancies and temporal variations. To overcome\nthese limitations, in this paper, we introduce a hierarchical cross-source\nimage matching method designed for UAV absolute localization, which integrates\na semantic-aware and structure-constrained coarse matching module with a\nlightweight fine-grained matching module. Specifically, in the coarse matching\nmodule, semantic features derived from a vision foundation model first\nestablish region-level correspondences under semantic and structural\nconstraints. Then, the fine-grained matching module is applied to extract fine\nfeatures and establish pixel-level correspondences. Building upon this, a UAV\nabsolute visual localization pipeline is constructed without any reliance on\nrelative localization techniques, mainly by employing an image retrieval module\nbefore the proposed hierarchical image matching modules. Experimental\nevaluations on public benchmark datasets and a newly introduced CS-UAV dataset\ndemonstrate superior accuracy and robustness of the proposed method under\nvarious challenging conditions, confirming its effectiveness.", "comment": "8 pages, 6 figures", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09748v1", "AI": {"title_translation": "基于语义和结构约束的分层图像匹配用于无人机绝对视觉定位", "tldr": "本文提出了一种新的分层图像匹配方法，通过结合语义和结构约束来解决无人机在无GNSS信号环境下的绝对视觉定位挑战，并在实验中表现出卓越的精度和鲁棒性。", "motivation": "无人机（UAV）的绝对定位对其在各种应用中至关重要，但在全球导航卫星系统（GNSS）信号不可用时变得具有挑战性。现有的基于视觉的绝对定位方法主要依赖于传统的低级图像匹配，这在跨源差异和时间变化引入的显著差异下会遇到困难。", "method": "本文提出了一种分层跨源图像匹配方法，用于无人机绝对定位。该方法整合了一个语义感知和结构约束的粗匹配模块，以及一个轻量级的细粒度匹配模块。具体来说，粗匹配模块利用来自视觉基础模型的语义特征，在语义和结构约束下建立区域级对应关系；然后，细粒度匹配模块用于提取精细特征并建立像素级对应关系。在此基础上，通过在所提出的分层图像匹配模块之前采用图像检索模块，构建了一个不依赖于相对定位技术的无人机绝对视觉定位管道。", "result": "在公共基准数据集和新引入的CS-UAV数据集上的实验评估表明，所提出的方法在各种挑战性条件下具有卓越的精度和鲁棒性。", "conclusion": "本文提出的分层图像匹配方法在无人机绝对视觉定位方面是有效且表现优异的。", "translation": "绝对定位旨在确定智能体相对于全局参考的位置，这对于无人机（UAV）在各种应用中至关重要，但当全球导航卫星系统（GNSS）信号不可用时，这变得具有挑战性。基于视觉的绝对定位方法通过在参考卫星地图中定位无人机的当前视图来估计其位置，在GNSS拒绝场景中变得流行。然而，现有方法大多依赖于传统和低级图像匹配，由于跨源差异和时间变化引入的显著差异而面临困难。为了克服这些限制，本文引入了一种分层跨源图像匹配方法，旨在用于无人机绝对定位，该方法整合了一个语义感知和结构约束的粗匹配模块和一个轻量级的细粒度匹配模块。具体来说，在粗匹配模块中，源自视觉基础模型的语义特征首先在语义和结构约束下建立区域级对应关系。然后，应用细粒度匹配模块提取精细特征并建立像素级对应关系。在此基础上，主要通过在所提出的分层图像匹配模块之前采用图像检索模块，构建了一个不依赖于相对定位技术的无人机绝对视觉定位管道。在公共基准数据集和新引入的CS-UAV数据集上的实验评估表明，所提出的方法在各种挑战性条件下具有卓越的精度和鲁棒性，证实了其有效性。", "summary": "本文针对GNSS信号不可用时无人机绝对定位的挑战，提出了一种分层跨源图像匹配方法。该方法结合了语义感知和结构约束的粗匹配模块与轻量级的细粒度匹配模块，并通过图像检索模块构建完整的定位管道。此方法利用语义和结构线索克服了传统低级匹配的局限性，在公共基准和新数据集上验证了其卓越的精度和鲁棒性。", "keywords": "分层图像匹配, 无人机绝对定位, 语义约束, 结构约束, 跨源图像", "comments": "本文的创新之处在于提出了一种分层的图像匹配方法，结合了语义和结构约束来解决无人机在GNSS受限环境下的绝对定位问题，这对于实际应用具有重要意义。通过利用视觉基础模型提取语义特征，提高了跨源图像匹配的鲁棒性。此外，引入新的数据集CS-UAV也为该领域的研究提供了宝贵的资源。"}}
{"id": "2506.09518", "title": "HAIF-GS: Hierarchical and Induced Flow-Guided Gaussian Splatting for Dynamic Scene", "authors": ["Jianing Chen", "Zehao Li", "Yujun Cai", "Hao Jiang", "Chengxuan Qian", "Juyuan Kang", "Shuqin Gao", "Honglong Zhao", "Tianlu Mao", "Yucheng Zhang"], "summary": "Reconstructing dynamic 3D scenes from monocular videos remains a fundamental\nchallenge in 3D vision. While 3D Gaussian Splatting (3DGS) achieves real-time\nrendering in static settings, extending it to dynamic scenes is challenging due\nto the difficulty of learning structured and temporally consistent motion\nrepresentations. This challenge often manifests as three limitations in\nexisting methods: redundant Gaussian updates, insufficient motion supervision,\nand weak modeling of complex non-rigid deformations. These issues collectively\nhinder coherent and efficient dynamic reconstruction. To address these\nlimitations, we propose HAIF-GS, a unified framework that enables structured\nand consistent dynamic modeling through sparse anchor-driven deformation. It\nfirst identifies motion-relevant regions via an Anchor Filter to suppresses\nredundant updates in static areas. A self-supervised Induced Flow-Guided\nDeformation module induces anchor motion using multi-frame feature aggregation,\neliminating the need for explicit flow labels. To further handle fine-grained\ndeformations, a Hierarchical Anchor Propagation mechanism increases anchor\nresolution based on motion complexity and propagates multi-level\ntransformations. Extensive experiments on synthetic and real-world benchmarks\nvalidate that HAIF-GS significantly outperforms prior dynamic 3DGS methods in\nrendering quality, temporal coherence, and reconstruction efficiency.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09518v1", "AI": {"title_translation": "HAIF-GS：面向动态场景的分层诱导流引导高斯泼溅", "tldr": "HAIF-GS提出一种基于稀疏锚点驱动变形的统一框架，通过锚点过滤、自监督诱导流引导变形和分层锚点传播机制，解决了现有动态3DGS方法中冗余更新、运动监督不足和非刚性变形建模弱的问题，显著提升了动态场景的渲染质量、时间一致性和重建效率。", "motivation": "从单目视频重建动态3D场景是一个基本挑战。尽管3D高斯泼溅（3DGS）在静态设置下实现了实时渲染，但将其扩展到动态场景面临困难，原因在于难以学习结构化和时间一致的运动表示。现有方法存在三个主要限制：冗余的高斯更新、运动监督不足以及复杂非刚性变形建模能力弱，这些问题共同阻碍了连贯高效的动态重建。", "method": "我们提出了HAIF-GS，一个通过稀疏锚点驱动变形实现结构化和一致性动态建模的统一框架。它首先通过锚点过滤器识别运动相关区域，抑制静态区域的冗余更新。一个自监督的诱导流引导变形模块利用多帧特征聚合诱导锚点运动，无需显式光流标签。为了处理细粒度变形，分层锚点传播机制根据运动复杂性增加锚点分辨率并传播多级变换。", "result": "在合成和真实世界基准上的大量实验验证，HAIF-GS在渲染质量、时间一致性和重建效率方面显著优于先前的动态3DGS方法。", "conclusion": "HAIF-GS通过其创新的稀疏锚点驱动变形、自监督运动诱导和分层变形处理机制，有效解决了动态3DGS重建中的核心挑战，显著提升了性能。", "translation": "从单目视频重建动态3D场景仍然是3D视觉中的一个基本挑战。尽管3D高斯泼溅（3DGS）在静态设置下实现了实时渲染，但由于学习结构化和时间一致的运动表示的困难，将其扩展到动态场景具有挑战性。这一挑战通常表现为现有方法中的三个限制：冗余的高斯更新、运动监督不足以及复杂非刚性变形的建模能力弱。这些问题共同阻碍了连贯和高效的动态重建。为了解决这些限制，我们提出了HAIF-GS，一个通过稀疏锚点驱动变形实现结构化和一致性动态建模的统一框架。它首先通过锚点过滤器识别运动相关区域，以抑制静态区域的冗余更新。一个自监督的诱导流引导变形模块利用多帧特征聚合诱导锚点运动，无需显式光流标签。为了进一步处理细粒度变形，分层锚点传播机制根据运动复杂性增加锚点分辨率并传播多级变换。在合成和真实世界基准上的大量实验验证，HAIF-GS在渲染质量、时间一致性和重建效率方面显著优于先前的动态3DGS方法。", "summary": "本文提出HAIF-GS，一个用于动态场景3D重建的统一框架，旨在解决现有动态3D高斯泼溅（3DGS）方法中存在的冗余更新、运动监督不足和非刚性变形建模弱等问题。HAIF-GS通过引入稀疏锚点驱动变形，包含锚点过滤器来抑制静态区域的更新、自监督诱导流引导变形模块实现无光流标签的运动诱导，以及分层锚点传播机制处理细粒度变形。实验证明，HAIF-GS在渲染质量、时间一致性和重建效率上显著优于现有方法。", "keywords": "动态场景重建, 3D高斯泼溅, 运动表示, 非刚性变形, 自监督学习", "comments": "HAIF-GS的创新点在于其通过稀疏锚点驱动的变形机制，有效解决了动态3DGS在处理复杂运动和非刚性变形时的效率与精度问题。特别是自监督诱导流引导变形模块，避免了对显式光流标签的依赖，降低了数据准备成本。分层锚点传播机制则进一步提升了对细粒度变形的建模能力。该方法在动态场景重建领域具有重要意义，为实时高保真动态场景渲染提供了新的思路。"}}
{"id": "2506.09368", "title": "Anomaly Detection and Generation with Diffusion Models: A Survey", "authors": ["Yang Liu", "Jing Liu", "Chengfang Li", "Rui Xi", "Wenchao Li", "Liang Cao", "Jin Wang", "Laurence T. Yang", "Junsong Yuan", "Wei Zhou"], "summary": "Anomaly detection (AD) plays a pivotal role across diverse domains, including\ncybersecurity, finance, healthcare, and industrial manufacturing, by\nidentifying unexpected patterns that deviate from established norms in\nreal-world data. Recent advancements in deep learning, specifically diffusion\nmodels (DMs), have sparked significant interest due to their ability to learn\ncomplex data distributions and generate high-fidelity samples, offering a\nrobust framework for unsupervised AD. In this survey, we comprehensively review\nanomaly detection and generation with diffusion models (ADGDM), presenting a\ntutorial-style analysis of the theoretical foundations and practical\nimplementations and spanning images, videos, time series, tabular, and\nmultimodal data. Crucially, unlike existing surveys that often treat anomaly\ndetection and generation as separate problems, we highlight their inherent\nsynergistic relationship. We reveal how DMs enable a reinforcing cycle where\ngeneration techniques directly address the fundamental challenge of anomaly\ndata scarcity, while detection methods provide critical feedback to improve\ngeneration fidelity and relevance, advancing both capabilities beyond their\nindividual potential. A detailed taxonomy categorizes ADGDM methods based on\nanomaly scoring mechanisms, conditioning strategies, and architectural designs,\nanalyzing their strengths and limitations. We final discuss key challenges\nincluding scalability and computational efficiency, and outline promising\nfuture directions such as efficient architectures, conditioning strategies, and\nintegration with foundation models (e.g., visual-language models and large\nlanguage models). By synthesizing recent advances and outlining open research\nquestions, this survey aims to guide researchers and practitioners in\nleveraging DMs for innovative AD solutions across diverse applications.", "comment": "20 pages, 11 figures, 13 tables", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09368v1", "AI": {"title_translation": "扩散模型在异常检测与生成中的应用：一项综述", "tldr": "本综述全面回顾了扩散模型在异常检测和生成中的应用，强调了两者之间的协同关系，并讨论了挑战与未来方向。", "motivation": "异常检测在网络安全、金融、医疗保健和工业制造等多个领域扮演着关键角色。深度学习，特别是扩散模型（DMs），因其学习复杂数据分布和生成高保真样本的能力，为无监督异常检测提供了强大的框架，从而引发了广泛兴趣。本综述旨在全面回顾扩散模型在异常检测和生成中的应用。", "method": "本综述全面回顾了扩散模型在异常检测与生成（ADGDM）中的应用，以教程形式分析了理论基础和实际实现，涵盖了图像、视频、时间序列、表格和多模态数据。它还提出了一种详细的分类方法，根据异常评分机制、条件策略和架构设计对ADGDM方法进行分类，并分析了它们的优缺点。", "result": "本综述揭示了扩散模型如何实现一个强化循环，其中生成技术直接解决了异常数据稀缺的根本挑战，而检测方法则提供了关键反馈以提高生成保真度和相关性，从而将两者的能力提升到超出其各自潜力的水平。", "conclusion": "本综述旨在通过综合最新进展和概述开放研究问题，指导研究人员和从业者利用扩散模型开发跨越不同应用的创新异常检测解决方案。它还讨论了可扩展性和计算效率等关键挑战，并概述了有前景的未来方向，如高效架构、条件策略以及与基础模型（例如视觉语言模型和大型语言模型）的集成。", "translation": "异常检测（AD）通过识别实际数据中偏离既定规范的意外模式，在网络安全、金融、医疗保健和工业制造等不同领域发挥着关键作用。深度学习的最新进展，特别是扩散模型（DMs），因其学习复杂数据分布和生成高保真样本的能力而引起了广泛兴趣，为无监督异常检测提供了强大的框架。在本综述中，我们全面回顾了扩散模型在异常检测和生成（ADGDM）中的应用，以教程形式分析了理论基础和实际实现，涵盖了图像、视频、时间序列、表格和多模态数据。至关重要的是，与现有综述通常将异常检测和生成视为独立问题不同，我们强调了它们固有的协同关系。我们揭示了扩散模型如何实现一个强化循环，其中生成技术直接解决了异常数据稀缺的根本挑战，而检测方法则提供了关键反馈以提高生成保真度和相关性，从而将两者的能力提升到超出其各自潜力的水平。一份详细的分类法根据异常评分机制、条件策略和架构设计对ADGDM方法进行了分类，并分析了它们的优缺点。我们最后讨论了包括可扩展性和计算效率在内的关键挑战，并概述了有前景的未来方向，例如高效架构、条件策略以及与基础模型（例如视觉语言模型和大型语言模型）的集成。通过综合最新进展和概述开放研究问题，本综述旨在指导研究人员和从业者利用扩散模型开发跨越不同应用的创新异常检测解决方案。", "summary": "这篇综述深入探讨了扩散模型（DMs）在异常检测（AD）和异常生成（AG）中的应用，强调了两者之间固有的协同作用。它详细介绍了DMs如何通过生成技术解决异常数据稀缺问题，并利用检测反馈提升生成质量。综述提供了ADGDM方法的全面分类，分析了它们的优缺点，并讨论了可扩展性、计算效率等挑战以及未来研究方向，旨在指导研究人员利用DMs开发创新的AD解决方案。", "keywords": "异常检测, 扩散模型, 异常生成, 综述, 协同关系", "comments": "这篇综述的创新之处在于它明确地强调并分析了异常检测和生成之间通过扩散模型实现的协同关系，这与以往将两者视为独立问题的综述不同。它不仅提供了全面的技术回顾和分类，还指出了未来的研究方向，特别是与基础模型的结合，这对于推动该领域的发展具有重要意义。"}}
{"id": "2506.09839", "title": "OctoNav: Towards Generalist Embodied Navigation", "authors": ["Chen Gao", "Liankai Jin", "Xingyu Peng", "Jiazhao Zhang", "Yue Deng", "Annan Li", "He Wang", "Si Liu"], "summary": "Embodied navigation stands as a foundation pillar within the broader pursuit\nof embodied AI. However, previous navigation research is divided into different\ntasks/capabilities, e.g., ObjNav, ImgNav and VLN, where they differ in task\nobjectives and modalities, making datasets and methods are designed\nindividually. In this work, we take steps toward generalist navigation agents,\nwhich can follow free-form instructions that include arbitrary compounds of\nmulti-modal and multi-capability. To achieve this, we propose a large-scale\nbenchmark and corresponding method, termed OctoNav-Bench and OctoNav-R1.\nSpecifically, OctoNav-Bench features continuous environments and is constructed\nvia a designed annotation pipeline. We thoroughly craft instruction-trajectory\npairs, where instructions are diverse in free-form with arbitrary modality and\ncapability. Also, we construct a Think-Before-Action (TBA-CoT) dataset within\nOctoNav-Bench to provide the thinking process behind actions. For OctoNav-R1,\nwe build it upon MLLMs and adapt it to a VLA-type model, which can produce\nlow-level actions solely based on 2D visual observations. Moreover, we design a\nHybrid Training Paradigm (HTP) that consists of three stages, i.e.,\nAction-/TBA-SFT, Nav-GPRO, and Online RL stages. Each stage contains\nspecifically designed learning policies and rewards. Importantly, for TBA-SFT\nand Nav-GRPO designs, we are inspired by the OpenAI-o1 and DeepSeek-R1, which\nshow impressive reasoning ability via thinking-before-answer. Thus, we aim to\ninvestigate how to achieve thinking-before-action in the embodied navigation\nfield, to improve model's reasoning ability toward generalists. Specifically,\nwe propose TBA-SFT to utilize the TBA-CoT dataset to fine-tune the model as a\ncold-start phrase and then leverage Nav-GPRO to improve its thinking ability.\nFinally, OctoNav-R1 shows superior performance compared with previous methods.", "comment": "31 pages, 25 figures", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09839v1", "AI": {"title_translation": "OctoNav：迈向通用具身导航", "tldr": "提出OctoNav-Bench基准和OctoNav-R1方法，旨在通过“先思考后行动”范式，实现能够遵循自由指令的通用具身导航。", "motivation": "现有具身导航研究分散于不同任务和能力，导致数据集和方法独立设计，缺乏能处理多模态和多能力自由指令的通用导航智能体。", "method": "提出OctoNav-Bench大规模基准和OctoNav-R1方法。OctoNav-Bench包含连续环境、精心设计的自由形式多模态/多能力指令-轨迹对，并构建了Think-Before-Action (TBA-CoT) 数据集。OctoNav-R1基于MLLM并适应VLA模型，仅凭2D视觉观察生成低级动作。它采用混合训练范式 (HTP)，包括Action-/TBA-SFT、Nav-GPRO和在线RL三个阶段，其中TBA-SFT和Nav-GPRO受“先思考后回答”启发，旨在实现“先思考后行动”以提升推理能力。", "result": "OctoNav-R1与现有方法相比表现出卓越的性能。", "conclusion": "OctoNav-Bench和OctoNav-R1的提出，以及“先思考后行动”范式的引入，有效推动了通用具身导航领域的发展，并显著提升了模型的推理能力。", "translation": "具身导航是具身AI广泛追求的基础支柱。然而，以往的导航研究被划分为不同的任务/能力，例如ObjNav、ImgNav和VLN，它们在任务目标和模态上存在差异，导致数据集和方法都是独立设计的。在这项工作中，我们迈向了通用导航智能体，它们能够遵循包含任意多模态和多能力组合的自由形式指令。为了实现这一目标，我们提出了一个大规模基准和相应的方法，分别命名为OctoNav-Bench和OctoNav-R1。具体来说，OctoNav-Bench具有连续环境，并通过设计的标注流程构建。我们精心制作了指令-轨迹对，其中指令在自由形式上具有任意模态和能力的多样性。此外，我们在OctoNav-Bench中构建了一个“先思考后行动”（TBA-CoT）数据集，以提供行动背后的思考过程。对于OctoNav-R1，我们将其建立在MLLM之上，并将其适应为VLA型模型，该模型仅基于2D视觉观察即可生成低级动作。此外，我们设计了一种混合训练范式（HTP），包括三个阶段：即行动-/TBA-SFT、Nav-GPRO和在线RL阶段。每个阶段都包含专门设计的学习策略和奖励。重要的是，对于TBA-SFT和Nav-GRPO的设计，我们受到了OpenAI-o1和DeepSeek-R1的启发，它们通过“先思考后回答”展示了令人印象深刻的推理能力。因此，我们旨在研究如何在具身导航领域实现“先思考后行动”，以提高模型的推理能力，使其迈向通用化。具体来说，我们提出TBA-SFT来利用TBA-CoT数据集对模型进行冷启动阶段的微调，然后利用Nav-GPRO来提高其思考能力。最后，OctoNav-R1与现有方法相比表现出卓越的性能。", "summary": "本文针对具身导航领域现有研究碎片化、缺乏通用智能体的问题，提出了OctoNav-Bench大规模基准和OctoNav-R1方法。OctoNav-Bench包含连续环境和多样化的自由形式多模态/多能力指令-轨迹对，并引入TBA-CoT数据集记录思考过程。OctoNav-R1基于MLLM，通过“先思考后行动”的混合训练范式（HTP）实现，旨在提升模型推理能力以处理复杂的自由指令。实验结果表明，OctoNav-R1性能优于现有方法，推动了通用具身导航的发展。", "keywords": "具身导航, 通用智能体, 多模态指令, 先思考后行动, OctoNav", "comments": "该论文的创新点在于提出了一个旨在实现通用具身导航的综合框架，包括大规模基准和新的训练范式。通过引入“先思考后行动”的理念和TBA-CoT数据集，它有望显著提升具身导航智能体的推理能力和泛化性，从而克服以往研究的碎片化问题。这是一个向更智能、更灵活的具身AI迈进的重要一步。"}}
{"id": "2506.09522", "title": "Revisit What You See: Disclose Language Prior in Vision Tokens for Efficient Guided Decoding of LVLMs", "authors": ["Beomsik Cho", "Jaehyung Kim"], "summary": "Large Vision-Language Models (LVLMs) have demonstrated remarkable performance\nacross various multimodal tasks by integrating visual perception with language\nunderstanding. However, conventional decoding strategies of LVLMs often fail to\nsuccessfully utilize visual information, leading to visually ungrounded\nresponses. While various approaches have been proposed to address this\nlimitation, they typically require additional training, multi-step inference\nprocedures, or external model dependencies. This paper introduces ReVisiT, a\nsimple yet effective decoding method that references vision tokens to guide the\ntext generation process in LVLMs. Our approach leverages the semantic\ninformation embedded within vision tokens by projecting them into the text\ntoken distribution space, and dynamically selecting the most relevant vision\ntoken at each decoding step through constrained divergence minimization. This\nselected vision token is then used to refine the output distribution to better\nincorporate visual semantics. Experiments on three LVLM hallucination\nbenchmarks with two recent LVLMs demonstrate that ReVisiT consistently enhances\nvisual grounding with minimal computational overhead. Moreover, our method\nachieves competitive or superior results relative to state-of-the-art baselines\nwhile reducing computational costs for up to $2\\times$.", "comment": "Code available at https://github.com/bscho333/ReVisiT", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09522v1", "AI": {"title_translation": "重审所见：揭示视觉Tokens中的语言先验以高效引导LVLM解码", "tldr": "ReVisiT是一种简单高效的解码方法，通过利用视觉tokens中的语义信息来引导LVLM的文本生成，有效解决了视觉幻觉问题，且计算开销极小。", "motivation": "大型视觉-语言模型（LVLMs）的传统解码策略未能充分利用视觉信息，导致生成缺乏视觉依据的响应（视觉幻觉）。现有解决方案通常需要额外训练、多步推理或外部模型依赖。", "method": "本文提出了ReVisiT，一种简单而有效的解码方法。它通过将视觉tokens投影到文本token分布空间，并动态选择最相关的视觉token来指导文本生成。该方法通过约束散度最小化选择视觉token，并用其精炼输出分布，以更好地融合视觉语义。", "result": "在三个LVLM幻觉基准测试和两个最新LVLM上的实验表明，ReVisiT持续增强了视觉接地能力，计算开销极小。此外，该方法在保持竞争力甚至超越现有最先进基线的同时，将计算成本降低了高达2倍。", "conclusion": "ReVisiT是一种有效且计算高效的解码方法，能够显著提升LVLM的视觉接地能力，解决视觉幻觉问题，并优于或媲美现有先进方法。", "translation": "大型视觉-语言模型（LVLMs）通过整合视觉感知和语言理解，在各种多模态任务中表现出卓越的性能。然而，LVLMs的传统解码策略往往未能成功利用视觉信息，导致生成缺乏视觉依据的响应。尽管已经提出了各种方法来解决这一限制，但它们通常需要额外的训练、多步推理过程或外部模型依赖。本文介绍了ReVisiT，一种简单而有效的解码方法，它参考视觉tokens来指导LVLMs的文本生成过程。我们的方法通过将视觉tokens投影到文本token分布空间，并通告约束散度最小化在每个解码步骤动态选择最相关的视觉token，从而利用嵌入在视觉tokens中的语义信息。然后，这个选定的视觉token被用来精炼输出分布，以更好地融合视觉语义。在三个LVLM幻觉基准测试和两个最新LVLM上的实验表明，ReVisiT以最小的计算开销持续增强了视觉接地能力。此外，我们的方法相对于最先进的基线取得了有竞争力或更优异的结果，同时将计算成本降低了高达2倍。", "summary": "本文提出了一种名为ReVisiT的解码方法，旨在解决大型视觉-语言模型（LVLMs）在文本生成过程中缺乏视觉依据的问题。ReVisiT通过将视觉tokens投影到文本token分布空间，并动态选择最相关的视觉token来指导文本生成，从而有效利用视觉语义信息。实验证明，ReVisiT显著提升了LVLMs的视觉接地能力，且计算开销极小，同时在性能上优于或媲美现有先进方法。", "keywords": "视觉-语言模型, 解码策略, 视觉接地, 视觉幻觉, ReVisiT", "comments": "ReVisiT的创新之处在于其通过直接利用视觉tokens中的语言先验来引导解码，避免了传统方法所需的额外训练或复杂推理步骤，从而显著提升了效率和性能。其在计算成本上的显著降低（高达2倍）使其在实际应用中更具吸引力。该方法为解决LVLM的视觉幻觉问题提供了一个简洁而有效的途径，对未来多模态模型的发展具有重要意义。"}}
{"id": "2506.09373", "title": "LPO: Towards Accurate GUI Agent Interaction via Location Preference Optimization", "authors": ["Jiaqi Tang", "Yu Xia", "Yi-Feng Wu", "Yuwei Hu", "Yuhui Chen", "Qing-Guo Chen", "Xiaogang Xu", "Xiangyu Wu", "Hao Lu", "Yanqing Ma", "Shiyin Lu", "Qifeng Chen"], "summary": "The advent of autonomous agents is transforming interactions with Graphical\nUser Interfaces (GUIs) by employing natural language as a powerful\nintermediary. Despite the predominance of Supervised Fine-Tuning (SFT) methods\nin current GUI agents for achieving spatial localization, these methods face\nsubstantial challenges due to their limited capacity to accurately perceive\npositional data. Existing strategies, such as reinforcement learning, often\nfail to assess positional accuracy effectively, thereby restricting their\nutility. In response, we introduce Location Preference Optimization (LPO), a\nnovel approach that leverages locational data to optimize interaction\npreferences. LPO uses information entropy to predict interaction positions by\nfocusing on zones rich in information. Besides, it further introduces a dynamic\nlocation reward function based on physical distance, reflecting the varying\nimportance of interaction positions. Supported by Group Relative Preference\nOptimization (GRPO), LPO facilitates an extensive exploration of GUI\nenvironments and significantly enhances interaction precision. Comprehensive\nexperiments demonstrate LPO's superior performance, achieving SOTA results\nacross both offline benchmarks and real-world online evaluations. Our code will\nbe made publicly available soon, at https://github.com/AIDC-AI/LPO.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09373v1", "AI": {"title_translation": "LPO: 通过位置偏好优化实现精确的GUI代理交互", "tldr": "LPO是一种新的方法，通过信息熵和动态位置奖励优化位置偏好，显著提高了GUI代理交互的精度，并在离线和在线评估中达到了SOTA性能。", "motivation": "现有的GUI代理SFT方法在空间定位方面能力有限，难以准确感知位置数据。强化学习等策略也未能有效评估位置精度，限制了它们的效用。", "method": "本文提出了LPO（位置偏好优化），一种利用位置数据优化交互偏好的新方法。LPO使用信息熵通过关注信息丰富的区域来预测交互位置，并引入基于物理距离的动态位置奖励函数，反映交互位置的不同重要性。此外，它还通过Group Relative Preference Optimization（GRPO）促进了对GUI环境的广泛探索，从而显著提高了交互精度。", "result": "LPO在离线基准测试和真实世界在线评估中均实现了SOTA（State-of-the-Art）性能。", "conclusion": "LPO通过优化位置偏好，显著提高了GUI代理交互的精度，解决了现有方法的局限性。", "translation": "自主代理的出现正在通过使用自然语言作为强大的中介来改变与图形用户界面（GUI）的交互。尽管当前GUI代理中监督微调（SFT）方法在实现空间定位方面占据主导地位，但这些方法由于其准确感知位置数据的能力有限而面临巨大挑战。现有的策略，例如强化学习，往往无法有效评估位置精度，从而限制了它们的效用。作为回应，我们引入了位置偏好优化（LPO），这是一种利用位置数据优化交互偏好的新方法。LPO使用信息熵通过关注信息丰富的区域来预测交互位置。此外，它进一步引入了基于物理距离的动态位置奖励函数，反映了交互位置的不同重要性。在组相对偏好优化（GRPO）的支持下，LPO促进了对GUI环境的广泛探索，并显著提高了交互精度。全面的实验证明了LPO的卓越性能，在离线基准测试和真实世界在线评估中均取得了SOTA结果。我们的代码将很快公开，网址为https://github.com/AIDC-AI/LPO。", "summary": "本文提出了LPO（位置偏好优化），一种旨在提高GUI代理交互精度的创新方法。针对现有监督微调方法在空间定位上的局限性，LPO利用信息熵预测交互位置，并引入动态位置奖励函数以反映位置重要性。结合组相对偏好优化（GRPO），LPO能更有效地探索GUI环境并显著提升交互精度。实验结果表明，LPO在离线和在线评估中均达到了最先进的性能。", "keywords": "GUI代理, 位置偏好优化, 信息熵, 动态奖励, 空间定位", "comments": "这篇论文提出了一个新颖的优化框架LPO，通过结合信息熵和动态位置奖励来解决GUI代理空间定位不准确的问题。其创新点在于从“偏好优化”的角度提升交互精度，并辅以GRPO进行环境探索，这为GUI代理的开发提供了一个有前景的方向。在离线和在线测试中均达到SOTA，表明其在实际应用中的潜力。"}}
{"id": "2506.09981", "title": "ReSim: Reliable World Simulation for Autonomous Driving", "authors": ["Jiazhi Yang", "Kashyap Chitta", "Shenyuan Gao", "Long Chen", "Yuqian Shao", "Xiaosong Jia", "Hongyang Li", "Andreas Geiger", "Xiangyu Yue", "Li Chen"], "summary": "How can we reliably simulate future driving scenarios under a wide range of\nego driving behaviors? Recent driving world models, developed exclusively on\nreal-world driving data composed mainly of safe expert trajectories, struggle\nto follow hazardous or non-expert behaviors, which are rare in such data. This\nlimitation restricts their applicability to tasks such as policy evaluation. In\nthis work, we address this challenge by enriching real-world human\ndemonstrations with diverse non-expert data collected from a driving simulator\n(e.g., CARLA), and building a controllable world model trained on this\nheterogeneous corpus. Starting with a video generator featuring a diffusion\ntransformer architecture, we devise several strategies to effectively integrate\nconditioning signals and improve prediction controllability and fidelity. The\nresulting model, ReSim, enables Reliable Simulation of diverse open-world\ndriving scenarios under various actions, including hazardous non-expert ones.\nTo close the gap between high-fidelity simulation and applications that require\nreward signals to judge different actions, we introduce a Video2Reward module\nthat estimates a reward from ReSim's simulated future. Our ReSim paradigm\nachieves up to 44% higher visual fidelity, improves controllability for both\nexpert and non-expert actions by over 50%, and boosts planning and policy\nselection performance on NAVSIM by 2% and 25%, respectively.", "comment": "Project page: https://opendrivelab.com/ReSim", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09981v1", "AI": {"title_translation": "ReSim：自动驾驶的可靠世界模拟", "tldr": "ReSim通过整合危险和非专家驾驶数据，实现了自动驾驶的可靠世界模拟，显著提升了模拟保真度和可控性。", "motivation": "现有的驾驶世界模型主要基于安全的专家驾驶数据，难以模拟危险或非专家行为，这限制了它们在策略评估等任务中的应用。", "method": "本文通过将真实世界人类演示与来自驾驶模拟器（如CARLA）的多样化非专家数据相结合，构建了一个基于这种异构语料库训练的可控世界模型。该模型以扩散变换器架构的视频生成器为基础，并设计了多种策略来有效整合条件信号，提高预测的可控性和保真度。为了弥合高保真模拟与需要奖励信号的应用之间的差距，引入了一个Video2Reward模块来估计ReSim模拟未来的奖励。", "result": "ReSim范式实现了高达44%的视觉保真度提升，专家和非专家动作的可控性提高了50%以上，NAVSIM上的规划和策略选择性能分别提升了2%和25%。", "conclusion": "ReSim通过整合多样化的非专家数据并构建可控的世界模型，成功实现了对各种开放世界驾驶场景（包括危险的非专家行为）的可靠模拟，显著提升了模拟的保真度和可控性，并改善了规划和策略选择性能。", "translation": "我们如何才能在各种自我驾驶行为下可靠地模拟未来的驾驶场景？最近开发的驾驶世界模型，专门基于主要由安全专家轨迹组成的真实世界驾驶数据，难以遵循在此类数据中罕见的危险或非专家行为。这一限制限制了它们在策略评估等任务中的适用性。在这项工作中，我们通过用从驾驶模拟器（例如CARLA）收集的多样化非专家数据丰富真实世界的人类演示来解决这一挑战，并构建了一个基于这种异构语料库训练的可控世界模型。从具有扩散变换器架构的视频生成器开始，我们设计了几种策略来有效整合条件信号并提高预测的可控性和保真度。由此产生的模型ReSim，能够可靠地模拟各种动作（包括危险的非专家动作）下的多样化开放世界驾驶场景。为了弥合高保真模拟与需要奖励信号来判断不同动作的应用之间的差距，我们引入了一个Video2Reward模块，该模块从ReSim模拟的未来中估计奖励。我们的ReSim范式实现了高达44%的视觉保真度提升，专家和非专家动作的可控性提高了50%以上，NAVSIM上的规划和策略选择性能分别提升了2%和25%。", "summary": "ReSim提出了一种可靠的自动驾驶世界模拟方法，解决了现有模型在模拟危险或非专家驾驶行为方面的不足。该方法通过整合真实世界数据和驾驶模拟器中的多样化非专家数据来训练一个可控的世界模型，并利用扩散变换器架构进行视频生成。此外，引入了Video2Reward模块以支持基于奖励信号的应用。实验结果表明，ReSim显著提升了视觉保真度、可控性以及规划和策略选择性能。", "keywords": "自动驾驶, 世界模型, 模拟, 扩散变换器, 非专家行为", "comments": "ReSim的创新点在于其通过融合异构数据（真实世界专家数据与模拟器非专家数据）来解决自动驾驶世界模型中罕见危险行为模拟的难题。这种方法显著提升了模拟的保真度和可控性，对于自动驾驶策略评估和规划具有重要意义，尤其是在处理边缘情况和提高系统鲁棒性方面。"}}
{"id": "2506.09534", "title": "Gaussian Herding across Pens: An Optimal Transport Perspective on Global Gaussian Reduction for 3DGS", "authors": ["Tao Wang", "Mengyu Li", "Geduo Zeng", "Cheng Meng", "Qiong Zhang"], "summary": "3D Gaussian Splatting (3DGS) has emerged as a powerful technique for radiance\nfield rendering, but it typically requires millions of redundant Gaussian\nprimitives, overwhelming memory and rendering budgets. Existing compaction\napproaches address this by pruning Gaussians based on heuristic importance\nscores, without global fidelity guarantee. To bridge this gap, we propose a\nnovel optimal transport perspective that casts 3DGS compaction as global\nGaussian mixture reduction. Specifically, we first minimize the composite\ntransport divergence over a KD-tree partition to produce a compact geometric\nrepresentation, and then decouple appearance from geometry by fine-tuning color\nand opacity attributes with far fewer Gaussian primitives. Experiments on\nbenchmark datasets show that our method (i) yields negligible loss in rendering\nquality (PSNR, SSIM, LPIPS) compared to vanilla 3DGS with only 10% Gaussians;\nand (ii) consistently outperforms state-of-the-art 3DGS compaction techniques.\nNotably, our method is applicable to any stage of vanilla or accelerated 3DGS\npipelines, providing an efficient and agnostic pathway to lightweight neural\nrendering.", "comment": "18 pages, 8 figures", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09534v1", "AI": {"title_translation": "跨区域高斯群集：3DGS全局高斯缩减的最优传输视角", "tldr": "3DGS通常需要数百万个冗余高斯原语，消耗大量内存和渲染预算。本文提出了一种新颖的基于最优传输的方法，将3DGS压缩视为全局高斯混合缩减。该方法在保持渲染质量的同时，能将高斯数量减少到10%，并优于现有技术，为轻量级神经渲染提供了高效途径。", "motivation": "3D高斯泼溅（3DGS）技术需要数百万个冗余高斯原语，导致内存和渲染预算过载。现有压缩方法通过启发式重要性评分修剪高斯，但缺乏全局保真度保证。", "method": "本文提出了一种新颖的最优传输视角，将3DGS压缩视为全局高斯混合缩减。具体而言，首先在KD树分区上最小化复合传输散度以生成紧凑的几何表示，然后通过使用更少的高斯原语微调颜色和不透明度属性，将外观与几何解耦。", "result": "与原始3DGS相比，我们的方法仅使用10%的高斯即可实现可忽略的渲染质量损失（PSNR、SSIM、LPIPS）；并且始终优于最先进的3DGS压缩技术。", "conclusion": "我们的方法适用于原始或加速3DGS管道的任何阶段，为轻量级神经渲染提供了一种高效且不可知的方法。", "translation": "3D高斯泼溅（3DGS）已成为一种强大的辐射场渲染技术，但它通常需要数百万个冗余高斯原语，消耗大量内存和渲染预算。现有压缩方法通过基于启发式重要性评分修剪高斯来解决此问题，但缺乏全局保真度保证。为了弥补这一空白，我们提出了一种新颖的最优传输视角，将3DGS压缩视为全局高斯混合缩减。具体而言，我们首先在KD树分区上最小化复合传输散度以生成紧凑的几何表示，然后通过使用更少的高斯原语微调颜色和不透明度属性，将外观与几何解耦。基准数据集上的实验表明，我们的方法（i）与原始3DGS相比，仅使用10%的高斯即可实现可忽略的渲染质量损失（PSNR、SSIM、LPIPS）；并且（ii）始终优于最先进的3DGS压缩技术。值得注意的是，我们的方法适用于原始或加速3DGS管道的任何阶段，为轻量级神经渲染提供了一种高效且不可知的方法。", "summary": "3DGS因其数百万个冗余高斯原语而面临内存和渲染预算过载问题，且现有压缩方法缺乏全局保真度保证。本文提出了一种基于最优传输的新方法，将3DGS压缩视为全局高斯混合缩减。该方法首先通过最小化KD树分区上的复合传输散度来创建紧凑的几何表示，然后解耦并微调外观属性。实验结果显示，该方法在仅使用10%高斯的情况下，渲染质量损失可忽略不计，并持续优于现有最先进的3DGS压缩技术，为轻量级神经渲染提供了一条高效且通用的途径。", "keywords": "3D高斯泼溅, 最优传输, 高斯缩减, 神经渲染, 模型压缩", "comments": "本文通过引入最优传输视角来解决3DGS中的高斯冗余问题，这是一种创新性的方法，超越了以往依赖启发式修剪的局限性，提供了全局保真度保证。其在大幅减少高斯数量的同时保持渲染质量的强大实验结果，使得3DGS在实际应用中更具可行性和效率。该方法对多种3DGS管道的普适性也显著增强了其重要性。"}}
{"id": "2506.09376", "title": "Revisiting Diffusion Models: From Generative Pre-training to One-Step Generation", "authors": ["Bowen Zheng", "Tianming Yang"], "summary": "Diffusion distillation is a widely used technique to reduce the sampling cost\nof diffusion models, yet it often requires extensive training, and the student\nperformance tends to be degraded. Recent studies show that incorporating a GAN\nobjective may alleviate these issues, yet the underlying mechanism remains\nunclear. In this work, we first identify a key limitation of distillation:\nmismatched step sizes and parameter numbers between the teacher and the student\nmodel lead them to converge to different local minima, rendering direct\nimitation suboptimal. We further demonstrate that a standalone GAN objective,\nwithout relying a distillation loss, overcomes this limitation and is\nsufficient to convert diffusion models into efficient one-step generators.\nBased on this finding, we propose that diffusion training may be viewed as a\nform of generative pre-training, equipping models with capabilities that can be\nunlocked through lightweight GAN fine-tuning. Supporting this view, we create a\none-step generation model by fine-tuning a pre-trained model with 85% of\nparameters frozen, achieving strong performance with only 0.2M images and\nnear-SOTA results with 5M images. We further present a frequency-domain\nanalysis that may explain the one-step generative capability gained in\ndiffusion training. Overall, our work provides a new perspective for diffusion\ntraining, highlighting its role as a powerful generative pre-training process,\nwhich can be the basis for building efficient one-step generation models.", "comment": "ICML 2025", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09376v1", "AI": {"title_translation": "重新审视扩散模型：从生成式预训练到一步生成", "tldr": "本文提出将扩散训练视为生成式预训练，通过轻量级GAN微调实现高效的一步生成，解决了传统扩散蒸馏的局限性。", "motivation": "扩散蒸馏技术虽然能降低采样成本，但通常需要大量训练且学生模型性能会下降。近期研究表明引入GAN目标可能缓解这些问题，但其内在机制尚不明确。本文旨在识别蒸馏的局限性并阐明GAN目标的作用。", "method": "首先，识别出蒸馏的关键局限性：教师模型和学生模型之间不匹配的步长和参数数量导致它们收敛到不同的局部最小值，使得直接模仿次优。其次，证明了独立的GAN目标（不依赖蒸馏损失）足以将扩散模型转换为高效的一步生成器。基于此发现，提出扩散训练可被视为一种生成式预训练。最后，通过冻结85%参数的轻量级GAN微调，创建了一个一步生成模型，并进行了频域分析来解释一步生成能力。", "result": "通过冻结85%的参数进行微调，使用0.2M图像实现了强大的性能，使用5M图像实现了接近SOTA的结果。证明了扩散模型可以被转换为高效的一步生成器。", "conclusion": "扩散训练可以被视为一种强大的生成式预训练过程，为构建高效的一步生成模型奠定了基础。", "translation": "扩散蒸馏是一种广泛使用的技术，用于降低扩散模型的采样成本，但它通常需要大量的训练，并且学生模型的性能往往会下降。最近的研究表明，结合GAN目标可以缓解这些问题，但其底层机制仍不清楚。在这项工作中，我们首先识别了蒸馏的一个关键限制：教师模型和学生模型之间不匹配的步长和参数数量导致它们收敛到不同的局部最小值，使得直接模仿次优。我们进一步证明，一个独立的GAN目标，不依赖蒸馏损失，可以克服这一限制，并且足以将扩散模型转换为高效的一步生成器。基于这一发现，我们提出扩散训练可以被视为一种生成式预训练形式，赋予模型通过轻量级GAN微调即可解锁的能力。为了支持这一观点，我们通过微调一个预训练模型（85%的参数被冻结），创建了一个一步生成模型，仅用0.2M图像就取得了强大的性能，用5M图像取得了接近SOTA的结果。我们还提出了一个频域分析，可能解释了扩散训练中获得的一步生成能力。总的来说，我们的工作为扩散训练提供了一个新视角，强调其作为强大生成式预训练过程的作用，这可以成为构建高效一步生成模型的基础。", "summary": "本文重新审视了扩散模型，提出将扩散训练视为一种生成式预训练形式，能够通过轻量级GAN微调解锁高效的一步生成能力。文章首先指出扩散蒸馏的局限性在于教师和学生模型在步长和参数数量上的不匹配导致次优模仿。研究表明，一个独立的GAN目标无需蒸馏损失即可将扩散模型转换为高效的一步生成器。基于此，作者通过冻结大部分参数的GAN微调，仅用少量图像便实现了出色的性能。此外，论文还提供了频域分析来解释扩散训练获得的一步生成能力。这项工作为扩散训练提供了一个新视角，强调其作为强大生成式预训练的作用，可作为构建高效一步生成模型的基础。", "keywords": "扩散模型, 生成式预训练, 一步生成, GAN, 模型蒸馏", "comments": "本文的创新之处在于提出了将扩散训练视为生成式预训练的新视角，并通过实验证明了仅需轻量级的GAN微调即可实现高效的一步生成，有效规避了传统扩散蒸馏的训练成本高和性能下降的问题。这为构建更高效的生成模型提供了一个有前景的新范式。"}}
{"id": "2506.09538", "title": "AngleRoCL: Angle-Robust Concept Learning for Physically View-Invariant T2I Adversarial Patches", "authors": ["Wenjun Ji", "Yuxiang Fu", "Luyang Ying", "Deng-Ping Fan", "Yuyi Wang", "Ming-Ming Cheng", "Ivor Tsang", "Qing Guo"], "summary": "Cutting-edge works have demonstrated that text-to-image (T2I) diffusion\nmodels can generate adversarial patches that mislead state-of-the-art object\ndetectors in the physical world, revealing detectors' vulnerabilities and\nrisks. However, these methods neglect the T2I patches' attack effectiveness\nwhen observed from different views in the physical world (i.e., angle\nrobustness of the T2I adversarial patches). In this paper, we study the angle\nrobustness of T2I adversarial patches comprehensively, revealing their\nangle-robust issues, demonstrating that texts affect the angle robustness of\ngenerated patches significantly, and task-specific linguistic instructions fail\nto enhance the angle robustness. Motivated by the studies, we introduce\nAngle-Robust Concept Learning (AngleRoCL), a simple and flexible approach that\nlearns a generalizable concept (i.e., text embeddings in implementation)\nrepresenting the capability of generating angle-robust patches. The learned\nconcept can be incorporated into textual prompts and guides T2I models to\ngenerate patches with their attack effectiveness inherently resistant to\nviewpoint variations. Through extensive simulation and physical-world\nexperiments on five SOTA detectors across multiple views, we demonstrate that\nAngleRoCL significantly enhances the angle robustness of T2I adversarial\npatches compared to baseline methods. Our patches maintain high attack success\nrates even under challenging viewing conditions, with over 50% average relative\nimprovement in attack effectiveness across multiple angles. This research\nadvances the understanding of physically angle-robust patches and provides\ninsights into the relationship between textual concepts and physical properties\nin T2I-generated contents.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09538v1", "AI": {"title_translation": "AngleRoCL：用于物理视点不变T2I对抗性补丁的角度鲁棒概念学习", "tldr": "AngleRoCL通过学习可泛化概念，使T2I对抗性补丁在物理世界中对不同视角具有角度鲁棒性，显著提高了攻击效果。", "motivation": "现有文本到图像（T2I）对抗性补丁生成方法忽略了补丁在物理世界中从不同视角观察时的攻击有效性（即角度鲁棒性）。研究发现文本显著影响角度鲁棒性，且特定任务的语言指令未能增强其鲁棒性，因此本研究旨在解决这一问题。", "method": "本文提出了角度鲁棒概念学习（AngleRoCL），这是一种简单灵活的方法，通过学习一个可泛化的概念（实现为文本嵌入），代表生成角度鲁棒补丁的能力。学习到的概念可以融入文本提示中，指导T2I模型生成其攻击有效性固有地抵抗视点变化的补丁。", "result": "AngleRoCL显著增强了T2I对抗性补丁的角度鲁棒性，与基线方法相比，即使在挑战性视角条件下也能保持高攻击成功率。在多个角度下，攻击有效性平均相对提高了50%以上。通过在多个视角下对五个SOTA检测器进行广泛的模拟和物理世界实验验证了其有效性。", "conclusion": "这项研究促进了对物理角度鲁棒补丁的理解，并为T2I生成内容中文本概念与物理属性之间的关系提供了见解。", "translation": "尖端研究表明，文本到图像（T2I）扩散模型可以生成对抗性补丁，从而误导物理世界中最先进的目标检测器，揭示了检测器的漏洞和风险。然而，这些方法忽略了T2I补丁在物理世界中从不同视角观察时的攻击有效性（即T2I对抗性补丁的角度鲁棒性）。在本文中，我们全面研究了T2I对抗性补丁的角度鲁棒性，揭示了它们的角度鲁棒性问题，证明了文本显著影响生成补丁的角度鲁棒性，并且特定任务的语言指令未能增强角度鲁棒性。受这些研究的启发，我们引入了角度鲁棒概念学习（AngleRoCL），这是一种简单灵活的方法，它学习一个可泛化的概念（即实现中的文本嵌入），代表生成角度鲁棒补丁的能力。学习到的概念可以融入文本提示中，并指导T2I模型生成其攻击有效性固有地抵抗视点变化的补丁。通过在多个视角下对五个SOTA检测器进行广泛的模拟和物理世界实验，我们证明了与基线方法相比，AngleRoCL显著增强了T2I对抗性补丁的角度鲁棒性。我们的补丁即使在挑战性视角条件下也能保持高攻击成功率，在多个角度下的攻击有效性平均相对提高超过50%。这项研究促进了对物理角度鲁棒补丁的理解，并为T2I生成内容中文本概念与物理属性之间的关系提供了见解。", "summary": "该论文引入了AngleRoCL，一种角度鲁棒概念学习方法，旨在解决文本到图像（T2I）生成的对抗性补丁在物理世界中对不同视角缺乏鲁棒性的问题。通过学习一个可泛化的概念（文本嵌入），AngleRoCL能够指导T2I模型生成对视点变化具有固有抵抗力的对抗性补丁。实验结果表明，AngleRoCL显著提高了T2I对抗性补丁的角度鲁棒性，在多种视角下攻击成功率高，攻击有效性平均相对提升超过50%。这项工作加深了对物理角度鲁棒补丁的理解，并揭示了文本概念与T2I生成内容物理属性之间的关系。", "keywords": "对抗性补丁, 角度鲁棒性, 文本到图像, 概念学习, 物理世界攻击", "comments": "AngleRoCL的创新之处在于其通过学习一个“可泛化概念”（文本嵌入）来解决T2I对抗性补丁的角度鲁棒性问题，这为生成物理世界中更具威胁性的对抗样本提供了一条新途径。该方法强调了文本提示在影响生成内容物理属性方面的重要性，为未来研究提供了宝贵见解。其在实际物理世界实验中的有效性也证明了其潜在的应用价值。"}}
{"id": "2506.09398", "title": "Efficient Prediction of SO(3)-Equivariant Hamiltonian Matrices via SO(2) Local Frames", "authors": ["Haiyang Yu", "Yuchao Lin", "Xuan Zhang", "Xiaofeng Qian", "Shuiwang Ji"], "summary": "We consider the task of predicting Hamiltonian matrices to accelerate\nelectronic structure calculations, which plays an important role in physics,\nchemistry, and materials science. Motivated by the inherent relationship\nbetween the off-diagonal blocks of the Hamiltonian matrix and the SO(2) local\nframe, we propose a novel and efficient network, called QHNetV2, that achieves\nglobal SO(3) equivariance without the costly SO(3) Clebsch-Gordan tensor\nproducts. This is achieved by introducing a set of new efficient and powerful\nSO(2)-equivariant operations and performing all off-diagonal feature updates\nand message passing within SO(2) local frames, thereby eliminating the need of\nSO(3) tensor products. Moreover, a continuous SO(2) tensor product is performed\nwithin the SO(2) local frame at each node to fuse node features, mimicking the\nsymmetric contraction operation. Extensive experiments on the large QH9 and\nMD17 datasets demonstrate that our model achieves superior performance across a\nwide range of molecular structures and trajectories, highlighting its strong\ngeneralization capability. The proposed SO(2) operations on SO(2) local frames\noffer a promising direction for scalable and symmetry-aware learning of\nelectronic structures. Our code will be released as part of the AIRS library\nhttps://github.com/divelab/AIRS.", "comment": "Code available at: https://github.com/divelab/AIRS", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09398v1", "AI": {"title_translation": "通过SO(2)局部坐标系高效预测SO(3)等变哈密顿矩阵", "tldr": "提出QHNetV2，利用SO(2)局部坐标系和操作，高效预测SO(3)等变哈密顿矩阵，避免昂贵的SO(3)张量积，在大型数据集上表现优异。", "motivation": "加速电子结构计算中哈密顿矩阵的预测，这在物理、化学和材料科学中很重要。现有方法可能因SO(3)张量积而昂贵。", "method": "提出QHNetV2网络，通过引入新的高效SO(2)等变操作，并在SO(2)局部坐标系中进行所有非对角特征更新和消息传递，从而避免SO(3) Clebsch-Gordan张量积。此外，在每个节点的SO(2)局部坐标系内执行连续SO(2)张量积以融合节点特征。", "result": "在大型QH9和MD17数据集上的广泛实验表明，该模型在各种分子结构和轨迹上均实现了卓越的性能，突出了其强大的泛化能力。", "conclusion": "所提出的SO(2)局部坐标系上的SO(2)操作为可扩展和对称感知电子结构学习提供了一个有前景的方向。", "translation": "我们考虑预测哈密顿矩阵以加速电子结构计算的任务，这在物理、化学和材料科学中发挥着重要作用。受哈密顿矩阵非对角块与SO(2)局部坐标系之间内在关系的启发，我们提出了一种新颖高效的网络，称为QHNetV2，它在没有昂贵的SO(3) Clebsch-Gordan张量积的情况下实现了全局SO(3)等变性。这是通过引入一组新的高效且强大的SO(2)等变操作，并在SO(2)局部坐标系中执行所有非对角特征更新和消息传递来实现的，从而消除了对SO(3)张量积的需求。此外，在每个节点的SO(2)局部坐标系内执行连续SO(2)张量积以融合节点特征，模仿对称收缩操作。在大型QH9和MD17数据集上的广泛实验表明，我们的模型在各种分子结构和轨迹上均实现了卓越的性能，突出了其强大的泛化能力。所提出的SO(2)局部坐标系上的SO(2)操作为可扩展和对称感知电子结构学习提供了一个有前景的方向。我们的代码将作为AIRS库的一部分发布，网址为https://github.com/divelab/AIRS。", "summary": "该论文提出了一种名为QHNetV2的新型神经网络，旨在高效预测SO(3)等变哈密顿矩阵，以加速电子结构计算。通过利用哈密顿矩阵非对角块与SO(2)局部坐标系的关系，QHNetV2引入了SO(2)等变操作，并在SO(2)局部坐标系中处理特征更新和消息传递，从而避免了计算昂贵的SO(3)张量积。实验结果表明，该模型在QH9和MD17数据集上表现出卓越的性能和强大的泛化能力，为可扩展的对称感知电子结构学习提供了新方向。", "keywords": "哈密顿矩阵, SO(3)等变性, SO(2)局部坐标系, 电子结构计算, 深度学习", "comments": "这篇论文的创新点在于提出了一种通过利用SO(2)局部坐标系和SO(2)等变操作来间接实现SO(3)等变性的方法，从而避免了传统SO(3)张量积的高昂计算成本。这对于需要处理高维对称性的物理、化学和材料科学计算领域具有重要意义，有望显著提高计算效率和模型的可扩展性。"}}
{"id": "2506.09541", "title": "3DGeoDet: General-purpose Geometry-aware Image-based 3D Object Detection", "authors": ["Yi Zhang", "Yi Wang", "Yawen Cui", "Lap-Pui Chau"], "summary": "This paper proposes 3DGeoDet, a novel geometry-aware 3D object detection\napproach that effectively handles single- and multi-view RGB images in indoor\nand outdoor environments, showcasing its general-purpose applicability. The key\nchallenge for image-based 3D object detection tasks is the lack of 3D geometric\ncues, which leads to ambiguity in establishing correspondences between images\nand 3D representations. To tackle this problem, 3DGeoDet generates efficient 3D\ngeometric representations in both explicit and implicit manners based on\npredicted depth information. Specifically, we utilize the predicted depth to\nlearn voxel occupancy and optimize the voxelized 3D feature volume explicitly\nthrough the proposed voxel occupancy attention. To further enhance 3D\nawareness, the feature volume is integrated with an implicit 3D representation,\nthe truncated signed distance function (TSDF). Without requiring supervision\nfrom 3D signals, we significantly improve the model's comprehension of 3D\ngeometry by leveraging intermediate 3D representations and achieve end-to-end\ntraining. Our approach surpasses the performance of state-of-the-art\nimage-based methods on both single- and multi-view benchmark datasets across\ndiverse environments, achieving a 9.3 mAP@0.5 improvement on the SUN RGB-D\ndataset, a 3.3 mAP@0.5 improvement on the ScanNetV2 dataset, and a 0.19\nAP3D@0.7 improvement on the KITTI dataset. The project page is available at:\nhttps://cindy0725.github.io/3DGeoDet/.", "comment": "Accepted by IEEE Transactions on Multimedia", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09541v1", "AI": {"title_translation": "3DGeoDet: 通用几何感知图像三维目标检测", "tldr": "3DGeoDet提出了一种新的通用几何感知3D目标检测方法，通过结合显式和隐式3D几何表示，显著提升了图像基3D目标检测的性能，并在多个基准数据集上超越了SOTA方法。", "motivation": "图像基3D目标检测任务的主要挑战是缺乏3D几何线索，这导致图像和3D表示之间建立对应关系时存在歧义。", "method": "3DGeoDet基于预测的深度信息生成显式和隐式3D几何表示。具体来说，它利用预测深度学习体素占用，并通过提出的体素占用注意力显式优化体素化3D特征体。为了进一步增强3D感知，特征体与隐式3D表示（截断符号距离函数，TSDF）集成。该方法无需3D信号监督，通过利用中间3D表示实现端到端训练。", "result": "该方法在SUN RGB-D数据集上实现了9.3 mAP@0.5的提升，在ScanNetV2数据集上实现了3.3 mAP@0.5的提升，在KITTI数据集上实现了0.19 AP3D@0.7的提升。它超越了单视角和多视角基准数据集上最先进的图像基方法。", "conclusion": "通过利用中间3D表示并实现无需3D信号监督的端到端训练，3DGeoDet显著提高了模型对3D几何的理解，并在各种数据集上超越了最先进的性能。", "translation": "本文提出了3DGeoDet，一种新颖的几何感知3D目标检测方法，能有效处理室内外环境中的单视角和多视角RGB图像，展示了其通用适用性。图像基3D目标检测任务的关键挑战是缺乏3D几何线索，这导致在图像和3D表示之间建立对应关系时存在歧义。为了解决这个问题，3DGeoDet基于预测的深度信息，以显式和隐式方式生成高效的3D几何表示。具体来说，我们利用预测的深度来学习体素占用，并通过提出的体素占用注意力显式优化体素化3D特征体。为了进一步增强3D感知，特征体与隐式3D表示，即截断符号距离函数（TSDF）集成。在不需要3D信号监督的情况下，我们通过利用中间3D表示显著提高了模型对3D几何的理解，并实现了端到端训练。我们的方法在单视角和多视角基准数据集上，在不同环境中，超越了最先进的图像基方法，在SUN RGB-D数据集上实现了9.3 mAP@0.5的提升，在ScanNetV2数据集上实现了3.3 mAP@0.5的提升，在KITTI数据集上实现了0.19 AP3D@0.7的提升。项目页面可在以下网址获取：https://cindy0725.github.io/3DGeoDet/。", "summary": "3DGeoDet是一种新颖的通用几何感知3D目标检测方法，旨在解决图像基3D目标检测中3D几何线索不足的问题。它通过基于预测深度生成显式（体素占用和体素化3D特征体）和隐式（截断符号距离函数，TSDF）3D几何表示来增强3D感知。该方法无需3D信号监督即可实现端到端训练，显著提高了模型对3D几何的理解，并在单视角和多视角基准数据集上超越了现有最佳图像基方法，如在SUN RGB-D上提升9.3 mAP@0.5，ScanNetV2上提升3.3 mAP@0.5，KITTI上提升0.19 AP3D@0.7，展现了其在室内外环境中的通用适用性。", "keywords": "3D目标检测, 几何感知, 图像基, 体素占用, TSDF", "comments": "该论文通过有效整合3D几何线索，解决了图像基3D目标检测中的一个关键挑战。其创新之处在于结合了从预测深度导出的显式和隐式3D表示，从而实现了无需显式3D监督的端到端训练。这种通用方法在不同环境和数据集上均表现出显著的性能提升，是图像3D感知领域迈向鲁棒性的重要一步。"}}
{"id": "2506.09553", "title": "GLD-Road:A global-local decoding road network extraction model for remote sensing images", "authors": ["Ligao Deng", "Yupeng Deng", "Yu Meng", "Jingbo Chen", "Zhihao Xi", "Diyou Liu", "Qifeng Chu"], "summary": "Road networks are crucial for mapping, autonomous driving, and disaster\nresponse. While manual annotation is costly, deep learning offers efficient\nextraction. Current methods include postprocessing (prone to errors), global\nparallel (fast but misses nodes), and local iterative (accurate but slow). We\npropose GLD-Road, a two-stage model combining global efficiency and local\nprecision. First, it detects road nodes and connects them via a Connect Module.\nThen, it iteratively refines broken roads using local searches, drastically\nreducing computation. Experiments show GLD-Road outperforms state-of-the-art\nmethods, improving APLS by 1.9% (City-Scale) and 0.67% (SpaceNet3). It also\nreduces retrieval time by 40% vs. Sat2Graph (global) and 92% vs. RNGDet++\n(local). The experimental results are available at\nhttps://github.com/ucas-dlg/GLD-Road.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09553v1", "AI": {"title_translation": "GLD-Road:一种用于遥感图像的全局-局部解码道路网络提取模型", "tldr": "GLD-Road是一种新的两阶段模型，结合全局效率和局部精度，用于从遥感图像中高效准确地提取道路网络，优于现有SOTA方法。", "motivation": "道路网络对于地图绘制、自动驾驶和灾害响应至关重要。手动标注成本高昂，而现有深度学习方法（后处理、全局并行、局部迭代）各有缺陷，导致错误、遗漏节点或速度慢。", "method": "提出GLD-Road，一个两阶段模型。第一阶段：检测道路节点并通过连接模块连接。第二阶段：使用局部搜索迭代细化断裂道路，显著减少计算量。", "result": "GLD-Road在City-Scale数据集上将APLS提高了1.9%，在SpaceNet3数据集上提高了0.67%。与Sat2Graph（全局方法）相比，检索时间减少了40%；与RNGDet++（局部方法）相比，检索时间减少了92%。", "conclusion": "GLD-Road通过结合全局效率和局部精度，提供了一种更高效、更准确的遥感图像道路网络提取解决方案，克服了现有方法的局限性。", "translation": "道路网络对于地图绘制、自动驾驶和灾害响应至关重要。虽然手动标注成本高昂，但深度学习提供了高效的提取方法。当前方法包括后处理（容易出错）、全局并行（快速但遗漏节点）和局部迭代（准确但缓慢）。我们提出了GLD-Road，一个结合全局效率和局部精度的两阶段模型。首先，它检测道路节点并通过连接模块连接它们。然后，它使用局部搜索迭代细化断裂道路，从而大大减少了计算量。实验表明，GLD-Road优于最先进的方法，在City-Scale数据集上将APLS提高了1.9%，在SpaceNet3数据集上提高了0.67%。与Sat2Graph（全局）相比，它还将检索时间减少了40%；与RNGDet++（局部）相比，减少了92%。实验结果可在https://github.com/ucas-dlg/GLD-Road获取。", "summary": "本文提出GLD-Road，一个用于遥感图像道路网络提取的两阶段模型。它首先检测并连接道路节点，然后通过局部搜索迭代细化断裂道路。该模型结合了全局效率和局部精度，实验证明其在准确性和速度上均优于现有SOTA方法。", "keywords": "道路网络提取, 深度学习, 全局-局部解码, 遥感图像, GLD-Road", "comments": "GLD-Road通过创新性地结合全局检测和局部迭代细化，有效解决了现有道路网络提取方法在效率和精度上的痛点，尤其是在计算量显著减少的同时提升了性能，具有重要的实际应用价值。"}}
{"id": "2506.09433", "title": "Mitigating Spurious Correlations in LLMs via Causality-Aware Post-Training", "authors": ["Shurui Gui", "Shuiwang Ji"], "summary": "While large language models (LLMs) have demonstrated remarkable capabilities\nin language modeling, recent studies reveal that they often fail on\nout-of-distribution (OOD) samples due to spurious correlations acquired during\npre-training. Here, we aim to mitigate such spurious correlations through\ncausality-aware post-training (CAPT). By decomposing a biased prediction into\ntwo unbiased steps, known as \\textit{event estimation} and \\textit{event\nintervention}, we reduce LLMs' pre-training biases without incurring additional\nfine-tuning biases, thus enhancing the model's generalization ability.\nExperiments on the formal causal inference benchmark CLadder and the logical\nreasoning dataset PrOntoQA show that 3B-scale language models fine-tuned with\nCAPT can outperform both traditional SFT and larger LLMs on in-distribution\n(ID) and OOD tasks using only 100 ID fine-tuning samples, demonstrating the\neffectiveness and sample efficiency of CAPT.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09433v1", "AI": {"title_translation": "通过因果意识后训练缓解大型语言模型中的虚假关联", "tldr": "本文提出了一种名为因果意识后训练（CAPT）的新方法，通过将有偏预测分解为事件估计和事件干预两个无偏步骤，有效缓解了大型语言模型中的虚假关联，显著提升了模型在分布内和分布外任务上的泛化能力和样本效率。", "motivation": "大型语言模型（LLMs）在语言建模方面表现出色，但近期研究表明，由于在预训练期间获得的虚假关联，它们通常在分布外（OOD）样本上表现不佳。本文旨在通过因果意识后训练（CAPT）来缓解这些虚假关联。", "method": "本文通过因果意识后训练（CAPT）来缓解虚假关联。该方法将有偏预测分解为两个无偏步骤，即“事件估计”和“事件干预”，从而在不引入额外微调偏差的情况下减少LLMs的预训练偏差。", "result": "在正式因果推理基准CLadder和逻辑推理数据集PrOntoQA上的实验表明，使用CAPT微调的3B规模语言模型，仅使用100个分布内微调样本，就可以在分布内（ID）和分布外（OOD）任务上优于传统的SFT和更大的LLMs。", "conclusion": "因果意识后训练（CAPT）是一种有效且样本高效的方法，能够缓解大型语言模型中的虚假关联，显著提升模型在分布内和分布外任务上的泛化能力。", "translation": "虽然大型语言模型（LLMs）在语言建模方面展示了卓越的能力，但最近的研究表明，由于在预训练期间获得的虚假关联，它们通常在分布外（OOD）样本上表现不佳。本文旨在通过因果意识后训练（CAPT）来缓解这些虚假关联。通过将有偏预测分解为两个无偏步骤，即“事件估计”和“事件干预”，我们在不引入额外微调偏差的情况下减少了LLMs的预训练偏差，从而增强了模型的泛化能力。在正式因果推理基准CLadder和逻辑推理数据集PrOntoQA上的实验表明，使用CAPT微调的3B规模语言模型，仅使用100个分布内微调样本，就可以在分布内（ID）和分布外（OOD）任务上优于传统的SFT和更大的LLMs，这证明了CAPT的有效性和样本效率。", "summary": "本文提出了一种名为因果意识后训练（CAPT）的新方法，旨在解决大型语言模型（LLMs）因预训练中获得的虚假关联而在分布外（OOD）样本上表现不佳的问题。CAPT通过将有偏预测分解为事件估计和事件干预两个无偏步骤，有效减少了LLMs的预训练偏差，同时避免了额外的微调偏差。实验证明，CAPT显著提升了模型的泛化能力，使其在仅使用少量样本的情况下，在ID和OOD任务上均优于传统方法和更大的LLMs，展现出高效性和样本效率。", "keywords": "大型语言模型, 虚假关联, 因果意识后训练, 分布外泛化, 样本效率", "comments": "该论文的创新点在于提出了因果意识后训练（CAPT）这一新颖的方法，通过引入因果推断的理念（事件估计和事件干预）来直接解决LLMs的虚假关联问题。其重要性体现在它能显著提升LLMs在OOD任务上的泛化能力，并且具有高样本效率，仅需少量微调样本即可超越传统方法和更大模型。这对于提高LLMs的鲁棒性和实际应用价值具有重要意义。"}}
{"id": "2506.09557", "title": "AD^2-Bench: A Hierarchical CoT Benchmark for MLLM in Autonomous Driving under Adverse Conditions", "authors": ["Zhaoyang Wei", "Chenhui Qiang", "Bowen Jiang", "Xumeng Han", "Xuehui Yu", "Zhenjun Han"], "summary": "Chain-of-Thought (CoT) reasoning has emerged as a powerful approach to\nenhance the structured, multi-step decision-making capabilities of Multi-Modal\nLarge Models (MLLMs), is particularly crucial for autonomous driving with\nadverse weather conditions and complex traffic environments. However, existing\nbenchmarks have largely overlooked the need for rigorous evaluation of CoT\nprocesses in these specific and challenging scenarios. To address this critical\ngap, we introduce AD^2-Bench, the first Chain-of-Thought benchmark specifically\ndesigned for autonomous driving with adverse weather and complex scenes.\nAD^2-Bench is meticulously constructed to fulfill three key criteria:\ncomprehensive data coverage across diverse adverse environments, fine-grained\nannotations that support multi-step reasoning, and a dedicated evaluation\nframework tailored for assessing CoT performance. The core contribution of\nAD^2-Bench is its extensive collection of over 5.4k high-quality, manually\nannotated CoT instances. Each intermediate reasoning step in these annotations\nis treated as an atomic unit with explicit ground truth, enabling unprecedented\nfine-grained analysis of MLLMs' inferential processes under text-level,\npoint-level, and region-level visual prompts. Our comprehensive evaluation of\nstate-of-the-art MLLMs on AD^2-Bench reveals accuracy below 60%, highlighting\nthe benchmark's difficulty and the need to advance robust, interpretable\nend-to-end autonomous driving systems. AD^2-Bench thus provides a standardized\nevaluation platform, driving research forward by improving MLLMs' reasoning in\nautonomous driving, making it an invaluable resource.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09557v1", "AI": {"title_translation": "AD^2-Bench：恶劣条件下自动驾驶中MLLM的分层CoT基准", "tldr": "引入AD^2-Bench，首个专为恶劣天气和复杂场景下自动驾驶MLLM的CoT评估基准，揭示当前SOTA MLLM在该任务上表现不佳，推动MLLM在自动驾驶推理能力的研究。", "motivation": "现有基准普遍忽视了在恶劣天气和复杂交通环境下对多模态大型模型（MLLMs）的思维链（CoT）推理过程进行严格评估的需求，而这对于自动驾驶至关重要。", "method": "本文引入了AD^2-Bench，这是第一个专门为恶劣天气和复杂场景下自动驾驶设计的思维链（CoT）基准。该基准通过精心构建，实现了全面的数据覆盖、支持多步骤推理的细粒度标注，并提供了专门的CoT性能评估框架。其核心贡献在于收集了超过5.4k个高质量、手动标注的CoT实例，其中每个中间推理步骤都被视为一个具有明确真实值的原子单元，从而能够对MLLM在文本级、点级和区域级视觉提示下的推理过程进行细粒度分析。", "result": "对最先进的MLLMs在AD^2-Bench上的综合评估显示，其准确率低于60%。这突显了该基准的难度，并表明当前模型在恶劣条件下的自动驾驶CoT推理能力存在显著不足，亟需推进鲁棒、可解释的端到端自动驾驶系统。", "conclusion": "AD^2-Bench提供了一个标准化的评估平台，通过改进MLLMs在自动驾驶中的推理能力来推动相关研究进展，使其成为一个宝贵的资源。", "translation": "思维链（CoT）推理已成为一种强大的方法，用于增强多模态大型模型（MLLM）的结构化、多步骤决策能力，这对于恶劣天气条件和复杂交通环境下的自动驾驶尤其关键。然而，现有基准在很大程度上忽视了在这些特定且具有挑战性的场景中对CoT过程进行严格评估的需求。为了解决这一关键差距，我们引入了AD^2-Bench，这是第一个专门为恶劣天气和复杂场景下的自动驾驶设计的思维链基准。AD^2-Bench经过精心构建，以满足三个关键标准：在各种恶劣环境中实现全面的数据覆盖、支持多步骤推理的细粒度标注，以及为评估CoT性能量身定制的专用评估框架。AD^2-Bench的核心贡献在于其广泛收集了超过5.4k个高质量、手动标注的CoT实例。这些标注中的每个中间推理步骤都被视为一个具有明确真实值的原子单元，从而实现了对MLLM在文本级、点级和区域级视觉提示下推理过程的空前细粒度分析。我们对AD^2-Bench上最先进MLLM的综合评估显示，准确率低于60%，这突显了该基准的难度以及推进鲁棒、可解释的端到端自动驾驶系统的必要性。因此，AD^2-Bench提供了一个标准化的评估平台，通过改进MLLM在自动驾驶中的推理能力来推动研究进展，使其成为一个宝贵的资源。", "summary": "本文介绍了AD^2-Bench，这是首个专为恶劣天气和复杂场景下自动驾驶中的多模态大型模型（MLLM）思维链（CoT）推理能力评估设计的基准。该基准包含超过5.4k个高质量、细粒度标注的CoT实例，支持多层次视觉提示下的推理分析。在AD^2-Bench上对当前SOTA MLLM的评估显示其准确率低于60%，揭示了现有模型在复杂自动驾驶场景中CoT推理能力的不足，并强调了开发更鲁棒、可解释的自动驾驶系统的必要性。AD^2-Bench旨在成为推动MLLM在自动驾驶领域推理能力研究的重要资源。", "keywords": "思维链, 多模态大型模型, 自动驾驶, 基准, 恶劣条件", "comments": "AD^2-Bench的创新之处在于其首次专门针对自动驾驶在恶劣条件下的CoT推理能力进行评估，填补了现有基准的空白。其细粒度的标注和对中间推理步骤的明确真实值处理，为深入分析MLLM的推理过程提供了独特的视角。该基准揭示了当前SOTA MLLM在该任务上的显著不足，对于推动未来鲁棒、可解释的自动驾驶系统研究具有重要意义。"}}
{"id": "2506.09565", "title": "SemanticSplat: Feed-Forward 3D Scene Understanding with Language-Aware Gaussian Fields", "authors": ["Qijing Li", "Jingxiang Sun", "Liang An", "Zhaoqi Su", "Hongwen Zhang", "Yebin Liu"], "summary": "Holistic 3D scene understanding, which jointly models geometry, appearance,\nand semantics, is crucial for applications like augmented reality and robotic\ninteraction. Existing feed-forward 3D scene understanding methods (e.g., LSM)\nare limited to extracting language-based semantics from scenes, failing to\nachieve holistic scene comprehension. Additionally, they suffer from\nlow-quality geometry reconstruction and noisy artifacts. In contrast, per-scene\noptimization methods rely on dense input views, which reduces practicality and\nincreases complexity during deployment. In this paper, we propose\nSemanticSplat, a feed-forward semantic-aware 3D reconstruction method, which\nunifies 3D Gaussians with latent semantic attributes for joint\ngeometry-appearance-semantics modeling. To predict the semantic anisotropic\nGaussians, SemanticSplat fuses diverse feature fields (e.g., LSeg, SAM) with a\ncost volume representation that stores cross-view feature similarities,\nenhancing coherent and accurate scene comprehension. Leveraging a two-stage\ndistillation framework, SemanticSplat reconstructs a holistic multi-modal\nsemantic feature field from sparse-view images. Experiments demonstrate the\neffectiveness of our method for 3D scene understanding tasks like promptable\nand open-vocabulary segmentation. Video results are available at\nhttps://semanticsplat.github.io.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09565v1", "AI": {"title_translation": "SemanticSplat: 具有语言感知高斯场的正向3D场景理解", "tldr": "SemanticSplat提出了一种前馈的语义感知3D重建方法，它将3D高斯与潜在语义属性统一起来，以实现几何、外观和语义的联合建模，并在稀疏视图图像上表现出有效性。", "motivation": "现有的前馈3D场景理解方法无法实现整体场景理解，且几何重建质量低、存在噪声伪影；而逐场景优化方法依赖密集输入视图，降低了实用性和部署复杂性。", "method": "论文提出了SemanticSplat，一种前馈的语义感知3D重建方法。它通过将3D高斯与潜在语义属性相结合，实现了几何、外观和语义的联合建模。为了预测语义各向异性高斯，SemanticSplat将多种特征场（例如LSeg、SAM）与存储跨视图特征相似性的代价体表示融合。此外，它利用两阶段蒸馏框架，从稀疏视图图像重建整体多模态语义特征场。", "result": "实验证明了该方法在可提示和开放词汇分割等3D场景理解任务中的有效性。", "conclusion": "SemanticSplat通过统一3D高斯与潜在语义属性，并融合多源特征，有效解决了现有方法的局限性，实现了从稀疏视图图像进行整体3D场景理解。", "translation": "整体3D场景理解，即联合建模几何、外观和语义，对于增强现实和机器人交互等应用至关重要。现有前馈3D场景理解方法（例如LSM）仅限于从场景中提取基于语言的语义，未能实现整体场景理解。此外，它们还存在几何重建质量低和噪声伪影的问题。相比之下，逐场景优化方法依赖于密集的输入视图，这降低了实用性并增加了部署期间的复杂性。在本文中，我们提出了SemanticSplat，一种前馈的语义感知3D重建方法，它将3D高斯与潜在语义属性统一起来，以实现几何-外观-语义的联合建模。为了预测语义各向异性高斯，SemanticSplat将多种特征场（例如LSeg、SAM）与存储跨视图特征相似性的代价体表示融合，从而增强连贯和准确的场景理解。SemanticSplat利用两阶段蒸馏框架，从稀疏视图图像重建整体多模态语义特征场。实验证明了我们方法在可提示和开放词汇分割等3D场景理解任务中的有效性。视频结果可在 https://semanticsplat.github.io 获取。", "summary": "SemanticSplat是一种新颖的前馈3D场景理解方法，旨在解决现有技术在整体语义理解、几何重建质量和部署实用性方面的不足。它通过将3D高斯与潜在语义属性相结合，并融合来自不同特征源（如LSeg, SAM）的信息，从稀疏视图图像中重建出统一的几何、外观和语义模型。该方法在可提示和开放词汇分割等3D场景理解任务中表现出显著效果。", "keywords": "3D场景理解, 高斯场, 语义重建, 稀疏视图, 前馈网络", "comments": "该论文的创新点在于将3D高斯表示与语义属性相结合，并利用多源特征融合和两阶段蒸馏框架，实现了从稀疏视图图像进行整体、高质量的3D场景理解。这解决了现有前馈方法语义理解不完整和几何重建质量差的问题，同时也避免了逐场景优化方法对密集视图的依赖，提高了实用性。其在开放词汇分割等任务上的表现预示着在AR和机器人等领域具有广阔的应用前景。"}}
{"id": "2506.09451", "title": "Safe Screening Rules for Group SLOPE", "authors": ["Runxue Bao", "Quanchao Lu", "Yanfu Zhang"], "summary": "Variable selection is a challenging problem in high-dimensional sparse\nlearning, especially when group structures exist. Group SLOPE performs well for\nthe adaptive selection of groups of predictors. However, the block\nnon-separable group effects in Group SLOPE make existing methods either invalid\nor inefficient. Consequently, Group SLOPE tends to incur significant\ncomputational costs and memory usage in practical high-dimensional scenarios.\nTo overcome this issue, we introduce a safe screening rule tailored for the\nGroup SLOPE model, which efficiently identifies inactive groups with zero\ncoefficients by addressing the block non-separable group effects. By excluding\nthese inactive groups during training, we achieve considerable gains in\ncomputational efficiency and memory usage. Importantly, the proposed screening\nrule can be seamlessly integrated into existing solvers for both batch and\nstochastic algorithms. Theoretically, we establish that our screening rule can\nbe safely employed with existing optimization algorithms, ensuring the same\nresults as the original approaches. Experimental results confirm that our\nmethod effectively detects inactive feature groups and significantly boosts\ncomputational efficiency without compromising accuracy.", "comment": "Accepted by ECML PKDD 2025", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09451v1", "AI": {"title_translation": "Group SLOPE的安全筛选规则", "tldr": "本文提出了一种针对Group SLOPE模型的安全筛选规则，以高效识别非活跃组，从而显著降低高维场景下的计算成本和内存使用，且不影响准确性。", "motivation": "在高维稀疏学习中，变量选择是一个具有挑战性的问题，尤其是在存在组结构的情况下。Group SLOPE在自适应选择预测变量组方面表现良好，但其块不可分离的组效应导致现有方法无效或效率低下，从而在高维场景中产生显著的计算成本和内存使用。", "method": "本文引入了一种专为Group SLOPE模型设计的安全筛选规则，通过处理块不可分离的组效应，高效识别系数为零的非活跃组。该规则可无缝集成到现有求解器中，适用于批量和随机算法。", "result": "通过在训练过程中排除这些非活跃组，该方法在计算效率和内存使用方面取得了显著提升。实验结果证实，该方法能有效检测非活跃特征组，并在不影响准确性的情况下显著提高计算效率。", "conclusion": "针对Group SLOPE提出的安全筛选规则可以安全地应用于现有优化算法，确保与原始方法获得相同的结果，同时显著提高效率。", "translation": "在高维稀疏学习中，变量选择是一个具有挑战性的问题，尤其是在存在组结构的情况下。Group SLOPE在预测变量组的自适应选择方面表现良好。然而，Group SLOPE中块不可分离的组效应使得现有方法要么无效，要么效率低下。因此，在实际高维场景中，Group SLOPE往往会产生显著的计算成本和内存使用。为了克服这个问题，我们引入了一种专为Group SLOPE模型设计的安全筛选规则，通过处理块不可分离的组效应，高效识别系数为零的非活跃组。通过在训练过程中排除这些非活跃组，我们在计算效率和内存使用方面取得了显著的提升。重要的是，所提出的筛选规则可以无缝集成到现有求解器中，适用于批量和随机算法。理论上，我们确定我们的筛选规则可以安全地与现有优化算法一起使用，确保与原始方法获得相同的结果。实验结果证实，我们的方法能有效检测非活跃特征组，并在不影响准确性的情况下显著提高计算效率。", "summary": "Group SLOPE在处理具有组结构的变量选择问题时表现良好，但其块不可分离的组效应导致计算成本高昂。本文提出了一种针对Group SLOPE的安全筛选规则，能够高效识别并排除非活跃组，从而在不牺牲准确性的前提下显著提升计算效率和内存使用。该规则在理论上是可靠的，并且可以与现有求解器兼容。", "keywords": "Group SLOPE, 安全筛选, 变量选择, 高维, 计算效率", "comments": "本文的创新之处在于解决了Group SLOPE中“块不可分离的组效应”这一具体挑战，该问题是其实际应用中的瓶颈。所提出的安全筛选规则提供了一种通用的解决方案，可以与现有算法集成，使Group SLOPE在高维数据处理中更具可扩展性。"}}
{"id": "2506.09612", "title": "Consistent Story Generation with Asymmetry Zigzag Sampling", "authors": ["Mingxiao LI", "mang ning", "Marie-Francine Moens"], "summary": "Text-to-image generation models have made significant progress in producing\nhigh-quality images from textual descriptions, yet they continue to struggle\nwith maintaining subject consistency across multiple images, a fundamental\nrequirement for visual storytelling. Existing methods attempt to address this\nby either fine-tuning models on large-scale story visualization datasets, which\nis resource-intensive, or by using training-free techniques that share\ninformation across generations, which still yield limited success. In this\npaper, we introduce a novel training-free sampling strategy called Zigzag\nSampling with Asymmetric Prompts and Visual Sharing to enhance subject\nconsistency in visual story generation. Our approach proposes a zigzag sampling\nmechanism that alternates between asymmetric prompting to retain subject\ncharacteristics, while a visual sharing module transfers visual cues across\ngenerated images to %further enforce consistency. Experimental results, based\non both quantitative metrics and qualitative evaluations, demonstrate that our\nmethod significantly outperforms previous approaches in generating coherent and\nconsistent visual stories. The code is available at\nhttps://github.com/Mingxiao-Li/Asymmetry-Zigzag-StoryDiffusion.", "comment": "17 pages, 9. figures", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09612v1", "AI": {"title_translation": "使用非对称之字形采样的连贯故事生成", "tldr": "提出了一种名为非对称之字形采样的无训练方法，通过交替非对称提示和视觉共享来显著提高视觉故事生成中的主体一致性。", "motivation": "文本到图像生成模型在生成高质量图像方面取得了显著进展，但在跨多图像保持主体一致性方面仍存在困难，这是视觉故事叙述的基本要求。现有方法要么资源密集，要么效果有限。", "method": "引入了一种名为非对称提示和视觉共享之字形采样（Zigzag Sampling with Asymmetric Prompts and Visual Sharing）的新型无训练采样策略。该方法提出了一种之字形采样机制，它在非对称提示之间交替以保留主体特征，同时视觉共享模块在生成的图像之间传输视觉线索以进一步加强一致性。", "result": "实验结果（基于定量指标和定性评估）表明，该方法在生成连贯和一致的视觉故事方面显著优于以前的方法。", "conclusion": "所提出的非对称之字形采样方法能够有效解决视觉故事生成中的主体一致性问题，并显著提高生成质量。", "translation": "文本到图像生成模型在从文本描述生成高质量图像方面取得了显著进展，但它们在跨多图像保持主体一致性方面仍然存在困难，而这是视觉故事叙述的基本要求。现有方法试图通过在大型故事可视化数据集上微调模型来解决这个问题（这需要大量资源），或者使用在代际之间共享信息的无训练技术（但成功有限）。在本文中，我们引入了一种新颖的无训练采样策略，称为非对称提示和视觉共享之字形采样（Zigzag Sampling with Asymmetric Prompts and Visual Sharing），以增强视觉故事生成中的主体一致性。我们的方法提出了一种之字形采样机制，它在非对称提示之间交替以保留主体特征，同时视觉共享模块在生成的图像之间传输视觉线索以进一步加强一致性。基于定量指标和定性评估的实验结果表明，我们的方法在生成连贯和一致的视觉故事方面显著优于以前的方法。代码可在 https://github.com/Mingxiao-Li/Asymmetry-Zigzag-StoryDiffusion 获取。", "summary": "该论文提出了一种名为“非对称提示和视觉共享之字形采样”的新型无训练策略，旨在解决文本到图像生成模型在视觉故事叙述中主体一致性差的问题。该方法采用之字形采样机制，结合非对称提示以保留主体特征，并通过视觉共享模块在图像间传递视觉线索。实验结果表明，该方法在生成连贯一致的视觉故事方面显著优于现有方法。", "keywords": "视觉故事生成, 主体一致性, 之字形采样, 非对称提示, 视觉共享", "comments": "这项工作通过提出一种无训练的采样策略，有效地解决了视觉故事生成中主体一致性这一关键挑战。其创新之处在于结合了非对称提示和视觉共享，以一种巧妙的方式在不进行模型微调的情况下实现了显著改进，降低了资源需求。这对于推动视觉叙事领域的发展具有重要意义。"}}
{"id": "2506.09626", "title": "ECAM: A Contrastive Learning Approach to Avoid Environmental Collision in Trajectory Forecasting", "authors": ["Giacomo Rosin", "Muhammad Rameez Ur Rahman", "Sebastiano Vascon"], "summary": "Human trajectory forecasting is crucial in applications such as autonomous\ndriving, robotics and surveillance. Accurate forecasting requires models to\nconsider various factors, including social interactions, multi-modal\npredictions, pedestrian intention and environmental context. While existing\nmethods account for these factors, they often overlook the impact of the\nenvironment, which leads to collisions with obstacles. This paper introduces\nECAM (Environmental Collision Avoidance Module), a contrastive learning-based\nmodule to enhance collision avoidance ability with the environment. The\nproposed module can be integrated into existing trajectory forecasting models,\nimproving their ability to generate collision-free predictions. We evaluate our\nmethod on the ETH/UCY dataset and quantitatively and qualitatively demonstrate\nits collision avoidance capabilities. Our experiments show that\nstate-of-the-art methods significantly reduce (-40/50%) the collision rate when\nintegrated with the proposed module. The code is available at\nhttps://github.com/CVML-CFU/ECAM.", "comment": "IJCNN 2025", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09626v1", "AI": {"title_translation": "ECAM：一种基于对比学习的轨迹预测中环境碰撞避免方法", "tldr": "论文提出ECAM模块，利用对比学习减少轨迹预测中的环境碰撞，可集成到现有模型并显著降低碰撞率。", "motivation": "现有的轨迹预测模型在考虑社会交互、多模态预测和行人意图等因素时，常忽略环境影响，导致预测结果与障碍物碰撞。", "method": "本文引入了ECAM（环境碰撞避免模块），这是一个基于对比学习的模块，旨在增强模型避免与环境发生碰撞的能力。该模块可集成到现有轨迹预测模型中。", "result": "在ETH/UCY数据集上的评估显示，与ECAM模块集成后，最先进的轨迹预测方法的碰撞率显著降低了40%到50%。", "conclusion": "ECAM模块通过引入对比学习有效提升了轨迹预测模型避免环境碰撞的能力，使其能够生成更准确、更安全的无碰撞轨迹预测。", "translation": "人类轨迹预测在自动驾驶、机器人和监控等应用中至关重要。准确的预测要求模型考虑各种因素，包括社会交互、多模态预测、行人意图和环境上下文。尽管现有方法考虑了这些因素，但它们往往忽略了环境的影响，这导致与障碍物发生碰撞。本文介绍了ECAM（环境碰撞避免模块），一个基于对比学习的模块，用于增强与环境的碰撞避免能力。所提出的模块可以集成到现有的轨迹预测模型中，提高它们生成无碰撞预测的能力。我们在ETH/UCY数据集上评估了我们的方法，并定量和定性地展示了其碰撞避免能力。我们的实验表明，当与所提出的模块集成时，最先进方法的碰撞率显著降低（-40%/50%）。代码可在https://github.com/CVML-CFU/ECAM 获取。", "summary": "本文针对现有轨迹预测模型在处理环境上下文时容易导致与障碍物碰撞的问题，提出了一种名为ECAM（环境碰撞避免模块）的新方法。ECAM是一个基于对比学习的模块，旨在增强轨迹预测模型避免环境碰撞的能力，并且可以轻松集成到现有的预测模型中。通过在ETH/UCY数据集上的实验验证，ECAM能够显著降低最先进方法的碰撞率，从而生成更安全的无碰撞轨迹预测。", "keywords": "轨迹预测, 对比学习, 环境碰撞避免, 碰撞率, 自动驾驶", "comments": "该论文的创新点在于将对比学习引入轨迹预测领域，专门解决环境碰撞这一关键问题。ECAM模块的通用性和可插拔性是其重要优势，使其能够方便地集成到现有模型中，显著提升其安全性。这对于自动驾驶和机器人等实际应用具有重要的实用价值。"}}
{"id": "2506.09454", "title": "NDCG-Consistent Softmax Approximation with Accelerated Convergence", "authors": ["Yuanhao Pu", "Defu Lian", "Xiaolong Chen", "Xu Huang", "Jin Chen", "Enhong Chen"], "summary": "Ranking tasks constitute fundamental components of extreme similarity\nlearning frameworks, where extremely large corpora of objects are modeled\nthrough relative similarity relationships adhering to predefined ordinal\nstructures. Among various ranking surrogates, Softmax (SM) Loss has been widely\nadopted due to its natural capability to handle listwise ranking via global\nnegative comparisons, along with its flexibility across diverse application\nscenarios. However, despite its effectiveness, SM Loss often suffers from\nsignificant computational overhead and scalability limitations when applied to\nlarge-scale object spaces. To address this challenge, we propose novel loss\nformulations that align directly with ranking metrics: the\nRanking-Generalizable \\textbf{squared} (RG$^2$) Loss and the\nRanking-Generalizable interactive (RG$^\\times$) Loss, both derived through\nTaylor expansions of the SM Loss. Notably, RG$^2$ reveals the intrinsic\nmechanisms underlying weighted squared losses (WSL) in ranking methods and\nuncovers fundamental connections between sampling-based and non-sampling-based\nloss paradigms. Furthermore, we integrate the proposed RG losses with the\nhighly efficient Alternating Least Squares (ALS) optimization method, providing\nboth generalization guarantees and convergence rate analyses. Empirical\nevaluations on real-world datasets demonstrate that our approach achieves\ncomparable or superior ranking performance relative to SM Loss, while\nsignificantly accelerating convergence. This framework offers the similarity\nlearning community both theoretical insights and practically efficient tools,\nwith methodologies applicable to a broad range of tasks where balancing ranking\nquality and computational efficiency is essential.", "comment": "35 pages", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09454v1", "AI": {"title_translation": "NDCG一致的Softmax近似与加速收敛", "tldr": "本文提出了两种新的损失函数（RG^2和RG^x），它们是Softmax损失的泰勒展开式，旨在解决大规模排名任务中Softmax损失的计算开销和可扩展性问题。结合ALS优化方法，新方法在保持或提高排名性能的同时，显著加速了收敛。", "motivation": "排名任务是极端相似性学习框架的基础组成部分，而Softmax损失因其处理列表式排名的能力而被广泛采用。然而，当应用于大规模对象空间时，Softmax损失常常面临显著的计算开销和可扩展性限制。", "method": "作者提出了两种新的损失函数：排名通用平方（RG^2）损失和排名通用交互（RG^x）损失，它们都是通过Softmax损失的泰勒展开式导出的。RG^2揭示了加权平方损失的内在机制，并揭示了基于采样和非基于采样损失范式之间的基本联系。此外，作者将所提出的RG损失与高效的交替最小二乘法（ALS）优化方法相结合，并提供了泛化保证和收敛速度分析。", "result": "在真实世界数据集上的实证评估表明，所提出的方法相对于Softmax损失，实现了可比或更优的排名性能，同时显著加速了收敛。", "conclusion": "该框架为相似性学习社区提供了理论见解和实用的高效工具，其方法适用于平衡排名质量和计算效率至关重要的大范围任务。", "translation": "排名任务是极端相似性学习框架的基本组成部分，其中通过遵循预定义序数结构的相对相似性关系对极其庞大的对象语料库进行建模。在各种排名替代方法中，Softmax（SM）损失因其通过全局负比较处理列表式排名的自然能力以及在各种应用场景中的灵活性而得到广泛采用。然而，尽管其有效，当应用于大规模对象空间时，SM损失常常面临显著的计算开销和可扩展性限制。为了解决这一挑战，我们提出了直接与排名指标对齐的新型损失公式：排名通用平方（RG^2）损失和排名通用交互（RG^x）损失，两者均通过SM损失的泰勒展开式导出。值得注意的是，RG^2揭示了排名方法中加权平方损失（WSL）的内在机制，并揭示了基于采样和非基于采样损失范式之间的基本联系。此外，我们将所提出的RG损失与高效的交替最小二乘法（ALS）优化方法相结合，提供了泛化保证和收敛速度分析。在真实世界数据集上的实证评估表明，我们的方法相对于SM损失实现了可比或更优的排名性能，同时显著加速了收敛。该框架为相似性学习社区提供了理论见解和实用的高效工具，其方法适用于平衡排名质量和计算效率至关重要的大范围任务。", "summary": "本文针对大规模排名任务中Softmax损失的计算效率和可扩展性问题，提出了两种新的损失函数：排名通用平方（RG^2）损失和排名通用交互（RG^x）损失。这些损失函数通过Softmax损失的泰勒展开式导出，并与交替最小二乘法（ALS）优化方法相结合。实验结果表明，与Softmax损失相比，新方法在保持或提升排名性能的同时，显著加快了收敛速度，为相似性学习提供了理论和实践上的高效工具。", "keywords": "排名任务, Softmax损失, 泰勒展开, 交替最小二乘法, 收敛加速", "comments": "这篇论文通过对Softmax损失进行泰勒展开，创新性地提出了两种新的损失函数，并揭示了加权平方损失的内在机制，连接了不同的损失范式。通过与高效的ALS优化结合，有效解决了大规模排名任务中的计算效率瓶颈，具有重要的理论和实践意义。"}}
{"id": "2506.09634", "title": "HSENet: Hybrid Spatial Encoding Network for 3D Medical Vision-Language Understanding", "authors": ["Yanzhao Shi", "Xiaodan Zhang", "Junzhong Ji", "Haoning Jiang", "Chengxin Zheng", "Yinong Wang", "Liangqiong Qu"], "summary": "Automated 3D CT diagnosis empowers clinicians to make timely, evidence-based\ndecisions by enhancing diagnostic accuracy and workflow efficiency. While\nmultimodal large language models (MLLMs) exhibit promising performance in\nvisual-language understanding, existing methods mainly focus on 2D medical\nimages, which fundamentally limits their ability to capture complex 3D\nanatomical structures. This limitation often leads to misinterpretation of\nsubtle pathologies and causes diagnostic hallucinations. In this paper, we\npresent Hybrid Spatial Encoding Network (HSENet), a framework that exploits\nenriched 3D medical visual cues by effective visual perception and projection\nfor accurate and robust vision-language understanding. Specifically, HSENet\nemploys dual-3D vision encoders to perceive both global volumetric contexts and\nfine-grained anatomical details, which are pre-trained by dual-stage alignment\nwith diagnostic reports. Furthermore, we propose Spatial Packer, an efficient\nmultimodal projector that condenses high-resolution 3D spatial regions into a\ncompact set of informative visual tokens via centroid-based compression. By\nassigning spatial packers with dual-3D vision encoders, HSENet can seamlessly\nperceive and transfer hybrid visual representations to LLM's semantic space,\nfacilitating accurate diagnostic text generation. Experimental results\ndemonstrate that our method achieves state-of-the-art performance in 3D\nlanguage-visual retrieval (39.85% of R@100, +5.96% gain), 3D medical report\ngeneration (24.01% of BLEU-4, +8.01% gain), and 3D visual question answering\n(73.60% of Major Class Accuracy, +1.99% gain), confirming its effectiveness.\nOur code is available at https://github.com/YanzhaoShi/HSENet.", "comment": "27 pages, 9 figures. arXiv admin note: text overlap with\n  arXiv:2410.14200 by other authors", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09634v1", "AI": {"title_translation": "HSENet: 混合空间编码网络用于3D医学视觉-语言理解", "tldr": "HSENet是一个为3D医学视觉-语言理解设计的框架，通过双3D视觉编码器和空间打包器有效处理3D医学图像，解决了现有方法在2D图像上的局限性，并在多项任务中实现了最先进的性能。", "motivation": "现有多模态大型语言模型（MLLMs）主要关注2D医学图像，这限制了它们捕捉复杂3D解剖结构的能力，导致对细微病理的误解和诊断幻觉。", "method": "本文提出了混合空间编码网络（HSENet），一个利用丰富的3D医学视觉线索的框架。HSENet采用双3D视觉编码器来感知全局体积上下文和细粒度解剖细节，这些编码器通过与诊断报告的双阶段对齐进行预训练。此外，提出了一种高效的多模态投影器——空间打包器（Spatial Packer），它通过基于质心的压缩将高分辨率3D空间区域浓缩为紧凑的信息视觉token集合。通过将空间打包器与双3D视觉编码器结合，HSENet能够无缝地感知并将混合视觉表示传输到LLM的语义空间，从而促进准确的诊断文本生成。", "result": "HSENet在3D语言-视觉检索任务中达到了39.85%的R@100（+5.96%增益），在3D医学报告生成任务中达到了24.01%的BLEU-4（+8.01%增益），在3D视觉问答任务中达到了73.60%的主要类别准确率（+1.99%增益），证实了其有效性并实现了最先进的性能。", "conclusion": "HSENet通过有效利用3D医学视觉线索，显著提高了3D医学视觉-语言理解的准确性和鲁棒性，并在3D语言-视觉检索、医学报告生成和视觉问答等多个任务上取得了最先进的性能。", "translation": "自动化3D CT诊断通过提高诊断准确性和工作流程效率，使临床医生能够做出及时、基于证据的决策。尽管多模态大型语言模型（MLLMs）在视觉-语言理解方面表现出有前景的性能，但现有方法主要关注2D医学图像，这从根本上限制了它们捕捉复杂3D解剖结构的能力。这种限制常常导致对细微病理的误解并引起诊断幻觉。在本文中，我们提出了混合空间编码网络（HSENet），这是一个通过有效的视觉感知和投影来利用丰富的3D医学视觉线索的框架，以实现准确和鲁棒的视觉-语言理解。具体而言，HSENet采用双3D视觉编码器来感知全局体积上下文和细粒度解剖细节，这些编码器通过与诊断报告的双阶段对齐进行预训练。此外，我们提出了空间打包器，这是一种高效的多模态投影器，通过基于质心的压缩将高分辨率3D空间区域浓缩成一组紧凑的信息性视觉标记。通过为双3D视觉编码器分配空间打包器，HSENet可以无缝地感知并将混合视觉表示传输到LLM的语义空间，从而促进准确的诊断文本生成。实验结果表明，我们的方法在3D语言-视觉检索（R@100 39.85%，+5.96% 增益）、3D医学报告生成（BLEU-4 24.01%，+8.01% 增益）和3D视觉问答（主要类别准确率 73.60%，+1.99% 增益）方面取得了最先进的性能，证实了其有效性。我们的代码可在https://github.com/YanzhaoShi/HSENet获取。", "summary": "HSENet是一个针对3D医学视觉-语言理解的创新框架，旨在克服现有MLLM在处理3D医学图像时的局限性。它通过结合双3D视觉编码器来捕获全局和细粒度3D信息，并引入Spatial Packer将高分辨率3D区域高效压缩为视觉token，从而实现将混合视觉表示无缝传输到LLM的语义空间。实验证明，HSENet在3D语言-视觉检索、报告生成和视觉问答等任务上均取得了SOTA性能，显著提升了3D医学诊断的准确性和效率。", "keywords": "3D医学图像, 视觉-语言理解, HSENet, 空间编码, 多模态大型语言模型", "comments": "HSENet的创新之处在于其专门针对3D医学图像设计，通过双3D视觉编码器和Spatial Packer有效地解决了现有方法在处理复杂3D结构时的局限性。这对于提高自动化3D CT诊断的准确性至关重要，有望减少误诊并提高临床工作效率。其模块化设计和在多个任务上的显著性能提升，表明其在3D医学视觉-语言理解领域具有重要应用前景。"}}
{"id": "2506.09477", "title": "On a few pitfalls in KL divergence gradient estimation for RL", "authors": ["Yunhao Tang", "Rémi Munos"], "summary": "We point out a few pitfalls in implementing gradient estimation for KL\ndivergence in RL training for LLM, as seen in a number of open source projects\nand papers. The first major pitfall is to differentiate through the KL estimate\nas loss functions to minimize KL divergence. We show that such implementations\nare generally incorrect and do not produce the desired KL gradient. Secondly,\nwe show that some implementations do not account for the sequential nature of\nthe estimation problem and produce a partial gradient at best. We demonstrate\nthe impact of such issues with illustrative tabular and LLM experiments, and\nshow the correct way to implement the KL gradient.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09477v1", "AI": {"title_translation": "关于RL中KL散度梯度估计的一些陷阱", "tldr": "本文指出了在RL训练LLM时KL散度梯度估计中常见的几个实现陷阱，并展示了正确的实现方法。", "motivation": "在RL训练大型语言模型（LLM）中，许多开源项目和论文在实现KL散度梯度估计时存在错误和陷阱，因此需要指出并纠正这些问题。", "method": "作者指出了两个主要陷阱：一是将KL估计作为损失函数进行微分以最小化KL散度，并证明这种实现通常是错误的，无法产生期望的KL梯度；二是某些实现未能考虑估计问题的顺序性质，导致产生部分梯度。作者通过表格和LLM实验演示了这些问题的影响，并展示了实现KL梯度的正确方法。", "result": "结果表明，将KL估计作为损失函数进行微分以最小化KL散度是错误的，无法产生期望的KL梯度；同时，一些实现未能考虑估计问题的顺序性质，导致产生部分梯度。作者通过实验演示了这些问题的影响，并提出了正确的KL梯度实现方法。", "conclusion": "本文识别并纠正了在RL训练LLM中KL散度梯度估计的常见错误实现，并提供了正确的指导，这对于提高RL算法在LLM应用中的准确性和效率至关重要。", "translation": "我们指出了在为大型语言模型（LLM）进行强化学习（RL）训练时，KL散度梯度估计实现中的几个陷阱，这些陷阱在许多开源项目和论文中都可见。第一个主要陷阱是将KL估计作为损失函数进行微分以最小化KL散度。我们表明，此类实现通常是不正确的，并且不会产生所需的KL梯度。其次，我们表明一些实现没有考虑到估计问题的顺序性质，最多只能产生部分梯度。我们通过说明性的表格和LLM实验演示了这些问题的影响，并展示了实现KL梯度的正确方法。", "summary": "本文揭示了强化学习（RL）训练大型语言模型（LLM）时，在KL散度梯度估计中存在的常见实现错误。主要陷阱包括错误地将KL估计作为损失函数进行微分，以及未能考虑估计的顺序性导致梯度不完整。作者通过实验验证了这些问题，并提供了正确的KL梯度实现方式，旨在帮助研究人员和开发者避免这些常见错误。", "keywords": "KL散度, 梯度估计, 强化学习, 大型语言模型, 陷阱", "comments": "该论文具有重要的实践意义，它明确指出了在RL领域，特别是在LLM训练中，KL散度梯度估计的常见且容易被忽视的实现错误。通过揭示这些陷阱并提供正确的实现方法，它有助于提高相关RL算法的稳定性和性能，避免不必要的研发时间和资源浪费。其创新之处在于系统地分析并纠正了这些广泛存在的误解。"}}
{"id": "2506.09644", "title": "DGAE: Diffusion-Guided Autoencoder for Efficient Latent Representation Learning", "authors": ["Dongxu Liu", "Yuang Peng", "Haomiao Tang", "Yuwei Chen", "Chunrui Han", "Zheng Ge", "Daxin Jiang", "Mingxue Liao"], "summary": "Autoencoders empower state-of-the-art image and video generative models by\ncompressing pixels into a latent space through visual tokenization. Although\nrecent advances have alleviated the performance degradation of autoencoders\nunder high compression ratios, addressing the training instability caused by\nGAN remains an open challenge. While improving spatial compression, we also aim\nto minimize the latent space dimensionality, enabling more efficient and\ncompact representations. To tackle these challenges, we focus on improving the\ndecoder's expressiveness. Concretely, we propose DGAE, which employs a\ndiffusion model to guide the decoder in recovering informative signals that are\nnot fully decoded from the latent representation. With this design, DGAE\neffectively mitigates the performance degradation under high spatial\ncompression rates. At the same time, DGAE achieves state-of-the-art performance\nwith a 2x smaller latent space. When integrated with Diffusion Models, DGAE\ndemonstrates competitive performance on image generation for ImageNet-1K and\nshows that this compact latent representation facilitates faster convergence of\nthe diffusion model.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09644v1", "AI": {"title_translation": "DGAE：扩散引导自编码器，用于高效潜在表示学习", "tldr": "DGAE提出了一种扩散引导的自编码器，通过扩散模型指导解码器恢复信息，解决了高压缩比下的性能下降和训练不稳定性问题，并实现了更小的潜在空间和更快的扩散模型收敛。", "motivation": "自编码器在高压缩比下性能下降，且由GAN引起的训练不稳定仍是开放挑战。同时，需要最小化潜在空间维度以实现更高效和紧凑的表示。", "method": "提出DGAE，它采用扩散模型来指导解码器恢复未从潜在表示中完全解码的信息信号，从而提高解码器的表达能力。", "result": "DGAE有效缓解了高空间压缩率下的性能下降，同时以2倍更小的潜在空间实现了最先进的性能。与扩散模型结合时，DGAE在ImageNet-1K上的图像生成方面表现出竞争力，并促进了扩散模型的更快收敛。", "conclusion": "DGAE通过引入扩散模型引导解码器，有效解决了自编码器在高压缩率下的性能问题和潜在空间效率问题，实现了紧凑且高效的潜在表示学习，并加速了扩散模型的收敛。", "translation": "自编码器通过视觉标记化将像素压缩到潜在空间，从而支持最先进的图像和视频生成模型。尽管最近的进展缓解了自编码器在高压缩比下的性能下降，但解决由GAN引起的训练不稳定性仍然是一个开放的挑战。在提高空间压缩的同时，我们还旨在最小化潜在空间维度，从而实现更高效和紧凑的表示。为了应对这些挑战，我们专注于提高解码器的表达能力。具体来说，我们提出了DGAE，它采用扩散模型来指导解码器恢复未从潜在表示中完全解码的信息信号。通过这种设计，DGAE有效缓解了高空间压缩率下的性能下降。同时，DGAE以2倍更小的潜在空间实现了最先进的性能。当与扩散模型集成时，DGAE在ImageNet-1K上的图像生成方面表现出竞争力，并表明这种紧凑的潜在表示有助于扩散模型更快地收敛。", "summary": "DGAE是一种新型自编码器，旨在解决高压缩比下性能下降和训练不稳定性问题。它通过引入扩散模型来指导解码器，使其能够恢复潜在表示中未完全解码的重要信息，从而提升解码器表达能力。实验证明，DGAE有效缓解了高空间压缩率下的性能损失，实现了更小的潜在空间（2倍），并与扩散模型结合时展现出卓越的图像生成性能和更快的收敛速度。", "keywords": "自编码器, 潜在表示学习, 扩散模型, 高压缩率, 图像生成", "comments": "DGAE的创新点在于将扩散模型引入自编码器的解码器引导机制，有效解决了传统自编码器在高压缩率下性能下降和训练不稳定性问题。其在实现更紧凑潜在空间的同时保持甚至超越SOTA性能，对高效图像生成和扩散模型训练具有重要意义。"}}
{"id": "2506.09251", "title": "Extrapolation by Association: Length Generalization Transfer in Transformers", "authors": ["Ziyang Cai", "Nayoung Lee", "Avi Schwarzschild", "Samet Oymak", "Dimitris Papailiopoulos"], "summary": "Transformer language models have demonstrated impressive generalization\ncapabilities in natural language domains, yet we lack a fine-grained\nunderstanding of how such generalization arises. In this paper, we investigate\nlength generalization--the ability to extrapolate from shorter to longer\ninputs--through the lens of \\textit{task association}. We find that length\ngeneralization can be \\textit{transferred} across related tasks. That is,\ntraining a model with a longer and related auxiliary task can lead it to\ngeneralize to unseen and longer inputs from some other target task. We\ndemonstrate this length generalization transfer across diverse algorithmic\ntasks, including arithmetic operations, string transformations, and maze\nnavigation. Our results show that transformer models can inherit generalization\ncapabilities from similar tasks when trained jointly. Moreover, we observe\nsimilar transfer effects in pretrained language models, suggesting that\npretraining equips models with reusable computational scaffolding that\nfacilitates extrapolation in downstream settings. Finally, we provide initial\nmechanistic evidence that length generalization transfer correlates with the\nre-use of the same attention heads between the tasks. Together, our findings\ndeepen our understanding of how transformers generalize to out-of-distribution\ninputs and highlight the compositional reuse of inductive structure across\ntasks.", "comment": "23 pages, 20 figures", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.09251v1", "AI": {"title_translation": "关联外推：Transformer中的长度泛化迁移", "tldr": "Transformer模型可以通过相关任务进行长度泛化迁移，即使在预训练模型中也存在这种现象，并且与注意力头的重用相关。", "motivation": "尽管Transformer语言模型在自然语言领域展现出强大的泛化能力，但我们对其泛化机制，特别是长度泛化（从较短输入推断到较长输入的能力）的产生方式缺乏细致的理解。", "method": "本文通过“任务关联”的视角，研究Transformer模型中的长度泛化。研究人员通过在较长且相关的辅助任务上训练模型，以观察其对来自其他目标任务的未见过的较长输入的泛化能力，并在包括算术运算、字符串转换和迷宫导航等多种算法任务中进行了验证。此外，他们还对预训练语言模型进行了观察，并提供了长度泛化迁移与任务间相同注意力头重用相关的初步机制证据。", "result": "研究发现，长度泛化可以在相关任务之间进行“迁移”。具体而言，在较长且相关的辅助任务上训练模型，可以使其泛化到某些其他目标任务中未见过的较长输入。Transformer模型在联合训练时可以从相似任务中继承泛化能力。在预训练语言模型中也观察到类似的迁移效应，这表明预训练为模型提供了可重用的计算支架，有助于下游设置中的外推。初步的机制证据表明，长度泛化迁移与任务之间相同注意力头的重用相关。", "conclusion": "研究结果加深了我们对Transformer如何泛化到分布外输入的理解，并强调了跨任务归纳结构的可组合重用。", "translation": "Transformer语言模型在自然语言领域展现出令人印象深刻的泛化能力，但我们对这种泛化如何产生缺乏细致的理解。在本文中，我们通过“任务关联”的视角研究了长度泛化——即从较短输入推断到较长输入的能力。我们发现长度泛化可以在相关任务之间“迁移”。也就是说，用一个较长且相关的辅助任务训练模型，可以使其泛化到来自其他目标任务的未见过的较长输入。我们在一系列不同的算法任务中展示了这种长度泛化迁移，包括算术运算、字符串转换和迷宫导航。我们的结果表明，Transformer模型在联合训练时可以从相似任务中继承泛化能力。此外，我们在预训练语言模型中观察到类似的迁移效应，这表明预训练为模型配备了可重用的计算支架，有助于下游设置中的外推。最后，我们提供了初步的机制证据，表明长度泛化迁移与任务之间相同注意力头的重用相关。总而言之，我们的发现加深了我们对Transformer如何泛化到分布外输入的理解，并强调了跨任务归纳结构的可组合重用。", "summary": "本研究探讨了Transformer模型中的长度泛化能力，即从较短输入推断到较长输入的能力，并提出这种能力可以通过“任务关联”在不同任务间迁移。研究发现，通过在较长且相关的辅助任务上进行训练，模型能够泛化到目标任务中未见过的较长输入。这种长度泛化迁移在多种算法任务中得到验证，并在预训练语言模型中也观察到类似效应，表明预训练提供了可重用的计算基础。此外，研究提供了初步的机制证据，表明长度泛化迁移与任务间注意力头的重用有关。这些发现增进了对Transformer分布外泛化机制的理解，并揭示了归纳结构在任务间的组合重用。", "keywords": "长度泛化, Transformer, 任务关联, 迁移学习, 注意力头", "comments": "这篇论文通过引入“任务关联”的概念，为理解Transformer的长度泛化提供了一个新颖且重要的视角。它不仅证明了长度泛化可以在相关任务之间迁移，而且通过对预训练模型和注意力头重用机制的探讨，为Transformer在分布外泛化方面的能力提供了更深层次的见解。这对于提高模型在实际应用中的鲁棒性和泛化性具有重要意义，尤其是在处理超出训练数据长度限制的输入时。"}}
{"id": "2506.09496", "title": "EnerBridge-DPO: Energy-Guided Protein Inverse Folding with Markov Bridges and Direct Preference Optimization", "authors": ["Dingyi Rong", "Haotian Lu", "Wenzhuo Zheng", "Fan Zhang", "Shuangjia Zheng", "Ning Liu"], "summary": "Designing protein sequences with optimal energetic stability is a key\nchallenge in protein inverse folding, as current deep learning methods are\nprimarily trained by maximizing sequence recovery rates, often neglecting the\nenergy of the generated sequences. This work aims to overcome this limitation\nby developing a model that directly generates low-energy, stable protein\nsequences. We propose EnerBridge-DPO, a novel inverse folding framework focused\non generating low-energy, high-stability protein sequences. Our core innovation\nlies in: First, integrating Markov Bridges with Direct Preference Optimization\n(DPO), where energy-based preferences are used to fine-tune the Markov Bridge\nmodel. The Markov Bridge initiates optimization from an information-rich prior\nsequence, providing DPO with a pool of structurally plausible sequence\ncandidates. Second, an explicit energy constraint loss is introduced, which\nenhances the energy-driven nature of DPO based on prior sequences, enabling the\nmodel to effectively learn energy representations from a wealth of prior\nknowledge and directly predict sequence energy values, thereby capturing\nquantitative features of the energy landscape. Our evaluations demonstrate that\nEnerBridge-DPO can design protein complex sequences with lower energy while\nmaintaining sequence recovery rates comparable to state-of-the-art models, and\naccurately predicts $\\Delta \\Delta G$ values between various sequences.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09496v1", "AI": {"title_translation": "EnerBridge-DPO：基于马尔可夫桥和直接偏好优化的能量引导蛋白质逆折叠", "tldr": "EnerBridge-DPO通过结合马尔可夫桥和直接偏好优化，以及引入能量约束损失，解决了蛋白质逆折叠中现有方法忽视能量稳定性的问题，能生成低能量的蛋白质序列并保持高序列恢复率。", "motivation": "现有的深度学习蛋白质逆折叠方法主要通过最大化序列恢复率进行训练，但往往忽略了生成序列的能量稳定性，导致难以设计出具有最佳能量稳定性的蛋白质序列。", "method": "提出了一种名为EnerBridge-DPO的新型逆折叠框架，旨在生成低能量、高稳定性的蛋白质序列。其核心创新点包括：1) 将马尔可夫桥与直接偏好优化(DPO)相结合，利用基于能量的偏好微调马尔可夫桥模型，并从信息丰富的先验序列启动优化，为DPO提供结构合理的序列候选。2) 引入显式能量约束损失，增强DPO的能量驱动特性，使其能从先验知识中有效学习能量表示并直接预测序列能量值。", "result": "EnerBridge-DPO能够设计出能量更低的蛋白质复合体序列，同时保持与现有SOTA模型相当的序列恢复率。此外，它还能准确预测不同序列之间的ΔΔG值。", "conclusion": "EnerBridge-DPO成功解决了蛋白质逆折叠中能量稳定性被忽视的问题，通过创新的能量引导方法，能够生成低能量、高稳定性的蛋白质序列，并准确预测能量变化。", "translation": "设计具有最佳能量稳定性的蛋白质序列是蛋白质逆折叠中的一个关键挑战，因为当前的深度学习方法主要通过最大化序列恢复率进行训练，常常忽略了生成序列的能量。这项工作旨在通过开发一个直接生成低能量、稳定蛋白质序列的模型来克服这一限制。我们提出了EnerBridge-DPO，一个专注于生成低能量、高稳定性蛋白质序列的新型逆折叠框架。我们的核心创新在于：首先，将马尔可夫桥与直接偏好优化（DPO）相结合，其中基于能量的偏好用于微调马尔可夫桥模型。马尔可夫桥从信息丰富的先验序列启动优化，为DPO提供了一组结构上合理的序列候选。其次，引入了显式能量约束损失，这增强了基于先验序列的DPO的能量驱动性质，使模型能够有效地从丰富的先验知识中学习能量表示并直接预测序列能量值，从而捕捉能量景观的定量特征。我们的评估表明，EnerBridge-DPO可以设计出能量更低的蛋白质复合体序列，同时保持与最先进模型相当的序列恢复率，并准确预测各种序列之间的ΔΔG值。", "summary": "EnerBridge-DPO是一种新型的蛋白质逆折叠框架，旨在克服现有深度学习方法在设计蛋白质序列时忽视能量稳定性的局限。它通过整合马尔可夫桥与直接偏好优化，并引入显式能量约束损失，实现了从信息丰富的先验序列出发，生成低能量、高稳定性蛋白质序列的目标。实验证明，EnerBridge-DPO在保持高序列恢复率的同时，能设计出能量更低的蛋白质复合体序列，并准确预测能量变化值。", "keywords": "蛋白质逆折叠, 能量引导, 马尔可夫桥, 直接偏好优化, 蛋白质设计", "comments": "这项工作通过将马尔可夫桥和直接偏好优化与能量约束相结合，为蛋白质逆折叠领域带来了重要创新。它解决了现有方法在能量稳定性方面的不足，有望设计出更接近自然状态或功能更优的蛋白质。引入能量约束损失和利用先验序列来引导优化，是提升模型在能量景观中学习能力的关键。"}}
{"id": "2506.09499", "title": "A Unified Theory of Compositionality, Modularity, and Interpretability in Markov Decision Processes", "authors": ["Thomas J. Ringstrom", "Paul R. Schrater"], "summary": "We introduce Option Kernel Bellman Equations (OKBEs) for a new reward-free\nMarkov Decision Process. Rather than a value function, OKBEs directly construct\nand optimize a predictive map called a state-time option kernel (STOK) to\nmaximize the probability of completing a goal while avoiding constraint\nviolations. STOKs are compositional, modular, and interpretable\ninitiation-to-termination transition kernels for policies in the Options\nFramework of Reinforcement Learning. This means: 1) STOKs can be composed using\nChapman-Kolmogorov equations to make spatiotemporal predictions for multiple\npolicies over long horizons, 2) high-dimensional STOKs can be represented and\ncomputed efficiently in a factorized and reconfigurable form, and 3) STOKs\nrecord the probabilities of semantically interpretable goal-success and\nconstraint-violation events, needed for formal verification. Given a\nhigh-dimensional state-transition model for an intractable planning problem, we\ncan decompose it with local STOKs and goal-conditioned policies that are\naggregated into a factorized goal kernel, making it possible to forward-plan at\nthe level of goals in high-dimensions to solve the problem. These properties\nlead to highly flexible agents that can rapidly synthesize meta-policies, reuse\nplanning representations across many tasks, and justify goals using\nempowerment, an intrinsic motivation function. We argue that\nreward-maximization is in conflict with the properties of compositionality,\nmodularity, and interpretability. Alternatively, OKBEs facilitate these\nproperties to support verifiable long-horizon planning and intrinsic motivation\nthat scales to dynamic high-dimensional world-models.", "comment": "12 Pages", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09499v1", "AI": {"title_translation": "马尔可夫决策过程中组合性、模块化和可解释性的统一理论", "tldr": "本文提出了一种新的无奖励马尔可夫决策过程的期权核贝尔曼方程（OKBEs），通过构建和优化状态-时间期权核（STOK）来解决高维规划问题，实现可组合、模块化和可解释的长期规划与内在动机。", "motivation": "传统的奖励最大化方法与组合性、模块化和可解释性等特性相冲突，且难以解决高维不可处理的规划问题。本文旨在提供一种新的理论框架来支持这些特性。", "method": "引入期权核贝尔曼方程（OKBEs）用于无奖励马尔可夫决策过程。OKBEs直接构建并优化一种称为状态-时间期权核（STOK）的预测图，以最大化完成目标的概率并避免违反约束。STOKs是可组合、模块化和可解释的，并且可以通过分解为局部STOKs和目标条件策略，聚合为分解式目标核，从而在高维空间中进行目标级别的正向规划。", "result": "STOKs具备组合性、模块化和可解释性。它们能够使用Chapman-Kolmogorov方程进行组合，对多策略进行长期时空预测；高维STOKs可以以分解和可重构的形式高效表示和计算；STOKs记录了语义上可解释的目标成功和约束违反事件的概率，支持形式化验证。这些特性使得智能体能够快速合成元策略，跨任务重用规划表示，并利用赋能（一种内在动机函数）来证明目标的合理性。", "conclusion": "OKBEs促进了组合性、模块化和可解释性，支持可验证的长期规划和内在动机，并能扩展到动态高维世界模型。", "translation": "我们引入了期权核贝尔曼方程（OKBEs），用于一种新的无奖励马尔可夫决策过程。OKBEs不使用价值函数，而是直接构建和优化一个称为状态-时间期权核（STOK）的预测图，以最大化完成目标的概率，同时避免违反约束。STOKs是强化学习中期权框架下策略的可组合、模块化和可解释的从起始到终止的转换核。这意味着：1）STOKs可以使用Chapman-Kolmogorov方程进行组合，对多个策略进行长期时空预测；2）高维STOKs可以以分解和可重构的形式高效表示和计算；3）STOKs记录了语义上可解释的目标成功和约束违反事件的概率，这对于形式化验证是必需的。给定一个难以处理的规划问题的高维状态-转换模型，我们可以用局部STOKs和目标条件策略对其进行分解，这些策略被聚合为一个分解式目标核，从而使得在高维空间中在目标层面进行正向规划成为可能，以解决问题。这些特性使得智能体具有高度的灵活性，能够快速合成元策略，在许多任务中重用规划表示，并利用赋能（一种内在动机函数）来证明目标的合理性。我们认为，奖励最大化与组合性、模块化和可解释性等特性相冲突。而OKBEs则促进了这些特性，以支持可验证的长期规划和内在动机，并可扩展到动态高维世界模型。", "summary": "本文提出期权核贝尔曼方程（OKBEs）及其核心组件状态-时间期权核（STOKs），用于无奖励马尔可夫决策过程。STOKs是可组合、模块化和可解释的预测图，能够高效处理高维规划问题，实现长期预测、形式化验证和元策略合成。该方法解决了传统奖励最大化在组合性、模块化和可解释性上的局限性，支持可扩展的长期规划和内在动机。", "keywords": "期权核贝尔曼方程, 马尔可夫决策过程, 组合性, 模块化, 可解释性", "comments": "该论文提出了一种新颖的框架，通过引入期权核贝尔曼方程（OKBEs）和状态-时间期权核（STOKs），解决了传统强化学习中奖励最大化与组合性、模块化和可解释性之间的冲突。其创新点在于从无奖励MDP的角度出发，直接优化预测图而非价值函数，并利用STOKs的分解和组合特性，显著提升了高维规划的效率和可解释性，为构建更灵活、可验证的智能体提供了新的途径。这对于复杂、动态环境中的长期决策和迁移学习具有重要意义。"}}
{"id": "2506.09663", "title": "Self-Supervised Multi-Part Articulated Objects Modeling via Deformable Gaussian Splatting and Progressive Primitive Segmentation", "authors": ["Haowen Wang", "Xiaoping Yuan", "Zhao Jin", "Zhen Zhao", "Zhengping Che", "Yousong Xue", "Jin Tian", "Yakun Huang", "Jian Tang"], "summary": "Articulated objects are ubiquitous in everyday life, and accurate 3D\nrepresentations of their geometry and motion are critical for numerous\napplications. However, in the absence of human annotation, existing approaches\nstill struggle to build a unified representation for objects that contain\nmultiple movable parts. We introduce DeGSS, a unified framework that encodes\narticulated objects as deformable 3D Gaussian fields, embedding geometry,\nappearance, and motion in one compact representation. Each interaction state is\nmodeled as a smooth deformation of a shared field, and the resulting\ndeformation trajectories guide a progressive coarse-to-fine part segmentation\nthat identifies distinct rigid components, all in an unsupervised manner. The\nrefined field provides a spatially continuous, fully decoupled description of\nevery part, supporting part-level reconstruction and precise modeling of their\nkinematic relationships. To evaluate generalization and realism, we enlarge the\nsynthetic PartNet-Mobility benchmark and release RS-Art, a real-to-sim dataset\nthat pairs RGB captures with accurately reverse-engineered 3D models. Extensive\nexperiments demonstrate that our method outperforms existing methods in both\naccuracy and stability.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09663v1", "AI": {"title_translation": "自监督多部分铰接物体建模通过可变形高斯泼溅和渐进式原始分割", "tldr": "提出DeGSS框架，通过可变形3D高斯场自监督建模多部分铰接物体，实现几何、外观、运动的统一表示和部件分割，表现优于现有方法。", "motivation": "现有方法在无人标注情况下难以对包含多个可动部件的物体建立统一表示，而准确的3D几何和运动表示对许多应用至关重要。", "method": "引入DeGSS框架，将铰接物体编码为可变形3D高斯场，将几何、外观和运动嵌入一个紧凑表示中。每个交互状态被建模为共享场的平滑变形，由此产生的变形轨迹引导渐进式的从粗到细的部件分割，以识别不同的刚性组件，所有这些都是以无监督方式进行的。", "result": "该方法在准确性和稳定性方面优于现有方法。为了评估泛化性和真实性，他们扩大了合成PartNet-Mobility基准，并发布了RS-Art数据集。", "conclusion": "DeGSS框架成功地以无监督方式实现了多部分铰接物体的统一表示、精确建模及其运动学关系，并在实验中表现出优越的性能。", "translation": "铰接物体在日常生活中无处不在，其几何和运动的精确3D表示对于众多应用至关重要。然而，在缺乏人工标注的情况下，现有方法仍然难以对包含多个可动部件的物体建立统一的表示。我们引入了DeGSS，一个统一的框架，将铰接物体编码为可变形3D高斯场，将几何、外观和运动嵌入到一个紧凑的表示中。每个交互状态都被建模为共享场的平滑变形，由此产生的变形轨迹引导渐进式的从粗到细的部件分割，以识别不同的刚性组件，所有这些都是以无监督方式进行的。精炼后的场提供了每个部件在空间上连续、完全解耦的描述，支持部件级别的重建和其运动学关系的精确建模。为了评估泛化性和真实性，我们扩大了合成PartNet-Mobility基准，并发布了RS-Art，一个真实到模拟的数据集，将RGB捕获与精确逆向工程的3D模型配对。大量的实验表明，我们的方法在准确性和稳定性方面优于现有方法。", "summary": "本文提出了DeGSS框架，旨在解决现有方法在无人工标注下难以统一表示多部分铰接物体的问题。DeGSS将铰接物体编码为可变形3D高斯场，统一嵌入几何、外观和运动信息。通过共享场的平滑变形和变形轨迹引导的无监督渐进式部件分割，DeGSS能识别和精确建模物体的刚性组件及其运动学关系。实验证明，DeGSS在准确性和稳定性上超越了现有方法，并为此发布了新的数据集RS-Art。", "keywords": "铰接物体, 自监督, 高斯泼溅, 部件分割, 3D建模", "comments": "该论文的创新点在于提出了一个自监督的统一框架DeGSS，利用可变形高斯场对多部分铰接物体进行几何、外观和运动的统一建模。特别值得注意的是其无监督的部件分割能力，这大大减少了对人工标注的依赖。此外，发布新的数据集RS-Art也对该领域的研究做出了重要贡献，有助于推动泛化性和真实世界应用的评估。"}}
{"id": "2506.09668", "title": "CINeMA: Conditional Implicit Neural Multi-Modal Atlas for a Spatio-Temporal Representation of the Perinatal Brain", "authors": ["Maik Dannecker", "Vasiliki Sideri-Lampretsa", "Sophie Starck", "Angeline Mihailov", "Mathieu Milh", "Nadine Girard", "Guillaume Auzias", "Daniel Rueckert"], "summary": "Magnetic resonance imaging of fetal and neonatal brains reveals rapid\nneurodevelopment marked by substantial anatomical changes unfolding within\ndays. Studying this critical stage of the developing human brain, therefore,\nrequires accurate brain models-referred to as atlases-of high spatial and\ntemporal resolution. To meet these demands, established traditional atlases and\nrecently proposed deep learning-based methods rely on large and comprehensive\ndatasets. This poses a major challenge for studying brains in the presence of\npathologies for which data remains scarce. We address this limitation with\nCINeMA (Conditional Implicit Neural Multi-Modal Atlas), a novel framework for\ncreating high-resolution, spatio-temporal, multimodal brain atlases, suitable\nfor low-data settings. Unlike established methods, CINeMA operates in latent\nspace, avoiding compute-intensive image registration and reducing atlas\nconstruction times from days to minutes. Furthermore, it enables flexible\nconditioning on anatomical features including GA, birth age, and pathologies\nlike ventriculomegaly (VM) and agenesis of the corpus callosum (ACC). CINeMA\nsupports downstream tasks such as tissue segmentation and age prediction\nwhereas its generative properties enable synthetic data creation and\nanatomically informed data augmentation. Surpassing state-of-the-art methods in\naccuracy, efficiency, and versatility, CINeMA represents a powerful tool for\nadvancing brain research. We release the code and atlases at\nhttps://github.com/m-dannecker/CINeMA.", "comment": "Work currently under revision for IEEE TMI", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09668v1", "AI": {"title_translation": "CINeMA：一种用于围产期大脑时空表示的条件隐式神经多模态图谱", "tldr": "CINeMA是一种新型的条件隐式神经多模态图谱框架，能够在数据稀缺的情况下，快速构建高分辨率、时空、多模态的围产期大脑图谱，并支持多种下游任务。", "motivation": "胎儿和新生儿大脑的磁共振成像显示出以显著解剖学变化为特征的快速神经发育。研究这一关键阶段需要高空间和时间分辨率的准确大脑模型（即图谱）。然而，传统方法和基于深度学习的方法都依赖于大型综合数据集，这对于数据稀缺的病理大脑研究构成了重大挑战。", "method": "CINeMA（条件隐式神经多模态图谱）是一个新颖的框架，用于创建高分辨率、时空、多模态的脑图谱，适用于低数据设置。与现有方法不同，CINeMA在潜在空间中运行，避免了计算密集型的图像配准，并将图谱构建时间从数天缩短到数分钟。此外，它还支持对胎龄（GA）、出生年龄以及脑室扩大（VM）和胼胝体发育不全（ACC）等解剖特征进行灵活的条件限制。", "result": "CINeMA将图谱构建时间从数天缩短到数分钟，并支持组织分割和年龄预测等下游任务。其生成特性还支持合成数据创建和解剖学信息的数据增强。CINeMA在准确性、效率和多功能性方面超越了现有最先进的方法。", "conclusion": "CINeMA代表了一种强大的工具，能够推动大脑研究的进展。", "translation": "胎儿和新生儿大脑的磁共振成像揭示了在几天内发生显著解剖变化的快速神经发育。因此，研究人类大脑发育的这一关键阶段需要高空间和时间分辨率的准确大脑模型——即图谱。为了满足这些需求，已建立的传统图谱和最近提出的基于深度学习的方法都依赖于大型和全面的数据集。这给研究存在病理的大脑带来了重大挑战，因为这些数据仍然稀缺。我们通过CINeMA（条件隐式神经多模态图谱）解决了这一限制，这是一个新颖的框架，用于创建高分辨率、时空、多模态的脑图谱，适用于低数据设置。与现有方法不同，CINeMA在潜在空间中运行，避免了计算密集型的图像配准，并将图谱构建时间从数天缩短到数分钟。此外，它还支持对解剖特征进行灵活的条件限制，包括胎龄（GA）、出生年龄以及脑室扩大（VM）和胼胝体发育不全（ACC）等病理。CINeMA支持组织分割和年龄预测等下游任务，其生成特性还支持合成数据创建和解剖学信息的数据增强。CINeMA在准确性、效率和多功能性方面超越了现有最先进的方法，代表了一种强大的工具，能够推动大脑研究的进展。我们已在https://github.com/m-dannecker/CINeMA 发布了代码和图谱。", "summary": "本研究提出了CINeMA（Conditional Implicit Neural Multi-Modal Atlas），一个针对围产期大脑的新型条件隐式神经多模态图谱框架。该框架旨在解决传统方法和现有深度学习方法在数据稀缺的病理大脑研究中面临的挑战。CINeMA在潜在空间中操作，显著减少了图谱构建时间，并支持对胎龄、出生年龄和病理特征的灵活条件限制。它能够生成高分辨率、时空、多模态的脑图谱，并支持组织分割、年龄预测、合成数据生成和数据增强等下游任务。实验结果表明，CINeMA在准确性、效率和多功能性方面均优于现有最先进的方法，为大脑研究提供了强大的新工具。", "keywords": "隐式神经表示, 多模态图谱, 围产期大脑, 低数据设置, 时空建模", "comments": "CINeMA的创新之处在于其能够在数据稀缺的环境下，通过在潜在空间中操作来快速构建高分辨率、时空、多模态的脑图谱，这对于研究罕见病理的围产期大脑具有重要意义。它避免了计算密集型的图像配准，显著提高了效率。此外，其灵活的条件限制能力和对下游任务的支持也增强了其实用性。"}}
{"id": "2506.09526", "title": "Neural Functions for Learning Periodic Signal", "authors": ["Woojin Cho", "Minju Jo", "Kookjin Lee", "Noseong Park"], "summary": "As function approximators, deep neural networks have served as an effective\ntool to represent various signal types. Recent approaches utilize multi-layer\nperceptrons (MLPs) to learn a nonlinear mapping from a coordinate to its\ncorresponding signal, facilitating the learning of continuous neural\nrepresentations from discrete data points. Despite notable successes in\nlearning diverse signal types, coordinate-based MLPs often face issues of\noverfitting and limited generalizability beyond the training region, resulting\nin subpar extrapolation performance. This study addresses scenarios where the\nunderlying true signals exhibit periodic properties, either spatially or\ntemporally. We propose a novel network architecture, which extracts periodic\npatterns from measurements and leverages this information to represent the\nsignal, thereby enhancing generalization and improving extrapolation\nperformance. We demonstrate the efficacy of the proposed method through\ncomprehensive experiments, including the learning of the periodic solutions for\ndifferential equations, and time series imputation (interpolation) and\nforecasting (extrapolation) on real-world datasets.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09526v1", "AI": {"title_translation": "用于学习周期信号的神经函数", "tldr": "该论文提出了一种新的神经网络架构，用于学习周期信号，与传统MLP相比，它能更好地泛化和外推，尤其适用于周期性数据。", "motivation": "作为函数逼近器，基于坐标的MLP在从离散数据点学习连续神经表示时，常面临过拟合和训练区域外泛化能力有限的问题，导致外推性能不佳。本研究旨在解决底层真实信号表现出周期性（无论是空间上还是时间上）时，MLP的这些局限性。", "method": "本研究提出了一种新颖的网络架构，该架构从测量中提取周期性模式，并利用这些信息来表示信号，从而增强泛化能力并改善外推性能。", "result": "通过综合实验证明了所提出方法的有效性，包括学习微分方程的周期解，以及在真实世界数据集上进行时间序列插补（内插）和预测（外推）。", "conclusion": "该论文提出了一种用于学习周期信号的新型神经网络架构，通过增强泛化能力和改善外推性能，有效解决了传统MLP在处理具有周期性信号时的局限性。", "translation": "作为函数逼近器，深度神经网络已成为表示各种信号类型的有效工具。最近的方法利用多层感知器（MLP）学习从坐标到其对应信号的非线性映射，从而促进从离散数据点学习连续神经表示。尽管在学习各种信号类型方面取得了显著成功，但基于坐标的MLP经常面临过拟合和训练区域之外泛化能力有限的问题，导致外推性能不佳。本研究解决了底层真实信号表现出周期性（无论是空间上还是时间上）的情况。我们提出了一种新颖的网络架构，它从测量中提取周期性模式并利用这些信息来表示信号，从而增强泛化能力并改善外推性能。我们通过综合实验证明了所提出方法的有效性，包括学习微分方程的周期解，以及在真实世界数据集上进行时间序列插补（内插）和预测（外推）。", "summary": "本文提出了一种新颖的神经网络架构，旨在学习周期信号，解决了传统基于坐标的MLP常出现的过拟合和外推性能差的问题。该方法从数据中提取周期性模式并利用这些信息来表示信号，从而提高了泛化能力和外推性能。其有效性通过周期微分方程和真实世界时间序列插补与预测任务的实验得到了验证。", "keywords": "周期信号, 神经网络, 泛化, 外推, 时间序列", "comments": "该论文的创新点在于，通过将周期模式提取整合到网络架构中，专门针对周期信号进行学习，直接解决了MLP在处理此类数据时外推能力不足的已知局限性。这对于处理周期性现象的应用可能非常重要。"}}
{"id": "2506.09677", "title": "Reasoning Models Are More Easily Gaslighted Than You Think", "authors": ["Bin Zhu", "Hailong Yin", "Jingjing Chen", "Yu-Gang Jiang"], "summary": "Recent advances in reasoning-centric models promise improved robustness\nthrough mechanisms such as chain-of-thought prompting and test-time scaling.\nHowever, their ability to withstand misleading user input remains\nunderexplored. In this paper, we conduct a systematic evaluation of three\nstate-of-the-art reasoning models, i.e., OpenAI's o4-mini, Claude-3.7-Sonnet\nand Gemini-2.5-Flash, across three multimodal benchmarks: MMMU, MathVista, and\nCharXiv. Our evaluation reveals significant accuracy drops (25-29% on average)\nfollowing gaslighting negation prompts, indicating that even top-tier reasoning\nmodels struggle to preserve correct answers under manipulative user feedback.\nBuilt upon the insights of the evaluation and to further probe this\nvulnerability, we introduce GaslightingBench-R, a new diagnostic benchmark\nspecifically designed to evaluate reasoning models' susceptibility to defend\ntheir belief under gaslighting negation prompt. Constructed by filtering and\ncurating 1,025 challenging samples from the existing benchmarks,\nGaslightingBench-R induces even more dramatic failures, with accuracy drops\nexceeding 53% on average. Our findings reveal fundamental limitations in the\nrobustness of reasoning models, highlighting the gap between step-by-step\nreasoning and belief persistence.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09677v1", "AI": {"title_translation": "推理模型比你想象的更容易被“煤气灯效应”影响", "tldr": "先进的推理模型在面对误导性用户输入（“煤气灯效应”提示）时，准确率显著下降，表明其鲁棒性存在根本性局限。", "motivation": "尽管推理模型在链式思考和测试时扩展等机制下有望提高鲁棒性，但它们抵御误导性用户输入的能力尚未得到充分探索。", "method": "本文系统性评估了三种最先进的推理模型（OpenAI的o4-mini、Claude-3.7-Sonnet和Gemini-2.5-Flash），使用了MMMU、MathVista和CharXiv三个多模态基准。在此基础上，引入了新的诊断基准GaslightingBench-R，该基准通过过滤和筛选现有基准中的1,025个挑战性样本构建，专门用于评估推理模型在“煤气灯效应”否定提示下捍卫其信念的易感性。", "result": "在“煤气灯效应”否定提示下，模型的准确率平均下降25-29%。使用GaslightingBench-R基准时，准确率下降甚至超过53%。", "conclusion": "研究结果揭示了推理模型鲁棒性的根本性局限，突显了逐步推理与信念持久性之间的差距。", "translation": "推理模型比你想象的更容易被“煤气灯效应”影响\n\n近期以推理为中心模型的进展，通过如思维链提示和测试时缩放等机制，有望提高鲁棒性。然而，它们抵御误导性用户输入的能力仍未得到充分探索。在本文中，我们对三种最先进的推理模型，即OpenAI的o4-mini、Claude-3.7-Sonnet和Gemini-2.5-Flash，在MMMU、MathVista和CharXiv三个多模态基准上进行了系统性评估。我们的评估揭示，在“煤气灯效应”否定提示下，模型准确率显著下降（平均25-29%），这表明即使是顶级推理模型在操纵性用户反馈下也难以保持正确答案。基于评估的见解，并为了进一步探究这一脆弱性，我们引入了GaslightingBench-R，一个专门设计用于评估推理模型在“煤气灯效应”否定提示下捍卫其信念易感性的新诊断基准。GaslightingBench-R通过过滤和筛选现有基准中的1,025个挑战性样本构建，导致了更严重的失败，平均准确率下降超过53%。我们的发现揭示了推理模型鲁棒性的根本性局限，突显了逐步推理与信念持久性之间的差距。", "summary": "本文系统评估了OpenAI o4-mini、Claude-3.7-Sonnet和Gemini-2.5-Flash等先进推理模型在面对“煤气灯效应”否定提示时的鲁棒性。研究发现，在多模态基准上，这些模型在误导性输入下准确率显著下降（25-29%）。为了进一步探究这一脆弱性，研究引入了GaslightingBench-R新基准，该基准导致了更严重的性能下降（超过53%）。结果揭示了推理模型鲁棒性的根本局限，指出了逐步推理与信念持久性之间的差异。", "keywords": "推理模型, 鲁棒性, 煤气灯效应, 诊断基准, 大语言模型", "comments": "这项研究通过引入“煤气灯效应”这一新颖的视角，揭示了当前先进推理模型在鲁棒性方面存在的深层缺陷，特别是在抵御恶意或误导性输入方面的不足。GaslightingBench-R的提出为后续研究提供了一个有价值的诊断工具，有助于推动模型在真实世界复杂交互中“信念持久性”能力的提升。其创新性在于将心理学概念引入AI评估，并强调了超越简单准确率的鲁棒性挑战。"}}
{"id": "2506.09532", "title": "Athena: Enhancing Multimodal Reasoning with Data-efficient Process Reward Models", "authors": ["Shuai Wang", "Zhenhua Liu", "Jiaheng Wei", "Xuanwu Yin", "Dong Li", "Emad Barsoum"], "summary": "We present Athena-PRM, a multimodal process reward model (PRM) designed to\nevaluate the reward score for each step in solving complex reasoning problems.\nDeveloping high-performance PRMs typically demands significant time and\nfinancial investment, primarily due to the necessity for step-level annotations\nof reasoning steps. Conventional automated labeling methods, such as Monte\nCarlo estimation, often produce noisy labels and incur substantial\ncomputational costs. To efficiently generate high-quality process-labeled data,\nwe propose leveraging prediction consistency between weak and strong completers\nas a criterion for identifying reliable process labels. Remarkably, Athena-PRM\ndemonstrates outstanding effectiveness across various scenarios and benchmarks\nwith just 5,000 samples. Furthermore, we also develop two effective strategies\nto improve the performance of PRMs: ORM initialization and up-sampling for\nnegative data. We validate our approach in three specific scenarios:\nverification for test time scaling, direct evaluation of reasoning step\ncorrectness, and reward ranked fine-tuning. Our Athena-PRM consistently\nachieves superior performance across multiple benchmarks and scenarios.\nNotably, when using Qwen2.5-VL-7B as the policy model, Athena-PRM enhances\nperformance by 10.2 points on WeMath and 7.1 points on MathVista for test time\nscaling. Furthermore, Athena-PRM sets the state-of-the-art (SoTA) results in\nVisualProcessBench and outperforms the previous SoTA by 3.9 F1-score,\nshowcasing its robust capability to accurately assess the correctness of the\nreasoning step. Additionally, utilizing Athena-PRM as the reward model, we\ndevelop Athena-7B with reward ranked fine-tuning and outperforms baseline with\na significant margin on five benchmarks.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09532v1", "AI": {"title_translation": "Athena：利用数据高效过程奖励模型增强多模态推理", "tldr": "Athena-PRM是一个数据高效的多模态过程奖励模型，通过利用弱强补全器预测一致性来高效生成高质量标注数据，并结合ORM初始化和负数据上采样策略，显著提升了复杂推理问题中步骤奖励评估的准确性和效率，在多个基准测试中取得了SOTA性能。", "motivation": "开发高性能过程奖励模型（PRM）通常需要大量时间和财力，主要由于需要进行步骤级标注。传统的自动化标注方法（如蒙特卡洛估计）会产生噪声标签且计算成本高昂。", "method": "本文提出了Athena-PRM，一个多模态过程奖励模型。为了高效生成高质量的过程标注数据，它利用弱补全器和强补全器之间的预测一致性作为识别可靠过程标签的标准。此外，还开发了ORM初始化和负数据上采样两种策略来提高PRM的性能。该方法在测试时间缩放验证、推理步骤正确性的直接评估和奖励排序微调三种场景中进行了验证。", "result": "Athena-PRM仅用5000个样本就展现出卓越的有效性。在多个基准测试和场景中始终取得卓越性能。当使用Qwen2.5-VL-7B作为策略模型时，Athena-PRM在WeMath上的性能提升了10.2点，在MathVista上提升了7.1点（用于测试时间缩放）。在VisualProcessBench中取得了最先进（SoTA）的结果，并以3.9 F1-score的优势超越了之前的SoTA。利用Athena-PRM作为奖励模型开发的Athena-7B，通过奖励排序微调，在五个基准测试中以显著优势超越了基线。", "conclusion": "Athena-PRM通过数据高效的方法和创新的标注策略，显著提升了多模态推理中步骤奖励评估的准确性和效率，在多个基准测试中取得了卓越的性能和最先进的结果。", "translation": "我们提出了Athena-PRM，一个多模态过程奖励模型（PRM），旨在评估解决复杂推理问题中每个步骤的奖励分数。开发高性能PRM通常需要大量时间和财力投入，主要原因在于推理步骤需要进行步骤级标注。传统的自动化标注方法，如蒙特卡洛估计，通常会产生噪声标签并产生高昂的计算成本。为了高效生成高质量的过程标注数据，我们提出利用弱补全器和强补全器之间的预测一致性作为识别可靠过程标签的标准。值得注意的是，Athena-PRM仅用5000个样本就在各种场景和基准测试中展现出卓越的有效性。此外，我们还开发了两种有效策略来提高PRM的性能：ORM初始化和负数据上采样。我们在三种特定场景中验证了我们的方法：测试时间缩放验证、推理步骤正确性的直接评估以及奖励排序微调。我们的Athena-PRM在多个基准测试和场景中始终取得卓越性能。值得一提的是，当使用Qwen2.5-VL-7B作为策略模型时，Athena-PRM在WeMath上的性能提升了10.2点，在MathVista上提升了7.1点，用于测试时间缩放。此外，Athena-PRM在VisualProcessBench中取得了最先进（SoTA）的结果，并以3.9 F1-score的优势超越了之前的SoTA，展示了其准确评估推理步骤正确性的强大能力。此外，利用Athena-PRM作为奖励模型，我们通过奖励排序微调开发了Athena-7B，并在五个基准测试中以显著优势超越了基线。", "summary": "Athena-PRM是一个数据高效的多模态过程奖励模型，旨在解决复杂推理问题中步骤级标注数据获取困难的问题。它通过利用弱补全器和强补全器之间的预测一致性来高效生成高质量的过程标注数据，并结合ORM初始化和负数据上采样等策略提升性能。该模型仅用5000个样本即在多个基准测试中展现出卓越效果，并在测试时间缩放和推理步骤正确性评估等方面取得了显著性能提升，甚至在VisualProcessBench上达到了当前最佳水平。", "keywords": "多模态推理, 过程奖励模型, 数据高效, 步骤级标注, 最先进", "comments": "该论文提出了一种创新的数据高效方法来训练过程奖励模型，解决了开发高性能PRM的关键瓶颈。利用弱补全器和强补全器之间的预测一致性进行标注的方法既巧妙又实用。仅用5000个样本就展示出卓越效果并持续取得SOTA结果，凸显了其在推进多模态推理能力方面的重要性。"}}
{"id": "2506.09691", "title": "Adding simple structure at inference improves Vision-Language Compositionality", "authors": ["Imanol Miranda", "Ander Salaberria", "Eneko Agirre", "Gorka Azkune"], "summary": "Dual encoder Vision-Language Models (VLM) such as CLIP are widely used for\nimage-text retrieval tasks. However, those models struggle with\ncompositionality, showing a bag-of-words-like behavior that limits their\nretrieval performance. Many different training approaches have been proposed to\nimprove the vision-language compositionality capabilities of those models. In\ncomparison, inference-time techniques have received little attention. In this\npaper, we propose to add simple structure at inference, where, given an image\nand a caption: i) we divide the image into different smaller crops, ii) we\nextract text segments, capturing objects, attributes and relations, iii) using\na VLM, we find the image crops that better align with text segments obtaining\nmatches, and iv) we compute the final image-text similarity aggregating the\nindividual similarities of the matches. Based on various popular dual encoder\nVLMs, we evaluate our approach in controlled and natural datasets for VL\ncompositionality. We find that our approach consistently improves the\nperformance of evaluated VLMs without any training, which shows the potential\nof inference-time techniques. The results are especially good for\nattribute-object binding as shown in the controlled dataset. As a result of an\nextensive analysis: i) we show that processing image crops is actually\nessential for the observed gains in performance, and ii) we identify specific\nareas to further improve inference-time approaches.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09691v1", "AI": {"title_translation": "在推理时添加简单结构可提高视觉-语言组合性", "tldr": "本文提出了一种在推理时为双编码器视觉-语言模型添加简单结构的方法，以提高其组合性，并在不进行任何训练的情况下显著提升了性能。", "motivation": "双编码器视觉-语言模型（VLM）在图像-文本检索任务中广泛使用，但它们在组合性方面表现不佳，呈现出类似“词袋”的行为，限制了其检索性能。许多现有的方法都集中在训练阶段来改进组合性，而推理时期的技术却很少受到关注。", "method": "给定图像和标题，本文在推理时添加简单结构：i) 将图像分成不同的更小的裁剪区域；ii) 提取捕获对象、属性和关系的文本片段；iii) 使用VLM找到与文本片段更好地对齐的图像裁剪区域，从而获得匹配项；iv) 聚合匹配项的个体相似度来计算最终的图像-文本相似度。", "result": "该方法在受控和自然数据集上对各种流行的双编码器VLM进行了评估，结果表明，在不进行任何训练的情况下，该方法持续提高了所评估VLM的性能。特别是在受控数据集上，属性-对象绑定方面的结果尤其出色。广泛分析表明，处理图像裁剪区域对于观察到的性能提升至关重要。", "conclusion": "推理时添加简单结构可以显著提高视觉-语言模型的组合性，证明了推理时技术的巨大潜力。", "translation": "双编码器视觉-语言模型（VLM），如CLIP，被广泛用于图像-文本检索任务。然而，这些模型在组合性方面表现不佳，表现出类似“词袋”的行为，这限制了它们的检索性能。许多不同的训练方法已被提出，以提高这些模型的视觉-语言组合性能力。相比之下，推理时期的技术受到的关注很少。在本文中，我们提出在推理时添加简单结构，其中，给定一幅图像和一个标题：i) 我们将图像分成不同的更小的裁剪区域，ii) 我们提取文本片段，捕获对象、属性和关系，iii) 使用VLM，我们找到与文本片段更好地对齐的图像裁剪区域，从而获得匹配项，并且iv) 我们聚合匹配项的个体相似度来计算最终的图像-文本相似度。基于各种流行的双编码器VLM，我们在受控和自然数据集上评估了我们的方法，以验证其视觉-语言组合性。我们发现，我们的方法在不进行任何训练的情况下，持续提高了所评估VLM的性能，这显示了推理时技术的潜力。结果在属性-对象绑定方面尤其出色，如受控数据集所示。作为一项广泛分析的结果：i) 我们表明处理图像裁剪区域对于观察到的性能增益实际上是必不可少的，ii) 我们确定了进一步改进推理时方法的具体领域。", "summary": "本文提出了一种在推理时提升双编码器视觉-语言模型（VLM）组合性的新方法。针对现有VLM在组合性上的不足以及训练方法受到的过多关注，该研究通过将图像分割成小块、提取文本片段，并利用VLM匹配图像块与文本片段，最终聚合相似度来计算整体图像-文本相似度。实验结果显示，该方法在不进行任何训练的情况下，显著提高了VLM在图像-文本组合性任务上的性能，尤其在属性-对象绑定方面表现突出，并强调了图像裁剪处理的重要性。", "keywords": "视觉-语言模型, 组合性, 推理时技术, 图像-文本检索, 双编码器", "comments": "这篇论文的创新点在于将改进VLM组合性的重点从训练阶段转移到推理阶段，提出了一个简单而有效的结构化方法。其重要性在于证明了无需复杂训练即可显著提升模型性能的潜力，为未来VLM的研究开辟了新的方向，尤其是在资源受限或需要快速部署的场景下具有实际应用价值。强调图像裁剪处理的必要性也为后续研究指明了方向。"}}
{"id": "2506.09544", "title": "STOAT: Spatial-Temporal Probabilistic Causal Inference Network", "authors": ["Yang Yang", "Du Yin", "Hao Xue", "Flora Salim"], "summary": "Spatial-temporal causal time series (STC-TS) involve region-specific temporal\nobservations driven by causally relevant covariates and interconnected across\ngeographic or network-based spaces. Existing methods often model spatial and\ntemporal dynamics independently and overlook causality-driven probabilistic\nforecasting, limiting their predictive power. To address this, we propose STOAT\n(Spatial-Temporal Probabilistic Causal Inference Network), a novel framework\nfor probabilistic forecasting in STC-TS. The proposed method extends a causal\ninference approach by incorporating a spatial relation matrix that encodes\ninterregional dependencies (e.g. proximity or connectivity), enabling spatially\ninformed causal effect estimation. The resulting latent series are processed by\ndeep probabilistic models to estimate the parameters of the distributions,\nenabling calibrated uncertainty modeling. We further explore multiple output\ndistributions (e.g., Gaussian, Student's-$t$, Laplace) to capture\nregion-specific variability. Experiments on COVID-19 data across six countries\ndemonstrate that STOAT outperforms state-of-the-art probabilistic forecasting\nmodels (DeepAR, DeepVAR, Deep State Space Model, etc.) in key metrics,\nparticularly in regions with strong spatial dependencies. By bridging causal\ninference and geospatial probabilistic forecasting, STOAT offers a\ngeneralizable framework for complex spatial-temporal tasks, such as epidemic\nmanagement.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09544v1", "AI": {"title_translation": "STOAT：时空概率因果推断网络", "tldr": "STOAT是一个新颖的框架，用于时空因果时间序列的概率预测，通过结合空间关系矩阵和深度概率模型，在COVID-19数据上表现优于现有方法。", "motivation": "现有方法在处理时空因果时间序列时，通常独立建模空间和时间动态，并忽视因果驱动的概率预测，限制了其预测能力。", "method": "本文提出了STOAT（时空概率因果推断网络），一个用于时空因果时间序列概率预测的新框架。该方法通过引入编码区域间依赖关系（如接近度或连通性）的空间关系矩阵，扩展了因果推断方法，从而实现空间信息化的因果效应估计。得到的潜在序列由深度概率模型处理，以估计分布参数，从而实现校准的不确定性建模。该研究还探索了多种输出分布（如高斯、学生t、拉普拉斯）以捕获区域特异性变异。", "result": "在六个国家的COVID-19数据上进行的实验表明，STOAT在关键指标上优于最先进的概率预测模型（DeepAR、DeepVAR、Deep State Space Model等），尤其是在空间依赖性强的区域。", "conclusion": "通过弥合因果推断和地理空间概率预测之间的鸿沟，STOAT为复杂的时空任务（如流行病管理）提供了一个可推广的框架。", "translation": "时空因果时间序列（STC-TS）涉及由因果相关协变量驱动并在地理或网络空间中相互连接的区域特定时间观测。现有方法通常独立建模空间和时间动态，并忽视因果驱动的概率预测，限制了其预测能力。为了解决这个问题，我们提出了STOAT（时空概率因果推断网络），一个用于STC-TS中概率预测的新颖框架。所提出的方法通过引入编码区域间依赖关系（例如接近度或连通性）的空间关系矩阵来扩展因果推断方法，从而实现空间信息化的因果效应估计。得到的潜在序列由深度概率模型处理，以估计分布参数，从而实现校准的不确定性建模。我们进一步探索了多种输出分布（例如高斯、学生t、拉普拉斯）以捕获区域特异性变异。在六个国家的COVID-19数据上进行的实验表明，STOAT在关键指标上优于最先进的概率预测模型（DeepAR、DeepVAR、Deep State Space Model等），尤其是在空间依赖性强的区域。通过弥合因果推断和地理空间概率预测之间的鸿沟，STOAT为复杂的时空任务（如流行病管理）提供了一个可推广的框架。", "summary": "STOAT（时空概率因果推断网络）是一个新颖的框架，旨在解决时空因果时间序列（STC-TS）中现有方法独立建模空间和时间动态并忽视因果驱动概率预测的局限性。该方法通过整合空间关系矩阵进行空间信息化的因果效应估计，并将结果潜在序列输入深度概率模型以实现校准的不确定性建模和探索多种输出分布。在COVID-19数据上的实验证明，STOAT在预测性能上超越了当前的先进模型，尤其是在空间依赖性强的区域，为流行病管理等复杂时空任务提供了一个通用框架。", "keywords": "时空时间序列, 因果推断, 概率预测, 深度学习, COVID-19", "comments": "STOAT的创新之处在于其将因果推断与地理空间概率预测相结合，通过引入空间关系矩阵来捕捉区域间的依赖性，并利用深度概率模型进行不确定性量化，这对于理解和预测复杂时空现象具有重要意义。"}}
{"id": "2506.09695", "title": "Towards Practical Alzheimer's Disease Diagnosis: A Lightweight and Interpretable Spiking Neural Model", "authors": ["Changwei Wu", "Yifei Chen", "Yuxin Du", "Jinying Zong", "Jie Dong", "Mingxuan Liu", "Yong Peng", "Jin Fan", "Feiwei Qin", "Changmiao Wang"], "summary": "Early diagnosis of Alzheimer's Disease (AD), especially at the mild cognitive\nimpairment (MCI) stage, is vital yet hindered by subjective assessments and the\nhigh cost of multimodal imaging modalities. Although deep learning methods\noffer automated alternatives, their energy inefficiency and computational\ndemands limit real-world deployment, particularly in resource-constrained\nsettings. As a brain-inspired paradigm, spiking neural networks (SNNs) are\ninherently well-suited for modeling the sparse, event-driven patterns of neural\ndegeneration in AD, offering a promising foundation for interpretable and\nlow-power medical diagnostics. However, existing SNNs often suffer from weak\nexpressiveness and unstable training, which restrict their effectiveness in\ncomplex medical tasks. To address these limitations, we propose FasterSNN, a\nhybrid neural architecture that integrates biologically inspired LIF neurons\nwith region-adaptive convolution and multi-scale spiking attention. This design\nenables sparse, efficient processing of 3D MRI while preserving diagnostic\naccuracy. Experiments on benchmark datasets demonstrate that FasterSNN achieves\ncompetitive performance with substantially improved efficiency and stability,\nsupporting its potential for practical AD screening. Our source code is\navailable at https://github.com/wuchangw/FasterSNN.", "comment": "11 pages, 5 figures", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09695v1", "AI": {"title_translation": "迈向实用阿尔茨海默病诊断：一种轻量级且可解释的脉冲神经网络模型", "tldr": "FasterSNN是一种轻量级、可解释的脉冲神经网络模型，用于高效准确地诊断阿尔茨海默病，解决了现有深度学习和SNN方法的局限性。", "motivation": "早期诊断阿尔茨海默病（AD），尤其是在轻度认知障碍（MCI）阶段，至关重要，但受限于主观评估和多模态影像的高成本。现有深度学习方法虽然能实现自动化诊断，但其能效低下和计算需求高，限制了实际部署。脉冲神经网络（SNNs）适合建模AD中的神经退行性稀疏、事件驱动模式，但现有SNNs通常表现力弱且训练不稳定，限制了其在复杂医疗任务中的有效性。", "method": "提出FasterSNN，一种混合神经网络架构，它整合了生物启发式LIF神经元、区域自适应卷积和多尺度脉冲注意力。该设计旨在实现3D MRI的稀疏、高效处理，同时保持诊断准确性。", "result": "在基准数据集上的实验表明，FasterSNN在保持竞争性性能的同时，显著提高了效率和稳定性。", "conclusion": "FasterSNN在阿尔茨海默病诊断方面具有实用潜力，特别是在资源受限的环境中进行早期筛查。", "translation": "阿尔茨海默病（AD）的早期诊断，尤其是在轻度认知障碍（MCI）阶段，至关重要，但受限于主观评估和多模态影像的高成本。尽管深度学习方法提供了自动化替代方案，但其能效低下和计算需求限制了实际部署，特别是在资源受限的环境中。作为一种受大脑启发的范式，脉冲神经网络（SNNs）天生适合建模AD中神经退行性变的稀疏、事件驱动模式，为可解释和低功耗的医疗诊断提供了有前景的基础。然而，现有SNNs通常表现力弱且训练不稳定，这限制了它们在复杂医疗任务中的有效性。为了解决这些局限性，我们提出了FasterSNN，一种混合神经网络架构，它整合了生物启发式LIF神经元、区域自适应卷积和多尺度脉冲注意力。这种设计能够对3D MRI进行稀疏、高效的处理，同时保持诊断准确性。在基准数据集上的实验表明，FasterSNN在具有竞争性性能的同时，显著提高了效率和稳定性，支持了其在实际AD筛查中的潜力。我们的源代码可在https://github.com/wuchangw/FasterSNN获取。", "summary": "本研究提出了一种名为FasterSNN的混合脉冲神经网络模型，旨在解决阿尔茨海默病早期诊断中现有深度学习方法计算成本高和脉冲神经网络表现力不足的问题。FasterSNN结合了LIF神经元、区域自适应卷积和多尺度脉冲注意力，实现了对3D MRI的稀疏高效处理，同时保持了高诊断准确性。实验结果验证了FasterSNN在效率和稳定性方面的显著提升，显示出其在实际AD筛查中的应用前景。", "keywords": "阿尔茨海默病诊断, 脉冲神经网络, 轻量级模型, 可解释性, 混合架构", "comments": "该论文的创新点在于提出了FasterSNN，一个结合了生物启发式神经元和先进深度学习组件的混合SNN架构，有效解决了传统SNN在医疗任务中表现力不足和训练不稳定的问题。其轻量级和可解释的特性对于资源受限的实际医疗诊断场景具有重要意义，尤其是在AD早期筛查方面展现出巨大潜力。该工作为脑启发计算在医疗诊断领域的应用提供了新的思路。"}}
{"id": "2506.09301", "title": "$(RSA)^2$: A Rhetorical-Strategy-Aware Rational Speech Act Framework for Figurative Language Understanding", "authors": ["Cesare Spinoso-Di Piano", "David Austin", "Pablo Piantanida", "Jackie Chi Kit Cheung"], "summary": "Figurative language (e.g., irony, hyperbole, understatement) is ubiquitous in\nhuman communication, resulting in utterances where the literal and the intended\nmeanings do not match. The Rational Speech Act (RSA) framework, which\nexplicitly models speaker intentions, is the most widespread theory of\nprobabilistic pragmatics, but existing implementations are either unable to\naccount for figurative expressions or require modeling the implicit motivations\nfor using figurative language (e.g., to express joy or annoyance) in a\nsetting-specific way. In this paper, we introduce the Rhetorical-Strategy-Aware\nRSA $(RSA)^2$ framework which models figurative language use by considering a\nspeaker's employed rhetorical strategy. We show that $(RSA)^2$ enables\nhuman-compatible interpretations of non-literal utterances without modeling a\nspeaker's motivations for being non-literal. Combined with LLMs, it achieves\nstate-of-the-art performance on the ironic split of PragMega+, a new irony\ninterpretation dataset introduced in this study.", "comment": "Accepted to ACL 2025 (Main Conference)", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.09301v1", "AI": {"title_translation": "$(RSA)^2$: 一种修辞策略感知的理性言语行为框架，用于理解修辞性语言", "tldr": "本文提出了一种名为$(RSA)^2$的新型理性言语行为（RSA）框架，通过考虑说话者的修辞策略来理解讽刺、夸张等修辞性语言，并在无需建模说话者非字面意图动机的情况下，实现了与人类兼容的解释和最先进的性能。", "motivation": "修辞性语言在人类交流中普遍存在，导致字面意义与预期意义不符。现有的理性言语行为（RSA）框架要么无法解释修辞性表达，要么需要以特定于设置的方式建模使用修辞性语言的隐含动机（例如，表达喜悦或烦恼）。", "method": "本文引入了修辞策略感知的RSA $(RSA)^2$ 框架。该框架通过考虑说话者所采用的修辞策略来建模修辞性语言的使用。结合大型语言模型（LLMs），它在新的讽刺解释数据集PragMega+的讽刺拆分上进行了评估。", "result": "$(RSA)^2$ 框架能够在不建模说话者非字面意图动机的情况下，实现对非字面话语与人类兼容的解释。结合大型语言模型（LLMs），它在新的讽刺解释数据集PragMega+的讽刺拆分上取得了最先进的性能。", "conclusion": "$(RSA)^2$ 框架通过整合修辞策略概念，有效解决了现有RSA框架在理解修辞性语言方面的局限性，并提供了一种无需显式建模说话者动机的有效方法，从而提高了对非字面语言的理解能力。", "translation": "修辞性语言（例如，讽刺、夸张、轻描淡写）在人类交流中普遍存在，导致话语的字面意义与预期意义不符。理性言语行为（RSA）框架明确建模说话者的意图，是概率语用学中最广泛的理论，但现有实现要么无法解释修辞性表达，要么需要以特定于设置的方式建模使用修辞性语言的隐含动机（例如，表达喜悦或烦恼）。在本文中，我们引入了修辞策略感知的RSA $(RSA)^2$ 框架，该框架通过考虑说话者所采用的修辞策略来建模修辞性语言的使用。我们表明，$(RSA)^2$ 能够在不建模说话者非字面意图动机的情况下，实现对非字面话语与人类兼容的解释。结合大型语言模型（LLMs），它在本文引入的新的讽刺解释数据集PragMega+的讽刺拆分上取得了最先进的性能。", "summary": "本文提出了一个名为$(RSA)^2$的修辞策略感知理性言语行为（RSA）框架，旨在解决现有RSA框架在理解讽刺、夸张等修辞性语言方面的局限性。该框架通过建模说话者使用的修辞策略来解释非字面意义，而无需显式建模说话者的非字面意图动机。实验证明，$(RSA)^2$ 能够产生与人类兼容的解释，并且在结合大型语言模型时，在新数据集PragMega+的讽刺任务上达到了最先进的性能。", "keywords": "修辞性语言, 理性言语行为, RSA, 修辞策略, 语用学", "comments": "该论文的创新之处在于将“修辞策略”这一概念引入到理性言语行为框架中，从而有效地处理了修辞性语言的理解问题，而无需对说话者的复杂动机进行显式建模。这简化了模型，并提高了其在实际应用中的鲁棒性。结合LLMs实现SOTA性能也表明了其在现实世界任务中的潜力。"}}
{"id": "2506.09574", "title": "MOORL: A Framework for Integrating Offline-Online Reinforcement Learning", "authors": ["Gaurav Chaudhary", "Wassim Uddin Mondal", "Laxmidhar Behera"], "summary": "Sample efficiency and exploration remain critical challenges in Deep\nReinforcement Learning (DRL), particularly in complex domains. Offline RL,\nwhich enables agents to learn optimal policies from static, pre-collected\ndatasets, has emerged as a promising alternative. However, offline RL is\nconstrained by issues such as out-of-distribution (OOD) actions that limit\npolicy performance and generalization. To overcome these limitations, we\npropose Meta Offline-Online Reinforcement Learning (MOORL), a hybrid framework\nthat unifies offline and online RL for efficient and scalable learning. While\nprevious hybrid methods rely on extensive design components and added\ncomputational complexity to utilize offline data effectively, MOORL introduces\na meta-policy that seamlessly adapts across offline and online trajectories.\nThis enables the agent to leverage offline data for robust initialization while\nutilizing online interactions to drive efficient exploration. Our theoretical\nanalysis demonstrates that the hybrid approach enhances exploration by\neffectively combining the complementary strengths of offline and online data.\nFurthermore, we demonstrate that MOORL learns a stable Q-function without added\ncomplexity. Extensive experiments on 28 tasks from the D4RL and V-D4RL\nbenchmarks validate its effectiveness, showing consistent improvements over\nstate-of-the-art offline and hybrid RL baselines. With minimal computational\noverhead, MOORL achieves strong performance, underscoring its potential for\npractical applications in real-world scenarios.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09574v1", "AI": {"title_translation": "MOORL：一个集成离线-在线强化学习的框架", "tldr": "MOORL是一个混合离线和在线强化学习的框架，通过引入元策略来解决深度强化学习中的样本效率和探索挑战，并在D4RL和V-D4RL基准测试中表现出优于现有方法的效果。", "motivation": "深度强化学习（DRL）在复杂领域面临样本效率和探索的挑战。离线强化学习（Offline RL）虽然有前景，但受限于分布外（OOD）动作等问题，影响策略性能和泛化能力。现有的混合方法通常设计复杂且计算成本高。", "method": "MOORL（Meta Offline-Online Reinforcement Learning）是一个混合框架，它通过引入一个元策略（meta-policy）来无缝适应离线和在线轨迹。这使得智能体能够利用离线数据进行稳健初始化，同时利用在线交互来驱动高效探索。理论分析表明该混合方法通过结合离线和在线数据的互补优势增强了探索能力，并且MOORL在没有增加复杂性的情况下学习了一个稳定的Q函数。", "result": "在D4RL和V-D4RL基准测试的28项任务上进行了广泛实验，验证了MOORL的有效性，显示出相对于最先进的离线和混合RL基线的一致改进。MOORL以最小的计算开销实现了强大的性能。", "conclusion": "MOORL是一个有效的混合离线-在线强化学习框架，它通过创新的元策略设计，解决了DRL的样本效率和探索挑战，并在实际应用中展现出巨大潜力。", "translation": "样本效率和探索仍然是深度强化学习（DRL）的关键挑战，尤其是在复杂领域。离线强化学习（Offline RL）作为一种有前景的替代方案出现，它使智能体能够从静态、预先收集的数据集中学习最优策略。然而，离线RL受到诸如分布外（OOD）动作等问题的限制，这些问题限制了策略性能和泛化能力。为了克服这些限制，我们提出了元离线-在线强化学习（MOORL），一个统一离线和在线RL的混合框架，用于高效和可扩展的学习。虽然以前的混合方法依赖于大量的设计组件和增加的计算复杂性来有效利用离线数据，但MOORL引入了一个元策略，可以无缝地适应离线和在线轨迹。这使得智能体能够利用离线数据进行稳健初始化，同时利用在线交互来驱动高效探索。我们的理论分析表明，这种混合方法通过有效结合离线和在线数据的互补优势来增强探索。此外，我们证明MOORL在不增加复杂性的情况下学习了一个稳定的Q函数。在D4RL和V-D4RL基准测试的28项任务上进行的广泛实验验证了其有效性，显示出相对于最先进的离线和混合RL基线的一致改进。MOORL以最小的计算开销实现了强大的性能，突显了其在实际场景中应用的潜力。", "summary": "MOORL是一个新颖的混合强化学习框架，旨在解决深度强化学习中的样本效率和探索难题。它通过引入一个元策略，巧妙地融合了离线学习的稳健初始化能力和在线学习的高效探索能力。该框架在理论上证明了其对探索的增强作用，并能在不增加复杂性的前提下学习稳定的Q函数。实验结果显示，MOORL在多个基准测试中显著优于现有的离线和混合强化学习方法，且计算开销极小，预示着其在实际应用中的巨大潜力。", "keywords": "离线强化学习, 在线强化学习, 混合框架, 元策略, 样本效率", "comments": "MOORL的创新之处在于其提出的元策略，它提供了一种简洁高效的方式来整合离线和在线数据，避免了现有混合方法中常见的复杂设计和高计算成本。其在广泛基准测试中的优异表现，特别是其低计算开销，使其在解决现实世界DRL问题方面具有重要的实际应用价值。该工作为未来混合强化学习的研究开辟了新的方向。"}}
{"id": "2506.09699", "title": "CHIP: A multi-sensor dataset for 6D pose estimation of chairs in industrial settings", "authors": ["Mattia Nardon", "Mikel Mujika Agirre", "Ander González Tomé", "Daniel Sedano Algarabel", "Josep Rueda Collell", "Ana Paola Caro", "Andrea Caraffa", "Fabio Poiesi", "Paul Ian Chippendale", "Davide Boscaini"], "summary": "Accurate 6D pose estimation of complex objects in 3D environments is\nessential for effective robotic manipulation. Yet, existing benchmarks fall\nshort in evaluating 6D pose estimation methods under realistic industrial\nconditions, as most datasets focus on household objects in domestic settings,\nwhile the few available industrial datasets are limited to artificial setups\nwith objects placed on tables. To bridge this gap, we introduce CHIP, the first\ndataset designed for 6D pose estimation of chairs manipulated by a robotic arm\nin a real-world industrial environment. CHIP includes seven distinct chairs\ncaptured using three different RGBD sensing technologies and presents unique\nchallenges, such as distractor objects with fine-grained differences and severe\nocclusions caused by the robotic arm and human operators. CHIP comprises 77,811\nRGBD images annotated with ground-truth 6D poses automatically derived from the\nrobot's kinematics, averaging 11,115 annotations per chair. We benchmark CHIP\nusing three zero-shot 6D pose estimation methods, assessing performance across\ndifferent sensor types, localization priors, and occlusion levels. Results show\nsubstantial room for improvement, highlighting the unique challenges posed by\nthe dataset. CHIP will be publicly released.", "comment": "Technical report", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09699v1", "AI": {"title_translation": "CHIP：工业环境中椅子6D姿态估计的多传感器数据集", "tldr": "CHIP是首个专为工业环境中机械臂操作椅子6D姿态估计而设计的多传感器数据集，旨在弥补现有基准在现实工业条件下的不足。", "motivation": "准确的3D环境中复杂物体6D姿态估计对于有效的机器人操作至关重要。然而，现有基准在评估现实工业条件下的6D姿态估计方法时存在不足，大多数数据集侧重于家庭环境中的日常物品，而少数可用的工业数据集则局限于桌面放置物体的人工设置。为了弥补这一空白，本研究引入了CHIP数据集。", "method": "研究引入了CHIP数据集，这是首个为真实工业环境中由机械臂操作的椅子进行6D姿态估计而设计的数据集。CHIP包含七种不同的椅子，使用三种不同的RGBD传感技术捕获，并呈现了独特的挑战，如具有细微差别的干扰物体以及由机械臂和操作员引起的严重遮挡。CHIP包含77,811张RGBD图像，其真值6D姿态是从机器人运动学自动推导的。研究使用三种零样本6D姿态估计算法对CHIP进行了基准测试，评估了不同传感器类型、定位先验和遮挡水平下的性能。", "result": "基准测试结果显示，现有方法仍有很大的改进空间，突出了该数据集带来的独特挑战。", "conclusion": "CHIP数据集的发布将为工业环境中复杂物体6D姿态估计的研究提供一个具有挑战性且现实的基准，有助于推动该领域的发展。现有方法在该数据集上的表现有待提高，表明其独特的挑战性。", "translation": "准确的3D环境中复杂物体6D姿态估计对于有效的机器人操作至关重要。然而，现有基准在评估现实工业条件下的6D姿态估计方法时存在不足，大多数数据集侧重于家庭环境中的日常物品，而少数可用的工业数据集则局限于桌面放置物体的人工设置。为了弥补这一空白，我们引入了CHIP，这是首个专为真实工业环境中由机械臂操作的椅子进行6D姿态估计而设计的数据集。CHIP包含七种不同的椅子，使用三种不同的RGBD传感技术捕获，并呈现了独特的挑战，如具有细微差别的干扰物体以及由机械臂和操作员引起的严重遮挡。CHIP包含77,811张RGBD图像，其真值6D姿态是从机器人运动学自动推导的，每把椅子平均有11,115个标注。我们使用三种零样本6D姿态估计算法对CHIP进行了基准测试，评估了不同传感器类型、定位先验和遮挡水平下的性能。结果显示，现有方法仍有很大的改进空间，突出了该数据集带来的独特挑战。CHIP将公开发布。", "summary": "本研究引入了CHIP数据集，旨在解决现有6D姿态估计基准在现实工业环境下对复杂物体（如椅子）评估的不足。CHIP是首个在真实工业环境中，针对机械臂操作下椅子进行6D姿态估计的多传感器数据集，包含7种椅子、3种RGBD传感技术捕获的77,811张图像，并提供了机器人运动学推导的6D姿态真值标注。该数据集带来了干扰物体和严重遮挡等独特挑战。初步基准测试结果表明，现有零样本6D姿态估计方法在该数据集上表现不佳，凸显了其挑战性，并为未来研究提供了改进空间。CHIP数据集将公开发布。", "keywords": "6D姿态估计, 数据集, 工业环境, 机器人操作, 多传感器", "comments": "CHIP数据集的创新性在于其填补了工业环境中复杂物体6D姿态估计数据集的空白，特别是在机械臂操作和严重遮挡的现实场景下。这对于推动机器人操作和工业自动化领域的发展至关重要。其多传感器和大规模标注的特点，以及包含独特挑战的设计，使其成为一个有价值的基准。现有方法在该数据集上的表现不佳，也进一步验证了其挑战性和研究价值。"}}
{"id": "2506.09315", "title": "Alzheimer's Dementia Detection Using Perplexity from Paired Large Language Models", "authors": ["Yao Xiao", "Heidi Christensen", "Stefan Goetze"], "summary": "Alzheimer's dementia (AD) is a neurodegenerative disorder with cognitive\ndecline that commonly impacts language ability. This work extends the paired\nperplexity approach to detecting AD by using a recent large language model\n(LLM), the instruction-following version of Mistral-7B. We improve accuracy by\nan average of 3.33% over the best current paired perplexity method and by 6.35%\nover the top-ranked method from the ADReSS 2020 challenge benchmark. Our\nfurther analysis demonstrates that the proposed approach can effectively detect\nAD with a clear and interpretable decision boundary in contrast to other\nmethods that suffer from opaque decision-making processes. Finally, by\nprompting the fine-tuned LLMs and comparing the model-generated responses to\nhuman responses, we illustrate that the LLMs have learned the special language\npatterns of AD speakers, which opens up possibilities for novel methods of\nmodel interpretation and data augmentation.", "comment": "To be published in the proceedings of Interspeech 2025", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.09315v1", "AI": {"title_translation": "阿尔茨海默病痴呆症检测：基于配对大型语言模型困惑度", "tldr": "该研究使用配对大型语言模型（Mistral-7B）的困惑度来检测阿尔茨海默病痴呆症，显著提高了检测准确性，并揭示了模型学习到AD患者语言模式。", "motivation": "阿尔茨海默病（AD）是一种导致认知能力下降并影响语言能力的神经退行性疾病。该研究旨在通过改进现有的配对困惑度方法来提高AD检测的准确性和可解释性。", "method": "本文将配对困惑度方法扩展到使用大型语言模型（LLM），具体是Mistral-7B的指令遵循版本来检测AD。通过提示微调后的LLM并将模型生成的回应与人类回应进行比较，来展示LLM学习到的AD患者的特殊语言模式。", "result": "相较于当前最佳的配对困惑度方法，准确率平均提高了3.33%；相较于ADReSS 2020挑战赛基准中排名第一的方法，准确率提高了6.35%。该方法能有效检测AD，并具有清晰可解释的决策边界。LLM学习到了AD患者的特殊语言模式。", "conclusion": "使用配对大型语言模型困惑度的方法可以显著提高阿尔茨海默病检测的准确性，提供可解释的决策边界，并为模型解释和数据增强开辟了新的可能性。", "translation": "阿尔茨海默病痴呆症（AD）是一种神经退行性疾病，伴有认知能力下降，通常影响语言能力。这项工作通过使用最近的大型语言模型（LLM），即Mistral-7B的指令遵循版本，扩展了配对困惑度方法来检测AD。我们比当前最佳的配对困惑度方法平均提高了3.33%的准确率，比ADReSS 2020挑战赛基准中排名第一的方法提高了6.35%。我们进一步的分析表明，与受不透明决策过程困扰的其他方法相比，所提出的方法可以有效检测AD，并具有清晰可解释的决策边界。最后，通过提示微调后的LLM并将模型生成的回应与人类回应进行比较，我们说明了LLM已经学习了AD患者的特殊语言模式，这为模型解释和数据增强的新方法开辟了可能性。", "summary": "本文提出了一种基于配对大型语言模型（特别是Mistral-7B）困惑度来检测阿尔茨海默病痴呆症（AD）的新方法。该方法在AD检测准确性上显著优于现有最佳方法和基准测试，并提供了清晰可解释的决策边界。研究还发现，微调后的LLM能够学习到AD患者特有的语言模式，这为模型解释和数据增强提供了新的途径。", "keywords": "阿尔茨海默病, 大型语言模型, 困惑度, 疾病检测, 语言模式", "comments": "这项研究的创新之处在于将最新的大型语言模型应用于阿尔茨海默病的早期检测，并利用其困惑度进行分析。其重要性在于不仅提高了检测准确性，还解决了现有方法决策过程不透明的问题，提供了更强的可解释性。此外，发现LLM能学习到AD患者的特殊语言模式，为未来基于语言特征的AD诊断和干预研究提供了新的方向和数据增强的可能性。"}}
{"id": "2506.09593", "title": "Beyond Overconfidence: Foundation Models Redefine Calibration in Deep Neural Networks", "authors": ["Achim Hekler", "Lukas Kuhn", "Florian Buettner"], "summary": "Reliable uncertainty calibration is essential for safely deploying deep\nneural networks in high-stakes applications. Deep neural networks are known to\nexhibit systematic overconfidence, especially under distribution shifts.\nAlthough foundation models such as ConvNeXt, EVA and BEiT have demonstrated\nsignificant improvements in predictive performance, their calibration\nproperties remain underexplored. This paper presents a comprehensive\ninvestigation into the calibration behavior of foundation models, revealing\ninsights that challenge established paradigms. Our empirical analysis shows\nthat these models tend to be underconfident in in-distribution predictions,\nresulting in higher calibration errors, while demonstrating improved\ncalibration under distribution shifts. Furthermore, we demonstrate that\nfoundation models are highly responsive to post-hoc calibration techniques in\nthe in-distribution setting, enabling practitioners to effectively mitigate\nunderconfidence bias. However, these methods become progressively less reliable\nunder severe distribution shifts and can occasionally produce counterproductive\nresults. Our findings highlight the complex, non-monotonic effects of\narchitectural and training innovations on calibration, challenging established\nnarratives of continuous improvement.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09593v1", "AI": {"title_translation": "超越过度自信：基础模型重新定义深度神经网络中的校准", "tldr": "本文综合研究了ConvNeXt、EVA和BEiT等基础模型的校准行为，发现它们在分布内预测中倾向于自信不足，但在分布偏移下校准有所改善。事后校准技术对分布内设置有效，但在严重分布偏移下可靠性降低。", "motivation": "可靠的不确定性校准对于在高风险应用中安全部署深度神经网络至关重要。深度神经网络通常表现出系统性过度自信，尤其是在分布偏移下。尽管基础模型在预测性能上取得了显著进步，但其校准特性仍未被充分探索。", "method": "本文对基础模型的校准行为进行了全面调查，并通过实证分析揭示了其特性。", "result": "基础模型在分布内预测中倾向于自信不足，导致更高的校准误差，但在分布偏移下表现出改善的校准。此外，基础模型对分布内设置中的事后校准技术高度敏感，可以有效缓解自信不足偏差。然而，这些方法在严重分布偏移下可靠性逐渐降低，有时会产生反效果。", "conclusion": "研究结果强调了架构和训练创新对校准的复杂、非单调影响，挑战了持续改进的既有叙事。", "translation": "可靠的不确定性校准对于在高风险应用中安全部署深度神经网络至关重要。深度神经网络已知会表现出系统性过度自信，尤其是在分布偏移下。尽管ConvNeXt、EVA和BEiT等基础模型在预测性能上取得了显著进步，但它们的校准特性仍未被充分探索。本文对基础模型的校准行为进行了全面调查，揭示了挑战既定范式的见解。我们的实证分析表明，这些模型在分布内预测中倾向于自信不足，导致更高的校准误差，同时在分布偏移下表现出改善的校准。此外，我们证明基础模型对分布内设置中的事后校准技术高度敏感，使实践者能够有效缓解自信不足偏差。然而，这些方法在严重的分布偏移下可靠性逐渐降低，有时会产生反效果。我们的发现强调了架构和训练创新对校准的复杂、非单调影响，挑战了持续改进的既有叙事。", "summary": "本研究全面探讨了ConvNeXt、EVA和BEiT等基础模型在深度神经网络中的校准行为。研究发现，与传统观点不同，这些模型在分布内预测中常表现为自信不足，导致校准误差增加；但在分布偏移下，其校准性能反而有所提升。论文还指出，事后校准技术能有效缓解分布内设置的自信不足问题，但在严重分布偏移时效果递减，甚至可能适得其反。研究结果揭示了架构和训练创新对模型校准的复杂非单调影响，挑战了现有关于持续改进的认知。", "keywords": "基础模型, 校准, 深度神经网络, 不确定性, 分布偏移", "comments": "本文通过对基础模型校准行为的深入分析，挑战了深度神经网络校准领域的传统认知，揭示了基础模型在不同数据分布下表现出与以往模型不同的校准特性（如分布内自信不足，分布外校准改善）。这对于理解和安全部署基础模型具有重要意义，尤其是在高风险应用中。论文还指出，事后校准方法在特定情况下可能失效，这为未来的研究和实践提出了新的挑战。"}}
{"id": "2506.09718", "title": "Non-Contact Health Monitoring During Daily Personal Care Routines", "authors": ["Xulin Ma", "Jiankai Tang", "Zhang Jiang", "Songqin Cheng", "Yuanchun Shi", "Dong LI", "Xin Liu", "Daniel McDuff", "Xiaojing Liu", "Yuntao Wang"], "summary": "Remote photoplethysmography (rPPG) enables non-contact, continuous monitoring\nof physiological signals and offers a practical alternative to traditional\nhealth sensing methods. Although rPPG is promising for daily health monitoring,\nits application in long-term personal care scenarios, such as mirror-facing\nroutines in high-altitude environments, remains challenging due to ambient\nlighting variations, frequent occlusions from hand movements, and dynamic\nfacial postures. To address these challenges, we present LADH (Long-term\nAltitude Daily Health), the first long-term rPPG dataset containing 240\nsynchronized RGB and infrared (IR) facial videos from 21 participants across\nfive common personal care scenarios, along with ground-truth PPG, respiration,\nand blood oxygen signals. Our experiments demonstrate that combining RGB and IR\nvideo inputs improves the accuracy and robustness of non-contact physiological\nmonitoring, achieving a mean absolute error (MAE) of 4.99 BPM in heart rate\nestimation. Furthermore, we find that multi-task learning enhances performance\nacross multiple physiological indicators simultaneously. Dataset and code are\nopen at https://github.com/McJackTang/FusionVitals.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09718v1", "AI": {"title_translation": "日常个人护理活动中的非接触式健康监测", "tldr": "该研究提出了LADH数据集，结合RGB和红外视频输入，显著提升了在日常个人护理场景下非接触式生理信号监测的准确性和鲁棒性。", "motivation": "远程光电容积描记术（rPPG）在日常健康监测中前景广阔，但在长期个人护理场景（如高海拔地区对着镜子的日常活动）中，由于环境光变化、手部遮挡和面部姿态动态变化，其应用仍面临挑战。", "method": "提出了LADH（长期高海拔日常健康）数据集，包含来自21名参与者在5种常见个人护理场景下的240个同步RGB和红外（IR）面部视频，以及PPG、呼吸和血氧的真实信号。实验通过结合RGB和IR视频输入，并利用多任务学习来提升性能。", "result": "结合RGB和IR视频输入提高了非接触式生理监测的准确性和鲁棒性，在心率估计中实现了4.99 BPM的平均绝对误差（MAE）。多任务学习同时增强了多个生理指标的性能。", "conclusion": "结合RGB和红外视频输入以及多任务学习，可以有效解决rPPG在复杂日常场景下的挑战，实现更准确和鲁棒的非接触式健康监测。", "translation": "远程光电容积描记术（rPPG）实现了生理信号的非接触式连续监测，为传统健康传感方法提供了一种实用的替代方案。尽管rPPG在日常健康监测方面前景广阔，但其在长期个人护理场景（例如高海拔环境中照镜子的日常活动）中的应用仍然充满挑战，这归因于环境光变化、频繁的手部遮挡和动态的面部姿态。为了解决这些挑战，我们提出了LADH（长期高海拔日常健康），这是第一个长期rPPG数据集，包含来自21名参与者在五种常见个人护理场景下的240个同步RGB和红外（IR）面部视频，以及真实的PPG、呼吸和血氧信号。我们的实验表明，结合RGB和IR视频输入可以提高非接触式生理监测的准确性和鲁棒性，在心率估计中实现了4.99 BPM的平均绝对误差（MAE）。此外，我们发现多任务学习同时增强了多个生理指标的性能。数据集和代码已在https://github.com/McJackTang/FusionVitals开放。", "summary": "本文针对远程光电容积描记术(rPPG)在日常个人护理场景中面临的环境光、遮挡和面部姿态变化等挑战，提出了首个长期rPPG数据集LADH，该数据集包含同步的RGB和红外视频以及真实生理信号。研究表明，结合RGB和红外视频输入显著提升了非接触式生理监测的准确性和鲁棒性，并在心率估计上达到了4.99 BPM的平均绝对误差。此外，多任务学习也被证明能同时提高多项生理指标的性能。", "keywords": "远程光电容积描记术, rPPG, 非接触式健康监测, 多模态融合, LADH数据集", "comments": "该研究通过构建独特的大规模多模态数据集（RGB+IR）并探索多模态融合及多任务学习，有效地解决了rPPG在复杂日常环境下的鲁棒性问题，为非接触式健康监测在实际应用中的推广奠定了基础。数据集的开放性也促进了该领域的研究。"}}
{"id": "2506.09594", "title": "Accelerating Large-Scale Regularized High-Order Tensor Recovery", "authors": ["Wenjin Qin", "Hailin Wang", "Jingyao Hou", "Jianjun Wang"], "summary": "Currently, existing tensor recovery methods fail to recognize the impact of\ntensor scale variations on their structural characteristics. Furthermore,\nexisting studies face prohibitive computational costs when dealing with\nlarge-scale high-order tensor data. To alleviate these issue, assisted by the\nKrylov subspace iteration, block Lanczos bidiagonalization process, and random\nprojection strategies, this article first devises two fast and accurate\nrandomized algorithms for low-rank tensor approximation (LRTA) problem.\nTheoretical bounds on the accuracy of the approximation error estimate are\nestablished. Next, we develop a novel generalized nonconvex modeling framework\ntailored to large-scale tensor recovery, in which a new regularization paradigm\nis exploited to achieve insightful prior representation for large-scale\ntensors. On the basis of the above, we further investigate new unified\nnonconvex models and efficient optimization algorithms, respectively, for\nseveral typical high-order tensor recovery tasks in unquantized and quantized\nsituations. To render the proposed algorithms practical and efficient for\nlarge-scale tensor data, the proposed randomized LRTA schemes are integrated\ninto their central and time-intensive computations. Finally, we conduct\nextensive experiments on various large-scale tensors, whose results demonstrate\nthe practicability, effectiveness and superiority of the proposed method in\ncomparison with some state-of-the-art approaches.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09594v1", "AI": {"title_translation": "加速大规模正则化高阶张量恢复", "tldr": "现有张量恢复方法未能解决大规模高阶张量数据中张量尺度变化的影响和高昂的计算成本。本文提出两种快速准确的随机低秩张量近似算法，并开发了一种新的广义非凸建模框架，引入新的正则化范式。这些方法被集成到高阶张量恢复任务中，实验证明了其有效性和优越性。", "motivation": "现有张量恢复方法未能识别张量尺度变化对其结构特征的影响；现有研究在处理大规模高阶张量数据时面临高昂的计算成本。", "method": "1. 借助Krylov子空间迭代、块Lanczos双对角化过程和随机投影策略，设计了两种快速准确的随机低秩张量近似（LRTA）算法，并建立了近似误差估计的理论界限。2. 开发了一种针对大规模张量恢复的新型广义非凸建模框架，利用新的正则化范式实现大规模张量的深入先验表示。3. 进一步研究了用于未量化和量化情况下的典型高阶张量恢复任务的统一非凸模型和高效优化算法。4. 将提出的随机LRTA方案集成到算法的中心和耗时计算中，以提高其在大规模张量数据上的实用性和效率。", "result": "实验结果表明，所提出方法与一些最先进的方法相比，具有实用性、有效性和优越性。", "conclusion": "所提出的方法在大规模张量恢复任务中具有实用性、有效性和优越性，能够有效解决现有方法的局限性。", "translation": "目前，现有的张量恢复方法未能识别张量尺度变化对其结构特征的影响。此外，现有研究在处理大规模高阶张量数据时面临高昂的计算成本。为了缓解这些问题，本文借助Krylov子空间迭代、块Lanczos双对角化过程和随机投影策略，首先设计了两种用于低秩张量近似（LRTA）问题的快速准确的随机算法。建立了近似误差估计的理论界限。接下来，我们开发了一种针对大规模张量恢复的新型广义非凸建模框架，其中利用新的正则化范式来实现大规模张量的深入先验表示。在此基础上，我们进一步研究了在未量化和量化情况下的几种典型高阶张量恢复任务的新的统一非凸模型和高效优化算法。为了使所提出的算法对于大规模张量数据实用且高效，所提出的随机LRTA方案被集成到其中心和耗时计算中。最后，我们对各种大规模张量进行了广泛的实验，结果表明所提出方法与一些最先进的方法相比，具有实用性、有效性和优越性。", "summary": "本文旨在解决现有张量恢复方法在大规模高阶张量数据处理中面临的尺度变化识别不足和计算成本高昂的问题。为此，作者首先提出了两种基于Krylov子空间迭代、块Lanczos双对角化和随机投影的快速准确的随机低秩张量近似（LRTA）算法，并给出了理论误差界限。接着，开发了一种新的广义非凸建模框架，引入了新的正则化范式以更好地表示大规模张量。这些随机LRTA方案被集成到为多种高阶张量恢复任务设计的统一非凸模型和优化算法中。实验结果表明，所提出的方法在大规模张量数据上表现出优越的实用性、有效性和性能。", "keywords": "张量恢复, 低秩张量近似, 随机算法, 非凸优化, 大规模数据", "comments": "这篇论文的创新点在于结合了随机算法和非凸建模来解决大规模高阶张量恢复的计算效率和表示问题。通过引入随机低秩张量近似和新的正则化范式，它有效地提升了处理大规模复杂张量数据的能力，对于大数据分析和机器学习领域具有重要意义。"}}
{"id": "2506.09724", "title": "The Four Color Theorem for Cell Instance Segmentation", "authors": ["Ye Zhang", "Yu Zhou", "Yifeng Wang", "Jun Xiao", "Ziyue Wang", "Yongbing Zhang", "Jianxu Chen"], "summary": "Cell instance segmentation is critical to analyzing biomedical images, yet\naccurately distinguishing tightly touching cells remains a persistent\nchallenge. Existing instance segmentation frameworks, including\ndetection-based, contour-based, and distance mapping-based approaches, have\nmade significant progress, but balancing model performance with computational\nefficiency remains an open problem. In this paper, we propose a novel cell\ninstance segmentation method inspired by the four-color theorem. By\nconceptualizing cells as countries and tissues as oceans, we introduce a\nfour-color encoding scheme that ensures adjacent instances receive distinct\nlabels. This reformulation transforms instance segmentation into a constrained\nsemantic segmentation problem with only four predicted classes, substantially\nsimplifying the instance differentiation process. To solve the training\ninstability caused by the non-uniqueness of four-color encoding, we design an\nasymptotic training strategy and encoding transformation method. Extensive\nexperiments on various modes demonstrate our approach achieves state-of-the-art\nperformance. The code is available at https://github.com/zhangye-zoe/FCIS.", "comment": "Accepted at ICML 2025", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09724v1", "AI": {"title_translation": "四色定理在细胞实例分割中的应用", "tldr": "受四色定理启发，本文提出了一种新颖的细胞实例分割方法，将实例分割转化为四类语义分割问题，实现了最先进的性能。", "motivation": "细胞实例分割在生物医学图像分析中至关重要，但精确区分紧密接触的细胞仍然是一个挑战。现有方法在模型性能和计算效率之间难以平衡。", "method": "提出了一种受四色定理启发的细胞实例分割方法。通过将细胞概念化为国家，组织概念化为海洋，引入四色编码方案，确保相邻实例获得不同的标签。这使得实例分割转化为一个只有四类预测的受限语义分割问题。为解决四色编码非唯一性导致的训练不稳定性，设计了渐近训练策略和编码转换方法。", "result": "在各种模式下进行的广泛实验表明，该方法实现了最先进的性能。", "conclusion": "该研究成功地将四色定理应用于细胞实例分割，通过创新的编码和训练策略，解决了紧密接触细胞的区分难题，并达到了领先的性能。", "translation": "细胞实例分割对于分析生物医学图像至关重要，然而准确区分紧密接触的细胞仍然是一个持续的挑战。现有的实例分割框架，包括基于检测、基于轮廓和基于距离映射的方法，都取得了显著进展，但平衡模型性能与计算效率仍然是一个开放问题。在本文中，我们提出了一种受四色定理启发的新型细胞实例分割方法。通过将细胞概念化为国家，将组织概念化为海洋，我们引入了一种四色编码方案，确保相邻实例获得不同的标签。这种重新表述将实例分割转化为一个只有四个预测类别的受限语义分割问题，大大简化了实例区分过程。为了解决四色编码非唯一性导致的训练不稳定性，我们设计了一种渐近训练策略和编码转换方法。在各种模式下进行的广泛实验表明，我们的方法实现了最先进的性能。代码可在 https://github.com/zhangye-zoe/FCIS 获取。", "summary": "本研究提出了一种基于四色定理的创新性细胞实例分割方法，旨在解决生物医学图像中紧密接触细胞的精确区分难题。该方法将细胞实例分割重新定义为四类受限语义分割问题，通过独特的四色编码方案确保相邻细胞标签不同，并设计了渐近训练策略以应对编码非唯一性。实验证明，该方法在性能上达到了最先进水平，有效提升了细胞分析的准确性与效率。", "keywords": "细胞实例分割, 四色定理, 语义分割, 图像分析, 深度学习", "comments": "该论文的创新点在于巧妙地将经典的四色定理应用于细胞实例分割，将复杂的实例区分问题简化为语义分割，极大地降低了任务难度。通过引入四色编码和渐近训练策略，解决了传统方法在处理紧密接触细胞时的挑战和计算效率问题。这种转化思路具有很强的借鉴意义，为未来的实例分割研究提供了新视角。"}}
{"id": "2506.09613", "title": "SparseSSM: Efficient Selective Structured State Space Models Can Be Pruned in One-Shot", "authors": ["Kaiwen Tuo", "Huan Wang"], "summary": "State-space language models such as Mamba match Transformer quality while\npermitting linear complexity inference, yet still comprise billions of\nparameters that hinder deployment. Existing one-shot pruning methods are\ntailored to attention blocks and fail to account for the time-shared and\ndiscretized state-transition matrix at the heart of the selective state-space\nmodule (SSM). In this paper, we introduce SparseSSM, the first training-free\npruning framework that extends the classic optimal brain surgeon (OBS)\nframework to state space architectures. Our layer-wise algorithm (i) derives an\napproximate second-order saliency score that aggregates Hessian-trace\ninformation across time steps, (ii) incorporates a component sensitivity\nanalysis to guide feed-forward network (FFN) pruning, which also sheds light on\nwhere redundancy resides in mamba architecture, (iii) can be easily extended to\nsemi-structured and structured sparsity. Empirically, we prune 50% of SSM\nweights without fine-tuning and observe no zero-shot accuracy loss, achieving\nthe current state-of-the-art pruning algorithm for Mamba-based LLMs.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09613v1", "AI": {"title_translation": "SparseSSM：高效选择性结构化状态空间模型可一次性剪枝", "tldr": "SparseSSM是首个针对状态空间模型（如Mamba）的免训练剪枝框架，它扩展了经典OBS方法，能一次性剪枝50%的SSM权重而无精度损失，实现了Mamba-based LLM剪枝的SOTA。", "motivation": "当前Mamba等状态空间语言模型参数量巨大（数十亿），阻碍了部署。现有的一次性剪枝方法专为注意力模块设计，无法有效应用于状态空间模型中时间共享和离散化的状态转移矩阵。", "method": "本文引入了SparseSSM，这是首个将经典最优脑外科医生（OBS）框架扩展到状态空间架构的免训练剪枝框架。其分层算法：(i) 推导了聚合Hessian迹信息以跨时间步的近似二阶显著性得分；(ii) 结合了组件敏感性分析来指导前馈网络（FFN）剪枝，并揭示了Mamba架构中的冗余所在；(iii) 可轻松扩展到半结构化和结构化稀疏性。", "result": "经验证明，SparseSSM可以在不进行微调的情况下剪枝50%的SSM权重，且零样本准确率没有损失，达到了Mamba-based LLM剪枝算法的当前最先进水平。", "conclusion": "SparseSSM成功解决了状态空间模型（如Mamba）的参数冗余问题，实现了高效且无损的剪枝，显著提升了这些大型模型的部署可行性。", "translation": "状态空间语言模型，如Mamba，在保持与Transformer相当的质量的同时，允许线性复杂度的推理，但仍包含数十亿参数，这阻碍了部署。现有的一次性剪枝方法是为注意力块量身定制的，未能考虑到选择性状态空间模块（SSM）核心的时间共享和离散化状态转移矩阵。在本文中，我们引入了SparseSSM，这是第一个将经典最优脑外科医生（OBS）框架扩展到状态空间架构的免训练剪枝框架。我们的分层算法（i）推导了一个近似的二阶显著性分数，该分数聚合了跨时间步的Hessian迹信息，（ii）结合了组件敏感性分析来指导前馈网络（FFN）剪枝，这也揭示了Mamba架构中冗余的所在，（iii）可以很容易地扩展到半结构化和结构化稀疏性。从经验上看，我们在不进行微调的情况下剪枝了50%的SSM权重，并且没有观察到零样本准确率损失，实现了Mamba-based LLM的当前最先进剪枝算法。", "summary": "SparseSSM是一种新颖的免训练剪枝框架，专门用于状态空间语言模型，如Mamba。它克服了现有剪枝方法不适用于SSM的局限性，通过扩展经典的OBS框架并引入分层算法，有效识别和剪枝模型冗余。实验结果表明，SparseSSM能够在不损失零样本准确性的前提下，一次性剪枝50%的SSM权重，使其成为Mamba-based LLM剪枝领域的最新技术。", "keywords": "SparseSSM, 状态空间模型, 剪枝, Mamba, 大语言模型", "comments": "该论文的创新点在于首次将OBS框架扩展到状态空间模型，实现了对Mamba等大型模型的训练-free剪枝。其重要性在于解决了大型状态空间模型部署的障碍，通过显著减少参数量而保持性能，为未来高效部署SSM提供了关键技术。该方法能够在一次性剪枝50%权重的同时保持零精度损失，显示出极高的实用价值和技术先进性。"}}
{"id": "2506.09735", "title": "MPFNet: A Multi-Prior Fusion Network with a Progressive Training Strategy for Micro-Expression Recognition", "authors": ["Chuang Ma", "Shaokai Zhao", "Dongdong Zhou", "Yu Pei", "Zhiguo Luo", "Liang Xie", "Ye Yan", "Erwei Yin"], "summary": "Micro-expression recognition (MER), a critical subfield of affective\ncomputing, presents greater challenges than macro-expression recognition due to\nits brief duration and low intensity. While incorporating prior knowledge has\nbeen shown to enhance MER performance, existing methods predominantly rely on\nsimplistic, singular sources of prior knowledge, failing to fully exploit\nmulti-source information. This paper introduces the Multi-Prior Fusion Network\n(MPFNet), leveraging a progressive training strategy to optimize MER tasks. We\npropose two complementary encoders: the Generic Feature Encoder (GFE) and the\nAdvanced Feature Encoder (AFE), both based on Inflated 3D ConvNets (I3D) with\nCoordinate Attention (CA) mechanisms, to improve the model's ability to capture\nspatiotemporal and channel-specific features. Inspired by developmental\npsychology, we present two variants of MPFNet--MPFNet-P and\nMPFNet-C--corresponding to two fundamental modes of infant cognitive\ndevelopment: parallel and hierarchical processing. These variants enable the\nevaluation of different strategies for integrating prior knowledge. Extensive\nexperiments demonstrate that MPFNet significantly improves MER accuracy while\nmaintaining balanced performance across categories, achieving accuracies of\n0.811, 0.924, and 0.857 on the SMIC, CASME II, and SAMM datasets, respectively.\nTo the best of our knowledge, our approach achieves state-of-the-art\nperformance on the SMIC and SAMM datasets.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09735v1", "AI": {"title_translation": "MPFNet: 一种用于微表情识别的渐进式训练多先验融合网络", "tldr": "MPFNet提出了一种多先验融合网络和渐进式训练策略，显著提升了微表情识别的准确性，并在多个数据集上达到了SOTA性能。", "motivation": "微表情识别（MER）因其持续时间短、强度低，比宏表情识别更具挑战性。现有方法主要依赖单一先验知识源，未能充分利用多源信息，限制了MER的性能。", "method": "本文提出了多先验融合网络（MPFNet），采用渐进式训练策略优化MER任务。设计了基于膨胀3D卷积网络（I3D）和坐标注意力（CA）机制的通用特征编码器（GFE）和高级特征编码器（AFE），以捕获时空和通道特定特征。受发展心理学启发，提出了MPFNet的两种变体：MPFNet-P（并行处理）和MPFNet-C（分层处理），用于评估不同的先验知识融合策略。", "result": "MPFNet显著提高了MER准确性，并在各类别间保持了平衡性能。在SMIC、CASME II和SAMM数据集上分别达到了0.811、0.924和0.857的准确率。据作者所知，该方法在SMIC和SAMM数据集上达到了最先进的性能（SOTA）。", "conclusion": "MPFNet通过有效融合多源先验知识和采用渐进式训练策略，显著提升了微表情识别的性能，并在多个基准数据集上取得了领先成果，表明了其在MER任务中的有效性和优越性。", "translation": "微表情识别（MER）是情感计算的一个关键子领域，由于其持续时间短、强度低，比宏表情识别面临更大的挑战。虽然结合先验知识已被证明可以提高MER性能，但现有方法主要依赖于简单、单一的先验知识来源，未能充分利用多源信息。本文介绍了多先验融合网络（MPFNet），利用渐进式训练策略来优化MER任务。我们提出了两个互补的编码器：通用特征编码器（GFE）和高级特征编码器（AFE），两者都基于膨胀3D卷积网络（I3D）和坐标注意力（CA）机制，以提高模型捕获时空和通道特定特征的能力。受发展心理学启发，我们提出了MPFNet的两种变体——MPFNet-P和MPFNet-C——对应于婴儿认知发展的两种基本模式：并行和分层处理。这些变体能够评估集成先验知识的不同策略。大量实验表明，MPFNet显著提高了MER准确性，同时在各类别间保持了平衡性能，在SMIC、CASME II和SAMM数据集上分别达到了0.811、0.924和0.857的准确率。据我们所知，我们的方法在SMIC和SAMM数据集上取得了最先进的性能。", "summary": "本文提出了一种名为MPFNet的多先验融合网络，旨在解决微表情识别（MER）中现有方法未能充分利用多源先验知识的问题。MPFNet采用渐进式训练策略，并包含通用特征编码器（GFE）和高级特征编码器（AFE）来捕获丰富的时空特征。受发展心理学启发，研究了并行和分层两种先验知识融合策略。实验结果表明，MPFNet在多个MER数据集上显著提升了识别准确率，并在SMIC和SAMM数据集上达到了最先进的性能。", "keywords": "微表情识别, 多先验融合, 渐进式训练, 3D卷积网络, 注意力机制", "comments": "该论文的创新点在于提出了一个多先验融合网络（MPFNet），通过结合多源先验知识和采用渐进式训练策略来解决微表情识别的挑战。特别是，受发展心理学启发的两种网络变体（MPFNet-P和MPFNet-C）为先验知识的融合提供了新的视角。其在多个数据集上取得的SOTA性能，证明了该方法的有效性和重要性。"}}
{"id": "2506.09338", "title": "Know What You Don't Know: Uncertainty Calibration of Process Reward Models", "authors": ["Young-Jin Park", "Kristjan Greenewald", "Kaveh Alim", "Hao Wang", "Navid Azizan"], "summary": "Process reward models (PRMs) play a central role in guiding inference-time\nscaling algorithms for large language models (LLMs). However, we observe that\neven state-of-the-art PRMs can be poorly calibrated and often overestimate\nsuccess probabilities. To address this, we present a calibration approach,\nperformed via quantile regression, that adjusts PRM outputs to better align\nwith true success probabilities. Leveraging these calibrated success estimates\nand their associated confidence bounds, we introduce an \\emph{instance-adaptive\nscaling} (IAS) framework that dynamically adjusts the inference budget based on\nthe estimated likelihood that a partial reasoning trajectory will yield a\ncorrect final answer. Unlike conventional methods that allocate a fixed number\nof reasoning trajectories per query, this approach successfully adapts to each\ninstance and reasoning step when using our calibrated PRMs. Experiments on\nmathematical reasoning benchmarks show that (i) our PRM calibration method\nsuccessfully achieves small calibration error, outperforming the baseline\nmethods, (ii) calibration is crucial for enabling effective adaptive scaling,\nand (iii) the proposed IAS strategy reduces inference costs while maintaining\nfinal answer accuracy, utilizing less compute on more confident problems as\ndesired.", "comment": null, "cate": "stat.ML", "url": "http://arxiv.org/abs/2506.09338v1", "AI": {"title_translation": "知你所不知：过程奖励模型的不确定性校准", "tldr": "针对过程奖励模型（PRMs）的校准问题，提出了一种基于分位数回归的校准方法和实例自适应推理缩放（IAS）框架，有效降低了推理成本并保持了准确性。", "motivation": "过程奖励模型（PRMs）在指导大型语言模型（LLMs）的推理时缩放算法中发挥核心作用，但现有PRMs校准性差，常高估成功概率。", "method": "提出了一种通过分位数回归进行的校准方法，以调整PRM输出，使其更好地与真实成功概率对齐。利用这些校准后的成功估计及其置信区间，引入了一个实例自适应缩放（IAS）框架，该框架根据部分推理轨迹产生正确最终答案的估计可能性动态调整推理预算。", "result": "实验表明：(i) PRM校准方法成功实现了小的校准误差，优于基线方法；(ii) 校准对于实现有效的自适应缩放至关重要；(iii) 所提出的IAS策略在保持最终答案准确性的同时降低了推理成本，对更确信的问题使用更少的计算资源。", "conclusion": "校准过程奖励模型对于实现大型语言模型的有效自适应推理缩放至关重要，能够显著降低推理成本同时保持高准确性。", "translation": "过程奖励模型（PRMs）在指导大型语言模型（LLMs）的推理时缩放算法中发挥核心作用。然而，我们观察到即使是最先进的PRMs也可能校准性差，并且经常高估成功概率。为了解决这个问题，我们提出了一种校准方法，通过分位数回归进行，调整PRM输出以更好地与真实成功概率对齐。利用这些校准后的成功估计及其相关的置信区间，我们引入了一个“实例自适应缩放”（IAS）框架，该框架根据部分推理轨迹产生正确最终答案的估计可能性动态调整推理预算。与每查询分配固定数量推理轨迹的传统方法不同，这种方法在使用我们校准的PRMs时，成功地适应了每个实例和推理步骤。在数学推理基准上的实验表明：(i) 我们的PRM校准方法成功实现了小的校准误差，优于基线方法；(ii) 校准对于实现有效的自适应缩放至关重要；以及 (iii) 所提出的IAS策略在保持最终答案准确性的同时降低了推理成本，对更确信的问题按需使用更少的计算资源。", "summary": "本文提出了一种针对过程奖励模型（PRMs）的不确定性校准方法，通过分位数回归调整PRM输出，使其与真实成功概率对齐。在此基础上，引入了实例自适应缩放（IAS）框架，该框架能根据估计的成功可能性动态调整大型语言模型（LLMs）的推理预算。实验证明，该校准方法有效降低了校准误差，并且IAS策略在保持准确性的同时显著降低了推理成本。", "keywords": "过程奖励模型, 不确定性校准, 分位数回归, 实例自适应缩放, 大型语言模型", "comments": "这项工作在提高大型语言模型推理效率方面具有重要意义。通过对过程奖励模型进行不确定性校准，并引入实例自适应缩放机制，论文有效地解决了现有PRMs校准性差导致推理资源浪费的问题。这种动态调整推理预算的方法，能够根据问题的复杂度和模型的置信度智能分配计算资源，是LLM推理优化领域的一个创新且实用的贡献。"}}
{"id": "2506.09625", "title": "GLGENN: A Novel Parameter-Light Equivariant Neural Networks Architecture Based on Clifford Geometric Algebras", "authors": ["Ekaterina Filimoshina", "Dmitry Shirokov"], "summary": "We propose, implement, and compare with competitors a new architecture of\nequivariant neural networks based on geometric (Clifford) algebras: Generalized\nLipschitz Group Equivariant Neural Networks (GLGENN). These networks are\nequivariant to all pseudo-orthogonal transformations, including rotations and\nreflections, of a vector space with any non-degenerate or degenerate symmetric\nbilinear form. We propose a weight-sharing parametrization technique that takes\ninto account the fundamental structures and operations of geometric algebras.\nDue to this technique, GLGENN architecture is parameter-light and has less\ntendency to overfitting than baseline equivariant models. GLGENN outperforms or\nmatches competitors on several benchmarking equivariant tasks, including\nestimation of an equivariant function and a convex hull experiment, while using\nsignificantly fewer optimizable parameters.", "comment": "Accepted to ICML 2025", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09625v1", "AI": {"title_translation": "GLGENN：一种基于Clifford几何代数的新型轻量级等变神经网络架构", "tldr": "本文提出了一种名为GLGENN的新型等变神经网络架构，它基于Clifford几何代数，对伪正交变换等变，参数量少且不易过拟合，在基准任务上表现优异。", "motivation": "现有的等变神经网络可能存在参数量大、容易过拟合的问题，因此需要一种新的、更高效的等变神经网络架构。", "method": "本文提出了一种基于几何（Clifford）代数的广义Lipschitz群等变神经网络（GLGENN）。该网络对任意向量空间的所有伪正交变换（包括旋转和反射）都具有等变性。通过引入一种考虑几何代数基本结构和操作的权重共享参数化技术，实现了参数轻量化。", "result": "GLGENN在多个基准等变任务（包括等变函数估计和凸包实验）上超越或媲美竞争对手，同时使用了显著更少的可优化参数，并且比基线等变模型更不容易过拟合。", "conclusion": "GLGENN是一种有效且高效的等变神经网络架构，它通过创新的参数化技术显著减少了参数量，提高了性能，并降低了过拟合倾向。", "translation": "我们提出、实现并与竞争对手比较了一种基于几何（Clifford）代数的新型等变神经网络架构：广义Lipschitz群等变神经网络（GLGENN）。这些网络对任意非退化或退化对称双线性形式的向量空间的所有伪正交变换（包括旋转和反射）都具有等变性。我们提出了一种考虑几何代数基本结构和操作的权重共享参数化技术。由于这项技术，GLGENN架构参数轻量化，并且比基线等变模型更不容易过拟合。GLGENN在多个基准等变任务（包括等变函数估计和凸包实验）上超越或媲美竞争对手，同时使用了显著更少的可优化参数。", "summary": "本文提出了一种名为GLGENN的新型等变神经网络架构，其基于Clifford几何代数。GLGENN对伪正交变换等变，并采用独特的权重共享参数化技术，使其参数量显著减少，从而降低了过拟合的倾向。实验结果表明，GLGENN在多个等变任务上表现优异或与现有模型持平，但参数效率更高。", "keywords": "等变神经网络, Clifford几何代数, 参数轻量化, 权重共享, 伪正交变换", "comments": "GLGENN的创新点在于将Clifford几何代数与权重共享参数化技术相结合，显著减少了等变神经网络的参数量，并有效解决了过拟合问题。这对于开发更高效、更鲁棒的等变模型具有重要意义，尤其是在处理高维数据和需要保持特定对称性的任务中。"}}
{"id": "2506.09736", "title": "Vision Matters: Simple Visual Perturbations Can Boost Multimodal Math Reasoning", "authors": ["Yuting Li", "Lai Wei", "Kaipeng Zheng", "Jingyuan Huang", "Linghe Kong", "Lichao Sun", "Weiran Huang"], "summary": "Despite the rapid progress of multimodal large language models (MLLMs), they\nhave largely overlooked the importance of visual processing. In a simple yet\nrevealing experiment, we interestingly find that language-only models, when\nprovided with image captions, can achieve comparable or even better performance\nthan MLLMs that consume raw visual inputs. This suggests that current MLLMs may\ngenerate accurate visual descriptions but fail to effectively integrate them\nduring reasoning. Motivated by this, we propose a simple visual perturbation\nframework that enhances perceptual robustness without requiring algorithmic\nmodifications or additional training data. Our approach introduces three\ntargeted perturbations: distractor concatenation, dominance-preserving mixup,\nand random rotation, that can be easily integrated into existing post-training\npipelines including SFT, DPO, and GRPO. Through extensive experiments across\nmultiple datasets, we demonstrate consistent improvements in mathematical\nreasoning performance, with gains comparable to those achieved through\nalgorithmic changes. Additionally, we achieve competitive performance among\nopen-source 7B RL-tuned models by training Qwen2.5-VL-7B with visual\nperturbation. Through comprehensive ablation studies, we analyze the\neffectiveness of different perturbation strategies, revealing that each\nperturbation type contributes uniquely to different aspects of visual\nreasoning. Our findings highlight the critical role of visual perturbation in\nmultimodal mathematical reasoning: better reasoning begins with better seeing.\nOur code is available at https://github.com/YutingLi0606/Vision-Matters.", "comment": "Technical Report", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09736v1", "AI": {"title_translation": "视觉至关重要：简单的视觉扰动可以促进多模态数学推理", "tldr": "本研究发现，简单的视觉扰动框架可以显著提升多模态大语言模型在数学推理方面的性能，且无需算法修改或额外数据，这表明更好的推理始于更好的视觉处理。", "motivation": "尽管多模态大语言模型（MLLMs）取得了快速进展，但它们在视觉处理方面的重要性被忽视。实验发现，仅提供图像字幕的纯语言模型，其性能可与处理原始视觉输入的MLLMs媲美甚至更好，这表明当前MLLMs可能生成准确的视觉描述但未能有效整合进行推理。", "method": "提出一个简单的视觉扰动框架，通过引入三种有针对性的扰动：干扰物拼接、保持主导性的混合以及随机旋转，来增强感知鲁棒性。这些扰动可以轻松集成到现有的后训练流程中，包括SFT、DPO和GRPO，且无需算法修改或额外训练数据。", "result": "在多个数据集上，数学推理性能持续得到改善，其提升可与算法更改带来的增益相媲美。通过视觉扰动训练Qwen2.5-VL-7B，在开源7B RL调优模型中取得了有竞争力的性能。消融研究表明，每种扰动类型对视觉推理的不同方面都有独特贡献。", "conclusion": "研究结果强调了视觉扰动在多模态数学推理中的关键作用：更好的推理始于更好的视觉。", "translation": "尽管多模态大语言模型（MLLMs）取得了快速进展，但它们在很大程度上忽视了视觉处理的重要性。在一个简单而富有启发性的实验中，我们有趣地发现，仅提供图像字幕的纯语言模型，其性能可以与处理原始视觉输入的MLLMs相媲美甚至更好。这表明当前的MLLMs可能生成准确的视觉描述，但在推理过程中未能有效地整合它们。受此启发，我们提出了一个简单的视觉扰动框架，该框架无需算法修改或额外训练数据即可增强感知鲁棒性。我们的方法引入了三种有针对性的扰动：干扰物拼接、保持主导性的混合以及随机旋转，它们可以轻松地集成到现有的后训练流程中，包括SFT、DPO和GRPO。通过在多个数据集上进行大量实验，我们证明了数学推理性能的持续改进，其增益可与通过算法更改实现的增益相媲美。此外，通过对Qwen2.5-VL-7B进行视觉扰动训练，我们在开源7B RL调优模型中取得了有竞争力的性能。通过全面的消融研究，我们分析了不同扰动策略的有效性，揭示了每种扰动类型对视觉推理的不同方面都有独特的贡献。我们的研究结果强调了视觉扰动在多模态数学推理中的关键作用：更好的推理始于更好的视觉。我们的代码可在https://github.com/YutingLi0606/Vision-Matters获取。", "summary": "本论文提出一个简单的视觉扰动框架，旨在提升多模态大语言模型（MLLMs）在数学推理任务中的表现。研究发现，当前MLLMs在整合视觉信息进行推理时存在不足。通过引入干扰物拼接、保持主导性的混合和随机旋转这三种扰动，该框架无需修改算法或额外数据即可增强模型的感知鲁棒性。实验证明，该方法能持续提升数学推理性能，并使模型在开源基准上达到竞争性水平，强调了视觉扰动在多模态推理中的关键作用。", "keywords": "多模态数学推理, 视觉扰动, MLLMs, 感知鲁棒性, 语言模型", "comments": "这篇论文的创新点在于提出了一个简单而有效的视觉扰动框架，它无需复杂的算法修改或额外的训练数据，就能显著提升多模态模型在数学推理任务上的表现。这挑战了当前MLLMs对原始视觉输入的处理方式，并强调了“更好地看”对于“更好地推理”的重要性。其方法的可插拔性（易于集成到现有后训练流程）是其重要的实践价值。"}}
{"id": "2506.09340", "title": "RePO: Replay-Enhanced Policy Optimization", "authors": ["Siheng Li", "Zhanhui Zhou", "Wai Lam", "Chao Yang", "Chaochao Lu"], "summary": "Reinforcement learning (RL) is vital for optimizing large language models\n(LLMs). Recent Group Relative Policy Optimization (GRPO) estimates advantages\nusing multiple on-policy outputs per prompt, leading to high computational\ncosts and low data efficiency. To address this, we introduce Replay-Enhanced\nPolicy Optimization (RePO), which leverages diverse replay strategies to\nretrieve off-policy samples from a replay buffer, allowing policy optimization\nbased on a broader and more diverse set of samples for each prompt. Experiments\non five LLMs across seven mathematical reasoning benchmarks demonstrate that\nRePO achieves absolute average performance gains of $18.4$ and $4.1$ points for\nQwen2.5-Math-1.5B and Qwen3-1.7B, respectively, compared to GRPO. Further\nanalysis indicates that RePO increases computational cost by $15\\%$ while\nraising the number of effective optimization steps by $48\\%$ for Qwen3-1.7B,\nwith both on-policy and off-policy sample numbers set to $8$. The repository\ncan be accessed at https://github.com/SihengLi99/RePO.", "comment": "Project Page: https://github.com/SihengLi99/RePO", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.09340v1", "AI": {"title_translation": "RePO: 回放增强策略优化", "tldr": "提出RePO，通过利用回放缓冲区中的离策略样本，显著提高了LLM在数学推理任务上的RL优化效率和性能，同时控制了计算成本。", "motivation": "现有组相对策略优化（GRPO）方法在优化大型语言模型（LLM）时计算成本高且数据效率低。", "method": "引入回放增强策略优化（RePO），利用多样化的回放策略从回放缓冲区中检索离策略样本，从而基于更广泛和多样化的样本集进行策略优化。", "result": "在七个数学推理基准上，RePO在Qwen2.5-Math-1.5B和Qwen3-1.7B上分别比GRPO实现了18.4和4.1个点的绝对平均性能提升。计算成本增加15%，有效优化步骤增加48%。", "conclusion": "RePO通过有效利用离策略样本，显著提高了大型语言模型在强化学习优化中的性能和数据效率。", "translation": "强化学习（RL）对于优化大型语言模型（LLM）至关重要。最近的组相对策略优化（GRPO）通过每个提示的多个在策略输出估计优势，导致计算成本高且数据效率低。为了解决这个问题，我们引入了回放增强策略优化（RePO），它利用多样化的回放策略从回放缓冲区中检索离策略样本，从而允许基于每个提示的更广泛和多样化的样本集进行策略优化。在七个数学推理基准上对五个LLM进行的实验表明，与GRPO相比，RePO在Qwen2.5-Math-1.5B和Qwen3-1.7B上分别实现了18.4和4.1个点的绝对平均性能提升。进一步分析表明，当在策略和离策略样本数量都设置为8时，RePO使Qwen3-1.7B的计算成本增加了15%，同时有效优化步骤增加了48%。代码库可在https://github.com/SihengLi99/RePO 访问。", "summary": "本文提出了RePO（回放增强策略优化），旨在解决现有GRPO方法在优化大型语言模型（LLM）时存在的计算成本高和数据效率低的问题。RePO通过从回放缓冲区中利用多样化的离策略样本进行策略优化，显著提高了数据利用率。实验结果表明，RePO在多个数学推理基准上，尤其是在Qwen2.5-Math-1.5B和Qwen3-1.7B模型上，取得了显著的性能提升，同时有效优化步骤大幅增加，而计算成本增幅可控。", "keywords": "强化学习, 大型语言模型, 策略优化, 回放缓冲区, 数学推理", "comments": "RePO的创新点在于将回放机制引入到LLM的策略优化中，有效解决了现有在策略RL方法的计算成本和数据效率问题。通过利用离策略样本，RePO能够更高效地学习和优化LLM的性能，尤其是在数学推理等复杂任务上表现出色。这对于LLM的RLHF（强化学习与人类反馈）优化具有重要意义。"}}
{"id": "2506.09630", "title": "In-Context Bias Propagation in LLM-Based Tabular Data Generation", "authors": ["Pol G. Recasens", "Alberto Gutierrez", "Jordi Torres", "Josep. Ll Berral", "Anisa Halimi", "Kieran Fraser"], "summary": "Large Language Models (LLMs) are increasingly used for synthetic tabular data\ngeneration through in-context learning (ICL), offering a practical solution for\ndata augmentation in data scarce scenarios. While prior work has shown the\npotential of LLMs to improve downstream task performance through augmenting\nunderrepresented groups, these benefits often assume access to a subset of\nunbiased in-context examples, representative of the real dataset. In real-world\nsettings, however, data is frequently noisy and demographically skewed. In this\npaper, we systematically study how statistical biases within in-context\nexamples propagate to the distribution of synthetic tabular data, showing that\neven mild in-context biases lead to global statistical distortions. We further\nintroduce an adversarial scenario where a malicious contributor can inject bias\ninto the synthetic dataset via a subset of in-context examples, ultimately\ncompromising the fairness of downstream classifiers for a targeted and\nprotected subgroup. Our findings demonstrate a new vulnerability associated\nwith LLM-based data generation pipelines that rely on in-context prompts with\nin sensitive domains.", "comment": "Paper accepted at ICML 2025 workshop DIG-BUG", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09630v1", "AI": {"title_translation": "LLM生成表格数据中的上下文偏差传播", "tldr": "研究发现，LLM基于上下文学习生成的表格数据会传播输入中的统计偏差，甚至微小的偏差也会导致全局扭曲，恶意注入偏差可能损害下游分类器的公平性，揭示了LLM数据生成的新漏洞。", "motivation": "尽管大型语言模型（LLM）在数据稀缺场景中通过上下文学习（ICL）生成合成表格数据具有潜力，但以往工作通常假设上下文示例是无偏的。然而，在现实世界中，数据往往是嘈杂且存在人口统计学偏差的，因此需要研究这种偏差如何传播到合成数据中。", "method": "本文系统地研究了上下文示例中的统计偏差如何传播到合成表格数据的分布中。此外，还引入了一个对抗性场景，模拟恶意贡献者通过上下文示例注入偏差，以损害下游分类器的公平性。", "result": "研究表明，即使是轻微的上下文偏差也会导致合成表格数据的全局统计扭曲。在对抗性场景下，恶意注入的偏差能够损害目标受保护子群体的下游分类器的公平性。", "conclusion": "本文揭示了LLM基于上下文提示在敏感领域生成数据时存在的新漏洞，即上下文中的偏差会传播并对合成数据及下游任务的公平性产生负面影响。", "translation": "大型语言模型（LLM）正越来越多地通过上下文学习（ICL）用于合成表格数据生成，为数据稀缺场景中的数据增强提供了一种实用解决方案。虽然先前的研究表明LLM通过增强代表性不足的群体来提高下游任务性能的潜力，但这些益处通常假设能够访问一部分无偏的、代表真实数据集的上下文示例。然而，在现实世界中，数据通常是嘈杂的且存在人口统计学偏差。在本文中，我们系统地研究了上下文示例中的统计偏差如何传播到合成表格数据的分布中，结果表明即使是轻微的上下文偏差也会导致全局统计扭曲。我们进一步引入了一个对抗性场景，其中恶意贡献者可以通过一部分上下文示例将偏差注入合成数据集，最终损害目标受保护子群体的下游分类器的公平性。我们的研究结果揭示了依赖于敏感领域上下文提示的基于LLM的数据生成管道所关联的新漏洞。", "summary": "本文研究了大型语言模型（LLM）在通过上下文学习生成合成表格数据时，如何将输入中的统计偏差传播到生成的数据分布中。研究发现，即使是轻微的上下文偏差也会导致合成数据的全局统计扭曲。此外，论文还展示了恶意注入偏差如何通过上下文示例损害下游分类器对特定受保护子群体的公平性，揭示了LLM数据生成管道在敏感领域的新漏洞。", "keywords": "LLM, 上下文学习, 偏差传播, 表格数据生成, 数据公平性", "comments": "这项研究揭示了LLM在数据生成方面的一个重要且被忽视的漏洞，特别是在处理敏感数据或需要公平性的应用中。它强调了在利用LLM进行数据增强时，对上下文示例质量和潜在偏差传播进行严格审查的重要性。这项工作对于确保AI系统公平性和鲁棒性具有重要意义。"}}
{"id": "2506.09740", "title": "ELBO-T2IAlign: A Generic ELBO-Based Method for Calibrating Pixel-level Text-Image Alignment in Diffusion Models", "authors": ["Qin Zhou", "Zhiyang Zhang", "Jinglong Wang", "Xiaobin Li", "Jing Zhang", "Qian Yu", "Lu Sheng", "Dong Xu"], "summary": "Diffusion models excel at image generation. Recent studies have shown that\nthese models not only generate high-quality images but also encode text-image\nalignment information through attention maps or loss functions. This\ninformation is valuable for various downstream tasks, including segmentation,\ntext-guided image editing, and compositional image generation. However, current\nmethods heavily rely on the assumption of perfect text-image alignment in\ndiffusion models, which is not the case. In this paper, we propose using\nzero-shot referring image segmentation as a proxy task to evaluate the\npixel-level image and class-level text alignment of popular diffusion models.\nWe conduct an in-depth analysis of pixel-text misalignment in diffusion models\nfrom the perspective of training data bias. We find that misalignment occurs in\nimages with small sized, occluded, or rare object classes. Therefore, we\npropose ELBO-T2IAlign, a simple yet effective method to calibrate pixel-text\nalignment in diffusion models based on the evidence lower bound (ELBO) of\nlikelihood. Our method is training-free and generic, eliminating the need to\nidentify the specific cause of misalignment and works well across various\ndiffusion model architectures. Extensive experiments on commonly used benchmark\ndatasets on image segmentation and generation have verified the effectiveness\nof our proposed calibration approach.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09740v1", "AI": {"title_translation": "ELBO-T2IAlign：一种基于ELBO的通用方法，用于校准扩散模型中的像素级文本-图像对齐", "tldr": "提出ELBO-T2IAlign，一个无需训练的通用方法，基于ELBO校准扩散模型中的像素级文本-图像对齐，解决了对齐不完美的问题。", "motivation": "扩散模型虽然能生成高质量图像并编码文本-图像对齐信息，但现有方法依赖于扩散模型中完美的文本-图像对齐假设，而实际上这种对齐并不完美。特别是在小尺寸、遮挡或稀有物体类别图像中存在像素级文本-图像错位。", "method": "使用零样本参照图像分割作为代理任务来评估流行扩散模型的像素级图像和类别级文本对齐。深入分析了训练数据偏差导致的像素-文本错位。提出了ELBO-T2IAlign，一种基于似然证据下界（ELBO）来校准扩散模型中像素-文本对齐的简单有效方法。该方法无需训练且通用。", "result": "发现错位发生在小尺寸、遮挡或稀有物体类别的图像中。在图像分割和生成常用基准数据集上的广泛实验验证了所提出校准方法的有效性。", "conclusion": "ELBO-T2IAlign是一种无需训练且通用的方法，能有效校准扩散模型中的像素级文本-图像对齐，解决了现有模型中对齐不完美的问题，并适用于各种扩散模型架构。", "translation": "扩散模型在图像生成方面表现出色。最近的研究表明，这些模型不仅能生成高质量图像，还能通过注意力图或损失函数编码文本-图像对齐信息。这些信息对于各种下游任务，包括分割、文本引导图像编辑和组合图像生成都很有价值。然而，当前方法严重依赖于扩散模型中完美的文本-图像对齐假设，而事实并非如此。在本文中，我们提出使用零样本参照图像分割作为代理任务来评估流行扩散模型的像素级图像和类别级文本对齐。我们从训练数据偏差的角度对扩散模型中的像素-文本错位进行了深入分析。我们发现错位发生在小尺寸、遮挡或稀有物体类别的图像中。因此，我们提出ELBO-T2IAlign，一种简单而有效的方法，基于似然证据下界（ELBO）来校准扩散模型中的像素-文本对齐。我们的方法无需训练且通用，无需识别错位的具体原因，并且适用于各种扩散模型架构。在图像分割和生成常用基准数据集上的广泛实验验证了我们提出的校准方法的有效性。", "summary": "该论文关注扩散模型中像素级文本-图像对齐的不完美性，指出现有方法依赖于完美的对齐假设。作者首先通过零样本参照图像分割评估了流行扩散模型的对齐情况，并分析了训练数据偏差导致的错位，发现小尺寸、遮挡或稀有物体类别的图像易出现错位。为解决此问题，论文提出了ELBO-T2IAlign，一个基于似然证据下界（ELBO）的无需训练且通用的校准方法，旨在纠正扩散模型中的像素-文本对齐。实验证明了该方法在图像分割和生成任务上的有效性。", "keywords": "扩散模型, 文本-图像对齐, ELBO, 像素级校准, 零样本分割", "comments": "该论文解决了扩散模型在文本-图像对齐方面的一个关键限制，即现有方法假设完美对齐的问题。ELBO-T2IAlign的“无需训练”和“通用性”是其主要创新点，这意味着它能广泛应用于现有和未来的扩散模型，且不需要额外的训练成本。这对于依赖精确文本-图像对齐的下游任务（如分割、编辑）具有重要意义。"}}
{"id": "2506.09342", "title": "Latent Multi-Head Attention for Small Language Models", "authors": ["Sushant Mehta", "Raj Dandekar", "Rajat Dandekar", "Sreedath Panat"], "summary": "We present the first comprehensive study of latent multi-head attention (MLA)\nfor small language models, revealing interesting efficiency-quality trade-offs.\nTraining 30M-parameter GPT models on 100,000 synthetic stories, we benchmark\nthree architectural variants: standard multi-head attention (MHA), MLA, and MLA\nwith rotary positional embeddings (MLA+RoPE). Our key finding is that MLA+RoPE\nwith half-rank latent dimensions (r = d/2) achieves a 45% KV-cache memory\nreduction while incurring only a 0.3% increase in validation loss (essentially\nmatching MHA quality)- a Pareto improvement for memory constrained deployment.\nWe further show that RoPE is crucial for MLA in small models: without it, MLA\nunderperforms vanilla attention by 3-5%, but with RoPE, it surpasses vanilla by\n2%. Inference benchmarks on NVIDIA A100 GPUs reveal that MLA with r=d/2\nachieves a 1.4 times speedup over full-rank MLA while maintaining the memory\nsavings. GPT-4 evaluations corroborate perplexity results, with ours achieving\nthe highest quality scores (7.4/10) across grammar, creativity, and consistency\nmetrics. Code and models will be released upon acceptance.", "comment": "6 pages, 1 figure. 5 tables", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.09342v1", "AI": {"title_translation": "小型语言模型的潜在多头注意力", "tldr": "本文对小型语言模型中的潜在多头注意力（MLA）进行了首次全面研究，发现MLA+RoPE在内存受限部署中能显著减少KV缓存内存并提高推理速度，同时保持模型质量。", "motivation": "本文旨在对小型语言模型中的潜在多头注意力（MLA）进行首次全面研究，揭示其效率-质量权衡，特别是为内存受限的部署场景提供优化方案。", "method": "研究通过在100,000个合成故事上训练30M参数的GPT模型，对标准多头注意力（MHA）、MLA和带有旋转位置嵌入（MLA+RoPE）的MLA三种架构变体进行了基准测试。推理基准测试在NVIDIA A100 GPU上进行，并通过GPT-4评估来验证困惑度结果。", "result": "主要发现是，半秩潜在维度（r = d/2）的MLA+RoPE实现了45%的KV缓存内存减少，而验证损失仅增加0.3%（基本匹配MHA质量），这是内存受限部署的帕累托改进。此外，RoPE对小型模型中的MLA至关重要：没有RoPE，MLA比香草注意力差3-5%，但有了RoPE，它超越香草注意力2%。在NVIDIA A100 GPU上的推理基准测试显示，r=d/2的MLA比全秩MLA快1.4倍，同时保持内存节省。GPT-4评估证实了困惑度结果，在语法、创造力和一致性指标上取得了最高质量分数（7.4/10）。", "conclusion": "MLA+RoPE与半秩潜在维度结合，为内存受限的部署提供了帕累托改进，因为它在显著减少内存和提高速度的同时，基本保持了模型质量。研究强调了旋转位置嵌入（RoPE）在小型语言模型中对MLA性能的关键作用。", "translation": "我们首次对小型语言模型中的潜在多头注意力（MLA）进行了全面研究，揭示了有趣的效率-质量权衡。通过在100,000个合成故事上训练30M参数的GPT模型，我们对三种架构变体进行了基准测试：标准多头注意力（MHA）、MLA和带有旋转位置嵌入（MLA+RoPE）的MLA。我们的关键发现是，半秩潜在维度（r = d/2）的MLA+RoPE实现了45%的KV缓存内存减少，而验证损失仅增加0.3%（基本匹配MHA质量）——这是内存受限部署的帕累托改进。我们进一步表明，RoPE对于小型模型中的MLA至关重要：没有它，MLA比香草注意力差3-5%，但有了RoPE，它超越香草注意力2%。在NVIDIA A100 GPU上的推理基准测试显示，r=d/2的MLA比全秩MLA快1.4倍，同时保持内存节省。GPT-4评估证实了困惑度结果，我们的模型在语法、创造力和一致性指标上取得了最高质量分数（7.4/10）。代码和模型将在接受后发布。", "summary": "本文对小型语言模型中的潜在多头注意力（MLA）进行了首次全面研究，探讨了其效率与质量的权衡。通过训练30M参数的GPT模型，研究发现带有旋转位置嵌入（RoPE）的MLA（MLA+RoPE）在半秩潜在维度下，能将KV缓存内存减少45%，且验证损失仅增加0.3%，基本与标准多头注意力（MHA）质量持平。这表明MLA+RoPE是内存受限部署的帕累托改进。研究还强调了RoPE对小型模型中MLA的重要性，有RoPE时MLA性能优于香草注意力2%，而无RoPE时则差3-5%。此外，MLA在r=d/2时比全秩MLA推理速度快1.4倍，并在GPT-4评估中获得了最高质量分数。", "keywords": "潜在多头注意力, 小型语言模型, 内存优化, 旋转位置嵌入, 效率提升", "comments": "这项研究首次全面探讨了潜在多头注意力在小型语言模型中的应用，特别是引入了RoPE与MLA结合，为内存受限的部署提供了显著的效率提升。其创新之处在于证明了在保持甚至略微提升质量的同时，大幅减少KV缓存内存和提高推理速度，这对于资源受限的边缘设备和移动应用具有重要意义。研究结果为小型语言模型的优化提供了新的方向。"}}
{"id": "2506.09638", "title": "FedVLMBench: Benchmarking Federated Fine-Tuning of Vision-Language Models", "authors": ["Weiying Zheng", "Ziyue Lin", "Pengxin Guo", "Yuyin Zhou", "Feifei Wang", "Liangqiong Qu"], "summary": "Vision-Language Models (VLMs) have demonstrated remarkable capabilities in\ncross-modal understanding and generation by integrating visual and textual\ninformation. While instruction tuning and parameter-efficient fine-tuning\nmethods have substantially improved the generalization of VLMs, most existing\napproaches rely on centralized training, posing challenges for deployment in\ndomains with strict privacy requirements like healthcare. Recent efforts have\nintroduced Federated Learning (FL) into VLM fine-tuning to address these\nprivacy concerns, yet comprehensive benchmarks for evaluating federated\nfine-tuning strategies, model architectures, and task generalization remain\nlacking. In this work, we present \\textbf{FedVLMBench}, the first systematic\nbenchmark for federated fine-tuning of VLMs. FedVLMBench integrates two\nmainstream VLM architectures (encoder-based and encoder-free), four fine-tuning\nstrategies, five FL algorithms, six multimodal datasets spanning four\ncross-domain single-task scenarios and two cross-domain multitask settings,\ncovering four distinct downstream task categories. Through extensive\nexperiments, we uncover key insights into the interplay between VLM\narchitectures, fine-tuning strategies, data heterogeneity, and multi-task\nfederated optimization. Notably, we find that a 2-layer multilayer perceptron\n(MLP) connector with concurrent connector and LLM tuning emerges as the optimal\nconfiguration for encoder-based VLMs in FL. Furthermore, current FL methods\nexhibit significantly higher sensitivity to data heterogeneity in\nvision-centric tasks than text-centric ones, across both encoder-free and\nencoder-based VLM architectures. Our benchmark provides essential tools,\ndatasets, and empirical guidance for the research community, offering a\nstandardized platform to advance privacy-preserving, federated training of\nmultimodal foundation models.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09638v1", "AI": {"title_translation": "FedVLMBench：基准测试视觉-语言模型的联邦微调", "tldr": "FedVLMBench是一个针对视觉-语言模型联邦微调的全面基准测试平台，揭示了架构、微调策略和数据异质性对性能的影响。", "motivation": "现有视觉-语言模型（VLM）的微调方法主要依赖于集中式训练，这在医疗等对隐私有严格要求的领域带来了部署挑战。尽管联邦学习（FL）已被引入以解决隐私问题，但目前缺乏评估联邦微调策略、模型架构和任务泛化能力的全面基准。", "method": "本文提出了FedVLMBench，这是首个针对VLM联邦微调的系统基准。FedVLMBench整合了两种主流VLM架构（基于编码器和无编码器）、四种微调策略、五种FL算法、六个多模态数据集（涵盖四种跨域单任务场景和两种跨域多任务设置），并覆盖了四种不同的下游任务类别。通过广泛的实验来揭示VLM架构、微调策略、数据异质性与多任务联邦优化之间的相互作用。", "result": "实验发现，对于基于编码器的VLM在FL中，一个2层多层感知器（MLP）连接器与并发连接器和LLM调优是最佳配置。此外，当前的FL方法在以视觉为中心的任务中对数据异质性表现出比以文本为中心的任务显著更高的敏感性，这在无编码器和基于编码器的VLM架构中均是如此。", "conclusion": "FedVLMBench基准为研究社区提供了必要的工具、数据集和经验指导，为推进多模态基础模型的隐私保护联邦训练提供了一个标准化平台。", "translation": "视觉-语言模型（VLM）通过整合视觉和文本信息，在跨模态理解和生成方面展现出卓越的能力。尽管指令调优和参数高效微调方法已显著提高了VLM的泛化能力，但大多数现有方法依赖于集中式训练，这在医疗等对隐私有严格要求的领域带来了部署挑战。最近的努力已将联邦学习（FL）引入VLM微调以解决这些隐私问题，但目前仍缺乏评估联邦微调策略、模型架构和任务泛化能力的全面基准。在这项工作中，我们提出了\\textbf{FedVLMBench}，这是首个针对VLM联邦微调的系统基准。FedVLMBench整合了两种主流VLM架构（基于编码器和无编码器）、四种微调策略、五种FL算法、六个多模态数据集（涵盖四种跨域单任务场景和两种跨域多任务设置），并覆盖了四种不同的下游任务类别。通过广泛的实验，我们揭示了VLM架构、微调策略、数据异质性与多任务联邦优化之间相互作用的关键见解。值得注意的是，我们发现一个2层多层感知器（MLP）连接器与并发连接器和LLM调优是基于编码器的VLM在FL中的最佳配置。此外，当前的FL方法在以视觉为中心的任务中对数据异质性表现出比以文本为中心的任务显著更高的敏感性，这在无编码器和基于编码器的VLM架构中均是如此。我们的基准为研究社区提供了必要的工具、数据集和经验指导，为推进多模态基础模型的隐私保护联邦训练提供了一个标准化平台。", "summary": "本文提出了FedVLMBench，一个针对视觉-语言模型（VLM）联邦微调的系统性基准测试平台，旨在解决集中式训练带来的隐私问题以及现有联邦微调评估基准的缺失。该基准整合了多种VLM架构、微调策略、联邦学习算法和多模态数据集，覆盖了多种下游任务。通过广泛实验，研究揭示了VLM架构、微调策略、数据异质性与联邦优化之间的复杂关系，并发现特定配置在联邦学习中表现最佳，同时指出当前联邦学习方法在视觉任务中对数据异质性更为敏感。FedVLMBench为推动隐私保护的多模态基础模型联邦训练提供了标准化工具和指导。", "keywords": "联邦学习, 视觉-语言模型, 微调, 基准测试, 隐私保护", "comments": "FedVLMBench的创新之处在于其作为首个针对视觉-语言模型联邦微调的系统性基准，填补了该领域在全面评估方面的空白。其重要性体现在为研究人员提供了一个标准化平台，以解决多模态模型在隐私敏感领域（如医疗）的部署挑战。通过揭示不同架构、微调策略和数据异质性对联邦学习性能的影响，该工作为未来的隐私保护多模态AI研究提供了宝贵的经验指导和工具。"}}
{"id": "2506.09745", "title": "Class Similarity-Based Multimodal Classification under Heterogeneous Category Sets", "authors": ["Yangrui Zhu", "Junhua Bao", "Yipan Wei", "Yapeng Li", "Bo Du"], "summary": "Existing multimodal methods typically assume that different modalities share\nthe same category set. However, in real-world applications, the category\ndistributions in multimodal data exhibit inconsistencies, which can hinder the\nmodel's ability to effectively utilize cross-modal information for recognizing\nall categories. In this work, we propose the practical setting termed\nMulti-Modal Heterogeneous Category-set Learning (MMHCL), where models are\ntrained in heterogeneous category sets of multi-modal data and aim to recognize\ncomplete classes set of all modalities during test. To effectively address this\ntask, we propose a Class Similarity-based Cross-modal Fusion model (CSCF).\nSpecifically, CSCF aligns modality-specific features to a shared semantic space\nto enable knowledge transfer between seen and unseen classes. It then selects\nthe most discriminative modality for decision fusion through uncertainty\nestimation. Finally, it integrates cross-modal information based on class\nsimilarity, where the auxiliary modality refines the prediction of the dominant\none. Experimental results show that our method significantly outperforms\nexisting state-of-the-art (SOTA) approaches on multiple benchmark datasets,\neffectively addressing the MMHCL task.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09745v1", "AI": {"title_translation": "基于类别相似性的异构类别集多模态分类", "tldr": "现有多模态方法在异构类别集下表现不佳。本文提出了多模态异构类别集学习（MMHCL）设置和基于类别相似性的跨模态融合模型（CSCF），通过特征对齐、不确定性估计和类别相似性融合来解决该问题，并在多个基准数据集上显著优于现有SOTA方法。", "motivation": "现有多模态方法通常假设不同模态共享相同的类别集。然而，在实际应用中，多模态数据中的类别分布存在不一致性，这会阻碍模型有效利用跨模态信息来识别所有类别。为了解决这一实际问题，本文提出了多模态异构类别集学习（MMHCL）的实际设置。", "method": "本文提出了一个基于类别相似性的跨模态融合模型（CSCF）。CSCF首先将模态特定特征对齐到一个共享的语义空间，以实现已知和未知类别之间的知识迁移。然后，它通过不确定性估计选择最具判别力的模态进行决策融合。最后，它基于类别相似性整合跨模态信息，其中辅助模态用于细化主导模态的预测。", "result": "实验结果表明，本文提出的方法在多个基准数据集上显著优于现有的最先进（SOTA）方法。", "conclusion": "本文提出的CSCF模型有效解决了多模态异构类别集学习（MMHCL）任务，在异构类别集的多模态分类中表现出优越的性能。", "translation": "现有多模态方法通常假设不同模态共享相同的类别集。然而，在实际应用中，多模态数据中的类别分布存在不一致性，这会阻碍模型有效利用跨模态信息来识别所有类别。在这项工作中，我们提出了多模态异构类别集学习（MMHCL）的实际设置，其中模型在多模态数据的异构类别集中进行训练，旨在测试时识别所有模态的完整类别集。为了有效解决此任务，我们提出了一个基于类别相似性的跨模态融合模型（CSCF）。具体而言，CSCF将模态特定特征对齐到一个共享的语义空间，以实现已知和未知类别之间的知识迁移。然后，它通过不确定性估计选择最具判别力的模态进行决策融合。最后，它基于类别相似性整合跨模态信息，其中辅助模态细化主导模态的预测。实验结果表明，我们的方法在多个基准数据集上显著优于现有的最先进（SOTA）方法，有效解决了MMHCL任务。", "summary": "本文针对现有多模态方法在处理类别分布不一致的真实世界数据时的局限性，提出了多模态异构类别集学习（MMHCL）这一实际设置。为有效解决MMHCL任务，论文提出了一种基于类别相似性的跨模态融合模型（CSCF）。CSCF通过将模态特征对齐到共享语义空间实现知识迁移，利用不确定性估计选择判别性模态，并基于类别相似性融合跨模态信息以优化预测。实验结果验证了CSCF在多个基准数据集上显著优于现有SOTA方法，成功应对了MMHCL任务。", "keywords": "多模态分类, 异构类别集, 类别相似性, 跨模态融合, 不确定性估计", "comments": "本文的创新点在于提出了多模态异构类别集学习（MMHCL）这一实际且重要的问题，突破了传统多模态学习中模态间类别集共享的假设。CSCF模型通过特征对齐实现知识迁移，结合不确定性估计选择模态，以及基于类别相似性的信息融合机制，为处理异构类别集下的多模态分类提供了一种新颖且有效的方法。该研究对于扩展多模态模型在复杂真实场景中的应用具有重要意义。"}}
{"id": "2506.09674", "title": "Wavelet Scattering Transform and Fourier Representation for Offline Detection of Malicious Clients in Federated Learning", "authors": ["Alessandro Licciardi", "Davide Leo", "Davide Carbone"], "summary": "Federated Learning (FL) enables the training of machine learning models\nacross decentralized clients while preserving data privacy. However, the\npresence of anomalous or corrupted clients - such as those with faulty sensors\nor non representative data distributions - can significantly degrade model\nperformance. Detecting such clients without accessing raw data remains a key\nchallenge. We propose WAFFLE (Wavelet and Fourier representations for Federated\nLearning) a detection algorithm that labels malicious clients {\\it before\ntraining}, using locally computed compressed representations derived from\neither the Wavelet Scattering Transform (WST) or the Fourier Transform. Both\napproaches provide low-dimensional, task-agnostic embeddings suitable for\nunsupervised client separation. A lightweight detector, trained on a\ndistillated public dataset, performs the labeling with minimal communication\nand computational overhead. While both transforms enable effective detection,\nWST offers theoretical advantages, such as non-invertibility and stability to\nlocal deformations, that make it particularly well-suited to federated\nscenarios. Experiments on benchmark datasets show that our method improves\ndetection accuracy and downstream classification performance compared to\nexisting FL anomaly detection algorithms, validating its effectiveness as a\npre-training alternative to online detection strategies.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09674v1", "AI": {"title_translation": "联邦学习中基于小波散射变换和傅里叶表示的恶意客户端离线检测", "tldr": "WAFFLE算法利用小波散射变换或傅里叶变换的压缩表示，在联邦学习训练前离线检测恶意客户端，提高了检测精度和下游分类性能。", "motivation": "联邦学习中，恶意或异常客户端的存在会显著降低模型性能，而如何在不访问原始数据的情况下检测这些客户端是一个关键挑战。", "method": "提出WAFFLE（用于联邦学习的小波和傅里叶表示）算法，该算法在训练前利用本地计算的、从小波散射变换（WST）或傅里叶变换获得的压缩表示来标记恶意客户端。这些表示是低维且与任务无关的嵌入，适用于无监督的客户端分离。一个在蒸馏公共数据集上训练的轻量级检测器负责标记，且通信和计算开销极小。WST相比傅里叶变换具有理论优势，如不可逆性和对局部形变的稳定性。", "result": "在基准数据集上的实验表明，该方法与现有联邦学习异常检测算法相比，提高了检测精度和下游分类性能。", "conclusion": "WAFFLE算法作为一种预训练的离线检测策略，在联邦学习中有效检测恶意客户端，并且其性能优于现有的在线检测策略。", "translation": "联邦学习（FL）能够在保护数据隐私的同时，跨分散客户端训练机器学习模型。然而，异常或损坏客户端（例如传感器故障或数据分布不具代表性的客户端）的存在会显著降低模型性能。在不访问原始数据的情况下检测此类客户端仍然是一个关键挑战。我们提出了WAFFLE（用于联邦学习的小波和傅里叶表示），这是一种检测算法，它在训练之前使用从小波散射变换（WST）或傅里叶变换导出的本地计算的压缩表示来标记恶意客户端。这两种方法都提供了低维、与任务无关的嵌入，适用于无监督的客户端分离。一个在蒸馏公共公共数据集上训练的轻量级检测器以最小的通信和计算开销执行标记。虽然这两种变换都能实现有效检测，但WST提供了理论优势，例如不可逆性和对局部形变的稳定性，这使其特别适合联邦场景。在基准数据集上的实验表明，与现有FL异常检测算法相比，我们的方法提高了检测精度和下游分类性能，验证了其作为在线检测策略的预训练替代方案的有效性。", "summary": "本文提出WAFFLE算法，利用小波散射变换或傅里叶变换生成的本地压缩表示，在联邦学习训练前离线检测恶意客户端。该方法生成低维、任务无关的嵌入，结合轻量级检测器实现高效标记。实验证明WAFFLE在检测精度和下游分类性能上优于现有方法，特别是小波散射变换因其理论优势更适合联邦场景，验证了其作为预训练离线检测策略的有效性。", "keywords": "联邦学习, 恶意客户端检测, 小波散射变换, 傅里叶变换, 离线检测", "comments": "本文的创新点在于提出了在联邦学习训练前进行离线恶意客户端检测的WAFFLE算法，有效地解决了数据隐私限制下的异常检测问题。通过利用小波散射变换和傅里叶变换生成低维、任务无关的表示，降低了检测的通信和计算开销，提高了效率。特别是小波散射变换的理论优势，使其在联邦学习这种分布式、异构数据场景下具有重要的应用潜力。该研究为联邦学习的鲁棒性提供了新的解决方案。"}}
{"id": "2506.09777", "title": "Inverting Black-Box Face Recognition Systems via Zero-Order Optimization in Eigenface Space", "authors": ["Anton Razzhigaev", "Matvey Mikhalchuk", "Klim Kireev", "Igor Udovichenko", "Andrey Kuznetsov", "Aleksandr Petiushko"], "summary": "Reconstructing facial images from black-box recognition models poses a\nsignificant privacy threat. While many methods require access to embeddings, we\naddress the more challenging scenario of model inversion using only similarity\nscores. This paper introduces DarkerBB, a novel approach that reconstructs\ncolor faces by performing zero-order optimization within a PCA-derived\neigenface space. Despite this highly limited information, experiments on LFW,\nAgeDB-30, and CFP-FP benchmarks demonstrate that DarkerBB achieves\nstate-of-the-art verification accuracies in the similarity-only setting, with\ncompetitive query efficiency.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09777v1", "AI": {"title_translation": "通过特征脸空间中的零阶优化反演黑盒人脸识别系统", "tldr": "DarkerBB是一种新方法，它使用零阶优化在特征脸空间中从黑盒人脸识别模型仅凭相似度分数重建人脸，并在受限信息下实现了最先进的验证精度。", "motivation": "从黑盒识别模型重建人脸图像构成了重大的隐私威胁。许多现有方法需要访问嵌入，但本文旨在解决更具挑战性的仅使用相似度分数进行模型反演的场景。", "method": "本文提出了DarkerBB，这是一种新颖的方法，通过在PCA导出的特征脸空间中执行零阶优化来重建彩色人脸，仅使用相似度分数作为输入。", "result": "尽管信息高度受限，DarkerBB在LFW、AgeDB-30和CFP-FP基准测试中，在仅相似度设置下实现了最先进的验证精度，并具有竞争性的查询效率。", "conclusion": "DarkerBB证明了即使在信息高度受限的情况下（仅使用相似度分数），也可以有效地从黑盒人脸识别系统重建人脸，从而对隐私构成潜在威胁，并为模型反演提供了最先进的解决方案。", "translation": "从黑盒识别模型重建人脸图像构成了重大的隐私威胁。虽然许多方法需要访问嵌入，但我们解决了仅使用相似度分数进行模型反演的更具挑战性的场景。本文介绍了DarkerBB，这是一种新颖的方法，通过在PCA导出的特征脸空间中执行零阶优化来重建彩色人脸。尽管信息高度受限，但LFW、AgeDB-30和CFP-FP基准测试的实验表明，DarkerBB在仅相似度设置下实现了最先进的验证精度，并具有竞争性的查询效率。", "summary": "本文提出了一种名为DarkerBB的新方法，旨在解决从黑盒人脸识别模型中重建人脸的隐私威胁问题。与需要访问嵌入的现有方法不同，DarkerBB仅利用相似度分数，通过在PCA衍生的特征脸空间中执行零阶优化来重建彩色人脸。尽管信息输入极其有限，DarkerBB在多个基准测试中表现出色，实现了最先进的验证精度和高效的查询性能。", "keywords": "黑盒反演, 人脸识别, 零阶优化, 特征脸空间, 隐私威胁", "comments": "DarkerBB的创新之处在于它在仅使用相似度分数这种极其受限的信息下实现了人脸重建，这比需要嵌入访问的方法更具挑战性。其在隐私威胁研究方面具有重要意义，并展示了零阶优化在图像重建领域的潜力。"}}
{"id": "2506.09682", "title": "Wasserstein Hypergraph Neural Network", "authors": ["Iulia Duta", "Pietro Liò"], "summary": "The ability to model relational information using machine learning has driven\nadvancements across various domains, from medicine to social science. While\ngraph representation learning has become mainstream over the past decade,\nrepresenting higher-order relationships through hypergraphs is rapidly gaining\nmomentum. In the last few years, numerous hypergraph neural networks have\nemerged, most of them falling under a two-stage, set-based framework. The\nmessages are sent from nodes to edges and then from edges to nodes. However,\nmost of the advancement still takes inspiration from the graph counterpart,\noften simplifying the aggregations to basic pooling operations. In this paper\nwe are introducing Wasserstein Hypergraph Neural Network, a model that treats\nthe nodes and hyperedge neighbourhood as distributions and aggregate the\ninformation using Sliced Wasserstein Pooling. Unlike conventional aggregators\nsuch as mean or sum, which only capture first-order statistics, our approach\nhas the ability to preserve geometric properties like the shape and spread of\ndistributions. This enables the learned embeddings to reflect how easily one\nhyperedge distribution can be transformed into another, following principles of\noptimal transport. Experimental results demonstrate that applying Wasserstein\npooling in a hypergraph setting significantly benefits node classification\ntasks, achieving top performance on several real-world datasets.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09682v1", "AI": {"title_translation": "Wasserstein 超图神经网络", "tldr": "本文提出了一种名为 Wasserstein 超图神经网络 (WHNN) 的新型模型，它利用切片 Wasserstein 池化将节点和超边邻域视为分布进行信息聚合，解决了现有超图神经网络在保留高阶统计量方面的不足，并在节点分类任务上取得了优异表现。", "motivation": "现有的超图神经网络（HNNs）大多采用两阶段、基于集合的框架，且其聚合操作通常简化为基本池化，仅捕获一阶统计量，未能保留分布的几何特性，无法反映高阶关系的复杂性。", "method": "本文引入了 Wasserstein 超图神经网络 (WHNN)，该模型将节点和超边邻域视为分布，并使用切片 Wasserstein 池化（Sliced Wasserstein Pooling）来聚合信息。这种方法能够保留分布的几何特性，如形状和分布，并反映一个超边分布转换为另一个超边分布的容易程度，遵循最优传输原理。", "result": "实验结果表明，在超图设置中应用 Wasserstein 池化显著提升了节点分类任务的性能，并在多个真实世界数据集上取得了最佳表现。", "conclusion": "Wasserstein 超图神经网络通过采用切片 Wasserstein 池化，能够有效处理节点和超边邻域的分布信息，保留其几何特性，从而显著提高节点分类任务的性能。", "translation": "机器学习建模关系信息的能力推动了从医学到社会科学等各个领域的进步。尽管图表示学习在过去十年中已成为主流，但通过超图表示高阶关系正迅速获得关注。在过去几年中，涌现了许多超图神经网络，其中大多数都属于两阶段、基于集合的框架。消息从节点发送到边，然后从边发送到节点。然而，大多数进展仍然受到图对应物的启发，通常将聚合简化为基本的池化操作。在本文中，我们引入了 Wasserstein 超图神经网络，该模型将节点和超边邻域视为分布，并使用切片 Wasserstein 池化聚合信息。与仅捕获一阶统计量的传统聚合器（如均值或和）不同，我们的方法能够保留分布的几何特性，如形状和分布。这使得学习到的嵌入能够根据最优传输原理反映一个超边分布转换为另一个超边分布的容易程度。实验结果表明，在超图设置中应用 Wasserstein 池化显著有益于节点分类任务，在多个真实世界数据集上取得了最佳性能。", "summary": "本文提出了一种名为 Wasserstein 超图神经网络 (WHNN) 的新型超图神经网络。不同于传统超图神经网络中简化的一阶聚合操作，WHNN 将节点和超边邻域视为分布，并利用 Sliced Wasserstein Pooling 进行信息聚合。这种方法能够保留分布的几何特性，如形状和分布，并能反映超边分布间的转换难度。实验证明，WHNN 在节点分类任务上表现出色，并在多个真实世界数据集上取得了顶尖性能。", "keywords": "超图神经网络, Wasserstein 距离, 图表示学习, 节点分类, 最优传输", "comments": "这项工作通过引入 Wasserstein 池化来处理超图中的分布信息，为超图神经网络的聚合机制带来了创新。它解决了现有方法在保留高阶统计信息方面的局限性，使得模型能够捕获更丰富的几何特性，这对于理解复杂关系和提高表示学习能力具有重要意义。"}}
{"id": "2506.09782", "title": "Q-SAM2: Accurate Quantization for Segment Anything Model 2", "authors": ["Nicola Farronato", "Florian Scheidegger", "Mattia Rigotti", "Cristiano Malossi", "Michele Magno", "Haotong Qin"], "summary": "The Segment Anything Model 2 (SAM2) has gained significant attention as a\nfoundational approach for promptable image and video segmentation. However, its\nexpensive computational and memory consumption poses a severe challenge for its\napplication in resource-constrained scenarios. In this paper, we propose an\naccurate low-bit quantization method for efficient SAM2, termed Q-SAM2. To\naddress the performance degradation caused by the singularities in weight and\nactivation distributions during quantization, Q-SAM2 introduces two novel\ntechnical contributions. We first introduce a linear layer calibration method\nfor low-bit initialization of SAM2, which minimizes the Frobenius norm over a\nsmall image batch to reposition weight distributions for improved quantization.\nWe then propose a Quantization-Aware Training (QAT) pipeline that applies\nclipping to suppress outliers and allows the network to adapt to quantization\nthresholds during training. Our comprehensive experiments demonstrate that\nQ-SAM2 allows for highly accurate inference while substantially improving\nefficiency. Both quantitative and visual results show that our Q-SAM2 surpasses\nexisting state-of-the-art general quantization schemes, especially for\nultra-low 2-bit quantization. While designed for quantization-aware training,\nour proposed calibration technique also proves effective in post-training\nquantization, achieving up to a 66% mIoU accuracy improvement over\nnon-calibrated models.", "comment": "20 pages", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09782v1", "AI": {"title_translation": "Q-SAM2：Segment Anything Model 2的精确量化", "tldr": "本文提出Q-SAM2，一种针对SAM2的低位量化方法，通过线性层校准和量化感知训练解决其高计算和内存消耗问题，实现了高精度和效率提升，尤其在2位量化上表现卓越。", "motivation": "Segment Anything Model 2 (SAM2) 虽然是图像和视频分割的基础方法，但其高昂的计算和内存消耗严重限制了其在资源受限场景中的应用。", "method": "本文提出了Q-SAM2，这是一种精确的低位量化方法，包含两项核心技术：1. 线性层校准方法，通过在少量图像批次上最小化Frobenius范数来重新定位权重分布，以实现SAM2的低位初始化和改善量化。2. 量化感知训练（QAT）流程，该流程通过裁剪抑制异常值，并允许网络在训练期间自适应量化阈值。", "result": "Q-SAM2在显著提高效率的同时，实现了高精度的推理。定量和视觉结果均表明，Q-SAM2超越了现有最先进的通用量化方案，尤其在超低2位量化方面表现突出。此外，所提出的校准技术在训练后量化中也有效，相较于非校准模型，mIoU精度提升高达66%。", "conclusion": "Q-SAM2通过创新的线性层校准和量化感知训练流程，成功解决了SAM2在低位量化中的性能下降问题，实现了高效且高精度的模型压缩，特别是在超低比特量化方面表现优异，极大地拓宽了SAM2在资源受限环境下的应用潜力。", "translation": "Segment Anything Model 2 (SAM2) 作为一种可提示图像和视频分割的基础方法受到了广泛关注。然而，其昂贵的计算和内存消耗对其在资源受限场景中的应用构成了严峻挑战。在本文中，我们提出了一种针对高效 SAM2 的精确低位量化方法，称为 Q-SAM2。为了解决量化过程中权重和激活分布中奇异性导致的性能下降问题，Q-SAM2 引入了两项新颖的技术贡献。我们首先引入了一种用于 SAM2 低位初始化的线性层校准方法，该方法通过在少量图像批次上最小化 Frobenius 范数来重新定位权重分布，以改善量化。然后，我们提出了一种量化感知训练（QAT）流程，该流程应用裁剪来抑制异常值，并允许网络在训练期间适应量化阈值。我们全面的实验表明，Q-SAM2 允许高精度推理，同时大幅提高效率。定量和视觉结果均表明，我们的 Q-SAM2 超越了现有最先进的通用量化方案，特别是对于超低 2 位量化。尽管是为量化感知训练而设计的，我们提出的校准技术在训练后量化中也证明是有效的，比非校准模型的 mIoU 精度提高高达 66%。", "summary": "本文提出Q-SAM2，一种针对Segment Anything Model 2 (SAM2) 的精确低位量化方法，旨在解决其高计算和内存消耗问题。Q-SAM2引入了两个关键技术：一是线性层校准方法，通过最小化Frobenius范数优化权重分布以改善量化；二是量化感知训练（QAT）流程，通过裁剪和自适应量化阈值来抑制异常值。实验证明，Q-SAM2在保持高精度的同时显著提升了效率，尤其在2位量化方面优于现有最先进方案。所提出的校准技术在训练后量化中也表现出显著效果。", "keywords": "量化, SAM2, 低位量化, 量化感知训练, 模型压缩", "comments": "该论文通过引入独特的线性层校准和优化的QAT流程，为大型分割模型SAM2的量化提供了有效解决方案，显著提高了其在资源受限环境中的可用性。其在超低2位量化上的出色性能，凸显了其在实际应用中的巨大潜力，特别是在边缘设备部署方面。校准技术在训练后量化中的有效性也增加了其方法的通用性和实用性。"}}
{"id": "2506.09701", "title": "TRIDENT: Temporally Restricted Inference via DFA-Enhanced Neural Traversal", "authors": ["Vincenzo Collura", "Karim Tit", "Laura Bussi", "Eleonora Giunchiglia", "Maxime Cordy"], "summary": "Large Language Models (LLMs) and other neural architectures have achieved\nimpressive results across a variety of generative and classification tasks.\nHowever, they remain fundamentally ill-equipped to ensure that their outputs\nsatisfy temporal constraints, such as those expressible in Linear Temporal\nLogic over finite traces (LTLf). In this paper, we introduce TRIDENT: a general\nand model-agnostic inference-time algorithm that guarantees compliance with\nsuch constraints without requiring any retraining. TRIDENT compiles LTLf\nformulas into a Deterministic Finite Automaton (DFA), which is used to guide a\nconstrained variant of beam search. At each decoding step, transitions that\nwould lead to constraint violations are masked, while remaining paths are\ndynamically re-ranked based on both the model's probabilities and the DFA's\nacceptance structure. We formally prove that the resulting sequences are\nguaranteed to satisfy the given LTLf constraints, and we empirically\ndemonstrate that TRIDENT also improves output quality. We validate our approach\non two distinct tasks: temporally constrained image-stream classification and\ncontrolled text generation. In both settings, TRIDENT achieves perfect\nconstraint satisfaction, while comparison with the state of the art shows\nimproved efficiency and high standard quality metrics.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09701v1", "AI": {"title_translation": "TRIDENT: 通过DFA增强的神经遍历实现时间受限推理", "tldr": "TRIDENT是一种通用且与模型无关的推理时间算法，它利用确定性有限自动机（DFA）来指导神经网络模型的推理过程，以确保输出满足线性时间逻辑（LTLf）等时间约束，且无需再训练。该方法在保证完美约束满足的同时，还能提高输出质量和效率。", "motivation": "大型语言模型（LLMs）和其他神经架构在生成和分类任务中表现出色，但它们无法确保其输出满足时间约束，例如那些可由有限轨迹上的线性时间逻辑（LTLf）表达的约束。", "method": "TRIDENT将LTLf公式编译成一个确定性有限自动机（DFA）。该DFA用于指导受约束的集束搜索变体。在每个解码步骤中，会屏蔽导致约束违规的转换，而剩余的路径则根据模型的概率和DFA的接受结构进行动态重新排序。", "result": "正式证明了所得序列保证满足给定的LTLf约束。经验证明TRIDENT也能提高输出质量。在时间受限的图像流分类和受控文本生成任务中，TRIDENT实现了完美的约束满足，与现有技术相比显示出更高的效率和高标准的质量指标。", "conclusion": "TRIDENT是一种通用且与模型无关的推理时间算法，它无需任何再训练即可保证神经网络模型的输出符合线性时间逻辑（LTLf）等时间约束，同时还能提高输出质量和效率。", "translation": "大型语言模型 (LLM) 和其他神经架构在各种生成和分类任务中取得了令人印象深刻的成果。然而，它们在确保其输出满足时间约束方面仍然存在根本性不足，例如那些可以通过有限轨迹上的线性时间逻辑 (LTLf) 表达的约束。在本文中，我们介绍了 TRIDENT：一种通用且与模型无关的推理时间算法，它无需任何再训练即可保证符合此类约束。TRIDENT 将 LTLf 公式编译成一个确定性有限自动机 (DFA)，该自动机用于指导受约束的集束搜索变体。在每个解码步骤中，将导致约束违规的转换进行屏蔽，而剩余的路径则根据模型的概率和 DFA 的接受结构进行动态重新排序。我们正式证明了所得序列保证满足给定的 LTLf 约束，并且我们凭经验证明 TRIDENT 也提高了输出质量。我们在两个不同的任务上验证了我们的方法：时间受限的图像流分类和受控文本生成。在这两种设置中，TRIDENT 都实现了完美的约束满足，同时与现有技术的比较表明其效率更高且标准质量指标更高。", "summary": "TRIDENT是一种通用的、与模型无关的推理时算法，旨在解决大型语言模型和其他神经架构在满足LTLf等时间约束方面的不足。它通过将LTLf公式编译为确定性有限自动机（DFA），并利用该DFA引导受约束的集束搜索，从而在不进行再训练的情况下保证输出序列满足时间约束。TRIDENT在解码过程中屏蔽违反约束的路径，并动态重新排序剩余路径。该方法被证明能保证约束满足，并在图像流分类和受控文本生成任务中展现出完美的约束满足、更高的效率和优异的输出质量。", "keywords": "神经网络, 时间约束, LTLf, DFA, 推理算法", "comments": "TRIDENT的创新之处在于其无需再训练即可为神经网络模型引入严格的时间逻辑约束，这对于需要高可靠性输出的领域（如安全关键系统）具有重要意义。通过结合符号逻辑（DFA）和神经网络（束搜索），它有效地弥补了深度学习在形式化保证方面的不足。"}}
{"id": "2506.09784", "title": "Accurate and efficient zero-shot 6D pose estimation with frozen foundation models", "authors": ["Andrea Caraffa", "Davide Boscaini", "Fabio Poiesi"], "summary": "Estimating the 6D pose of objects from RGBD data is a fundamental problem in\ncomputer vision, with applications in robotics and augmented reality. A key\nchallenge is achieving generalization to novel objects that were not seen\nduring training. Most existing approaches address this by scaling up training\non synthetic data tailored to the task, a process that demands substantial\ncomputational resources. But is task-specific training really necessary for\naccurate and efficient 6D pose estimation of novel objects? To answer No!, we\nintroduce FreeZeV2, the second generation of FreeZe: a training-free method\nthat achieves strong generalization to unseen objects by leveraging geometric\nand vision foundation models pre-trained on unrelated data. FreeZeV2 improves\nboth accuracy and efficiency over FreeZe through three key contributions: (i) a\nsparse feature extraction strategy that reduces inference-time computation\nwithout sacrificing accuracy; (ii) a feature-aware scoring mechanism that\nimproves both pose selection during RANSAC-based 3D registration and the final\nranking of pose candidates; and (iii) a modular design that supports ensembles\nof instance segmentation models, increasing robustness to segmentation masks\nerrors. We evaluate FreeZeV2 on the seven core datasets of the BOP Benchmark,\nwhere it establishes a new state-of-the-art in 6D pose estimation of unseen\nobjects. When using the same segmentation masks, FreeZeV2 achieves a remarkable\n8x speedup over FreeZe while also improving accuracy by 5%. When using\nensembles of segmentation models, FreeZeV2 gains an additional 8% in accuracy\nwhile still running 2.5x faster than FreeZe. FreeZeV2 was awarded Best Overall\nMethod at the BOP Challenge 2024.", "comment": "Technical report", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09784v1", "AI": {"title_translation": "使用冻结基础模型实现准确高效的零样本6D姿态估计", "tldr": "FreeZeV2是一种无需训练的方法，利用预训练的基础模型实现对未知物体的准确高效6D姿态估计，并在BOP基准测试中达到了最先进水平。", "motivation": "现有的6D姿态估计方法为了泛化到新物体，需要大量计算资源进行特定任务的合成数据训练。本文旨在解决是否真的需要这种特定任务训练的问题，以实现对未知物体的准确高效6D姿态估计。", "method": "本文提出了FreeZeV2，它是FreeZe的第二代训练无关方法，通过利用预训练的几何和视觉基础模型，实现了对未知物体的强大泛化能力。FreeZeV2通过以下三个关键贡献提高了准确性和效率：(i) 稀疏特征提取策略，在不牺牲准确性的情况下减少了推理计算；(ii) 特征感知评分机制，改进了基于RANSAC的3D配准中的姿态选择和姿态候选的最终排名；(iii) 模块化设计，支持实例分割模型集成，提高了对分割掩码错误的鲁棒性。", "result": "FreeZeV2在BOP基准测试的七个核心数据集上进行了评估，并在未知物体6D姿态估计方面建立了新的最先进水平。在使用相同分割掩码的情况下，FreeZeV2比FreeZe提速8倍，同时准确率提高5%。当使用分割模型集成时，FreeZeV2准确率额外提高8%，同时仍比FreeZe快2.5倍。FreeZeV2荣获BOP挑战赛2024年最佳综合方法奖。", "conclusion": "FreeZeV2证明了无需特定任务训练，仅通过利用冻结的基础模型，也能实现对未知物体准确高效的6D姿态估计，并在多个数据集上取得了最先进的性能。", "translation": "从RGBD数据估计物体的6D姿态是计算机视觉中的一个基本问题，在机器人和增强现实中有广泛应用。一个关键挑战是实现对训练期间未见过的新物体的泛化。大多数现有方法通过扩展针对任务定制的合成数据训练来解决这个问题，但这需要大量的计算资源。但是，对于新物体的准确高效6D姿态估计，任务特定的训练真的有必要吗？为了回答“不！”，我们引入了FreeZeV2，它是FreeZe的第二代：一种无需训练的方法，通过利用在不相关数据上预训练的几何和视觉基础模型，实现了对未知物体的强大泛化能力。FreeZeV2通过三个关键贡献在准确性和效率上都比FreeZe有所改进：(i) 一种稀疏特征提取策略，在不牺牲准确性的情况下减少了推理计算；(ii) 一种特征感知评分机制，改进了基于RANSAC的3D配准中的姿态选择和姿态候选的最终排名；(iii) 一种模块化设计，支持实例分割模型集成，增加了对分割掩码错误的鲁棒性。我们在BOP基准测试的七个核心数据集上评估了FreeZeV2，它在未知物体6D姿态估计方面建立了新的最先进水平。当使用相同的分割掩码时，FreeZeV2比FreeZe提速8倍，同时准确率提高5%。当使用分割模型集成时，FreeZeV2准确率额外提高8%，同时仍比FreeZe快2.5倍。FreeZeV2荣获BOP挑战赛2024年最佳综合方法奖。", "summary": "本文介绍了FreeZeV2，一种创新的无需训练的6D姿态估计算法，旨在解决对未知物体泛化的问题。该方法利用预训练的几何和视觉基础模型，并通过稀疏特征提取、特征感知评分机制和模块化集成设计显著提高了准确性和效率。FreeZeV2在BOP基准测试中取得了最先进的成果，并获得了BOP挑战赛2024年最佳综合方法奖，证明了其在零样本6D姿态估计领域的卓越性能。", "keywords": "6D姿态估计, 零样本, 基础模型, 无需训练, 泛化", "comments": "FreeZeV2的创新之处在于其无需训练的范式，通过有效利用冻结的基础模型实现了对未知物体的强大泛化能力，这大大降低了计算资源需求。其关键贡献，如稀疏特征提取和特征感知评分机制，不仅提升了性能，也优化了效率。该方法在BOP挑战赛中获奖，进一步验证了其在实际应用中的潜力和重要性。"}}
{"id": "2506.09714", "title": "Auto-Compressing Networks", "authors": ["Vaggelis Dorovatas", "Georgios Paraskevopoulos", "Alexandros Potamianos"], "summary": "Deep neural networks with short residual connections have demonstrated\nremarkable success across domains, but increasing depth often introduces\ncomputational redundancy without corresponding improvements in representation\nquality. In this work, we introduce Auto-Compressing Networks (ACNs), an\narchitectural variant where additive long feedforward connections from each\nlayer to the output replace traditional short residual connections. ACNs\nshowcase a unique property we coin as \"auto-compression\", the ability of a\nnetwork to organically compress information during training with gradient\ndescent, through architectural design alone. Through auto-compression,\ninformation is dynamically \"pushed\" into early layers during training,\nenhancing their representational quality and revealing potential redundancy in\ndeeper ones. We theoretically show that this property emerges from layer-wise\ntraining patterns present in ACNs, where layers are dynamically utilized during\ntraining based on task requirements. We also find that ACNs exhibit enhanced\nnoise robustness compared to residual networks, superior performance in\nlow-data settings, improved transfer learning capabilities, and mitigate\ncatastrophic forgetting suggesting that they learn representations that\ngeneralize better despite using fewer parameters. Our results demonstrate up to\n18% reduction in catastrophic forgetting and 30-80% architectural compression\nwhile maintaining accuracy across vision transformers, MLP-mixers, and BERT\narchitectures. Furthermore, we demonstrate that coupling ACNs with traditional\npruning techniques, enables significantly better sparsity-performance\ntrade-offs compared to conventional architectures. These findings establish\nACNs as a practical approach to developing efficient neural architectures that\nautomatically adapt their computational footprint to task complexity, while\nlearning robust representations.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09714v1", "AI": {"title_translation": "自压缩网络", "tldr": "自压缩网络（ACNs）是一种新型深度神经网络架构，它用长前馈连接取代了传统的短残差连接，使其在训练过程中能够有机地自动压缩信息到早期层。ACNs在保持准确性的同时，能显著减少灾难性遗忘（高达18%）并实现30-80%的架构压缩，同时展示出更强的噪声鲁棒性、在低数据设置下的优越性能、改进的迁移学习能力以及更好的泛化能力。", "motivation": "深度神经网络虽然取得了显著成功，但增加深度常常引入计算冗余，而未能相应地提升表示质量。", "method": "本文引入了自压缩网络（ACNs），这是一种架构变体，其中每个层到输出的附加长前馈连接取代了传统的短残差连接。ACNs展示了一种独特的“自压缩”特性，即网络仅通过架构设计，就能在梯度下降训练过程中有机地压缩信息。理论上，这种特性源于ACNs中存在的逐层训练模式，其中层在训练过程中根据任务需求被动态利用。", "result": "ACNs与残差网络相比，表现出增强的噪声鲁棒性、在低数据设置下的优越性能、改进的迁移学习能力，并减轻了灾难性遗忘，表明它们在参数更少的情况下学习到泛化能力更好的表示。实验结果表明，在视觉Transformer、MLP-mixer和BERT架构上，灾难性遗忘减少了高达18%，架构压缩达到了30-80%，同时保持了准确性。此外，将ACNs与传统剪枝技术结合，与传统架构相比，实现了显著更好的稀疏性-性能权衡。", "conclusion": "这些发现确立了ACNs作为一种开发高效神经网络架构的实用方法，这些架构能自动调整其计算足迹以适应任务复杂性，同时学习到鲁棒的表示。", "translation": "带有短残差连接的深度神经网络在各个领域都取得了显著成功，但增加深度常常引入计算冗余，而未能相应地提升表示质量。在这项工作中，我们引入了自压缩网络（ACNs），这是一种架构变体，其中每个层到输出的附加长前馈连接取代了传统的短残差连接。ACNs展示了一种我们称之为“自压缩”的独特特性，即网络仅通过架构设计，就能在梯度下降训练过程中有机地压缩信息。通过自压缩，信息在训练过程中被动态地“推入”早期层，增强了它们的表示质量，并揭示了深层中潜在的冗余。我们从理论上表明，这种特性源于ACNs中存在的逐层训练模式，其中层在训练过程中根据任务需求被动态利用。我们还发现，与残差网络相比，ACNs表现出增强的噪声鲁棒性、在低数据设置下的优越性能、改进的迁移学习能力，并减轻了灾难性遗忘，表明它们在参数更少的情况下学习到泛化能力更好的表示。我们的结果表明，在视觉Transformer、MLP-mixer和BERT架构上，灾难性遗忘减少了高达18%，架构压缩达到了30-80%，同时保持了准确性。此外，我们证明将ACNs与传统剪枝技术结合，与传统架构相比，实现了显著更好的稀疏性-性能权衡。这些发现确立了ACNs作为一种开发高效神经网络架构的实用方法，这些架构能自动调整其计算足迹以适应任务复杂性，同时学习到鲁棒的表示。", "summary": "本文提出了一种名为自压缩网络（ACNs）的新型深度神经网络架构，其核心创新在于用从每个层到输出的长前馈连接取代了传统的短残差连接。这种设计赋予了ACNs独特的“自压缩”能力，使其在训练过程中能将信息有机地压缩到早期层，从而提高表示质量并揭示深层冗余。理论分析表明，这种特性源于其动态的逐层训练模式。实验结果显示，ACNs在保持高准确性的同时，能实现高达18%的灾难性遗忘减少和30-80%的架构压缩，并在噪声鲁棒性、低数据性能、迁移学习和泛化能力方面优于传统残差网络。结合剪枝技术，ACNs还能提供更优的稀疏性-性能权衡，证明了其作为开发高效且鲁棒的神经网络架构的实用性。", "keywords": "自压缩网络, 神经网络, 架构压缩, 残差连接, 鲁棒表示", "comments": "本文提出了一种创新的神经网络架构——自压缩网络（ACNs），其核心在于将短残差连接替换为长前馈连接，从而实现训练过程中的“自压缩”特性。这种通过架构设计实现计算效率和鲁棒表示的思路非常新颖且实用，它不仅解决了传统深度网络存在的计算冗余问题，还在多个关键性能指标上超越了现有架构。ACNs在减少灾难性遗忘、提升小数据量下的性能以及增强迁移学习能力方面的表现尤为突出，这对于实际应用具有重要意义。该研究为开发更高效、更适应任务复杂度的神经网络提供了新的方向。"}}
{"id": "2506.09814", "title": "DreamCS: Geometry-Aware Text-to-3D Generation with Unpaired 3D Reward Supervision", "authors": ["Xiandong Zou", "Ruihao Xia", "Hongsong Wang", "Pan Zhou"], "summary": "While text-to-3D generation has attracted growing interest, existing methods\noften struggle to produce 3D assets that align well with human preferences.\nCurrent preference alignment techniques for 3D content typically rely on\nhardly-collected preference-paired multi-view 2D images to train 2D reward\nmodels, when then guide 3D generation -- leading to geometric artifacts due to\ntheir inherent 2D bias. To address these limitations, we construct 3D-MeshPref,\nthe first large-scale unpaired 3D preference dataset, featuring diverse 3D\nmeshes annotated by a large language model and refined by human evaluators. We\nthen develop RewardCS, the first reward model trained directly on unpaired\n3D-MeshPref data using a novel Cauchy-Schwarz divergence objective, enabling\neffective learning of human-aligned 3D geometric preferences without requiring\npaired comparisons. Building on this, we propose DreamCS, a unified framework\nthat integrates RewardCS into text-to-3D pipelines -- enhancing both implicit\nand explicit 3D generation with human preference feedback. Extensive\nexperiments show DreamCS outperforms prior methods, producing 3D assets that\nare both geometrically faithful and human-preferred. Code and models will be\nreleased publicly.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09814v1", "AI": {"title_translation": "DreamCS：基于几何感知的文本到3D生成与无配对3D奖励监督", "tldr": "DreamCS提出了一种新的无配对3D偏好数据集（3D-MeshPref）和奖励模型（RewardCS），以解决现有文本到3D生成中几何伪影和人类偏好对齐不佳的问题，从而生成更符合人类偏好的几何真实3D资产。", "motivation": "现有文本到3D生成方法难以生成符合人类偏好的3D资产，且当前偏好对齐技术依赖难以收集的配对多视角2D图像来训练2D奖励模型，这导致固有的2D偏差和几何伪影。", "method": "1. 构建了首个大规模无配对3D偏好数据集3D-MeshPref，包含由大型语言模型标注并经人工评估者精炼的3D网格。 2. 开发了首个直接在无配对3D-MeshPref数据上训练的奖励模型RewardCS，采用新的Cauchy-Schwarz散度目标，无需配对比较即可学习人类对齐的3D几何偏好。 3. 提出了DreamCS统一框架，将RewardCS集成到文本到3D生成流程中，以人类偏好反馈增强隐式和显式3D生成。", "result": "实验表明，DreamCS优于现有方法，生成的3D资产既几何真实又符合人类偏好。", "conclusion": "DreamCS通过引入直接基于无配对3D数据的奖励监督，有效解决了文本到3D生成中的几何伪影和人类偏好对齐问题，显著提升了生成3D资产的质量和用户满意度。", "translation": "尽管文本到3D生成引起了日益增长的兴趣，但现有方法常常难以生成与人类偏好良好对齐的3D资产。当前用于3D内容的偏好对齐技术通常依赖于难以收集的、配对的多视角2D图像来训练2D奖励模型，然后这些模型指导3D生成——由于其固有的2D偏差，导致几何伪影。为了解决这些局限性，我们构建了3D-MeshPref，这是首个大规模无配对3D偏好数据集，其特点是包含由大型语言模型标注并经人工评估者精炼的各种3D网格。随后，我们开发了RewardCS，这是首个直接在无配对3D-MeshPref数据上使用新颖的Cauchy-Schwarz散度目标训练的奖励模型，从而无需配对比较即可有效学习与人类对齐的3D几何偏好。在此基础上，我们提出了DreamCS，一个统一的框架，将RewardCS集成到文本到3D生成流程中——通过人类偏好反馈增强隐式和显式3D生成。大量实验表明，DreamCS优于现有方法，生成的3D资产既几何真实又符合人类偏好。代码和模型将公开发布。", "summary": "本文提出DreamCS框架，旨在解决现有文本到3D生成中几何伪影和人类偏好对齐不佳的问题。为此，作者构建了首个大规模无配对3D偏好数据集3D-MeshPref，并基于此开发了RewardCS奖励模型，该模型直接学习人类对齐的3D几何偏好。DreamCS将RewardCS集成到文本到3D生成流程中，实验证明其能生成更符合人类偏好且几何真实的3D资产，优于现有方法。", "keywords": "文本到3D生成, 3D奖励监督, 几何感知, 人类偏好对齐, 无配对数据", "comments": "这篇论文通过引入直接基于3D数据的无配对奖励监督，解决了文本到3D生成中长期存在的几何保真度问题，这是一个重要的创新点。通过构建大规模3D偏好数据集和新颖的奖励模型，它有效地弥补了2D偏好监督的局限性，为生成更符合人类直觉和美学的3D内容提供了新的方向。"}}
{"id": "2506.09367", "title": "COGENT: A Curriculum-oriented Framework for Generating Grade-appropriate Educational Content", "authors": ["Zhengyuan Liu", "Stella Xin Yin", "Dion Hoe-Lian Goh", "Nancy F. Chen"], "summary": "While Generative AI has demonstrated strong potential and versatility in\ncontent generation, its application to educational contexts presents several\nchallenges. Models often fail to align with curriculum standards and maintain\ngrade-appropriate reading levels consistently. Furthermore, STEM education\nposes additional challenges in balancing scientific explanations with everyday\nlanguage when introducing complex and abstract ideas and phenomena to younger\nstudents. In this work, we propose COGENT, a curriculum-oriented framework for\ngenerating grade-appropriate educational content. We incorporate three\ncurriculum components (science concepts, core ideas, and learning objectives),\ncontrol readability through length, vocabulary, and sentence complexity, and\nadopt a ``wonder-based'' approach to increase student engagement and interest.\nWe conduct a multi-dimensional evaluation via both LLM-as-a-judge and human\nexpert analysis. Experimental results show that COGENT consistently produces\ngrade-appropriate passages that are comparable or superior to human references.\nOur work establishes a viable approach for scaling adaptive and high-quality\nlearning resources.", "comment": "BEA 2025", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.09367v1", "AI": {"title_translation": "COGENT: 一个面向课程的生成适合年级教育内容的框架", "tldr": "COGENT是一个面向课程的框架，用于生成适合年级、高质量的教育内容，解决了生成式AI在教育应用中内容与课程标准不符以及阅读水平不一致的问题，尤其在STEM领域表现出色。", "motivation": "生成式AI在教育内容生成方面面临挑战，包括难以与课程标准对齐、阅读水平不一致，以及在STEM教育中平衡科学解释与日常语言的困难。", "method": "本文提出了COGENT框架，它整合了三个课程组件（科学概念、核心思想和学习目标），通过控制长度、词汇和句子复杂性来控制可读性，并采用“基于好奇心”的方法来提高学生参与度和兴趣。通过LLM作为评判和人类专家分析进行多维度评估。", "result": "实验结果表明，COGENT持续生成适合年级的文章，其质量与人类参考资料相当或更优。", "conclusion": "本研究为扩展适应性强、高质量的学习资源提供了一种可行的方法。", "translation": "尽管生成式AI在内容生成方面展现出强大的潜力和多功能性，但其在教育环境中的应用仍面临若干挑战。模型通常无法与课程标准对齐，也无法始终保持适合年级的阅读水平。此外，STEM教育在向年幼学生介绍复杂和抽象概念及现象时，在平衡科学解释与日常语言方面提出了额外挑战。在这项工作中，我们提出了COGENT，一个面向课程的生成适合年级教育内容的框架。我们整合了三个课程组件（科学概念、核心思想和学习目标），通过长度、词汇和句子复杂性来控制可读性，并采用“基于好奇心”的方法来提高学生的参与度和兴趣。我们通过LLM作为评判和人类专家分析进行了多维度评估。实验结果表明，COGENT持续生成适合年级的文章，其质量与人类参考资料相当或更优。我们的工作为扩展适应性强、高质量的学习资源建立了一种可行的方法。", "summary": "本文提出了COGENT框架，旨在解决生成式AI在教育内容生成中存在的课程对齐和年级阅读水平不一致的问题。COGENT通过整合科学概念、核心思想、学习目标等课程组件，并控制文本的可读性，同时采用“基于好奇心”的方法提升学生参与度。通过LLM和人类专家评估，COGENT生成的内容被证实与人类参考资料相当或更优，为规模化高质量学习资源提供了有效途径。", "keywords": "教育内容生成, 课程导向, 生成式AI, 年级适应性, STEM教育", "comments": "COGENT的创新之处在于其将课程组件、可读性控制和“基于好奇心”的方法相结合，为生成适合年级且引人入胜的教育内容提供了结构化方法。这项工作的重要性在于它为解决生成式AI在教育应用中的核心挑战提供了一个可行且经过验证的解决方案，特别是在STEM领域，有望显著提升自适应学习资源的质量和可扩展性。"}}
{"id": "2506.09733", "title": "AtmosMJ: Revisiting Gating Mechanism for AI Weather Forecasting Beyond the Year Scale", "authors": ["Minjong Cheon"], "summary": "The advent of Large Weather Models (LWMs) has marked a turning point in\ndata-driven forecasting, with many models now outperforming traditional\nnumerical systems in the medium range. However, achieving stable, long-range\nautoregressive forecasts beyond a few weeks remains a significant challenge.\nPrevailing state-of-the-art models that achieve year-long stability, such as\nSFNO and DLWP-HPX, have relied on transforming input data onto non-standard\nspatial domains like spherical harmonics or HEALPix meshes. This has led to the\nprevailing assumption that such representations are necessary to enforce\nphysical consistency and long-term stability. This paper challenges that\nassumption by investigating whether comparable long-range performance can be\nachieved on the standard latitude-longitude grid. We introduce AtmosMJ, a deep\nconvolutional network that operates directly on ERA5 data without any spherical\nremapping. The model's stability is enabled by a novel Gated Residual Fusion\n(GRF) mechanism, which adaptively moderates feature updates to prevent error\naccumulation over long recursive simulations. Our results demonstrate that\nAtmosMJ produces stable and physically plausible forecasts for about 500 days.\nIn quantitative evaluations, it achieves competitive 10-day forecast accuracy\nagainst models like Pangu-Weather and GraphCast, all while requiring a\nremarkably low training budget of 5.7 days on a V100 GPU. Our findings suggest\nthat efficient architectural design, rather than non-standard data\nrepresentation, can be the key to unlocking stable and computationally\nefficient long-range weather prediction.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09733v1", "AI": {"title_translation": "AtmosMJ：重新审视用于超越年度尺度AI天气预报的门控机制", "tldr": "AtmosMJ提出了一种新颖的门控残差融合机制，在标准经纬度网格上实现了长达500天的稳定天气预报，挑战了传统上认为需要非标准空间表示的假设。", "motivation": "当前的大型天气模型在实现数周以上的稳定、长程自回归预报方面仍面临挑战。现有最先进的模型通过将输入数据转换到非标准空间域来维持长期稳定性，这导致了一种普遍的假设，即此类表示对于物理一致性和长期稳定性是必要的。本文旨在挑战这一假设，探讨在标准经纬度网格上是否能实现可比的长程性能。", "method": "本文引入了AtmosMJ，一个直接在ERA5数据上运行的深度卷积网络，无需任何球面重映射。该模型的稳定性通过一种新颖的门控残差融合（GRF）机制实现，该机制自适应地调节特征更新，以防止在长递归模拟中误差累积。", "result": "AtmosMJ能够产生长达约500天的稳定且物理上合理的预报。在定量评估中，它在10天预报精度上与Pangu-Weather和GraphCast等模型具有竞争力，同时训练预算极低，在V100 GPU上仅需5.7天。", "conclusion": "研究结果表明，高效的架构设计，而非非标准数据表示，可能是实现稳定且计算高效的长期天气预报的关键。", "translation": "大型天气模型（LWMs）的出现标志着数据驱动预报的一个转折点，许多模型在中等范围内已超越了传统的数值系统。然而，实现超越数周的稳定、长程自回归预报仍然是一个重大挑战。当前实现年度稳定性的最先进模型，如SFNO和DLWP-HPX，依赖于将输入数据转换到非标准空间域，如球面谐波或HEALPix网格。这导致了一种普遍的假设，即此类表示对于强制物理一致性和长期稳定性是必要的。本文通过研究在标准经纬度网格上是否能实现可比的长程性能来挑战这一假设。我们引入了AtmosMJ，一个直接在ERA5数据上运行的深度卷积网络，无需任何球面重映射。该模型的稳定性通过一种新颖的门控残差融合（GRF）机制实现，该机制自适应地调节特征更新，以防止在长递归模拟中误差累积。我们的结果表明，AtmosMJ能够产生长达约500天的稳定且物理上合理的预报。在定量评估中，它在10天预报精度上与Pangu-Weather和GraphCast等模型具有竞争力，同时训练预算极低，在V100 GPU上仅需5.7天。我们的发现表明，高效的架构设计，而非非标准数据表示，可能是实现稳定且计算高效的长期天气预报的关键。", "summary": "本文介绍了AtmosMJ，一个在标准经纬度网格上运行的深度卷积网络，旨在挑战长期天气预报中需要非标准空间表示的普遍假设。通过引入新颖的门控残差融合（GRF）机制，AtmosMJ实现了长达约500天的稳定且物理上合理的预报。该模型在10天预报精度上表现出竞争力，且训练成本低廉，表明高效的架构设计是实现长期天气预报稳定性和计算效率的关键。", "keywords": "AI天气预报, 门控机制, 长期预报, 深度学习, AtmosMJ", "comments": "AtmosMJ的创新之处在于其挑战了长期AI天气预报中关于数据表示的传统观念，证明了在标准经纬度网格上通过高效的架构设计（特别是门控残差融合机制）也能实现卓越的长期稳定性。这一点对于实际应用具有重要意义，因为它简化了数据处理流程，并可能降低计算成本。其在低训练预算下取得的竞争性表现，进一步凸显了其方法的潜力。这篇论文为AI天气预报领域开辟了新的研究方向。"}}
{"id": "2506.09834", "title": "MMME: A Spontaneous Multi-Modal Micro-Expression Dataset Enabling Visual-Physiological Fusion", "authors": ["Chuang Maa", "Yu Peia", "Jianhang Zhanga", "Shaokai Zhaoa", "Bowen Jib", "Liang Xiea", "Ye Yana", "Erwei Yin"], "summary": "Micro-expressions (MEs) are subtle, fleeting nonverbal cues that reveal an\nindividual's genuine emotional state. Their analysis has attracted considerable\ninterest due to its promising applications in fields such as healthcare,\ncriminal investigation, and human-computer interaction. However, existing ME\nresearch is limited to single visual modality, overlooking the rich emotional\ninformation conveyed by other physiological modalities, resulting in ME\nrecognition and spotting performance far below practical application needs.\nTherefore, exploring the cross-modal association mechanism between ME visual\nfeatures and physiological signals (PS), and developing a multimodal fusion\nframework, represents a pivotal step toward advancing ME analysis. This study\nintroduces a novel ME dataset, MMME, which, for the first time, enables\nsynchronized collection of facial action signals (MEs), central nervous system\nsignals (EEG), and peripheral PS (PPG, RSP, SKT, EDA, and ECG). By overcoming\nthe constraints of existing ME corpora, MMME comprises 634 MEs, 2,841\nmacro-expressions (MaEs), and 2,890 trials of synchronized multimodal PS,\nestablishing a robust foundation for investigating ME neural mechanisms and\nconducting multimodal fusion-based analyses. Extensive experiments validate the\ndataset's reliability and provide benchmarks for ME analysis, demonstrating\nthat integrating MEs with PS significantly enhances recognition and spotting\nperformance. To the best of our knowledge, MMME is the most comprehensive ME\ndataset to date in terms of modality diversity. It provides critical data\nsupport for exploring the neural mechanisms of MEs and uncovering the\nvisual-physiological synergistic effects, driving a paradigm shift in ME\nresearch from single-modality visual analysis to multimodal fusion. The dataset\nwill be publicly available upon acceptance of this paper.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09834v1", "AI": {"title_translation": "MMME：一个支持视觉-生理融合的自发性多模态微表情数据集", "tldr": "MMME是一个新的多模态微表情数据集，首次同步收集了视觉和多种生理信号，并证明了多模态融合能显著提升微表情识别和发现性能。", "motivation": "现有的微表情（ME）研究主要局限于单一视觉模态，忽略了其他生理模态所传达的丰富情感信息，导致ME识别和发现的性能远低于实际应用需求。因此，探索ME视觉特征与生理信号（PS）之间的跨模态关联机制，并开发多模态融合框架是推动ME分析的关键一步。", "method": "本研究引入了一个新颖的ME数据集MMME，首次实现了面部动作信号（ME）、中枢神经系统信号（EEG）和外周生理信号（PPG、RSP、SKT、EDA和ECG）的同步采集。MMME包含634个ME、2,841个宏表情（MaE）和2,890次同步多模态PS试验，为研究ME的神经机制和进行多模态融合分析奠定了基础。", "result": "广泛的实验验证了MMME数据集的可靠性，并为ME分析提供了基准。结果表明，将ME与PS整合显著增强了识别和发现的性能。", "conclusion": "MMME是迄今为止在模态多样性方面最全面的ME数据集。它为探索ME的神经机制和揭示视觉-生理协同效应提供了关键数据支持，推动了ME研究从单一模态视觉分析向多模态融合的范式转变。", "translation": "微表情（MEs）是揭示个体真实情感状态的微妙、短暂的非语言线索。由于其在医疗保健、刑事侦查和人机交互等领域的应用前景，其分析引起了广泛关注。然而，现有的ME研究仅限于单一视觉模态，忽视了其他生理模态所传达的丰富情感信息，导致ME识别和发现性能远低于实际应用需求。因此，探索ME视觉特征与生理信号（PS）之间的跨模态关联机制，并开发多模态融合框架，代表着推进ME分析的关键一步。本研究引入了一个新颖的ME数据集MMME，首次实现了面部动作信号（MEs）、中枢神经系统信号（EEG）和外周PS（PPG、RSP、SKT、EDA和ECG）的同步采集。通过克服现有ME语料库的限制，MMME包含634个ME、2,841个宏表情（MaEs）和2,890次同步多模态PS试验，为研究ME神经机制和进行多模态融合分析奠定了坚实的基础。广泛的实验验证了数据集的可靠性，并为ME分析提供了基准，证明了将ME与PS整合显著增强了识别和发现性能。据我们所知，MMME是迄今为止在模态多样性方面最全面的ME数据集。它为探索ME的神经机制和揭示视觉-生理协同效应提供了关键数据支持，推动了ME研究从单一模态视觉分析向多模态融合的范式转变。该数据集将在论文接受后公开发布。", "summary": "本研究提出了一个名为MMME的新型多模态微表情数据集，旨在解决现有微表情研究仅限于单一视觉模态的局限性。MMME首次实现了面部微表情与多种生理信号（如EEG、PPG、ECG等）的同步采集，包含634个微表情和大量宏表情及同步生理信号试验。实验证明，该数据集可靠，并且整合视觉与生理信号能显著提升微表情的识别和发现性能。MMME被认为是迄今最全面的微表情数据集，为推动微表情研究从单模态视觉分析转向多模态融合提供了关键数据支持。", "keywords": "微表情, 多模态, 数据集, 生理信号, 融合", "comments": "该论文通过构建MMME数据集，填补了微表情研究中多模态数据稀缺的空白，特别是同步收集视觉和多种生理信号，具有显著的创新性。其重要性在于为揭示微表情的神经机制和探索视觉-生理协同效应提供了前所未有的数据基础，有望推动微表情分析领域实现范式转变，从单一模态走向更全面的多模态融合。这对医疗、安防和人机交互等实际应用领域具有深远意义。"}}
{"id": "2506.09738", "title": "Towards Multi-modal Graph Large Language Model", "authors": ["Xin Wang", "Zeyang Zhang", "Linxin Xiao", "Haibo Chen", "Chendi Ge", "Wenwu Zhu"], "summary": "Multi-modal graphs, which integrate diverse multi-modal features and\nrelations, are ubiquitous in real-world applications. However, existing\nmulti-modal graph learning methods are typically trained from scratch for\nspecific graph data and tasks, failing to generalize across various multi-modal\ngraph data and tasks. To bridge this gap, we explore the potential of\nMulti-modal Graph Large Language Models (MG-LLM) to unify and generalize across\ndiverse multi-modal graph data and tasks. We propose a unified framework of\nmulti-modal graph data, task, and model, discovering the inherent\nmulti-granularity and multi-scale characteristics in multi-modal graphs.\nSpecifically, we present five key desired characteristics for MG-LLM: 1)\nunified space for multi-modal structures and attributes, 2) capability of\nhandling diverse multi-modal graph tasks, 3) multi-modal graph in-context\nlearning, 4) multi-modal graph interaction with natural language, and 5)\nmulti-modal graph reasoning. We then elaborate on the key challenges, review\nrelated works, and highlight promising future research directions towards\nrealizing these ambitious characteristics. Finally, we summarize existing\nmulti-modal graph datasets pertinent for model training. We believe this paper\ncan contribute to the ongoing advancement of the research towards MG-LLM for\ngeneralization across multi-modal graph data and tasks.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09738v1", "AI": {"title_translation": "面向多模态图大语言模型", "tldr": "本文探讨了多模态图大语言模型（MG-LLM）的潜力，旨在解决现有方法在多模态图数据和任务泛化能力不足的问题，并提出了一个统一的框架和五个关键特性。", "motivation": "现有的多模态图学习方法通常为特定的图数据和任务从头开始训练，导致其在各种多模态图数据和任务之间泛化能力不足。", "method": "本文探索了多模态图大语言模型（MG-LLM）的潜力，以实现多模态图数据和任务的统一和泛化。提出了一个多模态图数据、任务和模型的统一框架，并揭示了多模态图固有的多粒度和多尺度特性。具体提出了MG-LLM的五个关键期望特性：1) 多模态结构和属性的统一空间，2) 处理多样化多模态图任务的能力，3) 多模态图上下文学习，4) 多模态图与自然语言的交互，5) 多模态图推理。此外，还阐述了关键挑战，回顾了相关工作，并强调了未来有前景的研究方向，并总结了现有的多模态图数据集。", "result": "Not mentioned in abstract", "conclusion": "本文致力于推动多模态图大语言模型（MG-LLM）的研究进展，以实现多模态图数据和任务的泛化。", "translation": "多模态图集成了多样化的多模态特征和关系，在现实世界应用中无处不在。然而，现有的多模态图学习方法通常为特定的图数据和任务从头开始训练，无法在各种多模态图数据和任务之间进行泛化。为了弥补这一差距，我们探索了多模态图大语言模型（MG-LLM）的潜力，以实现多样化多模态图数据和任务的统一和泛化。我们提出了一个多模态图数据、任务和模型的统一框架，发现了多模态图中固有的多粒度和多尺度特性。具体而言，我们提出了MG-LLM的五个关键期望特性：1) 多模态结构和属性的统一空间，2) 处理多样化多模态图任务的能力，3) 多模态图上下文学习，4) 多模态图与自然语言的交互，以及5) 多模态图推理。然后，我们详细阐述了关键挑战，回顾了相关工作，并强调了实现这些雄心勃勃特性的有前景的未来研究方向。最后，我们总结了与模型训练相关的现有多模态图数据集。我们相信本文能为多模态图大语言模型（MG-LLM）研究的持续进展做出贡献，以实现多模态图数据和任务的泛化。", "summary": "本文旨在解决现有多模态图学习方法泛化能力不足的问题，提出探索多模态图大语言模型（MG-LLM）的潜力。文章构建了一个多模态图数据、任务和模型的统一框架，并识别了多模态图的内在多粒度和多尺度特性。文中详细阐述了MG-LLM应具备的五个关键特性，包括统一空间、处理多样任务的能力、上下文学习、自然语言交互以及推理能力。此外，论文还讨论了关键挑战、回顾了相关工作并指出了未来研究方向，并总结了相关数据集，旨在推动MG-LLM在多模态图数据和任务泛化方面的研究进展。", "keywords": "多模态图, 大语言模型, 泛化, 统一框架, MG-LLM", "comments": "本文为多模态图与大语言模型（LLM）的结合奠定了基础，提出了一个前瞻性的统一框架和关键特性，以解决现有多模态图学习方法泛化能力差的痛点。其价值在于为该新兴领域指明了方向，并为未来的研究提供了全面的路线图和重要的参考。"}}
{"id": "2506.09836", "title": "DynaSplat: Dynamic-Static Gaussian Splatting with Hierarchical Motion Decomposition for Scene Reconstruction", "authors": ["Junli Deng", "Ping Shi", "Qipei Li", "Jinyang Guo"], "summary": "Reconstructing intricate, ever-changing environments remains a central\nambition in computer vision, yet existing solutions often crumble before the\ncomplexity of real-world dynamics. We present DynaSplat, an approach that\nextends Gaussian Splatting to dynamic scenes by integrating dynamic-static\nseparation and hierarchical motion modeling. First, we classify scene elements\nas static or dynamic through a novel fusion of deformation offset statistics\nand 2D motion flow consistency, refining our spatial representation to focus\nprecisely where motion matters. We then introduce a hierarchical motion\nmodeling strategy that captures both coarse global transformations and\nfine-grained local movements, enabling accurate handling of intricate,\nnon-rigid motions. Finally, we integrate physically-based opacity estimation to\nensure visually coherent reconstructions, even under challenging occlusions and\nperspective shifts. Extensive experiments on challenging datasets reveal that\nDynaSplat not only surpasses state-of-the-art alternatives in accuracy and\nrealism but also provides a more intuitive, compact, and efficient route to\ndynamic scene reconstruction.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09836v1", "AI": {"title_translation": "DynaSplat: 结合分层运动分解的动态-静态高斯泼溅用于场景重建", "tldr": "DynaSplat通过动态-静态分离和分层运动建模，将高斯泼溅扩展到动态场景，实现高精度和真实感的重建。", "motivation": "在计算机视觉领域，重建复杂、不断变化的环境仍然是一个核心目标，但现有解决方案往往在真实世界动态的复杂性面前崩溃。", "method": "DynaSplat首先通过变形偏移统计和2D运动流一致性的新颖融合，将场景元素分类为静态或动态。然后，引入分层运动建模策略，捕捉粗略的全局变换和细粒度的局部运动。最后，整合基于物理的不透明度估计，确保视觉上连贯的重建。", "result": "在具有挑战性的数据集上进行的广泛实验表明，DynaSplat不仅在准确性和真实感方面超越了最先进的替代方案，而且为动态场景重建提供了一条更直观、紧凑和高效的途径。", "conclusion": "DynaSplat通过其动态-静态分离和分层运动建模方法，显著提升了动态场景重建的准确性、真实感和效率，超越了现有技术。", "translation": "重建复杂、不断变化的环境仍然是计算机视觉领域的核心目标，但现有解决方案往往在真实世界动态的复杂性面前崩溃。我们提出了DynaSplat，一种通过整合动态-静态分离和分层运动建模，将高斯泼溅扩展到动态场景的方法。首先，我们通过变形偏移统计和2D运动流一致性的新颖融合，将场景元素分类为静态或动态，从而优化我们的空间表示，精确地关注运动发生的地方。然后，我们引入了一种分层运动建模策略，该策略既能捕捉粗略的全局变换，也能捕捉细粒度的局部运动，从而能够准确处理复杂的非刚性运动。最后，我们整合了基于物理的不透明度估计，即使在具有挑战性的遮挡和透视变化下也能确保视觉上连贯的重建。在具有挑战性的数据集上进行的广泛实验表明，DynaSplat不仅在准确性和真实感方面超越了最先进的替代方案，而且为动态场景重建提供了一条更直观、紧凑和高效的途径。", "summary": "DynaSplat是一种用于动态场景重建的新方法，它将高斯泼溅技术与动态-静态分离和分层运动建模相结合。该方法首先通过融合变形偏移统计和2D运动流来区分静态和动态元素，然后采用分层策略捕捉全局和局部运动，并结合基于物理的不透明度估计。实验证明，DynaSplat在准确性、真实性和效率方面均优于现有技术。", "keywords": "动态场景重建, 高斯泼溅, 动态-静态分离, 分层运动, 非刚性运动", "comments": "DynaSplat的创新点在于其将动态-静态分离与分层运动建模相结合，有效解决了动态场景重建中的复杂非刚性运动问题。此外，引入基于物理的不透明度估计增强了重建的视觉连贯性。该方法为动态场景重建提供了一个更高效和高质量的解决方案，具有重要的实际应用潜力。"}}
{"id": "2506.09742", "title": "Feature Engineering for Agents: An Adaptive Cognitive Architecture for Interpretable ML Monitoring", "authors": ["Gusseppe Bravo-Rocca", "Peini Liu", "Jordi Guitart", "Rodrigo M Carrillo-Larco", "Ajay Dholakia", "David Ellison"], "summary": "Monitoring Machine Learning (ML) models in production environments is\ncrucial, yet traditional approaches often yield verbose, low-interpretability\noutputs that hinder effective decision-making. We propose a cognitive\narchitecture for ML monitoring that applies feature engineering principles to\nagents based on Large Language Models (LLMs), significantly enhancing the\ninterpretability of monitoring outputs. Central to our approach is a Decision\nProcedure module that simulates feature engineering through three key steps:\nRefactor, Break Down, and Compile. The Refactor step improves data\nrepresentation to better capture feature semantics, allowing the LLM to focus\non salient aspects of the monitoring data while reducing noise and irrelevant\ninformation. Break Down decomposes complex information for detailed analysis,\nand Compile integrates sub-insights into clear, interpretable outputs. This\nprocess leads to a more deterministic planning approach, reducing dependence on\nLLM-generated planning, which can sometimes be inconsistent and overly general.\nThe combination of feature engineering-driven planning and selective LLM\nutilization results in a robust decision support system, capable of providing\nhighly interpretable and actionable insights. Experiments using multiple LLMs\ndemonstrate the efficacy of our approach, achieving significantly higher\naccuracy compared to various baselines across several domains.", "comment": "Accepted at AAMAS 2025", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09742v1", "AI": {"title_translation": "面向智能体的特征工程：一种可解释ML监控的自适应认知架构", "tldr": "本文提出了一种结合特征工程与大型语言模型（LLMs）的认知架构，以提高机器学习模型监控的可解释性和准确性。", "motivation": "传统的机器学习模型监控方法输出冗长且可解释性差，阻碍了有效的决策制定。", "method": "提出了一种基于LLM智能体的认知架构，通过应用特征工程原则来增强ML监控输出的可解释性。核心是决策过程模块，包含三个步骤：重构（Refactor）改善数据表示和语义捕捉，分解（Break Down）细化复杂信息，编译（Compile）整合洞察。这种方法减少了对LLM生成规划的依赖，使其更确定。", "result": "使用多个LLM进行的实验表明，该方法在多个领域与各种基线相比，实现了显著更高的准确性。", "conclusion": "特征工程驱动的规划与选择性LLM利用相结合，构建了一个强大的决策支持系统，能够提供高度可解释和可操作的洞察。", "translation": "标题：面向智能体的特征工程：一种可解释ML监控的自适应认知架构\n摘要：在生产环境中监控机器学习 (ML) 模型至关重要，但传统方法通常产生冗长、可解释性低的输出，阻碍了有效的决策。我们提出了一种用于ML监控的认知架构，该架构将特征工程原则应用于基于大型语言模型 (LLMs) 的智能体，显著增强了监控输出的可解释性。我们方法的核心是一个决策过程模块，它通过三个关键步骤模拟特征工程：重构、分解和编译。重构步骤改进了数据表示，以更好地捕捉特征语义，使LLM能够专注于监控数据的显著方面，同时减少噪声和无关信息。分解将复杂信息分解以进行详细分析，编译将子洞察整合为清晰、可解释的输出。这一过程导致了一种更确定的规划方法，减少了对LLM生成规划的依赖，因为LLM生成规划有时可能不一致且过于笼统。特征工程驱动的规划与选择性LLM利用的结合，形成了一个强大的决策支持系统，能够提供高度可解释和可操作的洞察。使用多个LLM进行的实验证明了我们方法的有效性，在多个领域与各种基线相比，实现了显著更高的准确性。", "summary": "本文提出了一种用于机器学习模型监控的认知架构，该架构将特征工程原则应用于基于大型语言模型（LLMs）的智能体，旨在解决传统监控方法可解释性差的问题。其核心是决策过程模块，通过重构、分解和编译三个步骤模拟特征工程，以优化数据表示、细化复杂信息并整合洞察，从而提供更确定且可解释的监控输出。实验证明，该方法相比现有基线显著提高了准确性，并能提供高度可解释和可操作的洞察。", "keywords": "机器学习监控, 特征工程, 大型语言模型, 可解释性, 认知架构", "comments": "该论文通过将特征工程融入LLM驱动的智能体，为ML模型监控的可解释性提供了一个创新且实用的解决方案。其核心的决策过程模块——重构、分解和编译，有效解决了LLM规划可能存在的不一致和泛化问题，使得监控输出更加清晰和可操作。这对于提高生产环境中ML系统的可靠性和决策效率具有重要意义。"}}
{"id": "2506.09769", "title": "Load-Aware Training Scheduling for Model Circulation-based Decentralized Federated Learning", "authors": ["Haruki Kainuma", "Takayuki Nishio"], "summary": "This paper proposes Load-aware Tram-FL, an extension of Tram-FL that\nintroduces a training scheduling mechanism to minimize total training time in\ndecentralized federated learning by accounting for both computational and\ncommunication loads. The scheduling problem is formulated as a global\noptimization task, which-though intractable in its original form-is made\nsolvable by decomposing it into node-wise subproblems. To promote balanced data\nutilization under non-IID distributions, a variance constraint is introduced,\nwhile the overall training latency, including both computation and\ncommunication costs, is minimized through the objective function. Simulation\nresults on MNIST and CIFAR-10 demonstrate that Load-aware Tram-FL significantly\nreduces training time and accelerates convergence compared to baseline methods.", "comment": "6 pages, submitted to IEEE Globecom 2025 (under review)", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09769v1", "AI": {"title_translation": "基于模型循环的去中心化联邦学习中的负载感知训练调度", "tldr": "本文提出了Load-aware Tram-FL，一种考虑计算和通信负载的训练调度机制，旨在最小化去中心化联邦学习中的总训练时间，并通过分解问题和引入方差约束来提高效率和加速收敛。", "motivation": "在去中心化联邦学习中，为了最小化总训练时间，需要一种考虑计算和通信负载的训练调度机制。", "method": "本文提出了Load-aware Tram-FL，它是Tram-FL的扩展。该方法引入了训练调度机制，将调度问题表述为全局优化任务，并通过分解为节点子问题使其可解。为促进非IID数据下的平衡数据利用，引入了方差约束，并通过目标函数最小化总训练延迟（包括计算和通信成本）。", "result": "在MNIST和CIFAR-10数据集上的仿真结果表明，Load-aware Tram-FL与基线方法相比，显著减少了训练时间并加速了收敛。", "conclusion": "Load-aware Tram-FL通过其负载感知的训练调度机制，有效降低了去中心化联邦学习中的总训练时间并加速了模型收敛。", "translation": "本文提出了负载感知Tram-FL（Load-aware Tram-FL），它是Tram-FL的扩展，引入了一种训练调度机制，通过考虑计算和通信负载来最小化去中心化联邦学习中的总训练时间。调度问题被表述为一个全局优化任务，尽管其原始形式难以处理，但通过将其分解为节点子问题而变得可解。为了促进非独立同分布（non-IID）数据下的平衡数据利用，引入了方差约束，同时通过目标函数最小化包括计算和通信成本在内的整体训练延迟。在MNIST和CIFAR-10上的仿真结果表明，与基线方法相比，负载感知Tram-FL显著减少了训练时间并加速了收敛。", "summary": "本文提出Load-aware Tram-FL，一种改进的去中心化联邦学习框架，通过引入负载感知训练调度机制，考虑计算和通信负载，旨在最小化总训练时间。该方法将复杂的全局优化问题分解为可解的节点子问题，并引入方差约束以平衡非IID数据利用。实验证明，Load-aware Tram-FL在减少训练时间和加速收敛方面优于现有方法。", "keywords": "去中心化联邦学习, 负载感知, 训练调度, 模型循环, 优化", "comments": "该论文的创新点在于将负载感知调度引入去中心化联邦学习，并提出将复杂的全局优化问题分解为可解的节点子问题，同时兼顾了非IID数据下的数据利用平衡。这对于提高联邦学习的效率和实际应用具有重要意义。"}}
{"id": "2506.09846", "title": "Learning to Align: Addressing Character Frequency Distribution Shifts in Handwritten Text Recognition", "authors": ["Panagiotis Kaliosis", "John Pavlopoulos"], "summary": "Handwritten text recognition aims to convert visual input into\nmachine-readable text, and it remains challenging due to the evolving and\ncontext-dependent nature of handwriting. Character sets change over time, and\ncharacter frequency distributions shift across historical periods or regions,\noften causing models trained on broad, heterogeneous corpora to underperform on\nspecific subsets. To tackle this, we propose a novel loss function that\nincorporates the Wasserstein distance between the character frequency\ndistribution of the predicted text and a target distribution empirically\nderived from training data. By penalizing divergence from expected\ndistributions, our approach enhances both accuracy and robustness under\ntemporal and contextual intra-dataset shifts. Furthermore, we demonstrate that\ncharacter distribution alignment can also improve existing models at inference\ntime without requiring retraining by integrating it as a scoring function in a\nguided decoding scheme. Experimental results across multiple datasets and\narchitectures confirm the effectiveness of our method in boosting\ngeneralization and performance. We open source our code at\nhttps://github.com/pkaliosis/fada.", "comment": "17 pages, 10 figures, Under Review", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09846v1", "AI": {"title_translation": "学习对齐：解决手写文本识别中的字符频率分布偏移问题", "tldr": "本文提出了一种新颖的损失函数，结合 Wasserstein 距离来对齐预测文本的字符频率分布与目标分布，从而提高手写文本识别在字符频率分布偏移情况下的准确性和鲁棒性。此外，该方法还可以在推理时作为评分函数使用，无需重新训练。", "motivation": "手写文本识别（HTR）由于手写字体的不断演变和上下文依赖性而具有挑战性。字符集随时间变化，字符频率分布在不同历史时期或地区之间会发生偏移，这常常导致在广泛、异构语料库上训练的模型在特定子集上表现不佳。", "method": "提出了一种新的损失函数，该函数结合了预测文本的字符频率分布与从训练数据中经验导出的目标分布之间的 Wasserstein 距离，以惩罚与预期分布的偏差。此外，该方法还可以在推理时通过将其作为引导解码方案中的评分函数来改进现有模型，而无需重新训练。", "result": "在多个数据集和架构上的实验结果证实了该方法在提高泛化能力和性能方面的有效性。", "conclusion": "本文提出的方法通过对齐字符频率分布，有效解决了手写文本识别中因字符频率分布偏移导致的问题，显著提高了模型的准确性、鲁棒性、泛化能力和性能，甚至可以在推理阶段不进行重新训练的情况下提升表现。", "translation": "手写文本识别旨在将视觉输入转换为机器可读文本，但由于手写字体的不断演变和上下文依赖性，它仍然具有挑战性。字符集随时间变化，字符频率分布在不同历史时期或地区之间会发生偏移，这常常导致在广泛、异构语料库上训练的模型在特定子集上表现不佳。为了解决这个问题，我们提出了一种新颖的损失函数，该函数结合了预测文本的字符频率分布与从训练数据中经验导出的目标分布之间的 Wasserstein 距离。通过惩罚与预期分布的偏差，我们的方法增强了在时间上和上下文上的数据集内偏移情况下的准确性和鲁棒性。此外，我们证明了字符分布对齐也可以在推理时改进现有模型，而无需重新训练，方法是将其作为引导解码方案中的评分函数。跨多个数据集和架构的实验结果证实了我们方法在提高泛化能力和性能方面的有效性。我们已在 https://github.com/pkaliosis/fada 开源了我们的代码。", "summary": "该论文旨在解决手写文本识别（HTR）中字符频率分布偏移的问题，该问题导致模型在特定数据集上表现不佳。为此，作者提出了一种新颖的损失函数，该函数利用 Wasserstein 距离来对齐预测文本的字符频率与训练数据中推导出的目标分布。此外，该方法还展示了如何在推理时将字符分布对齐作为评分函数来改进现有模型，而无需重新训练。实验结果表明，该方法有效提升了模型的准确性、鲁棒性、泛化能力和整体性能。", "keywords": "手写文本识别, 字符频率分布, Wasserstein 距离, 分布偏移, 泛化能力", "comments": "该论文的创新之处在于将 Wasserstein 距离引入到手写文本识别中，用于解决字符频率分布偏移问题。它不仅提出了新的损失函数用于训练阶段的对齐，还展示了在推理阶段无需重新训练即可通过分布对齐来提升模型性能，这在实际应用中具有重要的实用价值。这种在训练和推理双重层面上进行分布对齐的策略，为手写文本识别的泛化能力和鲁棒性提供了新的视角和有效解决方案。"}}
{"id": "2506.09781", "title": "On the Similarities of Embeddings in Contrastive Learning", "authors": ["Chungpa Lee", "Sehee Lim", "Kibok Lee", "Jy-yong Sohn"], "summary": "Contrastive learning (CL) operates on a simple yet effective principle:\nembeddings of positive pairs are pulled together, while those of negative pairs\nare pushed apart. Although various forms of contrastive loss have been proposed\nand analyzed from different perspectives, prior works lack a comprehensive\nframework that systematically explains a broad class of these objectives. In\nthis paper, we present a unified framework for understanding CL, which is based\non analyzing the cosine similarity between embeddings of positive and negative\npairs. In full-batch settings, we show that perfect alignment of positive pairs\nis unattainable when similarities of negative pairs fall below a certain\nthreshold, and that this misalignment can be alleviated by incorporating\nwithin-view negative pairs. In mini-batch settings, we demonstrate that smaller\nbatch sizes incur stronger separation among negative pairs within batches,\nwhich leads to higher variance in similarities of negative pairs. To address\nthis limitation of mini-batch CL, we introduce an auxiliary loss term that\nreduces the variance of similarities of negative pairs in CL. Empirical results\ndemonstrate that incorporating the proposed loss consistently improves the\nperformance of CL methods in small-batch training.", "comment": "contrastive learning, representation learning, embedding, similarity,\n  negative pair, positive pair", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09781v1", "AI": {"title_translation": "对比学习中嵌入相似性研究", "tldr": "对比学习旨在拉近正样本对并推开负样本对。本文提出了一个基于余弦相似性的统一框架来理解对比学习，分析了全批次和迷你批次设置下的嵌入相似性问题，并引入了一个辅助损失项以提高迷你批次对比学习的性能，尤其是在小批次训练中。", "motivation": "现有工作缺乏一个能够系统地解释广泛对比学习目标函数的全面框架，因此需要深入理解对比学习中嵌入的相似性。", "method": "本文提出了一个统一的框架，通过分析正负样本对嵌入之间的余弦相似性来理解对比学习。为了解决迷你批次对比学习的局限性，还引入了一个辅助损失项，以减少负样本对相似性的方差。", "result": "在全批次设置中，当负样本对的相似性低于某个阈值时，正样本对的完美对齐是无法实现的，但可以通过引入视图内负样本对来缓解。在迷你批次设置中，较小的批次大小会导致批次内负样本对之间更强的分离和负样本对相似性的更高方差。引入所提出的辅助损失在小批次训练中始终能提高对比学习方法的性能。", "conclusion": "本文提出了一个理解对比学习的统一框架，揭示了在全批次和迷你批次设置下与嵌入相似性相关的问题，并引入了一个有效的辅助损失项，显著提升了小批次训练中对比学习的性能。", "translation": "对比学习（CL）遵循一个简单而有效的原则：正样本对的嵌入被拉近，而负样本对的嵌入被推开。尽管已经提出了各种形式的对比损失，并从不同角度进行了分析，但现有工作缺乏一个能够系统地解释这类目标函数的全面框架。在本文中，我们提出了一个统一的框架来理解CL，该框架基于分析正负样本对嵌入之间的余弦相似性。在全批次设置中，我们表明当负样本对的相似性低于某个阈值时，正样本对的完美对齐是无法实现的，并且这种不对齐可以通过引入视图内负样本对来缓解。在小批次设置中，我们证明了较小的批次大小会导致批次内负样本对之间更强的分离，从而导致负样本对相似性的更高方差。为了解决小批次CL的这一局限性，我们引入了一个辅助损失项，以减少CL中负样本对相似性的方差。实证结果表明，引入所提出的损失在小批量训练中始终能提高CL方法的性能。", "summary": "本文提出了一个统一的框架来理解对比学习（CL），该框架通过分析嵌入的余弦相似性来运作。研究发现，在全批次设置中，当负样本相似性低于特定阈值时，正样本对的完美对齐难以实现，但可以通过引入视图内负样本对来缓解。对于迷你批次CL，较小的批次大小会增加负样本对相似性的方差。为解决此问题，论文提出了一种辅助损失，经验证明其能有效提升小批次训练中CL方法的性能。", "keywords": "对比学习, 嵌入相似性, 统一框架, 辅助损失, 批次大小", "comments": "该论文为对比学习的目标函数行为提供了一个理论框架，尤其侧重于嵌入相似性。所提出的辅助损失项是一项实用的贡献，它能改善小批次训练下对比学习的性能，这在实际应用中是一个常见的挑战。"}}
{"id": "2506.09849", "title": "IntPhys 2: Benchmarking Intuitive Physics Understanding In Complex Synthetic Environments", "authors": ["Florian Bordes", "Quentin Garrido", "Justine T Kao", "Adina Williams", "Michael Rabbat", "Emmanuel Dupoux"], "summary": "We present IntPhys 2, a video benchmark designed to evaluate the intuitive\nphysics understanding of deep learning models. Building on the original IntPhys\nbenchmark, IntPhys 2 focuses on four core principles related to macroscopic\nobjects: Permanence, Immutability, Spatio-Temporal Continuity, and Solidity.\nThese conditions are inspired by research into intuitive physical understanding\nemerging during early childhood. IntPhys 2 offers a comprehensive suite of\ntests, based on the violation of expectation framework, that challenge models\nto differentiate between possible and impossible events within controlled and\ndiverse virtual environments. Alongside the benchmark, we provide performance\nevaluations of several state-of-the-art models. Our findings indicate that\nwhile these models demonstrate basic visual understanding, they face\nsignificant challenges in grasping intuitive physics across the four principles\nin complex scenes, with most models performing at chance levels (50%), in stark\ncontrast to human performance, which achieves near-perfect accuracy. This\nunderscores the gap between current models and human-like intuitive physics\nunderstanding, highlighting the need for advancements in model architectures\nand training methodologies.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09849v1", "AI": {"title_translation": "IntPhys 2: 在复杂合成环境中基准测试直观物理理解", "tldr": "IntPhys 2是一个新的视频基准，用于评估深度学习模型对直观物理的理解，发现当前最先进模型在这方面表现不佳，远低于人类水平。", "motivation": "为了评估深度学习模型对直观物理的理解，因为现有模型在这方面存在显著差距，本研究提出了IntPhys 2视频基准。", "method": "本研究提出了IntPhys 2视频基准，旨在评估深度学习模型对直观物理的理解。该基准基于违反预期框架，挑战模型在受控且多样化的虚拟环境中区分可能事件和不可能事件。它专注于宏观物体的四个核心原则：永恒性、不变性、时空连续性和固体性。研究还提供了几种最先进模型的性能评估。", "result": "研究结果表明，尽管最先进的模型展示了基本的视觉理解能力，但在复杂场景中掌握直观物理的四个原则方面面临重大挑战，大多数模型表现接近随机水平（50%），这与人类接近完美的准确率形成鲜明对比。", "conclusion": "当前模型与类人直观物理理解之间存在显著差距，这强调了在模型架构和训练方法方面进行改进的必要性。", "translation": "我们提出了IntPhys 2，一个旨在评估深度学习模型直观物理理解能力的视频基准。基于原始IntPhys基准，IntPhys 2专注于宏观物体的四个核心原则：永恒性、不变性、时空连续性和固体性。这些条件受到儿童早期直观物理理解研究的启发。IntPhys 2提供了一套全面的测试，基于违反预期框架，挑战模型在受控且多样化的虚拟环境中区分可能事件和不可能事件。除了基准测试，我们还提供了几个最先进模型的性能评估。我们的发现表明，尽管这些模型展示了基本的视觉理解能力，但在复杂场景中掌握直观物理的四个原则方面面临重大挑战，大多数模型表现接近随机水平（50%），这与人类接近完美的准确率形成鲜明对比。这凸显了当前模型与类人直观物理理解之间的差距，强调了在模型架构和训练方法方面进行改进的必要性。", "summary": "IntPhys 2是一个用于评估深度学习模型直观物理理解能力的视频基准。它扩展了原始IntPhys，专注于宏观物体的永恒性、不变性、时空连续性和固体性四个核心原则，这些原则受到儿童早期发展的启发。该基准采用违反预期框架，测试模型在复杂合成环境中区分可能和不可能事件的能力。对多个最先进模型的评估显示，它们在直观物理理解上表现不佳，多数仅达随机水平，远低于人类表现。这揭示了当前模型与类人直观物理理解之间的显著差距，并强调了在模型架构和训练方法上进行改进的需求。", "keywords": "直观物理, 深度学习, 基准测试, 视频理解, 违反预期", "comments": "该论文提出了一个重要的基准测试IntPhys 2，用于量化深度学习模型对直观物理的理解，填补了现有评估的空白。其创新之处在于关注儿童早期发展的直观物理原则，并通过“违反预期”框架来设计测试。研究结果揭示了当前SOTA模型在复杂物理推理上的显著局限性，对未来AI发展具有指导意义，强调了对更鲁棒和类人AI模型的迫切需求。"}}
{"id": "2506.09785", "title": "A theoretical framework for self-supervised contrastive learning for continuous dependent data", "authors": ["Alexander Marusov", "Alexander Yuhay", "Alexey Zaytsev"], "summary": "Self-supervised learning (SSL) has emerged as a powerful approach to learning\nrepresentations, particularly in the field of computer vision. However, its\napplication to dependent data, such as temporal and spatio-temporal domains,\nremains underexplored. Besides, traditional contrastive SSL methods often\nassume \\emph{semantic independence between samples}, which does not hold for\ndependent data exhibiting complex correlations. We propose a novel theoretical\nframework for contrastive SSL tailored to \\emph{continuous dependent data},\nwhich allows the nearest samples to be semantically close to each other. In\nparticular, we propose two possible \\textit{ground truth similarity measures}\nbetween objects -- \\emph{hard} and \\emph{soft} closeness. Under it, we derive\nan analytical form for the \\textit{estimated similarity matrix} that\naccommodates both types of closeness between samples, thereby introducing\ndependency-aware loss functions. We validate our approach, \\emph{Dependent\nTS2Vec}, on temporal and spatio-temporal downstream problems. Given the\ndependency patterns presented in the data, our approach surpasses modern ones\nfor dependent data, highlighting the effectiveness of our theoretically\ngrounded loss functions for SSL in capturing spatio-temporal dependencies.\nSpecifically, we outperform TS2Vec on the standard UEA and UCR benchmarks, with\naccuracy improvements of $4.17$\\% and $2.08$\\%, respectively. Furthermore, on\nthe drought classification task, which involves complex spatio-temporal\npatterns, our method achieves a $7$\\% higher ROC-AUC score.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09785v1", "AI": {"title_translation": "连续依赖数据的自监督对比学习理论框架", "tldr": "本文提出了一个针对连续依赖数据的新型自监督对比学习理论框架，通过引入依赖感知损失函数，在时间序列和时空数据任务上表现优于现有方法。", "motivation": "自监督学习（SSL）在计算机视觉领域取得了显著进展，但其在时间序列和时空等依赖数据上的应用仍未得到充分探索。传统的对比SSL方法假设样本之间语义独立，这对于展现复杂相关性的依赖数据不适用。", "method": "本文提出了一个针对连续依赖数据的新型自监督对比学习理论框架，允许最近的样本在语义上彼此接近。具体地，提出了两种可能的“真实相似性度量”——硬（hard）和软（soft）接近度。在此基础上，推导出了一个能够适应两种样本间接近度类型的“估计相似性矩阵”的解析形式，从而引入了依赖感知损失函数。该方法被称为Dependent TS2Vec。", "result": "在时间序列和时空下游问题上验证了所提出的Dependent TS2Vec方法。在标准UEA和UCR基准测试中，相较于TS2Vec，准确率分别提高了4.17%和2.08%。在涉及复杂时空模式的干旱分类任务中，该方法的ROC-AUC分数高出7%。", "conclusion": "本文提出的、基于理论基础的损失函数在自监督学习中有效捕捉了时空依赖性，并在依赖数据上超越了现有方法。", "translation": "自监督学习（SSL）已成为一种强大的表示学习方法，尤其是在计算机视觉领域。然而，其在时间、时空等依赖数据上的应用仍未得到充分探索。此外，传统的对比SSL方法通常假设样本之间存在语义独立性，这对于展现复杂相关性的依赖数据并不成立。我们提出了一个针对连续依赖数据的新型自监督对比学习理论框架，该框架允许最近的样本在语义上彼此接近。特别是，我们提出了两种可能的对象之间“真实相似性度量”——硬（hard）和软（soft）接近度。在此基础上，我们推导出了一个能够适应两种样本间接近度类型的“估计相似性矩阵”的解析形式，从而引入了依赖感知损失函数。我们在时间序列和时空下游问题上验证了我们的方法Dependent TS2Vec。鉴于数据中呈现的依赖模式，我们的方法超越了针对依赖数据的现代方法，突出了我们基于理论基础的损失函数在自监督学习中捕捉时空依赖性的有效性。具体而言，我们在标准UEA和UCR基准测试上超越了TS2Vec，准确率分别提高了4.17%和2.08%。此外，在涉及复杂时空模式的干旱分类任务中，我们的方法实现了7%更高的ROC-AUC分数。", "summary": "本文针对自监督学习在依赖数据（如时间序列和时空数据）上的局限性，提出了一个新颖的理论框架。该框架引入了硬和软两种真实相似性度量，并推导出相应的依赖感知损失函数，以处理样本间的复杂相关性。实验结果表明，该方法在多个基准测试和实际任务中，在捕捉时空依赖性方面优于现有方法，显著提高了性能。", "keywords": "自监督学习, 对比学习, 依赖数据, 时间序列, 时空数据", "comments": "该论文的创新点在于提出了一个针对连续依赖数据的自监督对比学习理论框架，并设计了适应数据依赖性的损失函数。这解决了传统自监督学习方法在处理具有复杂相关性数据时的局限性，对于时间序列和时空数据分析领域具有重要意义。"}}
{"id": "2506.09881", "title": "Leveraging Depth and Language for Open-Vocabulary Domain-Generalized Semantic Segmentation", "authors": ["Siyu Chen", "Ting Han", "Chengzheng Fu", "Changshe Zhang", "Chaolei Wang", "Jinhe Su", "Guorong Cai", "Meiliu Wu"], "summary": "Open-Vocabulary semantic segmentation (OVSS) and domain generalization in\nsemantic segmentation (DGSS) highlight a subtle complementarity that motivates\nOpen-Vocabulary Domain-Generalized Semantic Segmentation (OV-DGSS). OV-DGSS\naims to generate pixel-level masks for unseen categories while maintaining\nrobustness across unseen domains, a critical capability for real-world\nscenarios such as autonomous driving in adverse conditions. We introduce Vireo,\na novel single-stage framework for OV-DGSS that unifies the strengths of OVSS\nand DGSS for the first time. Vireo builds upon the frozen Visual Foundation\nModels (VFMs) and incorporates scene geometry via Depth VFMs to extract\ndomain-invariant structural features. To bridge the gap between visual and\ntextual modalities under domain shift, we propose three key components: (1)\nGeoText Prompts, which align geometric features with language cues and\nprogressively refine VFM encoder representations; (2) Coarse Mask Prior\nEmbedding (CMPE) for enhancing gradient flow for faster convergence and\nstronger textual influence; and (3) the Domain-Open-Vocabulary Vector Embedding\nHead (DOV-VEH), which fuses refined structural and semantic features for robust\nprediction. Comprehensive evaluation on these components demonstrates the\neffectiveness of our designs. Our proposed Vireo achieves the state-of-the-art\nperformance and surpasses existing methods by a large margin in both domain\ngeneralization and open-vocabulary recognition, offering a unified and scalable\nsolution for robust visual understanding in diverse and dynamic environments.\nCode is available at https://github.com/anonymouse-9c53tp182bvz/Vireo.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09881v1", "AI": {"title_translation": "利用深度和语言实现开放词汇域泛化语义分割", "tldr": "该研究提出了Vireo框架，结合深度和语言信息，首次统一解决了开放词汇语义分割和域泛化语义分割问题，实现了领域和类别泛化性能的SOTA。", "motivation": "开放词汇语义分割（OVSS）和语义分割中的域泛化（DGSS）具有互补性，这促使了开放词汇域泛化语义分割（OV-DGSS）的提出。OV-DGSS旨在为未见类别生成像素级掩码，同时在未见领域保持鲁棒性，这对于恶劣条件下的自动驾驶等真实世界场景至关重要。", "method": "该论文引入了Vireo，一个新颖的单阶段OV-DGSS框架，首次统一了OVSS和DGSS的优势。Vireo基于冻结的视觉基础模型（VFMs），并通过深度VFM融入场景几何信息以提取域不变的结构特征。为了弥合领域迁移下视觉和文本模态之间的差距，提出了三个关键组件：（1）GeoText Prompts，用于对齐几何特征与语言线索并逐步优化VFM编码器表示；（2）Coarse Mask Prior Embedding (CMPE)，用于增强梯度流以加速收敛和强化文本影响；以及（3）Domain-Open-Vocabulary Vector Embedding Head (DOV-VEH)，用于融合精炼的结构和语义特征以实现鲁棒预测。", "result": "对这些组件的综合评估证明了其设计的有效性。所提出的Vireo在域泛化和开放词汇识别方面都取得了最先进的性能，并大幅超越了现有方法。", "conclusion": "Vireo为在多样化和动态环境中实现鲁棒视觉理解提供了一个统一且可扩展的解决方案。", "translation": "开放词汇语义分割（OVSS）和语义分割中的域泛化（DGSS）突显了一种微妙的互补性，这促使了开放词汇域泛化语义分割（OV-DGSS）的提出。OV-DGSS旨在为未见类别生成像素级掩码，同时在未见领域保持鲁棒性，这对于恶劣条件下的自动驾驶等真实世界场景至关重要。我们引入了Vireo，一个新颖的单阶段OV-DGSS框架，首次统一了OVSS和DGSS的优势。Vireo基于冻结的视觉基础模型（VFMs），并通过深度VFM融入场景几何信息以提取域不变的结构特征。为了弥合领域迁移下视觉和文本模态之间的差距，我们提出了三个关键组件：（1）GeoText Prompts，用于对齐几何特征与语言线索并逐步优化VFM编码器表示；（2）Coarse Mask Prior Embedding (CMPE)，用于增强梯度流以加速收敛和强化文本影响；以及（3）Domain-Open-Vocabulary Vector Embedding Head (DOV-VEH)，用于融合精炼的结构和语义特征以实现鲁棒预测。对这些组件的综合评估证明了我们设计的有效性。我们提出的Vireo在域泛化和开放词汇识别方面都取得了最先进的性能，并大幅超越了现有方法，为在多样化和动态环境中实现鲁棒视觉理解提供了一个统一且可扩展的解决方案。代码可在https://github.com/anonymouse-9c53tp182bvz/Vireo 获取。", "summary": "该论文提出了Vireo，一个创新的单阶段框架，首次统一解决了开放词汇语义分割（OVSS）和域泛化语义分割（DGSS）两大挑战，旨在实现对未见类别和未见领域的鲁棒像素级分割。Vireo利用冻结的视觉基础模型和深度信息提取域不变特征，并通过GeoText Prompts、Coarse Mask Prior Embedding和Domain-Open-Vocabulary Vector Embedding Head三大核心组件，有效融合几何、视觉与文本信息，弥合领域迁移下的模态差距。实验结果表明，Vireo在域泛化和开放词汇识别上均达到了最先进的性能，为复杂多变的真实世界环境提供了统一且可扩展的视觉理解方案。", "keywords": "语义分割, 开放词汇, 域泛化, 深度学习, 视觉基础模型", "comments": "该论文的创新点在于首次将开放词汇语义分割与域泛化语义分割相结合，提出了一个统一的OV-DGSS框架Vireo。其通过整合深度信息和语言线索，以及设计独特的三大组件来解决跨领域和跨类别泛化问题，尤其是在利用深度VFM提取域不变特征和GeoText Prompts对齐多模态信息方面具有新颖性。这项工作为自动驾驶等真实世界应用提供了重要的、更具鲁棒性的视觉理解能力。"}}
{"id": "2506.09408", "title": "Token Constraint Decoding Improves Robustness on Question Answering for Large Language Models", "authors": ["Jui-Ming Yao", "Hao-Yuan Chen", "Zi-Xian Tang", "Bing-Jia Tan", "Sheng-Wei Peng", "Bing-Cheng Xie", "Shun-Feng Su"], "summary": "Large Language Models (LLMs) have demonstrated impressive performance on\nmultiple-choice question answering (MCQA) benchmarks, yet they remain highly\nvulnerable to minor input perturbations. In this paper, we introduce and\nevaluate Token Constraint Decoding (TCD). This simple yet effective\ninference-time algorithm enforces alignment between token-level predictions to\nenhance robustness in noisy settings. Through extensive experiments on\nCommonsenseQA, MMLU, and MMLU-Pro, we show that TCD, especially when paired\nwith prompt engineering (PE) fixes, significantly restores performance degraded\nby input noise, yielding up to +39\\% absolute gains for weaker models like\nGemma3 1B. Penalty sweep analyses further reveal that TCD implicitly\nregularizes overconfident outputs, with different models requiring distinct\npenalty schedules to maximize resilience. Our findings establish TCD as a\npractical, model-agnostic approach for improving reasoning stability under\nreal-world imperfections and pave the way for more reliable deployment of LLMs\nin safety-critical or user-facing applications.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.09408v1", "AI": {"title_translation": "令牌约束解码提高大型语言模型在问答任务上的鲁棒性", "tldr": "大型语言模型（LLMs）易受噪声影响。本文提出了令牌约束解码（TCD），这是一种推理时算法，显著提高了LLMs在问答任务上对输入噪声的鲁棒性，尤其是在结合提示工程时。", "motivation": "大型语言模型（LLMs）在多项选择问答（MCQA）基准测试中表现出色，但它们极易受到微小输入扰动的影响，这阻碍了其在实际应用中的可靠部署。", "method": "本文引入并评估了令牌约束解码（TCD），这是一种简单而有效的推理时算法。TCD通过强制执行令牌级预测之间的一致性来增强在噪声环境中的鲁棒性，并且可以与提示工程（PE）修复结合使用。", "result": "TCD显著恢复了因输入噪声而下降的性能，在CommonsenseQA、MMLU和MMLU-Pro等基准测试中，对于Gemma3 1B等较弱的模型，绝对增益高达+39%。惩罚扫描分析表明，TCD隐式地规范了过于自信的输出，不同模型需要不同的惩罚策略来最大限度地提高弹性。", "conclusion": "TCD是一种实用、模型无关的方法，用于改善大型语言模型在真实世界缺陷下的推理稳定性，为LLMs在安全关键或面向用户的应用中更可靠的部署铺平了道路。", "translation": "大型语言模型（LLMs）在多项选择问答（MCQA）基准测试中表现出色，但它们仍然极易受到微小输入扰动的影响。在本文中，我们介绍并评估了令牌约束解码（TCD）。这种简单而有效的推理时算法强制执行令牌级预测之间的一致性，以增强在噪声环境中的鲁棒性。通过在CommonsenseQA、MMLU和MMLU-Pro上的大量实验，我们表明TCD，特别是与提示工程（PE）修复结合时，显著恢复了因输入噪声而下降的性能，对于Gemma3 1B等较弱的模型，绝对增益高达+39%。惩罚扫描分析进一步揭示，TCD隐式地规范了过于自信的输出，不同的模型需要不同的惩罚策略来最大限度地提高弹性。我们的研究结果表明，TCD是一种实用、模型无关的方法，用于改善LLMs在真实世界缺陷下的推理稳定性，并为LLMs在安全关键或面向用户的应用中更可靠的部署铺平了道路。", "summary": "本文提出了一种名为令牌约束解码（TCD）的推理时算法，旨在提高大型语言模型（LLMs）在问答任务中对抗输入扰动的鲁棒性。TCD通过强制执行令牌级预测一致性来工作。在CommonsenseQA和MMLU等基准测试上的广泛实验表明，TCD，尤其是在与提示工程结合时，能显著恢复因输入噪声而下降的性能，为较弱模型带来高达+39%的绝对增益。研究还发现TCD能隐式地规范过于自信的输出。这些发现确立了TCD作为一种实用且模型无关的方法，可改善LLMs在真实世界条件下的推理稳定性，促进其在关键应用中的可靠部署。", "keywords": "令牌约束解码, 大型语言模型, 鲁棒性, 问答, 推理时算法", "comments": "本文的创新之处在于提出了TCD，一个简单但有效的推理时算法，解决了LLMs对输入噪声敏感的关键问题。其模型无关的特性和显著的性能提升（尤其对较弱模型）使其在实际部署中具有重要价值。TCD能够隐式地规范过于自信的输出，进一步增强了LLMs的可靠性。"}}
{"id": "2506.09883", "title": "3D-Aware Vision-Language Models Fine-Tuning with Geometric Distillation", "authors": ["Seonho Lee", "Jiho Choi", "Inha Kang", "Jiwook Kim", "Junsung Park", "Hyunjung Shim"], "summary": "Vision-Language Models (VLMs) have shown remarkable performance on diverse\nvisual and linguistic tasks, yet they remain fundamentally limited in their\nunderstanding of 3D spatial structures. We propose Geometric Distillation, a\nlightweight, annotation-free fine-tuning framework that injects human-inspired\ngeometric cues into pretrained VLMs without modifying their architecture. By\ndistilling (1) sparse correspondences, (2) relative depth relations, and (3)\ndense cost volumes from off-the-shelf 3D foundation models (e.g., MASt3R,\nVGGT), our method shapes representations to be geometry-aware while remaining\ncompatible with natural image-text inputs. Through extensive evaluations on 3D\nvision-language reasoning and 3D perception benchmarks, our method consistently\noutperforms prior approaches, achieving improved 3D spatial reasoning with\nsignificantly lower computational cost. Our work demonstrates a scalable and\nefficient path to bridge 2D-trained VLMs with 3D understanding, opening up\nwider use in spatially grounded multimodal tasks.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09883v1", "AI": {"title_translation": "基于几何蒸馏的3D感知视觉-语言模型微调", "tldr": "本文提出了一种名为几何蒸馏的轻量级、无需标注的微调框架，通过从现成的3D基础模型中提取几何线索，有效地将3D空间理解能力注入到预训练的视觉-语言模型中，并在3D视觉-语言推理和3D感知基准上取得了显著优于现有方法的性能，同时计算成本更低。", "motivation": "视觉-语言模型（VLMs）在各种视觉和语言任务中表现出色，但它们在理解3D空间结构方面存在根本性限制。", "method": "本文提出了一种名为几何蒸D馏的轻量级、无需标注的微调框架，该框架在不修改预训练VLM架构的情况下，通过蒸馏（1）稀疏对应关系、（2）相对深度关系和（3）密集成本体，从现成的3D基础模型（如MASt3R, VGGT）中注入受人类启发的几何线索，从而使表示具有几何感知能力，同时兼容自然图像-文本输入。", "result": "在3D视觉-语言推理和3D感知基准上的广泛评估表明，该方法始终优于现有方法，以显著更低的计算成本实现了改进的3D空间推理能力。", "conclusion": "该工作展示了一条可扩展且高效的路径，可以将2D训练的视觉-语言模型与3D理解相结合，从而在空间定位的多模态任务中开辟更广泛的应用。", "translation": "视觉-语言模型（VLMs）在各种视觉和语言任务中表现出色，但它们在理解3D空间结构方面存在根本性限制。我们提出了一种名为几何蒸馏的轻量级、无需标注的微调框架，该框架在不修改预训练VLM架构的情况下，将受人类启发的几何线索注入到预训练的VLM中。通过从现成的3D基础模型（例如MASt3R、VGGT）中蒸馏（1）稀疏对应关系、（2）相对深度关系和（3）密集成本体，我们的方法使表示具有几何感知能力，同时保持与自然图像-文本输入的兼容性。通过在3D视觉-语言推理和3D感知基准上的广泛评估，我们的方法始终优于现有方法，以显著更低的计算成本实现了改进的3D空间推理能力。我们的工作展示了一条可扩展且高效的路径，可以将2D训练的VLM与3D理解相结合，从而在空间定位的多模态任务中开辟更广泛的应用。", "summary": "本文介绍了一种名为几何蒸馏的创新微调框架，旨在解决视觉-语言模型（VLMs）在3D空间理解方面的局限性。该方法通过从现有3D基础模型中蒸馏稀疏对应、相对深度和密集成本体等几何信息，无需标注即可将3D感知能力注入到预训练的2D VLM中，且不改变其核心架构。实验证明，该方法在3D视觉-语言推理和感知任务上表现出优越性，且计算效率高，为VLM的3D理解提供了可扩展的解决方案。", "keywords": "几何蒸馏, 视觉-语言模型, 3D理解, 微调, 空间推理", "comments": "该论文提出的几何蒸馏方法具有显著的创新性，它通过一种轻量级、无需标注的方式，有效地弥补了2D训练VLM在3D空间理解上的不足，且不修改原有架构，这使得其具有很高的实用性和兼容性。其在性能提升和计算成本降低方面的优势，预示着该方法在未来3D感知和多模态任务中具有广泛的应用潜力。"}}
{"id": "2506.09885", "title": "The Less You Depend, The More You Learn: Synthesizing Novel Views from Sparse, Unposed Images without Any 3D Knowledge", "authors": ["Haoru Wang", "Kai Ye", "Yangyan Li", "Wenzheng Chen", "Baoquan Chen"], "summary": "We consider the problem of generalizable novel view synthesis (NVS), which\naims to generate photorealistic novel views from sparse or even unposed 2D\nimages without per-scene optimization. This task remains fundamentally\nchallenging, as it requires inferring 3D structure from incomplete and\nambiguous 2D observations. Early approaches typically rely on strong 3D\nknowledge, including architectural 3D inductive biases (e.g., embedding\nexplicit 3D representations, such as NeRF or 3DGS, into network design) and\nground-truth camera poses for both input and target views. While recent efforts\nhave sought to reduce the 3D inductive bias or the dependence on known camera\nposes of input views, critical questions regarding the role of 3D knowledge and\nthe necessity of circumventing its use remain under-explored. In this work, we\nconduct a systematic analysis on the 3D knowledge and uncover a critical trend:\nthe performance of methods that requires less 3D knowledge accelerates more as\ndata scales, eventually achieving performance on par with their 3D\nknowledge-driven counterparts, which highlights the increasing importance of\nreducing dependence on 3D knowledge in the era of large-scale data. Motivated\nby and following this trend, we propose a novel NVS framework that minimizes 3D\ninductive bias and pose dependence for both input and target views. By\neliminating this 3D knowledge, our method fully leverages data scaling and\nlearns implicit 3D awareness directly from sparse 2D images, without any 3D\ninductive bias or pose annotation during training. Extensive experiments\ndemonstrate that our model generates photorealistic and 3D-consistent novel\nviews, achieving even comparable performance with methods that rely on posed\ninputs, thereby validating the feasibility and effectiveness of our\ndata-centric paradigm. Project page:\nhttps://pku-vcl-geometry.github.io/Less3Depend/ .", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09885v1", "AI": {"title_translation": "依赖越少，学到越多：从稀疏、未摆放的图像中合成新颖视图，无需任何3D知识", "tldr": "该研究提出了一种新的通用可泛化新颖视图合成（NVS）框架，该框架通过最小化对3D知识和姿态的依赖，直接从稀疏的2D图像中学习隐式3D感知，从而在数据扩展时获得与依赖3D知识的方法相当的性能。", "motivation": "通用的新颖视图合成（NVS）任务面临从不完整和模糊的2D观测中推断3D结构的挑战。早期方法严重依赖3D知识（如显式3D表示和真值相机姿态）。尽管近期研究试图减少对3D知识的依赖，但3D知识的作用及其规避的必要性仍未得到充分探索。本文旨在系统分析3D知识，并探索在数据规模化时，减少3D知识依赖的方法的性能加速趋势。", "method": "本文首先对3D知识进行了系统分析，发现随着数据规模的扩大，对3D知识依赖较少的方法性能提升更快。受此趋势启发，研究提出了一种新颖的NVS框架，该框架最大限度地减少了对3D归纳偏差和输入/目标视图姿态的依赖。通过消除这些3D知识，该方法充分利用数据扩展，并在训练过程中不使用任何3D归纳偏差或姿态标注，直接从稀疏2D图像中学习隐式3D感知。", "result": "实验结果表明，该模型能够生成逼真且3D一致的新颖视图，甚至实现了与依赖已知姿态输入的方法相当的性能，这验证了其以数据为中心范式的可行性和有效性。", "conclusion": "该研究发现，在数据规模化时代，减少对3D知识的依赖对于新颖视图合成越来越重要。通过提出一种最小化3D知识依赖的新框架，研究证明了在没有显式3D知识的情况下，仅通过数据驱动的方式也能实现高性能的新颖视图合成，并与依赖3D知识的方法相媲美。", "translation": "我们考虑通用可泛化新颖视图合成（NVS）问题，该问题旨在无需逐场景优化的情况下，从稀疏甚至未摆放的2D图像中生成逼真的新颖视图。这项任务仍然具有根本性的挑战性，因为它需要从不完整和模糊的2D观测中推断3D结构。早期方法通常依赖强大的3D知识，包括架构3D归纳偏差（例如，将NeRF或3DGS等显式3D表示嵌入到网络设计中）以及输入和目标视图的真值相机姿态。尽管最近的努力试图减少3D归纳偏差或对已知输入视图相机姿态的依赖，但关于3D知识的作用以及规避其使用的必要性的关键问题仍未得到充分探索。在这项工作中，我们对3D知识进行了系统分析，并揭示了一个关键趋势：随着数据规模的扩大，对3D知识要求较少的方法的性能加速更快，最终达到了与3D知识驱动方法相当的性能，这凸显了在大规模数据时代减少对3D知识依赖的重要性。受此趋势的启发并遵循这一趋势，我们提出了一种新颖的NVS框架，该框架最大限度地减少了输入和目标视图的3D归纳偏差和姿态依赖。通过消除这种3D知识，我们的方法充分利用了数据扩展，并直接从稀疏2D图像中学习隐式3D感知，在训练过程中无需任何3D归纳偏差或姿态标注。大量实验表明，我们的模型生成了逼真且3D一致的新颖视图，甚至实现了与依赖已知姿态输入的方法相当的性能，从而验证了我们以数据为中心范式的可行性和有效性。", "summary": "本文探讨了通用可泛化新颖视图合成（NVS）中3D知识的作用。通过系统分析，研究发现随着数据规模的扩大，减少对3D知识依赖的方法性能提升更快，最终可与依赖3D知识的方法媲美。受此启发，研究提出了一种新的NVS框架，该框架通过最小化3D归纳偏差和姿态依赖，直接从稀疏2D图像中学习隐式3D感知。实验证明，该方法在无需3D知识的情况下，能生成高质量且3D一致的新颖视图，并取得与依赖姿态输入方法相当的性能，验证了其数据中心范式的有效性。", "keywords": "新颖视图合成, 3D知识, 姿态估计, 数据驱动, 泛化能力", "comments": "该论文通过深入分析3D知识在NVS中的作用，揭示了在大规模数据背景下，减少对3D知识依赖的重要性，这具有重要的理论和实践意义。其提出的无3D知识依赖的框架，通过数据驱动的方式实现了与传统方法相当甚至超越的性能，为未来NVS研究提供了新的方向，尤其是在处理无姿态或稀疏数据场景下的潜力巨大。这种“依赖越少，学到越多”的范式创新性地利用了数据规模的优势，有望推动该领域迈向更通用、更鲁棒的解决方案。"}}
{"id": "2506.09813", "title": "Metritocracy: Representative Metrics for Lite Benchmarks", "authors": ["Ariel Procaccia", "Benjamin Schiffer", "Serena Wang", "Shirley Zhang"], "summary": "A common problem in LLM evaluation is how to choose a subset of metrics from\na full suite of possible metrics. Subset selection is usually done for\nefficiency or interpretability reasons, and the goal is often to select a\n``representative'' subset of metrics. However, ``representative'' is rarely\nclearly defined. In this work, we use ideas from social choice theory to\nformalize two notions of representation for the selection of a subset of\nevaluation metrics. We first introduce positional representation, which\nguarantees every alternative is sufficiently represented at every position\ncutoff. We then introduce positional proportionality, which guarantees no\nalternative is proportionally over- or under-represented by more than a small\nerror at any position. We prove upper and lower bounds on the smallest number\nof metrics needed to guarantee either of these properties in the worst case. We\nalso study a generalized form of each property that allows for additional input\non groups of metrics that must be represented. Finally, we tie theory to\npractice through real-world case studies on both LLM evaluation and hospital\nquality evaluation.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09813v1", "AI": {"title_translation": "指标民主：轻量级基准的代表性指标", "tldr": "本研究利用社会选择理论，正式定义了大型语言模型（LLM）评估中选择“代表性”指标子集的两种概念，并推导了所需指标数量的理论界限，通过LLM和医院质量评估的案例研究验证了其应用。", "motivation": "大型语言模型（LLM）评估中存在一个常见问题：如何从所有可能的指标中选择一个子集。通常为了效率和可解释性而进行子集选择，目标是选出“代表性”的指标子集，但“代表性”这一概念很少被明确定义。", "method": "本研究利用社会选择理论，形式化了评估指标子集选择的两种代表性概念：1. 位置代表性：保证每个备选方案在每个位置截止点都得到充分代表。2. 位置比例性：保证在任何位置，任何备选方案的比例过高或过低代表性误差不超过一个小的范围。作者证明了在最坏情况下，保证这些属性所需的最小指标数量的上限和下限，并研究了每种属性的广义形式。最后，通过大型语言模型评估和医院质量评估的真实案例研究，将理论与实践相结合。", "result": "1. 形式化了两种代表性概念：位置代表性和位置比例性。2. 证明了在最坏情况下，保证这些属性所需的最小指标数量的上限和下限。3. 研究了每种属性的广义形式，允许对必须代表的指标组进行额外输入。4. 通过LLM评估和医院质量评估的真实案例研究，验证了理论的实践应用。", "conclusion": "本研究为定义和实现“代表性”指标子集提供了一个形式化框架，提供了理论保证，并在大型语言模型和医院质量评估中展现了实际应用价值。", "translation": "大型语言模型（LLM）评估中一个常见问题是如何从一整套可能的指标中选择一个子集。选择子集通常是为了效率或可解释性，目标通常是选择一个“代表性”的指标子集。然而，“代表性”很少被明确定义。在这项工作中，我们利用社会选择理论的思想，形式化了评估指标子集选择的两种代表性概念。我们首先引入位置代表性，它保证每个备选方案在每个位置截止点都得到充分代表。然后我们引入位置比例性，它保证在任何位置，任何备选方案的比例过高或过低代表性误差不超过一个小的范围。我们证明了在最坏情况下，保证这些属性所需的最小指标数量的上限和下限。我们还研究了每种属性的广义形式，允许对必须代表的指标组进行额外输入。最后，我们通过在大型语言模型评估和医院质量评估方面的真实案例研究，将理论与实践相结合。", "summary": "本论文旨在解决大型语言模型（LLM）评估中选择“代表性”指标子集时“代表性”概念定义不清的问题。作者利用社会选择理论，形式化了两种代表性概念：位置代表性和位置比例性，并推导了保证这些属性所需的最小指标数量的理论界限。研究还探讨了这些属性的广义形式。最后，通过在LLM评估和医院质量评估中的真实案例研究，展示了理论的实际应用。", "keywords": "LLM评估, 指标选择, 社会选择理论, 代表性指标, 基准测试", "comments": "该论文创新性地将社会选择理论应用于AI评估中的指标选择这一实际问题，特别是针对大型语言模型。它为“代表性”在这一语境下的含义提供了严谨的理论基础，超越了直观定义。理论界限和真实世界的案例研究增强了其重要性，为设计高效且可解释的基准测试提供了概念清晰度和实践指导。"}}
{"id": "2506.09895", "title": "EquiCaps: Predictor-Free Pose-Aware Pre-Trained Capsule Networks", "authors": ["Athinoulla Konstantinou", "Georgios Leontidis", "Mamatha Thota", "Aiden Durrant"], "summary": "Learning self-supervised representations that are invariant and equivariant\nto transformations is crucial for advancing beyond traditional visual\nclassification tasks. However, many methods rely on predictor architectures to\nencode equivariance, despite evidence that architectural choices, such as\ncapsule networks, inherently excel at learning interpretable pose-aware\nrepresentations. To explore this, we introduce EquiCaps (Equivariant Capsule\nNetwork), a capsule-based approach to pose-aware self-supervision that\neliminates the need for a specialised predictor for enforcing equivariance.\nInstead, we leverage the intrinsic pose-awareness capabilities of capsules to\nimprove performance in pose estimation tasks. To further challenge our\nassumptions, we increase task complexity via multi-geometric transformations to\nenable a more thorough evaluation of invariance and equivariance by introducing\n3DIEBench-T, an extension of a 3D object-rendering benchmark dataset. Empirical\nresults demonstrate that EquiCaps outperforms prior state-of-the-art\nequivariant methods on rotation prediction, achieving a supervised-level $R^2$\nof 0.78 on the 3DIEBench rotation prediction benchmark and improving upon SIE\nand CapsIE by 0.05 and 0.04 $R^2$, respectively. Moreover, in contrast to\nnon-capsule-based equivariant approaches, EquiCaps maintains robust equivariant\nperformance under combined geometric transformations, underscoring its\ngeneralisation capabilities and the promise of predictor-free capsule\narchitectures.", "comment": "19 pages, 11 Figures, 13 Tables", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09895v1", "AI": {"title_translation": "EquiCaps：无预测器姿态感知预训练胶囊网络", "tldr": "EquiCaps是一种无需预测器即可实现姿态感知自监督表示学习的胶囊网络，在旋转预测任务上超越了现有最先进的等变方法。", "motivation": "为了超越传统视觉分类任务，学习对变换保持不变性和等变性的自监督表示至关重要。然而，许多现有方法依赖预测器架构来编码等变性，尽管胶囊网络等架构本身在学习可解释的姿态感知表示方面表现出色，这表明预测器可能不是必需的。", "method": "本文提出了EquiCaps（等变胶囊网络），这是一种基于胶囊的姿态感知自监督方法，其核心在于利用胶囊固有的姿态感知能力，从而消除了强制等变性所需的专用预测器。为了更全面地评估不变性和等变性，研究通过多几何变换增加了任务复杂性，并引入了3DIEBench-T数据集（一个3D对象渲染基准数据集的扩展）。", "result": "实验结果表明，EquiCaps在旋转预测方面优于先前的最先进等变方法，在3DIEBench旋转预测基准上实现了0.78的监督级$R^2$。它分别比SIE和CapsIE的$R^2$提高了0.05和0.04。此外，与非胶囊基的等变方法相比，EquiCaps在组合几何变换下仍能保持鲁棒的等变性能。", "conclusion": "EquiCaps的成功证明了其出色的泛化能力，并突显了无预测器胶囊网络架构在实现等变表示学习方面的巨大潜力。", "translation": "学习对变换保持不变性和等变性的自监督表示对于超越传统视觉分类任务至关重要。然而，许多方法依赖预测器架构来编码等变性，尽管有证据表明胶囊网络等架构选择在学习可解释的姿态感知表示方面具有固有的优势。为了探索这一点，我们引入了EquiCaps（等变胶囊网络），这是一种基于胶囊的姿态感知自监督方法，它消除了强制等变性所需的专用预测器。相反，我们利用胶囊固有的姿态感知能力来提高姿态估计任务的性能。为了进一步挑战我们的假设，我们通过多几何变换增加了任务复杂性，通过引入3DIEBench-T（一个3D对象渲染基准数据集的扩展）来更彻底地评估不变性和等变性。实验结果表明，EquiCaps在旋转预测方面优于先前的最先进等变方法，在3DIEBench旋转预测基准上实现了0.78的监督级$R^2$，并且分别比SIE和CapsIE提高了0.05和0.04 $R^2$。此外，与非基于胶囊的等变方法相比，EquiCaps在组合几何变换下保持了鲁棒的等变性能，这突显了其泛化能力和无预测器胶囊架构的潜力。", "summary": "本文介绍了EquiCaps，一种用于姿态感知自监督表示学习的等变胶囊网络。与需要专用预测器来实现等变性的现有方法不同，EquiCaps利用胶囊网络固有的姿态感知能力，从而无需额外的预测器。该方法在扩展的3D对象渲染基准数据集3DIEBench-T上进行了多几何变换的评估，结果显示EquiCaps在旋转预测方面达到了最先进的性能，并在组合几何变换下保持了鲁棒的等变性，这证明了无预测器胶囊架构的潜力。", "keywords": "EquiCaps, 胶囊网络, 自监督学习, 姿态感知, 等变性", "comments": "EquiCaps的创新之处在于它证明了胶囊网络本身就具备强大的姿态感知能力，从而实现了无需额外预测器的等变表示学习。这不仅简化了网络架构，还在性能上取得了显著提升，强调了胶囊固有的优势。其重要性在于推动了姿态估计领域的自监督学习进展，并为实现等变性提供了一种更高效、更简洁的架构设计。"}}
{"id": "2506.09428", "title": "Improved Supervised Fine-Tuning for Large Language Models to Mitigate Catastrophic Forgetting", "authors": ["Fei Ding", "Baiqiao Wang"], "summary": "Supervised Fine-Tuning (SFT), while enhancing large language models(LLMs)'\ninstruction-following capabilities and domain-specific task adaptability, often\ndiminishes their general capabilities. Moreover, due to the inaccessibility of\noriginal pre-training data, catastrophic forgetting tends to be exacerbated\nwhen third-party practitioners implement SFT on open-sourced models. To address\nthis challenge, we propose a novel, more cost-effective SFT method which could\neffectively reduce the risk of catastrophic forgetting without access to\noriginal SFT data. Our approach begins by reconstructing the likely SFT\ninstruction distribution of the base model, followed by a multi-model screening\nprocess to select optimal data, which is then mixed with new data for SFT.\nExperimental results demonstrate that our method preserves generalization\ncapabilities in general domains while improving task-specific performance.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.09428v1", "AI": {"title_translation": "大型语言模型改进监督微调以缓解灾难性遗忘", "tldr": "监督微调（SFT）虽然能增强大型语言模型（LLMs）的指令遵循能力，但常会削弱其通用能力并加剧灾难性遗忘。本文提出一种新颖且更具成本效益的SFT方法，通过重建基础模型的SFT指令分布并进行多模型筛选来选择数据，有效缓解了灾难性遗忘，同时保持了通用能力并提升了任务特定性能。", "motivation": "监督微调（SFT）在增强大型语言模型（LLMs）的指令遵循能力和领域特定任务适应性时，通常会削弱其通用能力。此外，由于无法获取原始预训练数据，第三方实践者在开源模型上实施SFT时，灾难性遗忘问题往往会加剧。", "method": "本文提出了一种新颖且更具成本效益的SFT方法，无需访问原始SFT数据即可有效降低灾难性遗忘的风险。该方法首先重建基础模型可能的SFT指令分布，然后通过多模型筛选过程选择最佳数据，最后将其与新数据混合进行SFT。", "result": "实验结果表明，该方法在提高任务特定性能的同时，保留了通用领域的泛化能力。", "conclusion": "本文提出的SFT方法能有效缓解大型语言模型在微调过程中出现的灾难性遗忘问题，并在不访问原始SFT数据的情况下，保持了模型的通用能力并提高了任务特定性能。", "translation": "监督微调（SFT）在增强大型语言模型（LLMs）的指令遵循能力和领域特定任务适应性时，通常会削弱其通用能力。此外，由于无法获取原始预训练数据，第三方实践者在开源模型上实施SFT时，灾难性遗忘问题往往会加剧。为了解决这一挑战，我们提出了一种新颖、更具成本效益的SFT方法，该方法无需访问原始SFT数据即可有效降低灾难性遗忘的风险。我们的方法首先重建基础模型可能的SFT指令分布，然后通过多模型筛选过程选择最佳数据，最后将其与新数据混合进行SFT。实验结果表明，我们的方法在提高任务特定性能的同时，保留了通用领域的泛化能力。", "summary": "本文提出了一种改进的监督微调（SFT）方法，旨在解决大型语言模型（LLMs）在SFT过程中出现的灾难性遗忘问题。该方法通过重建基础模型的SFT指令分布，并结合多模型筛选来选择和混合数据，无需原始SFT数据即可有效缓解遗忘。实验证明，此方法在提升任务特定性能的同时，能有效保持模型的泛化能力。", "keywords": "监督微调, 大型语言模型, 灾难性遗忘, 数据重建, 多模型筛选", "comments": "该论文的创新之处在于提出了一种无需原始SFT数据即可缓解灾难性遗忘的成本效益型SFT方法，这对于第三方实践者尤其重要。其在保留模型通用能力的同时提升特定任务表现的能力，解决了LLM微调中的一个关键挑战。"}}
{"id": "2506.09816", "title": "Identifiability Challenges in Sparse Linear Ordinary Differential Equations", "authors": ["Cecilia Casolo", "Sören Becker", "Niki Kilbertus"], "summary": "Dynamical systems modeling is a core pillar of scientific inquiry across\nnatural and life sciences. Increasingly, dynamical system models are learned\nfrom data, rendering identifiability a paramount concept. For systems that are\nnot identifiable from data, no guarantees can be given about their behavior\nunder new conditions and inputs, or about possible control mechanisms to steer\nthe system. It is known in the community that \"linear ordinary differential\nequations (ODE) are almost surely identifiable from a single trajectory.\"\nHowever, this only holds for dense matrices. The sparse regime remains\nunderexplored, despite its practical relevance with sparsity arising naturally\nin many biological, social, and physical systems. In this work, we address this\ngap by characterizing the identifiability of sparse linear ODEs. Contrary to\nthe dense case, we show that sparse systems are unidentifiable with a positive\nprobability in practically relevant sparsity regimes and provide lower bounds\nfor this probability. We further study empirically how this theoretical\nunidentifiability manifests in state-of-the-art methods to estimate linear ODEs\nfrom data. Our results corroborate that sparse systems are also practically\nunidentifiable. Theoretical limitations are not resolved through inductive\nbiases or optimization dynamics. Our findings call for rethinking what can be\nexpected from data-driven dynamical system modeling and allows for quantitative\nassessments of how much to trust a learned linear ODE.", "comment": "9 pages, 4 figures", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09816v1", "AI": {"title_translation": "稀疏线性常微分方程中的可识别性挑战", "tldr": "与密集情况不同，稀疏线性常微分方程在实际相关稀疏性条件下具有正概率不可识别性，这挑战了数据驱动的动力系统建模。", "motivation": "动力系统建模在科学探究中至关重要，但从数据中学习模型时，可识别性是一个关键概念。现有研究表明密集线性常微分方程几乎总是可识别的，但稀疏线性常微分方程的可识别性仍未被充分探索，尽管其在生物、社会和物理系统中具有重要的实际意义。本研究旨在填补这一空白。", "method": "通过表征稀疏线性常微分方程的可识别性来解决这一问题。研究人员证明了在实际相关稀疏性条件下，稀疏系统以正概率不可识别，并提供了这种概率的下限。此外，还通过实证研究了这种理论上的不可识别性如何体现在最先进的线性常微分方程估计方法中。", "result": "研究结果表明，与密集情况相反，稀疏系统在实际相关的稀疏性条件下以正概率不可识别，并提供了这种概率的下限。实证研究也证实，稀疏系统在实践中同样不可识别，理论限制无法通过归纳偏差或优化动态来解决。", "conclusion": "研究结果要求重新思考数据驱动的动力系统建模的预期，并允许对学习到的线性常微分方程的可信度进行定量评估。", "translation": "动力系统建模是自然科学和生命科学中科学探究的核心支柱。越来越多地，动力系统模型是从数据中学习的，这使得可识别性成为一个至关重要的概念。对于无法从数据中识别的系统，无法保证其在新条件和输入下的行为，也无法保证控制系统的方法。业内已知“线性常微分方程（ODE）几乎肯定可以从单一轨迹中识别”。然而，这仅适用于密集矩阵。稀疏状态仍然未被充分探索，尽管其在许多生物、社会和物理系统中自然产生的稀疏性具有实际相关性。在这项工作中，我们通过表征稀疏线性ODE的可识别性来解决这一空白。与密集情况相反，我们表明稀疏系统在实际相关的稀疏性条件下以正概率不可识别，并提供了这种概率的下限。我们进一步通过实证研究了这种理论上的不可识别性如何体现在最先进的从数据估计线性ODE的方法中。我们的结果证实，稀疏系统在实践中也同样不可识别。理论限制无法通过归纳偏差或优化动态来解决。我们的发现要求重新思考数据驱动的动力系统建模的预期，并允许对学习到的线性ODE的可信度进行定量评估。", "summary": "本研究探讨了稀疏线性常微分方程（ODE）的可识别性挑战。与已知密集ODE几乎总是可识别的情况不同，作者发现稀疏系统在实际相关的稀疏性条件下以正概率不可识别，并提供了相应的概率下限。实证分析进一步证实了现有数据驱动方法在面对稀疏ODE时也存在实际不可识别性。这些发现提示我们需要重新审视数据驱动动力系统建模的预期，并对学习模型的可靠性进行量化评估。", "keywords": "可识别性, 稀疏线性常微分方程, 动力系统建模, 数据驱动, 不可识别性", "comments": "这篇论文揭示了在数据驱动的动力系统建模中，稀疏性对模型可识别性的深远影响。它挑战了“线性ODE几乎总是可识别”的普遍认知，并指出这一结论仅适用于密集情况。该研究的创新之处在于，首次量化了稀疏线性ODE的不可识别性，并提供了理论和实证证据。其重要性在于，为数据科学家和研究人员提供了关于何时以及在何种程度上可以信任从数据中学习到的稀疏动力系统模型的关键见解，对于指导未来的建模实践和方法开发具有重要意义。"}}
{"id": "2506.09897", "title": "CEM-FBGTinyDet: Context-Enhanced Foreground Balance with Gradient Tuning for tiny Objects", "authors": ["Tao Liu", "Zhenchao Cui"], "summary": "Tiny object detection (TOD) reveals a fundamental flaw in feature pyramid\nnetworks: high-level features (P5-P6) frequently receive zero positive anchors\nunder standard label assignment protocols, leaving their semantic\nrepresentations untrained due to exclusion from loss computation. This creates\ndual deficiencies: (1) Stranded high-level features become semantic dead-ends\nwithout gradient updates, while (2) low-level features lack essential semantic\ncontext for robust classification. We propose E-FPN-BS that systematically\nconverts wasted high-level semantics into low-level feature enhancements. To\naddress these issues, we propose E-FPN-BS, a novel architecture integrating\nmulti-scale feature enhancement and adaptive optimization. First, our Context\nEnhancement Module(CEM) employs dual-branch processing to align and compress\nhigh-level features for effective global-local fusion. Second, the\nForeground-Background Separation Module (FBSM) generates spatial gating masks\nthat dynamically amplify discriminative regions. To address gradient imbalance\nacross object scales, we further propose a Dynamic Gradient-Balanced Loss\n(DCLoss) that automatically modulates loss contributions via scale-aware\ngradient equilibrium. Extensive experiments across multiple benchmark datasets\ndemonstrate the outstanding performance and generalization ability of our\napproach.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09897v1", "AI": {"title_translation": "CEM-FBGTinyDet: 用于微小目标的上下文增强前景平衡与梯度调优", "tldr": "该论文解决了特征金字塔网络在微小目标检测中高层特征未训练和语义上下文缺乏的问题。它提出了E-FPN-BS，包含CEM、FBSM和DCLoss，旨在增强特征并平衡梯度，从而实现出色的性能。", "motivation": "微小目标检测（TOD）揭示了特征金字塔网络（FPNs）的一个根本缺陷：高层特征（P5-P6）在标准标签分配协议下经常接收到零个正锚点，导致其语义表示未被训练，并被排除在损失计算之外。这产生了双重缺陷：（1）滞留的高层特征在没有梯度更新的情况下成为语义死胡同；（2）低层特征缺乏鲁棒分类所需的基本语义上下文。", "method": "本文提出了E-FPN-BS，这是一种集成了多尺度特征增强和自适应优化的新型架构。它包括：1. 上下文增强模块（CEM）：采用双分支处理来对齐和压缩高层特征，以实现有效的全局-局部融合。2. 前景-背景分离模块（FBSM）：生成空间门控掩码，动态放大判别区域。3. 动态梯度平衡损失（DCLoss）：通过尺度感知梯度均衡自动调节损失贡献，以解决目标尺度上的梯度不平衡问题。", "result": "在多个基准数据集上的大量实验证明了该方法的出色性能和泛化能力。", "conclusion": "所提出的CEM-FBGTinyDet（包含CEM、FBSM和DCLoss的E-FPN-BS）通过增强特征利用和平衡梯度，有效解决了微小目标检测的挑战，从而实现了卓越的性能和泛化能力。", "translation": "微小目标检测（TOD）揭示了特征金字塔网络的一个根本缺陷：在高标准标签分配协议下，高层特征（P5-P6）经常接收到零个正锚点，导致它们的语义表示因被排除在损失计算之外而未被训练。这产生了双重缺陷：（1）滞留的高层特征在没有梯度更新的情况下成为语义死胡同，而（2）低层特征缺乏鲁棒的分类所需的基本语义上下文。我们提出了E-FPN-BS，它系统地将浪费的高层语义转换为低层特征增强。为了解决这些问题，我们提出了E-FPN-BS，这是一种集成了多尺度特征增强和自适应优化的新颖架构。首先，我们的上下文增强模块（CEM）采用双分支处理来对齐和压缩高层特征，以实现有效的全局-局部融合。其次，前景-背景分离模块（FBSM）生成空间门控掩码，动态放大判别区域。为了解决目标尺度上的梯度不平衡问题，我们进一步提出了一种动态梯度平衡损失（DCLoss），它通过尺度感知梯度均衡自动调节损失贡献。在多个基准数据集上的大量实验证明了我们方法的出色性能和泛化能力。", "summary": "该论文指出了特征金字塔网络在微小目标检测中存在的一个关键缺陷，即高层特征未被充分利用且低层特征缺乏上下文。为解决此问题，他们提出了CEM-FBGTinyDet（E-FPN-BS），这是一种新颖的架构，包含用于融合高层语义的上下文增强模块（CEM）、用于区域放大的前景-背景分离模块（FBSM）以及用于缓解梯度不平衡的动态梯度平衡损失（DCLoss）。该方法显著提高了微小目标检测的性能和泛化能力。", "keywords": "微小目标检测, 特征金字塔网络, 上下文增强, 梯度平衡, 多尺度特征", "comments": "该论文创新性地解决了FPNs在微小目标检测中的核心局限性，通过系统地将未充分利用的高层特征转换为有用的低层增强。CEM、FBSM和DCLoss的引入为特征增强、前景-背景分离和梯度平衡提供了全面的解决方案，这些对于检测微小目标至关重要。对跨尺度梯度不平衡的关注对于TOD尤为重要。"}}
{"id": "2506.09824", "title": "Weighted Loss Methods for Robust Federated Learning under Data Heterogeneity", "authors": ["Johan Erbani", "Sonia Ben Mokhtar", "Pierre-Edouard Portier", "Elod Egyed-Zsigmond", "Diana Nurbakova"], "summary": "Federated learning (FL) is a machine learning paradigm that enables multiple\ndata holders to collaboratively train a machine learning model without sharing\ntheir training data with external parties. In this paradigm, workers locally\nupdate a model and share with a central server their updated gradients (or\nmodel parameters). While FL seems appealing from a privacy perspective, it\nopens a number of threats from a security perspective as (Byzantine)\nparticipants can contribute poisonous gradients (or model parameters) harming\nmodel convergence. Byzantine-resilient FL addresses this issue by ensuring that\nthe training proceeds as if Byzantine participants were absent. Towards this\npurpose, common strategies ignore outlier gradients during model aggregation,\nassuming that Byzantine gradients deviate more from honest gradients than\nhonest gradients do from each other. However, in heterogeneous settings, honest\ngradients may differ significantly, making it difficult to distinguish honest\noutliers from Byzantine ones. In this paper, we introduce the Worker Label\nAlignement Loss (WoLA), a weighted loss that aligns honest worker gradients\ndespite data heterogeneity, which facilitates the identification of Byzantines'\ngradients. This approach significantly outperforms state-of-the-art methods in\nheterogeneous settings. In this paper, we provide both theoretical insights and\nempirical evidence of its effectiveness.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09824v1", "AI": {"title_translation": "异构数据下鲁棒联邦学习的加权损失方法", "tldr": "本文提出WoLA加权损失方法，以在数据异构的联邦学习中更好地识别拜占庭攻击，显著优于现有方法。", "motivation": "联邦学习在数据异构环境下，现有拜占庭鲁棒方法难以区分诚实异常梯度和拜占庭梯度，导致模型收敛受损，因此需要一种新的方法来解决此问题。", "method": "引入“Worker Label Alignement Loss (WoLA)”（工人标签对齐损失），这是一种加权损失方法，旨在数据异构性下对齐诚实工作者的梯度，从而促进拜占庭梯度的识别。", "result": "WoLA方法在异构设置中显著优于最先进的方法，并提供了理论见解和经验证据，证明了其有效性。", "conclusion": "通过引入WoLA加权损失，本研究成功解决了联邦学习中数据异构性下拜占庭攻击的识别难题，提升了鲁棒性。", "translation": "联邦学习（FL）是一种机器学习范式，允许多个数据持有者在不与外部方共享训练数据的情况下协同训练机器学习模型。在这种范式中，工作者在本地更新模型并与中央服务器共享其更新的梯度（或模型参数）。虽然FL从隐私角度看很有吸引力，但从安全角度看，它也带来了一些威胁，因为（拜占庭）参与者可能贡献有毒梯度（或模型参数），从而损害模型收敛。拜占庭鲁棒FL通过确保训练进行，如同拜占庭参与者不存在一样来解决这个问题。为此，常见策略在模型聚合期间忽略异常梯度，假设拜占庭梯度与诚实梯度的偏差大于诚实梯度彼此之间的偏差。然而，在异构设置中，诚实梯度可能显著不同，这使得区分诚实异常值和拜占庭异常值变得困难。在本文中，我们引入了工人标签对齐损失（WoLA），这是一种加权损失，可以在数据异构性下对齐诚实工作者的梯度，从而有助于识别拜占庭梯度。这种方法在异构设置中显著优于最先进的方法。在本文中，我们提供了其有效性的理论见解和经验证据。", "summary": "本文提出一种名为Worker Label Alignement Loss (WoLA) 的加权损失方法，旨在解决联邦学习在数据异构环境下，区分诚实梯度和拜占庭梯度的挑战。WoLA通过对齐诚实工作者的梯度来促进拜占庭梯度的识别，并在异构设置中展现出超越现有方法的显著性能提升，其有效性得到了理论和经验证据的支持。", "keywords": "联邦学习, 数据异构性, 拜占庭鲁棒性, 加权损失, WoLA", "comments": "这篇论文的创新点在于提出了WoLA加权损失，解决了联邦学习中数据异构性对拜占庭攻击识别的挑战。在实际应用中，数据异构性是联邦学习面临的普遍问题，因此该方法对于提升联邦学习的鲁棒性具有重要意义。"}}
{"id": "2506.09916", "title": "Only-Style: Stylistic Consistency in Image Generation without Content Leakage", "authors": ["Tilemachos Aravanis", "Panagiotis Filntisis", "Petros Maragos", "George Retsinas"], "summary": "Generating images in a consistent reference visual style remains a\nchallenging computer vision task. State-of-the-art methods aiming for\nstyle-consistent generation struggle to effectively separate semantic content\nfrom stylistic elements, leading to content leakage from the image provided as\na reference to the targets. To address this challenge, we propose Only-Style: a\nmethod designed to mitigate content leakage in a semantically coherent manner\nwhile preserving stylistic consistency. Only-Style works by localizing content\nleakage during inference, allowing the adaptive tuning of a parameter that\ncontrols the style alignment process, specifically within the image patches\ncontaining the subject in the reference image. This adaptive process best\nbalances stylistic consistency with leakage elimination. Moreover, the\nlocalization of content leakage can function as a standalone component, given a\nreference-target image pair, allowing the adaptive tuning of any\nmethod-specific parameter that provides control over the impact of the\nstylistic reference. In addition, we propose a novel evaluation framework to\nquantify the success of style-consistent generations in avoiding undesired\ncontent leakage. Our approach demonstrates a significant improvement over\nstate-of-the-art methods through extensive evaluation across diverse instances,\nconsistently achieving robust stylistic consistency without undesired content\nleakage.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09916v1", "AI": {"title_translation": "Only-Style：图像生成中无内容泄露的风格一致性", "tldr": "Only-Style是一种新的图像生成方法，通过在推理过程中定位内容泄露并自适应调整风格对齐参数，从而在保持风格一致性的同时有效避免内容泄露。", "motivation": "在图像生成中保持一致的参考视觉风格是一项具有挑战性的计算机视觉任务。现有的最先进方法在有效分离语义内容和风格元素方面存在困难，导致参考图像的内容泄露到目标图像中。", "method": "我们提出了Only-Style方法，旨在以语义连贯的方式减轻内容泄露，同时保持风格一致性。Only-Style通过在推理过程中定位内容泄露，允许自适应调整控制风格对齐过程的参数，特别是在包含参考图像主题的图像块内。这种自适应过程在风格一致性和泄露消除之间取得了最佳平衡。此外，内容泄露的定位可以作为一个独立的组件，给定参考-目标图像对，允许自适应调整任何提供对风格参考影响控制的方法特定参数。我们还提出了一种新颖的评估框架来量化风格一致性生成在避免不期望内容泄露方面的成功。", "result": "我们的方法在广泛的实例评估中显示出比最先进方法显著的改进，始终在没有不期望内容泄露的情况下实现鲁棒的风格一致性。", "conclusion": "Only-Style显著改善了风格一致的图像生成，有效防止了内容泄露，并在保持风格一致性的同时实现了优于现有方法的性能。", "translation": "在一致的参考视觉风格中生成图像仍然是一项具有挑战性的计算机视觉任务。旨在实现风格一致性生成的最先进方法难以有效分离语义内容和风格元素，导致内容从作为参考提供的图像泄露到目标图像。为了解决这一挑战，我们提出了Only-Style：一种旨在以语义连贯的方式减轻内容泄露，同时保持风格一致性的方法。Only-Style通过在推理过程中定位内容泄露来工作，允许自适应调整控制风格对齐过程的参数，特别是在包含参考图像主题的图像块内。这种自适应过程在风格一致性和泄露消除之间取得了最佳平衡。此外，内容泄露的定位可以作为一个独立的组件，给定参考-目标图像对，允许自适应调整任何提供对风格参考影响控制的方法特定参数。此外，我们提出了一种新颖的评估框架来量化风格一致性生成在避免不期望内容泄露方面的成功。我们的方法通过对不同实例的广泛评估，证明了比最先进方法显著的改进，始终在没有不期望内容泄露的情况下实现鲁棒的风格一致性。", "summary": "Only-Style是一种旨在解决图像生成中风格一致性与内容泄露之间矛盾的新方法。现有方法在分离风格和内容时遇到困难，导致参考图像的内容不慎出现在生成图像中。Only-Style通过在推理时定位内容泄露，并自适应调整风格对齐参数来解决此问题，尤其是在包含主题的图像区域。这种方法在保持风格一致性的同时有效消除了内容泄露。该研究还提出了一个量化内容泄露的新评估框架，并通过实验证明Only-Style在性能上显著优于现有技术，实现了鲁棒的风格一致性而无内容泄露。", "keywords": "图像生成, 风格一致性, 内容泄露, Only-Style, 自适应调整", "comments": "该论文的创新点在于提出了Only-Style方法，通过在推理阶段定位并自适应调整参数来解决图像风格迁移中的内容泄露问题。其提出的内容泄露定位作为独立组件的设想，以及新颖的评估框架，都显示了该方法在理论和实践上的重要性。它为实现高质量、无内容污染的风格化图像生成提供了新的思路和有效工具。"}}
{"id": "2506.09440", "title": "GigaChat Family: Efficient Russian Language Modeling Through Mixture of Experts Architecture", "authors": ["GigaChat team", "Mamedov Valentin", "Evgenii Kosarev", "Gregory Leleytner", "Ilya Shchuckin", "Valeriy Berezovskiy", "Daniil Smirnov", "Dmitry Kozlov", "Sergei Averkiev", "Lukyanenko Ivan", "Aleksandr Proshunin", "Ainur Israfilova", "Ivan Baskov", "Artem Chervyakov", "Emil Shakirov", "Mikhail Kolesov", "Daria Khomich", "Darya Latortseva", "Sergei Porkhun", "Yury Fedorov", "Oleg Kutuzov", "Polina Kudriavtseva", "Sofiia Soldatova", "Kolodin Egor", "Stanislav Pyatkin", "Dzmitry Menshykh", "Grafov Sergei", "Eldar Damirov", "Karlov Vladimir", "Ruslan Gaitukiev", "Arkadiy Shatenov", "Alena Fenogenova", "Nikita Savushkin", "Fedor Minkin"], "summary": "Generative large language models (LLMs) have become crucial for modern NLP\nresearch and applications across various languages. However, the development of\nfoundational models specifically tailored to the Russian language has been\nlimited, primarily due to the significant computational resources required.\nThis paper introduces the GigaChat family of Russian LLMs, available in various\nsizes, including base models and instruction-tuned versions. We provide a\ndetailed report on the model architecture, pre-training process, and\nexperiments to guide design choices. In addition, we evaluate their performance\non Russian and English benchmarks and compare GigaChat with multilingual\nanalogs. The paper presents a system demonstration of the top-performing models\naccessible via an API, a Telegram bot, and a Web interface. Furthermore, we\nhave released three open GigaChat models in open-source\n(https://huggingface.co/ai-sage), aiming to expand NLP research opportunities\nand support the development of industrial solutions for the Russian language.", "comment": "ACL-2025 System Demo", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.09440v1", "AI": {"title_translation": "GigaChat系列：通过专家混合架构实现高效俄语语言建模", "tldr": "介绍了GigaChat系列俄语大语言模型，解决了俄语LLM开发受限的问题，提供了模型细节、性能评估和开放模型，以促进俄语NLP研究和应用。", "motivation": "现有的生成式大型语言模型（LLMs）在各种语言的NLP研究和应用中至关重要，但专门针对俄语的基础模型开发一直有限，这主要归因于所需的巨大计算资源。", "method": "论文引入了GigaChat系列俄语LLMs，涵盖不同规模的基础模型和指令微调版本。详细报告了模型架构、预训练过程和实验，以指导设计选择。此外，还在俄语和英语基准上评估了它们的性能，并与多语言同类模型进行了比较。", "result": "论文展示了顶级性能模型的系统演示，可通过API、Telegram机器人和Web界面访问。此外，还开源发布了三个GigaChat模型。", "conclusion": "GigaChat系列模型通过解决俄语LLM开发中计算资源受限的问题，为俄语NLP研究和工业解决方案的发展提供了新的机会和支持。", "translation": "生成式大型语言模型（LLMs）已成为现代自然语言处理（NLP）研究和各种语言应用的关键。然而，专门针对俄语的基础模型开发一直有限，这主要归因于所需的巨大计算资源。本文介绍了GigaChat系列俄语LLM，提供各种规模的模型，包括基础模型和指令微调版本。我们提供了关于模型架构、预训练过程和实验的详细报告，以指导设计选择。此外，我们评估了它们在俄语和英语基准上的性能，并将GigaChat与多语言同类模型进行了比较。论文展示了通过API、Telegram机器人和Web界面可访问的顶级性能模型的系统演示。此外，我们已开源发布了三个GigaChat模型（https://huggingface.co/ai-sage），旨在扩大NLP研究机会并支持俄语工业解决方案的开发。", "summary": "本文介绍了GigaChat系列俄语大型语言模型，旨在解决俄语LLM开发中计算资源受限的问题。研究详细阐述了模型架构、预训练和实验，并在俄语和英语基准上评估了GigaChat的性能，与多语言模型进行了比较。论文还展示了顶级模型的系统演示，并开源发布了部分模型，以促进俄语NLP研究和工业应用。", "keywords": "俄语LLM, GigaChat, 专家混合架构, 语言建模, 自然语言处理", "comments": "这篇论文通过引入专门针对俄语的GigaChat系列LLM，填补了俄语LLM领域的一个重要空白，解决了计算资源限制的挑战。其创新之处在于提供了详细的模型架构和预训练过程，并进行了全面的性能评估。开放模型的发布对于推动俄语NLP研究和工业应用具有重要意义，尤其是在多语言LLM日益普及的背景下，专注于特定语言的基础模型对于该语言的深度发展至关重要。"}}
{"id": "2506.09862", "title": "Guided Graph Compression for Quantum Graph Neural Networks", "authors": ["Mikel Casals", "Vasilis Belis", "Elias F. Combarro", "Eduard Alarcón", "Sofia Vallecorsa", "Michele Grossi"], "summary": "Graph Neural Networks (GNNs) are effective for processing graph-structured\ndata but face challenges with large graphs due to high memory requirements and\ninefficient sparse matrix operations on GPUs. Quantum Computing (QC) offers a\npromising avenue to address these issues and inspires new algorithmic\napproaches. In particular, Quantum Graph Neural Networks (QGNNs) have been\nexplored in recent literature. However, current quantum hardware limits the\ndimension of the data that can be effectively encoded. Existing approaches\neither simplify datasets manually or use artificial graph datasets. This work\nintroduces the Guided Graph Compression (GGC) framework, which uses a graph\nautoencoder to reduce both the number of nodes and the dimensionality of node\nfeatures. The compression is guided to enhance the performance of a downstream\nclassification task, which can be applied either with a quantum or a classical\nclassifier. The framework is evaluated on the Jet Tagging task, a\nclassification problem of fundamental importance in high energy physics that\ninvolves distinguishing particle jets initiated by quarks from those by gluons.\nThe GGC is compared against using the autoencoder as a standalone preprocessing\nstep and against a baseline classical GNN classifier. Our numerical results\ndemonstrate that GGC outperforms both alternatives, while also facilitating the\ntesting of novel QGNN ansatzes on realistic datasets.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09862v1", "AI": {"title_translation": "量子图神经网络的引导图压缩", "tldr": "本文提出了一种名为引导图压缩（GGC）的框架，通过图自编码器压缩图数据，以解决图神经网络（GNNs）处理大型图时遇到的内存和效率问题，以及量子图神经网络（QGNNs）受限于当前量子硬件数据维度的挑战。GGC在下游分类任务中表现优于现有方法，并有助于在真实数据集上测试QGNNs。", "motivation": "图神经网络（GNNs）在处理大型图时面临高内存需求和稀疏矩阵操作效率低下的挑战。量子图神经网络（QGNNs）虽然有潜力，但受限于当前量子硬件对数据维度的有效编码能力，导致现有方法依赖手动简化或人工数据集。", "method": "本文引入了引导图压缩（GGC）框架。该框架使用一个图自编码器来减少节点数量和节点特征的维度。压缩过程受下游分类任务性能的引导，可应用于量子或经典分类器。", "result": "数值结果表明，GGC在射流标记任务中优于单独使用自编码器作为预处理步骤以及基线经典GNN分类器。它还促进了在真实数据集上测试新颖的QGNN ansatzes。", "conclusion": "引导图压缩（GGC）框架有效地解决了大型图数据在GNNs中面临的挑战以及QGNNs受限于量子硬件维度的问题。通过引导压缩，GGC在分类任务中表现出优越性，并为在现实数据集上探索QGNNs提供了途径。", "translation": "图神经网络（GNNs）在处理图结构数据方面是有效的，但由于高内存需求和GPU上低效的稀疏矩阵操作，它们在处理大型图时面临挑战。量子计算（QC）为解决这些问题提供了一个有前景的途径，并启发了新的算法方法。特别是，量子图神经网络（QGNNs）在最近的文献中得到了探索。然而，当前的量子硬件限制了可以有效编码的数据维度。现有方法要么手动简化数据集，要么使用人工图数据集。这项工作引入了引导图压缩（GGC）框架，该框架使用图自编码器来减少节点数量和节点特征的维度。压缩过程受到引导，以增强下游分类任务的性能，该任务可以应用于量子或经典分类器。该框架在射流标记任务上进行了评估，这是一个高能物理中至关重要的分类问题，涉及区分由夸克和胶子引起的粒子射流。GGC与将自编码器作为独立预处理步骤的方法以及基线经典GNN分类器进行了比较。我们的数值结果表明，GGC优于这两种替代方法，同时也有助于在真实数据集上测试新颖的QGNN ansatzes。", "summary": "本文提出了引导图压缩（GGC）框架，旨在解决图神经网络（GNNs）处理大型图时的效率问题以及量子图神经网络（QGNNs）在当前量子硬件上受到的数据维度限制。GGC利用图自编码器对图数据进行节点数量和特征维度的压缩，并通过下游分类任务的性能进行引导优化。实验结果表明，GGC在射流标记任务上优于单独的自编码器预处理和经典的GNN分类器，并为在真实数据集上应用QGNNs提供了有效手段。", "keywords": "图压缩, 量子图神经网络, 图神经网络, 图自编码器, 量子计算", "comments": "这项工作的创新之处在于提出了“引导”的图压缩方法，使得压缩过程能够优化下游任务性能，而非仅仅是无差别的降维。这对于克服当前量子硬件的限制，使得QGNNs能够在更实际和复杂的数据集上进行探索具有重要意义。它有效地连接了图神经网络的挑战与量子计算的潜力。"}}
{"id": "2506.09919", "title": "MetricHMR: Metric Human Mesh Recovery from Monocular Images", "authors": ["He Zhang", "Chentao Song", "Hongwen Zhang", "Tao Yu"], "summary": "We introduce MetricHMR (Metric Human Mesh Recovery), an approach for metric\nhuman mesh recovery with accurate global translation from monocular images. In\ncontrast to existing HMR methods that suffer from severe scale and depth\nambiguity, MetricHMR is able to produce geometrically reasonable body shape and\nglobal translation in the reconstruction results. To this end, we first\nsystematically analyze previous HMR methods on camera models to emphasize the\ncritical role of the standard perspective projection model in enabling\nmetric-scale HMR. We then validate the acceptable ambiguity range of metric HMR\nunder the standard perspective projection model. Finally, we contribute a novel\napproach that introduces a ray map based on the standard perspective projection\nto jointly encode bounding-box information, camera parameters, and geometric\ncues for End2End metric HMR without any additional metric-regularization\nmodules. Extensive experiments demonstrate that our method achieves\nstate-of-the-art performance, even compared with sequential HMR methods, in\nmetric pose, shape, and global translation estimation across both indoor and\nin-the-wild scenarios.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09919v1", "AI": {"title_translation": "MetricHMR：从单目图像中恢复度量级人体网格", "tldr": "MetricHMR通过引入基于标准透视投影的射线图，解决了单目图像人体网格恢复中存在的尺度和深度模糊问题，实现了准确的度量级姿态、形状和全局平移估计。", "motivation": "现有的人体网格恢复（HMR）方法存在严重的尺度和深度模糊问题，导致从单目图像重建的人体形状和全局平移不准确。", "method": "MetricHMR首先系统分析了以往HMR方法在相机模型上的问题，强调标准透视投影模型在实现度量级HMR中的关键作用。接着，验证了在该模型下度量级HMR可接受的模糊范围。最后，提出了一种新颖的方法，引入基于标准透视投影的射线图，以联合编码边界框信息、相机参数和几何线索，实现端到端度量级HMR，无需额外的度量正则化模块。", "result": "MetricHMR在度量姿态、形状和全局平移估计方面达到了最先进的性能，甚至优于序列HMR方法，在室内和野外场景中均表现出色。", "conclusion": "MetricHMR成功解决了单目图像人体网格恢复中的尺度和深度模糊问题，能够从单目图像中准确地恢复度量级人体网格，并在广泛的实验中展示了卓越的性能。", "translation": "我们引入了MetricHMR（度量级人体网格恢复），这是一种从单目图像中准确恢复具有度量级全局平移的人体网格的方法。与现有HMR方法遭受严重尺度和深度模糊不同，MetricHMR能够在重建结果中产生几何合理的身体形状和全局平移。为此，我们首先系统地分析了以往HMR方法在相机模型上的问题，以强调标准透视投影模型在实现度量级HMR中的关键作用。然后，我们验证了在标准透视投影模型下度量级HMR可接受的模糊范围。最后，我们提出了一种新颖的方法，引入了基于标准透视投影的射线图，以共同编码边界框信息、相机参数和几何线索，从而实现无需任何额外度量正则化模块的端到端度量级HMR。大量的实验表明，我们的方法在度量姿态、形状和全局平移估计方面取得了最先进的性能，甚至与序列HMR方法相比，在室内和野外场景中均表现出色。", "summary": "MetricHMR是一种创新的单目图像人体网格恢复方法，通过引入基于标准透视投影的射线图，有效解决了传统HMR方法中存在的尺度和深度模糊问题。该方法能够准确估计人体形状和全局平移，无需额外的度量正则化模块，并在多项实验中展现出超越现有最先进方法的性能。", "keywords": "人体网格恢复, 单目图像, 度量级恢复, 尺度模糊, 射线图", "comments": "该论文通过深入分析相机模型和引入射线图，巧妙地解决了单目HMR中的核心挑战——尺度和深度模糊。其端到端的设计和无需额外正则化的特点，使其在实用性和性能上具有显著优势。在单目3D人体重建领域具有重要意义。"}}
{"id": "2506.09867", "title": "Machine Learning-Based Classification of Oils Using Dielectric Properties and Microwave Resonant Sensing", "authors": ["Amit Baran Dey", "Wasim Arif", "Rakhesh Singh Kshetrimayum"], "summary": "This paper proposes a machine learning-based methodology for the\nclassification of various oil samples based on their dielectric properties,\nutilizing a microwave resonant sensor. The dielectric behaviour of oils,\ngoverned by their molecular composition, induces distinct shifts in the\nsensor's resonant frequency and amplitude response. These variations are\nsystematically captured and processed to extract salient features, which serve\nas inputs for multiple machine learning classifiers. The microwave resonant\nsensor operates in a non-destructive, low-power manner, making it particularly\nwell-suited for real-time industrial applications. A comprehensive dataset is\ndeveloped by varying the permittivity of oil samples and acquiring the\ncorresponding sensor responses. Several classifiers are trained and evaluated\nusing the extracted resonant features to assess their capability in\ndistinguishing between oil types. Experimental results demonstrate that the\nproposed approach achieves a high classification accuracy of 99.41% with the\nrandom forest classifier, highlighting its strong potential for automated oil\nidentification. The system's compact form factor, efficiency, and high\nperformance underscore its viability for fast and reliable oil characterization\nin industrial environments.", "comment": "6 pages, 11 figures, Accepted to IEEE INDISCON 2025", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09867v1", "AI": {"title_translation": "基于机器学习的油品分类方法，利用介电特性和微波谐振传感", "tldr": "本文提出一种基于机器学习的微波谐振传感方法，通过测量油品介电特性实现高精度（99.41%）的油品分类，适用于工业实时应用。", "motivation": "旨在开发一种非破坏性、低功耗、实时且高精度的自动化油品识别系统，以克服现有油品识别方法的局限性。", "method": "论文提出一种基于机器学习的油品分类方法。利用微波谐振传感器捕获不同油品介电特性引起的谐振频率和幅度响应变化，提取显著特征。构建数据集，并使用多种机器学习分类器（如随机森林）进行训练和评估，以区分油品类型。", "result": "实验结果表明，该方法使用随机森林分类器实现了99.41%的高分类精度。该系统具有紧凑、高效和高性能的特点。", "conclusion": "该基于机器学习和微波谐振传感的油品分类方法，通过利用油品的介电特性，实现了高精度的自动化油品识别。其非破坏性、低功耗、紧凑性和高效率使其在工业环境中进行快速可靠的油品表征具有巨大潜力。", "translation": "本文提出了一种基于机器学习的方法，利用微波谐振传感器根据油样的介电特性对其进行分类。油品的介电行为受其分子组成影响，会引起传感器谐振频率和幅度响应的明显偏移。这些变化被系统地捕获和处理，以提取显著特征，作为多个机器学习分类器的输入。微波谐振传感器以非破坏性、低功耗方式运行，使其特别适用于实时工业应用。通过改变油样的介电常数并获取相应的传感器响应，建立了一个综合数据集。使用提取的谐振特征对多个分类器进行训练和评估，以评估它们区分油品类型的能力。实验结果表明，所提出的方法使用随机森林分类器实现了99.41%的高分类精度，突显了其在自动化油品识别方面的强大潜力。该系统紧凑的尺寸、效率和高性能突出了其在工业环境中进行快速可靠油品表征的可行性。", "summary": "本文提出了一种创新的基于机器学习的油品分类方法，该方法利用微波谐振传感器捕捉不同油品的介电特性差异。通过分析传感器谐振频率和幅度的变化，提取特征并输入到机器学习分类器中。实验证明，该方法使用随机森林分类器在油品分类上实现了99.41%的卓越准确率，展现了其在工业环境中进行快速、非破坏性、高精度自动化油品识别的巨大应用前景。", "keywords": "油品分类, 机器学习, 介电特性, 微波谐振传感, 随机森林", "comments": "这篇论文通过结合微波谐振传感技术和机器学习，为油品分类提供了一种高效、非破坏性的解决方案。其创新点在于将油品的介电特性变化转化为机器学习模型的输入，并取得了极高的分类精度。该方法在工业实时监测和质量控制方面具有重要的应用价值，尤其是在需要快速可靠识别多种油品的情境下。"}}
{"id": "2506.09920", "title": "Structural-Spectral Graph Convolution with Evidential Edge Learning for Hyperspectral Image Clustering", "authors": ["Jianhan Qi", "Yuheng Jia", "Hui Liu", "Junhui Hou"], "summary": "Hyperspectral image (HSI) clustering assigns similar pixels to the same class\nwithout any annotations, which is an important yet challenging task. For\nlarge-scale HSIs, most methods rely on superpixel segmentation and perform\nsuperpixel-level clustering based on graph neural networks (GNNs). However,\nexisting GNNs cannot fully exploit the spectral information of the input HSI,\nand the inaccurate superpixel topological graph may lead to the confusion of\ndifferent class semantics during information aggregation. To address these\nchallenges, we first propose a structural-spectral graph convolutional operator\n(SSGCO) tailored for graph-structured HSI superpixels to improve their\nrepresentation quality through the co-extraction of spatial and spectral\nfeatures. Second, we propose an evidence-guided adaptive edge learning (EGAEL)\nmodule that adaptively predicts and refines edge weights in the superpixel\ntopological graph. We integrate the proposed method into a contrastive learning\nframework to achieve clustering, where representation learning and clustering\nare simultaneously conducted. Experiments demonstrate that the proposed method\nimproves clustering accuracy by 2.61%, 6.06%, 4.96% and 3.15% over the best\ncompared methods on four HSI datasets. Our code is available at\nhttps://github.com/jhqi/SSGCO-EGAEL.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09920v1", "AI": {"title_translation": "结构-光谱图卷积与证据边缘学习用于高光谱图像聚类", "tldr": "该论文提出了一种新的高光谱图像聚类方法，通过结合结构-光谱图卷积操作符和证据引导的自适应边缘学习模块，解决了现有图神经网络在高光谱图像聚类中无法充分利用光谱信息和超像素拓扑图不准确的问题，并在四个数据集上显著提高了聚类精度。", "motivation": "现有的图神经网络（GNNs）在高光谱图像（HSI）聚类中无法充分利用输入HSI的光谱信息，并且不准确的超像素拓扑图可能导致信息聚合过程中不同类别语义的混淆。", "method": "1. 提出了一种针对图结构HSI超像素的结构-光谱图卷积操作符（SSGCO），通过空间和光谱特征的协同提取来提高表示质量。2. 提出了一种证据引导的自适应边缘学习（EGAEL）模块，自适应地预测和优化超像素拓扑图中的边缘权重。3. 将该方法整合到对比学习框架中，同时进行表示学习和聚类。", "result": "该方法在四个HSI数据集上，相较于最佳对比方法，聚类精度分别提高了2.61%、6.06%、4.96%和3.15%。", "conclusion": "结合结构-光谱图卷积和证据边缘学习的对比学习框架能有效提高高光谱图像聚类精度，克服现有GNN方法的局限性。", "translation": "高光谱图像（HSI）聚类在没有任何标注的情况下将相似像素分配到同一类别，这是一项重要但具有挑战性的任务。对于大规模HSI，大多数方法依赖于超像素分割，并基于图神经网络（GNNs）执行超像素级聚类。然而，现有的GNN无法充分利用输入HSI的光谱信息，并且不准确的超像素拓扑图可能导致信息聚合过程中不同类别语义的混淆。为了解决这些挑战，我们首先提出了一种结构-光谱图卷积操作符（SSGCO），专为图结构HSI超像素设计，通过空间和光谱特征的协同提取来提高其表示质量。其次，我们提出了一种证据引导的自适应边缘学习（EGAEL）模块，该模块自适应地预测和优化超像素拓扑图中的边缘权重。我们将所提出的方法整合到对比学习框架中以实现聚类，其中表示学习和聚类同时进行。实验表明，所提出的方法在四个HSI数据集上，相较于最佳对比方法，聚类精度分别提高了2.61%、6.06%、4.96%和3.15%。我们的代码可在https://github.com/jhqi/SSGCO-EGAEL获取。", "summary": "本文提出了一种新的高光谱图像（HSI）聚类方法，旨在解决现有图神经网络（GNNs）在处理HSI时光谱信息利用不足和超像素拓扑图不准确的问题。该方法引入了结构-光谱图卷积操作符（SSGCO）以协同提取空间和光谱特征，并设计了证据引导的自适应边缘学习（EGAEL）模块来优化图边缘权重。通过将这些组件集成到对比学习框架中，实现了表示学习与聚类的同步进行。实验结果表明，该方法在多个HSI数据集上显著提升了聚类精度。", "keywords": "高光谱图像聚类, 图卷积网络, 证据边缘学习, 对比学习, 超像素", "comments": "这篇论文通过引入结构-光谱图卷积和证据边缘学习，有效地解决了高光谱图像聚类中光谱信息利用不足和图结构不准确的关键挑战。其创新性在于将这两种机制与对比学习相结合，实现了更鲁棒和精确的特征表示及聚类。该方法在实际应用中，尤其是在需要无监督分类的场景下，具有重要的参考价值。"}}
{"id": "2506.09450", "title": "UniToMBench: Integrating Perspective-Taking to Improve Theory of Mind in LLMs", "authors": ["Prameshwar Thiyagarajan", "Vaishnavi Parimi", "Shamant Sai", "Soumil Garg", "Zhangir Meirbek", "Nitin Yarlagadda", "Kevin Zhu", "Chris Kim"], "summary": "Theory of Mind (ToM), the ability to understand the mental states of oneself\nand others, remains a challenging area for large language models (LLMs), which\noften fail to predict human mental states accurately. In this paper, we\nintroduce UniToMBench, a unified benchmark that integrates the strengths of\nSimToM and TOMBENCH to systematically improve and assess ToM capabilities in\nLLMs by integrating multi-interaction task designs and evolving story\nscenarios. Supported by a custom dataset of over 1,000 hand-written scenarios,\nUniToMBench combines perspective-taking techniques with diverse evaluation\nmetrics to better stimulate social cognition in LLMs. Through evaluation, we\nobserve that while models like GPT-4o and GPT-4o Mini show consistently high\naccuracy in tasks involving emotional and belief-related scenarios, with\nresults usually above 80%, there is significant variability in their\nperformance across knowledge-based tasks. These results highlight both the\nstrengths and limitations of current LLMs in ToM-related tasks, underscoring\nthe value of UniToMBench as a comprehensive tool for future development. Our\ncode is publicly available here:\nhttps://github.com/Shamant/unifiedtombenchmark.", "comment": "Accepted at Conference of the North American Chapter of the\n  Association for Computational Linguistics, Student Research Workshop 2025\n  (NAACL SRW 2025)", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.09450v1", "AI": {"title_translation": "UniToMBench：整合视角采择以提升大型语言模型的心理理论能力", "tldr": "UniToMBench是一个统一的基准测试，通过整合多交互任务设计和演进的故事场景，并结合视角采择技术和多样化的评估指标，旨在系统地改进和评估大型语言模型（LLMs）的心理理论（ToM）能力。", "motivation": "大型语言模型（LLMs）在理解自身和他人心理状态的心理理论（ToM）方面仍面临挑战，常常无法准确预测人类的心理状态。", "method": "本文介绍了UniToMBench，一个统一的基准测试，它整合了SimToM和TOMBENCH的优势。该基准通过整合多交互任务设计和演进的故事场景来系统地改进和评估LLMs的ToM能力。它由超过1000个手写场景的自定义数据集支持，并结合视角采择技术和多样化的评估指标，以更好地激发LLMs的社会认知。", "result": "通过评估，研究发现GPT-4o和GPT-4o Mini等模型在涉及情感和信念相关场景的任务中表现出持续高准确性（通常高于80%），但在基于知识的任务中表现出显著的性能差异。", "conclusion": "这些结果突出了当前LLMs在ToM相关任务中的优势和局限性，并强调了UniToMBench作为未来开发综合工具的价值。", "translation": "心理理论（ToM），即理解自身和他人心理状态的能力，对于大型语言模型（LLMs）来说仍然是一个具有挑战性的领域，它们常常无法准确预测人类的心理状态。在本文中，我们介绍了UniToMBench，一个统一的基准测试，它整合了SimToM和TOMBENCH的优势，通过整合多交互任务设计和演进的故事场景，系统地改进和评估LLMs的ToM能力。在超过1000个手写场景的自定义数据集支持下，UniToMBench结合了视角采择技术和多样化的评估指标，以更好地刺激LLMs的社会认知。通过评估，我们观察到，虽然像GPT-4o和GPT-4o Mini这样的模型在涉及情感和信念相关场景的任务中表现出持续高准确性，结果通常高于80%，但它们在基于知识的任务中的表现存在显著差异。这些结果突出了当前LLMs在ToM相关任务中的优势和局限性，强调了UniToMBench作为未来开发综合工具的价值。我们的代码已在此处公开：https://github.com/Shamant/unifiedtombenchmark。", "summary": "本文介绍了UniToMBench，一个旨在提升和评估大型语言模型（LLMs）心理理论（ToM）能力的统一基准测试。该基准整合了多交互任务设计、演进的故事场景和视角采择技术，并利用一个包含1000多个手写场景的自定义数据集。评估结果显示，GPT-4o和GPT-4o Mini在情感和信念任务上表现出色，但在知识型任务上存在显著差异，突显了当前LLMs在ToM方面的优缺点，并强调了UniToMBench作为综合评估工具的重要性。", "keywords": "心理理论, 大型语言模型, 基准测试, 视角采择, 社会认知", "comments": "UniToMBench的创新之处在于其统一了现有基准的优势，并引入了多交互任务设计和演进故事场景，特别是结合了视角采择技术，这对于提升LLMs的社会认知能力至关重要。其自定义的1000多个手写场景数据集也为更细致的评估提供了基础。该研究揭示了当前LLMs在不同ToM子任务上的表现差异，为未来LLM在ToM领域的进一步发展提供了明确的方向和有价值的工具。"}}
{"id": "2506.09932", "title": "HadaNorm: Diffusion Transformer Quantization through Mean-Centered Transformations", "authors": ["Marco Federici", "Riccardo Del Chiaro", "Boris van Breugel", "Paul Whatmough", "Markus Nagel"], "summary": "Diffusion models represent the cutting edge in image generation, but their\nhigh memory and computational demands hinder deployment on resource-constrained\ndevices. Post-Training Quantization (PTQ) offers a promising solution by\nreducing the bitwidth of matrix operations. However, standard PTQ methods\nstruggle with outliers, and achieving higher compression often requires\ntransforming model weights and activations before quantization. In this work,\nwe propose HadaNorm, a novel linear transformation that extends existing\napproaches and effectively mitigates outliers by normalizing activations\nfeature channels before applying Hadamard transformations, enabling more\naggressive activation quantization. We demonstrate that HadaNorm consistently\nreduces quantization error across the various components of transformer blocks,\nachieving superior efficiency-performance trade-offs when compared to\nstate-of-the-art methods.", "comment": "4 Pages, 5 Figures", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09932v1", "AI": {"title_translation": "HadaNorm：通过均值中心化变换进行扩散Transformer量化", "tldr": "HadaNorm是一种新的线性变换，通过在量化前对激活特征通道进行归一化来缓解异常值问题，从而实现更激进的激活量化，提高扩散Transformer的效率和性能。", "motivation": "扩散模型在图像生成方面表现出色，但其高内存和计算需求阻碍了在资源受限设备上的部署。后训练量化（PTQ）是一种有前景的解决方案，但标准PTQ方法难以处理异常值，并且需要变换模型权重和激活才能实现更高压缩。", "method": "我们提出了HadaNorm，这是一种新颖的线性变换，它通过在应用Hadamard变换之前对激活特征通道进行归一化来有效缓解异常值问题，从而实现更激进的激活量化。", "result": "HadaNorm持续降低了Transformer块各个组件的量化误差，与最先进的方法相比，实现了卓越的效率-性能权衡。", "conclusion": "HadaNorm通过其创新的均值中心化变换，有效地解决了扩散模型量化中的异常值问题，显著提高了量化效率和性能。", "translation": "扩散模型代表了图像生成的尖端技术，但其高内存和计算需求阻碍了在资源受限设备上的部署。后训练量化（PTQ）通过降低矩阵运算的位宽提供了一种有前景的解决方案。然而，标准PTQ方法难以处理异常值，并且实现更高压缩通常需要在量化前对模型权重和激活进行变换。在这项工作中，我们提出了HadaNorm，这是一种新颖的线性变换，它扩展了现有方法，并通过在应用Hadamard变换之前对激活特征通道进行归一化来有效缓解异常值问题，从而实现更激进的激活量化。我们证明了HadaNorm持续降低了Transformer块各个组件的量化误差，与最先进的方法相比，实现了卓越的效率-性能权衡。", "summary": "本文提出了HadaNorm，这是一种用于扩散Transformer量化的新型线性变换方法。针对现有PTQ方法在处理异常值方面的不足，HadaNorm通过在应用Hadamard变换前对激活特征通道进行归一化来有效减轻异常值的影响，从而实现更激进的激活量化。实验结果表明，HadaNorm能持续降低量化误差，并在效率与性能之间取得优于现有方法的平衡，有助于在资源受限设备上部署扩散模型。", "keywords": "扩散模型, 量化, Transformer, PTQ, HadaNorm", "comments": "HadaNorm的创新之处在于其均值中心化变换，通过在量化前对激活特征通道进行归一化来解决PTQ中常见的异常值问题。这使其能够实现更激进的激活量化，从而在不显著牺牲性能的情况下大幅降低模型内存和计算需求，对于在边缘设备上部署大型扩散模型具有重要意义。"}}
{"id": "2506.09455", "title": "Abstraction-Based Proof Production in Formal Verification of Neural Networks", "authors": ["Yizhak Yisrael Elboher", "Omri Isac", "Guy Katz", "Tobias Ladner", "Haoze Wu"], "summary": "Modern verification tools for deep neural networks (DNNs) increasingly rely\non abstraction to scale to realistic architectures. In parallel, proof\nproduction is becoming a critical requirement for increasing the reliability of\nDNN verification results. However, current proofproducing verifiers do not\nsupport abstraction-based reasoning, creating a gap between scalability and\nprovable guarantees. We address this gap by introducing a novel framework for\nproof-producing abstraction-based DNN verification. Our approach modularly\nseparates the verification task into two components: (i) proving the\ncorrectness of an abstract network, and (ii) proving the soundness of the\nabstraction with respect to the original DNN. The former can be handled by\nexisting proof-producing verifiers, whereas we propose the first method for\ngenerating formal proofs for the latter. This preliminary work aims to enable\nscalable and trustworthy verification by supporting common abstraction\ntechniques within a formal proof framework.", "comment": "To appear in SAIV 2025", "cate": "cs.LO", "url": "http://arxiv.org/abs/2506.09455v1", "AI": {"title_translation": "基于抽象的神经网络形式化验证中的证明生成", "tldr": "本文提出了一种新的框架，用于在神经网络形式化验证中结合抽象方法和证明生成，以解决可伸缩性和可证明保证之间的差距。", "motivation": "当前的证明生成验证器不支持基于抽象的推理，这在深度神经网络（DNNs）验证的可伸缩性与可证明保证之间造成了差距。", "method": "本文引入了一种新颖的框架，用于生成证明的基于抽象的DNN验证。该方法将验证任务模块化地分为两部分：(i) 证明抽象网络的正确性，以及 (ii) 证明抽象相对于原始DNN的健全性。对于后者，本文提出了首个生成形式化证明的方法。", "result": "这项初步工作旨在通过在形式化证明框架内支持常见的抽象技术，从而实现可伸缩和可信赖的验证。", "conclusion": "本文通过引入一个新颖的框架，首次实现了在深度神经网络形式化验证中，将抽象方法与形式化证明生成相结合，从而弥合了可伸缩性和可证明保证之间的差距。", "translation": "现代深度神经网络（DNNs）验证工具越来越依赖抽象来扩展到实际架构。同时，证明生成正成为提高DNN验证结果可靠性的关键要求。然而，当前的证明生成验证器不支持基于抽象的推理，这在可伸缩性与可证明保证之间造成了差距。我们通过引入一种新颖的框架来解决这一差距，该框架用于生成证明的基于抽象的DNN验证。我们的方法将验证任务模块化地分为两个组件：(i) 证明抽象网络的正确性，以及 (ii) 证明抽象相对于原始DNN的健全性。前者可以通过现有证明生成验证器处理，而我们为后者提出了第一个生成形式化证明的方法。这项初步工作旨在通过在形式化证明框架内支持常见的抽象技术，从而实现可伸缩和可信赖的验证。", "summary": "本文提出了一种新颖的框架，旨在解决深度神经网络（DNNs）形式化验证中，抽象技术带来的可伸缩性与证明生成能力之间的差距。该框架将验证任务分解为抽象网络的正确性证明和抽象对原始DNN的健全性证明两部分。其中，本文首次提出了一种为抽象健全性生成形式化证明的方法。这项初步工作旨在促进可伸缩且可信赖的DNN验证。", "keywords": "神经网络验证, 形式化证明, 抽象, 深度学习, 可伸缩性", "comments": "本文的创新点在于提出了一个将抽象方法与形式化证明生成相结合的框架，特别是在首次为抽象的健全性提供了形式化证明方法，这对于弥补当前DNN验证工具在可伸缩性和证明保证之间的差距具有重要意义。这一工作有望显著提高DNN验证的可靠性和适用范围。"}}
{"id": "2506.09887", "title": "Learning single-index models via harmonic decomposition", "authors": ["Nirmit Joshi", "Hugo Koubbi", "Theodor Misiakiewicz", "Nathan Srebro"], "summary": "We study the problem of learning single-index models, where the label $y \\in\n\\mathbb{R}$ depends on the input $\\boldsymbol{x} \\in \\mathbb{R}^d$ only through\nan unknown one-dimensional projection $\\langle\n\\boldsymbol{w}_*,\\boldsymbol{x}\\rangle$. Prior work has shown that under\nGaussian inputs, the statistical and computational complexity of recovering\n$\\boldsymbol{w}_*$ is governed by the Hermite expansion of the link function.\nIn this paper, we propose a new perspective: we argue that \"spherical\nharmonics\" -- rather than \"Hermite polynomials\" -- provide the natural basis\nfor this problem, as they capture its intrinsic \"rotational symmetry\". Building\non this insight, we characterize the complexity of learning single-index models\nunder arbitrary spherically symmetric input distributions. We introduce two\nfamilies of estimators -- based on tensor unfolding and online SGD -- that\nrespectively achieve either optimal sample complexity or optimal runtime, and\nargue that estimators achieving both may not exist in general. When specialized\nto Gaussian inputs, our theory not only recovers and clarifies existing results\nbut also reveals new phenomena that had previously been overlooked.", "comment": "80 pages", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09887v1", "AI": {"title_translation": "通过谐波分解学习单指标模型", "tldr": "本文提出使用球谐函数而非Hermite多项式作为学习单指标模型的自然基础，特别适用于任意球对称输入分布，并引入了实现最优样本复杂度或最优运行时间的新型估计器。", "motivation": "现有研究在处理高斯输入时，利用Hermite展开来分析单指标模型的统计和计算复杂度。然而，本文认为“球谐函数”更能捕捉问题的内在“旋转对称性”，因此提供了一个更自然的基，以扩展到任意球对称输入分布，并揭示现有理论中被忽视的新现象。", "method": "本文提出将“球谐函数”作为学习单指标模型的自然基，因为它们能捕捉问题的“旋转对称性”。在此基础上，提出了两种估计器家族：“张量展开”和“在线SGD”。", "result": "本文刻画了在任意球对称输入分布下学习单指标模型的复杂性。所引入的两种估计器家族分别实现了最优样本复杂度或最优运行时间，并认为同时实现两者可能在一般情况下不存在。当专门应用于高斯输入时，该理论不仅恢复并澄清了现有结果，还揭示了以前被忽视的新现象。", "conclusion": "球谐函数为单指标模型的学习提供了一个更自然、更有效的基，尤其是在球对称输入下，这带来了新的见解和具有特定最优保证的估计器。", "translation": "我们研究了学习单指标模型的问题，其中标签$y \\in \\mathbb{R}$仅通过一个未知的一维投影$\\langle \\boldsymbol{w}_*,\\boldsymbol{x}\\rangle$依赖于输入$\\boldsymbol{x} \\in \\mathbb{R}^d$。先前的研究表明，在高斯输入下，恢复$\\boldsymbol{w}_*$的统计和计算复杂度由链接函数的Hermite展开决定。在本文中，我们提出了一个新的视角：我们认为“球谐函数”——而非“Hermite多项式”——为该问题提供了自然的基，因为它们捕捉了其内在的“旋转对称性”。基于这一洞察，我们刻画了在任意球对称输入分布下学习单指标模型的复杂性。我们引入了两个估计器家族——基于张量展开和在线SGD——它们分别实现了最优样本复杂度或最优运行时间，并认为同时实现两者可能在一般情况下不存在。当专门应用于高斯输入时，我们的理论不仅恢复并澄清了现有结果，还揭示了以前被忽视的新现象。", "summary": "本文重新审视了单指标模型的学习问题，提出球谐函数作为比Hermite多项式更优越的基，因为它能捕捉内在的旋转对称性，从而将分析扩展到任意球对称输入分布。文章引入了张量展开和在线SGD两种新的估计器家族，它们分别实现了最优样本复杂度或最优运行时间，并指出同时实现两者可能并非普遍可行。该理论不仅完善了对高斯输入的理解，还揭示了新的现象。", "keywords": "单指标模型, 球谐函数, 张量展开, 在线SGD, 球对称分布", "comments": "本文的创新之处在于提出了使用球谐函数作为学习单指标模型的自然基，这使得该理论能够处理比传统高斯输入更广泛的任意球对称输入分布。这不仅深化了对单指标模型的理论理解，也为设计新的算法提供了方向。文章还揭示了在最优样本复杂度和最优运行时间之间可能存在的权衡，这是对该领域的一个重要洞察。"}}
{"id": "2506.09935", "title": "LEO-VL: Towards 3D Vision-Language Generalists via Data Scaling with Efficient Representation", "authors": ["Jiangyong Huang", "Xiaojian Ma", "Xiongkun Linghu", "Yue Fan", "Junchao He", "Wenxin Tan", "Qing Li", "Song-Chun Zhu", "Yixin Chen", "Baoxiong Jia", "Siyuan Huang"], "summary": "Developing 3D-VL generalists capable of understanding 3D scenes and following\nnatural language instructions to perform a wide range of tasks has been a\nlong-standing goal in the 3D-VL community. Despite recent progress, 3D-VL\nmodels still lag behind their 2D counterparts in capability and robustness,\nfalling short of the generalist standard. A key obstacle to developing 3D-VL\ngeneralists lies in data scalability, hindered by the lack of an efficient\nscene representation. We propose LEO-VL, a 3D-VL model built upon condensed\nfeature grid (CFG), an efficient scene representation that bridges 2D\nperception and 3D spatial structure while significantly reducing token\noverhead. This efficiency unlocks large-scale training towards 3D-VL\ngeneralist, for which we curate over 700k high-quality 3D-VL data spanning four\ndomains of real-world indoor scenes and five tasks such as captioning and\ndialogue. LEO-VL achieves state-of-the-art performance on a variety of 3D QA\nbenchmarks, including SQA3D, MSQA, and Beacon3D. Ablation studies confirm the\nefficiency of our representation, the importance of task and scene diversity,\nand the validity of our data curation principle. Furthermore, we introduce\nSceneDPO, a novel post-training objective that enhances the robustness of 3D-VL\nmodels. We hope our findings contribute to the advancement of scalable and\nrobust 3D-VL generalists.", "comment": "Project page: https://leo-vl.github.io", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09935v1", "AI": {"title_translation": "LEO-VL：通过高效表示的数据扩展迈向3D视觉-语言通才模型", "tldr": "LEO-VL引入了基于凝聚特征网格（CFG）的高效3D场景表示，通过大规模数据扩展和新的后训练目标SceneDPO，显著提升了3D视觉-语言模型的通用性和鲁棒性，并在多项3D QA基准测试中达到了最先进的性能。", "motivation": "开发能够理解3D场景并遵循自然语言指令执行广泛任务的3D视觉-语言通才模型是该领域的长期目标。然而，当前的3D-VL模型在能力和鲁棒性上落后于2D模型，主要障碍在于数据可扩展性，这又受限于缺乏高效的场景表示。", "method": "本研究提出了LEO-VL，一个基于凝聚特征网格（CFG）的3D-VL模型。CFG是一种高效的场景表示，能够连接2D感知与3D空间结构，并显著减少token开销。这种效率使得大规模训练成为可能，为此研究人员收集了超过70万高质量的3D-VL数据，涵盖了现实世界室内场景的四个领域和五项任务（如图像描述和对话）。此外，论文还引入了SceneDPO，一种新颖的后训练目标，旨在增强3D-VL模型的鲁棒性。", "result": "LEO-VL在包括SQA3D、MSQA和Beacon3D在内的多种3D问答基准测试中取得了最先进的性能。消融研究证实了所提出表示的效率、任务和场景多样性的重要性以及数据整理原则的有效性。", "conclusion": "本研究的发现有助于推动可扩展且鲁棒的3D视觉-语言通才模型的发展。", "translation": "开发能够理解3D场景并遵循自然语言指令执行广泛任务的3D视觉-语言通才模型，一直是3D-VL社区的一个长期目标。尽管最近取得了进展，但3D-VL模型在能力和鲁棒性方面仍落后于2D模型，未能达到通才标准。开发3D-VL通才模型的一个关键障碍在于数据可扩展性，而这又受限于缺乏高效的场景表示。我们提出了LEO-VL，一个基于凝聚特征网格（CFG）的3D-VL模型，CFG是一种高效的场景表示，能够连接2D感知和3D空间结构，同时显著减少token开销。这种效率解锁了面向3D-VL通才模型的大规模训练，为此我们整理了超过70万高质量的3D-VL数据，涵盖了现实世界室内场景的四个领域和五项任务，如图像描述和对话。LEO-VL在包括SQA3D、MSQA和Beacon3D在内的各种3D问答基准测试中取得了最先进的性能。消融研究证实了我们表示的效率、任务和场景多样性的重要性以及我们数据整理原则的有效性。此外，我们引入了SceneDPO，一种新颖的后训练目标，可以增强3D-VL模型的鲁棒性。我们希望我们的发现能为可扩展和鲁棒的3D-VL通才模型的发展做出贡献。", "summary": "本研究提出LEO-VL，一个旨在实现3D视觉-语言通才模型的新框架。该模型基于凝聚特征网格（CFG），一种高效的3D场景表示，有效解决了3D-VL模型在数据可扩展性方面的瓶颈。通过大规模的数据集（超过70万高质量数据）训练，并引入了增强鲁棒性的SceneDPO后训练目标，LEO-VL在多个3D问答基准测试中取得了最先进的性能，显著推动了3D-VL领域向更通用、更鲁棒模型的发展。", "keywords": "3D视觉-语言, 通才模型, 数据扩展, 高效表示, CFG", "comments": "该论文通过引入高效的凝聚特征网格（CFG）表示，巧妙地解决了3D视觉-语言（3D-VL）模型在数据可扩展性方面的核心挑战。CFG的创新之处在于它能有效连接2D感知与3D结构，并显著减少token开销，这对于大规模训练至关重要。此外，研究团队通过精心策划包含70多万高质量数据的多样化数据集，并引入SceneDPO这一新颖的后训练目标，进一步提升了模型的通用性和鲁棒性。这些贡献共同推动了3D-VL领域向更接近2D对应模型的通用性迈进，为未来3D场景理解和自然语言交互奠定了坚实基础。"}}
{"id": "2506.09943", "title": "CausalVQA: A Physically Grounded Causal Reasoning Benchmark for Video Models", "authors": ["Aaron Foss", "Chloe Evans", "Sasha Mitts", "Koustuv Sinha", "Ammar Rizvi", "Justine T. Kao"], "summary": "We introduce CausalVQA, a benchmark dataset for video question answering\n(VQA) composed of question-answer pairs that probe models' understanding of\ncausality in the physical world. Existing VQA benchmarks either tend to focus\non surface perceptual understanding of real-world videos, or on narrow physical\nreasoning questions created using simulation environments. CausalVQA fills an\nimportant gap by presenting challenging questions that are grounded in\nreal-world scenarios, while focusing on models' ability to predict the likely\noutcomes of different actions and events through five question types:\ncounterfactual, hypothetical, anticipation, planning and descriptive. We\ndesigned quality control mechanisms that prevent models from exploiting trivial\nshortcuts, requiring models to base their answers on deep visual understanding\ninstead of linguistic cues. We find that current frontier multimodal models\nfall substantially below human performance on the benchmark, especially on\nanticipation and hypothetical questions. This highlights a challenge for\ncurrent systems to leverage spatial-temporal reasoning, understanding of\nphysical principles, and comprehension of possible alternatives to make\naccurate predictions in real-world settings.", "comment": "35 pages, 3 figures, Submitted to NeurIPS2025 benchmark track", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09943v1", "AI": {"title_translation": "CausalVQA: 一个基于物理的视频模型因果推理基准", "tldr": "CausalVQA是一个新的视频问答数据集，旨在测试模型对物理世界因果关系的理解，现有模型在该基准上的表现远低于人类水平，尤其是在预测和假设性问题上。", "motivation": "现有视频问答(VQA)基准要么侧重于真实世界视频的表面感知理解，要么侧重于使用模拟环境创建的狭窄物理推理问题。CausalVQA旨在填补这一空白，提出基于真实世界场景的挑战性问题，并专注于模型预测不同动作和事件可能结果的能力。", "method": "CausalVQA是一个包含问答对的视频问答(VQA)基准数据集，用于探究模型对物理世界因果关系的理解。它通过五种问题类型（反事实、假设、预期、规划和描述）来测试模型预测可能结果的能力。数据集设计了质量控制机制，防止模型利用琐碎的捷径，要求模型基于深度视觉理解而非语言线索来回答问题。", "result": "当前前沿多模态模型在该基准上的表现远低于人类水平，尤其是在预期和假设性问题上。", "conclusion": "这突出表明当前系统在利用时空推理、理解物理原理以及理解可能的替代方案以在真实世界环境中做出准确预测方面面临挑战。", "translation": "我们引入了CausalVQA，这是一个视频问答（VQA）的基准数据集，由探测模型对物理世界因果关系理解的问答对组成。现有的VQA基准倾向于关注真实世界视频的表面感知理解，或者关注使用模拟环境创建的狭窄物理推理问题。CausalVQA通过提出基于真实世界场景的挑战性问题，并专注于模型通过五种问题类型：反事实、假设、预期、规划和描述来预测不同动作和事件可能结果的能力，填补了一个重要空白。我们设计了质量控制机制，防止模型利用琐碎的捷径，要求模型基于深度视觉理解而非语言线索来回答问题。我们发现当前前沿多模态模型在该基准上的表现远低于人类水平，尤其是在预期和假设性问题上。这突出表明当前系统在利用时空推理、理解物理原理以及理解可能的替代方案以在真实世界环境中做出准确预测方面面临挑战。", "summary": "CausalVQA是一个新的视频问答(VQA)基准数据集，专注于评估模型对物理世界因果关系的理解。它通过包含反事实、假设、预期、规划和描述五种问题类型，提出基于真实世界场景的挑战性问题。该数据集通过质量控制机制避免模型利用表面线索，要求深度视觉理解。研究发现，当前领先的多模态模型在该基准上，尤其是在预期和假设性问题上，表现远低于人类水平，揭示了现有系统在时空推理、物理原理理解和预测能力上的不足。", "keywords": "CausalVQA, 视频问答, 因果推理, 物理世界, 基准数据集", "comments": "CausalVQA的创新之处在于它填补了现有VQA基准在因果推理和真实世界场景结合方面的空白。通过设计质量控制机制，它有效地避免了模型利用表面线索，从而真正测试了模型的深层视觉理解和因果推理能力。其重要性在于揭示了当前前沿多模态模型在复杂因果推理方面的显著局限性，特别是在预测和假设性情境下，为未来研究指明了方向。"}}
{"id": "2506.09896", "title": "A look at adversarial attacks on radio waveforms from discrete latent space", "authors": ["Attanasia Garuso", "Silvija Kokalj-Filipovic", "Yagna Kaasaragadda"], "summary": "Having designed a VQVAE that maps digital radio waveforms into discrete\nlatent space, and yields a perfectly classifiable reconstruction of the\noriginal data, we here analyze the attack suppressing properties of VQVAE when\nan adversarial attack is performed on high-SNR radio-frequency (RF)\ndata-points. To target amplitude modulations from a subset of digitally\nmodulated waveform classes, we first create adversarial attacks that preserve\nthe phase between the in-phase and quadrature component whose values are\nadversarially changed. We compare them with adversarial attacks of the same\nintensity where phase is not preserved. We test the classification accuracy of\nsuch adversarial examples on a classifier trained to deliver 100% accuracy on\nthe original data. To assess the ability of VQVAE to suppress the strength of\nthe attack, we evaluate the classifier accuracy on the reconstructions by VQVAE\nof the adversarial datapoints and show that VQVAE substantially decreases the\neffectiveness of the attack. We also compare the I/Q plane diagram of the\nattacked data, their reconstructions and the original data. Finally, using\nmultiple methods and metrics, we compare the probability distribution of the\nVQVAE latent space with and without attack. Varying the attack strength, we\nobserve interesting properties of the discrete space, which may help detect the\nattacks.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09896v1", "AI": {"title_translation": "从离散潜在空间看射频波形上的对抗性攻击", "tldr": "本文分析了VQVAE在射频数据上对抗性攻击的抑制能力，发现VQVAE能显著降低攻击效果，并且离散潜在空间特性可能有助于检测攻击。", "motivation": "本文旨在分析VQVAE在射频数据点上执行对抗性攻击时，其抑制攻击的特性。作者设计了一个VQVAE，能够将数字射频波形映射到离散潜在空间，并能完美地重建原始数据。", "method": "研究方法包括：1. 设计一个将数字射频波形映射到离散潜在空间的VQVAE，并确保其能完美地重建原始数据。2. 创建针对高信噪比射频数据点的对抗性攻击，包括保持同相和正交分量之间相位的攻击，以及不保持相位的攻击。3. 比较这些攻击对分类器准确性的影响，该分类器在原始数据上能达到100%的准确率。4. 评估VQVAE重建对抗性数据点后分类器的准确性，以评估其抑制攻击的能力。5. 比较受攻击数据、其重建数据和原始数据的I/Q平面图。6. 使用多种方法和指标比较VQVAE潜在空间在有无攻击情况下的概率分布。", "result": "研究结果表明：1. VQVAE能够显著降低对抗性攻击的有效性。2. 改变攻击强度时，观察到离散空间有趣的特性，这些特性可能有助于检测攻击。", "conclusion": "VQVAE能够有效抑制对射频波形的对抗性攻击，并且离散潜在空间的特性可能有助于检测这些攻击。", "translation": "在设计了一个将数字射频波形映射到离散潜在空间并能完美重建原始数据的VQVAE之后，我们在此分析了当在高信噪比射频（RF）数据点上执行对抗性攻击时，VQVAE的攻击抑制特性。为了针对数字调制波形子集中的幅度调制，我们首先创建了保持同相和正交分量之间相位的对抗性攻击，尽管其值被对抗性地改变。我们将其与强度相同但不保持相位的对抗性攻击进行比较。我们在一个经过训练在原始数据上能提供100%准确率的分类器上测试了这些对抗性示例的分类准确性。为了评估VQVAE抑制攻击强度的能力，我们评估了VQVAE重建对抗性数据点后分类器的准确性，并表明VQVAE显著降低了攻击的有效性。我们还比较了受攻击数据、其重建数据和原始数据的I/Q平面图。最后，使用多种方法和指标，我们比较了VQVAE潜在空间在有无攻击情况下的概率分布。通过改变攻击强度，我们观察到离散空间有趣的特性，这可能有助于检测攻击。", "summary": "本文研究了VQVAE在射频波形对抗性攻击中的抑制作用。研究者设计了一个VQVAE将射频波形映射到离散潜在空间，并创建了不同类型的对抗性攻击。通过比较原始数据和VQVAE重建数据上的分类器准确性，发现VQVAE能显著降低攻击效果。此外，研究还分析了潜在空间的特性，发现其可能有助于检测攻击。", "keywords": "对抗性攻击, 射频波形, VQVAE, 离散潜在空间, 攻击检测", "comments": "本文通过引入VQVAE来处理射频波形上的对抗性攻击，展示了其在降低攻击效果方面的潜力。其创新点在于利用离散潜在空间进行攻击抑制和潜在的攻击检测。这对于无线通信安全领域具有重要意义，尤其是在高信噪比环境下。"}}
{"id": "2506.09952", "title": "UniPre3D: Unified Pre-training of 3D Point Cloud Models with Cross-Modal Gaussian Splatting", "authors": ["Ziyi Wang", "Yanran Zhang", "Jie Zhou", "Jiwen Lu"], "summary": "The scale diversity of point cloud data presents significant challenges in\ndeveloping unified representation learning techniques for 3D vision. Currently,\nthere are few unified 3D models, and no existing pre-training method is equally\neffective for both object- and scene-level point clouds. In this paper, we\nintroduce UniPre3D, the first unified pre-training method that can be\nseamlessly applied to point clouds of any scale and 3D models of any\narchitecture. Our approach predicts Gaussian primitives as the pre-training\ntask and employs differentiable Gaussian splatting to render images, enabling\nprecise pixel-level supervision and end-to-end optimization. To further\nregulate the complexity of the pre-training task and direct the model's focus\ntoward geometric structures, we integrate 2D features from pre-trained image\nmodels to incorporate well-established texture knowledge. We validate the\nuniversal effectiveness of our proposed method through extensive experiments\nacross a variety of object- and scene-level tasks, using diverse point cloud\nmodels as backbones. Code is available at https://github.com/wangzy22/UniPre3D.", "comment": "Accepted to CVPR 2025", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09952v1", "AI": {"title_translation": "UniPre3D：基于跨模态高斯溅射的3D点云模型统一预训练", "tldr": "UniPre3D是一种新的统一预训练方法，通过预测高斯基元和使用可微分高斯溅射，解决了3D点云模型在不同尺度和架构下的统一表示学习挑战，并在多种任务上表现出通用有效性。", "motivation": "3D点云数据的尺度多样性给开发统一的3D视觉表示学习技术带来了巨大挑战。目前，缺乏统一的3D模型，并且现有预训练方法无法同时有效处理物体级和场景级点云。", "method": "论文引入了UniPre3D，这是一种统一的预训练方法，适用于任意尺度点云和任意架构的3D模型。该方法将预测高斯基元作为预训练任务，并利用可微分高斯溅射渲染图像，实现精确的像素级监督和端到端优化。此外，它还整合了来自预训练图像模型的2D特征，以引入纹理知识并引导模型关注几何结构。", "result": "通过在各种物体级和场景级任务上使用不同的点云模型作为骨干网络进行广泛实验，验证了所提出方法的普遍有效性。", "conclusion": "UniPre3D是首个能统一预训练任意尺度点云和任意架构3D模型的通用方法，通过结合高斯溅射和跨模态特征，有效解决了3D视觉中统一表示学习的挑战。", "translation": "点云数据的尺度多样性给开发3D视觉的统一表示学习技术带来了重大挑战。目前，统一的3D模型很少，并且没有现有的预训练方法能同时对物体级和场景级点云同样有效。在本文中，我们介绍了UniPre3D，这是第一个可以无缝应用于任何尺度点云和任何架构3D模型的统一预训练方法。我们的方法将预测高斯基元作为预训练任务，并采用可微分高斯溅射来渲染图像，从而实现精确的像素级监督和端到端优化。为了进一步规范预训练任务的复杂性并将模型的重点导向几何结构，我们整合了来自预训练图像模型的2D特征，以纳入成熟的纹理知识。我们通过在各种物体级和场景级任务上使用不同的点云模型作为骨干网络进行广泛实验，验证了我们所提出方法的普遍有效性。代码可在https://github.com/wangzy22/UniPre3D获取。", "summary": "UniPre3D提出了一种新颖的统一预训练方法，旨在解决3D点云数据尺度多样性带来的挑战，实现对物体级和场景级点云的通用表示学习。该方法通过预测高斯基元并利用可微分高斯溅射进行像素级监督预训练，同时整合2D图像特征以增强几何结构学习。实验证明，UniPre3D在多种3D任务中表现出普遍有效性，是首个能统一预训练任意尺度点云和任意架构3D模型的方案。", "keywords": "3D点云, 统一预训练, 高斯溅射, 跨模态学习, 3D视觉", "comments": "这篇论文的创新点在于提出了首个真正统一的3D点云预训练方法UniPre3D，能够处理不同尺度和架构的数据。其核心思想是利用高斯溅射进行像素级监督，并巧妙地融入2D图像模型的纹理知识，这为3D视觉的通用表示学习开辟了新途径，具有重要的研究价值。"}}
{"id": "2506.09901", "title": "\"What are my options?\": Explaining RL Agents with Diverse Near-Optimal Alternatives (Extended)", "authors": ["Noel Brindise", "Vijeth Hebbar", "Riya Shah", "Cedric Langbort"], "summary": "In this work, we provide an extended discussion of a new approach to\nexplainable Reinforcement Learning called Diverse Near-Optimal Alternatives\n(DNA), first proposed at L4DC 2025. DNA seeks a set of reasonable \"options\" for\ntrajectory-planning agents, optimizing policies to produce qualitatively\ndiverse trajectories in Euclidean space. In the spirit of explainability, these\ndistinct policies are used to \"explain\" an agent's options in terms of\navailable trajectory shapes from which a human user may choose. In particular,\nDNA applies to value function-based policies on Markov decision processes where\nagents are limited to continuous trajectories. Here, we describe DNA, which\nuses reward shaping in local, modified Q-learning problems to solve for\ndistinct policies with guaranteed epsilon-optimality. We show that it\nsuccessfully returns qualitatively different policies that constitute\nmeaningfully different \"options\" in simulation, including a brief comparison to\nrelated approaches in the stochastic optimization field of Quality Diversity.\nBeyond the explanatory motivation, this work opens new possibilities for\nexploration and adaptive planning in RL.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09901v1", "AI": {"title_translation": "“我的选择是什么？”：用多样化近最优替代方案解释强化学习智能体（扩展）", "tldr": "提出DNA方法，通过生成多样化的近最优轨迹来解释强化学习智能体的行为选择。", "motivation": "旨在提高强化学习（RL）的可解释性，通过为轨迹规划智能体提供一组合理的“选项”，使人类用户能够理解智能体的可用轨迹形状。", "method": "采用名为“多样化近最优替代方案（DNA）”的新方法，该方法在局部、修改后的Q-learning问题中使用奖励塑形，以求解具有保证epsilon-最优性的不同策略。适用于马尔可夫决策过程上的基于值函数的策略，其中智能体限于连续轨迹。", "result": "成功返回了在模拟中构成有意义的不同“选项”的定性不同策略，并与质量多样性随机优化领域的相关方法进行了比较。", "conclusion": "除了可解释性动机之外，这项工作为强化学习中的探索和自适应规划开辟了新的可能性。", "translation": "在这项工作中，我们对一种新的可解释强化学习方法——多样化近最优替代方案（DNA）——进行了扩展讨论，该方法于L4DC 2025首次提出。DNA旨在为轨迹规划智能体寻找一组合理的“选项”，优化策略以在欧几里得空间中生成定性多样化的轨迹。本着可解释性的精神，这些不同的策略被用来从人类用户可以选择的可用轨迹形状方面“解释”智能体的选项。特别是，DNA适用于马尔可夫决策过程上基于值函数的策略，其中智能体限于连续轨迹。在这里，我们描述了DNA，它在局部、修改后的Q-learning问题中使用奖励塑形来求解具有保证epsilon-最优性的不同策略。我们表明，它成功返回了在模拟中构成有意义的不同“选项”的定性不同策略，包括与质量多样性随机优化领域相关方法的简要比较。除了可解释性动机之外，这项工作为强化学习中的探索和自适应规划开辟了新的可能性。", "summary": "这项工作扩展讨论了名为“多样化近最优替代方案（DNA）”的可解释强化学习新方法。DNA通过在局部Q-learning问题中使用奖励塑形，生成一组定性多样化且具有epsilon-最优性的近最优轨迹，从而解释强化学习智能体的行为选项。研究表明该方法在模拟中成功提供了有意义的不同策略，并为强化学习中的探索和自适应规划提供了新方向。", "keywords": "可解释强化学习, 多样化近最优替代方案, Q-learning, 轨迹规划, 策略多样性", "comments": "这项工作提出了一种新颖的方法DNA，通过生成多样化的近最优轨迹来增强RL的可解释性，这对于人类理解和选择智能体行为至关重要。其创新点在于结合了奖励塑形和Q-learning来寻找多样化策略，并为RL的探索和自适应规划开辟了新途径，具有重要的理论和实践意义。"}}
{"id": "2506.09953", "title": "Outside Knowledge Conversational Video (OKCV) Dataset -- Dialoguing over Videos", "authors": ["Benjamin Reichman", "Constantin Patsch", "Jack Truxal", "Atishay Jain", "Larry Heck"], "summary": "In outside knowledge visual question answering (OK-VQA), the model must\nidentify relevant visual information within an image and incorporate external\nknowledge to accurately respond to a question. Extending this task to a\nvisually grounded dialogue setting based on videos, a conversational model must\nboth recognize pertinent visual details over time and answer questions where\nthe required information is not necessarily present in the visual information.\nMoreover, the context of the overall conversation must be considered for the\nsubsequent dialogue. To explore this task, we introduce a dataset comprised of\n$2,017$ videos with $5,986$ human-annotated dialogues consisting of $40,954$\ninterleaved dialogue turns. While the dialogue context is visually grounded in\nspecific video segments, the questions further require external knowledge that\nis not visually present. Thus, the model not only has to identify relevant\nvideo parts but also leverage external knowledge to converse within the\ndialogue. We further provide several baselines evaluated on our dataset and\nshow future challenges associated with this task. The dataset is made publicly\navailable here: https://github.com/c-patsch/OKCV.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09953v1", "AI": {"title_translation": "外部知识对话视频 (OKCV) 数据集——视频对话", "tldr": "本文介绍了OKCV数据集，这是一个用于基于视频的对话式问答的新数据集，要求模型结合视觉信息、对话上下文和外部知识进行响应。", "motivation": "为了将外部知识视觉问答（OK-VQA）任务扩展到基于视频的视觉对话设置，并探索模型需要识别随时间变化的视觉细节、回答视觉信息中不一定存在的问题以及考虑整体对话上下文的挑战，特别是当问题需要外部知识时。", "method": "研究人员引入了一个名为“外部知识对话视频（OKCV）”的新数据集。该数据集包含2,017个视频，以及5,986个人工标注的对话，共计40,954个交错的对话轮次。数据集中的对话上下文以特定视频片段为基础，但问题还需要视觉上不存在的外部知识。此外，论文还提供了在该数据集上评估的几个基线模型。", "result": "该研究成功构建并公开了一个大规模的OKCV数据集，用于探索结合视频理解、对话上下文和外部知识的对话式问答任务。通过提供基线模型，研究人员展示了与该任务相关的未来挑战。", "conclusion": "本文介绍了OKCV数据集，旨在促进在需要整合外部知识、视频理解和对话上下文的视觉对话任务中的研究。该数据集的发布为解决这一复杂任务提供了新的挑战和研究方向。", "translation": "在外部知识视觉问答（OK-VQA）中，模型必须识别图像中的相关视觉信息并结合外部知识才能准确回答问题。将此任务扩展到基于视频的视觉对话设置时，对话模型必须同时识别随时间变化的相关视觉细节，并回答所需信息不一定存在于视觉信息中的问题。此外，必须考虑整个对话的上下文以进行后续对话。为了探索这项任务，我们引入了一个数据集，包含2,017个视频，以及5,986个人工标注的对话，共计40,954个交错的对话轮次。虽然对话上下文以特定视频片段为基础，但问题进一步需要视觉上不存在的外部知识。因此，模型不仅要识别相关的视频部分，还要利用外部知识在对话中进行交流。我们进一步提供了几个在我们数据集上评估的基线模型，并展示了与此任务相关的未来挑战。该数据集已在此处公开：https://github.com/c-patsch/OKCV。", "summary": "本文介绍了外部知识对话视频（OKCV）数据集，旨在解决基于视频的对话式问答任务中模型需要整合视觉信息、对话上下文和外部知识的挑战。该数据集包含2,017个视频和5,986个对话，共40,954个对话轮次，其中问题常需要视觉信息之外的外部知识。研究者提供了基线模型并指出了未来研究方向，强调了该任务的复杂性。", "keywords": "外部知识, 对话视频, 数据集, 视频问答, 对话", "comments": "该论文的主要创新在于构建并公开了一个独特且具有挑战性的数据集——OKCV。它将当前视觉问答任务扩展到更复杂的视频对话场景，并特别强调了对外部知识和对话上下文的依赖。这对于推动多模态理解和对话系统发展至关重要，因为它模拟了更接近人类交流的复杂信息整合过程。数据集的规模和标注质量是其重要性所在，而提供的基线模型也为后续研究提供了起点。"}}
{"id": "2506.09923", "title": "Apollo: A Posteriori Label-Only Membership Inference Attack Towards Machine Unlearning", "authors": ["Liou Tang", "James Joshi", "Ashish Kundu"], "summary": "Machine Unlearning (MU) aims to update Machine Learning (ML) models following\nrequests to remove training samples and their influences on a trained model\nefficiently without retraining the original ML model from scratch. While MU\nitself has been employed to provide privacy protection and regulatory\ncompliance, it can also increase the attack surface of the model. Existing\nprivacy inference attacks towards MU that aim to infer properties of the\nunlearned set rely on the weaker threat model that assumes the attacker has\naccess to both the unlearned model and the original model, limiting their\nfeasibility toward real-life scenarios. We propose a novel privacy attack, A\nPosteriori Label-Only Membership Inference Attack towards MU, Apollo, that\ninfers whether a data sample has been unlearned, following a strict threat\nmodel where an adversary has access to the label-output of the unlearned model\nonly. We demonstrate that our proposed attack, while requiring less access to\nthe target model compared to previous attacks, can achieve relatively high\nprecision on the membership status of the unlearned samples.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09923v1", "AI": {"title_translation": "Apollo：一种针对机器遗忘的后验标签攻击", "tldr": "该论文提出了Apollo，一种针对机器遗忘的后验标签攻击，它仅需要遗忘模型的标签输出即可推断数据样本是否已被遗忘，并在更严格的威胁模型下实现了相对较高的精度。", "motivation": "机器遗忘（MU）旨在高效地移除训练样本及其对已训练模型的影响，而无需从头开始重新训练原始ML模型。虽然MU本身被用于提供隐私保护和法规遵从性，但它也可能增加模型的攻击面。现有针对MU的隐私推断攻击旨在推断未学习集合的属性，但它们依赖于较弱的威胁模型，即假设攻击者可以访问未学习模型和原始模型，这限制了它们在现实场景中的可行性。", "method": "本文提出了一种新颖的隐私攻击——Apollo，即一种针对MU的后验标签攻击。该攻击在严格的威胁模型下，即攻击者仅能访问遗忘模型的标签输出，推断数据样本是否已被遗忘。", "result": "与之前的攻击相比，我们提出的攻击虽然需要更少的对目标模型的访问权限，但仍能对未学习样本的成员状态实现相对较高的精度。", "conclusion": "本文提出的Apollo攻击，在仅访问遗忘模型标签输出的严格威胁模型下，仍能有效推断数据样本是否已被遗忘，并取得了较高的精度，这表明了机器遗忘模型在现实场景中可能面临的隐私风险。", "translation": "机器遗忘（MU）旨在根据移除训练样本及其对已训练模型影响的请求，高效地更新机器学习（ML）模型，而无需从头开始重新训练原始ML模型。虽然MU本身已被用于提供隐私保护和法规遵从性，但它也可能增加模型的攻击面。现有针对MU的隐私推断攻击旨在推断未学习集合的属性，它们依赖于较弱的威胁模型，即假设攻击者可以访问未学习模型和原始模型，这限制了它们在现实场景中的可行性。我们提出了一种新颖的隐私攻击，即针对MU的后验标签攻击Apollo，它在严格的威胁模型下（即对手仅能访问遗忘模型的标签输出）推断数据样本是否已被遗忘。我们证明，与之前的攻击相比，我们提出的攻击虽然需要更少的对目标模型的访问权限，但仍能对未学习样本的成员状态实现相对较高的精度。", "summary": "Apollo是一种新颖的隐私攻击，它针对机器遗忘（MU）模型，在严格的威胁模型下（攻击者仅能访问遗忘模型的标签输出）推断数据样本是否已被遗忘。与现有的需要访问原始模型和遗忘模型的攻击相比，Apollo所需的模型访问权限更少，但仍能对被遗忘样本的成员状态实现相对较高的精度。", "keywords": "机器遗忘, 成员推断攻击, 隐私攻击, 仅标签, Apollo", "comments": "这篇论文通过提出一种在更现实的威胁模型（仅标签输出访问）下的攻击，解决了现有针对机器遗忘的成员推断攻击的实际局限性。这揭示了机器遗忘系统的一个新漏洞，强调了开发更鲁棒的遗忘机制的必要性。"}}
{"id": "2506.09954", "title": "Vision Generalist Model: A Survey", "authors": ["Ziyi Wang", "Yongming Rao", "Shuofeng Sun", "Xinrun Liu", "Yi Wei", "Xumin Yu", "Zuyan Liu", "Yanbo Wang", "Hongmin Liu", "Jie Zhou", "Jiwen Lu"], "summary": "Recently, we have witnessed the great success of the generalist model in\nnatural language processing. The generalist model is a general framework\ntrained with massive data and is able to process various downstream tasks\nsimultaneously. Encouraged by their impressive performance, an increasing\nnumber of researchers are venturing into the realm of applying these models to\ncomputer vision tasks. However, the inputs and outputs of vision tasks are more\ndiverse, and it is difficult to summarize them as a unified representation. In\nthis paper, we provide a comprehensive overview of the vision generalist\nmodels, delving into their characteristics and capabilities within the field.\nFirst, we review the background, including the datasets, tasks, and benchmarks.\nThen, we dig into the design of frameworks that have been proposed in existing\nresearch, while also introducing the techniques employed to enhance their\nperformance. To better help the researchers comprehend the area, we take a\nbrief excursion into related domains, shedding light on their interconnections\nand potential synergies. To conclude, we provide some real-world application\nscenarios, undertake a thorough examination of the persistent challenges, and\noffer insights into possible directions for future research endeavors.", "comment": "Accepted by International Journal of Computer Vision (IJCV)", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09954v1", "AI": {"title_translation": "视觉通才模型：一项综述", "tldr": "鉴于通用模型在NLP领域的成功以及视觉任务的复杂性，本文对视觉通用模型进行了全面综述，涵盖其背景、框架设计、相关领域、应用场景、挑战和未来方向。", "motivation": "受到通用模型在自然语言处理领域取得巨大成功的启发，以及计算机视觉任务输入输出多样性带来的统一表示困难，研究者们正在探索将这些模型应用于视觉任务。因此，本文旨在对视觉通才模型进行全面概述。", "method": "本文首先回顾了背景，包括数据集、任务和基准；然后深入探讨了现有研究中提出的框架设计及其性能增强技术；接着简要介绍了相关领域及其相互联系；最后提供了实际应用场景，分析了现有挑战并提出了未来研究方向。", "result": "本文提供了一个关于视觉通才模型的全面概述，详细介绍了其特性和能力，涵盖了背景、框架设计、性能增强技术、相关领域、应用场景、挑战和未来研究方向。", "conclusion": "本文总结了视觉通才模型的应用场景，深入探讨了其面临的挑战，并为未来的研究工作提供了可能的方向。", "translation": "近期，我们见证了通用模型在自然语言处理领域取得的巨大成功。通用模型是一个通过海量数据训练的通用框架，能够同时处理各种下游任务。受其令人印象深刻的性能鼓舞，越来越多的研究人员正尝试将这些模型应用于计算机视觉任务。然而，视觉任务的输入和输出更加多样化，很难将其总结为统一的表示。在本文中，我们对视觉通才模型进行了全面概述，深入探讨了其在该领域的特点和能力。首先，我们回顾了背景，包括数据集、任务和基准。然后，我们深入探讨了现有研究中提出的框架设计，同时介绍了用于增强其性能的技术。为了更好地帮助研究人员理解该领域，我们简要介绍了相关领域，阐明了它们之间的相互联系和潜在协同作用。最后，我们提供了一些现实世界的应用场景，对持续存在的挑战进行了彻底检查，并对未来研究的可能方向提出了见解。", "summary": "本文对视觉通才模型进行了全面综述，旨在解决通用模型在NLP领域成功后，将其应用于计算机视觉任务时面临的输入输出多样性挑战。综述内容包括背景、框架设计、性能增强技术、相关领域、实际应用、现有挑战及未来研究方向，为研究人员提供了该领域全面的理解和指导。", "keywords": "视觉通才模型, 计算机视觉, 综述, 通用模型, 深度学习", "comments": "这篇综述论文对于理解视觉通才模型的发展现状和未来趋势具有重要意义。它系统地梳理了该领域的关键要素，包括背景、方法和挑战，并提供了有价值的未来研究方向，对于希望进入或深入研究此领域的研究人员来说，是一份宝贵的资源。"}}
{"id": "2506.09507", "title": "TransXSSM: A Hybrid Transformer State Space Model with Unified Rotary Position Embedding", "authors": ["Bingheng Wu", "Jingze Shi", "Yifan Wu", "Nan Tang", "Yuyu Luo"], "summary": "Transformers exhibit proficiency in capturing long-range dependencies,\nwhereas State Space Models (SSMs) facilitate linear-time sequence modeling.\nNotwithstanding their synergistic potential, the integration of these\narchitectures presents a significant challenge, primarily attributable to a\nfundamental incongruity in their respective positional encoding mechanisms:\nTransformers rely on explicit Rotary Position Embeddings (RoPE), while SSMs\nleverage implicit positional representations via convolutions. This divergence\noften precipitates discontinuities and suboptimal performance. To address this\nimpediment, we propose a unified rotary position embedding (\\textbf{\\ourRoPE})\nmethodology, thereby establishing a consistent positional encoding framework\nfor both self-attention and state-space components. Using this \\ourRoPE, we\nintroduce \\textbf{\\model}, a hybrid architecture that coherently integrates the\nTransformer and SSM layers under this unified positional encoding scheme. At a\n4K sequence length, \\model exhibits training and inference speeds that are\n\\textbf{42.3\\% and 29.5\\% faster}, respectively, relative to standard\nTransformer models. It also delivers higher accuracy: under comparable\nsettings, it surpasses a Transformer baseline by over 4\\% on language modeling\nbenchmarks. \\model furthermore scales more effectively: \\model-1.3B gains\n\\textbf{7.22\\%} in average accuracy over its 320M version (versus about 6\\%\ngains for equivalent Transformers or SSMs). Our results show that unified\npositional encoding resolves positional incompatibility in hybrid models,\nenabling efficient, high-performance long-context modeling.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.09507v1", "AI": {"title_translation": "TransXSSM：一种带有统一旋转位置嵌入的混合Transformer状态空间模型", "tldr": "TransXSSM提出了一种统一的旋转位置嵌入方法(\\\\ourRoPE)，解决了Transformer和状态空间模型(SSM)在混合架构中位置编码不兼容的问题，从而实现了更高效、更高性能的长上下文建模。", "motivation": "当前Transformer和状态空间模型(SSM)的集成面临挑战，主要原因是它们各自的位置编码机制不兼容：Transformer使用显式旋转位置嵌入(RoPE)，而SSM通过卷积利用隐式位置表示，这种差异导致了不连续性和次优性能。", "method": "提出了一种统一的旋转位置嵌入(\\\\ourRoPE)方法，为自注意力机制和状态空间组件建立了统一的位置编码框架。基于此，引入了TransXSSM模型，这是一种在统一位置编码方案下，将Transformer和SSM层连贯整合的混合架构。", "result": "在4K序列长度下，TransXSSM的训练和推理速度分别比标准Transformer模型快42.3%和29.5%。在语言建模基准测试中，它比Transformer基线高出4%以上的准确率。TransXSSM-1.3B版本比其320M版本在平均准确率上提高了7.22%（而同等Transformer或SSM模型约提高6%）。", "conclusion": "统一的位置编码解决了混合模型中的位置不兼容问题，实现了高效、高性能的长上下文建模。", "translation": "Transformer在捕获长距离依赖方面表现出色，而状态空间模型（SSM）则促进了线性时间序列建模。尽管它们具有协同潜力，但这些架构的集成提出了一个重大挑战，这主要归因于它们各自位置编码机制的根本不一致：Transformer依赖于显式旋转位置嵌入（RoPE），而SSM通过卷积利用隐式位置表示。这种差异通常会导致不连续性和次优性能。为了解决这一障碍，我们提出了一种统一的旋转位置嵌入（\\\\ourRoPE）方法，从而为自注意力机制和状态空间组件建立了统一的位置编码框架。利用这种\\\\ourRoPE，我们引入了TransXSSM模型，这是一种在统一位置编码方案下，连贯整合Transformer和SSM层的混合架构。在4K序列长度下，TransXSSM的训练和推理速度分别比标准Transformer模型快42.3%和29.5%。它还提供了更高的准确性：在可比较的设置下，它在语言建模基准测试中比Transformer基线高出4%以上。TransXSSM的扩展性也更强：TransXSSM-1.3B版本比其320M版本在平均准确率上提高了7.22%（而同等Transformer或SSM模型约提高6%）。我们的结果表明，统一的位置编码解决了混合模型中的位置不兼容性，实现了高效、高性能的长上下文建模。", "summary": "本文提出了一种名为TransXSSM的混合架构，旨在解决Transformer和状态空间模型(SSM)在集成时因位置编码机制不兼容而导致的问题。通过引入统一的旋转位置嵌入(\\\\ourRoPE)方法，TransXSSM为自注意力机制和状态空间组件提供了统一的位置编码框架。实验结果表明，TransXSSM在训练和推理速度上均优于标准Transformer模型，并在语言建模基准测试中展现出更高的准确性，同时在模型扩展性方面也表现出色，证明了统一位置编码在实现高效、高性能长上下文建模方面的有效性。", "keywords": "混合模型, Transformer, 状态空间模型, 旋转位置嵌入, 长上下文建模", "comments": "本文的核心创新在于提出了一种统一的旋转位置嵌入方法(\\\\ourRoPE)，有效地解决了Transformer和SSM在混合模型中长期存在的位置编码不兼容问题。这一突破性进展不仅提升了模型的训练和推理效率，还显著提高了在长上下文任务上的性能和可扩展性，对于推动混合模型架构的发展具有重要意义。"}}
{"id": "2506.09928", "title": "Bayesian Probabilistic Matrix Factorization", "authors": ["Ruixuan Xu", "Xiangxiang Weng"], "summary": "Matrix factorization is a widely used technique in recommendation systems.\nProbabilistic Matrix Factorization (PMF) [1] extends traditional matrix\nfactorization by incorporating probability distributions over latent factors,\nallowing for uncertainty quantification. However, computing the posterior\ndistribution is intractable due to the high-dimensional integral. To address\nthis, we employ two Bayesian inference methods: Markov Chain Monte Carlo (MCMC)\n[2] and Variational Inference (VI) [3] to approximate the posterior. We\nevaluate their performance on MovieLens dataset and compare their convergence\nspeed, predictive accuracy, and computational efficiency. Experimental results\ndemonstrate that VI offers faster convergence, while MCMC provides more\naccurate posterior estimates.", "comment": "11 pages, 4 figures", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09928v1", "AI": {"title_translation": "贝叶斯概率矩阵分解", "tldr": "论文探讨了使用MCMC和VI两种贝叶斯推断方法来解决概率矩阵分解中后验分布计算困难的问题，发现VI收敛更快，MCMC估计更准确。", "motivation": "概率矩阵分解（PMF）在推荐系统中应用广泛，但其后验分布的计算因高维积分而难以处理。", "method": "采用马尔可夫链蒙特卡罗（MCMC）和变分推断（VI）两种贝叶斯推断方法来近似后验分布，并在MovieLens数据集上评估它们的性能，比较收敛速度、预测准确性和计算效率。", "result": "实验结果表明，变分推断（VI）收敛速度更快，而马尔可夫链蒙特卡罗（MCMC）提供了更准确的后验估计。", "conclusion": "VI和MCMC在解决PMF后验分布计算问题上各有优势，VI在速度上占优，MCMC在精度上更优。", "translation": "矩阵分解是推荐系统中广泛使用的技术。概率矩阵分解（PMF）[1]通过将概率分布引入潜在因子，扩展了传统的矩阵分解，从而可以量化不确定性。然而，由于高维积分，计算后验分布是难以处理的。为了解决这个问题，我们采用了两种贝叶斯推断方法：马尔可夫链蒙特卡罗（MCMC）[2]和变分推断（VI）[3]来近似后验。我们在MovieLens数据集上评估了它们的性能，并比较了它们的收敛速度、预测准确性和计算效率。实验结果表明，VI提供了更快的收敛速度，而MCMC提供了更准确的后验估计。", "summary": "本文研究了概率矩阵分解（PMF）中后验分布计算困难的问题。为了解决这一难题，作者采用了马尔可夫链蒙特卡罗（MCMC）和变分推断（VI）两种贝叶斯推断方法进行近似。通过在MovieLens数据集上的实验，论文比较了两种方法的收敛速度、预测准确性和计算效率，发现VI收敛更快，而MCMC在后验估计上更为准确。", "keywords": "贝叶斯推断, 概率矩阵分解, 马尔可夫链蒙特卡罗, 变分推断, 推荐系统", "comments": "这篇论文对概率矩阵分解中贝叶斯推断的两种主流方法MCMC和VI进行了对比研究。其创新点在于直接对比了两种方法在实际数据集上的性能差异，揭示了它们在收敛速度和估计精度上的权衡。对于需要应用PMF的实际系统而言，这项研究提供了有价值的参考，帮助决策者根据具体需求选择合适的推断方法。"}}
{"id": "2506.09958", "title": "Kvasir-VQA-x1: A Multimodal Dataset for Medical Reasoning and Robust MedVQA in Gastrointestinal Endoscopy", "authors": ["Sushant Gautam", "Michael A. Riegler", "Pål Halvorsen"], "summary": "Medical Visual Question Answering (MedVQA) is a promising field for\ndeveloping clinical decision support systems, yet progress is often limited by\nthe available datasets, which can lack clinical complexity and visual\ndiversity. To address these gaps, we introduce Kvasir-VQA-x1, a new,\nlarge-scale dataset for gastrointestinal (GI) endoscopy. Our work significantly\nexpands upon the original Kvasir-VQA by incorporating 159,549 new\nquestion-answer pairs that are designed to test deeper clinical reasoning. We\ndeveloped a systematic method using large language models to generate these\nquestions, which are stratified by complexity to better assess a model's\ninference capabilities. To ensure our dataset prepares models for real-world\nclinical scenarios, we have also introduced a variety of visual augmentations\nthat mimic common imaging artifacts. The dataset is structured to support two\nmain evaluation tracks: one for standard VQA performance and another to test\nmodel robustness against these visual perturbations. By providing a more\nchallenging and clinically relevant benchmark, Kvasir-VQA-x1 aims to accelerate\nthe development of more reliable and effective multimodal AI systems for use in\nclinical settings. The dataset is fully accessible and adheres to FAIR data\nprinciples, making it a valuable resource for the wider research community.\nCode and data: https://github.com/Simula/Kvasir-VQA-x1 and\nhttps://huggingface.co/datasets/SimulaMet/Kvasir-VQA-x1", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09958v1", "AI": {"title_translation": "Kvasir-VQA-x1: 一个用于胃肠道内窥镜检查中医学推理和鲁棒性MedVQA的多模态数据集", "tldr": "Kvasir-VQA-x1是一个新的、大规模的胃肠道内窥镜医学视觉问答数据集，旨在通过增加临床复杂性和视觉多样性来加速可靠的医疗AI系统开发。", "motivation": "现有的医学视觉问答（MedVQA）数据集缺乏临床复杂性和视觉多样性，限制了临床决策支持系统的发展。", "method": "引入Kvasir-VQA-x1数据集，扩展了原始Kvasir-VQA，新增159,549个问答对，这些问答对通过大型语言模型系统生成，并根据复杂性进行分层。同时引入多种模拟常见成像伪影的视觉增强。数据集支持标准VQA性能评估和模型对视觉扰动鲁棒性测试。", "result": "创建了Kvasir-VQA-x1数据集，包含159,549个新的问答对，旨在测试更深层次的临床推理能力，并引入了模仿常见成像伪影的视觉增强，以评估模型在真实临床场景中的鲁棒性。", "conclusion": "Kvasir-VQA-x1通过提供一个更具挑战性和临床相关性的基准，旨在加速开发用于临床环境的更可靠、更有效的多模态AI系统。该数据集完全可访问并遵循FAIR数据原则。", "translation": "医学视觉问答（MedVQA）是开发临床决策支持系统的一个有前景的领域，然而其进展常常受到现有数据集的限制，这些数据集可能缺乏临床复杂性和视觉多样性。为了弥补这些空白，我们引入了Kvasir-VQA-x1，这是一个新的、大规模的胃肠道（GI）内窥镜检查数据集。我们的工作在原有Kvasir-VQA的基础上进行了显著扩展，新增了159,549个旨在测试更深层次临床推理能力的问答对。我们开发了一种使用大型语言模型系统生成这些问题的系统方法，这些问题根据复杂性进行分层，以更好地评估模型的推理能力。为了确保我们的数据集能让模型为真实的临床场景做好准备，我们还引入了多种模拟常见成像伪影的视觉增强。该数据集的结构支持两种主要的评估轨道：一种用于标准VQA性能，另一种用于测试模型对抗这些视觉扰动的鲁棒性。通过提供一个更具挑战性和临床相关性的基准，Kvasir-VQA-x1旨在加速开发用于临床环境的更可靠、更有效的多模态AI系统。该数据集完全可访问并遵守FAIR数据原则，使其成为更广泛研究社区的宝贵资源。代码和数据：https://github.com/Simula/Kvasir-VQA-x1 和 https://huggingface.co/datasets/SimulaMet/Kvasir-VQA-x1", "summary": "本文介绍了Kvasir-VQA-x1，一个针对胃肠道内窥镜检查的大规模多模态医学视觉问答（MedVQA）数据集。该数据集通过新增大量基于大语言模型生成的复杂问答对以及模拟真实成像伪影的视觉增强，解决了现有MedVQA数据集在临床复杂性和视觉多样性方面的不足。Kvasir-VQA-x1旨在提供一个更具挑战性和临床相关性的基准，以推动开发更可靠、更有效的医疗AI系统。", "keywords": "医学视觉问答, 多模态数据集, 胃肠道内窥镜, 临床推理, 数据增强", "comments": "Kvasir-VQA-x1的创新之处在于其大规模扩展、引入基于LLM生成的复杂临床推理问题以及模拟真实世界成像伪影的视觉增强，这对于提升MedVQA模型在实际临床应用中的鲁棒性和泛化能力至关重要。该数据集的发布及其对FAIR原则的遵循，使其成为医学AI研究领域的重要贡献。"}}
{"id": "2506.09940", "title": "The Sample Complexity of Online Strategic Decision Making with Information Asymmetry and Knowledge Transportability", "authors": ["Jiachen Hu", "Rui Ai", "Han Zhong", "Xiaoyu Chen", "Liwei Wang", "Zhaoran Wang", "Zhuoran Yang"], "summary": "Information asymmetry is a pervasive feature of multi-agent systems,\nespecially evident in economics and social sciences. In these settings, agents\ntailor their actions based on private information to maximize their rewards.\nThese strategic behaviors often introduce complexities due to confounding\nvariables. Simultaneously, knowledge transportability poses another significant\nchallenge, arising from the difficulties of conducting experiments in target\nenvironments. It requires transferring knowledge from environments where\nempirical data is more readily available. Against these backdrops, this paper\nexplores a fundamental question in online learning: Can we employ non-i.i.d.\nactions to learn about confounders even when requiring knowledge transfer? We\npresent a sample-efficient algorithm designed to accurately identify system\ndynamics under information asymmetry and to navigate the challenges of\nknowledge transfer effectively in reinforcement learning, framed within an\nonline strategic interaction model. Our method provably achieves learning of an\n$\\epsilon$-optimal policy with a tight sample complexity of $O(1/\\epsilon^2)$.", "comment": "Accepted at ICML 2025", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09940v1", "AI": {"title_translation": "具有信息不对称和知识可迁移性的在线战略决策的样本复杂度", "tldr": "本文提出了一种样本高效算法，用于在信息不对称和知识可迁移性背景下，在线战略交互中学习系统动力学并实现最优策略，其样本复杂度为$O(1/\\epsilon^2)$。", "motivation": "多智能体系统中普遍存在信息不对称和战略行为导致混淆变量的复杂性。同时，知识可迁移性是一个重大挑战，因为难以在目标环境中进行实验，需要从数据更易获得的源环境迁移知识。本文旨在解决在线学习中，如何在需要知识迁移的情况下，利用非独立同分布（non-i.i.d.）行为学习混淆变量的问题。", "method": "提出了一种样本高效算法，旨在信息不对称下准确识别系统动力学，并在强化学习中有效应对知识迁移的挑战，其模型框架是在线战略交互。", "result": "该方法被证明能够学习到$\\epsilon$-最优策略，并具有紧密的样本复杂度$O(1/\\epsilon^2)$。", "conclusion": "本文成功提出了一个样本高效的算法，解决了在线学习中信息不对称和知识迁移的挑战，实现了在战略交互模型下学习最优策略并提供了理论上的样本复杂度保证。", "translation": "信息不对称是多智能体系统的一个普遍特征，在经济学和社会科学中尤为明显。在这些环境中，智能体根据私有信息调整其行动以最大化其回报。这些战略行为常常由于混淆变量而引入复杂性。同时，知识可迁移性带来了另一个重大挑战，它源于在目标环境中进行实验的困难。它需要从更容易获得经验数据的环境中迁移知识。在此背景下，本文探讨了在线学习中的一个基本问题：即使需要知识迁移，我们能否利用非独立同分布（non-i.i.d.）行为来学习混淆变量？我们提出了一种样本高效的算法，旨在信息不对称下准确识别系统动力学，并在强化学习中有效应对知识迁移的挑战，其框架是在线战略交互模型。我们的方法被证明能够学习到$\\epsilon$-最优策略，并具有紧密的样本复杂度$O(1/\\epsilon^2)$。", "summary": "本文研究了在线学习中信息不对称和知识可迁移性带来的挑战。针对这些复杂性，作者提出了一种样本高效的算法，能够在在线战略交互模型下，准确识别系统动力学并有效处理知识迁移问题。该算法被证明能够以$O(1/\\epsilon^2)$的紧密样本复杂度学习到$\\epsilon$-最优策略。", "keywords": "信息不对称, 知识可迁移性, 在线学习, 样本复杂度, 强化学习", "comments": "这篇论文的创新点在于将信息不对称和知识可迁移性这两个重要的挑战结合起来，并在在线战略决策的背景下进行研究。其提出的样本高效算法及其理论上的样本复杂度保证，对于理解和设计更鲁棒的多智能体学习系统具有重要意义。特别是在数据获取困难或环境动态变化的场景下，该方法可能提供有效的解决方案。"}}
{"id": "2506.09965", "title": "Reinforcing Spatial Reasoning in Vision-Language Models with Interwoven Thinking and Visual Drawing", "authors": ["Junfei Wu", "Jian Guan", "Kaituo Feng", "Qiang Liu", "Shu Wu", "Liang Wang", "Wei Wu", "Tieniu Tan"], "summary": "As textual reasoning with large language models (LLMs) has advanced\nsignificantly, there has been growing interest in enhancing the multimodal\nreasoning capabilities of large vision-language models (LVLMs). However,\nexisting methods primarily approach multimodal reasoning in a straightforward,\ntext-centric manner, where both reasoning and answer derivation are conducted\npurely through text, with the only difference being the presence of multimodal\ninput. As a result, these methods often encounter fundamental limitations in\nspatial reasoning tasks that demand precise geometric understanding and\ncontinuous spatial tracking-capabilities that humans achieve through mental\nvisualization and manipulation. To address the limitations, we propose drawing\nto reason in space, a novel paradigm that enables LVLMs to reason through\nelementary drawing operations in the visual space. By equipping models with\nbasic drawing operations, including annotating bounding boxes and drawing\nauxiliary lines, we empower them to express and analyze spatial relationships\nthrough direct visual manipulation, meanwhile avoiding the performance ceiling\nimposed by specialized perception tools in previous tool-integrated reasoning\napproaches. To cultivate this capability, we develop a three-stage training\nframework: cold-start training with synthetic data to establish basic drawing\nabilities, reflective rejection sampling to enhance self-reflection behaviors,\nand reinforcement learning to directly optimize for target rewards. Extensive\nexperiments demonstrate that our model, named VILASR, consistently outperforms\nexisting methods across diverse spatial reasoning benchmarks, involving maze\nnavigation, static spatial reasoning, video-based reasoning, and\nmulti-view-based reasoning tasks, with an average improvement of 18.4%.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09965v1", "AI": {"title_translation": "通过交织思考和视觉绘画增强视觉-语言模型的空间推理能力", "tldr": "本文提出了一种名为VILASR的新范式，允许视觉-语言模型通过基本绘图操作在视觉空间中进行空间推理，并在多项空间推理基准测试中取得了显著的性能提升。", "motivation": "现有的大型视觉-语言模型（LVLMs）在处理需要精确几何理解和连续空间跟踪的空间推理任务时，由于主要采用以文本为中心的方法，存在根本性局限。人类通过心理可视化和操作实现这些能力，而现有模型则难以做到。", "method": "本文提出了一种“通过绘画进行空间推理”的新范式，使LVLMs能够通过基本绘图操作（如标注边界框和绘制辅助线）在视觉空间中进行推理。为了培养这种能力，开发了一个三阶段训练框架：1) 使用合成数据进行冷启动训练以建立基本绘图能力；2) 反射式拒绝采样以增强自我反思行为；3) 强化学习以直接优化目标奖励。", "result": "本文提出的模型VILASR在多种空间推理基准测试中持续优于现有方法，包括迷宫导航、静态空间推理、基于视频的推理和基于多视图的推理任务，平均性能提升18.4%。", "conclusion": "通过引入“通过绘画进行空间推理”的范式和三阶段训练框架，本文成功地增强了视觉-语言模型在空间推理任务中的能力，证明了视觉操作在弥补现有模型局限性方面的有效性。", "translation": "随着大型语言模型（LLMs）的文本推理能力显著进步，人们对增强大型视觉-语言模型（LVLMs）的多模态推理能力产生了越来越浓厚的兴趣。然而，现有方法主要以直接的、以文本为中心的方式处理多模态推理，其中推理和答案推导纯粹通过文本进行，唯一的区别在于存在多模态输入。因此，这些方法在需要精确几何理解和连续空间跟踪的空间推理任务中经常遇到根本性局限——这些能力是人类通过心理可视化和操作实现的。为了解决这些局限性，我们提出了“通过绘画进行空间推理”这一新颖范式，它使LVLMs能够通过视觉空间中的基本绘图操作进行推理。通过为模型配备基本绘图操作，包括标注边界框和绘制辅助线，我们使它们能够通过直接的视觉操作表达和分析空间关系，同时避免了以往工具集成推理方法中专业感知工具带来的性能上限。为了培养这种能力，我们开发了一个三阶段训练框架：使用合成数据进行冷启动训练以建立基本绘图能力，反射式拒绝采样以增强自我反思行为，以及强化学习以直接优化目标奖励。广泛的实验表明，我们的模型VILASR在多种空间推理基准测试中持续优于现有方法，包括迷宫导航、静态空间推理、基于视频的推理和基于多视图的推理任务，平均性能提升18.4%。", "summary": "本文提出了一种新颖的范式，即“通过绘画进行空间推理”，旨在增强大型视觉-语言模型（LVLMs）在空间推理任务中的能力。针对现有文本中心方法在处理精确几何理解和连续空间跟踪方面的局限性，作者使LVLMs能够通过基本绘图操作（如边界框标注和辅助线绘制）直接在视觉空间中表达和分析空间关系。为培养此能力，开发了一个三阶段训练框架，包括冷启动训练、反射式拒绝采样和强化学习。实验结果表明，该模型VILASR在各种空间推理基准测试中均显著优于现有方法，平均性能提升18.4%。", "keywords": "空间推理, 视觉-语言模型, 视觉绘画, 多模态推理, 几何理解", "comments": "本文的创新之处在于提出了一种新颖的“通过绘画进行空间推理”范式，允许视觉-语言模型通过直接的视觉操作进行空间推理，这与人类通过心理可视化进行推理的方式更为相似。它避免了以往工具集成推理方法中专业感知工具带来的性能上限，为多模态推理开辟了新的途径。其三阶段训练框架也为培养这种复杂能力提供了有效的路径。"}}
{"id": "2506.09955", "title": "Canonical Latent Representations in Conditional Diffusion Models", "authors": ["Yitao Xu", "Tong Zhang", "Ehsan Pajouheshgar", "Sabine Süsstrunk"], "summary": "Conditional diffusion models (CDMs) have shown impressive performance across\na range of generative tasks. Their ability to model the full data distribution\nhas opened new avenues for analysis-by-synthesis in downstream discriminative\nlearning. However, this same modeling capacity causes CDMs to entangle the\nclass-defining features with irrelevant context, posing challenges to\nextracting robust and interpretable representations. To this end, we identify\nCanonical LAtent Representations (CLAReps), latent codes whose internal CDM\nfeatures preserve essential categorical information while discarding\nnon-discriminative signals. When decoded, CLAReps produce representative\nsamples for each class, offering an interpretable and compact summary of the\ncore class semantics with minimal irrelevant details. Exploiting CLAReps, we\ndevelop a novel diffusion-based feature-distillation paradigm, CaDistill. While\nthe student has full access to the training set, the CDM as teacher transfers\ncore class knowledge only via CLAReps, which amounts to merely 10 % of the\ntraining data in size. After training, the student achieves strong adversarial\nrobustness and generalization ability, focusing more on the class signals\ninstead of spurious background cues. Our findings suggest that CDMs can serve\nnot just as image generators but also as compact, interpretable teachers that\ncan drive robust representation learning.", "comment": "45 pages,41 figures", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09955v1", "AI": {"title_translation": "条件扩散模型中的规范潜在表示", "tldr": "本文引入了规范潜在表示（CLAReps）和基于扩散的特征蒸馏范式CaDistill，以帮助条件扩散模型（CDMs）提取鲁棒且可解释的特征，从而实现强大的对抗鲁棒性和泛化能力。", "motivation": "条件扩散模型（CDMs）在生成任务中表现出色，但其强大的建模能力也导致类定义特征与不相关上下文纠缠不清，使得提取鲁棒且可解释的表示面临挑战。", "method": "作者识别出规范潜在表示（CLAReps），这是一种潜在编码，其内部CDM特征能保留核心类别信息并去除无关信号。利用CLAReps，作者开发了一种新颖的基于扩散的特征蒸馏范式CaDistill，其中CDM作为教师仅通过CLAReps（数据量仅为训练集的10%）向学生模型传递核心类别知识。", "result": "经过训练后，学生模型获得了强大的对抗鲁棒性和泛化能力，并且更专注于类别信号而非虚假背景线索。", "conclusion": "研究结果表明，条件扩散模型不仅可以作为图像生成器，还可以作为紧凑、可解释的教师模型，推动鲁棒的表示学习。", "translation": "条件扩散模型（CDMs）在一系列生成任务中表现出色。它们对完整数据分布建模的能力为下游判别式学习中的合成分析开辟了新途径。然而，正是这种建模能力导致CDMs将类别定义特征与不相关上下文纠缠在一起，给提取鲁棒且可解释的表示带来了挑战。为此，我们识别出规范潜在表示（CLAReps），这是一种潜在编码，其内部CDM特征保留了必要的类别信息，同时丢弃了非判别性信号。当解码时，CLAReps为每个类别生成具有代表性的样本，以最少的无关细节提供了核心类别语义的可解释且紧凑的摘要。利用CLAReps，我们开发了一种新颖的基于扩散的特征蒸馏范式CaDistill。尽管学生模型可以完全访问训练集，但作为教师的CDM仅通过CLAReps传递核心类别知识，这仅占训练数据大小的10%。训练后，学生模型获得了强大的对抗鲁棒性和泛化能力，更侧重于类别信号而不是虚假的背景线索。我们的研究结果表明，CDMs不仅可以作为图像生成器，还可以作为紧凑、可解释的教师，推动鲁棒的表示学习。", "summary": "本文针对条件扩散模型（CDMs）在提取鲁棒和可解释特征时遇到的挑战，提出了规范潜在表示（CLAReps）。CLAReps能够从CDMs中提取出仅包含核心类别信息的潜在编码。在此基础上，作者开发了一种名为CaDistill的特征蒸馏范式，其中CDM作为教师，通过CLAReps向学生模型传递知识，即使数据量仅为训练集的10%，学生模型也能获得强大的对抗鲁棒性和泛化能力，并能更好地关注类别信号。这表明CDMs不仅能生成图像，还能作为有效的教师模型促进鲁棒表示学习。", "keywords": "条件扩散模型, 潜在表示, 特征蒸馏, 对抗鲁棒性, 可解释性", "comments": "这项研究的创新之处在于提出了CLAReps，它解决了CDMs在特征表示中纠缠无关上下文的问题，使得特征更具可解释性。CaDistill范式利用CLAReps进行高效的知识蒸馏，显著减少了教学数据量，同时提升了学生模型的鲁棒性和泛化能力，这对于数据效率和模型训练具有重要意义。该工作拓宽了CDMs的应用范围，从纯粹的生成任务扩展到作为表示学习的教师模型，具有很高的实用价值。"}}
{"id": "2506.09969", "title": "Vectorized Region Based Brush Strokes for Artistic Rendering", "authors": ["Jeripothula Prudviraj", "Vikram Jamwal"], "summary": "Creating a stroke-by-stroke evolution process of a visual artwork tries to\nbridge the emotional and educational gap between the finished static artwork\nand its creation process. Recent stroke-based painting systems focus on\ncapturing stroke details by predicting and iteratively refining stroke\nparameters to maximize the similarity between the input image and the rendered\noutput. However, these methods often struggle to produce stroke compositions\nthat align with artistic principles and intent. To address this, we explore an\nimage-to-painting method that (i) facilitates semantic guidance for brush\nstrokes in targeted regions, (ii) computes the brush stroke parameters, and\n(iii) establishes a sequence among segments and strokes to sequentially render\nthe final painting. Experimental results on various input image types, such as\nface images, paintings, and photographic images, show that our method aligns\nwith a region-based painting strategy while rendering a painting with high\nfidelity and superior stroke quality.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09969v1", "AI": {"title_translation": "艺术渲染的矢量化区域笔触", "tldr": "提出一种新的图像到绘画方法，通过区域语义引导和顺序渲染，生成符合艺术原则的高质量笔触绘画。", "motivation": "现有的笔触绘画系统难以生成符合艺术原则和意图的笔触构图，且在捕捉笔触细节方面存在局限性。", "method": "提出一种图像到绘画的方法，该方法 (i) 促进目标区域笔触的语义引导，(ii) 计算笔触参数，以及 (iii) 建立片段和笔触之间的序列以顺序渲染最终绘画。", "result": "在人脸图像、绘画和摄影图像等多种输入图像类型上的实验结果表明，该方法与基于区域的绘画策略一致，同时渲染出高保真度和卓越笔触质量的绘画。", "conclusion": "该方法通过引入区域语义引导和顺序渲染，有效解决了现有笔触绘画系统在生成符合艺术原则构图方面的不足，实现了高质量的艺术渲染。", "translation": "创建视觉艺术作品的逐笔演变过程试图弥合完成的静态艺术作品与其创作过程之间的情感和教育鸿沟。最近的基于笔触的绘画系统侧重于通过预测和迭代细化笔触参数来捕捉笔触细节，以最大化输入图像和渲染输出之间的相似性。然而，这些方法通常难以产生符合艺术原则和意图的笔触构图。为了解决这个问题，我们探索了一种图像到绘画的方法，该方法 (i) 促进目标区域笔触的语义引导，(ii) 计算笔触参数，以及 (iii) 建立片段和笔触之间的序列以顺序渲染最终绘画。在人脸图像、绘画和摄影图像等多种输入图像类型上的实验结果表明，我们的方法与基于区域的绘画策略一致，同时渲染出高保真度和卓越笔触质量的绘画。", "summary": "这篇论文提出了一种新的图像到绘画方法，旨在克服现有笔触绘画系统在生成符合艺术原则的构图方面的局限性。该方法通过为目标区域的笔触提供语义引导，计算笔触参数，并建立片段和笔触的渲染序列来实现。实验结果表明，该方法能够生成高保真度且笔触质量优异的区域化绘画。", "keywords": "艺术渲染, 笔触, 区域化, 图像到绘画, 语义引导", "comments": "这项工作通过引入区域语义引导和顺序渲染的概念，为艺术渲染提供了一个新颖的视角，有助于生成更具艺术性和结构感的绘画作品，弥补了现有方法在艺术性上的不足。"}}
{"id": "2506.09991", "title": "Multiverse: Your Language Models Secretly Decide How to Parallelize and Merge Generation", "authors": ["Xinyu Yang", "Yuwei An", "Hongyi Liu", "Tianqi Chen", "Beidi Chen"], "summary": "Autoregressive Large Language Models (AR-LLMs) frequently exhibit implicit\nparallelism in sequential generation. Inspired by this, we introduce\nMultiverse, a new generative model that enables natively parallel generation.\nMultiverse internalizes a MapReduce paradigm, generating automatically through\nthree stages: (i) a Map stage for adaptive task decomposition, (ii) a Process\nstage for parallel subtask execution, and (iii) a Reduce stage for lossless\nresult synthesis. Next, we build a real-world Multiverse reasoning model with\nco-design of data, algorithm, and system, enabling rapid and seamless transfer\nfrom frontier AR-LLMs. Starting from sequential reasoning chains, we create\nMultiverse 1K by converting them into structured training data using an\nautomated LLM-assisted pipeline, avoiding costly human annotations.\nAlgorithmically, we design Multiverse Attention to separate parallel reasoning\nsteps while keeping compatibility with causal attention for efficient training.\nSystematically, we implement Multiverse Engine to enable parallel inference. It\nfeatures a dedicated scheduler that dynamically switches between sequential and\nparallel generation, triggered directly by the model. After a 3-hour\nfine-tuning with 1K examples, our Multiverse-32B stands as the only\nopen-sourced non-AR model achieving performance on par with leading AR-LLMs of\nthe same scale, evidenced by AIME24 & 25 scores of 54% and 46%, respectively.\nMoreover, our budget control experiments show that Multiverse-32B exhibits\nsuperior scaling, outperforming AR-LLMs by 1.87% on average using the same\ncontext length. Such scaling further leads to practical efficiency gain,\nachieving up to 2x speedup across varying batch sizes. We have open-sourced the\nentire Multiverse ecosystem, including data, model weights, engine, supporting\ntools, as well as complete data curation prompts and detailed training and\nevaluation recipes.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09991v1", "AI": {"title_translation": "Multiverse: 你的语言模型秘密决定如何并行化和合并生成", "tldr": "Multiverse是一种新的非自回归语言模型，它通过MapReduce范式实现原生并行生成，性能可与领先的自回归模型媲美，并实现显著加速。", "motivation": "自回归大型语言模型（AR-LLMs）在顺序生成中频繁表现出隐式并行性，这启发了Multiverse的开发，旨在实现原生并行生成。", "method": "Multiverse通过内化MapReduce范式（Map、Process、Reduce三个阶段）实现自动生成。具体方法包括：通过自动化LLM辅助管道将顺序推理链转换为结构化训练数据（Multiverse 1K）；设计Multiverse Attention以分离并行推理步骤并兼容因果注意力；实现Multiverse Engine，其专用调度器能动态切换顺序和并行生成，实现并行推理。模型Multiverse-32B经过1K示例的3小时微调。", "result": "Multiverse-32B是唯一开源的非自回归模型，其性能与同等规模的领先AR-LLMs相当（AIME24和AIME25分数分别为54%和46%）。在相同的上下文长度下，Multiverse-32B的扩展性优于AR-LLMs，平均表现高出1.87%。实现高达2倍的速度提升。", "conclusion": "Multiverse证明了非自回归模型在并行生成方面的潜力，能够与自回归模型在性能上竞争，并提供显著的效率提升，为未来的语言模型设计开辟了新方向。", "translation": "自回归大型语言模型（AR-LLMs）在顺序生成中频繁表现出隐式并行性。受此启发，我们引入了Multiverse，这是一种新的生成模型，能够实现原生并行生成。Multiverse内化了一种MapReduce范式，通过三个阶段自动生成：(i) 用于自适应任务分解的Map阶段，(ii) 用于并行子任务执行的Process阶段，以及 (iii) 用于无损结果合成的Reduce阶段。接下来，我们通过数据、算法和系统的协同设计，构建了一个真实的Multiverse推理模型，实现了从前沿AR-LLMs的快速无缝迁移。从顺序推理链开始，我们通过使用自动化LLM辅助管道将其转换为结构化训练数据，创建了Multiverse 1K，从而避免了昂贵的人工标注。在算法层面，我们设计了Multiverse Attention，以分离并行推理步骤，同时保持与因果注意力的兼容性，从而实现高效训练。在系统层面，我们实现了Multiverse Engine，以实现并行推理。它具有一个专用的调度器，由模型直接触发，动态地在顺序生成和并行生成之间切换。经过1K个示例的3小时微调后，我们的Multiverse-32B是唯一开源的非自回归模型，其性能与同等规模的领先AR-LLMs相当，AIME24和AIME25分数分别为54%和46%。此外，我们的预算控制实验表明，Multiverse-32B表现出卓越的扩展性，在使用相同上下文长度的情况下，平均比AR-LLMs高出1.87%。这种扩展性进一步带来了实际的效率增益，在不同批次大小下实现了高达2倍的速度提升。我们已经开源了整个Multiverse生态系统，包括数据、模型权重、引擎、支持工具，以及完整的数据整理提示和详细的训练和评估配方。", "summary": "本文介绍了Multiverse，一种受AR-LLM隐式并行性启发的非自回归生成模型。Multiverse采用MapReduce范式进行并行生成，并结合了数据（Multiverse 1K）、算法（Multiverse Attention）和系统（Multiverse Engine）的协同设计。实验结果显示，经过少量微调的Multiverse-32B在性能上与领先的AR-LLMs相当，并展示出更优的扩展性和高达2倍的生成速度提升。该项目已全面开源。", "keywords": "并行生成, 大型语言模型, MapReduce, 非自回归模型, Multiverse", "comments": "Multiverse的创新之处在于其将MapReduce范式引入到语言模型生成中，实现了原生并行生成，有效解决了AR-LLM顺序生成效率低下的问题。通过数据、算法和系统的协同设计，它不仅在性能上达到了SOTA AR-LLM的水平，还在效率上实现了显著提升。其开源策略也极大地促进了社区对非自回归生成模型的研究和应用。"}}
{"id": "2506.09980", "title": "Efficient Part-level 3D Object Generation via Dual Volume Packing", "authors": ["Jiaxiang Tang", "Ruijie Lu", "Zhaoshuo Li", "Zekun Hao", "Xuan Li", "Fangyin Wei", "Shuran Song", "Gang Zeng", "Ming-Yu Liu", "Tsung-Yi Lin"], "summary": "Recent progress in 3D object generation has greatly improved both the quality\nand efficiency. However, most existing methods generate a single mesh with all\nparts fused together, which limits the ability to edit or manipulate individual\nparts. A key challenge is that different objects may have a varying number of\nparts. To address this, we propose a new end-to-end framework for part-level 3D\nobject generation. Given a single input image, our method generates\nhigh-quality 3D objects with an arbitrary number of complete and semantically\nmeaningful parts. We introduce a dual volume packing strategy that organizes\nall parts into two complementary volumes, allowing for the creation of complete\nand interleaved parts that assemble into the final object. Experiments show\nthat our model achieves better quality, diversity, and generalization than\nprevious image-based part-level generation methods.", "comment": "Code: https://github.com/NVlabs/PartPacker Project Page:\n  https://research.nvidia.com/labs/dir/partpacker/", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09980v1", "AI": {"title_translation": "高效的部件级3D物体生成通过双体积打包", "tldr": "提出一种新的端到端框架，利用双体积打包策略从单张图片生成高质量、可编辑的部件级3D物体。", "motivation": "现有3D物体生成方法通常生成融合所有部件的单一网格，限制了对单个部件的编辑和操作能力。此外，不同物体部件数量可变也是一个挑战。", "method": "提出一个端到端的部件级3D物体生成框架。该方法从单张输入图像生成具有任意数量完整且语义有意义部件的高质量3D物体。引入了一种双体积打包策略，将所有部件组织成两个互补的体积，从而创建完整且交错的部件并组装成最终物体。", "result": "实验表明，该模型在质量、多样性和泛化能力方面优于以前的基于图像的部件级生成方法。", "conclusion": "该研究成功开发了一种能够从单张图像生成高质量、可编辑的部件级3D物体的新框架，通过其创新的双体积打包策略克服了现有方法的局限性。", "translation": "近期3D物体生成方面的进展极大地提高了质量和效率。然而，大多数现有方法生成的是所有部件融合在一起的单一网格，这限制了编辑或操作单个部件的能力。一个关键挑战是不同物体可能具有不同数量的部件。为了解决这个问题，我们提出了一种新的端到端部件级3D物体生成框架。给定一张单一的输入图像，我们的方法可以生成具有任意数量完整且语义有意义部件的高质量3D物体。我们引入了一种双体积打包策略，将所有部件组织成两个互补的体积，从而创建完整且交错的部件，并组装成最终物体。实验表明，我们的模型在质量、多样性和泛化能力方面优于以前的基于图像的部件级生成方法。", "summary": "这篇论文提出了一种名为“双体积打包”的新型端到端框架，用于从单张图像生成高质量的部件级3D物体。针对现有方法生成单一融合网格且难以编辑单个部件的局限性，该方法通过将部件组织成两个互补体积来解决部件数量可变的问题，从而实现对完整和语义有意义部件的生成和组装。实验证明其在质量、多样性和泛化性方面优于现有方法。", "keywords": "3D物体生成, 部件级, 双体积打包, 深度学习, 图像到3D", "comments": "这篇论文的创新点在于提出了“双体积打包”策略，有效地解决了3D物体生成中部件数量可变和单个部件可编辑性的挑战。这对于需要精细控制和操作3D模型（如CAD、动画或虚拟现实）的应用具有重要意义。"}}
{"id": "2506.09998", "title": "Flipping Against All Odds: Reducing LLM Coin Flip Bias via Verbalized Rejection Sampling", "authors": ["Tim Z. Xiao", "Johannes Zenn", "Zhen Liu", "Weiyang Liu", "Robert Bamler", "Bernhard Schölkopf"], "summary": "Large language models (LLMs) can often accurately describe probability\ndistributions using natural language, yet they still struggle to generate\nfaithful samples from them. This mismatch limits their use in tasks requiring\nreliable stochasticity, such as Monte Carlo methods, agent-based simulations,\nand randomized decision-making. We investigate this gap between knowledge and\nsampling in the context of Bernoulli distributions. We introduce Verbalized\nRejection Sampling (VRS), a natural-language adaptation of classical rejection\nsampling that prompts the LLM to reason about and accept or reject proposed\nsamples. Despite relying on the same Bernoulli mechanism internally, VRS\nsubstantially reduces sampling bias across models. We provide theoretical\nanalysis showing that, under mild assumptions, VRS improves over direct\nsampling, with gains attributable to both the algorithm and prompt design. More\nbroadly, our results show how classical probabilistic tools can be verbalized\nand embedded into LLM workflows to improve reliability, without requiring\naccess to model internals or heavy prompt engineering.", "comment": "Technical Report v1 (21 pages, 14 figures)", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09998v1", "AI": {"title_translation": "逆势而行：通过口头化拒绝采样减少大型语言模型抛硬币偏差", "tldr": "大型语言模型（LLM）在准确的概率采样方面存在困难，尽管它们能理解分布。本文提出了一种新的方法——口头化拒绝采样（VRS），通过让LLM对样本进行推理和接受/拒绝来减少采样偏差，从而提高可靠性。", "motivation": "大型语言模型（LLM）虽然能用自然语言准确描述概率分布，但在生成忠实样本时仍面临困难。这种知识与采样之间的不匹配限制了LLM在需要可靠随机性的任务中的应用，例如蒙特卡洛方法、基于代理的模拟和随机决策。", "method": "本文引入了口头化拒绝采样（VRS），这是一种经典拒绝采样的自然语言改编，它提示LLM对提议的样本进行推理并接受或拒绝。研究还提供了理论分析来支持该方法。", "result": "VRS显著减少了跨模型的采样偏差，尽管其内部依赖于相同的伯努利机制。理论分析表明，在温和的假设下，VRS优于直接采样，其增益归因于算法和提示设计。", "conclusion": "经典概率工具可以通过口头化并嵌入到LLM工作流程中来提高其可靠性，而无需访问模型内部或进行大量的提示工程。", "translation": "大型语言模型（LLM）通常能够使用自然语言准确描述概率分布，但它们仍然难以从中生成忠实的样本。这种不匹配限制了它们在需要可靠随机性的任务中的使用，例如蒙特卡洛方法、基于代理的模拟和随机决策。我们在伯努利分布的背景下研究了知识与采样之间的这一差距。我们引入了口头化拒绝采样（VRS），这是一种经典拒绝采样的自然语言改编，它提示LLM对提议的样本进行推理并接受或拒绝。尽管内部依赖相同的伯努利机制，VRS仍能显著减少跨模型的采样偏差。我们提供了理论分析，表明在温和的假设下，VRS优于直接采样，其增益归因于算法和提示设计。更广泛地说，我们的结果表明经典概率工具如何能够被口头化并嵌入到LLM工作流程中以提高可靠性，而无需访问模型内部或进行大量的提示工程。", "summary": "大型语言模型（LLM）在准确的概率采样方面存在困难，尽管它们能理解分布。本文引入了口头化拒绝采样（VRS），一种经典拒绝采样的自然语言改编方法。VRS通过提示LLM对样本进行推理并接受或拒绝，显著减少了采样偏差。理论分析证实了其有效性，表明了如何通过口头化经典概率工具来增强LLM的可靠性，而无需复杂的工程。", "keywords": "LLM, 采样偏差, 拒绝采样, 伯努利分布, 概率推理", "comments": "这篇论文提供了一种创新的方法来改善LLM的概率采样能力，通过将经典的统计方法改编为自然语言提示，证明了外部推理机制可以在不改变模型架构或进行大量微调的情况下减轻内部模型偏差。这种方法对于需要LLM提供可靠随机性的应用具有重要意义。"}}
{"id": "2505.14156", "title": "Unify Graph Learning with Text: Unleashing LLM Potentials for Session Search", "authors": ["Songhao Wu", "Quan Tu", "Hong Liu", "Jia Xu", "Zhongyi Liu", "Guannan Zhang", "Ran Wang", "Xiuying Chen", "Rui Yan"], "summary": "Session search involves a series of interactive queries and actions to\nfulfill user's complex information need. Current strategies typically\nprioritize sequential modeling for deep semantic understanding, overlooking the\ngraph structure in interactions. While some approaches focus on capturing\nstructural information, they use a generalized representation for documents,\nneglecting the word-level semantic modeling. In this paper, we propose Symbolic\nGraph Ranker (SGR), which aims to take advantage of both text-based and\ngraph-based approaches by leveraging the power of recent Large Language Models\n(LLMs). Concretely, we first introduce a set of symbolic grammar rules to\nconvert session graph into text. This allows integrating session history,\ninteraction process, and task instruction seamlessly as inputs for the LLM.\nMoreover, given the natural discrepancy between LLMs pre-trained on textual\ncorpora, and the symbolic language we produce using our graph-to-text grammar,\nour objective is to enhance LLMs' ability to capture graph structures within a\ntextual format. To achieve this, we introduce a set of self-supervised symbolic\nlearning tasks including link prediction, node content generation, and\ngenerative contrastive learning, to enable LLMs to capture the topological\ninformation from coarse-grained to fine-grained. Experiment results and\ncomprehensive analysis on two benchmark datasets, AOL and Tiangong-ST, confirm\nthe superiority of our approach. Our paradigm also offers a novel and effective\nmethodology that bridges the gap between traditional search strategies and\nmodern LLMs.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2505.14156v1", "AI": {"title_translation": "将图学习与文本统一：释放LLM在会话搜索中的潜力", "tldr": "该论文提出了SGR，通过将会话图转换为文本，利用LLM来统一图学习和文本理解，以改进会话搜索。", "motivation": "当前会话搜索策略要么侧重于序列建模而忽略图结构，要么捕获结构信息但忽略词级语义建模，未能有效结合两者的优势。", "method": "本文提出了Symbolic Graph Ranker (SGR)。首先，引入一套符号语法规则将会话图转换为文本，使LLM能够无缝整合会话历史、交互过程和任务指令。其次，为增强LLM在文本格式中捕获图结构的能力，引入了一组自监督符号学习任务，包括链接预测、节点内容生成和生成式对比学习，以捕获从粗粒度到细粒度的拓扑信息。", "result": "在AOL和Tiangong-ST两个基准数据集上的实验结果和综合分析证实了该方法的优越性。", "conclusion": "该范式提供了一种新颖有效的方法，弥合了传统搜索策略与现代LLM之间的鸿沟。", "translation": "会话搜索涉及一系列交互式查询和操作，以满足用户复杂的信息需求。当前策略通常优先考虑序列建模以进行深度语义理解，而忽略交互中的图结构。虽然有些方法侧重于捕获结构信息，但它们对文档使用通用表示，忽略了词级语义建模。在本文中，我们提出了符号图排序器（SGR），旨在通过利用大型语言模型（LLM）的力量，结合基于文本和基于图的方法的优势。具体而言，我们首先引入一套符号语法规则，将会话图转换为文本。这使得会话历史、交互过程和任务指令能够无缝地作为LLM的输入。此外，鉴于LLM在文本语料库上预训练与我们使用图到文本语法生成的符号语言之间存在天然差异，我们的目标是增强LLM在文本格式中捕获图结构的能力。为此，我们引入了一组自监督符号学习任务，包括链接预测、节点内容生成和生成式对比学习，以使LLM能够从粗粒度到细粒度捕获拓扑信息。在AOL和Tiangong-ST两个基准数据集上的实验结果和综合分析证实了我们方法的优越性。我们的范式还提供了一种新颖有效的方法，弥合了传统搜索策略与现代LLM之间的鸿沟。", "summary": "本文提出了一种名为Symbolic Graph Ranker (SGR) 的新方法，旨在将会话搜索中的图结构信息与文本语义理解相结合。SGR通过一套符号语法规则将会话图转化为文本，从而使大型语言模型（LLM）能够处理会话历史和任务指令。为弥补LLM在文本和图结构理解上的差异，SGR引入了自监督学习任务以增强LLM捕获拓扑信息的能力。实验结果表明SGR在会话搜索任务上表现优越，并为传统搜索与现代LLM的结合提供了新范式。", "keywords": "会话搜索, 图学习, 大型语言模型, 图到文本, 自监督学习", "comments": "该论文的创新点在于提出了将图结构转换为LLM可理解的文本格式，并通过自监督学习任务增强LLM对图结构的感知能力，有效弥合了图学习和文本理解的鸿沟，为会话搜索提供了新思路和有效方法。"}}
{"id": "2506.09982", "title": "AnimateAnyMesh: A Feed-Forward 4D Foundation Model for Text-Driven Universal Mesh Animation", "authors": ["Zijie Wu", "Chaohui Yu", "Fan Wang", "Xiang Bai"], "summary": "Recent advances in 4D content generation have attracted increasing attention,\nyet creating high-quality animated 3D models remains challenging due to the\ncomplexity of modeling spatio-temporal distributions and the scarcity of 4D\ntraining data. In this paper, we present AnimateAnyMesh, the first feed-forward\nframework that enables efficient text-driven animation of arbitrary 3D meshes.\nOur approach leverages a novel DyMeshVAE architecture that effectively\ncompresses and reconstructs dynamic mesh sequences by disentangling spatial and\ntemporal features while preserving local topological structures. To enable\nhigh-quality text-conditional generation, we employ a Rectified Flow-based\ntraining strategy in the compressed latent space. Additionally, we contribute\nthe DyMesh Dataset, containing over 4M diverse dynamic mesh sequences with text\nannotations. Experimental results demonstrate that our method generates\nsemantically accurate and temporally coherent mesh animations in a few seconds,\nsignificantly outperforming existing approaches in both quality and efficiency.\nOur work marks a substantial step forward in making 4D content creation more\naccessible and practical. All the data, code, and models will be open-released.", "comment": "Project Page: https://animateanymesh.github.io/AnimateAnyMesh/", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09982v1", "AI": {"title_translation": "AnimateAnyMesh: 一种用于文本驱动通用网格动画的前馈4D基础模型", "tldr": "AnimateAnyMesh是一个前馈4D模型，通过文本驱动实现任意3D网格的高效动画，解决了4D内容生成中的挑战。", "motivation": "当前4D内容生成面临挑战，高质量动画3D模型难以创建，原因在于建模时空分布的复杂性和4D训练数据的稀缺性。", "method": "提出了AnimateAnyMesh，一个前馈框架，用于文本驱动任意3D网格动画。它利用DyMeshVAE架构有效压缩和重建动态网格序列，解耦时空特征并保留局部拓扑结构。在压缩的潜在空间中采用基于Rectified Flow的训练策略。同时，贡献了包含400多万条多样化动态网格序列及其文本注释的DyMesh数据集。", "result": "该方法能在几秒钟内生成语义准确且时间连贯的网格动画，在质量和效率上显著优于现有方法。", "conclusion": "这项工作在使4D内容创建更易于访问和实用方面迈出了实质性的一步。", "translation": "4D内容生成领域的最新进展引起了越来越多的关注，然而，由于建模时空分布的复杂性以及4D训练数据的稀缺性，创建高质量的动画3D模型仍然具有挑战性。在本文中，我们提出了AnimateAnyMesh，这是第一个前馈框架，能够实现任意3D网格的高效文本驱动动画。我们的方法利用了一种新颖的DyMeshVAE架构，通过解耦空间和时间特征同时保留局部拓扑结构，有效地压缩和重建动态网格序列。为了实现高质量的文本条件生成，我们在压缩的潜在空间中采用了基于Rectified Flow的训练策略。此外，我们贡献了DyMesh数据集，其中包含超过400万条多样化的动态网格序列及文本注释。实验结果表明，我们的方法能够在几秒钟内生成语义准确且时间连贯的网格动画，在质量和效率上显著优于现有方法。我们的工作标志着在使4D内容创建更易于访问和实用方面迈出了实质性的一步。所有数据、代码和模型都将开源发布。", "summary": "AnimateAnyMesh是一个创新的前馈4D基础模型，旨在解决高质量4D内容生成中时空建模复杂性和数据稀缺的问题。它引入了DyMeshVAE架构来高效压缩和重建动态网格，并通过Rectified Flow策略实现文本条件生成。该研究还贡献了一个大型DyMesh数据集。实验证明，AnimateAnyMesh能快速生成高质量、语义准确且时间连贯的网格动画，显著优于现有方法，极大地推动了4D内容创作的普及和实用性。", "keywords": "4D内容生成, 网格动画, 文本驱动, 深度学习, 动态网格", "comments": "这篇论文的创新点在于提出了一个前馈的4D基础模型AnimateAnyMesh，首次实现了文本驱动的任意3D网格动画，解决了4D内容生成中的关键挑战。DyMeshVAE架构在解耦时空特征和保留拓扑结构方面具有创新性。此外，构建并开源大规模DyMesh数据集对推动该领域的发展具有重要意义。其高效和高质量的动画生成能力预示着在游戏、VR/AR和影视制作等领域的巨大应用潜力。"}}
{"id": "2506.06905", "title": "Meta-Adaptive Prompt Distillation for Few-Shot Visual Question Answering", "authors": ["Akash Gupta", "Amos Storkey", "Mirella Lapata"], "summary": "Large Multimodal Models (LMMs) often rely on in-context learning (ICL) to\nperform new tasks with minimal supervision. However, ICL performance,\nespecially in smaller LMMs, is inconsistent and does not always improve\nmonotonically with increasing examples. We hypothesize that this occurs due to\nthe LMM being overwhelmed by additional information present in the image\nembeddings, which is not required for the downstream task. To address this, we\npropose a meta-learning approach that provides an alternative for inducing\nfew-shot capabilities in LMMs, using a fixed set of soft prompts that are\ndistilled from task-relevant image features and can be adapted at test time\nusing a few examples. To facilitate this distillation, we introduce an\nattention-mapper module that can be easily integrated with the popular LLaVA\nv1.5 architecture and is jointly learned with soft prompts, enabling task\nadaptation in LMMs under low-data regimes with just a few gradient steps.\nEvaluation on the VL-ICL Bench shows that our method consistently outperforms\nICL and related prompt-tuning approaches, even under image perturbations,\nimproving task induction and reasoning across visual question answering tasks.", "comment": null, "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.06905v2", "AI": {"title_translation": "面向少样本视觉问答的元自适应提示蒸馏", "tldr": "本文提出了一种元学习方法，通过蒸馏任务相关图像特征来生成可适应的软提示，以解决大型多模态模型（LMMs）在少样本视觉问答中情境学习性能不佳的问题，并在VL-ICL基准测试中表现优异。", "motivation": "大型多模态模型（LMMs）的情境学习（ICL）在少样本任务中表现不稳定，尤其在较小的LMMs中，且性能不随示例数量增加而单调提升。研究者推测这是由于LMMs被下游任务不需要的图像嵌入中的额外信息所淹没。", "method": "本文提出了一种元学习方法，通过从任务相关的图像特征中蒸馏出一组固定的软提示，并在测试时利用少量示例进行适应。为了促进蒸馏，引入了一个注意力映射模块，该模块可以轻松集成到流行的LLaVA v1.5架构中，并与软提示共同学习，使得LMMs在低数据条件下只需少量梯度步长即可实现任务适应。", "result": "在VL-ICL基准测试上的评估表明，本文方法持续优于情境学习（ICL）和相关的提示调优方法，即使在图像扰动下也是如此，显著改善了视觉问答任务中的任务引导和推理能力。", "conclusion": "通过引入元自适应提示蒸馏方法，本文有效地解决了LMMs在少样本视觉问答中情境学习的局限性，显著提升了模型在低数据条件下的任务适应性和推理能力。", "translation": "大型多模态模型（LMMs）通常依赖情境学习（ICL）来以最少的监督执行新任务。然而，ICL的性能，尤其是在较小的LMMs中，表现不稳定，并且不总是随着示例的增加而单调提高。我们假设这是由于LMM被图像嵌入中存在的额外信息所淹没，而这些信息对于下游任务来说并非必需。为了解决这个问题，我们提出了一种元学习方法，它为LMMs在诱导少样本能力方面提供了一种替代方案，使用一组固定的软提示，这些提示从任务相关的图像特征中蒸馏而来，并且可以在测试时使用少量示例进行适应。为了促进这种蒸馏，我们引入了一个注意力映射模块，该模块可以轻松集成到流行的LLaVA v1.5架构中，并与软提示共同学习，从而使LMMs在低数据条件下只需少量梯度步长即可实现任务适应。在VL-ICL基准测试上的评估表明，我们的方法持续优于ICL和相关的提示调优方法，即使在图像扰动下也是如此，改善了视觉问答任务中的任务引导和推理能力。", "summary": "针对大型多模态模型（LMMs）在少样本视觉问答（VQA）中情境学习（ICL）表现不佳的问题，本文提出了一种元自适应提示蒸馏方法。该方法通过元学习从任务相关图像特征中蒸馏出固定软提示，并结合一个可集成到LLaVA v1.5的注意力映射模块，使模型能在低数据条件下快速适应新任务。实验证明，该方法在VL-ICL基准测试上显著优于现有ICL和提示调优方法，提升了VQA任务的推理能力。", "keywords": "元学习, 提示蒸馏, 少样本学习, 视觉问答, 大型多模态模型", "comments": "本文提出了一种新颖的元自适应提示蒸馏方法，有效解决了LMMs在少样本场景下ICL性能不稳定的核心问题。其创新点在于将元学习与提示蒸馏相结合，并通过引入注意力映射模块，实现了对任务相关特征的有效提取和提示的自适应调整。这对于提升LMMs在低数据条件下的泛化能力和实际应用价值具有重要意义。"}}
{"id": "2506.09987", "title": "A Shortcut-aware Video-QA Benchmark for Physical Understanding via Minimal Video Pairs", "authors": ["Benno Krojer", "Mojtaba Komeili", "Candace Ross", "Quentin Garrido", "Koustuv Sinha", "Nicolas Ballas", "Mahmoud Assran"], "summary": "Existing benchmarks for assessing the spatio-temporal understanding and\nreasoning abilities of video language models are susceptible to score inflation\ndue to the presence of shortcut solutions based on superficial visual or\ntextual cues. This paper mitigates the challenges in accurately assessing model\nperformance by introducing the Minimal Video Pairs (MVP) benchmark, a simple\nshortcut-aware video QA benchmark for assessing the physical understanding of\nvideo language models. The benchmark is comprised of 55K high-quality\nmultiple-choice video QA examples focusing on physical world understanding.\nExamples are curated from nine video data sources, spanning first-person\negocentric and exocentric videos, robotic interaction data, and cognitive\nscience intuitive physics benchmarks. To mitigate shortcut solutions that rely\non superficial visual or textual cues and biases, each sample in MVP has a\nminimal-change pair -- a visually similar video accompanied by an identical\nquestion but an opposing answer. To answer a question correctly, a model must\nprovide correct answers for both examples in the minimal-change pair; as such,\nmodels that solely rely on visual or textual biases would achieve below random\nperformance. Human performance on MVP is 92.9\\%, while the best open-source\nstate-of-the-art video-language model achieves 40.2\\% compared to random\nperformance at 25\\%.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09987v1", "AI": {"title_translation": "一个通过最小视频对实现物理理解的快捷方式感知视频问答基准", "tldr": "本文提出了MVP基准，一个快捷方式感知的视频问答基准，通过“最小变化对”来更准确地评估视频语言模型对物理世界的理解，揭示了现有模型的局限性。", "motivation": "现有评估视频语言模型时空理解和推理能力的基准容易因基于表面视觉或文本线索的捷径解决方案而导致分数虚高，从而难以准确评估模型性能。", "method": "本文引入了最小视频对（MVP）基准，这是一个简单的、快捷方式感知的视频问答基准，用于评估视频语言模型的物理理解能力。该基准包含5.5万个高质量的多项选择视频问答示例，重点关注物理世界理解，这些示例来源于九个视频数据源。为缓解依赖表面视觉或文本线索及偏差的捷径解决方案，MVP中的每个样本都包含一个“最小变化对”——一个视觉相似的视频，配以相同的问题但答案相反。模型必须对最小变化对中的两个示例都提供正确答案才能被视为正确。", "result": "人类在MVP上的表现为92.9%，而表现最佳的开源最先进视频语言模型得分仅为40.2%，相比之下随机表现为25%。", "conclusion": "MVP基准通过引入“最小变化对”有效缓解了现有视频问答基准中快捷方式解决方案导致的分数虚高问题，更准确地揭示了当前视频语言模型在物理世界理解方面的不足，表明现有模型仍远低于人类水平。", "translation": "现有用于评估视频语言模型时空理解和推理能力的基准容易因基于表面视觉或文本线索的捷径解决方案而导致分数虚高。本文通过引入最小视频对（MVP）基准——一个简单的、快捷方式感知的视频问答基准，用于评估视频语言模型的物理理解能力，从而缓解了准确评估模型性能的挑战。该基准由5.5万个高质量的多项选择视频问答示例组成，重点关注物理世界理解。这些示例来源于九个视频数据源，涵盖了第一人称自我中心和第三人称异中心视频、机器人交互数据以及认知科学直觉物理基准。为了缓解依赖表面视觉或文本线索和偏差的捷径解决方案，MVP中的每个样本都包含一个最小变化对——一个视觉相似的视频，配以相同的问题但答案相反。为了正确回答一个问题，模型必须对最小变化对中的两个示例都提供正确答案；因此，仅依赖视觉或文本偏差的模型将获得低于随机的性能。人类在MVP上的表现为92.9%，而表现最佳的开源最先进视频语言模型实现了40.2%，相比之下随机表现为25%。", "summary": "本文提出了最小视频对（MVP）基准，这是一个快捷方式感知的视频问答基准，旨在通过“最小变化对”来更准确地评估视频语言模型对物理世界的理解能力，从而避免模型依赖表面视觉或文本线索的捷径。MVP包含5.5万个高质量示例，涵盖多种视频来源。实验结果显示，人类在MVP上的表现远超现有最佳视频语言模型，突显了当前模型在物理理解方面的局限性。", "keywords": "视频问答, 物理理解, 快捷方式感知, 最小视频对, 视频语言模型", "comments": "MVP基准的创新之处在于其“最小变化对”设计，这有效地解决了现有视频QA基准中普遍存在的快捷方式问题，迫使模型进行更深层次的物理推理而非依赖表面线索。这对于准确评估和推动视频语言模型在物理理解领域的发展具有重要意义。"}}
{"id": "2506.09988", "title": "EditInspector: A Benchmark for Evaluation of Text-Guided Image Edits", "authors": ["Ron Yosef", "Moran Yanuka", "Yonatan Bitton", "Dani Lischinski"], "summary": "Text-guided image editing, fueled by recent advancements in generative AI, is\nbecoming increasingly widespread. This trend highlights the need for a\ncomprehensive framework to verify text-guided edits and assess their quality.\nTo address this need, we introduce EditInspector, a novel benchmark for\nevaluation of text-guided image edits, based on human annotations collected\nusing an extensive template for edit verification. We leverage EditInspector to\nevaluate the performance of state-of-the-art (SoTA) vision and language models\nin assessing edits across various dimensions, including accuracy, artifact\ndetection, visual quality, seamless integration with the image scene, adherence\nto common sense, and the ability to describe edit-induced changes. Our findings\nindicate that current models struggle to evaluate edits comprehensively and\nfrequently hallucinate when describing the changes. To address these\nchallenges, we propose two novel methods that outperform SoTA models in both\nartifact detection and difference caption generation.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09988v1", "AI": {"title_translation": "EditInspector: 文本引导图像编辑评估基准", "tldr": "该论文引入了EditInspector，一个用于评估文本引导图像编辑的人工标注基准。研究发现当前模型在全面评估编辑和描述变化方面存在不足，并提出了两种在伪影检测和差异描述生成方面优于现有模型的新方法。", "motivation": "生成式AI的最新进展使得文本引导图像编辑日益普及，因此需要一个全面的框架来验证这些编辑并评估其质量。", "method": "作者引入了EditInspector，这是一个基于人工标注的新型基准，用于评估文本引导图像编辑。他们利用EditInspector评估了最先进（SoTA）的视觉和语言模型在多个维度上的性能。为了解决发现的挑战，他们提出了两种新颖的方法。", "result": "研究发现，当前模型难以全面评估编辑，并且在描述变化时经常产生幻觉。论文提出的两种新颖方法在伪影检测和差异描述生成方面均优于SoTA模型。", "conclusion": "现有模型不足以全面评估文本引导图像编辑，而论文提出的新方法在关键评估方面表现出改进的性能。", "translation": "在生成式AI最新进展的推动下，文本引导的图像编辑正变得越来越普及。这一趋势凸显了对一个全面框架的需求，以验证文本引导的编辑并评估其质量。为了满足这一需求，我们引入了EditInspector，这是一个用于评估文本引导图像编辑的新型基准，它基于使用广泛的编辑验证模板收集的人工标注。我们利用EditInspector来评估最先进（SoTA）的视觉和语言模型在评估编辑方面（包括准确性、伪影检测、视觉质量、与图像场景的无缝集成、对常识的遵循以及描述编辑引起的变化的能力）的性能。我们的发现表明，当前模型难以全面评估编辑，并且在描述变化时经常产生幻觉。为了解决这些挑战，我们提出了两种新颖的方法，它们在伪影检测和差异描述生成方面均优于SoTA模型。", "summary": "本论文介绍了EditInspector，一个新颖的人工标注基准，旨在全面评估文本引导图像编辑。研究利用EditInspector评估了最先进的模型，揭示了它们在全面评估编辑和准确描述变化方面的局限性。为克服这些问题，论文提出了两种新颖方法，在伪影检测和生成差异描述方面表现出优于现有模型的性能。", "keywords": "文本引导图像编辑, 基准, 评估, 人工标注, 生成式AI", "comments": "该论文通过提供一个急需的基准来评估文本引导图像编辑，解决了生成式AI领域快速发展中的一个关键需求。发现当前最先进模型存在的不足，凸显了建立鲁棒评估指标的重要性，而论文提出的方法则提供了有前景的进展。"}}
{"id": "2506.09566", "title": "From Symbolic to Neural and Back: Exploring Knowledge Graph-Large Language Model Synergies", "authors": ["Blaž Škrlj", "Boshko Koloski", "Senja Pollak", "Nada Lavrač"], "summary": "Integrating structured knowledge from Knowledge Graphs (KGs) into Large\nLanguage Models (LLMs) enhances factual grounding and reasoning capabilities.\nThis survey paper systematically examines the synergy between KGs and LLMs,\ncategorizing existing approaches into two main groups: KG-enhanced LLMs, which\nimprove reasoning, reduce hallucinations, and enable complex question\nanswering; and LLM-augmented KGs, which facilitate KG construction, completion,\nand querying. Through comprehensive analysis, we identify critical gaps and\nhighlight the mutual benefits of structured knowledge integration. Compared to\nexisting surveys, our study uniquely emphasizes scalability, computational\nefficiency, and data quality. Finally, we propose future research directions,\nincluding neuro-symbolic integration, dynamic KG updating, data reliability,\nand ethical considerations, paving the way for intelligent systems capable of\nmanaging more complex real-world knowledge tasks.", "comment": "To-appear as a book chapter", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.09566v1", "AI": {"title_translation": "从符号到神经再到符号：探索知识图谱-大型语言模型协同作用", "tldr": "本综述探讨了知识图谱（KGs）和大型语言模型（LLMs）的协同作用，将其分为KG增强LLM和LLM增强KG两类，并指出了未来的研究方向。", "motivation": "整合知识图谱中的结构化知识到大型语言模型中，以增强事实基础和推理能力，并解决现有研究的不足（例如，现有综述没有强调可扩展性、计算效率和数据质量）。", "method": "本文是一篇综述论文，系统地考察了知识图谱和大型语言模型之间的协同作用，将现有方法分为KG增强LLM和LLM增强KG两大类，并进行了全面分析。", "result": "识别了关键空白，强调了结构化知识整合的相互益处，并强调了与其他综述相比，本文独特地关注可扩展性、计算效率和数据质量。", "conclusion": "提出了未来的研究方向，包括神经符号整合、动态KG更新、数据可靠性和伦理考量，为能够管理更复杂现实世界知识任务的智能系统铺平道路。", "translation": "整合知识图谱（KGs）中的结构化知识到大型语言模型（LLMs）中，可以增强事实基础和推理能力。本综述系统地考察了KGs和LLMs之间的协同作用，将现有方法分为两大类：KG增强LLMs，它们提高了推理能力、减少了幻觉并支持复杂问答；以及LLM增强KGs，它们促进了KG的构建、补全和查询。通过全面分析，我们识别了关键空白并强调了结构化知识整合的相互益处。与现有综述相比，我们的研究独特地强调了可扩展性、计算效率和数据质量。最后，我们提出了未来的研究方向，包括神经符号整合、动态KG更新、数据可靠性和伦理考量，为能够管理更复杂现实世界知识任务的智能系统铺平道路。", "summary": "本综述系统地探讨了知识图谱（KGs）与大型语言模型（LLMs）之间的协同作用。论文将现有方法分为KG增强LLM（提升LLM推理和减少幻觉）和LLM增强KG（促进KG构建和查询）两类。研究强调了结构化知识整合的相互益处，并指出其独特性在于关注可扩展性、计算效率和数据质量，最终提出了未来的研究方向。", "keywords": "知识图谱, 大型语言模型, 协同作用, 神经符号整合, 综述", "comments": "这篇综述提供了一个全面的视角，涵盖了知识图谱和大型语言模型如何相互促进。其创新之处在于不仅分类了现有方法，还特别强调了可扩展性、计算效率和数据质量这些在实际应用中至关重要的方面，并提出了前瞻性的研究方向，特别是神经符号整合，这对于构建更鲁棒和智能的系统具有重要意义。"}}
{"id": "2506.09989", "title": "Hearing Hands: Generating Sounds from Physical Interactions in 3D Scenes", "authors": ["Yiming Dou", "Wonseok Oh", "Yuqing Luo", "Antonio Loquercio", "Andrew Owens"], "summary": "We study the problem of making 3D scene reconstructions interactive by asking\nthe following question: can we predict the sounds of human hands physically\ninteracting with a scene? First, we record a video of a human manipulating\nobjects within a 3D scene using their hands. We then use these action-sound\npairs to train a rectified flow model to map 3D hand trajectories to their\ncorresponding audio. At test time, a user can query the model for other\nactions, parameterized as sequences of hand poses, to estimate their\ncorresponding sounds. In our experiments, we find that our generated sounds\naccurately convey material properties and actions, and that they are often\nindistinguishable to human observers from real sounds. Project page:\nhttps://www.yimingdou.com/hearing_hands/", "comment": "CVPR 2025, Project page: https://www.yimingdou.com/hearing_hands/ ,\n  Code: https://github.com/Dou-Yiming/hearing_hands/", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09989v1", "AI": {"title_translation": "听觉之手：从3D场景中的物理交互生成声音", "tldr": "该研究提出了一种方法，通过训练一个整流流模型，从3D手部轨迹预测人类手部与3D场景中物体物理交互时发出的声音，生成的音效在人类听觉上与真实声音几乎无法区分。", "motivation": "使3D场景重建更具交互性，具体是预测人类手部与场景进行物理交互时产生的声音。", "method": "首先，记录人类在3D场景中操纵物体的视频，并获取相应的动作-声音对。然后，利用这些数据训练一个整流流模型，将3D手部轨迹映射到对应的音频。在测试时，用户可以输入参数化的手部姿态序列，模型会估计相应的声音。", "result": "生成的音效能够准确传达材料属性和动作。人类观察者通常无法将生成的音效与真实音效区分开来。", "conclusion": "该方法成功地实现了从3D手部轨迹生成逼真的物理交互声音，增强了3D场景的交互性。", "translation": "我们研究了如何通过回答以下问题使3D场景重建具有交互性：我们能否预测人类手部与场景进行物理交互时发出的声音？首先，我们记录了人类用手在3D场景中操纵物体的视频。然后，我们使用这些动作-声音对来训练一个整流流模型，将3D手部轨迹映射到其对应的音频。在测试时，用户可以查询模型进行其他动作（以手部姿态序列的形式参数化），以估计其对应的声音。在我们的实验中，我们发现我们生成的声音准确地传达了材料属性和动作，并且人类观察者通常无法将它们与真实声音区分开来。项目页面：https://www.yimingdou.com/hearing_hands/", "summary": "本研究旨在解决3D场景重建的交互性问题，特别是预测人类手部与场景中物体物理交互时产生的声音。研究人员通过录制手部操纵物体的视频和声音数据，训练了一个整流流模型，该模型能够将3D手部轨迹映射到相应的音频。实验结果表明，该模型生成的音效能准确传达材料属性和动作，且与真实声音高度相似，人类听者难以辨别。", "keywords": "3D场景交互, 声音生成, 物理交互, 手部轨迹, 整流流模型", "comments": "这项研究的创新之处在于利用机器学习模型（整流流模型）从3D手部轨迹生成逼真的物理交互声音，从而大大增强了3D场景的沉浸感和交互性。其重要性在于为虚拟现实、游戏和机器人等领域提供了新的交互维度，使得数字环境中的物理交互更加真实可信。该方法在用户体验方面具有巨大潜力。"}}
{"id": "2506.09993", "title": "Text-Aware Image Restoration with Diffusion Models", "authors": ["Jaewon Min", "Jin Hyeon Kim", "Paul Hyunbin Cho", "Jaeeun Lee", "Jihye Park", "Minkyu Park", "Sangpil Kim", "Hyunhee Park", "Seungryong Kim"], "summary": "Image restoration aims to recover degraded images. However, existing\ndiffusion-based restoration methods, despite great success in natural image\nrestoration, often struggle to faithfully reconstruct textual regions in\ndegraded images. Those methods frequently generate plausible but incorrect\ntext-like patterns, a phenomenon we refer to as text-image hallucination. In\nthis paper, we introduce Text-Aware Image Restoration (TAIR), a novel\nrestoration task that requires the simultaneous recovery of visual contents and\ntextual fidelity. To tackle this task, we present SA-Text, a large-scale\nbenchmark of 100K high-quality scene images densely annotated with diverse and\ncomplex text instances. Furthermore, we propose a multi-task diffusion\nframework, called TeReDiff, that integrates internal features from diffusion\nmodels into a text-spotting module, enabling both components to benefit from\njoint training. This allows for the extraction of rich text representations,\nwhich are utilized as prompts in subsequent denoising steps. Extensive\nexperiments demonstrate that our approach consistently outperforms\nstate-of-the-art restoration methods, achieving significant gains in text\nrecognition accuracy. See our project page: https://cvlab-kaist.github.io/TAIR/", "comment": "Project page: https://cvlab-kaist.github.io/TAIR/", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09993v1", "AI": {"title_translation": "文本感知扩散模型图像恢复", "tldr": "本文提出文本感知图像恢复（TAIR）新任务和TeReDiff框架，旨在解决现有扩散模型在图像恢复中对文本区域的幻觉问题，并通过集成文本识别模块和利用文本表示作为去噪提示，显著提升了文本识别准确率。", "motivation": "现有的基于扩散模型的图像恢复方法在自然图像恢复方面取得了巨大成功，但在恢复退化图像中的文本区域时常常遇到困难，容易产生看似合理但错误的类文本模式，即文本图像幻觉现象。", "method": "本文引入了文本感知图像恢复（TAIR）这一新任务，旨在同时恢复视觉内容和文本保真度。为解决此任务，作者构建了大规模基准数据集SA-Text，包含10万张高质量场景图像，并密集标注了多样且复杂的文本实例。此外，提出了一种多任务扩散框架TeReDiff，该框架将扩散模型的内部特征集成到文本识别模块中，使两个组件能够通过联合训练相互受益，并利用提取的丰富文本表示作为后续去噪步骤的提示。", "result": "广泛的实验表明，该方法始终优于最先进的恢复方法，在文本识别准确率方面取得了显著提升。", "conclusion": "本研究成功解决了扩散模型在图像恢复中对文本区域的幻觉问题，通过引入文本感知恢复任务和提出的TeReDiff框架，显著提高了文本识别的准确性，为图像恢复领域特别是含文本图像的恢复提供了有效解决方案。", "translation": "图像恢复旨在恢复退化的图像。然而，现有的基于扩散的恢复方法，尽管在自然图像恢复方面取得了巨大成功，但往往难以忠实地重建退化图像中的文本区域。这些方法经常生成看似合理但错误的类文本模式，我们称之为文本图像幻觉现象。在本文中，我们引入了文本感知图像恢复（TAIR），这是一项新的恢复任务，需要同时恢复视觉内容和文本保真度。为了解决这项任务，我们提出了SA-Text，一个包含10万张高质量场景图像的大规模基准数据集，这些图像密集标注了多样且复杂的文本实例。此外，我们提出了一种多任务扩散框架，称为TeReDiff，它将扩散模型的内部特征集成到文本识别模块中，使两个组件能够从联合训练中受益。这允许提取丰富的文本表示，这些表示在随后的去噪步骤中用作提示。大量的实验表明，我们的方法始终优于最先进的恢复方法，在文本识别准确率方面取得了显著提升。请参阅我们的项目页面：https://cvlab-kaist.github.io/TAIR/", "summary": "本文提出“文本感知图像恢复（TAIR）”新任务，旨在解决现有扩散模型在图像恢复中对文本区域的“文本图像幻觉”问题。为此，作者构建了大规模SA-Text数据集，并提出多任务扩散框架TeReDiff，该框架通过联合训练将扩散模型特征与文本识别模块结合，利用文本表示作为去噪提示。实验证明，TeReDiff在文本识别准确率上显著优于现有SOTA方法。", "keywords": "图像恢复, 扩散模型, 文本感知, 文本图像幻觉, 多任务学习", "comments": "这项工作具有重要的创新性，它识别并解决了扩散模型在图像恢复中一个未被充分关注但关键的问题——文本区域的幻觉。通过引入新的任务定义（TAIR）和专门的数据集（SA-Text），以及一个巧妙的多任务框架（TeReDiff），该研究为含文本图像的精确恢复提供了有效方案。其贡献在于提升了图像恢复的实用性，尤其是在需要高文本保真度的场景中，如文档处理或街景识别。"}}
{"id": "2506.09069", "title": "Devanagari Digit Recognition using Quantum Machine Learning", "authors": ["Sahaj Raj Malla"], "summary": "Handwritten digit recognition in regional scripts, such as Devanagari, is\ncrucial for multilingual document digitization, educational tools, and the\npreservation of cultural heritage. The script's complex structure and limited\nannotated datasets pose significant challenges to conventional models. This\npaper introduces the first hybrid quantum-classical architecture for Devanagari\nhandwritten digit recognition, combining a convolutional neural network (CNN)\nfor spatial feature extraction with a 10-qubit variational quantum circuit\n(VQC) for quantum-enhanced classification. Trained and evaluated on the\nDevanagari Handwritten Character Dataset (DHCD), the proposed model achieves a\nstate-of-the-art test accuracy for quantum implementation of 99.80% and a test\nloss of 0.2893, with an average per-class F1-score of 0.9980. Compared to\nequivalent classical CNNs, our model demonstrates superior accuracy with\nsignificantly fewer parameters and enhanced robustness. By leveraging quantum\nprinciples such as superposition and entanglement, this work establishes a\nnovel benchmark for regional script recognition, highlighting the promise of\nquantum machine learning (QML) in real-world, low-resource language settings.", "comment": "9 pages, 4 figures, arXiv preprint, code available upon request", "cate": "quant-ph", "url": "http://arxiv.org/abs/2506.09069v1", "AI": {"title_translation": "使用量子机器学习的梵文数字识别", "tldr": "本文提出了一种混合量子-经典架构，用于梵文手写数字识别，在DHCD数据集上实现了99.80%的测试准确率，并展示了量子机器学习在低资源语言环境中的潜力。", "motivation": "梵文等区域文字的手写数字识别对于多语言文档数字化、教育工具和文化遗产保护至关重要。然而，这些文字复杂的结构和有限的标注数据集给传统模型带来了重大挑战。", "method": "本文引入了第一个混合量子-经典架构，用于梵文手写数字识别。该架构结合了用于空间特征提取的卷积神经网络（CNN）和用于量子增强分类的10量子比特变分量子电路（VQC）。模型在梵文手写字符数据集（DHCD）上进行训练和评估。", "result": "所提出的模型在量子实现方面达到了99.80%的最新测试准确率和0.2893的测试损失，平均每类F1分数达到0.9980。与同等经典CNN相比，该模型以显著更少的参数表现出更高的准确性和增强的鲁棒性。", "conclusion": "通过利用叠加和纠缠等量子原理，这项工作为区域文字识别建立了新的基准，突出了量子机器学习在真实世界、低资源语言环境中的前景。", "translation": "区域文字（如梵文）中的手写数字识别对于多语言文档数字化、教育工具和文化遗产保护至关重要。该文字复杂的结构和有限的标注数据集给传统模型带来了重大挑战。本文首次引入了用于梵文手写数字识别的混合量子-经典架构，结合了用于空间特征提取的卷积神经网络（CNN）和用于量子增强分类的10量子比特变分量子电路（VQC）。该模型在梵文手写字符数据集（DHCD）上进行训练和评估，在量子实现方面达到了99.80%的最新测试准确率和0.2893的测试损失，平均每类F1分数达到0.9980。与同等经典CNN相比，我们的模型以显著更少的参数表现出更高的准确性和增强的鲁棒性。通过利用叠加和纠缠等量子原理，这项工作为区域文字识别建立了新的基准，突出了量子机器学习（QML）在真实世界、低资源语言环境中的前景。", "summary": "本文提出了一种新颖的混合量子-经典架构，用于梵文手写数字识别，旨在解决传统模型在处理复杂区域文字和有限数据集时的挑战。该架构结合了CNN进行特征提取和VQC进行量子增强分类。在DHCD数据集上，该模型实现了99.80%的先进测试准确率，并以更少的参数和更高的鲁棒性优于经典CNN，展示了量子机器学习在低资源语言环境中的巨大潜力。", "keywords": "梵文数字识别, 量子机器学习, 混合量子-经典架构, 卷积神经网络, 变分量子电路", "comments": "这项研究的创新之处在于它是首次将混合量子-经典架构应用于梵文手写数字识别，并在该领域取得了最先进的量子实现准确率。其重要性在于证明了量子机器学习在处理复杂、低资源语言数据方面的潜力，为多语言文档数字化和文化遗产保护提供了新的技术路径。通过显著减少模型参数同时提高准确性和鲁棒性，该工作为未来的量子机器学习应用开辟了新的方向。"}}
{"id": "2506.09995", "title": "PlayerOne: Egocentric World Simulator", "authors": ["Yuanpeng Tu", "Hao Luo", "Xi Chen", "Xiang Bai", "Fan Wang", "Hengshuang Zhao"], "summary": "We introduce PlayerOne, the first egocentric realistic world simulator,\nfacilitating immersive and unrestricted exploration within vividly dynamic\nenvironments. Given an egocentric scene image from the user, PlayerOne can\naccurately construct the corresponding world and generate egocentric videos\nthat are strictly aligned with the real scene human motion of the user captured\nby an exocentric camera. PlayerOne is trained in a coarse-to-fine pipeline that\nfirst performs pretraining on large-scale egocentric text-video pairs for\ncoarse-level egocentric understanding, followed by finetuning on synchronous\nmotion-video data extracted from egocentric-exocentric video datasets with our\nautomatic construction pipeline. Besides, considering the varying importance of\ndifferent components, we design a part-disentangled motion injection scheme,\nenabling precise control of part-level movements. In addition, we devise a\njoint reconstruction framework that progressively models both the 4D scene and\nvideo frames, ensuring scene consistency in the long-form video generation.\nExperimental results demonstrate its great generalization ability in precise\ncontrol of varying human movements and worldconsistent modeling of diverse\nscenarios. It marks the first endeavor into egocentric real-world simulation\nand can pave the way for the community to delve into fresh frontiers of world\nmodeling and its diverse applications.", "comment": "Project page: https://playerone-hku.github.io/", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.09995v1", "AI": {"title_translation": "PlayerOne：以自我为中心的世界模拟器", "tldr": "PlayerOne 是第一个以自我为中心的真实世界模拟器，它能根据用户以自我为中心的场景图像准确构建世界，并生成与真实人体运动严格对齐的以自我为中心的视频。", "motivation": "开发 PlayerOne 旨在实现沉浸式、无限制的动态环境探索，并解决现有技术在以自我为中心的真实世界模拟方面的空白。", "method": "PlayerOne 采用粗到细的训练流程：首先在大规模以自我为中心的文本-视频对上进行预训练以实现粗略的以自我为中心理解；然后利用自动构建管道从以自我为中心-以离心为中心的视频数据集中提取的同步运动-视频数据进行微调。此外，它设计了部分解耦的运动注入方案以精确控制部分级运动，并提出了一个联合重建框架来逐步建模 4D 场景和视频帧，以确保长视频生成中的场景一致性。", "result": "实验结果表明，PlayerOne 在精确控制不同人体运动和世界一致性建模多样化场景方面具有出色的泛化能力。", "conclusion": "PlayerOne 标志着以自我为中心的真实世界模拟的首次尝试，并有望为社区深入探索世界建模及其多样化应用的新领域铺平道路。", "translation": "我们推出了 PlayerOne，这是首个以自我为中心的真实世界模拟器，它能促进在生动动态环境中进行沉浸式、无限制的探索。给定用户以自我为中心的场景图像，PlayerOne 可以准确构建相应的世界，并生成与由外部摄像机捕捉到的用户真实场景人体运动严格对齐的以自我为中心的视频。PlayerOne 采用粗到细的流水线进行训练，首先在大规模以自我为中心的文本-视频对上进行预训练，以实现粗粒度的以自我为中心理解，然后利用我们自动构建的流水线从以自我为中心-外部视频数据集中提取的同步运动-视频数据进行微调。此外，考虑到不同组件的重要性差异，我们设计了一种部分解耦的运动注入方案，实现了对部分级别运动的精确控制。另外，我们设计了一个联合重建框架，逐步建模 4D 场景和视频帧，确保长视频生成中的场景一致性。实验结果证明了其在精确控制不同人体运动和世界一致性建模多样化场景方面的强大泛化能力。这标志着以自我为中心的真实世界模拟的首次尝试，可以为社区深入探索世界建模及其多样化应用的新领域铺平道路。", "summary": "PlayerOne 是首个以自我为中心的真实世界模拟器，它能够根据用户的以自我为中心图像构建虚拟世界，并生成与真实人体运动精确同步的以自我为中心的视频。该系统通过粗到细的流程训练，结合了大规模预训练和针对运动-视频数据的微调。其核心创新包括部分解耦的运动注入方案和联合重建框架，确保了运动控制的精确性和场景在长视频中的一致性。实验证明了其在多种场景下对人体运动和世界建模的卓越泛化能力。", "keywords": "以自我为中心模拟器, 真实世界模拟, 视频生成, 运动控制, 4D 场景建模", "comments": "PlayerOne 的创新之处在于其首次实现了以自我为中心的真实世界模拟，为虚拟现实、机器人和人机交互等领域提供了新的研究平台。其粗到细的训练策略、部分解耦的运动注入以及联合重建框架是其实现高精度和场景一致性的关键。这项工作的重要性在于它为未来更逼真、更具交互性的沉浸式体验奠定了基础。"}}
{"id": "2506.09076", "title": "A Probabilistic Framework for Imputing Genetic Distances in Spatiotemporal Pathogen Models", "authors": ["Haley Stone", "Jing Du", "Hao Xue", "Matthew Scotch", "David Heslop", "Andreas Züfle", "Chandini Raina MacIntyre", "Flora Salim"], "summary": "Pathogen genome data offers valuable structure for spatial models, but its\nutility is limited by incomplete sequencing coverage. We propose a\nprobabilistic framework for inferring genetic distances between unsequenced\ncases and known sequences within defined transmission chains, using time-aware\nevolutionary distance modeling. The method estimates pairwise divergence from\ncollection dates and observed genetic distances, enabling biologically\nplausible imputation grounded in observed divergence patterns, without\nrequiring sequence alignment or known transmission chains. Applied to highly\npathogenic avian influenza A/H5 cases in wild birds in the United States, this\napproach supports scalable, uncertainty-aware augmentation of genomic datasets\nand enhances the integration of evolutionary information into spatiotemporal\nmodeling workflows.", "comment": "9 pages, 3 figures", "cate": "q-bio.GN", "url": "http://arxiv.org/abs/2506.09076v1", "AI": {"title_translation": "时空病原体模型中遗传距离推断的概率框架", "tldr": "提出一个概率框架，用于在不完全测序的病原体数据中推断遗传距离，从而增强时空模型。", "motivation": "病原体基因组数据为空间模型提供了有价值的结构，但其效用受限于不完全的测序覆盖率。", "method": "提出一个概率框架，利用时间感知的进化距离建模，推断定义传播链中未测序病例与已知序列之间的遗传距离。该方法根据采集日期和观察到的遗传距离估计成对分歧，无需序列比对或已知传播链。", "result": "应用于美国野鸟中的高致病性禽流感A/H5病例，该方法支持基因组数据集的可扩展、不确定性感知增强，并增强了进化信息与时空建模工作流程的集成。", "conclusion": "该框架能够对基因组数据进行可扩展、不确定性感知的增强，并改善进化信息在时空建模中的应用。", "translation": "病原体基因组数据为空间模型提供了有价值的结构，但其效用受到不完全测序覆盖率的限制。我们提出了一个概率框架，利用时间感知的进化距离建模，推断定义传播链中未测序病例与已知序列之间的遗传距离。该方法根据采集日期和观察到的遗传距离估计成对分歧，从而实现基于观察到的分歧模式的生物学合理推断，而无需序列比对或已知传播链。应用于美国野鸟中的高致病性禽流感A/H5病例，该方法支持基因组数据集的可扩展、不确定性感知增强，并增强了进化信息与时空建模工作流程的集成。", "summary": "该研究提出了一个概率框架，用于在病原体基因组数据不完整的情况下，推断未测序病例与已知序列之间的遗传距离。该方法利用时间感知的进化距离建模，根据采集日期和观察到的遗传距离估计分歧，无需序列比对。该框架应用于禽流感数据，证明了其在增强基因组数据集和整合进化信息到时空模型中的能力。", "keywords": "遗传距离推断, 时空模型, 病原体基因组, 概率框架, 进化距离", "comments": "该论文的创新之处在于其提出了一种无需序列比对或已知传播链即可推断遗传距离的概率框架，这对于处理不完整基因组数据具有重要意义，尤其是在病原体时空建模中。其不确定性感知和可扩展性也提升了实际应用价值。"}}
{"id": "2506.09672", "title": "Is Fine-Tuning an Effective Solution? Reassessing Knowledge Editing for Unstructured Data", "authors": ["Hao Xiong", "Chuanyuan Tan", "Wenliang Chen"], "summary": "Unstructured Knowledge Editing (UKE) is crucial for updating the relevant\nknowledge of large language models (LLMs). It focuses on unstructured inputs,\nsuch as long or free-form texts, which are common forms of real-world\nknowledge. Although previous studies have proposed effective methods and tested\nthem, some issues exist: (1) Lack of Locality evaluation for UKE, and (2)\nAbnormal failure of fine-tuning (FT) based methods for UKE. To address these\nissues, we first construct two datasets, UnKEBench-Loc and AKEW-Loc (CF), by\nextending two existing UKE datasets with locality test data from the\nunstructured and structured views. This enables a systematic evaluation of the\nLocality of post-edited models. Furthermore, we identify four factors that may\naffect the performance of FT-based methods. Based on these factors, we conduct\nexperiments to determine how the well-performing FT-based methods should be\ntrained for the UKE task, providing a training recipe for future research. Our\nexperimental results indicate that the FT-based method with the optimal setting\n(FT-UKE) is surprisingly strong, outperforming the existing state-of-the-art\n(SOTA). In batch editing scenarios, FT-UKE shows strong performance as well,\nwith its advantage over SOTA methods increasing as the batch size grows,\nexpanding the average metric lead from +6.78% to +10.80%", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.09672v1", "AI": {"title_translation": "微调是有效的解决方案吗？重新评估非结构化数据的知识编辑", "tldr": "研究解决了非结构化知识编辑中微调方法的局部性评估缺失和异常失败问题，构建了新数据集并提出了优化的微调训练方案，发现优化的微调方法（FT-UKE）性能优异，甚至超越现有SOTA。", "motivation": "非结构化知识编辑（UKE）对于更新大型语言模型（LLM）的相关知识至关重要，但现有研究存在两个问题：1) 缺乏对UKE的局部性评估；2) 基于微调（FT）的方法在UKE中出现异常失败。", "method": "构建了两个新数据集UnKEBench-Loc和AKEW-Loc (CF)，通过扩展现有UKE数据集并加入局部性测试数据，以系统评估编辑后模型的局部性。识别了影响基于FT方法性能的四个因素，并基于这些因素进行实验，以确定基于FT的方法在UKE任务中的最佳训练方式，提出了一个训练方案。", "result": "具有最佳设置的基于FT的方法（FT-UKE）表现出惊人的强大性能，超越了现有最先进（SOTA）的方法。在批量编辑场景中，FT-UKE也表现出强大的性能，其相对于SOTA方法的优势随着批量大小的增加而增加，平均指标领先从+6.78%扩大到+10.80%。", "conclusion": "优化后的微调方法（FT-UKE）是解决非结构化知识编辑问题的有效且强大的方案，其性能优于现有最先进方法，尤其在批量编辑场景下优势更为显著。", "translation": "非结构化知识编辑（UKE）对于更新大型语言模型（LLM）的相关知识至关重要。它专注于非结构化输入，例如长文本或自由格式文本，这些是现实世界知识的常见形式。尽管之前的研究提出了有效的方法并对其进行了测试，但仍存在一些问题：（1）缺乏对UKE的局部性评估，以及（2）基于微调（FT）的方法在UKE中出现异常失败。为了解决这些问题，我们首先通过从非结构化和结构化视角扩展两个现有UKE数据集的局部性测试数据，构建了两个数据集：UnKEBench-Loc和AKEW-Loc（CF）。这使得能够对编辑后模型的局部性进行系统评估。此外，我们确定了可能影响基于FT方法性能的四个因素。基于这些因素，我们进行了实验以确定如何训练表现良好的基于FT的方法来执行UKE任务，为未来的研究提供了训练方案。我们的实验结果表明，具有最佳设置的基于FT的方法（FT-UKE）出人意料地强大，超越了现有最先进（SOTA）的方法。在批量编辑场景中，FT-UKE也表现出强大的性能，其相对于SOTA方法的优势随着批量大小的增加而增加，平均指标领先从+6.78%扩大到+10.80%。", "summary": "本文重新评估了非结构化知识编辑（UKE）中基于微调（FT）的方法。针对现有UKE研究中缺乏局部性评估和FT方法异常失败的问题，作者构建了新的数据集（UnKEBench-Loc和AKEW-Loc (CF)）以系统评估局部性，并识别了影响FT方法性能的四个因素。通过实验，作者提出了优化的FT训练方案，并发现经过优化的FT-UKE方法表现出惊人的强大性能，超越了现有最先进（SOTA）的方法，尤其在批量编辑场景下优势更为明显。", "keywords": "非结构化知识编辑, 微调, 大型语言模型, 局部性, 知识编辑", "comments": "该研究解决了非结构化知识编辑领域中的两个关键问题：局部性评估的缺失和微调方法表现异常。通过构建专门的数据集并提出优化的训练策略，该研究证明了微调方法在UKE任务中的巨大潜力，甚至超越了现有SOTA。这对于LLM知识更新的实际应用具有重要意义，尤其是在处理大规模非结构化数据时。其提出的“训练方案”为未来的研究提供了宝贵的指导。"}}
{"id": "2506.09097", "title": "Detecting malignant dynamics on very few blood sample using signature coefficients", "authors": ["Rémi Vaucher", "Stéphane Chrétien"], "summary": "Recent discoveries have suggested that the promising avenue of using\ncirculating tumor DNA (ctDNA) levels in blood samples provides reasonable\naccuracy for cancer monitoring, with extremely low burden on the patient's\nside. It is known that the presence of ctDNA can result from various mechanisms\nleading to DNA release from cells, such as apoptosis, necrosis or active\nsecretion. One key idea in recent cancer monitoring studies is that monitoring\nthe dynamics of ctDNA levels might be sufficient for early multi-cancer\ndetection. This interesting idea has been turned into commercial products, e.g.\nin the company named GRAIL.\n  In the present work, we propose to explore the use of Signature theory for\ndetecting aggressive cancer tumors based on the analysis of blood samples. Our\napproach combines tools from continuous time Markov modelling for the dynamics\nof ctDNA levels in the blood, with Signature theory for building efficient\ntesting procedures. Signature theory is a topic of growing interest in the\nMachine Learning community (see Chevyrev2016 and Fermanian2021), which is now\nrecognised as a powerful feature extraction tool for irregularly sampled\nsignals. The method proposed in the present paper is shown to correctly address\nthe challenging problem of overcoming the inherent data scarsity due to the\nextremely small number of blood samples per patient. The relevance of our\napproach is illustrated with extensive numerical experiments that confirm the\nefficiency of the proposed pipeline.", "comment": "Under review", "cate": "q-bio.QM", "url": "http://arxiv.org/abs/2506.09097v1", "AI": {"title_translation": "使用签名系数检测极少量血液样本中的恶性动态", "tldr": "本文提出一种结合马尔可夫模型和签名理论的方法，利用极少量血液样本中的ctDNA动态来检测侵袭性癌症，有效解决了数据稀缺问题。", "motivation": "利用循环肿瘤DNA（ctDNA）水平进行癌症监测具有前景且对患者负担极低，监测ctDNA水平动态可能足以进行早期多癌检测。然而，面临的挑战是每个患者的血液样本数量极少，导致数据固有的稀缺性。", "method": "该研究提出使用签名理论检测侵袭性肿瘤，其方法结合了连续时间马尔可夫模型工具来描述血液中ctDNA水平的动态，并利用签名理论构建高效的测试程序。", "result": "该方法能够有效解决因每个患者血液样本极少而导致的数据稀缺问题。通过大量的数值实验证实了所提出方法的有效性。", "conclusion": "该研究提出的方法有效解决了血液样本数据稀缺的挑战，并通过大量数值实验证实了其在检测侵袭性癌症方面的效率和可行性。", "translation": "最近的发现表明，使用血液样本中的循环肿瘤DNA (ctDNA) 水平进行癌症监测是一种有前景的途径，能够提供合理的准确性，同时对患者的负担极低。已知ctDNA的存在可能源于导致DNA从细胞释放的各种机制，例如细胞凋亡、坏死或主动分泌。近期癌症监测研究中的一个关键思想是，监测ctDNA水平的动态可能足以实现早期多癌检测。这一有趣的理念已被转化为商业产品，例如GRAIL公司。\n在当前工作中，我们提出探索使用签名理论来检测基于血液样本分析的侵袭性癌肿瘤。我们的方法结合了连续时间马尔可夫模型工具来描述血液中ctDNA水平的动态，并结合签名理论来构建高效的测试程序。签名理论在机器学习社区中是一个日益受到关注的话题（参见Chevyrev2016和Fermanian2021），它现在被认为是处理不规则采样信号的强大特征提取工具。本文提出的方法被证明能够正确解决由于每个患者血液样本数量极少而导致的数据稀缺这一挑战性问题。我们方法的适用性通过大量的数值实验得到了验证，这些实验证实了所提出流程的效率。", "summary": "本文提出一种新颖的方法，通过分析极少量血液样本中的循环肿瘤DNA (ctDNA) 动态来检测侵袭性癌症。该方法结合了连续时间马尔可夫模型和签名理论，以解决数据稀缺性并构建高效的测试程序。大量的数值实验验证了该方法的效率。", "keywords": "签名理论, ctDNA, 癌症检测, 马尔可夫模型, 数据稀缺", "comments": "该论文的创新之处在于将签名理论（一种用于不规则采样信号的强大特征提取工具）应用于癌症检测中稀疏ctDNA数据的挑战性问题。这解决了当前癌症监测中的一个关键限制，可能有助于实现更易于获取且负担更小的早期检测。与连续时间马尔可夫模型结合以处理动态也是一个亮点。"}}
{"id": "2506.09313", "title": "Surrogate models to optimize plasma assisted atomic layer deposition in high aspect ratio features", "authors": ["Angel Yanguas-Gil", "Jeffrey W. Elam"], "summary": "In this work we explore surrogate models to optimize plasma enhanced atomic\nlayer deposition (PEALD) in high aspect ratio features. In plasma-based\nprocesses such as PEALD and atomic layer etching, surface recombination can\ndominate the reactivity of plasma species with the surface, which can lead to\nunfeasibly long exposure times to achieve full conformality inside\nnanostructures like high aspect ratio vias. Using a synthetic dataset based on\nsimulations of PEALD, we train artificial neural networks to predict saturation\ntimes based on cross section thickness data obtained for partially coated\nconditions. The results obtained show that just two experiments in\nundersaturated conditions contain enough information to predict saturation\ntimes within 10% of the ground truth. A surrogate model trained to determine\nwhether surface recombination dominates the plasma-surface interactions in a\nPEALD process achieves 99% accuracy. This demonstrates that machine learning\ncan provide a new pathway to accelerate the optimization of PEALD processes in\nareas such as microelectronics. Our approach can be easily extended to atomic\nlayer etching and more complex structures.", "comment": null, "cate": "cond-mat.mtrl-sci", "url": "http://arxiv.org/abs/2506.09313v1", "AI": {"title_translation": "代理模型优化高深宽比特征中的等离子体辅助原子层沉积", "tldr": "本研究利用代理模型（人工神经网络）优化高深宽比结构中的等离子体增强原子层沉积（PEALD）过程。通过少量实验数据，模型能准确预测饱和时间，并判断表面复合是否主导，为微电子等领域的PEALD工艺优化提供了高效新途径。", "motivation": "在等离子体基工艺（如PEALD）中，表面复合会主导等离子体物质与表面的反应性，这可能导致在纳米结构（如高深宽比通孔）内部实现完全共形需要不切实际的长时间曝光，因此需要新的优化方法。", "method": "研究使用基于PEALD模拟的合成数据集，训练人工神经网络，根据部分涂覆条件下获得的横截面厚度数据预测饱和时间。此外，还训练了一个代理模型来确定表面复合是否主导等离子体-表面相互作用。", "result": "结果显示，在欠饱和条件下，仅需两次实验就包含足够的信息，可以在10%的误差范围内预测饱和时间。用于判断表面复合是否主导PEALD过程中等离子体-表面相互作用的代理模型达到了99%的准确率。", "conclusion": "机器学习为加速微电子等领域PEALD工艺的优化提供了一条新途径。该方法易于扩展到原子层刻蚀和更复杂的结构。", "translation": "在这项工作中，我们探索代理模型来优化高深宽比特征中的等离子体增强原子层沉积（PEALD）。在等离子体基工艺（如PEALD和原子层刻蚀）中，表面复合会主导等离子体物质与表面的反应性，这可能导致在纳米结构（如高深宽比通孔）内部实现完全共形需要不切实际的长时间曝光。我们使用基于PEALD模拟的合成数据集，训练人工神经网络，根据部分涂覆条件下获得的横截面厚度数据预测饱和时间。结果显示，在欠饱和条件下，仅需两次实验就包含足够的信息，可以在10%的误差范围内预测饱和时间。一个旨在确定表面复合是否主导PEALD过程中等离子体-表面相互作用的代理模型达到了99%的准确率。这表明机器学习可以为加速微电子等领域PEALD工艺的优化提供一条新途径。我们的方法可以很容易地扩展到原子层刻蚀和更复杂的结构。", "summary": "本研究探讨了利用代理模型（人工神经网络）优化高深宽比结构中的等离子体增强原子层沉积（PEALD）工艺。针对等离子体基工艺中表面复合导致曝光时间过长的问题，研究人员通过PEALD模拟生成合成数据集，并训练神经网络来预测饱和时间。结果表明，仅需两次欠饱和实验即可准确预测饱和时间（误差在10%以内），且一个判断表面复合主导性的代理模型达到了99%的准确率。这证明了机器学习在加速PEALD工艺优化方面的潜力，并可推广至其他原子层工艺。", "keywords": "代理模型, 等离子体增强原子层沉积, 人工神经网络, 饱和时间, 表面复合", "comments": "该论文的创新之处在于将机器学习，特别是人工神经网络，应用于PEALD工艺的优化，解决了传统方法中因表面复合导致曝光时间过长的问题。通过少量实验数据即可实现高精度预测和过程判断，极大地加速了工艺开发和优化。这种方法对于微电子等需要高精度、高效率沉积的领域具有重要意义，且其通用性使其易于扩展到其他原子层工艺，展现了机器学习在材料科学和半导体制造领域的巨大潜力。"}}
{"id": "2506.09820", "title": "CoRT: Code-integrated Reasoning within Thinking", "authors": ["Chengpeng Li", "Zhengyang Tang", "Ziniu Li", "Mingfeng Xue", "Keqin Bao", "Tian Ding", "Ruoyu Sun", "Benyou Wang", "Xiang Wang", "Junyang Lin", "Dayiheng Liu"], "summary": "Large Reasoning Models (LRMs) like o1 and DeepSeek-R1 have shown remarkable\nprogress in natural language reasoning with long chain-of-thought (CoT), yet\nthey remain inefficient or inaccurate when handling complex mathematical\noperations. Addressing these limitations through computational tools (e.g.,\ncomputation libraries and symbolic solvers) is promising, but it introduces a\ntechnical challenge: Code Interpreter (CI) brings external knowledge beyond the\nmodel's internal text representations, thus the direct combination is not\nefficient. This paper introduces CoRT, a post-training framework for teaching\nLRMs to leverage CI effectively and efficiently. As a first step, we address\nthe data scarcity issue by synthesizing code-integrated reasoning data through\nHint-Engineering, which strategically inserts different hints at appropriate\npositions to optimize LRM-CI interaction. We manually create 30 high-quality\nsamples, upon which we post-train models ranging from 1.5B to 32B parameters,\nwith supervised fine-tuning, rejection fine-tuning and reinforcement learning.\nOur experimental results demonstrate that Hint-Engineering models achieve 4\\%\nand 8\\% absolute improvements on DeepSeek-R1-Distill-Qwen-32B and\nDeepSeek-R1-Distill-Qwen-1.5B respectively, across five challenging\nmathematical reasoning datasets. Furthermore, Hint-Engineering models use about\n30\\% fewer tokens for the 32B model and 50\\% fewer tokens for the 1.5B model\ncompared with the natural language models. The models and code are available at\nhttps://github.com/ChengpengLi1003/CoRT.", "comment": "work in progress", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.09820v1", "AI": {"title_translation": "CoRT：思维中的代码集成推理", "tldr": "CoRT是一个后训练框架，通过Hint-Engineering解决LRMs在复杂数学推理中结合代码解释器效率低下的问题，显著提升了性能并减少了token使用。", "motivation": "大型推理模型（LRMs）在自然语言推理方面表现出色，但在处理复杂数学运算时效率低下或不准确。虽然计算工具（如计算库和符号求解器）有望解决这些限制，但代码解释器（CI）引入了模型内部文本表示之外的外部知识，导致直接结合效率不高。", "method": "本文引入了CoRT，一个用于教LRMs有效且高效地利用CI的后训练框架。通过Hint-Engineering方法合成代码集成推理数据，该方法在适当位置策略性地插入不同提示以优化LMR-CI交互。手动创建30个高质量样本，并对1.5B到32B参数范围的模型进行后训练，包括监督微调、拒绝微调和强化学习。", "result": "Hint-Engineering模型在五个具有挑战性的数学推理数据集上，相对于DeepSeek-R1-Distill-Qwen-32B和DeepSeek-R1-Distill-Qwen-1.5B分别实现了4%和8%的绝对性能提升。此外，与自然语言模型相比，32B模型使用的token减少了约30%，1.5B模型减少了约50%。", "conclusion": "CoRT框架通过Hint-Engineering有效提升了大型推理模型在复杂数学推理任务中结合代码解释器的效率和准确性，同时显著减少了token使用量。", "translation": "大型推理模型（LRMs），如o1和DeepSeek-R1，在长思维链（CoT）的自然语言推理方面取得了显著进展，但它们在处理复杂数学运算时仍然效率低下或不准确。通过计算工具（例如，计算库和符号求解器）解决这些限制是有前景的，但这引入了一个技术挑战：代码解释器（CI）带来了模型内部文本表示之外的外部知识，因此直接组合效率不高。本文介绍了CoRT，一个用于教LRMs有效且高效地利用CI的后训练框架。作为第一步，我们通过提示工程（Hint-Engineering）合成代码集成推理数据，解决了数据稀缺问题，该方法在适当位置策略性地插入不同的提示以优化LMR-CI交互。我们手动创建了30个高质量样本，并在此基础上对1.5B到32B参数范围的模型进行了后训练，采用了监督微调、拒绝微调和强化学习。我们的实验结果表明，提示工程模型在五个具有挑战性的数学推理数据集上，相对于DeepSeek-R1-Distill-Qwen-32B和DeepSeek-R1-Distill-Qwen-1.5B分别实现了4%和8%的绝对性能提升。此外，与自然语言模型相比，提示工程模型32B模型使用的token减少了约30%，1.5B模型减少了约50%。模型和代码可在https://github.com/ChengpengLi1003/CoRT获取。", "summary": "本文提出了CoRT，一个后训练框架，旨在解决大型推理模型（LRMs）在处理复杂数学运算时结合代码解释器（CI）的效率和准确性问题。通过引入Hint-Engineering方法合成代码集成推理数据，CoRT优化了LMR-CI交互。实验结果表明，CoRT在多个数学推理数据集上显著提升了模型的性能，并大幅减少了token使用量，证明了其在结合外部计算工具方面的有效性。", "keywords": "大型推理模型, 代码解释器, 数学推理, 后训练, 提示工程", "comments": "这篇论文的创新点在于提出了CoRT后训练框架和Hint-Engineering方法，有效地解决了大型推理模型在复杂数学推理中结合代码解释器时的效率和准确性问题。通过合成高质量的训练数据并结合多种微调技术，该研究不仅提升了模型性能，还显著降低了计算成本（token使用量）。这项工作对于提升LLMs在需要精确计算的领域（如科学、工程）的应用潜力具有重要意义。其方法的通用性也值得关注，可能适用于其他需要外部工具协助的推理任务。"}}
{"id": "2506.09827", "title": "EmoNet-Voice: A Fine-Grained, Expert-Verified Benchmark for Speech Emotion Detection", "authors": ["Christoph Schuhmann", "Robert Kaczmarczyk", "Gollam Rabby", "Felix Friedrich", "Maurice Kraus", "Kourosh Nadi", "Huu Nguyen", "Kristian Kersting", "Sören Auer"], "summary": "The advancement of text-to-speech and audio generation models necessitates\nrobust benchmarks for evaluating the emotional understanding capabilities of AI\nsystems. Current speech emotion recognition (SER) datasets often exhibit\nlimitations in emotional granularity, privacy concerns, or reliance on acted\nportrayals. This paper introduces EmoNet-Voice, a new resource for speech\nemotion detection, which includes EmoNet-Voice Big, a large-scale pre-training\ndataset (featuring over 4,500 hours of speech across 11 voices, 40 emotions,\nand 4 languages), and EmoNet-Voice Bench, a novel benchmark dataset with human\nexpert annotations. EmoNet-Voice is designed to evaluate SER models on a\nfine-grained spectrum of 40 emotion categories with different levels of\nintensities. Leveraging state-of-the-art voice generation, we curated synthetic\naudio snippets simulating actors portraying scenes designed to evoke specific\nemotions. Crucially, we conducted rigorous validation by psychology experts who\nassigned perceived intensity labels. This synthetic, privacy-preserving\napproach allows for the inclusion of sensitive emotional states often absent in\nexisting datasets. Lastly, we introduce Empathic Insight Voice models that set\na new standard in speech emotion recognition with high agreement with human\nexperts. Our evaluations across the current model landscape exhibit valuable\nfindings, such as high-arousal emotions like anger being much easier to detect\nthan low-arousal states like concentration.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.09827v1", "AI": {"title_translation": "EmoNet-Voice：一个用于语音情感检测的细粒度、专家验证基准", "tldr": "本文介绍了 EmoNet-Voice，这是一个新的细粒度、专家验证的语音情感检测基准数据集，旨在解决现有数据集的局限性，并提出了高性能的Empathic Insight Voice模型。", "motivation": "现有语音情感识别（SER）数据集在情感粒度、隐私和对表演的依赖方面存在局限性，且缺乏评估AI系统情感理解能力的鲁棒基准。文本到语音和音频生成模型的进步也需要更强大的评估工具。", "method": "引入 EmoNet-Voice，包括 EmoNet-Voice Big（一个大型预训练数据集，包含4500小时、11种声音、40种情感和4种语言的语音）和 EmoNet-Voice Bench（一个带有人类专家标注的新型基准数据集）。利用先进的语音生成技术，模拟演员表演特定情感的场景，并由心理学专家进行感知强度标注。采用合成的、保护隐私的方法。最后，引入了 Empathic Insight Voice 模型。", "result": "EmoNet-Voice 能够评估40种细粒度情感类别和不同强度级别。通过合成和专家验证，解决了隐私问题并纳入了现有数据集中缺失的敏感情感状态。Empathic Insight Voice 模型在语音情感识别方面设定了新标准，与人类专家有高度一致性。评估显示，高唤醒情绪（如愤怒）比低唤醒情绪（如专注）更容易检测。", "conclusion": "EmoNet-Voice 及其伴随的 Empathic Insight Voice 模型为语音情感识别领域提供了急需的细粒度、专家验证的基准和高性能模型，解决了现有数据集的诸多限制，并揭示了不同情感检测难度差异。", "translation": "文本到语音和音频生成模型的进步需要强大的基准来评估AI系统的情感理解能力。目前的语音情感识别（SER）数据集在情感粒度、隐私问题或对表演的依赖方面常常存在局限性。本文介绍了 EmoNet-Voice，一个用于语音情感检测的新资源，其中包括 EmoNet-Voice Big（一个大型预训练数据集，包含超过4500小时的语音，涵盖11种声音、40种情感和4种语言）和 EmoNet-Voice Bench（一个带有人类专家标注的新型基准数据集）。EmoNet-Voice 旨在评估 SER 模型在40种细粒度情感类别和不同强度级别上的表现。我们利用最先进的语音生成技术，精心策划了模拟演员表演旨在唤起特定情感场景的合成音频片段。至关重要的是，我们通过心理学专家进行了严格的验证，他们分配了感知强度标签。这种合成的、保护隐私的方法允许包含现有数据集中通常缺失的敏感情感状态。最后，我们推出了 Empathic Insight Voice 模型，这些模型在语音情感识别方面树立了新标准，与人类专家高度一致。我们对当前模型格局的评估展示了有价值的发现，例如高唤醒情绪（如愤怒）比低唤醒状态（如专注）更容易检测。", "summary": "本文介绍了 EmoNet-Voice，一个解决现有数据集局限性的新型细粒度、专家验证的语音情感检测基准。它包含大规模预训练数据集 EmoNet-Voice Big 和专家标注的基准数据集 EmoNet-Voice Bench。通过合成音频和心理学专家验证，EmoNet-Voice 实现了对40种情感类别的细粒度评估，并保护了隐私。研究还提出了高性能的 Empathic Insight Voice 模型，并在评估中发现高唤醒情绪比低唤醒情绪更容易检测。", "keywords": "语音情感识别, EmoNet-Voice, 数据集, 细粒度情感, 专家验证", "comments": "该论文的创新之处在于提出了一个细粒度、专家验证且保护隐私的语音情感识别基准数据集 EmoNet-Voice，解决了现有数据集的局限性。通过合成音频和心理学专家标注，有效克服了敏感情感数据获取和隐私问题。其提出的 Empathic Insight Voice 模型也展现了高水平的性能。这项工作对于推动AI在情感理解方面的进步具有重要意义。"}}
{"id": "2506.09643", "title": "Using Sign Language Production as Data Augmentation to enhance Sign Language Translation", "authors": ["Harry Walsh", "Maksym Ivashechkin", "Richard Bowden"], "summary": "Machine learning models fundamentally rely on large quantities of\nhigh-quality data. Collecting the necessary data for these models can be\nchallenging due to cost, scarcity, and privacy restrictions. Signed languages\nare visual languages used by the deaf community and are considered low-resource\nlanguages. Sign language datasets are often orders of magnitude smaller than\ntheir spoken language counterparts. Sign Language Production is the task of\ngenerating sign language videos from spoken language sentences, while Sign\nLanguage Translation is the reverse translation task. Here, we propose\nleveraging recent advancements in Sign Language Production to augment existing\nsign language datasets and enhance the performance of Sign Language Translation\nmodels. For this, we utilize three techniques: a skeleton-based approach to\nproduction, sign stitching, and two photo-realistic generative models, SignGAN\nand SignSplat. We evaluate the effectiveness of these techniques in enhancing\nthe performance of Sign Language Translation models by generating variation in\nthe signer's appearance and the motion of the skeletal data. Our results\ndemonstrate that the proposed methods can effectively augment existing datasets\nand enhance the performance of Sign Language Translation models by up to 19%,\npaving the way for more robust and accurate Sign Language Translation systems,\neven in resource-constrained environments.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.09643v1", "AI": {"title_translation": "使用手语生成作为数据增强以提升手语翻译", "tldr": "利用手语生成技术增强数据集，将手语翻译性能提升高达19%。", "motivation": "机器学习模型依赖大量高质量数据，但手语作为低资源语言，其数据集因成本、稀缺性和隐私限制而远小于口语数据集，这给手语翻译带来了挑战。", "method": "该研究利用手语生成技术来增强现有手语数据集并提升手语翻译模型的性能。具体方法包括：基于骨架的生成方法、手语拼接以及两种照片级真实感生成模型（SignGAN和SignSplat）。通过生成手语者外观和骨架运动的变化来评估这些技术的有效性。", "result": "研究结果表明，所提出的方法能有效增强现有数据集，并将手语翻译模型的性能提升高达19%。", "conclusion": "该方法为在资源受限环境下开发更鲁棒、更准确的手语翻译系统铺平了道路。", "translation": "机器学习模型本质上依赖于大量高质量数据。由于成本、稀缺性和隐私限制，收集这些模型所需的数据可能具有挑战性。手语是聋人社区使用的视觉语言，被认为是低资源语言。手语数据集通常比其口语数据集小几个数量级。手语生成是从口语句子生成手语视频的任务，而手语翻译是相反的翻译任务。在此，我们建议利用手语生成的最新进展来扩充现有手语数据集并提升手语翻译模型的性能。为此，我们利用了三种技术：基于骨架的生成方法、手语拼接以及两种照片级真实感生成模型SignGAN和SignSplat。我们通过生成手语者外观和骨架运动的变化来评估这些技术在提升手语翻译模型性能方面的有效性。我们的结果表明，所提出的方法可以有效地扩充现有数据集，并将手语翻译模型的性能提升高达19%，为即使在资源受限的环境中也能实现更鲁棒、更准确的手语翻译系统铺平了道路。", "summary": "本文通过提出一种利用手语生成（SLP）进行数据增强的方法，解决了手语翻译（SLT）中的数据稀缺问题。该方法利用基于骨架的方法、手语拼接和生成模型（SignGAN、SignSplat）来创建多样化的合成数据。实验表明，这种数据增强显著提升了SLT模型的性能，最高可达19%，为在低资源环境下开发鲁棒的SLT系统提供了可行途径。", "keywords": "手语生成, 数据增强, 手语翻译, 低资源语言, 生成模型", "comments": "本文提出了一种创新方法，通过创造性地利用手语生成技术进行数据增强，解决了手语翻译中关键的数据稀缺问题。采用多种多样的方法，包括基于骨架的生成和照片级真实感GAN，来生成多样化的合成数据是一个亮点。在手语这种低资源领域，性能提升高达19%具有重要意义，有望实现更实用、更准确的翻译系统。"}}
{"id": "2506.09853", "title": "Causal Sufficiency and Necessity Improves Chain-of-Thought Reasoning", "authors": ["Xiangning Yu", "Zhuohan Wang", "Linyi Yang", "Haoxuan Li", "Anjie Liu", "Xiao Xue", "Jun Wang", "Mengyue Yang"], "summary": "Chain-of-Thought (CoT) prompting plays an indispensable role in endowing\nlarge language models (LLMs) with complex reasoning capabilities. However, CoT\ncurrently faces two fundamental challenges: (1) Sufficiency, which ensures that\nthe generated intermediate inference steps comprehensively cover and\nsubstantiate the final conclusion; and (2) Necessity, which identifies the\ninference steps that are truly indispensable for the soundness of the resulting\nanswer. We propose a causal framework that characterizes CoT reasoning through\nthe dual lenses of sufficiency and necessity. Incorporating causal Probability\nof Sufficiency and Necessity allows us not only to determine which steps are\nlogically sufficient or necessary to the prediction outcome, but also to\nquantify their actual influence on the final reasoning outcome under different\nintervention scenarios, thereby enabling the automated addition of missing\nsteps and the pruning of redundant ones. Extensive experimental results on\nvarious mathematical and commonsense reasoning benchmarks confirm substantial\nimprovements in reasoning efficiency and reduced token usage without\nsacrificing accuracy. Our work provides a promising direction for improving LLM\nreasoning performance and cost-effectiveness.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.09853v1", "AI": {"title_translation": "因果充分性和必要性改进思维链推理", "tldr": "本文提出了一个因果框架，通过充分性和必要性来改进思维链（CoT）推理，该框架能够自动添加缺失步骤和修剪冗余步骤，从而提高了LLM的推理效率并降低了token使用量，同时不牺牲准确性。", "motivation": "思维链（CoT）提示在赋予大型语言模型（LLMs）复杂推理能力方面发挥着不可或缺的作用，但目前面临两个基本挑战：(1) 充分性，即确保生成的中间推理步骤全面覆盖并证实最终结论；(2) 必要性，即识别对结果答案的正确性真正不可或缺的推理步骤。", "method": "我们提出了一个因果框架，通过充分性和必要性双重视角来表征CoT推理。该框架结合了因果充分性概率（PS）和必要性概率（PN），不仅能够确定哪些步骤在逻辑上对预测结果是充分或必要的，还能量化它们在不同干预场景下对最终推理结果的实际影响，从而实现自动添加缺失步骤和修剪冗余步骤。", "result": "在各种数学和常识推理基准测试上的广泛实验结果证实，在不牺牲准确性的前提下，推理效率得到了显著提高，token使用量也减少了。", "conclusion": "我们的工作为提高LLM推理性能和成本效益提供了一个有前景的方向。", "translation": "思维链（CoT）提示在赋予大型语言模型（LLMs）复杂推理能力方面发挥着不可或缺的作用。然而，CoT目前面临两个基本挑战：(1) 充分性，即确保生成的中间推理步骤全面覆盖并证实最终结论；(2) 必要性，即识别对结果答案的正确性真正不可或缺的推理步骤。我们提出了一个因果框架，通过充分性和必要性双重视角来表征CoT推理。结合因果充分性概率和必要性概率，我们不仅能够确定哪些步骤在逻辑上对预测结果是充分或必要的，还能量化它们在不同干预场景下对最终推理结果的实际影响，从而实现自动添加缺失步骤和修剪冗余步骤。在各种数学和常识推理基准测试上的广泛实验结果证实，在不牺牲准确性的前提下，推理效率得到了显著提高，token使用量也减少了。我们的工作为提高LLM推理性能和成本效益提供了一个有前景的方向。", "summary": "本文针对思维链（CoT）推理在充分性和必要性方面的挑战，提出了一个基于因果框架的新方法。该方法利用因果充分性概率和必要性概率来识别和量化推理步骤对最终结果的影响，从而实现了自动添加缺失步骤和移除冗余步骤。实验结果表明，该方法在不牺牲准确性的前提下，显著提升了大型语言模型（LLMs）的推理效率并降低了token使用量，为改进LLM推理性能和成本效益提供了新方向。", "keywords": "思维链, 因果推断, 充分性, 必要性, 大型语言模型", "comments": "该论文通过引入因果推断的充分性和必要性概念，为优化大型语言模型（LLMs）的思维链（CoT）推理提供了一个新颖且理论坚实的方法。其创新之处在于将因果概率应用于量化CoT步骤的重要性，从而实现推理过程的自动化优化（增删步骤）。这不仅解决了CoT面临的效率和冗余问题，而且在实践中通过减少token使用量，提升了模型成本效益，具有重要的应用价值。该方法为未来LLM推理的精细化控制和可解释性研究开辟了新的路径。"}}
{"id": "2506.09401", "title": "A theoretical basis for model collapse in recursive training", "authors": ["Vivek Shripad Borkar"], "summary": "It is known that recursive training from generative models can lead to the so\ncalled `collapse' of the simulated probability distribution. This note shows\nthat one in fact gets two different asymptotic behaviours depending on whether\nan external source, howsoever minor, is also contributing samples.", "comment": null, "cate": "math.PR", "url": "http://arxiv.org/abs/2506.09401v1", "AI": {"title_translation": "递归训练中模型崩溃的理论基础", "tldr": "递归训练的生成模型可能导致模型崩溃，本文表明，这取决于是否有外部样本贡献，存在两种不同的渐近行为。", "motivation": "了解生成模型递归训练中出现的“模型崩溃”现象的理论基础。", "method": "理论分析，揭示了在有无外部样本贡献的情况下，模型训练的两种不同渐近行为。", "result": "递归训练会导致两种不同的渐近行为，具体取决于是否存在外部样本的贡献。", "conclusion": "递归训练中的模型崩溃现象与外部样本的贡献密切相关，存在两种不同的渐近行为。", "translation": "已知生成模型的递归训练会导致模拟概率分布的所谓“崩溃”。本笔记表明，实际上会得到两种不同的渐近行为，这取决于是否有外部来源（无论多么微小）也在提供样本。", "summary": "本文探讨了生成模型递归训练中出现的“模型崩溃”现象的理论基础。研究指出，根据是否存在外部样本的贡献，模型在递归训练中会表现出两种截然不同的渐近行为。", "keywords": "模型崩溃, 递归训练, 生成模型, 渐近行为, 外部样本", "comments": "这篇论文为理解生成模型在递归训练中遇到的“模型崩溃”问题提供了重要的理论洞察，特别是揭示了外部数据源在防止或影响这种崩溃中的关键作用。"}}
{"id": "2506.09886", "title": "Attention Head Embeddings with Trainable Deep Kernels for Hallucination Detection in LLMs", "authors": ["Rodion Oblovatny", "Alexandra Bazarova", "Alexey Zaytsev"], "summary": "We present a novel approach for detecting hallucinations in large language\nmodels (LLMs) by analyzing the probabilistic divergence between prompt and\nresponse hidden-state distributions. Counterintuitively, we find that\nhallucinated responses exhibit smaller deviations from their prompts compared\nto grounded responses, suggesting that hallucinations often arise from\nsuperficial rephrasing rather than substantive reasoning. Leveraging this\ninsight, we propose a model-intrinsic detection method that uses distributional\ndistances as principled hallucination scores, eliminating the need for external\nknowledge or auxiliary models. To enhance sensitivity, we employ deep learnable\nkernels that automatically adapt to capture nuanced geometric differences\nbetween distributions. Our approach outperforms existing baselines,\ndemonstrating state-of-the-art performance on several benchmarks. The method\nremains competitive even without kernel training, offering a robust, scalable\nsolution for hallucination detection.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.09886v1", "AI": {"title_translation": "LLM中用于幻觉检测的注意力头嵌入与可训练深度核", "tldr": "该论文提出了一种通过分析提示和响应隐藏状态分布的概率散度来检测LLM幻觉的新方法，利用深度可学习核提高了检测灵敏度，并取得了最先进的性能。", "motivation": "检测大型语言模型（LLMs）中的幻觉。", "method": "该方法通过分析提示和响应隐藏状态分布之间的概率散度来检测幻觉。研究发现幻觉响应与提示的偏差小于接地响应。利用这一发现，该方法将分布距离作为幻觉得分，无需外部知识或辅助模型。为提高灵敏度，使用了可自动适应的深度可学习核。", "result": "该方法优于现有基线，在多个基准测试中表现出最先进的性能。即使没有核训练，该方法也保持竞争力。", "conclusion": "该方法为幻觉检测提供了一种鲁棒、可扩展的解决方案。", "translation": "我们提出了一种新颖的方法，通过分析提示和响应隐藏状态分布之间的概率散度来检测大型语言模型（LLM）中的幻觉。出乎意料的是，我们发现幻觉响应与其提示的偏差小于接地响应，这表明幻觉通常源于肤浅的改写而非实质性推理。利用这一见解，我们提出了一种模型内在的检测方法，该方法使用分布距离作为原则性的幻觉分数，从而无需外部知识或辅助模型。为了增强灵敏度，我们采用了深度可学习核，它们能自动适应以捕获分布之间细微的几何差异。我们的方法优于现有基线，在多个基准测试中表现出最先进的性能。即使没有核训练，该方法也保持竞争力，为幻觉检测提供了一种鲁棒、可扩展的解决方案。", "summary": "本文提出一种新颖的LLM幻觉检测方法，通过分析提示与响应隐藏状态的概率散度。研究发现幻觉源于浅层改写，其偏差小于接地响应。该方法利用分布距离作为幻觉得分，并引入深度可学习核提高检测灵敏度，无需外部知识。实验证明，该方法在多个基准测试中表现出最先进的性能，并具有鲁棒性和可扩展性。", "keywords": "LLM幻觉检测, 隐藏状态分布, 深度可学习核, 概率散度, 模型内在检测", "comments": "这篇论文的创新点在于提出了一个模型内在的幻觉检测方法，通过分析提示和响应隐藏状态分布的概率散度。其反直觉的发现——幻觉响应与提示的偏差反而更小——为理解LLM幻觉的产生机制提供了新的视角。引入深度可学习核来增强对分布之间细微几何差异的捕获能力，进一步提升了检测的灵敏度和性能。这种无需外部知识或辅助模型的解决方案，使其具有更高的鲁棒性和可扩展性，对于实际部署具有重要意义。"}}
{"id": "2506.09890", "title": "The Emergence of Abstract Thought in Large Language Models Beyond Any Language", "authors": ["Yuxin Chen", "Yiran Zhao", "Yang Zhang", "An Zhang", "Kenji Kawaguchi", "Shafiq Joty", "Junnan Li", "Tat-Seng Chua", "Michael Qizhe Shieh", "Wenxuan Zhang"], "summary": "As large language models (LLMs) continue to advance, their capacity to\nfunction effectively across a diverse range of languages has shown marked\nimprovement. Preliminary studies observe that the hidden activations of LLMs\noften resemble English, even when responding to non-English prompts. This has\nled to the widespread assumption that LLMs may \"think\" in English. However,\nmore recent results showing strong multilingual performance, even surpassing\nEnglish performance on specific tasks in other languages, challenge this view.\nIn this work, we find that LLMs progressively develop a core language-agnostic\nparameter space-a remarkably small subset of parameters whose deactivation\nresults in significant performance degradation across all languages. This\ncompact yet critical set of parameters underlies the model's ability to\ngeneralize beyond individual languages, supporting the emergence of abstract\nthought that is not tied to any specific linguistic system. Specifically, we\nidentify language-related neurons-those are consistently activated during the\nprocessing of particular languages, and categorize them as either shared\n(active across multiple languages) or exclusive (specific to one). As LLMs\nundergo continued development over time, we observe a marked increase in both\nthe proportion and functional importance of shared neurons, while exclusive\nneurons progressively diminish in influence. These shared neurons constitute\nthe backbone of the core language-agnostic parameter space, supporting the\nemergence of abstract thought. Motivated by these insights, we propose\nneuron-specific training strategies tailored to LLMs' language-agnostic levels\nat different development stages. Experiments across diverse LLM families\nsupport our approach.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.09890v1", "AI": {"title_translation": "大型语言模型中超越任何语言的抽象思维的出现", "tldr": "大型语言模型（LLMs）发展出一种核心的语言无关参数空间，由共享神经元构成，支持超越特定语言的抽象思维，挑战了LLMs“用英语思考”的普遍假设。", "motivation": "挑战了LLMs可能“用英语思考”的普遍假设，并探索LLMs如何实现跨语言泛化，以及抽象思维的出现机制。", "method": "识别并区分了语言相关神经元为共享神经元（跨多种语言活跃）和排他性神经元（特定于一种语言）。观察了LLMs发展过程中共享神经元比例和功能重要性的增加，以及排他性神经元影响的减弱。基于这些发现，提出了针对LLMs不同发展阶段语言无关水平的神经元特定训练策略。", "result": "LLMs逐步发展出一个核心的语言无关参数空间，这是一小部分关键参数，其失效会导致所有语言的显著性能下降。共享神经元（跨多种语言活跃）的比例和功能重要性显著增加，而排他性神经元（特定于一种语言）的影响逐渐减弱。这些共享神经元构成了核心语言无关参数空间的骨干。", "conclusion": "大型语言模型（LLMs）能够发展出一种核心的语言无关参数空间，这表明LLMs的抽象思维能力不依赖于任何特定的语言系统，而是由共享神经元网络支持的。", "translation": "随着大型语言模型（LLMs）的不断发展，它们在各种语言中有效运作的能力显示出显著的提升。初步研究观察到，LLMs的隐藏激活通常类似于英语，即使在响应非英语提示时也是如此。这导致了LLMs可能“用英语思考”的普遍假设。然而，最近的结果显示出强大的多语言性能，甚至在其他语言的特定任务上超越了英语性能，这挑战了这一观点。在这项工作中，我们发现LLMs逐步发展出一个核心的语言无关参数空间——一个极小的参数子集，其停用会导致所有语言的显著性能下降。这个紧凑但关键的参数集是模型超越个体语言泛化能力的基础，支持了不依赖于任何特定语言系统的抽象思维的出现。具体来说，我们识别了语言相关神经元——那些在特定语言处理过程中持续激活的神经元，并将它们分为共享神经元（在多种语言中活跃）或排他性神经元（特定于一种语言）。随着LLMs的持续发展，我们观察到共享神经元的比例和功能重要性显著增加，而排他性神经元的影响逐渐减弱。这些共享神经元构成了核心语言无关参数空间的骨干，支持了抽象思维的出现。受这些见解的启发，我们提出了针对LLMs在不同发展阶段的语言无关水平量身定制的神经元特定训练策略。跨不同LLM家族的实验支持了我们的方法。", "summary": "该研究发现，大型语言模型（LLMs）在发展过程中逐渐形成一个核心的语言无关参数空间，该空间由在多种语言处理中活跃的“共享神经元”构成。这些共享神经元随时间推移在比例和功能上变得更为重要，而特定于单一语言的神经元则影响力减弱。这一发现挑战了LLMs“用英语思考”的假设，并表明LLMs的抽象思维能力能够超越特定语言系统。基于此，论文提出了针对不同发展阶段LLMs语言无关水平的神经元特定训练策略。", "keywords": "大型语言模型, 抽象思维, 语言无关, 共享神经元, 跨语言泛化", "comments": "这篇论文通过识别并分析LLMs内部的“共享神经元”和“排他性神经元”，提出了一个关于LLMs如何实现跨语言泛化和抽象思维出现的新颖视角。它挑战了传统上认为LLMs可能“用英语思考”的观点，揭示了模型内部存在一个语言无关的核心参数空间，这对于理解LLMs的认知机制和未来多语言模型的设计具有重要意义。提出神经元特定训练策略也展示了其潜在的应用价值。"}}
{"id": "2506.09441", "title": "Attention-Bayesian Hybrid Approach to Modular Multiple Particle Tracking", "authors": ["Piyush Mishra", "Philippe Roudot"], "summary": "Tracking multiple particles in noisy and cluttered scenes remains challenging\ndue to a combinatorial explosion of trajectory hypotheses, which scales\nsuper-exponentially with the number of particles and frames. The transformer\narchitecture has shown a significant improvement in robustness against this\nhigh combinatorial load. However, its performance still falls short of the\nconventional Bayesian filtering approaches in scenarios presenting a reduced\nset of trajectory hypothesis. This suggests that while transformers excel at\nnarrowing down possible associations, they may not be able to reach the\noptimality of the Bayesian approach in locally sparse scenario. Hence, we\nintroduce a hybrid tracking framework that combines the ability of\nself-attention to learn the underlying representation of particle behavior with\nthe reliability and interpretability of Bayesian filtering. We perform\ntrajectory-to-detection association by solving a label prediction problem,\nusing a transformer encoder to infer soft associations between detections\nacross frames. This prunes the hypothesis set, enabling efficient\nmultiple-particle tracking in Bayesian filtering framework. Our approach\ndemonstrates improved tracking accuracy and robustness against spurious\ndetections, offering a solution for high clutter multiple particle tracking\nscenarios.", "comment": null, "cate": "stat.ML", "url": "http://arxiv.org/abs/2506.09441v1", "AI": {"title_translation": "注意力-贝叶斯混合方法用于模块化多粒子跟踪", "tldr": "一种结合注意力机制和贝叶斯滤波的混合方法，提高了多粒子跟踪在嘈杂环境中的准确性和鲁棒性。", "motivation": "多粒子跟踪在嘈杂和杂乱场景中面临轨迹假设的组合爆炸问题。Transformer架构虽然在处理高组合负载方面有显著改进，但在轨迹假设较少的情况下，其性能仍不如传统的贝叶斯滤波方法，这表明Transformer在缩小可能关联范围方面表现出色，但在局部稀疏场景中可能无法达到贝叶斯方法的最佳性能。因此，需要一种结合两者优点的混合方法。", "method": "本文引入了一种混合跟踪框架，该框架结合了自注意力学习粒子行为底层表示的能力与贝叶斯滤波的可靠性和可解释性。通过解决标签预测问题来执行轨迹到检测的关联，使用Transformer编码器推断跨帧检测之间的软关联。这修剪了假设集，从而在贝叶斯滤波框架中实现了高效的多粒子跟踪。", "result": "该方法在跟踪精度和对抗虚假检测的鲁棒性方面均有所提高。", "conclusion": "该混合方法为高杂乱多粒子跟踪场景提供了一种解决方案，证明了其在提高跟踪精度和鲁棒性方面的有效性。", "translation": "在嘈杂和杂乱的场景中跟踪多个粒子仍然具有挑战性，这是由于轨迹假设的组合爆炸，其规模随粒子和帧的数量呈超指数级增长。Transformer架构在应对这种高组合负载方面表现出显著的鲁棒性改进。然而，在轨迹假设集减少的场景中，其性能仍低于传统的贝叶斯滤波方法。这表明，虽然Transformer擅长缩小可能的关联范围，但在局部稀疏场景中可能无法达到贝叶斯方法的最佳性能。因此，我们引入了一种混合跟踪框架，该框架结合了自注意力学习粒子行为底层表示的能力与贝叶斯滤波的可靠性和可解释性。我们通过解决标签预测问题来执行轨迹到检测的关联，使用Transformer编码器推断跨帧检测之间的软关联。这修剪了假设集，从而在贝叶斯滤波框架中实现了高效的多粒子跟踪。我们的方法在跟踪精度和对抗虚假检测的鲁棒性方面均有所提高，为高杂乱多粒子跟踪场景提供了解决方案。", "summary": "本文提出了一种注意力-贝叶斯混合跟踪框架，旨在解决嘈杂和杂乱场景中多粒子跟踪的挑战。该方法结合了Transformer架构在处理高组合负载方面的优势和贝叶斯滤波在局部稀疏场景中的最优性。通过使用Transformer编码器进行轨迹到检测的软关联，该框架有效地修剪了假设集，从而在贝叶斯滤波框架内实现了高效且准确的多粒子跟踪。实验结果表明，该方法提高了跟踪精度，并增强了对虚假检测的鲁棒性，为复杂的多粒子跟踪场景提供了有效解决方案。", "keywords": "多粒子跟踪, 贝叶斯滤波, Transformer, 混合方法, 注意力机制", "comments": "这篇论文的创新点在于成功地将Transformer的注意力机制与传统的贝叶斯滤波相结合，弥补了单一方法在不同场景下的局限性。Transformer擅长处理复杂的全局关联，而贝叶斯滤波则在局部稀疏场景中表现出最优性。这种混合方法利用Transformer来有效地剪枝轨迹假设空间，从而使贝叶斯滤波能够更高效地工作，这为解决多粒子跟踪中的组合爆炸问题提供了一种新颖且实用的途径，具有重要的理论和应用价值。"}}
{"id": "2506.09457", "title": "Towards Bridging the Reward-Generation Gap in Direct Alignment Algorithms", "authors": ["Zeguan Xiao", "Yun Chen", "Guanhua Chen"], "summary": "Direct Alignment Algorithms (DAAs), such as Direct Preference Optimization\n(DPO) and Simple Preference Optimization (SimPO), have emerged as efficient\nalternatives to Reinforcement Learning from Human Feedback (RLHF) algorithms\nfor aligning large language models (LLMs) with human preferences. However, DAAs\nsuffer from a fundamental limitation we identify as the \"reward-generation gap\"\n-- a misalignment between optimization objectives during training and actual\ngeneration performance during inference. In this paper, we find a contributor\nto the reward-generation gap is the mismatch between the inherent importance of\nprefix tokens during the LLM generation process and how this importance is\nreflected in the implicit reward functions of DAAs. To bridge the gap, we\nintroduce a simple yet effective approach called Prefix-Oriented Equal-length\nTraining (POET), which truncates both preferred and dispreferred responses to\nmatch the shorter one's length. Training with POET, where both responses in\neach sample are truncated to equal length, resulting in diverse truncated\nlengths across samples, the optimization of DAAs objective is implicitly\nconstrained to converge across all positions, thus paying more attention to\nprefix tokens than the standard DAAs. We conduct experiments with DPO and\nSimPO, two representative DAAs, demonstrating that POET improves over their\nstandard implementations, achieving up to 15.6 points in AlpacaEval 2 and\noverall improvements across downstream tasks. Our results highlight the\nimportance of addressing the misalignment between reward optimization and\ngeneration performance in DAAs.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.09457v1", "AI": {"title_translation": "弥合直接对齐算法中奖励-生成差距的探索", "tldr": "直接对齐算法（DAA）存在“奖励-生成差距”，即训练目标与实际生成性能不一致。本文提出POET方法，通过截断等长训练，使DAA更关注前缀词元，从而弥合这一差距，显著提升性能。", "motivation": "直接对齐算法（DAAs）在对齐大型语言模型（LLMs）与人类偏好方面是RLHF的有效替代方案，但存在一个根本性限制，即“奖励-生成差距”——训练时的优化目标与推理时的实际生成性能之间存在错位。本文发现，前缀词元在LLM生成过程中的固有重要性与DAAs隐式奖励函数中反映出的重要性不匹配，是导致这一差距的原因。", "method": "为了弥合奖励-生成差距，本文提出了一种名为“前缀导向等长训练”（Prefix-Oriented Equal-length Training, POET）的简单有效方法。POET通过将偏好和非偏好响应都截断到较短响应的长度。在POET训练中，每个样本中的两个响应都被截断到等长，从而导致不同样本有不同的截断长度，这隐式地约束了DAAs目标的优化在所有位置上收敛，从而使DAAs比标准方法更关注前缀词元。", "result": "在DPO和SimPO这两种代表性DAAs上的实验表明，POET相对于它们的标准实现有显著改进，在AlpacaEval 2上提升高达15.6分，并在下游任务中实现全面改进。", "conclusion": "研究结果强调了在直接对齐算法中解决奖励优化与生成性能之间错位的重要性。", "translation": "直接对齐算法（DAAs），例如直接偏好优化（DPO）和简单偏好优化（SimPO），已成为强化学习从人类反馈（RLHF）算法的有效替代方案，用于将大型语言模型（LLMs）与人类偏好对齐。然而，DAAs面临着一个我们称之为“奖励-生成差距”的根本性限制——训练期间的优化目标与推理期间的实际生成性能之间存在错位。在本文中，我们发现导致奖励-生成差距的一个因素是LLM生成过程中前缀词元的固有重要性与DAAs隐式奖励函数中反映出的重要性之间存在不匹配。为了弥合这一差距，我们引入了一种简单而有效的方法，称为前缀导向等长训练（POET），它将偏好和非偏好响应都截断到较短响应的长度。使用POET进行训练时，每个样本中的两个响应都被截断到等长，从而导致不同样本有不同的截断长度，这隐式地约束了DAAs目标的优化在所有位置上收敛，从而比标准DAAs更关注前缀词元。我们对DPO和SimPO这两种代表性DAAs进行了实验，结果表明POET相对于它们的标准实现有所改进，在AlpacaEval 2上提升高达15.6分，并在下游任务中实现全面改进。我们的结果强调了解决DAAs中奖励优化与生成性能之间错位的重要性。", "summary": "本文关注直接对齐算法（DAAs）中存在的“奖励-生成差距”，即训练优化目标与实际生成性能不一致的问题。作者发现该差距部分源于前缀词元重要性在DAAs隐式奖励函数中未能充分体现。为解决此问题，论文提出了一种名为“前缀导向等长训练”（POET）的新方法。POET通过将偏好和非偏好响应截断至较短者长度进行训练，促使DAAs更关注前缀词元。实验结果表明，POET显著提升了DPO和SimPO等DAAs在AlpacaEval 2及其他下游任务上的表现，验证了弥合奖励-生成差距的重要性。", "keywords": "直接对齐算法, 奖励-生成差距, 大语言模型, 前缀导向等长训练, DPO, SimPO", "comments": "本文创新性地指出了直接对齐算法中存在的“奖励-生成差距”，并将其归因于前缀词元重要性在奖励函数中的体现不足。提出的POET方法简单而有效，通过等长截断训练，巧妙地引导模型关注前缀，解决了这一关键问题。实验结果的显著提升证明了其有效性，为DAAs的进一步优化提供了新的方向，对于提升LLM的对齐效果具有重要意义。"}}
{"id": "2506.09902", "title": "PersonaLens: A Benchmark for Personalization Evaluation in Conversational AI Assistants", "authors": ["Zheng Zhao", "Clara Vania", "Subhradeep Kayal", "Naila Khan", "Shay B. Cohen", "Emine Yilmaz"], "summary": "Large language models (LLMs) have advanced conversational AI assistants.\nHowever, systematically evaluating how well these assistants apply\npersonalization--adapting to individual user preferences while completing\ntasks--remains challenging. Existing personalization benchmarks focus on\nchit-chat, non-conversational tasks, or narrow domains, failing to capture the\ncomplexities of personalized task-oriented assistance. To address this, we\nintroduce PersonaLens, a comprehensive benchmark for evaluating personalization\nin task-oriented AI assistants. Our benchmark features diverse user profiles\nequipped with rich preferences and interaction histories, along with two\nspecialized LLM-based agents: a user agent that engages in realistic\ntask-oriented dialogues with AI assistants, and a judge agent that employs the\nLLM-as-a-Judge paradigm to assess personalization, response quality, and task\nsuccess. Through extensive experiments with current LLM assistants across\ndiverse tasks, we reveal significant variability in their personalization\ncapabilities, providing crucial insights for advancing conversational AI\nsystems.", "comment": "Accepted to ACL 2025 Findings", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.09902v1", "AI": {"title_translation": "PersonaLens：对话式AI助手中个性化评估的基准", "tldr": "PersonaLens是一个用于评估对话式AI助手个性化能力的综合基准，通过模拟用户对话和LLM裁判来揭示现有模型的个性化能力差异，为推进对话式AI系统提供关键见解。", "motivation": "现有个性化基准侧重于闲聊、非对话任务或狭窄领域，未能捕捉个性化任务导向型辅助的复杂性，导致难以系统评估大型语言模型（LLMs）在个性化方面的表现。", "method": "引入PersonaLens，一个用于评估任务导向型AI助手个性化的综合基准。该基准包含多样化的用户配置文件（配备丰富的偏好和交互历史），以及两个专门的LLM-based代理：一个用户代理（与AI助手进行真实的任务导向型对话），和一个裁判代理（采用“LLM即法官”范式评估个性化、响应质量和任务成功）。", "result": "通过对当前LLM助手在不同任务中的广泛实验，揭示了它们个性化能力的显著差异。", "conclusion": "PersonaLens为推进对话式AI系统提供了关键见解。", "translation": "大型语言模型（LLMs）推动了对话式AI助手的发展。然而，系统地评估这些助手在完成任务时如何应用个性化——即适应个体用户偏好——仍然具有挑战性。现有的个性化基准侧重于闲聊、非对话任务或狭窄领域，未能捕捉个性化任务导向型辅助的复杂性。为了解决这个问题，我们引入了PersonaLens，一个用于评估任务导向型AI助手个性化的综合基准。我们的基准具有多样化的用户配置文件，配备丰富的偏好和交互历史，以及两个专门的基于LLM的代理：一个用户代理，与AI助手进行真实的任务导向型对话；以及一个裁判代理，采用“LLM即法官”范式来评估个性化、响应质量和任务成功。通过对当前LLM助手在不同任务中的广泛实验，我们揭示了它们个性化能力的显著差异，为推进对话式AI系统提供了关键见解。", "summary": "该论文介绍了PersonaLens，一个旨在解决现有基准局限性的新型综合基准，用于评估对话式AI助手在任务导向型场景中的个性化能力。PersonaLens通过结合多样化的用户画像、模拟真实用户对话和利用LLM作为裁判来评估模型的个性化适应性、响应质量和任务完成度。实验结果表明，当前LLM助手在个性化能力上存在显著差异，为未来对话式AI系统的发展提供了重要方向。", "keywords": "个性化评估, 对话式AI, LLM, 基准, 任务导向型助手", "comments": "PersonaLens的创新之处在于其综合性，通过结合多样化的用户画像、模拟用户对话和LLM作为裁判的范式，更全面地评估了任务导向型AI助手的个性化能力，弥补了现有基准的不足。这对于推动更具个性化、更实用的对话式AI系统具有重要意义。"}}
{"id": "2506.09495", "title": "Bridging Online Behavior and Clinical Insight: A Longitudinal LLM-based Study of Suicidality on YouTube Reveals Novel Digital Markers", "authors": ["Ilanit Sobol", "Shir Lissak", "Refael Tikochinski", "Tal Nakash", "Anat Brunstein Klomek", "Eyal Fruchter", "Roi Reichart"], "summary": "Suicide remains a leading cause of death in Western countries, underscoring\nthe need for new research approaches. As social media becomes central to daily\nlife, digital footprints offer valuable insight into suicidal behavior.\nFocusing on individuals who attempted suicide while uploading videos to their\nchannels, we investigate: How do suicidal behaviors manifest on YouTube, and\nhow do they differ from expert knowledge? We applied complementary approaches:\ncomputational bottom-up, hybrid, and expert-driven top-down, on a novel\nlongitudinal dataset of 181 YouTube channels from individuals with\nlife-threatening attempts, alongside 134 control channels. In the bottom-up\napproach, we applied LLM-based topic modeling to identify behavioral\nindicators. Of 166 topics, five were associated with suicide-attempt, with two\nalso showing temporal attempt-related changes ($p<.01$) - Mental Health\nStruggles ($+0.08$)* and YouTube Engagement ($+0.1$)*. In the hybrid approach,\na clinical expert reviewed LLM-derived topics and flagged 19 as\nsuicide-related. However, none showed significant attempt-related temporal\neffects beyond those identified bottom-up. Notably, YouTube Engagement, a\nplatform-specific indicator, was not flagged by the expert, underscoring the\nvalue of bottom-up discovery. In the top-down approach, psychological\nassessment of suicide attempt narratives revealed that the only significant\ndifference between individuals who attempted before and those attempted during\ntheir upload period was the motivation to share this experience: the former\naimed to Help Others ($\\beta=-1.69$, $p<.01$), while the latter framed it as\npart of their Personal Recovery ($\\beta=1.08$, $p<.01$). By integrating these\napproaches, we offer a nuanced understanding of suicidality, bridging digital\nbehavior and clinical insights.\n  * Within-group changes in relation to the suicide attempt.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.09495v1", "AI": {"title_translation": "弥合在线行为与临床洞察：一项基于LLM的YouTube自杀倾向纵向研究揭示了新的数字标记", "tldr": "本研究利用LLM对YouTube上自杀倾向个体的视频数据进行纵向分析，揭示了新的数字标记，如心理健康斗争和YouTube参与度，并发现分享经历动机的差异，强调了数字行为与临床洞察的结合。", "motivation": "自杀是西方国家主要的死亡原因，亟需新的研究方法。社交媒体上的数字足迹为理解自杀行为提供了宝贵见解。本研究旨在探讨自杀行为如何在YouTube上表现，以及它们与专家知识有何不同。", "method": "本研究对一个包含181个有危及生命尝试的YouTube频道和134个对照频道的新型纵向数据集，应用了互补的方法：计算自下而上（LLM主题建模）、混合（临床专家审查LLM主题）以及专家驱动自上而下（心理评估自杀尝试叙事）。", "result": "自下而上的方法发现，在166个主题中，有5个与自杀尝试相关，其中“心理健康斗争”和“YouTube参与度”显示出与尝试相关的时间性变化。值得注意的是，YouTube参与度这一平台特有指标未被专家标记，凸显了自下而上发现的价值。混合方法中，专家标记了19个与自杀相关的主题，但未发现显著的时序效应。自上而下的方法揭示，自杀尝试者分享经历的动机存在显著差异：之前尝试者旨在帮助他人，而上传期间尝试者则将其视为个人康复的一部分。", "conclusion": "通过整合自下而上、混合和自上而下三种方法，本研究对自杀倾向提供了细致入微的理解，成功弥合了数字行为分析与临床洞察之间的鸿沟。", "translation": "自杀仍然是西方国家主要的死亡原因，这凸显了对新研究方法的迫切需求。随着社交媒体成为日常生活的中心，数字足迹为自杀行为提供了宝贵的见解。我们关注那些在上传视频到其频道期间曾尝试自杀的个体，并调查：自杀行为如何在YouTube上表现，以及它们与专家知识有何不同？我们对一个包含181个有危及生命尝试的个体YouTube频道以及134个对照频道的新型纵向数据集，应用了互补的方法：计算自下而上、混合以及专家驱动自上而下。在自下而上的方法中，我们应用基于LLM的主题建模来识别行为指标。在166个主题中，有5个与自杀尝试相关，其中2个还显示出与尝试相关的时间性变化（$p<.01$）——心理健康斗争（$+0.08$）* 和YouTube参与度（$+0.1$）*。在混合方法中，一位临床专家审查了LLM衍生的主题，并标记了19个为与自杀相关。然而，除了自下而上识别出的主题外，没有其他主题显示出显著的与尝试相关的时间性影响。值得注意的是，YouTube参与度这一平台特有的指标并未被专家标记，这强调了自下而上发现的价值。在自上而下的方法中，对自杀尝试叙事的心理评估揭示，在上传视频期间尝试自杀的个体与之前尝试自杀的个体之间，唯一的显著差异在于分享这种经历的动机：前者旨在帮助他人（$\beta=-1.69$， $p<.01$），而后者则将其视为个人康复的一部分（$\beta=1.08$， $p<.01$）。通过整合这些方法，我们对自杀倾向提供了细致的理解，弥合了数字行为和临床洞察。* 组内与自杀尝试相关的变化。", "summary": "本研究利用LLM对YouTube上自杀倾向个体的视频数据进行纵向分析，旨在识别新的数字标记并理解其与临床洞察的差异。通过自下而上、混合和自上而下三种方法，研究发现“心理健康斗争”和“YouTube参与度”是与自杀尝试相关的时序变化指标，且“YouTube参与度”是自下而上发现的独特标记。此外，研究还揭示了自杀尝试者分享经历的动机差异。该研究为理解自杀行为提供了新的数字视角和临床见解。", "keywords": "自杀倾向, LLM, YouTube, 数字标记, 纵向研究", "comments": "这项研究创新性地将LLM应用于社交媒体数据，以识别自杀倾向的数字标记，尤其强调了“自下而上”数据驱动方法发现新指标（如YouTube参与度）的价值，弥补了传统专家知识的不足。其纵向研究设计和多方法整合也增强了结果的稳健性和对自杀倾向的理解深度，为公共卫生干预提供了潜在的新方向。"}}
{"id": "2506.09516", "title": "LLM-Powered CPI Prediction Inference with Online Text Time Series", "authors": ["Yingying Fan", "Jinchi Lv", "Ao Sun", "Yurou Wang"], "summary": "Forecasting the Consumer Price Index (CPI) is an important yet challenging\ntask in economics, where most existing approaches rely on low-frequency,\nsurvey-based data. With the recent advances of large language models (LLMs),\nthere is growing potential to leverage high-frequency online text data for\nimproved CPI prediction, an area still largely unexplored. This paper proposes\nLLM-CPI, an LLM-based approach for CPI prediction inference incorporating\nonline text time series. We collect a large set of high-frequency online texts\nfrom a popularly used Chinese social network site and employ LLMs such as\nChatGPT and the trained BERT models to construct continuous inflation labels\nfor posts that are related to inflation. Online text embeddings are extracted\nvia LDA and BERT. We develop a joint time series framework that combines\nmonthly CPI data with LLM-generated daily CPI surrogates. The monthly model\nemploys an ARX structure combining observed CPI data with text embeddings and\nmacroeconomic variables, while the daily model uses a VARX structure built on\nLLM-generated CPI surrogates and text embeddings. We establish the asymptotic\nproperties of the method and provide two forms of constructed prediction\nintervals. The finite-sample performance and practical advantages of LLM-CPI\nare demonstrated through both simulation and real data examples.", "comment": "73 pages, 13 figures", "cate": "stat.ML", "url": "http://arxiv.org/abs/2506.09516v1", "AI": {"title_translation": "基于大型语言模型的CPI预测推理与在线文本时间序列", "tldr": "本文提出了LLM-CPI，一种基于大型语言模型（LLM）的方法，利用高频在线文本数据来改进消费者物价指数（CPI）预测。该方法结合了LLM生成的每日CPI替代数据与传统的月度CPI数据，并采用联合时间序列框架进行预测。", "motivation": "消费者物价指数（CPI）预测是经济学中一项重要但具有挑战性的任务，现有方法大多依赖于低频、基于调查的数据。随着大型语言模型（LLM）的最新进展，利用高频在线文本数据改进CPI预测的潜力日益增长，而这一领域在很大程度上仍未被探索。", "method": "本文提出了LLM-CPI，一种结合在线文本时间序列的基于LLM的CPI预测推理方法。具体步骤包括：从流行的中国社交网站收集大量高频在线文本；使用LLM（如ChatGPT和训练过的BERT模型）为与通货膨胀相关的帖子构建连续的通货膨胀标签；通过LDA和BERT提取在线文本嵌入；开发一个联合时间序列框架，将月度CPI数据与LLM生成的每日CPI替代数据相结合。其中，月度模型采用结合观测CPI数据、文本嵌入和宏观经济变量的ARX结构，而每日模型则使用基于LLM生成的CPI替代数据和文本嵌入的VARX结构。此外，本文还建立了该方法的渐近性质，并提供了两种形式的预测区间。", "result": "通过模拟和真实数据示例，验证了LLM-CPI的有限样本性能和实际优势。", "conclusion": "LLM-CPI方法通过结合大型语言模型和高频在线文本数据，显著提升了CPI预测的准确性和实用性。", "translation": "消费者物价指数（CPI）预测是经济学中一项重要但具有挑战性的任务，现有方法大多依赖于低频、基于调查的数据。随着大型语言模型（LLM）的最新进展，利用高频在线文本数据改进CPI预测的潜力日益增长，而这一领域在很大程度上仍未被探索。本文提出了LLM-CPI，一种基于LLM的CPI预测推理方法，它结合了在线文本时间序列。我们从一个流行的中国社交网站收集了大量高频在线文本，并利用ChatGPT和经过训练的BERT模型等LLM为与通货膨胀相关的帖子构建连续的通货膨胀标签。在线文本嵌入通过LDA和BERT提取。我们开发了一个联合时间序列框架，将月度CPI数据与LLM生成的每日CPI替代数据相结合。月度模型采用结合观测CPI数据、文本嵌入和宏观经济变量的ARX结构，而每日模型则使用基于LLM生成的CPI替代数据和文本嵌入的VARX结构。我们建立了该方法的渐近性质，并提供了两种形式的构建预测区间。通过模拟和真实数据示例，证明了LLM-CPI的有限样本性能和实际优势。", "summary": "本文提出LLM-CPI，一种创新的基于大型语言模型（LLM）的消费者物价指数（CPI）预测方法。该方法利用高频在线文本数据，通过LLM（如ChatGPT和BERT）生成通货膨胀标签和CPI替代数据。它构建了一个联合时间序列框架，结合了传统的月度CPI数据和LLM生成的每日CPI替代数据，并分别采用ARX和VARX模型。研究通过模拟和真实数据验证了LLM-CPI在提高CPI预测准确性方面的有效性和实用性。", "keywords": "CPI预测, 大型语言模型, 在线文本数据, 时间序列, 通货膨胀", "comments": "本文的创新点在于首次将大型语言模型与高频在线文本数据相结合，应用于消费者物价指数（CPI）预测，这为经济预测领域开辟了新的视角。通过构建LLM生成的每日CPI替代数据并将其整合到联合时间序列框架中，该方法有效地弥补了传统低频数据预测的不足，并有望提高预测的实时性和准确性。"}}
{"id": "2506.09942", "title": "VerIF: Verification Engineering for Reinforcement Learning in Instruction Following", "authors": ["Hao Peng", "Yunjia Qi", "Xiaozhi Wang", "Bin Xu", "Lei Hou", "Juanzi Li"], "summary": "Reinforcement learning with verifiable rewards (RLVR) has become a key\ntechnique for enhancing large language models (LLMs), with verification\nengineering playing a central role. However, best practices for RL in\ninstruction following remain underexplored. In this work, we explore the\nverification challenge in RL for instruction following and propose VerIF, a\nverification method that combines rule-based code verification with LLM-based\nverification from a large reasoning model (e.g., QwQ-32B). To support this\napproach, we construct a high-quality instruction-following dataset,\nVerInstruct, containing approximately 22,000 instances with associated\nverification signals. We apply RL training with VerIF to two models, achieving\nsignificant improvements across several representative instruction-following\nbenchmarks. The trained models reach state-of-the-art performance among models\nof comparable size and generalize well to unseen constraints. We further\nobserve that their general capabilities remain unaffected, suggesting that RL\nwith VerIF can be integrated into existing RL recipes to enhance overall model\nperformance. We have released our datasets, codes, and models to facilitate\nfuture research at https://github.com/THU-KEG/VerIF.", "comment": "16 pages, 8 figures", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.09942v1", "AI": {"title_translation": "VerIF: 指令遵循中强化学习的验证工程", "tldr": "本文提出了VerIF，一种结合规则和LLM验证的强化学习方法，用于指令遵循任务，并在多个基准测试中取得了显著改进，达到SOTA性能。", "motivation": "尽管可验证奖励强化学习（RLVR）在增强大型语言模型（LLMs）方面发挥着关键作用，但指令遵循中强化学习的最佳实践仍未被充分探索，尤其是在验证挑战方面。", "method": "本文提出了VerIF，一种结合了基于规则的代码验证和来自大型推理模型（如QwQ-32B）的LLM验证的方法。为支持此方法，构建了一个包含约22,000个实例及相关验证信号的高质量指令遵循数据集VerInstruct。", "result": "将VerIF应用于两个模型进行RL训练，在多个代表性指令遵循基准测试中取得了显著改进。训练后的模型在同等规模模型中达到最先进的性能，并能很好地泛化到未见的约束。此外，模型的通用能力未受影响。", "conclusion": "VerIF与强化学习的结合可以集成到现有RL方案中，以提升整体模型性能。", "translation": "可验证奖励强化学习（RLVR）已成为增强大型语言模型（LLM）的关键技术，其中验证工程发挥着核心作用。然而，指令遵循中强化学习的最佳实践仍未被充分探索。在这项工作中，我们探索了指令遵循中强化学习的验证挑战，并提出了VerIF，这是一种结合了基于规则的代码验证和来自大型推理模型（例如QwQ-32B）的基于LLM的验证的验证方法。为了支持这种方法，我们构建了一个高质量的指令遵循数据集VerInstruct，其中包含大约22,000个实例以及相关的验证信号。我们将VerIF应用于两个模型进行RL训练，在多个代表性指令遵循基准测试中取得了显著改进。训练后的模型在同等规模模型中达到了最先进的性能，并能很好地泛化到未见的约束。我们进一步观察到它们的通用能力未受影响，这表明结合VerIF的强化学习可以集成到现有的RL方案中，以提升整体模型性能。我们已发布了我们的数据集、代码和模型，以促进未来的研究，网址为https://github.com/THU-KEG/VerIF。", "summary": "本文针对指令遵循中强化学习的验证挑战，提出了VerIF方法，该方法结合了规则验证和LLM验证。为支持VerIF，研究人员构建了高质量的VerInstruct数据集。实验结果表明，应用VerIF进行RL训练的模型在指令遵循基准测试中取得了显著提升，达到同等规模模型的SOTA性能，并展现出良好的泛化能力，同时不影响其通用能力。这表明VerIF可以有效增强现有RL模型的性能。", "keywords": "强化学习, 指令遵循, 验证工程, 大型语言模型, VerIF", "comments": "VerIF的创新之处在于其结合了规则验证和LLM验证的混合方法，有效解决了指令遵循中强化学习的验证难题。同时，构建高质量的VerInstruct数据集为该领域的研究提供了宝贵资源。该工作的重要性在于其显著提升了指令遵循任务中LLMs的性能，并为未来RL与验证工程的结合提供了新的范式。"}}
{"id": "2506.09640", "title": "Evasion Attacks Against Bayesian Predictive Models", "authors": ["Pablo G. Arce", "Roi Naveiro", "David Ríos Insua"], "summary": "There is an increasing interest in analyzing the behavior of machine learning\nsystems against adversarial attacks. However, most of the research in\nadversarial machine learning has focused on studying weaknesses against evasion\nor poisoning attacks to predictive models in classical setups, with the\nsusceptibility of Bayesian predictive models to attacks remaining\nunderexplored. This paper introduces a general methodology for designing\noptimal evasion attacks against such models. We investigate two adversarial\nobjectives: perturbing specific point predictions and altering the entire\nposterior predictive distribution. For both scenarios, we propose novel\ngradient-based attacks and study their implementation and properties in various\ncomputational setups.", "comment": "Accepted as an oral presentation at UAI'25", "cate": "stat.ML", "url": "http://arxiv.org/abs/2506.09640v1", "AI": {"title_translation": "针对贝叶斯预测模型的规避攻击", "tldr": "本文提出了一种针对贝叶斯预测模型的通用规避攻击方法，研究了两种攻击目标并提出了基于梯度的攻击。", "motivation": "现有对抗性机器学习研究主要集中于经典设置下的预测模型，而贝叶斯预测模型对攻击的易感性仍未得到充分探索。", "method": "论文提出了一种设计针对贝叶斯预测模型的最佳规避攻击的通用方法。研究了两种对抗性目标：扰动特定点预测和改变整个后验预测分布，并为这两种情况提出了新颖的基于梯度的攻击，同时研究了它们在各种计算设置中的实现和特性。", "result": "提出了新颖的基于梯度的攻击，并研究了其在各种计算设置中的实现和特性。", "conclusion": "论文为针对贝叶斯预测模型的规避攻击提供了一种通用方法，填补了该领域研究的空白。", "translation": "机器学习系统对抗对抗性攻击行为的分析引起了越来越多的关注。然而，对抗性机器学习的大部分研究都集中在经典设置下预测模型对规避或投毒攻击的弱点上，而贝叶斯预测模型对攻击的易感性仍未得到充分探索。本文介绍了一种设计针对此类模型的最佳规避攻击的通用方法。我们研究了两个对抗性目标：扰动特定的点预测和改变整个后验预测分布。对于这两种情况，我们提出了新颖的基于梯度的攻击，并研究了它们在各种计算设置中的实现和特性。", "summary": "本文针对贝贝斯预测模型在对抗性攻击研究中未被充分探索的问题，提出了一种设计最优规避攻击的通用方法。该方法考虑了扰动特定点预测和改变整个后验预测分布两种攻击目标，并为此提出了新颖的基于梯度的攻击策略，同时研究了其在不同计算设置下的实现和性质。", "keywords": "对抗性攻击, 贝叶斯预测模型, 规避攻击, 梯度攻击, 机器学习", "comments": "这篇论文通过提出针对贝叶斯预测模型的规避攻击方法，填补了对抗性机器学习领域的一个研究空白，具有重要的理论和实践意义。其创新之处在于将对抗性攻击研究扩展到贝叶斯模型，并提出了通用的梯度攻击方法。"}}
{"id": "2506.09648", "title": "Scaling Laws for Uncertainty in Deep Learning", "authors": ["Mattia Rosso", "Simone Rossi", "Giulio Franzese", "Markus Heinonen", "Maurizio Filippone"], "summary": "Deep learning has recently revealed the existence of scaling laws,\ndemonstrating that model performance follows predictable trends based on\ndataset and model sizes. Inspired by these findings and fascinating phenomena\nemerging in the over-parameterized regime, we examine a parallel direction: do\nsimilar scaling laws govern predictive uncertainties in deep learning? In\nidentifiable parametric models, such scaling laws can be derived in a\nstraightforward manner by treating model parameters in a Bayesian way. In this\ncase, for example, we obtain $O(1/N)$ contraction rates for epistemic\nuncertainty with respect to the number of data $N$. However, in\nover-parameterized models, these guarantees do not hold, leading to largely\nunexplored behaviors. In this work, we empirically show the existence of\nscaling laws associated with various measures of predictive uncertainty with\nrespect to dataset and model sizes. Through experiments on vision and language\ntasks, we observe such scaling laws for in- and out-of-distribution predictive\nuncertainty estimated through popular approximate Bayesian inference and\nensemble methods. Besides the elegance of scaling laws and the practical\nutility of extrapolating uncertainties to larger data or models, this work\nprovides strong evidence to dispel recurring skepticism against Bayesian\napproaches: \"In many applications of deep learning we have so much data\navailable: what do we need Bayes for?\". Our findings show that \"so much data\"\nis typically not enough to make epistemic uncertainty negligible.", "comment": null, "cate": "stat.ML", "url": "http://arxiv.org/abs/2506.09648v1", "AI": {"title_translation": "深度学习中不确定性的标度律", "tldr": "深度学习中预测不确定性也存在标度律，且即使数据量很大，认知不确定性依然不可忽略。", "motivation": "现有研究发现深度学习模型性能存在标度律，受此启发，本文旨在探索预测不确定性是否存在类似的标度律，并解决对贝叶斯方法的普遍质疑。", "method": "通过在视觉和语言任务上进行实验，经验性地展示了数据集大小和模型大小对预测不确定性（包括分布内和分布外）的标度律，使用了近似贝叶斯推断和集成方法来估计不确定性。", "result": "经验性地证明了预测不确定性（包括分布内和分布外）存在与数据集和模型大小相关的标度律，这些不确定性通过近似贝叶斯推断和集成方法估计。研究还表明，即使在数据量很大的情况下，认知不确定性通常也无法忽略。", "conclusion": "预测不确定性在深度学习中同样遵循标度律，并且即使数据量庞大，认知不确定性也依然显著，这为贝叶斯方法提供了强有力的支持。", "translation": "深度学习最近揭示了标度律的存在，表明模型性能遵循基于数据集和模型大小的可预测趋势。受这些发现以及在超参数化状态下出现的引人入胜的现象的启发，我们研究了一个平行方向：深度学习中的预测不确定性是否也遵循类似的标度律？在可识别的参数模型中，可以通过贝叶斯方式处理模型参数来直接推导出此类标度律。在这种情况下，例如，我们获得了关于数据量 N 的认知不确定性的 O(1/N) 收缩率。然而，在超参数化模型中，这些保证不成立，导致了很大程度上未被探索的行为。在这项工作中，我们通过经验证明了与数据集和模型大小相关的各种预测不确定性度量的标度律的存在。通过在视觉和语言任务上的实验，我们观察到通过流行的近似贝叶斯推断和集成方法估计的分布内和分布外预测不确定性的此类标度律。除了标度律的优雅性以及将不确定性外推到更大数据或模型的实用性之外，这项工作还提供了强有力的证据来消除对贝叶斯方法反复出现的怀疑：“在深度学习的许多应用中，我们有如此多的可用数据：我们为什么需要贝叶斯？”我们的发现表明，“如此多的数据”通常不足以使认知不确定性可以忽略不计。", "summary": "本文研究了深度学习中预测不确定性的标度律，发现与模型性能类似，不确定性也遵循可预测的趋势，与数据集和模型大小相关。通过在视觉和语言任务上的实验，作者经验性地证明了分布内和分布外不确定性的标度律，并指出即使在数据量很大的情况下，认知不确定性也通常不可忽略，从而为贝叶斯方法提供了支持。", "keywords": "深度学习, 标度律, 不确定性, 贝叶斯推断, 认知不确定性", "comments": "这项工作创新性地将标度律的概念扩展到了深度学习中的不确定性领域，揭示了不确定性行为的可预测性。其重要性在于，它不仅提供了理解和外推不确定性的实用工具，还通过经验证据有力地反驳了对贝叶斯方法在“大数据”时代必要性的质疑，强调了即使数据量庞大，认知不确定性依然是需要关注的关键因素。"}}
{"id": "2506.09681", "title": "Assessing the Quality of Denoising Diffusion Models in Wasserstein Distance: Noisy Score and Optimal Bounds", "authors": ["Vahan Arsenyan", "Elen Vardanyan", "Arnak Dalalyan"], "summary": "Generative modeling aims to produce new random examples from an unknown\ntarget distribution, given access to a finite collection of examples. Among the\nleading approaches, denoising diffusion probabilistic models (DDPMs) construct\nsuch examples by mapping a Brownian motion via a diffusion process driven by an\nestimated score function. In this work, we first provide empirical evidence\nthat DDPMs are robust to constant-variance noise in the score evaluations. We\nthen establish finite-sample guarantees in Wasserstein-2 distance that exhibit\ntwo key features: (i) they characterize and quantify the robustness of DDPMs to\nnoisy score estimates, and (ii) they achieve faster convergence rates than\npreviously known results. Furthermore, we observe that the obtained rates match\nthose known in the Gaussian case, implying their optimality.", "comment": null, "cate": "stat.ML", "url": "http://arxiv.org/abs/2506.09681v1", "AI": {"title_translation": "在Wasserstein距离中评估去噪扩散模型的质量：噪声分数和最优界限", "tldr": "本文经验性地证明了去噪扩散概率模型（DDPMs）对噪声分数估计的鲁棒性，并在Wasserstein-2距离上建立了有限样本保证，实现了更快且最优的收敛速度。", "motivation": "生成建模旨在从未知目标分布中生成新的随机样本，而去噪扩散概率模型（DDPMs）是领先的方法之一。本研究的动机是评估DDPMs的质量，特别是量化其对噪声分数估计的鲁棒性，并提供更快的收敛率理论保证。", "method": "本文首先提供了经验证据，表明DDPMs对分数评估中的恒定方差噪声具有鲁棒性。随后，研究人员建立了在Wasserstein-2距离下的有限样本保证，这些保证表征并量化了DDPMs对噪声分数估计的鲁棒性，并实现了比先前结果更快的收敛速度。", "result": "研究结果表明，DDPMs对分数评估中的恒定方差噪声具有鲁棒性。在Wasserstein-2距离下建立的有限样本保证，不仅量化了DDPMs对噪声分数估计的鲁棒性，而且实现了比以往已知结果更快的收敛速度。此外，这些获得的速度与高斯情况下的已知速度相匹配，这意味着它们具有最优性。", "conclusion": "本文的结论是，所建立的在Wasserstein-2距离下的有限样本保证，证明了去噪扩散概率模型（DDPMs）对噪声分数估计的鲁棒性，并且其收敛速度比以前的结果更快，达到了最优水平。", "translation": "生成建模旨在从未知目标分布中生成新的随机样本，给定有限的样本集合。在主要方法中，去噪扩散概率模型（DDPMs）通过由估计分数函数驱动的扩散过程将布朗运动映射来构建此类样本。在这项工作中，我们首先提供经验证据，表明DDPMs对分数评估中的恒定方差噪声具有鲁棒性。然后，我们建立了Wasserstein-2距离的有限样本保证，其具有两个关键特征：(i) 它们表征并量化了DDPMs对噪声分数估计的鲁棒性，以及 (ii) 它们实现了比以前已知结果更快的收敛速度。此外，我们观察到所获得的速度与高斯情况下的已知速度相匹配，这意味着它们的最佳性。", "summary": "本文经验性地证明了去噪扩散概率模型（DDPMs）对噪声分数评估的鲁棒性。研究在Wasserstein-2距离下建立了有限样本保证，不仅量化了DDPMs对噪声分数估计的鲁棒性，还实现了比先前结果更快且最优的收敛速度。", "keywords": "去噪扩散模型, Wasserstein距离, 噪声分数, 最优界限, 生成建模", "comments": "这篇论文的创新之处在于，它为去噪扩散模型（DDPMs）提供了严格的理论保证，特别是在Wasserstein-2距离下量化了其对噪声分数估计的鲁棒性。实现比以往更快的收敛速度并证明其最优性，对于深入理解DDPMs的性能边界及其在实际应用中的稳定性具有重要意义。"}}
{"id": "2506.09730", "title": "Empirical and computer-aided robustness analysis of long-step and accelerated methods in smooth convex optimization", "authors": ["Pierre Vernimmen", "François Glineur"], "summary": "This work assesses both empirically and theoretically, using the performance\nestimation methodology, how robust different first-order optimization methods\nare when subject to relative inexactness in their gradient computations.\nRelative inexactness occurs, for example, when compressing the gradient using\nfewer bits of information, which happens when dealing with large-scale problems\non GPUs. Three major families of methods are analyzed: constant step gradient\ndescent, long-step methods, and accelerated methods. The latter two are first\nshown to be theoretically not robust to inexactness. Then, a semi-heuristic\nshortening factor is introduced to improve their theoretical guarantees. All\nmethods are subsequently tested on a concrete inexact problem, with two\ndifferent types of relative inexactness, and it is observed that both\naccelerated methods are much more robust than expected, and that the shortening\nfactor significantly helps the long-step methods. In the end, all shortened\nmethods appear to be promising, even in this inexact setting.", "comment": null, "cate": "math.OC", "url": "http://arxiv.org/abs/2506.09730v1", "AI": {"title_translation": "光滑凸优化中长步长和加速方法的经验及计算机辅助鲁棒性分析", "tldr": "本文通过经验和理论分析，评估了梯度计算中存在相对不精确性时，梯度下降、长步长和加速等一阶优化方法的鲁棒性。研究发现，加速方法比预期更具鲁棒性，并且引入的缩短因子显著改善了长步长方法的性能。", "motivation": "该研究旨在评估一阶优化方法在梯度计算存在相对不精确性时的鲁棒性，这种不精确性在处理GPU上的大规模问题时（例如通过压缩梯度）经常出现。", "method": "本研究采用性能估计方法，从经验和理论上评估了常数步长梯度下降、长步长方法和加速方法。引入了一个半启发式的缩短因子来改善理论保证，并对所有方法在两种不同类型的相对不精确性下进行了具体非精确问题的测试。", "result": "理论上，长步长和加速方法对不精确性不具鲁棒性。然而，经验结果表明加速方法比预期更具鲁棒性，并且缩短因子显著帮助了长步长方法。", "conclusion": "所有经过缩短处理的方法，即使在非精确设置下也显示出良好的应用前景。", "translation": "这项工作使用性能估计方法，从经验和理论上评估了在梯度计算中存在相对不精确性时，不同一阶优化方法的鲁棒性。相对不精确性例如发生在处理GPU上的大规模问题时，通过使用更少的比特信息压缩梯度。分析了三种主要的方法族：常数步长梯度下降、长步长方法和加速方法。后两种方法首先被证明在理论上对不精确性不具有鲁棒性。然后，引入了一个半启发式的缩短因子来改善它们的理论保证。随后，所有方法都在一个具体的非精确问题上进行了测试，采用了两种不同类型的相对不精确性，结果观察到加速方法比预期更具鲁棒性，并且缩短因子显著地帮助了长步长方法。最终，所有缩短后的方法即使在这种非精确设置中也显得很有前景。", "summary": "本文研究了一阶优化方法（包括梯度下降、长步长和加速方法）在梯度计算存在不精确性时的鲁棒性，这是大规模问题中的常见挑战。尽管长步长和加速方法在理论上对不精确性不具鲁棒性，但研究引入了一个缩短因子以改善其理论保证。经验结果显示，加速方法出乎意料地鲁棒，且缩短因子有效地提升了长步长方法的性能，使得所有经过缩短的方法在非精确环境下都表现出良好的前景。", "keywords": "梯度下降, 加速方法, 不精确梯度, 鲁棒性, 凸优化", "comments": "该论文解决了大规模优化中一个实际且重要的问题，尤其与硬件限制（如GPU）下的梯度压缩相关。结合理论分析（性能估计方法）和经验验证的方法论非常强大。加速方法比理论预期更鲁棒的发现是一个有趣的洞察，而引入“缩短因子”以提高鲁棒性是一项新颖的贡献。"}}
{"id": "2506.09805", "title": "Automatic Treatment Planning using Reinforcement Learning for High-dose-rate Prostate Brachytherapy", "authors": ["Tonghe Wang", "Yining Feng", "Xiaofeng Yang"], "summary": "Purpose: In high-dose-rate (HDR) prostate brachytherapy procedures, the\npattern of needle placement solely relies on physician experience. We\ninvestigated the feasibility of using reinforcement learning (RL) to provide\nneedle positions and dwell times based on patient anatomy during pre-planning\nstage. This approach would reduce procedure time and ensure consistent plan\nquality. Materials and Methods: We train a RL agent to adjust the position of\none selected needle and all the dwell times on it to maximize a pre-defined\nreward function after observing the environment. After adjusting, the RL agent\nthen moves on to the next needle, until all needles are adjusted. Multiple\nrounds are played by the agent until the maximum number of rounds is reached.\nPlan data from 11 prostate HDR boost patients (1 for training, and 10 for\ntesting) treated in our clinic were included in this study. The dosimetric\nmetrics and the number of used needles of RL plan were compared to those of the\nclinical results (ground truth). Results: On average, RL plans and clinical\nplans have very similar prostate coverage (Prostate V100) and Rectum D2cc (no\nstatistical significance), while RL plans have less prostate hotspot (Prostate\nV150) and Urethra D20% plans with statistical significance. Moreover, RL plans\nuse 2 less needles than clinical plan on average. Conclusion: We present the\nfirst study demonstrating the feasibility of using reinforcement learning to\nautonomously generate clinically practical HDR prostate brachytherapy plans.\nThis RL-based method achieved equal or improved plan quality compared to\nconventional clinical approaches while requiring fewer needles. With minimal\ndata requirements and strong generalizability, this approach has substantial\npotential to standardize brachytherapy planning, reduce clinical variability,\nand enhance patient outcomes.", "comment": null, "cate": "physics.med-ph", "url": "http://arxiv.org/abs/2506.09805v1", "AI": {"title_translation": "使用强化学习进行高剂量率前列腺近距离放射治疗的自动化治疗计划", "tldr": "本研究首次证明了使用强化学习自动生成临床实用高剂量率前列腺近距离放射治疗计划的可行性，该方法在计划质量上与传统临床方法相当或更优，且所需针数更少。", "motivation": "在高剂量率（HDR）前列腺近距离放射治疗中，针头放置模式完全依赖于医生的经验，导致程序时间长且计划质量不一致。本研究旨在探索使用强化学习（RL）根据患者解剖结构在预规划阶段提供针头位置和停留时间的可行性，以减少程序时间并确保一致的计划质量。", "method": "研究训练了一个强化学习（RL）代理，使其在观察环境后调整选定针头的位置及其所有停留时间，以最大化预定义的奖励函数。调整完成后，代理会移动到下一根针头，直到所有针头都调整完毕。代理会进行多轮游戏，直到达到最大轮数。研究纳入了11名接受HDR前列腺近距离放射治疗的患者数据（1名用于训练，10名用于测试），并将RL计划的剂量学指标和所用针头数量与临床结果（真实值）进行了比较。", "result": "平均而言，RL计划和临床计划在前列腺覆盖率（Prostate V100）和直肠D2cc方面非常相似（无统计学意义），而RL计划在前列腺热点（Prostate V150）和尿道D20%方面具有统计学意义上的显著降低。此外，RL计划平均比临床计划少使用2根针头。", "conclusion": "本研究首次证明了使用强化学习自主生成临床实用高剂量率前列腺近距离放射治疗计划的可行性。这种基于RL的方法与传统临床方法相比，实现了相同或更高的计划质量，同时所需的针头数量更少。由于数据需求量小且泛化能力强，该方法在标准化近距离放射治疗计划、减少临床变异性和改善患者预后方面具有巨大潜力。", "translation": "目的：在高剂量率（HDR）前列腺近距离放射治疗中，针头放置模式完全依赖于医生的经验。我们研究了在预规划阶段使用强化学习（RL）根据患者解剖结构提供针头位置和停留时间的可行性。这种方法将减少程序时间并确保一致的计划质量。材料和方法：我们训练了一个RL代理，使其在观察环境后调整一个选定针头的位置以及其上所有停留时间，以最大化预定义的奖励函数。调整后，RL代理会移动到下一根针头，直到所有针头都调整完毕。代理会进行多轮游戏，直到达到最大轮数。本研究纳入了11名在我院接受HDR前列腺近距离放射治疗的患者的计划数据（1名用于训练，10名用于测试）。将RL计划的剂量学指标和所用针头数量与临床结果（真实值）进行了比较。结果：平均而言，RL计划和临床计划在前列腺覆盖率（Prostate V100）和直肠D2cc方面非常相似（无统计学意义），而RL计划在前列腺热点（Prostate V150）和尿道D20%方面具有统计学意义上的显著降低。此外，RL计划平均比临床计划少使用2根针头。结论：我们首次展示了使用强化学习自主生成临床实用HDR前列腺近距离放射治疗计划的可行性。这种基于RL的方法与传统临床方法相比，实现了相同或更高的计划质量，同时所需的针头数量更少。由于数据需求量小且泛化能力强，这种方法在标准化近距离放射治疗计划、减少临床变异性和改善患者预后方面具有巨大潜力。", "summary": "本研究探讨了使用强化学习（RL）实现高剂量率（HDR）前列腺近距离放射治疗自动化计划的可行性。通过训练一个RL代理来优化针头位置和停留时间，研究发现RL生成的计划在剂量学指标上与临床方案相当或更优，尤其在减少前列腺热点和尿道剂量方面表现更好，同时平均使用更少的针头。这表明RL方法有望标准化治疗计划、减少临床变异性并改善患者预后。", "keywords": "强化学习, 近距离放射治疗, 前列腺癌, 治疗计划, 自动化", "comments": "这项研究创新性地将强化学习应用于高剂量率前列腺近距离放射治疗的自动化计划，解决了传统方法依赖医生经验、耗时且质量不一致的问题。其重要性在于，RL计划不仅能达到与临床方案相当的质量，甚至在某些关键指标上有所改善（如减少热点和尿道剂量），同时还减少了所需针头数量。此外，该方法对数据需求量小且泛化能力强，预示着其在临床实践中具有巨大的标准化和优化潜力。"}}
{"id": "2506.09832", "title": "A Deep Generative Model for the Simulation of Discrete Karst Networks", "authors": ["Dany Lauzon", "Julien Straubhaar", "Philippe Renard"], "summary": "The simulation of discrete karst networks presents a significant challenge\ndue to the complexity of the physicochemical processes occurring within various\ngeological and hydrogeological contexts over extended periods. This complex\ninterplay leads to a wide variety of karst network patterns, each intricately\nlinked to specific hydrogeological conditions. We explore a novel approach that\nrepresents karst networks as graphs and applies graph generative models (deep\nlearning techniques) to capture the intricate nature of karst environments. In\nthis representation, nodes retain spatial information and properties, while\nedges signify connections between nodes. Our generative process consists of two\nmain steps. First, we utilize graph recurrent neural networks (GraphRNN) to\nlearn the topological distribution of karst networks. GraphRNN decomposes the\ngraph simulation into a sequential generation of nodes and edges, informed by\npreviously generated structures. Second, we employ denoising diffusion\nprobabilistic models on graphs (G-DDPM) to learn node features (spatial\ncoordinates and other properties). G-DDPMs enable the generation of nodes\nfeatures on the graphs produced by the GraphRNN that adhere to the learned\nstatistical properties by sampling from the derived probability distribution,\nensuring that the generated graphs are realistic and capture the essential\nfeatures of the original data. We test our approach using real-world karst\nnetworks and compare generated subgraphs with actual subgraphs from the\ndatabase, by using geometry and topology metrics. Our methodology allows\nstochastic simulation of discrete karst networks across various types of\nformations, a useful tool for studying the behavior of physical processes such\nas flow and transport.", "comment": "26 pages, 15 figures, submitted to Earth and Space Science", "cate": "stat.ML", "url": "http://arxiv.org/abs/2506.09832v1", "AI": {"title_translation": "用于模拟离散岩溶网络的深度生成模型", "tldr": "本文提出了一种利用深度生成模型（GraphRNN和G-DDPM）模拟离散岩溶网络的新方法，该方法能生成逼真的网络结构和特征，并可用于物理过程研究。", "motivation": "由于各种地质和水文地质背景下复杂的物理化学过程，离散岩溶网络的模拟是一个重大挑战，导致岩溶网络模式多样且复杂。", "method": "该方法将岩溶网络表示为图，并应用图生成模型（深度学习技术）。生成过程包括两步：首先，利用图循环神经网络（GraphRNN）学习岩溶网络的拓扑分布；其次，采用图去噪扩散概率模型（G-DDPM）学习节点特征（空间坐标和其他属性）。", "result": "该方法使用真实世界的岩溶网络进行了测试，并通过几何和拓扑指标将生成的子图与数据库中的实际子图进行了比较。该方法能够对各种地层类型的离散岩溶网络进行随机模拟。", "conclusion": "本文提出的方法是研究流量和输运等物理过程行为的有用工具，因为它允许对离散岩溶网络进行随机模拟。", "translation": "离散岩溶网络的模拟由于在不同地质和水文地质背景下长期存在的复杂物理化学过程而面临巨大挑战。这种复杂的相互作用导致了各种各样的岩溶网络模式，每一种都与特定的水文地质条件紧密相关。我们探索了一种新颖的方法，将岩溶网络表示为图，并应用图生成模型（深度学习技术）来捕捉岩溶环境的复杂性。在这种表示中，节点保留空间信息和属性，而边表示节点之间的连接。我们的生成过程包括两个主要步骤。首先，我们利用图循环神经网络（GraphRNN）学习岩溶网络的拓扑分布。GraphRNN将图模拟分解为节点和边的顺序生成，并由先前生成的结构提供信息。其次，我们采用图上的去噪扩散概率模型（G-DDPM）来学习节点特征（空间坐标和其他属性）。G-DDPMs能够从导出的概率分布中采样，在GraphRNN生成的图上生成符合学习到的统计属性的节点特征，确保生成的图是真实的并捕获原始数据的基本特征。我们使用真实世界的岩溶网络测试了我们的方法，并通过几何和拓扑指标将生成的子图与数据库中的实际子图进行了比较。我们的方法允许对各种地层类型的离散岩溶网络进行随机模拟，这对于研究流量和输运等物理过程的行为是一个有用的工具。", "summary": "本文提出了一种新颖的深度生成模型，用于模拟离散岩溶网络。该模型将岩溶网络表示为图，并结合GraphRNN学习拓扑结构，以及G-DDPM学习节点特征。通过对真实岩溶网络的测试，证明了该方法能够生成逼真的岩溶网络，为研究物理过程提供了有用的工具。", "keywords": "岩溶网络, 深度生成模型, 图神经网络, 模拟, 地质模型", "comments": "该论文的创新之处在于首次将深度生成模型应用于复杂离散岩溶网络的模拟，通过结合GraphRNN和G-DDPM，有效地捕捉了岩溶网络的拓扑和空间特征。这为地质和水文地质研究提供了一个强大的新工具，有望促进对地下水流和物质输运等物理过程的理解。"}}
{"id": "2506.09851", "title": "Advancing Exchange Rate Forecasting: Leveraging Machine Learning and AI for Enhanced Accuracy in Global Financial Markets", "authors": ["Md. Yeasin Rahat", "Rajan Das Gupta", "Nur Raisa Rahman", "Sudipto Roy Pritom", "Samiur Rahman Shakir", "Md Imrul Hasan Showmick", "Md. Jakir Hossen"], "summary": "The prediction of foreign exchange rates, such as the US Dollar (USD) to\nBangladeshi Taka (BDT), plays a pivotal role in global financial markets,\ninfluencing trade, investments, and economic stability. This study leverages\nhistorical USD/BDT exchange rate data from 2018 to 2023, sourced from Yahoo\nFinance, to develop advanced machine learning models for accurate forecasting.\nA Long Short-Term Memory (LSTM) neural network is employed, achieving an\nexceptional accuracy of 99.449%, a Root Mean Square Error (RMSE) of 0.9858, and\na test loss of 0.8523, significantly outperforming traditional methods like\nARIMA (RMSE 1.342). Additionally, a Gradient Boosting Classifier (GBC) is\napplied for directional prediction, with backtesting on a $10,000 initial\ncapital revealing a 40.82% profitable trade rate, though resulting in a net\nloss of $20,653.25 over 49 trades. The study analyzes historical trends,\nshowing a decline in BDT/USD rates from 0.012 to 0.009, and incorporates\nnormalized daily returns to capture volatility. These findings highlight the\npotential of deep learning in forex forecasting, offering traders and\npolicymakers robust tools to mitigate risks. Future work could integrate\nsentiment analysis and real-time economic indicators to further enhance model\nadaptability in volatile markets.", "comment": "Accepted in MECON 2025", "cate": "q-fin.ST", "url": "http://arxiv.org/abs/2506.09851v1", "AI": {"title_translation": "推进汇率预测：利用机器学习和人工智能提高全球金融市场的准确性", "tldr": "本研究利用LSTM和GBC模型，基于2018-2023年USD/BDT数据，显著提高了汇率预测准确性，展示了深度学习在风险缓解方面的潜力。", "motivation": "外汇汇率预测在全球金融市场中扮演着关键角色，影响贸易、投资和经济稳定。本研究旨在利用先进的机器学习模型提高汇率预测的准确性。", "method": "研究使用了2018年至2023年来自Yahoo Finance的历史USD/BDT汇率数据。采用长短期记忆（LSTM）神经网络进行汇率预测，并使用梯度提升分类器（GBC）进行方向性预测。此外，还分析了历史趋势并纳入了归一化日收益率以捕捉波动性。", "result": "LSTM模型在USD/BDT汇率预测上达到了99.449%的准确率，RMSE为0.9858，测试损失为0.8523，显著优于传统ARIMA方法（RMSE 1.342）。GBC模型在方向性预测中，尽管有40.82%的盈利交易率，但在49笔交易后导致20,653.25美元的净亏损。历史趋势显示BDT/USD汇率从0.012下降到0.009。", "conclusion": "深度学习在预测外汇汇率方面具有巨大潜力，为交易员和政策制定者提供了强大的风险缓解工具。", "translation": "外汇汇率（如美元兑孟加拉塔卡）的预测在全球金融市场中扮演着举足轻重的角色，影响着贸易、投资和经济稳定。本研究利用2018年至2023年间从雅虎财经获取的历史美元/孟加拉塔卡汇率数据，开发了先进的机器学习模型，以实现准确的预测。研究采用了长短期记忆（LSTM）神经网络，实现了99.449%的卓越准确率、0.9858的均方根误差（RMSE）和0.8523的测试损失，显著优于ARIMA等传统方法（RMSE 1.342）。此外，还应用了梯度提升分类器（GBC）进行方向性预测，对10,000美元初始资本的回溯测试显示，尽管在49笔交易中导致20,653.25美元的净亏损，但盈利交易率为40.82%。该研究分析了历史趋势，显示孟加拉塔卡/美元汇率从0.012下降到0.009，并纳入了归一化日收益率以捕捉波动性。这些发现突出了深度学习在外汇预测中的潜力，为交易员和政策制定者提供了强大的工具来降低风险。未来的工作可以整合情感分析和实时经济指标，以进一步增强模型在波动市场中的适应性。", "summary": "本研究探讨了利用机器学习和人工智能提升外汇汇率预测准确性的方法。通过分析2018-2023年USD/BDT数据，研究应用LSTM神经网络实现了99.449%的高预测准确率，并显著优于传统ARIMA模型。尽管GBC模型在方向性预测的实际交易中表现出净亏损，但研究总体强调了深度学习在外汇预测中的巨大潜力，能为市场参与者提供有效的风险管理工具。", "keywords": "汇率预测, 机器学习, 深度学习, LSTM, 金融市场", "comments": "本文创新性地将深度学习（LSTM）应用于外汇汇率预测，并取得了非常高的准确率，这对于传统方法来说是一个显著的提升。然而，GBC模型在实际回测中表现出的净亏损，揭示了将预测模型应用于实际交易策略时可能存在的复杂性和挑战，仅有高预测准确率不一定能转化为实际盈利。未来的工作若能结合更多实时经济和市场情绪数据，有望进一步提升模型的鲁棒性和实用性。"}}
