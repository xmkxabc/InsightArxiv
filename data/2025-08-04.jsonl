{"id": "2507.22951", "title": "Unifying Post-hoc Explanations of Knowledge Graph Completions", "authors": ["Alessandro Lonardi", "Samy Badreddine", "Tarek R. Besold", "Pablo Sanchez Martin"], "categories": ["cs.AI", "cs.LG"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22951v1", "summary": "Post-hoc explainability for Knowledge Graph Completion (KGC) lacks\nformalization and consistent evaluations, hindering reproducibility and\ncross-study comparisons. This paper argues for a unified approach to post-hoc\nexplainability in KGC. First, we propose a general framework to characterize\npost-hoc explanations via multi-objective optimization, balancing their\neffectiveness and conciseness. This unifies existing post-hoc explainability\nalgorithms in KGC and the explanations they produce. Next, we suggest and\nempirically support improved evaluation protocols using popular metrics like\nMean Reciprocal Rank and Hits@$k$. Finally, we stress the importance of\ninterpretability as the ability of explanations to address queries meaningful\nto end-users. By unifying methods and refining evaluation standards, this work\naims to make research in KGC explainability more reproducible and impactful.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22951v1", "cate": "cs.AI", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.23018", "title": "Data Readiness for Scientific AI at Scale", "authors": ["Wesley Brewer", "Patrick Widener", "Valentine Anantharaj", "Feiyi Wang", "Tom Beck", "Arjun Shankar", "Sarp Oral"], "categories": ["cs.AI", "cs.CE", "cs.DC", "cs.LG", "I.2.6"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      10 pages, 1 figure, 2 tables", "url": "http://arxiv.org/abs/2507.23018v1", "summary": "This paper examines how Data Readiness for AI (DRAI) principles apply to\nleadership-scale scientific datasets used to train foundation models. We\nanalyze archetypal workflows across four representative domains - climate,\nnuclear fusion, bio/health, and materials - to identify common preprocessing\npatterns and domain-specific constraints. We introduce a two-dimensional\nreadiness framework composed of Data Readiness Levels (raw to AI-ready) and\nData Processing Stages (ingest to shard), both tailored to high performance\ncomputing (HPC) environments. This framework outlines key challenges in\ntransforming scientific data for scalable AI training, emphasizing\ntransformer-based generative models. Together, these dimensions form a\nconceptual maturity matrix that characterizes scientific data readiness and\nguides infrastructure development toward standardized, cross-domain support for\nscalable and reproducible AI for science.", "comment": "10 pages, 1 figure, 2 tables", "pdf_url": "http://arxiv.org/pdf/2507.23018v1", "cate": "cs.AI", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.23067", "title": "FairReason: Balancing Reasoning and Social Bias in MLLMs", "authors": ["Zhenyu Pan", "Yutong Zhang", "Jianshu Zhang", "Haoran Lu", "Haozheng Luo", "Yuwei Han", "Philip S. Yu", "Manling Li", "Han Liu"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23067v1", "summary": "Multimodal Large Language Models (MLLMs) already achieve state-of-the-art\nresults across a wide range of tasks and modalities. To push their reasoning\nability further, recent studies explore advanced prompting schemes and\npost-training fine-tuning. Although these techniques improve logical accuracy,\nthey frequently leave the models' outputs burdened with pronounced social\nbiases. Clarifying how reasoning gains interact with bias mitigation-and\nwhether the two objectives inherently trade off-therefore remains an open and\npressing research problem. Our study begins by benchmarking three\nbias-mitigation strategies-supervised fine-uning (SFT), knowledge distillation\n(KD), and rule-based reinforcement learning (RL)-under identical conditions,\nestablishing their baseline strengths and weaknesses. Building on these\nresults, we vary the proportion of debias-focused and reasoning-centric samples\nwithin each paradigm to chart the reasoning-versus-bias trade-off. Our sweeps\nreveal a consistent sweet spot: a roughly 1:4 mix trained with reinforcement\nlearning cuts stereotype scores by 10% while retaining 88% of the model's\noriginal reasoning accuracy, offering concrete guidance for balancing fairness\nand capability in MLLMs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23067v1", "cate": "cs.AI", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.23091", "title": "Moravec's Paradox: Towards an Auditory Turing Test", "authors": ["David Noever", "Forrest McKee"], "categories": ["cs.AI", "cs.SD", "eess.AS"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23091v1", "summary": "This research work demonstrates that current AI systems fail catastrophically\non auditory tasks that humans perform effortlessly. Drawing inspiration from\nMoravec's paradox (i.e., tasks simple for humans often prove difficult for\nmachines, and vice versa), we introduce an auditory Turing test comprising 917\nchallenges across seven categories: overlapping speech, speech in noise,\ntemporal distortion, spatial audio, coffee-shop noise, phone distortion, and\nperceptual illusions. Our evaluation of state-of-the-art audio models including\nGPT-4's audio capabilities and OpenAI's Whisper reveals a striking failure rate\nexceeding 93%, with even the best-performing model achieving only 6.9% accuracy\non tasks that humans solved at 7.5 times higher success (52%). These results\nexpose focusing failures in how AI systems process complex auditory scenes,\nparticularly in selective attention, noise robustness, and contextual\nadaptation. Our benchmark not only quantifies the human-machine auditory gap\nbut also provides insights into why these failures occur, suggesting that\ncurrent architectures lack fundamental mechanisms for human-like auditory scene\nanalysis. The traditional design of audio CAPTCHAs highlights common filters\nthat humans evolved but machines fail to select in multimodal language models.\nThis work establishes a diagnostic framework for measuring progress toward\nhuman-level machine listening and highlights the need for novel approaches\nintegrating selective attention, physics-based audio understanding, and\ncontext-aware perception into multimodal AI systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23091v1", "cate": "cs.AI", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.23163", "title": "Argumentatively Coherent Judgmental Forecasting", "authors": ["Deniz Gorur", "Antonio Rago", "Francesca Toni"], "categories": ["cs.AI", "I.2.7"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      17 pages, 18 figures, ECAI 2025", "url": "http://arxiv.org/abs/2507.23163v1", "summary": "Judgmental forecasting employs human opinions to make predictions about\nfuture events, rather than exclusively historical data as in quantitative\nforecasting. When these opinions form an argumentative structure around\nforecasts, it is useful to study the properties of the forecasts from an\nargumentative perspective. In this paper, we advocate and formally define a\nproperty of argumentative coherence, which, in essence, requires that a\nforecaster's reasoning is coherent with their forecast. We then conduct three\nevaluations with our notion of coherence. First, we assess the impact of\nenforcing coherence on human forecasters as well as on Large Language Model\n(LLM)-based forecasters, given that they have recently shown to be competitive\nwith human forecasters. In both cases, we show that filtering out incoherent\npredictions improves forecasting accuracy consistently, supporting the\npractical value of coherence in both human and LLM-based forecasting. Then, via\ncrowd-sourced user experiments, we show that, despite its apparent\nintuitiveness and usefulness, users do not generally align with this coherence\nproperty. This points to the need to integrate, within argumentation-based\njudgmental forecasting, mechanisms to filter out incoherent opinions before\nobtaining group forecasting predictions.", "comment": "17 pages, 18 figures, ECAI 2025", "pdf_url": "http://arxiv.org/pdf/2507.23163v1", "cate": "cs.AI", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.23191", "title": "Tractable Responsibility Measures for Ontology-Mediated Query Answering", "authors": ["Meghyn Bienvenu", "Diego Figueira", "Pierre Lafourcade"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      Long version of a paper to appear at KR 2025, which contains further proof details in the appendix", "url": "http://arxiv.org/abs/2507.23191v1", "summary": "Recent work on quantitative approaches to explaining query answers employs\nresponsibility measures to assign scores to facts in order to quantify their\nrespective contributions to obtaining a given answer. In this paper, we study\nthe complexity of computing such responsibility scores in the setting of\nontology-mediated query answering, focusing on a very recently introduced\nfamily of Shapley-value-based responsibility measures defined in terms of\nweighted sums of minimal supports (WSMS). By exploiting results from the\ndatabase setting, we can show that such measures enjoy polynomial data\ncomplexity for classes of ontology-mediated queries that are\nfirst-order-rewritable, whereas the problem becomes \"shP\"-hard when the\nontology language can encode reachability queries (via axioms like $\\exists R.\nA \\sqsubseteq A$). To better understand the tractability frontier, we next\nexplore the combined complexity of WSMS computation. We prove that\nintractability applies already to atomic queries if the ontology language\nsupports conjunction, as well as to unions of `well-behaved' conjunctive\nqueries, even in the absence of an ontology. By contrast, our study yields\npositive results for common DL-Lite dialects: by means of careful analysis, we\nidentify classes of structurally restricted conjunctive queries (which\nintuitively disallow undesirable interactions between query atoms) that admit\ntractable WSMS computation.", "comment": "Long version of a paper to appear at KR 2025, which contains further\n  proof details in the appendix", "pdf_url": "http://arxiv.org/pdf/2507.23191v1", "cate": "cs.AI", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23197", "title": "Solution-aware vs global ReLU selection: partial MILP strikes back for DNN verification", "authors": ["Yuke Liao", "Blaise Genest", "Kuldeep Meel", "Shaan Aryaman"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23197v1", "summary": "To handle complex instances, we revisit a divide-and-conquer approach to\nbreak down the complexity: instead of few complex BaB calls, we rely on many\nsmall {\\em partial} MILP calls. The crucial step is to select very few but very\nimportant ReLUs to treat using (costly) binary variables. The previous attempts\nwere suboptimal in that respect. To select these important ReLU variables, we\npropose a novel {\\em solution-aware} ReLU scoring ({\\sf SAS}), as well as adapt\nthe BaB-SR and BaB-FSB branching functions as {\\em global} ReLU scoring ({\\sf\nGS}) functions. We compare them theoretically as well as experimentally, and\n{\\sf SAS} is more efficient at selecting a set of variables to open using\nbinary variables. Compared with previous attempts, SAS reduces the number of\nbinary variables by around 6 times, while maintaining the same level of\naccuracy. Implemented in {\\em Hybrid MILP}, calling first $\\alpha,\\beta$-CROWN\nwith a short time-out to solve easier instances, and then partial MILP,\nproduces a very accurate yet efficient verifier, reducing by up to $40\\%$ the\nnumber of undecided instances to low levels ($8-15\\%$), while keeping a\nreasonable runtime ($46s-417s$ on average per instance), even for fairly large\nCNNs with 2 million parameters.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23197v1", "cate": "cs.AI", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23276", "title": "How Far Are AI Scientists from Changing the World?", "authors": ["Qiujie Xie", "Yixuan Weng", "Minjun Zhu", "Fuchen Shen", "Shulin Huang", "Zhen Lin", "Jiahui Zhou", "Zilan Mao", "Zijie Yang", "Linyi Yang", "Jian Wu", "Yue Zhang"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23276v2", "summary": "The emergence of large language models (LLMs) is propelling automated\nscientific discovery to the next level, with LLM-based Artificial Intelligence\n(AI) Scientist systems now taking the lead in scientific research. Several\ninfluential works have already appeared in the field of AI Scientist systems,\nwith AI-generated research papers having been accepted at the ICLR 2025\nworkshop, suggesting that a human-level AI Scientist capable of uncovering\nphenomena previously unknown to humans, may soon become a reality. In this\nsurvey, we focus on the central question: How far are AI scientists from\nchanging the world and reshaping the scientific research paradigm? To answer\nthis question, we provide a prospect-driven review that comprehensively\nanalyzes the current achievements of AI Scientist systems, identifying key\nbottlenecks and the critical components required for the emergence of a\nscientific agent capable of producing ground-breaking discoveries that solve\ngrand challenges. We hope this survey will contribute to a clearer\nunderstanding of limitations of current AI Scientist systems, showing where we\nare, what is missing, and what the ultimate goals for scientific AI should be.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23276v2", "cate": "cs.AI", "date": "2025-07-31", "updated": "2025-08-01"}
{"id": "2507.23330", "title": "AI Must not be Fully Autonomous", "authors": ["Tosin Adewumi", "Lama Alkhaled", "Florent Imbert", "Hui Han", "Nudrat Habib", "Karl Löwenmark"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      11 pages, 1 figure", "url": "http://arxiv.org/abs/2507.23330v1", "summary": "Autonomous Artificial Intelligence (AI) has many benefits. It also has many\nrisks. In this work, we identify the 3 levels of autonomous AI. We are of the\nposition that AI must not be fully autonomous because of the many risks,\nespecially as artificial superintelligence (ASI) is speculated to be just\ndecades away. Fully autonomous AI, which can develop its own objectives, is at\nlevel 3 and without responsible human oversight. However, responsible human\noversight is crucial for mitigating the risks. To ague for our position, we\ndiscuss theories of autonomy, AI and agents. Then, we offer 12 distinct\narguments and 6 counterarguments with rebuttals to the counterarguments. We\nalso present 15 pieces of recent evidence of AI misaligned values and other\nrisks in the appendix.", "comment": "11 pages, 1 figure", "pdf_url": "http://arxiv.org/pdf/2507.23330v1", "cate": "cs.AI", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23035", "title": "KLLM: Fast LLM Inference with K-Means Quantization", "authors": ["Xueying Wu", "Baijun Zhou", "Zhihui Gao", "Yuzhe Fu", "Qilin Zheng", "Yintao He", "Hai Li"], "categories": ["cs.LG", "cs.AR"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23035v1", "summary": "Large language model (LLM) inference poses significant challenges due to its\nintensive memory and computation demands. Weight and activation quantization\n(WAQ) offers a promising solution by reducing both memory footprint and\narithmetic complexity. However, two key challenges remain in the existing WAQ\ndesigns. (1) Traditional WAQ designs rely on uniform integer-based quantization\nfor hardware efficiency, but this often results in significant accuracy\ndegradation at low precision. K-Means-based quantization, a non-uniform\nquantization technique, achieves higher accuracy by matching the Gaussian-like\ndistributions of weights and activations in LLMs. However, its non-uniform\nnature prevents direct execution on low-precision compute units, requiring\ndequantization and floating-point matrix multiplications (MatMuls) during\ninference. (2) Activation outliers further hinder effective low-precision WAQ.\nOffline thresholding methods for outlier detection can lead to significant\nmodel performance degradation, while existing online detection techniques\nintroduce substantial runtime overhead.\n  To address the aforementioned challenges and fully unleash the potential of\nWAQ with K-Means quantization for LLM inference, in this paper, we propose\nKLLM, a hardware-software co-design framework. KLLM features an index-based\ncomputation scheme for efficient execution of MatMuls and nonlinear operations\non K-Means-quantized data, which avoids most of the dequantization and\nfull-precision computations. Moreover, KLLM incorporates a novel outlier\ndetection engine, Orizuru, that efficiently identifies the top-$k$ largest and\nsmallest elements in the activation data stream during online inference.\n  Extensive experiments show that, on average, KLLM achieves speedups of 9.67x,\n7.03x and energy efficiency improvements of 229.50x, 150.21x compared to the\nA100 GPU and Atom, respectively.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23035v1", "cate": "cs.LG", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.23336", "title": "DSBC : Data Science task Benchmarking with Context engineering", "authors": ["Ram Mohan Rao Kadiyala", "Siddhant Gupta", "Jebish Purbey", "Giulio Martini", "Suman Debnath", "Hamza Farooq"], "categories": ["cs.AI", "cs.CL", "cs.MA"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      32 pages", "url": "http://arxiv.org/abs/2507.23336v1", "summary": "Recent advances in large language models (LLMs) have significantly impacted\ndata science workflows, giving rise to specialized data science agents designed\nto automate analytical tasks. Despite rapid adoption, systematic benchmarks\nevaluating the efficacy and limitations of these agents remain scarce. In this\npaper, we introduce a comprehensive benchmark specifically crafted to reflect\nreal-world user interactions with data science agents by observing usage of our\ncommercial applications. We evaluate three LLMs: Claude-4.0-Sonnet,\nGemini-2.5-Flash, and OpenAI-o4-Mini across three approaches: zero-shot with\ncontext engineering, multi-step with context engineering, and with SmolAgent.\nOur benchmark assesses performance across a diverse set of eight data science\ntask categories, additionally exploring the sensitivity of models to common\nprompting issues, such as data leakage and slightly ambiguous instructions. We\nfurther investigate the influence of temperature parameters on overall and\ntask-specific outcomes for each model and approach. Our findings reveal\ndistinct performance disparities among the evaluated models and methodologies,\nhighlighting critical factors that affect practical deployment. The benchmark\ndataset and evaluation framework introduced herein aim to provide a foundation\nfor future research of more robust and effective data science agents.", "comment": "32 pages", "pdf_url": "http://arxiv.org/pdf/2507.23336v1", "cate": "cs.AI", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23398", "title": "Smart Video Capsule Endoscopy: Raw Image-Based Localization for Enhanced GI Tract Investigation", "authors": ["Oliver Bause", "Julia Werner", "Paul Palomero Bernardo", "Oliver Bringmann"], "categories": ["eess.IV", "cs.AR", "cs.CV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "Comments:      Accepted at the 32nd International Conference on Neural Information Processing - ICONIP 2025", "url": "http://arxiv.org/abs/2507.23398v1", "summary": "For many real-world applications involving low-power sensor edge devices deep\nneural networks used for image classification might not be suitable. This is\ndue to their typically large model size and require- ment of operations often\nexceeding the capabilities of such resource lim- ited devices. Furthermore,\ncamera sensors usually capture images with a Bayer color filter applied, which\nare subsequently converted to RGB images that are commonly used for neural\nnetwork training. However, on resource-constrained devices, such conversions\ndemands their share of energy and optimally should be skipped if possible. This\nwork ad- dresses the need for hardware-suitable AI targeting sensor edge\ndevices by means of the Video Capsule Endoscopy, an important medical proce-\ndure for the investigation of the small intestine, which is strongly limited by\nits battery lifetime. Accurate organ classification is performed with a final\naccuracy of 93.06% evaluated directly on Bayer images involv- ing a CNN with\nonly 63,000 parameters and time-series analysis in the form of Viterbi\ndecoding. Finally, the process of capturing images with a camera and raw image\nprocessing is demonstrated with a customized PULPissimo System-on-Chip with a\nRISC-V core and an ultra-low power hardware accelerator providing an\nenergy-efficient AI-based image clas- sification approach requiring just 5.31\n{\\mu}J per image. As a result, it is possible to save an average of 89.9% of\nenergy before entering the small intestine compared to classic video capsules.", "comment": "Accepted at the 32nd International Conference on Neural Information\n  Processing - ICONIP 2025", "pdf_url": "http://arxiv.org/pdf/2507.23398v1", "cate": "eess.IV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23377", "title": "LLM4Rail: An LLM-Augmented Railway Service Consulting Platform", "authors": ["Zhuo Li", "Xianghuai Deng", "Chiwei Feng", "Hanmeng Li", "Shenjie Wang", "Haichao Zhang", "Teng Jia", "Conlin Chen", "Louis Linchun Wu", "Jia Wang"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23377v1", "summary": "Large language models (LLMs) have significantly reshaped different walks of\nbusiness. To meet the increasing demands for individualized railway service, we\ndevelop LLM4Rail - a novel LLM-augmented railway service consulting platform.\nEmpowered by LLM, LLM4Rail can provide custom modules for ticketing, railway\nfood & drink recommendations, weather information, and chitchat. In LLM4Rail,\nwe propose the iterative \"Question-Thought-Action-Observation (QTAO)\" prompting\nframework. It meticulously integrates verbal reasoning with task-oriented\nactions, that is, reasoning to guide action selection, to effectively retrieve\nexternal observations relevant to railway operation and service to generate\naccurate responses. To provide personalized onboard dining services, we first\nconstruct the Chinese Railway Food and Drink (CRFD-25) - a publicly accessible\ntakeout dataset tailored for railway services. CRFD-25 covers a wide range of\nsignature dishes categorized by cities, cuisines, age groups, and spiciness\nlevels. We further introduce an LLM-based zero-shot conversational recommender\nfor railway catering. To address the unconstrained nature of open\nrecommendations, the feature similarity-based post-processing step is\nintroduced to ensure all the recommended items are aligned with CRFD-25\ndataset.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23377v1", "cate": "cs.AI", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23562", "title": "Hardware-Aware Fine-Tuning of Spiking Q-Networks on the SpiNNaker2 Neuromorphic Platform", "authors": ["Sirine Arfa", "Bernhard Vogginger", "Christian Mayr"], "categories": ["cs.LG", "cs.AR"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      8 pages, 5 figures, 3 tables", "url": "http://arxiv.org/abs/2507.23562v1", "summary": "Spiking Neural Networks (SNNs) promise orders-of-magnitude lower power\nconsumption and low-latency inference on neuromorphic hardware for a wide range\nof robotic tasks. In this work, we present an energy-efficient implementation\nof a reinforcement learning (RL) algorithm using quantized SNNs to solve two\nclassical control tasks. The network is trained using the Q-learning algorithm,\nthen fine-tuned and quantized to low-bit (8-bit) precision for embedded\ndeployment on the SpiNNaker2 neuromorphic chip. To evaluate the comparative\nadvantage of SpiNNaker2 over conventional computing platforms, we analyze\ninference latency, dynamic power consumption, and energy cost per inference for\nour SNN models, comparing performance against a GTX 1650 GPU baseline. Our\nresults demonstrate SpiNNaker2's strong potential for scalable, low-energy\nneuromorphic computing, achieving up to 32x reduction in energy consumption.\nInference latency remains on par with GPU-based execution, with improvements\nobserved in certain task settings, reinforcing SpiNNaker2's viability for\nreal-time neuromorphic control and making the neuromorphic approach a\ncompelling direction for efficient deep Q-learning.", "comment": "8 pages, 5 figures, 3 tables", "pdf_url": "http://arxiv.org/pdf/2507.23562v1", "cate": "cs.LG", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23429", "title": "Chatting with your ERP: A Recipe", "authors": ["Jorge Ruiz Gómez", "Lidia Andrés Susinos", "Jorge Alamo Olivé", "Sonia Rey Osorno", "Manuel Luis Gonzalez Hernández"], "categories": ["cs.AI", "cs.DB", "cs.ET", "cs.HC", "cs.MA", "68T50, 68P20", "I.2.7; H.2.5; H.2.8; H.5.m"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      11 pages, includes 3 tables summarizing schema and model performance. Submitted on July 31, 2025. Targets integration of LLM agents with ERP systems using open-weight models and Ollama deployment", "url": "http://arxiv.org/abs/2507.23429v1", "summary": "This paper presents the design, implementation, and evaluation behind a Large\nLanguage Model (LLM) agent that chats with an industrial production-grade ERP\nsystem. The agent is capable of interpreting natural language queries and\ntranslating them into executable SQL statements, leveraging open-weight LLMs. A\nnovel dual-agent architecture combining reasoning and critique stages was\nproposed to improve query generation reliability.", "comment": "11 pages, includes 3 tables summarizing schema and model performance.\n  Submitted on July 31, 2025. Targets integration of LLM agents with ERP\n  systems using open-weight models and Ollama deployment", "pdf_url": "http://arxiv.org/pdf/2507.23429v1", "cate": "cs.AI", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2408.01254", "title": "TrIM, Triangular Input Movement Systolic Array for Convolutional Neural Networks: Dataflow and Analytical Modelling", "authors": ["Cristian Sestito", "Shady Agwa", "Themis Prodromakis"], "categories": ["cs.AI", "cs.AR"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      This work has been accepted by IEEE TCASAI for publication", "url": "http://arxiv.org/abs/2408.01254v3", "summary": "In order to follow the ever-growing computational complexity and data\nintensity of state-of-the-art AI models, new computing paradigms are being\nproposed. These paradigms aim at achieving high energy efficiency by mitigating\nthe Von Neumann bottleneck that relates to the energy cost of moving data\nbetween the processing cores and the memory. Convolutional Neural Networks\n(CNNs) are susceptible to this bottleneck, given the massive data they have to\nmanage. Systolic arrays (SAs) are promising architectures to mitigate data\ntransmission cost, thanks to high data utilization of Processing Elements\n(PEs). These PEs continuously exchange and process data locally based on\nspecific dataflows (such as weight stationary and row stationary), in turn\nreducing the number of memory accesses to the main memory. In SAs, convolutions\nare managed either as matrix multiplications or exploiting the raster-order\nscan of sliding windows. However, data redundancy is a primary concern\naffecting area, power, and energy. In this paper, we propose TrIM: a novel\ndataflow for SAs based on a Triangular Input Movement and compatible with CNN\ncomputing. TrIM maximizes the local input utilization, minimizes the weight\ndata movement, and solves the data redundancy problem. Furthermore, TrIM does\nnot incur the significant on-chip memory penalty introduced by the row\nstationary dataflow. When compared to state-of-the-art SA dataflows, the high\ndata utilization offered by TrIM guarantees ~10X less memory access.\nFurthermore, considering that PEs continuously overlap multiplications and\naccumulations, TrIM achieves high throughput (up to 81.8% higher than row\nstationary), other than requiring a limited number of registers (up to 15.6X\nfewer registers than row stationary).", "comment": "This work has been accepted by IEEE TCASAI for publication", "pdf_url": "http://arxiv.org/pdf/2408.01254v3", "cate": "cs.AI", "date": "2024-08-02", "updated": "2025-07-31"}
{"id": "2507.23440", "title": "Self-Foveate: Enhancing Diversity and Difficulty of Synthesized Instructions from Unsupervised Text via Multi-Level Foveation", "authors": ["Mingzhe Li", "Xin Lu", "Yanyan Zhao"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      Accepted by Findings of ACL 2025", "url": "http://arxiv.org/abs/2507.23440v1", "summary": "Large language models (LLMs) with instruction following capabilities have\ndemonstrated impressive problem-solving abilities. While synthesizing\ninstructional data from unsupervised text has become a common approach for\ntraining such models, conventional methods rely heavily on human effort for\ndata annotation. Although existing automated synthesis paradigms have\nalleviated this constraint, they still exhibit significant limitations in\nensuring adequate diversity and difficulty of synthesized instructions. To\naddress these challenges, we propose Self-Foveate, an innovative LLM-driven\nmethod for instruction synthesis. This approach introduces a\n\"Micro-Scatter-Macro\" multi-level foveation methodology that effectively guides\nthe LLM to deeply excavate fine-grained information embedded in unsupervised\ntext, thereby enhancing both the diversity and difficulty of synthesized\ninstructions. Comprehensive experiments across multiple unsupervised corpora\nand diverse model architectures validate the effectiveness and superiority of\nour proposed method. We publicly release our data and codes:\nhttps://github.com/Mubuky/Self-Foveate", "comment": "Accepted by Findings of ACL 2025", "pdf_url": "http://arxiv.org/pdf/2507.23440v1", "cate": "cs.AI", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2506.05588", "title": "Preprocessing Methods for Memristive Reservoir Computing for Image Recognition", "authors": ["Rishona Daniels", "Duna Wattad", "Ronny Ronen", "David Saad", "Shahar Kvatinsky"], "categories": ["cs.NE", "cs.AR", "cs.ET"], "primary_category": "Subjects:       Neural and Evolutionary Computing (cs.NE)", "pdf_link": null, "comments": "Comments:      6 pages, 5 figures, Accepted for presentation in IEEE MetroXRAINE 2025 conference", "url": "http://arxiv.org/abs/2506.05588v3", "summary": "Reservoir computing (RC) has attracted attention as an efficient recurrent\nneural network architecture due to its simplified training, requiring only its\nlast perceptron readout layer to be trained. When implemented with memristors,\nRC systems benefit from their dynamic properties, which make them ideal for\nreservoir construction. However, achieving high performance in memristor-based\nRC remains challenging, as it critically depends on the input preprocessing\nmethod and reservoir size. Despite growing interest, a comprehensive evaluation\nthat quantifies the impact of these factors is still lacking. This paper\nsystematically compares various preprocessing methods for memristive RC\nsystems, assessing their effects on accuracy and energy consumption. We also\npropose a parity-based preprocessing method that improves accuracy by 2-6%\nwhile requiring only a modest increase in device count compared to other\nmethods. Our findings highlight the importance of informed preprocessing\nstrategies to improve the efficiency and scalability of memristive RC systems.", "comment": "6 pages, 5 figures, Accepted for presentation in IEEE MetroXRAINE\n  2025 conference", "pdf_url": "http://arxiv.org/pdf/2506.05588v3", "cate": "cs.NE", "date": "2025-06-05", "updated": "2025-07-31"}
{"id": "2507.23008", "title": "Exponential Lower Bounds on the Size of ResLin Proofs of Nearly Quadratic Depth", "authors": ["Sreejata Kishor Bhattacharya", "Arkadev Chattopadhyay"], "categories": ["cs.CC"], "primary_category": "Subjects:       Computational Complexity (cs.CC)", "pdf_link": null, "comments": "Comments:      35 pages", "url": "http://arxiv.org/abs/2507.23008v1", "summary": "Itsykson and Sokolov identified resolution over parities, denoted by\n$\\text{Res}(\\oplus)$, as a natural and simple fragment of\n$\\text{AC}^0[2]$-Frege for which no super-polynomial lower bounds on size of\nproofs are known. Building on a recent line of work, Efremenko and Itsykson\nproved lower bounds of the form $\\text{exp}(N^{\\Omega(1)})$, on the size of\n$\\text{Res}(\\oplus)$ proofs whose depth is upper bounded by $O(N\\log N)$, where\n$N$ is the number of variables of the unsatisfiable CNF formula. The hard\nformula they used was Tseitin on an appropriately expanding graph, lifted by a\n$2$-stifling gadget. They posed the natural problem of proving super-polynomial\nlower bounds on the size of proofs that are $\\Omega(N^{1+\\epsilon})$ deep, for\nany constant $\\epsilon > 0$.\n  We provide a significant improvement by proving a lower bound on size of the\nform $\\text{exp}(\\tilde{\\Omega}(N^{\\epsilon}))$, as long as the depth of the\n$\\text{Res}(\\oplus)$ proofs are $O(N^{2-\\epsilon})$, for every $\\epsilon > 0$.\nOur hard formula is again Tseitin on an expander graph, albeit lifted with a\ndifferent type of gadget. Our gadget needs to have small correlation with all\nparities.\n  An important ingredient in our work is to show that arbitrary distributions\n\\emph{lifted} with such gadgets fool \\emph{safe} affine spaces, an idea which\noriginates in the earlier work of Bhattacharya, Chattopadhyay and Dvorak.", "comment": "35 pages", "pdf_url": "http://arxiv.org/pdf/2507.23008v1", "cate": "cs.CC", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.23488", "title": "Causal Reasoning in Pieces: Modular In-Context Learning for Causal Discovery", "authors": ["Kacper Kadziolka", "Saber Salehkaleybar"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23488v1", "summary": "Causal inference remains a fundamental challenge for large language models.\nRecent advances in internal reasoning with large language models have sparked\ninterest in whether state-of-the-art reasoning models can robustly perform\ncausal discovery-a task where conventional models often suffer from severe\noverfitting and near-random performance under data perturbations. We study\ncausal discovery on the Corr2Cause benchmark using the emergent OpenAI's\no-series and DeepSeek-R model families and find that these reasoning-first\narchitectures achieve significantly greater native gains than prior approaches.\nTo capitalize on these strengths, we introduce a modular in-context pipeline\ninspired by the Tree-of-Thoughts and Chain-of-Thoughts methodologies, yielding\nnearly three-fold improvements over conventional baselines. We further probe\nthe pipeline's impact by analyzing reasoning chain length, complexity, and\nconducting qualitative and quantitative comparisons between conventional and\nreasoning models. Our findings suggest that while advanced reasoning models\nrepresent a substantial leap forward, carefully structured in-context\nframeworks are essential to maximize their capabilities and offer a\ngeneralizable blueprint for causal discovery across diverse domains.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23488v1", "cate": "cs.AI", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23345", "title": "On a better complexity upper bound of Ward-Szabo theorem", "authors": ["Takashi Ishizuka"], "categories": ["cs.CC", "cs.DM"], "primary_category": "Subjects:       Computational Complexity (cs.CC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23345v1", "summary": "Ward and Szab\\'o [WS94] have shown that a complete graph with $N^2$ nodes\nwhose edges are colored by $N$ colors and that has at least two colors contains\na bichromatic triangle. This fact leads us to a total search problem: Given an\nedge-coloring on a complete graph with N2 nodes using at least two colors and\nat most N colors, find a bichromatic triangle. Bourneuf, Folwarczn\\'y,\nHub\\'acek, Rosen, and Schwartzbach [Bou+23] have proven that such a total\nsearch problem, called Ward-Szab\\'o, is PWPP-hard and belongs to the class\nTFNP, a class for total search problems in which the correctness of every\ncandidate solution is efficiently verifiable. However, it is open which TFNP\nsubclass contains Ward-Szab\\'o. This paper will improve the complexity upper\nbound of Ward-Szab\\'o. We prove that Ward-Szab\\'o is in belongs to the\ncomplexity class PPP, a TFNP subclass of problems in which the pigeonhole\nprinciple guarantees the existence of solutions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23345v1", "cate": "cs.CC", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23020", "title": "Axioms for Model Fidelity Evaluation", "authors": ["Evan Taylor", "Edward Louis", "Gregory Mocko"], "categories": ["cs.CE"], "primary_category": "Subjects:       Computational Engineering, Finance, and Science (cs.CE)", "pdf_link": null, "comments": "Comments:      This is the authors' preprint (submitted version) of a paper accepted for ASME IDETC/CIE 2025, edited only to note preprint status. Posted in compliance with ASME 's preprint and copyright policy. The final version is copyrighted by ASME and will appear in the ASME Digital Collection. The DOI will be added once published", "url": "http://arxiv.org/abs/2507.23020v1", "summary": "Digital engineering has transformed the design and development process.\nHowever, the utility of digital engineering is fundamentally dependent on the\nassumption that a simulation provides information consistent with reality. This\nrelationship is described as model fidelity. Despite the widespread use of the\nterm, existing definitions of model fidelity often lack formal rigor in\npractical application, which leaves ambiguity in how this similarity should be\nevaluated. This paper presents seven fundamental axioms to aid the development\nof future fidelity evaluation frameworks. An example of a ground vehicle model\nis used under an existing fidelity evaluation framework to observe the\napplicability of these axioms. In addition, these axioms are used as a\nreference point for considering future opportunities in future work related to\nmodel fidelity.", "comment": "This is the authors' preprint (submitted version) of a paper accepted\n  for ASME IDETC/CIE 2025, edited only to note preprint status. Posted in\n  compliance with ASME 's preprint and copyright policy. The final version is\n  copyrighted by ASME and will appear in the ASME Digital Collection. The DOI\n  will be added once published", "pdf_url": "http://arxiv.org/pdf/2507.23020v1", "cate": "cs.CE", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.23497", "title": "Causal Identification of Sufficient, Contrastive and Complete Feature Sets in Image Classification", "authors": ["David A Kelly", "Hana Chockler"], "categories": ["cs.AI", "cs.CV"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      13 pages, 13 figures, appendix included", "url": "http://arxiv.org/abs/2507.23497v1", "summary": "Existing algorithms for explaining the outputs of image classifiers are based\non a variety of approaches and produce explanations that lack formal rigor. On\nthe other hand, logic-based explanations are formally and rigorously defined\nbut their computability relies on strict assumptions about the model that do\nnot hold on image classifiers.\n  In this paper, we show that causal explanations, in addition to being\nformally and rigorously defined, enjoy the same formal properties as\nlogic-based ones, while still lending themselves to black-box algorithms and\nbeing a natural fit for image classifiers. We prove formal properties of causal\nexplanations and introduce contrastive causal explanations for image\nclassifiers. Moreover, we augment the definition of explanation with confidence\nawareness and introduce complete causal explanations: explanations that are\nclassified with exactly the same confidence as the original image.\n  We implement our definitions, and our experimental results demonstrate that\ndifferent models have different patterns of sufficiency, contrastiveness, and\ncompleteness. Our algorithms are efficiently computable, taking on average 6s\nper image on a ResNet50 model to compute all types of explanations, and are\ntotally black-box, needing no knowledge of the model, no access to model\ninternals, no access to gradient, nor requiring any properties, such as\nmonotonicity, of the model.", "comment": "13 pages, 13 figures, appendix included", "pdf_url": "http://arxiv.org/pdf/2507.23497v1", "cate": "cs.AI", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23563", "title": "The Complexity of Logarithmic Space Bounded Counting Classes", "authors": ["T. C. Vijayaraghavan"], "categories": ["cs.CC"], "primary_category": "Subjects:       Computational Complexity (cs.CC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23563v1", "summary": "In this monograph, we study complexity classes that are defined using $O(\\log\nn)$-space bounded non-deterministic Turing machines. We prove salient results\nof Computational Complexity in this topic such as the\nImmerman-Szelepcs$\\rm\\acute{e}$nyi Theorem, the Isolating Lemma, theorems of\nMahajan-Vinay on the determinant and many consequences of these very important\nresults. The manuscript is intended to be a comprehensive textbook on the topic\nof The Complexity of Logarithmic Space Bounded Counting Classes.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23563v1", "cate": "cs.CC", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23218", "title": "An Information Bottleneck Asset Pricing Model", "authors": ["Che Sun"], "categories": ["cs.CE", "cs.AI"], "primary_category": "Subjects:       Computational Engineering, Finance, and Science (cs.CE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23218v1", "summary": "Deep neural networks (DNNs) have garnered significant attention in financial\nasset pricing, due to their strong capacity for modeling complex nonlinear\nrelationships within financial data. However, sophisticated models are prone to\nover-fitting to the noise information in financial data, resulting in inferior\nperformance. To address this issue, we propose an information bottleneck asset\npricing model that compresses data with low signal-to-noise ratios to eliminate\nredundant information and retain the critical information for asset pricing.\nOur model imposes constraints of mutual information during the nonlinear\nmapping process. Specifically, we progressively reduce the mutual information\nbetween the input data and the compressed representation while increasing the\nmutual information between the compressed representation and the output\nprediction. The design ensures that irrelevant information, which is\nessentially the noise in the data, is forgotten during the modeling of\nfinancial nonlinear relationships without affecting the final asset pricing. By\nleveraging the constraints of the Information bottleneck, our model not only\nharnesses the nonlinear modeling capabilities of deep networks to capture the\nintricate relationships within financial data but also ensures that noise\ninformation is filtered out during the information compression process.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23218v1", "cate": "cs.CE", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23554", "title": "DICE: Dynamic In-Context Example Selection in LLM Agents via Efficient Knowledge Transfer", "authors": ["Ruoyu Wang", "Junda Wu", "Yu Xia", "Tong Yu", "Ryan A. Rossi", "Julian McAuley", "Lina Yao"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23554v1", "summary": "Large language model-based agents, empowered by in-context learning (ICL),\nhave demonstrated strong capabilities in complex reasoning and tool-use tasks.\nHowever, existing works have shown that the effectiveness of ICL is highly\nsensitive to the choice of demonstrations, with suboptimal examples often\nleading to unstable or degraded performance. While prior work has explored\nexample selection, including in some agentic or multi-step settings, existing\napproaches typically rely on heuristics or task-specific designs and lack a\ngeneral, theoretically grounded criterion for what constitutes an effective\ndemonstration across reasoning steps. Therefore, it is non-trivial to develop a\nprincipled, general-purpose method for selecting demonstrations that\nconsistently benefit agent performance. In this paper, we address this\nchallenge with DICE, Dynamic In-Context Example Selection for LLM Agents, a\ntheoretically grounded ICL framework for agentic tasks that selects the most\nrelevant demonstrations at each step of reasoning. Our approach decomposes\ndemonstration knowledge into transferable and non-transferable components\nthrough a causal lens, showing how the latter can introduce spurious\ndependencies that impair generalization. We further propose a stepwise\nselection criterion with a formal guarantee of improved agent performance.\nImportantly, DICE is a general, framework-agnostic solution that can be\nintegrated as a plug-in module into existing agentic frameworks without any\nadditional training cost. Extensive experiments across diverse domains\ndemonstrate our method's effectiveness and generality, highlighting the\nimportance of principled, context-aware demo selection for robust and efficient\nLLM agents.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23554v1", "cate": "cs.AI", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23787", "title": "Amplitude amplification and estimation require inverses", "authors": ["Ewin Tang", "John Wright"], "categories": ["quant-ph", "cs.CC", "cs.DS"], "primary_category": "Subjects:       Quantum Physics (quant-ph)", "pdf_link": null, "comments": "Comments:      20 pages", "url": "http://arxiv.org/abs/2507.23787v1", "summary": "We prove that the generic quantum speedups for brute-force search and\ncounting only hold when the process we apply them to can be efficiently\ninverted. The algorithms speeding up these problems, amplitude amplification\nand amplitude estimation, assume the ability to apply a state preparation\nunitary $U$ and its inverse $U^\\dagger$; we give problem instances based on\ntrace estimation where no algorithm which uses only $U$ beats the naive,\nquadratically slower approach. Our proof of this is simple and goes through the\ncompressed oracle method introduced by Zhandry. Since these two subroutines are\nresponsible for the ubiquity of the quadratic \"Grover\" speedup in quantum\nalgorithms, our result explains why such speedups are far harder to come by in\nthe settings of quantum learning, metrology, and sensing. In these settings,\n$U$ models the evolution of an experimental system, so implementing $U^\\dagger$\ncan be much harder -- tantamount to reversing time within the system. Our\nresult suggests a dichotomy: without inverse access, quantum speedups are\nscarce; with it, quantum speedups abound.", "comment": "20 pages", "pdf_url": "http://arxiv.org/pdf/2507.23787v1", "cate": "quant-ph", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23443", "title": "Adjoint-Based Aerodynamic Shape Optimization with a Manifold Constraint Learned by Diffusion Models", "authors": ["Long Chen", "Emre Oezkaya", "Jan Rottmayer", "Nicolas R. Gauger", "Zebang Shen", "Yinyu Ye"], "categories": ["cs.CE", "cs.LG", "math.OC"], "primary_category": "Subjects:       Computational Engineering, Finance, and Science (cs.CE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23443v1", "summary": "We introduce an adjoint-based aerodynamic shape optimization framework that\nintegrates a diffusion model trained on existing designs to learn a smooth\nmanifold of aerodynamically viable shapes. This manifold is enforced as an\nequality constraint to the shape optimization problem. Central to our method is\nthe computation of adjoint gradients of the design objectives (e.g., drag and\nlift) with respect to the manifold space. These gradients are derived by first\ncomputing shape derivatives with respect to conventional shape design\nparameters (e.g., Hicks-Henne parameters) and then backpropagating them through\nthe diffusion model to its latent space via automatic differentiation. Our\nframework preserves mathematical rigor and can be integrated into existing\nadjoint-based design workflows with minimal modification. Demonstrated on\nextensive transonic RANS airfoil design cases using off-the-shelf and\ngeneral-purpose nonlinear optimizers, our approach eliminates ad hoc parameter\ntuning and variable scaling, maintains robustness across initialization and\noptimizer choices, and achieves superior aerodynamic performance compared to\nconventional approaches. This work establishes how AI generated priors\nintegrates effectively with adjoint methods to enable robust, high-fidelity\naerodynamic shape optimization through automatic differentiation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23443v1", "cate": "cs.CE", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23105", "title": "The Squishy Grid Problem", "authors": ["Zixi Cai", "Kuowen Chen", "Shengquan Du", "Arnold Filtser", "Seth Pettie", "Daniel Skora"], "categories": ["cs.CG", "cs.DM", "cs.DS", "math.CO", "math.PR"], "primary_category": "Subjects:       Computational Geometry (cs.CG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23105v1", "summary": "In this paper we consider the problem of approximating Euclidean distances by\nthe infinite integer grid graph. Although the topology of the graph is fixed,\nwe have control over the edge-weight assignment $w:E\\to \\mathbb{R}_{\\ge 0}$,\nand hope to have grid distances be asymptotically isometric to Euclidean\ndistances, that is, for all grid points $u,v$, $\\mathrm{dist}_w(u,v) = (1\\pm\no(1))\\|u-v\\|_2$. We give three methods for solving this problem, each\nattractive in its own way.\n  * Our first construction is based on an embedding of the recursive,\nnon-periodic pinwheel tiling of Radin and Conway into the integer grid.\nDistances in the pinwheel graph are asymptotically isometric to Euclidean\ndistances, but no explicit bound on the rate of convergence was known. We prove\nthat the multiplicative distortion of the pinwheel graph is\n$(1+1/\\Theta(\\log^\\xi \\log D))$, where $D$ is the Euclidean distance and\n$\\xi=\\Theta(1)$. The pinwheel tiling approach is conceptually simple, but can\nbe improved quantitatively.\n  * Our second construction is based on a hierarchical arrangement of\n\"highways.\" It is simple, achieving stretch $(1 + 1/\\Theta(D^{1/9}))$, which\nconverges doubly exponentially faster than the pinwheel tiling approach.\n  * The first two methods are deterministic. An even simpler approach is to\nsample the edge weights independently from a common distribution $\\mathscr{D}$.\nWhether there exists a distribution $\\mathscr{D}^*$ that makes grid distances\nEuclidean, asymptotically and in expectation, is major open problem in the\ntheory of first passage percolation. Previous experiments show that when\n$\\mathscr{D}$ is a Fisher distribution, grid distances are within 1\\% of\nEuclidean. We demonstrate experimentally that this level of accuracy can be\nachieved by a simple 2-point distribution that assigns weights 0.41 or 4.75\nwith probability 44\\% and 56\\%, respectively.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23105v1", "cate": "cs.CG", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.23565", "title": "Semantic Chain-of-Trust: Autonomous Trust Orchestration for Collaborator Selection via Hypergraph-Aided Agentic AI", "authors": ["Botao Zhu", "Xianbin Wang", "Dusit Niyato"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23565v2", "summary": "In collaborative systems, the effective completion of tasks hinges on\ntask-specific trust evaluations of potential devices for distributed\ncollaboration. However, the complexity of tasks, the spatiotemporal dynamism of\ndistributed device resources, and the inevitable assessment overhead\ndramatically increase the complexity and resource consumption of the trust\nevaluation process. As a result, ill-timed or overly frequent trust evaluations\ncan reduce utilization rate of constrained resources, negatively affecting\ncollaborative task execution. To address this challenge, this paper proposes an\nautonomous trust orchestration method based on a new concept of semantic\nchain-of-trust. Our technique employs agentic AI and hypergraph to establish\nand maintain trust relationships among devices. By leveraging its strengths in\nautonomous perception, task decomposition, and semantic reasoning, we propose\nagentic AI to perceive device states and autonomously perform trust evaluations\nof collaborators based on historical performance data only during device idle\nperiods, thereby enabling efficient utilization of distributed resources. In\naddition, agentic AI performs task-specific trust evaluations on collaborator\nresources by analyzing the alignment between resource capabilities and task\nrequirements. Moreover, by maintaining a trust hypergraph embedded with trust\nsemantics for each device, agentic AI enables hierarchical management of\ncollaborators and identifies collaborators requiring trust evaluation based on\ntrust semantics, thereby achieving a balance between overhead and trust\naccuracy. Furthermore, local trust hypergraphs from multiple devices can be\nchained together to support multi-hop collaboration, enabling efficient\ncoordination in large-scale systems. Experimental results demonstrate that the\nproposed method achieves resource-efficient trust evaluation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23565v2", "cate": "cs.AI", "date": "2025-07-31", "updated": "2025-08-01"}
{"id": "2406.07108", "title": "On the power of adaption and randomization", "authors": ["David Krieg", "Erich Novak", "Mario Ullrich"], "categories": ["math.NA", "cs.CC", "cs.NA", "math.FA"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2406.07108v2", "summary": "We present bounds on the maximal gain of adaptive and randomized algorithms\nover non-adaptive, deterministic ones for approximating linear operators on\nconvex sets. If the sets are additionally symmetric, then our results are\noptimal. For non-symmetric sets, we unify some notions of $n$-widths and\ns-numbers, and show their connection to minimal errors. We also discuss\nextensions to non-linear widths and approximation based on function values, and\nconclude with a list of open problems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2406.07108v2", "cate": "math.NA", "date": "2024-06-11", "updated": "2025-07-31"}
{"id": "2507.22936", "title": "Evaluating Large Language Models (LLMs) in Financial NLP: A Comparative Study on Financial Report Analysis", "authors": ["Md Talha Mohsin"], "categories": ["cs.CL", "cs.AI", "cs.CE", "cs.HC", "q-fin.CP"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      22 Pages, 6 Tables, 7 Figures", "url": "http://arxiv.org/abs/2507.22936v1", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\na wide variety of Financial Natural Language Processing (FinNLP) tasks.\nHowever, systematic comparisons among widely used LLMs remain underexplored.\nGiven the rapid advancement and growing influence of LLMs in financial\nanalysis, this study conducts a thorough comparative evaluation of five leading\nLLMs, GPT, Claude, Perplexity, Gemini and DeepSeek, using 10-K filings from the\n'Magnificent Seven' technology companies. We create a set of domain-specific\nprompts and then use three methodologies to evaluate model performance: human\nannotation, automated lexical-semantic metrics (ROUGE, Cosine Similarity,\nJaccard), and model behavior diagnostics (prompt-level variance and\nacross-model similarity). The results show that GPT gives the most coherent,\nsemantically aligned, and contextually relevant answers; followed by Claude and\nPerplexity. Gemini and DeepSeek, on the other hand, have more variability and\nless agreement. Also, the similarity and stability of outputs change from\ncompany to company and over time, showing that they are sensitive to how\nprompts are written and what source material is used.", "comment": "22 Pages, 6 Tables, 7 Figures", "pdf_url": "http://arxiv.org/pdf/2507.22936v1", "cate": "cs.CL", "date": "2025-07-24", "updated": "2025-07-24"}
{"id": "2412.03865", "title": "Dudeney's Dissection is Optimal", "authors": ["Erik D. Demaine", "Tonan Kamata", "Ryuhei Uehara"], "categories": ["cs.CG", "cs.DM", "math.GT"], "primary_category": "Subjects:       Computational Geometry (cs.CG)", "pdf_link": null, "comments": "Comments:      26 pages, 32 figures, Refinement and Correction of Case Analysis of Cut graphs", "url": "http://arxiv.org/abs/2412.03865v4", "summary": "In 1907, Henry Ernest Dudeney posed a puzzle: ``cut any equilateral triangle\n\\dots\\ into as few pieces as possible that will fit together and form a perfect\nsquare'' (without overlap, via translation and rotation).\n  Four weeks later, Dudeney demonstrated a beautiful four-piece solution, which\ntoday remains perhaps the most famous example of dissection.\n  In this paper (over a century later), we finally solve Dudeney's puzzle, by\nproving that the equilateral triangle and square have no common dissection with\nthree or fewer polygonal pieces.\n  We reduce the problem to the analysis of discrete graph structures\nrepresenting the correspondence between the edges and the vertices of the\npieces forming each polygon.", "comment": "26 pages, 32 figures. The previous version mistakenly compiled an\n  outdated file. This update corrects that and includes the intended version\n  with refined and corrected case analysis of cut graphs", "pdf_url": "http://arxiv.org/pdf/2412.03865v4", "cate": "cs.CG", "date": "2024-12-05", "updated": "2025-08-01"}
{"id": "2507.23633", "title": "MemoCue: Empowering LLM-Based Agents for Human Memory Recall via Strategy-Guided Querying", "authors": ["Qian Zhao", "Zhuo Sun", "Bin Guo", "Zhiwen Yu"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23633v1", "summary": "Agent-assisted memory recall is one critical research problem in the field of\nhuman-computer interaction. In conventional methods, the agent can retrieve\ninformation from its equipped memory module to help the person recall\nincomplete or vague memories. The limited size of memory module hinders the\nacquisition of complete memories and impacts the memory recall performance in\npractice. Memory theories suggest that the person's relevant memory can be\nproactively activated through some effective cues. Inspired by this, we propose\na novel strategy-guided agent-assisted memory recall method, allowing the agent\nto transform an original query into a cue-rich one via the judiciously designed\nstrategy to help the person recall memories. To this end, there are two key\nchallenges. (1) How to choose the appropriate recall strategy for diverse\nforgetting scenarios with distinct memory-recall characteristics? (2) How to\nobtain the high-quality responses leveraging recall strategies, given only\nabstract and sparsely annotated strategy patterns? To address the challenges,\nwe propose a Recall Router framework. Specifically, we design a 5W Recall Map\nto classify memory queries into five typical scenarios and define fifteen\nrecall strategy patterns across the corresponding scenarios. We then propose a\nhierarchical recall tree combined with the Monte Carlo Tree Search algorithm to\noptimize the selection of strategy and the generation of strategy responses. We\nconstruct an instruction tuning dataset and fine-tune multiple open-source\nlarge language models (LLMs) to develop MemoCue, an agent that excels in\nproviding memory-inspired responses. Experiments on three representative\ndatasets show that MemoCue surpasses LLM-based methods by 17.74% in recall\ninspiration. Further human evaluation highlights its advantages in\nmemory-recall applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23633v1", "cate": "cs.AI", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2407.00688", "title": "On the Number of Quantifiers Needed to Define Boolean Functions", "authors": ["Marco Carmosino", "Ronald Fagin", "Neil Immerman", "Phokion Kolaitis", "Jonathan Lenchner", "Rik Sengupta"], "categories": ["cs.LO", "cs.CC"], "primary_category": "Subjects:       Logic in Computer Science (cs.LO)", "pdf_link": null, "comments": "Comments:      Full version of version that is to appear in Proceedings of 49th International Symposium on Mathematical Foundations of Computer Science, 2024. arXiv admin note: substantial text overlap with arXiv:2402.10293", "url": "http://arxiv.org/abs/2407.00688v3", "summary": "The number of quantifiers needed to express first-order (FO) properties is\ncaptured by two-player combinatorial games called multi-structural games. We\nanalyze these games on binary strings with an ordering relation, using a\ntechnique we call parallel play, which significantly reduces the number of\nquantifiers needed in many cases. Ordered structures such as strings have\nhistorically been notoriously difficult to analyze in the context of these and\nsimilar games. Nevertheless, in this paper, we provide essentially tight upper\nbounds on the number of quantifiers needed to characterize different-sized\nsubsets of strings. The results immediately give bounds on the number of\nquantifiers necessary to define several different classes of Boolean functions.\nOne of our results is analogous to Lupanov's upper bounds on circuit size and\nformula size in propositional logic: we show that every Boolean function on\n$n$-bit inputs can be defined by a FO sentence having $(1 +\n\\varepsilon)n\\log(n) + O(1)$ quantifiers, and that this is essentially tight.\nWe reduce this number to $(1 + \\varepsilon)\\log(n) + O(1)$ when the Boolean\nfunction in question is sparse.", "comment": "Full version of version that is to appear in Proceedings of 49th\n  International Symposium on Mathematical Foundations of Computer Science,\n  2024. arXiv admin note: substantial text overlap with arXiv:2402.10293", "pdf_url": "http://arxiv.org/pdf/2407.00688v3", "cate": "cs.LO", "date": "2024-06-30", "updated": "2025-07-31"}
{"id": "2507.22959", "title": "Scientific Machine Learning with Kolmogorov-Arnold Networks", "authors": ["Salah A. Faroughi", "Farinaz Mostajeran", "Amin Hamed Mashhadzadeh", "Shirko Faroughi"], "categories": ["cs.LG", "cs.CE", "math-ph", "math.MP"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22959v1", "summary": "The field of scientific machine learning, which originally utilized\nmultilayer perceptrons (MLPs), is increasingly adopting Kolmogorov-Arnold\nNetworks (KANs) for data encoding. This shift is driven by the limitations of\nMLPs, including poor interpretability, fixed activation functions, and\ndifficulty capturing localized or high-frequency features. KANs address these\nissues with enhanced interpretability and flexibility, enabling more efficient\nmodeling of complex nonlinear interactions and effectively overcoming the\nconstraints associated with conventional MLP architectures. This review\ncategorizes recent progress in KAN-based models across three distinct\nperspectives: (i) data-driven learning, (ii) physics-informed modeling, and\n(iii) deep operator learning. Each perspective is examined through the lens of\narchitectural design, training strategies, application efficacy, and\ncomparative evaluation against MLP-based counterparts. By benchmarking KANs\nagainst MLPs, we highlight consistent improvements in accuracy, convergence,\nand spectral representation, clarifying KANs' advantages in capturing complex\ndynamics while learning more effectively. Finally, this review identifies\ncritical challenges and open research questions in KAN development,\nparticularly regarding computational efficiency, theoretical guarantees,\nhyperparameter tuning, and algorithm complexity. We also outline future\nresearch directions aimed at improving the robustness, scalability, and\nphysical consistency of KAN-based frameworks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22959v1", "cate": "cs.LG", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.23664", "title": "Personalized Education with Ranking Alignment Recommendation", "authors": ["Haipeng Liu", "Yuxuan Liu", "Ting Long"], "categories": ["cs.AI", "cs.IR"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23664v1", "summary": "Personalized question recommendation aims to guide individual students\nthrough questions to enhance their mastery of learning targets. Most previous\nmethods model this task as a Markov Decision Process and use reinforcement\nlearning to solve, but they struggle with efficient exploration, failing to\nidentify the best questions for each student during training. To address this,\nwe propose Ranking Alignment Recommendation (RAR), which incorporates\ncollaborative ideas into the exploration mechanism, enabling more efficient\nexploration within limited training episodes. Experiments show that RAR\neffectively improves recommendation performance, and our framework can be\napplied to any RL-based question recommender. Our code is available in\nhttps://github.com/wuming29/RAR.git.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23664v1", "cate": "cs.AI", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2410.03094", "title": "Entanglement-induced provable and robust quantum learning advantages", "authors": ["Haimeng Zhao", "Dong-Ling Deng"], "categories": ["quant-ph", "cs.CC", "cs.LG"], "primary_category": "Subjects:       Quantum Physics (quant-ph)", "pdf_link": null, "comments": "Comments:      7 pages, 2 figures + 13-page supplementary materials", "url": "http://arxiv.org/abs/2410.03094v2", "summary": "Quantum computing holds unparalleled potentials to enhance machine learning.\nHowever, a demonstration of quantum learning advantage has not been achieved so\nfar. We make a step forward by rigorously establishing a noise-robust,\nunconditional quantum learning advantage in expressivity, inference speed, and\ntraining efficiency, compared to commonly-used classical models. Our proof is\ninformation-theoretic and pinpoints the origin of this advantage: entanglement\ncan be used to reduce the communication required by non-local tasks. In\nparticular, we design a task that can be solved with certainty by quantum\nmodels with a constant number of parameters using entanglement, whereas\ncommonly-used classical models must scale linearly to achieve a\nlarger-than-exponentially-small accuracy. We show that the quantum model is\ntrainable with constant resources and robust against constant noise. Through\nnumerical and trapped-ion experiments on IonQ Aria, we demonstrate the desired\nadvantage. Our results provide valuable guidance for demonstrating quantum\nlearning advantages with current noisy intermediate-scale devices.", "comment": "7 pages, 2 figures + 13-page supplementary materials", "pdf_url": "http://arxiv.org/pdf/2410.03094v2", "cate": "quant-ph", "date": "2024-10-04", "updated": "2025-07-31"}
{"id": "2507.23600", "title": "EB-gMCR: Energy-Based Generative Modeling for Signal Unmixing and Multivariate Curve Resolution", "authors": ["Yu-Tang Chang", "Shih-Fang Chen"], "categories": ["cs.LG", "cs.CE", "G.1.6; G.3; G.4; I.6.5"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23600v1", "summary": "Signal unmixing analysis decomposes data into basic patterns and is widely\napplied in chemical and biological research. Multivariate curve resolution\n(MCR), a branch of signal unmixing, separates mixed chemical signals into base\npatterns (components) and their concentrations, playing a key role in\nunderstanding composition. Classical MCR is typically framed as matrix\nfactorization (MF) and requires a user-specified component count, usually\nunknown in real data. As dataset size or component count increases, the\nscalability and reliability of MF-based MCR face significant challenges. This\nstudy reformulates MCR as a generative process (gMCR), and introduces an\nenergy-based deep learning solver, EB-gMCR, that automatically discovers the\nsmallest component set able to reconstruct the data faithfully. EB-gMCR starts\nfrom a large candidate pool (e.g., 1024 spectra) and employs a differentiable\ngating network to retain only active components while estimating their\nconcentrations. On noisy synthetic datasets containing up to 256 latent\nsources, EB-gMCR maintained R^2 >= 0.98 and recovered the component count\nwithin 5% of the ground truth; at lower noise it achieved R^2 >= 0.99 with near\nexact component estimation. Additional chemical priors, such as non-negativity\nor nonlinear mixing, enter as simple plug-in functions, enabling adaptation to\nother instruments or domains without altering the core learning process. By\nuniting high-capacity generative modeling and hard component selection, EB-gMCR\noffers a practical route to large-scale signal unmixing analysis, including\nchemical library-driven scenarios. The source code is available at\nhttps://github.com/b05611038/ebgmcr_solver.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23600v1", "cate": "cs.LG", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23701", "title": "TextQuests: How Good are LLMs at Text-Based Video Games?", "authors": ["Long Phan", "Mantas Mazeika", "Andy Zou", "Dan Hendrycks"], "categories": ["cs.AI", "cs.CL"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23701v1", "summary": "Evaluating AI agents within complex, interactive environments that mirror\nreal-world challenges is critical for understanding their practical\ncapabilities. While existing agent benchmarks effectively assess skills like\ntool use or performance on structured tasks, they often do not fully capture an\nagent's ability to operate autonomously in exploratory environments that demand\nsustained, self-directed reasoning over a long and growing context. To spur the\ndevelopment of agents capable of more robust intrinsic reasoning over long\nhorizons, we introduce TextQuests, a benchmark based on the Infocom suite of\ninteractive fiction games. These text-based adventures, which can take human\nplayers over 30 hours and require hundreds of precise actions to solve, serve\nas an effective proxy for evaluating AI agents on focused, stateful tasks. The\nbenchmark is specifically designed to assess an LLM agent's capacity for\nself-contained problem-solving by precluding the use of external tools, thereby\nfocusing on intrinsic long-context reasoning capabilities in an exploratory\nenvironment characterized by the need for trial-and-error learning and\nsustained problem-solving within a single interactive session. We release\nTextQuests at https://textquests.ai.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23701v1", "cate": "cs.AI", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.22838", "title": "Modelling and simulation of electro-mechanically coupled dielectric elastomers and myocardial tissue using smoothed finite element methods", "authors": ["Tan Tran", "Denisa Martonova", "Sigrid Leyendecker"], "categories": ["cs.CE"], "primary_category": "Subjects:       Computational Engineering, Finance, and Science (cs.CE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22838v2", "summary": "Computational modelling offers a cost-effective and time-efficient\nalternative to experimental studies in biomedical engineering. In cardiac\nelectro-mechanics, finite element method (FEM)-based simulations provide\nvaluable insights into diseased tissue behaviour and the development of\nassistive systems such as di-electric elastomer actuators. However, the use of\nautomatically generated tetrahedral meshes, commonly applied due to geometric\ncomplexity, often leads to numerical issues including overly stiff responses\nand volume locking, particularly in incompressible materials. Smoothed finite\nelement methods (S-FEMs) offer a promising alternative by softening the\nstiffness matrix through gradient smoothing over defined smoothing domains.\nThis work extends S-FEM formulations to electro-mechanically coupled problems\nand compares their performance against standard linear FEM. We implement and\nevaluate four approaches in the Abaqus environment via custom user elements:\nstandard linear FEM, face-based S-FEM (FS-FEM), node-based S-FEM (NS-FEM), and\nthe hybrid face/node-based S-FEM (FSNS-FEM). Two benchmark problems are\nstudied: the electrically induced contraction of a compressible dielectric\nelastomer and an incompressible, orthotropic myocardial tissue sample.\nReference solutions are obtained using a mesh consisting of higher-order\nelements. Our results demonstrate that FSNS-FEM provides the best balance\nbetween accuracy and computational efficiency, closely matching reference data.\nNS-FEM produces softer results, which leads to an overestimation of the true\ndeformation. FS-FEM and standard FEM consistently exhibit overly stiff\nbehaviour, with pronounced volume locking in the myocardial case. These\nfindings support the potential of S-FEMs, in particular FSNS-FEM, for accurate\nsimulation of coupled electro-mechanical behaviour in complex biomedical\napplications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22838v2", "cate": "cs.CE", "date": "2025-07-30", "updated": "2025-07-31"}
{"id": "2507.23726", "title": "Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving", "authors": ["Luoxin Chen", "Jinming Gu", "Liankai Huang", "Wenhao Huang", "Zhicheng Jiang", "Allan Jie", "Xiaoran Jin", "Xing Jin", "Chenggang Li", "Kaijing Ma", "Cheng Ren", "Jiawei Shen", "Wenlei Shi", "Tong Sun", "He Sun", "Jiahui Wang", "Siran Wang", "Zhihong Wang", "Chenrui Wei", "Shufa Wei", "Yonghui Wu", "Yuchen Wu", "Yihang Xia", "Huajian Xin", "Fan Yang", "Huaiyuan Ying", "Hongyi Yuan", "Zheng Yuan", "Tianyang Zhan", "Chi Zhang", "Yue Zhang", "Ge Zhang", "Tianyun Zhao", "Jianqiu Zhao", "Yichi Zhou", "Thomas Hanwen Zhu"], "categories": ["cs.AI", "cs.CL"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23726v2", "summary": "LLMs have demonstrated strong mathematical reasoning abilities by leveraging\nreinforcement learning with long chain-of-thought, yet they continue to\nstruggle with theorem proving due to the lack of clear supervision signals when\nsolely using natural language. Dedicated domain-specific languages like Lean\nprovide clear supervision via formal verification of proofs, enabling effective\ntraining through reinforcement learning. In this work, we propose\n\\textbf{Seed-Prover}, a lemma-style whole-proof reasoning model. Seed-Prover\ncan iteratively refine its proof based on Lean feedback, proved lemmas, and\nself-summarization. To solve IMO-level contest problems, we design three\ntest-time inference strategies that enable both deep and broad reasoning.\nSeed-Prover proves $78.1\\%$ of formalized past IMO problems, saturates MiniF2F,\nand achieves over 50\\% on PutnamBench, outperforming the previous\nstate-of-the-art by a large margin. To address the lack of geometry support in\nLean, we introduce a geometry reasoning engine \\textbf{Seed-Geometry}, which\noutperforms previous formal geometry engines. We use these two systems to\nparticipate in IMO 2025 and fully prove 5 out of 6 problems. This work\nrepresents a significant advancement in automated mathematical reasoning,\ndemonstrating the effectiveness of formal verification with long\nchain-of-thought reasoning.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23726v2", "cate": "cs.AI", "date": "2025-07-31", "updated": "2025-08-01"}
{"id": "2504.14928", "title": "EducationQ: Evaluating LLMs' Teaching Capabilities Through Multi-Agent Dialogue Framework", "authors": ["Yao Shi", "Rongkeng Liang", "Yong Xu"], "categories": ["cs.AI", "cs.CE", "cs.CL", "cs.CY", "cs.HC"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      Paper URL: this https URL ;Presentation Video: this https URL", "url": "http://arxiv.org/abs/2504.14928v3", "summary": "Large language models (LLMs) increasingly serve as educational tools, yet\nevaluating their teaching capabilities remains challenging due to the\nresource-intensive, context-dependent, and methodologically complex nature of\nteacher-student interactions. We introduce EducationQ, a multi-agent dialogue\nframework that efficiently assesses teaching capabilities through simulated\ndynamic educational scenarios, featuring specialized agents for teaching,\nlearning, and evaluation. Testing 14 LLMs across major AI Organizations\n(OpenAI, Meta, Google, Anthropic, and others) on 1,498 questions spanning 13\ndisciplines and 10 difficulty levels reveals that teaching effectiveness does\nnot correlate linearly with model scale or general reasoning capabilities -\nwith some smaller open-source models outperforming larger commercial\ncounterparts in teaching contexts. This finding highlights a critical gap in\ncurrent evaluations that prioritize knowledge recall over interactive pedagogy.\nOur mixed-methods evaluation, combining quantitative metrics with qualitative\nanalysis and expert case studies, identifies distinct pedagogical strengths\nemployed by top-performing models (e.g., sophisticated questioning strategies,\nadaptive feedback mechanisms). Human expert evaluations show 78% agreement with\nour automated qualitative analysis of effective teaching behaviors, validating\nour methodology. EducationQ demonstrates that LLMs-as-teachers require\nspecialized optimization beyond simple scaling, suggesting next-generation\neducational AI prioritize targeted enhancement of specific pedagogical\neffectiveness.", "comment": "Paper URL: https://aclanthology.org/2025.acl-long.1576 ;Presentation\n  Video: https://www.youtube.com/watch?v=j63ooKE50I0", "pdf_url": "http://arxiv.org/pdf/2504.14928v3", "cate": "cs.AI", "date": "2025-04-21", "updated": "2025-07-31"}
{"id": "2507.23751", "title": "CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning and non-reasoning tasks", "authors": ["Ping Yu", "Jack Lanchantin", "Tianlu Wang", "Weizhe Yuan", "Olga Golovneva", "Ilia Kulikov", "Sainbayar Sukhbaatar", "Jason Weston", "Jing Xu"], "categories": ["cs.AI", "cs.CL"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23751v1", "summary": "We propose CoT-Self-Instruct, a synthetic data generation method that\ninstructs LLMs to first reason and plan via Chain-of-Thought (CoT) based on the\ngiven seed tasks, and then to generate a new synthetic prompt of similar\nquality and complexity for use in LLM training, followed by filtering for\nhigh-quality data with automatic metrics. In verifiable reasoning, our\nsynthetic data significantly outperforms existing training datasets, such as\ns1k and OpenMathReasoning, across MATH500, AMC23, AIME24 and GPQA-Diamond. For\nnon-verifiable instruction-following tasks, our method surpasses the\nperformance of human or standard self-instruct prompts on both AlpacaEval 2.0\nand Arena-Hard.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23751v1", "cate": "cs.AI", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.07426", "title": "DrugMCTS: a drug repurposing framework combining multi-agent, RAG and Monte Carlo Tree Search", "authors": ["Zerui Yang", "Yuwei Wan", "Siyu Yan", "Yudai Matsuda", "Tong Xie", "Bram Hoex", "Linqi Song"], "categories": ["cs.AI", "cs.CE"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07426v3", "summary": "Recent advances in large language models have demonstrated considerable\npotential in scientific domains such as drug repositioning. However, their\neffectiveness remains constrained when reasoning extends beyond the knowledge\nacquired during pretraining. Conventional approaches, such as fine-tuning or\nretrieval-augmented generation, face limitations in either imposing high\ncomputational overhead or failing to fully exploit structured scientific data.\nTo overcome these challenges, we propose DrugMCTS, a novel framework that\nsynergistically integrates RAG, multi-agent collaboration, and Monte Carlo Tree\nSearch for drug repositioning. The framework employs five specialized agents\ntasked with retrieving and analyzing molecular and protein information, thereby\nenabling structured and iterative reasoning. Extensive experiments on the\nDrugBank and KIBA datasets demonstrate that DrugMCTS achieves substantially\nhigher recall and robustness compared to both general-purpose LLMs and deep\nlearning baselines. Our results highlight the importance of structured\nreasoning, agent-based collaboration, and feedback-driven search mechanisms in\nadvancing LLM applications for drug repositioning.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07426v3", "cate": "cs.AI", "date": "2025-07-10", "updated": "2025-07-31"}
{"id": "2507.23773", "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "authors": ["Mingkai Deng", "Jinyu Hou", "Yilin Shen", "Hongxia Jin", "Graham Neubig", "Zhiting Hu", "Eric Xing"], "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.RO"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23773v1", "summary": "AI agents built on large language models (LLMs) hold enormous promise, but\ncurrent practice focuses on a one-task-one-agent approach, which not only falls\nshort of scalability and generality, but also suffers from the fundamental\nlimitations of autoregressive LLMs. On the other hand, humans are general\nagents who reason by mentally simulating the outcomes of their actions and\nplans. Moving towards a more general and powerful AI agent, we introduce\nSimuRA, a goal-oriented architecture for generalized agentic reasoning. Based\non a principled formulation of optimal agent in any environment, \\modelname\novercomes the limitations of autoregressive reasoning by introducing a world\nmodel for planning via simulation. The generalized world model is implemented\nusing LLM, which can flexibly plan in a wide range of environments using the\nconcept-rich latent space of natural language. Experiments on difficult web\nbrowsing tasks show that \\modelname improves the success of flight search from\n0\\% to 32.2\\%. World-model-based planning, in particular, shows consistent\nadvantage of up to 124\\% over autoregressive planning, demonstrating the\nadvantage of world model simulation as a reasoning paradigm. We are excited\nabout the possibility for training a single, general agent model based on LLMs\nthat can act superintelligently in all environments. To start, we make SimuRA,\na web-browsing agent built on \\modelname with pretrained LLMs, available as a\nresearch demo for public testing.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23773v1", "cate": "cs.AI", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23229", "title": "Fine-Grained Privacy Extraction from Retrieval-Augmented Generation Systems via Knowledge Asymmetry Exploitation", "authors": ["Yufei Chen", "Yao Wang", "Haibin Zhang", "Tao Gu"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23229v1", "summary": "Retrieval-augmented generation (RAG) systems enhance large language models\n(LLMs) by integrating external knowledge bases, but this advancement introduces\nsignificant privacy risks. Existing privacy attacks on RAG systems can trigger\ndata leakage but often fail to accurately isolate knowledge-base-derived\nsentences within mixed responses. They also lack robustness when applied across\nmultiple domains. This paper addresses these challenges by presenting a novel\nblack-box attack framework that exploits knowledge asymmetry between RAG and\nstandard LLMs to achieve fine-grained privacy extraction across heterogeneous\nknowledge landscapes. We propose a chain-of-thought reasoning strategy that\ncreates adaptive prompts to steer RAG systems away from sensitive content.\nSpecifically, we first decompose adversarial queries to maximize information\ndisparity and then apply a semantic relationship scoring to resolve lexical and\nsyntactic ambiguities. We finally train a neural network on these feature\nscores to precisely identify sentences containing private information. Unlike\nprior work, our framework generalizes to unseen domains through iterative\nrefinement without pre-defined knowledge. Experimental results show that we\nachieve over 91% privacy extraction rate in single-domain and 83% in\nmulti-domain scenarios, reducing sensitive sentence exposure by over 65% in\ncase studies. This work bridges the gap between attack and defense in RAG\nsystems, enabling precise extraction of private information while providing a\nfoundation for adaptive mitigation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23229v1", "cate": "cs.CR", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2503.21813", "title": "OAEI-LLM-T: A TBox Benchmark Dataset for Understanding Large Language Model Hallucinations in Ontology Matching", "authors": ["Zhangcheng Qiang", "Kerry Taylor", "Weiqing Wang", "Jing Jiang"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      14 pages, 4 figures, 4 tables, 2 prompt templates", "url": "http://arxiv.org/abs/2503.21813v3", "summary": "Hallucinations are often inevitable in downstream tasks using large language\nmodels (LLMs). To tackle the substantial challenge of addressing hallucinations\nfor LLM-based ontology matching (OM) systems, we introduce a new benchmark\ndataset OAEI-LLM-T. The dataset evolves from seven TBox datasets in the\nOntology Alignment Evaluation Initiative (OAEI), capturing hallucinations of\nten different LLMs performing OM tasks. These OM-specific hallucinations are\norganised into two primary categories and six sub-categories. We showcase the\nusefulness of the dataset in constructing an LLM leaderboard for OM tasks and\nfor fine-tuning LLMs used in OM tasks.", "comment": "14 pages, 4 figures, 4 tables, 2 prompt templates", "pdf_url": "http://arxiv.org/pdf/2503.21813v3", "cate": "cs.CL", "date": "2025-03-25", "updated": "2025-05-14"}
{"id": "2507.23453", "title": "Counterfactual Evaluation for Blind Attack Detection in LLM-based Evaluation Systems", "authors": ["Lijia Liu", "Takumi Kondo", "Kyohei Atarashi", "Koh Takeuchi", "Jiyi Li", "Shigeru Saito", "Hisashi Kashima"], "categories": ["cs.CR", "cs.CL"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23453v1", "summary": "This paper investigates defenses for LLM-based evaluation systems against\nprompt injection. We formalize a class of threats called blind attacks, where a\ncandidate answer is crafted independently of the true answer to deceive the\nevaluator. To counter such attacks, we propose a framework that augments\nStandard Evaluation (SE) with Counterfactual Evaluation (CFE), which\nre-evaluates the submission against a deliberately false ground-truth answer.\nAn attack is detected if the system validates an answer under both standard and\ncounterfactual conditions. Experiments show that while standard evaluation is\nhighly vulnerable, our SE+CFE framework significantly improves security by\nboosting attack detection with minimal performance trade-offs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23453v1", "cate": "cs.CR", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.22890", "title": "Evaluating LLMs for Visualization Generation and Understanding", "authors": ["Saadiq Rauf Khan", "Vinit Chandak", "Sougata Mukherjea"], "categories": ["cs.HC", "cs.AI"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22890v1", "summary": "Information Visualization has been utilized to gain insights from complex\ndata. In recent times, Large Language models (LLMs) have performed very well in\nmany tasks. In this paper, we showcase the capabilities of different popular\nLLMs to generate code for visualization based on simple prompts. We also\nanalyze the power of LLMs to understand some common visualizations by answering\nquestions. Our study shows that LLMs could generate code for some simpler\nvisualizations such as bar and pie charts. Moreover, they could answer simple\nquestions about visualizations. However, LLMs also have several limitations.\nFor example, some of them had difficulty generating complex visualizations,\nsuch as violin plot. LLMs also made errors in answering some questions about\nvisualizations, for example, identifying relationships between close boundaries\nand determining lengths of shapes. We believe that our insights can be used to\nimprove both LLMs and Information Visualization systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22890v1", "cate": "cs.HC", "date": "2025-06-16", "updated": "2025-06-16"}
{"id": "2507.23611", "title": "LLM-Based Identification of Infostealer Infection Vectors from Screenshots: The Case of Aurora", "authors": ["Estelle Ruellan", "Eric Clay", "Nicholas Ascoli"], "categories": ["cs.CR", "cs.AI", "cs.CV"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23611v1", "summary": "Infostealers exfiltrate credentials, session cookies, and sensitive data from\ninfected systems. With over 29 million stealer logs reported in 2024, manual\nanalysis and mitigation at scale are virtually unfeasible/unpractical. While\nmost research focuses on proactive malware detection, a significant gap remains\nin leveraging reactive analysis of stealer logs and their associated artifacts.\nSpecifically, infection artifacts such as screenshots, image captured at the\npoint of compromise, are largely overlooked by the current literature. This\npaper introduces a novel approach leveraging Large Language Models (LLMs), more\nspecifically gpt-4o-mini, to analyze infection screenshots to extract potential\nIndicators of Compromise (IoCs), map infection vectors, and track campaigns.\nFocusing on the Aurora infostealer, we demonstrate how LLMs can process\nscreenshots to identify infection vectors, such as malicious URLs, installer\nfiles, and exploited software themes. Our method extracted 337 actionable URLs\nand 246 relevant files from 1000 screenshots, revealing key malware\ndistribution methods and social engineering tactics. By correlating extracted\nfilenames, URLs, and infection themes, we identified three distinct malware\ncampaigns, demonstrating the potential of LLM-driven analysis for uncovering\ninfection workflows and enhancing threat intelligence. By shifting malware\nanalysis from traditional log-based detection methods to a reactive,\nartifact-driven approach that leverages infection screenshots, this research\npresents a scalable method for identifying infection vectors and enabling early\nintervention.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23611v1", "cate": "cs.CR", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.22910", "title": "Large Language Models in the Travel Domain: An Industrial Experience", "authors": ["Sergio Di Meglio", "Aniello Somma", "Luigi Libero Lucio Starace", "Fabio Scippacercola", "Giancarlo Sperlì", "Sergio Di Martino"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Manuscript accepted to the International Conference on Software Engineering and Knowledge Engineering (SEKE) 2025", "url": "http://arxiv.org/abs/2507.22910v1", "summary": "Online property booking platforms are widely used and rely heavily on\nconsistent, up-to-date information about accommodation facilities, often\nsourced from third-party providers. However, these external data sources are\nfrequently affected by incomplete or inconsistent details, which can frustrate\nusers and result in a loss of market. In response to these challenges, we\npresent an industrial case study involving the integration of Large Language\nModels (LLMs) into CALEIDOHOTELS, a property reservation platform developed by\nFERVENTO. We evaluate two well-known LLMs in this context: Mistral 7B,\nfine-tuned with QLoRA, and Mixtral 8x7B, utilized with a refined system prompt.\nBoth models were assessed based on their ability to generate consistent and\nhomogeneous descriptions while minimizing hallucinations. Mixtral 8x7B\noutperformed Mistral 7B in terms of completeness (99.6% vs. 93%), precision\n(98.8% vs. 96%), and hallucination rate (1.2% vs. 4%), producing shorter yet\nmore concise content (249 vs. 277 words on average). However, this came at a\nsignificantly higher computational cost: 50GB VRAM and $1.61/hour versus 5GB\nand $0.16/hour for Mistral 7B. Our findings provide practical insights into the\ntrade-offs between model quality and resource efficiency, offering guidance for\ndeploying LLMs in production environments and demonstrating their effectiveness\nin enhancing the consistency and reliability of accommodation data.", "comment": "Manuscript accepted to the International Conference on Software\n  Engineering and Knowledge Engineering (SEKE) 2025", "pdf_url": "http://arxiv.org/pdf/2507.22910v1", "cate": "cs.CL", "date": "2025-07-18", "updated": "2025-07-18"}
{"id": "2507.22893", "title": "Invisible Architectures of Thought: Toward a New Science of AI as Cognitive Infrastructure", "authors": ["Giuseppe Riva"], "categories": ["cs.HC", "cs.AI"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22893v1", "summary": "Contemporary human-AI interaction research overlooks how AI systems\nfundamentally reshape human cognition pre-consciously, a critical blind spot\nfor understanding distributed cognition. This paper introduces \"Cognitive\nInfrastructure Studies\" (CIS) as a new interdisciplinary domain to\nreconceptualize AI as \"cognitive infrastructures\": foundational, often\ninvisible systems conditioning what is knowable and actionable in digital\nsocieties. These semantic infrastructures transport meaning, operate through\nanticipatory personalization, and exhibit adaptive invisibility, making their\ninfluence difficult to detect. Critically, they automate \"relevance judgment,\"\nshifting the \"locus of epistemic agency\" to non-human systems. Through\nnarrative scenarios spanning individual (cognitive dependency), collective\n(democratic deliberation), and societal (governance) scales, we describe how\ncognitive infrastructures reshape human cognition, public reasoning, and social\nepistemologies. CIS aims to address how AI preprocessing reshapes distributed\ncognition across individual, collective, and cultural scales, requiring\nunprecedented integration of diverse disciplinary methods. The framework also\naddresses critical gaps across disciplines: cognitive science lacks\npopulation-scale preprocessing analysis capabilities, digital sociology cannot\naccess individual cognitive mechanisms, and computational approaches miss\ncultural transmission dynamics. To achieve this goal CIS also provides\nmethodological innovations for studying invisible algorithmic influence:\n\"infrastructure breakdown methodologies\", experimental approaches that reveal\ncognitive dependencies by systematically withdrawing AI preprocessing after\nperiods of habituation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22893v1", "cate": "cs.HC", "date": "2025-06-19", "updated": "2025-06-19"}
{"id": "2507.23641", "title": "Polynomial Lattices for the BIKE Cryptosystem", "authors": ["Michael Schaller"], "categories": ["cs.CR", "11T71, 94A60"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23641v1", "summary": "In this paper we introduce a rank $2$ lattice over a polynomial ring arising\nfrom the public key of the BIKE cryptosystem \\cite{aragon2022bike}. The secret\nkey is a sparse vector in this lattice. We study properties of this lattice and\ngeneralize the recovery of weak keys from \\cite{BardetDLO16}. In particular, we\nshow that they implicitly solved a shortest vector problem in the lattice we\nconstructed. Rather than finding only a shortest vector, we obtain a reduced\nbasis of the lattice which makes it possible to check for more weak keys.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23641v1", "cate": "cs.CR", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.22911", "title": "ElectriQ: A Benchmark for Assessing the Response Capability of Large Language Models in Power Marketing", "authors": ["Jinzhi Wang", "Qingke Peng", "Haozhou Li", "Zeyuan Zeng", "Qinfeng Song", "Kaixuan Yang", "Jiangbo Zhang", "Yaoying Wang", "Ruimeng Li", "Biyi Zhou"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22911v1", "summary": "Electric power marketing customer service plays a critical role in addressing\ninquiries, complaints, and service requests. However, current systems, such as\nChina's 95598 hotline, often struggle with slow response times, inflexible\nprocedures, and limited accuracy in domain-specific tasks. While large language\nmodels (LLMs) like GPT-4o and Claude 3 demonstrate strong general capabilities,\nthey lack the domain expertise and empathy required in this field. To bridge\nthis gap, we introduce ElectriQ, the first benchmark designed to evaluate and\nenhance LLMs in electric power marketing scenarios. ElectriQ consists of a\ndialogue dataset covering six key service categories and introduces four\nevaluation metrics: professionalism, popularity, readability, and\nuser-friendliness. We further incorporate a domain-specific knowledge base and\npropose a knowledge augmentation method to boost model performance. Experiments\non 13 LLMs reveal that smaller models such as LLama3-8B, when fine-tuned and\naugmented, can surpass GPT-4o in terms of professionalism and\nuser-friendliness. ElectriQ establishes a comprehensive foundation for\ndeveloping LLMs tailored to the needs of power marketing services.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22911v1", "cate": "cs.CL", "date": "2025-07-19", "updated": "2025-07-19"}
{"id": "2507.22896", "title": "iLearnRobot: An Interactive Learning-Based Multi-Modal Robot with Continuous Improvement", "authors": ["Kohou Wang", "ZhaoXiang Liu", "Lin Bai", "Kun Fan", "Xiang Liu", "Huan Hu", "Kai Wang", "Shiguo Lian"], "categories": ["cs.HC", "cs.AI", "cs.CV", "cs.RO"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      17 pages, 12 figures", "url": "http://arxiv.org/abs/2507.22896v1", "summary": "It is crucial that robots' performance can be improved after deployment, as\nthey are inherently likely to encounter novel scenarios never seen before. This\npaper presents an innovative solution: an interactive learning-based robot\nsystem powered by a Multi-modal Large Language Model(MLLM). A key feature of\nour system is its ability to learn from natural dialogues with non-expert\nusers. We also propose chain of question to clarify the exact intent of the\nquestion before providing an answer and dual-modality retrieval modules to\nleverage these interaction events to avoid repeating same mistakes, ensuring a\nseamless user experience before model updates, which is in contrast to current\nmainstream MLLM-based robotic systems. Our system marks a novel approach in\nrobotics by integrating interactive learning, paving the way for superior\nadaptability and performance in diverse environments. We demonstrate the\neffectiveness and improvement of our method through experiments, both\nquantitively and qualitatively.", "comment": "17 pages, 12 figures", "pdf_url": "http://arxiv.org/pdf/2507.22896v1", "cate": "cs.HC", "date": "2025-06-25", "updated": "2025-06-25"}
{"id": "2507.23002", "title": "Noise-Coded Illumination for Forensic and Photometric Video Analysis", "authors": ["Peter F. Michael", "Zekun Hao", "Serge Belongie", "Abe Davis"], "categories": ["cs.GR", "cs.CR", "cs.CV"], "primary_category": "Subjects:       Graphics (cs.GR)", "pdf_link": null, "comments": "Comments:      ACM Transactions on Graphics (2025), presented at SIGGRAPH 2025", "url": "http://arxiv.org/abs/2507.23002v1", "summary": "The proliferation of advanced tools for manipulating video has led to an arms\nrace, pitting those who wish to sow disinformation against those who want to\ndetect and expose it. Unfortunately, time favors the ill-intentioned in this\nrace, with fake videos growing increasingly difficult to distinguish from real\nones. At the root of this trend is a fundamental advantage held by those\nmanipulating media: equal access to a distribution of what we consider\nauthentic (i.e., \"natural\") video. In this paper, we show how coding very\nsubtle, noise-like modulations into the illumination of a scene can help combat\nthis advantage by creating an information asymmetry that favors verification.\nOur approach effectively adds a temporal watermark to any video recorded under\ncoded illumination. However, rather than encoding a specific message, this\nwatermark encodes an image of the unmanipulated scene as it would appear lit\nonly by the coded illumination. We show that even when an adversary knows that\nour technique is being used, creating a plausible coded fake video amounts to\nsolving a second, more difficult version of the original adversarial content\ncreation problem at an information disadvantage. This is a promising avenue for\nprotecting high-stakes settings like public events and interviews, where the\ncontent on display is a likely target for manipulation, and while the\nillumination can be controlled, the cameras capturing video cannot.", "comment": "ACM Transactions on Graphics (2025), presented at SIGGRAPH 2025", "pdf_url": "http://arxiv.org/pdf/2507.23002v1", "cate": "cs.GR", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.22912", "title": "A Language Model-Driven Semi-Supervised Ensemble Framework for Illicit Market Detection Across Deep/Dark Web and Social Platforms", "authors": ["Navid Yazdanjue", "Morteza Rakhshaninejad", "Hossein Yazdanjouei", "Mohammad Sadegh Khorshidi", "Mikko S. Niemela", "Fang Chen", "Amir H. Gandomi"], "categories": ["cs.CL", "cs.AI", "cs.LG", "68T07, 68T50"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      16 pages, 5 figures, 9 tables", "url": "http://arxiv.org/abs/2507.22912v1", "summary": "Illegal marketplaces have increasingly shifted to concealed parts of the\ninternet, including the deep and dark web, as well as platforms such as\nTelegram, Reddit, and Pastebin. These channels enable the anonymous trade of\nillicit goods including drugs, weapons, and stolen credentials. Detecting and\ncategorizing such content remains challenging due to limited labeled data, the\nevolving nature of illicit language, and the structural heterogeneity of online\nsources. This paper presents a hierarchical classification framework that\ncombines fine-tuned language models with a semi-supervised ensemble learning\nstrategy to detect and classify illicit marketplace content across diverse\nplatforms. We extract semantic representations using ModernBERT, a transformer\nmodel for long documents, finetuned on domain-specific data from deep and dark\nweb pages, Telegram channels, Subreddits, and Pastebin pastes to capture\nspecialized jargon and ambiguous linguistic patterns. In addition, we\nincorporate manually engineered features such as document structure, embedded\npatterns including Bitcoin addresses, emails, and IPs, and metadata, which\ncomplement language model embeddings. The classification pipeline operates in\ntwo stages. The first stage uses a semi-supervised ensemble of XGBoost, Random\nForest, and SVM with entropy-based weighted voting to detect sales-related\ndocuments. The second stage further classifies these into drug, weapon, or\ncredential sales. Experiments on three datasets, including our multi-source\ncorpus, DUTA, and CoDA, show that our model outperforms several baselines,\nincluding BERT, ModernBERT, DarkBERT, ALBERT, Longformer, and BigBird. The\nmodel achieves an accuracy of 0.96489, an F1-score of 0.93467, and a TMCC of\n0.95388, demonstrating strong generalization, robustness under limited\nsupervision, and effectiveness in real-world illicit content detection.", "comment": "16 pages, 5 figures, 9 tables", "pdf_url": "http://arxiv.org/pdf/2507.22912v1", "cate": "cs.CL", "date": "2025-07-19", "updated": "2025-07-19"}
{"id": "2507.22897", "title": "RecUserSim: A Realistic and Diverse User Simulator for Evaluating Conversational Recommender Systems", "authors": ["Luyu Chen", "Quanyu Dai", "Zeyu Zhang", "Xueyang Feng", "Mingyu Zhang", "Pengcheng Tang", "Xu Chen", "Yue Zhu", "Zhenhua Dong"], "categories": ["cs.HC", "cs.AI"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      Accepted by TheWebConf'25 Industry Track", "url": "http://arxiv.org/abs/2507.22897v1", "summary": "Conversational recommender systems (CRS) enhance user experience through\nmulti-turn interactions, yet evaluating CRS remains challenging. User\nsimulators can provide comprehensive evaluations through interactions with CRS,\nbut building realistic and diverse simulators is difficult. While recent work\nleverages large language models (LLMs) to simulate user interactions, they\nstill fall short in emulating individual real users across diverse scenarios\nand lack explicit rating mechanisms for quantitative evaluation. To address\nthese gaps, we propose RecUserSim, an LLM agent-based user simulator with\nenhanced simulation realism and diversity while providing explicit scores.\nRecUserSim features several key modules: a profile module for defining\nrealistic and diverse user personas, a memory module for tracking interaction\nhistory and discovering unknown preferences, and a core action module inspired\nby Bounded Rationality theory that enables nuanced decision-making while\ngenerating more fine-grained actions and personalized responses. To further\nenhance output control, a refinement module is designed to fine-tune final\nresponses. Experiments demonstrate that RecUserSim generates diverse,\ncontrollable outputs and produces realistic, high-quality dialogues, even with\nsmaller base LLMs. The ratings generated by RecUserSim show high consistency\nacross different base LLMs, highlighting its effectiveness for CRS evaluation.", "comment": "Accepted by TheWebConf'25 Industry Track", "pdf_url": "http://arxiv.org/pdf/2507.22897v1", "cate": "cs.HC", "date": "2025-06-25", "updated": "2025-06-25"}
{"id": "2507.23432", "title": "Scalable contribution bounding to achieve privacy", "authors": ["Vincent Cohen-Addad", "Alessandro Epasto", "Jason Lee", "Morteza Zadimoghaddam"], "categories": ["cs.DS", "cs.CR", "cs.DC"], "primary_category": "Subjects:       Data Structures and Algorithms (cs.DS)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23432v1", "summary": "In modern datasets, where single records can have multiple owners, enforcing\nuser-level differential privacy requires capping each user's total\ncontribution. This \"contribution bounding\" becomes a significant combinatorial\nchallenge. Existing sequential algorithms for this task are computationally\nintensive and do not scale to the massive datasets prevalent today. To address\nthis scalability bottleneck, we propose a novel and efficient distributed\nalgorithm. Our approach models the complex ownership structure as a hypergraph,\nwhere users are vertices and records are hyperedges. The algorithm proceeds in\nrounds, allowing users to propose records in parallel. A record is added to the\nfinal dataset only if all its owners unanimously agree, thereby ensuring that\nno user's predefined contribution limit is violated. This method aims to\nmaximize the size of the resulting dataset for high utility while providing a\npractical, scalable solution for implementing user-level privacy in large,\nreal-world systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23432v1", "cate": "cs.DS", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.22913", "title": "A Hybrid Framework for Subject Analysis: Integrating Embedding-Based Regression Models with Large Language Models", "authors": ["Jinyu Liu", "Xiaoying Song", "Diana Zhang", "Jason Thomale", "Daqing He", "Lingzi Hong"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      13 pages, 2 figures, accepted by ASIST 2025", "url": "http://arxiv.org/abs/2507.22913v1", "summary": "Providing subject access to information resources is an essential function of\nany library management system. Large language models (LLMs) have been widely\nused in classification and summarization tasks, but their capability to perform\nsubject analysis is underexplored. Multi-label classification with traditional\nmachine learning (ML) models has been used for subject analysis but struggles\nwith unseen cases. LLMs offer an alternative but often over-generate and\nhallucinate. Therefore, we propose a hybrid framework that integrates\nembedding-based ML models with LLMs. This approach uses ML models to (1)\npredict the optimal number of LCSH labels to guide LLM predictions and (2)\npost-edit the predicted terms with actual LCSH terms to mitigate\nhallucinations. We experimented with LLMs and the hybrid framework to predict\nthe subject terms of books using the Library of Congress Subject Headings\n(LCSH). Experiment results show that providing initial predictions to guide LLM\ngenerations and imposing post-edits result in more controlled and\nvocabulary-aligned outputs.", "comment": "13 pages, 2 figures, accepted by ASIST 2025", "pdf_url": "http://arxiv.org/pdf/2507.22913v1", "cate": "cs.CL", "date": "2025-07-19", "updated": "2025-07-19"}
{"id": "2507.22900", "title": "Tool or Trouble? Exploring Student Attitudes Toward AI Coding Assistants", "authors": ["Sergio Rojas-Galeano"], "categories": ["cs.HC", "cs.AI"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22900v1", "summary": "This exploratory study examines how AI code assistants shape novice\nprogrammers' experiences during a two-part exam in an introductory programming\ncourse. In the first part, students completed a programming task with access to\nAI support; in the second, they extended their solutions without AI. We\ncollected Likert-scale and open-ended responses from 20 students to evaluate\ntheir perceptions and challenges. Findings suggest that AI tools were perceived\nas helpful for understanding code and increasing confidence, particularly\nduring initial development. However, students reported difficulties\ntransferring knowledge to unaided tasks, revealing possible overreliance and\ngaps in conceptual understanding. These insights highlight the need for\npedagogical strategies that integrate AI meaningfully while reinforcing\nfoundational programming skills.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22900v1", "cate": "cs.HC", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2507.23608", "title": "Medical Image De-Identification Benchmark Challenge", "authors": ["Linmin Pei", "Granger Sutton", "Michael Rutherford", "Ulrike Wagner", "Tracy Nolan", "Kirk Smith", "Phillip Farmer", "Peter Gu", "Ambar Rana", "Kailing Chen", "Thomas Ferleman", "Brian Park", "Ye Wu", "Jordan Kojouharov", "Gargi Singh", "Jon Lemon", "Tyler Willis", "Milos Vukadinovic", "Grant Duffy", "Bryan He", "David Ouyang", "Marco Pereanez", "Daniel Samber", "Derek A. Smith", "Christopher Cannistraci", "Zahi Fayad", "David S. Mendelson", "Michele Bufano", "Elmar Kotter", "Hamideh Haghiri", "Rajesh Baidya", "Stefan Dvoretskii", "Klaus H. Maier-Hein", "Marco Nolden", "Christopher Ablett", "Silvia Siggillino", "Sandeep Kaushik", "Hongzhu Jiang", "Sihan Xie", "Zhiyu Wan", "Alex Michie", "Simon J Doran", "Angeline Aurelia Waly", "Felix A. Nathaniel Liang", "Humam Arshad Mustagfirin", "Michelle Grace Felicia", "Kuo Po Chih", "Rahul Krish", "Ghulam Rasool", "Nidhal Bouaynaya", "Nikolas Koutsoubis", "Kyle Naddeo", "Kartik Pandit", "Tony O'Sullivan", "Raj Krish", "Qinyan Pan", "Scott Gustafson", "Benjamin Kopchick", "Laura Opsahl-Ong", "Andrea Olvera-Morales", "Jonathan Pinney", "Kathryn Johnson", "Theresa Do", "Juergen Klenk", "Maria Diaz", "Arti Singh", "Rong Chai", "David A. Clunie", "Fred Prior", "Keyvan Farahani"], "categories": ["cs.CV", "cs.CR"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      19 pages", "url": "http://arxiv.org/abs/2507.23608v1", "summary": "The de-identification (deID) of protected health information (PHI) and\npersonally identifiable information (PII) is a fundamental requirement for\nsharing medical images, particularly through public repositories, to ensure\ncompliance with patient privacy laws. In addition, preservation of non-PHI\nmetadata to inform and enable downstream development of imaging artificial\nintelligence (AI) is an important consideration in biomedical research. The\ngoal of MIDI-B was to provide a standardized platform for benchmarking of DICOM\nimage deID tools based on a set of rules conformant to the HIPAA Safe Harbor\nregulation, the DICOM Attribute Confidentiality Profiles, and best practices in\npreservation of research-critical metadata, as defined by The Cancer Imaging\nArchive (TCIA). The challenge employed a large, diverse, multi-center, and\nmulti-modality set of real de-identified radiology images with synthetic\nPHI/PII inserted.\n  The MIDI-B Challenge consisted of three phases: training, validation, and\ntest. Eighty individuals registered for the challenge. In the training phase,\nwe encouraged participants to tune their algorithms using their in-house or\npublic data. The validation and test phases utilized the DICOM images\ncontaining synthetic identifiers (of 216 and 322 subjects, respectively). Ten\nteams successfully completed the test phase of the challenge. To measure\nsuccess of a rule-based approach to image deID, scores were computed as the\npercentage of correct actions from the total number of required actions. The\nscores ranged from 97.91% to 99.93%. Participants employed a variety of\nopen-source and proprietary tools with customized configurations, large\nlanguage models, and optical character recognition (OCR). In this paper we\nprovide a comprehensive report on the MIDI-B Challenge's design,\nimplementation, results, and lessons learned.", "comment": "19 pages", "pdf_url": "http://arxiv.org/pdf/2507.23608v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.22914", "title": "Full Triple Matcher: Integrating all triple elements between heterogeneous Knowledge Graphs", "authors": ["Victor Eiti Yamamoto", "Hideaki Takeda"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22914v1", "summary": "Knowledge graphs (KGs) are powerful tools for representing and reasoning over\nstructured information. Their main components include schema, identity, and\ncontext. While schema and identity matching are well-established in ontology\nand entity matching research, context matching remains largely unexplored. This\nis particularly important because real-world KGs often vary significantly in\nsource, size, and information density - factors not typically represented in\nthe datasets on which current entity matching methods are evaluated. As a\nresult, existing approaches may fall short in scenarios where diverse and\ncomplex contexts need to be integrated.\n  To address this gap, we propose a novel KG integration method consisting of\nlabel matching and triple matching. We use string manipulation, fuzzy matching,\nand vector similarity techniques to align entity and predicate labels. Next, we\nidentify mappings between triples that convey comparable information, using\nthese mappings to improve entity-matching accuracy. Our approach demonstrates\ncompetitive performance compared to leading systems in the OAEI competition and\nagainst supervised methods, achieving high accuracy across diverse test cases.\nAdditionally, we introduce a new dataset derived from the benchmark dataset to\nevaluate the triple-matching step more comprehensively.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22914v1", "cate": "cs.CL", "date": "2025-07-20", "updated": "2025-07-20"}
{"id": "2507.22902", "title": "Toward the Autonomous AI Doctor: Quantitative Benchmarking of an Autonomous Agentic AI Versus Board-Certified Clinicians in a Real World Setting", "authors": ["Hashim Hayat", "Maksim Kudrautsau", "Evgeniy Makarov", "Vlad Melnichenko", "Tim Tsykunou", "Piotr Varaksin", "Matt Pavelle", "Adam Z. Oskowitz"], "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.MA"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22902v1", "summary": "Background: Globally we face a projected shortage of 11 million healthcare\npractitioners by 2030, and administrative burden consumes 50% of clinical time.\nArtificial intelligence (AI) has the potential to help alleviate these\nproblems. However, no end-to-end autonomous large language model (LLM)-based AI\nsystem has been rigorously evaluated in real-world clinical practice. In this\nstudy, we evaluated whether a multi-agent LLM-based AI framework can function\nautonomously as an AI doctor in a virtual urgent care setting. Methods: We\nretrospectively compared the performance of the multi-agent AI system Doctronic\nand board-certified clinicians across 500 consecutive urgent-care telehealth\nencounters. The primary end points: diagnostic concordance, treatment plan\nconsistency, and safety metrics, were assessed by blinded LLM-based\nadjudication and expert human review. Results: The top diagnosis of Doctronic\nand clinician matched in 81% of cases, and the treatment plan aligned in 99.2%\nof cases. No clinical hallucinations occurred (e.g., diagnosis or treatment not\nsupported by clinical findings). In an expert review of discordant cases, AI\nperformance was superior in 36.1%, and human performance was superior in 9.3%;\nthe diagnoses were equivalent in the remaining cases. Conclusions: In this\nfirst large-scale validation of an autonomous AI doctor, we demonstrated strong\ndiagnostic and treatment plan concordance with human clinicians, with AI\nperformance matching and in some cases exceeding that of practicing clinicians.\nThese findings indicate that multi-agent AI systems achieve comparable clinical\ndecision-making to human providers and offer a potential solution to healthcare\nworkforce shortages.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22902v1", "cate": "cs.HC", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2405.05846", "title": "An Inversion-based Measure of Memorization for Diffusion Models", "authors": ["Zhe Ma", "Qingming Li", "Xuhong Zhang", "Tianyu Du", "Ruixiao Lin", "Zonghui Wang", "Shouling Ji", "Wenzhi Chen"], "categories": ["cs.CR", "cs.CV"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      Accepted by ICCV 2025", "url": "http://arxiv.org/abs/2405.05846v3", "summary": "The past few years have witnessed substantial advances in image generation\npowered by diffusion models. However, it was shown that diffusion models are\nsusceptible to training data memorization, raising significant concerns\nregarding copyright infringement and privacy invasion. This study delves into a\nrigorous analysis of memorization in diffusion models. We introduce InvMM, an\ninversion-based measure of memorization, which is based on inverting a\nsensitive latent noise distribution accounting for the replication of an image.\nFor accurate estimation of the measure, we propose an adaptive algorithm that\nbalances the normality and sensitivity of the noise distribution. Comprehensive\nexperiments across four datasets, conducted on both unconditional and\ntext-guided diffusion models, demonstrate that InvMM provides a reliable and\ncomplete quantification of memorization. Notably, InvMM is commensurable\nbetween samples, reveals the true extent of memorization from an adversarial\nstandpoint and implies how memorization differs from membership. In practice,\nit serves as an auditing tool for developers to reliably assess the risk of\nmemorization, thereby contributing to the enhancement of trustworthiness and\nprivacy-preserving capabilities of diffusion models.", "comment": "Accepted by ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2405.05846v3", "cate": "cs.CR", "date": "2024-05-09", "updated": "2025-07-31"}
{"id": "2507.22915", "title": "Theoretical Foundations and Mitigation of Hallucination in Large Language Models", "authors": ["Esmail Gumaan"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      12 pages", "url": "http://arxiv.org/abs/2507.22915v1", "summary": "Hallucination in Large Language Models (LLMs) refers to the generation of\ncontent that is not faithful to the input or the real-world facts. This paper\nprovides a rigorous treatment of hallucination in LLMs, including formal\ndefinitions and theoretical analyses. We distinguish between intrinsic and\nextrinsic hallucinations, and define a \\textit{hallucination risk} for models.\nWe derive bounds on this risk using learning-theoretic frameworks (PAC-Bayes\nand Rademacher complexity). We then survey detection strategies for\nhallucinations, such as token-level uncertainty estimation, confidence\ncalibration, and attention alignment checks. On the mitigation side, we discuss\napproaches including retrieval-augmented generation, hallucination-aware\nfine-tuning, logit calibration, and the incorporation of fact-verification\nmodules. We propose a unified detection and mitigation workflow, illustrated\nwith a diagram, to integrate these strategies. Finally, we outline evaluation\nprotocols for hallucination, recommending datasets, metrics, and experimental\nsetups to quantify and reduce hallucinations. Our work lays a theoretical\nfoundation and practical guidelines for addressing the crucial challenge of\nhallucination in LLMs.", "comment": "12 pages", "pdf_url": "http://arxiv.org/pdf/2507.22915v1", "cate": "cs.CL", "date": "2025-07-20", "updated": "2025-07-20"}
{"id": "2507.22904", "title": "SketchMind: A Multi-Agent Cognitive Framework for Assessing Student-Drawn Scientific Sketches", "authors": ["Ehsan Latif", "Zirak Khan", "Xiaoming Zhai"], "categories": ["cs.HC", "cs.AI"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      Submitted to NeurIPS2025", "url": "http://arxiv.org/abs/2507.22904v1", "summary": "Scientific sketches (e.g., models) offer a powerful lens into students'\nconceptual understanding, yet AI-powered automated assessment of such\nfree-form, visually diverse artifacts remains a critical challenge. Existing\nsolutions often treat sketch evaluation as either an image classification task\nor monolithic vision-language models, which lack interpretability, pedagogical\nalignment, and adaptability across cognitive levels. To address these\nlimitations, we present SketchMind, a cognitively grounded, multi-agent\nframework for evaluating and improving student-drawn scientific sketches.\nSketchMind comprises modular agents responsible for rubric parsing, sketch\nperception, cognitive alignment, and iterative feedback with sketch\nmodification, enabling personalized and transparent evaluation. We evaluate\nSketchMind on a curated dataset of 3,575 student-generated sketches across six\nscience assessment items with different highest order of Bloom's level that\nrequire students to draw models to explain phenomena. Compared to baseline\nGPT-4o performance without SRG (average accuracy: 55.6%), and with SRG\nintegration achieves 77.1% average accuracy (+21.4% average absolute gain). We\nalso demonstrate that multi-agent orchestration with SRG enhances SketchMind\nperformance, for example, GPT-4.1 gains an average 8.9% increase in sketch\nprediction accuracy, outperforming single-agent pipelines across all items.\nHuman evaluators rated the feedback and co-created sketches generated by\n\\textsc{SketchMind} with GPT-4.1, which achieved an average of 4.1 out of 5,\nsignificantly higher than those of baseline models (e.g., 2.3 for GPT-4o).\nExperts noted the system's potential to meaningfully support conceptual growth\nthrough guided revision. Our code and (pending approval) dataset will be\nreleased to support reproducibility and future research in AI-driven education.", "comment": "Submitted to NeurIPS2025", "pdf_url": "http://arxiv.org/pdf/2507.22904v1", "cate": "cs.HC", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2405.13156", "title": "A Privacy-Preserving DAO Model Using NFT Authentication for the Punishment not Reward Blockchain Architecture", "authors": ["Talgar Bayan", "Richard Banach"], "categories": ["cs.CR", "cs.SE"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      This paper was accepted and presented at the International Conference on Blockchain Research and Applications (BCRA 2024), Hangzhou, China, July 26-27, 2024. An extended version has been submitted to the journal Blockchain: Research and Applications (Elsevier) for publication consideration. This arXiv version corresponds to the conference-accepted manuscript", "url": "http://arxiv.org/abs/2405.13156v2", "summary": "This paper presents a decentralised autonomous organisation (DAO) model that\nuses non-fungible tokens (NFTs) for identity management and privacy-preserving\ninteractions within a Punishment not Reward (PnR) blockchain mechanism. The\nproposed model introduces a dual NFT architecture deployed on Layer 2 networks:\nMembership NFTs (\\(NFT_{auth}\\)) for authentication and access control and\ninteraction NFTs (\\(NFT_{priv}\\)) for private interactions among participants.\nOur Layer 2 implementation achieves 97\\% gas cost reduction while maintaining\nsecurity through cross-chain mechanisms. The identity management system\nincorporates decentralised KYC processes and Sybil attack resistance using\nsoulbound token characteristics. Governance operates through smart contracts\nthat manage reputation and administer punitive measures, including conditional\nidentity disclosure for forensic purposes. Governance operates through smart\ncontracts that manage reputation and administer punitive measures, including\nconditional identity disclosure when misconduct is detected.", "comment": "This paper was accepted and presented at the International Conference\n  on Blockchain Research and Applications (BCRA 2024), Hangzhou, China, July\n  26-27, 2024. An extended version has been submitted to the journal\n  Blockchain: Research and Applications (Elsevier) for publication\n  consideration. This arXiv version corresponds to the conference-accepted\n  manuscript", "pdf_url": "http://arxiv.org/pdf/2405.13156v2", "cate": "cs.CR", "date": "2024-05-21", "updated": "2025-07-31"}
{"id": "2507.22917", "title": "Reading Between the Timelines: RAG for Answering Diachronic Questions", "authors": ["Kwun Hang Lau", "Ruiyuan Zhang", "Weijie Shi", "Xiaofang Zhou", "Xiaojun Cheng"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22917v1", "summary": "While Retrieval-Augmented Generation (RAG) excels at injecting static,\nfactual knowledge into Large Language Models (LLMs), it exhibits a critical\ndeficit in handling longitudinal queries that require tracking entities and\nphenomena across time. This blind spot arises because conventional,\nsemantically-driven retrieval methods are not equipped to gather evidence that\nis both topically relevant and temporally coherent for a specified duration. We\naddress this challenge by proposing a new framework that fundamentally\nredesigns the RAG pipeline to infuse temporal logic. Our methodology begins by\ndisentangling a user's query into its core subject and its temporal window. It\nthen employs a specialized retriever that calibrates semantic matching against\ntemporal relevance, ensuring the collection of a contiguous evidence set that\nspans the entire queried period. To enable rigorous evaluation of this\ncapability, we also introduce the Analytical Diachronic Question Answering\nBenchmark (ADQAB), a challenging evaluation suite grounded in a hybrid corpus\nof real and synthetic financial news. Empirical results on ADQAB show that our\napproach yields substantial gains in answer accuracy, surpassing standard RAG\nimplementations by 13% to 27%. This work provides a validated pathway toward\nRAG systems capable of performing the nuanced, evolutionary analysis required\nfor complex, real-world questions. The dataset and code for this study are\npublicly available at https://github.com/kwunhang/TA-RAG.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22917v1", "cate": "cs.CL", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2507.22906", "title": "DNN-based Methods of Jointly Sensing Number and Directions of Targets via a Green Massive H2AD MIMO Receiver", "authors": ["Bin Deng", "Jiatong Bai", "Feilong Zhao", "Zuming Xie", "Maolin Li", "Yan Wang", "Feng Shu"], "categories": ["eess.SP", "cs.AI", "cs.IT", "cs.LG", "math.IT"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22906v1", "summary": "As a green MIMO structure, the heterogeneous hybrid analog-digital H2AD MIMO\narchitecture has been shown to own a great potential to replace the massive or\nextremely large-scale fully-digital MIMO in the future wireless networks to\naddress the three challenging problems faced by the latter: high energy\nconsumption, high circuit cost, and high complexity. However, how to\nintelligently sense the number and direction of multi-emitters via such a\nstructure is still an open hard problem. To address this, we propose a\ntwo-stage sensing framework that jointly estimates the number and direction\nvalues of multiple targets. Specifically, three target number sensing methods\nare designed: an improved eigen-domain clustering (EDC) framework, an enhanced\ndeep neural network (DNN) based on five key statistical features, and an\nimproved one-dimensional convolutional neural network (1D-CNN) utilizing full\neigenvalues. Subsequently, a low-complexity and high-accuracy DOA estimation is\nachieved via the introduced online micro-clustering (OMC-DOA) method.\nFurthermore, we derive the Cram\\'er-Rao lower bound (CRLB) for the H2AD under\nmultiple-source conditions as a theoretical performance benchmark. Simulation\nresults show that the developed three methods achieve 100\\% number of targets\nsensing at moderate-to-high SNRs, while the improved 1D-CNN exhibits superior\nunder extremely-low SNR conditions. The introduced OMC-DOA outperforms existing\nclustering and fusion-based DOA methods in multi-source environments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22906v1", "cate": "eess.SP", "date": "2025-07-15", "updated": "2025-07-15"}
{"id": "2408.05997", "title": "On the Formalization of Cryptographic Migration", "authors": ["Daniel Loebenberger", "Stefan-Lukas Gazdag", "Daniel Herzinger", "Eduard Hirsch", "Christian Näther", "Jan-Philipp Steghöfer"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2408.05997v4", "summary": "We present a novel approach to gaining insight into the structure of\ncryptographic migration problems which are classic problems in applied\ncryptography. We use a formal model to capture the inherent dependencies and\ncomplexities of such transitions. Using classical mathematical results from\ncombinatorics, probability theory, and combinatorial analysis, we evaluate the\nchallenges of migrating large cryptographic IT infrastructures and prove that -\nin a suitable sense - cryptographic migration exhibits a certain expected\ncomplexity. We also provide numerical data for selected parameter sets.\nFurthermore, we analyze the proposed model in terms of real-world patterns and\nits practical applicability. Additionally, we discuss the challenges of\nmodeling real-world migration projects. As concrete examples we examine the\ntransition to post-quantum cryptography of the CI/CD system GitLab and the\nmulti-level technological transition of distribution power grids. This work\npaves the way for future advancements in both the theoretical understanding and\npractical implementation of cryptographic migration strategies.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2408.05997v4", "cate": "cs.CR", "date": "2024-08-12", "updated": "2025-07-31"}
{"id": "2507.22918", "title": "Semantic Convergence: Investigating Shared Representations Across Scaled LLMs", "authors": ["Daniel Son", "Sanjana Rathore", "Andrew Rufail", "Adrian Simon", "Daniel Zhang", "Soham Dave", "Cole Blondin", "Kevin Zhu", "Sean O'Brien"], "categories": ["cs.CL", "cs.LG", "68T50", "I.2.6; I.2.7"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Submitted to ACL 2025 Student Research Workshop (poster)", "url": "http://arxiv.org/abs/2507.22918v1", "summary": "We investigate feature universality in Gemma-2 language models (Gemma-2-2B\nand Gemma-2-9B), asking whether models with a four-fold difference in scale\nstill converge on comparable internal concepts. Using the Sparse Autoencoder\n(SAE) dictionary-learning pipeline, we utilize SAEs on each model's\nresidual-stream activations, align the resulting monosemantic features via\nactivation correlation, and compare the matched feature spaces with SVCCA and\nRSA. Middle layers yield the strongest overlap, while early and late layers\nshow far less similarity. Preliminary experiments extend the analysis from\nsingle tokens to multi-token subspaces, showing that semantically similar\nsubspaces interact similarly with language models. These results strengthen the\ncase that large language models carve the world into broadly similar,\ninterpretable features despite size differences, reinforcing universality as a\nfoundation for cross-model interpretability.", "comment": "Submitted to ACL 2025 Student Research Workshop (poster)", "pdf_url": "http://arxiv.org/pdf/2507.22918v1", "cate": "cs.CL", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2507.22946", "title": "SmartCourse: A Contextual AI-Powered Course Advising System for Undergraduates", "authors": ["Yixuan Mi", "Yiduo Yu", "Yiyi Zhao"], "categories": ["cs.CY", "cs.AI"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "Comments:      7 pages, 6 figures, 1 table. *Corresponding author: Yixuan Mi. Code: this https URL", "url": "http://arxiv.org/abs/2507.22946v1", "summary": "We present SmartCourse, an integrated course management and AI-driven\nadvising system for undergraduate students (specifically tailored to the\nComputer Science (CPS) major). SmartCourse addresses the limitations of\ntraditional advising tools by integrating transcript and plan information for\nstudent-specific context. The system combines a command-line interface (CLI)\nand a Gradio web GUI for instructors and students, manages user accounts,\ncourse enrollment, grading, and four-year degree plans, and integrates a\nlocally hosted large language model (via Ollama) for personalized course\nrecommendations. It leverages transcript and major plan to offer contextual\nadvice (e.g., prioritizing requirements or retakes). We evaluated the system on\n25 representative advising queries and introduced custom metrics: PlanScore,\nPersonalScore, Lift, and Recall to assess recommendation quality across\ndifferent context conditions. Experiments show that using full context yields\nsubstantially more relevant recommendations than context-omitted modes,\nconfirming the necessity of transcript and plan information for personalized\nacademic advising. SmartCourse thus demonstrates how transcript-aware AI can\nenhance academic planning.", "comment": "7 pages, 6 figures, 1 table. *Corresponding author: Yixuan Mi. Code:\n  https://github.com/EthanYixuanMi/Smartcourse-Contextual-Advising", "pdf_url": "http://arxiv.org/pdf/2507.22946v1", "cate": "cs.CY", "date": "2025-07-26", "updated": "2025-07-26"}
{"id": "2507.22908", "title": "A Privacy-Preserving Federated Framework with Hybrid Quantum-Enhanced Learning for Financial Fraud Detection", "authors": ["Abhishek Sawaika", "Swetang Krishna", "Tushar Tomar", "Durga Pritam Suggisetti", "Aditi Lal", "Tanmaya Shrivastav", "Nouhaila Innan", "Muhammad Shafique"], "categories": ["q-fin.CP", "cs.AI", "cs.LG", "I.2"], "primary_category": "Subjects:       Computational Finance (q-fin.CP)", "pdf_link": null, "comments": "Comments:      To be published in proceedings of IEEE International Conference on Quantum Computing and Engineering (QCE) 2025", "url": "http://arxiv.org/abs/2507.22908v1", "summary": "Rapid growth of digital transactions has led to a surge in fraudulent\nactivities, challenging traditional detection methods in the financial sector.\nTo tackle this problem, we introduce a specialised federated learning framework\nthat uniquely combines a quantum-enhanced Long Short-Term Memory (LSTM) model\nwith advanced privacy preserving techniques. By integrating quantum layers into\nthe LSTM architecture, our approach adeptly captures complex\ncross-transactional patters, resulting in an approximate 5% performance\nimprovement across key evaluation metrics compared to conventional models.\nCentral to our framework is \"FedRansel\", a novel method designed to defend\nagainst poisoning and inference attacks, thereby reducing model degradation and\ninference accuracy by 4-8%, compared to standard differential privacy\nmechanisms. This pseudo-centralised setup with a Quantum LSTM model, enhances\nfraud detection accuracy and reinforces the security and confidentiality of\nsensitive financial data.", "comment": "To be published in proceedings of IEEE International Conference on\n  Quantum Computing and Engineering (QCE) 2025", "pdf_url": "http://arxiv.org/pdf/2507.22908v1", "cate": "q-fin.CP", "date": "2025-07-15", "updated": "2025-07-15"}
{"id": "2502.11641", "title": "A Zero-Knowledge Proof for the Syndrome Decoding Problem in the Lee Metric", "authors": ["Mladen Kovačević", "Tatjana Grbić", "Darko Čapko", "Nemanja Nedić", "Srdjan Vukmirović"], "categories": ["cs.CR", "cs.IT", "math.IT", "94A60, 68P25"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2502.11641v3", "summary": "The syndrome decoding problem is one of the NP-complete problems lying at the\nfoundation of code-based cryptography. The variant thereof where the distance\nbetween vectors is measured with respect to the Lee metric, rather than the\nmore commonly used Hamming metric, has been analyzed recently in several works\ndue to its potential relevance for building more efficient code-based\ncryptosystems. The purpose of this article is to present a zero-knowledge proof\nof knowledge for this variant of the problem.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2502.11641v3", "cate": "cs.CR", "date": "2025-02-17", "updated": "2025-07-31"}
{"id": "2507.22919", "title": "A novel language model for predicting serious adverse event results in clinical trials from their prospective registrations", "authors": ["Qixuan Hu", "Xumou Zhang", "Jinman Kim", "Florence Bourgeois", "Adam G. Dunn"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22919v1", "summary": "Objectives: With accurate estimates of expected safety results, clinical\ntrials could be designed to avoid terminations and limit exposing participants\nto unnecessary risks. We evaluated methods for predicting serious adverse event\n(SAE) results in clinical trials using information only from their\nregistrations prior to the trial. Material and Methods: We analysed 22,107\ntwo-arm parallel interventional clinical trials from ClinicalTrials.gov with\nstructured summary results. Two prediction models were developed: a classifier\npredicting will experimental arm have higher SAE rates (area under the receiver\noperating characteristic curve; AUC) than control arm, and a regression model\nto predict the proportion of SAEs in control arms (root mean squared error;\nRMSE). A transfer learning approach using pretrained language models (e.g.,\nClinicalT5, BioBERT) was used for feature extraction, combined with downstream\nmodel for prediction. To maintain semantic representation in long trial texts\nexceeding localised language model input limits, a sliding window method was\ndeveloped for embedding extraction. Results: The best model\n(ClinicalT5+Transformer+MLP) had 77.6% AUC predicting which trial arm has a\nhigher proportion of patients with SAEs. When predicting proportion of\nparticipants experiencing SAE in the control arm, the same model achieved RMSE\nof 18.6%. The sliding window approach consistently outperformed methods without\nit. Across 12 classifiers, the average absolute AUC increase was 2.00%; across\n12 regressors, the average absolute RMSE reduction was 1.58%. Discussion:\nSummary results data available at ClinicalTrials.gov remains underutilised. The\npotential to estimate results of trials before they start is an opportunity to\nimprove trial design and flag discrepancies between expected and reported\nsafety results.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22919v1", "cate": "cs.CL", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2507.22947", "title": "ELMES: An Automated Framework for Evaluating Large Language Models in Educational Scenarios", "authors": ["Shou'ang Wei", "Xinyun Wang", "Shuzhen Bi", "Jian Chen", "Ruijia Li", "Bo Jiang", "Xin Lin", "Min Zhang", "Yu Song", "BingDong Li", "Aimin Zhou", "Hao Hao"], "categories": ["cs.CY", "cs.CL", "cs.LG"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22947v1", "summary": "The emergence of Large Language Models (LLMs) presents transformative\nopportunities for education, generating numerous novel application scenarios.\nHowever, significant challenges remain: evaluation metrics vary substantially\nacross different educational scenarios, while many emerging scenarios lack\nappropriate assessment metrics. Current benchmarks predominantly measure\ngeneral intelligence rather than pedagogical capabilities. To address this gap,\nwe introduce ELMES, an open-source automated evaluation framework specifically\ndesigned for assessing LLMs in educational settings. ELMES features a modular\narchitecture that enables researchers to create dynamic, multi-agent dialogues\nthrough simple configuration files, facilitating flexible scenario design\nwithout requiring extensive programming expertise. The framework incorporates a\nhybrid evaluation engine that objectively quantifies traditionally subjective\npedagogical metrics using an LLM-as-a-Judge methodology. We conduct systematic\nbenchmarking of state-of-the-art LLMs across four critical educational\nscenarios: Knowledge Point Explanation, Guided Problem-Solving Teaching,\nInterdisciplinary Lesson Plan Generation, and Contextualized Question\nGeneration, employing fine-grained metrics developed in collaboration with\neducation specialists. Our results demonstrate distinct capability\ndistributions among models, revealing context-specific strengths and\nlimitations. ELMES provides educators and researchers with an accessible\nevaluation framework that significantly reduces adaptation barriers for diverse\neducational applications while advancing the practical implementation of LLMs\nin pedagogy. The framework is publicly available at\n\\emph{https://github.com/sii-research/elmes.git}.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22947v1", "cate": "cs.CY", "date": "2025-07-27", "updated": "2025-07-27"}
{"id": "2507.22916", "title": "From Propagator to Oscillator: The Dual Role of Symmetric Differential Equations in Neural Systems", "authors": ["Kun Jiang"], "categories": ["cs.NE", "cs.AI"], "primary_category": "Subjects:       Neural and Evolutionary Computing (cs.NE)", "pdf_link": null, "comments": "Comments:      20 pages, 7 figures", "url": "http://arxiv.org/abs/2507.22916v1", "summary": "In our previous work, we proposed a novel neuron model based on symmetric\ndifferential equations and demonstrated its potential as an efficient signal\npropagator. Building upon that foundation, the present study delves deeper into\nthe intrinsic dynamics and functional diversity of this model. By\nsystematically exploring the parameter space and employing a range of\nmathematical analysis tools, we theoretically reveal the system 's core\nproperty of functional duality. Specifically, the model exhibits two distinct\ntrajectory behaviors: one is asymptotically stable, corresponding to a reliable\nsignal propagator; the other is Lyapunov stable, characterized by sustained\nself-excited oscillations, functioning as a signal generator. To enable\neffective monitoring and prediction of system states during simulations, we\nintroduce a novel intermediate-state metric termed on-road energy. Simulation\nresults confirm that transitions between the two functional modes can be\ninduced through parameter adjustments or modifications to the connection\nstructure. Moreover, we show that oscillations can be effectively suppressed by\nintroducing external signals. These findings draw a compelling parallel to the\ndual roles of biological neurons in both information transmission and rhythm\ngeneration, thereby establishing a solid theoretical basis and a clear\nfunctional roadmap for the broader application of this model in neuromorphic\nengineering.", "comment": "20 pages, 7 figures", "pdf_url": "http://arxiv.org/pdf/2507.22916v1", "cate": "cs.NE", "date": "2025-07-20", "updated": "2025-07-20"}
{"id": "2503.06989", "title": "Probabilistic Modeling of Jailbreak on Multimodal LLMs: From Quantification to Application", "authors": ["Wenzhuo Xu", "Zhipeng Wei", "Xiongtao Sun", "Zonghao Ying", "Deyue Zhang", "Dongdong Yang", "Xiangzheng Zhang", "Quanchen Zou"], "categories": ["cs.CR", "cs.CV"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.06989v2", "summary": "Recently, Multimodal Large Language Models (MLLMs) have demonstrated their\nsuperior ability in understanding multimodal content. However, they remain\nvulnerable to jailbreak attacks, which exploit weaknesses in their safety\nalignment to generate harmful responses. Previous studies categorize jailbreaks\nas successful or failed based on whether responses contain malicious content.\nHowever, given the stochastic nature of MLLM responses, this binary\nclassification of an input's ability to jailbreak MLLMs is inappropriate.\nDerived from this viewpoint, we introduce jailbreak probability to quantify the\njailbreak potential of an input, which represents the likelihood that MLLMs\ngenerated a malicious response when prompted with this input. We approximate\nthis probability through multiple queries to MLLMs. After modeling the\nrelationship between input hidden states and their corresponding jailbreak\nprobability using Jailbreak Probability Prediction Network (JPPN), we use\ncontinuous jailbreak probability for optimization. Specifically, we propose\nJailbreak-Probability-based Attack (JPA) that optimizes adversarial\nperturbations on input image to maximize jailbreak probability, and further\nenhance it as Multimodal JPA (MJPA) by including monotonic text rephrasing. To\ncounteract attacks, we also propose Jailbreak-Probability-based Finetuning\n(JPF), which minimizes jailbreak probability through MLLM parameter updates.\nExtensive experiments show that (1) (M)JPA yields significant improvements when\nattacking a wide range of models under both white and black box settings. (2)\nJPF vastly reduces jailbreaks by at most over 60\\%. Both of the above results\ndemonstrate the significance of introducing jailbreak probability to make\nnuanced distinctions among input jailbreak abilities.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.06989v2", "cate": "cs.CR", "date": "2025-03-10", "updated": "2025-07-31"}
{"id": "2507.22920", "title": "Discrete Tokenization for Multimodal LLMs: A Comprehensive Survey", "authors": ["Jindong Li", "Yali Fu", "Jiahong Liu", "Linxiao Cao", "Wei Ji", "Menglin Yang", "Irwin King", "Ming-Hsuan Yang"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22920v1", "summary": "The rapid advancement of large language models (LLMs) has intensified the\nneed for effective mechanisms to transform continuous multimodal data into\ndiscrete representations suitable for language-based processing. Discrete\ntokenization, with vector quantization (VQ) as a central approach, offers both\ncomputational efficiency and compatibility with LLM architectures. Despite its\ngrowing importance, there is a lack of a comprehensive survey that\nsystematically examines VQ techniques in the context of LLM-based systems. This\nwork fills this gap by presenting the first structured taxonomy and analysis of\ndiscrete tokenization methods designed for LLMs. We categorize 8 representative\nVQ variants that span classical and modern paradigms and analyze their\nalgorithmic principles, training dynamics, and integration challenges with LLM\npipelines. Beyond algorithm-level investigation, we discuss existing research\nin terms of classical applications without LLMs, LLM-based single-modality\nsystems, and LLM-based multimodal systems, highlighting how quantization\nstrategies influence alignment, reasoning, and generation performance. In\naddition, we identify key challenges including codebook collapse, unstable\ngradient estimation, and modality-specific encoding constraints. Finally, we\ndiscuss emerging research directions such as dynamic and task-adaptive\nquantization, unified tokenization frameworks, and biologically inspired\ncodebook learning. This survey bridges the gap between traditional vector\nquantization and modern LLM applications, serving as a foundational reference\nfor the development of efficient and generalizable multimodal systems. A\ncontinuously updated version is available at:\nhttps://github.com/jindongli-Ai/LLM-Discrete-Tokenization-Survey.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22920v1", "cate": "cs.CL", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2507.23434", "title": "Future Illiteracies -- Architectural Epistemology and Artificial Intelligence", "authors": ["Mustapha El Moussaoui"], "categories": ["cs.CY"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "Comments:      14 pages, 7 figures. MDPI - Architecture 2025", "url": "http://arxiv.org/abs/2507.23434v1", "summary": "In the age of artificial intelligence, architectural practice faces a paradox\nof immense potential and creeping standardization. As humans are increasingly\nrelying on AI-generated outputs, architecture risks becoming a spectacle of\nrepetition- a shuffling of data that neither truly innovates nor progresses\nvertically in creative depth. This paper explores the critical role of data in\nAI systems, scrutinizing the training datasets that form the basis of AI's\ngenerative capabilities and the implications for architectural practice. We\nargue that when architects approach AI passively, without actively engaging\ntheir own creative and critical faculties, they risk becoming passive users\nlocked in an endless loop of horizontal expansion without meaningful vertical\ngrowth. By examining the epistemology of architecture in the AI age, this paper\ncalls for a paradigm where AI serves as a tool for vertical and horizontal\ngrowth, contingent on human creativity and agency. Only by mastering this\ndynamic relationship can architects avoid the trap of passive, standardized\ndesign and unlock the true potential of AI.", "comment": "14 pages, 7 figures. MDPI - Architecture 2025", "pdf_url": "http://arxiv.org/pdf/2507.23434v1", "cate": "cs.CY", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.22921", "title": "Fast and Accurate Contextual Knowledge Extraction Using Cascading Language Model Chains and Candidate Answers", "authors": ["Lee Harris"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22921v1", "summary": "Language models can capture complex relationships in given text, but these\nare notorious for being costly and for producing information that does not\nexist (i.e., hallucinations). Furthermore, the resources invested into\nproducing this information would be wasted if it were incorrect. We address\nthese issues by proposing, implementing, and applying the Language Model Chain\n(LMC) algorithm. In this, a language model's response to a given prompt about\ngiven text is only correct if it exists in the collection of possible (i.e.,\ncandidate) answers, and text corresponding to incorrect responses is fed into a\nmore predictive (but slower) language model. This process is repeated for a\ncollection of language models, or until all predictions about the text are\ncorrect. We used the LMC algorithm to extract patient dates of birth from\nmedical documents, and combining a collection of language models in a\nmulti-stage cascade significantly increased prediction speed and accuracy over\nindividual language models, while greatly reducing the number of corresponding\nhallucinations. We believe that the novel LMC algorithm significantly\ncontributes to the knowledge extraction field, and that this should be explored\nmuch further in the future.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22921v1", "cate": "cs.CL", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2504.13201", "title": "CEE: An Inference-Time Jailbreak Defense for Embodied Intelligence via Subspace Concept Rotation", "authors": ["Jirui Yang", "Zheyu Lin", "Zhihui Lu", "Yinggui Wang", "Lei Wang", "Tao Wei", "Xin Du", "Shuhan Yang"], "categories": ["cs.CR", "cs.LG", "cs.MA"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.13201v2", "summary": "Large Language Models (LLMs) are increasingly becoming the cognitive core of\nEmbodied Intelligence (EI) systems, such as robots and autonomous vehicles.\nHowever, this integration also exposes them to serious jailbreak risks, where\nmalicious instructions can be transformed into dangerous physical actions.\nExisting defense mechanisms suffer from notable drawbacks--including high\ntraining costs, significant inference delays, and complex hyperparameter\ntuning--which limit their practical applicability. To address these challenges,\nwe propose a novel and efficient inference-time defense framework: Concept\nEnhancement Engineering (CEE). CEE enhances the model's inherent safety\nmechanisms by directly manipulating its internal representations, requiring\nneither additional training nor external modules, thereby improving defense\nefficiency. Furthermore, CEE introduces a rotation-based control mechanism that\nenables stable and linearly tunable behavioral control of the model. This\ndesign eliminates the need for tedious manual tuning and avoids the output\ndegradation issues commonly observed in other representation engineering\nmethods. Extensive experiments across multiple EI safety benchmarks and diverse\nattack scenarios demonstrate that CEE significantly improves the defense\nsuccess rates of various multimodal LLMs. It effectively mitigates safety risks\nwhile preserving high-quality generation and inference efficiency, offering a\npromising solution for deploying safer embodied intelligence systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.13201v2", "cate": "cs.CR", "date": "2025-04-15", "updated": "2025-07-31"}
{"id": "2507.22922", "title": "Predicting stock prices with ChatGPT-annotated Reddit sentiment", "authors": ["Mateusz Kmak", "Kamil Chmurzyński", "Kamil Matejuk", "Paweł Kotzbach", "Jan Kocoń"], "categories": ["cs.CL", "cs.AI", "cs.SI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      International Conference on Computational Science 2025", "url": "http://arxiv.org/abs/2507.22922v1", "summary": "The surge of retail investor activity on social media, exemplified by the\n2021 GameStop short squeeze, raised questions about the influence of online\nsentiment on stock prices. This paper explores whether sentiment derived from\nsocial media discussions can meaningfully predict stock market movements. We\nfocus on Reddit's r/wallstreetbets and analyze sentiment related to two\ncompanies: GameStop (GME) and AMC Entertainment (AMC). To assess sentiment's\nrole, we employ two existing text-based sentiment analysis methods and\nintroduce a third, a ChatGPT-annotated and fine-tuned RoBERTa-based model\ndesigned to better interpret the informal language and emojis prevalent in\nsocial media discussions. We use correlation and causality metrics to determine\nthese models' predictive power. Surprisingly, our findings suggest that social\nmedia sentiment has only a weak correlation with stock prices. At the same\ntime, simpler metrics, such as the volume of comments and Google search trends,\nexhibit stronger predictive signals. These results highlight the complexity of\nretail investor behavior and suggest that traditional sentiment analysis may\nnot fully capture the nuances of market-moving online discussions.", "comment": "International Conference on Computational Science 2025", "pdf_url": "http://arxiv.org/pdf/2507.22922v1", "cate": "cs.CL", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2507.23653", "title": "Architectural practice process and artificial intelligence -- an evolving practice", "authors": ["Mustapha El Moussaoui"], "categories": ["cs.CY"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "Comments:      15 pages, 7 figures. De Gruyter Brill - Open Engineering 2025", "url": "http://arxiv.org/abs/2507.23653v1", "summary": "In an era of exponential technological advancement, artificial intelligence\n(AI) has emerged as a transformative force in architecture, reshaping\ntraditional design and construction practices. This article explores the\nmultifaceted roles of AI in the architectural process, emphasizing its\npotential to enhance creativity and efficiency while addressing its limitations\nin capturing multisensory and experiential dimensions of space. Historically,\narchitectural innovation has paralleled technological progress, from basic\ntools to advanced computer-aided design systems. However, the integration of AI\npresents unique challenges, requiring architects to critically evaluate its\nrole in design. A narrative review methodology was adopted, focusing on\nacademic sources selected for their relevance, recency, and credibility. The\nfindings reveal that AI is increasingly integrated across various stages of the\narchitectural process, from early conceptualization and site analysis to\ngenerative design and construction detailing. AI tools excel at automating\nrepetitive tasks and generating innovative design solutions, freeing architects\nto focus on creativity and problem-solving. Additionally, AI's (text- toimage)\nvisual representation strength challenges the ocularcentric approaches in\narchitecture, which should push future architects to address the holistic\nsensory and experiential qualities of space or the critical thinking inherent\nto architectural design. While AI offers transformative potential, architects\nmust view it as a collaborative partner rather than a passive tool.", "comment": "15 pages, 7 figures. De Gruyter Brill - Open Engineering 2025", "pdf_url": "http://arxiv.org/pdf/2507.23653v1", "cate": "cs.CY", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.22923", "title": "How and Where to Translate? The Impact of Translation Strategies in Cross-lingual LLM Prompting", "authors": ["Aman Gupta", "Yingying Zhuang", "Zhou Yu", "Ziji Zhang", "Anurag Beniwal"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Accepted at Prompt Optimization KDD '25", "url": "http://arxiv.org/abs/2507.22923v1", "summary": "Despite advances in the multilingual capabilities of Large Language Models\n(LLMs), their performance varies substantially across different languages and\ntasks. In multilingual retrieval-augmented generation (RAG)-based systems,\nknowledge bases (KB) are often shared from high-resource languages (such as\nEnglish) to low-resource ones, resulting in retrieved information from the KB\nbeing in a different language than the rest of the context. In such scenarios,\ntwo common practices are pre-translation to create a mono-lingual prompt and\ncross-lingual prompting for direct inference. However, the impact of these\nchoices remains unclear. In this paper, we systematically evaluate the impact\nof different prompt translation strategies for classification tasks with\nRAG-enhanced LLMs in multilingual systems. Experimental results show that an\noptimized prompting strategy can significantly improve knowledge sharing across\nlanguages, therefore improve the performance on the downstream classification\ntask. The findings advocate for a broader utilization of multilingual resource\nsharing and cross-lingual prompt optimization for non-English languages,\nespecially the low-resource ones.", "comment": "Accepted at Prompt Optimization KDD '25", "pdf_url": "http://arxiv.org/pdf/2507.22923v1", "cate": "cs.CL", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2507.01694", "title": "Graph Representation-based Model Poisoning on Federated Large Language Models", "authors": ["Hanlin Cai", "Haofan Dong", "Houtianfu Wang", "Kai Li", "Ozgur B. Akan"], "categories": ["cs.CR", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      7 pages, 5 figures (Submitted to IEEE Communication Magazine)", "url": "http://arxiv.org/abs/2507.01694v2", "summary": "Federated large language models (FedLLMs) enable powerful generative\ncapabilities within wireless networks while preserving data privacy.\nNonetheless, FedLLMs remain vulnerable to model poisoning attacks. This article\nfirst reviews recent advancements in model poisoning techniques and existing\ndefense mechanisms for FedLLMs, underscoring critical limitations, especially\nwhen dealing with non-IID textual data distributions. Current defense\nstrategies predominantly employ distance or similarity-based outlier detection\nmechanisms, relying on the assumption that malicious updates markedly differ\nfrom benign statistical patterns. However, this assumption becomes inadequate\nagainst adaptive adversaries targeting billion-parameter LLMs. The article\nfurther investigates graph representation-based model poisoning (GRMP), an\nemerging attack paradigm that exploits higher-order correlations among benign\nclient gradients to craft malicious updates indistinguishable from legitimate\nones. GRMP can effectively circumvent advanced defense systems, causing\nsubstantial degradation in model accuracy and overall performance. Moreover,\nthe article outlines a forward-looking research roadmap that emphasizes the\nnecessity of graph-aware secure aggregation methods, specialized vulnerability\nmetrics tailored for FedLLMs, and evaluation frameworks to enhance the\nrobustness of federated language model deployments.", "comment": "7 pages, 5 figures (Submitted to IEEE Communication Magazine)", "pdf_url": "http://arxiv.org/pdf/2507.01694v2", "cate": "cs.CR", "date": "2025-07-02", "updated": "2025-07-31"}
{"id": "2507.22924", "title": "Using Sentiment Analysis to Investigate Peer Feedback by Native and Non-Native English Speakers", "authors": ["Brittney Exline", "Melanie Duffin", "Brittany Harbison", "Chrissa da Gomez", "David Joyner"], "categories": ["cs.CL", "I.2.7; K.3.1"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22924v1", "summary": "Graduate-level CS programs in the U.S. increasingly enroll international\nstudents, with 60.2 percent of master's degrees in 2023 awarded to non-U.S.\nstudents. Many of these students take online courses, where peer feedback is\nused to engage students and improve pedagogy in a scalable manner. Since these\ncourses are conducted in English, many students study in a language other than\ntheir first. This paper examines how native versus non-native English speaker\nstatus affects three metrics of peer feedback experience in online U.S.-based\ncomputing courses. Using the Twitter-roBERTa-based model, we analyze the\nsentiment of peer reviews written by and to a random sample of 500 students. We\nthen relate sentiment scores and peer feedback ratings to students' language\nbackground. Results show that native English speakers rate feedback less\nfavorably, while non-native speakers write more positively but receive less\npositive sentiment in return. When controlling for sex and age, significant\ninteractions emerge, suggesting that language background plays a modest but\ncomplex role in shaping peer feedback experiences.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22924v1", "cate": "cs.CL", "date": "2025-07-23", "updated": "2025-07-23"}
{"id": "2507.23669", "title": "Automating AI Failure Tracking: Semantic Association of Reports in AI Incident Database", "authors": ["Diego Russo", "Gian Marco Orlando", "Valerio La Gatta", "Vincenzo Moscato"], "categories": ["cs.CY", "cs.AI", "cs.IR"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "Comments:      Accepted at the 28th European Conference on Artificial Intelligence (ECAI 2025)", "url": "http://arxiv.org/abs/2507.23669v1", "summary": "Artificial Intelligence (AI) systems are transforming critical sectors such\nas healthcare, finance, and transportation, enhancing operational efficiency\nand decision-making processes. However, their deployment in high-stakes domains\nhas exposed vulnerabilities that can result in significant societal harm. To\nsystematically study and mitigate these risk, initiatives like the AI Incident\nDatabase (AIID) have emerged, cataloging over 3,000 real-world AI failure\nreports. Currently, associating a new report with the appropriate AI Incident\nrelies on manual expert intervention, limiting scalability and delaying the\nidentification of emerging failure patterns.\n  To address this limitation, we propose a retrieval-based framework that\nautomates the association of new reports with existing AI Incidents through\nsemantic similarity modeling. We formalize the task as a ranking problem, where\neach report-comprising a title and a full textual description-is compared to\npreviously documented AI Incidents based on embedding cosine similarity.\nBenchmarking traditional lexical methods, cross-encoder architectures, and\ntransformer-based sentence embedding models, we find that the latter\nconsistently achieve superior performance. Our analysis further shows that\ncombining titles and descriptions yields substantial improvements in ranking\naccuracy compared to using titles alone. Moreover, retrieval performance\nremains stable across variations in description length, highlighting the\nrobustness of the framework. Finally, we find that retrieval performance\nconsistently improves as the training set expands. Our approach provides a\nscalable and efficient solution for supporting the maintenance of the AIID.", "comment": "Accepted at the 28th European Conference on Artificial Intelligence\n  (ECAI 2025)", "pdf_url": "http://arxiv.org/pdf/2507.23669v1", "cate": "cs.CY", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.22958", "title": "CHECK-MAT: Checking Hand-Written Mathematical Answers for the Russian Unified State Exam", "authors": ["Ruslan Khrulev"], "categories": ["cs.CV", "cs.AI", "cs.LG", "68T07, 97D50", "I.2.7; I.4; K.3.1"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      15 pages, 3 figures, 10 tables. Code is available at: this https URL", "url": "http://arxiv.org/abs/2507.22958v1", "summary": "This paper introduces a novel benchmark, EGE-Math Solutions Assessment\nBenchmark, for evaluating Vision-Language Models (VLMs) on their ability to\nassess hand-written mathematical solutions. Unlike existing benchmarks that\nfocus on problem solving, our approach centres on understanding student\nsolutions, identifying mistakes, and assigning grades according to fixed\ncriteria. We compile 122 scanned solutions from the Russian Unified State Exam\n(EGE) together with official expert grades, and evaluate seven modern VLMs from\nGoogle, OpenAI, Arcee AI, and Alibaba Cloud in three inference modes. The\nresults reveal current limitations in mathematical reasoning and human-rubric\nalignment, opening new research avenues in AI-assisted assessment. You can find\ncode in https://github.com/Karifannaa/Auto-check-EGE-math", "comment": "15 pages, 3 figures, 10 tables. Code is available at:\n  https://github.com/Karifannaa/Auto-check-EGE-math", "pdf_url": "http://arxiv.org/pdf/2507.22958v1", "cate": "cs.CV", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.23084", "title": "AutoIndexer: A Reinforcement Learning-Enhanced Index Advisor Towards Scaling Workloads", "authors": ["Taiyi Wang", "Eiko Yoneki"], "categories": ["cs.DB", "cs.AI"], "primary_category": "Subjects:       Databases (cs.DB)", "pdf_link": null, "comments": "Comments:      14 pages", "url": "http://arxiv.org/abs/2507.23084v1", "summary": "Efficiently selecting indexes is fundamental to database performance\noptimization, particularly for systems handling large-scale analytical\nworkloads. While deep reinforcement learning (DRL) has shown promise in\nautomating index selection through its ability to learn from experience, few\nworks address how these RL-based index advisors can adapt to scaling workloads\ndue to exponentially growing action spaces and heavy trial and error. To\naddress these challenges, we introduce AutoIndexer, a framework that combines\nworkload compression, query optimization, and specialized RL models to scale\nindex selection effectively. By operating on compressed workloads, AutoIndexer\nsubstantially lowers search complexity without sacrificing much index quality.\nExtensive evaluations show that it reduces end-to-end query execution time by\nup to 95% versus non-indexed baselines. On average, it outperforms\nstate-of-the-art RL-based index advisors by approximately 20% in workload cost\nsavings while cutting tuning time by over 50%. These results affirm\nAutoIndexer's practicality for large and diverse workloads.", "comment": "14 pages", "pdf_url": "http://arxiv.org/pdf/2507.23084v1", "cate": "cs.DB", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.22925", "title": "Hierarchical Memory for High-Efficiency Long-Term Reasoning in LLM Agents", "authors": ["Haoran Sun", "Shaoning Zeng"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22925v1", "summary": "Long-term memory is one of the key factors influencing the reasoning\ncapabilities of Large Language Model Agents (LLM Agents). Incorporating a\nmemory mechanism that effectively integrates past interactions can\nsignificantly enhance decision-making and contextual coherence of LLM Agents.\nWhile recent works have made progress in memory storage and retrieval, such as\nencoding memory into dense vectors for similarity-based search or organizing\nknowledge in the form of graph, these approaches often fall short in structured\nmemory organization and efficient retrieval. To address these limitations, we\npropose a Hierarchical Memory (H-MEM) architecture for LLM Agents that\norganizes and updates memory in a multi-level fashion based on the degree of\nsemantic abstraction. Each memory vector is embedded with a positional index\nencoding pointing to its semantically related sub-memories in the next layer.\nDuring the reasoning phase, an index-based routing mechanism enables efficient,\nlayer-by-layer retrieval without performing exhaustive similarity computations.\nWe evaluate our method on five task settings from the LoCoMo dataset.\nExperimental results show that our approach consistently outperforms five\nbaseline methods, demonstrating its effectiveness in long-term dialogue\nscenarios.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22925v1", "cate": "cs.CL", "date": "2025-07-23", "updated": "2025-07-23"}
{"id": "2507.08540", "title": "White-Basilisk: A Hybrid Model for Code Vulnerability Detection", "authors": ["Ioannis Lamprou", "Alexander Shevtsov", "Ioannis Arapakis", "Sotiris Ioannidis"], "categories": ["cs.CR", "cs.AI"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.08540v2", "summary": "The proliferation of software vulnerabilities presents a significant\nchallenge to cybersecurity, necessitating more effective detection\nmethodologies. We introduce White-Basilisk, a novel approach to vulnerability\ndetection that demonstrates superior performance while challenging prevailing\nassumptions in AI model scaling. Utilizing an innovative architecture that\nintegrates Mamba layers, linear self-attention, and a Mixture of Experts\nframework, White-Basilisk achieves state-of-the-art results in vulnerability\ndetection tasks with a parameter count of only 200M. The model's capacity to\nprocess sequences of unprecedented length enables comprehensive analysis of\nextensive codebases in a single pass, surpassing the context limitations of\ncurrent Large Language Models (LLMs). White-Basilisk exhibits robust\nperformance on imbalanced, real-world datasets, while maintaining computational\nefficiency that facilitates deployment across diverse organizational scales.\nThis research not only establishes new benchmarks in code security but also\nprovides empirical evidence that compact, efficiently designed models can\noutperform larger counterparts in specialized tasks, potentially redefining\noptimization strategies in AI development for domain-specific applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.08540v2", "cate": "cs.CR", "date": "2025-07-11", "updated": "2025-07-31"}
{"id": "2507.22926", "title": "Multi-Relation Extraction in Entity Pairs using Global Context", "authors": ["Nilesh", "Atul Gupta", "Avinash C Panday"], "categories": ["cs.CL", "cs.IR"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      11 pages, 9 figures", "url": "http://arxiv.org/abs/2507.22926v1", "summary": "In document-level relation extraction, entities may appear multiple times in\na document, and their relationships can shift from one context to another.\nAccurate prediction of the relationship between two entities across an entire\ndocument requires building a global context spanning all relevant sentences.\nPrevious approaches have focused only on the sentences where entities are\nmentioned, which fails to capture the complete document context necessary for\naccurate relation extraction. Therefore, this paper introduces a novel input\nembedding approach to capture the positions of mentioned entities throughout\nthe document rather than focusing solely on the span where they appear. The\nproposed input encoding approach leverages global relationships and\nmulti-sentence reasoning by representing entities as standalone segments,\nindependent of their positions within the document. The performance of the\nproposed method has been tested on three benchmark relation extraction\ndatasets, namely DocRED, Re-DocRED, and REBEL. The experimental results\ndemonstrated that the proposed method accurately predicts relationships between\nentities in a document-level setting. The proposed research also has\ntheoretical and practical implications. Theoretically, it advances global\ncontext modeling and multi-sentence reasoning in document-level relation\nextraction. Practically, it enhances relationship detection, enabling improved\nperformance in real-world NLP applications requiring comprehensive entity-level\ninsights and interpretability.", "comment": "11 pages, 9 figures", "pdf_url": "http://arxiv.org/pdf/2507.22926v1", "cate": "cs.CL", "date": "2025-07-23", "updated": "2025-07-23"}
{"id": "2507.23718", "title": "Informing AI Risk Assessment with News Media: Analyzing National and Political Variation in the Coverage of AI Risks", "authors": ["Mowafak Allaham", "Kimon Kieslich", "Nicholas Diakopoulos"], "categories": ["cs.CY"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "Comments:      Accepted to 8th AAAI/ACM Conference on AI, Ethics, and Society (2025)", "url": "http://arxiv.org/abs/2507.23718v1", "summary": "Risk-based approaches to AI governance often center the technological\nartifact as the primary focus of risk assessments, overlooking systemic risks\nthat emerge from the complex interaction between AI systems and society. One\npotential source to incorporate more societal context into these approaches is\nthe news media, as it embeds and reflects complex interactions between AI\nsystems, human stakeholders, and the larger society. News media is influential\nin terms of which AI risks are emphasized and discussed in the public sphere,\nand thus which risks are deemed important. Yet, variations in the news media\nbetween countries and across different value systems (e.g. political\norientations) may differentially shape the prioritization of risks through the\nmedia's agenda setting and framing processes. To better understand these\nvariations, this work presents a comparative analysis of a cross-national\nsample of news media spanning 6 countries (the U.S., the U.K., India,\nAustralia, Israel, and South Africa). Our findings show that AI risks are\nprioritized differently across nations and shed light on how left vs. right\nleaning U.S. based outlets not only differ in the prioritization of AI risks in\ntheir coverage, but also use politicized language in the reporting of these\nrisks. These findings can inform risk assessors and policy-makers about the\nnuances they should account for when considering news media as a supplementary\nsource for risk-based governance approaches.", "comment": "Accepted to 8th AAAI/ACM Conference on AI, Ethics, and Society (2025)", "pdf_url": "http://arxiv.org/pdf/2507.23718v1", "cate": "cs.CY", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23006", "title": "Robust and Efficient 3D Gaussian Splatting for Urban Scene Reconstruction", "authors": ["Zhensheng Yuan", "Haozhi Huang", "Zhen Xiong", "Di Wang", "Guanghua Yang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23006v1", "summary": "We present a framework that enables fast reconstruction and real-time\nrendering of urban-scale scenes while maintaining robustness against appearance\nvariations across multi-view captures. Our approach begins with scene\npartitioning for parallel training, employing a visibility-based image\nselection strategy to optimize training efficiency. A controllable\nlevel-of-detail (LOD) strategy explicitly regulates Gaussian density under a\nuser-defined budget, enabling efficient training and rendering while\nmaintaining high visual fidelity. The appearance transformation module\nmitigates the negative effects of appearance inconsistencies across images\nwhile enabling flexible adjustments. Additionally, we utilize enhancement\nmodules, such as depth regularization, scale regularization, and antialiasing,\nto improve reconstruction fidelity. Experimental results demonstrate that our\nmethod effectively reconstructs urban-scale scenes and outperforms previous\napproaches in both efficiency and quality. The source code is available at:\nhttps://yzslab.github.io/REUrbanGS.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23006v1", "cate": "cs.CV", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.23499", "title": "Jelly-Patch: a Fast Format for Recording Changes in RDF Datasets", "authors": ["Piotr Sowinski", "Kacper Grzymkowski", "Anastasiya Danilenka"], "categories": ["cs.DB"], "primary_category": "Subjects:       Databases (cs.DB)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23499v1", "summary": "Recording data changes in RDF systems is a crucial capability, needed to\nsupport auditing, incremental backups, database replication, and event-driven\nworkflows. In large-scale and low-latency RDF applications, the high volume and\nfrequency of updates can cause performance bottlenecks in the serialization and\ntransmission of changes. To alleviate this, we propose Jelly-Patch -- a\nhigh-performance, compressed binary serialization format for changes in RDF\ndatasets. To evaluate its performance, we benchmark Jelly-Patch against\nexisting RDF Patch formats, using two datasets representing different use cases\n(change data capture and IoT streams). Jelly-Patch is shown to achieve\n3.5--8.9x better compression, and up to 2.5x and 4.6x higher throughput in\nserialization and parsing, respectively. These significant advancements in\nthroughput and compression are expected to improve the performance of\nlarge-scale and low-latency RDF systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23499v1", "cate": "cs.DB", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.22928", "title": "How does Chain of Thought Think? Mechanistic Interpretability of Chain-of-Thought Reasoning with Sparse Autoencoding", "authors": ["Xi Chen", "Aske Plaat", "Niki van Stein"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22928v1", "summary": "Chain-of-thought (CoT) prompting boosts Large Language Models accuracy on\nmulti-step tasks, yet whether the generated \"thoughts\" reflect the true\ninternal reasoning process is unresolved. We present the first feature-level\ncausal study of CoT faithfulness. Combining sparse autoencoders with activation\npatching, we extract monosemantic features from Pythia-70M and Pythia-2.8B\nwhile they tackle GSM8K math problems under CoT and plain (noCoT) prompting.\nSwapping a small set of CoT-reasoning features into a noCoT run raises answer\nlog-probabilities significantly in the 2.8B model, but has no reliable effect\nin 70M, revealing a clear scale threshold. CoT also leads to significantly\nhigher activation sparsity and feature interpretability scores in the larger\nmodel, signalling more modular internal computation. For example, the model's\nconfidence in generating correct answers improves from 1.2 to 4.3. We introduce\npatch-curves and random-feature patching baselines, showing that useful CoT\ninformation is not only present in the top-K patches but widely distributed.\nOverall, our results indicate that CoT can induce more interpretable internal\nstructures in high-capacity LLMs, validating its role as a structured prompting\nmethod.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22928v1", "cate": "cs.CL", "date": "2025-07-24", "updated": "2025-07-24"}
{"id": "2507.19060", "title": "PurpCode: Reasoning for Safer Code Generation", "authors": ["Jiawei Liu", "Nirav Diwan", "Zhe Wang", "Haoyu Zhai", "Xiaona Zhou", "Kiet A. Nguyen", "Tianjiao Yu", "Muntasir Wahed", "Yinlin Deng", "Hadjer Benkraouda", "Yuxiang Wei", "Lingming Zhang", "Ismini Lourentzou", "Gang Wang"], "categories": ["cs.CR", "cs.CL", "cs.LG", "cs.SE"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19060v2", "summary": "We introduce PurpCode, the first post-training recipe for training safe code\nreasoning models towards generating secure code and defending against malicious\ncyberactivities. PurpCode trains a reasoning model in two stages: (i) Rule\nLearning, which explicitly teaches the model to reference cybersafety rules to\ngenerate vulnerability-free code and to avoid facilitating malicious\ncyberactivities; and (ii) Reinforcement Learning, which optimizes model safety\nand preserves model utility through diverse, multi-objective reward mechanisms.\nTo empower the training pipelines with comprehensive cybersafety data, we\nconduct internal red-teaming to synthesize comprehensive and high-coverage\nprompts based on real-world tasks for inducing unsafe cyberactivities in the\nmodel. Based on PurpCode, we develop a reasoning-based coding model, namely\nPurpCode-32B, which demonstrates state-of-the-art cybersafety, outperforming\nvarious frontier models. Meanwhile, our alignment method decreases the model\noverrefusal rates in both general and cybersafety-specific scenarios, while\npreserving model utility in both code generation and common security knowledge.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19060v2", "cate": "cs.CR", "date": "2025-07-25", "updated": "2025-07-31"}
{"id": "2507.22927", "title": "PRGB Benchmark: A Robust Placeholder-Assisted Algorithm for Benchmarking Retrieval-Augmented Generation", "authors": ["Zhehao Tan", "Yihan Jiao", "Dan Yang", "Lei Liu", "Jie Feng", "Duolin Sun", "Yue Shen", "Jian Wang", "Peng Wei", "Jinjie Gu"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22927v1", "summary": "Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by\nintegrating external knowledge, where the LLM's ability to generate responses\nbased on the combination of a given query and retrieved documents is crucial.\nHowever, most benchmarks focus on overall RAG system performance, rarely\nassessing LLM-specific capabilities. Current benchmarks emphasize broad aspects\nsuch as noise robustness, but lack a systematic and granular evaluation\nframework on document utilization. To this end, we introduce\n\\textit{Placeholder-RAG-Benchmark}, a multi-level fine-grained benchmark,\nemphasizing the following progressive dimensions: (1) multi-level filtering\nabilities, (2) combination abilities, and (3) reference reasoning. To provide a\nmore nuanced understanding of LLMs' roles in RAG systems, we formulate an\ninnovative placeholder-based approach to decouple the contributions of the\nLLM's parametric knowledge and the external knowledge. Experiments demonstrate\nthe limitations of representative LLMs in the RAG system's generation\ncapabilities, particularly in error resilience and context faithfulness. Our\nbenchmark provides a reproducible framework for developing more reliable and\nefficient RAG systems. Our code is available in\nhttps://github.com/Alipay-Med/PRGB.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22927v1", "cate": "cs.CL", "date": "2025-07-23", "updated": "2025-07-23"}
{"id": "2507.22889", "title": "Knowledge Is More Than Performance: How Knowledge Diversity Drives Human-Human and Human-AI Interaction Synergy and Reveals Pure-AI Interaction Shortfalls", "authors": ["Tom Sheffer", "Alon Miron", "Yaniv Dover", "Ariel Goldstein"], "categories": ["cs.HC", "cs.CY"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22889v1", "summary": "Conversations transform individual knowledge into collective insight,\nallowing groups of humans and increasingly groups of artificial intelligence\n(AI) agents to collaboratively solve complex problems. Whether interactions\nbetween AI agents can replicate the synergy observed in human discussions\nremains an open question. To investigate this, we systematically compared four\nconversational configurations: pairs of large language models (LLM-LLM), trios\nof LLMs, trios of humans, and mixed human-LLM pairs. After agents answered\nquestions individually, they engaged in open-ended discussions and then\nreconsidered their initial answers. Interactions involving humans consistently\nled to accuracy improvements after the conversations, benefiting both stronger\nand weaker participants. By contrast, purely LLM-based pairs and trios\nexhibited declines in accuracy, demonstrating limited conversational synergy.\nAnalysis of participants' confidence and answer-switching behavior revealed\nthat knowledge diversity is a critical factor enabling collaborative\nimprovement. Crucially, the lack of gains in LLM-LLM interactions did not stem\nfrom a fundamental limitation of the models' ability to collaborate, but from\nhighly similar knowledge states that left little room for productive exchange.\nOur findings argue for a paradigm shift in AI development: rather than\noptimizing individual models solely for standalone performance, explicitly\ncultivating diversity across agents, even at the cost of slightly lower\nindividual accuracy, may yield AI collaborators that are more effective in\ngroup settings with humans or other AI systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22889v1", "cate": "cs.HC", "date": "2025-06-15", "updated": "2025-06-15"}
{"id": "2507.23021", "title": "Modeling Human Gaze Behavior with Diffusion Models for Unified Scanpath Prediction", "authors": ["Giuseppe Cartella", "Vittorio Cuculo", "Alessandro D'Amelio", "Marcella Cornia", "Giuseppe Boccignone", "Rita Cucchiara"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2025", "url": "http://arxiv.org/abs/2507.23021v1", "summary": "Predicting human gaze scanpaths is crucial for understanding visual\nattention, with applications in human-computer interaction, autonomous systems,\nand cognitive robotics. While deep learning models have advanced scanpath\nprediction, most existing approaches generate averaged behaviors, failing to\ncapture the variability of human visual exploration. In this work, we present\nScanDiff, a novel architecture that combines diffusion models with Vision\nTransformers to generate diverse and realistic scanpaths. Our method explicitly\nmodels scanpath variability by leveraging the stochastic nature of diffusion\nmodels, producing a wide range of plausible gaze trajectories. Additionally, we\nintroduce textual conditioning to enable task-driven scanpath generation,\nallowing the model to adapt to different visual search objectives. Experiments\non benchmark datasets show that ScanDiff surpasses state-of-the-art methods in\nboth free-viewing and task-driven scenarios, producing more diverse and\naccurate scanpaths. These results highlight its ability to better capture the\ncomplexity of human visual behavior, pushing forward gaze prediction research.\nSource code and models are publicly available at\nhttps://aimagelab.github.io/ScanDiff.", "comment": "Proceedings of the IEEE/CVF International Conference on Computer\n  Vision (ICCV), 2025", "pdf_url": "http://arxiv.org/pdf/2507.23021v1", "cate": "cs.CV", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.23515", "title": "DataLens: Enhancing Dataset Discovery via Network Topologies", "authors": ["Anaïs Ollagnier", "Aline Menin"], "categories": ["cs.DB"], "primary_category": "Subjects:       Databases (cs.DB)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23515v1", "summary": "The rapid growth of publicly available textual resources, such as lexicons\nand domain-specific corpora, presents challenges in efficiently identifying\nrelevant resources. While repositories are emerging, they often lack advanced\nsearch and exploration features. Most search methods rely on keyword queries\nand metadata filtering, which require prior knowledge and fail to reveal\nconnections between resources. To address this, we present DataLens, a\nweb-based platform that combines faceted search with advanced visualization\ntechniques to enhance resource discovery. DataLens offers network-based\nvisualizations, where the network structure can be adapted to suit the specific\nanalysis task. It also supports a chained views approach, enabling users to\nexplore data from multiple perspectives. A formative user study involving six\ndata practitioners revealed that users highly value visualization\ntools-especially network-based exploration-and offered insights to help refine\nour approach to better support dataset search.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23515v1", "cate": "cs.DB", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.22931", "title": "Enhancing RAG Efficiency with Adaptive Context Compression", "authors": ["Shuyu Guo", "Zhaochun Ren"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22931v1", "summary": "Retrieval-augmented generation (RAG) enhances large language models (LLMs)\nwith external knowledge but incurs significant inference costs due to lengthy\nretrieved contexts. While context compression mitigates this issue, existing\nmethods apply fixed compression rates, over-compressing simple queries or\nunder-compressing complex ones. We propose Adaptive Context Compression for RAG\n(ACC-RAG), a framework that dynamically adjusts compression rates based on\ninput complexity, optimizing inference efficiency without sacrificing accuracy.\nACC-RAG combines a hierarchical compressor (for multi-granular embeddings) with\na context selector to retain minimal sufficient information, akin to human\nskimming. Evaluated on Wikipedia and five QA datasets, ACC-RAG outperforms\nfixed-rate methods and matches/unlocks over 4 times faster inference versus\nstandard RAG while maintaining or improving accuracy.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22931v1", "cate": "cs.CL", "date": "2025-07-24", "updated": "2025-07-24"}
{"id": "2503.05727", "title": "Toward Integrated Solutions: A Systematic Interdisciplinary Review of Cybergrooming Research", "authors": ["Heajun An", "Marcos Silva", "Qi Zhang", "Arav Singh", "Minqian Liu", "Xinyi Zhang", "Sarvech Qadir", "Sang Won Lee", "Lifu Huang", "Pamela J. Wisniewski", "Jin-Hee Cho"], "categories": ["cs.CY", "cs.CR"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.05727v2", "summary": "Cybergrooming exploits minors through online trust-building, yet research\nremains fragmented, limiting holistic prevention. Social sciences focus on\nbehavioral insights, while computational methods emphasize detection, but their\nintegration remains insufficient. This review systematically synthesizes both\nfields using the PRISMA framework to enhance clarity, reproducibility, and\ncross-disciplinary collaboration. Findings show that qualitative methods offer\ndeep insights but are resource-intensive, machine learning models depend on\ndata quality, and standard metrics struggle with imbalance and cultural\nnuances. By bridging these gaps, this review advances interdisciplinary\ncybergrooming research, guiding future efforts toward more effective prevention\nand detection strategies.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.05727v2", "cate": "cs.CY", "date": "2025-02-18", "updated": "2025-07-31"}
{"id": "2507.22929", "title": "EH-Benchmark Ophthalmic Hallucination Benchmark and Agent-Driven Top-Down Traceable Reasoning Workflow", "authors": ["Xiaoyu Pan", "Yang Bai", "Ke Zou", "Yang Zhou", "Jun Zhou", "Huazhu Fu", "Yih-Chung Tham", "Yong Liu"], "categories": ["cs.CL", "cs.CV", "cs.MA"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      9 figures, 5 tables. submit/6621751", "url": "http://arxiv.org/abs/2507.22929v1", "summary": "Medical Large Language Models (MLLMs) play a crucial role in ophthalmic\ndiagnosis, holding significant potential to address vision-threatening\ndiseases. However, their accuracy is constrained by hallucinations stemming\nfrom limited ophthalmic knowledge, insufficient visual localization and\nreasoning capabilities, and a scarcity of multimodal ophthalmic data, which\ncollectively impede precise lesion detection and disease diagnosis.\nFurthermore, existing medical benchmarks fail to effectively evaluate various\ntypes of hallucinations or provide actionable solutions to mitigate them. To\naddress the above challenges, we introduce EH-Benchmark, a novel ophthalmology\nbenchmark designed to evaluate hallucinations in MLLMs. We categorize MLLMs'\nhallucinations based on specific tasks and error types into two primary\nclasses: Visual Understanding and Logical Composition, each comprising multiple\nsubclasses. Given that MLLMs predominantly rely on language-based reasoning\nrather than visual processing, we propose an agent-centric, three-phase\nframework, including the Knowledge-Level Retrieval stage, the Task-Level Case\nStudies stage, and the Result-Level Validation stage. Experimental results show\nthat our multi-agent framework significantly mitigates both types of\nhallucinations, enhancing accuracy, interpretability, and reliability. Our\nproject is available at https://github.com/ppxy1/EH-Benchmark.", "comment": "9 figures, 5 tables. submit/6621751", "pdf_url": "http://arxiv.org/pdf/2507.22929v1", "cate": "cs.CL", "date": "2025-07-24", "updated": "2025-07-24"}
{"id": "2507.22941", "title": "SigBERT: Combining Narrative Medical Reports and Rough Path Signature Theory for Survival Risk Estimation in Oncology", "authors": ["Paul Minchella", "Loïc Verlingue", "Stéphane Chrétien", "Rémi Vaucher", "Guillaume Metzler"], "categories": ["cs.CL", "cs.CY", "cs.LG", "stat.AP"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      12 pages, 2 figures, accepted for ECML PKDD 2025", "url": "http://arxiv.org/abs/2507.22941v1", "summary": "Electronic medical reports (EHR) contain a vast amount of information that\ncan be leveraged for machine learning applications in healthcare. However,\nexisting survival analysis methods often struggle to effectively handle the\ncomplexity of textual data, particularly in its sequential form. Here, we\npropose SigBERT, an innovative temporal survival analysis framework designed to\nefficiently process a large number of clinical reports per patient. SigBERT\nprocesses timestamped medical reports by extracting and averaging word\nembeddings into sentence embeddings. To capture temporal dynamics from the time\nseries of sentence embedding coordinates, we apply signature extraction from\nrough path theory to derive geometric features for each patient, which\nsignificantly enhance survival model performance by capturing complex temporal\ndynamics. These features are then integrated into a LASSO-penalized Cox model\nto estimate patient-specific risk scores. The model was trained and evaluated\non a real-world oncology dataset from the L\\'eon B\\'erard Center corpus, with a\nC-index score of 0.75 (sd 0.014) on the independent test cohort. SigBERT\nintegrates sequential medical data to enhance risk estimation, advancing\nnarrative-based survival analysis.", "comment": "12 pages, 2 figures, accepted for ECML PKDD 2025", "pdf_url": "http://arxiv.org/pdf/2507.22941v1", "cate": "cs.CL", "date": "2025-07-25", "updated": "2025-07-25"}
{"id": "2507.23027", "title": "Recovering Diagnostic Value: Super-Resolution-Aided Echocardiographic Classification in Resource-Constrained Imaging", "authors": ["Krishan Agyakari Raja Babu", "Om Prabhu", "Annu", "Mohanasankar Sivaprakasam"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted at the MICCAI Workshop on \"Medical Image Computing in Resource Constrained Settings & Knowledge Interchange (MIRASOL)\" 2025", "url": "http://arxiv.org/abs/2507.23027v1", "summary": "Automated cardiac interpretation in resource-constrained settings (RCS) is\noften hindered by poor-quality echocardiographic imaging, limiting the\neffectiveness of downstream diagnostic models. While super-resolution (SR)\ntechniques have shown promise in enhancing magnetic resonance imaging (MRI) and\ncomputed tomography (CT) scans, their application to echocardiography-a widely\naccessible but noise-prone modality-remains underexplored. In this work, we\ninvestigate the potential of deep learning-based SR to improve classification\naccuracy on low-quality 2D echocardiograms. Using the publicly available CAMUS\ndataset, we stratify samples by image quality and evaluate two clinically\nrelevant tasks of varying complexity: a relatively simple Two-Chamber vs.\nFour-Chamber (2CH vs. 4CH) view classification and a more complex End-Diastole\nvs. End-Systole (ED vs. ES) phase classification. We apply two widely used SR\nmodels-Super-Resolution Generative Adversarial Network (SRGAN) and\nSuper-Resolution Residual Network (SRResNet), to enhance poor-quality images\nand observe significant gains in performance metric-particularly with SRResNet,\nwhich also offers computational efficiency. Our findings demonstrate that SR\ncan effectively recover diagnostic value in degraded echo scans, making it a\nviable tool for AI-assisted care in RCS, achieving more with less.", "comment": "Accepted at the MICCAI Workshop on \"Medical Image Computing in\n  Resource Constrained Settings & Knowledge Interchange (MIRASOL)\" 2025", "pdf_url": "http://arxiv.org/pdf/2507.23027v1", "cate": "cs.CV", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.23358", "title": "Text-to-SQL Task-oriented Dialogue Ontology Construction", "authors": ["Renato Vukovic", "Carel van Niekerk", "Michael Heck", "Benjamin Ruppik", "Hsien-Chin Lin", "Shutong Feng", "Nurul Lubis", "Milica Gasic"], "categories": ["cs.CL", "cs.AI", "cs.DB", "cs.IR"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23358v1", "summary": "Large language models (LLMs) are widely used as general-purpose knowledge\nsources, but they rely on parametric knowledge, limiting explainability and\ntrustworthiness. In task-oriented dialogue (TOD) systems, this separation is\nexplicit, using an external database structured by an explicit ontology to\nensure explainability and controllability. However, building such ontologies\nrequires manual labels or supervised training. We introduce TeQoDO: a\nText-to-SQL task-oriented Dialogue Ontology construction method. Here, an LLM\nautonomously builds a TOD ontology from scratch without supervision using its\ninherent SQL programming capabilities combined with dialogue theory provided in\nthe prompt. We show that TeQoDO outperforms transfer learning approaches, and\nits constructed ontology is competitive on a downstream dialogue state tracking\ntask. Ablation studies demonstrate the key role of dialogue theory. TeQoDO also\nscales to allow construction of much larger ontologies, which we investigate on\na Wikipedia and ArXiv dataset. We view this as a step towards broader\napplication of ontologies to increase LLM explainability.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23358v1", "cate": "cs.CL", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23387", "title": "SGEMM-cube: Emulating FP32 GEMM on Ascend NPUs Using FP16 Cube Units with Precision Recovery", "authors": ["Weicheng Xue", "Baisong Xu", "Kai Yang", "Yongxiang Liu", "Dengdeng Fan", "Pengxiang Xu", "Yonghong Tian"], "categories": ["cs.DC"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23387v2", "summary": "Low-precision matrix engines, such as FP16 cube, offer high throughput but\nlack support for full-precision computation. In this work, we propose\nSGEMM-cube, a high-performance algorithm for emulating FP32 general\nmatrix-matrix multiplication (GEMM) using only FP16 computation units on a\nrepresentative AI accelerator. The method decomposes each FP32 operand into two\nFP16 values and compensates for numerical errors through a tunable scaling\nstrategy. A detailed analysis of numerical errors, including underflow\nconditions and precision loss, guides the selection of scaling parameters to\npreserve up to 22 bits of mantissa accuracy. We further investigate the effect\nof computation order on accuracy and demonstrate that a term-wise accumulation\nscheme improves numerical stability over conventional FP32 GEMM in low-exponent\nregimes. Finally, a cache-aware blocking strategy and double-buffered pipeline\nare introduced to overlap memory transfers with computation, enabling\nSGEMM-cube to achieve up to 77\\% of the theoretical FP32-equivalent peak\nperformance on Ascend 910A NPU lacking native FP32 support. Extensive numerical\nexperiments confirm that our method not only recovers the accuracy of native\nFP32 GEMM but also exhibits superior numerical stability under certain\nconditions, due to its structured and error-aware computation order.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23387v2", "cate": "cs.DC", "date": "2025-07-31", "updated": "2025-08-01"}
{"id": "2507.22933", "title": "Augmented Vision-Language Models: A Systematic Review", "authors": ["Anthony C Davis", "Burhan Sadiq", "Tianmin Shu", "Chien-Ming Huang"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22933v1", "summary": "Recent advances in visual-language machine learning models have demonstrated\nexceptional ability to use natural language and understand visual scenes by\ntraining on large, unstructured datasets. However, this training paradigm\ncannot produce interpretable explanations for its outputs, requires retraining\nto integrate new information, is highly resource-intensive, and struggles with\ncertain forms of logical reasoning. One promising solution involves integrating\nneural networks with external symbolic information systems, forming neural\nsymbolic systems that can enhance reasoning and memory abilities. These neural\nsymbolic systems provide more interpretable explanations to their outputs and\nthe capacity to assimilate new information without extensive retraining.\nUtilizing powerful pre-trained Vision-Language Models (VLMs) as the core neural\ncomponent, augmented by external systems, offers a pragmatic approach to\nrealizing the benefits of neural-symbolic integration. This systematic\nliterature review aims to categorize techniques through which visual-language\nunderstanding can be improved by interacting with external symbolic information\nsystems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22933v1", "cate": "cs.CL", "date": "2025-07-24", "updated": "2025-07-24"}
{"id": "2507.22930", "title": "Protecting Vulnerable Voices: Synthetic Dataset Generation for Self-Disclosure Detection", "authors": ["Shalini Jangra", "Suparna De", "Nishanth Sastry", "Saeed Fadaei"], "categories": ["cs.CL", "cs.SI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      15 pages, 4 Figures, Accepted in \"The 17th International Conference on Advances in Social Networks Analysis and Mining -ASONAM-2025\"", "url": "http://arxiv.org/abs/2507.22930v1", "summary": "Social platforms such as Reddit have a network of communities of shared\ninterests, with a prevalence of posts and comments from which one can infer\nusers' Personal Information Identifiers (PIIs). While such self-disclosures can\nlead to rewarding social interactions, they pose privacy risks and the threat\nof online harms. Research into the identification and retrieval of such risky\nself-disclosures of PIIs is hampered by the lack of open-source labeled\ndatasets. To foster reproducible research into PII-revealing text detection, we\ndevelop a novel methodology to create synthetic equivalents of PII-revealing\ndata that can be safely shared. Our contributions include creating a taxonomy\nof 19 PII-revealing categories for vulnerable populations and the creation and\nrelease of a synthetic PII-labeled multi-text span dataset generated from 3\ntext generation Large Language Models (LLMs), Llama2-7B, Llama3-8B, and\nzephyr-7b-beta, with sequential instruction prompting to resemble the original\nReddit posts. The utility of our methodology to generate this synthetic dataset\nis evaluated with three metrics: First, we require reproducibility equivalence,\ni.e., results from training a model on the synthetic data should be comparable\nto those obtained by training the same models on the original posts. Second, we\nrequire that the synthetic data be unlinkable to the original users, through\ncommon mechanisms such as Google Search. Third, we wish to ensure that the\nsynthetic data be indistinguishable from the original, i.e., trained humans\nshould not be able to tell them apart. We release our dataset and code at\nhttps://netsys.surrey.ac.uk/datasets/synthetic-self-disclosure/ to foster\nreproducible research into PII privacy risks in online social media.", "comment": "15 pages, 4 Figures, Accepted in \"The 17th International Conference\n  on Advances in Social Networks Analysis and Mining -ASONAM-2025\"", "pdf_url": "http://arxiv.org/pdf/2507.22930v1", "cate": "cs.CL", "date": "2025-07-24", "updated": "2025-07-24"}
{"id": "2507.22944", "title": "Opacity as Authority: Arbitrariness and the Preclusion of Contestation", "authors": ["Naomi Omeonga wa Kayembe"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22944v1", "summary": "This article redefines arbitrariness not as a normative flaw or a symptom of\ndomination, but as a foundational functional mechanism structuring human\nsystems and interactions. Diverging from critical traditions that conflate\narbitrariness with injustice, it posits arbitrariness as a semiotic trait: a\nproperty enabling systems - linguistic, legal, or social - to operate\neffectively while withholding their internal rationale. Building on Ferdinand\nde Saussure's concept of l'arbitraire du signe, the analysis extends this\nprinciple beyond language to demonstrate its cross-domain applicability,\nparticularly in law and social dynamics. The paper introduces the \"Motivation\n-> Constatability -> Contestability\" chain, arguing that motivation functions\nas a crucial interface rendering an act's logic vulnerable to intersubjective\ncontestation. When this chain is broken through mechanisms like\n\"immotivization\" or \"Conflict Lateralization\" (exemplified by \"the blur of the\nwolf drowned in the fish\"), acts produce binding effects without exposing their\nrationale, thus precluding justiciability. This structural opacity, while\nappearing illogical, is a deliberate design protecting authority from\naccountability. Drawing on Shannon's entropy model, the paper formalizes\narbitrariness as A = H(L|M) (conditional entropy). It thereby proposes a modern\ntheory of arbitrariness as a neutral operator central to control as well as\ncare, an overlooked dimension of interpersonal relations. While primarily\ndeveloped through human social systems, this framework also illuminates a new\npathway for analyzing explainability in advanced artificial intelligence\nsystems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22944v1", "cate": "cs.CL", "date": "2025-07-25", "updated": "2025-07-25"}
{"id": "2507.23033", "title": "Adaptive Time-step Training for Enhancing Spike-Based Neural Radiance Fields", "authors": ["Ranxi Lin", "Canming Yao", "Jiayi Li", "Weihang Liu", "Xin Lou", "Pingqiang Zhou"], "categories": ["cs.CV", "cs.NE"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23033v1", "summary": "Neural Radiance Fields (NeRF)-based models have achieved remarkable success\nin 3D reconstruction and rendering tasks. However, during both training and\ninference, these models rely heavily on dense point sampling along rays from\nmultiple viewpoints, resulting in a surge in floating-point operations and\nseverely limiting their use in resource-constrained scenarios like edge\ncomputing. Spiking Neural Networks (SNNs), which communicate via binary spikes\nover discrete time steps, offer a promising alternative due to their\nenergy-efficient nature. Given the inherent variability in scene scale and\ntexture complexity in neural rendering and the prevailing practice of training\nseparate models per scene, we propose a spike-based NeRF framework with a\ndynamic time step training strategy, termed Pretrain-Adaptive Time-step\nAdjustment (PATA). This approach automatically explores the trade-off between\nrendering quality and time step length during training. Consequently, it\nenables scene-adaptive inference with variable time steps and reduces the\nadditional consumption of computational resources in the inference process.\nAnchoring to the established Instant-NGP architecture, we evaluate our method\nacross diverse datasets. The experimental results show that PATA can preserve\nrendering fidelity while reducing inference time steps by 64\\% and running\npower by 61.55\\%.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23033v1", "cate": "cs.CV", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2506.11298", "title": "Jelly: a Fast and Convenient RDF Serialization Format", "authors": ["Piotr Sowinski", "Karolina Bogacka", "Anastasiya Danilenka", "Nikita Kozlov"], "categories": ["cs.DB", "cs.NI"], "primary_category": "Subjects:       Databases (cs.DB)", "pdf_link": null, "comments": "Comments:      Developers Workshop, co-located with SEMANTiCS'25: International Conference on Semantic Systems, September 3-5, 2025, Vienna, Austria", "url": "http://arxiv.org/abs/2506.11298v2", "summary": "Existing RDF serialization formats such as Turtle, N-Quads, and JSON-LD are\nwidely used for communication and storage in knowledge graph and Semantic Web\napplications. However, they suffer from limitations in performance, compression\nratio, and lack of native support for RDF streams. To address these\nshortcomings, we introduce Jelly, a fast and convenient binary serialization\nformat for RDF data that supports both batch and streaming use cases. Jelly is\ndesigned to maximize serialization throughput, reduce file size with\nlightweight streaming compression, and minimize compute resource usage. Built\non Protocol Buffers, Jelly is easy to integrate with modern programming\nlanguages and RDF libraries. To maximize reusability, Jelly has an open\nprotocol specification, open-source implementations in Java and Python\nintegrated with popular RDF libraries, and a versatile command-line tool. To\nillustrate its usefulness, we outline concrete use cases where Jelly can\nprovide tangible benefits. We consider that by combining practical usability\nwith state-of-the-art efficiency, Jelly is an important contribution to the\nSemantic Web tool stack.", "comment": "Developers Workshop, co-located with SEMANTiCS'25: International\n  Conference on Semantic Systems, September 3-5, 2025, Vienna, Austria", "pdf_url": "http://arxiv.org/pdf/2506.11298v2", "cate": "cs.DB", "date": "2025-06-12", "updated": "2025-07-31"}
{"id": "2507.23431", "title": "Towards a Testbed for Scalable FaaS Platforms", "authors": ["Trever Schirmer", "David Bermbach"], "categories": ["cs.DC"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "Comments:      Accepted for Publication at the 13th IEEE International Conference on Cloud Engineering (IC2E 2025)", "url": "http://arxiv.org/abs/2507.23431v1", "summary": "Most cloud platforms have a Function-as-a-Service (FaaS) offering that\nenables users to easily write highly scalable applications. To better\nunderstand how the platform's architecture impacts its performance, we present\na research-focused testbed that can be adapted to quickly evaluate the impact\nof different architectures and technologies on the characteristics of\nscalability-focused FaaS platforms.", "comment": "Accepted for Publication at the 13th IEEE International Conference on\n  Cloud Engineering (IC2E 2025)", "pdf_url": "http://arxiv.org/pdf/2507.23431v1", "cate": "cs.DC", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.22934", "title": "Deep Learning Approaches for Multimodal Intent Recognition: A Survey", "authors": ["Jingwei Zhao", "Yuhua Wen", "Qifei Li", "Minchi Hu", "Yingying Zhou", "Jingyao Xue", "Junyang Wu", "Yingming Gao", "Zhengqi Wen", "Jianhua Tao", "Ya Li"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Submitted to ACM Computing Surveys", "url": "http://arxiv.org/abs/2507.22934v1", "summary": "Intent recognition aims to identify users' underlying intentions,\ntraditionally focusing on text in natural language processing. With growing\ndemands for natural human-computer interaction, the field has evolved through\ndeep learning and multimodal approaches, incorporating data from audio, vision,\nand physiological signals. Recently, the introduction of Transformer-based\nmodels has led to notable breakthroughs in this domain. This article surveys\ndeep learning methods for intent recognition, covering the shift from unimodal\nto multimodal techniques, relevant datasets, methodologies, applications, and\ncurrent challenges. It provides researchers with insights into the latest\ndevelopments in multimodal intent recognition (MIR) and directions for future\nresearch.", "comment": "Submitted to ACM Computing Surveys", "pdf_url": "http://arxiv.org/pdf/2507.22934v1", "cate": "cs.CL", "date": "2025-07-24", "updated": "2025-07-24"}
{"id": "2507.22932", "title": "FinMarBa: A Market-Informed Dataset for Financial Sentiment Classification", "authors": ["Baptiste Lefort", "Eric Benhamou", "Beatrice Guez", "Jean-Jacques Ohana", "Ethan Setrouk", "Alban Etienne"], "categories": ["cs.CL", "q-fin.GN"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      8 pages", "url": "http://arxiv.org/abs/2507.22932v1", "summary": "This paper presents a novel hierarchical framework for portfolio\noptimization, integrating lightweight Large Language Models (LLMs) with Deep\nReinforcement Learning (DRL) to combine sentiment signals from financial news\nwith traditional market indicators. Our three-tier architecture employs base RL\nagents to process hybrid data, meta-agents to aggregate their decisions, and a\nsuper-agent to merge decisions based on market data and sentiment analysis.\nEvaluated on data from 2018 to 2024, after training on 2000-2017, the framework\nachieves a 26% annualized return and a Sharpe ratio of 1.2, outperforming\nequal-weighted and S&P 500 benchmarks. Key contributions include scalable\ncross-modal integration, a hierarchical RL structure for enhanced stability,\nand open-source reproducibility.", "comment": "8 pages", "pdf_url": "http://arxiv.org/pdf/2507.22932v1", "cate": "cs.CL", "date": "2025-07-24", "updated": "2025-07-24"}
{"id": "2507.23399", "title": "Beyond the Cloud: Assessing the Benefits and Drawbacks of Local LLM Deployment for Translators", "authors": ["Peter Sandrini"], "categories": ["cs.CL", "cs.CY", "I.2.7; K.4.3"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23399v1", "summary": "The rapid proliferation of Large Language Models presents both opportunities\nand challenges for the translation field. While commercial, cloud-based AI\nchatbots have garnered significant attention in translation studies, concerns\nregarding data privacy, security, and equitable access necessitate exploration\nof alternative deployment models. This paper investigates the feasibility and\nperformance of locally deployable, free language models as a viable alternative\nto proprietary, cloud-based AI solutions. This study evaluates three\nopen-source models installed on CPU-based platforms and compared against\ncommercially available online chat-bots. The evaluation focuses on functional\nperformance rather than a comparative analysis of human-machine translation\nquality, an area already subject to extensive research. The platforms assessed\nwere chosen for their accessibility and ease of use across various operating\nsystems. While local deployment introduces its own challenges, the benefits of\nenhanced data control, improved privacy, and reduced dependency on cloud\nservices are compelling. The findings of this study contribute to a growing\nbody of knowledge concerning the democratization of AI technology and inform\nfuture research and development efforts aimed at making LLMs more accessible\nand practical for a wider range of users, specifically focusing on the needs of\nindividual translators and small businesses.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23399v1", "cate": "cs.CL", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23042", "title": "Early Goal-Guided Multi-Scale Fusion for Real-Time Vision-Language Driving", "authors": ["Santosh Patapati", "Trisanth Srinivasan"], "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.MM", "cs.RO", "I.2.6; I.2.9; I.2.10; C.3.3"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      6 pages", "url": "http://arxiv.org/abs/2507.23042v1", "summary": "Autonomous vehicles must react in milliseconds while reasoning about road\ngeometry and traffic intent to navigate complex situations. We introduce\nNovaDrive, a single-branch vision-language architecture that processes\nfront-camera images, HD-map tiles, LiDAR depth, and textual waypoints in a\nsingle branch. A lightweight, two-stage cross-attention block first aligns\nwaypoint tokens with the HD map, then refines attention over fine-grained image\nand depth patches. Coupled with a novel smoothness loss that discourages abrupt\nsteering and speed changes, this design eliminates the need for recurrent\nmemory. We fine-tune the top 15 layers of an 11B LLaMA-3.2 vision-language\nbackbone, enabling real-time inference. On the nuScenes / Waymo subset of the\nMD-NEX Outdoor benchmark, NovaDrive raises success rate to 84% (+4%), boosts\npath-efficiency (SPL) to 0.66 (+0.11), and reduces collision frequency from\n2.6% to 1.2% (-1.4%) relative to the previous state-of-the-art. Our ablations\nconfirm that waypoint tokens, partial VLM fine-tuning, and the cross-attention\nfusion each contribute the most to these gains. Beyond safety, NovaDrive's\nshorter routes (resulting from the novel smoothness loss) translate to lower\nfuel or battery usage, pointing toward leaner, more easily updated driving\nstacks. NovaDrive can be extended to other embodied-AI domains as well.", "comment": "6 pages", "pdf_url": "http://arxiv.org/pdf/2507.23042v1", "cate": "cs.CV", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.06005", "title": "Towards Serverless Processing of Spatiotemporal Big Data Queries", "authors": ["Diana Baumann", "Tim C. Rese", "David Bermbach"], "categories": ["cs.DB", "cs.DC"], "primary_category": "Subjects:       Databases (cs.DB)", "pdf_link": null, "comments": "Comments:      Accepted for publication in 13th IEEE International Conference on Cloud Engineering (IC2E 2025)", "url": "http://arxiv.org/abs/2507.06005v2", "summary": "Spatiotemporal data are being produced in continuously growing volumes by a\nvariety of data sources and a variety of application fields rely on rapid\nanalysis of such data. Existing systems such as PostGIS or MobilityDB usually\nbuild on relational database systems, thus, inheriting their scale-out\ncharacteristics. As a consequence, big spatiotemporal data scenarios still have\nlimited support even though many query types can easily be parallelized. In\nthis paper, we propose our vision of a native serverless data processing\napproach for spatiotemporal data: We break down queries into small subqueries\nwhich then leverage the near-instant scaling of Function-as-a-Service platforms\nto execute them in parallel. With this, we partially solve the scalability\nneeds of big spatiotemporal data processing.", "comment": "Accepted for publication in 13th IEEE International Conference on\n  Cloud Engineering (IC2E 2025)", "pdf_url": "http://arxiv.org/pdf/2507.06005v2", "cate": "cs.DB", "date": "2025-07-08", "updated": "2025-07-31"}
{"id": "2507.23533", "title": "Threshold-Driven Streaming Graph: Expansion and Rumor Spreading", "authors": ["Flora Angileri", "Andrea Clementi", "Emanuele Natale", "Michele Salvi", "Isabella Ziccardi"], "categories": ["cs.DC", "math.PR"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23533v1", "summary": "A randomized distributed algorithm called RAES was introduced in [Becchetti\net al., SODA 2020] to extract a bounded-degree expander from a dense $n$-vertex\nexpander graph $G = (V, E)$. The algorithm relies on a simple threshold-based\nprocedure. A key assumption in [Becchetti et al., SODA 2020] is that the input\ngraph $G$ is static - i.e., both its vertex set $V$ and edge set $E$ remain\nunchanged throughout the process - while the analysis of RAES in dynamic models\nis left as a major open question.\n  In this work, we investigate the behavior of RAES under a dynamic graph model\ninduced by a streaming node-churn process (also known as the sliding window\nmodel), where, at each discrete round, a new node joins the graph and the\noldest node departs. This process yields a bounded-degree dynamic graph\n$\\mathcal{G} =\\{ G_t = (V_t, E_t) : t \\in \\mathbb{N}\\}$ that captures essential\ncharacteristics of peer-to-peer networks -- specifically, node churn and\nthreshold on the number of connections each node can manage. We prove that\nevery snapshot $G_t$ in the dynamic graph sequence has good expansion\nproperties with high probability. Furthermore, we leverage this property to\nestablish a logarithmic upper bound on the completion time of the well-known\nPUSH and PULL rumor spreading protocols over the dynamic graph $\\mathcal{G}$.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23533v1", "cate": "cs.DC", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.22935", "title": "Trusted Knowledge Extraction for Operations and Maintenance Intelligence", "authors": ["Kathleen Mealey", "Jonathan A. Karr Jr.", "Priscila Saboia Moreira", "Paul R. Brenner", "Charles F. Vardeman II"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22935v1", "summary": "Deriving operational intelligence from organizational data repositories is a\nkey challenge due to the dichotomy of data confidentiality vs data integration\nobjectives, as well as the limitations of Natural Language Processing (NLP)\ntools relative to the specific knowledge structure of domains such as\noperations and maintenance. In this work, we discuss Knowledge Graph\nconstruction and break down the Knowledge Extraction process into its Named\nEntity Recognition, Coreference Resolution, Named Entity Linking, and Relation\nExtraction functional components. We then evaluate sixteen NLP tools in concert\nwith or in comparison to the rapidly advancing capabilities of Large Language\nModels (LLMs). We focus on the operational and maintenance intelligence use\ncase for trusted applications in the aircraft industry. A baseline dataset is\nderived from a rich public domain US Federal Aviation Administration dataset\nfocused on equipment failures or maintenance requirements. We assess the\nzero-shot performance of NLP and LLM tools that can be operated within a\ncontrolled, confidential environment (no data is sent to third parties). Based\non our observation of significant performance limitations, we discuss the\nchallenges related to trusted NLP and LLM tools as well as their Technical\nReadiness Level for wider use in mission-critical industries such as aviation.\nWe conclude with recommendations to enhance trust and provide our open-source\ncurated dataset to support further baseline testing and evaluation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22935v1", "cate": "cs.CL", "date": "2025-07-24", "updated": "2025-07-24"}
{"id": "2507.22937", "title": "CoE-Ops: Collaboration of LLM-based Experts for AIOps Question-Answering", "authors": ["Jinkun Zhao", "Yuanshuai Wang", "Xingjian Zhang", "Ruibo Chen", "Xingchuang Liao", "Junle Wang", "Lei Huang", "Kui Zhang", "Wenjun Wu"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22937v1", "summary": "With the rapid evolution of artificial intelligence, AIOps has emerged as a\nprominent paradigm in DevOps. Lots of work has been proposed to improve the\nperformance of different AIOps phases. However, constrained by domain-specific\nknowledge, a single model can only handle the operation requirement of a\nspecific task,such as log parser,root cause analysis. Meanwhile, combining\nmultiple models can achieve more efficient results, which have been proved in\nboth previous ensemble learning and the recent LLM training domain. Inspired by\nthese works,to address the similar challenges in AIOPS, this paper first\nproposes a collaboration-of-expert framework(CoE-Ops) incorporating a\ngeneral-purpose large language model task classifier. A retrieval-augmented\ngeneration mechanism is introduced to improve the framework's capability in\nhandling both Question-Answering tasks with high-level(Code,build,Test,etc.)\nand low-level(fault analysis,anomaly detection,etc.). Finally, the proposed\nmethod is implemented in the AIOps domain, and extensive experiments are\nconducted on the DevOps-EVAL dataset. Experimental results demonstrate that\nCoE-Ops achieves a 72% improvement in routing accuracy for high-level AIOps\ntasks compared to existing CoE methods, delivers up to 8% accuracy enhancement\nover single AIOps models in DevOps problem resolution, and outperforms\nlarger-scale Mixture-of-Experts (MoE) models by up to 14% in accuracy.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22937v1", "cate": "cs.CL", "date": "2025-07-25", "updated": "2025-07-25"}
{"id": "2507.23454", "title": "Breaking the mould of Social Mixed Reality -- State-of-the-Art and Glossary", "authors": ["Marta Bieńkiewicz", "Julia Ayache", "Panayiotis Charalambous", "Cristina Becchio", "Marco Corragio", "Bertram Taetz", "Francesco De Lellis", "Antonio Grotta", "Anna Server", "Daniel Rammer", "Richard Kulpa", "Franck Multon", "Azucena Garcia-Palacios", "Jessica Sutherland", "Kathleen Bryson", "Stéphane Donikian", "Didier Stricker", "Benoît Bardy"], "categories": ["cs.HC", "cs.CY", "cs.ET", "cs.GR", "q-bio.NC", "I.3.0; I.2; J.4; K.4"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      pre-print", "url": "http://arxiv.org/abs/2507.23454v2", "summary": "This article explores a critical gap in Mixed Reality (MR) technology: while\nadvances have been made, MR still struggles to authentically replicate human\nembodiment and socio-motor interaction. For MR to enable truly meaningful\nsocial experiences, it needs to incorporate multi-modal data streams and\nmulti-agent interaction capabilities. To address this challenge, we present a\ncomprehensive glossary covering key topics such as Virtual Characters and\nAutonomisation, Responsible AI, Ethics by Design, and the Scientific Challenges\nof Social MR within Neuroscience, Embodiment, and Technology. Our aim is to\ndrive the transformative evolution of MR technologies that prioritize\nhuman-centric innovation, fostering richer digital connections. We advocate for\nMR systems that enhance social interaction and collaboration between humans and\nvirtual autonomous agents, ensuring inclusivity, ethical design and\npsychological safety in the process.", "comment": "pre-print", "pdf_url": "http://arxiv.org/pdf/2507.23454v2", "cate": "cs.HC", "date": "2025-07-31", "updated": "2025-08-01"}
{"id": "2507.23058", "title": "Reference-Guided Diffusion Inpainting For Multimodal Counterfactual Generation", "authors": ["Alexandru Buburuzan"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      A dissertation submitted to The University of Manchester for the degree of Bachelor of Science in Artificial Intelligence", "url": "http://arxiv.org/abs/2507.23058v1", "summary": "Safety-critical applications, such as autonomous driving and medical image\nanalysis, require extensive multimodal data for rigorous testing. Synthetic\ndata methods are gaining prominence due to the cost and complexity of gathering\nreal-world data, but they demand a high degree of realism and controllability\nto be useful. This work introduces two novel methods for synthetic data\ngeneration in autonomous driving and medical image analysis, namely MObI and\nAnydoorMed, respectively. MObI is a first-of-its-kind framework for Multimodal\nObject Inpainting that leverages a diffusion model to produce realistic and\ncontrollable object inpaintings across perceptual modalities, demonstrated\nsimultaneously for camera and lidar. Given a single reference RGB image, MObI\nenables seamless object insertion into existing multimodal scenes at a\nspecified 3D location, guided by a bounding box, while maintaining semantic\nconsistency and multimodal coherence. Unlike traditional inpainting methods\nthat rely solely on edit masks, this approach uses 3D bounding box conditioning\nto ensure accurate spatial positioning and realistic scaling. AnydoorMed\nextends this paradigm to the medical imaging domain, focusing on\nreference-guided inpainting for mammography scans. It leverages a\ndiffusion-based model to inpaint anomalies with impressive detail preservation,\nmaintaining the reference anomaly's structural integrity while semantically\nblending it with the surrounding tissue. Together, these methods demonstrate\nthat foundation models for reference-guided inpainting in natural images can be\nreadily adapted to diverse perceptual modalities, paving the way for the next\ngeneration of systems capable of constructing highly realistic, controllable\nand multimodal counterfactual scenarios.", "comment": "A dissertation submitted to The University of Manchester for the\n  degree of Bachelor of Science in Artificial Intelligence", "pdf_url": "http://arxiv.org/pdf/2507.23058v1", "cate": "cs.CV", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.22305", "title": "Is SHACL Suitable for Data Quality Assessment?", "authors": ["Carolina Cortés", "Lisa Ehrlinger", "Lorena Etcheverry", "Felix Naumann"], "categories": ["cs.DB"], "primary_category": "Subjects:       Databases (cs.DB)", "pdf_link": null, "comments": "Comments:      43 pages", "url": "http://arxiv.org/abs/2507.22305v2", "summary": "Knowledge graphs have been widely adopted in both enterprises, such as the\nGoogle Knowledge Graph, and open platforms like Wikidata, to represent domain\nknowledge and support artificial intelligence applications. They model\nreal-world information as nodes and edges. To embrace flexibility, knowledge\ngraphs often lack enforced schemas (i.e., ontologies), leading to potential\ndata quality issues, such as semantically overlapping nodes. Yet ensuring their\nquality is essential, as issues in the data can affect applications relying on\nthem. To assess the quality of knowledge graphs, existing works propose either\nhigh-level frameworks comprising various data quality dimensions without\nconcrete implementations, define tools that measure data quality with ad-hoc\nSPARQL queries, or promote the usage of constraint languages, such as the\nShapes Constraint Language (SHACL), to assess and improve the quality of the\ngraph. Although the latter approaches claim to address data quality assessment,\nnone of them comprehensively tries to cover all data quality dimensions. In\nthis paper, we explore this gap by investigating the extent to which SHACL can\nbe used to assess data quality in knowledge graphs. Specifically, we defined\nSHACL shapes for 69 data quality metrics proposed by Zaveri et al. [1] and\nimplemented a prototype that automatically instantiates these shapes and\ncomputes the corresponding data quality measures from their validation results.\nAll resources are provided for repeatability.", "comment": "43 pages", "pdf_url": "http://arxiv.org/pdf/2507.22305v2", "cate": "cs.DB", "date": "2025-07-30", "updated": "2025-07-31"}
{"id": "2507.23700", "title": "The ArborX library: version 2.0", "authors": ["Andrey Prokopenko", "Daniel Arndt", "Damien Lebrun-Grandié", "Bruno Turcksin"], "categories": ["cs.DC"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23700v1", "summary": "This paper provides an overview of the 2.0 release of the ArborX library, a\nperformance portable geometric search library based on Kokkos. We describe the\nmajor changes in ArborX 2.0 including a new interface for the library to\nsupport a wider range of user problems, new search data structures (brute\nforce, distributed), support for user functions to be executed on the results\n(callbacks), and an expanded set of the supported algorithms (ray tracing,\nclustering).", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23700v1", "cate": "cs.DC", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.05903", "title": "AI-Reporter: A Path to a New Genre of Scientific Communication", "authors": ["Gerd Graßhoff"], "categories": ["cs.DL", "cs.CL"], "primary_category": "Subjects:       Digital Libraries (cs.DL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.05903v2", "summary": "The AI-Reporter represents a paradigmatic shift in scientific publication\npractice. This document demonstrates through a concrete case study how our\nsystem transforms academic presentations into publication-ready chapters -- in\nless than three minutes. Using Arno Simons' lecture on Large Language Models\nfrom the ``Large Language Models for the History, Philosophy, and Sociology of\nScience'' workshop (NEPI) as an example, we show how technological innovation\nbridges the gap between ephemeral presentation and permanent scientific\ndocumentation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.05903v2", "cate": "cs.DL", "date": "2025-07-08", "updated": "2025-07-31"}
{"id": "2507.22938", "title": "A Graph-based Approach for Multi-Modal Question Answering from Flowcharts in Telecom Documents", "authors": ["Sumit Soman", "H. G. Ranjani", "Sujoy Roychowdhury", "Venkata Dharma Surya Narayana Sastry", "Akshat Jain", "Pranav Gangrade", "Ayaaz Khan"], "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Accepted for publication at the KDD 2025 Workshop on Structured Knowledge for Large Language Models", "url": "http://arxiv.org/abs/2507.22938v1", "summary": "Question-Answering (QA) from technical documents often involves questions\nwhose answers are present in figures, such as flowcharts or flow diagrams.\nText-based Retrieval Augmented Generation (RAG) systems may fail to answer such\nquestions. We leverage graph representations of flowcharts obtained from Visual\nlarge Language Models (VLMs) and incorporate them in a text-based RAG system to\nshow that this approach can enable image retrieval for QA in the telecom\ndomain. We present the end-to-end approach from processing technical documents,\nclassifying image types, building graph representations, and incorporating them\nwith the text embedding pipeline for efficient retrieval. We benchmark the same\non a QA dataset created based on proprietary telecom product information\ndocuments. Results show that the graph representations obtained using a\nfine-tuned VLM model have lower edit distance with respect to the ground truth,\nwhich illustrate the robustness of these representations for flowchart images.\nFurther, the approach for QA using these representations gives good retrieval\nperformance using text-based embedding models, including a telecom-domain\nadapted one. Our approach also alleviates the need for a VLM in inference,\nwhich is an important cost benefit for deployed QA systems.", "comment": "Accepted for publication at the KDD 2025 Workshop on Structured\n  Knowledge for Large Language Models", "pdf_url": "http://arxiv.org/pdf/2507.22938v1", "cate": "cs.CL", "date": "2025-07-25", "updated": "2025-07-25"}
{"id": "2507.22939", "title": "PARROT: An Open Multilingual Radiology Reports Dataset", "authors": ["Bastien Le Guellec", "Kokou Adambounou", "Lisa C Adams", "Thibault Agripnidis", "Sung Soo Ahn", "Radhia Ait Chalal", "Tugba Akinci D Antonoli", "Philippe Amouyel", "Henrik Andersson", "Raphael Bentegeac", "Claudio Benzoni", "Antonino Andrea Blandino", "Felix Busch", "Elif Can", "Riccardo Cau", "Armando Ugo Cavallo", "Christelle Chavihot", "Erwin Chiquete", "Renato Cuocolo", "Eugen Divjak", "Gordana Ivanac", "Barbara Dziadkowiec Macek", "Armel Elogne", "Salvatore Claudio Fanni", "Carlos Ferrarotti", "Claudia Fossataro", "Federica Fossataro", "Katarzyna Fulek", "Michal Fulek", "Pawel Gac", "Martyna Gachowska", "Ignacio Garcia Juarez", "Marco Gatti", "Natalia Gorelik", "Alexia Maria Goulianou", "Aghiles Hamroun", "Nicolas Herinirina", "Krzysztof Kraik", "Dominik Krupka", "Quentin Holay", "Felipe Kitamura", "Michail E Klontzas", "Anna Kompanowska", "Rafal Kompanowski", "Alexandre Lefevre", "Tristan Lemke", "Maximilian Lindholz", "Lukas Muller", "Piotr Macek", "Marcus Makowski", "Luigi Mannacio", "Aymen Meddeb", "Antonio Natale", "Beatrice Nguema Edzang", "Adriana Ojeda", "Yae Won Park", "Federica Piccione", "Andrea Ponsiglione", "Malgorzata Poreba", "Rafal Poreba", "Philipp Prucker", "Jean Pierre Pruvo", "Rosa Alba Pugliesi", "Feno Hasina Rabemanorintsoa", "Vasileios Rafailidis", "Katarzyna Resler", "Jan Rotkegel", "Luca Saba", "Ezann Siebert", "Arnaldo Stanzione", "Ali Fuat Tekin", "Liz Toapanta Yanchapaxi", "Matthaios Triantafyllou", "Ekaterini Tsaoulia", "Evangelia Vassalou", "Federica Vernuccio", "Johan Wasselius", "Weilang Wang", "Szymon Urban", "Adrian Wlodarczak", "Szymon Wlodarczak", "Andrzej Wysocki", "Lina Xu", "Tomasz Zatonski", "Shuhang Zhang", "Sebastian Ziegelmayer", "Gregory Kuchcinski", "Keno K Bressem"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22939v1", "summary": "Rationale and Objectives: To develop and validate PARROT (Polyglottal\nAnnotated Radiology Reports for Open Testing), a large, multicentric,\nopen-access dataset of fictional radiology reports spanning multiple languages\nfor testing natural language processing applications in radiology. Materials\nand Methods: From May to September 2024, radiologists were invited to\ncontribute fictional radiology reports following their standard reporting\npractices. Contributors provided at least 20 reports with associated metadata\nincluding anatomical region, imaging modality, clinical context, and for\nnon-English reports, English translations. All reports were assigned ICD-10\ncodes. A human vs. AI report differentiation study was conducted with 154\nparticipants (radiologists, healthcare professionals, and non-healthcare\nprofessionals) assessing whether reports were human-authored or AI-generated.\nResults: The dataset comprises 2,658 radiology reports from 76 authors across\n21 countries and 13 languages. Reports cover multiple imaging modalities (CT:\n36.1%, MRI: 22.8%, radiography: 19.0%, ultrasound: 16.8%) and anatomical\nregions, with chest (19.9%), abdomen (18.6%), head (17.3%), and pelvis (14.1%)\nbeing most prevalent. In the differentiation study, participants achieved 53.9%\naccuracy (95% CI: 50.7%-57.1%) in distinguishing between human and AI-generated\nreports, with radiologists performing significantly better (56.9%, 95% CI:\n53.3%-60.6%, p<0.05) than other groups. Conclusion: PARROT represents the\nlargest open multilingual radiology report dataset, enabling development and\nvalidation of natural language processing applications across linguistic,\ngeographic, and clinical boundaries without privacy constraints.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22939v1", "cate": "cs.CL", "date": "2025-07-25", "updated": "2025-07-25"}
{"id": "2507.23535", "title": "Transparent AI: The Case for Interpretability and Explainability", "authors": ["Dhanesh Ramachandram", "Himanshu Joshi", "Judy Zhu", "Dhari Gandhi", "Lucas Hartman", "Ananya Raval"], "categories": ["cs.LG", "cs.AI", "cs.CY"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23535v1", "summary": "As artificial intelligence systems increasingly inform high-stakes decisions\nacross sectors, transparency has become foundational to responsible and\ntrustworthy AI implementation. Leveraging our role as a leading institute in\nadvancing AI research and enabling industry adoption, we present key insights\nand lessons learned from practical interpretability applications across diverse\ndomains. This paper offers actionable strategies and implementation guidance\ntailored to organizations at varying stages of AI maturity, emphasizing the\nintegration of interpretability as a core design principle rather than a\nretrospective add-on.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23535v1", "cate": "cs.LG", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23064", "title": "Vision-Language Fusion for Real-Time Autonomous Driving: Goal-Centered Cross-Attention of Camera, HD-Map, & Waypoints", "authors": ["Santosh Patapati", "Trisanth Srinivasan", "Murari Ambati"], "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO", "I.4.8; I.2.10; I.2.6; C.3.3; I.4.9"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      5 pages", "url": "http://arxiv.org/abs/2507.23064v1", "summary": "Autonomous cars need geometric accuracy and semantic understanding to\nnavigate complex environments, yet most stacks handle them separately. We\npresent XYZ-Drive, a single vision-language model that reads a front-camera\nframe, a 25m $\\times$ 25m overhead map, and the next waypoint, then outputs\nsteering and speed. A lightweight goal-centered cross-attention layer lets\nwaypoint tokens highlight relevant image and map patches, supporting both\naction and textual explanations, before the fused tokens enter a partially\nfine-tuned LLaMA-3.2 11B model.\n  On the MD-NEX Outdoor-Driving benchmark XYZ-Drive attains 95% success and\n0.80 Success weighted by Path Length (SPL), surpassing PhysNav-DG by 15%. and\nhalving collisions, all while significantly improving efficiency by using only\na single branch. Sixteen ablations explain the gains. Removing any modality\n(vision, waypoint, map) drops success by up to 11%, confirming their\ncomplementary roles and rich connections. Replacing goal-centered attention\nwith simple concatenation cuts 3% in performance, showing query-based fusion\ninjects map knowledge more effectively. Keeping the transformer frozen loses\n5%, showing the importance of fine-tuning when applying VLMs for specific tasks\nsuch as autonomous driving. Coarsening map resolution from 10 cm to 40 cm blurs\nlane edges and raises crash rate.\n  Overall, these results demonstrate that early, token-level fusion of intent\nand map layout enables accurate, transparent, real-time driving.", "comment": "5 pages", "pdf_url": "http://arxiv.org/pdf/2507.23064v1", "cate": "cs.CV", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.22701", "title": "SAM: A Stability-Aware Cache Manager for Multi-Tenant Embedded Databases", "authors": ["Haoran Zhang", "Decheng Zuo", "Yu Yan", "Zhiyu Liang", "Hongzhi Wang"], "categories": ["cs.DB", "H.2.4; H.2.7"], "primary_category": "Subjects:       Databases (cs.DB)", "pdf_link": null, "comments": "Comments:      17 pages, 10 figures. An extended version of a paper under review at the VLDB 2026 conference", "url": "http://arxiv.org/abs/2507.22701v2", "summary": "The co-location of multiple database instances on resource constrained edge\nnodes creates significant cache contention, where traditional schemes are\ninefficient and unstable under dynamic workloads. To address this, we present\nSAM(a Stability-Aware Manager), an autonomic cache manager that establishes\ndecision stability as a first-class design principle. It achieves this through\nits core control policy, AURA(Autonomic Utility-balancing Resource Allocator),\nwhich resolves the classic exploitation-exploration dilemma by synthesizing two\northogonal factors: the H-factor, representing proven historical efficiency\n(exploitation), and the V-factor, for estimated marginal gain (exploration).\nThrough this practical synthesis and adaptive control, SAM achieves sustained\nhigh performance with strategic stability and robustness in volatile\nconditions.\n  Extensive experiments against 14 diverse baselines demonstrate SAM's\nsuperiority. It achieves top-tier throughput while being uniquely resilient to\ncomplex workload shifts and adversarial workloads like cache pollution.\nFurthermore, its decision latency is highly scalable, remaining nearly constant\nas the system grows to 120 databases. Crucially, SAM achieves superior decision\nstability -- maintaining consistent optimization directions despite noise,\navoiding performance oscillations while ensuring predictable Quality of\nService. These results prove that a principled, stability-aware design is\nessential for sustained high performance in real-world, large-scale systems.", "comment": "17 pages, 10 figures. An extended version of a paper under review at\n  the VLDB 2026 conference", "pdf_url": "http://arxiv.org/pdf/2507.22701v2", "cate": "cs.DB", "date": "2025-07-30", "updated": "2025-07-31"}
{"id": "2507.23609", "title": "Consistent Point Matching", "authors": ["Halid Ziya Yerebakan", "Gerardo Hermosillo Valadez"], "categories": ["cs.CV", "cs.DC", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23609v1", "summary": "This study demonstrates that incorporating a consistency heuristic into the\npoint-matching algorithm \\cite{yerebakan2023hierarchical} improves robustness\nin matching anatomical locations across pairs of medical images. We validated\nour approach on diverse longitudinal internal and public datasets spanning CT\nand MRI modalities. Notably, it surpasses state-of-the-art results on the Deep\nLesion Tracking dataset. Additionally, we show that the method effectively\naddresses landmark localization. The algorithm operates efficiently on standard\nCPU hardware and allows configurable trade-offs between speed and robustness.\nThe method enables high-precision navigation between medical images without\nrequiring a machine learning model or training data.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23609v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.22940", "title": "Trustworthy Reasoning: Evaluating and Enhancing Factual Accuracy in LLM Intermediate Thought Processes", "authors": ["Rui Jiao", "Yue Zhang", "Jinku Li"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22940v1", "summary": "We present RELIANCE (Reasoning Evaluation with Logical Integrity and Accuracy\nfor Confidence Enhancement), a novel framework addressing a critical\nvulnerability in Large Language Models (LLMs): the prevalence of factual\ninaccuracies within intermediate reasoning steps despite correct final answers.\nThis phenomenon poses substantial risks in high-stakes domains including\nhealthcare, legal analysis, and scientific research, where erroneous yet\nconfidently presented reasoning can mislead users into dangerous decisions. Our\nframework integrates three core components: (1) a specialized fact-checking\nclassifier trained on counterfactually augmented data to detect subtle factual\ninconsistencies within reasoning chains; (2) a Group Relative Policy\nOptimization (GRPO) reinforcement learning approach that balances factuality,\ncoherence, and structural correctness through multi-dimensional rewards; and\n(3) a mechanistic interpretability module examining how factuality improvements\nmanifest in model activations during reasoning processes. Extensive evaluation\nacross ten state-of-the-art models reveals concerning patterns: even leading\nmodels like Claude-3.7 and GPT-o1 demonstrate reasoning factual accuracy of\nonly 81.93% and 82.57% respectively. RELIANCE significantly enhances factual\nrobustness (up to 49.90% improvement) while maintaining or improving\nperformance on challenging benchmarks including Math-500, AIME-2024, and GPQA.\nFurthermore, our activation-level analysis provides actionable insights into\nhow factual enhancements reshape reasoning trajectories within model\narchitectures, establishing foundations for future training methodologies that\nexplicitly target factual robustness through activation-guided optimization.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22940v1", "cate": "cs.CL", "date": "2025-07-25", "updated": "2025-07-25"}
{"id": "2507.22943", "title": "A chart review process aided by natural language processing and multi-wave adaptive sampling to expedite validation of code-based algorithms for large database studies", "authors": ["Shirley V Wang", "Georg Hahn", "Sushama Kattinakere Sreedhara", "Mufaddal Mahesri", "Haritha S. Pillai", "Rajendra Aldis", "Joyce Lii", "Sarah K. Dutcher", "Rhoda Eniafe", "Jamal T. Jones", "Keewan Kim", "Jiwei He", "Hana Lee", "Sengwee Toh", "Rishi J Desai", "Jie Yang"], "categories": ["cs.CL", "stat.ME"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22943v1", "summary": "Background: One of the ways to enhance analyses conducted with large claims\ndatabases is by validating the measurement characteristics of code-based\nalgorithms used to identify health outcomes or other key study parameters of\ninterest. These metrics can be used in quantitative bias analyses to assess the\nrobustness of results for an inferential study given potential bias from\noutcome misclassification. However, extensive time and resource allocation are\ntypically re-quired to create reference-standard labels through manual chart\nreview of free-text notes from linked electronic health records. Methods: We\ndescribe an expedited process that introduces efficiency in a validation study\nus-ing two distinct mechanisms: 1) use of natural language processing (NLP) to\nreduce time spent by human reviewers to review each chart, and 2) a multi-wave\nadaptive sampling approach with pre-defined criteria to stop the validation\nstudy once performance characteristics are identified with sufficient\nprecision. We illustrate this process in a case study that validates the\nperformance of a claims-based outcome algorithm for intentional self-harm in\npatients with obesity. Results: We empirically demonstrate that the\nNLP-assisted annotation process reduced the time spent on review per chart by\n40% and use of the pre-defined stopping rule with multi-wave samples would have\nprevented review of 77% of patient charts with limited compromise to precision\nin derived measurement characteristics. Conclusion: This approach could\nfacilitate more routine validation of code-based algorithms used to define key\nstudy parameters, ultimately enhancing understanding of the reliability of\nfind-ings derived from database studies.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22943v1", "cate": "cs.CL", "date": "2025-07-25", "updated": "2025-07-25"}
{"id": "2401.13481", "title": "How AI Ideas Affect the Creativity, Diversity, and Evolution of Human Ideas: Evidence From a Large, Dynamic Experiment", "authors": ["Joshua Ashkinaze", "Julia Mendelsohn", "Li Qiwei", "Ceren Budak", "Eric Gilbert"], "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.HC"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "Comments:      Accepted at ACM Collective Intelligence 2025. Originally posted 2024", "url": "http://arxiv.org/abs/2401.13481v3", "summary": "Exposure to large language model output is rapidly increasing. How will\nseeing AI-generated ideas affect human ideas? We conducted an experiment (800+\nparticipants, 40+ countries) where participants viewed creative ideas that were\nfrom ChatGPT or prior experimental participants and then brainstormed their own\nidea. We varied the number of AI-generated examples (none, low, or high\nexposure) and if the examples were labeled as 'AI' (disclosure). Our dynamic\nexperiment design -- ideas from prior participants in an experimental condition\nare used as stimuli for future participants in the same experimental condition\n-- speaks to the interdependent process of cultural creation: creative ideas\nare built upon prior ideas. Hence, we capture the compounding effects of having\nLLMs 'in the culture loop'. We find that high AI exposure (but not low AI\nexposure) did not affect the creativity of individual ideas but did increase\nthe average amount and rate of change of collective idea diversity. AI made\nideas different, not better. There were no main effects of disclosure. We also\nfound that self-reported creative people were less influenced by knowing an\nidea was from AI and that participants may knowingly adopt AI ideas when the\ntask is difficult. Our findings suggest that introducing AI ideas may increase\ncollective diversity but not individual creativity.", "comment": "Accepted at ACM Collective Intelligence 2025. Originally posted 2024", "pdf_url": "http://arxiv.org/pdf/2401.13481v3", "cate": "cs.CY", "date": "2024-01-24", "updated": "2025-07-31"}
{"id": "2507.23070", "title": "Vocabulary-free Fine-grained Visual Recognition via Enriched Contextually Grounded Vision-Language Model", "authors": ["Dmitry Demidov", "Zaigham Zaheer", "Omkar Thawakar", "Salman Khan", "Fahad Shahbaz Khan"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted to ICCV 2025", "url": "http://arxiv.org/abs/2507.23070v1", "summary": "Fine-grained image classification, the task of distinguishing between\nvisually similar subcategories within a broader category (e.g., bird species,\ncar models, flower types), is a challenging computer vision problem.\nTraditional approaches rely heavily on fixed vocabularies and closed-set\nclassification paradigms, limiting their scalability and adaptability in\nreal-world settings where novel classes frequently emerge. Recent research has\ndemonstrated that combining large language models (LLMs) with vision-language\nmodels (VLMs) makes open-set recognition possible without the need for\npredefined class labels. However, the existing methods are often limited in\nharnessing the power of LLMs at the classification phase, and also rely heavily\non the guessed class names provided by an LLM without thorough analysis and\nrefinement. To address these bottlenecks, we propose our training-free method,\nEnriched-FineR (or E-FineR for short), which demonstrates state-of-the-art\nresults in fine-grained visual recognition while also offering greater\ninterpretability, highlighting its strong potential in real-world scenarios and\nnew domains where expert annotations are difficult to obtain. Additionally, we\ndemonstrate the application of our proposed approach to zero-shot and few-shot\nclassification, where it demonstrated performance on par with the existing SOTA\nwhile being training-free and not requiring human interventions. Overall, our\nvocabulary-free framework supports the shift in image classification from rigid\nlabel prediction to flexible, language-driven understanding, enabling scalable\nand generalizable systems for real-world applications. Well-documented code is\navailable on https://github.com/demidovd98/e-finer.", "comment": "Accepted to ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.23070v1", "cate": "cs.CV", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2506.12365", "title": "Advances in LLMs with Focus on Reasoning, Adaptability, Efficiency and Ethics", "authors": ["Asifullah Khan", "Muhammad Zaeem Khan", "Saleha Jamshed", "Sadia Ahmad", "Aleesha Zainab", "Kaynat Khatib", "Faria Bibi", "Abdul Rehman"], "categories": ["cs.CL", "cs.DB"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.12365v2", "summary": "This survey paper outlines the key developments in the field of Large\nLanguage Models (LLMs), including enhancements to their reasoning skills,\nadaptability to various tasks, increased computational efficiency, and the\nability to make ethical decisions. The techniques that have been most effective\nin bridging the gap between human and machine communications include the\nChain-of-Thought prompting, Instruction Tuning, and Reinforcement Learning from\nHuman Feedback. The improvements in multimodal learning and few-shot or\nzero-shot techniques have further empowered LLMs to handle complex jobs with\nminor input. A significant focus is placed on efficiency, detailing scaling\nstrategies, optimization techniques, and the influential Mixture-of-Experts\n(MoE) architecture, which strategically routes inputs to specialized\nsubnetworks to boost predictive accuracy, while optimizing resource allocation.\nThis survey also offers a broader perspective on recent advancements in LLMs,\ngoing beyond isolated aspects such as model architecture or ethical concerns.\nAdditionally, it explores the role of LLMs in Agentic AI and their use as\nAutonomous Decision-Making Systems, and categorizes emerging methods that\nenhance LLM reasoning, efficiency, and ethical alignment. The survey also\nidentifies underexplored areas such as interpretability, cross-modal\nintegration, and sustainability. While significant advancements have been made\nin LLMs, challenges such as high computational costs, biases, and ethical risks\nremain. Overcoming these requires a focus on bias mitigation, transparent\ndecision-making, and explicit ethical guidelines. Future research will\ngenerally focus on enhancing the model's ability to handle multiple inputs,\nthereby making it more intelligent, safe, and reliable.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.12365v2", "cate": "cs.CL", "date": "2025-06-14", "updated": "2025-07-31"}
{"id": "2505.12928", "title": "Minos: Exploiting Cloud Performance Variation with Function-as-a-Service Instance Selection", "authors": ["Trever Schirmer", "Valentin Carl", "Nils Höller", "Tobias Pfandzelter", "David Bermbach"], "categories": ["cs.DC"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "Comments:      Accepted for Publication at the 13th IEEE International Conference on Cloud Engineering (IC2E 2025)", "url": "http://arxiv.org/abs/2505.12928v2", "summary": "Serverless Function-as-a-Service (FaaS) is a popular cloud paradigm to\nquickly and cheaply implement complex applications. Because the function\ninstances cloud providers start to execute user code run on shared\ninfrastructure, their performance can vary. From a user perspective, slower\ninstances not only take longer to complete, but also increase cost due to the\npay-per-use model of FaaS services where execution duration is billed with\nmicrosecond accuracy. In this paper, we present Minos, a system to take\nadvantage of this performance variation by intentionally terminating instances\nthat are slow. Fast instances are not terminated, so that they can be re-used\nfor subsequent invocations. One use case for this are data processing and\nmachine learning workflows, which often download files as a first step, during\nwhich Minos can run a short benchmark. Only if the benchmark passes, the main\npart of the function is actually executed. Otherwise, the request is re-queued\nand the instance crashes itself, so that the platform has to assign the request\nto another (potentially faster) instance. In our experiments, this leads to a\nspeedup of up to 13% in the resource intensive part of a data processing\nworkflow, resulting in up to 4% faster overall performance (and consequently 4%\ncheaper prices). Longer and complex workflows lead to increased savings, as the\npool of fast instances is re-used more often. For platforms exhibiting this\nbehavior, users get better performance and save money by wasting more of the\nplatforms resources.", "comment": "Accepted for Publication at the 13th IEEE International Conference on\n  Cloud Engineering (IC2E 2025)", "pdf_url": "http://arxiv.org/pdf/2505.12928v2", "cate": "cs.DC", "date": "2025-05-19", "updated": "2025-07-31"}
{"id": "2507.23025", "title": "Constructing and Sampling Directed Graphs with Linearly Rescaled Degree Matrices", "authors": ["Yunxiang Yan", "Meng Jiang"], "categories": ["cs.SI", "cs.DM"], "primary_category": "Subjects:       Social and Information Networks (cs.SI)", "pdf_link": null, "comments": "Comments:      SIGKDD 2022", "url": "http://arxiv.org/abs/2507.23025v1", "summary": "In recent years, many large directed networks such as online social networks\nare collected with the help of powerful data engineering and data storage\ntechniques. Analyses of such networks attract significant attention from both\nthe academics and industries. However, analyses of large directed networks are\noften time-consuming and expensive because the complexities of a lot of graph\nalgorithms are often polynomial with the size of the graph. Hence, sampling\nalgorithms that can generate graphs preserving properties of original graph are\nof great importance because they can speed up the analysis process. We propose\na promising framework to sample directed graphs: Construct a sample graph with\nlinearly rescaled Joint Degree Matrix (JDM) and Degree Correlation Matrix\n(DCM). Previous work shows that graphs with the same JDM and DCM will have a\nrange of very similar graph properties. We also conduct experiments on\nreal-world datasets to show that the numbers of non-zero entries in JDM and DCM\nare quite small compared to the number of edges and nodes. Adopting this\nframework, we propose a novel graph sampling algorithm that can provably\npreserves in-degree and out-degree distributions, which are two most\nfundamental properties of a graph. We also prove the upper bound for deviations\nin the joint degree distribution and degree correlation distribution, which\ncorrespond to JDM and DCM. Besides, we prove that the deviations in these\ndistributions are negatively correlated with the sparsity of the JDM and DCM.\nConsidering that these two matrices are always quite sparse, we believe that\nproposed algorithm will have a better-than-theory performance on real-world\nlarge directed networks.", "comment": "SIGKDD 2022", "pdf_url": "http://arxiv.org/pdf/2507.23025v1", "cate": "cs.SI", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.22968", "title": "C3: A Bilingual Benchmark for Spoken Dialogue Models Exploring Challenges in Complex Conversations", "authors": ["Chengqian Ma", "Wei Tao", "Yiwen Guo"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22968v1", "summary": "Spoken Dialogue Models (SDMs) have recently attracted significant attention\nfor their ability to generate voice responses directly to users' spoken\nqueries. Despite their increasing popularity, there exists a gap in research\nfocused on comprehensively understanding their practical effectiveness in\ncomprehending and emulating human conversations. This is especially true\ncompared to text-based Large Language Models (LLMs), which benefit from\nextensive benchmarking. Human voice interactions are inherently more complex\nthan text due to characteristics unique to spoken dialogue. Ambiguity poses one\nchallenge, stemming from semantic factors like polysemy, as well as\nphonological aspects such as heterograph, heteronyms, and stress patterns.\nAdditionally, context-dependency, like omission, coreference, and multi-turn\ninteraction, adds further complexity to human conversational dynamics. To\nilluminate the current state of SDM development and to address these\nchallenges, we present a benchmark dataset in this paper, which comprises 1,079\ninstances in English and Chinese. Accompanied by an LLM-based evaluation method\nthat closely aligns with human judgment, this dataset facilitates a\ncomprehensive exploration of the performance of SDMs in tackling these\npractical challenges.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22968v1", "cate": "cs.CL", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.23063", "title": "Math Natural Language Inference: this should be easy!", "authors": ["Valeria de Paiva", "Qiyue Gao", "Hai Hu", "Pavel Kovalev", "Yikang Liu", "Lawrence S. Moss", "Zhiheng Qian"], "categories": ["cs.CL", "68T50", "I.2.7"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      9 pages plus appendices", "url": "http://arxiv.org/abs/2507.23063v1", "summary": "We ask whether contemporary LLMs are able to perform natural language\ninference (NLI) tasks on mathematical texts. We call this the Math NLI problem.\nWe construct a corpus of Math NLI pairs whose premises are from extant\nmathematical text and whose hypotheses and gold labels were provided by people\nwith experience in both research-level mathematics and also in the NLI field.\nWe also investigate the quality of corpora using the same premises but whose\nhypotheses are provided by LLMs themselves. We not only investigate the\nperformance but also the inter-group consistency of the diverse group of LLMs.\nWe have both positive and negative findings. Among our positive findings: in\nsome settings, using a majority vote of LLMs is approximately equivalent to\nusing human-labeled data in the Math NLI area. On the negative side: LLMs still\nstruggle with mathematical language. They occasionally fail at even basic\ninferences. Current models are not as prone to hypothesis-only \"inference\" in\nour data the way the previous generation had been. In addition to our findings,\nwe also provide our corpora as data to support future work on Math NLI.", "comment": "9 pages plus appendices", "pdf_url": "http://arxiv.org/pdf/2507.23063v1", "cate": "cs.CL", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2505.12892", "title": "\"I will never pay for this\" Perception of fairness and factors affecting behaviour on 'pay-or-ok' models", "authors": ["Victor Morel", "Farzaneh Karegar", "Cristiana Santos"], "categories": ["cs.CY"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "Comments:      Accepted for publication at APF2025", "url": "http://arxiv.org/abs/2505.12892v4", "summary": "The rise of cookie paywalls ('pay-or-ok' models) has prompted growing debates\naround the right to privacy and data protection, monetisation, and the\nlegitimacy of user consent. Despite their increasing use across sectors,\nlimited research has explored how users perceive these models or what shapes\ntheir decisions to either consent to tracking or pay. To address this gap, we\nconducted four focus groups (n= 14) to examine users' perceptions of cookie\npaywalls, their judgments of fairness, and the conditions under which they\nmight consider paying, alongside a legal analysis within the EU data protection\nlegal framework.\n  Participants primarily viewed cookie paywalls as profit-driven, with fairness\nperceptions varying depending on factors such as the presence of a third option\nbeyond consent or payment, transparency of data practices, and the authenticity\nor exclusivity of the paid content. Participants voiced expectations for\ngreater transparency, meaningful control over data collection, and less\ncoercive alternatives, such as contextual advertising or \"reject all\" buttons.\nAlthough some conditions, including trusted providers, exclusive content, and\nreasonable pricing, could make participants consider paying, most expressed\nreluctance or unwillingness to do so.\n  Crucially, our findings raise concerns about economic exclusion, where\nprivacy and data protection might end up becoming a privilege rather than\nfundamental rights. Consent given under financial pressure may not meet the\nstandard of being freely given, as required by the GDPR. To address these\nconcerns, we recommend user-centred approaches that enhance transparency,\nreduce coercion, ensure the value of paid content, and explore inclusive\nalternatives. These measures are essential for supporting fairness, meaningful\nchoice, and user autonomy in consent-driven digital environments.", "comment": "Accepted for publication at APF2025", "pdf_url": "http://arxiv.org/pdf/2505.12892v4", "cate": "cs.CY", "date": "2025-05-19", "updated": "2025-07-31"}
{"id": "2507.23110", "title": "Rethink Domain Generalization in Heterogeneous Sequence MRI Segmentation", "authors": ["Zheyuan Zhang", "Linkai Peng", "Wanying Dou", "Cuiling Sun", "Halil Ertugrul Aktas", "Andrea M. Bejar", "Elif Keles", "Gorkem Durak", "Ulas Bagci"], "categories": ["eess.IV", "cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23110v1", "summary": "Clinical magnetic-resonance (MR) protocols generate many T1 and T2 sequences\nwhose appearance differs more than the acquisition sites that produce them.\nExisting domain-generalization benchmarks focus almost on cross-center shifts\nand overlook this dominant source of variability. Pancreas segmentation remains\na major challenge in abdominal imaging: the gland is small, irregularly,\nsurrounded by organs and fat, and often suffers from low T1 contrast.\nState-of-the-art deep networks that already achieve >90% Dice on the liver or\nkidneys still miss 20-30% of the pancreas. The organ is also systematically\nunder-represented in public cross-domain benchmarks, despite its clinical\nimportance in early cancer detection, surgery, and diabetes research. To close\nthis gap, we present PancreasDG, a large-scale multi-center 3D MRI pancreas\nsegmentation dataset for investigating domain generalization in medical\nimaging. The dataset comprises 563 MRI scans from six institutions, spanning\nboth venous phase and out-of-phase sequences, enabling study of both\ncross-center and cross-sequence variations with pixel-accurate pancreas masks\ncreated by a double-blind, two-pass protocol. Through comprehensive analysis,\nwe reveal three insights: (i) limited sampling introduces significant variance\nthat may be mistaken for distribution shifts, (ii) cross-center performance\ncorrelates with source domain performance for identical sequences, and (iii)\ncross-sequence shifts require specialized solutions. We also propose a\nsemi-supervised approach that leverages anatomical invariances, significantly\noutperforming state-of-the-art domain generalization techniques with 61.63%\nDice score improvements and 87.00% on two test centers for cross-sequence\nsegmentation. PancreasDG sets a new benchmark for domain generalization in\nmedical imaging. Dataset, code, and models will be available at\nhttps://pancreasdg.netlify.app.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23110v1", "cate": "eess.IV", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.15230", "title": "GALE: Leveraging Heterogeneous Systems for Efficient Unstructured Mesh Data Analysis", "authors": ["Guoxi Liu", "Thomas Randall", "Rong Ge", "Federico Iuricich"], "categories": ["cs.DC", "cs.GR"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "Comments:      Accepted at IEEE VIS 2025", "url": "http://arxiv.org/abs/2507.15230v3", "summary": "Unstructured meshes present challenges in scientific data analysis due to\nirregular distribution and complex connectivity. Computing and storing\nconnectivity information is a major bottleneck for visualization algorithms,\naffecting both time and memory performance. Recent task-parallel data\nstructures address this by precomputing connectivity information at runtime\nwhile the analysis algorithm executes, effectively hiding computation costs and\nimproving performance. However, existing approaches are CPU-bound, forcing the\ndata structure and analysis algorithm to compete for the same computational\nresources, limiting potential speedups. To overcome this limitation, we\nintroduce a novel task-parallel approach optimized for heterogeneous CPU-GPU\nsystems. Specifically, we offload the computation of mesh connectivity\ninformation to GPU threads, enabling CPU threads to focus on executing the\nvisualization algorithm. Following this paradigm, we propose GALE (GPU-Aided\nLocalized data structurE), the first open-source CUDA-based data structure\ndesigned for heterogeneous task parallelism. Experiments on two 20-core CPUs\nand an NVIDIA V100 GPU show that GALE achieves up to 2.7x speedup over\nstate-of-the-art localized data structures while maintaining memory efficiency.", "comment": "Accepted at IEEE VIS 2025", "pdf_url": "http://arxiv.org/pdf/2507.15230v3", "cate": "cs.DC", "date": "2025-07-21", "updated": "2025-07-30"}
{"id": "2507.23659", "title": "Nyldon Factorization of Thue-Morse Words and Fibonacci Words", "authors": ["Kaisei Kishi", "Kazuki Kai", "Yuto Nakashima", "Shunsuke Inenaga", "Hideo Bannai"], "categories": ["cs.DS", "cs.DM"], "primary_category": "Subjects:       Data Structures and Algorithms (cs.DS)", "pdf_link": null, "comments": "Comments:      A full version of our conference paper accepted for SPIRE 2025", "url": "http://arxiv.org/abs/2507.23659v1", "summary": "The Nyldon factorization is a string factorization that is a non-decreasing\nproduct of Nyldon words. Nyldon words and Nyldon factorizations are recently\ndefined combinatorial objects inspired by the well-known Lyndon words and\nLyndon factorizations. In this paper, we investigate the Nyldon factorization\nof several words. First, we fully characterize the Nyldon factorizations of the\n(finite) Fibonacci and the (finite) Thue-Morse words. Moreover, we show that\nthere exists a non-decreasing product of Nyldon words that is a factorization\nof the infinite Thue-Morse word.", "comment": "A full version of our conference paper accepted for SPIRE 2025", "pdf_url": "http://arxiv.org/pdf/2507.23659v1", "cate": "cs.DS", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23009", "title": "Stop Evaluating AI with Human Tests, Develop Principled, AI-specific Tests instead", "authors": ["Tom Sühr", "Florian E. Dorner", "Olawale Salaudeen", "Augustin Kelava", "Samira Samadi"], "categories": ["cs.LG", "cs.AI", "91E45", "I.2"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23009v1", "summary": "Large Language Models (LLMs) have achieved remarkable results on a range of\nstandardized tests originally designed to assess human cognitive and\npsychological traits, such as intelligence and personality. While these results\nare often interpreted as strong evidence of human-like characteristics in LLMs,\nthis paper argues that such interpretations constitute an ontological error.\nHuman psychological and educational tests are theory-driven measurement\ninstruments, calibrated to a specific human population. Applying these tests to\nnon-human subjects without empirical validation, risks mischaracterizing what\nis being measured. Furthermore, a growing trend frames AI performance on\nbenchmarks as measurements of traits such as ``intelligence'', despite known\nissues with validity, data contamination, cultural bias and sensitivity to\nsuperficial prompt changes. We argue that interpreting benchmark performance as\nmeasurements of human-like traits, lacks sufficient theoretical and empirical\njustification. This leads to our position: Stop Evaluating AI with Human Tests,\nDevelop Principled, AI-specific Tests instead. We call for the development of\nprincipled, AI-specific evaluation frameworks tailored to AI systems. Such\nframeworks might build on existing frameworks for constructing and validating\npsychometrics tests, or could be created entirely from scratch to fit the\nunique context of AI.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23009v1", "cate": "cs.LG", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.23082", "title": "Exploring In-Context Learning for Frame-Semantic Parsing", "authors": ["Diego Garat", "Guillermo Moncecchi", "Dina Wonsever"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23082v1", "summary": "Frame Semantic Parsing (FSP) entails identifying predicates and labeling\ntheir arguments according to Frame Semantics. This paper investigates the use\nof In-Context Learning (ICL) with Large Language Models (LLMs) to perform FSP\nwithout model fine-tuning. We propose a method that automatically generates\ntask-specific prompts for the Frame Identification (FI) and Frame Semantic Role\nLabeling (FSRL) subtasks, relying solely on the FrameNet database. These\nprompts, constructed from frame definitions and annotated examples, are used to\nguide six different LLMs. Experiments are conducted on a subset of frames\nrelated to violent events. The method achieves competitive results, with F1\nscores of 94.3% for FI and 77.4% for FSRL. The findings suggest that ICL offers\na practical and effective alternative to traditional fine-tuning for\ndomain-specific FSP tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23082v1", "cate": "cs.CL", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2206.03234", "title": "Disparate Conditional Prediction in Multiclass Classifiers", "authors": ["Sivan Sabato", "Eran Treister", "Elad Yom-Tov"], "categories": ["cs.LG", "cs.CY", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Published at ICML 2025", "url": "http://arxiv.org/abs/2206.03234v3", "summary": "We propose methods for auditing multiclass classifiers for fairness under\nmulticlass equalized odds,by estimating the deviation from equalized odds when\nthe classifier is not completely fair. We generalize to multiclass classifiers\nthe measure of Disparate Conditional Prediction (DCP), originally suggested by\nSabato & Yom-Tov (2020) for binary classifiers. DCP is defined as the fraction\nof the population for which the classifier predicts with conditional prediction\nprobabilities that differ from the closest common baseline. We provide new\nlocal-optimization methods for estimating the multiclass DCPunder two different\nregimes,one in which the conditional confusion matrices for each protected\nsub-population are known, and one in which these cannot be estimated, for\ninstance, because the classifier is inaccessible or because good-quality\nindividual-level data is not available. These methods can be used to detect\nclassifiers that likely treat a significant fraction of the population\nunfairly. Experiments demonstrate the accuracy of the methods. Code is provided\nat https://github.com/sivansabato/ DCPmulticlass.", "comment": "Published at ICML 2025", "pdf_url": "http://arxiv.org/pdf/2206.03234v3", "cate": "cs.LG", "date": "2022-06-07", "updated": "2025-07-31"}
{"id": "2507.23134", "title": "Details Matter for Indoor Open-vocabulary 3D Instance Segmentation", "authors": ["Sanghun Jung", "Jingjing Zheng", "Ke Zhang", "Nan Qiao", "Albert Y. C. Chen", "Lu Xia", "Chi Liu", "Yuyin Sun", "Xiao Zeng", "Hsiang-Wei Huang", "Byron Boots", "Min Sun", "Cheng-Hao Kuo"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV 2025", "url": "http://arxiv.org/abs/2507.23134v1", "summary": "Unlike closed-vocabulary 3D instance segmentation that is often trained\nend-to-end, open-vocabulary 3D instance segmentation (OV-3DIS) often leverages\nvision-language models (VLMs) to generate 3D instance proposals and classify\nthem. While various concepts have been proposed from existing research, we\nobserve that these individual concepts are not mutually exclusive but\ncomplementary. In this paper, we propose a new state-of-the-art solution for\nOV-3DIS by carefully designing a recipe to combine the concepts together and\nrefining them to address key challenges. Our solution follows the two-stage\nscheme: 3D proposal generation and instance classification. We employ robust 3D\ntracking-based proposal aggregation to generate 3D proposals and remove\noverlapped or partial proposals by iterative merging/removal. For the\nclassification stage, we replace the standard CLIP model with Alpha-CLIP, which\nincorporates object masks as an alpha channel to reduce background noise and\nobtain object-centric representation. Additionally, we introduce the\nstandardized maximum similarity (SMS) score to normalize text-to-proposal\nsimilarity, effectively filtering out false positives and boosting precision.\nOur framework achieves state-of-the-art performance on ScanNet200 and S3DIS\nacross all AP and AR metrics, even surpassing an end-to-end closed-vocabulary\nmethod.", "comment": "ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.23134v1", "cate": "cs.CV", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2307.02968", "title": "A Simple $(1-ε)$-Approximation Semi-Streaming Algorithm for Maximum (Weighted) Matching", "authors": ["Sepehr Assadi"], "categories": ["cs.DS", "cs.DC"], "primary_category": "Subjects:       Data Structures and Algorithms (cs.DS)", "pdf_link": null, "comments": "Comments:      25 pages. This is the TheoretiCS journal version", "url": "http://arxiv.org/abs/2307.02968v4", "summary": "We present a simple semi-streaming algorithm for $(1-\\epsilon)$-approximation\nof bipartite matching in $O(\\log{\\!(n)}/\\epsilon)$ passes. This matches the\nperformance of state-of-the-art \"$\\epsilon$-efficient\" algorithms -- the ones\nwith much better dependence on $\\epsilon$ albeit with some mild dependence on\n$n$ -- while being considerably simpler.\n  The algorithm relies on a direct application of the multiplicative weight\nupdate method with a self-contained primal-dual analysis that can be of\nindependent interest. To show case this, we use the same ideas, alongside\nstandard tools from matching theory, to present an equally simple\nsemi-streaming algorithm for $(1-\\epsilon)$-approximation of weighted matchings\nin general (not necessarily bipartite) graphs, again in\n$O(\\log{\\!(n)}/\\epsilon)$ passes.", "comment": "25 pages. This is the TheoretiCS journal version", "pdf_url": "http://arxiv.org/pdf/2307.02968v4", "cate": "cs.DS", "date": "2023-07-06", "updated": "2025-07-31"}
{"id": "2507.00059", "title": "Computational Verification of the Buratti--Horak--Rosa Conjecture for Small Integers and Inductive Approaches", "authors": ["Ranjan N Naik"], "categories": ["cs.DM", "cs.DS", "math.CO"], "primary_category": "Subjects:       Discrete Mathematics (cs.DM)", "pdf_link": null, "comments": "Comments:      This result supports the results by Mariusz Meszka for all primes up to 23 (included) with the aid of a computer. Additional results on Coprime BHR Conjecture verifications for p < 31 and Inductive Approaches are included in this revision", "url": "http://arxiv.org/abs/2507.00059v4", "summary": "This paper presents a comprehensive computational approach to verify and\ninductively construct Hamiltonian paths for the Buratti--Horak--Rosa (BHR)\nConjecture. The conjecture posits that for any multiset $L$ of $p-1$ positive\nintegers not exceeding $\\lfloor p/2 \\rfloor$, there exists a Hamiltonian path\nin the complete graph $K_p$ with vertex-set $\\{0, 1, \\dots, p-1\\}$ whose edge\nlengths (under the cyclic metric) match $L$, if and only if for every divisor\n$d$ of $p$, the number of multiples of $d$ appearing in $L$ is at most $p - d$.\n  Building upon prior computational work by Mariusz Meszka, which verified the\nconjecture for all primes up to $p=23$, our Python program extends this\nverification significantly. We approach the problem by systematically\ngenerating frequency partitions (FPs) of edge lengths and employing a recursive\nbacktracking algorithm. We report successful computational verification for all\nfrequency partitions for integers $p < 32$, specifically presenting results for\n$p=31$ and a composite $p=26$. For the composite number $p=30$, the Python code\ntook approximately 11 hours to verify on a Lenovo laptop. For $p=16$, $167,898$\nvalid multisets were processed, taking around 20 hours on Google Colab Pro+.\n  Furthermore, we introduce and implement two constructive, inductive\nstrategies for building Hamiltonian paths: (1) increasing the multiplicity of\nan existing edge length, and (2) adding a new edge length. These methods,\nsupported by a reuse-insertion heuristic and backtracking search, demonstrate\nsuccessful constructions for evolving FPs up to $p=40$. Through these empirical\ntests and performance metrics, we provide strong computational evidence for the\nvalidity of the BHR conjecture within the scope tested, and outline the\nscalability of our approach for higher integer values.", "comment": "This result supports the results by Mariusz Meszka for all primes up\n  to 23 (included) with the aid of a computer. Additional results on Coprime\n  BHR Conjecture verifications for p < 31 and Inductive Approaches are included\n  in this revision", "pdf_url": "http://arxiv.org/pdf/2507.00059v4", "cate": "cs.DM", "date": "2025-06-26", "updated": "2025-07-31"}
{"id": "2507.23047", "title": "Competitive Bundle Trading", "authors": ["Yossi Azar", "Niv Buchbinder", "Roie Levin", "Or Vardi"], "categories": ["cs.DS", "cs.GT"], "primary_category": "Subjects:       Data Structures and Algorithms (cs.DS)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23047v1", "summary": "A retailer is purchasing goods in bundles from suppliers and then selling\nthese goods in bundles to customers; her goal is to maximize profit, which is\nthe revenue obtained from selling goods minus the cost of purchasing those\ngoods. In this paper, we study this general trading problem from the retailer's\nperspective, where both suppliers and customers arrive online. The retailer has\ninventory constraints on the number of goods from each type that she can store,\nand she must decide upon arrival of each supplier/customer which goods to\nbuy/sell in order to maximize profit.\n  We design an algorithm with logarithmic competitive ratio compared to an\noptimal offline solution. We achieve this via an exponential-weight-update\ndynamic pricing scheme, and our analysis dual fits the retailer's profit with\nrespect to a linear programming formulation upper bounding the optimal offline\nprofit. We prove (almost) matching lower bounds, and we also extend our result\nto an incentive compatible mechanism. Prior to our work, algorithms for trading\nbundles were known only for the special case of selling an initial inventory.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23047v1", "cate": "cs.DS", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.23010", "title": "Investigating the Invertibility of Multimodal Latent Spaces: Limitations of Optimization-Based Methods", "authors": ["Siwoo Park"], "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.SD", "eess.AS"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23010v1", "summary": "This paper investigates the inverse capabilities and broader utility of\nmultimodal latent spaces within task-specific AI (Artificial Intelligence)\nmodels. While these models excel at their designed forward tasks (e.g.,\ntext-to-image generation, audio-to-text transcription), their potential for\ninverse mappings remains largely unexplored. We propose an optimization-based\nframework to infer input characteristics from desired outputs, applying it\nbidirectionally across Text-Image (BLIP, Flux.1-dev) and Text-Audio\n(Whisper-Large-V3, Chatterbox-TTS) modalities.\n  Our central hypothesis posits that while optimization can guide models\ntowards inverse tasks, their multimodal latent spaces will not consistently\nsupport semantically meaningful and perceptually coherent inverse mappings.\nExperimental results consistently validate this hypothesis. We demonstrate that\nwhile optimization can force models to produce outputs that align textually\nwith targets (e.g., a text-to-image model generating an image that an image\ncaptioning model describes correctly, or an ASR model transcribing optimized\naudio accurately), the perceptual quality of these inversions is chaotic and\nincoherent. Furthermore, when attempting to infer the original semantic input\nfrom generative models, the reconstructed latent space embeddings frequently\nlack semantic interpretability, aligning with nonsensical vocabulary tokens.\n  These findings highlight a critical limitation. multimodal latent spaces,\nprimarily optimized for specific forward tasks, do not inherently possess the\nstructure required for robust and interpretable inverse mappings. Our work\nunderscores the need for further research into developing truly semantically\nrich and invertible multimodal latent spaces.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23010v1", "cate": "cs.LG", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.23083", "title": "Context-aware Rotary Position Embedding", "authors": ["Ali Veisi", "Delaram Fartoot", "Hamidreza Amirzadeh"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      4 pages, 1 table", "url": "http://arxiv.org/abs/2507.23083v1", "summary": "Positional encoding is a vital component of Transformer architectures,\nenabling models to incorporate sequence order into self-attention mechanisms.\nRotary Positional Embeddings (RoPE) have become a widely adopted solution due\nto their compatibility with relative position encoding and computational\nefficiency. However, RoPE relies on static, input-independent sinusoidal\nfrequency patterns, limiting its ability to model context-sensitive\nrelationships. In this work, we propose CARoPE (Context-Aware Rotary Positional\nEmbedding), a novel generalization of RoPE that dynamically generates\nhead-specific frequency patterns conditioned on token embeddings. This design\nintroduces token- and context-sensitive positional representations while\npreserving RoPE efficiency and architectural simplicity. CARoPE computes\ninput-dependent phase shifts using a bounded transformation of token embeddings\nand integrates them into the rotary mechanism across attention heads. We\nevaluate CARoPE on the FineWeb-Edu-10B dataset using GPT-2 variants trained on\nnext-token prediction tasks. Experimental results show that CARoPE consistently\noutperforms RoPE and other common positional encoding baselines, achieving\nsignificantly lower perplexity, even at longer context lengths. Additionally,\nCARoPE enables faster training throughput without sacrificing model stability.\nThese findings demonstrate that CARoPE offers a scalable, expressive, and\nefficient upgrade to existing positional encoding strategies in Transformer\nmodels.", "comment": "4 pages, 1 table", "pdf_url": "http://arxiv.org/pdf/2507.23083v1", "cate": "cs.CL", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2505.02851", "title": "Leveraging LLMs to Create Content Corpora for Niche Domains", "authors": ["Franklin Zhang", "Sonya Zhang", "Alon Halevy"], "categories": ["cs.CL", "cs.AI", "cs.CY", "I.2.7; H.3.1; H.3.3"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      9 pages (main content), 5 figures. Supplementary materials can be found at this https URL", "url": "http://arxiv.org/abs/2505.02851v2", "summary": "Constructing specialized content corpora from vast, unstructured web sources\nfor domain-specific applications poses substantial data curation challenges. In\nthis paper, we introduce a streamlined approach for generating high-quality,\ndomain-specific corpora by efficiently acquiring, filtering, structuring, and\ncleaning web-based data. We showcase how Large Language Models (LLMs) can be\nleveraged to address complex data curation at scale, and propose a strategical\nframework incorporating LLM-enhanced techniques for structured content\nextraction and semantic deduplication. We validate our approach in the behavior\neducation domain through its integration into 30 Day Me, a habit formation\napplication. Our data pipeline, named 30DayGen, enabled the extraction and\nsynthesis of 3,531 unique 30-day challenges from over 15K webpages. A user\nsurvey reports a satisfaction score of 4.3 out of 5, with 91% of respondents\nindicating willingness to use the curated content for their habit-formation\ngoals.", "comment": "9 pages (main content), 5 figures. Supplementary materials can be\n  found at https://github.com/pigfyy/30DayGen-Supplementary-Materials", "pdf_url": "http://arxiv.org/pdf/2505.02851v2", "cate": "cs.CL", "date": "2025-05-02", "updated": "2025-07-31"}
{"id": "2507.23143", "title": "X-NeMo: Expressive Neural Motion Reenactment via Disentangled Latent Attention", "authors": ["Xiaochen Zhao", "Hongyi Xu", "Guoxian Song", "You Xie", "Chenxu Zhang", "Xiu Li", "Linjie Luo", "Jinli Suo", "Yebin Liu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICLR 2025, code is available at this https URL", "url": "http://arxiv.org/abs/2507.23143v1", "summary": "We propose X-NeMo, a novel zero-shot diffusion-based portrait animation\npipeline that animates a static portrait using facial movements from a driving\nvideo of a different individual. Our work first identifies the root causes of\nthe key issues in prior approaches, such as identity leakage and difficulty in\ncapturing subtle and extreme expressions. To address these challenges, we\nintroduce a fully end-to-end training framework that distills a 1D\nidentity-agnostic latent motion descriptor from driving image, effectively\ncontrolling motion through cross-attention during image generation. Our\nimplicit motion descriptor captures expressive facial motion in fine detail,\nlearned end-to-end from a diverse video dataset without reliance on pretrained\nmotion detectors. We further enhance expressiveness and disentangle motion\nlatents from identity cues by supervising their learning with a dual GAN\ndecoder, alongside spatial and color augmentations. By embedding the driving\nmotion into a 1D latent vector and controlling motion via cross-attention\nrather than additive spatial guidance, our design eliminates the transmission\nof spatial-aligned structural clues from the driving condition to the diffusion\nbackbone, substantially mitigating identity leakage. Extensive experiments\ndemonstrate that X-NeMo surpasses state-of-the-art baselines, producing highly\nexpressive animations with superior identity resemblance. Our code and models\nare available for research.", "comment": "ICLR 2025, code is available at\n  https://github.com/bytedance/x-nemo-inference", "pdf_url": "http://arxiv.org/pdf/2507.23143v1", "cate": "cs.CV", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2407.15738", "title": "Parallel Split Learning with Global Sampling", "authors": ["Mohammad Kohankhaki", "Ahmad Ayad", "Mahdi Barhoush", "Anke Schmeink"], "categories": ["cs.LG", "cs.AI", "cs.DC"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2407.15738v4", "summary": "Distributed deep learning in resource-constrained environments faces\nscalability and generalization challenges due to large effective batch sizes\nand non-identically distributed client data. We introduce a server-driven\nsampling strategy that maintains a fixed global batch size by dynamically\nadjusting client-side batch sizes. This decouples the effective batch size from\nthe number of participating devices and ensures that global batches better\nreflect the overall data distribution. Using standard concentration bounds, we\nestablish tighter deviation guarantees compared to existing approaches.\nEmpirical results on a benchmark dataset confirm that the proposed method\nimproves model accuracy, training efficiency, and convergence stability,\noffering a scalable solution for learning at the network edge.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2407.15738v4", "cate": "cs.LG", "date": "2024-07-22", "updated": "2025-07-31"}
{"id": "2409.17092", "title": "Accumulator-Aware Post-Training Quantization for Large Language Models", "authors": ["Ian Colbert", "Giuseppe Franco", "Fabian Grob", "Jinjie Zhang", "Rayan Saab"], "categories": ["cs.LG", "cs.AI", "cs.DM"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2409.17092v2", "summary": "When quantizing weights and activations to increasingly narrower\nrepresentations, the cost of additions begins to dominate that of\nmultiplications in multiply-accumulate (MAC) units. Recent studies show that\nreducing addition costs via low-precision accumulation improves throughput,\npower, and area across inference platforms, albeit with an increased risk of\noverflow. Accumulator-aware quantization research has so far only considered\nthe quantization-aware training (QAT) paradigm, in which models are fine-tuned\nor trained from scratch with quantization in the loop. As models and datasets\ncontinue to grow in size, QAT techniques become increasingly more expensive,\nwhich has motivated the recent surge in post-training quantization (PTQ)\nresearch. To bridge this gap, we introduce AXE, the first accumulator-aware\nquantization framework explicitly designed to endow overflow avoidance\nguarantees to PTQ algorithms. We present theoretical motivation for AXE and\ndemonstrate its flexibility by implementing it on top of two existing\nalgorithms: GPFQ and OPTQ. We design AXE to support multi-stage accumulation,\nopening the door to full datapath optimization for the first time. We evaluate\nAXE using recent language generation models; when quantizing Llama3 8B for a\n16-bit multi-stage accumulation datapath, AXE maintains up to 98% of the FP16\nperplexity, surpassing naive bit width manipulation by up to 15%.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2409.17092v2", "cate": "cs.LG", "date": "2024-09-25", "updated": "2025-07-31"}
{"id": "2507.23216", "title": "Efficient algorithm for linear diophantine equations in two variables", "authors": ["Mayank Deora", "Pinakpani Pal"], "categories": ["cs.DS"], "primary_category": "Subjects:       Data Structures and Algorithms (cs.DS)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23216v1", "summary": "Solving linear diophantine equations in two variables have applications in\ncomputer science and mathematics. In this paper, we revisit an algorithm for\nsolving linear diophantine equations in two variables, which we refer as DEA-R\nalgorithm. The DEA-R algorithm always incurs equal or less number of recursions\nor recursive calls as compared to extended euclidean algorithm. With the\nobjective of taking advantage of the less number of recursive calls , we\npropose an optimized version of the DEA-R algorithm as DEA-OPTD. In the\nrecursive function calls in DEA-OPTD, we propose a sequence of more efficient\ncomputations. We do a theoretical comparison of the execution times of DEA-OPTD\nalgorithm and DEA-R algorithm to find any possible bound on the value of $c$\nfor DEA-OPTD being better than DEA-R. We implement and compare an iterative\nversion of DEA-OPTD (DEA-OPTDI) with two versions of a widely used algorithm on\nan specific input setting. In this comparison, we find out that our algorithm\noutperforms on the other algorithm against atleast 96% of the inputs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23216v1", "cate": "cs.DS", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23087", "title": "On LLM-Assisted Generation of Smart Contracts from Business Processes", "authors": ["Fabian Stiehle", "Hans Weytjens", "Ingo Weber"], "categories": ["cs.SE", "cs.AI"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      Accepted at the Workshop on Distributed Ledger Technologies in Business Process Management, At the International Conference for Business Process Management (BPM), 2025", "url": "http://arxiv.org/abs/2507.23087v1", "summary": "Large language models (LLMs) have changed the reality of how software is\nproduced. Within the wider software engineering community, among many other\npurposes, they are explored for code generation use cases from different types\nof input. In this work, we present an exploratory study to investigate the use\nof LLMs for generating smart contract code from business process descriptions,\nan idea that has emerged in recent literature to overcome the limitations of\ntraditional rule-based code generation approaches. However, current LLM-based\nwork evaluates generated code on small samples, relying on manual inspection,\nor testing whether code compiles but ignoring correct execution. With this\nwork, we introduce an automated evaluation framework and provide empirical data\nfrom larger data sets of process models. We test LLMs of different types and\nsizes in their capabilities of achieving important properties of process\nexecution, including enforcing process flow, resource allocation, and\ndata-based conditions. Our results show that LLM performance falls short of the\nperfect reliability required for smart contract development. We suggest future\nwork to explore responsible LLM integrations in existing tools for code\ngeneration to ensure more reliable output. Our benchmarking framework can serve\nas a foundation for developing and evaluating such integrations.", "comment": "Accepted at the Workshop on Distributed Ledger Technologies in\n  Business Process Management, At the International Conference for Business\n  Process Management (BPM), 2025", "pdf_url": "http://arxiv.org/pdf/2507.23087v1", "cate": "cs.SE", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.23095", "title": "SMART-Editor: A Multi-Agent Framework for Human-Like Design Editing with Structural Integrity", "authors": ["Ishani Mondal", "Meera Bharadwaj", "Ayush Roy", "Aparna Garimella", "Jordan Lee Boyd-Graber"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Under Submission", "url": "http://arxiv.org/abs/2507.23095v1", "summary": "We present SMART-Editor, a framework for compositional layout and content\nediting across structured (posters, websites) and unstructured (natural images)\ndomains. Unlike prior models that perform local edits, SMART-Editor preserves\nglobal coherence through two strategies: Reward-Refine, an inference-time\nrewardguided refinement method, and RewardDPO, a training-time preference\noptimization approach using reward-aligned layout pairs. To evaluate model\nperformance, we introduce SMARTEdit-Bench, a benchmark covering multi-domain,\ncascading edit scenarios. SMART-Editor outperforms strong baselines like\nInstructPix2Pix and HIVE, with RewardDPO achieving up to 15% gains in\nstructured settings and Reward-Refine showing advantages on natural images.\nAutomatic and human evaluations confirm the value of reward-guided planning in\nproducing semantically consistent and visually aligned edits.", "comment": "Under Submission", "pdf_url": "http://arxiv.org/pdf/2507.23095v1", "cate": "cs.CL", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2506.18199", "title": "Prompt Engineering Techniques for Mitigating Cultural Bias Against Arabs and Muslims in Large Language Models: A Systematic Review", "authors": ["Bushra Asseri", "Estabrag Abdelaziz", "Areej Al-Wabil"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Research is incomplete", "url": "http://arxiv.org/abs/2506.18199v2", "summary": "Large language models have demonstrated remarkable capabilities across\nvarious domains, yet concerns about cultural bias - particularly towards Arabs\nand Muslims - pose significant ethical challenges by perpetuating harmful\nstereotypes and marginalization. Despite growing recognition of bias in LLMs,\nprompt engineering strategies specifically addressing Arab and Muslim\nrepresentation remain understudied. This mixed-methods systematic review\nexamines such techniques, offering evidence-based guidance for researchers and\npractitioners. Following PRISMA guidelines and Kitchenham's systematic review\nmethodology, we analyzed 8 empirical studies published between 2021-2024\ninvestigating bias mitigation strategies. Our findings reveal five primary\nprompt engineering approaches: cultural prompting, affective priming,\nself-debiasing techniques, structured multi-step pipelines, and\nparameter-optimized continuous prompts. Although all approaches show potential\nfor reducing bias, effectiveness varied substantially across studies and bias\ntypes. Evidence suggests that certain bias types may be more resistant to\nprompt-based mitigation than others. Structured multi-step pipelines\ndemonstrated the highest overall effectiveness, achieving up to 87.7% reduction\nin bias, though they require greater technical expertise. Cultural prompting\noffers broader accessibility with substantial effectiveness. These results\nunderscore the accessibility of prompt engineering for mitigating cultural bias\nwithout requiring access to model parameters. The limited number of studies\nidentified highlights a significant research gap in this critical area. Future\nresearch should focus on developing culturally adaptive prompting techniques,\ncreating Arab and Muslim-specific evaluation resources, and integrating prompt\nengineering with complementary debiasing methods to address deeper stereotypes\nwhile maintaining model utility.", "comment": "Research is incomplete", "pdf_url": "http://arxiv.org/pdf/2506.18199v2", "cate": "cs.CL", "date": "2025-06-22", "updated": "2025-07-30"}
{"id": "2507.23150", "title": "Towards High-Resolution Alignment and Super-Resolution of Multi-Sensor Satellite Imagery", "authors": ["Philip Wootaek Shin", "Vishal Gaur", "Rahul Ramachandran", "Manil Maskey", "Jack Sampson", "Vijaykrishnan Narayanan", "Sujit Roy"], "categories": ["eess.IV", "cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23150v1", "summary": "High-resolution satellite imagery is essential for geospatial analysis, yet\ndifferences in spatial resolution across satellite sensors present challenges\nfor data fusion and downstream applications. Super-resolution techniques can\nhelp bridge this gap, but existing methods rely on artificially downscaled\nimages rather than real sensor data and are not well suited for heterogeneous\nsatellite sensors with differing spectral, temporal characteristics. In this\nwork, we develop a preliminary framework to align and Harmonized Landsat\nSentinel 30m(HLS 30) imagery using Harmonized Landsat Sentinel 10m(HLS10) as a\nreference from the HLS dataset. Our approach aims to bridge the resolution gap\nbetween these sensors and improve the quality of super-resolved Landsat\nimagery. Quantitative and qualitative evaluations demonstrate the effectiveness\nof our method, showing its potential for enhancing satellite-based sensing\napplications. This study provides insights into the feasibility of\nheterogeneous satellite image super-resolution and highlights key\nconsiderations for future advancements in the field.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23150v1", "cate": "eess.IV", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2504.10403", "title": "Satellite Federated Fine-Tuning for Foundation Models in Space Computing Power Networks", "authors": ["Yan Zhu", "Jingyang Zhu", "Ting Wang", "Yuanming Shi", "Chunxiao Jiang", "Khaled Ben Letaief"], "categories": ["cs.LG", "cs.DC", "cs.NI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.10403v3", "summary": "Advancements in artificial intelligence (AI) and low-earth orbit (LEO)\nsatellites have promoted the application of large remote sensing foundation\nmodels for various downstream tasks. However, direct downloading of these\nmodels for fine-tuning on the ground is impeded by privacy concerns and limited\nbandwidth. Satellite federated learning (FL) offers a solution by enabling\nmodel fine-tuning directly on-board satellites and aggregating model updates\nwithout data downloading. Nevertheless, for large foundation models, the\ncomputational capacity of satellites is insufficient to support effective\non-board fine-tuning in traditional satellite FL frameworks. To address these\nchallenges, we propose a satellite-ground collaborative federated fine-tuning\nframework. The key of the framework lies in how to reasonably decompose and\nallocate model components to alleviate insufficient on-board computation\ncapabilities. During fine-tuning, satellites exchange intermediate results with\nground stations or other satellites for forward propagation and back\npropagation, which brings communication challenges due to the special\ncommunication topology of space transmission networks, such as intermittent\nsatellite-ground communication, short duration of satellite-ground\ncommunication windows, and unstable inter-orbit inter-satellite links (ISLs).\nTo reduce transmission delays, we further introduce tailored communication\nstrategies that integrate both communication and computing resources.\nSpecifically, we propose a parallel intra-orbit communication strategy, a\ntopology-aware satellite-ground communication strategy, and a\nlatency-minimalization inter-orbit communication strategy to reduce space\ncommunication costs. Simulation results demonstrate significant reductions in\ntraining time with improvements of approximately 33%.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.10403v3", "cate": "cs.LG", "date": "2025-04-14", "updated": "2025-07-31"}
{"id": "2412.20300", "title": "A note on the structure of locally finite planar quasi-transitive graphs", "authors": ["Ugo Giocanti"], "categories": ["math.CO", "cs.DM", "05C10 (Primary), 05C63 (Secondary), 05C75 (Secondary), 68R10\n  (Secondary)", "G.2.2"], "primary_category": "Subjects:       Combinatorics (math.CO)", "pdf_link": null, "comments": "Comments:      16 pages, 4 figures", "url": "http://arxiv.org/abs/2412.20300v2", "summary": "In an early work from 1896, Maschke established the complete list of all\nfinite planar Cayley graphs. This result initiated a long line of research over\nthe next century, aiming at characterizing in a similar way all planar infinite\nCayley graphs. Droms (2006) proved a structure theorem for finitely generated\nplanar groups, i.e., finitely generated groups admitting a planar Cayley graph,\nin terms of Bass-Serre decompositions. As a byproduct of his structure theorem,\nDroms proved that such groups are finitely presented. More recently, Hamann\n(2018) gave a graph theoretical proof that every planar quasi-transitive graph\n$G$ admits a generating $\\mathrm{Aut}(G)$-invariant set of closed walks with\nonly finitely many orbits, and showed that a consequence is an alternative\nproof of Droms' result. Based on the work of Hamann, we show in this note that\nwe can also obtain a general structure theorem for $3$-connected locally finite\nplanar quasi-transitive graphs, namely that every such graph admits a canonical\ntree-decomposition whose edge-separations correspond to cycle-separations in\nthe (unique) embedding of $G$, and in which every part admits a\nvertex-accumulation free embedding. This result can be seen as a version of\nDroms' structure theorem for quasi-transitive planar graphs. As a corollary, we\nobtain an alternative proof of a result of Hamann, Lehner, Miraftab and\nR\\\"uhmann (2022) that every locally finite quasi-transitive planar graph admits\na canonical tree-decomposition, whose parts are either $1$-ended or finite\nplanar graphs.", "comment": "16 pages, 4 figures", "pdf_url": "http://arxiv.org/pdf/2412.20300v2", "cate": "math.CO", "date": "2024-12-28", "updated": "2025-07-30"}
{"id": "2507.23500", "title": "Online Combinatorial Allocation with Interdependent Values", "authors": ["Michal Feldman", "Simon Mauras", "Divyarthi Mohan", "Rebecca Reiffenhäuser"], "categories": ["cs.GT", "cs.DS"], "primary_category": "Subjects:       Computer Science and Game Theory (cs.GT)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23500v1", "summary": "We study online combinatorial allocation problems in the secretary setting,\nunder interdependent values. In the interdependent model, introduced by Milgrom\nand Weber (1982), each agent possesses a private signal that captures her\ninformation about an item for sale, and the value of every agent depends on the\nsignals held by all agents. Mauras, Mohan, and Reiffenh\\\"auser (2024) were the\nfirst to study interdependent values in online settings, providing\nconstant-approximation guarantees for secretary settings, where agents arrive\nonline along with their signals and values, and the goal is to select the agent\nwith the highest value.\n  In this work, we extend this framework to {\\em combinatorial} secretary\nproblems, where agents have interdependent valuations over {\\em bundles} of\nitems, introducing additional challenges due to both combinatorial structure\nand interdependence. We provide $2e$-competitive algorithms for a broad class\nof valuation functions, including submodular and XOS functions, matching the\napproximation guarantees in the single-choice secretary setting. Furthermore,\nour results cover the same range of valuation classes for which constant-factor\nalgorithms exist in classical (non-interdependent) secretary settings, while\nincurring only an additional factor of $2$ due to interdependence. Finally, we\nextend our study to strategic settings, and provide a $4e$-competitive truthful\nmechanism for online bipartite matching with interdependent valuations, again\nmeeting the frontier of what is known, even without interdependence.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23500v1", "cate": "cs.GT", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23088", "title": "Beyond Rigid AI: Towards Natural Human-Machine Symbiosis for Interoperative Surgical Assistance", "authors": ["Lalithkumar Seenivasan", "Jiru Xu", "Roger D. Soberanis Mukul", "Hao Ding", "Grayson Byrd", "Yu-Chun Ku", "Jose L. Porras", "Masaru Ishii", "Mathias Unberath"], "categories": ["cs.RO", "cs.AI", "cs.HC"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23088v1", "summary": "Emerging surgical data science and robotics solutions, especially those\ndesigned to provide assistance in situ, require natural human-machine\ninterfaces to fully unlock their potential in providing adaptive and intuitive\naid. Contemporary AI-driven solutions remain inherently rigid, offering limited\nflexibility and restricting natural human-machine interaction in dynamic\nsurgical environments. These solutions rely heavily on extensive task-specific\npre-training, fixed object categories, and explicit manual-prompting. This work\nintroduces a novel Perception Agent that leverages speech-integrated\nprompt-engineered large language models (LLMs), segment anything model (SAM),\nand any-point tracking foundation models to enable a more natural human-machine\ninteraction in real-time intraoperative surgical assistance. Incorporating a\nmemory repository and two novel mechanisms for segmenting unseen elements,\nPerception Agent offers the flexibility to segment both known and unseen\nelements in the surgical scene through intuitive interaction. Incorporating the\nability to memorize novel elements for use in future surgeries, this work takes\na marked step towards human-machine symbiosis in surgical procedures. Through\nquantitative analysis on a public dataset, we show that the performance of our\nagent is on par with considerably more labor-intensive manual-prompting\nstrategies. Qualitatively, we show the flexibility of our agent in segmenting\nnovel elements (instruments, phantom grafts, and gauze) in a custom-curated\ndataset. By offering natural human-machine interaction and overcoming rigidity,\nour Perception Agent potentially brings AI-based real-time assistance in\ndynamic surgical environments closer to reality.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23088v1", "cate": "cs.RO", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.23104", "title": "RASL: Retrieval Augmented Schema Linking for Massive Database Text-to-SQL", "authors": ["Jeffrey Eben", "Aitzaz Ahmad", "Stephen Lau"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23104v1", "summary": "Despite advances in large language model (LLM)-based natural language\ninterfaces for databases, scaling to enterprise-level data catalogs remains an\nunder-explored challenge. Prior works addressing this challenge rely on\ndomain-specific fine-tuning - complicating deployment - and fail to leverage\nimportant semantic context contained within database metadata. To address these\nlimitations, we introduce a component-based retrieval architecture that\ndecomposes database schemas and metadata into discrete semantic units, each\nseparately indexed for targeted retrieval. Our approach prioritizes effective\ntable identification while leveraging column-level information, ensuring the\ntotal number of retrieved tables remains within a manageable context budget.\nExperiments demonstrate that our method maintains high recall and accuracy,\nwith our system outperforming baselines over massive databases with varying\nstructure and available metadata. Our solution enables practical text-to-SQL\nsystems deployable across diverse enterprise settings without specialized\nfine-tuning, addressing a critical scalability gap in natural language database\ninterfaces.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23104v1", "cate": "cs.CL", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.12103", "title": "DeepShade: Enable Shade Simulation by Text-conditioned Image Generation", "authors": ["Longchao Da", "Xiangrui Liu", "Mithun Shivakoti", "Thirulogasankar Pranav Kutralingam", "Yezhou Yang", "Hua Wei"], "categories": ["cs.CV", "cs.CY", "68T45, 68U10, 62H35", "I.2.10; I.4.8; I.5.1"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      7pages, 4 figures", "url": "http://arxiv.org/abs/2507.12103v3", "summary": "Heatwaves pose a significant threat to public health, especially as global\nwarming intensifies. However, current routing systems (e.g., online maps) fail\nto incorporate shade information due to the difficulty of estimating shades\ndirectly from noisy satellite imagery and the limited availability of training\ndata for generative models. In this paper, we address these challenges through\ntwo main contributions. First, we build an extensive dataset covering diverse\nlongitude-latitude regions, varying levels of building density, and different\nurban layouts. Leveraging Blender-based 3D simulations alongside building\noutlines, we capture building shadows under various solar zenith angles\nthroughout the year and at different times of day. These simulated shadows are\naligned with satellite images, providing a rich resource for learning shade\npatterns. Second, we propose the DeepShade, a diffusion-based model designed to\nlearn and synthesize shade variations over time. It emphasizes the nuance of\nedge features by jointly considering RGB with the Canny edge layer, and\nincorporates contrastive learning to capture the temporal change rules of\nshade. Then, by conditioning on textual descriptions of known conditions (e.g.,\ntime of day, solar angles), our framework provides improved performance in\ngenerating shade images. We demonstrate the utility of our approach by using\nour shade predictions to calculate shade ratios for real-world route planning\nin Tempe, Arizona. We believe this work will benefit society by providing a\nreference for urban planning in extreme heat weather and its potential\npractical applications in the environment.", "comment": "7pages, 4 figures", "pdf_url": "http://arxiv.org/pdf/2507.12103v3", "cate": "cs.CV", "date": "2025-07-16", "updated": "2025-07-30"}
{"id": "2507.23162", "title": "Neural Multi-View Self-Calibrated Photometric Stereo without Photometric Stereo Cues", "authors": ["Xu Cao", "Takafumi Taketomi"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted to ICCV 2025", "url": "http://arxiv.org/abs/2507.23162v1", "summary": "We propose a neural inverse rendering approach that jointly reconstructs\ngeometry, spatially varying reflectance, and lighting conditions from\nmulti-view images captured under varying directional lighting. Unlike prior\nmulti-view photometric stereo methods that require light calibration or\nintermediate cues such as per-view normal maps, our method jointly optimizes\nall scene parameters from raw images in a single stage. We represent both\ngeometry and reflectance as neural implicit fields and apply shadow-aware\nvolume rendering. A spatial network first predicts the signed distance and a\nreflectance latent code for each scene point. A reflectance network then\nestimates reflectance values conditioned on the latent code and angularly\nencoded surface normal, view, and light directions. The proposed method\noutperforms state-of-the-art normal-guided approaches in shape and lighting\nestimation accuracy, generalizes to view-unaligned multi-light images, and\nhandles objects with challenging geometry and reflectance.", "comment": "Accepted to ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.23162v1", "cate": "cs.CV", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.23539", "title": "Improved Algorithms for Kernel Matrix-Vector Multiplication Under Sparsity Assumptions", "authors": ["Piotr Indyk", "Michael Kapralov", "Kshiteej Sheth", "Tal Wagner"], "categories": ["cs.LG", "cs.DS"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Published in ICLR 2025", "url": "http://arxiv.org/abs/2507.23539v1", "summary": "Motivated by the problem of fast processing of attention matrices, we study\nfast algorithms for computing matrix-vector products for asymmetric Gaussian\nKernel matrices $K\\in \\mathbb{R}^{n\\times n}$. $K$'s columns are indexed by a\nset of $n$ keys $k_1,k_2\\ldots, k_n\\in \\mathbb{R}^d$, rows by a set of $n$\nqueries $q_1,q_2,\\ldots,q_n\\in \\mathbb{R}^d $, and its $i,j$ entry is $K_{ij} =\ne^{-\\|q_i-k_j\\|_2^2/2\\sigma^2}$ for some bandwidth parameter $\\sigma>0$. Given\na vector $x\\in \\mathbb{R}^n$ and error parameter $\\epsilon>0$, our task is to\noutput a $y\\in \\mathbb{R}^n$ such that $\\|Kx-y\\|_2\\leq \\epsilon \\|x\\|_2$ in\ntime subquadratic in $n$ and linear in $d$. Our algorithms rely on the\nfollowing modelling assumption about the matrices $K$: the sum of the entries\nof $K$ scales linearly in $n$, as opposed to worst case quadratic growth. We\nvalidate this assumption experimentally, for Gaussian kernel matrices\nencountered in various settings such as fast attention computation in LLMs. We\nobtain the first subquadratic-time algorithm that works under this assumption,\nfor unrestricted vectors.", "comment": "Published in ICLR 2025", "pdf_url": "http://arxiv.org/pdf/2507.23539v1", "cate": "cs.LG", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23093", "title": "On the Sustainability of AI Inferences in the Edge", "authors": ["Ghazal Sobhani", "Md. Monzurul Amin Ifath", "Tushar Sharma", "Israat Haque"], "categories": ["cs.LG", "cs.AI", "cs.PF"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      14 pages, 8 figures, 6 tables, in preparation for journal submission", "url": "http://arxiv.org/abs/2507.23093v1", "summary": "The proliferation of the Internet of Things (IoT) and its cutting-edge\nAI-enabled applications (e.g., autonomous vehicles and smart industries)\ncombine two paradigms: data-driven systems and their deployment on the edge.\nUsually, edge devices perform inferences to support latency-critical\napplications. In addition to the performance of these resource-constrained edge\ndevices, their energy usage is a critical factor in adopting and deploying edge\napplications. Examples of such devices include Raspberry Pi (RPi), Intel Neural\nCompute Stick (INCS), NVIDIA Jetson nano (NJn), and Google Coral USB (GCU).\nDespite their adoption in edge deployment for AI inferences, there is no study\non their performance and energy usage for informed decision-making on the\ndevice and model selection to meet the demands of applications. This study\nfills the gap by rigorously characterizing the performance of traditional,\nneural networks, and large language models on the above-edge devices.\nSpecifically, we analyze trade-offs among model F1 score, inference time,\ninference power, and memory usage. Hardware and framework optimization, along\nwith external parameter tuning of AI models, can balance between model\nperformance and resource usage to realize practical edge AI deployments.", "comment": "14 pages, 8 figures, 6 tables, in preparation for journal submission", "pdf_url": "http://arxiv.org/pdf/2507.23093v1", "cate": "cs.LG", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.23121", "title": "Uncovering the Fragility of Trustworthy LLMs through Chinese Textual Ambiguity", "authors": ["Xinwei Wu", "Haojie Li", "Hongyu Liu", "Xinyu Ji", "Ruohan Li", "Yule Chen", "Yigeng Zhang"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Accepted at KDD workshop on Evaluation and Trustworthiness of Agentic and Generative AI Models (Agentic & GenAI Evaluation Workshop KDD '25)", "url": "http://arxiv.org/abs/2507.23121v1", "summary": "In this work, we study a critical research problem regarding the\ntrustworthiness of large language models (LLMs): how LLMs behave when\nencountering ambiguous narrative text, with a particular focus on Chinese\ntextual ambiguity. We created a benchmark dataset by collecting and generating\nambiguous sentences with context and their corresponding disambiguated pairs,\nrepresenting multiple possible interpretations. These annotated examples are\nsystematically categorized into 3 main categories and 9 subcategories. Through\nexperiments, we discovered significant fragility in LLMs when handling\nambiguity, revealing behavior that differs substantially from humans.\nSpecifically, LLMs cannot reliably distinguish ambiguous text from unambiguous\ntext, show overconfidence in interpreting ambiguous text as having a single\nmeaning rather than multiple meanings, and exhibit overthinking when attempting\nto understand the various possible meanings. Our findings highlight a\nfundamental limitation in current LLMs that has significant implications for\ntheir deployment in real-world applications where linguistic ambiguity is\ncommon, calling for improved approaches to handle uncertainty in language\nunderstanding. The dataset and code are publicly available at this GitHub\nrepository: https://github.com/ictup/LLM-Chinese-Textual-Disambiguation.", "comment": "Accepted at KDD workshop on Evaluation and Trustworthiness of Agentic\n  and Generative AI Models (Agentic & GenAI Evaluation Workshop KDD '25)", "pdf_url": "http://arxiv.org/pdf/2507.23121v1", "cate": "cs.CL", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.23174", "title": "CNN-based solution for mango classification in agricultural environments", "authors": ["Beatriz Díaz Peón", "Jorge Torres Gómez", "Ariel Fajardo Márquez"], "categories": ["cs.CV", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23174v1", "summary": "This article exemplifies the design of a fruit detection and classification\nsystem using Convolutional\n  Neural Networks (CNN). The goal is to develop a system that automatically\nassesses fruit quality for\n  farm inventory management. Specifically, a method for mango fruit\nclassification was developed using\n  image processing, ensuring both accuracy and efficiency. Resnet-18 was\nselected as the preliminary\n  architecture for classification, while a cascade detector was used for\ndetection, balancing execution speed\n  and computational resource consumption. Detection and classification results\nwere displayed through a\n  graphical interface developed in MatLab App Designer, streamlining system\ninteraction. The integration\n  of convolutional neural networks and cascade detectors proffers a reliable\nsolution for fruit classification\n  and detection, with potential applications in agricultural quality control.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23174v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.17170", "title": "Advancing Quantum State Preparation Using Decision Diagram with Local Invertible Maps", "authors": ["Xin Hong", "Aochu Dai", "Chenjian Li", "Sanjiang Li", "Shenggang Ying", "Mingsheng Ying"], "categories": ["cs.DS", "quant-ph"], "primary_category": "Subjects:       Data Structures and Algorithms (cs.DS)", "pdf_link": null, "comments": "Comments:      arXiv admin note: text overlap with arXiv:2507.14496", "url": "http://arxiv.org/abs/2507.17170v2", "summary": "Quantum state preparation (QSP) is a fundamental task in quantum computing\nand quantum information processing. It is critical to the execution of many\nquantum algorithms, including those in quantum machine learning. In this paper,\nwe propose a family of efficient QSP algorithms tailored to different numbers\nof available ancilla qubits - ranging from no ancilla qubits, to a single\nancilla qubit, to a sufficiently large number of ancilla qubits. Our approach\nexploits the power of Local Invertible Map Tensor Decision Diagrams (LimTDDs) -\na highly compact representation of quantum states that combines tensor networks\nand decision diagrams to reduce quantum circuit complexity. Extensive\nexperiments demonstrate that our methods significantly outperform existing\napproaches and exhibit better scalability for large-scale quantum states, both\nin terms of runtime and gate complexity. Furthermore, our method shows\nexponential improvement in best-case scenarios.", "comment": "arXiv admin note: text overlap with arXiv:2507.14496", "pdf_url": "http://arxiv.org/pdf/2507.17170v2", "cate": "cs.DS", "date": "2025-07-23", "updated": "2025-07-31"}
{"id": "2507.23419", "title": "WiRM: Wireless Respiration Monitoring Using Conjugate Multiple Channel State Information and Fast Iterative Filtering in Wi-Fi Systems", "authors": ["James Rhodes", "Lawrence Ong", "Duy T. Ngo"], "categories": ["cs.ET", "eess.SP"], "primary_category": "Subjects:       Emerging Technologies (cs.ET)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23419v1", "summary": "Monitoring respiratory health with the use of channel state information (CSI)\nhas shown promising results. Many existing methods focus on monitoring only the\nrespiratory rate, while others focus on monitoring the motion of the chest as a\npatient breathes, which is referred to as the respiratory waveform. This paper\npresents WiRM, a two-staged approach to contactless respiration monitoring. In\nthe first stage, WiRM improves upon existing respiratory rate estimation\ntechniques by using conjugate multiplication for phase sanitisation and\nadaptive multi-trace carving (AMTC) for tracing how the respiratory rate\nchanges over time. When compared against three state-of-the-art methods, WiRM\nhas achieved an average reduction of $38\\%$ in respiratory rate root mean\nsquared error (RMSE). In the second stage, WiRM uses this improved respiratory\nrate estimate to inform the decomposition and selection of the respiratory\nwaveform from the CSI data. Remarkably, WiRM delivers a $178.3\\%$ improvement\nin average absolute correlation with the ground truth respiratory waveform.\nWithin the literature, it is difficult to compare the robustness of existing\nalgorithms in noisy environments. In this paper, we develop a purpose-built\nsimulation toolkit to evaluate the robustness of respiration monitoring\nsolutions under various noise conditions, including thermal, multiplicative,\nand phase noise. Our results show that WiRM demonstrates improved or comparable\nresilience to these common noise sources.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23419v1", "cate": "cs.ET", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23115", "title": "FLOSS: Federated Learning with Opt-Out and Straggler Support", "authors": ["David J Goetze", "Dahlia J Felten", "Jeannie R Albrecht", "Rohit Bhattacharya"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      5 pages", "url": "http://arxiv.org/abs/2507.23115v1", "summary": "Previous work on data privacy in federated learning systems focuses on\nprivacy-preserving operations for data from users who have agreed to share\ntheir data for training. However, modern data privacy agreements also empower\nusers to use the system while opting out of sharing their data as desired. When\ncombined with stragglers that arise from heterogeneous device capabilities, the\nresult is missing data from a variety of sources that introduces bias and\ndegrades model performance. In this paper, we present FLOSS, a system that\nmitigates the impacts of such missing data on federated learning in the\npresence of stragglers and user opt-out, and empirically demonstrate its\nperformance in simulations.", "comment": "5 pages", "pdf_url": "http://arxiv.org/pdf/2507.23115v1", "cate": "cs.LG", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.23135", "title": "ISO-Bench: Benchmarking Multimodal Causal Reasoning in Visual-Language Models through Procedural Plans", "authors": ["Ananya Sadana", "Yash Kumar Lal", "Jiawei Zhou"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23135v1", "summary": "Understanding causal relationships across modalities is a core challenge for\nmultimodal models operating in real-world environments. We introduce ISO-Bench,\na benchmark for evaluating whether models can infer causal dependencies between\nvisual observations and procedural text. Each example presents an image of a\ntask step and a text snippet from a plan, with the goal of deciding whether the\nvisual step occurs before or after the referenced text step. Evaluation results\non ten frontier vision-language models show underwhelming performance: the best\nzero-shot F1 is only 0.57, and chain-of-thought reasoning yields only modest\ngains (up to 0.62 F1), largely behind humans (0.98 F1). Our analysis further\nhighlights concrete directions for improving causal understanding in multimodal\nmodels.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23135v1", "cate": "cs.CL", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.23185", "title": "Single Image Rain Streak Removal Using Harris Corner Loss and R-CBAM Network", "authors": ["Jongwook Si", "Sungyoung Kim"], "categories": ["cs.CV", "eess.IV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      21 pages", "url": "http://arxiv.org/abs/2507.23185v1", "summary": "The problem of single-image rain streak removal goes beyond simple noise\nsuppression, requiring the simultaneous preservation of fine structural details\nand overall visual quality. In this study, we propose a novel image restoration\nnetwork that effectively constrains the restoration process by introducing a\nCorner Loss, which prevents the loss of object boundaries and detailed texture\ninformation during restoration. Furthermore, we propose a Residual\nConvolutional Block Attention Module (R-CBAM) Block into the encoder and\ndecoder to dynamically adjust the importance of features in both spatial and\nchannel dimensions, enabling the network to focus more effectively on regions\nheavily affected by rain streaks. Quantitative evaluations conducted on the\nRain100L and Rain100H datasets demonstrate that the proposed method\nsignificantly outperforms previous approaches, achieving a PSNR of 33.29 dB on\nRain100L and 26.16 dB on Rain100H.", "comment": "21 pages", "pdf_url": "http://arxiv.org/pdf/2507.23185v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.22450", "title": "Settling Weighted Token Swapping up to Algorithmic Barriers", "authors": ["Nicole Wein", "Guanyu Tony Zhang"], "categories": ["cs.DS"], "primary_category": "Subjects:       Data Structures and Algorithms (cs.DS)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22450v2", "summary": "We study the weighted token swapping problem, in which we are given a graph\non $n$ vertices, $n$ weighted tokens, an initial assignment of one token to\neach vertex, and a final assignment of one token to each vertex. The goal is to\nfind a minimum-cost sequence of swaps of adjacent tokens to reach the final\nassignment from the initial assignment, where the cost is the sum over all\nswaps of the sum of the weights of the two swapped tokens. Unweighted token\nswapping has been extensively studied: it is NP-hard to approximate to a factor\nbetter than $14/13$, and there is a polynomial-time 4-approximation, along with\na tight \"barrier\" result showing that the class of locally optimal algorithms\ncannot achieve a ratio better than 4. For trees, the problem remains NP-hard to\nsolve exactly, and there is a polynomial-time 2-approximation, along with a\ntight barrier result showing that the class of $\\ell$-straying algorithms\ncannot achieve a ratio better than 2. Weighted token swapping with $\\{0,1\\}$\nweights is much harder to approximation: it is NP-hard to approximate even to a\nfactor of $(1-\\varepsilon) \\cdot \\ln n$ for any constant $\\varepsilon>0$.\nRestricting to positive weights, no approximation algorithms are known, and the\nonly known lower bounds are those inherited directly from the unweighted\nversion. We provide the first approximation algorithms for weighted token\nswapping on both trees and general graphs, along with tight barrier results.\nLetting $w$ and $W$ be the minimum and maximum token weights, our approximation\nratio is $2+2W/w$ for general graphs and $1+W/w$ for trees.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22450v2", "cate": "cs.DS", "date": "2025-07-30", "updated": "2025-07-31"}
{"id": "2507.23618", "title": "SOME: Symmetric One-Hot Matching Elector -- A Lightweight Microsecond Decoder for Quantum Error Correction", "authors": ["Xinyi Guo", "Geguang Miao", "Shinichi Nishizawa", "Hiromitsu Awano", "Shinji Kimura", "Takashi Sato"], "categories": ["cs.ET", "quant-ph"], "primary_category": "Subjects:       Emerging Technologies (cs.ET)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23618v1", "summary": "Conventional quantum error correction (QEC) decoders such as Minimum-Weight\nPerfect Matching (MWPM) and Union-Find (UF) offer high thresholds and fast\ndecoding, respectively, but both suffer from high topological complexity. In\ncontrast, Ising model-based decoders reduce topological complexity but demand\nconsiderable decoding time. We propose the Symmetric One-Hot Matching Elector\n(SOME), a novel decoder that reformulates the QEC decoding task as a Quadratic\nUnconstrained Binary Optimization (QUBO) problem -- termed the One-Hot QUBO\n(OHQ). Each variable in the QUBO represents whether a given pair of flipped\nsyndromes is matched, while the error probabilities between the pair are\nencoded as interaction coefficients (weight). Constraints ensure that each\nflipped syndrome is matched exactly once. Valid solutions of OHQ correspond to\nself-inverse permutation matrices, characterized by symmetric one-hot encoding.\nTo solve the OHQ efficiently, SOME reformulates the decoding task as the\nconstruction of permutation matrices that minimize the total weight. It\ninitializes each candidate matrix from one of the minimum-weight syndrome\npairs, then iteratively appends additional pairs in ascending order of weight,\nand finally selects the permutation matrix with the lowest total energy. SOME\nachieves up to a 99.9x reduction in variable count and reduces decoding times\nfrom milliseconds to microseconds on a single-threaded commodity CPU. OHQ also\nmaintains performance up to a 10.5% physical error rate, surpassing the highest\nknown threshold of MWPM@.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23618v1", "cate": "cs.ET", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23154", "title": "FuseTen: A Generative Model for Daily 10 m Land Surface Temperature Estimation from Spatio-Temporal Satellite Observations", "authors": ["Sofiane Bouaziz", "Adel Hafiane", "Raphael Canals", "Rachid Nedjai"], "categories": ["cs.LG", "cs.AI", "cs.CV"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted in the 2025 International Conference on Machine Intelligence for GeoAnalytics and Remote Sensing (MIGARS)", "url": "http://arxiv.org/abs/2507.23154v1", "summary": "Urban heatwaves, droughts, and land degradation are pressing and growing\nchallenges in the context of climate change. A valuable approach to studying\nthem requires accurate spatio-temporal information on land surface conditions.\nOne of the most important variables for assessing and understanding these\nphenomena is Land Surface Temperature (LST), which is derived from satellites\nand provides essential information about the thermal state of the Earth's\nsurface. However, satellite platforms inherently face a trade-off between\nspatial and temporal resolutions. To bridge this gap, we propose FuseTen, a\nnovel generative framework that produces daily LST observations at a fine 10 m\nspatial resolution by fusing spatio-temporal observations derived from\nSentinel-2, Landsat 8, and Terra MODIS. FuseTen employs a generative\narchitecture trained using an averaging-based supervision strategy grounded in\nphysical principles. It incorporates attention and normalization modules within\nthe fusion process and uses a PatchGAN discriminator to enforce realism.\nExperiments across multiple dates show that FuseTen outperforms linear\nbaselines, with an average 32.06% improvement in quantitative metrics and\n31.42% in visual fidelity. To the best of our knowledge, this is the first\nnon-linear method to generate daily LST estimates at such fine spatial\nresolution.", "comment": "Accepted in the 2025 International Conference on Machine Intelligence\n  for GeoAnalytics and Remote Sensing (MIGARS)", "pdf_url": "http://arxiv.org/pdf/2507.23154v1", "cate": "cs.LG", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.23158", "title": "User Feedback in Human-LLM Dialogues: A Lens to Understand Users But Noisy as a Learning Signal", "authors": ["Yuhan Liu", "Michael J. Q. Zhang", "Eunsol Choi"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Earlier version of this paper was presented at 2nd Workshop on Models of Human Feedback for AI Alignment (MoFA), ICML 2025", "url": "http://arxiv.org/abs/2507.23158v1", "summary": "Once language models (LMs) are deployed, they can interact with users\nlong-term, ideally evolving continuously based on their feedback. Asking for\ndirect user feedback can be disruptive; thus, we study harvesting user feedback\nfrom user-LM interaction logs. We study implicit user feedback in two user-LM\ninteraction datasets (WildChat and LMSYS). First, we analyze user feedback in\nthe user-LLM conversation trajectory, providing insights into when and why such\nfeedback occurs. Second, we study harvesting learning signals from such\nimplicit user feedback. We find that the contents of user feedback (e.g., user\nwanted clarification), not just the polarity (e.g., users were unhappy with the\nprevious model response), can improve model performance in short human-designed\nquestions (MTBench) but not on longer and more complex questions (WildBench).\nWe also find that the usefulness of user feedback is largely tied to the\nquality of the user's initial prompt. Together, we provide an in-depth study of\nimplicit user feedback, showing its potential and limitations.", "comment": "Earlier version of this paper was presented at 2nd Workshop on Models\n  of Human Feedback for AI Alignment (MoFA), ICML 2025", "pdf_url": "http://arxiv.org/pdf/2507.23158v1", "cate": "cs.CL", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.23188", "title": "Multi-Modal Motion Retrieval by Learning a Fine-Grained Joint Embedding Space", "authors": ["Shiyao Yu", "Zi-An Wang", "Kangning Yin", "Zheng Tian", "Mingyuan Zhang", "Weixin Si", "Shihao Zou"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by IEEE TMM 2025", "url": "http://arxiv.org/abs/2507.23188v1", "summary": "Motion retrieval is crucial for motion acquisition, offering superior\nprecision, realism, controllability, and editability compared to motion\ngeneration. Existing approaches leverage contrastive learning to construct a\nunified embedding space for motion retrieval from text or visual modality.\nHowever, these methods lack a more intuitive and user-friendly interaction mode\nand often overlook the sequential representation of most modalities for\nimproved retrieval performance. To address these limitations, we propose a\nframework that aligns four modalities -- text, audio, video, and motion --\nwithin a fine-grained joint embedding space, incorporating audio for the first\ntime in motion retrieval to enhance user immersion and convenience. This\nfine-grained space is achieved through a sequence-level contrastive learning\napproach, which captures critical details across modalities for better\nalignment. To evaluate our framework, we augment existing text-motion datasets\nwith synthetic but diverse audio recordings, creating two multi-modal motion\nretrieval datasets. Experimental results demonstrate superior performance over\nstate-of-the-art methods across multiple sub-tasks, including an 10.16%\nimprovement in R@10 for text-to-motion retrieval and a 25.43% improvement in\nR@1 for video-to-motion retrieval on the HumanML3D dataset. Furthermore, our\nresults show that our 4-modal framework significantly outperforms its 3-modal\ncounterpart, underscoring the potential of multi-modal motion retrieval for\nadvancing motion acquisition.", "comment": "Accepted by IEEE TMM 2025", "pdf_url": "http://arxiv.org/pdf/2507.23188v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2402.03158", "title": "Optimal and Near-Optimal Adaptive Vector Quantization", "authors": ["Ran Ben-Basat", "Yaniv Ben-Itzhak", "Michael Mitzenmacher", "Shay Vargaftik"], "categories": ["cs.LG", "cs.DS", "cs.IT", "cs.NI", "math.IT"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2402.03158v2", "summary": "Quantization is a fundamental optimization for many machine-learning use\ncases, including compressing gradients, model weights and activations, and\ndatasets. The most accurate form of quantization is \\emph{adaptive}, where the\nerror is minimized with respect to a given input, rather than optimizing for\nthe worst case. However, optimal adaptive quantization methods are considered\ninfeasible in terms of both their runtime and memory requirements.\n  We revisit the Adaptive Vector Quantization (AVQ) problem and present\nalgorithms that find optimal solutions with asymptotically improved time and\nspace complexity. We also present an even faster near-optimal algorithm for\nlarge inputs. Our experiments show our algorithms may open the door to using\nAVQ more extensively in a variety of machine learning applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2402.03158v2", "cate": "cs.LG", "date": "2024-02-05", "updated": "2025-07-31"}
{"id": "2507.22972", "title": "Complexity-energy trade-off in programmable unitary interferometers", "authors": ["Nikita A. Nemkov", "Stanislav S. Straupe"], "categories": ["physics.optics", "cs.ET"], "primary_category": "Subjects:       Optics (physics.optics)", "pdf_link": null, "comments": "Comments:      6 pages", "url": "http://arxiv.org/abs/2507.22972v1", "summary": "Coherent multiport interferometers are a promising approach to realize matrix\nmultiplication in integrated photonics. However, most known architectures -\nsuch as MZI and beamsplitter meshes, as well as more general interferometers -\nsuffer from complicated procedures for mapping the matrix elements of the\ndesired transformation to specific phaseshifts in the device. We point out that\nthe high programming complexity is intrinsic, rather than accidental. At the\nsame time, we argue that interferometers admitting efficient programming\nalgorithms in general yield a much lower useful output energy, which ultimately\nlimits their accuracy and energy efficiency.", "comment": "6 pages", "pdf_url": "http://arxiv.org/pdf/2507.22972v1", "cate": "physics.optics", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.23167", "title": "LENS: Learning Ensemble Confidence from Neural States for Multi-LLM Answer Integration", "authors": ["Jizhou Guo"], "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MA"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23167v1", "summary": "Large Language Models (LLMs) have demonstrated impressive performance across\nvarious tasks, with different models excelling in distinct domains and specific\nabilities. Effectively combining the predictions of multiple LLMs is crucial\nfor enhancing system robustness and performance. However, existing ensemble\nmethods often rely on simple techniques like voting or logits ensembling, which\noverlook the varying confidence and reliability of models in different\ncontexts. In this work, we propose LENS (Learning ENsemble confidence from\nNeural States), a novel approach that learns to estimate model confidence by\nanalyzing internal representations. For each LLM, we train a lightweight linear\nconfidence predictor that leverages layer-wise hidden states and normalized\nprobabilities as inputs. This allows for more nuanced weighting of model\npredictions based on their context-dependent reliability. Our method does not\nrequire modifying the model parameters and requires negligible additional\ncomputation. Experimental results on multiple-choice and boolean\nquestion-answering tasks demonstrate that LENS outperforms traditional ensemble\nmethods by a substantial margin. Our findings suggest that internal\nrepresentations provide valuable signals for determining model confidence and\ncan be effectively leveraged for ensemble learning.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23167v1", "cate": "cs.CL", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23194", "title": "Geak: Introducing Triton Kernel AI Agent & Evaluation Benchmarks", "authors": ["Jianghui Wang", "Vinay Joshi", "Saptarshi Majumder", "Xu Chao", "Bin Ding", "Ziqiong Liu", "Pratik Prabhanjan Brahma", "Dong Li", "Zicheng Liu", "Emad Barsoum"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23194v1", "summary": "The demand for AI-generated GPU kernels is rapidly growing, influenced by the\nneed for scalable, hardware-optimized solutions in both industry and academia.\nAs deep learning workloads grow in complexity and diversity, it is imperative\nto automate low-level kernel development to meet performance and productivity\ndemands. Major cloud providers, semiconductor companies, and research\ninstitutions are now investing heavily in AI-driven code generation for GPUs,\naiming to reduce manual optimization efforts while achieving near-expert\nperformance on hardware like AMD MI300X. The Triton language, a Python-based\nDSL for GPU programming, has emerged as a popular target for such AI-generated\nkernels due to its balance of performance and ease-of-coding. In this work, we\npresent an evaluation suite for Triton-based GPU kernels and GEAK (Generating\nEfficient AI-centric GPU Kernels)-a framework that leverages cutting-edge LLMs\nto generate performant Triton code specifically for AMD GPUs, including the AMD\nMI300X and MI250. GEAK leverages inference-time compute scaling to produce\nTriton-based GPU kernels using a reasoning loop adapted from Reflexion-style\nfeedback mechanisms. On two evaluation benchmarks, GEAK significantly\noutperformed the baselines of directly prompting frontier LLMs as well as\nReflexion-based generation pipelines by achieving correctness up to $63$% and\nexecution speed up of up to $2.59$X. These results highlight the promise of\nGEAK-like agentic code generation for accelerating the adoption of diverse\nhardware platforms and democratizing access to expert-level kernel performance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23194v1", "cate": "cs.CL", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23193", "title": "A Novel Dataset for Flood Detection Robust to Seasonal Changes in Satellite Imagery", "authors": ["Youngsun Jang", "Dongyoun Kim", "Chulwoo Pack", "Kwanghee Won"], "categories": ["cs.CV", "I.4.6; I.2.10; I.5.4"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      8 pages, 2 figures. Presented at ACM RACS 2024 (Pompei, Italy, Nov 5-8, 2024)", "url": "http://arxiv.org/abs/2507.23193v1", "summary": "This study introduces a novel dataset for segmenting flooded areas in\nsatellite images. After reviewing 77 existing benchmarks utilizing satellite\nimagery, we identified a shortage of suitable datasets for this specific task.\nTo fill this gap, we collected satellite imagery of the 2019 Midwestern USA\nfloods from Planet Explorer by Planet Labs (Image \\c{opyright} 2024 Planet Labs\nPBC). The dataset consists of 10 satellite images per location, each containing\nboth flooded and non-flooded areas. We selected ten locations from each of the\nfive states: Iowa, Kansas, Montana, Nebraska, and South Dakota. The dataset\nensures uniform resolution and resizing during data processing. For evaluating\nsemantic segmentation performance, we tested state-of-the-art models in\ncomputer vision and remote sensing on our dataset. Additionally, we conducted\nan ablation study varying window sizes to capture temporal characteristics.\nOverall, the models demonstrated modest results, suggesting a requirement for\nfuture multimodal and temporal learning strategies. The dataset will be\npublicly available on\n<https://github.com/youngsunjang/SDSU_MidWest_Flood_2019>.", "comment": "8 pages, 2 figures. Presented at ACM RACS 2024 (Pompei, Italy, Nov\n  5-8, 2024)", "pdf_url": "http://arxiv.org/pdf/2507.23193v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2405.04261", "title": "Graph Reconstruction from Noisy Random Subgraphs", "authors": ["Andrew McGregor", "Rik Sengupta"], "categories": ["cs.IT", "cs.DS", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "Comments:      6 pages, to appear in ISIT 2024", "url": "http://arxiv.org/abs/2405.04261v2", "summary": "We consider the problem of reconstructing an undirected graph $G$ on $n$\nvertices given multiple random noisy subgraphs or \"traces\". Specifically, a\ntrace is generated by sampling each vertex with probability $p_v$, then taking\nthe resulting induced subgraph on the sampled vertices, and then adding noise\nin the form of either (a) deleting each edge in the subgraph with probability\n$1-p_e$, or (b) deleting each edge with probability $f_e$ and transforming a\nnon-edge into an edge with probability $f_e$. We show that, under mild\nassumptions on $p_v$, $p_e$ and $f_e$, if $G$ is selected uniformly at random,\nthen $O(p_e^{-1} p_v^{-2} \\log n)$ or $O((f_e-1/2)^{-2} p_v^{-2} \\log n)$\ntraces suffice to reconstruct $G$ with high probability. In contrast, if $G$ is\narbitrary, then $\\exp(\\Omega(n))$ traces are necessary even when $p_v=1,\np_e=1/2$.", "comment": "6 pages, to appear in ISIT 2024", "pdf_url": "http://arxiv.org/pdf/2405.04261v2", "cate": "cs.IT", "date": "2024-05-07", "updated": "2025-07-31"}
{"id": "2507.23342", "title": "FAST-LoRa: An Efficient Simulation Framework for Evaluating LoRaWAN Networks and Transmission Parameter Strategies", "authors": ["Laura Acosta García", "Juan Aznar Poveda", "Fabian Margreiter", "Antonio-Javier García Sánchez", "Joan García Haro", "Thomas Fahringer", "José Lorente López", "José-Víctor Rodríguez"], "categories": ["cs.NI", "cs.ET"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23342v1", "summary": "The Internet of Things (IoT) has transformed many industries, and LoRaWAN\n(Long Range Wide Area Network), built on LoRa (Long Range) technology, has\nbecome a crucial solution for enabling scalable, low-cost, and energy-efficient\ncommunication in wide-area networks. Simulation tools are essential for\noptimizing the transmission parameters and, therefore, the energy efficiency\nand performance of LoRaWAN networks. While existing simulation frameworks\naccurately replicate real-world scenarios by including multiple layers of\ncommunication protocols, they often imply significant computational overhead\nand simulation times. To address this issue, this paper introduces FAST-LoRa, a\nnovel simulation framework designed to enable fast and efficient evaluation of\nLoRaWAN networks and selection of transmission parameters. FAST-LoRa\nstreamlines computation by relying on analytical models without complex\npacket-level simulations and implementing gateway reception using efficient\nmatrix operations. Rather than aiming to replace discrete-event simulators,\nFAST-LoRa is intended as a lightweight and accurate approximation tool for\nevaluating transmission parameter strategies in scenarios with stable traffic\npatterns and uplink-focused communications. In our evaluation, we compare\nFAST-LoRa with a well-established simulator using multiple network\nconfigurations with varying numbers of end devices and gateways. The results\nshow that FAST-LoRa achieves similar accuracy in estimating key network\nmetrics, even in complex scenarios with interference and multi-gateway\nreception, with a Mean Absolute Error (MAE) of 0.940 $\\times 10^{-2}$ for the\nPacket Delivery Ratio (PDR) and 0.040 bits/mJ for Energy Efficiency (EE), while\nsignificantly reducing computational time by up to three orders of magnitude.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23342v1", "cate": "cs.NI", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2504.08575", "title": "Prophecies all the Way: Game-based Model-Checking for HyperQPTL beyond $\\forall^*\\exists^*$", "authors": ["Sarah Winter", "Martin Zimmermann"], "categories": ["cs.LO", "cs.FL"], "primary_category": "Subjects:       Logic in Computer Science (cs.LO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.08575v4", "summary": "Model-checking HyperLTL, a temporal logic expressing properties of sets of\ntraces with applications to information-flow based security and privacy, has a\ndecidable, but TOWER-complete, model-checking problem. While the classical\nmodel-checking algorithm for full HyperLTL is automata-theoretic, more\nrecently, a game-based alternative for the $\\forall^*\\exists^*$-fragment has\nbeen presented.\n  Here, we employ imperfect information-games to extend the game-based approach\nto full HyperQPTL, which features arbitrary quantifier prefixes and\nquantification over propositions and can express every $\\omega$-regular\nhyperproperty. As a byproduct of our game-based algorithm, we obtain\nfinite-state implementations of Skolem functions via transducers with lookahead\nthat explain satisfaction or violation of HyperQPTL properties.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.08575v4", "cate": "cs.LO", "date": "2025-04-11", "updated": "2025-07-31"}
{"id": "2507.23178", "title": "AutoBridge: Automating Smart Device Integration with Centralized Platform", "authors": ["Siyuan Liu", "Zhice Yang", "Huangxun Chen"], "categories": ["cs.SE", "cs.AI", "I.2.5"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      14 pages, 12 figures, under review", "url": "http://arxiv.org/abs/2507.23178v1", "summary": "Multimodal IoT systems coordinate diverse IoT devices to deliver\nhuman-centered services. The ability to incorporate new IoT devices under the\nmanagement of a centralized platform is an essential requirement. However, it\nrequires significant human expertise and effort to program the complex IoT\nintegration code that enables the platform to understand and control the device\nfunctions. Therefore, we propose AutoBridge to automate IoT integration code\ngeneration. Specifically, AutoBridge adopts a divide-and-conquer strategy: it\nfirst generates device control logic by progressively retrieving\ndevice-specific knowledge, then synthesizes platformcompliant integration code\nusing platform-specific knowledge. To ensure correctness, AutoBridge features a\nmulti-stage debugging pipeline, including an automated debugger for virtual IoT\ndevice testing and an interactive hardware-in-the-loop debugger that requires\nonly binary user feedback (yes and no) for real-device verification. We\nevaluate AutoBridge on a benchmark of 34 IoT devices across two open-source IoT\nplatforms. The results demonstrate that AutoBridge can achieves an average\nsuccess rate of 93.87% and an average function coverage of 94.87%, without any\nhuman involvement. With minimal binary yes and no feedback from users, the code\nis then revised to reach 100% function coverage. A user study with 15\nparticipants further shows that AutoBridge outperforms expert programmers by\n50% to 80% in code accuracy, even when the programmers are allowed to use\ncommercial code LLMs.", "comment": "14 pages, 12 figures, under review", "pdf_url": "http://arxiv.org/pdf/2507.23178v1", "cate": "cs.SE", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23211", "title": "Failures Are the Stepping Stones to Success: Enhancing Few-Shot In-Context Learning by Leveraging Negative Samples", "authors": ["Yunhao Liang", "Ruixuan Ying", "Takuya Taniguchi", "Zhe Cui"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23211v1", "summary": "Large Language Models exhibit powerful few-shot in-context learning (ICL)\ncapabilities, but the performance is highly sensitive to provided examples.\n  Recent research has focused on retrieving corresponding examples for each\ninput query, not only enhancing the efficiency and scalability of the learning\nprocess but also mitigating inherent biases in manual example selection.\n  However, these studies have primarily emphasized leveraging Positive samples\nwhile overlooking the additional information within Negative samples for\ncontextual learning.\n  We propose a novel method that utilizes Negative samples to better select\nPositive sample examples, thereby enhancing the performance of few-shot ICL.\nInitially, we construct Positive and Negative sample corpora based on\nZero-Shot-Cot. Then, during inference, we employ a semantic similarity-based\napproach to select the most similar examples from both the Positive and\nNegative corpora for a given query. Subsequently, we further retrieve Positive\nexamples from the Positive sample corpus based on semantic similarity to the\nNegative examples, then concatenating them with the previously selected\nPositive examples to serve as ICL demonstrations. Experimental results\ndemonstrate that our approach surpasses methods solely relying on the most\nsimilar positive examples for context, validating that the additional\ninformation in negative samples aids in enhancing ICL performance through\nimproved Positive sample selection.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23211v1", "cate": "cs.CL", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23202", "title": "Adversarial-Guided Diffusion for Multimodal LLM Attacks", "authors": ["Chengwei Xia", "Fan Ma", "Ruijie Quan", "Kun Zhan", "Yi Yang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23202v1", "summary": "This paper addresses the challenge of generating adversarial image using a\ndiffusion model to deceive multimodal large language models (MLLMs) into\ngenerating the targeted responses, while avoiding significant distortion of the\nclean image. To address the above challenges, we propose an adversarial-guided\ndiffusion (AGD) approach for adversarial attack MLLMs. We introduce\nadversarial-guided noise to ensure attack efficacy. A key observation in our\ndesign is that, unlike most traditional adversarial attacks which embed\nhigh-frequency perturbations directly into the clean image, AGD injects target\nsemantics into the noise component of the reverse diffusion. Since the added\nnoise in a diffusion model spans the entire frequency spectrum, the adversarial\nsignal embedded within it also inherits this full-spectrum property.\nImportantly, during reverse diffusion, the adversarial image is formed as a\nlinear combination of the clean image and the noise. Thus, when applying\ndefenses such as a simple low-pass filtering, which act independently on each\ncomponent, the adversarial image within the noise component is less likely to\nbe suppressed, as it is not confined to the high-frequency band. This makes AGD\ninherently robust to variety defenses. Extensive experiments demonstrate that\nour AGD outperforms state-of-the-art methods in attack performance as well as\nin model robustness to some defenses.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23202v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.22810", "title": "VRISE: A Virtual Reality Platfrom for Immersive and Interactive Surveying Education", "authors": ["Daniel Udekwe", "Dimitrios Bolkas", "Eren Erman Ozguven", "Ren Moses", "Qianwen Guo"], "categories": ["cs.HC", "cs.ET", "cs.SE"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22810v2", "summary": "Surveying is a core component of civil engineering education, requiring\nstudents to engage in hands-on spatial measurement, instrumentation handling,\nand field-based decision-making. However, traditional instruction often poses\nlogistical and cognitive challenges that can hinder accessibility and student\nengagement. While virtual laboratories have gained traction in engineering\neducation, few are purposefully designed to support flexible, adaptive learning\nin surveying. To address this gap, we developed Virtual Reality for Immersive\nand Interactive Surveying Education (VRISE), an immersive virtual reality\nlaboratory that replicates ground-based and aerial surveying tasks through\ncustomizable, accessible, and user-friendly modules. VRISE features interactive\nexperiences such as differential leveling with a digital level equipment and\nwaypoint-based drone navigation, enhanced by input smoothing, adaptive\ninterfaces, and real-time feedback to accommodate diverse learning styles.\nEvaluation across multiple user sessions demonstrated consistent gains in\nmeasurement accuracy, task efficiency, and interaction quality, with a clear\nprogression in skill development across the ground-based and aerial surveying\nmodalities. By reducing cognitive load and physical demands, even in tasks\nrequiring fine motor control and spatial reasoning, VRISE demonstrates the\npotential of immersive, repeatable digital environments to enhance surveying\neducation, broaden participation, and strengthen core competencies in a safe\nand engaging setting.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22810v2", "cate": "cs.HC", "date": "2025-07-30", "updated": "2025-07-31"}
{"id": "2507.23190", "title": "Accessibility Scout: Personalized Accessibility Scans of Built Environments", "authors": ["William Huang", "Xia Su", "Jon E. Froehlich", "Yang Zhang"], "categories": ["cs.HC", "cs.AI", "cs.CV", "cs.MA"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      18 pages, 16 figures. Presented at ACM UIST 2025", "url": "http://arxiv.org/abs/2507.23190v1", "summary": "Assessing the accessibility of unfamiliar built environments is critical for\npeople with disabilities. However, manual assessments, performed by users or\ntheir personal health professionals, are laborious and unscalable, while\nautomatic machine learning methods often neglect an individual user's unique\nneeds. Recent advances in Large Language Models (LLMs) enable novel approaches\nto this problem, balancing personalization with scalability to enable more\nadaptive and context-aware assessments of accessibility. We present\nAccessibility Scout, an LLM-based accessibility scanning system that identifies\naccessibility concerns from photos of built environments. With use,\nAccessibility Scout becomes an increasingly capable \"accessibility scout\",\ntailoring accessibility scans to an individual's mobility level, preferences,\nand specific environmental interests through collaborative Human-AI\nassessments. We present findings from three studies: a formative study with six\nparticipants to inform the design of Accessibility Scout, a technical\nevaluation of 500 images of built environments, and a user study with 10\nparticipants of varying mobility. Results from our technical evaluation and\nuser study show that Accessibility Scout can generate personalized\naccessibility scans that extend beyond traditional ADA considerations. Finally,\nwe conclude with a discussion on the implications of our work and future steps\nfor building more scalable and personalized accessibility assessments of the\nphysical world.", "comment": "18 pages, 16 figures. Presented at ACM UIST 2025", "pdf_url": "http://arxiv.org/pdf/2507.23190v1", "cate": "cs.HC", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23220", "title": "Model Directions, Not Words: Mechanistic Topic Models Using Sparse Autoencoders", "authors": ["Carolina Zheng", "Nicolas Beltran-Velez", "Sweta Karlekar", "Claudia Shi", "Achille Nazaret", "Asif Mallik", "Amir Feder", "David M. Blei"], "categories": ["cs.CL", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23220v1", "summary": "Traditional topic models are effective at uncovering latent themes in large\ntext collections. However, due to their reliance on bag-of-words\nrepresentations, they struggle to capture semantically abstract features. While\nsome neural variants use richer representations, they are similarly constrained\nby expressing topics as word lists, which limits their ability to articulate\ncomplex topics. We introduce Mechanistic Topic Models (MTMs), a class of topic\nmodels that operate on interpretable features learned by sparse autoencoders\n(SAEs). By defining topics over this semantically rich space, MTMs can reveal\ndeeper conceptual themes with expressive feature descriptions. Moreover,\nuniquely among topic models, MTMs enable controllable text generation using\ntopic-based steering vectors. To properly evaluate MTM topics against\nword-list-based approaches, we propose \\textit{topic judge}, an LLM-based\npairwise comparison evaluation framework. Across five datasets, MTMs match or\nexceed traditional and neural baselines on coherence metrics, are consistently\npreferred by topic judge, and enable effective steering of LLM outputs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23220v1", "cate": "cs.CL", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23206", "title": "Confidence-aware agglomeration classification and segmentation of 2D microscopic food crystal images", "authors": ["Xiaoyu Ji", "Ali Shakouri", "Fengqing Zhu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23206v1", "summary": "Food crystal agglomeration is a phenomenon occurs during crystallization\nwhich traps water between crystals and affects food product quality. Manual\nannotation of agglomeration in 2D microscopic images is particularly difficult\ndue to the transparency of water bonding and the limited perspective focusing\non a single slide of the imaged sample. To address this challenge, we first\npropose a supervised baseline model to generate segmentation pseudo-labels for\nthe coarsely labeled classification dataset. Next, an instance classification\nmodel that simultaneously performs pixel-wise segmentation is trained. Both\nmodels are used in the inference stage to combine their respective strengths in\nclassification and segmentation. To preserve crystal properties, a post\nprocessing module is designed and included to both steps. Our method improves\ntrue positive agglomeration classification accuracy and size distribution\npredictions compared to other existing methods. Given the variability in\nconfidence levels of manual annotations, our proposed method is evaluated under\ntwo confidence levels and successfully classifies potential agglomerated\ninstances.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23206v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23217", "title": "Zero-Shot Document Understanding using Pseudo Table of Contents-Guided Retrieval-Augmented Generation", "authors": ["Hyeon Seong Jeong", "Sangwoo Jo", "Byeong Hyun Yoon", "Yoonseok Heo", "Haedong Jeong", "Taehoon Kim"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23217v1", "summary": "Understanding complex multimodal documents remains challenging due to their\nstructural inconsistencies and limited training data availability. We introduce\n\\textit{DocsRay}, a training-free document understanding system that integrates\npseudo Table of Contents (TOC) generation with hierarchical Retrieval-Augmented\nGeneration (RAG). Our approach leverages multimodal Large Language Models'\n(LLMs) native capabilities to seamlessly process documents containing diverse\nelements such as text, images, charts, and tables without requiring specialized\nmodels or additional training. DocsRay's framework synergistically combines\nthree key techniques: (1) a semantic structuring module using prompt-based LLM\ninteractions to generate a hierarchical pseudo-TOC, (2) zero-shot multimodal\nanalysis that converts diverse document elements into unified, text-centric\nrepresentations using the inherent capabilities of multimodal LLMs, and (3) an\nefficient two-stage hierarchical retrieval system that reduces retrieval\ncomplexity from $O(N)$ to $O(S + k_1 \\cdot N_s)$. Evaluated on documents\naveraging 49.4 pages and 20,971 textual tokens, DocsRay reduced query latency\nfrom 3.89 to 2.12 seconds, achieving a 45% efficiency improvement. On the\nMMLongBench-Doc benchmark, DocsRay-Pro attains an accuracy of 64.7%,\nsubstantially surpassing previous state-of-the-art results.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23217v1", "cate": "cs.LG", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23227", "title": "Enabling Few-Shot Alzheimer's Disease Diagnosis on Tabular Biomarker Data with LLMs", "authors": ["Sophie Kearney", "Shu Yang", "Zixuan Wen", "Bojian Hou", "Duy Duong-Tran", "Tianlong Chen", "Jason Moore", "Marylyn Ritchie", "Li Shen"], "categories": ["cs.CL", "cs.LG", "q-bio.QM"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23227v1", "summary": "Early and accurate diagnosis of Alzheimer's disease (AD), a complex\nneurodegenerative disorder, requires analysis of heterogeneous biomarkers\n(e.g., neuroimaging, genetic risk factors, cognitive tests, and cerebrospinal\nfluid proteins) typically represented in a tabular format. With flexible\nfew-shot reasoning, multimodal integration, and natural-language-based\ninterpretability, large language models (LLMs) offer unprecedented\nopportunities for prediction with structured biomedical data. We propose a\nnovel framework called TAP-GPT, Tabular Alzheimer's Prediction GPT, that adapts\nTableGPT2, a multimodal tabular-specialized LLM originally developed for\nbusiness intelligence tasks, for AD diagnosis using structured biomarker data\nwith small sample sizes. Our approach constructs few-shot tabular prompts using\nin-context learning examples from structured biomedical data and finetunes\nTableGPT2 using the parameter-efficient qLoRA adaption for a clinical binary\nclassification task of AD or cognitively normal (CN). The TAP-GPT framework\nharnesses the powerful tabular understanding ability of TableGPT2 and the\nencoded prior knowledge of LLMs to outperform more advanced general-purpose\nLLMs and a tabular foundation model (TFM) developed for prediction tasks. To\nour knowledge, this is the first application of LLMs to the prediction task\nusing tabular biomarker data, paving the way for future LLM-driven multi-agent\nframeworks in biomedical informatics.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23227v1", "cate": "cs.CL", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23225", "title": "YOLO-ROC: A High-Precision and Ultra-Lightweight Model for Real-Time Road Damage Detection", "authors": ["Zicheng Lin", "Weichao Pan"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23225v1", "summary": "Road damage detection is a critical task for ensuring traffic safety and\nmaintaining infrastructure integrity. While deep learning-based detection\nmethods are now widely adopted, they still face two core challenges: first, the\ninadequate multi-scale feature extraction capabilities of existing networks for\ndiverse targets like cracks and potholes, leading to high miss rates for\nsmall-scale damage; and second, the substantial parameter counts and\ncomputational demands of mainstream models, which hinder their deployment for\nefficient, real-time detection in practical applications. To address these\nissues, this paper proposes a high-precision and lightweight model, YOLO - Road\nOrthogonal Compact (YOLO-ROC). We designed a Bidirectional Multi-scale Spatial\nPyramid Pooling Fast (BMS-SPPF) module to enhance multi-scale feature\nextraction and implemented a hierarchical channel compression strategy to\nreduce computational complexity. The BMS-SPPF module leverages a bidirectional\nspatial-channel attention mechanism to improve the detection of small targets.\nConcurrently, the channel compression strategy reduces the parameter count from\n3.01M to 0.89M and GFLOPs from 8.1 to 2.6. Experiments on the\nRDD2022_China_Drone dataset demonstrate that YOLO-ROC achieves a mAP50 of\n67.6%, surpassing the baseline YOLOv8n by 2.11%. Notably, the mAP50 for the\nsmall-target D40 category improved by 16.8%, and the final model size is only\n2.0 MB. Furthermore, the model exhibits excellent generalization performance on\nthe RDD2022_China_Motorbike dataset.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23225v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23257", "title": "Efficient Machine Unlearning via Influence Approximation", "authors": ["Jiawei Liu", "Chenwang Wu", "Defu Lian", "Enhong Chen"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      12 pages, 4 figures", "url": "http://arxiv.org/abs/2507.23257v1", "summary": "Due to growing privacy concerns, machine unlearning, which aims at enabling\nmachine learning models to ``forget\" specific training data, has received\nincreasing attention. Among existing methods, influence-based unlearning has\nemerged as a prominent approach due to its ability to estimate the impact of\nindividual training samples on model parameters without retraining. However,\nthis approach suffers from prohibitive computational overhead arising from the\nnecessity to compute the Hessian matrix and its inverse across all training\nsamples and parameters, rendering it impractical for large-scale models and\nscenarios involving frequent data deletion requests. This highlights the\ndifficulty of forgetting. Inspired by cognitive science, which suggests that\nmemorizing is easier than forgetting, this paper establishes a theoretical link\nbetween memorizing (incremental learning) and forgetting (unlearning). This\nconnection allows machine unlearning to be addressed from the perspective of\nincremental learning. Unlike the time-consuming Hessian computations in\nunlearning (forgetting), incremental learning (memorizing) typically relies on\nmore efficient gradient optimization, which supports the aforementioned\ncognitive theory. Based on this connection, we introduce the Influence\nApproximation Unlearning (IAU) algorithm for efficient machine unlearning from\nthe incremental perspective. Extensive empirical evaluations demonstrate that\nIAU achieves a superior balance among removal guarantee, unlearning efficiency,\nand comparable model utility, while outperforming state-of-the-art methods\nacross diverse datasets and model architectures. Our code is available at\nhttps://github.com/Lolo1222/IAU.", "comment": "12 pages, 4 figures", "pdf_url": "http://arxiv.org/pdf/2507.23257v1", "cate": "cs.LG", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23247", "title": "P-ReMIS: Pragmatic Reasoning in Mental Health and a Social Implication", "authors": ["Sneha Oram", "Pushpak Bhattacharyya"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23247v1", "summary": "There has been an increase in recent advancements in the explainability and\ndevelopment of personalized chatbots for mental health. However, the reasoning\naspects for explainability and dialogue discourse have not been explored\npreviously for mental health. Hence, we are investigating the pragmatic\nreasoning capability of large language models (LLMs) in this domain. We\nintroduce P-ReMe dataset, and propose a modified definition for the pragmatic\nphenomena of implicature (implied meaning) and presupposition (implicit\nassumption) in mental health. Following the definition, we formulate two tasks\nin implicature and one task in presupposition. To benchmark the dataset and the\npresented tasks, we consider four models - Llama3.1, Mistral, MentaLLaMa, and\nQwen. The results of the experiments suggest that Mistral and Qwen show\nsubstantial reasoning capabilities in the domain. In addition, we also propose\nStiPRompts to study the stigma around mental health with the state-of-the-art\nLLMs, GPT-4o mini, Deepseek-chat, and Claude-3.5-haiku. Our evaluated findings\nshow that Claude-3.5-haiku deals with the stigma more responsibly compared to\nthe other two LLMs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23247v1", "cate": "cs.CL", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23226", "title": "Toward Safe, Trustworthy and Realistic Augmented Reality User Experience", "authors": ["Yanming Xiu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      2 pages, 4 figures", "url": "http://arxiv.org/abs/2507.23226v1", "summary": "As augmented reality (AR) becomes increasingly integrated into everyday life,\nensuring the safety and trustworthiness of its virtual content is critical. Our\nresearch addresses the risks of task-detrimental AR content, particularly that\nwhich obstructs critical information or subtly manipulates user perception. We\ndeveloped two systems, ViDDAR and VIM-Sense, to detect such attacks using\nvision-language models (VLMs) and multimodal reasoning modules. Building on\nthis foundation, we propose three future directions: automated, perceptually\naligned quality assessment of virtual content; detection of multimodal attacks;\nand adaptation of VLMs for efficient and user-centered deployment on AR\ndevices. Overall, our work aims to establish a scalable, human-aligned\nframework for safeguarding AR experiences and seeks feedback on perceptual\nmodeling, multimodal AR content implementation, and lightweight model\nadaptation.", "comment": "2 pages, 4 figures", "pdf_url": "http://arxiv.org/pdf/2507.23226v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23261", "title": "DynaSwarm: Dynamically Graph Structure Selection for LLM-based Multi-agent System", "authors": ["Hui Yi Leong", "Yuqing Wu"], "categories": ["cs.LG", "cs.AI", "cs.MA"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23261v1", "summary": "Current multi-agent systems (MAS) frameworks often rely on manually designed\nand static collaboration graph structures, limiting adaptability and\nperformance. To address these limitations, we propose DynaSwarm, a dynamic\nframework that enhances LLM-based MAS through two key innovations: (1) an\nactor-critic reinforcement learning (A2C) mechanism to optimize graph\nstructures with improved stability over prior RL methods, and (2) a dynamic\ngraph selector that adaptively chooses the optimal graph structure for each\ninput sample via parameter-efficient LLM fine-tuning. DynaSwarm eliminates the\nneed for rigid, one-fits-all graph architectures, instead leveraging\nsample-specific idiosyncrasies to dynamically route queries through specialized\nagent networks. (c) We propose to fine-tune the demonstration retriever to\nfully exploit the power of in-context learning (ICL). Extensive experiments on\nquestion answering, mathematical reasoning, and coding tasks demonstrate that\nDynaSwarm consistently outperforms state-of-the-art single-agent and MAS\nbaselines across multiple LLM backbones. Our findings highlight the importance\nof sample-aware structural flexibility in LLM MAS designs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23261v1", "cate": "cs.LG", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23248", "title": "Evaluating LLMs' Multilingual Capabilities for Bengali: Benchmark Creation and Performance Analysis", "authors": ["Shimanto Bhowmik", "Tawsif Tashwar Dipto", "Md Sazzad Islam", "Sheryl Hsu", "Tahsin Reasat"], "categories": ["cs.CL", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23248v1", "summary": "Bengali is an underrepresented language in NLP research. However, it remains\na challenge due to its unique linguistic structure and computational\nconstraints. In this work, we systematically investigate the challenges that\nhinder Bengali NLP performance by focusing on the absence of standardized\nevaluation benchmarks. We then evaluated 10 recent open source Large Language\nModels (LLMs) in 8 of the translated datasets and performed a comprehensive\nerror analysis to pinpoint their primary failure modes. Our findings reveal\nconsistent performance gaps for Bengali compared to English, particularly for\nsmaller models and specific model families like Mistral. We also identified\npromising robustness in certain architectures, such as DeepSeek, that maintain\nmore stable performance across languages. Our analysis reveals an inverse\nrelationship between tokenization efficiency and LLM accuracy where models tend\nto perform worse when inputs are excessively tokenized, whereas more efficient\n\\& concise tokenization results in improved performance. These findings\nhighlight critical areas where current models fall short and underscore the\nneed for improved dataset quality and evaluation methodologies tailored to\nmultilingual contexts. This work will catalyze further research on NLP for\nunderrepresented languages, helping to democratize access to advanced language\ntechnologies worldwide. The code and dataset used in this research is publicly\navailable at https://github.com/BengaliAI/bn-llm-benchmark.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23248v1", "cate": "cs.CL", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23237", "title": "Ambiguity-Guided Learnable Distribution Calibration for Semi-Supervised Few-Shot Class-Incremental Learning", "authors": ["Fan Lyu", "Linglan Zhao", "Chengyan Liu", "Yinying Mei", "Zhang Zhang", "Jian Zhang", "Fuyuan Hu", "Liang Wang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      6 pages, 5 figures", "url": "http://arxiv.org/abs/2507.23237v1", "summary": "Few-Shot Class-Incremental Learning (FSCIL) focuses on models learning new\nconcepts from limited data while retaining knowledge of previous classes.\nRecently, many studies have started to leverage unlabeled samples to assist\nmodels in learning from few-shot samples, giving rise to the field of\nSemi-supervised Few-shot Class-Incremental Learning (Semi-FSCIL). However,\nthese studies often assume that the source of unlabeled data is only confined\nto novel classes of the current session, which presents a narrow perspective\nand cannot align well with practical scenarios. To better reflect real-world\nscenarios, we redefine Semi-FSCIL as Generalized Semi-FSCIL (GSemi-FSCIL) by\nincorporating both base and all the ever-seen novel classes in the unlabeled\nset. This change in the composition of unlabeled samples poses a new challenge\nfor existing methods, as they struggle to distinguish between unlabeled samples\nfrom base and novel classes. To address this issue, we propose an\nAmbiguity-guided Learnable Distribution Calibration (ALDC) strategy. ALDC\ndynamically uses abundant base samples to correct biased feature distributions\nfor few-shot novel classes. Experiments on three benchmark datasets show that\nour method outperforms existing works, setting new state-of-the-art results.", "comment": "6 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2507.23237v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23269", "title": "XABPs: Towards eXplainable Autonomous Business Processes", "authors": ["Peter Fettke", "Fabiana Fournier", "Lior Limonad", "Andreas Metzger", "Stefanie Rinderle-Ma", "Barbara Weber"], "categories": ["cs.SE", "cs.AI", "cs.MA"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23269v1", "summary": "Autonomous business processes (ABPs), i.e., self-executing workflows\nleveraging AI/ML, have the potential to improve operational efficiency, reduce\nerrors, lower costs, improve response times, and free human workers for more\nstrategic and creative work. However, ABPs may raise specific concerns\nincluding decreased stakeholder trust, difficulties in debugging, hindered\naccountability, risk of bias, and issues with regulatory compliance. We argue\nfor eXplainable ABPs (XABPs) to address these concerns by enabling systems to\narticulate their rationale. The paper outlines a systematic approach to XABPs,\ncharacterizing their forms, structuring explainability, and identifying key BPM\nresearch challenges towards XABPs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23269v1", "cate": "cs.SE", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23279", "title": "Unveiling Super Experts in Mixture-of-Experts Large Language Models", "authors": ["Zunhai Su", "Qingyuan Li", "Hao Zhang", "YuLei Qian", "Yuchen Xie", "Kehong Yuan"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23279v1", "summary": "Sparsely activated Mixture-of-Experts (MoE) models have shown promise in\nenhancing the learning capacity of large language models (LLMs). Leveraging the\nintrinsic importance differences among experts, recent research has explored\nexpert-level compression techniques to improve the efficiency of MoE LLMs.\nHowever, existing approaches often rely on empirical criteria to identify\ncritical experts, lacking a deeper exploration and understanding of the\nheterogeneous importance of experts. In this study, we present the first\ndiscovery and investigation of a distinct subset of experts that play a crucial\nrole in the underlying mechanisms during the model's forward inference. These\nexperts are prevalent in open-source MoE LLMs, and despite their limited\nnumber, pruning them leads to a significant decline in model performance (e.g.,\npruning three causes Qwen3-30B-A3B to produce repetitive and uninformative\noutputs). We refer to these experts as Super Experts (SEs). Our comprehensive\nanalysis provides progressively deeper insights into SEs. (i) SEs are\ncharacterized by rare but extreme activation outliers in the output of the\ndown_proj, which give rise to massive activations in the hidden states between\ndecoder layers. Moreover, the distribution of SEs remains model-specific and is\nunaffected by post-training processes. (ii) By pruning SEs, we assess their\nsignificance across a variety of tasks, revealing their considerable impact on\nthe model's overall performance, particularly in mathematical reasoning. (iii)\nWe further enhance our understanding of the influence of SEs compression. Our\nfindings confirm that MoE LLMs rely on SEs to induce attention sinks, which are\ncrucial for the distribution of attention scores but are significantly\ndisrupted by SE pruning. The code is available at\nhttps://github.com/ZunhaiSu/Super-Experts-Profilling.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23279v1", "cate": "cs.CL", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23242", "title": "Generalized Reinforcement Learning for Retriever-Specific Query Rewriter with Unstructured Real-World Documents", "authors": ["Sungguk Cha", "DongWook Kim", "Taeseung Hahn", "Mintae Kim", "Youngsub Han", "Byoung-Ki Jeon"], "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23242v1", "summary": "Retrieval-Augmented Generation (RAG) systems rely heavily on effective query\nformulation to unlock external knowledge, yet optimizing queries for diverse,\nunstructured real-world documents remains a challenge. We introduce\n\\textbf{RL-QR}, a reinforcement learning framework for retriever-specific query\nrewriting that eliminates the need for human-annotated datasets and extends\napplicability to both text-only and multi-modal databases. By synthesizing\nscenario-question pairs and leveraging Generalized Reward Policy Optimization\n(GRPO), RL-QR trains query rewriters tailored to specific retrievers, enhancing\nretrieval performance across varied domains. Experiments on industrial in-house\ndata demonstrate significant improvements, with\n$\\text{RL-QR}_{\\text{multi-modal}}$ achieving an 11\\% relative gain in NDCG@3\nfor multi-modal RAG and $\\text{RL-QR}_{\\text{lexical}}$ yielding a 9\\% gain for\nlexical retrievers. However, challenges persist with semantic and hybrid\nretrievers, where rewriters failed to improve performance, likely due to\ntraining misalignments. Our findings highlight RL-QR's potential to\nrevolutionize query optimization for RAG systems, offering a scalable,\nannotation-free solution for real-world retrieval tasks, while identifying\navenues for further refinement in semantic retrieval contexts.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23242v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23272", "title": "Towards Affordable Tumor Segmentation and Visualization for 3D Breast MRI Using SAM2", "authors": ["Solha Kang", "Eugene Kim", "Joris Vankerschaver", "Utku Ozbulak"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted for publication in the 28th International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI), 2nd Deep Breast Workshop on AI and Imaging for Diagnostic and Treatment Challenges in Breast Care (DeepBreath), 2025", "url": "http://arxiv.org/abs/2507.23272v1", "summary": "Breast MRI provides high-resolution volumetric imaging critical for tumor\nassessment and treatment planning, yet manual interpretation of 3D scans\nremains labor-intensive and subjective. While AI-powered tools hold promise for\naccelerating medical image analysis, adoption of commercial medical AI products\nremains limited in low- and middle-income countries due to high license costs,\nproprietary software, and infrastructure demands. In this work, we investigate\nwhether the Segment Anything Model 2 (SAM2) can be adapted for low-cost,\nminimal-input 3D tumor segmentation in breast MRI. Using a single bounding box\nannotation on one slice, we propagate segmentation predictions across the 3D\nvolume using three different slice-wise tracking strategies: top-to-bottom,\nbottom-to-top, and center-outward. We evaluate these strategies across a large\ncohort of patients and find that center-outward propagation yields the most\nconsistent and accurate segmentations. Despite being a zero-shot model not\ntrained for volumetric medical data, SAM2 achieves strong segmentation\nperformance under minimal supervision. We further analyze how segmentation\nperformance relates to tumor size, location, and shape, identifying key failure\nmodes. Our results suggest that general-purpose foundation models such as SAM2\ncan support 3D medical image analysis with minimal supervision, offering an\naccessible and affordable alternative for resource-constrained settings.", "comment": "Accepted for publication in the 28th International Conference on\n  Medical Image Computing and Computer Assisted Intervention (MICCAI), 2nd Deep\n  Breast Workshop on AI and Imaging for Diagnostic and Treatment Challenges in\n  Breast Care (DeepBreath), 2025", "pdf_url": "http://arxiv.org/pdf/2507.23272v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23319", "title": "What's Taboo for You? - An Empirical Evaluation of LLMs Behavior Toward Sensitive Content", "authors": ["Alfio Ferrara", "Sergio Picascia", "Laura Pinnavaia", "Vojimir Ranitovic", "Elisabetta Rocchetti", "Alice Tuveri"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23319v1", "summary": "Proprietary Large Language Models (LLMs) have shown tendencies toward\npoliteness, formality, and implicit content moderation. While previous research\nhas primarily focused on explicitly training models to moderate and detoxify\nsensitive content, there has been limited exploration of whether LLMs\nimplicitly sanitize language without explicit instructions. This study\nempirically analyzes the implicit moderation behavior of GPT-4o-mini when\nparaphrasing sensitive content and evaluates the extent of sensitivity shifts.\nOur experiments indicate that GPT-4o-mini systematically moderates content\ntoward less sensitive classes, with substantial reductions in derogatory and\ntaboo language. Also, we evaluate the zero-shot capabilities of LLMs in\nclassifying sentence sensitivity, comparing their performances against\ntraditional methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23319v1", "cate": "cs.CL", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23245", "title": "Automated Mapping the Pathways of Cranial Nerve II, III, V, and VII/VIII: A Multi-Parametric Multi-Stage Diffusion Tractography Atlas", "authors": ["Lei Xie", "Jiahao Huang", "Jiawei Zhang", "Jianzhong He", "Yiang Pan", "Guoqiang Xie", "Mengjun Li", "Qingrun Zeng", "Mingchu Li", "Yuanjing Feng"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23245v1", "summary": "Cranial nerves (CNs) play a crucial role in various essential functions of\nthe human brain, and mapping their pathways from diffusion MRI (dMRI) provides\nvaluable preoperative insights into the spatial relationships between\nindividual CNs and key tissues. However, mapping a comprehensive and detailed\nCN atlas is challenging because of the unique anatomical structures of each CN\npair and the complexity of the skull base environment.In this work, we present\nwhat we believe to be the first study to develop a comprehensive diffusion\ntractography atlas for automated mapping of CN pathways in the human brain. The\nCN atlas is generated by fiber clustering by using the streamlines generated by\nmulti-parametric fiber tractography for each pair of CNs. Instead of disposable\nclustering, we explore a new strategy of multi-stage fiber clustering for\nmultiple analysis of approximately 1,000,000 streamlines generated from the 50\nsubjects from the Human Connectome Project (HCP). Quantitative and visual\nexperiments demonstrate that our CN atlas achieves high spatial correspondence\nwith expert manual annotations on multiple acquisition sites, including the HCP\ndataset, the Multi-shell Diffusion MRI (MDM) dataset and two clinical cases of\npituitary adenoma patients. The proposed CN atlas can automatically identify 8\nfiber bundles associated with 5 pairs of CNs, including the optic nerve CN II,\noculomotor nerve CN III, trigeminal nerve CN V and facial-vestibulocochlear\nnerve CN VII/VIII, and its robustness is demonstrated experimentally. This work\ncontributes to the field of diffusion imaging by facilitating more efficient\nand automated mapping the pathways of multiple pairs of CNs, thereby enhancing\nthe analysis and understanding of complex brain structures through\nvisualization of their spatial relationships with nearby anatomy.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23245v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23291", "title": "Evaluating the Dynamics of Membership Privacy in Deep Learning", "authors": ["Yuetian Chen", "Zhiqi Wang", "Nathalie Baracaldo", "Swanand Ravindra Kadhe", "Lei Yu"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23291v1", "summary": "Membership inference attacks (MIAs) pose a critical threat to the privacy of\ntraining data in deep learning. Despite significant progress in attack\nmethodologies, our understanding of when and how models encode membership\ninformation during training remains limited. This paper presents a dynamic\nanalytical framework for dissecting and quantifying privacy leakage dynamics at\nthe individual sample level. By tracking per-sample vulnerabilities on an\nFPR-TPR plane throughout training, our framework systematically measures how\nfactors such as dataset complexity, model architecture, and optimizer choice\ninfluence the rate and severity at which samples become vulnerable. Crucially,\nwe discover a robust correlation between a sample's intrinsic learning\ndifficulty, and find that the privacy risk of samples highly vulnerable in the\nfinal trained model is largely determined early during training. Our results\nthus provide a deeper understanding of how privacy risks dynamically emerge\nduring training, laying the groundwork for proactive, privacy-aware model\ntraining strategies.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23291v1", "cate": "cs.LG", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23334", "title": "MUST-RAG: MUSical Text Question Answering with Retrieval Augmented Generation", "authors": ["Daeyong Kwon", "SeungHeon Doh", "Juhan Nam"], "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      8 pages, 2 figures", "url": "http://arxiv.org/abs/2507.23334v1", "summary": "Recent advancements in Large language models (LLMs) have demonstrated\nremarkable capabilities across diverse domains. While they exhibit strong\nzero-shot performance on various tasks, LLMs' effectiveness in music-related\napplications remains limited due to the relatively small proportion of\nmusic-specific knowledge in their training data. To address this limitation, we\npropose MusT-RAG, a comprehensive framework based on Retrieval Augmented\nGeneration (RAG) to adapt general-purpose LLMs for text-only music question\nanswering (MQA) tasks. RAG is a technique that provides external knowledge to\nLLMs by retrieving relevant context information when generating answers to\nquestions. To optimize RAG for the music domain, we (1) propose MusWikiDB, a\nmusic-specialized vector database for the retrieval stage, and (2) utilizes\ncontext information during both inference and fine-tuning processes to\neffectively transform general-purpose LLMs into music-specific models. Our\nexperiment demonstrates that MusT-RAG significantly outperforms traditional\nfine-tuning approaches in enhancing LLMs' music domain adaptation capabilities,\nshowing consistent improvements across both in-domain and out-of-domain MQA\nbenchmarks. Additionally, our MusWikiDB proves substantially more effective\nthan general Wikipedia corpora, delivering superior performance and\ncomputational efficiency.", "comment": "8 pages, 2 figures", "pdf_url": "http://arxiv.org/pdf/2507.23334v1", "cate": "cs.CL", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23251", "title": "A Deep Dive into Generic Object Tracking: A Survey", "authors": ["Fereshteh Aghaee Meibodi", "Shadi Alijani", "Homayoun Najjaran"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      55 pages, 29 figures, 9 tables", "url": "http://arxiv.org/abs/2507.23251v1", "summary": "Generic object tracking remains an important yet challenging task in computer\nvision due to complex spatio-temporal dynamics, especially in the presence of\nocclusions, similar distractors, and appearance variations. Over the past two\ndecades, a wide range of tracking paradigms, including Siamese-based trackers,\ndiscriminative trackers, and, more recently, prominent transformer-based\napproaches, have been introduced to address these challenges. While a few\nexisting survey papers in this field have either concentrated on a single\ncategory or widely covered multiple ones to capture progress, our paper\npresents a comprehensive review of all three categories, with particular\nemphasis on the rapidly evolving transformer-based methods. We analyze the core\ndesign principles, innovations, and limitations of each approach through both\nqualitative and quantitative comparisons. Our study introduces a novel\ncategorization and offers a unified visual and tabular comparison of\nrepresentative methods. Additionally, we organize existing trackers from\nmultiple perspectives and summarize the major evaluation benchmarks,\nhighlighting the fast-paced advancements in transformer-based tracking driven\nby their robust spatio-temporal modeling capabilities.", "comment": "55 pages, 29 figures, 9 tables", "pdf_url": "http://arxiv.org/pdf/2507.23251v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23315", "title": "Impact of Hyperparameter Optimization on the Accuracy of Lightweight Deep Learning Models for Real-Time Image Classification", "authors": ["Vineet Kumar Rakesh", "Soumya Mazumdar", "Tapas Samanta", "Sarbajit Pal", "Amitabha Das"], "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      13 pages, 4 figures, 4 tables. Includes ablation study and evaluation on 7 lightweight deep learning models. Code and logs available at this https URL", "url": "http://arxiv.org/abs/2507.23315v1", "summary": "Lightweight convolutional and transformer-based models have become vital for\nreal-time image classification in resource-constrained applications, such as\nembedded systems and edge devices. This work analyzes the influence of\nhyperparameter adjustment on the accuracy and convergence behavior of seven\nefficient deep learning architectures: EfficientNetV2-S, ConvNeXt-T, MobileViT\nv2 (XXS/XS/S), MobileNetV3-L, TinyViT-21M, and RepVGG-A2. All models are\ntrained on the ImageNet-1K dataset under consistent training settings, with an\nemphasis on real-time practicality. An comprehensive ablation study is\nundertaken to separate the effect of critical hyperparameters, including\nlearning rate schedules, batch sizes, input resolution, data augmentation,\nregularization approaches, and optimizer choice. To assess appropriateness for\nreal-time applications, each model is assessed not only in terms of Top-1 and\nTop-5 classification accuracy, but also in terms of inference time, parameter\ncount, model size, and frames-per-second (FPS) on a GPU-accelerated edge\ndeployment simulation. Results demonstrate that cosine learning rate decay and\nadjustable batch size may greatly boost both accuracy and convergence speed,\nwhile keeping low latency and memory cost. Notably, RepVGG-A2 achieves over 80%\nTop-1 accuracy with efficient inference performance, offering a compelling\nbalance between accuracy and deployment cost for VGG-style models. The results\ngive practical guidance for constructing resource-efficient deep learning\nmodels appropriate for real-time image processing pipelines. All code and\ntraining logs are publicly accessible at\nhttps://github.com/VineetKumarRakesh/lcnn-opt.", "comment": "13 pages, 4 figures, 4 tables. Includes ablation study and evaluation\n  on 7 lightweight deep learning models. Code and logs available at\n  https://github.com/VineetKumarRakesh/lcnn-opt", "pdf_url": "http://arxiv.org/pdf/2507.23315v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23382", "title": "MPCC: A Novel Benchmark for Multimodal Planning with Complex Constraints in Multimodal Large Language Models", "authors": ["Yiyan Ji", "Haoran Chen", "Qiguang Chen", "Chengyue Wu", "Libo Qin", "Wanxiang Che"], "categories": ["cs.CL", "cs.AI", "cs.CV", "I.2.8; I.2.10"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Accepted to ACM Multimedia 2025", "url": "http://arxiv.org/abs/2507.23382v1", "summary": "Multimodal planning capabilities refer to the ability to predict, reason, and\ndesign steps for task execution with multimodal context, which is essential for\ncomplex reasoning and decision-making across multiple steps. However, current\nbenchmarks face two key challenges: (1) they cannot directly assess multimodal\nreal-world planning capabilities, and (2) they lack constraints or implicit\nconstraints across modalities. To address these issues, we introduce Multimodal\nPlanning with Complex Constraints (MPCC), the first benchmark to systematically\nevaluate MLLMs' ability to handle multimodal constraints in planning. To\naddress the first challenge, MPCC focuses on three real-world tasks: Flight\nPlanning, Calendar Planning, and Meeting Planning. To solve the second\nchallenge, we introduce complex constraints (e.g. budget, temporal, and\nspatial) in these tasks, with graded difficulty levels (EASY, MEDIUM, HARD) to\nseparate constraint complexity from search space expansion. Experiments on 13\nadvanced MLLMs reveal significant challenges: closed-source models achieve only\n21.3% feasible plans, while open-source models average below 11%. Additionally,\nwe observe that MLLMs are highly sensitive to constraint complexity and that\ntraditional multimodal prompting strategies fail in multi-constraint scenarios.\nOur work formalizes multimodal constraints in planning, provides a rigorous\nevaluation framework, and highlights the need for advancements in\nconstraint-aware reasoning for real-world MLLM applications.", "comment": "Accepted to ACM Multimedia 2025", "pdf_url": "http://arxiv.org/pdf/2507.23382v1", "cate": "cs.CL", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23253", "title": "Towards Measuring and Modeling Geometric Structures in Time Series Forecasting via Image Modality", "authors": ["Mingyang Yu", "Xiahui Guo", "Peng chen", "Zhenkai Li", "Yang Shu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23253v1", "summary": "Time Series forecasting is critical in diverse domains such as weather\nforecasting, financial investment, and traffic management. While traditional\nnumerical metrics like mean squared error (MSE) can quantify point-wise\naccuracy, they fail to evaluate the geometric structure of time series data,\nwhich is essential to understand temporal dynamics. To address this issue, we\npropose the time series Geometric Structure Index (TGSI), a novel evaluation\nmetric that transforms time series into images to leverage their inherent\ntwo-dimensional geometric representations. However, since the image\ntransformation process is non-differentiable, TGSI cannot be directly\nintegrated as a training loss. We further introduce the Shape-Aware Temporal\nLoss (SATL), a multi-component loss function operating in the time series\nmodality to bridge this gap and enhance structure modeling during training.\nSATL combines three components: a first-order difference loss that measures\nstructural consistency through the MSE between first-order differences, a\nfrequency domain loss that captures essential periodic patterns using the Fast\nFourier Transform while minimizing noise, and a perceptual feature loss that\nmeasures geometric structure difference in time-series by aligning temporal\nfeatures with geometric structure features through a pre-trained temporal\nfeature extractor and time-series image autoencoder. Experiments across\nmultiple datasets demonstrate that models trained with SATL achieve superior\nperformance in both MSE and the proposed TGSI metrics compared to baseline\nmethods, without additional computational cost during inference.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23253v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23777", "title": "XSpecMesh: Quality-Preserving Auto-Regressive Mesh Generation Acceleration via Multi-Head Speculative Decoding", "authors": ["Dian Chen", "Yansong Qu", "Xinyang Li", "Ming Li", "Shengchuan Zhang"], "categories": ["cs.GR", "cs.CV", "cs.LG"], "primary_category": "Subjects:       Graphics (cs.GR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23777v1", "summary": "Current auto-regressive models can generate high-quality, topologically\nprecise meshes; however, they necessitate thousands-or even tens of\nthousands-of next-token predictions during inference, resulting in substantial\nlatency. We introduce XSpecMesh, a quality-preserving acceleration method for\nauto-regressive mesh generation models. XSpecMesh employs a lightweight,\nmulti-head speculative decoding scheme to predict multiple tokens in parallel\nwithin a single forward pass, thereby accelerating inference. We further\npropose a verification and resampling strategy: the backbone model verifies\neach predicted token and resamples any tokens that do not meet the quality\ncriteria. In addition, we propose a distillation strategy that trains the\nlightweight decoding heads by distilling from the backbone model, encouraging\ntheir prediction distributions to align and improving the success rate of\nspeculative predictions. Extensive experiments demonstrate that our method\nachieves a 1.7x speedup without sacrificing generation quality. Our code will\nbe released.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23777v1", "cate": "cs.GR", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23318", "title": "FastDriveVLA: Efficient End-to-End Driving via Plug-and-Play Reconstruction-based Token Pruning", "authors": ["Jiajun Cao", "Qizhe Zhang", "Peidong Jia", "Xuhui Zhao", "Bo Lan", "Xiaoan Zhang", "Xiaobao Wei", "Sixiang Chen", "Zhuo Li", "Yang Wang", "Liyun Li", "Xianming Liu", "Ming Lu", "Shanghang Zhang"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      9 pages, 5 figures", "url": "http://arxiv.org/abs/2507.23318v1", "summary": "Vision-Language-Action (VLA) models have demonstrated significant potential\nin complex scene understanding and action reasoning, leading to their\nincreasing adoption in end-to-end autonomous driving systems. However, the long\nvisual tokens of VLA models greatly increase computational costs. Current\nvisual token pruning methods in Vision-Language Models (VLM) rely on either\nvisual token similarity or visual-text attention, but both have shown poor\nperformance in autonomous driving scenarios. Given that human drivers\nconcentrate on relevant foreground areas while driving, we assert that\nretaining visual tokens containing this foreground information is essential for\neffective decision-making. Inspired by this, we propose FastDriveVLA, a novel\nreconstruction-based vision token pruning framework designed specifically for\nautonomous driving. FastDriveVLA includes a plug-and-play visual token pruner\ncalled ReconPruner, which prioritizes foreground information through MAE-style\npixel reconstruction. A novel adversarial foreground-background reconstruction\nstrategy is designed to train ReconPruner for the visual encoder of VLA models.\nOnce trained, ReconPruner can be seamlessly applied to different VLA models\nwith the same visual encoder without retraining. To train ReconPruner, we also\nintroduce a large-scale dataset called nuScenes-FG, consisting of 241K\nimage-mask pairs with annotated foreground regions. Our approach achieves\nstate-of-the-art results on the nuScenes closed-loop planning benchmark across\ndifferent pruning ratios.", "comment": "9 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2507.23318v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23386", "title": "Causal2Vec: Improving Decoder-only LLMs as Versatile Embedding Models", "authors": ["Ailiang Lin", "Zhuoyun Li", "Kotaro Funakoshi"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23386v1", "summary": "Decoder-only large language models (LLMs) are increasingly used to build\nembedding models that effectively encode the semantic information of natural\nlanguage texts into dense vector representations for various embedding tasks.\nHowever, many existing methods primarily focus on removing the causal attention\nmask in LLMs to enable bidirectional attention, potentially undermining the\nmodel's ability to extract semantic information acquired during pretraining.\nAdditionally, leading unidirectional approaches often rely on extra input text\nto overcome the inherent limitations of causal attention, inevitably increasing\ncomputational costs. In this work, we propose Causal2Vec, a general-purpose\nembedding model tailored to enhance the performance of decoder-only LLMs\nwithout altering their original architectures or introducing significant\ncomputational overhead. Specifically, we first employ a lightweight BERT-style\nmodel to pre-encode the input text into a single Contextual token, which is\nthen prepended to the LLM's input sequence, allowing each token to capture\ncontextualized information even without attending to future tokens.\nFurthermore, to mitigate the recency bias introduced by last-token pooling and\nhelp LLMs better leverage the semantic information encoded in the Contextual\ntoken, we concatenate the last hidden states of Contextual and EOS tokens as\nthe final text embedding. In practice, Causal2Vec achieves state-of-the-art\nperformance on the Massive Text Embeddings Benchmark (MTEB) among models\ntrained solely on publicly available retrieval datasets, while reducing the\nrequired sequence length by up to 85% and inference time by up to 82% compared\nto best-performing methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23386v1", "cate": "cs.CL", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23263", "title": "Learning Semantic-Aware Threshold for Multi-Label Image Recognition with Partial Labels", "authors": ["Haoxian Ruan", "Zhihua Xu", "Zhijing Yang", "Guang Ma", "Jieming Xie", "Changxiang Fan", "Tianshui Chen"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      15 pages, 13 figures, publish to ESWA (Expert Systems With Applications)", "url": "http://arxiv.org/abs/2507.23263v1", "summary": "Multi-label image recognition with partial labels (MLR-PL) is designed to\ntrain models using a mix of known and unknown labels. Traditional methods rely\non semantic or feature correlations to create pseudo-labels for unidentified\nlabels using pre-set thresholds. This approach often overlooks the varying\nscore distributions across categories, resulting in inaccurate and incomplete\npseudo-labels, thereby affecting performance. In our study, we introduce the\nSemantic-Aware Threshold Learning (SATL) algorithm. This innovative approach\ncalculates the score distribution for both positive and negative samples within\neach category and determines category-specific thresholds based on these\ndistributions. These distributions and thresholds are dynamically updated\nthroughout the learning process. Additionally, we implement a differential\nranking loss to establish a significant gap between the score distributions of\npositive and negative samples, enhancing the discrimination of the thresholds.\nComprehensive experiments and analysis on large-scale multi-label datasets,\nsuch as Microsoft COCO and VG-200, demonstrate that our method significantly\nimproves performance in scenarios with limited labels.", "comment": "15 pages, 13 figures, publish to ESWA (Expert Systems With\n  Applications)", "pdf_url": "http://arxiv.org/pdf/2507.23263v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23485", "title": "Rational complex Bezier curves", "authors": ["A. Canton", "L. Fernandez-Jambrina", "M. J. Vazquez-Gallo"], "categories": ["math.NA", "cs.GR", "cs.NA", "65D17, 68U07"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "Comments:      9 pages, 6 figures", "url": "http://arxiv.org/abs/2507.23485v1", "summary": "In this paper we develop the formalism of rational complex Bezier curves.\nThis framework is a simple extension of the CAD paradigm, since it describes\narc of curves in terms of control polygons and weights, which are extended to\ncomplex values. One of the major advantages of this extension is that we may\nmake use of two different groups of projective transformations. Besides the\ngroup of projective transformations of the real plane, we have the group of\ncomplex projective transformations. This allows us to apply useful\ntransformations like the geometric inversion to curves in design. In addition\nto this, the use of the complex formulation allows to lower the degree of the\ncurves in some cases. This can be checked using the resultant of two\npolynomials and provides a simple formula for determining whether a rational\ncubic curve is a conic or not. Examples of application of the formalism to\nclassical curves are included.", "comment": "9 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2507.23485v1", "cate": "math.NA", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23350", "title": "Multi-Waypoint Path Planning and Motion Control for Non-holonomic Mobile Robots in Agricultural Applications", "authors": ["Mahmoud Ghorab", "Matthias Lorenzen"], "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      6 pages", "url": "http://arxiv.org/abs/2507.23350v1", "summary": "There is a growing demand for autonomous mobile robots capable of navigating\nunstructured agricultural environments. Tasks such as weed control in meadows\nrequire efficient path planning through an unordered set of coordinates while\nminimizing travel distance and adhering to curvature constraints to prevent\nsoil damage and protect vegetation. This paper presents an integrated\nnavigation framework combining a global path planner based on the Dubins\nTraveling Salesman Problem (DTSP) with a Nonlinear Model Predictive Control\n(NMPC) strategy for local path planning and control. The DTSP generates a\nminimum-length, curvature-constrained path that efficiently visits all targets,\nwhile the NMPC leverages this path to compute control signals to accurately\nreach each waypoint. The system's performance was validated through comparative\nsimulation analysis on real-world field datasets, demonstrating that the\ncoupled DTSP-based planner produced smoother and shorter paths, with a\nreduction of about 16% in the provided scenario, compared to decoupled methods.\nBased thereon, the NMPC controller effectively steered the robot to the desired\nwaypoints, while locally optimizing the trajectory and ensuring adherence to\nconstraints. These findings demonstrate the potential of the proposed framework\nfor efficient autonomous navigation in agricultural environments.", "comment": "6 pages", "pdf_url": "http://arxiv.org/pdf/2507.23350v1", "cate": "cs.RO", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23400", "title": "MRGSEM-Sum: An Unsupervised Multi-document Summarization Framework based on Multi-Relational Graphs and Structural Entropy Minimization", "authors": ["Yongbing Zhang", "Fang Nan", "Shengxiang Gao", "Yuxin Huang", "Kaiwen Tan", "Zhengtao Yu"], "categories": ["cs.CL", "cs.IR"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23400v1", "summary": "The core challenge faced by multi-document summarization is the complexity of\nrelationships among documents and the presence of information redundancy. Graph\nclustering is an effective paradigm for addressing this issue, as it models the\ncomplex relationships among documents using graph structures and reduces\ninformation redundancy through clustering, achieving significant research\nprogress. However, existing methods often only consider single-relational\ngraphs and require a predefined number of clusters, which hinders their ability\nto fully represent rich relational information and adaptively partition\nsentence groups to reduce redundancy. To overcome these limitations, we propose\nMRGSEM-Sum, an unsupervised multi-document summarization framework based on\nmulti-relational graphs and structural entropy minimization. Specifically, we\nconstruct a multi-relational graph that integrates semantic and discourse\nrelations between sentences, comprehensively modeling the intricate and dynamic\nconnections among sentences across documents. We then apply a two-dimensional\nstructural entropy minimization algorithm for clustering, automatically\ndetermining the optimal number of clusters and effectively organizing sentences\ninto coherent groups. Finally, we introduce a position-aware compression\nmechanism to distill each cluster, generating concise and informative\nsummaries. Extensive experiments on four benchmark datasets (Multi-News,\nDUC-2004, PubMed, and WikiSum) demonstrate that our approach consistently\noutperforms previous unsupervised methods and, in several cases, achieves\nperformance comparable to supervised models and large language models. Human\nevaluation demonstrates that the summaries generated by MRGSEM-Sum exhibit high\nconsistency and coverage, approaching human-level quality.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23400v1", "cate": "cs.CL", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23268", "title": "PixNerd: Pixel Neural Field Diffusion", "authors": ["Shuai Wang", "Ziteng Gao", "Chenhui Zhu", "Weilin Huang", "Limin Wang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      a single-scale, single-stage, efficient, end-to-end pixel space diffusion model", "url": "http://arxiv.org/abs/2507.23268v1", "summary": "The current success of diffusion transformers heavily depends on the\ncompressed latent space shaped by the pre-trained variational autoencoder(VAE).\nHowever, this two-stage training paradigm inevitably introduces accumulated\nerrors and decoding artifacts. To address the aforementioned problems,\nresearchers return to pixel space at the cost of complicated cascade pipelines\nand increased token complexity. In contrast to their efforts, we propose to\nmodel the patch-wise decoding with neural field and present a single-scale,\nsingle-stage, efficient, end-to-end solution, coined as pixel neural field\ndiffusion~(PixelNerd). Thanks to the efficient neural field representation in\nPixNerd, we directly achieved 2.15 FID on ImageNet $256\\times256$ and 2.84 FID\non ImageNet $512\\times512$ without any complex cascade pipeline or VAE. We also\nextend our PixNerd framework to text-to-image applications. Our PixNerd-XXL/16\nachieved a competitive 0.73 overall score on the GenEval benchmark and 80.9\noverall score on the DPG benchmark.", "comment": "a single-scale, single-stage, efficient, end-to-end pixel space\n  diffusion model", "pdf_url": "http://arxiv.org/pdf/2507.23268v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2401.13639", "title": "Winding Clearness for Differentiable Point Cloud Optimization", "authors": ["Dong Xiao", "Yueji Ma", "Zuoqiang Shi", "Shiqing Xin", "Wenping Wang", "Bailin Deng", "Bin Wang"], "categories": ["cs.GR"], "primary_category": "Subjects:       Graphics (cs.GR)", "pdf_link": null, "comments": "Comments:      Accepted by Computer-Aided Design through SPM 2025", "url": "http://arxiv.org/abs/2401.13639v2", "summary": "We propose to explore the properties of raw point clouds through the\n\\emph{winding clearness}, a concept we first introduce for measuring the\nclarity of the interior/exterior relationships represented by the winding\nnumber field of the point cloud. In geometric modeling, the winding number is a\npowerful tool for distinguishing the interior and exterior of a given surface\n$\\partial \\Omega$, and it has been previously used for point normal orientation\nand surface reconstruction. In this work, we introduce a novel approach to\nevaluate and optimize the quality of point clouds based on the winding\nclearness. We observe that point clouds with less noise generally exhibit\nbetter winding clearness. Accordingly, we propose an objective function that\nquantifies the error in winding clearness, solely utilizing the coordinates of\nthe point clouds. Moreover, we demonstrate that the winding clearness error is\ndifferentiable and can serve as a loss function in point cloud processing. We\npresent this observation from two aspects: 1) We update the coordinates of the\npoints by back-propagating the loss function for individual point clouds,\nresulting in an overall improvement without involving a neural network. 2) We\nincorporate winding clearness as a geometric constraint in the diffusion-based\n3D generative model and update the network parameters to generate point clouds\nwith less noise. Experimental results demonstrate the effectiveness of\noptimizing the winding clearness in enhancing the point cloud quality. Notably,\nour method exhibits superior performance in handling noisy point clouds with\nthin structures, highlighting the benefits of the global perspective enabled by\nthe winding number.", "comment": "Accepted by Computer-Aided Design through SPM 2025", "pdf_url": "http://arxiv.org/pdf/2401.13639v2", "cate": "cs.GR", "date": "2024-01-24", "updated": "2025-07-31"}
{"id": "2507.23356", "title": "Quality Evaluation of COBOL to Java Code Transformation", "authors": ["Shmulik Froimovich", "Raviv Gal", "Wesam Ibraheem", "Avi Ziv"], "categories": ["cs.SE", "cs.AI"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      Submitted to ASE 2025", "url": "http://arxiv.org/abs/2507.23356v1", "summary": "We present an automated evaluation system for assessing COBOL-to-Java code\ntranslation within IBM's watsonx Code Assistant for Z (WCA4Z). The system\naddresses key challenges in evaluating LLM-based translators, including model\nopacity and the complexity of translation quality assessment. Our approach\ncombines analytic checkers with LLM-as-a-judge (LaaJ) techniques to deliver\nscalable, multi-faceted evaluations. The system supports continuous integration\nworkflows, enables large-scale benchmarking, and reduces reliance on manual\nreview. We describe the system architecture, evaluation strategies, and\nreporting mechanisms that provide actionable insights for developers and\nproject managers, facilitating the evolution of high-quality, modernized\ncodebases.", "comment": "Submitted to ASE 2025", "pdf_url": "http://arxiv.org/pdf/2507.23356v1", "cate": "cs.SE", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23404", "title": "Enhanced Arabic Text Retrieval with Attentive Relevance Scoring", "authors": ["Salah Eddine Bekhouche", "Azeddine Benlamoudi", "Yazid Bounab", "Fadi Dornaika", "Abdenour Hadid"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23404v1", "summary": "Arabic poses a particular challenge for natural language processing (NLP) and\ninformation retrieval (IR) due to its complex morphology, optional diacritics\nand the coexistence of Modern Standard Arabic (MSA) and various dialects.\nDespite the growing global significance of Arabic, it is still underrepresented\nin NLP research and benchmark resources. In this paper, we present an enhanced\nDense Passage Retrieval (DPR) framework developed specifically for Arabic. At\nthe core of our approach is a novel Attentive Relevance Scoring (ARS) that\nreplaces standard interaction mechanisms with an adaptive scoring function that\nmore effectively models the semantic relevance between questions and passages.\nOur method integrates pre-trained Arabic language models and architectural\nrefinements to improve retrieval performance and significantly increase ranking\naccuracy when answering Arabic questions. The code is made publicly available\nat \\href{https://github.com/Bekhouche/APR}{GitHub}.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23404v1", "cate": "cs.CL", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23277", "title": "iLRM: An Iterative Large 3D Reconstruction Model", "authors": ["Gyeongjin Kang", "Seungtae Nam", "Xiangyu Sun", "Sameh Khamis", "Abdelrahman Mohamed", "Eunbyung Park"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Project page: this https URL", "url": "http://arxiv.org/abs/2507.23277v1", "summary": "Feed-forward 3D modeling has emerged as a promising approach for rapid and\nhigh-quality 3D reconstruction. In particular, directly generating explicit 3D\nrepresentations, such as 3D Gaussian splatting, has attracted significant\nattention due to its fast and high-quality rendering, as well as numerous\napplications. However, many state-of-the-art methods, primarily based on\ntransformer architectures, suffer from severe scalability issues because they\nrely on full attention across image tokens from multiple input views, resulting\nin prohibitive computational costs as the number of views or image resolution\nincreases. Toward a scalable and efficient feed-forward 3D reconstruction, we\nintroduce an iterative Large 3D Reconstruction Model (iLRM) that generates 3D\nGaussian representations through an iterative refinement mechanism, guided by\nthree core principles: (1) decoupling the scene representation from input-view\nimages to enable compact 3D representations; (2) decomposing fully-attentional\nmulti-view interactions into a two-stage attention scheme to reduce\ncomputational costs; and (3) injecting high-resolution information at every\nlayer to achieve high-fidelity reconstruction. Experimental results on widely\nused datasets, such as RE10K and DL3DV, demonstrate that iLRM outperforms\nexisting methods in both reconstruction quality and speed. Notably, iLRM\nexhibits superior scalability, delivering significantly higher reconstruction\nquality under comparable computational cost by efficiently leveraging a larger\nnumber of input views.", "comment": "Project page: https://gynjn.github.io/iLRM/", "pdf_url": "http://arxiv.org/pdf/2507.23277v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.21288", "title": "Learning Simulatable Models of Cloth with Spatially-varying Constitutive Properties", "authors": ["Guanxiong Chen", "Shashwat Suri", "Yuhao Wu", "Etienne Voulga", "David I. W. Levin", "Dinesh K. Pai"], "categories": ["cs.GR", "cs.AI"], "primary_category": "Subjects:       Graphics (cs.GR)", "pdf_link": null, "comments": "Comments:      Added middle name of Prof. Pai", "url": "http://arxiv.org/abs/2507.21288v2", "summary": "Materials used in real clothing exhibit remarkable complexity and spatial\nvariation due to common processes such as stitching, hemming, dyeing, printing,\npadding, and bonding. Simulating these materials, for instance using finite\nelement methods, is often computationally demanding and slow. Worse, such\nmethods can suffer from numerical artifacts called ``membrane locking'' that\nmakes cloth appear artificially stiff. Here we propose a general framework,\ncalled Mass-Spring Net, for learning a simple yet efficient surrogate model\nthat captures the effects of these complex materials using only motion\nobservations. The cloth is discretized into a mass-spring network with unknown\nmaterial parameters that are learned directly from the motion data, using a\nnovel force-and-impulse loss function. Our approach demonstrates the ability to\naccurately model spatially varying material properties from a variety of data\nsources, and immunity to membrane locking which plagues FEM-based simulations.\nCompared to graph-based networks and neural ODE-based architectures, our method\nachieves significantly faster training times, higher reconstruction accuracy,\nand improved generalization to novel dynamic scenarios.", "comment": "Added middle name of Prof. Pai", "pdf_url": "http://arxiv.org/pdf/2507.21288v2", "cate": "cs.GR", "date": "2025-07-28", "updated": "2025-07-30"}
{"id": "2507.23365", "title": "\"I made this (sort of)\": Negotiating authorship, confronting fraudulence, and exploring new musical spaces with prompt-based AI music generation", "authors": ["Bob L. T. Sturm"], "categories": ["cs.SD", "cs.AI", "eess.AS", "I.2; J.5"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23365v1", "summary": "I reflect on my experience creating two music albums centered on\nstate-of-the-art prompt-based AI music generation platforms. The first album\nexplicitly poses the question: What happens when I collide my junk mail with\nthese platforms? The second album is a direct response to the first, and toys\nwith the inability of state-of-the-art prompt-based AI music generation\nplatforms to generate music that is not ``practiced'', ``polished'', and\n``produced''. I seed a large language model (LLM) with information about these\nalbums and have it interview me, which results in the exploration of several\ndeeper questions: To what extent am I the author? Where am I in the resulting\nmusic? How is my musical identity changing as I am faced with machines that are\nin some ways far more talented than I? What new musical spaces does my work\nopen, for me or anyone/thing else? I conclude by reflecting on my reflections,\nas well as LLM-mediated self-reflection as method.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23365v1", "cate": "cs.SD", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23407", "title": "Beyond Passive Critical Thinking: Fostering Proactive Questioning to Enhance Human-AI Collaboration", "authors": ["Ante Wang", "Yujie Lin", "Jingyao Liu", "Suhang Wu", "Hao Liu", "Xinyan Xiao", "Jinsong Su"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23407v1", "summary": "Critical thinking is essential for building robust AI systems, preventing\nthem from blindly accepting flawed data or biased reasoning. However, prior\nwork has primarily focused on passive critical thinking, where models simply\nreject problematic queries without taking constructive steps to address user\nrequests. In this work, we introduce proactive critical thinking, a paradigm\nwhere models actively seek missing or clarifying information from users to\nresolve their queries better. To evaluate this capability, we present GSM-MC\nand GSM-MCE, two novel benchmarks based on GSM8K for assessing mathematical\nreasoning under incomplete or misleading conditions. GSM-MC contains 1,368 math\nproblems with a key variable deliberately removed, requiring models to identify\nand request the missing information. GSM-MCE further increases the difficulty\nby introducing irrelevant details to test robustness against distractions.\nExperiments on Qwen3 and Llama series models show that, while these models\nexcel in traditional reasoning tasks due to extensive post-training and\ninference-time scaling, they struggle with proactive critical thinking,\nespecially smaller ones. However, we demonstrate that reinforcement learning\n(RL) can significantly improve this ability. Using our enhanced RL algorithm,\nwe achieve substantial gains, boosting the Qwen3-1.7B's accuracy from 0.15% to\n73.98% on GSM-MC. We hope this work advances models that collaborate more\neffectively with users in problem-solving through proactive critical thinking.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23407v1", "cate": "cs.CL", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23278", "title": "UniLiP: Adapting CLIP for Unified Multimodal Understanding, Generation and Editing", "authors": ["Hao Tang", "Chenwei Xie", "Xiaoyi Bao", "Tingyu Weng", "Pandeng Li", "Yun Zheng", "Liwei Wang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23278v1", "summary": "In this paper, we propose UniLIP, which extends CLIP to reconstruction,\ngeneration and editing, thereby building a unified tokenizer upon its\nexceptional comprehension capabilities. Previous CLIP-based unified methods\noften require additional diffusion decoders or quantization to support\nreconstruction and generation tasks, leading to inconsistent reconstruction or\ndegradation of original comprehension performance.In contrast, we introduce a\ntwo-stage training scheme and a self-distillation strategy that progressively\nintegrates reconstruction capabilities into CLIP, allowing it to maintain\noriginal comprehension performance while achieving effective image\nreconstruction. Furthermore, we propose a dual-condition architecture to\nconnect the MLLM and diffusion transformer, using both learnable queries and\nthe last layer multimodal hidden states as joint conditions. This method not\nonly enables the utilization of the MLLM's strong reasoning capabilities in\ngeneration tasks, but also maximizes the exploitation of the rich information\nin UniLIP features during editing tasks. In text-to-image generation tasks,\nUniLIP obtains scores of 0.87 and 0.53 on GenEval and WISE benchmark\nrespectively, surpassing all previous unified models of similar scale. In image\nediting, UniLIP also achieves a score of 3.62 on the ImgEdit Benchmark,\nsurpassing recent state-of-the-art models such as BAGEL and UniWorld-V1. UniLIP\neffectively expand the application scope of CLIP, enabling continuous CLIP\nfeatures to not only serve as the optimal choice for understanding tasks but\nalso achieve highly competitive performance in generation and editing tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23278v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.01631", "title": "Tile and Slide : A New Framework for Scaling NeRF from Local to Global 3D Earth Observation", "authors": ["Camille Billouard", "Dawa Derksen", "Alexandre Constantin", "Bruno Vallet"], "categories": ["cs.CV", "cs.AI", "cs.GR", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted at ICCV 2025 Workshop 3D-VAST (From street to space: 3D Vision Across Altitudes). Our code will be made public after the conference at this https URL", "url": "http://arxiv.org/abs/2507.01631v2", "summary": "Neural Radiance Fields (NeRF) have recently emerged as a paradigm for 3D\nreconstruction from multiview satellite imagery. However, state-of-the-art NeRF\nmethods are typically constrained to small scenes due to the memory footprint\nduring training, which we study in this paper. Previous work on large-scale\nNeRFs palliate this by dividing the scene into NeRFs. This paper introduces\nSnake-NeRF, a framework that scales to large scenes. Our out-of-core method\neliminates the need to load all images and networks simultaneously, and\noperates on a single device. We achieve this by dividing the region of interest\ninto NeRFs that 3D tile without overlap. Importantly, we crop the images with\noverlap to ensure each NeRFs is trained with all the necessary pixels. We\nintroduce a novel $2\\times 2$ 3D tile progression strategy and segmented\nsampler, which together prevent 3D reconstruction errors along the tile edges.\nOur experiments conclude that large satellite images can effectively be\nprocessed with linear time complexity, on a single GPU, and without compromise\nin quality.", "comment": "Accepted at ICCV 2025 Workshop 3D-VAST (From street to space: 3D\n  Vision Across Altitudes). Our code will be made public after the conference\n  at https://github.com/Ellimac0/Snake-NeRF", "pdf_url": "http://arxiv.org/pdf/2507.01631v2", "cate": "cs.CV", "date": "2025-07-02", "updated": "2025-07-31"}
{"id": "2507.23370", "title": "Trae Agent: An LLM-based Agent for Software Engineering with Test-time Scaling", "authors": ["Trae Research Team", "Pengfei Gao", "Zhao Tian", "Xiangxin Meng", "Xinchen Wang", "Ruida Hu", "Yuanan Xiao", "Yizhou Liu", "Zhao Zhang", "Junjie Chen", "Cuiyun Gao", "Yun Lin", "Yingfei Xiong", "Chao Peng", "Xia Liu"], "categories": ["cs.SE", "cs.AI"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      Pengfei Gao and Zhao Tian contributed equally to this technical report", "url": "http://arxiv.org/abs/2507.23370v1", "summary": "Software issue resolution is a critical challenge in software engineering and\nhas garnered increasing attention in recent years. With the rapid advancement\nof large language models (LLMs), substantial progress has been made in\naddressing real-world software engineering tasks. Recent studies have\nintroduced ensemble reasoning techniques to enhance the performance of\nLLM-based issue resolution. However, existing prompting-based methods still\nface limitations in effectively exploring large ensemble spaces and lack the\ncapacity for repository-level understanding, both of which constrain their\noverall effectiveness. In this paper, we propose Trae Agent, the first\nagent-based ensemble reasoning approach for repository-level issue resolution.\nTrae Agent formulates our goal as an optimal solution search problem and\naddresses two key challenges, i.e., large ensemble spaces and repository-level\nunderstanding, through modular agents for generation, pruning, and selection.\nWe conduct extensive experiments using three leading LLMs on the widely-adopted\nSWE-bench benchmark, comparing Trae Agent against four state-of-the-art\nensemble reasoning techniques. Experimental results demonstrate that Trae Agent\nconsistently achieves superior performance, with an average improvement of\n10.22% over all baselines in terms of Pass@1. Trae Agent has achieved first\nplace on the SWE-bench Verified leaderboard, with a notable Pass@1 score of\n75.20%. We are pleased to release Trae Agent as an open-source project to\nsupport the research community, with all resources available at\nhttps://github.com/bytedance/trae-agent.", "comment": "Pengfei Gao and Zhao Tian contributed equally to this technical\n  report", "pdf_url": "http://arxiv.org/pdf/2507.23370v1", "cate": "cs.SE", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23465", "title": "Role-Aware Language Models for Secure and Contextualized Access Control in Organizations", "authors": ["Saeed Almheiri", "Yerulan Kongrat", "Adrian Santosh", "Ruslan Tasmukhanov", "Josemaria Vera", "Muhammad Dehan Al Kautsar", "Fajri Koto"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23465v1", "summary": "As large language models (LLMs) are increasingly deployed in enterprise\nsettings, controlling model behavior based on user roles becomes an essential\nrequirement. Existing safety methods typically assume uniform access and focus\non preventing harmful or toxic outputs, without addressing role-specific access\nconstraints. In this work, we investigate whether LLMs can be fine-tuned to\ngenerate responses that reflect the access privileges associated with different\norganizational roles. We explore three modeling strategies: a BERT-based\nclassifier, an LLM-based classifier, and role-conditioned generation. To\nevaluate these approaches, we construct two complementary datasets. The first\nis adapted from existing instruction-tuning corpora through clustering and role\nlabeling, while the second is synthetically generated to reflect realistic,\nrole-sensitive enterprise scenarios. We assess model performance across varying\norganizational structures and analyze robustness to prompt injection, role\nmismatch, and jailbreak attempts.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23465v1", "cate": "cs.CL", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23284", "title": "Bidirectional Likelihood Estimation with Multi-Modal Large Language Models for Text-Video Retrieval", "authors": ["Dohwan Ko", "Ji Soo Lee", "Minhyuk Choi", "Zihang Meng", "Hyunwoo J. Kim"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV 2025 Highlight", "url": "http://arxiv.org/abs/2507.23284v1", "summary": "Text-Video Retrieval aims to find the most relevant text (or video) candidate\ngiven a video (or text) query from large-scale online databases. Recent work\nleverages multi-modal large language models (MLLMs) to improve retrieval,\nespecially for long or complex query-candidate pairs. However, we observe that\nthe naive application of MLLMs, i.e., retrieval based on candidate likelihood,\nintroduces candidate prior bias, favoring candidates with inherently higher\npriors over those more relevant to the query. To this end, we propose a novel\nretrieval framework, Bidirectional Likelihood Estimation with MLLM (BLiM),\nwhich leverages both query and candidate likelihoods by training the model to\ngenerate text from a given video as well as video features from a given text.\nFurthermore, we introduce Candidate Prior Normalization (CPN), a simple yet\neffective training-free score calibration module designed to mitigate candidate\nprior bias in candidate likelihood. On four Text-Video Retrieval benchmarks,\nour BLiM equipped with CPN outperforms previous state-of-the-art models by 6.4\nR@1 on average, effectively alleviating candidate prior bias and emphasizing\nquery-candidate relevance. Our in-depth analysis across various multi-modal\ntasks beyond retrieval highlights the broad applicability of CPN which enhances\nvisual understanding by reducing reliance on textual priors. Code is available\nat https://github.com/mlvlab/BLiM.", "comment": "ICCV 2025 Highlight", "pdf_url": "http://arxiv.org/pdf/2507.23284v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23402", "title": "AGA: An adaptive group alignment framework for structured medical cross-modal representation learning", "authors": ["Wei Li", "Xun Gong", "Jiao Li", "Xiaobin Sun"], "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23402v1", "summary": "Learning medical visual representations from paired images and reports is a\npromising direction in representation learning. However, current\nvision-language pretraining methods in the medical domain often simplify\nclinical reports into single entities or fragmented tokens, ignoring their\ninherent structure. In addition, contrastive learning frameworks typically\ndepend on large quantities of hard negative samples, which is impractical for\nsmall-scale medical datasets. To tackle these challenges, we propose Adaptive\nGrouped Alignment (AGA), a new framework that captures structured semantics\nfrom paired medical images and reports. AGA introduces a bidirectional grouping\nmechanism based on a sparse similarity matrix. For each image-report pair, we\ncompute fine-grained similarities between text tokens and image patches. Each\ntoken selects its top-matching patches to form a visual group, and each patch\nselects its most related tokens to form a language group. To enable adaptive\ngrouping, we design two threshold gating modules, called Language Grouped\nThreshold Gate and Vision Grouped Threshold Gate, which learn grouping\nthresholds dynamically. Group representations are computed as weighted averages\nbased on similarity scores. To align each token with its group representation,\nwe introduce an Instance Aware Group Alignment loss that operates within each\nimage-text pair, removing the need for external negatives. Finally, a\nBidirectional Cross-modal Grouped Alignment module is applied to enhance\nfine-grained alignment between visual and linguistic group representations.\nExtensive experiments on public and private datasets show that our method\nachieves strong performance on image-text retrieval and classification tasks\nunder both fine-tuning and zero-shot settings.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23402v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23486", "title": "A Novel Evaluation Benchmark for Medical LLMs: Illuminating Safety and Effectiveness in Clinical Domains", "authors": ["Shirui Wang", "Zhihui Tang", "Huaxia Yang", "Qiuhong Gong", "Tiantian Gu", "Hongyang Ma", "Yongxin Wang", "Wubin Sun", "Zeliang Lian", "Kehang Mao", "Yinan Jiang", "Zhicheng Huang", "Lingyun Ma", "Wenjie Shen", "Yajie Ji", "Yunhui Tan", "Chunbo Wang", "Yunlu Gao", "Qianling Ye", "Rui Lin", "Mingyu Chen", "Lijuan Niu", "Zhihao Wang", "Peng Yu", "Mengran Lang", "Yue Liu", "Huimin Zhang", "Haitao Shen", "Long Chen", "Qiguang Zhao", "Si-Xuan Liu", "Lina Zhou", "Hua Gao", "Dongqiang Ye", "Lingmin Meng", "Youtao Yu", "Naixin Liang", "Jianxiong Wu"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23486v1", "summary": "Large language models (LLMs) hold promise in clinical decision support but\nface major challenges in safety evaluation and effectiveness validation. We\ndeveloped the Clinical Safety-Effectiveness Dual-Track Benchmark (CSEDB), a\nmultidimensional framework built on clinical expert consensus, encompassing 30\ncriteria covering critical areas like critical illness recognition, guideline\nadherence, and medication safety, with weighted consequence measures.\nThirty-two specialist physicians developed and reviewed 2,069 open-ended Q&A\nitems aligned with these criteria, spanning 26 clinical departments to simulate\nreal-world scenarios. Benchmark testing of six LLMs revealed moderate overall\nperformance (average total score 57.2%, safety 54.7%, effectiveness 62.3%),\nwith a significant 13.3% performance drop in high-risk scenarios (p < 0.0001).\nDomain-specific medical LLMs showed consistent performance advantages over\ngeneral-purpose models, with relatively higher top scores in safety (0.912) and\neffectiveness (0.861). The findings of this study not only provide a\nstandardized metric for evaluating the clinical application of medical LLMs,\nfacilitating comparative analyses, risk exposure identification, and\nimprovement directions across different scenarios, but also hold the potential\nto promote safer and more effective deployment of large language models in\nhealthcare environments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23486v1", "cate": "cs.CL", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23295", "title": "LED Benchmark: Diagnosing Structural Layout Errors for Document Layout Analysis", "authors": ["Inbum Heo", "Taewook Hwang", "Jeesu Jung", "Sangkeun Jung"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23295v1", "summary": "Recent advancements in Document Layout Analysis through Large Language Models\nand Multimodal Models have significantly improved layout detection. However,\ndespite these improvements, challenges remain in addressing critical structural\nerrors, such as region merging, splitting, and missing content. Conventional\nevaluation metrics like IoU and mAP, which focus primarily on spatial overlap,\nare insufficient for detecting these errors. To address this limitation, we\npropose Layout Error Detection (LED), a novel benchmark designed to evaluate\nthe structural robustness of document layout predictions. LED defines eight\nstandardized error types, and formulates three complementary tasks: error\nexistence detection, error type classification, and element-wise error type\nclassification. Furthermore, we construct LED-Dataset, a synthetic dataset\ngenerated by injecting realistic structural errors based on empirical\ndistributions from DLA models. Experimental results across a range of LMMs\nreveal that LED effectively differentiates structural understanding\ncapabilities, exposing modality biases and performance trade-offs not visible\nthrough traditional metrics.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23295v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23455", "title": "Machine learning and machine learned prediction in chest X-ray images", "authors": ["Shereiff Garrett", "Abhinav Adhikari", "Sarina Gautam", "DaShawn Marquis Morris", "Chandra Mani Adhikari"], "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      8 pages, 7 figures", "url": "http://arxiv.org/abs/2507.23455v1", "summary": "Machine learning and artificial intelligence are fast-growing fields of\nresearch in which data is used to train algorithms, learn patterns, and make\npredictions. This approach helps to solve seemingly intricate problems with\nsignificant accuracy without explicit programming by recognizing complex\nrelationships in data. Taking an example of 5824 chest X-ray images, we\nimplement two machine learning algorithms, namely, a baseline convolutional\nneural network (CNN) and a DenseNet-121, and present our analysis in making\nmachine-learned predictions in predicting patients with ailments. Both baseline\nCNN and DenseNet-121 perform very well in the binary classification problem\npresented in this work. Gradient-weighted class activation mapping shows that\nDenseNet-121 correctly focuses on essential parts of the input chest X-ray\nimages in its decision-making more than the baseline CNN.", "comment": "8 pages, 7 figures", "pdf_url": "http://arxiv.org/pdf/2507.23455v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23541", "title": "Med-R$^3$: Enhancing Medical Retrieval-Augmented Reasoning of LLMs via Progressive Reinforcement Learning", "authors": ["Keer Lu", "Zheng Liang", "Youquan Li", "Jiejun Tan", "Da Pan", "Shusen Zhang", "Guosheng Dong", "Huang Leng"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23541v1", "summary": "In medical scenarios, effectively retrieving external knowledge and\nleveraging it for rigorous logical reasoning is of significant importance.\nDespite their potential, existing work has predominantly focused on enhancing\neither retrieval or reasoning capabilities of the models in isolation, with\nlittle attention given to their joint optimization, which leads to limited\ncoordination between the two processes. Additionally, current methods rely\nheavily on supervised fine-tuning (SFT), which can cause models to memorize\nexisting problem-solving pathways, thereby restricting their generalization\nability when confronted with novel problem contexts. Furthermore, while some\nstudies have explored to improve retrieval-augmented reasoning in general\ndomains via reinforcement learning, their reward function designs do not\nadequately capture the specific demands of the medical domain. To address these\nchallenges, we introduce **Med-R$^3$**, a **Med**ical **R**etrieval-augmented\n**R**easoning framework driven by progressive **R**einforcement learning. In\nthis framework, we first develop the model's ability to perform logical\nreasoning over medical problems. Subsequently, on the basis of this foundation,\nwe adaptively optimize the retrieval capability to better align with the\ncharacteristics of knowledge corpus and external information utilization\nthroughout the reasoning process. Finally, we conduct joint optimization of the\nmodel's retrieval and reasoning coordination. Extensive experiments indicate\nthat **Med-R$^3$** could achieve state-of-the-art performances, with\nLLaMA3.1-8B-Instruct + Med-R$^3$ surpassing closed-sourced GPT-4o-mini by\n3.93\\% at a comparable parameter scale, while Qwen2.5-14B augmented with\nMed-R$^3$ shows a more substantial gain of 13.53\\%.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23541v1", "cate": "cs.CL", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23300", "title": "Training-free Geometric Image Editing on Diffusion Models", "authors": ["Hanshen Zhu", "Zhen Zhu", "Kaile Zhang", "Yiming Gong", "Yuliang Liu", "Xiang Bai"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      24 pages, 22 figures, ICCV", "url": "http://arxiv.org/abs/2507.23300v2", "summary": "We tackle the task of geometric image editing, where an object within an\nimage is repositioned, reoriented, or reshaped while preserving overall scene\ncoherence. Previous diffusion-based editing methods often attempt to handle all\nrelevant subtasks in a single step, proving difficult when transformations\nbecome large or structurally complex. We address this by proposing a decoupled\npipeline that separates object transformation, source region inpainting, and\ntarget region refinement. Both inpainting and refinement are implemented using\na training-free diffusion approach, FreeFine. In experiments on our new\nGeoBench benchmark, which contains both 2D and 3D editing scenarios, FreeFine\noutperforms state-of-the-art alternatives in image fidelity, and edit\nprecision, especially under demanding transformations. Code and benchmark are\navailable at: https://github.com/CIawevy/FreeFine", "comment": "Accepted by ICCV2025", "pdf_url": "http://arxiv.org/pdf/2507.23300v2", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-08-01"}
{"id": "2507.23459", "title": "KLAN: Kuaishou Landing-page Adaptive Navigator", "authors": ["Fan Li", "Chang Meng", "Jiaqi Fu", "Shuchang Liu", "Jiashuo Zhang", "Tianke Zhang", "Xueliang Wang", "Xiaoqiang Feng"], "categories": ["cs.IR", "cs.AI"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "Comments:      We propose PLPM, a new task for selecting optimal landing pages upon user entry. Our solution, KLAN, models static and dynamic user interests and is successfully deployed on Kuaishou, improving DAU and user lifetime", "url": "http://arxiv.org/abs/2507.23459v1", "summary": "Modern online platforms configure multiple pages to accommodate diverse user\nneeds. This multi-page architecture inherently establishes a two-stage\ninteraction paradigm between the user and the platform: (1) Stage I: page\nnavigation, navigating users to a specific page and (2) Stage II: in-page\ninteraction, where users engage with customized content within the specific\npage. While the majority of research has been focusing on the sequential\nrecommendation task that improves users' feedback in Stage II, there has been\nlittle investigation on how to achieve better page navigation in Stage I. To\nfill this gap, we formally define the task of Personalized Landing Page\nModeling (PLPM) into the field of recommender systems: Given a user upon app\nentry, the goal of PLPM is to proactively select the most suitable landing page\nfrom a set of candidates (e.g., functional tabs, content channels, or\naggregation pages) to optimize the short-term PDR metric and the long-term user\nengagement and satisfaction metrics, while adhering to industrial constraints.\nAdditionally, we propose KLAN (Kuaishou Landing-page Adaptive Navigator), a\nhierarchical solution framework designed to provide personalized landing pages\nunder the formulation of PLPM. KLAN comprises three key components: (1)\nKLAN-ISP captures inter-day static page preference; (2) KLAN-IIT captures\nintra-day dynamic interest transitions and (3) KLAN-AM adaptively integrates\nboth components for optimal navigation decisions. Extensive online experiments\nconducted on the Kuaishou platform demonstrate the effectiveness of KLAN,\nobtaining +0.205% and +0.192% improvements on in Daily Active Users (DAU) and\nuser Lifetime (LT). Our KLAN is ultimately deployed on the online platform at\nfull traffic, serving hundreds of millions of users. To promote further\nresearch in this important area, we will release our dataset and code upon\npaper acceptance.", "comment": "We propose PLPM, a new task for selecting optimal landing pages upon\n  user entry. Our solution, KLAN, models static and dynamic user interests and\n  is successfully deployed on Kuaishou, improving DAU and user lifetime", "pdf_url": "http://arxiv.org/pdf/2507.23459v1", "cate": "cs.IR", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23577", "title": "T-Detect: Tail-Aware Statistical Normalization for Robust Detection of Adversarial Machine-Generated Text", "authors": ["Alva West", "Luodan Zhang", "Liuliu Zhang", "Minjun Zhu", "Yixuan Weng", "Yue Zhang"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23577v1", "summary": "The proliferation of sophisticated text generation models necessitates the\ndevelopment of robust detection methods capable of identifying\nmachine-generated content, particularly text designed to evade detection\nthrough adversarial perturbations. Existing zero-shot detectors often rely on\nstatistical measures that implicitly assume Gaussian distributions, a premise\nthat falters when confronted with the heavy-tailed statistical artifacts\ncharacteristic of adversarial or non-native English texts. This paper\nintroduces T-Detect, a novel detection method that fundamentally redesigns the\nstatistical core of curvature-based detectors. Our primary innovation is the\nreplacement of standard Gaussian normalization with a heavy-tailed discrepancy\nscore derived from the Student's t-distribution. This approach is theoretically\ngrounded in the empirical observation that adversarial texts exhibit\nsignificant leptokurtosis, rendering traditional statistical assumptions\ninadequate. T-Detect computes a detection score by normalizing the\nlog-likelihood of a passage against the expected moments of a t-distribution,\nproviding superior resilience to statistical outliers. We validate our approach\non the challenging RAID benchmark for adversarial text and the comprehensive\nHART dataset. Experiments show that T-Detect provides a consistent performance\nuplift over strong baselines, improving AUROC by up to 3.9\\% in targeted\ndomains. When integrated into a two-dimensional detection framework (CT), our\nmethod achieves state-of-the-art performance, with an AUROC of 0.926 on the\nBooks domain of RAID. Our contributions are a new, theoretically-justified\nstatistical foundation for text detection, an ablation-validated method that\ndemonstrates superior robustness, and a comprehensive analysis of its\nperformance under adversarial conditions. Ours code are released at\nhttps://github.com/ResearAI/t-detect.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23577v1", "cate": "cs.CL", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23307", "title": "ST-SAM: SAM-Driven Self-Training Framework for Semi-Supervised Camouflaged Object Detection", "authors": ["Xihang Hu", "Fuming Sun", "Jiazhe Liu", "Feilong Xu", "Xiaoli Zhang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      10 pages, 6 figures, ACM MM 2025", "url": "http://arxiv.org/abs/2507.23307v1", "summary": "Semi-supervised Camouflaged Object Detection (SSCOD) aims to reduce reliance\non costly pixel-level annotations by leveraging limited annotated data and\nabundant unlabeled data. However, existing SSCOD methods based on\nTeacher-Student frameworks suffer from severe prediction bias and error\npropagation under scarce supervision, while their multi-network architectures\nincur high computational overhead and limited scalability. To overcome these\nlimitations, we propose ST-SAM, a highly annotation-efficient yet concise\nframework that breaks away from conventional SSCOD constraints. Specifically,\nST-SAM employs Self-Training strategy that dynamically filters and expands\nhigh-confidence pseudo-labels to enhance a single-model architecture, thereby\nfundamentally circumventing inter-model prediction bias. Furthermore, by\ntransforming pseudo-labels into hybrid prompts containing domain-specific\nknowledge, ST-SAM effectively harnesses the Segment Anything Model's potential\nfor specialized tasks to mitigate error accumulation in self-training.\nExperiments on COD benchmark datasets demonstrate that ST-SAM achieves\nstate-of-the-art performance with only 1\\% labeled data, outperforming existing\nSSCOD methods and even matching fully supervised methods. Remarkably, ST-SAM\nrequires training only a single network, without relying on specific models or\nloss functions. This work establishes a new paradigm for annotation-efficient\nSSCOD. Codes will be available at https://github.com/hu-xh/ST-SAM.", "comment": "10 pages, 6 figures, ACM MM 2025", "pdf_url": "http://arxiv.org/pdf/2507.23307v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23149", "title": "Learning with Episodic Hypothesis Testing in General Games: A Framework for Equilibrium Selection", "authors": ["Ruifan Yang", "Manxi Wu"], "categories": ["cs.GT", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Computer Science and Game Theory (cs.GT)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23149v1", "summary": "We introduce a new hypothesis testing-based learning dynamics in which\nplayers update their strategies by combining hypothesis testing with\nutility-driven exploration. In this dynamics, each player forms beliefs about\nopponents' strategies and episodically tests these beliefs using empirical\nobservations. Beliefs are resampled either when the hypothesis test is rejected\nor through exploration, where the probability of exploration decreases with the\nplayer's (transformed) utility. In general finite normal-form games, we show\nthat the learning process converges to a set of approximate Nash equilibria\nand, more importantly, to a refinement that selects equilibria maximizing the\nminimum (transformed) utility across all players. Our result establishes\nconvergence to equilibrium in general finite games and reveals a novel\nmechanism for equilibrium selection induced by the structure of the learning\ndynamics.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23149v1", "cate": "cs.GT", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.23461", "title": "Mitigating Resolution-Drift in Federated Learning: Case of Keypoint Detection", "authors": ["Taeheon Lim", "Joohyung Lee", "Kyungjae Lee", "Jungchan Cho"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23461v1", "summary": "The Federated Learning (FL) approach enables effective learning across\ndistributed systems, while preserving user data privacy. To date, research has\nprimarily focused on addressing statistical heterogeneity and communication\nefficiency, through which FL has achieved success in classification tasks.\nHowever, its application to non-classification tasks, such as human pose\nestimation, remains underexplored. This paper identifies and investigates a\ncritical issue termed ``resolution-drift,'' where performance degrades\nsignificantly due to resolution variability across clients. Unlike class-level\nheterogeneity, resolution drift highlights the importance of resolution as\nanother axis of not independent or identically distributed (non-IID) data. To\naddress this issue, we present resolution-adaptive federated learning (RAF), a\nmethod that leverages heatmap-based knowledge distillation. Through\nmulti-resolution knowledge distillation between higher-resolution outputs\n(teachers) and lower-resolution outputs (students), our approach enhances\nresolution robustness without overfitting. Extensive experiments and\ntheoretical analysis demonstrate that RAF not only effectively mitigates\nresolution drift and achieves significant performance improvements, but also\ncan be integrated seamlessly into existing FL frameworks. Furthermore, although\nthis paper focuses on human pose estimation, our t-SNE analysis reveals\ndistinct characteristics between classification and high-resolution\nrepresentation tasks, supporting the generalizability of RAF to other tasks\nthat rely on preserving spatial detail.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23461v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23588", "title": "DiffLoRA: Differential Low-Rank Adapters for Large Language Models", "authors": ["Alexandre Misrahi", "Nadezhda Chirkova", "Maxime Louis", "Vassilina Nikoulina"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23588v1", "summary": "Differential Transformer has recently been proposed to improve performance in\nTransformer models by canceling out noise through a denoiser attention\nmechanism. In this work, we introduce DiffLoRA, a parameter-efficient\nadaptation of the differential attention mechanism, with low-rank adapters on\nboth positive and negative attention terms. This approach retains the\nefficiency of LoRA while aiming to benefit from the performance gains of\ndifferential attention. We evaluate DiffLoRA across a broad range of NLP tasks,\nincluding general benchmarks, many-shot in-context learning, RAG, and\nlong-context tests. We observe that, although DiffLoRA falls short of other\nparameter-efficient fine-tuning methods in most evaluation tasks, it shows\ninteresting results in certain domains (+11 pts on LoRA for HumanEval). We\nanalyze the attention patterns post-finetuning to identify the reasons for this\nbehavior.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23588v1", "cate": "cs.CL", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23309", "title": "PriorFusion: Unified Integration of Priors for Robust Road Perception in Autonomous Driving", "authors": ["Xuewei Tang", "Mengmeng Yang", "Tuopu Wen", "Peijin Jia", "Le Cui", "Mingshang Luo", "Kehua Sheng", "Bo Zhang", "Diange Yang", "Kun Jiang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23309v1", "summary": "With the growing interest in autonomous driving, there is an increasing\ndemand for accurate and reliable road perception technologies. In complex\nenvironments without high-definition map support, autonomous vehicles must\nindependently interpret their surroundings to ensure safe and robust\ndecision-making. However, these scenarios pose significant challenges due to\nthe large number, complex geometries, and frequent occlusions of road elements.\nA key limitation of existing approaches lies in their insufficient exploitation\nof the structured priors inherently present in road elements, resulting in\nirregular, inaccurate predictions. To address this, we propose PriorFusion, a\nunified framework that effectively integrates semantic, geometric, and\ngenerative priors to enhance road element perception. We introduce an\ninstance-aware attention mechanism guided by shape-prior features, then\nconstruct a data-driven shape template space that encodes low-dimensional\nrepresentations of road elements, enabling clustering to generate anchor points\nas reference priors. We design a diffusion-based framework that leverages these\nprior anchors to generate accurate and complete predictions. Experiments on\nlarge-scale autonomous driving datasets demonstrate that our method\nsignificantly improves perception accuracy, particularly under challenging\nconditions. Visualization results further confirm that our approach produces\nmore accurate, regular, and coherent predictions of road elements.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23309v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2405.18048", "title": "Expectation in Stochastic Games with Prefix-independent Objectives", "authors": ["Laurent Doyen", "Pranshu Gaba", "Shibashis Guha"], "categories": ["cs.GT"], "primary_category": "Subjects:       Computer Science and Game Theory (cs.GT)", "pdf_link": null, "comments": "Comments:      31 pages, 5 figures. Full version of paper accepted in CONCUR 2025", "url": "http://arxiv.org/abs/2405.18048v4", "summary": "Stochastic two-player games model systems with an environment that is both\nadversarial and stochastic. In this paper, we study the expected value of\nbounded quantitative prefix-independent objectives in the context of stochastic\ngames. We show a generic reduction from the expectation problem to linearly\nmany instances of the almost-sure satisfaction problem for threshold Boolean\nobjectives. The result follows from partitioning the vertices of the game into\nso-called value classes where each class consists of vertices of the same\nvalue. Our procedure further entails that the memory required by both players\nto play optimally for the expectation problem is no more than the memory\nrequired by the players to play optimally for the almost-sure satisfaction\nproblem for a corresponding threshold Boolean objective.\n  We show the applicability of the framework to compute the expected window\nmean-payoff measure in stochastic games. The window mean-payoff measure\nstrengthens the classical mean-payoff measure by computing the mean payoff over\nwindows of bounded length that slide along an infinite path. We show that the\ndecision problem to check if the expected window mean-payoff value is at least\na given threshold is in UP $\\cap$ coUP when the window length is given in\nunary.", "comment": "31 pages, 5 figures. Full version of paper accepted in CONCUR 2025", "pdf_url": "http://arxiv.org/pdf/2405.18048v4", "cate": "cs.GT", "date": "2024-05-28", "updated": "2025-07-31"}
{"id": "2507.23470", "title": "Automated Feedback on Student-Generated UML and ER Diagrams Using Large Language Models", "authors": ["Sebastian Gürtl", "Gloria Schimetta", "David Kerschbaumer", "Michael Liut", "Alexander Steinmaurer"], "categories": ["cs.HC", "cs.AI"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      Learnersourcing: Student-generated Content @ Scale Workshop at L@S 2025", "url": "http://arxiv.org/abs/2507.23470v1", "summary": "UML and ER diagrams are foundational in computer science education but come\nwith challenges for learners due to the need for abstract thinking, contextual\nunderstanding, and mastery of both syntax and semantics. These complexities are\ndifficult to address through traditional teaching methods, which often struggle\nto provide scalable, personalized feedback, especially in large classes. We\nintroduce DUET (Diagrammatic UML & ER Tutor), a prototype of an LLM-based tool,\nwhich converts a reference diagram and a student-submitted diagram into a\ntextual representation and provides structured feedback based on the\ndifferences. It uses a multi-stage LLM pipeline to compare diagrams and\ngenerate reflective feedback. Furthermore, the tool enables analytical insights\nfor educators, aiming to foster self-directed learning and inform instructional\nstrategies. We evaluated DUET through semi-structured interviews with six\nparticipants, including two educators and four teaching assistants. They\nidentified strengths such as accessibility, scalability, and learning support\nalongside limitations, including reliability and potential misuse. Participants\nalso suggested potential improvements, such as bulk upload functionality and\ninteractive clarification features. DUET presents a promising direction for\nintegrating LLMs into modeling education and offers a foundation for future\nclassroom integration and empirical evaluation.", "comment": "Learnersourcing: Student-generated Content @ Scale Workshop at L@S\n  2025", "pdf_url": "http://arxiv.org/pdf/2507.23470v1", "cate": "cs.HC", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23661", "title": "Arabic Hate Speech Identification and Masking in Social Media using Deep Learning Models and Pre-trained Models Fine-tuning", "authors": ["Salam Thabet Doghmash", "Motaz Saad"], "categories": ["cs.CL", "I.2.7"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      23 pages, 5 figures", "url": "http://arxiv.org/abs/2507.23661v1", "summary": "Hate speech identification in social media has become an increasingly\nimportant issue in recent years. In this research, we address two problems: 1)\nto detect hate speech in Arabic text, 2) to clean a given text from hate\nspeech. The meaning of cleaning here is replacing each bad word with stars\nbased on the number of letters for each word. Regarding the first problem, we\nconduct several experiments using deep learning models and transformers to\ndetermine the best model in terms of the F1 score. Regarding second problem, we\nconsider it as a machine translation task, where the input is a sentence\ncontaining dirty text and the output is the same sentence with masking the\ndirty text. The presented methods achieve the best model in hate speech\ndetection with a 92\\% Macro F1 score and 95\\% accuracy. Regarding the text\ncleaning experiment, the best result in the hate speech masking model reached\n0.3 in BLEU score with 1-gram, which is a good result compared with the state\nof the art machine translation systems.", "comment": "23 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2507.23661v1", "cate": "cs.CL", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23311", "title": "Forgetting of task-specific knowledge in model merging-based continual learning", "authors": ["Timm Hess", "Gido M van de Ven", "Tinne Tuytelaars"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23311v1", "summary": "This paper investigates the linear merging of models in the context of\ncontinual learning (CL). Using controlled visual cues in computer vision\nexperiments, we demonstrate that merging largely preserves or enhances shared\nknowledge, while unshared task-specific knowledge rapidly degrades. We further\nfind that merging models from an incremental training process consistently\noutperforms merging models trained in parallel.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23311v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2404.18154", "title": "Explaining vague language", "authors": ["Paul Égré", "Benjamin Spector"], "categories": ["cs.CL", "cs.GT", "cs.IT", "math.IT", "91A86", "I.2.7"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2404.18154v2", "summary": "Why is language vague? Vagueness may be explained and rationalized if it can\nbe shown that vague language is more useful to speaker and hearer than precise\nlanguage. In a well-known paper, Lipman proposes a game-theoretic account of\nvagueness in terms of mixed strategy that leads to a puzzle: vagueness cannot\nbe strictly better than precision at equilibrium. More recently, \\'Egr\\'e,\nSpector, Mortier and Verheyen have put forward a Bayesian account of vagueness\nestablishing that using vague words can be strictly more informative than using\nprecise words. This paper proposes to compare both results and to explain why\nthey are not in contradiction. Lipman's definition of vagueness relies\nexclusively on a property of signaling strategies, without making any\nassumptions about the lexicon, whereas \\'Egr\\'e et al.'s involves a layer of\nsemantic content. We argue that the semantic account of vagueness is needed,\nand more adequate and explanatory of vagueness.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2404.18154v2", "cate": "cs.CL", "date": "2024-04-28", "updated": "2025-07-31"}
{"id": "2507.23492", "title": "Digital literacy interventions can boost humans in discerning deepfakes", "authors": ["Dominique Geissler", "Claire Robertson", "Stefan Feuerriegel"], "categories": ["cs.HC", "cs.AI"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23492v1", "summary": "Deepfakes, i.e., images generated by artificial intelligence (AI), can erode\ntrust in institutions and compromise election outcomes, as people often\nstruggle to discern real images from deepfakes. Improving digital literacy can\nhelp address these challenges, yet scalable and effective approaches remain\nlargely unexplored. Here, we compare the efficacy of five digital literacy\ninterventions to boost people's ability to discern deepfakes: (1) textual\nguidance on common indicators of deepfakes; (2) visual demonstrations of these\nindicators; (3) a gamified exercise for identifying deepfakes; (4) implicit\nlearning through repeated exposure and feedback; and (5) explanations of how\ndeepfakes are generated with the help of AI. We conducted an experiment with\nN=1,200 participants from the United States to test the immediate and long-term\neffectiveness of our interventions. Our results show that our interventions can\nboost deepfake discernment by up to 13 percentage points while maintaining\ntrust in real images. Altogether, our approach is scalable, suitable for\ndiverse populations, and highly effective for boosting deepfake detection while\nmaintaining trust in truthful information.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23492v1", "cate": "cs.HC", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23740", "title": "Rule2Text: Natural Language Explanation of Logical Rules in Knowledge Graphs", "authors": ["Nasim Shirvani-Mahdavi", "Devin Wingfield", "Amin Ghasemi", "Chengkai Li"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23740v1", "summary": "Knowledge graphs (KGs) often contain sufficient information to support the\ninference of new facts. Identifying logical rules not only improves the\ncompleteness of a knowledge graph but also enables the detection of potential\nerrors, reveals subtle data patterns, and enhances the overall capacity for\nreasoning and interpretation. However, the complexity of such rules, combined\nwith the unique labeling conventions of each KG, can make them difficult for\nhumans to understand. In this paper, we explore the potential of large language\nmodels to generate natural language explanations for logical rules.\nSpecifically, we extract logical rules using the AMIE 3.5.1 rule discovery\nalgorithm from the benchmark dataset FB15k-237 and two large-scale datasets,\nFB-CVT-REV and FB+CVT-REV. We examine various prompting strategies, including\nzero- and few-shot prompting, including variable entity types, and\nchain-of-thought reasoning. We conduct a comprehensive human evaluation of the\ngenerated explanations based on correctness, clarity, and hallucination, and\nalso assess the use of large language models as automatic judges. Our results\ndemonstrate promising performance in terms of explanation correctness and\nclarity, although several challenges remain for future research. All scripts\nand data used in this study are publicly available at\nhttps://github.com/idirlab/KGRule2NL}{https://github.com/idirlab/KGRule2NL.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23740v1", "cate": "cs.CL", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23313", "title": "The Cow of Rembrandt - Analyzing Artistic Prompt Interpretation in Text-to-Image Models", "authors": ["Alfio Ferrara", "Sergio Picascia", "Elisabetta Rocchetti"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      to be published in: Applications of AI in the Analysis of Cultural and Artistic Heritage, organized within the 35th IEEE International Workshop on Machine Learning for Signal Processing (MLSP) 2025", "url": "http://arxiv.org/abs/2507.23313v1", "summary": "Text-to-image diffusion models have demonstrated remarkable capabilities in\ngenerating artistic content by learning from billions of images, including\npopular artworks. However, the fundamental question of how these models\ninternally represent concepts, such as content and style in paintings, remains\nunexplored. Traditional computer vision assumes content and style are\northogonal, but diffusion models receive no explicit guidance about this\ndistinction during training. In this work, we investigate how transformer-based\ntext-to-image diffusion models encode content and style concepts when\ngenerating artworks. We leverage cross-attention heatmaps to attribute pixels\nin generated images to specific prompt tokens, enabling us to isolate image\nregions influenced by content-describing versus style-describing tokens. Our\nfindings reveal that diffusion models demonstrate varying degrees of\ncontent-style separation depending on the specific artistic prompt and style\nrequested. In many cases, content tokens primarily influence object-related\nregions while style tokens affect background and texture areas, suggesting an\nemergent understanding of the content-style distinction. These insights\ncontribute to our understanding of how large-scale generative models internally\nrepresent complex artistic concepts without explicit supervision. We share the\ncode and dataset, together with an exploratory tool for visualizing attention\nmaps at https://github.com/umilISLab/artistic-prompt-interpretation.", "comment": "to be published in: Applications of AI in the Analysis of Cultural\n  and Artistic Heritage, organized within the 35th IEEE International Workshop\n  on Machine Learning for Signal Processing (MLSP) 2025", "pdf_url": "http://arxiv.org/pdf/2507.23313v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23509", "title": "I Am Big, You Are Little; I Am Right, You Are Wrong", "authors": ["David A. Kelly", "Akchunya Chanchal", "Nathan Blake"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      10 pages, International Conference on Computer Vision, ICCV 2025", "url": "http://arxiv.org/abs/2507.23509v1", "summary": "Machine learning for image classification is an active and rapidly developing\nfield. With the proliferation of classifiers of different sizes and different\narchitectures, the problem of choosing the right model becomes more and more\nimportant.\n  While we can assess a model's classification accuracy statistically, our\nunderstanding of the way these models work is unfortunately limited. In order\nto gain insight into the decision-making process of different vision models, we\npropose using minimal sufficient pixels sets to gauge a model's\n`concentration': the pixels that capture the essence of an image through the\nlens of the model. By comparing position, overlap, and size of sets of pixels,\nwe identify that different architectures have statistically different\nconcentration, in both size and position. In particular, ConvNext and EVA\nmodels differ markedly from the others. We also identify that images which are\nmisclassified are associated with larger pixels sets than correct\nclassifications.", "comment": "10 pages, International Conference on Computer Vision, ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.23509v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23776", "title": "Cascaded Information Disclosure for Generalized Evaluation of Problem Solving Capabilities", "authors": ["Yunxiang Yan", "Tomohiro Sawada", "Kartik Goyal"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Under review", "url": "http://arxiv.org/abs/2507.23776v1", "summary": "While question-answering~(QA) benchmark performance is an automatic and\nscalable method to compare LLMs, it is an indirect method of evaluating their\nunderlying problem-solving capabilities. Therefore, we propose a holistic and\ngeneralizable framework based on \\emph{cascaded question disclosure} that\nprovides a more accurate estimate of the models' problem-solving capabilities\nwhile maintaining the scalability and automation. This approach collects model\nresponses in a stagewise manner with each stage revealing partial information\nabout the question designed to elicit generalized reasoning in LLMs. We find\nthat our approach not only provides a better comparison between LLMs, but also\ninduces better intermediate traces in models compared to the standard QA\nparadigm. We empirically verify this behavior on diverse reasoning and\nknowledge-heavy QA datasets by comparing LLMs of varying sizes and families.\nOur approach narrows the performance gap observed in the standard QA evaluation\nsettings, indicating that the prevalent indirect QA paradigm of evaluation\noverestimates the differences in performance between models. We further\nvalidate our findings by extensive ablation studies.", "comment": "Under review", "pdf_url": "http://arxiv.org/pdf/2507.23776v1", "cate": "cs.CL", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23325", "title": "FASTopoWM: Fast-Slow Lane Segment Topology Reasoning with Latent World Models", "authors": ["Yiming Yang", "Hongbin Lin", "Yueru Luo", "Suzhong Fu", "Chao Zheng", "Xinrui Yan", "Shuqi Mei", "Kun Tang", "Shuguang Cui", "Zhen Li"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23325v1", "summary": "Lane segment topology reasoning provides comprehensive bird's-eye view (BEV)\nroad scene understanding, which can serve as a key perception module in\nplanning-oriented end-to-end autonomous driving systems. Existing lane topology\nreasoning methods often fall short in effectively leveraging temporal\ninformation to enhance detection and reasoning performance. Recently,\nstream-based temporal propagation method has demonstrated promising results by\nincorporating temporal cues at both the query and BEV levels. However, it\nremains limited by over-reliance on historical queries, vulnerability to pose\nestimation failures, and insufficient temporal propagation. To overcome these\nlimitations, we propose FASTopoWM, a novel fast-slow lane segment topology\nreasoning framework augmented with latent world models. To reduce the impact of\npose estimation failures, this unified framework enables parallel supervision\nof both historical and newly initialized queries, facilitating mutual\nreinforcement between the fast and slow systems. Furthermore, we introduce\nlatent query and BEV world models conditioned on the action latent to propagate\nthe state representations from past observations to the current timestep. This\ndesign substantially improves the performance of temporal perception within the\nslow pipeline. Extensive experiments on the OpenLane-V2 benchmark demonstrate\nthat FASTopoWM outperforms state-of-the-art methods in both lane segment\ndetection (37.4% v.s. 33.6% on mAP) and centerline perception (46.3% v.s. 41.5%\non OLS).", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23325v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23511", "title": "MECAT: A Multi-Experts Constructed Benchmark for Fine-Grained Audio Understanding Tasks", "authors": ["Yadong Niu", "Tianzi Wang", "Heinrich Dinkel", "Xingwei Sun", "Jiahao Zhou", "Gang Li", "Jizhong Liu", "Xunying Liu", "Junbo Zhang", "Jian Luan"], "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "primary_category": "Subjects:       Audio and Speech Processing (eess.AS)", "pdf_link": null, "comments": "Comments:      9 main pages, 5 figures, 3 tables, and 14 appendix pages", "url": "http://arxiv.org/abs/2507.23511v1", "summary": "While large audio-language models have advanced open-ended audio\nunderstanding, they still fall short of nuanced human-level comprehension. This\ngap persists largely because current benchmarks, limited by data annotations\nand evaluation metrics, fail to reliably distinguish between generic and highly\ndetailed model outputs. To this end, this work introduces MECAT, a Multi-Expert\nConstructed Benchmark for Fine-Grained Audio Understanding Tasks. Generated via\na pipeline that integrates analysis from specialized expert models with\nChain-of-Thought large language model reasoning, MECAT provides\nmulti-perspective, fine-grained captions and open-set question-answering pairs.\nThe benchmark is complemented by a novel metric: DATE (Discriminative-Enhanced\nAudio Text Evaluation). This metric penalizes generic terms and rewards\ndetailed descriptions by combining single-sample semantic similarity with\ncross-sample discriminability. A comprehensive evaluation of state-of-the-art\naudio models is also presented, providing new insights into their current\ncapabilities and limitations. The data and code are available at\nhttps://github.com/xiaomi-research/mecat", "comment": "9 main pages, 5 figures, 3 tables, and 14 appendix pages", "pdf_url": "http://arxiv.org/pdf/2507.23511v1", "cate": "eess.AS", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.22892", "title": "Hybrid EEG--Driven Brain--Computer Interface: A Large Language Model Framework for Personalized Language Rehabilitation", "authors": ["Ismail Hossain", "Mridul Banik"], "categories": ["cs.HC", "cs.CL"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22892v1", "summary": "Conventional augmentative and alternative communication (AAC) systems and\nlanguage-learning platforms often fail to adapt in real time to the user's\ncognitive and linguistic needs, especially in neurological conditions such as\npost-stroke aphasia or amyotrophic lateral sclerosis. Recent advances in\nnoninvasive electroencephalography (EEG)--based brain-computer interfaces\n(BCIs) and transformer--based large language models (LLMs) offer complementary\nstrengths: BCIs capture users' neural intent with low fatigue, while LLMs\ngenerate contextually tailored language content. We propose and evaluate a\nnovel hybrid framework that leverages real-time EEG signals to drive an\nLLM-powered language rehabilitation assistant. This system aims to: (1) enable\nusers with severe speech or motor impairments to navigate language-learning\nmodules via mental commands; (2) dynamically personalize vocabulary,\nsentence-construction exercises, and corrective feedback; and (3) monitor\nneural markers of cognitive effort to adjust task difficulty on the fly.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22892v1", "cate": "cs.HC", "date": "2025-06-18", "updated": "2025-06-18"}
{"id": "2507.23326", "title": "Learning Semantic Directions for Feature Augmentation in Domain-Generalized Medical Segmentation", "authors": ["Yingkai Wang", "Yaoyao Zhu", "Xiuding Cai", "Yuhao Xiao", "Haotian Wu", "Yu Yao"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23326v1", "summary": "Medical image segmentation plays a crucial role in clinical workflows, but\ndomain shift often leads to performance degradation when models are applied to\nunseen clinical domains. This challenge arises due to variations in imaging\nconditions, scanner types, and acquisition protocols, limiting the practical\ndeployment of segmentation models. Unlike natural images, medical images\ntypically exhibit consistent anatomical structures across patients, with\ndomain-specific variations mainly caused by imaging conditions. This unique\ncharacteristic makes medical image segmentation particularly challenging.\n  To address this challenge, we propose a domain generalization framework\ntailored for medical image segmentation. Our approach improves robustness to\ndomain-specific variations by introducing implicit feature perturbations guided\nby domain statistics. Specifically, we employ a learnable semantic direction\nselector and a covariance-based semantic intensity sampler to modulate\ndomain-variant features while preserving task-relevant anatomical consistency.\nFurthermore, we design an adaptive consistency constraint that is selectively\napplied only when feature adjustment leads to degraded segmentation\nperformance. This constraint encourages the adjusted features to align with the\noriginal predictions, thereby stabilizing feature selection and improving the\nreliability of the segmentation.\n  Extensive experiments on two public multi-center benchmarks show that our\nframework consistently outperforms existing domain generalization approaches,\nachieving robust and generalizable segmentation performance across diverse\nclinical domains.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23326v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23536", "title": "From LLMs to Edge: Parameter-Efficient Fine-Tuning on Edge Devices", "authors": ["Georg Slamanig", "Francesco Corti", "Olga Saukh"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23536v1", "summary": "Parameter-efficient fine-tuning (PEFT) methods reduce the computational costs\nof updating deep learning models by minimizing the number of additional\nparameters used to adapt a model to a down- stream task. While extensively\nresearched in large language models (LLMs), their application to smaller models\nused on edge devices, such as convolutional neural networks, remains\nunderexplored. This paper benchmarks and analyzes popular PEFT methods on\nconvolutional architectures typically deployed in resource-constrained edge\nenvironments. We evaluate LoRA, DoRA, and GaLore for updating standard and\ndepthwise convolutional architectures to handle distribution shifts and\naccommodate unseen classes. We utilize recently proposed PyTorch profilers to\ncompare the updated model performance and computational costs of these PEFT\nmethods with traditional fine-tuning approaches. With resource efficiency in\nmind, we investigate their update behavior across different rank dimensions. We\nfind that the evaluated PEFT methods are only half as memory-efficient when\napplied to depthwise-separable convolution architectures, compared to their\nefficiency with LLMs. Conversely, when targeting convolu- tional architectures\noptimized for edge deployment, adapter-based PEFT methods can reduce floating\npoint operations (FLOPs) during model updates by up to 95%. These insights\noffer valuable guidance for selecting PEFT methods based on hardware\nconstraints, performance requirements, and application needs. Our code is\nonline.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23536v1", "cate": "cs.LG", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.22898", "title": "Voice-guided Orchestrated Intelligence for Clinical Evaluation (VOICE): A Voice AI Agent System for Prehospital Stroke Assessment", "authors": ["Julian Acosta", "Scott Adams", "Julius Kernbach", "Romain Hardy", "Sung Eun Kim", "Luyang Luo", "Xiaoman Zhang", "Shreya Johri", "Mohammed Baharoon", "Pranav Rajpurkar"], "categories": ["cs.HC", "cs.CL"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22898v1", "summary": "We developed a voice-driven artificial intelligence (AI) system that guides\nanyone - from paramedics to family members - through expert-level stroke\nevaluations using natural conversation, while also enabling smartphone video\ncapture of key examination components for documentation and potential expert\nreview. This addresses a critical gap in emergency care: current stroke\nrecognition by first responders is inconsistent and often inaccurate, with\nsensitivity for stroke detection as low as 58%, causing life-threatening delays\nin treatment. Three non-medical volunteers used our AI system to assess ten\nsimulated stroke patients, including cases with likely large vessel occlusion\n(LVO) strokes and stroke-like conditions, while we measured diagnostic\naccuracy, completion times, user confidence, and expert physician review of the\nAI-generated reports. The AI system correctly identified 84% of individual\nstroke signs and detected 75% of likely LVOs, completing evaluations in just\nover 6 minutes. Users reported high confidence (median 4.5/5) and ease of use\n(mean 4.67/5). The system successfully identified 86% of actual strokes but\nalso incorrectly flagged 2 of 3 non-stroke cases as strokes. When an expert\nphysician reviewed the AI reports with videos, they identified the correct\ndiagnosis in 100% of cases, but felt confident enough to make preliminary\ntreatment decisions in only 40% of cases due to observed AI errors including\nincorrect scoring and false information. While the current system's limitations\nnecessitate human oversight, ongoing rapid advancements in speech-to-speech AI\nmodels suggest that future versions are poised to enable highly accurate\nassessments. Achieving human-level voice interaction could transform emergency\nmedical care, putting expert-informed assessment capabilities in everyone's\nhands.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22898v1", "cate": "cs.HC", "date": "2025-06-25", "updated": "2025-06-25"}
{"id": "2507.23331", "title": "Contrastive Learning-Driven Traffic Sign Perception: Multi-Modal Fusion of Text and Vision", "authors": ["Qiang Lu", "Waikit Xiu", "Xiying Li", "Shenyu Hu", "Shengbo Sun"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      11pages, 5 figures", "url": "http://arxiv.org/abs/2507.23331v1", "summary": "Traffic sign recognition, as a core component of autonomous driving\nperception systems, directly influences vehicle environmental awareness and\ndriving safety. Current technologies face two significant challenges: first,\nthe traffic sign dataset exhibits a pronounced long-tail distribution,\nresulting in a substantial decline in recognition performance of traditional\nconvolutional networks when processing low-frequency and out-of-distribution\nclasses; second, traffic signs in real-world scenarios are predominantly small\ntargets with significant scale variations, making it difficult to extract\nmulti-scale features.To overcome these issues, we propose a novel two-stage\nframework combining open-vocabulary detection and cross-modal learning. For\ntraffic sign detection, our NanoVerse YOLO model integrates a reparameterizable\nvision-language path aggregation network (RepVL-PAN) and an SPD-Conv module to\nspecifically enhance feature extraction for small, multi-scale targets. For\ntraffic sign classification, we designed a Traffic Sign Recognition Multimodal\nContrastive Learning model (TSR-MCL). By contrasting visual features from a\nVision Transformer with semantic features from a rule-based BERT, TSR-MCL\nlearns robust, frequency-independent representations, effectively mitigating\nclass confusion caused by data imbalance. On the TT100K dataset, our method\nachieves a state-of-the-art 78.4% mAP in the long-tail detection task for\nall-class recognition. The model also obtains 91.8% accuracy and 88.9% recall,\nsignificantly outperforming mainstream algorithms and demonstrating superior\naccuracy and generalization in complex, open-world scenarios.", "comment": "11pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2507.23331v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.22891", "title": "Real-time energy monitoring infrastructure for residential collective self-consumption operations using Linky meter", "authors": ["Jérôme Ferrari", "Benoit Delinchant", "Frédéric Wurtz", "Olga Rouchouze"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      Cired 2025, Jun 2025, Gen{è}ve (CH), Switzerland", "url": "http://arxiv.org/abs/2507.22891v1", "summary": "As part of the energy transition and the rise in energy prices, the number of\ncollective self-consumption operations in France is steadily increasing.\nHowever, energy flow monitoring currently relies on historical ''day+1'' data\nprovided by Linky meters, which does not offer real time feedback to help\nparticipants adapt their energy consumption behaviors. This article introduces\na new open-source infrastructure for real-time monitoring based on Linky meter\ndata, enabling participants to make informed decisions and take timely actions.\nIt includes a description of the xKy device, applied to a collective\nself-consumption operation involving nine participants, supported by the Energy\nTransition Observatory (OTE). The project encompasses the implementation of\ngateways in participants' homes and the development and operation of real-time\nmonitoring website, aimed at increasing participants' self-consumption rate.", "comment": "Cired 2025, Jun 2025, Gen{\\`e}ve (CH), Switzerland", "pdf_url": "http://arxiv.org/pdf/2507.22891v1", "cate": "cs.HC", "date": "2025-06-17", "updated": "2025-06-17"}
{"id": "2507.23540", "title": "A Unified Perception-Language-Action Framework for Adaptive Autonomous Driving", "authors": ["Yi Zhang", "Erik Leo Haß", "Kuo-Yi Chao", "Nenad Petrovic", "Yinglei Song", "Chengdong Wu", "Alois Knoll"], "categories": ["cs.RO", "cs.AI", "cs.CV"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23540v1", "summary": "Autonomous driving systems face significant challenges in achieving\nhuman-like adaptability, robustness, and interpretability in complex,\nopen-world environments. These challenges stem from fragmented architectures,\nlimited generalization to novel scenarios, and insufficient semantic extraction\nfrom perception. To address these limitations, we propose a unified\nPerception-Language-Action (PLA) framework that integrates multi-sensor fusion\n(cameras, LiDAR, radar) with a large language model (LLM)-augmented\nVision-Language-Action (VLA) architecture, specifically a GPT-4.1-powered\nreasoning core. This framework unifies low-level sensory processing with\nhigh-level contextual reasoning, tightly coupling perception with natural\nlanguage-based semantic understanding and decision-making to enable\ncontext-aware, explainable, and safety-bounded autonomous driving. Evaluations\non an urban intersection scenario with a construction zone demonstrate superior\nperformance in trajectory tracking, speed prediction, and adaptive planning.\nThe results highlight the potential of language-augmented cognitive frameworks\nfor advancing the safety, interpretability, and scalability of autonomous\ndriving systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23540v1", "cate": "cs.RO", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.22964", "title": "Exploring Dynamic Parameters for Vietnamese Gender-Independent ASR", "authors": ["Sotheara Leang", "Éric Castelli", "Dominique Vaufreydaz", "Sethserey Sam"], "categories": ["eess.AS", "cs.CL", "cs.SD", "eess.SP"], "primary_category": "Subjects:       Audio and Speech Processing (eess.AS)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22964v1", "summary": "The dynamic characteristics of speech signal provides temporal information\nand play an important role in enhancing Automatic Speech Recognition (ASR). In\nthis work, we characterized the acoustic transitions in a ratio plane of\nSpectral Subband Centroid Frequencies (SSCFs) using polar parameters to capture\nthe dynamic characteristics of the speech and minimize spectral variation.\nThese dynamic parameters were combined with Mel-Frequency Cepstral Coefficients\n(MFCCs) in Vietnamese ASR to capture more detailed spectral information. The\nSSCF0 was used as a pseudo-feature for the fundamental frequency (F0) to\ndescribe the tonal information robustly. The findings showed that the proposed\nparameters significantly reduce word error rates and exhibit greater gender\nindependence than the baseline MFCCs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22964v1", "cate": "eess.AS", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.23340", "title": "MagicRoad: Semantic-Aware 3D Road Surface Reconstruction via Obstacle Inpainting", "authors": ["Xingyue Peng", "Yuandong Lyu", "Lang Zhang", "Jian Zhu", "Songtao Wang", "Jiaxin Deng", "Songxin Lu", "Weiliang Ma", "Dangen She", "Peng Jia", "XianPeng Lang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23340v1", "summary": "Road surface reconstruction is essential for autonomous driving, supporting\ncentimeter-accurate lane perception and high-definition mapping in complex\nurban environments.While recent methods based on mesh rendering or 3D Gaussian\nsplatting (3DGS) achieve promising results under clean and static conditions,\nthey remain vulnerable to occlusions from dynamic agents, visual clutter from\nstatic obstacles, and appearance degradation caused by lighting and weather\nchanges. We present a robust reconstruction framework that integrates\nocclusion-aware 2D Gaussian surfels with semantic-guided color enhancement to\nrecover clean, consistent road surfaces. Our method leverages a planar-adapted\nGaussian representation for efficient large-scale modeling, employs\nsegmentation-guided video inpainting to remove both dynamic and static\nforeground objects, and enhances color coherence via semantic-aware correction\nin HSV space. Extensive experiments on urban-scale datasets demonstrate that\nour framework produces visually coherent and geometrically faithful\nreconstructions, significantly outperforming prior methods under real-world\nconditions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23340v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.22894", "title": "When no one shows up (at first): Navigating the uncertainties of participatory workshops in interdisciplinary research", "authors": ["Monique Munarini"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      Presented at HHAI25:The 4th International Conference Series on Hybrid Human-Artificial Intelligence, workshop Mind the AI-GAP 2025:Co-Designing Socio-Technical Systems. (June 9-13, 2025 in Pisa, Italy)", "url": "http://arxiv.org/abs/2507.22894v1", "summary": "This reflective paper explores often-unspoken challenges of designing and\nfacilitating co-design and participatory workshops, offering practical\nstrategies for early career researchers (ECRs) navigating these methods.\nDrawing from personal experience conducting a series of workshops titled: How\nto Think About Equity in the AI Ecosystem. It follows the full arc of the\nworkshop experience, from conceptualization and activity planning to\nparticipant recruitment and facilitation, offering a grounded account of what\nhappens when participation does not go as expected. The paper examines the\nmethodological challenges of engaging non-expert participants, particularly\nwhen operating without institutional support, financial incentives, or\nintegration into larger events. Despite initial difficulties such as low\nattendance, the workshop fostered rich discussions among a demographically\ndiverse group and ultimately led to one participant volunteering to\nco-facilitate a subsequent session. This transition from participant to\nco-facilitator exemplifies the redistribution of epistemic authority,\npositioning lived experience as central to research and engagement practices.\nBy reframing perceived failure as a productive site of learning, the paper\noffers practical strategies for ECRs working across disciplines who often\nnavigate unfamiliar methodological terrains, contributing to broader\nconversations on the realities of doing interdisciplinary, participatory work\nin practice.", "comment": "Presented at HHAI25:The 4th International Conference Series on Hybrid\n  Human-Artificial Intelligence, workshop Mind the AI-GAP 2025:Co-Designing\n  Socio-Technical Systems. (June 9-13, 2025 in Pisa, Italy)", "pdf_url": "http://arxiv.org/pdf/2507.22894v1", "cate": "cs.HC", "date": "2025-06-20", "updated": "2025-06-20"}
{"id": "2507.23543", "title": "ART: Adaptive Relation Tuning for Generalized Relation Prediction", "authors": ["Gopika Sudhakaran", "Hikaru Shindo", "Patrick Schramowski", "Simone Schaub-Meyer", "Kristian Kersting", "Stefan Roth"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted for publication in ICCV 2025", "url": "http://arxiv.org/abs/2507.23543v1", "summary": "Visual relation detection (VRD) is the task of identifying the relationships\nbetween objects in a scene. VRD models trained solely on relation detection\ndata struggle to generalize beyond the relations on which they are trained.\nWhile prompt tuning has been used to adapt vision-language models (VLMs) for\nVRD, it uses handcrafted prompts and struggles with novel or complex relations.\nWe argue that instruction tuning offers a more effective solution by\nfine-tuning VLMs on diverse instructional data. We thus introduce ART, an\nAdaptive Relation Tuning framework that adapts VLMs for VRD through instruction\ntuning and strategic instance selection. By converting VRD datasets into an\ninstruction tuning format and employing an adaptive sampling algorithm, ART\ndirects the VLM to focus on informative relations while maintaining\ngeneralizability. Specifically, we focus on the relation classification, where\nsubject-object boxes are given and the model predicts the predicate between\nthem. We tune on a held-in set and evaluate across multiple held-out datasets\nof varying complexity. Our approach strongly improves over its baselines and\ncan infer unseen relation concepts, a capability absent in mainstream VRD\nmethods. We demonstrate ART's practical value by using the predicted relations\nfor segmenting complex scenes.", "comment": "Accepted for publication in ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.23543v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23292", "title": "SequenceLayers: Sequence Processing and Streaming Neural Networks Made Easy", "authors": ["RJ Skerry-Ryan", "Julian Salazar", "Soroosh Mariooryad", "David Kao", "Daisy Stanton", "Eric Battenberg", "Matt Shannon", "Ron J. Weiss", "Robin Scheibler", "Jonas Rothfuss", "Tom Bagby"], "categories": ["cs.LG", "cs.CL", "cs.PL", "cs.SE", "eess.AS"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23292v1", "summary": "We introduce a neural network layer API and library for sequence modeling,\ndesigned for easy creation of sequence models that can be executed both\nlayer-by-layer (e.g., teacher-forced training) and step-by-step (e.g.,\nautoregressive sampling). To achieve this, layers define an explicit\nrepresentation of their state over time (e.g., a Transformer KV cache, a\nconvolution buffer, an RNN hidden state), and a step method that evolves that\nstate, tested to give identical results to a stateless layer-wise invocation.\nThis and other aspects of the SequenceLayers contract enables complex models to\nbe immediately streamable, mitigates a wide range of common bugs arising in\nboth streaming and parallel sequence processing, and can be implemented in any\ndeep learning library. A composable and declarative API, along with a\ncomprehensive suite of layers and combinators, streamlines the construction of\nproduction-scale models from simple streamable components while preserving\nstrong correctness guarantees. Our current implementations of SequenceLayers\n(JAX, TensorFlow 2) are available at https://github.com/google/sequence-layers.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23292v1", "cate": "cs.LG", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23341", "title": "The Impact of Image Resolution on Face Detection: A Comparative Analysis of MTCNN, YOLOv XI and YOLOv XII models", "authors": ["Ahmet Can Ömercikoğlu", "Mustafa Mansur Yönügül", "Pakize Erdoğmuş"], "categories": ["cs.CV", "68T45, 68T07", "I.4.8; I.4.9; I.5.4"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      6 pages, 5 figures, 4 tables", "url": "http://arxiv.org/abs/2507.23341v1", "summary": "Face detection is a crucial component in many AI-driven applications such as\nsurveillance, biometric authentication, and human-computer interaction.\nHowever, real-world conditions like low-resolution imagery present significant\nchallenges that degrade detection performance. In this study, we systematically\ninvestigate the impact of input resolution on the accuracy and robustness of\nthree prominent deep learning-based face detectors: YOLOv11, YOLOv12, and\nMTCNN. Using the WIDER FACE dataset, we conduct extensive evaluations across\nmultiple image resolutions (160x160, 320x320, and 640x640) and assess each\nmodel's performance using metrics such as precision, recall, mAP50, mAP50-95,\nand inference time. Results indicate that YOLOv11 outperforms YOLOv12 and MTCNN\nin terms of detection accuracy, especially at higher resolutions, while YOLOv12\nexhibits slightly better recall. MTCNN, although competitive in landmark\nlocalization, lags in real-time inference speed. Our findings provide\nactionable insights for selecting resolution-aware face detection models\nsuitable for varying operational constraints.", "comment": "6 pages, 5 figures, 4 tables", "pdf_url": "http://arxiv.org/pdf/2507.23341v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.22895", "title": "Brain motor intention Extraction Amplifier: Non-invasive brain-muscle interface", "authors": ["Ye Sun", "Bowei Zhao", "Dezhong Yao", "Rui Zhang", "Bohan Zhang", "Xiaoyuan Li", "Jing Wang", "Mingxuan Qu", "Gang Liu"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      18 pages, 9 figures", "url": "http://arxiv.org/abs/2507.22895v1", "summary": "Brain-computer interfaces (BCIs) enable real-time interaction between the\nbrain and external devices by decoding neural signals. However, existing\nmotor-based BCI paradigms, like motor imagery BCI, face challenges with\nimprecise labeling in real-world use. This mismatch between EEG signals and\ntrue behavioral intentions leads to pseudo-labels, undermining decoding\naccuracy and system robustness. To overcome this bottleneck, this paper first\nproposes a novel motor intention extraction framework based on a non-invasive\nbrain-muscle interface (BMuI)($\\text{BCI} =\n\\frac{\\text{Brain}}{\\text{Computer}} \\text{ Interface} =\n\\frac{\\text{Brain}}{\\not\\text{Muscle}}\\! \\text{ (BMuI)} \\times\n\\!\\frac{\\not\\text{Muscle}}{\\text{Computer}}\\! \\text{ Interface}$). This method\nsimulates the neural pathway from the brain to the muscles in order to capture\nand enhance the weak motor intention signals originating in the brain. It then\nuses EMG as a high-fidelity relay medium to achieve more accurate intention\nrecognition and transmission. To systematically validate the feasibility and\neffectiveness of this approach, we conducted both offline experiments (to\nrepeatedly verify feasibility) and online experiments (to construct a real-time\ninteractive system and evaluate its performance). The results show that BMuI is\nfeasible, achieving a prediction accuracy of 0.8314; in the online experiment,\nall participants are able to successfully control the Unity virtual arm.", "comment": "18 pages, 9 figures", "pdf_url": "http://arxiv.org/pdf/2507.22895v1", "cate": "cs.HC", "date": "2025-06-21", "updated": "2025-06-21"}
{"id": "2507.23589", "title": "Can LLM-Reasoning Models Replace Classical Planning? A Benchmark Study", "authors": ["Kai Goebel", "Patrik Zips"], "categories": ["cs.RO", "cs.AI"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23589v1", "summary": "Recent advancements in Large Language Models have sparked interest in their\npotential for robotic task planning. While these models demonstrate strong\ngenerative capabilities, their effectiveness in producing structured and\nexecutable plans remains uncertain. This paper presents a systematic evaluation\nof a broad spectrum of current state of the art language models, each directly\nprompted using Planning Domain Definition Language domain and problem files,\nand compares their planning performance with the Fast Downward planner across a\nvariety of benchmarks. In addition to measuring success rates, we assess how\nfaithfully the generated plans translate into sequences of actions that can\nactually be executed, identifying both strengths and limitations of using these\nmodels in this setting. Our findings show that while the models perform well on\nsimpler planning tasks, they continue to struggle with more complex scenarios\nthat require precise resource management, consistent state tracking, and strict\nconstraint compliance. These results underscore fundamental challenges in\napplying language models to robotic planning in real world environments. By\noutlining the gaps that emerge during execution, we aim to guide future\nresearch toward combined approaches that integrate language models with\nclassical planners in order to enhance the reliability and scalability of\nplanning in autonomous robotics.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23589v1", "cate": "cs.RO", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23348", "title": "SWE-Debate: Competitive Multi-Agent Debate for Software Issue Resolution", "authors": ["Han Li", "Yuling Shi", "Shaoxin Lin", "Xiaodong Gu", "Heng Lian", "Xin Wang", "Yantao Jia", "Tao Huang", "Qianxiang Wang"], "categories": ["cs.SE", "cs.CL", "cs.LG"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      Our code and data are available at this https URL", "url": "http://arxiv.org/abs/2507.23348v1", "summary": "Issue resolution has made remarkable progress thanks to the advanced\nreasoning capabilities of large language models (LLMs). Recently, agent-based\nframeworks such as SWE-agent have further advanced this progress by enabling\nautonomous, tool-using agents to tackle complex software engineering tasks.\nWhile existing agent-based issue resolution approaches are primarily based on\nagents' independent explorations, they often get stuck in local solutions and\nfail to identify issue patterns that span across different parts of the\ncodebase. To address this limitation, we propose SWE-Debate, a competitive\nmulti-agent debate framework that encourages diverse reasoning paths and\nachieves more consolidated issue localization. SWE-Debate first creates\nmultiple fault propagation traces as localization proposals by traversing a\ncode dependency graph. Then, it organizes a three-round debate among\nspecialized agents, each embodying distinct reasoning perspectives along the\nfault propagation trace. This structured competition enables agents to\ncollaboratively converge on a consolidated fix plan. Finally, this consolidated\nfix plan is integrated into an MCTS-based code modification agent for patch\ngeneration. Experiments on the SWE-bench benchmark show that SWE-Debate\nachieves new state-of-the-art results in open-source agent frameworks and\noutperforms baselines by a large margin.", "comment": "Our code and data are available at\n  https://github.com/YerbaPage/SWE-Debate", "pdf_url": "http://arxiv.org/pdf/2507.23348v1", "cate": "cs.SE", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23343", "title": "Who is a Better Talker: Subjective and Objective Quality Assessment for AI-Generated Talking Heads", "authors": ["Yingjie Zhou", "Jiezhang Cao", "Zicheng Zhang", "Farong Wen", "Yanwei Jiang", "Jun Jia", "Xiaohong Liu", "Xiongkuo Min", "Guangtao Zhai"], "categories": ["cs.CV", "eess.IV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23343v1", "summary": "Speech-driven methods for portraits are figuratively known as \"Talkers\"\nbecause of their capability to synthesize speaking mouth shapes and facial\nmovements. Especially with the rapid development of the Text-to-Image (T2I)\nmodels, AI-Generated Talking Heads (AGTHs) have gradually become an emerging\ndigital human media. However, challenges persist regarding the quality of these\ntalkers and AGTHs they generate, and comprehensive studies addressing these\nissues remain limited. To address this gap, this paper presents the largest\nAGTH quality assessment dataset THQA-10K to date, which selects 12 prominent\nT2I models and 14 advanced talkers to generate AGTHs for 14 prompts. After\nexcluding instances where AGTH generation is unsuccessful, the THQA-10K dataset\ncontains 10,457 AGTHs. Then, volunteers are recruited to subjectively rate the\nAGTHs and give the corresponding distortion categories. In our analysis for\nsubjective experimental results, we evaluate the performance of talkers in\nterms of generalizability and quality, and also expose the distortions of\nexisting AGTHs. Finally, an objective quality assessment method based on the\nfirst frame, Y-T slice and tone-lip consistency is proposed. Experimental\nresults show that this method can achieve state-of-the-art (SOTA) performance\nin AGTH quality assessment. The work is released at\nhttps://github.com/zyj-2000/Talker.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23343v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.22899", "title": "A visual analytics tool for taxonomy-based trajectory data exploration", "authors": ["Ivan A. Hanono Cozzetti", "Ahmad Abdou"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      71 pages, 92 figures", "url": "http://arxiv.org/abs/2507.22899v1", "summary": "The analysis of spatio-temporal data presents significant challenges due to\nthe complexity and heterogeneity of movement patterns. This project proposes a\ndata analytics tool that combines data visualization and statistical\ncomputation to facilitate spatio-temporal data analysis through a multi-level\napproach. The tool categorizes moving objects into distinct taxonomies using\nMachine Learning models, adding meaningful structure to the analysis. Two case\nstudies demonstrate the methodology's effectiveness. The first analyzed Arctic\nfox trajectories, successfully identifying and labeling foxes with Geometric or\nKinematic-based behaviors, further categorized into Curvature and Acceleration\ngroups. Statistical indicators revealed that foxes with Acceleration-based\nbehavior showed constant, steady acceleration, while those with Curvature-based\nbehavior exhibited acceleration peaks and sudden deceleration. The second case\nstudy examined tropical cyclone data, labeling trajectories with Speed,\nCurvature, and hybrid Geometric-based behaviors through unique statistical\nvariables. Analysis of hybrid Geometric behavior (Curvature and Indentation\ncombined) identified specific angles with the highest impact on hurricane shape\nand geometry. The proposed method and tool demonstrate that spatio-temporal\ndata, despite inherent complexity, can be analyzed and explained in detail,\nproviding a theoretical and practical blueprint applicable to multiple domains.", "comment": "71 pages, 92 figures", "pdf_url": "http://arxiv.org/pdf/2507.22899v1", "cate": "cs.HC", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2507.23208", "title": "Are Recommenders Self-Aware? Label-Free Recommendation Performance Estimation via Model Uncertainty", "authors": ["Jiayu Li", "Ziyi Ye", "Guohao Jian", "Zhiqiang Guo", "Weizhi Ma", "Qingyao Ai", "Min Zhang"], "categories": ["cs.IR", "cs.LG"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23208v1", "summary": "Can a recommendation model be self-aware? This paper investigates the\nrecommender's self-awareness by quantifying its uncertainty, which provides a\nlabel-free estimation of its performance. Such self-assessment can enable more\ninformed understanding and decision-making before the recommender engages with\nany users. To this end, we propose an intuitive and effective method,\nprobability-based List Distribution uncertainty (LiDu). LiDu measures\nuncertainty by determining the probability that a recommender will generate a\ncertain ranking list based on the prediction distributions of individual items.\nWe validate LiDu's ability to represent model self-awareness in two settings:\n(1) with a matrix factorization model on a synthetic dataset, and (2) with\npopular recommendation algorithms on real-world datasets. Experimental results\nshow that LiDu is more correlated with recommendation performance than a series\nof label-free performance estimators. Additionally, LiDu provides valuable\ninsights into the dynamic inner states of models throughout training and\ninference. This work establishes an empirical connection between recommendation\nuncertainty and performance, framing it as a step towards more transparent and\nself-evaluating recommender systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23208v1", "cate": "cs.IR", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23607", "title": "Deep Learning-based Prediction of Clinical Trial Enrollment with Uncertainty Estimates", "authors": ["Tien Huu Do", "Antoine Masquelier", "Nae Eoun Lee", "Jonathan Crowther"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23607v1", "summary": "Clinical trials are a systematic endeavor to assess the safety and efficacy\nof new drugs or treatments. Conducting such trials typically demands\nsignificant financial investment and meticulous planning, highlighting the need\nfor accurate predictions of trial outcomes. Accurately predicting patient\nenrollment, a key factor in trial success, is one of the primary challenges\nduring the planning phase. In this work, we propose a novel deep learning-based\nmethod to address this critical challenge. Our method, implemented as a neural\nnetwork model, leverages pre-trained language models (PLMs) to capture the\ncomplexities and nuances of clinical documents, transforming them into\nexpressive representations. These representations are then combined with\nencoded tabular features via an attention mechanism. To account for\nuncertainties in enrollment prediction, we enhance the model with a\nprobabilistic layer based on the Gamma distribution, which enables range\nestimation. We apply the proposed model to predict clinical trial duration,\nassuming site-level enrollment follows a Poisson-Gamma process. We carry out\nextensive experiments on real-world clinical trial data, and show that the\nproposed method can effectively predict the number of patients enrolled at a\nnumber of sites for a given clinical trial, outperforming established baseline\nmodels.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23607v1", "cate": "cs.LG", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23361", "title": "SWE-Exp: Experience-Driven Software Issue Resolution", "authors": ["Silin Chen", "Shaoxin Lin", "Xiaodong Gu", "Yuling Shi", "Heng Lian", "Longfei Yun", "Dong Chen", "Weiguo Sun", "Lin Cao", "Qianxiang Wang"], "categories": ["cs.SE", "cs.CL", "cs.LG"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      Our code and data are available at this https URL", "url": "http://arxiv.org/abs/2507.23361v1", "summary": "Recent advances in large language model (LLM) agents have shown remarkable\nprogress in software issue resolution, leveraging advanced techniques such as\nmulti-agent collaboration and Monte Carlo Tree Search (MCTS). However, current\nagents act as memoryless explorers - treating each problem separately without\nretaining or reusing knowledge from previous repair experiences. This leads to\nredundant exploration of failed trajectories and missed chances to adapt\nsuccessful issue resolution methods to similar problems. To address this\nproblem, we introduce SWE-Exp, an experience - enhanced approach that distills\nconcise and actionable experience from prior agent trajectories, enabling\ncontinuous learning across issues. Our method introduces a multi-faceted\nexperience bank that captures both successful and failed repair attempts.\nSpecifically, it extracts reusable issue resolution knowledge at different\nlevels - from high-level problem comprehension to specific code changes.\nExperiments show that SWE-Exp achieves state-of-the-art resolution rate (41.6%\nPass@1) on SWE-bench-Verified under open-source agent frameworks. Our approach\nestablishes a new paradigm in which automated software engineering agents\nsystematically accumulate and leverage repair expertise, fundamentally shifting\nfrom trial-and-error exploration to strategic, experience-driven issue\nresolution.", "comment": "Our code and data are available at\n  https://github.com/YerbaPage/SWE-Exp", "pdf_url": "http://arxiv.org/pdf/2507.23361v1", "cate": "cs.SE", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23357", "title": "IN45023 Neural Network Design Patterns in Computer Vision Seminar Report, Summer 2025", "authors": ["Radu-Andrei Bourceanu", "Neil De La Fuente", "Jan Grimm", "Andrei Jardan", "Andriy Manucharyan", "Cornelius Weiss", "Roman Pflugfelder"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23357v1", "summary": "This report analyzes the evolution of key design patterns in computer vision\nby examining six influential papers. The analy- sis begins with foundational\narchitectures for image recognition. We review ResNet, which introduced\nresidual connections to overcome the vanishing gradient problem and enable\neffective training of significantly deeper convolutional networks.\nSubsequently, we examine the Vision Transformer (ViT), which established a new\nparadigm by applying the Transformer ar- chitecture to sequences of image\npatches, demonstrating the efficacy of attention-based models for large-scale\nimage recogni- tion. Building on these visual representation backbones, we\ninvestigate generative models. Generative Adversarial Networks (GANs) are\nanalyzed for their novel adversarial training process, which challenges a\ngenerator against a discriminator to learn complex data distributions. Then,\nLatent Diffusion Models (LDMs) are covered, which improve upon prior generative\nmethods by performing a sequential denoising process in a perceptually\ncompressed latent space. LDMs achieve high-fidelity synthesis with greater\ncomputational efficiency, representing the current state-of-the-art for image\ngeneration. Finally, we explore self-supervised learning techniques that reduce\ndependency on labeled data. DINO is a self-distillation framework in which a\nstudent network learns to match the output of a momentum-updated teacher,\nyielding features with strong k-NN classification performance. We conclude with\nMasked Autoencoders (MAE), which utilize an asymmetric encoder-decoder design\nto reconstruct heavily masked inputs, providing a highly scalable and effective\nmethod for pre-training large-scale vision models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23357v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.22901", "title": "Accelerated and Optimized Search of Imperceptible Color Vibration for Embedding Information into LCD images", "authors": ["Shingo Hattori", "Takefumi Hiraki"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      Presented at ACM SIGGRAPH Asia 2022 Posters", "url": "http://arxiv.org/abs/2507.22901v1", "summary": "Large, high-resolution displays are installed throughout the city as public\ndisplays. By superimposing invisible information on the images of these\ndisplays, large numbers of devices with cameras and sensors can communicate\nwith the displays without prior pairing. Several applications have been\nproposed, such as operating robots or communicating information to users by\ndisplaying 2D codes on images. However, the display of 2D codes has the problem\nof compromising the appearance of displayed content.\n  Abe et al. proposed a method of communicating with devices by superimposing\ninvisible information using color vibration on images displayed on\noff-the-shelf liquid-crystal displays (LCD). Using this method, we can embed\nthe information for devices in images without interfering with the displayed\ncontent. Abe et al. uses a simple serial loop operation to search for color\npairs comprising a color vibration, which requires a very long processing time\ndue to the huge search space.\n  In this paper, we propose an accelerated and optimized search method for\ncolor pairs that constitute the imperceptible color vibration for embedding\ninformation on LCD images. To achieve fast color pair search, we parallelized\nthe search process, which is previously done individually, by using arrays\nrepresenting the amount of movement and an operation to extract elements from\nthe array that satisfy the conditions. In addition, we investigate the amount\nof information that can be superimposed on nine color images using the\nimperceptible color vibration and clarify the applicability of embedding\ninformation into images using the color vibration.", "comment": "Presented at ACM SIGGRAPH Asia 2022 Posters", "pdf_url": "http://arxiv.org/pdf/2507.22901v1", "cate": "cs.HC", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2507.23209", "title": "Not Just What, But When: Integrating Irregular Intervals to LLM for Sequential Recommendation", "authors": ["Wei-Wei Du", "Takuma Udagawa", "Kei Tateno"], "categories": ["cs.IR", "cs.LG"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "Comments:      Accepted by RecSys 2025 short paper track", "url": "http://arxiv.org/abs/2507.23209v1", "summary": "Time intervals between purchasing items are a crucial factor in sequential\nrecommendation tasks, whereas existing approaches focus on item sequences and\noften overlook by assuming the intervals between items are static. However,\ndynamic intervals serve as a dimension that describes user profiling on not\nonly the history within a user but also different users with the same item\nhistory. In this work, we propose IntervalLLM, a novel framework that\nintegrates interval information into LLM and incorporates the novel\ninterval-infused attention to jointly consider information of items and\nintervals. Furthermore, unlike prior studies that address the cold-start\nscenario only from the perspectives of users and items, we introduce a new\nviewpoint: the interval perspective to serve as an additional metric for\nevaluating recommendation methods on the warm and cold scenarios. Extensive\nexperiments on 3 benchmarks with both traditional- and LLM-based baselines\ndemonstrate that our IntervalLLM achieves not only 4.4% improvements in average\nbut also the best-performing warm and cold scenarios across all users, items,\nand the proposed interval perspectives. In addition, we observe that the cold\nscenario from the interval perspective experiences the most significant\nperformance drop among all recommendation methods. This finding underscores the\nnecessity of further research on interval-based cold challenges and our\nintegration of interval information in the realm of sequential recommendation\ntasks. Our code is available here:\nhttps://github.com/sony/ds-research-code/tree/master/recsys25-IntervalLLM.", "comment": "Accepted by RecSys 2025 short paper track", "pdf_url": "http://arxiv.org/pdf/2507.23209v1", "cate": "cs.IR", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23615", "title": "L-GTA: Latent Generative Modeling for Time Series Augmentation", "authors": ["Luis Roque", "Carlos Soares", "Vitor Cerqueira", "Luis Torgo"], "categories": ["cs.LG", "cs.AI", "68T01", "I.5.1; G.3; H.2.8; I.2.1"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23615v1", "summary": "Data augmentation is gaining importance across various aspects of time series\nanalysis, from forecasting to classification and anomaly detection tasks. We\nintroduce the Latent Generative Transformer Augmentation (L-GTA) model, a\ngenerative approach using a transformer-based variational recurrent\nautoencoder. This model uses controlled transformations within the latent space\nof the model to generate new time series that preserve the intrinsic properties\nof the original dataset. L-GTA enables the application of diverse\ntransformations, ranging from simple jittering to magnitude warping, and\ncombining these basic transformations to generate more complex synthetic time\nseries datasets. Our evaluation of several real-world datasets demonstrates the\nability of L-GTA to produce more reliable, consistent, and controllable\naugmented data. This translates into significant improvements in predictive\naccuracy and similarity measures compared to direct transformation methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23615v1", "cate": "cs.LG", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23364", "title": "Holistic Evaluations of Topic Models", "authors": ["Thomas Compton"], "categories": ["cs.IR", "cs.CL"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "Comments:      10 pages, 6 tables", "url": "http://arxiv.org/abs/2507.23364v1", "summary": "Topic models are gaining increasing commercial and academic interest for\ntheir ability to summarize large volumes of unstructured text. As unsupervised\nmachine learning methods, they enable researchers to explore data and help\ngeneral users understand key themes in large text collections. However, they\nrisk becoming a 'black box', where users input data and accept the output as an\naccurate summary without scrutiny. This article evaluates topic models from a\ndatabase perspective, drawing insights from 1140 BERTopic model runs. The goal\nis to identify trade-offs in optimizing model parameters and to reflect on what\nthese findings mean for the interpretation and responsible use of topic models", "comment": "10 pages, 6 tables", "pdf_url": "http://arxiv.org/pdf/2507.23364v1", "cate": "cs.IR", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23362", "title": "Short-LVLM: Compressing and Accelerating Large Vision-Language Models by Pruning Redundant Layers", "authors": ["Ji Ma", "Wei Suo", "Peng Wang", "Yanning Zhang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted By ACM MM 25", "url": "http://arxiv.org/abs/2507.23362v1", "summary": "Although large vision-language models (LVLMs) have demonstrated impressive\ncapabilities in multi-modal understanding and reasoning, their practical\napplications are still limited by massive model parameters and high\ncomputational costs. Recent efforts from natural language processing (NLP) have\nshown the effectiveness of layer pruning, offering a plausible training-free\ncompression solution. However, due to the modality divergence between vision\nand language, it is unclear whether these NLP techniques are still effective in\nLVLMs. In this paper, we empirically prove that directly applying these layer\npruning methods to LVLMs is ineffective. Through extensive experiments, we find\nthat non-essential vision-language (VL) tokens and inter-layer feature gaps\npose critical challenges to pruning layers in LVLMs. Based on these insights,\nwe propose a novel framework Short-LVLM (SVL) that can utilize important VL\ntokens and mitigate the layer-wise feature gaps. Notably, Short-LVLM not only\nachieves a superior trade-off between performance and efficiency but also\nexhibits several potential advantages, i.e., training-free, model-agnostic, and\nhighly compatible. The code for this work is publicly available at\nhttps://github.com/ASGO-MM/Short-LVLM.", "comment": "Accepted By ACM MM 25", "pdf_url": "http://arxiv.org/pdf/2507.23362v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.22903", "title": "A blessing or a burden? Exploring worker perspectives of using a social robot in a church", "authors": ["Andrew Blair", "Peggy Gregory", "Mary Ellen Foster"], "categories": ["cs.HC", "cs.RO"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      Accepted by the 2025 34th IEEE International Conference on Robot and Human Interactive Communication (ROMAN)", "url": "http://arxiv.org/abs/2507.22903v1", "summary": "Recent technological advances have allowed robots to assist in the service\nsector, and consequently accelerate job and sector transformation. Less\nattention has been paid to the use of robots in real-world organisations where\nsocial benefits, as opposed to profits, are the primary motivator. To explore\nthese opportunities, we have partnered with a working church and visitor\nattraction. We conducted interviews with 15 participants from a range of\nstakeholder groups within the church to understand worker perspectives of\nintroducing a social robot to the church and analysed the results using\nreflexive thematic analysis. Findings indicate mixed responses to the use of a\nrobot, with participants highlighting the empathetic responsibility the church\nhas towards people and the potential for unintended consequences. However,\ninformation provision and alleviation of menial or mundane tasks were\nidentified as potential use cases. This highlights the need to consider not\nonly the financial aspects of robot introduction, but also how social and\nintangible values shape what roles a robot should take on within an\norganisation.", "comment": "Accepted by the 2025 34th IEEE International Conference on Robot and\n  Human Interactive Communication (ROMAN)", "pdf_url": "http://arxiv.org/pdf/2507.22903v1", "cate": "cs.HC", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2507.23267", "title": "Your Spending Needs Attention: Modeling Financial Habits with Transformers", "authors": ["D. T. Braithwaite", "Misael Cavalcanti", "R. Austin McEver", "Hiroto Udagawa", "Daniel Silva", "Rohan Ramanath", "Felipe Meneses", "Arissa Yoshida", "Evan Wingert", "Matheus Ramos", "Brian Zanfelice", "Aman Gupta"], "categories": ["cs.IR"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23267v1", "summary": "Predictive models play a crucial role in the financial industry, enabling\nrisk prediction, fraud detection, and personalized recommendations, where\nslight changes in core model performance can result in billions of dollars in\nrevenue or losses. While financial institutions have access to enormous amounts\nof user data (e.g., bank transactions, in-app events, and customer support\nlogs), leveraging this data effectively remains challenging due to its\ncomplexity and scale. Thus, in many financial institutions, most production\nmodels follow traditional machine learning (ML) approaches by converting\nunstructured data into manually engineered tabular features. Conversely, other\ndomains (e.g., natural language processing) have effectively utilized\nself-supervised learning (SSL) to learn rich representations from raw data,\nremoving the need for manual feature extraction. In this paper, we investigate\nusing transformer-based representation learning models for transaction data,\nhypothesizing that these models, trained on massive data, can provide a novel\nand powerful approach to understanding customer behavior. We propose a new\nmethod enabling the use of SSL with transaction data by adapting\ntransformer-based models to handle both textual and structured attributes. Our\napproach, denoted nuFormer, includes an end-to-end fine-tuning method that\nintegrates user embeddings with existing tabular features. Our experiments\ndemonstrate improvements for large-scale recommendation problems at Nubank.\nNotably, these gains are achieved solely through enhanced representation\nlearning rather than incorporating new data sources.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23267v1", "cate": "cs.IR", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23638", "title": "OptiGradTrust: Byzantine-Robust Federated Learning with Multi-Feature Gradient Analysis and Reinforcement Learning-Based Trust Weighting", "authors": ["Mohammad Karami", "Fatemeh Ghassemi", "Hamed Kebriaei", "Hamid Azadegan"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23638v1", "summary": "Federated Learning (FL) enables collaborative model training across\ndistributed medical institutions while preserving patient privacy, but remains\nvulnerable to Byzantine attacks and statistical heterogeneity. We present\nOptiGradTrust, a comprehensive defense framework that evaluates gradient\nupdates through a novel six-dimensional fingerprint including VAE\nreconstruction error, cosine similarity metrics, $L_2$ norm, sign-consistency\nratio, and Monte Carlo Shapley value, which drive a hybrid RL-attention module\nfor adaptive trust scoring. To address convergence challenges under data\nheterogeneity, we develop FedBN-Prox (FedBN-P), combining Federated Batch\nNormalization with proximal regularization for optimal accuracy-convergence\ntrade-offs. Extensive evaluation across MNIST, CIFAR-10, and Alzheimer's MRI\ndatasets under various Byzantine attack scenarios demonstrates significant\nimprovements over state-of-the-art defenses, achieving up to +1.6 percentage\npoints over FLGuard under non-IID conditions while maintaining robust\nperformance against diverse attack patterns through our adaptive learning\napproach.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23638v1", "cate": "cs.LG", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23674", "title": "TweakLLM: A Routing Architecture for Dynamic Tailoring of Cached Responses", "authors": ["Muhammad Taha Cheema", "Abeer Aamir", "Khawaja Gul Muhammad", "Naveed Anwar Bhatti", "Ihsan Ayyub Qazi", "Zafar Ayyub Qazi"], "categories": ["cs.LG", "cs.CL"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      13 pages, 9 figures", "url": "http://arxiv.org/abs/2507.23674v1", "summary": "Large Language Models (LLMs) process millions of queries daily, making\nefficient response caching a compelling optimization for reducing cost and\nlatency. However, preserving relevance to user queries using this approach\nproves difficult due to the personalized nature of chatbot interactions and the\nlimited accuracy of semantic similarity search. To address this, we present\nTweakLLM, a novel routing architecture that employs a lightweight LLM to\ndynamically adapt cached responses to incoming prompts. Through comprehensive\nevaluation, including user studies with side-by-side comparisons, satisfaction\nvoting, as well as multi-agent LLM debates, we demonstrate that TweakLLM\nmaintains response quality comparable to frontier models while significantly\nimproving cache effectiveness. Our results across real-world datasets highlight\nTweakLLM as a scalable, resource-efficient caching solution for high-volume LLM\ndeployments without compromising user experience.", "comment": "13 pages, 9 figures", "pdf_url": "http://arxiv.org/pdf/2507.23674v1", "cate": "cs.LG", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23371", "title": "VMatcher: State-Space Semi-Dense Local Feature Matching", "authors": ["Ali Youssef"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23371v1", "summary": "This paper introduces VMatcher, a hybrid Mamba-Transformer network for\nsemi-dense feature matching between image pairs. Learning-based feature\nmatching methods, whether detector-based or detector-free, achieve\nstate-of-the-art performance but depend heavily on the Transformer's attention\nmechanism, which, while effective, incurs high computational costs due to its\nquadratic complexity. In contrast, Mamba introduces a Selective State-Space\nModel (SSM) that achieves comparable or superior performance with linear\ncomplexity, offering significant efficiency gains. VMatcher leverages a hybrid\napproach, integrating Mamba's highly efficient long-sequence processing with\nthe Transformer's attention mechanism. Multiple VMatcher configurations are\nproposed, including hierarchical architectures, demonstrating their\neffectiveness in setting new benchmarks efficiently while ensuring robustness\nand practicality for real-time applications where rapid inference is crucial.\nSource Code is available at: https://github.com/ayoussf/VMatcher", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23371v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.22905", "title": "Exploring LLM-generated Culture-specific Affective Human-Robot Tactile Interaction", "authors": ["Qiaoqiao Ren", "Tony Belpaeme"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22905v1", "summary": "As large language models (LLMs) become increasingly integrated into robotic\nsystems, their potential to generate socially and culturally appropriate\naffective touch remains largely unexplored. This study investigates whether\nLLMs-specifically GPT-3.5, GPT-4, and GPT-4o --can generate culturally adaptive\ntactile behaviours to convey emotions in human-robot interaction. We produced\ntext based touch descriptions for 12 distinct emotions across three cultural\ncontexts (Chinese, Belgian, and unspecified), and examined their\ninterpretability in both robot-to-human and human-to-robot scenarios. A total\nof 90 participants (36 Chinese, 36 Belgian, and 18 culturally unspecified)\nevaluated these LLM-generated tactile behaviours for emotional decoding and\nperceived appropriateness. Results reveal that: (1) under matched cultural\nconditions, participants successfully decoded six out of twelve emotions-mainly\nsocially oriented emotions such as love and Ekman emotions such as anger,\nhowever, self-focused emotions like pride and embarrassment were more difficult\nto interpret; (2) tactile behaviours were perceived as more appropriate when\ndirected from human to robot than from robot to human, revealing an asymmetry\nin social expectations based on interaction roles; (3) behaviours interpreted\nas aggressive (e.g., anger), overly intimate (e.g., love), or emotionally\nambiguous (i.e., not clearly decodable) were significantly more likely to be\nrated as inappropriate; and (4) cultural mismatches reduced decoding accuracy\nand increased the likelihood of behaviours being judged as inappropriate.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22905v1", "cate": "cs.HC", "date": "2025-07-02", "updated": "2025-07-02"}
{"id": "2507.23410", "title": "Towards LLM-Enhanced Product Line Scoping", "authors": ["Alexander Felfernig", "Damian Garber", "Viet-Man Le", "Sebastian Lubos", "Thi Ngoc Trang Tran"], "categories": ["cs.IR"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23410v1", "summary": "The idea of product line scoping is to identify the set of features and\nconfigurations that a product line should include, i.e., offer for\nconfiguration purposes. In this context, a major scoping task is to find a\nbalance between commercial relevance and technical feasibility. Traditional\nproduct line scoping approaches rely on formal feature models and require a\nmanual analysis which can be quite time-consuming. In this paper, we sketch how\nLarge Language Models (LLMs) can be applied to support product line scoping\ntasks with a natural language interaction based scoping process. Using a\nworking example from the smarthome domain, we sketch how LLMs can be applied to\nevaluate different feature model alternatives. We discuss open research\nchallenges regarding the integration of LLMs with product line scoping.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23410v1", "cate": "cs.IR", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23642", "title": "Efficient Masked Attention Transformer for Few-Shot Classification and Segmentation", "authors": ["Dustin Carrión-Ojeda", "Stefan Roth", "Simone Schaub-Meyer"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted for GCPR 2025. Project page: this https URL", "url": "http://arxiv.org/abs/2507.23642v1", "summary": "Few-shot classification and segmentation (FS-CS) focuses on jointly\nperforming multi-label classification and multi-class segmentation using few\nannotated examples. Although the current state of the art (SOTA) achieves high\naccuracy in both tasks, it struggles with small objects. To overcome this, we\npropose the Efficient Masked Attention Transformer (EMAT), which improves\nclassification and segmentation accuracy, especially for small objects. EMAT\nintroduces three modifications: a novel memory-efficient masked attention\nmechanism, a learnable downscaling strategy, and parameter-efficiency\nenhancements. EMAT outperforms all FS-CS methods on the PASCAL-5$^i$ and\nCOCO-20$^i$ datasets, using at least four times fewer trainable parameters.\nMoreover, as the current FS-CS evaluation setting discards available\nannotations, despite their costly collection, we introduce two novel evaluation\nsettings that consider these annotations to better reflect practical scenarios.", "comment": "Accepted for GCPR 2025. Project page: https://visinf.github.io/emat", "pdf_url": "http://arxiv.org/pdf/2507.23642v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2404.12829", "title": "LiMe: a Latin Corpus of Late Medieval Criminal Sentences", "authors": ["Alessandra Bassani", "Beatrice Del Bo", "Alfio Ferrara", "Marta Mangini", "Sergio Picascia", "Ambra Stefanello"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2404.12829v2", "summary": "The Latin language has received attention from the computational linguistics\nresearch community, which has built, over the years, several valuable\nresources, ranging from detailed annotated corpora to sophisticated tools for\nlinguistic analysis. With the recent advent of large language models,\nresearchers have also started developing models capable of generating vector\nrepresentations of Latin texts. The performances of such models remain behind\nthe ones for modern languages, given the disparity in available data. In this\npaper, we present the LiMe dataset, a corpus of 325 documents extracted from a\nseries of medieval manuscripts called Libri sententiarum potestatis Mediolani,\nand thoroughly annotated by experts, in order to be employed for masked\nlanguage model, as well as supervised natural language processing tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2404.12829v2", "cate": "cs.CL", "date": "2024-04-19", "updated": "2025-07-31"}
{"id": "2507.23372", "title": "UniEmo: Unifying Emotional Understanding and Generation with Learnable Expert Queries", "authors": ["Yijie Zhu", "Lingsen Zhang", "Zitong Yu", "Rui Shao", "Tao Tan", "Liqiang Nie"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23372v1", "summary": "Emotional understanding and generation are often treated as separate tasks,\nyet they are inherently complementary and can mutually enhance each other. In\nthis paper, we propose the UniEmo, a unified framework that seamlessly\nintegrates these two tasks. The key challenge lies in the abstract nature of\nemotions, necessitating the extraction of visual representations beneficial for\nboth tasks. To address this, we propose a hierarchical emotional understanding\nchain with learnable expert queries that progressively extracts multi-scale\nemotional features, thereby serving as a foundational step for unification.\nSimultaneously, we fuse these expert queries and emotional representations to\nguide the diffusion model in generating emotion-evoking images. To enhance the\ndiversity and fidelity of the generated emotional images, we further introduce\nthe emotional correlation coefficient and emotional condition loss into the\nfusion process. This step facilitates fusion and alignment for emotional\ngeneration guided by the understanding. In turn, we demonstrate that joint\ntraining allows the generation component to provide implicit feedback to the\nunderstanding part. Furthermore, we propose a novel data filtering algorithm to\nselect high-quality and diverse emotional images generated by the well-trained\nmodel, which explicitly feedback into the understanding part. Together, these\ngeneration-driven dual feedback processes enhance the model's understanding\ncapacity. Extensive experiments show that UniEmo significantly outperforms\nstate-of-the-art methods in both emotional understanding and generation tasks.\nThe code for the proposed method is available at\nhttps://github.com/JiuTian-VL/UniEmo.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23372v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.22952", "title": "Automated Label Placement on Maps via Large Language Models", "authors": ["Harry Shomer", "Jiejun Xu"], "categories": ["cs.HC", "cs.CV", "cs.LG"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      Workshop on AI for Data Editing (AI4DE) at KDD 2025", "url": "http://arxiv.org/abs/2507.22952v1", "summary": "Label placement is a critical aspect of map design, serving as a form of\nspatial annotation that directly impacts clarity and interpretability. Despite\nits importance, label placement remains largely manual and difficult to scale,\nas existing automated systems struggle to integrate cartographic conventions,\nadapt to context, or interpret labeling instructions. In this work, we\nintroduce a new paradigm for automatic label placement (ALP) that formulates\nthe task as a data editing problem and leverages large language models (LLMs)\nfor context-aware spatial annotation. To support this direction, we curate\nMAPLE, the first known benchmarking dataset for evaluating ALP on real-world\nmaps, encompassing diverse landmark types and label placement annotations from\nopen-source data. Our method retrieves labeling guidelines relevant to each\nlandmark type leveraging retrieval-augmented generation (RAG), integrates them\ninto prompts, and employs instruction-tuned LLMs to generate ideal label\ncoordinates. We evaluate four open-source LLMs on MAPLE, analyzing both overall\nperformance and generalization across different types of landmarks. This\nincludes both zero-shot and instruction-tuned performance. Our results\ndemonstrate that LLMs, when guided by structured prompts and domain-specific\nretrieval, can learn to perform accurate spatial edits, aligning the generated\noutputs with expert cartographic standards. Overall, our work presents a\nscalable framework for AI-assisted map finishing and demonstrates the potential\nof foundation models in structured data editing tasks. The code and data can be\nfound at https://github.com/HarryShomer/MAPLE.", "comment": "Workshop on AI for Data Editing (AI4DE) at KDD 2025", "pdf_url": "http://arxiv.org/pdf/2507.22952v1", "cate": "cs.HC", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2402.01124", "title": "TransFR: Transferable Federated Recommendation with Adapter Tuning on Pre-trained Language Models", "authors": ["Honglei Zhang", "Zhiwei Li", "Haoxuan Li", "Xin Zhou", "Jie Zhang", "Yidong Li"], "categories": ["cs.IR"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2402.01124v2", "summary": "Federated recommendations (FRs), facilitating multiple local clients to\ncollectively learn a global model without disclosing user private data, have\nemerged as a prevalent on-device service. In conventional FRs, a dominant\nparadigm is to utilize discrete identities to represent clients and items,\nwhich are then mapped to domain-specific embeddings to participate in model\ntraining. Despite considerable performance, we reveal three inherent\nlimitations that can not be ignored in federated settings, i.e.,\nnon-transferability across domains, ineffectiveness in cold-start settings, and\npotential privacy violations during federated training. To this end, we propose\na transferable federated recommendation model, TransFR, which delicately\nincorporates the general capabilities empowered by pre-trained models and the\npersonalized abilities by fine-tuning local private data. Specifically, it\nfirst learns domain-agnostic representations of items by exploiting pre-trained\nmodels with public textual corpora. To tailor for FR tasks, we further\nintroduce efficient federated adapter-tuning and test-time adaptation\nmechanisms, which facilitate personalized local adapters for each client by\nfitting their private data distributions. We theoretically prove the advantages\nof incorporating adapter tuning in FRs regarding both effectiveness and\nprivacy. Through extensive experiments, we show that our TransFR model\nsurpasses several state-of-the-art FRs on transferability.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2402.01124v2", "cate": "cs.IR", "date": "2024-02-02", "updated": "2025-07-31"}
{"id": "2507.23029", "title": "A CPFSK Transceiver with Hybrid CSS-DSSS Spreading for LPWAN PHY Communication", "authors": ["Wenkun Wen", "Ruiqi Zhang", "Peiran Wu", "Tierui Min", "Minghua Xia"], "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "Comments:      15 pages, 12 figures, and 4 tables. To appear in IEEE Internet of Things Journal", "url": "http://arxiv.org/abs/2507.23029v1", "summary": "Traditional low-power wide-area network (LPWAN) transceivers typically\ncompromise data rates to achieve deep coverage. This paper presents a novel\ntransceiver that achieves high receiver sensitivity and low computational\ncomplexity. At the transmitter, we replace the conventional direct sequence\nspread spectrum (DSSS) preamble with a chirp spread spectrum (CSS) preamble,\nconsisting of a pair of down-chirp and up-chirp signals that are conjugate to\neach other, simplifying packet synchronization. For enhanced coverage, the\npayload incorporates continuous phase frequency shift keying (CPFSK) to\nmaintain a constant envelope and phase continuity, in conjunction with DSSS to\nachieve a high spreading gain. At the receiver, we develop a double-peak\ndetection method to improve synchronization and a non-coherent joint\ndespreading and demodulation scheme that increases receiver sensitivity while\nmaintaining simplicity in implementation. Furthermore, we optimize the preamble\ndetection threshold and spreading sequences for maximum non-coherent receiver\nperformance. The software-defined radio (SDR) prototype, developed using GNU\nRadio and USRP, along with operational snapshots, showcases its practical\nengineering applications. Extensive Monte Carlo simulations and field-test\ntrials demonstrate that our transceiver outperforms traditional ones in terms\nof receiver sensitivity, while also being low in complexity and cost-effective\nfor LPWAN requirements.", "comment": "15 pages, 12 figures, and 4 tables. To appear in IEEE Internet of\n  Things Journal", "pdf_url": "http://arxiv.org/pdf/2507.23029v1", "cate": "cs.IT", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.23682", "title": "villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models", "authors": ["Xiaoyu Chen", "Hangxing Wei", "Pushi Zhang", "Chuheng Zhang", "Kaixin Wang", "Yanjiang Guo", "Rushuai Yang", "Yucen Wang", "Xinquan Xiao", "Li Zhao", "Jianyu Chen", "Jiang Bian"], "categories": ["cs.RO", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      Project page: this https URL", "url": "http://arxiv.org/abs/2507.23682v1", "summary": "Visual-Language-Action (VLA) models have emerged as a popular paradigm for\nlearning robot manipulation policies that can follow language instructions and\ngeneralize to novel scenarios. Recent work has begun to explore the\nincorporation of latent actions, an abstract representation of visual change\nbetween two frames, into VLA pre-training. In this paper, we introduce villa-X,\na novel Visual-Language-Latent-Action (ViLLA) framework that advances latent\naction modeling for learning generalizable robot manipulation policies. Our\napproach improves both how latent actions are learned and how they are\nincorporated into VLA pre-training. Together, these contributions enable\nvilla-X to achieve superior performance across simulated environments including\nSIMPLER and LIBERO, as well as on two real-world robot setups including gripper\nand dexterous hand manipulation. We believe the ViLLA paradigm holds\nsignificant promise, and that our villa-X provides a strong foundation for\nfuture research.", "comment": "Project page: https://aka.ms/villa-x", "pdf_url": "http://arxiv.org/pdf/2507.23682v1", "cate": "cs.RO", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2406.14313", "title": "Iterative Repair with Weak Verifiers for Few-shot Transfer in KBQA with Unanswerability", "authors": ["Riya Sawhney", "Samrat Yadav", "Indrajit Bhattacharya", "Mausam"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2406.14313v3", "summary": "Real-world applications of KBQA require models to handle unanswerable\nquestions with a limited volume of in-domain labeled training data. We propose\nthe novel task of few-shot transfer for KBQA with unanswerable questions and\ncontribute two new datasets for performance evaluation. We present FUn-FuSIC -\na novel solution for our task that extends FuSIC KBQA, the state-of-the-art\nfew-shot transfer model for answerable-only KBQA. We first note that\nFuSIC-KBQA's iterative repair makes a strong assumption that all questions are\nunanswerable. As a remedy, we propose Feedback for Unanswerability (FUn), which\nuses iterative repair using feedback from a suite of strong and weak verifiers,\nand an adaptation of self consistency for unanswerabilty to better assess the\nanswerability of a question. Our experiments show that FUn-FuSIC significantly\noutperforms suitable adaptations of multiple LLM based and supervised SoTA\nmodels on our task, while establishing a new SoTA for answerable few-shot\ntransfer as well.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2406.14313v3", "cate": "cs.CL", "date": "2024-06-20", "updated": "2025-07-31"}
{"id": "2507.23373", "title": "Multi-Prompt Progressive Alignment for Multi-Source Unsupervised Domain Adaptation", "authors": ["Haoran Chen", "Zexiao Wang", "Haidong Cao", "Zuxuan Wu", "Yu-Gang Jiang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23373v1", "summary": "Large Vision-Language Models like CLIP have become a powerful foundation for\nUnsupervised Domain Adaptation due to their strong zero-shot generalization.\nState-of-the-art methods typically leverage CLIP to generate pseudo-labels for\nthe target domain, then fine-tune the model to learn domain-invariant features.\nHowever, these methods attempt to align source and target domains using all\npseudo-labeled data simultaneously. This one-shot alignment struggles with\nnoisy, hard-to-classify samples, leading to error propagation and suboptimal\nfeature learning. The problem is even more amplified in the multi-source\nscenario, where diverse domain gaps and varying noise levels across multiple\nsource domains further destabilize the alignment process. To address this\nissue, in this work, we propose a progressive alignment strategy for adapting\nCLIP to unlabeled downstream task. Our method begins by training the model on a\nhigh-confidence subset of target samples, allowing it to first learn a\nwell-aligned representation from the most reliable data. As training\nprogresses, it gradually incorporates more challenging samples, guiding the\nmodel to refine its understanding without being overwhelmed by initial label\nnoise. This progressive approach effectively mitigates confirmation bias and\npromotes a more robust convergence, allowing for the learning of genuinely\ndomain-invariant features. We name our approach MP^2A and test it on three\npopular UDA benchmarks, namely ImageCLEF, Office-Home, and the most challenging\nDomainNet. Experiments showcase that MP^2A achieves state-of-the-art\nperformance when compared with most recent CLIP-based MS-UDA approaches,\ndemonstrating the effectiveness of our approach.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23373v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23096", "title": "ChatVis: Large Language Model Agent for Generating Scientific Visualizations", "authors": ["Tom Peterka", "Tanwi Mallick", "Orcun Yildiz", "David Lenz", "Cory Quammen", "Berk Geveci"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23096v1", "summary": "Large language models (LLMs) are rapidly increasing in capability, but they\nstill struggle with highly specialized programming tasks such as scientific\nvisualization. We present an LLM assistant, ChatVis, that aids the LLM to\ngenerate Python code for ParaView scientific visualization tasks, without the\nneed for retraining or fine-tuning the LLM. ChatVis employs chain-of-thought\nprompt simplification, retrieval-augmented prompt generation using a vector\ndatabase of documentation and code examples, and error checking with iterative\nprompt feedback to correct errors until a visualization is produced. An\nintegral part of our approach is a benchmark suite of canonical visualization\ntasks, ParaView regression tests, and scientific use cases that includes\ncomprehensive evaluation metrics. We evaluate our visualization assistant by\ncomparing results with a variety of top-performing unassisted LLMs. We find\nthat all the metrics are significantly improved with ChatVis.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23096v1", "cate": "cs.HC", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2411.06254", "title": "KeyB2: Selecting Key Blocks is Also Important for Long Document Ranking with Large Language Models", "authors": ["Minghan Li", "Eric Gaussier", "Juntao Li", "Guodong Zhou"], "categories": ["cs.IR"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2411.06254v2", "summary": "The emergence of large language models (LLMs) such as Llama has significantly\nadvanced neural information retrieval (IR). However, applying LLMs to long\ndocument reranking remains computationally expensive and may be ineffective.\nMoreover, the internal behavior of LLMs during document relevance judgment is\nstill underexplored. In this paper, we begin with an in-depth analysis of\ndecoder-only LLM attention patterns and find that several attention heads\nconsistently align with relevance signals, yet this alignment deteriorates as\nirrelevant content increases. Motivated by this observation, we revisit and\nextend the block selection paradigm, introducing KeyB2, a scalable reranking\nframework that combines block pre-selection with powerful decoder-only LLMs.\nKeyB2 generalizes the selection stage to support BM25, cross-encoder, and\nbi-encoder, and adapts LLM to compute fine-grained relevance scores. We further\nintroduce a new bi-encoder strategy that performs strongly and efficiently.\nExtensive experiments on TREC DL 2019/2023 document task, Robust04, and MLDR-zh\ndemonstrate that KeyB2 outperforms baselines including RankLLaMA,\nRankLLaMA-MaxP/AvgP, and KeyB, achieving new state-of-the-art (SOTA) results on\nTREC DL 2019 document reranking task. In addition, KeyB2 reduces reranking\nlatency compared with RankLLaMA by over 83% and memory usage by over 74%,\npositioning it as a practical and effective solution for long document ranking\nwith LLMs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2411.06254v2", "cate": "cs.IR", "date": "2024-11-09", "updated": "2025-07-31"}
{"id": "2507.23175", "title": "Optimal compressed sensing for mixing stochastic processes", "authors": ["Yonatan Gutman", "Adam Śpiewak"], "categories": ["cs.IT", "math.DS", "math.IT", "math.PR", "68P30, 94A29, 31E05, 37A35, 60G10"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23175v1", "summary": "Jalali and Poor introduced an asymptotic framework for compressed sensing of\nstochastic processes, demonstrating that any rate strictly greater than the\nmean information dimension serves as an upper bound on the number of random\nlinear measurements required for (universal) almost lossless recovery of\n$\\psi^*$-mixing processes, as measured in the normalized $L^2$ norm. In this\nwork, we show that if the normalized number of random linear measurements is\nstrictly less than the mean information dimension, then almost lossless\nrecovery of a $\\psi^*$-mixing process is impossible by any sequence of\ndecompressors. This establishes the mean information dimension as the\nfundamental limit for compressed sensing in this setting (and, in fact, the\nprecise threshold for the problem). To this end, we introduce a new quantity,\nrelated to techniques from geometric measure theory: the correlation dimension\nrate, which is shown to be a lower bound for compressed sensing of arbitrary\nstationary stochastic processes.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23175v1", "cate": "cs.IT", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23694", "title": "A survey of multi-agent geosimulation methodologies: from ABM to LLM", "authors": ["Virginia Padilla", "Jacinto Dávila"], "categories": ["cs.MA", "cs.AI", "68T42", "I.2.11"], "primary_category": "Subjects:       Multiagent Systems (cs.MA)", "pdf_link": null, "comments": "Comments:      20 pages, 1 table", "url": "http://arxiv.org/abs/2507.23694v1", "summary": "We provide a comprehensive examination of agent-based approaches that codify\nthe principles and linkages underlying multi-agent systems, simulations, and\ninformation systems. Based on two decades of study, this paper confirms a\nframework intended as a formal specification for geosimulation platforms. Our\nfindings show that large language models (LLMs) can be effectively incorporated\nas agent components if they follow a structured architecture specific to\nfundamental agent activities such as perception, memory, planning, and action.\nThis integration is precisely consistent with the architecture that we\nformalize, providing a solid platform for next-generation geosimulation\nsystems.", "comment": "20 pages, 1 table", "pdf_url": "http://arxiv.org/pdf/2507.23694v1", "cate": "cs.MA", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2406.15444", "title": "Cutting Through the Noise: Boosting LLM Performance on Math Word Problems", "authors": ["Ujjwala Anantheswaran", "Himanshu Gupta", "Kevin Scaria", "Shreyas Verma", "Chitta Baral", "Swaroop Mishra"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Published at ICLR 2025 Workshop on Reasoning and Planning for LLMs", "url": "http://arxiv.org/abs/2406.15444v4", "summary": "Large Language Models (LLMs) excel at various tasks, including solving math\nword problems (MWPs), but struggle with real-world problems containing\nirrelevant information. To address this, we propose a prompting framework that\ngenerates adversarial variants of MWPs by adding irrelevant variables. We\nintroduce a dataset, PROBLEMATHIC, containing both adversarial and\nnon-adversarial MWPs. Our experiments reveal that LLMs are susceptible to\ndistraction by numerical noise, resulting in an average relative performance\ndrop of ~26% on adversarial MWPs. To mitigate this, we fine-tune LLMs (Llama-2,\nMistral) on the adversarial samples from our dataset. Fine-tuning on\nadversarial training instances improves performance on adversarial MWPs by ~8%,\nindicating increased robustness to noise and improved ability to identify\nrelevant data for reasoning. Finally, to assess the generalizability of our\nprompting framework, we introduce GSM-8K-Adv, an adversarial variant of the\nGSM-8K benchmark. LLMs continue to struggle when faced with adversarial\ninformation, reducing performance by up to 6%.", "comment": "Published at ICLR 2025 Workshop on Reasoning and Planning for LLMs", "pdf_url": "http://arxiv.org/pdf/2406.15444v4", "cate": "cs.CL", "date": "2024-05-30", "updated": "2025-07-31"}
{"id": "2507.23374", "title": "NeRF Is a Valuable Assistant for 3D Gaussian Splatting", "authors": ["Shuangkang Fang", "I-Chao Shen", "Takeo Igarashi", "Yufeng Wang", "ZeSheng Wang", "Yi Yang", "Wenrui Ding", "Shuchang Zhou"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ICCV", "url": "http://arxiv.org/abs/2507.23374v1", "summary": "We introduce NeRF-GS, a novel framework that jointly optimizes Neural\nRadiance Fields (NeRF) and 3D Gaussian Splatting (3DGS). This framework\nleverages the inherent continuous spatial representation of NeRF to mitigate\nseveral limitations of 3DGS, including sensitivity to Gaussian initialization,\nlimited spatial awareness, and weak inter-Gaussian correlations, thereby\nenhancing its performance. In NeRF-GS, we revisit the design of 3DGS and\nprogressively align its spatial features with NeRF, enabling both\nrepresentations to be optimized within the same scene through shared 3D spatial\ninformation. We further address the formal distinctions between the two\napproaches by optimizing residual vectors for both implicit features and\nGaussian positions to enhance the personalized capabilities of 3DGS.\nExperimental results on benchmark datasets show that NeRF-GS surpasses existing\nmethods and achieves state-of-the-art performance. This outcome confirms that\nNeRF and 3DGS are complementary rather than competing, offering new insights\ninto hybrid approaches that combine 3DGS and NeRF for efficient 3D scene\nrepresentation.", "comment": "Accepted by ICCV", "pdf_url": "http://arxiv.org/pdf/2507.23374v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23215", "title": "Silent Impact: Tracking Tennis Shots from the Passive Arm", "authors": ["Junyong Park", "Saelyne Yang", "Sungho Jo"], "categories": ["cs.HC", "H.5.2; I.5.4"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      15 pages, 9 figures,", "url": "http://arxiv.org/abs/2507.23215v1", "summary": "Wearable technology has transformed sports analytics, offering new dimensions\nin enhancing player experience. Yet, many solutions involve cumbersome setups\nthat inhibit natural motion. In tennis, existing products require sensors on\nthe racket or dominant arm, causing distractions and discomfort. We propose\nSilent Impact, a novel and user-friendly system that analyzes tennis shots\nusing a sensor placed on the passive arm. Collecting Inertial Measurement Unit\nsensor data from 20 recreational tennis players, we developed neural networks\nthat exclusively utilize passive arm data to detect and classify six shots,\nachieving a classification accuracy of 88.2% and a detection F1 score of 86.0%,\ncomparable to the dominant arm. These models were then incorporated into an\nend-to-end prototype, which records passive arm motion through a smartwatch and\ndisplays a summary of shots on a mobile app. User study (N=10) showed that\nparticipants felt less burdened physically and mentally using Silent Impact on\nthe passive arm. Overall, our research establishes the passive arm as an\neffective, comfortable alternative for tennis shot analysis, advancing\nuser-friendly sports analytics.", "comment": "15 pages, 9 figures,", "pdf_url": "http://arxiv.org/pdf/2507.23215v1", "cate": "cs.HC", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.12311", "title": "An Ecosystem for Ontology Interoperability", "authors": ["Zhangcheng Qiang"], "categories": ["cs.IR"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "Comments:      5 pages, 8 figures", "url": "http://arxiv.org/abs/2507.12311v3", "summary": "Ontology interoperability is one of the complicated issues that restricts the\nuse of ontologies in knowledge graphs (KGs). Different ontologies with\nconflicting and overlapping concepts make it difficult to design, develop, and\ndeploy an interoperable ontology for downstream tasks. We propose an ecosystem\nfor ontology interoperability. The ecosystem employs three state-of-the-art\nsemantic techniques in different phases of the ontology engineering life cycle:\nontology design patterns (ODPs) in the design phase, ontology matching and\nversioning (OM\\&OV) in the develop phase, and ontology-compliant knowledge\ngraphs (OCKGs) in the deploy phase, to achieve better ontology interoperability\nand data integration in real-world applications. A case study of sensor\nobservation in the building domain validates the usefulness of the proposed\necosystem.", "comment": "5 pages, 8 figures", "pdf_url": "http://arxiv.org/pdf/2507.12311v3", "cate": "cs.IR", "date": "2025-07-16", "updated": "2025-07-31"}
{"id": "2507.23180", "title": "The Construction of Near-optimal Universal Coding of Integers", "authors": ["Wei Yan", "Yunghsiang S. Han"], "categories": ["cs.IT", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23180v1", "summary": "Universal Coding of Integers (UCI) is suitable for discrete memoryless\nsources with unknown probability distributions and infinitely countable\nalphabet sizes. The UCI is a class of prefix codes, such that the ratio of the\naverage codeword length to $\\max\\{1, H(P)\\}$ is within a constant expansion\nfactor $K_{\\mathcal{C}}$ for any decreasing probability distribution $P$, where\n$H(P)$ is the entropy of $P$. For any UCI code $\\mathcal{C}$, define \\emph{the\nminimum expansion factor} $K_{\\mathcal{C}}^{*}$ to represent the infimum of the\nset of extension factors of $\\mathcal{C}$. Each $\\mathcal{C}$ has a unique\ncorresponding $K_{\\mathcal{C}}^{*}$, and the smaller $K_{\\mathcal{C}}^{*}$ is,\nthe better the compression performance of $\\mathcal{C}$ is. A class of UCI\n$\\mathcal{C}$ (or family $\\{\\mathcal{C}_i\\}_{i=1}^{\\infty}$) achieving the\nsmallest $K_{\\mathcal{C}}^{*}$ is defined as the \\emph{optimal UCI}. The best\nresult currently is that the range of $C_{\\mathcal{C}}^{*}$ for the optimal UCI\nis $2\\leq C_{\\mathcal{C}}^{*}\\leq 2.5$. In this paper, we prove that there\nexists a class of near-optimal UCIs, called $\\nu$ code, to achieve\n$K_\\nu=2.0386$. This narrows the range of the minimum expansion factor for\noptimal UCI to $2\\leq C_{\\mathcal{C}}^{*}\\leq 2.0386$. Another new class of\nUCI, called $\\Delta\\delta$ code, is specifically constructed. We show that the\n$\\Delta\\delta$ code and $\\nu$ code are currently optimal in terms of minimum\nexpansion factor. In addition, we propose a new proof that shows the minimum\nexpansion factor of the optimal UCI is lower bounded by $2$.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23180v1", "cate": "cs.IT", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23698", "title": "Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents", "authors": ["Shaofei Cai", "Zhancun Mu", "Haiwen Xia", "Bowei Zhang", "Anji Liu", "Yitao Liang"], "categories": ["cs.RO", "cs.AI"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23698v1", "summary": "While Reinforcement Learning (RL) has achieved remarkable success in language\nmodeling, its triumph hasn't yet fully translated to visuomotor agents. A\nprimary challenge in RL models is their tendency to overfit specific tasks or\nenvironments, thereby hindering the acquisition of generalizable behaviors\nacross diverse settings. This paper provides a preliminary answer to this\nchallenge by demonstrating that RL-finetuned visuomotor agents in Minecraft can\nachieve zero-shot generalization to unseen worlds. Specifically, we explore\nRL's potential to enhance generalizable spatial reasoning and interaction\ncapabilities in 3D worlds. To address challenges in multi-task RL\nrepresentation, we analyze and establish cross-view goal specification as a\nunified multi-task goal space for visuomotor policies. Furthermore, to overcome\nthe significant bottleneck of manual task design, we propose automated task\nsynthesis within the highly customizable Minecraft environment for large-scale\nmulti-task RL training, and we construct an efficient distributed RL framework\nto support this. Experimental results show RL significantly boosts interaction\nsuccess rates by $4\\times$ and enables zero-shot generalization of spatial\nreasoning across diverse environments, including real-world settings. Our\nfindings underscore the immense potential of RL training in 3D simulated\nenvironments, especially those amenable to large-scale task generation, for\nsignificantly advancing visuomotor agents' spatial reasoning.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23698v1", "cate": "cs.RO", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2410.02744", "title": "Neutral Residues: Revisiting Adapters for Model Extension", "authors": ["Franck Signe Talla", "Edouard Grave", "Hervé Jégou"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Accepted at ICML 2025", "url": "http://arxiv.org/abs/2410.02744v3", "summary": "We address the problem of extending a pretrained large language model to a\nnew domain that was not seen during training. Standard techniques, such as\nfinetuning or low-rank adaptation (LoRA) are successful at domain adaptation,\nbut do not formally add capacity to the model. This often leads to a trade-off,\nbetween performing well on the new domain vs. degrading performance on the\noriginal domain. Here, we revisit and improve adapters to extend LLMs from\nthree angles: data, architecture and training procedure, which are\nadvantageously considered jointly. The resulting method, called neutral\nresidues, modifies adapters in a way that leads each new residual block to\noutput near-zeros on the original domain. This solution leads to strong results\nwhen adapting a state-of-the-art model originally trained on English to a new\nlanguage. Neutral residues significantly outperform competing approaches such\nas finetuning, LoRA or vanilla adapters in terms of the trade-off between\nlearning the new language and not forgetting English.", "comment": "Accepted at ICML 2025", "pdf_url": "http://arxiv.org/pdf/2410.02744v3", "cate": "cs.CL", "date": "2024-10-03", "updated": "2025-07-31"}
{"id": "2507.23411", "title": "Out-of-Distribution Detection in Medical Imaging via Diffusion Trajectories", "authors": ["Lemar Abdi", "Francisco Caetano", "Amaan Valiuddin", "Christiaan Viviers", "Hamdi Joudeh", "Fons van der Sommen"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted at Uncertainty for Safe Utilization of Machine Learning in Medical Imaging, MICCAI 2025", "url": "http://arxiv.org/abs/2507.23411v1", "summary": "In medical imaging, unsupervised out-of-distribution (OOD) detection offers\nan attractive approach for identifying pathological cases with extremely low\nincidence rates. In contrast to supervised methods, OOD-based approaches\nfunction without labels and are inherently robust to data imbalances. Current\ngenerative approaches often rely on likelihood estimation or reconstruction\nerror, but these methods can be computationally expensive, unreliable, and\nrequire retraining if the inlier data changes. These limitations hinder their\nability to distinguish nominal from anomalous inputs efficiently, consistently,\nand robustly. We propose a reconstruction-free OOD detection method that\nleverages the forward diffusion trajectories of a Stein score-based denoising\ndiffusion model (SBDDM). By capturing trajectory curvature via the estimated\nStein score, our approach enables accurate anomaly scoring with only five\ndiffusion steps. A single SBDDM pre-trained on a large, semantically aligned\nmedical dataset generalizes effectively across multiple Near-OOD and Far-OOD\nbenchmarks, achieving state-of-the-art performance while drastically reducing\ncomputational cost during inference. Compared to existing methods, SBDDM\nachieves a relative improvement of up to 10.43% and 18.10% for Near-OOD and\nFar-OOD detection, making it a practical building block for real-time, reliable\ncomputer-aided diagnosis.", "comment": "Accepted at Uncertainty for Safe Utilization of Machine Learning in\n  Medical Imaging, MICCAI 2025", "pdf_url": "http://arxiv.org/pdf/2507.23411v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23298", "title": "Real-time Generation of Various Types of Nodding for Avatar Attentive Listening System", "authors": ["Kazushi Kato", "Koji Inoue", "Divesh Lala", "Keiko Ochi", "Tatsuya Kawahara"], "categories": ["cs.HC", "cs.SD", "eess.AS"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      Accepted by 27th ACM International Conference on Multimodal Interaction (ICMI '25), Long paper", "url": "http://arxiv.org/abs/2507.23298v1", "summary": "In human dialogue, nonverbal information such as nodding and facial\nexpressions is as crucial as verbal information, and spoken dialogue systems\nare also expected to express such nonverbal behaviors. We focus on nodding,\nwhich is critical in an attentive listening system, and propose a model that\npredicts both its timing and type in real time. The proposed model builds on\nthe voice activity projection (VAP) model, which predicts voice activity from\nboth listener and speaker audio. We extend it to prediction of various types of\nnodding in a continuous and real-time manner unlike conventional models. In\naddition, the proposed model incorporates multi-task learning with verbal\nbackchannel prediction and pretraining on general dialogue data. In the timing\nand type prediction task, the effectiveness of multi-task learning was\nsignificantly demonstrated. We confirmed that reducing the processing rate\nenables real-time operation without a substantial drop in accuracy, and\nintegrated the model into an avatar attentive listening system. Subjective\nevaluations showed that it outperformed the conventional method, which always\ndoes nodding in sync with verbal backchannel. The code and trained models are\navailable at https://github.com/MaAI-Kyoto/MaAI.", "comment": "Accepted by 27th ACM International Conference on Multimodal\n  Interaction (ICMI '25), Long paper", "pdf_url": "http://arxiv.org/pdf/2507.23298v1", "cate": "cs.HC", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.18518", "title": "Transform Before You Query: A Privacy-Preserving Approach for Vector Retrieval with Embedding Space Alignment", "authors": ["Ruiqi He", "Zekun Fei", "Jiaqi Li", "Xinyuan Zhu", "Biao Yi", "Siyi Lv", "Weijie Liu", "Zheli Liu"], "categories": ["cs.IR"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18518v2", "summary": "Vector Database (VDB) can efficiently index and search high-dimensional\nvector embeddings from unstructured data, crucially enabling fast semantic\nsimilarity search essential for modern AI applications like generative AI and\nrecommendation systems. Since current VDB service providers predominantly use\nproprietary black-box models, users are forced to expose raw query text to them\nvia API in exchange for the vector retrieval services. Consequently, if query\ntext involves confidential records from finance or healthcare domains, this\nmechanism inevitably leads to critical leakage of user's sensitive information.\nTo address this issue, we introduce STEER (\\textbf{S}ecure \\textbf{T}ransformed\n\\textbf{E}mbedding v\\textbf{E}ctor\\textbf{ R}etrieval), a private vector\nretrieval framework that leverages the alignment relationship between the\nsemantic spaces of different embedding models to derive approximate embeddings\nfor the query text. STEER performs the retrieval using the approximate\nembeddings within the original VDB and requires no modifications to the server\nside. Our theoretical and experimental analyses demonstrate that STEER\neffectively safeguards query text privacy while maintaining the retrieval\naccuracy. Even though approximate embeddings are approximations of the\nembeddings from proprietary models, they still prevent the providers from\nrecovering the query text through Embedding Inversion Attacks (EIAs). Extensive\nexperimental results show that Recall@100 of STEER can basically achieve a\ndecrease of less than 5\\%. Furthermore, even when searching within a text\ncorpus of millions of entries, STEER achieves a Recall@20 accuracy 20\\% higher\nthan current baselines.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18518v2", "cate": "cs.IR", "date": "2025-07-24", "updated": "2025-07-31"}
{"id": "2507.23200", "title": "Efficient DFT of Zadoff-Chu Sequences using lmFH Pattern", "authors": ["Fanping Du"], "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "Comments:      8 pages, 7 figures", "url": "http://arxiv.org/abs/2507.23200v1", "summary": "Having established that Zadoff-Chu (ZC) sequences are inherently linear\nmicro-frequency hopping (lmFH) symbols, this paper first presents an intuitive\nand visual exposition of the computation of the DFT and IDFT of ZC sequences\nusing the lmFH pattern. This yields interesting results. Subsequently, an\nalternative form for computing the cumulative sum of ZC sequences using the\nGeneralized Quadratic Gauss Sum is introduced. Furthermore, building on the\nmicro-frequency hopping (mFH) concept, this paper shows that the DFT of ZC\nsequences can be transformed into an lmFH symbol with frequency shift and phase\noffset. Therefore, the DFT of ZC sequences can be computed via cumulative\nfrequency points, similar to the computation of normal mFH symbols.", "comment": "8 pages, 7 figures", "pdf_url": "http://arxiv.org/pdf/2507.23200v1", "cate": "cs.IT", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23704", "title": "Enhanced Velocity Field Modeling for Gaussian Video Reconstruction", "authors": ["Zhenyang Li", "Xiaoyang Bai", "Tongchen Zhang", "Pengfei Shen", "Weiwei Xu", "Yifan Peng"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      17 pages, 8 figures", "url": "http://arxiv.org/abs/2507.23704v1", "summary": "High-fidelity 3D video reconstruction is essential for enabling real-time\nrendering of dynamic scenes with realistic motion in virtual and augmented\nreality (VR/AR). The deformation field paradigm of 3D Gaussian splatting has\nachieved near-photorealistic results in video reconstruction due to the great\nrepresentation capability of deep deformation networks. However, in videos with\ncomplex motion and significant scale variations, deformation networks often\noverfit to irregular Gaussian trajectories, leading to suboptimal visual\nquality. Moreover, the gradient-based densification strategy designed for\nstatic scene reconstruction proves inadequate to address the absence of dynamic\ncontent. In light of these challenges, we propose a flow-empowered velocity\nfield modeling scheme tailored for Gaussian video reconstruction, dubbed\nFlowGaussian-VR. It consists of two core components: a velocity field rendering\n(VFR) pipeline which enables optical flow-based optimization, and a\nflow-assisted adaptive densification (FAD) strategy that adjusts the number and\nsize of Gaussians in dynamic regions. We validate our model's effectiveness on\nmulti-view dynamic reconstruction and novel view synthesis with multiple\nreal-world datasets containing challenging motion scenarios, demonstrating not\nonly notable visual improvements (over 2.5 dB gain in PSNR) and less blurry\nartifacts in dynamic textures, but also regularized and trackable per-Gaussian\ntrajectories.", "comment": "17 pages, 8 figures", "pdf_url": "http://arxiv.org/pdf/2507.23704v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2411.18337", "title": "Can LLMs assist with Ambiguity? A Quantitative Evaluation of various Large Language Models on Word Sense Disambiguation", "authors": ["T. G. D. K. Sumanathilaka", "Nicholas Micallef", "Julian Hough"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      12 pages,6 tables, 1 figure, Proceedings of the 1st International Conference on NLP & AI for Cyber Security", "url": "http://arxiv.org/abs/2411.18337v4", "summary": "Ambiguous words are often found in modern digital communications. Lexical\nambiguity challenges traditional Word Sense Disambiguation (WSD) methods, due\nto limited data. Consequently, the efficiency of translation, information\nretrieval, and question-answering systems is hindered by these limitations.\nThis study investigates the use of Large Language Models (LLMs) to improve WSD\nusing a novel approach combining a systematic prompt augmentation mechanism\nwith a knowledge base (KB) consisting of different sense interpretations. The\nproposed method incorporates a human-in-loop approach for prompt augmentation\nwhere prompt is supported by Part-of-Speech (POS) tagging, synonyms of\nambiguous words, aspect-based sense filtering and few-shot prompting to guide\nthe LLM. By utilizing a few-shot Chain of Thought (COT) prompting-based\napproach, this work demonstrates a substantial improvement in performance. The\nevaluation was conducted using FEWS test data and sense tags. This research\nadvances accurate word interpretation in social media and digital\ncommunication.", "comment": "12 pages,6 tables, 1 figure, Proceedings of the 1st International\n  Conference on NLP & AI for Cyber Security", "pdf_url": "http://arxiv.org/pdf/2411.18337v4", "cate": "cs.CL", "date": "2024-11-27", "updated": "2025-07-31"}
{"id": "2507.23416", "title": "Honey Adulteration Detection using Hyperspectral Imaging and Machine Learning", "authors": ["Mokhtar A. Al-Awadhi", "Ratnadeep R. Deshmukh"], "categories": ["cs.CV", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23416v1", "summary": "This paper aims to develop a machine learning-based system for automatically\ndetecting honey adulteration with sugar syrup, based on honey hyperspectral\nimaging data. First, the floral source of a honey sample is classified by a\nbotanical origin identification subsystem. Then, the sugar syrup adulteration\nis identified, and its concentration is quantified by an adulteration detection\nsubsystem. Both subsystems consist of two steps. The first step involves\nextracting relevant features from the honey sample using Linear Discriminant\nAnalysis (LDA). In the second step, we utilize the K-Nearest Neighbors (KNN)\nmodel to classify the honey botanical origin in the first subsystem and\nidentify the adulteration level in the second subsystem. We assess the proposed\nsystem performance on a public honey hyperspectral image dataset. The result\nindicates that the proposed system can detect adulteration in honey with an\noverall cross-validation accuracy of 96.39%, making it an appropriate\nalternative to the current chemical-based detection methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23416v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23585", "title": "Agency Among Agents: Designing with Hypertextual Friction in the Algorithmic Web", "authors": ["Sophia Liu", "Shm Garanganao Almeda"], "categories": ["cs.HC", "cs.AI", "cs.MM", "cs.SI"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      To appear in: Adjunct Proceedings of the 36th ACM Conference on Hypertext and Social Media, Chicago, IL, USA, September 15-18, 2025", "url": "http://arxiv.org/abs/2507.23585v1", "summary": "Today's algorithm-driven interfaces, from recommendation feeds to GenAI\ntools, often prioritize engagement and efficiency at the expense of user\nagency. As systems take on more decision-making, users have less control over\nwhat they see and how meaning or relationships between content are constructed.\nThis paper introduces \"Hypertextual Friction,\" a conceptual design stance that\nrepositions classical hypertext principles--friction, traceability, and\nstructure--as actionable values for reclaiming agency in algorithmically\nmediated environments. Through a comparative analysis of real-world\ninterfaces--Wikipedia vs. Instagram Explore, and Are.na vs. GenAI image\ntools--we examine how different systems structure user experience, navigation,\nand authorship. We show that hypertext systems emphasize provenance,\nassociative thinking, and user-driven meaning-making, while algorithmic systems\ntend to obscure process and flatten participation. We contribute: (1) a\ncomparative analysis of how interface structures shape agency in user-driven\nversus agent-driven systems, and (2) a conceptual stance that offers\nhypertextual values as design commitments for reclaiming agency in an\nincreasingly algorithmic web.", "comment": "To appear in: Adjunct Proceedings of the 36th ACM Conference on\n  Hypertext and Social Media, Chicago, IL, USA, September 15-18, 2025", "pdf_url": "http://arxiv.org/pdf/2507.23585v1", "cate": "cs.HC", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.22879", "title": "RecGPT Technical Report", "authors": ["Chao Yi", "Dian Chen", "Gaoyang Guo", "Jiakai Tang", "Jian Wu", "Jing Yu", "Mao Zhang", "Sunhao Dai", "Wen Chen", "Wenjun Yang", "Yuning Jiang", "Zhujin Gao", "Bo Zheng", "Chi Li", "Dimin Wang", "Dixuan Wang", "Fan Li", "Fan Zhang", "Haibin Chen", "Haozhuang Liu", "Jialin Zhu", "Jiamang Wang", "Jiawei Wu", "Jin Cui", "Ju Huang", "Kai Zhang", "Kan Liu", "Lang Tian", "Liang Rao", "Longbin Li", "Lulu Zhao", "Na He", "Peiyang Wang", "Qiqi Huang", "Tao Luo", "Wenbo Su", "Xiaoxiao He", "Xin Tong", "Xu Chen", "Xunke Xi", "Yang Li", "Yaxuan Wu", "Yeqiu Yang", "Yi Hu", "Yinnan Song", "Yuchen Li", "Yujie Luo", "Yujin Yuan", "Yuliang Yan", "Zhengyang Wang", "Zhibo Xiao", "Zhixin Ma", "Zile Zhou", "Ziqi Zhang"], "categories": ["cs.IR", "cs.CL"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22879v2", "summary": "Recommender systems are among the most impactful applications of artificial\nintelligence, serving as critical infrastructure connecting users, merchants,\nand platforms. However, most current industrial systems remain heavily reliant\non historical co-occurrence patterns and log-fitting objectives, i.e.,\noptimizing for past user interactions without explicitly modeling user intent.\nThis log-fitting approach often leads to overfitting to narrow historical\npreferences, failing to capture users' evolving and latent interests. As a\nresult, it reinforces filter bubbles and long-tail phenomena, ultimately\nharming user experience and threatening the sustainability of the whole\nrecommendation ecosystem.\n  To address these challenges, we rethink the overall design paradigm of\nrecommender systems and propose RecGPT, a next-generation framework that places\nuser intent at the center of the recommendation pipeline. By integrating large\nlanguage models (LLMs) into key stages of user interest mining, item retrieval,\nand explanation generation, RecGPT transforms log-fitting recommendation into\nan intent-centric process. To effectively align general-purpose LLMs to the\nabove domain-specific recommendation tasks at scale, RecGPT incorporates a\nmulti-stage training paradigm, which integrates reasoning-enhanced\npre-alignment and self-training evolution, guided by a Human-LLM cooperative\njudge system. Currently, RecGPT has been fully deployed on the Taobao App.\nOnline experiments demonstrate that RecGPT achieves consistent performance\ngains across stakeholders: users benefit from increased content diversity and\nsatisfaction, merchants and the platform gain greater exposure and conversions.\nThese comprehensive improvement results across all stakeholders validates that\nLLM-driven, intent-centric design can foster a more sustainable and mutually\nbeneficial recommendation ecosystem.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22879v2", "cate": "cs.IR", "date": "2025-07-30", "updated": "2025-07-31"}
{"id": "2507.23234", "title": "Secure Integrated Sensing and Communication Networks: Stochastic Performance Analysis", "authors": ["Marziyeh Soltani", "Mahtab Mirmohseni", "Rahim Tafazolli"], "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23234v1", "summary": "This paper analyzes the stochastic security performance of a multiple-input\nmultiple-output (MIMO) integrated sensing and communication (ISAC) system in a\ndownlink scenario. A base station (BS) transmits a multi-functional signal to\nsimultaneously communicate with a user, sense a target's angular location, and\ncounteract eavesdropping threats. The attack model considers a passive\nsingle-antenna communication eavesdropper intercepting communication data, as\nwell as a multi-antenna sensing eavesdropper attempting to infer the target's\nlocation. We also consider a malicious target scenario where the target plays\nthe role of the communication eavesdropper. The BS-user and BS-eavesdroppers\nchannels follow Rayleigh fading, while the target's azimuth angle is uniformly\ndistributed. To evaluate the performance in this random network, we derive the\nergodic secrecy rate (ESR) and the ergodic Cramer-Rao lower bound (CRB), for\ntarget localization, at both the BS and the sensing eavesdropper. This involves\ncomputing the probability density functions (PDFs) of the signal-to-noise ratio\n(SNR) and CRB, leveraging the central limit theorem for tractability. We\ncharacterize the boundary of the CRB-secrecy rate region, and interpret the\nperformance tradeoffs between communication and sensing while guaranteeing a\nlevel of security and privacy in the random ISAC networks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23234v1", "cate": "cs.IT", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23735", "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "authors": ["Markus Buchholz", "Ignacio Carlucho", "Michele Grimaldi", "Yvan R. Petillot"], "categories": ["cs.RO", "cs.AI", "cs.MA"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23735v1", "summary": "Achieving robust cognitive autonomy in robots navigating complex,\nunpredictable environments remains a fundamental challenge in robotics. This\npaper presents Underwater Robot Self-Organizing Autonomy (UROSA), a\ngroundbreaking architecture leveraging distributed Large Language Model AI\nagents integrated within the Robot Operating System 2 (ROS 2) framework to\nenable advanced cognitive capabilities in Autonomous Underwater Vehicles. UROSA\ndecentralises cognition into specialised AI agents responsible for multimodal\nperception, adaptive reasoning, dynamic mission planning, and real-time\ndecision-making. Central innovations include flexible agents dynamically\nadapting their roles, retrieval-augmented generation utilising vector databases\nfor efficient knowledge management, reinforcement learning-driven behavioural\noptimisation, and autonomous on-the-fly ROS 2 node generation for runtime\nfunctional extensibility. Extensive empirical validation demonstrates UROSA's\npromising adaptability and reliability through realistic underwater missions in\nsimulation and real-world deployments, showing significant advantages over\ntraditional rule-based architectures in handling unforeseen scenarios,\nenvironmental uncertainties, and novel mission objectives. This work not only\nadvances underwater autonomy but also establishes a scalable, safe, and\nversatile cognitive robotics framework capable of generalising to a diverse\narray of real-world applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23735v1", "cate": "cs.RO", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2412.11167", "title": "Cultural Palette: Pluralising Culture Alignment via Multi-agent Palette", "authors": ["Jiahao Yuan", "Zixiang Di", "Shangzixin Zhao", "Zhiqing Cui", "Hanqing Wang", "Guisong Yang", "Usman Naseem"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      20 pages, 10 figures", "url": "http://arxiv.org/abs/2412.11167v3", "summary": "Large language models (LLMs) face challenges in aligning with diverse\ncultural values despite their remarkable performance in generation, which stems\nfrom inherent monocultural biases and difficulties in capturing nuanced\ncultural semantics. Existing methods struggle to adapt to unknown culture after\nfine-tuning. Inspired by cultural geography across five continents, we propose\nCultural Palette, a multi-agent framework that redefines cultural alignment as\nan adaptive \"color-blending\" process for country-specific adaptation. Our\napproach harnesses cultural geography across five continents (Africa, America,\nAsia, Europe, Oceania) through three key steps: First, we synthesize the\nPentachromatic Cultural Palette Dataset using GPT-4o, refining\ncontinental-level dialogues with Hofstede's cultural dimensions to establish\nfoundational cultural representations. Second, five continent-level alignment\nagents form specialized cultural communities that generate region-specific\ndraft responses. Third, a Meta Agent employs Cultural MoErges to dynamically\nblend these cultural \"colors\" through attention-gated parameter merging, akin\nto mixing pigments on a palette, resolving conflicts while preserving cultural\nnuances to produce the final culturally-aligned response. Extensive experiments\nacross various countries demonstrate that Cultural Palette surpasses existing\nbaselines in cultural alignment.", "comment": "20 pages, 10 figures", "pdf_url": "http://arxiv.org/pdf/2412.11167v3", "cate": "cs.CL", "date": "2024-12-15", "updated": "2025-07-31"}
{"id": "2507.23436", "title": "Beyond Linear Bottlenecks: Spline-Based Knowledge Distillation for Culturally Diverse Art Style Classification", "authors": ["Abdellah Zakaria Sellam", "Salah Eddine Bekhouche", "Cosimo Distante", "Abdelmalik Taleb-Ahmed"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23436v1", "summary": "Art style classification remains a formidable challenge in computational\naesthetics due to the scarcity of expertly labeled datasets and the intricate,\noften nonlinear interplay of stylistic elements. While recent dual-teacher\nself-supervised frameworks reduce reliance on labeled data, their linear\nprojection layers and localized focus struggle to model global compositional\ncontext and complex style-feature interactions. We enhance the dual-teacher\nknowledge distillation framework to address these limitations by replacing\nconventional MLP projection and prediction heads with Kolmogorov-Arnold\nNetworks (KANs). Our approach retains complementary guidance from two teacher\nnetworks, one emphasizing localized texture and brushstroke patterns, the other\ncapturing broader stylistic hierarchies while leveraging KANs' spline-based\nactivations to model nonlinear feature correlations with mathematical\nprecision. Experiments on WikiArt and Pandora18k demonstrate that our approach\noutperforms the base dual teacher architecture in Top-1 accuracy. Our findings\nhighlight the importance of KANs in disentangling complex style manifolds,\nleading to better linear probe accuracy than MLP projections.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23436v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.22956", "title": "LLM-Assisted Cheating Detection in Korean Language via Keystrokes", "authors": ["Dong Hyun Roh", "Rajesh Kumar", "An Ngo"], "categories": ["cs.LG", "cs.HC", "K.3.1"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      This paper has 11 pages, 6 figures, 2 tables, and has been accepted for publication at IEEE-IJCB 2025", "url": "http://arxiv.org/abs/2507.22956v1", "summary": "This paper presents a keystroke-based framework for detecting LLM-assisted\ncheating in Korean, addressing key gaps in prior research regarding language\ncoverage, cognitive context, and the granularity of LLM involvement. Our\nproposed dataset includes 69 participants who completed writing tasks under\nthree conditions: Bona fide writing, paraphrasing ChatGPT responses, and\ntranscribing ChatGPT responses. Each task spans six cognitive processes defined\nin Bloom's Taxonomy (remember, understand, apply, analyze, evaluate, and\ncreate). We extract interpretable temporal and rhythmic features and evaluate\nmultiple classifiers under both Cognition-Aware and Cognition-Unaware settings.\nTemporal features perform well under Cognition-Aware evaluation scenarios,\nwhile rhythmic features generalize better under cross-cognition scenarios.\nMoreover, detecting bona fide and transcribed responses was easier than\nparaphrased ones for both the proposed models and human evaluators, with the\nmodels significantly outperforming the humans. Our findings affirm that\nkeystroke dynamics facilitate reliable detection of LLM-assisted writing across\nvarying cognitive demands and writing strategies, including paraphrasing and\ntranscribing LLM-generated responses.", "comment": "This paper has 11 pages, 6 figures, 2 tables, and has been accepted\n  for publication at IEEE-IJCB 2025", "pdf_url": "http://arxiv.org/pdf/2507.22956v1", "cate": "cs.LG", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2507.16725", "title": "RAVine: Reality-Aligned Evaluation for Agentic Search", "authors": ["Yilong Xu", "Xiang Long", "Zhi Zheng", "Jinhua Gao"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16725v2", "summary": "Agentic search, as a more autonomous and adaptive paradigm of retrieval\naugmentation, is driving the evolution of intelligent search systems. However,\nexisting evaluation frameworks fail to align well with the goals of agentic\nsearch. First, the complex queries commonly used in current benchmarks often\ndeviate from realistic user search scenarios. Second, prior approaches tend to\nintroduce noise when extracting ground truth for end-to-end evaluations,\nleading to distorted assessments at a fine-grained level. Third, most current\nframeworks focus solely on the quality of final answers, neglecting the\nevaluation of the iterative process inherent to agentic search. To address\nthese limitations, we propose RAVine -- a Reality-Aligned eValuation framework\nfor agentic LLMs with search. RAVine targets multi-point queries and long-form\nanswers that better reflect user intents, and introduces an attributable ground\ntruth construction strategy to enhance the accuracy of fine-grained evaluation.\nMoreover, RAVine examines model's interaction with search tools throughout the\niterative process, and accounts for factors of efficiency. We benchmark a\nseries of models using RAVine and derive several insights, which we hope will\ncontribute to advancing the development of agentic search systems. The code and\ndatasets are available at https://github.com/SwordFaith/RAVine.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16725v2", "cate": "cs.CL", "date": "2025-07-22", "updated": "2025-07-31"}
{"id": "2507.23296", "title": "Exploiting Movable Elements of Intelligent Reflecting Surface for Enhancement of Integrated Sensing and Communication", "authors": ["Xingyu Peng", "Qin Tao", "Yong Liang Guan", "Xiaoming Chen"], "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "Comments:      16 pages, 13 figures", "url": "http://arxiv.org/abs/2507.23296v1", "summary": "In this paper, we propose to exploit movable elements of intelligent\nreflecting surface (IRS) to enhance the overall performance of integrated\nsensing and communication (ISAC) systems. Firstly, focusing on a single-user\nscenario, we reveal the function of movable elements by performance analysis,\nand then design a joint beamforming and element position optimization scheme.\nFurther, we extend it to a general multi-user scenario, and also propose an\nelement position optimization scheme according to the derived performance\nexpressions. Finally, simulation results confirm that the movement of IRS\nelements can improve the communication rate and the sensing accuracy, and\nespecially broaden the coverage of ISAC.", "comment": "16 pages, 13 figures", "pdf_url": "http://arxiv.org/pdf/2507.23296v1", "cate": "cs.IT", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23771", "title": "Consensus-Driven Active Model Selection", "authors": ["Justin Kay", "Grant Van Horn", "Subhransu Maji", "Daniel Sheldon", "Sara Beery"], "categories": ["cs.LG", "cs.AI", "cs.CV"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      ICCV 2025 Highlight. 16 pages, 8 figures", "url": "http://arxiv.org/abs/2507.23771v1", "summary": "The widespread availability of off-the-shelf machine learning models poses a\nchallenge: which model, of the many available candidates, should be chosen for\na given data analysis task? This question of model selection is traditionally\nanswered by collecting and annotating a validation dataset -- a costly and\ntime-intensive process. We propose a method for active model selection, using\npredictions from candidate models to prioritize the labeling of test data\npoints that efficiently differentiate the best candidate. Our method, CODA,\nperforms consensus-driven active model selection by modeling relationships\nbetween classifiers, categories, and data points within a probabilistic\nframework. The framework uses the consensus and disagreement between models in\nthe candidate pool to guide the label acquisition process, and Bayesian\ninference to update beliefs about which model is best as more information is\ncollected. We validate our approach by curating a collection of 26 benchmark\ntasks capturing a range of model selection scenarios. CODA outperforms existing\nmethods for active model selection significantly, reducing the annotation\neffort required to discover the best model by upwards of 70% compared to the\nprevious state-of-the-art. Code and data are available at\nhttps://github.com/justinkay/coda.", "comment": "ICCV 2025 Highlight. 16 pages, 8 figures", "pdf_url": "http://arxiv.org/pdf/2507.23771v1", "cate": "cs.LG", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2503.15299", "title": "Inside-Out: Hidden Factual Knowledge in LLMs", "authors": ["Zorik Gekhman", "Eyal Ben David", "Hadas Orgad", "Eran Ofek", "Yonatan Belinkov", "Idan Szpektor", "Jonathan Herzig", "Roi Reichart"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Accepted to COLM 2025", "url": "http://arxiv.org/abs/2503.15299v3", "summary": "This work presents a framework for assessing whether large language models\n(LLMs) encode more factual knowledge in their parameters than what they express\nin their outputs. While a few studies hint at this possibility, none has\nclearly defined or demonstrated this phenomenon. We first propose a formal\ndefinition of knowledge, quantifying it for a given question as the fraction of\ncorrect-incorrect answer pairs where the correct one is ranked higher. This\ngives rise to external and internal knowledge, depending on the information\nused to score individual answer candidates: either the model's observable\ntoken-level probabilities or its intermediate computations. Hidden knowledge\narises when internal knowledge exceeds external knowledge. We then present a\ncase study, applying this framework to three popular open-weights LLMs in a\nclosed-book QA setup. Our results indicate that: (1) LLMs consistently encode\nmore factual knowledge internally than what they express externally, with an\naverage relative gap of 40%. (2) Surprisingly, some knowledge is so deeply\nhidden that a model can internally know an answer perfectly, yet fail to\ngenerate it even once, despite large-scale repeated sampling of 1,000 answers.\nThis reveals fundamental limitations in the generation capabilities of LLMs,\nwhich (3) put a practical constraint on scaling test-time compute via repeated\nanswer sampling in closed-book QA: significant performance improvements remain\ninaccessible because some answers are practically never sampled, yet if they\nwere, we would be guaranteed to rank them first.", "comment": "Accepted to COLM 2025", "pdf_url": "http://arxiv.org/pdf/2503.15299v3", "cate": "cs.CL", "date": "2025-03-19", "updated": "2025-07-31"}
{"id": "2507.23447", "title": "Adjustable Spatio-Spectral Hyperspectral Image Compression Network", "authors": ["Martin Hermann Paul Fuchs", "Behnood Rasti", "Begüm Demir"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23447v1", "summary": "With the rapid growth of hyperspectral data archives in remote sensing (RS),\nthe need for efficient storage has become essential, driving significant\nattention toward learning-based hyperspectral image (HSI) compression. However,\na comprehensive investigation of the individual and joint effects of spectral\nand spatial compression on learning-based HSI compression has not been\nthoroughly examined yet. Conducting such an analysis is crucial for\nunderstanding how the exploitation of spectral, spatial, and joint\nspatio-spectral redundancies affects HSI compression. To address this issue, we\npropose Adjustable Spatio-Spectral Hyperspectral Image Compression Network\n(HyCASS), a learning-based model designed for adjustable HSI compression in\nboth spectral and spatial dimensions. HyCASS consists of six main modules: 1)\nspectral encoder; 2) spatial encoder; 3) compression ratio (CR) adapter\nencoder; 4) CR adapter decoder; 5) spatial decoder; and 6) spectral decoder\nmodule. The modules employ convolutional layers and transformer blocks to\ncapture both short-range and long-range redundancies. Experimental results on\ntwo HSI benchmark datasets demonstrate the effectiveness of our proposed\nadjustable model compared to existing learning-based compression models. Based\non our results, we establish a guideline for effectively balancing spectral and\nspatial compression across different CRs, taking into account the spatial\nresolution of the HSIs. Our code and pre-trained model weights are publicly\navailable at https://git.tu-berlin.de/rsim/hycass .", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23447v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23544", "title": "User Experience Estimation in Human-Robot Interaction Via Multi-Instance Learning of Multimodal Social Signals", "authors": ["Ryo Miyoshi", "Yuki Okafuji", "Takuya Iwamoto", "Junya Nakanishi", "Jun Baba"], "categories": ["cs.RO", "cs.CV", "cs.HC"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      This paper has been accepted for presentation at IEEE/RSJ International Conference on Intelligent Robots and Systems 2025 (IROS 2025)", "url": "http://arxiv.org/abs/2507.23544v1", "summary": "In recent years, the demand for social robots has grown, requiring them to\nadapt their behaviors based on users' states. Accurately assessing user\nexperience (UX) in human-robot interaction (HRI) is crucial for achieving this\nadaptability. UX is a multi-faceted measure encompassing aspects such as\nsentiment and engagement, yet existing methods often focus on these\nindividually. This study proposes a UX estimation method for HRI by leveraging\nmultimodal social signals. We construct a UX dataset and develop a\nTransformer-based model that utilizes facial expressions and voice for\nestimation. Unlike conventional models that rely on momentary observations, our\napproach captures both short- and long-term interaction patterns using a\nmulti-instance learning framework. This enables the model to capture temporal\ndynamics in UX, providing a more holistic representation. Experimental results\ndemonstrate that our method outperforms third-party human evaluators in UX\nestimation.", "comment": "This paper has been accepted for presentation at IEEE/RSJ\n  International Conference on Intelligent Robots and Systems 2025 (IROS 2025)", "pdf_url": "http://arxiv.org/pdf/2507.23544v1", "cate": "cs.RO", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.21903", "title": "Who's important? -- SUnSET: Synergistic Understanding of Stakeholder, Events and Time for Timeline Generation", "authors": ["Tiviatis Sim", "Kaiwen Yang", "Shen Xin", "Kenji Kawaguchi"], "categories": ["cs.SI", "cs.CL", "cs.IR"], "primary_category": "Subjects:       Social and Information Networks (cs.SI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21903v2", "summary": "As news reporting becomes increasingly global and decentralized online,\ntracking related events across multiple sources presents significant\nchallenges. Existing news summarization methods typically utilizes Large\nLanguage Models and Graphical methods on article-based summaries. However, this\nis not effective since it only considers the textual content of similarly dated\narticles to understand the gist of the event. To counteract the lack of\nanalysis on the parties involved, it is essential to come up with a novel\nframework to gauge the importance of stakeholders and the connection of related\nevents through the relevant entities involved. Therefore, we present SUnSET:\nSynergistic Understanding of Stakeholder, Events and Time for the task of\nTimeline Summarization (TLS). We leverage powerful Large Language Models (LLMs)\nto build SET triplets and introduced the use of stakeholder-based ranking to\nconstruct a $Relevancy$ metric, which can be extended into general situations.\nOur experimental results outperform all prior baselines and emerged as the new\nState-of-the-Art, highlighting the impact of stakeholder information within\nnews article.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21903v2", "cate": "cs.SI", "date": "2025-07-29", "updated": "2025-07-31"}
{"id": "2507.23528", "title": "Hybrid Generative Semantic and Bit Communications in Satellite Networks: Trade-offs in Latency, Generation Quality, and Computation", "authors": ["Chong Huang", "Gaojie Chen", "Jing Zhu", "Qu Luo", "Pei Xiao", "Wei Huang", "Rahim Tafazolli"], "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "Comments:      6 pages, accepted for pulication in IEEE Globecom 2025", "url": "http://arxiv.org/abs/2507.23528v1", "summary": "As satellite communications play an increasingly important role in future\nwireless networks, the issue of limited link budget in satellite systems has\nattracted significant attention in current research. Although semantic\ncommunications emerge as a promising solution to address these constraints, it\nintroduces the challenge of increased computational resource consumption in\nwireless communications. To address these challenges, we propose a multi-layer\nhybrid bit and generative semantic communication framework which can adapt to\nthe dynamic satellite communication networks. Furthermore, to balance the\nsemantic communication efficiency and performance in satellite-to-ground\ntransmissions, we introduce a novel semantic communication efficiency metric\n(SEM) that evaluates the trade-offs among latency, computational consumption,\nand semantic reconstruction quality in the proposed framework. Moreover, we\nutilize a novel deep reinforcement learning (DRL) algorithm group relative\npolicy optimization (GRPO) to optimize the resource allocation in the proposed\nnetwork. Simulation results demonstrate the flexibility of our proposed\ntransmission framework and the effectiveness of the proposed metric SEM,\nillustrate the relationships among various semantic communication metrics.", "comment": "6 pages, accepted for pulication in IEEE Globecom 2025", "pdf_url": "http://arxiv.org/pdf/2507.23528v1", "cate": "cs.IT", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23779", "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "authors": ["Miaosen Zhang", "Ziqiang Xu", "Jialiang Zhu", "Qi Dai", "Kai Qiu", "Yifan Yang", "Chong Luo", "Tianyi Chen", "Justin Wagle", "Tim Franklin", "Baining Guo"], "categories": ["cs.CV", "cs.AI", "cs.MM"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23779v1", "summary": "With the development of multimodal reasoning models, Computer Use Agents\n(CUAs), akin to Jarvis from \\textit{\"Iron Man\"}, are becoming a reality. GUI\ngrounding is a core component for CUAs to execute actual actions, similar to\nmechanical control in robotics, and it directly leads to the success or failure\nof the system. It determines actions such as clicking and typing, as well as\nrelated parameters like the coordinates for clicks. Current end-to-end\ngrounding models still achieve less than 65\\% accuracy on challenging\nbenchmarks like ScreenSpot-pro and UI-Vision, indicating they are far from\nbeing ready for deployment. % , as a single misclick can result in unacceptable\nconsequences. In this work, we conduct an empirical study on the training of\ngrounding models, examining details from data collection to model training.\nUltimately, we developed the \\textbf{Phi-Ground} model family, which achieves\nstate-of-the-art performance across all five grounding benchmarks for models\nunder $10B$ parameters in agent settings. In the end-to-end model setting, our\nmodel still achieves SOTA results with scores of \\textit{\\textbf{43.2}} on\nScreenSpot-pro and \\textit{\\textbf{27.2}} on UI-Vision. We believe that the\nvarious details discussed in this paper, along with our successes and failures,\nnot only clarify the construction of grounding models but also benefit other\nperception tasks. Project homepage:\n\\href{https://zhangmiaosen2000.github.io/Phi-Ground/}{https://zhangmiaosen2000.github.io/Phi-Ground/}", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23779v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2503.15768", "title": "Can one size fit all?: Measuring Failure in Multi-Document Summarization Domain Transfer", "authors": ["Alexandra DeLucia", "Mark Dredze"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.15768v2", "summary": "Abstractive multi-document summarization (MDS) is the task of automatically\nsummarizing information in multiple documents, from news articles to\nconversations with multiple speakers. The training approaches for current MDS\nmodels can be grouped into four approaches: end-to-end with special\npre-training (\"direct\"), chunk-then-summarize, extract-then-summarize, and\ninference with GPT-style models. In this work, we evaluate MDS models across\ntraining approaches, domains, and dimensions (reference similarity, quality,\nand factuality), to analyze how and why models trained on one domain can fail\nto summarize documents from another (News, Science, and Conversation) in the\nzero-shot domain transfer setting. We define domain-transfer \"failure\" as a\ndecrease in factuality, higher deviation from the target, and a general\ndecrease in summary quality. In addition to exploring domain transfer for MDS\nmodels, we examine potential issues with applying popular summarization metrics\nout-of-the-box.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.15768v2", "cate": "cs.CL", "date": "2025-03-20", "updated": "2025-07-30"}
{"id": "2507.23473", "title": "CST Anti-UAV: A Thermal Infrared Benchmark for Tiny UAV Tracking in Complex Scenes", "authors": ["Bin Xie", "Congxuan Zhang", "Fagan Wang", "Peng Liu", "Feng Lu", "Zhen Chen", "Weiming Hu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ICCVW2025", "url": "http://arxiv.org/abs/2507.23473v1", "summary": "The widespread application of Unmanned Aerial Vehicles (UAVs) has raised\nserious public safety and privacy concerns, making UAV perception crucial for\nanti-UAV tasks. However, existing UAV tracking datasets predominantly feature\nconspicuous objects and lack diversity in scene complexity and attribute\nrepresentation, limiting their applicability to real-world scenarios. To\novercome these limitations, we present the CST Anti-UAV, a new thermal infrared\ndataset specifically designed for Single Object Tracking (SOT) in Complex\nScenes with Tiny UAVs (CST). It contains 220 video sequences with over 240k\nhigh-quality bounding box annotations, highlighting two key properties: a\nsignificant number of tiny-sized UAV targets and the diverse and complex\nscenes. To the best of our knowledge, CST Anti-UAV is the first dataset to\nincorporate complete manual frame-level attribute annotations, enabling precise\nevaluations under varied challenges. To conduct an in-depth performance\nanalysis for CST Anti-UAV, we evaluate 20 existing SOT methods on the proposed\ndataset. Experimental results demonstrate that tracking tiny UAVs in complex\nenvironments remains a challenge, as the state-of-the-art method achieves only\n35.92% state accuracy, much lower than the 67.69% observed on the Anti-UAV410\ndataset. These findings underscore the limitations of existing benchmarks and\nthe need for further advancements in UAV tracking research. The CST Anti-UAV\nbenchmark is about to be publicly released, which not only fosters the\ndevelopment of more robust SOT methods but also drives innovation in anti-UAV\nsystems.", "comment": "Accepted by ICCVW2025", "pdf_url": "http://arxiv.org/pdf/2507.23473v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23592", "title": "Human-Exoskeleton Kinematic Calibration to Improve Hand Tracking for Dexterous Teleoperation", "authors": ["Haiyun Zhang", "Stefano Dalla Gasperina", "Saad N. Yousaf", "Toshimitsu Tsuboi", "Tetsuya Narita", "Ashish D. Deshpande"], "categories": ["cs.RO", "cs.HC", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      8 pages, 10 figures, submitted to RA-L", "url": "http://arxiv.org/abs/2507.23592v1", "summary": "Hand exoskeletons are critical tools for dexterous teleoperation and\nimmersive manipulation interfaces, but achieving accurate hand tracking remains\na challenge due to user-specific anatomical variability and donning\ninconsistencies. These issues lead to kinematic misalignments that degrade\ntracking performance and limit applicability in precision tasks. We propose a\nsubject-specific calibration framework for exoskeleton-based hand tracking that\nuses redundant joint sensing and a residual-weighted optimization strategy to\nestimate virtual link parameters. Implemented on the Maestro exoskeleton, our\nmethod improves joint angle and fingertip position estimation across users with\nvarying hand geometries. We introduce a data-driven approach to empirically\ntune cost function weights using motion capture ground truth, enabling more\naccurate and consistent calibration across participants. Quantitative results\nfrom seven subjects show substantial reductions in joint and fingertip tracking\nerrors compared to uncalibrated and evenly weighted models. Qualitative\nvisualizations using a Unity-based virtual hand further confirm improvements in\nmotion fidelity. The proposed framework generalizes across exoskeleton designs\nwith closed-loop kinematics and minimal sensing, and lays the foundation for\nhigh-fidelity teleoperation and learning-from-demonstration applications.", "comment": "8 pages, 10 figures, submitted to RA-L", "pdf_url": "http://arxiv.org/pdf/2507.23592v1", "cate": "cs.RO", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23686", "title": "From Link Diversity to Cross-Band Feedback Collaboration: A New Perspective on Hybrid Optical-RF Systems", "authors": ["Menghan Li", "Yulin Shao", "Runxin Zhang", "Lu Lu"], "categories": ["cs.IT", "cs.SY", "eess.SY", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23686v1", "summary": "We suggest a re-examination of the conventional view that hybrid\noptical-radio frequency (O-RF) systems are primarily diversity-driven networks\nthat switch between RF and optical links for robustness. Instead, we uncover a\nnew architectural opportunity: repurposing the optical downlink to enable\nreal-time feedback channel coding over the RF uplink, where structured decoder\nfeedback is delivered from the access point to guide the transmitter's coding\nstrategy. This insight marks a conceptual paradigm shift from passive link\ndiversity to active cross-band collaboration, where the wideband,\ninterference-free optical wireless communication (OWC) is no longer merely a\ndownlink backup but a functional enabler of uplink reliability. To realize this\nvision, we propose a novel architecture, O-RF with Cross-Band Feedback\n(O-RF-CBF), that exploits the optical downlink feedback to facilitate adaptive\nRF uplink coding. Numerical results reveal that O-RF-CBF achieves significant\nuplink throughput gains over traditional O-RF systems. Our findings highlight\nthat inter-band synergy, not redundancy, is the key to unlocking the full\npotential of hybrid wireless networks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23686v1", "cate": "cs.IT", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23603", "title": "Explanations for Unrealizability of Infinite-State Safety Shields", "authors": ["Andoni Rodriguez", "Irfansha Shaik", "Davide Corsi", "Roy Fox", "Cesar Sanchez"], "categories": ["cs.LO"], "primary_category": "Subjects:       Logic in Computer Science (cs.LO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23603v1", "summary": "Safe Reinforcement Learning focuses on developing optimal policies while\nensuring safety. A popular method to address such task is shielding, in which a\ncorrect-by-construction safety component is synthesized from logical\nspecifications. Recently, shield synthesis has been extended to infinite-state\ndomains, such as continuous environments. This makes shielding more applicable\nto realistic scenarios. However, often shields might be unrealizable because\nthe specification is inconsistent (e.g., contradictory). In order to address\nthis gap, we present a method to obtain simple unconditional and conditional\nexplanations that witness unrealizability, which goes by temporal formula\nunrolling. In this paper, we show different variants of the technique and its\napplicability.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23603v1", "cate": "cs.LO", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23784", "title": "SUB: Benchmarking CBM Generalization via Synthetic Attribute Substitutions", "authors": ["Jessica Bader", "Leander Girrbach", "Stephan Alaniz", "Zeynep Akata"], "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted at ICCV 2025", "url": "http://arxiv.org/abs/2507.23784v1", "summary": "Concept Bottleneck Models (CBMs) and other concept-based interpretable models\nshow great promise for making AI applications more transparent, which is\nessential in fields like medicine. Despite their success, we demonstrate that\nCBMs struggle to reliably identify the correct concepts under distribution\nshifts. To assess the robustness of CBMs to concept variations, we introduce\nSUB: a fine-grained image and concept benchmark containing 38,400 synthetic\nimages based on the CUB dataset. To create SUB, we select a CUB subset of 33\nbird classes and 45 concepts to generate images which substitute a specific\nconcept, such as wing color or belly pattern. We introduce a novel Tied\nDiffusion Guidance (TDG) method to precisely control generated images, where\nnoise sharing for two parallel denoising processes ensures that both the\ncorrect bird class and the correct attribute are generated. This novel\nbenchmark enables rigorous evaluation of CBMs and similar interpretable models,\ncontributing to the development of more robust methods. Our code is available\nat https://github.com/ExplainableML/sub and the dataset at\nhttp://huggingface.co/datasets/Jessica-bader/SUB.", "comment": "Accepted at ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.23784v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2504.04640", "title": "Splits! A Flexible Dataset and Evaluation Framework for Sociocultural Linguistic Investigation", "authors": ["Eylon Caplan", "Tania Chakraborty", "Dan Goldwasser"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Preprint, under review", "url": "http://arxiv.org/abs/2504.04640v2", "summary": "Variation in language use, shaped by speakers' sociocultural background and\nspecific context of use, offers a rich lens into cultural perspectives, values,\nand opinions. However, the computational study of these Sociocultural\nLinguistic Phenomena (SLP) has often been limited to bespoke analyses of\nspecific groups or topics, hindering the pace of scientific discovery. To\naddress this, we introduce Splits!, a 9.7 million-post dataset from Reddit\ndesigned for systematic and flexible research. The dataset contains posts from\nover 53,000 users across 6 demographic groups, organized into 89 discussion\ntopics to enable comparative analysis. We validate Splits! via\nself-identification and by successfully replicating several known SLPs from\nexisting literature. We complement this dataset with a framework that leverages\nefficient retrieval methods to rapidly validate potential SLPs (PSLPs) by\nautomatically evaluating whether a given hypothesis is supported by our data.\nCrucially, to distinguish between novel and obvious insights, the framework\nincorporates a human-validated measure of a hypothesis's ``unexpectedness.'' We\ndemonstrate that the two-stage process reduces the number of statistically\nsignificant findings requiring manual inspection by a factor of 1.5-1.8x,\nstreamlining the discovery of promising phenomena for further investigation.", "comment": "Preprint, under review", "pdf_url": "http://arxiv.org/pdf/2504.04640v2", "cate": "cs.CL", "date": "2025-04-06", "updated": "2025-07-31"}
{"id": "2507.23478", "title": "3D-R1: Enhancing Reasoning in 3D VLMs for Unified Scene Understanding", "authors": ["Ting Huang", "Zeyu Zhang", "Hao Tang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23478v1", "summary": "Large vision-language models (VLMs) have made significant strides in 2D\nvisual understanding tasks, sparking interest in extending these capabilities\nto 3D scene understanding. However, current 3D VLMs often struggle with robust\nreasoning and generalization due to limitations in high-quality spatial data\nand the static nature of viewpoint assumptions. To address these challenges, we\npropose 3D-R1, a foundation model that enhances the reasoning capabilities of\n3D VLMs. Specifically, we first construct a high-quality synthetic dataset with\nCoT, named Scene-30K, leveraging existing 3D-VL datasets and a data engine\nbased on Gemini 2.5 Pro. It serves as cold-start initialization data for 3D-R1.\nMoreover, we leverage RLHF policy such as GRPO in the reinforcement learning\ntraining process to enhance reasoning capabilities and introduce three reward\nfunctions: a perception reward, a semantic similarity reward and a format\nreward to maintain detection accuracy and answer semantic precision.\nFurthermore, we introduce a dynamic view selection strategy that adaptively\nchooses the most informative perspectives for 3D scene understanding. Extensive\nexperiments demonstrate that 3D-R1 delivers an average improvement of 10%\nacross various 3D scene benchmarks, highlighting its effectiveness in enhancing\nreasoning and generalization in 3D scene understanding. Code:\nhttps://github.com/AIGeeksGroup/3D-R1. Website:\nhttps://aigeeksgroup.github.io/3D-R1.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23478v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23756", "title": "Improving annotator selection in Active Learning using a mood and fatigue-aware Recommender System", "authors": ["Diana Mortagua"], "categories": ["cs.LG", "cs.HC"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23756v1", "summary": "This study centers on overcoming the challenge of selecting the best\nannotators for each query in Active Learning (AL), with the objective of\nminimizing misclassifications. AL recognizes the challenges related to cost and\ntime when acquiring labeled data, and decreases the number of labeled data\nneeded. Nevertheless, there is still the necessity to reduce annotation errors,\naiming to be as efficient as possible, to achieve the expected accuracy faster.\nMost strategies for query-annotator pairs do not consider internal factors that\naffect productivity, such as mood, attention, motivation, and fatigue levels.\nThis work addresses this gap in the existing literature, by not only\nconsidering how the internal factors influence annotators (mood and fatigue\nlevels) but also presenting a new query-annotator pair strategy, using a\nKnowledge-Based Recommendation System (RS). The RS ranks the available\nannotators, allowing to choose one or more to label the queried instance using\ntheir past accuracy values, and their mood and fatigue levels, as well as\ninformation about the instance queried. This work bases itself on existing\nliterature on mood and fatigue influence on human performance, simulating\nannotators in a realistic manner, and predicting their performance with the RS.\nThe results show that considering past accuracy values, as well as mood and\nfatigue levels reduces the number of annotation errors made by the annotators,\nand the uncertainty of the model through its training, when compared to not\nusing internal factors. Accuracy and F1-score values were also better in the\nproposed approach, despite not being as substantial as the aforementioned. The\nmethodologies and findings presented in this study begin to explore the open\nchallenge of human cognitive factors affecting AL.", "comment": "Master's thesis", "pdf_url": "http://arxiv.org/pdf/2507.23756v1", "cate": "cs.LG", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23702", "title": "Cell-Free Massive MIMO SWIPT with Beyond Diagonal Reconfigurable Intelligent Surfaces", "authors": ["Duc Thien Hua", "Mohammadali Mohammadi", "Hien Quoc Ngo", "Michail Matthaiou"], "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23702v1", "summary": "We investigate the integration of beyond diagonal reconfigurable intelligent\nsurfaces (BDRISs) into cell free massive multiple input multiple output\n(CFmMIMO) systems to enhance simultaneous wireless information and power\ntransfer (SWIPT). To simultaneously support two groups of users energy\nreceivers (ERs) and information receivers (IRs) without sacrificing time\nfrequency resources, a subset of access points (APs) is dedicated to serving\nERs with the aid of a BDRIS, while the remaining APs focus on supporting IRs. A\nprotective partial zero forcing precoding technique is implemented at the APs\nto manage the non coherent interference between the ERs and IRs. Subsequently,\nclosed form expressions for the spectral efficiency of the IRs and the average\nsum of harvested energy at the ERs are leveraged to formulate a comprehensive\noptimization problem. This problem jointly optimizes the AP selection, AP power\ncontrol, and scattering matrix design at the BDRIS, all based on long term\nstatistical channel state information. This challenging problem is then\neffectively transformed into more tractable forms. To solve these sub problems,\nefficient algorithms are proposed, including a heuristic search for the\nscattering matrix design, as well as successive convex approximation and deep\nreinforcement learning methods for the joint AP mode selection and power\ncontrol design. Numerical results show that a BDRIS with a group or fully\nconnected architecture achieves significant energy harvesting gains over the\nconventional diagonal RIS, especially delivering up to a seven fold increase in\nthe average sum of harvested energy when a heuristic based scattering matrix\ndesign is employed.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23702v1", "cate": "cs.IT", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23151", "title": "Abstractions of Sequences, Functions and Operators", "authors": ["Louis Rustenholz", "Pedro Lopez-Garcia", "Manuel V. Hermenegildo"], "categories": ["cs.PL", "cs.LO"], "primary_category": "Subjects:       Programming Languages (cs.PL)", "pdf_link": null, "comments": "Comments:      Under consideration for publication in STTT", "url": "http://arxiv.org/abs/2507.23151v1", "summary": "We present theoretical and practical results on the order theory of lattices\nof functions, focusing on Galois connections that abstract (sets of) functions\n- a topic known as higher-order abstract interpretation.\n  We are motivated by the challenge of inferring closed-form bounds on\nfunctions which are defined recursively, i.e. as the fixed point of an operator\nor, equivalently, as the solution to a functional equation. This has multiple\napplications in program analysis (e.g. cost analysis, loop acceleration,\ndeclarative language analysis) and in hybrid systems governed by differential\nequations.\n  Our main contribution is a new family of constraint-based abstract domains\nfor abstracting numerical functions, B-bound domains, which abstract a function\nf by a conjunction of bounds from a preselected set of boundary functions. They\nallow inferring highly non-linear numerical invariants, which classical\nnumerical abstract domains struggle with. We uncover a convexity property in\nthe constraint space that simplifies, and, in some cases, fully automates,\ntransfer function design.\n  We also introduce domain abstraction, a functor that lifts arbitrary mappings\nin value space to Galois connections in function space. This supports\nabstraction from symbolic to numerical functions (i.e. size abstraction), and\nenables dimensionality reduction of equations.\n  We base our constructions of transfer functions on a simple operator\nlanguage, starting with sequences, and extending to more general functions,\nincluding multivariate, piecewise, and non-discrete domains.", "comment": "Under consideration for publication in STTT", "pdf_url": "http://arxiv.org/pdf/2507.23151v1", "cate": "cs.PL", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2402.11461", "title": "FGeo-HyperGNet: Geometric Problem Solving Integrating FormalGeo Symbolic System and Hypergraph Neural Network", "authors": ["Xiaokai Zhang", "Yang Li", "Na Zhu", "Cheng Qin", "Zhenbing Zeng", "Tuo Leng"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      Accepted by IJCAI 2025", "url": "http://arxiv.org/abs/2402.11461v3", "summary": "Geometric problem solving has always been a long-standing challenge in the\nfields of mathematical reasoning and artificial intelligence. We built a\nneural-symbolic system, called FGeo-HyperGNet, to automatically perform\nhuman-like geometric problem solving. The symbolic component is a formal system\nbuilt on FormalGeo, which can automatically perform geometric relational\nreasoning and algebraic calculations and organize the solution into a\nhypergraph with conditions as hypernodes and theorems as hyperedges. The neural\ncomponent, called HyperGNet, is a hypergraph neural network based on the\nattention mechanism, including an encoder to encode the structural and semantic\ninformation of the hypergraph and a theorem predictor to provide guidance in\nsolving problems. The neural component predicts theorems according to the\nhypergraph, and the symbolic component applies theorems and updates the\nhypergraph, thus forming a predict-apply cycle to ultimately achieve readable\nand traceable automatic solving of geometric problems. Experiments demonstrate\nthe effectiveness of this neural-symbolic architecture. We achieved\nstate-of-the-art results with a TPA of 93.50% and a PSSR of 88.36% on the\nFormalGeo7K dataset. The code is available at\nhttps://github.com/BitSecret/HyperGNet.", "comment": "Accepted by IJCAI 2025", "pdf_url": "http://arxiv.org/pdf/2402.11461v3", "cate": "cs.AI", "date": "2024-02-18", "updated": "2025-07-31"}
{"id": "2504.09753", "title": "Improving Multilingual Capabilities with Cultural and Local Knowledge in Large Language Models While Enhancing Native Performance", "authors": ["Ram Mohan Rao Kadiyala", "Siddartha Pullakhandam", "Siddhant Gupta", "Drishti Sharma", "Jebish Purbey", "Kanwal Mehreen", "Muhammad Arham", "Suman Debnath", "Hamza Farooq"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      24 pages, 18 figures", "url": "http://arxiv.org/abs/2504.09753v3", "summary": "Large Language Models (LLMs) have shown remarkable capabilities, but their\ndevelopment has primarily focused on English and other high-resource languages,\nleaving many languages underserved. We present our latest Hindi-English\nbi-lingual LLM \\textbf{Mantra-14B} with ~3\\% average improvement in benchmark\nscores over both languages, outperforming models twice its size. Using a\ncurated dataset composed of English and Hindi instruction data of 485K samples,\nwe instruction tuned models such as Qwen-2.5-14B-Instruct and Phi-4 to improve\nperformance over both English and Hindi. Our experiments encompassing seven\ndifferent LLMs of varying parameter sizes and over 140 training attempts with\nvarying English-Hindi training data ratios demonstrated that it is possible to\nsignificantly improve multilingual performance without compromising native\nperformance. Further, our approach avoids resource-intensive techniques like\nvocabulary expansion or architectural modifications, thus keeping the model\nsize small. Our results indicate that modest fine-tuning with culturally and\nlocally informed data can bridge performance gaps without incurring significant\ncomputational overhead. We release our training code, datasets, and models\nunder mit and apache licenses to aid further research towards under-represented\nand low-resource languages.", "comment": "24 pages, 18 figures", "pdf_url": "http://arxiv.org/pdf/2504.09753v3", "cate": "cs.CL", "date": "2025-04-13", "updated": "2025-07-31"}
{"id": "2507.23479", "title": "Seeing More with Less: Video Capsule Endoscopy with Multi-Task Learning", "authors": ["Julia Werner", "Oliver Bause", "Julius Oexle", "Maxime Le Floch", "Franz Brinkmann", "Jochen Hampe", "Oliver Bringmann"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted at Applications of Medical AI (AMAI workshop) at MICCAI 2025 (submitted version)", "url": "http://arxiv.org/abs/2507.23479v1", "summary": "Video capsule endoscopy has become increasingly important for investigating\nthe small intestine within the gastrointestinal tract. However, a persistent\nchallenge remains the short battery lifetime of such compact sensor edge\ndevices. Integrating artificial intelligence can help overcome this limitation\nby enabling intelligent real-time decision- making, thereby reducing the energy\nconsumption and prolonging the battery life. However, this remains challenging\ndue to data sparsity and the limited resources of the device restricting the\noverall model size. In this work, we introduce a multi-task neural network that\ncombines the functionalities of precise self-localization within the\ngastrointestinal tract with the ability to detect anomalies in the small\nintestine within a single model. Throughout the development process, we\nconsistently restricted the total number of parameters to ensure the\nfeasibility to deploy such model in a small capsule. We report the first\nmulti-task results using the recently published Galar dataset, integrating\nestablished multi-task methods and Viterbi decoding for subsequent time-series\nanalysis. This outperforms current single-task models and represents a\nsignificant ad- vance in AI-based approaches in this field. Our model achieves\nan accu- racy of 93.63% on the localization task and an accuracy of 87.48% on\nthe anomaly detection task. The approach requires only 1 million parameters\nwhile surpassing the current baselines.", "comment": "Accepted at Applications of Medical AI (AMAI workshop) at MICCAI 2025\n  (submitted version)", "pdf_url": "http://arxiv.org/pdf/2507.23479v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2309.12365", "title": "An Efficient Intelligent Semi-Automated Warehouse Inventory Stocktaking System", "authors": ["Chunan Tong"], "categories": ["cs.HC", "cs.AI", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2309.12365v3", "summary": "In the context of evolving supply chain management, the significance of\nefficient inventory management has grown substantially for businesses. However,\nconventional manual and experience-based approaches often struggle to meet the\ncomplexities of modern market demands. This research introduces an intelligent\ninventory management system to address challenges related to inaccurate data,\ndelayed monitoring, and overreliance on subjective experience in forecasting.\nThe proposed system integrates bar code and distributed flutter application\ntechnologies for intelligent perception, alongside comprehensive big data\nanalytics to enable data-driven decision-making. Through meticulous analysis,\nsystem design, critical technology exploration, and simulation validation, the\neffectiveness of the proposed system is successfully demonstrated. The\nintelligent system facilitates second-level monitoring, high-frequency checks,\nand artificial intelligence-driven forecasting, consequently enhancing the\nautomation, precision, and intelligence of inventory management. This system\ncontributes to cost reduction and optimized inventory sizes through accurate\npredictions and informed decisions, ultimately achieving a mutually beneficial\nscenario. The outcomes of this research offer", "comment": null, "pdf_url": "http://arxiv.org/pdf/2309.12365v3", "cate": "cs.HC", "date": "2023-09-13", "updated": "2025-07-31"}
{"id": "2507.22992", "title": "Improved Simulation of Asynchronous Entanglement Distribution in Noisy Quantum Networks", "authors": ["Emma Hughes", "William Munizzi", "Prineha Narang"], "categories": ["quant-ph", "cs.IT", "math.IT", "physics.optics"], "primary_category": "Subjects:       Quantum Physics (quant-ph)", "pdf_link": null, "comments": "Comments:      26 pages, 2 figures, 1 computational package", "url": "http://arxiv.org/abs/2507.22992v1", "summary": "This work introduces a lightweight simulation framework for evaluating\nasynchronous entanglement distribution protocols under realistic error models.\nWe focus on two contemporary protocols: sequential, where entanglement is\nestablished one node at a time, and parallel, where all nodes attempt to\ngenerate entanglement simultaneously. We evaluate the performance of each\nprotocol using two key metrics: the fidelity of distributed entangled states,\nand the hashing rate, a measure of entanglement efficiency. These metrics are\ncompared between both protocols across a range of network sizes and noise\nparameters. We demonstrate that the parallel protocol consistently outperforms\nthe sequential, particularly in the hashing rate metric due to reduced runtime,\nsuggesting that parallel protocols are a strong candidate for a realizable\nquantum Internet. Our framework offers an accessible and scalable tool for\nevaluating entanglement distribution strategies, by reducing the simulation of\ncomplex quantum processes to simple memory time calculations.", "comment": "26 pages, 2 figures, 1 computational package", "pdf_url": "http://arxiv.org/pdf/2507.22992v1", "cate": "quant-ph", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2411.06409", "title": "Automated Strategy Invention for Confluence of Term Rewrite Systems", "authors": ["Liao Zhang", "Fabian Mitterwallner", "Jan Jakubuv", "Cezary Kaliszyk"], "categories": ["cs.LO", "cs.AI", "F.4.2; I.2.8"], "primary_category": "Subjects:       Logic in Computer Science (cs.LO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2411.06409v2", "summary": "Term rewriting plays a crucial role in software verification and compiler\noptimization. With dozens of highly parameterizable techniques developed to\nprove various system properties, automatic term rewriting tools work in an\nextensive parameter space. This complexity exceeds human capacity for parameter\nselection, motivating an investigation into automated strategy invention. In\nthis paper, we focus on confluence, an important property of term rewrite\nsystems, and apply machine learning to develop the first learning-guided\nautomatic confluence prover. Moreover, we randomly generate a large dataset to\nanalyze confluence for term rewrite systems. Our results focus on improving the\nstate-of-the-art automatic confluence prover CSI: When equipped with our\ninvented strategies, it surpasses its human-designed strategies both on the\naugmented dataset and on the original human-created benchmark dataset Cops,\nproving/disproving the confluence of several term rewrite systems for which no\nautomated proofs were known before.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2411.06409v2", "cate": "cs.LO", "date": "2024-11-10", "updated": "2025-07-31"}
{"id": "2412.02508", "title": "When Words Smile: Generating Diverse Emotional Facial Expressions from Text", "authors": ["Haidong Xu", "Meishan Zhang", "Hao Ju", "Zhedong Zheng", "Erik Cambria", "Min Zhang", "Hao Fei"], "categories": ["cs.AI", "cs.CV"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      19 pages. Resources: this https URL", "url": "http://arxiv.org/abs/2412.02508v3", "summary": "Enabling digital humans to express rich emotions has significant applications\nin dialogue systems, gaming, and other interactive scenarios. While recent\nadvances in talking head synthesis have achieved impressive results in lip\nsynchronization, they tend to overlook the rich and dynamic nature of facial\nexpressions. To fill this critical gap, we introduce an end-to-end\ntext-to-expression model that explicitly focuses on emotional dynamics. Our\nmodel learns expressive facial variations in a continuous latent space and\ngenerates expressions that are diverse, fluid, and emotionally coherent. To\nsupport this task, we introduce EmoAva, a large-scale and high-quality dataset\ncontaining 15,000 text-3D expression pairs. Extensive experiments on both\nexisting datasets and EmoAva demonstrate that our method significantly\noutperforms baselines across multiple evaluation metrics, marking a significant\nadvancement in the field.", "comment": "19 pages. Resources: https://github.com/WalkerMitty/EmoAva", "pdf_url": "http://arxiv.org/pdf/2412.02508v3", "cate": "cs.AI", "date": "2024-12-03", "updated": "2025-07-31"}
{"id": "2504.11952", "title": "Robust and Fine-Grained Detection of AI Generated Texts", "authors": ["Ram Mohan Rao Kadiyala", "Siddartha Pullakhandam", "Kanwal Mehreen", "Drishti Sharma", "Siddhant Gupta", "Jebish Purbey", "Ashay Srivastava", "Subhasya TippaReddy", "Arvind Reddy Bobbili", "Suraj Telugara Chandrashekhar", "Modabbir Adeeb", "Srinadh Vura", "Suman Debnath", "Hamza Farooq"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      18 pages, 6 figures", "url": "http://arxiv.org/abs/2504.11952v3", "summary": "An ideal detection system for machine generated content is supposed to work\nwell on any generator as many more advanced LLMs come into existence day by\nday. Existing systems often struggle with accurately identifying AI-generated\ncontent over shorter texts. Further, not all texts might be entirely authored\nby a human or LLM, hence we focused more over partial cases i.e human-LLM\nco-authored texts. Our paper introduces a set of models built for the task of\ntoken classification which are trained on an extensive collection of\nhuman-machine co-authored texts, which performed well over texts of unseen\ndomains, unseen generators, texts by non-native speakers and those with\nadversarial inputs. We also introduce a new dataset of over 2.4M such texts\nmostly co-authored by several popular proprietary LLMs over 23 languages. We\nalso present findings of our models' performance over each texts of each domain\nand generator. Additional findings include comparison of performance against\neach adversarial method, length of input texts and characteristics of generated\ntexts compared to the original human authored texts.", "comment": "18 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2504.11952v3", "cate": "cs.CL", "date": "2025-04-16", "updated": "2025-07-31"}
{"id": "2507.23480", "title": "FastPoint: Accelerating 3D Point Cloud Model Inference via Sample Point Distance Prediction", "authors": ["Donghyun Lee", "Dawoon Jeong", "Jae W. Lee", "Hongil Yoon"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted to ICCV 2025", "url": "http://arxiv.org/abs/2507.23480v1", "summary": "Deep neural networks have revolutionized 3D point cloud processing, yet\nefficiently handling large and irregular point clouds remains challenging. To\ntackle this problem, we introduce FastPoint, a novel software-based\nacceleration technique that leverages the predictable distance trend between\nsampled points during farthest point sampling. By predicting the distance\ncurve, we can efficiently identify subsequent sample points without\nexhaustively computing all pairwise distances. Our proposal substantially\naccelerates farthest point sampling and neighbor search operations while\npreserving sampling quality and model performance. By integrating FastPoint\ninto state-of-the-art 3D point cloud models, we achieve 2.55x end-to-end\nspeedup on NVIDIA RTX 3090 GPU without sacrificing accuracy.", "comment": "Accepted to ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.23480v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2408.09186", "title": "EEG-SCMM: Soft Contrastive Masked Modeling for Cross-Corpus EEG-Based Emotion Recognition", "authors": ["Qile Liu", "Weishan Ye", "Lingli Zhang", "Zhen Liang"], "categories": ["cs.HC", "cs.AI"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      18 pages, 10 figures, 14 tables. Accepted in ACMMM 2025", "url": "http://arxiv.org/abs/2408.09186v2", "summary": "Emotion recognition using electroencephalography (EEG) signals has attracted\nincreasing attention in recent years. However, existing methods often lack\ngeneralization in cross-corpus settings, where a model trained on one dataset\nis directly applied to another without retraining, due to differences in data\ndistribution and recording conditions. To tackle the challenge of cross-corpus\nEEG-based emotion recognition, we propose a novel framework termed Soft\nContrastive Masked Modeling (SCMM). Grounded in the theory of emotional\ncontinuity, SCMM integrates soft contrastive learning with a hybrid masking\nstrategy to effectively capture emotion dynamics (refer to short-term\ncontinuity). Specifically, in the self-supervised learning stage, we propose a\nsoft weighting mechanism that assigns similarity scores to sample pairs,\nenabling fine-grained modeling of emotional transitions and capturing the\ntemporal continuity of human emotions. To further enhance representation\nlearning, we design a similarity-aware aggregator that fuses complementary\ninformation from semantically related samples based on pairwise similarities,\nthereby improving feature expressiveness and reconstruction quality. This dual\ndesign contributes to a more discriminative and transferable representation,\nwhich is crucial for robust cross-corpus generalization. Extensive experiments\non the SEED, SEED-IV, and DEAP datasets show that SCMM achieves\nstate-of-the-art (SOTA) performance, outperforming the second-best method by an\naverage accuracy of 4.26% under both same-class and different-class\ncross-corpus settings. The source code is available at\nhttps://github.com/Kyler-RL/SCMM.", "comment": "18 pages, 10 figures, 14 tables. Accepted in ACMMM 2025", "pdf_url": "http://arxiv.org/pdf/2408.09186v2", "cate": "cs.HC", "date": "2024-08-17", "updated": "2025-07-31"}
{"id": "2507.23179", "title": "Cyclotomy, cyclotomic cosets and arithmetic properties of some families in $\\frac{\\mathbb{F}_l[x]}{\\langle x^{p^sq^t}-1\\rangle}$", "authors": ["Juncheng Zhou", "Hongfeng Wu"], "categories": ["math.NT", "cs.IT", "math.IT"], "primary_category": "Subjects:       Number Theory (math.NT)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23179v2", "summary": "Arithmetic properties of some families in $\\frac{\\mathbb{F}_l[x]}{\\langle\nx^{p^sq^t}-1\\rangle}$ are obtained by using the cyclotomic classes of order 2\nwith respect to $n=p^sq^t$, where $p\\equiv3 \\mathrm{mod} 4$,\n$\\gcd(\\phi(p^s),\\phi(q^t))=2$, $l$ is a primitive root modulo $q^t$ and\n$\\mathrm{ord}_{p^s}(l)=\\phi(p^s)/2$. The form of these cyclotomic classes\nenables us to further generalize the results obtained in \\cite{ref1}. The\nexplicit expressions of primitive idempotents of minimal ideals in\n$\\frac{\\mathbb{F}_l[x]}{\\langle x^{p^sq^t}-1\\rangle}$ are also obtained.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23179v2", "cate": "math.NT", "date": "2025-07-31", "updated": "2025-08-01"}
{"id": "2507.18718", "title": "Who Wins the Multi-Structural Game?", "authors": ["Ronald Fagin", "Neil Immerman", "Phokion Kolaitis", "Jonathan Lenchner", "Rik Sengupta"], "categories": ["cs.LO", "F.4.1"], "primary_category": "Subjects:       Logic in Computer Science (cs.LO)", "pdf_link": null, "comments": "Comments:      27 pages, 7 figures", "url": "http://arxiv.org/abs/2507.18718v2", "summary": "Combinatorial games played between two players, called Spoiler and\nDuplicator, have often been used to capture syntactic properties of formal\nlogical languages. For instance, the widely used Ehrenfeucht-Fra\\\"iss\\'e (EF)\ngame captures the syntactic measure of quantifier rank of first-order formulas.\nFor every such game, there is an associated natural decision problem: \"given an\ninstance of the game, does Spoiler win the game on that instance?\" For EF\ngames, this problem was shown to be PSPACE-complete by Pezzoli in 1998. In this\npresent paper, we show that the same problem for the *multi-structural* (MS)\ngames of recent interest is PSPACE-hard, but contained in NEXPTIME. In the\nprocess, we also resolve an open problem posed by Pezzoli about the dependence\nof the hardness results for EF games on the arity of the schema under\nconsideration. Our techniques combine adaptations of Pezzoli's constructions\ntogether with insights from the theory of inapproximability of optimization\nproblems, as well as the recently developed technique of parallel play for MS\ngames.", "comment": "27 pages, 7 figures", "pdf_url": "http://arxiv.org/pdf/2507.18718v2", "cate": "cs.LO", "date": "2025-07-24", "updated": "2025-07-31"}
{"id": "2507.23080", "title": "Causal-Inspired Multi-Agent Decision-Making via Graph Reinforcement Learning", "authors": ["Jing Wang", "Yan Jin", "Fei Ding", "Chongfeng Wei"], "categories": ["cs.MA"], "primary_category": "Subjects:       Multiagent Systems (cs.MA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23080v1", "summary": "Since the advent of autonomous driving technology, it has experienced\nremarkable progress over the last decade. However, most existing research still\nstruggles to address the challenges posed by environments where multiple\nvehicles have to interact seamlessly. This study aims to integrate causal\nlearning with reinforcement learning-based methods by leveraging causal\ndisentanglement representation learning (CDRL) to identify and extract causal\nfeatures that influence optimal decision-making in autonomous vehicles. These\nfeatures are then incorporated into graph neural network-based reinforcement\nlearning algorithms to enhance decision-making in complex traffic scenarios. By\nusing causal features as inputs, the proposed approach enables the optimization\nof vehicle behavior at an unsignalized intersection. Experimental results\ndemonstrate that our proposed method achieves the highest average reward during\ntraining and our approach significantly outperforms other learning-based\nmethods in several key metrics such as collision rate and average cumulative\nreward during testing. This study provides a promising direction for advancing\nmulti-agent autonomous driving systems and make autonomous vehicles' navigation\nsafer and more efficient in complex traffic environments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23080v1", "cate": "cs.MA", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2503.18666", "title": "AgentSpec: Customizable Runtime Enforcement for Safe and Reliable LLM Agents", "authors": ["Haoyu Wang", "Christopher M. Poskitt", "Jun Sun"], "categories": ["cs.AI", "cs.CL"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      Accepted by the 48th IEEE/ACM International Conference on Software Engineering (ICSE 2026)", "url": "http://arxiv.org/abs/2503.18666v3", "summary": "Agents built on LLMs are increasingly deployed across diverse domains,\nautomating complex decision-making and task execution. However, their autonomy\nintroduces safety risks, including security vulnerabilities, legal violations,\nand unintended harmful actions. Existing mitigation methods, such as\nmodel-based safeguards and early enforcement strategies, fall short in\nrobustness, interpretability, and adaptability. To address these challenges, we\npropose AgentSpec, a lightweight domain-specific language for specifying and\nenforcing runtime constraints on LLM agents. With AgentSpec, users define\nstructured rules that incorporate triggers, predicates, and enforcement\nmechanisms, ensuring agents operate within predefined safety boundaries. We\nimplement AgentSpec across multiple domains, including code execution, embodied\nagents, and autonomous driving, demonstrating its adaptability and\neffectiveness. Our evaluation shows that AgentSpec successfully prevents unsafe\nexecutions in over 90% of code agent cases, eliminates all hazardous actions in\nembodied agent tasks, and enforces 100% compliance by autonomous vehicles\n(AVs). Despite its strong safety guarantees, AgentSpec remains computationally\nlightweight, with overheads in milliseconds. By combining interpretability,\nmodularity, and efficiency, AgentSpec provides a practical and scalable\nsolution for enforcing LLM agent safety across diverse applications. We also\nautomate the generation of rules using LLMs and assess their effectiveness. Our\nevaluation shows that the rules generated by OpenAI o1 achieve a precision of\n95.56% and recall of 70.96% for embodied agents, successfully identify 87.26%\nof the risky code, and prevent AVs from breaking laws in 5 out of 8 scenarios.", "comment": "Accepted by the 48th IEEE/ACM International Conference on Software\n  Engineering (ICSE 2026)", "pdf_url": "http://arxiv.org/pdf/2503.18666v3", "cate": "cs.AI", "date": "2025-03-24", "updated": "2025-07-31"}
{"id": "2504.16060", "title": "Vision-Language Models Are Not Pragmatically Competent in Referring Expression Generation", "authors": ["Ziqiao Ma", "Jing Ding", "Xuejun Zhang", "Dezhi Luo", "Jiahe Ding", "Sihan Xu", "Yuchen Huang", "Run Peng", "Joyce Chai"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      COLM 2025 & CVinW @ CVPR 2025 (Spotlight). Homepage: this https URL", "url": "http://arxiv.org/abs/2504.16060v3", "summary": "Referring Expression Generation (REG) is a core task for evaluating the\npragmatic competence of vision-language systems, requiring not only accurate\nsemantic grounding but also adherence to principles of cooperative\ncommunication (Grice, 1975). However, current evaluations of vision-language\nmodels (VLMs) often overlook the pragmatic dimension, reducing REG to a\nregion-based captioning task and neglecting Gricean maxims. In this work, we\nrevisit REG from a pragmatic perspective, introducing a new dataset (RefOI) of\n1.5k images annotated with both written and spoken referring expressions.\nThrough a systematic evaluation of state-of-the-art VLMs, we identify three key\nfailures of pragmatic competence: (1) failure to uniquely identify the\nreferent, (2) inclusion of excessive or irrelevant information, and (3)\nmisalignment with human pragmatic preference, such as the underuse of minimal\nspatial cues. We also show that standard automatic evaluations fail to capture\nthese pragmatic violations, reinforcing superficial cues rather than genuine\nreferential success. Our findings call for a renewed focus on pragmatically\ninformed models and evaluation frameworks that align with real human\ncommunication.", "comment": "COLM 2025 & CVinW @ CVPR 2025 (Spotlight). Homepage:\n  https://vlm-reg.github.io/", "pdf_url": "http://arxiv.org/pdf/2504.16060v3", "cate": "cs.CL", "date": "2025-04-22", "updated": "2025-07-31"}
{"id": "2507.23483", "title": "Stable-Sim2Real: Exploring Simulation of Real-Captured 3D Data with Two-Stage Depth Diffusion", "authors": ["Mutian Xu", "Chongjie Ye", "Haolin Liu", "Yushuang Wu", "Jiahao Chang", "Xiaoguang Han"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV 2025 (Highlight). Project page: this https URL", "url": "http://arxiv.org/abs/2507.23483v1", "summary": "3D data simulation aims to bridge the gap between simulated and real-captured\n3D data, which is a fundamental problem for real-world 3D visual tasks. Most 3D\ndata simulation methods inject predefined physical priors but struggle to\ncapture the full complexity of real data. An optimal approach involves learning\nan implicit mapping from synthetic to realistic data in a data-driven manner,\nbut progress in this solution has met stagnation in recent studies. This work\nexplores a new solution path of data-driven 3D simulation, called\nStable-Sim2Real, based on a novel two-stage depth diffusion model. The initial\nstage finetunes Stable-Diffusion to generate the residual between the real and\nsynthetic paired depth, producing a stable but coarse depth, where some local\nregions may deviate from realistic patterns. To enhance this, both the\nsynthetic and initial output depth are fed into a second-stage diffusion, where\ndiffusion loss is adjusted to prioritize these distinct areas identified by a\n3D discriminator. We provide a new benchmark scheme to evaluate 3D data\nsimulation methods. Extensive experiments show that training the network with\nthe 3D simulated data derived from our method significantly enhances\nperformance in real-world 3D visual tasks. Moreover, the evaluation\ndemonstrates the high similarity between our 3D simulated data and\nreal-captured patterns. Project page:\nhttps://mutianxu.github.io/stable-sim2real/.", "comment": "ICCV 2025 (Highlight). Project page:\n  https://mutianxu.github.io/stable-sim2real/", "pdf_url": "http://arxiv.org/pdf/2507.23483v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2409.11911", "title": "AI vs. Human Paintings? Deciphering Public Interactions and Perceptions towards AI-Generated Paintings on TikTok", "authors": ["Jiajun Wang", "Xiangzhe Yuan", "Siying Hu", "Zhicong Lu"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      Published online in International Journal of Human Computer Interaction", "url": "http://arxiv.org/abs/2409.11911v2", "summary": "With the development of generative AI technology, a vast array of\nAI-generated paintings (AIGP) have gone viral on social media like TikTok.\nHowever, some negative news about AIGP has also emerged. For example, in 2022,\nnumerous painters worldwide organized a large-scale anti-AI movement because of\nthe infringement in generative AI model training. This event reflected a social\nissue that, with the development and application of generative AI, public\nfeedback and feelings towards it may have been overlooked. Therefore, to\ninvestigate public interactions and perceptions towards AIGP on social media,\nwe analyzed user engagement level and comment sentiment scores of AIGP using\nhuman painting videos as a baseline. In analyzing user engagement, we also\nconsidered the possible moderating effect of the aesthetic quality of\nPaintings. Utilizing topic modeling, we identified seven reasons, including\nhyperrealistic quality, ambivalent reactions, perceived theft of art, etc.,\nleading to negative public perceptions of AIGP. Our work may provide\ninstructive suggestions for future generative AI technology development and\navoid potential crises in human-AI collaboration.", "comment": "Published online in International Journal of Human Computer\n  Interaction", "pdf_url": "http://arxiv.org/pdf/2409.11911v2", "cate": "cs.HC", "date": "2024-09-18", "updated": "2025-07-31"}
{"id": "2507.23433", "title": "From Timestamps to Versions: Version AoI in Single- and Multi-Hop Networks", "authors": ["Erfan Delfani", "Nikolaos Pappas"], "categories": ["cs.NI", "cs.IT", "math.IT"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23433v1", "summary": "Timely and informative data dissemination in communication networks is\nessential for enhancing system performance and energy efficiency, as it reduces\nthe transmission of outdated or redundant data. Timeliness metrics, such as Age\nof Information (AoI), effectively quantify data freshness; however, these\nmetrics fail to account for the intrinsic informativeness of the content\nitself. To address this limitation, content-based metrics have been proposed\nthat combine both timeliness and informativeness. Nevertheless, existing\nstudies have predominantly focused on evaluating average metric values, leaving\nthe complete distribution-particularly in multi-hop network scenarios-largely\nunexplored. In this paper, we provide a comprehensive analysis of the\nstationary distribution of the Version Age of Information (VAoI), a\ncontent-based metric, under various scheduling policies, including randomized\nstationary, uniform, and threshold-based policies, with transmission\nconstraints in single-hop and multi-hop networks. We derive closed-form\nexpressions for the stationary distribution and average VAoI under these\nscheduling approaches. Furthermore, for threshold-based scheduling, we\nanalytically determine the optimal threshold value that minimizes VAoI and\nderive the corresponding optimal VAoI in closed form. Numerical evaluations\nverify our analytical findings, providing valuable insights into leveraging\nVAoI in the design of efficient communication networks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23433v1", "cate": "cs.NI", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23644", "title": "Barriers to Healthcare: Agent-Based Modeling to Mitigate Inequity", "authors": ["Alba Aguilera", "Georgina Curto", "Nardine Osman"], "categories": ["cs.MA"], "primary_category": "Subjects:       Multiagent Systems (cs.MA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23644v1", "summary": "Agent-based simulations have an enormous potential as tools to evaluate\nsocial policies in a non-invasive way, before these are implemented to\nreal-world populations. However, the recommendations that these computational\napproaches may offer to tackle urgent human development challenges can vary\nsubstantially depending on how we model agents' (people) behaviour and the\ncriteria that we use to measure inequity. In this paper, we integrate the\nconceptual framework of the capability approach (CA), which is explicitly\ndesigned to promote and assess human well-being, to guide the simulation and\nevaluate the effectiveness of policies. We define a reinforcement learning\nenvironment where agents behave to restore their capabilities under the\nconstraints of a specific policy. Working in collaboration with local\nstakeholders, non-profits and domain experts, we apply our model in a case\nstudy to mitigate health inequity among the population experiencing\nhomelessness (PEH) in Barcelona. By doing so, we present the first proof of\nconcept simulation, aligned with the CA for human development, to assess the\nimpact of policies under parliamentary discussion.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23644v1", "cate": "cs.MA", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2505.11122", "title": "Navigating the Alpha Jungle: An LLM-Powered MCTS Framework for Formulaic Factor Mining", "authors": ["Yu Shi", "Yitong Duan", "Jian Li"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.11122v2", "summary": "Alpha factor mining is pivotal in quantitative investment for identifying\npredictive signals from complex financial data. While traditional formulaic\nalpha mining relies on human expertise, contemporary automated methods, such as\nthose based on genetic programming or reinforcement learning, often struggle\nwith search inefficiency or yield alpha factors that are difficult to\ninterpret. This paper introduces a novel framework that integrates Large\nLanguage Models (LLMs) with Monte Carlo Tree Search (MCTS) to overcome these\nlimitations. Our framework leverages the LLM's instruction-following and\nreasoning capability to iteratively generate and refine symbolic alpha formulas\nwithin an MCTS-driven exploration. A key innovation is the guidance of MCTS\nexploration by rich, quantitative feedback from financial backtesting of each\ncandidate factor, enabling efficient navigation of the vast search space.\nFurthermore, a frequent subtree avoidance mechanism is introduced to enhance\nsearch diversity and prevent formulaic homogenization, further improving\nperformance. Experimental results on real-world stock market data demonstrate\nthat our LLM-based framework outperforms existing methods by mining alphas with\nsuperior predictive accuracy and trading performance. The resulting formulas\nare also more amenable to human interpretation, establishing a more effective\nand efficient paradigm for formulaic alpha mining.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.11122v2", "cate": "cs.AI", "date": "2025-05-16", "updated": "2025-07-31"}
{"id": "2505.14874", "title": "Towards Inclusive ASR: Investigating Voice Conversion for Dysarthric Speech Recognition in Low-Resource Languages", "authors": ["Chin-Jou Li", "Eunjung Yeo", "Kwanghee Choi", "Paula Andrea Pérez-Toro", "Masao Someki", "Rohan Kumar Das", "Zhengjun Yue", "Juan Rafael Orozco-Arroyave", "Elmar Nöth", "David R. Mortensen"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      5 pages, 1 figure, Accepted to Interspeech 2025", "url": "http://arxiv.org/abs/2505.14874v4", "summary": "Automatic speech recognition (ASR) for dysarthric speech remains challenging\ndue to data scarcity, particularly in non-English languages. To address this,\nwe fine-tune a voice conversion model on English dysarthric speech (UASpeech)\nto encode both speaker characteristics and prosodic distortions, then apply it\nto convert healthy non-English speech (FLEURS) into non-English dysarthric-like\nspeech. The generated data is then used to fine-tune a multilingual ASR model,\nMassively Multilingual Speech (MMS), for improved dysarthric speech\nrecognition. Evaluation on PC-GITA (Spanish), EasyCall (Italian), and SSNCE\n(Tamil) demonstrates that VC with both speaker and prosody conversion\nsignificantly outperforms the off-the-shelf MMS performance and conventional\naugmentation techniques such as speed and tempo perturbation. Objective and\nsubjective analyses of the generated data further confirm that the generated\nspeech simulates dysarthric characteristics.", "comment": "5 pages, 1 figure, Accepted to Interspeech 2025", "pdf_url": "http://arxiv.org/pdf/2505.14874v4", "cate": "cs.CL", "date": "2025-05-20", "updated": "2025-07-31"}
{"id": "2507.23487", "title": "Online Estimation of Table-Top Grown Strawberry Mass in Field Conditions with Occlusions", "authors": ["Jinshan Zhen", "Yuanyue Ge", "Tianxiao Zhu", "Hui Zhao", "Ya Xiong"], "categories": ["cs.CV", "cs.RO"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by IROS 2025", "url": "http://arxiv.org/abs/2507.23487v1", "summary": "Accurate mass estimation of table-top grown strawberries under field\nconditions remains challenging due to frequent occlusions and pose variations.\nThis study proposes a vision-based pipeline integrating RGB-D sensing and deep\nlearning to enable non-destructive, real-time and online mass estimation. The\nmethod employed YOLOv8-Seg for instance segmentation, Cycle-consistent\ngenerative adversarial network (CycleGAN) for occluded region completion, and\ntilt-angle correction to refine frontal projection area calculations. A\npolynomial regression model then mapped the geometric features to mass.\nExperiments demonstrated mean mass estimation errors of 8.11% for isolated\nstrawberries and 10.47% for occluded cases. CycleGAN outperformed large mask\ninpainting (LaMa) model in occlusion recovery, achieving superior pixel area\nratios (PAR) (mean: 0.978 vs. 1.112) and higher intersection over union (IoU)\nscores (92.3% vs. 47.7% in the [0.9-1] range). This approach addresses critical\nlimitations of traditional methods, offering a robust solution for automated\nharvesting and yield monitoring with complex occlusion patterns.", "comment": "Accepted by IROS 2025", "pdf_url": "http://arxiv.org/pdf/2507.23487v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2410.07486", "title": "Visual Story-Writing: Writing by Manipulating Visual Representations of Stories", "authors": ["Damien Masson", "Zixin Zhao", "Fanny Chevalier"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      In Proceedings of the 38th Annual ACM Symposium on User Interface Software and Technology (UIST '25)", "url": "http://arxiv.org/abs/2410.07486v2", "summary": "We define \"visual story-writing\" as using visual representations of story\nelements to support writing and revising narrative texts. To demonstrate this\napproach, we developed a text editor that automatically visualizes a graph of\nentity interactions, movement between locations, and a timeline of story\nevents. Interacting with these visualizations results in suggested text edits:\nfor example, connecting two characters in the graph creates an interaction\nbetween them, moving an entity updates their described location, and\nrearranging events on the timeline reorganizes the narrative sequence. Through\ntwo user studies on narrative text editing and writing, we found that visuals\nsupported participants in planning high-level revisions, tracking story\nelements, and exploring story variations in ways that encourage creativity.\nBroadly, our work lays the foundation for writing support, not just through\nwords, but also visuals.", "comment": "In Proceedings of the 38th Annual ACM Symposium on User Interface\n  Software and Technology (UIST '25)", "pdf_url": "http://arxiv.org/pdf/2410.07486v2", "cate": "cs.HC", "date": "2024-10-09", "updated": "2025-07-31"}
{"id": "2507.23526", "title": "Channel Estimation for 6G Near-Field Wireless Communications: A Comprehensive Survey", "authors": ["Wen-Xuan Long", "Shengyu Ye", "Marco Moretti", "Michele Morelli", "Luca Sanguinetti", "Rui Chen", "Cheng-Xiang Wang"], "categories": ["eess.SP", "cs.IT", "math.IT"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23526v1", "summary": "The sixth-generation (6G) wireless systems are expected to adopt extremely\nlarge aperture arrays (ELAAs), novel antenna architectures, and operate in\nextremely high-frequency bands to meet growing data demands. ELAAs\nsignificantly increase the number of antennas, enabling finer spatial\nresolution and improved beamforming. At high frequencies, ELAAs shift\ncommunication from the conventional far-field to near-field regime, where\nspherical wavefronts dominate and the channel response depends on both angle\nand distance, increasing channel dimensionality. Conventional far-field channel\nestimation methods, which rely on angular information, struggle in near-field\nscenarios due to increased pilot overhead and computational complexity. This\npaper presents a comprehensive survey of recent advances in near-field channel\nestimation. It first defines the near- and far-field boundary from an\nelectromagnetic perspective and discusses key propagation differences,\nalongside a brief review of ELAA developments. Then, it introduces mainstream\nnear-field channel models and compares them with far-field models. Major\nestimation techniques are reviewed under different configurations\n(single/multi-user, single/multi-carrier), including both direct estimation and\nRIS-assisted cascaded estimation. These techniques reveal trade-offs among\nestimation accuracy, complexity, and overhead. This survey aims to provide\ninsights and foundations for efficient and scalable near-field channel\nestimation in 6G systems, while identifying key challenges and future research\ndirections.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23526v1", "cate": "eess.SP", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23344", "title": "Designing Dynamic Pricing for Bike-sharing Systems via Differentiable Agent-based Simulation", "authors": ["Tatsuya Mitomi", "Fumiyasu Makinoshima", "Fumiya Makihara", "Eigo Segawa"], "categories": ["cs.LG", "cs.MA"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23344v1", "summary": "Bike-sharing systems are emerging in various cities as a new ecofriendly\ntransportation system. In these systems, spatiotemporally varying user demands\nlead to imbalanced inventory at bicycle stations, resulting in additional\nrelocation costs. Therefore, it is essential to manage user demand through\noptimal dynamic pricing for the system. However, optimal pricing design for\nsuch a system is challenging because the system involves users with diverse\nbackgrounds and their probabilistic choices. To address this problem, we\ndevelop a differentiable agent-based simulation to rapidly design dynamic\npricing in bike-sharing systems, achieving balanced bicycle inventory despite\nspatiotemporally heterogeneous trips and probabilistic user decisions. We first\nvalidate our approach against conventional methods through numerical\nexperiments involving 25 bicycle stations and five time slots, yielding 100\nparameters. Compared to the conventional methods, our approach obtains a more\naccurate solution with a 73% to 78% reduction in loss while achieving more than\na 100-fold increase in convergence speed. We further validate our approach on a\nlarge-scale urban bike-sharing system scenario involving 289 bicycle stations,\nresulting in a total of 1156 parameters. Through simulations using the obtained\npricing policies, we confirm that these policies can naturally induce balanced\ninventory without any manual relocation. Additionally, we find that the cost of\ndiscounts to induce the balanced inventory can be minimized by setting\nappropriate initial conditions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23344v1", "cate": "cs.LG", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2505.17696", "title": "Enhancing AI System Resiliency: Formulation and Guarantee for LSTM Resilience Based on Control Theory", "authors": ["Sota Yoshihara", "Ryosuke Yamamoto", "Hiroyuki Kusumoto", "Masanari Shimura"], "categories": ["cs.AI", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      9 pages, 6 figures. Appendix: 17 pages. First three listed authors have equal contributions", "url": "http://arxiv.org/abs/2505.17696v3", "summary": "This paper proposes a novel theoretical framework for guaranteeing and\nevaluating the resilience of long short-term memory (LSTM) networks in control\nsystems. We introduce \"recovery time\" as a new metric of resilience in order to\nquantify the time required for an LSTM to return to its normal state after\nanomalous inputs. By mathematically refining incremental input-to-state\nstability ($\\delta$ISS) theory for LSTM, we derive a practical data-independent\nupper bound on recovery time. This upper bound gives us resilience-aware\ntraining. Experimental validation on simple models demonstrates the\neffectiveness of our resilience estimation and control methods, enhancing a\nfoundation for rigorous quality assurance in safety-critical AI applications.", "comment": "9 pages, 6 figures. Appendix: 17 pages. First three listed authors\n  have equal contributions", "pdf_url": "http://arxiv.org/pdf/2505.17696v3", "cate": "cs.AI", "date": "2025-05-23", "updated": "2025-07-31"}
{"id": "2505.18497", "title": "The Pragmatic Mind of Machines: Tracing the Emergence of Pragmatic Competence in Large Language Models", "authors": ["Kefan Yu", "Qingcheng Zeng", "Weihao Xuan", "Wanxin Li", "Jingyi Wu", "Rob Voigt"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.18497v2", "summary": "Current large language models (LLMs) have demonstrated emerging capabilities\nin social intelligence tasks, including implicature resolution and\ntheory-of-mind reasoning, both of which require substantial pragmatic\nunderstanding. However, how LLMs acquire this pragmatic competence throughout\nthe training process remains poorly understood. In this work, we introduce\nALTPRAG, a dataset grounded in the pragmatic concept of alternatives, to\nevaluate whether LLMs at different training stages can accurately infer nuanced\nspeaker intentions. Each instance pairs two equally plausible yet pragmatically\ndivergent continuations and requires the model to (i) infer the speaker's\nintended meaning and (ii) explain when and why a speaker would choose one\nutterance over its alternative, thus directly probing pragmatic competence\nthrough contrastive reasoning. We systematically evaluate 22 LLMs across 3 key\ntraining stages: after pre-training, supervised fine-tuning (SFT), and\npreference optimization, to examine the development of pragmatic competence.\nOur results show that even base models exhibit notable sensitivity to pragmatic\ncues, which improves consistently with increases in model and data scale.\nAdditionally, SFT and RLHF contribute further gains, particularly in\ncognitive-pragmatic scenarios. These findings highlight pragmatic competence as\nan emergent and compositional property of LLM training and offer new insights\nfor aligning models with human communicative norms.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.18497v2", "cate": "cs.CL", "date": "2025-05-24", "updated": "2025-07-31"}
{"id": "2507.23508", "title": "Hyperbolic Cycle Alignment for Infrared-Visible Image Fusion", "authors": ["Timing Li", "Bing Cao", "Jiahe Feng", "Haifang Cao", "Qinghau Hu", "Pengfei Zhu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23508v1", "summary": "Image fusion synthesizes complementary information from multiple sources,\nmitigating the inherent limitations of unimodal imaging systems. Accurate image\nregistration is essential for effective multi-source data fusion. However,\nexisting registration methods, often based on image translation in Euclidean\nspace, fail to handle cross-modal misalignment effectively, resulting in\nsuboptimal alignment and fusion quality. To overcome this limitation, we\nexplore image alignment in non-Euclidean space and propose a Hyperbolic Cycle\nAlignment Network (Hy-CycleAlign). To the best of our knowledge, Hy-CycleAlign\nis the first image registration method based on hyperbolic space. It introduces\na dual-path cross-modal cyclic registration framework, in which a forward\nregistration network aligns cross-modal inputs, while a backward registration\nnetwork reconstructs the original image, forming a closed-loop registration\nstructure with geometric consistency. Additionally, we design a Hyperbolic\nHierarchy Contrastive Alignment (H$^{2}$CA) module, which maps images into\nhyperbolic space and imposes registration constraints, effectively reducing\ninterference caused by modality discrepancies. We further analyze image\nregistration in both Euclidean and hyperbolic spaces, demonstrating that\nhyperbolic space enables more sensitive and effective multi-modal image\nregistration. Extensive experiments on misaligned multi-modal images\ndemonstrate that our method significantly outperforms existing approaches in\nboth image alignment and fusion. Our code will be publicly available.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23508v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2505.19101", "title": "Agentic Visualization: Extracting Agent-based Design Patterns from Visualization Systems", "authors": ["Vaishali Dhanoa", "Anton Wolter", "Gabriela Molina León", "Hans-Jörg Schulz", "Niklas Elmqvist"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.19101v2", "summary": "Autonomous agents powered by Large Language Models are transforming AI,\ncreating an imperative for the visualization field to embrace agentic\nframeworks. However, our field's focus on a human in the sensemaking loop\nraises critical questions about autonomy, delegation, and coordination for such\n\\textit{agentic visualization} that preserve human agency while amplifying\nanalytical capabilities. This paper addresses these questions by reinterpreting\nexisting visualization systems with semi-automated or fully automatic AI\ncomponents through an agentic lens. Based on this analysis, we extract a\ncollection of design patterns for agentic visualization, including agentic\nroles, communication and coordination. These patterns provide a foundation for\nfuture agentic visualization systems that effectively harness AI agents while\nmaintaining human insight and control.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.19101v2", "cate": "cs.HC", "date": "2025-05-25", "updated": "2025-07-31"}
{"id": "2507.23646", "title": "Information geometry of Lévy processes and financial models", "authors": ["Jaehyung Choi"], "categories": ["stat.TH", "cs.IT", "math.DG", "math.IT", "math.PR", "q-fin.MF"], "primary_category": "Subjects:       Statistics Theory (math.ST)", "pdf_link": null, "comments": "Comments:      21 pages", "url": "http://arxiv.org/abs/2507.23646v1", "summary": "We explore the information geometry of L\\'evy processes. As a starting point,\nwe derive the $\\alpha$-divergence between two L\\'evy processes. Subsequently,\nthe Fisher information matrix and the $\\alpha$-connection associated with the\ngeometry of L\\'evy processes are computed from the $\\alpha$-divergence. In\naddition, we discuss statistical applications of this information geometry. As\nillustrative examples, we investigate the differential-geometric structures of\nvarious L\\'evy processes relevant to financial modeling, including tempered\nstable processes, the CGMY model, and variance gamma processes.", "comment": "21 pages", "pdf_url": "http://arxiv.org/pdf/2507.23646v1", "cate": "stat.TH", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2504.06684", "title": "SDHN: Skewness-Driven Hypergraph Networks for Enhanced Localized Multi-Robot Coordination", "authors": ["Delin Zhao", "Yanbo Shan", "Chang Liu", "Shenghang Lin", "Yingxin Shou", "Bin Xu"], "categories": ["cs.RO", "cs.MA"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.06684v2", "summary": "Multi-Agent Reinforcement Learning is widely used for multi-robot\ncoordination, where simple graphs typically model pairwise interactions.\nHowever, such representations fail to capture higher-order collaborations,\nlimiting effectiveness in complex tasks. While hypergraph-based approaches\nenhance cooperation, existing methods often generate arbitrary hypergraph\nstructures and lack adaptability to environmental uncertainties. To address\nthese challenges, we propose the Skewness-Driven Hypergraph Network (SDHN),\nwhich employs stochastic Bernoulli hyperedges to explicitly model higher-order\nmulti-robot interactions. By introducing a skewness loss, SDHN promotes an\nefficient structure with Small-Hyperedge Dominant Hypergraph, allowing robots\nto prioritize localized synchronization while still adhering to the overall\ninformation, similar to human coordination. Extensive experiments on Moving\nAgents in Formation and Robotic Warehouse tasks validate SDHN's effectiveness,\ndemonstrating superior performance over state-of-the-art baselines.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.06684v2", "cate": "cs.RO", "date": "2025-04-09", "updated": "2025-07-31"}
{"id": "2505.19219", "title": "Where Paths Collide: A Comprehensive Survey of Classic and Learning-Based Multi-Agent Pathfinding", "authors": ["Shiyue Wang", "Haozheng Xu", "Yuhan Zhang", "Jingran Lin", "Changhong Lu", "Xiangfeng Wang", "Wenhao Li"], "categories": ["cs.AI", "cs.LG", "cs.MA", "math.CO"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      112 pages, 21 figures, 20 tables. The project website is: this https URL", "url": "http://arxiv.org/abs/2505.19219v2", "summary": "Multi-Agent Path Finding (MAPF) is a fundamental problem in artificial\nintelligence and robotics, requiring the computation of collision-free paths\nfor multiple agents navigating from their start locations to designated goals.\nAs autonomous systems become increasingly prevalent in warehouses, urban\ntransportation, and other complex environments, MAPF has evolved from a\ntheoretical challenge to a critical enabler of real-world multi-robot\ncoordination. This comprehensive survey bridges the long-standing divide\nbetween classical algorithmic approaches and emerging learning-based methods in\nMAPF research. We present a unified framework that encompasses search-based\nmethods (including Conflict-Based Search, Priority-Based Search, and Large\nNeighborhood Search), compilation-based approaches (SAT, SMT, CSP, ASP, and MIP\nformulations), and data-driven techniques (reinforcement learning, supervised\nlearning, and hybrid strategies). Through systematic analysis of experimental\npractices across 200+ papers, we uncover significant disparities in evaluation\nmethodologies, with classical methods typically tested on larger-scale\ninstances (up to 200 by 200 grids with 1000+ agents) compared to learning-based\napproaches (predominantly 10-100 agents). We provide a comprehensive taxonomy\nof evaluation metrics, environment types, and baseline selections, highlighting\nthe need for standardized benchmarking protocols. Finally, we outline promising\nfuture directions including mixed-motive MAPF with game-theoretic\nconsiderations, language-grounded planning with large language models, and\nneural solver architectures that combine the rigor of classical methods with\nthe flexibility of deep learning. This survey serves as both a comprehensive\nreference for researchers and a practical guide for deploying MAPF solutions in\nincreasingly complex real-world applications.", "comment": "112 pages, 21 figures, 20 tables. The project website is:\n  https://wangsh1yue.github.io/Where-Paths-Collide", "pdf_url": "http://arxiv.org/pdf/2505.19219v2", "cate": "cs.AI", "date": "2025-05-25", "updated": "2025-07-31"}
{"id": "2505.23628", "title": "AutoSchemaKG: Autonomous Knowledge Graph Construction through Dynamic Schema Induction from Web-Scale Corpora", "authors": ["Jiaxin Bai", "Wei Fan", "Qi Hu", "Qing Zong", "Chunyang Li", "Hong Ting Tsang", "Hongyu Luo", "Yauwai Yim", "Haoyu Huang", "Xiao Zhou", "Feng Qin", "Tianshi Zheng", "Xi Peng", "Xin Yao", "Huiwen Yang", "Leijie Wu", "Yi Ji", "Gong Zhang", "Renhai Chen", "Yangqiu Song"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      9 pages, preprint, code: this https URL", "url": "http://arxiv.org/abs/2505.23628v3", "summary": "We present AutoSchemaKG, a framework for fully autonomous knowledge graph\nconstruction that eliminates the need for predefined schemas. Our system\nleverages large language models to simultaneously extract knowledge triples and\ninduce comprehensive schemas directly from text, modeling both entities and\nevents while employing conceptualization to organize instances into semantic\ncategories. Processing over 50 million documents, we construct ATLAS (Automated\nTriple Linking And Schema induction), a family of knowledge graphs with 900+\nmillion nodes and 5.9 billion edges. This approach outperforms state-of-the-art\nbaselines on multi-hop QA tasks and enhances LLM factuality. Notably, our\nschema induction achieves 92\\% semantic alignment with human-crafted schemas\nwith zero manual intervention, demonstrating that billion-scale knowledge\ngraphs with dynamically induced schemas can effectively complement parametric\nknowledge in large language models.", "comment": "9 pages, preprint, code:\n  https://github.com/HKUST-KnowComp/AutoSchemaKG", "pdf_url": "http://arxiv.org/pdf/2505.23628v3", "cate": "cs.CL", "date": "2025-05-29", "updated": "2025-08-01"}
{"id": "2507.23567", "title": "3D-MOOD: Lifting 2D to 3D for Monocular Open-Set Object Detection", "authors": ["Yung-Hsu Yang", "Luigi Piccinelli", "Mattia Segu", "Siyuan Li", "Rui Huang", "Yuqian Fu", "Marc Pollefeys", "Hermann Blum", "Zuria Bauer"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV 2025", "url": "http://arxiv.org/abs/2507.23567v1", "summary": "Monocular 3D object detection is valuable for various applications such as\nrobotics and AR/VR. Existing methods are confined to closed-set settings, where\nthe training and testing sets consist of the same scenes and/or object\ncategories. However, real-world applications often introduce new environments\nand novel object categories, posing a challenge to these methods. In this\npaper, we address monocular 3D object detection in an open-set setting and\nintroduce the first end-to-end 3D Monocular Open-set Object Detector (3D-MOOD).\nWe propose to lift the open-set 2D detection into 3D space through our designed\n3D bounding box head, enabling end-to-end joint training for both 2D and 3D\ntasks to yield better overall performance. We condition the object queries with\ngeometry prior and overcome the generalization for 3D estimation across diverse\nscenes. To further improve performance, we design the canonical image space for\nmore efficient cross-dataset training. We evaluate 3D-MOOD on both closed-set\nsettings (Omni3D) and open-set settings (Omni3D to Argoverse 2, ScanNet), and\nachieve new state-of-the-art results. Code and models are available at\nroyyang0714.github.io/3D-MOOD.", "comment": "ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.23567v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.06460", "title": "Ragged Blocks: Rendering Structured Text with Style", "authors": ["Sam Cohen", "Ravi Chugh"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      UIST 2025 Paper + Appendices", "url": "http://arxiv.org/abs/2507.06460v2", "summary": "Whether it be source code in a programming language, prose in natural\nlanguage, or otherwise, text is highly structured. Currently, text\nvisualizations are confined either to _flat, line-based_ decorations, which can\nconvey only limited information about textual structure, or _nested boxes_,\nwhich convey structure but often destroy the typographic layout of the\nunderlying text. We hypothesize that the lack of rich styling options limits\nthe kinds of information that are displayed alongside text, wherever it may be\ndisplayed.\n  In this paper, we show that it is possible to achieve arbitrarily nested\ndecorations while minimally disturbing the underlying typographic layout.\nSpecifically, we present a layout algorithm that generates _ragged blocks_, or\n_rocks_, which are rectilinear polygons that allow nested text to be compactly\nrendered even when styled with borders and padding. Our layout algorithm is\nevaluated on a benchmark suite comprising representative source code files in\nmultiple programming languages. The (ragged block) layouts produced by our\nalgorithm are substantially more compact than the (rectangular block) layouts\nproduced by conventional techniques, when uniformly styling every element in\nthe syntax tree with borders and padding.", "comment": "UIST 2025 Paper + Appendices", "pdf_url": "http://arxiv.org/pdf/2507.06460v2", "cate": "cs.HC", "date": "2025-07-09", "updated": "2025-07-31"}
{"id": "2507.23707", "title": "Cellular, Cell-less, and Everything in Between: A Unified Framework for Utility Region Analysis in Wireless Networks", "authors": ["Renato Luis Garrido Cavalcante", "Tomasz Piotrowski", "Slawomir Stanczak"], "categories": ["eess.SP", "cs.IT", "math.IT"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23707v1", "summary": "We introduce a unified framework for analyzing utility regions of wireless\nnetworks, with a focus on the signal-to-interference-noise-ratio (SINR) and\nachievable rate regions. The framework provides valuable insights into\ninterference patterns of modern network architectures, such as cell-less and\nextremely large MIMO networks, and it generalizes existing characterizations of\nthe weak Pareto boundary. A central contribution is the derivation of\nsufficient conditions that guarantee convexity of the utility regions.\nConvexity is an important property because it ensures that time sharing (or\nuser grouping) cannot simultaneously increase the utility of all users when the\nnetwork operates on the weak Pareto boundary. These sufficient conditions also\nhave two key implications. First, they identify a family of (weighted) sum-rate\nmaximization problems that are inherently convex without any variable\ntransformations, thus paving the way for the development of efficient, provably\noptimal solvers for this family. Second, they provide a rigorous justification\nfor formulating sum-rate maximization problems directly in terms of achievable\nrates, rather than SINR levels. Our theoretical insights also motivate an\nalternative to the concept of favorable propagation in the massive MIMO\nliterature -- one that explicitly accounts for self-interference and the\nbeamforming strategy.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23707v1", "cate": "eess.SP", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.21035", "title": "GenoMAS: A Multi-Agent Framework for Scientific Discovery via Code-Driven Gene Expression Analysis", "authors": ["Haoyang Liu", "Yijiang Li", "Haohan Wang"], "categories": ["cs.AI", "cs.LG", "cs.MA", "q-bio.GN"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      51 pages (13 pages for the main text, 9 pages for references, and 29 pages for the appendix)", "url": "http://arxiv.org/abs/2507.21035v2", "summary": "Gene expression analysis holds the key to many biomedical discoveries, yet\nextracting insights from raw transcriptomic data remains formidable due to the\ncomplexity of multiple large, semi-structured files and the need for extensive\ndomain expertise. Current automation approaches are often limited by either\ninflexible workflows that break down in edge cases or by fully autonomous\nagents that lack the necessary precision for rigorous scientific inquiry.\nGenoMAS charts a different course by presenting a team of LLM-based scientists\nthat integrates the reliability of structured workflows with the adaptability\nof autonomous agents. GenoMAS orchestrates six specialized LLM agents through\ntyped message-passing protocols, each contributing complementary strengths to a\nshared analytic canvas. At the heart of GenoMAS lies a guided-planning\nframework: programming agents unfold high-level task guidelines into Action\nUnits and, at each juncture, elect to advance, revise, bypass, or backtrack,\nthereby maintaining logical coherence while bending gracefully to the\nidiosyncrasies of genomic data.\n  On the GenoTEX benchmark, GenoMAS reaches a Composite Similarity Correlation\nof 89.13% for data preprocessing and an F$_1$ of 60.48% for gene\nidentification, surpassing the best prior art by 10.61% and 16.85%\nrespectively. Beyond metrics, GenoMAS surfaces biologically plausible\ngene-phenotype associations corroborated by the literature, all while adjusting\nfor latent confounders. Code is available at https://github.com/Liu-Hy/GenoMAS.", "comment": "51 pages (13 pages for the main text, 9 pages for references, and 29\n  pages for the appendix)", "pdf_url": "http://arxiv.org/pdf/2507.21035v2", "cate": "cs.AI", "date": "2025-07-28", "updated": "2025-07-31"}
{"id": "2508.00579", "title": "MMRAG-DocQA: A Multi-Modal Retrieval-Augmented Generation Method for Document Question-Answering with Hierarchical Index and Multi-Granularity Retrieval", "authors": ["Ziyu Gong", "Yihua Huang", "Chengcheng Mai"], "categories": ["cs.MM", "cs.IR"], "primary_category": "Subjects:       Multimedia (cs.MM)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00579v1", "summary": "The multi-modal long-context document question-answering task aims to locate\nand integrate multi-modal evidences (such as texts, tables, charts, images, and\nlayouts) distributed across multiple pages, for question understanding and\nanswer generation. The existing methods can be categorized into Large\nVision-Language Model (LVLM)-based and Retrieval-Augmented Generation\n(RAG)-based methods. However, the former were susceptible to hallucinations,\nwhile the latter struggled for inter-modal disconnection and cross-page\nfragmentation. To address these challenges, a novel multi-modal RAG model,\nnamed MMRAG-DocQA, was proposed, leveraging both textual and visual information\nacross long-range pages to facilitate accurate question answering. A\nhierarchical indexing method with the integration of flattened in-page chunks\nand topological cross-page chunks was designed to jointly establish in-page\nmulti-modal associations and long-distance cross-page dependencies. By means of\njoint similarity evaluation and large language model (LLM)-based re-ranking, a\nmulti-granularity semantic retrieval method, including the page-level parent\npage retrieval and document-level summary retrieval, was proposed to foster\nmulti-modal evidence connection and long-distance evidence integration and\nreasoning. Experimental results performed on public datasets, MMLongBench-Doc\nand LongDocURL, demonstrated the superiority of our MMRAG-DocQA method in\nunderstanding and answering modality-rich and multi-page documents.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00579v1", "cate": "cs.MM", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2506.07528", "title": "Coordinating Search-Informed Reasoning and Reasoning-Guided Search in Claim Verification", "authors": ["Qisheng Hu", "Quanyu Long", "Wenya Wang"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      Work in progress", "url": "http://arxiv.org/abs/2506.07528v2", "summary": "Multi-hop claim verification is inherently challenging, requiring multi-step\nreasoning to construct verification chains while iteratively searching for\ninformation to uncover hidden bridging facts. This process is fundamentally\ninterleaved, as effective reasoning relies on dynamically retrieved evidence,\nwhile effective search demands reasoning to refine queries based on partial\ninformation. To achieve this, we propose Hierarchical Agent Reasoning and\nInformation Search (HARIS), explicitly modeling the coordinated process of\nreasoning-driven searching and search-informed reasoning. HARIS consists of a\nhigh-level reasoning agent that focuses on constructing the main verification\nchain, generating factual questions when more information is needed, and a\nlow-level search agent that iteratively retrieves more information, refining\nits search based on intermediate findings. This design allows each agent to\nspecialize in its respective task, enhancing verification accuracy and\ninterpretability. HARIS is trained using reinforcement learning with\noutcome-based rewards. Experimental results on the EX-FEVER and HOVER\nbenchmarks demonstrate that HARIS achieves strong performance, greatly\nadvancing multi-hop claim verification.", "comment": "Work in progress", "pdf_url": "http://arxiv.org/pdf/2506.07528v2", "cate": "cs.AI", "date": "2025-06-09", "updated": "2025-07-31"}
{"id": "2506.00068", "title": "Framing Political Bias in Multilingual LLMs Across Pakistani Languages", "authors": ["Afrozah Nadeem", "Mark Dras", "Usman Naseem"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Preprint", "url": "http://arxiv.org/abs/2506.00068v2", "summary": "Large Language Models (LLMs) increasingly shape public discourse, yet most\nevaluations of political and economic bias have focused on high-resource,\nWestern languages and contexts. This leaves critical blind spots in\nlow-resource, multilingual regions such as Pakistan, where linguistic identity\nis closely tied to political, religious, and regional ideologies. We present a\nsystematic evaluation of political bias in 13 state-of-the-art LLMs across five\nPakistani languages: Urdu, Punjabi, Sindhi, Pashto, and Balochi. Our framework\nintegrates a culturally adapted Political Compass Test (PCT) with multi-level\nframing analysis, capturing both ideological stance (economic/social axes) and\nstylistic framing (content, tone, emphasis). Prompts are aligned with 11\nsocio-political themes specific to the Pakistani context. Results show that\nwhile LLMs predominantly reflect liberal-left orientations consistent with\nWestern training data, they exhibit more authoritarian framing in regional\nlanguages, highlighting language-conditioned ideological modulation. We also\nidentify consistent model-specific bias patterns across languages. These\nfindings show the need for culturally grounded, multilingual bias auditing\nframeworks in global NLP.", "comment": "Preprint", "pdf_url": "http://arxiv.org/pdf/2506.00068v2", "cate": "cs.CL", "date": "2025-05-29", "updated": "2025-07-31"}
{"id": "2507.23569", "title": "Gaussian Splatting Feature Fields for Privacy-Preserving Visual Localization", "authors": ["Maxime Pietrantoni", "Gabriela Csurka", "Torsten Sattler"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      CVPR 2025", "url": "http://arxiv.org/abs/2507.23569v1", "summary": "Visual localization is the task of estimating a camera pose in a known\nenvironment. In this paper, we utilize 3D Gaussian Splatting (3DGS)-based\nrepresentations for accurate and privacy-preserving visual localization. We\npropose Gaussian Splatting Feature Fields (GSFFs), a scene representation for\nvisual localization that combines an explicit geometry model (3DGS) with an\nimplicit feature field. We leverage the dense geometric information and\ndifferentiable rasterization algorithm from 3DGS to learn robust feature\nrepresentations grounded in 3D. In particular, we align a 3D scale-aware\nfeature field and a 2D feature encoder in a common embedding space through a\ncontrastive framework. Using a 3D structure-informed clustering procedure, we\nfurther regularize the representation learning and seamlessly convert the\nfeatures to segmentations, which can be used for privacy-preserving visual\nlocalization. Pose refinement, which involves aligning either feature maps or\nsegmentations from a query image with those rendered from the GSFFs scene\nrepresentation, is used to achieve localization. The resulting privacy- and\nnon-privacy-preserving localization pipelines, evaluated on multiple real-world\ndatasets, show state-of-the-art performances.", "comment": "CVPR 2025", "pdf_url": "http://arxiv.org/pdf/2507.23569v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.22252", "title": "Multidimensional Assessment of Takeover Performance in Conditionally Automated Driving", "authors": ["Kexin Liang", "Jan Luca Kästle", "Bani Anvari", "Simeon C. Calvert", "J. W. C. van Lint"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22252v2", "summary": "When automated driving systems encounter complex situations beyond their\noperational capabilities, they issue takeover requests, prompting drivers to\nresume vehicle control and return to the driving loop as a critical safety\nbackup. However, this control transition places significant demands on drivers,\nrequiring them to promptly respond to takeover requests while executing\nhigh-quality interventions. To ensure safe and comfortable control transitions,\nit is essential to develop a deep understanding of the key factors influencing\nvarious takeover performance aspects. This study evaluates drivers' takeover\nperformance across three dimensions: response efficiency, user experience, and\ndriving safety - using a driving simulator experiment. EXtreme Gradient\nBoosting (XGBoost) models are used to investigate the contributions of two\ncritical factors, i.e., Situational Awareness (SA) and Spare Capacity (SC), in\npredicting various takeover performance metrics by comparing the predictive\nresults to the baseline models that rely solely on basic Driver Characteristics\n(DC). The results reveal that (i) higher SA enables drivers to respond to\ntakeover requests more quickly, particularly for reflexive responses; and (ii)\nSC shows a greater overall impact on takeover quality than SA, where higher SC\ngenerally leads to enhanced subjective rating scores and objective execution\ntrajectories. These findings highlight the distinct yet complementary roles of\nSA and SC in shaping performance components, offering valuable insights for\noptimizing human-vehicle interactions and enhancing automated driving system\ndesign.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22252v2", "cate": "cs.HC", "date": "2025-07-29", "updated": "2025-07-31"}
{"id": "2409.14148", "title": "A New Upper Bound for Distributed Hypothesis Testing Using the Auxiliary Receiver Approach", "authors": ["Zhenduo Wen", "Amin Gohari"], "categories": ["cs.IT", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2409.14148v4", "summary": "This paper employs the add-and-subtract technique of the auxiliary receiver\napproach to establish a new upper bound for the distributed hypothesis testing\nproblem. This new bound has fewer assumptions than the upper bound proposed by\nRahman and Wagner, is at least as tight as the bound by Rahman and Wagner, and\ncan outperform it in certain Gaussian settings. Conceptually speaking, unlike\nRahman and Wagner, who view their additional receiver as side information, we\nview it as an auxiliary receiver and use a different manipulation for\nsingle-letterization.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2409.14148v4", "cate": "cs.IT", "date": "2024-09-21", "updated": "2025-07-31"}
{"id": "2508.00260", "title": "Instruction-Grounded Visual Projectors for Continual Learning of Generative Vision-Language Models", "authors": ["Hyundong Jin", "Hyung Jin Chang", "Eunwoo Kim"], "categories": ["cs.CV", "cs.MM"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted to ICCV 2025", "url": "http://arxiv.org/abs/2508.00260v1", "summary": "Continual learning enables pre-trained generative vision-language models\n(VLMs) to incorporate knowledge from new tasks without retraining data from\nprevious ones. Recent methods update a visual projector to translate visual\ninformation for new tasks, connecting pre-trained vision encoders with large\nlanguage models. However, such adjustments may cause the models to prioritize\nvisual inputs over language instructions, particularly learning tasks with\nrepetitive types of textual instructions. To address the neglect of language\ninstructions, we propose a novel framework that grounds the translation of\nvisual information on instructions for language models. We introduce a mixture\nof visual projectors, each serving as a specialized visual-to-language\ntranslation expert based on the given instruction context to adapt to new\ntasks. To avoid using experts for irrelevant instruction contexts, we propose\nan expert recommendation strategy that reuses experts for tasks similar to\nthose previously learned. Additionally, we introduce expert pruning to\nalleviate interference from the use of experts that cumulatively activated in\nprevious tasks. Extensive experiments on diverse vision-language tasks\ndemonstrate that our method outperforms existing continual learning approaches\nby generating instruction-following responses.", "comment": "Accepted to ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2508.00260v1", "cate": "cs.CV", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2506.17114", "title": "Mathematical Proof as a Litmus Test: Revealing Failure Modes of Advanced Large Reasoning Models", "authors": ["Dadi Guo", "Jiayu Liu", "Zhiyuan Fan", "Zhitao He", "Haoran Li", "Yumeng Wang", "Yi R. Fung"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.17114v3", "summary": "Large reasoning models (e.g., R1, o3) have demonstrated remarkable\nmathematical problem-solving abilities. However, the high reported accuracy of\nthese advanced models on popular datasets, reliance on purely numerical\nevaluation and potential benchmark leakage, often masks their true reasoning\nshortcomings. To address this, we propose leveraging the inherent rigor and\nmethodological complexity of mathematical proofs as a diagnostic tool to expose\nthese hidden failures. Specifically, we introduce the RFMDataset (Reveal\nFailure Modes), a collection of 200 diverse mathematical proof problems, and\nthoroughly evaluate advanced models' performance on it. Our in-depth analysis\nof their failures uncovers 10 fine-grained error types, which shows fundamental\nlimitations in current large reasoning models: 1) large reasoning models\ngrapple profoundly with mathematical proofs, with some generating entirely\ncorrect proofs for less than 20% of problems and failing even on basic ones; 2)\nmodels exhibit a diverse spectrum of reasoning failures, prominently\ndemonstrating the lack of guarantees for the correctness and rigor of\nsingle-step reasoning; and 3) models show hallucination and incompleteness\nduring the reasoning process. Our findings reveal that models' self-reflection\nis insufficient to resolve the current logical dilemmas, necessitating\nformalized and fine-grained logical training.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.17114v3", "cate": "cs.AI", "date": "2025-06-20", "updated": "2025-07-31"}
{"id": "2506.07106", "title": "Theorem-of-Thought: A Multi-Agent Framework for Abductive, Deductive, and Inductive Reasoning in Language Models", "authors": ["Samir Abdaljalil", "Hasan Kurban", "Khalid Qaraqe", "Erchin Serpedin"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      ACL 2025 KnowFM", "url": "http://arxiv.org/abs/2506.07106v2", "summary": "Large language models (LLMs) have shown strong performance across natural\nlanguage reasoning tasks, yet their reasoning processes remain brittle and\ndifficult to interpret. Prompting techniques like Chain-of-Thought (CoT)\nenhance reliability by eliciting intermediate reasoning steps or aggregating\nmultiple outputs. However, they lack mechanisms for enforcing logical structure\nand assessing internal coherence. We introduce Theorem-of-Thought (ToTh), a\nnovel framework that models reasoning as collaboration among three parallel\nagents, each simulating a distinct mode of inference: abductive, deductive, and\ninductive. Each agent produces a reasoning trace, which is structured into a\nformal reasoning graph. To evaluate consistency, we apply Bayesian belief\npropagation guided by natural language inference (NLI), assigning confidence\nscores to each step. The most coherent graph is selected to derive the final\nanswer. Experiments on symbolic (WebOfLies) and numerical (MultiArith)\nreasoning benchmarks show that ToTh consistently outperforms CoT,\nSelf-Consistency, and CoT-Decoding across multiple LLMs, while producing\ninterpretable and logically grounded reasoning chains. Our findings suggest a\npromising direction for building more robust and cognitively inspired LLM\nreasoning. The implementation is available at\nhttps://github.com/KurbanIntelligenceLab/theorem-of-thought.", "comment": "ACL 2025 KnowFM", "pdf_url": "http://arxiv.org/pdf/2506.07106v2", "cate": "cs.CL", "date": "2025-06-08", "updated": "2025-07-31"}
{"id": "2507.23575", "title": "Beyond Gloss: A Hand-Centric Framework for Gloss-Free Sign Language Translation", "authors": ["Sobhan Asasi", "Mohamed Ilyas Lakhal", "Ozge Mercanoglu Sincan", "Richard Bowden"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted at BMVC 2025", "url": "http://arxiv.org/abs/2507.23575v1", "summary": "Sign Language Translation (SLT) is a challenging task that requires bridging\nthe modality gap between visual and linguistic information while capturing\nsubtle variations in hand shapes and movements. To address these challenges, we\nintroduce \\textbf{BeyondGloss}, a novel gloss-free SLT framework that leverages\nthe spatio-temporal reasoning capabilities of Video Large Language Models\n(VideoLLMs). Since existing VideoLLMs struggle to model long videos in detail,\nwe propose a novel approach to generate fine-grained, temporally-aware textual\ndescriptions of hand motion. A contrastive alignment module aligns these\ndescriptions with video features during pre-training, encouraging the model to\nfocus on hand-centric temporal dynamics and distinguish signs more effectively.\nTo further enrich hand-specific representations, we distill fine-grained\nfeatures from HaMeR. Additionally, we apply a contrastive loss between sign\nvideo representations and target language embeddings to reduce the modality gap\nin pre-training. \\textbf{BeyondGloss} achieves state-of-the-art performance on\nthe Phoenix14T and CSL-Daily benchmarks, demonstrating the effectiveness of the\nproposed framework. We will release the code upon acceptance of the paper.", "comment": "Accepted at BMVC 2025", "pdf_url": "http://arxiv.org/pdf/2507.23575v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2501.11842", "title": "Harnessing Rydberg Atomic Receivers: From Quantum Physics to Wireless Communications", "authors": ["Yuanbin Chen", "Xufeng Guo", "Chau Yuen", "Yufei Zhao", "Yong Liang Guan", "Chong Meng Samson See", "Merouane Débbah", "Lajos Hanzo"], "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "Comments:      This revised manuscript has been submitted to IEEE journal, 16 pages, 10 figures", "url": "http://arxiv.org/abs/2501.11842v2", "summary": "The intrinsic integration of Rydberg atomic receivers into wireless\ncommunication systems is proposed, by harnessing the principles of quantum\nphysics in wireless communications. More particularly, we conceive a pair of\nRydberg atomic receivers, one incorporates a local oscillator (LO), referred to\nas an LO-dressed receiver, while the other operates without an LO and is termed\nan LO-free receiver. The appropriate wireless model is developed for each\nconfiguration, elaborating on the receiver's responses to the radio frequency\n(RF) signal, on the potential noise sources, and on the signal-to-noise ratio\n(SNR) performance. The developed wireless model conforms to the classical RF\nframework, facilitating compatibility with established signal processing\nmethodologies. Next, we investigate the associated distortion effects that\nmight occur, specifically identifying the conditions under which distortion\narises and demonstrating the boundaries of linear dynamic ranges. This provides\ncritical insights into its practical implementations in wireless systems.\nFinally, extensive simulation results are provided for characterizing the\nperformance of wireless systems, harnessing this pair of Rydberg atomic\nreceivers. Our results demonstrate that LO-dressed systems achieve a\nsignificant SNR gain of approximately 40~50 dB over conventional RF receivers\nin the standard quantum limit regime. This SNR head-room translates into\nreduced symbol error rates, enabling efficient and reliable transmission with\nhigher-order constellations.", "comment": "This revised manuscript has been submitted to IEEE journal, 16 pages,\n  10 figures", "pdf_url": "http://arxiv.org/pdf/2501.11842v2", "cate": "cs.IT", "date": "2025-01-21", "updated": "2025-07-31"}
{"id": "2508.00632", "title": "Multi-Agent Game Generation and Evaluation via Audio-Visual Recordings", "authors": ["Alexia Jolicoeur-Martineau"], "categories": ["cs.AI", "cs.MA", "cs.MM"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00632v1", "summary": "While AI excels at generating text, audio, images, and videos, creating\ninteractive audio-visual content such as video games remains challenging.\nCurrent LLMs can generate JavaScript games and animations, but lack automated\nevaluation metrics and struggle with complex content that normally requires\nteams of humans working for many months (multi-shot, multi-agents) using assets\nmade by artists. To tackle these issues, we built a new metric and a\nmulti-agent system.\n  We propose AVR-Eval, a relative metric for multimedia content quality using\nAudio-Visual Recordings (AVRs). An omni-modal model (processing text, video,\nand audio) compares the AVRs of two contents, with a text model reviewing\nevaluations to determine superiority. We show that AVR-Eval properly identifies\ngood from broken or mismatched content.\n  We built AVR-Agent, a multi-agent system generating JavaScript code from a\nbank of multimedia assets (audio, images, 3D models). The coding agent selects\nrelevant assets, generates multiple initial codes, uses AVR-Eval to identify\nthe best version, and iteratively improves it through omni-modal agent feedback\nfrom the AVR.\n  We run experiments on games and animations with AVR-Eval (win rate of content\nA against B). We find that content generated by AVR-Agent has a significantly\nhigher win rate against content made through one-shot generation. However,\nmodels struggle to leverage custom assets and AVR feedback effectively, showing\nno higher win rate. This reveals a critical gap: while humans benefit from\nhigh-quality assets and audio-visual feedback, current coding models do not\nseem to utilize these resources as effectively, highlighting fundamental\ndifferences between human and machine content creation approaches.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00632v1", "cate": "cs.AI", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2507.07820", "title": "AI Should Sense Better, Not Just Scale Bigger: Adaptive Sensing as a Paradigm Shift", "authors": ["Eunsu Baek", "Keondo Park", "Jeonggil Ko", "Min-hwan Oh", "Taesik Gong", "Hyung-Sin Kim"], "categories": ["cs.AI", "cs.LG"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07820v2", "summary": "Current AI advances largely rely on scaling neural models and expanding\ntraining datasets to achieve generalization and robustness. Despite notable\nsuccesses, this paradigm incurs significant environmental, economic, and\nethical costs, limiting sustainability and equitable access. Inspired by\nbiological sensory systems, where adaptation occurs dynamically at the input\n(e.g., adjusting pupil size, refocusing vision)--we advocate for adaptive\nsensing as a necessary and foundational shift. Adaptive sensing proactively\nmodulates sensor parameters (e.g., exposure, sensitivity, multimodal\nconfigurations) at the input level, significantly mitigating covariate shifts\nand improving efficiency. Empirical evidence from recent studies demonstrates\nthat adaptive sensing enables small models (e.g., EfficientNet-B0) to surpass\nsubstantially larger models (e.g., OpenCLIP-H) trained with significantly more\ndata and compute. We (i) outline a roadmap for broadly integrating adaptive\nsensing into real-world applications spanning humanoid, healthcare, autonomous\nsystems, agriculture, and environmental monitoring, (ii) critically assess\ntechnical and ethical integration challenges, and (iii) propose targeted\nresearch directions, such as standardized benchmarks, real-time adaptive\nalgorithms, multimodal integration, and privacy-preserving methods.\nCollectively, these efforts aim to transition the AI community toward\nsustainable, robust, and equitable artificial intelligence systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07820v2", "cate": "cs.AI", "date": "2025-07-10", "updated": "2025-07-31"}
{"id": "2506.08184", "title": "Unable to Forget: Proactive Interference Reveals Working Memory Limits in LLMs Beyond Context Length", "authors": ["Chupei Wang", "Jiaqiu Vince Sun"], "categories": ["cs.CL", "cs.AI", "q-bio.NC"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Accepted at ICML 2025 Workshop on Long Context Foundation Models (ICFM). Code: this https URL", "url": "http://arxiv.org/abs/2506.08184v3", "summary": "Information retrieval in Large Language Models (LLMs) is increasingly\nrecognized as intertwined with generation capabilities rather than mere lookup.\nWhile longer contexts are often assumed to improve retrieval, the effects of\nintra-context interference remain understudied. To address this, we adapt the\nproactive interference (PI) paradigm from cognitive science, where earlier\ninformation disrupts recall of newer updates. In humans, susceptibility to such\ninterference is inversely linked to working memory capacity. We introduce\nPI-LLM, an evaluation that sequentially streams semantically related key-value\nupdates and queries only the final values. Although these final values are\nclearly positioned just before the query, LLM retrieval accuracy declines\nlog-linearly toward zero as interference accumulates; errors arise from\nretrieving previously overwritten values. Attempts to mitigate interference via\nprompt engineering (e.g., instructing models to ignore earlier input) yield\nlimited success. These findings reveal a fundamental constraint on LLMs'\nability to disentangle interference and flexibly manipulate information,\nsuggesting a working memory bottleneck beyond mere context access. This calls\nfor approaches that strengthen models' ability to suppress irrelevant content\nduring retrieval.", "comment": "Accepted at ICML 2025 Workshop on Long Context Foundation Models\n  (ICFM). Code: https://github.com/zhuangziGiantfish/Unable-to-Forget", "pdf_url": "http://arxiv.org/pdf/2506.08184v3", "cate": "cs.CL", "date": "2025-06-09", "updated": "2025-07-31"}
{"id": "2507.23595", "title": "MamV2XCalib: V2X-based Target-less Infrastructure Camera Calibration with State Space Model", "authors": ["Yaoye Zhu", "Zhe Wang", "Yan Wang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV25 poster", "url": "http://arxiv.org/abs/2507.23595v1", "summary": "As cooperative systems that leverage roadside cameras to assist autonomous\nvehicle perception become increasingly widespread, large-scale precise\ncalibration of infrastructure cameras has become a critical issue. Traditional\nmanual calibration methods are often time-consuming, labor-intensive, and may\nrequire road closures. This paper proposes MamV2XCalib, the first V2X-based\ninfrastructure camera calibration method with the assistance of vehicle-side\nLiDAR. MamV2XCalib only requires autonomous vehicles equipped with LiDAR to\ndrive near the cameras to be calibrated in the infrastructure, without the need\nfor specific reference objects or manual intervention. We also introduce a new\ntargetless LiDAR-camera calibration method, which combines multi-scale features\nand a 4D correlation volume to estimate the correlation between vehicle-side\npoint clouds and roadside images. We model the temporal information and\nestimate the rotation angles with Mamba, effectively addressing calibration\nfailures in V2X scenarios caused by defects in the vehicle-side data (such as\nocclusions) and large differences in viewpoint. We evaluate MamV2XCalib on the\nV2X-Seq and TUMTraf-V2X real-world datasets, demonstrating the effectiveness\nand robustness of our V2X-based automatic calibration approach. Compared to\nprevious LiDAR-camera methods designed for calibration on one car, our approach\nachieves better and more stable calibration performance in V2X scenarios with\nfewer parameters. The code is available at\nhttps://github.com/zhuyaoye/MamV2XCalib.", "comment": "ICCV25 poster", "pdf_url": "http://arxiv.org/pdf/2507.23595v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2504.05326", "title": "Totally Disjoint 3-Digit Decimal Check Digit Codes", "authors": ["Larry A. Dunning"], "categories": ["cs.IT", "math.CO", "math.IT", "68P30, 94B25, 05B15, 05B40, 20N15", "H.1.1; G.2.1; F.2.1"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.05326v2", "summary": "In 1969 J. Verhoeff provided the first examples of a decimal error detecting\ncode using a single check digit to provide protection against all single,\ntransposition and adjacent twin errors. The three versions of such a code that\nhe presented are length 3-digit codes with 2 information digits. Existence of a\n4-digit code would imply the existence of 10 such disjoint 3-digit codes. This\npaper presents 3 pairwise disjoint 3-digit codes. The codes developed herein,\nhave the property that the knowledge of the multiset of digits included in a\nword is sufficient to determine the entire codeword even though their positions\nwere unknown. Thus the codes are permutation-free, and this fulfills Verhoeff's\ndesire to eliminate \"cyclic errors\". Phonetic errors, where 2 digit pairs of\nthe forms X0 and 1X are interchanged, are also eliminated.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.05326v2", "cate": "cs.IT", "date": "2025-03-26", "updated": "2025-07-31"}
{"id": "2508.00733", "title": "AudioGen-Omni: A Unified Multimodal Diffusion Transformer for Video-Synchronized Audio, Speech, and Song Generation", "authors": ["Le Wang", "Jun Wang", "Feng Deng", "Chen Zhang", "Kun Gai", "Di Zhang"], "categories": ["cs.SD", "cs.CV", "cs.MM", "eess.AS"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "Comments:      12 pages, 2 figures", "url": "http://arxiv.org/abs/2508.00733v1", "summary": "We present AudioGen-Omni - a unified approach based on multimodal diffusion\ntransformers (MMDit), capable of generating high-fidelity audio, speech, and\nsongs coherently synchronized with the input video. AudioGen-Omni introduces a\nnovel joint training paradigm that seamlessly integrates large-scale\nvideo-text-audio corpora, enabling a model capable of generating semantically\nrich, acoustically diverse audio conditioned on multimodal inputs and adaptable\nto a wide range of audio generation tasks. AudioGen-Omni employs a unified\nlyrics-transcription encoder that encodes graphemes and phonemes from both sung\nand spoken inputs into dense frame-level representations. Dense frame-level\nrepresentations are fused using an AdaLN-based joint attention mechanism\nenhanced with phase-aligned anisotropic positional infusion (PAAPI), wherein\nRoPE is selectively applied to temporally structured modalities to ensure\nprecise and robust cross-modal alignment. By unfreezing all modalities and\nmasking missing inputs, AudioGen-Omni mitigates the semantic constraints of\ntext-frozen paradigms, enabling effective cross-modal conditioning. This joint\ntraining approach enhances audio quality, semantic alignment, and lip-sync\naccuracy, while also achieving state-of-the-art results on\nText-to-Audio/Speech/Song tasks. With an inference time of 1.91 seconds for 8\nseconds of audio, it offers substantial improvements in both efficiency and\ngenerality.", "comment": "12 pages, 2 figures", "pdf_url": "http://arxiv.org/pdf/2508.00733v1", "cate": "cs.SD", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00015", "title": "Extended Abstract: Partial-encapsulate and Its Support for Floating-point Operations in ACL2", "authors": ["Matt Kaufmann", "J Strother Moore"], "categories": ["cs.LO", "cs.MS"], "primary_category": "Subjects:       Logic in Computer Science (cs.LO)", "pdf_link": null, "comments": "Comments:      In Proceedings ACL2 2025, arXiv:2507.18567", "url": "http://arxiv.org/abs/2508.00015v1", "summary": "We illustrate the power of partial-encapsulate, showing how it is used in the\nimplementation of floating-point operations in ACL2.", "comment": "In Proceedings ACL2 2025, arXiv:2507.18567", "pdf_url": "http://arxiv.org/pdf/2508.00015v1", "cate": "cs.LO", "date": "2025-07-25", "updated": "2025-07-25"}
{"id": "2507.21872", "title": "MultiEditor: Controllable Multimodal Object Editing for Driving Scenarios Using 3D Gaussian Splatting Priors", "authors": ["Shouyi Lu", "Zihan Lin", "Chao Lu", "Huanran Wang", "Guirong Zhuo", "Lianqing Zheng"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21872v3", "summary": "Autonomous driving systems rely heavily on multimodal perception data to\nunderstand complex environments. However, the long-tailed distribution of\nreal-world data hinders generalization, especially for rare but safety-critical\nvehicle categories. To address this challenge, we propose MultiEditor, a\ndual-branch latent diffusion framework designed to edit images and LiDAR point\nclouds in driving scenarios jointly. At the core of our approach is introducing\n3D Gaussian Splatting (3DGS) as a structural and appearance prior for target\nobjects. Leveraging this prior, we design a multi-level appearance control\nmechanism--comprising pixel-level pasting, semantic-level guidance, and\nmulti-branch refinement--to achieve high-fidelity reconstruction across\nmodalities. We further propose a depth-guided deformable cross-modality\ncondition module that adaptively enables mutual guidance between modalities\nusing 3DGS-rendered depth, significantly enhancing cross-modality consistency.\nExtensive experiments demonstrate that MultiEditor achieves superior\nperformance in visual and geometric fidelity, editing controllability, and\ncross-modality consistency. Furthermore, generating rare-category vehicle data\nwith MultiEditor substantially enhances the detection accuracy of perception\nmodels on underrepresented classes.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21872v3", "cate": "cs.AI", "date": "2025-07-29", "updated": "2025-07-31"}
{"id": "2506.21875", "title": "WildSpeech-Bench: Benchmarking Audio LLMs in Natural Speech Conversation", "authors": ["Jian Zhang", "Linhao Zhang", "Bokai Lei", "Chuhan Wu", "Wei Jia", "Xiao Zhou"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.21875v2", "summary": "Recent multi-modal Large Language Models (LLMs) such as GPT-4o have\ndemonstrated strong capabilities of direct speech interaction. However, the\nlack of specialized and comprehensive benchmarks for end-to-end speech LLM\nevaluation hinders optimizing the user experience of Audio LLMs in real-world\napplications. Existing evaluation methods often adapt text-based benchmarks,\noverlooking speech's unique characteristics and challenges, including prosody,\nhomophones, stuttering, and differing user expectations. Here, we present a\nnovel approach to thoroughly evaluate LLMs in practical speech conversations.\nWe systematically curate real-world chat data relevant to spoken scenarios,\nintroduce diversity in speaker attributes and acoustic conditions, and augment\nthe dataset with speech-specific phenomena. We further design a query-aware\nevaluation method to use customized evaluation checklists and prompts to\nenhance the accuracy of automatic evaluation. We conduct comprehensive testing\nand detailed analysis of various mainstream speech models, revealing\nsignificant differences in model performance across different speech scenarios.\nThe use of query-aware evaluation further enables a finer-grained assessment\nunder various speech-specific scenarios. Our benchmark can provide valuable\ninsights for speech model development and evaluation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21875v2", "cate": "cs.CL", "date": "2025-06-27", "updated": "2025-07-31"}
{"id": "2507.23597", "title": "MoGA: 3D Generative Avatar Prior for Monocular Gaussian Avatar Reconstruction", "authors": ["Zijian Dong", "Longteng Duan", "Jie Song", "Michael J. Black", "Andreas Geiger"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV 2025 (Highlight), Project Page: this https URL", "url": "http://arxiv.org/abs/2507.23597v1", "summary": "We present MoGA, a novel method to reconstruct high-fidelity 3D Gaussian\navatars from a single-view image. The main challenge lies in inferring unseen\nappearance and geometric details while ensuring 3D consistency and realism.\nMost previous methods rely on 2D diffusion models to synthesize unseen views;\nhowever, these generated views are sparse and inconsistent, resulting in\nunrealistic 3D artifacts and blurred appearance. To address these limitations,\nwe leverage a generative avatar model, that can generate diverse 3D avatars by\nsampling deformed Gaussians from a learned prior distribution. Due to the\nlimited amount of 3D training data such a 3D model alone cannot capture all\nimage details of unseen identities. Consequently, we integrate it as a prior,\nensuring 3D consistency by projecting input images into its latent space and\nenforcing additional 3D appearance and geometric constraints. Our novel\napproach formulates Gaussian avatar creation as a model inversion process by\nfitting the generative avatar to synthetic views from 2D diffusion models. The\ngenerative avatar provides a meaningful initialization for model fitting,\nenforces 3D regularization, and helps in refining pose estimation. Experiments\nshow that our method surpasses state-of-the-art techniques and generalizes well\nto real-world scenarios. Our Gaussian avatars are also inherently animatable", "comment": "ICCV 2025 (Highlight), Project Page: https://zj-dong.github.io/MoGA/", "pdf_url": "http://arxiv.org/pdf/2507.23597v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2412.19792", "title": "InfAlign: Inference-aware language model alignment", "authors": ["Ananth Balashankar", "Ziteng Sun", "Jonathan Berant", "Jacob Eisenstein", "Michael Collins", "Adrian Hutter", "Jong Lee", "Chirag Nagpal", "Flavien Prost", "Aradhana Sinha", "Ananda Theertha Suresh", "Ahmad Beirami"], "categories": ["cs.LG", "cs.CL", "cs.IT", "math.IT"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2412.19792v4", "summary": "Language model alignment is a critical step in training modern generative\nlanguage models. Alignment targets to improve win rate of a sample from the\naligned model against the base model. Today, we are increasingly using\ninference-time algorithms (e.g., Best-of-N, controlled decoding, tree search)\nto decode from language models rather than standard sampling. We show that this\ntrain/test mismatch makes standard RLHF framework sub-optimal in view of such\ninference-time methods. To this end, we propose a framework for inference-aware\nalignment (InfAlign), which aims to optimize inference-time win rate of the\naligned policy against the base model. We prove that for any inference-time\ndecoding procedure, the optimal aligned policy is the solution to the standard\nRLHF problem with a transformation of the reward. This motivates us to provide\nthe calibrate-and-transform RL (InfAlign-CTRL) algorithm to solve this problem,\nwhich involves a reward calibration step and a KL-regularized reward\nmaximization step with a transformation of the calibrated reward. For best-of-N\nsampling and best-of-N jailbreaking, we propose specific transformations\noffering up to 3-8% improvement on inference-time win rates. Finally, we also\nshow that our proposed reward calibration method is a strong baseline for\noptimizing standard win rate.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2412.19792v4", "cate": "cs.LG", "date": "2024-12-27", "updated": "2025-07-31"}
{"id": "2508.00748", "title": "Is It Really You? Exploring Biometric Verification Scenarios in Photorealistic Talking-Head Avatar Videos", "authors": ["Laura Pedrouzo-Rodriguez", "Pedro Delgado-DeRobles", "Luis F. Gomez", "Ruben Tolosana", "Ruben Vera-Rodriguez", "Aythami Morales", "Julian Fierrez"], "categories": ["cs.CV", "cs.AI", "cs.CR", "cs.MM"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted at the IEEE International Joint Conference on Biometrics (IJCB 2025)", "url": "http://arxiv.org/abs/2508.00748v1", "summary": "Photorealistic talking-head avatars are becoming increasingly common in\nvirtual meetings, gaming, and social platforms. These avatars allow for more\nimmersive communication, but they also introduce serious security risks. One\nemerging threat is impersonation: an attacker can steal a user's\navatar-preserving their appearance and voice-making it nearly impossible to\ndetect its fraudulent usage by sight or sound alone. In this paper, we explore\nthe challenge of biometric verification in such avatar-mediated scenarios. Our\nmain question is whether an individual's facial motion patterns can serve as\nreliable behavioral biometrics to verify their identity when the avatar's\nvisual appearance is a facsimile of its owner. To answer this question, we\nintroduce a new dataset of realistic avatar videos created using a\nstate-of-the-art one-shot avatar generation model, GAGAvatar, with genuine and\nimpostor avatar videos. We also propose a lightweight, explainable\nspatio-temporal Graph Convolutional Network architecture with temporal\nattention pooling, that uses only facial landmarks to model dynamic facial\ngestures. Experimental results demonstrate that facial motion cues enable\nmeaningful identity verification with AUC values approaching 80%. The proposed\nbenchmark and biometric system are available for the research community in\norder to bring attention to the urgent need for more advanced behavioral\nbiometric defenses in avatar-based communication systems.", "comment": "Accepted at the IEEE International Joint Conference on Biometrics\n  (IJCB 2025)", "pdf_url": "http://arxiv.org/pdf/2508.00748v1", "cate": "cs.CV", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00269", "title": "chipfiring: A Python Package for Efficient Mathematical Analysis of Chip-Firing Games on Multigraphs", "authors": ["Dhyey Dharmendrakumar Mavani", "Tairan Ji", "Nathan Pflueger"], "categories": ["math.CO", "cs.CG", "cs.MS", "math.AG"], "primary_category": "Subjects:       Combinatorics (math.CO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00269v1", "summary": "This paper presents `chipfiring`, a comprehensive Python package for the\nmathematical analysis of chip-firing games on finite graphs. The package\nprovides a robust toolkit for defining graphs and chip configurations\n(divisors), performing chip-firing operations, and analyzing fundamental\nproperties such as winnability, linear equivalence, and divisor rank. We detail\nthe core components of the library, including its object-oriented graph and\ndivisor implementations, integrated Laplacian matrix computations, and an\nefficient implementation of Dhar's algorithm for determining the solvability of\nthe dollar game. The `chipfiring` package is designed for researchers and\nstudents in graph theory, combinatorics, and algebraic geometry, providing\nessential algorithms and data structures for exploring these rich mathematical\nmodels. We describe the library's architecture, illustrate its usage with\ncomprehensive examples, and highlight its specialized contributions compared to\ngeneral-purpose graph libraries.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00269v1", "cate": "math.CO", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2507.21875", "title": "Tiny-BioMoE: a Lightweight Embedding Model for Biosignal Analysis", "authors": ["Stefanos Gkikas", "Ioannis Kyprakis", "Manolis Tsiknakis"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21875v3", "summary": "Pain is a complex and pervasive condition that affects a significant portion\nof the population. Accurate and consistent assessment is essential for\nindividuals suffering from pain, as well as for developing effective management\nstrategies in a healthcare system. Automatic pain assessment systems enable\ncontinuous monitoring, support clinical decision-making, and help minimize\npatient distress while mitigating the risk of functional deterioration.\nLeveraging physiological signals offers objective and precise insights into a\nperson's state, and their integration in a multimodal framework can further\nenhance system performance. This study has been submitted to the \\textit{Second\nMultimodal Sensing Grand Challenge for Next-Gen Pain Assessment (AI4PAIN)}. The\nproposed approach introduces \\textit{Tiny-BioMoE}, a lightweight pretrained\nembedding model for biosignal analysis. Trained on $4.4$ million biosignal\nimage representations and consisting of only $7.3$ million parameters, it\nserves as an effective tool for extracting high-quality embeddings for\ndownstream tasks. Extensive experiments involving electrodermal activity, blood\nvolume pulse, respiratory signals, peripheral oxygen saturation, and their\ncombinations highlight the model's effectiveness across diverse modalities in\nautomatic pain recognition tasks. \\textit{\\textcolor{blue}{The model's\narchitecture (code) and weights are available at\nhttps://github.com/GkikasStefanos/Tiny-BioMoE.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21875v3", "cate": "cs.AI", "date": "2025-07-29", "updated": "2025-07-31"}
{"id": "2507.06448", "title": "Perception-Aware Policy Optimization for Multimodal Reasoning", "authors": ["Zhenhailong Wang", "Xuehang Guo", "Sofia Stoica", "Haiyang Xu", "Hongru Wang", "Hyeonjeong Ha", "Xiusi Chen", "Yangyi Chen", "Ming Yan", "Fei Huang", "Heng Ji"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.06448v3", "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has proven to be a\nhighly effective strategy for endowing Large Language Models (LLMs) with robust\nmulti-step reasoning abilities. However, its design and optimizations remain\ntailored to purely textual domains, resulting in suboptimal performance when\napplied to multimodal reasoning tasks. In particular, we observe that a major\nsource of error in current multimodal reasoning lies in the perception of\nvisual inputs. To address this bottleneck, we propose PAPO, a novel policy\ngradient algorithm that encourages the model to learn to perceive while\nlearning to reason. Specifically, we introduce the Implicit Perception Loss in\nthe form of a KL divergence term, which can be seamlessly plugged into\nmainstream RLVR algorithms such as GRPO and DAPO. Notably, PAPO does not rely\non additional data curation, reward models, or stronger teacher models. To\nfurther enhance the training stability of PAPO, we introduce the Double Entropy\nLoss, which effectively regularizes the new KL objective without compromising\nperformance. Despite its simplicity, PAPO yields significant overall\nimprovements of 4.4%-17.5% on diverse multimodal benchmarks. The improvements\nare more pronounced, approaching 8.0%-19.1%, on tasks with high vision\ndependency. We also observe a substantial reduction of 30.5% in perception\nerrors, indicating improved perceptual capabilities with PAPO. Overall, our\nwork introduces a deeper integration of perception-aware supervision into core\nlearning objectives and lays the groundwork for a new RL framework that\nencourages visually grounded reasoning. Code and data will be made publicly\navailable for research purposes. Project page:\nhttps://mikewangwzhl.github.io/PAPO.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.06448v3", "cate": "cs.CL", "date": "2025-07-08", "updated": "2025-07-31"}
{"id": "2507.23599", "title": "DA-Occ: Efficient 3D Voxel Occupancy Prediction via Directional 2D for Geometric Structure Preservation", "authors": ["Yuchen Zhou", "Yan Luo", "Xiangang Wang", "Xingjian Gu", "Mingzhou Lu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23599v1", "summary": "Efficient and high-accuracy 3D occupancy prediction is crucial for ensuring\nthe performance of autonomous driving (AD) systems. However, many current\nmethods focus on high accuracy at the expense of real-time processing needs. To\naddress this challenge of balancing accuracy and inference speed, we propose a\ndirectional pure 2D approach. Our method involves slicing 3D voxel features to\npreserve complete vertical geometric information. This strategy compensates for\nthe loss of height cues in Bird's-Eye View (BEV) representations, thereby\nmaintaining the integrity of the 3D geometric structure. By employing a\ndirectional attention mechanism, we efficiently extract geometric features from\ndifferent orientations, striking a balance between accuracy and computational\nefficiency. Experimental results highlight the significant advantages of our\napproach for autonomous driving. On the Occ3D-nuScenes, the proposed method\nachieves an mIoU of 39.3% and an inference speed of 27.7 FPS, effectively\nbalancing accuracy and efficiency. In simulations on edge devices, the\ninference speed reaches 14.8 FPS, further demonstrating the method's\napplicability for real-time deployment in resource-constrained environments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23599v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2505.10933", "title": "Cross-layer Integrated Sensing and Communication: A Joint Industrial and Academic Perspective", "authors": ["Henk Wymeersch", "Nuutti Tervo", "Stefan Wänstedt", "Sharief Saleh", "Joerg Ahlendorf", "Ozgur Akgul", "Vasileios Tsekenis", "Sokratis Barmpounakis", "Liping Bai", "Martin Beale", "Rafael Berkvens", "Nabeel Nisar Bhat", "Hui Chen", "Shrayan Das", "Claude Desset", "Antonio de la Oliva", "Prajnamaya Dass", "Jeroen Famaey", "Hamed Farhadi", "Gerhard P. Fettweis", "Yu Ge", "Hao Guo", "Rreze Halili", "Katsuyuki Haneda", "Abdur Rahman Mohamed Ismail", "Akshay Jain", "Sylvaine Kerboeuf", "Musa Furkan Keskin", "Emad Ibrahim", "Bilal Khan", "Siddhartha Kumar", "Stefan Köpsell", "Apostolos Kousaridas", "Pekka Kyösti", "Simon Lindberg", "Mohammad Hossein Moghaddam", "Ahmad Nimr", "Victor Pettersson", "Aarno Pärssinen", "Basuki Priyanto", "Athanasios Stavridis", "Tommy Svensson", "Sonika Ujjwal"], "categories": ["eess.SP", "cs.IT", "math.IT"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.10933v2", "summary": "Integrated sensing and communication (ISAC) enables radio systems to\nsimultaneously sense and communicate with their environment. This paper,\ndeveloped within the Hexa-X-II project funded by the European Union, presents a\ncomprehensive cross-layer vision for ISAC in 6G networks, integrating insights\nfrom physical-layer design, hardware architectures, AI-driven intelligence, and\nprotocol-level innovations. We begin by revisiting the foundational principles\nof ISAC, highlighting synergies and trade-offs between sensing and\ncommunication across different integration levels. Enabling technologies (such\nas multiband operation, massive and distributed MIMO, non-terrestrial networks,\nreconfigurable intelligent surfaces, and machine learning) are analyzed in\nconjunction with hardware considerations including waveform design,\nsynchronization, and full-duplex operation. To bridge implementation and\nsystem-level evaluation, we introduce a quantitative cross-layer framework\nlinking design parameters to key performance and value indicators. By\nsynthesizing perspectives from both academia and industry, this paper outlines\nhow deeply integrated ISAC can transform 6G into a programmable and\ncontext-aware platform supporting applications from reliable wireless access to\nautonomous mobility and digital twinning.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.10933v2", "cate": "eess.SP", "date": "2025-05-16", "updated": "2025-07-31"}
{"id": "2508.00782", "title": "SpA2V: Harnessing Spatial Auditory Cues for Audio-driven Spatially-aware Video Generation", "authors": ["Kien T. Pham", "Yingqing He", "Yazhou Xing", "Qifeng Chen", "Long Chen"], "categories": ["cs.GR", "cs.AI", "cs.CV", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "Subjects:       Graphics (cs.GR)", "pdf_link": null, "comments": "Comments:      The 33rd ACM Multimedia Conference (MM '25)", "url": "http://arxiv.org/abs/2508.00782v1", "summary": "Audio-driven video generation aims to synthesize realistic videos that align\nwith input audio recordings, akin to the human ability to visualize scenes from\nauditory input. However, existing approaches predominantly focus on exploring\nsemantic information, such as the classes of sounding sources present in the\naudio, limiting their ability to generate videos with accurate content and\nspatial composition. In contrast, we humans can not only naturally identify the\nsemantic categories of sounding sources but also determine their deeply encoded\nspatial attributes, including locations and movement directions. This useful\ninformation can be elucidated by considering specific spatial indicators\nderived from the inherent physical properties of sound, such as loudness or\nfrequency. As prior methods largely ignore this factor, we present SpA2V, the\nfirst framework explicitly exploits these spatial auditory cues from audios to\ngenerate videos with high semantic and spatial correspondence. SpA2V decomposes\nthe generation process into two stages: 1) Audio-guided Video Planning: We\nmeticulously adapt a state-of-the-art MLLM for a novel task of harnessing\nspatial and semantic cues from input audio to construct Video Scene Layouts\n(VSLs). This serves as an intermediate representation to bridge the gap between\nthe audio and video modalities. 2) Layout-grounded Video Generation: We develop\nan efficient and effective approach to seamlessly integrate VSLs as conditional\nguidance into pre-trained diffusion models, enabling VSL-grounded video\ngeneration in a training-free manner. Extensive experiments demonstrate that\nSpA2V excels in generating realistic videos with semantic and spatial alignment\nto the input audios.", "comment": "The 33rd ACM Multimedia Conference (MM '25)", "pdf_url": "http://arxiv.org/pdf/2508.00782v1", "cate": "cs.GR", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00441", "title": "DGEMM without FP64 Arithmetic -- using FP64 Emulation and FP8 Tensor Cores with Ozaki Scheme", "authors": ["Daichi Mukunoki"], "categories": ["cs.PF", "cs.AR", "cs.MS"], "primary_category": "Subjects:       Performance (cs.PF)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00441v1", "summary": "Since AI computations require low-precision matrix multiplications,\nprocessors with enhanced performance for these operations are increasing along\nwith the growing demand for AI computations. However, it is difficult to use\nthese operations directly for scientific computations. The Ozaki scheme, an\naccurate matrix multiplication method proposed by Ozaki et al. in 2012, enables\nFP64 matrix multiplication (DGEMM) using low-precision floating-point\noperations such as FP16. The method was subsequently extended to utilize\ninteger arithmetic. The use of integer operations reduces computational cost\ncompared to the floating-point based approach. It has also demonstrated higher\nperformance than hardware FP64 operations on GPUs with fast INT8 Tensor Cores\nfor AI workloads. However, the latest hardware tends to enhance low-precision\nfloating-point operation performance such as FP8 instead of INT8. This study\nrevisits the utilization of low-precision floating-point operations in the\nOzaki scheme, considering the latest AI hardware. Specifically, we consider the\nuse of FP6 and FP8 Tensor Cores. Moreover, for processors that support very\nslow FP64 operations or do not support them at all, we consider the use of the\nFP64 emulation based on integer arithmetic. We also examine a new blocking\nstrategy. We demonstrate the effectiveness of these methods by evaluating the\nperformance of DGEMM using FP8 Tensor Cores and FP64 emulation on a Blackwell\narchitecture GPU.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00441v1", "cate": "cs.PF", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2507.21881", "title": "Multi-Representation Diagrams for Pain Recognition: Integrating Various Electrodermal Activity Signals into a Single Image", "authors": ["Stefanos Gkikas", "Ioannis Kyprakis", "Manolis Tsiknakis"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      arXiv admin note: text overlap with arXiv:2507.21875", "url": "http://arxiv.org/abs/2507.21881v3", "summary": "Pain is a multifaceted phenomenon that affects a substantial portion of the\npopulation. Reliable and consistent evaluation benefits those experiencing pain\nand underpins the development of effective and advanced management strategies.\nAutomatic pain-assessment systems deliver continuous monitoring, inform\nclinical decision-making, and aim to reduce distress while preventing\nfunctional decline. By incorporating physiological signals, these systems\nprovide objective, accurate insights into an individual's condition. This study\nhas been submitted to the \\textit{Second Multimodal Sensing Grand Challenge for\nNext-Gen Pain Assessment (AI4PAIN)}. The proposed method introduces a pipeline\nthat leverages electrodermal activity signals as input modality. Multiple\nrepresentations of the signal are created and visualized as waveforms, and they\nare jointly visualized within a single multi-representation diagram. Extensive\nexperiments incorporating various processing and filtering techniques, along\nwith multiple representation combinations, demonstrate the effectiveness of the\nproposed approach. It consistently yields comparable, and in several cases\nsuperior, results to traditional fusion methods, establishing it as a robust\nalternative for integrating different signal representations or modalities.", "comment": "arXiv admin note: text overlap with arXiv:2507.21875", "pdf_url": "http://arxiv.org/pdf/2507.21881v3", "cate": "cs.AI", "date": "2025-07-29", "updated": "2025-07-31"}
{"id": "2507.07695", "title": "KeyKnowledgeRAG (K^2RAG): An Enhanced RAG method for improved LLM question-answering capabilities", "authors": ["Hruday Markondapatnaikuni", "Basem Suleiman", "Abdelkarim Erradi", "Shijing Chen"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      21 pages, 14 figures", "url": "http://arxiv.org/abs/2507.07695v2", "summary": "Fine-tuning is an immensely resource-intensive process when retraining Large\nLanguage Models (LLMs) to incorporate a larger body of knowledge. Although many\nfine-tuning techniques have been developed to reduce the time and computational\ncost involved, the challenge persists as LLMs continue to grow in size and\ncomplexity. To address this, a new approach to knowledge expansion in LLMs is\nneeded. Retrieval-Augmented Generation (RAG) offers one such alternative by\nstoring external knowledge in a database and retrieving relevant chunks to\nsupport question answering. However, naive implementations of RAG face\nsignificant limitations in scalability and answer accuracy. This paper\nintroduces KeyKnowledgeRAG (K2RAG), a novel framework designed to overcome\nthese limitations. Inspired by the divide-and-conquer paradigm, K2RAG\nintegrates dense and sparse vector search, knowledge graphs, and text\nsummarization to improve retrieval quality and system efficiency. The framework\nalso includes a preprocessing step that summarizes the training data,\nsignificantly reducing the training time. K2RAG was evaluated using the\nMultiHopRAG dataset, where the proposed pipeline was trained on the document\ncorpus and tested on a separate evaluation set. Results demonstrated notable\nimprovements over common naive RAG implementations. K2RAG achieved the highest\nmean answer similarity score of 0.57, and reached the highest third quartile\n(Q3) similarity of 0.82, indicating better alignment with ground-truth answers.\nIn addition to improved accuracy, the framework proved highly efficient. The\nsummarization step reduced the average training time of individual components\nby 93%, and execution speed was up to 40% faster than traditional knowledge\ngraph-based RAG systems. K2RAG also demonstrated superior scalability,\nrequiring three times less VRAM than several naive RAG implementations tested\nin this study.", "comment": "21 pages, 14 figures", "pdf_url": "http://arxiv.org/pdf/2507.07695v2", "cate": "cs.CL", "date": "2025-07-10", "updated": "2025-07-31"}
{"id": "2507.23601", "title": "Mamba-based Efficient Spatio-Frequency Motion Perception for Video Camouflaged Object Detection", "authors": ["Xin Li", "Keren Fu", "Qijun Zhao"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      11 pages, 11 figures", "url": "http://arxiv.org/abs/2507.23601v1", "summary": "Existing video camouflaged object detection (VCOD) methods primarily rely on\nspatial appearance features to perceive motion cues for breaking camouflage.\nHowever, the high similarity between foreground and background in VCOD results\nin limited discriminability of spatial appearance features (e.g., color and\ntexture), restricting detection accuracy and completeness. Recent studies\ndemonstrate that frequency features can not only enhance feature representation\nto compensate for appearance limitations but also perceive motion through\ndynamic variations in frequency energy. Furthermore, the emerging state space\nmodel called Mamba, enables efficient perception of motion cues in frame\nsequences due to its linear-time long-sequence modeling capability. Motivated\nby this, we propose a novel visual camouflage Mamba (Vcamba) based on\nspatio-frequency motion perception that integrates frequency and spatial\nfeatures for efficient and accurate VCOD. Specifically, we propose a receptive\nfield visual state space (RFVSS) module to extract multi-scale spatial features\nafter sequence modeling. For frequency learning, we introduce an adaptive\nfrequency component enhancement (AFE) module with a novel frequency-domain\nsequential scanning strategy to maintain semantic consistency. Then we propose\na space-based long-range motion perception (SLMP) module and a frequency-based\nlong-range motion perception (FLMP) module to model spatio-temporal and\nfrequency-temporal sequences in spatial and frequency phase domains. Finally,\nthe space and frequency motion fusion module (SFMF) integrates dual-domain\nfeatures for unified motion representation. Experimental results show that our\nVcamba outperforms state-of-the-art methods across 6 evaluation metrics on 2\ndatasets with lower computation cost, confirming the superiority of Vcamba. Our\ncode is available at: https://github.com/BoydeLi/Vcamba.", "comment": "11 pages, 11 figures", "pdf_url": "http://arxiv.org/pdf/2507.23601v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2502.05695", "title": "Semantic-Aware Adaptive Video Streaming Using Latent Diffusion Models for Wireless Networks", "authors": ["Zijiang Yan", "Jianhua Pei", "Hongda Wu", "Hina Tabassum", "Ping Wang"], "categories": ["cs.MM", "cs.AI", "cs.CV", "cs.LG", "eess.IV"], "primary_category": "Subjects:       Multimedia (cs.MM)", "pdf_link": null, "comments": "Comments:      Accepted in IEEE Wireless Communications", "url": "http://arxiv.org/abs/2502.05695v3", "summary": "This paper proposes a novel Semantic Communication (SemCom) framework for\nreal-time adaptive-bitrate video streaming by integrating Latent Diffusion\nModels (LDMs) within the FFmpeg techniques. This solution addresses the\nchallenges of high bandwidth usage, storage inefficiencies, and quality of\nexperience (QoE) degradation associated with traditional Constant Bitrate\nStreaming (CBS) and Adaptive Bitrate Streaming (ABS). The proposed approach\nleverages LDMs to compress I-frames into a latent space, offering significant\nstorage and semantic transmission savings without sacrificing high visual\nquality. While retaining B-frames and P-frames as adjustment metadata to\nsupport efficient refinement of video reconstruction at the user side, the\nproposed framework further incorporates state-of-the-art denoising and Video\nFrame Interpolation (VFI) techniques. These techniques mitigate semantic\nambiguity and restore temporal coherence between frames, even in noisy wireless\ncommunication environments. Experimental results demonstrate the proposed\nmethod achieves high-quality video streaming with optimized bandwidth usage,\noutperforming state-of-the-art solutions in terms of QoE and resource\nefficiency. This work opens new possibilities for scalable real-time video\nstreaming in 5G and future post-5G networks.", "comment": "Accepted in IEEE Wireless Communications", "pdf_url": "http://arxiv.org/pdf/2502.05695v3", "cate": "cs.MM", "date": "2025-02-08", "updated": "2025-08-01"}
{"id": "2507.21886", "title": "Efficient Pain Recognition via Respiration Signals: A Single Cross-Attention Transformer Multi-Window Fusion Pipeline", "authors": ["Stefanos Gkikas", "Ioannis Kyprakis", "Manolis Tsiknakis"], "categories": ["cs.AI", "cs.LG", "eess.SP"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      arXiv admin note: text overlap with arXiv:2507.21881 , arXiv:2507.21875", "url": "http://arxiv.org/abs/2507.21886v3", "summary": "Pain is a complex condition affecting a large portion of the population.\nAccurate and consistent evaluation is essential for individuals experiencing\npain, and it supports the development of effective and advanced management\nstrategies. Automatic pain assessment systems provide continuous monitoring and\nsupport clinical decision-making, aiming to reduce distress and prevent\nfunctional decline. This study has been submitted to the \\textit{Second\nMultimodal Sensing Grand Challenge for Next-Gen Pain Assessment (AI4PAIN)}. The\nproposed method introduces a pipeline that leverages respiration as the input\nsignal and incorporates a highly efficient cross-attention transformer\nalongside a multi-windowing strategy. Extensive experiments demonstrate that\nrespiration is a valuable physiological modality for pain assessment. Moreover,\nexperiments revealed that compact and efficient models, when properly\noptimized, can achieve strong performance, often surpassing larger\ncounterparts. The proposed multi-window approach effectively captures both\nshort-term and long-term features, as well as global characteristics, thereby\nenhancing the model's representational capacity.", "comment": "arXiv admin note: text overlap with arXiv:2507.21881,\n  arXiv:2507.21875", "pdf_url": "http://arxiv.org/pdf/2507.21886v3", "cate": "cs.AI", "date": "2025-07-29", "updated": "2025-07-31"}
{"id": "2507.08606", "title": "DocPolarBERT: A Pre-trained Model for Document Understanding with Relative Polar Coordinate Encoding of Layout Structures", "authors": ["Benno Uthayasooriyar", "Antoine Ly", "Franck Vermet", "Caio Corro"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.08606v3", "summary": "We introduce DocPolarBERT, a layout-aware BERT model for document\nunderstanding that eliminates the need for absolute 2D positional embeddings.\nWe extend self-attention to take into account text block positions in relative\npolar coordinate system rather than the Cartesian one. Despite being\npre-trained on a dataset more than six times smaller than the widely used\nIIT-CDIP corpus, DocPolarBERT achieves state-of-the-art results. These results\ndemonstrate that a carefully designed attention mechanism can compensate for\nreduced pre-training data, offering an efficient and effective alternative for\ndocument understanding.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.08606v3", "cate": "cs.CL", "date": "2025-07-11", "updated": "2025-07-31"}
{"id": "2507.23620", "title": "DivControl: Knowledge Diversion for Controllable Image Generation", "authors": ["Yucheng Xie", "Fu Feng", "Ruixiao Shi", "Jing Wang", "Yong Rui", "Xin Geng"], "categories": ["cs.CV", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23620v1", "summary": "Diffusion models have advanced from text-to-image (T2I) to image-to-image\n(I2I) generation by incorporating structured inputs such as depth maps,\nenabling fine-grained spatial control. However, existing methods either train\nseparate models for each condition or rely on unified architectures with\nentangled representations, resulting in poor generalization and high adaptation\ncosts for novel conditions. To this end, we propose DivControl, a decomposable\npretraining framework for unified controllable generation and efficient\nadaptation. DivControl factorizes ControlNet via SVD into basic\ncomponents-pairs of singular vectors-which are disentangled into\ncondition-agnostic learngenes and condition-specific tailors through knowledge\ndiversion during multi-condition training. Knowledge diversion is implemented\nvia a dynamic gate that performs soft routing over tailors based on the\nsemantics of condition instructions, enabling zero-shot generalization and\nparameter-efficient adaptation to novel conditions. To further improve\ncondition fidelity and training efficiency, we introduce a representation\nalignment loss that aligns condition embeddings with early diffusion features.\nExtensive experiments demonstrate that DivControl achieves state-of-the-art\ncontrollability with 36.4$\\times$ less training cost, while simultaneously\nimproving average performance on basic conditions. It also delivers strong\nzero-shot and few-shot performance on unseen conditions, demonstrating superior\nscalability, modularity, and transferability.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23620v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2412.01064", "title": "FLOAT: Generative Motion Latent Flow Matching for Audio-driven Talking Portrait", "authors": ["Taekyung Ki", "Dongchan Min", "Gyeongsu Chae"], "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.MM", "eess.IV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV 2025. Project page: this https URL", "url": "http://arxiv.org/abs/2412.01064v4", "summary": "With the rapid advancement of diffusion-based generative models, portrait\nimage animation has achieved remarkable results. However, it still faces\nchallenges in temporally consistent video generation and fast sampling due to\nits iterative sampling nature. This paper presents FLOAT, an audio-driven\ntalking portrait video generation method based on flow matching generative\nmodel. Instead of a pixel-based latent space, we take advantage of a learned\northogonal motion latent space, enabling efficient generation and editing of\ntemporally consistent motion. To achieve this, we introduce a transformer-based\nvector field predictor with an effective frame-wise conditioning mechanism.\nAdditionally, our method supports speech-driven emotion enhancement, enabling a\nnatural incorporation of expressive motions. Extensive experiments demonstrate\nthat our method outperforms state-of-the-art audio-driven talking portrait\nmethods in terms of visual quality, motion fidelity, and efficiency.", "comment": "ICCV 2025. Project page:\n  https://deepbrainai-research.github.io/float/", "pdf_url": "http://arxiv.org/pdf/2412.01064v4", "cate": "cs.CV", "date": "2024-12-02", "updated": "2025-08-01"}
{"id": "2507.22359", "title": "LLM-Crowdsourced: A Benchmark-Free Paradigm for Mutual Evaluation of Large Language Models", "authors": ["Qianhong Guo", "Wei Xie", "Xiaofang Cai", "Enze Wang", "Shuoyoucheng Ma", "Kai Chen", "Xiaofeng Wang", "Baosheng Wang"], "categories": ["cs.AI", "cs.CL"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22359v2", "summary": "Although large language models (LLMs) demonstrate remarkable capabilities\nacross various tasks, evaluating their capabilities remains a challenging task.\nExisting evaluation methods suffer from issues such as data contamination,\nblack-box operation, and subjective preference. These issues make it difficult\nto evaluate the LLMs' true capabilities comprehensively. To tackle these\nchallenges, we propose a novel benchmark-free evaluation paradigm,\nLLM-Crowdsourced. It utilizes LLMs to generate questions, answer independently,\nand evaluate mutually. This method integrates four key evaluation criteria:\ndynamic, transparent, objective, and professional, which existing evaluation\nmethods cannot satisfy simultaneously. Experiments on eight mainstream LLMs\nacross mathematics and programming verify the advantages of our method in\ndistinguishing LLM performance. Furthermore, our study reveals several novel\nfindings that are difficult for traditional methods to detect, including but\nnot limited to: (1) Gemini demonstrates the highest original and professional\nquestion-design capabilities among others; (2) Some LLMs exhibit\n''memorization-based answering'' by misrecognizing questions as familiar ones\nwith a similar structure; (3) LLM evaluation results demonstrate high\nconsistency (robustness).", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22359v2", "cate": "cs.AI", "date": "2025-07-30", "updated": "2025-07-31"}
{"id": "2507.10073", "title": "Cultural Bias in Large Language Models: Evaluating AI Agents through Moral Questionnaires", "authors": ["Simon Münker"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      15pages, 1 figure, 2 tables", "url": "http://arxiv.org/abs/2507.10073v2", "summary": "Are AI systems truly representing human values, or merely averaging across\nthem? Our study suggests a concerning reality: Large Language Models (LLMs)\nfail to represent diverse cultural moral frameworks despite their linguistic\ncapabilities. We expose significant gaps between AI-generated and human moral\nintuitions by applying the Moral Foundations Questionnaire across 19 cultural\ncontexts. Comparing multiple state-of-the-art LLMs' origins against human\nbaseline data, we find these models systematically homogenize moral diversity.\nSurprisingly, increased model size doesn't consistently improve cultural\nrepresentation fidelity. Our findings challenge the growing use of LLMs as\nsynthetic populations in social science research and highlight a fundamental\nlimitation in current AI alignment approaches. Without data-driven alignment\nbeyond prompting, these systems cannot capture the nuanced, culturally-specific\nmoral intuitions. Our results call for more grounded alignment objectives and\nevaluation metrics to ensure AI systems represent diverse human values rather\nthan flattening the moral landscape.", "comment": "15pages, 1 figure, 2 tables", "pdf_url": "http://arxiv.org/pdf/2507.10073v2", "cate": "cs.CL", "date": "2025-07-14", "updated": "2025-07-31"}
{"id": "2507.23643", "title": "FFGAF-SNN: The Forward-Forward Based Gradient Approximation Free Training Framework for Spiking Neural Networks", "authors": ["Changqing Xu", "Ziqiang Yang", "Yi Liu", "Xinfang Liao", "Guiqi Mo", "Hao Zeng", "Yintang Yang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23643v2", "summary": "Spiking Neural Networks (SNNs) offer a biologically plausible framework for\nenergy-efficient neuromorphic computing. However, it is a challenge to train\nSNNs due to their non-differentiability, efficiently. Existing gradient\napproximation approaches frequently sacrifice accuracy and face deployment\nlimitations on edge devices due to the substantial computational requirements\nof backpropagation. To address these challenges, we propose a Forward-Forward\n(FF) based gradient approximation-free training framework for Spiking Neural\nNetworks, which treats spiking activations as black-box modules, thereby\neliminating the need for gradient approximation while significantly reducing\ncomputational complexity. Furthermore, we introduce a class-aware complexity\nadaptation mechanism that dynamically optimizes the loss function based on\ninter-class difficulty metrics, enabling efficient allocation of network\nresources across different categories. Experimental results demonstrate that\nour proposed training framework achieves test accuracies of 99.58%, 92.13%, and\n75.64% on the MNIST, Fashion-MNIST, and CIFAR-10 datasets, respectively,\nsurpassing all existing FF-based SNN approaches. Additionally, our proposed\nmethod exhibits significant advantages in terms of memory access and\ncomputational power consumption.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23643v2", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-08-01"}
{"id": "2507.15066", "title": "Time-RA: Towards Time Series Reasoning for Anomaly with LLM Feedback", "authors": ["Yiyuan Yang", "Zichuan Liu", "Lei Song", "Kai Ying", "Zhiguang Wang", "Tom Bamford", "Svitlana Vyetrenko", "Jiang Bian", "Qingsong Wen"], "categories": ["cs.LG", "cs.AI", "cs.MM"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Under review. 19 pages, 8 figures, 12 tables. Code and dataset are publicly available", "url": "http://arxiv.org/abs/2507.15066v2", "summary": "Time series anomaly detection is critical across various domains, yet current\napproaches often limit analysis to mere binary anomaly classification without\ndetailed categorization or further explanatory reasoning. To address these\nlimitations, we propose a novel task, Time-series Reasoning for Anomaly\n(Time-RA) that transforms classical time series anomaly detection from a\ndiscriminative into a generative, reasoning-intensive task leveraging Large\nLanguage Models (LLMs). Also, we introduce the first real-world multimodal\nbenchmark dataset, RATs40K, explicitly annotated for anomaly reasoning,\ncomprising approximately 40,000 samples across 10 real-world domains. Each\nsample includes numeric time series data, contextual text information, and\nvisual representations, each annotated with fine-grained categories (14 types\nfor univariate anomalies and 6 for multivariate anomalies) and structured\nexplanatory reasoning. We develop a sophisticated annotation framework\nutilizing ensemble-generated labels refined through GPT-4-driven feedback,\nensuring accuracy and interpretability. Extensive benchmarking of LLMs and\nmultimodal LLMs demonstrates the capabilities and limitations of current\nmodels, highlighting the critical role of supervised fine-tuning. Our dataset\nand task pave the way for significant advancements in interpretable time series\nanomaly detection and reasoning. The code\n(https://github.com/yyysjz1997/Time-RA) and dataset\n(https://huggingface.co/datasets/Time-RA/RATs40K) have been fully open-sourced\nto support and accelerate future research in this area.", "comment": "Under review. 19 pages, 8 figures, 12 tables. Code and dataset are\n  publicly available", "pdf_url": "http://arxiv.org/pdf/2507.15066v2", "cate": "cs.LG", "date": "2025-07-20", "updated": "2025-08-01"}
{"id": "2508.00096", "title": "Zeroing Diagonals, Conjugate Hollowization, and Characterizing Nondefinite Operators", "authors": ["David R. Nicholus"], "categories": ["math.NA", "cs.NA", "math.RA", "65F25, 15A21, 15B10, 15A23, 15B99, 15A86"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "Comments:      24 pages, 4 figures", "url": "http://arxiv.org/abs/2508.00096v1", "summary": "We prove the conjecture by Damm and Fassbender that, for any pair $L,M$ of\nreal traceless matrices, there exists an orthogonal $V$ such that $V^{-1} L \\,\nV$ is hollow and $V M V^{-1}$ is almost hollow, where a matrix is hollow if and\nonly if its main diagonal consists only of 0s, and a traceless matrix is almost\nhollow if and only if all its main diagonal elements are 0 except, at most, the\nlast two.\n  The claim is a corollary to our considerably more general theorem, as well as\nanother corollary, revealing conditions on $L,M$ under which 0s can be\nintroduced by $V$ to all but the first or first two diagonal elements of\n$V^{-1} L \\, V$ and to all but the last two diagonal elements of $V M V^{-1}$.\n  By setting $L = M$, much is revealed concerning freedom and constraint\ninvolved in introducing 0s to the diagonal of a single operator. From this we\nprove novel characterizations of real traceless matrices, and a stronger\nversion of the seminal theorem by Fillmore that every real matrix is\northogonally similar to a matrix with a constant main diagonal.\n  Our results are contextualized in a characterization and classification of\nnondefinite matrices by, roughly, how many zeros can be introduced to their\ndiagonals, and it what ways.", "comment": "24 pages, 4 figures", "pdf_url": "http://arxiv.org/pdf/2508.00096v1", "cate": "math.NA", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.22782", "title": "Enhancing Multi-Agent Collaboration with Attention-Based Actor-Critic Policies", "authors": ["Hugo Garrido-Lestache", "Jeremy Kedziora"], "categories": ["cs.AI", "cs.LG", "I.2.0; I.2.8"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      8 pages", "url": "http://arxiv.org/abs/2507.22782v2", "summary": "This paper introduces Team-Attention-Actor-Critic (TAAC), a reinforcement\nlearning algorithm designed to enhance multi-agent collaboration in cooperative\nenvironments. TAAC employs a Centralized Training/Centralized Execution scheme\nincorporating multi-headed attention mechanisms in both the actor and critic.\nThis design facilitates dynamic, inter-agent communication, allowing agents to\nexplicitly query teammates, thereby efficiently managing the exponential growth\nof joint-action spaces while ensuring a high degree of collaboration. We\nfurther introduce a penalized loss function which promotes diverse yet\ncomplementary roles among agents. We evaluate TAAC in a simulated soccer\nenvironment against benchmark algorithms representing other multi-agent\nparadigms, including Proximal Policy Optimization and Multi-Agent\nActor-Attention-Critic. We find that TAAC exhibits superior performance and\nenhanced collaborative behaviors across a variety of metrics (win rates, goal\ndifferentials, Elo ratings, inter-agent connectivity, balanced spatial\ndistributions, and frequent tactical interactions such as ball possession\nswaps).", "comment": "8 pages", "pdf_url": "http://arxiv.org/pdf/2507.22782v2", "cate": "cs.AI", "date": "2025-07-30", "updated": "2025-07-31"}
{"id": "2507.11832", "title": "ILID: Native Script Language Identification for Indian Languages", "authors": ["Yash Ingle", "Pruthwik Mishra"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      10 pages, 1 figure, 6 tables, Paper accepted in RANLP 2025", "url": "http://arxiv.org/abs/2507.11832v2", "summary": "The language identification task is a crucial fundamental step in NLP. Often\nit serves as a pre-processing step for widely used NLP applications such as\nmultilingual machine translation, information retrieval, question and\nanswering, and text summarization. The core challenge of language\nidentification lies in distinguishing languages in noisy, short, and code-mixed\nenvironments. This becomes even harder in case of diverse Indian languages that\nexhibit lexical and phonetic similarities, but have distinct differences. Many\nIndian languages share the same script, making the task even more challenging.\nTaking all these challenges into account, we develop and release a dataset of\n250K sentences consisting of 23 languages including English and all 22 official\nIndian languages labeled with their language identifiers, where data in most\nlanguages are newly created. We also develop and release baseline models using\nstate-of-the-art approaches in machine learning and fine-tuning pre-trained\ntransformer models. Our models outperforms the state-of-the-art pre-trained\ntransformer models for the language identification task. The dataset and the\ncodes are available at https://yashingle-ai.github.io/ILID/ and in Huggingface\nopen source libraries.", "comment": "10 pages, 1 figure, 6 tables, Paper accepted in RANLP 2025", "pdf_url": "http://arxiv.org/pdf/2507.11832v2", "cate": "cs.CL", "date": "2025-07-16", "updated": "2025-07-31"}
{"id": "2507.23652", "title": "Adaptively Distilled ControlNet: Accelerated Training and Superior Sampling for Medical Image Synthesis", "authors": ["Kunpeng Qiu", "Zhiying Zhou", "Yongxin Guo"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by MICCAI2025", "url": "http://arxiv.org/abs/2507.23652v1", "summary": "Medical image annotation is constrained by privacy concerns and\nlabor-intensive labeling, significantly limiting the performance and\ngeneralization of segmentation models. While mask-controllable diffusion models\nexcel in synthesis, they struggle with precise lesion-mask alignment. We\npropose \\textbf{Adaptively Distilled ControlNet}, a task-agnostic framework\nthat accelerates training and optimization through dual-model distillation.\nSpecifically, during training, a teacher model, conditioned on mask-image\npairs, regularizes a mask-only student model via predicted noise alignment in\nparameter space, further enhanced by adaptive regularization based on\nlesion-background ratios. During sampling, only the student model is used,\nenabling privacy-preserving medical image generation. Comprehensive evaluations\non two distinct medical datasets demonstrate state-of-the-art performance:\nTransUNet improves mDice/mIoU by 2.4%/4.2% on KiTS19, while SANet achieves\n2.6%/3.5% gains on Polyps, highlighting its effectiveness and superiority. Code\nis available at GitHub.", "comment": "Accepted by MICCAI2025", "pdf_url": "http://arxiv.org/pdf/2507.23652v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.19209", "title": "Querying Autonomous Vehicle Point Clouds: Enhanced by 3D Object Counting with CounterNet", "authors": ["Xiaoyu Zhang", "Zhifeng Bao", "Hai Dong", "Ziwei Wang", "Jiajun Liu"], "categories": ["cs.CV", "cs.MM"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19209v2", "summary": "Autonomous vehicles generate massive volumes of point cloud data, yet only a\nsubset is relevant for specific tasks such as collision detection, traffic\nanalysis, or congestion monitoring. Effectively querying this data is essential\nto enable targeted analytics. In this work, we formalize point cloud querying\nby defining three core query types: RETRIEVAL, COUNT, and AGGREGATION, each\naligned with distinct analytical scenarios. All these queries rely heavily on\naccurate object counts to produce meaningful results, making precise object\ncounting a critical component of query execution. Prior work has focused on\nindexing techniques for 2D video data, assuming detection models provide\naccurate counting information. However, when applied to 3D point cloud data,\nstate-of-the-art detection models often fail to generate reliable object\ncounts, leading to substantial errors in query results. To address this\nlimitation, we propose CounterNet, a heatmap-based network designed for\naccurate object counting in large-scale point cloud data. Rather than focusing\non accurate object localization, CounterNet detects object presence by finding\nobject centers to improve counting accuracy. We further enhance its performance\nwith a feature map partitioning strategy using overlapping regions, enabling\nbetter handling of both small and large objects in complex traffic scenes. To\nadapt to varying frame characteristics, we introduce a per-frame dynamic model\nselection strategy that selects the most effective configuration for each\ninput. Evaluations on three real-world autonomous vehicle datasets show that\nCounterNet improves counting accuracy by 5% to 20% across object categories,\nresulting in more reliable query outcomes across all supported query types.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19209v2", "cate": "cs.CV", "date": "2025-07-25", "updated": "2025-08-01"}
{"id": "2508.00101", "title": "Leveraging Operator Learning to Accelerate Convergence of the Preconditioned Conjugate Gradient Method", "authors": ["Alena Kopaničáková", "Youngkyu Lee", "George Em Karniadakis"], "categories": ["math.NA", "cs.LG", "cs.NA", "math.OC", "65M55, 68T05, 49K20"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "Comments:      31 pages", "url": "http://arxiv.org/abs/2508.00101v1", "summary": "We propose a new deflation strategy to accelerate the convergence of the\npreconditioned conjugate gradient(PCG) method for solving parametric\nlarge-scale linear systems of equations. Unlike traditional deflation\ntechniques that rely on eigenvector approximations or recycled Krylov\nsubspaces, we generate the deflation subspaces using operator learning,\nspecifically the Deep Operator Network~(DeepONet). To this aim, we introduce\ntwo complementary approaches for assembling the deflation operators. The first\napproach approximates near-null space vectors of the discrete PDE operator\nusing the basis functions learned by the DeepONet. The second approach directly\nleverages solutions predicted by the DeepONet. To further enhance convergence,\nwe also propose several strategies for prescribing the sparsity pattern of the\ndeflation operator. A comprehensive set of numerical experiments encompassing\nsteady-state, time-dependent, scalar, and vector-valued problems posed on both\nstructured and unstructured geometries is presented and demonstrates the\neffectiveness of the proposed DeepONet-based deflated PCG method, as well as\nits generalization across a wide range of model parameters and problem\nresolutions.", "comment": "31 pages", "pdf_url": "http://arxiv.org/pdf/2508.00101v1", "cate": "math.NA", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2304.01430", "title": "Divided Attention: Unsupervised Multi-Object Discovery with Contextually Separated Slots", "authors": ["Dong Lao", "Zhengyang Hu", "Francesco Locatello", "Yanchao Yang", "Stefano Soatto"], "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2304.01430v3", "summary": "We investigate the emergence of objects in visual perception in the absence\nof any semantic annotation. The resulting model has received no supervision,\ndoes not use any pre-trained features, and yet it can segment the domain of an\nimage into multiple independently moving regions. The resulting motion\nsegmentation method can handle an unknown and varying number of objects in\nreal-time. The core multi-modal conditional encoder-decoder architecture has\none modality (optical flow) feed the encoder to produce a collection of latent\ncodes (slots), and the other modality (color image) conditions the decoder to\ngenerate the first modality (flow) from the slots. The training criterion is\ndesigned to foster 'information separation' among the slots, while the\narchitecture explicitly allocates activations to individual slots, leading to a\nmethod we call Divided Attention (DivA). At test time, DivA handles a different\nnumber of objects and different image resolution than seen at training, and is\ninvariant to permutations of the slots. DivA achieves state-of-the-art\nperformance while tripling the runtime speed of comparable methods, up to 104\nFPS, and reduces the performance gap from supervised methods to 12% or less.\nObjects bootstrapped by DivA can then be used to prime static classifiers via\ncontrastive learning. On fewer than 5,000 video clips, training DINO on DivA's\nobject proposals narrows the performance gap to ImageNet-based training by up\nto 30.2% compared to training directly on the video frames.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2304.01430v3", "cate": "cs.CV", "date": "2023-04-04", "updated": "2025-07-31"}
{"id": "2507.17186", "title": "FinGAIA: A Chinese Benchmark for AI Agents in Real-World Financial Domain", "authors": ["Lingfeng Zeng", "Fangqi Lou", "Zixuan Wang", "Jiajie Xu", "Jinyi Niu", "Mengping Li", "Yifan Dong", "Qi Qi", "Wei Zhang", "Ziwei Yang", "Jun Han", "Ruilun Feng", "Ruiqi Hu", "Lejie Zhang", "Zhengbo Feng", "Yicheng Ren", "Xin Guo", "Zhaowei Liu", "Dongpo Cheng", "Weige Cai", "Liwen Zhang"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17186v2", "summary": "The booming development of AI agents presents unprecedented opportunities for\nautomating complex tasks across various domains. However, their multi-step,\nmulti-tool collaboration capabilities in the financial sector remain\nunderexplored. This paper introduces FinGAIA, an end-to-end benchmark designed\nto evaluate the practical abilities of AI agents in the financial domain.\nFinGAIA comprises 407 meticulously crafted tasks, spanning seven major\nfinancial sub-domains: securities, funds, banking, insurance, futures, trusts,\nand asset management. These tasks are organized into three hierarchical levels\nof scenario depth: basic business analysis, asset decision support, and\nstrategic risk management. We evaluated 10 mainstream AI agents in a zero-shot\nsetting. The best-performing agent, ChatGPT, achieved an overall accuracy of\n48.9\\%, which, while superior to non-professionals, still lags financial\nexperts by over 35 percentage points. Error analysis has revealed five\nrecurring failure patterns: Cross-modal Alignment Deficiency, Financial\nTerminological Bias, Operational Process Awareness Barrier, among others. These\npatterns point to crucial directions for future research. Our work provides the\nfirst agent benchmark closely related to the financial domain, aiming to\nobjectively assess and promote the development of agents in this crucial field.\nPartial data is available at https://github.com/SUFE-AIFLM-Lab/FinGAIA.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17186v2", "cate": "cs.CL", "date": "2025-07-23", "updated": "2025-07-31"}
{"id": "2507.23657", "title": "OmniTraj: Pre-Training on Heterogeneous Data for Adaptive and Zero-Shot Human Trajectory Prediction", "authors": ["Yang Gao", "Po-Chien Luan", "Kaouther Messaoud", "Lan Feng", "Alexandre Alahi"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23657v1", "summary": "While large-scale pre-training has advanced human trajectory prediction, a\ncritical challenge remains: zero-shot transfer to unseen dataset with varying\ntemporal dynamics. State-of-the-art pre-trained models often require\nfine-tuning to adapt to new datasets with different frame rates or observation\nhorizons, limiting their scalability and practical utility. In this work, we\nsystematically investigate this limitation and propose a robust solution. We\nfirst demonstrate that existing data-aware discrete models struggle when\ntransferred to new scenarios with shifted temporal setups. We then isolate the\ntemporal generalization from dataset shift, revealing that a simple, explicit\nconditioning mechanism for temporal metadata is a highly effective solution.\nBased on this insight, we present OmniTraj, a Transformer-based model\npre-trained on a large-scale, heterogeneous dataset. Our experiments show that\nexplicitly conditioning on the frame rate enables OmniTraj to achieve\nstate-of-the-art zero-shot transfer performance, reducing prediction error by\nover 70\\% in challenging cross-setup scenarios. After fine-tuning, OmniTraj\nachieves state-of-the-art results on four datasets, including NBA, JTA,\nWorldPose, and ETH-UCY. The code is publicly available:\nhttps://github.com/vita-epfl/omnitraj", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23657v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2508.00221", "title": "Partial Floquet Transformation and Model Order Reduction of Linear Time-Periodic Systems", "authors": ["Sam Bender", "Christopher Beattie"], "categories": ["math.NA", "cs.NA", "math.DS"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "Comments:      21 pages", "url": "http://arxiv.org/abs/2508.00221v1", "summary": "Time-periodic dynamical systems occur commonly both in nature and as\nengineered systems. Large-scale linear time-periodic dynamical systems, for\nexample, may arise through linearization of a nonlinear system about a given\nperiodic solution (possibly as a consequence of a baseline periodic forcing)\nwith subsequent spatial discretization. The potential need to simulate\nresponses to a wide variety of input profiles (viewed as perturbations off a\nbaseline periodic forcing) creates a potent incentive for effective model\nreduction strategies applicable to linear time-periodic (LTP) systems.\nClassical approaches that take into account the underlying time-periodic system\nstructure often utilize the Floquet transform; however, computation of the\nFloquet transform is typically intractable for large order systems. In this\npaper, we develop the notion of a partial Floquet transformation connected to\nselected invariant subspaces of a time-varying differential operator associated\nwith the LTP system. We modify and repurpose the Dominant Pole Algorithm of\nRommes to identify effective invariant subspaces useful for model reduction. We\ndiscuss the construction of associated partial Floquet transformations and\ntime-varying reduction bases with which to produce effective reduced-order LTP\nmodels and illustrate the process on a simple time-periodic system.", "comment": "21 pages", "pdf_url": "http://arxiv.org/pdf/2508.00221v1", "cate": "math.NA", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2407.03080", "title": "Artificial Inductive Bias for Synthetic Tabular Data Generation in Data-Scarce Scenarios", "authors": ["Patricia A. Apellániz", "Ana Jiménez", "Borja Arroyo Galende", "Juan Parras", "Santiago Zazo"], "categories": ["cs.LG", "cs.AI", "I.2.0"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      19 pages, 6 Figures", "url": "http://arxiv.org/abs/2407.03080v2", "summary": "While synthetic tabular data generation using Deep Generative Models (DGMs)\noffers a compelling solution to data scarcity and privacy concerns, their\neffectiveness relies on the availability of substantial training data, often\nlacking in real-world scenarios. To overcome this limitation, we propose a\nnovel methodology that explicitly integrates artificial inductive biases into\nthe generative process to improve data quality in low-data regimes. Our\nframework leverages transfer learning and meta-learning techniques to construct\nand inject informative inductive biases into DGMs. We evaluate four approaches\n(pre-training, model averaging, Model-Agnostic Meta-Learning (MAML), and Domain\nRandomized Search (DRS)) and analyze their impact on the quality of the\ngenerated text. Experimental results show that incorporating inductive bias\nsubstantially improves performance, with transfer learning methods\noutperforming meta-learning, achieving up to 60\\% gains in Jensen-Shannon\ndivergence. The methodology is model-agnostic and especially relevant in\ndomains such as healthcare and finance, where high-quality synthetic data are\nessential, and data availability is often limited.", "comment": "19 pages, 6 Figures", "pdf_url": "http://arxiv.org/pdf/2407.03080v2", "cate": "cs.LG", "date": "2024-07-03", "updated": "2025-07-31"}
{"id": "2507.21568", "title": "Multi-Hypothesis Distillation of Multilingual Neural Translation Models for Low-Resource Languages", "authors": ["Aarón Galiano-Jiménez", "Juan Antonio Pérez-Ortiz", "Felipe Sánchez-Martínez", "Víctor M. Sánchez-Cartagena"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      17 pages, 12 figures", "url": "http://arxiv.org/abs/2507.21568v2", "summary": "This paper explores sequence-level knowledge distillation (KD) of\nmultilingual pre-trained encoder-decoder translation models. We argue that the\nteacher model's output distribution holds valuable insights for the student,\nbeyond the approximated mode obtained through beam search (the standard\ndecoding method), and present Multi-Hypothesis Distillation (MHD), a\nsequence-level KD method that generates multiple translations for each source\nsentence. This provides a larger representation of the teacher model\ndistribution and exposes the student model to a wider range of target-side\nprefixes. We leverage $n$-best lists from beam search to guide the student's\nlearning and examine alternative decoding methods to address issues like low\nvariability and the under-representation of infrequent tokens. For low-resource\nlanguages, our research shows that while sampling methods may slightly\ncompromise translation quality compared to beam search based approaches, they\nenhance the generated corpora with greater variability and lexical richness.\nThis ultimately improves student model performance and mitigates the gender\nbias amplification often associated with KD.", "comment": "17 pages, 12 figures", "pdf_url": "http://arxiv.org/pdf/2507.21568v2", "cate": "cs.CL", "date": "2025-07-29", "updated": "2025-07-31"}
{"id": "2507.23673", "title": "SAMSA: Segment Anything Model Enhanced with Spectral Angles for Hyperspectral Interactive Medical Image Segmentation", "authors": ["Alfie Roddan", "Tobias Czempiel", "Chi Xu", "Daniel S. Elson", "Stamatia Giannarou"], "categories": ["cs.CV", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23673v1", "summary": "Hyperspectral imaging (HSI) provides rich spectral information for medical\nimaging, yet encounters significant challenges due to data limitations and\nhardware variations. We introduce SAMSA, a novel interactive segmentation\nframework that combines an RGB foundation model with spectral analysis. SAMSA\nefficiently utilizes user clicks to guide both RGB segmentation and spectral\nsimilarity computations. The method addresses key limitations in HSI\nsegmentation through a unique spectral feature fusion strategy that operates\nindependently of spectral band count and resolution. Performance evaluation on\npublicly available datasets has shown 81.0% 1-click and 93.4% 5-click DICE on a\nneurosurgical and 81.1% 1-click and 89.2% 5-click DICE on an intraoperative\nporcine hyperspectral dataset. Experimental results demonstrate SAMSA's\neffectiveness in few-shot and zero-shot learning scenarios and using minimal\ntraining examples. Our approach enables seamless integration of datasets with\ndifferent spectral characteristics, providing a flexible framework for\nhyperspectral medical image analysis.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23673v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2508.00242", "title": "A reduced-IRKA method for large-scale $\\mathcal{H}_2$-optimal model order reduction", "authors": ["Yiding Lin", "Valeria Simoncini"], "categories": ["math.NA", "cs.NA"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00242v1", "summary": "The $\\mathcal{H}_2$-optimal Model Order Reduction (MOR) is one of the most\nsignificant frameworks for reduction methodologies for linear dynamical\nsystems. In this context, the Iterative Rational Krylov Algorithm (\\IRKA) is a\nwell established method for computing an optimal projection space of fixed\ndimension $r$, when the system has small or medium dimensions. However, for\nlarge problems the performance of \\IRKA\\ is not satisfactory. In this paper, we\nintroduce a new rational Krylov subspace projection method with conveniently\nselected shifts, that can effectively handle large-scale problems. The\nprojection subspace is generated sequentially, and the \\IRKA\\ procedure is\nemployed on the projected problem to produce a new optimal rational space of\ndimension $r$ for the reduced problem, and the associated shifts. The latter\nare then injected to expand the projection space. Truncation of older\ninformation of the generated space is performed to limit memory requirements.\nNumerical experiments on benchmark problems illustrate the effectiveness of the\nnew method.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00242v1", "cate": "math.NA", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2410.05343", "title": "EgoOops: A Dataset for Mistake Action Detection from Egocentric Videos referring to Procedural Texts", "authors": ["Yuto Haneji", "Taichi Nishimura", "Hirotaka Kameko", "Keisuke Shirai", "Tomoya Yoshida", "Keiya Kajimura", "Koki Yamamoto", "Taiyu Cui", "Tomohiro Nishimoto", "Shinsuke Mori"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Main 8 pages, supplementary 6 pages", "url": "http://arxiv.org/abs/2410.05343v3", "summary": "Mistake action detection is crucial for developing intelligent archives that\ndetect workers' errors and provide feedback. Existing studies have focused on\nvisually apparent mistakes in free-style activities, resulting in video-only\napproaches to mistake detection. However, in text-following activities, models\ncannot determine the correctness of some actions without referring to the\ntexts. Additionally, current mistake datasets rarely use procedural texts for\nvideo recording except for cooking. To fill these gaps, this paper proposes the\nEgoOops dataset, where egocentric videos record erroneous activities when\nfollowing procedural texts across diverse domains. It features three types of\nannotations: video-text alignment, mistake labels, and descriptions for\nmistakes. We also propose a mistake detection approach, combining video-text\nalignment and mistake label classification to leverage the texts. Our\nexperimental results show that incorporating procedural texts is essential for\nmistake detection. Data is available through\nhttps://y-haneji.github.io/EgoOops-project-page/.", "comment": "Main 8 pages, supplementary 6 pages", "pdf_url": "http://arxiv.org/pdf/2410.05343v3", "cate": "cs.CV", "date": "2024-10-07", "updated": "2025-07-31"}
{"id": "2507.22581", "title": "Unveiling the Influence of Amplifying Language-Specific Neurons", "authors": ["Inaya Rahmanisa", "Lyzander Marciano Andrylie", "Mahardika Krisna Ihsani", "Alfan Farizki Wicaksono", "Haryo Akbarianto Wibowo", "Alham Fikri Aji"], "categories": ["cs.CL", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Our code and dataset are made available at this https URL", "url": "http://arxiv.org/abs/2507.22581v2", "summary": "Language-specific neurons in LLMs that strongly correlate with individual\nlanguages have been shown to influence model behavior by deactivating them.\nHowever, their role in amplification remains underexplored. This work\ninvestigates the effect of amplifying language-specific neurons through\ninterventions across 18 languages, including low-resource ones, using three\nmodels primarily trained in different languages. We compare amplification\nfactors by their effectiveness in steering to the target language using a\nproposed Language Steering Shift (LSS) evaluation score, then evaluate it on\ndownstream tasks: commonsense reasoning (XCOPA, XWinograd), knowledge\n(Include), and translation (FLORES). The optimal amplification factors\neffectively steer output toward nearly all tested languages. Intervention using\nthis factor on downstream tasks improves self-language performance in some\ncases but generally degrades cross-language results. These findings highlight\nthe effect of language-specific neurons in multilingual behavior, where\namplification can be beneficial especially for low-resource languages, but\nprovides limited advantage for cross-lingual transfer.", "comment": "Our code and dataset are made available at\n  https://github.com/tauimbz/lang-task-neuron", "pdf_url": "http://arxiv.org/pdf/2507.22581v2", "cate": "cs.CL", "date": "2025-07-30", "updated": "2025-07-31"}
{"id": "2507.23683", "title": "I2V-GS: Infrastructure-to-Vehicle View Transformation with Gaussian Splatting for Autonomous Driving Data Generation", "authors": ["Jialei Chen", "Wuhao Xu", "Sipeng He", "Baoru Huang", "Dongchun Ren"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23683v1", "summary": "Vast and high-quality data are essential for end-to-end autonomous driving\nsystems. However, current driving data is mainly collected by vehicles, which\nis expensive and inefficient. A potential solution lies in synthesizing data\nfrom real-world images. Recent advancements in 3D reconstruction demonstrate\nphotorealistic novel view synthesis, highlighting the potential of generating\ndriving data from images captured on the road. This paper introduces a novel\nmethod, I2V-GS, to transfer the Infrastructure view To the Vehicle view with\nGaussian Splatting. Reconstruction from sparse infrastructure viewpoints and\nrendering under large view transformations is a challenging problem. We adopt\nthe adaptive depth warp to generate dense training views. To further expand the\nrange of views, we employ a cascade strategy to inpaint warped images, which\nalso ensures inpainting content is consistent across views. To further ensure\nthe reliability of the diffusion model, we utilize the cross-view information\nto perform a confidenceguided optimization. Moreover, we introduce RoadSight, a\nmulti-modality, multi-view dataset from real scenarios in infrastructure views.\nTo our knowledge, I2V-GS is the first framework to generate autonomous driving\ndatasets with infrastructure-vehicle view transformation. Experimental results\ndemonstrate that I2V-GS significantly improves synthesis quality under vehicle\nview, outperforming StreetGaussian in NTA-Iou, NTL-Iou, and FID by 45.7%,\n34.2%, and 14.9%, respectively.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23683v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2508.00375", "title": "A COGENT case study: Supporting Applications with Chombo", "authors": ["Daniel F. Martin", "Milo Dorr", "Mikhail Dorf", "Lee F. Ricketson"], "categories": ["math.NA", "cs.NA", "physics.plasm-ph"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00375v1", "summary": "We present a case study of how a software framework (Chombo) supported the\nspecific needs of a scientific application (COGENT). Since its inception in\n2000, the Chombo framework has supported various applications. One example of\nsuch support has been the collaboration with the Edge Simulation Laboratory to\nbuild the COGENT model. The specific needs of the COGENT effort required the\ndesign and implementation of a set of new capabilities in the Chombo framework,\nsuch as higher-order mapped-multiblock discretizations and multi-dimensional\ncode organization. These capabilities allowed COGENT to develop a unique\nsimulation capability for modeling the edge layers in tokamaks. Once developed,\nthese capabilities were able to support other applications which had similar\nneeds.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00375v1", "cate": "math.NA", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00212", "title": "Reinitializing weights vs units for maintaining plasticity in neural networks", "authors": ["J. Fernando Hernandez-Garcia", "Shibhansh Dohare", "Jun Luo", "Rich S. Sutton"], "categories": ["cs.NE", "cs.AI"], "primary_category": "Subjects:       Neural and Evolutionary Computing (cs.NE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00212v1", "summary": "Loss of plasticity is a phenomenon in which a neural network loses its\nability to learn when trained for an extended time on non-stationary data. It\nis a crucial problem to overcome when designing systems that learn continually.\nAn effective technique for preventing loss of plasticity is reinitializing\nparts of the network. In this paper, we compare two different reinitialization\nschemes: reinitializing units vs reinitializing weights. We propose a new\nalgorithm, which we name \\textit{selective weight reinitialization}, for\nreinitializing the least useful weights in a network. We compare our algorithm\nto continual backpropagation and ReDo, two previously proposed algorithms that\nreinitialize units in the network. Through our experiments in continual\nsupervised learning problems, we identify two settings when reinitializing\nweights is more effective at maintaining plasticity than reinitializing units:\n(1) when the network has a small number of units and (2) when the network\nincludes layer normalization. Conversely, reinitializing weights and units are\nequally effective at maintaining plasticity when the network is of sufficient\nsize and does not include layer normalization. We found that reinitializing\nweights maintains plasticity in a wider variety of settings than reinitializing\nunits.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00212v1", "cate": "cs.NE", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2410.16593", "title": "Graph Sampling for Scalable and Expressive Graph Neural Networks on Homophilic Graphs", "authors": ["Haolin Li", "Haoyu Wang", "Luana Ruiz"], "categories": ["eess.SP", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2410.16593v4", "summary": "Graph Neural Networks (GNNs) excel in many graph machine learning tasks but\nface challenges when scaling to large networks. GNN transferability allows\ntraining on smaller graphs and applying the model to larger ones, but existing\nmethods often rely on random subsampling, leading to disconnected subgraphs and\nreduced model expressivity. We propose a novel graph sampling algorithm that\nleverages feature homophily to preserve graph structure. By minimizing the\ntrace of the data correlation matrix, our method better preserves the graph\nLaplacian trace -- a proxy for the graph connectivity -- than random sampling,\nwhile achieving lower complexity than spectral methods. Experiments on citation\nnetworks show improved performance in preserving Laplacian trace and GNN\ntransferability compared to random sampling.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2410.16593v4", "cate": "eess.SP", "date": "2024-10-22", "updated": "2025-07-30"}
{"id": "2503.15621", "title": "LLaVA-MORE: A Comparative Study of LLMs and Visual Backbones for Enhanced Visual Instruction Tuning", "authors": ["Federico Cocchi", "Nicholas Moratelli", "Davide Caffagni", "Sara Sarto", "Lorenzo Baraldi", "Marcella Cornia", "Rita Cucchiara"], "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MM"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV 2025 Workshop on What is Next in Multimodal Foundation Models", "url": "http://arxiv.org/abs/2503.15621v2", "summary": "Recent progress in Multimodal Large Language Models (MLLMs) has highlighted\nthe critical roles of both the visual backbone and the underlying language\nmodel. While prior work has primarily focused on scaling these components to\nbillions of parameters, the trade-offs between model size, architecture, and\nperformance remain underexplored. Additionally, inconsistencies in training\ndata and evaluation protocols have hindered direct comparisons, making it\ndifficult to derive optimal design choices. In this paper, we introduce\nLLaVA-MORE, a new family of MLLMs that integrates recent language models with\ndiverse visual backbones. To ensure fair comparisons, we employ a unified\ntraining protocol applied consistently across all architectures. Our analysis\nsystematically explores both small- and medium-scale LLMs -- including Phi-4,\nLLaMA-3.1, and Gemma-2 -- to evaluate multimodal reasoning, generation, and\ninstruction following, while examining the relationship between model size and\nperformance. Beyond evaluating the LLM impact on final results, we conduct a\ncomprehensive study of various visual encoders, ranging from CLIP-based\narchitectures to alternatives such as DINOv2, SigLIP, and SigLIP2. Additional\nexperiments investigate the effects of increased image resolution and\nvariations in pre-training datasets. Overall, our results provide insights into\nthe design of more effective MLLMs, offering a reproducible evaluation\nframework that facilitates direct comparisons and can guide future model\ndevelopment. Our source code and trained models are publicly available at:\nhttps://github.com/aimagelab/LLaVA-MORE.", "comment": "ICCV 2025 Workshop on What is Next in Multimodal Foundation Models", "pdf_url": "http://arxiv.org/pdf/2503.15621v2", "cate": "cs.CV", "date": "2025-03-19", "updated": "2025-07-31"}
{"id": "2507.23685", "title": "UniLDiff: Unlocking the Power of Diffusion Priors for All-in-One Image Restoration", "authors": ["Zihan Cheng", "Liangtai Zhou", "Dian Chen", "Ni Tang", "Xiaotong Luo", "Yanyun Qu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23685v1", "summary": "All-in-One Image Restoration (AiOIR) has emerged as a promising yet\nchallenging research direction. To address its core challenges, we propose a\nnovel unified image restoration framework based on latent diffusion models\n(LDMs). Our approach structurally integrates low-quality visual priors into the\ndiffusion process, unlocking the powerful generative capacity of diffusion\nmodels for diverse degradations. Specifically, we design a Degradation-Aware\nFeature Fusion (DAFF) module to enable adaptive handling of diverse degradation\ntypes. Furthermore, to mitigate detail loss caused by the high compression and\niterative sampling of LDMs, we design a Detail-Aware Expert Module (DAEM) in\nthe decoder to enhance texture and fine-structure recovery. Extensive\nexperiments across multi-task and mixed degradation settings demonstrate that\nour method consistently achieves state-of-the-art performance, highlighting the\npractical potential of diffusion priors for unified image restoration. Our code\nwill be released.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23685v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2508.00515", "title": "A new addition theorem for the 3-D Navier-Lamé system and its application to the method of fundamental solutions", "authors": ["J. A. Barceló", "C. Castro", "A. Ruiz", "M. C. Vilela"], "categories": ["math.NA", "cs.NA"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00515v1", "summary": "We obtain a new addition theorem for the fundamental solution of the\nNavier-Lam\\'e system in dimension 3 satisfying the Kupradze radiation\nconditions. This provides an expansion of this fundamental solution that\ninvolves only the evaluation of Bessel functions and scalar spherical\nharmonics. This is particularly useful in collocation numerical methods based\non fundamental solutions, such as the boundary element method or the method of\nfundamental solutions. For this last method, we show its efficiency when\napproximating the Navier-Lam\\'e system in exterior domains.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00515v1", "cate": "math.NA", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00229", "title": "Sequential, Parallel and Consecutive Hybrid Evolutionary-Swarm Optimization Metaheuristics", "authors": ["Piotr Urbańczyk", "Aleksandra Urbańczyk", "Magdalena Król", "Leszek Rutkowski", "Marek Kisiel-Dorohinicki"], "categories": ["cs.NE", "math.OC", "90C59 (Primary), 90C27, 68T20, 68W10 (Secondary)", "I.2.8; I.2.6; G.1.6; F.2.1; I.6.6"], "primary_category": "Subjects:       Neural and Evolutionary Computing (cs.NE)", "pdf_link": null, "comments": "Comments:      16 pages, 2 figures, 5 tables, 5 algorithms, conference", "url": "http://arxiv.org/abs/2508.00229v1", "summary": "The goal of this paper is twofold. First, it explores hybrid\nevolutionary-swarm metaheuristics that combine the features of PSO and GA in a\nsequential, parallel and consecutive manner in comparison with their standard\nbasic form: Genetic Algorithm and Particle Swarm Optimization. The algorithms\nwere tested on a set of benchmark functions, including Ackley, Griewank, Levy,\nMichalewicz, Rastrigin, Schwefel, and Shifted Rotated Weierstrass, across\nmultiple dimensions. The experimental results demonstrate that the hybrid\napproaches achieve superior convergence and consistency, especially in\nhigher-dimensional search spaces. The second goal of this paper is to introduce\na novel consecutive hybrid PSO-GA evolutionary algorithm that ensures\ncontinuity between PSO and GA steps through explicit information transfer\nmechanisms, specifically by modifying GA's variation operators to inherit\nvelocity and personal best information.", "comment": "16 pages, 2 figures, 5 tables, 5 algorithms, conference", "pdf_url": "http://arxiv.org/pdf/2508.00229v1", "cate": "cs.NE", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2411.18659", "title": "DHCP: Detecting Hallucinations by Cross-modal Attention Pattern in Large Vision-Language Models", "authors": ["Yudong Zhang", "Ruobing Xie", "Xingwu Sun", "Yiqing Huang", "Jiansheng Chen", "Zhanhui Kang", "Di Wang", "Yu Wang"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ACM Multimedia 2025", "url": "http://arxiv.org/abs/2411.18659v2", "summary": "Large vision-language models (LVLMs) have demonstrated exceptional\nperformance on complex multimodal tasks. However, they continue to suffer from\nsignificant hallucination issues, including object, attribute, and relational\nhallucinations. To accurately detect these hallucinations, we investigated the\nvariations in cross-modal attention patterns between hallucination and\nnon-hallucination states. Leveraging these distinctions, we developed a\nlightweight detector capable of identifying hallucinations. Our proposed\nmethod, Detecting Hallucinations by Cross-modal Attention Patterns (DHCP), is\nstraightforward and does not require additional LVLM training or extra LVLM\ninference steps. Experimental results show that DHCP achieves remarkable\nperformance in hallucination detection. By offering novel insights into the\nidentification and analysis of hallucinations in LVLMs, DHCP contributes to\nadvancing the reliability and trustworthiness of these models. The code is\navailable at https://github.com/btzyd/DHCP.", "comment": "Accepted by ACM Multimedia 2025", "pdf_url": "http://arxiv.org/pdf/2411.18659v2", "cate": "cs.CV", "date": "2024-11-27", "updated": "2025-07-31"}
{"id": "2505.18102", "title": "How Can I Publish My LLM Benchmark Without Giving the True Answers Away?", "authors": ["Takashi Ishida", "Thanawat Lodkaew", "Ikko Yamane"], "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ME"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Extended version of the paper presented as an Oral at the ICML 2025 Workshop on the Impact of Memorization on Trustworthy Foundation Models", "url": "http://arxiv.org/abs/2505.18102v2", "summary": "Publishing a large language model (LLM) benchmark on the Internet risks\ncontaminating future LLMs: the benchmark may be unintentionally (or\nintentionally) used to train or select a model. A common mitigation is to keep\nthe benchmark private and let participants submit their models or predictions\nto the organizers. However, this strategy will require trust in a single\norganization and still permits test-set overfitting through repeated queries.\nTo overcome this issue, we propose a way to publish benchmarks without\ncompletely disclosing the ground-truth answers to the questions, while still\nmaintaining the ability to openly evaluate LLMs. Our main idea is to inject\nrandomness to the answers by preparing several logically correct answers, and\nonly include one of them as the solution in the benchmark. This reduces the\nbest possible accuracy, i.e., Bayes accuracy, of the benchmark. Not only is\nthis helpful to keep us from disclosing the ground truth, but this approach\nalso offers a test for detecting data contamination. In principle, even fully\ncapable models should not surpass the Bayes accuracy. If a model surpasses this\nceiling despite this expectation, this is a strong signal of data\ncontamination. We present experimental evidence that our method can detect data\ncontamination accurately on a wide range of benchmarks, models, and training\nmethodologies.", "comment": "Extended version of the paper presented as an Oral at the ICML 2025\n  Workshop on the Impact of Memorization on Trustworthy Foundation Models", "pdf_url": "http://arxiv.org/pdf/2505.18102v2", "cate": "cs.LG", "date": "2025-05-23", "updated": "2025-07-31"}
{"id": "2507.23709", "title": "Explainable Image Classification with Reduced Overconfidence for Tissue Characterisation", "authors": ["Alfie Roddan", "Chi Xu", "Serine Ajlouni", "Irini Kakaletri", "Patra Charalampaki", "Stamatia Giannarou"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23709v1", "summary": "The deployment of Machine Learning models intraoperatively for tissue\ncharacterisation can assist decision making and guide safe tumour resections.\nFor image classification models, pixel attribution methods are popular to infer\nexplainability. However, overconfidence in deep learning model's predictions\ntranslates to overconfidence in pixel attribution. In this paper, we propose\nthe first approach which incorporates risk estimation into a pixel attribution\nmethod for improved image classification explainability. The proposed method\niteratively applies a classification model with a pixel attribution method to\ncreate a volume of PA maps. This volume is used for the first time, to generate\na pixel-wise distribution of PA values. We introduce a method to generate an\nenhanced PA map by estimating the expectation values of the pixel-wise\ndistributions. In addition, the coefficient of variation (CV) is used to\nestimate pixel-wise risk of this enhanced PA map. Hence, the proposed method\nnot only provides an improved PA map but also produces an estimation of risk on\nthe output PA values. Performance evaluation on probe-based Confocal Laser\nEndomicroscopy (pCLE) data and ImageNet verifies that our improved\nexplainability method outperforms the state-of-the-art.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23709v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2508.00559", "title": "Solitary-wave solutions of the fractional nonlinear Schrödinger equation. II. A numerical study of the dynamics", "authors": ["Angel Durán", "Nuria Reguera"], "categories": ["math.NA", "cs.NA", "math.AP", "76B25, 35C07, 65H10"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00559v1", "summary": "The present paper is a numerical study of the dynamics of solitary wave\nsolutions of the fractional nonlinear Schr\\\"{o}dinger equation, whose existence\nwas analyzed by the authors in the first part of the project. The computational\nstudy will be made from the approximation of the periodic initial-value problem\nwith a fully discrete scheme consisting of a Fourier spectral method for the\nspatial discretization and a fourth-order, Runge-Kutta-Composition method as\ntime integrator. Several issues regarding the stability of the waves, such as\nthe effects of small and large perturbations, interactions of solitary waves\nand the resolution of initial data into trains of waves are discussed.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00559v1", "cate": "math.NA", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00380", "title": "Evolutionary Generative Optimization: Towards Fully Data-Driven Evolutionary Optimization via Generative Learning", "authors": ["Kebin Sun", "Tao Jiang", "Ran Cheng", "Yaochu Jin", "Kay Chen Tan"], "categories": ["cs.NE"], "primary_category": "Subjects:       Neural and Evolutionary Computing (cs.NE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00380v1", "summary": "Recent advances in data-driven evolutionary algorithms (EAs) have\ndemonstrated the potential of leveraging data to improve optimization accuracy\nand adaptability. Nevertheless, most existing approaches remain dependent on\nhandcrafted heuristics, which limits their generality and automation. To\naddress this challenge, we propose Evolutionary Generative Optimization\n(EvoGO), a fully data-driven framework empowered by generative learning. EvoGO\nstreamlines the evolutionary optimization process into three stages: data\npreparation, model training, and population generation. The data preparation\nstage constructs a pairwise dataset to enrich training diversity without\nincurring additional evaluation costs. During model training, a tailored\ngenerative model learns to transform inferior solutions into superior ones. In\nthe population generation stage, EvoGO replaces traditional reproduction\noperators with a scalable and parallelizable generative mechanism. Extensive\nexperiments on numerical benchmarks, classical control problems, and\nhigh-dimensional robotic tasks demonstrate that EvoGO consistently converges\nwithin merely 10 generations and significantly outperforms a wide spectrum of\noptimization approaches, including traditional EAs, Bayesian optimization, and\nreinforcement learning based methods. Source code will be made publicly\navailable.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00380v1", "cate": "cs.NE", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2412.12098", "title": "MaxInfoRL: Boosting exploration in reinforcement learning through information gain maximization", "authors": ["Bhavya Sukhija", "Stelian Coros", "Andreas Krause", "Pieter Abbeel", "Carmelo Sferrazza"], "categories": ["cs.LG", "cs.AI", "cs.RO"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2412.12098v2", "summary": "Reinforcement learning (RL) algorithms aim to balance exploiting the current\nbest strategy with exploring new options that could lead to higher rewards.\nMost common RL algorithms use undirected exploration, i.e., select random\nsequences of actions. Exploration can also be directed using intrinsic rewards,\nsuch as curiosity or model epistemic uncertainty. However, effectively\nbalancing task and intrinsic rewards is challenging and often task-dependent.\nIn this work, we introduce a framework, MaxInfoRL, for balancing intrinsic and\nextrinsic exploration. MaxInfoRL steers exploration towards informative\ntransitions, by maximizing intrinsic rewards such as the information gain about\nthe underlying task. When combined with Boltzmann exploration, this approach\nnaturally trades off maximization of the value function with that of the\nentropy over states, rewards, and actions. We show that our approach achieves\nsublinear regret in the simplified setting of multi-armed bandits. We then\napply this general formulation to a variety of off-policy model-free RL methods\nfor continuous state-action spaces, yielding novel algorithms that achieve\nsuperior performance across hard exploration problems and complex scenarios\nsuch as visual control tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2412.12098v2", "cate": "cs.LG", "date": "2024-12-16", "updated": "2025-07-31"}
{"id": "2507.14534", "title": "Conan: A Chunkwise Online Network for Zero-Shot Adaptive Voice Conversion", "authors": ["Yu Zhang", "Baotong Tian", "Zhiyao Duan"], "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "Subjects:       Audio and Speech Processing (eess.AS)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.14534v3", "summary": "Zero-shot online voice conversion (VC) holds significant promise for\nreal-time communications and entertainment. However, current VC models struggle\nto preserve semantic fidelity under real-time constraints, deliver\nnatural-sounding conversions, and adapt effectively to unseen speaker\ncharacteristics. To address these challenges, we introduce Conan, a chunkwise\nonline zero-shot voice conversion model that preserves the content of the\nsource while matching the voice timbre and styles of reference speech. Conan\ncomprises three core components: 1) a Stream Content Extractor that leverages\nEmformer for low-latency streaming content encoding; 2) an Adaptive Style\nEncoder that extracts fine-grained stylistic features from reference speech for\nenhanced style adaptation; 3) a Causal Shuffle Vocoder that implements a fully\ncausal HiFiGAN using a pixel-shuffle mechanism. Experimental evaluations\ndemonstrate that Conan outperforms baseline models in subjective and objective\nmetrics. Audio samples can be found at https://aaronz345.github.io/ConanDemo.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.14534v3", "cate": "eess.AS", "date": "2025-07-19", "updated": "2025-07-30"}
{"id": "2507.23715", "title": "DiffuMatch: Category-Agnostic Spectral Diffusion Priors for Robust Non-rigid Shape Matching", "authors": ["Emery Pierson", "Lei Li", "Angela Dai", "Maks Ovsjanikov"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Presented at ICCV 2025", "url": "http://arxiv.org/abs/2507.23715v1", "summary": "Deep functional maps have recently emerged as a powerful tool for solving\nnon-rigid shape correspondence tasks. Methods that use this approach combine\nthe power and flexibility of the functional map framework, with data-driven\nlearning for improved accuracy and generality. However, most existing methods\nin this area restrict the learning aspect only to the feature functions and\nstill rely on axiomatic modeling for formulating the training loss or for\nfunctional map regularization inside the networks. This limits both the\naccuracy and the applicability of the resulting approaches only to scenarios\nwhere assumptions of the axiomatic models hold. In this work, we show, for the\nfirst time, that both in-network regularization and functional map training can\nbe replaced with data-driven methods. For this, we first train a generative\nmodel of functional maps in the spectral domain using score-based generative\nmodeling, built from a large collection of high-quality maps. We then exploit\nthe resulting model to promote the structural properties of ground truth\nfunctional maps on new shape collections. Remarkably, we demonstrate that the\nlearned models are category-agnostic, and can fully replace commonly used\nstrategies such as enforcing Laplacian commutativity or orthogonality of\nfunctional maps. Our key technical contribution is a novel distillation\nstrategy from diffusion models in the spectral domain. Experiments demonstrate\nthat our learned regularization leads to better results than axiomatic\napproaches for zero-shot non-rigid shape matching. Our code is available at:\nhttps://github.com/daidedou/diffumatch/", "comment": "Presented at ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.23715v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2508.00722", "title": "Towards a mixed-precision ADI method for Lyapunov equations", "authors": ["Jonas Schulze", "Jens Saak"], "categories": ["math.NA", "cs.NA", "15A24, 65F10, 65F45, 65F55"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "Comments:      11 pages, 3 figures, 1 table; submitted to PAMM 2025", "url": "http://arxiv.org/abs/2508.00722v1", "summary": "We apply mixed-precision to the low-rank Lyapunov ADI (LR-ADI) by performing\ncertain aspects of the algorithm in a lower working precision. Namely, we\naccumulate the overall solution, solve the linear systems comprising the ADI\niteration, and store the inner low-rank factors of the residuals in various\ncombinations of IEEE 754 single and double precision. We empirically test our\nimplementation on Lyapunov equations arising from first- and second-order\ndescriptor systems. For the first-order examples, accumulating the solution in\nsingle-precision yields an almost-as-small residual as for the double-precision\nsolution. For certain applications, like computing the H2 norm of a descriptor\nsystem, low- or mixed-precision variants of the ADI can be quite competitive", "comment": "11 pages, 3 figures, 1 table; submitted to PAMM 2025", "pdf_url": "http://arxiv.org/pdf/2508.00722v1", "cate": "math.NA", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00387", "title": "STF: Shallow-Level Temporal Feedback to Enhance Spiking Transformers", "authors": ["Zeqi Zheng", "Zizheng Zhu", "Yingchao Yu", "Yanchen Huang", "Changze Lv", "Junfeng Tang", "Zhaofei Yu", "Yaochu Jin"], "categories": ["cs.NE", "cs.CV"], "primary_category": "Subjects:       Neural and Evolutionary Computing (cs.NE)", "pdf_link": null, "comments": "Comments:      32 pages, 4 figures", "url": "http://arxiv.org/abs/2508.00387v1", "summary": "Transformer-based Spiking Neural Networks (SNNs) suffer from a great\nperformance gap compared to floating-point Artificial Neural Networks (ANNs)\ndue to the binary nature of spike trains. Recent efforts have introduced\ndeep-level feedback loops to transmit high-level semantic information to narrow\nthis gap. However, these designs often span multiple deep layers, resulting in\ncostly feature transformations, higher parameter overhead, increased energy\nconsumption, and longer inference latency. To address this issue, we propose\nShallow-level Temporal Feedback (STF), a lightweight plug-and-play module for\nthe encoding layer, which consists of Temporal-Spatial Position Embedding\n(TSPE) and Temporal Feedback (TF).Extensive experiments show that STF\nconsistently improves performance across various Transformer-based SNN\nbackbones on static datasets, including CIFAR-10, CIFAR-100, and ImageNet-1K,\nunder different spike timestep settings. Further analysis reveals that STF\nenhances the diversity of the spike patterns, which is key to performance gain.\nMoreover, evaluations on adversarial robustness and temporal sensitivity\nconfirm that STF outperforms direct coding and its variants, highlighting its\npotential as a new spike encoding scheme for static scenarios. Our code will be\nreleased upon acceptance.", "comment": "32 pages, 4 figures", "pdf_url": "http://arxiv.org/pdf/2508.00387v1", "cate": "cs.NE", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2412.15441", "title": "Insights into resource utilization of code small language models serving with runtime engines and execution providers", "authors": ["Francisco Durán", "Matias Martinez", "Patricia Lago", "Silverio Martínez-Fernández"], "categories": ["cs.SE", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      Accepted in Journal of Systems and Software (JSS). For its published version refer to the Journal of JSS", "url": "http://arxiv.org/abs/2412.15441v2", "summary": "The rapid growth of language models, particularly in code generation,\nrequires substantial computational resources, raising concerns about energy\nconsumption and environmental impact. Optimizing language models inference\nresource utilization is crucial, and Small Language Models (SLMs) offer a\npromising solution to reduce resource demands. Our goal is to analyze the\nimpact of deep learning serving configurations, defined as combinations of\nruntime engines and execution providers, on resource utilization, in terms of\nenergy consumption, execution time, and computing-resource utilization from the\npoint of view of software engineers conducting inference in the context of code\ngeneration SLMs. We conducted a technology-oriented, multi-stage experimental\npipeline using twelve code generation SLMs to investigate energy consumption,\nexecution time, and computing-resource utilization across the configurations.\nSignificant differences emerged across configurations. CUDA execution provider\nconfigurations outperformed CPU execution provider configurations in both\nenergy consumption and execution time. Among the configurations, TORCH paired\nwith CUDA demonstrated the greatest energy efficiency, achieving energy savings\nfrom 37.99% up to 89.16% compared to other serving configurations. Similarly,\noptimized runtime engines like ONNX with the CPU execution provider achieved\nfrom 8.98% up to 72.04% energy savings within CPU-based configurations. Also,\nTORCH paired with CUDA exhibited efficient computing-resource utilization.\nServing configuration choice significantly impacts resource utilization. While\nfurther research is needed, we recommend the above configurations best suited\nto software engineers' requirements for enhancing serving resource utilization\nefficiency.", "comment": "Accepted in Journal of Systems and Software (JSS). For its published\n  version refer to the Journal of JSS", "pdf_url": "http://arxiv.org/pdf/2412.15441v2", "cate": "cs.SE", "date": "2024-12-19", "updated": "2025-07-30"}
{"id": "2507.22062", "title": "Meta CLIP 2: A Worldwide Scaling Recipe", "authors": ["Yung-Sung Chuang", "Yang Li", "Dong Wang", "Ching-Feng Yeh", "Kehan Lyu", "Ramya Raghavendra", "James Glass", "Lifei Huang", "Jason Weston", "Luke Zettlemoyer", "Xinlei Chen", "Zhuang Liu", "Saining Xie", "Wen-tau Yih", "Shang-Wen Li", "Hu Xu"], "categories": ["cs.CV", "cs.CL"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      10 pages", "url": "http://arxiv.org/abs/2507.22062v3", "summary": "Contrastive Language-Image Pretraining (CLIP) is a popular foundation model,\nsupporting from zero-shot classification, retrieval to encoders for multimodal\nlarge language models (MLLMs). Although CLIP is successfully trained on\nbillion-scale image-text pairs from the English world, scaling CLIP's training\nfurther to learning from the worldwide web data is still challenging: (1) no\ncuration method is available to handle data points from non-English world; (2)\nthe English performance from existing multilingual CLIP is worse than its\nEnglish-only counterpart, i.e., \"curse of multilinguality\" that is common in\nLLMs. Here, we present Meta CLIP 2, the first recipe training CLIP from scratch\non worldwide web-scale image-text pairs. To generalize our findings, we conduct\nrigorous ablations with minimal changes that are necessary to address the above\nchallenges and present a recipe enabling mutual benefits from English and\nnon-English world data. In zero-shot ImageNet classification, Meta CLIP 2\nViT-H/14 surpasses its English-only counterpart by 0.8% and mSigLIP by 0.7%,\nand surprisingly sets new state-of-the-art without system-level confounding\nfactors (e.g., translation, bespoke architecture changes) on multilingual\nbenchmarks, such as CVQA with 57.4%, Babel-ImageNet with 50.2% and XM3600 with\n64.3% on image-to-text retrieval.", "comment": "10 pages", "pdf_url": "http://arxiv.org/pdf/2507.22062v3", "cate": "cs.CV", "date": "2025-07-29", "updated": "2025-08-01"}
{"id": "2507.23734", "title": "RAGNet: Large-scale Reasoning-based Affordance Segmentation Benchmark towards General Grasping", "authors": ["Dongming Wu", "Yanping Fu", "Saike Huang", "Yingfei Liu", "Fan Jia", "Nian Liu", "Feng Dai", "Tiancai Wang", "Rao Muhammad Anwer", "Fahad Shahbaz Khan", "Jianbing Shen"], "categories": ["cs.CV", "cs.RO"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ICCV 2025. The code is at this https URL", "url": "http://arxiv.org/abs/2507.23734v1", "summary": "General robotic grasping systems require accurate object affordance\nperception in diverse open-world scenarios following human instructions.\nHowever, current studies suffer from the problem of lacking reasoning-based\nlarge-scale affordance prediction data, leading to considerable concern about\nopen-world effectiveness. To address this limitation, we build a large-scale\ngrasping-oriented affordance segmentation benchmark with human-like\ninstructions, named RAGNet. It contains 273k images, 180 categories, and 26k\nreasoning instructions. The images cover diverse embodied data domains, such as\nwild, robot, ego-centric, and even simulation data. They are carefully\nannotated with an affordance map, while the difficulty of language instructions\nis largely increased by removing their category name and only providing\nfunctional descriptions. Furthermore, we propose a comprehensive\naffordance-based grasping framework, named AffordanceNet, which consists of a\nVLM pre-trained on our massive affordance data and a grasping network that\nconditions an affordance map to grasp the target. Extensive experiments on\naffordance segmentation benchmarks and real-robot manipulation tasks show that\nour model has a powerful open-world generalization ability. Our data and code\nis available at https://github.com/wudongming97/AffordanceNet.", "comment": "Accepted by ICCV 2025. The code is at\n  https://github.com/wudongming97/AffordanceNet", "pdf_url": "http://arxiv.org/pdf/2507.23734v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2508.00028", "title": "Scalable Spectrum Availability Prediction using a Markov Chain Framework and ITU-R Propagation Models", "authors": ["Abir Ray"], "categories": ["cs.NI", "cs.AI", "cs.CL", "cs.NA", "math.NA"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "Comments:      12 pages", "url": "http://arxiv.org/abs/2508.00028v1", "summary": "Spectrum resources are often underutilized across time and space, motivating\ndynamic spectrum access strategies that allow secondary users to exploit unused\nfrequencies. A key challenge is predicting when and where spectrum will be\navailable (i.e., unused by primary licensed users) in order to enable proactive\nand interference-free access. This paper proposes a scalable framework for\nspectrum availability prediction that combines a two-state Markov chain model\nof primary user activity with high-fidelity propagation models from the ITU-R\n(specifically Recommendations P.528 and P.2108). The Markov chain captures\ntemporal occupancy patterns, while the propagation models incorporate path loss\nand clutter effects to determine if primary signals exceed interference\nthresholds at secondary user locations. By integrating these components, the\nproposed method can predict spectrum opportunities both in time and space with\nimproved accuracy. We develop the system model and algorithm for the approach,\nanalyze its scalability and computational efficiency, and discuss assumptions,\nlimitations, and potential applications. The framework is flexible and can be\nadapted to various frequency bands and scenarios. The results and analysis show\nthat the proposed approach can effectively identify available spectrum with low\ncomputational cost, making it suitable for real-time spectrum management in\ncognitive radio networks and other dynamic spectrum sharing systems.", "comment": "12 pages", "pdf_url": "http://arxiv.org/pdf/2508.00028v1", "cate": "cs.NI", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2508.00063", "title": "Anomaly detection with spiking neural networks for LHC physics", "authors": ["Barry M. Dillon", "Jim Harkin", "Aqib Javed"], "categories": ["hep-ph", "cs.NE", "hep-ex"], "primary_category": "Subjects:       High Energy Physics - Phenomenology (hep-ph)", "pdf_link": null, "comments": "Comments:      24 pages, 15 figures, 1 table", "url": "http://arxiv.org/abs/2508.00063v1", "summary": "Anomaly detection offers a promising strategy for discovering new physics at\nthe Large Hadron Collider (LHC). This paper investigates AutoEncoders built\nusing neuromorphic Spiking Neural Networks (SNNs) for this purpose. One key\napplication is at the trigger level, where anomaly detection tools could\ncapture signals that would otherwise be discarded by conventional selection\ncuts. These systems must operate under strict latency and computational\nconstraints. SNNs are inherently well-suited for low-latency, low-memory,\nreal-time inference, particularly on Field-Programmable Gate Arrays (FPGAs).\nFurther gains are expected with the rapid progress in dedicated neuromorphic\nhardware development. Using the CMS ADC2021 dataset, we design and evaluate a\nsimple SNN AutoEncoder architecture. Our results show that the SNN AutoEncoders\nare competitive with conventional AutoEncoders for LHC anomaly detection across\nall signal models.", "comment": "24 pages, 15 figures, 1 table", "pdf_url": "http://arxiv.org/pdf/2508.00063v1", "cate": "hep-ph", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2501.09112", "title": "Mantis Shrimp: Exploring Photometric Band Utilization in Computer Vision Networks for Photometric Redshift Estimation", "authors": ["Andrew Engel", "Nell Byler", "Adam Tsou", "Gautham Narayan", "Emmanuel Bonilla", "Ian Smith"], "categories": ["astro-ph.IM", "cs.AI"], "primary_category": "Subjects:       Instrumentation and Methods for Astrophysics (astro-ph.IM)", "pdf_link": null, "comments": "Comments:      Accepted at ApJ", "url": "http://arxiv.org/abs/2501.09112v2", "summary": "We present Mantis Shrimp, a multi-survey deep learning model for photometric\nredshift estimation that fuses ultra-violet (GALEX), optical (PanSTARRS), and\ninfrared (UnWISE) imagery. Machine learning is now an established approach for\nphotometric redshift estimation, with generally acknowledged higher performance\nin areas with a high density of spectroscopically identified galaxies over\ntemplate-based methods. Multiple works have shown that image-based\nconvolutional neural networks can outperform tabular-based color/magnitude\nmodels. In comparison to tabular models, image models have additional design\ncomplexities: it is largely unknown how to fuse inputs from different\ninstruments which have different resolutions or noise properties. The Mantis\nShrimp model estimates the conditional density estimate of redshift using\ncutout images. The density estimates are well calibrated and the point\nestimates perform well in the distribution of available spectroscopically\nconfirmed galaxies with (bias = 1e-2), scatter (NMAD = 2.44e-2) and\ncatastrophic outlier rate ($\\eta$=17.53$\\%$). We find that early fusion\napproaches (e.g., resampling and stacking images from different instruments)\nmatch the performance of late fusion approaches (e.g., concatenating latent\nspace representations), so that the design choice ultimately is left to the\nuser. Finally, we study how the models learn to use information across bands,\nfinding evidence that our models successfully incorporates information from all\nsurveys. The applicability of our model to the analysis of large populations of\ngalaxies is limited by the speed of downloading cutouts from external servers;\nhowever, our model could be useful in smaller studies such as generating priors\nover redshift for stellar population synthesis.", "comment": "Accepted at ApJ", "pdf_url": "http://arxiv.org/pdf/2501.09112v2", "cate": "astro-ph.IM", "date": "2025-01-15", "updated": "2025-07-31"}
{"id": "2507.22607", "title": "VL-Cogito: Progressive Curriculum Reinforcement Learning for Advanced Multimodal Reasoning", "authors": ["Ruifeng Yuan", "Chenghao Xiao", "Sicong Leng", "Jianyu Wang", "Long Li", "Weiwen Xu", "Hou Pong Chan", "Deli Zhao", "Tingyang Xu", "Zhongyu Wei", "Hao Zhang", "Yu Rong"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      21 pages, 5 figures, 6 tables. Work in progress", "url": "http://arxiv.org/abs/2507.22607v2", "summary": "Reinforcement learning has proven its effectiveness in enhancing the\nreasoning capabilities of large language models. Recent research efforts have\nprogressively extended this paradigm to multimodal reasoning tasks. Due to the\ninherent complexity and diversity of multimodal tasks, especially in semantic\ncontent and problem formulations, existing models often exhibit unstable\nperformance across various domains and difficulty levels. To address these\nlimitations, we propose VL-Cogito, an advanced multimodal reasoning model\ntrained via a novel multi-stage Progressive Curriculum Reinforcement Learning\n(PCuRL) framework. PCuRL systematically guides the model through tasks of\ngradually increasing difficulty, substantially improving its reasoning\nabilities across diverse multimodal contexts. The framework introduces two key\ninnovations: (1) an online difficulty soft weighting mechanism, dynamically\nadjusting training difficulty across successive RL training stages; and (2) a\ndynamic length reward mechanism, which encourages the model to adaptively\nregulate its reasoning path length according to task complexity, thus balancing\nreasoning efficiency with correctness. Experimental evaluations demonstrate\nthat VL-Cogito consistently matches or surpasses existing reasoning-oriented\nmodels across mainstream multimodal benchmarks spanning mathematics, science,\nlogic, and general understanding, validating the effectiveness of our approach.", "comment": "21 pages, 5 figures, 6 tables. Work in progress", "pdf_url": "http://arxiv.org/pdf/2507.22607v2", "cate": "cs.CV", "date": "2025-07-30", "updated": "2025-07-31"}
{"id": "2507.23755", "title": "Slot Attention with Re-Initialization and Self-Distillation", "authors": ["Rongzhen Zhao", "Yi Zhao", "Juho Kannala", "Joni Pajarinen"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ACM MM 2025", "url": "http://arxiv.org/abs/2507.23755v1", "summary": "Unlike popular solutions based on dense feature maps, Object-Centric Learning\n(OCL) represents visual scenes as sub-symbolic object-level feature vectors,\ntermed slots, which are highly versatile for tasks involving visual modalities.\nOCL typically aggregates object superpixels into slots by iteratively applying\ncompetitive cross attention, known as Slot Attention, with the slots as the\nquery. However, once initialized, these slots are reused naively, causing\nredundant slots to compete with informative ones for representing objects. This\noften results in objects being erroneously segmented into parts. Additionally,\nmainstream methods derive supervision signals solely from decoding slots into\nthe input's reconstruction, overlooking potential supervision based on internal\ninformation. To address these issues, we propose Slot Attention with\nre-Initialization and self-Distillation (DIAS): $\\emph{i)}$ We reduce\nredundancy in the aggregated slots and re-initialize extra aggregation to\nupdate the remaining slots; $\\emph{ii)}$ We drive the bad attention map at the\nfirst aggregation iteration to approximate the good at the last iteration to\nenable self-distillation. Experiments demonstrate that DIAS achieves\nstate-of-the-art on OCL tasks like object discovery and recognition, while also\nimproving advanced visual prediction and reasoning. Our code is available on\nhttps://github.com/Genera1Z/DIAS.", "comment": "Accepted by ACM MM 2025", "pdf_url": "http://arxiv.org/pdf/2507.23755v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2508.00197", "title": "Graph Lineages and Skeletal Graph Products", "authors": ["Eric Mjolsness", "Cory B. Scott"], "categories": ["cs.CV", "cs.LG", "cs.NA", "math.CT", "math.NA"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      42 pages. 33 Figures. Under review", "url": "http://arxiv.org/abs/2508.00197v1", "summary": "Graphs, and sequences of growing graphs, can be used to specify the\narchitecture of mathematical models in many fields including machine learning\nand computational science. Here we define structured graph \"lineages\" (ordered\nby level number) that grow in a hierarchical fashion, so that: (1) the number\nof graph vertices and edges increases exponentially in level number; (2)\nbipartite graphs connect successive levels within a graph lineage and, as in\nmultigrid methods, can constrain matrices relating successive levels; (3) using\nprolongation maps within a graph lineage, process-derived distance measures\nbetween graphs at successive levels can be defined; (4) a category of \"graded\ngraphs\" can be defined, and using it low-cost \"skeletal\" variants of standard\nalgebraic graph operations and type constructors (cross product, box product,\ndisjoint sum, and function types) can be derived for graded graphs and hence\nhierarchical graph lineages; (5) these skeletal binary operators have similar\nbut not identical algebraic and category-theoretic properties to their standard\ncounterparts; (6) graph lineages and their skeletal product constructors can\napproach continuum limit objects. Additional space-efficient unary operators on\ngraded graphs are also derived: thickening, which creates a graph lineage of\nmultiscale graphs, and escalation to a graph lineage of search frontiers\n(useful as a generalization of adaptive grids and in defining \"skeletal\"\nfunctions). The result is an algebraic type theory for graded graphs and\n(hierarchical) graph lineages. The approach is expected to be well suited to\ndefining hierarchical model architectures - \"hierarchitectures\" - and local\nsampling, search, or optimization algorithms on them. We demonstrate such\napplication to deep neural networks (including visual and feature scale spaces)\nand to multigrid numerical methods.", "comment": "42 pages. 33 Figures. Under review", "pdf_url": "http://arxiv.org/pdf/2508.00197v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2508.00475", "title": "E2ATST: A Temporal-Spatial Optimized Energy-Efficient Architecture for Training Spiking Transformer", "authors": ["Yunhao Ma", "Yanyu Lin", "Mingjing Li", "Puli Quan", "Chenlin Zhou", "Wenyue Zhang", "Zhiwei Zhong", "Wanyi Jia", "Xueke Zhu", "Qingyan Meng", "Huihui Zhou", "Fengwei An"], "categories": ["cs.AR", "cs.NE"], "primary_category": "Subjects:       Hardware Architecture (cs.AR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00475v1", "summary": "(1) Pengcheng Laboratory, (2) Southern University of Science and Technology,\n(3) Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences,\n(4) University of Chinese Academy of Sciences", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00475v1", "cate": "cs.AR", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00007", "title": "Agent Network Protocol Technical White Paper", "authors": ["Gaowei Chang", "Eidan Lin", "Chengxuan Yuan", "Rizhao Cai", "Binbin Chen", "Xuan Xie", "Yin Zhang"], "categories": ["cs.NI", "cs.AI"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "Comments:      This white paper is a reformatted version of the open-source community edition previously released by the ANP Open Source Technology Community( this https URL )", "url": "http://arxiv.org/abs/2508.00007v1", "summary": "With the development of large models and autonomous decision-making AI,\nagents are rapidly becoming the new entities of the internet, following mobile\napps. However, existing internet infrastructure is primarily designed for human\ninteraction, creating data silos, unfriendly interfaces, and high collaboration\ncosts among agents, making it difficult to support the needs for large-scale\nagent interconnection and collaboration. The internet is undergoing a profound\ntransformation, showing four core trends: agents replacing traditional\nsoftware, universal agent interconnection, native protocol-based connections,\nand autonomous agent organization and collaboration. To align with these\ntrends, Agent Network Protocol (ANP) proposes a new generation of communication\nprotocols for the Agentic Web. ANP adheres to AI-native design, maintains\ncompatibility with existing internet protocols, adopts a modular composable\narchitecture, follows minimalist yet extensible principles, and enables rapid\ndeployment based on existing infrastructure. Through a three-layer protocol\nsystem--identity and encrypted communication layer, meta-protocol negotiation\nlayer, and application protocol layer--ANP. systematically solves the problems\nof agent identity authentication, dynamic negotiation, and capability discovery\ninteroperability.", "comment": "This white paper is a reformatted version of the open-source\n  community edition previously released by the ANP Open Source Technology\n  Community(https://github.com/agent-network-protocol)", "pdf_url": "http://arxiv.org/pdf/2508.00007v1", "cate": "cs.NI", "date": "2025-07-18", "updated": "2025-07-18"}
{"id": "2501.15544", "title": "Advancing Generative Artificial Intelligence and Large Language Models for Demand Side Management with Internet of Electric Vehicles", "authors": ["Hanwen Zhang", "Ruichen Zhang", "Wei Zhang", "Dusit Niyato", "Yonggang Wen", "Chunyan Miao"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      11 Pages", "url": "http://arxiv.org/abs/2501.15544v4", "summary": "Generative artificial intelligence, particularly through large language\nmodels (LLMs), is poised to transform energy optimization and demand side\nmanagement (DSM) within microgrids. This paper explores the integration of LLMs\ninto energy management, emphasizing their roles in automating the optimization\nof DSM strategies with Internet of electric vehicles. We investigate challenges\nand solutions associated with DSM and explore the new opportunities presented\nby leveraging LLMs. Then, we propose an innovative solution that enhances LLMs\nwith retrieval-augmented generation for automatic problem formulation, code\ngeneration, and customizing optimization. We present a case study to\ndemonstrate the effectiveness of our proposed solution in charging scheduling\nand optimization for electric vehicles, highlighting our solution's significant\nadvancements in energy efficiency and user adaptability. This work underscores\nthe potential of LLMs for energy optimization and fosters a new era of\nintelligent DSM solutions.", "comment": "11 Pages", "pdf_url": "http://arxiv.org/pdf/2501.15544v4", "cate": "cs.LG", "date": "2025-01-26", "updated": "2025-07-31"}
{"id": "2507.23772", "title": "SeqAffordSplat: Scene-level Sequential Affordance Reasoning on 3D Gaussian Splatting", "authors": ["Di Li", "Jie Feng", "Jiahao Chen", "Weisheng Dong", "Guanbin Li", "Yuhui Zheng", "Mingtao Feng", "Guangming Shi"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23772v1", "summary": "3D affordance reasoning, the task of associating human instructions with the\nfunctional regions of 3D objects, is a critical capability for embodied agents.\nCurrent methods based on 3D Gaussian Splatting (3DGS) are fundamentally limited\nto single-object, single-step interactions, a paradigm that falls short of\naddressing the long-horizon, multi-object tasks required for complex real-world\napplications. To bridge this gap, we introduce the novel task of Sequential 3D\nGaussian Affordance Reasoning and establish SeqAffordSplat, a large-scale\nbenchmark featuring 1800+ scenes to support research on long-horizon affordance\nunderstanding in complex 3DGS environments. We then propose SeqSplatNet, an\nend-to-end framework that directly maps an instruction to a sequence of 3D\naffordance masks. SeqSplatNet employs a large language model that\nautoregressively generates text interleaved with special segmentation tokens,\nguiding a conditional decoder to produce the corresponding 3D mask. To handle\ncomplex scene geometry, we introduce a pre-training strategy, Conditional\nGeometric Reconstruction, where the model learns to reconstruct complete\naffordance region masks from known geometric observations, thereby building a\nrobust geometric prior. Furthermore, to resolve semantic ambiguities, we design\na feature injection mechanism that lifts rich semantic features from 2D Vision\nFoundation Models (VFM) and fuses them into the 3D decoder at multiple scales.\nExtensive experiments demonstrate that our method sets a new state-of-the-art\non our challenging benchmark, effectively advancing affordance reasoning from\nsingle-step interactions to complex, sequential tasks at the scene level.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23772v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2508.00247", "title": "Sinusoidal Approximation Theorem for Kolmogorov-Arnold Networks", "authors": ["Sergei Gleyzer", "Hanh Nguyen", "Dinesh P. Ramakrishnan", "Eric A. F. Reinhardt"], "categories": ["stat.ML", "cs.LG", "cs.NA", "math.NA"], "primary_category": "Subjects:       Machine Learning (stat.ML)", "pdf_link": null, "comments": "Comments:      15 pages, 3 figures", "url": "http://arxiv.org/abs/2508.00247v1", "summary": "The Kolmogorov-Arnold representation theorem states that any continuous\nmultivariable function can be exactly represented as a finite superposition of\ncontinuous single variable functions. Subsequent simplifications of this\nrepresentation involve expressing these functions as parameterized sums of a\nsmaller number of unique monotonic functions. These developments led to the\nproof of the universal approximation capabilities of multilayer perceptron\nnetworks with sigmoidal activations, forming the alternative theoretical\ndirection of most modern neural networks.\n  Kolmogorov-Arnold Networks (KANs) have been recently proposed as an\nalternative to multilayer perceptrons. KANs feature learnable nonlinear\nactivations applied directly to input values, modeled as weighted sums of basis\nspline functions. This approach replaces the linear transformations and\nsigmoidal post-activations used in traditional perceptrons. Subsequent works\nhave explored alternatives to spline-based activations. In this work, we\npropose a novel KAN variant by replacing both the inner and outer functions in\nthe Kolmogorov-Arnold representation with weighted sinusoidal functions of\nlearnable frequencies. Inspired by simplifications introduced by Lorentz and\nSprecher, we fix the phases of the sinusoidal activations to linearly spaced\nconstant values and provide a proof of its theoretical validity. We also\nconduct numerical experiments to evaluate its performance on a range of\nmultivariable functions, comparing it with fixed-frequency Fourier transform\nmethods and multilayer perceptrons (MLPs). We show that it outperforms the\nfixed-frequency Fourier transform and achieves comparable performance to MLPs.", "comment": "15 pages, 3 figures", "pdf_url": "http://arxiv.org/pdf/2508.00247v1", "cate": "stat.ML", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00545", "title": "Foundations of Interpretable Models", "authors": ["Pietro Barbiero", "Mateo Espinosa Zarlenga", "Alberto Termine", "Mateja Jamnik", "Giuseppe Marra"], "categories": ["cs.LG", "cs.AI", "cs.NE", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00545v1", "summary": "We argue that existing definitions of interpretability are not actionable in\nthat they fail to inform users about general, sound, and robust interpretable\nmodel design. This makes current interpretability research fundamentally\nill-posed. To address this issue, we propose a definition of interpretability\nthat is general, simple, and subsumes existing informal notions within the\ninterpretable AI community. We show that our definition is actionable, as it\ndirectly reveals the foundational properties, underlying assumptions,\nprinciples, data structures, and architectural features necessary for designing\ninterpretable models. Building on this, we propose a general blueprint for\ndesigning interpretable models and introduce the first open-sourced library\nwith native support for interpretable data structures and processes.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00545v1", "cate": "cs.LG", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00009", "title": "Enabling Immersive XR Collaborations over FTTR Networks (Invited)", "authors": ["Sourav Mondal", "Elaine Wong"], "categories": ["cs.NI", "cs.AI"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "Comments:      This invited paper was presented in Optica Advanced Photonic Congress 2025", "url": "http://arxiv.org/abs/2508.00009v1", "summary": "Fiber-To-The-Room is a potential solution to achieve in-premise extended\nreality collaborations. This paper explores predictive bandwidth allocation and\nseamless handover schemes over FTTR, showing high-quality immersive experience\nfor in-premise collaborations can be achieved. \\c{opyright} 2025 The Author(s).", "comment": "This invited paper was presented in Optica Advanced Photonic Congress\n  2025", "pdf_url": "http://arxiv.org/pdf/2508.00009v1", "cate": "cs.NI", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2502.20632", "title": "Lattice Protein Folding with Variational Annealing", "authors": ["Shoummo Ahsan Khandoker", "Estelle M. Inack", "Mohamed Hibat-Allah"], "categories": ["cond-mat.dis-nn", "cs.AI", "cs.LG", "q-bio.BM"], "primary_category": "Subjects:       Disordered Systems and Neural Networks (cond-mat.dis-nn)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2502.20632v2", "summary": "Understanding the principles of protein folding is a cornerstone of\ncomputational biology, with implications for drug design, bioengineering, and\nthe understanding of fundamental biological processes. Lattice protein folding\nmodels offer a simplified yet powerful framework for studying the complexities\nof protein folding, enabling the exploration of energetically optimal folds\nunder constrained conditions. However, finding these optimal folds is a\ncomputationally challenging combinatorial optimization problem. In this work,\nwe introduce a novel upper-bound training scheme that employs masking to\nidentify the lowest-energy folds in two-dimensional Hydrophobic-Polar (HP)\nlattice protein folding. By leveraging Dilated Recurrent Neural Networks (RNNs)\nintegrated with an annealing process driven by temperature-like fluctuations,\nour method accurately predicts optimal folds for benchmark systems of up to 60\nbeads. Our approach also effectively masks invalid folds from being sampled\nwithout compromising the autoregressive sampling properties of RNNs. This\nscheme is generalizable to three spatial dimensions and can be extended to\nlattice protein models with larger alphabets. Our findings emphasize the\npotential of advanced machine learning techniques in tackling complex protein\nfolding problems and a broader class of constrained combinatorial optimization\nchallenges.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2502.20632v2", "cate": "cond-mat.dis-nn", "date": "2025-02-28", "updated": "2025-07-30"}
{"id": "2507.23778", "title": "Half-Physics: Enabling Kinematic 3D Human Model with Physical Interactions", "authors": ["Li Siyao", "Yao Feng", "Omid Tehari", "Chen Change Loy", "Michael J. Black"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23778v1", "summary": "While current general-purpose 3D human models (e.g., SMPL-X) efficiently\nrepresent accurate human shape and pose, they lacks the ability to physically\ninteract with the environment due to the kinematic nature. As a result,\nkinematic-based interaction models often suffer from issues such as\ninterpenetration and unrealistic object dynamics. To address this limitation,\nwe introduce a novel approach that embeds SMPL-X into a tangible entity capable\nof dynamic physical interactions with its surroundings. Specifically, we\npropose a \"half-physics\" mechanism that transforms 3D kinematic motion into a\nphysics simulation. Our approach maintains kinematic control over inherent\nSMPL-X poses while ensuring physically plausible interactions with scenes and\nobjects, effectively eliminating penetration and unrealistic object dynamics.\nUnlike reinforcement learning-based methods, which demand extensive and complex\ntraining, our half-physics method is learning-free and generalizes to any body\nshape and motion; meanwhile, it operates in real time. Moreover, it preserves\nthe fidelity of the original kinematic motion while seamlessly integrating\nphysical interactions", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23778v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2201.00145", "title": "Matrix Decomposition and Applications", "authors": ["Jun Lu"], "categories": ["math.NA", "cs.LG", "cs.NA"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2201.00145v5", "summary": "In 1954, Alston S. Householder published Principles of Numerical Analysis,\none of the first modern treatments on matrix decomposition that favored a\n(block) LU decomposition-the factorization of a matrix into the product of\nlower and upper triangular matrices. And now, matrix decomposition has become a\ncore technology in machine learning, largely due to the development of the\nbackpropagation algorithm in fitting a neural network. The sole aim of this\nsurvey is to give a self-contained introduction to concepts and mathematical\ntools in numerical linear algebra and matrix analysis in order to seamlessly\nintroduce matrix decomposition techniques and their applications in subsequent\nsections. However, we clearly realize our inability to cover all the useful and\ninteresting results concerning matrix decomposition, given the paucity of scope\nto present this discussion, e.g., the separated analysis of the Euclidean\nspace, Hermitian space, Hilbert space, and things in the complex domain. We\nrefer the reader to literature in the field of linear algebra for a more\ndetailed introduction to the related fields.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2201.00145v5", "cate": "math.NA", "date": "2022-01-01", "updated": "2025-08-01"}
{"id": "2504.10053", "title": "Synthetic Biology meets Neuromorphic Computing: Towards a bio-inspired Olfactory Perception System", "authors": ["Kevin Max", "Larissa Sames", "Shimeng Ye", "Jan Steinkühler", "Federico Corradi"], "categories": ["cs.NE", "cs.ET", "q-bio.NC"], "primary_category": "Subjects:       Neural and Evolutionary Computing (cs.NE)", "pdf_link": null, "comments": "Comments:      Updated after revision at Neuromorphic Computing and Engineering", "url": "http://arxiv.org/abs/2504.10053v2", "summary": "In this study, we explore how the combination of synthetic biology,\nneuroscience modeling, and neuromorphic electronic systems offers a new\napproach to creating an artificial system that mimics the natural sense of\nsmell. We argue that a co-design approach offers significant advantages in\nreplicating the complex dynamics of odor sensing and processing. We propose a\nhybrid system of synthetic sensory neurons that provides three key features:\n(a) receptor-gated ion channels, (b) interface between synthetic biology and\nsemiconductors and (c) event-based encoding and computing based on spiking\nnetworks. Our approach is validated using simulation-based modeling of the\ncomplete sensing and processing pipeline. This research seeks to develop a\nplatform for ultra-sensitive, specific, and energy-efficient odor detection,\nwith potential implications for environmental monitoring, medical diagnostics,\nand security.", "comment": "Updated after revision at Neuromorphic Computing and Engineering", "pdf_url": "http://arxiv.org/pdf/2504.10053v2", "cate": "cs.NE", "date": "2025-04-14", "updated": "2025-08-01"}
{"id": "2508.00010", "title": "Non-Terrestrial Network Models Using Stochastic Geometry: Planar or Spherical?", "authors": ["Ruibo Wang", "Baha Eddine Youcef Belmekki", "Howard H. Yang", "Mohamed Slim Alouini"], "categories": ["cs.NI"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00010v1", "summary": "With the explosive deployment of non-terrestrial networks (NTNs), the\ncomputational complexity of network performance analysis is rapidly escalating.\nAs one of the most suitable mathematical tools for analyzing large-scale\nnetwork topologies, stochastic geometry (SG) enables the representation of\nnetwork performance metrics as functions of network parameters, thus offering\nlow-complexity performance analysis solutions. However, choosing between planar\nand spherical models remains challenging. Planar models neglect Earth's\ncurvature, causing deviations in high-altitude NTN analysis, yet are still\noften used for simplicity. This paper introduces relative error to quantify the\ngap between planar and spherical models, helping determine when planar modeling\nis sufficient. To calculate the relative error, we first propose a point\nprocess (PP) generation algorithm that simultaneously generates a pair of\nhomogeneous and asymptotically similar planar and spherical PPs. We then\nintroduce several typical similarity metrics, including topology-related and\nnetwork-level metrics, and further develop a relative error estimation\nalgorithm based on these metrics. In addition, we derive an analytical\nexpression for the optimal planar altitude, which reduces computational\ncomplexity and provides theoretical support for planar approximation. Finally,\nnumerical results investigate how deployment altitude and region affect NTN\nmodeling, with case studies on HAP and LEO satellite constellations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00010v1", "cate": "cs.NI", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2502.20934", "title": "Revisiting the Evaluation Bias Introduced by Frame Sampling Strategies in Surgical Video Segmentation Using SAM2", "authors": ["Utku Ozbulak", "Seyed Amir Mousavi", "Francesca Tozzi", "Niki Rashidian", "Wouter Willaert", "Wesley De Neve", "Joris Vankerschaver"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted for publication in the 28th International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI) Workshop on Fairness of AI in Medical Imaging (FAIMI), 2025", "url": "http://arxiv.org/abs/2502.20934v3", "summary": "Real-time video segmentation is a promising opportunity for AI-assisted\nsurgery, offering intraoperative guidance by identifying tools and anatomical\nstructures. Despite growing interest in surgical video segmentation, annotation\nprotocols vary widely across datasets -- some provide dense, frame-by-frame\nlabels, while others rely on sparse annotations sampled at low frame rates such\nas 1 FPS. In this study, we investigate how such inconsistencies in annotation\ndensity and frame rate sampling influence the evaluation of zero-shot\nsegmentation models, using SAM2 as a case study for cholecystectomy procedures.\nSurprisingly, we find that under conventional sparse evaluation settings, lower\nframe rates can appear to outperform higher ones due to a smoothing effect that\nconceals temporal inconsistencies. However, when assessed under real-time\nstreaming conditions, higher frame rates yield superior segmentation stability,\nparticularly for dynamic objects like surgical graspers. To understand how\nthese differences align with human perception, we conducted a survey among\nsurgeons, nurses, and machine learning engineers and found that participants\nconsistently preferred high-FPS segmentation overlays, reinforcing the\nimportance of evaluating every frame in real-time applications rather than\nrelying on sparse sampling strategies. Our findings highlight the risk of\nevaluation bias that is introduced by inconsistent dataset protocols and bring\nattention to the need for temporally fair benchmarking in surgical video AI.", "comment": "Accepted for publication in the 28th International Conference on\n  Medical Image Computing and Computer Assisted Intervention (MICCAI) Workshop\n  on Fairness of AI in Medical Imaging (FAIMI), 2025", "pdf_url": "http://arxiv.org/pdf/2502.20934v3", "cate": "cs.CV", "date": "2025-02-28", "updated": "2025-07-31"}
{"id": "2507.23782", "title": "MonoFusion: Sparse-View 4D Reconstruction via Monocular Fusion", "authors": ["Zihan Wang", "Jeff Tan", "Tarasha Khurana", "Neehar Peri", "Deva Ramanan"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV 2025. Project Page: this https URL", "url": "http://arxiv.org/abs/2507.23782v1", "summary": "We address the problem of dynamic scene reconstruction from sparse-view\nvideos. Prior work often requires dense multi-view captures with hundreds of\ncalibrated cameras (e.g. Panoptic Studio). Such multi-view setups are\nprohibitively expensive to build and cannot capture diverse scenes in-the-wild.\nIn contrast, we aim to reconstruct dynamic human behaviors, such as repairing a\nbike or dancing, from a small set of sparse-view cameras with complete scene\ncoverage (e.g. four equidistant inward-facing static cameras). We find that\ndense multi-view reconstruction methods struggle to adapt to this sparse-view\nsetup due to limited overlap between viewpoints. To address these limitations,\nwe carefully align independent monocular reconstructions of each camera to\nproduce time- and view-consistent dynamic scene reconstructions. Extensive\nexperiments on PanopticStudio and Ego-Exo4D demonstrate that our method\nachieves higher quality reconstructions than prior art, particularly when\nrendering novel views. Code, data, and data-processing scripts are available on\nhttps://github.com/ImNotPrepared/MonoFusion.", "comment": "ICCV 2025. Project Page:\n  https://imnotprepared.github.io/research/25_DSR/", "pdf_url": "http://arxiv.org/pdf/2507.23782v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2312.12699", "title": "Propagation of chaos in infinite horizon and numerical stability for stochastic McKean-Vlasov equations", "authors": ["Zhuoqi Liu", "Shuaibin Gao", "Chenggui Yuan", "Qian Guo"], "categories": ["math.NA", "cs.NA"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2312.12699v2", "summary": "This paper focuses on the numerical stability of stochastic McKean-Vlasov\nequations (SMVEs) via the stochastic particle method. Firstly, the long-time\npropagation of chaos in the mean-square sense is obtained, and the almost sure\npropagation in infinite horizon is also proved. Next, when the coefficients\nsatisfy linear growth conditions, the mean-square and almost sure exponential\nstabilities of the Euler-Maruyama (EM) scheme associated with the corresponding\ninteracting particle system are shown through an ingenious manipulation of\nempirical measure. Then, for the case that the state variables in drift and\ndiffusion are both superlinear, the mean-square exponential stability of the\nbackward EM scheme for the interacting system is achieved without the particle\ncorruption, which is a novel conclusion. Moreover, under the linear growth\ncondition on the diffusion coefficient, the almost sure stability of the\nbackward EM scheme is studied. Combining these assertions enables the numerical\nsolutions to reproduce the stabilities of the original SMVEs. The examples,\nincluding a feedback control problem and a stochastic opinion dynamics model,\nare provided to demonstrate the importance of theoretical analysis of numerical\nstability.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2312.12699v2", "cate": "math.NA", "date": "2023-12-20", "updated": "2025-08-01"}
{"id": "2508.00011", "title": "AoI-Aware Resource Allocation with Deep Reinforcement Learning for HAPS-V2X Networks", "authors": ["Ahmet Melih Ince", "Ayse Elif Canbilen", "Halim Yanikomeroglu"], "categories": ["cs.NI", "cs.AI", "cs.LG", "cs.MA", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "Comments:      6 pages, 3 figures, to appear in IEEE conference proceedings", "url": "http://arxiv.org/abs/2508.00011v1", "summary": "Sixth-generation (6G) networks are designed to meet the hyper-reliable and\nlow-latency communication (HRLLC) requirements of safety-critical applications\nsuch as autonomous driving. Integrating non-terrestrial networks (NTN) into the\n6G infrastructure brings redundancy to the network, ensuring continuity of\ncommunications even under extreme conditions. In particular, high-altitude\nplatform stations (HAPS) stand out for their wide coverage and low latency\nadvantages, supporting communication reliability and enhancing information\nfreshness, especially in rural areas and regions with infrastructure\nconstraints. In this paper, we present reinforcement learning-based approaches\nusing deep deterministic policy gradient (DDPG) to dynamically optimize the\nage-of-information (AoI) in HAPS-enabled vehicle-to-everything (V2X) networks.\nThe proposed method improves information freshness and overall network\nreliability by enabling independent learning without centralized coordination.\nThe findings reveal the potential of HAPS-supported solutions, combined with\nDDPG-based learning, for efficient AoI-aware resource allocation in\nplatoon-based autonomous vehicle systems.", "comment": "6 pages, 3 figures, to appear in IEEE conference proceedings", "pdf_url": "http://arxiv.org/pdf/2508.00011v1", "cate": "cs.NI", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2503.09215", "title": "Other Vehicle Trajectories Are Also Needed: A Driving World Model Unifies Ego-Other Vehicle Trajectories in Video Latent Space", "authors": ["Jian Zhu", "Zhengyu Jia", "Tian Gao", "Jiaxin Deng", "Shidi Li", "Lang Zhang", "Fu Liu", "Peng Jia", "Xianpeng Lang"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      8 pages, 7 figures", "url": "http://arxiv.org/abs/2503.09215v3", "summary": "Advanced end-to-end autonomous driving systems predict other vehicles'\nmotions and plan ego vehicle's trajectory. The world model that can foresee the\noutcome of the trajectory has been used to evaluate the autonomous driving\nsystem. However, existing world models predominantly emphasize the trajectory\nof the ego vehicle and leave other vehicles uncontrollable. This limitation\nhinders their ability to realistically simulate the interaction between the ego\nvehicle and the driving scenario. In this paper, we propose a driving World\nModel named EOT-WM, unifying Ego-Other vehicle Trajectories in videos for\ndriving simulation. Specifically, it remains a challenge to match multiple\ntrajectories in the BEV space with each vehicle in the video to control the\nvideo generation. We first project ego-other vehicle trajectories in the BEV\nspace into the image coordinate for vehicle-trajectory match via pixel\npositions. Then, trajectory videos are encoded by the Spatial-Temporal\nVariational Auto Encoder to align with driving video latents spatially and\ntemporally in the unified visual space. A trajectory-injected diffusion\nTransformer is further designed to denoise the noisy video latents for video\ngeneration with the guidance of ego-other vehicle trajectories. In addition, we\npropose a metric based on control latent similarity to evaluate the\ncontrollability of trajectories. Extensive experiments are conducted on the\nnuScenes dataset, and the proposed model outperforms the state-of-the-art\nmethod by 30% in FID and 55% in FVD. The model can also predict unseen driving\nscenes with self-produced trajectories.", "comment": "8 pages, 7 figures", "pdf_url": "http://arxiv.org/pdf/2503.09215v3", "cate": "cs.CV", "date": "2025-03-12", "updated": "2025-07-31"}
{"id": "2507.23785", "title": "Gaussian Variation Field Diffusion for High-fidelity Video-to-4D Synthesis", "authors": ["Bowen Zhang", "Sicheng Xu", "Chuxin Wang", "Jiaolong Yang", "Feng Zhao", "Dong Chen", "Baining Guo"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV 2025. Project page: this https URL", "url": "http://arxiv.org/abs/2507.23785v1", "summary": "In this paper, we present a novel framework for video-to-4D generation that\ncreates high-quality dynamic 3D content from single video inputs. Direct 4D\ndiffusion modeling is extremely challenging due to costly data construction and\nthe high-dimensional nature of jointly representing 3D shape, appearance, and\nmotion. We address these challenges by introducing a Direct 4DMesh-to-GS\nVariation Field VAE that directly encodes canonical Gaussian Splats (GS) and\ntheir temporal variations from 3D animation data without per-instance fitting,\nand compresses high-dimensional animations into a compact latent space.\nBuilding upon this efficient representation, we train a Gaussian Variation\nField diffusion model with temporal-aware Diffusion Transformer conditioned on\ninput videos and canonical GS. Trained on carefully-curated animatable 3D\nobjects from the Objaverse dataset, our model demonstrates superior generation\nquality compared to existing methods. It also exhibits remarkable\ngeneralization to in-the-wild video inputs despite being trained exclusively on\nsynthetic data, paving the way for generating high-quality animated 3D content.\nProject page: https://gvfdiffusion.github.io/.", "comment": "ICCV 2025. Project page: https://gvfdiffusion.github.io/", "pdf_url": "http://arxiv.org/pdf/2507.23785v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2504.02629", "title": "A fully segregated and unconditionally stable IMEX scheme for dispersed multiphase flows", "authors": ["Douglas Pacheco", "Richard Schussnig"], "categories": ["math.NA", "cs.NA"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.02629v2", "summary": "Euler--Euler or volume-averaged Navier--Stokes equations are used in various\napplications to model systems with two or more interpenetrating phases. Each\nfluid obeys its own momentum and mass equations, and the phases are typically\ncoupled via drag forces and a shared pressure. Monolithic solvers can therefore\nbe very expensive and difficult to implement, so there is great computational\nappeal for decoupled methods. However, splitting the subproblems requires\ntreating the coupling terms (pressure and drag) explicitly, which must be done\ncarefully to avoid time-step restrictions. In this context, we derive a new\nfirst-order pressure-correction method based on the incompressibility of the\nmean velocity field, combined with an explicit treatment of the drag forces.\nFurthermore, both the convective and viscous terms are treated semi-implicitly.\nThis gives us an implicit-explicit (IMEX) method that is very robust not only\ndue to its unconditional energy stability, but also because it does not require\nany type of fixed-point iterations. Each time step has only linear, scalar\ntransport equations and a single pressure Poisson problem as building blocks.\nWe rigorously prove temporal stability without any CFL-like conditions, and the\ntheory is confirmed through two-phase numerical examples.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.02629v2", "cate": "math.NA", "date": "2025-04-03", "updated": "2025-08-01"}
{"id": "2508.00020", "title": "Performance Analysis of SAGIN from the Relay Perspective: A Spherical Stochastic Geometry Approach", "authors": ["Ferdaous Tarhouni", "Ruibo Wang", "Mohamed-Slim Alouini"], "categories": ["cs.NI"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00020v1", "summary": "In recent years, the satellite-aerial-ground integrated network (SAGIN) has\nbecome essential in meeting the increasing demands for global wireless\ncommunications. In SAGIN, high-altitude platforms (HAPs) can serve as\ncommunication hubs and act as relays to enhance communication performance. In\nthis paper, we evaluate network performance and analyze the role of HAPs in\nSAGIN from the relay perspective. Based on this unique perspective, we\nintroduce three metrics to evaluate the performance, named the average access\ndata rate, the average backhaul data rate, and the backhaul rate exceedance\nprobability (BREP). Considering the need for dynamic topology and interference\nanalysis, we choose spherical stochastic geometry (SSG) as a tool and derive\nanalytical expressions for the above metrics to achieve low-complexity\nperformance evaluation. Specifically, we provide a closed-form expression for\nthe end-to-end performance metric BREP. Given that there is no existing\nliterature in the SSG field studying networks from a relay perspective, we\nspecifically investigate the impact of satellite network topology on\nperformance in our numerical results to further highlight the advantages of the\nSSG framework. Additionally, we analyze the minimum HAP transmission power\nrequired to maintain both short-term and long-term data rate demands.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00020v1", "cate": "cs.NI", "date": "2025-07-27", "updated": "2025-07-27"}
{"id": "2503.17788", "title": "Learning to Align and Refine: A Foundation-to-Diffusion Framework for Occlusion-Robust Two-Hand Reconstruction", "authors": ["Gaoge Han", "Yongkang Cheng", "Zhe Chen", "Shaoli Huang", "Tongliang Liu"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.17788v2", "summary": "Two-hand reconstruction from monocular images faces persistent challenges due\nto complex and dynamic hand postures and occlusions, causing significant\ndifficulty in achieving plausible interaction alignment. Existing approaches\nstruggle with such alignment issues, often resulting in misalignment and\npenetration artifacts. To tackle this, we propose a dual-stage\nFoundation-to-Diffusion framework that precisely align 2D prior guidance from\nvision foundation models and diffusion-based generative 3D interaction\nrefinement to achieve occlusion-robust two-hand reconstruction. First, we\nintroduce a lightweight fusion alignment encoder that aligns fused multimodal\n2D priors like key points, segmentation maps, and depth cues from vision\nfoundation models during training. This provides robust structured guidance,\nfurther enabling efficient inference without heavy foundation model encoders at\ntest time while maintaining high reconstruction accuracy. Second, we implement\na two-hand diffusion model explicitly trained to convert interpenetrated 3D\nposes into plausible, penetration-free counterparts. Through collision\ngradient-guided denoising, the model rectifies artifacts while preserving\nnatural spatial relationships between hands. Extensive evaluations demonstrate\nthat our method achieves state-of-the-art performance on InterHand2.6M, HIC,\nand FreiHAND datasets, significantly advancing occlusion handling and\ninteraction robustness. Our code will be publicly released.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.17788v2", "cate": "cs.CV", "date": "2025-03-22", "updated": "2025-07-31"}
{"id": "2507.23000", "title": "Planning for Cooler Cities: A Multimodal AI Framework for Predicting and Mitigating Urban Heat Stress through Urban Landscape Transformation", "authors": ["Shengao Yi", "Xiaojiang Li", "Wei Tu", "Tianhong Zhao"], "categories": ["cs.LG", "cs.CV"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23000v1", "summary": "As extreme heat events intensify due to climate change and urbanization,\ncities face increasing challenges in mitigating outdoor heat stress. While\ntraditional physical models such as SOLWEIG and ENVI-met provide detailed\nassessments of human-perceived heat exposure, their computational demands limit\nscalability for city-wide planning. In this study, we propose GSM-UTCI, a\nmultimodal deep learning framework designed to predict daytime average\nUniversal Thermal Climate Index (UTCI) at 1-meter hyperlocal resolution. The\nmodel fuses surface morphology (nDSM), high-resolution land cover data, and\nhourly meteorological conditions using a feature-wise linear modulation (FiLM)\narchitecture that dynamically conditions spatial features on atmospheric\ncontext. Trained on SOLWEIG-derived UTCI maps, GSM-UTCI achieves near-physical\naccuracy, with an R2 of 0.9151 and a mean absolute error (MAE) of 0.41{\\deg}C,\nwhile reducing inference time from hours to under five minutes for an entire\ncity. To demonstrate its planning relevance, we apply GSM-UTCI to simulate\nsystematic landscape transformation scenarios in Philadelphia, replacing bare\nearth, grass, and impervious surfaces with tree canopy. Results show spatially\nheterogeneous but consistently strong cooling effects, with impervious-to-tree\nconversion producing the highest aggregated benefit (-4.18{\\deg}C average\nchange in UTCI across 270.7 km2). Tract-level bivariate analysis further\nreveals strong alignment between thermal reduction potential and land cover\nproportions. These findings underscore the utility of GSM-UTCI as a scalable,\nfine-grained decision support tool for urban climate adaptation, enabling\nscenario-based evaluation of greening strategies across diverse urban\nenvironments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23000v1", "cate": "cs.LG", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.01762", "title": "Global Energy Minimization for Simplex Mesh Optimization: A Radius Ratio Approach to Sliver Elimination", "authors": ["Dong Wang", "Chunyu Chen", "Huayi Wei"], "categories": ["math.NA", "cs.NA", "math.OC", "65N50, 65K10, 65F08"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.01762v3", "summary": "The quality of simplex mesh is crucial for the stability and accuracy of\nnumerical simulations in finite element analysis and computational geometry.\nHowever, the presence of sliver elements in 3D simplex mesh can severely impact\nthe results. This paper presents a novel method based on a radius ratio energy\nfunction to optimize the quality of simplex mesh elements. This method can\neffectively eliminate sliver elements, thereby enhancing mesh quality.The\ngradient of the proposed energy function can be decomposed into a matrix-vector\nproduct. With minor processing, the matrix becomes symmetric positive definite,\nand this symmetric positive definite matrix can serve as a preconditioner to\nsignificantly accelerate the optimization process. Experimental results\ndemonstrate that this method has significant advantages in eliminating sliver\nelements and improving mesh quality.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.01762v3", "cate": "math.NA", "date": "2025-07-02", "updated": "2025-08-01"}
{"id": "2508.00042", "title": "Towards Reliable AI in 6G: Detecting Concept Drift in Wireless Network", "authors": ["Athanasios Tziouvaras", "Carolina Fortuna", "George Floros", "Kostas Kolomvatsos", "Panagiotis Sarigiannidis", "Marko Grobelnik", "Blaž Bertalanič"], "categories": ["cs.NI", "cs.LG"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "Comments:      10 pages, 12 figures", "url": "http://arxiv.org/abs/2508.00042v1", "summary": "AI-native 6G networks promise unprecedented automation and performance by\nembedding machine-learning models throughout the radio access and core segments\nof the network. However, the non-stationary nature of wireless environments due\nto infrastructure changes, user mobility, and emerging traffic patterns,\ninduces concept drifts that can quickly degrade these model accuracies.\nExisting methods in general are very domain specific, or struggle with certain\ntype of concept drift. In this paper, we introduce two unsupervised,\nmodel-agnostic, batch concept drift detectors. Both methods compute an\nexpected-utility score to decide when concept drift occurred and if model\nretraining is warranted, without requiring ground-truth labels after\ndeployment. We validate our framework on two real-world wireless use cases in\noutdoor fingerprinting for localization and for link-anomaly detection, and\ndemonstrate that both methods are outperforming classical detectors such as\nADWIN, DDM, CUSUM by 20-40 percentage points. Additionally, they achieve an\nF1-score of 0.94 and 1.00 in correctly triggering retraining alarm, thus\nreducing the false alarm rate by up to 20 percentage points compared to the\nbest classical detectors.", "comment": "10 pages, 12 figures", "pdf_url": "http://arxiv.org/pdf/2508.00042v1", "cate": "cs.NI", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2503.22688", "title": "CodeIF-Bench: Evaluating Instruction-Following Capabilities of Large Language Models in Interactive Code Generation", "authors": ["Peiding Wang", "Li Zhang", "Fang Liu", "Lin Shi", "Minxiao Li", "Bo Shen", "An Fu"], "categories": ["cs.SE", "cs.AI", "cs.PL"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.22688v3", "summary": "Large Language Models (LLMs) have demonstrated exceptional performance in\ncode generation tasks and have become indispensable programming assistants for\ndevelopers. However, existing code generation benchmarks primarily assess the\nfunctional correctness of code generated by LLMs in single-turn interactions.\nThey offer limited insight into LLMs' abilities to generate code that strictly\nfollows users' instructions in multi-turn interaction scenarios. In this paper,\nwe introduce CodeIF-Bench, a benchmark for evaluating the instruction-following\ncapabilities of LLMs in interactive code generation. Specifically, CodeIF-Bench\nincorporates nine types of verifiable instructions aligned with the real-world\nsoftware development requirements, which can be independently and objectively\nvalidated through specified test cases, facilitating the evaluation of\ninstruction-following capability in multi-turn interactions. In both\n\\textit{Static Conversation} and \\textit{Dynamic Conversation} settings, we\nevaluate the performance of 7 state-of-the-art LLMs and summarize the important\nfactors influencing the instruction-following ability of LLMs in multi-turn\ninteractions, as well as potential directions for improvement.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.22688v3", "cate": "cs.SE", "date": "2025-03-05", "updated": "2025-07-31"}
{"id": "2507.23001", "title": "LesionGen: A Concept-Guided Diffusion Model for Dermatology Image Synthesis", "authors": ["Jamil Fayyad", "Nourhan Bayasi", "Ziyang Yu", "Homayoun Najjaran"], "categories": ["eess.IV", "cs.CV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "Comments:      Accepted at the MICCAI 2025 ISIC Workshop", "url": "http://arxiv.org/abs/2507.23001v1", "summary": "Deep learning models for skin disease classification require large, diverse,\nand well-annotated datasets. However, such resources are often limited due to\nprivacy concerns, high annotation costs, and insufficient demographic\nrepresentation. While text-to-image diffusion probabilistic models (T2I-DPMs)\noffer promise for medical data synthesis, their use in dermatology remains\nunderexplored, largely due to the scarcity of rich textual descriptions in\nexisting skin image datasets. In this work, we introduce LesionGen, a\nclinically informed T2I-DPM framework for dermatology image synthesis. Unlike\nprior methods that rely on simplistic disease labels, LesionGen is trained on\nstructured, concept-rich dermatological captions derived from expert\nannotations and pseudo-generated, concept-guided reports. By fine-tuning a\npretrained diffusion model on these high-quality image-caption pairs, we enable\nthe generation of realistic and diverse skin lesion images conditioned on\nmeaningful dermatological descriptions. Our results demonstrate that models\ntrained solely on our synthetic dataset achieve classification accuracy\ncomparable to those trained on real images, with notable gains in worst-case\nsubgroup performance. Code and data are available here.", "comment": "Accepted at the MICCAI 2025 ISIC Workshop", "pdf_url": "http://arxiv.org/pdf/2507.23001v1", "cate": "eess.IV", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.22850", "title": "Dynamic analysis of free-free Timoshenko beams on elastic foundation under transverse transient ground deformation", "authors": ["Gersena Banushi"], "categories": ["math.NA", "cs.NA"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "Comments:      Key words: buried Timoshenko beam, semi-analytical model, dynamic amplification, transient ground deformation (TGD), modal analysis", "url": "http://arxiv.org/abs/2507.22850v2", "summary": "Underground infrastructure, such as pipelines and tunnels, can be vulnerable\nto the effect of transient ground deformation (TGD) caused by different\nvibration sources, including earthquakes and traffic loads. Current design\nmethods are based on simple analytical models that idealize the soil movement\nas a traveling sinusoidal wave, neglecting both the system inertia and the\nrelative displacement at the soil-structure interface. However, this assumption\nmay not be valid for buried large diameter pipelines and tunnels requiring\naccurate dynamic analysis. To analyse the dynamic response of a buried straight\nbeam subjected to transverse TGD, this study introduces a new semi-analytical\nmodel based on the Timoshenko beam on Winkler foundation theory. The\nclosed-form analytical solution revealed that the vibration spectrum is divided\nin four parts, separated by three transition frequencies. Across each\ntransition frequency, the oscillatory characteristics of the vibration modes\nchange, significantly affecting the dynamic response of the system. To verify\nthe validity of the proposed model, this work analyses the case study of a\nburied steel water pipeline of varying lengths and operating conditions,\nsubjected to transverse TGD. Comparison of the obtained analytical solutions\nwith the finite element analysis results showed excellent agreement between the\ntwo approaches. The frequency response analysis revealed dynamic amplification\nof the soil-structure interaction for forcing frequencies near the system's\nfundamental frequency. These may fall within the range of dominant frequencies\ncharacterizing seismic waves, requiring accurate dynamic analysis. The proposed\nmethodology provides a robust analytical framework for evaluating the primary\nfactors impacting the dynamic behavior of buried beams, giving a deeper\nunderstanding of the system response under various sources of ground vibration.", "comment": "Key words: buried Timoshenko beam, semi-analytical model, dynamic\n  amplification, transient ground deformation (TGD), modal analysis", "pdf_url": "http://arxiv.org/pdf/2507.22850v2", "cate": "math.NA", "date": "2025-07-30", "updated": "2025-08-01"}
{"id": "2508.00228", "title": "Benchmarking XRootD-HTTPS on 400Gbps Links with Variable Latencies", "authors": ["Aashay Arora", "Diego Davila", "Frank Würthwein", "John Graham", "Dima Mishin", "Justas Balcas", "Tom Lehman", "Xi Yang", "Chin Guok", "Harvey Newman"], "categories": ["cs.NI"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "Comments:      Submitted to CHEP 24", "url": "http://arxiv.org/abs/2508.00228v1", "summary": "In anticipation of the High Luminosity-LHC era, there is a critical need to\noversee software readiness for upcoming growth in network traffic for\nproduction and user data analysis access. This paper looks into software and\nhardware required improvements in US-CMS Tier-2 sites to be able to sustain and\nmeet the projected 400 Gbps bandwidth demands while tackling the challenge\nposed by varying latencies between sites. Specifically, our study focuses on\nidentifying the performance of XRootD HTTP third-party copies across multiple\n400 Gbps links and exploring different host and transfer configurations. Our\napproach involves systematic testing with variations in the number of origins\nper cluster and CPU allocations for each origin. By replicating real network\nconditions and creating network \"loops\" that traverse multiple switches across\nthe wide area network, we are able to replicate authentic network conditions", "comment": "Submitted to CHEP 24", "pdf_url": "http://arxiv.org/pdf/2508.00228v1", "cate": "cs.NI", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2505.06275", "title": "SinBasis Networks: Matrix-Equivalent Feature Extraction for Wave-Like Optical Spectrograms", "authors": ["Yuzhou Zhu", "Zheng Zhang", "Ruyi Zhang", "Liang Zhou"], "categories": ["cs.LG", "cs.AI", "cs.CV", "physics.optics"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.06275v2", "summary": "Wave-like images-from attosecond streaking spectrograms to optical spectra,\naudio mel-spectrograms and periodic video frames-encode critical harmonic\nstructures that elude conventional feature extractors. We propose a unified,\nmatrix-equivalent framework that reinterprets convolution and attention as\nlinear transforms on flattened inputs, revealing filter weights as basis\nvectors spanning latent feature subspaces. To infuse spectral priors we apply\nelementwise $\\sin(\\cdot)$ mappings to each weight matrix. Embedding these\ntransforms into CNN, ViT and Capsule architectures yields Sin-Basis Networks\nwith heightened sensitivity to periodic motifs and built-in invariance to\nspatial shifts. Experiments on a diverse collection of wave-like image\ndatasets-including 80,000 synthetic attosecond streaking spectrograms,\nthousands of Raman, photoluminescence and FTIR spectra, mel-spectrograms from\nAudioSet and cycle-pattern frames from Kinetics-demonstrate substantial gains\nin reconstruction accuracy, translational robustness and zero-shot cross-domain\ntransfer. Theoretical analysis via matrix isomorphism and Mercer-kernel\ntruncation quantifies how sinusoidal reparametrization enriches expressivity\nwhile preserving stability in data-scarce regimes. Sin-Basis Networks thus\noffer a lightweight, physics-informed approach to deep learning across all\nwave-form imaging modalities.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.06275v2", "cate": "cs.LG", "date": "2025-05-06", "updated": "2025-07-31"}
{"id": "2507.23129", "title": "MRpro - open PyTorch-based MR reconstruction and processing package", "authors": ["Felix Frederik Zimmermann", "Patrick Schuenke", "Christoph S. Aigner", "Bill A. Bernhardt", "Mara Guastini", "Johannes Hammacher", "Noah Jaitner", "Andreas Kofler", "Leonid Lunin", "Stefan Martin", "Catarina Redshaw Kranich", "Jakob Schattenfroh", "David Schote", "Yanglei Wu", "Christoph Kolbitsch"], "categories": ["eess.IV", "cs.CV", "physics.med-ph"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "Comments:      Submitted to Magnetic Resonance in Medicine", "url": "http://arxiv.org/abs/2507.23129v1", "summary": "We introduce MRpro, an open-source image reconstruction package built upon\nPyTorch and open data formats. The framework comprises three main areas. First,\nit provides unified data structures for the consistent manipulation of MR\ndatasets and their associated metadata (e.g., k-space trajectories). Second, it\noffers a library of composable operators, proximable functionals, and\noptimization algorithms, including a unified Fourier operator for all common\ntrajectories and an extended phase graph simulation for quantitative MR. These\ncomponents are used to create ready-to-use implementations of key\nreconstruction algorithms. Third, for deep learning, MRpro includes essential\nbuilding blocks such as data consistency layers, differentiable optimization\nlayers, and state-of-the-art backbone networks and integrates public datasets\nto facilitate reproducibility. MRpro is developed as a collaborative project\nsupported by automated quality control. We demonstrate the versatility of MRpro\nacross multiple applications, including Cartesian, radial, and spiral\nacquisitions; motion-corrected reconstruction; cardiac MR fingerprinting;\nlearned spatially adaptive regularization weights; model-based learned image\nreconstruction and quantitative parameter estimation. MRpro offers an\nextensible framework for MR image reconstruction. With reproducibility and\nmaintainability at its core, it facilitates collaborative development and\nprovides a foundation for future MR imaging research.", "comment": "Submitted to Magnetic Resonance in Medicine", "pdf_url": "http://arxiv.org/pdf/2507.23129v1", "cate": "eess.IV", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2404.00523", "title": "The algebraic structure of hyperinterpolation class on the sphere", "authors": ["Congpei An", "Jiashu Ran"], "categories": ["math.FA", "cs.NA", "math.NA", "41A10, 41A36, 47L20, 47L80"], "primary_category": "Subjects:       Functional Analysis (math.FA)", "pdf_link": null, "comments": "Comments:      16 pages, 1 figure", "url": "http://arxiv.org/abs/2404.00523v2", "summary": "This paper investigates the algebraic properties of the hyperinterpolation\nclass $\\mathbf{HC}(\\mathbb{S}^d)$ on the unit sphere $ \\mathbb{S}^d $. We focus\non operators derived from the classical hyperinterpolation with bounded $ L_2 $\noperator norms. By utilizing a discrete (semi) inner product framework, we\ndevelop the theory of hyper self-adjoint operators, hyper projection operators,\nand hyper semigroups. We analyze four specific operators: filtered, Lasso, hard\nthresholding, and generalized hyperinterpolations. We prove that the\ngeneralized hyperinterpolation operator is hyper self-adjoint and commutative\nwith the hyperinterpolation operator. Additionally, we demonstrate that hard\nthresholding and classical hyperinterpolation operators form a hyper semigroup,\nwith hard thresholding hyperinterpolation constituting the minimal prime hyper\nideal. Finally, we establish that hyperinterpolation operators act as hyper\nhomomorphisms on the hyper semigroup.", "comment": "16 pages, 1 figure", "pdf_url": "http://arxiv.org/pdf/2404.00523v2", "cate": "math.FA", "date": "2024-03-31", "updated": "2025-08-01"}
{"id": "2508.00234", "title": "Quality-of-Service Aware LLM Routing for Edge Computing with Multiple Experts", "authors": ["Jin Yang", "Qiong Wu", "Zhiying Feng", "Zhi Zhou", "Deke Guo", "Xu Chen"], "categories": ["cs.NI", "cs.AI", "cs.DC", "cs.MA"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "Comments:      Accepted by IEEE Transactions on Mobile Computing", "url": "http://arxiv.org/abs/2508.00234v1", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities,\nleading to a significant increase in user demand for LLM services. However,\ncloud-based LLM services often suffer from high latency, unstable\nresponsiveness, and privacy concerns. Therefore, multiple LLMs are usually\ndeployed at the network edge to boost real-time responsiveness and protect data\nprivacy, particularly for many emerging smart mobile and IoT applications.\nGiven the varying response quality and latency of LLM services, a critical\nissue is how to route user requests from mobile and IoT devices to an\nappropriate LLM service (i.e., edge LLM expert) to ensure acceptable\nquality-of-service (QoS). Existing routing algorithms fail to simultaneously\naddress the heterogeneity of LLM services, the interference among requests, and\nthe dynamic workloads necessary for maintaining long-term stable QoS. To meet\nthese challenges, in this paper we propose a novel deep reinforcement learning\n(DRL)-based QoS-aware LLM routing framework for sustained high-quality LLM\nservices. Due to the dynamic nature of the global state, we propose a dynamic\nstate abstraction technique to compactly represent global state features with a\nheterogeneous graph attention network (HAN). Additionally, we introduce an\naction impact estimator and a tailored reward function to guide the DRL agent\nin maximizing QoS and preventing latency violations. Extensive experiments on\nboth Poisson and real-world workloads demonstrate that our proposed algorithm\nsignificantly improves average QoS and computing resource efficiency compared\nto existing baselines.", "comment": "Accepted by IEEE Transactions on Mobile Computing", "pdf_url": "http://arxiv.org/pdf/2508.00234v1", "cate": "cs.NI", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00604", "title": "Composable OS Kernel Architectures for Autonomous Intelligence", "authors": ["Rajpreet Singh", "Vidhi Kothari"], "categories": ["cs.OS", "cs.AI"], "primary_category": "Subjects:       Operating Systems (cs.OS)", "pdf_link": null, "comments": "Comments:      8 pages", "url": "http://arxiv.org/abs/2508.00604v1", "summary": "As intelligent systems permeate edge devices, cloud infrastructure, and\nembedded real-time environments, this research proposes a new OS kernel\narchitecture for intelligent systems, transforming kernels from static resource\nmanagers to adaptive, AI-integrated platforms. Key contributions include: (1)\ntreating Loadable Kernel Modules (LKMs) as AI-oriented computation units for\nfast sensory and cognitive processing in kernel space; (2) expanding the Linux\nkernel into an AI-native environment with built-in deep learning inference,\nfloating-point acceleration, and real-time adaptive scheduling for efficient ML\nworkloads; and (3) introducing a Neurosymbolic kernel design leveraging\nCategory Theory and Homotopy Type Theory to unify symbolic reasoning and\ndifferentiable logic within OS internals. Together, these approaches enable\noperating systems to proactively anticipate and adapt to the cognitive needs of\nautonomous intelligent applications.", "comment": "8 pages", "pdf_url": "http://arxiv.org/pdf/2508.00604v1", "cate": "cs.OS", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2506.10006", "title": "HER2 Expression Prediction with Flexible Multi-Modal Inputs via Dynamic Bidirectional Reconstruction", "authors": ["Jie Qin", "Wei Yang", "Yan Su", "Yiran Zhu", "Weizhen Li", "Yunyue Pan", "Chengchang Pan", "Honggang Qi"], "categories": ["cs.MM", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "Subjects:       Multimedia (cs.MM)", "pdf_link": null, "comments": "Comments:      8 pages,6 figures,3 tables,accepted by the 33rd ACM International Conference on Multimedia(ACM MM 2025)", "url": "http://arxiv.org/abs/2506.10006v2", "summary": "In breast cancer HER2 assessment, clinical evaluation relies on combined H&E\nand IHC images, yet acquiring both modalities is often hindered by clinical\nconstraints and cost. We propose an adaptive bimodal prediction framework that\nflexibly supports single- or dual-modality inputs through two core innovations:\na dynamic branch selector activating modality completion or joint inference\nbased on input availability, and a cross-modal GAN (CM-GAN) enabling\nfeature-space reconstruction of missing modalities. This design dramatically\nimproves H&E-only accuracy from 71.44% to 94.25%, achieves 95.09% with full\ndual-modality inputs, and maintains 90.28% reliability under single-modality\nconditions. The \"dual-modality preferred, single-modality compatible\"\narchitecture delivers near-dual-modality accuracy without mandatory\nsynchronized acquisition, offering a cost-effective solution for\nresource-limited regions and significantly improving HER2 assessment\naccessibility.", "comment": "8 pages,6 figures,3 tables,accepted by the 33rd ACM International\n  Conference on Multimedia(ACM MM 2025)", "pdf_url": "http://arxiv.org/pdf/2506.10006v2", "cate": "cs.MM", "date": "2025-04-12", "updated": "2025-07-31"}
{"id": "2507.23219", "title": "Learning Arbitrary-Scale RAW Image Downscaling with Wavelet-based Recurrent Reconstruction", "authors": ["Yang Ren", "Hai Jiang", "Wei Li", "Menglong Yang", "Heng Zhang", "Zehua Sheng", "Qingsheng Ye", "Shuaicheng Liu"], "categories": ["eess.IV", "cs.CV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "Comments:      Accepted by ACM MM 2025", "url": "http://arxiv.org/abs/2507.23219v1", "summary": "Image downscaling is critical for efficient storage and transmission of\nhigh-resolution (HR) images. Existing learning-based methods focus on\nperforming downscaling within the sRGB domain, which typically suffers from\nblurred details and unexpected artifacts. RAW images, with their unprocessed\nphotonic information, offer greater flexibility but lack specialized\ndownscaling frameworks. In this paper, we propose a wavelet-based recurrent\nreconstruction framework that leverages the information lossless attribute of\nwavelet transformation to fulfill the arbitrary-scale RAW image downscaling in\na coarse-to-fine manner, in which the Low-Frequency Arbitrary-Scale Downscaling\nModule (LASDM) and the High-Frequency Prediction Module (HFPM) are proposed to\npreserve structural and textural integrity of the reconstructed low-resolution\n(LR) RAW images, alongside an energy-maximization loss to align high-frequency\nenergy between HR and LR domain. Furthermore, we introduce the Realistic\nNon-Integer RAW Downscaling (Real-NIRD) dataset, featuring a non-integer\ndownscaling factor of 1.3$\\times$, and incorporate it with publicly available\ndatasets with integer factors (2$\\times$, 3$\\times$, 4$\\times$) for\ncomprehensive benchmarking arbitrary-scale image downscaling purposes.\nExtensive experiments demonstrate that our method outperforms existing\nstate-of-the-art competitors both quantitatively and visually. The code and\ndataset will be released at https://github.com/RenYangSCU/ASRD.", "comment": "Accepted by ACM MM 2025", "pdf_url": "http://arxiv.org/pdf/2507.23219v1", "cate": "eess.IV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2405.07961", "title": "A potential theory on weighted graphs", "authors": ["Trent DeGiovanni", "Fernando Guevara Vasquez"], "categories": ["math.PR", "cs.NA", "math.NA", "31C20, 65M80, 31B10"], "primary_category": "Subjects:       Probability (math.PR)", "pdf_link": null, "comments": "Comments:      30 pages, 7 figures", "url": "http://arxiv.org/abs/2405.07961v2", "summary": "We present an analog to classic potential theory on weighted graphs. With\nnodes partitioned into exterior, boundary and interior nodes and an appropriate\ndecomposition of the Laplacian, we define discrete analogues to the trace\noperators, the single and double layer potential operators, and the boundary\nlayer operators. As in the continuum, these operators can represent exterior or\ninterior harmonic functions with different boundary conditions. The formalism\nwe introduce includes a discrete Calder\\'on calculus and brings some well known\nresults from potential theory to weighted graphs, e.g. on the spectrum of the\nNeumann-Poincar\\'e operator. We illustrate the formalism with a cloaking\nstrategy on weighted graphs which allows to hide an anomaly from the\nperspective of electrical measurements made away from the anomaly.", "comment": "30 pages, 7 figures", "pdf_url": "http://arxiv.org/pdf/2405.07961v2", "cate": "math.PR", "date": "2024-05-13", "updated": "2025-08-01"}
{"id": "2508.00256", "title": "Large AI Model-Enabled Secure Communications in Low-Altitude Wireless Networks: Concepts, Perspectives and Case Study", "authors": ["Chuang Zhang", "Geng Sun", "Jiacheng Wang", "Yijing Lin", "Weijie Yuan", "Sinem Coleri", "Dusit Niyato", "Tony Q. S. Quek"], "categories": ["cs.NI", "cs.AI"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "Comments:      This paper has been submitted to IEEE Communications Magazine for consideration", "url": "http://arxiv.org/abs/2508.00256v1", "summary": "Low-altitude wireless networks (LAWNs) have the potential to revolutionize\ncommunications by supporting a range of applications, including urban parcel\ndelivery, aerial inspections and air taxis. However, compared with traditional\nwireless networks, LAWNs face unique security challenges due to low-altitude\noperations, frequent mobility and reliance on unlicensed spectrum, making it\nmore vulnerable to some malicious attacks. In this paper, we investigate some\nlarge artificial intelligence model (LAM)-enabled solutions for secure\ncommunications in LAWNs. Specifically, we first explore the amplified security\nrisks and important limitations of traditional AI methods in LAWNs. Then, we\nintroduce the basic concepts of LAMs and delve into the role of LAMs in\naddressing these challenges. To demonstrate the practical benefits of LAMs for\nsecure communications in LAWNs, we propose a novel LAM-based optimization\nframework that leverages large language models (LLMs) to generate enhanced\nstate features on top of handcrafted representations, and to design intrinsic\nrewards accordingly, thereby improving reinforcement learning performance for\nsecure communication tasks. Through a typical case study, simulation results\nvalidate the effectiveness of the proposed framework. Finally, we outline\nfuture directions for integrating LAMs into secure LAWN applications.", "comment": "This paper has been submitted to IEEE Communications Magazine for\n  consideration", "pdf_url": "http://arxiv.org/pdf/2508.00256v1", "cate": "cs.NI", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00629", "title": "Energy-Aware CPU Orchestration in O-RAN: A dApp-Driven Lightweight Approach", "authors": ["Francisco Crespo", "Javier Villegas", "Carlos Baena", "Eduardo Baena", "Sergio Fortes", "Raquel Barco"], "categories": ["cs.NI", "cs.OS", "cs.PF"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00629v1", "summary": "The transition toward softwarized Radio Access Networks (RANs), driven by the\nOpen RAN (O-RAN) paradigm, enables flexible, vendor-neutral deployments through\ndisaggregation and virtualization of base station functions. However, this\nshift introduces new challenges in managing CPU resources efficiently under\nstrict real-time constraints. In particular, the interplay between\nlatency-sensitive RAN workloads and general-purpose Operating System (OS)\nschedulers often leads to sub-optimal performance and unnecessary energy\nconsumption. This work proposes a lightweight, programmable distributed\napplication (dApp) deployed at the Distributed Unit (DU) level to dynamically\norchestrate CPU usage. The dApp operates in closed loop with the OS, leveraging\nthread-level telemetry like context switches, Instructions Per Cycle (IPC), and\ncache metrics, to adapt CPU thread affinity, core isolation, and frequency\nscaling in real time. Unlike existing solutions, it requires no access to\nproprietary RAN software, hardware-specific features, or kernel modifications.\nFully compliant with the O-RAN architecture and agnostic to the underlying RAN\nstack, the proposed solution introduces negligible overhead while improving\nenergy efficiency and CPU utilization. Experimental results using a\ncommercial-grade srsRAN deployment demonstrate consistent power savings without\ncompromising real-time processing performance, highlighting the potential of\nlow-latency dApps for fine-grained resource control in next-generation networks", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00629v1", "cate": "cs.NI", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2506.17247", "title": "Recursive Learning-Based Virtual Buffering for Analytical Global Placement", "authors": ["Andrew B. Kahng", "Yiting Liu", "Zhiang Wang"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.17247v2", "summary": "Due to the skewed scaling of interconnect versus cell delay in modern\ntechnology nodes, placement with buffer porosity (i.e., cell density) awareness\nis essential for timing closure in physical synthesis flows. However, existing\napproaches face two key challenges: (i) traditional van Ginneken-Lillis-style\nbuffering approaches are computationally expensive during global placement; and\n(ii) machine learning-based approaches, such as BufFormer, lack a thorough\nconsideration of Electrical Rule Check (ERC) violations and fail to \"close the\nloop\" back into the physical design flow. In this work, we propose\nMLBuf-RePlAce, the first open-source learning-driven virtual buffering-aware\nanalytical global placement framework, built on top of the OpenROAD\ninfrastructure. MLBuf-RePlAce adopts an efficient recursive learning-based\ngenerative buffering approach to predict buffer types and locations, addressing\nERC violations during global placement. We compare MLBuf-RePlAce against the\ndefault virtual buffering-based timing-driven global placer in OpenROAD, using\nopen-source testcases from the TILOS MacroPlacement and OpenROAD-flow-scripts\nrepositories. Without degradation of post-route power, MLBuf-RePlAce achieves\n(maximum, average) improvements of (56%, 31%) in total negative slack (TNS)\nwithin the open-source OpenROAD flow. When evaluated by completion in a\ncommercial flow, MLBuf-RePlAce achieves (maximum, average) improvements of\n(53%, 28%) in TNS with an average of 0.2% improvement in post-route power.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.17247v2", "cate": "cs.LG", "date": "2025-06-07", "updated": "2025-07-30"}
{"id": "2507.23256", "title": "EMedNeXt: An Enhanced Brain Tumor Segmentation Framework for Sub-Saharan Africa using MedNeXt V2 with Deep Supervision", "authors": ["Ahmed Jaheen", "Abdelrahman Elsayed", "Damir Kim", "Daniil Tikhonov", "Matheus Scatolin", "Mohor Banerjee", "Qiankun Ji", "Mostafa Salem", "Hu Wang", "Sarim Hashmi", "Mohammad Yaqub"], "categories": ["eess.IV", "cs.CV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "Comments:      Submitted to the BraTS-Lighthouse 2025 Challenge (MICCAI 2025)", "url": "http://arxiv.org/abs/2507.23256v1", "summary": "Brain cancer affects millions worldwide, and in nearly every clinical\nsetting, doctors rely on magnetic resonance imaging (MRI) to diagnose and\nmonitor gliomas. However, the current standard for tumor quantification through\nmanual segmentation of multi-parametric MRI is time-consuming, requires expert\nradiologists, and is often infeasible in under-resourced healthcare systems.\nThis problem is especially pronounced in low-income regions, where MRI scanners\nare of lower quality and radiology expertise is scarce, leading to incorrect\nsegmentation and quantification. In addition, the number of acquired MRI scans\nin Africa is typically small. To address these challenges, the BraTS-Lighthouse\n2025 Challenge focuses on robust tumor segmentation in sub-Saharan Africa\n(SSA), where resource constraints and image quality degradation introduce\nsignificant shifts. In this study, we present EMedNeXt -- an enhanced brain\ntumor segmentation framework based on MedNeXt V2 with deep supervision and\noptimized post-processing pipelines tailored for SSA. EMedNeXt introduces three\nkey contributions: a larger region of interest, an improved nnU-Net v2-based\narchitectural skeleton, and a robust model ensembling system. Evaluated on the\nhidden validation set, our solution achieved an average LesionWise DSC of 0.897\nwith an average LesionWise NSD of 0.541 and 0.84 at a tolerance of 0.5 mm and\n1.0 mm, respectively.", "comment": "Submitted to the BraTS-Lighthouse 2025 Challenge (MICCAI 2025)", "pdf_url": "http://arxiv.org/pdf/2507.23256v1", "cate": "eess.IV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2409.19974", "title": "Adaptive Mesh Refinement for Two-Phase Viscoelastic Fluid Mixture Models", "authors": ["Bindi M. Nagda", "Aaron Barrett", "Boyce E. Griffith", "Aaron L. Fogelson", "Jian Du"], "categories": ["physics.flu-dyn", "cs.NA", "math.NA", "76-10, 76T06", "G.1.8; G.4"], "primary_category": "Subjects:       Fluid Dynamics (physics.flu-dyn)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2409.19974v3", "summary": "Multiphase flows are an important class of fluid flow and their study\nfacilitates the development of diverse applications in industrial, natural, and\nbiomedical systems. We consider a model that uses a continuum description of\nboth phases in which separate momentum equations are used for each phase along\nwith a co-incompressibility condition on the velocity fields. The resulting\nsystem of equations poses numerical challenges due to the presence of multiple\nnon-linear terms and the co-incompressibility condition, and the resulting\nfluid dynamics motivate the development of an adaptive mesh refinement (AMR)\ntechnique to accurately capture regions of high stresses and large material\ngradients while keeping computational costs low. We present an accurate,\nrobust, and efficient computational method for simulating multiphase mixtures\non adaptive grids, and utilize a multigrid solver to precondition the\nsaddle-point system. We demonstrate that the AMR discretization asymptotically\napproaches second order accuracy in $L^1$, $L^2$ and $L^\\infty$ norms. The\nsolver can accurately resolve sharp gradients in the solution and, with the\nmultigrid preconditioning strategy introduced herein, the linear solver\niterations are independent of grid spacing. Our AMR solver offers a major cost\nsavings benefit, providing up to ten fold speedup over a uniform grid in the\nnumerical experiments presented here, with greater speedup possible depending\non the problem set-up.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2409.19974v3", "cate": "physics.flu-dyn", "date": "2024-09-30", "updated": "2025-08-01"}
{"id": "2508.00261", "title": "Energy Efficient Trajectory Control and Resource Allocation in Multi-UAV-assisted MEC via Deep Reinforcement Learning", "authors": ["Saichao Liu", "Geng Sun", "Chuang Zhang", "Xuejie Liu", "Jiacheng Wang", "Changyuan Zhao", "Dusit Niyato"], "categories": ["cs.NI", "eess.SP"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "Comments:      This paper has been accepted by IEEE GLOBECOM 2025", "url": "http://arxiv.org/abs/2508.00261v1", "summary": "Mobile edge computing (MEC) is a promising technique to improve the\ncomputational capacity of smart devices (SDs) in Internet of Things (IoT).\nHowever, the performance of MEC is restricted due to its fixed location and\nlimited service scope. Hence, we investigate an unmanned aerial vehicle\n(UAV)-assisted MEC system, where multiple UAVs are dispatched and each UAV can\nsimultaneously provide computing service for multiple SDs. To improve the\nperformance of system, we formulated a UAV-based trajectory control and\nresource allocation multi-objective optimization problem (TCRAMOP) to\nsimultaneously maximize the offloading number of UAVs and minimize total\noffloading delay and total energy consumption of UAVs by optimizing the flight\npaths of UAVs as well as the computing resource allocated to served SDs. Then,\nconsider that the solution of TCRAMOP requires continuous decision-making and\nthe system is dynamic, we propose an enhanced deep reinforcement learning (DRL)\nalgorithm, namely, distributed proximal policy optimization with imitation\nlearning (DPPOIL). This algorithm incorporates the generative adversarial\nimitation learning technique to improve the policy performance. Simulation\nresults demonstrate the effectiveness of our proposed DPPOIL and prove that the\nlearned strategy of DPPOIL is better compared with other baseline methods.", "comment": "This paper has been accepted by IEEE GLOBECOM 2025", "pdf_url": "http://arxiv.org/pdf/2508.00261v1", "cate": "cs.NI", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2507.15857", "title": "Diffusion Beats Autoregressive in Data-Constrained Settings", "authors": ["Mihir Prabhudesai", "Mengning Wu", "Amir Zadeh", "Katerina Fragkiadaki", "Deepak Pathak"], "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.RO"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Project Webpage: this https URL", "url": "http://arxiv.org/abs/2507.15857v4", "summary": "Autoregressive (AR) models have long dominated the landscape of large\nlanguage models, driving progress across a wide range of tasks. Recently,\ndiffusion-based language models have emerged as a promising alternative, though\ntheir advantages over AR models remain underexplored. In this paper, we\nsystematically study masked diffusion models in data-constrained settings-where\ntraining involves repeated passes over limited data-and find that they\nsignificantly outperform AR models when compute is abundant but data is scarce.\nDiffusion models make better use of repeated data, achieving lower validation\nloss and superior downstream performance. We interpret this advantage as\nimplicit data augmentation: masked diffusion exposes the model to a diverse\ndistribution of token orderings and prediction tasks, unlike AR's fixed\nleft-to-right factorization. We find new scaling laws for diffusion models and\nderive a closed-form expression for the critical compute threshold at which\ndiffusion begins to outperform AR. These results suggest that when data, not\ncompute, is the bottleneck, diffusion models offer a compelling alternative to\nthe standard AR paradigm. Our code is available at:\nhttps://diffusion-scaling.github.io.", "comment": "Project Webpage: https://diffusion-scaling.github.io", "pdf_url": "http://arxiv.org/pdf/2507.15857v4", "cate": "cs.LG", "date": "2025-07-21", "updated": "2025-07-31"}
{"id": "2507.23273", "title": "GSFusion:Globally Optimized LiDAR-Inertial-Visual Mapping for Gaussian Splatting", "authors": ["Jaeseok Park", "Chanoh Park", "Minsu Kim", "Soohwan Kim"], "categories": ["cs.RO", "cs.CV"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23273v1", "summary": "While 3D Gaussian Splatting (3DGS) has revolutionized photorealistic mapping,\nconventional approaches based on camera sensor, even RGB-D, suffer from\nfundamental limitations such as high computational load, failure in\nenvironments with poor texture or illumination, and short operational ranges.\nLiDAR emerges as a robust alternative, but its integration with 3DGS introduces\nnew challenges, such as the need for exceptional global alignment for\nphotorealistic quality and prolonged optimization times caused by sparse data.\nTo address these challenges, we propose GSFusion, an online\nLiDAR-Inertial-Visual mapping system that ensures high-precision map\nconsistency through a surfel-to-surfel constraint in the global pose-graph\noptimization. To handle sparse data, our system employs a pixel-aware Gaussian\ninitialization strategy for efficient representation and a bounded sigmoid\nconstraint to prevent uncontrolled Gaussian growth. Experiments on public and\nour datasets demonstrate our system outperforms existing 3DGS SLAM systems in\nterms of rendering quality and map-building efficiency.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23273v1", "cate": "cs.RO", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2505.00288", "title": "Nyström Type Exponential Integrators for Strongly Magnetized Charged Particle Dynamics", "authors": ["Tri P. Nguyen", "Ilon Joseph", "Mayya Tokman"], "categories": ["physics.comp-ph", "cs.NA", "math.NA", "physics.plasm-ph", "65L04, 78A35"], "primary_category": "Subjects:       Computational Physics (physics.comp-ph)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.00288v5", "summary": "Calculating the dynamics of charged particles in electromagnetic fields (i.e.\nthe particle pushing problem) is one of the most computationally intensive\ncomponents of particle-in-cell (PIC) methods for plasma physics simulations.\nThis task is especially challenging when the plasma is strongly magnetized,\nsince in this case the particle motion consists of a wide range of temporal\nscales from highly oscillatory fast gyromotion to slow macroscopic behavior and\nthe resulting numerical model is very stiff. Current state-of-the-art time\nintegrators used to simulate particle motion have limitations given the severe\nnumerical stiffness of the problem and more efficient methods are of interest.\nRecently, exponential integrators have been proposed as a promising new\napproach for these simulations and shown to offer computational advantages over\ncommonly used schemes. Exponential methods can solve linear problems exactly\nand are A-stable. In this paper, the standard exponential algorithms framework\nis extended to derive Nystr\\\"om-type exponential methods that integrate the\nNewtonian equations of motion as a second-order differential equation. Specific\nNystr\\\"om-type schemes of second and third orders are derived and applied to\nstrongly magnetized particle pushing problems. Numerical experiments are\npresented to demonstrate that the Nystr\\\"om-type exponential integrators can\nprovide significant improvement in computational efficiency over the standard\nexponential methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.00288v5", "cate": "physics.comp-ph", "date": "2025-05-01", "updated": "2025-08-01"}
{"id": "2508.00403", "title": "Mamba for Wireless Communications and Networking: Principles and Opportunities", "authors": ["Rongsheng Zhang", "Ruichen Zhang", "Yang Lu", "Wei Chen", "Bo Ai", "Dusit Niyato"], "categories": ["cs.NI"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00403v1", "summary": "Mamba has emerged as a powerful model for efficiently addressing tasks\ninvolving temporal and spatial data. Regarding the escalating heterogeneity and\ndynamics in wireless networks, Mamba holds the potential to revolutionize\nwireless communication and networking designs by balancing the trade-off\nbetween computational efficiency and effectiveness. This article presents a\ncomprehensive overview of Mamba' applications in wireless systems.\nSpecifically, we first analyze the potentials of Mamba for wireless signal\nprocessing tasks from the perspectives of long-range dependency modeling and\nspatial feature extraction. Then we propose two application frameworks for\nMamba in wireless communications, i.e., replacement of traditional algorithms,\nand enabler of novel paradigms. Guided by the two frameworks, we conduct case\nstudies on intelligent resource allocation and joint source and channel\ndecoding to demonstrate Mamba's improvements in both feature enhancement and\ncomputational efficiency. Finally, we highlight critical challenges and outline\npotential research directions for Mamba in wireless communications and\nnetworking.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00403v1", "cate": "cs.NI", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00305", "title": "Systematic Evaluation of Optimization Techniques for Long-Context Language Models", "authors": ["Ammar Ahmed", "Sheng Di", "Franck Cappello", "Zirui Liu", "Jingoo Han", "Ali Anwar"], "categories": ["cs.CL", "cs.LG", "cs.PF"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00305v1", "summary": "Large language models (LLMs) excel across diverse natural language processing\ntasks but face resource demands and limited context windows. Although\ntechniques like pruning, quantization, and token dropping can mitigate these\nissues, their efficacy in long-context scenarios and system evaluation remains\nunderexplored. This paper systematically benchmarks these optimizations,\ncharacterizing memory usage, latency, and throughput, and studies how these\nmethods impact the quality of text generation. We first analyze individual\noptimization methods for two LLM architectures supporting long context and then\nsystematically evaluate combinations of these techniques to assess how this\ndeeper analysis impacts performance metrics. We subsequently study the\nscalability of individual optimization methods on a larger variant with 70\nbillion-parameter model. Our novel insights reveal that naive combination\ninference optimization algorithms can adversely affect larger models due to\ncompounded approximation errors, as compared to their smaller counterparts.\nExperiments show that relying solely on F1 obscures these effects by hiding\nprecision-recall trade-offs in question answering tasks. By integrating\nsystem-level profiling with task-specific insights, this study helps LLM\npractitioners and researchers explore and balance efficiency, accuracy, and\nscalability across tasks and hardware configurations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00305v1", "cate": "cs.CL", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2507.17745", "title": "Ultra3D: Efficient and High-Fidelity 3D Generation with Part Attention", "authors": ["Yiwen Chen", "Zhihao Li", "Yikai Wang", "Hu Zhang", "Qin Li", "Chi Zhang", "Guosheng Lin"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Project Page: this https URL", "url": "http://arxiv.org/abs/2507.17745v3", "summary": "Recent advances in sparse voxel representations have significantly improved\nthe quality of 3D content generation, enabling high-resolution modeling with\nfine-grained geometry. However, existing frameworks suffer from severe\ncomputational inefficiencies due to the quadratic complexity of attention\nmechanisms in their two-stage diffusion pipelines. In this work, we propose\nUltra3D, an efficient 3D generation framework that significantly accelerates\nsparse voxel modeling without compromising quality. Our method leverages the\ncompact VecSet representation to efficiently generate a coarse object layout in\nthe first stage, reducing token count and accelerating voxel coordinate\nprediction. To refine per-voxel latent features in the second stage, we\nintroduce Part Attention, a geometry-aware localized attention mechanism that\nrestricts attention computation within semantically consistent part regions.\nThis design preserves structural continuity while avoiding unnecessary global\nattention, achieving up to 6.7x speed-up in latent generation. To support this\nmechanism, we construct a scalable part annotation pipeline that converts raw\nmeshes into part-labeled sparse voxels. Extensive experiments demonstrate that\nUltra3D supports high-resolution 3D generation at 1024 resolution and achieves\nstate-of-the-art performance in both visual fidelity and user preference.", "comment": "Project Page: https://buaacyw.github.io/ultra3d/", "pdf_url": "http://arxiv.org/pdf/2507.17745v3", "cate": "cs.CV", "date": "2025-07-23", "updated": "2025-07-31"}
{"id": "2507.23359", "title": "Pixel Embedding Method for Tubular Neurite Segmentation", "authors": ["Huayu Fu", "Jiamin Li", "Haozhi Qu", "Xiaolin Hu", "Zengcai Guo"], "categories": ["eess.IV", "cs.CV", "q-bio.NC"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23359v1", "summary": "Automatic segmentation of neuronal topology is critical for handling large\nscale neuroimaging data, as it can greatly accelerate neuron annotation and\nanalysis. However, the intricate morphology of neuronal branches and the\nocclusions among fibers pose significant challenges for deep learning based\nsegmentation. To address these issues, we propose an improved framework: First,\nwe introduce a deep network that outputs pixel level embedding vectors and\ndesign a corresponding loss function, enabling the learned features to\neffectively distinguish different neuronal connections within occluded regions.\nSecond, building on this model, we develop an end to end pipeline that directly\nmaps raw neuronal images to SWC formatted neuron structure trees. Finally,\nrecognizing that existing evaluation metrics fail to fully capture segmentation\naccuracy, we propose a novel topological assessment metric to more\nappropriately quantify the quality of neuron segmentation and reconstruction.\nExperiments on our fMOST imaging dataset demonstrate that, compared to several\nclassical methods, our approach significantly reduces the error rate in\nneuronal topology reconstruction.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23359v1", "cate": "eess.IV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2505.22480", "title": "Hybrid High-Order formulations with turbulence modelling capabilities for incompressible flow problems", "authors": ["Lorenzo Botti", "Daniele Antonio Di Pietro", "Francesco Carlo Massa"], "categories": ["physics.flu-dyn", "cs.NA", "math.NA"], "primary_category": "Subjects:       Fluid Dynamics (physics.flu-dyn)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.22480v2", "summary": "We propose a Hybrid High-Order (HHO) formulation of the incompressible\nNavier--Stokes equations, that is well suited to be employed for the simulation\nof turbulent flows. The spatial discretization relies on hybrid velocity and\npressure spaces and the temporal discretization is based on Explicit Singly\nDiagonal Implicit Runge-Kutta (ESDIRK) methods. The formulation possesses some\nattractive features that can be fruitfully exploited when high-fidelity\ncomputations are required, namely: pressure-robustness, conservation of mass\nenforced cell-by-cell up to machine precision, robustness in the inviscid\nlimit, implicit high-order accurate time stepping with local time step\nadaptation, reduced memory footprint thanks to static condensation of both\nvelocity and pressure, possibility to exploit inherited $p$-multilevel solution\nstrategies to improve performance of iterative solvers. After demonstrating the\nrelevant properties of the scheme in practice, performing challenging 2D and 3D\ntest cases, we consider the simulation of the Taylor--Green Vortex flow problem\nat Reynolds 1600.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.22480v2", "cate": "physics.flu-dyn", "date": "2025-05-28", "updated": "2025-07-31"}
{"id": "2508.00583", "title": "Enhancing Wireless Networks for IoT with Large Vision Models: Foundations and Applications", "authors": ["Yunting Xu", "Jiacheng Wang", "Ruichen Zhang", "Dusit Niyato", "Deepu Rajan", "Liang Yu", "Haibo Zhou", "Abbas Jamalipour", "Xianbin Wang"], "categories": ["cs.NI"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "Comments:      7 pages, 6 figures", "url": "http://arxiv.org/abs/2508.00583v1", "summary": "Large vision models (LVMs) have emerged as a foundational paradigm in visual\nintelligence, achieving state-of-the-art performance across diverse visual\ntasks. Recent advances in LVMs have facilitated their integration into Internet\nof Things (IoT) scenarios, offering superior generalization and adaptability\nfor vision-assisted network optimization. In this paper, we first investigate\nthe functionalities and core architectures of LVMs, highlighting their\ncapabilities across classification, segmentation, generation, and multimodal\nvisual processing. We then explore a variety of LVM applications in wireless\ncommunications, covering representative tasks across the physical layer,\nnetwork layer, and application layer. Furthermore, given the substantial model\nsize of LVMs and the challenges of model retraining in wireless domains, we\npropose a progressive fine-tuning framework that incrementally adapts\npretrained LVMs for joint optimization of multiple IoT tasks. A case study in\nlow-altitude economy networks (LAENets) demonstrates the effectiveness of the\nproposed framework over conventional CNNs in joint beamforming and positioning\ntasks for Internet of drones, underscoring a promising direction for\nintegrating LVMs into intelligent wireless systems.", "comment": "7 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2508.00583v1", "cate": "cs.NI", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00816", "title": "Efficient Solving of Large Single Input Superstate Decomposable Markovian Decision Process", "authors": ["Youssef Ait El Mahjoub", "Jean-Michel Fourneau", "Salma Alouah"], "categories": ["math.OC", "cs.LG", "cs.PF"], "primary_category": "Subjects:       Optimization and Control (math.OC)", "pdf_link": null, "comments": "Comments:      Preprint article submitted to ValueTools2025", "url": "http://arxiv.org/abs/2508.00816v1", "summary": "Solving Markov Decision Processes (MDPs) remains a central challenge in\nsequential decision-making, especially when dealing with large state spaces and\nlong-term optimization criteria. A key step in Bellman dynamic programming\nalgorithms is the policy evaluation, which becomes computationally demanding in\ninfinite-horizon settings such as average-reward or discounted-reward\nformulations. In the context of Markov chains, aggregation and disaggregation\ntechniques have for a long time been used to reduce complexity by exploiting\nstructural decompositions. In this work, we extend these principles to a\nstructured class of MDPs. We define the Single-Input Superstate Decomposable\nMarkov Decision Process (SISDMDP), which combines Chiu's single-input\ndecomposition with Robertazzi's single-cycle recurrence property. When a policy\ninduces this structure, the resulting transition graph can be decomposed into\ninteracting components with centralized recurrence. We develop an exact and\nefficient policy evaluation method based on this structure. This yields a\nscalable solution applicable to both average and discounted reward MDPs.", "comment": "Preprint article submitted to ValueTools2025", "pdf_url": "http://arxiv.org/pdf/2508.00816v1", "cate": "math.OC", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2507.19115", "title": "Automated Code Review Using Large Language Models at Ericsson: An Experience Report", "authors": ["Shweta Ramesh", "Joy Bose", "Hamender Singh", "A K Raghavan", "Sujoy Roychowdhury", "Giriprasad Sridhara", "Nishrith Saini", "Ricardo Britto"], "categories": ["cs.SE", "cs.AI", "D.2.7"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      6 pages, 4 figures, 1 table. Accepted in ICSME 2025 conference in Auckland", "url": "http://arxiv.org/abs/2507.19115v2", "summary": "Code review is one of the primary means of assuring the quality of released\nsoftware along with testing and static analysis. However, code review requires\nexperienced developers who may not always have the time to perform an in-depth\nreview of code. Thus, automating code review can help alleviate the cognitive\nburden on experienced software developers allowing them to focus on their\nprimary activities of writing code to add new features and fix bugs. In this\npaper, we describe our experience in using Large Language Models towards\nautomating the code review process in Ericsson. We describe the development of\na lightweight tool using LLMs and static program analysis. We then describe our\npreliminary experiments with experienced developers in evaluating our code\nreview tool and the encouraging results.", "comment": "6 pages, 4 figures, 1 table. Accepted in ICSME 2025 conference in\n  Auckland", "pdf_url": "http://arxiv.org/pdf/2507.19115v2", "cate": "cs.SE", "date": "2025-07-25", "updated": "2025-07-31"}
{"id": "2507.23521", "title": "JPEG Processing Neural Operator for Backward-Compatible Coding", "authors": ["Woo Kyoung Han", "Yongjun Lee", "Byeonghun Lee", "Sang Hyun Park", "Sunghoon Im", "Kyong Hwan Jin"], "categories": ["eess.IV", "cs.CV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23521v1", "summary": "Despite significant advances in learning-based lossy compression algorithms,\nstandardizing codecs remains a critical challenge. In this paper, we present\nthe JPEG Processing Neural Operator (JPNeO), a next-generation JPEG algorithm\nthat maintains full backward compatibility with the current JPEG format. Our\nJPNeO improves chroma component preservation and enhances reconstruction\nfidelity compared to existing artifact removal methods by incorporating neural\noperators in both the encoding and decoding stages. JPNeO achieves practical\nbenefits in terms of reduced memory usage and parameter count. We further\nvalidate our hypothesis about the existence of a space with high mutual\ninformation through empirical evidence. In summary, the JPNeO functions as a\nhigh-performance out-of-the-box image compression pipeline without changing\nsource coding's protocol. Our source code is available at\nhttps://github.com/WooKyoungHan/JPNeO.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23521v1", "cate": "eess.IV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2508.00616", "title": "Joint Association and Phase Shifts Design for UAV-mounted Stacked Intelligent Metasurfaces-assisted Communications", "authors": ["Mingzhe Fan", "Geng Sun", "Hongyang Pan", "Jiacheng Wang", "Jiancheng An", "Hongyang Du", "Chau Yuen"], "categories": ["cs.NI"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "Comments:      This papar has been submitted to the IEEE Global Communications Conference. arXiv admin note: substantial text overlap with arXiv:2506.23488", "url": "http://arxiv.org/abs/2508.00616v1", "summary": "Stacked intelligent metasurfaces (SIMs) have emerged as a promising\ntechnology for realizing wave-domain signal processing, while the fixed SIMs\nwill limit the communication performance of the system compared to the mobile\nSIMs. In this work, we consider a UAV-mounted SIMs (UAV-SIMs) assisted\ncommunication system, where UAVs as base stations (BSs) can cache the data\nprocessed by SIMs, and also as mobile vehicles flexibly deploy SIMs to enhance\nthe communication performance. To this end, we formulate a UAV-SIM-based joint\noptimization problem (USBJOP) to comprehensively consider the association\nbetween UAV-SIMs and users, the locations of UAV-SIMs, and the phase shifts of\nUAV-SIMs, aiming to maximize the network capacity. Due to the non-convexity and\nNP-hardness of USBJOP, we decompose it into three sub-optimization problems,\nwhich are the association between UAV-SIMs and users optimization problem\n(AUUOP), the UAV location optimization problem (ULOP), and the UAV-SIM phase\nshifts optimization problem (USPSOP). Then, these three sub-optimization\nproblems are solved by an alternating optimization (AO) strategy. Specifically,\nAUUOP and ULOP are transformed to a convex form and then solved by the CVX\ntool, while we employ a layer-by-layer iterative optimization method for\nUSPSOP. Simulation results verify the effectiveness of the proposed strategy\nunder different simulation setups.", "comment": "This papar has been submitted to the IEEE Global Communications\n  Conference. arXiv admin note: substantial text overlap with arXiv:2506.23488", "pdf_url": "http://arxiv.org/pdf/2508.00616v1", "cate": "cs.NI", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2506.16676", "title": "Fast solvers for Tokamak fluid models with PETSC", "authors": ["Mark F. Adams", "Jin Chen", "Benjamin Sturdevant"], "categories": ["physics.plasm-ph", "cs.PF"], "primary_category": "Subjects:       Plasma Physics (physics.plasm-ph)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.16676v5", "summary": "This work begins the development of fast, scalable solvers for scientific and\nengineering-relevant magnetohydrodynamics (MHD) models of tokamaks using\nmultigrid methods. These models are characterized by a distinguished direction\nfollowing the magnetic field around the torus, which dominates the plasma\ndynamics. All tokamak models exploit this structure, for example, NIMROD uses\n2D, unstructured, high-order finite elements in the poloidal plane with Fourier\nmodes in the toroidal coordinate, and the 3D, extended MHD code M3D-C1 uses 2D,\nunstructured C1 elements in the poloidal plane with cubic Hermite functions in\nthe toroidal direction and a regular grid that is partially aligned with the\nmagnetic field. This structure suggests addressing the toroidal coordinate\nfirst, which NIMROD does at the formulation level, but M3D-C1 uses a full 3D\ndiscretization. The resulting algebraic system is solved at each time step in\nan implicit time integrator. This work addressed the toroidal coordinate in the\nM3D-C1 velocity solve by adding semi-coarsening multigrid to the existing PETSC\n- Portable, Extensible Toolkit for Scientific Computation - block Jacobi\nsolver, with the addition of little new code. Competitive performance of this\nnew solver configuration is demonstrated on a self-consistent runaway electron\nmodel of a SPARC4 disruption, and the next steps in the development of this\nsolver are outlined.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.16676v5", "cate": "physics.plasm-ph", "date": "2025-06-20", "updated": "2025-08-01"}
{"id": "2507.19119", "title": "PatchTraj: Unified Time-Frequency Representation Learning via Dynamic Patches for Trajectory Prediction", "authors": ["Yanghong Liu", "Xingping Dong", "Ming Li", "Weixing Zhang", "Yidong Lou"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19119v3", "summary": "Pedestrian trajectory prediction is crucial for autonomous driving and\nrobotics. While existing point-based and grid-based methods expose two main\nlimitations: insufficiently modeling human motion dynamics, as they fail to\nbalance local motion details with long-range spatiotemporal dependencies, and\nthe time representations lack interaction with their frequency components in\njointly modeling trajectory sequences. To address these challenges, we propose\nPatchTraj, a dynamic patch-based framework that integrates time-frequency joint\nmodeling for trajectory prediction. Specifically, we decompose the trajectory\ninto raw time sequences and frequency components, and employ dynamic patch\npartitioning to perform multi-scale segmentation, capturing hierarchical motion\npatterns. Each patch undergoes adaptive embedding with scale-aware feature\nextraction, followed by hierarchical feature aggregation to model both\nfine-grained and long-range dependencies. The outputs of the two branches are\nfurther enhanced via cross-modal attention, facilitating complementary fusion\nof temporal and spectral cues. The resulting enhanced embeddings exhibit strong\nexpressive power, enabling accurate predictions even when using a vanilla\nTransformer architecture. Extensive experiments on ETH-UCY, SDD, NBA, and JRDB\ndatasets demonstrate that our method achieves state-of-the-art performance.\nNotably, on the egocentric JRDB dataset, PatchTraj attains significant relative\nimprovements of 26.7% in ADE and 17.4% in FDE, underscoring its substantial\npotential in embodied intelligence.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19119v3", "cate": "cs.CV", "date": "2025-07-25", "updated": "2025-07-31"}
{"id": "2507.23523", "title": "H-RDT: Human Manipulation Enhanced Bimanual Robotic Manipulation", "authors": ["Hongzhe Bi", "Lingxuan Wu", "Tianwei Lin", "Hengkai Tan", "Zhizhong Su", "Hang Su", "Jun Zhu"], "categories": ["cs.RO", "cs.CV", "cs.LG"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23523v2", "summary": "Imitation learning for robotic manipulation faces a fundamental challenge:\nthe scarcity of large-scale, high-quality robot demonstration data. Recent\nrobotic foundation models often pre-train on cross-embodiment robot datasets to\nincrease data scale, while they face significant limitations as the diverse\nmorphologies and action spaces across different robot embodiments make unified\ntraining challenging. In this paper, we present H-RDT (Human to Robotics\nDiffusion Transformer), a novel approach that leverages human manipulation data\nto enhance robot manipulation capabilities. Our key insight is that large-scale\negocentric human manipulation videos with paired 3D hand pose annotations\nprovide rich behavioral priors that capture natural manipulation strategies and\ncan benefit robotic policy learning. We introduce a two-stage training\nparadigm: (1) pre-training on large-scale egocentric human manipulation data,\nand (2) cross-embodiment fine-tuning on robot-specific data with modular action\nencoders and decoders. Built on a diffusion transformer architecture with 2B\nparameters, H-RDT uses flow matching to model complex action distributions.\nExtensive evaluations encompassing both simulation and real-world experiments,\nsingle-task and multitask scenarios, as well as few-shot learning and\nrobustness assessments, demonstrate that H-RDT outperforms training from\nscratch and existing state-of-the-art methods, including Pi0 and RDT, achieving\nsignificant improvements of 13.9% and 40.5% over training from scratch in\nsimulation and real-world experiments, respectively. The results validate our\ncore hypothesis that human manipulation data can serve as a powerful foundation\nfor learning bimanual robotic manipulation policies.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23523v2", "cate": "cs.RO", "date": "2025-07-31", "updated": "2025-08-01"}
{"id": "2508.00688", "title": "Criticality-Based Dynamic Topology Optimization for Enhancing Aerial-Marine Swarm Resilience", "authors": ["Ruiyang Huang", "Haocheng Wang", "Yixuan Shen", "Ning Gao", "Qiang Ni", "Shi Jin", "Yifan Wu"], "categories": ["cs.NI", "eess.SP"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "Comments:      Submit to INFOCOM 2026", "url": "http://arxiv.org/abs/2508.00688v1", "summary": "Heterogeneous marine-aerial swarm networks encounter substantial difficulties\ndue to targeted communication disruptions and structural weaknesses in\nadversarial environments. This paper proposes a two-step framework to\nstrengthen the network's resilience. Specifically, our framework combines the\nnode prioritization based on criticality with multi-objective topology\noptimization. First, we design a three-layer architecture to represent\nstructural, communication, and task dependencies of the swarm networks. Then,\nwe introduce the SurBi-Ranking method, which utilizes graph convolutional\nnetworks, to dynamically evaluate and rank the criticality of nodes and edges\nin real time. Next, we apply the NSGA-III algorithm to optimize the network\ntopology, aiming to balance communication efficiency, global connectivity, and\nmission success rate. Experiments demonstrate that compared to traditional\nmethods like K-Shell, our SurBi-Ranking method identifies critical nodes and\nedges with greater accuracy, as deliberate attacks on these components cause\nmore significant connectivity degradation. Furthermore, our optimization\napproach, when prioritizing SurBi-Ranked critical components under attack,\nreduces the natural connectivity degradation by around 30%, achieves higher\nmission success rates, and incurs lower communication reconfiguration costs,\nensuring sustained connectivity and mission effectiveness across multi-phase\noperations.", "comment": "Submit to INFOCOM 2026", "pdf_url": "http://arxiv.org/pdf/2508.00688v1", "cate": "cs.NI", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2507.21004", "title": "Compositional Function Networks: A High-Performance Alternative to Deep Neural Networks with Built-in Interpretability", "authors": ["Fang Li"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      The project has been open sourced at Github ( this https URL )", "url": "http://arxiv.org/abs/2507.21004v2", "summary": "Deep Neural Networks (DNNs) deliver impressive performance but their\nblack-box nature limits deployment in high-stakes domains requiring\ntransparency. We introduce Compositional Function Networks (CFNs), a novel\nframework that builds inherently interpretable models by composing elementary\nmathematical functions with clear semantics. Unlike existing interpretable\napproaches that are limited to simple additive structures, CFNs support diverse\ncompositional patterns -- sequential, parallel, and conditional -- enabling\ncomplex feature interactions while maintaining transparency. A key innovation\nis that CFNs are fully differentiable, allowing efficient training through\nstandard gradient descent. We demonstrate CFNs' versatility across multiple\ndomains, from symbolic regression to image classification with deep\nhierarchical networks. Our empirical evaluation shows CFNs achieve competitive\nperformance against black-box models (96.24% accuracy on CIFAR-10) while\noutperforming state-of-the-art interpretable models like Explainable Boosting\nMachines. By combining the hierarchical expressiveness and efficient training\nof deep learning with the intrinsic interpretability of well-defined\nmathematical functions, CFNs offer a powerful framework for applications where\nboth performance and accountability are paramount.", "comment": "The project has been open sourced at Github\n  (https://github.com/fanglioc/Compositional_Function_Networks)", "pdf_url": "http://arxiv.org/pdf/2507.21004v2", "cate": "cs.LG", "date": "2025-07-28", "updated": "2025-07-31"}
{"id": "2507.23534", "title": "Continual Learning with Synthetic Boundary Experience Blending", "authors": ["Chih-Fan Hsu", "Ming-Ching Chang", "Wei-Chao Chen"], "categories": ["cs.LG", "cs.CV"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23534v1", "summary": "Continual learning (CL) aims to address catastrophic forgetting in models\ntrained sequentially on multiple tasks. While experience replay has shown\npromise, its effectiveness is often limited by the sparse distribution of\nstored key samples, leading to overly simplified decision boundaries. We\nhypothesize that introducing synthetic data near the decision boundary\n(Synthetic Boundary Data, or SBD) during training serves as an implicit\nregularizer, improving boundary stability and mitigating forgetting. To\nvalidate this hypothesis, we propose a novel training framework, {\\bf\nExperience Blending}, which integrates knowledge from both stored key samples\nand synthetic, boundary-adjacent data. Experience blending consists of two core\ncomponents: (1) a multivariate Differential Privacy (DP) noise mechanism that\ninjects batch-wise noise into low-dimensional feature representations,\ngenerating SBD; and (2) an end-to-end training strategy that jointly leverages\nboth stored key samples and SBD. Extensive experiments on CIFAR-10, CIFAR-100,\nand Tiny ImageNet demonstrate that our method outperforms nine CL baselines,\nachieving accuracy improvements of 10%, 6%, and 13%, respectively.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23534v1", "cate": "cs.LG", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2508.00715", "title": "Deep Joint Source-Channel Coding for Small Satellite Applications", "authors": ["Olga Kondrateva", "Grace Li Zhang", "Julian Zobel", "Björn Scheuermann", "Stefan Dietzel"], "categories": ["cs.NI"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00715v1", "summary": "Small satellites used for Earth observation generate vast amounts of\nhigh-dimensional data, but their operation in low Earth orbit creates a\nsignificant communication bottleneck due to limited contact times and harsh,\nvarying channel conditions. While deep joint source-channel coding (DJSCC) has\nemerged as a promising technique, its practical application to the complex\nsatellite environment remains an open question. This paper presents a\ncomprehensive DJSCC framework tailored for satellite communications. We first\nestablish a basic system, DJSCC-SAT, and integrate a realistic, multi-state\nstatistical channel model to guide its training and evaluation. To overcome the\nimpracticality of using separate models for every channel condition, we then\nintroduce an adaptable architecture, ADJSCC-SAT, which leverages attention\nmodules to allow a single neural network to adjust to a wide range of channel\nstates with minimal overhead. Through extensive evaluation on Sentinel-2\nmulti-spectral data, we demonstrate that our adaptable approach achieves\nperformance comparable to using multiple specialized networks while\nsignificantly reducing model storage requirements. Furthermore, the adaptable\nmodel shows enhanced robustness to channel estimation errors, outperforming the\nnon-adaptable baseline. The proposed framework is a practical and efficient\nstep toward deploying robust, adaptive DJSCC systems for real-world satellite\nmissions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00715v1", "cate": "cs.NI", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2507.21433", "title": "MemShare: Memory Efficient Inference for Large Reasoning Models through KV Cache Reuse", "authors": ["Kaiwen Chen", "Xin Tan", "Minchen Yu", "Hong Xu"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      11 pages, 7 figures", "url": "http://arxiv.org/abs/2507.21433v2", "summary": "Large Reasoning Models (LRMs) have achieved significant advances in\nmathematical reasoning and formal logic tasks. However, their tendency to\ngenerate lengthy chain-of-thought sequences leads to substantial memory\noverhead during inference. We observe that LRMs frequently produce highly\nsimilar intermediate reasoning steps, which correspond to similar KV cache\nstates across layers. Motivated by this observation, we propose MemShare, a\nnovel KV cache management approach that effectively reduces memory overhead.\nMemShare employs a collaborative filtering algorithm to efficiently identify\nreusable KV cache blocks and enables zero copy cache reuse to significantly\nreduce memory overhead, improve throughput while maintaining accuracy.\nExperimental results demonstrate that MemShare delivers up to 84.79\\%\nimprovement in throughput while maintaining better accuracy compared to\nexisting KV cache management methods.", "comment": "11 pages, 7 figures", "pdf_url": "http://arxiv.org/pdf/2507.21433v2", "cate": "cs.LG", "date": "2025-07-29", "updated": "2025-07-31"}
{"id": "2507.23648", "title": "Towards Field-Ready AI-based Malaria Diagnosis: A Continual Learning Approach", "authors": ["Louise Guillon", "Soheib Biga", "Yendoube E. Kantchire", "Mouhamadou Lamine Sane", "Grégoire Pasquier", "Kossi Yakpa", "Stéphane E. Sossou", "Marc Thellier", "Laurent Bonnardot", "Laurence Lachaud", "Renaud Piarroux", "Ameyo M. Dorkenoo"], "categories": ["eess.IV", "cs.CV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "Comments:      MICCAI 2025 AMAI Workshop, Accepted, Submitted Manuscript Version", "url": "http://arxiv.org/abs/2507.23648v1", "summary": "Malaria remains a major global health challenge, particularly in low-resource\nsettings where access to expert microscopy may be limited. Deep learning-based\ncomputer-aided diagnosis (CAD) systems have been developed and demonstrate\npromising performance on thin blood smear images. However, their clinical\ndeployment may be hindered by limited generalization across sites with varying\nconditions. Yet very few practical solutions have been proposed. In this work,\nwe investigate continual learning (CL) as a strategy to enhance the robustness\nof malaria CAD models to domain shifts. We frame the problem as a\ndomain-incremental learning scenario, where a YOLO-based object detector must\nadapt to new acquisition sites while retaining performance on previously seen\ndomains. We evaluate four CL strategies, two rehearsal-based and two\nregularization-based methods, on real-life conditions thanks to a multi-site\nclinical dataset of thin blood smear images. Our results suggest that CL, and\nrehearsal-based methods in particular, can significantly improve performance.\nThese findings highlight the potential of continual learning to support the\ndevelopment of deployable, field-ready CAD tools for malaria.", "comment": "MICCAI 2025 AMAI Workshop, Accepted, Submitted Manuscript Version", "pdf_url": "http://arxiv.org/pdf/2507.23648v1", "cate": "eess.IV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2508.00735", "title": "Overlapping IPv4, IPv6, and TCP data: exploring errors, test case context and multiple overlaps inside network stacks and NIDSes with PYROLYSE", "authors": ["Lucas Aubard", "Johan Mazel", "Gilles Guette", "Pierre Chifflier"], "categories": ["cs.NI"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00735v1", "summary": "IP fragmentation and TCP segmentation allow for splitting large data packets\ninto smaller ones, e.g., for transmission across network links of limited\ncapacity. These mechanisms permit complete or partial overlaps with different\ndata on the overlapping portions. IPv4, IPv6, and TCP reassembly policies,\ni.e., the data chunk preferences that depend on the overlap types, differ\nacross protocol implementations. This leads to vulnerabilities, as NIDSes may\ninterpret the packet differently from the monitored host OSes. Some NIDSes,\nsuch as Suricata or Snort, can be configured so that their policies are\nconsistent with the monitored OSes. The first contribution of the paper is\nPYROLYSE, an audit tool that exhaustively tests and describes the reassembly\npolicies of various IP and TCP implementation types. This tool ensures that\nimplementations reassemble overlapping chunk sequences without errors. The\nsecond contribution is the analysis of PYROLYSE artifacts. We first show that\nthe reassembly policies are much more diverse than previously thought. Indeed,\nby testing all the overlap possibilities for n <= 3 test case chunks and\ndifferent testing scenarios, we observe from 14 to 20 different behaviors out\nof 23 tested implementations depending on the protocol. Second, we report eight\nerrors impacting one OS, two NIDSes, and two embedded stacks, which can lead to\nsecurity issues such as NIDS pattern-matching bypass or DoS attacks. A CVE was\nassigned to a NIDS error. Finally, we show that implemented IP and TCP policies\nobtained through chunk pair testing are usually inconsistent with the observed\ntriplet reassemblies. Therefore, contrarily to what they currently do, NIDSes\nor other network traffic analysis tools should not apply n = 2 pair policies\nwhen the number of overlapping chunks exceeds two.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00735v1", "cate": "cs.NI", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2507.22069", "title": "A Compute-Matched Re-Evaluation of TroVE on MATH", "authors": ["Tobias Sesterhenn", "Ian Berlot-Attwell", "Janis Zenkner", "Christian Bartelt"], "categories": ["cs.PL", "cs.AI"], "primary_category": "Subjects:       Programming Languages (cs.PL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22069v2", "summary": "Reusing established theorems and formulas is central to mathematical problem\nsolving, serving as essential building blocks for tackling increasingly complex\nchallenges. Recent work, TroVE, argues that code-generating Large Language\nModels (LLMs) can benefit similarly on the MATH benchmark by inducing and\nreusing higher-level toolboxes. By allocating computational budget across an\nensemble of three modes -- directly generating code, creating tools, and\nreusing tools -- TroVE claims to outperform a PRIMITIVE baseline that only\nperforms direct generation. However, recent analysis (Berlot-Attwell et al.,\n2024) casts doubt on these gains, noting that the tools created are often\ntrivial or rarely reused, suggesting that improvements may stem from\nself-consistency or self-correction. In this work, we re-evaluate TroVE on\nMATH, analyze the impact of each of its modes, and show that its benefit does\nnot come from these mechanisms, but simply from a higher computational budget\nspent for TroVE compared to PRIMITIVE. To this end, we also perform a small\ncorrection in the original implementation of TroVE's selection mechanism,\nboosting TroVE's performance on MATH by 3\\% in accuracy. After matching for\ncompute, the benefit of TroVE reduces to a marginal improvement of 1\\%,\nsuggesting that this toolbox approach does not provide a significant benefit on\nMATH.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22069v2", "cate": "cs.PL", "date": "2025-07-16", "updated": "2025-07-31"}
{"id": "2507.23676", "title": "DepMicroDiff: Diffusion-Based Dependency-Aware Multimodal Imputation for Microbiome Data", "authors": ["Rabeya Tus Sadia", "Qiang Cheng"], "categories": ["cs.LG", "cs.CV"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23676v1", "summary": "Microbiome data analysis is essential for understanding host health and\ndisease, yet its inherent sparsity and noise pose major challenges for accurate\nimputation, hindering downstream tasks such as biomarker discovery. Existing\nimputation methods, including recent diffusion-based models, often fail to\ncapture the complex interdependencies between microbial taxa and overlook\ncontextual metadata that can inform imputation. We introduce DepMicroDiff, a\nnovel framework that combines diffusion-based generative modeling with a\nDependency-Aware Transformer (DAT) to explicitly capture both mutual pairwise\ndependencies and autoregressive relationships. DepMicroDiff is further enhanced\nby VAE-based pretraining across diverse cancer datasets and conditioning on\npatient metadata encoded via a large language model (LLM). Experiments on TCGA\nmicrobiome datasets show that DepMicroDiff substantially outperforms\nstate-of-the-art baselines, achieving higher Pearson correlation (up to 0.712),\ncosine similarity (up to 0.812), and lower RMSE and MAE across multiple cancer\ntypes, demonstrating its robustness and generalizability for microbiome\nimputation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23676v1", "cate": "cs.LG", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2508.00792", "title": "Data Movement Manager (DMM) for the SENSE-Rucio Interoperation Prototype", "authors": ["Aashay Arora", "Diego Davila", "Jonathan Guiang", "Frank Würthwein", "Harvey Newman", "Justas Balcas", "Tom Lehman", "Xi Yang"], "categories": ["cs.NI"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "Comments:      Submitted to CHEP 24", "url": "http://arxiv.org/abs/2508.00792v1", "summary": "The Data Movement Manager (DMM) is a prototype interface that connects CERN's\ndata management software, Rucio, with the Sofware-Defined Networking (SDN)\nservice SENSE by ESNet. It enables SDN-enabled high-energy physics data flows\nusing the existing worldwide LHC computing grid infrastructure. A key feature\nof DMM is transfer priority-based bandwidth allocation, optimizing network\nusage. Additionally, it provides fine-grained monitoring of underperforming\nflows by leveraging end-to-end data flow monitoring. This is achieved through\naccess to host-level (network interface) throughput metrics and transfer-tool\n(FTS) data transfer job-level metrics. This paper details the design and\nimplementation of DMM.", "comment": "Submitted to CHEP 24", "pdf_url": "http://arxiv.org/pdf/2508.00792v1", "cate": "cs.NI", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00005", "title": "Modelling Program Spaces in Program Synthesis with Constraints", "authors": ["Tilman Hinnerichs", "Bart Swinkels", "Jaap de Jong", "Reuben Gardos Reid", "Tudor Magirescu", "Neil Yorke-Smith", "Sebastijan Dumancic"], "categories": ["cs.PL", "cs.AI"], "primary_category": "Subjects:       Programming Languages (cs.PL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00005v1", "summary": "A core challenge in program synthesis is taming the large space of possible\nprograms. Since program synthesis is essentially a combinatorial search, the\ncommunity has sought to leverage powerful combinatorial constraint solvers.\nHere, constraints are used to express the program semantics, but not as a\npotentially potent tool to remove unwanted programs. Recent inductive logic\nprogramming approaches introduce constraints on the program's syntax to be\nsynthesized. These syntactic constraints allow for checking and propagating a\nconstraint without executing the program, and thus for arbitrary operators. In\nthis work, we leverage syntactic constraints to model program spaces, defining\nnot just solutions that are feasible, but also ones that are likely useful. To\ndemonstrate this idea, we introduce BART, a solver that efficiently propagates\nand solves these constraints. We evaluate BART on program space enumeration\ntasks, finding that the constraints eliminate up to 99 percent of the program\nspace, and that modeling program spaces significantly reduces enumeration time.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00005v1", "cate": "cs.PL", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2507.22174", "title": "Spatial-Temporal Reinforcement Learning for Network Routing with Non-Markovian Traffic", "authors": ["Molly Wang", "Kin. K Leung"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22174v2", "summary": "Reinforcement Learning (RL) has been widely used for packet routing in\ncommunication networks, but traditional RL methods rely on the Markov\nassumption that the current state contains all necessary information for\ndecision-making. In reality, internet traffic is non-Markovian, and past states\ndo influence routing performance. Moreover, common deep RL approaches use\nfunction approximators, such as neural networks, that do not model the spatial\nstructure in network topologies. To address these shortcomings, we design a\nnetwork environment with non-Markovian traffic and introduce a spatial-temporal\nRL (STRL) framework for packet routing. Our approach outperforms traditional\nbaselines by more than 19% during training and 7% for inference despite a\nchange in network topology.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22174v2", "cate": "cs.LG", "date": "2025-07-27", "updated": "2025-07-31"}
{"id": "2507.23763", "title": "Topology Optimization in Medical Image Segmentation with Fast Euler Characteristic", "authors": ["Liu Li", "Qiang Ma", "Cheng Ouyang", "Johannes C. Paetzold", "Daniel Rueckert", "Bernhard Kainz"], "categories": ["eess.IV", "cs.CV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23763v1", "summary": "Deep learning-based medical image segmentation techniques have shown\npromising results when evaluated based on conventional metrics such as the Dice\nscore or Intersection-over-Union. However, these fully automatic methods often\nfail to meet clinically acceptable accuracy, especially when topological\nconstraints should be observed, e.g., continuous boundaries or closed surfaces.\nIn medical image segmentation, the correctness of a segmentation in terms of\nthe required topological genus sometimes is even more important than the\npixel-wise accuracy. Existing topology-aware approaches commonly estimate and\nconstrain the topological structure via the concept of persistent homology\n(PH). However, these methods are difficult to implement for high dimensional\ndata due to their polynomial computational complexity. To overcome this\nproblem, we propose a novel and fast approach for topology-aware segmentation\nbased on the Euler Characteristic ($\\chi$). First, we propose a fast\nformulation for $\\chi$ computation in both 2D and 3D. The scalar $\\chi$ error\nbetween the prediction and ground-truth serves as the topological evaluation\nmetric. Then we estimate the spatial topology correctness of any segmentation\nnetwork via a so-called topological violation map, i.e., a detailed map that\nhighlights regions with $\\chi$ errors. Finally, the segmentation results from\nthe arbitrary network are refined based on the topological violation maps by a\ntopology-aware correction network. Our experiments are conducted on both 2D and\n3D datasets and show that our method can significantly improve topological\ncorrectness while preserving pixel-wise segmentation accuracy.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23763v1", "cate": "eess.IV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2412.12485", "title": "Rydberg Atomic Receiver: Next Frontier of Wireless Communications", "authors": ["Mingyao Cui", "Qunsong Zeng", "Kaibin Huang"], "categories": ["eess.SP", "cs.NI", "physics.app-ph"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "Comments:      Accepted by IEEE Communications Magazine", "url": "http://arxiv.org/abs/2412.12485v3", "summary": "Rydberg Atomic REceiver (RARE) is driving a paradigm shift in electromagnetic\n(EM) wave measurement by harnessing the electron transition phenomenon of\nRydberg atoms. Operating at the quantum scale, such receivers have the\npotential to breakthrough the performance limit of classic receivers, sparking\na revolution in physical-layer wireless communications. The objective of this\npaper is to offer insights into RARE-empowered communications. We first provide\na comprehensive introduction to the fundamental principles of RAREs. Then, a\nthorough comparison between RAREs and classic receivers is conducted in terms\nof the antenna size, sensitivity, and bandwidth. Subsequently, we overview the\nrecent progresses in RARE-aided wireless communications, covering the\nfrequency-division multiplexing, multiple-input-multiple-output, wireless\nsensing, and quantum many-body techniques. Moreover, the unique application of\nRARE in multiband sensing and communication is introduced. Finally, we conclude\nby providing promising research directions.", "comment": "Accepted by IEEE Communications Magazine", "pdf_url": "http://arxiv.org/pdf/2412.12485v3", "cate": "eess.SP", "date": "2024-12-17", "updated": "2025-08-01"}
{"id": "2508.00013", "title": "From Provable Correctness to Probabilistic Generation: A Comparative Review of Program Synthesis Paradigms", "authors": ["Zurabi Kobaladze", "Anna Arnania", "Tamar Sanikidze"], "categories": ["cs.PL", "I.2.6; F.1.1"], "primary_category": "Subjects:       Programming Languages (cs.PL)", "pdf_link": null, "comments": "Comments:      78 pages. Undergraduate thesis project submitted in partial fulfillment of the requirements for the Bachelor's degree in Computer Science at Kutaisi International University", "url": "http://arxiv.org/abs/2508.00013v1", "summary": "Program synthesis--the automated generation of executable code from\nhigh-level specifications--has been a central goal of computer science for over\nfifty years. This thesis provides a comparative literature review of the main\nparadigms that have shaped the field, tracing its evolution from formal logic\nbased methods to recent advances using large scale neural models. We examine\nfive key approaches: logic based (deductive) synthesis, inductive (example\nbased) synthesis, sketch/schema based synthesis, large language model based\nsynthesis, and neuro-symbolic hybrids. For each, we analyze foundational\nprinciples, notable systems, and practical applications, highlighting trade\noffs between correctness guarantees, specification requirements, search\ncomplexity, and expressive power. By reviewing developments from formally\nverified synthesis tools such as KIDS and Coq to data driven models generating\nprobabilistic code from natural language like Codex, we present a comprehensive\nnarrative of progress and ongoing challenges. This work emphasizes the\ntransition from symbolic to hybrid neuro-symbolic methods and outlines future\ndirections for reliable and scalable program synthesis.", "comment": "78 pages. Undergraduate thesis project submitted in partial\n  fulfillment of the requirements for the Bachelor's degree in Computer Science\n  at Kutaisi International University", "pdf_url": "http://arxiv.org/pdf/2508.00013v1", "cate": "cs.PL", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2507.22477", "title": "LIDAR: Lightweight Adaptive Cue-Aware Fusion Vision Mamba for Multimodal Segmentation of Structural Cracks", "authors": ["Hui Liu", "Chen Jia", "Fan Shi", "Xu Cheng", "Mengfei Shi", "Xia Xie", "Shengyong Chen"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      This paper has been accepted by ACM MM 2025", "url": "http://arxiv.org/abs/2507.22477v2", "summary": "Achieving pixel-level segmentation with low computational cost using\nmultimodal data remains a key challenge in crack segmentation tasks. Existing\nmethods lack the capability for adaptive perception and efficient interactive\nfusion of cross-modal features. To address these challenges, we propose a\nLightweight Adaptive Cue-Aware Vision Mamba network (LIDAR), which efficiently\nperceives and integrates morphological and textural cues from different\nmodalities under multimodal crack scenarios, generating clear pixel-level crack\nsegmentation maps. Specifically, LIDAR is composed of a Lightweight Adaptive\nCue-Aware Visual State Space module (LacaVSS) and a Lightweight Dual Domain\nDynamic Collaborative Fusion module (LD3CF). LacaVSS adaptively models crack\ncues through the proposed mask-guided Efficient Dynamic Guided Scanning\nStrategy (EDG-SS), while LD3CF leverages an Adaptive Frequency Domain\nPerceptron (AFDP) and a dual-pooling fusion strategy to effectively capture\nspatial and frequency-domain cues across modalities. Moreover, we design a\nLightweight Dynamically Modulated Multi-Kernel convolution (LDMK) to perceive\ncomplex morphological structures with minimal computational overhead, replacing\nmost convolutional operations in LIDAR. Experiments on three datasets\ndemonstrate that our method outperforms other state-of-the-art (SOTA) methods.\nOn the light-field depth dataset, our method achieves 0.8204 in F1 and 0.8465\nin mIoU with only 5.35M parameters. Code and datasets are available at\nhttps://github.com/Karl1109/LIDAR-Mamba.", "comment": "This paper has been accepted by ACM MM 2025", "pdf_url": "http://arxiv.org/pdf/2507.22477v2", "cate": "cs.CV", "date": "2025-07-30", "updated": "2025-07-31"}
{"id": "2311.18266", "title": "Prompt-Based Exemplar Super-Compression and Regeneration for Class-Incremental Learning", "authors": ["Ruxiao Duan", "Jieneng Chen", "Adam Kortylewski", "Alan Yuille", "Yaoyao Liu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      BMVC 2025. Code: this https URL", "url": "http://arxiv.org/abs/2311.18266v3", "summary": "Replay-based methods in class-incremental learning (CIL) have attained\nremarkable success. Despite their effectiveness, the inherent memory\nrestriction results in saving a limited number of exemplars with poor\ndiversity. In this paper, we introduce PESCR, a novel approach that\nsubstantially increases the quantity and enhances the diversity of exemplars\nbased on a pre-trained general-purpose diffusion model, without fine-tuning it\non target datasets or storing it in the memory buffer. Images are compressed\ninto visual and textual prompts, which are saved instead of the original\nimages, decreasing memory consumption by a factor of 24. In subsequent phases,\ndiverse exemplars are regenerated by the diffusion model. We further propose\npartial compression and diffusion-based data augmentation to minimize the\ndomain gap between generated exemplars and real images. PESCR significantly\nimproves CIL performance across multiple benchmarks, e.g., 3.2% above the\nprevious state-of-the-art on ImageNet-100.", "comment": "BMVC 2025. Code: https://github.com/KerryDRX/PESCR", "pdf_url": "http://arxiv.org/pdf/2311.18266v3", "cate": "cs.CV", "date": "2023-11-30", "updated": "2025-07-31"}
{"id": "2505.01712", "title": "World Model-Based Learning for Long-Term Age of Information Minimization in Vehicular Networks", "authors": ["Lingyi Wang", "Rashed Shelim", "Walid Saad", "Naren Ramakrishnan"], "categories": ["cs.AI", "cs.NI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.01712v2", "summary": "Traditional reinforcement learning (RL)-based learning approaches for\nwireless networks rely on expensive trial-and-error mechanisms and real-time\nfeedback based on extensive environment interactions, which leads to low data\nefficiency and short-sighted policies. These limitations become particularly\nproblematic in complex, dynamic networks with high uncertainty and long-term\nplanning requirements. To address these limitations, in this paper, a novel\nworld model-based learning framework is proposed to minimize\npacket-completeness-aware age of information (CAoI) in a vehicular network.\nParticularly, a challenging representative scenario is considered pertaining to\na millimeter-wave (mmWave) vehicle-to-everything (V2X) communication network,\nwhich is characterized by high mobility, frequent signal blockages, and\nextremely short coherence time. Then, a world model framework is proposed to\njointly learn a dynamic model of the mmWave V2X environment and use it to\nimagine trajectories for learning how to perform link scheduling. In\nparticular, the long-term policy is learned in differentiable imagined\ntrajectories instead of environment interactions. Moreover, owing to its\nimagination abilities, the world model can jointly predict time-varying\nwireless data and optimize link scheduling in real-world wireless and V2X\nnetworks. Thus, during intervals without actual observations, the world model\nremains capable of making efficient decisions. Extensive experiments are\nperformed on a realistic simulator based on Sionna that integrates\nphysics-based end-to-end channel modeling, ray-tracing, and scene geometries\nwith material properties. Simulation results show that the proposed world model\nachieves a significant improvement in data efficiency, and achieves 26%\nimprovement and 16% improvement in CAoI, respectively, compared to the\nmodel-based RL (MBRL) method and the model-free RL (MFRL) method.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.01712v2", "cate": "cs.AI", "date": "2025-05-03", "updated": "2025-08-01"}
{"id": "2508.00016", "title": "Extended Abstract: Mutable Objects with Several Implementations", "authors": ["Matt Kaufmann", "Yahya Sohail", "Warren A. Hunt Jr"], "categories": ["cs.PL", "cs.LO"], "primary_category": "Subjects:       Programming Languages (cs.PL)", "pdf_link": null, "comments": "Comments:      In Proceedings ACL2 2025, arXiv:2507.18567", "url": "http://arxiv.org/abs/2508.00016v1", "summary": "This extended abstract outlines an ACL2 feature, attach-stobj, that first\nappeared in ACL2 Version 8.6 (October, 2024). This feature supports different\nexecutable operations for a given abstract stobj, without requiring\nrecertification of the book that introduces that stobj or theorems about it.\nThe paper provides background as well as a user-level overview and some\nimplementation notes.", "comment": "In Proceedings ACL2 2025, arXiv:2507.18567", "pdf_url": "http://arxiv.org/pdf/2508.00016v1", "cate": "cs.PL", "date": "2025-07-25", "updated": "2025-07-25"}
{"id": "2507.22498", "title": "Robust Adverse Weather Removal via Spectral-based Spatial Grouping", "authors": ["Yuhwan Jeong", "Yunseo Yang", "Youngho Yoon", "Kuk-Jin Yoon"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      accepted by ICCV25", "url": "http://arxiv.org/abs/2507.22498v2", "summary": "Adverse weather conditions cause diverse and complex degradation patterns,\ndriving the development of All-in-One (AiO) models. However, recent AiO\nsolutions still struggle to capture diverse degradations, since global\nfiltering methods like direct operations on the frequency domain fail to handle\nhighly variable and localized distortions. To address these issue, we propose\nSpectral-based Spatial Grouping Transformer (SSGformer), a novel approach that\nleverages spectral decomposition and group-wise attention for multi-weather\nimage restoration. SSGformer decomposes images into high-frequency edge\nfeatures using conventional edge detection and low-frequency information via\nSingular Value Decomposition. We utilize multi-head linear attention to\neffectively model the relationship between these features. The fused features\nare integrated with the input to generate a grouping-mask that clusters regions\nbased on the spatial similarity and image texture. To fully leverage this mask,\nwe introduce a group-wise attention mechanism, enabling robust adverse weather\nremoval and ensuring consistent performance across diverse weather conditions.\nWe also propose a Spatial Grouping Transformer Block that uses both channel\nattention and spatial attention, effectively balancing feature-wise\nrelationships and spatial dependencies. Extensive experiments show the\nsuperiority of our approach, validating its effectiveness in handling the\nvaried and intricate adverse weather degradations.", "comment": "accepted by ICCV25", "pdf_url": "http://arxiv.org/pdf/2507.22498v2", "cate": "cs.CV", "date": "2025-07-30", "updated": "2025-07-31"}
{"id": "2404.17484", "title": "Sparse Reconstruction of Optical Doppler Tomography with Alternative State Space Model and Attention", "authors": ["Zhenghong Li", "Jiaxiang Ren", "Wensheng Cheng", "Yanzuo Liu", "Congwu Du", "Yingtian Pan", "Haibin Ling"], "categories": ["cs.CV", "eess.IV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      MICCAI25, 10 pages, 3 figures", "url": "http://arxiv.org/abs/2404.17484v3", "summary": "Optical coherence Doppler tomography (ODT) is an emerging blood flow imaging\ntechnique. The fundamental unit of ODT is the 1D depth-resolved trace named raw\nA-scans (or A-line). A 2D ODT image (B-scan) is formed by reconstructing a\ncross-sectional flow image via Doppler phase-subtraction of raw A-scans along\nB-line. To obtain a high-fidelity B-scan, densely sampled A-scans are required\ncurrently, leading to prolonged scanning time and increased storage demands.\nAddressing this issue, we propose a novel sparse ODT reconstruction framework\nwith an Alternative State Space Attention Network (ASSAN) that effectively\nreduces raw A-scans needed. Inspired by the distinct distributions of\ninformation along A-line and B-line, ASSAN applies 1D State Space Model (SSM)\nto each A-line to learn the intra-A-scan representation, while using 1D gated\nself-attention along B-line to capture the inter-A-scan features. In addition,\nan effective feedforward network based on sequential 1D convolutions along\ndifferent axes is employed to enhance the local feature. In validation\nexperiments on real animal data, ASSAN shows clear effectiveness in the\nreconstruction in comparison with state-of-the-art reconstruction methods.", "comment": "MICCAI25, 10 pages, 3 figures", "pdf_url": "http://arxiv.org/pdf/2404.17484v3", "cate": "cs.CV", "date": "2024-04-26", "updated": "2025-07-30"}
{"id": "2508.00422", "title": "Automated Type Annotation in Python Using Large Language Models", "authors": ["Varun Bharti", "Shashwat Jha", "Dhruv Kumar", "Pankaj Jalote"], "categories": ["cs.PL", "cs.LG"], "primary_category": "Subjects:       Programming Languages (cs.PL)", "pdf_link": null, "comments": "Comments:      Under Review", "url": "http://arxiv.org/abs/2508.00422v1", "summary": "Type annotations in Python enhance maintainability and error detection.\nHowever, generating these annotations manually is error prone and requires\nextra effort. Traditional automation approaches like static analysis, machine\nlearning, and deep learning struggle with limited type vocabularies, behavioral\nover approximation, and reliance on large labeled datasets. In this work, we\nexplore the use of LLMs for generating type annotations in Python. We develop a\ngenerate check repair pipeline: the LLM proposes annotations guided by a\nConcrete Syntax Tree representation, a static type checker (Mypy) verifies\nthem, and any errors are fed back for iterative refinement. We evaluate four\nLLM variants: GPT 4oMini, GPT 4.1mini (general-purpose), and O3Mini, O4Mini\n(reasoning optimized), on 6000 code snippets from the ManyTypes4Py benchmark.\nWe first measure the proportion of code snippets annotated by LLMs for which\nMyPy reported no errors (i.e., consistent results): GPT 4oMini achieved\nconsistency on 65.9% of cases (34.1% inconsistent), while GPT 4.1mini, O3Mini,\nand O4Mini each reached approximately 88.6% consistency (around 11.4%\nfailures). To measure annotation quality, we then compute exact-match and\nbase-type match accuracies over all 6000 snippets: GPT 4.1mini and O3Mini\nperform the best, achieving up to 70.5% exact match and 79.1% base type\naccuracy, requiring under one repair iteration on average. Our results\ndemonstrate that general-purpose and reasoning optimized LLMs, without any task\nspecific fine tuning or additional training can be effective in generating\nconsistent type annotations.They perform competitively with traditional deep\nlearning techniques which require large labeled dataset for training. While our\nwork focuses on Python, the pipeline can be extended to other optionally typed\nimperative languages like Ruby", "comment": "Under Review", "pdf_url": "http://arxiv.org/pdf/2508.00422v1", "cate": "cs.PL", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2507.22530", "title": "HRVVS: A High-resolution Video Vasculature Segmentation Network via Hierarchical Autoregressive Residual Priors", "authors": ["Xincheng Yao", "Yijun Yang", "Kangwei Guo", "Ruiqiang Xiao", "Haipeng Zhou", "Haisu Tao", "Jian Yang", "Lei Zhu"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by MICCAI 2025", "url": "http://arxiv.org/abs/2507.22530v2", "summary": "The segmentation of the hepatic vasculature in surgical videos holds\nsubstantial clinical significance in the context of hepatectomy procedures.\nHowever, owing to the dearth of an appropriate dataset and the inherently\ncomplex task characteristics, few researches have been reported in this domain.\nTo address this issue, we first introduce a high quality frame-by-frame\nannotated hepatic vasculature dataset containing 35 long hepatectomy videos and\n11442 high-resolution frames. On this basis, we propose a novel high-resolution\nvideo vasculature segmentation network, dubbed as HRVVS. We innovatively embed\na pretrained visual autoregressive modeling (VAR) model into different layers\nof the hierarchical encoder as prior information to reduce the information\ndegradation generated during the downsampling process. In addition, we designed\na dynamic memory decoder on a multi-view segmentation network to minimize the\ntransmission of redundant information while preserving more details between\nframes. Extensive experiments on surgical video datasets demonstrate that our\nproposed HRVVS significantly outperforms the state-of-the-art methods. The\nsource code and dataset will be publicly available at\n\\{https://github.com/scott-yjyang/HRVVS}.", "comment": "Accepted by MICCAI 2025", "pdf_url": "http://arxiv.org/pdf/2507.22530v2", "cate": "cs.CV", "date": "2025-07-30", "updated": "2025-07-31"}
{"id": "2408.02123", "title": "FovEx: Human-Inspired Explanations for Vision Transformers and Convolutional Neural Networks", "authors": ["Mahadev Prasad Panda", "Matteo Tiezzi", "Martina Vilas", "Gemma Roig", "Bjoern M. Eskofier", "Dario Zanca"], "categories": ["cs.CV", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted in the International Journal of Computer Vision (Springer Nature)", "url": "http://arxiv.org/abs/2408.02123v3", "summary": "Explainability in artificial intelligence (XAI) remains a crucial aspect for\nfostering trust and understanding in machine learning models. Current visual\nexplanation techniques, such as gradient-based or class-activation-based\nmethods, often exhibit a strong dependence on specific model architectures.\nConversely, perturbation-based methods, despite being model-agnostic, are\ncomputationally expensive as they require evaluating models on a large number\nof forward passes. In this work, we introduce Foveation-based Explanations\n(FovEx), a novel XAI method inspired by human vision. FovEx seamlessly\nintegrates biologically inspired perturbations by iteratively creating foveated\nrenderings of the image and combines them with gradient-based visual\nexplorations to determine locations of interest efficiently. These locations\nare selected to maximize the performance of the model to be explained with\nrespect to the downstream task and then combined to generate an attribution\nmap. We provide a thorough evaluation with qualitative and quantitative\nassessments on established benchmarks. Our method achieves state-of-the-art\nperformance on both transformers (on 4 out of 5 metrics) and convolutional\nmodels (on 3 out of 5 metrics), demonstrating its versatility among various\narchitectures. Furthermore, we show the alignment between the explanation map\nproduced by FovEx and human gaze patterns (+14\\% in NSS compared to RISE,\n+203\\% in NSS compared to GradCAM). This comparison enhances our confidence in\nFovEx's ability to close the interpretation gap between humans and machines.", "comment": "Accepted in the International Journal of Computer Vision (Springer\n  Nature)", "pdf_url": "http://arxiv.org/pdf/2408.02123v3", "cate": "cs.CV", "date": "2024-08-04", "updated": "2025-07-31"}
{"id": "2508.00482", "title": "Semantic Subtyping for Maps in Erlang", "authors": ["Erdem Yildirim", "Albert Schimpf", "Stefan Wehr", "Annette Bieniusa"], "categories": ["cs.PL"], "primary_category": "Subjects:       Programming Languages (cs.PL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00482v1", "summary": "In this paper we will construct a set-theoretic model of types featuring type\nvariables, base types, set-theoretic types and map types. Syntax of map types\nspans all the map types available in Erlang. The model of types is used to\ndefine a semantic subtyping relation based on set containment. The novelty of\nthis work is the definition of subtyping over parameteric map types.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00482v1", "cate": "cs.PL", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2507.22633", "title": "H2Tune: Federated Foundation Model Fine-Tuning with Hybrid Heterogeneity", "authors": ["Wei Guo", "Siyuan Lu", "Yiqi Tong", "Zhaojun Hu", "Fuzhen Zhuang", "Xiao Zhang", "Tao Fan", "Jin Dong"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22633v2", "summary": "Different from existing federated fine-tuning (FFT) methods for foundation\nmodels, hybrid heterogeneous federated fine-tuning (HHFFT) is an under-explored\nscenario where clients exhibit double heterogeneity in model architectures and\ndownstream tasks. This hybrid heterogeneity introduces two significant\nchallenges: 1) heterogeneous matrix aggregation, where clients adopt different\nlarge-scale foundation models based on their task requirements and resource\nlimitations, leading to dimensional mismatches during LoRA parameter\naggregation; and 2) multi-task knowledge interference, where local shared\nparameters, trained with both task-shared and task-specific knowledge, cannot\nensure only task-shared knowledge is transferred between clients. To address\nthese challenges, we propose H2Tune, a federated foundation model fine-tuning\nwith hybrid heterogeneity. Our framework H2Tune consists of three key\ncomponents: (i) sparsified triple matrix decomposition to align hidden\ndimensions across clients through constructing rank-consistent middle matrices,\nwith adaptive sparsification based on client resources; (ii) relation-guided\nmatrix layer alignment to handle heterogeneous layer structures and\nrepresentation capabilities; and (iii) alternating task-knowledge\ndisentanglement mechanism to decouple shared and specific knowledge of local\nmodel parameters through alternating optimization. Theoretical analysis proves\na convergence rate of O(1/\\sqrt{T}). Extensive experiments show our method\nachieves up to 15.4% accuracy improvement compared to state-of-the-art\nbaselines. Our code is available at\nhttps://anonymous.4open.science/r/H2Tune-1407.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22633v2", "cate": "cs.LG", "date": "2025-07-30", "updated": "2025-07-31"}
{"id": "2409.16178", "title": "SDFit: 3D Object Pose and Shape by Fitting a Morphable SDF to a Single Image", "authors": ["Dimitrije Antić", "Georgios Paschalidis", "Shashank Tripathi", "Theo Gevers", "Sai Kumar Dwivedi", "Dimitrios Tzionas"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV'25 Camera Ready; 12 pages, 11 figures, 5 tables", "url": "http://arxiv.org/abs/2409.16178v3", "summary": "Recovering 3D object pose and shape from a single image is a challenging and\nill-posed problem. This is due to strong (self-)occlusions, depth ambiguities,\nthe vast intra- and inter-class shape variance, and the lack of 3D ground truth\nfor natural images. Existing deep-network methods are trained on synthetic\ndatasets to predict 3D shapes, so they often struggle generalizing to\nreal-world images. Moreover, they lack an explicit feedback loop for refining\nnoisy estimates, and primarily focus on geometry without directly considering\npixel alignment. To tackle these limitations, we develop a novel\nrender-and-compare optimization framework, called SDFit. This has three key\ninnovations: First, it uses a learned category-specific and morphable\nsigned-distance-function (mSDF) model, and fits this to an image by iteratively\nrefining both 3D pose and shape. The mSDF robustifies inference by constraining\nthe search on the manifold of valid shapes, while allowing for arbitrary shape\ntopologies. Second, SDFit retrieves an initial 3D shape that likely matches the\nimage, by exploiting foundational models for efficient look-up into 3D shape\ndatabases. Third, SDFit initializes pose by establishing rich 2D-3D\ncorrespondences between the image and the mSDF through foundational features.\nWe evaluate SDFit on three image datasets, i.e., Pix3D, Pascal3D+, and COMIC.\nSDFit performs on par with SotA feed-forward networks for unoccluded images and\ncommon poses, but is uniquely robust to occlusions and uncommon poses.\nMoreover, it requires no retraining for unseen images. Thus, SDFit contributes\nnew insights for generalizing in the wild. Code is available at\nhttps://anticdimi.github.io/sdfit.", "comment": "ICCV'25 Camera Ready; 12 pages, 11 figures, 5 tables", "pdf_url": "http://arxiv.org/pdf/2409.16178v3", "cate": "cs.CV", "date": "2024-09-24", "updated": "2025-07-31"}
{"id": "2508.00534", "title": "Towards a unified framework for programming paradigms: A systematic review of classification formalisms and methodological foundations", "authors": ["Mikel Vandeloise"], "categories": ["cs.PL", "cs.CL", "D.3.2; F.3.2; D.3.1"], "primary_category": "Subjects:       Programming Languages (cs.PL)", "pdf_link": null, "comments": "Comments:      Preprint submitted to the Journal of Object Technology on July 29, 2025. Data available upon request until peer-review is completed", "url": "http://arxiv.org/abs/2508.00534v1", "summary": "The rise of multi-paradigm languages challenges traditional classification\nmethods, leading to practical software engineering issues like interoperability\ndefects. This systematic literature review (SLR) maps the formal foundations of\nprogramming paradigms. Our objective is twofold: (1) to assess the state of the\nart of classification formalisms and their limitations, and (2) to identify the\nconceptual primitives and mathematical frameworks for a more powerful,\nreconstructive approach.\n  Based on a synthesis of 74 primary studies, we find that existing taxonomies\nlack conceptual granularity, a unified formal basis, and struggle with hybrid\nlanguages. In response, our analysis reveals a strong convergence toward a\ncompositional reconstruction of paradigms. This approach identifies a minimal\nset of orthogonal, atomic primitives and leverages mathematical frameworks,\npredominantly Type theory, Category theory and Unifying Theories of Programming\n(UTP), to formally guarantee their compositional properties.\n  We conclude that the literature reflects a significant intellectual shift\naway from classification towards these promising formal, reconstructive\nframeworks. This review provides a map of this evolution and proposes a\nresearch agenda for their unification.", "comment": "Preprint submitted to the Journal of Object Technology on July 29,\n  2025. Data available upon request until peer-review is completed", "pdf_url": "http://arxiv.org/pdf/2508.00534v1", "cate": "cs.PL", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00097", "title": "XRoboToolkit: A Cross-Platform Framework for Robot Teleoperation", "authors": ["Zhigen Zhao", "Liuchuan Yu", "Ke Jing", "Ning Yang"], "categories": ["cs.RO", "cs.AI"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      6 pages, 6 figures, project link: this https URL", "url": "http://arxiv.org/abs/2508.00097v1", "summary": "The rapid advancement of Vision-Language-Action models has created an urgent\nneed for large-scale, high-quality robot demonstration datasets. Although\nteleoperation is the predominant method for data collection, current approaches\nsuffer from limited scalability, complex setup procedures, and suboptimal data\nquality. This paper presents XRoboToolkit, a cross-platform framework for\nextended reality based robot teleoperation built on the OpenXR standard. The\nsystem features low-latency stereoscopic visual feedback, optimization-based\ninverse kinematics, and support for diverse tracking modalities including head,\ncontroller, hand, and auxiliary motion trackers. XRoboToolkit's modular\narchitecture enables seamless integration across robotic platforms and\nsimulation environments, spanning precision manipulators, mobile robots, and\ndexterous hands. We demonstrate the framework's effectiveness through precision\nmanipulation tasks and validate data quality by training VLA models that\nexhibit robust autonomous performance.", "comment": "6 pages, 6 figures, project link: https://github.com/XR-Robotics", "pdf_url": "http://arxiv.org/pdf/2508.00097v1", "cate": "cs.RO", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.22789", "title": "G-Core: A Simple, Scalable and Balanced RLHF Trainer", "authors": ["Junyu Wu", "Weiming Chang", "Xiaotao Liu", "Guanyou He", "Haoqiang Hong", "Boqi Liu", "Hongtao Tian", "Tao Yang", "Yunsheng Shi", "Feng Lin", "Ting Yao"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      I haven't received company approval yet, and I uploaded it by mistake", "url": "http://arxiv.org/abs/2507.22789v2", "summary": "Reinforcement Learning from Human Feedback (RLHF) has become an increasingly\npopular paradigm for training large language models (LLMs) and diffusion\nmodels. While existing RLHF training systems have enabled significant progress,\nthey often face challenges in scaling to multi-modal and diffusion workflows\nand adapting to dynamic workloads. In particular, current approaches may\nencounter limitations in controller scalability, flexible resource placement,\nand efficient orchestration when handling complex RLHF pipelines, especially in\nscenarios involving dynamic sampling or generative reward modeling. In this\npaper, we present \\textbf{G-Core}, a simple, scalable, and balanced RLHF\ntraining framework designed to address these challenges. G-Core introduces a\nparallel controller programming model, enabling flexible and efficient\norchestration of complex RLHF workflows without the bottlenecks of a single\ncentralized controller. Furthermore, we propose a dynamic placement schema that\nadaptively partitions resources and schedules workloads, significantly reducing\nhardware idle time and improving utilization, even under highly variable\ntraining conditions. G-Core has successfully trained models that support WeChat\nproduct features serving a large-scale user base, demonstrating its\neffectiveness and robustness in real-world scenarios. Our results show that\nG-Core advances the state of the art in RLHF training, providing a solid\nfoundation for future research and deployment of large-scale, human-aligned\nmodels.", "comment": "I haven't received company approval yet, and I uploaded it by mistake", "pdf_url": "http://arxiv.org/pdf/2507.22789v2", "cate": "cs.LG", "date": "2025-07-30", "updated": "2025-07-31"}
{"id": "2410.02630", "title": "Understanding implementation pitfalls of distance-based metrics for image segmentation", "authors": ["Gasper Podobnik", "Tomaz Vrtovec"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2410.02630v2", "summary": "Distance-based metrics, such as the Hausdorff distance (HD), are widely used\nto validate segmentation performance in (bio)medical imaging. However, their\nimplementation is complex, and critical differences across open-source tools\nremain largely unrecognized by the community. These discrepancies undermine\nbenchmarking efforts, introduce bias in biomarker calculations, and potentially\ndistort medical device development and clinical commissioning. In this study,\nwe systematically dissect 11 open-source tools that implement distance-based\nmetric computation by performing both a conceptual analysis of their\ncomputational steps and an empirical analysis on representative two- and\nthree-dimensional image datasets. Alarmingly, we observed deviations in HD\nexceeding 100 mm and identified multiple statistically significant differences\nbetween tools - demonstrating that statistically significant improvements on\nthe same set of segmentations can be achieved simply by selecting a particular\nimplementation. These findings cast doubts on the validity of prior comparisons\nof results across studies without accounting for the differences in metric\nimplementations. To address this, we provide practical recommendations for tool\nselection; additionally, our conceptual analysis informs about the future\nevolution of implementing open-source tools.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2410.02630v2", "cate": "cs.CV", "date": "2024-10-03", "updated": "2025-07-31"}
{"id": "2508.00244", "title": "Functional vs. Object-Oriented: Comparing How Programming Paradigms Affect the Architectural Characteristics of Systems", "authors": ["Briza Mel Dias de Sousa", "Renato Cordeiro Ferreira", "Alfredo Goldman"], "categories": ["cs.SE", "cs.PL", "D.3.2; D.2.11; D.2.13"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      11 pages, 16 figures (1 table, 3 diagrams, 5 graphics, 7 listings), submitted to CTICQS capstone project competition at SBQS 2025", "url": "http://arxiv.org/abs/2508.00244v1", "summary": "After decades of dominance by object-oriented programming (OOP), functional\nprogramming (FP) is gaining increasing attention in the software industry. This\nstudy compares the impact of OOP and FP on the architectural characteristics of\nsoftware systems. For that, it examines the design and implementation of a\nDigital Wallet system, developed in Kotlin (representing OOP) and Scala\n(representing FP). The comparison is made through both qualitative and\nquantitative analyses to explore how each paradigm influences the system's\narchitectural characteristics. The self-ethnographic qualitative analysis\nprovides a side-by-side comparison of both implementations, revealing the\nperspective of those writing such code. The survey-based quantitative analysis\ngathers feedback from developers with diverse backgrounds, showing their\nimpressions of those reading this code. Hopefully, these results may be useful\nfor developers or organizations seeking to make more informed decisions about\nwhich paradigm is best suited for their next project.", "comment": "11 pages, 16 figures (1 table, 3 diagrams, 5 graphics, 7 listings),\n  submitted to CTICQS capstone project competition at SBQS 2025", "pdf_url": "http://arxiv.org/pdf/2508.00244v1", "cate": "cs.SE", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00162", "title": "CHILD (Controller for Humanoid Imitation and Live Demonstration): a Whole-Body Humanoid Teleoperation System", "authors": ["Noboru Myers", "Obin Kwon", "Sankalp Yamsani", "Joohyung Kim"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00162v1", "summary": "Recent advances in teleoperation have demonstrated robots performing complex\nmanipulation tasks. However, existing works rarely support whole-body\njoint-level teleoperation for humanoid robots, limiting the diversity of tasks\nthat can be accomplished. This work presents Controller for Humanoid Imitation\nand Live Demonstration (CHILD), a compact reconfigurable teleoperation system\nthat enables joint level control over humanoid robots. CHILD fits within a\nstandard baby carrier, allowing the operator control over all four limbs, and\nsupports both direct joint mapping for full-body control and loco-manipulation.\nAdaptive force feedback is incorporated to enhance operator experience and\nprevent unsafe joint movements. We validate the capabilities of this system by\nconducting loco-manipulation and full-body control examples on a humanoid robot\nand multiple dual-arm systems. Lastly, we open-source the design of the\nhardware promoting accessibility and reproducibility. Additional details and\nopen-source information are available at our project website:\nhttps://uiuckimlab.github.io/CHILD-pages.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00162v1", "cate": "cs.RO", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2508.00505", "title": "A Variant of Non-uniform Cylindrical Algebraic Decomposition for Real Quantifier Elimination", "authors": ["Jasper Nalbach", "Erika Ábrahám"], "categories": ["cs.SC"], "primary_category": "Subjects:       Symbolic Computation (cs.SC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00505v1", "summary": "The Cylindrical Algebraic Decomposition (CAD) method is currently the only\ncomplete algorithm used in practice for solving real-algebraic problems. To\nameliorate its doubly-exponential complexity, different exploration-guided\nadaptations try to avoid some of the computations. The first such adaptation\nnamed NLSAT was followed by Non-uniform CAD (NuCAD) and the Cylindrical\nAlgebraic Covering (CAlC). Both NLSAT and CAlC have been developed and\nimplemented in SMT solvers for satisfiability checking, and CAlC was recently\nalso adapted for quantifier elimination. However, NuCAD was designed for\nquantifier elimination only, and no complete implementation existed before this\nwork.\n  In this paper, we present a novel variant of NuCAD for both real quantifier\nelimination and SMT solving, provide an implementation, and evaluate the method\nby experimentally comparing it to CAlC.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00505v1", "cate": "cs.SC", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2411.04351", "title": "LidaRefer: Context-aware Outdoor 3D Visual Grounding for Autonomous Driving", "authors": ["Yeong-Seung Baek", "Heung-Seon Oh"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      18 pages, 5 figures", "url": "http://arxiv.org/abs/2411.04351v2", "summary": "3D visual grounding (VG) aims to locate objects or regions within 3D scenes\nguided by natural language descriptions. While indoor 3D VG has advanced,\noutdoor 3D VG remains underexplored due to two challenges: (1) large-scale\noutdoor LiDAR scenes are dominated by background points and contain limited\nforeground information, making cross-modal alignment and contextual\nunderstanding more difficult; and (2) most outdoor datasets lack spatial\nannotations for referential non-target objects, which hinders explicit learning\nof referential context. To this end, we propose LidaRefer, a context-aware 3D\nVG framework for outdoor scenes. LidaRefer incorporates an object-centric\nfeature selection strategy to focus on semantically relevant visual features\nwhile reducing computational overhead. Then, its transformer-based\nencoder-decoder architecture excels at establishing fine-grained cross-modal\nalignment between refined visual features and word-level text features, and\ncapturing comprehensive global context. Additionally, we present\nDiscriminative-Supportive Collaborative localization (DiSCo), a novel\nsupervision strategy that explicitly models spatial relationships between\ntarget, contextual, and ambiguous objects for accurate target identification.\nTo enable this without manual labeling, we introduce a pseudo-labeling approach\nthat retrieves 3D localization labels for referential non-target objects.\nLidaRefer achieves state-of-the-art performance on Talk2Car-3D dataset under\nvarious evaluation settings.", "comment": "18 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2411.04351v2", "cate": "cs.CV", "date": "2024-11-07", "updated": "2025-07-31"}
{"id": "2508.00419", "title": "Loop Invariant Generation: A Hybrid Framework of Reasoning optimised LLMs and SMT Solvers", "authors": ["Varun Bharti", "Shashwat Jha", "Dhruv Kumar", "Pankaj Jalote"], "categories": ["cs.LO", "cs.LG", "cs.PL"], "primary_category": "Subjects:       Logic in Computer Science (cs.LO)", "pdf_link": null, "comments": "Comments:      Under Review", "url": "http://arxiv.org/abs/2508.00419v1", "summary": "Loop invariants are essential for proving the correctness of programs with\nloops. Developing loop invariants is challenging, and fully automatic synthesis\ncannot be guaranteed for arbitrary programs. Some approaches have been proposed\nto synthesize loop invariants using symbolic techniques and more recently using\nneural approaches. These approaches are able to correctly synthesize loop\ninvariants only for subsets of standard benchmarks. In this work, we\ninvestigate whether modern, reasoning-optimized large language models can do\nbetter. We integrate OpenAI's O1, O1-mini, and O3-mini into a tightly coupled\ngenerate-and-check pipeline with the Z3 SMT solver, using solver\ncounterexamples to iteratively guide invariant refinement. We use Code2Inv\nbenchmark, which provides C programs along with their formal preconditions and\npostconditions. On this benchmark of 133 tasks, our framework achieves 100%\ncoverage (133 out of 133), outperforming the previous best of 107 out of 133,\nwhile requiring only 1-2 model proposals per instance and 14-55 seconds of\nwall-clock time. These results demonstrate that LLMs possess latent logical\nreasoning capabilities which can help automate loop invariant synthesis. While\nour experiments target C-specific programs, this approach should be\ngeneralizable to other imperative languages.", "comment": "Under Review", "pdf_url": "http://arxiv.org/pdf/2508.00419v1", "cate": "cs.LO", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00258", "title": "Topology-Inspired Morphological Descriptor for Soft Continuum Robots", "authors": ["Zhiwei Wu", "Siyi Wei", "Jiahao Luo", "Jinhui Zhang"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00258v1", "summary": "This paper presents a topology-inspired morphological descriptor for soft\ncontinuum robots by combining a pseudo-rigid-body (PRB) model with Morse theory\nto achieve a quantitative characterization of robot morphologies. By counting\ncritical points of directional projections, the proposed descriptor enables a\ndiscrete representation of multimodal configurations and facilitates\nmorphological classification. Furthermore, we apply the descriptor to\nmorphology control by formulating the target configuration as an optimization\nproblem to compute actuation parameters that generate equilibrium shapes with\ndesired topological features. The proposed framework provides a unified\nmethodology for quantitative morphology description, classification, and\ncontrol of soft continuum robots, with the potential to enhance their precision\nand adaptability in medical applications such as minimally invasive surgery and\nendovascular interventions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00258v1", "cate": "cs.RO", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00512", "title": "Projective Delineability for Single Cell Construction", "authors": ["Jasper Nalbach", "Lucas Michel", "Erika Ábrahám", "Christopher W. Brown", "James H. Davenport", "Matthew England", "Pierre Mathonet", "Naïm Zénaïdi"], "categories": ["cs.SC"], "primary_category": "Subjects:       Symbolic Computation (cs.SC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00512v1", "summary": "The cylindrical algebraic decomposition (CAD) is the only complete method\nused in practice for solving problems like quantifier elimination or SMT\nsolving related to real algebra, despite its doubly exponential complexity.\nRecent exploration-guided algorithms like NLSAT, NuCAD, and CAlC rely on CAD\ntechnology but reduce the computational effort heuristically. Single cell\nconstruction is a paradigm that is used in each of these algorithms.\n  The central property on which the CAD algorithm is based is called\ndelineability. Recently, we introduced a weaker notion called projective\ndelineability which can require fewer computations to guarantee, but needs to\nbe applied carefully. This paper adapts the single cell construction for\nexploiting projective delineability and reports on experimental results.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00512v1", "cate": "cs.SC", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2411.11098", "title": "MolParser: End-to-end Visual Recognition of Molecule Structures in the Wild", "authors": ["Xi Fang", "Jiankun Wang", "Xiaochen Cai", "Shangqian Chen", "Shuwen Yang", "Haoyi Tao", "Nan Wang", "Lin Yao", "Linfeng Zhang", "Guolin Ke"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2411.11098v3", "summary": "In recent decades, chemistry publications and patents have increased rapidly.\nA significant portion of key information is embedded in molecular structure\nfigures, complicating large-scale literature searches and limiting the\napplication of large language models in fields such as biology, chemistry, and\npharmaceuticals. The automatic extraction of precise chemical structures is of\ncritical importance. However, the presence of numerous Markush structures in\nreal-world documents, along with variations in molecular image quality, drawing\nstyles, and noise, significantly limits the performance of existing optical\nchemical structure recognition (OCSR) methods. We present MolParser, a novel\nend-to-end OCSR method that efficiently and accurately recognizes chemical\nstructures from real-world documents, including difficult Markush structure. We\nuse a extended SMILES encoding rule to annotate our training dataset. Under\nthis rule, we build MolParser-7M, the largest annotated molecular image dataset\nto our knowledge. While utilizing a large amount of synthetic data, we employed\nactive learning methods to incorporate substantial in-the-wild data,\nspecifically samples cropped from real patents and scientific literature, into\nthe training process. We trained an end-to-end molecular image captioning\nmodel, MolParser, using a curriculum learning approach. MolParser significantly\noutperforms classical and learning-based methods across most scenarios, with\npotential for broader downstream applications. The dataset is publicly\navailable in huggingface.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2411.11098v3", "cate": "cs.CV", "date": "2024-11-17", "updated": "2025-07-31"}
{"id": "2508.00508", "title": "Desyan: A Platform for Seamless Value-Flow and Symbolic Analysis", "authors": ["Panagiotis Diamantakis", "Thanassis Avgerinos", "Yannis Smaragdakis"], "categories": ["cs.SE", "cs.PL"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00508v1", "summary": "Over the past two decades, two different types of static analyses have\nemerged as dominant paradigms both in academia and industry: value-flow\nanalysis (e.g., data-flow analysis or points-to analysis) and symbolic analysis\n(e.g., symbolic execution). Despite their individual successes in numerous\napplication fields, the two approaches have remained largely separate; an\nartifact of the simple reality that there is no broadly adopted unifying\nplatform for effortless and efficient integration of symbolic techniques with\nhigh-performance data-flow reasoning.\n  To bridge this gap, we introduce Desyan: a platform for writing program\nanalyses with seamless integration of value-flow and symbolic reasoning. Desyan\nexpands a production-ready Datalog fixpoint engine (Souffl\\'e) with\nfull-fledged SMT solving invoking industry-leading SMT engines. Desyan provides\nconstructs for automatically (and efficiently!) handling typical patterns that\ncome up in program analysis. At the same time, the integration is agnostic with\nrespect to the solving technology, and supports Datalog-native symbolic\nreasoning, via a bottom-up algebraic reasoning module.\n  The result is an engine that allows blending different kinds of reasoning, as\nneeded for the underlying analysis. For value-flow analysis, the engine is the\nbest-in-class Datalog evaluator (often by a factor of over 20x in execution\ntime); for applications that require full SMT (e.g., a concolic execution\nengine or other symbolic evaluator that needs to solve arbitrarily complex\nconditions), the engine is leveraging the leading SMT solvers; for lightweight\nsymbolic evaluation (e.g., solving simple conditionals in the context of a\npath-sensitive analysis), the engine can use Datalog-native symbolic reasoning,\nachieving large speedups (often of over 2x) compared to eagerly appealing to an\nSMT solver.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00508v1", "cate": "cs.SE", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00288", "title": "UAV-ON: A Benchmark for Open-World Object Goal Navigation with Aerial Agents", "authors": ["Jianqiang Xiao", "Yuexuan Sun", "Yixin Shao", "Boxi Gan", "Rongqiang Liu", "Yanjing Wu", "Weili Gua", "Xiang Deng"], "categories": ["cs.RO", "cs.CV"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      Accepted to ACM MM Dataset Track 2025", "url": "http://arxiv.org/abs/2508.00288v1", "summary": "Aerial navigation is a fundamental yet underexplored capability in embodied\nintelligence, enabling agents to operate in large-scale, unstructured\nenvironments where traditional navigation paradigms fall short. However, most\nexisting research follows the Vision-and-Language Navigation (VLN) paradigm,\nwhich heavily depends on sequential linguistic instructions, limiting its\nscalability and autonomy. To address this gap, we introduce UAV-ON, a benchmark\nfor large-scale Object Goal Navigation (ObjectNav) by aerial agents in\nopen-world environments, where agents operate based on high-level semantic\ngoals without relying on detailed instructional guidance as in VLN. UAV-ON\ncomprises 14 high-fidelity Unreal Engine environments with diverse semantic\nregions and complex spatial layouts, covering urban, natural, and mixed-use\nsettings. It defines 1270 annotated target objects, each characterized by an\ninstance-level instruction that encodes category, physical footprint, and\nvisual descriptors, allowing grounded reasoning. These instructions serve as\nsemantic goals, introducing realistic ambiguity and complex reasoning\nchallenges for aerial agents. To evaluate the benchmark, we implement several\nbaseline methods, including Aerial ObjectNav Agent (AOA), a modular policy that\nintegrates instruction semantics with egocentric observations for long-horizon,\ngoal-directed exploration. Empirical results show that all baselines struggle\nin this setting, highlighting the compounded challenges of aerial navigation\nand semantic goal grounding. UAV-ON aims to advance research on scalable UAV\nautonomy driven by semantic goal descriptions in complex real-world\nenvironments.", "comment": "Accepted to ACM MM Dataset Track 2025", "pdf_url": "http://arxiv.org/pdf/2508.00288v1", "cate": "cs.RO", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00153", "title": "Green Computing: The Ultimate Carbon Destroyer for a Sustainable Future", "authors": ["Sayed Mahbub Hasan Amiri", "Prasun Goswami", "Md. Mainul Islam", "Mohammad Shakhawat Hossen", "Marzana Mithila", "Naznin Akter"], "categories": ["cs.CY", "cs.SC"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "Comments:      26 Pages, 6 Tables", "url": "http://arxiv.org/abs/2508.00153v1", "summary": "Green computing represents a critical pathway to decarbonize the digital\neconomy while maintaining technological progress. This article examines how\nsustainable IT strategies including energy-efficient hardware, AI-optimized\ndata centres, and circular e-waste systems can transform computing into a net\ncarbon sink. Through analysis of industry best practices and emerging\ntechnologies like quantum computing and biodegradable electronics, we\ndemonstrate achievable reductions of 40-60% in energy consumption without\ncompromising performance. The study highlights three key findings: (1) current\nsolutions already deliver both environmental and economic benefits, with\ntypical payback periods of 3-5 years; (2) systemic barriers including cost\npremiums and policy fragmentation require coordinated action; and (3)\nnext-generation innovations promise order-of-magnitude improvements in\nefficiency. We present a practical framework for stakeholders from corporations\nadopting renewable-powered cloud services to individuals extending device\nlifespans to accelerate the transition. The research underscores computing's\nunique potential as a climate solution through its rapid innovation cycles and\nmeasurable impacts, concluding that strategic investments in green IT today can\nyield disproportionate sustainability dividends across all sectors tomorrow.\nThis work provides both a compelling case for urgent action and a clear roadmap\nto realize computing's potential as a powerful carbon destruction tool in the\nclimate crisis era.", "comment": "26 Pages, 6 Tables", "pdf_url": "http://arxiv.org/pdf/2508.00153v1", "cate": "cs.CY", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2411.18823", "title": "Multi-Task Label Discovery via Hierarchical Task Tokens for Partially Annotated Dense Predictions", "authors": ["Jingdong Zhang", "Hanrong Ye", "Xin Li", "Wenping Wang", "Dan Xu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2411.18823v2", "summary": "In recent years, simultaneous learning of multiple dense prediction tasks\nwith partially annotated label data has emerged as an important research area.\nPrevious works primarily focus on leveraging cross-task relations or conducting\nadversarial training for extra regularization, which achieve promising\nperformance improvements, while still suffering from the lack of direct\npixel-wise supervision and extra training of heavy mapping networks. To\neffectively tackle this challenge, we propose a novel approach to optimize a\nset of compact learnable hierarchical task tokens, including global and\nfine-grained ones, to discover consistent pixel-wise supervision signals in\nboth feature and prediction levels. Specifically, the global task tokens are\ndesigned for effective cross-task feature interactions in a global context.\nThen, a group of fine-grained task-specific spatial tokens for each task is\nlearned from the corresponding global task tokens. It is embedded to have dense\ninteractions with each task-specific feature map. The learned global and local\nfine-grained task tokens are further used to discover pseudo task-specific\ndense labels at different levels of granularity, and they can be utilized to\ndirectly supervise the learning of the multi-task dense prediction framework.\nExtensive experimental results on challenging NYUD-v2, Cityscapes, and PASCAL\nContext datasets demonstrate significant improvements over existing\nstate-of-the-art methods for partially annotated multi-task dense prediction.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2411.18823v2", "cate": "cs.CV", "date": "2024-11-27", "updated": "2025-07-31"}
{"id": "2508.00772", "title": "From Code to Career: Assessing Competitive Programmers for Industry Placement", "authors": ["Md Imranur Rahman Akib", "Fathima Binthe Muhammed", "Umit Saha", "Md Fazlul Karim Patwary", "Mehrin Anannya", "Md Alomgeer Hussein", "Md Biplob Hosen"], "categories": ["cs.SE", "cs.PL"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00772v1", "summary": "In today's fast-paced tech industry, there is a growing need for tools that\nevaluate a programmer's job readiness based on their coding performance. This\nstudy focuses on predicting the potential of Codeforces users to secure various\nlevels of software engineering jobs. The primary objective is to analyze how a\nuser's competitive programming activity correlates with their chances of\nobtaining positions, ranging from entry-level roles to jobs at major tech\ncompanies. We collect user data using the Codeforces API, process key\nperformance metrics, and build a prediction model using a Random Forest\nclassifier. The model categorizes users into four levels of employability,\nranging from those needing further development to those ready for top-tier tech\njobs. The system is implemented using Flask and deployed on Render for\nreal-time predictions. Our evaluation demonstrates that the approach\neffectively distinguishes between different skill levels based on coding\nproficiency and participation. This work lays a foundation for the use of\nmachine learning in career assessment and could be extended to predict job\nreadiness in broader technical fields.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00772v1", "cate": "cs.SE", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00303", "title": "TopoDiffuser: A Diffusion-Based Multimodal Trajectory Prediction Model with Topometric Maps", "authors": ["Zehui Xu", "Junhui Wang", "Yongliang Shi", "Chao Gao", "Guyue Zhou"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00303v1", "summary": "This paper introduces TopoDiffuser, a diffusion-based framework for\nmultimodal trajectory prediction that incorporates topometric maps to generate\naccurate, diverse, and road-compliant future motion forecasts. By embedding\nstructural cues from topometric maps into the denoising process of a\nconditional diffusion model, the proposed approach enables trajectory\ngeneration that naturally adheres to road geometry without relying on explicit\nconstraints. A multimodal conditioning encoder fuses LiDAR observations,\nhistorical motion, and route information into a unified bird's-eye-view (BEV)\nrepresentation. Extensive experiments on the KITTI benchmark demonstrate that\nTopoDiffuser outperforms state-of-the-art methods, while maintaining strong\ngeometric consistency. Ablation studies further validate the contribution of\neach input modality, as well as the impact of denoising steps and the number of\ntrajectory samples. To support future research, we publicly release our code at\nhttps://github.com/EI-Nav/TopoDiffuser.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00303v1", "cate": "cs.RO", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00749", "title": "Dynamic Symbolic Execution for Semantic Difference Analysis of Component and Connector Architectures", "authors": ["Johanna Grahl", "Bernhard Rumpe", "Max Stachon", "Sebastian Stüber"], "categories": ["cs.SE", "cs.FL", "cs.SC", "68N30", "D.2.4"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00749v1", "summary": "In the context of model-driven development, ensuring the correctness and\nconsistency of evolving models is paramount. This paper investigates the\napplication of Dynamic Symbolic Execution (DSE) for semantic difference\nanalysis of component-and-connector architectures, specifically utilizing\nMontiArc models. We have enhanced the existing MontiArc-to-Java generator to\ngather both symbolic and concrete execution data at runtime, encompassing\ntransition conditions, visited states, and internal variables of automata. This\ndata facilitates the identification of significant execution traces that\nprovide critical insights into system behavior. We evaluate various execution\nstrategies based on the criteria of runtime efficiency, minimality, and\ncompleteness, establishing a framework for assessing the applicability of DSE\nin semantic difference analysis. Our findings indicate that while DSE shows\npromise for analyzing component and connector architectures, scalability\nremains a primary limitation, suggesting further research is needed to enhance\nits practical utility in larger systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00749v1", "cate": "cs.SE", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00317", "title": "Advancing Speech Quality Assessment Through Scientific Challenges and Open-source Activities", "authors": ["Wen-Chin Huang"], "categories": ["cs.SD", "eess.AS"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "Comments:      APSIPA ASC 2025 perspective paper", "url": "http://arxiv.org/abs/2508.00317v1", "summary": "Speech quality assessment (SQA) refers to the evaluation of speech quality,\nand developing an accurate automatic SQA method that reflects human perception\nhas become increasingly important, in order to keep up with the generative AI\nboom. In recent years, SQA has progressed to a point that researchers started\nto faithfully use automatic SQA in research papers as a rigorous measurement of\ngoodness for speech generation systems. We believe that the scientific\nchallenges and open-source activities of late have stimulated the growth in\nthis field. In this paper, we review recent challenges as well as open-source\nimplementations and toolkits for SQA, and highlight the importance of\nmaintaining such activities to facilitate the development of not only SQA\nitself but also generative AI for speech.", "comment": "APSIPA ASC 2025 perspective paper", "pdf_url": "http://arxiv.org/pdf/2508.00317v1", "cate": "cs.SD", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2412.06458", "title": "Pruning All-Rounder: Rethinking and Improving Inference Efficiency for Large Vision Language Models", "authors": ["Wei Suo", "Ji Ma", "Mengyang Sun", "Lin Yuanbo Wu", "Peng Wang", "Yanning Zhang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ICCV 25", "url": "http://arxiv.org/abs/2412.06458v2", "summary": "Although Large Vision-Language Models (LVLMs) have achieved impressive\nresults, their high computational costs pose a significant barrier to wide\napplication. To enhance inference efficiency, most existing approaches can be\ncategorized as parameter-dependent or token-dependent strategies to reduce\ncomputational demands. However, parameter-dependent methods require retraining\nLVLMs to recover performance while token-dependent strategies struggle to\nconsistently select the most relevant tokens. In this paper, we systematically\nanalyze the above challenges and provide a series of valuable insights for\ninference acceleration. Based on these findings, we propose a novel framework,\nthe Pruning All-Rounder (PAR). Different from previous works, PAR develops a\nmeta-router to adaptively organize pruning flows across both tokens and layers.\nWith a self-supervised learning manner, our method achieves a superior balance\nbetween performance and efficiency. Notably, PAR is highly flexible, offering\nmultiple pruning versions to address a range of acceleration scenarios. The\ncode for this work is publicly available at\nhttps://github.com/ASGO-MM/Pruning-All-Rounder.", "comment": "Accepted by ICCV 25", "pdf_url": "http://arxiv.org/pdf/2412.06458v2", "cate": "cs.CV", "date": "2024-12-09", "updated": "2025-07-31"}
{"id": "2411.16544", "title": "Float Self-Tagging", "authors": ["Olivier Melançon", "Manuel Serrano", "Marc Feeley"], "categories": ["cs.PL", "D.3.4"], "primary_category": "Subjects:       Programming Languages (cs.PL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2411.16544v3", "summary": "Dynamic and polymorphic languages attach information, such as types, to run\ntime objects, and therefore adapt the memory layout of values to include space\nfor this information. This makes it difficult to efficiently implement IEEE754\nfloating-point numbers as this format does not leave an easily accessible space\nto store type information. The three main floating-point number encodings in\nuse today, tagged pointers, NaN-boxing, and NuN-boxing, have drawbacks. Tagged\npointers entail a heap allocation of all float objects, and NaN/NuN-boxing puts\nadditional run time costs on type checks and the handling of other objects.\n  This paper introduces self-tagging, a new approach to object tagging that\nuses an invertible bitwise transformation to map floating-point numbers to\ntagged values that contain the correct type information at the correct position\nin their bit pattern, superimposing both their value and type information in a\nsingle machine word. Such a transformation can only map a subset of all floats\nto correctly typed tagged values, hence self-tagging takes advantage of the\nnon-uniform distribution of floating point numbers used in practice to avoid\nheap allocation of the most frequently encountered floats.\n  Variants of self-tagging were implemented in two distinct Scheme compilers\nand evaluated on four microarchitectures to assess their performance and\ncompare them to tagged pointers, NaN-boxing, and NuN-boxing. Experiments\ndemonstrate that, in practice, the approach eliminates heap allocation of\nnearly all floating-point numbers and provides good execution speed of\nfloat-intensive benchmarks in Scheme with a negligible performance impact on\nother benchmarks, making it an attractive alternative to tagged pointers,\nalongside NaN-boxing and NuN-boxing.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2411.16544v3", "cate": "cs.PL", "date": "2024-11-25", "updated": "2025-08-01"}
{"id": "2508.00354", "title": "Omni-Scan: Creating Visually-Accurate Digital Twin Object Models Using a Bimanual Robot with Handover and Gaussian Splat Merging", "authors": ["Tianshuang Qiu", "Zehan Ma", "Karim El-Refai", "Hiya Shah", "Chung Min Kim", "Justin Kerr", "Ken Goldberg"], "categories": ["cs.RO", "cs.CV"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00354v1", "summary": "3D Gaussian Splats (3DGSs) are 3D object models derived from multi-view\nimages. Such \"digital twins\" are useful for simulations, virtual reality,\nmarketing, robot policy fine-tuning, and part inspection. 3D object scanning\nusually requires multi-camera arrays, precise laser scanners, or robot\nwrist-mounted cameras, which have restricted workspaces. We propose Omni-Scan,\na pipeline for producing high-quality 3D Gaussian Splat models using a\nbi-manual robot that grasps an object with one gripper and rotates the object\nwith respect to a stationary camera. The object is then re-grasped by a second\ngripper to expose surfaces that were occluded by the first gripper. We present\nthe Omni-Scan robot pipeline using DepthAny-thing, Segment Anything, as well as\nRAFT optical flow models to identify and isolate objects held by a robot\ngripper while removing the gripper and the background. We then modify the 3DGS\ntraining pipeline to support concatenated datasets with gripper occlusion,\nproducing an omni-directional (360 degree view) model of the object. We apply\nOmni-Scan to part defect inspection, finding that it can identify visual or\ngeometric defects in 12 different industrial and household objects with an\naverage accuracy of 83%. Interactive videos of Omni-Scan 3DGS models can be\nfound at https://berkeleyautomation.github.io/omni-scan/", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00354v1", "cate": "cs.RO", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00160", "title": "DeformTune: A Deformable XAI Music Prototype for Non-Musicians", "authors": ["Ziqing Xu", "Nick Bryan-Kinns"], "categories": ["cs.HC", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      In Proceedings of Explainable AI for the Arts Workshop 2025 (XAIxArts 2025) arXiv:2406.14485", "url": "http://arxiv.org/abs/2508.00160v1", "summary": "Many existing AI music generation tools rely on text prompts, complex\ninterfaces, or instrument-like controls, which may require musical or technical\nknowledge that non-musicians do not possess. This paper introduces DeformTune,\na prototype system that combines a tactile deformable interface with the\nMeasureVAE model to explore more intuitive, embodied, and explainable AI\ninteraction. We conducted a preliminary study with 11 adult participants\nwithout formal musical training to investigate their experience with\nAI-assisted music creation. Thematic analysis of their feedback revealed\nrecurring challenge--including unclear control mappings, limited expressive\nrange, and the need for guidance throughout use. We discuss several design\nopportunities for enhancing explainability of AI, including multimodal feedback\nand progressive interaction support. These findings contribute early insights\ntoward making AI music systems more explainable and empowering for novice\nusers.", "comment": "In Proceedings of Explainable AI for the Arts Workshop 2025 (XAIxArts\n  2025) arXiv:2406.14485", "pdf_url": "http://arxiv.org/pdf/2508.00160v1", "cate": "cs.HC", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2501.02201", "title": "Acknowledging Focus Ambiguity in Visual Questions", "authors": ["Chongyan Chen", "Yu-Yun Tseng", "Zhuoheng Li", "Anush Venkatesh", "Danna Gurari"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2501.02201v2", "summary": "No published work on visual question answering (VQA) accounts for ambiguity\nregarding where the content described in the question is located in the image.\nTo fill this gap, we introduce VQ-FocusAmbiguity, the first VQA dataset that\nvisually grounds each plausible image region a question could refer to when\narriving at valid answers. We next analyze and compare our dataset to existing\ndatasets to reveal its unique properties. Finally, we benchmark modern models\nfor two novel tasks related to acknowledging focus ambiguity: recognizing\nwhether a visual question has focus ambiguity and locating all plausible focus\nregions within the image. Results show that the dataset is challenging for\nmodern models. To facilitate future progress on these tasks, we publicly share\nthe dataset with an evaluation server at\nhttps://vizwiz.org/tasks-and-datasets/focus-ambiguity-in-visual-questions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2501.02201v2", "cate": "cs.CV", "date": "2025-01-04", "updated": "2025-07-31"}
{"id": "2503.21691", "title": "Place Capability Graphs: A General-Purpose Model of Rust's Ownership and Borrowing Guarantees", "authors": ["Zachary Grannan", "Aurel Bílý", "Jonáš Fiala", "Jasper Geer", "Markus de Medeiros", "Peter Müller", "Alexander J. Summers"], "categories": ["cs.PL"], "primary_category": "Subjects:       Programming Languages (cs.PL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.21691v4", "summary": "Rust's novel type system has proved an attractive target for verification and\nprogram analysis tools, due to the rich guarantees it provides for controlling\naliasing and mutability. However, fully understanding, extracting and\nexploiting these guarantees is subtle and challenging: existing models for\nRust's type checking either support a smaller idealised language disconnected\nfrom real-world Rust code, or come with severe limitations in terms of precise\nmodelling of Rust borrows, composite types storing them, function signatures\nand loops.\n  In this paper, we present a novel model of Rust's type-checking called Place\nCapability Graphs, which lifts these limitations, and which can be directly\ncalculated from the Rust compiler's own programmatic representations and\nanalyses. We demonstrate that our model supports over 97% of Rust functions in\nthe most popular public crates, and show its suitability as a general-purpose\nbasis for verification and program analysis tools by developing promising new\nprototype versions of the existing Flowistry and Prusti tools.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.21691v4", "cate": "cs.PL", "date": "2025-03-27", "updated": "2025-08-01"}
{"id": "2508.00355", "title": "TOP: Time Optimization Policy for Stable and Accurate Standing Manipulation with Humanoid Robots", "authors": ["Zhenghan Chen", "Haocheng Xu", "Haodong Zhang", "Liang Zhang", "He Li", "Dongqi Wang", "Jiyu Yu", "Yifei Yang", "Zhongxiang Zhou", "Rong Xiong"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00355v1", "summary": "Humanoid robots have the potential capability to perform a diverse range of\nmanipulation tasks, but this is based on a robust and precise standing\ncontroller. Existing methods are either ill-suited to precisely control\nhigh-dimensional upper-body joints, or difficult to ensure both robustness and\naccuracy, especially when upper-body motions are fast. This paper proposes a\nnovel time optimization policy (TOP), to train a standing manipulation control\nmodel that ensures balance, precision, and time efficiency simultaneously, with\nthe idea of adjusting the time trajectory of upper-body motions but not only\nstrengthening the disturbance resistance of the lower-body. Our approach\nconsists of three parts. Firstly, we utilize motion prior to represent\nupper-body motions to enhance the coordination ability between the upper and\nlower-body by training a variational autoencoder (VAE). Then we decouple the\nwhole-body control into an upper-body PD controller for precision and a\nlower-body RL controller to enhance robust stability. Finally, we train TOP\nmethod in conjunction with the decoupled controller and VAE to reduce the\nbalance burden resulting from fast upper-body motions that would destabilize\nthe robot and exceed the capabilities of the lower-body RL policy. The\neffectiveness of the proposed approach is evaluated via both simulation and\nreal world experiments, which demonstrate the superiority on standing\nmanipulation tasks stably and accurately. The project page can be found at\nhttps://anonymous.4open.science/w/top-258F/.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00355v1", "cate": "cs.RO", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00240", "title": "Ambisonics Super-Resolution Using A Waveform-Domain Neural Network", "authors": ["Ismael Nawfal", "Symeon Delikaris Manias", "Mehrez Souden", "Juha Merimaa", "Joshua Atkins", "Elisabeth McMullin", "Shadi Pirhosseinloo", "Daniel Phillips"], "categories": ["eess.AS", "cs.SD"], "primary_category": "Subjects:       Audio and Speech Processing (eess.AS)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00240v1", "summary": "Ambisonics is a spatial audio format describing a sound field. First-order\nAmbisonics (FOA) is a popular format comprising only four channels. This\nlimited channel count comes at the expense of spatial accuracy. Ideally one\nwould be able to take the efficiency of a FOA format without its limitations.\nWe have devised a data-driven spatial audio solution that retains the\nefficiency of the FOA format but achieves quality that surpasses conventional\nrenderers. Utilizing a fully convolutional time-domain audio neural network\n(Conv-TasNet), we created a solution that takes a FOA input and provides a\nhigher order Ambisonics (HOA) output. This data driven approach is novel when\ncompared to typical physics and psychoacoustic based renderers. Quantitative\nevaluations showed a 0.6dB average positional mean squared error difference\nbetween predicted and actual 3rd order HOA. The median qualitative rating\nshowed an 80% improvement in perceived quality over the traditional rendering\napproach.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00240v1", "cate": "eess.AS", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2502.02171", "title": "DeepForest: Sensing Into Self-Occluding Volumes of Vegetation With Aerial Imaging", "authors": ["Mohamed Youssef", "Jian Peng", "Oliver Bimber"], "categories": ["cs.CV", "eess.IV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2502.02171v3", "summary": "Access to below-canopy volumetric vegetation data is crucial for\nunderstanding ecosystem dynamics. We address the long-standing limitation of\nremote sensing to penetrate deep into dense canopy layers. LiDAR and radar are\ncurrently considered the primary options for measuring 3D vegetation\nstructures, while cameras can only extract the reflectance and depth of top\nlayers. Using conventional, high-resolution aerial images, our approach allows\nsensing deep into self-occluding vegetation volumes, such as forests. It is\nsimilar in spirit to the imaging process of wide-field microscopy, but can\nhandle much larger scales and strong occlusion. We scan focal stacks by\nsynthetic-aperture imaging with drones and reduce out-of-focus signal\ncontributions using pre-trained 3D convolutional neural networks with mean\nsquared error (MSE) as the loss function. The resulting volumetric reflectance\nstacks contain low-frequency representations of the vegetation volume.\nCombining multiple reflectance stacks from various spectral channels provides\ninsights into plant health, growth, and environmental conditions throughout the\nentire vegetation volume. Compared with simulated ground truth, our correction\nleads to ~x7 average improvements (min: ~x2, max: ~x12) for forest densities of\n220 trees/ha - 1680 trees/ha. In our field experiment, we achieved an MSE of\n0.05 when comparing with the top-vegetation layer that was measured with\nclassical multispectral aerial imaging.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2502.02171v3", "cate": "cs.CV", "date": "2025-02-04", "updated": "2025-07-31"}
{"id": "2410.19940", "title": "Cobblestone: Iterative Automation for Formal Verification", "authors": ["Saketh Ram Kasibatla", "Arpan Agarwal", "Yuriy Brun", "Sorin Lerner", "Talia Ringer", "Emily First"], "categories": ["cs.LO", "cs.AI", "cs.PL"], "primary_category": "Subjects:       Logic in Computer Science (cs.LO)", "pdf_link": null, "comments": "Comments:      14 pages, 14 figures", "url": "http://arxiv.org/abs/2410.19940v2", "summary": "Formal verification using proof assistants, such as Coq, is an effective way\nof improving software quality, but requires significant effort and expertise.\nMachine learning can automatically synthesize proofs, but such tools are able\nto prove only a fraction of desired software properties. We introduce\nCobblestone, a divide-and-conquer approach for proof synthesis. Cobblestone\nuses a large language model (LLM) to generate potential proofs, uses those\nproofs to break the problem into simpler parts, automatically identifies which\nof those parts were successfully proven, and iterates on the remaining parts to\nbuild a correct proof that is guaranteed to be sound, despite the reliance on\nunsound LLMs. We evaluate Cobblestone on four benchmarks of open-source Coq\nprojects, controlling for training data leakage. Fully automatically,\nCobblestone outperforms state-of-the-art non-LLM tools, and proves many\ntheorems that other LLM-based tools cannot, and on many benchmarks, outperforms\nthem. Each Cobblestone run costs only $1.25 and takes 14.7 minutes, on average.\nCobblestone can also be used with external input, from a user or another tool,\nproviding a proof structure or relevant lemmas. Evaluated with such an oracle,\nCobblestone proves up to 58% of theorems. Overall, our research shows that\ntools can make use of partial progress and external input to more effectively\nautomate formal verification.", "comment": "14 pages, 14 figures", "pdf_url": "http://arxiv.org/pdf/2410.19940v2", "cate": "cs.LO", "date": "2024-10-25", "updated": "2025-07-31"}
{"id": "2508.00362", "title": "A Whole-Body Motion Imitation Framework from Human Data for Full-Size Humanoid Robot", "authors": ["Zhenghan Chen", "Haodong Zhang", "Dongqi Wang", "Jiyu Yu", "Haocheng Xu", "Yue Wang", "Rong Xiong"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00362v1", "summary": "Motion imitation is a pivotal and effective approach for humanoid robots to\nachieve a more diverse range of complex and expressive movements, making their\nperformances more human-like. However, the significant differences in\nkinematics and dynamics between humanoid robots and humans present a major\nchallenge in accurately imitating motion while maintaining balance. In this\npaper, we propose a novel whole-body motion imitation framework for a full-size\nhumanoid robot. The proposed method employs contact-aware whole-body motion\nretargeting to mimic human motion and provide initial values for reference\ntrajectories, and the non-linear centroidal model predictive controller ensures\nthe motion accuracy while maintaining balance and overcoming external\ndisturbances in real time. The assistance of the whole-body controller allows\nfor more precise torque control. Experiments have been conducted to imitate a\nvariety of human motions both in simulation and in a real-world humanoid robot.\nThese experiments demonstrate the capability of performing with accuracy and\nadaptability, which validates the effectiveness of our approach.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00362v1", "cate": "cs.RO", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00307", "title": "Beamformed 360° Sound Maps: U-Net-Driven Acoustic Source Segmentation and Localization", "authors": ["Belman Jahir Rodriguez", "Sergio F. Chevtchenko", "Marcelo Herrera Martinez", "Yeshwant Bethy", "Saeed Afshar"], "categories": ["eess.AS", "cs.AI", "cs.SD", "eess.SP"], "primary_category": "Subjects:       Audio and Speech Processing (eess.AS)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00307v1", "summary": "We introduce a U-net model for 360{\\deg} acoustic source localization\nformulated as a spherical semantic segmentation task. Rather than regressing\ndiscrete direction-of-arrival (DoA) angles, our model segments beamformed audio\nmaps (azimuth and elevation) into regions of active sound presence. Using\ndelay-and-sum (DAS) beamforming on a custom 24-microphone array, we generate\nsignals aligned with drone GPS telemetry to create binary supervision masks. A\nmodified U-Net, trained on frequency-domain representations of these maps,\nlearns to identify spatially distributed source regions while addressing class\nimbalance via the Tversky loss. Because the network operates on beamformed\nenergy maps, the approach is inherently array-independent and can adapt to\ndifferent microphone configurations without retraining from scratch. The\nsegmentation outputs are post-processed by computing centroids over activated\nregions, enabling robust DoA estimates. Our dataset includes real-world\nopen-field recordings of a DJI Air 3 drone, synchronized with 360{\\deg} video\nand flight logs across multiple dates and locations. Experimental results show\nthat U-net generalizes across environments, providing improved angular\nprecision, offering a new paradigm for dense spatial audio understanding beyond\ntraditional Sound Source Localization (SSL).", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00307v1", "cate": "eess.AS", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2502.16421", "title": "Learning from Rendering: Realistic and Controllable Extreme Rainy Image Synthesis for Autonomous Driving Simulation", "authors": ["Kaibin Zhou", "Kaifeng Huang", "Hao Deng", "Zelin Tao", "Ziniu Liu", "Lin Zhang", "Shengjie Zhao"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2502.16421v2", "summary": "Autonomous driving simulators provide an effective and low-cost alternative\nfor evaluating or enhancing visual perception models. However, the reliability\nof evaluation depends on the diversity and realism of the generated scenes.\nExtreme weather conditions, particularly extreme rainfalls, are rare and costly\nto capture in real-world settings. While simulated environments can help\naddress this limitation, existing rainy image synthesizers often suffer from\npoor controllability over illumination and limited realism, which significantly\nundermines the effectiveness of the model evaluation. To that end, we propose a\nlearning-from-rendering rainy image synthesizer, which combines the benefits of\nthe realism of rendering-based methods and the controllability of\nlearning-based methods. To validate the effectiveness of our extreme rainy\nimage synthesizer on semantic segmentation task, we require a continuous set of\nwell-labeled extreme rainy images. By integrating the proposed synthesizer with\nthe CARLA driving simulator, we develop CARLARain an extreme rainy street scene\nsimulator which can obtain paired rainy-clean images and labels under complex\nillumination conditions. Qualitative and quantitative experiments validate that\nCARLARain can effectively improve the accuracy of semantic segmentation models\nin extreme rainy scenes, with the models' accuracy (mIoU) improved by 5% - 8%\non the synthetic dataset and significantly enhanced in real extreme rainy\nscenarios under complex illuminations. Our source code and datasets are\navailable at https://github.com/kb824999404/CARLARain/.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2502.16421v2", "cate": "cs.CV", "date": "2025-02-23", "updated": "2025-07-31"}
{"id": "2507.23186", "title": "NaN-Propagation: A Novel Method for Sparsity Detection in Black-Box Computational Functions", "authors": ["Peter Sharpe"], "categories": ["cs.LG", "cs.PL"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23186v2", "summary": "When numerically evaluating a function's gradient, sparsity detection can\nenable substantial computational speedups through Jacobian coloring and\ncompression. However, sparsity detection techniques for black-box functions are\nlimited, and existing finite-difference-based methods suffer from false\nnegatives due to coincidental zero gradients. These false negatives can\nsilently corrupt gradient calculations, leading to difficult-to-diagnose\nerrors. We introduce NaN-propagation, which exploits the universal\ncontamination property of IEEE 754 Not-a-Number values to trace input-output\ndependencies through floating-point numerical computations. By systematically\ncontaminating inputs with NaN and observing which outputs become NaN, the\nmethod reconstructs conservative sparsity patterns that eliminate a major\nsource of false negatives. We demonstrate this approach on an aerospace wing\nweight model, achieving a 1.52x speedup while uncovering dozens of dependencies\nmissed by conventional methods -- a significant practical improvement since\ngradient computation is often the bottleneck in optimization workflows. The\ntechnique leverages IEEE 754 compliance to work across programming languages\nand math libraries without requiring modifications to existing black-box codes.\nFurthermore, advanced strategies such as NaN payload encoding via direct bit\nmanipulation enable faster-than-linear time complexity, yielding speed\nimprovements over existing black-box sparsity detection methods. Practical\nalgorithms are also proposed to mitigate challenges from branching code\nexecution common in engineering applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23186v2", "cate": "cs.LG", "date": "2025-07-31", "updated": "2025-08-01"}
{"id": "2508.00384", "title": "On Learning Closed-Loop Probabilistic Multi-Agent Simulator", "authors": ["Juanwu Lu", "Rohit Gupta", "Ahmadreza Moradipari", "Kyungtae Han", "Ruqi Zhang", "Ziran Wang"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      Accepted to IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) 2025. Source Code: this https URL", "url": "http://arxiv.org/abs/2508.00384v1", "summary": "The rapid iteration of autonomous vehicle (AV) deployments leads to\nincreasing needs for building realistic and scalable multi-agent traffic\nsimulators for efficient evaluation. Recent advances in this area focus on\nclosed-loop simulators that enable generating diverse and interactive\nscenarios. This paper introduces Neural Interactive Agents (NIVA), a\nprobabilistic framework for multi-agent simulation driven by a hierarchical\nBayesian model that enables closed-loop, observation-conditioned simulation\nthrough autoregressive sampling from a latent, finite mixture of Gaussian\ndistributions. We demonstrate how NIVA unifies preexisting sequence-to-sequence\ntrajectory prediction models and emerging closed-loop simulation models trained\non Next-token Prediction (NTP) from a Bayesian inference perspective.\nExperiments on the Waymo Open Motion Dataset demonstrate that NIVA attains\ncompetitive performance compared to the existing method while providing\nembellishing control over intentions and driving styles.", "comment": "Accepted to IEEE/RSJ International Conference on Intelligent Robots\n  and Systems (IROS) 2025. Source Code: https://github.com/juanwulu/niva", "pdf_url": "http://arxiv.org/pdf/2508.00384v1", "cate": "cs.RO", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00479", "title": "Wavelet-Based Time-Frequency Fingerprinting for Feature Extraction of Traditional Irish Music", "authors": ["Noah Shore"], "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "Subjects:       Audio and Speech Processing (eess.AS)", "pdf_link": null, "comments": "Comments:      Master's thesis. The focus of the thesis is on the underlying techniques for signal fingerprinting", "url": "http://arxiv.org/abs/2508.00479v1", "summary": "This work presents a wavelet-based approach to time-frequency fingerprinting\nfor time series feature extraction, with a focus on audio identification from\nlive recordings of traditional Irish tunes. The challenges of identifying\nfeatures in time-series data are addressed by employing a continuous wavelet\ntransform to extract spectral features and wavelet coherence analysis is used\nto compare recorded audio spectrograms to synthetically generated tunes. The\nsynthetic tunes are derived from ABC notation, which is a common symbolic\nrepresentation for Irish music. Experimental results demonstrate that the\nwavelet-based method can accurately and efficiently identify recorded tunes.\nThis research study also details the performance of the wavelet coherence\nmodel, highlighting its strengths over other methods of time-frequency\ndecomposition. Additionally, we discuss and deploy the model on several\napplications beyond music, including in EEG signal analysis and financial time\nseries forecasting.", "comment": "Master's thesis. The focus of the thesis is on the underlying\n  techniques for signal fingerprinting", "pdf_url": "http://arxiv.org/pdf/2508.00479v1", "cate": "eess.AS", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2502.20263", "title": "Vector-Quantized Vision Foundation Models for Object-Centric Learning", "authors": ["Rongzhen Zhao", "Vivienne Wang", "Juho Kannala", "Joni Pajarinen"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ACM MM 2025", "url": "http://arxiv.org/abs/2502.20263v4", "summary": "Perceiving visual scenes as objects and background--like humans\ndo--Object-Centric Learning (OCL) aggregates image or video feature maps into\nobject-level feature vectors, termed \\textit{slots}. OCL's self-supervision of\nreconstructing the input from these aggregated slots struggles with complex\nobject textures, thus Vision Foundation Model (VFM) representations are used as\nthe aggregation input and reconstruction target. However, existing methods\nleverage VFM representations in diverse ways and often fail to fully exploit\ntheir potential. In response, we propose a clean architecture--Vector-Quantized\nVFMs for OCL (VQ-VFM-OCL, or VVO)--that unifies mainstream OCL methods. The key\nto our unification is simple yet effective, just shared quantizing the same VFM\nrepresentation as the reconstruction target. Through mathematical modeling and\nstatistical verification, we further analyze why VFM representations facilitate\nOCL aggregation and how their shared quantization as reconstruction targets\nstrengthens OCL supervision. Experiments show that across different VFMs,\naggregators and decoders, our VVO consistently outperforms baselines in object\ndiscovery and recognition, as well as downstream visual prediction and\nreasoning. The implementation and model checkpoints are available on\nhttps://github.com/Genera1Z/VQ-VFM-OCL.", "comment": "Accepted by ACM MM 2025", "pdf_url": "http://arxiv.org/pdf/2502.20263v4", "cate": "cs.CV", "date": "2025-02-27", "updated": "2025-07-31"}
{"id": "2508.00467", "title": "SubCDM: Collective Decision-Making with a Swarm Subset", "authors": ["Samratul Fuady", "Danesh Tarapore", "Mohammad D. Soorati"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      6 pages, 7 figures. This paper has been accepted for presentation at the 2025 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2025)", "url": "http://arxiv.org/abs/2508.00467v1", "summary": "Collective decision-making is a key function of autonomous robot swarms,\nenabling them to reach a consensus on actions based on environmental features.\nExisting strategies require the participation of all robots in the\ndecision-making process, which is resource-intensive and prevents the swarm\nfrom allocating the robots to any other tasks. We propose Subset-Based\nCollective Decision-Making (SubCDM), which enables decisions using only a swarm\nsubset. The construction of the subset is dynamic and decentralized, relying\nsolely on local information. Our method allows the swarm to adaptively\ndetermine the size of the subset for accurate decision-making, depending on the\ndifficulty of reaching a consensus. Simulation results using one hundred robots\nshow that our approach achieves accuracy comparable to using the entire swarm\nwhile reducing the number of robots required to perform collective\ndecision-making, making it a resource-efficient solution for collective\ndecision-making in swarm robotics.", "comment": "6 pages, 7 figures. This paper has been accepted for presentation at\n  the 2025 IEEE/RSJ International Conference on Intelligent Robots and Systems\n  (IROS 2025)", "pdf_url": "http://arxiv.org/pdf/2508.00467v1", "cate": "cs.RO", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00501", "title": "VR-PTOLEMAIC: A Virtual Environment for the Perceptual Testing of Spatial Audio Algorithms", "authors": ["Paolo Ostan", "Francesca Del Gaudio", "Federico Miotello", "Mirco Pezzoli", "Fabio Antonacci"], "categories": ["eess.AS", "cs.SD"], "primary_category": "Subjects:       Audio and Speech Processing (eess.AS)", "pdf_link": null, "comments": "Comments:      to appear in EAA Forum Acusticum 2025", "url": "http://arxiv.org/abs/2508.00501v1", "summary": "The perceptual evaluation of spatial audio algorithms is an important step in\nthe development of immersive audio applications, as it ensures that synthesized\nsound fields meet quality standards in terms of listening experience, spatial\nperception and auditory realism. To support these evaluations, virtual reality\ncan offer a powerful platform by providing immersive and interactive testing\nenvironments. In this paper, we present VR-PTOLEMAIC, a virtual reality\nevaluation system designed for assessing spatial audio algorithms. The system\nimplements the MUSHRA (MUlti-Stimulus test with Hidden Reference and Anchor)\nevaluation methodology into a virtual environment. In particular, users can\nposition themselves in each of the 25 simulated listening positions of a\nvirtually recreated seminar room and evaluate simulated acoustic responses with\nrespect to the actually recorded second-order ambisonic room impulse responses,\nall convolved with various source signals. We evaluated the usability of the\nproposed framework through an extensive testing campaign in which assessors\nwere asked to compare the reconstruction capabilities of various sound field\nreconstruction algorithms. Results show that the VR platform effectively\nsupports the assessment of spatial audio algorithms, with generally positive\nfeedback on user experience and immersivity.", "comment": "to appear in EAA Forum Acusticum 2025", "pdf_url": "http://arxiv.org/pdf/2508.00501v1", "cate": "eess.AS", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00031", "title": "Git Context Controller: Manage the Context of LLM-based Agents like Git", "authors": ["Junde Wu"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      in updating", "url": "http://arxiv.org/abs/2508.00031v1", "summary": "Large language model (LLM) based agents have shown impressive capabilities by\ninterleaving internal reasoning with external tool use. However, as these\nagents are deployed in long-horizon workflows, such as coding for a big,\nlong-term project, context management becomes a critical bottleneck. We\nintroduce Git-Context-Controller (GCC), a structured context management\nframework inspired by software version control systems. GCC elevates context as\nversioned memory hierarchy like Git. It structures agent memory as a persistent\nfile system with explicit operations: COMMIT, BRANCH, MERGE, and CONTEXT,\nenabling milestone-based checkpointing, exploration of alternative plans, and\nstructured reflection. Our approach empowers agents to manage long-term goals,\nisolate architectural experiments, and recover or hand off memory across\nsessions and agents. Empirically, agents equipped with GCC achieve\nstate-of-the-art performance on the SWE-Bench-Lite benchmark, resolving 48.00\nof software bugs, outperforming 26 competitive systems. In a self-replication\ncase study, a GCC-augmented agent builds a new CLI agent from scratch,\nachieving 40.7 task resolution, compared to only 11.7 without GCC. The code is\nreleased at: https://github.com/theworldofagents/GCC", "comment": "in updating", "pdf_url": "http://arxiv.org/pdf/2508.00031v1", "cate": "cs.SE", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2502.20760", "title": "VRM: Knowledge Distillation via Virtual Relation Matching", "authors": ["Weijia Zhang", "Fei Xie", "Weidong Cai", "Chao Ma"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ICCV 2025 (Highlight)", "url": "http://arxiv.org/abs/2502.20760v3", "summary": "Knowledge distillation (KD) aims to transfer the knowledge of a more capable\nyet cumbersome teacher model to a lightweight student model. In recent years,\nrelation-based KD methods have fallen behind, as their instance-matching\ncounterparts dominate in performance. In this paper, we revive relational KD by\nidentifying and tackling several key issues in relation-based methods,\nincluding their susceptibility to overfitting and spurious responses.\nSpecifically, we transfer novelly constructed affinity graphs that compactly\nencapsulate a wealth of beneficial inter-sample, inter-class, and inter-view\ncorrelations by exploiting virtual views and relations as a new kind of\nknowledge. As a result, the student has access to richer guidance signals and\nstronger regularisation throughout the distillation process. To further\nmitigate the adverse impact of spurious responses, we prune the affinity graphs\nby dynamically detaching redundant and unreliable edges. Extensive experiments\non CIFAR-100, ImageNet, and MS-COCO datasets demonstrate the superior\nperformance of the proposed virtual relation matching (VRM) method, where it\nconsistently sets new state-of-the-art records over a range of models,\narchitectures, tasks, and set-ups. For instance, VRM for the first time hits\n74.0% accuracy for ResNet50-to-MobileNetV2 distillation on ImageNet, and\nimproves DeiT-T by 14.44% on CIFAR-100 with a ResNet56 teacher.", "comment": "Accepted by ICCV 2025 (Highlight)", "pdf_url": "http://arxiv.org/pdf/2502.20760v3", "cate": "cs.CV", "date": "2025-02-28", "updated": "2025-07-31"}
{"id": "2508.00491", "title": "HannesImitation: Grasping with the Hannes Prosthetic Hand via Imitation Learning", "authors": ["Carlo Alessi", "Federico Vasile", "Federico Ceola", "Giulia Pasquale", "Nicolò Boccardo", "Lorenzo Natale"], "categories": ["cs.RO", "cs.AI"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      Paper accepted at IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)", "url": "http://arxiv.org/abs/2508.00491v1", "summary": "Recent advancements in control of prosthetic hands have focused on increasing\nautonomy through the use of cameras and other sensory inputs. These systems aim\nto reduce the cognitive load on the user by automatically controlling certain\ndegrees of freedom. In robotics, imitation learning has emerged as a promising\napproach for learning grasping and complex manipulation tasks while simplifying\ndata collection. Its application to the control of prosthetic hands remains,\nhowever, largely unexplored. Bridging this gap could enhance dexterity\nrestoration and enable prosthetic devices to operate in more unconstrained\nscenarios, where tasks are learned from demonstrations rather than relying on\nmanually annotated sequences. To this end, we present HannesImitationPolicy, an\nimitation learning-based method to control the Hannes prosthetic hand, enabling\nobject grasping in unstructured environments. Moreover, we introduce the\nHannesImitationDataset comprising grasping demonstrations in table, shelf, and\nhuman-to-prosthesis handover scenarios. We leverage such data to train a single\ndiffusion policy and deploy it on the prosthetic hand to predict the wrist\norientation and hand closure for grasping. Experimental evaluation demonstrates\nsuccessful grasps across diverse objects and conditions. Finally, we show that\nthe policy outperforms a segmentation-based visual servo controller in\nunstructured scenarios. Additional material is provided on our project page:\nhttps://hsp-iit.github.io/HannesImitation", "comment": "Paper accepted at IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS)", "pdf_url": "http://arxiv.org/pdf/2508.00491v1", "cate": "cs.RO", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2505.24437", "title": "SwitchCodec: A High-Fidelity Nerual Audio Codec With Sparse Quantization", "authors": ["Jin Wang", "Wenbin Jiang", "Xiangbo Wang", "Yubo You", "Sheng Fang"], "categories": ["cs.SD", "eess.AS"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "Comments:      12 pages,8 figures", "url": "http://arxiv.org/abs/2505.24437v3", "summary": "Neural audio compression has emerged as a promising technology for\nefficiently representing speech, music, and general audio. However, existing\nmethods suffer from significant performance degradation at limited bitrates,\nwhere the available embedding space is sharply constrained. To address this, we\npropose a universal high-fidelity neural audio compression algorithm featuring\nResidual Experts Vector Quantization (REVQ), which substantially expands the\nembedding space with minimal impact on bandwidth. A gentle load-balancing\nstrategy is introduced to ensure the full utilization of this expanded space.\nFurthermore, we develop a novel multi-tiered discriminator that periodically\nstratifies STFT spectra, guiding the generator to focus on critical spectral\nregions. To support multiple bitrates without quality loss at the lower end, we\nadopt an efficient post-training strategy. Our proposed model achieves\nimpressive performance, with PESQ and ViSQOL scores of 2.87 and 4.27,\nrespectively, at 2.67 kbps bandwidth. The approach effectively reduces spectral\nblur, decreasing the distance to the original mel-spectrogram by 13%. Notably,\nour post-training strategy achieves performance comparable to dedicated\nfixed-bitrate models while reducing the required training time by half.\nExtensive ablation studies confirm the superiority of our method over\nbaselines.", "comment": "12 pages,8 figures", "pdf_url": "http://arxiv.org/pdf/2505.24437v3", "cate": "cs.SD", "date": "2025-05-30", "updated": "2025-08-01"}
{"id": "2508.00033", "title": "GPT-4.1 Sets the Standard in Automated Experiment Design Using Novel Python Libraries", "authors": ["Nuno Fachada", "Daniel Fernandes", "Carlos M. Fernandes", "Bruno D. Ferreira-Saraiva", "João P. Matos-Carvalho"], "categories": ["cs.SE", "cs.AI", "cs.CL", "68T50", "I.2.2; I.2.7; D.2.3"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00033v1", "summary": "Large Language Models (LLMs) have advanced rapidly as tools for automating\ncode generation in scientific research, yet their ability to interpret and use\nunfamiliar Python APIs for complex computational experiments remains poorly\ncharacterized. This study systematically benchmarks a selection of\nstate-of-the-art LLMs in generating functional Python code for two increasingly\nchallenging scenarios: conversational data analysis with the \\textit{ParShift}\nlibrary, and synthetic data generation and clustering using \\textit{pyclugen}\nand \\textit{scikit-learn}. Both experiments use structured, zero-shot prompts\nspecifying detailed requirements but omitting in-context examples. Model\noutputs are evaluated quantitatively for functional correctness and prompt\ncompliance over multiple runs, and qualitatively by analyzing the errors\nproduced when code execution fails. Results show that only a small subset of\nmodels consistently generate correct, executable code, with GPT-4.1 standing\nout as the only model to always succeed in both tasks. In addition to\nbenchmarking LLM performance, this approach helps identify shortcomings in\nthird-party libraries, such as unclear documentation or obscure implementation\nbugs. Overall, these findings highlight current limitations of LLMs for\nend-to-end scientific automation and emphasize the need for careful prompt\ndesign, comprehensive library documentation, and continued advances in language\nmodel capabilities.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00033v1", "cate": "cs.SE", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2503.01199", "title": "LiteGS: A High-performance Framework to Train 3DGS in Subminutes via System and Algorithm Codesign", "authors": ["Kaimin Liao", "Hua Wang", "Zhi Chen", "Luchao Wang", "Yaohua Tang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.01199v2", "summary": "3D Gaussian Splatting (3DGS) has emerged as promising alternative in 3D\nrepresentation. However, it still suffers from high training cost. This paper\nintroduces LiteGS, a high performance framework that systematically optimizes\nthe 3DGS training pipeline from multiple aspects. At the low-level computation\nlayer, we design a ``warp-based raster'' associated with two hardware-aware\noptimizations to significantly reduce gradient reduction overhead. At the\nmid-level data management layer, we introduce dynamic spatial sorting based on\nMorton coding to enable a performant ``Cluster-Cull-Compact'' pipeline and\nimprove data locality, therefore reducing cache misses. At the top-level\nalgorithm layer, we establish a new robust densification criterion based on the\nvariance of the opacity gradient, paired with a more stable opacity control\nmechanism, to achieve more precise parameter growth. Experimental results\ndemonstrate that LiteGS accelerates the original 3DGS training by up to 13.4x\nwith comparable or superior quality and surpasses the current SOTA in\nlightweight models by up to 1.4x speedup. For high-quality reconstruction\ntasks, LiteGS sets a new accuracy record and decreases the training time by an\norder of magnitude.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.01199v2", "cate": "cs.CV", "date": "2025-03-03", "updated": "2025-07-31"}
{"id": "2508.00580", "title": "OmniUnet: A Multimodal Network for Unstructured Terrain Segmentation on Planetary Rovers Using RGB, Depth, and Thermal Imagery", "authors": ["Raul Castilla-Arquillo", "Carlos Perez-del-Pulgar", "Levin Gerdes", "Alfonso Garcia-Cerezo", "Miguel A. Olivares-Mendez"], "categories": ["cs.RO", "cs.AI"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00580v1", "summary": "Robot navigation in unstructured environments requires multimodal perception\nsystems that can support safe navigation. Multimodality enables the integration\nof complementary information collected by different sensors. However, this\ninformation must be processed by machine learning algorithms specifically\ndesigned to leverage heterogeneous data. Furthermore, it is necessary to\nidentify which sensor modalities are most informative for navigation in the\ntarget environment. In Martian exploration, thermal imagery has proven valuable\nfor assessing terrain safety due to differences in thermal behaviour between\nsoil types. This work presents OmniUnet, a transformer-based neural network\narchitecture for semantic segmentation using RGB, depth, and thermal (RGB-D-T)\nimagery. A custom multimodal sensor housing was developed using 3D printing and\nmounted on the Martian Rover Testbed for Autonomy (MaRTA) to collect a\nmultimodal dataset in the Bardenas semi-desert in northern Spain. This location\nserves as a representative environment of the Martian surface, featuring\nterrain types such as sand, bedrock, and compact soil. A subset of this dataset\nwas manually labeled to support supervised training of the network. The model\nwas evaluated both quantitatively and qualitatively, achieving a pixel accuracy\nof 80.37% and demonstrating strong performance in segmenting complex\nunstructured terrain. Inference tests yielded an average prediction time of 673\nms on a resource-constrained computer (Jetson Orin Nano), confirming its\nsuitability for on-robot deployment. The software implementation of the network\nand the labeled dataset have been made publicly available to support future\nresearch in multimodal terrain perception for planetary robotics.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00580v1", "cate": "cs.RO", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2506.00291", "title": "Improving Code Switching with Supervised Fine Tuning and GELU Adapters", "authors": ["Linh Pham"], "categories": ["cs.SD", "eess.AS"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "Comments:      Incorrect results", "url": "http://arxiv.org/abs/2506.00291v2", "summary": "There are few code switching datasets, labeled or unlabled, that exist today.\nAs a result, ASR requires new methods to utilize the vast monolingual data and\nmodels that exist. This paper uses OpenAI's open source ASR model, Whisper,\nwhich has been pre-trained on 680K hours of audio to perform monolingual ASR\ntasks. In Part 1, this paper examines how exploiting Whisper's monolingual\nability to individually tokenize training text, called \"Switching Tokenizers\nMethod\", improves transcription accuracy. In Part 2, we combine the Switching\nTokenizers Method from part 1 and train a GELU based adapter on the encoder.\nThese two methods reduced Total Mixed Error Rate (MER) to 9.4% for the ASCEND\ndataset, 6% for SEAME devman and 9.7% for SEAME devsge, outperforming current\nSoTA methods.", "comment": "Incorrect results", "pdf_url": "http://arxiv.org/pdf/2506.00291v2", "cate": "cs.SD", "date": "2025-05-30", "updated": "2025-08-01"}
{"id": "2508.00045", "title": "Machine Learning Pipeline for Software Engineering: A Systematic Literature Review", "authors": ["Samah Kansab"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00045v1", "summary": "The rapid advancement of software development practices has introduced\nchallenges in ensuring quality and efficiency across the software engineering\n(SE) lifecycle. As SE systems grow in complexity, traditional approaches often\nfail to scale, resulting in longer debugging times, inefficient defect\ndetection, and resource-heavy development cycles. Machine Learning (ML) has\nemerged as a key solution, enabling automation in tasks such as defect\nprediction, code review, and release quality estimation. However, the\neffectiveness of ML in SE depends on the robustness of its pipeline, including\ndata collection, preprocessing, feature engineering, algorithm selection,\nvalidation, and evaluation.\n  This systematic literature review (SLR) examines state-of-the-art ML\npipelines designed for SE, consolidating best practices, challenges, and gaps.\nOur findings show that robust preprocessing, such as SMOTE for data balancing\nand SZZ-based algorithms for feature selection, improves model reliability.\nEnsemble methods like Random Forest and Gradient Boosting dominate performance\nacross tasks, while simpler models such as Naive Bayes remain valuable for\nefficiency and interpretability. Evaluation metrics including AUC, F1-score,\nand precision are most common, with new metrics like Best Arithmetic Mean (BAM)\nemerging in niche applications. Validation techniques such as bootstrapping are\nwidely used to ensure model stability and generalizability.\n  This SLR highlights the importance of well-designed ML pipelines for\naddressing SE challenges and provides actionable insights for researchers and\npractitioners seeking to optimize software quality and efficiency. By\nidentifying gaps and trends, this study sets a foundation for advancing ML\nadoption and fostering innovation in increasingly complex development\nenvironments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00045v1", "cate": "cs.SE", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2508.00497", "title": "From Individuals to Crowds: Dual-Level Public Response Prediction in Social Media", "authors": ["Jinghui Zhang", "Kaiyang Wan", "Longwei Xu", "Ao Li", "Zongfang Liu", "Xiuying Chen"], "categories": ["cs.SI"], "primary_category": "Subjects:       Social and Information Networks (cs.SI)", "pdf_link": null, "comments": "Comments:      ACM MM 2025", "url": "http://arxiv.org/abs/2508.00497v1", "summary": "Public response prediction is critical for understanding how individuals or\ngroups might react to specific events, policies, or social phenomena, making it\nhighly valuable for crisis management, policy-making, and social media\nanalysis. However, existing works face notable limitations. First, they lack\nmicro-level personalization, producing generic responses that ignore individual\nuser preferences. Moreover, they overlook macro-level sentiment distribution\nand only deal with individual-level sentiment, constraining them from analyzing\nbroader societal trends and group sentiment dynamics. To address these\nchallenges, we propose SocialAlign, a unified framework that predicts\nreal-world responses at both micro and macro levels in social contexts. At the\nmicro level, SocialAlign employs SocialLLM with an articulate Personalized\nAnalyze-Compose LoRA (PAC-LoRA) structure, which deploys specialized expert\nmodules for content analysis and response generation across diverse topics and\nuser profiles, enabling the generation of personalized comments with\ncorresponding sentiments. At the macro level, it models group sentiment\ndistributions and aligns predictions with real-world sentiment trends derived\nfrom social media data. To evaluate SocialAlign in real-world scenarios, we\nintroduce SentiWeibo, a large-scale dataset curated from authentic social\ninteractions on the Weibo platform. Experimental results on our SentiWeibo and\nrelated LaMP benchmark demonstrate that SocialAlign surpasses strong baselines,\nshowing improved accuracy, interpretability, and generalization in public\nresponse prediction. We hope our work inspires further research in public\nresponse prediction and computational social science:\nhttps://github.com/Znull-1220/SocialAlign.", "comment": "ACM MM 2025", "pdf_url": "http://arxiv.org/pdf/2508.00497v1", "cate": "cs.SI", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2503.03222", "title": "Mocap-2-to-3: Multi-view Lifting for Monocular Motion Recovery with 2D Pretraining", "authors": ["Zhumei Wang", "Zechen Hu", "Ruoxi Guo", "Huaijin Pi", "Ziyong Feng", "Sida Peng", "Xiaowei Zhou", "Mingtao Pei", "Siyuan Huang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Project page: this https URL", "url": "http://arxiv.org/abs/2503.03222v5", "summary": "Recovering absolute human motion from monocular inputs is challenging due to\ntwo main issues. First, existing methods depend on 3D training data collected\nfrom limited environments, constraining out-of-distribution generalization. The\nsecond issue is the difficulty of estimating metric-scale poses from monocular\ninput. To address these challenges, we introduce Mocap-2-to-3, a novel\nframework that performs multi-view lifting from monocular input by leveraging\n2D data pre-training, enabling the reconstruction of metrically accurate 3D\nmotions with absolute positions. To leverage abundant 2D data, we decompose\ncomplex 3D motion into multi-view syntheses. We first pretrain a single-view\ndiffusion model on extensive 2D datasets, then fine-tune a multi-view model\nusing public 3D data to enable view-consistent motion generation from monocular\ninput, allowing the model to acquire action priors and diversity through 2D\ndata. Furthermore, to recover absolute poses, we propose a novel human motion\nrepresentation that decouples the learning of local pose and global movements,\nwhile encoding geometric priors of the ground to accelerate convergence. This\nenables progressive recovery of motion in absolute space during inference.\nExperimental results on in-the-wild benchmarks demonstrate that our method\nsurpasses state-of-the-art approaches in both camera-space motion realism and\nworld-grounded human positioning, while exhibiting superior generalization\ncapability. Our code will be made publicly available.", "comment": "Project page: https://wangzhumei.github.io/mocap-2-to-3/", "pdf_url": "http://arxiv.org/pdf/2503.03222v5", "cate": "cs.CV", "date": "2025-03-05", "updated": "2025-07-31"}
{"id": "2508.00584", "title": "A control scheme for collaborative object transportation between a human and a quadruped robot using the MIGHTY suction cup", "authors": ["Konstantinos Plotas", "Emmanouil Papadakis", "Drosakis Drosakis", "Panos Trahanias", "Dimitrios Papageorgiou"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      Please find the citation info @ Zenodo, ArXiv or Zenodo, as the proceedings of ICRA are no longer sent to IEEE Xplore", "url": "http://arxiv.org/abs/2508.00584v1", "summary": "In this work, a control scheme for human-robot collaborative object\ntransportation is proposed, considering a quadruped robot equipped with the\nMIGHTY suction cup that serves both as a gripper for holding the object and a\nforce/torque sensor. The proposed control scheme is based on the notion of\nadmittance control, and incorporates a variable damping term aiming towards\nincreasing the controllability of the human and, at the same time, decreasing\nher/his effort. Furthermore, to ensure that the object is not detached from the\nsuction cup during the collaboration, an additional control signal is proposed,\nwhich is based on a barrier artificial potential. The proposed control scheme\nis proven to be passive and its performance is demonstrated through\nexperimental evaluations conducted using the Unitree Go1 robot equipped with\nthe MIGHTY suction cup.", "comment": "Please find the citation info @ Zenodo, ArXiv or Zenodo, as the\n  proceedings of ICRA are no longer sent to IEEE Xplore", "pdf_url": "http://arxiv.org/pdf/2508.00584v1", "cate": "cs.RO", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2507.22746", "title": "Next Tokens Denoising for Speech Synthesis", "authors": ["Yanqing Liu", "Ruiqing Xue", "Chong Zhang", "Yufei Liu", "Gang Wang", "Bohan Li", "Yao Qian", "Lei He", "Shujie Liu", "Sheng Zhao"], "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22746v2", "summary": "While diffusion and autoregressive (AR) models have significantly advanced\ngenerative modeling, they each present distinct limitations. AR models, which\nrely on causal attention, cannot exploit future context and suffer from slow\ngeneration speeds. Conversely, diffusion models struggle with key-value (KV)\ncaching. To overcome these challenges, we introduce Dragon-FM, a novel\ntext-to-speech (TTS) design that unifies AR and flow-matching. This model\nprocesses 48 kHz audio codec tokens in chunks at a compact rate of 12.5 tokens\nper second. This design enables AR modeling across chunks, ensuring global\ncoherence, while parallel flow-matching within chunks facilitates fast\niterative denoising. Thus, the model leverages KV-cache across chunks and\nutilizes bidirectional context within each chunk. Furthermore, it bridges\ncontinuous and discrete feature modeling, demonstrating that continuous AR\nflow-matching can predict discrete tokens with finite scalar quantizers. This\nefficient codec and fast chunk-autoregressive architecture also make the model\nhighly effective for generating long-form content, such as podcasts.\nExperiments on podcast datasets demonstrate its capability to efficiently\ngenerate high-quality zero-shot podcasts.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22746v2", "cate": "cs.SD", "date": "2025-07-30", "updated": "2025-08-01"}
{"id": "2508.00083", "title": "A Survey on Code Generation with LLM-based Agents", "authors": ["Yihong Dong", "Xue Jiang", "Jiaru Qian", "Tian Wang", "Kechi Zhang", "Zhi Jin", "Ge Li"], "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      Work in progress", "url": "http://arxiv.org/abs/2508.00083v1", "summary": "Code generation agents powered by large language models (LLMs) are\nrevolutionizing the software development paradigm. Distinct from previous code\ngeneration techniques, code generation agents are characterized by three core\nfeatures. 1) Autonomy: the ability to independently manage the entire workflow,\nfrom task decomposition to coding and debugging. 2) Expanded task scope:\ncapabilities that extend beyond generating code snippets to encompass the full\nsoftware development lifecycle (SDLC). 3) Enhancement of engineering\npracticality: a shift in research emphasis from algorithmic innovation toward\npractical engineering challenges, such as system reliability, process\nmanagement, and tool integration. This domain has recently witnessed rapid\ndevelopment and an explosion in research, demonstrating significant application\npotential. This paper presents a systematic survey of the field of LLM-based\ncode generation agents. We trace the technology's developmental trajectory from\nits inception and systematically categorize its core techniques, including both\nsingle-agent and multi-agent architectures. Furthermore, this survey details\nthe applications of LLM-based agents across the full SDLC, summarizes\nmainstream evaluation benchmarks and metrics, and catalogs representative\ntools. Finally, by analyzing the primary challenges, we identify and propose\nseveral foundational, long-term research directions for the future work of the\nfield.", "comment": "Work in progress", "pdf_url": "http://arxiv.org/pdf/2508.00083v1", "cate": "cs.SE", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2508.00149", "title": "Data Bias in Human Mobility is a Universal Phenomenon but is Highly Location-specific", "authors": ["Katinka den Nijs", "Elisa Omodei", "Vedran Sekara"], "categories": ["cs.CY", "cs.SI", "physics.soc-ph"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00149v1", "summary": "Large-scale human mobility datasets play increasingly critical roles in many\nalgorithmic systems, business processes and policy decisions. Unfortunately\nthere has been little focus on understanding bias and other fundamental\nshortcomings of the datasets and how they impact downstream analyses and\nprediction tasks. In this work, we study `data production', quantifying not\nonly whether individuals are represented in big digital datasets, but also how\nthey are represented in terms of how much data they produce. We study GPS\nmobility data collected from anonymized smartphones for ten major US cities and\nfind that data points can be more unequally distributed between users than\nwealth. We build models to predict the number of data points we can expect to\nbe produced by the composition of demographic groups living in census tracts,\nand find strong effects of wealth, ethnicity, and education on data production.\nWhile we find that bias is a universal phenomenon, occurring in all cities, we\nfurther find that each city suffers from its own manifestation of it, and that\nlocation-specific models are required to model bias for each city. This work\nraises serious questions about general approaches to debias human mobility data\nand urges further research.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00149v1", "cate": "cs.CY", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2503.04165", "title": "WeakSupCon: Weakly Supervised Contrastive Learning for Encoder Pre-training", "authors": ["Bodong Zhang", "Hamid Manoochehri", "Xiwen Li", "Beatrice S. Knudsen", "Tolga Tasdizen"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Medical Image Computing and Computer Assisted Intervention (MICCAI) 2025 workshop on Efficient Medical AI", "url": "http://arxiv.org/abs/2503.04165v2", "summary": "Weakly supervised multiple instance learning (MIL) is a challenging task\ngiven that only bag-level labels are provided, while each bag typically\ncontains multiple instances. This topic has been extensively studied in\nhistopathological image analysis, where labels are usually available only at\nthe whole slide image (WSI) level, while each WSI could be divided into\nthousands of small image patches for training. The dominant MIL approaches\nfocus on feature aggregation and take fixed patch features as inputs. However,\nweakly supervised feature representation learning in MIL settings is always\nneglected. Those features used to be generated by self-supervised learning\nmethods that do not utilize weak labels, or by foundation encoders pre-trained\non other large datasets. In this paper, we propose a novel weakly supervised\nfeature representation learning method called Weakly Supervised Contrastive\nLearning (WeakSupCon) that utilizes bag-level labels. In our method, we employ\nmulti-task learning and define distinct contrastive losses for samples with\ndifferent bag labels. Our experiments demonstrate that the features generated\nusing WeakSupCon with limited computing resources significantly enhance MIL\nclassification performance compared to self-supervised approaches across three\ndatasets. Our WeakSupCon code is available at\ngithub.com/BzhangURU/Paper_WeakSupCon", "comment": "Medical Image Computing and Computer Assisted Intervention (MICCAI)\n  2025 workshop on Efficient Medical AI", "pdf_url": "http://arxiv.org/pdf/2503.04165v2", "cate": "cs.CV", "date": "2025-03-06", "updated": "2025-07-30"}
{"id": "2508.00625", "title": "OpenScout v1.1 mobile robot: a case study on open hardware continuation", "authors": ["Bartosz Krawczyk", "Ahmed Elbary", "Robbie Cato", "Jagdish Patil", "Kaung Myat", "Anyeh Ndi-Tah", "Nivetha Sakthivel", "Mark Crampton", "Gautham Das", "Charles Fox"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      6 pages, 4 figures, a TAROS2025 short paper", "url": "http://arxiv.org/abs/2508.00625v1", "summary": "OpenScout is an Open Source Hardware (OSH) mobile robot for research and\nindustry. It is extended to v1.1 which includes simplified, cheaper and more\npowerful onboard compute hardware; a simulated ROS2 interface; and a Gazebo\nsimulation. Changes, their rationale, project methodology, and results are\nreported as an OSH case study.", "comment": "6 pages, 4 figures, a TAROS2025 short paper", "pdf_url": "http://arxiv.org/pdf/2508.00625v1", "cate": "cs.RO", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2409.08374", "title": "OpenACE: An Open Benchmark for Evaluating Audio Coding Performance", "authors": ["Jozef Coldenhoff", "Niclas Granqvist", "Milos Cernak"], "categories": ["eess.AS", "cs.SD"], "primary_category": "Subjects:       Audio and Speech Processing (eess.AS)", "pdf_link": null, "comments": "Comments:      ICASSP 2025", "url": "http://arxiv.org/abs/2409.08374v2", "summary": "Audio and speech coding lack unified evaluation and open-source testing. Many\ncandidate systems were evaluated on proprietary, non-reproducible, or small\ndata, and machine learning-based codecs are often tested on datasets with\nsimilar distributions as trained on, which is unfairly compared to digital\nsignal processing-based codecs that usually work well with unseen data. This\npaper presents a full-band audio and speech coding quality benchmark with more\nvariable content types, including traditional open test vectors. An example use\ncase of audio coding quality assessment is presented with open-source Opus,\n3GPP's EVS, and recent ETSI's LC3 with LC3+ used in Bluetooth LE Audio\nprofiles. Besides, quality variations of emotional speech encoding at 16 kbps\nare shown. The proposed open-source benchmark contributes to audio and speech\ncoding democratization and is available at\nhttps://github.com/JozefColdenhoff/OpenACE.", "comment": "ICASSP 2025", "pdf_url": "http://arxiv.org/pdf/2409.08374v2", "cate": "eess.AS", "date": "2024-09-12", "updated": "2025-08-01"}
{"id": "2508.00128", "title": "How Quantization Impacts Privacy Risk on LLMs for Code?", "authors": ["Md Nazmul Haque", "Hua Yang", "Zhou Yang", "Bowen Xu"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00128v1", "summary": "Large language models for code (LLMs4Code) rely heavily on massive training\ndata, including sensitive data, such as cloud service credentials of the\nprojects and personal identifiable information of the developers, raising\nserious privacy concerns. Membership inference (MI) has recently emerged as an\neffective tool for assessing privacy risk by identifying whether specific data\nbelong to a model's training set. In parallel, model compression techniques,\nespecially quantization, have gained traction for reducing computational costs\nand enabling the deployment of large models. However, while quantized models\nstill retain knowledge learned from the original training data, it remains\nunclear whether quantization affects their ability to retain and expose privacy\ninformation. Answering this question is of great importance to understanding\nprivacy risks in real-world deployments. In this work, we conduct the first\nempirical study on how quantization influences task performance and privacy\nrisk simultaneously in LLMs4Code. To do this, we implement widely used\nquantization techniques (static and dynamic) to three representative model\nfamilies, namely Pythia, CodeGen, and GPTNeo. Our results demonstrate that\nquantization has a significant impact on reducing the privacy risk relative to\nthe original model. We also uncover a positive correlation between task\nperformance and privacy risk, indicating an underlying tradeoff. Moreover, we\nreveal the possibility that quantizing larger models could yield better balance\nthan using full-precision small models. Finally, we demonstrate that these\nfindings generalize across different architectures, model sizes and MI methods,\noffering practical guidance for safeguarding privacy when deploying compressed\nLLMs4Code.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00128v1", "cate": "cs.SE", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2504.01718", "title": "A Novel Dynamic Epidemic Model for Successive Opinion Diffusion in Social Networks", "authors": ["Bin Han", "Fabienne Renckens", "C. Clark Cao", "Hans D. Schotten"], "categories": ["cs.SI"], "primary_category": "Subjects:       Social and Information Networks (cs.SI)", "pdf_link": null, "comments": "Comments:      To appear in IEEE GLOBECOM 2025", "url": "http://arxiv.org/abs/2504.01718v2", "summary": "This paper proposes a dynamic epidemic model for successive opinion diffusion\nin social networks, extending the SHIMR model. It incorporates dynamic\ndecision-making influenced by social distances and captures accumulative\nopinion diffusion caused by interrelated rumors. The model reflects the impact\nof rumor spread on social network structures. Simulations validate its\neffectiveness in explaining phenomena like the echo chamber effect and provide\ninsights into opinion diffusion dynamics, with implications for understanding\nsocial polarization and network evolution.", "comment": "To appear in IEEE GLOBECOM 2025", "pdf_url": "http://arxiv.org/pdf/2504.01718v2", "cate": "cs.SI", "date": "2025-04-02", "updated": "2025-07-31"}
{"id": "2503.04351", "title": "PLMP -- Point-Line Minimal Problems for Projective SfM", "authors": ["Kim Kiehn", "Albin Ahlbäck", "Kathlén Kohn"], "categories": ["cs.CV", "math.AG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.04351v2", "summary": "We completely classify all minimal problems for Structure-from-Motion (SfM)\nwhere arrangements of points and lines are fully observed by multiple\nuncalibrated pinhole cameras. We find 291 minimal problems, 73 of which have\nunique solutions and can thus be solved linearly. Two of the linear problems\nallow an arbitrary number of views, while all other minimal problems have at\nmost 9 cameras. All minimal problems have at most 7 points and at most 12\nlines. We compute the number of solutions of each minimal problem, as this\ngives a measurement of the problem's intrinsic difficulty, and find that these\nnumber are relatively low (e.g., when comparing with minimal problems for\ncalibrated cameras). Finally, by exploring stabilizer subgroups of\nsubarrangements, we develop a geometric and systematic way to 1) factorize\nminimal problems into smaller problems, 2) identify minimal problems in\nunderconstrained problems, and 3) formally prove non-minimality.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.04351v2", "cate": "cs.CV", "date": "2025-03-06", "updated": "2025-07-31"}
{"id": "2508.00691", "title": "Towards Data-Driven Adaptive Exoskeleton Assistance for Post-stroke Gait", "authors": ["Fabian C. Weigend", "Dabin K. Choe", "Santiago Canete", "Conor J. Walsh"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      8 pages, 6 figures, 2 tables", "url": "http://arxiv.org/abs/2508.00691v1", "summary": "Recent work has shown that exoskeletons controlled through data-driven\nmethods can dynamically adapt assistance to various tasks for healthy young\nadults. However, applying these methods to populations with neuromotor gait\ndeficits, such as post-stroke hemiparesis, is challenging. This is due not only\nto high population heterogeneity and gait variability but also to a lack of\npost-stroke gait datasets to train accurate models. Despite these challenges,\ndata-driven methods offer a promising avenue for control, potentially allowing\nexoskeletons to function safely and effectively in unstructured community\nsettings. This work presents a first step towards enabling adaptive\nplantarflexion and dorsiflexion assistance from data-driven torque estimation\nduring post-stroke walking. We trained a multi-task Temporal Convolutional\nNetwork (TCN) using collected data from four post-stroke participants walking\non a treadmill ($R^2$ of $0.74 \\pm 0.13$). The model uses data from three\ninertial measurement units (IMU) and was pretrained on healthy walking data\nfrom 6 participants. We implemented a wearable prototype for our ankle torque\nestimation approach for exoskeleton control and demonstrated the viability of\nreal-time sensing, estimation, and actuation with one post-stroke participant.", "comment": "8 pages, 6 figures, 2 tables", "pdf_url": "http://arxiv.org/pdf/2508.00691v1", "cate": "cs.RO", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2502.02929", "title": "AudioMiXR: Spatial Audio Object Manipulation with 6DoF for Sound Design in Augmented Reality", "authors": ["Brandon Woodard", "Margarita Geleta", "Joseph J. LaViola Jr.", "Andrea Fanelli", "Rhonda Wilson"], "categories": ["cs.HC", "cs.SD", "eess.AS", "H.5.2; H.5.5; H.5.1"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      Revision necessary for accuracy", "url": "http://arxiv.org/abs/2502.02929v3", "summary": "We present AudioMiXR, an augmented reality (AR) interface intended to assess\nhow users manipulate virtual audio objects situated in their physical space\nusing six degrees of freedom (6DoF) deployed on a head-mounted display (Apple\nVision Pro) for 3D sound design. Existing tools for 3D sound design are\ntypically constrained to desktop displays, which may limit spatial awareness of\nmixing within the execution environment. Utilizing an XR HMD to create\nsoundscapes may provide a real-time test environment for 3D sound design, as\nmodern HMDs can provide precise spatial localization assisted by cross-modal\ninteractions. However, there is no research on design guidelines specific to\nsound design with six degrees of freedom (6DoF) in XR. To provide a first step\ntoward identifying design-related research directions in this space, we\nconducted an exploratory study where we recruited 27 participants, consisting\nof expert and non-expert sound designers. The goal was to assess design lessons\nthat can be used to inform future research venues in 3D sound design. We ran a\nwithin-subjects study where users designed both a music and cinematic\nsoundscapes. After thematically analyzing participant data, we constructed two\ndesign lessons: 1. Proprioception for AR Sound Design, and 2. Balancing\nAudio-Visual Modalities in AR GUIs. Additionally, we provide application\ndomains that can benefit most from 6DoF sound design based on our results.", "comment": "Revision necessary for accuracy", "pdf_url": "http://arxiv.org/pdf/2502.02929v3", "cate": "cs.HC", "date": "2025-02-05", "updated": "2025-08-01"}
{"id": "2508.00198", "title": "Testing the Untestable? An Empirical Study on the Testing Process of LLM-Powered Software Systems", "authors": ["Cleyton Magalhaes", "Italo Santos", "Brody Stuart-Verner", "Ronnie de Souza Santos"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00198v1", "summary": "Background: Software systems powered by large language models are becoming a\nroutine part of everyday technologies, supporting applications across a wide\nrange of domains. In software engineering, many studies have focused on how\nLLMs support tasks such as code generation, debugging, and documentation.\nHowever, there has been limited focus on how full systems that integrate LLMs\nare tested during development. Aims: This study explores how LLM-powered\nsystems are tested in the context of real-world application development.\nMethod: We conducted an exploratory case study using 99 individual reports\nwritten by students who built and deployed LLM-powered applications as part of\na university course. Each report was independently analyzed using thematic\nanalysis, supported by a structured coding process. Results: Testing strategies\ncombined manual and automated methods to evaluate both system logic and model\nbehavior. Common practices included exploratory testing, unit testing, and\nprompt iteration. Reported challenges included integration failures,\nunpredictable outputs, prompt sensitivity, hallucinations, and uncertainty\nabout correctness. Conclusions: Testing LLM-powered systems required\nadaptations to traditional verification methods, blending source-level\nreasoning with behavior-aware evaluations. These findings provide evidence on\nthe practical context of testing generative components in software systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00198v1", "cate": "cs.SE", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2502.20083", "title": "Block-corrected Modularity for Community Detection", "authors": ["Hasti Narimanzadeh", "Takayuki Hiraoka", "Mikko Kivelä"], "categories": ["physics.soc-ph", "cond-mat.stat-mech", "cs.SI"], "primary_category": "Subjects:       Physics and Society (physics.soc-ph)", "pdf_link": null, "comments": "Comments:      22 pages, 11 figures", "url": "http://arxiv.org/abs/2502.20083v3", "summary": "Unknown node attributes in complex networks may introduce community\nstructures that are important to distinguish from those driven by known\nattributes. We propose a block-corrected modularity that discounts given block\nstructures present in the network to reveal communities masked by them. We show\nanalytically how the proposed modularity finds the community structure driven\nby an unknown attribute in a simple network model. Further, we observe that the\nblock-corrected modularity finds the underlying community structure on a number\nof simple synthetic network models while methods using different null models\nfail. We develop an efficient spectral method as well as two Louvain-inspired\nfine-tuning algorithms to maximize the proposed modularity and demonstrate\ntheir performance on several synthetic network models. Finally, we assess our\nmethodology on various real-world citation networks built using the OpenAlex\ndata by correcting for the temporal citation patterns.", "comment": "22 pages, 11 figures", "pdf_url": "http://arxiv.org/pdf/2502.20083v3", "cate": "physics.soc-ph", "date": "2025-02-27", "updated": "2025-08-01"}
{"id": "2503.07047", "title": "Recovering Partially Corrupted Objects via Sketch-Guided Bidirectional Feature Interaction", "authors": ["Yongle Zhang", "Yimin Liu", "Yan Huang", "Qiang Wu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      13 pages. This work has been submitted to the IEEE for possible publication", "url": "http://arxiv.org/abs/2503.07047v2", "summary": "Text-guided diffusion models have achieved remarkable success in object\ninpainting by providing high-level semantic guidance through text prompts.\nHowever, they often lack precise pixel-level spatial control, especially in\nscenarios involving partially corrupted objects where critical uncorrupted cues\nremain. To overcome this limitation, sketch-guided methods have been\nintroduced, using either indirect gradient modulation or direct sketch\ninjection to improve structural control. Yet, existing approaches typically\nestablish a one-way mapping from the sketch to the masked regions only,\nneglecting the contextual information from unmasked object areas. This leads to\na disconnection between the sketch and the uncorrupted content, thereby causing\nsketch-guided inconsistency and structural mismatch. To tackle this challenge,\nwe propose a sketch-guided bidirectional feature interaction framework built\nupon a pretrained Stable Diffusion model. Our bidirectional interaction\nfeatures two complementary directions, context-to-sketch and\nsketch-to-inpainting, that enable fine-grained spatial control for partially\ncorrupted object inpainting. In the context-to-sketch direction, multi-scale\nlatents from uncorrupted object regions are propagated to the sketch branch to\ngenerate a visual mask that adapts the sketch features to the visible context\nand denoising progress. In the sketch-to-inpainting direction, a\nsketch-conditional affine transformation modulates the influence of sketch\nguidance based on the learned visual mask, ensuring consistency with\nuncorrupted object content. This interaction is applied at multiple scales\nwithin the encoder of the diffusion U-Net, enabling the model to restore object\nstructures with enhanced spatial fidelity. Extensive experiments on two newly\nconstructed benchmark datasets demonstrate that our approach outperforms\nstate-of-the-art methods.", "comment": "13 pages. This work has been submitted to the IEEE for possible\n  publication", "pdf_url": "http://arxiv.org/pdf/2503.07047v2", "cate": "cs.CV", "date": "2025-03-10", "updated": "2025-07-31"}
{"id": "2508.00697", "title": "On-Device Diffusion Transformer Policy for Efficient Robot Manipulation", "authors": ["Yiming Wu", "Huan Wang", "Zhenghao Chen", "Jianxin Pang", "Dong Xu"], "categories": ["cs.RO", "cs.AI", "cs.CV"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      ICCV 2025", "url": "http://arxiv.org/abs/2508.00697v1", "summary": "Diffusion Policies have significantly advanced robotic manipulation tasks via\nimitation learning, but their application on resource-constrained mobile\nplatforms remains challenging due to computational inefficiency and extensive\nmemory footprint. In this paper, we propose LightDP, a novel framework\nspecifically designed to accelerate Diffusion Policies for real-time deployment\non mobile devices. LightDP addresses the computational bottleneck through two\ncore strategies: network compression of the denoising modules and reduction of\nthe required sampling steps. We first conduct an extensive computational\nanalysis on existing Diffusion Policy architectures, identifying the denoising\nnetwork as the primary contributor to latency. To overcome performance\ndegradation typically associated with conventional pruning methods, we\nintroduce a unified pruning and retraining pipeline, optimizing the model's\npost-pruning recoverability explicitly. Furthermore, we combine pruning\ntechniques with consistency distillation to effectively reduce sampling steps\nwhile maintaining action prediction accuracy. Experimental evaluations on the\nstandard datasets, \\ie, PushT, Robomimic, CALVIN, and LIBERO, demonstrate that\nLightDP achieves real-time action prediction on mobile devices with competitive\nperformance, marking an important step toward practical deployment of\ndiffusion-based policies in resource-limited environments. Extensive real-world\nexperiments also show the proposed LightDP can achieve performance comparable\nto state-of-the-art Diffusion Policies.", "comment": "ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2508.00697v1", "cate": "cs.RO", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00253", "title": "Leveraging Large Language Model for Information Retrieval-based Bug Localization", "authors": ["Moumita Asad", "Rafed Muhammad Yasir", "Armin Geramirad", "Sam Malek"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00253v1", "summary": "Information Retrieval-based Bug Localization aims to identify buggy source\nfiles for a given bug report. While existing approaches -- ranging from vector\nspace models to deep learning models -- have shown potential in this domain,\ntheir effectiveness is often limited by the vocabulary mismatch between bug\nreports and source code. To address this issue, we propose a novel Large\nLanguage Model (LLM) based bug localization approach, called GenLoc. Given a\nbug report, GenLoc leverages an LLM equipped with code-exploration functions to\niteratively analyze the code base and identify potential buggy files. To gather\nbetter context, GenLoc may optionally retrieve semantically relevant files\nusing vector embeddings. GenLoc has been evaluated on over 9,000 real-world bug\nreports from six large-scale Java projects. Experimental results show that\nGenLoc outperforms five state-of-the-art bug localization techniques across\nmultiple metrics, achieving an average improvement of more than 60\\% in\nAccuracy@1.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00253v1", "cate": "cs.SE", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2504.16604", "title": "Debunking with Dialogue? Exploring AI-Generated Counterspeech to Challenge Conspiracy Theories", "authors": ["Mareike Lisker", "Christina Gottschalk", "Helena Mihaljević"], "categories": ["cs.CL", "cs.AI", "cs.SI", "I.2.7"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      16 pages, Association for Computational Linguistics, Proceedings of the 9th Workshop on Online Abuse and Harms (WOAH 2025)", "url": "http://arxiv.org/abs/2504.16604v2", "summary": "Counterspeech is a key strategy against harmful online content, but scaling\nexpert-driven efforts is challenging. Large Language Models (LLMs) present a\npotential solution, though their use in countering conspiracy theories is\nunder-researched. Unlike for hate speech, no datasets exist that pair\nconspiracy theory comments with expert-crafted counterspeech. We address this\ngap by evaluating the ability of GPT-4o, Llama 3, and Mistral to effectively\napply counterspeech strategies derived from psychological research provided\nthrough structured prompts. Our results show that the models often generate\ngeneric, repetitive, or superficial results. Additionally, they\nover-acknowledge fear and frequently hallucinate facts, sources, or figures,\nmaking their prompt-based use in practical applications problematic.", "comment": "16 pages, Association for Computational Linguistics, Proceedings of\n  the 9th Workshop on Online Abuse and Harms (WOAH 2025)", "pdf_url": "http://arxiv.org/pdf/2504.16604v2", "cate": "cs.CL", "date": "2025-04-23", "updated": "2025-08-01"}
{"id": "2503.10410", "title": "RoCo-Sim: Enhancing Roadside Collaborative Perception through Foreground Simulation", "authors": ["Yuwen Du", "Anning Hu", "Zichen Chao", "Yifan Lu", "Junhao Ge", "Genjia Liu", "Weitao Wu", "Lanjun Wang", "Siheng Chen"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.10410v3", "summary": "Roadside Collaborative Perception refers to a system where multiple roadside\nunits collaborate to pool their perceptual data, assisting vehicles in\nenhancing their environmental awareness. Existing roadside perception methods\nconcentrate on model design but overlook data issues like calibration errors,\nsparse information, and multi-view consistency, leading to poor performance on\nrecent published datasets. To significantly enhance roadside collaborative\nperception and address critical data issues, we present the first simulation\nframework RoCo-Sim for road-side collaborative perception. RoCo-Sim is capable\nof generating diverse, multi-view consistent simulated roadside data through\ndynamic foreground editing and full-scene style transfer of a single image.\nRoCo-Sim consists of four components: (1) Camera Extrinsic Optimization ensures\naccurate 3D to 2D projection for roadside cameras; (2) A novel Multi-View\nOcclusion-Aware Sampler (MOAS) determines the placement of diverse digital\nassets within 3D space; (3) DepthSAM innovatively models foreground-background\nrelationships from single-frame fixed-view images, ensuring multi-view\nconsistency of foreground; and (4) Scalable Post-Processing Toolkit generates\nmore realistic and enriched scenes through style transfer and other\nenhancements. RoCo-Sim significantly improves roadside 3D object detection,\noutperforming SOTA methods by 83.74 on Rcooper-Intersection and 83.12 on\nTUMTraf-V2X for AP70. RoCo-Sim fills a critical gap in roadside perception\nsimulation. Code and pre-trained models will be released soon:\nhttps://github.com/duyuwen-duen/RoCo-Sim", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.10410v3", "cate": "cs.CV", "date": "2025-03-13", "updated": "2025-07-31"}
{"id": "2508.00795", "title": "Video Generators are Robot Policies", "authors": ["Junbang Liang", "Pavel Tokmakov", "Ruoshi Liu", "Sruthi Sudhakar", "Paarth Shah", "Rares Ambrus", "Carl Vondrick"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00795v1", "summary": "Despite tremendous progress in dexterous manipulation, current visuomotor\npolicies remain fundamentally limited by two challenges: they struggle to\ngeneralize under perceptual or behavioral distribution shifts, and their\nperformance is constrained by the size of human demonstration data. In this\npaper, we use video generation as a proxy for robot policy learning to address\nboth limitations simultaneously. We propose Video Policy, a modular framework\nthat combines video and action generation that can be trained end-to-end. Our\nresults demonstrate that learning to generate videos of robot behavior allows\nfor the extraction of policies with minimal demonstration data, significantly\nimproving robustness and sample efficiency. Our method shows strong\ngeneralization to unseen objects, backgrounds, and tasks, both in simulation\nand the real world. We further highlight that task success is closely tied to\nthe generated video, with action-free video data providing critical benefits\nfor generalizing to novel tasks. By leveraging large-scale video generative\nmodels, we achieve superior performance compared to traditional behavior\ncloning, paving the way for more scalable and data-efficient robot policy\nlearning.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00795v1", "cate": "cs.RO", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00255", "title": "Accurate and Consistent Graph Model Generation from Text with Large Language Models", "authors": ["Boqi Chen", "Ou Wei", "Bingzhou Zheng", "Gunter Mussbacher"], "categories": ["cs.SE", "cs.AI"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      Accepted at ACM / IEEE 28th International Conference on Model Driven Engineering Languages and Systems (MODELS 2025)", "url": "http://arxiv.org/abs/2508.00255v1", "summary": "Graph model generation from natural language description is an important task\nwith many applications in software engineering. With the rise of large language\nmodels (LLMs), there is a growing interest in using LLMs for graph model\ngeneration. Nevertheless, LLM-based graph model generation typically produces\npartially correct models that suffer from three main issues: (1) syntax\nviolations: the generated model may not adhere to the syntax defined by its\nmetamodel, (2) constraint inconsistencies: the structure of the model might not\nconform to some domain-specific constraints, and (3) inaccuracy: due to the\ninherent uncertainty in LLMs, the models can include inaccurate, hallucinated\nelements. While the first issue is often addressed through techniques such as\nconstraint decoding or filtering, the latter two remain largely unaddressed.\nMotivated by recent self-consistency approaches in LLMs, we propose a novel\nabstraction-concretization framework that enhances the consistency and quality\nof generated graph models by considering multiple outputs from an LLM. Our\napproach first constructs a probabilistic partial model that aggregates all\ncandidate outputs and then refines this partial model into the most appropriate\nconcrete model that satisfies all constraints. We evaluate our framework on\nseveral popular open-source and closed-source LLMs using diverse datasets for\nmodel generation tasks. The results demonstrate that our approach significantly\nimproves both the consistency and quality of the generated graph models.", "comment": "Accepted at ACM / IEEE 28th International Conference on Model Driven\n  Engineering Languages and Systems (MODELS 2025)", "pdf_url": "http://arxiv.org/pdf/2508.00255v1", "cate": "cs.SE", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2503.11806", "title": "Human-in-the-Loop Local Corrections of 3D Scene Layouts via Infilling", "authors": ["Christopher Xie", "Armen Avetisyan", "Henry Howard-Jenkins", "Yawar Siddiqui", "Julian Straub", "Richard Newcombe", "Vasileios Balntas", "Jakob Engel"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Project page: this https URL", "url": "http://arxiv.org/abs/2503.11806v2", "summary": "We present a novel human-in-the-loop approach to estimate 3D scene layout\nthat uses human feedback from an egocentric standpoint. We study this approach\nthrough introduction of a novel local correction task, where users identify\nlocal errors and prompt a model to automatically correct them. Building on\nSceneScript, a state-of-the-art framework for 3D scene layout estimation that\nleverages structured language, we propose a solution that structures this\nproblem as \"infilling\", a task studied in natural language processing. We train\na multi-task version of SceneScript that maintains performance on global\npredictions while significantly improving its local correction ability. We\nintegrate this into a human-in-the-loop system, enabling a user to iteratively\nrefine scene layout estimates via a low-friction \"one-click fix'' workflow. Our\nsystem enables the final refined layout to diverge from the training\ndistribution, allowing for more accurate modelling of complex layouts.", "comment": "Project page: https://www.projectaria.com/scenescript/", "pdf_url": "http://arxiv.org/pdf/2503.11806v2", "cate": "cs.CV", "date": "2025-03-14", "updated": "2025-07-30"}
{"id": "2508.00088", "title": "The Monado SLAM Dataset for Egocentric Visual-Inertial Tracking", "authors": ["Mateo de Mayo", "Daniel Cremers", "Taihú Pire"], "categories": ["cs.CV", "cs.RO"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted to IROS 2025", "url": "http://arxiv.org/abs/2508.00088v1", "summary": "Humanoid robots and mixed reality headsets benefit from the use of\nhead-mounted sensors for tracking. While advancements in visual-inertial\nodometry (VIO) and simultaneous localization and mapping (SLAM) have produced\nnew and high-quality state-of-the-art tracking systems, we show that these are\nstill unable to gracefully handle many of the challenging settings presented in\nthe head-mounted use cases. Common scenarios like high-intensity motions,\ndynamic occlusions, long tracking sessions, low-textured areas, adverse\nlighting conditions, saturation of sensors, to name a few, continue to be\ncovered poorly by existing datasets in the literature. In this way, systems may\ninadvertently overlook these essential real-world issues. To address this, we\npresent the Monado SLAM dataset, a set of real sequences taken from multiple\nvirtual reality headsets. We release the dataset under a permissive CC BY 4.0\nlicense, to drive advancements in VIO/SLAM research and development.", "comment": "Accepted to IROS 2025", "pdf_url": "http://arxiv.org/pdf/2508.00088v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2508.00408", "title": "Benchmarking LLMs for Unit Test Generation from Real-World Functions", "authors": ["Dong Huang", "Jie M. Zhang", "Mark Harman", "Qianru Zhang", "Mingzhe Du", "See-Kiong Ng"], "categories": ["cs.SE", "cs.CL"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      Under Review", "url": "http://arxiv.org/abs/2508.00408v1", "summary": "Recently, large language models (LLMs) have shown great promise in automating\nunit test generation, significantly reducing the manual effort required by\ndevelopers. To effectively evaluate the capabilities of LLMs in this domain, it\nis crucial to have a well-designed benchmark that accurately reflects\nreal-world scenarios and mitigates common pitfalls. Existing LLM test\ngeneration benchmarks are limited by two critical drawbacks: data contamination\nand structurally simple function code. As a result, we often cannot rely on the\nvalidity of scientific conclusions drawn from empirical studies using these\nlimited benchmarks. The empirical evidence presented may be biased due to\ncontamination and may fail to generalize beyond toy programs due to structural\nsimplicity.\n  To address these problems, we introduce ULT (UnLeakedTestbench), a new\nbenchmark specifically designed for function-level unit test generation from\nreal-world Python functions. ULT is constructed through a multi-stage curation\nprocess that ensures high cyclomatic complexity and mitigates test case\ncontamination. With 3,909 carefully selected function-level tasks, ULT provides\na more realistic and challenging evaluation of LLMs' test generation\ncapabilities. We also provide PLT (PreLeakedTestbench), a pair benchmark of ULT\nwith leaked tests designed to enable a controlled analysis of memorization\nversus reasoning in test generation. Our evaluation results demonstrate that\nULT is significantly more challenging. For example, test cases generated by\nLLMs only achieve 41.32\\%, 45.10\\%, 30.22\\%, and 40.21\\% for accuracy,\nstatement coverage, branch coverage, and mutation score on average for all\nLLMs, respectively. These results are substantially lower than the\ncorresponding metrics on TestEval (91.79\\%, 92.18\\%, 82.04\\%, and 49.69\\%) and\nPLT (47.07\\%, 55.13\\%, 40.07\\%, and 50.80\\%).", "comment": "Under Review", "pdf_url": "http://arxiv.org/pdf/2508.00408v1", "cate": "cs.SE", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00154", "title": "Data-Driven Motion Planning for Uncertain Nonlinear Systems", "authors": ["Babak Esmaeili", "Hamidreza Modares", "Stefano Di Cairano"], "categories": ["eess.SY", "cs.LG", "cs.RO", "cs.SY", "math.OC"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00154v1", "summary": "This paper proposes a data-driven motion-planning framework for nonlinear\nsystems that constructs a sequence of overlapping invariant polytopes. Around\neach randomly sampled waypoint, the algorithm identifies a convex admissible\nregion and solves data-driven linear-matrix-inequality problems to learn\nseveral ellipsoidal invariant sets together with their local state-feedback\ngains. The convex hull of these ellipsoids, still invariant under a\npiece-wise-affine controller obtained by interpolating the gains, is then\napproximated by a polytope. Safe transitions between nodes are ensured by\nverifying the intersection of consecutive convex-hull polytopes and introducing\nan intermediate node for a smooth transition. Control gains are interpolated in\nreal time via simplex-based interpolation, keeping the state inside the\ninvariant polytopes throughout the motion. Unlike traditional approaches that\nrely on system dynamics models, our method requires only data to compute safe\nregions and design state-feedback controllers. The approach is validated\nthrough simulations, demonstrating the effectiveness of the proposed method in\nachieving safe, dynamically feasible paths for complex nonlinear systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00154v1", "cate": "eess.SY", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2503.14939", "title": "VisNumBench: Evaluating Number Sense of Multimodal Large Language Models", "authors": ["Tengjin Weng", "Jingyi Wang", "Wenhao Jiang", "Zhong Ming"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      accepted by ICCV 2025", "url": "http://arxiv.org/abs/2503.14939v2", "summary": "Can Multimodal Large Language Models (MLLMs) develop an intuitive number\nsense similar to humans? Targeting this problem, we introduce Visual Number\nBenchmark (VisNumBench) to evaluate the number sense abilities of MLLMs across\na wide range of visual numerical tasks. VisNumBench consists of about 1,900\nmultiple-choice question-answer pairs derived from both synthetic and\nreal-world visual data, covering seven visual numerical attributes and four\ntypes of visual numerical estimation tasks. Our experiments on VisNumBench led\nto the following key findings: (i) The 17 MLLMs we tested, including\nopen-source models such as Qwen2.5-VL and InternVL2.5, as well as proprietary\nmodels like GPT-4o and Gemini 2.0 Flash, perform significantly below human\nlevels in number sense-related tasks. (ii) Multimodal mathematical models and\nmultimodal chain-of-thought (CoT) models did not exhibit significant\nimprovements in number sense abilities. (iii) Stronger MLLMs with larger\nparameter sizes and broader general abilities demonstrate modest gains in\nnumber sense abilities. We believe VisNumBench will serve as a valuable\nresource for the research community, encouraging further advancements in\nenhancing MLLMs' number sense abilities. Code and dataset are available at\nhttps://wwwtttjjj.github.io/VisNumBench/.", "comment": "accepted by ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2503.14939v2", "cate": "cs.CV", "date": "2025-03-19", "updated": "2025-07-31"}
{"id": "2508.00299", "title": "Controllable Pedestrian Video Editing for Multi-View Driving Scenarios via Motion Sequence", "authors": ["Danzhen Fu", "Jiagao Hu", "Daiguo Zhou", "Fei Wang", "Zepeng Wang", "Wenhua Liao"], "categories": ["cs.CV", "cs.AI", "cs.RO"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV 2025 Workshop (HiGen)", "url": "http://arxiv.org/abs/2508.00299v1", "summary": "Pedestrian detection models in autonomous driving systems often lack\nrobustness due to insufficient representation of dangerous pedestrian scenarios\nin training datasets. To address this limitation, we present a novel framework\nfor controllable pedestrian video editing in multi-view driving scenarios by\nintegrating video inpainting and human motion control techniques. Our approach\nbegins by identifying pedestrian regions of interest across multiple camera\nviews, expanding detection bounding boxes with a fixed ratio, and resizing and\nstitching these regions into a unified canvas while preserving cross-view\nspatial relationships. A binary mask is then applied to designate the editable\narea, within which pedestrian editing is guided by pose sequence control\nconditions. This enables flexible editing functionalities, including pedestrian\ninsertion, replacement, and removal. Extensive experiments demonstrate that our\nframework achieves high-quality pedestrian editing with strong visual realism,\nspatiotemporal coherence, and cross-view consistency. These results establish\nthe proposed method as a robust and versatile solution for multi-view\npedestrian video generation, with broad potential for applications in data\naugmentation and scenario simulation in autonomous driving.", "comment": "ICCV 2025 Workshop (HiGen)", "pdf_url": "http://arxiv.org/pdf/2508.00299v1", "cate": "cs.CV", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00462", "title": "Managing Power Gaps as a Topic of Pair Programming Skill: A Grounded Theory", "authors": ["Linus Ververs", "Lutz Prechelt"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00462v1", "summary": "Context: Pair Programming as a work mode is used (occasionally or frequently)\nthroughout professional software development. Objective: Understand what\npower-related phenomena occur in pair programming as it is used in industry;\ngive advice to practitioners on how to do better pair programming. Method:\nAnalyze 22 industrial pair programming sessions using Grounded Theory\nMethodology. Formulate a Grounded Theory on power-related behaviors. Run a\nsurvey with 292 participants about that theory. Use it to demonstrate that the\nphenomena are common. Results: Our theory describes the phenomenon of Power\nGap: a perceived difference in participation opportunities. The theory shows\nthe behaviors that create a Power Gap or result from it. Power Gaps tend to\ndamage knowledge transfer, code quality, and process effi ciency. The survey\nresults show that all concepts from our theory are frequent in practice. They\nalso provide more grounding for concepts that are observable only indirectly.\nConclusions: It is a valuable component of pair programming skill to be able to\navoid Power Gaps. Specifically, pair partners need to avoid Hierarchical\nBehavior (which tends to create or increase a Power Gap) and should perform\nenough Equalizing Behavior (which prevents or reduces a Power Gap).", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00462v1", "cate": "cs.SE", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00156", "title": "Integrating Opinion Dynamics into Safety Control for Decentralized Airplane Encounter Resolution", "authors": ["Shuhao Qi", "Zhiqi Tang", "Zhiyong Sun", "Sofie Haesaert"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00156v1", "summary": "As the airspace becomes increasingly congested, decentralized conflict\nresolution methods for airplane encounters have become essential. While\ndecentralized safety controllers can prevent dangerous midair collisions, they\ndo not always ensure prompt conflict resolution. As a result, airplane progress\nmay be blocked for extended periods in certain situations. To address this\nblocking phenomenon, this paper proposes integrating bio-inspired nonlinear\nopinion dynamics into the airplane safety control framework, thereby\nguaranteeing both safety and blocking-free resolution. In particular, opinion\ndynamics enable the safety controller to achieve collaborative decision-making\nfor blocking resolution and facilitate rapid, safe coordination without relying\non communication or preset rules. Extensive simulation results validate the\nimproved flight efficiency and safety guarantees. This study provides practical\ninsights into the design of autonomous controllers for airplanes.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00156v1", "cate": "eess.SY", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2503.15897", "title": "Learning 3D Scene Analogies with Neural Contextual Scene Maps", "authors": ["Junho Kim", "Gwangtak Bae", "Eun Sun Lee", "Young Min Kim"], "categories": ["cs.CV", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted to ICCV 2025", "url": "http://arxiv.org/abs/2503.15897v2", "summary": "Understanding scene contexts is crucial for machines to perform tasks and\nadapt prior knowledge in unseen or noisy 3D environments. As data-driven\nlearning is intractable to comprehensively encapsulate diverse ranges of\nlayouts and open spaces, we propose teaching machines to identify relational\ncommonalities in 3D spaces. Instead of focusing on point-wise or object-wise\nrepresentations, we introduce 3D scene analogies, which are smooth maps between\n3D scene regions that align spatial relationships. Unlike well-studied single\ninstance-level maps, these scene-level maps smoothly link large scene regions,\npotentially enabling unique applications in trajectory transfer in AR/VR, long\ndemonstration transfer for imitation learning, and context-aware object\nrearrangement. To find 3D scene analogies, we propose neural contextual scene\nmaps, which extract descriptor fields summarizing semantic and geometric\ncontexts, and holistically align them in a coarse-to-fine manner for map\nestimation. This approach reduces reliance on individual feature points, making\nit robust to input noise or shape variations. Experiments demonstrate the\neffectiveness of our approach in identifying scene analogies and transferring\ntrajectories or object placements in diverse indoor scenes, indicating its\npotential for robotics and AR/VR applications. Project page including the code\nis available through this link:\nhttps://82magnolia.github.io/3d_scene_analogies/.", "comment": "Accepted to ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2503.15897v2", "cate": "cs.CV", "date": "2025-03-20", "updated": "2025-07-31"}
{"id": "2508.00440", "title": "Reducing the gap between general purpose data and aerial images in concentrated solar power plants", "authors": ["M. A. Pérez-Cutiño", "J. Valverde", "J. Capitán", "J. M. Díaz-Báñez"], "categories": ["cs.CV", "cs.AI", "cs.RO"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00440v1", "summary": "In the context of Concentrated Solar Power (CSP) plants, aerial images\ncaptured by drones present a unique set of challenges. Unlike urban or natural\nlandscapes commonly found in existing datasets, solar fields contain highly\nreflective surfaces, and domain-specific elements that are uncommon in\ntraditional computer vision benchmarks. As a result, machine learning models\ntrained on generic datasets struggle to generalize to this setting without\nextensive retraining and large volumes of annotated data. However, collecting\nand labeling such data is costly and time-consuming, making it impractical for\nrapid deployment in industrial applications.\n  To address this issue, we propose a novel approach: the creation of\nAerialCSP, a virtual dataset that simulates aerial imagery of CSP plants. By\ngenerating synthetic data that closely mimic real-world conditions, our\nobjective is to facilitate pretraining of models before deployment,\nsignificantly reducing the need for extensive manual labeling. Our main\ncontributions are threefold: (1) we introduce AerialCSP, a high-quality\nsynthetic dataset for aerial inspection of CSP plants, providing annotated data\nfor object detection and image segmentation; (2) we benchmark multiple models\non AerialCSP, establishing a baseline for CSP-related vision tasks; and (3) we\ndemonstrate that pretraining on AerialCSP significantly improves real-world\nfault detection, particularly for rare and small defects, reducing the need for\nextensive manual labeling. AerialCSP is made publicly available at\nhttps://mpcutino.github.io/aerialcsp/.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00440v1", "cate": "cs.CV", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00546", "title": "SPENCER: Self-Adaptive Model Distillation for Efficient Code Retrieval", "authors": ["Wenchao Gu", "Zongyi Lyu", "Yanlin Wang", "Hongyu Zhang", "Cuiyun Gao", "Michael R. Lyu"], "categories": ["cs.SE", "cs.AI"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00546v1", "summary": "Code retrieval aims to provide users with desired code snippets based on\nusers' natural language queries. With the development of deep learning\ntechnologies, adopting pre-trained models for this task has become mainstream.\nConsidering the retrieval efficiency, most of the previous approaches adopt a\ndual-encoder for this task, which encodes the description and code snippet into\nrepresentation vectors, respectively. However, the model structure of the\ndual-encoder tends to limit the model's performance, since it lacks the\ninteraction between the code snippet and description at the bottom layer of the\nmodel during training. To improve the model's effectiveness while preserving\nits efficiency, we propose a framework, which adopts Self-AdaPtive Model\nDistillation for Efficient CodE Retrieval, named SPENCER. SPENCER first adopts\nthe dual-encoder to narrow the search space and then adopts the cross-encoder\nto improve accuracy. To improve the efficiency of SPENCER, we propose a novel\nmodel distillation technique, which can greatly reduce the inference time of\nthe dual-encoder while maintaining the overall performance. We also propose a\nteaching assistant selection strategy for our model distillation, which can\nadaptively select the suitable teaching assistant models for different\npre-trained models during the model distillation to ensure the model\nperformance. Extensive experiments demonstrate that the combination of\ndual-encoder and cross-encoder improves overall performance compared to solely\ndual-encoder-based models for code retrieval. Besides, our model distillation\ntechnique retains over 98% of the overall performance while reducing the\ninference time of the dual-encoder by 70%.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00546v1", "cate": "cs.SE", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00175", "title": "Adaptive Compensation of Nonlinear Friction in Mechanical Systems Without Velocity Measurement", "authors": ["Jose Guadalupe Romero", "Romeo Ortega", "Leyan Fang", "Alexey Bobtsov"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00175v1", "summary": "Friction is an unavoidable phenomenon that exists in all mechanical systems\nincorporating parts with relative motion. It is well-known that friction is a\nserious impediment for precise servo control, hence the interest to devise a\nprocedure to compensate for it -- a subject that has been studied by many\nresearchers for many years. The vast majority of friction compensation schemes\nreported in the literature rely on the availability of velocity measurements,\nan information that is hard to obtain. A second limitation of the existing\nprocedures is that they rely on mathematical models of friction that contain\nseveral unknown parameters, some of them entering nonlinearly in the dynamic\nequations. In this paper we propose a globally convergent tracking controller\nfor a mechanical system perturbed by static and Coulomb friction, which is a\nreliable mathematical model of the friction phenomenon, that does not rely one\nmeasurement of velocity. The key component is an immersion and invariance-based\nadaptive speed observer, used for the friction compensation. To the best of our\nknowledge, this is the first globally convergent solution to this challenging\nproblem. We also present simulation results of the application of our observer\nfor systems affected by friction, which is described by the more advanced LuGre\nmodel.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00175v1", "cate": "eess.SY", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2503.17526", "title": "Beyond the Encoder: Joint Encoder-Decoder Contrastive Pre-Training Improves Dense Prediction", "authors": ["Sébastien Quetin", "Tapotosh Ghosh", "Farhad Maleki"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.17526v2", "summary": "Contrastive learning methods in self-supervised settings have primarily\nfocused on pre-training encoders, while decoders are typically introduced and\ntrained separately for downstream dense prediction tasks. However, this\nconventional approach overlooks the potential benefits of jointly pre-training\nboth encoder and decoder. In this paper, we propose DeCon, an efficient\nencoder-decoder self-supervised learning (SSL) framework that supports joint\ncontrastive pre-training. We first extend existing SSL architectures to\naccommodate diverse decoders and their corresponding contrastive losses. Then,\nwe introduce a weighted encoder-decoder contrastive loss with non-competing\nobjectives to enable the joint pre-training of encoder-decoder architectures.\nBy adapting an established contrastive SSL framework for dense prediction\ntasks, DeCon achieves new state-of-the-art results: on COCO object detection\nand instance segmentation when pre-trained on COCO dataset; across almost all\ndense downstream benchmark tasks when pre-trained on COCO+ and ImageNet-1K. Our\nresults demonstrate that joint pre-training enhances the representation power\nof the encoder and improves performance in dense prediction tasks. This gain\npersists across heterogeneous decoder architectures, various encoder\narchitectures, and in out-of-domain limited-data scenarios.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.17526v2", "cate": "cs.CV", "date": "2025-03-21", "updated": "2025-07-31"}
{"id": "2508.00543", "title": "Towards Efficient Certification of Maritime Remote Operation Centers", "authors": ["Christian Neurohr", "Marcel Saager", "Lina Putze", "Jan-Patrick Osterloh", "Karina Rothemann", "Hilko Wiards", "Eckard Böde", "Axel Hahn"], "categories": ["cs.CY", "cs.RO"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00543v1", "summary": "Additional automation being build into ships implies a shift of crew from\nship to shore. However, automated ships still have to be monitored and, in some\nsituations, controlled remotely. These tasks are carried out by human operators\nlocated in shore-based remote operation centers. In this work, we present a\nconcept for a hazard database that supports the safeguarding and certification\nof such remote operation centers. The concept is based on a categorization of\nhazard sources which we derive from a generic functional architecture. A\nsubsequent preliminary suitability analysis unveils which methods for hazard\nanalysis and risk assessment can adequately fill this hazard database.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00543v1", "cate": "cs.CY", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00593", "title": "Can User Feedback Help Issue Detection? An Empirical Study on a One-billion-user Online Service System", "authors": ["Shuyao Jiang", "Jiazhen Gu", "Wujie Zheng", "Yangfan Zhou", "Michael R. Lyu"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      Accepted by the 19th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM 2025)", "url": "http://arxiv.org/abs/2508.00593v1", "summary": "Background: It has long been suggested that user feedback, typically written\nin natural language by end-users, can help issue detection. However, for\nlarge-scale online service systems that receive a tremendous amount of\nfeedback, it remains a challenging task to identify severe issues from user\nfeedback. Aims: To develop a better feedback-based issue detection approach, it\nis crucial first to gain a comprehensive understanding of the characteristics\nof user feedback in real production systems. Method: In this paper, we conduct\nan empirical study on 50,378,766 user feedback items from six real-world\nservices in a one-billion-user online service system. We first study what users\nprovide in their feedback. We then examine whether certain features of feedback\nitems can be good indicators of severe issues. Finally, we investigate whether\nadopting machine learning techniques to analyze user feedback is reasonable.\nResults: Our results show that a large proportion of user feedback provides\nirrelevant information about system issues. As a result, it is crucial to\nfilter out issue-irrelevant information when processing user feedback.\nMoreover, we find severe issues that cannot be easily detected based solely on\nuser feedback characteristics. Finally, we find that the distributions of the\nfeedback topics in different time intervals are similar. This confirms that\ndesigning machine learning-based approaches is a viable direction for better\nanalyzing user feedback. Conclusions: We consider that our findings can serve\nas an empirical foundation for feedback-based issue detection in large-scale\nservice systems, which sheds light on the design and implementation of\npractical issue detection approaches.", "comment": "Accepted by the 19th ACM/IEEE International Symposium on Empirical\n  Software Engineering and Measurement (ESEM 2025)", "pdf_url": "http://arxiv.org/pdf/2508.00593v1", "cate": "cs.SE", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00188", "title": "Optimal Messaging Strategy for Incentivizing Agents in Dynamic Systems", "authors": ["Renyan Sun", "Ashutosh Nayyar"], "categories": ["eess.SY", "cs.GT", "cs.SY", "math.OC"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "Comments:      We submitted a full paper to IEEE TAC for review. A preliminary version of this paper is scheduled to be presented at IEEE CDC conference in December 2025", "url": "http://arxiv.org/abs/2508.00188v1", "summary": "We consider a finite-horizon discrete-time dynamic system jointly controlled\nby a designer and one or more agents, where the designer can influence the\nagents' actions through selective information disclosure. At each time step,\nthe designer sends a message to the agent(s) from a prespecified message space.\nThe designer may also take an action that directly influences system dynamics\nand rewards. Each agent uses its received message (and its own information) to\nchoose its action. We are interested in the setting where the designer would\nlike to incentivize each agent to play a specific strategy. We consider a\nnotion of incentive compatibility that is based on sequential rationality at\neach realization of the common information between the designer and the\nagent(s). Our objective is to find a messaging and action strategy for the\ndesigner that maximizes its total expected reward while incentivizing each\nagent to follow a prespecified strategy. Under certain assumptions on the\ninformation structure of the problem, we show that an optimal designer strategy\ncan be computed using a backward inductive algorithm that solves a family of\nlinear programs.", "comment": "We submitted a full paper to IEEE TAC for review. A preliminary\n  version of this paper is scheduled to be presented at IEEE CDC conference in\n  December 2025", "pdf_url": "http://arxiv.org/pdf/2508.00188v1", "cate": "eess.SY", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2503.17856", "title": "ClaraVid: A Holistic Scene Reconstruction Benchmark From Aerial Perspective With Delentropy-Based Complexity Profiling", "authors": ["Radu Beche", "Sergiu Nedevschi"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted ICCV 2025", "url": "http://arxiv.org/abs/2503.17856v2", "summary": "The development of aerial holistic scene understanding algorithms is hindered\nby the scarcity of comprehensive datasets that enable both semantic and\ngeometric reconstruction. While synthetic datasets offer an alternative,\nexisting options exhibit task-specific limitations, unrealistic scene\ncompositions, and rendering artifacts that compromise real-world applicability.\nWe introduce ClaraVid, a synthetic aerial dataset specifically designed to\novercome these limitations. Comprising 16,917 high-resolution images captured\nat 4032x3024 from multiple viewpoints across diverse landscapes, ClaraVid\nprovides dense depth maps, panoptic segmentation, sparse point clouds, and\ndynamic object masks, while mitigating common rendering artifacts. To further\nadvance neural reconstruction, we introduce the Delentropic Scene Profile\n(DSP), a novel complexity metric derived from differential entropy analysis,\ndesigned to quantitatively assess scene difficulty and inform reconstruction\ntasks. Utilizing DSP, we systematically benchmark neural reconstruction\nmethods, uncovering a consistent, measurable correlation between scene\ncomplexity and reconstruction accuracy. Empirical results indicate that higher\ndelentropy strongly correlates with increased reconstruction errors, validating\nDSP as a reliable complexity prior. The data and code are available on the\nproject page at https://rdbch.github.io/claravid/", "comment": "Accepted ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2503.17856v2", "cate": "cs.CV", "date": "2025-03-22", "updated": "2025-07-31"}
{"id": "2508.00589", "title": "Context-based Motion Retrieval using Open Vocabulary Methods for Autonomous Driving", "authors": ["Stefan Englmeier", "Max A. Büttner", "Katharina Winter", "Fabian B. Flohr"], "categories": ["cs.CV", "cs.CL", "cs.IR", "cs.RO", "68T45, 68P20, 68T10, 68T50, 68T07, 68T40", "I.2.10; I.4.8; I.2.9; H.3.3"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      9 pages, 10 figure, project page this https URL , submitted to IEEE Transactions on Intelligent Vehicles (T-IV), This work has been submitted to the IEEE for possible publication", "url": "http://arxiv.org/abs/2508.00589v1", "summary": "Autonomous driving systems must operate reliably in safety-critical\nscenarios, particularly those involving unusual or complex behavior by\nVulnerable Road Users (VRUs). Identifying these edge cases in driving datasets\nis essential for robust evaluation and generalization, but retrieving such rare\nhuman behavior scenarios within the long tail of large-scale datasets is\nchallenging. To support targeted evaluation of autonomous driving systems in\ndiverse, human-centered scenarios, we propose a novel context-aware motion\nretrieval framework. Our method combines Skinned Multi-Person Linear\n(SMPL)-based motion sequences and corresponding video frames before encoding\nthem into a shared multimodal embedding space aligned with natural language.\nOur approach enables the scalable retrieval of human behavior and their context\nthrough text queries. This work also introduces our dataset WayMoCo, an\nextension of the Waymo Open Dataset. It contains automatically labeled motion\nand scene context descriptions derived from generated pseudo-ground-truth SMPL\nsequences and corresponding image data. Our approach outperforms\nstate-of-the-art models by up to 27.5% accuracy in motion-context retrieval,\nwhen evaluated on the WayMoCo dataset.", "comment": "9 pages, 10 figure, project page\n  https://iv.ee.hm.edu/contextmotionclip/, submitted to IEEE Transactions on\n  Intelligent Vehicles (T-IV), This work has been submitted to the IEEE for\n  possible publication", "pdf_url": "http://arxiv.org/pdf/2508.00589v1", "cate": "cs.CV", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00630", "title": "MCeT: Behavioral Model Correctness Evaluation using Large Language Models", "authors": ["Khaled Ahmed", "Jialing Song", "Boqi Chen", "Ou Wei", "Bingzhou Zheng"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      MODELS 2025", "url": "http://arxiv.org/abs/2508.00630v1", "summary": "Behavioral model diagrams, e.g., sequence diagrams, are an essential form of\ndocumentation that are typically designed by system engineers from requirements\ndocumentation, either fully manually or assisted by design tools. With the\ngrowing use of Large Language Models (LLM) as AI modeling assistants, more\nautomation will be involved in generating diagrams. This necessitates the\nadvancement of automatic model correctness evaluation tools. Such a tool can be\nused to evaluate both manually and AI automatically generated models; to\nprovide feedback to system engineers, and enable AI assistants to self-evaluate\nand self-enhance their generated models.\n  In this paper, we propose MCeT, the first fully automated tool to evaluate\nthe correctness of a behavioral model, sequence diagrams in particular, against\nits corresponding requirements text and produce a list of issues that the model\nhas. We utilize LLMs for the correctness evaluation tasks as they have shown\noutstanding natural language understanding ability. However, we show that\ndirectly asking an LLM to compare a diagram to requirements finds less than 35%\nof issues that experienced engineers can find. We propose to supplement the\ndirect check with a fine-grained, multi-perspective approach; we split the\ndiagram into atomic, non-divisible interactions, and split the requirements\ntext into atomic, self-contained items. We compare the diagram with atomic\nrequirements and each diagram-atom with the requirements. We also propose a\nself-consistency checking approach that combines perspectives to mitigate LLM\nhallucinated issues. Our combined approach improves upon the precision of the\ndirect approach from 0.58 to 0.81 in a dataset of real requirements. Moreover,\nthe approach finds 90% more issues that the experienced engineers found than\nthe direct approach, and reports an average of 6 new issues per diagram.", "comment": "MODELS 2025", "pdf_url": "http://arxiv.org/pdf/2508.00630v1", "cate": "cs.SE", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00283", "title": "Neural Co-state Projection Regulator: A Model-free Paradigm for Real-time Optimal Control with Input Constraints", "authors": ["Lihan Lian", "Uduak Inyang-Udoh"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00283v1", "summary": "Learning-based approaches, notably Reinforcement Learning (RL), have shown\npromise for solving optimal control tasks without explicit system models.\nHowever, these approaches are often sample-inefficient, sensitive to reward\ndesign and hyperparameters, and prone to poor generalization, especially under\ninput constraints. To address these challenges, we introduce the neural\nco-state projection regulator (NCPR), a model-free learning-based optimal\ncontrol framework that is grounded in Pontryagin's Minimum Principle (PMP) and\ncapable of solving quadratic regulator problems in nonlinear control-affine\nsystems with input constraints. In this framework, a neural network (NN) is\ntrained in a self-supervised setting to take the current state of the system as\ninput and predict a finite-horizon trajectory of projected co-states (i.e., the\nco-state weighted by the system's input gain). Subsequently, only the first\nelement of the NN's prediction is extracted to solve a lightweight quadratic\nprogram (QP). This workflow is executed in a feedback control setting, allowing\nreal-time computation of control actions that satisfy both input constraints\nand first-order optimality conditions.\n  We test the proposed learning-based model-free quadratic regulator on (1) a\nunicycle model robot reference tracking problem and (2) a pendulum swing-up\ntask. For comparison, reinforcement learning is used on both tasks; and for\ncontext, a model-based controller is used in the unicycle model example. Our\nmethod demonstrates superior generalizability in terms of both unseen system\nstates and varying input constraints, and also shows improved sampling\nefficiency.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00283v1", "cate": "eess.SY", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2503.18711", "title": "Accenture-NVS1: A Novel View Synthesis Dataset", "authors": ["Thomas Sugg", "Kyle O'Brien", "Lekh Poudel", "Alex Dumouchelle", "Michelle Jou", "Marc Bosch", "Deva Ramanan", "Srinivasa Narasimhan", "Shubham Tulsiani"], "categories": ["cs.CV", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      6 pages, 7 figures", "url": "http://arxiv.org/abs/2503.18711v2", "summary": "This paper introduces ACC-NVS1, a specialized dataset designed for research\non Novel View Synthesis specifically for airborne and ground imagery. Data for\nACC-NVS1 was collected in Austin, TX and Pittsburgh, PA in 2023 and 2024. The\ncollection encompasses six diverse real-world scenes captured from both\nairborne and ground cameras, resulting in a total of 148,000 images. ACC-NVS1\naddresses challenges such as varying altitudes and transient objects. This\ndataset is intended to supplement existing datasets, providing additional\nresources for comprehensive research, rather than serving as a benchmark.", "comment": "6 pages, 7 figures", "pdf_url": "http://arxiv.org/pdf/2503.18711v2", "cate": "cs.CV", "date": "2025-03-24", "updated": "2025-07-30"}
{"id": "2508.00724", "title": "Petri Net Modeling and Deadlock-Free Scheduling of Attachable Heterogeneous AGV Systems", "authors": ["Boyu Li", "Zhengchen Li", "Weimin Wu", "Mengchu Zhou"], "categories": ["eess.SY", "cs.RO", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "Comments:      This work has been submitted to the IEEE for possible publication", "url": "http://arxiv.org/abs/2508.00724v1", "summary": "The increasing demand for automation and flexibility drives the widespread\nadoption of heterogeneous automated guided vehicles (AGVs). This work intends\nto investigate a new scheduling problem in a material transportation system\nconsisting of attachable heterogeneous AGVs, namely carriers and shuttles. They\ncan flexibly attach to and detach from each other to cooperatively execute\ncomplex transportation tasks. While such collaboration enhances operational\nefficiency, the attachment-induced synchronization and interdependence render\nthe scheduling coupled and susceptible to deadlock. To tackle this challenge,\nPetri nets are introduced to model AGV schedules, well describing the\nconcurrent and sequential task execution and carrier-shuttle synchronization.\nBased on Petri net theory, a firing-driven decoding method is proposed, along\nwith deadlock detection and prevention strategies to ensure deadlock-free\nschedules. Furthermore, a Petri net-based metaheuristic is developed in an\nadaptive large neighborhood search framework and incorporates an effective\nacceleration method to enhance computational efficiency. Finally, numerical\nexperiments using real-world industrial data validate the effectiveness of the\nproposed algorithm against the scheduling policy applied in engineering\npractice, an exact solver, and four state-of-the-art metaheuristics. A\nsensitivity analysis is also conducted to provide managerial insights.", "comment": "This work has been submitted to the IEEE for possible publication", "pdf_url": "http://arxiv.org/pdf/2508.00724v1", "cate": "eess.SY", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00700", "title": "Is LLM-Generated Code More Maintainable \\& Reliable than Human-Written Code?", "authors": ["Alfred Santa Molison", "Marcia Moraes", "Glaucia Melo", "Fabio Santos", "Wesley K. G. Assuncao"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      Accepted ESEM2025", "url": "http://arxiv.org/abs/2508.00700v1", "summary": "Background: The rise of Large Language Models (LLMs) in software development\nhas opened new possibilities for code generation. Despite the widespread use of\nthis technology, it remains unclear how well LLMs generate code solutions in\nterms of software quality and how they compare to human-written code. Aims:\nThis study compares the internal quality attributes of LLM-generated and\nhuman-written code. Method: Our empirical study integrates datasets of coding\ntasks, three LLM configurations (zero-shot, few-shot, and fine-tuning), and\nSonarQube to assess software quality. The dataset comprises Python code\nsolutions across three difficulty levels: introductory, interview, and\ncompetition. We analyzed key code quality metrics, including maintainability\nand reliability, and the estimated effort required to resolve code issues.\nResults: Our analysis shows that LLM-generated code has fewer bugs and requires\nless effort to fix them overall. Interestingly, fine-tuned models reduced the\nprevalence of high-severity issues, such as blocker and critical bugs, and\nshifted them to lower-severity categories, but decreased the model's\nperformance. In competition-level problems, the LLM solutions sometimes\nintroduce structural issues that are not present in human-written code.\nConclusion: Our findings provide valuable insights into the quality of\nLLM-generated code; however, the introduction of critical issues in more\ncomplex scenarios highlights the need for a systematic evaluation and\nvalidation of LLM solutions. Our work deepens the understanding of the\nstrengths and limitations of LLMs for code generation.", "comment": "Accepted ESEM2025", "pdf_url": "http://arxiv.org/pdf/2508.00700v1", "cate": "cs.SE", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00609", "title": "Low-dimensional observer design for stable linear systems by model reduction", "authors": ["M. F. Shakib", "M. Khalil", "R. Postoyan"], "categories": ["eess.SY", "cs.SY", "math.OC"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00609v1", "summary": "This paper presents a low-dimensional observer design for stable,\nsingle-input single-output, continuous-time linear time-invariant (LTI)\nsystems. Leveraging the model reduction by moment matching technique, we\napproximate the system with a reduced-order model. Based on this reduced-order\nmodel, we design a low-dimensional observer that estimates the states of the\noriginal system. We show that this observer establishes exact asymptotic state\nreconstruction for a given class of inputs tied to the observer's dimension.\nFurthermore, we establish an exponential input-to-state stability property for\ngeneric inputs, ensuring a bounded estimation error. Numerical simulations\nconfirm the effectiveness of the approach for a benchmark model reduction\nproblem.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00609v1", "cate": "eess.SY", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2503.19480", "title": "GenHancer: Imperfect Generative Models are Secretly Strong Vision-Centric Enhancers", "authors": ["Shijie Ma", "Yuying Ge", "Teng Wang", "Yuxin Guo", "Yixiao Ge", "Ying Shan"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV 2025. Project released at: this https URL", "url": "http://arxiv.org/abs/2503.19480v3", "summary": "The synergy between generative and discriminative models receives growing\nattention. While discriminative Contrastive Language-Image Pre-Training (CLIP)\nexcels in high-level semantics, it struggles with perceiving fine-grained\nvisual details. Generally, to enhance representations, generative models take\nCLIP's visual features as conditions for reconstruction. However, the\nunderlying principle remains underexplored. In this work, we empirically found\nthat visually perfect generations are not always optimal for representation\nenhancement. The essence lies in effectively extracting fine-grained knowledge\nfrom generative models while mitigating irrelevant information. To explore\ncritical factors, we delve into three aspects: (1) Conditioning mechanisms: We\nfound that even a small number of local tokens can drastically reduce the\ndifficulty of reconstruction, leading to collapsed training. We thus conclude\nthat utilizing only global visual tokens as conditions is the most effective\nstrategy. (2) Denoising configurations: We observed that end-to-end training\nintroduces extraneous information. To address this, we propose a two-stage\ntraining strategy to prioritize learning useful visual knowledge. Additionally,\nwe demonstrate that lightweight denoisers can yield remarkable improvements.\n(3) Generation paradigms: We explore both continuous and discrete denoisers\nwith desirable outcomes, validating the versatility of our method. Through our\nin-depth explorations, we have finally arrived at an effective method, namely\nGenHancer, which consistently outperforms prior arts on the MMVP-VLM benchmark,\ne.g., 6.0% on OpenAICLIP. The enhanced CLIP can be further plugged into\nmultimodal large language models for better vision-centric performance. All the\nmodels and codes are made publicly available.", "comment": "ICCV 2025. Project released at:\n  https://mashijie1028.github.io/GenHancer/", "pdf_url": "http://arxiv.org/pdf/2503.19480v3", "cate": "cs.CV", "date": "2025-03-25", "updated": "2025-07-31"}
{"id": "2508.00823", "title": "IGL-Nav: Incremental 3D Gaussian Localization for Image-goal Navigation", "authors": ["Wenxuan Guo", "Xiuwei Xu", "Hang Yin", "Ziwei Wang", "Jianjiang Feng", "Jie Zhou", "Jiwen Lu"], "categories": ["cs.CV", "cs.RO"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted to ICCV 2025. Project page: this https URL", "url": "http://arxiv.org/abs/2508.00823v1", "summary": "Visual navigation with an image as goal is a fundamental and challenging\nproblem. Conventional methods either rely on end-to-end RL learning or\nmodular-based policy with topological graph or BEV map as memory, which cannot\nfully model the geometric relationship between the explored 3D environment and\nthe goal image. In order to efficiently and accurately localize the goal image\nin 3D space, we build our navigation system upon the renderable 3D gaussian\n(3DGS) representation. However, due to the computational intensity of 3DGS\noptimization and the large search space of 6-DoF camera pose, directly\nleveraging 3DGS for image localization during agent exploration process is\nprohibitively inefficient. To this end, we propose IGL-Nav, an Incremental 3D\nGaussian Localization framework for efficient and 3D-aware image-goal\nnavigation. Specifically, we incrementally update the scene representation as\nnew images arrive with feed-forward monocular prediction. Then we coarsely\nlocalize the goal by leveraging the geometric information for discrete space\nmatching, which can be equivalent to efficient 3D convolution. When the agent\nis close to the goal, we finally solve the fine target pose with optimization\nvia differentiable rendering. The proposed IGL-Nav outperforms existing\nstate-of-the-art methods by a large margin across diverse experimental\nconfigurations. It can also handle the more challenging free-view image-goal\nsetting and be deployed on real-world robotic platform using a cellphone to\ncapture goal image at arbitrary pose. Project page:\nhttps://gwxuan.github.io/IGL-Nav/.", "comment": "Accepted to ICCV 2025. Project page:\n  https://gwxuan.github.io/IGL-Nav/", "pdf_url": "http://arxiv.org/pdf/2508.00823v1", "cate": "cs.CV", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00738", "title": "Tool-Assisted Conformance Checking to Reference Process Models", "authors": ["Bernhard Rumpe", "Max Stachon", "Sebastian Stüber", "Valdes Voufo"], "categories": ["cs.SE", "cs.FL", "68N30", "D.2.4"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00738v1", "summary": "Reference models convey best practices and standards. The reference\nframeworks necessitate conformance checks to ensure adherence to established\nguidelines and principles, which is crucial for maintaining quality and\nconsistency in various processes. This paper explores automated conformance\nchecks for concrete process models against reference models using causal\ndependency analysis of tasks and events. Existing notions of conformance\nchecking for process models focus on verifying process execution traces and\nlack the expressiveness and automation needed for semantic model comparison,\nleaving this question unresolved. We integrate our approach into a broader\nsemantic framework for defining reference model conformance. We outline an\nalgorithm for reference process model conformance checking, evaluate it through\na case study, and discuss its strengths and limitations. Our research provides\na tool-assisted solution enhancing accuracy and flexibility in process model\nconformance verification.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00738v1", "cate": "cs.SE", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00637", "title": "Cyber-Physical Co-Simulation of Load Frequency Control under Load-Altering Attacks", "authors": ["Michał Forystek", "Andrew D. Syrmakesis", "Alkistis Kontou", "Panos Kotsampopoulos", "Nikos D. Hatziargyriou", "Charalambos Konstantinou"], "categories": ["eess.SY", "cs.CR", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "Comments:      2025 IEEE International Conference on Communications, Control, and Computing Technologies for Smart Grids (SmartGridComm)", "url": "http://arxiv.org/abs/2508.00637v1", "summary": "Integrating Information and Communications Technology (ICT) devices into the\npower grid brings many benefits. However, it also exposes the grid to new\npotential cyber threats. Many control and protection mechanisms, such as Load\nFrequency Control (LFC), responsible for maintaining nominal frequency during\nload fluctuations and Under Frequency Load Shedding (UFLS) disconnecting\nportion of the load during an emergency, are dependent on information exchange\nthrough the communication network. The recently emerging Load Altering Attacks\n(LAAs) utilize a botnet of high-wattage devices to introduce load fluctuation.\nIn their dynamic form (DLAAs), they manipulate the load in response to live\ngrid frequency measurements for increased efficiency, posing a notable threat\nto grid stability. Recognizing the importance of communication networks in\npower grid cyber security research, this paper presents an open-source\nco-simulation environment that models the power grid with the corresponding\ncommunication network, implementing grid protective mechanisms. This setup\nallows the comprehensive analysis of the attacks in concrete LFC and UFLS\nscenarios.", "comment": "2025 IEEE International Conference on Communications, Control, and\n  Computing Technologies for Smart Grids (SmartGridComm)", "pdf_url": "http://arxiv.org/pdf/2508.00637v1", "cate": "eess.SY", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00123", "title": "Melody-Lyrics Matching with Contrastive Alignment Loss", "authors": ["Changhong Wang", "Michel Olvera", "Gaël Richard"], "categories": ["eess.AS", "cs.IR"], "primary_category": "Subjects:       Audio and Speech Processing (eess.AS)", "pdf_link": null, "comments": "Comments:      10 pages, 7 figures, 3 tables. This work has been submitted to the IEEE for possible publication", "url": "http://arxiv.org/abs/2508.00123v1", "summary": "The connection between music and lyrics is far beyond semantic bonds.\nConceptual pairs in the two modalities such as rhythm and rhyme, note duration\nand syllabic stress, and structure correspondence, raise a compelling yet\nseldom-explored direction in the field of music information retrieval. In this\npaper, we present melody-lyrics matching (MLM), a new task which retrieves\npotential lyrics for a given symbolic melody from text sources. Rather than\ngenerating lyrics from scratch, MLM essentially exploits the relationships\nbetween melody and lyrics. We propose a self-supervised representation learning\nframework with contrastive alignment loss for melody and lyrics. This has the\npotential to leverage the abundance of existing songs with paired melody and\nlyrics. No alignment annotations are required. Additionally, we introduce\nsylphone, a novel representation for lyrics at syllable-level activated by\nphoneme identity and vowel stress. We demonstrate that our method can match\nmelody with coherent and singable lyrics with empirical results and intuitive\nexamples. We open source code and provide matching examples on the companion\nwebpage: https://github.com/changhongw/mlm.", "comment": "10 pages, 7 figures, 3 tables. This work has been submitted to the\n  IEEE for possible publication", "pdf_url": "http://arxiv.org/pdf/2508.00123v1", "cate": "eess.AS", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2503.22351", "title": "One Look is Enough: Seamless Patchwise Refinement for Zero-Shot Monocular Depth Estimation on High-Resolution Images", "authors": ["Byeongjun Kwon", "Munchurl Kim"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV 2025 (camera-ready version). [Project page]( this https URL )", "url": "http://arxiv.org/abs/2503.22351v3", "summary": "Zero-shot depth estimation (DE) models exhibit strong generalization\nperformance as they are trained on large-scale datasets. However, existing\nmodels struggle with high-resolution images due to the discrepancy in image\nresolutions of training (with smaller resolutions) and inference (for high\nresolutions). Processing them at full resolution leads to decreased estimation\naccuracy on depth with tremendous memory consumption, while downsampling to the\ntraining resolution results in blurred edges in the estimated depth images.\nPrevailing high-resolution depth estimation methods adopt a patch-based\napproach, which introduces depth discontinuity issues when reassembling the\nestimated depth patches, resulting in test-time inefficiency. Additionally, to\nobtain fine-grained depth details, these methods rely on synthetic datasets due\nto the real-world sparse ground truth depth, leading to poor generalizability.\nTo tackle these limitations, we propose Patch Refine Once (PRO), an efficient\nand generalizable tile-based framework. Our PRO consists of two key components:\n(i) Grouped Patch Consistency Training that enhances test-time efficiency while\nmitigating the depth discontinuity problem by jointly processing four\noverlapping patches and enforcing a consistency loss on their overlapping\nregions within a single backpropagation step, and (ii) Bias Free Masking that\nprevents the DE models from overfitting to dataset-specific biases, enabling\nbetter generalization to real-world datasets even after training on synthetic\ndata. Zero-shot evaluations on Booster, ETH3D, Middlebury 2014, and NuScenes\ndemonstrate that our PRO can be seamlessly integrated into existing depth\nestimation models.", "comment": "ICCV 2025 (camera-ready version). [Project\n  page](https://kaist-viclab.github.io/One-Look-is-Enough_site)", "pdf_url": "http://arxiv.org/pdf/2503.22351v3", "cate": "cs.CV", "date": "2025-03-28", "updated": "2025-07-31"}
{"id": "2403.09596", "title": "Scalable Outdoors Autonomous Drone Flight with Visual-Inertial SLAM and Dense Submaps Built without LiDAR", "authors": ["Sebastián Barbas Laina", "Simon Boche", "Sotiris Papatheodorou", "Dimos Tzoumanikas", "Simon Schaefer", "Hanzhi Chen", "Stefan Leutenegger"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      8 pages, 8 figures", "url": "http://arxiv.org/abs/2403.09596v2", "summary": "Autonomous navigation is needed for several robotics applications. In this\npaper we present an autonomous Micro Aerial Vehicle (MAV) system which purely\nrelies on cost-effective and light-weight passive visual and inertial sensors\nto perform large-scale autonomous navigation in outdoor,unstructured and\ncluttered environments. We leverage visual-inertial simultaneous localization\nand mapping (VI-SLAM) for accurate MAV state estimates and couple it with a\nvolumetric occupancy submapping system to achieve a scalable mapping framework\nwhich can be directly used for path planning. To ensure the safety of the MAV\nduring navigation, we also propose a novel reference trajectory anchoring\nscheme that deforms the reference trajectory the MAV is tracking upon state\nupdates from the VI-SLAM system in a consistent way, even upon large state\nupdates due to loop-closures. We thoroughly validate our system in both real\nand simulated forest environments and at peak velocities up to 3 m/s while not\nencountering a single collision or system failure. To the best of our\nknowledge, this is the first system which achieves this level of performance in\nsuch an unstructured environment using low-cost passive visual sensors and\nfully on-board computation, including VI-SLAM.", "comment": "8 pages, 8 figures", "pdf_url": "http://arxiv.org/pdf/2403.09596v2", "cate": "cs.RO", "date": "2024-03-14", "updated": "2025-08-01"}
{"id": "2508.00178", "title": "The SPACE of AI: Real-World Lessons on AI's Impact on Developers", "authors": ["Brian Houck", "Travis Lowdermilk", "Cody Beyer", "Steven Clarke", "Ben Hanrahan"], "categories": ["cs.HC", "cs.AI", "cs.SE"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00178v1", "summary": "As artificial intelligence (AI) tools become increasingly embedded in\nsoftware development workflows, questions persist about their true impact on\ndeveloper productivity and experience. This paper presents findings from a\nmixed-methods study examining how developers perceive AI's influence across the\ndimensions of the SPACE framework: Satisfaction, Performance, Activity,\nCollaboration and Efficiency. Drawing on survey responses from over 500\ndevelopers and qualitative insights from interviews and observational studies,\nwe find that AI is broadly adopted and widely seen as enhancing productivity,\nparticularly for routine tasks. However, the benefits vary, depending on task\ncomplexity, individual usage patterns, and team-level adoption. Developers\nreport increased efficiency and satisfaction, with less evidence of impact on\ncollaboration. Organizational support and peer learning play key roles in\nmaximizing AI's value. These findings suggest that AI is augmenting developers\nrather than replacing them, and that effective integration depends as much on\nteam culture and support structures as on the tools themselves. We conclude\nwith practical recommendations for teams, organizations and researchers seeking\nto harness AI's potential in software engineering.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00178v1", "cate": "cs.HC", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2508.00775", "title": "Learning to optimize with guarantees: a complete characterization of linearly convergent algorithms", "authors": ["Andrea Martin", "Ian R. Manchester", "Luca Furieri"], "categories": ["eess.SY", "cs.LG", "cs.SY", "math.OC"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00775v1", "summary": "In high-stakes engineering applications, optimization algorithms must come\nwith provable worst-case guarantees over a mathematically defined class of\nproblems. Designing for the worst case, however, inevitably sacrifices\nperformance on the specific problem instances that often occur in practice. We\naddress the problem of augmenting a given linearly convergent algorithm to\nimprove its average-case performance on a restricted set of target problems -\nfor example, tailoring an off-the-shelf solver for model predictive control\n(MPC) for an application to a specific dynamical system - while preserving its\nworst-case guarantees across the entire problem class. Toward this goal, we\ncharacterize the class of algorithms that achieve linear convergence for\nclasses of nonsmooth composite optimization problems. In particular, starting\nfrom a baseline linearly convergent algorithm, we derive all - and only - the\nmodifications to its update rule that maintain its convergence properties. Our\nresults apply to augmenting legacy algorithms such as gradient descent for\nnonconvex, gradient-dominated functions; Nesterov's accelerated method for\nstrongly convex functions; and projected methods for optimization over\npolyhedral feasibility sets. We showcase effectiveness of the approach on\nsolving optimization problems with tight iteration budgets in application to\nill-conditioned systems of linear equations and MPC for linear systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00775v1", "cate": "eess.SY", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00509", "title": "Dynamic Real-Time Ambisonics Order Adaptation for Immersive Networked Music Performances", "authors": ["Paolo Ostan", "Carlo Centofanti", "Mirco Pezzoli", "Alberto Bernardini", "Claudia Rinaldi", "Fabio Antonacci"], "categories": ["eess.AS"], "primary_category": "Subjects:       Audio and Speech Processing (eess.AS)", "pdf_link": null, "comments": "Comments:      to appear in EUSIPCO 2025", "url": "http://arxiv.org/abs/2508.00509v1", "summary": "Advanced remote applications such as Networked Music Performance (NMP)\nrequire solutions to guarantee immersive real-world-like interaction among\nusers. Therefore, the adoption of spatial audio formats, such as Ambisonics, is\nfundamental to let the user experience an immersive acoustic scene. The\naccuracy of the sound scene reproduction increases with the order of the\nAmbisonics enconding, resulting in an improved immersivity at the cost of a\ngreater number of audio channels, which in turn escalates both bandwidth\nrequirements and susceptibility to network impairments (e.g., latency, jitter,\nand packet loss). These factors pose a significant challenge for interactive\nmusic sessions, which demand high spatial fidelity and low end-to-end delay. We\npropose a real-time adaptive higher-order Ambisonics strategy that continuously\nmonitors network throughput and dynamically scales the Ambisonics order. When\navailable bandwidth drops below a preset threshold, the order is lowered to\nprevent audio dropouts; it then reverts to higher orders once conditions\nrecover, thus balancing immersion and reliability. A MUSHRA-based evaluation\nindicates that this adaptive approach is promising to guarantee user experience\nin bandwidth-limited NMP scenarios.", "comment": "to appear in EUSIPCO 2025", "pdf_url": "http://arxiv.org/pdf/2508.00509v1", "cate": "eess.AS", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2504.05164", "title": "Balancing Task-invariant Interaction and Task-specific Adaptation for Unified Image Fusion", "authors": ["Xingyu Hu", "Junjun Jiang", "Chenyang Wang", "Kui Jiang", "Xianming Liu", "Jiayi Ma"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ICCV 2025", "url": "http://arxiv.org/abs/2504.05164v2", "summary": "Unified image fusion aims to integrate complementary information from\nmulti-source images, enhancing image quality through a unified framework\napplicable to diverse fusion tasks. While treating all fusion tasks as a\nunified problem facilitates task-invariant knowledge sharing, it often\noverlooks task-specific characteristics, thereby limiting the overall\nperformance. Existing general image fusion methods incorporate explicit task\nidentification to enable adaptation to different fusion tasks. However, this\ndependence during inference restricts the model's generalization to unseen\nfusion tasks. To address these issues, we propose a novel unified image fusion\nframework named \"TITA\", which dynamically balances both Task-invariant\nInteraction and Task-specific Adaptation. For task-invariant interaction, we\nintroduce the Interaction-enhanced Pixel Attention (IPA) module to enhance\npixel-wise interactions for better multi-source complementary information\nextraction. For task-specific adaptation, the Operation-based Adaptive Fusion\n(OAF) module dynamically adjusts operation weights based on task properties.\nAdditionally, we incorporate the Fast Adaptive Multitask Optimization (FAMO)\nstrategy to mitigate the impact of gradient conflicts across tasks during joint\ntraining. Extensive experiments demonstrate that TITA not only achieves\ncompetitive performance compared to specialized methods across three image\nfusion scenarios but also exhibits strong generalization to unseen fusion\ntasks. The source codes are released at https://github.com/huxingyuabc/TITA.", "comment": "Accepted by ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2504.05164v2", "cate": "cs.CV", "date": "2025-04-07", "updated": "2025-07-31"}
{"id": "2403.17667", "title": "Learning Goal-Directed Object Pushing in Cluttered Scenes With Location-Based Attention", "authors": ["Nils Dengler", "Juan Del Aguila Ferrandis", "João Moura", "Sethu Vijayakumar", "Maren Bennewitz"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      Accepted to IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)2025", "url": "http://arxiv.org/abs/2403.17667v3", "summary": "In complex scenarios where typical pick-and-place techniques are\ninsufficient, often non-prehensile manipulation can ensure that a robot is able\nto fulfill its task. However, non-prehensile manipulation is challenging due to\nits underactuated nature with hybrid-dynamics, where a robot needs to reason\nabout an object's long-term behavior and contact-switching, while being robust\nto contact uncertainty. The presence of clutter in the workspace further\ncomplicates this task, introducing the need to include more advanced spatial\nanalysis to avoid unwanted collisions. Building upon prior work on\nreinforcement learning with multimodal categorical exploration for planar\npushing, we propose to incorporate location-based attention to enable robust\nmanipulation in cluttered scenes. Unlike previous approaches addressing this\nobstacle avoiding pushing task, our framework requires no predefined global\npaths and considers the desired target orientation of the manipulated object.\nExperimental results in simulation as well as with a real KUKA iiwa robot arm\ndemonstrate that our learned policy manipulates objects successfully while\navoiding collisions through complex obstacle configurations, including dynamic\nobstacles, to reach the desired target pose.", "comment": "Accepted to IEEE/RSJ International Conference on Intelligent Robots\n  and Systems (IROS)2025", "pdf_url": "http://arxiv.org/pdf/2403.17667v3", "cate": "cs.RO", "date": "2024-03-26", "updated": "2025-08-01"}
{"id": "2508.00500", "title": "Pro2Guard: Proactive Runtime Enforcement of LLM Agent Safety via Probabilistic Model Checking", "authors": ["Haoyu Wang", "Chris M. Poskitt", "Jun Sun", "Jiali Wei"], "categories": ["cs.AI", "cs.SE"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00500v1", "summary": "Large Language Model (LLM) agents exhibit powerful autonomous capabilities\nacross domains such as robotics, virtual assistants, and web automation.\nHowever, their stochastic behavior introduces significant safety risks that are\ndifficult to anticipate. Existing rule-based enforcement systems, such as\nAgentSpec, focus on developing reactive safety rules, which typically respond\nonly when unsafe behavior is imminent or has already occurred. These systems\nlack foresight and struggle with long-horizon dependencies and distribution\nshifts. To address these limitations, we propose Pro2Guard, a proactive runtime\nenforcement framework grounded in probabilistic reachability analysis.\nPro2Guard abstracts agent behaviors into symbolic states and learns a\nDiscrete-Time Markov Chain (DTMC) from execution traces. At runtime, it\nanticipates future risks by estimating the probability of reaching unsafe\nstates, triggering interventions before violations occur when the predicted\nrisk exceeds a user-defined threshold. By incorporating semantic validity\nchecks and leveraging PAC bounds, Pro2Guard ensures statistical reliability\nwhile approximating the underlying ground-truth model. We evaluate Pro2Guard\nextensively across two safety-critical domains: embodied household agents and\nautonomous vehicles. In embodied agent tasks, Pro2Guard enforces safety early\non up to 93.6% of unsafe tasks using low thresholds, while configurable modes\n(e.g., reflect) allow balancing safety with task success, maintaining up to\n80.4% task completion. In autonomous driving scenarios, Pro2Guard achieves 100%\nprediction of traffic law violations and collisions, anticipating risks up to\n38.66 seconds ahead.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00500v1", "cate": "cs.AI", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00106", "title": "Hyperproperty-Constrained Secure Reinforcement Learning", "authors": ["Ernest Bonnah", "Luan Viet Nguyen", "Khaza Anuarul Hoque"], "categories": ["cs.AI", "cs.LG", "cs.LO", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      Accepted in IEEE/ACM MEMOCODE 2025", "url": "http://arxiv.org/abs/2508.00106v1", "summary": "Hyperproperties for Time Window Temporal Logic (HyperTWTL) is a\ndomain-specific formal specification language known for its effectiveness in\ncompactly representing security, opacity, and concurrency properties for\nrobotics applications. This paper focuses on HyperTWTL-constrained secure\nreinforcement learning (SecRL). Although temporal logic-constrained safe\nreinforcement learning (SRL) is an evolving research problem with several\nexisting literature, there is a significant research gap in exploring\nsecurity-aware reinforcement learning (RL) using hyperproperties. Given the\ndynamics of an agent as a Markov Decision Process (MDP) and opacity/security\nconstraints formalized as HyperTWTL, we propose an approach for learning\nsecurity-aware optimal policies using dynamic Boltzmann softmax RL while\nsatisfying the HyperTWTL constraints. The effectiveness and scalability of our\nproposed approach are demonstrated using a pick-up and delivery robotic mission\ncase study. We also compare our results with two other baseline RL algorithms,\nshowing that our proposed method outperforms them.", "comment": "Accepted in IEEE/ACM MEMOCODE 2025", "pdf_url": "http://arxiv.org/pdf/2508.00106v1", "cate": "cs.AI", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2508.00194", "title": "Audio Prototypical Network For Controllable Music Recommendation", "authors": ["Fırat Öncel", "Emiliano Penaloza", "Haolun Wu", "Shubham Gupta", "Mirco Ravanelli", "Laurent Charlin", "Cem Subakan"], "categories": ["cs.IR", "eess.AS"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "Comments:      Accepted to MLSP2025", "url": "http://arxiv.org/abs/2508.00194v1", "summary": "Traditional recommendation systems represent user preferences in dense\nrepresentations obtained through black-box encoder models. While these models\noften provide strong recommendation performance, they lack interpretability for\nusers, leaving users unable to understand or control the system's modeling of\ntheir preferences. This limitation is especially challenging in music\nrecommendation, where user preferences are highly personal and often evolve\nbased on nuanced qualities like mood, genre, tempo, or instrumentation. In this\npaper, we propose an audio prototypical network for controllable music\nrecommendation. This network expresses user preferences in terms of prototypes\nrepresentative of semantically meaningful features pertaining to musical\nqualities. We show that the model obtains competitive recommendation\nperformance compared to popular baseline models while also providing\ninterpretable and controllable user profiles.", "comment": "Accepted to MLSP2025", "pdf_url": "http://arxiv.org/pdf/2508.00194v1", "cate": "cs.IR", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2504.05422", "title": "EP-Diffuser: An Efficient Diffusion Model for Traffic Scene Generation and Prediction via Polynomial Representations", "authors": ["Yue Yao", "Mohamed-Khalil Bouzidi", "Daniel Goehring", "Joerg Reichardt"], "categories": ["cs.CV", "cs.LG", "cs.RO"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.05422v3", "summary": "As the prediction horizon increases, predicting the future evolution of\ntraffic scenes becomes increasingly difficult due to the multi-modal nature of\nagent motion. Most state-of-the-art (SotA) prediction models primarily focus on\nforecasting the most likely future. However, for the safe operation of\nautonomous vehicles, it is equally important to cover the distribution for\nplausible motion alternatives. To address this, we introduce EP-Diffuser, a\nnovel parameter-efficient diffusion-based generative model designed to capture\nthe distribution of possible traffic scene evolutions. Conditioned on road\nlayout and agent history, our model acts as a predictor and generates diverse,\nplausible scene continuations. We benchmark EP-Diffuser against two SotA models\nin terms of accuracy and plausibility of predictions on the Argoverse 2\ndataset. Despite its significantly smaller model size, our approach achieves\nboth highly accurate and plausible traffic scene predictions. We further\nevaluate model generalization ability in an out-of-distribution (OoD) test\nsetting using Waymo Open dataset and show superior robustness of our approach.\nThe code and model checkpoints are available at:\nhttps://github.com/continental/EP-Diffuser.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.05422v3", "cate": "cs.CV", "date": "2025-04-07", "updated": "2025-07-31"}
{"id": "2409.11372", "title": "PC-SRIF: Preconditioned Cholesky-based Square Root Information Filter for Vision-aided Inertial Navigation", "authors": ["Tong Ke", "Parth Agrawal", "Yun Zhang", "Weikun Zhen", "Chao X. Guo", "Toby Sharp", "Ryan C. Dutoit"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      This work has been accepted to the 2025 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2025)", "url": "http://arxiv.org/abs/2409.11372v3", "summary": "In this paper, we introduce a novel estimator for vision-aided inertial\nnavigation systems (VINS), the Preconditioned Cholesky-based Square Root\nInformation Filter (PC-SRIF). When solving linear systems, employing Cholesky\ndecomposition offers superior efficiency but can compromise numerical\nstability. Due to this, existing VINS utilizing (Square Root) Information\nFilters often opt for QR decomposition on platforms where single precision is\npreferred, avoiding the numerical challenges associated with Cholesky\ndecomposition. While these issues are often attributed to the ill-conditioned\ninformation matrix in VINS, our analysis reveals that this is not an inherent\nproperty of VINS but rather a consequence of specific parameterizations. We\nidentify several factors that contribute to an ill-conditioned information\nmatrix and propose a preconditioning technique to mitigate these conditioning\nissues. Building on this analysis, we present PC-SRIF, which exhibits\nremarkable stability in performing Cholesky decomposition in single precision\nwhen solving linear systems in VINS. Consequently, PC-SRIF achieves superior\ntheoretical efficiency compared to alternative estimators. To validate the\nefficiency advantages and numerical stability of PC-SRIF based VINS, we have\nconducted well controlled experiments, which provide empirical evidence in\nsupport of our theoretical findings. Remarkably, in our VINS implementation,\nPC-SRIF's runtime is 41% faster than QR-based SRIF.", "comment": "This work has been accepted to the 2025 IEEE/RSJ International\n  Conference on Intelligent Robots and Systems (IROS 2025)", "pdf_url": "http://arxiv.org/pdf/2409.11372v3", "cate": "cs.RO", "date": "2024-09-17", "updated": "2025-07-31"}
{"id": "2508.00654", "title": "LEO: An Open-Source Platform for Linking OMERO with Lab Notebooks and Heterogeneous Metadata Sources", "authors": ["Rodrigo Escobar Díaz Guerrero", "Jamile Mohammad Jafari", "Tobias Meyer-Zedler", "Michael Schmitt", "Juergen Popp", "Thomas Bocklitz"], "categories": ["cs.CE", "cs.SE"], "primary_category": "Subjects:       Computational Engineering, Finance, and Science (cs.CE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00654v1", "summary": "In the interdisciplinary field of microscopy research, managing and\nintegrating large volumes of data stored across disparate platforms remains a\nmajor challenge. Data types such as bioimages, experimental records, and\nspectral information are often maintained in separate repositories, each\nfollowing different management standards. However, linking these data sources\nacross the research lifecycle is essential to align with the FAIR principles of\ndata management: Findability, Accessibility, Interoperability, and Reusability.\nDespite this need, there is a notable lack of tools capable of effectively\nintegrating and linking data from heterogeneous sources. To address this gap,\nwe present LEO (Linking Electronic Lab Notebooks with OMERO), a web-based\nplatform designed to create and manage links between distributed data systems.\nLEO was initially developed to link objects between Electronic Lab Notebooks\n(ELNs) and OMERO, but its functionality has since been extended through a\nplugin-based architecture, allowing the integration of additional data sources.\nThis extensibility makes LEO a scalable and flexible solution for a wide range\nof microscopy research workflows.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00654v1", "cate": "cs.CE", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00193", "title": "A Practical Finite Element Approach for Simulating Dynamic Crack Growth in Cu/Ultra Low-k Interconnect Structures", "authors": ["Yuxi Xie", "Ethan J. Wu", "Lu Xu", "Jimmy Perez", "Shaofan Li"], "categories": ["cs.CE", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Computational Engineering, Finance, and Science (cs.CE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00193v1", "summary": "This work presents a practical finite element modeling strategy, the Crack\nElement Method (CEM), for simulating the dynamic crack propagation in\ntwo-dimensional structures. The method employs an element-splitting algorithm\nbased on the Edge-based Smoothed Finite Element Method (ES-FEM) to capture the\nelement-wise crack growth while reducing the formation of poorly shaped\nelements that can compromise numerical accuracy and computational performance.\nA fracture energy release rate formulation is also developed based on the\nevolving topology of the split elements. The proposed approach is validated\nthrough a series of classical benchmark problems, demonstrating its accuracy\nand robustness in addressing dynamic fracture scenarios. Finally, the\napplicability of the CEM is illustrated in a case study involving patterned\nCu/Ultra Low-k interconnect structures.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00193v1", "cate": "cs.CE", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2508.00391", "title": "Cued-Agent: A Collaborative Multi-Agent System for Automatic Cued Speech Recognition", "authors": ["Guanjie Huang", "Danny H. K. Tsang", "Shan Yang", "Guangzhi Lei", "Li Liu"], "categories": ["cs.CV", "eess.AS"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      9 pages", "url": "http://arxiv.org/abs/2508.00391v1", "summary": "Cued Speech (CS) is a visual communication system that combines lip-reading\nwith hand coding to facilitate communication for individuals with hearing\nimpairments. Automatic CS Recognition (ACSR) aims to convert CS hand gestures\nand lip movements into text via AI-driven methods. Traditionally, the temporal\nasynchrony between hand and lip movements requires the design of complex\nmodules to facilitate effective multimodal fusion. However, constrained by\nlimited data availability, current methods demonstrate insufficient capacity\nfor adequately training these fusion mechanisms, resulting in suboptimal\nperformance. Recently, multi-agent systems have shown promising capabilities in\nhandling complex tasks with limited data availability. To this end, we propose\nthe first collaborative multi-agent system for ACSR, named Cued-Agent. It\nintegrates four specialized sub-agents: a Multimodal Large Language Model-based\nHand Recognition agent that employs keyframe screening and CS expert prompt\nstrategies to decode hand movements, a pretrained Transformer-based Lip\nRecognition agent that extracts lip features from the input video, a Hand\nPrompt Decoding agent that dynamically integrates hand prompts with lip\nfeatures during inference in a training-free manner, and a Self-Correction\nPhoneme-to-Word agent that enables post-process and end-to-end conversion from\nphoneme sequences to natural language sentences for the first time through\nsemantic refinement. To support this study, we expand the existing Mandarin CS\ndataset by collecting data from eight hearing-impaired cuers, establishing a\nmixed dataset of fourteen subjects. Extensive experiments demonstrate that our\nCued-Agent performs superbly in both normal and hearing-impaired scenarios\ncompared with state-of-the-art methods. The implementation is available at\nhttps://github.com/DennisHgj/Cued-Agent.", "comment": "9 pages", "pdf_url": "http://arxiv.org/pdf/2508.00391v1", "cate": "cs.CV", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2504.13593", "title": "KAN or MLP? Point Cloud Shows the Way Forward", "authors": ["Yan Shi", "Qingdong He", "Yijun Liu", "Xiaoyu Liu", "Jingyong Su"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.13593v2", "summary": "Multi-Layer Perceptrons (MLPs) have become one of the fundamental\narchitectural component in point cloud analysis due to its effective feature\nlearning mechanism. However, when processing complex geometric structures in\npoint clouds, MLPs' fixed activation functions struggle to efficiently capture\nlocal geometric features, while suffering from poor parameter efficiency and\nhigh model redundancy. In this paper, we propose PointKAN, which applies\nKolmogorov-Arnold Networks (KANs) to point cloud analysis tasks to investigate\ntheir efficacy in hierarchical feature representation. First, we introduce a\nGeometric Affine Module (GAM) to transform local features, improving the\nmodel's robustness to geometric variations. Next, in the Local Feature\nProcessing (LFP), a parallel structure extracts both group-level features and\nglobal context, providing a rich representation of both fine details and\noverall structure. Finally, these features are combined and processed in the\nGlobal Feature Processing (GFP). By repeating these operations, the receptive\nfield gradually expands, enabling the model to capture complete geometric\ninformation of the point cloud. To overcome the high parameter counts and\ncomputational inefficiency of standard KANs, we develop Efficient-KANs in the\nPointKAN-elite variant, which significantly reduces parameters while\nmaintaining accuracy. Experimental results demonstrate that PointKAN\noutperforms PointMLP on benchmark datasets such as ModelNet40, ScanObjectNN,\nand ShapeNetPart, with particularly strong performance in Few-shot Learning\ntask. Additionally, PointKAN achieves substantial reductions in parameter\ncounts and computational complexity (FLOPs). This work highlights the potential\nof KANs-based architectures in 3D vision and opens new avenues for research in\npoint cloud understanding.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.13593v2", "cate": "cs.CV", "date": "2025-04-18", "updated": "2025-07-31"}
{"id": "2410.06372", "title": "Cooperative and Asynchronous Transformer-based Mission Planning for Heterogeneous Teams of Mobile Robots", "authors": ["Milad Farjadnasab", "Shahin Sirouspour"], "categories": ["cs.RO", "cs.AI", "I.2.9; I.2.11"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2410.06372v3", "summary": "Cooperative mission planning for heterogeneous teams of mobile robots\npresents a unique set of challenges, particularly when operating under\ncommunication constraints and limited computational resources. To address these\nchallenges, we propose the Cooperative and Asynchronous Transformer-based\nMission Planning (CATMiP) framework, which leverages multi-agent reinforcement\nlearning (MARL) to coordinate distributed decision making among agents with\ndiverse sensing, motion, and actuation capabilities, operating under sporadic\nad hoc communication. A Class-based Macro-Action Decentralized Partially\nObservable Markov Decision Process (CMacDec-POMDP) is also formulated to\neffectively model asynchronous decision-making for heterogeneous teams of\nagents. The framework utilizes an asynchronous centralized training and\ndistributed execution scheme, enabled by the proposed Asynchronous Multi-Agent\nTransformer (AMAT) architecture. This design allows a single trained model to\ngeneralize to larger environments and accommodate varying team sizes and\ncompositions. We evaluate CATMiP in a 2D grid-world simulation environment and\ncompare its performance against planning-based exploration methods. Results\ndemonstrate CATMiP's superior efficiency, scalability, and robustness to\ncommunication dropouts and input noise, highlighting its potential for\nreal-world heterogeneous mobile robot systems. The code is available at\nhttps://github.com/mylad13/CATMiP", "comment": null, "pdf_url": "http://arxiv.org/pdf/2410.06372v3", "cate": "cs.RO", "date": "2024-10-08", "updated": "2025-07-31"}
{"id": "2508.00682", "title": "Unveiling Dynamic Binary Instrumentation Techniques", "authors": ["Oscar Llorente-Vazquez", "Xabier Ugarte-Pedrero", "Igor Santos-Grueiro", "Pablo Garcia Bringas"], "categories": ["cs.CR", "cs.SE"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00682v1", "summary": "Dynamic Binary Instrumentation (DBI) is the set of techniques that enable\ninstrumentation of programs at run-time, making it possible to monitor and\nmodify the execution of compiled binaries or entire systems. DBI is used for\ncountless security applications and analyses, and is extensively used across\nmany fields in both industry and academia. Over the years, several DBI\napproaches have been proposed based on different technologies and implementing\ndiverse techniques. Every solution tries to overcome certain limitations, but\nthey sometimes bring other shortcomings. Some are specialized for one\nparticular domain or task, while others have a wider scope.\n  In this paper, we shed light into the labyrinth of DBI, bringing together\nprocess-level and whole-system approaches. We depict their building blocks and\nanalyze the underlying instrumentation techniques, comparing their ability to\ninstrument different primitives and run-time events. Then, we evaluate their\nperformance when implementing each primitive, and highlight relevant\nobservations. Our results show that no single technique is better than the rest\nin all circumstances.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00682v1", "cate": "cs.CR", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00603", "title": "Subband Architecture Aided Selective Fixed-Filter Active Noise Control", "authors": ["Hong-Cheng Liang", "Man-Wai Mak", "Kong Aik Lee"], "categories": ["eess.SP", "cs.SY", "eess.AS", "eess.SY"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00603v1", "summary": "The feedforward selective fixed-filter method selects the most suitable\npre-trained control filter based on the spectral features of the detected\nreference signal, effectively avoiding slow convergence in conventional\nadaptive algorithms. However, it can only handle limited types of noises, and\nthe performance degrades when the input noise exhibits non-uniform power\nspectral density. To address these limitations, this paper devises a novel\nselective fixed-filter scheme based on a delayless subband structure. In the\noff-line training stage, subband control filters are pre-trained for different\nfrequency ranges and stored in a dedicated sub-filter database. During the\non-line control stage, the incoming noise is decomposed using a polyphase FFT\nfilter bank, and a frequency-band-matching mechanism assigns each subband\nsignal the most appropriate control filter. Subsequently, a weight stacking\ntechnique is employed to combine all subband weights into a fullband filter,\nenabling real-time noise suppression. Experimental results demonstrate that the\nproposed scheme provides fast convergence, effective noise reduction, and\nstrong robustness in handling more complicated noisy environments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00603v1", "cate": "eess.SP", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00037", "title": "Predicting Large-scale Urban Network Dynamics with Energy-informed Graph Neural Diffusion", "authors": ["Tong Nie", "Jian Sun", "Wei Ma"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted at IEEE Transactions on Industrial Informatics", "url": "http://arxiv.org/abs/2508.00037v1", "summary": "Networked urban systems facilitate the flow of people, resources, and\nservices, and are essential for economic and social interactions. These systems\noften involve complex processes with unknown governing rules, observed by\nsensor-based time series. To aid decision-making in industrial and engineering\ncontexts, data-driven predictive models are used to forecast spatiotemporal\ndynamics of urban systems. Current models such as graph neural networks have\nshown promise but face a trade-off between efficacy and efficiency due to\ncomputational demands. Hence, their applications in large-scale networks still\nrequire further efforts. This paper addresses this trade-off challenge by\ndrawing inspiration from physical laws to inform essential model designs that\nalign with fundamental principles and avoid architectural redundancy. By\nunderstanding both micro- and macro-processes, we present a principled\ninterpretable neural diffusion scheme based on Transformer-like structures\nwhose attention layers are induced by low-dimensional embeddings. The proposed\nscalable spatiotemporal Transformer (ScaleSTF), with linear complexity, is\nvalidated on large-scale urban systems including traffic flow, solar power, and\nsmart meters, showing state-of-the-art performance and remarkable scalability.\nOur results constitute a fresh perspective on the dynamics prediction in\nlarge-scale urban networks.", "comment": "Accepted at IEEE Transactions on Industrial Informatics", "pdf_url": "http://arxiv.org/pdf/2508.00037v1", "cate": "cs.LG", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2504.17761", "title": "Step1X-Edit: A Practical Framework for General Image Editing", "authors": ["Shiyu Liu", "Yucheng Han", "Peng Xing", "Fukun Yin", "Rui Wang", "Wei Cheng", "Jiaqi Liao", "Yingming Wang", "Honghao Fu", "Chunrui Han", "Guopeng Li", "Yuang Peng", "Quan Sun", "Jingwei Wu", "Yan Cai", "Zheng Ge", "Ranchen Ming", "Lei Xia", "Xianfang Zeng", "Yibo Zhu", "Binxing Jiao", "Xiangyu Zhang", "Gang Yu", "Daxin Jiang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      code: this https URL", "url": "http://arxiv.org/abs/2504.17761v5", "summary": "In recent years, image editing models have witnessed remarkable and rapid\ndevelopment. The recent unveiling of cutting-edge multimodal models such as\nGPT-4o and Gemini2 Flash has introduced highly promising image editing\ncapabilities. These models demonstrate an impressive aptitude for fulfilling a\nvast majority of user-driven editing requirements, marking a significant\nadvancement in the field of image manipulation. However, there is still a large\ngap between the open-source algorithm with these closed-source models. Thus, in\nthis paper, we aim to release a state-of-the-art image editing model, called\nStep1X-Edit, which can provide comparable performance against the closed-source\nmodels like GPT-4o and Gemini2 Flash. More specifically, we adopt the\nMultimodal LLM to process the reference image and the user's editing\ninstruction. A latent embedding has been extracted and integrated with a\ndiffusion image decoder to obtain the target image. To train the model, we\nbuild a data generation pipeline to produce a high-quality dataset. For\nevaluation, we develop the GEdit-Bench, a novel benchmark rooted in real-world\nuser instructions. Experimental results on GEdit-Bench demonstrate that\nStep1X-Edit outperforms existing open-source baselines by a substantial margin\nand approaches the performance of leading proprietary models, thereby making\nsignificant contributions to the field of image editing.", "comment": "code: https://github.com/stepfun-ai/Step1X-Edit", "pdf_url": "http://arxiv.org/pdf/2504.17761v5", "cate": "cs.CV", "date": "2025-04-24", "updated": "2025-07-31"}
{"id": "2410.14968", "title": "AugInsert: Learning Robust Visual-Force Policies via Data Augmentation for Object Assembly Tasks", "authors": ["Ryan Diaz", "Adam Imdieke", "Vivek Veeriah", "Karthik Desingh"], "categories": ["cs.RO", "cs.LG"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      Accepted to IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) 2025", "url": "http://arxiv.org/abs/2410.14968v2", "summary": "Operating in unstructured environments like households requires robotic\npolicies that are robust to out-of-distribution conditions. Although much work\nhas been done in evaluating robustness for visuomotor policies, the robustness\nevaluation of a multisensory approach that includes force-torque sensing\nremains largely unexplored. This work introduces a novel, factor-based\nevaluation framework with the goal of assessing the robustness of multisensory\npolicies in a peg-in-hole assembly task. To this end, we develop a multisensory\npolicy framework utilizing the Perceiver IO architecture to learn the task. We\ninvestigate which factors pose the greatest generalization challenges in object\nassembly and explore a simple multisensory data augmentation technique to\nenhance out-of-distribution performance. We provide a simulation environment\nenabling controlled evaluation of these factors. Our results reveal that\nmultisensory variations such as Grasp Pose present the most significant\nchallenges for robustness, and naive unisensory data augmentation applied\nindependently to each sensory modality proves insufficient to overcome them.\nAdditionally, we find force-torque sensing to be the most informative modality\nfor our contact-rich assembly task, with vision being the least informative.\nFinally, we briefly discuss supporting real-world experimental results. For\nadditional experiments and qualitative results, we refer to the project webpage\nhttps://rpm-lab-umn.github.io/auginsert/ .", "comment": "Accepted to IEEE/RSJ International Conference on Intelligent Robots\n  and Systems (IROS) 2025", "pdf_url": "http://arxiv.org/pdf/2410.14968v2", "cate": "cs.RO", "date": "2024-10-19", "updated": "2025-08-01"}
{"id": "2407.13900", "title": "Exploring the Evidence-Based SE Beliefs of Generative AI Tools", "authors": ["Chris Brown", "Jason Cusati"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2407.13900v3", "summary": "Recent innovations in generative artificial intelligence (AI), primarily\npowered by large language models (LLMs), have transformed how programmers\ndevelop and maintain software -- leading to new frontiers in software\nengineering (SE). The advanced capabilities of generative AI tools to support\nsoftware development tasks have led to a rise in their adoption within software\ndevelopment workflows. However, little is known about how AI tools perceive\nevidence-based beliefs and practices verified by research findings. To this\nend, we conduct a preliminary evaluation conceptually replicating prior work to\nexplore the \"beliefs\" of generative AI tools used to support software\ndevelopment tasks. We investigate 17 evidence-based claims posited by empirical\nSE research across five generative AI tools. Our findings show that generative\nAI tools have ambiguous beliefs regarding research claims and lack credible\nevidence to support responses. Based on our results, we provide implications\nfor practitioners integrating generative AI-based systems into development\ncontexts and shed light on future research directions to enhance the\nreliability and trustworthiness of generative AI -- aiming to increase\nawareness and adoption of evidence-based SE research findings in practice.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2407.13900v3", "cate": "cs.SE", "date": "2024-07-18", "updated": "2025-08-01"}
{"id": "2508.00663", "title": "Organic Electrochemical Neurons: Nonlinear Tools for Complex Dynamics", "authors": ["Gonzalo Rivera-Sierra", "Roberto Fenollosa", "Juan Bisquert"], "categories": ["physics.chem-ph", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Chemical Physics (physics.chem-ph)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00663v1", "summary": "Hybrid oscillator architectures that combine feedback oscillators with\nself-sustained negative resistance oscillators have emerged as a promising\nplatform for artificial neuron design. In this work, we introduce a modeling\nand analysis framework for amplifier-assisted organic electrochemical neurons,\nleveraging nonlinear dynamical systems theory. By formulating the system as\ncoupled differential equations describing membrane voltage and internal state\nvariables, we identify the conditions for self-sustained oscillations and\ncharacterize the resulting dynamics through nullclines, phase-space analysis,\nand bifurcation behavior, providing complementary insight to standard\ncircuit-theoretic arguments of the operation of oscillators. Our simplified yet\nrigorous model enables tractable analysis of circuits integrating classical\nfeedback components (e.g., operational amplifiers) with novel devices\nexhibiting negative differential resistance, such as organic electrochemical\ntransistors (OECT). This approach reveals the core mechanisms behind\noscillation generation, demonstrating the utility of dynamic systems theory in\nunderstanding and designing complex hybrid circuits. Beyond neuromorphic and\nbioelectronic applications, the proposed framework offers a generalizable\nfoundation for developing tunable, biologically inspired oscillatory systems in\nsensing, signal processing, and adaptive control.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00663v1", "cate": "physics.chem-ph", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00039", "title": "Hybrid LSTM-Transformer Models for Profiling Highway-Railway Grade Crossings", "authors": ["Kaustav Chatterjee", "Joshua Q. Li", "Fatemeh Ansari", "Masud Rana Munna", "Kundan Parajulee", "Jared Schwennesen"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00039v1", "summary": "Hump crossings, or high-profile Highway Railway Grade Crossings (HRGCs), pose\nsafety risks to highway vehicles due to potential hang-ups. These crossings\ntypically result from post-construction railway track maintenance activities or\nnon-compliance with design guidelines for HRGC vertical alignments.\nConventional methods for measuring HRGC profiles are costly, time-consuming,\ntraffic-disruptive, and present safety challenges. To address these issues,\nthis research employed advanced, cost-effective techniques and innovative\nmodeling approaches for HRGC profile measurement. A novel hybrid deep learning\nframework combining Long Short-Term Memory (LSTM) and Transformer architectures\nwas developed by utilizing instrumentation and ground truth data.\nInstrumentation data were gathered using a highway testing vehicle equipped\nwith Inertial Measurement Unit (IMU) and Global Positioning System (GPS)\nsensors, while ground truth data were obtained via an industrial-standard\nwalking profiler. Field data was collected at the Red Rock Railroad Corridor in\nOklahoma. Three advanced deep learning models Transformer-LSTM sequential\n(model 1), LSTM-Transformer sequential (model 2), and LSTM-Transformer parallel\n(model 3) were evaluated to identify the most efficient architecture. Models 2\nand 3 outperformed the others and were deployed to generate 2D/3D HRGC\nprofiles. The deep learning models demonstrated significant potential to\nenhance highway and railroad safety by enabling rapid and accurate assessment\nof HRGC hang-up susceptibility.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00039v1", "cate": "cs.LG", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2505.02178", "title": "Sparfels: Fast Reconstruction from Sparse Unposed Imagery", "authors": ["Shubhendu Jena", "Amine Ouasfi", "Mae Younes", "Adnane Boukhayma"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV 2025. Project page : this https URL", "url": "http://arxiv.org/abs/2505.02178v4", "summary": "We present a method for Sparse view reconstruction with surface element\nsplatting that runs within 3 minutes on a consumer grade GPU. While few methods\naddress sparse radiance field learning from noisy or unposed sparse cameras,\nshape recovery remains relatively underexplored in this setting. Several\nradiance and shape learning test-time optimization methods address the sparse\nposed setting by learning data priors or using combinations of external\nmonocular geometry priors. Differently, we propose an efficient and simple\npipeline harnessing a single recent 3D foundation model. We leverage its\nvarious task heads, notably point maps and camera initializations to\ninstantiate a bundle adjusting 2D Gaussian Splatting (2DGS) model, and image\ncorrespondences to guide camera optimization midst 2DGS training. Key to our\ncontribution is a novel formulation of splatted color variance along rays,\nwhich can be computed efficiently. Reducing this moment in training leads to\nmore accurate shape reconstructions. We demonstrate state-of-the-art\nperformances in the sparse uncalibrated setting in reconstruction and novel\nview benchmarks based on established multi-view datasets.", "comment": "ICCV 2025. Project page :\n  https://shubhendu-jena.github.io/Sparfels-web/", "pdf_url": "http://arxiv.org/pdf/2505.02178v4", "cate": "cs.CV", "date": "2025-05-04", "updated": "2025-07-31"}
{"id": "2411.13587", "title": "Exploring the Adversarial Vulnerabilities of Vision-Language-Action Models in Robotics", "authors": ["Taowen Wang", "Cheng Han", "James Chenhao Liang", "Wenhao Yang", "Dongfang Liu", "Luna Xinyu Zhang", "Qifan Wang", "Jiebo Luo", "Ruixiang Tang"], "categories": ["cs.RO", "cs.AI"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      ICCV camera ready; Github: this https URL Homepage: this https URL", "url": "http://arxiv.org/abs/2411.13587v4", "summary": "Recently in robotics, Vision-Language-Action (VLA) models have emerged as a\ntransformative approach, enabling robots to execute complex tasks by\nintegrating visual and linguistic inputs within an end-to-end learning\nframework. Despite their significant capabilities, VLA models introduce new\nattack surfaces. This paper systematically evaluates their robustness.\nRecognizing the unique demands of robotic execution, our attack objectives\ntarget the inherent spatial and functional characteristics of robotic systems.\nIn particular, we introduce two untargeted attack objectives that leverage\nspatial foundations to destabilize robotic actions, and a targeted attack\nobjective that manipulates the robotic trajectory. Additionally, we design an\nadversarial patch generation approach that places a small, colorful patch\nwithin the camera's view, effectively executing the attack in both digital and\nphysical environments. Our evaluation reveals a marked degradation in task\nsuccess rates, with up to a 100\\% reduction across a suite of simulated robotic\ntasks, highlighting critical security gaps in current VLA architectures. By\nunveiling these vulnerabilities and proposing actionable evaluation metrics, we\nadvance both the understanding and enhancement of safety for VLA-based robotic\nsystems, underscoring the necessity for continuously developing robust defense\nstrategies prior to physical-world deployments.", "comment": "ICCV camera ready; Github:\n  https://github.com/William-wAng618/roboticAttack Homepage:\n  https://vlaattacker.github.io/", "pdf_url": "http://arxiv.org/pdf/2411.13587v4", "cate": "cs.RO", "date": "2024-11-18", "updated": "2025-08-01"}
{"id": "2503.07556", "title": "Novice Developers' Perspectives on Adopting LLMs for Software Development: A Systematic Literature Review", "authors": ["Samuel Ferino", "Rashina Hoda", "John Grundy", "Christoph Treude"], "categories": ["cs.SE", "cs.AI"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.07556v2", "summary": "Following the rise of large language models (LLMs), many studies have emerged\nin recent years focusing on exploring the adoption of LLM-based tools for\nsoftware development by novice developers: computer science/software\nengineering students and early-career industry developers with two years or\nless of professional experience. These studies have sought to understand the\nperspectives of novice developers on using these tools, a critical aspect of\nthe successful adoption of LLMs in software engineering. To systematically\ncollect and summarise these studies, we conducted a systematic literature\nreview (SLR) following the guidelines by Kitchenham et al. on 80 primary\nstudies published between April 2022 and June 2025 to answer four research\nquestions (RQs). In answering RQ1, we categorised the study motivations and\nmethodological approaches. In RQ2, we identified the software development tasks\nfor which novice developers use LLMs. In RQ3, we categorised the advantages,\nchallenges, and recommendations discussed in the studies. Finally, we discuss\nthe study limitations and future research needs suggested in the primary\nstudies in answering RQ4. Throughout the paper, we also indicate directions for\nfuture work and implications for software engineering researchers, educators,\nand developers. Our research artifacts are publicly available at\nhttps://github.com/Samuellucas97/SupplementaryInfoPackage-SLR.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.07556v2", "cate": "cs.SE", "date": "2025-03-10", "updated": "2025-08-01"}
{"id": "2508.00692", "title": "Wind Power Scenario Generation based on the Generalized Dynamic Factor Model and Generative Adversarial Network", "authors": ["Young-ho Cho", "Hao Zhu", "Duehee Lee", "Ross Baldick"], "categories": ["cs.LG", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00692v1", "summary": "For conducting resource adequacy studies, we synthesize multiple long-term\nwind power scenarios of distributed wind farms simultaneously by using the\nspatio-temporal features: spatial and temporal correlation, waveforms, marginal\nand ramp rates distributions of waveform, power spectral densities, and\nstatistical characteristics. Generating the spatial correlation in scenarios\nrequires the design of common factors for neighboring wind farms and\nantithetical factors for distant wind farms. The generalized dynamic factor\nmodel (GDFM) can extract the common factors through cross spectral density\nanalysis, but it cannot closely imitate waveforms. The GAN can synthesize\nplausible samples representing the temporal correlation by verifying samples\nthrough a fake sample discriminator. To combine the advantages of GDFM and GAN,\nwe use the GAN to provide a filter that extracts dynamic factors with temporal\ninformation from the observation data, and we then apply this filter in the\nGDFM to represent both spatial and frequency correlations of plausible\nwaveforms. Numerical tests on the combination of GDFM and GAN have demonstrated\nperformance improvements over competing alternatives in synthesizing wind power\nscenarios from Australia, better realizing plausible statistical\ncharacteristics of actual wind power compared to alternatives such as the GDFM\nwith a filter synthesized from distributions of actual dynamic filters and the\nGAN with direct synthesis without dynamic factors.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00692v1", "cate": "cs.LG", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00040", "title": "Regime-Aware Conditional Neural Processes with Multi-Criteria Decision Support for Operational Electricity Price Forecasting", "authors": ["Abhinav Das", "Stephan Schlüter"], "categories": ["cs.LG", "math.PR", "stat.AP", "stat.ML", "60J20, 68T07"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00040v1", "summary": "This work integrates Bayesian regime detection with conditional neural\nprocesses for 24-hour electricity price prediction in the German market. Our\nmethodology integrates regime detection using a disentangled sticky\nhierarchical Dirichlet process hidden Markov model (DS-HDP-HMM) applied to\ndaily electricity prices. Each identified regime is subsequently modeled by an\nindependent conditional neural process (CNP), trained to learn localized\nmappings from input contexts to 24-dimensional hourly price trajectories, with\nfinal predictions computed as regime-weighted mixtures of these CNP outputs. We\nrigorously evaluate R-NP against deep neural networks (DNN) and Lasso estimated\nauto-regressive (LEAR) models by integrating their forecasts into diverse\nbattery storage optimization frameworks, including price arbitrage, risk\nmanagement, grid services, and cost minimization. This operational utility\nassessment revealed complex performance trade-offs: LEAR often yielded superior\nabsolute profits or lower costs, while DNN showed exceptional optimality in\nspecific cost-minimization contexts. Recognizing that raw prediction accuracy\ndoesn't always translate to optimal operational outcomes, we employed TOPSIS as\na comprehensive multi-criteria evaluation layer. Our TOPSIS analysis identified\nLEAR as the top-ranked model for 2021, but crucially, our proposed R-NP model\nemerged as the most balanced and preferred solution for 2021, 2022 and 2023.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00040v1", "cate": "cs.LG", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2508.00155", "title": "GEPAR3D: Geometry Prior-Assisted Learning for 3D Tooth Segmentation", "authors": ["Tomasz Szczepański", "Szymon Płotka", "Michal K. Grzeszczyk", "Arleta Adamowicz", "Piotr Fudalej", "Przemysław Korzeniowski", "Tomasz Trzciński", "Arkadiusz Sitek"], "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "Comments:      Accepted for the 28th International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI) 2025", "url": "http://arxiv.org/abs/2508.00155v1", "summary": "Tooth segmentation in Cone-Beam Computed Tomography (CBCT) remains\nchallenging, especially for fine structures like root apices, which is critical\nfor assessing root resorption in orthodontics. We introduce GEPAR3D, a novel\napproach that unifies instance detection and multi-class segmentation into a\nsingle step tailored to improve root segmentation. Our method integrates a\nStatistical Shape Model of dentition as a geometric prior, capturing anatomical\ncontext and morphological consistency without enforcing restrictive adjacency\nconstraints. We leverage a deep watershed method, modeling each tooth as a\ncontinuous 3D energy basin encoding voxel distances to boundaries. This\ninstance-aware representation ensures accurate segmentation of narrow, complex\nroot apices. Trained on publicly available CBCT scans from a single center, our\nmethod is evaluated on external test sets from two in-house and two public\nmedical centers. GEPAR3D achieves the highest overall segmentation performance,\naveraging a Dice Similarity Coefficient (DSC) of 95.0% (+2.8% over the\nsecond-best method) and increasing recall to 95.2% (+9.5%) across all test\nsets. Qualitative analyses demonstrated substantial improvements in root\nsegmentation quality, indicating significant potential for more accurate root\nresorption assessment and enhanced clinical decision-making in orthodontics. We\nprovide the implementation and dataset at https://github.com/tomek1911/GEPAR3D.", "comment": "Accepted for the 28th International Conference on Medical Image\n  Computing and Computer Assisted Intervention (MICCAI) 2025", "pdf_url": "http://arxiv.org/pdf/2508.00155v1", "cate": "eess.IV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2505.12620", "title": "BusterX: MLLM-Powered AI-Generated Video Forgery Detection and Explanation", "authors": ["Haiquan Wen", "Yiwei He", "Zhenglin Huang", "Tianxiao Li", "Zihan Yu", "Xingru Huang", "Lu Qi", "Baoyuan Wu", "Xiangtai Li", "Guangliang Cheng"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.12620v4", "summary": "Advances in AI generative models facilitate super-realistic video synthesis,\namplifying misinformation risks via social media and eroding trust in digital\ncontent. Several research works have explored new deepfake detection methods on\nAI-generated images to alleviate these risks. However, with the fast\ndevelopment of video generation models, such as Sora and WanX, there is\ncurrently a lack of large-scale, high-quality AI-generated video datasets for\nforgery detection. In addition, existing detection approaches predominantly\ntreat the task as binary classification, lacking explainability in model\ndecision-making and failing to provide actionable insights or guidance for the\npublic. To address these challenges, we propose \\textbf{GenBuster-200K}, a\nlarge-scale AI-generated video dataset featuring 200K high-resolution video\nclips, diverse latest generative techniques, and real-world scenes. We further\nintroduce \\textbf{BusterX}, a novel AI-generated video detection and\nexplanation framework leveraging multimodal large language model (MLLM) and\nreinforcement learning for authenticity determination and explainable\nrationale. To our knowledge, GenBuster-200K is the {\\it \\textbf{first}}\nlarge-scale, high-quality AI-generated video dataset that incorporates the\nlatest generative techniques for real-world scenarios. BusterX is the {\\it\n\\textbf{first}} framework to integrate MLLM with reinforcement learning for\nexplainable AI-generated video detection. Extensive comparisons with\nstate-of-the-art methods and ablation studies validate the effectiveness and\ngeneralizability of BusterX. The code, models, and datasets will be released.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.12620v4", "cate": "cs.CV", "date": "2025-05-19", "updated": "2025-07-31"}
{"id": "2502.04600", "title": "Cooperative Payload Estimation by a Team of Mocobots", "authors": ["Haoxuan Zhang", "C. Lin Liu", "Matthew L. Elwin", "Randy A. Freeman", "Kevin M. Lynch"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      8 pages, 6 figures. Submitted to IEEE Robotics and Automation Letters (RA-L)", "url": "http://arxiv.org/abs/2502.04600v3", "summary": "For high-performance autonomous manipulation of a payload by a mobile\nmanipulator team, or for collaborative manipulation with the human, robots\nshould be able to discover where other robots are attached to the payload, as\nwell as the payload's mass and inertial properties. In this paper, we describe\na method for the robots to autonomously discover this information. The robots\ncooperatively manipulate the payload, and the twist, twist derivative, and\nwrench data at their grasp frames are used to estimate the transformation\nmatrices between the grasp frames, the location of the payload's center of\nmass, and the payload's inertia matrix. The method is validated experimentally\nwith a team of three mobile cobots, or mocobots.", "comment": "8 pages, 6 figures. Submitted to IEEE Robotics and Automation Letters\n  (RA-L)", "pdf_url": "http://arxiv.org/pdf/2502.04600v3", "cate": "cs.RO", "date": "2025-02-07", "updated": "2025-08-01"}
{"id": "2505.10375", "title": "Are Sparse Autoencoders Useful for Java Function Bug Detection?", "authors": ["Rui Melo", "Claudia Mamede", "Andre Catarino", "Rui Abreu", "Henrique Lopes Cardoso"], "categories": ["cs.SE", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      10 pages, 10 figures", "url": "http://arxiv.org/abs/2505.10375v3", "summary": "Software vulnerabilities such as buffer overflows and SQL injections are a\nmajor source of security breaches. Traditional methods for vulnerability\ndetection remain essential but are limited by high false positive rates,\nscalability issues, and reliance on manual effort. These constraints have\ndriven interest in AI-based approaches to automated vulnerability detection and\nsecure code generation. While Large Language Models (LLMs) have opened new\navenues for classification tasks, their complexity and opacity pose challenges\nfor interpretability and deployment. Sparse Autoencoder offer a promising\nsolution to this problem. We explore whether SAEs can serve as a lightweight,\ninterpretable alternative for bug detection in Java functions. We evaluate the\neffectiveness of SAEs when applied to representations from GPT-2 Small and\nGemma 2B, examining their capacity to highlight buggy behaviour without\nfine-tuning the underlying LLMs. We found that SAE-derived features enable bug\ndetection with an F1 score of up to 89%, consistently outperforming fine-tuned\ntransformer encoder baselines. Our work provides the first empirical evidence\nthat SAEs can be used to detect software bugs directly from the internal\nrepresentations of pretrained LLMs, without any fine-tuning or task-specific\nsupervision. Code available at\nhttps://github.com/rufimelo99/SAE-Java-Bug-Detection", "comment": "10 pages, 10 figures", "pdf_url": "http://arxiv.org/pdf/2505.10375v3", "cate": "cs.SE", "date": "2025-05-15", "updated": "2025-07-31"}
{"id": "2508.00804", "title": "Online Fine-Tuning of Carbon Emission Predictions using Real-Time Recurrent Learning for State Space Models", "authors": ["Julian Lemmel", "Manuel Kranzl", "Adam Lamine", "Philipp Neubauer", "Radu Grosu", "Sophie Neubauer"], "categories": ["cs.CE", "cs.LG", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Computational Engineering, Finance, and Science (cs.CE)", "pdf_link": null, "comments": "Comments:      6 pages", "url": "http://arxiv.org/abs/2508.00804v1", "summary": "This paper introduces a new approach for fine-tuning the predictions of\nstructured state space models (SSMs) at inference time using real-time\nrecurrent learning. While SSMs are known for their efficiency and long-range\nmodeling capabilities, they are typically trained offline and remain static\nduring deployment. Our method enables online adaptation by continuously\nupdating model parameters in response to incoming data. We evaluate our\napproach for linear-recurrent-unit SSMs using a small carbon emission dataset\ncollected from embedded automotive hardware. Experimental results show that our\nmethod consistently reduces prediction error online during inference,\ndemonstrating its potential for dynamic, resource-constrained environments.", "comment": "6 pages", "pdf_url": "http://arxiv.org/pdf/2508.00804v1", "cate": "cs.CE", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00041", "title": "Learning Like Humans: Resource-Efficient Federated Fine-Tuning through Cognitive Developmental Stages", "authors": ["Yebo Wu", "Jingguang Li", "Zhijiang Guo", "Li Li"], "categories": ["cs.LG", "cs.AI", "cs.DC"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00041v1", "summary": "Federated fine-tuning enables Large Language Models (LLMs) to adapt to\ndownstream tasks while preserving data privacy, but its resource-intensive\nnature limits deployment on edge devices. In this paper, we introduce\nDevelopmental Federated Tuning (DevFT), a resource-efficient approach inspired\nby cognitive development that progressively builds a powerful LLM from a\ncompact foundation. DevFT decomposes the fine-tuning process into developmental\nstages, each optimizing submodels with increasing parameter capacity. Knowledge\nfrom earlier stages transfers to subsequent submodels, providing optimized\ninitialization parameters that prevent convergence to local minima and\naccelerate training. This paradigm mirrors human learning, gradually\nconstructing comprehensive knowledge structure while refining existing skills.\nTo efficiently build stage-specific submodels, DevFT introduces\ndeconfliction-guided layer grouping and differential-based layer fusion to\ndistill essential information and construct representative layers. Evaluations\nacross multiple benchmarks demonstrate that DevFT significantly outperforms\nstate-of-the-art methods, achieving up to 4.59$\\times$ faster convergence,\n10.67$\\times$ reduction in communication overhead, and 9.07% average\nperformance improvement, while maintaining compatibility with existing\napproaches.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00041v1", "cate": "cs.LG", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2508.00164", "title": "On the Utility of Virtual Staining for Downstream Applications as it relates to Task Network Capacity", "authors": ["Sourya Sengupta", "Jianquan Xu", "Phuong Nguyen", "Frank J. Brooks", "Yang Liu", "Mark A. Anastasio"], "categories": ["eess.IV", "q-bio.QM"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00164v1", "summary": "Virtual staining, or in-silico-labeling, has been proposed to computationally\ngenerate synthetic fluorescence images from label-free images by use of deep\nlearning-based image-to-image translation networks. In most reported studies,\nvirtually stained images have been assessed only using traditional image\nquality measures such as structural similarity or signal-to-noise ratio.\nHowever, in biomedical imaging, images are typically acquired to facilitate an\nimage-based inference, which we refer to as a downstream biological or clinical\ntask. This study systematically investigates the utility of virtual staining\nfor facilitating clinically relevant downstream tasks (like segmentation or\nclassification) with consideration of the capacity of the deep neural networks\nemployed to perform the tasks. Comprehensive empirical evaluations were\nconducted using biological datasets, assessing task performance by use of\nlabel-free, virtually stained, and ground truth fluorescence images. The\nresults demonstrated that the utility of virtual staining is largely dependent\non the ability of the segmentation or classification task network to extract\nmeaningful task-relevant information, which is related to the concept of\nnetwork capacity. Examples are provided in which virtual staining does not\nimprove, or even degrades, segmentation or classification performance when the\ncapacity of the associated task network is sufficiently large. The results\ndemonstrate that task network capacity should be considered when deciding\nwhether to perform virtual staining.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00164v1", "cate": "eess.IV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2505.14729", "title": "Uncovering Cultural Representation Disparities in Vision-Language Models", "authors": ["Ram Mohan Rao Kadiyala", "Siddhant Gupta", "Jebish Purbey", "Srishti Yadav", "Suman Debnath", "Alejandro Salamanca", "Desmond Elliott"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      28 pages, 36 figures", "url": "http://arxiv.org/abs/2505.14729v3", "summary": "Vision-Language Models (VLMs) have demonstrated impressive capabilities\nacross a range of tasks, yet concerns about their potential biases exist. This\nwork investigates the extent to which prominent VLMs exhibit cultural biases by\nevaluating their performance on an image-based country identification task at a\ncountry level. Utilizing the geographically diverse Country211 dataset, we\nprobe several large vision language models (VLMs) under various prompting\nstrategies: open-ended questions, multiple-choice questions (MCQs) including\nchallenging setups like multilingual and adversarial settings. Our analysis\naims to uncover disparities in model accuracy across different countries and\nquestion formats, providing insights into how training data distribution and\nevaluation methodologies might influence cultural biases in VLMs. The findings\nhighlight significant variations in performance, suggesting that while VLMs\npossess considerable visual understanding, they inherit biases from their\npre-training data and scale that impact their ability to generalize uniformly\nacross diverse global contexts.", "comment": "28 pages, 36 figures", "pdf_url": "http://arxiv.org/pdf/2505.14729v3", "cate": "cs.CV", "date": "2025-05-20", "updated": "2025-07-31"}
{"id": "2502.08452", "title": "Learning to Push, Group, and Grasp: A Diffusion Policy Approach for Multi-Object Delivery", "authors": ["Takahiro Yonemaru", "Weiwei Wan", "Tatsuki Nishimura", "Kensuke Harada"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2502.08452v3", "summary": "Simultaneously grasping and delivering multiple objects can significantly\nenhance robotic work efficiency and has been a key research focus for decades.\nThe primary challenge lies in determining how to push objects, group them, and\nexecute simultaneous grasping for respective groups while considering object\ndistribution and the hardware constraints of the robot. Traditional rule-based\nmethods struggle to flexibly adapt to diverse scenarios. To address this\nchallenge, this paper proposes an imitation learning-based approach. We collect\na series of expert demonstrations through teleoperation and train a diffusion\npolicy network, enabling the robot to dynamically generate action sequences for\npushing, grouping, and grasping, thereby facilitating efficient multi-object\ngrasping and delivery. We conducted experiments to evaluate the method under\ndifferent training dataset sizes, varying object quantities, and real-world\nobject scenarios. The results demonstrate that the proposed approach can\neffectively and adaptively generate multi-object grouping and grasping\nstrategies. With the support of more training data, imitation learning is\nexpected to be an effective approach for solving the multi-object grasping\nproblem.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2502.08452v3", "cate": "cs.RO", "date": "2025-02-12", "updated": "2025-08-01"}
{"id": "2506.06509", "title": "Private GPTs for LLM-driven testing in software development and machine learning", "authors": ["Jakub Jagielski", "Consuelo Rojas", "Markus Abel"], "categories": ["cs.SE", "cs.AI", "I.2.1"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      5 pages, 10 figures", "url": "http://arxiv.org/abs/2506.06509v2", "summary": "In this contribution, we examine the capability of private GPTs to\nautomatically generate executable test code based on requirements. More\nspecifically, we use acceptance criteria as input, formulated as part of epics,\nor stories, which are typically used in modern development processes. This\ngives product owners, or business intelligence, respectively, a way to directly\nproduce testable criteria through the use of LLMs. We explore the quality of\nthe so-produced tests in two ways: i) directly by letting the LLM generate code\nfrom requirements, ii) through an intermediate step using Gherkin syntax. As a\nresult, it turns out that the two-step procedure yields better results -where\nwe define better in terms of human readability and best coding practices, i.e.\nlines of code and use of additional libraries typically used in testing.\nConcretely, we evaluate prompt effectiveness across two scenarios: a simple\n\"Hello World\" program and a digit classification model, showing that structured\nprompts lead to higher-quality test outputs.", "comment": "5 pages, 10 figures", "pdf_url": "http://arxiv.org/pdf/2506.06509v2", "cate": "cs.SE", "date": "2025-06-06", "updated": "2025-07-31"}
{"id": "2409.09769", "title": "Risk-Aware Autonomous Driving with Linear Temporal Logic Specifications", "authors": ["Shuhao Qi", "Zengjie Zhang", "Zhiyong Sun", "Sofie Haesaert"], "categories": ["eess.SY", "cs.FL", "cs.RO", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2409.09769v3", "summary": "Human drivers naturally balance the risks of different concerns while\ndriving, including traffic rule violations, minor accidents, and fatalities.\nHowever, achieving the same behavior in autonomous driving systems remains an\nopen problem. This paper extends a risk metric that has been verified in\nhuman-like driving studies to encompass more complex driving scenarios\nspecified by linear temporal logic (LTL) that go beyond just collision risks.\nThis extension incorporates the timing and severity of events into LTL\nspecifications, thereby reflecting a human-like risk awareness. Without\nsacrificing expressivity for traffic rules, we adopt LTL specifications\ncomposed of safety and co-safety formulas, allowing the control synthesis\nproblem to be reformulated as a reachability problem. By leveraging occupation\nmeasures, we further formulate a linear programming (LP) problem for this\nLTL-based risk metric. Consequently, the synthesized policy balances different\ntypes of driving risks, including both collision risks and traffic rule\nviolations. The effectiveness of the proposed approach is validated by three\ntypical traffic scenarios in Carla simulator.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2409.09769v3", "cate": "eess.SY", "date": "2024-09-15", "updated": "2025-07-31"}
{"id": "2508.00043", "title": "Improved Robustness and Functional Localization in Topographic CNNs Through Weight Similarity", "authors": ["Nhut Truong", "Uri Hasson"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00043v1", "summary": "Topographic neural networks are computational models that can simulate the\nspatial and functional organization of the brain. Topographic constraints in\nneural networks can be implemented in multiple ways, with potentially different\nimpacts on the representations learned by the network. The impact of such\ndifferent implementations has not been systematically examined. To this end,\nhere we compare topographic convolutional neural networks trained with two\nspatial constraints: Weight Similarity (WS), which pushes neighboring units to\ndevelop similar incoming weights, and Activation Similarity (AS), which\nenforces similarity in unit activations. We evaluate the resulting models on\nclassification accuracy, robustness to weight perturbations and input\ndegradation, and the spatial organization of learned representations. Compared\nto both AS and standard CNNs, WS provided three main advantages: i) improved\nrobustness to noise, also showing higher accuracy under weight corruption; ii)\ngreater input sensitivity, reflected in higher activation variance; and iii)\nstronger functional localization, with units showing similar activations\npositioned at closer distances. In addition, WS produced differences in\norientation tuning, symmetry sensitivity, and eccentricity profiles of units,\nindicating an influence of this spatial constraint on the representational\ngeometry of the network. Our findings suggest that during end-to-end training,\nWS constraints produce more robust representations than AS or non-topographic\nCNNs. These findings also suggest that weight-based spatial constraints can\nshape feature learning and functional organization in biophysical inspired\nmodels.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00043v1", "cate": "cs.LG", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2508.00235", "title": "Weakly Supervised Intracranial Aneurysm Detection and Segmentation in MR angiography via Multi-task UNet with Vesselness Prior", "authors": ["Erin Rainville", "Amirhossein Rasoulian", "Hassan Rivaz", "Yiming Xiao"], "categories": ["eess.IV", "cs.AI", "cs.CV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "Comments:      Accepted to ICCV 2025 Workshop CVAMD", "url": "http://arxiv.org/abs/2508.00235v1", "summary": "Intracranial aneurysms (IAs) are abnormal dilations of cerebral blood vessels\nthat, if ruptured, can lead to life-threatening consequences. However, their\nsmall size and soft contrast in radiological scans often make it difficult to\nperform accurate and efficient detection and morphological analyses, which are\ncritical in the clinical care of the disorder. Furthermore, the lack of large\npublic datasets with voxel-wise expert annotations pose challenges for\ndeveloping deep learning algorithms to address the issues. Therefore, we\nproposed a novel weakly supervised 3D multi-task UNet that integrates\nvesselness priors to jointly perform aneurysm detection and segmentation in\ntime-of-flight MR angiography (TOF-MRA). Specifically, to robustly guide IA\ndetection and segmentation, we employ the popular Frangi's vesselness filter to\nderive soft cerebrovascular priors for both network input and an attention\nblock to conduct segmentation from the decoder and detection from an auxiliary\nbranch. We train our model on the Lausanne dataset with coarse ground truth\nsegmentation, and evaluate it on the test set with refined labels from the same\ndatabase. To further assess our model's generalizability, we also validate it\nexternally on the ADAM dataset. Our results demonstrate the superior\nperformance of the proposed technique over the SOTA techniques for aneurysm\nsegmentation (Dice = 0.614, 95%HD =1.38mm) and detection (false positive rate =\n1.47, sensitivity = 92.9%).", "comment": "Accepted to ICCV 2025 Workshop CVAMD", "pdf_url": "http://arxiv.org/pdf/2508.00235v1", "cate": "eess.IV", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2505.20884", "title": "YOLO-FireAD: Efficient Fire Detection via Attention-Guided Inverted Residual Learning and Dual-Pooling Feature Preservation", "authors": ["Weichao Pan", "Bohan Xu", "Xu Wang", "Chengze Lv", "Shuoyang Wang", "Zhenke Duan", "Zhen Tian"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      2025 International Conference on Intelligent Computing (ICIC 2025)", "url": "http://arxiv.org/abs/2505.20884v3", "summary": "Fire detection in dynamic environments faces continuous challenges, including\nthe interference of illumination changes, many false detections or missed\ndetections, and it is difficult to achieve both efficiency and accuracy. To\naddress the problem of feature extraction limitation and information loss in\nthe existing YOLO-based models, this study propose You Only Look Once for Fire\nDetection with Attention-guided Inverted Residual and Dual-pooling Downscale\nFusion (YOLO-FireAD) with two core innovations: (1) Attention-guided Inverted\nResidual Block (AIR) integrates hybrid channel-spatial attention with inverted\nresiduals to adaptively enhance fire features and suppress environmental noise;\n(2) Dual Pool Downscale Fusion Block (DPDF) preserves multi-scale fire patterns\nthrough learnable fusion of max-average pooling outputs, mitigating small-fire\ndetection failures. Extensive evaluation on two public datasets shows the\nefficient performance of our model. Our proposed model keeps the sum amount of\nparameters (1.45M, 51.8% lower than YOLOv8n) (4.6G, 43.2% lower than YOLOv8n),\nand mAP75 is higher than the mainstream real-time object detection models\nYOLOv8n, YOL-Ov9t, YOLOv10n, YOLO11n, YOLOv12n and other YOLOv8 variants\n1.3-5.5%. For more details, please visit our repository:\nhttps://github.com/JEFfersusu/YOLO-FireAD", "comment": "2025 International Conference on Intelligent Computing (ICIC 2025)", "pdf_url": "http://arxiv.org/pdf/2505.20884v3", "cate": "cs.CV", "date": "2025-05-27", "updated": "2025-08-01"}
{"id": "2503.02198", "title": "FalconGym: A Photorealistic Simulation Framework for Zero-Shot Sim-to-Real Vision-Based Quadrotor Navigation", "authors": ["Yan Miao", "Will Shen", "Sayan Mitra"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      Accepted in IROS 2025", "url": "http://arxiv.org/abs/2503.02198v2", "summary": "We present a novel framework demonstrating zero-shot sim-to-real transfer of\nvisual control policies learned in a Neural Radiance Field (NeRF) environment\nfor quadrotors to fly through racing gates. Robust transfer from simulation to\nreal flight poses a major challenge, as standard simulators often lack\nsufficient visual fidelity. To address this, we construct a photorealistic\nsimulation environment of quadrotor racing tracks, called FalconGym, which\nprovides effectively unlimited synthetic images for training. Within FalconGym,\nwe develop a pipelined approach for crossing gates that combines (i) a Neural\nPose Estimator (NPE) coupled with a Kalman filter to reliably infer quadrotor\nposes from single-frame RGB images and IMU data, and (ii) a\nself-attention-based multi-modal controller that adaptively integrates visual\nfeatures and pose estimation. This multi-modal design compensates for\nperception noise and intermittent gate visibility. We train this controller\npurely in FalconGym with imitation learning and deploy the resulting policy to\nreal hardware with no additional fine-tuning. Simulation experiments on three\ndistinct tracks (circle, U-turn and figure-8) demonstrate that our controller\noutperforms a vision-only state-of-the-art baseline in both success rate and\ngate-crossing accuracy. In 30 live hardware flights spanning three tracks and\n120 gates, our controller achieves a 95.8% success rate and an average error of\njust 10 cm when flying through 38 cm-radius gates.", "comment": "Accepted in IROS 2025", "pdf_url": "http://arxiv.org/pdf/2503.02198v2", "cate": "cs.RO", "date": "2025-03-04", "updated": "2025-08-01"}
{"id": "2506.15884", "title": "How Do Community Smells Influence Self-Admitted Technical Debt in Machine Learning Projects?", "authors": ["Shamse Tasnim Cynthia", "Nuri Almarimi", "Banani Roy"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.15884v2", "summary": "Community smells reflect poor organizational practices that often lead to\nsocio-technical issues and the accumulation of Self-Admitted Technical Debt\n(SATD). While prior studies have explored these problems in general software\nsystems, their interplay in machine learning (ML)-based projects remains\nlargely underexamined. In this study, we investigated the prevalence of\ncommunity smells and their relationship with SATD in open-source ML projects,\nanalyzing data at the release level. First, we examined the prevalence of ten\ncommunity smell types across the releases of 155 ML-based systems and found\nthat community smells are widespread, exhibiting distinct distribution patterns\nacross small, medium, and large projects. Second, we detected SATD at the\nrelease level and applied statistical analysis to examine its correlation with\ncommunity smells. Our results showed that certain smells, such as Radio Silence\nand Organizational Silos, are strongly correlated with higher SATD occurrences.\nThird, we considered the six identified types of SATD to determine which\ncommunity smells are most associated with each debt category. Our analysis\nrevealed authority- and communication-related smells often co-occur with\npersistent code and design debt. Finally, we analyzed how the community smells\nand SATD evolve over the releases, uncovering project size-dependent trends and\nshared trajectories. Our findings emphasize the importance of early detection\nand mitigation of socio-technical issues to maintain the long-term quality and\nsustainability of ML-based systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.15884v2", "cate": "cs.SE", "date": "2025-06-18", "updated": "2025-07-31"}
{"id": "2504.04312", "title": "Prescribed-Time Boresight Control of Spacecraft Under Pointing Constraints", "authors": ["Xiaodong Shao", "Haoyang Yang", "Haoran Li", "Zongyu Zuo", "Jose Guadalupe Romero", "Qinglei Hu"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.04312v4", "summary": "This article proposes an integrated boresight guidance and control (IBGC)\nscheme to address the boresight reorientation problem of spacecraft under\ntemporal and pointing constraints. A $C^1$ continuous, saturated\nprescribed-time adjustment (PPTA) function is presented, along with the\nestablishment of a practical prescribed-time stability criterion. Utilizing the\ntime scale transformation technique and the PPTA function, we propose a\nprescribed-time guidance law that guides the boresight vector from almost any\ninitial orientation in free space to a small neighborhood of the goal\norientation within a preassigned time, while avoiding all forbidden zones\naugmented with safety margins. Subsequently, a prescribed-time disturbance\nobserver (PTDO) is derived to reconstruct the external disturbances. By\nleveraging barrier and PPTA functions, a PTDO-based reduced-attitude tracking\ncontroller is developed, which ensures prescribed-time boresight tracking\nwithin a ``safe tube''. By judiciously setting the safety margins, settling\ntimes, and safe tube for the guidance and control laws, the proposed IBGC\nscheme achieves pointing-constrained boresight reorientation within a required\ntask completion time. Simulation and experimental results demonstrate the\nefficacy of the proposed IBGC scheme.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.04312v4", "cate": "eess.SY", "date": "2025-04-06", "updated": "2025-08-01"}
{"id": "2508.00046", "title": "Benchmarking Partial Observability in Reinforcement Learning with a Suite of Memory-Improvable Domains", "authors": ["Ruo Yu Tao", "Kaicheng Guo", "Cameron Allen", "George Konidaris"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      To appear at RLC 2025. 1 cover page, 10 pages, 3 reference pages + 13 pages for supplementary material", "url": "http://arxiv.org/abs/2508.00046v1", "summary": "Mitigating partial observability is a necessary but challenging task for\ngeneral reinforcement learning algorithms. To improve an algorithm's ability to\nmitigate partial observability, researchers need comprehensive benchmarks to\ngauge progress. Most algorithms tackling partial observability are only\nevaluated on benchmarks with simple forms of state aliasing, such as feature\nmasking and Gaussian noise. Such benchmarks do not represent the many forms of\npartial observability seen in real domains, like visual occlusion or unknown\nopponent intent. We argue that a partially observable benchmark should have two\nkey properties. The first is coverage in its forms of partial observability, to\nensure an algorithm's generalizability. The second is a large gap between the\nperformance of a agents with more or less state information, all other factors\nroughly equal. This gap implies that an environment is memory improvable: where\nperformance gains in a domain are from an algorithm's ability to cope with\npartial observability as opposed to other factors. We introduce best-practice\nguidelines for empirically benchmarking reinforcement learning under partial\nobservability, as well as the open-source library POBAX: Partially Observable\nBenchmarks in JAX. We characterize the types of partial observability present\nin various environments and select representative environments for our\nbenchmark. These environments include localization and mapping, visual control,\ngames, and more. Additionally, we show that these tasks are all memory\nimprovable and require hard-to-learn memory functions, providing a concrete\nsignal for partial observability research. This framework includes recommended\nhyperparameters as well as algorithm implementations for fast, out-of-the-box\nevaluation, as well as highly performant environments implemented in JAX for\nGPU-scalable experimentation.", "comment": "To appear at RLC 2025. 1 cover page, 10 pages, 3 reference pages + 13\n  pages for supplementary material", "pdf_url": "http://arxiv.org/pdf/2508.00046v1", "cate": "cs.LG", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2508.00438", "title": "Diffusion-Based User-Guided Data Augmentation for Coronary Stenosis Detection", "authors": ["Sumin Seo", "In Kyu Lee", "Hyun-Woo Kim", "Jaesik Min", "Chung-Hwan Jung"], "categories": ["eess.IV", "cs.CV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "Comments:      Accepted at MICCAI 2025. Dataset available at this https URL", "url": "http://arxiv.org/abs/2508.00438v1", "summary": "Coronary stenosis is a major risk factor for ischemic heart events leading to\nincreased mortality, and medical treatments for this condition require\nmeticulous, labor-intensive analysis. Coronary angiography provides critical\nvisual cues for assessing stenosis, supporting clinicians in making informed\ndecisions for diagnosis and treatment. Recent advances in deep learning have\nshown great potential for automated localization and severity measurement of\nstenosis. In real-world scenarios, however, the success of these competent\napproaches is often hindered by challenges such as limited labeled data and\nclass imbalance. In this study, we propose a novel data augmentation approach\nthat uses an inpainting method based on a diffusion model to generate realistic\nlesions, allowing user-guided control of severity. Extensive evaluation on\nlesion detection and severity classification across various synthetic dataset\nsizes shows superior performance of our method on both a large-scale in-house\ndataset and a public coronary angiography dataset. Furthermore, our approach\nmaintains high detection and classification performance even when trained with\nlimited data, highlighting its clinical importance in improving the assessment\nof severity of stenosis and optimizing data utilization for more reliable\ndecision support.", "comment": "Accepted at MICCAI 2025. Dataset available at\n  https://github.com/medipixel/DiGDA", "pdf_url": "http://arxiv.org/pdf/2508.00438v1", "cate": "eess.IV", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00093", "title": "Closed-form Expression for the Power Profile in Wideband Systems with Inter-channel Stimulated Raman Scattering", "authors": ["Lucas Alves Zischler", "Chiara Lasagni", "Paolo Serena", "Alberto Bononi", "Giammarco Di Sciullo", "Divya A. Shaji", "Antonio Mecozzi", "Cristian Antonelli"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "Comments:      Submitted for the Journal of Lightwave Technology", "url": "http://arxiv.org/abs/2508.00093v1", "summary": "Wideband systems experience significant inter-channel stimulated Raman\nscattering (ISRS) and channel-dependent losses. Due to the non-uniform\nattenuation profile, the combined effects of ISRS and fiber loss can only be\naccurately estimated using numerical methods. In this work, we present an\napproximate closed-form expression for the channels' power profile accounting\nfor these combined effects. We validate the proposed expression against\nnumerical solutions in the case of CLU transmission, showing high accuracy for\nboth single-span and multi-span fiber-optic links. Additionally, we derive an\ninverse expression, formulated as a function of the output power, which can be\nutilized to target a desired optical signal-to-noise ratio (OSNR) profile\nthrough pre-emphasis of the launched channel powers.", "comment": "Submitted for the Journal of Lightwave Technology", "pdf_url": "http://arxiv.org/pdf/2508.00093v1", "cate": "eess.SP", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2505.21567", "title": "EaqVLA: Encoding-aligned Quantization for Vision-Language-Action Models", "authors": ["Feng Jiang", "Zihao Zheng", "Xiuping Cui", "Maoliang Li", "JIayu Chen", "Xiang Chen"], "categories": ["cs.CV", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      There is an error in this paper, and as the author, I request retraction", "url": "http://arxiv.org/abs/2505.21567v2", "summary": "With the development of Embodied Artificial intelligence, the end-to-end\ncontrol policy such as Vision-Language-Action (VLA) model has become the\nmainstream. Existing VLA models faces expensive computing/storage cost, which\nneed to be optimized. Quantization is considered as the most effective method\nwhich can not only reduce the memory cost but also achieve computation\nacceleration. However, we find the token alignment of VLA models hinders the\napplication of existing quantization methods. To address this, we proposed an\noptimized framework called EaqVLA, which apply encoding-aligned quantization to\nVLA models. Specifically, we propose an complete analysis method to find the\nmisalignment in various granularity. Based on the analysis results, we propose\na mixed precision quantization with the awareness of encoding alignment.\nExperiments shows that the porposed EaqVLA achieves better quantization\nperformance (with the minimal quantization loss for end-to-end action control\nand xxx times acceleration) than existing quantization methods.", "comment": "There is an error in this paper, and as the author, I request\n  retraction", "pdf_url": "http://arxiv.org/pdf/2505.21567v2", "cate": "cs.CV", "date": "2025-05-27", "updated": "2025-07-31"}
{"id": "2503.03254", "title": "SCORE: Saturated Consensus Relocalization in Semantic Line Maps", "authors": ["Haodong Jiang", "Xiang Zheng", "Yanglin Zhang", "Qingcheng Zeng", "Yiqian Li", "Ziyang Hong", "Junfeng Wu"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      12 pages, 13 figurs, arxiv version for paper published at IROS 2025", "url": "http://arxiv.org/abs/2503.03254v2", "summary": "We present SCORE, a visual relocalization system that achieves unprecedented\nmap compactness by adopting semantically labeled 3D line maps. SCORE requires\nonly 0.01\\%-0.1\\% of the storage needed by structure-based or learning-based\nbaselines, while maintaining practical accuracy and comparable runtime. The key\ninnovation is a novel robust estimation mechanism, Saturated Consensus\nMaximization (Sat-CM), which generalizes classical Consensus Maximization (CM)\nby assigning diminishing weights to inlier associations according to maximum\nlikelihood with probabilistic justification. Under extreme outlier ratios (up\nto 99.5\\%) arising from one-to-many ambiguity in semantic matching, Sat-CM\nenables accurate estimation when CM fails. To ensure computational efficiency,\nwe propose an accelerating framework for globally solving Sat-CM formulations\nand specialize it for the Perspective-n-Lines problem at the core of SCORE.", "comment": "12 pages, 13 figurs, arxiv version for paper published at IROS 2025", "pdf_url": "http://arxiv.org/pdf/2503.03254v2", "cate": "cs.RO", "date": "2025-03-05", "updated": "2025-08-01"}
{"id": "2507.17049", "title": "Evaluating Uncertainty and Quality of Visual Language Action-enabled Robots", "authors": ["Pablo Valle", "Chengjie Lu", "Shaukat Ali", "Aitor Arrieta"], "categories": ["cs.SE", "cs.RO"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17049v2", "summary": "Visual Language Action (VLA) models are a multi-modal class of Artificial\nIntelligence (AI) systems that integrate visual perception, natural language\nunderstanding, and action planning to enable agents to interpret their\nenvironment, comprehend instructions, and perform embodied tasks autonomously.\nRecently, significant progress has been made to advance this field. These kinds\nof models are typically evaluated through task success rates, which fail to\ncapture the quality of task execution and the mode's confidence in its\ndecisions. In this paper, we propose eight uncertainty metrics and five quality\nmetrics specifically designed for VLA models for robotic manipulation tasks. We\nassess their effectiveness through a large-scale empirical study involving 908\nsuccessful task executions from three state-of-the-art VLA models across four\nrepresentative robotic manipulation tasks. Human domain experts manually\nlabeled task quality, allowing us to analyze the correlation between our\nproposed metrics and expert judgments. The results reveal that several metrics\nshow moderate to strong correlation with human assessments, highlighting their\nutility for evaluating task quality and model confidence. Furthermore, we found\nthat some of the metrics can discriminate between high-, medium-, and\nlow-quality executions from unsuccessful tasks, which can be interesting when\ntest oracles are not available. Our findings challenge the adequacy of current\nevaluation practices that rely solely on binary success rates and pave the way\nfor improved real-time monitoring and adaptive enhancement of VLA-enabled\nrobotic systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17049v2", "cate": "cs.SE", "date": "2025-07-22", "updated": "2025-07-31"}
{"id": "2411.13951", "title": "PATH: A Discrete-sequence Dataset for Evaluating Online Unsupervised Anomaly Detection Approaches for Multivariate Time Series", "authors": ["Lucas Correia", "Jan-Christoph Goos", "Thomas Bäck", "Anna V. Kononova"], "categories": ["cs.LG", "cs.AI", "cs.CE", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Submitted to the Big Data Research journal", "url": "http://arxiv.org/abs/2411.13951v5", "summary": "Benchmarking anomaly detection approaches for multivariate time series is a\nchallenging task due to a lack of high-quality datasets. Current publicly\navailable datasets are too small, not diverse and feature trivial anomalies,\nwhich hinders measurable progress in this research area. We propose a solution:\na diverse, extensive, and non-trivial dataset generated via state-of-the-art\nsimulation tools that reflects realistic behaviour of an automotive powertrain,\nincluding its multivariate, dynamic and variable-state properties.\nAdditionally, our dataset represents a discrete-sequence problem, which remains\nunaddressed by previously-proposed solutions in literature. To cater for both\nunsupervised and semi-supervised anomaly detection settings, as well as time\nseries generation and forecasting, we make different versions of the dataset\navailable, where training and test subsets are offered in contaminated and\nclean versions, depending on the task. We also provide baseline results from a\nselection of approaches based on deterministic and variational autoencoders, as\nwell as a non-parametric approach. As expected, the baseline experimentation\nshows that the approaches trained on the semi-supervised version of the dataset\noutperform their unsupervised counterparts, highlighting a need for approaches\nmore robust to contaminated training data. Furthermore, results show that the\nthreshold used can have a large influence on detection performance, hence more\nwork needs to be invested in methods to find a suitable threshold without the\nneed for labelled data.", "comment": "Submitted to the Big Data Research journal", "pdf_url": "http://arxiv.org/pdf/2411.13951v5", "cate": "cs.LG", "date": "2024-11-21", "updated": "2025-07-31"}
{"id": "2508.00047", "title": "TriP-LLM: A Tri-Branch Patch-wise Large Language Model Framework for Time-Series Anomaly Detection", "authors": ["Yuan-Cheng Yu", "Yen-Chieh Ouyang", "Chun-An Lin"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      11 pages, 2 figures", "url": "http://arxiv.org/abs/2508.00047v1", "summary": "Time-series anomaly detection plays a central role across a wide range of\napplication domains. With the increasing proliferation of the Internet of\nThings (IoT) and smart manufacturing, time-series data has dramatically\nincreased in both scale and dimensionality. This growth has exposed the\nlimitations of traditional statistical methods in handling the high\nheterogeneity and complexity of such data. Inspired by the recent success of\nlarge language models (LLMs) in multimodal tasks across language and vision\ndomains, we propose a novel unsupervised anomaly detection framework: A\nTri-Branch Patch-wise Large Language Model Framework for Time-Series Anomaly\nDetection (TriP-LLM). TriP-LLM integrates local and global temporal features\nthrough a tri-branch design-Patching, Selection, and Global-to encode the input\ntime series into patch-wise tokens, which are then processed by a frozen,\npretrained LLM. A lightweight patch-wise decoder reconstructs the input, from\nwhich anomaly scores are derived. We evaluate TriP-LLM on several public\nbenchmark datasets using PATE, a recently proposed threshold-free evaluation\nmetric, and conduct all comparisons within a unified open-source framework to\nensure fairness. Experimental results show that TriP-LLM consistently\noutperforms recent state-of-the-art methods across all datasets, demonstrating\nstrong detection capabilities. Furthermore, through extensive ablation studies,\nwe verify the substantial contribution of the LLM to the overall architecture.\nCompared to LLM-based approaches using Channel Independence (CI) patch\nprocessing, TriP-LLM achieves significantly lower memory consumption, making it\nmore suitable for GPU memory-constrained environments. All code and model\ncheckpoints are publicly available on https://github.com/YYZStart/TriP-LLM.git", "comment": "11 pages, 2 figures", "pdf_url": "http://arxiv.org/pdf/2508.00047v1", "cate": "cs.LG", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2508.00721", "title": "FMPlug: Plug-In Foundation Flow-Matching Priors for Inverse Problems", "authors": ["Yuxiang Wan", "Ryan Devera", "Wenjie Zhang", "Ju Sun"], "categories": ["eess.IV", "cs.CV", "cs.LG", "eess.SP"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00721v1", "summary": "We present FMPlug, a novel plug-in framework that enhances foundation\nflow-matching (FM) priors for solving ill-posed inverse problems. Unlike\ntraditional approaches that rely on domain-specific or untrained priors, FMPlug\nsmartly leverages two simple but powerful insights: the similarity between\nobserved and desired objects and the Gaussianity of generative flows. By\nintroducing a time-adaptive warm-up strategy and sharp Gaussianity\nregularization, FMPlug unlocks the true potential of domain-agnostic foundation\nmodels. Our method beats state-of-the-art methods that use foundation FM priors\nby significant margins, on image super-resolution and Gaussian deblurring.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00721v1", "cate": "eess.IV", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00274", "title": "RIS-MAE: A Self-Supervised Modulation Classification Method Based on Raw IQ Signals and Masked Autoencoder", "authors": ["Yunfei Liu", "Mingxuan Liu", "Wupeng Xie", "Xinzhu Liu", "Wenxue Liu", "Yangang Sun", "Xin Qiu", "Cui Yuan", "Jinhai Li"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00274v1", "summary": "Automatic modulation classification (AMC) is a basic technology in\nintelligent wireless communication systems. It is important for tasks such as\nspectrum monitoring, cognitive radio, and secure communications. In recent\nyears, deep learning methods have made great progress in AMC. However,\nmainstream methods still face two key problems. First, they often use\ntime-frequency images instead of raw signals. This causes loss of key\nmodulation features and reduces adaptability to different communication\nconditions. Second, most methods rely on supervised learning. This needs a\nlarge amount of labeled data, which is hard to get in real-world environments.\nTo solve these problems, we propose a self-supervised learning framework called\nRIS-MAE. RIS-MAE uses masked autoencoders to learn signal features from\nunlabeled data. It takes raw IQ sequences as input. By applying random masking\nand reconstruction, it captures important time-domain features such as\namplitude, phase, etc. This helps the model learn useful and transferable\nrepresentations. RIS-MAE is tested on four datasets. The results show that it\nperforms better than existing methods in few-shot and cross-domain tasks.\nNotably, it achieves high classification accuracy on previously unseen datasets\nwith only a small number of fine-tuning samples, confirming its generalization\nability and potential for real-world deployment.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00274v1", "cate": "eess.SP", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2505.24329", "title": "DisTime: Distribution-based Time Representation for Video Large Language Models", "authors": ["Yingsen Zeng", "Zepeng Huang", "Yujie Zhong", "Chengjian Feng", "Jie Hu", "Lin Ma", "Yang Liu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ICCV 2025", "url": "http://arxiv.org/abs/2505.24329v2", "summary": "Despite advances in general video understanding, Video Large Language Models\n(Video-LLMs) face challenges in precise temporal localization due to discrete\ntime representations and limited temporally aware datasets. Existing methods\nfor temporal expression either conflate time with text-based numerical values,\nadd a series of dedicated temporal tokens, or regress time using specialized\ntemporal grounding heads. To address these issues, we introduce DisTime, a\nlightweight framework designed to enhance temporal comprehension in Video-LLMs.\nDisTime employs a learnable token to create a continuous temporal embedding\nspace and incorporates a Distribution-based Time Decoder that generates\ntemporal probability distributions, effectively mitigating boundary ambiguities\nand maintaining temporal continuity. Additionally, the Distribution-based Time\nEncoder re-encodes timestamps to provide time markers for Video-LLMs. To\novercome temporal granularity limitations in existing datasets, we propose an\nautomated annotation paradigm that combines the captioning capabilities of\nVideo-LLMs with the localization expertise of dedicated temporal models. This\nleads to the creation of InternVid-TG, a substantial dataset with 1.25M\ntemporally grounded events across 179k videos, surpassing ActivityNet-Caption\nby 55 times. Extensive experiments demonstrate that DisTime achieves\nstate-of-the-art performance across benchmarks in three time-sensitive tasks\nwhile maintaining competitive performance in Video QA tasks. Code and data are\nreleased at https://github.com/josephzpng/DisTime.", "comment": "Accepted by ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2505.24329v2", "cate": "cs.CV", "date": "2025-05-30", "updated": "2025-07-31"}
{"id": "2503.07504", "title": "PIPE Planner: Pathwise Information Gain with Map Predictions for Indoor Robot Exploration", "authors": ["Seungjae Baek", "Brady Moon", "Seungchan Kim", "Muqing Cao", "Cherie Ho", "Sebastian Scherer", "Jeong hwan Jeon"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      8 pages, 8 figures, IROS 2025", "url": "http://arxiv.org/abs/2503.07504v2", "summary": "Autonomous exploration in unknown environments requires estimating the\ninformation gain of an action to guide planning decisions. While prior\napproaches often compute information gain at discrete waypoints, pathwise\nintegration offers a more comprehensive estimation but is often computationally\nchallenging or infeasible and prone to overestimation. In this work, we propose\nthe Pathwise Information Gain with Map Prediction for Exploration (PIPE)\nplanner, which integrates cumulative sensor coverage along planned trajectories\nwhile leveraging map prediction to mitigate overestimation. To enable efficient\npathwise coverage computation, we introduce a method to efficiently calculate\nthe expected observation mask along the planned path, significantly reducing\ncomputational overhead. We validate PIPE on real-world floorplan datasets,\ndemonstrating its superior performance over state-of-the-art baselines. Our\nresults highlight the benefits of integrating predictive mapping with pathwise\ninformation gain for efficient and informed exploration. Website:\nhttps://pipe-planner.github.io", "comment": "8 pages, 8 figures, IROS 2025", "pdf_url": "http://arxiv.org/pdf/2503.07504v2", "cate": "cs.RO", "date": "2025-03-10", "updated": "2025-08-01"}
{"id": "2507.18130", "title": "NoCode-bench: A Benchmark for Evaluating Natural Language-Driven Feature Addition", "authors": ["Le Deng", "Zhonghao Jiang", "Jialun Cao", "Michael Pradel", "Zhongxin Liu"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18130v2", "summary": "Natural language-driven no-code development allows users to specify software\nfunctionality using natural language (NL) instead of editing source code,\npromising increased productivity and democratized development. Large language\nmodels (LLMs) show potential in enabling this paradigm. In this context,\nsoftware documentation acts as an NL specification for functionality. This work\nintroduces NoCode-bench, a benchmark designed to evaluate LLMs on real-world\nNL-driven feature addition tasks, consisting of 634 tasks across 10 projects\nand 114k code changes. Each task pairs documentation updates with corresponding\ncode implementations, validated by developer-written test cases. A subset of\n114 high-quality, human-verified instances, NoCode-bench Verified, ensures\nreliable evaluation. Our experiments reveal that, despite high token usage, the\nbest LLMs achieve a task success rate of only 15.79%, highlighting challenges\nin cross-file editing, codebase understanding, and tool calling. These findings\nindicate that LLMs are not yet ready for fully NL-driven no-code development.\nNoCode-bench lays the foundation for future advances in this area.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18130v2", "cate": "cs.SE", "date": "2025-07-24", "updated": "2025-08-01"}
{"id": "2504.00244", "title": "System Identification from Partial Observations under Adversarial Attacks", "authors": ["Jihun Kim", "Javad Lavaei"], "categories": ["math.OC", "cs.SY", "eess.SY", "93B15, 93B30, 93C05"], "primary_category": "Subjects:       Optimization and Control (math.OC)", "pdf_link": null, "comments": "Comments:      8 pages, 3 figures", "url": "http://arxiv.org/abs/2504.00244v2", "summary": "This paper is concerned with the partially observed linear system\nidentification, where the goal is to obtain reasonably accurate estimation of\nthe balanced truncation of the true system up to order $k$ from output\nmeasurements. We consider the challenging case of system identification under\nadversarial attacks, where the probability of having an attack at each time is\n$\\Theta(1/k)$ while the value of the attack is arbitrary. We first show that\nthe $\\ell_1$-norm estimator exactly identifies the true Markov parameter matrix\nfor nilpotent systems under any type of attack. We then build on this result to\nextend it to general systems and show that the estimation error exponentially\ndecays as $k$ grows. The estimated balanced truncation model accordingly shows\nan exponentially decaying error for the identification of the true system up to\na similarity transformation. This work is the first to provide the input-output\nanalysis of the system with partial observations under arbitrary attacks.", "comment": "8 pages, 3 figures", "pdf_url": "http://arxiv.org/pdf/2504.00244v2", "cate": "math.OC", "date": "2025-03-31", "updated": "2025-08-01"}
{"id": "2508.00078", "title": "Evaluating COVID 19 Feature Contributions to Bitcoin Return Forecasting: Methodology Based on LightGBM and Genetic Optimization", "authors": ["Imen Mahmoud", "Andrei Velichko"], "categories": ["cs.LG", "cs.AI", "econ.GN", "q-fin.EC"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      22 pages, 5 figures", "url": "http://arxiv.org/abs/2508.00078v1", "summary": "This study proposes a novel methodological framework integrating a LightGBM\nregression model and genetic algorithm (GA) optimization to systematically\nevaluate the contribution of COVID-19-related indicators to Bitcoin return\nprediction. The primary objective was not merely to forecast Bitcoin returns\nbut rather to determine whether including pandemic-related health data\nsignificantly enhances prediction accuracy. A comprehensive dataset comprising\ndaily Bitcoin returns and COVID-19 metrics (vaccination rates,\nhospitalizations, testing statistics) was constructed. Predictive models,\ntrained with and without COVID-19 features, were optimized using GA over 31\nindependent runs, allowing robust statistical assessment. Performance metrics\n(R2, RMSE, MAE) were statistically compared through distribution overlaps and\nMann-Whitney U tests. Permutation Feature Importance (PFI) analysis quantified\nindividual feature contributions. Results indicate that COVID-19 indicators\nsignificantly improved model performance, particularly in capturing extreme\nmarket fluctuations (R2 increased by 40%, RMSE decreased by 2%, both highly\nsignificant statistically). Among COVID-19 features, vaccination metrics,\nespecially the 75th percentile of fully vaccinated individuals, emerged as\ndominant predictors. The proposed methodology extends existing financial\nanalytics tools by incorporating public health signals, providing investors and\npolicymakers with refined indicators to navigate market uncertainty during\nsystemic crises.", "comment": "22 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2508.00078v1", "cate": "cs.LG", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2508.00755", "title": "AI-Driven Collaborative Satellite Object Detection for Space Sustainability", "authors": ["Peng Hu", "Wenxuan Zhang"], "categories": ["eess.IV", "cs.CV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "Comments:      Submitted to the 13th Annual IEEE International Conference on Wireless for Space and Extreme Environments (WiSEE 2025)", "url": "http://arxiv.org/abs/2508.00755v1", "summary": "The growing density of satellites in low-Earth orbit (LEO) presents serious\nchallenges to space sustainability, primarily due to the increased risk of\nin-orbit collisions. Traditional ground-based tracking systems are constrained\nby latency and coverage limitations, underscoring the need for onboard,\nvision-based space object detection (SOD) capabilities. In this paper, we\npropose a novel satellite clustering framework that enables the collaborative\nexecution of deep learning (DL)-based SOD tasks across multiple satellites. To\nsupport this approach, we construct a high-fidelity dataset simulating imaging\nscenarios for clustered satellite formations. A distance-aware viewpoint\nselection strategy is introduced to optimize detection performance, and recent\nDL models are used for evaluation. Experimental results show that the\nclustering-based method achieves competitive detection accuracy compared to\nsingle-satellite and existing approaches, while maintaining a low size, weight,\nand power (SWaP) footprint. These findings underscore the potential of\ndistributed, AI-enabled in-orbit systems to enhance space situational awareness\nand contribute to long-term space sustainability.", "comment": "Submitted to the 13th Annual IEEE International Conference on\n  Wireless for Space and Extreme Environments (WiSEE 2025)", "pdf_url": "http://arxiv.org/pdf/2508.00755v1", "cate": "eess.IV", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00326", "title": "Model-Driven Deep Learning Enhanced Joint Beamforming and Mode Switching for RDARS-Aided MIMO Systems", "authors": ["Chengwang Ji", "Kehui Li", "Haiquan Lu", "Qiaoyan Peng", "Jintao Wang", "Shaodan Ma"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00326v1", "summary": "Reconfigurable distributed antenna and reflecting surface (RDARS) is a\npromising architecture for future sixth-generation (6G) wireless networks. In\nparticular, the dynamic working mode configuration for the RDARS-aided system\nbrings an extra selection gain compared to the existing reconfigurable\nintelligent surface (RIS)-aided system and distributed antenna system (DAS). In\nthis paper, we consider the RDARS-aided downlink multiple-input multiple-output\n(MIMO) system and aim to maximize the weighted sum rate (WSR) by jointly\noptimizing the beamforming matrices at the based station (BS) and RDARS, as\nwell as mode switching matrix at RDARS. The optimization problem is challenging\nto be solved due to the non-convex objective function and mixed integer binary\nconstraint. To this end, a penalty term-based weight minimum mean square error\n(PWM) algorithm is proposed by integrating the majorization-minimization (MM)\nand weight minimum mean square error (WMMSE) methods. To further escape the\nlocal optimum point in the PWM algorithm, a model-driven DL method is\nintegrated into this algorithm, where the key variables related to the\nconvergence of PWM algorithm are trained to accelerate the convergence speed\nand improve the system performance. Simulation results are provided to show\nthat the PWM-based beamforming network (PWM-BFNet) can reduce the number of\niterations by half and achieve performance improvements of 26.53% and 103.2% at\nthe scenarios of high total transmit power and a large number of RDARS transmit\nelements (TEs), respectively.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00326v1", "cate": "eess.SP", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2506.00956", "title": "Continual-MEGA: A Large-scale Benchmark for Generalizable Continual Anomaly Detection", "authors": ["Geonu Lee", "Yujeong Oh", "Geonhui Jang", "Soyoung Lee", "Jeonghyo Song", "Sungmin Cha", "YoungJoon Yoo"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.00956v2", "summary": "In this paper, we introduce a new benchmark for continual learning in anomaly\ndetection, aimed at better reflecting real-world deployment scenarios. Our\nbenchmark, Continual-MEGA, includes a large and diverse dataset that\nsignificantly expands existing evaluation settings by combining carefully\ncurated existing datasets with our newly proposed dataset, ContinualAD. In\naddition to standard continual learning with expanded quantity, we propose a\nnovel scenario that measures zero-shot generalization to unseen classes, those\nnot observed during continual adaptation. This setting poses a new problem\nsetting that continual adaptation also enhances zero-shot performance. We also\npresent a unified baseline algorithm that improves robustness in few-shot\ndetection and maintains strong generalization. Through extensive evaluations,\nwe report three key findings: (1) existing methods show substantial room for\nimprovement, particularly in pixel-level defect localization; (2) our proposed\nmethod consistently outperforms prior approaches; and (3) the newly introduced\nContinualAD dataset enhances the performance of strong anomaly detection\nmodels. We release the benchmark and code in\nhttps://github.com/Continual-Mega/Continual-Mega.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.00956v2", "cate": "cs.CV", "date": "2025-06-01", "updated": "2025-07-31"}
{"id": "2504.06513", "title": "Safe Navigation in Uncertain Crowded Environments Using Risk Adaptive CVaR Barrier Functions", "authors": ["Xinyi Wang", "Taekyung Kim", "Bardh Hoxha", "Georgios Fainekos", "Dimitra Panagou"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      2025 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). Project page: { this https URL }", "url": "http://arxiv.org/abs/2504.06513v5", "summary": "Robot navigation in dynamic, crowded environments poses a significant\nchallenge due to the inherent uncertainties in the obstacle model. In this\nwork, we propose a risk-adaptive approach based on the Conditional\nValue-at-Risk Barrier Function (CVaR-BF), where the risk level is automatically\nadjusted to accept the minimum necessary risk, achieving a good performance in\nterms of safety and optimization feasibility under uncertainty. Additionally,\nwe introduce a dynamic zone-based barrier function which characterizes the\ncollision likelihood by evaluating the relative state between the robot and the\nobstacle. By integrating risk adaptation with this new function, our approach\nadaptively expands the safety margin, enabling the robot to proactively avoid\nobstacles in highly dynamic environments. Comparisons and ablation studies\ndemonstrate that our method outperforms existing social navigation approaches,\nand validate the effectiveness of our proposed framework.", "comment": "2025 IEEE/RSJ International Conference on Intelligent Robots and\n  Systems (IROS). Project page: {https://lawliet9666.github.io/cvarbf/}", "pdf_url": "http://arxiv.org/pdf/2504.06513v5", "cate": "cs.RO", "date": "2025-04-09", "updated": "2025-08-01"}
{"id": "2507.22766", "title": "Bayesian Optimization of Process Parameters of a Sensor-Based Sorting System using Gaussian Processes as Surrogate Models", "authors": ["Felix Kronenwett", "Georg Maier", "Thomas Längle"], "categories": ["cs.LG", "cs.AI", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted at the 30th IEEE International Conference on Emerging Technologies and Factory Automation (ETFA)", "url": "http://arxiv.org/abs/2507.22766v2", "summary": "Sensor-based sorting systems enable the physical separation of a material\nstream into two fractions. The sorting decision is based on the image data\nevaluation of the sensors used and is carried out using actuators. Various\nprocess parameters must be set depending on the properties of the material\nstream, the dimensioning of the system, and the required sorting accuracy.\nHowever, continuous verification and re-adjustment are necessary due to\nchanging requirements and material stream compositions. In this paper, we\nintroduce an approach for optimizing, recurrently monitoring and adjusting the\nprocess parameters of a sensor-based sorting system. Based on Bayesian\nOptimization, Gaussian process regression models are used as surrogate models\nto achieve specific requirements for system behavior with the uncertainties\ncontained therein. This method minimizes the number of necessary experiments\nwhile simultaneously considering two possible optimization targets based on the\nrequirements for both material output streams. In addition, uncertainties are\nconsidered during determining sorting accuracies in the model calculation. We\nevaluated the method with three example process parameters.", "comment": "Accepted at the 30th IEEE International Conference on Emerging\n  Technologies and Factory Automation (ETFA)", "pdf_url": "http://arxiv.org/pdf/2507.22766v2", "cate": "cs.LG", "date": "2025-07-30", "updated": "2025-08-01"}
{"id": "2508.00098", "title": "Stress-Aware Resilient Neural Training", "authors": ["Ashkan Shakarami", "Yousef Yeganeh", "Azade Farshad", "Lorenzo Nicole", "Stefano Ghidoni", "Nassir Navab"], "categories": ["cs.LG", "cs.AI", "cs.CV"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      16 pages, 11 figures", "url": "http://arxiv.org/abs/2508.00098v1", "summary": "This paper introduces Stress-Aware Learning, a resilient neural training\nparadigm in which deep neural networks dynamically adjust their optimization\nbehavior - whether under stable training regimes or in settings with uncertain\ndynamics - based on the concept of Temporary (Elastic) and Permanent (Plastic)\nDeformation, inspired by structural fatigue in materials science. To\ninstantiate this concept, we propose Plastic Deformation Optimizer, a\nstress-aware mechanism that injects adaptive noise into model parameters\nwhenever an internal stress signal - reflecting stagnation in training loss and\naccuracy - indicates persistent optimization difficulty. This enables the model\nto escape sharp minima and converge toward flatter, more generalizable regions\nof the loss landscape. Experiments across six architectures, four optimizers,\nand seven vision benchmarks demonstrate improved robustness and generalization\nwith minimal computational overhead. The code and 3D visuals will be available\non GitHub: https://github.com/Stress-Aware-Learning/SAL.", "comment": "16 pages, 11 figures", "pdf_url": "http://arxiv.org/pdf/2508.00098v1", "cate": "cs.LG", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2508.00172", "title": "DiSC-Med: Diffusion-based Semantic Communications for Robust Medical Image Transmission", "authors": ["Fupei Guo", "Hao Zheng", "Xiang Zhang", "Li Chen", "Yue Wang", "Songyang Zhang"], "categories": ["cs.LG", "eess.IV"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      To appear in 2025 IEEE Global Communications Conference (Globecom)", "url": "http://arxiv.org/abs/2508.00172v1", "summary": "The rapid development of artificial intelligence has driven smart health with\nnext-generation wireless communication technologies, stimulating exciting\napplications in remote diagnosis and intervention. To enable a timely and\neffective response for remote healthcare, efficient transmission of medical\ndata through noisy channels with limited bandwidth emerges as a critical\nchallenge. In this work, we propose a novel diffusion-based semantic\ncommunication framework, namely DiSC-Med, for the medical image transmission,\nwhere medical-enhanced compression and denoising blocks are developed for\nbandwidth efficiency and robustness, respectively. Unlike conventional\npixel-wise communication framework, our proposed DiSC-Med is able to capture\nthe key semantic information and achieve superior reconstruction performance\nwith ultra-high bandwidth efficiency against noisy channels. Extensive\nexperiments on real-world medical datasets validate the effectiveness of our\nframework, demonstrating its potential for robust and efficient telehealth\napplications.", "comment": "To appear in 2025 IEEE Global Communications Conference (Globecom)", "pdf_url": "http://arxiv.org/pdf/2508.00172v1", "cate": "cs.LG", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2508.00409", "title": "STAR-RIS-aided RSMA for the URLLC multi-user MIMO Downlink", "authors": ["Mohammad Soleymani", "Ignacio Santamaria", "Eduard Jorswieck", "Robert Schober", "Lajos Hanzo"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "Comments:      Accepted at 28th International Workshop on Smart Antennas 2025", "url": "http://arxiv.org/abs/2508.00409v1", "summary": "Rate splitting multiple access (RSMA) is intrinsically amalgamated with\nsimultaneously transmitting and reflecting (STAR) reconfigurable intelligent\nsurfaces (RIS) to enhance energy efficiency (EE) of the finite block length\n(FBL) multiple-input multiple-output (MIMO) downlink. An alternating\noptimization-based algorithm is proposed to jointly optimize the transmit\nbeamforming matrices, STAR-RIS configurations, and rate-splitting parameters.\nSTAR-RIS attains 360-degree full-plane coverage, while RSMA provides a\nprominent gain by efficiently managing interference. Numerical results reveal a\nstrong synergy between RSMA and STAR-RIS, demonstreating significant EE gains\nover reflective RIS and spatial division multiple access (SDMA).", "comment": "Accepted at 28th International Workshop on Smart Antennas 2025", "pdf_url": "http://arxiv.org/pdf/2508.00409v1", "cate": "eess.SP", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2506.19330", "title": "Comparative Performance of Finetuned ImageNet Pre-trained Models for Electronic Component Classification", "authors": ["Yidi Shao", "Longfei Zhou", "Fangshuo Tang", "Xinyi Shi", "Dalang Chen", "Shengtao Xia"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Due to issues related to author order and some problems in the current version regarding methodology, we would like to withdraw the preprint to avoid potential conflicts", "url": "http://arxiv.org/abs/2506.19330v2", "summary": "Electronic component classification and detection are crucial in\nmanufacturing industries, significantly reducing labor costs and promoting\ntechnological and industrial development. Pre-trained models, especially those\ntrained on ImageNet, are highly effective in image classification, allowing\nresearchers to achieve excellent results even with limited data. This paper\ncompares the performance of twelve ImageNet pre-trained models in classifying\nelectronic components. Our findings show that all models tested delivered\nrespectable accuracies. MobileNet-V2 recorded the highest at 99.95%, while\nEfficientNet-B0 had the lowest at 92.26%. These results underscore the\nsubstantial benefits of using ImageNet pre-trained models in image\nclassification tasks and confirm the practical applicability of these methods\nin the electronics manufacturing sector.", "comment": "Due to issues related to author order and some problems in the\n  current version regarding methodology, we would like to withdraw the preprint\n  to avoid potential conflicts", "pdf_url": "http://arxiv.org/pdf/2506.19330v2", "cate": "cs.CV", "date": "2025-06-24", "updated": "2025-07-31"}
{"id": "2504.10812", "title": "E2E Parking Dataset: An Open Benchmark for End-to-End Autonomous Parking", "authors": ["Kejia Gao", "Liguo Zhou", "Mingjun Liu", "Alois Knoll"], "categories": ["cs.RO", "cs.AI"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.10812v2", "summary": "End-to-end learning has shown great potential in autonomous parking, yet the\nlack of publicly available datasets limits reproducibility and benchmarking.\nWhile prior work introduced a visual-based parking model and a pipeline for\ndata generation, training, and close-loop test, the dataset itself was not\nreleased. To bridge this gap, we create and open-source a high-quality dataset\nfor end-to-end autonomous parking. Using the original model, we achieve an\noverall success rate of 85.16% with lower average position and orientation\nerrors (0.24 meters and 0.34 degrees).", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.10812v2", "cate": "cs.RO", "date": "2025-04-15", "updated": "2025-08-01"}
{"id": "2507.23339", "title": "Learning to Drift with Individual Wheel Drive: Maneuvering Autonomous Vehicle at the Handling Limits", "authors": ["Yihan Zhou", "Yiwen Lu", "Bo Yang", "Jiayun Li", "Yilin Mo"], "categories": ["cs.RO", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23339v2", "summary": "Drifting, characterized by controlled vehicle motion at high sideslip angles,\nis crucial for safely handling emergency scenarios at the friction limits.\nWhile recent reinforcement learning approaches show promise for drifting\ncontrol, they struggle with the significant simulation-to-reality gap, as\npolicies that perform well in simulation often fail when transferred to\nphysical systems. In this paper, we present a reinforcement learning framework\nwith GPU-accelerated parallel simulation and systematic domain randomization\nthat effectively bridges the gap. The proposed approach is validated on both\nsimulation and a custom-designed and open-sourced 1/10 scale Individual Wheel\nDrive (IWD) RC car platform featuring independent wheel speed control.\nExperiments across various scenarios from steady-state circular drifting to\ndirection transitions and variable-curvature path following demonstrate that\nour approach achieves precise trajectory tracking while maintaining controlled\nsideslip angles throughout complex maneuvers in both simulated and real-world\nenvironments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23339v2", "cate": "cs.RO", "date": "2025-07-31", "updated": "2025-08-01"}
{"id": "2508.00117", "title": "StackLiverNet: A Novel Stacked Ensemble Model for Accurate and Interpretable Liver Disease Detection", "authors": ["Md. Ehsanul Haque", "S. M. Jahidul Islam", "Shakil Mia", "Rumana Sharmin", "Ashikuzzaman", "Md Samir Morshed", "Md. Tahmidul Huque"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted and presented paper of THE 16th INTERNATIONAL IEEE CONFERENCE ON COMPUTING, COMMUNICATION AND NETWORKING TECHNOLOGIES (ICCCNT) INDIA", "url": "http://arxiv.org/abs/2508.00117v1", "summary": "Liver diseases are a serious health concern in the world, which requires\nprecise and timely diagnosis to enhance the survival chances of patients. The\ncurrent literature implemented numerous machine learning and deep learning\nmodels to classify liver diseases, but most of them had some issues like high\nmisclassification error, poor interpretability, prohibitive computational\nexpense, and lack of good preprocessing strategies. In order to address these\ndrawbacks, we introduced StackLiverNet in this study; an interpretable stacked\nensemble model tailored to the liver disease detection task. The framework uses\nadvanced data preprocessing and feature selection technique to increase model\nrobustness and predictive ability. Random undersampling is performed to deal\nwith class imbalance and make the training balanced. StackLiverNet is an\nensemble of several hyperparameter-optimized base classifiers, whose\ncomplementary advantages are used through a LightGBM meta-model. The provided\nmodel demonstrates excellent performance, with the testing accuracy of 99.89%,\nCohen Kappa of 0.9974, and AUC of 0.9993, having only 5 misclassifications, and\nefficient training and inference speeds that are amenable to clinical practice\n(training time 4.2783 seconds, inference time 0.1106 seconds). Besides, Local\nInterpretable Model-Agnostic Explanations (LIME) are applied to generate\ntransparent explanations of individual predictions, revealing high\nconcentrations of Alkaline Phosphatase and moderate SGOT as important\nobservations of liver disease. Also, SHAP was used to rank features by their\nglobal contribution to predictions, while the Morris method confirmed the most\ninfluential features through sensitivity analysis.", "comment": "Accepted and presented paper of THE 16th INTERNATIONAL IEEE\n  CONFERENCE ON COMPUTING, COMMUNICATION AND NETWORKING TECHNOLOGIES (ICCCNT)\n  INDIA", "pdf_url": "http://arxiv.org/pdf/2508.00117v1", "cate": "cs.LG", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2508.00418", "title": "IN2OUT: Fine-Tuning Video Inpainting Model for Video Outpainting Using Hierarchical Discriminator", "authors": ["Sangwoo Youn", "Minji Lee", "Nokap Tony Park", "Yeonggyoo Jeon", "Taeyoung Na"], "categories": ["cs.CV", "eess.IV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICIP 2025. Code: this https URL", "url": "http://arxiv.org/abs/2508.00418v1", "summary": "Video outpainting presents a unique challenge of extending the borders while\nmaintaining consistency with the given content. In this paper, we suggest the\nuse of video inpainting models that excel in object flow learning and\nreconstruction in outpainting rather than solely generating the background as\nin existing methods. However, directly applying or fine-tuning inpainting\nmodels to outpainting has shown to be ineffective, often leading to blurry\nresults. Our extensive experiments on discriminator designs reveal that a\ncritical component missing in the outpainting fine-tuning process is a\ndiscriminator capable of effectively assessing the perceptual quality of the\nextended areas. To tackle this limitation, we differentiate the objectives of\nadversarial training into global and local goals and introduce a hierarchical\ndiscriminator that meets both objectives. Additionally, we develop a\nspecialized outpainting loss function that leverages both local and global\nfeatures of the discriminator. Fine-tuning on this adversarial loss function\nenhances the generator's ability to produce both visually appealing and\nglobally coherent outpainted scenes. Our proposed method outperforms\nstate-of-the-art methods both quantitatively and qualitatively. Supplementary\nmaterials including the demo video and the code are available in SigPort.", "comment": "ICIP 2025. Code: https://github.com/sang-w00/IN2OUT", "pdf_url": "http://arxiv.org/pdf/2508.00418v1", "cate": "cs.CV", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00456", "title": "When Vision-Language Model (VLM) Meets Beam Prediction: A Multimodal Contrastive Learning Framework", "authors": ["Ji Wang", "Bin Tang", "Jian Xiao", "Qimei Cui", "Xingwang Li", "Tony Q. S. Quek"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00456v1", "summary": "As the real propagation environment becomes in creasingly complex and\ndynamic, millimeter wave beam prediction faces huge challenges. However, the\npowerful cross modal representation capability of vision-language model (VLM)\nprovides a promising approach. The traditional methods that rely on real-time\nchannel state information (CSI) are computationally expensive and often fail to\nmaintain accuracy in such environments. In this paper, we present a VLM-driven\ncontrastive learning based multimodal beam prediction framework that integrates\nmultimodal data via modality-specific encoders. To enforce cross-modal\nconsistency, we adopt a contrastive pretraining strategy to align image and\nLiDAR features in the latent space. We use location information as text prompts\nand connect it to the text encoder to introduce language modality, which\nfurther improves cross-modal consistency. Experiments on the DeepSense-6G\ndataset show that our VLM backbone provides additional semantic grounding.\nCompared with existing methods, the overall distance-based accuracy score\n(DBA-Score) of 0.9016, corresponding to 1.46% average improvement.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00456v1", "cate": "eess.SP", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2506.19955", "title": "ZIP: Scalable Crowd Counting via Zero-Inflated Poisson Modeling", "authors": ["Yiming Ma", "Victor Sanchez", "Tanaya Guha"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      15 pages, 11 figures", "url": "http://arxiv.org/abs/2506.19955v3", "summary": "Most crowd counting methods directly regress blockwise density maps using\nMean Squared Error (MSE) losses. This practice has two key limitations: (1) it\nfails to account for the extreme spatial sparsity of annotations - over 95% of\n8x8 blocks are empty across standard benchmarks, so supervision signals in\ninformative regions are diluted by the predominant zeros; (2) MSE corresponds\nto a Gaussian error model that poorly matches discrete, non-negative count\ndata. To address these issues, we introduce ZIP, a scalable crowd counting\nframework that models blockwise counts with a Zero-Inflated Poisson likelihood:\na zero-inflation term learns the probability a block is structurally empty\n(handling excess zeros), while the Poisson component captures expected counts\nwhen people are present (respecting discreteness). We provide a generalization\nanalysis showing a tighter risk bound for ZIP than MSE-based losses and DMCount\nprovided that the training resolution is moderately large. To assess the\nscalability of ZIP, we instantiate it on backbones spanning over 100x in\nparameters/compute. Experiments on ShanghaiTech A & B, UCF-QNRF, and NWPU-Crowd\ndemonstrate that ZIP consistently surpasses state-of-the-art methods across all\nmodel scales.", "comment": "15 pages, 11 figures", "pdf_url": "http://arxiv.org/pdf/2506.19955v3", "cate": "cs.CV", "date": "2025-06-24", "updated": "2025-07-31"}
{"id": "2505.09074", "title": "Trends in Motion Prediction Toward Deployable and Generalizable Autonomy: A Revisit and Perspectives", "authors": ["Letian Wang", "Marc-Antoine Lavoie", "Sandro Papais", "Barza Nisar", "Yuxiao Chen", "Wenhao Ding", "Boris Ivanovic", "Hao Shao", "Abulikemu Abuduweili", "Evan Cook", "Yang Zhou", "Peter Karkus", "Jiachen Li", "Changliu Liu", "Marco Pavone", "Steven Waslander"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      Updated draft. 163 pages, 40 figures, 13 tables", "url": "http://arxiv.org/abs/2505.09074v3", "summary": "Motion prediction, the anticipation of future agent states or scene\nevolution, is rooted in human cognition, bridging perception and\ndecision-making. It enables intelligent systems, such as robots and\nself-driving cars, to act safely in dynamic, human-involved environments, and\ninforms broader time-series reasoning challenges. With advances in methods,\nrepresentations, and datasets, the field has seen rapid progress, reflected in\nquickly evolving benchmark results. Yet, when state-of-the-art methods are\ndeployed in the real world, they often struggle to generalize to open-world\nconditions and fall short of deployment standards. This reveals a gap between\nresearch benchmarks, which are often idealized or ill-posed, and real-world\ncomplexity.\n  To address this gap, this survey revisits the generalization and\ndeployability of motion prediction models, with an emphasis on the applications\nof robotics, autonomous driving, and human motion. We first offer a\ncomprehensive taxonomy of motion prediction methods, covering representations,\nmodeling strategies, application domains, and evaluation protocols. We then\nstudy two key challenges: (1) how to push motion prediction models to be\ndeployable to realistic deployment standards, where motion prediction does not\nact in a vacuum, but functions as one module of closed-loop autonomy stacks -\nit takes input from the localization and perception, and informs downstream\nplanning and control. 2) how to generalize motion prediction models from\nlimited seen scenarios/datasets to the open-world settings. Throughout the\npaper, we highlight critical open challenges to guide future work, aiming to\nrecalibrate the community's efforts, fostering progress that is not only\nmeasurable but also meaningful for real-world applications. The project webpage\ncorresponding to this paper can be found here\nhttps://trends-in-motion-prediction-2025.github.io/.", "comment": "Updated draft. 163 pages, 40 figures, 13 tables", "pdf_url": "http://arxiv.org/pdf/2505.09074v3", "cate": "cs.RO", "date": "2025-05-14", "updated": "2025-07-31"}
{"id": "2508.00127", "title": "Structured Transformations for Stable and Interpretable Neural Computation", "authors": ["Saleh Nikooroo", "Thomas Engel"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00127v1", "summary": "Despite their impressive performance, contemporary neural networks often lack\nstructural safeguards that promote stable learning and interpretable behavior.\nIn this work, we introduce a reformulation of layer-level transformations that\ndeparts from the standard unconstrained affine paradigm. Each transformation is\ndecomposed into a structured linear operator and a residual corrective\ncomponent, enabling more disciplined signal propagation and improved training\ndynamics. Our formulation encourages internal consistency and supports stable\ninformation flow across depth, while remaining fully compatible with standard\nlearning objectives and backpropagation. Through a series of synthetic and\nreal-world experiments, we demonstrate that models constructed with these\nstructured transformations exhibit improved gradient conditioning, reduced\nsensitivity to perturbations, and layer-wise robustness. We further show that\nthese benefits persist across architectural scales and training regimes. This\nstudy serves as a foundation for a more principled class of neural\narchitectures that prioritize stability and transparency-offering new tools for\nreasoning about learning behavior without sacrificing expressive power.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00127v1", "cate": "cs.LG", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2508.00471", "title": "Semantic and Temporal Integration in Latent Diffusion Space for High-Fidelity Video Super-Resolution", "authors": ["Yiwen Wang", "Xinning Chai", "Yuhong Zhang", "Zhengxue Cheng", "Jun Zhao", "Rong Xie", "Li Song"], "categories": ["cs.CV", "eess.IV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00471v1", "summary": "Recent advancements in video super-resolution (VSR) models have demonstrated\nimpressive results in enhancing low-resolution videos. However, due to\nlimitations in adequately controlling the generation process, achieving high\nfidelity alignment with the low-resolution input while maintaining temporal\nconsistency across frames remains a significant challenge. In this work, we\npropose Semantic and Temporal Guided Video Super-Resolution (SeTe-VSR), a novel\napproach that incorporates both semantic and temporal-spatio guidance in the\nlatent diffusion space to address these challenges. By incorporating high-level\nsemantic information and integrating spatial and temporal information, our\napproach achieves a seamless balance between recovering intricate details and\nensuring temporal coherence. Our method not only preserves high-reality visual\ncontent but also significantly enhances fidelity. Extensive experiments\ndemonstrate that SeTe-VSR outperforms existing methods in terms of detail\nrecovery and perceptual quality, highlighting its effectiveness for complex\nvideo super-resolution tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00471v1", "cate": "cs.CV", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00494", "title": "Feasibility of Extracting Skin Nerve Activity from Electrocardiogram Recorded at A Low Sampling Frequency", "authors": ["Youngsun Kong", "Farnoush Baghestani", "I-Ping Chen", "Ki Chon"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "Comments:      Accepted and presented at the 47th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC 2025)", "url": "http://arxiv.org/abs/2508.00494v1", "summary": "Skin nerve activity (SKNA) derived from electrocardiogram (ECG) signals has\nbeen a promising non-invasive surrogate for accurate and effective assessment\nof the sympathetic nervous system (SNS). Typically, SKNA extraction requires a\nhigher sampling frequency than the typical ECG recording requirement (> 2 kHz)\nbecause analysis tools extract SKNA from the 0.5-1 kHz frequency band. However,\nECG recording systems commonly provide a sampling frequency of 1 kHz or lower,\nparticularly for wearable devices. Our recent power spectral analysis exhibited\nthat 150-500 Hz frequency bands are dominant during sympathetic stimulation.\nTherefore, we hypothesize that SKNA can be extracted from ECG sampled at a\nlower sampling frequency. We collected ECG signals from 16 participants during\nSNS stimulation and resampled the signals at 0.5, 1, and 4 kHz. Our statistical\nanalyses of significance, classification performance, and reliability indicate\nno significant difference between SKNA indices derived from ECG signals sampled\nat 0.5, 1, and 4 kHz. Our findings indicate that conventional ECG devices,\nwhich are limited to low sampling rates due to resource constraints or outdated\nguidelines, can be used to reliably collect SKNA if muscle artifact\ncontamination is minimal.", "comment": "Accepted and presented at the 47th Annual International Conference of\n  the IEEE Engineering in Medicine and Biology Society (EMBC 2025)", "pdf_url": "http://arxiv.org/pdf/2508.00494v1", "cate": "eess.SP", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2506.21509", "title": "Mitigating Hallucination of Large Vision-Language Models via Dynamic Logits Calibration", "authors": ["Jiahe Chen", "Jiaying He", "Qian Shao", "Qiyuan Chen", "Jiahe Ying", "Hongxia Xu", "Jintai Chen", "Jianwei Zheng", "Jian Wu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.21509v2", "summary": "Large Vision-Language Models (LVLMs) have demonstrated significant\nadvancements in multimodal understanding, yet they are frequently hampered by\nhallucination-the generation of text that contradicts visual input. Existing\ntraining-free decoding strategies exhibit critical limitations, including the\nuse of static constraints that do not adapt to semantic drift during\ngeneration, inefficiency stemming from the need for multiple forward passes,\nand degradation of detail due to overly rigid intervention rules. To overcome\nthese challenges, this paper introduces Dynamic Logits Calibration (DLC), a\nnovel training-free decoding framework designed to dynamically align text\ngeneration with visual evidence at inference time. At the decoding phase, DLC\nstep-wise employs CLIP to assess the semantic alignment between the input image\nand the generated text sequence. Then, the Relative Visual Advantage (RVA) of\ncandidate tokens is evaluated against a dynamically updated contextual\nbaseline, adaptively adjusting output logits to favor tokens that are visually\ngrounded. Furthermore, an adaptive weighting mechanism, informed by a real-time\ncontext alignment score, carefully balances the visual guidance while ensuring\nthe overall quality of the textual output. Extensive experiments conducted\nacross diverse benchmarks and various LVLM architectures (such as LLaVA,\nInstructBLIP, and MiniGPT-4) demonstrate that DLC significantly reduces\nhallucinations, outperforming current methods while maintaining high inference\nefficiency by avoiding multiple forward passes. Overall, we present an\neffective and efficient decoding-time solution to mitigate hallucinations,\nthereby enhancing the reliability of LVLMs for more practices. Code will be\nreleased on Github.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.21509v2", "cate": "cs.CV", "date": "2025-06-26", "updated": "2025-07-31"}
{"id": "2505.11494", "title": "SHIELD: Safety on Humanoids via CBFs In Expectation on Learned Dynamics", "authors": ["Lizhi Yang", "Blake Werner", "Ryan K. Cosner", "David Fridovich-Keil", "Preston Culbertson", "Aaron D. Ames"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      Video at this https URL . To appear at IROS 2025", "url": "http://arxiv.org/abs/2505.11494v2", "summary": "Robot learning has produced remarkably effective ``black-box'' controllers\nfor complex tasks such as dynamic locomotion on humanoids. Yet ensuring dynamic\nsafety, i.e., constraint satisfaction, remains challenging for such policies.\nReinforcement learning (RL) embeds constraints heuristically through reward\nengineering, and adding or modifying constraints requires retraining.\nModel-based approaches, like control barrier functions (CBFs), enable runtime\nconstraint specification with formal guarantees but require accurate dynamics\nmodels. This paper presents SHIELD, a layered safety framework that bridges\nthis gap by: (1) training a generative, stochastic dynamics residual model\nusing real-world data from hardware rollouts of the nominal controller,\ncapturing system behavior and uncertainties; and (2) adding a safety layer on\ntop of the nominal (learned locomotion) controller that leverages this model\nvia a stochastic discrete-time CBF formulation enforcing safety constraints in\nprobability. The result is a minimally-invasive safety layer that can be added\nto the existing autonomy stack to give probabilistic guarantees of safety that\nbalance risk and performance. In hardware experiments on an Unitree G1\nhumanoid, SHIELD enables safe navigation (obstacle avoidance) through varied\nindoor and outdoor environments using a nominal (unknown) RL controller and\nonboard perception.", "comment": "Video at https://youtu.be/-Qv1wR4jfj4. To appear at IROS 2025", "pdf_url": "http://arxiv.org/pdf/2505.11494v2", "cate": "cs.RO", "date": "2025-05-16", "updated": "2025-08-01"}
{"id": "2508.00131", "title": "ECG Latent Feature Extraction with Autoencoders for Downstream Prediction Tasks", "authors": ["Christopher Harvey", "Sumaiya Shomaji", "Zijun Yao", "Amit Noheria"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      arXiv admin note: substantial text overlap with arXiv:2410.02937", "url": "http://arxiv.org/abs/2508.00131v1", "summary": "The electrocardiogram (ECG) is an inexpensive and widely available tool for\ncardiac assessment. Despite its standardized format and small file size, the\nhigh complexity and inter-individual variability of ECG signals (typically a\n60,000-size vector with 12 leads at 500 Hz) make it challenging to use in deep\nlearning models, especially when only small training datasets are available.\nThis study addresses these challenges by exploring feature generation methods\nfrom representative beat ECGs, focusing on Principal Component Analysis (PCA)\nand Autoencoders to reduce data complexity. We introduce three novel\nVariational Autoencoder (VAE) variants-Stochastic Autoencoder (SAE), Annealed\nbeta-VAE (A beta-VAE), and Cyclical beta VAE (C beta-VAE)-and compare their\neffectiveness in maintaining signal fidelity and enhancing downstream\nprediction tasks using a Light Gradient Boost Machine (LGBM). The A beta-VAE\nachieved superior signal reconstruction, reducing the mean absolute error (MAE)\nto 15.7+/-3.2 muV, which is at the level of signal noise. Moreover, the SAE\nencodings, when combined with traditional ECG summary features, improved the\nprediction of reduced Left Ventricular Ejection Fraction (LVEF), achieving an\nholdout test set area under the receiver operating characteristic curve (AUROC)\nof 0.901 with a LGBM classifier. This performance nearly matches the 0.909\nAUROC of state-of-the-art CNN model but requires significantly less\ncomputational resources. Further, the ECG feature extraction-LGBM pipeline\navoids overfitting and retains predictive performance when trained with less\ndata. Our findings demonstrate that these VAE encodings are not only effective\nin simplifying ECG data but also provide a practical solution for applying deep\nlearning in contexts with limited-scale labeled training data.", "comment": "arXiv admin note: substantial text overlap with arXiv:2410.02937", "pdf_url": "http://arxiv.org/pdf/2508.00131v1", "cate": "cs.LG", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2508.00590", "title": "A Novel Modeling Framework and Data Product for Extended VIIRS-like Artificial Nighttime Light Image Reconstruction (1986-2024)", "authors": ["Yihe Tian", "Kwan Man Cheng", "Zhengbo Zhang", "Tao Zhang", "Suju Li", "Dongmei Yan", "Bing Xu"], "categories": ["cs.CV", "eess.IV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00590v1", "summary": "Artificial Night-Time Light (NTL) remote sensing is a vital proxy for\nquantifying the intensity and spatial distribution of human activities.\nAlthough the NPP-VIIRS sensor provides high-quality NTL observations, its\ntemporal coverage, which begins in 2012, restricts long-term time-series\nstudies that extend to earlier periods. Despite the progress in extending\nVIIRS-like NTL time-series, current methods still suffer from two significant\nshortcomings: the underestimation of light intensity and the structural\nomission. To overcome these limitations, we propose a novel reconstruction\nframework consisting of a two-stage process: construction and refinement. The\nconstruction stage features a Hierarchical Fusion Decoder (HFD) designed to\nenhance the fidelity of the initial reconstruction. The refinement stage\nemploys a Dual Feature Refiner (DFR), which leverages high-resolution\nimpervious surface masks to guide and enhance fine-grained structural details.\nBased on this framework, we developed the Extended VIIRS-like Artificial\nNighttime Light (EVAL) product for China, extending the standard data record\nbackwards by 26 years to begin in 1986. Quantitative evaluation shows that EVAL\nsignificantly outperforms existing state-of-the-art products, boosting the\n$\\text{R}^2$ from 0.68 to 0.80 while lowering the RMSE from 1.27 to 0.99.\nFurthermore, EVAL exhibits excellent temporal consistency and maintains a high\ncorrelation with socioeconomic parameters, confirming its reliability for\nlong-term analysis. The resulting EVAL dataset provides a valuable new resource\nfor the research community and is publicly available at\nhttps://doi.org/10.11888/HumanNat.tpdc.302930.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00590v1", "cate": "cs.CV", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00800", "title": "Multibeam High Throughput Satellite: Hardware Foundation, Resource Allocation, and Precoding", "authors": ["Rui Chen", "Wen-Xuan Long", "Bing-Qian Wang", "Yuan He", "Rui-Jin Sun", "Nan Cheng", "Gan Zheng", "Dusit Niyato"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "Comments:      38 pages, 18 figures", "url": "http://arxiv.org/abs/2508.00800v1", "summary": "With its wide coverage and uninterrupted service, satellite communication is\na critical technology for next-generation 6G communications. High throughput\nsatellite (HTS) systems, utilizing multipoint beam and frequency multiplexing\ntechniques, enable satellite communication capacity of up to Tbps to meet the\ngrowing traffic demand. Therefore, it is imperative to review\nthe-state-of-the-art of multibeam HTS systems and identify their associated\nchallenges and perspectives. Firstly, we summarize the multibeam HTS hardware\nfoundations, including ground station systems, on-board payloads, and user\nterminals. Subsequently, we review the flexible on-board radio resource\nallocation approaches of bandwidth, power, time slot, and joint allocation\nschemes of HTS systems to optimize resource utilization and cater to\nnon-uniform service demand. Additionally, we survey multibeam precoding methods\nfor the HTS system to achieve full-frequency reuse and interference\ncancellation, which are classified according to different deployments such as\nsingle gateway precoding, multiple gateway precoding, on-board precoding, and\nhybrid on-board/on-ground precoding. Finally, we disscuss the challenges\nrelated to Q/V band link outage, time and frequency synchronization of\ngateways, the accuracy of channel state information (CSI), payload light-weight\ndevelopment, and the application of deep learning (DL). Research on these\ntopics will contribute to enhancing the performance of HTS systems and finally\ndelivering high-speed data to areas underserved by terrestrial networks.", "comment": "38 pages, 18 figures", "pdf_url": "http://arxiv.org/pdf/2508.00800v1", "cate": "eess.SP", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2507.13373", "title": "Butter: Frequency Consistency and Hierarchical Fusion for Autonomous Driving Object Detection", "authors": ["Xiaojian Lin", "Wenxin Zhang", "Yuchu Jiang", "Wangyu Wu", "Yiran Guo", "Kangxu Wang", "Zongzheng Zhang", "Guijin Wang", "Lei Jin", "Hao Zhao"], "categories": ["cs.CV", "I.4.8; I.2.10; H.5.1; I.2.6"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      10 pages, 6 figures. Supplementary material: 8 pages, 7 figures. Accepted at ACM Multimedia 2025", "url": "http://arxiv.org/abs/2507.13373v2", "summary": "Hierarchical feature representations play a pivotal role in computer vision,\nparticularly in object detection for autonomous driving. Multi-level semantic\nunderstanding is crucial for accurately identifying pedestrians, vehicles, and\ntraffic signs in dynamic environments. However, existing architectures, such as\nYOLO and DETR, struggle to maintain feature consistency across different scales\nwhile balancing detection precision and computational efficiency. To address\nthese challenges, we propose Butter, a novel object detection framework\ndesigned to enhance hierarchical feature representations for improving\ndetection robustness. Specifically, Butter introduces two key innovations:\nFrequency-Adaptive Feature Consistency Enhancement (FAFCE) Component, which\nrefines multi-scale feature consistency by leveraging adaptive frequency\nfiltering to enhance structural and boundary precision, and Progressive\nHierarchical Feature Fusion Network (PHFFNet) Module, which progressively\nintegrates multi-level features to mitigate semantic gaps and strengthen\nhierarchical feature learning. Through extensive experiments on BDD100K, KITTI,\nand Cityscapes, Butter demonstrates superior feature representation\ncapabilities, leading to notable improvements in detection accuracy while\nreducing model complexity. By focusing on hierarchical feature refinement and\nintegration, Butter provides an advanced approach to object detection that\nachieves a balance between accuracy, deployability, and computational\nefficiency in real-time autonomous driving scenarios. Our model and\nimplementation are publicly available at https://github.com/Aveiro-Lin/Butter,\nfacilitating further research and validation within the autonomous driving\ncommunity.", "comment": "10 pages, 6 figures. Supplementary material: 8 pages, 7 figures.\n  Accepted at ACM Multimedia 2025", "pdf_url": "http://arxiv.org/pdf/2507.13373v2", "cate": "cs.CV", "date": "2025-07-12", "updated": "2025-07-31"}
{"id": "2506.18725", "title": "TopoRec: Point Cloud Recognition Using Topological Data Analysis", "authors": ["Anirban Ghosh", "Iliya Kulbaka", "Ian Dahlin", "Ayan Dutta"], "categories": ["cs.RO", "cs.CG", "cs.CV"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.18725v2", "summary": "Point cloud-based object/place recognition remains a problem of interest in\napplications such as autonomous driving, scene reconstruction, and\nlocalization. Extracting a meaningful global descriptor from a query point\ncloud that can be matched with the descriptors of the database point clouds is\na challenging problem. Furthermore, when the query point cloud is noisy or has\nbeen transformed (e.g., rotated), it adds to the complexity. To this end, we\npropose a novel methodology, named TopoRec, which utilizes Topological Data\nAnalysis (TDA) for extracting local descriptors from a point cloud, thereby\neliminating the need for resource-intensive GPU-based machine learning\ntraining. More specifically, we used the ATOL vectorization method to generate\nvectors for point clouds. To test the quality of the proposed TopoRec\ntechnique, we have implemented it on multiple real-world (e.g., Oxford\nRobotCar, NCLT) and realistic (e.g., ShapeNet) point cloud datasets for\nlarge-scale place and object recognition, respectively. Unlike existing\nlearning-based approaches such as PointNetVLAD and PCAN, our method does not\nrequire extensive training, making it easily adaptable to new environments.\nDespite this, it consistently outperforms both state-of-the-art learning-based\nand handcrafted baselines (e.g., M2DP, ScanContext) on standard benchmark\ndatasets, demonstrating superior accuracy and strong generalization.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.18725v2", "cate": "cs.RO", "date": "2025-06-23", "updated": "2025-08-01"}
{"id": "2508.00141", "title": "INSPIRE-GNN: Intelligent Sensor Placement to Improve Sparse Bicycling Network Prediction via Reinforcement Learning Boosted Graph Neural Networks", "authors": ["Mohit Gupta", "Debjit Bhowmick", "Rhys Newbury", "Meead Saberi", "Shirui Pan", "Ben Beck"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00141v1", "summary": "Accurate link-level bicycling volume estimation is essential for sustainable\nurban transportation planning. However, many cities face significant challenges\nof high data sparsity due to limited bicycling count sensor coverage. To\naddress this issue, we propose INSPIRE-GNN, a novel Reinforcement Learning\n(RL)-boosted hybrid Graph Neural Network (GNN) framework designed to optimize\nsensor placement and improve link-level bicycling volume estimation in\ndata-sparse environments. INSPIRE-GNN integrates Graph Convolutional Networks\n(GCN) and Graph Attention Networks (GAT) with a Deep Q-Network (DQN)-based RL\nagent, enabling a data-driven strategic selection of sensor locations to\nmaximize estimation performance. Applied to Melbourne's bicycling network,\ncomprising 15,933 road segments with sensor coverage on only 141 road segments\n(99% sparsity) - INSPIRE-GNN demonstrates significant improvements in volume\nestimation by strategically selecting additional sensor locations in\ndeployments of 50, 100, 200 and 500 sensors. Our framework outperforms\ntraditional heuristic methods for sensor placement such as betweenness\ncentrality, closeness centrality, observed bicycling activity and random\nplacement, across key metrics such as Mean Squared Error (MSE), Root Mean\nSquared Error (RMSE) and Mean Absolute Error (MAE). Furthermore, our\nexperiments benchmark INSPIRE-GNN against standard machine learning and deep\nlearning models in the bicycle volume estimation performance, underscoring its\neffectiveness. Our proposed framework provides transport planners actionable\ninsights to effectively expand sensor networks, optimize sensor placement and\nmaximize volume estimation accuracy and reliability of bicycling data for\ninformed transportation planning decisions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00141v1", "cate": "cs.LG", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2508.00750", "title": "SU-ESRGAN: Semantic and Uncertainty-Aware ESRGAN for Super-Resolution of Satellite and Drone Imagery with Fine-Tuning for Cross Domain Evaluation", "authors": ["Prerana Ramkumar"], "categories": ["cs.CV", "cs.LG", "eess.IV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00750v1", "summary": "Generative Adversarial Networks (GANs) have achieved realistic\nsuper-resolution (SR) of images however, they lack semantic consistency and\nper-pixel confidence, limiting their credibility in critical remote sensing\napplications such as disaster response, urban planning and agriculture. This\npaper introduces Semantic and Uncertainty-Aware ESRGAN (SU-ESRGAN), the first\nSR framework designed for satellite imagery to integrate the ESRGAN,\nsegmentation loss via DeepLabv3 for class detail preservation and Monte Carlo\ndropout to produce pixel-wise uncertainty maps. The SU-ESRGAN produces results\n(PSNR, SSIM, LPIPS) comparable to the Baseline ESRGAN on aerial imagery. This\nnovel model is valuable in satellite systems or UAVs that use wide\nfield-of-view (FoV) cameras, trading off spatial resolution for coverage. The\nmodular design allows integration in UAV data pipelines for on-board or\npost-processing SR to enhance imagery resulting due to motion blur, compression\nand sensor limitations. Further, the model is fine-tuned to evaluate its\nperformance on cross domain applications. The tests are conducted on two drone\nbased datasets which differ in altitude and imaging perspective. Performance\nevaluation of the fine-tuned models show a stronger adaptation to the Aerial\nMaritime Drone Dataset, whose imaging characteristics align with the training\ndata, highlighting the importance of domain-aware training in SR-applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00750v1", "cate": "cs.CV", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00202", "title": "Robust Classification under Noisy Labels: A Geometry-Aware Reliability Framework for Foundation Models", "authors": ["Ecem Bozkurt", "Antonio Ortega"], "categories": ["cs.LG", "cs.AI", "eess.SP"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      5 pages, 2 figures, under review at CAMSAP 2025", "url": "http://arxiv.org/abs/2508.00202v1", "summary": "Foundation models (FMs) pretrained on large datasets have become fundamental\nfor various downstream machine learning tasks, in particular in scenarios where\nobtaining perfectly labeled data is prohibitively expensive. In this paper, we\nassume an FM has to be fine-tuned with noisy data and present a two-stage\nframework to ensure robust classification in the presence of label noise\nwithout model retraining. Recent work has shown that simple k-nearest neighbor\n(kNN) approaches using an embedding derived from an FM can achieve good\nperformance even in the presence of severe label noise. Our work is motivated\nby the fact that these methods make use of local geometry. In this paper,\nfollowing a similar two-stage procedure, reliability estimation followed by\nreliability-weighted inference, we show that improved performance can be\nachieved by introducing geometry information. For a given instance, our\nproposed inference uses a local neighborhood of training data, obtained using\nthe non-negative kernel (NNK) neighborhood construction. We propose several\nmethods for reliability estimation that can rely less on distance and local\nneighborhood as the label noise increases. Our evaluation on CIFAR-10 and\nDermaMNIST shows that our methods improve robustness across various noise\nconditions, surpassing standard K-NN approaches and recent\nadaptive-neighborhood baselines.", "comment": "5 pages, 2 figures, under review at CAMSAP 2025", "pdf_url": "http://arxiv.org/pdf/2508.00202v1", "cate": "cs.LG", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.14632", "title": "BusterX++: Towards Unified Cross-Modal AI-Generated Content Detection and Explanation with MLLM", "authors": ["Haiquan Wen", "Tianxiao Li", "Zhenglin Huang", "Yiwei He", "Guangliang Cheng"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.14632v2", "summary": "Recent advances in generative AI have dramatically improved image and video\nsynthesis capabilities, significantly increasing the risk of misinformation\nthrough sophisticated fake content. In response, detection methods have evolved\nfrom traditional approaches to multimodal large language models (MLLMs),\noffering enhanced transparency and interpretability in identifying synthetic\nmedia. However, current detection systems remain fundamentally limited by their\nsingle-modality design. These approaches analyze images or videos separately,\nmaking them ineffective against synthetic content that combines multiple media\nformats. To address these challenges, we introduce \\textbf{BusterX++}, a novel\nframework designed specifically for cross-modal detection and explanation of\nsynthetic media. Our approach incorporates an advanced reinforcement learning\n(RL) post-training strategy that eliminates cold-start. Through Multi-stage\nTraining, Thinking Reward, and Hybrid Reasoning, BusterX++ achieves stable and\nsubstantial performance improvements. To enable comprehensive evaluation, we\nalso present \\textbf{GenBuster++}, a cross-modal benchmark leveraging\nstate-of-the-art image and video generation techniques. This benchmark\ncomprises 4,000 images and video clips, meticulously curated by human experts\nusing a novel filtering methodology to ensure high quality, diversity, and\nreal-world applicability. Extensive experiments demonstrate the effectiveness\nand generalizability of our approach.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.14632v2", "cate": "cs.CV", "date": "2025-07-19", "updated": "2025-07-31"}
{"id": "2507.13970", "title": "A Segmented Robot Grasping Perception Neural Network for Edge AI", "authors": ["Casper Bröcheler", "Thomas Vroom", "Derrick Timmermans", "Alan van den Akker", "Guangzhi Tang", "Charalampos S. Kouzinopoulos", "Rico Möckel"], "categories": ["cs.RO", "cs.AI", "I.2; I.2.9; I.2.10"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      Accepted by SMC 2025", "url": "http://arxiv.org/abs/2507.13970v2", "summary": "Robotic grasping, the ability of robots to reliably secure and manipulate\nobjects of varying shapes, sizes and orientations, is a complex task that\nrequires precise perception and control. Deep neural networks have shown\nremarkable success in grasp synthesis by learning rich and abstract\nrepresentations of objects. When deployed at the edge, these models can enable\nlow-latency, low-power inference, making real-time grasping feasible in\nresource-constrained environments. This work implements Heatmap-Guided Grasp\nDetection, an end-to-end framework for the detection of 6-Dof grasp poses, on\nthe GAP9 RISC-V System-on-Chip. The model is optimised using hardware-aware\ntechniques, including input dimensionality reduction, model partitioning, and\nquantisation. Experimental evaluation on the GraspNet-1Billion benchmark\nvalidates the feasibility of fully on-chip inference, highlighting the\npotential of low-power MCUs for real-time, autonomous manipulation.", "comment": "Accepted by SMC 2025", "pdf_url": "http://arxiv.org/pdf/2507.13970v2", "cate": "cs.RO", "date": "2025-07-18", "updated": "2025-08-01"}
{"id": "2508.00161", "title": "Watch the Weights: Unsupervised monitoring and control of fine-tuned LLMs", "authors": ["Ziqian Zhong", "Aditi Raghunathan"], "categories": ["cs.LG", "cs.CL"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00161v1", "summary": "The releases of powerful open-weight large language models (LLMs) are often\nnot accompanied by access to their full training data. Existing\ninterpretability methods, particularly those based on activations, often\nrequire or assume distributionally similar data. This is a significant\nlimitation when detecting and defending against novel potential threats like\nbackdoors, which are by definition out-of-distribution.\n  In this work, we introduce a new method for understanding, monitoring and\ncontrolling fine-tuned LLMs that interprets weights, rather than activations,\nthereby side stepping the need for data that is distributionally similar to the\nunknown training data. We demonstrate that the top singular vectors of the\nweight difference between a fine-tuned model and its base model correspond to\nnewly acquired behaviors. By monitoring the cosine similarity of activations\nalong these directions, we can detect salient behaviors introduced during\nfine-tuning with high precision.\n  For backdoored models that bypasses safety mechanisms when a secret trigger\nis present, our method stops up to 100% of attacks with a false positive rate\nbelow 1.2%. For models that have undergone unlearning, we detect inference on\nerased topics with accuracy up to 95.42% and can even steer the model to\nrecover \"unlearned\" information. Besides monitoring, our method also shows\npotential for pre-deployment model auditing: by analyzing commercial\ninstruction-tuned models (OLMo, Llama, Qwen), we are able to uncover\nmodel-specific fine-tuning focus including marketing strategies and Midjourney\nprompt generation.\n  Our implementation can be found at https://github.com/fjzzq2002/WeightWatch.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00161v1", "cate": "cs.LG", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2508.00781", "title": "Numerical Uncertainty in Linear Registration: An Experimental Study", "authors": ["Niusha Mirhakimi", "Yohan Chatelain", "Jean-Baptiste Poline", "Tristan Glatard"], "categories": ["q-bio.QM", "eess.IV"], "primary_category": "Subjects:       Quantitative Methods (q-bio.QM)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00781v1", "summary": "While linear registration is a critical step in MRI preprocessing pipelines,\nits numerical uncertainty is understudied. Using Monte-Carlo Arithmetic (MCA)\nsimulations, we assessed the most commonly used linear registration tools\nwithin major software packages (SPM, FSL, and ANTs) across multiple image\nsimilarity measures, two brain templates, and both healthy control (HC, n=50)\nand Parkinson's Disease (PD, n=50) cohorts. Our findings highlight the\ninfluence of linear registration tools and similarity measures on numerical\nstability. Among the evaluated tools and with default similarity measures, SPM\nexhibited the highest stability. FSL and ANTs showed greater and similar ranges\nof variability, with ANTs demonstrating particular sensitivity to numerical\nperturbations that occasionally led to registration failure. Furthermore, no\nsignificant differences were observed between healthy and PD cohorts,\nsuggesting that numerical stability analyses obtained with healthy subjects may\ngeneralise to clinical populations. Finally, we also demonstrated how numerical\nuncertainty measures may support automated quality control (QC) of linear\nregistration results. Overall, our experimental results characterize the\nnumerical stability of linear registration experimentally and can serve as a\nbasis for future uncertainty analyses.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00781v1", "cate": "q-bio.QM", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00379", "title": "Active IRS-Enabled Integrated Sensing and Communications with Extended Targets", "authors": ["Yuan Fang", "Xianxin Song", "Huazhou Hou", "Ziguo Zhong", "Xianghao Yu", "Jie Xu", "Yongming Huang"], "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00379v1", "summary": "This paper studies the active intelligent reflecting surface (IRS)-enabled\nintegrated sensing and communications (ISAC), in which an active IRS is\ndeployed to assist the base station (BS) in serving multiple communication\nusers (CUs) and simultaneously sensing an \\emph{extended} target at the\nnon-line-of-sight (NLoS) area of the BS. The active IRS has the capability of\namplifying the reflected signals so as to overcome significant reflection path\nloss in NLoS communication and sensing. In particular, we derive the sensing\nCram\\'{e}r-Rao bound (CRB) for estimating the target response matrix.\nAccordingly, we jointly optimize the transmit beamforming at the BS and the\nreflective beamforming at the active IRS to minimize the sensing CRB, subject\nto the signal-to-interference-plus-noise ratio (SINR) requirements at the CUs,\nthe transmit power budgets at the BS and active IRS, as well as the power\namplification gain constraints at the active IRS. The CRB minimization problem\nis highly non-convex and thus difficult to solve in general. To address this\nchallenge, we first focus on two specified conditions by considering the\nsensing-only scenario via ignoring the SINR constraints for communications, for\nwhich the closed-form optimal transmit beamforming is derived. Then, we propose\ntwo efficient alternating optimization (AO)-based algorithms to obtain\nhigh-quality solutions for the general ISAC scenarios. Next, we analyze the\ninherent relationship between the power scaling at the BS and the amplification\nscaling at the active IRS. It is shown that the active IRS always amplifies the\nsignal using the maximum amplification gain under practical system settings.\nFinally, numerical results are provided to verify the effectiveness of the\nproposed AO-based algorithms and the benefits of active IRS-enabled ISAC\ncompared to its passive IRSs counterparts.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00379v1", "cate": "cs.IT", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2507.18371", "title": "MVG4D: Image Matrix-Based Multi-View and Motion Generation for 4D Content Creation from a Single Image", "authors": ["DongFu Yin", "Xiaotian Chen", "Fei Richard Yu", "Xuanchen Li", "Xinhao Zhang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18371v2", "summary": "Advances in generative modeling have significantly enhanced digital content\ncreation, extending from 2D images to complex 3D and 4D scenes. Despite\nsubstantial progress, producing high-fidelity and temporally consistent dynamic\n4D content remains a challenge. In this paper, we propose MVG4D, a novel\nframework that generates dynamic 4D content from a single still image by\ncombining multi-view synthesis with 4D Gaussian Splatting (4D GS). At its core,\nMVG4D employs an image matrix module that synthesizes temporally coherent and\nspatially diverse multi-view images, providing rich supervisory signals for\ndownstream 3D and 4D reconstruction. These multi-view images are used to\noptimize a 3D Gaussian point cloud, which is further extended into the temporal\ndomain via a lightweight deformation network. Our method effectively enhances\ntemporal consistency, geometric fidelity, and visual realism, addressing key\nchallenges in motion discontinuity and background degradation that affect prior\n4D GS-based methods. Extensive experiments on the Objaverse dataset demonstrate\nthat MVG4D outperforms state-of-the-art baselines in CLIP-I, PSNR, FVD, and\ntime efficiency. Notably, it reduces flickering artifacts and sharpens\nstructural details across views and time, enabling more immersive AR/VR\nexperiences. MVG4D sets a new direction for efficient and controllable 4D\ngeneration from minimal inputs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18371v2", "cate": "cs.CV", "date": "2025-07-24", "updated": "2025-07-31"}
{"id": "2507.21553", "title": "Multi-robot LiDAR SLAM: a practical case study in underground tunnel environments", "authors": ["Federica Di Lauro", "Domenico G. Sorrenti", "Miguel Angel Sotelo"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      14 pages, 14 figures", "url": "http://arxiv.org/abs/2507.21553v3", "summary": "Multi-robot SLAM aims at localizing and building a map with multiple robots,\ninteracting with each other. In the work described in this article, we analyze\nthe pipeline of a decentralized LiDAR SLAM system to study the current\nlimitations of the state of the art, and we discover a significant source of\nfailures, i.e., that the loop detection is the source of too many false\npositives. We therefore develop and propose a new heuristic to overcome these\nlimitations. The environment taken as reference in this work is the highly\nchallenging case of underground tunnels. We also highlight potential new\nresearch areas still under-explored.", "comment": "14 pages, 14 figures", "pdf_url": "http://arxiv.org/pdf/2507.21553v3", "cate": "cs.RO", "date": "2025-07-29", "updated": "2025-08-01"}
{"id": "2508.00174", "title": "RL as Regressor: A Reinforcement Learning Approach for Function Approximation", "authors": ["Yongchao Huang"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      7 pages", "url": "http://arxiv.org/abs/2508.00174v1", "summary": "Standard regression techniques, while powerful, are often constrained by\npredefined, differentiable loss functions such as mean squared error. These\nfunctions may not fully capture the desired behavior of a system, especially\nwhen dealing with asymmetric costs or complex, non-differentiable objectives.\nIn this paper, we explore an alternative paradigm: framing regression as a\nReinforcement Learning (RL) problem. We demonstrate this by treating a model's\nprediction as an action and defining a custom reward signal based on the\nprediction error, and we can leverage powerful RL algorithms to perform\nfunction approximation. Through a progressive case study of learning a noisy\nsine wave, we illustrate the development of an Actor-Critic agent, iteratively\nenhancing it with Prioritized Experience Replay, increased network capacity,\nand positional encoding to enable a capable RL agent for this regression task.\nOur results show that the RL framework not only successfully solves the\nregression problem but also offers enhanced flexibility in defining objectives\nand guiding the learning process.", "comment": "7 pages", "pdf_url": "http://arxiv.org/pdf/2508.00174v1", "cate": "cs.LG", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2411.05824", "title": "Navigating Distribution Shifts in Medical Image Analysis: A Survey", "authors": ["Zixian Su", "Jingwei Guo", "Xi Yang", "Qiufeng Wang", "Frans Coenen", "Kaizhu Huang"], "categories": ["eess.IV", "cs.CV", "cs.LG"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2411.05824v2", "summary": "Medical Image Analysis (MedIA) has become indispensable in modern healthcare,\nenhancing clinical diagnostics and personalized treatment. Despite the\nremarkable advancements supported by deep learning (DL) technologies, their\npractical deployment faces challenges due to distribution shifts, where models\ntrained on specific datasets underperform across others from varying hospitals,\nregions, or patient populations. To navigate this issue, researchers have been\nactively developing strategies to increase the adaptability and robustness of\nDL models, enabling their effective use in unfamiliar and diverse environments.\nThis paper systematically reviews approaches that apply DL techniques to MedIA\nsystems affected by distribution shifts. Unlike traditional categorizations\nbased on technical specifications, our approach is grounded in the real-world\noperational constraints faced by healthcare institutions. Specifically, we\ncategorize the existing body of work into Joint Training, Federated Learning,\nFine-tuning, and Domain Generalization, with each method tailored to distinct\nscenarios caused by Data Accessibility, Privacy Concerns, and Collaborative\nProtocols. This perspective equips researchers with a nuanced understanding of\nhow DL can be strategically deployed to address distribution shifts in MedIA,\nensuring diverse and robust medical applications. By delving deeper into these\ntopics, we highlight potential pathways for future research that not only\naddress existing limitations but also push the boundaries of deployable MedIA\ntechnologies.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2411.05824v2", "cate": "eess.IV", "date": "2024-11-05", "updated": "2025-08-01"}
{"id": "2508.00458", "title": "LO-Aware Adaptive Modulation for Rydberg Atomic Receivers", "authors": ["Jiuyu Liu", "Yi Ma", "Rahim Tafazolli"], "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "Comments:      Accepted by IEEE GLOBECOM 2025", "url": "http://arxiv.org/abs/2508.00458v1", "summary": "Rydberg atomic (RA) receivers represent a revolutionary quantum technology\nfor wireless communications, offering unprecedented sensitivity beyond\nconventional radio frequency (RF) antennas. However, these receivers detect\nonly signal amplitude, losing critical phase information. While reference\nsignals generated by a local oscillator (LO) can assist in phase recovery,\nexisting modulation schemes designed for conventional systems perform poorly\nwith this quantum detection mechanism. This paper introduces a breakthrough\nLO-aware adaptive modulation (LOAM) scheme specifically developed for RA\nreceivers that dynamically adapts to complex fading channel coefficients. LOAM\nmaximizes the minimum amplitude difference between constellation points,\nensuring optimal detection performance. The innovation employs an adaptive\nco-linear constellation architecture aligned with the combined phase of\nreference signal and channel coefficient. For strong reference signals, LOAM\ngenerates symmetric constellation points centered at origin; for weak signals,\nit adopts non-symmetric distributions. The paper mathematically derives the\nthreshold governing these operational regimes. Simulation results reveal the\ntransformative impact of LOAM, demonstrating performance gains exceeding 45 dB\nover conventional modulation schemes, including quadrature amplitude modulation\n(QAM), phase-shift keying (PSK), and pulse-amplitude modulation (PAM).", "comment": "Accepted by IEEE GLOBECOM 2025", "pdf_url": "http://arxiv.org/pdf/2508.00458v1", "cate": "cs.IT", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2507.18675", "title": "Advancing Vision-based Human Action Recognition: Exploring Vision-Language CLIP Model for Generalisation in Domain-Independent Tasks", "authors": ["Utkarsh Shandilya", "Marsha Mariya Kappan", "Sanyam Jain", "Vijeta Sharma"], "categories": ["cs.CV", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18675v2", "summary": "Human action recognition plays a critical role in healthcare and medicine,\nsupporting applications such as patient behavior monitoring, fall detection,\nsurgical robot supervision, and procedural skill assessment. While traditional\nmodels like CNNs and RNNs have achieved moderate success, they often struggle\nto generalize across diverse and complex actions. Recent advancements in\nvision-language models, especially the transformer-based CLIP model, offer\npromising capabilities for generalizing action recognition from video data. In\nthis work, we evaluate CLIP on the UCF-101 dataset and systematically analyze\nits performance under three masking strategies: (1) percentage-based and\nshape-based black masking at 10%, 30%, and 50%, (2) feature-specific masking to\nsuppress bias-inducing elements, and (3) isolation masking that retains only\nclass-specific regions. Our results reveal that CLIP exhibits inconsistent\nbehavior and frequent misclassifications, particularly when essential visual\ncues are obscured. To overcome these limitations, we propose incorporating\nclass-specific noise, learned via a custom loss function, to reinforce\nattention to class-defining features. This enhancement improves classification\naccuracy and model confidence while reducing bias. We conclude with a\ndiscussion on the challenges of applying such models in clinical domains and\noutline directions for future work to improve generalizability across\ndomain-independent healthcare scenarios.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18675v2", "cate": "cs.CV", "date": "2025-07-24", "updated": "2025-07-30"}
{"id": "2507.23172", "title": "Benchmarking Massively Parallelized Multi-Task Reinforcement Learning for Robotics Tasks", "authors": ["Viraj Joshi", "Zifan Xu", "Bo Liu", "Peter Stone", "Amy Zhang"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      RLC 2025", "url": "http://arxiv.org/abs/2507.23172v2", "summary": "Multi-task Reinforcement Learning (MTRL) has emerged as a critical training\nparadigm for applying reinforcement learning (RL) to a set of complex\nreal-world robotic tasks, which demands a generalizable and robust policy. At\nthe same time, \\emph{massively parallelized training} has gained popularity,\nnot only for significantly accelerating data collection through GPU-accelerated\nsimulation but also for enabling diverse data collection across multiple tasks\nby simulating heterogeneous scenes in parallel. However, existing MTRL research\nhas largely been limited to off-policy methods like SAC in the\nlow-parallelization regime. MTRL could capitalize on the higher asymptotic\nperformance of on-policy algorithms, whose batches require data from the\ncurrent policy, and as a result, take advantage of massive parallelization\noffered by GPU-accelerated simulation. To bridge this gap, we introduce a\nmassively parallelized $\\textbf{M}$ulti-$\\textbf{T}$ask $\\textbf{Bench}$mark\nfor robotics (MTBench), an open-sourced benchmark featuring a broad\ndistribution of 50 manipulation tasks and 20 locomotion tasks, implemented\nusing the GPU-accelerated simulator IsaacGym. MTBench also includes four base\nRL algorithms combined with seven state-of-the-art MTRL algorithms and\narchitectures, providing a unified framework for evaluating their performance.\nOur extensive experiments highlight the superior speed of evaluating MTRL\napproaches using MTBench, while also uncovering unique challenges that arise\nfrom combining massive parallelism with MTRL. Code is available at\nhttps://github.com/Viraj-Joshi/MTBench", "comment": "RLC 2025", "pdf_url": "http://arxiv.org/pdf/2507.23172v2", "cate": "cs.RO", "date": "2025-07-31", "updated": "2025-08-01"}
{"id": "2508.00180", "title": "EMA Without the Lag: Bias-Corrected Iterate Averaging Schemes", "authors": ["Adam Block", "Cyril Zhang"], "categories": ["cs.LG", "cs.AI", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00180v1", "summary": "Stochasticity in language model fine-tuning, often caused by the small batch\nsizes typically used in this regime, can destabilize training by introducing\nlarge oscillations in generation quality. A popular approach to mitigating this\ninstability is to take an Exponential moving average (EMA) of weights\nthroughout training. While EMA reduces stochasticity, thereby smoothing\ntraining, the introduction of bias from old iterates often creates a lag in\noptimization relative to vanilla training. In this work, we propose the\nBias-Corrected Exponential Moving Average (BEMA), a simple and practical\naugmentation of EMA that retains variance-reduction benefits while eliminating\nbias. BEMA is motivated by a simple theoretical model wherein we demonstrate\nprovable acceleration of BEMA over both a standard EMA and vanilla training.\nThrough an extensive suite of experiments on Language Models, we show that BEMA\nleads to significantly improved convergence rates and final performance over\nboth EMA and vanilla training in a variety of standard LM benchmarks, making\nBEMA a practical and theoretically motivated intervention for more stable and\nefficient fine-tuning.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00180v1", "cate": "cs.LG", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2503.03778", "title": "Generating Novel Brain Morphology by Deforming Learned Templates", "authors": ["Alan Q. Wang", "Fangrui Huang", "Bailey Trang", "Wei Peng", "Mohammad Abbasi", "Kilian Pohl", "Mert Sabuncu", "Ehsan Adeli"], "categories": ["eess.IV", "q-bio.TO"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "Comments:      Provisional Acceptance at MICCAI 2025", "url": "http://arxiv.org/abs/2503.03778v3", "summary": "Designing generative models for 3D structural brain MRI that synthesize\nmorphologically-plausible and attribute-specific (e.g., age, sex, disease\nstate) samples is an active area of research. Existing approaches based on\nframeworks like GANs or diffusion models synthesize the image directly, which\nmay limit their ability to capture intricate morphological details. In this\nwork, we propose a 3D brain MRI generation method based on state-of-the-art\nlatent diffusion models (LDMs), called MorphLDM, that generates novel images by\napplying synthesized deformation fields to a learned template. Instead of using\na reconstruction-based autoencoder (as in a typical LDM), our encoder outputs a\nlatent embedding derived from both an image and a learned template that is\nitself the output of a template decoder; this latent is passed to a deformation\nfield decoder, whose output is applied to the learned template. A registration\nloss is minimized between the original image and the deformed template with\nrespect to the encoder and both decoders. Empirically, our approach outperforms\ngenerative baselines on metrics spanning image diversity, adherence with\nrespect to input conditions, and voxel-based morphometry. Our code is available\nat https://github.com/alanqrwang/morphldm.", "comment": "Provisional Acceptance at MICCAI 2025", "pdf_url": "http://arxiv.org/pdf/2503.03778v3", "cate": "eess.IV", "date": "2025-03-04", "updated": "2025-07-31"}
{"id": "2508.00540", "title": "Appendices for \"Closed-Form BER Analysis for Uplink NOMA with Dynamic SIC Decoding\"", "authors": ["Hequn Zhang", "Qu Luo", "Pei Xiao", "Yue Zhang", "Huiyu Zhou"], "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00540v1", "summary": "This document provides the supplementary materials for the paper Closed-Form\nBER Analysis for Uplink NOMA with Dynamic SIC Decoding. The appendices present\ndetailed mathematical derivations and proofs that support the analytical\nframework of the main paper. Specifically, we include: (i) cumulative\ndistribution functions for ordered channel gains; (ii) probability density\nfunctions of normalized signal-plus-interference variances in NOMA dynamic SIC\ndecoding; (iii) closed-form expressions for pairwise error probability (PEP)\nwith two users; (iv) probability derivations for channel gain ordering in the\ntwo-UE case, specifically when UE 1 and UE 2 have the strongest or second\nstrongest channel gains; (v) BER analysis for M-QAM modulation schemes\nincluding BPSK, 4QAM, 16QAM and 64QAM; (vi) PDF derivations for channel gains\nunder various ordering conditions; and (vii) challenges of PDF derivations for\nreal part of channel gain under various ordering condition. These mathematical\nfoundations enable the closed-form BER analysis of uplink NOMA systems with\ndynamic SIC decoding under Rayleigh fading channels, supporting analytical\nexpressions for various modulation schemes and system configurations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00540v1", "cate": "cs.IT", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00200", "title": "Predicting Formula 1 Race Outcomes: Decomposing the Roles of Drivers and Constructors through Linear Modeling", "authors": ["Saurabh Rane"], "categories": ["stat.AP"], "primary_category": "Subjects:       Applications (stat.AP)", "pdf_link": null, "comments": "Comments:      26 pages, 12 figures, 9 tables", "url": "http://arxiv.org/abs/2508.00200v1", "summary": "Formula 1 performance is a combination of the car's ability and the driver's\nability. While a given race or season can tell you how well a car and driver\nperformed jointly, isolating the individual impact of the driver and\nconstructor remains challenging. This paper extends a Regularized Adjusted Plus\nMinus (RAPM) methodology (Sill 2010), commonly used in basketball and hockey,\nto parse out individual driver and constructor impact. It employs a\ntime-decayed ridge regression with LOESS (Jacoby 2000) smoothing to predict\nrace results for the Hybrid Engine Era (2014 - 2024). By measuring the\nconstructor and driver coefficients over time, we measure the relative\nindividual impact of driver and constructor throughout the period. Results show\nthat constructors explain 64.0% of the variance in race outcomes in the Hybrid\nEngine Era. Additionally, constructors have increased importance in benchmarked\nrank-agnostic cohorts (e.g., Top 10 points finishers) and decreased importance\nin qualifying. By decomposing performance into individual driver and\nconstructor metrics, we create a robust framework for inter-constructor driver\ncomparisons that the Formula 1 points system obfuscates. Our work enhances the\nunderstanding of driver and constructor contributions to race success, offering\nvaluable insights for strategic decision-making in Formula 1.", "comment": "26 pages, 12 figures, 9 tables", "pdf_url": "http://arxiv.org/pdf/2508.00200v1", "cate": "stat.AP", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.19621", "title": "Exemplar Med-DETR: Toward Generalized and Robust Lesion Detection in Mammogram Images and beyond", "authors": ["Sheethal Bhat", "Bogdan Georgescu", "Adarsh Bhandary Panambur", "Mathias Zinnen", "Tri-Thien Nguyen", "Awais Mansoor", "Karim Khalifa Elbarbary", "Siming Bayer", "Florin-Cristian Ghesu", "Sasa Grbic", "Andreas Maier"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      I am asking for a withdrawal of the paper as I did not have institutional approval to release this paper right now", "url": "http://arxiv.org/abs/2507.19621v2", "summary": "Detecting abnormalities in medical images poses unique challenges due to\ndifferences in feature representations and the intricate relationship between\nanatomical structures and abnormalities. This is especially evident in\nmammography, where dense breast tissue can obscure lesions, complicating\nradiological interpretation. Despite leveraging anatomical and semantic\ncontext, existing detection methods struggle to learn effective class-specific\nfeatures, limiting their applicability across different tasks and imaging\nmodalities. In this work, we introduce Exemplar Med-DETR, a novel multi-modal\ncontrastive detector that enables feature-based detection. It employs\ncross-attention with inherently derived, intuitive class-specific exemplar\nfeatures and is trained with an iterative strategy. We achieve state-of-the-art\nperformance across three distinct imaging modalities from four public datasets.\nOn Vietnamese dense breast mammograms, we attain an mAP of 0.7 for mass\ndetection and 0.55 for calcifications, yielding an absolute improvement of 16\npercentage points. Additionally, a radiologist-supported evaluation of 100\nmammograms from an out-of-distribution Chinese cohort demonstrates a twofold\ngain in lesion detection performance. For chest X-rays and angiography, we\nachieve an mAP of 0.25 for mass and 0.37 for stenosis detection, improving\nresults by 4 and 7 percentage points, respectively. These results highlight the\npotential of our approach to advance robust and generalizable detection systems\nfor medical imaging.", "comment": "I am asking for a withdrawal of the paper as I did not have\n  institutional approval to release this paper right now", "pdf_url": "http://arxiv.org/pdf/2507.19621v2", "cate": "cs.CV", "date": "2025-07-25", "updated": "2025-07-30"}
{"id": "2412.19950", "title": "Data-driven tool wear prediction in milling, based on a process-integrated single-sensor approach", "authors": ["Eric Hirsch", "Christian Friedrich"], "categories": ["cs.LG", "cs.RO", "eess.SP"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      This work has been submitted to the IEEE Transactions on Automation Science and Engineering for possible publication. ,14 pages, 12 figures", "url": "http://arxiv.org/abs/2412.19950v4", "summary": "Accurate tool wear prediction is essential for maintaining productivity and\nminimizing costs in machining. However, the complex nature of the tool wear\nprocess poses significant challenges to achieving reliable predictions. This\nstudy explores data-driven methods, in particular deep learning, for tool wear\nprediction. Traditional data-driven approaches often focus on a single process,\nrelying on multi-sensor setups and extensive data generation, which limits\ngeneralization to new settings. Moreover, multi-sensor integration is often\nimpractical in industrial environments. To address these limitations, this\nresearch investigates the transferability of predictive models using minimal\ntraining data, validated across two processes. Furthermore, it uses a simple\nsetup with a single acceleration sensor to establish a low-cost data generation\napproach that facilitates the generalization of models to other processes via\ntransfer learning. The study evaluates several machine learning models,\nincluding transformer-inspired convolutional neural networks (CNN), long\nshort-term memory networks (LSTM), support vector machines (SVM), and decision\ntrees, trained on different input formats such as feature vectors and\nshort-time Fourier transform (STFT). The performance of the models is evaluated\non two machines and on different amounts of training data, including scenarios\nwith significantly reduced datasets, providing insight into their effectiveness\nunder constrained data conditions. The results demonstrate the potential of\nspecific models and configurations for effective tool wear prediction,\ncontributing to the development of more adaptable and efficient predictive\nmaintenance strategies in machining. Notably, the ConvNeXt model has an\nexceptional performance, achieving 99.1\\% accuracy in identifying tool wear\nusing data from only four milling tools operated until they are worn.", "comment": "This work has been submitted to the IEEE Transactions on Automation\n  Science and Engineering for possible publication. ,14 pages, 12 figures", "pdf_url": "http://arxiv.org/pdf/2412.19950v4", "cate": "cs.LG", "date": "2024-12-27", "updated": "2025-07-31"}
{"id": "2508.00201", "title": "RecoMind: A Reinforcement Learning Framework for Optimizing In-Session User Satisfaction in Recommendation Systems", "authors": ["Mehdi Ben Ayed", "Fei Feng", "Jay Adams", "Vishwakarma Singh", "Kritarth Anand", "Jiajing Xu"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00201v1", "summary": "Existing web-scale recommendation systems commonly use supervised learning\nmethods that prioritize immediate user feedback. Although reinforcement\nlearning (RL) offers a solution to optimize longer-term goals, such as\nin-session engagement, applying it at web scale is challenging due to the\nextremely large action space and engineering complexity. In this paper, we\nintroduce RecoMind, a simulator-based RL framework designed for the effective\noptimization of session-based goals at web-scale. RecoMind leverages existing\nrecommendation models to establish a simulation environment and to bootstrap\nthe RL policy to optimize immediate user interactions from the outset. This\nmethod integrates well with existing industry pipelines, simplifying the\ntraining and deployment of RL policies. Additionally, RecoMind introduces a\ncustom exploration strategy to efficiently explore web-scale action spaces with\nhundreds of millions of items. We evaluated RecoMind through extensive offline\nsimulations and online A/B testing on a video streaming platform. Both methods\nshowed that the RL policy trained using RecoMind significantly outperforms\ntraditional supervised learning recommendation approaches in in-session user\nsatisfaction. In online A/B tests, the RL policy increased videos watched for\nmore than 10 seconds by 15.81\\% and improved session depth by 4.71\\% for\nsessions with at least 10 interactions. As a result, RecoMind presents a\nsystematic and scalable approach for embedding RL into web-scale recommendation\nsystems, showing great promise for optimizing session-based user satisfaction.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00201v1", "cate": "cs.LG", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.22790", "title": "Optimizing Federated Learning Configurations for MRI Prostate Segmentation and Cancer Detection: A Simulation Study", "authors": ["Ashkan Moradi", "Fadila Zerka", "Joeran S. Bosma", "Mohammed R. S. Sunoqrot", "Bendik S. Abrahamsen", "Derya Yakar", "Jeroen Geerdink", "Henkjan Huisman", "Tone Frost Bathen", "Mattijs Elschot"], "categories": ["eess.IV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "Comments:      25 pages, 6 figures, 4 tables. Accepted for publication in Radiology: Artificial Intelligence, \\c{opyright} 2025 Radiological Society of North America (RSNA)", "url": "http://arxiv.org/abs/2507.22790v2", "summary": "Purpose: To develop and optimize a federated learning (FL) framework across\nmultiple clients for biparametric MRI prostate segmentation and clinically\nsignificant prostate cancer (csPCa) detection. Materials and Methods: A\nretrospective study was conducted using Flower FL to train a nnU-Net-based\narchitecture for MRI prostate segmentation and csPCa detection, using data\ncollected from January 2010 to August 2021. Model development included training\nand optimizing local epochs, federated rounds, and aggregation strategies for\nFL-based prostate segmentation on T2-weighted MRIs (four clients, 1294\npatients) and csPCa detection using biparametric MRIs (three clients, 1440\npatients). Performance was evaluated on independent test sets using the Dice\nscore for segmentation and the Prostate Imaging: Cancer Artificial Intelligence\n(PI-CAI) score, defined as the average of the area under the receiver operating\ncharacteristic curve and average precision, for csPCa detection. P-values for\nperformance differences were calculated using permutation testing. Results: The\nFL configurations were independently optimized for both tasks, showing improved\nperformance at 1 epoch 300 rounds using FedMedian for prostate segmentation and\n5 epochs 200 rounds using FedAdagrad, for csPCa detection. Compared with the\naverage performance of the clients, the optimized FL model significantly\nimproved performance in prostate segmentation and csPCa detection on the\nindependent test set. The optimized FL model showed higher lesion detection\nperformance compared to the FL-baseline model, but no evidence of a difference\nwas observed for prostate segmentation. Conclusions: FL enhanced the\nperformance and generalizability of MRI prostate segmentation and csPCa\ndetection compared with local models, and optimizing its configuration further\nimproved lesion detection performance.", "comment": "25 pages, 6 figures, 4 tables. Accepted for publication in Radiology:\n  Artificial Intelligence, \\c{opyright} 2025 Radiological Society of North\n  America (RSNA)", "pdf_url": "http://arxiv.org/pdf/2507.22790v2", "cate": "eess.IV", "date": "2025-07-30", "updated": "2025-07-31"}
{"id": "2508.00626", "title": "Deep Learning-Based Rate-Adaptive CSI Feedback for Wideband XL-MIMO Systems in the Near-Field Domain", "authors": ["Zhenyu Liu", "Yi Ma", "Rahim Tafazolli"], "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00626v1", "summary": "Accurate and efficient channel state information (CSI) feedback is crucial\nfor unlocking the substantial spectral efficiency gains of extremely\nlarge-scale MIMO (XL-MIMO) systems in future 6G networks. However, the\ncombination of near-field spherical wave propagation and frequency-dependent\nbeam split effects in wideband scenarios poses significant challenges for CSI\nrepresentation and compression. This paper proposes WideNLNet-CA, a\nrate-adaptive deep learning framework designed to enable efficient CSI feedback\nin wideband near-field XL-MIMO systems. WideNLNet-CA introduces a lightweight\nencoder-decoder architecture with multi-stage downsampling and upsampling,\nincorporating computationally efficient residual blocks to capture complex\nmulti-scale channel features with reduced overhead. A novel compression ratio\nadaptive module with feature importance estimation is introduced to dynamically\nmodulate feature selection based on target compression ratios, enabling\nflexible adaptation across a wide range of feedback rates using a single model.\nEvaluation results demonstrate that WideNLNet-CA consistently outperforms\nexisting compressive sensing and deep learning-based works across various\ncompression ratios and bandwidths, while maintaining fast inference and low\nmodel storage requirements.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00626v1", "cate": "cs.IT", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00176", "title": "New Pilot-Study Design in Functional Data Analysis", "authors": ["Ping-Han Huang", "Ming-Hung Kao"], "categories": ["stat.ME", "stat.AP"], "primary_category": "Subjects:       Methodology (stat.ME)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00176v1", "summary": "Efficient data collection is essential in applied studies where frequent\nmeasurements are costly, time-consuming, or burdensome. This challenge is\nespecially pronounced in functional data settings, where each subject is\nobserved at only a few time points due to practical constraints. Most existing\ndesign approaches focus on selecting optimal time points for individual\nsubjects, typically relying on model parameters estimated from a pilot study.\nHowever, the design of the pilot study itself has received limited attention.\nWe propose a framework for constructing pilot-study designs that support both\naccurate trajectory recovery and effective planning of future designs. A search\nalgorithm is developed to generate such high-quality pilot-study designs.\nSimulation studies and a real data application demonstrate that our approach\noutperforms commonly used alternatives, highlighting its value in\nresource-limited settings.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00176v1", "cate": "stat.ME", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.20216", "title": "Dual-Stream Global-Local Feature Collaborative Representation Network for Scene Classification of Mining Area", "authors": ["Shuqi Fan", "Haoyi Wang", "Xianju Li"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted to IJCNN 2025", "url": "http://arxiv.org/abs/2507.20216v2", "summary": "Scene classification of mining areas provides accurate foundational data for\ngeological environment monitoring and resource development planning. This study\nfuses multi-source data to construct a multi-modal mine land cover scene\nclassification dataset. A significant challenge in mining area classification\nlies in the complex spatial layout and multi-scale characteristics. By\nextracting global and local features, it becomes possible to comprehensively\nreflect the spatial distribution, thereby enabling a more accurate capture of\nthe holistic characteristics of mining scenes. We propose a dual-branch fusion\nmodel utilizing collaborative representation to decompose global features into\na set of key semantic vectors. This model comprises three key components:(1)\nMulti-scale Global Transformer Branch: It leverages adjacent large-scale\nfeatures to generate global channel attention features for small-scale\nfeatures, effectively capturing the multi-scale feature relationships. (2)\nLocal Enhancement Collaborative Representation Branch: It refines the attention\nweights by leveraging local features and reconstructed key semantic sets,\nensuring that the local context and detailed characteristics of the mining area\nare effectively integrated. This enhances the model's sensitivity to\nfine-grained spatial variations. (3) Dual-Branch Deep Feature Fusion Module: It\nfuses the complementary features of the two branches to incorporate more scene\ninformation. This fusion strengthens the model's ability to distinguish and\nclassify complex mining landscapes. Finally, this study employs multi-loss\ncomputation to ensure a balanced integration of the modules. The overall\naccuracy of this model is 83.63%, which outperforms other comparative models.\nAdditionally, it achieves the best performance across all other evaluation\nmetrics.", "comment": "Accepted to IJCNN 2025", "pdf_url": "http://arxiv.org/pdf/2507.20216v2", "cate": "cs.CV", "date": "2025-07-27", "updated": "2025-07-31"}
{"id": "2506.14709", "title": "DiFuse-Net: RGB and Dual-Pixel Depth Estimation using Window Bi-directional Parallax Attention and Cross-modal Transfer Learning", "authors": ["Kunal Swami", "Debtanu Gupta", "Amrit Kumar Muduli", "Chirag Jaiswal", "Pankaj Kumar Bajpai"], "categories": ["cs.CV", "cs.RO"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted in IROS 2025", "url": "http://arxiv.org/abs/2506.14709v2", "summary": "Depth estimation is crucial for intelligent systems, enabling applications\nfrom autonomous navigation to augmented reality. While traditional stereo and\nactive depth sensors have limitations in cost, power, and robustness,\ndual-pixel (DP) technology, ubiquitous in modern cameras, offers a compelling\nalternative. This paper introduces DiFuse-Net, a novel modality decoupled\nnetwork design for disentangled RGB and DP based depth estimation. DiFuse-Net\nfeatures a window bi-directional parallax attention mechanism (WBiPAM)\nspecifically designed to capture the subtle DP disparity cues unique to\nsmartphone cameras with small aperture. A separate encoder extracts contextual\ninformation from the RGB image, and these features are fused to enhance depth\nprediction. We also propose a Cross-modal Transfer Learning (CmTL) mechanism to\nutilize large-scale RGB-D datasets in the literature to cope with the\nlimitations of obtaining large-scale RGB-DP-D dataset. Our evaluation and\ncomparison of the proposed method demonstrates its superiority over the DP and\nstereo-based baseline methods. Additionally, we contribute a new, high-quality,\nreal-world RGB-DP-D training dataset, named Dual-Camera Dual-Pixel (DCDP)\ndataset, created using our novel symmetric stereo camera hardware setup, stereo\ncalibration and rectification protocol, and AI stereo disparity estimation\nmethod.", "comment": "Accepted in IROS 2025", "pdf_url": "http://arxiv.org/pdf/2506.14709v2", "cate": "cs.CV", "date": "2025-06-17", "updated": "2025-07-31"}
{"id": "2508.00230", "title": "Towards Higher Effective Rank in Parameter-efficient Fine-tuning using Khatri--Rao Product", "authors": ["Paul Albert", "Frederic Z. Zhang", "Hemanth Saratchandran", "Anton van den Hengel", "Ehsan Abbasnejad"], "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      To appear in ICCV 2025", "url": "http://arxiv.org/abs/2508.00230v1", "summary": "Parameter-efficient fine-tuning (PEFT) has become a standard approach for\nadapting large pre-trained models. Amongst PEFT methods, low-rank adaptation\n(LoRA) has achieved notable success. However, recent studies have highlighted\nits limitations compared against full-rank alternatives, particularly when\napplied to multimodal and large language models. In this work, we present a\nquantitative comparison amongst full-rank and low-rank PEFT methods using a\nsynthetic matrix approximation benchmark with controlled spectral properties.\nOur results confirm that LoRA struggles to approximate matrices with relatively\nflat spectrums or high frequency components -- signs of high effective ranks.\nTo this end, we introduce KRAdapter, a novel PEFT algorithm that leverages the\nKhatri-Rao product to produce weight updates, which, by construction, tends to\nproduce matrix product with a high effective rank. We demonstrate performance\ngains with KRAdapter on vision-language models up to 1B parameters and on large\nlanguage models up to 8B parameters, particularly on unseen common-sense\nreasoning tasks. In addition, KRAdapter maintains the memory and compute\nefficiency of LoRA, making it a practical and robust alternative to fine-tune\nbillion-scale parameter models.", "comment": "To appear in ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2508.00230v1", "cate": "cs.LG", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2406.04158", "title": "Accurate Cross-modal Reconstruction of Vehicle Target from Sparse-aspect Multi-baseline SAR data", "authors": ["Da Li", "Guoqiang Zhao", "Chen Yao", "Kaiqiang Zhu", "Houjun Sun", "Jiacheng Bao", "Maokun Li"], "categories": ["cs.CV", "eess.IV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2406.04158v5", "summary": "Multi-aspect multi-baseline SAR 3D imaging is a critical remote sensing\ntechnique, promising in urban mapping and monitoring. However, sparse\nobservation due to constrained flight trajectories degrade imaging quality,\nparticularly for anisotropic small targets like vehicles and aircraft. In the\npast, compressive sensing (CS) was the mainstream approach for sparse 3D SAR\nreconstruction. More recently, deep learning (DL) has emerged as a powerful\nalternative, markedly boosting reconstruction quality and efficiency through\nstrong data-driven representations capabilities and fast inference\ncharacteristics. However, existing DL methods typically train deep neural\nnetworks (DNNs) using only high-resolution radar images. This unimodal learning\nparadigm precludes the incorporation of complementary information from other\ndata sources, thereby limiting potential improvements in reconstruction\nperformance. In this paper, we introduce cross-modal learning and propose a\nCross-Modal 3D-SAR Reconstruction Network (CMAR-Net) that enhances sparse 3D\nSAR reconstruction by fusing heterogeneous information. Leveraging cross-modal\nsupervision from 2D optical images and error propagation guaranteed by\ndifferentiable rendering, CMAR-Net achieves efficient training and reconstructs\nhighly sparse-aspect multi-baseline SAR image into visually structured and\naccurate 3D images, particularly for vehicle targets. Trained solely on\nsimulated data, CMAR-Net exhibits strong generalization across extensive\nreal-world evaluations on parking lot measurements containing numerous civilian\nvehicles, outperforming state-of-the-art CS and DL methods in structural\naccuracy. Our work highlights the potential of cross-modal learning for 3D SAR\nreconstruction and introduces a novel framework for radar imaging research.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2406.04158v5", "cate": "cs.CV", "date": "2024-06-06", "updated": "2025-08-01"}
{"id": "2411.07001", "title": "DoF Analysis and Beamforming Design for Active IRS-aided Multi-user MIMO Wireless Communication in Rank-deficient Channels", "authors": ["Jinbing Jiang", "Feng Shu", "Xuehui Wang", "Ke Yang", "Chong Shen", "Qi Zhang", "Dongming Wang", "Jiangzhou Wang"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "Comments:      12 pages, 9 figures", "url": "http://arxiv.org/abs/2411.07001v3", "summary": "Due to its ability of significantly improving data rate, intelligent\nreflecting surface (IRS) will be a potential crucial technique for the future\ngeneration wireless networks like 6G. In this paper, we will focus on the\nanalysis of degree of freedom (DoF) in IRS-aided multi-user MIMO network.\nFirstly, the DoF upper bound of IRS-aided single-user MIMO network, i.e., the\nachievable maximum DoF of such a system, is derived, and the corresponding\nresults are extended to the case of IRS-aided multiuser MIMO by using the\nmatrix rank inequalities. In particular, in serious rank-deficient, also called\nlow-rank, channels like line-of-sight (LoS), the network DoF may doubles over\nno-IRS with the help of IRS. To verify the rate performance gain from augmented\nDoF, three closed-form beamforming methods, null-space projection plus maximize\ntransmit power and maximize receive power (NSP-MTP-MRP), Schmidt\northogonalization plus minimum mean square error (SO-MMSE) and two-layer\nleakage plus MMSE (TLL-MMSE) are proposed to achieve the maximum DoF.\nSimulation results shows that IRS does make a dramatic rate enhancement. For\nexample, in a serious deficient channel, the sum-rate of the proposed TLL-MMSE\naided by IRS is about twice that of no IRS. This means that IRS may achieve a\nsignificant DoF improvement in such a channel.", "comment": "12 pages, 9 figures", "pdf_url": "http://arxiv.org/pdf/2411.07001v3", "cate": "eess.SP", "date": "2024-11-11", "updated": "2025-08-01"}
{"id": "2508.00286", "title": "Toward using explainable data-driven surrogate models for treating performance-based seismic design as an inverse engineering problem", "authors": ["Mohsen Zaker Esteghamati"], "categories": ["cs.LG", "stat.AP", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00286v1", "summary": "This study presents a methodology to treat performance-based seismic design\nas an inverse engineering problem, where design parameters are directly derived\nto achieve specific performance objectives. By implementing explainable machine\nlearning models, this methodology directly maps design variables and\nperformance metrics, tackling computational inefficiencies of performance-based\ndesign. The resultant machine learning model is integrated as an evaluation\nfunction into a genetic optimization algorithm to solve the inverse problem.\nThe developed methodology is then applied to two different inventories of steel\nand concrete moment frames in Los Angeles and Charleston to obtain sectional\nproperties of frame members that minimize expected annualized seismic loss in\nterms of repair costs. The results show high accuracy of the surrogate models\n(e.g., R2> 90%) across a diverse set of building types, geometries, seismic\ndesign, and site hazard, where the optimization algorithm could identify the\noptimum values of members' properties for a fixed set of geometric variables,\nconsistent with engineering principles.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00286v1", "cate": "cs.LG", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2507.20356", "title": "Detecting Visual Information Manipulation Attacks in Augmented Reality: A Multimodal Semantic Reasoning Approach", "authors": ["Yanming Xiu", "Maria Gorlatova"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      11 pages, 7 figures", "url": "http://arxiv.org/abs/2507.20356v2", "summary": "The virtual content in augmented reality (AR) can introduce misleading or\nharmful information, leading to semantic misunderstandings or user errors. In\nthis work, we focus on visual information manipulation (VIM) attacks in AR\nwhere virtual content changes the meaning of real-world scenes in subtle but\nimpactful ways. We introduce a taxonomy that categorizes these attacks into\nthree formats: character, phrase, and pattern manipulation, and three purposes:\ninformation replacement, information obfuscation, and extra wrong information.\nBased on the taxonomy, we construct a dataset, AR-VIM. It consists of 452\nraw-AR video pairs spanning 202 different scenes, each simulating a real-world\nAR scenario. To detect such attacks, we propose a multimodal semantic reasoning\nframework, VIM-Sense. It combines the language and visual understanding\ncapabilities of vision-language models (VLMs) with optical character\nrecognition (OCR)-based textual analysis. VIM-Sense achieves an attack\ndetection accuracy of 88.94% on AR-VIM, consistently outperforming vision-only\nand text-only baselines. The system reaches an average attack detection latency\nof 7.07 seconds in a simulated video processing framework and 7.17 seconds in a\nreal-world evaluation conducted on a mobile Android AR application.", "comment": "11 pages, 7 figures", "pdf_url": "http://arxiv.org/pdf/2507.20356v2", "cate": "cs.CV", "date": "2025-07-27", "updated": "2025-07-31"}
{"id": "2507.21053", "title": "Flow Matching Policy Gradients", "authors": ["David McAllister", "Songwei Ge", "Brent Yi", "Chung Min Kim", "Ethan Weber", "Hongsuk Choi", "Haiwen Feng", "Angjoo Kanazawa"], "categories": ["cs.LG", "cs.RO"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      See our blog post at this https URL", "url": "http://arxiv.org/abs/2507.21053v2", "summary": "Flow-based generative models, including diffusion models, excel at modeling\ncontinuous distributions in high-dimensional spaces. In this work, we introduce\nFlow Policy Optimization (FPO), a simple on-policy reinforcement learning\nalgorithm that brings flow matching into the policy gradient framework. FPO\ncasts policy optimization as maximizing an advantage-weighted ratio computed\nfrom the conditional flow matching loss, in a manner compatible with the\npopular PPO-clip framework. It sidesteps the need for exact likelihood\ncomputation while preserving the generative capabilities of flow-based models.\nUnlike prior approaches for diffusion-based reinforcement learning that bind\ntraining to a specific sampling method, FPO is agnostic to the choice of\ndiffusion or flow integration at both training and inference time. We show that\nFPO can train diffusion-style policies from scratch in a variety of continuous\ncontrol tasks. We find that flow-based models can capture multimodal action\ndistributions and achieve higher performance than Gaussian policies,\nparticularly in under-conditioned settings.", "comment": "See our blog post at https://flowreinforce.github.io", "pdf_url": "http://arxiv.org/pdf/2507.21053v2", "cate": "cs.LG", "date": "2025-07-28", "updated": "2025-08-01"}
{"id": "2508.00264", "title": "Calibrated Language Models and How to Find Them with Label Smoothing", "authors": ["Jerry Huang", "Peng Lu", "Qiuhao Zeng"], "categories": ["cs.LG", "cs.AI", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted to the Forty-second International Conference on Machine Learning (ICML) 2025. First two authors contributed equally", "url": "http://arxiv.org/abs/2508.00264v1", "summary": "Recent advances in natural language processing (NLP) have opened up greater\nopportunities to enable fine-tuned large language models (LLMs) to behave as\nmore powerful interactive agents through improved instruction-following\nability. However, understanding how this impacts confidence calibration for\nreliable model output has not been researched in full. In this work, we examine\nvarious open-sourced LLMs, identifying significant calibration degradation\nafter instruction tuning in each. Seeking a practical solution, we look towards\nlabel smoothing, which has been shown as an effective method to regularize for\noverconfident predictions but has yet to be widely adopted in the supervised\nfine-tuning (SFT) of LLMs. We first provide insight as to why label smoothing\nis sufficient to maintain calibration throughout the SFT process. However,\nsettings remain where the effectiveness of smoothing is severely diminished, in\nparticular the case of large vocabulary LLMs (LV-LLMs). We posit the cause to\nstem from the ability to become over-confident, which has a direct relationship\nwith the hidden size and vocabulary size, and justify this theoretically and\nexperimentally. Finally, we address an outstanding issue regarding the memory\nfootprint of the cross-entropy loss computation in the label smoothed loss\nsetting, designing a customized kernel to dramatically reduce memory\nconsumption without sacrificing speed or performance in comparison to existing\nsolutions for non-smoothed losses.", "comment": "Accepted to the Forty-second International Conference on Machine\n  Learning (ICML) 2025. First two authors contributed equally", "pdf_url": "http://arxiv.org/pdf/2508.00264v1", "cate": "cs.LG", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2504.04029", "title": "Simultaneous Motion And Noise Estimation with Event Cameras", "authors": ["Shintaro Shiba", "Yoshimitsu Aoki", "Guillermo Gallego"], "categories": ["cs.CV", "cs.AI", "eess.IV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      13 pages, 13 figures, 6 tables, Project page this https URL", "url": "http://arxiv.org/abs/2504.04029v2", "summary": "Event cameras are emerging vision sensors whose noise is challenging to\ncharacterize. Existing denoising methods for event cameras are often designed\nin isolation and thus consider other tasks, such as motion estimation,\nseparately (i.e., sequentially after denoising). However, motion is an\nintrinsic part of event data, since scene edges cannot be sensed without\nmotion. We propose, to the best of our knowledge, the first method that\nsimultaneously estimates motion in its various forms (e.g., ego-motion, optical\nflow) and noise. The method is flexible, as it allows replacing the one-step\nmotion estimation of the widely-used Contrast Maximization framework with any\nother motion estimator, such as deep neural networks. The experiments show that\nthe proposed method achieves state-of-the-art results on the E-MLB denoising\nbenchmark and competitive results on the DND21 benchmark, while demonstrating\neffectiveness across motion estimation and intensity reconstruction tasks. Our\napproach advances event-data denoising theory and expands practical denoising\nuse-cases via open-source code. Project page: https://github.com/tub-rip/ESMD", "comment": "13 pages, 13 figures, 6 tables, Project page\n  https://github.com/tub-rip/ESMD", "pdf_url": "http://arxiv.org/pdf/2504.04029v2", "cate": "cs.CV", "date": "2025-04-05", "updated": "2025-08-01"}
{"id": "2507.17224", "title": "HuiduRep: A Robust Self-Supervised Framework for Learning Neural Representations from Extracellular Recordings", "authors": ["Feng Cao", "Zishuo Feng", "Wei Shi", "Jicong Zhang"], "categories": ["eess.SP", "cs.AI", "q-bio.NC"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "Comments:      9 pages, 3 figures, 6 tables", "url": "http://arxiv.org/abs/2507.17224v2", "summary": "Extracellular recordings are transient voltage fluctuations in the vicinity\nof neurons, serving as a fundamental modality in neuroscience for decoding\nbrain activity at single-neuron resolution. Spike sorting, the process of\nattributing each detected spike to its corresponding neuron, is a pivotal step\nin brain sensing pipelines. However, it remains challenging under low\nsignal-to-noise ratio (SNR), electrode drift, and cross-session variability. In\nthis paper, we propose HuiduRep, a robust self-supervised representation\nlearning framework that extracts discriminative and generalizable features from\nextracellular recordings. By integrating contrastive learning with a denoising\nautoencoder, HuiduRep learns latent representations robust to noise and drift.\nWith HuiduRep, we develop a spike sorting pipeline that clusters spike\nrepresentations without ground truth labels. Experiments on hybrid and\nreal-world datasets demonstrate that HuiduRep achieves strong robustness.\nFurthermore, the pipeline significantly outperforms state-of-the-art tools such\nas KiloSort4 and MountainSort5 on accuracy and precision on diverse datasets.\nThese findings demonstrate the potential of self-supervised spike\nrepresentation learning as a foundational tool for robust and generalizable\nprocessing of extracellular recordings. Code is available at:\nhttps://github.com/IgarashiAkatuki/HuiduRep", "comment": "9 pages, 3 figures, 6 tables", "pdf_url": "http://arxiv.org/pdf/2507.17224v2", "cate": "eess.SP", "date": "2025-07-23", "updated": "2025-08-01"}
{"id": "2408.13392", "title": "A Multivariate Space-Time Dynamic Model for Characterizing the Atmospheric Impacts Following the Mt Pinatubo Eruptio", "authors": ["Robert Garrett", "Lyndsay Shand", "J. Gabriel Huerta"], "categories": ["stat.AP"], "primary_category": "Subjects:       Applications (stat.AP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2408.13392v2", "summary": "The June 1991 Mt. Pinatubo eruption resulted in a massive increase of sulfate\naerosols in the atmosphere, absorbing radiation and leading to global changes\nin surface and stratospheric temperatures. A volcanic eruption of this\nmagnitude serves as a natural analog for stratospheric aerosol injection, a\nproposed solar radiation modification method to combat a warming climate. The\nimpacts of such an event are multifaceted and region-specific. Our goal is to\ncharacterize the multivariate and dynamic nature of the atmospheric impacts\nfollowing the Mt. Pinatubo eruption. We developed a multivariate space-time\ndynamic linear model to understand the full extent of the spatially- and\ntemporally-varying impacts. Specifically, spatial variation is modeled using a\nflexible set of basis functions for which the basis coefficients are allowed to\nvary in time through a vector autoregressive (VAR) structure. This novel model\nis caste in a Dynamic Linear Model (DLM) framework and estimated via a\ncustomized MCMC approach. We demonstrate how the model quantifies the\nrelationships between key atmospheric parameters prior to and following the Mt.\nPinatubo eruption with reanalysis data from MERRA-2 and highlight when such\nmodel is advantageous over univariate models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2408.13392v2", "cate": "stat.AP", "date": "2024-08-23", "updated": "2025-08-01"}
{"id": "2507.20414", "title": "Indian Sign Language Detection for Real-Time Translation using Machine Learning", "authors": ["Rajat Singhal", "Jatin Gupta", "Akhil Sharma", "Anushka Gupta", "Navya Sharma"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      7 pages, 6 figures, 2 tables. Published in Proceedings of the 6th International Conference on Recent Advances in Information Technology (RAIT), 2025, IEEE", "url": "http://arxiv.org/abs/2507.20414v2", "summary": "Gestural language is used by deaf & mute communities to communicate through\nhand gestures & body movements that rely on visual-spatial patterns known as\nsign languages. Sign languages, which rely on visual-spatial patterns of hand\ngestures & body movements, are the primary mode of communication for deaf &\nmute communities worldwide. Effective communication is fundamental to human\ninteraction, yet individuals in these communities often face significant\nbarriers due to a scarcity of skilled interpreters & accessible translation\ntechnologies. This research specifically addresses these challenges within the\nIndian context by focusing on Indian Sign Language (ISL). By leveraging machine\nlearning, this study aims to bridge the critical communication gap for the deaf\n& hard-of-hearing population in India, where technological solutions for ISL\nare less developed compared to other global sign languages. We propose a\nrobust, real-time ISL detection & translation system built upon a Convolutional\nNeural Network (CNN). Our model is trained on a comprehensive ISL dataset &\ndemonstrates exceptional performance, achieving a classification accuracy of\n99.95%. This high precision underscores the model's capability to discern the\nnuanced visual features of different signs. The system's effectiveness is\nrigorously evaluated using key performance metrics, including accuracy, F1\nscore, precision & recall, ensuring its reliability for real-world\napplications. For real-time implementation, the framework integrates MediaPipe\nfor precise hand tracking & motion detection, enabling seamless translation of\ndynamic gestures. This paper provides a detailed account of the model's\narchitecture, the data preprocessing pipeline & the classification methodology.\nThe research elaborates the model architecture, preprocessing & classification\nmethodologies for enhancing communication in deaf & mute communities.", "comment": "7 pages, 6 figures, 2 tables. Published in Proceedings of the 6th\n  International Conference on Recent Advances in Information Technology (RAIT),\n  2025, IEEE", "pdf_url": "http://arxiv.org/pdf/2507.20414v2", "cate": "cs.CV", "date": "2025-07-27", "updated": "2025-07-31"}
{"id": "2508.00270", "title": "Learning to Optimize Feedback for One Million Students: Insights from Multi-Armed and Contextual Bandits in Large-Scale Online Tutoring", "authors": ["Robin Schmucker", "Nimish Pachapurkar", "Shanmuga Bala", "Miral Shah", "Tom Mitchell"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00270v1", "summary": "We present an online tutoring system that learns to provide effective\nfeedback to students after they answer questions incorrectly. Using data from\none million students, the system learns which assistance action (e.g., one of\nmultiple hints) to provide for each question to optimize student learning.\nEmploying the multi-armed bandit (MAB) framework and offline policy evaluation,\nwe assess 43,000 assistance actions, and identify trade-offs between assistance\npolicies optimized for different student outcomes (e.g., response correctness,\nsession completion). We design an algorithm that for each question decides on a\nsuitable policy training objective to enhance students' immediate second\nattempt success and overall practice session performance. We evaluate the\nresulting MAB policies in 166,000 practice sessions, verifying significant\nimprovements in student outcomes. While MAB policies optimize feedback for the\noverall student population, we further investigate whether contextual bandit\n(CB) policies can enhance outcomes by personalizing feedback based on\nindividual student features (e.g., ability estimates, response times). Using\ncausal inference, we examine (i) how effects of assistance actions vary across\nstudents and (ii) whether CB policies, which leverage such effect\nheterogeneity, outperform MAB policies. While our analysis reveals that some\nactions for some questions exhibit effect heterogeneity, effect sizes may often\nbe too small for CB policies to provide significant improvements beyond what\nwell-optimized MAB policies that deliver the same action to all students\nalready achieve. We discuss insights gained from deploying data-driven systems\nat scale and implications for future refinements. Today, the teaching policies\noptimized by our system support thousands of students daily.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00270v1", "cate": "cs.LG", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2507.21696", "title": "Edge Agentic AI Framework for Autonomous Network Optimisation in O-RAN", "authors": ["Abdelaziz Salama", "Zeinab Nezami", "Mohammed M. H. Qazzaz", "Maryam Hafeez", "Syed Ali Raza Zaidi"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21696v2", "summary": "The deployment of AI agents within legacy Radio Access Network (RAN)\ninfrastructure poses significant safety and reliability challenges for future\n6G networks. This paper presents a novel Edge AI framework for autonomous\nnetwork optimisation in Open RAN environments, addressing these challenges\nthrough three core innovations: (1) a persona-based multi-tools architecture\nenabling distributed, context-aware decision-making; (2) proactive anomaly\ndetection agent powered by traffic predictive tool; and (3) a safety, aligned\nreward mechanism that balances performance with operational stability.\nIntegrated into the RAN Intelligent Controller (RIC), our framework leverages\nmultimodal data fusion, including network KPIs, a traffic prediction model, and\nexternal information sources, to anticipate and respond to dynamic network\nconditions. Extensive evaluation using realistic 5G scenarios demonstrates that\nthe edge framework achieves zero network outages under high-stress conditions,\ncompared to 8.4% for traditional fixed-power networks and 3.3% for large\nlanguage model (LLM) agent-based approaches, while maintaining near real-time\nresponsiveness and consistent QoS. These results establish that, when equipped\nwith the right tools and contextual awareness, AI agents can be safely and\neffectively deployed in critical network infrastructure, laying the framework\nfor intelligent and autonomous 5G and beyond network operations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21696v2", "cate": "eess.SP", "date": "2025-07-29", "updated": "2025-08-01"}
{"id": "2504.21688", "title": "Assessing Racial Disparities in Healthcare Expenditures via Mediator Distribution Shifts", "authors": ["Xiaxian Ou", "Xinwei He", "David Benkeser", "Razieh Nabi"], "categories": ["stat.AP", "stat.ME", "stat.ML"], "primary_category": "Subjects:       Applications (stat.AP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.21688v2", "summary": "Racial disparities in healthcare expenditures are well-documented, yet the\nunderlying drivers remain complex and require further investigation. This study\ndevelops a framework for decomposing such disparities through shifts in the\ndistributions of mediating variables, rather than treating race itself as a\nmanipulable exposure. We define disparities as differences in\ncovariate-adjusted outcome distributions across racial groups, and decompose\nthe total disparity into two components: one attributable to differences in\nmediator distributions, and another residual component that would remain even\nafter equalizing these distributions. Using data from the Medical Expenditures\nPanel Survey, we examine the extent to which expenditure disparities would\npersist or be reduced if mediators such as socioeconomic status, insurance\naccess, health behaviors, or health status were equalized across racial groups.\nTo ensure valid inference, we derive asymptotically linear estimators based on\ninfluence-function techniques and flexible machine learning tools, including\nsuper learners and a two-part model designed for the zero-inflated,\nright-skewed nature of expenditure data.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.21688v2", "cate": "stat.AP", "date": "2025-04-30", "updated": "2025-08-01"}
{"id": "2507.20469", "title": "Priority-Aware Clinical Pathology Hierarchy Training for Multiple Instance Learning", "authors": ["Sungrae Hong", "Kyungeun Kim", "Juhyeon Kim", "Sol Lee", "Jisu Shin", "Chanjae Song", "Mun Yong Yi"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      10 pages, 4 figures, Accepted for oral presentation by The 2nd MICCAI Student Board (MSB) EMERGE Workshop", "url": "http://arxiv.org/abs/2507.20469v2", "summary": "Multiple Instance Learning (MIL) is increasingly being used as a support tool\nwithin clinical settings for pathological diagnosis decisions, achieving high\nperformance and removing the annotation burden. However, existing approaches\nfor clinical MIL tasks have not adequately addressed the priority issues that\nexist in relation to pathological symptoms and diagnostic classes, causing MIL\nmodels to ignore priority among classes. To overcome this clinical limitation\nof MIL, we propose a new method that addresses priority issues using two\nhierarchies: vertical inter-hierarchy and horizontal intra-hierarchy. The\nproposed method aligns MIL predictions across each hierarchical level and\nemploys an implicit feature re-usability during training to facilitate\nclinically more serious classes within the same level. Experiments with\nreal-world patient data show that the proposed method effectively reduces\nmisdiagnosis and prioritizes more important symptoms in multiclass scenarios.\nFurther analysis verifies the efficacy of the proposed components and\nqualitatively confirms the MIL predictions against challenging cases with\nmultiple symptoms.", "comment": "10 pages, 4 figures, Accepted for oral presentation by The 2nd MICCAI\n  Student Board (MSB) EMERGE Workshop", "pdf_url": "http://arxiv.org/pdf/2507.20469v2", "cate": "cs.CV", "date": "2025-07-28", "updated": "2025-07-31"}
{"id": "2508.00304", "title": "Invariant Graph Transformer for Out-of-Distribution Generalization", "authors": ["Tianyin Liao", "Ziwei Zhang", "Yufei Sun", "Chunyu Hu", "Jianxin Li"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00304v1", "summary": "Graph Transformers (GTs) have demonstrated great effectiveness across various\ngraph analytical tasks. However, the existing GTs focus on training and testing\ngraph data originated from the same distribution, but fail to generalize under\ndistribution shifts. Graph invariant learning, aiming to capture generalizable\ngraph structural patterns with labels under distribution shifts, is potentially\na promising solution, but how to design attention mechanisms and positional and\nstructural encodings (PSEs) based on graph invariant learning principles\nremains challenging. To solve these challenges, we introduce Graph\nOut-Of-Distribution generalized Transformer (GOODFormer), aiming to learn\ngeneralized graph representations by capturing invariant relationships between\npredictive graph structures and labels through jointly optimizing three\nmodules. Specifically, we first develop a GT-based entropy-guided invariant\nsubgraph disentangler to separate invariant and variant subgraphs while\npreserving the sharpness of the attention function. Next, we design an evolving\nsubgraph positional and structural encoder to effectively and efficiently\ncapture the encoding information of dynamically changing subgraphs during\ntraining. Finally, we propose an invariant learning module utilizing subgraph\nnode representations and encodings to derive generalizable graph\nrepresentations that can to unseen graphs. We also provide theoretical\njustifications for our method. Extensive experiments on benchmark datasets\ndemonstrate the superiority of our method over state-of-the-art baselines under\ndistribution shifts.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00304v1", "cate": "cs.LG", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2404.11790", "title": "Constrained Stochastic Recursive Momentum Successive Convex Approximation", "authors": ["Basil M. Idrees", "Lavish Arora", "Ketan Rajawat"], "categories": ["math.OC", "eess.SP"], "primary_category": "Subjects:       Optimization and Control (math.OC)", "pdf_link": null, "comments": "Comments:      32 pages, 4 figures, journal submission", "url": "http://arxiv.org/abs/2404.11790v4", "summary": "We consider stochastic optimization problems with non-convex functional\nconstraints, such as those arising in trajectory generation, sparse\napproximation, and robust classification. To this end, we put forth a recursive\nmomentum-based accelerated successive convex approximation (SCA) algorithm. At\neach iteration, the proposed algorithm entails constructing convex surrogates\nof the stochastic objective and the constraint functions, and solving the\nresulting convex optimization problem. A recursive update rule is employed to\ntrack the gradient of the stochastic objective function, which contributes to\nvariance reduction and hence accelerates the algorithm convergence. A key\ningredient of the proof is a new parameterized version of the standard\nMangasarian-Fromowitz Constraints Qualification, that allows us to bound the\ndual variables and hence obtain problem-dependent bounds on the rate at which\nthe iterates approach an $\\epsilon$-stationary point. Remarkably, the proposed\nalgorithm achieves near-optimal stochastic first-order (SFO) complexity with\nadaptive step sizes closely matching that achieved by state-of-the-art\nstochastic optimization algorithms for solving unconstrained problems. As an\nexample, we detail an obstacle-avoiding trajectory optimization problem that\ncan be solved using the proposed algorithm and show that its performance is\nsuperior to that of the existing algorithms used for trajectory optimization.\nThe performance of the proposed algorithm is also shown to be comparable to\nthat of a specialized sparse classification algorithm applied to a binary\nclassification problem.", "comment": "32 pages, 4 figures, journal submission", "pdf_url": "http://arxiv.org/pdf/2404.11790v4", "cate": "math.OC", "date": "2024-04-17", "updated": "2025-08-01"}
{"id": "2409.01908", "title": "Bayesian CART models for aggregate claim modeling", "authors": ["Yaojun Zhang", "Lanpeng Ji", "Georgios Aivaliotis", "Charles C. Taylor"], "categories": ["stat.ME", "cs.LG", "q-fin.ST", "stat.AP", "stat.ML"], "primary_category": "Subjects:       Methodology (stat.ME)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2409.01908v2", "summary": "This paper proposes three types of Bayesian CART (or BCART) models for\naggregate claim amount, namely, frequency-severity models, sequential models\nand joint models. We propose a general framework for the BCART models\napplicable to data with multivariate responses, which is particularly useful\nfor the joint BCART models with a bivariate response: the number of claims and\naggregate claim amount. To facilitate frequency-severity modeling, we\ninvestigate BCART models for the right-skewed and heavy-tailed claim severity\ndata by using various distributions. We discover that the Weibull distribution\nis superior to gamma and lognormal distributions, due to its ability to capture\ndifferent tail characteristics in tree models. Additionally, we find that\nsequential BCART models and joint BCART models, which incorporate dependence\nbetween the number of claims and average severity, are beneficial and thus\npreferable to the frequency-severity BCART models in which independence is\nassumed. The effectiveness of these models' performance is illustrated by\ncarefully designed simulations and real insurance data.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2409.01908v2", "cate": "stat.ME", "date": "2024-09-03", "updated": "2025-08-01"}
{"id": "2507.23392", "title": "Volatility Modeling with Rough Paths: A Signature-Based Alternative to Classical Expansions", "authors": ["Elisa Alòs", "Òscar Burés", "Rafael de Santiago", "Josep Vives"], "categories": ["q-fin.MF", "math.PR", "60L70, 60H10, 91G20, 91G60, 60G22"], "primary_category": "Subjects:       Mathematical Finance (q-fin.MF)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23392v2", "summary": "We compare two methodologies for calibrating implied volatility surfaces: a\nsecond-order asymptotic expansion method derived via Malliavin calculus, and a\ndata-driven approach based on path signatures from rough path theory. The\nformer, developed in Al\\`os et al. (2015), yields efficient and accurate\ncalibration formulas under the assumption that the asset price follows a\nHeston-type stochastic volatility model. The latter models volatility as a\nlinear functional of the signature of a primary stochastic process, enabling a\nflexible approximation without requiring a specific parametric form.\n  Our numerical experiments show that the signature-based method achieves\ncalibration accuracy comparable to the asymptotic approach when the true\ndynamics are Heston. We then test the model in a more general setting where the\nasset follows a rough Bergomi volatility process-a regime beyond the scope of\nthe asymptotic expansion-and show that the signature approach continues to\ndeliver accurate results. These findings highlight the model-independence,\nrobustness and adaptability of signature-based calibration methods in settings\nwhere volatility exhibits rough or non-Markovian features.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23392v2", "cate": "q-fin.MF", "date": "2025-07-31", "updated": "2025-08-01"}
{"id": "2507.21358", "title": "Collaborative Perceiver: Elevating Vision-based 3D Object Detection via Local Density-Aware Spatial Occupancy", "authors": ["Jicheng Yuan", "Manh Nguyen Duc", "Qian Liu", "Manfred Hauswirth", "Danh Le Phuoc"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      The manuscript has been accepted by ICONIP2025", "url": "http://arxiv.org/abs/2507.21358v3", "summary": "Vision-based bird's-eye-view (BEV) 3D object detection has advanced\nsignificantly in autonomous driving by offering cost-effectiveness and rich\ncontextual information. However, existing methods often construct BEV\nrepresentations by collapsing extracted object features, neglecting intrinsic\nenvironmental contexts, such as roads and pavements. This hinders detectors\nfrom comprehensively perceiving the characteristics of the physical world. To\nalleviate this, we introduce a multi-task learning framework, Collaborative\nPerceiver (CoP), that leverages spatial occupancy as auxiliary information to\nmine consistent structural and conceptual similarities shared between 3D object\ndetection and occupancy prediction tasks, bridging gaps in spatial\nrepresentations and feature refinement. To this end, we first propose a\npipeline to generate dense occupancy ground truths incorporating local density\ninformation (LDO) for reconstructing detailed environmental information. Next,\nwe employ a voxel-height-guided sampling (VHS) strategy to distill fine-grained\nlocal features according to distinct object properties. Furthermore, we develop\na global-local collaborative feature fusion (CFF) module that seamlessly\nintegrates complementary knowledge between both tasks, thus composing more\nrobust BEV representations. Extensive experiments on the nuScenes benchmark\ndemonstrate that CoP outperforms existing vision-based frameworks, achieving\n49.5\\% mAP and 59.2\\% NDS on the test set. Code and supplementary materials are\navailable at this link https://github.com/jichengyuan/Collaborative-Perceiver.", "comment": "The manuscript has been accepted by ICONIP2025", "pdf_url": "http://arxiv.org/pdf/2507.21358v3", "cate": "cs.CV", "date": "2025-07-28", "updated": "2025-07-31"}
{"id": "2508.00325", "title": "PnP-DA: Towards Principled Plug-and-Play Integration of Variational Data Assimilation and Generative Models", "authors": ["Yongquan Qu", "Matthieu Blanke", "Sara Shamekh", "Pierre Gentine"], "categories": ["cs.LG", "physics.comp-ph"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00325v1", "summary": "Earth system modeling presents a fundamental challenge in scientific\ncomputing: capturing complex, multiscale nonlinear dynamics in computationally\nefficient models while minimizing forecast errors caused by necessary\nsimplifications. Even the most powerful AI- or physics-based forecast system\nsuffer from gradual error accumulation. Data assimilation (DA) aims to mitigate\nthese errors by optimally blending (noisy) observations with prior model\nforecasts, but conventional variational methods often assume Gaussian error\nstatistics that fail to capture the true, non-Gaussian behavior of chaotic\ndynamical systems. We propose PnP-DA, a Plug-and-Play algorithm that alternates\n(1) a lightweight, gradient-based analysis update (using a Mahalanobis-distance\nmisfit on new observations) with (2) a single forward pass through a pretrained\ngenerative prior conditioned on the background forecast via a conditional\nWasserstein coupling. This strategy relaxes restrictive statistical assumptions\nand leverages rich historical data without requiring an explicit regularization\nfunctional, and it also avoids the need to backpropagate gradients through the\ncomplex neural network that encodes the prior during assimilation cycles.\nExperiments on standard chaotic testbeds demonstrate that this strategy\nconsistently reduces forecast errors across a range of observation sparsities\nand noise levels, outperforming classical variational methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00325v1", "cate": "cs.LG", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2410.02332", "title": "Polynomial time constructive decision algorithm for multivariable quantum signal processing", "authors": ["Yuki Ito", "Hitomi Mori", "Kazuki Sakamoto", "Keisuke Fujii"], "categories": ["quant-ph", "eess.SP"], "primary_category": "Subjects:       Quantum Physics (quant-ph)", "pdf_link": null, "comments": "Comments:      20 pages, 2 figures", "url": "http://arxiv.org/abs/2410.02332v2", "summary": "Quantum signal processing (QSP) and quantum singular value transformation\n(QSVT) have provided a unified framework for understanding many quantum\nalgorithms, including factorization, matrix inversion, and Hamiltonian\nsimulation. As a multivariable version of QSP, multivariable quantum signal\nprocessing (M-QSP) is proposed. M-QSP interleaves signal operators\ncorresponding to each variable with signal processing operators, which provides\nan efficient means to perform multivariable polynomial transformations.\nHowever, the necessary and sufficient condition for what types of polynomials\ncan be constructed by M-QSP is unknown. In this paper, we propose a classical\nalgorithm to determine whether a given pair of multivariable Laurent\npolynomials can be implemented by M-QSP, which returns True or False. As one of\nthe most important properties of this algorithm, it returning True is the\nnecessary and sufficient condition. The proposed classical algorithm runs in\npolynomial time in the number of variables and signal operators. Our algorithm\nalso provides a constructive method to select the necessary parameters for\nimplementing M-QSP. These findings offer valuable insights for identifying\npractical applications of M-QSP.", "comment": "20 pages, 2 figures", "pdf_url": "http://arxiv.org/pdf/2410.02332v2", "cate": "quant-ph", "date": "2024-10-03", "updated": "2025-08-01"}
{"id": "2506.23068", "title": "Curious Causality-Seeking Agents Learn Meta Causal World", "authors": ["Zhiyu Zhao", "Haoxuan Li", "Haifeng Zhang", "Jun Wang", "Francesco Faccio", "Jürgen Schmidhuber", "Mengyue Yang"], "categories": ["cs.LG", "cs.AI", "stat.AP"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      33 pages", "url": "http://arxiv.org/abs/2506.23068v2", "summary": "When building a world model, a common assumption is that the environment has\na single, unchanging underlying causal rule, like applying Newton's laws to\nevery situation. In reality, what appears as a drifting causal mechanism is\noften the manifestation of a fixed underlying mechanism seen through a narrow\nobservational window. This brings about a problem that, when building a world\nmodel, even subtle shifts in policy or environment states can alter the very\nobserved causal mechanisms. In this work, we introduce the \\textbf{Meta-Causal\nGraph} as world models, a minimal unified representation that efficiently\nencodes the transformation rules governing how causal structures shift across\ndifferent latent world states. A single Meta-Causal Graph is composed of\nmultiple causal subgraphs, each triggered by meta state, which is in the latent\nstate space. Building on this representation, we introduce a\n\\textbf{Causality-Seeking Agent} whose objectives are to (1) identify the meta\nstates that trigger each subgraph, (2) discover the corresponding causal\nrelationships by agent curiosity-driven intervention policy, and (3)\niteratively refine the Meta-Causal Graph through ongoing curiosity-driven\nexploration and agent experiences. Experiments on both synthetic tasks and a\nchallenging robot arm manipulation task demonstrate that our method robustly\ncaptures shifts in causal dynamics and generalizes effectively to previously\nunseen contexts.", "comment": "33 pages", "pdf_url": "http://arxiv.org/pdf/2506.23068v2", "cate": "cs.LG", "date": "2025-06-29", "updated": "2025-08-01"}
{"id": "2309.12014", "title": "Singular Control in a Cash Management Model with Ambiguity", "authors": ["Arnon Archankul", "Giorgio Ferrari", "Tobias Hellmann", "Jacco J. J. Thijssen"], "categories": ["q-fin.RM", "math.OC", "q-fin.MF"], "primary_category": "Subjects:       Risk Management (q-fin.RM)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2309.12014v2", "summary": "We consider a singular control model of cash reserve management, driven by a\ndiffusion under ambiguity. The manager is assumed to have maxmin preferences\nover a set of priors characterized by $\\kappa$-ignorance. A verification\ntheorem is established to determine the firm's cost function and the optimal\ncash policy; the latter taking the form of a control barrier policy. In a model\ndriven by arithmetic Brownian motion, we use Dynkin games to show that an\nincrease in ambiguity leads to higher expected costs under the worst-case prior\nand a narrower inaction region. The latter effect can be used to provide an\nambiguity-driven explanation for observed cash management behavior. Our\nfindings can be applied to broader applications of singular control in managing\ninventories under ambiguity.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2309.12014v2", "cate": "q-fin.RM", "date": "2023-09-21", "updated": "2025-08-01"}
{"id": "2507.21584", "title": "TARS: MinMax Token-Adaptive Preference Strategy for Hallucination Reduction in MLLMs", "authors": ["Kejia Zhang", "Keda Tao", "Zhiming Luo", "Chang Liu", "Jiasheng Tang", "Huan Wang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21584v2", "summary": "Multimodal large language models (MLLMs) enable vision-language reasoning,\nyet often generate plausible outputs that are factually incorrect or visually\nungrounded, thereby compromising their reliability. Direct preference\noptimization (DPO) is a common strategy for correcting hallucinations by\naligning model outputs with human preferences. Existing DPO strategies\ntypically treat hallucination-related preferences as fixed targets, relying on\nstatic supervision signals during training. This approach tends to overfit to\nsuperficial linguistic cues in preference data, leading to distributional\nrigidity and spurious correlations that impair grounding in causally relevant\nvisual information. To overcome this limitation, we propose TARS, a\ntoken-adaptive preference strategy that reformulates DPO as a min-max\noptimization problem. TARS maximizes token-level distributional shifts under\nsemantic constraints to simulate alignment uncertainty, and simultaneously\nminimizes the expected preference loss under these controlled perturbations.\nThis joint objective preserves causal grounding while mitigating overfitting to\npreference patterns, thereby reducing hallucinations in multimodal reasoning.\nWe evaluate TARS on multiple hallucination benchmarks and find consistently\nstrong performance. Using only 4.8k preference samples and no expert feedback,\nTARS reduces hallucination rates from 26.4% to 13.2% and decreases cognition\nvalue from 2.5 to 0.4. It outperforms standard DPO and matches GPT-4o on\nseveral key metrics.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21584v2", "cate": "cs.CV", "date": "2025-07-29", "updated": "2025-07-31"}
{"id": "2508.00331", "title": "Embryology of a Language Model", "authors": ["George Wang", "Garrett Baker", "Andrew Gordon", "Daniel Murfet"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00331v1", "summary": "Understanding how language models develop their internal computational\nstructure is a central problem in the science of deep learning. While\nsusceptibilities, drawn from statistical physics, offer a promising analytical\ntool, their full potential for visualizing network organization remains\nuntapped. In this work, we introduce an embryological approach, applying UMAP\nto the susceptibility matrix to visualize the model's structural development\nover training. Our visualizations reveal the emergence of a clear ``body\nplan,'' charting the formation of known features like the induction circuit and\ndiscovering previously unknown structures, such as a ``spacing fin'' dedicated\nto counting space tokens. This work demonstrates that susceptibility analysis\ncan move beyond validation to uncover novel mechanisms, providing a powerful,\nholistic lens for studying the developmental principles of complex neural\nnetworks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00331v1", "cate": "cs.LG", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2505.00292", "title": "Conformal changepoint localization", "authors": ["Sanjit Dandapanthula", "Aaditya Ramdas"], "categories": ["math.ST", "eess.SP", "stat.ME", "stat.TH"], "primary_category": "Subjects:       Statistics Theory (math.ST)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.00292v3", "summary": "Changepoint localization is the problem of estimating the index at which a\nchange occurred in the data generating distribution of an ordered list of data,\nor declaring that no change occurred. We present the broadly applicable CONCH\n(CONformal CHangepoint localization) algorithm, which uses a matrix of\nconformal p-values to produce a confidence interval for a (single) changepoint\nunder the mild assumption that the pre-change and post-change distributions are\neach exchangeable. We exemplify the CONCH algorithm on a variety of synthetic\nand real-world datasets, including using black-box pre-trained classifiers to\ndetect changes in sequences of images, text, and accelerometer data.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.00292v3", "cate": "math.ST", "date": "2025-05-01", "updated": "2025-08-01"}
{"id": "2507.22136", "title": "Color as the Impetus: Transforming Few-Shot Learner", "authors": ["Chaofei Qi", "Zhitai Liu", "Jianbin Qiu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22136v2", "summary": "Humans possess innate meta-learning capabilities, partly attributable to\ntheir exceptional color perception. In this paper, we pioneer an innovative\nviewpoint on few-shot learning by simulating human color perception mechanisms.\nWe propose the ColorSense Learner, a bio-inspired meta-learning framework that\ncapitalizes on inter-channel feature extraction and interactive learning. By\nstrategically emphasizing distinct color information across different channels,\nour approach effectively filters irrelevant features while capturing\ndiscriminative characteristics. Color information represents the most intuitive\nvisual feature, yet conventional meta-learning methods have predominantly\nneglected this aspect, focusing instead on abstract feature differentiation\nacross categories. Our framework bridges the gap via synergistic color-channel\ninteractions, enabling better intra-class commonality extraction and larger\ninter-class differences. Furthermore, we introduce a meta-distiller based on\nknowledge distillation, ColorSense Distiller, which incorporates prior teacher\nknowledge to augment the student network's meta-learning capacity. We've\nconducted comprehensive coarse/fine-grained and cross-domain experiments on\neleven few-shot benchmarks for validation. Numerous experiments reveal that our\nmethods have extremely strong generalization ability, robustness, and\ntransferability, and effortless handle few-shot classification from the\nperspective of color perception.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22136v2", "cate": "cs.CV", "date": "2025-07-29", "updated": "2025-07-31"}
{"id": "2508.00350", "title": "BOOD: Boundary-based Out-Of-Distribution Data Generation", "authors": ["Qilin Liao", "Shuo Yang", "Bo Zhao", "Ping Luo", "Hengshuang Zhao"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      14 pages, 8 figures, To be published in the Proceedings of the International Conference on Machine Learning (ICML) 2025", "url": "http://arxiv.org/abs/2508.00350v1", "summary": "Harnessing the power of diffusion models to synthesize auxiliary training\ndata based on latent space features has proven effective in enhancing\nout-of-distribution (OOD) detection performance. However, extracting effective\nfeatures outside the in-distribution (ID) boundary in latent space remains\nchallenging due to the difficulty of identifying decision boundaries between\nclasses. This paper proposes a novel framework called Boundary-based\nOut-Of-Distribution data generation (BOOD), which synthesizes high-quality OOD\nfeatures and generates human-compatible outlier images using diffusion models.\nBOOD first learns a text-conditioned latent feature space from the ID dataset,\nselects ID features closest to the decision boundary, and perturbs them to\ncross the decision boundary to form OOD features. These synthetic OOD features\nare then decoded into images in pixel space by a diffusion model. Compared to\nprevious works, BOOD provides a more training efficient strategy for\nsynthesizing informative OOD features, facilitating clearer distinctions\nbetween ID and OOD data. Extensive experimental results on common benchmarks\ndemonstrate that BOOD surpasses the state-of-the-art method significantly,\nachieving a 29.64% decrease in average FPR95 (40.31% vs. 10.67%) and a 7.27%\nimprovement in average AUROC (90.15% vs. 97.42%) on the CIFAR-100 dataset.", "comment": "14 pages, 8 figures, To be published in the Proceedings of the\n  International Conference on Machine Learning (ICML) 2025", "pdf_url": "http://arxiv.org/pdf/2508.00350v1", "cate": "cs.LG", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2507.22886", "title": "Towards Omnimodal Expressions and Reasoning in Referring Audio-Visual Segmentation", "authors": ["Kaining Ying", "Henghui Ding", "Guangquan Jie", "Yu-Gang Jiang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV 2025, Project Page: this https URL", "url": "http://arxiv.org/abs/2507.22886v2", "summary": "Referring audio-visual segmentation (RAVS) has recently seen significant\nadvancements, yet challenges remain in integrating multimodal information and\ndeeply understanding and reasoning about audiovisual content. To extend the\nboundaries of RAVS and facilitate future research in this field, we propose\nOmnimodal Referring Audio-Visual Segmentation (OmniAVS), a new dataset\ncontaining 2,104 videos and 61,095 multimodal referring expressions. OmniAVS\nstands out with three key innovations: (1) 8 types of multimodal expressions\nthat flexibly combine text, speech, sound, and visual cues; (2) an emphasis on\nunderstanding audio content beyond just detecting their presence; and (3) the\ninclusion of complex reasoning and world knowledge in expressions. Furthermore,\nwe introduce Omnimodal Instructed Segmentation Assistant (OISA), to address the\nchallenges of multimodal reasoning and fine-grained understanding of\naudiovisual content in OmniAVS. OISA uses MLLM to comprehend complex cues and\nperform reasoning-based segmentation. Extensive experiments show that OISA\noutperforms existing methods on OmniAVS and achieves competitive results on\nother related tasks.", "comment": "ICCV 2025, Project Page: https://henghuiding.com/OmniAVS/", "pdf_url": "http://arxiv.org/pdf/2507.22886v2", "cate": "cs.CV", "date": "2025-07-30", "updated": "2025-07-31"}
{"id": "2508.00357", "title": "Sheaf Graph Neural Networks via PAC-Bayes Spectral Optimization", "authors": ["Yoonhyuk Choi", "Jiho Choi", "Chong-Kwon Kim"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00357v1", "summary": "Over-smoothing in Graph Neural Networks (GNNs) causes collapse in distinct\nnode features, particularly on heterophilic graphs where adjacent nodes often\nhave dissimilar labels. Although sheaf neural networks partially mitigate this\nproblem, they typically rely on static or heavily parameterized sheaf\nstructures that hinder generalization and scalability. Existing sheaf-based\nmodels either predefine restriction maps or introduce excessive complexity, yet\nfail to provide rigorous stability guarantees. In this paper, we introduce a\nnovel scheme called SGPC (Sheaf GNNs with PAC-Bayes Calibration), a unified\narchitecture that combines cellular-sheaf message passing with several\nmechanisms, including optimal transport-based lifting, variance-reduced\ndiffusion, and PAC-Bayes spectral regularization for robust semi-supervised\nnode classification. We establish performance bounds theoretically and\ndemonstrate that the resulting bound-aware objective can be achieved via\nend-to-end training in linear computational complexity. Experiments on nine\nhomophilic and heterophilic benchmarks show that SGPC outperforms\nstate-of-the-art spectral and sheaf-based GNNs while providing certified\nconfidence intervals on unseen nodes.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00357v1", "cate": "cs.LG", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2306.01654", "title": "Insights into Closed-form IPM-GAN Discriminator Guidance for Diffusion Modeling", "authors": ["Aadithya Srikanth", "Siddarth Asokan", "Nishanth Shetty", "Chandra Sekhar Seelamantula"], "categories": ["cs.LG", "cs.CV", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2306.01654v2", "summary": "Diffusion models are a state-of-the-art generative modeling framework that\ntransform noise to images via Langevin sampling, guided by the score, which is\nthe gradient of the logarithm of the data distribution. Recent works have shown\nempirically that the generation quality can be improved when guided by\nclassifier network, which is typically the discriminator trained in a\ngenerative adversarial network (GAN) setting. In this paper, we propose a\ntheoretical framework to analyze the effect of the GAN discriminator on\nLangevin-based sampling, and show that the IPM-GAN optimization can be seen as\none of smoothed score-matching, wherein the scores of the data and the\ngenerator distributions are convolved with the kernel function associated with\nthe IPM. The proposed approach serves to unify score-based training and\noptimization of IPM-GANs. Based on these insights, we demonstrate that\nclosed-form kernel-based discriminator guidance, results in improvements (in\nterms of CLIP-FID and KID metrics) when applied atop baseline diffusion models.\nWe demonstrate these results on the denoising diffusion implicit model (DDIM)\nand latent diffusion model (LDM) settings on various standard datasets. We also\nshow that the proposed approach can be combined with existing\naccelerated-diffusion techniques to improve latent-space image generation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2306.01654v2", "cate": "cs.LG", "date": "2023-06-02", "updated": "2025-07-31"}
{"id": "2508.00364", "title": "OID-PPO: Optimal Interior Design using Proximal Policy Optimization by Transforming Design Guidelines into Reward Functions", "authors": ["Chanyoung Yoon", "Sangbong Yoo", "Soobin Yim", "Chansoo Kim", "Yun Jang"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00364v1", "summary": "Designing residential interiors strongly impacts occupant satisfaction but\nremains challenging due to unstructured spatial layouts, high computational\ndemands, and reliance on expert knowledge. Existing methods based on\noptimization or deep learning are either computationally expensive or\nconstrained by data scarcity. Reinforcement learning (RL) approaches often\nlimit furniture placement to discrete positions and fail to incorporate design\nprinciples adequately. We propose OID-PPO, a novel RL framework for Optimal\nInterior Design using Proximal Policy Optimization, which integrates\nexpert-defined functional and visual guidelines into a structured reward\nfunction. OID-PPO utilizes a diagonal Gaussian policy for continuous and\nflexible furniture placement, effectively exploring latent environmental\ndynamics under partial observability. Experiments conducted across diverse room\nshapes and furniture configurations demonstrate that OID-PPO significantly\noutperforms state-of-the-art methods in terms of layout quality and\ncomputational efficiency. Ablation studies further demonstrate the impact of\nstructured guideline integration and reveal the distinct contributions of\nindividual design constraints.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00364v1", "cate": "cs.LG", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2407.07720", "title": "Exploiting Scale-Variant Attention for Segmenting Small Medical Objects", "authors": ["Wei Dai", "Rui Liu", "Zixuan Wu", "Tianyi Wu", "Min Wang", "Junxian Zhou", "Yixuan Yuan", "Jun Liu"], "categories": ["eess.IV", "cs.CV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "Comments:      14 pages, 9 figures, under review", "url": "http://arxiv.org/abs/2407.07720v5", "summary": "Early detection and accurate diagnosis can predict the risk of malignant\ndisease transformation, thereby increasing the probability of effective\ntreatment. Identifying mild syndrome with small pathological regions serves as\nan ominous warning and is fundamental in the early diagnosis of diseases. While\ndeep learning algorithms, particularly convolutional neural networks (CNNs),\nhave shown promise in segmenting medical objects, analyzing small areas in\nmedical images remains challenging. This difficulty arises due to information\nlosses and compression defects from convolution and pooling operations in CNNs,\nwhich become more pronounced as the network deepens, especially for small\nmedical objects. To address these challenges, we propose a novel scale-variant\nattention-based network (SvANet) for accurately segmenting small-scale objects\nin medical images. The SvANet consists of scale-variant attention, cross-scale\nguidance, Monte Carlo attention, and vision transformer, which incorporates\ncross-scale features and alleviates compression artifacts for enhancing the\ndiscrimination of small medical objects. Quantitative experimental results\ndemonstrate the superior performance of SvANet, achieving 96.12%, 96.11%,\n89.79%, 84.15%, 80.25%, 73.05%, and 72.58% in mean Dice coefficient for\nsegmenting kidney tumors, skin lesions, hepatic tumors, polyps, surgical\nexcision cells, retinal vasculatures, and sperms, which occupy less than 1% of\nthe image areas in KiTS23, ISIC 2018, ATLAS, PolypGen, TissueNet, FIVES, and\nSpermHealth datasets, respectively.", "comment": "14 pages, 9 figures, under review", "pdf_url": "http://arxiv.org/pdf/2407.07720v5", "cate": "eess.IV", "date": "2024-07-10", "updated": "2025-07-31"}
{"id": "2508.00392", "title": "Dual Adaptivity: Universal Algorithms for Minimizing the Adaptive Regret of Convex Functions", "authors": ["Lijun Zhang", "Wenhao Yang", "Guanghui Wang", "Wei Jiang", "Zhi-Hua Zhou"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      arXiv admin note: text overlap with arXiv:1906.10851", "url": "http://arxiv.org/abs/2508.00392v1", "summary": "To deal with changing environments, a new performance measure -- adaptive\nregret, defined as the maximum static regret over any interval, was proposed in\nonline learning. Under the setting of online convex optimization, several\nalgorithms have been successfully developed to minimize the adaptive regret.\nHowever, existing algorithms lack universality in the sense that they can only\nhandle one type of convex functions and need apriori knowledge of parameters,\nwhich hinders their application in real-world scenarios. To address this\nlimitation, this paper investigates universal algorithms with dual adaptivity,\nwhich automatically adapt to the property of functions (convex, exponentially\nconcave, or strongly convex), as well as the nature of environments (stationary\nor changing). Specifically, we propose a meta-expert framework for dual\nadaptive algorithms, where multiple experts are created dynamically and\naggregated by a meta-algorithm. The meta-algorithm is required to yield a\nsecond-order bound, which can accommodate unknown function types. We further\nincorporate the technique of sleeping experts to capture the changing\nenvironments. For the construction of experts, we introduce two strategies\n(increasing the number of experts or enhancing the capabilities of experts) to\nachieve universality. Theoretical analysis shows that our algorithms are able\nto minimize the adaptive regret for multiple types of convex functions\nsimultaneously, and also allow the type of functions to switch between rounds.\nMoreover, we extend our meta-expert framework to online composite optimization,\nand develop a universal algorithm for minimizing the adaptive regret of\ncomposite functions.", "comment": "arXiv admin note: text overlap with arXiv:1906.10851", "pdf_url": "http://arxiv.org/pdf/2508.00392v1", "cate": "cs.LG", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2407.08722", "title": "Controlling diverse robots by inferring Jacobian fields with deep networks", "authors": ["Sizhe Lester Li", "Annan Zhang", "Boyuan Chen", "Hanna Matusik", "Chao Liu", "Daniela Rus", "Vincent Sitzmann"], "categories": ["cs.RO", "cs.CV", "cs.LG"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      Project Page: this https URL", "url": "http://arxiv.org/abs/2407.08722v2", "summary": "Mirroring the complex structures and diverse functions of natural organisms\nis a long-standing challenge in robotics. Modern fabrication techniques have\ngreatly expanded the feasible hardware, but using these systems requires\ncontrol software to translate the desired motions into actuator commands.\nConventional robots can easily be modeled as rigid links connected by joints,\nbut it remains an open challenge to model and control biologically inspired\nrobots that are often soft or made of several materials, lack sensing\ncapabilities, and may change their material properties with use. Here, we\nintroduce a method that uses deep neural networks to map a video stream of a\nrobot to its visuomotor Jacobian field (the sensitivity of all 3D points to the\nrobot's actuators). Our method enables the control of robots from only a single\ncamera, makes no assumptions about the robots' materials, actuation, or\nsensing, and is trained without expert intervention by observing the execution\nof random commands. We demonstrate our method on a diverse set of robot\nmanipulators that vary in actuation, materials, fabrication, and cost. Our\napproach achieves accurate closed-loop control and recovers the causal dynamic\nstructure of each robot. Because it enables robot control using a generic\ncamera as the only sensor, we anticipate that our work will broaden the design\nspace of robotic systems and serve as a starting point for lowering the barrier\nto robotic automation.", "comment": "Project Page:\n  https://sizhe-li.github.io/publication/neural_jacobian_field", "pdf_url": "http://arxiv.org/pdf/2407.08722v2", "cate": "cs.RO", "date": "2024-07-11", "updated": "2025-07-30"}
{"id": "2508.00394", "title": "ExeKGLib: A Platform for Machine Learning Analytics based on Knowledge Graphs", "authors": ["Antonis Klironomos", "Baifan Zhou", "Zhipeng Tan", "Zhuoxun Zheng", "Mohamed H. Gad-Elrab", "Heiko Paulheim", "Evgeny Kharlamov"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00394v1", "summary": "Nowadays machine learning (ML) practitioners have access to numerous ML\nlibraries available online. Such libraries can be used to create ML pipelines\nthat consist of a series of steps where each step may invoke up to several ML\nlibraries that are used for various data-driven analytical tasks. Development\nof high-quality ML pipelines is non-trivial; it requires training, ML\nexpertise, and careful development of each step. At the same time, domain\nexperts in science and engineering may not possess such ML expertise and\ntraining while they are in pressing need of ML-based analytics. In this paper,\nwe present our ExeKGLib, a Python library enhanced with a graphical interface\nlayer that allows users with minimal ML knowledge to build ML pipelines. This\nis achieved by relying on knowledge graphs that encode ML knowledge in simple\nterms accessible to non-ML experts. ExeKGLib also allows improving the\ntransparency and reusability of the built ML workflows and ensures that they\nare executable. We show the usability and usefulness of ExeKGLib by presenting\nreal use cases.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00394v1", "cate": "cs.LG", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2409.19490", "title": "KineDepth: Utilizing Robot Kinematics for Online Metric Depth Estimation", "authors": ["Soofiyan Atar", "Yuheng Zhi", "Florian Richter", "Michael Yip"], "categories": ["cs.RO", "cs.CV"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      8 pages, 5 figures", "url": "http://arxiv.org/abs/2409.19490v2", "summary": "Depth perception is essential for a robot's spatial and geometric\nunderstanding of its environment, with many tasks traditionally relying on\nhardware-based depth sensors like RGB-D or stereo cameras. However, these\nsensors face practical limitations, including issues with transparent and\nreflective objects, high costs, calibration complexity, spatial and energy\nconstraints, and increased failure rates in compound systems. While monocular\ndepth estimation methods offer a cost-effective and simpler alternative, their\nadoption in robotics is limited due to their output of relative rather than\nmetric depth, which is crucial for robotics applications. In this paper, we\npropose a method that utilizes a single calibrated camera, enabling the robot\nto act as a \"measuring stick\" to convert relative depth estimates into metric\ndepth in real-time as tasks are performed. Our approach employs an LSTM-based\nmetric depth regressor, trained online and refined through probabilistic\nfiltering, to accurately restore the metric depth across the monocular depth\nmap, particularly in areas proximal to the robot's motion. Experiments with\nreal robots demonstrate that our method significantly outperforms current\nstate-of-the-art monocular metric depth estimation techniques, achieving a\n22.1% reduction in depth error and a 52% increase in success rate for a\ndownstream task.", "comment": "8 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2409.19490v2", "cate": "cs.RO", "date": "2024-09-29", "updated": "2025-07-31"}
{"id": "2508.00410", "title": "Co-Reward: Self-supervised Reinforcement Learning for Large Language Model Reasoning via Contrastive Agreement", "authors": ["Zizhuo Zhang", "Jianing Zhu", "Xinmu Ge", "Zihua Zhao", "Zhanke Zhou", "Xuan Li", "Xiao Feng", "Jiangchao Yao", "Bo Han"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00410v1", "summary": "Although reinforcement learning with verifiable rewards (RLVR) shows promise\nin improving the reasoning ability of large language models (LLMs), the scaling\nup dilemma remains due to the reliance on human annotated labels especially for\ncomplex tasks. Recent alternatives that explore various self-reward signals\nexhibit the eliciting potential of LLM reasoning, but suffer from the\nnon-negligible collapse issue. Inspired by the success of self-supervised\nlearning, we propose \\textit{Co-Reward}, a novel RL framework that leverages\ncontrastive agreement across semantically analogical questions as a reward\nbasis. Specifically, we construct a similar question for each training sample\n(without labels) and synthesize their individual surrogate labels through a\nsimple rollout voting, and then the reward is constructed by cross-referring\nthe labels of each question pair to enforce the internal reasoning consistency\nacross analogical inputs. Intuitively, such a self-supervised reward-shaping\nmechanism increases the difficulty of learning collapse into a trivial\nsolution, and promotes stable reasoning elicitation and improvement through\nexpanding the input sample variants. Empirically, Co-Reward achieves superior\nperformance compared to other self-reward baselines on multiple reasoning\nbenchmarks and LLM series, and reaches or even surpasses ground-truth (GT)\nlabeled reward, with improvements of up to $+6.8\\%$ on MATH500 over GT reward\non Llama-3.2-3B-Instruct. Our code is publicly available at\nhttps://github.com/tmlr-group/Co-Reward.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00410v1", "cate": "cs.LG", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2412.13811", "title": "A Lightweight Optimization Framework for Estimating 3D Brain Tumor Infiltration", "authors": ["Jonas Weidner", "Michal Balcerak", "Ivan Ezhov", "André Datchev", "Laurin Lux", "Lucas Zimmer", "Daniel Rueckert", "Björn Menze", "Benedikt Wiestler"], "categories": ["physics.med-ph", "cs.CV"], "primary_category": "Subjects:       Medical Physics (physics.med-ph)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2412.13811v2", "summary": "Glioblastoma, the most aggressive primary brain tumor, poses a severe\nclinical challenge due to its diffuse microscopic infiltration, which remains\nlargely undetected on standard MRI. As a result, current radiotherapy planning\nemploys a uniform 15 mm margin around the resection cavity, failing to capture\npatient-specific tumor spread. Tumor growth modeling offers a promising\napproach to reveal this hidden infiltration. However, methods based on partial\ndifferential equations or physics-informed neural networks tend to be\ncomputationally intensive or overly constrained, limiting their clinical\nadaptability to individual patients. In this work, we propose a lightweight,\nrapid, and robust optimization framework that estimates the 3D tumor\nconcentration by fitting it to MRI tumor segmentations while enforcing a smooth\nconcentration landscape. This approach achieves superior tumor recurrence\nprediction on 192 brain tumor patients across two public datasets,\noutperforming state-of-the-art baselines while reducing runtime from 30 minutes\nto less than one minute. Furthermore, we demonstrate the framework's\nversatility and adaptability by showing its ability to seamlessly integrate\nadditional imaging modalities or physical constraints.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2412.13811v2", "cate": "physics.med-ph", "date": "2024-12-18", "updated": "2025-07-31"}
{"id": "2508.00415", "title": "Transforming Credit Risk Analysis: A Time-Series-Driven ResE-BiLSTM Framework for Post-Loan Default Detection", "authors": ["Yue Yang", "Yuxiang Lin", "Ying Zhang", "Zihan Su", "Chang Chuan Goh", "Tangtangfang Fang", "Anthony Graham Bellotti", "Boon Giin Lee"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00415v1", "summary": "Prediction of post-loan default is an important task in credit risk\nmanagement, and can be addressed by detection of financial anomalies using\nmachine learning. This study introduces a ResE-BiLSTM model, using a sliding\nwindow technique, and is evaluated on 44 independent cohorts from the extensive\nFreddie Mac US mortgage dataset, to improve prediction performance. The\nResE-BiLSTM is compared with five baseline models: Long Short-Term Memory\n(LSTM), BiLSTM, Gated Recurrent Units (GRU), Convolutional Neural Networks\n(CNN), and Recurrent Neural Networks (RNN), across multiple metrics, including\nAccuracy, Precision, Recall, F1, and AUC. An ablation study was conducted to\nevaluate the contribution of individual components in the ResE-BiLSTM\narchitecture. Additionally, SHAP analysis was employed to interpret the\nunderlying features the model relied upon for its predictions. Experimental\nresults demonstrate that ResE-BiLSTM achieves superior predictive performance\ncompared to baseline models, underscoring its practical value and applicability\nin real-world scenarios.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00415v1", "cate": "cs.LG", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2503.05911", "title": "Generalizable Image Repair for Robust Visual Control", "authors": ["Carson Sobolewski", "Zhenjiang Mao", "Kshitij Maruti Vejre", "Ivan Ruchkin"], "categories": ["cs.RO", "cs.CV"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      8 pages, 4 figures, 2 tables, 2025 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2025)", "url": "http://arxiv.org/abs/2503.05911v2", "summary": "Vision-based control relies on accurate perception to achieve robustness.\nHowever, image distribution changes caused by sensor noise, adverse weather,\nand dynamic lighting can degrade perception, leading to suboptimal control\ndecisions. Existing approaches, including domain adaptation and adversarial\ntraining, improve robustness but struggle to generalize to unseen corruptions\nwhile introducing computational overhead. To address this challenge, we propose\na real-time image repair module that restores corrupted images before they are\nused by the controller. Our method leverages generative adversarial models,\nspecifically CycleGAN and pix2pix, for image repair. CycleGAN enables unpaired\nimage-to-image translation to adapt to novel corruptions, while pix2pix\nexploits paired image data when available to improve the quality. To ensure\nalignment with control performance, we introduce a control-focused loss\nfunction that prioritizes perceptual consistency in repaired images. We\nevaluated our method in a simulated autonomous racing environment with various\nvisual corruptions. The results show that our approach significantly improves\nperformance compared to baselines, mitigating distribution shift and enhancing\ncontroller reliability.", "comment": "8 pages, 4 figures, 2 tables, 2025 IEEE/RSJ International Conference\n  on Intelligent Robots and Systems (IROS 2025)", "pdf_url": "http://arxiv.org/pdf/2503.05911v2", "cate": "cs.RO", "date": "2025-03-07", "updated": "2025-07-31"}
{"id": "2508.00472", "title": "A Conditional GAN for Tabular Data Generation with Probabilistic Sampling of Latent Subspaces", "authors": ["Leonidas Akritidis", "Panayiotis Bozanis"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00472v1", "summary": "The tabular form constitutes the standard way of representing data in\nrelational database systems and spreadsheets. But, similarly to other forms,\ntabular data suffers from class imbalance, a problem that causes serious\nperformance degradation in a wide variety of machine learning tasks. One of the\nmost effective solutions dictates the usage of Generative Adversarial Networks\n(GANs) in order to synthesize artificial data instances for the\nunder-represented classes. Despite their good performance, none of the proposed\nGAN models takes into account the vector subspaces of the input samples in the\nreal data space, leading to data generation in arbitrary locations. Moreover,\nthe class labels are treated in the same manner as the other categorical\nvariables during training, so conditional sampling by class is rendered less\neffective. To overcome these problems, this study presents ctdGAN, a\nconditional GAN for alleviating class imbalance in tabular datasets. Initially,\nctdGAN executes a space partitioning step to assign cluster labels to the input\nsamples. Subsequently, it utilizes these labels to synthesize samples via a\nnovel probabilistic sampling strategy and a new loss function that penalizes\nboth cluster and class mis-predictions. In this way, ctdGAN is trained to\ngenerate samples in subspaces that resemble those of the original data\ndistribution. We also introduce several other improvements, including a simple,\nyet effective cluster-wise scaling technique that captures multiple feature\nmodes without affecting data dimensionality. The exhaustive evaluation of\nctdGAN with 14 imbalanced datasets demonstrated its superiority in generating\nhigh fidelity samples and improving classification accuracy.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00472v1", "cate": "cs.LG", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2503.17564", "title": "ModalTune: Fine-Tuning Slide-Level Foundation Models with Multi-Modal Information for Multi-task Learning in Digital Pathology", "authors": ["Vishwesh Ramanathan", "Tony Xu", "Pushpak Pati", "Faruk Ahmed", "Maged Goubran", "Anne L. Martel"], "categories": ["eess.IV", "cs.CV", "cs.LG"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.17564v2", "summary": "Prediction tasks in digital pathology are challenging due to the massive size\nof whole-slide images (WSIs) and the weak nature of training signals. Advances\nin computing, data availability, and self-supervised learning (SSL) have paved\nthe way for slide-level foundation models (SLFMs) that can improve prediction\ntasks in low-data regimes. However, current methods under-utilize shared\ninformation between tasks and modalities. To overcome this challenge, we\npropose ModalTune, a novel fine-tuning framework which introduces the Modal\nAdapter to integrate new modalities without modifying SLFM weights.\nAdditionally, we use large-language models (LLMs) to encode labels as text,\ncapturing semantic relationships across multiple tasks and cancer types in a\nsingle training recipe. ModalTune achieves state-of-the-art (SOTA) results\nagainst both uni-modal and multi-modal models across four cancer types, jointly\nimproving survival and cancer subtype prediction while remaining competitive in\npan-cancer settings. Additionally, we show ModalTune is generalizable to two\nout-of-distribution (OOD) datasets. To our knowledge, this is the first unified\nfine-tuning framework for multi-modal, multi-task, and pan-cancer modeling in\ndigital pathology.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.17564v2", "cate": "eess.IV", "date": "2025-03-21", "updated": "2025-07-30"}
{"id": "2508.00507", "title": "Court of LLMs: Evidence-Augmented Generation via Multi-LLM Collaboration for Text-Attributed Graph Anomaly Detection", "authors": ["Yiming Xu", "Jiarun Chen", "Zhen Peng", "Zihan Chen", "Qika Lin", "Lan Ma", "Bin Shi", "Bo Dong"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted by ACM Multimedia 2025 (MM '25)", "url": "http://arxiv.org/abs/2508.00507v1", "summary": "The natural combination of intricate topological structures and rich textual\ninformation in text-attributed graphs (TAGs) opens up a novel perspective for\ngraph anomaly detection (GAD). However, existing GAD methods primarily focus on\ndesigning complex optimization objectives within the graph domain, overlooking\nthe complementary value of the textual modality, whose features are often\nencoded by shallow embedding techniques, such as bag-of-words or skip-gram, so\nthat semantic context related to anomalies may be missed. To unleash the\nenormous potential of textual modality, large language models (LLMs) have\nemerged as promising alternatives due to their strong semantic understanding\nand reasoning capabilities. Nevertheless, their application to TAG anomaly\ndetection remains nascent, and they struggle to encode high-order structural\ninformation inherent in graphs due to input length constraints. For\nhigh-quality anomaly detection in TAGs, we propose CoLL, a novel framework that\ncombines LLMs and graph neural networks (GNNs) to leverage their complementary\nstrengths. CoLL employs multi-LLM collaboration for evidence-augmented\ngeneration to capture anomaly-relevant contexts while delivering human-readable\nrationales for detected anomalies. Moreover, CoLL integrates a GNN equipped\nwith a gating mechanism to adaptively fuse textual features with evidence while\npreserving high-order topological information. Extensive experiments\ndemonstrate the superiority of CoLL, achieving an average improvement of 13.37%\nin AP. This study opens a new avenue for incorporating LLMs in advancing GAD.", "comment": "Accepted by ACM Multimedia 2025 (MM '25)", "pdf_url": "http://arxiv.org/pdf/2508.00507v1", "cate": "cs.LG", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2504.02439", "title": "Estimating Scene Flow in Robot Surroundings with Distributed Miniaturized Time-of-Flight Sensors", "authors": ["Jack Sander", "Giammarco Caroleo", "Alessandro Albini", "Perla Maiolino"], "categories": ["cs.RO", "cs.CV"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      7 pages, 5 figures, 2 tables, 1 algorithm, IEEE RO-MAN 2025 accepted paper", "url": "http://arxiv.org/abs/2504.02439v2", "summary": "Tracking motions of humans or objects in the surroundings of the robot is\nessential to improve safe robot motions and reactions. In this work, we present\nan approach for scene flow estimation from low-density and noisy point clouds\nacquired from miniaturized Time of Flight (ToF) sensors distributed on the\nrobot body. The proposed method clusters points from consecutive frames and\napplies Iterative Closest Point (ICP) to estimate a dense motion flow, with\nadditional steps introduced to mitigate the impact of sensor noise and\nlow-density data points. Specifically, we employ a fitness-based classification\nto distinguish between stationary and moving points and an inlier removal\nstrategy to refine geometric correspondences. The proposed approach is\nvalidated in an experimental setup where 24 ToF are used to estimate the\nvelocity of an object moving at different controlled speeds. Experimental\nresults show that the method consistently approximates the direction of the\nmotion and its magnitude with an error which is in line with sensor noise.", "comment": "7 pages, 5 figures, 2 tables, 1 algorithm, IEEE RO-MAN 2025 accepted\n  paper", "pdf_url": "http://arxiv.org/pdf/2504.02439v2", "cate": "cs.RO", "date": "2025-04-03", "updated": "2025-07-31"}
{"id": "2508.00513", "title": "Text-Attributed Graph Anomaly Detection via Multi-Scale Cross- and Uni-Modal Contrastive Learning", "authors": ["Yiming Xu", "Xu Hua", "Zhen Peng", "Bin Shi", "Jiarun Chen", "Xingbo Fu", "Song Wang", "Bo Dong"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted by ECAI 2025", "url": "http://arxiv.org/abs/2508.00513v1", "summary": "The widespread application of graph data in various high-risk scenarios has\nincreased attention to graph anomaly detection (GAD). Faced with real-world\ngraphs that often carry node descriptions in the form of raw text sequences,\ntermed text-attributed graphs (TAGs), existing graph anomaly detection\npipelines typically involve shallow embedding techniques to encode such textual\ninformation into features, and then rely on complex self-supervised tasks\nwithin the graph domain to detect anomalies. However, this text encoding\nprocess is separated from the anomaly detection training objective in the graph\ndomain, making it difficult to ensure that the extracted textual features focus\non GAD-relevant information, seriously constraining the detection capability.\nHow to seamlessly integrate raw text and graph topology to unleash the vast\npotential of cross-modal data in TAGs for anomaly detection poses a challenging\nissue. This paper presents a novel end-to-end paradigm for text-attributed\ngraph anomaly detection, named CMUCL. We simultaneously model data from both\ntext and graph structures, and jointly train text and graph encoders by\nleveraging cross-modal and uni-modal multi-scale consistency to uncover\npotential anomaly-related information. Accordingly, we design an anomaly score\nestimator based on inconsistency mining to derive node-specific anomaly scores.\nConsidering the lack of benchmark datasets tailored for anomaly detection on\nTAGs, we release 8 datasets to facilitate future research. Extensive\nevaluations show that CMUCL significantly advances in text-attributed graph\nanomaly detection, delivering an 11.13% increase in average accuracy (AP) over\nthe suboptimal.", "comment": "Accepted by ECAI 2025", "pdf_url": "http://arxiv.org/pdf/2508.00513v1", "cate": "cs.LG", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2506.03956", "title": "Adapt before Continual Learning", "authors": ["Aojun Lu", "Tao Feng", "Hangjie Yuan", "Chunhui Ding", "Yanan Sun"], "categories": ["cs.LG", "cs.CV"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.03956v3", "summary": "Continual Learning (CL) seeks to enable neural networks to incrementally\nacquire new knowledge (plasticity) while retaining existing knowledge\n(stability). Although pre-trained models (PTMs) have provided a strong\nfoundation for CL, existing approaches face a fundamental challenge in\nbalancing these two competing objectives. Current methods typically address\nstability by freezing the PTM backbone, which severely limits the model's\nplasticity, particularly when incoming data distribution diverges largely from\nthe pre-training data. Alternatively, sequentially fine-tuning the entire PTM\ncan adapt to new knowledge but often leads to catastrophic forgetting,\nhighlighting the critical stability-plasticity trade-off in PTM-based CL. To\naddress this limitation, we propose Adapting PTMs before the core CL} process\n(ACL), a novel framework that introduces a plug-and-play adaptation phase prior\nto learning each new task. During this phase, ACL refines the PTM backbone by\naligning embeddings with their original class prototypes while distancing them\nfrom irrelevant classes. This mechanism theoretically and empirically\ndemonstrates desirable balance between stability and plasticity, significantly\nimproving CL performance across benchmarks and integrated methods. Code is\navailable at https://github.com/byyx666/ACL_code.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.03956v3", "cate": "cs.LG", "date": "2025-06-04", "updated": "2025-07-31"}
{"id": "2508.00523", "title": "Online Nonsubmodular Optimization with Delayed Feedback in the Bandit Setting", "authors": ["Sifan Yang", "Yuanyu Wan", "Lijun Zhang"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00523v1", "summary": "We investigate the online nonsubmodular optimization with delayed feedback in\nthe bandit setting, where the loss function is $\\alpha$-weakly DR-submodular\nand $\\beta$-weakly DR-supermodular. Previous work has established an\n$(\\alpha,\\beta)$-regret bound of $\\mathcal{O}(nd^{1/3}T^{2/3})$, where $n$ is\nthe dimensionality and $d$ is the maximum delay. However, its regret bound\nrelies on the maximum delay and is thus sensitive to irregular delays.\nAdditionally, it couples the effects of delays and bandit feedback as its bound\nis the product of the delay term and the $\\mathcal{O}(nT^{2/3})$ regret bound\nin the bandit setting without delayed feedback. In this paper, we develop two\nalgorithms to address these limitations, respectively. Firstly, we propose a\nnovel method, namely DBGD-NF, which employs the one-point gradient estimator\nand utilizes all the available estimated gradients in each round to update the\ndecision. It achieves a better $\\mathcal{O}(n\\bar{d}^{1/3}T^{2/3})$ regret\nbound, which is relevant to the average delay $\\bar{d} =\n\\frac{1}{T}\\sum_{t=1}^T d_t\\leq d$. Secondly, we extend DBGD-NF by employing a\nblocking update mechanism to decouple the joint effect of the delays and bandit\nfeedback, which enjoys an $\\mathcal{O}(n(T^{2/3} + \\sqrt{dT}))$ regret bound.\nWhen $d = \\mathcal{O}(T^{1/3})$, our regret bound matches the\n$\\mathcal{O}(nT^{2/3})$ bound in the bandit setting without delayed feedback.\nCompared to our first $\\mathcal{O}(n\\bar{d}^{1/3}T^{2/3})$ bound, it is more\nadvantageous when the maximum delay $d = o(\\bar{d}^{2/3}T^{1/3})$. Finally, we\nconduct experiments on structured sparse learning to demonstrate the\nsuperiority of our methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00523v1", "cate": "cs.LG", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00539", "title": "Phase-Locked SNR Band Selection for Weak Mineral Signal Detection in Hyperspectral Imagery", "authors": ["Judy X Yang"], "categories": ["cs.LG", "Cs"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      8 pages, 6 figures", "url": "http://arxiv.org/abs/2508.00539v1", "summary": "Hyperspectral imaging offers detailed spectral information for mineral\nmapping; however, weak mineral signatures are often masked by noisy and\nredundant bands, limiting detection performance. To address this, we propose a\ntwo-stage integrated framework for enhanced mineral detection in the Cuprite\nmining district. In the first stage, we compute the signal-to-noise ratio (SNR)\nfor each spectral band and apply a phase-locked thresholding technique to\ndiscard low-SNR bands, effectively removing redundancy and suppressing\nbackground noise. Savitzky-Golay filtering is then employed for spectral\nsmoothing, serving a dual role first to stabilize trends during band selection,\nand second to preserve fine-grained spectral features during preprocessing. In\nthe second stage, the refined HSI data is reintroduced into the model, where\nKMeans clustering is used to extract 12 endmember spectra (W1 custom), followed\nby non negative least squares (NNLS) for abundance unmixing. The resulting\nendmembers are quantitatively compared with laboratory spectra (W1 raw) using\ncosine similarity and RMSE metrics. Experimental results confirm that our\nproposed pipeline improves unmixing accuracy and enhances the detection of weak\nmineral zones. This two-pass strategy demonstrates a practical and reproducible\nsolution for spectral dimensionality reduction and unmixing in geological HSI\napplications.", "comment": "8 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2508.00539v1", "cate": "cs.LG", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00578", "title": "Learning Potential Energy Surfaces of Hydrogen Atom Transfer Reactions in Peptides", "authors": ["Marlen Neubert", "Patrick Reiser", "Frauke Gräter", "Pascal Friederich"], "categories": ["cs.LG", "cond-mat.mtrl-sci", "physics.chem-ph", "physics.comp-ph", "q-bio.BM"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      19 pages, 12 figures, and 4 tables (references and SI included)", "url": "http://arxiv.org/abs/2508.00578v1", "summary": "Hydrogen atom transfer (HAT) reactions are essential in many biological\nprocesses, such as radical migration in damaged proteins, but their mechanistic\npathways remain incompletely understood. Simulating HAT is challenging due to\nthe need for quantum chemical accuracy at biologically relevant scales; thus,\nneither classical force fields nor DFT-based molecular dynamics are applicable.\nMachine-learned potentials offer an alternative, able to learn potential energy\nsurfaces (PESs) with near-quantum accuracy. However, training these models to\ngeneralize across diverse HAT configurations, especially at radical positions\nin proteins, requires tailored data generation and careful model selection.\nHere, we systematically generate HAT configurations in peptides to build large\ndatasets using semiempirical methods and DFT. We benchmark three graph neural\nnetwork architectures (SchNet, Allegro, and MACE) on their ability to learn HAT\nPESs and indirectly predict reaction barriers from energy predictions. MACE\nconsistently outperforms the others in energy, force, and barrier prediction,\nachieving a mean absolute error of 1.13 kcal/mol on out-of-distribution DFT\nbarrier predictions. This accuracy enables integration of ML potentials into\nlarge-scale collagen simulations to compute reaction rates from predicted\nbarriers, advancing mechanistic understanding of HAT and radical migration in\npeptides. We analyze scaling laws, model transferability, and cost-performance\ntrade-offs, and outline strategies for improvement by combining ML potentials\nwith transition state search algorithms and active learning. Our approach is\ngeneralizable to other biomolecular systems, enabling quantum-accurate\nsimulations of chemical reactivity in complex environments.", "comment": "19 pages, 12 figures, and 4 tables (references and SI included)", "pdf_url": "http://arxiv.org/pdf/2508.00578v1", "cate": "cs.LG", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00586", "title": "The Role of Active Learning in Modern Machine Learning", "authors": ["Thorben Werner", "Lars Schmidt-Thieme", "Vijaya Krishna Yalavarthi"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00586v1", "summary": "Even though Active Learning (AL) is widely studied, it is rarely applied in\ncontexts outside its own scientific literature. We posit that the reason for\nthis is AL's high computational cost coupled with the comparatively small lifts\nit is typically able to generate in scenarios with few labeled points. In this\nwork we study the impact of different methods to combat this low data scenario,\nnamely data augmentation (DA), semi-supervised learning (SSL) and AL. We find\nthat AL is by far the least efficient method of solving the low data problem,\ngenerating a lift of only 1-4\\% over random sampling, while DA and SSL methods\ncan generate up to 60\\% lift in combination with random sampling. However, when\nAL is combined with strong DA and SSL techniques, it surprisingly is still able\nto provide improvements. Based on these results, we frame AL not as a method to\ncombat missing labels, but as the final building block to squeeze the last bits\nof performance out of data after appropriate DA and SSL methods as been\napplied.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00586v1", "cate": "cs.LG", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00615", "title": "Similarity-Based Self-Construct Graph Model for Predicting Patient Criticalness Using Graph Neural Networks and EHR Data", "authors": ["Mukesh Kumar Sahu", "Pinki Roy"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00615v1", "summary": "Accurately predicting the criticalness of ICU patients (such as in-ICU\nmortality risk) is vital for early intervention in critical care. However,\nconventional models often treat each patient in isolation and struggle to\nexploit the relational structure in Electronic Health Records (EHR). We propose\na Similarity-Based Self-Construct Graph Model (SBSCGM) that dynamically builds\na patient similarity graph from multi-modal EHR data, and a HybridGraphMedGNN\narchitecture that operates on this graph to predict patient mortality and a\ncontinuous criticalness score. SBSCGM uses a hybrid similarity measure\n(combining feature-based and structural similarities) to connect patients with\nanalogous clinical profiles in real-time. The HybridGraphMedGNN integrates\nGraph Convolutional Network (GCN), GraphSAGE, and Graph Attention Network (GAT)\nlayers to learn robust patient representations, leveraging both local and\nglobal graph patterns. In experiments on 6,000 ICU stays from the MIMIC-III\ndataset, our model achieves state-of-the-art performance (AUC-ROC $0.94$)\noutperforming baseline classifiers and single-type GNN models. We also\ndemonstrate improved precision/recall and show that the attention mechanism\nprovides interpretable insights into model predictions. Our framework offers a\nscalable and interpretable solution for critical care risk prediction, with\npotential to support clinicians in real-world ICU deployment.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00615v1", "cate": "cs.LG", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00627", "title": "IAMAP: Unlocking Deep Learning in QGIS for non-coders and limited computing resources", "authors": ["Paul Tresson", "Pierre Le Coz", "Hadrien Tulet", "Anthony Malkassian", "Maxime Réjou Méchain"], "categories": ["cs.LG", "I.4.9; I.4.6"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      11 pages, 5 figures", "url": "http://arxiv.org/abs/2508.00627v1", "summary": "Remote sensing has entered a new era with the rapid development of artificial\nintelligence approaches. However, the implementation of deep learning has\nlargely remained restricted to specialists and has been impractical because it\noften requires (i) large reference datasets for model training and validation;\n(ii) substantial computing resources; and (iii) strong coding skills. Here, we\nintroduce IAMAP, a user-friendly QGIS plugin that addresses these three\nchallenges in an easy yet flexible way. IAMAP builds on recent advancements in\nself-supervised learning strategies, which now provide robust feature\nextractors, often referred to as foundation models. These generalist models can\noften be reliably used in few-shot or zero-shot scenarios (i.e., with little to\nno fine-tuning). IAMAP's interface allows users to streamline several key steps\nin remote sensing image analysis: (i) extracting image features using a wide\nrange of deep learning architectures; (ii) reducing dimensionality with\nbuilt-in algorithms; (iii) performing clustering on features or their reduced\nrepresentations; (iv) generating feature similarity maps; and (v) calibrating\nand validating supervised machine learning models for prediction. By enabling\nnon-AI specialists to leverage the high-quality features provided by recent\ndeep learning approaches without requiring GPU capacity or extensive reference\ndatasets, IAMAP contributes to the democratization of computationally efficient\nand energy-conscious deep learning methods.", "comment": "11 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2508.00627v1", "cate": "cs.LG", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00628", "title": "Separated-Variable Spectral Neural Networks: A Physics-Informed Learning Approach for High-Frequency PDEs", "authors": ["Xiong Xiong", "Zhuo Zhang", "Rongchun Hu", "Chen Gao", "Zichen Deng"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00628v1", "summary": "Solving high-frequency oscillatory partial differential equations (PDEs) is a\ncritical challenge in scientific computing, with applications in fluid\nmechanics, quantum mechanics, and electromagnetic wave propagation. Traditional\nphysics-informed neural networks (PINNs) suffer from spectral bias, limiting\ntheir ability to capture high-frequency solution components. We introduce\nSeparated-Variable Spectral Neural Networks (SV-SNN), a novel framework that\naddresses these limitations by integrating separation of variables with\nadaptive spectral methods. Our approach features three key innovations: (1)\ndecomposition of multivariate functions into univariate function products,\nenabling independent spatial and temporal networks; (2) adaptive Fourier\nspectral features with learnable frequency parameters for high-frequency\ncapture; and (3) theoretical framework based on singular value decomposition to\nquantify spectral bias. Comprehensive evaluation on benchmark problems\nincluding Heat equation, Helmholtz equation, Poisson equations and\nNavier-Stokes equations demonstrates that SV-SNN achieves 1-3 orders of\nmagnitude improvement in accuracy while reducing parameter count by over 90\\%\nand training time by 60\\%. These results establish SV-SNN as an effective\nsolution to the spectral bias problem in neural PDE solving. The implementation\nwill be made publicly available upon acceptance at\nhttps://github.com/xgxgnpu/SV-SNN.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00628v1", "cate": "cs.LG", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00635", "title": "KFS: KAN based adaptive Frequency Selection learning architecture for long term time series forecasting", "authors": ["Changning Wu", "Gao Wu", "Rongyao Cai", "Yong Liu", "Kexin Zhang"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00635v1", "summary": "Multi-scale decomposition architectures have emerged as predominant\nmethodologies in time series forecasting. However, real-world time series\nexhibit noise interference across different scales, while heterogeneous\ninformation distribution among frequency components at varying scales leads to\nsuboptimal multi-scale representation. Inspired by Kolmogorov-Arnold Networks\n(KAN) and Parseval's theorem, we propose a KAN based adaptive Frequency\nSelection learning architecture (KFS) to address these challenges. This\nframework tackles prediction challenges stemming from cross-scale noise\ninterference and complex pattern modeling through its FreK module, which\nperforms energy-distribution-based dominant frequency selection in the spectral\ndomain. Simultaneously, KAN enables sophisticated pattern representation while\ntimestamp embedding alignment synchronizes temporal representations across\nscales. The feature mixing module then fuses scale-specific patterns with\naligned temporal features. Extensive experiments across multiple real-world\ntime series datasets demonstrate that KT achieves state-of-the-art performance\nas a simple yet effective architecture.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00635v1", "cate": "cs.LG", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00641", "title": "Reinforcement Learning for Decision-Level Interception Prioritization in Drone Swarm Defense", "authors": ["Alessandro Palmas"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      11 pages, 10 figures", "url": "http://arxiv.org/abs/2508.00641v1", "summary": "The growing threat of low-cost kamikaze drone swarms poses a critical\nchallenge to modern defense systems demanding rapid and strategic\ndecision-making to prioritize interceptions across multiple effectors and\nhigh-value target zones. In this work, we present a case study demonstrating\nthe practical advantages of reinforcement learning in addressing this\nchallenge. We introduce a high-fidelity simulation environment that captures\nrealistic operational constraints, within which a decision-level reinforcement\nlearning agent learns to coordinate multiple effectors for optimal interception\nprioritization. Operating in a discrete action space, the agent selects which\ndrone to engage per effector based on observed state features such as\npositions, classes, and effector status. We evaluate the learned policy against\na handcrafted rule-based baseline across hundreds of simulated attack\nscenarios. The reinforcement learning based policy consistently achieves lower\naverage damage and higher defensive efficiency in protecting critical zones.\nThis case study highlights the potential of reinforcement learning as a\nstrategic layer within defense architectures, enhancing resilience without\ndisplacing existing control systems. All code and simulation assets are\npublicly released for full reproducibility, and a video demonstration\nillustrates the policy's qualitative behavior.", "comment": "11 pages, 10 figures", "pdf_url": "http://arxiv.org/pdf/2508.00641v1", "cate": "cs.LG", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00643", "title": "Light-Weight Diffusion Multiplier and Uncertainty Quantification for Fourier Neural Operators", "authors": ["Albert Matveev", "Sanmitra Ghosh", "Aamal Hussain", "James-Michael Leahy", "Michalis Michaelides"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00643v1", "summary": "Operator learning is a powerful paradigm for solving partial differential\nequations, with Fourier Neural Operators serving as a widely adopted\nfoundation. However, FNOs face significant scalability challenges due to\noverparameterization and offer no native uncertainty quantification -- a key\nrequirement for reliable scientific and engineering applications. Instead,\nneural operators rely on post hoc UQ methods that ignore geometric inductive\nbiases. In this work, we introduce DINOZAUR: a diffusion-based neural operator\nparametrization with uncertainty quantification. Inspired by the structure of\nthe heat kernel, DINOZAUR replaces the dense tensor multiplier in FNOs with a\ndimensionality-independent diffusion multiplier that has a single learnable\ntime parameter per channel, drastically reducing parameter count and memory\nfootprint without compromising predictive performance. By defining priors over\nthose time parameters, we cast DINOZAUR as a Bayesian neural operator to yield\nspatially correlated outputs and calibrated uncertainty estimates. Our method\nachieves competitive or superior performance across several PDE benchmarks\nwhile providing efficient uncertainty quantification.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00643v1", "cate": "cs.LG", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00657", "title": "TrajSurv: Learning Continuous Latent Trajectories from Electronic Health Records for Trustworthy Survival Prediction", "authors": ["Sihang Zeng", "Lucas Jing Liu", "Jun Wen", "Meliha Yetisgen", "Ruth Etzioni", "Gang Luo"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted by MLHC 2025", "url": "http://arxiv.org/abs/2508.00657v1", "summary": "Trustworthy survival prediction is essential for clinical decision making.\nLongitudinal electronic health records (EHRs) provide a uniquely powerful\nopportunity for the prediction. However, it is challenging to accurately model\nthe continuous clinical progression of patients underlying the irregularly\nsampled clinical features and to transparently link the progression to survival\noutcomes. To address these challenges, we develop TrajSurv, a model that learns\ncontinuous latent trajectories from longitudinal EHR data for trustworthy\nsurvival prediction. TrajSurv employs a neural controlled differential equation\n(NCDE) to extract continuous-time latent states from the irregularly sampled\ndata, forming continuous latent trajectories. To ensure the latent trajectories\nreflect the clinical progression, TrajSurv aligns the latent state space with\npatient state space through a time-aware contrastive learning approach. To\ntransparently link clinical progression to the survival outcome, TrajSurv uses\nlatent trajectories in a two-step divide-and-conquer interpretation process.\nFirst, it explains how the changes in clinical features translate into the\nlatent trajectory's evolution using a learned vector field. Second, it clusters\nthese latent trajectories to identify key clinical progression patterns\nassociated with different survival outcomes. Evaluations on two real-world\nmedical datasets, MIMIC-III and eICU, show TrajSurv's competitive accuracy and\nsuperior transparency over existing deep learning methods.", "comment": "Accepted by MLHC 2025", "pdf_url": "http://arxiv.org/pdf/2508.00657v1", "cate": "cs.LG", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00664", "title": "DP-DGAD: A Generalist Dynamic Graph Anomaly Detector with Dynamic Prototypes", "authors": ["Jialun Zheng", "Jie Liu", "Jiannong Cao", "Xiao Wang", "Hanchen Yang", "Yankai Chen", "Philip S. Yu"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00664v1", "summary": "Dynamic graph anomaly detection (DGAD) is essential for identifying anomalies\nin evolving graphs across domains such as finance, traffic, and social\nnetworks. Recently, generalist graph anomaly detection (GAD) models have shown\npromising results. They are pretrained on multiple source datasets and\ngeneralize across domains. While effective on static graphs, they struggle to\ncapture evolving anomalies in dynamic graphs. Moreover, the continuous\nemergence of new domains and the lack of labeled data further challenge\ngeneralist DGAD. Effective cross-domain DGAD requires both domain-specific and\ndomain-agnostic anomalous patterns. Importantly, these patterns evolve\ntemporally within and across domains. Building on these insights, we propose a\nDGAD model with Dynamic Prototypes (DP) to capture evolving domain-specific and\ndomain-agnostic patterns. Firstly, DP-DGAD extracts dynamic prototypes, i.e.,\nevolving representations of normal and anomalous patterns, from temporal\nego-graphs and stores them in a memory buffer. The buffer is selectively\nupdated to retain general, domain-agnostic patterns while incorporating new\ndomain-specific ones. Then, an anomaly scorer compares incoming data with\ndynamic prototypes to flag both general and domain-specific anomalies. Finally,\nDP-DGAD employs confidence-based pseudo-labeling for effective self-supervised\nadaptation in target domains. Extensive experiments demonstrate\nstate-of-the-art performance across ten real-world datasets from different\ndomains.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00664v1", "cate": "cs.LG", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00695", "title": "Classification of Psychiatry Clinical Notes by Diagnosis: A Deep Learning and Machine Learning Approach", "authors": ["Sergio Rubio-Martín", "María Teresa García-Ordás", "Antonio Serrano-García", "Clara Margarita Franch-Pato", "Arturo Crespo-Álvaro", "José Alberto Benítez-Andrades"], "categories": ["cs.LG", "cs.CL"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00695v1", "summary": "The classification of clinical notes into specific diagnostic categories is\ncritical in healthcare, especially for mental health conditions like Anxiety\nand Adjustment Disorder. In this study, we compare the performance of various\nArtificial Intelligence models, including both traditional Machine Learning\napproaches (Random Forest, Support Vector Machine, K-nearest neighbors,\nDecision Tree, and eXtreme Gradient Boost) and Deep Learning models (DistilBERT\nand SciBERT), to classify clinical notes into these two diagnoses.\nAdditionally, we implemented three oversampling strategies: No Oversampling,\nRandom Oversampling, and Synthetic Minority Oversampling Technique (SMOTE), to\nassess their impact on model performance. Hyperparameter tuning was also\napplied to optimize model accuracy. Our results indicate that oversampling\ntechniques had minimal impact on model performance overall. The only exception\nwas SMOTE, which showed a positive effect specifically with BERT-based models.\nHowever, hyperparameter optimization significantly improved accuracy across the\nmodels, enhancing their ability to generalize and perform on the dataset. The\nDecision Tree and eXtreme Gradient Boost models achieved the highest accuracy\namong machine learning approaches, both reaching 96%, while the DistilBERT and\nSciBERT models also attained 96% accuracy in the deep learning category. These\nfindings underscore the importance of hyperparameter tuning in maximizing model\nperformance. This study contributes to the ongoing research on AI-assisted\ndiagnostic tools in mental health by providing insights into the efficacy of\ndifferent model architectures and data balancing methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00695v1", "cate": "cs.LG", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00706", "title": "Learning Network Dismantling without Handcrafted Inputs", "authors": ["Haozhe Tian", "Pietro Ferraro", "Robert Shorten", "Mahdi Jalili", "Homayoun Hamedmoghadam"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00706v1", "summary": "The application of message-passing Graph Neural Networks has been a\nbreakthrough for important network science problems. However, the competitive\nperformance often relies on using handcrafted structural features as inputs,\nwhich increases computational cost and introduces bias into the otherwise\npurely data-driven network representations. Here, we eliminate the need for\nhandcrafted features by introducing an attention mechanism and utilizing\nmessage-iteration profiles, in addition to an effective algorithmic approach to\ngenerate a structurally diverse training set of small synthetic networks.\nThereby, we build an expressive message-passing framework and use it to\nefficiently solve the NP-hard problem of Network Dismantling, virtually\nequivalent to vital node identification, with significant real-world\napplications. Trained solely on diversified synthetic networks, our proposed\nmodel -- MIND: Message Iteration Network Dismantler -- generalizes to large,\nunseen real networks with millions of nodes, outperforming state-of-the-art\nnetwork dismantling methods. Increased efficiency and generalizability of the\nproposed model can be leveraged beyond dismantling in a range of complex\nnetwork problems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00706v1", "cate": "cs.LG", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00707", "title": "Efficient Solution and Learning of Robust Factored MDPs", "authors": ["Yannik Schnitzer", "Alessandro Abate", "David Parker"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00707v1", "summary": "Robust Markov decision processes (r-MDPs) extend MDPs by explicitly modelling\nepistemic uncertainty about transition dynamics. Learning r-MDPs from\ninteractions with an unknown environment enables the synthesis of robust\npolicies with provable (PAC) guarantees on performance, but this can require a\nlarge number of sample interactions. We propose novel methods for solving and\nlearning r-MDPs based on factored state-space representations that leverage the\nindependence between model uncertainty across system components. Although\npolicy synthesis for factored r-MDPs leads to hard, non-convex optimisation\nproblems, we show how to reformulate these into tractable linear programs.\nBuilding on these, we also propose methods to learn factored model\nrepresentations directly. Our experimental results show that exploiting\nfactored structure can yield dimensional gains in sample efficiency, producing\nmore effective robust policies with tighter performance guarantees than\nstate-of-the-art methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00707v1", "cate": "cs.LG", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00712", "title": "JSON-Bag: A generic game trajectory representation", "authors": ["Dien Nguyen", "Diego Perez-Liebana", "Simon Lucas"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      8 pages, 3 figures, 6 tables, to be published in IEEE Conference on Games 2025", "url": "http://arxiv.org/abs/2508.00712v1", "summary": "We introduce JSON Bag-of-Tokens model (JSON-Bag) as a method to generically\nrepresent game trajectories by tokenizing their JSON descriptions and apply\nJensen-Shannon distance (JSD) as distance metric for them. Using a\nprototype-based nearest-neighbor search (P-NNS), we evaluate the validity of\nJSON-Bag with JSD on six tabletop games -- \\textit{7 Wonders},\n\\textit{Dominion}, \\textit{Sea Salt and Paper}, \\textit{Can't Stop},\n\\textit{Connect4}, \\textit{Dots and boxes} -- each over three game trajectory\nclassification tasks: classifying the playing agents, game parameters, or game\nseeds that were used to generate the trajectories.\n  Our approach outperforms a baseline using hand-crafted features in the\nmajority of tasks. Evaluating on N-shot classification suggests using JSON-Bag\nprototype to represent game trajectory classes is also sample efficient.\nAdditionally, we demonstrate JSON-Bag ability for automatic feature extraction\nby treating tokens as individual features to be used in Random Forest to solve\nthe tasks above, which significantly improves accuracy on underperforming\ntasks. Finally, we show that, across all six games, the JSD between JSON-Bag\nprototypes of agent classes highly correlates with the distances between\nagents' policies.", "comment": "8 pages, 3 figures, 6 tables, to be published in IEEE Conference on\n  Games 2025", "pdf_url": "http://arxiv.org/pdf/2508.00712v1", "cate": "cs.LG", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00716", "title": "Nested Graph Pseudo-Label Refinement for Noisy Label Domain Adaptation Learning", "authors": ["Yingxu Wang", "Mengzhu Wang", "Zhichao Huang", "Suyu Liu"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00716v1", "summary": "Graph Domain Adaptation (GDA) facilitates knowledge transfer from labeled\nsource graphs to unlabeled target graphs by learning domain-invariant\nrepresentations, which is essential in applications such as molecular property\nprediction and social network analysis. However, most existing GDA methods rely\non the assumption of clean source labels, which rarely holds in real-world\nscenarios where annotation noise is pervasive. This label noise severely\nimpairs feature alignment and degrades adaptation performance under domain\nshifts. To address this challenge, we propose Nested Graph Pseudo-Label\nRefinement (NeGPR), a novel framework tailored for graph-level domain\nadaptation with noisy labels. NeGPR first pretrains dual branches, i.e.,\nsemantic and topology branches, by enforcing neighborhood consistency in the\nfeature space, thereby reducing the influence of noisy supervision. To bridge\ndomain gaps, NeGPR employs a nested refinement mechanism in which one branch\nselects high-confidence target samples to guide the adaptation of the other,\nenabling progressive cross-domain learning. Furthermore, since pseudo-labels\nmay still contain noise and the pre-trained branches are already overfitted to\nthe noisy labels in the source domain, NeGPR incorporates a noise-aware\nregularization strategy. This regularization is theoretically proven to\nmitigate the adverse effects of pseudo-label noise, even under the presence of\nsource overfitting, thus enhancing the robustness of the adaptation process.\nExtensive experiments on benchmark datasets demonstrate that NeGPR consistently\noutperforms state-of-the-art methods under severe label noise, achieving gains\nof up to 12.7% in accuracy.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00716v1", "cate": "cs.LG", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00718", "title": "Democratizing Tabular Data Access with an Open$\\unicode{x2013}$Source Synthetic$\\unicode{x2013}$Data SDK", "authors": ["Ivona Krchova", "Mariana Vargas Vieyra", "Mario Scriminaci", "Andrey Sidorenko"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00718v1", "summary": "Machine learning development critically depends on access to high-quality\ndata. However, increasing restrictions due to privacy, proprietary interests,\nand ethical concerns have created significant barriers to data accessibility.\nSynthetic data offers a viable solution by enabling safe, broad data usage\nwithout compromising sensitive information. This paper presents the MOSTLY AI\nSynthetic Data Software Development Kit (SDK), an open-source toolkit designed\nspecifically for synthesizing high-quality tabular data. The SDK integrates\nrobust features such as differential privacy guarantees, fairness-aware data\ngeneration, and automated quality assurance into a flexible and accessible\nPython interface. Leveraging the TabularARGN autoregressive framework, the SDK\nsupports diverse data types and complex multi-table and sequential datasets,\ndelivering competitive performance with notable improvements in speed and\nusability. Currently deployed both as a cloud service and locally installable\nsoftware, the SDK has seen rapid adoption, highlighting its practicality in\naddressing real-world data bottlenecks and promoting widespread data\ndemocratization.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00718v1", "cate": "cs.LG", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00734", "title": "Adaptive Machine Learning-Driven Multi-Fidelity Stratified Sampling for Failure Analysis of Nonlinear Stochastic Systems", "authors": ["Liuyun Xu", "Seymour M. J. Spence"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00734v1", "summary": "Existing variance reduction techniques used in stochastic simulations for\nrare event analysis still require a substantial number of model evaluations to\nestimate small failure probabilities. In the context of complex, nonlinear\nfinite element modeling environments, this can become computationally\nchallenging-particularly for systems subjected to stochastic excitation. To\naddress this challenge, a multi-fidelity stratified sampling scheme with\nadaptive machine learning metamodels is introduced for efficiently propagating\nuncertainties and estimating small failure probabilities. In this approach, a\nhigh-fidelity dataset generated through stratified sampling is used to train a\ndeep learning-based metamodel, which then serves as a cost-effective and highly\ncorrelated low-fidelity model. An adaptive training scheme is proposed to\nbalance the trade-off between approximation quality and computational demand\nassociated with the development of the low-fidelity model. By integrating the\nlow-fidelity outputs with additional high-fidelity results, an unbiased\nestimate of the strata-wise failure probabilities is obtained using a\nmulti-fidelity Monte Carlo framework. The overall probability of failure is\nthen computed using the total probability theorem. Application to a full-scale\nhigh-rise steel building subjected to stochastic wind excitation demonstrates\nthat the proposed scheme can accurately estimate exceedance probability curves\nfor nonlinear responses of interest, while achieving significant computational\nsavings compared to single-fidelity variance reduction approaches.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00734v1", "cate": "cs.LG", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00754", "title": "A Simple and Effective Method for Uncertainty Quantification and OOD Detection", "authors": ["Yaxin Ma", "Benjamin Colburn", "Jose C. Principe"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00754v1", "summary": "Bayesian neural networks and deep ensemble methods have been proposed for\nuncertainty quantification; however, they are computationally intensive and\nrequire large storage. By utilizing a single deterministic model, we can solve\nthe above issue. We propose an effective method based on feature space density\nto quantify uncertainty for distributional shifts and out-of-distribution (OOD)\ndetection. Specifically, we leverage the information potential field derived\nfrom kernel density estimation to approximate the feature space density of the\ntraining set. By comparing this density with the feature space representation\nof test samples, we can effectively determine whether a distributional shift\nhas occurred. Experiments were conducted on a 2D synthetic dataset (Two Moons\nand Three Spirals) as well as an OOD detection task (CIFAR-10 vs. SVHN). The\nresults demonstrate that our method outperforms baseline models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00754v1", "cate": "cs.LG", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00758", "title": "Diffusion-Scheduled Denoising Autoencoders for Anomaly Detection in Tabular Data", "authors": ["Timur Sattarov", "Marco Schreyer", "Damian Borth"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      22 pages, 16 figures, 7 tables, preprint version", "url": "http://arxiv.org/abs/2508.00758v1", "summary": "Anomaly detection in tabular data remains challenging due to complex feature\ninteractions and the scarcity of anomalous examples. Denoising autoencoders\nrely on fixed-magnitude noise, limiting adaptability to diverse data\ndistributions. Diffusion models introduce scheduled noise and iterative\ndenoising, but lack explicit reconstruction mappings. We propose the\nDiffusion-Scheduled Denoising Autoencoder (DDAE), a framework that integrates\ndiffusion-based noise scheduling and contrastive learning into the encoding\nprocess to improve anomaly detection. We evaluated DDAE on 57 datasets from\nADBench. Our method outperforms in semi-supervised settings and achieves\ncompetitive results in unsupervised settings, improving PR-AUC by up to 65%\n(9%) and ROC-AUC by 16% (6%) over state-of-the-art autoencoder (diffusion)\nmodel baselines. We observed that higher noise levels benefit unsupervised\ntraining, while lower noise with linear scheduling is optimal in\nsemi-supervised settings. These findings underscore the importance of\nprincipled noise strategies in tabular anomaly detection.", "comment": "22 pages, 16 figures, 7 tables, preprint version", "pdf_url": "http://arxiv.org/pdf/2508.00758v1", "cate": "cs.LG", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00768", "title": "Evaluating Angle and Amplitude Encoding Strategies for Variational Quantum Machine Learning: their impact on model's accuracy", "authors": ["Antonio Tudisco", "Andrea Marchesin", "Maurizio Zamboni", "Mariagrazia Graziano", "Giovanna Turvani"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00768v1", "summary": "Recent advancements in Quantum Computing and Machine Learning have increased\nattention to Quantum Machine Learning (QML), which aims to develop machine\nlearning models by exploiting the quantum computing paradigm. One of the widely\nused models in this area is the Variational Quantum Circuit (VQC), a hybrid\nmodel where the quantum circuit handles data inference while classical\noptimization adjusts the parameters of the circuit. The quantum circuit\nconsists of an encoding layer, which loads data into the circuit, and a\ntemplate circuit, known as the ansatz, responsible for processing the data.\nThis work involves performing an analysis by considering both Amplitude- and\nAngle-encoding models, and examining how the type of rotational gate applied\naffects the classification performance of the model. This comparison is carried\nout by training the different models on two datasets, Wine and Diabetes, and\nevaluating their performance. The study demonstrates that, under identical\nmodel topologies, the difference in accuracy between the best and worst models\nranges from 10% to 30%, with differences reaching up to 41%. Moreover, the\nresults highlight how the choice of rotational gates used in encoding can\nsignificantly impact the model's classification performance. The findings\nconfirm that the embedding represents a hyperparameter for VQC models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00768v1", "cate": "cs.LG", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00785", "title": "Explainable AI and Machine Learning for Exam-based Student Evaluation: Causal and Predictive Analysis of Socio-academic and Economic Factors", "authors": ["Bushra Akter", "Md Biplob Hosen", "Sabbir Ahmed", "Mehrin Anannya", "Md. Farhad Hossain"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00785v1", "summary": "Academic performance depends on a multivariable nexus of socio-academic and\nfinancial factors. This study investigates these influences to develop\neffective strategies for optimizing students' CGPA. To achieve this, we\nreviewed various literature to identify key influencing factors and constructed\nan initial hypothetical causal graph based on the findings. Additionally, an\nonline survey was conducted, where 1,050 students participated, providing\ncomprehensive data for analysis. Rigorous data preprocessing techniques,\nincluding cleaning and visualization, ensured data quality before analysis.\nCausal analysis validated the relationships among variables, offering deeper\ninsights into their direct and indirect effects on CGPA. Regression models were\nimplemented for CGPA prediction, while classification models categorized\nstudents based on performance levels. Ridge Regression demonstrated strong\npredictive accuracy, achieving a Mean Absolute Error of 0.12 and a Mean Squared\nError of 0.023. Random Forest outperformed in classification, attaining an\nF1-score near perfection and an accuracy of 98.68%. Explainable AI techniques\nsuch as SHAP, LIME, and Interpret enhanced model interpretability, highlighting\ncritical factors such as study hours, scholarships, parental education, and\nprior academic performance. The study culminated in the development of a\nweb-based application that provides students with personalized insights,\nallowing them to predict academic performance, identify areas for improvement,\nand make informed decisions to enhance their outcomes.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00785v1", "cate": "cs.LG", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00806", "title": "Adacc: Adaptive Compression and Activation Checkpointing for LLM Memory Management", "authors": ["Ping Chen", "Zhuohong Deng", "Ping Li", "Shuibing He", "Hongzi Zhu", "Yi Zheng", "Zhefeng Wang", "Baoxing Huai", "Minyi Guo"], "categories": ["cs.LG", "cs.DC"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      8 pages", "url": "http://arxiv.org/abs/2508.00806v1", "summary": "Training large language models often employs recomputation to alleviate\nmemory pressure, which can introduce up to 30% overhead in real-world\nscenarios. In this paper, we propose Adacc, a novel memory management framework\nthat combines adaptive compression and activation checkpointing to reduce the\nGPU memory footprint. It comprises three modules: (1) We design layer-specific\ncompression algorithms that account for outliers in LLM tensors, instead of\ndirectly quantizing floats from FP16 to INT4, to ensure model accuracy. (2) We\npropose an optimal scheduling policy that employs MILP to determine the best\nmemory optimization for each tensor. (3) To accommodate changes in training\ntensors, we introduce an adaptive policy evolution mechanism that adjusts the\npolicy during training to enhance throughput. Experimental results show that\nAdacc can accelerate the LLM training by 1.01x to 1.37x compared to\nstate-of-the-art frameworks, while maintaining comparable model accuracy to the\nBaseline.", "comment": "8 pages", "pdf_url": "http://arxiv.org/pdf/2508.00806v1", "cate": "cs.LG", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2502.18148", "title": "NusaAksara: A Multimodal and Multilingual Benchmark for Preserving Indonesian Indigenous Scripts", "authors": ["Muhammad Farid Adilazuarda", "Musa Izzanardi Wijanarko", "Lucky Susanto", "Khumaisa Nur'aini", "Derry Wijaya", "Alham Fikri Aji"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2502.18148v1", "summary": "Indonesia is rich in languages and scripts. However, most NLP progress has\nbeen made using romanized text. In this paper, we present NusaAksara, a novel\npublic benchmark for Indonesian languages that includes their original scripts.\nOur benchmark covers both text and image modalities and encompasses diverse\ntasks such as image segmentation, OCR, transliteration, translation, and\nlanguage identification. Our data is constructed by human experts through\nrigorous steps. NusaAksara covers 8 scripts across 7 languages, including\nlow-resource languages not commonly seen in NLP benchmarks. Although\nunsupported by Unicode, the Lampung script is included in this dataset. We\nbenchmark our data across several models, from LLMs and VLMs such as GPT-4o,\nLlama 3.2, and Aya 23 to task-specific systems such as PP-OCR and LangID, and\nshow that most NLP technologies cannot handle Indonesia's local scripts, with\nmany achieving near-zero performance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2502.18148v1", "cate": "cs.CL", "date": "2025-02-25", "updated": "2025-02-25"}
{"id": "2508.00024", "title": "Embedding-Aware Quantum-Classical SVMs for Scalable Quantum Machine Learning", "authors": ["Sebastián Andrés Cajas Ordóñez", "Luis Fernando Torres Torres", "Mario Bifulco", "Carlos Andrés Durán", "Cristian Bosch", "Ricardo Simón Carbajo"], "categories": ["quant-ph", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Quantum Physics (quant-ph)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00024v1", "summary": "Quantum Support Vector Machines face scalability challenges due to\nhigh-dimensional quantum states and hardware limitations. We propose an\nembedding-aware quantum-classical pipeline combining class-balanced k-means\ndistillation with pretrained Vision Transformer embeddings. Our key finding:\nViT embeddings uniquely enable quantum advantage, achieving up to 8.02%\naccuracy improvements over classical SVMs on Fashion-MNIST and 4.42% on MNIST,\nwhile CNN features show performance degradation. Using 16-qubit tensor network\nsimulation via cuTensorNet, we provide the first systematic evidence that\nquantum kernel advantage depends critically on embedding choice, revealing\nfundamental synergy between transformer attention and quantum feature spaces.\nThis provides a practical pathway for scalable quantum machine learning that\nleverages modern neural architectures.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00024v1", "cate": "quant-ph", "date": "2025-07-28", "updated": "2025-07-28"}
{"id": "2508.00027", "title": "Quantum Semi-Random Forests for Qubit-Efficient Recommender Systems", "authors": ["Azadeh Alavi", "Fatemeh Kouchmeshki", "Abdolrahman Alavi", "Yongli Ren", "Jiayang Niu"], "categories": ["quant-ph", "cs.LG"], "primary_category": "Subjects:       Quantum Physics (quant-ph)", "pdf_link": null, "comments": "Comments:      Submitted to IEEE Quantum AI Conference (QAI 2025), awaiting peer review", "url": "http://arxiv.org/abs/2508.00027v1", "summary": "Modern recommenders describe each item with hundreds of sparse semantic tags,\nyet most quantum pipelines still map one qubit per tag, demanding well beyond\none hundred qubits, far out of reach for current noisy-intermediate-scale\nquantum (NISQ) devices and prone to deep, error-amplifying circuits. We close\nthis gap with a three-stage hybrid machine learning algorithm that compresses\ntag profiles, optimizes feature selection under a fixed qubit budget via QAOA,\nand scores recommendations with a Quantum semi-Random Forest (QsRF) built on\njust five qubits, while performing similarly to the state-of-the-art methods.\nLeveraging SVD sketching and k-means, we learn a 1000-atom dictionary ($>$97 \\%\nvariance), then solve a 2020 QUBO via depth-3 QAOA to select 5 atoms. A\n100-tree QsRF trained on these codes matches full-feature baselines on\nICM-150/500.", "comment": "Submitted to IEEE Quantum AI Conference (QAI 2025), awaiting peer\n  review", "pdf_url": "http://arxiv.org/pdf/2508.00027v1", "cate": "quant-ph", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2508.00029", "title": "Hybrid Quantum Classical Surrogate for Real Time Inverse Finite Element Modeling in Digital Twins", "authors": ["Azadeh Alavi", "Sanduni Jayasinghe", "Mojtaba Mahmoodian", "Sam Mazaheri", "John Thangarajah", "Sujeeva Setunge"], "categories": ["quant-ph", "cs.LG"], "primary_category": "Subjects:       Quantum Physics (quant-ph)", "pdf_link": null, "comments": "Comments:      Submitted to Scientific Report", "url": "http://arxiv.org/abs/2508.00029v1", "summary": "Large-scale civil structures, such as bridges, pipelines, and offshore\nplatforms, are vital to modern infrastructure, where unexpected failures can\ncause significant economic and safety repercussions. Although finite element\n(FE) modeling is widely used for real-time structural health monitoring (SHM),\nits high computational cost and the complexity of inverse FE analysis, where\nlow dimensional sensor data must map onto high-dimensional displacement or\nstress fields pose ongoing challenges. Here, we propose a hybrid quantum\nclassical multilayer perceptron (QMLP) framework to tackle these issues and\nfacilitate swift updates to digital twins across a range of structural\napplications.\n  Our approach embeds sensor data using symmetric positive definite (SPD)\nmatrices and polynomial features, yielding a representation well suited to\nquantum processing. A parameterized quantum circuit (PQC) transforms these\nfeatures, and the resultant quantum outputs feed into a classical neural\nnetwork for final inference. By fusing quantum capabilities with classical\nmodeling, the QMLP handles large scale inverse FE mapping while preserving\ncomputational viability.\n  Through extensive experiments on a bridge, we demonstrate that the QMLP\nachieves a mean squared error (MSE) of 0.0000000000316, outperforming purely\nclassical baselines with a large margin. These findings confirm the potential\nof quantum-enhanced methods for real time SHM, establishing a pathway toward\nmore efficient, scalable digital twins that can robustly monitor and diagnose\nstructural integrity in near real time.", "comment": "Submitted to Scientific Report", "pdf_url": "http://arxiv.org/pdf/2508.00029v1", "cate": "quant-ph", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2508.00048", "title": "Dimension reduction with structure-aware quantum circuits for hybrid machine learning", "authors": ["Ammar Daskin"], "categories": ["quant-ph", "cs.LG"], "primary_category": "Subjects:       Quantum Physics (quant-ph)", "pdf_link": null, "comments": "Comments:      Any comments are welcome! The simulation code is provided at this https URL", "url": "http://arxiv.org/abs/2508.00048v1", "summary": "Schmidt decomposition of a vector can be understood as writing the singular\nvalue decomposition (SVD) in vector form. A vector can be written as a linear\ncombination of tensor product of two dimensional vectors by recursively\napplying Schmidt decompositions via SVD to all subsystems. Given a vector\nexpressed as a linear combination of tensor products, using only the $k$\nprincipal terms yields a $k$-rank approximation of the vector. Therefore,\nwriting a vector in this reduced form allows to retain most important parts of\nthe vector while removing small noises from it, analogous to SVD-based\ndenoising.\n  In this paper, we show that quantum circuits designed based on a value $k$\n(determined from the tensor network decomposition of the mean vector of the\ntraining sample) can approximate the reduced-form representations of entire\ndatasets. We then employ this circuit ansatz with a classical neural network\nhead to construct a hybrid machine learning model. Since the output of the\nquantum circuit for an $2^n$ dimensional vector is an $n$ dimensional\nprobability vector, this provides an exponential compression of the input and\npotentially can reduce the number of learnable parameters for training\nlarge-scale models. We use datasets provided in the Python scikit-learn module\nfor the experiments. The results confirm the quantum circuit is able to\ncompress data successfully to provide effective $k$-rank approximations to the\nclassical processing component.", "comment": "Any comments are welcome! The simulation code is provided at\n  https://github.com/adaskin/structure-aware-circuits", "pdf_url": "http://arxiv.org/pdf/2508.00048v1", "cate": "quant-ph", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2508.00091", "title": "Riemannian Optimization for Distance Geometry: A Study of Convergence, Robustness, and Incoherence", "authors": ["Chandler Smith", "HanQin Cai", "Abiy Tasissa"], "categories": ["math.OC", "cs.CG", "cs.LG"], "primary_category": "Subjects:       Optimization and Control (math.OC)", "pdf_link": null, "comments": "Comments:      54 pages, 6 figures", "url": "http://arxiv.org/abs/2508.00091v1", "summary": "The problem of recovering a configuration of points from partial pairwise\ndistances, referred to as the Euclidean Distance Geometry (EDG) problem, arises\nin a broad range of applications, including sensor network localization,\nmolecular conformation, and manifold learning. In this paper, we propose a\nRiemannian optimization framework for solving the EDG problem by formulating it\nas a low-rank matrix completion task over the space of positive semi-definite\nGram matrices. The available distance measurements are encoded as expansion\ncoefficients in a non-orthogonal basis, and optimization over the Gram matrix\nimplicitly enforces geometric consistency through the triangle inequality, a\nstructure inherited from classical multidimensional scaling. Under a Bernoulli\nsampling model for observed distances, we prove that Riemannian gradient\ndescent on the manifold of rank-$r$ matrices locally converges linearly with\nhigh probability when the sampling probability satisfies $p \\geq\n\\mathcal{O}(\\nu^2 r^2 \\log(n)/n)$, where $\\nu$ is an EDG-specific incoherence\nparameter. Furthermore, we provide an initialization candidate using a one-step\nhard thresholding procedure that yields convergence, provided the sampling\nprobability satisfies $p \\geq \\mathcal{O}(\\nu r^{3/2} \\log^{3/4}(n)/n^{1/4})$.\nA key technical contribution of this work is the analysis of a symmetric linear\noperator arising from a dual basis expansion in the non-orthogonal basis, which\nrequires a novel application of the Hanson--Wright inequality to establish an\noptimal restricted isometry property in the presence of coupled terms.\nEmpirical evaluations on synthetic data demonstrate that our algorithm achieves\ncompetitive performance relative to state-of-the-art methods. Moreover, we\npropose a novel notion of matrix incoherence tailored to the EDG setting and\nprovide robustness guarantees for our method.", "comment": "54 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2508.00091v1", "cate": "math.OC", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2508.00110", "title": "funOCLUST: Clustering Functional Data with Outliers", "authors": ["Katharine M. Clark", "Paul D. McNicholas"], "categories": ["stat.ML", "cs.LG"], "primary_category": "Subjects:       Machine Learning (stat.ML)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00110v1", "summary": "Functional data present unique challenges for clustering due to their\ninfinite-dimensional nature and potential sensitivity to outliers. An extension\nof the OCLUST algorithm to the functional setting is proposed to address these\nissues. The approach leverages the OCLUST framework, creating a robust method\nto cluster curves and trim outliers. The methodology is evaluated on both\nsimulated and real-world functional datasets, demonstrating strong performance\nin clustering and outlier identification.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00110v1", "cate": "stat.ML", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2508.00135", "title": "Exploring the Feasibility of Deep Learning Techniques for Accurate Gender Classification from Eye Images", "authors": ["Basna Mohammed Salih Hasan", "Ramadhan J. Mstafa"], "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      12 pages, 18 figures, 5 tables", "url": "http://arxiv.org/abs/2508.00135v1", "summary": "Gender classification has emerged as a crucial aspect in various fields,\nincluding security, human-machine interaction, surveillance, and advertising.\nNonetheless, the accuracy of this classification can be influenced by factors\nsuch as cosmetics and disguise. Consequently, our study is dedicated to\naddressing this concern by concentrating on gender classification using color\nimages of the periocular region. The periocular region refers to the area\nsurrounding the eye, including the eyelids, eyebrows, and the region between\nthem. It contains valuable visual cues that can be used to extract key features\nfor gender classification. This paper introduces a sophisticated Convolutional\nNeural Network (CNN) model that utilizes color image databases to evaluate the\neffectiveness of the periocular region for gender classification. To validate\nthe model's performance, we conducted tests on two eye datasets, namely CVBL\nand (Female and Male). The recommended architecture achieved an outstanding\naccuracy of 99% on the previously unused CVBL dataset while attaining a\ncommendable accuracy of 96% with a small number of learnable parameters\n(7,235,089) on the (Female and Male) dataset. To ascertain the effectiveness of\nour proposed model for gender classification using the periocular region, we\nevaluated its performance through an extensive range of metrics and compared it\nwith other state-of-the-art approaches. The results unequivocally demonstrate\nthe efficacy of our model, thereby suggesting its potential for practical\napplication in domains such as security and surveillance.", "comment": "12 pages, 18 figures, 5 tables", "pdf_url": "http://arxiv.org/pdf/2508.00135v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2508.00159", "title": "Model-Based Soft Maximization of Suitable Metrics of Long-Term Human Power", "authors": ["Jobst Heitzig", "Ram Potham"], "categories": ["cs.AI", "cs.CY", "cs.LG", "econ.TH", "math.OC", "68Txx", "I.2"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00159v1", "summary": "Power is a key concept in AI safety: power-seeking as an instrumental goal,\nsudden or gradual disempowerment of humans, power balance in human-AI\ninteraction and international AI governance. At the same time, power as the\nability to pursue diverse goals is essential for wellbeing.\n  This paper explores the idea of promoting both safety and wellbeing by\nforcing AI agents explicitly to empower humans and to manage the power balance\nbetween humans and AI agents in a desirable way. Using a principled, partially\naxiomatic approach, we design a parametrizable and decomposable objective\nfunction that represents an inequality- and risk-averse long-term aggregate of\nhuman power. It takes into account humans' bounded rationality and social\nnorms, and, crucially, considers a wide variety of possible human goals.\n  We derive algorithms for computing that metric by backward induction or\napproximating it via a form of multi-agent reinforcement learning from a given\nworld model. We exemplify the consequences of (softly) maximizing this metric\nin a variety of paradigmatic situations and describe what instrumental\nsub-goals it will likely imply. Our cautious assessment is that softly\nmaximizing suitable aggregate metrics of human power might constitute a\nbeneficial objective for agentic AI systems that is safer than direct\nutility-based objectives.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00159v1", "cate": "cs.AI", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2508.00213", "title": "SAM-PTx: Text-Guided Fine-Tuning of SAM with Parameter-Efficient, Parallel-Text Adapters", "authors": ["Shayan Jalilian", "Abdul Bais"], "categories": ["cs.CV", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00213v1", "summary": "The Segment Anything Model (SAM) has demonstrated impressive generalization\nin prompt-based segmentation. Yet, the potential of semantic text prompts\nremains underexplored compared to traditional spatial prompts like points and\nboxes. This paper introduces SAM-PTx, a parameter-efficient approach for\nadapting SAM using frozen CLIP-derived text embeddings as class-level semantic\nguidance. Specifically, we propose a lightweight adapter design called\nParallel-Text that injects text embeddings into SAM's image encoder, enabling\nsemantics-guided segmentation while keeping most of the original architecture\nfrozen. Our adapter modifies only the MLP-parallel branch of each transformer\nblock, preserving the attention pathway for spatial reasoning. Through\nsupervised experiments and ablations on the COD10K dataset as well as low-data\nsubsets of COCO and ADE20K, we show that incorporating fixed text embeddings as\ninput improves segmentation performance over purely spatial prompt baselines.\nTo our knowledge, this is the first work to use text prompts for segmentation\non the COD10K dataset. These results suggest that integrating semantic\nconditioning into SAM's architecture offers a practical and scalable path for\nefficient adaptation with minimal computational complexity.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00213v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2508.00217", "title": "Tabular Data Understanding with LLMs: A Survey of Recent Advances and Challenges", "authors": ["Xiaofeng Wu", "Alan Ritter", "Wei Xu"], "categories": ["cs.CL", "cs.DB", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00217v1", "summary": "Tables have gained significant attention in large language models (LLMs) and\nmultimodal large language models (MLLMs) due to their complex and flexible\nstructure. Unlike linear text inputs, tables are two-dimensional, encompassing\nformats that range from well-structured database tables to complex,\nmulti-layered spreadsheets, each with different purposes. This diversity in\nformat and purpose has led to the development of specialized methods and tasks,\ninstead of universal approaches, making navigation of table understanding tasks\nchallenging. To address these challenges, this paper introduces key concepts\nthrough a taxonomy of tabular input representations and an introduction of\ntable understanding tasks. We highlight several critical gaps in the field that\nindicate the need for further research: (1) the predominance of\nretrieval-focused tasks that require minimal reasoning beyond mathematical and\nlogical operations; (2) significant challenges faced by models when processing\ncomplex table structures, large-scale tables, length context, or multi-table\nscenarios; and (3) the limited generalization of models across different\ntabular representations and formats.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00217v1", "cate": "cs.CL", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2508.00218", "title": "Object-Centric Cropping for Visual Few-Shot Classification", "authors": ["Aymane Abdali", "Bartosz Boguslawski", "Lucas Drumetz", "Vincent Gripon"], "categories": ["cs.CV", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00218v1", "summary": "In the domain of Few-Shot Image Classification, operating with as little as\none example per class, the presence of image ambiguities stemming from multiple\nobjects or complex backgrounds can significantly deteriorate performance. Our\nresearch demonstrates that incorporating additional information about the local\npositioning of an object within its image markedly enhances classification\nacross established benchmarks. More importantly, we show that a significant\nfraction of the improvement can be achieved through the use of the Segment\nAnything Model, requiring only a pixel of the object of interest to be pointed\nout, or by employing fully unsupervised foreground object extraction methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00218v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2508.00222", "title": "RL-PLUS: Countering Capability Boundary Collapse of LLMs in Reinforcement Learning with Hybrid-policy Optimization", "authors": ["Yihong Dong", "Xue Jiang", "Yongding Tao", "Huanyu Liu", "Kechi Zhang", "Lili Mou", "Rongyu Cao", "Yingwei Ma", "Jue Chen", "Binhua Li", "Zhi Jin", "Fei Huang", "Yongbin Li", "Ge Li"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00222v1", "summary": "Reinforcement Learning with Verifiable Reward (RLVR) has significantly\nadvanced the complex reasoning abilities of Large Language Models (LLMs).\nHowever, it struggles to break through the inherent capability boundaries of\nthe base LLM, due to its inherently on-policy strategy with LLM's immense\naction space and sparse reward. Further, RLVR can lead to the capability\nboundary collapse, narrowing the LLM's problem-solving scope. To address this\nproblem, we propose RL-PLUS, a novel approach that synergizes internal\nexploitation (i.e., Thinking) with external data (i.e., Learning) to achieve\nstronger reasoning capabilities and surpass the boundaries of base models.\nRL-PLUS integrates two core components: Multiple Importance Sampling to address\nfor distributional mismatch from external data, and an Exploration-Based\nAdvantage Function to guide the model towards high-value, unexplored reasoning\npaths. We provide both theoretical analysis and extensive experiments to\ndemonstrate the superiority and generalizability of our approach. The results\nshow that RL-PLUS achieves state-of-the-art performance compared with existing\nRLVR methods on six math reasoning benchmarks and exhibits superior performance\non six out-of-distribution reasoning tasks. It also achieves consistent and\nsignificant gains across diverse model families, with average relative\nimprovements ranging from 21.1\\% to 69.2\\%. Moreover, Pass@k curves across\nmultiple benchmarks indicate that RL-PLUS effectively resolves the capability\nboundary collapse problem.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00222v1", "cate": "cs.AI", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2508.00250", "title": "Jet Image Generation in High Energy Physics Using Diffusion Models", "authors": ["Victor D. Martinez", "Vidya Manian", "Sudhir Malik"], "categories": ["hep-ph", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "Subjects:       High Energy Physics - Phenomenology (hep-ph)", "pdf_link": null, "comments": "Comments:      The paper is under review at IEEE Transactions in Nuclear Science", "url": "http://arxiv.org/abs/2508.00250v1", "summary": "This article presents, for the first time, the application of diffusion\nmodels for generating jet images corresponding to proton-proton collision\nevents at the Large Hadron Collider (LHC). The kinematic variables of quark,\ngluon, W-boson, Z-boson, and top quark jets from the JetNet simulation dataset\nare mapped to two-dimensional image representations. Diffusion models are\ntrained on these images to learn the spatial distribution of jet constituents.\nWe compare the performance of score-based diffusion models and consistency\nmodels in accurately generating class-conditional jet images. Unlike approaches\nbased on latent distributions, our method operates directly in image space. The\nfidelity of the generated images is evaluated using several metrics, including\nthe Fr\\'echet Inception Distance (FID), which demonstrates that consistency\nmodels achieve higher fidelity and generation stability compared to score-based\ndiffusion models. These advancements offer significant improvements in\ncomputational efficiency and generation accuracy, providing valuable tools for\nHigh Energy Physics (HEP) research.", "comment": "The paper is under review at IEEE Transactions in Nuclear Science", "pdf_url": "http://arxiv.org/pdf/2508.00250v1", "cate": "hep-ph", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00267", "title": "Neighbor-Sampling Based Momentum Stochastic Methods for Training Graph Neural Networks", "authors": ["Molly Noel", "Gabriel Mancino-Ball", "Yangyang Xu"], "categories": ["math.OC", "cs.LG"], "primary_category": "Subjects:       Optimization and Control (math.OC)", "pdf_link": null, "comments": "Comments:      32 pages", "url": "http://arxiv.org/abs/2508.00267v1", "summary": "Graph convolutional networks (GCNs) are a powerful tool for graph\nrepresentation learning. Due to the recursive neighborhood aggregations\nemployed by GCNs, efficient training methods suffer from a lack of theoretical\nguarantees or are missing important practical elements from modern deep\nlearning algorithms, such as adaptivity and momentum. In this paper, we present\nseveral neighbor-sampling (NS) based Adam-type stochastic methods for solving a\nnonconvex GCN training problem. We utilize the control variate technique\nproposed by [1] to reduce the stochastic error caused by neighbor sampling.\nUnder standard assumptions for Adam-type methods, we show that our methods\nenjoy the optimal convergence rate. In addition, we conduct extensive numerical\nexperiments on node classification tasks with several benchmark datasets. The\nresults demonstrate superior performance of our methods over classic NS-based\nSGD that also uses the control-variate technique, especially for large-scale\ngraph datasets. Our code is available at https://github.com/RPI-OPT/CV-ADAM-GNN .", "comment": "32 pages", "pdf_url": "http://arxiv.org/pdf/2508.00267v1", "cate": "math.OC", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00300", "title": "MetaExplainer: A Framework to Generate Multi-Type User-Centered Explanations for AI Systems", "authors": ["Shruthi Chari", "Oshani Seneviratne", "Prithwish Chakraborty", "Pablo Meyer", "Deborah L. McGuinness"], "categories": ["cs.HC", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00300v1", "summary": "Explanations are crucial for building trustworthy AI systems, but a gap often\nexists between the explanations provided by models and those needed by users.\nTo address this gap, we introduce MetaExplainer, a neuro-symbolic framework\ndesigned to generate user-centered explanations. Our approach employs a\nthree-stage process: first, we decompose user questions into machine-readable\nformats using state-of-the-art large language models (LLM); second, we delegate\nthe task of generating system recommendations to model explainer methods; and\nfinally, we synthesize natural language explanations that summarize the\nexplainer outputs. Throughout this process, we utilize an Explanation Ontology\nto guide the language models and explainer methods. By leveraging LLMs and a\nstructured approach to explanation generation, MetaExplainer aims to enhance\nthe interpretability and trustworthiness of AI systems across various\napplications, providing users with tailored, question-driven explanations that\nbetter meet their needs. Comprehensive evaluations of MetaExplainer demonstrate\na step towards evaluating and utilizing current state-of-the-art explanation\nframeworks. Our results show high performance across all stages, with a 59.06%\nF1-score in question reframing, 70% faithfulness in model explanations, and 67%\ncontext-utilization in natural language synthesis. User studies corroborate\nthese findings, highlighting the creativity and comprehensiveness of generated\nexplanations. Tested on the Diabetes (PIMA Indian) tabular dataset,\nMetaExplainer supports diverse explanation types, including Contrastive,\nCounterfactual, Rationale, Case-Based, and Data explanations. The framework's\nversatility and traceability from using ontology to guide LLMs suggest broad\napplicability beyond the tested scenarios, positioning MetaExplainer as a\npromising tool for enhancing AI explainability across various domains.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00300v1", "cate": "cs.HC", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00319", "title": "Steering Guidance for Personalized Text-to-Image Diffusion Models", "authors": ["Sunghyun Park", "Seokeon Choi", "Hyoungwoo Park", "Sungrack Yun"], "categories": ["cs.CV", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV 2025", "url": "http://arxiv.org/abs/2508.00319v1", "summary": "Personalizing text-to-image diffusion models is crucial for adapting the\npre-trained models to specific target concepts, enabling diverse image\ngeneration. However, fine-tuning with few images introduces an inherent\ntrade-off between aligning with the target distribution (e.g., subject\nfidelity) and preserving the broad knowledge of the original model (e.g., text\neditability). Existing sampling guidance methods, such as classifier-free\nguidance (CFG) and autoguidance (AG), fail to effectively guide the output\ntoward well-balanced space: CFG restricts the adaptation to the target\ndistribution, while AG compromises text alignment. To address these\nlimitations, we propose personalization guidance, a simple yet effective method\nleveraging an unlearned weak model conditioned on a null text prompt. Moreover,\nour method dynamically controls the extent of unlearning in a weak model\nthrough weight interpolation between pre-trained and fine-tuned models during\ninference. Unlike existing guidance methods, which depend solely on guidance\nscales, our method explicitly steers the outputs toward a balanced latent space\nwithout additional computational overhead. Experimental results demonstrate\nthat our proposed guidance can improve text alignment and target distribution\nfidelity, integrating seamlessly with various fine-tuning strategies.", "comment": "ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2508.00319v1", "cate": "cs.CV", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00368", "title": "Preliminary Investigation into Uncertainty-Aware Attack Stage Classification", "authors": ["Alessandro Gaudenzi", "Lorenzo Nodari", "Lance Kaplan", "Alessandra Russo", "Murat Sensoy", "Federico Cerutti"], "categories": ["cs.CR", "cs.LG"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      Proceedings for SPAIML2025 workshop, 26/10/2025 Bologna Italy, co-located with ECAI2025", "url": "http://arxiv.org/abs/2508.00368v1", "summary": "Advanced Persistent Threats (APTs) represent a significant challenge in\ncybersecurity due to their prolonged, multi-stage nature and the sophistication\nof their operators. Traditional detection systems typically focus on\nidentifying malicious activity in binary terms (benign or malicious) without\naccounting for the progression of an attack. However, effective response\nstrategies depend on accurate inference of the attack's current stage, as\ncountermeasures must be tailored to whether an adversary is in the early\nreconnaissance phase or actively conducting exploitation or exfiltration. This\nwork addresses the problem of attack stage inference under uncertainty, with a\nfocus on robustness to out-of-distribution (OOD) inputs. We propose a\nclassification approach based on Evidential Deep Learning (EDL), which models\npredictive uncertainty by outputting parameters of a Dirichlet distribution\nover possible stages. This allows the system not only to predict the most\nlikely stage of an attack but also to indicate when it is uncertain or the\ninput lies outside the training distribution. Preliminary experiments in a\nsimulated environment demonstrate that the proposed model can accurately infer\nthe stage of an attack with calibrated confidence while effectively detecting\nOOD inputs, which may indicate changes in the attackers' tactics. These results\nsupport the feasibility of deploying uncertainty-aware models for staged threat\ndetection in dynamic and adversarial environments.", "comment": "Proceedings for SPAIML2025 workshop, 26/10/2025 Bologna Italy,\n  co-located with ECAI2025", "pdf_url": "http://arxiv.org/pdf/2508.00368v1", "cate": "cs.CR", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00370", "title": "EdgeInfinite-Instruct: Bridging SFT-Based Optimization and NPU-Level Efficiency for Edge Devices", "authors": ["Jiyu Chen", "Poh Seng Lim", "Shuang Peng", "Daxiong Luo", "JungHau Foo", "Yap Deep", "Timothy Lee Jun Jie", "Kelvin Teh Kae Wen", "Fan Yang", "Danyu Feng", "Hao-Yun Chen", "Peng-Wen Chen", "Fangyuan Li", "Xiaoxin Chen", "Wong Wai Mun"], "categories": ["cs.CL", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      9 pages", "url": "http://arxiv.org/abs/2508.00370v1", "summary": "Deploying Transformer-based large language models (LLMs) on\nresource-constrained edge devices for long-sequence tasks remains challenging\ndue to the quadratic time complexity of self-attention and growing Key-Value\n(KV) cache demands. While existing KV cache optimizations improve memory\nefficiency, they often fail to reduce time to first token (TTFT) and may\ndegrade performance through token pruning. Alternative sequence modeling\narchitectures address some of these limitations, but typically require full\nretraining and lack infrastructure support. EdgeInfinite offers an efficient\nsolution by fine-tuning only a small subset of parameters, maintaining quality\nwhile reducing both computational and memory costs, including improved TTFT.\nHowever, its instruction-following ability is limited, and it lacks\nmobile-specific optimizations. To address these issues, we propose\nEdgeInfinite-Instruct, which introduces a Segmented Supervised Fine-Tuning\n(S-SFT) strategy tailored to long-sequence tasks such as summarization and\nquestion answering. We further optimized EdgeInfinite-Instruct for efficient\ndeployment on edge NPUs by employing fine-grained post-training quantization\n(PTQ) to reduce computational demands while maintaining accuracy, and by\nimplementing a fixed-shape computation graph that balances memory usage and\non-device efficiency through scenario-specific customization of input token and\ncache sizes. Experiments on long-context benchmarks and real-world mobile tasks\nshow that our approach improves domain-specific performance while maintaining\nefficiency on NPU-accelerated edge devices.", "comment": "9 pages", "pdf_url": "http://arxiv.org/pdf/2508.00370v1", "cate": "cs.CL", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00381", "title": "Advancing Welding Defect Detection in Maritime Operations via Adapt-WeldNet and Defect Detection Interpretability Analysis", "authors": ["Kamal Basha S", "Athira Nambiar"], "categories": ["cs.CV", "cs.AI", "cs.CE", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00381v1", "summary": "Weld defect detection is crucial for ensuring the safety and reliability of\npiping systems in the oil and gas industry, especially in challenging marine\nand offshore environments. Traditional non-destructive testing (NDT) methods\noften fail to detect subtle or internal defects, leading to potential failures\nand costly downtime. Furthermore, existing neural network-based approaches for\ndefect classification frequently rely on arbitrarily selected pretrained\narchitectures and lack interpretability, raising safety concerns for\ndeployment. To address these challenges, this paper introduces\n``Adapt-WeldNet\", an adaptive framework for welding defect detection that\nsystematically evaluates various pre-trained architectures, transfer learning\nstrategies, and adaptive optimizers to identify the best-performing model and\nhyperparameters, optimizing defect detection and providing actionable insights.\nAdditionally, a novel Defect Detection Interpretability Analysis (DDIA)\nframework is proposed to enhance system transparency. DDIA employs Explainable\nAI (XAI) techniques, such as Grad-CAM and LIME, alongside domain-specific\nevaluations validated by certified ASNT NDE Level II professionals.\nIncorporating a Human-in-the-Loop (HITL) approach and aligning with the\nprinciples of Trustworthy AI, DDIA ensures the reliability, fairness, and\naccountability of the defect detection system, fostering confidence in\nautomated decisions through expert validation. By improving both performance\nand interpretability, this work enhances trust, safety, and reliability in\nwelding defect detection systems, supporting critical operations in offshore\nand marine environments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00381v1", "cate": "cs.CV", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00383", "title": "$MV_{Hybrid}$: Improving Spatial Transcriptomics Prediction with Hybrid State Space-Vision Transformer Backbone in Pathology Vision Foundation Models", "authors": ["Won June Cho", "Hongjun Yoon", "Daeky Jeong", "Hyeongyeol Lim", "Yosep Chong"], "categories": ["cs.CV", "cs.AI", "cs.CE", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted (Oral) in MICCAI 2025 COMPAYL Workshop", "url": "http://arxiv.org/abs/2508.00383v1", "summary": "Spatial transcriptomics reveals gene expression patterns within tissue\ncontext, enabling precision oncology applications such as treatment response\nprediction, but its high cost and technical complexity limit clinical adoption.\nPredicting spatial gene expression (biomarkers) from routine histopathology\nimages offers a practical alternative, yet current vision foundation models\n(VFMs) in pathology based on Vision Transformer (ViT) backbones perform below\nclinical standards. Given that VFMs are already trained on millions of diverse\nwhole slide images, we hypothesize that architectural innovations beyond ViTs\nmay better capture the low-frequency, subtle morphological patterns correlating\nwith molecular phenotypes. By demonstrating that state space models initialized\nwith negative real eigenvalues exhibit strong low-frequency bias, we introduce\n$MV_{Hybrid}$, a hybrid backbone architecture combining state space models\n(SSMs) with ViT. We compare five other different backbone architectures for\npathology VFMs, all pretrained on identical colorectal cancer datasets using\nthe DINOv2 self-supervised learning method. We evaluate all pretrained models\nusing both random split and leave-one-study-out (LOSO) settings of the same\nbiomarker dataset. In LOSO evaluation, $MV_{Hybrid}$ achieves 57% higher\ncorrelation than the best-performing ViT and shows 43% smaller performance\ndegradation compared to random split in gene expression prediction,\ndemonstrating superior performance and robustness, respectively. Furthermore,\n$MV_{Hybrid}$ shows equal or better downstream performance in classification,\npatch retrieval, and survival prediction tasks compared to that of ViT, showing\nits promise as a next-generation pathology VFM backbone. Our code is publicly\navailable at: https://github.com/deepnoid-ai/MVHybrid.", "comment": "Accepted (Oral) in MICCAI 2025 COMPAYL Workshop", "pdf_url": "http://arxiv.org/pdf/2508.00383v1", "cate": "cs.CV", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00385", "title": "Multi-Layer Attention is the Amplifier of Demonstration Effectiveness", "authors": ["Dingzirui Wang", "Xuangliang Zhang", "Keyan Xu", "Qingfu Zhu", "Wanxiang Che", "Yang Deng"], "categories": ["cs.CL", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00385v1", "summary": "Numerous studies have investigated the underlying mechanisms of in-context\nlearning (ICL) effectiveness to inspire the design of related methods. However,\nexisting work predominantly assumes the effectiveness of the demonstrations\nprovided within ICL, while many research indicates that not all demonstrations\nare effective, failing to yielding any performance improvement during ICL.\nTherefore, in this paper, we investigate the reasons behind demonstration\nineffectiveness. Our analysis is based on gradient flow and linear\nself-attention models. By setting the gradient flow to zero, we deduce that a\ndemonstration becomes ineffective if its information has either been learned by\nthe model or is irrelevant to the user query. Furthermore, we demonstrate that\nin multi-layer models, the disparity in effectiveness among demonstrations is\namplified with layer increasing, causing the model to focus more on effective\nones. Considering that current demonstration selection methods primarily focus\non the relevance to the user query while overlooking the information that the\nmodel has already assimilated, we propose a novel method called GradS, which\nleverages gradient flow for demonstration selection. We use the magnitude of\nthe gradient flow of the demonstration with respect to a given user query as\nthe criterion, thereby ensuring the effectiveness of the chosen ones. We\nvalidate our derivation and GradS on four prominent LLMs across five mainstream\ndatasets. The experimental results confirm that the disparity in effectiveness\namong demonstrations is magnified as the model layer increases, substantiating\nour derivations. Moreover, GradS achieves a relative improvement of $6.8\\%$ on\naverage over the strongest baselines, demonstrating its effectiveness.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00385v1", "cate": "cs.CL", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00429", "title": "ReaGAN: Node-as-Agent-Reasoning Graph Agentic Network", "authors": ["Minghao Guo", "Xi Zhu", "Jingyuan Huang", "Kai Mei", "Yongfeng Zhang"], "categories": ["cs.CL", "cs.LG", "cs.MA"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      17 pages, work in progress", "url": "http://arxiv.org/abs/2508.00429v1", "summary": "Graph Neural Networks (GNNs) have achieved remarkable success in graph-based\nlearning by propagating information among neighbor nodes via predefined\naggregation mechanisms. However, such fixed schemes often suffer from two key\nlimitations. First, they cannot handle the imbalance in node informativeness --\nsome nodes are rich in information, while others remain sparse. Second,\npredefined message passing primarily leverages local structural similarity\nwhile ignoring global semantic relationships across the graph, limiting the\nmodel's ability to capture distant but relevant information. We propose\nRetrieval-augmented Graph Agentic Network (ReaGAN), an agent-based framework\nthat empowers each node with autonomous, node-level decision-making. Each node\nacts as an agent that independently plans its next action based on its internal\nmemory, enabling node-level planning and adaptive message propagation.\nAdditionally, retrieval-augmented generation (RAG) allows nodes to access\nsemantically relevant content and build global relationships in the graph.\nReaGAN achieves competitive performance under few-shot in-context settings\nusing a frozen LLM backbone without fine-tuning, showcasing the potential of\nagentic planning and local-global retrieval in graph learning.", "comment": "17 pages, work in progress", "pdf_url": "http://arxiv.org/pdf/2508.00429v1", "cate": "cs.CL", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00447", "title": "CLIPTime: Time-Aware Multimodal Representation Learning from Images and Text", "authors": ["Anju Rani", "Daniel Ortiz-Arroyo", "Petar Durdevic"], "categories": ["cs.CV", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      11 pages, 8 figures", "url": "http://arxiv.org/abs/2508.00447v1", "summary": "Understanding the temporal dynamics of biological growth is critical across\ndiverse fields such as microbiology, agriculture, and biodegradation research.\nAlthough vision-language models like Contrastive Language Image Pretraining\n(CLIP) have shown strong capabilities in joint visual-textual reasoning, their\neffectiveness in capturing temporal progression remains limited. To address\nthis, we propose CLIPTime, a multimodal, multitask framework designed to\npredict both the developmental stage and the corresponding timestamp of fungal\ngrowth from image and text inputs. Built upon the CLIP architecture, our model\nlearns joint visual-textual embeddings and enables time-aware inference without\nrequiring explicit temporal input during testing. To facilitate training and\nevaluation, we introduce a synthetic fungal growth dataset annotated with\naligned timestamps and categorical stage labels. CLIPTime jointly performs\nclassification and regression, predicting discrete growth stages alongside\ncontinuous timestamps. We also propose custom evaluation metrics, including\ntemporal accuracy and regression error, to assess the precision of time-aware\npredictions. Experimental results demonstrate that CLIPTime effectively models\nbiological progression and produces interpretable, temporally grounded outputs,\nhighlighting the potential of vision-language models in real-world biological\nmonitoring applications.", "comment": "11 pages, 8 figures", "pdf_url": "http://arxiv.org/pdf/2508.00447v1", "cate": "cs.CV", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00544", "title": "PaPaformer: Language Model from Pre-trained Paraller Paths", "authors": ["Joonas Tapaninaho", "Mourad Oussala"], "categories": ["cs.CL", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00544v1", "summary": "The training of modern large-language models requires an increasingly amount\nof computation power and time. Even smaller variants, such as small-language\nmodels (SLMs), take several days to train in the best-case scenarios, often\nrequiring multiple GPUs. This paper explores methods to train and evaluate\ndecoder-only transformer-based language models in hours instead of days/weeks.\nWe introduces \\textit{PaPaformer}, a decoder-only transformer architecture\nvariant, whose lower-dimensional parallel paths are combined into larger model.\nThe paper shows that these lower-dimensional paths can be trained individually\nwith different types of training data and then combined into one larger model.\nThis method gives the option to reduce the total number of model parameters and\nthe training time with increasing performance. Moreover, the use of parallel\npath structure opens interesting possibilities to customize paths to\naccommodate specific task requirements.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00544v1", "cate": "cs.CL", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00558", "title": "Guiding Diffusion-Based Articulated Object Generation by Partial Point Cloud Alignment and Physical Plausibility Constraints", "authors": ["Jens U. Kreber", "Joerg Stueckler"], "categories": ["cs.CV", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted for publication at the IEEE/CVF International Conference on Computer Vision (ICCV), 2025", "url": "http://arxiv.org/abs/2508.00558v1", "summary": "Articulated objects are an important type of interactable objects in everyday\nenvironments. In this paper, we propose PhysNAP, a novel diffusion model-based\napproach for generating articulated objects that aligns them with partial point\nclouds and improves their physical plausibility. The model represents part\nshapes by signed distance functions (SDFs). We guide the reverse diffusion\nprocess using a point cloud alignment loss computed using the predicted SDFs.\nAdditionally, we impose non-penetration and mobility constraints based on the\npart SDFs for guiding the model to generate more physically plausible objects.\nWe also make our diffusion approach category-aware to further improve point\ncloud alignment if category information is available. We evaluate the\ngenerative ability and constraint consistency of samples generated with PhysNAP\nusing the PartNet-Mobility dataset. We also compare it with an unguided\nbaseline diffusion model and demonstrate that PhysNAP can improve constraint\nconsistency and provides a tradeoff with generative ability.", "comment": "Accepted for publication at the IEEE/CVF International Conference on\n  Computer Vision (ICCV), 2025", "pdf_url": "http://arxiv.org/pdf/2508.00558v1", "cate": "cs.CV", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00596", "title": "Information-Theoretic Decentralized Secure Aggregation with Collusion Resilience", "authors": ["Xiang Zhang", "Zhou Li", "Shuangyang Li", "Kai Wan", "Derrick Wing Kwan Ng", "Giuseppe Caire"], "categories": ["cs.IT", "cs.CR", "cs.DC", "cs.LG", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "Comments:      Submitted to IEEE for potential journal publication", "url": "http://arxiv.org/abs/2508.00596v1", "summary": "In decentralized federated learning (FL), multiple clients collaboratively\nlearn a shared machine learning (ML) model by leveraging their privately held\ndatasets distributed across the network, through interactive exchange of the\nintermediate model updates. To ensure data security, cryptographic techniques\nare commonly employed to protect model updates during aggregation. Despite\ngrowing interest in secure aggregation, existing works predominantly focus on\nprotocol design and computational guarantees, with limited understanding of the\nfundamental information-theoretic limits of such systems. Moreover, optimal\nbounds on communication and key usage remain unknown in decentralized settings,\nwhere no central aggregator is available. Motivated by these gaps, we study the\nproblem of decentralized secure aggregation (DSA) from an information-theoretic\nperspective. Specifically, we consider a network of $K$ fully-connected users,\neach holding a private input -- an abstraction of local training data -- who\naim to securely compute the sum of all inputs. The security constraint requires\nthat no user learns anything beyond the input sum, even when colluding with up\nto $T$ other users. We characterize the optimal rate region, which specifies\nthe minimum achievable communication and secret key rates for DSA. In\nparticular, we show that to securely compute one symbol of the desired input\nsum, each user must (i) transmit at least one symbol to others, (ii) hold at\nleast one symbol of secret key, and (iii) all users must collectively hold no\nfewer than $K - 1$ independent key symbols. Our results establish the\nfundamental performance limits of DSA, providing insights for the design of\nprovably secure and communication-efficient protocols in distributed learning\nsystems.", "comment": "Submitted to IEEE for potential journal publication", "pdf_url": "http://arxiv.org/pdf/2508.00596v1", "cate": "cs.IT", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00600", "title": "A Context-Aware Dual-Metric Framework for Confidence Estimation in Large Language Models", "authors": ["Mingruo Yuan", "Shuyi Zhang", "Ben Kao"], "categories": ["cs.CL", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00600v1", "summary": "Accurate confidence estimation is essential for trustworthy large language\nmodels (LLMs) systems, as it empowers the user to determine when to trust\noutputs and enables reliable deployment in safety-critical applications.\nCurrent confidence estimation methods for LLMs neglect the relevance between\nresponses and contextual information, a crucial factor in output quality\nevaluation, particularly in scenarios where background knowledge is provided.\nTo bridge this gap, we propose CRUX (Context-aware entropy Reduction and\nUnified consistency eXamination), the first framework that integrates context\nfaithfulness and consistency for confidence estimation via two novel metrics.\nFirst, contextual entropy reduction represents data uncertainty with the\ninformation gain through contrastive sampling with and without context. Second,\nunified consistency examination captures potential model uncertainty through\nthe global consistency of the generated answers with and without context.\nExperiments across three benchmark datasets (CoQA, SQuAD, QuAC) and two\ndomain-specific datasets (BioASQ, EduQG) demonstrate CRUX's effectiveness,\nachieving the highest AUROC than existing baselines.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00600v1", "cate": "cs.CL", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00602", "title": "LeakSealer: A Semisupervised Defense for LLMs Against Prompt Injection and Leakage Attacks", "authors": ["Francesco Panebianco", "Stefano Bonfanti", "Francesco Trovò", "Michele Carminati"], "categories": ["cs.CR", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      22 pages, preprint", "url": "http://arxiv.org/abs/2508.00602v1", "summary": "The generalization capabilities of Large Language Models (LLMs) have led to\ntheir widespread deployment across various applications. However, this\nincreased adoption has introduced several security threats, notably in the\nforms of jailbreaking and data leakage attacks. Additionally, Retrieval\nAugmented Generation (RAG), while enhancing context-awareness in LLM responses,\nhas inadvertently introduced vulnerabilities that can result in the leakage of\nsensitive information. Our contributions are twofold. First, we introduce a\nmethodology to analyze historical interaction data from an LLM system, enabling\nthe generation of usage maps categorized by topics (including adversarial\ninteractions). This approach further provides forensic insights for tracking\nthe evolution of jailbreaking attack patterns. Second, we propose LeakSealer, a\nmodel-agnostic framework that combines static analysis for forensic insights\nwith dynamic defenses in a Human-In-The-Loop (HITL) pipeline. This technique\nidentifies topic groups and detects anomalous patterns, allowing for proactive\ndefense mechanisms. We empirically evaluate LeakSealer under two scenarios: (1)\njailbreak attempts, employing a public benchmark dataset, and (2) PII leakage,\nsupported by a curated dataset of labeled LLM interactions. In the static\nsetting, LeakSealer achieves the highest precision and recall on the ToxicChat\ndataset when identifying prompt injection. In the dynamic setting, PII leakage\ndetection achieves an AUPRC of $0.97$, significantly outperforming baselines\nsuch as Llama Guard.", "comment": "22 pages, preprint", "pdf_url": "http://arxiv.org/pdf/2508.00602v1", "cate": "cs.CR", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00617", "title": "Constructive Disintegration and Conditional Modes", "authors": ["Nathaël Da Costa", "Marvin Pförtner", "Jon Cockayne"], "categories": ["math.ST", "cs.LG", "math.PR", "stat.ML", "stat.TH"], "primary_category": "Subjects:       Statistics Theory (math.ST)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00617v1", "summary": "Conditioning, the central operation in Bayesian statistics, is formalised by\nthe notion of disintegration of measures. However, due to the implicit nature\nof their definition, constructing disintegrations is often difficult. A\nfolklore result in machine learning conflates the construction of a\ndisintegration with the restriction of probability density functions onto the\nsubset of events that are consistent with a given observation. We provide a\ncomprehensive set of mathematical tools which can be used to construct\ndisintegrations and apply these to find densities of disintegrations on\ndifferentiable manifolds. Using our results, we provide a disturbingly simple\nexample in which the restricted density and the disintegration density\ndrastically disagree. Motivated by applications in approximate Bayesian\ninference and Bayesian inverse problems, we further study the modes of\ndisintegrations. We show that the recently introduced notion of a \"conditional\nmode\" does not coincide in general with the modes of the conditional measure\nobtained through disintegration, but rather the modes of the restricted\nmeasure. We also discuss the implications of the discrepancy between the two\nmeasures in practice, advocating for the utility of both approaches depending\non the modelling context.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00617v1", "cate": "math.ST", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00619", "title": "DACTYL: Diverse Adversarial Corpus of Texts Yielded from Large Language Models", "authors": ["Shantanu Thorat", "Andrew Caines"], "categories": ["cs.CL", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      MPhil in Advanced Computer Science thesis for University of Cambridge", "url": "http://arxiv.org/abs/2508.00619v1", "summary": "Existing AIG (AI-generated) text detectors struggle in real-world settings\ndespite succeeding in internal testing, suggesting that they may not be robust\nenough. We rigorously examine the machine-learning procedure to build these\ndetectors to address this. Most current AIG text detection datasets focus on\nzero-shot generations, but little work has been done on few-shot or one-shot\ngenerations, where LLMs are given human texts as an example. In response, we\nintroduce the Diverse Adversarial Corpus of Texts Yielded from Language models\n(DACTYL), a challenging AIG text detection dataset focusing on\none-shot/few-shot generations. We also include texts from domain-specific\ncontinued-pre-trained (CPT) language models, where we fully train all\nparameters using a memory-efficient optimization approach. Many existing AIG\ntext detectors struggle significantly on our dataset, indicating a potential\nvulnerability to one-shot/few-shot and CPT-generated texts. We also train our\nown classifiers using two approaches: standard binary cross-entropy (BCE)\noptimization and a more recent approach, deep X-risk optimization (DXO). While\nBCE-trained classifiers marginally outperform DXO classifiers on the DACTYL\ntest set, the latter excels on out-of-distribution (OOD) texts. In our mock\ndeployment scenario in student essay detection with an OOD student essay\ndataset, the best DXO classifier outscored the best BCE-trained classifier by\n50.56 macro-F1 score points at the lowest false positive rates for both. Our\nresults indicate that DXO classifiers generalize better without overfitting to\nthe test set. Our experiments highlight several areas of improvement for AIG\ntext detectors.", "comment": "MPhil in Advanced Computer Science thesis for University of Cambridge", "pdf_url": "http://arxiv.org/pdf/2508.00619v1", "cate": "cs.CL", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00620", "title": "Backdoor Attacks on Deep Learning Face Detection", "authors": ["Quentin Le Roux", "Yannick Teglia", "Teddy Furon", "Philippe Loubet-Moundi"], "categories": ["cs.CV", "cs.AI", "cs.CR", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00620v1", "summary": "Face Recognition Systems that operate in unconstrained environments capture\nimages under varying conditions,such as inconsistent lighting, or diverse face\nposes. These challenges require including a Face Detection module that\nregresses bounding boxes and landmark coordinates for proper Face Alignment.\nThis paper shows the effectiveness of Object Generation Attacks on Face\nDetection, dubbed Face Generation Attacks, and demonstrates for the first time\na Landmark Shift Attack that backdoors the coordinate regression task performed\nby face detectors. We then offer mitigations against these vulnerabilities.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00620v1", "cate": "cs.CV", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00658", "title": "Multi-Band Variable-Lag Granger Causality: A Unified Framework for Causal Time Series Inference across Frequencies", "authors": ["Chakattrai Sookkongwaree", "Tattep Lakmuang", "Chainarong Amornbunchornvej"], "categories": ["cs.AI", "cs.LG", "econ.EM", "stat.ME"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      First draft", "url": "http://arxiv.org/abs/2508.00658v1", "summary": "Understanding causal relationships in time series is fundamental to many\ndomains, including neuroscience, economics, and behavioral science. Granger\ncausality is one of the well-known techniques for inferring causality in time\nseries. Typically, Granger causality frameworks have a strong fix-lag\nassumption between cause and effect, which is often unrealistic in complex\nsystems. While recent work on variable-lag Granger causality (VLGC) addresses\nthis limitation by allowing a cause to influence an effect with different time\nlags at each time point, it fails to account for the fact that causal\ninteractions may vary not only in time delay but also across frequency bands.\nFor example, in brain signals, alpha-band activity may influence another region\nwith a shorter delay than slower delta-band oscillations. In this work, we\nformalize Multi-Band Variable-Lag Granger Causality (MB-VLGC) and propose a\nnovel framework that generalizes traditional VLGC by explicitly modeling\nfrequency-dependent causal delays. We provide a formal definition of MB-VLGC,\ndemonstrate its theoretical soundness, and propose an efficient inference\npipeline. Extensive experiments across multiple domains demonstrate that our\nframework significantly outperforms existing methods on both synthetic and\nreal-world datasets, confirming its broad applicability to any type of time\nseries data. Code and datasets are publicly available.", "comment": "First draft", "pdf_url": "http://arxiv.org/pdf/2508.00658v1", "cate": "cs.AI", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00665", "title": "Transparent Adaptive Learning via Data-Centric Multimodal Explainable AI", "authors": ["Maryam Mosleh", "Marie Devlin", "Ellis Solaiman"], "categories": ["cs.AI", "cs.HC", "cs.LG"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00665v1", "summary": "Artificial intelligence-driven adaptive learning systems are reshaping\neducation through data-driven adaptation of learning experiences. Yet many of\nthese systems lack transparency, offering limited insight into how decisions\nare made. Most explainable AI (XAI) techniques focus on technical outputs but\nneglect user roles and comprehension. This paper proposes a hybrid framework\nthat integrates traditional XAI techniques with generative AI models and user\npersonalisation to generate multimodal, personalised explanations tailored to\nuser needs. We redefine explainability as a dynamic communication process\ntailored to user roles and learning goals. We outline the framework's design,\nkey XAI limitations in education, and research directions on accuracy,\nfairness, and personalisation. Our aim is to move towards explainable AI that\nenhances transparency while supporting user-centred experiences.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00665v1", "cate": "cs.AI", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00669", "title": "Medical Reasoning in the Era of LLMs: A Systematic Review of Enhancement Techniques and Applications", "authors": ["Wenxuan Wang", "Zizhan Ma", "Meidan Ding", "Shiyi Zheng", "Shengyuan Liu", "Jie Liu", "Jiaming Ji", "Wenting Chen", "Xiang Li", "Linlin Shen", "Yixuan Yuan"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00669v1", "summary": "The proliferation of Large Language Models (LLMs) in medicine has enabled\nimpressive capabilities, yet a critical gap remains in their ability to perform\nsystematic, transparent, and verifiable reasoning, a cornerstone of clinical\npractice. This has catalyzed a shift from single-step answer generation to the\ndevelopment of LLMs explicitly designed for medical reasoning. This paper\nprovides the first systematic review of this emerging field. We propose a\ntaxonomy of reasoning enhancement techniques, categorized into training-time\nstrategies (e.g., supervised fine-tuning, reinforcement learning) and test-time\nmechanisms (e.g., prompt engineering, multi-agent systems). We analyze how\nthese techniques are applied across different data modalities (text, image,\ncode) and in key clinical applications such as diagnosis, education, and\ntreatment planning. Furthermore, we survey the evolution of evaluation\nbenchmarks from simple accuracy metrics to sophisticated assessments of\nreasoning quality and visual interpretability. Based on an analysis of 60\nseminal studies from 2022-2025, we conclude by identifying critical challenges,\nincluding the faithfulness-plausibility gap and the need for native multimodal\nreasoning, and outlining future directions toward building efficient, robust,\nand sociotechnically responsible medical AI.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00669v1", "cate": "cs.CL", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00674", "title": "Context-Aware Visualization for Explainable AI Recommendations in Social Media: A Vision for User-Aligned Explanations", "authors": ["Banan Alkhateeb", "Ellis Solaiman"], "categories": ["cs.AI", "cs.HC", "cs.LG"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00674v1", "summary": "Social media platforms today strive to improve user experience through AI\nrecommendations, yet the value of such recommendations vanishes as users do not\nunderstand the reasons behind them. This issue arises because explainability in\nsocial media is general and lacks alignment with user-specific needs. In this\nvision paper, we outline a user-segmented and context-aware explanation layer\nby proposing a visual explanation system with diverse explanation methods. The\nproposed system is framed by the variety of user needs and contexts, showing\nexplanations in different visualized forms, including a technically detailed\nversion for AI experts and a simplified one for lay users. Our framework is the\nfirst to jointly adapt explanation style (visual vs. numeric) and granularity\n(expert vs. lay) inside a single pipeline. A public pilot with 30 X users will\nvalidate its impact on decision-making and trust.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00674v1", "cate": "cs.AI", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00679", "title": "Segment First, Retrieve Better: Realistic Legal Search via Rhetorical Role-Based Queries", "authors": ["Shubham Kumar Nigam", "Tanmay Dubey", "Noel Shallum", "Arnab Bhattacharya"], "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00679v1", "summary": "Legal precedent retrieval is a cornerstone of the common law system, governed\nby the principle of stare decisis, which demands consistency in judicial\ndecisions. However, the growing complexity and volume of legal documents\nchallenge traditional retrieval methods. TraceRetriever mirrors real-world\nlegal search by operating with limited case information, extracting only\nrhetorically significant segments instead of requiring complete documents. Our\npipeline integrates BM25, Vector Database, and Cross-Encoder models, combining\ninitial results through Reciprocal Rank Fusion before final re-ranking.\nRhetorical annotations are generated using a Hierarchical BiLSTM CRF classifier\ntrained on Indian judgments. Evaluated on IL-PCR and COLIEE 2025 datasets,\nTraceRetriever addresses growing document volume challenges while aligning with\npractical search constraints, reliable and scalable foundation for precedent\nretrieval enhancing legal research when only partial case knowledge is\navailable.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00679v1", "cate": "cs.CL", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00709", "title": "NyayaRAG: Realistic Legal Judgment Prediction with RAG under the Indian Common Law System", "authors": ["Shubham Kumar Nigam", "Balaramamahanthi Deepak Patnaik", "Shivam Mishra", "Ajay Varghese Thomas", "Noel Shallum", "Kripabandhu Ghosh", "Arnab Bhattacharya"], "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00709v1", "summary": "Legal Judgment Prediction (LJP) has emerged as a key area in AI for law,\naiming to automate judicial outcome forecasting and enhance interpretability in\nlegal reasoning. While previous approaches in the Indian context have relied on\ninternal case content such as facts, issues, and reasoning, they often overlook\na core element of common law systems, which is reliance on statutory provisions\nand judicial precedents. In this work, we propose NyayaRAG, a\nRetrieval-Augmented Generation (RAG) framework that simulates realistic\ncourtroom scenarios by providing models with factual case descriptions,\nrelevant legal statutes, and semantically retrieved prior cases. NyayaRAG\nevaluates the effectiveness of these combined inputs in predicting court\ndecisions and generating legal explanations using a domain-specific pipeline\ntailored to the Indian legal system. We assess performance across various input\nconfigurations using both standard lexical and semantic metrics as well as\nLLM-based evaluators such as G-Eval. Our results show that augmenting factual\ninputs with structured legal knowledge significantly improves both predictive\naccuracy and explanation quality.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00709v1", "cate": "cs.CL", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00742", "title": "Applying Psychometrics to Large Language Model Simulated Populations: Recreating the HEXACO Personality Inventory Experiment with Generative Agents", "authors": ["Sarah Mercer", "Daniel P. Martin", "Phil Swatton"], "categories": ["cs.CL", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      26 pages, 14 figures", "url": "http://arxiv.org/abs/2508.00742v1", "summary": "Generative agents powered by Large Language Models demonstrate human-like\ncharacteristics through sophisticated natural language interactions. Their\nability to assume roles and personalities based on predefined character\nbiographies has positioned them as cost-effective substitutes for human\nparticipants in social science research. This paper explores the validity of\nsuch persona-based agents in representing human populations; we recreate the\nHEXACO personality inventory experiment by surveying 310 GPT-4 powered agents,\nconducting factor analysis on their responses, and comparing these results to\nthe original findings presented by Ashton, Lee, & Goldberg in 2004. Our results\nfound 1) a coherent and reliable personality structure was recoverable from the\nagents' responses demonstrating partial alignment to the HEXACO framework. 2)\nthe derived personality dimensions were consistent and reliable within GPT-4,\nwhen coupled with a sufficiently curated population, and 3) cross-model\nanalysis revealed variability in personality profiling, suggesting\nmodel-specific biases and limitations. We discuss the practical considerations\nand challenges encountered during the experiment. This study contributes to the\nongoing discourse on the potential benefits and limitations of using generative\nagents in social science research and provides useful guidance on designing\nconsistent and representative agent personas to maximise coverage and\nrepresentation of human personality traits.", "comment": "26 pages, 14 figures", "pdf_url": "http://arxiv.org/pdf/2508.00742v1", "cate": "cs.CL", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00743", "title": "Agentic large language models improve retrieval-based radiology question answering", "authors": ["Sebastian Wind", "Jeta Sopa", "Daniel Truhn", "Mahshad Lotfinia", "Tri-Thien Nguyen", "Keno Bressem", "Lisa Adams", "Mirabela Rusu", "Harald Köstler", "Gerhard Wellein", "Andreas Maier", "Soroosh Tayebi Arasteh"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00743v1", "summary": "Clinical decision-making in radiology increasingly benefits from artificial\nintelligence (AI), particularly through large language models (LLMs). However,\ntraditional retrieval-augmented generation (RAG) systems for radiology question\nanswering (QA) typically rely on single-step retrieval, limiting their ability\nto handle complex clinical reasoning tasks. Here we propose an agentic RAG\nframework enabling LLMs to autonomously decompose radiology questions,\niteratively retrieve targeted clinical evidence from Radiopaedia, and\ndynamically synthesize evidence-based responses. We evaluated 24 LLMs spanning\ndiverse architectures, parameter scales (0.5B to >670B), and training paradigms\n(general-purpose, reasoning-optimized, clinically fine-tuned), using 104\nexpert-curated radiology questions from previously established RSNA-RadioQA and\nExtendedQA datasets. Agentic retrieval significantly improved mean diagnostic\naccuracy over zero-shot prompting (73% vs. 64%; P<0.001) and conventional\nonline RAG (73% vs. 68%; P<0.001). The greatest gains occurred in mid-sized\nmodels (e.g., Mistral Large improved from 72% to 81%) and small-scale models\n(e.g., Qwen 2.5-7B improved from 55% to 71%), while very large models (>200B\nparameters) demonstrated minimal changes (<2% improvement). Additionally,\nagentic retrieval reduced hallucinations (mean 9.4%) and retrieved clinically\nrelevant context in 46% of cases, substantially aiding factual grounding. Even\nclinically fine-tuned models exhibited meaningful improvements (e.g.,\nMedGemma-27B improved from 71% to 81%), indicating complementary roles of\nretrieval and fine-tuning. These results highlight the potential of agentic\nframeworks to enhance factuality and diagnostic accuracy in radiology QA,\nparticularly among mid-sized LLMs, warranting future studies to validate their\nclinical utility.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00743v1", "cate": "cs.CL", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2212.01233", "title": "Safe machine learning model release from Trusted Research Environments: The SACRO-ML package", "authors": ["Jim Smith", "Richard J. Preen", "Andrew McCarthy", "Maha Albashir", "Alba Crespi-Boixader", "Shahzad Mumtaz", "Christian Cole", "James Liley", "Jost Migenda", "Simon Rogers", "Yola Jones"], "categories": ["cs.LG", "cs.CR", "cs.IR"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2212.01233v4", "summary": "We present SACRO-ML, an integrated suite of open source Python tools to\nfacilitate the statistical disclosure control (SDC) of machine learning (ML)\nmodels trained on confidential data prior to public release. SACRO-ML combines\n(i) a SafeModel package that extends commonly used ML models to provide\nante-hoc SDC by assessing the vulnerability of disclosure posed by the training\nregime; and (ii) an Attacks package that provides post-hoc SDC by rigorously\nassessing the empirical disclosure risk of a model through a variety of\nsimulated attacks after training. The SACRO-ML code and documentation are\navailable under an MIT license at https://github.com/AI-SDC/SACRO-ML", "comment": null, "pdf_url": "http://arxiv.org/pdf/2212.01233v4", "cate": "cs.LG", "date": "2022-12-02", "updated": "2025-08-01"}
{"id": "2305.04095", "title": "Gradient Leakage Defense with Key-Lock Module for Federated Learning", "authors": ["Hanchi Ren", "Jingjing Deng", "Xianghua Xie"], "categories": ["cs.LG", "cs.AI", "cs.CV"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      The source code can be found at this https URL", "url": "http://arxiv.org/abs/2305.04095v3", "summary": "Federated Learning (FL) is a widely adopted privacy-preserving machine\nlearning approach where private data remains local, enabling secure\ncomputations and the exchange of local model gradients between local clients\nand third-party parameter servers. However, recent findings reveal that privacy\nmay be compromised and sensitive information potentially recovered from shared\ngradients. In this study, we offer detailed analysis and a novel perspective on\nunderstanding the gradient leakage problem. These theoretical works lead to a\nnew gradient leakage defense technique that secures arbitrary model\narchitectures using a private key-lock module. Only the locked gradient is\ntransmitted to the parameter server for global model aggregation. Our proposed\nlearning method is resistant to gradient leakage attacks, and the key-lock\nmodule is designed and trained to ensure that, without the private information\nof the key-lock module: a) reconstructing private training data from the shared\ngradient is infeasible; and b) the global model's inference performance is\nsignificantly compromised. We discuss the theoretical underpinnings of why\ngradients can leak private information and provide theoretical proof of our\nmethod's effectiveness. We conducted extensive empirical evaluations with many\nmodels on several popular benchmarks, demonstrating the robustness of our\nproposed approach in both maintaining model performance and defending against\ngradient leakage attacks.", "comment": "The source code can be found at https://github.com/Rand2AI/FedKL", "pdf_url": "http://arxiv.org/pdf/2305.04095v3", "cate": "cs.LG", "date": "2023-05-06", "updated": "2025-08-01"}
{"id": "2305.15611", "title": "Tackling Size Generalization of Graph Neural Networks on Biological Data from a Spectral Perspective", "authors": ["Gaotang Li", "Danai Koutra", "Yujun Yan"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      KDD 2025", "url": "http://arxiv.org/abs/2305.15611v5", "summary": "We address the key challenge of size-induced distribution shifts in graph\nneural networks (GNNs) and their impact on the generalization of GNNs to larger\ngraphs. Existing literature operates under diverse assumptions about\ndistribution shifts, resulting in varying conclusions about the\ngeneralizability of GNNs. In contrast to prior work, we adopt a data-driven\napproach to identify and characterize the types of size-induced distribution\nshifts and explore their impact on GNN performance from a spectral standpoint,\na perspective that has been largely underexplored. Leveraging the significant\nvariance in graph sizes in real biological datasets, we analyze biological\ngraphs and find that spectral differences, driven by subgraph patterns (e.g.,\naverage cycle length), strongly correlate with GNN performance on larger,\nunseen graphs. Based on these insights, we propose three model-agnostic\nstrategies to enhance GNNs' awareness of critical subgraph patterns,\nidentifying size-intensive attention as the most effective approach. Extensive\nexperiments with six GNN architectures and seven model-agnostic strategies\nacross five datasets show that our size-intensive attention strategy\nsignificantly improves graph classification on test graphs 2 to 10 times larger\nthan the training graphs, boosting F1 scores by up to 8% over strong baselines.", "comment": "KDD 2025", "pdf_url": "http://arxiv.org/pdf/2305.15611v5", "cate": "cs.LG", "date": "2023-05-24", "updated": "2025-08-01"}
{"id": "2401.01100", "title": "Sampling-enabled scalable manifold learning unveils discriminative cluster structure of high-dimensional data", "authors": ["Dehua Peng", "Zhipeng Gui", "Wenzhang Wei", "Fa Li", "Jie Gui", "Huayi Wu", "Jianya Gong"], "categories": ["cs.LG", "I.5.3"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      80 pages, 37 figures", "url": "http://arxiv.org/abs/2401.01100v4", "summary": "As a pivotal branch of machine learning, manifold learning uncovers the\nintrinsic low-dimensional structure within complex nonlinear manifolds in\nhigh-dimensional space for visualization, classification, clustering, and\ngaining key insights. Although existing techniques have achieved remarkable\nsuccesses, they suffer from extensive distortions of cluster structure, which\nhinders the understanding of underlying patterns. Scalability issues also limit\ntheir applicability for handling large-scale data. We hence propose a\nsampling-based Scalable manifold learning technique that enables Uniform and\nDiscriminative Embedding, namely SUDE, for large-scale and high-dimensional\ndata. It starts by seeking a set of landmarks to construct the low-dimensional\nskeleton of the entire data, and then incorporates the non-landmarks into the\nlearned space based on the constrained locally linear embedding (CLLE). We\nempirically validated the effectiveness of SUDE on synthetic datasets and\nreal-world benchmarks, and applied it to analyze single-cell data and detect\nanomalies in electrocardiogram (ECG) signals. SUDE exhibits distinct advantage\nin scalability with respect to data size and embedding dimension, and has\npromising performance in cluster separation, integrity, and global structure\npreservation. The experiments also demonstrate notable robustness in embedding\nquality as the sampling rate decreases.", "comment": "80 pages, 37 figures", "pdf_url": "http://arxiv.org/pdf/2401.01100v4", "cate": "cs.LG", "date": "2024-01-02", "updated": "2025-08-01"}
{"id": "2402.02364", "title": "Loss Landscape Degeneracy and Stagewise Development in Transformers", "authors": ["Jesse Hoogland", "George Wang", "Matthew Farrugia-Roberts", "Liam Carroll", "Susan Wei", "Daniel Murfet"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      To appear, TMLR. Material on essential dynamics from v1 of this preprint has been removed and developed in arXiv:2501.17745", "url": "http://arxiv.org/abs/2402.02364v3", "summary": "Deep learning involves navigating a high-dimensional loss landscape over the\nneural network parameter space. Over the course of training, complex\ncomputational structures form and re-form inside the neural network, leading to\nshifts in input/output behavior. It is a priority for the science of deep\nlearning to uncover principles governing the development of neural network\nstructure and behavior. Drawing on the framework of singular learning theory,\nwe propose that model development is deeply linked to degeneracy in the local\ngeometry of the loss landscape. We investigate this link by monitoring loss\nlandscape degeneracy throughout training, as quantified by the local learning\ncoefficient, for a transformer language model and an in-context linear\nregression transformer. We show that training can be divided into distinct\nperiods of change in loss landscape degeneracy, and that these changes in\ndegeneracy coincide with significant changes in the internal computational\nstructure and the input/output behavior of the transformers. This finding\nprovides suggestive evidence that degeneracy and development are linked in\ntransformers, underscoring the potential of a degeneracy-based perspective for\nunderstanding modern deep learning.", "comment": "To appear, TMLR. Material on essential dynamics from v1 of this\n  preprint has been removed and developed in arXiv:2501.17745", "pdf_url": "http://arxiv.org/pdf/2402.02364v3", "cate": "cs.LG", "date": "2024-02-04", "updated": "2025-08-01"}
{"id": "2403.19522", "title": "Model Stock: All we need is just a few fine-tuned models", "authors": ["Dong-Hwan Jang", "Sangdoo Yun", "Dongyoon Han"], "categories": ["cs.LG", "cs.CV"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      ECCV 2024 oral presenetation; Code at this https URL", "url": "http://arxiv.org/abs/2403.19522v2", "summary": "This paper introduces an efficient fine-tuning method for large pre-trained\nmodels, offering strong in-distribution (ID) and out-of-distribution (OOD)\nperformance. Breaking away from traditional practices that need a multitude of\nfine-tuned models for averaging, our approach employs significantly fewer\nmodels to achieve final weights yet yield superior accuracy. Drawing from key\ninsights in the weight space of fine-tuned weights, we uncover a strong link\nbetween the performance and proximity to the center of weight space. Based on\nthis, we introduce a method that approximates a center-close weight using only\ntwo fine-tuned models, applicable during or after training. Our innovative\nlayer-wise weight averaging technique surpasses state-of-the-art model methods\nsuch as Model Soup, utilizing only two fine-tuned models. This strategy can be\naptly coined Model Stock, highlighting its reliance on selecting a minimal\nnumber of models to draw a more optimized-averaged model. We demonstrate the\nefficacy of Model Stock with fine-tuned models based upon pre-trained CLIP\narchitectures, achieving remarkable performance on both ID and OOD tasks on the\nstandard benchmarks, all while barely bringing extra computational demands. Our\ncode and pre-trained models are available at\nhttps://github.com/naver-ai/model-stock.", "comment": "ECCV 2024 oral presenetation; Code at\n  https://github.com/naver-ai/model-stock", "pdf_url": "http://arxiv.org/pdf/2403.19522v2", "cate": "cs.LG", "date": "2024-03-28", "updated": "2025-08-01"}
{"id": "2407.02811", "title": "SPLITZ: Certifiable Robustness via Split Lipschitz Randomized Smoothing", "authors": ["Meiyu Zhong", "Ravi Tandon"], "categories": ["cs.LG", "cs.IT", "math.IT"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      IEEE Transactions on Information Forensics and Security, accepted", "url": "http://arxiv.org/abs/2407.02811v3", "summary": "Certifiable robustness gives the guarantee that small perturbations around an\ninput to a classifier will not change the prediction. There are two approaches\nto provide certifiable robustness to adversarial examples: a) explicitly\ntraining classifiers with small Lipschitz constants, and b) Randomized\nsmoothing, which adds random noise to the input to create a smooth classifier.\nWe propose SPLITZ, a practical and novel approach which leverages the\nsynergistic benefits of both the above ideas into a single framework. Our main\nidea is to split a classifier into two halves, constrain the Lipschitz constant\nof the first half, and smooth the second half via randomization. Motivation for\nSPLITZ comes from the observation that many standard deep networks exhibit\nheterogeneity in Lipschitz constants across layers. SPLITZ can exploit this\nheterogeneity while inheriting the scalability of randomized smoothing. We\npresent a principled approach to train SPLITZ and provide theoretical analysis\nto derive certified robustness guarantees during inference. We present a\ncomprehensive comparison of robustness-accuracy trade-offs and show that SPLITZ\nconsistently improves on existing state-of-the-art approaches in the MNIST,\nCIFAR-10 and ImageNet datasets. For instance, with $\\ell_2$ norm perturbation\nbudget of $\\epsilon=1$, SPLITZ achieves $43.2\\%$ top-1 test accuracy on\nCIFAR-10 dataset compared to state-of-art top-1 test accuracy $39.8\\%$.", "comment": "IEEE Transactions on Information Forensics and Security, accepted", "pdf_url": "http://arxiv.org/pdf/2407.02811v3", "cate": "cs.LG", "date": "2024-07-03", "updated": "2025-07-31"}
{"id": "2407.02827", "title": "Convergence of Implicit Gradient Descent for Training Two-Layer Physics-Informed Neural Networks", "authors": ["Xianliang Xu", "Ting Du", "Wang Kong", "Bin Shan", "Ye Li", "Zhongyi Huang"], "categories": ["cs.LG", "math.OC"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2407.02827v3", "summary": "The optimization algorithms are crucial in training physics-informed neural\nnetworks (PINNs), as unsuitable methods may lead to poor solutions. Compared to\nthe common gradient descent (GD) algorithm, implicit gradient descent (IGD)\noutperforms it in handling certain multi-scale problems. In this paper, we\nprovide convergence analysis for the IGD in training over-parameterized\ntwo-layer PINNs. We first derive the training dynamics of IGD in training\ntwo-layer PINNs. Then, over-parameterization allows us to prove that the\nrandomly initialized IGD converges to a globally optimal solution at a linear\nconvergence rate. Moreover, due to the distinct training dynamics of IGD\ncompared to GD, the learning rate can be selected independently of the sample\nsize and the least eigenvalue of the Gram matrix. Additionally, the novel\napproach used in our convergence analysis imposes a milder requirement on the\nnetwork width. Finally, empirical results validate our theoretical findings.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2407.02827v3", "cate": "cs.LG", "date": "2024-07-03", "updated": "2025-08-01"}
{"id": "2409.18749", "title": "TensorSocket: Shared Data Loading for Deep Learning Training", "authors": ["Ties Robroek", "Neil Kim Nielsen", "Pınar Tözün"], "categories": ["cs.LG", "cs.DC"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2409.18749v4", "summary": "Training deep learning models is a repetitive and resource-intensive process.\nData scientists often train several models before landing on a set of\nparameters (e.g., hyper-parameter tuning) and model architecture (e.g., neural\narchitecture search), among other things that yield the highest accuracy. The\ncomputational efficiency of these training tasks depends highly on how well the\ntraining data is supplied to the training process. The repetitive nature of\nthese tasks results in the same data processing pipelines running over and\nover, exacerbating the need for and costs of computational resources. In this\npaper, we present TensorSocket to reduce the computational needs of deep\nlearning training by enabling simultaneous training processes to share the same\ndata loader. TensorSocket mitigates CPU-side bottlenecks in cases where the\ncollocated training workloads have high throughput on GPU, but are held back by\nlower data-loading throughput on CPU. TensorSocket achieves this by reducing\nredundant computations and data duplication across collocated training\nprocesses and leveraging modern GPU-GPU interconnects. While doing so,\nTensorSocket is able to train and balance differently-sized models and serve\nmultiple batch sizes simultaneously and is hardware- and pipeline-agnostic in\nnature. Our evaluation shows that TensorSocket enables scenarios that are\ninfeasible without data sharing, increases training throughput by up to 100%,\nand when utilizing cloud instances, achieves cost savings of 50% by reducing\nthe hardware resource needs on the CPU side. Furthermore, TensorSocket\noutperforms the state-of-the-art solutions for shared data loading such as\nCoorDL and Joader; it is easier to deploy and maintain and either achieves\nhigher or matches their throughput while requiring fewer CPU resources.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2409.18749v4", "cate": "cs.LG", "date": "2024-09-27", "updated": "2025-08-01"}
{"id": "2410.01312", "title": "Sampling from Energy-based Policies using Diffusion", "authors": ["Vineet Jain", "Tara Akhound-Sadegh", "Siamak Ravanbakhsh"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2410.01312v2", "summary": "Energy-based policies offer a flexible framework for modeling complex,\nmultimodal behaviors in reinforcement learning (RL). In maximum entropy RL, the\noptimal policy is a Boltzmann distribution derived from the soft Q-function,\nbut direct sampling from this distribution in continuous action spaces is\ncomputationally intractable. As a result, existing methods typically use\nsimpler parametric distributions, like Gaussians, for policy representation --\nlimiting their ability to capture the full complexity of multimodal action\ndistributions. In this paper, we introduce a diffusion-based approach for\nsampling from energy-based policies, where the negative Q-function defines the\nenergy function. Based on this approach, we propose an actor-critic method\ncalled Diffusion Q-Sampling (DQS) that enables more expressive policy\nrepresentations, allowing stable learning in diverse environments. We show that\nour approach enhances sample efficiency in continuous control tasks and\ncaptures multimodal behaviors, addressing key limitations of existing methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2410.01312v2", "cate": "cs.LG", "date": "2024-10-02", "updated": "2025-07-31"}
{"id": "2410.21072", "title": "Federated Time Series Generation on Feature and Temporally Misaligned Data", "authors": ["Zhi Wen Soi", "Chenrui Fan", "Aditya Shankar", "Abele Mălan", "Lydia Y. Chen"], "categories": ["cs.LG", "cs.DC"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2410.21072v2", "summary": "Distributed time series data presents a challenge for federated learning, as\nclients often possess different feature sets and have misaligned time steps.\nExisting federated time series models are limited by the assumption of perfect\ntemporal or feature alignment across clients. In this paper, we propose FedTDD,\na novel federated time series diffusion model that jointly learns a synthesizer\nacross clients. At the core of FedTDD is a novel data distillation and\naggregation framework that reconciles the differences between clients by\nimputing the misaligned timesteps and features. In contrast to traditional\nfederated learning, FedTDD learns the correlation across clients' time series\nthrough the exchange of local synthetic outputs instead of model parameters. A\ncoordinator iteratively improves a global distiller network by leveraging\nshared knowledge from clients through the exchange of synthetic data. As the\ndistiller becomes more refined over time, it subsequently enhances the quality\nof the clients' local feature estimates, allowing each client to then improve\nits local imputations for missing data using the latest, more accurate\ndistiller. Experimental results on five datasets demonstrate FedTDD's\neffectiveness compared to centralized training, and the effectiveness of\nsharing synthetic outputs to transfer knowledge of local time series. Notably,\nFedTDD achieves 79.4% and 62.8% improvement over local training in Context-FID\nand Correlational scores.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2410.21072v2", "cate": "cs.LG", "date": "2024-10-28", "updated": "2025-08-01"}
{"id": "2411.15173", "title": "Un-mixing Test-time Adaptation under Heterogeneous Data Streams", "authors": ["Zixian Su", "Jingwei Guo", "Xi Yang", "Qiufeng Wang", "Kaizhu Huang"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2411.15173v2", "summary": "Deploying deep models in real-world scenarios remains challenging due to\nsignificant performance drops under distribution shifts between training and\ndeployment environments. Test-Time Adaptation (TTA) has recently emerged as a\npromising solution, enabling on-the-fly model adaptation without access to\nsource data. However, its effectiveness degrades significantly in the presence\nof complex, mixed distribution shifts - common in practical settings - where\nmultiple latent domains coexist. Adapting under such intrinsic heterogeneity,\nespecially in unlabeled and online conditions, remains an open and\nunderexplored challenge. In this paper, we study TTA under mixed distribution\nshifts and move beyond conventional homogeneous adaptation paradigms. By\nrevisiting TTA from a frequency-domain perspective, we observe that\ndistribution heterogeneity often manifests in Fourier space - for instance,\nhigh-frequency components tend to carry domain-specific variations. This\nmotivates us to perform domain-aware separation using high-frequency texture\ncues, making diverse shift patterns more tractable. To this end, we propose\nFreDA, a novel Frequency-based Decentralized Adaptation framework that\ndecomposes globally heterogeneous data into locally homogeneous components in\nthe frequency domain. It further employs decentralized learning and\naugmentation strategies to robustly adapt under complex, evolving shifts.\nExtensive experiments across various environments (corrupted, natural, and\nmedical) demonstrate the superiority of our proposed framework over the\nstate-of-the-arts.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2411.15173v2", "cate": "cs.LG", "date": "2024-11-16", "updated": "2025-08-01"}
{"id": "2412.12201", "title": "Embracing Large Language Models in Traffic Flow Forecasting", "authors": ["Yusheng Zhao", "Xiao Luo", "Haomin Wen", "Zhiping Xiao", "Wei Ju", "Ming Zhang"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted by ACL 2025", "url": "http://arxiv.org/abs/2412.12201v2", "summary": "Traffic flow forecasting aims to predict future traffic flows based on the\nhistorical traffic conditions and the road network. It is an important problem\nin intelligent transportation systems, with a plethora of methods been\nproposed. Existing efforts mainly focus on capturing and utilizing\nspatio-temporal dependencies to predict future traffic flows. Though promising,\nthey fall short in adapting to test-time environmental changes of traffic\nconditions. To tackle this challenge, we propose to introduce large language\nmodels (LLMs) to help traffic flow forecasting and design a novel method named\nLarge Language Model Enhanced Traffic Flow Predictor (LEAF). LEAF adopts two\nbranches, capturing different spatio-temporal relations using graph and\nhypergraph structures respectively. The two branches are first pre-trained\nindividually, and during test-time, they yield different predictions. Based on\nthese predictions, a large language model is used to select the most likely\nresult. Then, a ranking loss is applied as the learning objective to enhance\nthe prediction ability of the two branches. Extensive experiments on several\ndatasets demonstrate the effectiveness of the proposed LEAF.", "comment": "Accepted by ACL 2025", "pdf_url": "http://arxiv.org/pdf/2412.12201v2", "cate": "cs.LG", "date": "2024-12-15", "updated": "2025-08-01"}
{"id": "2502.17077", "title": "A comparative analysis of rank aggregation methods for the partial label ranking problem", "authors": ["Jiayi Wang", "Juan C. Alfaro", "Viktor Bengs"], "categories": ["cs.LG", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Full version of the paper accepted at ECAI 2025", "url": "http://arxiv.org/abs/2502.17077v3", "summary": "The label ranking problem is a supervised learning scenario in which the\nlearner predicts a total order of the class labels for a given input instance.\nRecently, research has increasingly focused on the partial label ranking\nproblem, a generalization of the label ranking problem that allows ties in the\npredicted orders. So far, most existing learning approaches for the partial\nlabel ranking problem rely on approximation algorithms for rank aggregation in\nthe final prediction step. This paper explores several alternative aggregation\nmethods for this critical step, including scoring-based and non-parametric\nprobabilistic-based rank aggregation approaches. To enhance their suitability\nfor the more general partial label ranking problem, the investigated methods\nare extended to increase the likelihood of producing ties. Experimental\nevaluations on standard benchmarks demonstrate that scoring-based variants\nconsistently outperform the current state-of-the-art method in handling\nincomplete information. In contrast, non-parametric probabilistic-based\nvariants fail to achieve competitive performance.", "comment": "Full version of the paper accepted at ECAI 2025", "pdf_url": "http://arxiv.org/pdf/2502.17077v3", "cate": "cs.LG", "date": "2025-02-24", "updated": "2025-08-01"}
{"id": "2502.19651", "title": "Unlocking Multi-Modal Potentials for Link Prediction on Dynamic Text-Attributed Graphs", "authors": ["Yuanyuan Xu", "Wenjie Zhang", "Ying Zhang", "Xuemin Lin", "Xiwei Xu"], "categories": ["cs.LG", "cs.CL"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2502.19651v2", "summary": "Dynamic Text-Attributed Graphs (DyTAGs) are a novel graph paradigm that\ncaptures evolving temporal events (edges) alongside rich textual attributes.\nExisting studies can be broadly categorized into TGNN-driven and LLM-driven\napproaches, both of which encode textual attributes and temporal structures for\nDyTAG representation. We observe that DyTAGs inherently comprise three distinct\nmodalities: temporal, textual, and structural, often exhibiting completely\ndisjoint distributions. However, the first two modalities are largely\noverlooked by existing studies, leading to suboptimal performance. To address\nthis, we propose MoMent, a multi-modal model that explicitly models,\nintegrates, and aligns each modality to learn node representations for link\nprediction. Given the disjoint nature of the original modality distributions,\nwe first construct modality-specific features and encode them using individual\nencoders to capture correlations across temporal patterns, semantic context,\nand local structures. Each encoder generates modality-specific tokens, which\nare then fused into comprehensive node representations with a theoretical\nguarantee. To avoid disjoint subspaces of these heterogeneous modalities, we\npropose a dual-domain alignment loss that first aligns their distributions\nglobally and then fine-tunes coherence at the instance level. This enhances\ncoherent representations from temporal, textual, and structural views.\nExtensive experiments across seven datasets show that MoMent achieves up to\n17.28% accuracy improvement and up to 31x speed-up against eight baselines.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2502.19651v2", "cate": "cs.LG", "date": "2025-02-27", "updated": "2025-08-01"}
{"id": "2503.06101", "title": "ULTHO: Ultra-Lightweight yet Efficient Hyperparameter Optimization in Deep Reinforcement Learning", "authors": ["Mingqi Yuan", "Bo Li", "Xin Jin", "Wenjun Zeng"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      24 pages, 25 figures", "url": "http://arxiv.org/abs/2503.06101v2", "summary": "Hyperparameter optimization (HPO) is a billion-dollar problem in machine\nlearning, which significantly impacts the training efficiency and model\nperformance. However, achieving efficient and robust HPO in deep reinforcement\nlearning (RL) is consistently challenging due to its high non-stationarity and\ncomputational cost. To tackle this problem, existing approaches attempt to\nadapt common HPO techniques (e.g., population-based training or Bayesian\noptimization) to the RL scenario. However, they remain sample-inefficient and\ncomputationally expensive, which cannot facilitate a wide range of\napplications. In this paper, we propose ULTHO, an ultra-lightweight yet\npowerful framework for fast HPO in deep RL within single runs. Specifically, we\nformulate the HPO process as a multi-armed bandit with clustered arms (MABC)\nand link it directly to long-term return optimization. ULTHO also provides a\nquantified and statistical perspective to filter the HPs efficiently. We test\nULTHO on benchmarks including ALE, Procgen, MiniGrid, and PyBullet. Extensive\nexperiments demonstrate that the ULTHO can achieve superior performance with a\nsimple architecture, contributing to the development of advanced and automated\nRL systems.", "comment": "24 pages, 25 figures", "pdf_url": "http://arxiv.org/pdf/2503.06101v2", "cate": "cs.LG", "date": "2025-03-08", "updated": "2025-08-01"}
{"id": "2503.10845", "title": "Panopticon: Advancing Any-Sensor Foundation Models for Earth Observation", "authors": ["Leonard Waldmann", "Ando Shah", "Yi Wang", "Nils Lehmann", "Adam J. Stewart", "Zhitong Xiong", "Xiao Xiang Zhu", "Stefan Bauer", "John Chuang"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      First two authors contributed equally. Code is available at: this https URL . Accepted to CVPR 2025", "url": "http://arxiv.org/abs/2503.10845v2", "summary": "Earth observation (EO) data features diverse sensing platforms with varying\nspectral bands, spatial resolutions, and sensing modalities. While most prior\nwork has constrained inputs to fixed sensors, a new class of any-sensor\nfoundation models able to process arbitrary sensors has recently emerged.\nContributing to this line of work, we propose Panopticon, an any-sensor\nfoundation model built on the DINOv2 framework. We extend DINOv2 by (1)\ntreating images of the same geolocation across sensors as natural\naugmentations, (2) subsampling channels to diversify spectral input, and (3)\nadding a cross attention over channels as a flexible patch embedding mechanism.\nBy encoding the wavelength and modes of optical and synthetic aperture radar\nsensors, respectively, Panopticon can effectively process any combination of\narbitrary channels. In extensive evaluations, we achieve state-of-the-art\nperformance on GEO-Bench, especially on the widely-used Sentinel-1 and\nSentinel-2 sensors, while out-competing other any-sensor models, as well as\ndomain adapted fixed-sensor models on unique sensor configurations. Panopticon\nenables immediate generalization to both existing and future satellite\nplatforms, advancing sensor-agnostic EO.", "comment": "First two authors contributed equally. Code is available at:\n  https://github.com/Panopticon-FM/panopticon. Accepted to CVPR 2025", "pdf_url": "http://arxiv.org/pdf/2503.10845v2", "cate": "cs.LG", "date": "2025-03-13", "updated": "2025-08-01"}
{"id": "2503.13544", "title": "Decision by Supervised Learning with Deep Ensembles: A Practical Framework for Robust Portfolio Optimization", "authors": ["Juhyeong Kim", "Sungyoon Choi", "Youngbin Lee", "Yejin Kim", "Yongmin Choi", "Yongjae Lee"], "categories": ["cs.LG", "q-fin.CP", "q-fin.PM"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      8 pages, 3 figures", "url": "http://arxiv.org/abs/2503.13544v5", "summary": "We propose Decision by Supervised Learning (DSL), a practical framework for\nrobust portfolio optimization. DSL reframes portfolio construction as a\nsupervised learning problem: models are trained to predict optimal portfolio\nweights, using cross-entropy loss and portfolios constructed by maximizing the\nSharpe or Sortino ratio. To further enhance stability and reliability, DSL\nemploys Deep Ensemble methods, substantially reducing variance in portfolio\nallocations. Through comprehensive backtesting across diverse market universes\nand neural architectures, shows superior performance compared to both\ntraditional strategies and leading machine learning-based methods, including\nPrediction-Focused Learning and End-to-End Learning. We show that increasing\nthe ensemble size leads to higher median returns and more stable risk-adjusted\nperformance. The code is available at https://github.com/DSLwDE/DSLwDE.", "comment": "8 pages, 3 figures", "pdf_url": "http://arxiv.org/pdf/2503.13544v5", "cate": "cs.LG", "date": "2025-03-16", "updated": "2025-08-01"}
{"id": "2504.04202", "title": "Directional Sign Loss: A Topology-Preserving Loss Function that Approximates the Sign of Finite Differences", "authors": ["Harvey Dam", "Tripti Agarwal", "Ganesh Gopalakrishnan"], "categories": ["cs.LG", "I.2.6"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.04202v3", "summary": "Preserving topological features in learned latent spaces is a fundamental\nchallenge in representation learning, particularly for topology-sensitive data.\nThis paper introduces directional sign loss (DSL), an efficient, differentiable\nloss function that approximates the number of mismatches in the signs of finite\ndifferences between corresponding elements of two arrays. By penalizing\ndiscrepancies in critical points between input and reconstructed data, DSL\nencourages autoencoders and other learnable compressors to retain the\ntopological features of the original data. We present the formulation and\ncomplexity analysis of DSL, comparing it to other non-differentiable\ntopological measures. Experiments on multidimensional array data show that\ncombining DSL with traditional loss functions preserves topological features\nmore effectively than traditional losses alone. DSL serves as a differentiable,\nefficient proxy for common topology-based metrics, enabling topological feature\npreservation on previously impractical problem sizes and in a wider range of\ngradient-based optimization frameworks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.04202v3", "cate": "cs.LG", "date": "2025-04-05", "updated": "2025-07-31"}
{"id": "2504.09664", "title": "Adapting to the Unknown: Robust Meta-Learning for Zero-Shot Financial Time Series Forecasting", "authors": ["Anxian Liu", "Junying Ma", "Guang Zhang"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.09664v2", "summary": "Financial time series forecasting in zero-shot settings is critical for\ninvestment decisions, especially during abrupt market regime shifts or in\nemerging markets with limited historical data. While Model-Agnostic\nMeta-Learning (MAML) approaches show promise, existing meta-task construction\nstrategies often yield suboptimal performance for highly turbulent financial\nseries. To address this, we propose a novel task-construction method that\nleverages learned embeddings for both meta task and also downstream\npredictions, enabling effective zero-shot meta-learning. Specifically, we use\nGaussian Mixture Models (GMMs) to softly cluster embeddings, constructing two\ncomplementary meta-task types: intra-cluster tasks and inter-cluster tasks. By\nassigning embeddings to multiple latent regimes probabilistically, GMMs enable\nricher, more diverse meta-learning. This dual approach ensures the model can\nquickly adapt to local patterns while simultaneously capturing invariant\ncross-series features. Furthermore, we enhance inter-cluster generalization\nthrough hard task mining, which identifies robust patterns across divergent\nmarket regimes. Our method was validated using real-world financial data from\nhigh-volatility periods and multiple international markets (including emerging\nmarkets). The results demonstrate significant out-performance over existing\napproaches and stronger generalization in zero-shot scenarios.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.09664v2", "cate": "cs.LG", "date": "2025-04-13", "updated": "2025-08-01"}
{"id": "2504.20908", "title": "MOSIC: Model-Agnostic Optimal Subgroup Identification with Multi-Constraint for Improved Reliability", "authors": ["Wenxin Chen", "Weishen Pan", "Kyra Gan", "Fei Wang"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.20908v2", "summary": "Current subgroup identification methods typically follow a two-step approach:\nfirst estimate conditional average treatment effects and then apply\nthresholding or rule-based procedures to define subgroups. While intuitive,\nthis decoupled approach fails to incorporate key constraints essential for\nreal-world clinical decision-making, such as subgroup size and propensity\noverlap. These constraints operate on fundamentally different axes than CATE\nestimation and are not naturally accommodated within existing frameworks,\nthereby limiting the practical applicability of these methods. We propose a\nunified optimization framework that directly solves the primal constrained\noptimization problem to identify optimal subgroups. Our key innovation is a\nreformulation of the constrained primal problem as an unconstrained\ndifferentiable min-max objective, solved via a gradient descent-ascent\nalgorithm. We theoretically establish that our solution converges to a feasible\nand locally optimal solution. Unlike threshold-based CATE methods that apply\nconstraints as post-hoc filters, our approach enforces them directly during\noptimization. The framework is model-agnostic, compatible with a wide range of\nCATE estimators, and extensible to additional constraints like cost limits or\nfairness criteria. Extensive experiments on synthetic and real-world datasets\ndemonstrate its effectiveness in identifying high-benefit subgroups while\nmaintaining better satisfaction of constraints.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.20908v2", "cate": "cs.LG", "date": "2025-04-29", "updated": "2025-08-01"}
{"id": "2505.02634", "title": "Transfer learning-enhanced deep reinforcement learning for aerodynamic airfoil optimisation subject to structural constraints", "authors": ["David Ramos", "Lucas Lacasa", "Eusebio Valero", "Gonzalo Rubio"], "categories": ["cs.LG", "physics.comp-ph"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted in Physics of Fluids 20 pages, 7 figures", "url": "http://arxiv.org/abs/2505.02634v2", "summary": "The main objective of this paper is to introduce a transfer learning-enhanced\ndeep reinforcement learning (DRL) methodology that is able to optimise the\ngeometry of any airfoil based on concomitant aerodynamic and structural\nintegrity criteria. To showcase the method, we aim to maximise the lift-to-drag\nratio $C_L/C_D$ while preserving the structural integrity of the airfoil -- as\nmodelled by its maximum thickness -- and train the DRL agent using a list of\ndifferent transfer learning (TL) strategies. The performance of the DRL agent\nis compared with Particle Swarm Optimisation (PSO), a traditional gradient-free\noptimisation method. Results indicate that DRL agents are able to perform\npurely aerodynamic and hybrid aerodynamic/structural shape optimisation, that\nthe DRL approach outperforms PSO in terms of computational efficiency and\naerodynamic improvement, and that the TL-enhanced DRL agent achieves\nperformance comparable to the DRL one, while further saving substantial\ncomputational resources.", "comment": "Accepted in Physics of Fluids 20 pages, 7 figures", "pdf_url": "http://arxiv.org/pdf/2505.02634v2", "cate": "cs.LG", "date": "2025-05-05", "updated": "2025-08-01"}
{"id": "2505.06351", "title": "Latent Diffeomorphic Dynamic Mode Decomposition", "authors": ["Willem Diepeveen", "Jon Schwenk", "Andrea Bertozzi"], "categories": ["cs.LG", "math.DS"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.06351v2", "summary": "We present Latent Diffeomorphic Dynamic Mode Decomposition (LDDMD), a new\ndata reduction approach for the analysis of non-linear systems that combines\nthe interpretability of Dynamic Mode Decomposition (DMD) with the predictive\npower of Recurrent Neural Networks (RNNs). Notably, LDDMD maintains simplicity,\nwhich enhances interpretability, while effectively modeling and learning\ncomplex non-linear systems with memory, enabling accurate predictions. This is\nexemplified by its successful application in streamflow prediction.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.06351v2", "cate": "cs.LG", "date": "2025-05-09", "updated": "2025-08-01"}
{"id": "2505.08320", "title": "Adaptive Branch Specialization in Spectral-Spatial Graph Neural Networks for Certified Robustness", "authors": ["Yoonhyuk Choi", "Jiho Choi", "Chong-Kwon Kim"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.08320v3", "summary": "Recent Graph Neural Networks (GNNs) combine spectral-spatial architectures\nfor enhanced representation learning. However, limited attention has been paid\nto certified robustness, particularly regarding training strategies and\nunderlying rationale. In this paper, we explicitly specialize each branch: the\nspectral network is trained to withstand l0 edge flips and capture homophilic\nstructures, while the spatial part is designed to resist linf feature\nperturbations and heterophilic patterns. A context-aware gating network\nadaptively fuses the two representations, dynamically routing each node's\nprediction to the more reliable branch. This specialized adversarial training\nscheme uses branch-specific inner maximization (structure vs feature attacks)\nand a unified alignment objective. We provide theoretical guarantees: (i)\nexpressivity of the gating mechanism beyond 1-WL, (ii) spectral-spatial\nfrequency bias, and (iii) certified robustness with trade-off. Empirically,\nSpecSphere attains state-of-the-art node classification accuracy and offers\ntighter certified robustness on real-world benchmarks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.08320v3", "cate": "cs.LG", "date": "2025-05-13", "updated": "2025-08-01"}
{"id": "2505.09503", "title": "Towards Fair In-Context Learning with Tabular Foundation Models", "authors": ["Patrik Kenfack", "Samira Ebrahimi Kahou", "Ulrich Aïvodji"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      30 pages, 12 figures, 5 tables", "url": "http://arxiv.org/abs/2505.09503v3", "summary": "Transformer-based tabular foundation models have recently demonstrated\npromising in-context learning (ICL) performance on structured data, emerging as\ncompetitive alternatives to gradient-boosted trees. However, the fairness\nimplications of this new paradigm remain largely unexplored. We present the\nfirst investigation of fairness in tabular ICL, evaluating three recently\nproposed foundation models -- TabPFNv2, TabICL, and TabDPT -- on multiple\nbenchmark datasets. To mitigate biases, we explore three pre-processing\nfairness-enhancing methods: correlation removal (decorrelating input features\nfrom the sensitive attribute), group-balanced sample selection (ensuring equal\nrepresentation of protected groups in context examples), and uncertainty-based\nsample selection (prioritizing context examples with high sensitive-attribute\nprediction uncertainty). Our experiments show that the uncertainty-based\nstrategy consistently improves group fairness metrics (e.g., demographic\nparity, equalized odds, and equal opportunity) with minimal impact on\npredictive accuracy. We release our code to facilitate reproducibility\n(https://github.com/patrikken/Fair-TabICL)", "comment": "30 pages, 12 figures, 5 tables", "pdf_url": "http://arxiv.org/pdf/2505.09503v3", "cate": "cs.LG", "date": "2025-05-14", "updated": "2025-08-01"}
{"id": "2505.11250", "title": "Rethinking Irregular Time Series Forecasting: A Simple yet Effective Baseline", "authors": ["Xvyuan Liu", "Xiangfei Qiu", "Xingjian Wu", "Zhengyu Li", "Chenjuan Guo", "Jilin Hu", "Bin Yang"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.11250v2", "summary": "The forecasting of irregular multivariate time series (IMTS) is a critical\ntask in domains like healthcare and climate science. However, this task faces\ntwo significant hurdles: 1) the inherent non-uniformity and missing data in\nIMTS complicate the modeling of temporal dynamics, and 2) existing methods\noften rely on computationally expensive architectures. To address these dual\nchallenges, we introduce APN, a general and efficient forecasting framework. At\nthe core of APN is a novel Time-Aware Patch Aggregation (TAPA) module that\nintroduces an aggregation-based paradigm for adaptive patching, moving beyond\nthe limitations of fixed-span segmentation and interpolation-based methods.\nTAPA first learns dynamic temporal boundaries to define data-driven segments.\nCrucially, instead of resampling or interpolating, it directly computes patch\nrepresentations via a time-aware weighted aggregation of all raw observations,\nwhere weights are determined by each observation's temporal relevance to the\nsegment. This approach provides two key advantages: it preserves data fidelity\nby avoiding the introduction of artificial data points and ensures complete\ninformation coverage by design.The resulting regularized and information-rich\npatch representations enable the use of a lightweight query module for\nhistorical context aggregation and a simple MLP for final prediction. Extensive\nexperiments on multiple real-world datasets demonstrate that APN establishes a\nnew state-of-the-art, significantly outperforming existing methods in both\nprediction accuracy and computational efficiency.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.11250v2", "cate": "cs.LG", "date": "2025-05-16", "updated": "2025-08-01"}
{"id": "2505.17340", "title": "Conformal Predictive Distributions for Order Fulfillment Time Forecasting", "authors": ["Tinghan Ye", "Amira Hijazi", "Pascal Van Hentenryck"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.17340v2", "summary": "Accurate estimation of order fulfillment time is critical for e-commerce\nlogistics, yet traditional rule-based approaches often fail to capture the\ninherent uncertainties in delivery operations. This paper introduces a novel\nframework for distributional forecasting of order fulfillment time, leveraging\nConformal Predictive Systems and Cross Venn-Abers Predictors -- model-agnostic\ntechniques that provide rigorous coverage or validity guarantees. The proposed\nmachine learning methods integrate granular spatiotemporal features, capturing\nfulfillment location and carrier performance dynamics to enhance predictive\naccuracy. Additionally, a cost-sensitive decision rule is developed to convert\nprobabilistic forecasts into reliable point predictions. Experimental\nevaluation on a large-scale industrial dataset demonstrates that the proposed\nmethods generate competitive distributional forecasts, while machine\nlearning-based point predictions significantly outperform the existing\nrule-based system -- achieving up to 14% higher prediction accuracy and up to\n75% improvement in identifying late deliveries.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.17340v2", "cate": "cs.LG", "date": "2025-05-22", "updated": "2025-08-01"}
{"id": "2505.23246", "title": "How to Evaluate Participant Contributions in Decentralized Federated Learning", "authors": ["Honoka Anada", "Tatsuya Kaneko", "Shinya Takamaeda-Yamazaki"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.23246v2", "summary": "Federated learning (FL) enables multiple clients to collaboratively train\nmachine learning models without sharing local data. In particular,\ndecentralized FL (DFL), where clients exchange models without a central server,\nhas gained attention for mitigating communication bottlenecks. Evaluating\nparticipant contributions is crucial in DFL to incentivize active participation\nand enhance transparency. However, existing contribution evaluation methods for\nFL assume centralized settings and cannot be applied directly to DFL due to two\nchallenges: the inaccessibility of each client to non-neighboring clients'\nmodels, and the necessity to trace how contributions propagate in conjunction\nwith peer-to-peer model exchanges over time. To address these challenges, we\npropose TRIP-Shapley, a novel contribution evaluation method for DFL.\nTRIP-Shapley formulates the clients' overall contributions by tracing the\npropagation of the round-wise local contributions. In this way, TRIP-Shapley\naccurately reflects the delayed and gradual influence propagation, as well as\nallowing a lightweight coordinator node to estimate the overall contributions\nwithout collecting models, but based solely on locally observable contributions\nreported by each client. Experiments demonstrate that TRIP-Shapley is\nsufficiently close to the ground-truth Shapley value, is scalable to\nlarge-scale scenarios, and remains robust in the presence of dishonest clients.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.23246v2", "cate": "cs.LG", "date": "2025-05-29", "updated": "2025-08-01"}
{"id": "2506.07288", "title": "EVINET: Towards Open-World Graph Learning via Evidential Reasoning Network", "authors": ["Weijie Guan", "Haohui Wang", "Jian Kang", "Lihui Liu", "Dawei Zhou"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      KDD 2025", "url": "http://arxiv.org/abs/2506.07288v3", "summary": "Graph learning has been crucial to many real-world tasks, but they are often\nstudied with a closed-world assumption, with all possible labels of data known\na priori. To enable effective graph learning in an open and noisy environment,\nit is critical to inform the model users when the model makes a wrong\nprediction to in-distribution data of a known class, i.e., misclassification\ndetection or when the model encounters out-of-distribution from novel classes,\ni.e., out-of-distribution detection. This paper introduces Evidential Reasoning\nNetwork (EVINET), a framework that addresses these two challenges by\nintegrating Beta embedding within a subjective logic framework. EVINET includes\ntwo key modules: Dissonance Reasoning for misclassification detection and\nVacuity Reasoning for out-of-distribution detection. Extensive experiments\ndemonstrate that EVINET outperforms state-of-the-art methods across multiple\nmetrics in the tasks of in-distribution classification, misclassification\ndetection, and out-of-distribution detection. EVINET demonstrates the necessity\nof uncertainty estimation and logical reasoning for misclassification detection\nand out-of-distribution detection and paves the way for open-world graph\nlearning. Our code and data are available at https://github.com/SSSKJ/EviNET.", "comment": "KDD 2025", "pdf_url": "http://arxiv.org/pdf/2506.07288v3", "cate": "cs.LG", "date": "2025-06-08", "updated": "2025-08-01"}
{"id": "2506.20644", "title": "Efficient Federated Learning with Encrypted Data Sharing for Data-Heterogeneous Edge Devices", "authors": ["Hangyu Li", "Hongyue Wu", "Guodong Fan", "Zhen Zhang", "Shizhan Chen", "Zhiyong Feng"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted by ICWS 2025", "url": "http://arxiv.org/abs/2506.20644v2", "summary": "As privacy protection gains increasing importance, more models are being\ntrained on edge devices and subsequently merged into the central server through\nFederated Learning (FL). However, current research overlooks the impact of\nnetwork topology, physical distance, and data heterogeneity on edge devices,\nleading to issues such as increased latency and degraded model performance. To\naddress these issues, we propose a new federated learning scheme on edge\ndevices that called Federated Learning with Encrypted Data Sharing(FedEDS).\nFedEDS uses the client model and the model's stochastic layer to train the data\nencryptor. The data encryptor generates encrypted data and shares it with other\nclients. The client uses the corresponding client's stochastic layer and\nencrypted data to train and adjust the local model. FedEDS uses the client's\nlocal private data and encrypted shared data from other clients to train the\nmodel. This approach accelerates the convergence speed of federated learning\ntraining and mitigates the negative impact of data heterogeneity, making it\nsuitable for application services deployed on edge devices requiring rapid\nconvergence. Experiments results show the efficacy of FedEDS in promoting model\nperformance.", "comment": "Accepted by ICWS 2025", "pdf_url": "http://arxiv.org/pdf/2506.20644v2", "cate": "cs.LG", "date": "2025-06-25", "updated": "2025-08-01"}
{"id": "2507.02659", "title": "OmniDraft: A Cross-vocabulary, Online Adaptive Drafter for On-device Speculative Decoding", "authors": ["Ramchalam Kinattinkara Ramakrishnan", "Zhaocong Yuan", "Shaojie Zhuo", "Chen Feng", "Yicheng Lin", "Chenzheng Su", "Xiaopeng Zhang"], "categories": ["cs.LG", "cs.CL"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02659v2", "summary": "Speculative decoding generally dictates having a small, efficient draft model\nthat is either pretrained or distilled offline to a particular target model\nseries, for instance, Llama or Qwen models. However, within online deployment\nsettings, there are two major challenges: 1) usage of a target model that is\nincompatible with the draft model; 2) expectation of latency improvements over\nusage and time. In this work, we propose OmniDraft, a unified framework that\nenables a single draft model to operate with any target model and adapt\ndynamically to user data. We introduce an online n-gram cache with hybrid\ndistillation fine-tuning to address the cross-vocabulary mismatch across draft\nand target models; and further improve decoding speed by leveraging adaptive\ndrafting techniques. OmniDraft is particularly suitable for on-device LLM\napplications where model cost, efficiency and user customization are the major\npoints of contention. This further highlights the need to tackle the above\nchallenges and motivates the \\textit{``one drafter for all''} paradigm. We\nshowcase the proficiency of the OmniDraft framework by performing online\nlearning on math reasoning, coding and text generation tasks. Notably,\nOmniDraft enables a single Llama-68M model to pair with various target models\nincluding Vicuna-7B, Qwen2-7B and Llama3-8B models for speculative decoding;\nand additionally provides up to 1.5-2x speedup.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02659v2", "cate": "cs.LG", "date": "2025-07-03", "updated": "2025-07-31"}
{"id": "2507.02724", "title": "Hierarchical Multi-Label Contrastive Learning for Protein-Protein Interaction Prediction Across Organisms", "authors": ["Shiyi Liu", "Buwen Liang", "Yuetong Fang", "Zixuan Jiang", "Renjing Xu"], "categories": ["cs.LG", "q-bio.BM"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02724v2", "summary": "Recent advances in AI for science have highlighted the power of contrastive\nlearning in bridging heterogeneous biological data modalities. Building on this\nparadigm, we propose HIPPO (HIerarchical Protein-Protein interaction prediction\nacross Organisms), a hierarchical contrastive framework for protein-protein\ninteraction(PPI) prediction, where protein sequences and their hierarchical\nattributes are aligned through multi-tiered biological representation matching.\nThe proposed approach incorporates hierarchical contrastive loss functions that\nemulate the structured relationship among functional classes of proteins. The\nframework adaptively incorporates domain and family knowledge through a\ndata-driven penalty mechanism, enforcing consistency between the learned\nembedding space and the intrinsic hierarchy of protein functions. Experiments\non benchmark datasets demonstrate that HIPPO achieves state-of-the-art\nperformance, outperforming existing methods and showing robustness in low-data\nregimes. Notably, the model demonstrates strong zero-shot transferability to\nother species without retraining, enabling reliable PPI prediction and\nfunctional inference even in less characterized or rare organisms where\nexperimental data are limited. Further analysis reveals that hierarchical\nfeature fusion is critical for capturing conserved interaction determinants,\nsuch as binding motifs and functional annotations. This work advances\ncross-species PPI prediction and provides a unified framework for interaction\nprediction in scenarios with sparse or imbalanced multi-species data.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02724v2", "cate": "cs.LG", "date": "2025-07-03", "updated": "2025-08-01"}
{"id": "2507.04562", "title": "Evaluating LLMs on Real-World Forecasting Against Human Superforecasters", "authors": ["Janna Lu"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.04562v2", "summary": "Large language models (LLMs) have demonstrated remarkable capabilities across\ndiverse tasks, but their ability to forecast future events remains\nunderstudied. A year ago, large language models struggle to come close to the\naccuracy of a human crowd. I evaluate state-of-the-art LLMs on 464 forecasting\nquestions from Metaculus, comparing their performance against human\nsuperforecasters. Frontier models achieve Brier scores that ostensibly surpass\nthe human crowd but still significantly underperform a group of\nsuperforecasters.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.04562v2", "cate": "cs.LG", "date": "2025-07-06", "updated": "2025-08-01"}
{"id": "2507.05416", "title": "EmissionNet: Air Quality Pollution Forecasting for Agriculture", "authors": ["Prady Saligram", "Tanvir Bhathal"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      The appendix figures are mixed up - several emission plots (e.g. CO2, CH4, GWP) are mislabeled and appear in the wrong order, leading to confusion in interpreting the results", "url": "http://arxiv.org/abs/2507.05416v3", "summary": "Air pollution from agricultural emissions is a significant yet often\noverlooked contributor to environmental and public health challenges.\nTraditional air quality forecasting models rely on physics-based approaches,\nwhich struggle to capture complex, nonlinear pollutant interactions. In this\nwork, we explore forecasting N$_2$O agricultural emissions through evaluating\npopular architectures, and proposing two novel deep learning architectures,\nEmissionNet (ENV) and EmissionNet-Transformer (ENT). These models leverage\nconvolutional and transformer-based architectures to extract spatial-temporal\ndependencies from high-resolution emissions data", "comment": "The appendix figures are mixed up - several emission plots (e.g. CO2,\n  CH4, GWP) are mislabeled and appear in the wrong order, leading to confusion\n  in interpreting the results", "pdf_url": "http://arxiv.org/pdf/2507.05416v3", "cate": "cs.LG", "date": "2025-07-07", "updated": "2025-08-01"}
{"id": "2507.10546", "title": "Disentangling Neural Disjunctive Normal Form Models", "authors": ["Kexin Gu Baugh", "Vincent Perreault", "Matthew Baugh", "Luke Dickens", "Katsumi Inoue", "Alessandra Russo"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted at NeSy 2025", "url": "http://arxiv.org/abs/2507.10546v2", "summary": "Neural Disjunctive Normal Form (DNF) based models are powerful and\ninterpretable approaches to neuro-symbolic learning and have shown promising\nresults in classification and reinforcement learning settings without prior\nknowledge of the tasks. However, their performance is degraded by the\nthresholding of the post-training symbolic translation process. We show here\nthat part of the performance degradation during translation is due to its\nfailure to disentangle the learned knowledge represented in the form of the\nnetworks' weights. We address this issue by proposing a new disentanglement\nmethod; by splitting nodes that encode nested rules into smaller independent\nnodes, we are able to better preserve the models' performance. Through\nexperiments on binary, multiclass, and multilabel classification tasks\n(including those requiring predicate invention), we demonstrate that our\ndisentanglement method provides compact and interpretable logical\nrepresentations for the neural DNF-based models, with performance closer to\nthat of their pre-translation counterparts. Our code is available at\nhttps://github.com/kittykg/disentangling-ndnf-classification.", "comment": "Accepted at NeSy 2025", "pdf_url": "http://arxiv.org/pdf/2507.10546v2", "cate": "cs.LG", "date": "2025-07-14", "updated": "2025-08-01"}
{"id": "2507.13703", "title": "Binarizing Physics-Inspired GNNs for Combinatorial Optimization", "authors": ["Martin Krutský", "Gustav Šír", "Vyacheslav Kungurtsev", "Georgios Korpas"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted to the 28th European Conference on Artificial Intelligence (ECAI 2025). This archival version includes supplementary appendices", "url": "http://arxiv.org/abs/2507.13703v2", "summary": "Physics-inspired graph neural networks (PI-GNNs) have been utilized as an\nefficient unsupervised framework for relaxing combinatorial optimization\nproblems encoded through a specific graph structure and loss, reflecting\ndependencies between the problem's variables. While the framework has yielded\npromising results in various combinatorial problems, we show that the\nperformance of PI-GNNs systematically plummets with an increasing density of\nthe combinatorial problem graphs. Our analysis reveals an interesting phase\ntransition in the PI-GNNs' training dynamics, associated with degenerate\nsolutions for the denser problems, highlighting a discrepancy between the\nrelaxed, real-valued model outputs and the binary-valued problem solutions. To\naddress the discrepancy, we propose principled alternatives to the naive\nstrategy used in PI-GNNs by building on insights from fuzzy logic and binarized\nneural networks. Our experiments demonstrate that the portfolio of proposed\nmethods significantly improves the performance of PI-GNNs in increasingly dense\nsettings.", "comment": "Accepted to the 28th European Conference on Artificial Intelligence\n  (ECAI 2025). This archival version includes supplementary appendices", "pdf_url": "http://arxiv.org/pdf/2507.13703v2", "cate": "cs.LG", "date": "2025-07-18", "updated": "2025-08-01"}
{"id": "2507.18926", "title": "Geometric Multi-color Message Passing Graph Neural Networks for Blood-brain Barrier Permeability Prediction", "authors": ["Trung Nguyen", "Md Masud Rana", "Farjana Tasnim Mukta", "Chang-Guo Zhan", "Duc Duy Nguyen"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18926v2", "summary": "Accurate prediction of blood-brain barrier permeability (BBBP) is essential\nfor central nervous system (CNS) drug development. While graph neural networks\n(GNNs) have advanced molecular property prediction, they often rely on\nmolecular topology and neglect the three-dimensional geometric information\ncrucial for modeling transport mechanisms. This paper introduces the geometric\nmulti-color message-passing graph neural network (GMC-MPNN), a novel framework\nthat enhances standard message-passing architectures by explicitly\nincorporating atomic-level geometric features and long-range interactions. Our\nmodel constructs weighted colored subgraphs based on atom types to capture the\nspatial relationships and chemical context that govern BBB permeability. We\nevaluated GMC-MPNN on three benchmark datasets for both classification and\nregression tasks, using rigorous scaffold-based splitting to ensure a robust\nassessment of generalization. The results demonstrate that GMC-MPNN\nconsistently outperforms existing state-of-the-art models, achieving superior\nperformance in both classifying compounds as permeable/non-permeable (AUC-ROC\nof 0.9704 and 0.9685) and in regressing continuous permeability values (RMSE of\n0.4609, Pearson correlation of 0.7759). An ablation study further quantified\nthe impact of specific atom-pair interactions, revealing that the model's\npredictive power derives from its ability to learn from both common and rare,\nbut chemically significant, functional motifs. By integrating spatial geometry\ninto the graph representation, GMC-MPNN sets a new performance benchmark and\noffers a more accurate and generalizable tool for drug discovery pipelines.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18926v2", "cate": "cs.LG", "date": "2025-07-25", "updated": "2025-08-01"}
{"id": "2507.22186", "title": "SourceSplice: Source Selection for Machine Learning Tasks", "authors": ["Ambarish Singh", "Romila Pradhan"], "categories": ["cs.LG", "cs.AI", "cs.DB", "I.2.6"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22186v2", "summary": "Data quality plays a pivotal role in the predictive performance of machine\nlearning (ML) tasks - a challenge amplified by the deluge of data sources\navailable in modern organizations. Prior work in data discovery largely focus\non metadata matching, semantic similarity or identifying tables that should be\njoined to answer a particular query, but do not consider source quality for\nhigh performance of the downstream ML task. This paper addresses the problem of\ndetermining the best subset of data sources that must be combined to construct\nthe underlying training dataset for a given ML task. We propose SourceGrasp and\nSourceSplice, frameworks designed to efficiently select a suitable subset of\nsources that maximizes the utility of the downstream ML model. Both the\nalgorithms rely on the core idea that sources (or their combinations)\ncontribute differently to the task utility, and must be judiciously chosen.\nWhile SourceGrasp utilizes a metaheuristic based on a greediness criterion and\nrandomization, the SourceSplice framework presents a source selection mechanism\ninspired from gene splicing - a core concept used in protein synthesis. We\nempirically evaluate our algorithms on three real-world datasets and synthetic\ndatasets and show that, with significantly fewer subset explorations,\nSourceSplice effectively identifies subsets of data sources leading to high\ntask utility. We also conduct studies reporting the sensitivity of SourceSplice\nto the decision choices under several settings.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22186v2", "cate": "cs.LG", "date": "2025-07-29", "updated": "2025-07-31"}
{"id": "2507.22767", "title": "Teaching the Teacher: Improving Neural Network Distillability for Symbolic Regression via Jacobian Regularization", "authors": ["Soumyadeep Dhar", "Kei Sen Fong", "Mehul Motani"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22767v2", "summary": "Distilling large neural networks into simple, human-readable symbolic\nformulas is a promising path toward trustworthy and interpretable AI. However,\nthis process is often brittle, as the complex functions learned by standard\nnetworks are poor targets for symbolic discovery, resulting in low-fidelity\nstudent models. In this work, we propose a novel training paradigm to address\nthis challenge. Instead of passively distilling a pre-trained network, we\nintroduce a \\textbf{Jacobian-based regularizer} that actively encourages the\n``teacher'' network to learn functions that are not only accurate but also\ninherently smoother and more amenable to distillation. We demonstrate through\nextensive experiments on a suite of real-world regression benchmarks that our\nmethod is highly effective. By optimizing the regularization strength for each\nproblem, we improve the $R^2$ score of the final distilled symbolic model by an\naverage of \\textbf{120\\% (relative)} compared to the standard distillation\npipeline, all while maintaining the teacher's predictive accuracy. Our work\npresents a practical and principled method for significantly improving the\nfidelity of interpretable models extracted from complex neural networks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22767v2", "cate": "cs.LG", "date": "2025-07-30", "updated": "2025-08-01"}
{"id": "2507.23437", "title": "Coflex: Enhancing HW-NAS with Sparse Gaussian Processes for Efficient and Scalable DNN Accelerator Design", "authors": ["Yinhui Ma", "Tomomasa Yamasaki", "Zhehui Wang", "Tao Luo", "Bo Wang"], "categories": ["cs.LG", "I.2.6; C.1.3; C.3"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted to the 2025 International Conference on Computer-Aided Design (ICCAD); 9 pages, including 6 figures and 7 tables", "url": "http://arxiv.org/abs/2507.23437v2", "summary": "Hardware-Aware Neural Architecture Search (HW-NAS) is an efficient approach\nto automatically co-optimizing neural network performance and hardware energy\nefficiency, making it particularly useful for the development of Deep Neural\nNetwork accelerators on the edge. However, the extensive search space and high\ncomputational cost pose significant challenges to its practical adoption. To\naddress these limitations, we propose Coflex, a novel HW-NAS framework that\nintegrates the Sparse Gaussian Process (SGP) with multi-objective Bayesian\noptimization. By leveraging sparse inducing points, Coflex reduces the GP\nkernel complexity from cubic to near-linear with respect to the number of\ntraining samples, without compromising optimization performance. This enables\nscalable approximation of large-scale search space, substantially decreasing\ncomputational overhead while preserving high predictive accuracy. We evaluate\nthe efficacy of Coflex across various benchmarks, focusing on\naccelerator-specific architecture. Our experimental results show that Coflex\noutperforms state-of-the-art methods in terms of network accuracy and\nEnergy-Delay-Product, while achieving a computational speed-up ranging from\n1.9x to 9.5x.", "comment": "Accepted to the 2025 International Conference on Computer-Aided\n  Design (ICCAD); 9 pages, including 6 figures and 7 tables", "pdf_url": "http://arxiv.org/pdf/2507.23437v2", "cate": "cs.LG", "date": "2025-07-31", "updated": "2025-08-01"}
{"id": "2303.16955", "title": "Quantum Generative Modeling using Parameterized Quantum Circuits", "authors": ["Soumyadip Sarkar"], "categories": ["quant-ph", "cs.LG"], "primary_category": "Subjects:       Quantum Physics (quant-ph)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2303.16955v2", "summary": "Quantum generative models use the intrinsic probabilistic nature of quantum\nmechanics to learn and reproduce complex probability distributions. In this\npaper, we present an implementation of a 3-qubit quantum circuit Born machine\ntrained to model a 3-bit Gaussian distribution using a Kullback-Leibler (KL)\ndivergence loss and parameter-shift gradient optimization. The variational\nquantum circuit consists of layers of parameterized rotations and entangling\ngates, and is optimized such that the Born rule output distribution closely\nmatches the target distribution. We detail the mathematical formulation of the\nmodel distribution, the KL divergence cost function, and the parameter-shift\nrule for gradient evaluation. Training results on a statevector simulator show\nthat the KL divergence is minimized to near zero, and the final generated\ndistribution aligns quantitatively with the target probabilities. We analyze\nthe convergence behavior and discuss the implications for scalability and\nquantum advantage. Our results demonstrate the feasibility of small-scale\nquantum generative learning and provide insight into the training dynamics of\nquantum circuit models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2303.16955v2", "cate": "quant-ph", "date": "2023-03-25", "updated": "2025-07-31"}
{"id": "2312.01046", "title": "Bagged Regularized $k$-Distances for Anomaly Detection", "authors": ["Yuchao Cai", "Hanfang Yang", "Yuheng Ma", "Hanyuan Hang"], "categories": ["stat.ML", "cs.LG", "math.ST", "stat.TH"], "primary_category": "Subjects:       Machine Learning (stat.ML)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2312.01046v3", "summary": "We consider the paradigm of unsupervised anomaly detection, which involves\nthe identification of anomalies within a dataset in the absence of labeled\nexamples. Though distance-based methods are top-performing for unsupervised\nanomaly detection, they suffer heavily from the sensitivity to the choice of\nthe number of the nearest neighbors. In this paper, we propose a new\ndistance-based algorithm called bagged regularized $k$-distances for anomaly\ndetection (BRDAD), converting the unsupervised anomaly detection problem into a\nconvex optimization problem. Our BRDAD algorithm selects the weights by\nminimizing the surrogate risk, i.e., the finite sample bound of the empirical\nrisk of the bagged weighted $k$-distances for density estimation (BWDDE). This\napproach enables us to successfully address the sensitivity challenge of the\nhyperparameter choice in distance-based algorithms. Moreover, when dealing with\nlarge-scale datasets, the efficiency issues can be addressed by the\nincorporated bagging technique in our BRDAD algorithm. On the theoretical side,\nwe establish fast convergence rates of the AUC regret of our algorithm and\ndemonstrate that the bagging technique significantly reduces the computational\ncomplexity. On the practical side, we conduct numerical experiments to\nillustrate the insensitivity of the parameter selection of our algorithm\ncompared with other state-of-the-art distance-based methods. Furthermore, our\nmethod achieves superior performance on real-world datasets with the introduced\nbagging technique compared to other approaches.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2312.01046v3", "cate": "stat.ML", "date": "2023-12-02", "updated": "2025-08-01"}
{"id": "2405.16958", "title": "Large Deviations of Gaussian Neural Networks with ReLU activation", "authors": ["Quirin Vogel"], "categories": ["stat.ML", "cs.LG", "math.PR", "60F10, 68T07"], "primary_category": "Subjects:       Machine Learning (stat.ML)", "pdf_link": null, "comments": "Comments:      13 pages, 2 figures, proof simplified", "url": "http://arxiv.org/abs/2405.16958v2", "summary": "We prove a large deviation principle for deep neural networks with Gaussian\nweights and at most linearly growing activation functions, such as ReLU. This\ngeneralises earlier work, in which bounded and continuous activation functions\nwere considered. In practice, linearly growing activation functions such as\nReLU are most commonly used. We furthermore simplify previous expressions for\nthe rate function and provide a power-series expansions for the ReLU case.", "comment": "13 pages, 2 figures, proof simplified", "pdf_url": "http://arxiv.org/pdf/2405.16958v2", "cate": "stat.ML", "date": "2024-05-27", "updated": "2025-08-01"}
{"id": "2406.15500", "title": "Pure interaction effects unseen by Random Forests", "authors": ["Ricardo Blum", "Munir Hiabu", "Enno Mammen", "Joseph Theo Meyer"], "categories": ["stat.ML", "cs.LG"], "primary_category": "Subjects:       Machine Learning (stat.ML)", "pdf_link": null, "comments": "Comments:      arXiv admin note: substantial text overlap with arXiv:2309.01460", "url": "http://arxiv.org/abs/2406.15500v2", "summary": "Random Forests are widely claimed to capture interactions well. However, some\nsimple examples suggest that they perform poorly in the presence of certain\npure interactions that the conventional CART criterion struggles to capture\nduring tree construction. Motivated from this, it is argued that simple\nalternative partitioning schemes used in the tree growing procedure can enhance\nidentification of these interactions. In a simulation study these variants are\ncompared to conventional Random Forests and Extremely Randomized Trees. The\nresults validate that the modifications considered enhance the model's fitting\nability in scenarios where pure interactions play a crucial role. Finally, the\nmethods are applied to real datasets.", "comment": "arXiv admin note: substantial text overlap with arXiv:2309.01460", "pdf_url": "http://arxiv.org/pdf/2406.15500v2", "cate": "stat.ML", "date": "2024-06-19", "updated": "2025-08-01"}
{"id": "2408.07877", "title": "BCR-DRL: Behavior- and Context-aware Reward for Deep Reinforcement Learning in Human-AI Coordination", "authors": ["Xin Hao", "Bahareh Nakisa", "Mohmmad Naim Rastgoo", "Gaoyang Pang"], "categories": ["cs.AI", "cs.LG"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2408.07877v5", "summary": "Deep reinforcement Learning (DRL) offers a powerful framework for training AI\nagents to coordinate with human partners. However, DRL faces two critical\nchallenges in human-AI coordination (HAIC): sparse rewards and unpredictable\nhuman behaviors. These challenges significantly limit DRL to identify effective\ncoordination policies, due to its impaired capability of optimizing exploration\nand exploitation. To address these limitations, we propose an innovative\nbehavior- and context-aware reward (BCR) for DRL, which optimizes exploration\nand exploitation by leveraging human behaviors and contextual information in\nHAIC. Our BCR consists of two components: (i) A novel dual intrinsic rewarding\nscheme to enhance exploration. This scheme composes an AI self-motivated\nintrinsic reward and a human-motivated intrinsic reward, which are designed to\nincrease the capture of sparse rewards by a logarithmic-based strategy; and\n(ii) A new context-aware weighting mechanism for the designed rewards to\nimprove exploitation. This mechanism helps the AI agent prioritize actions that\nbetter coordinate with the human partner by utilizing contextual information\nthat can reflect the evolution of learning. Extensive simulations in the\nOvercooked environment demonstrate that our approach can increase the\ncumulative sparse rewards by approximately 20%, and improve the sample\nefficiency by around 38% compared to state-of-the-art baselines.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2408.07877v5", "cate": "cs.AI", "date": "2024-08-15", "updated": "2025-08-01"}
{"id": "2409.01413", "title": "Probabilistic Iterative Hard Thresholding for Sparse Learning", "authors": ["Matteo Bergamaschi", "Andrea Cristofari", "Vyacheslav Kungurtsev", "Francesco Rinaldi"], "categories": ["math.OC", "cs.LG"], "primary_category": "Subjects:       Optimization and Control (math.OC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2409.01413v2", "summary": "For statistical modeling wherein the data regime is unfavorable in terms of\ndimensionality relative to the sample size, finding hidden sparsity in the\nground truth can be critical in formulating an accurate statistical model. The\nso-called \"l0 norm\" which counts the number of non-zero components in a vector,\nis a strong reliable mechanism of enforcing sparsity when incorporated into an\noptimization problem for minimizing the fit of a given model to a set of\nobservations. However, in big data settings wherein noisy estimates of the\ngradient must be evaluated out of computational necessity, the literature is\nscant on methods that reliably converge. In this paper we present an approach\ntowards solving expectation objective optimization problems with cardinality\nconstraints. We prove convergence of the underlying stochastic process, and\ndemonstrate the performance on two Machine Learning problems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2409.01413v2", "cate": "math.OC", "date": "2024-09-02", "updated": "2025-08-01"}
{"id": "2409.15126", "title": "UTrace: Poisoning Forensics for Private Collaborative Learning", "authors": ["Evan Rose", "Hidde Lycklama", "Harsh Chaudhari", "Anwar Hithnawi", "Alina Oprea"], "categories": ["cs.CR", "cs.LG"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      28 pages, 10 figures; update ack", "url": "http://arxiv.org/abs/2409.15126v2", "summary": "Privacy-preserving machine learning (PPML) enables multiple data owners to\ncontribute their data privately to a set of servers that run a secure\nmulti-party computation (MPC) protocol to train a joint ML model. In these\nprotocols, the input data remains private throughout the training process, and\nonly the resulting model is made available. While this approach benefits\nprivacy, it also exacerbates the risks of data poisoning, where compromised\ndata owners induce undesirable model behavior by contributing malicious\ndatasets. Existing MPC mechanisms can mitigate certain poisoning attacks, but\nthese measures are not exhaustive. To complement existing poisoning defenses,\nwe introduce UTrace: a framework for User-level Traceback of poisoning attacks\nin PPML. Utrace computes user responsibility scores using gradient similarity\nmetrics aggregated across the most relevant samples in an owner's dataset.\nUTrace is effective at low poisoning rates and is resilient to poisoning\nattacks distributed across multiple data owners, unlike existing\nunlearning-based methods. We introduce methods for checkpointing gradients with\nlow storage overhead, enabling traceback in the absence of data owners at\ndeployment time. We also design several optimizations that reduce traceback\ntime and communication in MPC. We provide a comprehensive evaluation of UTrace\nacross four datasets from three data modalities (vision, text, and malware) and\nshow its effectiveness against 10 poisoning attacks.", "comment": "28 pages, 10 figures; update ack", "pdf_url": "http://arxiv.org/pdf/2409.15126v2", "cate": "cs.CR", "date": "2024-09-23", "updated": "2025-08-01"}
{"id": "2409.18203", "title": "Policy Maps: Tools for Guiding the Unbounded Space of LLM Behaviors", "authors": ["Michelle S. Lam", "Fred Hohman", "Dominik Moritz", "Jeffrey P. Bigham", "Kenneth Holstein", "Mary Beth Kery"], "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      UIST 2025", "url": "http://arxiv.org/abs/2409.18203v2", "summary": "AI policy sets boundaries on acceptable behavior for AI models, but this is\nchallenging in the context of large language models (LLMs): how do you ensure\ncoverage over a vast behavior space? We introduce policy maps, an approach to\nAI policy design inspired by the practice of physical mapmaking. Instead of\naiming for full coverage, policy maps aid effective navigation through\nintentional design choices about which aspects to capture and which to abstract\naway. With Policy Projector, an interactive tool for designing LLM policy maps,\nan AI practitioner can survey the landscape of model input-output pairs, define\ncustom regions (e.g., \"violence\"), and navigate these regions with if-then\npolicy rules that can act on LLM outputs (e.g., if output contains \"violence\"\nand \"graphic details,\" then rewrite without \"graphic details\"). Policy\nProjector supports interactive policy authoring using LLM classification and\nsteering and a map visualization reflecting the AI practitioner's work. In an\nevaluation with 12 AI safety experts, our system helps policy designers craft\npolicies around problematic model behaviors such as incorrect gender\nassumptions and handling of immediate physical safety threats.", "comment": "UIST 2025", "pdf_url": "http://arxiv.org/pdf/2409.18203v2", "cate": "cs.HC", "date": "2024-09-26", "updated": "2025-08-01"}
{"id": "2411.16719", "title": "Learn2Synth: Learning Optimal Data Synthesis Using Hypergradients for Brain Image Segmentation", "authors": ["Xiaoling Hu", "Xiangrui Zeng", "Oula Puonti", "Juan Eugenio Iglesias", "Bruce Fischl", "Yael Balbastre"], "categories": ["cs.CV", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      16 pages, 5 figures. Accepted by ICCV'25", "url": "http://arxiv.org/abs/2411.16719v3", "summary": "Domain randomization through synthesis is a powerful strategy to train\nnetworks that are unbiased with respect to the domain of the input images.\nRandomization allows networks to see a virtually infinite range of intensities\nand artifacts during training, thereby minimizing overfitting to appearance and\nmaximizing generalization to unseen data. Although powerful, this approach\nrelies on the accurate tuning of a large set of hyperparameters that govern the\nprobabilistic distribution of the synthesized images. Instead of manually\ntuning these parameters, we introduce Learn2Synth, a novel procedure in which\nsynthesis parameters are learned using a small set of real labeled data. Unlike\nmethods that impose constraints to align synthetic data with real data (e.g.,\ncontrastive or adversarial techniques), which risk misaligning the image and\nits label map, we tune an augmentation engine such that a segmentation network\ntrained on synthetic data has optimal accuracy when applied to real data. This\napproach allows the training procedure to benefit from real labeled examples,\nwithout ever using these real examples to train the segmentation network, which\navoids biasing the network towards the properties of the training set.\nSpecifically, we develop parametric and nonparametric strategies to enhance\nsynthetic images in a way that improves the performance of the segmentation\nnetwork. We demonstrate the effectiveness of this learning strategy on\nsynthetic and real-world brain scans. Code is available at:\nhttps://github.com/HuXiaoling/Learn2Synth.", "comment": "16 pages, 5 figures. Accepted by ICCV'25", "pdf_url": "http://arxiv.org/pdf/2411.16719v3", "cate": "cs.CV", "date": "2024-11-23", "updated": "2025-08-01"}
{"id": "2412.09727", "title": "A Large Sensor Foundation Model Pretrained on Continuous Glucose Monitor Data for Diabetes Management", "authors": ["Junjie Luo", "Abhimanyu Kumbara", "Mansur Shomali", "Rui Han", "Anand Iyer", "Ritu Agarwal", "Gordon Gao"], "categories": ["q-bio.QM", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Quantitative Methods (q-bio.QM)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2412.09727v3", "summary": "Continuous glucose monitoring (CGM) combined with AI offers new opportunities\nfor proactive diabetes management through real-time glucose forecasting.\nHowever, most existing models are task-specific and lack generalization across\npatient populations. Inspired by the autoregressive paradigm of large language\nmodels, we introduce CGM-LSM, a Transformer decoder-based Large Sensor Model\n(LSM) pretrained on 1.6 million CGM records from patients with different\ndiabetes types, ages, and genders. We model patients as sequences of glucose\ntime steps to learn latent knowledge embedded in CGM data and apply it to the\nprediction of glucose readings for a 2-hour horizon. Compared with prior\nmethods, CGM-LSM significantly improves prediction accuracy and robustness: a\n48.51% reduction in root mean square error in one-hour horizon forecasting and\nconsistent zero-shot prediction performance across held-out patient groups. We\nanalyze model performance variations across patient subgroups and prediction\nscenarios and outline key opportunities and challenges for advancing CGM\nfoundation models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2412.09727v3", "cate": "q-bio.QM", "date": "2024-12-12", "updated": "2025-08-01"}
{"id": "2412.10537", "title": "ExclaveFL: Providing Transparency to Federated Learning using Exclaves", "authors": ["Jinnan Guo", "Kapil Vaswani", "Andrew Paverd", "Peter Pietzuch"], "categories": ["cs.CR", "cs.DC", "cs.LG"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2412.10537v2", "summary": "In federated learning (FL), data providers jointly train a model without\ndisclosing their training data. Despite its inherent privacy benefits, a\nmalicious data provider can simply deviate from the correct training protocol\nwithout being detected, potentially compromising the trained model. While\ncurrent solutions have explored the use of trusted execution environments\n(TEEs) to combat such attacks, they usually assume side-channel attacks against\nthe TEEs are out of scope. However, such side-channel attacks can undermine the\nsecurity properties of TEE-based FL frameworks, not by extracting the FL data,\nbut by leaking keys that allow the adversary to impersonate as the TEE whilst\ndeviating arbitrarily from the correct training protocol.\n  We describe ExclaveFL, an FL platform that provides end-to-end integrity and\ntransparency, even in the presence of side-channel attacks on TEEs. We propose\na new paradigm in which existing TEEs are used as exclaves --\nintegrity-protected execution environments that do not contain any secrets,\nmaking them immune to side-channel attacks. Whereas previous approaches attest\nthe TEE itself and bind this attestation to a key held by the TEE, ExclaveFL\nattests individual data transformations at runtime. These runtime attestations\nform an attested dataflow graph, which can be checked to ensure the FL training\njob satisfies claims, such as deviations from the correct computation. We\nimplement ExclaveFL by extending the popular NVFlare FL framework to use\nexclaves, and show experimentally that ExclaveFL introduces less than 10%\noverhead compared to the same FL framework without TEEs, whilst providing\nstronger security guarantees.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2412.10537v2", "cate": "cs.CR", "date": "2024-12-13", "updated": "2025-08-01"}
{"id": "2502.08441", "title": "Better Embeddings with Coupled Adam", "authors": ["Felix Stollenwerk", "Tobias Stollenwerk"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      ACL 2025 (Main), see this https URL", "url": "http://arxiv.org/abs/2502.08441v3", "summary": "Despite their remarkable capabilities, LLMs learn word representations that\nexhibit the undesirable yet poorly understood feature of anisotropy. In this\npaper, we argue that the second moment in Adam is a cause of anisotropic\nembeddings, and suggest a modified optimizer called Coupled Adam to mitigate\nthe problem. Our experiments demonstrate that Coupled Adam significantly\nimproves the quality of embeddings, while also leading to better upstream and\ndownstream performance on large enough datasets.", "comment": "ACL 2025 (Main), see https://aclanthology.org/2025.acl-long.1321/", "pdf_url": "http://arxiv.org/pdf/2502.08441v3", "cate": "cs.CL", "date": "2025-02-12", "updated": "2025-08-01"}
{"id": "2502.19573", "title": "Do Large Language Models Know How Much They Know?", "authors": ["Gabriele Prato", "Jerry Huang", "Prasanna Parthasarathi", "Shagun Sodhani", "Sarath Chandar"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      ublished as a long paper at the 2024 Conference on Empirical Methods in Natural Language Processing (EMNLP). Official version of paper within conference proceedings is available at this https URL", "url": "http://arxiv.org/abs/2502.19573v2", "summary": "Large Language Models (LLMs) have emerged as highly capable systems and are\nincreasingly being integrated into various uses. However, the rapid pace of\ntheir deployment has outpaced a comprehensive understanding of their internal\nmechanisms and a delineation of their capabilities and limitations. A desired\nattribute of an intelligent system is its ability to recognize the scope of its\nown knowledge. To investigate whether LLMs embody this characteristic, we\ndevelop a benchmark designed to challenge these models to enumerate all\ninformation they possess on specific topics. This benchmark evaluates whether\nthe models recall excessive, insufficient, or the precise amount of\ninformation, thereby indicating their awareness of their own knowledge. Our\nfindings reveal that all tested LLMs, given sufficient scale, demonstrate an\nunderstanding of how much they know about specific topics. While different\narchitectures exhibit varying rates of this capability's emergence, the results\nsuggest that awareness of knowledge may be a generalizable attribute of LLMs.\nFurther research is needed to confirm this potential and fully elucidate the\nunderlying mechanisms.", "comment": "ublished as a long paper at the 2024 Conference on Empirical Methods\n  in Natural Language Processing (EMNLP). Official version of paper within\n  conference proceedings is available at\n  https://aclanthology.org/2024.emnlp-main.348/", "pdf_url": "http://arxiv.org/pdf/2502.19573v2", "cate": "cs.CL", "date": "2025-02-26", "updated": "2025-08-01"}
{"id": "2504.20401", "title": "Nonlinear Computation with Linear Optics via Source-Position Encoding", "authors": ["N. Richardson", "C. Bosch", "R. P. Adams"], "categories": ["physics.optics", "cs.AR", "cs.LG"], "primary_category": "Subjects:       Optics (physics.optics)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.20401v2", "summary": "Optical computing systems provide an alternate hardware model which appears\nto be aligned with the demands of neural network workloads. However, the\nchallenge of implementing energy efficient nonlinearities in optics -- a key\nrequirement for realizing neural networks -- is a conspicuous missing link. In\nthis work we introduce a novel method to achieve nonlinear computation in fully\nlinear media. Our method can operate at low power and requires only the ability\nto drive the optical system at a data-dependent spatial position. Leveraging\nthis positional encoding, we formulate a fully automated,\ntopology-optimization-based hardware design framework for extremely specialized\noptical neural networks, drawing on modern advancements in optimization and\nmachine learning. We evaluate our optical designs on machine learning\nclassification tasks: demonstrating significant improvements over linear\nmethods, and competitive performance when compared to standard artificial\nneural networks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.20401v2", "cate": "physics.optics", "date": "2025-04-29", "updated": "2025-08-01"}
{"id": "2505.10498", "title": "Batched Nonparametric Bandits via k-Nearest Neighbor UCB", "authors": ["Sakshi Arya"], "categories": ["stat.ML", "cs.LG", "math.ST", "stat.ME", "stat.TH", "68T05, 62L05, 62G08, 68Q32", "F.2.2; I.2.6"], "primary_category": "Subjects:       Machine Learning (stat.ML)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.10498v2", "summary": "We study sequential decision-making in batched nonparametric contextual\nbandits, where actions are selected over a finite horizon divided into a small\nnumber of batches. Motivated by constraints in domains such as medicine and\nmarketing -- where online feedback is limited -- we propose a nonparametric\nalgorithm that combines adaptive k-nearest neighbor (k-NN) regression with the\nupper confidence bound (UCB) principle. Our method, BaNk-UCB, is fully\nnonparametric, adapts to the context dimension, and is simple to implement.\nUnlike prior work relying on parametric or binning-based estimators, BaNk-UCB\nuses local geometry to estimate rewards and adaptively balances exploration and\nexploitation. We provide near-optimal regret guarantees under standard\nLipschitz smoothness and margin assumptions, using a theoretically motivated\nbatch schedule that balances regret across batches and achieves minimax-optimal\nrates. Empirical evaluations on synthetic and real-world datasets demonstrate\nthat BaNk-UCB consistently outperforms binning-based baselines.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.10498v2", "cate": "stat.ML", "date": "2025-05-15", "updated": "2025-08-01"}
{"id": "2507.19861", "title": "Quantum-Informed Machine Learning for Chaotic Systems", "authors": ["Maida Wang", "Xiao Xue", "Peter V. Coveney"], "categories": ["quant-ph", "cs.LG"], "primary_category": "Subjects:       Quantum Physics (quant-ph)", "pdf_link": null, "comments": "Comments:      33 pages, 4 figures", "url": "http://arxiv.org/abs/2507.19861v2", "summary": "Learning the behaviour of chaotic systems remains challenging due to\ninstability in long-term predictions and difficulties in accurately capturing\ninvariant statistical properties. While quantum machine learning offers a\npromising route to efficiently capture physical properties from\nhigh-dimensional data, its practical deployment is hindered by current hardware\nnoise and limited scalability. Here, we introduce a quantum-informed machine\nlearning framework for learning partial differential equations, with an\napplication focus on chaotic systems. A quantum circuit Born machine is\nemployed to learn the invariant properties of chaotic dynamical systems,\nachieving substantial memory efficiency by representing these complex physical\nstatistics with a compact set of trainable circuit parameters. This approach\nreduces the data storage requirement by over two orders of magnitude compared\nto the raw simulation data. The resulting statistical quantum-informed prior is\nthen incorporated into a Koopman-based auto-regressive model to address issues\nsuch as gradient vanishing or explosion, while maintaining long-term\nstatistical fidelity. The framework is evaluated on three representative\nsystems: the Kuramoto-Sivashinsky equation, two-dimensional Kolmogorov flow and\nturbulent channel flow. In all cases, the quantum-informed model achieves\nsuperior performance compared to its classical counterparts without quantum\npriors. This hybrid architecture offers a practical route for learning\ndynamical systems using near-term quantum hardware.", "comment": "33 pages, 4 figures", "pdf_url": "http://arxiv.org/pdf/2507.19861v2", "cate": "quant-ph", "date": "2025-07-26", "updated": "2025-08-01"}
{"id": "2507.20403", "title": "A General Framework for Estimating Preferences Using Response Time Data", "authors": ["Federico Echenique", "Alireza Fallah", "Michael I. Jordan"], "categories": ["econ.TH", "cs.LG"], "primary_category": "Subjects:       Theoretical Economics (econ.TH)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.20403v2", "summary": "We propose a general methodology for recovering preference parameters from\ndata on choices and response times. Our methods yield estimates with fast\n($1/n$ for $n$ data points) convergence rates when specialized to the popular\nDrift Diffusion Model (DDM), but are broadly applicable to generalizations of\nthe DDM as well as to alternative models of decision making that make use of\nresponse time data. The paper develops an empirical application to an\nexperiment on intertemporal choice, showing that the use of response times\ndelivers predictive accuracy and matters for the estimation of economically\nrelevant parameters.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.20403v2", "cate": "econ.TH", "date": "2025-07-27", "updated": "2025-07-31"}
