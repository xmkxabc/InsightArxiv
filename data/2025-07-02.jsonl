{"id": "2507.00096", "title": "AI-Governed Agent Architecture for Web-Trustworthy Tokenization of Alternative Assets", "authors": ["Ailiya Borjigin", "Wei Zhou", "Cong He"], "summary": "Alternative Assets tokenization is transforming non-traditional financial\ninstruments are represented and traded on the web. However, ensuring\ntrustworthiness in web-based tokenized ecosystems poses significant challenges,\nfrom verifying off-chain asset data to enforcing regulatory compliance. This\npaper proposes an AI-governed agent architecture that integrates intelligent\nagents with blockchain to achieve web-trustworthy tokenization of alternative\nassets. In the proposed architecture, autonomous agents orchestrate the\ntokenization process (asset verification, valuation, compliance checking, and\nlifecycle management), while an AI-driven governance layer monitors agent\nbehavior and enforces trust through adaptive policies and cryptoeconomic\nincentives. We demonstrate that this approach enhances transparency, security,\nand compliance in asset tokenization, addressing key concerns around data\nauthenticity and fraud. A case study on tokenizing real estate assets\nillustrates how the architecture mitigates risks (e.g., fraudulent listings and\nmoney laundering) through real-time AI anomaly detection and on-chain\nenforcement. Our evaluation and analysis suggest that combining AI governance\nwith multi-agent systems and blockchain can significantly bolster trust in\ntokenized asset ecosystems. This work offers a novel framework for trustworthy\nasset tokenization on the web and provides insights for practitioners aiming to\ndeploy secure, compliant tokenization platforms.", "comment": "8 Pages, 1 figure", "pdf_url": "http://arxiv.org/pdf/2507.00096v1", "categories": ["cs.CR", "cs.AI"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2507.00096v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00145", "title": "AI-Hybrid TRNG: Kernel-Based Deep Learning for Near-Uniform Entropy Harvesting from Physical Noise", "authors": ["Hasan Yiğit"], "summary": "AI-Hybrid TRNG is a deep-learning framework that extracts near-uniform\nentropy directly from physical noise, eliminating the need for bulky quantum\ndevices or expensive laboratory-grade RF receivers. Instead, it relies on a\nlow-cost, thumb-sized RF front end, plus CPU-timing jitter, for training, and\nthen emits 32-bit high-entropy streams without any quantization step.\n  Unlike deterministic or trained artificial intelligence random number\ngenerators (RNGs), our dynamic inner-outer network couples adaptive natural\nsources and reseeding, yielding truly unpredictable and autonomous sequences.\nGenerated numbers pass the NIST SP 800-22 battery better than a CPU-based\nmethod. It also passes nineteen bespoke statistical tests for both bit- and\ninteger-level analysis. All results satisfy cryptographic standards, while\nforward and backward prediction experiments reveal no exploitable biases. The\nmodel's footprint is below 0.5 MB, making it deployable on MCUs and FPGA soft\ncores, as well as suitable for other resource-constrained platforms.\n  By detaching randomness quality from dedicated hardware, AI-Hybrid TRNG\nbroadens the reach of high-integrity random number generators across secure\nsystems, cryptographic protocols, embedded and edge devices, stochastic\nsimulations, and server applications that need randomness.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00145v1", "categories": ["cs.CR", "cs.AI", "cs.ET", "cs.IT", "eess.SP", "math.IT"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2507.00145v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00189", "title": "Plug. Play. Persist. Inside a Ready-to-Go Havoc C2 Infrastructure", "authors": ["Alessio Di Santo"], "summary": "This analysis focuses on a single Azure-hosted Virtual Machine at\n52.230.23.114 that the adversary converted into an all-in-one delivery, staging\nand Command-and-Control node. The host advertises an out-of-date Apache 2.4.52\ninstance whose open directory exposes phishing lures, PowerShell loaders,\nReflective Shell-Code, compiled Havoc Demon implants and a toolbox of\nlateral-movement binaries; the same server also answers on 8443/80 for\nencrypted beacon traffic. The web tier is riddled with publicly documented\ncritical vulnerabilities, that would have allowed initial code-execution had\nthe attackers not already owned the device.\n  Initial access is delivered through an HTML file that, once de-obfuscated,\nperfectly mimics Google Unusual sign-in attempt notification and funnels\nvictims toward credential collection. A PowerShell command follows: it disables\nAMSI in-memory, downloads a Base64-encoded stub, allocates RWX pages and starts\nthe shell-code without ever touching disk. That stub reconstructs a DLL in\nmemory using the Reflective-Loader technique and hands control to Havoc Demon\nimplant. Every Demon variant-32- and 64-bit alike-talks to the same backend,\nresolves Windows APIs with hashed look-ups, and hides its activity behind\nindirect syscalls.\n  Runtime telemetry shows interests in registry under Image File Execution\nOptions, deliberate queries to Software Restriction Policy keys, and heavy use\nof Crypto DLLs to protect payloads and C2 traffic. The attacker toolkit further\ncontains Chisel, PsExec, Doppelganger and Whisker, some of them re-compiled\nunder user directories that leak the developer personas tonzking123 and thobt.\nCollectively the findings paint a picture of a technically adept actor who\nvalues rapid re-tooling over deep operational security, leaning on Havoc\nmodularity and on legitimate cloud services to blend malicious flows into\nordinary enterprise traffic.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00189v1", "categories": ["cs.CR", "cs.OS"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2507.00189v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00348", "title": "Addressing malware family concept drift with triplet autoencoder", "authors": ["Numan Halit Guldemir", "Oluwafemi Olukoya", "Jesús Martínez-del-Rincón"], "summary": "Machine learning is increasingly vital in cybersecurity, especially in\nmalware detection. However, concept drift, where the characteristics of malware\nchange over time, poses a challenge for maintaining the efficacy of these\ndetection systems. Concept drift can occur in two forms: the emergence of\nentirely new malware families and the evolution of existing ones. This paper\nproposes an innovative method to address the former, focusing on effectively\nidentifying new malware families. Our approach leverages a supervised\nautoencoder combined with triplet loss to differentiate between known and new\nmalware families. We create clear and robust clusters that enhance the accuracy\nand resilience of malware family classification by utilizing this metric\nlearning technique and the Density-Based Spatial Clustering of Applications\nwith Noise (DBSCAN) algorithm. The effectiveness of our method is validated\nusing an Android malware dataset and a Windows portable executable (PE) malware\ndataset, showcasing its capability to sustain model performance within the\ndynamic landscape of emerging malware threats. Our results demonstrate a\nsignificant improvement in detecting new malware families, offering a reliable\nsolution for ongoing cybersecurity challenges.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00348v1", "categories": ["cs.CR"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2507.00348v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00347", "title": "VTS-Guided AI Interaction Workflow for Business Insights", "authors": ["Sun Ding", "Ude Enebeli", "Atilhan", "Manay", "Ryan Pua", "Kamal Kotak"], "summary": "Modern firms face a flood of dense, unstructured reports. Turning these\ndocuments into usable insights takes heavy effort and is far from agile when\nquick answers are needed. VTS-AI tackles this gap. It integrates Visual\nThinking Strategies, which emphasize evidence-based observation, linking, and\nthinking, into AI agents, so the agents can extract business insights from\nunstructured text, tables, and images at scale. The system works in three tiers\n(micro, meso, macro). It tags issues, links them to source pages, and rolls\nthem into clear action levers stored in a searchable YAML file. In tests on an\n18-page business report, VTS-AI matched the speed of a one-shot ChatGPT prompt\nyet produced richer findings: page locations, verbatim excerpts, severity\nscores, and causal links. Analysts can accept or adjust these outputs in the\nsame IDE, keeping human judgment in the loop. Early results show VTS-AI spots\nthe direction of key metrics and flags where deeper number-crunching is needed.\nNext steps include mapping narrative tags to financial ratios, adding\nfinance-tuned language models through a Model-Context Protocol, and building a\nRisk & Safety Layer to stress-test models and secure data. These upgrades aim\nto make VTS-AI a production-ready, audit-friendly tool for rapid business\nanalysis.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00347v1", "categories": ["cs.SE", "cs.AI"], "cate": "cs.SE", "url": "http://arxiv.org/abs/2507.00347v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00306", "title": "Origin-Destination Travel Demand Estimation: An Approach That Scales Worldwide, and Its Application to Five Metropolitan Highway Networks", "authors": ["Chao Zhang", "Neha Arora", "Christopher Bian", "Yechen Li", "Willa Ng", "Andrew Tomkins", "Bin Yan", "Janny Zhang", "Carolina Osorio"], "summary": "Estimating Origin-Destination (OD) travel demand is vital for effective urban\nplanning and traffic management. Developing universally applicable OD\nestimation methodologies is significantly challenged by the pervasive scarcity\nof high-fidelity traffic data and the difficulty in obtaining city-specific\nprior OD estimates (or seed ODs), which are often prerequisite for traditional\napproaches. Our proposed method directly estimates OD travel demand by\nsystematically leveraging aggregated, anonymized statistics from Google Maps\nTraffic Trends, obviating the need for conventional census or city-provided OD\ndata. The OD demand is estimated by formulating a single-level,\none-dimensional, continuous nonlinear optimization problem with nonlinear\nequality and bound constraints to replicate highway path travel times. The\nmethod achieves efficiency and scalability by employing a differentiable\nanalytical macroscopic network model. This model by design is computationally\nlightweight, distinguished by its parsimonious parameterization that requires\nminimal calibration effort and its capacity for instantaneous evaluation. These\nattributes ensure the method's broad applicability and practical utility across\ndiverse cities globally. Using segment sensor counts from Los Angeles and San\nDiego highway networks, we validate our proposed approach, demonstrating a\ntwo-thirds to three-quarters improvement in the fit to segment count data over\na baseline. Beyond validation, we establish the method's scalability and robust\nperformance in replicating path travel times across diverse highway networks,\nincluding Seattle, Orlando, Denver, Philadelphia, and Boston. In these expanded\nevaluations, our method not only aligns with simulation-based benchmarks but\nalso achieves an average 13% improvement in it's ability to fit travel time\ndata compared to the baseline during afternoon peak hours.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00306v1", "categories": ["cs.ET"], "cate": "cs.ET", "url": "http://arxiv.org/abs/2507.00306v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00422", "title": "Evolutionary Dynamics with Self-Interaction Learning in Networked Systems", "authors": ["Ziyan Zeng", "Minyu Feng", "Attila Szolnoki"], "summary": "The evolution of cooperation in networked systems helps to understand the\ndynamics in social networks, multi-agent systems, and biological species. The\nself-persistence of individual strategies is common in real-world decision\nmaking. The self-replacement of strategies in evolutionary dynamics forms a\nselection amplifier, allows an agent to insist on its autologous strategy, and\nhelps the networked system to avoid full defection. In this paper, we study the\nself-interaction learning in the networked evolutionary dynamics. We propose a\nself-interaction landscape to capture the strength of an agent's self-loop to\nreproduce the strategy based on local topology. We find that proper\nself-interaction can reduce the condition for cooperation and help cooperators\nto prevail in the system. For a system that favors the evolution of spite, the\nself-interaction can save cooperative agents from being harmed. Our results on\nrandom networks further suggest that an appropriate self-interaction landscape\ncan significantly reduce the critical condition for advantageous mutants,\nespecially for large-degree networks.", "comment": "15 pages, 10 figures", "pdf_url": "http://arxiv.org/pdf/2507.00422v1", "categories": ["cs.SI", "physics.soc-ph"], "cate": "cs.SI", "url": "http://arxiv.org/abs/2507.00422v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00166", "title": "Novel Design of 3D Printed Tumbling Microrobots for in vivo Targeted Drug Delivery", "authors": ["Aaron C. Davis", "Siting Zhang", "Adalyn Meeks", "Diya Sakhrani", "Luis Carlos Sanjuan Acosta", "D. Ethan Kelley", "Emma Caldwell", "Luis Solorio", "Craig J. Goergen", "David J. Cappelleri"], "summary": "This paper presents innovative designs for 3D-printed tumbling microrobots,\nspecifically engineered for targeted in vivo drug delivery applications. The\nmicrorobot designs, created using stereolithography 3D printing technologies,\nincorporate permanent micro-magnets to enable actuation via a rotating magnetic\nfield actuator system. The experimental framework encompasses a series of\nlocomotion characterization tests to evaluate microrobot performance under\nvarious conditions. Testing variables include variations in microrobot\ngeometries, actuation frequencies, and environmental conditions, such as dry\nand wet environments, and temperature changes. The paper outlines designs for\nthree drug loading methods, along with comprehensive assessments thermal drug\nrelease using a focused ultrasound system, as well as biocompatibility tests.\nAnimal model testing involves tissue phantoms and in vivo rat models, ensuring\na thorough evaluation of the microrobots' performance and compatibility. The\nresults highlight the robustness and adaptability of the proposed microrobot\ndesigns, showcasing the potential for efficient and targeted in vivo drug\ndelivery. This novel approach addresses current limitations in existing\ntumbling microrobot designs and paves the way for advancements in targeted drug\ndelivery within the large intestine.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00166v1", "categories": ["cs.RO"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2507.00166v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00237", "title": "Plan-Based Scalable Online Virtual Network Embedding", "authors": ["Oleg Kolosov", "David Breitgand", "Dean H. Lorenz", "Gala Yadgar"], "summary": "Network virtualization allows hosting applications with diverse computation\nand communication requirements on shared edge infrastructure. Given a set of\nrequests for deploying virtualized applications, the edge provider has to\ndeploy a maximum number of them to the underlying physical network, subject to\ncapacity constraints. This challenge is known as the virtual network embedding\n(VNE) problem: it models applications as virtual networks, where virtual nodes\nrepresent functions and virtual links represent communication between the\nvirtual nodes.\n  All variants of VNE are known to be strongly NP-hard. Because of its\ncentrality to network virtualization, VNE has been extensively studied. We\nfocus on the online variant of VNE, in which deployment requests are not known\nin advance. This reflects the highly skewed and unpredictable demand intrinsic\nto the edge. Unfortunately, existing solutions to online VNE do not scale well\nwith the number of requests per second and the physical topology size.\n  We propose a novel approach in which our new online algorithm, OLIVE,\nleverages a nearly optimal embedding for an aggregated expected demand. This\nembedding is computed offline. It serves as a plan that OLIVE uses as a guide\nfor handling actual individual requests while dynamically compensating for\ndeviations from the plan. We demonstrate that our solution can handle a number\nof requests per second greater by two orders of magnitude than the best results\nreported in the literature. Thus, it is particularly suitable for realistic\nedge environments.", "comment": "Accepted to IEEE ICDCS 2025", "pdf_url": "http://arxiv.org/pdf/2507.00237v1", "categories": ["cs.NI"], "cate": "cs.NI", "url": "http://arxiv.org/abs/2507.00237v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00091", "title": "On the Optimality of Coded Distributed Computing for Ring Networks", "authors": ["Zhenhao Huang", "Minquan Cheng", "Kai Wan", "Qifu Tyler Sun", "Youlong Wu"], "summary": "We consider a coded distributed computing problem in a ring-based\ncommunication network, where $N$ computing nodes are arranged in a ring\ntopology and each node can only communicate with its neighbors within a\nconstant distance $d$. To mitigate the communication bottleneck in exchanging\nintermediate values, we propose new coded distributed computing schemes for the\nring-based network that exploit both ring topology and redundant computation\n(i.e., each map function is computed by $r$ nodes). Two typical cases are\nconsidered: all-gather where each node requires all intermediate values mapped\nfrom all input files, and all-to-all where each node requires a distinct set of\nintermediate values from other nodes. For the all-gather case, we propose a new\ncoded scheme based on successive reverse carpooling where nodes transmit every\nencoded packet containing two messages traveling in opposite directions along\nthe same path. Theoretical converse proof shows that our scheme achieves the\noptimal tradeoff between communication load, computation load $r$, and\nbroadcast distance $d$ when $N\\gg d$. For the all-to-all case, instead of\nsimply repeating our all-gather scheme, we delicately deliver intermediate\nvalues based on their proximity to intended nodes to reduce unnecessary\ntransmissions. We derive an information-theoretic lower bound on the optimal\ncommunication load and show that our scheme is asymptotically optimal under the\ncyclic placement when $N\\gg r$. The optimality results indicate that in\nring-based networks, the redundant computation $r$ only leads to an additive\ngain in reducing communication load while the broadcast distance $d$\ncontributes to a multiplicative gain.", "comment": "Part of the work has been presented at ISIT 2025", "pdf_url": "http://arxiv.org/pdf/2507.00091v1", "categories": ["cs.IT", "math.IT"], "cate": "cs.IT", "url": "http://arxiv.org/abs/2507.00091v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00008", "title": "DiMo-GUI: Advancing Test-time Scaling in GUI Grounding via Modality-Aware Visual Reasoning", "authors": ["Hang Wu", "Hongkai Chen", "Yujun Cai", "Chang Liu", "Qingwen Ye", "Ming-Hsuan Yang", "Yiwei Wang"], "summary": "Grounding natural language queries in graphical user interfaces (GUIs) poses\nunique challenges due to the diversity of visual elements, spatial clutter, and\nthe ambiguity of language. In this paper, we introduce DiMo-GUI, a\ntraining-free framework for GUI grounding that leverages two core strategies:\ndynamic visual grounding and modality-aware optimization. Instead of treating\nthe GUI as a monolithic image, our method splits the input into textual\nelements and iconic elements, allowing the model to reason over each modality\nindependently using general-purpose vision-language models. When predictions\nare ambiguous or incorrect, DiMo-GUI dynamically focuses attention by\ngenerating candidate focal regions centered on the model's initial predictions\nand incrementally zooms into subregions to refine the grounding result. This\nhierarchical refinement process helps disambiguate visually crowded layouts\nwithout the need for additional training or annotations. We evaluate our\napproach on standard GUI grounding benchmarks and demonstrate consistent\nimprovements over baseline inference pipelines, highlighting the effectiveness\nof combining modality separation with region-focused reasoning.", "comment": "8 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2507.00008v1", "categories": ["cs.AI", "cs.CV", "cs.HC"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2507.00008v1", "date": "2025-06-12", "updated": "2025-06-12"}
{"id": "2507.00002", "title": "Hypertokens: Holographic Associative Memory in Tokenized LLMs", "authors": ["Christopher James Augeri"], "summary": "Large Language Models (LLMs) exhibit remarkable capabilities but suffer from\napparent precision loss, reframed here as information spreading. This reframing\nshifts the problem from computational precision to an information-theoretic\ncommunication issue. We address the K:V and V:K memory problem in LLMs by\nintroducing HDRAM (Holographically Defined Random Access Memory), a symbolic\nmemory framework treating transformer latent space as a spread-spectrum\nchannel. Built upon hypertokens, structured symbolic codes integrating\nclassical error-correcting codes (ECC), holographic computing, and\nquantum-inspired search, HDRAM recovers distributed information through\nprincipled despreading. These phase-coherent memory addresses enable efficient\nkey-value operations and Grover-style search in latent space. By combining ECC\ngrammar with compressed sensing and Krylov subspace alignment, HDRAM\nsignificantly improves associative retrieval without architectural changes,\ndemonstrating how Classical-Holographic-Quantum-inspired (CHQ) principles can\nfortify transformer architectures.", "comment": "preprint as accepted to https://qnlp.ai/ - Quantum AI and NLP\n  Conference 2025", "pdf_url": "http://arxiv.org/pdf/2507.00002v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00002v1", "date": "2025-06-02", "updated": "2025-06-02"}
{"id": "2507.00217", "title": "CrossPipe: Towards Optimal Pipeline Schedules for Cross-Datacenter Training", "authors": ["Tiancheng Chen", "Ales Kubicek", "Langwen Huang", "Torsten Hoefler"], "summary": "Training large language models (LLMs) now requires resources that exceed a\nsingle datacenter, making cross-datacenter strategies increasingly crucial. We\npresent CrossPipe, a framework designed to optimize model training across\ngeographically distributed datacenters by explicitly modeling and mitigating\nthe impact of network latency and limited bandwidth. It enables unified\nanalysis and optimization incorporating both pipeline parallelism (PP) and\nopportunities for overlapping data parallelism (DP) communication. CrossPipe\ngenerates optimized pipeline schedules using either solver-based optimal or\nfast near-optimal greedy algorithms, built upon a flexible execution engine\nthat separates scheduling logic from communication details. Our evaluation\nshows that CrossPipe reduces training time by up to 33.6\\% compared to\ntraditional pipeline schedules under identical memory constraints. When memory\nconstraints are relaxed, CrossPipe maintains strong performance despite\ncommunication delays, approaching the efficiency of idealized schedules without\ndelays. CrossPipe offers improved scalability and resource utilization,\nparticularly in environments with high network latency or limited bandwidth.", "comment": "USENIX ATC '25", "pdf_url": "http://arxiv.org/pdf/2507.00217v1", "categories": ["cs.DC"], "cate": "cs.DC", "url": "http://arxiv.org/abs/2507.00217v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00033", "title": "Moment Sampling in Video LLMs for Long-Form Video QA", "authors": ["Mustafa Chasmai", "Gauri Jagatap", "Gouthaman KV", "Grant Van Horn", "Subhransu Maji", "Andrea Fanelli"], "summary": "Recent advancements in video large language models (Video LLMs) have\nsignificantly advanced the field of video question answering (VideoQA). While\nexisting methods perform well on short videos, they often struggle with\nlong-range reasoning in longer videos. To scale Video LLMs for longer video\ncontent, frame sub-sampling (selecting frames at regular intervals) is commonly\nused. However, this approach is suboptimal, often leading to the loss of\ncrucial frames or the inclusion of redundant information from multiple similar\nframes. Missing key frames impairs the model's ability to answer questions\naccurately, while redundant frames lead the model to focus on irrelevant video\nsegments and increase computational resource consumption. In this paper, we\ninvestigate the use of a general-purpose text-to-video moment retrieval model\nto guide the frame sampling process. We propose \"moment sampling\", a novel,\nmodel-agnostic approach that enables the model to select the most relevant\nframes according to the context of the question. Specifically, we employ a\nlightweight moment retrieval model to prioritize frame selection. By focusing\non the frames most pertinent to the given question, our method enhances\nlong-form VideoQA performance in Video LLMs. Through extensive experiments on\nfour long-form VideoQA datasets, using four state-of-the-art Video LLMs, we\ndemonstrate the effectiveness of the proposed approach.", "comment": "Workshop on Video Large Language Models (VidLLMs) at CVPR 2025", "pdf_url": "http://arxiv.org/pdf/2507.00033v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00033v1", "date": "2025-06-18", "updated": "2025-06-18"}
{"id": "2507.00007", "title": "Integrating Universal Generative AI Platforms in Educational Labs to Foster Critical Thinking and Digital Literacy", "authors": ["Vasiliy Znamenskiy", "Rafael Niyazov", "Joel Hernandez"], "summary": "This paper presents a new educational framework for integrating generative\nartificial intelligence (GenAI) platforms such as ChatGPT, Claude, and Gemini\ninto laboratory activities aimed at developing critical thinking and digital\nliteracy among undergraduate students. Recognizing the limitations and risks of\nuncritical reliance on large language models (LLMs), the proposed pedagogical\nmodel reframes GenAI as a research subject and cognitive tool. Students\nformulate discipline-specific prompts and evaluate GenAI-generated responses in\ntext, image, and video modalities. A pilot implementation in a general\nastronomy course for non-science majors demonstrated high levels of engagement\nand critical reflection, with many students continuing the activity after class\nand presenting results at a research symposium. The results highlight the\nimportance of structured AI interactions in education and suggest that GenAI\ncan improve learning outcomes when combined with reflective assessment methods.\nThe study proposes a replicable model for interdisciplinary AI-integrated lab\nwork, adaptable to scientific disciplines. See the guide to learning activities\nbased on Generative-Ai platforms: https://doi.org/10.5281/zenodo.15555802", "comment": "http://doi.org/10.5121/ijci.2025.140302", "pdf_url": "http://arxiv.org/pdf/2507.00007v1", "categories": ["cs.CY", "cs.AI", "cs.LG", "68T50, 68U20, 97U50, 97D40", "I.2.7; K.3.1; K.3.2; H.5.3"], "cate": "cs.CY", "url": "http://arxiv.org/abs/2507.00007v1", "date": "2025-06-11", "updated": "2025-06-11"}
{"id": "2507.00051", "title": "Real-Time Guidewire Tip Tracking Using a Siamese Network for Image-Guided Endovascular Procedures", "authors": ["Tianliang Yao", "Zhiqiang Pei", "Yong Li", "Yixuan Yuan", "Peng Qi"], "summary": "An ever-growing incorporation of AI solutions into clinical practices\nenhances the efficiency and effectiveness of healthcare services. This paper\nfocuses on guidewire tip tracking tasks during image-guided therapy for\ncardiovascular diseases, aiding physicians in improving diagnostic and\ntherapeutic quality. A novel tracking framework based on a Siamese network with\ndual attention mechanisms combines self- and cross-attention strategies for\nrobust guidewire tip tracking. This design handles visual ambiguities, tissue\ndeformations, and imaging artifacts through enhanced spatial-temporal feature\nlearning. Validation occurred on 3 randomly selected clinical digital\nsubtraction angiography (DSA) sequences from a dataset of 15 sequences,\ncovering multiple interventional scenarios. The results indicate a mean\nlocalization error of 0.421 $\\pm$ 0.138 mm, with a maximum error of 1.736 mm,\nand a mean Intersection over Union (IoU) of 0.782. The framework maintains an\naverage processing speed of 57.2 frames per second, meeting the temporal\ndemands of endovascular imaging. Further validations with robotic platforms for\nautomating diagnostics and therapies in clinical routines yielded tracking\nerrors of 0.708 $\\pm$ 0.695 mm and 0.148 $\\pm$ 0.057 mm in two distinct\nexperimental scenarios.", "comment": "This paper has been accepted by Advanced Intelligent Systems", "pdf_url": "http://arxiv.org/pdf/2507.00051v1", "categories": ["eess.IV", "cs.CV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2507.00051v1", "date": "2025-06-25", "updated": "2025-06-25"}
{"id": "2507.00081", "title": "State and Memory is All You Need for Robust and Reliable AI Agents", "authors": ["Matthew Muhoberac", "Atharva Parikh", "Nirvi Vakharia", "Saniya Virani", "Aco Radujevic", "Savannah Wood", "Meghav Verma", "Dimitri Metaxotos", "Jeyaraman Soundararajan", "Thierry Masquelin", "Alexander G. Godfrey", "Sean Gardner", "Dobrila Rudnicki", "Sam Michael", "Gaurav Chopra"], "summary": "Large language models (LLMs) have enabled powerful advances in natural\nlanguage understanding and generation. Yet their application to complex,\nreal-world scientific workflows remain limited by challenges in memory,\nplanning, and tool integration. Here, we introduce SciBORG (Scientific Bespoke\nArtificial Intelligence Agents Optimized for Research Goals), a modular agentic\nframework that allows LLM-based agents to autonomously plan, reason, and\nachieve robust and reliable domain-specific task execution. Agents are\nconstructed dynamically from source code documentation and augmented with\nfinite-state automata (FSA) memory, enabling persistent state tracking and\ncontext-aware decision-making. This approach eliminates the need for manual\nprompt engineering and allows for robust, scalable deployment across diverse\napplications via maintaining context across extended workflows and to recover\nfrom tool or execution failures. We validate SciBORG through integration with\nboth physical and virtual hardware, such as microwave synthesizers for\nexecuting user-specified reactions, with context-aware decision making and\ndemonstrate its use in autonomous multi-step bioassay retrieval from the\nPubChem database utilizing multi-step planning, reasoning, agent-to-agent\ncommunication and coordination for execution of exploratory tasks. Systematic\nbenchmarking shows that SciBORG agents achieve reliable execution, adaptive\nplanning, and interpretable state transitions. Our results show that memory and\nstate awareness are critical enablers of agentic planning and reliability,\noffering a generalizable foundation for deploying AI agents in complex\nenvironments.", "comment": "5 Main Figures, 10 Extended Data Figures (37 Pages) for Manuscript ;\n  9 Supplementary Tables, 40 Supplementary Figures (180 Pages) for Supporting\n  Information", "pdf_url": "http://arxiv.org/pdf/2507.00081v1", "categories": ["cs.MA", "cs.AI", "cs.CL", "cs.ET", "physics.chem-ph"], "cate": "cs.MA", "url": "http://arxiv.org/abs/2507.00081v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00066", "title": "InSight-R: A Framework for Risk-informed Human Failure Event Identification and Interface-Induced Risk Assessment Driven by AutoGraph", "authors": ["Xingyu Xiao", "Jiejuan Tong", "Peng Chen", "Jun Sun", "Zhe Sui", "Jingang Liang", "Hongru Zhao", "Jun Zhao", "Haitao Wang"], "summary": "Human reliability remains a critical concern in safety-critical domains such\nas nuclear power, where operational failures are often linked to human error.\nWhile conventional human reliability analysis (HRA) methods have been widely\nadopted, they rely heavily on expert judgment for identifying human failure\nevents (HFEs) and assigning performance influencing factors (PIFs). This\nreliance introduces challenges related to reproducibility, subjectivity, and\nlimited integration of interface-level data. In particular, current approaches\nlack the capacity to rigorously assess how human-machine interface design\ncontributes to operator performance variability and error susceptibility. To\naddress these limitations, this study proposes a framework for risk-informed\nhuman failure event identification and interface-induced risk assessment driven\nby AutoGraph (InSight-R). By linking empirical behavioral data to the\ninterface-embedded knowledge graph (IE-KG) constructed by the automated\ngraph-based execution framework (AutoGraph), the InSight-R framework enables\nautomated HFE identification based on both error-prone and time-deviated\noperational paths. Furthermore, we discuss the relationship between\ndesigner-user conflicts and human error. The results demonstrate that InSight-R\nnot only enhances the objectivity and interpretability of HFE identification\nbut also provides a scalable pathway toward dynamic, real-time human\nreliability assessment in digitalized control environments. This framework\noffers actionable insights for interface design optimization and contributes to\nthe advancement of mechanism-driven HRA methodologies.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00066v1", "categories": ["cs.HC", "cs.AI"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2507.00066v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2507.00367", "title": "Presto: Hardware Acceleration of Ciphers for Hybrid Homomorphic Encryption", "authors": ["Yeonsoo Jeon", "Mattan Erez", "Michael Orshansky"], "summary": "Hybrid Homomorphic Encryption (HHE) combines symmetric key and homomorphic\nencryption to reduce ciphertext expansion crucial in client-server deployments\nof HE. Special symmetric ciphers, amenable to efficient HE evaluation, have\nbeen developed. Their client-side deployment calls for performant and\nenergy-efficient implementation, and in this paper we develop and evaluate\nhardware accelerators for the two known CKKS-targeting HHE ciphers, HERA and\nRubato.\n  We design vectorized and overlapped functional modules. The design exploits\ntransposition-invariance property of the MixColumns and MixRows function and\nalternates the order of intermediate state to eliminate bubbles in stream key\ngeneration, improving latency and throughput. We decouple the RNG and key\ncomputation phases to hide the latency of RNG and to reduce the critical path\nin FIFOs, achieving higher operating frequency.\n  We implement the accelerator on an AMD Virtex UltraScale+ FPGA. Both Rubato\nand HERA achieve a 6x improvement in throughput compared to the software\nimplementation. In terms of latency, Rubato achieves a 5x reduction, while HERA\nachieves a 3x reduction. Additionally, our hardware implementations reduce\nenergy consumption by 75x for Rubato and 47x for HERA compared to their\nsoftware implementation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00367v1", "categories": ["cs.AR", "cs.CR"], "cate": "cs.AR", "url": "http://arxiv.org/abs/2507.00367v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00423", "title": "Find a Scapegoat: Poisoning Membership Inference Attack and Defense to Federated Learning", "authors": ["Wenjin Mo", "Zhiyuan Li", "Minghong Fang", "Mingwei Fang"], "summary": "Federated learning (FL) allows multiple clients to collaboratively train a\nglobal machine learning model with coordination from a central server, without\nneeding to share their raw data. This approach is particularly appealing in the\nera of privacy regulations like the GDPR, leading many prominent companies to\nadopt it. However, FL's distributed nature makes it susceptible to poisoning\nattacks, where malicious clients, controlled by an attacker, send harmful data\nto compromise the model. Most existing poisoning attacks in FL aim to degrade\nthe model's integrity, such as reducing its accuracy, with limited attention to\nprivacy concerns from these attacks. In this study, we introduce FedPoisonMIA,\na novel poisoning membership inference attack targeting FL. FedPoisonMIA\ninvolves malicious clients crafting local model updates to infer membership\ninformation. Additionally, we propose a robust defense mechanism to mitigate\nthe impact of FedPoisonMIA attacks. Extensive experiments across various\ndatasets demonstrate the attack's effectiveness, while our defense approach\nreduces its impact to a degree.", "comment": "To appear in ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.00423v1", "categories": ["cs.CR", "cs.DC", "cs.LG"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2507.00423v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00352", "title": "An AST-guided LLM Approach for SVRF Code Synthesis", "authors": ["Abanoub E. Abdelmalak", "Mohamed A. Elsayed", "David Abercrombie", "Ilhami Torunoglu"], "summary": "Standard Verification Rule Format (SVRF) is essential for semiconductor\napplications like Design Rule Check (DRC), Layout Versus Schematic (LVS), and\nOptical Proximity Correction (OPC) and it faces challenges as advancing nodes\ncreate complex design rules that renders traditional SVRF development\nineffective and highlight an expertise gap. This paper introduces a novel\nmethodology integrating Abstract Syntax Tree (AST) embedding and\nRetrieval-Augmented Generation (RAG) for enhanced SVRF code synthesis, ensuring\nsemantic accuracy and error minimization through structural validation with\ndomain-specific insights for precise code generation.\n  We evaluate different T5-based models and propose an innovative SVRF-specific\nscoring framework that complements standard metrics like BLEU and ROUGE-L. In\nour approach, AST provides rigorous structural validation, while RAG infuses\nrelevant domain knowledge, effectively enhancing the code generation workflow.\n  Testing on a comprehensive benchmark of 740 DRC rule implementations, our\nmethodology demonstrates up to a 40\\% improvement in code generation accuracy\ncompared to basic text-based fine-tuning process. This fusion of industry\nexpertise with advanced coding strategies not only optimizes SVRF development\nunder limited dataset constraints but also creates a more intuitive and\nefficient coding environment. Consequently, users can rapidly iterate through\ndesign cycles, reduce manual error correction, and significantly improve\noverall productivity.", "comment": "9 Pages, 5 Figures, 2 Tables", "pdf_url": "http://arxiv.org/pdf/2507.00352v1", "categories": ["cs.SE", "cs.AI", "cs.ET"], "cate": "cs.SE", "url": "http://arxiv.org/abs/2507.00352v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00444", "title": "DiffCkt: A Diffusion Model-Based Hybrid Neural Network Framework for Automatic Transistor-Level Generation of Analog Circuits", "authors": ["Chengjie Liu", "Jiajia Li", "Yabing Feng", "Wenhao Huang", "Weiyu Chen", "Yuan Du", "Jun Yang", "Li Du"], "summary": "Analog circuit design consists of the pre-layout and layout phases. Among\nthem, the pre-layout phase directly decides the final circuit performance, but\nheavily depends on experienced engineers to do manual design according to\nspecific application scenarios. To overcome these challenges and automate the\nanalog circuit pre-layout design phase, we introduce DiffCkt: a diffusion\nmodel-based hybrid neural network framework for the automatic transistor-level\ngeneration of analog circuits, which can directly generate corresponding\ncircuit structures and device parameters tailored to specific performance\nrequirements. To more accurately quantify the efficiency of circuits generated\nby DiffCkt, we introduce the Circuit Generation Efficiency Index (CGEI), which\nis determined by both the figure of merit (FOM) of a single generated circuit\nand the time consumed. Compared with relative research, DiffCkt has improved\nCGEI by a factor of $2.21 \\sim 8365\\times$, reaching a state-of-the-art (SOTA)\nlevel. In conclusion, this work shows that the diffusion model has the\nremarkable ability to learn and generate analog circuit structures and device\nparameters, providing a revolutionary method for automating the pre-layout\ndesign of analog circuits. The circuit dataset will be open source, its preview\nversion is available at https://github.com/CjLiu-NJU/DiffCkt.", "comment": "Accepted by ICCAD2025", "pdf_url": "http://arxiv.org/pdf/2507.00444v1", "categories": ["cs.ET"], "cate": "cs.ET", "url": "http://arxiv.org/abs/2507.00444v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00600", "title": "A Practical Guide to Interpretable Role-Based Clustering in Multi-Layer Financial Networks", "authors": ["Christian Franssen", "Iman van Lelyveld", "Bernd Heidergott"], "summary": "Understanding the functional roles of financial institutions within\ninterconnected markets is critical for effective supervision, systemic risk\nassessment, and resolution planning. We propose an interpretable role-based\nclustering approach for multi-layer financial networks, designed to identify\nthe functional positions of institutions across different market segments. Our\nmethod follows a general clustering framework defined by proximity measures,\ncluster evaluation criteria, and algorithm selection. We construct explainable\nnode embeddings based on egonet features that capture both direct and indirect\ntrading relationships within and across market layers. Using transaction-level\ndata from the ECB's Money Market Statistical Reporting (MMSR), we demonstrate\nhow the approach uncovers heterogeneous institutional roles such as market\nintermediaries, cross-segment connectors, and peripheral lenders or borrowers.\nThe results highlight the flexibility and practical value of role-based\nclustering in analyzing financial networks and understanding institutional\nbehavior in complex market structures.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00600v1", "categories": ["cs.SI", "cs.LG"], "cate": "cs.SI", "url": "http://arxiv.org/abs/2507.00600v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00190", "title": "Rethink 3D Object Detection from Physical World", "authors": ["Satoshi Tanaka", "Koji Minoda", "Fumiya Watanabe", "Takamasa Horibe"], "summary": "High-accuracy and low-latency 3D object detection is essential for autonomous\ndriving systems. While previous studies on 3D object detection often evaluate\nperformance based on mean average precision (mAP) and latency, they typically\nfail to address the trade-off between speed and accuracy, such as 60.0 mAP at\n100 ms vs 61.0 mAP at 500 ms. A quantitative assessment of the trade-offs\nbetween different hardware devices and accelerators remains unexplored, despite\nbeing critical for real-time applications. Furthermore, they overlook the\nimpact on collision avoidance in motion planning, for example, 60.0 mAP leading\nto safer motion planning or 61.0 mAP leading to high-risk motion planning. In\nthis paper, we introduce latency-aware AP (L-AP) and planning-aware AP (P-AP)\nas new metrics, which consider the physical world such as the concept of time\nand physical constraints, offering a more comprehensive evaluation for\nreal-time 3D object detection. We demonstrate the effectiveness of our metrics\nfor the entire autonomous driving system using nuPlan dataset, and evaluate 3D\nobject detection models accounting for hardware differences and accelerators.\nWe also develop a state-of-the-art performance model for real-time 3D object\ndetection through latency-aware hyperparameter optimization (L-HPO) using our\nmetrics. Additionally, we quantitatively demonstrate that the assumption \"the\nmore point clouds, the better the recognition performance\" is incorrect for\nreal-time applications and optimize both hardware and model selection using our\nmetrics.", "comment": "15 pages, 10 figures", "pdf_url": "http://arxiv.org/pdf/2507.00190v1", "categories": ["cs.RO", "cs.CV"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2507.00190v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00337", "title": "Seeing Through the Fog: Empowering Mobile Devices to Expose and Mitigate RAN Buffer Effects on Delay-Sensitive Protocols", "authors": ["Yuxin Liu", "Tianyang Zhang", "Qiang Wu", "Ju Ren", "Kyle Jamieson", "Yaxiong Xie"], "summary": "Delay-based protocols rely on end-to-end delay measurements to detect network\ncongestion. However, in cellular networks, Radio Access Network (RAN) buffers\nintroduce significant delays unrelated to congestion, fundamentally challenging\nthese protocols' assumptions. We identify two major types of RAN buffers -\nretransmission buffers and uplink scheduling buffers - that can introduce\ndelays comparable to congestion-induced delays, severely degrading protocol\nperformance. We present CellNinjia, a software-based system providing real-time\nvisibility into RAN operations, and Gandalf, which leverages this visibility to\nsystematically handle RAN-induced delays. Unlike existing approaches that treat\nthese delays as random noise, Gandalf identifies specific RAN operations and\ncompensates for their effects. Our evaluation in commercial 4G LTE and 5G\nnetworks shows that Gandalf enables substantial performance improvements - up\nto 7.49x for Copa and 9.53x for PCC Vivace - without modifying the protocols'\ncore algorithms, demonstrating that delay-based protocols can realize their\nfull potential in cellular networks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00337v1", "categories": ["cs.NI"], "cate": "cs.NI", "url": "http://arxiv.org/abs/2507.00337v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00366", "title": "Wireless AI Evolution: From Statistical Learners to Electromagnetic-Guided Foundation Models", "authors": ["Jian Xiao", "Ji Wang", "Kunrui Cao", "Xingwang Li", "Zhao Chen", "Chau Yuen"], "summary": "While initial applications of artificial intelligence (AI) in wireless\ncommunications over the past decade have demonstrated considerable potential\nusing specialized models for targeted communication tasks, the revolutionary\ndemands of sixth-generation (6G) networks for holographic communications,\nubiquitous sensing, and native intelligence are propelling a necessary\nevolution towards AI-native wireless networks. The arrival of large AI models\npaves the way for the next phase of Wireless AI, driven by wireless foundation\nmodels (WFMs). In particular, pre-training on universal electromagnetic (EM)\nprinciples equips WFMs with the essential adaptability for a multitude of\ndemanding 6G applications. However, existing large AI models face critical\nlimitations, including pre-training strategies disconnected from EM-compliant\nconstraints leading to physically inconsistent predictions, a lack of embedded\nunderstanding of wave propagation physics, and the inaccessibility of massive\nlabeled datasets for comprehensive EM-aware training. To address these\nchallenges, this article presents an electromagnetic information theory-guided\nself-supervised pre-training (EIT-SPT) framework designed to systematically\ninject EM physics into WFMs. The EIT-SPT framework aims to infuse WFMs with\nintrinsic EM knowledge, thereby enhancing their physical consistency,\ngeneralization capabilities across varied EM landscapes, and overall data\nefficiency. Building upon the proposed EIT-SPT framework, this article first\nelaborates on diverse potential applications in 6G scenarios of WFMs, then\nvalidates the efficacy of the proposed framework through illustrative case\nstudies, and finally summarizes critical open research challenges and future\ndirections for WFMs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00366v1", "categories": ["cs.IT", "eess.SP", "math.IT"], "cate": "cs.IT", "url": "http://arxiv.org/abs/2507.00366v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00041", "title": "TalentMine: LLM-Based Extraction and Question-Answering from Multimodal Talent Tables", "authors": ["Varun Mannam", "Fang Wang", "Chaochun Liu", "Xin Chen"], "summary": "In talent management systems, critical information often resides in complex\ntabular formats, presenting significant retrieval challenges for conventional\nlanguage models. These challenges are pronounced when processing Talent\ndocumentation that requires precise interpretation of tabular relationships for\naccurate information retrieval and downstream decision-making. Current table\nextraction methods struggle with semantic understanding, resulting in poor\nperformance when integrated into retrieval-augmented chat applications. This\npaper identifies a key bottleneck - while structural table information can be\nextracted, the semantic relationships between tabular elements are lost,\ncausing downstream query failures. To address this, we introduce TalentMine, a\nnovel LLM-enhanced framework that transforms extracted tables into semantically\nenriched representations. Unlike conventional approaches relying on CSV or text\nlinearization, our method employs specialized multimodal reasoning to preserve\nboth structural and semantic dimensions of tabular data. Experimental\nevaluation across employee benefits document collections demonstrates\nTalentMine's superior performance, achieving 100% accuracy in query answering\ntasks compared to 0% for standard AWS Textract extraction and 40% for AWS\nTextract Visual Q&A capabilities. Our comparative analysis also reveals that\nthe Claude v3 Haiku model achieves optimal performance for talent management\napplications. The key contributions of this work include (1) a systematic\nanalysis of semantic information loss in current table extraction pipelines,\n(2) a novel LLM-based method for semantically enriched table representation,\n(3) an efficient integration framework for retrieval-augmented systems as\nend-to-end systems, and (4) comprehensive benchmarks on talent analytics tasks\nshowing substantial improvements across multiple categories.", "comment": "Submitted to KDD conference, workshop: Talent and Management\n  Computing (TMC 2025), https://tmcworkshop.github.io/2025/", "pdf_url": "http://arxiv.org/pdf/2507.00041v1", "categories": ["cs.AI", "cs.CV", "cs.IR"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2507.00041v1", "date": "2025-06-22", "updated": "2025-06-22"}
{"id": "2507.00003", "title": "Deciding When Not to Decide: Indeterminacy-Aware Intrusion Detection with NeutroSENSE", "authors": ["Eyhab Al-Masri"], "summary": "This paper presents NeutroSENSE, a neutrosophic-enhanced ensemble framework\nfor interpretable intrusion detection in IoT environments. By integrating\nRandom Forest, XGBoost, and Logistic Regression with neutrosophic logic, the\nsystem decomposes prediction confidence into truth (T), falsity (F), and\nindeterminacy (I) components, enabling uncertainty quantification and\nabstention. Predictions with high indeterminacy are flagged for review using\nboth global and adaptive, class-specific thresholds. Evaluated on the IoT-CAD\ndataset, NeutroSENSE achieved 97% accuracy, while demonstrating that\nmisclassified samples exhibit significantly higher indeterminacy (I = 0.62)\nthan correct ones (I = 0.24). The use of indeterminacy as a proxy for\nuncertainty enables informed abstention and targeted review-particularly\nvaluable in edge deployments. Figures and tables validate the correlation\nbetween I-scores and error likelihood, supporting more trustworthy,\nhuman-in-the-loop AI decisions. This work shows that neutrosophic logic\nenhances both accuracy and explainability, providing a practical foundation for\ntrust-aware AI in edge and fog-based IoT security systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00003v1", "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.NI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00003v1", "date": "2025-06-05", "updated": "2025-06-05"}
{"id": "2507.00418", "title": "Serving LLMs in HPC Clusters: A Comparative Study of Qualcomm Cloud AI 100 Ultra and High-Performance GPUs", "authors": ["Mohammad Firas Sada", "John J. Graham", "Elham E Khoda", "Mahidhar Tatineni", "Dmitry Mishin", "Rajesh K. Gupta", "Rick Wagner", "Larry Smarr", "Thomas A. DeFanti", "Frank Würthwein"], "summary": "This study presents a benchmarking analysis of the Qualcomm Cloud AI 100\nUltra (QAic) accelerator for large language model (LLM) inference, evaluating\nits energy efficiency (throughput per watt) and performance against leading\nNVIDIA (A100, H200) and AMD (MI300A) GPUs within the National Research Platform\n(NRP) ecosystem. A total of 15 open-source LLMs, ranging from 117 million to 90\nbillion parameters, are served using the vLLM framework. The QAic inference\ncards appears to be energy efficient and performs well in the energy efficiency\nmetric in most cases. The findings offer insights into the potential of the\nQualcomm Cloud AI 100 Ultra for high-performance computing (HPC) applications\nwithin the National Research Platform (NRP).", "comment": "To appear in Proceedings of the Practice and Experience in Advanced\n  Research Computing (PEARC '25)", "pdf_url": "http://arxiv.org/pdf/2507.00418v1", "categories": ["cs.DC", "cs.AI"], "cate": "cs.DC", "url": "http://arxiv.org/abs/2507.00418v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00042", "title": "Catastrophic Forgetting Mitigation via Discrepancy-Weighted Experience Replay", "authors": ["Xinrun Xu", "Jianwen Yang", "Qiuhong Zhang", "Zhanbiao Lian", "Zhiming Ding", "Shan Jiang"], "summary": "Continually adapting edge models in cloud-edge collaborative object detection\nfor traffic monitoring suffers from catastrophic forgetting, where models lose\npreviously learned knowledge when adapting to new data distributions. This is\nespecially problematic in dynamic traffic environments characterised by\nperiodic variations (e.g., day/night, peak hours), where past knowledge remains\nvaluable. Existing approaches like experience replay and visual prompts offer\nsome mitigation, but struggle to effectively prioritize and leverage historical\ndata for optimal knowledge retention and adaptation. Specifically, simply\nstoring and replaying all historical data can be inefficient, while treating\nall historical experiences as equally important overlooks their varying\nrelevance to the current domain. This paper proposes ER-EMU, an edge model\nupdate algorithm based on adaptive experience replay, to address these\nlimitations. ER-EMU utilizes a limited-size experience buffer managed using a\nFirst-In-First-Out (FIFO) principle, and a novel Domain Distance Metric-based\nExperience Selection (DDM-ES) algorithm. DDM-ES employs the multi-kernel\nmaximum mean discrepancy (MK-MMD) to quantify the dissimilarity between target\ndomains, prioritizing the selection of historical data that is most dissimilar\nto the current target domain. This ensures training diversity and facilitates\nthe retention of knowledge from a wider range of past experiences, while also\npreventing overfitting to the new domain. The experience buffer is also updated\nusing a simple random sampling strategy to maintain a balanced representation\nof previous domains. Experiments on the Bellevue traffic video dataset,\ninvolving repeated day/night cycles, demonstrate that ER-EMU consistently\nimproves the performance of several state-of-the-art cloud-edge collaborative\nobject detection frameworks.", "comment": "ICANN 2025", "pdf_url": "http://arxiv.org/pdf/2507.00042v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00042v1", "date": "2025-06-23", "updated": "2025-06-23"}
{"id": "2507.00032", "title": "Ken Utilization Layer: Hebbian Replay Within a Student's Ken for Adaptive Knowledge Tracing", "authors": ["Grey Kuling", "Marinka Zitnik"], "summary": "We introduce KUL-KT, a biologically inspired architecture for knowledge\ntracing (KT), combining Hebbian memory encoding with gradient-based\nconsolidation in a scalable, input-agnostic framework. KUL-KT adapts the\nprinciple of memory consolidation in neural systems, to student modeling by\nintroducing two key innovations: (i) a time-decaying Hebbian memory update that\nenables graceful forgetting, and (ii) a novel Loss-aligned Internal Target\n(LIT) method to compute an ideal internal state, allowing continual learning\nwithout backpropagation through time. The architecture consists of a fast\nHebbian memory that captures each learner interaction via a single associative\nupdate, and a slower linear network that consolidates recalled samples through\ngradient descent. This design enables few-shot personalization and natural\nforgetting without storing raw data or relying on large cohort training.\nOperating entirely in embedding space, KUL-KT supports both structured\n(tabular) and unstructured (short-answer) inputs. Empirically, KUL-KT\noutperforms strong baselines on ten public KT benchmarks in rank-sensitive\nmetrics such as nDCG and Recall@10. In a classroom deployment, KUL-KT\npersonalized quizzes from short-answer data, leading to improved\nlearner-perceived helpfulness and reduced difficulty (p < 0.05). Ablation\nstudies confirm that Hebbian decay and LIT are critical for continual\nadaptation. Compared to a strong graph-based KT model, KUL-KT trains 1.75x\nfaster and uses 99.01\\% less memory. These results position KUL-KT as a\nbiologically grounded, memory-efficient, and input-flexible framework for\npersonalized learning at scale.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00032v1", "categories": ["cs.CY", "cs.AI", "cs.LG", "cs.NE"], "cate": "cs.CY", "url": "http://arxiv.org/abs/2507.00032v1", "date": "2025-06-18", "updated": "2025-06-18"}
{"id": "2507.00185", "title": "Multimodal, Multi-Disease Medical Imaging Foundation Model (MerMED-FM)", "authors": ["Yang Zhou", "Chrystie Wan Ning Quek", "Jun Zhou", "Yan Wang", "Yang Bai", "Yuhe Ke", "Jie Yao", "Laura Gutierrez", "Zhen Ling Teo", "Darren Shu Jeng Ting", "Brian T. Soetikno", "Christopher S. Nielsen", "Tobias Elze", "Zengxiang Li", "Linh Le Dinh", "Lionel Tim-Ee Cheng", "Tran Nguyen Tuan Anh", "Chee Leong Cheng", "Tien Yin Wong", "Nan Liu", "Iain Beehuat Tan", "Tony Kiat Hon Lim", "Rick Siow Mong Goh", "Yong Liu", "Daniel Shu Wei Ting"], "summary": "Current artificial intelligence models for medical imaging are predominantly\nsingle modality and single disease. Attempts to create multimodal and\nmulti-disease models have resulted in inconsistent clinical accuracy.\nFurthermore, training these models typically requires large, labour-intensive,\nwell-labelled datasets. We developed MerMED-FM, a state-of-the-art multimodal,\nmulti-specialty foundation model trained using self-supervised learning and a\nmemory module. MerMED-FM was trained on 3.3 million medical images from over\nten specialties and seven modalities, including computed tomography (CT), chest\nX-rays (CXR), ultrasound (US), pathology patches, color fundus photography\n(CFP), optical coherence tomography (OCT) and dermatology images. MerMED-FM was\nevaluated across multiple diseases and compared against existing foundational\nmodels. Strong performance was achieved across all modalities, with AUROCs of\n0.988 (OCT); 0.982 (pathology); 0.951 (US); 0.943 (CT); 0.931 (skin); 0.894\n(CFP); 0.858 (CXR). MerMED-FM has the potential to be a highly adaptable,\nversatile, cross-specialty foundation model that enables robust medical imaging\ninterpretation across diverse medical disciplines.", "comment": "42 pages, 3 composite figures, 4 tables", "pdf_url": "http://arxiv.org/pdf/2507.00185v1", "categories": ["eess.IV", "cs.AI", "cs.CV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2507.00185v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00491", "title": "Twill: Scheduling Compound AI Systems on Heterogeneous Mobile Edge Platforms", "authors": ["Zain Taufique", "Aman Vyas", "Antonio Miele", "Pasi Liljeberg", "Anil Kanduri"], "summary": "Compound AI (cAI) systems chain multiple AI models to solve complex problems.\ncAI systems are typically composed of deep neural networks (DNNs),\ntransformers, and large language models (LLMs), exhibiting a high degree of\ncomputational diversity and dynamic workload variation. Deploying cAI services\non mobile edge platforms poses a significant challenge in scheduling concurrent\nDNN-transformer inference tasks, which arrive dynamically in an unknown\nsequence. Existing mobile edge AI inference strategies manage multi-DNN or\ntransformer-only workloads, relying on design-time profiling, and cannot handle\nconcurrent inference of DNNs and transformers required by cAI systems. In this\nwork, we address the challenge of scheduling cAI systems on heterogeneous\nmobile edge platforms. We present Twill, a run-time framework to handle\nconcurrent inference requests of cAI workloads through task affinity-aware\ncluster mapping and migration, priority-aware task freezing/unfreezing, and\nDVFS, while minimizing inference latency within power budgets. We implement and\ndeploy our Twill framework on the Nvidia Jetson Orin NX platform. We evaluate\nTwill against state-of-the-art edge AI inference techniques over contemporary\nDNNs and LLMs, reducing inference latency by 54% on average, while honoring\npower budgets.", "comment": "9 Pages, 9 Figures, Accepted in International Conference on\n  Computer-Aided Design (ICCAD) 2025", "pdf_url": "http://arxiv.org/pdf/2507.00491v1", "categories": ["cs.MA", "cs.AI", "cs.CV", "cs.PF"], "cate": "cs.MA", "url": "http://arxiv.org/abs/2507.00491v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00161", "title": "Designing an Adaptive Storytelling Platform to Promote Civic Education in Politically Polarized Learning Environments", "authors": ["Christopher M. Wegemer", "Edward Halim", "Jeff Burke"], "summary": "Political polarization undermines democratic civic education by exacerbating\nidentity-based resistance to opposing viewpoints. Emerging AI technologies\noffer new opportunities to advance interventions that reduce polarization and\npromote political open-mindedness. We examined novel design strategies that\nleverage adaptive and emotionally-responsive civic narratives that may sustain\nstudents' emotional engagement in stories, and in turn, promote\nperspective-taking toward members of political out-groups. Drawing on theories\nfrom political psychology and narratology, we investigate how affective\ncomputing techniques can support three storytelling mechanisms: transportation\ninto a story world, identification with characters, and interaction with the\nstoryteller. Using a design-based research (DBR) approach, we iteratively\ndeveloped and refined an AI-mediated Digital Civic Storytelling (AI-DCS)\nplatform. Our prototype integrates facial emotion recognition and attention\ntracking to assess users' affective and attentional states in real time.\nNarrative content is organized around pre-structured story outlines, with\nbeat-by-beat language adaptation implemented via GPT-4, personalizing\nlinguistic tone to sustain students' emotional engagement in stories that\ncenter political perspectives different from their own. Our work offers a\nfoundation for AI-supported, emotionally-sensitive strategies that address\naffective polarization while preserving learner autonomy. We conclude with\nimplications for civic education interventions, algorithmic literacy, and HCI\nchallenges associated with AI dialogue management and affect-adaptive learning\nenvironments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00161v1", "categories": ["cs.HC", "cs.AI"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2507.00161v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00642", "title": "ChatHLS: Towards Systematic Design Automation and Optimization for High-Level Synthesis", "authors": ["Runkai Li", "Jia Xiong", "Xiuyuan He", "Jieru Zhao", "Qiang Xu", "Xi Wang"], "summary": "The increasing complexity of computational demands has accelerated the\nadoption of domain-specific accelerators, yet traditional hardware design\nmethodologies remain constrained by prolonged development and verification\ncycles. High-Level Synthesis (HLS) bridges the gap between software and\nhardware by enabling hardware design from high-level programming languages.\nHowever, its widespread adoption is hindered by strict coding constraints and\nintricate hardware-specific optimizations, creating significant obstacles for\ndevelopers. Recent advancements in Large Language Models (LLMs) demonstrate\nsubstantial potential in hardware design automation. However, their\neffectiveness is limited by the scarcity of high-quality datasets, particularly\nin the context of HLS. To address these challenges, we introduce ChatHLS, an\nagile HLS design automation and optimization workflow that leverages fine-tuned\nLLMs integrated within a multi-agent framework for error correction and design\noptimization. Our extensive evaluations reveal that ChatHLS achieves an average\nrepair pass rate of 82.7% over 612 test cases, outperforming the GPT-4o and\nLlama3-8B by 19.1% and 63.0%, respectively. Furthermore, ChatHLS delivers\nperformance enhancements ranging from 1.9$\\times$ to 14.8$\\times$ upon\nresource-constrained kernels. By enabling sophisticated optimization reasoning\nwithin practical computational budgets, ChatHLS attains a 4.9$\\times$ geometric\nmean speedup compared to state-of-the-art DSL-based approaches. These results\nunderscore the potential of ChatHLS in substantially expediting hardware\ndevelopment cycles while maintaining rigorous standards of design reliability\nand optimization quality.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00642v1", "categories": ["cs.AR"], "cate": "cs.AR", "url": "http://arxiv.org/abs/2507.00642v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00006", "title": "MVGBench: Comprehensive Benchmark for Multi-view Generation Models", "authors": ["Xianghui Xie", "Chuhang Zou", "Meher Gitika Karumuri", "Jan Eric Lenssen", "Gerard Pons-Moll"], "summary": "We propose MVGBench, a comprehensive benchmark for multi-view image\ngeneration models (MVGs) that evaluates 3D consistency in geometry and texture,\nimage quality, and semantics (using vision language models). Recently, MVGs\nhave been the main driving force in 3D object creation. However, existing\nmetrics compare generated images against ground truth target views, which is\nnot suitable for generative tasks where multiple solutions exist while\ndiffering from ground truth. Furthermore, different MVGs are trained on\ndifferent view angles, synthetic data and specific lightings -- robustness to\nthese factors and generalization to real data are rarely evaluated thoroughly.\nWithout a rigorous evaluation protocol, it is also unclear what design choices\ncontribute to the progress of MVGs. MVGBench evaluates three different aspects:\nbest setup performance, generalization to real data and robustness. Instead of\ncomparing against ground truth, we introduce a novel 3D self-consistency metric\nwhich compares 3D reconstructions from disjoint generated multi-views. We\nsystematically compare 12 existing MVGs on 4 different curated real and\nsynthetic datasets. With our analysis, we identify important limitations of\nexisting methods specially in terms of robustness and generalization, and we\nfind the most critical design choices. Using the discovered best practices, we\npropose ViFiGen, a method that outperforms all evaluated MVGs on 3D\nconsistency. Our code, model, and benchmark suite will be publicly released.", "comment": "17 pages, 11 figures, 9 tables, project page:\n  https://virtualhumans.mpi-inf.mpg.de/MVGBench/", "pdf_url": "http://arxiv.org/pdf/2507.00006v1", "categories": ["cs.GR", "cs.LG", "eess.IV"], "cate": "cs.GR", "url": "http://arxiv.org/abs/2507.00006v1", "date": "2025-06-11", "updated": "2025-06-11"}
{"id": "2507.00539", "title": "Ensemble Kalman Filter for Data Assimilation coupled with low-resolution computations techniques applied in Fluid Dynamics", "authors": ["Paul Jeanney", "Ashton Hetherington", "Shady E. Ahmed", "David Lanceta", "Susana Saiz", "José Miguel Perez", "Soledad Le CLainche"], "summary": "This paper presents an innovative Reduced-Order Model (ROM) for merging\nexperimental and simulation data using Data Assimilation (DA) to estimate the\n\"True\" state of a fluid dynamics system, leading to more accurate predictions.\nOur methodology introduces a novel approach implementing the Ensemble Kalman\nFilter (EnKF) within a reduced-dimensional framework, grounded in a robust\ntheoretical foundation and applied to fluid dynamics. To address the\nsubstantial computational demands of DA, the proposed ROM employs\nlow-resolution (LR) techniques to drastically reduce computational costs. This\napproach involves downsampling datasets for DA computations, followed by an\nadvanced reconstruction technique based on low-cost Singular Value\nDecomposition (lcSVD). The lcSVD method, a key innovation in this paper, has\nnever been applied to DA before and offers a highly efficient way to enhance\nresolution with minimal computational resources. Our results demonstrate\nsignificant reductions in both computation time and RAM usage through the LR\ntechniques without compromising the accuracy of the estimations. For instance,\nin a turbulent test case, the LR approach with a compression rate of 15.9 can\nachieve a speed-up of 13.7 and a RAM compression of 90.9% while maintaining a\nlow Relative Root Mean Square Error (RRMSE) of 2.6%, compared to 0.8% in the\nhigh-resolution (HR) reference. Furthermore, we highlight the effectiveness of\nthe EnKF in estimating and predicting the state of fluid flow systems based on\nlimited observations and low-fidelity numerical data. This paper highlights the\npotential of the proposed DA method in fluid dynamics applications,\nparticularly for improving computational efficiency in CFD and related fields.\nIts ability to balance accuracy with low computational and memory costs makes\nit suitable for large-scale and real-time applications, such as environmental\nmonitoring or aerospace.", "comment": "article, 49 pages, 29 figures, 4 tables", "pdf_url": "http://arxiv.org/pdf/2507.00539v1", "categories": ["cs.CE", "physics.flu-dyn", "62M20 (Primary) 65F30, 65C20, 76M12 (Secondary)", "G.1.3; G.3; I.6.3; G.1.10"], "cate": "cs.CE", "url": "http://arxiv.org/abs/2507.00539v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00409", "title": "Eilenberg correspondence for Stone recognition", "authors": ["Jorge Almeida", "Ondřej Klíma"], "summary": "We develop and explore the idea of recognition of languages (in the general\nsense of subsets of topological algebras) as preimages of clopen sets under\ncontinuous homomorphisms into Stone topological algebras. We obtain an\nEilenberg correspondence between varieties of languages and varieties of\nordered Stone topological algebras and a Birkhoff/Reiterman-type theorem\nshowing that the latter may me defined by certain pseudo-inequalities. In the\ncase of classical formal languages, of words over a finite alphabet, we also\nshow how this extended framework goes beyond the class of regular languages by\nworking with Stone completions of minimal automata, viewed as unary algebras.\nThis leads to a general method for showing that a language does not belong to a\nvariety of languages, expressed in terms of sequences of pairs of words, which\nis illustrated when the class consists of all finite intersections of\ncontext-free languages.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00409v1", "categories": ["cs.FL", "46H05 (Primary) 06E15, 08A62 (Secondary)"], "cate": "cs.FL", "url": "http://arxiv.org/abs/2507.00409v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00196", "title": "A Simple Algorithm for Trimmed Multipoint Evaluation", "authors": ["Nick Fischer", "Melvin Kallmayer", "Leo Wennmann"], "summary": "Evaluating a polynomial on a set of points is a fundamental task in computer\nalgebra. In this work, we revisit a particular variant called trimmed\nmultipoint evaluation: given an $n$-variate polynomial with bounded individual\ndegree $d$ and total degree $D$, the goal is to evaluate it on a natural class\nof input points. This problem arises as a key subroutine in recent algorithmic\nresults [Dinur; SODA '21], [Dell, Haak, Kallmayer, Wennmann; SODA '25]. It is\nknown that trimmed multipoint evaluation can be solved in near-linear time [van\nder Hoeven, Schost; AAECC '13] by a clever yet somewhat involved algorithm. We\ngive a simple recursive algorithm that avoids heavy computer-algebraic\nmachinery, and can be readily understood by researchers without specialized\nbackground.", "comment": "To appear at ESA 25", "pdf_url": "http://arxiv.org/pdf/2507.00196v1", "categories": ["cs.DS"], "cate": "cs.DS", "url": "http://arxiv.org/abs/2507.00196v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00522", "title": "Cyber Attacks Detection, Prevention, and Source Localization in Digital Substation Communication using Hybrid Statistical-Deep Learning", "authors": ["Nicola Cibin", "Bas Mulder", "Herman Carstens", "Peter Palensky", "Alexandru Ştefanov"], "summary": "The digital transformation of power systems is accelerating the adoption of\nIEC 61850 standard. However, its communication protocols, including Sampled\nValues (SV), lack built-in security features such as authentication and\nencryption, making them vulnerable to malicious packet injection. Such cyber\nattacks can delay fault clearance or trigger unintended circuit breaker\noperations. While most existing research focuses on detecting cyber attacks in\ndigital substations, intrusion prevention systems have been disregarded because\nof the risk of potential communication network disruptions. This paper proposes\na novel method using hybrid statistical-deep learning for the detection,\nprevention, and source localization of IEC 61850 SV injection attacks. The\nmethod uses exponentially modified Gaussian distributions to model\ncommunication network latency and long short-term memory and Elman recurrent\nneural network to detect anomalous variations in the estimated probability\ndistributions. It effectively discards malicious SV frames with minimal\nprocessing overhead and latency, maintains robustness against communication\nnetwork latency variation and time-synchronization issues, and guarantees a\nnear-zero false positive rate in non-attack scenarios. Comprehensive validation\nis conducted on three testbeds involving industrial-grade devices,\nhardware-in-the-loop simulations, virtualized intelligent electronic devices\nand merging units, and high-fidelity emulated communication networks. Results\ndemonstrate the method's suitability for practical deployment in IEC\n61850-compliant digital substations.", "comment": "10 pages, 6 figures. This work has been submitted to the IEEE for\n  possible publication", "pdf_url": "http://arxiv.org/pdf/2507.00522v1", "categories": ["cs.CR", "cs.SY", "eess.SY"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2507.00522v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00378", "title": "iPanda: An Intelligent Protocol Testing and Debugging Agent for Conformance Testing", "authors": ["Xikai Sun", "Fan Dang", "Kebin Liu", "Xin Miao", "Zihao Yang", "Haimo Lu", "Yawen Zheng", "Yunhao Liu"], "summary": "Conformance testing is essential for ensuring that protocol implementations\ncomply with their specifications. However, traditional testing approaches\ninvolve manually creating numerous test cases and scripts, making the process\nlabor-intensive and inefficient. Recently, Large Language Models (LLMs) have\ndemonstrated impressive text comprehension and code generation abilities,\nproviding promising opportunities for automation. In this paper, we propose\niPanda, the first end-to-end framework that leverages LLMs to automate protocol\nconformance testing. Given a protocol specification document and its\nimplementation, iPanda first employs a keyword-based method to automatically\ngenerate comprehensive test cases. Then, it utilizes a code-based\nretrieval-augmented generation approach to effectively interpret the\nimplementation and produce executable test code. To further enhance code\nquality, iPanda incorporates an iterative self-correction mechanism to refine\ngenerated test scripts interactively. Finally, by executing and analyzing the\ngenerated tests, iPanda systematically verifies compliance between\nimplementations and protocol specifications. Comprehensive experiments on\nvarious protocols show that iPanda significantly outperforms pure LLM-based\napproaches, improving the success rate (Pass@1) of test-code generation by\nfactors ranging from 4.675 times to 10.751 times.", "comment": "14 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2507.00378v1", "categories": ["cs.SE", "cs.AI"], "cate": "cs.SE", "url": "http://arxiv.org/abs/2507.00378v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00710", "title": "Robust Task Offloading for UAV-enabled Secure MEC Against Aerial Eavesdropper", "authors": ["Can Cui", "ZIye Jia", "Chao Dong", "Qihui Wu"], "summary": "Unmanned aerial vehicles (UAVs) are recognized as a promising candidate for\nthe multi-access edge computing (MEC) in the future sixth generation\ncommunication networks. However, the aerial eavesdropping UAVs (EUAVs) pose a\nsignificant security threat to the data offloading. In this paper, we\ninvestigate a robust MEC scenario with multiple service UAVs (SUAVs) towards\nthe potential eavesdropping from the EUAV, in which the random parameters such\nas task complexities are considered in the practical applications. In detail,\nthe problem is formulated to optimize the deployment positions of SUAVs, the\nconnection relationships between GUs and SUAVs, and the offloading ratios. With\nthe uncertain task complexities, the corresponding chance constraints are\nconstructed under the uncertainty set, which is tricky to deal with. Therefore,\nwe first optimize the pre-deployment of SUAVs by the K-means algorithm. Then,\nthe distributionally robust optimization method is employed, and the\nconditional value at risk is utilized to transform the chance constraints into\nconvex forms, which can be solved via convex toolkits. Finally, the simulation\nresults show that with the consideration of uncertainties, just 5% more energy\nis consumed compared with the ideal circumstance, which verifies the robustness\nof the proposed algorithms.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00710v1", "categories": ["cs.ET"], "cate": "cs.ET", "url": "http://arxiv.org/abs/2507.00710v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00619", "title": "Gender Differences in International Research Collaboration in European Union", "authors": ["Elsa Fontainha", "Tanya Araújo"], "summary": "This paper investigates International Research Collaboration (IRC) among\nEuropean Union (EU) countries from 2011 to 2022, with emphasis on gender-based\nauthorship patterns. Drawing from the Web of Science Social Science Citation\nIndex (WoS-SSCI) database, a large dataset of IRC articles was constructed,\nannotated with categories of authorship based on gender, author affiliation,\nand COVID-19 subject as topic. Using network science, the study maps\ncollaboration structures and reveals gendered differences in co-authorship\nnetworks. Results highlight a substantial rise in IRC over the decade,\nparticularly with the USA and China as key non-EU partners. Articles with at\nleast one female author were consistently less frequent than those with at\nleast one male author. Notably, female-exclusive collaborations showed\ndistinctive network topologies, with more centralized (star-like) patterns and\nshorter tree diameters. The COVID-19 pandemic further reshaped collaboration\ndynamics, temporarily reducing the gender gap in IRC but also revealing\nvulnerabilities in female-dominated research networks. These findings\nunderscore both progress and persistent disparities in the gender dynamics of\nEU participation in IRC.", "comment": "29 pages, 10 figures", "pdf_url": "http://arxiv.org/pdf/2507.00619v1", "categories": ["cs.SI"], "cate": "cs.SI", "url": "http://arxiv.org/abs/2507.00619v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00236", "title": "Sim2Real Diffusion: Learning Cross-Domain Adaptive Representations for Transferable Autonomous Driving", "authors": ["Chinmay Vilas Samak", "Tanmay Vilas Samak", "Bing Li", "Venkat Krovi"], "summary": "Simulation-based design, optimization, and validation of autonomous driving\nalgorithms have proven to be crucial for their iterative improvement over the\nyears. Nevertheless, the ultimate measure of effectiveness is their successful\ntransition from simulation to reality (sim2real). However, existing sim2real\ntransfer methods struggle to comprehensively address the autonomy-oriented\nrequirements of balancing: (i) conditioned domain adaptation, (ii) robust\nperformance with limited examples, (iii) modularity in handling multiple domain\nrepresentations, and (iv) real-time performance. To alleviate these pain\npoints, we present a unified framework for learning cross-domain adaptive\nrepresentations for sim2real transferable autonomous driving algorithms using\nconditional latent diffusion models. Our framework offers options to leverage:\n(i) alternate foundation models, (ii) a few-shot fine-tuning pipeline, and\n(iii) textual as well as image prompts for mapping across given source and\ntarget domains. It is also capable of generating diverse high-quality samples\nwhen diffusing across parameter spaces such as times of day, weather\nconditions, seasons, and operational design domains. We systematically analyze\nthe presented framework and report our findings in the form of critical\nquantitative metrics and ablation studies, as well as insightful qualitative\nexamples and remarks. Additionally, we demonstrate the serviceability of the\nproposed approach in bridging the sim2real gap for end-to-end autonomous\ndriving using a behavioral cloning case study. Our experiments indicate that\nthe proposed framework is capable of bridging the perceptual sim2real gap by\nover 40%. We hope that our approach underscores the potential of generative\ndiffusion models in sim2real transfer, offering a pathway toward more robust\nand adaptive autonomous driving.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00236v1", "categories": ["cs.RO"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2507.00236v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00623", "title": "Remote Rendering for Virtual Reality: performance comparison of multimedia frameworks and protocols", "authors": ["Daniel Mejías", "Inhar Yeregui", "Roberto Viola", "Miguel Fernández", "Mario Montagud"], "summary": "The increasing complexity of Extended Reality (XR) applications demands\nsubstantial processing power and high bandwidth communications, often\nunavailable on lightweight devices. Remote rendering consists of offloading\nprocessing tasks to a remote node with a powerful GPU, delivering the rendered\ncontent to the end device. The delivery is usually performed through popular\nstreaming protocols such as Web Real-Time Communications (WebRTC), offering a\ndata channel for interactions, or Dynamic Adaptive Streaming over HTTP (DASH),\nbetter suitable for scalability. Moreover, new streaming protocols based on\nQUIC are emerging as potential replacements for WebRTC and DASH and offer\nbenefits like connection migration, stream multiplexing and multipath delivery.\nThis work describes the integration of the two most popular multimedia\nframeworks, GStreamer and FFmpeg, with a rendering engine acting as a Remote\nRenderer, and analyzes their performance when offering different protocols for\ndelivering the rendered content to the end device over WIFI or 5G. This\nsolution constitutes a beyond state-of-the-art testbed to conduct cutting-edge\nresearch in the XR field.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00623v1", "categories": ["cs.NI"], "cate": "cs.NI", "url": "http://arxiv.org/abs/2507.00623v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00388", "title": "Accuracy and Security-Guaranteed Participant Selection and Beamforming Design for RIS-Assisted Federated Learning", "authors": ["Mengru Wu", "Yu Gao", "Weidang Lu", "Huimei Han", "Lei Sun", "Wanli Ni"], "summary": "Federated learning (FL) has emerged as an effective approach for training\nneural network models without requiring the sharing of participants' raw data,\nthereby addressing data privacy concerns. In this paper, we propose a\nreconfigurable intelligent surface (RIS)-assisted FL framework in the presence\nof eavesdropping, where partial edge devices are selected to participate in the\nFL training process. In contrast, the remaining devices serve as cooperative\njammers by transmitting jamming signals to disrupt eavesdropping. We aim to\nminimize the training latency in each FL round by jointly optimizing\nparticipant selection, bandwidth allocation, and RIS beamforming design,\nsubject to the convergence accuracy of FL and the secure uploading\nrequirements. To solve the resulting mixed-integer nonlinear programming\nproblem, we propose a twin delayed deep deterministic policy gradient (TD3)\nalgorithm. Simulation results demonstrate that the proposed scheme reduces the\nFL training latency by approximately 27$\\%$ compared to baselines.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00388v1", "categories": ["cs.IT", "eess.SP", "math.IT"], "cate": "cs.IT", "url": "http://arxiv.org/abs/2507.00388v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00048", "title": "A collaborative digital twin built on FAIR data and compute infrastructure", "authors": ["Thomas M. Deucher", "Juan C. Verduzco", "Michael Titus", "Alejandro Strachan"], "summary": "The integration of machine learning with automated experimentation in\nself-driving laboratories (SDL) offers a powerful approach to accelerate\ndiscovery and optimization tasks in science and engineering applications. When\nsupported by findable, accessible, interoperable, and reusable (FAIR) data\ninfrastructure, SDLs with overlapping interests can collaborate more\neffectively. This work presents a distributed SDL implementation built on\nnanoHUB services for online simulation and FAIR data management. In this\nframework, geographically dispersed collaborators conducting independent\noptimization tasks contribute raw experimental data to a shared central\ndatabase. These researchers can then benefit from analysis tools and machine\nlearning models that automatically update as additional data become available.\nNew data points are submitted through a simple web interface and automatically\nprocessed using a nanoHUB Sim2L, which extracts derived quantities and indexes\nall inputs and outputs in a FAIR data repository called ResultsDB. A separate\nnanoHUB workflow enables sequential optimization using active learning, where\nresearchers define the optimization objective, and machine learning models are\ntrained on-the-fly with all existing data, guiding the selection of future\nexperiments. Inspired by the concept of ``frugal twin\", the optimization task\nseeks to find the optimal recipe to combine food dyes to achieve the desired\ntarget color. With easily accessible and inexpensive materials, researchers and\nstudents can set up their own experiments, share data with collaborators, and\nexplore the combination of FAIR data, predictive ML models, and sequential\noptimization. The tools introduced are generally applicable and can easily be\nextended to other optimization problems.", "comment": "10 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2507.00048v1", "categories": ["cs.AI", "cond-mat.mtrl-sci", "cs.CE", "cs.LG"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2507.00048v1", "date": "2025-06-24", "updated": "2025-06-24"}
{"id": "2507.00004", "title": "A Theory of Inference Compute Scaling: Reasoning through Directed Stochastic Skill Search", "authors": ["Austin R. Ellis-Mohr", "Anuj K. Nayak", "Lav R. Varshney"], "summary": "Large language models (LLMs) demand considerable computational, energy, and\nfinancial resources during both training and deployment. While scaling laws for\ntraining have guided much of the field's recent progress, inference costs now\nrepresent a significant and growing component of the overall resource burden,\nparticularly for reasoning-focused models. Existing characterizations of\ncompute-optimality that consider model size, dataset size, and inference tokens\nin isolation or in fixed combinations risk overlooking more efficient operating\npoints. We introduce directed stochastic skill search (DS3), a general\nframework that represents inference as stochastic traversal over a learned\nskill graph. From a simplified yet expressive instantiation, we derive\nclosed-form expressions for task success and compute cost across a wide range\nof inference strategies -- including chain-of-thought (CoT) and tree-of-thought\n(ToT) -- enabling comparative analysis as a function of task difficulty and\nmodel capability. To that end, we extend a prior first-principles tripartite\ngraph framework of LLM training to incorporate inference, and separately bridge\nDS3 with empirical methods that characterize LLM scaling behavior. We\ntheoretically recover empirically observed patterns, including: linear accuracy\nscaling with logarithmic compute; variation in preferred inference strategies\nas a function of task difficulty and model capability; emergent behavior\nelicited by reasoning even when performance plateaus under parameter scaling;\nand both best-of-N (BoN) and majority voting behavior captured within a unified\nanalytical framework. By explicitly characterizing training-inference\ninterdependencies, our framework deepens theoretical understanding and supports\nprincipled algorithmic design and resource allocation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00004v1", "categories": ["cs.LG", "cs.AI", "cs.CY", "cs.PF"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00004v1", "date": "2025-06-10", "updated": "2025-06-10"}
{"id": "2507.00428", "title": "Real-Time In-Network Machine Learning on P4-Programmable FPGA SmartNICs with Fixed-Point Arithmetic and Taylor", "authors": ["Mohammad Firas Sada", "John J. Graham", "Mahidhar Tatineni", "Dmitry Mishin", "Thomas A. DeFanti", "Frank Würthwein"], "summary": "As machine learning (ML) applications become integral to modern network\noperations, there is an increasing demand for network programmability that\nenables low-latency ML inference for tasks such as Quality of Service (QoS)\nprediction and anomaly detection in cybersecurity. ML models provide\nadaptability through dynamic weight adjustments, making Programming\nProtocol-independent Packet Processors (P4)-programmable FPGA SmartNICs an\nideal platform for investigating In-Network Machine Learning (INML). These\ndevices offer high-throughput, low-latency packet processing and can be\ndynamically reconfigured via the control plane, allowing for flexible\nintegration of ML models directly at the network edge. This paper explores the\napplication of the P4 programming paradigm to neural networks and regression\nmodels, where weights and biases are stored in control plane table lookups.\nThis approach enables flexible programmability and efficient deployment of\nretrainable ML models at the network edge, independent of core infrastructure\nat the switch level.", "comment": "To appear in Proceedings of the Practice and Experience in Advanced\n  Research Computing (PEARC25)", "pdf_url": "http://arxiv.org/pdf/2507.00428v1", "categories": ["cs.DC", "cs.NI"], "cate": "cs.DC", "url": "http://arxiv.org/abs/2507.00428v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00043", "title": "MR-CLIP: Efficient Metadata-Guided Learning of MRI Contrast Representations", "authors": ["Mehmet Yigit Avci", "Pedro Borges", "Paul Wright", "Mehmet Yigitsoy", "Sebastien Ourselin", "Jorge Cardoso"], "summary": "Accurate interpretation of Magnetic Resonance Imaging scans in clinical\nsystems is based on a precise understanding of image contrast. This contrast is\nprimarily governed by acquisition parameters, such as echo time and repetition\ntime, which are stored in the DICOM metadata. To simplify contrast\nidentification, broad labels such as T1-weighted or T2-weighted are commonly\nused, but these offer only a coarse approximation of the underlying acquisition\nsettings. In many real-world datasets, such labels are entirely missing,\nleaving raw acquisition parameters as the only indicators of contrast. Adding\nto this challenge, the available metadata is often incomplete, noisy, or\ninconsistent. The lack of reliable and standardized metadata complicates tasks\nsuch as image interpretation, retrieval, and integration into clinical\nworkflows. Furthermore, robust contrast-aware representations are essential to\nenable more advanced clinical applications, such as achieving\nmodality-invariant representations and data harmonization. To address these\nchallenges, we propose MR-CLIP, a multimodal contrastive learning framework\nthat aligns MR images with their DICOM metadata to learn contrast-aware\nrepresentations, without relying on manual labels. Trained on a diverse\nclinical dataset that spans various scanners and protocols, MR-CLIP captures\ncontrast variations across acquisitions and within scans, enabling\nanatomy-invariant representations. We demonstrate its effectiveness in\ncross-modal retrieval and contrast classification, highlighting its scalability\nand potential for further clinical applications. The code and weights are\npublicly available at https://github.com/myigitavci/MR-CLIP.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00043v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00043v1", "date": "2025-06-23", "updated": "2025-06-23"}
{"id": "2507.00108", "title": "Teaching Programming in the Age of Generative AI: Insights from Literature, Pedagogical Proposals, and Student Perspectives", "authors": ["Clemente Rubio-Manzano", "Jazna Meza", "Rodolfo Fernandez-Santibanez", "Christian Vidal-Castro"], "summary": "Computer programming is undergoing a true transformation driven by powerful\nnew tools for automatic source code generation based on large language models.\nThis transformation is also manifesting in introductory programming courses at\nuniversities around the world, generating an in-depth debate about how\nprogramming content should be taught, learned, and assessed in the context of\ngenerative artificial intelligence.\n  This article aims, on the one hand, to review the most relevant studies on\nthis issue, highlighting the advantages and disadvantages identified in the\nspecialized literature. On the other hand, it proposes enriching teaching and\nlearning methodologies by focusing on code comprehension and execution rather\nthan on mere coding or program functionality. In particular, it advocates for\nthe use of visual representations of code and visual simulations of its\nexecution as effective tools for teaching, learning, and assessing programming,\nthus fostering a deeper understanding among students.\n  Finally, the opinions of students who took the object-oriented programming\ncourse are presented to provide preliminary context supporting the\nincorporation of visual simulations in Java (or other languages) as part of the\ntraining process.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00108v1", "categories": ["cs.CY", "cs.AI", "cs.ET", "cs.PL"], "cate": "cs.CY", "url": "http://arxiv.org/abs/2507.00108v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00206", "title": "Towards 3D Semantic Image Synthesis for Medical Imaging", "authors": ["Wenwu Tang", "Khaled Seyam", "Bin Yang"], "summary": "In the medical domain, acquiring large datasets is challenging due to both\naccessibility issues and stringent privacy regulations. Consequently, data\navailability and privacy protection are major obstacles to applying machine\nlearning in medical imaging. To address this, our study proposes the Med-LSDM\n(Latent Semantic Diffusion Model), which operates directly in the 3D domain and\nleverages de-identified semantic maps to generate synthetic data as a method of\nprivacy preservation and data augmentation. Unlike many existing methods that\nfocus on generating 2D slices, Med-LSDM is designed specifically for 3D\nsemantic image synthesis, making it well-suited for applications requiring full\nvolumetric data. Med-LSDM incorporates a guiding mechanism that controls the 3D\nimage generation process by applying a diffusion model within the latent space\nof a pre-trained VQ-GAN. By operating in the compressed latent space, the model\nsignificantly reduces computational complexity while still preserving critical\n3D spatial details. Our approach demonstrates strong performance in 3D semantic\nmedical image synthesis, achieving a 3D-FID score of 0.0054 on the conditional\nDuke Breast dataset and similar Dice scores (0.70964) to those of real images\n(0.71496). These results demonstrate that the synthetic data from our model\nhave a small domain gap with real data and are useful for data augmentation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00206v1", "categories": ["eess.IV", "cs.CV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2507.00206v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00914", "title": "Large Language Model Powered Intelligent Urban Agents: Concepts, Capabilities, and Applications", "authors": ["Jindong Han", "Yansong Ning", "Zirui Yuan", "Hang Ni", "Fan Liu", "Tengfei Lyu", "Hao Liu"], "summary": "The long-standing vision of intelligent cities is to create efficient,\nlivable, and sustainable urban environments using big data and artificial\nintelligence technologies. Recently, the advent of Large Language Models (LLMs)\nhas opened new ways toward realizing this vision. With powerful semantic\nunderstanding and reasoning capabilities, LLMs can be deployed as intelligent\nagents capable of autonomously solving complex problems across domains. In this\narticle, we focus on Urban LLM Agents, which are LLM-powered agents that are\nsemi-embodied within the hybrid cyber-physical-social space of cities and used\nfor system-level urban decision-making. First, we introduce the concept of\nurban LLM agents, discussing their unique capabilities and features. Second, we\nsurvey the current research landscape from the perspective of agent workflows,\nencompassing urban sensing, memory management, reasoning, execution, and\nlearning. Third, we categorize the application domains of urban LLM agents into\nfive groups: urban planning, transportation, environment, public safety, and\nurban society, presenting representative works in each group. Finally, we\ndiscuss trustworthiness and evaluation issues that are critical for real-world\ndeployment, and identify several open problems for future research. This survey\naims to establish a foundation for the emerging field of urban LLM agents and\nto provide a roadmap for advancing the intersection of LLMs and urban\nintelligence. A curated list of relevant papers and open-source resources is\nmaintained and continuously updated at\nhttps://github.com/usail-hkust/Awesome-Urban-LLM-Agents.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00914v1", "categories": ["cs.MA", "cs.AI"], "cate": "cs.MA", "url": "http://arxiv.org/abs/2507.00914v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00198", "title": "Exploring AR Label Placements in Visually Cluttered Scenarios", "authors": ["Ji Hwan Park", "Braden Roper", "Amirhossein Arezoumand", "Tien Tran"], "summary": "We investigate methods for placing labels in AR environments that have\nvisually cluttered scenes. As the number of items increases in a scene within\nthe user' FOV, it is challenging to effectively place labels based on existing\nlabel placement guidelines. To address this issue, we implemented three label\nplacement techniques for in-view objects for AR applications. We specifically\ntarget a scenario, where various items of different types are scattered within\nthe user's field of view, and multiple items of the same type are situated\nclose together. We evaluate three placement techniques for three target tasks.\nOur study shows that using a label to spatially group the same types of items\nis beneficial for identifying, comparing, and summarizing data.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00198v1", "categories": ["cs.HC"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2507.00198v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00797", "title": "VEDA: Efficient LLM Generation Through Voting-based KV Cache Eviction and Dataflow-flexible Accelerator", "authors": ["Zhican Wang", "Hongxiang Fan", "Haroon Waris", "Gang Wang", "Zhenyu Li", "Jianfei Jiang", "Yanan Sun", "Guanghui He"], "summary": "Large Language Models (LLMs) excel in natural language processing tasks but\npose significant computational and memory challenges for edge deployment due to\ntheir intensive resource demands. This work addresses the efficiency of LLM\ninference by algorithm-hardware-dataflow tri-optimizations. We propose a novel\nvoting-based KV cache eviction algorithm, balancing hardware efficiency and\nalgorithm accuracy by adaptively identifying unimportant kv vectors. From a\ndataflow perspective, we introduce a flexible-product dataflow and a runtime\nreconfigurable PE array for matrix-vector multiplication. The proposed approach\neffectively handles the diverse dimensional requirements and solves the\nchallenges of incrementally varying sequence lengths. Additionally, an\nelement-serial scheduling scheme is proposed for nonlinear operations, such as\nsoftmax and layer normalization (layernorm). Results demonstrate a substantial\nreduction in latency, accompanied by a significant decrease in hardware\ncomplexity, from O(N) to O(1). The proposed solution is realized in a\ncustom-designed accelerator, VEDA, which outperforms existing hardware\nplatforms. This research represents a significant advancement in LLM inference\non resource-constrained edge devices, facilitating real-time processing,\nenhancing data privacy, and enabling model customization.", "comment": "DAC 2025", "pdf_url": "http://arxiv.org/pdf/2507.00797v1", "categories": ["cs.AR"], "cate": "cs.AR", "url": "http://arxiv.org/abs/2507.00797v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00412", "title": "ViscoReg: Neural Signed Distance Functions via Viscosity Solutions", "authors": ["Meenakshi Krishnan", "Ramani Duraiswami"], "summary": "Implicit Neural Representations (INRs) that learn a Signed Distance Function\n(SDF) are a powerful tool for continuous 3D scene reconstruction. These models\nare trained by enforcing the Eikonal equation. We demonstrate theoretically\nthat despite the ill-posedness of the Eikonal equation, generalization error\nestimates may be obtained for Neural SDFs in terms of the training error.\nHowever, training with the Eikonal loss can lead to unstable gradient flows,\nnecessitating alternate stabilization techniques. Traditional numerical solvers\nfor the equation have relied on viscosity approaches for regularization. We\nenhance Neural SDF training using this well-developed theory, and introduce a\nnew loss formulation we call ViscoReg. We theoretically demonstrate the\nstability of the gradient flow equation of our proposed loss term. Empirically,\nViscoReg outperforms state-of-the-art approaches such as SIREN, DiGS, and StEik\nwithout adding significant computational cost.", "comment": "14 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2507.00412v1", "categories": ["cs.GR"], "cate": "cs.GR", "url": "http://arxiv.org/abs/2507.00412v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00034", "title": "Data Collection with Non-Uniform Axial Power for Phase II of the OECD/NEA AI/ML Critical Heat Flux Benchmark", "authors": ["Reece Bourisaw", "Reid McCants", "Jean-Marie Le Corre", "Anna Iskhakova", "Arsen S. Iskhakov"], "summary": "Critical heat flux (CHF) marks the onset of boiling crisis in light-water\nreactors, defining safe thermal-hydraulic operating limits. To support Phase II\nof the OECD/NEA AI/ML CHF benchmark, which introduces spatially varying power\nprofiles, this work compiles and digitizes a broad CHF dataset covering both\nuniform and non-uniform axial heating conditions. Heating profiles were\nextracted from technical reports, interpolated onto a consistent axial mesh,\nvalidated via energy-balance checks, and encoded in machine-readable formats\nfor benchmark compatibility.\n  Classical CHF correlations exhibit substantial errors under uniform heating\nand degrade markedly when applied to non-uniform profiles, while modern tabular\nmethods offer improved but still imperfect predictions. A neural network\ntrained solely on uniform data performs well in that regime but fails to\ngeneralize to spatially varying scenarios, underscoring the need for models\nthat explicitly incorporate axial power distributions. By providing these\ncurated datasets and baseline modeling results, this study lays the groundwork\nfor advanced transfer-learning strategies, rigorous uncertainty quantification,\nand design-optimization efforts in the next phase of the CHF benchmark.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00034v1", "categories": ["cs.LG", "cs.CE"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00034v1", "date": "2025-06-18", "updated": "2025-06-18"}
{"id": "2507.00277", "title": "Lazy B-Trees", "authors": ["Casper Moldrup Rysgaard", "Sebastian Wild"], "summary": "Lazy search trees (Sandlund & Wild FOCS 2020, Sandlund & Zhang SODA 2022) are\nsorted dictionaries whose update and query performance smoothly interpolates\nbetween that of efficient priority queues and binary search trees -\nautomatically, depending on actual use; no adjustments are necessary to the\ndata structure to realize the cost savings. In this paper, we design lazy\nB-trees, a variant of lazy search trees suitable for external memory that\ngeneralizes the speedup of B-trees over binary search trees wrt. input/output\noperations to the same smooth interpolation regime.\n  A key technical difficulty to overcome is the lack of a (fully satisfactory)\nexternal variant of biased search trees, on which lazy search trees crucially\nrely. We give a construction for a subset of performance guarantees sufficient\nto realize external-memory lazy search trees, which we deem of independent\ninterest.\n  As one special case, lazy B-trees can be used as an external-memory priority\nqueue, in which case they are competitive with some tailor-made heaps; indeed,\nthey offer faster decrease-key and insert operations than known data\nstructures.", "comment": "MFCS 2025", "pdf_url": "http://arxiv.org/pdf/2507.00277v1", "categories": ["cs.DS", "cs.DB"], "cate": "cs.DS", "url": "http://arxiv.org/abs/2507.00277v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00270", "title": "EMSpice 2.1: A Coupled EM and IR Drop Analysis Tool with Joule Heating and Thermal Map Integration for VLSI Reliability", "authors": ["Subed Lamichhane", "Haotian Lu", "Sheldon X. -D. Tan"], "summary": "Electromigration (EM) remains a critical reliability concern in current and\nfuture copper-based VLSI circuits. As technology scales down, EM-induced IR\ndrop becomes increasingly severe. While several EM-aware IR drop analysis tools\nhave been proposed, few incorporate the real impact of temperature distribution\non both EM and IR drop effects. In this work, we introduce EMSpice 2.1, an\nenhanced tool built upon the existing coupled IR-EM analysis framework, EMSpice\n2.0, for EM-aware IR drop analysis. For the first time, EMSpice 2.1 uniquely\nintegrates Joule heating effects and practical thermal maps derived from actual\nchip conditions. Additionally, it features improved interoperability with\ncommercial EDA tools, facilitating more comprehensive EM and IR drop sign-off\nanalysis. Our findings demonstrate that specific hotspot patterns significantly\nimpact the lifetime of interconnects and overall chip reliability due to EM\nfailures. Furthermore, our tool exhibits strong agreement with\nindustry-standard tools such as COMSOL, achieving a speedup of over 200 times\nwhile maintaining high accuracy.", "comment": "4 Pages, accepted to SMACD 2025", "pdf_url": "http://arxiv.org/pdf/2507.00270v1", "categories": ["eess.SY", "cs.SY"], "cate": "eess.SY", "url": "http://arxiv.org/abs/2507.00270v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00294", "title": "Fast Simulation of Damage Diffusion Distribution in Scanning Transmission Electron Microscopy", "authors": ["Amir Javadi Rad", "Amirafshar Moshtaghpour", "Dongdong Chen", "Angus I. Kirkland"], "summary": "Scanning Transmission Electron Microscopy (STEM) is a critical tool for\nimaging the properties of materials and biological specimens at atomic scale,\nyet our understanding of relevant electron beam damage mechanisms is\nincomplete. Recent studies suggest that certain types of damage can be modelled\nas a diffusion process. However, numerical simulation of such diffusion\nprocesses has remained computationally intensive. This work introduces a\nhigh-performance C++ framework for simulating damage diffusion process in STEM\nthat combines efficient numerical computation, advanced visualisations, and\nmultithreading to achieve efficient runtime while maintaining accuracy.", "comment": "Presented in ISCS25", "pdf_url": "http://arxiv.org/pdf/2507.00294v1", "categories": ["eess.SP", "cond-mat.mtrl-sci"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2507.00294v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00155", "title": "Do Music Source Separation Models Preserve Spatial Information in Binaural Audio?", "authors": ["Richa Namballa", "Agnieszka Roginska", "Magdalena Fuentes"], "summary": "Binaural audio remains underexplored within the music information retrieval\ncommunity. Motivated by the rising popularity of virtual and augmented reality\nexperiences as well as potential applications to accessibility, we investigate\nhow well existing music source separation (MSS) models perform on binaural\naudio. Although these models process two-channel inputs, it is unclear how\neffectively they retain spatial information. In this work, we evaluate how\nseveral popular MSS models preserve spatial information on both standard stereo\nand novel binaural datasets. Our binaural data is synthesized using stems from\nMUSDB18-HQ and open-source head-related transfer functions by positioning\ninstrument sources randomly along the horizontal plane. We then assess the\nspatial quality of the separated stems using signal processing and interaural\ncue-based metrics. Our results show that stereo MSS models fail to preserve the\nspatial information critical for maintaining the immersive quality of binaural\naudio, and that the degradation depends on model architecture as well as the\ntarget instrument. Finally, we highlight valuable opportunities for future work\nat the intersection of MSS and immersive audio.", "comment": "6 pages + references, 4 figures, 2 tables, 26th International Society\n  for Music Information Retrieval (ISMIR) Conference", "pdf_url": "http://arxiv.org/pdf/2507.00155v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "cate": "eess.AS", "url": "http://arxiv.org/abs/2507.00155v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00577", "title": "BadViM: Backdoor Attack against Vision Mamba", "authors": ["Yinghao Wu", "Liyan Zhang"], "summary": "Vision State Space Models (SSMs), particularly architectures like Vision\nMamba (ViM), have emerged as promising alternatives to Vision Transformers\n(ViTs). However, the security implications of this novel architecture,\nespecially their vulnerability to backdoor attacks, remain critically\nunderexplored. Backdoor attacks aim to embed hidden triggers into victim\nmodels, causing the model to misclassify inputs containing these triggers while\nmaintaining normal behavior on clean inputs. This paper investigates the\nsusceptibility of ViM to backdoor attacks by introducing BadViM, a novel\nbackdoor attack framework specifically designed for Vision Mamba. The proposed\nBadViM leverages a Resonant Frequency Trigger (RFT) that exploits the frequency\nsensitivity patterns of the victim model to create stealthy, distributed\ntriggers. To maximize attack efficacy, we propose a Hidden State Alignment loss\nthat strategically manipulates the internal representations of model by\naligning the hidden states of backdoor images with those of target classes.\nExtensive experimental results demonstrate that BadViM achieves superior attack\nsuccess rates while maintaining clean data accuracy. Meanwhile, BadViM exhibits\nremarkable resilience against common defensive measures, including PatchDrop,\nPatchShuffle and JPEG compression, which typically neutralize normal backdoor\nattacks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00577v1", "categories": ["cs.CR", "cs.AI", "cs.CV"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2507.00577v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00413", "title": "Recommending Variable Names for Extract Local Variable Refactorings", "authors": ["Taiming Wang", "Hui Liu", "Yuxia Zhang", "Yanjie Jiang"], "summary": "Extract local variable is one of the most popular refactorings, and most IDEs\nand refactoring tools provide automated support for this refactoring. However,\nwe find approximately 70% of the names recommended by these IDEs are different\nfrom what developers manually constructed, adding additional renaming burdens\nto developers and providing limited assistance. In this paper, we introduce\nVarNamer, an automated approach designed to recommend variable names for\nextract local variable refactorings. Through a large-scale empirical study, we\nidentify key contexts that are useful for composing variable names. Leveraging\nthese insights, we developed a set of heuristic rules through program static\nanalysis techniques and employ data mining techniques to recommend variable\nnames effectively. Notably, some of our heuristic rules have been successfully\nintegrated into Eclipse, where they are now distributed with the latest\nreleases of the IDE. Evaluation demonstrates its superiority over\nstate-of-the-art IDEs. Specifically, VarNamer significantly increases the\nchance of exact match by 52.6% compared to Eclipse and 40.7% compared to\nIntelliJ IDEA. We also evaluated the proposed approach with real-world extract\nlocal variable refactorings conducted in C++ projects, and the results suggest\nthat the approach can achieve comparable performance on programming languages\nbesides Java. It may suggest the generalizability of VarNamer. Finally, we\ndesigned and conducted a user study and the results of the user study suggest\nthat our approach can speed up the refactoring by 27.8% and reduce 49.3% edits\non the recommended variable names.", "comment": "Accepted by TOSEM", "pdf_url": "http://arxiv.org/pdf/2507.00413v1", "categories": ["cs.SE", "D.2.7"], "cate": "cs.SE", "url": "http://arxiv.org/abs/2507.00413v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00081", "title": "State and Memory is All You Need for Robust and Reliable AI Agents", "authors": ["Matthew Muhoberac", "Atharva Parikh", "Nirvi Vakharia", "Saniya Virani", "Aco Radujevic", "Savannah Wood", "Meghav Verma", "Dimitri Metaxotos", "Jeyaraman Soundararajan", "Thierry Masquelin", "Alexander G. Godfrey", "Sean Gardner", "Dobrila Rudnicki", "Sam Michael", "Gaurav Chopra"], "summary": "Large language models (LLMs) have enabled powerful advances in natural\nlanguage understanding and generation. Yet their application to complex,\nreal-world scientific workflows remain limited by challenges in memory,\nplanning, and tool integration. Here, we introduce SciBORG (Scientific Bespoke\nArtificial Intelligence Agents Optimized for Research Goals), a modular agentic\nframework that allows LLM-based agents to autonomously plan, reason, and\nachieve robust and reliable domain-specific task execution. Agents are\nconstructed dynamically from source code documentation and augmented with\nfinite-state automata (FSA) memory, enabling persistent state tracking and\ncontext-aware decision-making. This approach eliminates the need for manual\nprompt engineering and allows for robust, scalable deployment across diverse\napplications via maintaining context across extended workflows and to recover\nfrom tool or execution failures. We validate SciBORG through integration with\nboth physical and virtual hardware, such as microwave synthesizers for\nexecuting user-specified reactions, with context-aware decision making and\ndemonstrate its use in autonomous multi-step bioassay retrieval from the\nPubChem database utilizing multi-step planning, reasoning, agent-to-agent\ncommunication and coordination for execution of exploratory tasks. Systematic\nbenchmarking shows that SciBORG agents achieve reliable execution, adaptive\nplanning, and interpretable state transitions. Our results show that memory and\nstate awareness are critical enablers of agentic planning and reliability,\noffering a generalizable foundation for deploying AI agents in complex\nenvironments.", "comment": "5 Main Figures, 10 Extended Data Figures (37 Pages) for Manuscript ;\n  9 Supplementary Tables, 40 Supplementary Figures (180 Pages) for Supporting\n  Information", "pdf_url": "http://arxiv.org/pdf/2507.00081v1", "categories": ["cs.MA", "cs.AI", "cs.CL", "cs.ET", "physics.chem-ph"], "cate": "cs.MA", "url": "http://arxiv.org/abs/2507.00081v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00268", "title": "Control-Optimized Deep Reinforcement Learning for Artificially Intelligent Autonomous Systems", "authors": ["Oren Fivel", "Matan Rudman", "Kobi Cohen"], "summary": "Deep reinforcement learning (DRL) has become a powerful tool for complex\ndecision-making in machine learning and AI. However, traditional methods often\nassume perfect action execution, overlooking the uncertainties and deviations\nbetween an agent's selected actions and the actual system response. In\nreal-world applications, such as robotics, mechatronics, and communication\nnetworks, execution mismatches arising from system dynamics, hardware\nconstraints, and latency can significantly degrade performance. This work\nadvances AI by developing a novel control-optimized DRL framework that\nexplicitly models and compensates for action execution mismatches, a challenge\nlargely overlooked in existing methods. Our approach establishes a structured\ntwo-stage process: determining the desired action and selecting the appropriate\ncontrol signal to ensure proper execution. It trains the agent while accounting\nfor action mismatches and controller corrections. By incorporating these\nfactors into the training process, the AI agent optimizes the desired action\nwith respect to both the actual control signal and the intended outcome,\nexplicitly considering execution errors. This approach enhances robustness,\nensuring that decision-making remains effective under real-world uncertainties.\nOur approach offers a substantial advancement for engineering practice by\nbridging the gap between idealized learning and real-world implementation. It\nequips intelligent agents operating in engineering environments with the\nability to anticipate and adjust for actuation errors and system disturbances\nduring training. We evaluate the framework in five widely used open-source\nmechanical simulation environments we restructured and developed to reflect\nreal-world operating conditions, showcasing its robustness against\nuncertainties and offering a highly practical and efficient solution for\ncontrol-oriented applications.", "comment": "27 pages, 10 figures", "pdf_url": "http://arxiv.org/pdf/2507.00268v1", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2507.00268v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00672", "title": "Toward Edge General Intelligence with Multiple-Large Language Model (Multi-LLM): Architecture, Trust, and Orchestration", "authors": ["Haoxiang Luo", "Yinqiu Liu", "Ruichen Zhang", "Jiacheng Wang", "Gang Sun", "Dusit Niyato", "Hongfang Yu", "Zehui Xiong", "Xianbin Wang", "Xuemin Shen"], "summary": "Edge computing enables real-time data processing closer to its source, thus\nimproving the latency and performance of edge-enabled AI applications. However,\ntraditional AI models often fall short when dealing with complex, dynamic tasks\nthat require advanced reasoning and multimodal data processing. This survey\nexplores the integration of multi-LLMs (Large Language Models) to address this\nin edge computing, where multiple specialized LLMs collaborate to enhance task\nperformance and adaptability in resource-constrained environments. We review\nthe transition from conventional edge AI models to single LLM deployment and,\nultimately, to multi-LLM systems. The survey discusses enabling technologies\nsuch as dynamic orchestration, resource scheduling, and cross-domain knowledge\ntransfer that are key for multi-LLM implementation. A central focus is on\ntrusted multi-LLM systems, ensuring robust decision-making in environments\nwhere reliability and privacy are crucial. We also present multimodal multi-LLM\narchitectures, where multiple LLMs specialize in handling different data\nmodalities, such as text, images, and audio, by integrating their outputs for\ncomprehensive analysis. Finally, we highlight future directions, including\nimproving resource efficiency, trustworthy governance multi-LLM systems, while\naddressing privacy, trust, and robustness concerns. This survey provides a\nvaluable reference for researchers and practitioners aiming to leverage\nmulti-LLM systems in edge computing applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00672v1", "categories": ["cs.NI", "cs.DC"], "cate": "cs.NI", "url": "http://arxiv.org/abs/2507.00672v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00591", "title": "Construction of LDPC convolutional codes with large girth from Latin squares", "authors": ["Elisa Junghans", "Julia Lieb"], "summary": "Due to their capacity approaching performance low-density parity-check (LDPC)\ncodes gained a lot of attention in the last years. The parity-check matrix of\nthe codes can be associated with a bipartite graph, called Tanner graph. To\ndecrease the probability of decoding failure it is desirable to have LDPC codes\nwith large girth of the associated Tanner graph. Moreover, to store such codes\nefficiently, it is desirable to have compact constructions for them. In this\npaper, we present constructions of LDPC convolutional codes with girth up to\n$12$ using a special class of Latin squares and several lifting steps, which\nenables a compact representation of these codes. With these techniques, we can\nprovide constructions for well-performing and efficiently storable time-varying\nand time-invariant LDPC convolutional codes as well as for LDPC block codes.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00591v1", "categories": ["cs.IT", "math.IT"], "cate": "cs.IT", "url": "http://arxiv.org/abs/2507.00591v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00050", "title": "SEZ-HARN: Self-Explainable Zero-shot Human Activity Recognition Network", "authors": ["Devin Y. De Silva", "Sandareka Wickramanayake", "Dulani Meedeniya", "Sanka Rasnayaka"], "summary": "Human Activity Recognition (HAR), which uses data from Inertial Measurement\nUnit (IMU) sensors, has many practical applications in healthcare and assisted\nliving environments. However, its use in real-world scenarios has been limited\nby the lack of comprehensive IMU-based HAR datasets that cover a wide range of\nactivities and the lack of transparency in existing HAR models. Zero-shot HAR\n(ZS-HAR) overcomes the data limitations, but current models struggle to explain\ntheir decisions, making them less transparent. This paper introduces a novel\nIMU-based ZS-HAR model called the Self-Explainable Zero-shot Human Activity\nRecognition Network (SEZ-HARN). It can recognize activities not encountered\nduring training and provide skeleton videos to explain its decision-making\nprocess. We evaluate the effectiveness of the proposed SEZ-HARN on four\nbenchmark datasets PAMAP2, DaLiAc, HTD-MHAD and MHealth and compare its\nperformance against three state-of-the-art black-box ZS-HAR models. The\nexperiment results demonstrate that SEZ-HARN produces realistic and\nunderstandable explanations while achieving competitive Zero-shot recognition\naccuracy. SEZ-HARN achieves a Zero-shot prediction accuracy within 3\\% of the\nbest-performing black-box model on PAMAP2 while maintaining comparable\nperformance on the other three datasets.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00050v1", "categories": ["cs.AI", "cs.HC", "cs.LG", "I.2.0"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2507.00050v1", "date": "2025-06-25", "updated": "2025-06-25"}
{"id": "2507.00011", "title": "Novel RL approach for efficient Elevator Group Control Systems", "authors": ["Nathan Vaartjes", "Vincent Francois-Lavet"], "summary": "Efficient elevator traffic management in large buildings is critical for\nminimizing passenger travel times and energy consumption. Because heuristic- or\npattern-detection-based controllers struggle with the stochastic and\ncombinatorial nature of dispatching, we model the six-elevator, fifteen-floor\nsystem at Vrije Universiteit Amsterdam as a Markov Decision Process and train\nan end-to-end Reinforcement Learning (RL) Elevator Group Control System (EGCS).\nKey innovations include a novel action space encoding to handle the\ncombinatorial complexity of elevator dispatching, the introduction of\ninfra-steps to model continuous passenger arrivals, and a tailored reward\nsignal to improve learning efficiency. In addition, we explore various ways to\nadapt the discounting factor to the infra-step formulation. We investigate RL\narchitectures based on Dueling Double Deep Q-learning, showing that the\nproposed RL-based EGCS adapts to fluctuating traffic patterns, learns from a\nhighly stochastic environment, and thereby outperforms a traditional rule-based\nalgorithm.", "comment": "15 pages, 12 figures", "pdf_url": "http://arxiv.org/pdf/2507.00011v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00011v1", "date": "2025-06-12", "updated": "2025-06-12"}
{"id": "2507.00507", "title": "LLM-Mesh: Enabling Elastic Sharing for Serverless LLM Inference", "authors": ["Chuhao Xu", "Zijun Li", "Quan Chen", "Han Zhao", "Minyi Guo"], "summary": "The rise of LLMs has driven demand for private serverless deployments,\ncharacterized by moderate-scale models and infrequent requests. While existing\nsolutions follow exclusive GPU deployment, we take a step back to explore\nmodern platforms and find that: Emerging CPU architectures with built-in\naccelerators are capable of serving LLMs but remain underutilized, and both\nCPUs and GPUs can accommodate multiple LLMs simultaneously.\n  We propose LLM-Mesh, a serverless inference scheme for small-to-mid-sized\nLLMs that enables elastic sharing across heterogeneous hardware. LLM-Mesh\ntackles three fundamental challenges: (1) precise, fine-grained compute\nresource allocation at token-level to handle fluctuating computational demands;\n(2) a coordinated and forward-looking memory scaling mechanism to detect\nout-of-memory hazards and reduce operational overhead; and (3) a dual approach\nthat reduces resource fragmentation through proactive preemption and reactive\nbin-packing. Experimental results on 4 32-core CPUs and 4 A100 GPUs show that\nLLM-Meshimproves service capacity by 44% - 63% through sharing, while further\nleveraging CPUs boosts this to 91% - 159%.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00507v1", "categories": ["cs.DC"], "cate": "cs.DC", "url": "http://arxiv.org/abs/2507.00507v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00044", "title": "HistoART: Histopathology Artifact Detection and Reporting Tool", "authors": ["Seyed Kahaki", "Alexander R. Webber", "Ghada Zamzmi", "Adarsh Subbaswamy", "Rucha Deshpande", "Aldo Badano"], "summary": "In modern cancer diagnostics, Whole Slide Imaging (WSI) is widely used to\ndigitize tissue specimens for detailed, high-resolution examination; however,\nother diagnostic approaches, such as liquid biopsy and molecular testing, are\nalso utilized based on the cancer type and clinical context. While WSI has\nrevolutionized digital histopathology by enabling automated, precise analysis,\nit remains vulnerable to artifacts introduced during slide preparation and\nscanning. These artifacts can compromise downstream image analysis. To address\nthis challenge, we propose and compare three robust artifact detection\napproaches for WSIs: (1) a foundation model-based approach (FMA) using a\nfine-tuned Unified Neural Image (UNI) architecture, (2) a deep learning\napproach (DLA) built on a ResNet50 backbone, and (3) a knowledge-based approach\n(KBA) leveraging handcrafted features from texture, color, and frequency-based\nmetrics. The methods target six common artifact types: tissue folds,\nout-of-focus regions, air bubbles, tissue damage, marker traces, and blood\ncontamination. Evaluations were conducted on 50,000+ image patches from diverse\nscanners (Hamamatsu, Philips, Leica Aperio AT2) across multiple sites. The FMA\nachieved the highest patch-wise AUROC of 0.995 (95% CI [0.994, 0.995]),\noutperforming the ResNet50-based method (AUROC: 0.977, 95% CI [0.977, 0.978])\nand the KBA (AUROC: 0.940, 95% CI [0.933, 0.946]). To translate detection into\nactionable insights, we developed a quality report scorecard that quantifies\nhigh-quality patches and visualizes artifact distributions.", "comment": "14 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2507.00044v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00044v1", "date": "2025-06-23", "updated": "2025-06-23"}
{"id": "2507.00172", "title": "Intellectual Property Rights and Entrepreneurship in the NFT Ecosystem: Legal Frameworks, Business Models, and Innovation Opportunities", "authors": ["Pranav Darshan", "Rohan J S", "Raghuveer Rajesh", "Ruchitha M", "Sanika Kamath", "Manas M N"], "summary": "Non Fungible Tokens have changed digital ownership and how creators earn\nmoney. Between 2021 and 2024, the market value exceeded 40 billion. However,\nthe fast growth of the NFT ecosystem has revealed serious issues in managing\nintellectual property rights. There is a lot of confusion about the difference\nbetween owning an NFT and owning the copyright for the underlying content. This\nresearch looks at the gap between traditional copyright laws and\nblockchain-based transactions. We use a mixed methods approach to analyze this\ndisconnect. We create a new IP rights matrix that clearly shows how copyright\nlaw relates to NFT ownership structures. Additionally, we include a business\nmodel taxonomy that sorts new commercial applications by their IP risk and\nsustainability factors. By examining important legal cases, smart contracts,\nand interviews with stakeholders, we find key problems in enforcing laws across\ndifferent regions, standardizing licenses, and assessing business\nopportunities.", "comment": "11 pages", "pdf_url": "http://arxiv.org/pdf/2507.00172v1", "categories": ["cs.CY", "cs.ET"], "cate": "cs.CY", "url": "http://arxiv.org/abs/2507.00172v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00209", "title": "SurgiSR4K: A High-Resolution Endoscopic Video Dataset for Robotic-Assisted Minimally Invasive Procedures", "authors": ["Fengyi Jiang", "Xiaorui Zhang", "Lingbo Jin", "Ruixing Liang", "Yuxin Chen", "Adi Chola Venkatesh", "Jason Culman", "Tiantian Wu", "Lirong Shao", "Wenqing Sun", "Cong Gao", "Hallie McNamara", "Jingpei Lu", "Omid Mohareri"], "summary": "High-resolution imaging is crucial for enhancing visual clarity and enabling\nprecise computer-assisted guidance in minimally invasive surgery (MIS). Despite\nthe increasing adoption of 4K endoscopic systems, there remains a significant\ngap in publicly available native 4K datasets tailored specifically for\nrobotic-assisted MIS. We introduce SurgiSR4K, the first publicly accessible\nsurgical imaging and video dataset captured at a native 4K resolution,\nrepresenting realistic conditions of robotic-assisted procedures. SurgiSR4K\ncomprises diverse visual scenarios including specular reflections, tool\nocclusions, bleeding, and soft tissue deformations, meticulously designed to\nreflect common challenges faced during laparoscopic and robotic surgeries. This\ndataset opens up possibilities for a broad range of computer vision tasks that\nmight benefit from high resolution data, such as super resolution (SR), smoke\nremoval, surgical instrument detection, 3D tissue reconstruction, monocular\ndepth estimation, instance segmentation, novel view synthesis, and\nvision-language model (VLM) development. SurgiSR4K provides a robust foundation\nfor advancing research in high-resolution surgical imaging and fosters the\ndevelopment of intelligent imaging technologies aimed at enhancing performance,\nsafety, and usability in image-guided robotic surgeries.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00209v1", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.RO"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2507.00209v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00195", "title": "What Makes Local Updates Effective: The Role of Data Heterogeneity and Smoothness", "authors": ["Kumar Kshitij Patel"], "summary": "This thesis contributes to the theoretical understanding of local update\nalgorithms, especially Local SGD, in distributed and federated optimization\nunder realistic models of data heterogeneity. A central focus is on the bounded\nsecond-order heterogeneity assumption, which is shown to be both necessary and\nsufficient for local updates to outperform centralized or mini-batch methods in\nconvex and non-convex settings. The thesis establishes tight upper and lower\nbounds in several regimes for various local update algorithms and characterizes\nthe min-max complexity of multiple problem classes. At its core is a\nfine-grained consensus-error-based analysis framework that yields sharper\nfinite-time convergence bounds under third-order smoothness and relaxed\nheterogeneity assumptions. The thesis also extends to online federated\nlearning, providing fundamental regret bounds under both first-order and bandit\nfeedback. Together, these results clarify when and why local updates offer\nprovable advantages, and the thesis serves as a self-contained guide for\nanalyzing Local SGD in heterogeneous environments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00195v1", "categories": ["cs.LG", "cs.AI", "cs.MA", "math.OC", "stat.ML"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00195v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00202", "title": "Examining the Social Communication and Community Engagement of Autistic Adults through an Asynchronous Focus Group", "authors": ["Blade Frisch", "Betts Peters", "Keith Vertanen"], "summary": "Purpose: Little research has explored the communication needs of autistic\nadults and how their needs differ from those of other disabled populations.\nAugmentative and Alternative Communication (AAC) can support these\ncommunication needs, but more guidance is needed on how to design AAC to\nsupport this population.\n  Materials and Methods: We conducted an online, asynchronous, text-based focus\ngroup with five autistic adults to explore their social communication and\ncommunity engagement and how AAC can help support them.\n  Results and Conclusion: Our analysis of the participant responses found that\n1) participants' emotional experiences impacted the communication methods they\nused, 2) speaking autistic adults can benefit from AAC use, and 3) autistic\nshutdown creates dynamic communication needs. We present implications for\nfuture AAC design: supporting communication in times of shutdown, indicating\ncommunication ability to communication partners, and a need to better\nunderstand the fear of using AAC. These implications can inform the design for\nfuture AAC systems. We also provide themes for future autism research:\nexploring the impact of a late diagnosis, gaining a better understanding of the\ncommunication needs during autistic shutdown, and expanding research to include\nthe social and environmental factors that impact communication. Finally, we\nprovide guidance on how future online focus groups can be run in an accessible\nmanner.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00202v1", "categories": ["cs.HC"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2507.00202v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00855", "title": "A New Family of Thread to Core Allocation Policies for an SMT ARM Processor", "authors": ["Marta Navarro", "Josué Feliu", "Salvador Petit", "María E. Gómez", "Julio Sahuquillo"], "summary": "Modern high-performance servers commonly integrate Simultaneous\nMultithreading (SMT) processors, which efficiently boosts throughput over\nsingle-threaded cores. Optimizing performance in SMT processors faces\nchallenges due to the inter-application interference within each SMT core. To\nmitigate the interference, thread-to-core (T2C) allocation policies play a\npivotal role. State-of-the-art T2C policies work in two steps: i) building a\nper-application performance stack using performance counters and ii) building\nperformance prediction models to identify the best pairs of applications to run\non each core.\n  This paper explores distinct ways to build the performance stack in ARM\nprocessors and introduces the Instructions and Stalls Cycles (ISC) stack, a\nnovel approach to overcome ARM PMU limitations. The ISC stacks are used as\ninputs for a performance prediction model to estimate the applications'\nperformance considering the inter-application interference. The accuracy of the\nprediction model (second step) depends on the accuracy of the performance stack\n(first step); thus, the higher the accuracy of the performance stack, the\nhigher the potential performance gains obtained by the T2C allocation policy.\n  This paper presents SYNPA as a family of T2C allocation policies.\nExperimental results show that $SYNPA4$, the best-performing SYNPA variant,\noutperforms turnaround time by 38\\% over Linux, which represents 3$\\times$ the\ngains achieved by the state-of-the-art policies for ARM processors.\nFurthermore, the multiple discussions and refinements presented throughout this\npaper can be applied to other SMT processors from distinct vendors and are\naimed at helping performance analysts build performance stacks for accurate\nperformance estimates in real processors.", "comment": "13 pages", "pdf_url": "http://arxiv.org/pdf/2507.00855v1", "categories": ["cs.DC", "cs.AR"], "cate": "cs.DC", "url": "http://arxiv.org/abs/2507.00855v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00476", "title": "FreNBRDF: A Frequency-Rectified Neural Material Representation", "authors": ["Chenliang Zhou", "Zheyuan Hu", "Cengiz Oztireli"], "summary": "Accurate material modeling is crucial for achieving photorealistic rendering,\nbridging the gap between computer-generated imagery and real-world photographs.\nWhile traditional approaches rely on tabulated BRDF data, recent work has\nshifted towards implicit neural representations, which offer compact and\nflexible frameworks for a range of tasks. However, their behavior in the\nfrequency domain remains poorly understood. To address this, we introduce\nFreNBRDF, a frequency-rectified neural material representation. By leveraging\nspherical harmonics, we integrate frequency-domain considerations into neural\nBRDF modeling. We propose a novel frequency-rectified loss, derived from a\nfrequency analysis of neural materials, and incorporate it into a generalizable\nand adaptive reconstruction and editing pipeline. This framework enhances\nfidelity, adaptability, and efficiency. Extensive experiments demonstrate that\n\\ours improves the accuracy and robustness of material appearance\nreconstruction and editing compared to state-of-the-art baselines, enabling\nmore structured and interpretable downstream tasks and applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00476v1", "categories": ["cs.GR", "cs.CV"], "cate": "cs.GR", "url": "http://arxiv.org/abs/2507.00476v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00046", "title": "Evolutionary computing-based image segmentation method to detect defects and features in Additive Friction Stir Deposition Process", "authors": ["Akshansh Mishra", "Eyob Mesele Sefene", "Shivraman Thapliyal"], "summary": "This work proposes an evolutionary computing-based image segmentation\napproach for analyzing soundness in Additive Friction Stir Deposition (AFSD)\nprocesses. Particle Swarm Optimization (PSO) was employed to determine optimal\nsegmentation thresholds for detecting defects and features in multilayer AFSD\nbuilds. The methodology integrates gradient magnitude analysis with distance\ntransforms to create novel attention-weighted visualizations that highlight\ncritical interface regions. Five AFSD samples processed under different\nconditions were analyzed using multiple visualization techniques i.e.\nself-attention maps, and multi-channel visualization. These complementary\napproaches reveal subtle material transition zones and potential defect regions\nwhich were not readily observable through conventional imaging. The PSO\nalgorithm automatically identified optimal threshold values (ranging from\n156-173) for each sample, enabling precise segmentation of material interfaces.\nThe multi-channel visualization technique effectively combines boundary\ninformation (red channel), spatial relationships (green channel), and material\ndensity data (blue channel) into cohesive representations that quantify\ninterface quality. The results demonstrate that attention-based analysis\nsuccessfully identifies regions of incomplete bonding and inhomogeneities in\nAFSD joints, providing quantitative metrics for process optimization and\nquality assessment of additively manufactured components.", "comment": "7 pages, 4 figures", "pdf_url": "http://arxiv.org/pdf/2507.00046v1", "categories": ["cs.CV", "cs.CE"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00046v1", "date": "2025-06-24", "updated": "2025-06-24"}
{"id": "2507.00708", "title": "On the (In)Approximability of the Monitoring Edge Geodetic Set Problem", "authors": ["Davide Bilò", "Giodano Colli", "Luca Forlizzi", "Stefano Leucci"], "summary": "We study the minimum \\emph{Monitoring Edge Geodetic Set} (\\megset) problem\nintroduced in [Foucaud et al., CALDAM'23]: given a graph $G$, we say that an\nedge is monitored by a pair $u,v$ of vertices if \\emph{all} shortest paths\nbetween $u$ and $v$ traverse $e$; the goal of the problem consists in finding a\nsubset $M$ of vertices of $G$ such that each edge of $G$ is monitored by at\nleast one pair of vertices in $M$, and $|M|$ is minimized.\n  In this paper, we prove that all polynomial-time approximation algorithms for\nthe minimum \\megset problem must have an approximation ratio of $\\Omega(\\log\nn)$, unless \\p = \\np. To the best of our knowledge, this is the first\nnon-constant inapproximability result known for this problem. We also\nstrengthen the known \\np-hardness of the problem on $2$-apex graphs by showing\nthat the same result holds for $1$-apex graphs. This leaves open the problem of\ndetermining whether the problem remains \\np-hard on planar (i.e., $0$-apex)\ngraphs.\n  On the positive side, we design an algorithm that computes good approximate\nsolutions for hereditary graph classes that admit efficiently computable\nbalanced separators of truly sublinear size. This immediately results in\npolynomial-time approximation algorithms achieving an approximation ratio of\n$O(n^{\\frac{1}{4}} \\sqrt{\\log n})$ on planar graphs, graphs with bounded genus,\nand $k$-apex graphs with $k=O(n^{\\frac{1}{4}})$. On graphs with bounded\ntreewidth, we obtain an approximation ratio of $O(\\log^{3/2} n)$ for any\nconstant $\\varepsilon > 0$. This compares favorably with the best-known\napproximation algorithm for general graphs, which achieves an approximation\nratio of $O(\\sqrt{n \\log n})$ via a simple reduction to the \\textsc{Set Cover}\nproblem.", "comment": "arXiv admin note: text overlap with arXiv:2405.13875", "pdf_url": "http://arxiv.org/pdf/2507.00708v1", "categories": ["cs.DS", "cs.CC"], "cate": "cs.DS", "url": "http://arxiv.org/abs/2507.00708v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00272", "title": "Iteratively Saturated Kalman Filtering", "authors": ["Alan Yang", "Stephen Boyd"], "summary": "The Kalman filter (KF) provides optimal recursive state estimates for\nlinear-Gaussian systems and underpins applications in control, signal\nprocessing, and others. However, it is vulnerable to outliers in the\nmeasurements and process noise. We introduce the iteratively saturated Kalman\nfilter (ISKF), which is derived as a scaled gradient method for solving a\nconvex robust estimation problem. It achieves outlier robustness while\npreserving the KF's low per-step cost and implementation simplicity, since in\npractice it typically requires only one or two iterations to achieve good\nperformance. The ISKF also admits a steady-state variant that, like the\nstandard steady-state KF, does not require linear system solves in each time\nstep, making it well-suited for real-time systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00272v1", "categories": ["eess.SY", "cs.SY"], "cate": "eess.SY", "url": "http://arxiv.org/abs/2507.00272v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00508", "title": "Quadrature Over-the-Air-Computing for Multimodal Dual-Stream Signal Processing", "authors": ["Hyeon Seok Rou", "Kengo Ando", "Giuseppe Thadeu Freitas de Abreu", "David González G"], "summary": "We propose a novel quadrature over-the-air computing (Q-OTAC) framework that\nenables the simultaneously computation of two independent functions and/or data\nstream within a single transmission. In contrast to conventional OTAC schemes,\nwhere a single function is computed by treating each complex signal as a single\ncomponent, the proposed Q-OTAC exploits both in-phase and quadrature (IQ)\ncomponents of a complex signal, encoding two distinct functions and/or data\nstreams at the edge devices (EDs) and employing a novel low-complexity\nIQ-decoupled combiner at the access point (AP) to independently recover each\nstream, which effectively doubles the computation rate. A key strength of this\nframework lies in its simplicity and broad compatibility: the extension into\nthe quadrature domain is conceptually straightforward, yet remakably powerful,\nallowing seamless integration into existing OTAC techniques. Simulation results\nvalidate the effectiveness of this approach, including the first demonstration\nof dual-function aggregation (e.g., parallel summation and product),\nhighlighting the potential of Q-OTAC for enabling multi-modal and\nhigh-efficiency beyond fifth generation (B5G) applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00508v1", "categories": ["eess.SP"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2507.00508v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00227", "title": "Investigating Stochastic Methods for Prosody Modeling in Speech Synthesis", "authors": ["Paul Mayer", "Florian Lux", "Alejandro Pérez-González-de-Martos", "Angelina Elizarova", "Lindsey Vanderlyn", "Dirk Väth", "Ngoc Thang Vu"], "summary": "While generative methods have progressed rapidly in recent years, generating\nexpressive prosody for an utterance remains a challenging task in\ntext-to-speech synthesis. This is particularly true for systems that model\nprosody explicitly through parameters such as pitch, energy, and duration,\nwhich is commonly done for the sake of interpretability and controllability. In\nthis work, we investigate the effectiveness of stochastic methods for this\ntask, including Normalizing Flows, Conditional Flow Matching, and Rectified\nFlows. We compare these methods to a traditional deterministic baseline, as\nwell as to real human realizations. Our extensive subjective and objective\nevaluations demonstrate that stochastic methods produce natural prosody on par\nwith human speakers by capturing the variability inherent in human speech.\nFurther, they open up additional controllability options by allowing the\nsampling temperature to be tuned.", "comment": "Accepted at Interspeech 2025", "pdf_url": "http://arxiv.org/pdf/2507.00227v1", "categories": ["eess.AS", "cs.AI"], "cate": "eess.AS", "url": "http://arxiv.org/abs/2507.00227v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00152", "title": "Table Understanding and (Multimodal) LLMs: A Cross-Domain Case Study on Scientific vs. Non-Scientific Data", "authors": ["Ekaterina Borisova", "Fabio Barth", "Nils Feldhus", "Raia Abu Ahmad", "Malte Ostendorff", "Pedro Ortiz Suarez", "Georg Rehm", "Sebastian Möller"], "summary": "Tables are among the most widely used tools for representing structured data\nin research, business, medicine, and education. Although LLMs demonstrate\nstrong performance in downstream tasks, their efficiency in processing tabular\ndata remains underexplored. In this paper, we investigate the effectiveness of\nboth text-based and multimodal LLMs on table understanding tasks through a\ncross-domain and cross-modality evaluation. Specifically, we compare their\nperformance on tables from scientific vs. non-scientific contexts and examine\ntheir robustness on tables represented as images vs. text. Additionally, we\nconduct an interpretability analysis to measure context usage and input\nrelevance. We also introduce the TableEval benchmark, comprising 3017 tables\nfrom scholarly publications, Wikipedia, and financial reports, where each table\nis provided in five different formats: Image, Dictionary, HTML, XML, and LaTeX.\nOur findings indicate that while LLMs maintain robustness across table\nmodalities, they face significant challenges when processing scientific tables.", "comment": "TRL@ACL 2025, camera-ready version", "pdf_url": "http://arxiv.org/pdf/2507.00152v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2507.00152v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00005", "title": "SwarmFusion: Revolutionizing Disaster Response with Swarm Intelligence and Deep Learning", "authors": ["Vasavi Lankipalle"], "summary": "Disaster response requires rapid, adaptive decision-making in chaotic\nenvironments. SwarmFusion, a novel hybrid framework, integrates particle swarm\noptimization with convolutional neural networks to optimize real-time resource\nallocation and path planning. By processing live satellite, drone, and sensor\ndata, SwarmFusion enhances situational awareness and operational efficiency in\nflood and wildfire scenarios. Simulations using the DisasterSim2025 dataset\ndemonstrate up to 40 percentage faster response times and 90 percentage\nsurvivor coverage compared to baseline methods. This scalable, data-driven\napproach offers a transformative solution for time-critical disaster\nmanagement, with potential applications across diverse crisis scenarios.", "comment": "6", "pdf_url": "http://arxiv.org/pdf/2507.00005v1", "categories": ["cs.NE", "cs.LG"], "cate": "cs.NE", "url": "http://arxiv.org/abs/2507.00005v1", "date": "2025-06-10", "updated": "2025-06-10"}
{"id": "2507.00595", "title": "The Secrets Must Not Flow: Scaling Security Verification to Large Codebases (extended version)", "authors": ["Linard Arquint", "Samarth Kishor", "Jason R. Koenig", "Joey Dodds", "Daniel Kroening", "Peter Müller"], "summary": "Existing program verifiers can prove advanced properties about security\nprotocol implementations, but are difficult to scale to large codebases because\nof the manual effort required. We develop a novel methodology called *Diodon*\nthat addresses this challenge by splitting the codebase into the protocol\nimplementation (the *Core*) and the remainder (the *Application*). This split\nallows us to apply powerful semi-automated verification techniques to the\nsecurity-critical Core, while fully-automatic static analyses scale the\nverification to the entire codebase by ensuring that the Application cannot\ninvalidate the security properties proved for the Core. The static analyses\nachieve that by proving *I/O independence*, i.e., that the I/O operations\nwithin the Application are independent of the Core's security-relevant data\n(such as keys), and that the Application meets the Core's requirements. We have\nproved Diodon sound by first showing that we can safely allow the Application\nto perform I/O independent of the security protocol, and second that manual\nverification and static analyses soundly compose. We evaluate Diodon on two\ncase studies: an implementation of the signed Diffie-Hellman key exchange and a\nlarge (100k+ LoC) production Go codebase implementing a key exchange protocol\nfor which we obtained secrecy and injective agreement guarantees by verifying a\nCore of about 1% of the code with the auto-active program verifier Gobra in\nless than three person months.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00595v1", "categories": ["cs.CR", "cs.PL", "cs.SE"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2507.00595v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00421", "title": "Embedded DevOps: A Survey on the Application of DevOps Practices in Embedded Software and Firmware Development", "authors": ["Parthiv Katapara", "Anand Sharma"], "summary": "The adoption of DevOps practices in embedded systems and firmware development\nis emerging as a response to the growing complexity of modern\nhardware--software co-designed products. Unlike cloud-native applications,\nembedded systems introduce challenges such as hardware dependency, real-time\nconstraints, and safety-critical requirements. This literature review\nsynthesizes findings from 20 academic and industrial sources to examine how\nDevOps principles--particularly continuous integration, continuous delivery,\nand automated testing--are adapted to embedded contexts. We categorize efforts\nacross tooling, testing strategies, pipeline automation, and security\npractices. The review highlights current limitations in deployment workflows\nand observability, proposing a roadmap for future research. This work offers\nresearchers and practitioners a consolidated understanding of Embedded DevOps,\nbridging fragmented literature with a structured perspective.", "comment": "This paper present survey on DevOps practices which exists in\n  Embedded Software development", "pdf_url": "http://arxiv.org/pdf/2507.00421v1", "categories": ["cs.SE"], "cate": "cs.SE", "url": "http://arxiv.org/abs/2507.00421v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00108", "title": "Teaching Programming in the Age of Generative AI: Insights from Literature, Pedagogical Proposals, and Student Perspectives", "authors": ["Clemente Rubio-Manzano", "Jazna Meza", "Rodolfo Fernandez-Santibanez", "Christian Vidal-Castro"], "summary": "Computer programming is undergoing a true transformation driven by powerful\nnew tools for automatic source code generation based on large language models.\nThis transformation is also manifesting in introductory programming courses at\nuniversities around the world, generating an in-depth debate about how\nprogramming content should be taught, learned, and assessed in the context of\ngenerative artificial intelligence.\n  This article aims, on the one hand, to review the most relevant studies on\nthis issue, highlighting the advantages and disadvantages identified in the\nspecialized literature. On the other hand, it proposes enriching teaching and\nlearning methodologies by focusing on code comprehension and execution rather\nthan on mere coding or program functionality. In particular, it advocates for\nthe use of visual representations of code and visual simulations of its\nexecution as effective tools for teaching, learning, and assessing programming,\nthus fostering a deeper understanding among students.\n  Finally, the opinions of students who took the object-oriented programming\ncourse are presented to provide preliminary context supporting the\nincorporation of visual simulations in Java (or other languages) as part of the\ntraining process.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00108v1", "categories": ["cs.CY", "cs.AI", "cs.ET", "cs.PL"], "cate": "cs.CY", "url": "http://arxiv.org/abs/2507.00108v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00273", "title": "Mechanical Intelligence-Aware Curriculum Reinforcement Learning for Humanoids with Parallel Actuation", "authors": ["Yusuke Tanaka", "Alvin Zhu", "Quanyou Wang", "Dennis Hong"], "summary": "Reinforcement learning (RL) has enabled significant advances in humanoid\nrobot locomotion, yet most learning frameworks do not account for mechanical\nintelligence embedded in parallel actuation mechanisms due to limitations in\nsimulator support for closed kinematic chains. This omission can lead to\ninaccurate motion modeling and suboptimal policies, particularly for robots\nwith high actuation complexity. This paper presents an end-to-end curriculum RL\nframework for BRUCE, a kid-sized humanoid robot featuring three distinct\nparallel mechanisms in its legs: a differential pulley, a 5-bar linkage, and a\n4-bar linkage. Unlike prior approaches that rely on simplified serial\napproximations, we simulate all closed-chain constraints natively using\nGPU-accelerated MJX (MuJoCo), preserving the hardware's physical properties\nduring training. We benchmark our RL approach against a Model Predictive\nController (MPC), demonstrating better surface generalization and performance\nin real-world zero-shot deployment. This work highlights the computational\napproaches and performance benefits of fully simulating parallel mechanisms in\nend-to-end learning pipelines for legged humanoids.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00273v1", "categories": ["cs.RO"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2507.00273v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00856", "title": "Enhancing Vehicular Platooning with Wireless Federated Learning: A Resource-Aware Control Framework", "authors": ["Beining Wu", "Jun Huang", "Qiang Duan", "Liang Dong", "Zhipeng Cai"], "summary": "This paper aims to enhance the performance of Vehicular Platooning (VP)\nsystems integrated with Wireless Federated Learning (WFL). In highly dynamic\nenvironments, vehicular platoons experience frequent communication changes and\nresource constraints, which significantly affect information exchange and\nlearning model synchronization. To address these challenges, we first formulate\nWFL in VP as a joint optimization problem that simultaneously considers Age of\nInformation (AoI) and Federated Learning Model Drift (FLMD) to ensure timely\nand accurate control. Through theoretical analysis, we examine the impact of\nFLMD on convergence performance and develop a two-stage Resource-Aware Control\nframework (RACE). The first stage employs a Lagrangian dual decomposition\nmethod for resource configuration, while the second stage implements a\nmulti-agent deep reinforcement learning approach for vehicle selection. The\napproach integrates Multi-Head Self-Attention and Long Short-Term Memory\nnetworks to capture spatiotemporal correlations in communication states.\nExperimental results demonstrate that, compared to baseline methods, the\nproposed framework improves AoI optimization by up to 45%, accelerates learning\nconvergence, and adapts more effectively to dynamic VP environments on the\nAI4MARS dataset.", "comment": "Under review at IEEE Transactions on Networking", "pdf_url": "http://arxiv.org/pdf/2507.00856v1", "categories": ["cs.NI", "eess.SP"], "cate": "cs.NI", "url": "http://arxiv.org/abs/2507.00856v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00609", "title": "On the rank weight hierarchy of $M$-codes", "authors": ["G. Berhuy", "J. Molina"], "summary": "We study the rank weight hierarchy of linear codes which are stable under a\nlinear endomorphism defined over the base field, in particular when the\nendomorphism is cyclic. In this last case, we give a necessary and sufficient\ncondition for such a code to have first rank weight equal to $1$ in terms of\nits generator polynomial, as well as an explicit formula for its last rank\nweight.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00609v1", "categories": ["cs.IT", "math.IT"], "cate": "cs.IT", "url": "http://arxiv.org/abs/2507.00609v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00054", "title": "Enhancing Reasoning Capabilities in SLMs with Reward Guided Dataset Distillation", "authors": ["Shreyansh Padarha"], "summary": "The push to compress and impart the proficiency of Large Language Models\n(LLMs) into more deployable and efficient Small Language Models (SLMs) has\nbenefited from improvements in knowledge distillation (KD) techniques. These\ntechniques allow a smaller student model to learn from a more capable and\nlarger teacher model's responses. However, distillation often revolves around\nthe student model merely copying the teacher's in-distribution responses,\nlimiting its generalisability. This limitation is amplified on reasoning tasks\nand can be computationally expensive. In this study, we propose AdvDistill, a\nreward-guided dataset distillation framework. We utilise multiple generations\n(responses) from a teacher for each prompt and assign rewards based on\nrule-based verifiers. These varying and normally distributed rewards serve as\nweights when training student models. Our methods and their subsequent\nbehavioural analysis demonstrate a significant improvement in student model\nperformance for mathematical and complex reasoning tasks, showcasing the\nefficacy and benefits of incorporating a rewarding mechanism in dataset\ndistillation processes.", "comment": "17 Pages, 7 figures", "pdf_url": "http://arxiv.org/pdf/2507.00054v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2507.00054v1", "date": "2025-06-25", "updated": "2025-06-25"}
{"id": "2507.00012", "title": "Towards Undistillable Models by Minimizing Conditional Mutual Information", "authors": ["Linfeng Ye", "Shayan Mohajer Hamidi", "En-hui Yang"], "summary": "A deep neural network (DNN) is said to be undistillable if, when used as a\nblack-box input-output teacher, it cannot be distilled through knowledge\ndistillation (KD). In this case, the distilled student (referred to as the\nknockoff student) does not outperform a student trained independently with\nlabel smoothing (LS student) in terms of prediction accuracy. To protect\nintellectual property of DNNs, it is desirable to build undistillable DNNs. To\nthis end, it is first observed that an undistillable DNN may have the trait\nthat each cluster of its output probability distributions in response to all\nsample instances with the same label should be highly concentrated to the\nextent that each cluster corresponding to each label should ideally collapse\ninto one probability distribution. Based on this observation and by measuring\nthe concentration of each cluster in terms of conditional mutual information\n(CMI), a new training method called CMI minimized (CMIM) method is proposed,\nwhich trains a DNN by jointly minimizing the conventional cross entropy (CE)\nloss and the CMI values of all temperature scaled clusters across the entire\ntemperature spectrum. The resulting CMIM model is shown, by extensive\nexperiments, to be undistillable by all tested KD methods existing in the\nliterature. That is, the knockoff students distilled by these KD methods from\nthe CMIM model underperform the respective LS students. In addition, the CMIM\nmodel is also shown to performs better than the model trained with the CE loss\nalone in terms of their own prediction accuracy.", "comment": "27 pages, 6 figures, Transactions on Machine Learning Research", "pdf_url": "http://arxiv.org/pdf/2507.00012v1", "categories": ["cs.LG", "cs.AI", "E.4"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00012v1", "date": "2025-06-13", "updated": "2025-06-13"}
{"id": "2507.00550", "title": "Collaborative Multi-Agent Reinforcement Learning Approach for Elastic Cloud Resource Scaling", "authors": ["Bruce Fang", "Danyi Gao"], "summary": "This paper addresses the challenges of rapid resource variation and highly\nuncertain task loads in cloud computing environments. It proposes an\noptimization method for elastic cloud resource scaling based on a multi-agent\nsystem. The method deploys multiple autonomous agents to perceive resource\nstates in parallel and make local decisions. While maintaining the distributed\nnature of the system, it introduces a collaborative value function to achieve\nglobal coordination. This improves the responsiveness of resource scheduling\nand enhances overall system performance. To strengthen system foresight, a\nlightweight state prediction model is designed. It assists agents in\nidentifying future workload trends and optimizes the selection of scaling\nactions. For policy training, the method adopts a centralized training and\ndecentralized execution reinforcement learning framework. This enables agents\nto learn effectively and coordinate strategies under conditions of incomplete\ninformation. The paper also constructs typical cloud scenarios, including\nmulti-tenancy and burst traffic, to evaluate the proposed method. The\nevaluation focuses on resource isolation, service quality assurance, and\nrobustness. Experimental results show that the proposed multi-agent scaling\nstrategy outperforms existing methods in resource utilization, SLA violation\ncontrol, and scheduling latency. The results demonstrate strong adaptability\nand intelligent regulation. This provides an efficient and reliable new\napproach to solving the problem of elastic resource scaling in complex cloud\nplatforms.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00550v1", "categories": ["cs.DC"], "cate": "cs.DC", "url": "http://arxiv.org/abs/2507.00550v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00045", "title": "CaughtCheating: Is Your MLLM a Good Cheating Detective? Exploring the Boundary of Visual Perception and Reasoning", "authors": ["Ming Li", "Chenguang Wang", "Yijun Liang", "Xiyao Wang", "Yuhang Zhou", "Xiyang Wu", "Yuqing Zhang", "Ruiyi Zhang", "Tianyi Zhou"], "summary": "Recent agentic Multi-Modal Large Language Models (MLLMs) such as GPT-o3 have\nachieved near-ceiling scores on various existing benchmarks, motivating a\ndemand for more challenging test tasks. These MLLMs have been reported to excel\nin a few expert-level tasks for humans, e.g., GeoGuesser, reflecting their\npotential as a detective who can notice minuscule cues in an image and weave\nthem into coherent, situational explanations, leading to a reliable answer. But\ncan they match the performance of excellent human detectives? To answer this\nquestion, we investigate some hard scenarios where GPT-o3 can still handle, and\nfind a common scenario where o3's performance drops to nearly zero, which we\nname CaughtCheating. It is inspired by the social media requests that ask\nothers to detect suspicious clues from photos shared by the poster's partner.\nWe conduct extensive experiments and analysis to understand why existing MLLMs\nlack sufficient capability to solve this kind of task. CaughtCheating provides\na class of challenging visual perception and reasoning tasks with great value\nand practical usage. Success in these tasks paves the way for MLLMs to acquire\nhuman-level detective perception and reasoning capabilities.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00045v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00045v1", "date": "2025-06-23", "updated": "2025-06-23"}
{"id": "2507.00406", "title": "Partnering with AI: A Pedagogical Feedback System for LLM Integration into Programming Education", "authors": ["Niklas Scholz", "Manh Hung Nguyen", "Adish Singla", "Tomohiro Nagashima"], "summary": "Feedback is one of the most crucial components to facilitate effective\nlearning. With the rise of large language models (LLMs) in recent years,\nresearch in programming education has increasingly focused on automated\nfeedback generation to help teachers provide timely support to every student.\nHowever, prior studies often overlook key pedagogical principles, such as\nmastery and progress adaptation, that shape effective feedback strategies. This\npaper introduces a novel pedagogical framework for LLM-driven feedback\ngeneration derived from established feedback models and local insights from\nsecondary school teachers. To evaluate this framework, we implemented a\nweb-based application for Python programming with LLM-based feedback that\nfollows the framework and conducted a mixed-method evaluation with eight\nsecondary-school computer science teachers. Our findings suggest that teachers\nconsider that, when aligned with the framework, LLMs can effectively support\nstudents and even outperform human teachers in certain scenarios through\ninstant and precise feedback. However, we also found several limitations, such\nas its inability to adapt feedback to dynamic classroom contexts. Such a\nlimitation highlights the need to complement LLM-generated feedback with human\nexpertise to ensure effective student learning. This work demonstrates an\neffective way to use LLMs for feedback while adhering to pedagogical standards\nand highlights important considerations for future systems.", "comment": "This is an extended version of a poster paper accepted and published\n  at ECTEL-2025", "pdf_url": "http://arxiv.org/pdf/2507.00406v1", "categories": ["cs.CY"], "cate": "cs.CY", "url": "http://arxiv.org/abs/2507.00406v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00398", "title": "Accurate and Efficient Fetal Birth Weight Estimation from 3D Ultrasound", "authors": ["Jian Wang", "Qiongying Ni", "Hongkui Yu", "Ruixuan Yao", "Jinqiao Ying", "Bin Zhang", "Xingyi Yang", "Jin Peng", "Jiongquan Chen", "Junxuan Yu", "Wenlong Shi", "Chaoyu Chen", "Zhongnuo Yan", "Mingyuan Luo", "Gaocheng Cai", "Dong Ni", "Jing Lu", "Xin Yang"], "summary": "Accurate fetal birth weight (FBW) estimation is essential for optimizing\ndelivery decisions and reducing perinatal mortality. However, clinical methods\nfor FBW estimation are inefficient, operator-dependent, and challenging to\napply in cases of complex fetal anatomy. Existing deep learning methods are\nbased on 2D standard ultrasound (US) images or videos that lack spatial\ninformation, limiting their prediction accuracy. In this study, we propose the\nfirst method for directly estimating FBW from 3D fetal US volumes. Our approach\nintegrates a multi-scale feature fusion network (MFFN) and a synthetic\nsample-based learning framework (SSLF). The MFFN effectively extracts and fuses\nmulti-scale features under sparse supervision by incorporating channel\nattention, spatial attention, and a ranking-based loss function. SSLF generates\nsynthetic samples by simply combining fetal head and abdomen data from\ndifferent fetuses, utilizing semi-supervised learning to improve prediction\nperformance. Experimental results demonstrate that our method achieves superior\nperformance, with a mean absolute error of $166.4\\pm155.9$ $g$ and a mean\nabsolute percentage error of $5.1\\pm4.6$%, outperforming existing methods and\napproaching the accuracy of a senior doctor. Code is available at:\nhttps://github.com/Qioy-i/EFW.", "comment": "Accepted by MICCAI 2025", "pdf_url": "http://arxiv.org/pdf/2507.00398v1", "categories": ["eess.IV", "cs.CV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2507.00398v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00443", "title": "Novel Pigeon-inspired 3D Obstacle Detection and Avoidance Maneuver for Multi-UAV Systems", "authors": ["Reza Ahmadvand", "Sarah Safura Sharif", "Yaser Mike Banad"], "summary": "Recent advances in multi-agent systems manipulation have demonstrated a\nrising demand for the implementation of multi-UAV systems in urban areas, which\nare always subjected to the presence of static and dynamic obstacles. Inspired\nby the collective behavior of tilapia fish and pigeons, the focus of the\npresented research is on the introduction of a nature-inspired collision-free\nformation control for a multi-UAV system, considering the obstacle avoidance\nmaneuvers. The developed framework in this study utilizes a semi-distributed\ncontrol approach, in which, based on a probabilistic Lloyd's algorithm, a\ncentralized guidance algorithm works for optimal positioning of the UAVs, while\na distributed control approach has been used for the intervehicle collision and\nobstacle avoidance. Further, the presented framework has been extended to the\n3D space with a novel definition of 3D maneuvers. Finally, the presented\nframework has been applied to multi-UAV systems in 2D and 3D scenarios, and the\nobtained results demonstrated the validity of the presented method in dynamic\nenvironments with stationary and moving obstacles.", "comment": "11 Pages, 11 Pictures, 1 Table, 3 Algorithms", "pdf_url": "http://arxiv.org/pdf/2507.00443v1", "categories": ["cs.RO", "cs.AI", "cs.MA"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2507.00443v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00271", "title": "User Concerns Regarding Social Robots for Mood Regulation: A Case Study on the \"Sunday Blues\"", "authors": ["Zhuochao Peng", "Jiaxin Xu", "Jun Hu", "Haian Xue", "Laurens A. G. Kolks", "Pieter M. A. Desmet"], "summary": "While recent research highlights the potential of social robots to support\nmood regulation, little is known about how prospective users view their\nintegration into everyday life. To explore this, we conducted an exploratory\ncase study that used a speculative robot concept \"Mora\" to provoke reflection\nand facilitate meaningful discussion about using social robots to manage\nsubtle, day-to-day emotional experiences. We focused on the \"Sunday Blues,\" a\ncommon dip in mood that occurs at the end of the weekend, as a relatable\ncontext in which to explore individuals' insights. Using a video prototype and\na co-constructing stories method, we engaged 15 participants in imagining\ninteractions with Mora and discussing their expectations, doubts, and concerns.\nThe study surfaced a range of nuanced reflections around the attributes of\nsocial robots like empathy, intervention effectiveness, and ethical boundaries,\nwhich we translated into design considerations for future research and\ndevelopment in human-robot interaction.", "comment": "Accepted to International Conference on Social Robotics + AI (ICSR\n  2025)", "pdf_url": "http://arxiv.org/pdf/2507.00271v1", "categories": ["cs.HC", "cs.RO"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2507.00271v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00937", "title": "RaGNNarok: A Light-Weight Graph Neural Network for Enhancing Radar Point Clouds on Unmanned Ground Vehicles", "authors": ["David Hunt", "Shaocheng Luo", "Spencer Hallyburton", "Shafii Nillongo", "Yi Li", "Tingjun Chen", "Miroslav Pajic"], "summary": "Low-cost indoor mobile robots have gained popularity with the increasing\nadoption of automation in homes and commercial spaces. However, existing lidar\nand camera-based solutions have limitations such as poor performance in\nvisually obscured environments, high computational overhead for data\nprocessing, and high costs for lidars. In contrast, mmWave radar sensors offer\na cost-effective and lightweight alternative, providing accurate ranging\nregardless of visibility. However, existing radar-based localization suffers\nfrom sparse point cloud generation, noise, and false detections. Thus, in this\nwork, we introduce RaGNNarok, a real-time, lightweight, and generalizable graph\nneural network (GNN)-based framework to enhance radar point clouds, even in\ncomplex and dynamic environments. With an inference time of just 7.3 ms on the\nlow-cost Raspberry Pi 5, RaGNNarok runs efficiently even on such\nresource-constrained devices, requiring no additional computational resources.\nWe evaluate its performance across key tasks, including localization, SLAM, and\nautonomous navigation, in three different environments. Our results demonstrate\nstrong reliability and generalizability, making RaGNNarok a robust solution for\nlow-cost indoor mobile robots.", "comment": "8 pages, accepted by IROS 2025", "pdf_url": "http://arxiv.org/pdf/2507.00937v1", "categories": ["cs.RO", "cs.AR", "cs.CV", "cs.LG"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2507.00937v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00725", "title": "Analyzing Time-Varying Scalar Fields using Piecewise-Linear Morse-Cerf Theory", "authors": ["Amritendu Dhar", "Apratim Chakraborty", "Vijay Natarajan"], "summary": "Morse-Cerf theory considers a one-parameter family of smooth functions\ndefined on a manifold and studies the evolution of their critical points with\nthe parameter. This paper presents an adaptation of Morse-Cerf theory to a\nfamily of piecewise-linear (PL) functions. The vertex diagram and Cerf diagram\nare introduced as representations of the evolution of critical points of the PL\nfunction. The characterization of a crossing in the vertex diagram based on the\nhomology of the lower links of vertices leads to the definition of a\ntopological descriptor for time-varying scalar fields. An algorithm for\ncomputing the Cerf diagram and a measure for comparing two Cerf diagrams are\nalso described together with experimental results on time-varying scalar\nfields.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00725v1", "categories": ["cs.GR", "cs.CG", "I.3.5"], "cate": "cs.GR", "url": "http://arxiv.org/abs/2507.00725v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00048", "title": "A collaborative digital twin built on FAIR data and compute infrastructure", "authors": ["Thomas M. Deucher", "Juan C. Verduzco", "Michael Titus", "Alejandro Strachan"], "summary": "The integration of machine learning with automated experimentation in\nself-driving laboratories (SDL) offers a powerful approach to accelerate\ndiscovery and optimization tasks in science and engineering applications. When\nsupported by findable, accessible, interoperable, and reusable (FAIR) data\ninfrastructure, SDLs with overlapping interests can collaborate more\neffectively. This work presents a distributed SDL implementation built on\nnanoHUB services for online simulation and FAIR data management. In this\nframework, geographically dispersed collaborators conducting independent\noptimization tasks contribute raw experimental data to a shared central\ndatabase. These researchers can then benefit from analysis tools and machine\nlearning models that automatically update as additional data become available.\nNew data points are submitted through a simple web interface and automatically\nprocessed using a nanoHUB Sim2L, which extracts derived quantities and indexes\nall inputs and outputs in a FAIR data repository called ResultsDB. A separate\nnanoHUB workflow enables sequential optimization using active learning, where\nresearchers define the optimization objective, and machine learning models are\ntrained on-the-fly with all existing data, guiding the selection of future\nexperiments. Inspired by the concept of ``frugal twin\", the optimization task\nseeks to find the optimal recipe to combine food dyes to achieve the desired\ntarget color. With easily accessible and inexpensive materials, researchers and\nstudents can set up their own experiments, share data with collaborators, and\nexplore the combination of FAIR data, predictive ML models, and sequential\noptimization. The tools introduced are generally applicable and can easily be\nextended to other optimization problems.", "comment": "10 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2507.00048v1", "categories": ["cs.AI", "cond-mat.mtrl-sci", "cs.CE", "cs.LG"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2507.00048v1", "date": "2025-06-24", "updated": "2025-06-24"}
{"id": "2507.00930", "title": "Inverse matroid optimization under subset constraints", "authors": ["Kristóf íBérczi", "Lydia Mirabel Mendoza-Cadena", "José Soto"], "summary": "In the Inverse Matroid problem, we are given a matroid, a fixed basis $B$,\nand an initial weight function, and the goal is to minimally modify the weights\n-- measured by some function -- so that $B$ becomes a maximum-weight basis. The\nproblem arises naturally in settings where one wishes to explain or enforce a\ngiven solution by minimally perturbing the input.\n  We extend this classical problem by replacing the fixed basis with a subset\n$S_0$ of the ground set and imposing various structural constraints on the set\nof maximum-weight bases relative to $S_0$. Specifically, we study six variants:\n(A) Inverse Matroid Exists, where $S_0$ must contain at least one\nmaximum-weight basis; (B) Inverse Matroid All, where all bases contained in\n$S_0$ are maximum-weight; and (C) Inverse Matroid Only, where $S_0$ contains\nexactly the maximum-weight bases, along with their natural negated\ncounterparts.\n  For all variants, we develop combinatorial polynomial-time algorithms under\nthe $\\ell_\\infty$-norm. A key ingredient is a refined min-max theorem for\nInverse Matroid under the $\\ell_\\infty$-norm, which enables simpler and faster\nalgorithms than previous approaches and may be of independent combinatorial\ninterest. Our work significantly broadens the range of inverse optimization\nproblems on matroids that can be solved efficiently, especially those that\nconstrain the structure of optimal solutions through subset inclusion or\nexclusion.", "comment": "20 pages", "pdf_url": "http://arxiv.org/pdf/2507.00930v1", "categories": ["cs.DS", "cs.DM"], "cate": "cs.DS", "url": "http://arxiv.org/abs/2507.00930v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00353", "title": "Augmented Physics-Based Li-ion Battery Model via Adaptive Ensemble Sparse Learning and Conformal Prediction", "authors": ["Samuel Filgueira da Silva", "Mehmet Fatih Ozkan", "Faissal El Idrissi", "Marcello Canova"], "summary": "Accurate electrochemical models are essential for the safe and efficient\noperation of lithium-ion batteries in real-world applications such as\nelectrified vehicles and grid storage. Reduced-order models (ROM) offer a\nbalance between fidelity and computational efficiency but often struggle to\ncapture complex and nonlinear behaviors, such as the dynamics in the cell\nvoltage response under high C-rate conditions. To address these limitations,\nthis study proposes an Adaptive Ensemble Sparse Identification (AESI) framework\nthat enhances the accuracy of reduced-order li-ion battery models by\ncompensating for unpredictable dynamics. The approach integrates an Extended\nSingle Particle Model (ESPM) with an evolutionary ensemble sparse learning\nstrategy to construct a robust hybrid model. In addition, the AESI framework\nincorporates a conformal prediction method to provide theoretically guaranteed\nuncertainty quantification for voltage error dynamics, thereby improving the\nreliability of the model's predictions. Evaluation across diverse operating\nconditions shows that the hybrid model (ESPM + AESI) improves the voltage\nprediction accuracy, achieving mean squared error reductions of up to 46% on\nunseen data. Prediction reliability is further supported by conformal\nprediction, yielding statistically valid prediction intervals with coverage\nratios of 96.85% and 97.41% for the ensemble models based on bagging and\nstability selection, respectively.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00353v1", "categories": ["eess.SY", "cs.LG", "cs.SY"], "cate": "eess.SY", "url": "http://arxiv.org/abs/2507.00353v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00529", "title": "Fair Rate Maximization for Fluid Antenna Relay (FAR)-assisted Multi-user MISO Communications", "authors": ["Ruopeng Xu", "Zhaohui Yang", "Ting Zhang", "Mingzhe Chen", "Chen Zhu", "Zhaoyang Zhang"], "summary": "In this paper, we investigate the problem of max-min rate maximization in\nfluid antenna relay (FAR)-assisted multi-user uplink multiple-input\nsingle-output (MISO) wireless systems, where each user is equipped with a\nsingle fluid antenna (FA) and the base station (BS) is equipped with multiple\nFAs. Unlike most existing relevant work focusing on maximizing sum rate of the\nfluid antenna system (FAS), which may cause unbearable rate loss to weak users,\nwe propose to maximize the minimal rate of the system to ensure fairness. The\nmax-min optimization problem is formulated by jointly optimizing the positions\nof FAs with meeting the minimum distance requirements of FAs, maximum\ntransmitting power limit, and feasible antenna region constraints. To solve\nthis problem, we propose an alternating algorithm with utilizing the successive\nconvex approximation (SCA) method. Simulation results demonstrate that the\nproposed method significantly outperforms conventional methods in terms of\nmaximizing the minimal achievable rate across different signal-to-noise ratios\n(SNRs) and normalized region sizes.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00529v1", "categories": ["eess.SP"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2507.00529v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00324", "title": "Collecting, Curating, and Annotating Good Quality Speech deepfake dataset for Famous Figures: Process and Challenges", "authors": ["Hashim Ali", "Surya Subramani", "Raksha Varahamurthy", "Nithin Adupa", "Lekha Bollinani", "Hafiz Malik"], "summary": "Recent advances in speech synthesis have introduced unprecedented challenges\nin maintaining voice authenticity, particularly concerning public figures who\nare frequent targets of impersonation attacks. This paper presents a\ncomprehensive methodology for collecting, curating, and generating synthetic\nspeech data for political figures and a detailed analysis of challenges\nencountered. We introduce a systematic approach incorporating an automated\npipeline for collecting high-quality bonafide speech samples, featuring\ntranscription-based segmentation that significantly improves synthetic speech\nquality. We experimented with various synthesis approaches; from single-speaker\nto zero-shot synthesis, and documented the evolution of our methodology. The\nresulting dataset comprises bonafide and synthetic speech samples from ten\npublic figures, demonstrating superior quality with a NISQA-TTS naturalness\nscore of 3.69 and the highest human misclassification rate of 61.9\\%.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00324v1", "categories": ["eess.AS"], "cate": "eess.AS", "url": "http://arxiv.org/abs/2507.00324v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00163", "title": "Prompting as Scientific Inquiry", "authors": ["Ari Holtzman", "Chenhao Tan"], "summary": "Prompting is the primary method by which we study and control large language\nmodels. It is also one of the most powerful: nearly every major capability\nattributed to LLMs-few-shot learning, chain-of-thought, constitutional AI-was\nfirst unlocked through prompting. Yet prompting is rarely treated as science\nand is frequently frowned upon as alchemy. We argue that this is a category\nerror. If we treat LLMs as a new kind of complex and opaque organism that is\ntrained rather than programmed, then prompting is not a workaround: it is\nbehavioral science. Mechanistic interpretability peers into the neural\nsubstrate, prompting probes the model in its native interface: language. We\ncontend that prompting is not inferior, but rather a key component in the\nscience of LLMs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00163v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2507.00163v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00387", "title": "A Review on Zeroing Neural Networks", "authors": ["Chengze Jiang", "Jie Gui", "Long Jin", "Shuai Li"], "summary": "Zeroing neural networks (ZNNs) have demonstrated outstanding performance on\ntime-varying optimization and control problems. Nonetheless, few studies are\ncommitted to illustrating the relationship among different ZNNs and the\nderivation of them. Therefore, reviewing the advances for a systematical\nunderstanding of this field is desirable. This paper provides a survey of ZNNs'\nprogress regarding implementing methods, analysis theory, and practical\napplications.", "comment": "This is what we submitted to IJCAI 2023. Maybe we will update this\n  paper in the future", "pdf_url": "http://arxiv.org/pdf/2507.00387v1", "categories": ["cs.NE"], "cate": "cs.NE", "url": "http://arxiv.org/abs/2507.00387v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00477", "title": "Read the Docs Before Rewriting: Equip Rewriter with Domain Knowledge via Continual Pre-training", "authors": ["Qi Wang", "Yixuan Cao", "Yifan Liu", "Jiangtao Zhao", "Ping Luo"], "summary": "A Retrieval-Augmented Generation (RAG)-based question-answering (QA) system\nenhances a large language model's knowledge by retrieving relevant documents\nbased on user queries. Discrepancies between user queries and document\nphrasings often necessitate query rewriting. However, in specialized domains,\nthe rewriter model may struggle due to limited domain-specific knowledge. To\nresolve this, we propose the R\\&R (Read the doc before Rewriting) rewriter,\nwhich involves continual pre-training on professional documents, akin to how\nstudents prepare for open-book exams by reviewing textbooks. Additionally, it\ncan be combined with supervised fine-tuning for improved results. Experiments\non multiple datasets demonstrate that R\\&R excels in professional QA across\nmultiple domains, effectively bridging the query-document gap, while\nmaintaining good performance in general scenarios, thus advancing the\napplication of RAG-based QA systems in specialized fields.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00477v1", "categories": ["cs.IR"], "cate": "cs.IR", "url": "http://arxiv.org/abs/2507.00477v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00156", "title": "Localized evaluation and fast summation in the extrapolated regularization method for integrals in Stokes flow", "authors": ["Joseph Siebor", "Svetlana Tlupova"], "summary": "Boundary integral equation methods are widely used in the solution of many\npartial differential equations. The kernels that appear in these surface\nintegrals are nearly singular when evaluated near the boundary, and\nstraightforward numerical integration produces inaccurate results. In Beale and\nTlupova (Adv. Comput. Math, 2024), an extrapolated regularization method was\nproposed to accurately evaluate the nearly singular single and double-layer\nsurface integrals for harmonic potentials or Stokes flow. The kernels are\nregularized using a smoothing parameter, and then a standard quadrature is\napplied. The integrals are computed for three choices of the smoothing\nparameter to find the extrapolated value to fifth order accuracy. In this work,\nwe apply several techniques to reduce the computational cost of the\nextrapolated regularization method applied to the Stokes single and double\nlayer integrals. First, we use a straightforward OpenMP parallelization over\nthe target points. Second, we note that the effect of the regularization is\nlocal and evaluate only the local component of the sum for three values of the\nsmoothing parameter. The non-local component of the sum is only evaluated once\nand reused in the other sums. This component is still the computational\nbottleneck as it is $O(N^2)$, where $N$ is the system size. We apply the\nkernel-independent treecode to these far-field interactions to reduce the CPU\ntime. We carry out experiments to determine optimal parameters both in terms of\naccuracy and efficiency of the computations. We then use these techniques to\ncompute Stokes flow around two spheres that are nearly touching.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00156v1", "categories": ["math.NA", "cs.NA"], "cate": "math.NA", "url": "http://arxiv.org/abs/2507.00156v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00229", "title": "A High-Fidelity Speech Super Resolution Network using a Complex Global Attention Module with Spectro-Temporal Loss", "authors": ["Tarikul Islam Tamiti", "Biraj Joshi", "Rida Hasan", "Rashedul Hasan", "Taieba Athay", "Nursad Mamun", "Anomadarshi Barua"], "summary": "Speech super-resolution (SSR) enhances low-resolution speech by increasing\nthe sampling rate. While most SSR methods focus on magnitude reconstruction,\nrecent research highlights the importance of phase reconstruction for improved\nperceptual quality. Therefore, we introduce CTFT-Net, a Complex Time-Frequency\nTransformation Network that reconstructs both magnitude and phase in complex\ndomains for improved SSR tasks. It incorporates a complex global attention\nblock to model inter-phoneme and inter-frequency dependencies and a complex\nconformer to capture long-range and local features, improving frequency\nreconstruction and noise robustness. CTFT-Net employs time-domain and\nmulti-resolution frequency-domain loss functions for better generalization.\nExperiments show CTFT-Net outperforms state-of-the-art models (NU-Wave,\nWSRGlow, NVSR, AERO) on the VCTK dataset, particularly for extreme upsampling\n(2 kHz to 48 kHz), reconstructing high frequencies effectively without noisy\nartifacts.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00229v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "cate": "cs.SD", "url": "http://arxiv.org/abs/2507.00229v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00637", "title": "Integrating Network and Attack Graphs for Service-Centric Impact Analysis", "authors": ["Joni Herttuainen", "Vesa Kuikka", "Kimmo K. Kaski"], "summary": "We present a novel methodology for modelling, visualising, and analysing\ncyber threats, attack paths, as well as their impact on user services in\nenterprise or infrastructure networks of digital devices and services they\nprovide. Using probabilistic methods to track the propagation of an attack\nthrough attack graphs, via the service or application layers, and on physical\ncommunication networks, our model enables us to analyse cyber attacks at\ndifferent levels of detail. Understanding the propagation of an attack within a\nservice among microservices and its spread between different services or\napplication servers could help detect and mitigate it early. We demonstrate\nthat this network-based influence spreading modelling approach enables the\nevaluation of diverse attack scenarios and the development of protection and\nmitigation measures, taking into account the criticality of services from the\nuser's perspective. This methodology could also aid security specialists and\nsystem administrators in making well-informed decisions regarding risk\nmitigation strategies.", "comment": "17 pages, 13 figures, submitted for peer-review", "pdf_url": "http://arxiv.org/pdf/2507.00637v1", "categories": ["cs.CR", "cs.SI"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2507.00637v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00481", "title": "The Influence of HEXACO Personality Traits on the Teamwork Quality in Software Teams -- A Preliminary Research Approach", "authors": ["Philipp M. Zähl", "Sabine Theis", "Martin R. Wolf"], "summary": "Although software engineering research has focused on optimizing processes\nand technology, there is a growing recognition that human factors, particularly\nteamwork, also significantly impact optimization. Recent research suggests that\ndeveloper personality has a strong influence on teamwork. In fact, personality\nconsiderations may have a greater impact on software development than processes\nand tools. This paper aims to design a study that measures the impact of HEXACO\npersonality traits on the Teamwork Quality (TWQ) of software teams. A\npreliminary data collection (n=54) was conducted for this purpose. The analysis\nshowed that several personality traits, as well as their composition, had a\nsignificant impact on TWQ. Additionally, other variables, such as the\nproportion of women and age distribution, also affected TWQ. The study's\ninitial results demonstrate the usefulness and validity of the study design.\nThe results also suggest several opportunities to improve teamwork in IT\norganizations and avenues for further research.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00481v1", "categories": ["cs.SE", "cs.HC"], "cate": "cs.SE", "url": "http://arxiv.org/abs/2507.00481v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00145", "title": "AI-Hybrid TRNG: Kernel-Based Deep Learning for Near-Uniform Entropy Harvesting from Physical Noise", "authors": ["Hasan Yiğit"], "summary": "AI-Hybrid TRNG is a deep-learning framework that extracts near-uniform\nentropy directly from physical noise, eliminating the need for bulky quantum\ndevices or expensive laboratory-grade RF receivers. Instead, it relies on a\nlow-cost, thumb-sized RF front end, plus CPU-timing jitter, for training, and\nthen emits 32-bit high-entropy streams without any quantization step.\n  Unlike deterministic or trained artificial intelligence random number\ngenerators (RNGs), our dynamic inner-outer network couples adaptive natural\nsources and reseeding, yielding truly unpredictable and autonomous sequences.\nGenerated numbers pass the NIST SP 800-22 battery better than a CPU-based\nmethod. It also passes nineteen bespoke statistical tests for both bit- and\ninteger-level analysis. All results satisfy cryptographic standards, while\nforward and backward prediction experiments reveal no exploitable biases. The\nmodel's footprint is below 0.5 MB, making it deployable on MCUs and FPGA soft\ncores, as well as suitable for other resource-constrained platforms.\n  By detaching randomness quality from dedicated hardware, AI-Hybrid TRNG\nbroadens the reach of high-integrity random number generators across secure\nsystems, cryptographic protocols, embedded and edge devices, stochastic\nsimulations, and server applications that need randomness.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00145v1", "categories": ["cs.CR", "cs.AI", "cs.ET", "cs.IT", "eess.SP", "math.IT"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2507.00145v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00319", "title": "When Digital Twins Meet Large Language Models: Realistic, Interactive, and Editable Simulation for Autonomous Driving", "authors": ["Tanmay Vilas Samak", "Chinmay Vilas Samak", "Bing Li", "Venkat Krovi"], "summary": "Simulation frameworks have been key enablers for the development and\nvalidation of autonomous driving systems. However, existing methods struggle to\ncomprehensively address the autonomy-oriented requirements of balancing: (i)\ndynamical fidelity, (ii) photorealistic rendering, (iii) context-relevant\nscenario orchestration, and (iv) real-time performance. To address these\nlimitations, we present a unified framework for creating and curating\nhigh-fidelity digital twins to accelerate advancements in autonomous driving\nresearch. Our framework leverages a mix of physics-based and data-driven\ntechniques for developing and simulating digital twins of autonomous vehicles\nand their operating environments. It is capable of reconstructing real-world\nscenes and assets (real2sim) with geometric and photorealistic accuracy and\ninfusing them with various physical properties to enable real-time dynamical\nsimulation of the ensuing driving scenarios. Additionally, it also incorporates\na large language model (LLM) interface to flexibly edit the driving scenarios\nonline via natural language prompts. We analyze the presented framework in\nterms of its fidelity, performance, and serviceability. Results indicate that\nour framework can reconstruct 3D scenes and assets with up to 97% structural\nsimilarity, while maintaining frame rates above 60 Hz. We also demonstrate that\nit can handle natural language prompts to generate diverse driving scenarios\nwith up to 95% repeatability and 85% generalizability.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00319v1", "categories": ["cs.RO"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2507.00319v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00896", "title": "QUIC Delay Control: an implementation of congestion and delay control", "authors": ["Saverio Mascolo", "Andrea Vittorio Balillo", "Gioacchino Manfredi", "Davide D'Agostino", "Luca De Cicco"], "summary": "A new congestion and delay control algorithm named QUIC Delay Control\n(QUIC-DC) is proposed for controlling not only congestion but also the queuing\ndelay encountered along the forward communication path. The core idea is to\nestimate the one-way queuing delay of a connection to trigger an early reaction\nto congestion. This idea, along with the TCP Westwood+ congestion control\nalgorithm, has been implemented in QUIC-DC and compared with QUIC Cubic, BBRv2,\nNewReno, Westwood+. The results obtained in both emulated and real network\nconnections show that QUIC-DC can significantly reduce packet losses along with\nend-to-end communication delays, while preserving network utilization, features\nthat are both very useful for real-time applications.", "comment": "8 pages, 9 figures", "pdf_url": "http://arxiv.org/pdf/2507.00896v1", "categories": ["cs.NI", "C.2.2; C.2.1"], "cate": "cs.NI", "url": "http://arxiv.org/abs/2507.00896v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00643", "title": "Decentralized Pliable Index Coding For Federated Learning In Intelligent Transportation Systems", "authors": ["Sadina Kadakkottiri", "Narisetty Harish", "Nujoom Sageer Karat", "Deepthi Paramel Pattathil", "Balaji Sundar Rajan"], "summary": "Federated Learning is a promising option for data privacy and security in\nITS, because it allows edge devices, Road Side Units (RSUs), and Central Server\n(CS) to jointly train the machine learning model. Since RSU collects data from\nthe vehicles passing through its range, the local data of each RSU will have a\nnon-IID distribution, which adversely affects the convergence speed and\naccuracy of FL training. Generating synthetic data locally at individual nodes,\nfollowed by data shuffling among the nodes, is a promising approach to address\nthe Non-IID data problem. In this work, we propose pliable index coding (PIC)\nsolutions for efficient data shuffling among the nodes in an FL system. In\nPIC($S$) problems, a client is satisfied if it can retrieve any $S$ new\nmessages not originally present in its side-information. We particularly\nconsider decentralized pliable index coding problems (DPIC) where the clients\ncommunicate among themselves without a central server to model the data\nshuffling in FL. A class of DPIC, known as Consecutive Decentralized Pliable\nIndex Coding (CDPIC($S$,$K$)), where each client has $K$ consecutive messages\nas side-information, is considered. For CDPIC($S$,$K$) problems, pliable index\ncode designs are provided for any value of $K$ and $S$, and optimality proofs\nfor some of the cases are established. Further, these CDPIC solutions are\napplied for data shuffling in FL, to transform the local data distribution\ntowards IID progressively with each transmission, thereby enhancing the\nperformance of FL. The improvement in the accuracy and convergence of the most\npopular FL technique, FedAvg, and a promising federated submodel technique,\nCELL (Communication Efficient Lottery Learning), are analysed by providing\ndifferent degrees of data shuffling using the proposed CDPIC schemes.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00643v1", "categories": ["cs.IT", "math.IT"], "cate": "cs.IT", "url": "http://arxiv.org/abs/2507.00643v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00079", "title": "VoyagerVision: Investigating the Role of Multi-modal Information for Open-ended Learning Systems", "authors": ["Ethan Smyth", "Alessandro Suglia"], "summary": "Open-endedness is an active field of research in the pursuit of capable\nArtificial General Intelligence (AGI), allowing models to pursue tasks of their\nown choosing. Simultaneously, recent advancements in Large Language Models\n(LLMs) such as GPT-4o [9] have allowed such models to be capable of\ninterpreting image inputs. Implementations such as OMNI-EPIC [4] have made use\nof such features, providing an LLM with pixel data of an agent's POV to parse\nthe environment and allow it to solve tasks. This paper proposes that providing\nthese visual inputs to a model gives it greater ability to interpret spatial\nenvironments, and as such, can increase the number of tasks it can successfully\nperform, extending its open-ended potential. To this aim, this paper proposes\nVoyagerVision -- a multi-modal model capable of creating structures within\nMinecraft using screenshots as a form of visual feedback, building on the\nfoundation of Voyager. VoyagerVision was capable of creating an average of 2.75\nunique structures within fifty iterations of the system, as Voyager was\nincapable of this, it is an extension in an entirely new direction.\nAdditionally, in a set of building unit tests VoyagerVision was successful in\nhalf of all attempts in flat worlds, with most failures arising in more complex\nstructures. Project website is available at\nhttps://esmyth-dev.github.io/VoyagerVision.github.io/", "comment": "website: https://esmyth-dev.github.io/VoyagerVision.github.io/", "pdf_url": "http://arxiv.org/pdf/2507.00079v1", "categories": ["cs.AI", "cs.LG"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2507.00079v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2507.00013", "title": "ST-MTM: Masked Time Series Modeling with Seasonal-Trend Decomposition for Time Series Forecasting", "authors": ["Hyunwoo Seo", "Chiehyeon Lim"], "summary": "Forecasting complex time series is an important yet challenging problem that\ninvolves various industrial applications. Recently, masked time-series modeling\nhas been proposed to effectively model temporal dependencies for forecasting by\nreconstructing masked segments from unmasked ones. However, since the semantic\ninformation in time series is involved in intricate temporal variations\ngenerated by multiple time series components, simply masking a raw time series\nignores the inherent semantic structure, which may cause MTM to learn spurious\ntemporal patterns present in the raw data. To capture distinct temporal\nsemantics, we show that masked modeling techniques should address entangled\npatterns through a decomposition approach. Specifically, we propose ST-MTM, a\nmasked time-series modeling framework with seasonal-trend decomposition, which\nincludes a novel masking method for the seasonal-trend components that\nincorporates different temporal variations from each component. ST-MTM uses a\nperiod masking strategy for seasonal components to produce multiple masked\nseasonal series based on inherent multi-periodicity and a sub-series masking\nstrategy for trend components to mask temporal regions that share similar\nvariations. The proposed masking method presents an effective pre-training task\nfor learning intricate temporal variations and dependencies. Additionally,\nST-MTM introduces a contrastive learning task to support masked modeling by\nenhancing contextual consistency among multiple masked seasonal\nrepresentations. Experimental results show that our proposed ST-MTM achieves\nconsistently superior forecasting performance compared to existing masked\nmodeling, contrastive learning, and supervised forecasting methods.", "comment": "Accepted by KDD 2025 research track", "pdf_url": "http://arxiv.org/pdf/2507.00013v1", "categories": ["cs.LG", "cs.AI", "stat.ML"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00013v1", "date": "2025-06-13", "updated": "2025-06-13"}
{"id": "2507.00576", "title": "DynoStore: A wide-area distribution system for the management of data over heterogeneous storage", "authors": ["Dante D. Sanchez-Gallegos", "J. L. Gonzalez-Compean", "Maxime Gonthier", "Valerie Hayot-Sasson", "J. Gregory Pauloski", "Haochen Pan", "Kyle Chard", "Jesus Carretero", "Ian Foster"], "summary": "Data distribution across different facilities offers benefits such as\nenhanced resource utilization, increased resilience through replication, and\nimproved performance by processing data near its source. However, managing such\ndata is challenging due to heterogeneous access protocols, disparate\nauthentication models, and the lack of a unified coordination framework. This\npaper presents DynoStore, a system that manages data across heterogeneous\nstorage systems. At the core of DynoStore are data containers, an abstraction\nthat provides standardized interfaces for seamless data management,\nirrespective of the underlying storage systems. Multiple data container\nconnections create a cohesive wide-area storage network, ensuring resilience\nusing erasure coding policies. Furthermore, a load-balancing algorithm ensures\nequitable and efficient utilization of storage resources. We evaluate DynoStore\nusing benchmarks and real-world case studies, including the management of\nmedical and satellite data across geographically distributed environments. Our\nresults demonstrate a 10\\% performance improvement compared to centralized\ncloud-hosted systems while maintaining competitive performance with\nstate-of-the-art solutions such as Redis and IPFS. DynoStore also exhibits\nsuperior fault tolerance, withstanding more failures than traditional systems.", "comment": "10 pages. Conference: The 25th IEEE International Symposium on\n  Cluster, Cloud, and Internet Computing", "pdf_url": "http://arxiv.org/pdf/2507.00576v1", "categories": ["cs.DC"], "cate": "cs.DC", "url": "http://arxiv.org/abs/2507.00576v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00046", "title": "Evolutionary computing-based image segmentation method to detect defects and features in Additive Friction Stir Deposition Process", "authors": ["Akshansh Mishra", "Eyob Mesele Sefene", "Shivraman Thapliyal"], "summary": "This work proposes an evolutionary computing-based image segmentation\napproach for analyzing soundness in Additive Friction Stir Deposition (AFSD)\nprocesses. Particle Swarm Optimization (PSO) was employed to determine optimal\nsegmentation thresholds for detecting defects and features in multilayer AFSD\nbuilds. The methodology integrates gradient magnitude analysis with distance\ntransforms to create novel attention-weighted visualizations that highlight\ncritical interface regions. Five AFSD samples processed under different\nconditions were analyzed using multiple visualization techniques i.e.\nself-attention maps, and multi-channel visualization. These complementary\napproaches reveal subtle material transition zones and potential defect regions\nwhich were not readily observable through conventional imaging. The PSO\nalgorithm automatically identified optimal threshold values (ranging from\n156-173) for each sample, enabling precise segmentation of material interfaces.\nThe multi-channel visualization technique effectively combines boundary\ninformation (red channel), spatial relationships (green channel), and material\ndensity data (blue channel) into cohesive representations that quantify\ninterface quality. The results demonstrate that attention-based analysis\nsuccessfully identifies regions of incomplete bonding and inhomogeneities in\nAFSD joints, providing quantitative metrics for process optimization and\nquality assessment of additively manufactured components.", "comment": "7 pages, 4 figures", "pdf_url": "http://arxiv.org/pdf/2507.00046v1", "categories": ["cs.CV", "cs.CE"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00046v1", "date": "2025-06-24", "updated": "2025-06-24"}
{"id": "2507.00456", "title": "Teacher-AI Collaboration for Curating and Customizing Lesson Plans in Low-Resource Schools", "authors": ["Deepak Varuvel Dennison", "Bakhtawar Ahtisham", "Kavyansh Chourasia", "Nirmit Arora", "Rahul Singh", "Rene F. Kizilcec", "Akshay Nambi", "Tanuja Ganu", "Aditya Vashistha"], "summary": "This study investigates Shiksha copilot, an AI-assisted lesson planning tool\ndeployed in government schools across Karnataka, India. The system combined\nLLMs and human expertise through a structured process in which English and\nKannada lesson plans were co-created by curators and AI; teachers then further\ncustomized these curated plans for their classrooms using their own expertise\nalongside AI support. Drawing on a large-scale mixed-methods study involving\n1,043 teachers and 23 curators, we examine how educators collaborate with AI to\ngenerate context-sensitive lesson plans, assess the quality of AI-generated\ncontent, and analyze shifts in teaching practices within multilingual,\nlow-resource environments. Our findings show that teachers used Shiksha copilot\nboth to meet administrative documentation needs and to support their teaching.\nThe tool eased bureaucratic workload, reduced lesson planning time, and lowered\nteaching-related stress, while promoting a shift toward activity-based\npedagogy. However, systemic challenges such as staffing shortages and\nadministrative demands constrained broader pedagogical change. We frame these\nfindings through the lenses of teacher-AI collaboration and communities of\npractice to examine the effective integration of AI tools in teaching. Finally,\nwe propose design directions for future teacher-centered EdTech, particularly\nin multilingual and Global South contexts.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00456v1", "categories": ["cs.CY", "cs.HC"], "cate": "cs.CY", "url": "http://arxiv.org/abs/2507.00456v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00511", "title": "Medical Image Segmentation Using Advanced Unet: VMSE-Unet and VM-Unet CBAM+", "authors": ["Sayandeep Kanrar", "Raja Piyush", "Qaiser Razi", "Debanshi Chakraborty", "Vikas Hassija", "GSS Chalapathi"], "summary": "In this paper, we present the VMSE U-Net and VM-Unet CBAM+ model, two\ncutting-edge deep learning architectures designed to enhance medical image\nsegmentation. Our approach integrates Squeeze-and-Excitation (SE) and\nConvolutional Block Attention Module (CBAM) techniques into the traditional VM\nU-Net framework, significantly improving segmentation accuracy, feature\nlocalization, and computational efficiency. Both models show superior\nperformance compared to the baseline VM-Unet across multiple datasets. Notably,\nVMSEUnet achieves the highest accuracy, IoU, precision, and recall while\nmaintaining low loss values. It also exhibits exceptional computational\nefficiency with faster inference times and lower memory usage on both GPU and\nCPU. Overall, the study suggests that the enhanced architecture VMSE-Unet is a\nvaluable tool for medical image analysis. These findings highlight its\npotential for real-world clinical applications, emphasizing the importance of\nfurther research to optimize accuracy, robustness, and computational\nefficiency.", "comment": "under review", "pdf_url": "http://arxiv.org/pdf/2507.00511v1", "categories": ["eess.IV", "cs.CV", "cs.LG"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2507.00511v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00631", "title": "Horus: A Protocol for Trustless Delegation Under Uncertainty", "authors": ["David Shi", "Kevin Joo"], "summary": "Correctness is an emergent property of systems where exposing error is\ncheaper than committing it. In dynamic, low-trust environments, autonomous AI\nagents benefit from delegating work to sub-agents, yet correctness cannot be\nassured through upfront specification or centralized oversight. We propose a\nprotocol that enforces correctness through collateralized claims in a recursive\nverification game. Tasks are published as intents, and solvers compete to\nfulfill them. Selected solvers carry out tasks under risk, with correctness\nchecked post hoc by verifiers. Any challenger can challenge a result by staking\nagainst it to trigger the verification process. Incorrect agents are slashed\nand correct opposition is rewarded, with an escalation path that penalizes\nerroneous verifiers themselves. When incentives are aligned across solvers,\nchallengers, and verifiers, falsification conditions make correctness the Nash\nequilibrium.", "comment": "9 pages, 1 figure", "pdf_url": "http://arxiv.org/pdf/2507.00631v1", "categories": ["cs.GT", "cs.AI", "cs.MA", "I.2.11; F.2.2"], "cate": "cs.GT", "url": "http://arxiv.org/abs/2507.00631v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00286", "title": "Visual Privacy Management with Generative AI for Blind and Low-Vision People", "authors": ["Tanusree Sharma", "Yu-Yun Tseng", "Lotus Zhang", "Ayae Ide", "Kelly Avery Mack", "Leah Findlater", "Danna Gurari", "Yang Wang"], "summary": "Blind and low vision (BLV) individuals use Generative AI (GenAI) tools to\ninterpret and manage visual content in their daily lives. While such tools can\nenhance the accessibility of visual content and so enable greater user\nindependence, they also introduce complex challenges around visual privacy. In\nthis paper, we investigate the current practices and future design preferences\nof blind and low vision individuals through an interview study with 21\nparticipants. Our findings reveal a range of current practices with GenAI that\nbalance privacy, efficiency, and emotional agency, with users accounting for\nprivacy risks across six key scenarios, such as self-presentation,\nindoor/outdoor spatial privacy, social sharing, and handling professional\ncontent. Our findings reveal design preferences, including on-device\nprocessing, zero-retention guarantees, sensitive content redaction,\nprivacy-aware appearance indicators, and multimodal tactile mirrored\ninteraction methods. We conclude with actionable design recommendations to\nsupport user-centered visual privacy through GenAI, expanding the notion of\nprivacy and responsible handling of others data.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00286v1", "categories": ["cs.HC", "cs.AI", "cs.ET"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2507.00286v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00949", "title": "How Fast Can Graph Computations Go on Fine-grained Parallel Architectures", "authors": ["Yuqing Wang", "Charles Colley", "Brian Wheatman", "Jiya Su", "David F. Gleich", "Andrew A. Chien"], "summary": "Large-scale graph problems are of critical and growing importance and\nhistorically parallel architectures have provided little support. In the spirit\nof co-design, we explore the question, How fast can graph computing go on a\nfine-grained architecture? We explore the possibilities of an architecture\noptimized for fine-grained parallelism, natural programming, and the\nirregularity and skew found in real-world graphs. Using two graph benchmarks,\nPageRank (PR) and Breadth-First Search (BFS), we evaluate a Fine-Grained Graph\narchitecture, UpDown, to explore what performance codesign can achieve. To\ndemonstrate programmability, we wrote five variants of these algorithms.\nSimulations of up to 256 nodes (524,288 lanes) and projections to 16,384 nodes\n(33M lanes) show the UpDown system can achieve 637K GTEPS PR and 989K GTEPS BFS\non RMAT, exceeding the best prior results by 5x and 100x respectively.", "comment": "13 pages, 11 figures, 6 tables", "pdf_url": "http://arxiv.org/pdf/2507.00949v1", "categories": ["cs.DC", "cs.AR"], "cate": "cs.DC", "url": "http://arxiv.org/abs/2507.00949v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00261", "title": "VirtualFencer: Generating Fencing Bouts based on Strategies Extracted from In-the-Wild Videos", "authors": ["Zhiyin Lin", "Purvi Goel", "Joy Yun", "C. Karen Liu", "Joao Pedro Araujo"], "summary": "Fencing is a sport where athletes engage in diverse yet strategically logical\nmotions. While most motions fall into a few high-level actions (e.g. step,\nlunge, parry), the execution can vary widely-fast vs. slow, large vs. small,\noffensive vs. defensive. Moreover, a fencer's actions are informed by a\nstrategy that often comes in response to the opponent's behavior. This\ncombination of motion diversity with underlying two-player strategy motivates\nthe application of data-driven modeling to fencing. We present VirtualFencer, a\nsystem capable of extracting 3D fencing motion and strategy from in-the-wild\nvideo without supervision, and then using that extracted knowledge to generate\nrealistic fencing behavior. We demonstrate the versatile capabilities of our\nsystem by having it (i) fence against itself (self-play), (ii) fence against a\nreal fencer's motion from online video, and (iii) fence interactively against a\nprofessional fencer.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00261v1", "categories": ["cs.CV", "cs.GR"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00261v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00067", "title": "The gradual transformation of inland countries -- human plowing, horse plowing and equity incentives", "authors": ["Hongfa Zi", "Zhen Liu"], "summary": "Many modern countries have not learned their lessons and often hope for the\nwisdom of later generations, resulting in them only possessing modern\ntechnology and difficult to iterate ancient civilizations. At present, there is\nno way to tell how we should learn from history and promote the gradual\nupgrading of civilization. Therefore, we must tell the history of\ncivilization's progress and the means of governance, learn from experience to\nimprove the comprehensive strength and survival ability of civilization, and\nachieve an optimal solution for the tempering brought by conflicts and the\nreduction of internal conflicts. Firstly, we must follow the footsteps of\nhistory and explore the reasons for the long-term stability of each country in\nconflict, including providing economic benefits to the people and means of\nsuppressing them; then, use mathematical methods to demonstrate how we can\nachieve the optimal solution at the current stage. After analysis, we can\nconclude that the civilization transformed from human plowing to horse plowing\ncan easily suppress the resistance of the people and provide them with the\nability to resist; The selection of rulers should consider multiple\ninstitutional aspects, such as exams, elections, and drawing lots; Economic\ndevelopment follows a lognormal distribution and can be adjusted by expected\nvalue and variance. Using a lognormal distribution with the maximum value to\ndivide equity can adjust the wealth gap.", "comment": "9 pages,2 figures", "pdf_url": "http://arxiv.org/pdf/2507.00067v1", "categories": ["physics.soc-ph", "cs.CE", "econ.GN", "q-fin.EC"], "cate": "physics.soc-ph", "url": "http://arxiv.org/abs/2507.00067v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2507.00059", "title": "Verification of Hamiltonian Path Conjecture (BHR Conjecture) for Integers up to p=31", "authors": ["Ranjan N Naik"], "summary": "The BHR (Buratti-Horak-Rosa) Conjecture (2006) proposes that for every p and\na multiset L of (p-1) positive integers modulo p, there exists a Hamiltonian\npath in the Complete Graph Kp with consecutive edge lengths given by the\nelements of L. In this article, we outline an approach to the conjecture based\non frequency partitions and local/global adjustment operations and\nbacktracking. We describe the mathematical strategy, experimental evidence, and\nimplementation in a Python Program to explore valid Hamiltonian paths p < 37.\nThis is a result an improvement over by Mariusz Meszka for all primes up to 23\n(included) with the aid of a computer.", "comment": "This is a result an improvement over by Mariusz Meszka for all primes\n  up to 23 (included) with the aid of a computer", "pdf_url": "http://arxiv.org/pdf/2507.00059v1", "categories": ["cs.DM", "cs.DS", "math.CO"], "cate": "cs.DM", "url": "http://arxiv.org/abs/2507.00059v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2507.00415", "title": "Minimal Construction of Graphs with Maximum Robustness", "authors": ["Haejoon Lee", "Dimitra Panagou"], "summary": "The notions of network $r$-robustness and $(r,s)$-robustness have been\nearlier introduced in the literature to achieve resilient control in the\npresence of misbehaving agents. However, while higher robustness levels provide\nnetworks with higher tolerances against the misbehaving agents, they also\nrequire dense communication structures, which are not always desirable for\nsystems with limited capabilities and energy capacities. Therefore, this paper\nstudies the fundamental structures behind $r$-robustness and $(r,s)$-\nrobustness properties in two different ways. (a) We first explore and establish\nthe tight necessary conditions on the number of edges for undirected graphs\nwith any nodes must satisfy to achieve maximum $r$- and $(r,s)$-robustness. (b)\nWe then use these conditions to construct two classes of undirected graphs,\nreferred as to $\\gamma$- and $(\\gamma,\\gamma)$-Minimal Edge Robust Graphs\n(MERGs), that provably achieve maximum robustness with minimal numbers of\nedges. We finally validate our work through some sets of simulations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00415v1", "categories": ["eess.SY", "cs.SI", "cs.SY"], "cate": "eess.SY", "url": "http://arxiv.org/abs/2507.00415v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00571", "title": "Delay Bound Relaxation with Deep Learning-based Haptic Estimation for Tactile Internet", "authors": ["Georgios Kokkinis", "Alexandros Iosifidis", "Qi Zhang"], "summary": "Haptic teleoperation typically demands sub-millisecond latency and ultra-high\nreliability (99.999%) in Tactile Internet. At a 1 kHz haptic signal sampling\nrate, this translates into an extremely high packet transmission rate, posing\nsignificant challenges for timely delivery and introducing substantial\ncomplexity and overhead in radio resource allocation. To address this critical\nchallenge, we introduce a novel DL modelthat estimates force feedback using\nmulti-modal input, i.e. both force measurements from the remote side and local\noperator motion signals. The DL model can capture complex temporal features of\nhaptic time-series with the use of CNN and LSTM layers, followed by a\ntransformer encoder, and autoregressively produce a highly accurate estimation\nof the next force values for different teleoperation activities. By ensuring\nthat the estimation error is within a predefined threshold, the teleoperation\nsystem can safely relax its strict delay requirements. This enables the\nbatching and transmission of multiple haptic packets within a single resource\nblock, improving resource efficiency and facilitating scheduling in resource\nallocation. Through extensive simulations, we evaluated network performance in\nterms of reliability and capacity. Results show that, for both dynamic and\nrigid object interactions, the proposed method increases the number of reliably\nserved users by up to 66%.", "comment": "6 pages, 6 figures, 1 table, conference paper submitted in\n  GLOBECOM2025", "pdf_url": "http://arxiv.org/pdf/2507.00571v1", "categories": ["eess.SP"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2507.00571v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00458", "title": "Mitigating Language Mismatch in SSL-Based Speaker Anonymization", "authors": ["Zhe Zhang", "Wen-Chin Huang", "Xin Wang", "Xiaoxiao Miao", "Junichi Yamagishi"], "summary": "Speaker anonymization aims to protect speaker identity while preserving\ncontent information and the intelligibility of speech. However, most speaker\nanonymization systems (SASs) are developed and evaluated using only English,\nresulting in degraded utility for other languages. This paper investigates\nlanguage mismatch in SASs for Japanese and Mandarin speech. First, we fine-tune\na self-supervised learning (SSL)-based content encoder with Japanese speech to\nverify effective language adaptation. Then, we propose fine-tuning a\nmultilingual SSL model with Japanese speech and evaluating the SAS in Japanese\nand Mandarin. Downstream experiments show that fine-tuning an English-only SSL\nmodel with the target language enhances intelligibility while maintaining\nprivacy and that multilingual SSL further extends SASs' utility across\ndifferent languages. These findings highlight the importance of language\nadaptation and multilingual pre-training of SSLs for robust multilingual\nspeaker anonymization.", "comment": "Accepted to Interspeech 2025", "pdf_url": "http://arxiv.org/pdf/2507.00458v1", "categories": ["eess.AS", "cs.SD"], "cate": "eess.AS", "url": "http://arxiv.org/abs/2507.00458v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00210", "title": "LineRetriever: Planning-Aware Observation Reduction for Web Agents", "authors": ["Imene Kerboua", "Sahar Omidi Shayegan", "Megh Thakkar", "Xing Han Lù", "Massimo Caccia", "Véronique Eglin", "Alexandre Aussem", "Jérémy Espinas", "Alexandre Lacoste"], "summary": "While large language models have demonstrated impressive capabilities in web\nnavigation tasks, the extensive context of web pages, often represented as DOM\nor Accessibility Tree (AxTree) structures, frequently exceeds model context\nlimits. Current approaches like bottom-up truncation or embedding-based\nretrieval lose critical information about page state and action history. This\nis particularly problematic for adaptive planning in web agents, where\nunderstanding the current state is essential for determining future actions. We\nhypothesize that embedding models lack sufficient capacity to capture\nplan-relevant information, especially when retrieving content that supports\nfuture action prediction. This raises a fundamental question: how can retrieval\nmethods be optimized for adaptive planning in web navigation tasks? In\nresponse, we introduce \\textit{LineRetriever}, a novel approach that leverages\na language model to identify and retrieve observation lines most relevant to\nfuture navigation steps. Unlike traditional retrieval methods that focus solely\non semantic similarity, \\textit{LineRetriever} explicitly considers the\nplanning horizon, prioritizing elements that contribute to action prediction.\nOur experiments demonstrate that \\textit{LineRetriever} can reduce the size of\nthe observation at each step for the web agent while maintaining consistent\nperformance within the context limitations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00210v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2507.00210v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00461", "title": "Novel Complex-Valued Hopfield Neural Networks with Phase and Magnitude Quantization", "authors": ["Garimella Ramamurthy", "Marcos Eduardo Valle", "Tata Jagannadha Swamy"], "summary": "This research paper introduces two novel complex-valued Hopfield neural\nnetworks (CvHNNs) that incorporate phase and magnitude quantization. The first\nCvHNN employs a ceiling-type activation function that operates on the\nrectangular coordinate representation of the complex net contribution. The\nsecond CvHNN similarly incorporates phase and magnitude quantization but\nutilizes a ceiling-type activation function based on the polar coordinate\nrepresentation of the complex net contribution. The proposed CvHNNs, with their\nphase and magnitude quantization, significantly increase the number of states\ncompared to existing models in the literature, thereby expanding the range of\npotential applications for CvHNNs.", "comment": "Paper submitted to the Fifth International Conference on Emerging\n  Techniques in Computational Intelligence (ICETCI 2025)", "pdf_url": "http://arxiv.org/pdf/2507.00461v1", "categories": ["cs.NE", "cs.AI"], "cate": "cs.NE", "url": "http://arxiv.org/abs/2507.00461v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00479", "title": "On Mitigating Data Sparsity in Conversational Recommender Systems", "authors": ["Sixiao Zhang", "Mingrui Liu", "Cheng Long", "Wei Yuan", "Hongxu Chen", "Xiangyu Zhao", "Hongzhi Yin"], "summary": "Conversational recommender systems (CRSs) capture user preference through\ntextual information in dialogues. However, they suffer from data sparsity on\ntwo fronts: the dialogue space is vast and linguistically diverse, while the\nitem space exhibits long-tail and sparse distributions. Existing methods\nstruggle with (1) generalizing to varied dialogue expressions due to\nunderutilization of rich textual cues, and (2) learning informative item\nrepresentations under severe sparsity. To address these problems, we propose a\nCRS model named DACRS. It consists of three modules, namely Dialogue\nAugmentation, Knowledge-Guided Entity Modeling, and Dialogue-Entity Matching.\nIn the Dialogue Augmentation module, we apply a two-stage augmentation pipeline\nto augment the dialogue context to enrich the data and improve\ngeneralizability. In the Knowledge-Guided Entity Modeling, we propose a\nknowledge graph (KG) based entity substitution and an entity similarity\nconstraint to enhance the expressiveness of entity embeddings. In the\nDialogue-Entity Matching module, we fuse the dialogue embedding with the\nmentioned entity embeddings through a dialogue-guided attention aggregation to\nacquire user embeddings that contain both the explicit and implicit user\npreferences. Extensive experiments on two public datasets demonstrate the\nstate-of-the-art performance of DACRS.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00479v1", "categories": ["cs.IR"], "cate": "cs.IR", "url": "http://arxiv.org/abs/2507.00479v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00193", "title": "An energy-stable parametric finite element method for Willmore flow with normal-tangential velocity splitting", "authors": ["Harald Garcke", "Robert Nürnberg", "Quan Zhao"], "summary": "We propose and analyze an energy-stable fully discrete parametric\napproximation for Willmore flow of hypersurfaces in two and three space\ndimensions. We allow for the presence of spontaneous curvature effects and for\nopen surfaces with boundary. The presented scheme is based on a new geometric\npartial differential equation (PDE) that combines an evolution equation for the\nmean curvature with a separate equation that prescribes the tangential\nvelocity. The mean curvature is used to determine the normal velocity within\nthe gradient flow structure, thus guaranteeing an unconditional energy\nstability for the discrete solution upon suitable discretization. We introduce\na novel weak formulation for this geometric PDE, in which different types of\nboundary conditions can be naturally enforced. We further discretize the weak\nformulation to obtain a fully discrete parametric finite element method, for\nwhich well-posedness can be rigorously shown. Moreover, the constructed scheme\nadmits an unconditional stability estimate in terms of the discrete energy.\nExtensive numerical experiments are reported to showcase the accuracy and\nrobustness of the proposed method for computing Willmore flow of both curves in\n$\\mathbb{R}^2$ and surfaces in $\\mathbb{R}^3$.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00193v1", "categories": ["math.NA", "cs.NA", "65M60, 65M15, 65M12, 35R01"], "cate": "math.NA", "url": "http://arxiv.org/abs/2507.00193v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00466", "title": "Beat and Downbeat Tracking in Performance MIDI Using an End-to-End Transformer Architecture", "authors": ["Sebastian Murgul", "Michael Heizmann"], "summary": "Beat tracking in musical performance MIDI is a challenging and important task\nfor notation-level music transcription and rhythmical analysis, yet existing\nmethods primarily focus on audio-based approaches. This paper proposes an\nend-to-end transformer-based model for beat and downbeat tracking in\nperformance MIDI, leveraging an encoder-decoder architecture for\nsequence-to-sequence translation of MIDI input to beat annotations. Our\napproach introduces novel data preprocessing techniques, including dynamic\naugmentation and optimized tokenization strategies, to improve accuracy and\ngeneralizability across different datasets. We conduct extensive experiments\nusing the A-MAPS, ASAP, GuitarSet, and Leduc datasets, comparing our model\nagainst state-of-the-art hidden Markov models (HMMs) and deep learning-based\nbeat tracking methods. The results demonstrate that our model outperforms\nexisting symbolic music beat tracking approaches, achieving competitive\nF1-scores across various musical styles and instruments. Our findings highlight\nthe potential of transformer architectures for symbolic beat tracking and\nsuggest future integration with automatic music transcription systems for\nenhanced music analysis and score generation.", "comment": "Accepted to the 22nd Sound and Music Computing Conference (SMC), 2025", "pdf_url": "http://arxiv.org/pdf/2507.00466v1", "categories": ["cs.SD", "cs.CL", "cs.MM", "eess.AS"], "cate": "cs.SD", "url": "http://arxiv.org/abs/2507.00466v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00740", "title": "Safe Low Bandwidth SPV: A Formal Treatment of Simplified Payment Verification Protocols and Security Bounds", "authors": ["Craig S Wright"], "summary": "This paper presents a complete formal specification, protocol description,\nand mathematical proof structure for Simplified Payment Verification (SPV) as\noriginally defined in the Bitcoin whitepaper \\cite{nakamoto2008}. In stark\ncontrast to the misrepresentations proliferated by popular implementations, we\nshow that SPV is not only secure under bounded adversarial assumptions but\nstrictly optimal for digital cash systems requiring scalable and verifiable\ntransaction inclusion. We reconstruct the SPV protocol from first principles,\ngrounding its verification model in symbolic automata, Merkle membership\nrelations, and chain-of-proof dominance predicates. Through rigorous\nprobabilistic and game-theoretic analysis, we derive the economic bounds within\nwhich the protocol operates securely and verify its liveness and safety\nproperties under partial connectivity, hostile relay networks, and adversarial\npropagation delay. Our specification further introduces low-bandwidth\noptimisations such as adaptive polling and compressed header synchronisation\nwhile preserving correctness. This document serves both as a blueprint for\nsecure SPV implementation and a rebuttal of common misconceptions surrounding\nnon-validating clients.", "comment": "56 pages 5 images", "pdf_url": "http://arxiv.org/pdf/2507.00740v1", "categories": ["cs.CR", "cs.CL", "cs.DC", "68Q85, 68M10, 94A60, 91A80, 68Q17, 68W10, 68R10", "C.2.2; F.2.2; D.4.6; K.6.5"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2507.00740v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00496", "title": "Coverage-Guided Testing for Deep Learning Models: A Comprehensive Survey", "authors": ["Hongjing Guo", "Chuanqi Tao", "Zhiqiu Huang", "Weiqin Zou"], "summary": "As Deep Learning (DL) models are increasingly applied in safety-critical\ndomains, ensuring their quality has emerged as a pressing challenge in modern\nsoftware engineering. Among emerging validation paradigms, coverage-guided\ntesting (CGT) has gained prominence as a systematic framework for identifying\nerroneous or unexpected model behaviors. Despite growing research attention,\nexisting CGT studies remain methodologically fragmented, limiting the\nunderstanding of current advances and emerging trends. This work addresses that\ngap through a comprehensive review of state-of-the-art CGT methods for DL\nmodels, including test coverage analysis, coverage-guided test input\ngeneration, and coverage-guided test input optimization. This work provides\ndetailed taxonomies to organize these methods based on methodological\ncharacteristics and application scenarios. We also investigate evaluation\npractices adopted in existing studies, including the use of benchmark datasets,\nmodel architectures, and evaluation aspects. Finally, open challenges and\nfuture directions are highlighted in terms of the correlation between\nstructural coverage and testing objectives, method generalizability across\ntasks and models, practical deployment concerns, and the need for standardized\nevaluation and tool support. This work aims to provide a roadmap for future\nacademic research and engineering practice in DL model quality assurance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00496v1", "categories": ["cs.SE"], "cate": "cs.SE", "url": "http://arxiv.org/abs/2507.00496v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00172", "title": "Intellectual Property Rights and Entrepreneurship in the NFT Ecosystem: Legal Frameworks, Business Models, and Innovation Opportunities", "authors": ["Pranav Darshan", "Rohan J S", "Raghuveer Rajesh", "Ruchitha M", "Sanika Kamath", "Manas M N"], "summary": "Non Fungible Tokens have changed digital ownership and how creators earn\nmoney. Between 2021 and 2024, the market value exceeded 40 billion. However,\nthe fast growth of the NFT ecosystem has revealed serious issues in managing\nintellectual property rights. There is a lot of confusion about the difference\nbetween owning an NFT and owning the copyright for the underlying content. This\nresearch looks at the gap between traditional copyright laws and\nblockchain-based transactions. We use a mixed methods approach to analyze this\ndisconnect. We create a new IP rights matrix that clearly shows how copyright\nlaw relates to NFT ownership structures. Additionally, we include a business\nmodel taxonomy that sorts new commercial applications by their IP risk and\nsustainability factors. By examining important legal cases, smart contracts,\nand interviews with stakeholders, we find key problems in enforcing laws across\ndifferent regions, standardizing licenses, and assessing business\nopportunities.", "comment": "11 pages", "pdf_url": "http://arxiv.org/pdf/2507.00172v1", "categories": ["cs.CY", "cs.ET"], "cate": "cs.CY", "url": "http://arxiv.org/abs/2507.00172v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00416", "title": "Evo-0: Vision-Language-Action Model with Implicit Spatial Understanding", "authors": ["Tao Lin", "Gen Li", "Yilei Zhong", "Yanwen Zou", "Bo Zhao"], "summary": "Vision-Language-Action (VLA) models have emerged as a promising framework for\nenabling generalist robots capable of perceiving, reasoning, and acting in the\nreal world. These models usually build upon pretrained Vision-Language Models\n(VLMs), which excel at semantic understanding due to large-scale text\npretraining. However, VLMs typically lack precise spatial understanding\ncapabilities, as they are primarily tuned on 2D image-text pairs without 3D\nsupervision. To address this limitation, recent approaches have incorporated\nexplicit 3D inputs such as point clouds or depth maps, but this necessitates\nadditional depth sensors or defective estimation. In contrast, our work\nintroduces a plug-and-play module that implicitly injects 3D geometry features\ninto VLA models by leveraging an off-the-shelf visual geometry foundation\nmodels. We design five spatially challenging tasks that require precise spatial\nunderstanding ability to validate effectiveness of our method. Extensive\nevaluations show that our method significantly improves the performance of\nstate-of-the-art VLA models across diverse scenarios.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00416v1", "categories": ["cs.RO", "cs.CV"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2507.00416v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00003", "title": "Deciding When Not to Decide: Indeterminacy-Aware Intrusion Detection with NeutroSENSE", "authors": ["Eyhab Al-Masri"], "summary": "This paper presents NeutroSENSE, a neutrosophic-enhanced ensemble framework\nfor interpretable intrusion detection in IoT environments. By integrating\nRandom Forest, XGBoost, and Logistic Regression with neutrosophic logic, the\nsystem decomposes prediction confidence into truth (T), falsity (F), and\nindeterminacy (I) components, enabling uncertainty quantification and\nabstention. Predictions with high indeterminacy are flagged for review using\nboth global and adaptive, class-specific thresholds. Evaluated on the IoT-CAD\ndataset, NeutroSENSE achieved 97% accuracy, while demonstrating that\nmisclassified samples exhibit significantly higher indeterminacy (I = 0.62)\nthan correct ones (I = 0.24). The use of indeterminacy as a proxy for\nuncertainty enables informed abstention and targeted review-particularly\nvaluable in edge deployments. Figures and tables validate the correlation\nbetween I-scores and error likelihood, supporting more trustworthy,\nhuman-in-the-loop AI decisions. This work shows that neutrosophic logic\nenhances both accuracy and explainability, providing a practical foundation for\ntrust-aware AI in edge and fog-based IoT security systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00003v1", "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.NI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00003v1", "date": "2025-06-05", "updated": "2025-06-05"}
{"id": "2507.00656", "title": "The Rate-Distortion Function for Sampled Cyclostationary Gaussian Processes with Memory and with Bounded Processing Delay: Extended Version with Proofs", "authors": ["Zikun Tan", "Ron Dabora", "H. Vincent Poor"], "summary": "We study the rate-distortion function (RDF) for the lossy compression of\ndiscrete-time (DT) wide-sense almost cyclostationary (WSACS) Gaussian processes\nwith memory, arising from sampling continuous-time (CT) wide-sense\ncyclostationary (WSCS) Gaussian source processes. The importance of this\nproblem arises as such CT processes represent communications signals, and\nsampling must be applied to facilitate the DT processing associated with their\ncompression. Moreover, the physical characteristics of oscillators imply that\nthe sampling interval is incommensurate with the period of the autocorrelation\nfunction (AF) of the physical process, giving rise to the DT WSACS model\nconsidered. In addition, to reduce the loss, the sampling interval is generally\nshorter than the correlation length, and thus, the DT process is correlated as\nwell. The difficulty in the RDF characterization follows from the\ninformation-instability of WSACS processes, which renders the traditional\ninformation-theoretic tools inapplicable. In this work we utilize the\ninformation-spectrum framework to characterize the RDF when a finite and\nbounded delay is allowed between processing of subsequent source sequences.\nThis scenario extends our previous works which studied settings without\nprocessing delays or without memory. Numerical evaluations reveal the impact of\nscenario parameters on the RDF with asynchronous sampling.", "comment": "accepted by the 2025 IEEE International Symposium on Information\n  Theory (ISIT)", "pdf_url": "http://arxiv.org/pdf/2507.00656v1", "categories": ["cs.IT", "math.IT"], "cate": "cs.IT", "url": "http://arxiv.org/abs/2507.00656v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00092", "title": "Thinking About Thinking: SAGE-nano's Inverse Reasoning for Self-Aware Language Models", "authors": ["Basab Jha", "Firoj Paudel", "Ujjwal Puri", "Zhang Yuting", "Choi Donghyuk", "Wang Junhao"], "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities at\nsolving complex reasoning tasks with Chain-of-Thought (CoT) prompting, but\ntheir decision-making processes remain somewhat blackbox. We introduce\ntextbfinverse reasoning, a novel paradigm enabling LLMs to decompose and\nexplain their own reasoning chains post-hoc. Our approach, used in SAGE-nano, a\n4-billion-parameter reasoning model, employs a metacognitive structure that\nreflects back via attention processes to identify major decision points and\ngenerate explanations of reasoning choices. While typical CoT approaches are\ndirected towards forward reasoning generation, inverse reasoning provides\ninsight into why specific reasoning chains were selected over others. Through\nthorough testing of logical reasoning puzzles, math problems and ethical\ndilemmas from AQUA-RAT, CommonsenseQA, and customized benchmarks, we\ndemonstrate that SAGE-nano is at the cutting edge both on reasoning accuracy\n(74.6% on AQUA-RAT) and explanation quality (92.1% human preference score) for\nits task, and offers performance almost on par with models like Claude-3.5\nSonnet or GPT-4o. Our contributions are: (i) the first rigorous framework for\nLLM self-reflection via inverse reasoning, (ii) a novel metalearning framework\nto reverse the attention flow, (iii) comprehensive evaluation frameworks for\nreasoning transparency, and (iv) evidence that increasing reasoning using\ninverse reasoning improves interpretability along with reasoning performance.\nOur work creates new avenues for transparent AI systems and closes significant\ngaps in AI safety, education, and scientific discovery.", "comment": "19 pages, 2 figures, 9 tables", "pdf_url": "http://arxiv.org/pdf/2507.00092v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2507.00092v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00014", "title": "SWE-Bench-CL: Continual Learning for Coding Agents", "authors": ["Thomas Joshi", "Shayan Chowdhury", "Fatih Uysal"], "summary": "Large Language Models (LLMs) have achieved impressive results on static\ncode-generation benchmarks, but real-world software development unfolds as a\ncontinuous stream of evolving issues, fixes, and feature requests. We introduce\nSWE-Bench-CL, a novel continual learning benchmark built on the human-verified\nSWE-Bench Verified dataset introduced by OpenAI and Princeton-NLP in 2024. By\norganizing GitHub issues into chronologically ordered sequences that reflect\nnatural repository evolution, SWE-Bench-CL enables direct evaluation of an\nagent's ability to accumulate experience, transfer knowledge across tasks, and\nresist catastrophic forgetting. We complement the dataset with (i) a\npreliminary analysis of inter-task structural similarity and contextual\nsensitivity, (ii) an interactive LangGraph-based evaluation framework augmented\nwith a FAISS-backed semantic memory module, and (iii) a suite of specialized\ncontinual learning metrics -- including average accuracy, forgetting,\nforward/backward transfer, tool-use efficiency, and a generalized Composite\nContinual Learning Score and CL-F-beta score -- to capture the\nstability-plasticity trade-off. We outline a rigorous experimental protocol\ncomparing memory-enabled and memory-disabled agents across diverse Python\nrepositories. All code and data are publicly available at\nhttps://github.com/thomasjoshi/agents-never-forget, providing the community\nwith a reproducible platform for developing more adaptive and robust AI agents\nin software engineering.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00014v1", "categories": ["cs.LG", "cs.AI", "cs.SE"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00014v1", "date": "2025-06-13", "updated": "2025-06-13"}
{"id": "2507.00716", "title": "Accelerating Loading WebGraphs in ParaGrapher", "authors": ["Mohsen Koohi Esfahani"], "summary": "ParaGrapher is a graph loading API and library that enables graph processing\nframeworks to load large-scale compressed graphs with minimal overhead. This\ncapability accelerates the design and implementation of new high-performance\ngraph algorithms and their evaluation on a wide range of graphs and across\ndifferent frameworks. However, our previous study identified two major\nlimitations in ParaGrapher: inefficient utilization of high-bandwidth storage\nand reduced decompression bandwidth due to increased compression ratios. To\naddress these limitations, we present two optimizations for ParaGrapher in this\npaper. To improve storage utilization, particularly for high-bandwidth storage,\nwe introduce ParaGrapher-FUSE (PG-Fuse) a filesystem based on the FUSE\n(Filesystem in User Space). PG-Fuse optimizes storage access by increasing the\nsize of requested blocks, reducing the number of calls to the underlying\nfilesystem, and caching the received blocks in memory for future calls. To\nimprove the decompression bandwidth, we introduce CompBin, a compact binary\nrepresentation of the CSR format. CompBin facilitates direct accesses to\nneighbors while preventing storage usage for unused bytes. Our evaluation on 12\nreal-world and synthetic graphs with up to 128 billion edges shows that PG-Fuse\nand CompBin achieve up to 7.6 and 21.8 times speedup, respectively.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00716v1", "categories": ["cs.DC"], "cate": "cs.DC", "url": "http://arxiv.org/abs/2507.00716v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00049", "title": "AdaDeDup: Adaptive Hybrid Data Pruning for Efficient Large-Scale Object Detection Training", "authors": ["Feiyang Kang", "Nadine Chang", "Maying Shen", "Marc T. Law", "Rafid Mahmood", "Ruoxi Jia", "Jose M. Alvarez"], "summary": "The computational burden and inherent redundancy of large-scale datasets\nchallenge the training of contemporary machine learning models. Data pruning\noffers a solution by selecting smaller, informative subsets, yet existing\nmethods struggle: density-based approaches can be task-agnostic, while\nmodel-based techniques may introduce redundancy or prove computationally\nprohibitive. We introduce Adaptive De-Duplication (AdaDeDup), a novel hybrid\nframework that synergistically integrates density-based pruning with\nmodel-informed feedback in a cluster-adaptive manner. AdaDeDup first partitions\ndata and applies an initial density-based pruning. It then employs a proxy\nmodel to evaluate the impact of this initial pruning within each cluster by\ncomparing losses on kept versus pruned samples. This task-aware signal\nadaptively adjusts cluster-specific pruning thresholds, enabling more\naggressive pruning in redundant clusters while preserving critical data in\ninformative ones. Extensive experiments on large-scale object detection\nbenchmarks (Waymo, COCO, nuScenes) using standard models (BEVFormer, Faster\nR-CNN) demonstrate AdaDeDup's advantages. It significantly outperforms\nprominent baselines, substantially reduces performance degradation (e.g., over\n54% versus random sampling on Waymo), and achieves near-original model\nperformance while pruning 20% of data, highlighting its efficacy in enhancing\ndata efficiency for large-scale model training. Code is open-sourced.", "comment": "Preprint", "pdf_url": "http://arxiv.org/pdf/2507.00049v1", "categories": ["cs.CV", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00049v1", "date": "2025-06-24", "updated": "2025-06-24"}
{"id": "2507.00004", "title": "A Theory of Inference Compute Scaling: Reasoning through Directed Stochastic Skill Search", "authors": ["Austin R. Ellis-Mohr", "Anuj K. Nayak", "Lav R. Varshney"], "summary": "Large language models (LLMs) demand considerable computational, energy, and\nfinancial resources during both training and deployment. While scaling laws for\ntraining have guided much of the field's recent progress, inference costs now\nrepresent a significant and growing component of the overall resource burden,\nparticularly for reasoning-focused models. Existing characterizations of\ncompute-optimality that consider model size, dataset size, and inference tokens\nin isolation or in fixed combinations risk overlooking more efficient operating\npoints. We introduce directed stochastic skill search (DS3), a general\nframework that represents inference as stochastic traversal over a learned\nskill graph. From a simplified yet expressive instantiation, we derive\nclosed-form expressions for task success and compute cost across a wide range\nof inference strategies -- including chain-of-thought (CoT) and tree-of-thought\n(ToT) -- enabling comparative analysis as a function of task difficulty and\nmodel capability. To that end, we extend a prior first-principles tripartite\ngraph framework of LLM training to incorporate inference, and separately bridge\nDS3 with empirical methods that characterize LLM scaling behavior. We\ntheoretically recover empirically observed patterns, including: linear accuracy\nscaling with logarithmic compute; variation in preferred inference strategies\nas a function of task difficulty and model capability; emergent behavior\nelicited by reasoning even when performance plateaus under parameter scaling;\nand both best-of-N (BoN) and majority voting behavior captured within a unified\nanalytical framework. By explicitly characterizing training-inference\ninterdependencies, our framework deepens theoretical understanding and supports\nprincipled algorithmic design and resource allocation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00004v1", "categories": ["cs.LG", "cs.AI", "cs.CY", "cs.PF"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00004v1", "date": "2025-06-10", "updated": "2025-06-10"}
{"id": "2507.00527", "title": "Anti-aliasing Algorithm Based on Three-dimensional Display Image", "authors": ["Ziyang Liu", "Xingchen Xiao", "Yueyang Xu"], "summary": "3D-display technology has been a promising emerging area with potential to be\nthe core of next-generation display technology. When directly observing\nunprocessed images and text through a naked-eye 3D display device, severe\ndistortion and jaggedness will be displayed, which will make the display effect\nmuch worse. In this work, we try to settle down such degradation with spatial\nand frequency processing, furthermore, we make efforts to extract degenerate\nfunction of columnar lens array thus fundamentally eliminating degradation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00527v1", "categories": ["eess.IV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2507.00527v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00875", "title": "TransLaw: Benchmarking Large Language Models in Multi-Agent Simulation of the Collaborative Translation", "authors": ["Xi Xuan", "King-kui Sin", "Yufei Zhou", "Chunyu Kit"], "summary": "Multi-agent systems empowered by large language models (LLMs) have\ndemonstrated remarkable capabilities in a wide range of downstream\napplications, including machine translation. However, the potential of LLMs in\ntranslating Hong Kong legal judgments remains uncertain due to challenges such\nas intricate legal terminology, culturally embedded nuances, and strict\nlinguistic structures. In this work, we introduce TransLaw, a novel multi-agent\nframework implemented for real-world Hong Kong case law translation. It employs\nthree specialized agents, namely, Translator, Annotator, and Proofreader, to\ncollaboratively produce translations for high accuracy in legal meaning,\nappropriateness in style, and adequate coherence and cohesion in structure.\nThis framework supports customizable LLM configurations and achieves tremendous\ncost reduction compared to professional human translation services. We\nevaluated its performance using 13 open-source and commercial LLMs as agents\nand obtained interesting findings, including that it surpasses GPT-4o in legal\nsemantic accuracy, structural coherence, and stylistic fidelity, yet trails\nhuman experts in contextualizing complex terminology and stylistic naturalness.\nOur platform website is available at CityUHK, and our bilingual judgment corpus\nused for the evaluation is available at Hugging Face.", "comment": "arXiv admin note: text overlap with arXiv:2501.09444; text overlap\n  with arXiv:2409.20288 by other authors", "pdf_url": "http://arxiv.org/pdf/2507.00875v1", "categories": ["cs.CL", "cs.HC", "cs.MA"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2507.00875v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00299", "title": "When Kids Mode Isn't For Kids: Investigating TikTok's \"Under 13 Experience\"", "authors": ["Olivia Figueira", "Pranathi Chamarthi", "Tu Le", "Athina Markopoulou"], "summary": "TikTok, the social media platform that is popular among children and\nadolescents, offers a more restrictive \"Under 13 Experience\" exclusively for\nyoung users in the US, also known as TikTok's \"Kids Mode\". While prior research\nhas studied various aspects of TikTok's regular mode, including privacy and\npersonalization, TikTok's Kids Mode remains understudied, and there is a lack\nof transparency regarding its content curation and its safety and privacy\nprotections for children. In this paper, (i) we propose an auditing methodology\nto comprehensively investigate TikTok's Kids Mode and (ii) we apply it to\ncharacterize the platform's content curation and determine the prevalence of\nchild-directed content, based on regulations in the Children's Online Privacy\nProtection Act (COPPA). We find that 83% of videos observed on the \"For You\"\npage in Kids Mode are actually not child-directed, and even inappropriate\ncontent was found. The platform also lacks critical features, namely parental\ncontrols and accessibility settings. Our findings have important design and\nregulatory implications, as children may be incentivized to use TikTok's\nregular mode instead of Kids Mode, where they are known to be exposed to\nfurther safety and privacy risks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00299v1", "categories": ["cs.HC", "cs.CR"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2507.00299v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00333", "title": "Scope Meets Screen: Lessons Learned in Designing Composite Visualizations for Marksmanship Training Across Skill Levels", "authors": ["Emin Zerman", "Jonas Carlsson", "Mårten Sjöström"], "summary": "Marksmanship practices are required in various professions, including police,\nmilitary personnel, hunters, as well as sports shooters, such as Olympic\nshooting, biathlon, and modern pentathlon. The current form of training and\ncoaching is mostly based on repetition, where the coach does not see through\nthe eyes of the shooter, and analysis is limited to stance and accuracy\npost-session. In this study, we present a shooting visualization system and\nevaluate its perceived effectiveness for both novice and expert shooters. To\nachieve this, five composite visualizations were developed using first-person\nshooting video recordings enriched with overlaid metrics and graphical\nsummaries. These views were evaluated with 10 participants (5 expert marksmen,\n5 novices) through a mixed-methods study including shot-count and aiming\ninterpretation tasks, pairwise preference comparisons, and semi-structured\ninterviews. The results show that a dashboard-style composite view, combining\nraw video with a polar plot and selected graphs, was preferred in 9 of 10 cases\nand supported understanding across skill levels. The insights gained from this\ndesign study point to the broader value of integrating first-person video with\nvisual analytics for coaching, and we suggest directions for applying this\napproach to other precision-based sports.", "comment": "5 pages, accepted at IEEE VIS 2025", "pdf_url": "http://arxiv.org/pdf/2507.00333v1", "categories": ["cs.HC", "cs.CV", "cs.GR", "eess.IV"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2507.00333v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00093", "title": "$σ$-Maximal Ancestral Graphs", "authors": ["Binghua Yao", "Joris M. Mooij"], "summary": "Maximal Ancestral Graphs (MAGs) provide an abstract representation of\nDirected Acyclic Graphs (DAGs) with latent (selection) variables. These\ngraphical objects encode information about ancestral relations and\nd-separations of the DAGs they represent. This abstract representation has been\nused amongst others to prove the soundness and completeness of the FCI\nalgorithm for causal discovery, and to derive a do-calculus for its output. One\nsignificant inherent limitation of MAGs is that they rule out the possibility\nof cyclic causal relationships. In this work, we address that limitation. We\nintroduce and study a class of graphical objects that we coin\n''$\\sigma$-Maximal Ancestral Graphs'' (''$\\sigma$-MAGs''). We show how these\ngraphs provide an abstract representation of (possibly cyclic) Directed Graphs\n(DGs) with latent (selection) variables, analogously to how MAGs represent\nDAGs. We study the properties of these objects and provide a characterization\nof their Markov equivalence classes.", "comment": "It has beee accepted by the 41st Conference on Uncertainty in\n  Artificial Intelligence (UAI)", "pdf_url": "http://arxiv.org/pdf/2507.00093v1", "categories": ["cs.DM", "cs.AI", "cs.DS", "math.ST", "stat.TH"], "cate": "cs.DM", "url": "http://arxiv.org/abs/2507.00093v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00424", "title": "Multi-Agent Coordination under Poisson Observations: A Global Game Approach", "authors": ["Marcos M. Vasconcelos", "Behrouz Touri"], "summary": "We study a model of strategic coordination based on a class of games with\nincomplete information known as Global Games. Under the assumption of\nPoisson-distributed signals and a Gamma prior distribution on state of the\nsystem, we demonstrate the existence of a Bayesian Nash equilibrium within the\nclass of threshold policies for utility functions that are linear in the\nagents' actions. Although computing the exact threshold that constitutes an\nequilibrium in a system with finitely many agents is a highly non-trivial task,\nthe problem becomes tractable by analyzing the game's potential function with\ncountably infinitely many agents. Through numerical examples, we provide\nevidence that the resulting potential function is unimodal, exhibiting a\nwell-defined maximum. Our results are applicable to the modeling of bacterial\nQuorum Sensing systems, whose noisy observation signals are often\nwell-approximated using Poisson processes.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00424v1", "categories": ["eess.SY", "cs.SY"], "cate": "eess.SY", "url": "http://arxiv.org/abs/2507.00424v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00605", "title": "Quantize-Sample-and-Verify: LLM Acceleration via Adaptive Edge-Cloud Speculative Decoding", "authors": ["Guangyi Zhang", "Yunlong Cai", "Guanding Yu", "Petar Popovski", "Osvaldo Simeone"], "summary": "In edge-cloud speculative decoding (SD), edge devices equipped with small\nlanguage models (SLMs) generate draft tokens that are verified by large\nlanguage models (LLMs) in the cloud. A key bottleneck in such systems is the\nlimited communication bandwidth between edge and cloud, which necessitates\nquantization of the information transmitted about generated tokens. In this\nwork, we introduce a novel quantize-sample (Q-S) strategy that provably\npreserves the output distribution of the cloud-based model, ensuring that the\nverified tokens match the distribution of those that would have been generated\ndirectly by the LLM. We develop a throughput model for edge-cloud SD that\nexplicitly accounts for communication latency. Leveraging this model, we\npropose an adaptive mechanism that optimizes token throughput by dynamically\nadjusting the draft length and quantization precision in response to both\nsemantic uncertainty and channel conditions. Simulations demonstrate that the\nproposed Q-S approach significantly improves decoding efficiency in realistic\nedge-cloud deployment scenarios.", "comment": "Submit for review", "pdf_url": "http://arxiv.org/pdf/2507.00605v1", "categories": ["eess.SP"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2507.00605v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00755", "title": "LearnAFE: Circuit-Algorithm Co-design Framework for Learnable Audio Analog Front-End", "authors": ["Jinhai Hu", "Zhongyi Zhang", "Cong Sheng Leow", "Wang Ling Goh", "Yuan Gao"], "summary": "This paper presents a circuit-algorithm co-design framework for learnable\nanalog front-end (AFE) in audio signal classification. Designing AFE and\nbackend classifiers separately is a common practice but non-ideal, as shown in\nthis paper. Instead, this paper proposes a joint optimization of the backend\nclassifier with the AFE's transfer function to achieve system-level optimum.\nMore specifically, the transfer function parameters of an analog bandpass\nfilter (BPF) bank are tuned in a signal-to-noise ratio (SNR)-aware training\nloop for the classifier. Using a co-design loss function LBPF, this work shows\nsuperior optimization of both the filter bank and the classifier. Implemented\nin open-source SKY130 130nm CMOS process, the optimized design achieved\n90.5%-94.2% accuracy for 10-keyword classification task across a wide range of\ninput signal SNR from 5 dB to 20 dB, with only 22k classifier parameters.\nCompared to conventional approach, the proposed audio AFE achieves 8.7% and\n12.9% reduction in power and capacitor area respectively.", "comment": "11 pages, 15 figures, accepted for publication on IEEE Transactions\n  on Circuits and Systems I: Regular Papers", "pdf_url": "http://arxiv.org/pdf/2507.00755v1", "categories": ["eess.AS", "cs.AI", "cs.SD"], "cate": "eess.AS", "url": "http://arxiv.org/abs/2507.00755v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00214", "title": "Two-Stage Reasoning-Infused Learning: Improving Classification with LLM-Generated Reasoning", "authors": ["Mads Henrichsen", "Rasmus Krebs"], "summary": "Standard classification models often map inputs directly to labels without\nexplicit reasoning, potentially limiting their performance, robustness, and\ninterpretability. This paper introduces a novel two-stage approach to enhance\ntext classification by leveraging Large Language Model (LLM)-generated\nreasonings. In the first stage, we fine-tune a Llama-3.2-1B-Instruct model\n(henceforth Llama-R-Gen) on a general-purpose reasoning dataset\n(syvai/reasoning-gen) to generate textual reasoning (R) given a question and\nits answer. In the second stage, this generally trained Llama-R-Gen is used\noffline to create an augmented training dataset for a downstream generative\nmodel. This downstream model, based on Llama-3.2-1B-Instruct, takes only the\ninput text (Q) and is trained to output the generated reasoning (R) immediately\nfollowed by the predicted emotion (A). We demonstrate this methodology on the\ndair-ai/emotion dataset for emotion classification. Our experiments show that\nthe generative model trained to output reasoning and the emotion (Classifier\nQ->RA) achieves a significant improvement of 8.7 percentage points in accuracy\n(for emotion prediction) compared to a baseline generative model trained solely\nto output the emotion (Classifier Q->A), highlighting the strong generalization\ncapabilities of the reasoning generation and the benefit of explicit reasoning\ntraining. This work underscores the potential of LLM-generated reasonings for\ncreating richer training datasets, thereby improving the performance of diverse\ndownstream NLP tasks and providing explicit explanations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00214v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2507.00214v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00598", "title": "High-resolution spatial memory requires grid-cell-like neural codes", "authors": ["Madison Cotteret", "Christopher J. Kymn", "Hugh Greatorex", "Martin Ziegler", "Elisabetta Chicca", "Friedrich T. Sommer"], "summary": "Continuous attractor networks (CANs) are widely used to model how the brain\ntemporarily retains continuous behavioural variables via persistent recurrent\nactivity, such as an animal's position in an environment. However, this memory\nmechanism is very sensitive to even small imperfections, such as noise or\nheterogeneity, which are both common in biological systems. Previous work has\nshown that discretising the continuum into a finite set of discrete attractor\nstates provides robustness to these imperfections, but necessarily reduces the\nresolution of the represented variable, creating a dilemma between stability\nand resolution. We show that this stability-resolution dilemma is most severe\nfor CANs using unimodal bump-like codes, as in traditional models. To overcome\nthis, we investigate sparse binary distributed codes based on random feature\nembeddings, in which neurons have spatially-periodic receptive fields. We\ndemonstrate theoretically and with simulations that such grid-cell-like codes\nenable CANs to achieve both high stability and high resolution simultaneously.\nThe model extends to embedding arbitrary nonlinear manifolds into a CAN, such\nas spheres or tori, and generalises linear path integration to integration\nalong freely-programmable on-manifold vector fields. Together, this work\nprovides a theory of how the brain could robustly represent continuous\nvariables with high resolution and perform flexible computations over\ntask-relevant manifolds.", "comment": "14 pages, 4 figures. Supplementary material: 11 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2507.00598v1", "categories": ["cs.NE", "cs.AI", "cs.SC"], "cate": "cs.NE", "url": "http://arxiv.org/abs/2507.00598v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00487", "title": "MassTool: A Multi-Task Search-Based Tool Retrieval Framework for Large Language Models", "authors": ["Jianghao Lin", "Xinyuan Wang", "Xinyi Dai", "Menghui Zhu", "Bo Chen", "Ruiming Tang", "Yong Yu", "Weinan Zhang"], "summary": "Tool retrieval is a critical component in enabling large language models\n(LLMs) to interact effectively with external tools. It aims to precisely filter\nthe massive tools into a small set of candidates for the downstream\ntool-augmented LLMs. However, most existing approaches primarily focus on\noptimizing tool representations, often neglecting the importance of precise\nquery comprehension. To address this gap, we introduce MassTool, a multi-task\nsearch-based framework designed to enhance both query representation and tool\nretrieval accuracy. MassTool employs a two-tower architecture: a tool usage\ndetection tower that predicts the need for function calls, and a tool retrieval\ntower that leverages a query-centric graph convolution network (QC-GCN) for\neffective query-tool matching. It also incorporates search-based user intent\nmodeling (SUIM) to handle diverse and out-of-distribution queries, alongside an\nadaptive knowledge transfer (AdaKT) module for efficient multi-task learning.\nBy jointly optimizing tool usage detection loss, list-wise retrieval loss, and\ncontrastive regularization loss, MassTool establishes a robust dual-step\nsequential decision-making pipeline for precise query understanding. Extensive\nexperiments demonstrate its effectiveness in improving retrieval accuracy. Our\ncode is available at https://github.com/wxydada/MassTool.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00487v1", "categories": ["cs.IR", "cs.CL"], "cate": "cs.IR", "url": "http://arxiv.org/abs/2507.00487v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00219", "title": "Error Etimates for Non Conforming Discretisation of Time-dependent Convection-Diffusion-Reaction Model", "authors": ["Hasan Alzubaidi", "Yahya Alnashri"], "summary": "We use a generic framework, namely the gradient discretisation method (GDM),\nto propose a unified numerical analysis for general time-dependent\nconvection-diffusion-reaction models. We establish novel results for\nconvergence rates of numerical approximations of such models under reasonable\nassumptions on exact solutions, and prove the existence and uniqueness of the\napproximate solution for suitably small time steps. The main interest of our\nresults lies in covering several approximation methods and various applications\nof the considered model such as the generalised Burgers-Fisher (GBF) and the\ngeneralised Burgers-Huxley (GBH) models. Numerical tests based on the hybrid\nmimetic mixed (HMM) method for the GBF model are performed on various types of\ngeneral meshes to examine the accuracy of the proposed gradient scheme. The\nresults confirm our theoretical rates of convergence, even on mesh with extreme\ndistortions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00219v1", "categories": ["math.NA", "cs.NA"], "cate": "math.NA", "url": "http://arxiv.org/abs/2507.00219v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00475", "title": "AudioBERTScore: Objective Evaluation of Environmental Sound Synthesis Based on Similarity of Audio embedding Sequences", "authors": ["Minoru Kishi", "Ryosuke Sakai", "Shinnosuke Takamichi", "Yusuke Kanamori", "Yuki Okamoto"], "summary": "We propose a novel objective evaluation metric for synthesized audio in\ntext-to-audio (TTA), aiming to improve the performance of TTA models. In TTA,\nsubjective evaluation of the synthesized sound is an important, but its\nimplementation requires monetary costs. Therefore, objective evaluation such as\nmel-cepstral distortion are used, but the correlation between these objective\nmetrics and subjective evaluation values is weak. Our proposed objective\nevaluation metric, AudioBERTScore, calculates the similarity between embedding\nof the synthesized and reference sounds. The method is based not only on the\nmax-norm used in conventional BERTScore but also on the $p$-norm to reflect the\nnon-local nature of environmental sounds. Experimental results show that scores\nobtained by the proposed method have a higher correlation with subjective\nevaluation values than conventional metrics.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00475v1", "categories": ["cs.SD", "eess.AS"], "cate": "cs.SD", "url": "http://arxiv.org/abs/2507.00475v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00827", "title": "A Technique for the Detection of PDF Tampering or Forgery", "authors": ["Gabriel Grobler", "Sheunesu Makura", "Hein Venter"], "summary": "Tampering or forgery of digital documents has become widespread, most\ncommonly through altering images without any malicious intent such as enhancing\nthe overall appearance of the image. However, there are occasions when\ntampering of digital documents can have negative consequences, such as\nfinancial fraud and reputational damage. Tampering can occur through altering a\ndigital document's text or editing an image's pixels. Many techniques have been\ndeveloped to detect whether changes have been made to a document. Most of these\ntechniques rely on generating hashes or watermarking the document. These\ntechniques, however, have limitations in that they cannot detect alterations to\nportable document format (PDF) signatures or other non-visual aspects, such as\nmetadata. This paper presents a new technique that can be used to detect\ntampering within a PDF document by utilizing the PDF document's file page\nobjects. The technique employs a prototype that can detect changes to a PDF\ndocument, such as changes made to the text, images, or metadata of the said\nfile.", "comment": "19 Pages, 5 figures, published in Online Proceedings of the South\n  African Institute of Computer Scientists and Information Technologists 2024\n  Conference, ISSN 2959-8877", "pdf_url": "http://arxiv.org/pdf/2507.00827v1", "categories": ["cs.CR"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2507.00827v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00686", "title": "A Domain-specific Language and Architecture for Detecting Process Activities from Sensor Streams in IoT", "authors": ["Ronny Seiger", "Daniel Locher", "Marco Kaufmann", "Aaron F. Kurz"], "summary": "Modern Internet of Things (IoT) systems are equipped with a plethora of\nsensors providing real-time data about the current operations of their\ncomponents, which is crucial for the systems' internal control systems and\nprocesses. However, these data are often too fine-grained to derive useful\ninsights into the execution of the larger processes an IoT system might be part\nof. Process mining has developed advanced approaches for the analysis of\nbusiness processes that may also be used in the context of IoT. Bringing\nprocess mining to IoT requires an event abstraction step to lift the low-level\nsensor data to the business process level. In this work, we aim to empower\ndomain experts to perform this step using a newly developed domain-specific\nlanguage (DSL) called Radiant. Radiant supports the specification of patterns\nwithin the sensor data that indicate the execution of higher level process\nactivities. These patterns are translated to complex event processing (CEP)\napplications to be used for detecting activity executions at runtime. We\npropose a corresponding software architecture for online event abstraction from\nIoT sensor streams using the CEP applications. We evaluate these applications\nto monitor activity executions using IoT sensors in smart manufacturing and\nsmart healthcare. The evaluation method and results inform the domain expert\nabout the quality of activity detections and potential for improvement.", "comment": "Submitted to Internet of Things (ISSN 2542-6605)", "pdf_url": "http://arxiv.org/pdf/2507.00686v1", "categories": ["cs.SE", "cs.ET"], "cate": "cs.SE", "url": "http://arxiv.org/abs/2507.00686v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00286", "title": "Visual Privacy Management with Generative AI for Blind and Low-Vision People", "authors": ["Tanusree Sharma", "Yu-Yun Tseng", "Lotus Zhang", "Ayae Ide", "Kelly Avery Mack", "Leah Findlater", "Danna Gurari", "Yang Wang"], "summary": "Blind and low vision (BLV) individuals use Generative AI (GenAI) tools to\ninterpret and manage visual content in their daily lives. While such tools can\nenhance the accessibility of visual content and so enable greater user\nindependence, they also introduce complex challenges around visual privacy. In\nthis paper, we investigate the current practices and future design preferences\nof blind and low vision individuals through an interview study with 21\nparticipants. Our findings reveal a range of current practices with GenAI that\nbalance privacy, efficiency, and emotional agency, with users accounting for\nprivacy risks across six key scenarios, such as self-presentation,\nindoor/outdoor spatial privacy, social sharing, and handling professional\ncontent. Our findings reveal design preferences, including on-device\nprocessing, zero-retention guarantees, sensitive content redaction,\nprivacy-aware appearance indicators, and multimodal tactile mirrored\ninteraction methods. We conclude with actionable design recommendations to\nsupport user-centered visual privacy through GenAI, expanding the notion of\nprivacy and responsible handling of others data.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00286v1", "categories": ["cs.HC", "cs.AI", "cs.ET"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2507.00286v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00435", "title": "RoboEval: Where Robotic Manipulation Meets Structured and Scalable Evaluation", "authors": ["Yi Ru Wang", "Carter Ung", "Grant Tannert", "Jiafei Duan", "Josephine Li", "Amy Le", "Rishabh Oswal", "Markus Grotz", "Wilbert Pumacay", "Yuquan Deng", "Ranjay Krishna", "Dieter Fox", "Siddhartha Srinivasa"], "summary": "We present RoboEval, a simulation benchmark and structured evaluation\nframework designed to reveal the limitations of current bimanual manipulation\npolicies. While prior benchmarks report only binary task success, we show that\nsuch metrics often conceal critical weaknesses in policy behavior -- such as\npoor coordination, slipping during grasping, or asymmetric arm usage. RoboEval\nintroduces a suite of tiered, semantically grounded tasks decomposed into\nskill-specific stages, with variations that systematically challenge spatial,\nphysical, and coordination capabilities. Tasks are paired with fine-grained\ndiagnostic metrics and 3000+ human demonstrations to support imitation\nlearning. Our experiments reveal that policies with similar success rates\ndiverge in how tasks are executed -- some struggle with alignment, others with\ntemporally consistent bimanual control. We find that behavioral metrics\ncorrelate with success in over half of task-metric pairs, and remain\ninformative even when binary success saturates. By pinpointing when and how\npolicies fail, RoboEval enables a deeper, more actionable understanding of\nrobotic manipulation -- and highlights the need for evaluation tools that go\nbeyond success alone.", "comment": "Project page: https://robo-eval.github.io", "pdf_url": "http://arxiv.org/pdf/2507.00435v1", "categories": ["cs.RO", "cs.AI", "cs.CV"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2507.00435v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00304", "title": "MamNet: A Novel Hybrid Model for Time-Series Forecasting and Frequency Pattern Analysis in Network Traffic", "authors": ["Yujun Zhang", "Runlong Li", "Xiaoxiang Liang", "Xinhao Yang", "Tian Su", "Bo Liu", "Yan Zhou"], "summary": "The abnormal fluctuations in network traffic may indicate potential security\nthreats or system failures. Therefore, efficient network traffic prediction and\nanomaly detection methods are crucial for network security and traffic\nmanagement. This paper proposes a novel network traffic prediction and anomaly\ndetection model, MamNet, which integrates time-domain modeling and\nfrequency-domain feature extraction. The model first captures the long-term\ndependencies of network traffic through the Mamba module (time-domain\nmodeling), and then identifies periodic fluctuations in the traffic using\nFourier Transform (frequency-domain feature extraction). In the feature fusion\nlayer, multi-scale information is integrated to enhance the model's ability to\ndetect network traffic anomalies. Experiments conducted on the UNSW-NB15 and\nCAIDA datasets demonstrate that MamNet outperforms several recent mainstream\nmodels in terms of accuracy, recall, and F1-Score. Specifically, it achieves an\nimprovement of approximately 2% to 4% in detection performance for complex\ntraffic patterns and long-term trend detection. The results indicate that\nMamNet effectively captures anomalies in network traffic across different time\nscales and is suitable for anomaly detection tasks in network security and\ntraffic management. Future work could further optimize the model structure by\nincorporating external network event information, thereby improving the model's\nadaptability and stability in complex network environments.", "comment": "16 pages", "pdf_url": "http://arxiv.org/pdf/2507.00304v1", "categories": ["cs.LG", "cs.NI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00304v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00727", "title": "On Hierarchical Coded Caching with Offline Users", "authors": ["Rashid Ummer N. T.", "B. Sundar Rajan"], "summary": "This paper studies a two-layer hierarchical network in which some users are\noffline during the content delivery phase. A two-layer hierarchical network\nconsists of a single server connected to multiple cache-aided mirror sites, and\neach mirror site is connected to a distinct set of cache-aided users. A scheme\nfor such a hierarchical system with offline users has been proposed recently\nbut considered a special case where all mirror caches have zero memory, which\nis a significant limitation. We propose an array known as a hierarchical\nhotplug placement delivery array (HHPDA), which describes the placement and\ndelivery phases of a coded caching scheme for a general two-layer hierarchical\nnetwork with offline users. Further, we construct a class of HHPDAs using\ncombinatorial t-designs.", "comment": "A short version of this is accepted for presentation in 2025 IEEE\n  Information Theory Workshop; 8 pages, one figure", "pdf_url": "http://arxiv.org/pdf/2507.00727v1", "categories": ["cs.IT", "math.IT"], "cate": "cs.IT", "url": "http://arxiv.org/abs/2507.00727v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00180", "title": "BlackBoxToBlueprint: Extracting Interpretable Logic from Legacy Systems using Reinforcement Learning and Counterfactual Analysis", "authors": ["Vidhi Rathore"], "summary": "Modernizing legacy software systems is a critical but challenging task, often\nhampered by a lack of documentation and understanding of the original system's\nintricate decision logic. Traditional approaches like behavioral cloning merely\nreplicate input-output behavior without capturing the underlying intent. This\npaper proposes a novel pipeline to automatically extract interpretable decision\nlogic from legacy systems treated as black boxes. The approach uses a\nReinforcement Learning (RL) agent to explore the input space and identify\ncritical decision boundaries by rewarding actions that cause meaningful changes\nin the system's output. These counterfactual state transitions, where the\noutput changes, are collected and clustered using K-Means. Decision trees are\nthen trained on these clusters to extract human-readable rules that approximate\nthe system's decision logic near the identified boundaries. I demonstrated the\npipeline's effectiveness on three dummy legacy systems with varying complexity,\nincluding threshold-based, combined-conditional, and non-linear range logic.\nResults show that the RL agent successfully focuses exploration on relevant\nboundary regions, and the extracted rules accurately reflect the core logic of\nthe underlying dummy systems, providing a promising foundation for generating\nspecifications and test cases during legacy migration.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00180v1", "categories": ["cs.AI", "cs.LG"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2507.00180v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00015", "title": "Vision Transformer with Adversarial Indicator Token against Adversarial Attacks in Radio Signal Classifications", "authors": ["Lu Zhang", "Sangarapillai Lambotharan", "Gan Zheng", "Guisheng Liao", "Xuekang Liu", "Fabio Roli", "Carsten Maple"], "summary": "The remarkable success of transformers across various fields such as natural\nlanguage processing and computer vision has paved the way for their\napplications in automatic modulation classification, a critical component in\nthe communication systems of Internet of Things (IoT) devices. However, it has\nbeen observed that transformer-based classification of radio signals is\nsusceptible to subtle yet sophisticated adversarial attacks. To address this\nissue, we have developed a defensive strategy for transformer-based modulation\nclassification systems to counter such adversarial attacks. In this paper, we\npropose a novel vision transformer (ViT) architecture by introducing a new\nconcept known as adversarial indicator (AdvI) token to detect adversarial\nattacks. To the best of our knowledge, this is the first work to propose an\nAdvI token in ViT to defend against adversarial attacks. Integrating an\nadversarial training method with a detection mechanism using AdvI token, we\ncombine a training time defense and running time defense in a unified neural\nnetwork model, which reduces architectural complexity of the system compared to\ndetecting adversarial perturbations using separate models. We investigate into\nthe operational principles of our method by examining the attention mechanism.\nWe show the proposed AdvI token acts as a crucial element within the ViT,\ninfluencing attention weights and thereby highlighting regions or features in\nthe input data that are potentially suspicious or anomalous. Through\nexperimental results, we demonstrate that our approach surpasses several\ncompetitive methods in handling white-box attack scenarios, including those\nutilizing the fast gradient method, projected gradient descent attacks and\nbasic iterative method.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00015v1", "categories": ["cs.LG", "cs.AI", "cs.CR"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00015v1", "date": "2025-06-13", "updated": "2025-06-13"}
{"id": "2507.00824", "title": "PANDAS: Peer-to-peer, Adaptive Networking for Data Availability Sampling within Ethereum Consensus Timebounds", "authors": ["Matthieu Pigaglio", "Onur Ascigil", "Michał Król", "Sergi Rene", "Felix Lange", "Kaleem Peeroo", "Ramin Sadre", "Vladimir Stankovic", "Etienne Rivière"], "summary": "Layer-2 protocols can assist Ethereum's limited throughput, but globally\nbroadcasting layer-2 data limits their scalability. The Danksharding evolution\nof Ethereum aims to support the selective distribution of layer-2 data, whose\navailability in the network is verified using randomized data availability\nsampling (DAS). Integrating DAS into Ethereum's consensus process is\nchallenging, as pieces of layer-2 data must be disseminated and sampled within\nfour seconds of the beginning of each consensus slot. No existing solution can\nsupport dissemination and sampling under such strict time bounds.\n  We propose PANDAS, a practical approach to integrate DAS with Ethereum under\nDanksharding's requirements without modifying its protocols for consensus and\nnode discovery. PANDAS disseminates layer-2 data and samples its availability\nusing lightweight, direct exchanges. Its design accounts for message loss, node\nfailures, and unresponsive participants while anticipating the need to scale\nout the Ethereum network. Our evaluation of PANDAS's prototype in a 1,000-node\ncluster and simulations for up to 20,000 peers shows that it allows layer-2\ndata dissemination and sampling under planetary-scale latencies within the\n4-second deadline.", "comment": "14 pages, 10 figures, 1 algorithm, 1 table, and 18 plots", "pdf_url": "http://arxiv.org/pdf/2507.00824v1", "categories": ["cs.DC", "cs.NI", "cs.PF"], "cate": "cs.DC", "url": "http://arxiv.org/abs/2507.00824v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00052", "title": "VSF-Med:A Vulnerability Scoring Framework for Medical Vision-Language Models", "authors": ["Binesh Sadanandan", "Vahid Behzadan"], "summary": "Vision Language Models (VLMs) hold great promise for streamlining\nlabour-intensive medical imaging workflows, yet systematic security evaluations\nin clinical settings remain scarce. We introduce VSF--Med, an end-to-end\nvulnerability-scoring framework for medical VLMs that unites three novel\ncomponents: (i) a rich library of sophisticated text-prompt attack templates\ntargeting emerging threat vectors; (ii) imperceptible visual perturbations\ncalibrated by structural similarity (SSIM) thresholds to preserve clinical\nrealism; and (iii) an eight-dimensional rubric evaluated by two independent\njudge LLMs, whose raw scores are consolidated via z-score normalization to\nyield a 0--32 composite risk metric. Built entirely on publicly available\ndatasets and accompanied by open-source code, VSF--Med synthesizes over 30,000\nadversarial variants from 5,000 radiology images and enables reproducible\nbenchmarking of any medical VLM with a single command. Our consolidated\nanalysis reports mean z-score shifts of $0.90\\sigma$ for\npersistence-of-attack-effects, $0.74\\sigma$ for prompt-injection effectiveness,\nand $0.63\\sigma$ for safety-bypass success across state-of-the-art VLMs.\nNotably, Llama-3.2-11B-Vision-Instruct exhibits a peak vulnerability increase\nof $1.29\\sigma$ for persistence-of-attack-effects, while GPT-4o shows increases\nof $0.69\\sigma$ for that same vector and $0.28\\sigma$ for prompt-injection\nattacks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00052v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00052v1", "date": "2025-06-25", "updated": "2025-06-25"}
{"id": "2507.00026", "title": "ROSE: Toward Reality-Oriented Safety Evaluation of Large Language Models", "authors": ["Jiale Ding", "Xiang Zheng", "Cong Wang", "Wei-Bin Lee", "Xingjun Ma", "Yu-Gang Jiang"], "summary": "As Large Language Models (LLMs) are increasingly deployed as black-box\ncomponents in real-world applications, evaluating their safety-especially under\nadversarial prompting-has become critical. Arguably, effective safety\nevaluations should be adaptive, evolving with LLM capabilities, and also cover\na broad spectrum of harmful topics and real-world scenarios to fully expose\npotential vulnerabilities. Existing manual safety benchmarks, built on\nhandcrafted adversarial prompts, are limited by their static nature and the\nintensive labor required to update them, making it difficult to keep pace with\nrapidly advancing LLMs. In contrast, automated adversarial prompt generation\noffers a promising path toward adaptive evaluation. However, current methods\noften suffer from insufficient adversarial topic coverage (topic-level\ndiversity) and weak alignment with real-world contexts. These shortcomings stem\nfrom the exploration-exploitation dilemma in black-box optimization and a lack\nof real-world contextualization, resulting in adversarial prompts that are both\ntopically narrow and scenario-repetitive. To address these issues, we propose\nReality-Oriented Safety Evaluation (ROSE), a novel framework that uses\nmulti-objective reinforcement learning to fine-tune an adversarial LLM for\ngenerating topically diverse and contextually rich adversarial prompts.\nExperiments show that ROSE outperforms existing methods in uncovering safety\nvulnerabilities in state-of-the-art LLMs, with notable improvements in\nintegrated evaluation metrics. We hope ROSE represents a step toward more\npractical and reality-oriented safety evaluation of LLMs. WARNING: This paper\ncontains examples of potentially harmful text.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00026v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CY"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00026v1", "date": "2025-06-17", "updated": "2025-06-17"}
{"id": "2507.00582", "title": "Bridging Classical and Learning-based Iterative Registration through Deep Equilibrium Models", "authors": ["Yi Zhang", "Yidong Zhao", "Qian Tao"], "summary": "Deformable medical image registration is traditionally formulated as an\noptimization problem. While classical methods solve this problem iteratively,\nrecent learning-based approaches use recurrent neural networks (RNNs) to mimic\nthis process by unrolling the prediction of deformation fields in a fixed\nnumber of steps. However, classical methods typically converge after sufficient\niterations, but learning-based unrolling methods lack a theoretical convergence\nguarantee and show instability empirically. In addition, unrolling methods have\na practical bottleneck at training time: GPU memory usage grows linearly with\nthe unrolling steps due to backpropagation through time (BPTT). To address both\ntheoretical and practical challenges, we propose DEQReg, a novel registration\nframework based on Deep Equilibrium Models (DEQ), which formulates registration\nas an equilibrium-seeking problem, establishing a natural connection between\nclassical optimization and learning-based unrolling methods. DEQReg maintains\nconstant memory usage, enabling theoretically unlimited iteration steps.\nThrough extensive evaluation on the public brain MRI and lung CT datasets, we\nshow that DEQReg can achieve competitive registration performance, while\nsubstantially reducing memory consumption compared to state-of-the-art\nunrolling methods. We also reveal an intriguing phenomenon: the performance of\nexisting unrolling methods first increases slightly then degrades irreversibly\nwhen the inference steps go beyond the training configuration. In contrast,\nDEQReg achieves stable convergence with its inbuilt equilibrium-seeking\nmechanism, bridging the gap between classical optimization-based and modern\nlearning-based registration methods.", "comment": "Submitted version. Accepted by MICCAI 2025", "pdf_url": "http://arxiv.org/pdf/2507.00582v1", "categories": ["eess.IV", "cs.CV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2507.00582v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00305", "title": "EEG-Based Auditory BCI for Communication in a Completely Locked-In Patient Using Volitional Frequency Band Modulation", "authors": ["Deland Liu", "Frigyes Samuel Racz", "Zoe Lalji", "Jose del R. Millan"], "summary": "Patients with amyotrophic lateral sclerosis (ALS) in the completely locked-in\nstate (CLIS) can lose all reliable motor control and are left without any means\nof communication. It remains unknown whether non-invasive electroencephalogram\n(EEG) based brain-computer interfaces (BCIs) can support volitional\ncommunication in CLIS. Here, we show that a CLIS patient was able to operate an\nEEG-based BCI across multiple online sessions to respond to both general\nknowledge and personally relevant assistive questions. The patient delivered\n\"Yes\"/\"No\" responses by volitionally modulating alpha and beta band power at\ndifferent channels, guided by real-time auditory feedback from the BCI. The\npatient communicated assistive needs above chance in all sessions, achieving a\nperfect score in the final session. Performance on general knowledge questions\nvaried across sessions, with two sessions showing accurate and above-chance\nresponses, while the first and last sessions remained at chance level. The\npatient also showed consistent modulation patterns over time. These findings\nsuggest that non-invasive BCIs may offer a potential pathway for restoring\nbasic communication in CLIS.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00305v1", "categories": ["cs.HC", "q-bio.NC"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2507.00305v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00451", "title": "Best Agent Identification for General Game Playing", "authors": ["Matthew Stephenson", "Alex Newcombe", "Eric Piette", "Dennis Soemers"], "summary": "We present an efficient and generalised procedure to accurately identify the\nbest performing algorithm for each sub-task in a multi-problem domain. Our\napproach treats this as a set of best arm identification problems for\nmulti-armed bandits, where each bandit corresponds to a specific task and each\narm corresponds to a specific algorithm or agent. We propose an optimistic\nselection process based on the Wilson score interval (Optimistic-WS) that ranks\neach arm across all bandits in terms of their potential regret reduction. We\nevaluate the performance of Optimistic-WS on two of the most popular general\ngame domains, the General Video Game AI (GVGAI) framework and the Ludii general\ngame playing system, with the goal of identifying the highest performing agent\nfor each game within a limited number of trials. Compared to previous best arm\nidentification algorithms for multi-armed bandits, our results demonstrate a\nsubstantial performance improvement in terms of average simple regret. This\nnovel approach can be used to significantly improve the quality and accuracy of\nagent evaluation procedures for general game frameworks, as well as other\nmulti-task domains with high algorithm runtimes.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00451v1", "categories": ["cs.LG", "cs.AI", "cs.DS", "cs.IT", "math.IT", "stat.ML"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00451v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00452", "title": "The impact of the following vehicles behaviors on the car following behaviors of the ego-vehicle", "authors": ["Yang Liu", "Jiahao Zhang", "Yuxuan Ouyang", "Huan Yu", "Dengbo He"], "summary": "Among all types of crashes, rear-end crashes dominate, which are closely\nrelated to the car-following (CF) behaviors. Traditional CF behavior models\nfocused on the influence of the vehicle in front, but usually ignored the peer\npressure from the surrounding road users, including the following vehicle (FV).\nBased on an open dataset, the highD dataset, we investigated whether the FV's\nstates can affect the CF behavior of the ego-vehicle in CF events. Two types of\nCF events were extracted from highD database, including the tailgated events,\nwhere the time headway between the FV and the ego-vehicle (i.e., time gap) was\nsmaller than 1 second, and the gapped events, where the time gap was larger\nthan 3 seconds. The dynamic time warping was used to extract CF pairs with\nsimilar speed profiles of the leading vehicle (LV). Statistical analyses were\nconducted to compare the CF-performance metrics in tailgated and gapped events.\nThen, the inverse reinforcement learning was used to recover the reward\nfunction of the ego-vehicle drivers in different CF events. The results showed\nthat the ego-driver would adjust their CF behavior in response to the pressure\nfrom a tailgating FV, by maintaining a closer distance to the LV, but at the\nsame time, driving more cautiously. Further, drivers were still able to adjust\ntheir CF strategies based on the speed of traffic flow and the distance to the\nLV, even when being tailgated. These findings provide insights regarding more\naccurate modelling of traffic flow by considering the peer pressure from\nsurrounding road users.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00452v1", "categories": ["eess.SY", "cs.SY"], "cate": "eess.SY", "url": "http://arxiv.org/abs/2507.00452v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00714", "title": "Physical Layer Group Key Generation With the Aid of Reconfigurable Intelligent Surfaces", "authors": ["Vahid Shahiri", "Guyue Li", "Hamid Behroozi"], "summary": "Reconfigurable intelligent surfaces (RIS) have the ability to alter the\nwireless environment by making changes in the impinging signal. Motivated by\nthis ability, in this study, we exploit the RIS to make the aggregate\nreflecting channels of different user terminals (UTs) as similar as possible to\nbe able to extract common group secret keys from their channels. Specifically,\nthe RIS will adjust its parameters to pave the way for group key generation\n(GKG) based on the physical channels of the UTs. Our method exploits the\nalready gathered channel state information (CSI) in the RIS to beneficially\ndesign the phase shifts and does not impose additional probing burden on the\nnetwork. Additionally, this scheme is broadcast-based and does not entail the\noverheads of the pairwise-based key generation. We consider both passive RIS\n(PRIS) and active RIS (ARIS) to generate the group keys. The PRIS is widely\nadopted in physical layer key generation (PLKG) studies due to its use of\npassive elements, whereas the ARIS demonstrates superior capability in aligning\nthe aggregate reflected channels among nodes in the GKG scenario, as\ndemonstrated in this study. We will exploit various optimization methods like\nsuccessive convex approximation (SCA) and semidefinite relaxation with Gaussian\nrandomization (SDR-GR) to address the raised optimization problems. Unlike most\nof the studies in the literature, our scheme can achieve a high GKG rate in\nstatic environments as well. Finally, we will examine the performance of the\nproposed method by normalized mean squared error (NMSE), key error rate (KER),\nkey generation rate (KGR) and key randomness metrics. Our numerical results\nverify that for the equal available power budget, the ARIS significantly\noutperforms PRIS in NMSE and KER, achieving more than four times higher KGR.", "comment": "This manuscript has been submitted to IEEE Transactions on\n  Communications (TCOM) and is currently under review", "pdf_url": "http://arxiv.org/pdf/2507.00714v1", "categories": ["eess.SP"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2507.00714v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00874", "title": "Improving Stereo 3D Sound Event Localization and Detection: Perceptual Features, Stereo-specific Data Augmentation, and Distance Normalization", "authors": ["Jun-Wei Yeow", "Ee-Leng Tan", "Santi Peksi", "Woon-Seng Gan"], "summary": "This technical report presents our submission to Task 3 of the DCASE 2025\nChallenge: Stereo Sound Event Localization and Detection (SELD) in Regular\nVideo Content. We address the audio-only task in this report and introduce\nseveral key contributions. First, we design perceptually-motivated input\nfeatures that improve event detection, sound source localization, and distance\nestimation. Second, we adapt augmentation strategies specifically for the\nintricacies of stereo audio, including channel swapping and time-frequency\nmasking. We also incorporate the recently proposed FilterAugment technique that\nhas yet to be explored for SELD work. Lastly, we apply a distance normalization\napproach during training to stabilize regression targets. Experiments on the\nstereo STARSS23 dataset demonstrate consistent performance gains across all\nSELD metrics. Code to replicate our work is available in this repository:\nhttps://github.com/itsjunwei/NTU_SNTL_Task3", "comment": "Technical report for DCASE 2025 Challenge Task 3", "pdf_url": "http://arxiv.org/pdf/2507.00874v1", "categories": ["eess.AS"], "cate": "eess.AS", "url": "http://arxiv.org/abs/2507.00874v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00216", "title": "Towards Style Alignment in Cross-Cultural Translation", "authors": ["Shreya Havaldar", "Adam Stein", "Eric Wong", "Lyle Ungar"], "summary": "Successful communication depends on the speaker's intended style (i.e., what\nthe speaker is trying to convey) aligning with the listener's interpreted style\n(i.e., what the listener perceives). However, cultural differences often lead\nto misalignment between the two; for example, politeness is often lost in\ntranslation. We characterize the ways that LLMs fail to translate style -\nbiasing translations towards neutrality and performing worse in non-Western\nlanguages. We mitigate these failures with RASTA (Retrieval-Augmented STylistic\nAlignment), a method that leverages learned stylistic concepts to encourage LLM\ntranslation to appropriately convey cultural communication norms and align\nstyle.", "comment": "Accepted to ACL 2025", "pdf_url": "http://arxiv.org/pdf/2507.00216v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2507.00216v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00022", "title": "GLU Attention Improve Transformer", "authors": ["Zehao Wang"], "summary": "Gated Linear Units (GLU) have shown great potential in enhancing neural\nnetwork performance. In this paper, I introduce a novel attention mechanism\ncalled GLU Attention, which introduces nonlinearity into the values of\nAttention. My experiments demonstrate that GLU Attention improves both model\nperformance and convergence speed across text and vision modalities with zero\nadditional parameters and negligible computational costs. GLU Attention is\nlightweight and can seamlessly integrate with other technologies, such as Flash\nAttention, Rotary Position Embedding (RoPE), and various Multi-Head Attention\n(MHA) variants such as Grouped-Query Attention (GQA). This project is\nopen-sourced at github.", "comment": "4 pages 4 figures", "pdf_url": "http://arxiv.org/pdf/2507.00022v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.NE"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00022v1", "date": "2025-06-16", "updated": "2025-06-16"}
{"id": "2507.00521", "title": "\\texttt{WebANNS}: Fast and Efficient Approximate Nearest Neighbor Search in Web Browsers", "authors": ["Mugeng Liu", "Siqi Zhong", "Qi Yang", "Yudong Han", "Xuanzhe Liu", "Yun Ma"], "summary": "Approximate nearest neighbor search (ANNS) has become vital to modern AI\ninfrastructure, particularly in retrieval-augmented generation (RAG)\napplications. Numerous in-browser ANNS engines have emerged to seamlessly\nintegrate with popular LLM-based web applications, while addressing privacy\nprotection and challenges of heterogeneous device deployments. However, web\nbrowsers present unique challenges for ANNS, including computational\nlimitations, external storage access issues, and memory utilization\nconstraints, which state-of-the-art (SOTA) solutions fail to address\ncomprehensively.\n  We propose \\texttt{WebANNS}, a novel ANNS engine specifically designed for\nweb browsers. \\texttt{WebANNS} leverages WebAssembly to overcome computational\nbottlenecks, designs a lazy loading strategy to optimize data retrieval from\nexternal storage, and applies a heuristic approach to reduce memory usage.\nExperiments show that \\texttt{WebANNS} is fast and memory efficient, achieving\nup to $743.8\\times$ improvement in 99th percentile query latency over the SOTA\nengine, while reducing memory usage by up to 39\\%. Note that \\texttt{WebANNS}\ndecreases query time from 10 seconds to the 10-millisecond range in browsers,\nmaking in-browser ANNS practical with user-acceptable latency.", "comment": "SIGIR 2025", "pdf_url": "http://arxiv.org/pdf/2507.00521v1", "categories": ["cs.IR"], "cate": "cs.IR", "url": "http://arxiv.org/abs/2507.00521v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00267", "title": "Minimal residual rational Krylov subspace method for sequences of shifted linear systems", "authors": ["Hussam Al Daas", "Davide Palitta"], "summary": "The solution of sequences of shifted linear systems is a classic problem in\nnumerical linear algebra, and a variety of efficient methods have been proposed\nover the years. Nevertheless, there still exist challenging scenarios\nwitnessing a lack of performing solvers. For instance, state-of-the-art\nprocedures struggle to handle nonsymmetric problems where the shifts are\ncomplex numbers that do not come as conjugate pairs. We design a novel\nprojection strategy based on the rational Krylov subspace equipped with a\nminimal residual condition. We also devise a novel pole selection procedure,\ntailored to our problem, providing poles for the rational Krylov basis\nconstruction that yield faster convergence than those computed by available\ngeneral-purpose schemes. A panel of diverse numerical experiments shows that\nour novel approach performs better than state-of-the-art techniques, especially\non the very challenging problems mentioned above.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00267v1", "categories": ["math.NA", "cs.NA"], "cate": "math.NA", "url": "http://arxiv.org/abs/2507.00267v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00498", "title": "MuteSwap: Silent Face-based Voice Conversion", "authors": ["Yifan Liu", "Yu Fang", "Zhouhan Lin"], "summary": "Conventional voice conversion modifies voice characteristics from a source\nspeaker to a target speaker, relying on audio input from both sides. However,\nthis process becomes infeasible when clean audio is unavailable, such as in\nsilent videos or noisy environments. In this work, we focus on the task of\nSilent Face-based Voice Conversion (SFVC), which does voice conversion entirely\nfrom visual inputs. i.e., given images of a target speaker and a silent video\nof a source speaker containing lip motion, SFVC generates speech aligning the\nidentity of the target speaker while preserving the speech content in the\nsource silent video. As this task requires generating intelligible speech and\nconverting identity using only visual cues, it is particularly challenging. To\naddress this, we introduce MuteSwap, a novel framework that employs contrastive\nlearning to align cross-modality identities and minimize mutual information to\nseparate shared visual features. Experimental results show that MuteSwap\nachieves impressive performance in both speech synthesis and identity\nconversion, especially under noisy conditions where methods dependent on audio\ninput fail to produce intelligible results, demonstrating both the\neffectiveness of our training approach and the feasibility of SFVC.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00498v1", "categories": ["cs.SD", "cs.CV", "cs.LG", "cs.MM", "eess.AS"], "cate": "cs.SD", "url": "http://arxiv.org/abs/2507.00498v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00829", "title": "On the Surprising Efficacy of LLMs for Penetration-Testing", "authors": ["Andreas Happe", "Jürgen Cito"], "summary": "This paper presents a critical examination of the surprising efficacy of\nLarge Language Models (LLMs) in penetration testing. The paper thoroughly\nreviews the evolution of LLMs and their rapidly expanding capabilities which\nrender them increasingly suitable for complex penetration testing operations.\nIt systematically details the historical adoption of LLMs in both academic\nresearch and industry, showcasing their application across various offensive\nsecurity tasks and covering broader phases of the cyber kill chain. Crucially,\nthe analysis also extends to the observed adoption of LLMs by malicious actors,\nunderscoring the inherent dual-use challenge of this technology within the\nsecurity landscape.\n  The unexpected effectiveness of LLMs in this context is elucidated by several\nkey factors: the strong alignment between penetration testing's reliance on\npattern-matching and LLMs' core strengths, their inherent capacity to manage\nuncertainty in dynamic environments, and cost-effective access to competent\npre-trained models through LLM providers.\n  The current landscape of LLM-aided penetration testing is categorized into\ninteractive 'vibe-hacking' and the emergence of fully autonomous systems. The\npaper identifies and discusses significant obstacles impeding wider adoption\nand safe deployment. These include critical issues concerning model reliability\nand stability, paramount safety and security concerns, substantial monetary and\necological costs, implications for privacy and digital sovereignty, complex\nquestions of accountability, and profound ethical dilemmas. This comprehensive\nreview and analysis provides a foundation for discussion on future research\ndirections and the development of robust safeguards at the intersection of AI\nand security.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00829v1", "categories": ["cs.CR"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2507.00829v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00699", "title": "A Hierarchical and Evolvable Benchmark for Fine-Grained Code Instruction Following with Multi-Turn Feedback", "authors": ["Guoliang Duan", "Mingwei Liu", "Yanlin Wang", "Chong Wang", "Xin Peng", "Zibin Zheng"], "summary": "Large language models (LLMs) have advanced significantly in code generation,\nyet their ability to follow complex programming instructions with layered and\ndiverse constraints remains underexplored. Existing benchmarks often prioritize\nfunctional correctness, overlooking the nuanced requirements found in\nreal-world development. We introduce MultiCodeIF, a comprehensive benchmark\ndesigned to evaluate instruction-following in code generation across multiple\ndimensions: constraint type, hierarchical levels, and iterative refinement.\nBuilt upon a structured taxonomy of 9 categories and 27 constraint types,\nMultiCodeIF enables granular assessment of both functional and non-functional\ninstruction adherence. Using an automated pipeline, ConstraGen, we synthesize\nand evolve 2,021 code tasks sourced from 14 programming languages, supporting\nmulti-turn evaluation through feedback-driven task variants. Empirical\nevaluation of six state-of-the-art LLMs uncovers substantial performance\ndisparities. The top-performing model, Claude-3-7-Sonnet, achieves 63.0%\naverage constraint satisfaction, while smaller models like Qwen3-1.7B fall to\n44.8%. Models perform well on explicit constraints, but struggle with implicit\nor abstract constraints. Tasks with multiple hierarchical constraints\nsignificantly reduce model success rates, from 54.5% in single-level to just\n18.8% in multi-level scenarios. However, structured feedback enables\nprogressive improvement: average constraint satisfaction rises from 63.0% to\n83.4% over four iterative refinement rounds. MultiCodeIF provides a scalable,\nconstraint-aware, and feedback-sensitive framework to benchmark LLMs under\nrealistic code generation scenarios, bridging the gap between synthetic\nevaluations and real-world instruction complexity. The full benchmark dataset,\nevaluation pipeline, and source code are available at\nhttps://github.com/SYSUSELab/MultiCodeIF.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00699v1", "categories": ["cs.SE"], "cate": "cs.SE", "url": "http://arxiv.org/abs/2507.00699v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00288", "title": "Reconfiguring Digital Accountability: AI-Powered Innovations and Transnational Governance in a Postnational Accounting Context", "authors": ["Claire Li", "David Freeborn"], "summary": "This study explores how AI-powered digital innovations are reshaping\norganisational accountability in a transnational governance context. As AI\nsystems increasingly mediate decision-making in domains such as auditing and\nfinancial reporting, traditional mechanisms of accountability, based on\ncontrol, transparency, and auditability, are being destabilised. We integrate\nthe Technology Acceptance Model (TAM), Actor-Network Theory (ANT), and\ninstitutional theory to examine how organisations adopt AI technologies in\nresponse to regulatory, ethical, and cultural pressures that transcend national\nboundaries. We argue that accountability is co-constructed within global\nsocio-technical networks, shaped not only by user perceptions but also by\ngovernance logics and normative expectations. Extending TAM, we incorporate\ncompliance and legitimacy as key factors in perceived usefulness and usability.\nDrawing on ANT, we reconceptualise accountability as a relational and emergent\nproperty of networked assemblages. We propose two organisational strategies\nincluding internal governance reconfiguration and external actor-network\nengagement to foster responsible, legitimate, and globally accepted AI adoption\nin the accounting domain.", "comment": "22 pages", "pdf_url": "http://arxiv.org/pdf/2507.00288v1", "categories": ["econ.TH", "cs.AI", "cs.ET"], "cate": "econ.TH", "url": "http://arxiv.org/abs/2507.00288v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00443", "title": "Novel Pigeon-inspired 3D Obstacle Detection and Avoidance Maneuver for Multi-UAV Systems", "authors": ["Reza Ahmadvand", "Sarah Safura Sharif", "Yaser Mike Banad"], "summary": "Recent advances in multi-agent systems manipulation have demonstrated a\nrising demand for the implementation of multi-UAV systems in urban areas, which\nare always subjected to the presence of static and dynamic obstacles. Inspired\nby the collective behavior of tilapia fish and pigeons, the focus of the\npresented research is on the introduction of a nature-inspired collision-free\nformation control for a multi-UAV system, considering the obstacle avoidance\nmaneuvers. The developed framework in this study utilizes a semi-distributed\ncontrol approach, in which, based on a probabilistic Lloyd's algorithm, a\ncentralized guidance algorithm works for optimal positioning of the UAVs, while\na distributed control approach has been used for the intervehicle collision and\nobstacle avoidance. Further, the presented framework has been extended to the\n3D space with a novel definition of 3D maneuvers. Finally, the presented\nframework has been applied to multi-UAV systems in 2D and 3D scenarios, and the\nobtained results demonstrated the validity of the presented method in dynamic\nenvironments with stationary and moving obstacles.", "comment": "11 Pages, 11 Pictures, 1 Table, 3 Algorithms", "pdf_url": "http://arxiv.org/pdf/2507.00443v1", "categories": ["cs.RO", "cs.AI", "cs.MA"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2507.00443v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00428", "title": "Real-Time In-Network Machine Learning on P4-Programmable FPGA SmartNICs with Fixed-Point Arithmetic and Taylor", "authors": ["Mohammad Firas Sada", "John J. Graham", "Mahidhar Tatineni", "Dmitry Mishin", "Thomas A. DeFanti", "Frank Würthwein"], "summary": "As machine learning (ML) applications become integral to modern network\noperations, there is an increasing demand for network programmability that\nenables low-latency ML inference for tasks such as Quality of Service (QoS)\nprediction and anomaly detection in cybersecurity. ML models provide\nadaptability through dynamic weight adjustments, making Programming\nProtocol-independent Packet Processors (P4)-programmable FPGA SmartNICs an\nideal platform for investigating In-Network Machine Learning (INML). These\ndevices offer high-throughput, low-latency packet processing and can be\ndynamically reconfigured via the control plane, allowing for flexible\nintegration of ML models directly at the network edge. This paper explores the\napplication of the P4 programming paradigm to neural networks and regression\nmodels, where weights and biases are stored in control plane table lookups.\nThis approach enables flexible programmability and efficient deployment of\nretrainable ML models at the network edge, independent of core infrastructure\nat the switch level.", "comment": "To appear in Proceedings of the Practice and Experience in Advanced\n  Research Computing (PEARC25)", "pdf_url": "http://arxiv.org/pdf/2507.00428v1", "categories": ["cs.DC", "cs.NI"], "cate": "cs.DC", "url": "http://arxiv.org/abs/2507.00428v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00915", "title": "MichelangeRoll: Sculpting Rational Distributions Exactly and Efficiently", "authors": ["Jui-Hsiang Shao", "Hsin-Po Wang"], "summary": "Simulating an arbitrary discrete distribution $D \\in [0, 1]^n$ using fair\ncoin tosses incurs trade-offs between entropy complexity and space and time\ncomplexity. Shannon's theory suggests that $H(D)$ tosses are necessary and\nsufficient, but does not guarantee exact distribution. Knuth and Yao showed\nthat a decision tree consumes fewer than $H(D) + 2$ tosses for one exact\nsample. Drapper and Saad's recent work addresses the space and time aspect,\nshowing that $H(D) + 2$ tosses, $O(n \\log(n) \\log(m))$ memory, and $O(H(D))$\noperations are all it costs, where $m$ is the common denominator of the\nprobability masses in $D$ and $n$ is the number of possible outcomes.\n  In this paper, MichelangeRoll recycles leftover entropy to break the \"$+2$\"\nbarrier. With $O((n + 1/\\varepsilon) \\log(m/\\varepsilon))$ memory, the entropy\ncost of generating a ongoing sequence of $D$ is reduced to $H(D) + \\varepsilon$\nper sample.", "comment": "13 pages, 7 figures, RANDOM says no so here", "pdf_url": "http://arxiv.org/pdf/2507.00915v1", "categories": ["cs.IT", "math.IT"], "cate": "cs.IT", "url": "http://arxiv.org/abs/2507.00915v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00181", "title": "ChatGPT produces more \"lazy\" thinkers: Evidence of cognitive engagement decline", "authors": ["Georgios P. Georgiou"], "summary": "Despite the increasing use of large language models (LLMs) in education,\nconcerns have emerged about their potential to reduce deep thinking and active\nlearning. This study investigates the impact of generative artificial\nintelligence (AI) tools, specifically ChatGPT, on the cognitive engagement of\nstudents during academic writing tasks. The study employed an experimental\ndesign with participants randomly assigned to either an AI-assisted (ChatGPT)\nor a non-assisted (control) condition. Participants completed a structured\nargumentative writing task followed by a cognitive engagement scale (CES), the\nCES-AI, developed to assess mental effort, attention, deep processing, and\nstrategic thinking. The results revealed significantly lower cognitive\nengagement scores in the ChatGPT group compared to the control group. These\nfindings suggest that AI assistance may lead to cognitive offloading. The study\ncontributes to the growing body of literature on the psychological implications\nof AI in education and raises important questions about the integration of such\ntools into academic practice. It calls for pedagogical strategies that promote\nactive, reflective engagement with AI-generated content to avoid compromising\nself-regulated learning and deep cognitive involvement of students.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00181v1", "categories": ["cs.AI"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2507.00181v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00016", "title": "Gradient-based Fine-Tuning through Pre-trained Model Regularization", "authors": ["Xuanbo Liu", "Liu Liu", "Fuxiang Wu", "Fusheng Hao", "Xianglong Liu"], "summary": "Large pre-trained models have demonstrated extensive applications across\nvarious fields. However, fine-tuning these models for specific downstream tasks\ndemands significant computational resources and storage. One fine-tuning\nmethod, gradient-based parameter selection (GPS), focuses on fine-tuning only\nthe parameters with high gradients in each neuron, thereby reducing the number\nof training parameters. Nevertheless, this approach increases computational\nresource requirements and storage demands. In this paper, we propose an\nefficient gradient-based and regularized fine-tuning method (GRFT) that updates\nthe rows or columns of the weight matrix. We theoretically demonstrate that the\nrows or columns with the highest sum of squared gradients are optimal for\nupdating. This strategy effectively reduces storage overhead and improves the\nefficiency of parameter selection. Additionally, we incorporate regularization\nto enhance knowledge transfer from the pre-trained model. GRFT achieves\nstate-of-the-art performance, surpassing existing methods such as GPS, Adapter\nTuning, and LoRA. Notably, GRFT requires updating only 1.22% and 0.30% of the\ntotal parameters on FGVC and VTAB datasets, respectively, demonstrating its\nhigh efficiency and effectiveness. The source code will be released soon.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00016v1", "categories": ["cs.LG", "cs.AI", "cs.CV"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00016v1", "date": "2025-06-14", "updated": "2025-06-14"}
{"id": "2507.00855", "title": "A New Family of Thread to Core Allocation Policies for an SMT ARM Processor", "authors": ["Marta Navarro", "Josué Feliu", "Salvador Petit", "María E. Gómez", "Julio Sahuquillo"], "summary": "Modern high-performance servers commonly integrate Simultaneous\nMultithreading (SMT) processors, which efficiently boosts throughput over\nsingle-threaded cores. Optimizing performance in SMT processors faces\nchallenges due to the inter-application interference within each SMT core. To\nmitigate the interference, thread-to-core (T2C) allocation policies play a\npivotal role. State-of-the-art T2C policies work in two steps: i) building a\nper-application performance stack using performance counters and ii) building\nperformance prediction models to identify the best pairs of applications to run\non each core.\n  This paper explores distinct ways to build the performance stack in ARM\nprocessors and introduces the Instructions and Stalls Cycles (ISC) stack, a\nnovel approach to overcome ARM PMU limitations. The ISC stacks are used as\ninputs for a performance prediction model to estimate the applications'\nperformance considering the inter-application interference. The accuracy of the\nprediction model (second step) depends on the accuracy of the performance stack\n(first step); thus, the higher the accuracy of the performance stack, the\nhigher the potential performance gains obtained by the T2C allocation policy.\n  This paper presents SYNPA as a family of T2C allocation policies.\nExperimental results show that $SYNPA4$, the best-performing SYNPA variant,\noutperforms turnaround time by 38\\% over Linux, which represents 3$\\times$ the\ngains achieved by the state-of-the-art policies for ARM processors.\nFurthermore, the multiple discussions and refinements presented throughout this\npaper can be applied to other SMT processors from distinct vendors and are\naimed at helping performance analysts build performance stacks for accurate\nperformance estimates in real processors.", "comment": "13 pages", "pdf_url": "http://arxiv.org/pdf/2507.00855v1", "categories": ["cs.DC", "cs.AR"], "cate": "cs.DC", "url": "http://arxiv.org/abs/2507.00855v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00068", "title": "MANTA: Cross-Modal Semantic Alignment and Information-Theoretic Optimization for Long-form Multimodal Understanding", "authors": ["Ziqi Zhong", "Daniel Tang"], "summary": "While multi-modal learning has advanced significantly, current approaches\noften treat modalities separately, creating inconsistencies in representation\nand reasoning. We introduce MANTA (Multi-modal Abstraction and Normalization\nvia Textual Alignment), a theoretically-grounded framework that unifies visual\nand auditory inputs into a structured textual space for seamless processing\nwith large language models. MANTA addresses four key challenges: (1) semantic\nalignment across modalities with information-theoretic optimization, (2)\nadaptive temporal synchronization for varying information densities, (3)\nhierarchical content representation for multi-scale understanding, and (4)\ncontext-aware retrieval of sparse information from long sequences. We formalize\nour approach within a rigorous mathematical framework, proving its optimality\nfor context selection under token constraints. Extensive experiments on the\nchallenging task of Long Video Question Answering show that MANTA improves\nstate-of-the-art models by up to 22.6% in overall accuracy, with particularly\nsignificant gains (27.3%) on videos exceeding 30 minutes. Additionally, we\ndemonstrate MANTA's superiority on temporal reasoning tasks (23.8% improvement)\nand cross-modal understanding (25.1% improvement). Our framework introduces\nnovel density estimation techniques for redundancy minimization while\npreserving rare signals, establishing new foundations for unifying multimodal\nrepresentations through structured text.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00068v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00068v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2507.00513", "title": "Customer Service Representative's Perception of the AI Assistant in an Organization's Call Center", "authors": ["Kai Qin", "Kexin Du", "Yimeng Chen", "Yueyan Liu", "Jie Cai", "Zhiqiang Nie", "Nan Gao", "Guohui Wei", "Shengzhu Wang", "Chun Yu"], "summary": "The integration of various AI tools creates a complex socio-technical\nenvironment where employee-customer interactions form the core of work\npractices. This study investigates how customer service representatives (CSRs)\nat the power grid service customer service call center perceive AI assistance\nin their interactions with customers. Through a field visit and semi-structured\ninterviews with 13 CSRs, we found that AI can alleviate some traditional\nburdens during the call (e.g., typing and memorizing) but also introduces new\nburdens (e.g., earning, compliance, psychological burdens). This research\ncontributes to a more nuanced understanding of AI integration in organizational\nsettings and highlights the efforts and burdens undertaken by CSRs to adapt to\nthe updated system.", "comment": "ACM CSCW Poster 2025", "pdf_url": "http://arxiv.org/pdf/2507.00513v1", "categories": ["cs.HC", "cs.AI", "cs.CY"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2507.00513v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00613", "title": "Physics-Informed Neural ODEs for Temporal Dynamics Modeling in Cardiac T1 Mapping", "authors": ["Nuno Capitão", "Yi Zhang", "Yidong Zhao", "Qian Tao"], "summary": "Spin-lattice relaxation time ($T_1$) is an important biomarker in cardiac\nparametric mapping for characterizing myocardial tissue and diagnosing\ncardiomyopathies. Conventional Modified Look-Locker Inversion Recovery (MOLLI)\nacquires 11 breath-hold baseline images with interleaved rest periods to ensure\nmapping accuracy. However, prolonged scanning can be challenging for patients\nwith poor breathholds, often leading to motion artifacts that degrade image\nquality. In addition, $T_1$ mapping requires voxel-wise nonlinear fitting to a\nsignal recovery model involving an iterative estimation process. Recent studies\nhave proposed deep-learning approaches for rapid $T_1$ mapping using shortened\nsequences to reduce acquisition time for patient comfort. Nevertheless,\nexisting methods overlook important physics constraints, limiting\ninterpretability and generalization. In this work, we present an accelerated,\nend-to-end $T_1$ mapping framework leveraging Physics-Informed Neural Ordinary\nDifferential Equations (ODEs) to model temporal dynamics and address these\nchallenges. Our method achieves high-accuracy $T_1$ estimation from a sparse\nsubset of baseline images and ensures efficient null index estimation at test\ntime. Specifically, we develop a continuous-time LSTM-ODE model to enable\nselective Look-Locker (LL) data acquisition with arbitrary time lags.\nExperimental results show superior performance in $T_1$ estimation for both\nnative and post-contrast sequences and demonstrate the strong benefit of our\nphysics-based formulation over direct data-driven $T_1$ priors.", "comment": "Submitted version. Accepted at MICCAI 2025", "pdf_url": "http://arxiv.org/pdf/2507.00613v1", "categories": ["eess.IV", "cs.AI"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2507.00613v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00333", "title": "Scope Meets Screen: Lessons Learned in Designing Composite Visualizations for Marksmanship Training Across Skill Levels", "authors": ["Emin Zerman", "Jonas Carlsson", "Mårten Sjöström"], "summary": "Marksmanship practices are required in various professions, including police,\nmilitary personnel, hunters, as well as sports shooters, such as Olympic\nshooting, biathlon, and modern pentathlon. The current form of training and\ncoaching is mostly based on repetition, where the coach does not see through\nthe eyes of the shooter, and analysis is limited to stance and accuracy\npost-session. In this study, we present a shooting visualization system and\nevaluate its perceived effectiveness for both novice and expert shooters. To\nachieve this, five composite visualizations were developed using first-person\nshooting video recordings enriched with overlaid metrics and graphical\nsummaries. These views were evaluated with 10 participants (5 expert marksmen,\n5 novices) through a mixed-methods study including shot-count and aiming\ninterpretation tasks, pairwise preference comparisons, and semi-structured\ninterviews. The results show that a dashboard-style composite view, combining\nraw video with a polar plot and selected graphs, was preferred in 9 of 10 cases\nand supported understanding across skill levels. The insights gained from this\ndesign study point to the broader value of integrating first-person video with\nvisual analytics for coaching, and we suggest directions for applying this\napproach to other precision-based sports.", "comment": "5 pages, accepted at IEEE VIS 2025", "pdf_url": "http://arxiv.org/pdf/2507.00333v1", "categories": ["cs.HC", "cs.CV", "cs.GR", "eess.IV"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2507.00333v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00612", "title": "Hamiltonicity Parameterized by Mim-Width is (Indeed) Para-NP-Hard", "authors": ["Benjamin Bergougnoux", "Lars Jaffke"], "summary": "We prove that Hamiltonian Path and Hamiltonian Cycle are NP-hard on graphs of\nlinear mim-width 26, even when a linear order of the input graph with mim-width\n26 is provided together with input. This fills a gap left by a broken proof of\nthe para-NP-hardness of Hamiltonicity problems parameterized by mim-width.", "comment": "11 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2507.00612v1", "categories": ["cs.CC", "cs.DS", "68Q17, 68Q25, 68Q27"], "cate": "cs.CC", "url": "http://arxiv.org/abs/2507.00612v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00628", "title": "Price Aware Power Split Control in Heterogeneous Battery Storage Systems", "authors": ["Sheng Yin", "Vivek Teja Tanjavooru", "Thomas Hamacher", "Christoph Goebel", "Holger Hesse"], "summary": "This paper presents a unified framework for the optimal scheduling of battery\ndispatch and internal power allocation in Battery energy storage systems\n(BESS). This novel approach integrates both market-based (price-aware) signals\nand physical system constraints to simultaneously optimize (1) external energy\ndispatch and (2) internal heterogeneity management of BESS, enhancing its\noperational economic value and performance. This work compares both model-based\nLinear Programming (LP) and model-free Reinforcement Learning (RL) approaches\nfor optimization under varying forecast assumptions, using a custom Gym-based\nsimulation environment. The evaluation considers both long-term and short-term\nperformance, focusing on economic savings, State of Charge (SOC) and\ntemperature balancing, and overall system efficiency. In summary, the long-term\nresults show that the RL approach achieved 10% higher system efficiency\ncompared to LP, whereas the latter yielded 33% greater cumulative savings. In\nterms of internal heterogeneity, the LP approach resulted in lower mean SOC\nimbalance, while the RL approach achieved better temperature balance between\nstrings. This behavior is further examined in the short-term evaluation, which\nindicates that LP delivers strong optimization under known and stable\nconditions, whereas RL demonstrates higher adaptability in dynamic\nenvironments, offering potential advantages for real-time BESS control.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00628v1", "categories": ["eess.SY", "cs.SY"], "cate": "eess.SY", "url": "http://arxiv.org/abs/2507.00628v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00895", "title": "SComCP: Task-Oriented Semantic Communication for Collaborative Perception", "authors": ["Jipeng Gan", "Yucheng Sheng", "Hua Zhang", "Le Liang", "Hao Ye", "Chongtao Guo", "Shi Jin"], "summary": "Reliable detection of surrounding objects is critical for the safe operation\nof connected automated vehicles (CAVs). However, inherent limitations such as\nthe restricted perception range and occlusion effects compromise the\nreliability of single-vehicle perception systems in complex traffic\nenvironments. Collaborative perception has emerged as a promising approach by\nfusing sensor data from surrounding CAVs with diverse viewpoints, thereby\nimproving environmental awareness. Although collaborative perception holds\ngreat promise, its performance is bottlenecked by wireless communication\nconstraints, as unreliable and bandwidth-limited channels hinder the\ntransmission of sensor data necessary for real-time perception. To address\nthese challenges, this paper proposes SComCP, a novel task-oriented semantic\ncommunication framework for collaborative perception. Specifically, SComCP\nintegrates an importance-aware feature selection network that selects and\ntransmits semantic features most relevant to the perception task, significantly\nreducing communication overhead without sacrificing accuracy. Furthermore, we\ndesign a semantic codec network based on a joint source and channel coding\n(JSCC) architecture, which enables bidirectional transformation between\nsemantic features and noise-tolerant channel symbols, thereby ensuring stable\nperception under adverse wireless conditions. Extensive experiments demonstrate\nthe effectiveness of the proposed framework. In particular, compared to\nexisting approaches, SComCP can maintain superior perception performance across\nvarious channel conditions, especially in low signal-to-noise ratio (SNR)\nscenarios. In addition, SComCP exhibits strong generalization capability,\nenabling the framework to maintain high performance across diverse channel\nconditions, even when trained with a specific channel model.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00895v1", "categories": ["eess.SP"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2507.00895v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00055", "title": "Leveraging Unlabeled Audio-Visual Data in Speech Emotion Recognition using Knowledge Distillation", "authors": ["Varsha Pendyala", "Pedro Morgado", "William Sethares"], "summary": "Voice interfaces integral to the human-computer interaction systems can\nbenefit from speech emotion recognition (SER) to customize responses based on\nuser emotions. Since humans convey emotions through multi-modal audio-visual\ncues, developing SER systems using both the modalities is beneficial. However,\ncollecting a vast amount of labeled data for their development is expensive.\nThis paper proposes a knowledge distillation framework called LightweightSER\n(LiSER) that leverages unlabeled audio-visual data for SER, using large teacher\nmodels built on advanced speech and face representation models. LiSER transfers\nknowledge regarding speech emotions and facial expressions from the teacher\nmodels to lightweight student models. Experiments conducted on two benchmark\ndatasets, RAVDESS and CREMA-D, demonstrate that LiSER can reduce the dependence\non extensive labeled datasets for SER tasks.", "comment": "Accepted at INTERSPEECH 2025", "pdf_url": "http://arxiv.org/pdf/2507.00055v1", "categories": ["cs.LG", "cs.HC", "cs.MM", "eess.AS", "eess.IV", "eess.SP"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00055v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2507.00239", "title": "Linearly Decoding Refused Knowledge in Aligned Language Models", "authors": ["Aryan Shrivastava", "Ari Holtzman"], "summary": "Most commonly used language models (LMs) are instruction-tuned and aligned\nusing a combination of fine-tuning and reinforcement learning, causing them to\nrefuse users requests deemed harmful by the model. However, jailbreak prompts\ncan often bypass these refusal mechanisms and elicit harmful responses. In this\nwork, we study the extent to which information accessed via jailbreak prompts\nis decodable using linear probes trained on LM hidden states. We show that a\ngreat deal of initially refused information is linearly decodable. For example,\nacross models, the response of a jailbroken LM for the average IQ of a country\ncan be predicted by a linear probe with Pearson correlations exceeding $0.8$.\nSurprisingly, we find that probes trained on base models (which do not refuse)\nsometimes transfer to their instruction-tuned versions and are capable of\nrevealing information that jailbreaks decode generatively, suggesting that the\ninternal representations of many refused properties persist from base LMs\nthrough instruction-tuning. Importantly, we show that this information is not\nmerely \"leftover\" in instruction-tuned models, but is actively used by them: we\nfind that probe-predicted values correlate with LM generated pairwise\ncomparisons, indicating that the information decoded by our probes align with\nsuppressed generative behavior that may be expressed more subtly in other\ndownstream tasks. Overall, our results suggest that instruction-tuning does not\nwholly eliminate or even relocate harmful information in representation\nspace-they merely suppress its direct expression, leaving it both linearly\naccessible and indirectly influential in downstream behavior.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00239v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2507.00239v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00032", "title": "Ken Utilization Layer: Hebbian Replay Within a Student's Ken for Adaptive Knowledge Tracing", "authors": ["Grey Kuling", "Marinka Zitnik"], "summary": "We introduce KUL-KT, a biologically inspired architecture for knowledge\ntracing (KT), combining Hebbian memory encoding with gradient-based\nconsolidation in a scalable, input-agnostic framework. KUL-KT adapts the\nprinciple of memory consolidation in neural systems, to student modeling by\nintroducing two key innovations: (i) a time-decaying Hebbian memory update that\nenables graceful forgetting, and (ii) a novel Loss-aligned Internal Target\n(LIT) method to compute an ideal internal state, allowing continual learning\nwithout backpropagation through time. The architecture consists of a fast\nHebbian memory that captures each learner interaction via a single associative\nupdate, and a slower linear network that consolidates recalled samples through\ngradient descent. This design enables few-shot personalization and natural\nforgetting without storing raw data or relying on large cohort training.\nOperating entirely in embedding space, KUL-KT supports both structured\n(tabular) and unstructured (short-answer) inputs. Empirically, KUL-KT\noutperforms strong baselines on ten public KT benchmarks in rank-sensitive\nmetrics such as nDCG and Recall@10. In a classroom deployment, KUL-KT\npersonalized quizzes from short-answer data, leading to improved\nlearner-perceived helpfulness and reduced difficulty (p < 0.05). Ablation\nstudies confirm that Hebbian decay and LIT are critical for continual\nadaptation. Compared to a strong graph-based KT model, KUL-KT trains 1.75x\nfaster and uses 99.01\\% less memory. These results position KUL-KT as a\nbiologically grounded, memory-efficient, and input-flexible framework for\npersonalized learning at scale.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00032v1", "categories": ["cs.CY", "cs.AI", "cs.LG", "cs.NE"], "cate": "cs.CY", "url": "http://arxiv.org/abs/2507.00032v1", "date": "2025-06-18", "updated": "2025-06-18"}
{"id": "2507.00535", "title": "Rethinking Group Recommender Systems in the Era of Generative AI: From One-Shot Recommendations to Agentic Group Decision Support", "authors": ["Dietmar Jannach", "Amra Delić", "Francesco Ricci", "Markus Zanker"], "summary": "More than twenty-five years ago, first ideas were developed on how to design\na system that can provide recommendations to groups of users instead of\nindividual users. Since then, a rich variety of algorithmic proposals were\npublished, e.g., on how to acquire individual preferences, how to aggregate\nthem, and how to generate recommendations for groups of users. However, despite\nthe rich literature on the topic, barely any examples of real-world group\nrecommender systems can be found. This lets us question common assumptions in\nacademic research, in particular regarding communication processes in a group\nand how recommendation-supported decisions are made. In this essay, we argue\nthat these common assumptions and corresponding system designs often may not\nmatch the needs or expectations of users. We thus call for a reorientation in\nthis research area, leveraging the capabilities of modern Generative AI\nassistants like ChatGPT. Specifically, as one promising future direction, we\nenvision group recommender systems to be systems where human group members\ninteract in a chat and an AI-based group recommendation agent assists the\ndecision-making process in an agentic way. Ultimately, this shall lead to a\nmore natural group decision-making environment and finally to wider adoption of\ngroup recommendation systems in practice.", "comment": "Submitted for publication", "pdf_url": "http://arxiv.org/pdf/2507.00535v1", "categories": ["cs.IR", "cs.AI"], "cate": "cs.IR", "url": "http://arxiv.org/abs/2507.00535v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00278", "title": "Automatic discovery of optimal meta-solvers for time-dependent nonlinear PDEs", "authors": ["Youngkyu Lee", "Shanqing Liu", "Jerome Darbon", "George Em Karniadakis"], "summary": "We present a general and scalable framework for the automated discovery of\noptimal meta-solvers for the solution of time-dependent nonlinear partial\ndifferential equations after appropriate discretization. By integrating\nclassical numerical methods (e.g., Krylov-based methods) with modern deep\nlearning components, such as neural operators, our approach enables flexible,\non-demand solver design tailored to specific problem classes and objectives.\nThe fast solvers tackle the large linear system resulting from the\nNewton--Raphson iteration or by using an implicit-explicit (IMEX) time\nintegration scheme. Specifically, we formulate solver discovery as a\nmulti-objective optimization problem, balancing various performance criteria\nsuch as accuracy, speed, and memory usage. The resulting Pareto optimal set\nprovides a principled foundation for solver selection based on user-defined\npreference functions. When applied to problems in reaction--diffusion, fluid\ndynamics, and solid mechanics, the discovered meta-solvers consistently\noutperform conventional iterative methods, demonstrating both practical\nefficiency and broad applicability.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00278v1", "categories": ["math.NA", "cs.NA"], "cate": "math.NA", "url": "http://arxiv.org/abs/2507.00278v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00693", "title": "Leveraging Large Language Models for Spontaneous Speech-Based Suicide Risk Detection", "authors": ["Yifan Gao", "Jiao Fu", "Long Guo", "Hong Liu"], "summary": "Early identification of suicide risk is crucial for preventing suicidal\nbehaviors. As a result, the identification and study of patterns and markers\nrelated to suicide risk have become a key focus of current research. In this\npaper, we present the results of our work in the 1st SpeechWellness Challenge\n(SW1), which aims to explore speech as a non-invasive and easily accessible\nmental health indicator for identifying adolescents at risk of suicide.Our\napproach leverages large language model (LLM) as the primary tool for feature\nextraction, alongside conventional acoustic and semantic features. The proposed\nmethod achieves an accuracy of 74\\% on the test set, ranking first in the SW1\nchallenge. These findings demonstrate the potential of LLM-based methods for\nanalyzing speech in the context of suicide risk assessment.", "comment": "Accepted to Interspeech 2025", "pdf_url": "http://arxiv.org/pdf/2507.00693v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "cate": "cs.SD", "url": "http://arxiv.org/abs/2507.00693v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00847", "title": "Stealtooth: Breaking Bluetooth Security Abusing Silent Automatic Pairing", "authors": ["Keiichiro Kimura", "Hiroki Kuzuno", "Yoshiaki Shiraishi", "Masakatu Morii"], "summary": "Bluetooth is a pervasive wireless communication technology used by billions\nof devices for short-range connectivity. The security of Bluetooth relies on\nthe pairing process, where devices establish shared long-term keys for secure\ncommunications. However, many commercial Bluetooth devices implement automatic\npairing functions to improve user convenience, creating a previously unexplored\nattack surface.\n  We present Stealtooth, a novel attack that abuses unknown vulnerabilities in\nthe automatic pairing functions in commercial Bluetooth devices to achieve\ncompletely silent device link key overwriting. The Stealtooth attack leverages\nthe fact that Bluetooth audio devices automatically transition to pairing mode\nunder specific conditions, enabling attackers to hijack pairing processes\nwithout user awareness or specialized tools. We also extend the attack into the\nMitM Stealtooth attack, combining automatic pairing abuse with power-saving\nmode techniques to enable man-in-the-middle attacks.\n  We evaluate the attacks against 10 commercial Bluetooth devices from major\nmanufacturers, demonstrating widespread vulnerabilities across diverse device\ntypes and manufacturers. Our practical implementation requires only commodity\nhardware and open-source software, highlighting the low barrier to entry for\nattackers.\n  We propose defenses both device and protocol levels, including enhanced user\nnotifications and standardized automatic pairing guidelines. Our findings\nreveal a critical tension between security and usability, showing that current\nautomatic pairing implementations create systematic vulnerabilities. We\nresponsibly disclosed our findings to affected vendors, with several already\nreleasing patches.", "comment": "13 pages, 6 figures. We plan to extend our evaluation to additional\n  device categories. Responsible disclosure completed", "pdf_url": "http://arxiv.org/pdf/2507.00847v1", "categories": ["cs.CR", "cs.NI"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2507.00847v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00786", "title": "Snaps: Bloated and Outdated?", "authors": ["Jukka Ruohonen", "Qusai Ramadan"], "summary": "Snap is an alternative software packaging system developed by Canonical and\nprovided by default in the Ubuntu Linux distribution. Given the heterogeneity\nof various Linux distributions and their various releases, Snap allows an\ninteroperable delivery of software directly to users. However, concerns and\ncriticism have also been frequently expressed. Regarding this criticism, the\npaper shows that currently distributed snap packages are indeed on average\nbloated in terms of their sizes and outdated in terms updating frequencies.\nWith these empirical observations, this short paper contributes to the research\ndomain of software packaging, software packages, and package managers.", "comment": "Submitted as a \"poster paper\" to APSEC", "pdf_url": "http://arxiv.org/pdf/2507.00786v1", "categories": ["cs.SE"], "cate": "cs.SE", "url": "http://arxiv.org/abs/2507.00786v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00352", "title": "An AST-guided LLM Approach for SVRF Code Synthesis", "authors": ["Abanoub E. Abdelmalak", "Mohamed A. Elsayed", "David Abercrombie", "Ilhami Torunoglu"], "summary": "Standard Verification Rule Format (SVRF) is essential for semiconductor\napplications like Design Rule Check (DRC), Layout Versus Schematic (LVS), and\nOptical Proximity Correction (OPC) and it faces challenges as advancing nodes\ncreate complex design rules that renders traditional SVRF development\nineffective and highlight an expertise gap. This paper introduces a novel\nmethodology integrating Abstract Syntax Tree (AST) embedding and\nRetrieval-Augmented Generation (RAG) for enhanced SVRF code synthesis, ensuring\nsemantic accuracy and error minimization through structural validation with\ndomain-specific insights for precise code generation.\n  We evaluate different T5-based models and propose an innovative SVRF-specific\nscoring framework that complements standard metrics like BLEU and ROUGE-L. In\nour approach, AST provides rigorous structural validation, while RAG infuses\nrelevant domain knowledge, effectively enhancing the code generation workflow.\n  Testing on a comprehensive benchmark of 740 DRC rule implementations, our\nmethodology demonstrates up to a 40\\% improvement in code generation accuracy\ncompared to basic text-based fine-tuning process. This fusion of industry\nexpertise with advanced coding strategies not only optimizes SVRF development\nunder limited dataset constraints but also creates a more intuitive and\nefficient coding environment. Consequently, users can rapidly iterate through\ndesign cycles, reduce manual error correction, and significantly improve\noverall productivity.", "comment": "9 Pages, 5 Figures, 2 Tables", "pdf_url": "http://arxiv.org/pdf/2507.00352v1", "categories": ["cs.SE", "cs.AI", "cs.ET"], "cate": "cs.SE", "url": "http://arxiv.org/abs/2507.00352v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00446", "title": "DIJE: Dense Image Jacobian Estimation for Robust Robotic Self-Recognition and Visual Servoing", "authors": ["Yasunori Toshimitsu", "Kento Kawaharazuka", "Akihiro Miki", "Kei Okada", "Masayuki Inaba"], "summary": "For robots to move in the real world, they must first correctly understand\nthe state of its own body and the tools that it holds. In this research, we\npropose DIJE, an algorithm to estimate the image Jacobian for every pixel. It\nis based on an optical flow calculation and a simplified Kalman Filter that can\nbe efficiently run on the whole image in real time. It does not rely on markers\nnor knowledge of the robotic structure. We use the DIJE in a self-recognition\nprocess which can robustly distinguish between movement by the robot and by\nexternal entities, even when the motion overlaps. We also propose a visual\nservoing controller based on DIJE, which can learn to control the robot's body\nto conduct reaching movements or bimanual tool-tip control. The proposed\nalgorithms were implemented on a physical musculoskeletal robot and its\nperformance was verified. We believe that such global estimation of the\nvisuomotor policy has the potential to be extended into a more general\nframework for manipulation.", "comment": "2022 IEEE/RSJ International Conference on Intelligent Robots and\n  Systems (IROS)", "pdf_url": "http://arxiv.org/pdf/2507.00446v1", "categories": ["cs.RO"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2507.00446v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00523", "title": "Edge Computing and its Application in Robotics: A Survey", "authors": ["Nazish Tahir", "Ramviyas Parasuraman"], "summary": "The Edge computing paradigm has gained prominence in both academic and\nindustry circles in recent years. By implementing edge computing facilities and\nservices in robotics, it becomes a key enabler in the deployment of artificial\nintelligence applications to robots. Time-sensitive robotics applications\nbenefit from the reduced latency, mobility, and location awareness provided by\nthe edge computing paradigm, which enables real-time data processing and\nintelligence at the network's edge. While the advantages of integrating edge\ncomputing into robotics are numerous, there has been no recent survey that\ncomprehensively examines these benefits. This paper aims to bridge that gap by\nhighlighting important work in the domain of edge robotics, examining recent\nadvancements, and offering deeper insight into the challenges and motivations\nbehind both current and emerging solutions. In particular, this article\nprovides a comprehensive evaluation of recent developments in edge robotics,\nwith an emphasis on fundamental applications, providing in-depth analysis of\nthe key motivations, challenges, and future directions in this rapidly evolving\ndomain. It also explores the importance of edge computing in real-world\nrobotics scenarios where rapid response times are critical. Finally, the paper\noutlines various open research challenges in the field of edge robotics.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00523v1", "categories": ["cs.RO", "cs.DC", "cs.NI"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2507.00523v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00942", "title": "Optimal Feedback Schemes for Dirty Paper Channels With State Estimation at the Receiver", "authors": ["Dengfeng Xia", "Han Deng", "Haonan Zhang", "Fan Cheng", "Bin Dai", "Liuguo Yin"], "summary": "In the literature, it has been shown that feedback does not increase the\noptimal rate-distortion region of the dirty paper channel with state estimation\nat the receiver (SE-R). On the other hand, it is well-known that feedback helps\nto construct low-complexity coding schemes in Gaussian channels, such as the\nelegant Schalkwijk-Kailath (SK) feedback scheme. This motivates us to explore\ncapacity-achieving SK-type schemes in dirty paper channels with SE-R and\nfeedback. In this paper, we first propose a capacity-achieving feedback scheme\nfor the dirty paper channel with SE-R (DPC-SE-R), which combines the\nsuperposition coding and the classical SK-type scheme. Then, we extend this\nscheme to the dirty paper multiple-access channel with SE-R and feedback, and\nalso show the extended scheme is capacity-achieving. Finally, we discuss how to\nextend our scheme to a noisy state observation case of the DPC-SE-R. However,\nthe capacity-achieving SK-type scheme for such a case remains unknown.", "comment": "This paper will be presented at the 2025 IEEE Information Theory\n  Workshop (ITW)", "pdf_url": "http://arxiv.org/pdf/2507.00942v1", "categories": ["cs.IT", "math.IT"], "cate": "cs.IT", "url": "http://arxiv.org/abs/2507.00942v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00205", "title": "Holistic Artificial Intelligence in Medicine; improved performance and explainability", "authors": ["Periklis Petridis", "Georgios Margaritis", "Vasiliki Stoumpou", "Dimitris Bertsimas"], "summary": "With the increasing interest in deploying Artificial Intelligence in\nmedicine, we previously introduced HAIM (Holistic AI in Medicine), a framework\nthat fuses multimodal data to solve downstream clinical tasks. However, HAIM\nuses data in a task-agnostic manner and lacks explainability. To address these\nlimitations, we introduce xHAIM (Explainable HAIM), a novel framework\nleveraging Generative AI to enhance both prediction and explainability through\nfour structured steps: (1) automatically identifying task-relevant patient data\nacross modalities, (2) generating comprehensive patient summaries, (3) using\nthese summaries for improved predictive modeling, and (4) providing clinical\nexplanations by linking predictions to patient-specific medical knowledge.\nEvaluated on the HAIM-MIMIC-MM dataset, xHAIM improves average AUC from 79.9%\nto 90.3% across chest pathology and operative tasks. Importantly, xHAIM\ntransforms AI from a black-box predictor into an explainable decision support\nsystem, enabling clinicians to interactively trace predictions back to relevant\npatient data, bridging AI advancements with clinical utility.", "comment": "Submitted to npj Digital Medicine", "pdf_url": "http://arxiv.org/pdf/2507.00205v1", "categories": ["cs.AI", "cs.LG"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2507.00205v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00018", "title": "Implicit Reward as the Bridge: A Unified View of SFT and DPO Connections", "authors": ["Bo Wang", "Qinyuan Cheng", "Runyu Peng", "Rong Bao", "Peiji Li", "Qipeng Guo", "Linyang Li", "Zhiyuan Zeng", "Yunhua Zhou", "Xipeng Qiu"], "summary": "Post-training processes are essential phases in grounding pre-trained\nlanguage models to real-world tasks, with learning from demonstrations or\npreference signals playing a crucial role in this adaptation. We present a\nunified theoretical framework bridging Supervised Fine-Tuning (SFT) and\npreference learning in Large Language Model (LLM) post-training. Through\nrigorous mathematical derivation, we demonstrate that both SFT and preference\nlearning methods like Direct Preference Optimization (DPO) operate within the\nsame optimal policy-reward subspace, with SFT representing a special case of\nimplicit reward learning. Our analysis reveals a critical limitation in\nconventional SFT: the KL divergence term in distribution matching becomes\nconstant with respect to the policy during optimization, failing to constrain\nmodel updates. To address this, we propose a simple yet effective learning rate\nreduction approach that yields significant performance improvements (up to\n\\textbf{25\\%} relative gain and \\textbf{6\\%} absolute win rate increase in\ninstruction following tasks. Additionally, we derive alternative SFT objectives\nfrom various f-divergence functions that preserve the KL term during\noptimization, further enhancing post-DPO model performance. Finally, we extend\nthe theoretical relationship between LLM logits and Q-functions from preference\nlearning to the SFT context, providing mathematical derivations and\nexperimental validation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00018v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00018v1", "date": "2025-06-15", "updated": "2025-06-15"}
{"id": "2507.00909", "title": "Turning AI Data Centers into Grid-Interactive Assets: Results from a Field Demonstration in Phoenix, Arizona", "authors": ["Philip Colangelo", "Ayse K. Coskun", "Jack Megrue", "Ciaran Roberts", "Shayan Sengupta", "Varun Sivaram", "Ethan Tiao", "Aroon Vijaykar", "Chris Williams", "Daniel C. Wilson", "Zack MacFarland", "Daniel Dreiling", "Nathan Morey", "Anuja Ratnayake", "Baskar Vairamohan"], "summary": "Artificial intelligence (AI) is fueling exponential electricity demand\ngrowth, threatening grid reliability, raising prices for communities paying for\nnew energy infrastructure, and stunting AI innovation as data centers wait for\ninterconnection to constrained grids. This paper presents the first field\ndemonstration, in collaboration with major corporate partners, of a\nsoftware-only approach--Emerald Conductor--that transforms AI data centers into\nflexible grid resources that can efficiently and immediately harness existing\npower systems without massive infrastructure buildout. Conducted at a 256-GPU\ncluster running representative AI workloads within a commercial, hyperscale\ncloud data center in Phoenix, Arizona, the trial achieved a 25% reduction in\ncluster power usage for three hours during peak grid events while maintaining\nAI quality of service (QoS) guarantees. By orchestrating AI workloads based on\nreal-time grid signals without hardware modifications or energy storage, this\nplatform reimagines data centers as grid-interactive assets that enhance grid\nreliability, advance affordability, and accelerate AI's development.", "comment": "10 pages, 6 figures, 1 table", "pdf_url": "http://arxiv.org/pdf/2507.00909v1", "categories": ["cs.DC", "cs.AI", "cs.PF", "cs.SY", "eess.SY"], "cate": "cs.DC", "url": "http://arxiv.org/abs/2507.00909v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00070", "title": "An efficient plant disease detection using transfer learning approach", "authors": ["Bosubabu Sambana", "Hillary Sunday Nnadi", "Mohd Anas Wajid", "Nwosu Ogochukwu Fidelia", "Claudia Camacho-Zuñiga", "Henry Dozie Ajuzie", "Edeh Michael Onyema"], "summary": "Plant diseases pose significant challenges to farmers and the agricultural\nsector at large. However, early detection of plant diseases is crucial to\nmitigating their effects and preventing widespread damage, as outbreaks can\nseverely impact the productivity and quality of crops. With advancements in\ntechnology, there are increasing opportunities for automating the monitoring\nand detection of disease outbreaks in plants. This study proposed a system\ndesigned to identify and monitor plant diseases using a transfer learning\napproach. Specifically, the study utilizes YOLOv7 and YOLOv8, two\nstate-ofthe-art models in the field of object detection. By fine-tuning these\nmodels on a dataset of plant leaf images, the system is able to accurately\ndetect the presence of Bacteria, Fungi and Viral diseases such as Powdery\nMildew, Angular Leaf Spot, Early blight and Tomato mosaic virus. The model's\nperformance was evaluated using several metrics, including mean Average\nPrecision (mAP), F1-score, Precision, and Recall, yielding values of 91.05,\n89.40, 91.22, and 87.66, respectively. The result demonstrates the superior\neffectiveness and efficiency of YOLOv8 compared to other object detection\nmethods, highlighting its potential for use in modern agricultural practices.\nThe approach provides a scalable, automated solution for early any plant\ndisease detection, contributing to enhanced crop yield, reduced reliance on\nmanual monitoring, and supporting sustainable agricultural practices.", "comment": "15 pages , 4 figures. Scientific Reports 2025", "pdf_url": "http://arxiv.org/pdf/2507.00070v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00070v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2507.00675", "title": "Can Machines Philosophize?", "authors": ["Michele Pizzochero", "Giorgia Dellaferrera"], "summary": "Inspired by the Turing test, we present a novel methodological framework to\nassess the extent to which a population of machines mirrors the philosophical\nviews of a population of humans. The framework consists of three steps: (i)\ninstructing machines to impersonate each human in the population, reflecting\ntheir backgrounds and beliefs, (ii) administering a questionnaire covering\nvarious philosophical positions to both humans and machines, and (iii)\nstatistically analyzing the resulting responses. We apply this methodology to\nthe debate on scientific realism, a long-standing philosophical inquiry\nexploring the relationship between science and reality. By considering the\noutcome of a survey of over 500 human participants, including both physicists\nand philosophers of science, we generate their machine personas using an\nartificial intelligence engine based on a large-language generative model. We\nreveal that the philosophical views of a population of machines are, on\naverage, similar to those endorsed by a population of humans, irrespective of\nwhether they are physicists or philosophers of science. As compared to humans,\nhowever, machines exhibit a weaker inclination toward scientific realism and a\nstronger coherence in their philosophical positions. Given the observed\nsimilarities between the populations of humans and machines, this\nmethodological framework may offer unprecedented opportunities for advancing\nresearch in experimental philosophy by replacing human participants with their\nmachine-impersonated counterparts, possibly mitigating the efficiency and\nreproducibility issues that affect survey-based empirical studies.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00675v1", "categories": ["physics.soc-ph", "cs.CY", "physics.hist-ph"], "cate": "physics.soc-ph", "url": "http://arxiv.org/abs/2507.00675v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00660", "title": "MTCNet: Motion and Topology Consistency Guided Learning for Mitral Valve Segmentationin 4D Ultrasound", "authors": ["Rusi Chen", "Yuanting Yang", "Jiezhi Yao", "Hongning Song", "Ji Zhang", "Yongsong Zhou", "Yuhao Huang", "Ronghao Yang", "Dan Jia", "Yuhan Zhang", "Xing Tao", "Haoran Dou", "Qing Zhou", "Xin Yang", "Dong Ni"], "summary": "Mitral regurgitation is one of the most prevalent cardiac disorders.\nFour-dimensional (4D) ultrasound has emerged as the primary imaging modality\nfor assessing dynamic valvular morphology. However, 4D mitral valve (MV)\nanalysis remains challenging due to limited phase annotations, severe motion\nartifacts, and poor imaging quality. Yet, the absence of inter-phase dependency\nin existing methods hinders 4D MV analysis. To bridge this gap, we propose a\nMotion-Topology guided consistency network (MTCNet) for accurate 4D MV\nultrasound segmentation in semi-supervised learning (SSL). MTCNet requires only\nsparse end-diastolic and end-systolic annotations. First, we design a\ncross-phase motion-guided consistency learning strategy, utilizing a\nbi-directional attention memory bank to propagate spatio-temporal features.\nThis enables MTCNet to achieve excellent performance both per- and inter-phase.\nSecond, we devise a novel topology-guided correlation regularization that\nexplores physical prior knowledge to maintain anatomically plausible.\nTherefore, MTCNet can effectively leverage structural correspondence between\nlabeled and unlabeled phases. Extensive evaluations on the first largest 4D MV\ndataset, with 1408 phases from 160 patients, show that MTCNet performs superior\ncross-phase consistency compared to other advanced methods (Dice: 87.30%, HD:\n1.75mm). Both the code and the dataset are available at\nhttps://github.com/crs524/MTCNet.", "comment": "Accepted by MICCAI 2025", "pdf_url": "http://arxiv.org/pdf/2507.00660v1", "categories": ["eess.IV", "cs.AI", "cs.CV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2507.00660v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00513", "title": "Customer Service Representative's Perception of the AI Assistant in an Organization's Call Center", "authors": ["Kai Qin", "Kexin Du", "Yimeng Chen", "Yueyan Liu", "Jie Cai", "Zhiqiang Nie", "Nan Gao", "Guohui Wei", "Shengzhu Wang", "Chun Yu"], "summary": "The integration of various AI tools creates a complex socio-technical\nenvironment where employee-customer interactions form the core of work\npractices. This study investigates how customer service representatives (CSRs)\nat the power grid service customer service call center perceive AI assistance\nin their interactions with customers. Through a field visit and semi-structured\ninterviews with 13 CSRs, we found that AI can alleviate some traditional\nburdens during the call (e.g., typing and memorizing) but also introduces new\nburdens (e.g., earning, compliance, psychological burdens). This research\ncontributes to a more nuanced understanding of AI integration in organizational\nsettings and highlights the efforts and burdens undertaken by CSRs to adapt to\nthe updated system.", "comment": "ACM CSCW Poster 2025", "pdf_url": "http://arxiv.org/pdf/2507.00513v1", "categories": ["cs.HC", "cs.AI", "cs.CY"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2507.00513v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00823", "title": "Quantum Speedups for Polynomial-Time Dynamic Programming Algorithms", "authors": ["Susanna Caroppo", "Giordano Da Lozzo", "Giuseppe Di Battista", "Michael T. Goodrich", "Martin Nöllenburg"], "summary": "We introduce a quantum dynamic programming framework that allows us to\ndirectly extend to the quantum realm a large body of classical dynamic\nprogramming algorithms. The corresponding quantum dynamic programming\nalgorithms retain the same space complexity as their classical counterpart,\nwhile achieving a computational speedup. For a combinatorial (search or\noptimization) problem $\\mathcal P$ and an instance $I$ of $\\mathcal P$, such a\nspeedup can be expressed in terms of the average degree $\\delta$ of the\ndependency digraph $G_{\\mathcal{P}}(I)$ of $I$, determined by a recursive\nformulation of $\\mathcal P$. The nodes of this graph are the subproblems of\n$\\mathcal P$ induced by $I$ and its arcs are directed from each subproblem to\nthose on whose solution it relies. In particular, our framework allows us to\nsolve the considered problems in $\\tilde{O}(|V(G_{\\mathcal{P}}(I))|\n\\sqrt{\\delta})$ time. As an example, we obtain a quantum version of the\nBellman-Ford algorithm for computing shortest paths from a single source vertex\nto all the other vertices in a weighted $n$-vertex digraph with $m$ edges that\nruns in $\\tilde{O}(n\\sqrt{nm})$ time, which improves the best known classical\nupper bound when $m \\in \\Omega(n^{1.4})$.", "comment": "This is the extended version of a paper to appear at the 19th\n  Algorithms and Data Structures Symposium (WADS 2025)", "pdf_url": "http://arxiv.org/pdf/2507.00823v1", "categories": ["quant-ph", "cs.DS"], "cate": "quant-ph", "url": "http://arxiv.org/abs/2507.00823v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00826", "title": "Getting Dynamic Line Ratings into Markets", "authors": ["Zhiyi Zhou", "Christoph Graf", "Yury Dvorkin"], "summary": "Static transmission line ratings may lead to underutilization of line\ncapacity due to overly conservative (worst-case) assumptions. Grid-enhancing\ntechnologies (GETs) such as dynamic line ratings (DLRs), which adjust line\ncapacity based on real-time conditions, are a techno-economically viable\nalternative to increase the utilization of existing power lines. Nonetheless,\ntheir adoption has been slow, partly due to the absence of operational tools\nthat effectively account for simultaneous impacts on dispatch and pricing. In\nthis paper, we represent transmission capacity with DLRs as a stock-like\nresource with time-variant interdependency, which is modeled via an\napproximation of line temperature evolution process, decoupling the impacts of\nambient weather conditions and power flow on transmission line temperature and\nthus capacity. We integrate DLRs into a multi-period DC optimal power flow\nproblem, with chance constrains addressing correlated uncertainty in DLRs and\nrenewable generation. This yields non-convex problems that we transform into a\ntractable convex form by linearization. We derive locational marginal energy\nand ancillary services prices consistent with a competitive equilibrium.\nNumerical experiments on the 11-zone and 1814-node NYISO systems demonstrate\nits performance, including impacts on dispatch, pricing, and marginal carbon\nemissions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00826v1", "categories": ["eess.SY", "cs.SY"], "cate": "eess.SY", "url": "http://arxiv.org/abs/2507.00826v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00928", "title": "Enhancing Open RAN Digital Twin Through Power Consumption Measurement", "authors": ["Ahmed Al-Tahmeesschi", "Yi Chu", "Josh Shackleton", "Swarna Chetty", "Mostafa Rahmani", "David Grace", "Hamed Ahmadi"], "summary": "The increasing demand for high-speed, ultra-reliable and low-latency\ncommunications in 5G and beyond networks has led to a significant increase in\npower consumption, particularly within the Radio Access Network (RAN). This\ngrowing energy demand raises operational and sustainability challenges for\nmobile network operators, requiring novel solutions to enhance energy\nefficiency while maintaining Quality of Service (QoS). 5G networks are evolving\ntowards disaggregated, programmable, and intelligent architectures, with Open\nRadio Access Network (O-RAN) spearheaded by the O-RAN Alliance, enabling\ngreater flexibility, interoperability, and cost-effectiveness. However, this\ndisaggregated approach introduces new complexities, especially in terms of\npower consumption across different network components, including Open Radio\nUnits (RUs), Open Distributed Units (DUs) and Open Central Units (CUs).\nUnderstanding the power efficiency of different O-RAN functional splits is\ncrucial for optimising energy consumption and network sustainability. In this\npaper, we present a comprehensive measurement study of power consumption in\nRUs, DUs and CUs under varying network loads, specifically analysing the impact\nof Physical resource block (PRB) utilisation in Split 8 and Split 7.2b. The\nmeasurements were conducted on both software-defined radio (SDR)-based RUs and\ncommercial indoor and outdoor RU, as well as their corresponding DU and CU. By\nevaluating real-world hardware deployments under different operational\nconditions, this study provides empirical insights into the power efficiency of\nvarious O-RAN configurations. The results highlight that power consumption does\nnot scale significantly with network load, suggesting that a large portion of\nenergy consumption remains constant regardless of traffic demand.", "comment": "Accepted in PIMRC 2025", "pdf_url": "http://arxiv.org/pdf/2507.00928v1", "categories": ["eess.SP"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2507.00928v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00229", "title": "A High-Fidelity Speech Super Resolution Network using a Complex Global Attention Module with Spectro-Temporal Loss", "authors": ["Tarikul Islam Tamiti", "Biraj Joshi", "Rida Hasan", "Rashedul Hasan", "Taieba Athay", "Nursad Mamun", "Anomadarshi Barua"], "summary": "Speech super-resolution (SSR) enhances low-resolution speech by increasing\nthe sampling rate. While most SSR methods focus on magnitude reconstruction,\nrecent research highlights the importance of phase reconstruction for improved\nperceptual quality. Therefore, we introduce CTFT-Net, a Complex Time-Frequency\nTransformation Network that reconstructs both magnitude and phase in complex\ndomains for improved SSR tasks. It incorporates a complex global attention\nblock to model inter-phoneme and inter-frequency dependencies and a complex\nconformer to capture long-range and local features, improving frequency\nreconstruction and noise robustness. CTFT-Net employs time-domain and\nmulti-resolution frequency-domain loss functions for better generalization.\nExperiments show CTFT-Net outperforms state-of-the-art models (NU-Wave,\nWSRGlow, NVSR, AERO) on the VCTK dataset, particularly for extreme upsampling\n(2 kHz to 48 kHz), reconstructing high frequencies effectively without noisy\nartifacts.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00229v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "cate": "cs.SD", "url": "http://arxiv.org/abs/2507.00229v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00244", "title": "The Algebraic Structure of Morphosyntax", "authors": ["Isabella Senturia", "Matilde Marcolli"], "summary": "Within the context of the mathematical formulation of Merge and the Strong\nMinimalist Thesis, we present a mathematical model of the morphology-syntax\ninterface. In this setting, morphology has compositional properties responsible\nfor word formation, organized into a magma of morphological trees. However,\nunlike syntax, we do not have movement within morphology. A coproduct\ndecomposition exists, but it requires extending the set of morphological trees\nbeyond those which are generated solely by the magma, to a larger set of\npossible morphological inputs to syntactic trees. These participate in the\nformation of morphosyntactic trees as an algebra over an operad, and a\ncorrespondence between algebras over an operad. The process of structure\nformation for morphosyntactic trees can then be described in terms of this\noperadic correspondence that pairs syntactic and morphological data and the\nmorphology coproduct. We reinterpret in this setting certain operations of\nDistributed Morphology as transformation that allow for flexibility in moving\nthe boundary between syntax and morphology within the morphosyntactic objects.", "comment": "45 pages, LaTeX, 2 png figures", "pdf_url": "http://arxiv.org/pdf/2507.00244v1", "categories": ["cs.CL", "math.QA", "91F20, 18M60, 68Q70"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2507.00244v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00263", "title": "Room Scene Discovery and Grouping in Unstructured Vacation Rental Image Collections", "authors": ["Vignesh Ram Nithin Kappagantula", "Shayan Hassantabar"], "summary": "The rapid growth of vacation rental (VR) platforms has led to an increasing\nvolume of property images, often uploaded without structured categorization.\nThis lack of organization poses significant challenges for travelers attempting\nto understand the spatial layout of a property, particularly when multiple\nrooms of the same type are present. To address this issue, we introduce an\neffective approach for solving the room scene discovery and grouping problem,\nas well as identifying bed types within each bedroom group. This grouping is\nvaluable for travelers to comprehend the spatial organization, layout, and the\nsleeping configuration of the property. We propose a computationally efficient\nmachine learning pipeline characterized by low latency and the ability to\nperform effectively with sample-efficient learning, making it well-suited for\nreal-time and data-scarce environments. The pipeline integrates a supervised\nroom-type detection model, a supervised overlap detection model to identify the\noverlap similarity between two images, and a clustering algorithm to group the\nimages of the same space together using the similarity scores. Additionally,\nthe pipeline maps each bedroom group to the corresponding bed types specified\nin the property's metadata, based on the visual content present in the group's\nimages using a Multi-modal Large Language Model (MLLM) model. We evaluate the\naforementioned models individually and also assess the pipeline in its\nentirety, observing strong performance that significantly outperforms\nestablished approaches such as contrastive learning and clustering with\npretrained embeddings.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00263v1", "categories": ["cs.CV", "cs.LG", "cs.NE"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00263v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00543", "title": "Reliable Annotations with Less Effort: Evaluating LLM-Human Collaboration in Search Clarifications", "authors": ["Leila Tavakoli", "Hamed Zamani"], "summary": "Despite growing interest in using large language models (LLMs) to automate\nannotation, their effectiveness in complex, nuanced, and multi-dimensional\nlabelling tasks remains relatively underexplored. This study focuses on\nannotation for the search clarification task, leveraging a high-quality,\nmulti-dimensional dataset that includes five distinct fine-grained annotation\nsubtasks. Although LLMs have shown impressive capabilities in general settings,\nour study reveals that even state-of-the-art models struggle to replicate\nhuman-level performance in subjective or fine-grained evaluation tasks. Through\na systematic assessment, we demonstrate that LLM predictions are often\ninconsistent, poorly calibrated, and highly sensitive to prompt variations. To\naddress these limitations, we propose a simple yet effective human-in-the-loop\n(HITL) workflow that uses confidence thresholds and inter-model disagreement to\nselectively involve human review. Our findings show that this lightweight\nintervention significantly improves annotation reliability while reducing human\neffort by up to 45%, offering a relatively scalable and cost-effective yet\naccurate path forward for deploying LLMs in real-world evaluation settings.", "comment": "9 pages,5 figures", "pdf_url": "http://arxiv.org/pdf/2507.00543v1", "categories": ["cs.IR", "cs.HC"], "cate": "cs.IR", "url": "http://arxiv.org/abs/2507.00543v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00376", "title": "Adaptive finite element convergence analysis of AT1 phase-field model for quasi-static fracture in strain-limiting solids", "authors": ["Ram Manohar", "S. M. Mallikarjunaiah"], "summary": "This research rigorously investigates the convergence of adaptive finite\nelement methods for regularized variational models of quasi-static brittle\nfracture in elastic solids. We specifically examine a novel Ambrosio-Tortorelli\n(AT1) phase-field model within the framework of elasticity theories,\nparticularly for material models characterized by an algebraically nonlinear\nstress-strain relationship. Two distinct and novel adaptive mesh refinement\nalgorithms, underpinned by robust local error indicators, were introduced to\nefficiently solve the underlying nonlinear energy minimization problem. A\ndetailed convergence analysis was conducted on the sequences of minimizers\nproduced by these strategies. Our findings rigorously demonstrate that the\nminimizer sequences from the first adaptive algorithm achieve convergence to a\npredefined tolerance. Crucially, the second algorithm is proven to generate\ninherently convergent sequences, thereby eliminating the need for an explicit\nstopping criterion. The practical effectiveness of this proposed adaptive\nframework is thoroughly validated through extensive numerical simulations. A\ncase study involving an edge crack in an elastic body, governed by an\nalgebraically nonlinear strain-limiting relationship and subjected to\nanti-plane shear-type loading, is presented. Critical comparisons of the energy\ncomponents-bulk, surface, and total-showcase the superior performance of both\nadaptive algorithms.", "comment": "arXiv admin note: text overlap with arXiv:2505.19801", "pdf_url": "http://arxiv.org/pdf/2507.00376v1", "categories": ["math.NA", "cs.NA"], "cate": "math.NA", "url": "http://arxiv.org/abs/2507.00376v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00808", "title": "Multi-interaction TTS toward professional recording reproduction", "authors": ["Hiroki Kanagawa", "Kenichi Fujita", "Aya Watanabe", "Yusuke Ijima"], "summary": "Voice directors often iteratively refine voice actors' performances by\nproviding feedback to achieve the desired outcome. While this iterative\nfeedback-based refinement process is important in actual recordings, it has\nbeen overlooked in text-to-speech synthesis (TTS). As a result, fine-grained\nstyle refinement after the initial synthesis is not possible, even though the\nsynthesized speech often deviates from the user's intended style. To address\nthis issue, we propose a TTS method with multi-step interaction that allows\nusers to intuitively and rapidly refine synthetized speech. Our approach models\nthe interaction between the TTS model and its user to emulate the relationship\nbetween voice actors and voice directors. Experiments show that the proposed\nmodel with its corresponding dataset enable iterative style refinements in\naccordance with users' directions, thus demonstrating its multi-interaction\ncapability. Sample audios are available: https://ntt-hilab-gensp.\ngithub.io/ssw13multiinteraction_tts/", "comment": "7 pages,6 figures, Accepted to Speech Synthesis Workshop 2025 (SSW13)", "pdf_url": "http://arxiv.org/pdf/2507.00808v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "cate": "cs.SD", "url": "http://arxiv.org/abs/2507.00808v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00907", "title": "The Age of Sensorial Zero Trust: Why We Can No Longer Trust Our Senses", "authors": ["Fabio Correa Xavier"], "summary": "In a world where deepfakes and cloned voices are emerging as sophisticated\nattack vectors, organizations require a new security mindset: Sensorial Zero\nTrust [9]. This article presents a scientific analysis of the need to\nsystematically doubt information perceived through the senses, establishing\nrigorous verification protocols to mitigate the risks of fraud based on\ngenerative artificial intelligence. Key concepts, such as Out-of-Band\nverification, Vision-Language Models (VLMs) as forensic collaborators,\ncryptographic provenance, and human training, are integrated into a framework\nthat extends Zero Trust principles to human sensory information. The approach\nis grounded in empirical findings and academic research, emphasizing that in an\nera of AI-generated realities, even our eyes and ears can no longer be\nimplicitly trusted without verification. Leaders are called to foster a culture\nof methodological skepticism to protect organizational integrity in this new\nthreat landscape.", "comment": "14 pages", "pdf_url": "http://arxiv.org/pdf/2507.00907v1", "categories": ["cs.CR", "cs.AI", "68T07, 68T45, 94A60", "K.6.5; D.4.6; I.2.6"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2507.00907v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00788", "title": "Echoes of AI: Investigating the Downstream Effects of AI Assistants on Software Maintainability", "authors": ["Markus Borg", "Dave Hewett", "Nadim Hagatulah", "Noric Couderc", "Emma Söderberg", "Donald Graham", "Uttam Kini", "Dave Farley"], "summary": "[Context] AI assistants, like GitHub Copilot and Cursor, are transforming\nsoftware engineering. While several studies highlight productivity\nimprovements, their impact on maintainability requires further investigation.\n[Objective] This study investigates whether co-development with AI assistants\naffects software maintainability, specifically how easily other developers can\nevolve the resulting source code. [Method] We conducted a two-phase controlled\nexperiment involving 151 participants, 95% of whom were professional\ndevelopers. In Phase 1, participants added a new feature to a Java web\napplication, with or without AI assistance. In Phase 2, a randomized controlled\ntrial, new participants evolved these solutions without AI assistance.\n[Results] AI-assisted development in Phase 1 led to a modest speedup in\nsubsequent evolution and slightly higher average CodeHealth. Although neither\ndifference was significant overall, the increase in CodeHealth was\nstatistically significant when habitual AI users completed Phase 1. For Phase\n1, we also observed a significant effect that corroborates previous\nproductivity findings: using an AI assistant yielded a 30.7% median decrease in\ntask completion time. Moreover, for habitual AI users, the mean speedup was\n55.9%. [Conclusions] Our study adds to the growing evidence that AI assistants\ncan effectively accelerate development. Moreover, we did not observe warning\nsigns of degraded code-level maintainability. We recommend that future research\nfocus on risks such as code bloat from excessive code generation and the\nbuild-up of cognitive debt as developers invest less mental effort during\nimplementation.", "comment": "Preprint of study preregistered at ICSME 2025 with In-Principal\n  Acceptance.\n  https://conf.researchr.org/track/icsme-2024/icsme-2024-registered-reports-track", "pdf_url": "http://arxiv.org/pdf/2507.00788v1", "categories": ["cs.SE", "cs.AI"], "cate": "cs.SE", "url": "http://arxiv.org/abs/2507.00788v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00686", "title": "A Domain-specific Language and Architecture for Detecting Process Activities from Sensor Streams in IoT", "authors": ["Ronny Seiger", "Daniel Locher", "Marco Kaufmann", "Aaron F. Kurz"], "summary": "Modern Internet of Things (IoT) systems are equipped with a plethora of\nsensors providing real-time data about the current operations of their\ncomponents, which is crucial for the systems' internal control systems and\nprocesses. However, these data are often too fine-grained to derive useful\ninsights into the execution of the larger processes an IoT system might be part\nof. Process mining has developed advanced approaches for the analysis of\nbusiness processes that may also be used in the context of IoT. Bringing\nprocess mining to IoT requires an event abstraction step to lift the low-level\nsensor data to the business process level. In this work, we aim to empower\ndomain experts to perform this step using a newly developed domain-specific\nlanguage (DSL) called Radiant. Radiant supports the specification of patterns\nwithin the sensor data that indicate the execution of higher level process\nactivities. These patterns are translated to complex event processing (CEP)\napplications to be used for detecting activity executions at runtime. We\npropose a corresponding software architecture for online event abstraction from\nIoT sensor streams using the CEP applications. We evaluate these applications\nto monitor activity executions using IoT sensors in smart manufacturing and\nsmart healthcare. The evaluation method and results inform the domain expert\nabout the quality of activity detections and potential for improvement.", "comment": "Submitted to Internet of Things (ISSN 2542-6605)", "pdf_url": "http://arxiv.org/pdf/2507.00686v1", "categories": ["cs.SE", "cs.ET"], "cate": "cs.SE", "url": "http://arxiv.org/abs/2507.00686v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00464", "title": "A Miniature High-Resolution Tension Sensor Based on a Photo-Reflector for Robotic Hands and Grippers", "authors": ["Hyun-Bin Kim", "Kyung-Soo Kim"], "summary": "This paper presents a miniature tension sensor using a photo-reflector,\ndesigned for compact tendon-driven grippers and robotic hands. The proposed\nsensor has a small form factor of 13~mm x 7~mm x 6.5~mm and is capable of\nmeasuring tensile forces up to 200~N. A symmetric elastomer structure\nincorporating fillets and flexure hinges is designed based on Timoshenko beam\ntheory and verified via FEM analysis, enabling improved sensitivity and\nmechanical durability while minimizing torsional deformation. The sensor\nutilizes a compact photo-reflector (VCNT2020) to measure displacement in the\nnear-field region, eliminating the need for light-absorbing materials or\ngeometric modifications required in photo-interrupter-based designs. A 16-bit\nanalog-to-digital converter (ADC) and CAN-FD (Flexible Data-rate) communication\nenable efficient signal acquisition with up to 5~kHz sampling rate. Calibration\nexperiments demonstrate a resolution of 9.9~mN (corresponding to over 14-bit\naccuracy) and a root mean square error (RMSE) of 0.455~N. Force control\nexperiments using a twisted string actuator and PI control yield RMSEs as low\nas 0.073~N. Compared to previous research using photo-interrupter, the proposed\nmethod achieves more than tenfold improvement in resolution while also reducing\nnonlinearity and hysteresis. The design is mechanically simple, lightweight,\neasy to assemble, and suitable for integration into robotic and prosthetic\nsystems requiring high-resolution force feedback.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00464v1", "categories": ["cs.RO"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2507.00464v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00824", "title": "PANDAS: Peer-to-peer, Adaptive Networking for Data Availability Sampling within Ethereum Consensus Timebounds", "authors": ["Matthieu Pigaglio", "Onur Ascigil", "Michał Król", "Sergi Rene", "Felix Lange", "Kaleem Peeroo", "Ramin Sadre", "Vladimir Stankovic", "Etienne Rivière"], "summary": "Layer-2 protocols can assist Ethereum's limited throughput, but globally\nbroadcasting layer-2 data limits their scalability. The Danksharding evolution\nof Ethereum aims to support the selective distribution of layer-2 data, whose\navailability in the network is verified using randomized data availability\nsampling (DAS). Integrating DAS into Ethereum's consensus process is\nchallenging, as pieces of layer-2 data must be disseminated and sampled within\nfour seconds of the beginning of each consensus slot. No existing solution can\nsupport dissemination and sampling under such strict time bounds.\n  We propose PANDAS, a practical approach to integrate DAS with Ethereum under\nDanksharding's requirements without modifying its protocols for consensus and\nnode discovery. PANDAS disseminates layer-2 data and samples its availability\nusing lightweight, direct exchanges. Its design accounts for message loss, node\nfailures, and unresponsive participants while anticipating the need to scale\nout the Ethereum network. Our evaluation of PANDAS's prototype in a 1,000-node\ncluster and simulations for up to 20,000 peers shows that it allows layer-2\ndata dissemination and sampling under planetary-scale latencies within the\n4-second deadline.", "comment": "14 pages, 10 figures, 1 algorithm, 1 table, and 18 plots", "pdf_url": "http://arxiv.org/pdf/2507.00824v1", "categories": ["cs.DC", "cs.NI", "cs.PF"], "cate": "cs.DC", "url": "http://arxiv.org/abs/2507.00824v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00145", "title": "AI-Hybrid TRNG: Kernel-Based Deep Learning for Near-Uniform Entropy Harvesting from Physical Noise", "authors": ["Hasan Yiğit"], "summary": "AI-Hybrid TRNG is a deep-learning framework that extracts near-uniform\nentropy directly from physical noise, eliminating the need for bulky quantum\ndevices or expensive laboratory-grade RF receivers. Instead, it relies on a\nlow-cost, thumb-sized RF front end, plus CPU-timing jitter, for training, and\nthen emits 32-bit high-entropy streams without any quantization step.\n  Unlike deterministic or trained artificial intelligence random number\ngenerators (RNGs), our dynamic inner-outer network couples adaptive natural\nsources and reseeding, yielding truly unpredictable and autonomous sequences.\nGenerated numbers pass the NIST SP 800-22 battery better than a CPU-based\nmethod. It also passes nineteen bespoke statistical tests for both bit- and\ninteger-level analysis. All results satisfy cryptographic standards, while\nforward and backward prediction experiments reveal no exploitable biases. The\nmodel's footprint is below 0.5 MB, making it deployable on MCUs and FPGA soft\ncores, as well as suitable for other resource-constrained platforms.\n  By detaching randomness quality from dedicated hardware, AI-Hybrid TRNG\nbroadens the reach of high-integrity random number generators across secure\nsystems, cryptographic protocols, embedded and edge devices, stochastic\nsimulations, and server applications that need randomness.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00145v1", "categories": ["cs.CR", "cs.AI", "cs.ET", "cs.IT", "eess.SP", "math.IT"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2507.00145v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00218", "title": "Learning for routing: A guided review of recent developments and future directions", "authors": ["Fangting Zhou", "Attila Lischka", "Balazs Kulcsar", "Jiaming Wu", "Morteza Haghir Chehreghani", "Gilbert Laporte"], "summary": "This paper reviews the current progress in applying machine learning (ML)\ntools to solve NP-hard combinatorial optimization problems, with a focus on\nrouting problems such as the traveling salesman problem (TSP) and the vehicle\nrouting problem (VRP). Due to the inherent complexity of these problems, exact\nalgorithms often require excessive computational time to find optimal\nsolutions, while heuristics can only provide approximate solutions without\nguaranteeing optimality. With the recent success of machine learning models,\nthere is a growing trend in proposing and implementing diverse ML techniques to\nenhance the resolution of these challenging routing problems. We propose a\ntaxonomy categorizing ML-based routing methods into construction-based and\nimprovement-based approaches, highlighting their applicability to various\nproblem characteristics. This review aims to integrate traditional OR methods\nwith state-of-the-art ML techniques, providing a structured framework to guide\nfuture research and address emerging VRP variants.", "comment": "Accepted for publication in Transportation Research Part E: Logistics\n  and Transportation Review", "pdf_url": "http://arxiv.org/pdf/2507.00218v1", "categories": ["cs.AI", "math.OC"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2507.00218v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00019", "title": "Quantum Inspired Encoding Strategies for Machine Learning Models: Proposing and Evaluating Instance Level, Global Discrete, and Class Conditional Representations", "authors": ["Minati Rath", "Hema Date"], "summary": "In this study, we propose, evaluate and compare three quantum inspired data\nencoding strategies, Instance Level Strategy (ILS), Global Discrete Strategy\n(GDS) and Class Conditional Value Strategy (CCVS), for transforming classical\ndata into quantum data for use in pure classical machine learning models. The\nprimary objective is to reduce high encoding time while ensuring correct\nencoding values and analyzing their impact on classification performance. The\nInstance Level Strategy treats each row of dataset independently; mimics local\nquantum states. Global Discrete Value Based encoding strategy maps all unique\nfeature values across the full dataset to quantum states uniformly. In\ncontrast, the Class conditional Value based encoding strategy encodes unique\nvalues separately for each class, preserving class dependent information.\n  We apply these encoding strategies to a classification task and assess their\nimpact on en-coding efficiency, correctness, model accuracy, and computational\ncost. By analyzing the trade offs between encoding time, precision, and\npredictive performance, this study provides insights into optimizing quantum\ninspired data transformations for classical machine learning workflows.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00019v1", "categories": ["cs.LG", "cs.AI", "quant-ph"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00019v1", "date": "2025-06-15", "updated": "2025-06-15"}
{"id": "2507.00949", "title": "How Fast Can Graph Computations Go on Fine-grained Parallel Architectures", "authors": ["Yuqing Wang", "Charles Colley", "Brian Wheatman", "Jiya Su", "David F. Gleich", "Andrew A. Chien"], "summary": "Large-scale graph problems are of critical and growing importance and\nhistorically parallel architectures have provided little support. In the spirit\nof co-design, we explore the question, How fast can graph computing go on a\nfine-grained architecture? We explore the possibilities of an architecture\noptimized for fine-grained parallelism, natural programming, and the\nirregularity and skew found in real-world graphs. Using two graph benchmarks,\nPageRank (PR) and Breadth-First Search (BFS), we evaluate a Fine-Grained Graph\narchitecture, UpDown, to explore what performance codesign can achieve. To\ndemonstrate programmability, we wrote five variants of these algorithms.\nSimulations of up to 256 nodes (524,288 lanes) and projections to 16,384 nodes\n(33M lanes) show the UpDown system can achieve 637K GTEPS PR and 989K GTEPS BFS\non RMAT, exceeding the best prior results by 5x and 100x respectively.", "comment": "13 pages, 11 figures, 6 tables", "pdf_url": "http://arxiv.org/pdf/2507.00949v1", "categories": ["cs.DC", "cs.AR"], "cate": "cs.DC", "url": "http://arxiv.org/abs/2507.00949v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00153", "title": "Diffusion-Based Image Augmentation for Semantic Segmentation in Outdoor Robotics", "authors": ["Peter Mortimer", "Mirko Maehlisch"], "summary": "The performance of leaning-based perception algorithms suffer when deployed\nin out-of-distribution and underrepresented environments. Outdoor robots are\nparticularly susceptible to rapid changes in visual scene appearance due to\ndynamic lighting, seasonality and weather effects that lead to scenes\nunderrepresented in the training data of the learning-based perception system.\nIn this conceptual paper, we focus on preparing our autonomous vehicle for\ndeployment in snow-filled environments. We propose a novel method for\ndiffusion-based image augmentation to more closely represent the deployment\nenvironment in our training data. Diffusion-based image augmentations rely on\nthe public availability of vision foundation models learned on internet-scale\ndatasets. The diffusion-based image augmentations allow us to take control over\nthe semantic distribution of the ground surfaces in the training data and to\nfine-tune our model for its deployment environment. We employ open vocabulary\nsemantic segmentation models to filter out augmentation candidates that contain\nhallucinations. We believe that diffusion-based image augmentations can be\nextended to many other environments apart from snow surfaces, like sandy\nenvironments and volcanic terrains.", "comment": "Presented at the 2025 IEEE ICRA Workshop on Field Robotics", "pdf_url": "http://arxiv.org/pdf/2507.00153v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00153v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00814", "title": "Many LLMs Are More Utilitarian Than One", "authors": ["Anita Keshmirian", "Razan Baltaji", "Babak Hemmatian", "Hadi Asghari", "Lav R. Varshney"], "summary": "Moral judgment is integral to large language model (LLM) alignment and social\nreasoning. As multi-agent systems gain prominence, it becomes crucial to\nunderstand how LLMs function collectively during collaboration, compared to\nindividual agents. In human moral judgment, group deliberation leads to a\nutilitarian boost: a tendency to endorse norm violations that maximize benefits\nfor the greatest number of people despite harms. We study whether a similar\ndynamic emerges in multi-agent LLM systems. We tested six models on\nwell-established sets of moral dilemmas across two conditions: (1) Solo, where\nmodels reasoned independently, and (2) Group, where they engaged in multi-turn\ndiscussions in pairs or triads. In personal moral dilemmas, where agents must\ndecide to directly harm one individual to maximize the utility for others, all\nmodels found moral violations to be more acceptable when part of a group than\nindividually, similar to human experiments. Some models endorsed actions that\nmaximized overall well-being, even if they benefited strangers over familiar\nindividuals. Others became more willing to violate moral norms in groups.\nHowever, while human groups show a similar action bias, the mechanism for their\nutilitarian boost differs from LLMs. Whereas the human shift comes from\nheightened sensitivity to decision outcomes, LLM groups show either reduced\nnorm sensitivity or enhanced impartiality. This suggests that while the surface\nbehavior of LLM collectives mimics human group reasoning, the underlying\ndrivers differ. We discuss the implications for AI alignment, multi-agent\ndesign, and artificial moral reasoning.", "comment": "9 pages, 8 Figures, 7 tables", "pdf_url": "http://arxiv.org/pdf/2507.00814v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "I.2.7; I.2.11"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2507.00814v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00670", "title": "Mind the Detail: Uncovering Clinically Relevant Image Details in Accelerated MRI with Semantically Diverse Reconstructions", "authors": ["Jan Nikolas Morshuis", "Christian Schlarmann", "Thomas Küstner", "Christian F. Baumgartner", "Matthias Hein"], "summary": "In recent years, accelerated MRI reconstruction based on deep learning has\nled to significant improvements in image quality with impressive results for\nhigh acceleration factors. However, from a clinical perspective image quality\nis only secondary; much more important is that all clinically relevant\ninformation is preserved in the reconstruction from heavily undersampled data.\nIn this paper, we show that existing techniques, even when considering\nresampling for diffusion-based reconstruction, can fail to reconstruct small\nand rare pathologies, thus leading to potentially wrong diagnosis decisions\n(false negatives). To uncover the potentially missing clinical information we\npropose ``Semantically Diverse Reconstructions'' (\\SDR), a method which, given\nan original reconstruction, generates novel reconstructions with enhanced\nsemantic variability while all of them are fully consistent with the measured\ndata. To evaluate \\SDR automatically we train an object detector on the\nfastMRI+ dataset. We show that \\SDR significantly reduces the chance of\nfalse-negative diagnoses (higher recall) and improves mean average precision\ncompared to the original reconstructions. The code is available on\nhttps://github.com/NikolasMorshuis/SDR", "comment": "MICCAI 2025", "pdf_url": "http://arxiv.org/pdf/2507.00670v1", "categories": ["eess.IV", "cs.CV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2507.00670v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00596", "title": "Gaze3P: Gaze-Based Prediction of User-Perceived Privacy", "authors": ["Mayar Elfares", "Pascal Reisert", "Ralf Küsters", "Andreas Bulling"], "summary": "Privacy is a highly subjective concept and perceived variably by different\nindividuals. Previous research on quantifying user-perceived privacy has\nprimarily relied on questionnaires. Furthermore, applying user-perceived\nprivacy to optimise the parameters of privacy-preserving techniques (PPT)\nremains insufficiently explored. To address these limitations, we introduce\nGaze3P -- the first dataset specifically designed to facilitate systematic\ninvestigations into user-perceived privacy. Our dataset comprises gaze data\nfrom 100 participants and 1,000 stimuli, encompassing a range of private and\nsafe attributes. With Gaze3P, we train a machine learning model to implicitly\nand dynamically predict perceived privacy from human eye gaze. Through\ncomprehensive experiments, we show that the resulting models achieve high\naccuracy. Finally, we illustrate how predicted privacy can be used to optimise\nthe parameters of differentially private mechanisms, thereby enhancing their\nalignment with user expectations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00596v1", "categories": ["cs.HC", "cs.CR"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2507.00596v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00877", "title": "Verifiable Natural Language to Linear Temporal Logic Translation: A Benchmark Dataset and Evaluation Suite", "authors": ["William H English", "Chase Walker", "Dominic Simon", "Sumit Kumar Jha", "Rickard Ewetz"], "summary": "Empirical evaluation of state-of-the-art natural-language (NL) to\ntemporal-logic (TL) translation systems reveals near-perfect performance on\nexisting benchmarks. However, current studies measure only the accuracy of the\ntranslation of NL logic into formal TL, ignoring a system's capacity to ground\natomic propositions into new scenarios or environments. This is a critical\nfeature, necessary for the verification of resulting formulas in a concrete\nstate space. Consequently, most NL-to-TL translation frameworks propose their\nown bespoke dataset in which the correct grounding is known a-priori, inflating\nperformance metrics and neglecting the need for extensible, domain-general\nsystems. In this paper, we introduce the Verifiable Linear Temporal Logic\nBenchmark ( VLTL-Bench), a unifying benchmark that measures verification and\nverifiability of automated NL-to-LTL translation. The dataset consists of three\nunique state spaces and thousands of diverse natural language specifications\nand corresponding formal specifications in temporal logic. Moreover, the\nbenchmark contains sample traces to validate the temporal logic expressions.\nWhile the benchmark directly supports end-to-end evaluation, we observe that\nmany frameworks decompose the process into i) lifting, ii) grounding, iii)\ntranslation, and iv) verification. The benchmark provides ground truths after\neach of these steps to enable researches to improve and evaluate different\nsubsteps of the overall problem. To encourage methodologically sound advances\nin verifiable NL-to-LTL translation approaches, we release VLTL-Bench here:\nhttps://www.kaggle.com/datasets/dubascudes/vltl bench.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00877v1", "categories": ["eess.SY", "cs.CL", "cs.SY"], "cate": "eess.SY", "url": "http://arxiv.org/abs/2507.00877v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00055", "title": "Leveraging Unlabeled Audio-Visual Data in Speech Emotion Recognition using Knowledge Distillation", "authors": ["Varsha Pendyala", "Pedro Morgado", "William Sethares"], "summary": "Voice interfaces integral to the human-computer interaction systems can\nbenefit from speech emotion recognition (SER) to customize responses based on\nuser emotions. Since humans convey emotions through multi-modal audio-visual\ncues, developing SER systems using both the modalities is beneficial. However,\ncollecting a vast amount of labeled data for their development is expensive.\nThis paper proposes a knowledge distillation framework called LightweightSER\n(LiSER) that leverages unlabeled audio-visual data for SER, using large teacher\nmodels built on advanced speech and face representation models. LiSER transfers\nknowledge regarding speech emotions and facial expressions from the teacher\nmodels to lightweight student models. Experiments conducted on two benchmark\ndatasets, RAVDESS and CREMA-D, demonstrate that LiSER can reduce the dependence\non extensive labeled datasets for SER tasks.", "comment": "Accepted at INTERSPEECH 2025", "pdf_url": "http://arxiv.org/pdf/2507.00055v1", "categories": ["cs.LG", "cs.HC", "cs.MM", "eess.AS", "eess.IV", "eess.SP"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00055v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2507.00466", "title": "Beat and Downbeat Tracking in Performance MIDI Using an End-to-End Transformer Architecture", "authors": ["Sebastian Murgul", "Michael Heizmann"], "summary": "Beat tracking in musical performance MIDI is a challenging and important task\nfor notation-level music transcription and rhythmical analysis, yet existing\nmethods primarily focus on audio-based approaches. This paper proposes an\nend-to-end transformer-based model for beat and downbeat tracking in\nperformance MIDI, leveraging an encoder-decoder architecture for\nsequence-to-sequence translation of MIDI input to beat annotations. Our\napproach introduces novel data preprocessing techniques, including dynamic\naugmentation and optimized tokenization strategies, to improve accuracy and\ngeneralizability across different datasets. We conduct extensive experiments\nusing the A-MAPS, ASAP, GuitarSet, and Leduc datasets, comparing our model\nagainst state-of-the-art hidden Markov models (HMMs) and deep learning-based\nbeat tracking methods. The results demonstrate that our model outperforms\nexisting symbolic music beat tracking approaches, achieving competitive\nF1-scores across various musical styles and instruments. Our findings highlight\nthe potential of transformer architectures for symbolic beat tracking and\nsuggest future integration with automatic music transcription systems for\nenhanced music analysis and score generation.", "comment": "Accepted to the 22nd Sound and Music Computing Conference (SMC), 2025", "pdf_url": "http://arxiv.org/pdf/2507.00466v1", "categories": ["cs.SD", "cs.CL", "cs.MM", "eess.AS"], "cate": "cs.SD", "url": "http://arxiv.org/abs/2507.00466v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00246", "title": "EfficientXLang: Towards Improving Token Efficiency Through Cross-Lingual Reasoning", "authors": ["Sanchit Ahuja", "Praneetha Vaddamanu", "Barun Patra"], "summary": "Despite recent advances in Language Reasoning Models (LRMs), most research\nfocuses solely on English, even though many models are pretrained on\nmultilingual data. In this work, we investigate: Is English the most\ntoken-efficient language for reasoning? We evaluate three open-source RLMs:\nDeepSeek R1, Qwen 2.5 and Qwen 3, across four math datasets and seven\ntypologically diverse languages. We find that reasoning in non-English\nlanguages not only reduces token usage, but also preserves accuracy. These\ngains persist even after translating the reasoning traces into English,\nsuggesting genuine shifts in reasoning behavior rather than surface-level\nlinguistic effects. The extent of improvement, however, depends on the models\nmultilingual strength. Our findings motivate a broader view of reasoning in\nlanguage models, highlighting the potential of multilingual reasoning and the\nimportance of strong multilingual foundations. The code for our work can be\nfound: https://github.com/microsoft/EfficientXLang.", "comment": "15 pages, 5 figures, 9 tables", "pdf_url": "http://arxiv.org/pdf/2507.00246v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2507.00246v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00715", "title": "EARN: Efficient Inference Acceleration for LLM-based Generative Recommendation by Register Tokens", "authors": ["Chaoqun Yang", "Xinyu Lin", "Wenjie Wang", "Yongqi Li", "Teng Sun", "Xianjing Han", "Tat-Seng Chua"], "summary": "Large Language Model-based generative recommendation (LLMRec) has achieved\nnotable success, but it suffers from high inference latency due to massive\ncomputational overhead and memory pressure of KV Cache. Existing KV Cache\nreduction methods face critical limitations: cache compression offers marginal\nacceleration given recommendation tasks' short decoding steps, while prompt\ncompression risks discarding vital interaction history. Through systematic\nanalysis of attention patterns in LLMRec, we uncover two pivotal insights: 1)\nlayer-wise attention sparsity inversion where early layers retain dense\ninformative patterns while later layers exhibit high redundancy, and 2) dual\nattention sinks phenomenon where attention scores concentrate on both head and\ntail tokens of input sequences. Motivated by these insights, we propose EARN,\nan efficient inference framework that leverages the early layers to compress\ninformation into register tokens placed at the input sequence boundaries, then\nfocuses solely on these tokens in the subsequent layers. Extensive experiments\non three datasets, two LLMRec methods and two LLM architectures demonstrate\nEARN's superiority, achieving up to 3.79x speedup and 80.8% KV Cache reduction\nwith better accuracy than the general finetuning approach. Our work bridges the\nefficiency-effectiveness gap in LLMRec, offering practical deployment\nadvantages for industrial scenarios.", "comment": "Accepted by KDD 2025", "pdf_url": "http://arxiv.org/pdf/2507.00715v1", "categories": ["cs.IR"], "cate": "cs.IR", "url": "http://arxiv.org/abs/2507.00715v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00516", "title": "The Fourier spectral approach to the spatial discretization of quasilinear hyperbolic systems", "authors": ["Vincent Duchêne", "Johanna Ulvedal Marstrander"], "summary": "We discuss the rigorous justification of the spatial discretization by means\nof Fourier spectral methods of quasilinear first-order hyperbolic systems. We\nprovide uniform stability estimates that grant spectral convergence of the\n(spatially) semi-discretized solutions towards the corresponding continuous\nsolution provided that the underlying system satisfies some suitable structural\nassumptions. We consider a setting with sharp low-pass filters and a setting\nwith smooth low-pass filters and argue that - at least theoretically - smooth\nlow-pass filters are operable on a larger class of systems. While our\ntheoretical results are supported with numerical evidence, we also pinpoint\nsome behavior of the numerical method that currently has no theoretical\nexplanation.", "comment": "30 pages, 7 figures. Figures are reproducible using the Julia package\n  WaterWaves1D at https://github.com/WaterWavesModels/WaterWaves1D.jl", "pdf_url": "http://arxiv.org/pdf/2507.00516v1", "categories": ["math.NA", "cs.NA", "math.AP", "65M12, 65M70, 76M22"], "cate": "math.NA", "url": "http://arxiv.org/abs/2507.00516v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00966", "title": "MambAttention: Mamba with Multi-Head Attention for Generalizable Single-Channel Speech Enhancement", "authors": ["Nikolai Lund Kühne", "Jesper Jensen", "Jan Østergaard", "Zheng-Hua Tan"], "summary": "With the advent of new sequence models like Mamba and xLSTM, several studies\nhave shown that these models match or outperform state-of-the-art models in\nsingle-channel speech enhancement, automatic speech recognition, and\nself-supervised audio representation learning. However, prior research has\ndemonstrated that sequence models like LSTM and Mamba tend to overfit to the\ntraining set. To address this issue, previous works have shown that adding\nself-attention to LSTMs substantially improves generalization performance for\nsingle-channel speech enhancement. Nevertheless, neither the concept of hybrid\nMamba and time-frequency attention models nor their generalization performance\nhave been explored for speech enhancement. In this paper, we propose a novel\nhybrid architecture, MambAttention, which combines Mamba and shared time- and\nfrequency-multi-head attention modules for generalizable single-channel speech\nenhancement. To train our model, we introduce VoiceBank+Demand Extended\n(VB-DemandEx), a dataset inspired by VoiceBank+Demand but with more challenging\nnoise types and lower signal-to-noise ratios. Trained on VB-DemandEx, our\nproposed MambAttention model significantly outperforms existing\nstate-of-the-art LSTM-, xLSTM-, Mamba-, and Conformer-based systems of similar\ncomplexity across all reported metrics on two out-of-domain datasets: DNS 2020\nand EARS-WHAM_v2, while matching their performance on the in-domain dataset\nVB-DemandEx. Ablation studies highlight the role of weight sharing between the\ntime- and frequency-multi-head attention modules for generalization\nperformance. Finally, we explore integrating the shared time- and\nfrequency-multi-head attention modules with LSTM and xLSTM, which yields a\nnotable performance improvement on the out-of-domain datasets. However, our\nMambAttention model remains superior on both out-of-domain datasets across all\nreported evaluation metrics.", "comment": "Submitted to IEEE/ACM Transactions on Audio, Speech, and Language\n  Processing for possible publication", "pdf_url": "http://arxiv.org/pdf/2507.00966v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "cate": "cs.SD", "url": "http://arxiv.org/abs/2507.00966v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00003", "title": "Deciding When Not to Decide: Indeterminacy-Aware Intrusion Detection with NeutroSENSE", "authors": ["Eyhab Al-Masri"], "summary": "This paper presents NeutroSENSE, a neutrosophic-enhanced ensemble framework\nfor interpretable intrusion detection in IoT environments. By integrating\nRandom Forest, XGBoost, and Logistic Regression with neutrosophic logic, the\nsystem decomposes prediction confidence into truth (T), falsity (F), and\nindeterminacy (I) components, enabling uncertainty quantification and\nabstention. Predictions with high indeterminacy are flagged for review using\nboth global and adaptive, class-specific thresholds. Evaluated on the IoT-CAD\ndataset, NeutroSENSE achieved 97% accuracy, while demonstrating that\nmisclassified samples exhibit significantly higher indeterminacy (I = 0.62)\nthan correct ones (I = 0.24). The use of indeterminacy as a proxy for\nuncertainty enables informed abstention and targeted review-particularly\nvaluable in edge deployments. Figures and tables validate the correlation\nbetween I-scores and error likelihood, supporting more trustworthy,\nhuman-in-the-loop AI decisions. This work shows that neutrosophic logic\nenhances both accuracy and explainability, providing a practical foundation for\ntrust-aware AI in edge and fog-based IoT security systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00003v1", "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.NI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00003v1", "date": "2025-06-05", "updated": "2025-06-05"}
{"id": "2507.00803", "title": "Out of the Day Job: Perspectives of Industry Practitioners in Co-Design and Delivery of Software Engineering Courses", "authors": ["Gillian Daniel", "Chris Hall", "Per Hammer", "Alec-Angus Macdonald", "Hollie Marwick-Best", "Emma McKenzie", "George Popa", "Derek Somerville", "Tim Storer"], "summary": "Over more than two decades, The University of Glasgow has co-designed and\ndelivered numerous software engineering focused courses with industry partners,\ncovering both technical and discipline specific professional skills. Such\ncollaborations are not unique and many of the benefits are well recognised in\nthe literature. These include enhancing the real-world relevance of curricula,\ndeveloping student professional networks ahead of graduation and easing\nrecruitment opportunities for employers.\n  However, there is relatively little scholarship on the perspectives of\nindustry practitioners who participate in course design and delivery. This gap\nis significant, since the effort invested by practitioners is often substantial\nand may require ongoing support from both the industry partner and academic\ninstitution. Understanding the motivations, expectations and experiences of\npractitioners who engage in course delivery can guide the formation of future\npartnerships and ensure their long-term sustainability.\n  We begin to address this gap by reporting on the outcomes of a retrospective\nconducted amongst the practitioner coauthors of this paper, with the academic\ncoauthors acting as facilitators. All coauthors have participated in the recent\nco-design and delivery of software engineering courses, but we choose to focus\nexplicitly on the perspectives of the practitioners. We report on the themes\nthat emerged from the discussions and our resulting recommendations for future\ncollaborations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00803v1", "categories": ["cs.SE"], "cate": "cs.SE", "url": "http://arxiv.org/abs/2507.00803v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00523", "title": "Edge Computing and its Application in Robotics: A Survey", "authors": ["Nazish Tahir", "Ramviyas Parasuraman"], "summary": "The Edge computing paradigm has gained prominence in both academic and\nindustry circles in recent years. By implementing edge computing facilities and\nservices in robotics, it becomes a key enabler in the deployment of artificial\nintelligence applications to robots. Time-sensitive robotics applications\nbenefit from the reduced latency, mobility, and location awareness provided by\nthe edge computing paradigm, which enables real-time data processing and\nintelligence at the network's edge. While the advantages of integrating edge\ncomputing into robotics are numerous, there has been no recent survey that\ncomprehensively examines these benefits. This paper aims to bridge that gap by\nhighlighting important work in the domain of edge robotics, examining recent\nadvancements, and offering deeper insight into the challenges and motivations\nbehind both current and emerging solutions. In particular, this article\nprovides a comprehensive evaluation of recent developments in edge robotics,\nwith an emphasis on fundamental applications, providing in-depth analysis of\nthe key motivations, challenges, and future directions in this rapidly evolving\ndomain. It also explores the importance of edge computing in real-world\nrobotics scenarios where rapid response times are critical. Finally, the paper\noutlines various open research challenges in the field of edge robotics.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00523v1", "categories": ["cs.RO", "cs.DC", "cs.NI"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2507.00523v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00847", "title": "Stealtooth: Breaking Bluetooth Security Abusing Silent Automatic Pairing", "authors": ["Keiichiro Kimura", "Hiroki Kuzuno", "Yoshiaki Shiraishi", "Masakatu Morii"], "summary": "Bluetooth is a pervasive wireless communication technology used by billions\nof devices for short-range connectivity. The security of Bluetooth relies on\nthe pairing process, where devices establish shared long-term keys for secure\ncommunications. However, many commercial Bluetooth devices implement automatic\npairing functions to improve user convenience, creating a previously unexplored\nattack surface.\n  We present Stealtooth, a novel attack that abuses unknown vulnerabilities in\nthe automatic pairing functions in commercial Bluetooth devices to achieve\ncompletely silent device link key overwriting. The Stealtooth attack leverages\nthe fact that Bluetooth audio devices automatically transition to pairing mode\nunder specific conditions, enabling attackers to hijack pairing processes\nwithout user awareness or specialized tools. We also extend the attack into the\nMitM Stealtooth attack, combining automatic pairing abuse with power-saving\nmode techniques to enable man-in-the-middle attacks.\n  We evaluate the attacks against 10 commercial Bluetooth devices from major\nmanufacturers, demonstrating widespread vulnerabilities across diverse device\ntypes and manufacturers. Our practical implementation requires only commodity\nhardware and open-source software, highlighting the low barrier to entry for\nattackers.\n  We propose defenses both device and protocol levels, including enhanced user\nnotifications and standardized automatic pairing guidelines. Our findings\nreveal a critical tension between security and usability, showing that current\nautomatic pairing implementations create systematic vulnerabilities. We\nresponsibly disclosed our findings to affected vendors, with several already\nreleasing patches.", "comment": "13 pages, 6 figures. We plan to extend our evaluation to additional\n  device categories. Responsible disclosure completed", "pdf_url": "http://arxiv.org/pdf/2507.00847v1", "categories": ["cs.CR", "cs.NI"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2507.00847v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00254", "title": "Fully Parallelized BP Decoding for Quantum LDPC Codes Can Outperform BP-OSD", "authors": ["Ming Wang", "Ang Li", "Frank Mueller"], "summary": "In this work, we propose a lightweight decoder based solely on\nbelief-propagation (BP), augmented with a speculative post-processing strategy\ninspired by classical Chase decoding. Our method identifies unreliable bits via\nBP oscillation statistics, generates a set of modified test patterns, and\ndecodes them in parallel using low-iteration BP. We demonstrate that our\napproach can achieve logical error rates comparable to or even better than\nBP-OSD, but has lower latency over its parallelization for a variety of\nbivariate bicycle codes, which significantly reduces decoding complexity.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00254v1", "categories": ["quant-ph", "cs.IT", "math.IT"], "cate": "quant-ph", "url": "http://arxiv.org/abs/2507.00254v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00417", "title": "ASTRO: Teaching Language Models to Reason by Reflecting and Backtracking In-Context", "authors": ["Joongwon Kim", "Anirudh Goyal", "Liang Tan", "Hannaneh Hajishirzi", "Srinivasan Iyer", "Tianlu Wang"], "summary": "We introduce ASTRO, the \"Autoregressive Search-Taught Reasoner\", a framework\nfor training language models to reason like search algorithms, explicitly\nleveraging self-reflection, backtracking, and exploration in their outputs.\nRecently, training large language models (LLMs) via reinforcement learning (RL)\nhas led to the advent of reasoning models with greatly enhanced reasoning\ncapabilities. Open-source replications of reasoning models, while successful,\nbuild upon models that already exhibit strong reasoning capabilities along with\nsearch behavior observed even before RL. As a result, it is yet unclear how to\nboost the reasoning capabilities of other non-reasoner models including Llama\n3. ASTRO teaches such models to internalize structured search behavior through\na synthetic dataset derived from Monte Carlo Tree Search (MCTS) over\nmathematical problem-solving trajectories. By converting search traces into\nnatural language chain-of-thoughts that capture both successes and recoveries\nfrom failure, ASTRO bootstraps models with a rich prior for exploration during\nRL. We finetune our models on these search-derived traces and further improve\nperformance via RL with verifiable rewards. We apply ASTRO to the Llama 3\nfamily of models and achieve absolute performance gains of 16.0% on MATH-500,\n26.9% on AMC 2023, and 20.0% on AIME 2024, especially improving upon\nchallenging problems that require iterative correction. Our results demonstrate\nthat search-inspired training offers a principled way to instill robust\nreasoning capabilities into open LLMs.", "comment": "36 pages, 23 figures", "pdf_url": "http://arxiv.org/pdf/2507.00417v1", "categories": ["cs.AI", "cs.CL"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2507.00417v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00020", "title": "Variational Autoencoder for Generating Broader-Spectrum prior Proposals in Markov chain Monte Carlo Methods", "authors": ["Marcio Borges", "Felipe Pereira", "Michel Tosin"], "summary": "This study uses a Variational Autoencoder method to enhance the efficiency\nand applicability of Markov Chain Monte Carlo (McMC) methods by generating\nbroader-spectrum prior proposals. Traditional approaches, such as the\nKarhunen-Lo\\`eve Expansion (KLE), require previous knowledge of the covariance\nfunction, often unavailable in practical applications. The VAE framework\nenables a data-driven approach to flexibly capture a broader range of\ncorrelation structures in Bayesian inverse problems, particularly subsurface\nflow modeling. The methodology is tested on a synthetic groundwater flow\ninversion problem, where pressure data is used to estimate permeability fields.\nNumerical experiments demonstrate that the VAE-based parameterization achieves\ncomparable accuracy to KLE when the correlation length is known and outperforms\nKLE when the assumed correlation length deviates from the true value. Moreover,\nthe VAE approach significantly reduces stochastic dimensionality, improving\ncomputational efficiency. The results suggest that leveraging deep generative\nmodels in McMC methods can lead to more adaptable and efficient Bayesian\ninference in high-dimensional problems.", "comment": "The main contribution of this work is to show the advantages of using\n  deep generative models like VAE to provide more flexible and versatile prior\n  distributions", "pdf_url": "http://arxiv.org/pdf/2507.00020v1", "categories": ["cs.LG", "stat.ML"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00020v1", "date": "2025-06-16", "updated": "2025-06-16"}
{"id": "2507.00264", "title": "Rust vs. C for Python Libraries: Evaluating Rust-Compatible Bindings Toolchains", "authors": ["Isabella Basso do Amaral", "Renato Cordeiro Ferreira", "Alfredo Goldman"], "summary": "The Python programming language is best known for its syntax and scientific\nlibraries, but it is also notorious for its slow interpreter. Optimizing\ncritical sections in Python entails special knowledge of the binary\ninteractions between programming languages, and can be cumbersome to interface\nmanually, with implementers often resorting to convoluted third-party\nlibraries. This comparative study evaluates the performance and ease of use of\nthe PyO3 Python bindings toolchain for Rust against ctypes and cffi. By using\nRust tooling developed for Python, we can achieve state-of-the-art performance\nwith no concern for API compatibility.", "comment": "10 pages, 27 figures (1 diagram, 4 graphs, 9 tables, 13 code\n  listings), submitted to SBAC-PAD 2025", "pdf_url": "http://arxiv.org/pdf/2507.00264v1", "categories": ["cs.PL", "cs.DC", "cs.PF", "cs.SE", "D.2.13; D.2.8; D.3.3; B.8"], "cate": "cs.PL", "url": "http://arxiv.org/abs/2507.00264v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00162", "title": "FreeLong++: Training-Free Long Video Generation via Multi-band SpectralFusion", "authors": ["Yu Lu", "Yi Yang"], "summary": "Recent advances in video generation models have enabled high-quality short\nvideo generation from text prompts. However, extending these models to longer\nvideos remains a significant challenge, primarily due to degraded temporal\nconsistency and visual fidelity. Our preliminary observations show that naively\napplying short-video generation models to longer sequences leads to noticeable\nquality degradation. Further analysis identifies a systematic trend where\nhigh-frequency components become increasingly distorted as video length grows,\nan issue we term high-frequency distortion. To address this, we propose\nFreeLong, a training-free framework designed to balance the frequency\ndistribution of long video features during the denoising process. FreeLong\nachieves this by blending global low-frequency features, which capture holistic\nsemantics across the full video, with local high-frequency features extracted\nfrom short temporal windows to preserve fine details. Building on this,\nFreeLong++ extends FreeLong dual-branch design into a multi-branch architecture\nwith multiple attention branches, each operating at a distinct temporal scale.\nBy arranging multiple window sizes from global to local, FreeLong++ enables\nmulti-band frequency fusion from low to high frequencies, ensuring both\nsemantic continuity and fine-grained motion dynamics across longer video\nsequences. Without any additional training, FreeLong++ can be plugged into\nexisting video generation models (e.g. Wan2.1 and LTX-Video) to produce longer\nvideos with substantially improved temporal consistency and visual fidelity. We\ndemonstrate that our approach outperforms previous methods on longer video\ngeneration tasks (e.g. 4x and 8x of native length). It also supports coherent\nmulti-prompt video generation with smooth scene transitions and enables\ncontrollable video generation using long depth or pose sequences.", "comment": "under review", "pdf_url": "http://arxiv.org/pdf/2507.00162v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00162v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00945", "title": "Time Series Foundation Models are Flow Predictors", "authors": ["Massimiliano Luca", "Ciro Beneduce", "Bruno Lepri"], "summary": "We investigate the effectiveness of time series foundation models (TSFMs) for\ncrowd flow prediction, focusing on Moirai and TimesFM. Evaluated on three\nreal-world mobility datasets-Bike NYC, Taxi Beijing, and Spanish national OD\nflows-these models are deployed in a strict zero-shot setting, using only the\ntemporal evolution of each OD flow and no explicit spatial information. Moirai\nand TimesFM outperform both statistical and deep learning baselines, achieving\nup to 33% lower RMSE, 39% lower MAE and up to 49% higher CPC compared to\nstate-of-the-art competitors. Our results highlight the practical value of\nTSFMs for accurate, scalable flow prediction, even in scenarios with limited\nannotated data or missing spatial context.", "comment": "arXiv admin note: text overlap with arXiv:2203.07372", "pdf_url": "http://arxiv.org/pdf/2507.00945v1", "categories": ["cs.LG", "cs.CY"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00945v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00673", "title": "Prompt2SegCXR:Prompt to Segment All Organs and Diseases in Chest X-rays", "authors": ["Abduz Zami", "Shadman Sobhan", "Rounaq Hossain", "Md. Sawran Sorker", "Mohiuddin Ahmed", "Md. Redwan Hossain"], "summary": "Image segmentation plays a vital role in the medical field by isolating\norgans or regions of interest from surrounding areas. Traditionally,\nsegmentation models are trained on a specific organ or a disease, limiting\ntheir ability to handle other organs and diseases. At present, few advanced\nmodels can perform multi-organ or multi-disease segmentation, offering greater\nflexibility. Also, recently, prompt-based image segmentation has gained\nattention as a more flexible approach. It allows models to segment areas based\non user-provided prompts. Despite these advances, there has been no dedicated\nwork on prompt-based interactive multi-organ and multi-disease segmentation,\nespecially for Chest X-rays. This work presents two main contributions: first,\ngenerating doodle prompts by medical experts of a collection of datasets from\nmultiple sources with 23 classes, including 6 organs and 17 diseases,\nspecifically designed for prompt-based Chest X-ray segmentation. Second, we\nintroduce Prompt2SegCXR, a lightweight model for accurately segmenting multiple\norgans and diseases from Chest X-rays. The model incorporates multi-stage\nfeature fusion, enabling it to combine features from various network layers for\nbetter spatial and semantic understanding, enhancing segmentation accuracy.\nCompared to existing pre-trained models for prompt-based image segmentation,\nour model scores well, providing a reliable solution for segmenting Chest\nX-rays based on user prompts.", "comment": "29 Pages", "pdf_url": "http://arxiv.org/pdf/2507.00673v1", "categories": ["eess.IV", "cs.CV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2507.00673v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00657", "title": "Generative Exaggeration in LLM Social Agents: Consistency, Bias, and Toxicity", "authors": ["Jacopo Nudo", "Mario Edoardo Pandolfo", "Edoardo Loru", "Mattia Samory", "Matteo Cinelli", "Walter Quattrociocchi"], "summary": "We investigate how Large Language Models (LLMs) behave when simulating\npolitical discourse on social media. Leveraging 21 million interactions on X\nduring the 2024 U.S. presidential election, we construct LLM agents based on\n1,186 real users, prompting them to reply to politically salient tweets under\ncontrolled conditions. Agents are initialized either with minimal ideological\ncues (Zero Shot) or recent tweet history (Few Shot), allowing one-to-one\ncomparisons with human replies. We evaluate three model families (Gemini,\nMistral, and DeepSeek) across linguistic style, ideological consistency, and\ntoxicity. We find that richer contextualization improves internal consistency\nbut also amplifies polarization, stylized signals, and harmful language. We\nobserve an emergent distortion that we call \"generation exaggeration\": a\nsystematic amplification of salient traits beyond empirical baselines. Our\nanalysis shows that LLMs do not emulate users, they reconstruct them. Their\noutputs, indeed, reflect internal optimization dynamics more than observed\nbehavior, introducing structural biases that compromise their reliability as\nsocial proxies. This challenges their use in content moderation, deliberative\nsimulations, and policy modeling.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00657v1", "categories": ["cs.HC", "cs.AI", "cs.SI"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2507.00657v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00902", "title": "Constellation as a Service: Tailored Connectivity Management in Direct-Satellite-to-Device Networks", "authors": ["Feng Wang", "Shengyu Zhang", "Een-Kee Hong", "Tony Q. S. Quek"], "summary": "Direct-satellite-to-device (DS2D) communication is emerging as a promising\nsolution for global mobile service extension, leveraging the deployment of\nsatellite constellations. However, the challenge of managing DS2D connectivity\nfor multi-constellations becomes outstanding, including high interference and\nfrequent handovers caused by multi-coverage overlap and rapid satellite\nmovement. Moreover, existing approaches primarily operate within\nsingle-constellation shell, which inherently limits the ability to exploit the\nvast potential of multi-constellation connectivity provision, resulting in\nsuboptimal DS2D service performances. To address these challenges, this article\nproposes a Constellation as a Service (CaaS) framework, which treats the entire\nmulti-constellation infrastructure as a shared resource pool and dynamically\nforms optimal sub-constellations (SCs) for each DS2D service region. The\nformation of each SC integrates satellites from various orbits to provide\ntailored connectivity based on user demands, guided by two innovative\nstrategies: predictive satellite beamforming using generative artificial\nintelligence (GenAI) and pre-configured handover path for efficient satellite\naccess and mobility management. Simulation results demonstrate that CaaS\nsignificantly improves satellite service rates while reducing handover\noverhead, making it an efficient and continuable solution for managing DS2D\nconnectivity in multi-constellation environments.", "comment": "To appear in IEEE Communications Magazine", "pdf_url": "http://arxiv.org/pdf/2507.00902v1", "categories": ["eess.SY", "cs.AI", "cs.SY", "eess.SP"], "cate": "eess.SY", "url": "http://arxiv.org/abs/2507.00902v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00061", "title": "Smooth-Distill: A Self-distillation Framework for Multitask Learning with Wearable Sensor Data", "authors": ["Hoang-Dieu Vu", "Duc-Nghia Tran", "Quang-Tu Pham", "Hieu H. Pham", "Nicolas Vuillerme", "Duc-Tan Tran"], "summary": "This paper introduces Smooth-Distill, a novel self-distillation framework\ndesigned to simultaneously perform human activity recognition (HAR) and sensor\nplacement detection using wearable sensor data. The proposed approach utilizes\na unified CNN-based architecture, MTL-net, which processes accelerometer data\nand branches into two outputs for each respective task. Unlike conventional\ndistillation methods that require separate teacher and student models, the\nproposed framework utilizes a smoothed, historical version of the model itself\nas the teacher, significantly reducing training computational overhead while\nmaintaining performance benefits. To support this research, we developed a\ncomprehensive accelerometer-based dataset capturing 12 distinct sleep postures\nacross three different wearing positions, complementing two existing public\ndatasets (MHealth and WISDM). Experimental results show that Smooth-Distill\nconsistently outperforms alternative approaches across different evaluation\nscenarios, achieving notable improvements in both human activity recognition\nand device placement detection tasks. This method demonstrates enhanced\nstability in convergence patterns during training and exhibits reduced\noverfitting compared to traditional multitask learning baselines. This\nframework contributes to the practical implementation of knowledge distillation\nin human activity recognition systems, offering an effective solution for\nmultitask learning with accelerometer data that balances accuracy and training\nefficiency. More broadly, it reduces the computational cost of model training,\nwhich is critical for scenarios requiring frequent model updates or training on\nresource-constrained platforms. The code and model are available at\nhttps://github.com/Kuan2vn/smooth\\_distill.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00061v1", "categories": ["cs.LG", "cs.AI", "eess.SP"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00061v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2507.00475", "title": "AudioBERTScore: Objective Evaluation of Environmental Sound Synthesis Based on Similarity of Audio embedding Sequences", "authors": ["Minoru Kishi", "Ryosuke Sakai", "Shinnosuke Takamichi", "Yusuke Kanamori", "Yuki Okamoto"], "summary": "We propose a novel objective evaluation metric for synthesized audio in\ntext-to-audio (TTA), aiming to improve the performance of TTA models. In TTA,\nsubjective evaluation of the synthesized sound is an important, but its\nimplementation requires monetary costs. Therefore, objective evaluation such as\nmel-cepstral distortion are used, but the correlation between these objective\nmetrics and subjective evaluation values is weak. Our proposed objective\nevaluation metric, AudioBERTScore, calculates the similarity between embedding\nof the synthesized and reference sounds. The method is based not only on the\nmax-norm used in conventional BERTScore but also on the $p$-norm to reflect the\nnon-local nature of environmental sounds. Experimental results show that scores\nobtained by the proposed method have a higher correlation with subjective\nevaluation values than conventional metrics.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00475v1", "categories": ["cs.SD", "eess.AS"], "cate": "cs.SD", "url": "http://arxiv.org/abs/2507.00475v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00258", "title": "Impact of Fine-Tuning Methods on Memorization in Large Language Models", "authors": ["Jie Hou", "Chuxiong Wu", "Lannan Luo", "Qiang Zeng"], "summary": "As the capabilities of pre-trained large language models (LLMs) continue to\nadvance, the \"pre-train and fine-tune\" paradigm has become increasingly\nmainstream, leading to the development of various fine-tuning methods. However,\nthe privacy risks arising from memorization during fine-tuning have received\nrelatively little attention. To address this gap, we categorize popular\nfine-tuning approaches and assess their impact on memorization through the lens\nof membership inference attacks (MIAs). Our results show that, compared to\nparameter-based fine-tuning, prompt-based fine-tuning achieves competitive\nperformance while exhibiting lower vulnerability to MIAs. Furthermore,\nprompt-based methods maintain low memorization regardless of model scale. These\nfindings suggest that parameter-based fine-tuning is more prone to leaking\nprivate information, whereas prompt-based fine-tuning serves as a more\nprivacy-preserving option.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00258v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2507.00258v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00938", "title": "WebArXiv: Evaluating Multimodal Agents on Time-Invariant arXiv Tasks", "authors": ["Zihao Sun", "Meng Fang", "Ling Chen"], "summary": "Recent progress in large language models (LLMs) has enabled the development\nof autonomous web agents capable of navigating and interacting with real\nwebsites. However, evaluating such agents remains challenging due to the\ninstability and inconsistency of existing benchmarks, which often rely on\ndynamic content or oversimplified simulations. In this work, we introduce\nWebArXiv, a static and time-invariant benchmark comprising 275 web-based tasks\ngrounded in the arXiv platform. WebArXiv ensures reproducible and reliable\nevaluation by anchoring tasks in fixed web snapshots with deterministic ground\ntruths and standardized action trajectories. Through behavioral analysis, we\nidentify a common failure mode, Rigid History Reflection, where agents\nover-rely on fixed interaction histories. To address this, we propose a\nlightweight dynamic reflection mechanism that allows agents to selectively\nretrieve relevant past steps during decision-making. We evaluate ten\nstate-of-the-art web agents on WebArXiv. Results demonstrate clear performance\ndifferences across agents and validate the effectiveness of our proposed\nreflection strategy.", "comment": "10 pages, 9 figures, 4 tables", "pdf_url": "http://arxiv.org/pdf/2507.00938v1", "categories": ["cs.IR", "cs.AI", "cs.DB", "F.2.2; I.2.7"], "cate": "cs.IR", "url": "http://arxiv.org/abs/2507.00938v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00531", "title": "An inverse-free fixed-time stable dynamical system and its forward-Euler discretization for solving generalized absolute value equations", "authors": ["Xuehua Li", "Linjie Chen", "Dongmei Yu", "Cairong Chen", "Deren Han"], "summary": "An inverse-free dynamical system is proposed to solve the generalized\nabsolute value equation (GAVE) within a fixed time, where the time of\nconvergence is finite and is uniformly bounded for all initial points.\nMoreover, an iterative method obtained by using the forward-Euler\ndiscretization of the proposed dynamic model are developed and sufficient\nconditions which guarantee that the discrete iteration globally converge to an\narbitrarily small neighborhood of the unique solution of GAVE within a finite\nnumber of iterative steps are given.", "comment": "14 pages", "pdf_url": "http://arxiv.org/pdf/2507.00531v1", "categories": ["math.NA", "cs.NA", "math.OC"], "cate": "math.NA", "url": "http://arxiv.org/abs/2507.00531v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2503.04995", "title": "Musical Source Separation of Brazilian Percussion", "authors": ["Richa Namballa", "Giovana Morais", "Magdalena Fuentes"], "summary": "Musical source separation (MSS) has recently seen a big breakthrough in\nseparating instruments from a mixture in the context of Western music, but\nresearch on non-Western instruments is still limited due to a lack of data. In\nthis demo, we use an existing dataset of Brazilian sama percussion to create\nartificial mixtures for training a U-Net model to separate the surdo drum, a\ntraditional instrument in samba. Despite limited training data, the model\neffectively isolates the surdo, given the drum's repetitive patterns and its\ncharacteristic low-pitched timbre. These results suggest that MSS systems can\nbe successfully harnessed to work in more culturally-inclusive scenarios\nwithout the need of collecting extensive amounts of data.", "comment": "2 pages + references, 1 figure, 1 table, Extended Abstracts for the\n  Late-Breaking Demo Session of the 25th International Society for Music\n  Information Retrieval Conference", "pdf_url": "http://arxiv.org/pdf/2503.04995v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "cate": "eess.AS", "url": "http://arxiv.org/abs/2503.04995v1", "date": "2025-03-06", "updated": "2025-03-06"}
{"id": "2507.00015", "title": "Vision Transformer with Adversarial Indicator Token against Adversarial Attacks in Radio Signal Classifications", "authors": ["Lu Zhang", "Sangarapillai Lambotharan", "Gan Zheng", "Guisheng Liao", "Xuekang Liu", "Fabio Roli", "Carsten Maple"], "summary": "The remarkable success of transformers across various fields such as natural\nlanguage processing and computer vision has paved the way for their\napplications in automatic modulation classification, a critical component in\nthe communication systems of Internet of Things (IoT) devices. However, it has\nbeen observed that transformer-based classification of radio signals is\nsusceptible to subtle yet sophisticated adversarial attacks. To address this\nissue, we have developed a defensive strategy for transformer-based modulation\nclassification systems to counter such adversarial attacks. In this paper, we\npropose a novel vision transformer (ViT) architecture by introducing a new\nconcept known as adversarial indicator (AdvI) token to detect adversarial\nattacks. To the best of our knowledge, this is the first work to propose an\nAdvI token in ViT to defend against adversarial attacks. Integrating an\nadversarial training method with a detection mechanism using AdvI token, we\ncombine a training time defense and running time defense in a unified neural\nnetwork model, which reduces architectural complexity of the system compared to\ndetecting adversarial perturbations using separate models. We investigate into\nthe operational principles of our method by examining the attention mechanism.\nWe show the proposed AdvI token acts as a crucial element within the ViT,\ninfluencing attention weights and thereby highlighting regions or features in\nthe input data that are potentially suspicious or anomalous. Through\nexperimental results, we demonstrate that our approach surpasses several\ncompetitive methods in handling white-box attack scenarios, including those\nutilizing the fast gradient method, projected gradient descent attacks and\nbasic iterative method.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00015v1", "categories": ["cs.LG", "cs.AI", "cs.CR"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00015v1", "date": "2025-06-13", "updated": "2025-06-13"}
{"id": "2507.00014", "title": "SWE-Bench-CL: Continual Learning for Coding Agents", "authors": ["Thomas Joshi", "Shayan Chowdhury", "Fatih Uysal"], "summary": "Large Language Models (LLMs) have achieved impressive results on static\ncode-generation benchmarks, but real-world software development unfolds as a\ncontinuous stream of evolving issues, fixes, and feature requests. We introduce\nSWE-Bench-CL, a novel continual learning benchmark built on the human-verified\nSWE-Bench Verified dataset introduced by OpenAI and Princeton-NLP in 2024. By\norganizing GitHub issues into chronologically ordered sequences that reflect\nnatural repository evolution, SWE-Bench-CL enables direct evaluation of an\nagent's ability to accumulate experience, transfer knowledge across tasks, and\nresist catastrophic forgetting. We complement the dataset with (i) a\npreliminary analysis of inter-task structural similarity and contextual\nsensitivity, (ii) an interactive LangGraph-based evaluation framework augmented\nwith a FAISS-backed semantic memory module, and (iii) a suite of specialized\ncontinual learning metrics -- including average accuracy, forgetting,\nforward/backward transfer, tool-use efficiency, and a generalized Composite\nContinual Learning Score and CL-F-beta score -- to capture the\nstability-plasticity trade-off. We outline a rigorous experimental protocol\ncomparing memory-enabled and memory-disabled agents across diverse Python\nrepositories. All code and data are publicly available at\nhttps://github.com/thomasjoshi/agents-never-forget, providing the community\nwith a reproducible platform for developing more adaptive and robust AI agents\nin software engineering.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00014v1", "categories": ["cs.LG", "cs.AI", "cs.SE"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00014v1", "date": "2025-06-13", "updated": "2025-06-13"}
{"id": "2507.00552", "title": "Generation of Indoor Open Street Maps for Robot Navigation from CAD Files", "authors": ["Jiajie Zhang", "Shenrui Wu", "Xu Ma", "Sören Schwertfeger"], "summary": "The deployment of autonomous mobile robots is predicated on the availability\nof environmental maps, yet conventional generation via SLAM (Simultaneous\nLocalization and Mapping) suffers from significant limitations in time, labor,\nand robustness, particularly in dynamic, large-scale indoor environments where\nmap obsolescence can lead to critical localization failures. To address these\nchallenges, this paper presents a complete and automated system for converting\narchitectural Computer-Aided Design (CAD) files into a hierarchical topometric\nOpenStreetMap (OSM) representation, tailored for robust life-long robot\nnavigation. Our core methodology involves a multi-stage pipeline that first\nisolates key structural layers from the raw CAD data and then employs an\nAreaGraph-based topological segmentation to partition the building layout into\na hierarchical graph of navigable spaces. This process yields a comprehensive\nand semantically rich map, further enhanced by automatically associating\ntextual labels from the CAD source and cohesively merging multiple building\nfloors into a unified, topologically-correct model. By leveraging the permanent\nstructural information inherent in CAD files, our system circumvents the\ninefficiencies and fragility of SLAM, offering a practical and scalable\nsolution for deploying robots in complex indoor spaces. The software is\nencapsulated within an intuitive Graphical User Interface (GUI) to facilitate\npractical use. The code and dataset are available at\nhttps://github.com/jiajiezhang7/osmAG-from-cad.", "comment": "8 pages, 8 figures", "pdf_url": "http://arxiv.org/pdf/2507.00552v1", "categories": ["cs.RO"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2507.00552v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00451", "title": "Best Agent Identification for General Game Playing", "authors": ["Matthew Stephenson", "Alex Newcombe", "Eric Piette", "Dennis Soemers"], "summary": "We present an efficient and generalised procedure to accurately identify the\nbest performing algorithm for each sub-task in a multi-problem domain. Our\napproach treats this as a set of best arm identification problems for\nmulti-armed bandits, where each bandit corresponds to a specific task and each\narm corresponds to a specific algorithm or agent. We propose an optimistic\nselection process based on the Wilson score interval (Optimistic-WS) that ranks\neach arm across all bandits in terms of their potential regret reduction. We\nevaluate the performance of Optimistic-WS on two of the most popular general\ngame domains, the General Video Game AI (GVGAI) framework and the Ludii general\ngame playing system, with the goal of identifying the highest performing agent\nfor each game within a limited number of trials. Compared to previous best arm\nidentification algorithms for multi-armed bandits, our results demonstrate a\nsubstantial performance improvement in terms of average simple regret. This\nnovel approach can be used to significantly improve the quality and accuracy of\nagent evaluation procedures for general game frameworks, as well as other\nmulti-task domains with high algorithm runtimes.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00451v1", "categories": ["cs.LG", "cs.AI", "cs.DS", "cs.IT", "math.IT", "stat.ML"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00451v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00432", "title": "Does Math Reasoning Improve General LLM Capabilities? Understanding Transferability of LLM Reasoning", "authors": ["Maggie Huan", "Yuetai Li", "Tuney Zheng", "Xiaoyu Xu", "Seungone Kim", "Minxin Du", "Radha Poovendran", "Graham Neubig", "Xiang Yue"], "summary": "Math reasoning has become the poster child of progress in large language\nmodels (LLMs), with new models rapidly surpassing human-level performance on\nbenchmarks like MATH and AIME. But as math leaderboards improve week by week,\nit is worth asking: do these gains reflect broader problem-solving ability or\njust narrow overfitting? To answer this question, we evaluate over 20\nopen-weight reasoning-tuned models across a broad suite of tasks, including\nmath, scientific QA, agent planning, coding, and standard\ninstruction-following. We surprisingly find that most models that succeed in\nmath fail to transfer their gains to other domains. To rigorously study this\nphenomenon, we conduct controlled experiments on Qwen3-14B models using\nmath-only data but different tuning methods. We find that reinforcement\nlearning (RL)-tuned models generalize well across domains, while supervised\nfine-tuning (SFT)-tuned models often forget general capabilities. Latent-space\nrepresentation and token-space distribution shift analyses reveal that SFT\ninduces substantial representation and output drift, while RL preserves\ngeneral-domain structure. Our results suggest a need to rethink standard\npost-training recipes, particularly the reliance on SFT-distilled data for\nadvancing reasoning models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00432v1", "categories": ["cs.AI", "cs.CL"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2507.00432v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00022", "title": "GLU Attention Improve Transformer", "authors": ["Zehao Wang"], "summary": "Gated Linear Units (GLU) have shown great potential in enhancing neural\nnetwork performance. In this paper, I introduce a novel attention mechanism\ncalled GLU Attention, which introduces nonlinearity into the values of\nAttention. My experiments demonstrate that GLU Attention improves both model\nperformance and convergence speed across text and vision modalities with zero\nadditional parameters and negligible computational costs. GLU Attention is\nlightweight and can seamlessly integrate with other technologies, such as Flash\nAttention, Rotary Position Embedding (RoPE), and various Multi-Head Attention\n(MHA) variants such as Grouped-Query Attention (GQA). This project is\nopen-sourced at github.", "comment": "4 pages 4 figures", "pdf_url": "http://arxiv.org/pdf/2507.00022v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.NE"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00022v1", "date": "2025-06-16", "updated": "2025-06-16"}
{"id": "2507.00394", "title": "HelixPipe: Efficient Distributed Training of Long Sequence Transformers with Attention Parallel Pipeline Parallelism", "authors": ["Geng Zhang", "Shenggan Cheng", "Xuanlei Zhao", "Ziming Liu", "Yang You"], "summary": "As transformer sequence lengths grow, existing pipeline parallelisms incur\nsuboptimal performance due to the quadratic attention computation and the\nsubstantial memory overhead. To relieve these challenges, we propose HelixPipe,\na novel pipeline parallelism for long sequence transformer training. First,\nHelixPipe introduces attention parallel partition, which schedules attention\ncomputations of different micro batches across different pipeline stages in\nparallel, reducing pipeline bubbles. Second, it employs a two-fold\nfirst-in-last-out micro batch schedule to balance memory usage and overlap\ncommunication with computation. Additionally, HelixPipe utilizes recomputation\nwithout attention and chunked MLP to mitigate fragmentation and enable longer\nsequences. Experiments demonstrate that HelixPipe gains increasing advantages\nwith longer sequence lengths, and outperforms existing methods in throughput\nand scalability across varying pipeline sizes, model sizes, and cluster\nconfigurations. Notably, it achieves a 26\\% speedup over baseline methods when\ntraining a 7B model with 128k sequence length on 64 H20 GPUs. Code is available\nat https://github.com/code-tunnel/Megatron-LM/tree/dev.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00394v1", "categories": ["cs.LG", "cs.DC"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00394v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00170", "title": "SelvaBox: A high-resolution dataset for tropical tree crown detection", "authors": ["Hugo Baudchon", "Arthur Ouaknine", "Martin Weiss", "Mélisande Teng", "Thomas R. Walla", "Antoine Caron-Guay", "Christopher Pal", "Etienne Laliberté"], "summary": "Detecting individual tree crowns in tropical forests is essential to study\nthese complex and crucial ecosystems impacted by human interventions and\nclimate change. However, tropical crowns vary widely in size, structure, and\npattern and are largely overlapping and intertwined, requiring advanced remote\nsensing methods applied to high-resolution imagery. Despite growing interest in\ntropical tree crown detection, annotated datasets remain scarce, hindering\nrobust model development. We introduce SelvaBox, the largest open-access\ndataset for tropical tree crown detection in high-resolution drone imagery. It\nspans three countries and contains more than 83,000 manually labeled crowns -\nan order of magnitude larger than all previous tropical forest datasets\ncombined. Extensive benchmarks on SelvaBox reveal two key findings: (1)\nhigher-resolution inputs consistently boost detection accuracy; and (2) models\ntrained exclusively on SelvaBox achieve competitive zero-shot detection\nperformance on unseen tropical tree crown datasets, matching or exceeding\ncompeting methods. Furthermore, jointly training on SelvaBox and three other\ndatasets at resolutions from 3 to 10 cm per pixel within a unified\nmulti-resolution pipeline yields a detector ranking first or second across all\nevaluated datasets. Our dataset, code, and pre-trained weights are made public.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00170v1", "categories": ["cs.CV", "I.2.10; I.4.8; I.5.4"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00170v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00963", "title": "Social Robots for People with Dementia: A Literature Review on Deception from Design to Perception", "authors": ["Fan Wang", "Giulia Perugia", "Yuan Feng", "Wijnand IJsselsteijn"], "summary": "As social robots increasingly enter dementia care, concerns about deception,\nintentional or not, are gaining attention. Yet, how robotic design cues might\nelicit misleading perceptions in people with dementia, and how these\nperceptions arise, remains insufficiently understood. In this scoping review,\nwe examined 26 empirical studies on interactions between people with dementia\nand physical social robots. We identify four key design cue categories that may\ninfluence deceptive impressions: cues resembling physiological signs (e.g.,\nsimulated breathing), social intentions (e.g., playful movement), familiar\nbeings (e.g., animal-like form and sound), and, to a lesser extent, cues that\nreveal artificiality. Thematic analysis of user responses reveals that people\nwith dementia often attribute biological, social, and mental capacities to\nrobots, dynamically shifting between awareness and illusion. These findings\nunderscore the fluctuating nature of ontological perception in dementia\ncontexts. Existing definitions of robotic deception often rest on philosophical\nor behaviorist premises, but rarely engage with the cognitive mechanisms\ninvolved. We propose an empirically grounded definition: robotic deception\noccurs when Type 1 (automatic, heuristic) processing dominates over Type 2\n(deliberative, analytic) reasoning, leading to misinterpretation of a robot's\nartificial nature. This dual-process perspective highlights the ethical\ncomplexity of social robots in dementia care and calls for design approaches\nthat are not only engaging, but also epistemically respectful.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00963v1", "categories": ["cs.HC", "cs.CY", "cs.RO"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2507.00963v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00743", "title": "Tunable Wavelet Unit based Convolutional Neural Network in Optical Coherence Tomography Analysis Enhancement for Classifying Type of Epiretinal Membrane Surgery", "authors": ["An Le", "Nehal Mehta", "William Freeman", "Ines Nagel", "Melanie Tran", "Anna Heinke", "Akshay Agnihotri", "Lingyun Cheng", "Dirk-Uwe Bartsch", "Hung Nguyen", "Truong Nguyen", "Cheolhong An"], "summary": "In this study, we developed deep learning-based method to classify the type\nof surgery performed for epiretinal membrane (ERM) removal, either internal\nlimiting membrane (ILM) removal or ERM-alone removal. Our model, based on the\nResNet18 convolutional neural network (CNN) architecture, utilizes\npostoperative optical coherence tomography (OCT) center scans as inputs. We\nevaluated the model using both original scans and scans preprocessed with\nenergy crop and wavelet denoising, achieving 72% accuracy on preprocessed\ninputs, outperforming the 66% accuracy achieved on original scans. To further\nimprove accuracy, we integrated tunable wavelet units with two key adaptations:\nOrthogonal Lattice-based Wavelet Units (OrthLatt-UwU) and Perfect\nReconstruction Relaxation-based Wavelet Units (PR-Relax-UwU). These units\nallowed the model to automatically adjust filter coefficients during training\nand were incorporated into downsampling, stride-two convolution, and pooling\nlayers, enhancing its ability to distinguish between ERM-ILM removal and\nERM-alone removal, with OrthLattUwU boosting accuracy to 76% and PR-Relax-UwU\nincreasing performance to 78%. Performance comparisons showed that our AI model\noutperformed a trained human grader, who achieved only 50% accuracy in\nclassifying the removal surgery types from postoperative OCT scans. These\nfindings highlight the potential of CNN based models to improve clinical\ndecision-making by providing more accurate and reliable classifications. To the\nbest of our knowledge, this is the first work to employ tunable wavelets for\nclassifying different types of ERM removal surgery.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00743v1", "categories": ["eess.IV", "cs.CV", "eess.SP"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2507.00743v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00775", "title": "Designing Visualization Widgets for Tangible Data Exploration: A Systematic Review", "authors": ["Haonan Yao", "Lingyun Yu", "Lijie Yao"], "summary": "We present a systematic review on tasks, interactions, and visualization\nwidgets (refer to tangible entities that are used to accomplish data\nexploration tasks through specific interactions) in the context of tangible\ndata exploration. Tangible widgets have been shown to reduce cognitive load,\nenable more natural interactions, and support the completion of complex data\nexploration tasks. Yet, the field lacks a structured understanding of how task\ntypes, interaction methods, and widget designs are coordinated, limiting the\nability to identify recurring design patterns and opportunities for innovation.\nTo address this gap, we conduct a systematic review to analyze existing work\nand characterize the current design of data exploration tasks, interactions,\nand tangible visualization widgets. We next reflect based on our findings and\npropose a research agenda to inform the development of a future widget design\ntoolkit for tangible data exploration. Our systematic review and supplemental\nmaterials are available at physicalviswidget.github.io and osf.io/vjw5e.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00775v1", "categories": ["cs.HC"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2507.00775v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00997", "title": "Geometrization of Higher-Order Linear Control Laws for Attitude Control on $\\mathsf{SO(3)}$", "authors": ["Farooq Aslam", "Hafiz Zeeshan Iqbal Khan", "Muhammad Farooq Haydar", "Suhail Akhtar", "Jamshed Riaz"], "summary": "This paper presents a theoretical framework for analyzing the stability of\nhigher-order geometric nonlinear control laws for attitude control on the\nSpecial Orthogonal Group $\\mathrm{SO(3)}$. In particular, the paper extends\nexisting results on the analysis of PID-type geometric nonlinear control laws\nto more general higher-order dynamic state-feedback compensators on\n$\\mathrm{SO(3)}$. The candidate Lyapunov function is motivated by quadratic\nLyapunov functions of the form $V(x)=x^{\\top}Px$ typically considered in the\nanalysis of linear time-invariant (LTI) systems. The stability analysis is\ncarried out in two steps. In the first step, a sufficient condition is obtained\nfor the positive definiteness of the candidate Lyapunov function, and a\nnecessary and sufficient condition for the negative definiteness of the\ncorresponding Lyapunov rate. These conditions ensure that the desired\nequilibrium is almost globally asymptotically stable (AGAS). In the second\nstep, a convex relaxation of the proposed conditions is used to obtain\nsufficient conditions in the form of linear matrix inequalities (LMIs).\nOverall, the approach is motivated by the widespread use of LMI-based analysis\nand design tools for LTI systems. To reduce conservatism, matrix gains are\nconsidered for the controller gains as well as the Lyapunov function\ncoefficients. The applicability of the approach to practical problems is\nillustrated by designing and analyzing a 21-state geometric nonlinear attitude\ncontrol law for a multicopter.", "comment": "14 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2507.00997v1", "categories": ["eess.SY", "cs.SY", "math.OC"], "cate": "eess.SY", "url": "http://arxiv.org/abs/2507.00997v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00102", "title": "Towards transparent and data-driven fault detection in manufacturing: A case study on univariate, discrete time series", "authors": ["Bernd Hofmann", "Patrick Bruendl", "Huong Giang Nguyen", "Joerg Franke"], "summary": "Ensuring consistent product quality in modern manufacturing is crucial,\nparticularly in safety-critical applications. Conventional quality control\napproaches, reliant on manually defined thresholds and features, lack\nadaptability to the complexity and variability inherent in production data and\nnecessitate extensive domain expertise. Conversely, data-driven methods, such\nas machine learning, demonstrate high detection performance but typically\nfunction as black-box models, thereby limiting their acceptance in industrial\nenvironments where interpretability is paramount. This paper introduces a\nmethodology for industrial fault detection, which is both data-driven and\ntransparent. The approach integrates a supervised machine learning model for\nmulti-class fault classification, Shapley Additive Explanations for post-hoc\ninterpretability, and a do-main-specific visualisation technique that maps\nmodel explanations to operator-interpretable features. Furthermore, the study\nproposes an evaluation methodology that assesses model explanations through\nquantitative perturbation analysis and evaluates visualisations by qualitative\nexpert assessment. The approach was applied to the crimping process, a\nsafety-critical joining technique, using a dataset of univariate, discrete time\nseries. The system achieves a fault detection accuracy of 95.9 %, and both\nquantitative selectivity analysis and qualitative expert evaluations confirmed\nthe relevance and inter-pretability of the generated explanations. This\nhuman-centric approach is designed to enhance trust and interpretability in\ndata-driven fault detection, thereby contributing to applied system design in\nindustrial quality control.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00102v1", "categories": ["cs.LG", "cs.AI", "eess.SP"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00102v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00498", "title": "MuteSwap: Silent Face-based Voice Conversion", "authors": ["Yifan Liu", "Yu Fang", "Zhouhan Lin"], "summary": "Conventional voice conversion modifies voice characteristics from a source\nspeaker to a target speaker, relying on audio input from both sides. However,\nthis process becomes infeasible when clean audio is unavailable, such as in\nsilent videos or noisy environments. In this work, we focus on the task of\nSilent Face-based Voice Conversion (SFVC), which does voice conversion entirely\nfrom visual inputs. i.e., given images of a target speaker and a silent video\nof a source speaker containing lip motion, SFVC generates speech aligning the\nidentity of the target speaker while preserving the speech content in the\nsource silent video. As this task requires generating intelligible speech and\nconverting identity using only visual cues, it is particularly challenging. To\naddress this, we introduce MuteSwap, a novel framework that employs contrastive\nlearning to align cross-modality identities and minimize mutual information to\nseparate shared visual features. Experimental results show that MuteSwap\nachieves impressive performance in both speech synthesis and identity\nconversion, especially under noisy conditions where methods dependent on audio\ninput fail to produce intelligible results, demonstrating both the\neffectiveness of our training approach and the feasibility of SFVC.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00498v1", "categories": ["cs.SD", "cs.CV", "cs.LG", "cs.MM", "eess.AS"], "cate": "cs.SD", "url": "http://arxiv.org/abs/2507.00498v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00297", "title": "Natural language processing for African languages", "authors": ["David Ifeoluwa Adelani"], "summary": "Recent advances in word embeddings and language models use large-scale,\nunlabelled data and self-supervised learning to boost NLP performance.\nMultilingual models, often trained on web-sourced data like Wikipedia, face\nchallenges: few low-resource languages are included, their data is often noisy,\nand lack of labeled datasets makes it hard to evaluate performance outside\nhigh-resource languages like English. In this dissertation, we focus on\nlanguages spoken in Sub-Saharan Africa where all the indigenous languages in\nthis region can be regarded as low-resourced in terms of the availability of\nlabelled data for NLP tasks and unlabelled data found on the web. We analyse\nthe noise in the publicly available corpora, and curate a high-quality corpus,\ndemonstrating that the quality of semantic representations learned in word\nembeddings does not only depend on the amount of data but on the quality of\npre-training data. We demonstrate empirically the limitations of word\nembeddings, and the opportunities the multilingual pre-trained language model\n(PLM) offers especially for languages unseen during pre-training and\nlow-resource scenarios. We further study how to adapt and specialize\nmultilingual PLMs to unseen African languages using a small amount of\nmonolingual texts. To address the under-representation of the African languages\nin NLP research, we developed large scale human-annotated labelled datasets for\n21 African languages in two impactful NLP tasks: named entity recognition and\nmachine translation. We conduct an extensive empirical evaluation using\nstate-of-the-art methods across supervised, weakly-supervised, and transfer\nlearning settings.", "comment": "PhD thesis", "pdf_url": "http://arxiv.org/pdf/2507.00297v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2507.00297v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00041", "title": "TalentMine: LLM-Based Extraction and Question-Answering from Multimodal Talent Tables", "authors": ["Varun Mannam", "Fang Wang", "Chaochun Liu", "Xin Chen"], "summary": "In talent management systems, critical information often resides in complex\ntabular formats, presenting significant retrieval challenges for conventional\nlanguage models. These challenges are pronounced when processing Talent\ndocumentation that requires precise interpretation of tabular relationships for\naccurate information retrieval and downstream decision-making. Current table\nextraction methods struggle with semantic understanding, resulting in poor\nperformance when integrated into retrieval-augmented chat applications. This\npaper identifies a key bottleneck - while structural table information can be\nextracted, the semantic relationships between tabular elements are lost,\ncausing downstream query failures. To address this, we introduce TalentMine, a\nnovel LLM-enhanced framework that transforms extracted tables into semantically\nenriched representations. Unlike conventional approaches relying on CSV or text\nlinearization, our method employs specialized multimodal reasoning to preserve\nboth structural and semantic dimensions of tabular data. Experimental\nevaluation across employee benefits document collections demonstrates\nTalentMine's superior performance, achieving 100% accuracy in query answering\ntasks compared to 0% for standard AWS Textract extraction and 40% for AWS\nTextract Visual Q&A capabilities. Our comparative analysis also reveals that\nthe Claude v3 Haiku model achieves optimal performance for talent management\napplications. The key contributions of this work include (1) a systematic\nanalysis of semantic information loss in current table extraction pipelines,\n(2) a novel LLM-based method for semantically enriched table representation,\n(3) an efficient integration framework for retrieval-augmented systems as\nend-to-end systems, and (4) comprehensive benchmarks on talent analytics tasks\nshowing substantial improvements across multiple categories.", "comment": "Submitted to KDD conference, workshop: Talent and Management\n  Computing (TMC 2025), https://tmcworkshop.github.io/2025/", "pdf_url": "http://arxiv.org/pdf/2507.00041v1", "categories": ["cs.AI", "cs.CV", "cs.IR"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2507.00041v1", "date": "2025-06-22", "updated": "2025-06-22"}
{"id": "2507.00563", "title": "Isogeometric contact analysis in subsea umbilical and power cables", "authors": ["Tianjiao Dai", "Shuo Yang", "Xing Jin", "Svein Sævik", "Jiaxuan Zhang", "Jun Wu", "Naiquan Ye"], "summary": "Subsea umbilical and power cables contain a large number of contact\ninterfaces between different geometries and materials. These complex\ninteractions rise significant challenges for accurately considering contact\nsurface properties by using traditional analytical solutions or finite element\nmethods. These properties have been identified as the most sensitive parameters\nwhen performing the numerical simulation for stress analysis. Therefore, it is\nessential to apply a novel approach for contact analysis which improves the\naccuracy and efficiency for predicting contact properties. This paper presents\nan isogeometric analysis (IGA) approach addressing contact problems in dynamic\numbilicals and power cables. Firstly, this isogeometric contact algorithm is\nformulated in MATLAB as a tool including the geometry description, contact\ndetection and penalty function. Secondly, the contact interface between a steel\ntube and an outer sheath in an dynamic umbilical is established by this IGA\ncontact algorithm and validated against that in ABAQUS for proving the accuracy\nand efficiency of IGA. Finally, the effects of element refinement, geometrical\ndescription, penalty factor on the accuracy, efficiency and stability of IGA\nare discussed.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00563v1", "categories": ["math.NA", "cs.NA"], "cate": "math.NA", "url": "http://arxiv.org/abs/2507.00563v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00155", "title": "Do Music Source Separation Models Preserve Spatial Information in Binaural Audio?", "authors": ["Richa Namballa", "Agnieszka Roginska", "Magdalena Fuentes"], "summary": "Binaural audio remains underexplored within the music information retrieval\ncommunity. Motivated by the rising popularity of virtual and augmented reality\nexperiences as well as potential applications to accessibility, we investigate\nhow well existing music source separation (MSS) models perform on binaural\naudio. Although these models process two-channel inputs, it is unclear how\neffectively they retain spatial information. In this work, we evaluate how\nseveral popular MSS models preserve spatial information on both standard stereo\nand novel binaural datasets. Our binaural data is synthesized using stems from\nMUSDB18-HQ and open-source head-related transfer functions by positioning\ninstrument sources randomly along the horizontal plane. We then assess the\nspatial quality of the separated stems using signal processing and interaural\ncue-based metrics. Our results show that stereo MSS models fail to preserve the\nspatial information critical for maintaining the immersive quality of binaural\naudio, and that the degradation depends on model architecture as well as the\ntarget instrument. Finally, we highlight valuable opportunities for future work\nat the intersection of MSS and immersive audio.", "comment": "6 pages + references, 4 figures, 2 tables, 26th International Society\n  for Music Information Retrieval (ISMIR) Conference", "pdf_url": "http://arxiv.org/pdf/2507.00155v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "cate": "eess.AS", "url": "http://arxiv.org/abs/2507.00155v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00095", "title": "Authentication of Continuous-Variable Quantum Messages", "authors": ["Mehmet Hüseyin Temel", "Boris Škorić"], "summary": "We introduce the first quantum authentication scheme for continuous-variable\nstates. Our scheme is based on trap states, and is an adaptation of a\ndiscrete-variable scheme by Broadbent et al. (arXiv:1211.1080), but with more\nfreedom in choosing the number of traps. We provide a security proof, mostly\nfollowing the approach of Broadbent and Wainewright (arXiv:1607.03075). As a\nnecessary ingredient for the proof we derive the continuous-variable analogue\nof the Pauli Twirl.", "comment": "15 pages", "pdf_url": "http://arxiv.org/pdf/2507.00095v1", "categories": ["quant-ph", "cs.CR", "E.3"], "cate": "quant-ph", "url": "http://arxiv.org/abs/2507.00095v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00057", "title": "Estimating Correctness Without Oracles in LLM-Based Code Generation", "authors": ["Thomas Valentin", "Ardi Madadi", "Gaetano Sapia", "Marcel Böhme"], "summary": "Generating code from natural language specifications is one of the most\nsuccessful applications of Large Language Models (LLMs). Yet, they hallucinate:\nLLMs produce outputs that may be grammatically correct but are factually\nincorrect. Without an existing, correct implementation (i.e., an oracle), can\nwe quantify how likely the generated program is correct?\n  In this paper, we propose a measure of incorrectness, called incoherence,\nthat can be estimated efficiently in the absence of an oracle and provides a\nlower bound on the error, i.e., the probability that the LLM-generated program\nfor that specification is incorrect. Our experiments demonstrate an\nextraordinary effectiveness. For the average code generation task, our\nincoherence-based methodology can automatically identify about two-thirds of\nincorrect programs without reports of false positives. In fact, an oracle-based\nevaluation of LLMs can be reliably replaced by an incoherence-based evaluation.\nIn particular, we find a very strong agreement between the ranking of LLMs by\nthe number of programs deemed correct via an oracle (pass@1) and the ranking of\nLLMs by the number of programs deemed correct via our incoherence.", "comment": "8 pages + refs and appendix", "pdf_url": "http://arxiv.org/pdf/2507.00057v1", "categories": ["cs.PL", "cs.AI", "cs.LG", "cs.SE"], "cate": "cs.PL", "url": "http://arxiv.org/abs/2507.00057v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2507.00635", "title": "Stable Tracking of Eye Gaze Direction During Ophthalmic Surgery", "authors": ["Tinghe Hong", "Shenlin Cai", "Boyang Li", "Kai Huang"], "summary": "Ophthalmic surgical robots offer superior stability and precision by reducing\nthe natural hand tremors of human surgeons, enabling delicate operations in\nconfined surgical spaces. Despite the advancements in developing vision- and\nforce-based control methods for surgical robots, preoperative navigation\nremains heavily reliant on manual operation, limiting the consistency and\nincreasing the uncertainty. Existing eye gaze estimation techniques in the\nsurgery, whether traditional or deep learning-based, face challenges including\ndependence on additional sensors, occlusion issues in surgical environments,\nand the requirement for facial detection. To address these limitations, this\nstudy proposes an innovative eye localization and tracking method that combines\nmachine learning with traditional algorithms, eliminating the requirements of\nlandmarks and maintaining stable iris detection and gaze estimation under\nvarying lighting and shadow conditions. Extensive real-world experiment results\nshow that our proposed method has an average estimation error of 0.58 degrees\nfor eye orientation estimation and 2.08-degree average control error for the\nrobotic arm's movement based on the calculated orientation.", "comment": "Accepted by ICRA 2025", "pdf_url": "http://arxiv.org/pdf/2507.00635v1", "categories": ["cs.RO", "cs.CV", "cs.HC"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2507.00635v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00569", "title": "Linear rank-metric intersecting codes", "authors": ["Daniele Bartoli", "Martino Borello", "Giuseppe Marino", "Martin Scotti"], "summary": "In this paper we introduce and investigate rank-metric intersecting codes, a\nnew class of linear codes in the rank-metric context, inspired by the\nwell-studied notion of intersecting codes in the Hamming metric. A rank-metric\ncode is said to be intersecting if any two nonzero codewords have supports\nintersecting non trivially. We explore this class from both a coding-theoretic\nand geometric perspective, highlighting its relationship with minimal codes,\nMRD codes, and Hamming-metric intersecting codes. We derive structural\nproperties, sufficient conditions based on minimum distance, and geometric\ncharacterizations in terms of 2-spannable $q$-systems. We establish upper and\nlower bounds on code parameters and show some constructions, which leave a\nrange of unexplored parameters. Finally, we connect rank-intersecting codes to\nother combinatorial structures such as $(2,1)$-separating systems and\nframeproof codes.", "comment": "17 pages, 1 figure", "pdf_url": "http://arxiv.org/pdf/2507.00569v1", "categories": ["math.CO", "cs.IT", "math.IT"], "cate": "math.CO", "url": "http://arxiv.org/abs/2507.00569v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00557", "title": "Advancing Local Search in SMT-NRA with MCSAT Integration", "authors": ["Tianyi Ding", "Haokun Li", "Xinpeng Ni", "Bican Xia", "Tianqi Zhao"], "summary": "In this paper, we advance local search for Satisfiability Modulo the Theory\nof Nonlinear Real Arithmetic (SMT-NRA for short). First, we introduce a\ntwo-dimensional cell-jump move, called \\emph{$2d$-cell-jump}, generalizing the\nkey operation, cell-jump, of the local search method for SMT-NRA. Then, we\npropose an extended local search framework, named \\emph{$2d$-LS} (following the\nlocal search framework, LS, for SMT-NRA), integrating the model constructing\nsatisfiability calculus (MCSAT) framework to improve search efficiency. To\nfurther improve the efficiency of MCSAT, we implement a recently proposed\ntechnique called \\emph{sample-cell projection operator} for MCSAT, which is\nwell suited for CDCL-style search in the real domain and helps guide the search\naway from conflicting states. Finally, we design a hybrid framework for SMT-NRA\ncombining MCSAT, $2d$-LS and OpenCAD, to improve search efficiency through\ninformation exchange. The experimental results demonstrate improvements in\nlocal search performance, highlighting the effectiveness of the proposed\nmethods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00557v1", "categories": ["cs.AI", "cs.LO", "cs.SC"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2507.00557v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00024", "title": "AIMatDesign: Knowledge-Augmented Reinforcement Learning for Inverse Materials Design under Data Scarcity", "authors": ["Yeyong Yu", "Xilei Bian", "Jie Xiong", "Xing Wu", "Quan Qian"], "summary": "With the growing demand for novel materials, machine learning-driven inverse\ndesign methods face significant challenges in reconciling the high-dimensional\nmaterials composition space with limited experimental data. Existing approaches\nsuffer from two major limitations: (I) machine learning models often lack\nreliability in high-dimensional spaces, leading to prediction biases during the\ndesign process; (II) these models fail to effectively incorporate domain expert\nknowledge, limiting their capacity to support knowledge-guided inverse design.\nTo address these challenges, we introduce AIMatDesign, a reinforcement learning\nframework that addresses these limitations by augmenting experimental data\nusing difference-based algorithms to build a trusted experience pool,\naccelerating model convergence. To enhance model reliability, an automated\nrefinement strategy guided by large language models (LLMs) dynamically corrects\nprediction inconsistencies, reinforcing alignment between reward signals and\nstate value functions. Additionally, a knowledge-based reward function\nleverages expert domain rules to improve stability and efficiency during\ntraining. Our experiments demonstrate that AIMatDesign significantly surpasses\ntraditional machine learning and reinforcement learning methods in discovery\nefficiency, convergence speed, and success rates. Among the numerous candidates\nproposed by AIMatDesign, experimental synthesis of representative Zr-based\nalloys yielded a top-performing BMG with 1.7GPa yield strength and 10.2\\%\nelongation, closely matching predictions. Moreover, the framework accurately\ncaptured the trend of yield strength variation with composition, demonstrating\nits reliability and potential for closed-loop materials discovery.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00024v1", "categories": ["cs.LG", "cond-mat.mtrl-sci", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00024v1", "date": "2025-06-17", "updated": "2025-06-17"}
{"id": "2507.00423", "title": "Find a Scapegoat: Poisoning Membership Inference Attack and Defense to Federated Learning", "authors": ["Wenjin Mo", "Zhiyuan Li", "Minghong Fang", "Mingwei Fang"], "summary": "Federated learning (FL) allows multiple clients to collaboratively train a\nglobal machine learning model with coordination from a central server, without\nneeding to share their raw data. This approach is particularly appealing in the\nera of privacy regulations like the GDPR, leading many prominent companies to\nadopt it. However, FL's distributed nature makes it susceptible to poisoning\nattacks, where malicious clients, controlled by an attacker, send harmful data\nto compromise the model. Most existing poisoning attacks in FL aim to degrade\nthe model's integrity, such as reducing its accuracy, with limited attention to\nprivacy concerns from these attacks. In this study, we introduce FedPoisonMIA,\na novel poisoning membership inference attack targeting FL. FedPoisonMIA\ninvolves malicious clients crafting local model updates to infer membership\ninformation. Additionally, we propose a robust defense mechanism to mitigate\nthe impact of FedPoisonMIA attacks. Extensive experiments across various\ndatasets demonstrate the attack's effectiveness, while our defense approach\nreduces its impact to a degree.", "comment": "To appear in ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.00423v1", "categories": ["cs.CR", "cs.DC", "cs.LG"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2507.00423v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00182", "title": "Graph-Based Deep Learning for Component Segmentation of Maize Plants", "authors": ["J. I. Ruíz", "A. Méndez", "E. Rodríguez"], "summary": "In precision agriculture, one of the most important tasks when exploring crop\nproduction is identifying individual plant components. There are several\nattempts to accomplish this task by the use of traditional 2D imaging, 3D\nreconstructions, and Convolutional Neural Networks (CNN). However, they have\nseveral drawbacks when processing 3D data and identifying individual plant\ncomponents. Therefore, in this work, we propose a novel Deep Learning\narchitecture to detect components of individual plants on Light Detection and\nRanging (LiDAR) 3D Point Cloud (PC) data sets. This architecture is based on\nthe concept of Graph Neural Networks (GNN), and feature enhancing with\nPrincipal Component Analysis (PCA). For this, each point is taken as a vertex\nand by the use of a K-Nearest Neighbors (KNN) layer, the edges are established,\nthus representing the 3D PC data set. Subsequently, Edge-Conv layers are used\nto further increase the features of each point. Finally, Graph Attention\nNetworks (GAT) are applied to classify visible phenotypic components of the\nplant, such as the leaf, stem, and soil. This study demonstrates that our\ngraph-based deep learning approach enhances segmentation accuracy for\nidentifying individual plant components, achieving percentages above 80% in the\nIoU average, thus outperforming other existing models based on point clouds.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00182v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00182v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00780", "title": "Research on Improving the High Precision and Lightweight Diabetic Retinopathy Detection of YOLOv8n", "authors": ["Fei Yuhuan", "Sun Xufei", "Zang Ran", "Wang Gengchen", "Su Meng", "Liu Fenghao"], "summary": "Early detection and diagnosis of diabetic retinopathy is one of the current\nresearch focuses in ophthalmology. However, due to the subtle features of\nmicro-lesions and their susceptibility to background interference, ex-isting\ndetection methods still face many challenges in terms of accuracy and\nrobustness. To address these issues, a lightweight and high-precision detection\nmodel based on the improved YOLOv8n, named YOLO-KFG, is proposed. Firstly, a\nnew dynamic convolution KWConv and C2f-KW module are designed to improve the\nbackbone network, enhancing the model's ability to perceive micro-lesions.\nSecondly, a fea-ture-focused diffusion pyramid network FDPN is designed to\nfully integrate multi-scale context information, further improving the model's\nability to perceive micro-lesions. Finally, a lightweight shared detection head\nGSDHead is designed to reduce the model's parameter count, making it more\ndeployable on re-source-constrained devices. Experimental results show that\ncompared with the base model YOLOv8n, the improved model reduces the parameter\ncount by 20.7%, increases mAP@0.5 by 4.1%, and improves the recall rate by\n7.9%. Compared with single-stage mainstream algorithms such as YOLOv5n and\nYOLOv10n, YOLO-KFG demonstrates significant advantages in both detection\naccuracy and efficiency.", "comment": "in Chinese language", "pdf_url": "http://arxiv.org/pdf/2507.00780v1", "categories": ["eess.IV", "cs.CV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2507.00780v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00821", "title": "Sensemaking Through Making: Developing Clinical Domain Knowledge by Crafting Synthetic Datasets and Prototyping System Architectures", "authors": ["Mihnea Stefan Calota", "Wessel Nieuwenhuys", "Janet Yi-Ching Huang", "Lin-Lin Chen", "Mathias Funk"], "summary": "Designers have ample opportunities to impact the healthcare domain. However,\nhospitals are often closed ecosystems that pose challenges in engaging clinical\nstakeholders, developing domain knowledge, and accessing relevant systems and\ndata. In this paper, we introduce a making-oriented approach to help designers\nunderstand the intricacies of their target healthcare context. Using Remote\nPatient Monitoring (RPM) as a case study, we explore how manually crafting\nsynthetic datasets based on real-world observations enables designers to learn\nabout complex data-driven healthcare systems. Our process involves observing\nand modeling the real-world RPM context, crafting synthetic datasets, and\niteratively prototyping a simplified RPM system that balances contextual\nrichness and intentional abstraction. Through this iterative process of\nsensemaking through making, designers can still develop context familiarity\nwhen direct access to the actual healthcare system is limited. Our approach\nemphasizes the value of hands-on interaction with data structures to support\ndesigners in understanding opaque healthcare systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00821v1", "categories": ["cs.HC"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2507.00821v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00062", "title": "Enhancing Car-Following Models with Bike Dynamics for Improved Traffic Simulation", "authors": ["Nico Ostendorf", "Keno Garlichs", "Lars Wolf"], "summary": "Road traffic simulations are crucial for establishing safe and efficient\ntraffic environments. They are used to test various road applications before\nreal-world implementation. SUMO is a well-known simulator for road networks and\nintermodal traffic, often used in conjunction with other tools to test various\ntypes of applications. Realistic simulations require accurate movement models\nfor different road users, such as cars, bicycles, and buses. While realistic\nmodels are already implemented for most vehicle types, bicycles, which are\nessential for achieving safe and efficient traffic, can only be modeled as slow\nvehicles or fast pedestrians at present. This paper introduces the Realistic\nBicycle Dynamics Model (RBDM), the first dedicated bicycle model for SUMO,\naddressing this significant gap. Leveraging real-world bicycle data from the\nSimRa dataset, the RBDM implements realistic speed, acceleration, and\ndeceleration behaviors of bicycles in urban scenarios. The evaluation is\nconducted using the Monaco SUMO traffic scenario and a newly generated Berlin\nscenario in SUMO. The RBDM significantly outperforms the existing slow-vehicle\napproximation in SUMO, aligning more closely with real-world data. These\nresults underscore the necessity of a realistic bicycle movement model for\naccurate simulations, given the significant differences in the movement\nprofiles of bicycles, cars, and pedestrians. Furthermore, the model is tested\nfor its ability to generalize to disparate scenarios and urban topologies,\nwhich is dependent on the manner and geographical region in which the SimRa\ndata were gathered. In addition, recommendations are provided for how it could\nbe adapted for use in different city topologies.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00062v1", "categories": ["physics.soc-ph", "cs.SY", "eess.SY"], "cate": "physics.soc-ph", "url": "http://arxiv.org/abs/2507.00062v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2507.00145", "title": "AI-Hybrid TRNG: Kernel-Based Deep Learning for Near-Uniform Entropy Harvesting from Physical Noise", "authors": ["Hasan Yiğit"], "summary": "AI-Hybrid TRNG is a deep-learning framework that extracts near-uniform\nentropy directly from physical noise, eliminating the need for bulky quantum\ndevices or expensive laboratory-grade RF receivers. Instead, it relies on a\nlow-cost, thumb-sized RF front end, plus CPU-timing jitter, for training, and\nthen emits 32-bit high-entropy streams without any quantization step.\n  Unlike deterministic or trained artificial intelligence random number\ngenerators (RNGs), our dynamic inner-outer network couples adaptive natural\nsources and reseeding, yielding truly unpredictable and autonomous sequences.\nGenerated numbers pass the NIST SP 800-22 battery better than a CPU-based\nmethod. It also passes nineteen bespoke statistical tests for both bit- and\ninteger-level analysis. All results satisfy cryptographic standards, while\nforward and backward prediction experiments reveal no exploitable biases. The\nmodel's footprint is below 0.5 MB, making it deployable on MCUs and FPGA soft\ncores, as well as suitable for other resource-constrained platforms.\n  By detaching randomness quality from dedicated hardware, AI-Hybrid TRNG\nbroadens the reach of high-integrity random number generators across secure\nsystems, cryptographic protocols, embedded and edge devices, stochastic\nsimulations, and server applications that need randomness.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00145v1", "categories": ["cs.CR", "cs.AI", "cs.ET", "cs.IT", "eess.SP", "math.IT"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2507.00145v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00693", "title": "Leveraging Large Language Models for Spontaneous Speech-Based Suicide Risk Detection", "authors": ["Yifan Gao", "Jiao Fu", "Long Guo", "Hong Liu"], "summary": "Early identification of suicide risk is crucial for preventing suicidal\nbehaviors. As a result, the identification and study of patterns and markers\nrelated to suicide risk have become a key focus of current research. In this\npaper, we present the results of our work in the 1st SpeechWellness Challenge\n(SW1), which aims to explore speech as a non-invasive and easily accessible\nmental health indicator for identifying adolescents at risk of suicide.Our\napproach leverages large language model (LLM) as the primary tool for feature\nextraction, alongside conventional acoustic and semantic features. The proposed\nmethod achieves an accuracy of 74\\% on the test set, ranking first in the SW1\nchallenge. These findings demonstrate the potential of LLM-based methods for\nanalyzing speech in the context of suicide risk assessment.", "comment": "Accepted to Interspeech 2025", "pdf_url": "http://arxiv.org/pdf/2507.00693v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "cate": "cs.SD", "url": "http://arxiv.org/abs/2507.00693v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00322", "title": "Failure by Interference: Language Models Make Balanced Parentheses Errors When Faulty Mechanisms Overshadow Sound Ones", "authors": ["Daking Rai", "Samuel Miller", "Kevin Moran", "Ziyu Yao"], "summary": "Despite remarkable advances in coding capabilities, language models (LMs)\nstill struggle with simple syntactic tasks such as generating balanced\nparentheses. In this study, we investigate the underlying mechanisms behind the\npersistence of these errors across LMs of varying sizes (124M-7B) to both\nunderstand and mitigate the errors. Our study reveals that LMs rely on a number\nof components (attention heads and FF neurons) that independently make their\nown predictions. While some components reliably promote correct answers across\na generalized range of inputs (i.e., implementing \"sound mechanisms''), others\nare less reliable and introduce noise by promoting incorrect tokens (i.e.,\nimplementing \"faulty mechanisms''). Errors occur when the faulty mechanisms\novershadow the sound ones and dominantly affect the predictions. Motivated by\nthis insight, we introduce RASteer, a steering method to systematically\nidentify and increase the contribution of reliable components for improving\nmodel performance. RASteer substantially improves performance on balanced\nparentheses tasks, boosting accuracy of some models from $0$% to around $100$%\nwithout impairing the models' general coding ability. We further demonstrate\nits broader applicability in arithmetic reasoning tasks, achieving performance\ngains of up to around $20$%.", "comment": "23 pages, 10 figures, Preprint", "pdf_url": "http://arxiv.org/pdf/2507.00322v1", "categories": ["cs.CL", "cs.AI", "cs.SE", "I.2.7"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2507.00322v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00330", "title": "Modeling Data Diversity for Joint Instance and Verbalizer Selection in Cold-Start Scenarios", "authors": ["Mohna Chakraborty", "Adithya Kulkarni", "Qi Li"], "summary": "Prompt-based methods leverage the knowledge of pre-trained language models\n(PLMs) trained with a masked language modeling (MLM) objective; however, these\nmethods are sensitive to template, verbalizer, and few-shot instance selection,\nparticularly in cold-start settings with no labeled data. Existing studies\noverlook the dependency between instances and verbalizers, where instance-label\nprobabilities depend on verbalizer token proximity in the embedding space. To\naddress this, we propose COLDSELECT, a joint verbalizer and instance selection\napproach that models data diversity. COLDSELECT maps PLM vocabulary and\n$h_{[MASK]}$ embeddings into a shared space, applying dimensionality reduction\nand clustering to ensure efficient and diverse selection. By optimizing for\nminimal uncertainty and maximal diversity, COLDSELECT captures data\nrelationships effectively. Experiments on eight benchmarks demonstrate\nCOLDSELECT's superiority in reducing uncertainty and enhancing generalization,\noutperforming baselines in verbalizer and few-shot instance selection for\ncold-start scenarios.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00330v1", "categories": ["cs.CL", "cs.IR"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2507.00330v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00573", "title": "High order global flux schemes for general steady state preservation of shallow water moment equations with non-conservative products", "authors": ["Mirco Ciallella", "Julian Koellermeier"], "summary": "Shallow water moment equations are reduced-order models for free-surface\nflows that allow to represent vertical variations of the velocity profile at\nthe expense of additional evolution equations for a number of additional\nvariables, so called moments. This introduces non-linear non-conservative\nproducts in the system, which make the analytical characterization of steady\nstates much harder if not impossible. The lack of analytical steady states\nposes a challenge for the design of well-balanced schemes, which aim at\npreserving such steady states as crucial in many applications. In this work, we\npresent a family of fully well-balanced, high-order WENO finite volume methods\nfor general hyperbolic balance laws with non-conservative products like the\nshallow water moment equations, for which no analytical steady states are\navailable. The schemes are based on the flux globalization approach, in which\nboth source terms and non-conservative products are integrated with a tailored\nhigh order quadrature in the divergence term. The resulting global flux is then\nreconstructed instead of the conservative variables to preserve all steady\nstates. Numerical tests show the optimal convergence of the method and a\nsignificant error reduction for steady state solutions. Furthermore, we provide\na numerical comparison of perturbed steady states for different families of\nshallow water moment equations, which illustrates the flexibility of our method\nthat is valid for general equations without prior knowledge of steady states.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00573v1", "categories": ["math.NA", "cs.NA"], "cate": "math.NA", "url": "http://arxiv.org/abs/2507.00573v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00458", "title": "Mitigating Language Mismatch in SSL-Based Speaker Anonymization", "authors": ["Zhe Zhang", "Wen-Chin Huang", "Xin Wang", "Xiaoxiao Miao", "Junichi Yamagishi"], "summary": "Speaker anonymization aims to protect speaker identity while preserving\ncontent information and the intelligibility of speech. However, most speaker\nanonymization systems (SASs) are developed and evaluated using only English,\nresulting in degraded utility for other languages. This paper investigates\nlanguage mismatch in SASs for Japanese and Mandarin speech. First, we fine-tune\na self-supervised learning (SSL)-based content encoder with Japanese speech to\nverify effective language adaptation. Then, we propose fine-tuning a\nmultilingual SSL model with Japanese speech and evaluating the SAS in Japanese\nand Mandarin. Downstream experiments show that fine-tuning an English-only SSL\nmodel with the target language enhances intelligibility while maintaining\nprivacy and that multilingual SSL further extends SASs' utility across\ndifferent languages. These findings highlight the importance of language\nadaptation and multilingual pre-training of SSLs for robust multilingual\nspeaker anonymization.", "comment": "Accepted to Interspeech 2025", "pdf_url": "http://arxiv.org/pdf/2507.00458v1", "categories": ["eess.AS", "cs.SD"], "cate": "eess.AS", "url": "http://arxiv.org/abs/2507.00458v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00230", "title": "PPFL-RDSN: Privacy-Preserving Federated Learning-based Residual Dense Spatial Networks for Encrypted Lossy Image Reconstruction", "authors": ["Peilin He", "James Joshi"], "summary": "Reconstructing high-quality images from low-resolution inputs using Residual\nDense Spatial Networks (RDSNs) is crucial yet challenging, particularly in\ncollaborative scenarios where centralized training poses significant privacy\nrisks, including data leakage and inference attacks, as well as high\ncomputational costs. We propose a novel Privacy-Preserving Federated\nLearning-based RDSN (PPFL-RDSN) framework specifically tailored for lossy image\nreconstruction. PPFL-RDSN integrates Federated Learning (FL), local\ndifferential privacy, and robust model watermarking techniques, ensuring data\nremains secure on local devices, safeguarding sensitive information, and\nmaintaining model authenticity without revealing underlying data. Empirical\nevaluations show that PPFL-RDSN achieves comparable performance to the\nstate-of-the-art centralized methods while reducing computational burdens, and\neffectively mitigates security and privacy vulnerabilities, making it a\npractical solution for secure and privacy-preserving collaborative computer\nvision applications.", "comment": "This paper is under review; do not distribute", "pdf_url": "http://arxiv.org/pdf/2507.00230v1", "categories": ["cs.LG", "cs.CR"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00230v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00264", "title": "Rust vs. C for Python Libraries: Evaluating Rust-Compatible Bindings Toolchains", "authors": ["Isabella Basso do Amaral", "Renato Cordeiro Ferreira", "Alfredo Goldman"], "summary": "The Python programming language is best known for its syntax and scientific\nlibraries, but it is also notorious for its slow interpreter. Optimizing\ncritical sections in Python entails special knowledge of the binary\ninteractions between programming languages, and can be cumbersome to interface\nmanually, with implementers often resorting to convoluted third-party\nlibraries. This comparative study evaluates the performance and ease of use of\nthe PyO3 Python bindings toolchain for Rust against ctypes and cffi. By using\nRust tooling developed for Python, we can achieve state-of-the-art performance\nwith no concern for API compatibility.", "comment": "10 pages, 27 figures (1 diagram, 4 graphs, 9 tables, 13 code\n  listings), submitted to SBAC-PAD 2025", "pdf_url": "http://arxiv.org/pdf/2507.00264v1", "categories": ["cs.PL", "cs.DC", "cs.PF", "cs.SE", "D.2.13; D.2.8; D.3.3; B.8"], "cate": "cs.PL", "url": "http://arxiv.org/abs/2507.00264v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00644", "title": "Parallel Transmission Aware Co-Design: Enhancing Manipulator Performance Through Actuation-Space Optimization", "authors": ["Rohit Kumar", "Melya Boukheddimi", "Dennis Mronga", "Shivesh Kumar", "Frank Kirchner"], "summary": "In robotics, structural design and behavior optimization have long been\nconsidered separate processes, resulting in the development of systems with\nlimited capabilities. Recently, co-design methods have gained popularity, where\nbi-level formulations are used to simultaneously optimize the robot design and\nbehavior for specific tasks. However, most implementations assume a serial or\ntree-type model of the robot, overlooking the fact that many robot platforms\nincorporate parallel mechanisms. In this paper, we present a novel co-design\napproach that explicitly incorporates parallel coupling constraints into the\ndynamic model of the robot. In this framework, an outer optimization loop\nfocuses on the design parameters, in our case the transmission ratios of a\nparallel belt-driven manipulator, which map the desired torques from the joint\nspace to the actuation space. An inner loop performs trajectory optimization in\nthe actuation space, thus exploiting the entire dynamic range of the\nmanipulator. We compare the proposed method with a conventional co-design\napproach based on a simplified tree-type model. By taking advantage of the\nactuation space representation, our approach leads to a significant increase in\ndynamic payload capacity compared to the conventional co-design implementation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00644v1", "categories": ["cs.RO"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2507.00644v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00726", "title": "Can Large Language Models Develop Strategic Reasoning? Post-training Insights from Learning Chess", "authors": ["Dongyoon Hwang", "Hojoon Lee", "Jaegul Choo", "Dongmin Park", "Jongho Park"], "summary": "While reinforcement learning (RL) for large language models (LLMs) has shown\npromise in mathematical reasoning, strategic reasoning for LLMs using RL\nremains largely unexplored. We investigate whether LLMs can develop strategic\nreasoning capabilities through RL in chess. To this end, we leverage a\nchess-pretrained action-value network to provide dense reward on the LLM's\noutput move quality, which can be seen as a form of knowledge distillation. Our\nexperiments show that our distillation-based dense rewards often outperform\nsparse binary rewards. However, surprisingly, all models plateau far below\nexpert levels. We provide SFT and RL ablations on chess reasoning training and\nfind evidence that this limitation stems from a deficit in the pretrained\nmodels' internal understanding of chess--a deficit which RL alone may not be\nable to fully overcome.", "comment": "27 pages", "pdf_url": "http://arxiv.org/pdf/2507.00726v1", "categories": ["cs.AI", "cs.LG"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2507.00726v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00025", "title": "Generalizing to New Dynamical Systems via Frequency Domain Adaptation", "authors": ["Tiexin Qin", "Hong Yan", "Haoliang Li"], "summary": "Learning the underlying dynamics from data with deep neural networks has\nshown remarkable potential in modeling various complex physical dynamics.\nHowever, current approaches are constrained in their ability to make reliable\npredictions in a specific domain and struggle with generalizing to unseen\nsystems that are governed by the same general dynamics but differ in\nenvironmental characteristics. In this work, we formulate a parameter-efficient\nmethod, Fourier Neural Simulator for Dynamical Adaptation (FNSDA), that can\nreadily generalize to new dynamics via adaptation in the Fourier space.\nSpecifically, FNSDA identifies the shareable dynamics based on the known\nenvironments using an automatic partition in Fourier modes and learns to adjust\nthe modes specific for each new environment by conditioning on low-dimensional\nlatent systematic parameters for efficient generalization. We evaluate our\napproach on four representative families of dynamic systems, and the results\nshow that FNSDA can achieve superior or competitive generalization performance\ncompared to existing methods with a significantly reduced parameter cost. Our\ncode is available at https://github.com/WonderSeven/FNSDA.", "comment": "Accepted by TPAMI 2025", "pdf_url": "http://arxiv.org/pdf/2507.00025v1", "categories": ["cs.LG", "cs.AI", "stat.ML"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00025v1", "date": "2025-06-17", "updated": "2025-06-17"}
{"id": "2507.00523", "title": "Edge Computing and its Application in Robotics: A Survey", "authors": ["Nazish Tahir", "Ramviyas Parasuraman"], "summary": "The Edge computing paradigm has gained prominence in both academic and\nindustry circles in recent years. By implementing edge computing facilities and\nservices in robotics, it becomes a key enabler in the deployment of artificial\nintelligence applications to robots. Time-sensitive robotics applications\nbenefit from the reduced latency, mobility, and location awareness provided by\nthe edge computing paradigm, which enables real-time data processing and\nintelligence at the network's edge. While the advantages of integrating edge\ncomputing into robotics are numerous, there has been no recent survey that\ncomprehensively examines these benefits. This paper aims to bridge that gap by\nhighlighting important work in the domain of edge robotics, examining recent\nadvancements, and offering deeper insight into the challenges and motivations\nbehind both current and emerging solutions. In particular, this article\nprovides a comprehensive evaluation of recent developments in edge robotics,\nwith an emphasis on fundamental applications, providing in-depth analysis of\nthe key motivations, challenges, and future directions in this rapidly evolving\ndomain. It also explores the importance of edge computing in real-world\nrobotics scenarios where rapid response times are critical. Finally, the paper\noutlines various open research challenges in the field of edge robotics.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00523v1", "categories": ["cs.RO", "cs.DC", "cs.NI"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2507.00523v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00224", "title": "Computer Vision for Objects used in Group Work: Challenges and Opportunities", "authors": ["Changsoo Jung", "Sheikh Mannan", "Jack Fitzgerald", "Nathaniel Blanchard"], "summary": "Interactive and spatially aware technologies are transforming educational\nframeworks, particularly in K-12 settings where hands-on exploration fosters\ndeeper conceptual understanding. However, during collaborative tasks, existing\nsystems often lack the ability to accurately capture real-world interactions\nbetween students and physical objects. This issue could be addressed with\nautomatic 6D pose estimation, i.e., estimation of an object's position and\norientation in 3D space from RGB images or videos. For collaborative groups\nthat interact with physical objects, 6D pose estimates allow AI systems to\nrelate objects and entities. As part of this work, we introduce FiboSB, a novel\nand challenging 6D pose video dataset featuring groups of three participants\nsolving an interactive task featuring small hand-held cubes and a weight scale.\nThis setup poses unique challenges for 6D pose because groups are holistically\nrecorded from a distance in order to capture all participants -- this, coupled\nwith the small size of the cubes, makes 6D pose estimation inherently\nnon-trivial. We evaluated four state-of-the-art 6D pose estimation methods on\nFiboSB, exposing the limitations of current algorithms on collaborative group\nwork. An error analysis of these methods reveals that the 6D pose methods'\nobject detection modules fail. We address this by fine-tuning YOLO11-x for\nFiboSB, achieving an overall mAP_50 of 0.898. The dataset, benchmark results,\nand analysis of YOLO11-x errors presented here lay the groundwork for\nleveraging the estimation of 6D poses in difficult collaborative contexts.", "comment": "Accepted to AIED 2025 Late Breaking Results Track", "pdf_url": "http://arxiv.org/pdf/2507.00224v1", "categories": ["cs.CV", "cs.HC"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00224v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00831", "title": "Adiabatic Capacitive Neuron: An Energy-Efficient Functional Unit for Artificial Neural Networks", "authors": ["Sachin Maheshwari", "Mike Smart", "Himadri Singh Raghav", "Themis Prodromakis", "Alexander Serb"], "summary": "This paper introduces a new, highly energy-efficient, Adiabatic Capacitive\nNeuron (ACN) hardware implementation of an Artificial Neuron (AN) with improved\nfunctionality, accuracy, robustness and scalability over previous work. The\npaper describes the implementation of a \\mbox{12-bit} single neuron, with\npositive and negative weight support, in an $\\mathbf{0.18\\mu m}$ CMOS\ntechnology. The paper also presents a new Threshold Logic (TL) design for a\nbinary AN activation function that generates a low symmetrical offset across\nthree process corners and five temperatures between $-55^o$C and $125^o$C.\nPost-layout simulations demonstrate a maximum rising and falling offset voltage\nof 9$mV$ compared to conventional TL, which has rising and falling offset\nvoltages of 27$mV$ and 5$mV$ respectively, across temperature and process.\nMoreover, the proposed TL design shows a decrease in average energy of 1.5$\\%$\nat the SS corner and 2.3$\\%$ at FF corner compared to the conventional TL\ndesign. The total synapse energy saving for the proposed ACN was above 90$\\%$\n(over 12x improvement) when compared to a non-adiabatic CMOS Capacitive Neuron\n(CCN) benchmark for a frequency ranging from 500$kHz$ to 100$MHz$. A\n1000-sample Monte Carlo simulation including process variation and mismatch\nconfirms the worst-case energy savings of $\\>$90$\\%$ compared to CCN in the\nsynapse energy profile. Finally, the impact of supply voltage scaling shows\nconsistent energy savings of above 90$\\%$ (except all zero inputs) without loss\nof functionality.", "comment": "12 pages, 18 figures, 7 tables. This work has been submitted to the\n  IEEE for possible publication", "pdf_url": "http://arxiv.org/pdf/2507.00831v1", "categories": ["eess.IV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2507.00831v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00881", "title": "Towards Difficulty-Aware Analysis of Deep Neural Networks", "authors": ["Linhao Meng", "Stef van den Elzen", "Anna Vilanova"], "summary": "Traditional instance-based model analysis focuses mainly on misclassified\ninstances. However, this approach overlooks the varying difficulty associated\nwith different instances. Ideally, a robust model should recognize and reflect\nthe challenges presented by intrinsically difficult instances. It is also\nvaluable to investigate whether the difficulty perceived by the model aligns\nwith that perceived by humans. To address this, we propose incorporating\ninstance difficulty into the deep neural network evaluation process,\nspecifically for supervised classification tasks on image data. Specifically,\nwe consider difficulty measures from three perspectives -- data, model, and\nhuman -- to facilitate comprehensive evaluation and comparison. Additionally,\nwe develop an interactive visual tool, DifficultyEyes, to support the\nidentification of instances of interest based on various difficulty patterns\nand to aid in analyzing potential data or model issues. Case studies\ndemonstrate the effectiveness of our approach.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00881v1", "categories": ["cs.HC"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2507.00881v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00076", "title": "Time Invariant Sensor Tasking for Catalog Maintenance of LEO Space objects using Stochastic Geometry", "authors": ["Partha Chowdhury", "Harsha M", "Chinni Prabhunath Georg", "Arun Balaji Buduru", "Sanat K Biswas"], "summary": "Catalog maintenance of space objects by limited number of ground-based\nsensors presents a formidable challenging task to the space community. This\narticle presents a methodology for time-invariant tracking and surveillance of\nspace objects in low Earth orbit (LEO) by optimally directing ground sensors.\nOur methodology aims to maximize the expected number of space objects from a\nset of ground stations by utilizing concepts from stochastic geometry,\nparticularly the Poisson point process. We have provided a systematic framework\nto understand visibility patterns and enhance the efficiency of tracking\nmultiple objects simultaneously. Our approach contributes to more informed\ndecision-making in space operations, ultimately supporting efforts to maintain\nsafety and sustainability in LEO.", "comment": "This work has been accepted and presented at the 35th AAS/AIAA Space\n  Flight Mechanics Meeting, 2025, Kaua'i, Hawai", "pdf_url": "http://arxiv.org/pdf/2507.00076v1", "categories": ["astro-ph.IM", "cs.RO", "cs.SY", "eess.SY", "math-ph", "math.MP"], "cate": "astro-ph.IM", "url": "http://arxiv.org/abs/2507.00076v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2507.00155", "title": "Do Music Source Separation Models Preserve Spatial Information in Binaural Audio?", "authors": ["Richa Namballa", "Agnieszka Roginska", "Magdalena Fuentes"], "summary": "Binaural audio remains underexplored within the music information retrieval\ncommunity. Motivated by the rising popularity of virtual and augmented reality\nexperiences as well as potential applications to accessibility, we investigate\nhow well existing music source separation (MSS) models perform on binaural\naudio. Although these models process two-channel inputs, it is unclear how\neffectively they retain spatial information. In this work, we evaluate how\nseveral popular MSS models preserve spatial information on both standard stereo\nand novel binaural datasets. Our binaural data is synthesized using stems from\nMUSDB18-HQ and open-source head-related transfer functions by positioning\ninstrument sources randomly along the horizontal plane. We then assess the\nspatial quality of the separated stems using signal processing and interaural\ncue-based metrics. Our results show that stereo MSS models fail to preserve the\nspatial information critical for maintaining the immersive quality of binaural\naudio, and that the degradation depends on model architecture as well as the\ntarget instrument. Finally, we highlight valuable opportunities for future work\nat the intersection of MSS and immersive audio.", "comment": "6 pages + references, 4 figures, 2 tables, 26th International Society\n  for Music Information Retrieval (ISMIR) Conference", "pdf_url": "http://arxiv.org/pdf/2507.00155v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "cate": "eess.AS", "url": "http://arxiv.org/abs/2507.00155v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00808", "title": "Multi-interaction TTS toward professional recording reproduction", "authors": ["Hiroki Kanagawa", "Kenichi Fujita", "Aya Watanabe", "Yusuke Ijima"], "summary": "Voice directors often iteratively refine voice actors' performances by\nproviding feedback to achieve the desired outcome. While this iterative\nfeedback-based refinement process is important in actual recordings, it has\nbeen overlooked in text-to-speech synthesis (TTS). As a result, fine-grained\nstyle refinement after the initial synthesis is not possible, even though the\nsynthesized speech often deviates from the user's intended style. To address\nthis issue, we propose a TTS method with multi-step interaction that allows\nusers to intuitively and rapidly refine synthetized speech. Our approach models\nthe interaction between the TTS model and its user to emulate the relationship\nbetween voice actors and voice directors. Experiments show that the proposed\nmodel with its corresponding dataset enable iterative style refinements in\naccordance with users' directions, thus demonstrating its multi-interaction\ncapability. Sample audios are available: https://ntt-hilab-gensp.\ngithub.io/ssw13multiinteraction_tts/", "comment": "7 pages,6 figures, Accepted to Speech Synthesis Workshop 2025 (SSW13)", "pdf_url": "http://arxiv.org/pdf/2507.00808v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "cate": "cs.SD", "url": "http://arxiv.org/abs/2507.00808v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00330", "title": "Modeling Data Diversity for Joint Instance and Verbalizer Selection in Cold-Start Scenarios", "authors": ["Mohna Chakraborty", "Adithya Kulkarni", "Qi Li"], "summary": "Prompt-based methods leverage the knowledge of pre-trained language models\n(PLMs) trained with a masked language modeling (MLM) objective; however, these\nmethods are sensitive to template, verbalizer, and few-shot instance selection,\nparticularly in cold-start settings with no labeled data. Existing studies\noverlook the dependency between instances and verbalizers, where instance-label\nprobabilities depend on verbalizer token proximity in the embedding space. To\naddress this, we propose COLDSELECT, a joint verbalizer and instance selection\napproach that models data diversity. COLDSELECT maps PLM vocabulary and\n$h_{[MASK]}$ embeddings into a shared space, applying dimensionality reduction\nand clustering to ensure efficient and diverse selection. By optimizing for\nminimal uncertainty and maximal diversity, COLDSELECT captures data\nrelationships effectively. Experiments on eight benchmarks demonstrate\nCOLDSELECT's superiority in reducing uncertainty and enhancing generalization,\noutperforming baselines in verbalizer and few-shot instance selection for\ncold-start scenarios.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00330v1", "categories": ["cs.CL", "cs.IR"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2507.00330v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00518", "title": "Exploring Large Action Sets with Hyperspherical Embeddings using von Mises-Fisher Sampling", "authors": ["Walid Bendada", "Guillaume Salha-Galvan", "Romain Hennequin", "Théo Bontempelli", "Thomas Bouabça", "Tristan Cazenave"], "summary": "This paper introduces von Mises-Fisher exploration (vMF-exp), a scalable\nmethod for exploring large action sets in reinforcement learning problems where\nhyperspherical embedding vectors represent these actions. vMF-exp involves\ninitially sampling a state embedding representation using a von Mises-Fisher\ndistribution, then exploring this representation's nearest neighbors, which\nscales to virtually unlimited numbers of candidate actions. We show that, under\ntheoretical assumptions, vMF-exp asymptotically maintains the same probability\nof exploring each action as Boltzmann Exploration (B-exp), a popular\nalternative that, nonetheless, suffers from scalability issues as it requires\ncomputing softmax values for each action. Consequently, vMF-exp serves as a\nscalable alternative to B-exp for exploring large action sets with\nhyperspherical embeddings. Experiments on simulated data, real-world public\ndata, and the successful large-scale deployment of vMF-exp on the recommender\nsystem of a global music streaming service empirically validate the key\nproperties of the proposed method.", "comment": "42nd International Conference on Machine Learning (ICML 2025)", "pdf_url": "http://arxiv.org/pdf/2507.00518v1", "categories": ["cs.LG", "cs.IR"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00518v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00617", "title": "Accelerating MPGP-type Methods Through Preconditioning", "authors": ["Jakub Kružík", "David Horák"], "summary": "This work investigates the acceleration of MPGP-type algorithms using\npreconditioning for the solution of quadratic programming problems. The\npreconditioning needs to be done only on the free set so as not to change the\nconstraints. A variant of preconditioning restricted to the free set is the\npreconditioning in face. The inner preconditioner in preconditioning in face\nneeds to be recomputed or updated every time the free set changes. Here, we\ninvestigate an approximate variant of preconditioning in face that computes the\ninner preconditioner only once. We analyze the error of the approximate variant\nand provide numerical experiments demonstrating that very large speedups can be\nachieved by the approximate variant.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00617v1", "categories": ["math.NA", "cs.NA", "math.OC", "47N10, 65K10", "G.1.6"], "cate": "math.NA", "url": "http://arxiv.org/abs/2507.00617v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00755", "title": "LearnAFE: Circuit-Algorithm Co-design Framework for Learnable Audio Analog Front-End", "authors": ["Jinhai Hu", "Zhongyi Zhang", "Cong Sheng Leow", "Wang Ling Goh", "Yuan Gao"], "summary": "This paper presents a circuit-algorithm co-design framework for learnable\nanalog front-end (AFE) in audio signal classification. Designing AFE and\nbackend classifiers separately is a common practice but non-ideal, as shown in\nthis paper. Instead, this paper proposes a joint optimization of the backend\nclassifier with the AFE's transfer function to achieve system-level optimum.\nMore specifically, the transfer function parameters of an analog bandpass\nfilter (BPF) bank are tuned in a signal-to-noise ratio (SNR)-aware training\nloop for the classifier. Using a co-design loss function LBPF, this work shows\nsuperior optimization of both the filter bank and the classifier. Implemented\nin open-source SKY130 130nm CMOS process, the optimized design achieved\n90.5%-94.2% accuracy for 10-keyword classification task across a wide range of\ninput signal SNR from 5 dB to 20 dB, with only 22k classifier parameters.\nCompared to conventional approach, the proposed audio AFE achieves 8.7% and\n12.9% reduction in power and capacitor area respectively.", "comment": "11 pages, 15 figures, accepted for publication on IEEE Transactions\n  on Circuits and Systems I: Regular Papers", "pdf_url": "http://arxiv.org/pdf/2507.00755v1", "categories": ["eess.AS", "cs.AI", "cs.SD"], "cate": "eess.AS", "url": "http://arxiv.org/abs/2507.00755v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00299", "title": "When Kids Mode Isn't For Kids: Investigating TikTok's \"Under 13 Experience\"", "authors": ["Olivia Figueira", "Pranathi Chamarthi", "Tu Le", "Athina Markopoulou"], "summary": "TikTok, the social media platform that is popular among children and\nadolescents, offers a more restrictive \"Under 13 Experience\" exclusively for\nyoung users in the US, also known as TikTok's \"Kids Mode\". While prior research\nhas studied various aspects of TikTok's regular mode, including privacy and\npersonalization, TikTok's Kids Mode remains understudied, and there is a lack\nof transparency regarding its content curation and its safety and privacy\nprotections for children. In this paper, (i) we propose an auditing methodology\nto comprehensively investigate TikTok's Kids Mode and (ii) we apply it to\ncharacterize the platform's content curation and determine the prevalence of\nchild-directed content, based on regulations in the Children's Online Privacy\nProtection Act (COPPA). We find that 83% of videos observed on the \"For You\"\npage in Kids Mode are actually not child-directed, and even inappropriate\ncontent was found. The platform also lacks critical features, namely parental\ncontrols and accessibility settings. Our findings have important design and\nregulatory implications, as children may be incentivized to use TikTok's\nregular mode instead of Kids Mode, where they are known to be exposed to\nfurther safety and privacy risks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00299v1", "categories": ["cs.HC", "cs.CR"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2507.00299v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00322", "title": "Failure by Interference: Language Models Make Balanced Parentheses Errors When Faulty Mechanisms Overshadow Sound Ones", "authors": ["Daking Rai", "Samuel Miller", "Kevin Moran", "Ziyu Yao"], "summary": "Despite remarkable advances in coding capabilities, language models (LMs)\nstill struggle with simple syntactic tasks such as generating balanced\nparentheses. In this study, we investigate the underlying mechanisms behind the\npersistence of these errors across LMs of varying sizes (124M-7B) to both\nunderstand and mitigate the errors. Our study reveals that LMs rely on a number\nof components (attention heads and FF neurons) that independently make their\nown predictions. While some components reliably promote correct answers across\na generalized range of inputs (i.e., implementing \"sound mechanisms''), others\nare less reliable and introduce noise by promoting incorrect tokens (i.e.,\nimplementing \"faulty mechanisms''). Errors occur when the faulty mechanisms\novershadow the sound ones and dominantly affect the predictions. Motivated by\nthis insight, we introduce RASteer, a steering method to systematically\nidentify and increase the contribution of reliable components for improving\nmodel performance. RASteer substantially improves performance on balanced\nparentheses tasks, boosting accuracy of some models from $0$% to around $100$%\nwithout impairing the models' general coding ability. We further demonstrate\nits broader applicability in arithmetic reasoning tasks, achieving performance\ngains of up to around $20$%.", "comment": "23 pages, 10 figures, Preprint", "pdf_url": "http://arxiv.org/pdf/2507.00322v1", "categories": ["cs.CL", "cs.AI", "cs.SE", "I.2.7"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2507.00322v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00677", "title": "Learning Steerable Imitation Controllers from Unstructured Animal Motions", "authors": ["Dongho Kang", "Jin Cheng", "Fatemeh Zargarbashi", "Taerim Yoon", "Sungjoon Choi", "Stelian Coros"], "summary": "This paper presents a control framework for legged robots that leverages\nunstructured real-world animal motion data to generate animal-like and\nuser-steerable behaviors. Our framework learns to follow velocity commands\nwhile reproducing the diverse gait patterns in the original dataset. To begin\nwith, animal motion data is transformed into a robot-compatible database using\nconstrained inverse kinematics and model predictive control, bridging the\nmorphological and physical gap between the animal and the robot. Subsequently,\na variational autoencoder-based motion synthesis module captures the diverse\nlocomotion patterns in the motion database and generates smooth transitions\nbetween them in response to velocity commands. The resulting kinematic motions\nserve as references for a reinforcement learning-based feedback controller\ndeployed on physical robots. We show that this approach enables a quadruped\nrobot to adaptively switch gaits and accurately track user velocity commands\nwhile maintaining the stylistic coherence of the motion data. Additionally, we\nprovide component-wise evaluations to analyze the system's behavior in depth\nand demonstrate the efficacy of our method for more accurate and reliable\nmotion imitation.", "comment": "The supplementary video is available at https://youtu.be/DukyUGNYf5A", "pdf_url": "http://arxiv.org/pdf/2507.00677v1", "categories": ["cs.RO"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2507.00677v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00810", "title": "A Robust Algorithm for Non-IID Machine Learning Problems with Convergence Analysis", "authors": ["Qing Xu", "Xiaohua Xuan"], "summary": "In this paper, we propose an improved numerical algorithm for solving minimax\nproblems based on nonsmooth optimization, quadratic programming and iterative\nprocess. We also provide a rigorous proof of convergence for our algorithm\nunder some mild assumptions, such as gradient continuity and boundedness. Such\nan algorithm can be widely applied in various fields such as robust\noptimization, imbalanced learning, etc.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00810v1", "categories": ["cs.AI", "math.OC"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2507.00810v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00026", "title": "ROSE: Toward Reality-Oriented Safety Evaluation of Large Language Models", "authors": ["Jiale Ding", "Xiang Zheng", "Cong Wang", "Wei-Bin Lee", "Xingjun Ma", "Yu-Gang Jiang"], "summary": "As Large Language Models (LLMs) are increasingly deployed as black-box\ncomponents in real-world applications, evaluating their safety-especially under\nadversarial prompting-has become critical. Arguably, effective safety\nevaluations should be adaptive, evolving with LLM capabilities, and also cover\na broad spectrum of harmful topics and real-world scenarios to fully expose\npotential vulnerabilities. Existing manual safety benchmarks, built on\nhandcrafted adversarial prompts, are limited by their static nature and the\nintensive labor required to update them, making it difficult to keep pace with\nrapidly advancing LLMs. In contrast, automated adversarial prompt generation\noffers a promising path toward adaptive evaluation. However, current methods\noften suffer from insufficient adversarial topic coverage (topic-level\ndiversity) and weak alignment with real-world contexts. These shortcomings stem\nfrom the exploration-exploitation dilemma in black-box optimization and a lack\nof real-world contextualization, resulting in adversarial prompts that are both\ntopically narrow and scenario-repetitive. To address these issues, we propose\nReality-Oriented Safety Evaluation (ROSE), a novel framework that uses\nmulti-objective reinforcement learning to fine-tune an adversarial LLM for\ngenerating topically diverse and contextually rich adversarial prompts.\nExperiments show that ROSE outperforms existing methods in uncovering safety\nvulnerabilities in state-of-the-art LLMs, with notable improvements in\nintegrated evaluation metrics. We hope ROSE represents a step toward more\npractical and reality-oriented safety evaluation of LLMs. WARNING: This paper\ncontains examples of potentially harmful text.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00026v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CY"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00026v1", "date": "2025-06-17", "updated": "2025-06-17"}
{"id": "2507.00672", "title": "Toward Edge General Intelligence with Multiple-Large Language Model (Multi-LLM): Architecture, Trust, and Orchestration", "authors": ["Haoxiang Luo", "Yinqiu Liu", "Ruichen Zhang", "Jiacheng Wang", "Gang Sun", "Dusit Niyato", "Hongfang Yu", "Zehui Xiong", "Xianbin Wang", "Xuemin Shen"], "summary": "Edge computing enables real-time data processing closer to its source, thus\nimproving the latency and performance of edge-enabled AI applications. However,\ntraditional AI models often fall short when dealing with complex, dynamic tasks\nthat require advanced reasoning and multimodal data processing. This survey\nexplores the integration of multi-LLMs (Large Language Models) to address this\nin edge computing, where multiple specialized LLMs collaborate to enhance task\nperformance and adaptability in resource-constrained environments. We review\nthe transition from conventional edge AI models to single LLM deployment and,\nultimately, to multi-LLM systems. The survey discusses enabling technologies\nsuch as dynamic orchestration, resource scheduling, and cross-domain knowledge\ntransfer that are key for multi-LLM implementation. A central focus is on\ntrusted multi-LLM systems, ensuring robust decision-making in environments\nwhere reliability and privacy are crucial. We also present multimodal multi-LLM\narchitectures, where multiple LLMs specialize in handling different data\nmodalities, such as text, images, and audio, by integrating their outputs for\ncomprehensive analysis. Finally, we highlight future directions, including\nimproving resource efficiency, trustworthy governance multi-LLM systems, while\naddressing privacy, trust, and robustness concerns. This survey provides a\nvaluable reference for researchers and practitioners aiming to leverage\nmulti-LLM systems in edge computing applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00672v1", "categories": ["cs.NI", "cs.DC"], "cate": "cs.NI", "url": "http://arxiv.org/abs/2507.00672v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00243", "title": "VOCAL: Visual Odometry via ContrAstive Learning", "authors": ["Chi-Yao Huang", "Zeel Bhatt", "Yezhou Yang"], "summary": "Breakthroughs in visual odometry (VO) have fundamentally reshaped the\nlandscape of robotics, enabling ultra-precise camera state estimation that is\ncrucial for modern autonomous systems. Despite these advances, many\nlearning-based VO techniques rely on rigid geometric assumptions, which often\nfall short in interpretability and lack a solid theoretical basis within fully\ndata-driven frameworks. To overcome these limitations, we introduce VOCAL\n(Visual Odometry via ContrAstive Learning), a novel framework that reimagines\nVO as a label ranking challenge. By integrating Bayesian inference with a\nrepresentation learning framework, VOCAL organizes visual features to mirror\ncamera states. The ranking mechanism compels similar camera states to converge\ninto consistent and spatially coherent representations within the latent space.\nThis strategic alignment not only bolsters the interpretability of the learned\nfeatures but also ensures compatibility with multimodal data sources. Extensive\nevaluations on the KITTI dataset highlight VOCAL's enhanced interpretability\nand flexibility, pushing VO toward more general and explainable spatial\nintelligence.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00243v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00243v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00832", "title": "Automated anatomy-based post-processing reduces false positives and improved interpretability of deep learning intracranial aneurysm detection", "authors": ["Jisoo Kim", "Chu-Hsuan Lin", "Alberto Ceballos-Arroyo", "Ping Liu", "Huaizu Jiang", "Shrikanth Yadav", "Qi Wan", "Lei Qin", "Geoffrey S Young"], "summary": "Introduction: Deep learning (DL) models can help detect intracranial\naneurysms on CTA, but high false positive (FP) rates remain a barrier to\nclinical translation, despite improvement in model architectures and strategies\nlike detection threshold tuning. We employed an automated, anatomy-based,\nheuristic-learning hybrid artery-vein segmentation post-processing method to\nfurther reduce FPs. Methods: Two DL models, CPM-Net and a deformable 3D\nconvolutional neural network-transformer hybrid (3D-CNN-TR), were trained with\n1,186 open-source CTAs (1,373 annotated aneurysms), and evaluated with 143\nheld-out private CTAs (218 annotated aneurysms). Brain, artery, vein, and\ncavernous venous sinus (CVS) segmentation masks were applied to remove possible\nFPs in the DL outputs that overlapped with: (1) brain mask; (2) vein mask; (3)\nvein more than artery masks; (4) brain plus vein mask; (5) brain plus vein more\nthan artery masks. Results: CPM-Net yielded 139 true-positives (TP); 79\nfalse-negative (FN); 126 FP. 3D-CNN-TR yielded 179 TP; 39 FN; 182 FP. FPs were\ncommonly extracranial (CPM-Net 27.3%; 3D-CNN-TR 42.3%), venous (CPM-Net 56.3%;\n3D-CNN-TR 29.1%), arterial (CPM-Net 11.9%; 3D-CNN-TR 53.3%), and non-vascular\n(CPM-Net 25.4%; 3D-CNN-TR 9.3%) structures. Method 5 performed best, reducing\nCPM-Net FP by 70.6% (89/126) and 3D-CNN-TR FP by 51.6% (94/182), without\nreducing TP, lowering the FP/case rate from 0.88 to 0.26 for CPM-NET, and from\n1.27 to 0.62 for the 3D-CNN-TR. Conclusion: Anatomy-based, interpretable\npost-processing can improve DL-based aneurysm detection model performance. More\nbroadly, automated, domain-informed, hybrid heuristic-learning processing holds\npromise for improving the performance and clinical acceptance of aneurysm\ndetection models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00832v1", "categories": ["eess.IV", "cs.AI", "cs.CV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2507.00832v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00963", "title": "Social Robots for People with Dementia: A Literature Review on Deception from Design to Perception", "authors": ["Fan Wang", "Giulia Perugia", "Yuan Feng", "Wijnand IJsselsteijn"], "summary": "As social robots increasingly enter dementia care, concerns about deception,\nintentional or not, are gaining attention. Yet, how robotic design cues might\nelicit misleading perceptions in people with dementia, and how these\nperceptions arise, remains insufficiently understood. In this scoping review,\nwe examined 26 empirical studies on interactions between people with dementia\nand physical social robots. We identify four key design cue categories that may\ninfluence deceptive impressions: cues resembling physiological signs (e.g.,\nsimulated breathing), social intentions (e.g., playful movement), familiar\nbeings (e.g., animal-like form and sound), and, to a lesser extent, cues that\nreveal artificiality. Thematic analysis of user responses reveals that people\nwith dementia often attribute biological, social, and mental capacities to\nrobots, dynamically shifting between awareness and illusion. These findings\nunderscore the fluctuating nature of ontological perception in dementia\ncontexts. Existing definitions of robotic deception often rest on philosophical\nor behaviorist premises, but rarely engage with the cognitive mechanisms\ninvolved. We propose an empirically grounded definition: robotic deception\noccurs when Type 1 (automatic, heuristic) processing dominates over Type 2\n(deliberative, analytic) reasoning, leading to misinterpretation of a robot's\nartificial nature. This dual-process perspective highlights the ethical\ncomplexity of social robots in dementia care and calls for design approaches\nthat are not only engaging, but also epistemically respectful.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00963v1", "categories": ["cs.HC", "cs.CY", "cs.RO"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2507.00963v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00105", "title": "Graph Neural Networks in Wind Power Forecasting", "authors": ["Javier Castellano", "Ignacio Villanueva"], "summary": "We study the applicability of GNNs to the problem of wind energy forecasting.\nWe find that certain architectures achieve performance comparable to our best\nCNN-based benchmark. The study is conducted on three wind power facilities\nusing five years of historical data. Numerical Weather Prediction (NWP)\nvariables were used as predictors, and models were evaluated on a 24 to 36 hour\nahead test horizon.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00105v1", "categories": ["cs.LG", "cs.SY", "eess.SY"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00105v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00231", "title": "Observation of Blood Flow in Major Neck Vessels Modulated 1 by Physiological Maneuvers", "authors": ["Gennadi Saiko", "Timothy Burton", "Faraz Sadrzadeh-Afsharazar", "Shota Yamashita", "Kenshin Shimono", "Yasuyuki Kakihana", "Alexandre Douplik"], "summary": "Large neck vessels (carotid artery and internal jugular vein, IJV) offer a\nunique opportunity to monitor hemodynamics non-invasively by optical means. The\nprimary shortcoming of past work has been the focus on healthy volunteers in\nnormal physiological conditions and well-controlled environments. To drive the\ntechnology closer to the bedside, testing is required under more re-alistic\nconditions, including in pathologies and real-world environments (e.g., similar\ntoICU or emergency care settings). The primary goal of the current work was to\nextend the range of physiological maneuvers for blood flow modulation by\nintroducing new maneuvers and ob-serving PPG response to them. The data from\nthe necks of two healthy volunteers in a supine position were collected by\nclinical PPG and in-house built PPG sensors, accompanied by ECG signal\ncollection. Seven maneuvers (abdominojugular test, breath holding, Valsalva,\nproximal occlusion of right IJV, distal occlusion of right IJV, proximal\nocclusion of left IJV, distal occlusion of left IJV) were performed in sequence\nwith 1 min allocated for each maneuver. The 1 min was split into three\nsegments: baseline (15 s), experiment (15 s), and recovery (30 s). Thus, the\noverall du-ration of the experiment was 7 min. AC amplitude from clinical PPG,\nDC amplitudes from in-house built PPG, and ECG signal were compared during all\nseven physiological maneuvers. Newly proposed maneuvers (Valsalva and IJV\nocclusions) demonstrated modulation of blood flow, which was more significant\nthan previously reported maneuvers (abdominojugular test and breath holding).\nThe proposed physiological maneuvers demonstrate high potential as instruments\nfor modulating blood flow in major neck vessels.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00231v1", "categories": ["physics.med-ph", "cs.SY", "eess.SP", "eess.SY"], "cate": "physics.med-ph", "url": "http://arxiv.org/abs/2507.00231v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00966", "title": "MambAttention: Mamba with Multi-Head Attention for Generalizable Single-Channel Speech Enhancement", "authors": ["Nikolai Lund Kühne", "Jesper Jensen", "Jan Østergaard", "Zheng-Hua Tan"], "summary": "With the advent of new sequence models like Mamba and xLSTM, several studies\nhave shown that these models match or outperform state-of-the-art models in\nsingle-channel speech enhancement, automatic speech recognition, and\nself-supervised audio representation learning. However, prior research has\ndemonstrated that sequence models like LSTM and Mamba tend to overfit to the\ntraining set. To address this issue, previous works have shown that adding\nself-attention to LSTMs substantially improves generalization performance for\nsingle-channel speech enhancement. Nevertheless, neither the concept of hybrid\nMamba and time-frequency attention models nor their generalization performance\nhave been explored for speech enhancement. In this paper, we propose a novel\nhybrid architecture, MambAttention, which combines Mamba and shared time- and\nfrequency-multi-head attention modules for generalizable single-channel speech\nenhancement. To train our model, we introduce VoiceBank+Demand Extended\n(VB-DemandEx), a dataset inspired by VoiceBank+Demand but with more challenging\nnoise types and lower signal-to-noise ratios. Trained on VB-DemandEx, our\nproposed MambAttention model significantly outperforms existing\nstate-of-the-art LSTM-, xLSTM-, Mamba-, and Conformer-based systems of similar\ncomplexity across all reported metrics on two out-of-domain datasets: DNS 2020\nand EARS-WHAM_v2, while matching their performance on the in-domain dataset\nVB-DemandEx. Ablation studies highlight the role of weight sharing between the\ntime- and frequency-multi-head attention modules for generalization\nperformance. Finally, we explore integrating the shared time- and\nfrequency-multi-head attention modules with LSTM and xLSTM, which yields a\nnotable performance improvement on the out-of-domain datasets. However, our\nMambAttention model remains superior on both out-of-domain datasets across all\nreported evaluation metrics.", "comment": "Submitted to IEEE/ACM Transactions on Audio, Speech, and Language\n  Processing for possible publication", "pdf_url": "http://arxiv.org/pdf/2507.00966v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "cate": "cs.SD", "url": "http://arxiv.org/abs/2507.00966v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00355", "title": "Question Decomposition for Retrieval-Augmented Generation", "authors": ["Paul J. L. Ammann", "Jonas Golde", "Alan Akbik"], "summary": "Grounding large language models (LLMs) in verifiable external sources is a\nwell-established strategy for generating reliable answers. Retrieval-augmented\ngeneration (RAG) is one such approach, particularly effective for tasks like\nquestion answering: it retrieves passages that are semantically related to the\nquestion and then conditions the model on this evidence. However, multi-hop\nquestions, such as \"Which company among NVIDIA, Apple, and Google made the\nbiggest profit in 2023?,\" challenge RAG because relevant facts are often\ndistributed across multiple documents rather than co-occurring in one source,\nmaking it difficult for standard RAG to retrieve sufficient information. To\naddress this, we propose a RAG pipeline that incorporates question\ndecomposition: (i) an LLM decomposes the original query into sub-questions,\n(ii) passages are retrieved for each sub-question, and (iii) the merged\ncandidate pool is reranked to improve the coverage and precision of the\nretrieved evidence. We show that question decomposition effectively assembles\ncomplementary documents, while reranking reduces noise and promotes the most\nrelevant passages before answer generation. Although reranking itself is\nstandard, we show that pairing an off-the-shelf cross-encoder reranker with\nLLM-driven question decomposition bridges the retrieval gap on multi-hop\nquestions and provides a practical, drop-in enhancement, without any extra\ntraining or specialized indexing. We evaluate our approach on the MultiHop-RAG\nand HotpotQA, showing gains in retrieval (MRR@10: +36.7%) and answer accuracy\n(F1: +11.6%) over standard RAG baselines.", "comment": "Accepted to ACL SRW 2025. 9 Pages, 2 Figures, 4 Tables", "pdf_url": "http://arxiv.org/pdf/2507.00355v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2507.00355v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00961", "title": "Digital Collections Explorer: An Open-Source, Multimodal Viewer for Searching Digital Collections", "authors": ["Ying-Hsiang Huang", "Benjamin Charles Germain Lee"], "summary": "We present Digital Collections Explorer, a web-based, open-source exploratory\nsearch platform that leverages CLIP (Contrastive Language-Image Pre-training)\nfor enhanced visual discovery of digital collections. Our Digital Collections\nExplorer can be installed locally and configured to run on a visual collection\nof interest on disk in just a few steps. Building upon recent advances in\nmultimodal search techniques, our interface enables natural language queries\nand reverse image searches over digital collections with visual features. This\npaper describes the system's architecture, implementation, and application to\nvarious cultural heritage collections, demonstrating its potential for\ndemocratizing access to digital archives, especially those with impoverished\nmetadata. We present case studies with maps, photographs, and PDFs extracted\nfrom web archives in order to demonstrate the flexibility of the Digital\nCollections Explorer, as well as its ease of use. We demonstrate that the\nDigital Collections Explorer scales to hundreds of thousands of images on a\nMacBook Pro with an M4 chip. Lastly, we host a public demo of Digital\nCollections Explorer.", "comment": "14 pages, 8 figures, 2 tables", "pdf_url": "http://arxiv.org/pdf/2507.00961v1", "categories": ["cs.DL", "cs.IR"], "cate": "cs.DL", "url": "http://arxiv.org/abs/2507.00961v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00667", "title": "Special measures of smoothness for approximation by sampling operators in $L_p(\\Bbb{R}^d)$", "authors": ["Yurii Kolomoitsev"], "summary": "Traditional measures of smoothness often fail to provide accurate $L_p$-error\nestimates for approximation by sampling or interpolation operators, especially\nfor functions with low smoothness. To address this issue, we introduce a\nmodified measure of smoothness that incorporates the local behavior of a\nfunction at the sampling points through the use of averaged operators. With\nthis new tool, we obtain matching direct and inverse error estimates for a wide\nclass of sampling operators and functions in $L_p$ spaces. Additionally, we\nderive a criterion for the convergence of sampling operators in $L_p$, identify\nconditions that ensure the exact rate of approximation, construct realizations\nof $K$-functionals based on these operators, and study the smoothness\nproperties of sampling operators. We also demonstrate how our results apply to\nseveral well-known operators, including the classical Whittaker-Shannon\nsampling operator, sampling operators generated by $B$-splines, and those based\non the Gaussian.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00667v1", "categories": ["math.NA", "cs.NA", "math.CA", "41A05, 41A15, 41A17, 41A25, 41A27"], "cate": "math.NA", "url": "http://arxiv.org/abs/2507.00667v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00367", "title": "Presto: Hardware Acceleration of Ciphers for Hybrid Homomorphic Encryption", "authors": ["Yeonsoo Jeon", "Mattan Erez", "Michael Orshansky"], "summary": "Hybrid Homomorphic Encryption (HHE) combines symmetric key and homomorphic\nencryption to reduce ciphertext expansion crucial in client-server deployments\nof HE. Special symmetric ciphers, amenable to efficient HE evaluation, have\nbeen developed. Their client-side deployment calls for performant and\nenergy-efficient implementation, and in this paper we develop and evaluate\nhardware accelerators for the two known CKKS-targeting HHE ciphers, HERA and\nRubato.\n  We design vectorized and overlapped functional modules. The design exploits\ntransposition-invariance property of the MixColumns and MixRows function and\nalternates the order of intermediate state to eliminate bubbles in stream key\ngeneration, improving latency and throughput. We decouple the RNG and key\ncomputation phases to hide the latency of RNG and to reduce the critical path\nin FIFOs, achieving higher operating frequency.\n  We implement the accelerator on an AMD Virtex UltraScale+ FPGA. Both Rubato\nand HERA achieve a 6x improvement in throughput compared to the software\nimplementation. In terms of latency, Rubato achieves a 5x reduction, while HERA\nachieves a 3x reduction. Additionally, our hardware implementations reduce\nenergy consumption by 75x for Rubato and 47x for HERA compared to their\nsoftware implementation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00367v1", "categories": ["cs.AR", "cs.CR"], "cate": "cs.AR", "url": "http://arxiv.org/abs/2507.00367v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00595", "title": "The Secrets Must Not Flow: Scaling Security Verification to Large Codebases (extended version)", "authors": ["Linard Arquint", "Samarth Kishor", "Jason R. Koenig", "Joey Dodds", "Daniel Kroening", "Peter Müller"], "summary": "Existing program verifiers can prove advanced properties about security\nprotocol implementations, but are difficult to scale to large codebases because\nof the manual effort required. We develop a novel methodology called *Diodon*\nthat addresses this challenge by splitting the codebase into the protocol\nimplementation (the *Core*) and the remainder (the *Application*). This split\nallows us to apply powerful semi-automated verification techniques to the\nsecurity-critical Core, while fully-automatic static analyses scale the\nverification to the entire codebase by ensuring that the Application cannot\ninvalidate the security properties proved for the Core. The static analyses\nachieve that by proving *I/O independence*, i.e., that the I/O operations\nwithin the Application are independent of the Core's security-relevant data\n(such as keys), and that the Application meets the Core's requirements. We have\nproved Diodon sound by first showing that we can safely allow the Application\nto perform I/O independent of the security protocol, and second that manual\nverification and static analyses soundly compose. We evaluate Diodon on two\ncase studies: an implementation of the signed Diffie-Hellman key exchange and a\nlarge (100k+ LoC) production Go codebase implementing a key exchange protocol\nfor which we obtained secrecy and injective agreement guarantees by verifying a\nCore of about 1% of the code with the auto-active program verifier Gobra in\nless than three person months.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00595v1", "categories": ["cs.CR", "cs.PL", "cs.SE"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2507.00595v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00816", "title": "PI-WAN: A Physics-Informed Wind-Adaptive Network for Quadrotor Dynamics Prediction in Unknown Environments", "authors": ["Mengyun Wang", "Bo Wang", "Yifeng Niu", "Chang Wang"], "summary": "Accurate dynamics modeling is essential for quadrotors to achieve precise\ntrajectory tracking in various applications. Traditional physical\nknowledge-driven modeling methods face substantial limitations in unknown\nenvironments characterized by variable payloads, wind disturbances, and\nexternal perturbations. On the other hand, data-driven modeling methods suffer\nfrom poor generalization when handling out-of-distribution (OoD) data,\nrestricting their effectiveness in unknown scenarios. To address these\nchallenges, we introduce the Physics-Informed Wind-Adaptive Network (PI-WAN),\nwhich combines knowledge-driven and data-driven modeling methods by embedding\nphysical constraints directly into the training process for robust quadrotor\ndynamics learning. Specifically, PI-WAN employs a Temporal Convolutional\nNetwork (TCN) architecture that efficiently captures temporal dependencies from\nhistorical flight data, while a physics-informed loss function applies physical\nprinciples to improve model generalization and robustness across previously\nunseen conditions. By incorporating real-time prediction results into a model\npredictive control (MPC) framework, we achieve improvements in closed-loop\ntracking performance. Comprehensive simulations and real-world flight\nexperiments demonstrate that our approach outperforms baseline methods in terms\nof prediction accuracy, tracking precision, and robustness to unknown\nenvironments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00816v1", "categories": ["cs.RO", "cs.AI"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2507.00816v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00841", "title": "SafeMobile: Chain-level Jailbreak Detection and Automated Evaluation for Multimodal Mobile Agents", "authors": ["Siyuan Liang", "Tianmeng Fang", "Zhe Liu", "Aishan Liu", "Yan Xiao", "Jinyuan He", "Ee-Chien Chang", "Xiaochun Cao"], "summary": "With the wide application of multimodal foundation models in intelligent\nagent systems, scenarios such as mobile device control, intelligent assistant\ninteraction, and multimodal task execution are gradually relying on such large\nmodel-driven agents. However, the related systems are also increasingly exposed\nto potential jailbreak risks. Attackers may induce the agents to bypass the\noriginal behavioral constraints through specific inputs, and then trigger\ncertain risky and sensitive operations, such as modifying settings, executing\nunauthorized commands, or impersonating user identities, which brings new\nchallenges to system security. Existing security measures for intelligent\nagents still have limitations when facing complex interactions, especially in\ndetecting potentially risky behaviors across multiple rounds of conversations\nor sequences of tasks. In addition, an efficient and consistent automated\nmethodology to assist in assessing and determining the impact of such risks is\ncurrently lacking. This work explores the security issues surrounding mobile\nmultimodal agents, attempts to construct a risk discrimination mechanism by\nincorporating behavioral sequence information, and designs an automated\nassisted assessment scheme based on a large language model. Through preliminary\nvalidation in several representative high-risk tasks, the results show that the\nmethod can improve the recognition of risky behaviors to some extent and assist\nin reducing the probability of agents being jailbroken. We hope that this study\ncan provide some valuable references for the security risk modeling and\nprotection of multimodal intelligent agent systems.", "comment": "12 pages", "pdf_url": "http://arxiv.org/pdf/2507.00841v1", "categories": ["cs.AI", "cs.CR"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2507.00841v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00028", "title": "HiT-JEPA: A Hierarchical Self-supervised Trajectory Embedding Framework for Similarity Computation", "authors": ["Lihuan Li", "Hao Xue", "Shuang Ao", "Yang Song", "Flora Salim"], "summary": "The representation of urban trajectory data plays a critical role in\neffectively analyzing spatial movement patterns. Despite considerable progress,\nthe challenge of designing trajectory representations that can capture diverse\nand complementary information remains an open research problem. Existing\nmethods struggle in incorporating trajectory fine-grained details and\nhigh-level summary in a single model, limiting their ability to attend to both\nlong-term dependencies while preserving local nuances. To address this, we\npropose HiT-JEPA (Hierarchical Interactions of Trajectory Semantics via a Joint\nEmbedding Predictive Architecture), a unified framework for learning\nmulti-scale urban trajectory representations across semantic abstraction\nlevels. HiT-JEPA adopts a three-layer hierarchy that progressively captures\npoint-level fine-grained details, intermediate patterns, and high-level\ntrajectory abstractions, enabling the model to integrate both local dynamics\nand global semantics in one coherent structure. Extensive experiments on\nmultiple real-world datasets for trajectory similarity computation show that\nHiT-JEPA's hierarchical design yields richer, multi-scale representations. Code\nis available at: https://anonymous.4open.science/r/HiT-JEPA.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00028v1", "categories": ["cs.LG", "cs.AI", "cs.CV"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00028v1", "date": "2025-06-17", "updated": "2025-06-17"}
{"id": "2507.00740", "title": "Safe Low Bandwidth SPV: A Formal Treatment of Simplified Payment Verification Protocols and Security Bounds", "authors": ["Craig S Wright"], "summary": "This paper presents a complete formal specification, protocol description,\nand mathematical proof structure for Simplified Payment Verification (SPV) as\noriginally defined in the Bitcoin whitepaper \\cite{nakamoto2008}. In stark\ncontrast to the misrepresentations proliferated by popular implementations, we\nshow that SPV is not only secure under bounded adversarial assumptions but\nstrictly optimal for digital cash systems requiring scalable and verifiable\ntransaction inclusion. We reconstruct the SPV protocol from first principles,\ngrounding its verification model in symbolic automata, Merkle membership\nrelations, and chain-of-proof dominance predicates. Through rigorous\nprobabilistic and game-theoretic analysis, we derive the economic bounds within\nwhich the protocol operates securely and verify its liveness and safety\nproperties under partial connectivity, hostile relay networks, and adversarial\npropagation delay. Our specification further introduces low-bandwidth\noptimisations such as adaptive polling and compressed header synchronisation\nwhile preserving correctness. This document serves both as a blueprint for\nsecure SPV implementation and a rebuttal of common misconceptions surrounding\nnon-validating clients.", "comment": "56 pages 5 images", "pdf_url": "http://arxiv.org/pdf/2507.00740v1", "categories": ["cs.CR", "cs.CL", "cs.DC", "68Q85, 68M10, 94A60, 91A80, 68Q17, 68W10, 68R10", "C.2.2; F.2.2; D.4.6; K.6.5"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2507.00740v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00248", "title": "Developing Lightweight DNN Models With Limited Data For Real-Time Sign Language Recognition", "authors": ["Nikita Nikitin", "Eugene Fomin"], "summary": "We present a novel framework for real-time sign language recognition using\nlightweight DNNs trained on limited data. Our system addresses key challenges\nin sign language recognition, including data scarcity, high computational\ncosts, and discrepancies in frame rates between training and inference\nenvironments. By encoding sign language specific parameters, such as handshape,\npalm orientation, movement, and location into vectorized inputs, and leveraging\nMediaPipe for landmark extraction, we achieve highly separable input data\nrepresentations. Our DNN architecture, optimized for sub 10MB deployment,\nenables accurate classification of 343 signs with less than 10ms latency on\nedge devices. The data annotation platform 'slait data' facilitates structured\nlabeling and vector extraction. Our model achieved 92% accuracy in isolated\nsign recognition and has been integrated into the 'slait ai' web application,\nwhere it demonstrates stable inference.", "comment": "7 pages, 2 figures, 2 tables, for associated mpeg file, see\n  https://slait.app/static/Screen_Recording.mp4", "pdf_url": "http://arxiv.org/pdf/2507.00248v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00248v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00903", "title": "Deep learning-based segmentation of T1 and T2 cardiac MRI maps for automated disease detection", "authors": ["Andreea Bianca Popescu", "Andreas Seitz", "Heiko Mahrholdt", "Jens Wetzl", "Athira Jacob", "Lucian Mihai Itu", "Constantin Suciu", "Teodora Chitiboi"], "summary": "Objectives Parametric tissue mapping enables quantitative cardiac tissue\ncharacterization but is limited by inter-observer variability during manual\ndelineation. Traditional approaches relying on average relaxation values and\nsingle cutoffs may oversimplify myocardial complexity. This study evaluates\nwhether deep learning (DL) can achieve segmentation accuracy comparable to\ninter-observer variability, explores the utility of statistical features beyond\nmean T1/T2 values, and assesses whether machine learning (ML) combining\nmultiple features enhances disease detection. Materials & Methods T1 and T2\nmaps were manually segmented. The test subset was independently annotated by\ntwo observers, and inter-observer variability was assessed. A DL model was\ntrained to segment left ventricle blood pool and myocardium. Average (A), lower\nquartile (LQ), median (M), and upper quartile (UQ) were computed for the\nmyocardial pixels and employed in classification by applying cutoffs or in ML.\nDice similarity coefficient (DICE) and mean absolute percentage error evaluated\nsegmentation performance. Bland-Altman plots assessed inter-user and\nmodel-observer agreement. Receiver operating characteristic analysis determined\noptimal cutoffs. Pearson correlation compared features from model and manual\nsegmentations. F1-score, precision, and recall evaluated classification\nperformance. Wilcoxon test assessed differences between classification methods,\nwith p < 0.05 considered statistically significant. Results 144 subjects were\nsplit into training (100), validation (15) and evaluation (29) subsets.\nSegmentation model achieved a DICE of 85.4%, surpassing inter-observer\nagreement. Random forest applied to all features increased F1-score (92.7%, p <\n0.001). Conclusion DL facilitates segmentation of T1/ T2 maps. Combining\nmultiple features with ML improves disease detection.", "comment": "This work has been submitted for consideration at European Radiology\n  (Springer). Upon acceptance, this preprint will be updated with the journal\n  reference", "pdf_url": "http://arxiv.org/pdf/2507.00903v1", "categories": ["eess.IV", "cs.AI", "cs.CV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2507.00903v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.01017", "title": "A Comprehensive Review of Human Error in Risk-Informed Decision Making: Integrating Human Reliability Assessment, Artificial Intelligence, and Human Performance Models", "authors": ["Xingyu Xiao", "Hongxu Zhu", "Jingang Liang", "Jiejuan Tong", "Haitao Wang"], "summary": "Human error remains a dominant risk driver in safety-critical sectors such as\nnuclear power, aviation, and healthcare, where seemingly minor mistakes can\ncascade into catastrophic outcomes. Although decades of research have produced\na rich repertoire of mitigation techniques, persistent limitations: scarce\nhigh-quality data, algorithmic opacity, and residual reliance on expert\njudgment, continue to constrain progress. This review synthesizes recent\nadvances at the intersection of risk-informed decision making, human\nreliability assessment (HRA), artificial intelligence (AI), and cognitive\nscience to clarify how their convergence can curb human-error risk. We first\ncategorize the principal forms of human error observed in complex\nsociotechnical environments and outline their quantitative impact on system\nreliability. Next, we examine risk-informed frameworks that embed HRA within\nprobabilistic and data-driven methodologies, highlighting successes and gaps.\nWe then survey cognitive and human-performance models, detailing how\nmechanistic accounts of perception, memory, and decision-making enrich error\nprediction and complement HRA metrics. Building on these foundations, we\ncritically assess AI-enabled techniques for real-time error detection,\noperator-state estimation, and AI-augmented HRA workflows. Across these\nstrands, a recurring insight emerges: integrating cognitive models with\nAI-based analytics inside risk-informed HRA pipelines markedly enhances\npredictive fidelity, yet doing so demands richer datasets, transparent\nalgorithms, and rigorous validation. Finally, we identify promising research\ndirections, coupling resilience engineering concepts with grounded theory,\noperationalizing the iceberg model of incident causation, and establishing\ncross-domain data consortia, to foster a multidisciplinary paradigm that\nelevates human reliability in high-stakes systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.01017v1", "categories": ["cs.HC"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2507.01017v1", "date": "2025-06-10", "updated": "2025-06-10"}
{"id": "2507.00231", "title": "Observation of Blood Flow in Major Neck Vessels Modulated 1 by Physiological Maneuvers", "authors": ["Gennadi Saiko", "Timothy Burton", "Faraz Sadrzadeh-Afsharazar", "Shota Yamashita", "Kenshin Shimono", "Yasuyuki Kakihana", "Alexandre Douplik"], "summary": "Large neck vessels (carotid artery and internal jugular vein, IJV) offer a\nunique opportunity to monitor hemodynamics non-invasively by optical means. The\nprimary shortcoming of past work has been the focus on healthy volunteers in\nnormal physiological conditions and well-controlled environments. To drive the\ntechnology closer to the bedside, testing is required under more re-alistic\nconditions, including in pathologies and real-world environments (e.g., similar\ntoICU or emergency care settings). The primary goal of the current work was to\nextend the range of physiological maneuvers for blood flow modulation by\nintroducing new maneuvers and ob-serving PPG response to them. The data from\nthe necks of two healthy volunteers in a supine position were collected by\nclinical PPG and in-house built PPG sensors, accompanied by ECG signal\ncollection. Seven maneuvers (abdominojugular test, breath holding, Valsalva,\nproximal occlusion of right IJV, distal occlusion of right IJV, proximal\nocclusion of left IJV, distal occlusion of left IJV) were performed in sequence\nwith 1 min allocated for each maneuver. The 1 min was split into three\nsegments: baseline (15 s), experiment (15 s), and recovery (30 s). Thus, the\noverall du-ration of the experiment was 7 min. AC amplitude from clinical PPG,\nDC amplitudes from in-house built PPG, and ECG signal were compared during all\nseven physiological maneuvers. Newly proposed maneuvers (Valsalva and IJV\nocclusions) demonstrated modulation of blood flow, which was more significant\nthan previously reported maneuvers (abdominojugular test and breath holding).\nThe proposed physiological maneuvers demonstrate high potential as instruments\nfor modulating blood flow in major neck vessels.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00231v1", "categories": ["physics.med-ph", "cs.SY", "eess.SP", "eess.SY"], "cate": "physics.med-ph", "url": "http://arxiv.org/abs/2507.00231v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00366", "title": "Wireless AI Evolution: From Statistical Learners to Electromagnetic-Guided Foundation Models", "authors": ["Jian Xiao", "Ji Wang", "Kunrui Cao", "Xingwang Li", "Zhao Chen", "Chau Yuen"], "summary": "While initial applications of artificial intelligence (AI) in wireless\ncommunications over the past decade have demonstrated considerable potential\nusing specialized models for targeted communication tasks, the revolutionary\ndemands of sixth-generation (6G) networks for holographic communications,\nubiquitous sensing, and native intelligence are propelling a necessary\nevolution towards AI-native wireless networks. The arrival of large AI models\npaves the way for the next phase of Wireless AI, driven by wireless foundation\nmodels (WFMs). In particular, pre-training on universal electromagnetic (EM)\nprinciples equips WFMs with the essential adaptability for a multitude of\ndemanding 6G applications. However, existing large AI models face critical\nlimitations, including pre-training strategies disconnected from EM-compliant\nconstraints leading to physically inconsistent predictions, a lack of embedded\nunderstanding of wave propagation physics, and the inaccessibility of massive\nlabeled datasets for comprehensive EM-aware training. To address these\nchallenges, this article presents an electromagnetic information theory-guided\nself-supervised pre-training (EIT-SPT) framework designed to systematically\ninject EM physics into WFMs. The EIT-SPT framework aims to infuse WFMs with\nintrinsic EM knowledge, thereby enhancing their physical consistency,\ngeneralization capabilities across varied EM landscapes, and overall data\nefficiency. Building upon the proposed EIT-SPT framework, this article first\nelaborates on diverse potential applications in 6G scenarios of WFMs, then\nvalidates the efficacy of the proposed framework through illustrative case\nstudies, and finally summarizes critical open research challenges and future\ndirections for WFMs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00366v1", "categories": ["cs.IT", "eess.SP", "math.IT"], "cate": "cs.IT", "url": "http://arxiv.org/abs/2507.00366v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00380", "title": "Gregorian melody, modality, and memory: Segmenting chant with Bayesian nonparametrics", "authors": ["Vojtěch Lanz", "Jan Hajič jr"], "summary": "The idea that Gregorian melodies are constructed from some vocabulary of\nsegments has long been a part of chant scholarship. This so-called\n\"centonisation\" theory has received much musicological criticism, but frequent\nre-use of certain melodic segments has been observed in chant melodies, and the\nintractable number of possible segmentations allowed the option that some\nundiscovered segmentation exists that will yet prove the value of\ncentonisation, and recent empirical results have shown that segmentations can\noutperform music-theoretical features in mode classification. Inspired by the\nfact that Gregorian chant was memorised, we search for an optimal unsupervised\nsegmentation of chant melody using nested hierarchical Pitman-Yor language\nmodels. The segmentation we find achieves state-of-the-art performance in mode\nclassification. Modeling a monk memorising the melodies from one liturgical\nmanuscript, we then find empirical evidence for the link between mode\nclassification and memory efficiency, and observe more formulaic areas at the\nbeginnings and ends of melodies corresponding to the practical role of modality\nin performance. However, the resulting segmentations themselves indicate that\neven such a memory-optimal segmentation is not what is understood as\ncentonisation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00380v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2507.00380v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00674", "title": "A hyperboloidal method for numerical simulations of multidimensional nonlinear wave equations", "authors": ["Oliver Rinne"], "summary": "We consider the scalar wave equation with power nonlinearity in n+1\ndimensions. Unlike previous numerical studies, we go beyond the radial case and\ndo not assume any symmetries for n=3, and we only impose an SO(n-1) symmetry in\nhigher dimensions. Our method is based on a hyperboloidal foliation of\nMinkowski spacetime and conformal compactification. We focus on the late-time\npower-law decay (tails) of the solutions and compute decay exponents for\ndifferent spherical harmonic modes, for subcritical, critical and\nsupercritical, focusing and defocusing nonlinear wave equations.", "comment": "20 pages, 8 figures, 2 tables", "pdf_url": "http://arxiv.org/pdf/2507.00674v1", "categories": ["math.NA", "cs.NA", "math-ph", "math.AP", "math.MP"], "cate": "math.NA", "url": "http://arxiv.org/abs/2507.00674v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00596", "title": "Gaze3P: Gaze-Based Prediction of User-Perceived Privacy", "authors": ["Mayar Elfares", "Pascal Reisert", "Ralf Küsters", "Andreas Bulling"], "summary": "Privacy is a highly subjective concept and perceived variably by different\nindividuals. Previous research on quantifying user-perceived privacy has\nprimarily relied on questionnaires. Furthermore, applying user-perceived\nprivacy to optimise the parameters of privacy-preserving techniques (PPT)\nremains insufficiently explored. To address these limitations, we introduce\nGaze3P -- the first dataset specifically designed to facilitate systematic\ninvestigations into user-perceived privacy. Our dataset comprises gaze data\nfrom 100 participants and 1,000 stimuli, encompassing a range of private and\nsafe attributes. With Gaze3P, we train a machine learning model to implicitly\nand dynamically predict perceived privacy from human eye gaze. Through\ncomprehensive experiments, we show that the resulting models achieve high\naccuracy. Finally, we illustrate how predicted privacy can be used to optimise\nthe parameters of differentially private mechanisms, thereby enhancing their\nalignment with user expectations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00596v1", "categories": ["cs.HC", "cs.CR"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2507.00596v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00833", "title": "HumanoidGen: Data Generation for Bimanual Dexterous Manipulation via LLM Reasoning", "authors": ["Zhi Jing", "Siyuan Yang", "Jicong Ao", "Ting Xiao", "Yugang Jiang", "Chenjia Bai"], "summary": "For robotic manipulation, existing robotics datasets and simulation\nbenchmarks predominantly cater to robot-arm platforms. However, for humanoid\nrobots equipped with dual arms and dexterous hands, simulation tasks and\nhigh-quality demonstrations are notably lacking. Bimanual dexterous\nmanipulation is inherently more complex, as it requires coordinated arm\nmovements and hand operations, making autonomous data collection challenging.\nThis paper presents HumanoidGen, an automated task creation and demonstration\ncollection framework that leverages atomic dexterous operations and LLM\nreasoning to generate relational constraints. Specifically, we provide spatial\nannotations for both assets and dexterous hands based on the atomic operations,\nand perform an LLM planner to generate a chain of actionable spatial\nconstraints for arm movements based on object affordances and scenes. To\nfurther improve planning ability, we employ a variant of Monte Carlo tree\nsearch to enhance LLM reasoning for long-horizon tasks and insufficient\nannotation. In experiments, we create a novel benchmark with augmented\nscenarios to evaluate the quality of the collected data. The results show that\nthe performance of the 2D and 3D diffusion policies can scale with the\ngenerated dataset. Project page is https://openhumanoidgen.github.io.", "comment": "Project Page: https://openhumanoidgen.github.io", "pdf_url": "http://arxiv.org/pdf/2507.00833v1", "categories": ["cs.RO", "cs.AI"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2507.00833v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00951", "title": "Thinking Beyond Tokens: From Brain-Inspired Intelligence to Cognitive Foundations for Artificial General Intelligence and its Societal Impact", "authors": ["Rizwan Qureshi", "Ranjan Sapkota", "Abbas Shah", "Amgad Muneer", "Anas Zafar", "Ashmal Vayani", "Maged Shoman", "Abdelrahman B. M. Eldaly", "Kai Zhang", "Ferhat Sadak", "Shaina Raza", "Xinqi Fan", "Ravid Shwartz-Ziv", "Hong Yan", "Vinjia Jain", "Aman Chadha", "Manoj Karkee", "Jia Wu", "Philip Torr", "Seyedali Mirjalili"], "summary": "Can machines truly think, reason and act in domains like humans? This\nenduring question continues to shape the pursuit of Artificial General\nIntelligence (AGI). Despite the growing capabilities of models such as GPT-4.5,\nDeepSeek, Claude 3.5 Sonnet, Phi-4, and Grok 3, which exhibit multimodal\nfluency and partial reasoning, these systems remain fundamentally limited by\ntheir reliance on token-level prediction and lack of grounded agency. This\npaper offers a cross-disciplinary synthesis of AGI development, spanning\nartificial intelligence, cognitive neuroscience, psychology, generative models,\nand agent-based systems. We analyze the architectural and cognitive foundations\nof general intelligence, highlighting the role of modular reasoning, persistent\nmemory, and multi-agent coordination. In particular, we emphasize the rise of\nAgentic RAG frameworks that combine retrieval, planning, and dynamic tool use\nto enable more adaptive behavior. We discuss generalization strategies,\nincluding information compression, test-time adaptation, and training-free\nmethods, as critical pathways toward flexible, domain-agnostic intelligence.\nVision-Language Models (VLMs) are reexamined not just as perception modules but\nas evolving interfaces for embodied understanding and collaborative task\ncompletion. We also argue that true intelligence arises not from scale alone\nbut from the integration of memory and reasoning: an orchestration of modular,\ninteractive, and self-improving components where compression enables adaptive\nbehavior. Drawing on advances in neurosymbolic systems, reinforcement learning,\nand cognitive scaffolding, we explore how recent architectures begin to bridge\nthe gap between statistical learning and goal-directed cognition. Finally, we\nidentify key scientific, technical, and ethical challenges on the path to AGI.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00951v1", "categories": ["cs.AI"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2507.00951v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00029", "title": "LoRA-Mixer: Coordinate Modular LoRA Experts Through Serial Attention Routing", "authors": ["Wenbing Li", "Zikai Song", "Hang Zhou", "Yunyao Zhang", "Junqing Yu", "Wei Yang"], "summary": "Recent efforts to combine low-rank adaptation (LoRA) with mixture-of-experts\n(MoE) for adapting large language models (LLMs) to multiple tasks still exhibit\nprevailing limitations: they either swap entire attention/feed-forward layers\nfor switch experts or bolt on parallel expert branches, diluting parameter\nefficiency and task fidelity. We propose the LoRA-Mixer, a modular and\nlightweight MoE framework that integrates LoRA experts. Our core innovation\nlies in replacing the projection matrices of the attention module's\ninput/output linear layers with dynamically routed, task-specific LoRA experts.\nThis design ensures seamless compatibility with diverse foundation models,\nincluding transformers and state space models (SSMs), by leveraging their\ninherent linear projection structures. The framework supports two operational\nparadigms: (1) joint optimization of LoRA experts and routing mechanisms via a\nnovel hard-soft routing strategy, or (2) direct deployment of pre-trained,\nfrozen LoRA modules sourced from external repositories. To enable robust router\ntraining with limited data while ensuring stable routing decisions and\nmaximizing expert reuse, we introduce an adaptive Specialization Balance Loss\n(SBL) that jointly optimizes expert balance and task-specific alignment.\nExtensive experiments on seven benchmark datasets, including MedQA, CoLA,\nSST-2, GSM8K, ARC-E, ARC-C, and HumanEval, demonstrate the effectiveness of\nLoRA-Mixer. On datasets such as GSM8K, HumanEval, and MedQA, LoRA-Mixer\nachieves significant improvements of 7.61%, 4.88%, and 3.08% over the base\nmodels, respectively. Compared with state-of-the-art methods, LoRA-Mixer\nachieves additional improvements of 1.09%, 1.45%, and 1.68%, respectively,\nusing only 48% of the parameters, demonstrating its efficiency and strong\nperformance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00029v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00029v1", "date": "2025-06-17", "updated": "2025-06-17"}
{"id": "2507.00253", "title": "GazeTarget360: Towards Gaze Target Estimation in 360-Degree for Robot Perception", "authors": ["Zhuangzhuang Dai", "Vincent Gbouna Zakka", "Luis J. Manso", "Chen Li"], "summary": "Enabling robots to understand human gaze target is a crucial step to allow\ncapabilities in downstream tasks, for example, attention estimation and\nmovement anticipation in real-world human-robot interactions. Prior works have\naddressed the in-frame target localization problem with data-driven approaches\nby carefully removing out-of-frame samples. Vision-based gaze estimation\nmethods, such as OpenFace, do not effectively absorb background information in\nimages and cannot predict gaze target in situations where subjects look away\nfrom the camera. In this work, we propose a system to address the problem of\n360-degree gaze target estimation from an image in generalized visual scenes.\nThe system, named GazeTarget360, integrates conditional inference engines of an\neye-contact detector, a pre-trained vision encoder, and a multi-scale-fusion\ndecoder. Cross validation results show that GazeTarget360 can produce accurate\nand reliable gaze target predictions in unseen scenarios. This makes a\nfirst-of-its-kind system to predict gaze targets from realistic camera footage\nwhich is highly efficient and deployable. Our source code is made publicly\navailable at: https://github.com/zdai257/DisengageNet.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00253v1", "categories": ["cs.CV", "cs.HC"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00253v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00983", "title": "DMCIE: Diffusion Model with Concatenation of Inputs and Errors to Improve the Accuracy of the Segmentation of Brain Tumors in MRI Images", "authors": ["Sara Yavari", "Rahul Nitin Pandya", "Jacob Furst"], "summary": "Accurate segmentation of brain tumors in MRI scans is essential for reliable\nclinical diagnosis and effective treatment planning. Recently, diffusion models\nhave demonstrated remarkable effectiveness in image generation and segmentation\ntasks. This paper introduces a novel approach to corrective segmentation based\non diffusion models. We propose DMCIE (Diffusion Model with Concatenation of\nInputs and Errors), a novel framework for accurate brain tumor segmentation in\nmulti-modal MRI scans. We employ a 3D U-Net to generate an initial segmentation\nmask, from which an error map is generated by identifying the differences\nbetween the prediction and the ground truth. The error map, concatenated with\nthe original MRI images, are used to guide a diffusion model. Using multimodal\nMRI inputs (T1, T1ce, T2, FLAIR), DMCIE effectively enhances segmentation\naccuracy by focusing on misclassified regions, guided by the original inputs.\nEvaluated on the BraTS2020 dataset, DMCIE outperforms several state-of-the-art\ndiffusion-based segmentation methods, achieving a Dice Score of 93.46 and an\nHD95 of 5.94 mm. These results highlight the effectiveness of error-guided\ndiffusion in producing precise and reliable brain tumor segmentations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00983v1", "categories": ["eess.IV", "cs.CV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2507.00983v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00008", "title": "DiMo-GUI: Advancing Test-time Scaling in GUI Grounding via Modality-Aware Visual Reasoning", "authors": ["Hang Wu", "Hongkai Chen", "Yujun Cai", "Chang Liu", "Qingwen Ye", "Ming-Hsuan Yang", "Yiwei Wang"], "summary": "Grounding natural language queries in graphical user interfaces (GUIs) poses\nunique challenges due to the diversity of visual elements, spatial clutter, and\nthe ambiguity of language. In this paper, we introduce DiMo-GUI, a\ntraining-free framework for GUI grounding that leverages two core strategies:\ndynamic visual grounding and modality-aware optimization. Instead of treating\nthe GUI as a monolithic image, our method splits the input into textual\nelements and iconic elements, allowing the model to reason over each modality\nindependently using general-purpose vision-language models. When predictions\nare ambiguous or incorrect, DiMo-GUI dynamically focuses attention by\ngenerating candidate focal regions centered on the model's initial predictions\nand incrementally zooms into subregions to refine the grounding result. This\nhierarchical refinement process helps disambiguate visually crowded layouts\nwithout the need for additional training or annotations. We evaluate our\napproach on standard GUI grounding benchmarks and demonstrate consistent\nimprovements over baseline inference pipelines, highlighting the effectiveness\nof combining modality separation with region-focused reasoning.", "comment": "8 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2507.00008v1", "categories": ["cs.AI", "cs.CV", "cs.HC"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2507.00008v1", "date": "2025-06-12", "updated": "2025-06-12"}
{"id": "2507.00255", "title": "Robustness Analysis for Quantum Systems Controlled by Continuous-Time Pulses", "authors": ["Sean Patrick O'Neil", "Edmond Jonckheere", "Sophie Schirmer"], "summary": "Differential sensitivity techniques originally developed to study the\nrobustness of energy landscape controllers are generalized to the important\ncase of closed quantum systems subject to continuously varying controls.\nVanishing sensitivity to parameter variation is shown to coincide with perfect\nfidelity, as was the case for time-invariant controls. Bounds on the magnitude\nof the differential sensitivity to any parameter variation are derived based\nsimply on knowledge of the system Hamiltonian and the maximum size of the\ncontrol inputs", "comment": "6 pages, 2 figures", "pdf_url": "http://arxiv.org/pdf/2507.00255v1", "categories": ["quant-ph", "cs.SY", "eess.SY"], "cate": "quant-ph", "url": "http://arxiv.org/abs/2507.00255v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00388", "title": "Accuracy and Security-Guaranteed Participant Selection and Beamforming Design for RIS-Assisted Federated Learning", "authors": ["Mengru Wu", "Yu Gao", "Weidang Lu", "Huimei Han", "Lei Sun", "Wanli Ni"], "summary": "Federated learning (FL) has emerged as an effective approach for training\nneural network models without requiring the sharing of participants' raw data,\nthereby addressing data privacy concerns. In this paper, we propose a\nreconfigurable intelligent surface (RIS)-assisted FL framework in the presence\nof eavesdropping, where partial edge devices are selected to participate in the\nFL training process. In contrast, the remaining devices serve as cooperative\njammers by transmitting jamming signals to disrupt eavesdropping. We aim to\nminimize the training latency in each FL round by jointly optimizing\nparticipant selection, bandwidth allocation, and RIS beamforming design,\nsubject to the convergence accuracy of FL and the secure uploading\nrequirements. To solve the resulting mixed-integer nonlinear programming\nproblem, we propose a twin delayed deep deterministic policy gradient (TD3)\nalgorithm. Simulation results demonstrate that the proposed scheme reduces the\nFL training latency by approximately 27$\\%$ compared to baselines.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00388v1", "categories": ["cs.IT", "eess.SP", "math.IT"], "cate": "cs.IT", "url": "http://arxiv.org/abs/2507.00388v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00389", "title": "Causal Prompting for Implicit Sentiment Analysis with Large Language Models", "authors": ["Jing Ren", "Wenhao Zhou", "Bowen Li", "Mujie Liu", "Nguyen Linh Dan Le", "Jiade Cen", "Liping Chen", "Ziqi Xu", "Xiwei Xu", "Xiaodong Li"], "summary": "Implicit Sentiment Analysis (ISA) aims to infer sentiment that is implied\nrather than explicitly stated, requiring models to perform deeper reasoning\nover subtle contextual cues. While recent prompting-based methods using Large\nLanguage Models (LLMs) have shown promise in ISA, they often rely on majority\nvoting over chain-of-thought (CoT) reasoning paths without evaluating their\ncausal validity, making them susceptible to internal biases and spurious\ncorrelations. To address this challenge, we propose CAPITAL, a causal prompting\nframework that incorporates front-door adjustment into CoT reasoning. CAPITAL\ndecomposes the overall causal effect into two components: the influence of the\ninput prompt on the reasoning chains, and the impact of those chains on the\nfinal output. These components are estimated using encoder-based clustering and\nthe NWGM approximation, with a contrastive learning objective used to better\nalign the encoder's representation with the LLM's reasoning space. Experiments\non benchmark ISA datasets with three LLMs demonstrate that CAPITAL consistently\noutperforms strong prompting baselines in both accuracy and robustness,\nparticularly under adversarial conditions. This work offers a principled\napproach to integrating causal inference into LLM prompting and highlights its\nbenefits for bias-aware sentiment reasoning. The source code and case study are\navailable at: https://github.com/whZ62/CAPITAL.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00389v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2507.00389v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00678", "title": "Sectional Kolmogorov N-widths for parameter-dependent function spaces: A general framework with application to parametrized Friedrichs' systems", "authors": ["Christian Engwer", "Mario Ohlberger", "Lukas Renelt"], "summary": "We investigate parametrized variational problems where for each parameter the\nsolution may originate from a different parameter-dependent function space. Our\nmain motivation is the theory of Friedrichs' systems, a large abstract class of\nlinear PDE-problems whose solutions are sought in operator- (and thus\nparameter-)dependent graph spaces. Other applications include function spaces\non parametrized domains or discretizations involving data-dependent\nstabilizers. Concerning the set of all parameter-dependent solutions, we argue\nthat in these cases the interpretation as a \"solution manifold\" widely adopted\nin the model order reduction community is no longer applicable. Instead, we\npropose a novel framework based on the theory of fiber bundles and explain how\nestablished concepts such as approximability generalize by introducing a\nSectional Kolmogorov N-width. Further, we prove exponential approximation rates\nof this N-width if a norm equivalence criterion is fulfilled. Applying this\nresult to problems with Friedrichs' structure then gives a sufficient criterion\nthat can be easily verified.", "comment": "18 pages, 2 figures", "pdf_url": "http://arxiv.org/pdf/2507.00678v1", "categories": ["math.NA", "cs.NA", "65M22, 41A46, 65N30, 65J05"], "cate": "math.NA", "url": "http://arxiv.org/abs/2507.00678v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00690", "title": "Cage-Based Deformation for Transferable and Undefendable Point Cloud Attack", "authors": ["Keke Tang", "Ziyong Du", "Weilong Peng", "Xiaofei Wang", "Peican Zhu", "Ligang Liu", "Zhihong Tian"], "summary": "Adversarial attacks on point clouds often impose strict geometric constraints\nto preserve plausibility; however, such constraints inherently limit\ntransferability and undefendability. While deformation offers an alternative,\nexisting unstructured approaches may introduce unnatural distortions, making\nadversarial point clouds conspicuous and undermining their plausibility. In\nthis paper, we propose CageAttack, a cage-based deformation framework that\nproduces natural adversarial point clouds. It first constructs a cage around\nthe target object, providing a structured basis for smooth, natural-looking\ndeformation. Perturbations are then applied to the cage vertices, which\nseamlessly propagate to the point cloud, ensuring that the resulting\ndeformations remain intrinsic to the object and preserve plausibility.\nExtensive experiments on seven 3D deep neural network classifiers across three\ndatasets show that CageAttack achieves a superior balance among\ntransferability, undefendability, and plausibility, outperforming\nstate-of-the-art methods. Codes will be made public upon acceptance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00690v1", "categories": ["cs.CV", "cs.CR"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00690v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00882", "title": "I Move Therefore I Learn: Experience-Based Traversability in Outdoor Robotics", "authors": ["Miguel Ángel de Miguel", "Jorge Beltrán", "Juan S. Cely", "Francisco Martín", "Juan Carlos Manzanares", "Alberto García"], "summary": "Accurate traversability estimation is essential for safe and effective\nnavigation of outdoor robots operating in complex environments. This paper\nintroduces a novel experience-based method that allows robots to autonomously\nlearn which terrains are traversable based on prior navigation experience,\nwithout relying on extensive pre-labeled datasets. The approach integrates\nelevation and texture data into multi-layered grid maps, which are processed\nusing a variational autoencoder (VAE) trained on a generic texture dataset.\nDuring an initial teleoperated phase, the robot collects sensory data while\nmoving around the environment. These experiences are encoded into compact\nfeature vectors and clustered using the BIRCH algorithm to represent\ntraversable terrain areas efficiently. In deployment, the robot compares new\nterrain patches to its learned feature clusters to assess traversability in\nreal time. The proposed method does not require training with data from the\ntargeted scenarios, generalizes across diverse surfaces and platforms, and\ndynamically adapts as new terrains are encountered. Extensive evaluations on\nboth synthetic benchmarks and real-world scenarios with wheeled and legged\nrobots demonstrate its effectiveness, robustness, and superior adaptability\ncompared to state-of-the-art approaches.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00882v1", "categories": ["cs.RO"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2507.00882v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00979", "title": "Enhancing LLM Agent Safety via Causal Influence Prompting", "authors": ["Dongyoon Hahm", "Woogyeol Jin", "June Suk Choi", "Sungsoo Ahn", "Kimin Lee"], "summary": "As autonomous agents powered by large language models (LLMs) continue to\ndemonstrate potential across various assistive tasks, ensuring their safe and\nreliable behavior is crucial for preventing unintended consequences. In this\nwork, we introduce CIP, a novel technique that leverages causal influence\ndiagrams (CIDs) to identify and mitigate risks arising from agent\ndecision-making. CIDs provide a structured representation of cause-and-effect\nrelationships, enabling agents to anticipate harmful outcomes and make safer\ndecisions. Our approach consists of three key steps: (1) initializing a CID\nbased on task specifications to outline the decision-making process, (2)\nguiding agent interactions with the environment using the CID, and (3)\niteratively refining the CID based on observed behaviors and outcomes.\nExperimental results demonstrate that our method effectively enhances safety in\nboth code execution and mobile device control tasks.", "comment": "Accepted at ACL 2025 Findings, Source code:\n  https://github.com/HahmDY/causal_influence_prompting.git", "pdf_url": "http://arxiv.org/pdf/2507.00979v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2507.00979v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00030", "title": "Adaptive Action Duration with Contextual Bandits for Deep Reinforcement Learning in Dynamic Environments", "authors": ["Abhishek Verma", "Nallarasan V", "Balaraman Ravindran"], "summary": "Deep Reinforcement Learning (DRL) has achieved remarkable success in complex\nsequential decision-making tasks, such as playing Atari 2600 games and\nmastering board games. A critical yet underexplored aspect of DRL is the\ntemporal scale of action execution. We propose a novel paradigm that integrates\ncontextual bandits with DRL to adaptively select action durations, enhancing\npolicy flexibility and computational efficiency. Our approach augments a Deep\nQ-Network (DQN) with a contextual bandit module that learns to choose optimal\naction repetition rates based on state contexts. Experiments on Atari 2600\ngames demonstrate significant performance improvements over static duration\nbaselines, highlighting the efficacy of adaptive temporal abstractions in DRL.\nThis paradigm offers a scalable solution for real-time applications like gaming\nand robotics, where dynamic action durations are critical.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00030v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00030v1", "date": "2025-06-17", "updated": "2025-06-17"}
{"id": "2507.00261", "title": "VirtualFencer: Generating Fencing Bouts based on Strategies Extracted from In-the-Wild Videos", "authors": ["Zhiyin Lin", "Purvi Goel", "Joy Yun", "C. Karen Liu", "Joao Pedro Araujo"], "summary": "Fencing is a sport where athletes engage in diverse yet strategically logical\nmotions. While most motions fall into a few high-level actions (e.g. step,\nlunge, parry), the execution can vary widely-fast vs. slow, large vs. small,\noffensive vs. defensive. Moreover, a fencer's actions are informed by a\nstrategy that often comes in response to the opponent's behavior. This\ncombination of motion diversity with underlying two-player strategy motivates\nthe application of data-driven modeling to fencing. We present VirtualFencer, a\nsystem capable of extracting 3D fencing motion and strategy from in-the-wild\nvideo without supervision, and then using that extracted knowledge to generate\nrealistic fencing behavior. We demonstrate the versatile capabilities of our\nsystem by having it (i) fence against itself (self-play), (ii) fence against a\nreal fencer's motion from online video, and (iii) fence interactively against a\nprofessional fencer.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00261v1", "categories": ["cs.CV", "cs.GR"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00261v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00993", "title": "Advancing Lung Disease Diagnosis in 3D CT Scans", "authors": ["Qingqiu Li", "Runtian Yuan", "Junlin Hou", "Jilan Xu", "Yuejie Zhang", "Rui Feng", "Hao Chen"], "summary": "To enable more accurate diagnosis of lung disease in chest CT scans, we\npropose a straightforward yet effective model. Firstly, we analyze the\ncharacteristics of 3D CT scans and remove non-lung regions, which helps the\nmodel focus on lesion-related areas and reduces computational cost. We adopt\nResNeSt50 as a strong feature extractor, and use a weighted cross-entropy loss\nto mitigate class imbalance, especially for the underrepresented squamous cell\ncarcinoma category. Our model achieves a Macro F1 Score of 0.80 on the\nvalidation set of the Fair Disease Diagnosis Challenge, demonstrating its\nstrong performance in distinguishing between different lung conditions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00993v1", "categories": ["eess.IV", "cs.CV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2507.00993v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00050", "title": "SEZ-HARN: Self-Explainable Zero-shot Human Activity Recognition Network", "authors": ["Devin Y. De Silva", "Sandareka Wickramanayake", "Dulani Meedeniya", "Sanka Rasnayaka"], "summary": "Human Activity Recognition (HAR), which uses data from Inertial Measurement\nUnit (IMU) sensors, has many practical applications in healthcare and assisted\nliving environments. However, its use in real-world scenarios has been limited\nby the lack of comprehensive IMU-based HAR datasets that cover a wide range of\nactivities and the lack of transparency in existing HAR models. Zero-shot HAR\n(ZS-HAR) overcomes the data limitations, but current models struggle to explain\ntheir decisions, making them less transparent. This paper introduces a novel\nIMU-based ZS-HAR model called the Self-Explainable Zero-shot Human Activity\nRecognition Network (SEZ-HARN). It can recognize activities not encountered\nduring training and provide skeleton videos to explain its decision-making\nprocess. We evaluate the effectiveness of the proposed SEZ-HARN on four\nbenchmark datasets PAMAP2, DaLiAc, HTD-MHAD and MHealth and compare its\nperformance against three state-of-the-art black-box ZS-HAR models. The\nexperiment results demonstrate that SEZ-HARN produces realistic and\nunderstandable explanations while achieving competitive Zero-shot recognition\naccuracy. SEZ-HARN achieves a Zero-shot prediction accuracy within 3\\% of the\nbest-performing black-box model on PAMAP2 while maintaining comparable\nperformance on the other three datasets.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00050v1", "categories": ["cs.AI", "cs.HC", "cs.LG", "I.2.0"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2507.00050v1", "date": "2025-06-25", "updated": "2025-06-25"}
{"id": "2507.00268", "title": "Control-Optimized Deep Reinforcement Learning for Artificially Intelligent Autonomous Systems", "authors": ["Oren Fivel", "Matan Rudman", "Kobi Cohen"], "summary": "Deep reinforcement learning (DRL) has become a powerful tool for complex\ndecision-making in machine learning and AI. However, traditional methods often\nassume perfect action execution, overlooking the uncertainties and deviations\nbetween an agent's selected actions and the actual system response. In\nreal-world applications, such as robotics, mechatronics, and communication\nnetworks, execution mismatches arising from system dynamics, hardware\nconstraints, and latency can significantly degrade performance. This work\nadvances AI by developing a novel control-optimized DRL framework that\nexplicitly models and compensates for action execution mismatches, a challenge\nlargely overlooked in existing methods. Our approach establishes a structured\ntwo-stage process: determining the desired action and selecting the appropriate\ncontrol signal to ensure proper execution. It trains the agent while accounting\nfor action mismatches and controller corrections. By incorporating these\nfactors into the training process, the AI agent optimizes the desired action\nwith respect to both the actual control signal and the intended outcome,\nexplicitly considering execution errors. This approach enhances robustness,\nensuring that decision-making remains effective under real-world uncertainties.\nOur approach offers a substantial advancement for engineering practice by\nbridging the gap between idealized learning and real-world implementation. It\nequips intelligent agents operating in engineering environments with the\nability to anticipate and adjust for actuation errors and system disturbances\nduring training. We evaluate the framework in five widely used open-source\nmechanical simulation environments we restructured and developed to reflect\nreal-world operating conditions, showcasing its robustness against\nuncertainties and offering a highly practical and efficient solution for\ncontrol-oriented applications.", "comment": "27 pages, 10 figures", "pdf_url": "http://arxiv.org/pdf/2507.00268v1", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2507.00268v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00654", "title": "Neural Augmented Kalman Filters for Road Network assisted GNSS positioning", "authors": ["Hans van Gorp", "Davide Belli", "Amir Jalalirad", "Bence Major"], "summary": "The Global Navigation Satellite System (GNSS) provides critical positioning\ninformation globally, but its accuracy in dense urban environments is often\ncompromised by multipath and non-line-of-sight errors. Road network data can be\nused to reduce the impact of these errors and enhance the accuracy of a\npositioning system. Previous works employing road network data are either\nlimited to offline applications, or rely on Kalman Filter (KF) heuristics with\nlittle flexibility and robustness. We instead propose training a Temporal Graph\nNeural Network (TGNN) to integrate road network information into a KF. The TGNN\nis designed to predict the correct road segment and its associated uncertainty\nto be used in the measurement update step of the KF. We validate our approach\nwith real-world GNSS data and open-source road networks, observing a 29%\ndecrease in positioning error for challenging scenarios compared to a GNSS-only\nKF. To the best of our knowledge, ours is the first deep learning-based\napproach jointly employing road network data and GNSS measurements to determine\nthe user position on Earth.", "comment": "Accepted to ICML 2025 workshop ML4Wireless", "pdf_url": "http://arxiv.org/pdf/2507.00654v1", "categories": ["cs.LG", "cs.SY", "eess.SP", "eess.SY"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00654v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00439", "title": "Beyond Sociodemographic Prompting: Using Supervision to Align LLMs with Human Response Distributions", "authors": ["Gauri Kambhatla", "Sanjana Gautam", "Angela Zhang", "Alex Liu", "Ravi Srinivasan", "Junyi Jessy Li", "Matthew Lease"], "summary": "The ability to accurately predict how different population groups would\nanswer subjective questions would have great value. In this work, we show that\nuse of relatively simple supervision can greatly improve language model\nalignment with diverse population groups, as measured over three datasets\nspanning various topics. Beyond evaluating average performance, we also report\nhow alignment varies across specific groups. The simplicity and generality of\nour approach promotes easy adoption, while our broad findings provide useful\nguidance for when to use or not use our approach in practice. By conducting\nevaluation over many LLMs and prompting strategies, along with open-sourcing\nour work, we provide a useful benchmark to stimulate future research.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00439v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2507.00439v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00697", "title": "Analysis of A Mixed Finite Element Method for Poisson's Equation with Rough Boundary Data", "authors": ["Huadong Gao", "Yuhui Huang", "Wen Xie"], "summary": "This paper is concerned with finite element methods for Poisson's equation\nwith rough boundary data. Conventional methods require that the boundary data\n$g$ of the problem belongs to $H^{1/2} (\\partial \\Omega)$. However, in many\napplications one has to consider the case when $g$ is in $L^2(\\partial \\Omega)$\nonly. To this end, very weak solutions are considered to establish the\nwell-posedness of the problem. Most previously proposed numerical methods use\nregularizations of the boundary data. The main purpose of this paper is to use\nthe Raviart--Thomas mixed finite element method to solve the Poisson equation\nwith rough boundary data directly. We prove that the solution to the proposed\nmixed method converges to the very weak solution. In particular, we prove that\nthe convergence rate of the numerical solution is $O(h^{1/2})$ in convex\ndomains and $O(h^{s-1/2})$ in nonconvex domains, where $s > 1/2$ depends on the\ngeometry of the domain. The analysis is based on a regularized approach and a\nrigorous estimate for the corresponding dual problem. Numerical experiments\nconfirm the theoretically predicted convergence rates for the proposed mixed\nmethod for Poisson's equation with rough boundary data.", "comment": "18 pages,6 figures", "pdf_url": "http://arxiv.org/pdf/2507.00697v1", "categories": ["math.NA", "cs.NA"], "cate": "math.NA", "url": "http://arxiv.org/abs/2507.00697v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00841", "title": "SafeMobile: Chain-level Jailbreak Detection and Automated Evaluation for Multimodal Mobile Agents", "authors": ["Siyuan Liang", "Tianmeng Fang", "Zhe Liu", "Aishan Liu", "Yan Xiao", "Jinyuan He", "Ee-Chien Chang", "Xiaochun Cao"], "summary": "With the wide application of multimodal foundation models in intelligent\nagent systems, scenarios such as mobile device control, intelligent assistant\ninteraction, and multimodal task execution are gradually relying on such large\nmodel-driven agents. However, the related systems are also increasingly exposed\nto potential jailbreak risks. Attackers may induce the agents to bypass the\noriginal behavioral constraints through specific inputs, and then trigger\ncertain risky and sensitive operations, such as modifying settings, executing\nunauthorized commands, or impersonating user identities, which brings new\nchallenges to system security. Existing security measures for intelligent\nagents still have limitations when facing complex interactions, especially in\ndetecting potentially risky behaviors across multiple rounds of conversations\nor sequences of tasks. In addition, an efficient and consistent automated\nmethodology to assist in assessing and determining the impact of such risks is\ncurrently lacking. This work explores the security issues surrounding mobile\nmultimodal agents, attempts to construct a risk discrimination mechanism by\nincorporating behavioral sequence information, and designs an automated\nassisted assessment scheme based on a large language model. Through preliminary\nvalidation in several representative high-risk tasks, the results show that the\nmethod can improve the recognition of risky behaviors to some extent and assist\nin reducing the probability of agents being jailbroken. We hope that this study\ncan provide some valuable references for the security risk modeling and\nprotection of multimodal intelligent agent systems.", "comment": "12 pages", "pdf_url": "http://arxiv.org/pdf/2507.00841v1", "categories": ["cs.AI", "cs.CR"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2507.00841v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00917", "title": "A Survey: Learning Embodied Intelligence from Physical Simulators and World Models", "authors": ["Xiaoxiao Long", "Qingrui Zhao", "Kaiwen Zhang", "Zihao Zhang", "Dingrui Wang", "Yumeng Liu", "Zhengjie Shu", "Yi Lu", "Shouzheng Wang", "Xinzhe Wei", "Wei Li", "Wei Yin", "Yao Yao", "Jia Pan", "Qiu Shen", "Ruigang Yang", "Xun Cao", "Qionghai Dai"], "summary": "The pursuit of artificial general intelligence (AGI) has placed embodied\nintelligence at the forefront of robotics research. Embodied intelligence\nfocuses on agents capable of perceiving, reasoning, and acting within the\nphysical world. Achieving robust embodied intelligence requires not only\nadvanced perception and control, but also the ability to ground abstract\ncognition in real-world interactions. Two foundational technologies, physical\nsimulators and world models, have emerged as critical enablers in this quest.\nPhysical simulators provide controlled, high-fidelity environments for training\nand evaluating robotic agents, allowing safe and efficient development of\ncomplex behaviors. In contrast, world models empower robots with internal\nrepresentations of their surroundings, enabling predictive planning and\nadaptive decision-making beyond direct sensory input. This survey\nsystematically reviews recent advances in learning embodied AI through the\nintegration of physical simulators and world models. We analyze their\ncomplementary roles in enhancing autonomy, adaptability, and generalization in\nintelligent robots, and discuss the interplay between external simulation and\ninternal modeling in bridging the gap between simulated training and real-world\ndeployment. By synthesizing current progress and identifying open challenges,\nthis survey aims to provide a comprehensive perspective on the path toward more\ncapable and generalizable embodied AI systems. We also maintain an active\nrepository that contains up-to-date literature and open-source projects at\nhttps://github.com/NJU3DV-LoongGroup/Embodied-World-Models-Survey.", "comment": "https://github.com/NJU3DV-LoongGroup/Embodied-World-Models-Survey", "pdf_url": "http://arxiv.org/pdf/2507.00917v1", "categories": ["cs.RO"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2507.00917v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00002", "title": "Hypertokens: Holographic Associative Memory in Tokenized LLMs", "authors": ["Christopher James Augeri"], "summary": "Large Language Models (LLMs) exhibit remarkable capabilities but suffer from\napparent precision loss, reframed here as information spreading. This reframing\nshifts the problem from computational precision to an information-theoretic\ncommunication issue. We address the K:V and V:K memory problem in LLMs by\nintroducing HDRAM (Holographically Defined Random Access Memory), a symbolic\nmemory framework treating transformer latent space as a spread-spectrum\nchannel. Built upon hypertokens, structured symbolic codes integrating\nclassical error-correcting codes (ECC), holographic computing, and\nquantum-inspired search, HDRAM recovers distributed information through\nprincipled despreading. These phase-coherent memory addresses enable efficient\nkey-value operations and Grover-style search in latent space. By combining ECC\ngrammar with compressed sensing and Krylov subspace alignment, HDRAM\nsignificantly improves associative retrieval without architectural changes,\ndemonstrating how Classical-Holographic-Quantum-inspired (CHQ) principles can\nfortify transformer architectures.", "comment": "preprint as accepted to https://qnlp.ai/ - Quantum AI and NLP\n  Conference 2025", "pdf_url": "http://arxiv.org/pdf/2507.00002v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00002v1", "date": "2025-06-02", "updated": "2025-06-02"}
{"id": "2507.00031", "title": "Enhancing Spatio-Temporal Forecasting with Spatial Neighbourhood Fusion:A Case Study on COVID-19 Mobility in Peru", "authors": ["Chuan Li", "Jiang You", "Hassine Moungla", "Vincent Gauthier", "Miguel Nunez-del-Prado", "Hugo Alatrista-Salas"], "summary": "Accurate modeling of human mobility is critical for understanding epidemic\nspread and deploying timely interventions. In this work, we leverage a\nlarge-scale spatio-temporal dataset collected from Peru's national Digital\nContact Tracing (DCT) application during the COVID-19 pandemic to forecast\nmobility flows across urban regions. A key challenge lies in the spatial\nsparsity of hourly mobility counts across hexagonal grid cells, which limits\nthe predictive power of conventional time series models. To address this, we\npropose a lightweight and model-agnostic Spatial Neighbourhood Fusion (SPN)\ntechnique that augments each cell's features with aggregated signals from its\nimmediate H3 neighbors. We evaluate this strategy on three forecasting\nbackbones: NLinear, PatchTST, and K-U-Net, under various historical input\nlengths. Experimental results show that SPN consistently improves forecasting\nperformance, achieving up to 9.85 percent reduction in test MSE. Our findings\ndemonstrate that spatial smoothing of sparse mobility signals provides a simple\nyet effective path toward robust spatio-temporal forecasting during public\nhealth crises.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00031v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00031v1", "date": "2025-06-17", "updated": "2025-06-17"}
{"id": "2507.00263", "title": "Room Scene Discovery and Grouping in Unstructured Vacation Rental Image Collections", "authors": ["Vignesh Ram Nithin Kappagantula", "Shayan Hassantabar"], "summary": "The rapid growth of vacation rental (VR) platforms has led to an increasing\nvolume of property images, often uploaded without structured categorization.\nThis lack of organization poses significant challenges for travelers attempting\nto understand the spatial layout of a property, particularly when multiple\nrooms of the same type are present. To address this issue, we introduce an\neffective approach for solving the room scene discovery and grouping problem,\nas well as identifying bed types within each bedroom group. This grouping is\nvaluable for travelers to comprehend the spatial organization, layout, and the\nsleeping configuration of the property. We propose a computationally efficient\nmachine learning pipeline characterized by low latency and the ability to\nperform effectively with sample-efficient learning, making it well-suited for\nreal-time and data-scarce environments. The pipeline integrates a supervised\nroom-type detection model, a supervised overlap detection model to identify the\noverlap similarity between two images, and a clustering algorithm to group the\nimages of the same space together using the similarity scores. Additionally,\nthe pipeline maps each bedroom group to the corresponding bed types specified\nin the property's metadata, based on the visual content present in the group's\nimages using a Multi-modal Large Language Model (MLLM) model. We evaluate the\naforementioned models individually and also assess the pipeline in its\nentirety, observing strong performance that significantly outperforms\nestablished approaches such as contrastive learning and clustering with\npretrained embeddings.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00263v1", "categories": ["cs.CV", "cs.LG", "cs.NE"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00263v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00006", "title": "MVGBench: Comprehensive Benchmark for Multi-view Generation Models", "authors": ["Xianghui Xie", "Chuhang Zou", "Meher Gitika Karumuri", "Jan Eric Lenssen", "Gerard Pons-Moll"], "summary": "We propose MVGBench, a comprehensive benchmark for multi-view image\ngeneration models (MVGs) that evaluates 3D consistency in geometry and texture,\nimage quality, and semantics (using vision language models). Recently, MVGs\nhave been the main driving force in 3D object creation. However, existing\nmetrics compare generated images against ground truth target views, which is\nnot suitable for generative tasks where multiple solutions exist while\ndiffering from ground truth. Furthermore, different MVGs are trained on\ndifferent view angles, synthetic data and specific lightings -- robustness to\nthese factors and generalization to real data are rarely evaluated thoroughly.\nWithout a rigorous evaluation protocol, it is also unclear what design choices\ncontribute to the progress of MVGs. MVGBench evaluates three different aspects:\nbest setup performance, generalization to real data and robustness. Instead of\ncomparing against ground truth, we introduce a novel 3D self-consistency metric\nwhich compares 3D reconstructions from disjoint generated multi-views. We\nsystematically compare 12 existing MVGs on 4 different curated real and\nsynthetic datasets. With our analysis, we identify important limitations of\nexisting methods specially in terms of robustness and generalization, and we\nfind the most critical design choices. Using the discovered best practices, we\npropose ViFiGen, a method that outperforms all evaluated MVGs on 3D\nconsistency. Our code, model, and benchmark suite will be publicly released.", "comment": "17 pages, 11 figures, 9 tables, project page:\n  https://virtualhumans.mpi-inf.mpg.de/MVGBench/", "pdf_url": "http://arxiv.org/pdf/2507.00006v1", "categories": ["cs.GR", "cs.LG", "eess.IV"], "cate": "cs.GR", "url": "http://arxiv.org/abs/2507.00006v1", "date": "2025-06-11", "updated": "2025-06-11"}
{"id": "2507.00055", "title": "Leveraging Unlabeled Audio-Visual Data in Speech Emotion Recognition using Knowledge Distillation", "authors": ["Varsha Pendyala", "Pedro Morgado", "William Sethares"], "summary": "Voice interfaces integral to the human-computer interaction systems can\nbenefit from speech emotion recognition (SER) to customize responses based on\nuser emotions. Since humans convey emotions through multi-modal audio-visual\ncues, developing SER systems using both the modalities is beneficial. However,\ncollecting a vast amount of labeled data for their development is expensive.\nThis paper proposes a knowledge distillation framework called LightweightSER\n(LiSER) that leverages unlabeled audio-visual data for SER, using large teacher\nmodels built on advanced speech and face representation models. LiSER transfers\nknowledge regarding speech emotions and facial expressions from the teacher\nmodels to lightweight student models. Experiments conducted on two benchmark\ndatasets, RAVDESS and CREMA-D, demonstrate that LiSER can reduce the dependence\non extensive labeled datasets for SER tasks.", "comment": "Accepted at INTERSPEECH 2025", "pdf_url": "http://arxiv.org/pdf/2507.00055v1", "categories": ["cs.LG", "cs.HC", "cs.MM", "eess.AS", "eess.IV", "eess.SP"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00055v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2507.00358", "title": "Data-Driven Exploration for a Class of Continuous-Time Linear--Quadratic Reinforcement Learning Problems", "authors": ["Yilie Huang", "Xun Yu Zhou"], "summary": "We study reinforcement learning (RL) for the same class of continuous-time\nstochastic linear--quadratic (LQ) control problems as in\n\\cite{huang2024sublinear}, where volatilities depend on both states and\ncontrols while states are scalar-valued and running control rewards are absent.\nWe propose a model-free, data-driven exploration mechanism that adaptively\nadjusts entropy regularization by the critic and policy variance by the actor.\nUnlike the constant or deterministic exploration schedules employed in\n\\cite{huang2024sublinear}, which require extensive tuning for implementations\nand ignore learning progresses during iterations, our adaptive exploratory\napproach boosts learning efficiency with minimal tuning. Despite its\nflexibility, our method achieves a sublinear regret bound that matches the\nbest-known model-free results for this class of LQ problems, which were\npreviously derived only with fixed exploration schedules. Numerical experiments\ndemonstrate that adaptive explorations accelerate convergence and improve\nregret performance compared to the non-adaptive model-free and model-based\ncounterparts.", "comment": "36 pages, 10 figures", "pdf_url": "http://arxiv.org/pdf/2507.00358v1", "categories": ["cs.LG", "cs.AI", "cs.SY", "eess.SY", "math.OC"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00358v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00739", "title": "Biorthogonal Tunable Wavelet Unit with Lifting Scheme in Convolutional Neural Network", "authors": ["An Le", "Hung Nguyen", "Sungbal Seo", "You-Suk Bae", "Truong Nguyen"], "summary": "This work introduces a novel biorthogonal tunable wavelet unit constructed\nusing a lifting scheme that relaxes both the orthogonality and equal filter\nlength constraints, providing greater flexibility in filter design. The\nproposed unit enhances convolution, pooling, and downsampling operations,\nleading to improved image classification and anomaly detection in convolutional\nneural networks (CNN). When integrated into an 18-layer residual neural network\n(ResNet-18), the approach improved classification accuracy on CIFAR-10 by 2.12%\nand on the Describable Textures Dataset (DTD) by 9.73%, demonstrating its\neffectiveness in capturing fine-grained details. Similar improvements were\nobserved in ResNet-34. For anomaly detection in the hazelnut category of the\nMVTec Anomaly Detection dataset, the proposed method achieved competitive and\nwellbalanced performance in both segmentation and detection tasks,\noutperforming existing approaches in terms of accuracy and robustness.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00739v1", "categories": ["cs.CV", "eess.IV", "eess.SP"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00739v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00460", "title": "Pitfalls of Evaluating Language Models with Open Benchmarks", "authors": ["Md. Najib Hasan", "Mohammad Fakhruddin Babar", "Souvika Sarkar", "Monowar Hasan", "Santu Karmaker"], "summary": "Open Large Language Model (LLM) benchmarks, such as HELM and BIG-bench, offer\nstandardized, transparent protocols that facilitate the fair comparison,\nreproducibility, and iterative advancement of Language Models (LMs). However,\ntheir openness also introduces critical and underexplored pitfalls. This study\nexposes these weaknesses by systematically constructing ``cheating'' models --\nsmaller variants of BART, T5, and GPT-2 fine-tuned directly on public test sets\n-- which achieve top rankings on a prominent open, holistic benchmark (HELM)\ndespite poor generalization and limited practical utility. Our findings\nunderscore three key insights: \\ca high leaderboard performance on open\nbenchmarks may not always reflect real-world effectiveness; \\cb private or\ndynamic benchmarks must complement open evaluations to safeguard integrity; and\n\\cc a fundamental reevaluation of current benchmarking practices is essential\nto ensure robust and trustworthy LM assessments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00460v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2507.00460v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00723", "title": "Multi-goal-oriented anisotropic error control and mesh adaptivity for time-dependent convection-dominated problems", "authors": ["Markus Bause", "Marius Paul Bruchhäuser", "Bernhard Endtmayer", "Nils Margenberg", "Ioannis Toulopoulos", "Thomas Wick"], "summary": "In this work, we present an anisotropic multi-goal error control based on the\nDual Weighted Residual (DWR) method for time-dependent\nconvection-diffusion-reaction (CDR) equations. This multi-goal oriented\napproach allows for an accurate and efficient error control with regard to\nseveral quantities of interest simultaneously. Using anisotropic interpolation\nand restriction operators, we obtain elementwise error indicators in space and\ntime, where the spatial indicators are additionally separated with respect to\nthe single directions. The directional error indicators quantify anisotropy of\nthe solution with respect to the goals, and produce adaptive, anisotropic\nmeshes that efficiently capture layers. To prevent spurious oscillations the\nstreamline upwind Petrov-Galerkin (SUPG) method is applied to stabilize the\nunderlying system in the case of high P\\'{e}clet numbers. Numerical examples\nshow efficiency and robustness of the proposed approach for several goal\nquantities using established benchmarks for convection-dominated transport.", "comment": "14 pages, 5 Figures, 2 Tables. Submitted to PAMM. arXiv admin note:\n  substantial text overlap with arXiv:2504.04951", "pdf_url": "http://arxiv.org/pdf/2507.00723v1", "categories": ["math.NA", "cs.NA", "65M60, 65M50"], "cate": "math.NA", "url": "http://arxiv.org/abs/2507.00723v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00937", "title": "RaGNNarok: A Light-Weight Graph Neural Network for Enhancing Radar Point Clouds on Unmanned Ground Vehicles", "authors": ["David Hunt", "Shaocheng Luo", "Spencer Hallyburton", "Shafii Nillongo", "Yi Li", "Tingjun Chen", "Miroslav Pajic"], "summary": "Low-cost indoor mobile robots have gained popularity with the increasing\nadoption of automation in homes and commercial spaces. However, existing lidar\nand camera-based solutions have limitations such as poor performance in\nvisually obscured environments, high computational overhead for data\nprocessing, and high costs for lidars. In contrast, mmWave radar sensors offer\na cost-effective and lightweight alternative, providing accurate ranging\nregardless of visibility. However, existing radar-based localization suffers\nfrom sparse point cloud generation, noise, and false detections. Thus, in this\nwork, we introduce RaGNNarok, a real-time, lightweight, and generalizable graph\nneural network (GNN)-based framework to enhance radar point clouds, even in\ncomplex and dynamic environments. With an inference time of just 7.3 ms on the\nlow-cost Raspberry Pi 5, RaGNNarok runs efficiently even on such\nresource-constrained devices, requiring no additional computational resources.\nWe evaluate its performance across key tasks, including localization, SLAM, and\nautonomous navigation, in three different environments. Our results demonstrate\nstrong reliability and generalizability, making RaGNNarok a robust solution for\nlow-cost indoor mobile robots.", "comment": "8 pages, accepted by IROS 2025", "pdf_url": "http://arxiv.org/pdf/2507.00937v1", "categories": ["cs.RO", "cs.AR", "cs.CV", "cs.LG"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2507.00937v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00003", "title": "Deciding When Not to Decide: Indeterminacy-Aware Intrusion Detection with NeutroSENSE", "authors": ["Eyhab Al-Masri"], "summary": "This paper presents NeutroSENSE, a neutrosophic-enhanced ensemble framework\nfor interpretable intrusion detection in IoT environments. By integrating\nRandom Forest, XGBoost, and Logistic Regression with neutrosophic logic, the\nsystem decomposes prediction confidence into truth (T), falsity (F), and\nindeterminacy (I) components, enabling uncertainty quantification and\nabstention. Predictions with high indeterminacy are flagged for review using\nboth global and adaptive, class-specific thresholds. Evaluated on the IoT-CAD\ndataset, NeutroSENSE achieved 97% accuracy, while demonstrating that\nmisclassified samples exhibit significantly higher indeterminacy (I = 0.62)\nthan correct ones (I = 0.24). The use of indeterminacy as a proxy for\nuncertainty enables informed abstention and targeted review-particularly\nvaluable in edge deployments. Figures and tables validate the correlation\nbetween I-scores and error likelihood, supporting more trustworthy,\nhuman-in-the-loop AI decisions. This work shows that neutrosophic logic\nenhances both accuracy and explainability, providing a practical foundation for\ntrust-aware AI in edge and fog-based IoT security systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00003v1", "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.NI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00003v1", "date": "2025-06-05", "updated": "2025-06-05"}
{"id": "2507.00034", "title": "Data Collection with Non-Uniform Axial Power for Phase II of the OECD/NEA AI/ML Critical Heat Flux Benchmark", "authors": ["Reece Bourisaw", "Reid McCants", "Jean-Marie Le Corre", "Anna Iskhakova", "Arsen S. Iskhakov"], "summary": "Critical heat flux (CHF) marks the onset of boiling crisis in light-water\nreactors, defining safe thermal-hydraulic operating limits. To support Phase II\nof the OECD/NEA AI/ML CHF benchmark, which introduces spatially varying power\nprofiles, this work compiles and digitizes a broad CHF dataset covering both\nuniform and non-uniform axial heating conditions. Heating profiles were\nextracted from technical reports, interpolated onto a consistent axial mesh,\nvalidated via energy-balance checks, and encoded in machine-readable formats\nfor benchmark compatibility.\n  Classical CHF correlations exhibit substantial errors under uniform heating\nand degrade markedly when applied to non-uniform profiles, while modern tabular\nmethods offer improved but still imperfect predictions. A neural network\ntrained solely on uniform data performs well in that regime but fails to\ngeneralize to spatially varying scenarios, underscoring the need for models\nthat explicitly incorporate axial power distributions. By providing these\ncurated datasets and baseline modeling results, this study lays the groundwork\nfor advanced transfer-learning strategies, rigorous uncertainty quantification,\nand design-optimization efforts in the next phase of the CHF benchmark.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00034v1", "categories": ["cs.LG", "cs.CE"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00034v1", "date": "2025-06-18", "updated": "2025-06-18"}
{"id": "2507.00287", "title": "Self-Supervised Multiview Xray Matching", "authors": ["Mohamad Dabboussi", "Malo Huard", "Yann Gousseau", "Pietro Gori"], "summary": "Accurate interpretation of multi-view radiographs is crucial for diagnosing\nfractures, muscular injuries, and other anomalies. While significant advances\nhave been made in AI-based analysis of single images, current methods often\nstruggle to establish robust correspondences between different X-ray views, an\nessential capability for precise clinical evaluations. In this work, we present\na novel self-supervised pipeline that eliminates the need for manual annotation\nby automatically generating a many-to-many correspondence matrix between\nsynthetic X-ray views. This is achieved using digitally reconstructed\nradiographs (DRR), which are automatically derived from unannotated CT volumes.\nOur approach incorporates a transformer-based training phase to accurately\npredict correspondences across two or more X-ray views. Furthermore, we\ndemonstrate that learning correspondences among synthetic X-ray views can be\nleveraged as a pretraining strategy to enhance automatic multi-view fracture\ndetection on real data. Extensive evaluations on both synthetic and real X-ray\ndatasets show that incorporating correspondences improves performance in\nmulti-view fracture classification.", "comment": "MICCAI 2025", "pdf_url": "http://arxiv.org/pdf/2507.00287v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00287v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00055", "title": "Leveraging Unlabeled Audio-Visual Data in Speech Emotion Recognition using Knowledge Distillation", "authors": ["Varsha Pendyala", "Pedro Morgado", "William Sethares"], "summary": "Voice interfaces integral to the human-computer interaction systems can\nbenefit from speech emotion recognition (SER) to customize responses based on\nuser emotions. Since humans convey emotions through multi-modal audio-visual\ncues, developing SER systems using both the modalities is beneficial. However,\ncollecting a vast amount of labeled data for their development is expensive.\nThis paper proposes a knowledge distillation framework called LightweightSER\n(LiSER) that leverages unlabeled audio-visual data for SER, using large teacher\nmodels built on advanced speech and face representation models. LiSER transfers\nknowledge regarding speech emotions and facial expressions from the teacher\nmodels to lightweight student models. Experiments conducted on two benchmark\ndatasets, RAVDESS and CREMA-D, demonstrate that LiSER can reduce the dependence\non extensive labeled datasets for SER tasks.", "comment": "Accepted at INTERSPEECH 2025", "pdf_url": "http://arxiv.org/pdf/2507.00055v1", "categories": ["cs.LG", "cs.HC", "cs.MM", "eess.AS", "eess.IV", "eess.SP"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00055v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2507.00224", "title": "Computer Vision for Objects used in Group Work: Challenges and Opportunities", "authors": ["Changsoo Jung", "Sheikh Mannan", "Jack Fitzgerald", "Nathaniel Blanchard"], "summary": "Interactive and spatially aware technologies are transforming educational\nframeworks, particularly in K-12 settings where hands-on exploration fosters\ndeeper conceptual understanding. However, during collaborative tasks, existing\nsystems often lack the ability to accurately capture real-world interactions\nbetween students and physical objects. This issue could be addressed with\nautomatic 6D pose estimation, i.e., estimation of an object's position and\norientation in 3D space from RGB images or videos. For collaborative groups\nthat interact with physical objects, 6D pose estimates allow AI systems to\nrelate objects and entities. As part of this work, we introduce FiboSB, a novel\nand challenging 6D pose video dataset featuring groups of three participants\nsolving an interactive task featuring small hand-held cubes and a weight scale.\nThis setup poses unique challenges for 6D pose because groups are holistically\nrecorded from a distance in order to capture all participants -- this, coupled\nwith the small size of the cubes, makes 6D pose estimation inherently\nnon-trivial. We evaluated four state-of-the-art 6D pose estimation methods on\nFiboSB, exposing the limitations of current algorithms on collaborative group\nwork. An error analysis of these methods reveals that the 6D pose methods'\nobject detection modules fail. We address this by fine-tuning YOLO11-x for\nFiboSB, achieving an overall mAP_50 of 0.898. The dataset, benchmark results,\nand analysis of YOLO11-x errors presented here lay the groundwork for\nleveraging the estimation of 6D poses in difficult collaborative contexts.", "comment": "Accepted to AIED 2025 Late Breaking Results Track", "pdf_url": "http://arxiv.org/pdf/2507.00224v1", "categories": ["cs.CV", "cs.HC"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00224v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00522", "title": "Cyber Attacks Detection, Prevention, and Source Localization in Digital Substation Communication using Hybrid Statistical-Deep Learning", "authors": ["Nicola Cibin", "Bas Mulder", "Herman Carstens", "Peter Palensky", "Alexandru Ştefanov"], "summary": "The digital transformation of power systems is accelerating the adoption of\nIEC 61850 standard. However, its communication protocols, including Sampled\nValues (SV), lack built-in security features such as authentication and\nencryption, making them vulnerable to malicious packet injection. Such cyber\nattacks can delay fault clearance or trigger unintended circuit breaker\noperations. While most existing research focuses on detecting cyber attacks in\ndigital substations, intrusion prevention systems have been disregarded because\nof the risk of potential communication network disruptions. This paper proposes\na novel method using hybrid statistical-deep learning for the detection,\nprevention, and source localization of IEC 61850 SV injection attacks. The\nmethod uses exponentially modified Gaussian distributions to model\ncommunication network latency and long short-term memory and Elman recurrent\nneural network to detect anomalous variations in the estimated probability\ndistributions. It effectively discards malicious SV frames with minimal\nprocessing overhead and latency, maintains robustness against communication\nnetwork latency variation and time-synchronization issues, and guarantees a\nnear-zero false positive rate in non-attack scenarios. Comprehensive validation\nis conducted on three testbeds involving industrial-grade devices,\nhardware-in-the-loop simulations, virtualized intelligent electronic devices\nand merging units, and high-fidelity emulated communication networks. Results\ndemonstrate the method's suitability for practical deployment in IEC\n61850-compliant digital substations.", "comment": "10 pages, 6 figures. This work has been submitted to the IEEE for\n  possible publication", "pdf_url": "http://arxiv.org/pdf/2507.00522v1", "categories": ["cs.CR", "cs.SY", "eess.SY"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2507.00522v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00743", "title": "Tunable Wavelet Unit based Convolutional Neural Network in Optical Coherence Tomography Analysis Enhancement for Classifying Type of Epiretinal Membrane Surgery", "authors": ["An Le", "Nehal Mehta", "William Freeman", "Ines Nagel", "Melanie Tran", "Anna Heinke", "Akshay Agnihotri", "Lingyun Cheng", "Dirk-Uwe Bartsch", "Hung Nguyen", "Truong Nguyen", "Cheolhong An"], "summary": "In this study, we developed deep learning-based method to classify the type\nof surgery performed for epiretinal membrane (ERM) removal, either internal\nlimiting membrane (ILM) removal or ERM-alone removal. Our model, based on the\nResNet18 convolutional neural network (CNN) architecture, utilizes\npostoperative optical coherence tomography (OCT) center scans as inputs. We\nevaluated the model using both original scans and scans preprocessed with\nenergy crop and wavelet denoising, achieving 72% accuracy on preprocessed\ninputs, outperforming the 66% accuracy achieved on original scans. To further\nimprove accuracy, we integrated tunable wavelet units with two key adaptations:\nOrthogonal Lattice-based Wavelet Units (OrthLatt-UwU) and Perfect\nReconstruction Relaxation-based Wavelet Units (PR-Relax-UwU). These units\nallowed the model to automatically adjust filter coefficients during training\nand were incorporated into downsampling, stride-two convolution, and pooling\nlayers, enhancing its ability to distinguish between ERM-ILM removal and\nERM-alone removal, with OrthLattUwU boosting accuracy to 76% and PR-Relax-UwU\nincreasing performance to 78%. Performance comparisons showed that our AI model\noutperformed a trained human grader, who achieved only 50% accuracy in\nclassifying the removal surgery types from postoperative OCT scans. These\nfindings highlight the potential of CNN based models to improve clinical\ndecision-making by providing more accurate and reliable classifications. To the\nbest of our knowledge, this is the first work to employ tunable wavelets for\nclassifying different types of ERM removal surgery.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00743v1", "categories": ["eess.IV", "cs.CV", "eess.SP"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2507.00743v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00509", "title": "TeamCMU at Touché: Adversarial Co-Evolution for Advertisement Integration and Detection in Conversational Search", "authors": ["To Eun Kim", "João Coelho", "Gbemileke Onilude", "Jai Singh"], "summary": "As conversational search engines increasingly adopt generation-based\nparadigms powered by Large Language Models (LLMs) and Retrieval-Augmented\nGeneration (RAG), the integration of advertisements into generated responses\npresents both commercial opportunities and challenges for user experience.\nUnlike traditional search, where advertisements are clearly delineated,\ngenerative systems blur the boundary between informational content and\npromotional material, raising concerns around transparency and trust. In this\nwork, we propose a modular pipeline for advertisement management in RAG-based\nconversational systems, consisting of an ad-rewriter for seamless ad\nintegration and a robust ad-classifier for detection. We leverage synthetic\ndata to train high-performing classifiers, which are then used to guide two\ncomplementary ad-integration strategies: supervised fine-tuning of the\nad-rewriter and a best-of-N sampling approach that selects the least detectable\nad-integrated response among multiple candidates. Our evaluation focuses on two\ncore questions: the effectiveness of ad classifiers in detecting diverse ad\nintegration strategies, and the training methods that best support coherent,\nminimally intrusive ad insertion. Experimental results show that our\nad-classifier, trained on synthetic advertisement data inspired by marketing\nstrategies and enhanced through curriculum learning, achieves robust detection\nperformance. Additionally, we demonstrate that classifier-guided optimization,\nthrough both fine-tuning and best-of-N sampling, significantly improves ad\nstealth, enabling more seamless integration. These findings contribute an\nadversarial co-evolution framework for developing more sophisticated ad-aware\ngenerative search systems and robust ad classifiers.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00509v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2507.00509v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00807", "title": "A posteriori and a priori error estimates for linearized thin sheet folding", "authors": ["Harbir Antil", "Sean P. Carney", "Rohit Khandelwal"], "summary": "We describe a posteriori error analysis for a discontinuous Galerkin method\nfor a fourth order elliptic interface problem that arises from a linearized\nmodel of thin sheet folding. The primary contribution is a local efficiency\nbound for an estimator that measures the extent to which the interface\nconditions along the fold are satisfied, which is accomplished by constructing\na novel edge bubble function. We subsequently conduct a medius analysis to\nobtain improved a priori error estimates under the minimal regularity\nassumption on the exact solution. The performance of the method is illustrated\nby numerical experiments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00807v1", "categories": ["math.NA", "cs.NA"], "cate": "math.NA", "url": "http://arxiv.org/abs/2507.00807v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00984", "title": "Box Pose and Shape Estimation and Domain Adaptation for Large-Scale Warehouse Automation", "authors": ["Xihang Yu", "Rajat Talak", "Jingnan Shi", "Ulrich Viereck", "Igor Gilitschenski", "Luca Carlone"], "summary": "Modern warehouse automation systems rely on fleets of intelligent robots that\ngenerate vast amounts of data -- most of which remains unannotated. This paper\ndevelops a self-supervised domain adaptation pipeline that leverages\nreal-world, unlabeled data to improve perception models without requiring\nmanual annotations. Our work focuses specifically on estimating the pose and\nshape of boxes and presents a correct-and-certify pipeline for self-supervised\nbox pose and shape estimation. We extensively evaluate our approach across a\nrange of simulated and real industrial settings, including adaptation to a\nlarge-scale real-world dataset of 50,000 images. The self-supervised model\nsignificantly outperforms models trained solely in simulation and shows\nsubstantial improvements over a zero-shot 3D bounding box estimation baseline.", "comment": "12 pages, 6 figures. This work will be presented at the 19th\n  International Symposium on Experimental Robotics (ISER2025)", "pdf_url": "http://arxiv.org/pdf/2507.00984v1", "categories": ["cs.RO", "cs.CV", "cs.LG"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2507.00984v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00004", "title": "A Theory of Inference Compute Scaling: Reasoning through Directed Stochastic Skill Search", "authors": ["Austin R. Ellis-Mohr", "Anuj K. Nayak", "Lav R. Varshney"], "summary": "Large language models (LLMs) demand considerable computational, energy, and\nfinancial resources during both training and deployment. While scaling laws for\ntraining have guided much of the field's recent progress, inference costs now\nrepresent a significant and growing component of the overall resource burden,\nparticularly for reasoning-focused models. Existing characterizations of\ncompute-optimality that consider model size, dataset size, and inference tokens\nin isolation or in fixed combinations risk overlooking more efficient operating\npoints. We introduce directed stochastic skill search (DS3), a general\nframework that represents inference as stochastic traversal over a learned\nskill graph. From a simplified yet expressive instantiation, we derive\nclosed-form expressions for task success and compute cost across a wide range\nof inference strategies -- including chain-of-thought (CoT) and tree-of-thought\n(ToT) -- enabling comparative analysis as a function of task difficulty and\nmodel capability. To that end, we extend a prior first-principles tripartite\ngraph framework of LLM training to incorporate inference, and separately bridge\nDS3 with empirical methods that characterize LLM scaling behavior. We\ntheoretically recover empirically observed patterns, including: linear accuracy\nscaling with logarithmic compute; variation in preferred inference strategies\nas a function of task difficulty and model capability; emergent behavior\nelicited by reasoning even when performance plateaus under parameter scaling;\nand both best-of-N (BoN) and majority voting behavior captured within a unified\nanalytical framework. By explicitly characterizing training-inference\ninterdependencies, our framework deepens theoretical understanding and supports\nprincipled algorithmic design and resource allocation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00004v1", "categories": ["cs.LG", "cs.AI", "cs.CY", "cs.PF"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00004v1", "date": "2025-06-10", "updated": "2025-06-10"}
{"id": "2507.00036", "title": "IDRIFTNET: Physics-Driven Spatiotemporal Deep Learning for Iceberg Drift Forecasting", "authors": ["Rohan Putatunda", "Sanjay Purushotham", "Ratnaksha Lele", "Vandana P. Janeja"], "summary": "Drifting icebergs in the polar oceans play a key role in the Earth's climate\nsystem, impacting freshwater fluxes into the ocean and regional ecosystems\nwhile also posing a challenge to polar navigation. However, accurately\nforecasting iceberg trajectories remains a formidable challenge, primarily due\nto the scarcity of spatiotemporal data and the complex, nonlinear nature of\niceberg motion, which is also impacted by environmental variables. The iceberg\nmotion is influenced by multiple dynamic environmental factors, creating a\nhighly variable system that makes trajectory identification complex. These\nlimitations hinder the ability of deep learning models to effectively capture\nthe underlying dynamics and provide reliable predictive outcomes. To address\nthese challenges, we propose a hybrid IDRIFTNET model, a physics-driven deep\nlearning model that combines an analytical formulation of iceberg drift\nphysics, with an augmented residual learning model. The model learns the\npattern of mismatch between the analytical solution and ground-truth\nobservations, which is combined with a rotate-augmented spectral neural network\nthat captures both global and local patterns from the data to forecast future\niceberg drift positions. We compare IDRIFTNET model performance with\nstate-of-the-art models on two Antarctic icebergs: A23A and B22A. Our findings\ndemonstrate that IDRIFTNET outperforms other models by achieving a lower Final\nDisplacement Error (FDE) and Average Displacement Error (ADE) across a variety\nof time points. These results highlight IDRIFTNET's effectiveness in capturing\nthe complex, nonlinear drift of icebergs for forecasting iceberg trajectories\nunder limited data and dynamic environmental conditions.", "comment": "16 pages, 4 figures", "pdf_url": "http://arxiv.org/pdf/2507.00036v1", "categories": ["cs.LG", "physics.ao-ph"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00036v1", "date": "2025-06-18", "updated": "2025-06-18"}
{"id": "2507.00292", "title": "Reducing Variability of Multiple Instance Learning Methods for Digital Pathology", "authors": ["Ali Mammadov", "Loïc Le Folgoc", "Guillaume Hocquet", "Pietro Gori"], "summary": "Digital pathology has revolutionized the field by enabling the digitization\nof tissue samples into whole slide images (WSIs). However, the high resolution\nand large size of WSIs present significant challenges when it comes to applying\nDeep Learning models. As a solution, WSIs are often divided into smaller\npatches with a global label (\\textit{i.e., diagnostic}) per slide, instead of a\n(too) costly pixel-wise annotation. By treating each slide as a bag of patches,\nMultiple Instance Learning (MIL) methods have emerged as a suitable solution\nfor WSI classification. A major drawback of MIL methods is their high\nvariability in performance across different runs, which can reach up to 10-15\nAUC points on the test set, making it difficult to compare different MIL\nmethods reliably. This variability mainly comes from three factors: i) weight\ninitialization, ii) batch (shuffling) ordering, iii) and learning rate. To\naddress that, we introduce a Multi-Fidelity, Model Fusion strategy for MIL\nmethods. We first train multiple models for a few epochs and average the most\nstable and promising ones based on validation scores. This approach can be\napplied to any existing MIL model to reduce performance variability. It also\nsimplifies hyperparameter tuning and improves reproducibility while maintaining\ncomputational efficiency. We extensively validate our approach on WSI\nclassification tasks using 2 different datasets, 3 initialization strategies\nand 5 MIL methods, for a total of more than 2000 experiments.", "comment": "MICCAI 2025", "pdf_url": "http://arxiv.org/pdf/2507.00292v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00292v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00316", "title": "$μ^2$Tokenizer: Differentiable Multi-Scale Multi-Modal Tokenizer for Radiology Report Generation", "authors": ["Siyou Li", "Pengyao Qin", "Huanan Wu", "Dong Nie", "Arun J. Thirunavukarasu", "Juntao Yu", "Le Zhang"], "summary": "Automated radiology report generation (RRG) aims to produce detailed textual\nreports from clinical imaging, such as computed tomography (CT) scans, to\nimprove the accuracy and efficiency of diagnosis and provision of management\nadvice. RRG is complicated by two key challenges: (1) inherent complexity in\nextracting relevant information from imaging data under resource constraints,\nand (2) difficulty in objectively evaluating discrepancies between\nmodel-generated and expert-written reports. To address these challenges, we\npropose $\\mu^2$LLM, a $\\underline{\\textbf{mu}}$ltiscale\n$\\underline{\\textbf{mu}}$ltimodal large language models for RRG tasks. The\nnovel ${\\mu}^2$Tokenizer, as an intermediate layer, integrates multi-modal\nfeatures from the multiscale visual tokenizer and the text tokenizer, then\nenhances report generation quality through direct preference optimization\n(DPO), guided by GREEN-RedLlama. Experimental results on four large CT\nimage-report medical datasetdemonstrate that our method outperforms existing\napproaches, highlighting the potential of our fine-tuned $\\mu^2$LLMs on limited\ndata for RRG tasks.", "comment": "Accepted by MICCAI 2025", "pdf_url": "http://arxiv.org/pdf/2507.00316v1", "categories": ["cs.LG", "cs.CL", "eess.IV"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00316v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00253", "title": "GazeTarget360: Towards Gaze Target Estimation in 360-Degree for Robot Perception", "authors": ["Zhuangzhuang Dai", "Vincent Gbouna Zakka", "Luis J. Manso", "Chen Li"], "summary": "Enabling robots to understand human gaze target is a crucial step to allow\ncapabilities in downstream tasks, for example, attention estimation and\nmovement anticipation in real-world human-robot interactions. Prior works have\naddressed the in-frame target localization problem with data-driven approaches\nby carefully removing out-of-frame samples. Vision-based gaze estimation\nmethods, such as OpenFace, do not effectively absorb background information in\nimages and cannot predict gaze target in situations where subjects look away\nfrom the camera. In this work, we propose a system to address the problem of\n360-degree gaze target estimation from an image in generalized visual scenes.\nThe system, named GazeTarget360, integrates conditional inference engines of an\neye-contact detector, a pre-trained vision encoder, and a multi-scale-fusion\ndecoder. Cross validation results show that GazeTarget360 can produce accurate\nand reliable gaze target predictions in unseen scenarios. This makes a\nfirst-of-its-kind system to predict gaze targets from realistic camera footage\nwhich is highly efficient and deployable. Our source code is made publicly\navailable at: https://github.com/zdai257/DisengageNet.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00253v1", "categories": ["cs.CV", "cs.HC"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00253v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00624", "title": "A Simple Proof of Nehari's Theorem Based on Duality", "authors": ["Cristian R. Rojas"], "summary": "In this technical note we provide a simple proof of Nehari's theorem on the\noptimal approximation by $H_\\infty$ functions, based on convex duality.", "comment": "11 pages", "pdf_url": "http://arxiv.org/pdf/2507.00624v1", "categories": ["math.FA", "cs.SY", "eess.SY"], "cate": "math.FA", "url": "http://arxiv.org/abs/2507.00624v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00856", "title": "Enhancing Vehicular Platooning with Wireless Federated Learning: A Resource-Aware Control Framework", "authors": ["Beining Wu", "Jun Huang", "Qiang Duan", "Liang Dong", "Zhipeng Cai"], "summary": "This paper aims to enhance the performance of Vehicular Platooning (VP)\nsystems integrated with Wireless Federated Learning (WFL). In highly dynamic\nenvironments, vehicular platoons experience frequent communication changes and\nresource constraints, which significantly affect information exchange and\nlearning model synchronization. To address these challenges, we first formulate\nWFL in VP as a joint optimization problem that simultaneously considers Age of\nInformation (AoI) and Federated Learning Model Drift (FLMD) to ensure timely\nand accurate control. Through theoretical analysis, we examine the impact of\nFLMD on convergence performance and develop a two-stage Resource-Aware Control\nframework (RACE). The first stage employs a Lagrangian dual decomposition\nmethod for resource configuration, while the second stage implements a\nmulti-agent deep reinforcement learning approach for vehicle selection. The\napproach integrates Multi-Head Self-Attention and Long Short-Term Memory\nnetworks to capture spatiotemporal correlations in communication states.\nExperimental results demonstrate that, compared to baseline methods, the\nproposed framework improves AoI optimization by up to 45%, accelerates learning\nconvergence, and adapts more effectively to dynamic VP environments on the\nAI4MARS dataset.", "comment": "Under review at IEEE Transactions on Networking", "pdf_url": "http://arxiv.org/pdf/2507.00856v1", "categories": ["cs.NI", "eess.SP"], "cate": "cs.NI", "url": "http://arxiv.org/abs/2507.00856v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00534", "title": "NIRANTAR: Continual Learning with New Languages and Domains on Real-world Speech Data", "authors": ["Tahir Javed", "Kaushal Bhogale", "Mitesh M. Khapra"], "summary": "We introduce Nirantar, a comprehensive framework for evaluating continual\nlearning (CL) in multilingual and multi-domain ASR. Designed to reflect\nreal-world CL challenges, Nirantar leverages data collected incrementally\nacross 22 languages and 208 districts in India through natural episodes. This\nenables evaluation across Language-Incremental (LIL), Domain-Incremental (DIL),\nand the novel Language-Incremental Domain-Incremental Learning (LIDIL)\nscenarios. Unlike prior work that relies on simulated episodes, Nirantar\npresents dynamic, non-uniform language and domain shifts, making it an ideal\ntestbed for CL research. With 3250 hours of human-transcribed speech, including\n1720 hours newly introduced in this work, our framework enables systematic\nbenchmarking of CL methods. We evaluate existing approaches and demonstrate\nthat no single method performs consistently well, underscoring the need for\nmore robust CL strategies.", "comment": "Accepted in Interspecch 2025", "pdf_url": "http://arxiv.org/pdf/2507.00534v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2507.00534v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00065", "title": "Segmentation-Based Regression for Quantum Neural Networks", "authors": ["James C. Hateley"], "summary": "Recent advances in quantum hardware motivate the development of algorithmic\nframeworks that integrate quantum sampling with classical inference. This work\nintroduces a segmentation-based regression method tailored to quantum neural\nnetworks (QNNs), where real-valued outputs are encoded as base-b digit\nsequences and inferred through greedy digitwise optimization. By casting the\nregression task as a constrained combinatorial problem over a structured digit\nlattice, the method replaces continuous inference with interpretable and\ntractable updates. A hybrid quantum-classical architecture is employed: quantum\ncircuits generate candidate digits through projective measurement, while\nclassical forward models evaluate these candidates based on task-specific error\nfunctionals. We formalize the algorithm from first principles, derive\nconvergence and complexity bounds, and demonstrate its effectiveness on inverse\nproblems involving PDE-constrained models. The resulting framework provides a\nrobust, high-precision interface between quantum outputs and continuous\nscientific inference.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00065v1", "categories": ["quant-ph", "cs.NA", "math.NA"], "cate": "quant-ph", "url": "http://arxiv.org/abs/2507.00065v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2507.00990", "title": "Robotic Manipulation by Imitating Generated Videos Without Physical Demonstrations", "authors": ["Shivansh Patel", "Shraddhaa Mohan", "Hanlin Mai", "Unnat Jain", "Svetlana Lazebnik", "Yunzhu Li"], "summary": "This work introduces Robots Imitating Generated Videos (RIGVid), a system\nthat enables robots to perform complex manipulation tasks--such as pouring,\nwiping, and mixing--purely by imitating AI-generated videos, without requiring\nany physical demonstrations or robot-specific training. Given a language\ncommand and an initial scene image, a video diffusion model generates potential\ndemonstration videos, and a vision-language model (VLM) automatically filters\nout results that do not follow the command. A 6D pose tracker then extracts\nobject trajectories from the video, and the trajectories are retargeted to the\nrobot in an embodiment-agnostic fashion. Through extensive real-world\nevaluations, we show that filtered generated videos are as effective as real\ndemonstrations, and that performance improves with generation quality. We also\nshow that relying on generated videos outperforms more compact alternatives\nsuch as keypoint prediction using VLMs, and that strong 6D pose tracking\noutperforms other ways to extract trajectories, such as dense feature point\ntracking. These findings suggest that videos produced by a state-of-the-art\noff-the-shelf model can offer an effective source of supervision for robotic\nmanipulation.", "comment": "Project Page: https://rigvid-robot.github.io/", "pdf_url": "http://arxiv.org/pdf/2507.00990v1", "categories": ["cs.RO", "cs.AI", "cs.CV"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2507.00990v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00007", "title": "Integrating Universal Generative AI Platforms in Educational Labs to Foster Critical Thinking and Digital Literacy", "authors": ["Vasiliy Znamenskiy", "Rafael Niyazov", "Joel Hernandez"], "summary": "This paper presents a new educational framework for integrating generative\nartificial intelligence (GenAI) platforms such as ChatGPT, Claude, and Gemini\ninto laboratory activities aimed at developing critical thinking and digital\nliteracy among undergraduate students. Recognizing the limitations and risks of\nuncritical reliance on large language models (LLMs), the proposed pedagogical\nmodel reframes GenAI as a research subject and cognitive tool. Students\nformulate discipline-specific prompts and evaluate GenAI-generated responses in\ntext, image, and video modalities. A pilot implementation in a general\nastronomy course for non-science majors demonstrated high levels of engagement\nand critical reflection, with many students continuing the activity after class\nand presenting results at a research symposium. The results highlight the\nimportance of structured AI interactions in education and suggest that GenAI\ncan improve learning outcomes when combined with reflective assessment methods.\nThe study proposes a replicable model for interdisciplinary AI-integrated lab\nwork, adaptable to scientific disciplines. See the guide to learning activities\nbased on Generative-Ai platforms: https://doi.org/10.5281/zenodo.15555802", "comment": "http://doi.org/10.5121/ijci.2025.140302", "pdf_url": "http://arxiv.org/pdf/2507.00007v1", "categories": ["cs.CY", "cs.AI", "cs.LG", "68T50, 68U20, 97U50, 97D40", "I.2.7; K.3.1; K.3.2; H.5.3"], "cate": "cs.CY", "url": "http://arxiv.org/abs/2507.00007v1", "date": "2025-06-11", "updated": "2025-06-11"}
{"id": "2507.00037", "title": "Model Fusion via Neuron Interpolation", "authors": ["Phoomraphee Luenam", "Andreas Spanopoulos", "Amit Sant", "Thomas Hofmann", "Sotiris Anagnostidis", "Sidak Pal Singh"], "summary": "Model fusion aims to combine the knowledge of multiple models by creating one\nrepresentative model that captures the strengths of all of its parents.\nHowever, this process is non-trivial due to differences in internal\nrepresentations, which can stem from permutation invariance, random\ninitialization, or differently distributed training data. We present a novel,\nneuron-centric family of model fusion algorithms designed to integrate multiple\ntrained neural networks into a single network effectively regardless of\ntraining data distribution. Our algorithms group intermediate neurons of parent\nmodels to create target representations that the fused model approximates with\nits corresponding sub-network. Unlike prior approaches, our approach\nincorporates neuron attribution scores into the fusion process. Furthermore,\nour algorithms can generalize to arbitrary layer types. Experimental results on\nvarious benchmark datasets demonstrate that our algorithms consistently\noutperform previous fusion techniques, particularly in zero-shot and non-IID\nfusion scenarios. The code is available at\nhttps://github.com/AndrewSpano/neuron-interpolation-model-fusion.", "comment": "5 figures, 15 tables, 23 pages", "pdf_url": "http://arxiv.org/pdf/2507.00037v1", "categories": ["cs.LG", "cs.AI", "I.2.6; I.2.1"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00037v1", "date": "2025-06-18", "updated": "2025-06-18"}
{"id": "2507.00327", "title": "Beyond Low-Rank Tuning: Model Prior-Guided Rank Allocation for Effective Transfer in Low-Data and Large-Gap Regimes", "authors": ["Chuyan Zhang", "Kefan Wang", "Yun Gu"], "summary": "Low-Rank Adaptation (LoRA) has proven effective in reducing computational\ncosts while maintaining performance comparable to fully fine-tuned foundation\nmodels across various tasks. However, its fixed low-rank structure restricts\nits adaptability in scenarios with substantial domain gaps, where higher ranks\nare often required to capture domain-specific complexities. Current adaptive\nLoRA methods attempt to overcome this limitation by dynamically expanding or\nselectively allocating ranks, but these approaches frequently depend on\ncomputationally intensive techniques such as iterative pruning, rank searches,\nor additional regularization. To address these challenges, we introduce Stable\nRank-Guided Low-Rank Adaptation (SR-LoRA), a novel framework that utilizes the\nstable rank of pre-trained weight matrices as a natural prior for layer-wise\nrank allocation. By leveraging the stable rank, which reflects the intrinsic\ndimensionality of the weights, SR-LoRA enables a principled and efficient\nredistribution of ranks across layers, enhancing adaptability without incurring\nadditional search costs. Empirical evaluations on few-shot tasks with\nsignificant domain gaps show that SR-LoRA consistently outperforms recent\nadaptive LoRA variants, achieving a superior trade-off between performance and\nefficiency. Our code is available at\nhttps://github.com/EndoluminalSurgicalVision-IMR/SR-LoRA.", "comment": "Accepted by ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.00327v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00327v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00333", "title": "Scope Meets Screen: Lessons Learned in Designing Composite Visualizations for Marksmanship Training Across Skill Levels", "authors": ["Emin Zerman", "Jonas Carlsson", "Mårten Sjöström"], "summary": "Marksmanship practices are required in various professions, including police,\nmilitary personnel, hunters, as well as sports shooters, such as Olympic\nshooting, biathlon, and modern pentathlon. The current form of training and\ncoaching is mostly based on repetition, where the coach does not see through\nthe eyes of the shooter, and analysis is limited to stance and accuracy\npost-session. In this study, we present a shooting visualization system and\nevaluate its perceived effectiveness for both novice and expert shooters. To\nachieve this, five composite visualizations were developed using first-person\nshooting video recordings enriched with overlaid metrics and graphical\nsummaries. These views were evaluated with 10 participants (5 expert marksmen,\n5 novices) through a mixed-methods study including shot-count and aiming\ninterpretation tasks, pairwise preference comparisons, and semi-structured\ninterviews. The results show that a dashboard-style composite view, combining\nraw video with a polar plot and selected graphs, was preferred in 9 of 10 cases\nand supported understanding across skill levels. The insights gained from this\ndesign study point to the broader value of integrating first-person video with\nvisual analytics for coaching, and we suggest directions for applying this\napproach to other precision-based sports.", "comment": "5 pages, accepted at IEEE VIS 2025", "pdf_url": "http://arxiv.org/pdf/2507.00333v1", "categories": ["cs.HC", "cs.CV", "cs.GR", "eess.IV"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2507.00333v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00456", "title": "Teacher-AI Collaboration for Curating and Customizing Lesson Plans in Low-Resource Schools", "authors": ["Deepak Varuvel Dennison", "Bakhtawar Ahtisham", "Kavyansh Chourasia", "Nirmit Arora", "Rahul Singh", "Rene F. Kizilcec", "Akshay Nambi", "Tanuja Ganu", "Aditya Vashistha"], "summary": "This study investigates Shiksha copilot, an AI-assisted lesson planning tool\ndeployed in government schools across Karnataka, India. The system combined\nLLMs and human expertise through a structured process in which English and\nKannada lesson plans were co-created by curators and AI; teachers then further\ncustomized these curated plans for their classrooms using their own expertise\nalongside AI support. Drawing on a large-scale mixed-methods study involving\n1,043 teachers and 23 curators, we examine how educators collaborate with AI to\ngenerate context-sensitive lesson plans, assess the quality of AI-generated\ncontent, and analyze shifts in teaching practices within multilingual,\nlow-resource environments. Our findings show that teachers used Shiksha copilot\nboth to meet administrative documentation needs and to support their teaching.\nThe tool eased bureaucratic workload, reduced lesson planning time, and lowered\nteaching-related stress, while promoting a shift toward activity-based\npedagogy. However, systemic challenges such as staffing shortages and\nadministrative demands constrained broader pedagogical change. We frame these\nfindings through the lenses of teacher-AI collaboration and communities of\npractice to examine the effective integration of AI tools in teaching. Finally,\nwe propose design directions for future teacher-centered EdTech, particularly\nin multilingual and Global South contexts.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00456v1", "categories": ["cs.CY", "cs.HC"], "cate": "cs.CY", "url": "http://arxiv.org/abs/2507.00456v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00654", "title": "Neural Augmented Kalman Filters for Road Network assisted GNSS positioning", "authors": ["Hans van Gorp", "Davide Belli", "Amir Jalalirad", "Bence Major"], "summary": "The Global Navigation Satellite System (GNSS) provides critical positioning\ninformation globally, but its accuracy in dense urban environments is often\ncompromised by multipath and non-line-of-sight errors. Road network data can be\nused to reduce the impact of these errors and enhance the accuracy of a\npositioning system. Previous works employing road network data are either\nlimited to offline applications, or rely on Kalman Filter (KF) heuristics with\nlittle flexibility and robustness. We instead propose training a Temporal Graph\nNeural Network (TGNN) to integrate road network information into a KF. The TGNN\nis designed to predict the correct road segment and its associated uncertainty\nto be used in the measurement update step of the KF. We validate our approach\nwith real-world GNSS data and open-source road networks, observing a 29%\ndecrease in positioning error for challenging scenarios compared to a GNSS-only\nKF. To the best of our knowledge, ours is the first deep learning-based\napproach jointly employing road network data and GNSS measurements to determine\nthe user position on Earth.", "comment": "Accepted to ICML 2025 workshop ML4Wireless", "pdf_url": "http://arxiv.org/pdf/2507.00654v1", "categories": ["cs.LG", "cs.SY", "eess.SP", "eess.SY"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00654v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00902", "title": "Constellation as a Service: Tailored Connectivity Management in Direct-Satellite-to-Device Networks", "authors": ["Feng Wang", "Shengyu Zhang", "Een-Kee Hong", "Tony Q. S. Quek"], "summary": "Direct-satellite-to-device (DS2D) communication is emerging as a promising\nsolution for global mobile service extension, leveraging the deployment of\nsatellite constellations. However, the challenge of managing DS2D connectivity\nfor multi-constellations becomes outstanding, including high interference and\nfrequent handovers caused by multi-coverage overlap and rapid satellite\nmovement. Moreover, existing approaches primarily operate within\nsingle-constellation shell, which inherently limits the ability to exploit the\nvast potential of multi-constellation connectivity provision, resulting in\nsuboptimal DS2D service performances. To address these challenges, this article\nproposes a Constellation as a Service (CaaS) framework, which treats the entire\nmulti-constellation infrastructure as a shared resource pool and dynamically\nforms optimal sub-constellations (SCs) for each DS2D service region. The\nformation of each SC integrates satellites from various orbits to provide\ntailored connectivity based on user demands, guided by two innovative\nstrategies: predictive satellite beamforming using generative artificial\nintelligence (GenAI) and pre-configured handover path for efficient satellite\naccess and mobility management. Simulation results demonstrate that CaaS\nsignificantly improves satellite service rates while reducing handover\noverhead, making it an efficient and continuable solution for managing DS2D\nconnectivity in multi-constellation environments.", "comment": "To appear in IEEE Communications Magazine", "pdf_url": "http://arxiv.org/pdf/2507.00902v1", "categories": ["eess.SY", "cs.AI", "cs.SY", "eess.SP"], "cate": "eess.SY", "url": "http://arxiv.org/abs/2507.00902v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00540", "title": "Capsule Network-Based Semantic Intent Modeling for Human-Computer Interaction", "authors": ["Shixiao Wang", "Yifan Zhuang", "Runsheng Zhang", "Zhijun Song"], "summary": "This paper proposes a user semantic intent modeling algorithm based on\nCapsule Networks to address the problem of insufficient accuracy in intent\nrecognition for human-computer interaction. The method represents semantic\nfeatures in input text through a vectorized capsule structure. It uses a\ndynamic routing mechanism to transfer information across multiple capsule\nlayers. This helps capture hierarchical relationships and part-whole structures\nbetween semantic entities more effectively. The model uses a convolutional\nfeature extraction module as the low-level encoder. After generating initial\nsemantic capsules, it forms high-level abstract intent representations through\nan iterative routing process. To further enhance performance, a margin-based\nmechanism is introduced into the loss function. This improves the model's\nability to distinguish between intent classes. Experiments are conducted using\na public natural language understanding dataset. Multiple mainstream models are\nused for comparison. Results show that the proposed model outperforms\ntraditional methods and other deep learning structures in terms of accuracy,\nF1-score, and intent detection rate. The study also analyzes the effect of the\nnumber of dynamic routing iterations on model performance. A convergence curve\nof the loss function during training is provided. These results verify the\nstability and effectiveness of the proposed method in semantic modeling.\nOverall, this study presents a new structured modeling approach to improve\nintent recognition under complex semantic conditions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00540v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2507.00540v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00301", "title": "Structure-preserving Lift & Learn: Scientific machine learning for nonlinear conservative partial differential equations", "authors": ["Harsh Sharma", "Juan Diego Draxl Giannoni", "Boris Kramer"], "summary": "This work presents structure-preserving Lift & Learn, a scientific machine\nlearning method that employs lifting variable transformations to learn\nstructure-preserving reduced-order models for nonlinear partial differential\nequations (PDEs) with conservation laws. We propose a hybrid learning approach\nbased on a recently developed energy-quadratization strategy that uses\nknowledge of the nonlinearity at the PDE level to derive an equivalent\nquadratic lifted system with quadratic system energy. The lifted dynamics\nobtained via energy quadratization are linear in the old variables, making\nmodel learning very effective in the lifted setting. Based on the lifted\nquadratic PDE model form, the proposed method derives quadratic reduced terms\nanalytically and then uses those derived terms to formulate a constrained\noptimization problem to learn the remaining linear reduced operators in a\nstructure-preserving way. The proposed hybrid learning approach yields\ncomputationally efficient quadratic reduced-order models that respect the\nunderlying physics of the high-dimensional problem. We demonstrate the\ngeneralizability of quadratic models learned via the proposed\nstructure-preserving Lift & Learn method through three numerical examples: the\none-dimensional wave equation with exponential nonlinearity, the\ntwo-dimensional sine-Gordon equation, and the two-dimensional\nKlein-Gordon-Zakharov equations. The numerical results show that the proposed\nlearning approach is competitive with the state-of-the-art structure-preserving\ndata-driven model reduction method in terms of both accuracy and computational\nefficiency.", "comment": "arXiv admin note: substantial text overlap with arXiv:2503.02273", "pdf_url": "http://arxiv.org/pdf/2507.00301v1", "categories": ["cs.LG", "cs.NA", "math.NA"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00301v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.01008", "title": "DexWrist: A Robotic Wrist for Constrained and Dynamic Manipulation", "authors": ["Martin Peticco", "Gabriella Ulloa", "John Marangola", "Pulkit Agrawal"], "summary": "We present the DexWrist, a compliant robotic wrist designed to advance\nrobotic manipulation in highly-constrained environments, enable dynamic tasks,\nand speed up data collection. DexWrist is designed to be close to the\nfunctional capabilities of the human wrist and achieves mechanical compliance\nand a greater workspace as compared to existing robotic wrist designs. The\nDexWrist can supercharge policy learning by (i) enabling faster teleoperation\nand therefore making data collection more scalable; (ii) completing tasks in\nfewer steps which reduces trajectory lengths and therefore can ease policy\nlearning; (iii) DexWrist is designed to be torque transparent with easily\nsimulatable kinematics for simulated data collection; and (iv) most importantly\nexpands the workspace of manipulation for approaching highly cluttered scenes\nand tasks. More details about the wrist can be found at:\ndexwrist.csail.mit.edu.", "comment": "More details about the wrist can be found at: dexwrist.csail.mit.edu", "pdf_url": "http://arxiv.org/pdf/2507.01008v1", "categories": ["cs.RO"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2507.01008v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00011", "title": "Novel RL approach for efficient Elevator Group Control Systems", "authors": ["Nathan Vaartjes", "Vincent Francois-Lavet"], "summary": "Efficient elevator traffic management in large buildings is critical for\nminimizing passenger travel times and energy consumption. Because heuristic- or\npattern-detection-based controllers struggle with the stochastic and\ncombinatorial nature of dispatching, we model the six-elevator, fifteen-floor\nsystem at Vrije Universiteit Amsterdam as a Markov Decision Process and train\nan end-to-end Reinforcement Learning (RL) Elevator Group Control System (EGCS).\nKey innovations include a novel action space encoding to handle the\ncombinatorial complexity of elevator dispatching, the introduction of\ninfra-steps to model continuous passenger arrivals, and a tailored reward\nsignal to improve learning efficiency. In addition, we explore various ways to\nadapt the discounting factor to the infra-step formulation. We investigate RL\narchitectures based on Dueling Double Deep Q-learning, showing that the\nproposed RL-based EGCS adapts to fluctuating traffic patterns, learns from a\nhighly stochastic environment, and thereby outperforms a traditional rule-based\nalgorithm.", "comment": "15 pages, 12 figures", "pdf_url": "http://arxiv.org/pdf/2507.00011v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00011v1", "date": "2025-06-12", "updated": "2025-06-12"}
{"id": "2507.00038", "title": "Quality over Quantity: An Effective Large-Scale Data Reduction Strategy Based on Pointwise V-Information", "authors": ["Fei Chen", "Wenchi Zhou"], "summary": "Data reduction plays a vital role in data-centric AI by identifying the most\ninformative instance within large-scale datasets to enhance model training\nefficiency. The core challenge lies in how to select the optimal\ninstances-rather than the entire datasets-to improve data quality and training\nefficiency. In this paper, we propose an effective data reduction strategy\nbased on Pointwise V-information(PVI). First, we quantify instance difficulty\nusing PVI and filter out low-difficulty instances enabling a static approach.\nExperiments demonstrate that removing 10%-30% of the data preserves the\nclassifier performance with only a 0.0001% to 0.76% loss in accuracy.Second, we\nuse a progressive learning approach to training the classifiers on instances\nsorted by ascending PVI, accelerating convergence and achieving a 0.8% accuracy\ngain over conventional training. Our results suggest that with the effective\ndata reduction strategy, training a classifier on the selected optimal subset\ncould enhance the model performance and boost training efficiency. Moreover, we\nhave transferred the PVI framework, which previously applied only to English\ndatasets, to diverse Chinese NLP tasks and base models, leading to valuable\ninsights for cross-lingual data reduction and faster training. The codes are\nreleased at https://github.com/zhouwenchi/DatasetReductionStrategy.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00038v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00038v1", "date": "2025-06-19", "updated": "2025-06-19"}
{"id": "2507.00328", "title": "MammoTracker: Mask-Guided Lesion Tracking in Temporal Mammograms", "authors": ["Xuan Liu", "Yinhao Ren", "Marc D. Ryser", "Lars J. Grimm", "Joseph Y. Lo"], "summary": "Accurate lesion tracking in temporal mammograms is essential for monitoring\nbreast cancer progression and facilitating early diagnosis. However, automated\nlesion correspondence across exams remains a challenges in computer-aided\ndiagnosis (CAD) systems, limiting their effectiveness. We propose MammoTracker,\na mask-guided lesion tracking framework that automates lesion localization\nacross consecutively exams. Our approach follows a coarse-to-fine strategy\nincorporating three key modules: global search, local search, and score\nrefinement. To support large-scale training and evaluation, we introduce a new\ndataset with curated prior-exam annotations for 730 mass and calcification\ncases from the public EMBED mammogram dataset, yielding over 20000 lesion\npairs, making it the largest known resource for temporal lesion tracking in\nmammograms. Experimental results demonstrate that MammoTracker achieves 0.455\naverage overlap and 0.509 accuracy, surpassing baseline models by 8%,\nhighlighting its potential to enhance CAD-based lesion progression analysis.\nOur dataset will be available at\nhttps://gitlab.oit.duke.edu/railabs/LoGroup/mammotracker.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00328v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00328v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00365", "title": "An Improved U-Net Model for Offline handwriting signature denoising", "authors": ["Wanghui Xiao"], "summary": "Handwriting signatures, as an important means of identity recognition, are\nwidely used in multiple fields such as financial transactions, commercial\ncontracts and personal affairs due to their legal effect and uniqueness. In\nforensic science appraisals, the analysis of offline handwriting signatures\nrequires the appraiser to provide a certain number of signature samples, which\nare usually derived from various historical contracts or archival materials.\nHowever, the provided handwriting samples are often mixed with a large amount\nof interfering information, which brings severe challenges to handwriting\nidentification work. This study proposes a signature handwriting denoising\nmodel based on the improved U-net structure, aiming to enhance the robustness\nof the signature recognition system. By introducing discrete wavelet transform\nand PCA transform, the model's ability to suppress noise has been enhanced. The\nexperimental results show that this modelis significantly superior to the\ntraditional methods in denoising effect, can effectively improve the clarity\nand readability of the signed images, and provide more reliable technical\nsupport for signature analysis and recognition.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00365v1", "categories": ["cs.CV", "eess.IV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00365v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00481", "title": "The Influence of HEXACO Personality Traits on the Teamwork Quality in Software Teams -- A Preliminary Research Approach", "authors": ["Philipp M. Zähl", "Sabine Theis", "Martin R. Wolf"], "summary": "Although software engineering research has focused on optimizing processes\nand technology, there is a growing recognition that human factors, particularly\nteamwork, also significantly impact optimization. Recent research suggests that\ndeveloper personality has a strong influence on teamwork. In fact, personality\nconsiderations may have a greater impact on software development than processes\nand tools. This paper aims to design a study that measures the impact of HEXACO\npersonality traits on the Teamwork Quality (TWQ) of software teams. A\npreliminary data collection (n=54) was conducted for this purpose. The analysis\nshowed that several personality traits, as well as their composition, had a\nsignificant impact on TWQ. Additionally, other variables, such as the\nproportion of women and age distribution, also affected TWQ. The study's\ninitial results demonstrate the usefulness and validity of the study design.\nThe results also suggest several opportunities to improve teamwork in IT\norganizations and avenues for further research.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00481v1", "categories": ["cs.SE", "cs.HC"], "cate": "cs.SE", "url": "http://arxiv.org/abs/2507.00481v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00695", "title": "A Test-Function Approach to Incremental Stability", "authors": ["Daniel Pfrommer", "Max Simchowitz", "Ali Jadbabaie"], "summary": "This paper presents a novel framework for analyzing\nIncremental-Input-to-State Stability ($\\delta$ISS) based on the idea of using\nrewards as \"test functions.\" Whereas control theory traditionally deals with\nLyapunov functions that satisfy a time-decrease condition, reinforcement\nlearning (RL) value functions are constructed by exponentially decaying a\nLipschitz reward function that may be non-smooth and unbounded on both sides.\nThus, these RL-style value functions cannot be directly understood as Lyapunov\ncertificates. We develop a new equivalence between a variant of incremental\ninput-to-state stability of a closed-loop system under given a policy, and the\nregularity of RL-style value functions under adversarial selection of a\nH\\\"older-continuous reward function. This result highlights that the regularity\nof value functions, and their connection to incremental stability, can be\nunderstood in a way that is distinct from the traditional Lyapunov-based\napproach to certifying stability in control theory.", "comment": "8 pages", "pdf_url": "http://arxiv.org/pdf/2507.00695v1", "categories": ["cs.LG", "cs.SY", "eess.SY"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00695v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00920", "title": "Privacy-Preserving Quantized Federated Learning with Diverse Precision", "authors": ["Dang Qua Nguyen", "Morteza Hashemi", "Erik Perrins", "Sergiy A. Vorobyov", "David J. Love", "Taejoon Kim"], "summary": "Federated learning (FL) has emerged as a promising paradigm for distributed\nmachine learning, enabling collaborative training of a global model across\nmultiple local devices without requiring them to share raw data. Despite its\nadvancements, FL is limited by factors such as: (i) privacy risks arising from\nthe unprotected transmission of local model updates to the fusion center (FC)\nand (ii) decreased learning utility caused by heterogeneity in model\nquantization resolution across participating devices. Prior work typically\naddresses only one of these challenges because maintaining learning utility\nunder both privacy risks and quantization heterogeneity is a non-trivial task.\nIn this paper, our aim is therefore to improve the learning utility of a\nprivacy-preserving FL that allows clusters of devices with different\nquantization resolutions to participate in each FL round. Specifically, we\nintroduce a novel stochastic quantizer (SQ) that is designed to simultaneously\nachieve differential privacy (DP) and minimum quantization error. Notably, the\nproposed SQ guarantees bounded distortion, unlike other DP approaches. To\naddress quantization heterogeneity, we introduce a cluster size optimization\ntechnique combined with a linear fusion approach to enhance model aggregation\naccuracy. Numerical simulations validate the benefits of our approach in terms\nof privacy protection and learning utility compared to the conventional\nLaplaceSQ-FL algorithm.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00920v1", "categories": ["cs.LG", "eess.SP"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00920v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00547", "title": "Methodological Rigour in Algorithm Application: An Illustration of Topic Modelling Algorithm", "authors": ["Malmi Amadoru"], "summary": "The rise of advanced computational algorithms has opened new avenues for\ncomputationally intensive research approaches to theory development. However,\nthe opacity of these algorithms and lack of transparency and rigour in their\napplication pose methodological challenges, potentially undermining trust in\nresearch. The discourse on methodological rigour in this new genre of research\nis still emerging. Against this backdrop, I attempt to offer guidance on\nmethodological rigour, particularly in the context of topic modelling\nalgorithms. By illustrating the application of the structural topic modelling\nalgorithm and presenting a set of guidelines, I discuss how to ensure rigour in\ntopic modelling studies. Although the guidelines are for the application of\ntopic modelling algorithms, they can be applied to other algorithms with\ncontext-specific adjustments. The guidelines are helpful, especially for novice\nresearchers applying topic modelling, and editors and reviewers handling topic\nmodelling manuscripts. I contribute to the literature on topic modelling and\njoin the emerging dialogue on methodological rigour in computationally\nintensive theory construction research.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00547v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2507.00547v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00361", "title": "Affine-Invariant Global Non-Asymptotic Convergence Analysis of BFGS under Self-Concordance", "authors": ["Qiujiang Jin", "Aryan Mokhtari"], "summary": "In this paper, we establish global non-asymptotic convergence guarantees for\nthe BFGS quasi-Newton method without requiring strong convexity or the\nLipschitz continuity of the gradient or Hessian. Instead, we consider the\nsetting where the objective function is strictly convex and strongly\nself-concordant. For an arbitrary initial point and any arbitrary\npositive-definite initial Hessian approximation, we prove global linear and\nsuperlinear convergence guarantees for BFGS when the step size is determined\nusing a line search scheme satisfying the weak Wolfe conditions. Moreover, all\nour global guarantees are affine-invariant, with the convergence rates\ndepending solely on the initial error and the strongly self-concordant\nconstant. Our results extend the global non-asymptotic convergence theory of\nBFGS beyond traditional assumptions and, for the first time, establish\naffine-invariant convergence guarantees aligning with the inherent affine\ninvariance of the BFGS method.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00361v1", "categories": ["math.OC", "cs.NA", "math.NA"], "cate": "math.OC", "url": "http://arxiv.org/abs/2507.00361v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.01016", "title": "VQ-VLA: Improving Vision-Language-Action Models via Scaling Vector-Quantized Action Tokenizers", "authors": ["Yating Wang", "Haoyi Zhu", "Mingyu Liu", "Jiange Yang", "Hao-Shu Fang", "Tong He"], "summary": "In this paper, we introduce an innovative vector quantization based action\ntokenizer built upon the largest-scale action trajectory dataset to date,\nleveraging over 100 times more data than previous approaches. This extensive\ndataset enables our tokenizer to capture rich spatiotemporal dynamics,\nresulting in a model that not only accelerates inference but also generates\nsmoother and more coherent action outputs. Once trained, the tokenizer can be\nseamlessly adapted to a wide range of downstream tasks in a zero-shot manner,\nfrom short-horizon reactive behaviors to long-horizon planning. A key finding\nof our work is that the domain gap between synthetic and real action\ntrajectories is marginal, allowing us to effectively utilize a vast amount of\nsynthetic data during training without compromising real-world performance. To\nvalidate our approach, we conducted extensive experiments in both simulated\nenvironments and on real robotic platforms. The results demonstrate that as the\nvolume of synthetic trajectory data increases, the performance of our tokenizer\non downstream tasks improves significantly-most notably, achieving up to a 30%\nhigher success rate on two real-world tasks in long-horizon scenarios. These\nfindings highlight the potential of our action tokenizer as a robust and\nscalable solution for real-time embodied intelligence systems, paving the way\nfor more efficient and reliable robotic control in diverse application\ndomains.Project website: https://xiaoxiao0406.github.io/vqvla.github.io", "comment": "Accepted by ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.01016v1", "categories": ["cs.RO", "cs.CV"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2507.01016v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00012", "title": "Towards Undistillable Models by Minimizing Conditional Mutual Information", "authors": ["Linfeng Ye", "Shayan Mohajer Hamidi", "En-hui Yang"], "summary": "A deep neural network (DNN) is said to be undistillable if, when used as a\nblack-box input-output teacher, it cannot be distilled through knowledge\ndistillation (KD). In this case, the distilled student (referred to as the\nknockoff student) does not outperform a student trained independently with\nlabel smoothing (LS student) in terms of prediction accuracy. To protect\nintellectual property of DNNs, it is desirable to build undistillable DNNs. To\nthis end, it is first observed that an undistillable DNN may have the trait\nthat each cluster of its output probability distributions in response to all\nsample instances with the same label should be highly concentrated to the\nextent that each cluster corresponding to each label should ideally collapse\ninto one probability distribution. Based on this observation and by measuring\nthe concentration of each cluster in terms of conditional mutual information\n(CMI), a new training method called CMI minimized (CMIM) method is proposed,\nwhich trains a DNN by jointly minimizing the conventional cross entropy (CE)\nloss and the CMI values of all temperature scaled clusters across the entire\ntemperature spectrum. The resulting CMIM model is shown, by extensive\nexperiments, to be undistillable by all tested KD methods existing in the\nliterature. That is, the knockoff students distilled by these KD methods from\nthe CMIM model underperform the respective LS students. In addition, the CMIM\nmodel is also shown to performs better than the model trained with the CE loss\nalone in terms of their own prediction accuracy.", "comment": "27 pages, 6 figures, Transactions on Machine Learning Research", "pdf_url": "http://arxiv.org/pdf/2507.00012v1", "categories": ["cs.LG", "cs.AI", "E.4"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00012v1", "date": "2025-06-13", "updated": "2025-06-13"}
{"id": "2507.00039", "title": "Pattern-Based Graph Classification: Comparison of Quality Measures and Importance of Preprocessing", "authors": ["Lucas Potin", "Rosa Figueiredo", "Vincent Labatut", "Christine Largeron"], "summary": "Graph classification aims to categorize graphs based on their structural and\nattribute features, with applications in diverse fields such as social network\nanalysis and bioinformatics. Among the methods proposed to solve this task,\nthose relying on patterns (i.e. subgraphs) provide good explainability, as the\npatterns used for classification can be directly interpreted. To identify\nmeaningful patterns, a standard approach is to use a quality measure, i.e. a\nfunction that evaluates the discriminative power of each pattern. However, the\nliterature provides tens of such measures, making it difficult to select the\nmost appropriate for a given application. Only a handful of surveys try to\nprovide some insight by comparing these measures, and none of them specifically\nfocuses on graphs. This typically results in the systematic use of the most\nwidespread measures, without thorough evaluation. To address this issue, we\npresent a comparative analysis of 38 quality measures from the literature. We\ncharacterize them theoretically, based on four mathematical properties. We\nleverage publicly available datasets to constitute a benchmark, and propose a\nmethod to elaborate a gold standard ranking of the patterns. We exploit these\nresources to perform an empirical comparison of the measures, both in terms of\npattern ranking and classification performance. Moreover, we propose a\nclustering-based preprocessing step, which groups patterns appearing in the\nsame graphs to enhance classification performance. Our experimental results\ndemonstrate the effectiveness of this step, reducing the number of patterns to\nbe processed while achieving comparable performance. Additionally, we show that\nsome popular measures widely used in the literature are not associated with the\nbest results.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00039v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00039v1", "date": "2025-06-19", "updated": "2025-06-19"}
{"id": "2507.00334", "title": "Populate-A-Scene: Affordance-Aware Human Video Generation", "authors": ["Mengyi Shan", "Zecheng He", "Haoyu Ma", "Felix Juefei-Xu", "Peizhao Zhang", "Tingbo Hou", "Ching-Yao Chuang"], "summary": "Can a video generation model be repurposed as an interactive world simulator?\nWe explore the affordance perception potential of text-to-video models by\nteaching them to predict human-environment interaction. Given a scene image and\na prompt describing human actions, we fine-tune the model to insert a person\ninto the scene, while ensuring coherent behavior, appearance, harmonization,\nand scene affordance. Unlike prior work, we infer human affordance for video\ngeneration (i.e., where to insert a person and how they should behave) from a\nsingle scene image, without explicit conditions like bounding boxes or body\nposes. An in-depth study of cross-attention heatmaps demonstrates that we can\nuncover the inherent affordance perception of a pre-trained video model without\nlabeled affordance datasets.", "comment": "Project page: https://shanmy.github.io/Populate-A-Scene", "pdf_url": "http://arxiv.org/pdf/2507.00334v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00334v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00372", "title": "Efficient Depth- and Spatially-Varying Image Simulation for Defocus Deblur", "authors": ["Xinge Yang", "Chuong Nguyen", "Wenbin Wang", "Kaizhang Kang", "Wolfgang Heidrich", "Xiaoxing Li"], "summary": "Modern cameras with large apertures often suffer from a shallow depth of\nfield, resulting in blurry images of objects outside the focal plane. This\nlimitation is particularly problematic for fixed-focus cameras, such as those\nused in smart glasses, where adding autofocus mechanisms is challenging due to\nform factor and power constraints. Due to unmatched optical aberrations and\ndefocus properties unique to each camera system, deep learning models trained\non existing open-source datasets often face domain gaps and do not perform well\nin real-world settings. In this paper, we propose an efficient and scalable\ndataset synthesis approach that does not rely on fine-tuning with real-world\ndata. Our method simultaneously models depth-dependent defocus and spatially\nvarying optical aberrations, addressing both computational complexity and the\nscarcity of high-quality RGB-D datasets. Experimental results demonstrate that\na network trained on our low resolution synthetic images generalizes\neffectively to high resolution (12MP) real-world images across diverse scenes.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00372v1", "categories": ["cs.CV", "eess.IV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00372v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00543", "title": "Reliable Annotations with Less Effort: Evaluating LLM-Human Collaboration in Search Clarifications", "authors": ["Leila Tavakoli", "Hamed Zamani"], "summary": "Despite growing interest in using large language models (LLMs) to automate\nannotation, their effectiveness in complex, nuanced, and multi-dimensional\nlabelling tasks remains relatively underexplored. This study focuses on\nannotation for the search clarification task, leveraging a high-quality,\nmulti-dimensional dataset that includes five distinct fine-grained annotation\nsubtasks. Although LLMs have shown impressive capabilities in general settings,\nour study reveals that even state-of-the-art models struggle to replicate\nhuman-level performance in subjective or fine-grained evaluation tasks. Through\na systematic assessment, we demonstrate that LLM predictions are often\ninconsistent, poorly calibrated, and highly sensitive to prompt variations. To\naddress these limitations, we propose a simple yet effective human-in-the-loop\n(HITL) workflow that uses confidence thresholds and inter-model disagreement to\nselectively involve human review. Our findings show that this lightweight\nintervention significantly improves annotation reliability while reducing human\neffort by up to 45%, offering a relatively scalable and cost-effective yet\naccurate path forward for deploying LLMs in real-world evaluation settings.", "comment": "9 pages,5 figures", "pdf_url": "http://arxiv.org/pdf/2507.00543v1", "categories": ["cs.IR", "cs.HC"], "cate": "cs.IR", "url": "http://arxiv.org/abs/2507.00543v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00853", "title": "Ranking Quantilized Mean-Field Games with an Application to Early-Stage Venture Investments", "authors": ["Rinel Foguen Tchuendom", "Dena Firoozi", "Michèle Breton"], "summary": "Quantilized mean-field game models involve quantiles of the population's\ndistribution. We study a class of such games with a capacity for ranking games,\nwhere the performance of each agent is evaluated based on its terminal state\nrelative to the population's $\\alpha$-quantile value, $\\alpha \\in (0,1)$. This\nevaluation criterion is designed to select the top $(1-\\alpha)\\%$ performing\nagents. We provide two formulations for this competition: a target-based\nformulation and a threshold-based formulation. In the former and latter\nformulations, to satisfy the selection condition, each agent aims for its\nterminal state to be \\textit{exactly} equal and \\textit{at least} equal to the\npopulation's $\\alpha$-quantile value, respectively.\n  For the target-based formulation, we obtain an analytic solution and\ndemonstrate the $\\epsilon$-Nash property for the asymptotic best-response\nstrategies in the $N$-player game. Specifically, the quantilized mean-field\nconsistency condition is expressed as a set of forward-backward ordinary\ndifferential equations, characterizing the $\\alpha$-quantile value at\nequilibrium. For the threshold-based formulation, we obtain a semi-explicit\nsolution and numerically solve the resulting quantilized mean-field consistency\ncondition.\n  Subsequently, we propose a new application in the context of early-stage\nventure investments, where a venture capital firm financially supports a group\nof start-up companies engaged in a competition over a finite time horizon, with\nthe goal of selecting a percentage of top-ranking ones to receive the next\nround of funding at the end of the time horizon. We present the results and\ninterpretations of numerical experiments for both formulations discussed in\nthis context and show that the target-based formulation provides a very good\napproximation for the threshold-based formulation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00853v1", "categories": ["math.OC", "cs.SY", "eess.SY", "q-fin.MF"], "cate": "math.OC", "url": "http://arxiv.org/abs/2507.00853v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00579", "title": "TUM-MiKaNi at SemEval-2025 Task 3: Towards Multilingual and Knowledge-Aware Non-factual Hallucination Identification", "authors": ["Miriam Anschütz", "Ekaterina Gikalo", "Niklas Herbster", "Georg Groh"], "summary": "Hallucinations are one of the major problems of LLMs, hindering their\ntrustworthiness and deployment to wider use cases. However, most of the\nresearch on hallucinations focuses on English data, neglecting the multilingual\nnature of LLMs. This paper describes our submission to the SemEval-2025 Task-3\n- Mu-SHROOM, the Multilingual Shared-task on Hallucinations and Related\nObservable Overgeneration Mistakes. We propose a two-part pipeline that\ncombines retrieval-based fact verification against Wikipedia with a BERT-based\nsystem fine-tuned to identify common hallucination patterns. Our system\nachieves competitive results across all languages, reaching top-10 results in\neight languages, including English. Moreover, it supports multiple languages\nbeyond the fourteen covered by the shared task. This multilingual hallucination\nidentifier can help to improve LLM outputs and their usefulness in the future.", "comment": "6 pages, 3 figures, SemEval-2025 Task 3, ACL", "pdf_url": "http://arxiv.org/pdf/2507.00579v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2507.00579v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00408", "title": "Learning collective variables that respect permutational symmetry", "authors": ["Jiaxin Yuan", "Shashank Sule", "Yeuk Yin Lam", "Maria Cameron"], "summary": "In addition to translational and rotational symmetries, clusters of identical\ninteracting particles possess permutational symmetry. Coarse-grained models for\nsuch systems are instrumental in identifying metastable states, providing an\neffective description of their dynamics, and estimating transition rates. We\npropose a numerical framework for learning collective variables that respect\ntranslational, rotational, and permutational symmetries, and for estimating\ntransition rates and residence times. It combines a sort-based featurization,\nresidence manifold learning in the feature space, and learning collective\nvariables with autoencoders whose loss function utilizes the orthogonality\nrelationship (Legoll and Lelievre, 2010). The committor of the resulting\nreduced model is used as the reaction coordinate in the forward flux sampling\nand to design a control for sampling the transition path process. We offer two\ncase studies, the Lennard-Jones-7 in 2D and the Lennard-Jones-8 in 3D. The\ntransition rates and residence times computed with the aid of the reduced\nmodels agree with those obtained via brute-force methods.", "comment": "66 pages, 35 figures, 18 tables", "pdf_url": "http://arxiv.org/pdf/2507.00408v1", "categories": ["physics.chem-ph", "cs.NA", "math.NA", "82B31, 60G99, 70F99"], "cate": "physics.chem-ph", "url": "http://arxiv.org/abs/2507.00408v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00076", "title": "Time Invariant Sensor Tasking for Catalog Maintenance of LEO Space objects using Stochastic Geometry", "authors": ["Partha Chowdhury", "Harsha M", "Chinni Prabhunath Georg", "Arun Balaji Buduru", "Sanat K Biswas"], "summary": "Catalog maintenance of space objects by limited number of ground-based\nsensors presents a formidable challenging task to the space community. This\narticle presents a methodology for time-invariant tracking and surveillance of\nspace objects in low Earth orbit (LEO) by optimally directing ground sensors.\nOur methodology aims to maximize the expected number of space objects from a\nset of ground stations by utilizing concepts from stochastic geometry,\nparticularly the Poisson point process. We have provided a systematic framework\nto understand visibility patterns and enhance the efficiency of tracking\nmultiple objects simultaneously. Our approach contributes to more informed\ndecision-making in space operations, ultimately supporting efforts to maintain\nsafety and sustainability in LEO.", "comment": "This work has been accepted and presented at the 35th AAS/AIAA Space\n  Flight Mechanics Meeting, 2025, Kaua'i, Hawai", "pdf_url": "http://arxiv.org/pdf/2507.00076v1", "categories": ["astro-ph.IM", "cs.RO", "cs.SY", "eess.SY", "math-ph", "math.MP"], "cate": "astro-ph.IM", "url": "http://arxiv.org/abs/2507.00076v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2507.00013", "title": "ST-MTM: Masked Time Series Modeling with Seasonal-Trend Decomposition for Time Series Forecasting", "authors": ["Hyunwoo Seo", "Chiehyeon Lim"], "summary": "Forecasting complex time series is an important yet challenging problem that\ninvolves various industrial applications. Recently, masked time-series modeling\nhas been proposed to effectively model temporal dependencies for forecasting by\nreconstructing masked segments from unmasked ones. However, since the semantic\ninformation in time series is involved in intricate temporal variations\ngenerated by multiple time series components, simply masking a raw time series\nignores the inherent semantic structure, which may cause MTM to learn spurious\ntemporal patterns present in the raw data. To capture distinct temporal\nsemantics, we show that masked modeling techniques should address entangled\npatterns through a decomposition approach. Specifically, we propose ST-MTM, a\nmasked time-series modeling framework with seasonal-trend decomposition, which\nincludes a novel masking method for the seasonal-trend components that\nincorporates different temporal variations from each component. ST-MTM uses a\nperiod masking strategy for seasonal components to produce multiple masked\nseasonal series based on inherent multi-periodicity and a sub-series masking\nstrategy for trend components to mask temporal regions that share similar\nvariations. The proposed masking method presents an effective pre-training task\nfor learning intricate temporal variations and dependencies. Additionally,\nST-MTM introduces a contrastive learning task to support masked modeling by\nenhancing contextual consistency among multiple masked seasonal\nrepresentations. Experimental results show that our proposed ST-MTM achieves\nconsistently superior forecasting performance compared to existing masked\nmodeling, contrastive learning, and supervised forecasting methods.", "comment": "Accepted by KDD 2025 research track", "pdf_url": "http://arxiv.org/pdf/2507.00013v1", "categories": ["cs.LG", "cs.AI", "stat.ML"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00013v1", "date": "2025-06-13", "updated": "2025-06-13"}
{"id": "2507.00055", "title": "Leveraging Unlabeled Audio-Visual Data in Speech Emotion Recognition using Knowledge Distillation", "authors": ["Varsha Pendyala", "Pedro Morgado", "William Sethares"], "summary": "Voice interfaces integral to the human-computer interaction systems can\nbenefit from speech emotion recognition (SER) to customize responses based on\nuser emotions. Since humans convey emotions through multi-modal audio-visual\ncues, developing SER systems using both the modalities is beneficial. However,\ncollecting a vast amount of labeled data for their development is expensive.\nThis paper proposes a knowledge distillation framework called LightweightSER\n(LiSER) that leverages unlabeled audio-visual data for SER, using large teacher\nmodels built on advanced speech and face representation models. LiSER transfers\nknowledge regarding speech emotions and facial expressions from the teacher\nmodels to lightweight student models. Experiments conducted on two benchmark\ndatasets, RAVDESS and CREMA-D, demonstrate that LiSER can reduce the dependence\non extensive labeled datasets for SER tasks.", "comment": "Accepted at INTERSPEECH 2025", "pdf_url": "http://arxiv.org/pdf/2507.00055v1", "categories": ["cs.LG", "cs.HC", "cs.MM", "eess.AS", "eess.IV", "eess.SP"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00055v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2507.00339", "title": "Training for X-Ray Vision: Amodal Segmentation, Amodal Content Completion, and View-Invariant Object Representation from Multi-Camera Video", "authors": ["Alexander Moore", "Amar Saini", "Kylie Cancilla", "Doug Poland", "Carmen Carrano"], "summary": "Amodal segmentation and amodal content completion require using object priors\nto estimate occluded masks and features of objects in complex scenes. Until\nnow, no data has provided an additional dimension for object context: the\npossibility of multiple cameras sharing a view of a scene. We introduce\nMOVi-MC-AC: Multiple Object Video with Multi-Cameras and Amodal Content, the\nlargest amodal segmentation and first amodal content dataset to date. Cluttered\nscenes of generic household objects are simulated in multi-camera video.\nMOVi-MC-AC contributes to the growing literature of object detection, tracking,\nand segmentation by including two new contributions to the deep learning for\ncomputer vision world. Multiple Camera (MC) settings where objects can be\nidentified and tracked between various unique camera perspectives are rare in\nboth synthetic and real-world video. We introduce a new complexity to synthetic\nvideo by providing consistent object ids for detections and segmentations\nbetween both frames and multiple cameras each with unique features and motion\npatterns on a single scene. Amodal Content (AC) is a reconstructive task in\nwhich models predict the appearance of target objects through occlusions. In\nthe amodal segmentation literature, some datasets have been released with\namodal detection, tracking, and segmentation labels. While other methods rely\non slow cut-and-paste schemes to generate amodal content pseudo-labels, they do\nnot account for natural occlusions present in the modal masks. MOVi-MC-AC\nprovides labels for ~5.8 million object instances, setting a new maximum in the\namodal dataset literature, along with being the first to provide ground-truth\namodal content. The full dataset is available at\nhttps://huggingface.co/datasets/Amar-S/MOVi-MC-AC ,", "comment": "9 pages, 2 figures", "pdf_url": "http://arxiv.org/pdf/2507.00339v1", "categories": ["cs.CV", "cs.AI", "68T45, 68T07", "I.2.10; I.2.6; I.4.6"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00339v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00373", "title": "Customizable ROI-Based Deep Image Compression", "authors": ["Ian Jin", "Fanxin Xia", "Feng Ding", "Xinfeng Zhang", "Meiqin Liu", "Yao Zhao", "Weisi Lin", "Lili Meng"], "summary": "Region of Interest (ROI)-based image compression optimizes bit allocation by\nprioritizing ROI for higher-quality reconstruction. However, as the users\n(including human clients and downstream machine tasks) become more diverse,\nROI-based image compression needs to be customizable to support various\npreferences. For example, different users may define distinct ROI or require\ndifferent quality trade-offs between ROI and non-ROI. Existing ROI-based image\ncompression schemes predefine the ROI, making it unchangeable, and lack\neffective mechanisms to balance reconstruction quality between ROI and non-ROI.\nThis work proposes a paradigm for customizable ROI-based deep image\ncompression. First, we develop a Text-controlled Mask Acquisition (TMA) module,\nwhich allows users to easily customize their ROI for compression by just\ninputting the corresponding semantic \\emph{text}. It makes the encoder\ncontrolled by text. Second, we design a Customizable Value Assign (CVA)\nmechanism, which masks the non-ROI with a changeable extent decided by users\ninstead of a constant one to manage the reconstruction quality trade-off\nbetween ROI and non-ROI. Finally, we present a Latent Mask Attention (LMA)\nmodule, where the latent spatial prior of the mask and the latent\nRate-Distortion Optimization (RDO) prior of the image are extracted and fused\nin the latent space, and further used to optimize the latent representation of\nthe source image. Experimental results demonstrate that our proposed\ncustomizable ROI-based deep image compression paradigm effectively addresses\nthe needs of customization for ROI definition and mask acquisition as well as\nthe reconstruction quality trade-off management between the ROI and non-ROI.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00373v1", "categories": ["cs.CV", "eess.IV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00373v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00635", "title": "Stable Tracking of Eye Gaze Direction During Ophthalmic Surgery", "authors": ["Tinghe Hong", "Shenlin Cai", "Boyang Li", "Kai Huang"], "summary": "Ophthalmic surgical robots offer superior stability and precision by reducing\nthe natural hand tremors of human surgeons, enabling delicate operations in\nconfined surgical spaces. Despite the advancements in developing vision- and\nforce-based control methods for surgical robots, preoperative navigation\nremains heavily reliant on manual operation, limiting the consistency and\nincreasing the uncertainty. Existing eye gaze estimation techniques in the\nsurgery, whether traditional or deep learning-based, face challenges including\ndependence on additional sensors, occlusion issues in surgical environments,\nand the requirement for facial detection. To address these limitations, this\nstudy proposes an innovative eye localization and tracking method that combines\nmachine learning with traditional algorithms, eliminating the requirements of\nlandmarks and maintaining stable iris detection and gaze estimation under\nvarying lighting and shadow conditions. Extensive real-world experiment results\nshow that our proposed method has an average estimation error of 0.58 degrees\nfor eye orientation estimation and 2.08-degree average control error for the\nrobotic arm's movement based on the calculated orientation.", "comment": "Accepted by ICRA 2025", "pdf_url": "http://arxiv.org/pdf/2507.00635v1", "categories": ["cs.RO", "cs.CV", "cs.HC"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2507.00635v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00909", "title": "Turning AI Data Centers into Grid-Interactive Assets: Results from a Field Demonstration in Phoenix, Arizona", "authors": ["Philip Colangelo", "Ayse K. Coskun", "Jack Megrue", "Ciaran Roberts", "Shayan Sengupta", "Varun Sivaram", "Ethan Tiao", "Aroon Vijaykar", "Chris Williams", "Daniel C. Wilson", "Zack MacFarland", "Daniel Dreiling", "Nathan Morey", "Anuja Ratnayake", "Baskar Vairamohan"], "summary": "Artificial intelligence (AI) is fueling exponential electricity demand\ngrowth, threatening grid reliability, raising prices for communities paying for\nnew energy infrastructure, and stunting AI innovation as data centers wait for\ninterconnection to constrained grids. This paper presents the first field\ndemonstration, in collaboration with major corporate partners, of a\nsoftware-only approach--Emerald Conductor--that transforms AI data centers into\nflexible grid resources that can efficiently and immediately harness existing\npower systems without massive infrastructure buildout. Conducted at a 256-GPU\ncluster running representative AI workloads within a commercial, hyperscale\ncloud data center in Phoenix, Arizona, the trial achieved a 25% reduction in\ncluster power usage for three hours during peak grid events while maintaining\nAI quality of service (QoS) guarantees. By orchestrating AI workloads based on\nreal-time grid signals without hardware modifications or energy storage, this\nplatform reimagines data centers as grid-interactive assets that enhance grid\nreliability, advance affordability, and accelerate AI's development.", "comment": "10 pages, 6 figures, 1 table", "pdf_url": "http://arxiv.org/pdf/2507.00909v1", "categories": ["cs.DC", "cs.AI", "cs.PF", "cs.SY", "eess.SY"], "cate": "cs.DC", "url": "http://arxiv.org/abs/2507.00909v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00601", "title": "Transferable Modeling Strategies for Low-Resource LLM Tasks: A Prompt and Alignment-Based", "authors": ["Shuangquan Lyu", "Yingnan Deng", "Guiran Liu", "Zhen Qi", "Ruotong Wang"], "summary": "This paper addresses the limited transfer and adaptation capabilities of\nlarge language models in low-resource language scenarios. It proposes a unified\nframework that combines a knowledge transfer module with parameter-efficient\nfine-tuning strategies. The method introduces knowledge alignment loss and soft\nprompt tuning to guide the model in effectively absorbing the structural\nfeatures of target languages or tasks under minimal annotation. This enhances\nboth generalization performance and training stability. The framework includes\nlightweight adaptation modules to reduce computational costs. During training,\nit integrates freezing strategies and prompt injection to preserve the model's\noriginal knowledge while enabling quick adaptation to new tasks. The study also\nconducts stability analysis experiments and synthetic pseudo-data transfer\nexperiments to systematically evaluate the method's applicability and\nrobustness across different low-resource tasks. Experimental results show that\ncompared with existing multilingual pre-trained models and mainstream transfer\nmethods, the proposed approach achieves higher performance and stability on\ncross-lingual tasks such as MLQA, XQuAD, and PAWS-X. It demonstrates\nparticularly strong advantages under extremely data-scarce conditions. The\nproposed method offers strong generality and scalability. It enhances\ntask-specific adaptability while preserving the general capabilities of large\nlanguage models. This makes it well-suited for complex semantic modeling and\nmultilingual processing tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00601v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2507.00601v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00640", "title": "Forward Reverse Kernel Regression for the Schrödinger bridge problem", "authors": ["Denis Belomestny", "John. Schoenmakers"], "summary": "In this paper, we study the Schr\\\"odinger Bridge Problem (SBP), which is\ncentral to entropic optimal transport. For general reference processes and\nbegin--endpoint distributions, we propose a forward-reverse iterative Monte\nCarlo procedure to approximate the Schr\\\"odinger potentials in a nonparametric\nway. In particular, we use kernel based Monte Carlo regression in the context\nof Picard iteration of a corresponding fixed point problem. By preserving in\nthe iteration positivity and contractivity in a Hilbert metric sense, we\ndevelop a provably convergent algorithm. Furthermore, we provide convergence\nrates for the potential estimates and prove their optimality. Finally, as an\napplication, we propose a non-nested Monte Carlo procedure for the final\ndimensional distributions of the Schr\\\"odinger Bridge process, based on the\nconstructed potentials and the forward-reverse simulation method for\nconditional diffusions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00640v1", "categories": ["stat.ML", "cs.LG", "cs.NA", "math.NA", "90C40, 65C05, 62G08"], "cate": "stat.ML", "url": "http://arxiv.org/abs/2507.00640v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00209", "title": "SurgiSR4K: A High-Resolution Endoscopic Video Dataset for Robotic-Assisted Minimally Invasive Procedures", "authors": ["Fengyi Jiang", "Xiaorui Zhang", "Lingbo Jin", "Ruixing Liang", "Yuxin Chen", "Adi Chola Venkatesh", "Jason Culman", "Tiantian Wu", "Lirong Shao", "Wenqing Sun", "Cong Gao", "Hallie McNamara", "Jingpei Lu", "Omid Mohareri"], "summary": "High-resolution imaging is crucial for enhancing visual clarity and enabling\nprecise computer-assisted guidance in minimally invasive surgery (MIS). Despite\nthe increasing adoption of 4K endoscopic systems, there remains a significant\ngap in publicly available native 4K datasets tailored specifically for\nrobotic-assisted MIS. We introduce SurgiSR4K, the first publicly accessible\nsurgical imaging and video dataset captured at a native 4K resolution,\nrepresenting realistic conditions of robotic-assisted procedures. SurgiSR4K\ncomprises diverse visual scenarios including specular reflections, tool\nocclusions, bleeding, and soft tissue deformations, meticulously designed to\nreflect common challenges faced during laparoscopic and robotic surgeries. This\ndataset opens up possibilities for a broad range of computer vision tasks that\nmight benefit from high resolution data, such as super resolution (SR), smoke\nremoval, surgical instrument detection, 3D tissue reconstruction, monocular\ndepth estimation, instance segmentation, novel view synthesis, and\nvision-language model (VLM) development. SurgiSR4K provides a robust foundation\nfor advancing research in high-resolution surgical imaging and fosters the\ndevelopment of intelligent imaging technologies aimed at enhancing performance,\nsafety, and usability in image-guided robotic surgeries.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00209v1", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.RO"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2507.00209v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00014", "title": "SWE-Bench-CL: Continual Learning for Coding Agents", "authors": ["Thomas Joshi", "Shayan Chowdhury", "Fatih Uysal"], "summary": "Large Language Models (LLMs) have achieved impressive results on static\ncode-generation benchmarks, but real-world software development unfolds as a\ncontinuous stream of evolving issues, fixes, and feature requests. We introduce\nSWE-Bench-CL, a novel continual learning benchmark built on the human-verified\nSWE-Bench Verified dataset introduced by OpenAI and Princeton-NLP in 2024. By\norganizing GitHub issues into chronologically ordered sequences that reflect\nnatural repository evolution, SWE-Bench-CL enables direct evaluation of an\nagent's ability to accumulate experience, transfer knowledge across tasks, and\nresist catastrophic forgetting. We complement the dataset with (i) a\npreliminary analysis of inter-task structural similarity and contextual\nsensitivity, (ii) an interactive LangGraph-based evaluation framework augmented\nwith a FAISS-backed semantic memory module, and (iii) a suite of specialized\ncontinual learning metrics -- including average accuracy, forgetting,\nforward/backward transfer, tool-use efficiency, and a generalized Composite\nContinual Learning Score and CL-F-beta score -- to capture the\nstability-plasticity trade-off. We outline a rigorous experimental protocol\ncomparing memory-enabled and memory-disabled agents across diverse Python\nrepositories. All code and data are publicly available at\nhttps://github.com/thomasjoshi/agents-never-forget, providing the community\nwith a reproducible platform for developing more adaptive and robust AI agents\nin software engineering.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00014v1", "categories": ["cs.LG", "cs.AI", "cs.SE"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00014v1", "date": "2025-06-13", "updated": "2025-06-13"}
{"id": "2507.00061", "title": "Smooth-Distill: A Self-distillation Framework for Multitask Learning with Wearable Sensor Data", "authors": ["Hoang-Dieu Vu", "Duc-Nghia Tran", "Quang-Tu Pham", "Hieu H. Pham", "Nicolas Vuillerme", "Duc-Tan Tran"], "summary": "This paper introduces Smooth-Distill, a novel self-distillation framework\ndesigned to simultaneously perform human activity recognition (HAR) and sensor\nplacement detection using wearable sensor data. The proposed approach utilizes\na unified CNN-based architecture, MTL-net, which processes accelerometer data\nand branches into two outputs for each respective task. Unlike conventional\ndistillation methods that require separate teacher and student models, the\nproposed framework utilizes a smoothed, historical version of the model itself\nas the teacher, significantly reducing training computational overhead while\nmaintaining performance benefits. To support this research, we developed a\ncomprehensive accelerometer-based dataset capturing 12 distinct sleep postures\nacross three different wearing positions, complementing two existing public\ndatasets (MHealth and WISDM). Experimental results show that Smooth-Distill\nconsistently outperforms alternative approaches across different evaluation\nscenarios, achieving notable improvements in both human activity recognition\nand device placement detection tasks. This method demonstrates enhanced\nstability in convergence patterns during training and exhibits reduced\noverfitting compared to traditional multitask learning baselines. This\nframework contributes to the practical implementation of knowledge distillation\nin human activity recognition systems, offering an effective solution for\nmultitask learning with accelerometer data that balances accuracy and training\nefficiency. More broadly, it reduces the computational cost of model training,\nwhich is critical for scenarios requiring frequent model updates or training on\nresource-constrained platforms. The code and model are available at\nhttps://github.com/Kuan2vn/smooth\\_distill.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00061v1", "categories": ["cs.LG", "cs.AI", "eess.SP"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00061v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2507.00356", "title": "CGEarthEye:A High-Resolution Remote Sensing Vision Foundation Model Based on the Jilin-1 Satellite Constellation", "authors": ["Zhiwei Yi", "Xin Cheng", "Jingyu Ma", "Ruifei Zhu", "Junwei Tian", "Yuanxiu Zhou", "Xinge Zhao", "Hongzhe Li"], "summary": "Deep learning methods have significantly advanced the development of\nintelligent rinterpretation in remote sensing (RS), with foundational model\nresearch based on large-scale pre-training paradigms rapidly reshaping various\ndomains of Earth Observation (EO). However, compared to the open accessibility\nand high spatiotemporal coverage of medium-resolution data, the limited\nacquisition channels for ultra-high-resolution optical RS imagery have\nconstrained the progress of high-resolution remote sensing vision foundation\nmodels (RSVFM). As the world's largest sub-meter-level commercial RS satellite\nconstellation, the Jilin-1 constellation possesses abundant sub-meter-level\nimage resources. This study proposes CGEarthEye, a RSVFM framework specifically\ndesigned for Jilin-1 satellite characteristics, comprising five backbones with\ndifferent parameter scales with totaling 2.1 billion parameters. To enhance the\nrepresentational capacity of the foundation model, we developed JLSSD, the\nfirst 15-million-scale multi-temporal self-supervised learning (SSL) dataset\nfeaturing global coverage with quarterly temporal sampling within a single\nyear, constructed through multi-level representation clustering and sampling\nstrategies. The framework integrates seasonal contrast, augmentation-based\ncontrast, and masked patch token contrastive strategies for pre-training.\nComprehensive evaluations across 10 benchmark datasets covering four typical RS\ntasks demonstrate that the CGEarthEye consistently achieves state-of-the-art\n(SOTA) performance. Further analysis reveals CGEarthEye's superior\ncharacteristics in feature visualization, model convergence, parameter\nefficiency, and practical mapping applications. This study anticipates that the\nexceptional representation capabilities of CGEarthEye will facilitate broader\nand more efficient applications of Jilin-1 data in traditional EO application.", "comment": "A Remote Sensing Fundation Model for Very High Resolution Images", "pdf_url": "http://arxiv.org/pdf/2507.00356v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00356v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00447", "title": "Latent Posterior-Mean Rectified Flow for Higher-Fidelity Perceptual Face Restoration", "authors": ["Xin Luo", "Menglin Zhang", "Yunwei Lan", "Tianyu Zhang", "Rui Li", "Chang Liu", "Dong Liu"], "summary": "The Perception-Distortion tradeoff (PD-tradeoff) theory suggests that face\nrestoration algorithms must balance perceptual quality and fidelity. To achieve\nminimal distortion while maintaining perfect perceptual quality, Posterior-Mean\nRectified Flow (PMRF) proposes a flow based approach where source distribution\nis minimum distortion estimations. Although PMRF is shown to be effective, its\npixel-space modeling approach limits its ability to align with human\nperception, where human perception is defined as how humans distinguish between\ntwo image distributions. In this work, we propose Latent-PMRF, which\nreformulates PMRF in the latent space of a variational autoencoder (VAE),\nfacilitating better alignment with human perception during optimization. By\ndefining the source distribution on latent representations of minimum\ndistortion estimation, we bound the minimum distortion by the VAE's\nreconstruction error. Moreover, we reveal the design of VAE is crucial, and our\nproposed VAE significantly outperforms existing VAEs in both reconstruction and\nrestoration. Extensive experiments on blind face restoration demonstrate the\nsuperiority of Latent-PMRF, offering an improved PD-tradeoff compared to\nexisting methods, along with remarkable convergence efficiency, achieving a\n5.79X speedup over PMRF in terms of FID. Our code will be available as\nopen-source.", "comment": "Code and Models will be publicly available at\n  https://github.com/Luciennnnnnn/Latent-PMRF", "pdf_url": "http://arxiv.org/pdf/2507.00447v1", "categories": ["cs.CV", "eess.IV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00447v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00792", "title": "Real-Time Inverse Kinematics for Generating Multi-Constrained Movements of Virtual Human Characters", "authors": ["Hendric Voss", "Stefan Kopp"], "summary": "Generating accurate and realistic virtual human movements in real-time is of\nhigh importance for a variety of applications in computer graphics, interactive\nvirtual environments, robotics, and biomechanics. This paper introduces a novel\nreal-time inverse kinematics (IK) solver specifically designed for realistic\nhuman-like movement generation. Leveraging the automatic differentiation and\njust-in-time compilation of TensorFlow, the proposed solver efficiently handles\ncomplex articulated human skeletons with high degrees of freedom. By treating\nforward and inverse kinematics as differentiable operations, our method\neffectively addresses common challenges such as error accumulation and\ncomplicated joint limits in multi-constrained problems, which are critical for\nrealistic human motion modeling. We demonstrate the solver's effectiveness on\nthe SMPLX human skeleton model, evaluating its performance against widely used\niterative-based IK algorithms, like Cyclic Coordinate Descent (CCD), FABRIK,\nand the nonlinear optimization algorithm IPOPT. Our experiments cover both\nsimple end-effector tasks and sophisticated, multi-constrained problems with\nrealistic joint limits. Results indicate that our IK solver achieves real-time\nperformance, exhibiting rapid convergence, minimal computational overhead per\niteration, and improved success rates compared to existing methods. The project\ncode is available at https://github.com/hvoss-techfak/TF-JAX-IK", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00792v1", "categories": ["cs.CV", "cs.HC"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00792v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00606", "title": "Mixture of Reasonings: Teach Large Language Models to Reason with Adaptive Strategies", "authors": ["Tao Xiong", "Xavier Hu", "Wenyan Fan", "Shengyu Zhang"], "summary": "Large language models (LLMs) excel in complex tasks through advanced\nprompting techniques like Chain-of-Thought (CoT) and Tree-of-Thought (ToT), but\ntheir reliance on manually crafted, task-specific prompts limits adaptability\nand efficiency. We introduce Mixture of Reasoning (MoR), a training framework\nthat embeds diverse reasoning strategies into LLMs for autonomous,\ntask-adaptive reasoning without external prompt engineering. MoR has two\nphases: Thought Generation, creating reasoning chain templates with models like\nGPT-4o, and SFT Dataset Construction, pairing templates with benchmark datasets\nfor supervised fine-tuning.Our experiments show that MoR significantly enhances\nperformance, with MoR150 achieving 0.730 (2.2% improvement) using CoT prompting\nand 0.734 (13.5% improvement) compared to baselines. MoR eliminates the need\nfor task-specific prompts, offering a generalizable solution for robust\nreasoning across diverse tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00606v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2507.00606v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00645", "title": "A convex lifting approach for the Calderón problem", "authors": ["Giovanni S. Alberti", "Romain Petit", "Simone Sanna"], "summary": "The Calder\\'on problem consists in recovering an unknown coefficient of a\npartial differential equation from boundary measurements of its solution. These\nmeasurements give rise to a highly nonlinear forward operator. As a\nconsequence, the development of reconstruction methods for this inverse problem\nis challenging, as they usually suffer from the problem of local convergence.\nTo circumvent this issue, we propose an alternative approach based on lifting\nand convex relaxation techniques, that have been successfully developed for\nsolving finite-dimensional quadratic inverse problems. This leads to a convex\noptimization problem whose solution coincides with the sought-after\ncoefficient, provided that a non-degenerate source condition holds. We\ndemonstrate the validity of our approach on a toy model where the solution of\nthe partial differential equation is known everywhere in the domain. In this\nsimplified setting, we verify that the non-degenerate source condition holds\nunder certain assumptions on the unknown coefficient. We leave the\ninvestigation of its validity in the Calder\\'on setting for future works.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00645v1", "categories": ["math.AP", "cs.NA", "math.FA", "math.NA", "math.OC"], "cate": "math.AP", "url": "http://arxiv.org/abs/2507.00645v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00271", "title": "User Concerns Regarding Social Robots for Mood Regulation: A Case Study on the \"Sunday Blues\"", "authors": ["Zhuochao Peng", "Jiaxin Xu", "Jun Hu", "Haian Xue", "Laurens A. G. Kolks", "Pieter M. A. Desmet"], "summary": "While recent research highlights the potential of social robots to support\nmood regulation, little is known about how prospective users view their\nintegration into everyday life. To explore this, we conducted an exploratory\ncase study that used a speculative robot concept \"Mora\" to provoke reflection\nand facilitate meaningful discussion about using social robots to manage\nsubtle, day-to-day emotional experiences. We focused on the \"Sunday Blues,\" a\ncommon dip in mood that occurs at the end of the weekend, as a relatable\ncontext in which to explore individuals' insights. Using a video prototype and\na co-constructing stories method, we engaged 15 participants in imagining\ninteractions with Mora and discussing their expectations, doubts, and concerns.\nThe study surfaced a range of nuanced reflections around the attributes of\nsocial robots like empathy, intervention effectiveness, and ethical boundaries,\nwhich we translated into design considerations for future research and\ndevelopment in human-robot interaction.", "comment": "Accepted to International Conference on Social Robotics + AI (ICSR\n  2025)", "pdf_url": "http://arxiv.org/pdf/2507.00271v1", "categories": ["cs.HC", "cs.RO"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2507.00271v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00015", "title": "Vision Transformer with Adversarial Indicator Token against Adversarial Attacks in Radio Signal Classifications", "authors": ["Lu Zhang", "Sangarapillai Lambotharan", "Gan Zheng", "Guisheng Liao", "Xuekang Liu", "Fabio Roli", "Carsten Maple"], "summary": "The remarkable success of transformers across various fields such as natural\nlanguage processing and computer vision has paved the way for their\napplications in automatic modulation classification, a critical component in\nthe communication systems of Internet of Things (IoT) devices. However, it has\nbeen observed that transformer-based classification of radio signals is\nsusceptible to subtle yet sophisticated adversarial attacks. To address this\nissue, we have developed a defensive strategy for transformer-based modulation\nclassification systems to counter such adversarial attacks. In this paper, we\npropose a novel vision transformer (ViT) architecture by introducing a new\nconcept known as adversarial indicator (AdvI) token to detect adversarial\nattacks. To the best of our knowledge, this is the first work to propose an\nAdvI token in ViT to defend against adversarial attacks. Integrating an\nadversarial training method with a detection mechanism using AdvI token, we\ncombine a training time defense and running time defense in a unified neural\nnetwork model, which reduces architectural complexity of the system compared to\ndetecting adversarial perturbations using separate models. We investigate into\nthe operational principles of our method by examining the attention mechanism.\nWe show the proposed AdvI token acts as a crucial element within the ViT,\ninfluencing attention weights and thereby highlighting regions or features in\nthe input data that are potentially suspicious or anomalous. Through\nexperimental results, we demonstrate that our approach surpasses several\ncompetitive methods in handling white-box attack scenarios, including those\nutilizing the fast gradient method, projected gradient descent attacks and\nbasic iterative method.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00015v1", "categories": ["cs.LG", "cs.AI", "cs.CR"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00015v1", "date": "2025-06-13", "updated": "2025-06-13"}
{"id": "2507.00073", "title": "Fractional Policy Gradients: Reinforcement Learning with Long-Term Memory", "authors": ["Urvi Pawar", "Kunal Telangi"], "summary": "We propose Fractional Policy Gradients (FPG), a reinforcement learning\nframework incorporating fractional calculus for long-term temporal modeling in\npolicy optimization. Standard policy gradient approaches face limitations from\nMarkovian assumptions, exhibiting high variance and inefficient sampling. By\nreformulating gradients using Caputo fractional derivatives, FPG establishes\npower-law temporal correlations between state transitions. We develop an\nefficient recursive computation technique for fractional temporal-difference\nerrors with constant time and memory requirements. Theoretical analysis shows\nFPG achieves asymptotic variance reduction of order O(t^(-alpha)) versus\nstandard policy gradients while preserving convergence. Empirical validation\ndemonstrates 35-68% sample efficiency gains and 24-52% variance reduction\nversus state-of-the-art baselines. This framework provides a mathematically\ngrounded approach for leveraging long-range dependencies without computational\noverhead.", "comment": "Submitted to Journal of Machine Learning Research (JMLR), June 2025.\n  24 pages, 3 figures. Under review", "pdf_url": "http://arxiv.org/pdf/2507.00073v1", "categories": ["cs.LG", "stat.ML", "I.2.6; I.2.8"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00073v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2507.00363", "title": "GDGS: 3D Gaussian Splatting Via Geometry-Guided Initialization And Dynamic Density Control", "authors": ["Xingjun Wang", "Lianlei Shan"], "summary": "We propose a method to enhance 3D Gaussian Splatting (3DGS)~\\cite{Kerbl2023},\naddressing challenges in initialization, optimization, and density control.\nGaussian Splatting is an alternative for rendering realistic images while\nsupporting real-time performance, and it has gained popularity due to its\nexplicit 3D Gaussian representation. However, 3DGS heavily depends on accurate\ninitialization and faces difficulties in optimizing unstructured Gaussian\ndistributions into ordered surfaces, with limited adaptive density control\nmechanism proposed so far. Our first key contribution is a geometry-guided\ninitialization to predict Gaussian parameters, ensuring precise placement and\nfaster convergence. We then introduce a surface-aligned optimization strategy\nto refine Gaussian placement, improving geometric accuracy and aligning with\nthe surface normals of the scene. Finally, we present a dynamic adaptive\ndensity control mechanism that adjusts Gaussian density based on regional\ncomplexity, for visual fidelity. These innovations enable our method to achieve\nhigh-fidelity real-time rendering and significant improvements in visual\nquality, even in complex scenes. Our method demonstrates comparable or superior\nresults to state-of-the-art methods, rendering high-fidelity images in real\ntime.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00363v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00363v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00490", "title": "Just Noticeable Difference for Large Multimodal Models", "authors": ["Zijian Chen", "Yuan Tian", "Yuze Sun", "Wei Sun", "Zicheng Zhang", "Weisi Lin", "Guangtao Zhai", "Wenjun Zhang"], "summary": "Just noticeable difference (JND), the minimum change that the human visual\nsystem (HVS) can perceive, has been studied for decades. Although recent work\nhas extended this line of research into machine vision, there has been a\nscarcity of studies systematically exploring its perceptual boundaries across\nmultiple tasks and stimulus types, particularly in the current era of rapidly\nadvancing large multimodal models (LMMs), where studying the multifaceted\ncapabilities of models has become a mainstream focus. Moreover, the perceptual\ndefects of LMMs are not investigated thoroughly, resulting in potential\nsecurity issues and suboptimal response efficiency. In this paper, we take an\ninitial attempt and demonstrate that there exist significant visual blind spots\nin current LMMs. To systemically quantify this characteristic, we propose a new\nconcept, {\\bf LMM-JND}, together with its determination pipeline. Targeting\nuncovering the behavior commonalities in HVS-aligned visual perception tasks,\nwe delve into several LMM families and construct a large-scale dataset, named\nVPA-JND, which contains 21.5k reference images with over 489k stimuli across 12\ndistortion types, to facilitate LMM-JND studies. VPA-JND exposes areas where\nstate-of-the-art LMMs, including GPT-4o and the InternVL2.5 series, struggle\nwith basic comparison queries and fall significantly short of human-level\nvisual performance. We further explore the effects of vision and language\nbackbones and find a notable correlation between their design philosophy that\nmay instruct the future refinement of LMMs for their visual acuity. Together,\nour research underscores the significance of LMM-JND as a unique perspective\nfor studying LMMs, and predictable LMM-JND is crucial for security concerns.\nThis work will be available at https://github.com/zijianchen98/LMM-JND.", "comment": "19 pages, 19 figures", "pdf_url": "http://arxiv.org/pdf/2507.00490v1", "categories": ["cs.CV", "eess.IV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00490v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00875", "title": "TransLaw: Benchmarking Large Language Models in Multi-Agent Simulation of the Collaborative Translation", "authors": ["Xi Xuan", "King-kui Sin", "Yufei Zhou", "Chunyu Kit"], "summary": "Multi-agent systems empowered by large language models (LLMs) have\ndemonstrated remarkable capabilities in a wide range of downstream\napplications, including machine translation. However, the potential of LLMs in\ntranslating Hong Kong legal judgments remains uncertain due to challenges such\nas intricate legal terminology, culturally embedded nuances, and strict\nlinguistic structures. In this work, we introduce TransLaw, a novel multi-agent\nframework implemented for real-world Hong Kong case law translation. It employs\nthree specialized agents, namely, Translator, Annotator, and Proofreader, to\ncollaboratively produce translations for high accuracy in legal meaning,\nappropriateness in style, and adequate coherence and cohesion in structure.\nThis framework supports customizable LLM configurations and achieves tremendous\ncost reduction compared to professional human translation services. We\nevaluated its performance using 13 open-source and commercial LLMs as agents\nand obtained interesting findings, including that it surpasses GPT-4o in legal\nsemantic accuracy, structural coherence, and stylistic fidelity, yet trails\nhuman experts in contextualizing complex terminology and stylistic naturalness.\nOur platform website is available at CityUHK, and our bilingual judgment corpus\nused for the evaluation is available at Hugging Face.", "comment": "arXiv admin note: text overlap with arXiv:2501.09444; text overlap\n  with arXiv:2409.20288 by other authors", "pdf_url": "http://arxiv.org/pdf/2507.00875v1", "categories": ["cs.CL", "cs.HC", "cs.MA"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2507.00875v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00665", "title": "SAFER: Probing Safety in Reward Models with Sparse Autoencoder", "authors": ["Sihang Li", "Wei Shi", "Ziyuan Xie", "Tao Liang", "Guojun Ma", "Xiang Wang"], "summary": "Reinforcement learning from human feedback (RLHF) is a key paradigm for\naligning large language models (LLMs) with human values, yet the reward models\nat its core remain largely opaque. In this work, we present sparse Autoencoder\nFor Enhanced Reward model (\\textbf{SAFER}), a novel framework for interpreting\nand improving reward models through mechanistic analysis. Leveraging Sparse\nAutoencoders (SAEs), we uncover human-interpretable features in reward model\nactivations, enabling insight into safety-relevant decision-making. We apply\nSAFER to safety-oriented preference datasets and quantify the salience of\nindividual features by activation differences between chosen and rejected\nresponses. Using these feature-level signals, we design targeted data poisoning\nand denoising strategies. Experiments show that SAFER can precisely degrade or\nenhance safety alignment with minimal data modification, without sacrificing\ngeneral chat performance. Our approach contributes to interpreting, auditing\nand refining reward models in high-stakes LLM alignment tasks. Our codes are\navailable at https://github.com/xzy-101/SAFER-code. \\textit{This paper\ndiscusses topics related to large language model safety and may include\ndiscussions or examples that highlight potential risks or unsafe outcomes.}", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00665v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2507.00665v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00717", "title": "General Perturbation Resilient Dynamic String-Averaging for Inconsistent Problems with Superiorization", "authors": ["Kay Barshad", "Yair Censor"], "summary": "In this paper we introduce a General Dynamic String-Averaging (GDSA)\niterative scheme and investigate its convergence properties in the inconsistent\ncase, that is, when the input operators don't have a common fixed point. The\nDynamic String-Averaging Projection (DSAP) algorithm itself was introduced in\nan 2013 paper, where its strong convergence and bounded perturbation resilience\nwere studied in the consistent case (that is, when the sets under consideration\nhad a nonempty intersection). Results involving combination of the DSAP method\nwith superiorization, were presented in 2015. The proof of the weak convergence\nof our GDSA method is based on the notion of \"strong coherence\" of sequences of\noperators that was introduced in 2019. This is an improvement of the property\nof \"coherence\" of sequences of operators introduced in 2001 by Bauschke and\nCombettes. Strong coherence provides a more convenient sufficient convergence\ncondition for methods that employ infinite sequences of operators and it turns\nout to be a useful general tool when applied to proving the convergence of many\niterative methods. In this paper we combine the ideas of both dynamic\nstring-averaging and strong coherence, in order to analyze our GDSA method for\na general class of operators and its bounded perturbation resilience in the\ninconsistent case with weak and strong convergence. We then discuss an\napplication of the GDSA method to the Superiorization Methodology, developing\nresults on the behavior of its superiorized version.", "comment": "31 pages. Accepted for publication in Journal of Optimization Theory\n  and Applications", "pdf_url": "http://arxiv.org/pdf/2507.00717v1", "categories": ["math.OC", "cs.NA", "math.FA", "math.NA", "46N10, 46N40, 47H09, 47H10, 47J25, 47N10, 65F10, 65J99"], "cate": "math.OC", "url": "http://arxiv.org/abs/2507.00717v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00611", "title": "Residual Reward Models for Preference-based Reinforcement Learning", "authors": ["Chenyang Cao", "Miguel Rogel-García", "Mohamed Nabail", "Xueqian Wang", "Nicholas Rhinehart"], "summary": "Preference-based Reinforcement Learning (PbRL) provides a way to learn\nhigh-performance policies in environments where the reward signal is hard to\nspecify, avoiding heuristic and time-consuming reward design. However, PbRL can\nsuffer from slow convergence speed since it requires training in a reward\nmodel. Prior work has proposed learning a reward model from demonstrations and\nfine-tuning it using preferences. However, when the model is a neural network,\nusing different loss functions for pre-training and fine-tuning can pose\nchallenges to reliable optimization. In this paper, we propose a method to\neffectively leverage prior knowledge with a Residual Reward Model (RRM). An RRM\nassumes that the true reward of the environment can be split into a sum of two\nparts: a prior reward and a learned reward. The prior reward is a term\navailable before training, for example, a user's ``best guess'' reward\nfunction, or a reward function learned from inverse reinforcement learning\n(IRL), and the learned reward is trained with preferences. We introduce\nstate-based and image-based versions of RRM and evaluate them on several tasks\nin the Meta-World environment suite. Experimental results show that our method\nsubstantially improves the performance of a common PbRL method. Our method\nachieves performance improvements for a variety of different types of prior\nrewards, including proxy rewards, a reward obtained from IRL, and even a\nnegated version of the proxy reward. We also conduct experiments with a Franka\nPanda to show that our method leads to superior performance on a real robot. It\nsignificantly accelerates policy learning for different tasks, achieving\nsuccess in fewer steps than the baseline. The videos are presented at\nhttps://sunlighted.github.io/RRM-web/.", "comment": "26 pages, 22 figures", "pdf_url": "http://arxiv.org/pdf/2507.00611v1", "categories": ["cs.LG", "cs.AI", "cs.RO"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00611v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00016", "title": "Gradient-based Fine-Tuning through Pre-trained Model Regularization", "authors": ["Xuanbo Liu", "Liu Liu", "Fuxiang Wu", "Fusheng Hao", "Xianglong Liu"], "summary": "Large pre-trained models have demonstrated extensive applications across\nvarious fields. However, fine-tuning these models for specific downstream tasks\ndemands significant computational resources and storage. One fine-tuning\nmethod, gradient-based parameter selection (GPS), focuses on fine-tuning only\nthe parameters with high gradients in each neuron, thereby reducing the number\nof training parameters. Nevertheless, this approach increases computational\nresource requirements and storage demands. In this paper, we propose an\nefficient gradient-based and regularized fine-tuning method (GRFT) that updates\nthe rows or columns of the weight matrix. We theoretically demonstrate that the\nrows or columns with the highest sum of squared gradients are optimal for\nupdating. This strategy effectively reduces storage overhead and improves the\nefficiency of parameter selection. Additionally, we incorporate regularization\nto enhance knowledge transfer from the pre-trained model. GRFT achieves\nstate-of-the-art performance, surpassing existing methods such as GPS, Adapter\nTuning, and LoRA. Notably, GRFT requires updating only 1.22% and 0.30% of the\ntotal parameters on FGVC and VTAB datasets, respectively, demonstrating its\nhigh efficiency and effectiveness. The source code will be released soon.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00016v1", "categories": ["cs.LG", "cs.AI", "cs.CV"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00016v1", "date": "2025-06-14", "updated": "2025-06-14"}
{"id": "2507.00075", "title": "Theoretical Modeling of LLM Self-Improvement Training Dynamics Through Solver-Verifier Gap", "authors": ["Yifan Sun", "Yushan Liang", "Zhen Zhang", "Jiaye Teng"], "summary": "Self-improvement is among the most prominent techniques within the realm of\nlarge language models (LLM), aiming to enhance the LLM performance without\nrelying on external data. Despite its significance, generally how LLM\nperformances evolve during the self-improvement process remains underexplored.\nIn this paper, we theoretically model the training dynamics of self-improvement\nvia the concept of solver-verifier gap. This is inspired by the conjecture that\nthe performance enhancement of self-improvement stems from the gap between\nLLM's solver capability and verifier capability. Based on the theoretical\nframework, we further introduce how to predict the ultimate power of\nself-improvement using only information from the first few training epochs. We\nempirically validate the effectiveness of the theoretical model on various LLMs\nand datasets. Beyond self-improvement, we extend our analysis to investigate\nhow external data influences these dynamics within the framework. Notably, we\nfind that under limited external data regimes, such external data can be\nutilized at any stage without significantly affecting final performances, which\naccords with the empirical observations.", "comment": "24 pages", "pdf_url": "http://arxiv.org/pdf/2507.00075v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00075v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2507.00365", "title": "An Improved U-Net Model for Offline handwriting signature denoising", "authors": ["Wanghui Xiao"], "summary": "Handwriting signatures, as an important means of identity recognition, are\nwidely used in multiple fields such as financial transactions, commercial\ncontracts and personal affairs due to their legal effect and uniqueness. In\nforensic science appraisals, the analysis of offline handwriting signatures\nrequires the appraiser to provide a certain number of signature samples, which\nare usually derived from various historical contracts or archival materials.\nHowever, the provided handwriting samples are often mixed with a large amount\nof interfering information, which brings severe challenges to handwriting\nidentification work. This study proposes a signature handwriting denoising\nmodel based on the improved U-net structure, aiming to enhance the robustness\nof the signature recognition system. By introducing discrete wavelet transform\nand PCA transform, the model's ability to suppress noise has been enhanced. The\nexperimental results show that this modelis significantly superior to the\ntraditional methods in denoising effect, can effectively improve the clarity\nand readability of the signed images, and provide more reliable technical\nsupport for signature analysis and recognition.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00365v1", "categories": ["cs.CV", "eess.IV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00365v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00739", "title": "Biorthogonal Tunable Wavelet Unit with Lifting Scheme in Convolutional Neural Network", "authors": ["An Le", "Hung Nguyen", "Sungbal Seo", "You-Suk Bae", "Truong Nguyen"], "summary": "This work introduces a novel biorthogonal tunable wavelet unit constructed\nusing a lifting scheme that relaxes both the orthogonality and equal filter\nlength constraints, providing greater flexibility in filter design. The\nproposed unit enhances convolution, pooling, and downsampling operations,\nleading to improved image classification and anomaly detection in convolutional\nneural networks (CNN). When integrated into an 18-layer residual neural network\n(ResNet-18), the approach improved classification accuracy on CIFAR-10 by 2.12%\nand on the Describable Textures Dataset (DTD) by 9.73%, demonstrating its\neffectiveness in capturing fine-grained details. Similar improvements were\nobserved in ResNet-34. For anomaly detection in the hazelnut category of the\nMVTec Anomaly Detection dataset, the proposed method achieved competitive and\nwellbalanced performance in both segmentation and detection tasks,\noutperforming existing approaches in terms of accuracy and robustness.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00739v1", "categories": ["cs.CV", "eess.IV", "eess.SP"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00739v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00700", "title": "Contrasting Cognitive Styles in Vision-Language Models: Holistic Attention in Japanese Versus Analytical Focus in English", "authors": ["Ahmed Sabir", "Azinovič Gasper", "Mengsay Loem", "Rajesh Sharma"], "summary": "Cross-cultural research in perception and cognition has shown that\nindividuals from different cultural backgrounds process visual information in\ndistinct ways. East Asians, for example, tend to adopt a holistic perspective,\nattending to contextual relationships, whereas Westerners often employ an\nanalytical approach, focusing on individual objects and their attributes. In\nthis study, we investigate whether Vision-Language Models (VLMs) trained\npredominantly on different languages, specifically Japanese and English,\nexhibit similar culturally grounded attentional patterns. Using comparative\nanalysis of image descriptions, we examine whether these models reflect\ndifferences in holistic versus analytic tendencies. Our findings suggest that\nVLMs not only internalize the structural properties of language but also\nreproduce cultural behaviors embedded in the training data, indicating that\ncultural cognition may implicitly shape model outputs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00700v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2507.00700v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00720", "title": "A simplified unified wave-particle method for diatomic gases with rotational and vibrational non-equilibrium", "authors": ["Sirui Yang", "Chengwen Zhong", "Ningchao Ding", "Junzhe Cao", "He Zhang", "Congshan Zhuo", "Sha Liu"], "summary": "The hypersonic flow around near-space vehicles constitutes a multi-scale flow\nproblem. Due to insufficient molecular collisions to achieve equilibrium,\nrarefied gas effects are present in the flow field. Thus, numerical methods\ncapable of accurately resolving multi-scale flows are required. Furthermore,\nhigh-temperature gas effects in hypersonic flows mean vibrational excitation of\npolyatomic molecules. Consequently, numerical methods accounting for\nnon-equilibrium in rotational and vibrational internal energy modes are\nrequired. This study derives a quantified model-competition (QMC) mechanism for\ndiatomic gases with rotational and vibrational non-equilibrium, starting from\nintegral solutions of kinetic model equations with rotational and vibrational\nenergy. The QMC mechanism categorize collisional and free-transport particles\nin cell, applying computational weighting based on their local scale regimes.\nWe developed a simplified unified wave-particle (SUWP) method for diatomic\ngases based on QMC mechanism. For the macroscopic of the method, a\nthree-temperature model accounting for rotational and vibrational energy is\nincorporated into both the kinetic inviscid flux scheme and {Navier-Stokes}\nsolvers. For the microscopic of the method, a collisionless DSMC solver is\nemployed to resolve non-equilibrium flow physics. This work validates the\nproposed SUWP method with rotational and vibrational non-equilibrium through\nbenchmark cases, including shock tube, shock structures, flow past a cylinder,\nApollo 6 command module and space station Mir. Compared to the DSMC and\ndeterministic methods, the SUWP method exhibits favorable computational\nefficiency while maintaining accuracy.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00720v1", "categories": ["physics.flu-dyn", "cs.NA", "math.NA"], "cate": "physics.flu-dyn", "url": "http://arxiv.org/abs/2507.00720v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00669", "title": "Audio-3DVG: Unified Audio - Point Cloud Fusion for 3D Visual Grounding", "authors": ["Duc Cao-Dinh", "Khai Le-Duc", "Anh Dao", "Bach Phan Tat", "Chris Ngo", "Duy M. H. Nguyen", "Nguyen X. Khanh", "Thanh Nguyen-Tang"], "summary": "3D Visual Grounding (3DVG) involves localizing target objects in 3D point\nclouds based on natural language. While prior work has made strides using\ntextual descriptions, leveraging spoken language-known as Audio-based 3D Visual\nGrounding-remains underexplored and challenging. Motivated by advances in\nautomatic speech recognition (ASR) and speech representation learning, we\npropose Audio-3DVG, a simple yet effective framework that integrates audio and\nspatial information for enhanced grounding. Rather than treating speech as a\nmonolithic input, we decompose the task into two complementary components.\nFirst, we introduce Object Mention Detection, a multi-label classification task\nthat explicitly identifies which objects are referred to in the audio, enabling\nmore structured audio-scene reasoning. Second, we propose an Audio-Guided\nAttention module that captures interactions between candidate objects and\nrelational speech cues, improving target discrimination in cluttered scenes. To\nsupport benchmarking, we synthesize audio descriptions for standard 3DVG\ndatasets, including ScanRefer, Sr3D, and Nr3D. Experimental results demonstrate\nthat Audio-3DVG not only achieves new state-of-the-art performance in\naudio-based grounding, but also competes with text-based methods-highlighting\nthe promise of integrating spoken language into 3D vision tasks.", "comment": "Work in progress, 42 pages", "pdf_url": "http://arxiv.org/pdf/2507.00669v1", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.RO"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00669v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00018", "title": "Implicit Reward as the Bridge: A Unified View of SFT and DPO Connections", "authors": ["Bo Wang", "Qinyuan Cheng", "Runyu Peng", "Rong Bao", "Peiji Li", "Qipeng Guo", "Linyang Li", "Zhiyuan Zeng", "Yunhua Zhou", "Xipeng Qiu"], "summary": "Post-training processes are essential phases in grounding pre-trained\nlanguage models to real-world tasks, with learning from demonstrations or\npreference signals playing a crucial role in this adaptation. We present a\nunified theoretical framework bridging Supervised Fine-Tuning (SFT) and\npreference learning in Large Language Model (LLM) post-training. Through\nrigorous mathematical derivation, we demonstrate that both SFT and preference\nlearning methods like Direct Preference Optimization (DPO) operate within the\nsame optimal policy-reward subspace, with SFT representing a special case of\nimplicit reward learning. Our analysis reveals a critical limitation in\nconventional SFT: the KL divergence term in distribution matching becomes\nconstant with respect to the policy during optimization, failing to constrain\nmodel updates. To address this, we propose a simple yet effective learning rate\nreduction approach that yields significant performance improvements (up to\n\\textbf{25\\%} relative gain and \\textbf{6\\%} absolute win rate increase in\ninstruction following tasks. Additionally, we derive alternative SFT objectives\nfrom various f-divergence functions that preserve the KL term during\noptimization, further enhancing post-DPO model performance. Finally, we extend\nthe theoretical relationship between LLM logits and Q-functions from preference\nlearning to the SFT context, providing mathematical derivations and\nexperimental validation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00018v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00018v1", "date": "2025-06-15", "updated": "2025-06-15"}
{"id": "2507.00078", "title": "The language of time: a language model perspective on time-series foundation models", "authors": ["Yi Xie", "Yun Xiong", "Zejian Shi", "Hao Niu", "Zhengfu Liu"], "summary": "With the rise of large language models, the paradigm of training foundation\nmodels with massive parameter counts on vast datasets has been adopted in\nmultiple domains to achieve remarkable success. Time series foundation models\nrepresent a significant extension of this paradigm, demonstrating exceptional\nexpressive power, generalization, and cross-domain transferability. However,\nthis gives rise to a fundamental paradox: time series data reflect distinct\ndynamical systems, making cross-domain transfer intuitively implausible, yet\nthis is contradicted by the models' empirical success. To resolve this paradox,\nthis paper investigates, from both theoretical and experimental perspectives,\nthe representation learning mechanisms and generalization capabilities of\npatch-based time series foundation models. We argue that such models are not\nmerely applying a new architecture but are fundamentally generalizing the\nrepresentation paradigm of language models by extending deterministic\nvector-based representations to latent probabilistic distributional forms. Our\ntheoretical analysis supports this framework by demonstrating that continuous\ntime-series patches can be faithfully quantized into a discrete vocabulary\nwhose key statistical properties are highly consistent with those of natural\nlanguage. This generalization allows time series models to inherit the robust\nrepresentation and transfer abilities of large language models, thereby\nexplaining their superior performance in temporal tasks. Ultimately, our work\nprovides a rigorous theoretical cornerstone for understanding, evaluating, and\nimproving the safety and reliability of large-scale time series foundation\nmodels.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00078v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00078v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2507.00368", "title": "Out-of-Distribution Detection with Adaptive Top-K Logits Integration", "authors": ["Hikaru Shijo", "Yutaka Yoshihama", "Kenichi Yadani", "Norifumi Murata"], "summary": "Neural networks often make overconfident predictions from out-of-distribution\n(OOD) samples. Detection of OOD data is therefore crucial to improve the safety\nof machine learning. The simplest and most powerful method for OOD detection is\nMaxLogit, which uses the model's maximum logit to provide an OOD score. We have\ndiscovered that, in addition to the maximum logit, some other logits are also\nuseful for OOD detection. Based on this finding, we propose a new method called\nATLI (Adaptive Top-k Logits Integration), which adaptively determines effective\ntop-k logits that are specific to each model and combines the maximum logit\nwith the other top-k logits. In this study we evaluate our proposed method\nusing ImageNet-1K benchmark. Extensive experiments showed our proposed method\nto reduce the false positive rate (FPR95) by 6.73% compared to the MaxLogit\napproach, and decreased FPR95 by an additional 2.67% compared to other\nstate-of-the-art methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00368v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00368v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00718", "title": "AI Analyst: Framework and Comprehensive Evaluation of Large Language Models for Financial Time Series Report Generation", "authors": ["Elizabeth Fons", "Elena Kochkina", "Rachneet Kaur", "Zhen Zeng", "Berowne Hlavaty", "Charese Smiley", "Svitlana Vyetrenko", "Manuela Veloso"], "summary": "This paper explores the potential of large language models (LLMs) to generate\nfinancial reports from time series data. We propose a framework encompassing\nprompt engineering, model selection, and evaluation. We introduce an automated\nhighlighting system to categorize information within the generated reports,\ndifferentiating between insights derived directly from time series data,\nstemming from financial reasoning, and those reliant on external knowledge.\nThis approach aids in evaluating the factual grounding and reasoning\ncapabilities of the models. Our experiments, utilizing both data from the real\nstock market indices and synthetic time series, demonstrate the capability of\nLLMs to produce coherent and informative financial reports.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00718v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2507.00718v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00871", "title": "Swarm-based optimization with jumps: a kinetic BGK framework and convergence analysis", "authors": ["Giacomo Borghi", "Hyesung Im", "Lorenzo Pareschi"], "summary": "Metaheuristic algorithms are powerful tools for global optimization,\nparticularly for non-convex and non-differentiable problems where exact methods\nare often impractical. Particle-based optimization methods, inspired by swarm\nintelligence principles, have shown effectiveness due to their ability to\nbalance exploration and exploitation within the search space. In this work, we\nintroduce a novel particle-based optimization algorithm where velocities are\nupdated via random jumps, a strategy commonly used to enhance stochastic\nexploration. We formalize this approach by describing the dynamics through a\nkinetic modelling of BGK type, offering a unified framework that accommodates\ngeneral noise distributions, including heavy-tailed ones like Cauchy. Under\nsuitable parameter scaling, the model reduces to the Consensus-Based\nOptimization (CBO) dynamics. For non-degenerate Gaussian noise in bounded\ndomains, we prove propagation of chaos and convergence towards minimizers.\nNumerical results on benchmark problems validate the approach and highlight its\nconnection to CBO.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00871v1", "categories": ["math.OC", "cs.NA", "math.NA", "65K10, 90C26, 65C35, 82C40, 35Q90"], "cate": "math.OC", "url": "http://arxiv.org/abs/2507.00871v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00752", "title": "Multi-Modal Graph Convolutional Network with Sinusoidal Encoding for Robust Human Action Segmentation", "authors": ["Hao Xing", "Kai Zhe Boey", "Yuankai Wu", "Darius Burschka", "Gordon Cheng"], "summary": "Accurate temporal segmentation of human actions is critical for intelligent\nrobots in collaborative settings, where a precise understanding of sub-activity\nlabels and their temporal structure is essential. However, the inherent noise\nin both human pose estimation and object detection often leads to\nover-segmentation errors, disrupting the coherence of action sequences. To\naddress this, we propose a Multi-Modal Graph Convolutional Network (MMGCN) that\nintegrates low-frame-rate (e.g., 1 fps) visual data with high-frame-rate (e.g.,\n30 fps) motion data (skeleton and object detections) to mitigate fragmentation.\nOur framework introduces three key contributions. First, a sinusoidal encoding\nstrategy that maps 3D skeleton coordinates into a continuous sin-cos space to\nenhance spatial representation robustness. Second, a temporal graph fusion\nmodule that aligns multi-modal inputs with differing resolutions via\nhierarchical feature aggregation, Third, inspired by the smooth transitions\ninherent to human actions, we design SmoothLabelMix, a data augmentation\ntechnique that mixes input sequences and labels to generate synthetic training\nexamples with gradual action transitions, enhancing temporal consistency in\npredictions and reducing over-segmentation artifacts.\n  Extensive experiments on the Bimanual Actions Dataset, a public benchmark for\nhuman-object interaction understanding, demonstrate that our approach\noutperforms state-of-the-art methods, especially in action segmentation\naccuracy, achieving F1@10: 94.5% and F1@25: 92.8%.", "comment": "7 pages, 4 figures, accepted in IROS25, Hangzhou, China", "pdf_url": "http://arxiv.org/pdf/2507.00752v1", "categories": ["cs.CV", "cs.RO"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00752v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00019", "title": "Quantum Inspired Encoding Strategies for Machine Learning Models: Proposing and Evaluating Instance Level, Global Discrete, and Class Conditional Representations", "authors": ["Minati Rath", "Hema Date"], "summary": "In this study, we propose, evaluate and compare three quantum inspired data\nencoding strategies, Instance Level Strategy (ILS), Global Discrete Strategy\n(GDS) and Class Conditional Value Strategy (CCVS), for transforming classical\ndata into quantum data for use in pure classical machine learning models. The\nprimary objective is to reduce high encoding time while ensuring correct\nencoding values and analyzing their impact on classification performance. The\nInstance Level Strategy treats each row of dataset independently; mimics local\nquantum states. Global Discrete Value Based encoding strategy maps all unique\nfeature values across the full dataset to quantum states uniformly. In\ncontrast, the Class conditional Value based encoding strategy encodes unique\nvalues separately for each class, preserving class dependent information.\n  We apply these encoding strategies to a classification task and assess their\nimpact on en-coding efficiency, correctness, model accuracy, and computational\ncost. By analyzing the trade offs between encoding time, precision, and\npredictive performance, this study provides insights into optimizing quantum\ninspired data transformations for classical machine learning workflows.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00019v1", "categories": ["cs.LG", "cs.AI", "quant-ph"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00019v1", "date": "2025-06-15", "updated": "2025-06-15"}
{"id": "2507.00080", "title": "Online Meal Detection Based on CGM Data Dynamics", "authors": ["Ali Tavasoli", "Heman Shakeri"], "summary": "We utilize dynamical modes as features derived from Continuous Glucose\nMonitoring (CGM) data to detect meal events. By leveraging the inherent\nproperties of underlying dynamics, these modes capture key aspects of glucose\nvariability, enabling the identification of patterns and anomalies associated\nwith meal consumption. This approach not only improves the accuracy of meal\ndetection but also enhances the interpretability of the underlying glucose\ndynamics. By focusing on dynamical features, our method provides a robust\nframework for feature extraction, facilitating generalization across diverse\ndatasets and ensuring reliable performance in real-world applications. The\nproposed technique offers significant advantages over traditional approaches,\nimproving detection accuracy,", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00080v1", "categories": ["cs.LG", "nlin.AO", "stat.AP"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00080v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2507.00371", "title": "PlantSegNeRF: A few-shot, cross-dataset method for plant 3D instance point cloud reconstruction via joint-channel NeRF with multi-view image instance matching", "authors": ["Xin Yang", "Ruiming Du", "Hanyang Huang", "Jiayang Xie", "Pengyao Xie", "Leisen Fang", "Ziyue Guo", "Nanjun Jiang", "Yu Jiang", "Haiyan Cen"], "summary": "Organ segmentation of plant point clouds is a prerequisite for the\nhigh-resolution and accurate extraction of organ-level phenotypic traits.\nAlthough the fast development of deep learning has boosted much research on\nsegmentation of plant point clouds, the existing techniques for organ\nsegmentation still face limitations in resolution, segmentation accuracy, and\ngeneralizability across various plant species. In this study, we proposed a\nnovel approach called plant segmentation neural radiance fields (PlantSegNeRF),\naiming to directly generate high-precision instance point clouds from\nmulti-view RGB image sequences for a wide range of plant species. PlantSegNeRF\nperformed 2D instance segmentation on the multi-view images to generate\ninstance masks for each organ with a corresponding ID. The multi-view instance\nIDs corresponding to the same plant organ were then matched and refined using a\nspecially designed instance matching module. The instance NeRF was developed to\nrender an implicit scene, containing color, density, semantic and instance\ninformation. The implicit scene was ultimately converted into high-precision\nplant instance point clouds based on the volume density. The results proved\nthat in semantic segmentation of point clouds, PlantSegNeRF outperformed the\ncommonly used methods, demonstrating an average improvement of 16.1%, 18.3%,\n17.8%, and 24.2% in precision, recall, F1-score, and IoU compared to the\nsecond-best results on structurally complex datasets. More importantly,\nPlantSegNeRF exhibited significant advantages in plant point cloud instance\nsegmentation tasks. Across all plant datasets, it achieved average improvements\nof 11.7%, 38.2%, 32.2% and 25.3% in mPrec, mRec, mCov, mWCov, respectively.\nThis study extends the organ-level plant phenotyping and provides a\nhigh-throughput way to supply high-quality 3D data for the development of\nlarge-scale models in plant science.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00371v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00371v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00769", "title": "LitBench: A Benchmark and Dataset for Reliable Evaluation of Creative Writing", "authors": ["Daniel Fein", "Sebastian Russo", "Violet Xiang", "Kabir Jolly", "Rafael Rafailov", "Nick Haber"], "summary": "Evaluating creative writing generated by large language models (LLMs) remains\nchallenging because open-ended narratives lack ground truths. Without\nperformant automated evaluation methods, off-the-shelf (OTS) language models\nare employed as zero-shot judges, yet their reliability is unclear in this\ncontext. In pursuit of robust evaluation for creative writing, we introduce\nLitBench, the first standardized benchmark and paired dataset for creative\nwriting verification, comprising a held-out test set of 2,480 debiased,\nhuman-labeled story comparisons drawn from Reddit and a 43,827-pair training\ncorpus of human preference labels. Using LitBench, we (i) benchmark zero-shot\nLLM judges, (ii) train Bradley Terry and generative reward models, and (iii)\nconduct an online human study to validate reward model rankings on newly\nLLM-generated stories. Our benchmark identifies Claude-3.7-Sonnet as the\nstrongest off-the-shelf judge, reaching 73% agreement with human preferences;\namong trained reward models, Bradley-Terry and Generative reward models both\nattain an accuracy of 78%, outperforming all off-the-shelf judges. An online\nhuman study further confirms that our trained reward models consistently align\nwith human preferences in novel LLM-generated stories. We release LitBench and\nreward models at\nhttps://huggingface.co/collections/SAA-Lab/litbench-68267b5da3aafe58f9e43461,\nproviding a vetted resource for reliable, automated evaluation and optimization\nof creative writing systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00769v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2507.00769v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00976", "title": "Anatomy of High-Performance Column-Pivoted QR Decomposition", "authors": ["Maksim Melnichenko", "Riley Murray", "William Killian", "James Demmel", "Michael W. Mahoney", "Piotr Luszczek", "Mark Gates"], "summary": "We introduce an algorithmic framework for performing QR factorization with\ncolumn pivoting (QRCP) on general matrices. The framework enables the design of\npractical QRCP algorithms through user-controlled choices for the core\nsubroutines. We provide a comprehensive overview of how to navigate these\nchoices on modern hardware platforms, offering detailed descriptions of\nalternative methods for both CPUs and GPUs. The practical QRCP algorithms\ndeveloped within this framework are implemented as part of the open-source\nRandLAPACK library. Our empirical evaluation demonstrates that, on a dual AMD\nEPYC 9734 system, the proposed method achieves performance improvements of up\nto two orders of magnitude over LAPACK's standard QRCP routine and greatly\nsurpasses the performance of the current state-of-the-art randomized QRCP\nalgorithm. Additionally, on an NVIDIA H100 GPU, our method attains\napproximately 65 percent of the performance of cuSOLVER's unpivoted QR\nfactorization.", "comment": "v1: 33 pages in the body, 7 pages in the appendices, 17 figures", "pdf_url": "http://arxiv.org/pdf/2507.00976v1", "categories": ["cs.MS", "cs.NA", "math.NA"], "cate": "cs.MS", "url": "http://arxiv.org/abs/2507.00976v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00756", "title": "Towards Open-World Human Action Segmentation Using Graph Convolutional Networks", "authors": ["Hao Xing", "Kai Zhe Boey", "Gordon Cheng"], "summary": "Human-object interaction segmentation is a fundamental task of daily activity\nunderstanding, which plays a crucial role in applications such as assistive\nrobotics, healthcare, and autonomous systems. Most existing learning-based\nmethods excel in closed-world action segmentation, they struggle to generalize\nto open-world scenarios where novel actions emerge. Collecting exhaustive\naction categories for training is impractical due to the dynamic diversity of\nhuman activities, necessitating models that detect and segment\nout-of-distribution actions without manual annotation. To address this issue,\nwe formally define the open-world action segmentation problem and propose a\nstructured framework for detecting and segmenting unseen actions. Our framework\nintroduces three key innovations: 1) an Enhanced Pyramid Graph Convolutional\nNetwork (EPGCN) with a novel decoder module for robust spatiotemporal feature\nupsampling. 2) Mixup-based training to synthesize out-of-distribution data,\neliminating reliance on manual annotations. 3) A novel Temporal Clustering loss\nthat groups in-distribution actions while distancing out-of-distribution\nsamples.\n  We evaluate our framework on two challenging human-object interaction\nrecognition datasets: Bimanual Actions and 2 Hands and Object (H2O) datasets.\nExperimental results demonstrate significant improvements over state-of-the-art\naction segmentation models across multiple open-set evaluation metrics,\nachieving 16.9% and 34.6% relative gains in open-set segmentation (F1@50) and\nout-of-distribution detection performances (AUROC), respectively. Additionally,\nwe conduct an in-depth ablation study to assess the impact of each proposed\ncomponent, identifying the optimal framework configuration for open-world\naction segmentation.", "comment": "8 pages, 3 figures, accepted in IROS25, Hangzhou, China", "pdf_url": "http://arxiv.org/pdf/2507.00756v1", "categories": ["cs.CV", "cs.RO"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00756v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00022", "title": "GLU Attention Improve Transformer", "authors": ["Zehao Wang"], "summary": "Gated Linear Units (GLU) have shown great potential in enhancing neural\nnetwork performance. In this paper, I introduce a novel attention mechanism\ncalled GLU Attention, which introduces nonlinearity into the values of\nAttention. My experiments demonstrate that GLU Attention improves both model\nperformance and convergence speed across text and vision modalities with zero\nadditional parameters and negligible computational costs. GLU Attention is\nlightweight and can seamlessly integrate with other technologies, such as Flash\nAttention, Rotary Position Embedding (RoPE), and various Multi-Head Attention\n(MHA) variants such as Grouped-Query Attention (GQA). This project is\nopen-sourced at github.", "comment": "4 pages 4 figures", "pdf_url": "http://arxiv.org/pdf/2507.00022v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.NE"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00022v1", "date": "2025-06-16", "updated": "2025-06-16"}
{"id": "2507.00082", "title": "Federated Learning-Enabled Hybrid Language Models for Communication-Efficient Token Transmission", "authors": ["Faranaksadat Solat", "Joohyung Lee", "Mohamed Seif", "Dusit Niyato", "H. Vincent Poor"], "summary": "Hybrid Language Models (HLMs) combine the low-latency efficiency of Small\nLanguage Models (SLMs) on edge devices with the high accuracy of Large Language\nModels (LLMs) on centralized servers. Unlike traditional end-to-end LLM\ninference, HLMs reduce latency and communication by invoking LLMs only when\nlocal SLM predictions are uncertain, i.e., when token-level confidence is low\nor entropy is high. However, ambiguous or low-confidence predictions still\nrequire frequent offloading to the LLM, leading to significant communication\noverhead in bandwidth-constrained settings. To address this, we propose FedHLM,\na communication-efficient HLM framework that integrates uncertainty-aware\ninference with Federated Learning (FL). FedHLM's key innovation lies in\ncollaboratively learning token-level uncertainty thresholds that govern when\nLLM assistance is needed. Rather than using static or manually tuned\nthresholds, FedHLM employs FL to optimize these thresholds in a\nprivacy-preserving, distributed manner. Additionally, it leverages\nembedding-based token representations for Peer-to-Peer (P2P) resolution,\nenabling clients to reuse tokens inferred by semantically similar peers without\nengaging the LLM. We further introduce hierarchical model aggregation: edge\nservers refine local routing policies through client updates, while\ncross-cluster coordination aligns global decision boundaries. This layered\ndesign captures recurring uncertainty patterns, reducing redundant LLM queries.\nExperiments on large-scale news classification tasks show that FedHLM reduces\nLLM transmissions by over 95 percent with negligible accuracy loss, making it\nwell-suited for scalable and efficient edge-AI applications.", "comment": "17 pages, 16 figures, IEEE Internet of Things", "pdf_url": "http://arxiv.org/pdf/2507.00082v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00082v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00372", "title": "Efficient Depth- and Spatially-Varying Image Simulation for Defocus Deblur", "authors": ["Xinge Yang", "Chuong Nguyen", "Wenbin Wang", "Kaizhang Kang", "Wolfgang Heidrich", "Xiaoxing Li"], "summary": "Modern cameras with large apertures often suffer from a shallow depth of\nfield, resulting in blurry images of objects outside the focal plane. This\nlimitation is particularly problematic for fixed-focus cameras, such as those\nused in smart glasses, where adding autofocus mechanisms is challenging due to\nform factor and power constraints. Due to unmatched optical aberrations and\ndefocus properties unique to each camera system, deep learning models trained\non existing open-source datasets often face domain gaps and do not perform well\nin real-world settings. In this paper, we propose an efficient and scalable\ndataset synthesis approach that does not rely on fine-tuning with real-world\ndata. Our method simultaneously models depth-dependent defocus and spatially\nvarying optical aberrations, addressing both computational complexity and the\nscarcity of high-quality RGB-D datasets. Experimental results demonstrate that\na network trained on our low resolution synthetic images generalizes\neffectively to high resolution (12MP) real-world images across diverse scenes.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00372v1", "categories": ["cs.CV", "eess.IV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00372v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00782", "title": "A Diagrammatic Calculus for a Functional Model of Natural Language Semantics", "authors": ["Matthieu Pierre Boyer"], "summary": "In this paper, we study a functional programming approach to natural language\nsemantics, allowing us to increase the expressivity of a more traditional\ndenotation style. We will formalize a category based type and effect system,\nand construct a diagrammatic calculus to model parsing and handling of effects,\nand use it to efficiently compute the denotations for sentences.", "comment": "15 pages, preprint before submission to CSL 2026", "pdf_url": "http://arxiv.org/pdf/2507.00782v1", "categories": ["cs.CL", "cs.PL", "J.5; D.3.1; D.3.3"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2507.00782v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00991", "title": "Stable skeleton integral equations for general coefficient Helmholtz transmission problems", "authors": ["Benedikt Gräßle", "Ralf Hiptmair", "Stefan Sauter"], "summary": "A novel variational formulation of layer potentials and boundary integral\noperators generalizes their classical construction by Green's functions, which\nare not explicitly available for Helmholtz problems with variable coefficients.\nWavenumber explicit estimates and properties like jump conditions follow\ndirectly from their variational definition and enable a non-local\n(``integral'') formulation of acoustic transmission problems (TP) with\npiecewise Lipschitz coefficients. We obtain the well-posedness of the integral\nequations directly from the stability of the underlying TP. The simultaneous\nanalysis for general dimensions and complex wavenumbers (in this paper) imposes\nan artificial boundary on the external Helmholtz problem and employs recent\ninsights into the associated Dirichlet-to-Neumann map.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00991v1", "categories": ["math.AP", "cs.NA", "math.NA", "31B10, 35C15, 45A05, 65R20"], "cate": "math.AP", "url": "http://arxiv.org/abs/2507.00991v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00886", "title": "GaussianVLM: Scene-centric 3D Vision-Language Models using Language-aligned Gaussian Splats for Embodied Reasoning and Beyond", "authors": ["Anna-Maria Halacheva", "Jan-Nico Zaech", "Xi Wang", "Danda Pani Paudel", "Luc Van Gool"], "summary": "As multimodal language models advance, their application to 3D scene\nunderstanding is a fast-growing frontier, driving the development of 3D\nVision-Language Models (VLMs). Current methods show strong dependence on object\ndetectors, introducing processing bottlenecks and limitations in taxonomic\nflexibility. To address these limitations, we propose a scene-centric 3D VLM\nfor 3D Gaussian splat scenes that employs language- and task-aware scene\nrepresentations. Our approach directly embeds rich linguistic features into the\n3D scene representation by associating language with each Gaussian primitive,\nachieving early modality alignment. To process the resulting dense\nrepresentations, we introduce a dual sparsifier that distills them into\ncompact, task-relevant tokens via task-guided and location-guided pathways,\nproducing sparse, task-aware global and local scene tokens. Notably, we present\nthe first Gaussian splatting-based VLM, leveraging photorealistic 3D\nrepresentations derived from standard RGB images, demonstrating strong\ngeneralization: it improves performance of prior 3D VLM five folds, in\nout-of-the-domain settings.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00886v1", "categories": ["cs.CV", "cs.RO"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00886v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00024", "title": "AIMatDesign: Knowledge-Augmented Reinforcement Learning for Inverse Materials Design under Data Scarcity", "authors": ["Yeyong Yu", "Xilei Bian", "Jie Xiong", "Xing Wu", "Quan Qian"], "summary": "With the growing demand for novel materials, machine learning-driven inverse\ndesign methods face significant challenges in reconciling the high-dimensional\nmaterials composition space with limited experimental data. Existing approaches\nsuffer from two major limitations: (I) machine learning models often lack\nreliability in high-dimensional spaces, leading to prediction biases during the\ndesign process; (II) these models fail to effectively incorporate domain expert\nknowledge, limiting their capacity to support knowledge-guided inverse design.\nTo address these challenges, we introduce AIMatDesign, a reinforcement learning\nframework that addresses these limitations by augmenting experimental data\nusing difference-based algorithms to build a trusted experience pool,\naccelerating model convergence. To enhance model reliability, an automated\nrefinement strategy guided by large language models (LLMs) dynamically corrects\nprediction inconsistencies, reinforcing alignment between reward signals and\nstate value functions. Additionally, a knowledge-based reward function\nleverages expert domain rules to improve stability and efficiency during\ntraining. Our experiments demonstrate that AIMatDesign significantly surpasses\ntraditional machine learning and reinforcement learning methods in discovery\nefficiency, convergence speed, and success rates. Among the numerous candidates\nproposed by AIMatDesign, experimental synthesis of representative Zr-based\nalloys yielded a top-performing BMG with 1.7GPa yield strength and 10.2\\%\nelongation, closely matching predictions. Moreover, the framework accurately\ncaptured the trend of yield strength variation with composition, demonstrating\nits reliability and potential for closed-loop materials discovery.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00024v1", "categories": ["cs.LG", "cond-mat.mtrl-sci", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00024v1", "date": "2025-06-17", "updated": "2025-06-17"}
{"id": "2507.00083", "title": "Strategic Counterfactual Modeling of Deep-Target Airstrike Systems via Intervention-Aware Spatio-Causal Graph Networks", "authors": ["Wei Meng"], "summary": "This study addresses the lack of structured causal modeling between tactical\nstrike behavior and strategic delay in current strategic-level simulations,\nparticularly the structural bottlenecks in capturing intermediate variables\nwithin the \"resilience - nodal suppression - negotiation window\" chain. We\npropose the Intervention-Aware Spatio-Temporal Graph Neural Network (IA-STGNN),\na novel framework that closes the causal loop from tactical input to strategic\ndelay output. The model integrates graph attention mechanisms, counterfactual\nsimulation units, and spatial intervention node reconstruction to enable\ndynamic simulations of strike configurations and synchronization strategies.\nTraining data are generated from a multi-physics simulation platform (GEANT4 +\nCOMSOL) under NIST SP 800-160 standards, ensuring structural traceability and\npolicy-level validation. Experimental results demonstrate that IA-STGNN\nsignificantly outperforms baseline models (ST-GNN, GCN-LSTM, XGBoost),\nachieving a 12.8 percent reduction in MAE and 18.4 percent increase in Top-5\npercent accuracy, while improving causal path consistency and intervention\nstability. IA-STGNN enables interpretable prediction of strategic delay and\nsupports applications such as nuclear deterrence simulation, diplomatic window\nassessment, and multi-strategy optimization, providing a structured and\ntransparent AI decision-support mechanism for high-level policy modeling.", "comment": "This paper proposes the first closed-loop causal modeling framework\n  (IA-STGNN) that links tactical strike variables to strategic delay outcomes\n  via graph neural networks with counterfactual reasoning", "pdf_url": "http://arxiv.org/pdf/2507.00083v1", "categories": ["cs.LG", "cs.AI", "91A80, 91B62, 68T07", "I.2.6; J.7; K.4.1; C.2.4"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00083v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00373", "title": "Customizable ROI-Based Deep Image Compression", "authors": ["Ian Jin", "Fanxin Xia", "Feng Ding", "Xinfeng Zhang", "Meiqin Liu", "Yao Zhao", "Weisi Lin", "Lili Meng"], "summary": "Region of Interest (ROI)-based image compression optimizes bit allocation by\nprioritizing ROI for higher-quality reconstruction. However, as the users\n(including human clients and downstream machine tasks) become more diverse,\nROI-based image compression needs to be customizable to support various\npreferences. For example, different users may define distinct ROI or require\ndifferent quality trade-offs between ROI and non-ROI. Existing ROI-based image\ncompression schemes predefine the ROI, making it unchangeable, and lack\neffective mechanisms to balance reconstruction quality between ROI and non-ROI.\nThis work proposes a paradigm for customizable ROI-based deep image\ncompression. First, we develop a Text-controlled Mask Acquisition (TMA) module,\nwhich allows users to easily customize their ROI for compression by just\ninputting the corresponding semantic \\emph{text}. It makes the encoder\ncontrolled by text. Second, we design a Customizable Value Assign (CVA)\nmechanism, which masks the non-ROI with a changeable extent decided by users\ninstead of a constant one to manage the reconstruction quality trade-off\nbetween ROI and non-ROI. Finally, we present a Latent Mask Attention (LMA)\nmodule, where the latent spatial prior of the mask and the latent\nRate-Distortion Optimization (RDO) prior of the image are extracted and fused\nin the latent space, and further used to optimize the latent representation of\nthe source image. Experimental results demonstrate that our proposed\ncustomizable ROI-based deep image compression paradigm effectively addresses\nthe needs of customization for ROI definition and mask acquisition as well as\nthe reconstruction quality trade-off management between the ROI and non-ROI.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00373v1", "categories": ["cs.CV", "eess.IV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00373v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00783", "title": "Generative AI and the future of scientometrics: current topics and future questions", "authors": ["Benedetto Lepori", "Jens Peter Andersen", "Karsten Donnay"], "summary": "The aim of this paper is to review the use of GenAI in scientometrics, and to\nbegin a debate on the broader implications for the field. First, we provide an\nintroduction on GenAI's generative and probabilistic nature as rooted in\ndistributional linguistics. And we relate this to the debate on the extent to\nwhich GenAI might be able to mimic human 'reasoning'. Second, we leverage this\ndistinction for a critical engagement with recent experiments using GenAI in\nscientometrics, including topic labelling, the analysis of citation contexts,\npredictive applications, scholars' profiling, and research assessment. GenAI\nshows promise in tasks where language generation dominates, such as labelling,\nbut faces limitations in tasks that require stable semantics, pragmatic\nreasoning, or structured domain knowledge. However, these results might become\nquickly outdated. Our recommendation is, therefore, to always strive to\nsystematically compare the performance of different GenAI models for specific\ntasks. Third, we inquire whether, by generating large amounts of scientific\nlanguage, GenAI might have a fundamental impact on our field by affecting\ntextual characteristics used to measure science, such as authors, words, and\nreferences. We argue that careful empirical work and theoretical reflection\nwill be essential to remain capable of interpreting the evolving patterns of\nknowledge production.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00783v1", "categories": ["cs.CL", "cs.DL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2507.00783v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00963", "title": "Social Robots for People with Dementia: A Literature Review on Deception from Design to Perception", "authors": ["Fan Wang", "Giulia Perugia", "Yuan Feng", "Wijnand IJsselsteijn"], "summary": "As social robots increasingly enter dementia care, concerns about deception,\nintentional or not, are gaining attention. Yet, how robotic design cues might\nelicit misleading perceptions in people with dementia, and how these\nperceptions arise, remains insufficiently understood. In this scoping review,\nwe examined 26 empirical studies on interactions between people with dementia\nand physical social robots. We identify four key design cue categories that may\ninfluence deceptive impressions: cues resembling physiological signs (e.g.,\nsimulated breathing), social intentions (e.g., playful movement), familiar\nbeings (e.g., animal-like form and sound), and, to a lesser extent, cues that\nreveal artificiality. Thematic analysis of user responses reveals that people\nwith dementia often attribute biological, social, and mental capacities to\nrobots, dynamically shifting between awareness and illusion. These findings\nunderscore the fluctuating nature of ontological perception in dementia\ncontexts. Existing definitions of robotic deception often rest on philosophical\nor behaviorist premises, but rarely engage with the cognitive mechanisms\ninvolved. We propose an empirically grounded definition: robotic deception\noccurs when Type 1 (automatic, heuristic) processing dominates over Type 2\n(deliberative, analytic) reasoning, leading to misinterpretation of a robot's\nartificial nature. This dual-process perspective highlights the ethical\ncomplexity of social robots in dementia care and calls for design approaches\nthat are not only engaging, but also epistemically respectful.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00963v1", "categories": ["cs.HC", "cs.CY", "cs.RO"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2507.00963v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00025", "title": "Generalizing to New Dynamical Systems via Frequency Domain Adaptation", "authors": ["Tiexin Qin", "Hong Yan", "Haoliang Li"], "summary": "Learning the underlying dynamics from data with deep neural networks has\nshown remarkable potential in modeling various complex physical dynamics.\nHowever, current approaches are constrained in their ability to make reliable\npredictions in a specific domain and struggle with generalizing to unseen\nsystems that are governed by the same general dynamics but differ in\nenvironmental characteristics. In this work, we formulate a parameter-efficient\nmethod, Fourier Neural Simulator for Dynamical Adaptation (FNSDA), that can\nreadily generalize to new dynamics via adaptation in the Fourier space.\nSpecifically, FNSDA identifies the shareable dynamics based on the known\nenvironments using an automatic partition in Fourier modes and learns to adjust\nthe modes specific for each new environment by conditioning on low-dimensional\nlatent systematic parameters for efficient generalization. We evaluate our\napproach on four representative families of dynamic systems, and the results\nshow that FNSDA can achieve superior or competitive generalization performance\ncompared to existing methods with a significantly reduced parameter cost. Our\ncode is available at https://github.com/WonderSeven/FNSDA.", "comment": "Accepted by TPAMI 2025", "pdf_url": "http://arxiv.org/pdf/2507.00025v1", "categories": ["cs.LG", "cs.AI", "stat.ML"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00025v1", "date": "2025-06-17", "updated": "2025-06-17"}
{"id": "2507.00085", "title": "A Joint Topology-Data Fusion Graph Network for Robust Traffic Speed Prediction with Data Anomalism", "authors": ["Ruiyuan Jiang", "Dongyao Jia", "Eng Gee Lim", "Pengfei Fan", "Yuli Zhang", "Shangbo Wang"], "summary": "Accurate traffic prediction is essential for Intelligent Transportation\nSystems (ITS), yet current methods struggle with the inherent complexity and\nnon-linearity of traffic dynamics, making it difficult to integrate spatial and\ntemporal characteristics. Furthermore, existing approaches use static\ntechniques to address non-stationary and anomalous historical data, which\nlimits adaptability and undermines data smoothing. To overcome these\nchallenges, we propose the Graph Fusion Enhanced Network (GFEN), an innovative\nframework for network-level traffic speed prediction. GFEN introduces a novel\ntopological spatiotemporal graph fusion technique that meticulously extracts\nand merges spatial and temporal correlations from both data distribution and\nnetwork topology using trainable methods, enabling the modeling of multi-scale\nspatiotemporal features. Additionally, GFEN employs a hybrid methodology\ncombining a k-th order difference-based mathematical framework with an\nattention-based deep learning structure to adaptively smooth historical\nobservations and dynamically mitigate data anomalies and non-stationarity.\nExtensive experiments demonstrate that GFEN surpasses state-of-the-art methods\nby approximately 6.3% in prediction accuracy and exhibits convergence rates\nnearly twice as fast as recent hybrid models, confirming its superior\nperformance and potential to significantly enhance traffic prediction system\nefficiency.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00085v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00085v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00377", "title": "MedDiff-FT: Data-Efficient Diffusion Model Fine-tuning with Structural Guidance for Controllable Medical Image Synthesis", "authors": ["Jianhao Xie", "Ziang Zhang", "Zhenyu Weng", "Yuesheng Zhu", "Guibo Luo"], "summary": "Recent advancements in deep learning for medical image segmentation are often\nlimited by the scarcity of high-quality training data.While diffusion models\nprovide a potential solution by generating synthetic images, their\neffectiveness in medical imaging remains constrained due to their reliance on\nlarge-scale medical datasets and the need for higher image quality. To address\nthese challenges, we present MedDiff-FT, a controllable medical image\ngeneration method that fine-tunes a diffusion foundation model to produce\nmedical images with structural dependency and domain specificity in a\ndata-efficient manner. During inference, a dynamic adaptive guiding mask\nenforces spatial constraints to ensure anatomically coherent synthesis, while a\nlightweight stochastic mask generator enhances diversity through hierarchical\nrandomness injection. Additionally, an automated quality assessment protocol\nfilters suboptimal outputs using feature-space metrics, followed by mask\ncorrosion to refine fidelity. Evaluated on five medical segmentation\ndatasets,MedDiff-FT's synthetic image-mask pairs improve SOTA method's\nsegmentation performance by an average of 1% in Dice score. The framework\neffectively balances generation quality, diversity, and computational\nefficiency, offering a practical solution for medical data augmentation. The\ncode is available at https://github.com/JianhaoXie1/MedDiff-FT.", "comment": "11 pages,3 figures", "pdf_url": "http://arxiv.org/pdf/2507.00377v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00377v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00814", "title": "Many LLMs Are More Utilitarian Than One", "authors": ["Anita Keshmirian", "Razan Baltaji", "Babak Hemmatian", "Hadi Asghari", "Lav R. Varshney"], "summary": "Moral judgment is integral to large language model (LLM) alignment and social\nreasoning. As multi-agent systems gain prominence, it becomes crucial to\nunderstand how LLMs function collectively during collaboration, compared to\nindividual agents. In human moral judgment, group deliberation leads to a\nutilitarian boost: a tendency to endorse norm violations that maximize benefits\nfor the greatest number of people despite harms. We study whether a similar\ndynamic emerges in multi-agent LLM systems. We tested six models on\nwell-established sets of moral dilemmas across two conditions: (1) Solo, where\nmodels reasoned independently, and (2) Group, where they engaged in multi-turn\ndiscussions in pairs or triads. In personal moral dilemmas, where agents must\ndecide to directly harm one individual to maximize the utility for others, all\nmodels found moral violations to be more acceptable when part of a group than\nindividually, similar to human experiments. Some models endorsed actions that\nmaximized overall well-being, even if they benefited strangers over familiar\nindividuals. Others became more willing to violate moral norms in groups.\nHowever, while human groups show a similar action bias, the mechanism for their\nutilitarian boost differs from LLMs. Whereas the human shift comes from\nheightened sensitivity to decision outcomes, LLM groups show either reduced\nnorm sensitivity or enhanced impartiality. This suggests that while the surface\nbehavior of LLM collectives mimics human group reasoning, the underlying\ndrivers differ. We discuss the implications for AI alignment, multi-agent\ndesign, and artificial moral reasoning.", "comment": "9 pages, 8 Figures, 7 tables", "pdf_url": "http://arxiv.org/pdf/2507.00814v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "I.2.7; I.2.11"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2507.00814v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00026", "title": "ROSE: Toward Reality-Oriented Safety Evaluation of Large Language Models", "authors": ["Jiale Ding", "Xiang Zheng", "Cong Wang", "Wei-Bin Lee", "Xingjun Ma", "Yu-Gang Jiang"], "summary": "As Large Language Models (LLMs) are increasingly deployed as black-box\ncomponents in real-world applications, evaluating their safety-especially under\nadversarial prompting-has become critical. Arguably, effective safety\nevaluations should be adaptive, evolving with LLM capabilities, and also cover\na broad spectrum of harmful topics and real-world scenarios to fully expose\npotential vulnerabilities. Existing manual safety benchmarks, built on\nhandcrafted adversarial prompts, are limited by their static nature and the\nintensive labor required to update them, making it difficult to keep pace with\nrapidly advancing LLMs. In contrast, automated adversarial prompt generation\noffers a promising path toward adaptive evaluation. However, current methods\noften suffer from insufficient adversarial topic coverage (topic-level\ndiversity) and weak alignment with real-world contexts. These shortcomings stem\nfrom the exploration-exploitation dilemma in black-box optimization and a lack\nof real-world contextualization, resulting in adversarial prompts that are both\ntopically narrow and scenario-repetitive. To address these issues, we propose\nReality-Oriented Safety Evaluation (ROSE), a novel framework that uses\nmulti-objective reinforcement learning to fine-tune an adversarial LLM for\ngenerating topically diverse and contextually rich adversarial prompts.\nExperiments show that ROSE outperforms existing methods in uncovering safety\nvulnerabilities in state-of-the-art LLMs, with notable improvements in\nintegrated evaluation metrics. We hope ROSE represents a step toward more\npractical and reality-oriented safety evaluation of LLMs. WARNING: This paper\ncontains examples of potentially harmful text.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00026v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CY"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00026v1", "date": "2025-06-17", "updated": "2025-06-17"}
{"id": "2507.00087", "title": "pUniFind: a unified large pre-trained deep learning model pushing the limit of mass spectra interpretation", "authors": ["Jiale Zhao", "Pengzhi Mao", "Kaifei Wang", "Yiming Li", "Yaping Peng", "Ranfei Chen", "Shuqi Lu", "Xiaohong Ji", "Jiaxiang Ding", "Xin Zhang", "Yucheng Liao", "Weinan E", "Weijie Zhang", "Han Wen", "Hao Chi"], "summary": "Deep learning has advanced mass spectrometry data interpretation, yet most\nmodels remain feature extractors rather than unified scoring frameworks. We\npresent pUniFind, the first large-scale multimodal pre-trained model in\nproteomics that integrates end-to-end peptide-spectrum scoring with open,\nzero-shot de novo sequencing. Trained on over 100 million open search-derived\nspectra, pUniFind aligns spectral and peptide modalities via cross modality\nprediction and outperforms traditional engines across diverse datasets,\nparticularly achieving a 42.6 percent increase in the number of identified\npeptides in immunopeptidomics. Supporting over 1,300 modifications, pUniFind\nidentifies 60 percent more PSMs than existing de novo methods despite a\n300-fold larger search space. A deep learning based quality control module\nfurther recovers 38.5 percent additional peptides including 1,891 mapped to the\ngenome but absent from reference proteomes while preserving full fragment ion\ncoverage. These results establish a unified, scalable deep learning framework\nfor proteomic analysis, offering improved sensitivity, modification coverage,\nand interpretability.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00087v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00087v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00392", "title": "Learning Dense Feature Matching via Lifting Single 2D Image to 3D Space", "authors": ["Yingping Liang", "Yutao Hu", "Wenqi Shao", "Ying Fu"], "summary": "Feature matching plays a fundamental role in many computer vision tasks, yet\nexisting methods heavily rely on scarce and clean multi-view image collections,\nwhich constrains their generalization to diverse and challenging scenarios.\nMoreover, conventional feature encoders are typically trained on single-view 2D\nimages, limiting their capacity to capture 3D-aware correspondences. In this\npaper, we propose a novel two-stage framework that lifts 2D images to 3D space,\nnamed as \\textbf{Lift to Match (L2M)}, taking full advantage of large-scale and\ndiverse single-view images. To be specific, in the first stage, we learn a\n3D-aware feature encoder using a combination of multi-view image synthesis and\n3D feature Gaussian representation, which injects 3D geometry knowledge into\nthe encoder. In the second stage, a novel-view rendering strategy, combined\nwith large-scale synthetic data generation from single-view images, is employed\nto learn a feature decoder for robust feature matching, thus achieving\ngeneralization across diverse domains. Extensive experiments demonstrate that\nour method achieves superior generalization across zero-shot evaluation\nbenchmarks, highlighting the effectiveness of the proposed framework for robust\nfeature matching.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00392v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00392v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00828", "title": "ProxAnn: Use-Oriented Evaluations of Topic Models and Document Clustering", "authors": ["Alexander Hoyle", "Lorena Calvo-Bartolomé", "Jordan Boyd-Graber", "Philip Resnik"], "summary": "Topic model and document-clustering evaluations either use automated metrics\nthat align poorly with human preferences or require expert labels that are\nintractable to scale. We design a scalable human evaluation protocol and a\ncorresponding automated approximation that reflect practitioners' real-world\nusage of models. Annotators -- or an LLM-based proxy -- review text items\nassigned to a topic or cluster, infer a category for the group, then apply that\ncategory to other documents. Using this protocol, we collect extensive\ncrowdworker annotations of outputs from a diverse set of topic models on two\ndatasets. We then use these annotations to validate automated proxies, finding\nthat the best LLM proxies are statistically indistinguishable from a human\nannotator and can therefore serve as a reasonable substitute in automated\nevaluations. Package, web interface, and data are at\nhttps://github.com/ahoho/proxann", "comment": "Accepted to ACL 2025 (Main)", "pdf_url": "http://arxiv.org/pdf/2507.00828v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2507.00828v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00028", "title": "HiT-JEPA: A Hierarchical Self-supervised Trajectory Embedding Framework for Similarity Computation", "authors": ["Lihuan Li", "Hao Xue", "Shuang Ao", "Yang Song", "Flora Salim"], "summary": "The representation of urban trajectory data plays a critical role in\neffectively analyzing spatial movement patterns. Despite considerable progress,\nthe challenge of designing trajectory representations that can capture diverse\nand complementary information remains an open research problem. Existing\nmethods struggle in incorporating trajectory fine-grained details and\nhigh-level summary in a single model, limiting their ability to attend to both\nlong-term dependencies while preserving local nuances. To address this, we\npropose HiT-JEPA (Hierarchical Interactions of Trajectory Semantics via a Joint\nEmbedding Predictive Architecture), a unified framework for learning\nmulti-scale urban trajectory representations across semantic abstraction\nlevels. HiT-JEPA adopts a three-layer hierarchy that progressively captures\npoint-level fine-grained details, intermediate patterns, and high-level\ntrajectory abstractions, enabling the model to integrate both local dynamics\nand global semantics in one coherent structure. Extensive experiments on\nmultiple real-world datasets for trajectory similarity computation show that\nHiT-JEPA's hierarchical design yields richer, multi-scale representations. Code\nis available at: https://anonymous.4open.science/r/HiT-JEPA.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00028v1", "categories": ["cs.LG", "cs.AI", "cs.CV"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00028v1", "date": "2025-06-17", "updated": "2025-06-17"}
{"id": "2507.00089", "title": "A new machine learning framework for occupational accidents forecasting with safety inspections integration", "authors": ["Aho Yapi", "Pierre Latouche", "Arnaud Guillin", "Yan Bailly"], "summary": "We propose a generic framework for short-term occupational accident\nforecasting that leverages safety inspections and models accident occurrences\nas binary time series. The approach generates daily predictions, which are then\naggregated into weekly safety assessments to better inform decision making. To\nensure the reliability and operational applicability of the forecasts, we apply\na sliding-window cross-validation procedure specifically designed for time\nseries data, combined with an evaluation based on aggregated period-level\nmetrics. Several machine learning algorithms, including logistic regression,\ntree-based models, and neural networks, are trained and systematically compared\nwithin this framework. Unlike the other approaches, the long short-term memory\n(LSTM) network outperforms the other approaches and detects the upcoming\nhigh-risk periods with a balanced accuracy of 0.86, confirming the robustness\nof our methodology and demonstrating that a binary time series model can\nanticipate these critical periods based on safety inspections. The proposed\nmethodology converts routine safety inspection data into clear weekly risk\nscores, detecting the periods when accidents are most likely. Decision-makers\ncan integrate these scores into their planning tools to classify inspection\npriorities, schedule targeted interventions, and funnel resources to the sites\nor shifts classified as highest risk, stepping in before incidents occur and\ngetting the greatest return on safety investments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00089v1", "categories": ["cs.LG", "stat.ME"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00089v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00401", "title": "Few-shot Classification as Multi-instance Verification: Effective Backbone-agnostic Transfer across Domains", "authors": ["Xin Xu", "Eibe Frank", "Geoffrey Holmes"], "summary": "We investigate cross-domain few-shot learning under the constraint that\nfine-tuning of backbones (i.e., feature extractors) is impossible or infeasible\n-- a scenario that is increasingly common in practical use cases. Handling the\nlow-quality and static embeddings produced by frozen, \"black-box\" backbones\nleads to a problem representation of few-shot classification as a series of\nmultiple instance verification (MIV) tasks. Inspired by this representation, we\nintroduce a novel approach to few-shot domain adaptation, named the \"MIV-head\",\nakin to a classification head that is agnostic to any pretrained backbone and\ncomputationally efficient. The core components designed for the MIV-head, when\ntrained on few-shot data from a target domain, collectively yield strong\nperformance on test data from that domain. Importantly, it does so without\nfine-tuning the backbone, and within the \"meta-testing\" phase. Experimenting\nunder various settings and on an extension of the Meta-dataset benchmark for\ncross-domain few-shot image classification, using representative off-the-shelf\nconvolutional neural network and vision transformer backbones pretrained on\nImageNet1K, we show that the MIV-head achieves highly competitive accuracy when\ncompared to state-of-the-art \"adapter\" (or partially fine-tuning) methods\napplied to the same backbones, while incurring substantially lower adaptation\ncost. We also find well-known \"classification head\" approaches lag far behind\nin terms of accuracy. Ablation study empirically justifies the core components\nof our approach. We share our code at https://github.com/xxweka/MIV-head.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00401v1", "categories": ["cs.CV", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00401v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00838", "title": "Stylometry recognizes human and LLM-generated texts in short samples", "authors": ["Karol Przystalski", "Jan K. Argasiński", "Iwona Grabska-Gradzińska", "Jeremi K. Ochab"], "summary": "The paper explores stylometry as a method to distinguish between texts\ncreated by Large Language Models (LLMs) and humans, addressing issues of model\nattribution, intellectual property, and ethical AI use. Stylometry has been\nused extensively to characterise the style and attribute authorship of texts.\nBy applying it to LLM-generated texts, we identify their emergent writing\npatterns. The paper involves creating a benchmark dataset based on Wikipedia,\nwith (a) human-written term summaries, (b) texts generated purely by LLMs\n(GPT-3.5/4, LLaMa 2/3, Orca, and Falcon), (c) processed through multiple text\nsummarisation methods (T5, BART, Gensim, and Sumy), and (d) rephrasing methods\n(Dipper, T5). The 10-sentence long texts were classified by tree-based models\n(decision trees and LightGBM) using human-designed (StyloMetrix) and\nn-gram-based (our own pipeline) stylometric features that encode lexical,\ngrammatical, syntactic, and punctuation patterns. The cross-validated results\nreached a performance of up to .87 Matthews correlation coefficient in the\nmulticlass scenario with 7 classes, and accuracy between .79 and 1. in binary\nclassification, with the particular example of Wikipedia and GPT-4 reaching up\nto .98 accuracy on a balanced dataset. Shapley Additive Explanations pinpointed\nfeatures characteristic of the encyclopaedic text type, individual overused\nwords, as well as a greater grammatical standardisation of LLMs with respect to\nhuman-written texts. These results show -- crucially, in the context of the\nincreasingly sophisticated LLMs -- that it is possible to distinguish machine-\nfrom human-generated texts at least for a well-defined text type.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00838v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2507.00838v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00029", "title": "LoRA-Mixer: Coordinate Modular LoRA Experts Through Serial Attention Routing", "authors": ["Wenbing Li", "Zikai Song", "Hang Zhou", "Yunyao Zhang", "Junqing Yu", "Wei Yang"], "summary": "Recent efforts to combine low-rank adaptation (LoRA) with mixture-of-experts\n(MoE) for adapting large language models (LLMs) to multiple tasks still exhibit\nprevailing limitations: they either swap entire attention/feed-forward layers\nfor switch experts or bolt on parallel expert branches, diluting parameter\nefficiency and task fidelity. We propose the LoRA-Mixer, a modular and\nlightweight MoE framework that integrates LoRA experts. Our core innovation\nlies in replacing the projection matrices of the attention module's\ninput/output linear layers with dynamically routed, task-specific LoRA experts.\nThis design ensures seamless compatibility with diverse foundation models,\nincluding transformers and state space models (SSMs), by leveraging their\ninherent linear projection structures. The framework supports two operational\nparadigms: (1) joint optimization of LoRA experts and routing mechanisms via a\nnovel hard-soft routing strategy, or (2) direct deployment of pre-trained,\nfrozen LoRA modules sourced from external repositories. To enable robust router\ntraining with limited data while ensuring stable routing decisions and\nmaximizing expert reuse, we introduce an adaptive Specialization Balance Loss\n(SBL) that jointly optimizes expert balance and task-specific alignment.\nExtensive experiments on seven benchmark datasets, including MedQA, CoLA,\nSST-2, GSM8K, ARC-E, ARC-C, and HumanEval, demonstrate the effectiveness of\nLoRA-Mixer. On datasets such as GSM8K, HumanEval, and MedQA, LoRA-Mixer\nachieves significant improvements of 7.61%, 4.88%, and 3.08% over the base\nmodels, respectively. Compared with state-of-the-art methods, LoRA-Mixer\nachieves additional improvements of 1.09%, 1.45%, and 1.68%, respectively,\nusing only 48% of the parameters, demonstrating its efficiency and strong\nperformance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00029v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00029v1", "date": "2025-06-17", "updated": "2025-06-17"}
{"id": "2507.00090", "title": "Generating Heterogeneous Multi-dimensional Data : A Comparative Study", "authors": ["Corbeau Michael", "Claeys Emmanuelle", "Serrurier Mathieu", "Zaraté Pascale"], "summary": "Allocation of personnel and material resources is highly sensible in the case\nof firefighter interventions. This allocation relies on simulations to\nexperiment with various scenarios. The main objective of this allocation is the\nglobal optimization of the firefighters response. Data generation is then\nmandatory to study various scenarios In this study, we propose to compare\ndifferent data generation methods. Methods such as Random Sampling, Tabular\nVariational Autoencoders, standard Generative Adversarial Networks, Conditional\nTabular Generative Adversarial Networks and Diffusion Probabilistic Models are\nexamined to ascertain their efficacy in capturing the intricacies of\nfirefighter interventions. Traditional evaluation metrics often fall short in\ncapturing the nuanced requirements of synthetic datasets for real-world\nscenarios. To address this gap, an evaluation of synthetic data quality is\nconducted using a combination of domain-specific metrics tailored to the\nfirefighting domain and standard measures such as the Wasserstein distance.\nDomain-specific metrics include response time distribution, spatial-temporal\ndistribution of interventions, and accidents representation. These metrics are\ndesigned to assess data variability, the preservation of fine and complex\ncorrelations and anomalies such as event with a very low occurrence, the\nconformity with the initial statistical distribution and the operational\nrelevance of the synthetic data. The distribution has the particularity of\nbeing highly unbalanced, none of the variables following a Gaussian\ndistribution, adding complexity to the data generation process.", "comment": "accepted at IEEE SMC 2025 Vienna", "pdf_url": "http://arxiv.org/pdf/2507.00090v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00090v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00429", "title": "DiGA3D: Coarse-to-Fine Diffusional Propagation of Geometry and Appearance for Versatile 3D Inpainting", "authors": ["Jingyi Pan", "Dan Xu", "Qiong Luo"], "summary": "Developing a unified pipeline that enables users to remove, re-texture, or\nreplace objects in a versatile manner is crucial for text-guided 3D inpainting.\nHowever, there are still challenges in performing multiple 3D inpainting tasks\nwithin a unified framework: 1) Single reference inpainting methods lack\nrobustness when dealing with views that are far from the reference view. 2)\nAppearance inconsistency arises when independently inpainting multi-view images\nwith 2D diffusion priors; 3) Geometry inconsistency limits performance when\nthere are significant geometric changes in the inpainting regions. To tackle\nthese challenges, we introduce DiGA3D, a novel and versatile 3D inpainting\npipeline that leverages diffusion models to propagate consistent appearance and\ngeometry in a coarse-to-fine manner. First, DiGA3D develops a robust strategy\nfor selecting multiple reference views to reduce errors during propagation.\nNext, DiGA3D designs an Attention Feature Propagation (AFP) mechanism that\npropagates attention features from the selected reference views to other views\nvia diffusion models to maintain appearance consistency. Furthermore, DiGA3D\nintroduces a Texture-Geometry Score Distillation Sampling (TG-SDS) loss to\nfurther improve the geometric consistency of inpainted 3D scenes. Extensive\nexperiments on multiple 3D inpainting tasks demonstrate the effectiveness of\nour method. The project page is available at https://rorisis.github.io/DiGA3D/.", "comment": "ICCV 2025, Project page: https://rorisis.github.io/DiGA3D/", "pdf_url": "http://arxiv.org/pdf/2507.00429v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00429v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00875", "title": "TransLaw: Benchmarking Large Language Models in Multi-Agent Simulation of the Collaborative Translation", "authors": ["Xi Xuan", "King-kui Sin", "Yufei Zhou", "Chunyu Kit"], "summary": "Multi-agent systems empowered by large language models (LLMs) have\ndemonstrated remarkable capabilities in a wide range of downstream\napplications, including machine translation. However, the potential of LLMs in\ntranslating Hong Kong legal judgments remains uncertain due to challenges such\nas intricate legal terminology, culturally embedded nuances, and strict\nlinguistic structures. In this work, we introduce TransLaw, a novel multi-agent\nframework implemented for real-world Hong Kong case law translation. It employs\nthree specialized agents, namely, Translator, Annotator, and Proofreader, to\ncollaboratively produce translations for high accuracy in legal meaning,\nappropriateness in style, and adequate coherence and cohesion in structure.\nThis framework supports customizable LLM configurations and achieves tremendous\ncost reduction compared to professional human translation services. We\nevaluated its performance using 13 open-source and commercial LLMs as agents\nand obtained interesting findings, including that it surpasses GPT-4o in legal\nsemantic accuracy, structural coherence, and stylistic fidelity, yet trails\nhuman experts in contextualizing complex terminology and stylistic naturalness.\nOur platform website is available at CityUHK, and our bilingual judgment corpus\nused for the evaluation is available at Hugging Face.", "comment": "arXiv admin note: text overlap with arXiv:2501.09444; text overlap\n  with arXiv:2409.20288 by other authors", "pdf_url": "http://arxiv.org/pdf/2507.00875v1", "categories": ["cs.CL", "cs.HC", "cs.MA"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2507.00875v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00030", "title": "Adaptive Action Duration with Contextual Bandits for Deep Reinforcement Learning in Dynamic Environments", "authors": ["Abhishek Verma", "Nallarasan V", "Balaraman Ravindran"], "summary": "Deep Reinforcement Learning (DRL) has achieved remarkable success in complex\nsequential decision-making tasks, such as playing Atari 2600 games and\nmastering board games. A critical yet underexplored aspect of DRL is the\ntemporal scale of action execution. We propose a novel paradigm that integrates\ncontextual bandits with DRL to adaptively select action durations, enhancing\npolicy flexibility and computational efficiency. Our approach augments a Deep\nQ-Network (DQN) with a contextual bandit module that learns to choose optimal\naction repetition rates based on state contexts. Experiments on Atari 2600\ngames demonstrate significant performance improvements over static duration\nbaselines, highlighting the efficacy of adaptive temporal abstractions in DRL.\nThis paradigm offers a scalable solution for real-time applications like gaming\nand robotics, where dynamic action durations are critical.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00030v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00030v1", "date": "2025-06-17", "updated": "2025-06-17"}
{"id": "2507.00101", "title": "DFReg: A Physics-Inspired Framework for Global Weight Distribution Regularization in Neural Networks", "authors": ["Giovanni Ruggieri"], "summary": "We introduce DFReg, a physics-inspired regularization method for deep neural\nnetworks that operates on the global distribution of weights. Drawing from\nDensity Functional Theory (DFT), DFReg applies a functional penalty to\nencourage smooth, diverse, and well-distributed weight configurations. Unlike\ntraditional techniques such as Dropout or L2 decay, DFReg imposes global\nstructural regularity without architectural changes or stochastic\nperturbations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00101v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00101v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00430", "title": "MFH: Marrying Frequency Domain with Handwritten Mathematical Expression Recognition", "authors": ["Huanxin Yang", "Qiwen Wang"], "summary": "Handwritten mathematical expression recognition (HMER) suffers from complex\nformula structures and character layouts in sequence prediction. In this paper,\nwe incorporate frequency domain analysis into HMER and propose a method that\nmarries frequency domain with HMER (MFH), leveraging the discrete cosine\ntransform (DCT). We emphasize the structural analysis assistance of frequency\ninformation for recognizing mathematical formulas. When implemented on various\nbaseline models, our network exhibits a consistent performance enhancement,\ndemonstrating the efficacy of frequency domain information. Experiments show\nthat our MFH-CoMER achieves noteworthy accuracyrates of 61.66%/62.07%/63.72% on\nthe CROHME 2014/2016/2019 test sets. The source code is available at\nhttps://github.com/Hryxyhe/MFH.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00430v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00430v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00883", "title": "Mathematics Isn't Culture-Free: Probing Cultural Gaps via Entity and Scenario Perturbations", "authors": ["Aditya Tomar", "Nihar Ranjan Sahoo", "Ashish Mittal", "Rudra Murthy", "Pushpak Bhattacharyya"], "summary": "Although mathematics is often considered culturally neutral, the way\nmathematical problems are presented can carry implicit cultural context.\nExisting benchmarks like GSM8K are predominantly rooted in Western norms,\nincluding names, currencies, and everyday scenarios. In this work, we create\nculturally adapted variants of the GSM8K test set for five regions Africa,\nIndia, China, Korea, and Japan using prompt-based transformations followed by\nmanual verification. We evaluate six large language models (LLMs), ranging from\n8B to 72B parameters, across five prompting strategies to assess their\nrobustness to cultural variation in math problem presentation. Our findings\nreveal a consistent performance gap: models perform best on the original\nUS-centric dataset and comparatively worse on culturally adapted versions.\nHowever, models with reasoning capabilities are more resilient to these shifts,\nsuggesting that deeper reasoning helps bridge cultural presentation gaps in\nmathematical tasks", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00883v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2507.00883v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00032", "title": "Ken Utilization Layer: Hebbian Replay Within a Student's Ken for Adaptive Knowledge Tracing", "authors": ["Grey Kuling", "Marinka Zitnik"], "summary": "We introduce KUL-KT, a biologically inspired architecture for knowledge\ntracing (KT), combining Hebbian memory encoding with gradient-based\nconsolidation in a scalable, input-agnostic framework. KUL-KT adapts the\nprinciple of memory consolidation in neural systems, to student modeling by\nintroducing two key innovations: (i) a time-decaying Hebbian memory update that\nenables graceful forgetting, and (ii) a novel Loss-aligned Internal Target\n(LIT) method to compute an ideal internal state, allowing continual learning\nwithout backpropagation through time. The architecture consists of a fast\nHebbian memory that captures each learner interaction via a single associative\nupdate, and a slower linear network that consolidates recalled samples through\ngradient descent. This design enables few-shot personalization and natural\nforgetting without storing raw data or relying on large cohort training.\nOperating entirely in embedding space, KUL-KT supports both structured\n(tabular) and unstructured (short-answer) inputs. Empirically, KUL-KT\noutperforms strong baselines on ten public KT benchmarks in rank-sensitive\nmetrics such as nDCG and Recall@10. In a classroom deployment, KUL-KT\npersonalized quizzes from short-answer data, leading to improved\nlearner-perceived helpfulness and reduced difficulty (p < 0.05). Ablation\nstudies confirm that Hebbian decay and LIT are critical for continual\nadaptation. Compared to a strong graph-based KT model, KUL-KT trains 1.75x\nfaster and uses 99.01\\% less memory. These results position KUL-KT as a\nbiologically grounded, memory-efficient, and input-flexible framework for\npersonalized learning at scale.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00032v1", "categories": ["cs.CY", "cs.AI", "cs.LG", "cs.NE"], "cate": "cs.CY", "url": "http://arxiv.org/abs/2507.00032v1", "date": "2025-06-18", "updated": "2025-06-18"}
{"id": "2507.00102", "title": "Towards transparent and data-driven fault detection in manufacturing: A case study on univariate, discrete time series", "authors": ["Bernd Hofmann", "Patrick Bruendl", "Huong Giang Nguyen", "Joerg Franke"], "summary": "Ensuring consistent product quality in modern manufacturing is crucial,\nparticularly in safety-critical applications. Conventional quality control\napproaches, reliant on manually defined thresholds and features, lack\nadaptability to the complexity and variability inherent in production data and\nnecessitate extensive domain expertise. Conversely, data-driven methods, such\nas machine learning, demonstrate high detection performance but typically\nfunction as black-box models, thereby limiting their acceptance in industrial\nenvironments where interpretability is paramount. This paper introduces a\nmethodology for industrial fault detection, which is both data-driven and\ntransparent. The approach integrates a supervised machine learning model for\nmulti-class fault classification, Shapley Additive Explanations for post-hoc\ninterpretability, and a do-main-specific visualisation technique that maps\nmodel explanations to operator-interpretable features. Furthermore, the study\nproposes an evaluation methodology that assesses model explanations through\nquantitative perturbation analysis and evaluates visualisations by qualitative\nexpert assessment. The approach was applied to the crimping process, a\nsafety-critical joining technique, using a dataset of univariate, discrete time\nseries. The system achieves a fault detection accuracy of 95.9 %, and both\nquantitative selectivity analysis and qualitative expert evaluations confirmed\nthe relevance and inter-pretability of the generated explanations. This\nhuman-centric approach is designed to enhance trust and interpretability in\ndata-driven fault detection, thereby contributing to applied system design in\nindustrial quality control.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00102v1", "categories": ["cs.LG", "cs.AI", "eess.SP"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00102v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00447", "title": "Latent Posterior-Mean Rectified Flow for Higher-Fidelity Perceptual Face Restoration", "authors": ["Xin Luo", "Menglin Zhang", "Yunwei Lan", "Tianyu Zhang", "Rui Li", "Chang Liu", "Dong Liu"], "summary": "The Perception-Distortion tradeoff (PD-tradeoff) theory suggests that face\nrestoration algorithms must balance perceptual quality and fidelity. To achieve\nminimal distortion while maintaining perfect perceptual quality, Posterior-Mean\nRectified Flow (PMRF) proposes a flow based approach where source distribution\nis minimum distortion estimations. Although PMRF is shown to be effective, its\npixel-space modeling approach limits its ability to align with human\nperception, where human perception is defined as how humans distinguish between\ntwo image distributions. In this work, we propose Latent-PMRF, which\nreformulates PMRF in the latent space of a variational autoencoder (VAE),\nfacilitating better alignment with human perception during optimization. By\ndefining the source distribution on latent representations of minimum\ndistortion estimation, we bound the minimum distortion by the VAE's\nreconstruction error. Moreover, we reveal the design of VAE is crucial, and our\nproposed VAE significantly outperforms existing VAEs in both reconstruction and\nrestoration. Extensive experiments on blind face restoration demonstrate the\nsuperiority of Latent-PMRF, offering an improved PD-tradeoff compared to\nexisting methods, along with remarkable convergence efficiency, achieving a\n5.79X speedup over PMRF in terms of FID. Our code will be available as\nopen-source.", "comment": "Code and Models will be publicly available at\n  https://github.com/Luciennnnnnn/Latent-PMRF", "pdf_url": "http://arxiv.org/pdf/2507.00447v1", "categories": ["cs.CV", "eess.IV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00447v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00885", "title": "Scaling Laws Are Unreliable for Downstream Tasks: A Reality Check", "authors": ["Nicholas Lourie", "Michael Y. Hu", "Kyunghyun Cho"], "summary": "Downstream scaling laws aim to predict task performance at larger scales from\npretraining losses at smaller scales. Whether this prediction should be\npossible is unclear: some works demonstrate that task performance follows clear\nlinear scaling trends under transformation, whereas others point out\nfundamental challenges to downstream scaling laws, such as emergence and\ninverse scaling. In this work, we conduct a meta-analysis of existing data on\ndownstream scaling laws, finding that close fit to linear scaling laws only\noccurs in a minority of cases: 39% of the time. Furthermore, seemingly benign\nchanges to the experimental setting can completely change the scaling trend.\nOur analysis underscores the need to understand the conditions under which\nscaling laws succeed. To fully model the relationship between pretraining loss\nand downstream task performance, we must embrace the cases in which scaling\nbehavior deviates from linear trends.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00885v1", "categories": ["cs.CL", "cs.LG"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2507.00885v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00033", "title": "Moment Sampling in Video LLMs for Long-Form Video QA", "authors": ["Mustafa Chasmai", "Gauri Jagatap", "Gouthaman KV", "Grant Van Horn", "Subhransu Maji", "Andrea Fanelli"], "summary": "Recent advancements in video large language models (Video LLMs) have\nsignificantly advanced the field of video question answering (VideoQA). While\nexisting methods perform well on short videos, they often struggle with\nlong-range reasoning in longer videos. To scale Video LLMs for longer video\ncontent, frame sub-sampling (selecting frames at regular intervals) is commonly\nused. However, this approach is suboptimal, often leading to the loss of\ncrucial frames or the inclusion of redundant information from multiple similar\nframes. Missing key frames impairs the model's ability to answer questions\naccurately, while redundant frames lead the model to focus on irrelevant video\nsegments and increase computational resource consumption. In this paper, we\ninvestigate the use of a general-purpose text-to-video moment retrieval model\nto guide the frame sampling process. We propose \"moment sampling\", a novel,\nmodel-agnostic approach that enables the model to select the most relevant\nframes according to the context of the question. Specifically, we employ a\nlightweight moment retrieval model to prioritize frame selection. By focusing\non the frames most pertinent to the given question, our method enhances\nlong-form VideoQA performance in Video LLMs. Through extensive experiments on\nfour long-form VideoQA datasets, using four state-of-the-art Video LLMs, we\ndemonstrate the effectiveness of the proposed approach.", "comment": "Workshop on Video Large Language Models (VidLLMs) at CVPR 2025", "pdf_url": "http://arxiv.org/pdf/2507.00033v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00033v1", "date": "2025-06-18", "updated": "2025-06-18"}
{"id": "2507.00105", "title": "Graph Neural Networks in Wind Power Forecasting", "authors": ["Javier Castellano", "Ignacio Villanueva"], "summary": "We study the applicability of GNNs to the problem of wind energy forecasting.\nWe find that certain architectures achieve performance comparable to our best\nCNN-based benchmark. The study is conducted on three wind power facilities\nusing five years of historical data. Numerical Weather Prediction (NWP)\nvariables were used as predictors, and models were evaluated on a 24 to 36 hour\nahead test horizon.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00105v1", "categories": ["cs.LG", "cs.SY", "eess.SY"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00105v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00454", "title": "ATSTrack: Enhancing Visual-Language Tracking by Aligning Temporal and Spatial Scales", "authors": ["Yihao Zhen", "Qiang Wang", "Yu Qiao", "Liangqiong Qu", "Huijie Fan"], "summary": "A main challenge of Visual-Language Tracking (VLT) is the misalignment\nbetween visual inputs and language descriptions caused by target movement.\nPrevious trackers have explored many effective feature modification methods to\npreserve more aligned features. However, an important yet unexplored factor\nultimately hinders their capability, which is the inherent differences in the\ntemporal and spatial scale of information between visual and language inputs.\nTo address this issue, we propose a novel visual-language tracker that enhances\nthe effect of feature modification by \\textbf{A}ligning \\textbf{T}emporal and\n\\textbf{S}patial scale of different input components, named as\n\\textbf{ATSTrack}. Specifically, we decompose each language description into\nphrases with different attributes based on their temporal and spatial\ncorrespondence with visual inputs, and modify their features in a fine-grained\nmanner. Moreover, we introduce a Visual-Language token that comprises modified\nlinguistic information from the previous frame to guide the model to extract\nvisual features that are more relevant to language description, thereby\nreducing the impact caused by the differences in spatial scale. Experimental\nresults show that our proposed ATSTrack achieves performance comparable to\nexisting methods. Our code will be released.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00454v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00454v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00891", "title": "MemeCMD: An Automatically Generated Chinese Multi-turn Dialogue Dataset with Contextually Retrieved Memes", "authors": ["Yuheng Wang", "Xianhe Tang", "Pufeng Huang"], "summary": "Memes are widely used in online social interactions, providing vivid,\nintuitive, and often humorous means to express intentions and emotions.\nExisting dialogue datasets are predominantly limited to either manually\nannotated or pure-text conversations, lacking the expressiveness and contextual\nnuance that multimodal interactions provide.To address these challenges, we\nintroduce MemeCMD, an automatically generated Chinese Multi-turn Dialogue\ndataset with contextually retrieved memes. Our dataset combines a large-scale,\nMLLM-annotated meme library with dialogues auto-generated by dual agents across\ndiverse scenarios. We introduce a retrieval framework and adaptive threshold to\nensure contextually relevant, naturally spaced meme usage. Experiments\ndemonstrate the effectiveness of our approach in generating contextually\nappropriate and diverse meme-incorporated dialogues, offering a scalable and\nprivacy-preserving resource for advancing multimodal conversational AI.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00891v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2507.00891v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00037", "title": "Model Fusion via Neuron Interpolation", "authors": ["Phoomraphee Luenam", "Andreas Spanopoulos", "Amit Sant", "Thomas Hofmann", "Sotiris Anagnostidis", "Sidak Pal Singh"], "summary": "Model fusion aims to combine the knowledge of multiple models by creating one\nrepresentative model that captures the strengths of all of its parents.\nHowever, this process is non-trivial due to differences in internal\nrepresentations, which can stem from permutation invariance, random\ninitialization, or differently distributed training data. We present a novel,\nneuron-centric family of model fusion algorithms designed to integrate multiple\ntrained neural networks into a single network effectively regardless of\ntraining data distribution. Our algorithms group intermediate neurons of parent\nmodels to create target representations that the fused model approximates with\nits corresponding sub-network. Unlike prior approaches, our approach\nincorporates neuron attribution scores into the fusion process. Furthermore,\nour algorithms can generalize to arbitrary layer types. Experimental results on\nvarious benchmark datasets demonstrate that our algorithms consistently\noutperform previous fusion techniques, particularly in zero-shot and non-IID\nfusion scenarios. The code is available at\nhttps://github.com/AndrewSpano/neuron-interpolation-model-fusion.", "comment": "5 figures, 15 tables, 23 pages", "pdf_url": "http://arxiv.org/pdf/2507.00037v1", "categories": ["cs.LG", "cs.AI", "I.2.6; I.2.1"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00037v1", "date": "2025-06-18", "updated": "2025-06-18"}
{"id": "2507.00184", "title": "Text-to-Level Diffusion Models With Various Text Encoders for Super Mario Bros", "authors": ["Jacob Schrum", "Olivia Kilday", "Emilio Salas", "Bess Hagan", "Reid Williams"], "summary": "Recent research shows how diffusion models can unconditionally generate\ntile-based game levels, but use of diffusion models for text-to-level\ngeneration is underexplored. There are practical considerations for creating a\nusable model: caption/level pairs are needed, as is a text embedding model, and\na way of generating entire playable levels, rather than individual scenes. We\npresent strategies to automatically assign descriptive captions to an existing\nlevel dataset, and train diffusion models using both pretrained text encoders\nand simple transformer models trained from scratch. Captions are automatically\nassigned to generated levels so that the degree of overlap between input and\noutput captions can be compared. We also assess the diversity and playability\nof the resulting levels. Results are compared with an unconditional diffusion\nmodel and a generative adversarial network, as well as the text-to-level\napproaches Five-Dollar Model and MarioGPT. Notably, the best diffusion model\nuses a simple transformer model for text embedding, and takes less time to\ntrain than diffusion models employing more complex text encoders, indicating\nthat reliance on larger language models is not necessary. We also present a GUI\nallowing designers to construct long levels from model-generated scenes.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00184v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00184v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00462", "title": "Unleashing the Potential of All Test Samples: Mean-Shift Guided Test-Time Adaptation", "authors": ["Jizhou Han", "Chenhao Ding", "SongLin Dong", "Yuhang He", "Xinyuan Gao", "Yihong Gong"], "summary": "Visual-language models (VLMs) like CLIP exhibit strong generalization but\nstruggle with distribution shifts at test time. Existing training-free\ntest-time adaptation (TTA) methods operate strictly within CLIP's original\nfeature space, relying on high-confidence samples while overlooking the\npotential of low-confidence ones. We propose MS-TTA, a training-free approach\nthat enhances feature representations beyond CLIP's space using a single-step\nk-nearest neighbors (kNN) Mean-Shift. By refining all test samples, MS-TTA\nimproves feature compactness and class separability, leading to more stable\nadaptation. Additionally, a cache of refined embeddings further enhances\ninference by providing Mean Shift enhanced logits. Extensive evaluations on OOD\nand cross-dataset benchmarks demonstrate that MS-TTA consistently outperforms\nstate-of-the-art training-free TTA methods, achieving robust adaptation without\nrequiring additional training.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00462v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00462v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00911", "title": "The Cognate Data Bottleneck in Language Phylogenetics", "authors": ["Luise Häuser", "Alexandros Stamatakis"], "summary": "To fully exploit the potential of computational phylogenetic methods for\ncognate data one needs to leverage specific (complex) models an machine\nlearning-based techniques. However, both approaches require datasets that are\nsubstantially larger than the manually collected cognate data currently\navailable. To the best of our knowledge, there exists no feasible approach to\nautomatically generate larger cognate datasets. We substantiate this claim by\nautomatically extracting datasets from BabelNet, a large multilingual\nencyclopedic dictionary. We demonstrate that phylogenetic inferences on the\nrespective character matrices yield trees that are largely inconsistent with\nthe established gold standard ground truth trees. We also discuss why we\nconsider it as being unlikely to be able to extract more suitable character\nmatrices from other multilingual resources. Phylogenetic data analysis\napproaches that require larger datasets can therefore not be applied to cognate\ndata. Thus, it remains an open question how, and if these computational\napproaches can be applied in historical linguistics.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00911v1", "categories": ["cs.CL", "q-bio.PE"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2507.00911v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00038", "title": "Quality over Quantity: An Effective Large-Scale Data Reduction Strategy Based on Pointwise V-Information", "authors": ["Fei Chen", "Wenchi Zhou"], "summary": "Data reduction plays a vital role in data-centric AI by identifying the most\ninformative instance within large-scale datasets to enhance model training\nefficiency. The core challenge lies in how to select the optimal\ninstances-rather than the entire datasets-to improve data quality and training\nefficiency. In this paper, we propose an effective data reduction strategy\nbased on Pointwise V-information(PVI). First, we quantify instance difficulty\nusing PVI and filter out low-difficulty instances enabling a static approach.\nExperiments demonstrate that removing 10%-30% of the data preserves the\nclassifier performance with only a 0.0001% to 0.76% loss in accuracy.Second, we\nuse a progressive learning approach to training the classifiers on instances\nsorted by ascending PVI, accelerating convergence and achieving a 0.8% accuracy\ngain over conventional training. Our results suggest that with the effective\ndata reduction strategy, training a classifier on the selected optimal subset\ncould enhance the model performance and boost training efficiency. Moreover, we\nhave transferred the PVI framework, which previously applied only to English\ndatasets, to diverse Chinese NLP tasks and base models, leading to valuable\ninsights for cross-lingual data reduction and faster training. The codes are\nreleased at https://github.com/zhouwenchi/DatasetReductionStrategy.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00038v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00038v1", "date": "2025-06-19", "updated": "2025-06-19"}
{"id": "2507.00191", "title": "Beyond Sensor Data: Foundation Models of Behavioral Data from Wearables Improve Health Predictions", "authors": ["Eray Erturk", "Fahad Kamran", "Salar Abbaspourazad", "Sean Jewell", "Harsh Sharma", "Yujie Li", "Sinead Williamson", "Nicholas J Foti", "Joseph Futoma"], "summary": "Wearable devices record physiological and behavioral signals that can improve\nhealth predictions. While foundation models are increasingly used for such\npredictions, they have been primarily applied to low-level sensor data, despite\nbehavioral data often being more informative due to their alignment with\nphysiologically relevant timescales and quantities. We develop foundation\nmodels of such behavioral signals using over 2.5B hours of wearable data from\n162K individuals, systematically optimizing architectures and tokenization\nstrategies for this unique dataset. Evaluated on 57 health-related tasks, our\nmodel shows strong performance across diverse real-world applications including\nindividual-level classification and time-varying health state prediction. The\nmodel excels in behavior-driven tasks like sleep prediction, and improves\nfurther when combined with representations of raw sensor data. These results\nunderscore the importance of tailoring foundation model design to wearables and\ndemonstrate the potential to enable new health applications.", "comment": "Accepted to ICML 2025", "pdf_url": "http://arxiv.org/pdf/2507.00191v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00191v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00469", "title": "Bisecle: Binding and Separation in Continual Learning for Video Language Understanding", "authors": ["Yue Tan", "Xiaoqian Hu", "Hao Xue", "Celso De Melo", "Flora D. Salim"], "summary": "Frontier vision-language models (VLMs) have made remarkable improvements in\nvideo understanding tasks. However, real-world videos typically exist as\ncontinuously evolving data streams (e.g., dynamic scenes captured by wearable\nglasses), necessitating models to continually adapt to shifting data\ndistributions and novel scenarios. Considering the prohibitive computational\ncosts of fine-tuning models on new tasks, usually, a small subset of parameters\nis updated while the bulk of the model remains frozen. This poses new\nchallenges to existing continual learning frameworks in the context of large\nmultimodal foundation models, i.e., catastrophic forgetting and update\nconflict. While the foundation models struggle with parameter-efficient\ncontinual learning, the hippocampus in the human brain has evolved highly\nefficient mechanisms for memory formation and consolidation. Inspired by the\nrapid Binding and pattern separation mechanisms in the hippocampus, in this\nwork, we propose Bisecle for video-language continual learning, where a\nmulti-directional supervision module is used to capture more cross-modal\nrelationships and a contrastive prompt learning scheme is designed to isolate\ntask-specific knowledge to facilitate efficient memory storage. Binding and\nseparation processes further strengthen the ability of VLMs to retain complex\nexperiences, enabling robust and efficient continual learning in video\nunderstanding tasks. We perform a thorough evaluation of the proposed Bisecle,\ndemonstrating its ability to mitigate forgetting and enhance cross-task\ngeneralization on several VideoQA benchmarks.", "comment": "23 pages, 12 figures, 10 tables", "pdf_url": "http://arxiv.org/pdf/2507.00469v1", "categories": ["cs.CV", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00469v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00985", "title": "Discourse Heuristics For Paradoxically Moral Self-Correction", "authors": ["Guangliang Liu", "Zimo Qi", "Xitong Zhang", "Kristen Marie Johnson"], "summary": "Moral self-correction has emerged as a promising approach for aligning the\noutput of Large Language Models (LLMs) with human moral values. However, moral\nself-correction techniques are subject to two primary paradoxes. First, despite\nempirical and theoretical evidence to support the effectiveness of\nself-correction, this LLM capability only operates at a superficial level.\nSecond, while LLMs possess the capability of self-diagnosing immoral aspects of\ntheir output, they struggle to identify the cause of this moral inconsistency\nduring their self-correction process. To better understand and address these\nparadoxes, we analyze the discourse constructions in fine-tuning corpora\ndesigned to enhance moral self-correction, uncovering the existence of the\nheuristics underlying effective constructions. We demonstrate that moral\nself-correction relies on discourse constructions that reflect heuristic\nshortcuts, and that the presence of these heuristic shortcuts during\nself-correction leads to inconsistency when attempting to enhance both\nself-correction and self-diagnosis capabilities jointly. Based on our findings,\nwe propose a solution to improve moral self-correction by leveraging the\nheuristics of curated datasets. We also highlight the generalization challenges\nof this capability, particularly in terms of learning from situated context and\nmodel scales.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00985v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2507.00985v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00039", "title": "Pattern-Based Graph Classification: Comparison of Quality Measures and Importance of Preprocessing", "authors": ["Lucas Potin", "Rosa Figueiredo", "Vincent Labatut", "Christine Largeron"], "summary": "Graph classification aims to categorize graphs based on their structural and\nattribute features, with applications in diverse fields such as social network\nanalysis and bioinformatics. Among the methods proposed to solve this task,\nthose relying on patterns (i.e. subgraphs) provide good explainability, as the\npatterns used for classification can be directly interpreted. To identify\nmeaningful patterns, a standard approach is to use a quality measure, i.e. a\nfunction that evaluates the discriminative power of each pattern. However, the\nliterature provides tens of such measures, making it difficult to select the\nmost appropriate for a given application. Only a handful of surveys try to\nprovide some insight by comparing these measures, and none of them specifically\nfocuses on graphs. This typically results in the systematic use of the most\nwidespread measures, without thorough evaluation. To address this issue, we\npresent a comparative analysis of 38 quality measures from the literature. We\ncharacterize them theoretically, based on four mathematical properties. We\nleverage publicly available datasets to constitute a benchmark, and propose a\nmethod to elaborate a gold standard ranking of the patterns. We exploit these\nresources to perform an empirical comparison of the measures, both in terms of\npattern ranking and classification performance. Moreover, we propose a\nclustering-based preprocessing step, which groups patterns appearing in the\nsame graphs to enhance classification performance. Our experimental results\ndemonstrate the effectiveness of this step, reducing the number of patterns to\nbe processed while achieving comparable performance. Additionally, we show that\nsome popular measures widely used in the literature are not associated with the\nbest results.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00039v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00039v1", "date": "2025-06-19", "updated": "2025-06-19"}
{"id": "2507.00195", "title": "What Makes Local Updates Effective: The Role of Data Heterogeneity and Smoothness", "authors": ["Kumar Kshitij Patel"], "summary": "This thesis contributes to the theoretical understanding of local update\nalgorithms, especially Local SGD, in distributed and federated optimization\nunder realistic models of data heterogeneity. A central focus is on the bounded\nsecond-order heterogeneity assumption, which is shown to be both necessary and\nsufficient for local updates to outperform centralized or mini-batch methods in\nconvex and non-convex settings. The thesis establishes tight upper and lower\nbounds in several regimes for various local update algorithms and characterizes\nthe min-max complexity of multiple problem classes. At its core is a\nfine-grained consensus-error-based analysis framework that yields sharper\nfinite-time convergence bounds under third-order smoothness and relaxed\nheterogeneity assumptions. The thesis also extends to online federated\nlearning, providing fundamental regret bounds under both first-order and bandit\nfeedback. Together, these results clarify when and why local updates offer\nprovable advantages, and the thesis serves as a self-contained guide for\nanalyzing Local SGD in heterogeneous environments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00195v1", "categories": ["cs.LG", "cs.AI", "cs.MA", "math.OC", "stat.ML"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00195v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00472", "title": "ARIG: Autoregressive Interactive Head Generation for Real-time Conversations", "authors": ["Ying Guo", "Xi Liu", "Cheng Zhen", "Pengfei Yan", "Xiaoming Wei"], "summary": "Face-to-face communication, as a common human activity, motivates the\nresearch on interactive head generation. A virtual agent can generate motion\nresponses with both listening and speaking capabilities based on the audio or\nmotion signals of the other user and itself. However, previous clip-wise\ngeneration paradigm or explicit listener/speaker generator-switching methods\nhave limitations in future signal acquisition, contextual behavioral\nunderstanding, and switching smoothness, making it challenging to be real-time\nand realistic. In this paper, we propose an autoregressive (AR) based\nframe-wise framework called ARIG to realize the real-time generation with\nbetter interaction realism. To achieve real-time generation, we model motion\nprediction as a non-vector-quantized AR process. Unlike discrete codebook-index\nprediction, we represent motion distribution using diffusion procedure,\nachieving more accurate predictions in continuous space. To improve interaction\nrealism, we emphasize interactive behavior understanding (IBU) and detailed\nconversational state understanding (CSU). In IBU, based on dual-track\ndual-modal signals, we summarize short-range behaviors through\nbidirectional-integrated learning and perform contextual understanding over\nlong ranges. In CSU, we use voice activity signals and context features of IBU\nto understand the various states (interruption, feedback, pause, etc.) that\nexist in actual conversations. These serve as conditions for the final\nprogressive motion prediction. Extensive experiments have verified the\neffectiveness of our model.", "comment": "ICCV 2025. Homepage: https://jinyugy21.github.io/ARIG/", "pdf_url": "http://arxiv.org/pdf/2507.00472v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00472v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00994", "title": "Should We Still Pretrain Encoders with Masked Language Modeling?", "authors": ["Hippolyte Gisserot-Boukhlef", "Nicolas Boizard", "Manuel Faysse", "Duarte M. Alves", "Emmanuel Malherbe", "André F. T. Martins", "Céline Hudelot", "Pierre Colombo"], "summary": "Learning high-quality text representations is fundamental to a wide range of\nNLP tasks. While encoder pretraining has traditionally relied on Masked\nLanguage Modeling (MLM), recent evidence suggests that decoder models\npretrained with Causal Language Modeling (CLM) can be effectively repurposed as\nencoders, often surpassing traditional encoders on text representation\nbenchmarks. However, it remains unclear whether these gains reflect an inherent\nadvantage of the CLM objective or arise from confounding factors such as model\nand data scale. In this paper, we address this question through a series of\nlarge-scale, carefully controlled pretraining ablations, training a total of 30\nmodels ranging from 210 million to 1 billion parameters, and conducting over\n15,000 fine-tuning and evaluation runs. We find that while training with MLM\ngenerally yields better performance across text representation tasks,\nCLM-trained models are more data-efficient and demonstrate improved fine-tuning\nstability. Building on these findings, we experimentally show that a biphasic\ntraining strategy that sequentially applies CLM and then MLM, achieves optimal\nperformance under a fixed computational training budget. Moreover, we\ndemonstrate that this strategy becomes more appealing when initializing from\nreadily available pretrained CLM models (from the existing LLM ecosystem),\nreducing the computational burden needed to train best-in-class encoder models.\nWe release all project artifacts at https://hf.co/MLMvsCLM to foster further\nresearch.", "comment": "23 pages, 10 figures, 17 tables", "pdf_url": "http://arxiv.org/pdf/2507.00994v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2507.00994v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00042", "title": "Catastrophic Forgetting Mitigation via Discrepancy-Weighted Experience Replay", "authors": ["Xinrun Xu", "Jianwen Yang", "Qiuhong Zhang", "Zhanbiao Lian", "Zhiming Ding", "Shan Jiang"], "summary": "Continually adapting edge models in cloud-edge collaborative object detection\nfor traffic monitoring suffers from catastrophic forgetting, where models lose\npreviously learned knowledge when adapting to new data distributions. This is\nespecially problematic in dynamic traffic environments characterised by\nperiodic variations (e.g., day/night, peak hours), where past knowledge remains\nvaluable. Existing approaches like experience replay and visual prompts offer\nsome mitigation, but struggle to effectively prioritize and leverage historical\ndata for optimal knowledge retention and adaptation. Specifically, simply\nstoring and replaying all historical data can be inefficient, while treating\nall historical experiences as equally important overlooks their varying\nrelevance to the current domain. This paper proposes ER-EMU, an edge model\nupdate algorithm based on adaptive experience replay, to address these\nlimitations. ER-EMU utilizes a limited-size experience buffer managed using a\nFirst-In-First-Out (FIFO) principle, and a novel Domain Distance Metric-based\nExperience Selection (DDM-ES) algorithm. DDM-ES employs the multi-kernel\nmaximum mean discrepancy (MK-MMD) to quantify the dissimilarity between target\ndomains, prioritizing the selection of historical data that is most dissimilar\nto the current target domain. This ensures training diversity and facilitates\nthe retention of knowledge from a wider range of past experiences, while also\npreventing overfitting to the new domain. The experience buffer is also updated\nusing a simple random sampling strategy to maintain a balanced representation\nof previous domains. Experiments on the Bellevue traffic video dataset,\ninvolving repeated day/night cycles, demonstrate that ER-EMU consistently\nimproves the performance of several state-of-the-art cloud-edge collaborative\nobject detection frameworks.", "comment": "ICANN 2025", "pdf_url": "http://arxiv.org/pdf/2507.00042v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00042v1", "date": "2025-06-23", "updated": "2025-06-23"}
{"id": "2507.00230", "title": "PPFL-RDSN: Privacy-Preserving Federated Learning-based Residual Dense Spatial Networks for Encrypted Lossy Image Reconstruction", "authors": ["Peilin He", "James Joshi"], "summary": "Reconstructing high-quality images from low-resolution inputs using Residual\nDense Spatial Networks (RDSNs) is crucial yet challenging, particularly in\ncollaborative scenarios where centralized training poses significant privacy\nrisks, including data leakage and inference attacks, as well as high\ncomputational costs. We propose a novel Privacy-Preserving Federated\nLearning-based RDSN (PPFL-RDSN) framework specifically tailored for lossy image\nreconstruction. PPFL-RDSN integrates Federated Learning (FL), local\ndifferential privacy, and robust model watermarking techniques, ensuring data\nremains secure on local devices, safeguarding sensitive information, and\nmaintaining model authenticity without revealing underlying data. Empirical\nevaluations show that PPFL-RDSN achieves comparable performance to the\nstate-of-the-art centralized methods while reducing computational burdens, and\neffectively mitigates security and privacy vulnerabilities, making it a\npractical solution for secure and privacy-preserving collaborative computer\nvision applications.", "comment": "This paper is under review; do not distribute", "pdf_url": "http://arxiv.org/pdf/2507.00230v1", "categories": ["cs.LG", "cs.CR"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00230v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00474", "title": "ADAptation: Reconstruction-based Unsupervised Active Learning for Breast Ultrasound Diagnosis", "authors": ["Yaofei Duan", "Yuhao Huang", "Xin Yang", "Luyi Han", "Xinyu Xie", "Zhiyuan Zhu", "Ping He", "Ka-Hou Chan", "Ligang Cui", "Sio-Kei Im", "Dong Ni", "Tao Tan"], "summary": "Deep learning-based diagnostic models often suffer performance drops due to\ndistribution shifts between training (source) and test (target) domains.\nCollecting and labeling sufficient target domain data for model retraining\nrepresents an optimal solution, yet is limited by time and scarce resources.\nActive learning (AL) offers an efficient approach to reduce annotation costs\nwhile maintaining performance, but struggles to handle the challenge posed by\ndistribution variations across different datasets. In this study, we propose a\nnovel unsupervised Active learning framework for Domain Adaptation, named\nADAptation, which efficiently selects informative samples from multi-domain\ndata pools under limited annotation budget. As a fundamental step, our method\nfirst utilizes the distribution homogenization capabilities of diffusion models\nto bridge cross-dataset gaps by translating target images into source-domain\nstyle. We then introduce two key innovations: (a) a hypersphere-constrained\ncontrastive learning network for compact feature clustering, and (b) a\ndual-scoring mechanism that quantifies and balances sample uncertainty and\nrepresentativeness. Extensive experiments on four breast ultrasound datasets\n(three public and one in-house/multi-center) across five common deep\nclassifiers demonstrate that our method surpasses existing strong AL-based\ncompetitors, validating its effectiveness and generalization for clinical\ndomain adaptation. The code is available at the anonymized link:\nhttps://github.com/miccai25-966/ADAptation.", "comment": "11 pages, 4 figures, 4 tables. Accepted by conference MICCAI2025", "pdf_url": "http://arxiv.org/pdf/2507.00474v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00474v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00999", "title": "La Leaderboard: A Large Language Model Leaderboard for Spanish Varieties and Languages of Spain and Latin America", "authors": ["María Grandury", "Javier Aula-Blasco", "Júlia Falcão", "Clémentine Fourrier", "Miguel González", "Gonzalo Martínez", "Gonzalo Santamaría", "Rodrigo Agerri", "Nuria Aldama", "Luis Chiruzzo", "Javier Conde", "Helena Gómez", "Marta Guerrero", "Guido Ivetta", "Natalia López", "Flor Miriam Plaza-del-Arco", "María Teresa Martín-Valdivia", "Helena Montoro", "Carmen Muñoz", "Pedro Reviriego", "Leire Rosado", "Alejandro Vaca", "María Estrella Vallecillo-Rodríguez", "Jorge Vallego", "Irune Zubiaga"], "summary": "Leaderboards showcase the current capabilities and limitations of Large\nLanguage Models (LLMs). To motivate the development of LLMs that represent the\nlinguistic and cultural diversity of the Spanish-speaking community, we present\nLa Leaderboard, the first open-source leaderboard to evaluate generative LLMs\nin languages and language varieties of Spain and Latin America. La Leaderboard\nis a community-driven project that aims to establish an evaluation standard for\neveryone interested in developing LLMs for the Spanish-speaking community. This\ninitial version combines 66 datasets in Basque, Catalan, Galician, and\ndifferent Spanish varieties, showcasing the evaluation results of 50 models. To\nencourage community-driven development of leaderboards in other languages, we\nexplain our methodology, including guidance on selecting the most suitable\nevaluation setup for each downstream task. In particular, we provide a\nrationale for using fewer few-shot examples than typically found in the\nliterature, aiming to reduce environmental impact and facilitate access to\nreproducible results for a broader research community.", "comment": "Accepted at ACL 2025 Main", "pdf_url": "http://arxiv.org/pdf/2507.00999v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2507.00999v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00043", "title": "MR-CLIP: Efficient Metadata-Guided Learning of MRI Contrast Representations", "authors": ["Mehmet Yigit Avci", "Pedro Borges", "Paul Wright", "Mehmet Yigitsoy", "Sebastien Ourselin", "Jorge Cardoso"], "summary": "Accurate interpretation of Magnetic Resonance Imaging scans in clinical\nsystems is based on a precise understanding of image contrast. This contrast is\nprimarily governed by acquisition parameters, such as echo time and repetition\ntime, which are stored in the DICOM metadata. To simplify contrast\nidentification, broad labels such as T1-weighted or T2-weighted are commonly\nused, but these offer only a coarse approximation of the underlying acquisition\nsettings. In many real-world datasets, such labels are entirely missing,\nleaving raw acquisition parameters as the only indicators of contrast. Adding\nto this challenge, the available metadata is often incomplete, noisy, or\ninconsistent. The lack of reliable and standardized metadata complicates tasks\nsuch as image interpretation, retrieval, and integration into clinical\nworkflows. Furthermore, robust contrast-aware representations are essential to\nenable more advanced clinical applications, such as achieving\nmodality-invariant representations and data harmonization. To address these\nchallenges, we propose MR-CLIP, a multimodal contrastive learning framework\nthat aligns MR images with their DICOM metadata to learn contrast-aware\nrepresentations, without relying on manual labels. Trained on a diverse\nclinical dataset that spans various scanners and protocols, MR-CLIP captures\ncontrast variations across acquisitions and within scans, enabling\nanatomy-invariant representations. We demonstrate its effectiveness in\ncross-modal retrieval and contrast classification, highlighting its scalability\nand potential for further clinical applications. The code and weights are\npublicly available at https://github.com/myigitavci/MR-CLIP.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00043v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00043v1", "date": "2025-06-23", "updated": "2025-06-23"}
{"id": "2507.00234", "title": "Interpretable AI for Time-Series: Multi-Model Heatmap Fusion with Global Attention and NLP-Generated Explanations", "authors": ["Jiztom Kavalakkatt Francis", "Matthew J Darr"], "summary": "In this paper, we present a novel framework for enhancing model\ninterpretability by integrating heatmaps produced separately by ResNet and a\nrestructured 2D Transformer with globally weighted input saliency. We address\nthe critical problem of spatial-temporal misalignment in existing\ninterpretability methods, where convolutional networks fail to capture global\ncontext and Transformers lack localized precision - a limitation that impedes\nactionable insights in safety-critical domains like healthcare and industrial\nmonitoring. Our method merges gradient-weighted activation maps (ResNet) and\nTransformer attention rollout into a unified visualization, achieving full\nspatial-temporal alignment while preserving real-time performance. Empirical\nevaluations on clinical (ECG arrhythmia detection) and industrial (energy\nconsumption prediction) datasets demonstrate significant improvements: the\nhybrid framework achieves 94.1% accuracy (F1 0.93) on the PhysioNet dataset and\nreduces regression error to RMSE = 0.28 kWh (R2 = 0.95) on the UCI Energy\nAppliance dataset-outperforming standalone ResNet, Transformer, and\nInceptionTime baselines by 3.8-12.4%. An NLP module translates fused heatmaps\ninto domain-specific narratives (e.g., \"Elevated ST-segment between 2-4 seconds\nsuggests myocardial ischemia\"), validated via BLEU-4 (0.586) and ROUGE-L\n(0.650) scores. By formalizing interpretability as causal fidelity and\nspatial-temporal alignment, our approach bridges the gap between technical\noutputs and stakeholder understanding, offering a scalable solution for\ntransparent, time-aware decision-making.", "comment": "13 pages", "pdf_url": "http://arxiv.org/pdf/2507.00234v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00234v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00490", "title": "Just Noticeable Difference for Large Multimodal Models", "authors": ["Zijian Chen", "Yuan Tian", "Yuze Sun", "Wei Sun", "Zicheng Zhang", "Weisi Lin", "Guangtao Zhai", "Wenjun Zhang"], "summary": "Just noticeable difference (JND), the minimum change that the human visual\nsystem (HVS) can perceive, has been studied for decades. Although recent work\nhas extended this line of research into machine vision, there has been a\nscarcity of studies systematically exploring its perceptual boundaries across\nmultiple tasks and stimulus types, particularly in the current era of rapidly\nadvancing large multimodal models (LMMs), where studying the multifaceted\ncapabilities of models has become a mainstream focus. Moreover, the perceptual\ndefects of LMMs are not investigated thoroughly, resulting in potential\nsecurity issues and suboptimal response efficiency. In this paper, we take an\ninitial attempt and demonstrate that there exist significant visual blind spots\nin current LMMs. To systemically quantify this characteristic, we propose a new\nconcept, {\\bf LMM-JND}, together with its determination pipeline. Targeting\nuncovering the behavior commonalities in HVS-aligned visual perception tasks,\nwe delve into several LMM families and construct a large-scale dataset, named\nVPA-JND, which contains 21.5k reference images with over 489k stimuli across 12\ndistortion types, to facilitate LMM-JND studies. VPA-JND exposes areas where\nstate-of-the-art LMMs, including GPT-4o and the InternVL2.5 series, struggle\nwith basic comparison queries and fall significantly short of human-level\nvisual performance. We further explore the effects of vision and language\nbackbones and find a notable correlation between their design philosophy that\nmay instruct the future refinement of LMMs for their visual acuity. Together,\nour research underscores the significance of LMM-JND as a unique perspective\nfor studying LMMs, and predictable LMM-JND is crucial for security concerns.\nThis work will be available at https://github.com/zijianchen98/LMM-JND.", "comment": "19 pages, 19 figures", "pdf_url": "http://arxiv.org/pdf/2507.00490v1", "categories": ["cs.CV", "eess.IV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00490v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.01001", "title": "SciArena: An Open Evaluation Platform for Foundation Models in Scientific Literature Tasks", "authors": ["Yilun Zhao", "Kaiyan Zhang", "Tiansheng Hu", "Sihong Wu", "Ronan Le Bras", "Taira Anderson", "Jonathan Bragg", "Joseph Chee Chang", "Jesse Dodge", "Matt Latzke", "Yixin Liu", "Charles McGrady", "Xiangru Tang", "Zihang Wang", "Chen Zhao", "Hannaneh Hajishirzi", "Doug Downey", "Arman Cohan"], "summary": "We present SciArena, an open and collaborative platform for evaluating\nfoundation models on scientific literature tasks. Unlike traditional benchmarks\nfor scientific literature understanding and synthesis, SciArena engages the\nresearch community directly, following the Chatbot Arena evaluation approach of\ncommunity voting on model comparisons. By leveraging collective intelligence,\nSciArena offers a community-driven evaluation of model performance on\nopen-ended scientific tasks that demand literature-grounded, long-form\nresponses. The platform currently supports 23 open-source and proprietary\nfoundation models and has collected over 13,000 votes from trusted researchers\nacross diverse scientific domains. We analyze the data collected so far and\nconfirm that the submitted questions are diverse, aligned with real-world\nliterature needs, and that participating researchers demonstrate strong\nself-consistency and inter-annotator agreement in their evaluations. We discuss\nthe results and insights based on the model ranking leaderboard. To further\npromote research in building model-based automated evaluation systems for\nliterature tasks, we release SciArena-Eval, a meta-evaluation benchmark based\non our collected preference data. The benchmark measures the accuracy of models\nin judging answer quality by comparing their pairwise assessments with human\nvotes. Our experiments highlight the benchmark's challenges and emphasize the\nneed for more reliable automated evaluation methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.01001v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2507.01001v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00044", "title": "HistoART: Histopathology Artifact Detection and Reporting Tool", "authors": ["Seyed Kahaki", "Alexander R. Webber", "Ghada Zamzmi", "Adarsh Subbaswamy", "Rucha Deshpande", "Aldo Badano"], "summary": "In modern cancer diagnostics, Whole Slide Imaging (WSI) is widely used to\ndigitize tissue specimens for detailed, high-resolution examination; however,\nother diagnostic approaches, such as liquid biopsy and molecular testing, are\nalso utilized based on the cancer type and clinical context. While WSI has\nrevolutionized digital histopathology by enabling automated, precise analysis,\nit remains vulnerable to artifacts introduced during slide preparation and\nscanning. These artifacts can compromise downstream image analysis. To address\nthis challenge, we propose and compare three robust artifact detection\napproaches for WSIs: (1) a foundation model-based approach (FMA) using a\nfine-tuned Unified Neural Image (UNI) architecture, (2) a deep learning\napproach (DLA) built on a ResNet50 backbone, and (3) a knowledge-based approach\n(KBA) leveraging handcrafted features from texture, color, and frequency-based\nmetrics. The methods target six common artifact types: tissue folds,\nout-of-focus regions, air bubbles, tissue damage, marker traces, and blood\ncontamination. Evaluations were conducted on 50,000+ image patches from diverse\nscanners (Hamamatsu, Philips, Leica Aperio AT2) across multiple sites. The FMA\nachieved the highest patch-wise AUROC of 0.995 (95% CI [0.994, 0.995]),\noutperforming the ResNet50-based method (AUROC: 0.977, 95% CI [0.977, 0.978])\nand the KBA (AUROC: 0.940, 95% CI [0.933, 0.946]). To translate detection into\nactionable insights, we developed a quality report scorecard that quantifies\nhigh-quality patches and visualizes artifact distributions.", "comment": "14 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2507.00044v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00044v1", "date": "2025-06-23", "updated": "2025-06-23"}
{"id": "2507.00257", "title": "Gym4ReaL: A Suite for Benchmarking Real-World Reinforcement Learning", "authors": ["Davide Salaorni", "Vincenzo De Paola", "Samuele Delpero", "Giovanni Dispoto", "Paolo Bonetti", "Alessio Russo", "Giuseppe Calcagno", "Francesco Trovò", "Matteo Papini", "Alberto Maria Metelli", "Marco Mussi", "Marcello Restelli"], "summary": "In recent years, \\emph{Reinforcement Learning} (RL) has made remarkable\nprogress, achieving superhuman performance in a wide range of simulated\nenvironments. As research moves toward deploying RL in real-world applications,\nthe field faces a new set of challenges inherent to real-world settings, such\nas large state-action spaces, non-stationarity, and partial observability.\nDespite their importance, these challenges are often underexplored in current\nbenchmarks, which tend to focus on idealized, fully observable, and stationary\nenvironments, often neglecting to incorporate real-world complexities\nexplicitly. In this paper, we introduce \\texttt{Gym4ReaL}, a comprehensive\nsuite of realistic environments designed to support the development and\nevaluation of RL algorithms that can operate in real-world scenarios. The suite\nincludes a diverse set of tasks that expose algorithms to a variety of\npractical challenges. Our experimental results show that, in these settings,\nstandard RL algorithms confirm their competitiveness against rule-based\nbenchmarks, motivating the development of new methods to fully exploit the\npotential of RL to tackle the complexities of real-world tasks.", "comment": "9 pages", "pdf_url": "http://arxiv.org/pdf/2507.00257v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00257v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00493", "title": "Visual Anagrams Reveal Hidden Differences in Holistic Shape Processing Across Vision Models", "authors": ["Fenil R. Doshi", "Thomas Fel", "Talia Konkle", "George Alvarez"], "summary": "Humans are able to recognize objects based on both local texture cues and the\nconfiguration of object parts, yet contemporary vision models primarily harvest\nlocal texture cues, yielding brittle, non-compositional features. Work on\nshape-vs-texture bias has pitted shape and texture representations in\nopposition, measuring shape relative to texture, ignoring the possibility that\nmodels (and humans) can simultaneously rely on both types of cues, and\nobscuring the absolute quality of both types of representation. We therefore\nrecast shape evaluation as a matter of absolute configural competence,\noperationalized by the Configural Shape Score (CSS), which (i) measures the\nability to recognize both images in Object-Anagram pairs that preserve local\ntexture while permuting global part arrangement to depict different object\ncategories. Across 86 convolutional, transformer, and hybrid models, CSS (ii)\nuncovers a broad spectrum of configural sensitivity with fully self-supervised\nand language-aligned transformers -- exemplified by DINOv2, SigLIP2 and\nEVA-CLIP -- occupying the top end of the CSS spectrum. Mechanistic probes\nreveal that (iii) high-CSS networks depend on long-range interactions:\nradius-controlled attention masks abolish performance showing a distinctive\nU-shaped integration profile, and representational-similarity analyses expose a\nmid-depth transition from local to global coding. A BagNet control remains at\nchance (iv), ruling out \"border-hacking\" strategies. Finally, (v) we show that\nconfigural shape score also predicts other shape-dependent evals. Overall, we\npropose that the path toward truly robust, generalizable, and human-like vision\nsystems may not lie in forcing an artificial choice between shape and texture,\nbut rather in architectural and learning frameworks that seamlessly integrate\nboth local-texture and global configural shape.", "comment": "Project page: https://www.fenildoshi.com/configural-shape/", "pdf_url": "http://arxiv.org/pdf/2507.00493v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00493v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00002", "title": "Hypertokens: Holographic Associative Memory in Tokenized LLMs", "authors": ["Christopher James Augeri"], "summary": "Large Language Models (LLMs) exhibit remarkable capabilities but suffer from\napparent precision loss, reframed here as information spreading. This reframing\nshifts the problem from computational precision to an information-theoretic\ncommunication issue. We address the K:V and V:K memory problem in LLMs by\nintroducing HDRAM (Holographically Defined Random Access Memory), a symbolic\nmemory framework treating transformer latent space as a spread-spectrum\nchannel. Built upon hypertokens, structured symbolic codes integrating\nclassical error-correcting codes (ECC), holographic computing, and\nquantum-inspired search, HDRAM recovers distributed information through\nprincipled despreading. These phase-coherent memory addresses enable efficient\nkey-value operations and Grover-style search in latent space. By combining ECC\ngrammar with compressed sensing and Krylov subspace alignment, HDRAM\nsignificantly improves associative retrieval without architectural changes,\ndemonstrating how Classical-Holographic-Quantum-inspired (CHQ) principles can\nfortify transformer architectures.", "comment": "preprint as accepted to https://qnlp.ai/ - Quantum AI and NLP\n  Conference 2025", "pdf_url": "http://arxiv.org/pdf/2507.00002v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00002v1", "date": "2025-06-02", "updated": "2025-06-02"}
{"id": "2507.00045", "title": "CaughtCheating: Is Your MLLM a Good Cheating Detective? Exploring the Boundary of Visual Perception and Reasoning", "authors": ["Ming Li", "Chenguang Wang", "Yijun Liang", "Xiyao Wang", "Yuhang Zhou", "Xiyang Wu", "Yuqing Zhang", "Ruiyi Zhang", "Tianyi Zhou"], "summary": "Recent agentic Multi-Modal Large Language Models (MLLMs) such as GPT-o3 have\nachieved near-ceiling scores on various existing benchmarks, motivating a\ndemand for more challenging test tasks. These MLLMs have been reported to excel\nin a few expert-level tasks for humans, e.g., GeoGuesser, reflecting their\npotential as a detective who can notice minuscule cues in an image and weave\nthem into coherent, situational explanations, leading to a reliable answer. But\ncan they match the performance of excellent human detectives? To answer this\nquestion, we investigate some hard scenarios where GPT-o3 can still handle, and\nfind a common scenario where o3's performance drops to nearly zero, which we\nname CaughtCheating. It is inspired by the social media requests that ask\nothers to detect suspicious clues from photos shared by the poster's partner.\nWe conduct extensive experiments and analysis to understand why existing MLLMs\nlack sufficient capability to solve this kind of task. CaughtCheating provides\na class of challenging visual perception and reasoning tasks with great value\nand practical usage. Success in these tasks paves the way for MLLMs to acquire\nhuman-level detective perception and reasoning capabilities.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00045v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00045v1", "date": "2025-06-23", "updated": "2025-06-23"}
{"id": "2507.00259", "title": "Who Should I Listen To? Adaptive Collaboration in Personalized Federated Learning", "authors": ["Amr Abourayya", "Jens Kleesiek", "Bharat Rao", "Michael Kamp"], "summary": "Data heterogeneity is a central challenge in federated learning, and\npersonalized federated learning (PFL) aims to address it by tailoring models to\neach client's distribution. Yet many PFL methods fail to outperform local or\ncentralized baselines, suggesting a mismatch between the collaboration they\nenforce and the structure of the data. We propose an approach based on adaptive\ncollaboration, where clients decide adaptively not only how much to rely on\nothers, but also whom to trust at the level of individual examples. We\ninstantiate this principle in FEDMOSAIC, a federated co-training method in\nwhich clients exchange predictions over a shared unlabeled dataset. This\nenables fine-grained trust decisions that are difficult to achieve with\nparameter sharing alone. Each client adjusts its loss weighting based on the\nagreement between private and public data, and contributes to global\npseudo-labels in proportion to its estimated per-example confidence.\nEmpirically, FEDMOSAIC improves upon state-of-the-art PFL methods across\ndiverse non-IID settings, and we provide convergence guarantees under standard\nassumptions. Our results demonstrate the potential of data-aware collaboration\nfor robust and effective personalization.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00259v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00259v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00501", "title": "Laplace-Mamba: Laplace Frequency Prior-Guided Mamba-CNN Fusion Network for Image Dehazing", "authors": ["Yongzhen Wang", "Liangliang Chen", "Bingwen Hu", "Heng Liu", "Xiao-Ping Zhang", "Mingqiang Wei"], "summary": "Recent progress in image restoration has underscored Spatial State Models\n(SSMs) as powerful tools for modeling long-range dependencies, owing to their\nappealing linear complexity and computational efficiency. However, SSM-based\napproaches exhibit limitations in reconstructing localized structures and tend\nto be less effective when handling high-dimensional data, frequently resulting\nin suboptimal recovery of fine image features. To tackle these challenges, we\nintroduce Laplace-Mamba, a novel framework that integrates Laplace frequency\nprior with a hybrid Mamba-CNN architecture for efficient image dehazing.\nLeveraging the Laplace decomposition, the image is disentangled into\nlow-frequency components capturing global texture and high-frequency components\nrepresenting edges and fine details. This decomposition enables specialized\nprocessing via dual parallel pathways: the low-frequency branch employs SSMs\nfor global context modeling, while the high-frequency branch utilizes CNNs to\nrefine local structural details, effectively addressing diverse haze scenarios.\nNotably, the Laplace transformation facilitates information-preserving\ndownsampling of low-frequency components in accordance with the Nyquist theory,\nthereby significantly improving computational efficiency. Extensive evaluations\nacross multiple benchmarks demonstrate that our method outperforms\nstate-of-the-art approaches in both restoration quality and efficiency. The\nsource code and pretrained models are available at\nhttps://github.com/yz-wang/Laplace-Mamba.", "comment": "12 pages, 11 figures, 6 tables", "pdf_url": "http://arxiv.org/pdf/2507.00501v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00501v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00018", "title": "Implicit Reward as the Bridge: A Unified View of SFT and DPO Connections", "authors": ["Bo Wang", "Qinyuan Cheng", "Runyu Peng", "Rong Bao", "Peiji Li", "Qipeng Guo", "Linyang Li", "Zhiyuan Zeng", "Yunhua Zhou", "Xipeng Qiu"], "summary": "Post-training processes are essential phases in grounding pre-trained\nlanguage models to real-world tasks, with learning from demonstrations or\npreference signals playing a crucial role in this adaptation. We present a\nunified theoretical framework bridging Supervised Fine-Tuning (SFT) and\npreference learning in Large Language Model (LLM) post-training. Through\nrigorous mathematical derivation, we demonstrate that both SFT and preference\nlearning methods like Direct Preference Optimization (DPO) operate within the\nsame optimal policy-reward subspace, with SFT representing a special case of\nimplicit reward learning. Our analysis reveals a critical limitation in\nconventional SFT: the KL divergence term in distribution matching becomes\nconstant with respect to the policy during optimization, failing to constrain\nmodel updates. To address this, we propose a simple yet effective learning rate\nreduction approach that yields significant performance improvements (up to\n\\textbf{25\\%} relative gain and \\textbf{6\\%} absolute win rate increase in\ninstruction following tasks. Additionally, we derive alternative SFT objectives\nfrom various f-divergence functions that preserve the KL term during\noptimization, further enhancing post-DPO model performance. Finally, we extend\nthe theoretical relationship between LLM logits and Q-functions from preference\nlearning to the SFT context, providing mathematical derivations and\nexperimental validation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00018v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00018v1", "date": "2025-06-15", "updated": "2025-06-15"}
{"id": "2507.00052", "title": "VSF-Med:A Vulnerability Scoring Framework for Medical Vision-Language Models", "authors": ["Binesh Sadanandan", "Vahid Behzadan"], "summary": "Vision Language Models (VLMs) hold great promise for streamlining\nlabour-intensive medical imaging workflows, yet systematic security evaluations\nin clinical settings remain scarce. We introduce VSF--Med, an end-to-end\nvulnerability-scoring framework for medical VLMs that unites three novel\ncomponents: (i) a rich library of sophisticated text-prompt attack templates\ntargeting emerging threat vectors; (ii) imperceptible visual perturbations\ncalibrated by structural similarity (SSIM) thresholds to preserve clinical\nrealism; and (iii) an eight-dimensional rubric evaluated by two independent\njudge LLMs, whose raw scores are consolidated via z-score normalization to\nyield a 0--32 composite risk metric. Built entirely on publicly available\ndatasets and accompanied by open-source code, VSF--Med synthesizes over 30,000\nadversarial variants from 5,000 radiology images and enables reproducible\nbenchmarking of any medical VLM with a single command. Our consolidated\nanalysis reports mean z-score shifts of $0.90\\sigma$ for\npersistence-of-attack-effects, $0.74\\sigma$ for prompt-injection effectiveness,\nand $0.63\\sigma$ for safety-bypass success across state-of-the-art VLMs.\nNotably, Llama-3.2-11B-Vision-Instruct exhibits a peak vulnerability increase\nof $1.29\\sigma$ for persistence-of-attack-effects, while GPT-4o shows increases\nof $0.69\\sigma$ for that same vector and $0.28\\sigma$ for prompt-injection\nattacks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00052v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00052v1", "date": "2025-06-25", "updated": "2025-06-25"}
{"id": "2507.00265", "title": "Examining Reject Relations in Stimulus Equivalence Simulations", "authors": ["Alexis Carrillo", "Asieh Abolpour Mofrad", "Anis Yazidi", "Moises Betancort"], "summary": "Simulations offer a valuable tool for exploring stimulus equivalence (SE),\nyet the potential of reject relations to disrupt the assessment of equivalence\nclass formation is contentious. This study investigates the role of reject\nrelations in the acquisition of stimulus equivalence using computational\nmodels. We examined feedforward neural networks (FFNs), bidirectional encoder\nrepresentations from transformers (BERT), and generative pre-trained\ntransformers (GPT) across 18 conditions in matching-to-sample (MTS)\nsimulations. Conditions varied in training structure (linear series,\none-to-many, and many-to-one), relation type (select-only, reject-only, and\nselect-reject), and negative comparison selection (standard and biased). A\nprobabilistic agent served as a benchmark, embodying purely associative\nlearning. The primary goal was to determine whether artificial neural networks\ncould demonstrate equivalence class formation or whether their performance\nreflected associative learning. Results showed that reject relations influenced\nagent performance. While some agents achieved high accuracy on equivalence\ntests, particularly with reject relations and biased negative comparisons, this\nperformance was comparable to the probabilistic agent. These findings suggest\nthat artificial neural networks, including transformer models, may rely on\nassociative strategies rather than SE. This underscores the need for careful\nconsideration of reject relations and more stringent criteria in computational\nmodels of equivalence.", "comment": "18 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2507.00265v1", "categories": ["cs.LG", "q-bio.NC", "I.2.0; J.4; I.6.5"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00265v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00502", "title": "ExPaMoE: An Expandable Parallel Mixture of Experts for Continual Test-Time Adaptation", "authors": ["JianChao Zhao", "Songlin Dong"], "summary": "Continual Test-Time Adaptation (CTTA) aims to enable models to adapt\non-the-fly to a stream of unlabeled data under evolving distribution shifts.\nHowever, existing CTTA methods typically rely on shared model parameters across\nall domains, making them vulnerable to feature entanglement and catastrophic\nforgetting in the presence of large or non-stationary domain shifts. To address\nthis limitation, we propose \\textbf{ExPaMoE}, a novel framework based on an\n\\emph{Expandable Parallel Mixture-of-Experts} architecture. ExPaMoE decouples\ndomain-general and domain-specific knowledge via a dual-branch expert design\nwith token-guided feature separation, and dynamically expands its expert pool\nbased on a \\emph{Spectral-Aware Online Domain Discriminator} (SODD) that\ndetects distribution changes in real-time using frequency-domain cues.\nExtensive experiments demonstrate the superiority of ExPaMoE across diverse\nCTTA scenarios. We evaluate our method on standard benchmarks including\nCIFAR-10C, CIFAR-100C, ImageNet-C, and Cityscapes-to-ACDC for semantic\nsegmentation. Additionally, we introduce \\textbf{ImageNet++}, a large-scale and\nrealistic CTTA benchmark built from multiple ImageNet-derived datasets, to\nbetter reflect long-term adaptation under complex domain evolution. ExPaMoE\nconsistently outperforms prior arts, showing strong robustness, scalability,\nand resistance to forgetting.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00502v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00502v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00022", "title": "GLU Attention Improve Transformer", "authors": ["Zehao Wang"], "summary": "Gated Linear Units (GLU) have shown great potential in enhancing neural\nnetwork performance. In this paper, I introduce a novel attention mechanism\ncalled GLU Attention, which introduces nonlinearity into the values of\nAttention. My experiments demonstrate that GLU Attention improves both model\nperformance and convergence speed across text and vision modalities with zero\nadditional parameters and negligible computational costs. GLU Attention is\nlightweight and can seamlessly integrate with other technologies, such as Flash\nAttention, Rotary Position Embedding (RoPE), and various Multi-Head Attention\n(MHA) variants such as Grouped-Query Attention (GQA). This project is\nopen-sourced at github.", "comment": "4 pages 4 figures", "pdf_url": "http://arxiv.org/pdf/2507.00022v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.NE"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00022v1", "date": "2025-06-16", "updated": "2025-06-16"}
{"id": "2507.00057", "title": "Estimating Correctness Without Oracles in LLM-Based Code Generation", "authors": ["Thomas Valentin", "Ardi Madadi", "Gaetano Sapia", "Marcel Böhme"], "summary": "Generating code from natural language specifications is one of the most\nsuccessful applications of Large Language Models (LLMs). Yet, they hallucinate:\nLLMs produce outputs that may be grammatically correct but are factually\nincorrect. Without an existing, correct implementation (i.e., an oracle), can\nwe quantify how likely the generated program is correct?\n  In this paper, we propose a measure of incorrectness, called incoherence,\nthat can be estimated efficiently in the absence of an oracle and provides a\nlower bound on the error, i.e., the probability that the LLM-generated program\nfor that specification is incorrect. Our experiments demonstrate an\nextraordinary effectiveness. For the average code generation task, our\nincoherence-based methodology can automatically identify about two-thirds of\nincorrect programs without reports of false positives. In fact, an oracle-based\nevaluation of LLMs can be reliably replaced by an incoherence-based evaluation.\nIn particular, we find a very strong agreement between the ranking of LLMs by\nthe number of programs deemed correct via an oracle (pass@1) and the ranking of\nLLMs by the number of programs deemed correct via our incoherence.", "comment": "8 pages + refs and appendix", "pdf_url": "http://arxiv.org/pdf/2507.00057v1", "categories": ["cs.PL", "cs.AI", "cs.LG", "cs.SE"], "cate": "cs.PL", "url": "http://arxiv.org/abs/2507.00057v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2507.00275", "title": "Double Q-learning for Value-based Deep Reinforcement Learning, Revisited", "authors": ["Prabhat Nagarajan", "Martha White", "Marlos C. Machado"], "summary": "Overestimation is pervasive in reinforcement learning (RL), including in\nQ-learning, which forms the algorithmic basis for many value-based deep RL\nalgorithms. Double Q-learning is an algorithm introduced to address\nQ-learning's overestimation by training two Q-functions and using both to\nde-correlate action-selection and action-evaluation in bootstrap targets.\nShortly after Q-learning was adapted to deep RL in the form of deep Q-networks\n(DQN), Double Q-learning was adapted to deep RL in the form of Double DQN.\nHowever, Double DQN only loosely adapts Double Q-learning, forgoing the\ntraining of two different Q-functions that bootstrap off one another. In this\npaper, we study algorithms that adapt this core idea of Double Q-learning for\nvalue-based deep RL. We term such algorithms Deep Double Q-learning (DDQL). Our\naim is to understand whether DDQL exhibits less overestimation than Double DQN\nand whether performant instantiations of DDQL exist. We answer both questions\naffirmatively, demonstrating that DDQL reduces overestimation and outperforms\nDouble DQN in aggregate across 57 Atari 2600 games, without requiring\nadditional hyperparameters. We also study several aspects of DDQL, including\nits network architecture, replay ratio, and minibatch sampling strategy.", "comment": "44 pages", "pdf_url": "http://arxiv.org/pdf/2507.00275v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00275v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00505", "title": "LLaVA-SP: Enhancing Visual Representation with Visual Spatial Tokens for MLLMs", "authors": ["Haoran Lou", "Chunxiao Fan", "Ziyan Liu", "Yuexin Wu", "Xinxiang Wang"], "summary": "The architecture of multimodal large language models (MLLMs) commonly\nconnects a vision encoder, often based on CLIP-ViT, to a large language model.\nWhile CLIP-ViT works well for capturing global image features, it struggles to\nmodel local relationships between adjacent patches, leading to weaker visual\nrepresentation, which in turn affects the detailed understanding ability of\nMLLMs. To solve this, we propose LLaVA-SP, which \\textbf{ only adds six spatial\nvisual tokens} to the original visual tokens to enhance the visual\nrepresentation. Our approach offers three key advantages: 1)We propose a novel\nProjector, which uses convolutional kernels to derive visual spatial tokens\nfrom ViT patch features, simulating two visual spatial ordering approaches:\n``from central region to global\" and ``from abstract to specific\". Then, a\ncross-attention mechanism is applied to fuse fine-grained visual information,\nenriching the overall visual representation. 2) We present two model variants:\nLLaVA-SP-Cropping, which focuses on detail features through progressive\ncropping, and LLaVA-SP-Pooling, which captures global semantics through\nadaptive pooling, enabling the model to handle diverse visual understanding\ntasks. 3) Extensive experiments show that LLaVA-SP, fine-tuned with LoRA,\nachieves significant performance improvements across various multimodal\nbenchmarks, outperforming the state-of-the-art LLaVA-1.5 model in multiple\ntasks with nearly identical inference latency. The code and models are\navailable at\n\\href{https://github.com/CnFaker/LLaVA-SP}{\\texttt{https://github.com/CnFaker/LLaVA-SP}}.", "comment": "ICCV", "pdf_url": "http://arxiv.org/pdf/2507.00505v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00505v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00026", "title": "ROSE: Toward Reality-Oriented Safety Evaluation of Large Language Models", "authors": ["Jiale Ding", "Xiang Zheng", "Cong Wang", "Wei-Bin Lee", "Xingjun Ma", "Yu-Gang Jiang"], "summary": "As Large Language Models (LLMs) are increasingly deployed as black-box\ncomponents in real-world applications, evaluating their safety-especially under\nadversarial prompting-has become critical. Arguably, effective safety\nevaluations should be adaptive, evolving with LLM capabilities, and also cover\na broad spectrum of harmful topics and real-world scenarios to fully expose\npotential vulnerabilities. Existing manual safety benchmarks, built on\nhandcrafted adversarial prompts, are limited by their static nature and the\nintensive labor required to update them, making it difficult to keep pace with\nrapidly advancing LLMs. In contrast, automated adversarial prompt generation\noffers a promising path toward adaptive evaluation. However, current methods\noften suffer from insufficient adversarial topic coverage (topic-level\ndiversity) and weak alignment with real-world contexts. These shortcomings stem\nfrom the exploration-exploitation dilemma in black-box optimization and a lack\nof real-world contextualization, resulting in adversarial prompts that are both\ntopically narrow and scenario-repetitive. To address these issues, we propose\nReality-Oriented Safety Evaluation (ROSE), a novel framework that uses\nmulti-objective reinforcement learning to fine-tune an adversarial LLM for\ngenerating topically diverse and contextually rich adversarial prompts.\nExperiments show that ROSE outperforms existing methods in uncovering safety\nvulnerabilities in state-of-the-art LLMs, with notable improvements in\nintegrated evaluation metrics. We hope ROSE represents a step toward more\npractical and reality-oriented safety evaluation of LLMs. WARNING: This paper\ncontains examples of potentially harmful text.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00026v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CY"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00026v1", "date": "2025-06-17", "updated": "2025-06-17"}
{"id": "2507.00061", "title": "Smooth-Distill: A Self-distillation Framework for Multitask Learning with Wearable Sensor Data", "authors": ["Hoang-Dieu Vu", "Duc-Nghia Tran", "Quang-Tu Pham", "Hieu H. Pham", "Nicolas Vuillerme", "Duc-Tan Tran"], "summary": "This paper introduces Smooth-Distill, a novel self-distillation framework\ndesigned to simultaneously perform human activity recognition (HAR) and sensor\nplacement detection using wearable sensor data. The proposed approach utilizes\na unified CNN-based architecture, MTL-net, which processes accelerometer data\nand branches into two outputs for each respective task. Unlike conventional\ndistillation methods that require separate teacher and student models, the\nproposed framework utilizes a smoothed, historical version of the model itself\nas the teacher, significantly reducing training computational overhead while\nmaintaining performance benefits. To support this research, we developed a\ncomprehensive accelerometer-based dataset capturing 12 distinct sleep postures\nacross three different wearing positions, complementing two existing public\ndatasets (MHealth and WISDM). Experimental results show that Smooth-Distill\nconsistently outperforms alternative approaches across different evaluation\nscenarios, achieving notable improvements in both human activity recognition\nand device placement detection tasks. This method demonstrates enhanced\nstability in convergence patterns during training and exhibits reduced\noverfitting compared to traditional multitask learning baselines. This\nframework contributes to the practical implementation of knowledge distillation\nin human activity recognition systems, offering an effective solution for\nmultitask learning with accelerometer data that balances accuracy and training\nefficiency. More broadly, it reduces the computational cost of model training,\nwhich is critical for scenarios requiring frequent model updates or training on\nresource-constrained platforms. The code and model are available at\nhttps://github.com/Kuan2vn/smooth\\_distill.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00061v1", "categories": ["cs.LG", "cs.AI", "eess.SP"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00061v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2507.00301", "title": "Structure-preserving Lift & Learn: Scientific machine learning for nonlinear conservative partial differential equations", "authors": ["Harsh Sharma", "Juan Diego Draxl Giannoni", "Boris Kramer"], "summary": "This work presents structure-preserving Lift & Learn, a scientific machine\nlearning method that employs lifting variable transformations to learn\nstructure-preserving reduced-order models for nonlinear partial differential\nequations (PDEs) with conservation laws. We propose a hybrid learning approach\nbased on a recently developed energy-quadratization strategy that uses\nknowledge of the nonlinearity at the PDE level to derive an equivalent\nquadratic lifted system with quadratic system energy. The lifted dynamics\nobtained via energy quadratization are linear in the old variables, making\nmodel learning very effective in the lifted setting. Based on the lifted\nquadratic PDE model form, the proposed method derives quadratic reduced terms\nanalytically and then uses those derived terms to formulate a constrained\noptimization problem to learn the remaining linear reduced operators in a\nstructure-preserving way. The proposed hybrid learning approach yields\ncomputationally efficient quadratic reduced-order models that respect the\nunderlying physics of the high-dimensional problem. We demonstrate the\ngeneralizability of quadratic models learned via the proposed\nstructure-preserving Lift & Learn method through three numerical examples: the\none-dimensional wave equation with exponential nonlinearity, the\ntwo-dimensional sine-Gordon equation, and the two-dimensional\nKlein-Gordon-Zakharov equations. The numerical results show that the proposed\nlearning approach is competitive with the state-of-the-art structure-preserving\ndata-driven model reduction method in terms of both accuracy and computational\nefficiency.", "comment": "arXiv admin note: substantial text overlap with arXiv:2503.02273", "pdf_url": "http://arxiv.org/pdf/2507.00301v1", "categories": ["cs.LG", "cs.NA", "math.NA"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00301v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00506", "title": "SCING:Towards More Efficient and Robust Person Re-Identification through Selective Cross-modal Prompt Tuning", "authors": ["Yunfei Xie", "Yuxuan Cheng", "Juncheng Wu", "Haoyu Zhang", "Yuyin Zhou", "Shoudong Han"], "summary": "Recent advancements in adapting vision-language pre-training models like CLIP\nfor person re-identification (ReID) tasks often rely on complex adapter design\nor modality-specific tuning while neglecting cross-modal interaction, leading\nto high computational costs or suboptimal alignment. To address these\nlimitations, we propose a simple yet effective framework named Selective\nCross-modal Prompt Tuning (SCING) that enhances cross-modal alignment and\nrobustness against real-world perturbations. Our method introduces two key\ninnovations: Firstly, we proposed Selective Visual Prompt Fusion (SVIP), a\nlightweight module that dynamically injects discriminative visual features into\ntext prompts via a cross-modal gating mechanism. Moreover, the proposed\nPerturbation-Driven Consistency Alignment (PDCA) is a dual-path training\nstrategy that enforces invariant feature alignment under random image\nperturbations by regularizing consistency between original and augmented\ncross-modal embeddings. Extensive experiments are conducted on several popular\nbenchmarks covering Market1501, DukeMTMC-ReID, Occluded-Duke, Occluded-REID,\nand P-DukeMTMC, which demonstrate the impressive performance of the proposed\nmethod. Notably, our framework eliminates heavy adapters while maintaining\nefficient inference, achieving an optimal trade-off between performance and\ncomputational overhead. The code will be released upon acceptance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00506v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00506v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00033", "title": "Moment Sampling in Video LLMs for Long-Form Video QA", "authors": ["Mustafa Chasmai", "Gauri Jagatap", "Gouthaman KV", "Grant Van Horn", "Subhransu Maji", "Andrea Fanelli"], "summary": "Recent advancements in video large language models (Video LLMs) have\nsignificantly advanced the field of video question answering (VideoQA). While\nexisting methods perform well on short videos, they often struggle with\nlong-range reasoning in longer videos. To scale Video LLMs for longer video\ncontent, frame sub-sampling (selecting frames at regular intervals) is commonly\nused. However, this approach is suboptimal, often leading to the loss of\ncrucial frames or the inclusion of redundant information from multiple similar\nframes. Missing key frames impairs the model's ability to answer questions\naccurately, while redundant frames lead the model to focus on irrelevant video\nsegments and increase computational resource consumption. In this paper, we\ninvestigate the use of a general-purpose text-to-video moment retrieval model\nto guide the frame sampling process. We propose \"moment sampling\", a novel,\nmodel-agnostic approach that enables the model to select the most relevant\nframes according to the context of the question. Specifically, we employ a\nlightweight moment retrieval model to prioritize frame selection. By focusing\non the frames most pertinent to the given question, our method enhances\nlong-form VideoQA performance in Video LLMs. Through extensive experiments on\nfour long-form VideoQA datasets, using four state-of-the-art Video LLMs, we\ndemonstrate the effectiveness of the proposed approach.", "comment": "Workshop on Video Large Language Models (VidLLMs) at CVPR 2025", "pdf_url": "http://arxiv.org/pdf/2507.00033v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00033v1", "date": "2025-06-18", "updated": "2025-06-18"}
{"id": "2507.00066", "title": "InSight-R: A Framework for Risk-informed Human Failure Event Identification and Interface-Induced Risk Assessment Driven by AutoGraph", "authors": ["Xingyu Xiao", "Jiejuan Tong", "Peng Chen", "Jun Sun", "Zhe Sui", "Jingang Liang", "Hongru Zhao", "Jun Zhao", "Haitao Wang"], "summary": "Human reliability remains a critical concern in safety-critical domains such\nas nuclear power, where operational failures are often linked to human error.\nWhile conventional human reliability analysis (HRA) methods have been widely\nadopted, they rely heavily on expert judgment for identifying human failure\nevents (HFEs) and assigning performance influencing factors (PIFs). This\nreliance introduces challenges related to reproducibility, subjectivity, and\nlimited integration of interface-level data. In particular, current approaches\nlack the capacity to rigorously assess how human-machine interface design\ncontributes to operator performance variability and error susceptibility. To\naddress these limitations, this study proposes a framework for risk-informed\nhuman failure event identification and interface-induced risk assessment driven\nby AutoGraph (InSight-R). By linking empirical behavioral data to the\ninterface-embedded knowledge graph (IE-KG) constructed by the automated\ngraph-based execution framework (AutoGraph), the InSight-R framework enables\nautomated HFE identification based on both error-prone and time-deviated\noperational paths. Furthermore, we discuss the relationship between\ndesigner-user conflicts and human error. The results demonstrate that InSight-R\nnot only enhances the objectivity and interpretability of HFE identification\nbut also provides a scalable pathway toward dynamic, real-time human\nreliability assessment in digitalized control environments. This framework\noffers actionable insights for interface design optimization and contributes to\nthe advancement of mechanism-driven HRA methodologies.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00066v1", "categories": ["cs.HC", "cs.AI"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2507.00066v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2507.00304", "title": "MamNet: A Novel Hybrid Model for Time-Series Forecasting and Frequency Pattern Analysis in Network Traffic", "authors": ["Yujun Zhang", "Runlong Li", "Xiaoxiang Liang", "Xinhao Yang", "Tian Su", "Bo Liu", "Yan Zhou"], "summary": "The abnormal fluctuations in network traffic may indicate potential security\nthreats or system failures. Therefore, efficient network traffic prediction and\nanomaly detection methods are crucial for network security and traffic\nmanagement. This paper proposes a novel network traffic prediction and anomaly\ndetection model, MamNet, which integrates time-domain modeling and\nfrequency-domain feature extraction. The model first captures the long-term\ndependencies of network traffic through the Mamba module (time-domain\nmodeling), and then identifies periodic fluctuations in the traffic using\nFourier Transform (frequency-domain feature extraction). In the feature fusion\nlayer, multi-scale information is integrated to enhance the model's ability to\ndetect network traffic anomalies. Experiments conducted on the UNSW-NB15 and\nCAIDA datasets demonstrate that MamNet outperforms several recent mainstream\nmodels in terms of accuracy, recall, and F1-Score. Specifically, it achieves an\nimprovement of approximately 2% to 4% in detection performance for complex\ntraffic patterns and long-term trend detection. The results indicate that\nMamNet effectively captures anomalies in network traffic across different time\nscales and is suitable for anomaly detection tasks in network security and\ntraffic management. Future work could further optimize the model structure by\nincorporating external network event information, thereby improving the model's\nadaptability and stability in complex network environments.", "comment": "16 pages", "pdf_url": "http://arxiv.org/pdf/2507.00304v1", "categories": ["cs.LG", "cs.NI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00304v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00519", "title": "Topology-Constrained Learning for Efficient Laparoscopic Liver Landmark Detection", "authors": ["Ruize Cui", "Jiaan Zhang", "Jialun Pei", "Kai Wang", "Pheng-Ann Heng", "Jing Qin"], "summary": "Liver landmarks provide crucial anatomical guidance to the surgeon during\nlaparoscopic liver surgery to minimize surgical risk. However, the tubular\nstructural properties of landmarks and dynamic intraoperative deformations pose\nsignificant challenges for automatic landmark detection. In this study, we\nintroduce TopoNet, a novel topology-constrained learning framework for\nlaparoscopic liver landmark detection. Our framework adopts a snake-CNN\ndual-path encoder to simultaneously capture detailed RGB texture information\nand depth-informed topological structures. Meanwhile, we propose a\nboundary-aware topology fusion (BTF) module, which adaptively merges RGB-D\nfeatures to enhance edge perception while preserving global topology.\nAdditionally, a topological constraint loss function is embedded, which\ncontains a center-line constraint loss and a topological persistence loss to\nensure homotopy equivalence between predictions and labels. Extensive\nexperiments on L3D and P2ILF datasets demonstrate that TopoNet achieves\noutstanding accuracy and computational complexity, highlighting the potential\nfor clinical applications in laparoscopic liver surgery. Our code will be\navailable at https://github.com/cuiruize/TopoNet.", "comment": "This paper has been accepted by MICCAI 2025", "pdf_url": "http://arxiv.org/pdf/2507.00519v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00519v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00045", "title": "CaughtCheating: Is Your MLLM a Good Cheating Detective? Exploring the Boundary of Visual Perception and Reasoning", "authors": ["Ming Li", "Chenguang Wang", "Yijun Liang", "Xiyao Wang", "Yuhang Zhou", "Xiyang Wu", "Yuqing Zhang", "Ruiyi Zhang", "Tianyi Zhou"], "summary": "Recent agentic Multi-Modal Large Language Models (MLLMs) such as GPT-o3 have\nachieved near-ceiling scores on various existing benchmarks, motivating a\ndemand for more challenging test tasks. These MLLMs have been reported to excel\nin a few expert-level tasks for humans, e.g., GeoGuesser, reflecting their\npotential as a detective who can notice minuscule cues in an image and weave\nthem into coherent, situational explanations, leading to a reliable answer. But\ncan they match the performance of excellent human detectives? To answer this\nquestion, we investigate some hard scenarios where GPT-o3 can still handle, and\nfind a common scenario where o3's performance drops to nearly zero, which we\nname CaughtCheating. It is inspired by the social media requests that ask\nothers to detect suspicious clues from photos shared by the poster's partner.\nWe conduct extensive experiments and analysis to understand why existing MLLMs\nlack sufficient capability to solve this kind of task. CaughtCheating provides\na class of challenging visual perception and reasoning tasks with great value\nand practical usage. Success in these tasks paves the way for MLLMs to acquire\nhuman-level detective perception and reasoning capabilities.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00045v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00045v1", "date": "2025-06-23", "updated": "2025-06-23"}
{"id": "2507.00068", "title": "MANTA: Cross-Modal Semantic Alignment and Information-Theoretic Optimization for Long-form Multimodal Understanding", "authors": ["Ziqi Zhong", "Daniel Tang"], "summary": "While multi-modal learning has advanced significantly, current approaches\noften treat modalities separately, creating inconsistencies in representation\nand reasoning. We introduce MANTA (Multi-modal Abstraction and Normalization\nvia Textual Alignment), a theoretically-grounded framework that unifies visual\nand auditory inputs into a structured textual space for seamless processing\nwith large language models. MANTA addresses four key challenges: (1) semantic\nalignment across modalities with information-theoretic optimization, (2)\nadaptive temporal synchronization for varying information densities, (3)\nhierarchical content representation for multi-scale understanding, and (4)\ncontext-aware retrieval of sparse information from long sequences. We formalize\nour approach within a rigorous mathematical framework, proving its optimality\nfor context selection under token constraints. Extensive experiments on the\nchallenging task of Long Video Question Answering show that MANTA improves\nstate-of-the-art models by up to 22.6% in overall accuracy, with particularly\nsignificant gains (27.3%) on videos exceeding 30 minutes. Additionally, we\ndemonstrate MANTA's superiority on temporal reasoning tasks (23.8% improvement)\nand cross-modal understanding (25.1% improvement). Our framework introduces\nnovel density estimation techniques for redundancy minimization while\npreserving rare signals, establishing new foundations for unifying multimodal\nrepresentations through structured text.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00068v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00068v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2507.00310", "title": "Open-ended Scientific Discovery via Bayesian Surprise", "authors": ["Dhruv Agarwal", "Bodhisattwa Prasad Majumder", "Reece Adamson", "Megha Chakravorty", "Satvika Reddy Gavireddy", "Aditya Parashar", "Harshit Surana", "Bhavana Dalvi Mishra", "Andrew McCallum", "Ashish Sabharwal", "Peter Clark"], "summary": "The promise of autonomous scientific discovery (ASD) hinges not only on\nanswering questions, but also on knowing which questions to ask. Most recent\nworks in ASD explore the use of large language models (LLMs) in goal-driven\nsettings, relying on human-specified research questions to guide hypothesis\ngeneration. However, scientific discovery may be accelerated further by\nallowing the AI system to drive exploration by its own criteria. The few\nexisting approaches in open-ended ASD select hypotheses based on diversity\nheuristics or subjective proxies for human interestingness, but the former\nstruggles to meaningfully navigate the typically vast hypothesis space, and the\nlatter suffers from imprecise definitions. This paper presents AutoDS -- a\nmethod for open-ended ASD that instead drives scientific exploration using\nBayesian surprise. Here, we quantify the epistemic shift from the LLM's prior\nbeliefs about a hypothesis to its posterior beliefs after gathering\nexperimental results. To efficiently explore the space of nested hypotheses,\nour method employs a Monte Carlo tree search (MCTS) strategy with progressive\nwidening using surprisal as the reward function. We evaluate AutoDS in the\nsetting of data-driven discovery across 21 real-world datasets spanning domains\nsuch as biology, economics, finance, and behavioral science. Our results\ndemonstrate that under a fixed budget, AutoDS substantially outperforms\ncompetitors by producing 5--29\\% more discoveries deemed surprising by the LLM.\nOur human evaluation further finds that two-thirds of AutoDS discoveries are\nsurprising to the domain experts, suggesting this is an important step forward\ntowards building open-ended ASD systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00310v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00310v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00525", "title": "Box-QAymo: Box-Referring VQA Dataset for Autonomous Driving", "authors": ["Djamahl Etchegaray", "Yuxia Fu", "Zi Huang", "Yadan Luo"], "summary": "Interpretable communication is essential for safe and trustworthy autonomous\ndriving, yet current vision-language models (VLMs) often operate under\nidealized assumptions and struggle to capture user intent in real-world\nscenarios. Existing driving-oriented VQA datasets are limited to full-scene\ndescriptions or waypoint prediction, preventing the assessment of whether VLMs\ncan respond to localized user-driven queries. We introduce Box-QAymo, a\nbox-referring dataset and benchmark designed to both evaluate and finetune VLMs\non spatial and temporal reasoning over user-specified objects. Users express\nintent by drawing bounding boxes, offering a fast and intuitive interface for\nfocused queries in complex scenes. Specifically, we propose a hierarchical\nevaluation protocol that begins with binary sanity-check questions to assess\nbasic model capacities, and progresses to (1) attribute prediction for\nbox-referred objects, (2) motion understanding of target instances, and (3)\nspatiotemporal motion reasoning over inter-object dynamics across frames. To\nsupport this, we crowd-sourced fine-grained object classes and visual\nattributes that reflect the complexity drivers encounter, and extract object\ntrajectories to construct temporally grounded QA pairs. Rigorous quality\ncontrol through negative sampling, temporal consistency checks, and\ndifficulty-aware balancing guarantee dataset robustness and diversity. Our\ncomprehensive evaluation reveals significant limitations in current VLMs when\nqueried about perception questions, highlighting the gap in achieving\nreal-world performance. This work provides a foundation for developing more\nrobust and interpretable autonomous driving systems that can communicate\neffectively with users under real-world conditions. Project page and dataset\nare available at https://djamahl99.github.io/qaymo-pages/.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00525v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00525v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00054", "title": "Enhancing Reasoning Capabilities in SLMs with Reward Guided Dataset Distillation", "authors": ["Shreyansh Padarha"], "summary": "The push to compress and impart the proficiency of Large Language Models\n(LLMs) into more deployable and efficient Small Language Models (SLMs) has\nbenefited from improvements in knowledge distillation (KD) techniques. These\ntechniques allow a smaller student model to learn from a more capable and\nlarger teacher model's responses. However, distillation often revolves around\nthe student model merely copying the teacher's in-distribution responses,\nlimiting its generalisability. This limitation is amplified on reasoning tasks\nand can be computationally expensive. In this study, we propose AdvDistill, a\nreward-guided dataset distillation framework. We utilise multiple generations\n(responses) from a teacher for each prompt and assign rewards based on\nrule-based verifiers. These varying and normally distributed rewards serve as\nweights when training student models. Our methods and their subsequent\nbehavioural analysis demonstrate a significant improvement in student model\nperformance for mathematical and complex reasoning tasks, showcasing the\nefficacy and benefits of incorporating a rewarding mechanism in dataset\ndistillation processes.", "comment": "17 Pages, 7 figures", "pdf_url": "http://arxiv.org/pdf/2507.00054v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2507.00054v1", "date": "2025-06-25", "updated": "2025-06-25"}
{"id": "2507.00070", "title": "An efficient plant disease detection using transfer learning approach", "authors": ["Bosubabu Sambana", "Hillary Sunday Nnadi", "Mohd Anas Wajid", "Nwosu Ogochukwu Fidelia", "Claudia Camacho-Zuñiga", "Henry Dozie Ajuzie", "Edeh Michael Onyema"], "summary": "Plant diseases pose significant challenges to farmers and the agricultural\nsector at large. However, early detection of plant diseases is crucial to\nmitigating their effects and preventing widespread damage, as outbreaks can\nseverely impact the productivity and quality of crops. With advancements in\ntechnology, there are increasing opportunities for automating the monitoring\nand detection of disease outbreaks in plants. This study proposed a system\ndesigned to identify and monitor plant diseases using a transfer learning\napproach. Specifically, the study utilizes YOLOv7 and YOLOv8, two\nstate-ofthe-art models in the field of object detection. By fine-tuning these\nmodels on a dataset of plant leaf images, the system is able to accurately\ndetect the presence of Bacteria, Fungi and Viral diseases such as Powdery\nMildew, Angular Leaf Spot, Early blight and Tomato mosaic virus. The model's\nperformance was evaluated using several metrics, including mean Average\nPrecision (mAP), F1-score, Precision, and Recall, yielding values of 91.05,\n89.40, 91.22, and 87.66, respectively. The result demonstrates the superior\neffectiveness and efficiency of YOLOv8 compared to other object detection\nmethods, highlighting its potential for use in modern agricultural practices.\nThe approach provides a scalable, automated solution for early any plant\ndisease detection, contributing to enhanced crop yield, reduced reliance on\nmanual monitoring, and supporting sustainable agricultural practices.", "comment": "15 pages , 4 figures. Scientific Reports 2025", "pdf_url": "http://arxiv.org/pdf/2507.00070v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00070v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2507.00316", "title": "$μ^2$Tokenizer: Differentiable Multi-Scale Multi-Modal Tokenizer for Radiology Report Generation", "authors": ["Siyou Li", "Pengyao Qin", "Huanan Wu", "Dong Nie", "Arun J. Thirunavukarasu", "Juntao Yu", "Le Zhang"], "summary": "Automated radiology report generation (RRG) aims to produce detailed textual\nreports from clinical imaging, such as computed tomography (CT) scans, to\nimprove the accuracy and efficiency of diagnosis and provision of management\nadvice. RRG is complicated by two key challenges: (1) inherent complexity in\nextracting relevant information from imaging data under resource constraints,\nand (2) difficulty in objectively evaluating discrepancies between\nmodel-generated and expert-written reports. To address these challenges, we\npropose $\\mu^2$LLM, a $\\underline{\\textbf{mu}}$ltiscale\n$\\underline{\\textbf{mu}}$ltimodal large language models for RRG tasks. The\nnovel ${\\mu}^2$Tokenizer, as an intermediate layer, integrates multi-modal\nfeatures from the multiscale visual tokenizer and the text tokenizer, then\nenhances report generation quality through direct preference optimization\n(DPO), guided by GREEN-RedLlama. Experimental results on four large CT\nimage-report medical datasetdemonstrate that our method outperforms existing\napproaches, highlighting the potential of our fine-tuned $\\mu^2$LLMs on limited\ndata for RRG tasks.", "comment": "Accepted by MICCAI 2025", "pdf_url": "http://arxiv.org/pdf/2507.00316v1", "categories": ["cs.LG", "cs.CL", "eess.IV"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00316v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00537", "title": "Not All Attention Heads Are What You Need: Refining CLIP's Image Representation with Attention Ablation", "authors": ["Feng Lin", "Marco Chen", "Haokui Zhang", "Xiaotian Yu", "Guangming Lu", "Rong Xiao"], "summary": "This paper studies the role of attention heads in CLIP's image encoder. While\nCLIP has exhibited robust performance across diverse applications, we\nhypothesize that certain attention heads negatively affect final\nrepresentations and that ablating them can improve performance in downstream\ntasks. To capitalize on this insight, we propose a simple yet effective method,\ncalled Attention Ablation Technique (AAT), to suppress the contribution of\nspecific heads by manipulating attention weights. By integrating two\nalternative strategies tailored for different application scenarios, AAT\nsystematically identifies and ablates detrimental attention heads to enhance\nrepresentation quality. Experiments demonstrate that AAT consistently improves\ndownstream task performance across various domains, boosting recall rate by up\nto 11.1% on CLIP-family models for cross-modal retrieval. The results highlight\nthe potential of AAT to effectively refine large-scale vision-language models\nwith virtually no increase in inference cost.", "comment": "21 pages, 7 figures", "pdf_url": "http://arxiv.org/pdf/2507.00537v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00537v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00068", "title": "MANTA: Cross-Modal Semantic Alignment and Information-Theoretic Optimization for Long-form Multimodal Understanding", "authors": ["Ziqi Zhong", "Daniel Tang"], "summary": "While multi-modal learning has advanced significantly, current approaches\noften treat modalities separately, creating inconsistencies in representation\nand reasoning. We introduce MANTA (Multi-modal Abstraction and Normalization\nvia Textual Alignment), a theoretically-grounded framework that unifies visual\nand auditory inputs into a structured textual space for seamless processing\nwith large language models. MANTA addresses four key challenges: (1) semantic\nalignment across modalities with information-theoretic optimization, (2)\nadaptive temporal synchronization for varying information densities, (3)\nhierarchical content representation for multi-scale understanding, and (4)\ncontext-aware retrieval of sparse information from long sequences. We formalize\nour approach within a rigorous mathematical framework, proving its optimality\nfor context selection under token constraints. Extensive experiments on the\nchallenging task of Long Video Question Answering show that MANTA improves\nstate-of-the-art models by up to 22.6% in overall accuracy, with particularly\nsignificant gains (27.3%) on videos exceeding 30 minutes. Additionally, we\ndemonstrate MANTA's superiority on temporal reasoning tasks (23.8% improvement)\nand cross-modal understanding (25.1% improvement). Our framework introduces\nnovel density estimation techniques for redundancy minimization while\npreserving rare signals, establishing new foundations for unifying multimodal\nrepresentations through structured text.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00068v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00068v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2507.00075", "title": "Theoretical Modeling of LLM Self-Improvement Training Dynamics Through Solver-Verifier Gap", "authors": ["Yifan Sun", "Yushan Liang", "Zhen Zhang", "Jiaye Teng"], "summary": "Self-improvement is among the most prominent techniques within the realm of\nlarge language models (LLM), aiming to enhance the LLM performance without\nrelying on external data. Despite its significance, generally how LLM\nperformances evolve during the self-improvement process remains underexplored.\nIn this paper, we theoretically model the training dynamics of self-improvement\nvia the concept of solver-verifier gap. This is inspired by the conjecture that\nthe performance enhancement of self-improvement stems from the gap between\nLLM's solver capability and verifier capability. Based on the theoretical\nframework, we further introduce how to predict the ultimate power of\nself-improvement using only information from the first few training epochs. We\nempirically validate the effectiveness of the theoretical model on various LLMs\nand datasets. Beyond self-improvement, we extend our analysis to investigate\nhow external data influences these dynamics within the framework. Notably, we\nfind that under limited external data regimes, such external data can be\nutilized at any stage without significantly affecting final performances, which\naccords with the empirical observations.", "comment": "24 pages", "pdf_url": "http://arxiv.org/pdf/2507.00075v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00075v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2507.00320", "title": "Exploring Theory-Laden Observations in the Brain Basis of Emotional Experience", "authors": ["Christiana Westlin", "Ashutosh Singh", "Deniz Erdogmus", "Georgios Stratis", "Lisa Feldman Barrett"], "summary": "In the science of emotion, it is widely assumed that folk emotion categories\nform a biological and psychological typology, and studies are routinely\ndesigned and analyzed to identify emotion-specific patterns. This approach\nshapes the observations that studies report, ultimately reinforcing the\nassumption that guided the investigation. Here, we reanalyzed data from one\nsuch typologically-guided study that reported mappings between individual brain\npatterns and group-averaged ratings of 34 emotion categories. Our reanalysis\nwas guided by an alternative view of emotion categories as populations of\nvariable, situated instances, and which predicts a priori that there will be\nsignificant variation in brain patterns within a category across instances.\nCorrespondingly, our analysis made minimal assumptions about the structure of\nthe variance present in the data. As predicted, we did not observe the original\nmappings and instead observed significant variation across individuals. These\nfindings demonstrate how starting assumptions can ultimately impact scientific\nconclusions and suggest that a hypothesis must be supported using multiple\nanalytic methods before it is taken seriously.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00320v1", "categories": ["cs.LG", "cs.CV", "q-bio.NC"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00320v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00554", "title": "LOD-GS: Level-of-Detail-Sensitive 3D Gaussian Splatting for Detail Conserved Anti-Aliasing", "authors": ["Zhenya Yang", "Bingchen Gong", "Kai Chen", "Qi Dou"], "summary": "Despite the advancements in quality and efficiency achieved by 3D Gaussian\nSplatting (3DGS) in 3D scene rendering, aliasing artifacts remain a persistent\nchallenge. Existing approaches primarily rely on low-pass filtering to mitigate\naliasing. However, these methods are not sensitive to the sampling rate, often\nresulting in under-filtering and over-smoothing renderings. To address this\nlimitation, we propose LOD-GS, a Level-of-Detail-sensitive filtering framework\nfor Gaussian Splatting, which dynamically predicts the optimal filtering\nstrength for each 3D Gaussian primitive. Specifically, we introduce a set of\nbasis functions to each Gaussian, which take the sampling rate as input to\nmodel appearance variations, enabling sampling-rate-sensitive filtering. These\nbasis function parameters are jointly optimized with the 3D Gaussian in an\nend-to-end manner. The sampling rate is influenced by both focal length and\ncamera distance. However, existing methods and datasets rely solely on\ndown-sampling to simulate focal length changes for anti-aliasing evaluation,\noverlooking the impact of camera distance. To enable a more comprehensive\nassessment, we introduce a new synthetic dataset featuring objects rendered at\nvarying camera distances. Extensive experiments on both public datasets and our\nnewly collected dataset demonstrate that our method achieves SOTA rendering\nquality while effectively eliminating aliasing. The code and dataset have been\nopen-sourced.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00554v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00554v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00078", "title": "The language of time: a language model perspective on time-series foundation models", "authors": ["Yi Xie", "Yun Xiong", "Zejian Shi", "Hao Niu", "Zhengfu Liu"], "summary": "With the rise of large language models, the paradigm of training foundation\nmodels with massive parameter counts on vast datasets has been adopted in\nmultiple domains to achieve remarkable success. Time series foundation models\nrepresent a significant extension of this paradigm, demonstrating exceptional\nexpressive power, generalization, and cross-domain transferability. However,\nthis gives rise to a fundamental paradox: time series data reflect distinct\ndynamical systems, making cross-domain transfer intuitively implausible, yet\nthis is contradicted by the models' empirical success. To resolve this paradox,\nthis paper investigates, from both theoretical and experimental perspectives,\nthe representation learning mechanisms and generalization capabilities of\npatch-based time series foundation models. We argue that such models are not\nmerely applying a new architecture but are fundamentally generalizing the\nrepresentation paradigm of language models by extending deterministic\nvector-based representations to latent probabilistic distributional forms. Our\ntheoretical analysis supports this framework by demonstrating that continuous\ntime-series patches can be faithfully quantized into a discrete vocabulary\nwhose key statistical properties are highly consistent with those of natural\nlanguage. This generalization allows time series models to inherit the robust\nrepresentation and transfer abilities of large language models, thereby\nexplaining their superior performance in temporal tasks. Ultimately, our work\nprovides a rigorous theoretical cornerstone for understanding, evaluating, and\nimproving the safety and reliability of large-scale time series foundation\nmodels.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00078v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00078v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2507.00078", "title": "The language of time: a language model perspective on time-series foundation models", "authors": ["Yi Xie", "Yun Xiong", "Zejian Shi", "Hao Niu", "Zhengfu Liu"], "summary": "With the rise of large language models, the paradigm of training foundation\nmodels with massive parameter counts on vast datasets has been adopted in\nmultiple domains to achieve remarkable success. Time series foundation models\nrepresent a significant extension of this paradigm, demonstrating exceptional\nexpressive power, generalization, and cross-domain transferability. However,\nthis gives rise to a fundamental paradox: time series data reflect distinct\ndynamical systems, making cross-domain transfer intuitively implausible, yet\nthis is contradicted by the models' empirical success. To resolve this paradox,\nthis paper investigates, from both theoretical and experimental perspectives,\nthe representation learning mechanisms and generalization capabilities of\npatch-based time series foundation models. We argue that such models are not\nmerely applying a new architecture but are fundamentally generalizing the\nrepresentation paradigm of language models by extending deterministic\nvector-based representations to latent probabilistic distributional forms. Our\ntheoretical analysis supports this framework by demonstrating that continuous\ntime-series patches can be faithfully quantized into a discrete vocabulary\nwhose key statistical properties are highly consistent with those of natural\nlanguage. This generalization allows time series models to inherit the robust\nrepresentation and transfer abilities of large language models, thereby\nexplaining their superior performance in temporal tasks. Ultimately, our work\nprovides a rigorous theoretical cornerstone for understanding, evaluating, and\nimproving the safety and reliability of large-scale time series foundation\nmodels.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00078v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00078v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2507.00358", "title": "Data-Driven Exploration for a Class of Continuous-Time Linear--Quadratic Reinforcement Learning Problems", "authors": ["Yilie Huang", "Xun Yu Zhou"], "summary": "We study reinforcement learning (RL) for the same class of continuous-time\nstochastic linear--quadratic (LQ) control problems as in\n\\cite{huang2024sublinear}, where volatilities depend on both states and\ncontrols while states are scalar-valued and running control rewards are absent.\nWe propose a model-free, data-driven exploration mechanism that adaptively\nadjusts entropy regularization by the critic and policy variance by the actor.\nUnlike the constant or deterministic exploration schedules employed in\n\\cite{huang2024sublinear}, which require extensive tuning for implementations\nand ignore learning progresses during iterations, our adaptive exploratory\napproach boosts learning efficiency with minimal tuning. Despite its\nflexibility, our method achieves a sublinear regret bound that matches the\nbest-known model-free results for this class of LQ problems, which were\npreviously derived only with fixed exploration schedules. Numerical experiments\ndemonstrate that adaptive explorations accelerate convergence and improve\nregret performance compared to the non-adaptive model-free and model-based\ncounterparts.", "comment": "36 pages, 10 figures", "pdf_url": "http://arxiv.org/pdf/2507.00358v1", "categories": ["cs.LG", "cs.AI", "cs.SY", "eess.SY", "math.OC"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00358v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00566", "title": "Zero-shot Skeleton-based Action Recognition with Prototype-guided Feature Alignment", "authors": ["Kai Zhou", "Shuhai Zhang", "Zeng You", "Jinwu Hu", "Mingkui Tan", "Fei Liu"], "summary": "Zero-shot skeleton-based action recognition aims to classify unseen\nskeleton-based human actions without prior exposure to such categories during\ntraining. This task is extremely challenging due to the difficulty in\ngeneralizing from known to unknown actions. Previous studies typically use\ntwo-stage training: pre-training skeleton encoders on seen action categories\nusing cross-entropy loss and then aligning pre-extracted skeleton and text\nfeatures, enabling knowledge transfer to unseen classes through skeleton-text\nalignment and language models' generalization. However, their efficacy is\nhindered by 1) insufficient discrimination for skeleton features, as the fixed\nskeleton encoder fails to capture necessary alignment information for effective\nskeleton-text alignment; 2) the neglect of alignment bias between skeleton and\nunseen text features during testing. To this end, we propose a prototype-guided\nfeature alignment paradigm for zero-shot skeleton-based action recognition,\ntermed PGFA. Specifically, we develop an end-to-end cross-modal contrastive\ntraining framework to improve skeleton-text alignment, ensuring sufficient\ndiscrimination for skeleton features. Additionally, we introduce a\nprototype-guided text feature alignment strategy to mitigate the adverse impact\nof the distribution discrepancy during testing. We provide a theoretical\nanalysis to support our prototype-guided text feature alignment strategy and\nempirically evaluate our overall PGFA on three well-known datasets. Compared\nwith the top competitor SMIE method, our PGFA achieves absolute accuracy\nimprovements of 22.96%, 12.53%, and 18.54% on the NTU-60, NTU-120, and PKU-MMD\ndatasets, respectively.", "comment": "This paper is accepted by IEEE TIP 2025. Code is publicly available\n  at https://github.com/kaai520/PGFA", "pdf_url": "http://arxiv.org/pdf/2507.00566v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00566v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00081", "title": "State and Memory is All You Need for Robust and Reliable AI Agents", "authors": ["Matthew Muhoberac", "Atharva Parikh", "Nirvi Vakharia", "Saniya Virani", "Aco Radujevic", "Savannah Wood", "Meghav Verma", "Dimitri Metaxotos", "Jeyaraman Soundararajan", "Thierry Masquelin", "Alexander G. Godfrey", "Sean Gardner", "Dobrila Rudnicki", "Sam Michael", "Gaurav Chopra"], "summary": "Large language models (LLMs) have enabled powerful advances in natural\nlanguage understanding and generation. Yet their application to complex,\nreal-world scientific workflows remain limited by challenges in memory,\nplanning, and tool integration. Here, we introduce SciBORG (Scientific Bespoke\nArtificial Intelligence Agents Optimized for Research Goals), a modular agentic\nframework that allows LLM-based agents to autonomously plan, reason, and\nachieve robust and reliable domain-specific task execution. Agents are\nconstructed dynamically from source code documentation and augmented with\nfinite-state automata (FSA) memory, enabling persistent state tracking and\ncontext-aware decision-making. This approach eliminates the need for manual\nprompt engineering and allows for robust, scalable deployment across diverse\napplications via maintaining context across extended workflows and to recover\nfrom tool or execution failures. We validate SciBORG through integration with\nboth physical and virtual hardware, such as microwave synthesizers for\nexecuting user-specified reactions, with context-aware decision making and\ndemonstrate its use in autonomous multi-step bioassay retrieval from the\nPubChem database utilizing multi-step planning, reasoning, agent-to-agent\ncommunication and coordination for execution of exploratory tasks. Systematic\nbenchmarking shows that SciBORG agents achieve reliable execution, adaptive\nplanning, and interpretable state transitions. Our results show that memory and\nstate awareness are critical enablers of agentic planning and reliability,\noffering a generalizable foundation for deploying AI agents in complex\nenvironments.", "comment": "5 Main Figures, 10 Extended Data Figures (37 Pages) for Manuscript ;\n  9 Supplementary Tables, 40 Supplementary Figures (180 Pages) for Supporting\n  Information", "pdf_url": "http://arxiv.org/pdf/2507.00081v1", "categories": ["cs.MA", "cs.AI", "cs.CL", "cs.ET", "physics.chem-ph"], "cate": "cs.MA", "url": "http://arxiv.org/abs/2507.00081v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00081", "title": "State and Memory is All You Need for Robust and Reliable AI Agents", "authors": ["Matthew Muhoberac", "Atharva Parikh", "Nirvi Vakharia", "Saniya Virani", "Aco Radujevic", "Savannah Wood", "Meghav Verma", "Dimitri Metaxotos", "Jeyaraman Soundararajan", "Thierry Masquelin", "Alexander G. Godfrey", "Sean Gardner", "Dobrila Rudnicki", "Sam Michael", "Gaurav Chopra"], "summary": "Large language models (LLMs) have enabled powerful advances in natural\nlanguage understanding and generation. Yet their application to complex,\nreal-world scientific workflows remain limited by challenges in memory,\nplanning, and tool integration. Here, we introduce SciBORG (Scientific Bespoke\nArtificial Intelligence Agents Optimized for Research Goals), a modular agentic\nframework that allows LLM-based agents to autonomously plan, reason, and\nachieve robust and reliable domain-specific task execution. Agents are\nconstructed dynamically from source code documentation and augmented with\nfinite-state automata (FSA) memory, enabling persistent state tracking and\ncontext-aware decision-making. This approach eliminates the need for manual\nprompt engineering and allows for robust, scalable deployment across diverse\napplications via maintaining context across extended workflows and to recover\nfrom tool or execution failures. We validate SciBORG through integration with\nboth physical and virtual hardware, such as microwave synthesizers for\nexecuting user-specified reactions, with context-aware decision making and\ndemonstrate its use in autonomous multi-step bioassay retrieval from the\nPubChem database utilizing multi-step planning, reasoning, agent-to-agent\ncommunication and coordination for execution of exploratory tasks. Systematic\nbenchmarking shows that SciBORG agents achieve reliable execution, adaptive\nplanning, and interpretable state transitions. Our results show that memory and\nstate awareness are critical enablers of agentic planning and reliability,\noffering a generalizable foundation for deploying AI agents in complex\nenvironments.", "comment": "5 Main Figures, 10 Extended Data Figures (37 Pages) for Manuscript ;\n  9 Supplementary Tables, 40 Supplementary Figures (180 Pages) for Supporting\n  Information", "pdf_url": "http://arxiv.org/pdf/2507.00081v1", "categories": ["cs.MA", "cs.AI", "cs.CL", "cs.ET", "physics.chem-ph"], "cate": "cs.MA", "url": "http://arxiv.org/abs/2507.00081v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00390", "title": "MoNE: Replacing Redundant Experts with Lightweight Novices for Structured Pruning of MoE", "authors": ["Geng Zhang", "Yuxuan Han", "Yuxuan Lou", "Wangbo Zhao", "Yiqi Zhang", "Yang You"], "summary": "Mixture-of-Experts (MoE) enables efficient scaling of large language models\nby activating only a subset of experts per input token. However, deploying\nMoE-based models incurs significant memory overhead due to the need to retain\nall experts in memory. While structured pruning is promising to reduce memory\ncosts, existing methods often show suboptimal performance and unstable\ndegradation in three dimensions: model architectures, calibration data sources,\nand calibration sample sizes. This paper proposes\nMixture-of-Novices-and-Experts (MoNE), a novel expert pruning method that\nreplaces redundant experts with lightweight novices to achieve effective and\nrobust model compression. MoNE evaluates expert redundancy based on two\nmetrics: access frequency and output variance. Experts exhibiting low usage and\nstable outputs are pruned and replaced with lightweight novices-unbiased\nestimations of their original outputs-minimizing performance degradation.\nExtensive experiments demonstrate that MoNE consistently outperforms baseline\nmethods with minimal accuracy degradation across the three dimensions,\nconfirming its effectiveness and robustness. Notably, it improves the average\nzero shot accuracy across nine downstream tasks by up to 2.71 under 25\\%\npruning ratio and 3.61 under 50\\% pruning. The code is available at\nhttps://github.com/zxgx/mode-pd.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00390v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00390v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00570", "title": "Out-of-distribution detection in 3D applications: a review", "authors": ["Zizhao Li", "Xueyang Kang", "Joseph West", "Kourosh Khoshelham"], "summary": "The ability to detect objects that are not prevalent in the training set is a\ncritical capability in many 3D applications, including autonomous driving.\nMachine learning methods for object recognition often assume that all object\ncategories encountered during inference belong to a closed set of classes\npresent in the training data. This assumption limits generalization to the real\nworld, as objects not seen during training may be misclassified or entirely\nignored. As part of reliable AI, OOD detection identifies inputs that deviate\nsignificantly from the training distribution. This paper provides a\ncomprehensive overview of OOD detection within the broader scope of trustworthy\nand uncertain AI. We begin with key use cases across diverse domains, introduce\nbenchmark datasets spanning multiple modalities, and discuss evaluation\nmetrics. Next, we present a comparative analysis of OOD detection\nmethodologies, exploring model structures, uncertainty indicators, and\ndistributional distance taxonomies, alongside uncertainty calibration\ntechniques. Finally, we highlight promising research directions, including\nadversarially robust OOD detection and failure identification, particularly\nrelevant to 3D applications. The paper offers both theoretical and practical\ninsights into OOD detection, showcasing emerging research opportunities such as\n3D vision integration. These insights help new researchers navigate the field\nmore effectively, contributing to the development of reliable, safe, and robust\nAI systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00570v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00570v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00082", "title": "Federated Learning-Enabled Hybrid Language Models for Communication-Efficient Token Transmission", "authors": ["Faranaksadat Solat", "Joohyung Lee", "Mohamed Seif", "Dusit Niyato", "H. Vincent Poor"], "summary": "Hybrid Language Models (HLMs) combine the low-latency efficiency of Small\nLanguage Models (SLMs) on edge devices with the high accuracy of Large Language\nModels (LLMs) on centralized servers. Unlike traditional end-to-end LLM\ninference, HLMs reduce latency and communication by invoking LLMs only when\nlocal SLM predictions are uncertain, i.e., when token-level confidence is low\nor entropy is high. However, ambiguous or low-confidence predictions still\nrequire frequent offloading to the LLM, leading to significant communication\noverhead in bandwidth-constrained settings. To address this, we propose FedHLM,\na communication-efficient HLM framework that integrates uncertainty-aware\ninference with Federated Learning (FL). FedHLM's key innovation lies in\ncollaboratively learning token-level uncertainty thresholds that govern when\nLLM assistance is needed. Rather than using static or manually tuned\nthresholds, FedHLM employs FL to optimize these thresholds in a\nprivacy-preserving, distributed manner. Additionally, it leverages\nembedding-based token representations for Peer-to-Peer (P2P) resolution,\nenabling clients to reuse tokens inferred by semantically similar peers without\nengaging the LLM. We further introduce hierarchical model aggregation: edge\nservers refine local routing policies through client updates, while\ncross-cluster coordination aligns global decision boundaries. This layered\ndesign captures recurring uncertainty patterns, reducing redundant LLM queries.\nExperiments on large-scale news classification tasks show that FedHLM reduces\nLLM transmissions by over 95 percent with negligible accuracy loss, making it\nwell-suited for scalable and efficient edge-AI applications.", "comment": "17 pages, 16 figures, IEEE Internet of Things", "pdf_url": "http://arxiv.org/pdf/2507.00082v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00082v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00082", "title": "Federated Learning-Enabled Hybrid Language Models for Communication-Efficient Token Transmission", "authors": ["Faranaksadat Solat", "Joohyung Lee", "Mohamed Seif", "Dusit Niyato", "H. Vincent Poor"], "summary": "Hybrid Language Models (HLMs) combine the low-latency efficiency of Small\nLanguage Models (SLMs) on edge devices with the high accuracy of Large Language\nModels (LLMs) on centralized servers. Unlike traditional end-to-end LLM\ninference, HLMs reduce latency and communication by invoking LLMs only when\nlocal SLM predictions are uncertain, i.e., when token-level confidence is low\nor entropy is high. However, ambiguous or low-confidence predictions still\nrequire frequent offloading to the LLM, leading to significant communication\noverhead in bandwidth-constrained settings. To address this, we propose FedHLM,\na communication-efficient HLM framework that integrates uncertainty-aware\ninference with Federated Learning (FL). FedHLM's key innovation lies in\ncollaboratively learning token-level uncertainty thresholds that govern when\nLLM assistance is needed. Rather than using static or manually tuned\nthresholds, FedHLM employs FL to optimize these thresholds in a\nprivacy-preserving, distributed manner. Additionally, it leverages\nembedding-based token representations for Peer-to-Peer (P2P) resolution,\nenabling clients to reuse tokens inferred by semantically similar peers without\nengaging the LLM. We further introduce hierarchical model aggregation: edge\nservers refine local routing policies through client updates, while\ncross-cluster coordination aligns global decision boundaries. This layered\ndesign captures recurring uncertainty patterns, reducing redundant LLM queries.\nExperiments on large-scale news classification tasks show that FedHLM reduces\nLLM transmissions by over 95 percent with negligible accuracy loss, making it\nwell-suited for scalable and efficient edge-AI applications.", "comment": "17 pages, 16 figures, IEEE Internet of Things", "pdf_url": "http://arxiv.org/pdf/2507.00082v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00082v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00394", "title": "HelixPipe: Efficient Distributed Training of Long Sequence Transformers with Attention Parallel Pipeline Parallelism", "authors": ["Geng Zhang", "Shenggan Cheng", "Xuanlei Zhao", "Ziming Liu", "Yang You"], "summary": "As transformer sequence lengths grow, existing pipeline parallelisms incur\nsuboptimal performance due to the quadratic attention computation and the\nsubstantial memory overhead. To relieve these challenges, we propose HelixPipe,\na novel pipeline parallelism for long sequence transformer training. First,\nHelixPipe introduces attention parallel partition, which schedules attention\ncomputations of different micro batches across different pipeline stages in\nparallel, reducing pipeline bubbles. Second, it employs a two-fold\nfirst-in-last-out micro batch schedule to balance memory usage and overlap\ncommunication with computation. Additionally, HelixPipe utilizes recomputation\nwithout attention and chunked MLP to mitigate fragmentation and enable longer\nsequences. Experiments demonstrate that HelixPipe gains increasing advantages\nwith longer sequence lengths, and outperforms existing methods in throughput\nand scalability across varying pipeline sizes, model sizes, and cluster\nconfigurations. Notably, it achieves a 26\\% speedup over baseline methods when\ntraining a 7B model with 128k sequence length on 64 H20 GPUs. Code is available\nat https://github.com/code-tunnel/Megatron-LM/tree/dev.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00394v1", "categories": ["cs.LG", "cs.DC"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00394v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00583", "title": "AI-Generated Video Detection via Perceptual Straightening", "authors": ["Christian Internò", "Robert Geirhos", "Markus Olhofer", "Sunny Liu", "Barbara Hammer", "David Klindt"], "summary": "The rapid advancement of generative AI enables highly realistic synthetic\nvideos, posing significant challenges for content authentication and raising\nurgent concerns about misuse. Existing detection methods often struggle with\ngeneralization and capturing subtle temporal inconsistencies. We propose\nReStraV(Representation Straightening Video), a novel approach to distinguish\nnatural from AI-generated videos. Inspired by the \"perceptual straightening\"\nhypothesis -- which suggests real-world video trajectories become more straight\nin neural representation domain -- we analyze deviations from this expected\ngeometric property. Using a pre-trained self-supervised vision transformer\n(DINOv2), we quantify the temporal curvature and stepwise distance in the\nmodel's representation domain. We aggregate statistics of these measures for\neach video and train a classifier. Our analysis shows that AI-generated videos\nexhibit significantly different curvature and distance patterns compared to\nreal videos. A lightweight classifier achieves state-of-the-art detection\nperformance (e.g., 97.17% accuracy and 98.63% AUROC on the VidProM benchmark),\nsubstantially outperforming existing image- and video-based methods. ReStraV is\ncomputationally efficient, it is offering a low-cost and effective detection\nsolution. This work provides new insights into using neural representation\ngeometry for AI-generated video detection.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00583v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00583v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00092", "title": "Thinking About Thinking: SAGE-nano's Inverse Reasoning for Self-Aware Language Models", "authors": ["Basab Jha", "Firoj Paudel", "Ujjwal Puri", "Zhang Yuting", "Choi Donghyuk", "Wang Junhao"], "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities at\nsolving complex reasoning tasks with Chain-of-Thought (CoT) prompting, but\ntheir decision-making processes remain somewhat blackbox. We introduce\ntextbfinverse reasoning, a novel paradigm enabling LLMs to decompose and\nexplain their own reasoning chains post-hoc. Our approach, used in SAGE-nano, a\n4-billion-parameter reasoning model, employs a metacognitive structure that\nreflects back via attention processes to identify major decision points and\ngenerate explanations of reasoning choices. While typical CoT approaches are\ndirected towards forward reasoning generation, inverse reasoning provides\ninsight into why specific reasoning chains were selected over others. Through\nthorough testing of logical reasoning puzzles, math problems and ethical\ndilemmas from AQUA-RAT, CommonsenseQA, and customized benchmarks, we\ndemonstrate that SAGE-nano is at the cutting edge both on reasoning accuracy\n(74.6% on AQUA-RAT) and explanation quality (92.1% human preference score) for\nits task, and offers performance almost on par with models like Claude-3.5\nSonnet or GPT-4o. Our contributions are: (i) the first rigorous framework for\nLLM self-reflection via inverse reasoning, (ii) a novel metalearning framework\nto reverse the attention flow, (iii) comprehensive evaluation frameworks for\nreasoning transparency, and (iv) evidence that increasing reasoning using\ninverse reasoning improves interpretability along with reasoning performance.\nOur work creates new avenues for transparent AI systems and closes significant\ngaps in AI safety, education, and scientific discovery.", "comment": "19 pages, 2 figures, 9 tables", "pdf_url": "http://arxiv.org/pdf/2507.00092v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2507.00092v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00083", "title": "Strategic Counterfactual Modeling of Deep-Target Airstrike Systems via Intervention-Aware Spatio-Causal Graph Networks", "authors": ["Wei Meng"], "summary": "This study addresses the lack of structured causal modeling between tactical\nstrike behavior and strategic delay in current strategic-level simulations,\nparticularly the structural bottlenecks in capturing intermediate variables\nwithin the \"resilience - nodal suppression - negotiation window\" chain. We\npropose the Intervention-Aware Spatio-Temporal Graph Neural Network (IA-STGNN),\na novel framework that closes the causal loop from tactical input to strategic\ndelay output. The model integrates graph attention mechanisms, counterfactual\nsimulation units, and spatial intervention node reconstruction to enable\ndynamic simulations of strike configurations and synchronization strategies.\nTraining data are generated from a multi-physics simulation platform (GEANT4 +\nCOMSOL) under NIST SP 800-160 standards, ensuring structural traceability and\npolicy-level validation. Experimental results demonstrate that IA-STGNN\nsignificantly outperforms baseline models (ST-GNN, GCN-LSTM, XGBoost),\nachieving a 12.8 percent reduction in MAE and 18.4 percent increase in Top-5\npercent accuracy, while improving causal path consistency and intervention\nstability. IA-STGNN enables interpretable prediction of strategic delay and\nsupports applications such as nuclear deterrence simulation, diplomatic window\nassessment, and multi-strategy optimization, providing a structured and\ntransparent AI decision-support mechanism for high-level policy modeling.", "comment": "This paper proposes the first closed-loop causal modeling framework\n  (IA-STGNN) that links tactical strike variables to strategic delay outcomes\n  via graph neural networks with counterfactual reasoning", "pdf_url": "http://arxiv.org/pdf/2507.00083v1", "categories": ["cs.LG", "cs.AI", "91A80, 91B62, 68T07", "I.2.6; J.7; K.4.1; C.2.4"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00083v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00411", "title": "Diffusion Disambiguation Models for Partial Label Learning", "authors": ["Jinfu Fan", "Xiaohui Zhong", "Kangrui Ren", "Jiangnan Li", "Linqing Huang"], "summary": "Learning from ambiguous labels is a long-standing problem in practical\nmachine learning applications. The purpose of \\emph{partial label learning}\n(PLL) is to identify the ground-truth label from a set of candidate labels\nassociated with a given instance. Inspired by the remarkable performance of\ndiffusion models in various generation tasks, this paper explores their\npotential to denoise ambiguous labels through the reverse denoising process.\nTherefore, this paper reformulates the label disambiguation problem from the\nperspective of generative models, where labels are generated by iteratively\nrefining initial random guesses. This perspective enables the diffusion model\nto learn how label information is generated stochastically. By modeling the\ngeneration uncertainty, we can use the maximum likelihood estimate of the label\nfor classification inference. However, such ambiguous labels lead to a mismatch\nbetween instance and label, which reduces the quality of generated data. To\naddress this issue, this paper proposes a \\emph{diffusion disambiguation model\nfor PLL} (DDMP), which first uses the potential complementary information\nbetween instances and labels to construct pseudo-clean labels for initial\ndiffusion training. Furthermore, a transition-aware matrix is introduced to\nestimate the potential ground-truth labels, which are dynamically updated\nduring the diffusion generation. During training, the ground-truth label is\nprogressively refined, improving the classifier. Experiments show the advantage\nof the DDMP and its suitability for PLL.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00411v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00411v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00585", "title": "Similarity Memory Prior is All You Need for Medical Image Segmentation", "authors": ["Tang Hao", "Guo ZhiQing", "Wang LieJun", "Liu Chao"], "summary": "In recent years, it has been found that \"grandmother cells\" in the primary\nvisual cortex (V1) of macaques can directly recognize visual input with complex\nshapes. This inspires us to examine the value of these cells in promoting the\nresearch of medical image segmentation. In this paper, we design a Similarity\nMemory Prior Network (Sim-MPNet) for medical image segmentation. Specifically,\nwe propose a Dynamic Memory Weights-Loss Attention (DMW-LA), which matches and\nremembers the category features of specific lesions or organs in medical images\nthrough the similarity memory prior in the prototype memory bank, thus helping\nthe network to learn subtle texture changes between categories. DMW-LA also\ndynamically updates the similarity memory prior in reverse through Weight-Loss\nDynamic (W-LD) update strategy, effectively assisting the network directly\nextract category features. In addition, we propose the Double-Similarity Global\nInternal Enhancement Module (DS-GIM) to deeply explore the internal differences\nin the feature distribution of input data through cosine similarity and\neuclidean distance. Extensive experiments on four public datasets show that\nSim-MPNet has better segmentation performance than other state-of-the-art\nmethods. Our code is available on https://github.com/vpsg-research/Sim-MPNet.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00585v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00585v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00234", "title": "Interpretable AI for Time-Series: Multi-Model Heatmap Fusion with Global Attention and NLP-Generated Explanations", "authors": ["Jiztom Kavalakkatt Francis", "Matthew J Darr"], "summary": "In this paper, we present a novel framework for enhancing model\ninterpretability by integrating heatmaps produced separately by ResNet and a\nrestructured 2D Transformer with globally weighted input saliency. We address\nthe critical problem of spatial-temporal misalignment in existing\ninterpretability methods, where convolutional networks fail to capture global\ncontext and Transformers lack localized precision - a limitation that impedes\nactionable insights in safety-critical domains like healthcare and industrial\nmonitoring. Our method merges gradient-weighted activation maps (ResNet) and\nTransformer attention rollout into a unified visualization, achieving full\nspatial-temporal alignment while preserving real-time performance. Empirical\nevaluations on clinical (ECG arrhythmia detection) and industrial (energy\nconsumption prediction) datasets demonstrate significant improvements: the\nhybrid framework achieves 94.1% accuracy (F1 0.93) on the PhysioNet dataset and\nreduces regression error to RMSE = 0.28 kWh (R2 = 0.95) on the UCI Energy\nAppliance dataset-outperforming standalone ResNet, Transformer, and\nInceptionTime baselines by 3.8-12.4%. An NLP module translates fused heatmaps\ninto domain-specific narratives (e.g., \"Elevated ST-segment between 2-4 seconds\nsuggests myocardial ischemia\"), validated via BLEU-4 (0.586) and ROUGE-L\n(0.650) scores. By formalizing interpretability as causal fidelity and\nspatial-temporal alignment, our approach bridges the gap between technical\noutputs and stakeholder understanding, offering a scalable solution for\ntransparent, time-aware decision-making.", "comment": "13 pages", "pdf_url": "http://arxiv.org/pdf/2507.00234v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00234v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00085", "title": "A Joint Topology-Data Fusion Graph Network for Robust Traffic Speed Prediction with Data Anomalism", "authors": ["Ruiyuan Jiang", "Dongyao Jia", "Eng Gee Lim", "Pengfei Fan", "Yuli Zhang", "Shangbo Wang"], "summary": "Accurate traffic prediction is essential for Intelligent Transportation\nSystems (ITS), yet current methods struggle with the inherent complexity and\nnon-linearity of traffic dynamics, making it difficult to integrate spatial and\ntemporal characteristics. Furthermore, existing approaches use static\ntechniques to address non-stationary and anomalous historical data, which\nlimits adaptability and undermines data smoothing. To overcome these\nchallenges, we propose the Graph Fusion Enhanced Network (GFEN), an innovative\nframework for network-level traffic speed prediction. GFEN introduces a novel\ntopological spatiotemporal graph fusion technique that meticulously extracts\nand merges spatial and temporal correlations from both data distribution and\nnetwork topology using trainable methods, enabling the modeling of multi-scale\nspatiotemporal features. Additionally, GFEN employs a hybrid methodology\ncombining a k-th order difference-based mathematical framework with an\nattention-based deep learning structure to adaptively smooth historical\nobservations and dynamically mitigate data anomalies and non-stationarity.\nExtensive experiments demonstrate that GFEN surpasses state-of-the-art methods\nby approximately 6.3% in prediction accuracy and exhibits convergence rates\nnearly twice as fast as recent hybrid models, confirming its superior\nperformance and potential to significantly enhance traffic prediction system\nefficiency.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00085v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00085v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00425", "title": "Flexible Language Modeling in Continuous Space with Transformer-based Autoregressive Flows", "authors": ["Ruixiang Zhang", "Shuangfei Zhai", "Jiatao Gu", "Yizhe Zhang", "Huangjie Zheng", "Tianrong Chen", "Miguel Angel Bautista", "Josh Susskind", "Navdeep Jaitly"], "summary": "Autoregressive models have driven remarkable progress in language modeling.\nTheir foundational reliance on discrete tokens, unidirectional context, and\nsingle-pass decoding, while central to their success, also inspires the\nexploration of a design space that could offer new axes of modeling\nflexibility. In this work, we explore an alternative paradigm, shifting\nlanguage modeling from a discrete token space to a continuous latent space. We\npropose a novel framework TarFlowLM, that employs transformer-based\nautoregressive normalizing flows to model these continuous representations.\nThis approach unlocks substantial flexibility, enabling the construction of\nmodels that can capture global bi-directional context through stacked,\nalternating-direction autoregressive transformations, support block-wise\ngeneration with flexible token patch sizes, and facilitate a hierarchical\nmulti-pass generation process. We further propose new mixture-based coupling\ntransformations designed to capture complex dependencies within the latent\nspace shaped by discrete data, and demonstrate theoretical connections to\nconventional discrete autoregressive models. Extensive experiments on language\nmodeling benchmarks demonstrate strong likelihood performance and highlight the\nflexible modeling capabilities inherent in our framework.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00425v1", "categories": ["cs.LG", "cs.CL"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00425v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00586", "title": "Context-Aware Academic Emotion Dataset and Benchmark", "authors": ["Luming Zhao", "Jingwen Xuan", "Jiamin Lou", "Yonghui Yu", "Wenwu Yang"], "summary": "Academic emotion analysis plays a crucial role in evaluating students'\nengagement and cognitive states during the learning process. This paper\naddresses the challenge of automatically recognizing academic emotions through\nfacial expressions in real-world learning environments. While significant\nprogress has been made in facial expression recognition for basic emotions,\nacademic emotion recognition remains underexplored, largely due to the scarcity\nof publicly available datasets. To bridge this gap, we introduce RAER, a novel\ndataset comprising approximately 2,700 video clips collected from around 140\nstudents in diverse, natural learning contexts such as classrooms, libraries,\nlaboratories, and dormitories, covering both classroom sessions and individual\nstudy. Each clip was annotated independently by approximately ten annotators\nusing two distinct sets of academic emotion labels with varying granularity,\nenhancing annotation consistency and reliability. To our knowledge, RAER is the\nfirst dataset capturing diverse natural learning scenarios. Observing that\nannotators naturally consider context cues-such as whether a student is looking\nat a phone or reading a book-alongside facial expressions, we propose CLIP-CAER\n(CLIP-based Context-aware Academic Emotion Recognition). Our method utilizes\nlearnable text prompts within the vision-language model CLIP to effectively\nintegrate facial expression and context cues from videos. Experimental results\ndemonstrate that CLIP-CAER substantially outperforms state-of-the-art\nvideo-based facial expression recognition methods, which are primarily designed\nfor basic emotions, emphasizing the crucial role of context in accurately\nrecognizing academic emotions. Project page: https://zgsfer.github.io/CAER", "comment": "Accepted to ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.00586v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00586v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00248", "title": "Developing Lightweight DNN Models With Limited Data For Real-Time Sign Language Recognition", "authors": ["Nikita Nikitin", "Eugene Fomin"], "summary": "We present a novel framework for real-time sign language recognition using\nlightweight DNNs trained on limited data. Our system addresses key challenges\nin sign language recognition, including data scarcity, high computational\ncosts, and discrepancies in frame rates between training and inference\nenvironments. By encoding sign language specific parameters, such as handshape,\npalm orientation, movement, and location into vectorized inputs, and leveraging\nMediaPipe for landmark extraction, we achieve highly separable input data\nrepresentations. Our DNN architecture, optimized for sub 10MB deployment,\nenables accurate classification of 343 signs with less than 10ms latency on\nedge devices. The data annotation platform 'slait data' facilitates structured\nlabeling and vector extraction. Our model achieved 92% accuracy in isolated\nsign recognition and has been integrated into the 'slait ai' web application,\nwhere it demonstrates stable inference.", "comment": "7 pages, 2 figures, 2 tables, for associated mpeg file, see\n  https://slait.app/static/Screen_Recording.mp4", "pdf_url": "http://arxiv.org/pdf/2507.00248v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00248v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00087", "title": "pUniFind: a unified large pre-trained deep learning model pushing the limit of mass spectra interpretation", "authors": ["Jiale Zhao", "Pengzhi Mao", "Kaifei Wang", "Yiming Li", "Yaping Peng", "Ranfei Chen", "Shuqi Lu", "Xiaohong Ji", "Jiaxiang Ding", "Xin Zhang", "Yucheng Liao", "Weinan E", "Weijie Zhang", "Han Wen", "Hao Chi"], "summary": "Deep learning has advanced mass spectrometry data interpretation, yet most\nmodels remain feature extractors rather than unified scoring frameworks. We\npresent pUniFind, the first large-scale multimodal pre-trained model in\nproteomics that integrates end-to-end peptide-spectrum scoring with open,\nzero-shot de novo sequencing. Trained on over 100 million open search-derived\nspectra, pUniFind aligns spectral and peptide modalities via cross modality\nprediction and outperforms traditional engines across diverse datasets,\nparticularly achieving a 42.6 percent increase in the number of identified\npeptides in immunopeptidomics. Supporting over 1,300 modifications, pUniFind\nidentifies 60 percent more PSMs than existing de novo methods despite a\n300-fold larger search space. A deep learning based quality control module\nfurther recovers 38.5 percent additional peptides including 1,891 mapped to the\ngenome but absent from reference proteomes while preserving full fragment ion\ncoverage. These results establish a unified, scalable deep learning framework\nfor proteomic analysis, offering improved sensitivity, modification coverage,\nand interpretability.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00087v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00087v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00440", "title": "A Recipe for Causal Graph Regression: Confounding Effects Revisited", "authors": ["Yujia Yin", "Tianyi Qu", "Zihao Wang", "Yifan Chen"], "summary": "Through recognizing causal subgraphs, causal graph learning (CGL) has risen\nto be a promising approach for improving the generalizability of graph neural\nnetworks under out-of-distribution (OOD) scenarios. However, the empirical\nsuccesses of CGL techniques are mostly exemplified in classification settings,\nwhile regression tasks, a more challenging setting in graph learning, are\noverlooked. We thus devote this work to tackling causal graph regression (CGR);\nto this end we reshape the processing of confounding effects in existing CGL\nstudies, which mainly deal with classification. Specifically, we reflect on the\npredictive power of confounders in graph-level regression, and generalize\nclassification-specific causal intervention techniques to regression through a\nlens of contrastive learning. Extensive experiments on graph OOD benchmarks\nvalidate the efficacy of our proposals for CGR. The model implementation and\nthe code are provided on https://github.com/causal-graph/CGR.", "comment": "ICML 2025 accepted", "pdf_url": "http://arxiv.org/pdf/2507.00440v1", "categories": ["cs.LG", "cs.AI", "stat.ME"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00440v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00593", "title": "Overtake Detection in Trucks Using CAN Bus Signals: A Comparative Study of Machine Learning Methods", "authors": ["Fernando Alonso-Fernandez", "Talha Hanif Butt", "Prayag Tiwari"], "summary": "Safe overtaking manoeuvres in trucks are vital for preventing accidents and\nensuring efficient traffic flow. Accurate prediction of such manoeuvres is\nessential for Advanced Driver Assistance Systems (ADAS) to make timely and\ninformed decisions. In this study, we focus on overtake detection using\nController Area Network (CAN) bus data collected from five in-service trucks\nprovided by the Volvo Group. We evaluate three common classifiers for vehicle\nmanoeuvre detection, Artificial Neural Networks (ANN), Random Forest (RF), and\nSupport Vector Machines (SVM), and analyse how different preprocessing\nconfigurations affect performance. We find that variability in traffic\nconditions strongly influences the signal patterns, particularly in the\nno-overtake class, affecting classification performance if training data lacks\nadequate diversity. Since the data were collected under unconstrained,\nreal-world conditions, class diversity cannot be guaranteed a priori. However,\ntraining with data from multiple vehicles improves generalisation and reduces\ncondition-specific bias. Our pertruck analysis also reveals that classification\naccuracy, especially for overtakes, depends on the amount of training data per\nvehicle. To address this, we apply a score-level fusion strategy, which yields\nthe best per-truck performance across most cases. Overall, we achieve an\naccuracy via fusion of TNR=93% (True Negative Rate) and TPR=86.5% (True\nPositive Rate). This research has been part of the BIG FUN project, which\nexplores how Artificial Intelligence can be applied to logged vehicle data to\nunderstand and predict driver behaviour, particularly in relation to Camera\nMonitor Systems (CMS), being introduced as digital replacements for traditional\nexterior mirrors.", "comment": "Under review at ESWA", "pdf_url": "http://arxiv.org/pdf/2507.00593v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00593v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00310", "title": "Open-ended Scientific Discovery via Bayesian Surprise", "authors": ["Dhruv Agarwal", "Bodhisattwa Prasad Majumder", "Reece Adamson", "Megha Chakravorty", "Satvika Reddy Gavireddy", "Aditya Parashar", "Harshit Surana", "Bhavana Dalvi Mishra", "Andrew McCallum", "Ashish Sabharwal", "Peter Clark"], "summary": "The promise of autonomous scientific discovery (ASD) hinges not only on\nanswering questions, but also on knowing which questions to ask. Most recent\nworks in ASD explore the use of large language models (LLMs) in goal-driven\nsettings, relying on human-specified research questions to guide hypothesis\ngeneration. However, scientific discovery may be accelerated further by\nallowing the AI system to drive exploration by its own criteria. The few\nexisting approaches in open-ended ASD select hypotheses based on diversity\nheuristics or subjective proxies for human interestingness, but the former\nstruggles to meaningfully navigate the typically vast hypothesis space, and the\nlatter suffers from imprecise definitions. This paper presents AutoDS -- a\nmethod for open-ended ASD that instead drives scientific exploration using\nBayesian surprise. Here, we quantify the epistemic shift from the LLM's prior\nbeliefs about a hypothesis to its posterior beliefs after gathering\nexperimental results. To efficiently explore the space of nested hypotheses,\nour method employs a Monte Carlo tree search (MCTS) strategy with progressive\nwidening using surprisal as the reward function. We evaluate AutoDS in the\nsetting of data-driven discovery across 21 real-world datasets spanning domains\nsuch as biology, economics, finance, and behavioral science. Our results\ndemonstrate that under a fixed budget, AutoDS substantially outperforms\ncompetitors by producing 5--29\\% more discoveries deemed surprising by the LLM.\nOur human evaluation further finds that two-thirds of AutoDS discoveries are\nsurprising to the domain experts, suggesting this is an important step forward\ntowards building open-ended ASD systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00310v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00310v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00088", "title": "How large language models judge and influence human cooperation", "authors": ["Alexandre S. Pires", "Laurens Samson", "Sennay Ghebreab", "Fernando P. Santos"], "summary": "Humans increasingly rely on large language models (LLMs) to support decisions\nin social settings. Previous work suggests that such tools shape people's moral\nand political judgements. However, the long-term implications of LLM-based\nsocial decision-making remain unknown. How will human cooperation be affected\nwhen the assessment of social interactions relies on language models? This is a\npressing question, as human cooperation is often driven by indirect\nreciprocity, reputations, and the capacity to judge interactions of others.\nHere, we assess how state-of-the-art LLMs judge cooperative actions. We provide\n21 different LLMs with an extensive set of examples where individuals cooperate\n-- or refuse cooperating -- in a range of social contexts, and ask how these\ninteractions should be judged. Furthermore, through an evolutionary\ngame-theoretical model, we evaluate cooperation dynamics in populations where\nthe extracted LLM-driven judgements prevail, assessing the long-term impact of\nLLMs on human prosociality. We observe a remarkable agreement in evaluating\ncooperation against good opponents. On the other hand, we notice within- and\nbetween-model variance when judging cooperation with ill-reputed individuals.\nWe show that the differences revealed between models can significantly impact\nthe prevalence of cooperation. Finally, we test prompts to steer LLM norms,\nshowing that such interventions can shape LLM judgements, particularly through\ngoal-oriented prompts. Our research connects LLM-based advices and long-term\nsocial dynamics, and highlights the need to carefully align LLM norms in order\nto preserve human cooperation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00088v1", "categories": ["physics.soc-ph", "cs.AI", "cs.SI"], "cate": "physics.soc-ph", "url": "http://arxiv.org/abs/2507.00088v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00445", "title": "Iterative Distillation for Reward-Guided Fine-Tuning of Diffusion Models in Biomolecular Design", "authors": ["Xingyu Su", "Xiner Li", "Masatoshi Uehara", "Sunwoo Kim", "Yulai Zhao", "Gabriele Scalia", "Ehsan Hajiramezanali", "Tommaso Biancalani", "Degui Zhi", "Shuiwang Ji"], "summary": "We address the problem of fine-tuning diffusion models for reward-guided\ngeneration in biomolecular design. While diffusion models have proven highly\neffective in modeling complex, high-dimensional data distributions, real-world\napplications often demand more than high-fidelity generation, requiring\noptimization with respect to potentially non-differentiable reward functions\nsuch as physics-based simulation or rewards based on scientific knowledge.\nAlthough RL methods have been explored to fine-tune diffusion models for such\nobjectives, they often suffer from instability, low sample efficiency, and mode\ncollapse due to their on-policy nature. In this work, we propose an iterative\ndistillation-based fine-tuning framework that enables diffusion models to\noptimize for arbitrary reward functions. Our method casts the problem as policy\ndistillation: it collects off-policy data during the roll-in phase, simulates\nreward-based soft-optimal policies during roll-out, and updates the model by\nminimizing the KL divergence between the simulated soft-optimal policy and the\ncurrent model policy. Our off-policy formulation, combined with KL divergence\nminimization, enhances training stability and sample efficiency compared to\nexisting RL-based methods. Empirical results demonstrate the effectiveness and\nsuperior reward optimization of our approach across diverse tasks in protein,\nsmall molecule, and regulatory DNA design.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00445v1", "categories": ["cs.LG", "cs.AI", "q-bio.QM"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00445v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00603", "title": "World4Drive: End-to-End Autonomous Driving via Intention-aware Physical Latent World Model", "authors": ["Yupeng Zheng", "Pengxuan Yang", "Zebin Xing", "Qichao Zhang", "Yuhang Zheng", "Yinfeng Gao", "Pengfei Li", "Teng Zhang", "Zhongpu Xia", "Peng Jia", "Dongbin Zhao"], "summary": "End-to-end autonomous driving directly generates planning trajectories from\nraw sensor data, yet it typically relies on costly perception supervision to\nextract scene information. A critical research challenge arises: constructing\nan informative driving world model to enable perception annotation-free,\nend-to-end planning via self-supervised learning. In this paper, we present\nWorld4Drive, an end-to-end autonomous driving framework that employs vision\nfoundation models to build latent world models for generating and evaluating\nmulti-modal planning trajectories. Specifically, World4Drive first extracts\nscene features, including driving intention and world latent representations\nenriched with spatial-semantic priors provided by vision foundation models. It\nthen generates multi-modal planning trajectories based on current scene\nfeatures and driving intentions and predicts multiple intention-driven future\nstates within the latent space. Finally, it introduces a world model selector\nmodule to evaluate and select the best trajectory. We achieve perception\nannotation-free, end-to-end planning through self-supervised alignment between\nactual future observations and predicted observations reconstructed from the\nlatent space. World4Drive achieves state-of-the-art performance without manual\nperception annotations on both the open-loop nuScenes and closed-loop NavSim\nbenchmarks, demonstrating an 18.1\\% relative reduction in L2 error, 46.7% lower\ncollision rate, and 3.75 faster training convergence. Codes will be accessed at\nhttps://github.com/ucaszyp/World4Drive.", "comment": "ICCV 2025, first version", "pdf_url": "http://arxiv.org/pdf/2507.00603v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00603v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00316", "title": "$μ^2$Tokenizer: Differentiable Multi-Scale Multi-Modal Tokenizer for Radiology Report Generation", "authors": ["Siyou Li", "Pengyao Qin", "Huanan Wu", "Dong Nie", "Arun J. Thirunavukarasu", "Juntao Yu", "Le Zhang"], "summary": "Automated radiology report generation (RRG) aims to produce detailed textual\nreports from clinical imaging, such as computed tomography (CT) scans, to\nimprove the accuracy and efficiency of diagnosis and provision of management\nadvice. RRG is complicated by two key challenges: (1) inherent complexity in\nextracting relevant information from imaging data under resource constraints,\nand (2) difficulty in objectively evaluating discrepancies between\nmodel-generated and expert-written reports. To address these challenges, we\npropose $\\mu^2$LLM, a $\\underline{\\textbf{mu}}$ltiscale\n$\\underline{\\textbf{mu}}$ltimodal large language models for RRG tasks. The\nnovel ${\\mu}^2$Tokenizer, as an intermediate layer, integrates multi-modal\nfeatures from the multiscale visual tokenizer and the text tokenizer, then\nenhances report generation quality through direct preference optimization\n(DPO), guided by GREEN-RedLlama. Experimental results on four large CT\nimage-report medical datasetdemonstrate that our method outperforms existing\napproaches, highlighting the potential of our fine-tuned $\\mu^2$LLMs on limited\ndata for RRG tasks.", "comment": "Accepted by MICCAI 2025", "pdf_url": "http://arxiv.org/pdf/2507.00316v1", "categories": ["cs.LG", "cs.CL", "eess.IV"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00316v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00090", "title": "Generating Heterogeneous Multi-dimensional Data : A Comparative Study", "authors": ["Corbeau Michael", "Claeys Emmanuelle", "Serrurier Mathieu", "Zaraté Pascale"], "summary": "Allocation of personnel and material resources is highly sensible in the case\nof firefighter interventions. This allocation relies on simulations to\nexperiment with various scenarios. The main objective of this allocation is the\nglobal optimization of the firefighters response. Data generation is then\nmandatory to study various scenarios In this study, we propose to compare\ndifferent data generation methods. Methods such as Random Sampling, Tabular\nVariational Autoencoders, standard Generative Adversarial Networks, Conditional\nTabular Generative Adversarial Networks and Diffusion Probabilistic Models are\nexamined to ascertain their efficacy in capturing the intricacies of\nfirefighter interventions. Traditional evaluation metrics often fall short in\ncapturing the nuanced requirements of synthetic datasets for real-world\nscenarios. To address this gap, an evaluation of synthetic data quality is\nconducted using a combination of domain-specific metrics tailored to the\nfirefighting domain and standard measures such as the Wasserstein distance.\nDomain-specific metrics include response time distribution, spatial-temporal\ndistribution of interventions, and accidents representation. These metrics are\ndesigned to assess data variability, the preservation of fine and complex\ncorrelations and anomalies such as event with a very low occurrence, the\nconformity with the initial statistical distribution and the operational\nrelevance of the synthetic data. The distribution has the particularity of\nbeing highly unbalanced, none of the variables following a Gaussian\ndistribution, adding complexity to the data generation process.", "comment": "accepted at IEEE SMC 2025 Vienna", "pdf_url": "http://arxiv.org/pdf/2507.00090v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00090v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00449", "title": "Overcoming Long-Context Limitations of State-Space Models via Context-Dependent Sparse Attention", "authors": ["Zhihao Zhan", "Jianan Zhao", "Zhaocheng Zhu", "Jian Tang"], "summary": "Efficient long-context modeling remains a critical challenge for natural\nlanguage processing (NLP), as the time complexity of the predominant\nTransformer architecture scales quadratically with the sequence length. While\nstate-space models (SSMs) offer alternative sub-quadratic solutions, they\nstruggle to capture long-range dependencies effectively. In this work, we focus\non analyzing and improving the long-context modeling capabilities of SSMs. We\nshow that the widely used synthetic task, associative recall, which requires a\nmodel to recall a value associated with a single key without context,\ninsufficiently represents the complexities of real-world long-context modeling.\nTo address this limitation, we extend the associative recall to a novel\nsynthetic task, \\emph{joint recall}, which requires a model to recall the value\nassociated with a key given in a specified context. Theoretically, we prove\nthat SSMs do not have the expressiveness to solve multi-query joint recall in\nsub-quadratic time complexity. To resolve this issue, we propose a solution\nbased on integrating SSMs with Context-Dependent Sparse Attention (CDSA), which\nhas the expressiveness to solve multi-query joint recall with sub-quadratic\ncomputation. To bridge the gap between theoretical analysis and real-world\napplications, we propose locality-sensitive Hashing Attention with sparse Key\nSelection (HAX), which instantiates the theoretical solution and is further\ntailored to natural language domains. Extensive experiments on both synthetic\nand real-world long-context benchmarks show that HAX consistently outperforms\nSSM baselines and SSMs integrated with context-independent sparse attention\n(CISA).", "comment": "Proceedings of the 42nd International Conference on Machine Learning,\n  ES-FoMo III: 3rd Workshop on Efficient Systems for Foundation Models, 18\n  pages, 9 figures", "pdf_url": "http://arxiv.org/pdf/2507.00449v1", "categories": ["cs.LG", "cs.CL", "I.2.7"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00449v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00608", "title": "De-Simplifying Pseudo Labels to Enhancing Domain Adaptive Object Detection", "authors": ["Zehua Fu", "Chenguang Liu", "Yuyu Chen", "Jiaqi Zhou", "Qingjie Liu", "Yunhong Wang"], "summary": "Despite its significant success, object detection in traffic and\ntransportation scenarios requires time-consuming and laborious efforts in\nacquiring high-quality labeled data. Therefore, Unsupervised Domain Adaptation\n(UDA) for object detection has recently gained increasing research attention.\nUDA for object detection has been dominated by domain alignment methods, which\nachieve top performance. Recently, self-labeling methods have gained popularity\ndue to their simplicity and efficiency. In this paper, we investigate the\nlimitations that prevent self-labeling detectors from achieving commensurate\nperformance with domain alignment methods. Specifically, we identify the high\nproportion of simple samples during training, i.e., the simple-label bias, as\nthe central cause. We propose a novel approach called De-Simplifying Pseudo\nLabels (DeSimPL) to mitigate the issue. DeSimPL utilizes an instance-level\nmemory bank to implement an innovative pseudo label updating strategy. Then,\nadversarial samples are introduced during training to enhance the proportion.\nFurthermore, we propose an adaptive weighted loss to avoid the model suffering\nfrom an abundance of false positive pseudo labels in the late training period.\nExperimental results demonstrate that DeSimPL effectively reduces the\nproportion of simple samples during training, leading to a significant\nperformance improvement for self-labeling detectors. Extensive experiments\nconducted on four benchmarks validate our analysis and conclusions.", "comment": "Accepted by IEEE Transactions on Intelligent Transportation Systems.\n  15 pages, 10 figures", "pdf_url": "http://arxiv.org/pdf/2507.00608v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00608v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00417", "title": "ASTRO: Teaching Language Models to Reason by Reflecting and Backtracking In-Context", "authors": ["Joongwon Kim", "Anirudh Goyal", "Liang Tan", "Hannaneh Hajishirzi", "Srinivasan Iyer", "Tianlu Wang"], "summary": "We introduce ASTRO, the \"Autoregressive Search-Taught Reasoner\", a framework\nfor training language models to reason like search algorithms, explicitly\nleveraging self-reflection, backtracking, and exploration in their outputs.\nRecently, training large language models (LLMs) via reinforcement learning (RL)\nhas led to the advent of reasoning models with greatly enhanced reasoning\ncapabilities. Open-source replications of reasoning models, while successful,\nbuild upon models that already exhibit strong reasoning capabilities along with\nsearch behavior observed even before RL. As a result, it is yet unclear how to\nboost the reasoning capabilities of other non-reasoner models including Llama\n3. ASTRO teaches such models to internalize structured search behavior through\na synthetic dataset derived from Monte Carlo Tree Search (MCTS) over\nmathematical problem-solving trajectories. By converting search traces into\nnatural language chain-of-thoughts that capture both successes and recoveries\nfrom failure, ASTRO bootstraps models with a rich prior for exploration during\nRL. We finetune our models on these search-derived traces and further improve\nperformance via RL with verifiable rewards. We apply ASTRO to the Llama 3\nfamily of models and achieve absolute performance gains of 16.0% on MATH-500,\n26.9% on AMC 2023, and 20.0% on AIME 2024, especially improving upon\nchallenging problems that require iterative correction. Our results demonstrate\nthat search-inspired training offers a principled way to instill robust\nreasoning capabilities into open LLMs.", "comment": "36 pages, 23 figures", "pdf_url": "http://arxiv.org/pdf/2507.00417v1", "categories": ["cs.AI", "cs.CL"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2507.00417v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00093", "title": "$σ$-Maximal Ancestral Graphs", "authors": ["Binghua Yao", "Joris M. Mooij"], "summary": "Maximal Ancestral Graphs (MAGs) provide an abstract representation of\nDirected Acyclic Graphs (DAGs) with latent (selection) variables. These\ngraphical objects encode information about ancestral relations and\nd-separations of the DAGs they represent. This abstract representation has been\nused amongst others to prove the soundness and completeness of the FCI\nalgorithm for causal discovery, and to derive a do-calculus for its output. One\nsignificant inherent limitation of MAGs is that they rule out the possibility\nof cyclic causal relationships. In this work, we address that limitation. We\nintroduce and study a class of graphical objects that we coin\n''$\\sigma$-Maximal Ancestral Graphs'' (''$\\sigma$-MAGs''). We show how these\ngraphs provide an abstract representation of (possibly cyclic) Directed Graphs\n(DGs) with latent (selection) variables, analogously to how MAGs represent\nDAGs. We study the properties of these objects and provide a characterization\nof their Markov equivalence classes.", "comment": "It has beee accepted by the 41st Conference on Uncertainty in\n  Artificial Intelligence (UAI)", "pdf_url": "http://arxiv.org/pdf/2507.00093v1", "categories": ["cs.DM", "cs.AI", "cs.DS", "math.ST", "stat.TH"], "cate": "cs.DM", "url": "http://arxiv.org/abs/2507.00093v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00451", "title": "Best Agent Identification for General Game Playing", "authors": ["Matthew Stephenson", "Alex Newcombe", "Eric Piette", "Dennis Soemers"], "summary": "We present an efficient and generalised procedure to accurately identify the\nbest performing algorithm for each sub-task in a multi-problem domain. Our\napproach treats this as a set of best arm identification problems for\nmulti-armed bandits, where each bandit corresponds to a specific task and each\narm corresponds to a specific algorithm or agent. We propose an optimistic\nselection process based on the Wilson score interval (Optimistic-WS) that ranks\neach arm across all bandits in terms of their potential regret reduction. We\nevaluate the performance of Optimistic-WS on two of the most popular general\ngame domains, the General Video Game AI (GVGAI) framework and the Ludii general\ngame playing system, with the goal of identifying the highest performing agent\nfor each game within a limited number of trials. Compared to previous best arm\nidentification algorithms for multi-armed bandits, our results demonstrate a\nsubstantial performance improvement in terms of average simple regret. This\nnovel approach can be used to significantly improve the quality and accuracy of\nagent evaluation procedures for general game frameworks, as well as other\nmulti-task domains with high algorithm runtimes.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00451v1", "categories": ["cs.LG", "cs.AI", "cs.DS", "cs.IT", "math.IT", "stat.ML"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00451v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00648", "title": "UMDATrack: Unified Multi-Domain Adaptive Tracking Under Adverse Weather Conditions", "authors": ["Siyuan Yao", "Rui Zhu", "Ziqi Wang", "Wenqi Ren", "Yanyang Yan", "Xiaochun Cao"], "summary": "Visual object tracking has gained promising progress in past decades. Most of\nthe existing approaches focus on learning target representation in\nwell-conditioned daytime data, while for the unconstrained real-world scenarios\nwith adverse weather conditions, e.g. nighttime or foggy environment, the\ntremendous domain shift leads to significant performance degradation. In this\npaper, we propose UMDATrack, which is capable of maintaining high-quality\ntarget state prediction under various adverse weather conditions within a\nunified domain adaptation framework. Specifically, we first use a controllable\nscenario generator to synthesize a small amount of unlabeled videos (less than\n2% frames in source daytime datasets) in multiple weather conditions under the\nguidance of different text prompts. Afterwards, we design a simple yet\neffective domain-customized adapter (DCA), allowing the target objects'\nrepresentation to rapidly adapt to various weather conditions without redundant\nmodel updating. Furthermore, to enhance the localization consistency between\nsource and target domains, we propose a target-aware confidence alignment\nmodule (TCA) following optimal transport theorem. Extensive experiments\ndemonstrate that UMDATrack can surpass existing advanced visual trackers and\nlead new state-of-the-art performance by a significant margin. Our code is\navailable at https://github.com/Z-Z188/UMDATrack.", "comment": "Accepted to ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.00648v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00648v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00425", "title": "Flexible Language Modeling in Continuous Space with Transformer-based Autoregressive Flows", "authors": ["Ruixiang Zhang", "Shuangfei Zhai", "Jiatao Gu", "Yizhe Zhang", "Huangjie Zheng", "Tianrong Chen", "Miguel Angel Bautista", "Josh Susskind", "Navdeep Jaitly"], "summary": "Autoregressive models have driven remarkable progress in language modeling.\nTheir foundational reliance on discrete tokens, unidirectional context, and\nsingle-pass decoding, while central to their success, also inspires the\nexploration of a design space that could offer new axes of modeling\nflexibility. In this work, we explore an alternative paradigm, shifting\nlanguage modeling from a discrete token space to a continuous latent space. We\npropose a novel framework TarFlowLM, that employs transformer-based\nautoregressive normalizing flows to model these continuous representations.\nThis approach unlocks substantial flexibility, enabling the construction of\nmodels that can capture global bi-directional context through stacked,\nalternating-direction autoregressive transformations, support block-wise\ngeneration with flexible token patch sizes, and facilitate a hierarchical\nmulti-pass generation process. We further propose new mixture-based coupling\ntransformations designed to capture complex dependencies within the latent\nspace shaped by discrete data, and demonstrate theoretical connections to\nconventional discrete autoregressive models. Extensive experiments on language\nmodeling benchmarks demonstrate strong likelihood performance and highlight the\nflexible modeling capabilities inherent in our framework.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00425v1", "categories": ["cs.LG", "cs.CL"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00425v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00094", "title": "Efficient Conformance Checking of Rich Data-Aware Declare Specifications (Extended)", "authors": ["Jacobo Casas-Ramos", "Sarah Winkler", "Alessandro Gianola", "Marco Montali", "Manuel Mucientes", "Manuel Lama"], "summary": "Despite growing interest in process analysis and mining for data-aware\nspecifications, alignment-based conformance checking for declarative process\nmodels has focused on pure control-flow specifications, or mild data-aware\nextensions limited to numerical data and variable-to-constant comparisons. This\nis not surprising: finding alignments is computationally hard, even more so in\nthe presence of data dependencies. In this paper, we challenge this problem in\nthe case where the reference model is captured using data-aware Declare with\ngeneral data types and data conditions. We show that, unexpectedly, it is\npossible to compute data-aware optimal alignments in this rich setting,\nenjoying at once efficiency and expressiveness. This is achieved by carefully\ncombining the two best-known approaches to deal with control flow and data\ndependencies when computing alignments, namely A* search and SMT solving.\nSpecifically, we introduce a novel algorithmic technique that efficiently\nexplores the search space, generating descendant states through the application\nof repair actions aiming at incrementally resolving constraint violations. We\nprove the correctness of our algorithm and experimentally show its efficiency.\nThe evaluation witnesses that our approach matches or surpasses the performance\nof the state of the art while also supporting significantly more expressive\ndata dependencies, showcasing its potential to support real-world applications.", "comment": "Extended version of the paper of the same title accepted at the 23rd\n  International Conference on Business Process Management (BPM 2025)", "pdf_url": "http://arxiv.org/pdf/2507.00094v1", "categories": ["cs.DB", "cs.AI", "cs.PL"], "cate": "cs.DB", "url": "http://arxiv.org/abs/2507.00094v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00453", "title": "Recurrent Memory-Augmented Transformers with Chunked Attention for Long-Context Language Modeling", "authors": ["Ankit Kashyap"], "summary": "We present a Transformer architecture for long-context language modeling that\ncombines global attention with two biologically inspired components: chunked\nlocal attention and a gated FIFO memory mechanism. This unified attention block\nallows the model to efficiently handle both short-range and long-range\ndependencies without increasing attention cost quadratically. The memory module\npersistently stores past token representations using a gated update mechanism\ninspired by recurrent networks. Rotary positional encoding is applied per\nattention head to enable directionally disentangled, scale-invariant positional\nsignals. The architecture is implemented entirely from scratch in PyTorch, with\nno reliance on high-level libraries, enabling transparent and modular\nexperimentation. Our model offers a lightweight and extensible design for tasks\nsuch as dialogue modeling, code completion, and document understanding.", "comment": "19 pages, 9 figures, 1 table; implemented entirely from scratch in\n  PyTorch", "pdf_url": "http://arxiv.org/pdf/2507.00453v1", "categories": ["cs.LG", "F.2.2; I.2.6; I.2.7"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00453v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00659", "title": "LoD-Loc v2: Aerial Visual Localization over Low Level-of-Detail City Models using Explicit Silhouette Alignment", "authors": ["Juelin Zhu", "Shuaibang Peng", "Long Wang", "Hanlin Tan", "Yu Liu", "Maojun Zhang", "Shen Yan"], "summary": "We propose a novel method for aerial visual localization over low\nLevel-of-Detail (LoD) city models. Previous wireframe-alignment-based method\nLoD-Loc has shown promising localization results leveraging LoD models.\nHowever, LoD-Loc mainly relies on high-LoD (LoD3 or LoD2) city models, but the\nmajority of available models and those many countries plan to construct\nnationwide are low-LoD (LoD1). Consequently, enabling localization on low-LoD\ncity models could unlock drones' potential for global urban localization. To\naddress these issues, we introduce LoD-Loc v2, which employs a coarse-to-fine\nstrategy using explicit silhouette alignment to achieve accurate localization\nover low-LoD city models in the air. Specifically, given a query image, LoD-Loc\nv2 first applies a building segmentation network to shape building silhouettes.\nThen, in the coarse pose selection stage, we construct a pose cost volume by\nuniformly sampling pose hypotheses around a prior pose to represent the pose\nprobability distribution. Each cost of the volume measures the degree of\nalignment between the projected and predicted silhouettes. We select the pose\nwith maximum value as the coarse pose. In the fine pose estimation stage, a\nparticle filtering method incorporating a multi-beam tracking approach is used\nto efficiently explore the hypothesis space and obtain the final pose\nestimation. To further facilitate research in this field, we release two\ndatasets with LoD1 city models covering 10.7 km , along with real RGB queries\nand ground-truth pose annotations. Experimental results show that LoD-Loc v2\nimproves estimation accuracy with high-LoD models and enables localization with\nlow-LoD models for the first time. Moreover, it outperforms state-of-the-art\nbaselines by large margins, even surpassing texture-model-based methods, and\nbroadens the convergence basin to accommodate larger prior errors.", "comment": "Accepted by ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.00659v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00659v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00432", "title": "Does Math Reasoning Improve General LLM Capabilities? Understanding Transferability of LLM Reasoning", "authors": ["Maggie Huan", "Yuetai Li", "Tuney Zheng", "Xiaoyu Xu", "Seungone Kim", "Minxin Du", "Radha Poovendran", "Graham Neubig", "Xiang Yue"], "summary": "Math reasoning has become the poster child of progress in large language\nmodels (LLMs), with new models rapidly surpassing human-level performance on\nbenchmarks like MATH and AIME. But as math leaderboards improve week by week,\nit is worth asking: do these gains reflect broader problem-solving ability or\njust narrow overfitting? To answer this question, we evaluate over 20\nopen-weight reasoning-tuned models across a broad suite of tasks, including\nmath, scientific QA, agent planning, coding, and standard\ninstruction-following. We surprisingly find that most models that succeed in\nmath fail to transfer their gains to other domains. To rigorously study this\nphenomenon, we conduct controlled experiments on Qwen3-14B models using\nmath-only data but different tuning methods. We find that reinforcement\nlearning (RL)-tuned models generalize well across domains, while supervised\nfine-tuning (SFT)-tuned models often forget general capabilities. Latent-space\nrepresentation and token-space distribution shift analyses reveal that SFT\ninduces substantial representation and output drift, while RL preserves\ngeneral-domain structure. Our results suggest a need to rethink standard\npost-training recipes, particularly the reliance on SFT-distilled data for\nadvancing reasoning models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00432v1", "categories": ["cs.AI", "cs.CL"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2507.00432v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00096", "title": "AI-Governed Agent Architecture for Web-Trustworthy Tokenization of Alternative Assets", "authors": ["Ailiya Borjigin", "Wei Zhou", "Cong He"], "summary": "Alternative Assets tokenization is transforming non-traditional financial\ninstruments are represented and traded on the web. However, ensuring\ntrustworthiness in web-based tokenized ecosystems poses significant challenges,\nfrom verifying off-chain asset data to enforcing regulatory compliance. This\npaper proposes an AI-governed agent architecture that integrates intelligent\nagents with blockchain to achieve web-trustworthy tokenization of alternative\nassets. In the proposed architecture, autonomous agents orchestrate the\ntokenization process (asset verification, valuation, compliance checking, and\nlifecycle management), while an AI-driven governance layer monitors agent\nbehavior and enforces trust through adaptive policies and cryptoeconomic\nincentives. We demonstrate that this approach enhances transparency, security,\nand compliance in asset tokenization, addressing key concerns around data\nauthenticity and fraud. A case study on tokenizing real estate assets\nillustrates how the architecture mitigates risks (e.g., fraudulent listings and\nmoney laundering) through real-time AI anomaly detection and on-chain\nenforcement. Our evaluation and analysis suggest that combining AI governance\nwith multi-agent systems and blockchain can significantly bolster trust in\ntokenized asset ecosystems. This work offers a novel framework for trustworthy\nasset tokenization on the web and provides insights for practitioners aiming to\ndeploy secure, compliant tokenization platforms.", "comment": "8 Pages, 1 figure", "pdf_url": "http://arxiv.org/pdf/2507.00096v1", "categories": ["cs.CR", "cs.AI"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2507.00096v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00467", "title": "Diversity Conscious Refined Random Forest", "authors": ["Sijan Bhattarai", "Saurav Bhandari", "Girija Bhusal", "Saroj Shakya", "Tapendra Pandey"], "summary": "Random Forest (RF) is a widely used ensemble learning technique known for its\nrobust classification performance across diverse domains. However, it often\nrelies on hundreds of trees and all input features, leading to high inference\ncost and model redundancy. In this work, our goal is to grow trees dynamically\nonly on informative features and then enforce maximal diversity by clustering\nand retaining uncorrelated trees. Therefore, we propose a Refined Random Forest\nClassifier that iteratively refines itself by first removing the least\ninformative features and then analytically determines how many new trees should\nbe grown, followed by correlation-based clustering to remove redundant trees.\nThe classification accuracy of our model was compared against the standard RF\non the same number of trees. Experiments on 8 multiple benchmark datasets,\nincluding binary and multiclass datasets, demonstrate that the proposed model\nachieves improved accuracy compared to standard RF.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00467v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00467v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00676", "title": "A Unified Transformer-Based Framework with Pretraining For Whole Body Grasping Motion Generation", "authors": ["Edward Effendy", "Kuan-Wei Tseng", "Rei Kawakami"], "summary": "Accepted in the ICIP 2025\n  We present a novel transformer-based framework for whole-body grasping that\naddresses both pose generation and motion infilling, enabling realistic and\nstable object interactions. Our pipeline comprises three stages: Grasp Pose\nGeneration for full-body grasp generation, Temporal Infilling for smooth motion\ncontinuity, and a LiftUp Transformer that refines downsampled joints back to\nhigh-resolution markers. To overcome the scarcity of hand-object interaction\ndata, we introduce a data-efficient Generalized Pretraining stage on large,\ndiverse motion datasets, yielding robust spatio-temporal representations\ntransferable to grasping tasks. Experiments on the GRAB dataset show that our\nmethod outperforms state-of-the-art baselines in terms of coherence, stability,\nand visual realism. The modular design also supports easy adaptation to other\nhuman-motion applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00676v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00676v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00449", "title": "Overcoming Long-Context Limitations of State-Space Models via Context-Dependent Sparse Attention", "authors": ["Zhihao Zhan", "Jianan Zhao", "Zhaocheng Zhu", "Jian Tang"], "summary": "Efficient long-context modeling remains a critical challenge for natural\nlanguage processing (NLP), as the time complexity of the predominant\nTransformer architecture scales quadratically with the sequence length. While\nstate-space models (SSMs) offer alternative sub-quadratic solutions, they\nstruggle to capture long-range dependencies effectively. In this work, we focus\non analyzing and improving the long-context modeling capabilities of SSMs. We\nshow that the widely used synthetic task, associative recall, which requires a\nmodel to recall a value associated with a single key without context,\ninsufficiently represents the complexities of real-world long-context modeling.\nTo address this limitation, we extend the associative recall to a novel\nsynthetic task, \\emph{joint recall}, which requires a model to recall the value\nassociated with a key given in a specified context. Theoretically, we prove\nthat SSMs do not have the expressiveness to solve multi-query joint recall in\nsub-quadratic time complexity. To resolve this issue, we propose a solution\nbased on integrating SSMs with Context-Dependent Sparse Attention (CDSA), which\nhas the expressiveness to solve multi-query joint recall with sub-quadratic\ncomputation. To bridge the gap between theoretical analysis and real-world\napplications, we propose locality-sensitive Hashing Attention with sparse Key\nSelection (HAX), which instantiates the theoretical solution and is further\ntailored to natural language domains. Extensive experiments on both synthetic\nand real-world long-context benchmarks show that HAX consistently outperforms\nSSM baselines and SSMs integrated with context-independent sparse attention\n(CISA).", "comment": "Proceedings of the 42nd International Conference on Machine Learning,\n  ES-FoMo III: 3rd Workshop on Efficient Systems for Foundation Models, 18\n  pages, 9 figures", "pdf_url": "http://arxiv.org/pdf/2507.00449v1", "categories": ["cs.LG", "cs.CL", "I.2.7"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00449v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00102", "title": "Towards transparent and data-driven fault detection in manufacturing: A case study on univariate, discrete time series", "authors": ["Bernd Hofmann", "Patrick Bruendl", "Huong Giang Nguyen", "Joerg Franke"], "summary": "Ensuring consistent product quality in modern manufacturing is crucial,\nparticularly in safety-critical applications. Conventional quality control\napproaches, reliant on manually defined thresholds and features, lack\nadaptability to the complexity and variability inherent in production data and\nnecessitate extensive domain expertise. Conversely, data-driven methods, such\nas machine learning, demonstrate high detection performance but typically\nfunction as black-box models, thereby limiting their acceptance in industrial\nenvironments where interpretability is paramount. This paper introduces a\nmethodology for industrial fault detection, which is both data-driven and\ntransparent. The approach integrates a supervised machine learning model for\nmulti-class fault classification, Shapley Additive Explanations for post-hoc\ninterpretability, and a do-main-specific visualisation technique that maps\nmodel explanations to operator-interpretable features. Furthermore, the study\nproposes an evaluation methodology that assesses model explanations through\nquantitative perturbation analysis and evaluates visualisations by qualitative\nexpert assessment. The approach was applied to the crimping process, a\nsafety-critical joining technique, using a dataset of univariate, discrete time\nseries. The system achieves a fault detection accuracy of 95.9 %, and both\nquantitative selectivity analysis and qualitative expert evaluations confirmed\nthe relevance and inter-pretability of the generated explanations. This\nhuman-centric approach is designed to enhance trust and interpretability in\ndata-driven fault detection, thereby contributing to applied system design in\nindustrial quality control.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00102v1", "categories": ["cs.LG", "cs.AI", "eess.SP"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00102v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00480", "title": "Posterior Inference in Latent Space for Scalable Constrained Black-box Optimization", "authors": ["Kiyoung Om", "Kyuil Sim", "Taeyoung Yun", "Hyeongyu Kang", "Jinkyoo Park"], "summary": "Optimizing high-dimensional black-box functions under black-box constraints\nis a pervasive task in a wide range of scientific and engineering problems.\nThese problems are typically harder than unconstrained problems due to\nhard-to-find feasible regions. While Bayesian optimization (BO) methods have\nbeen developed to solve such problems, they often struggle with the curse of\ndimensionality. Recently, generative model-based approaches have emerged as a\npromising alternative for constrained optimization. However, they suffer from\npoor scalability and are vulnerable to mode collapse, particularly when the\ntarget distribution is highly multi-modal. In this paper, we propose a new\nframework to overcome these challenges. Our method iterates through two stages.\nFirst, we train flow-based models to capture the data distribution and\nsurrogate models that predict both function values and constraint violations\nwith uncertainty quantification. Second, we cast the candidate selection\nproblem as a posterior inference problem to effectively search for promising\ncandidates that have high objective values while not violating the constraints.\nDuring posterior inference, we find that the posterior distribution is highly\nmulti-modal and has a large plateau due to constraints, especially when\nconstraint feedback is given as binary indicators of feasibility. To mitigate\nthis issue, we amortize the sampling from the posterior distribution in the\nlatent space of flow-based models, which is much smoother than that in the data\nspace. We empirically demonstrate that our method achieves superior performance\non various synthetic and real-world constrained black-box optimization tasks.\nOur code is publicly available \\href{https://github.com/umkiyoung/CiBO}{here}.", "comment": "25 pages, 11 figures, 5 tables. Equal contribution by Kiyoung Om,\n  Kyuil Sim, and Taeyoung Yun", "pdf_url": "http://arxiv.org/pdf/2507.00480v1", "categories": ["cs.LG", "stat.ML"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00480v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00690", "title": "Cage-Based Deformation for Transferable and Undefendable Point Cloud Attack", "authors": ["Keke Tang", "Ziyong Du", "Weilong Peng", "Xiaofei Wang", "Peican Zhu", "Ligang Liu", "Zhihong Tian"], "summary": "Adversarial attacks on point clouds often impose strict geometric constraints\nto preserve plausibility; however, such constraints inherently limit\ntransferability and undefendability. While deformation offers an alternative,\nexisting unstructured approaches may introduce unnatural distortions, making\nadversarial point clouds conspicuous and undermining their plausibility. In\nthis paper, we propose CageAttack, a cage-based deformation framework that\nproduces natural adversarial point clouds. It first constructs a cage around\nthe target object, providing a structured basis for smooth, natural-looking\ndeformation. Perturbations are then applied to the cage vertices, which\nseamlessly propagate to the point cloud, ensuring that the resulting\ndeformations remain intrinsic to the object and preserve plausibility.\nExtensive experiments on seven 3D deep neural network classifiers across three\ndatasets show that CageAttack achieves a superior balance among\ntransferability, undefendability, and plausibility, outperforming\nstate-of-the-art methods. Codes will be made public upon acceptance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00690v1", "categories": ["cs.CV", "cs.CR"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00690v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00466", "title": "Beat and Downbeat Tracking in Performance MIDI Using an End-to-End Transformer Architecture", "authors": ["Sebastian Murgul", "Michael Heizmann"], "summary": "Beat tracking in musical performance MIDI is a challenging and important task\nfor notation-level music transcription and rhythmical analysis, yet existing\nmethods primarily focus on audio-based approaches. This paper proposes an\nend-to-end transformer-based model for beat and downbeat tracking in\nperformance MIDI, leveraging an encoder-decoder architecture for\nsequence-to-sequence translation of MIDI input to beat annotations. Our\napproach introduces novel data preprocessing techniques, including dynamic\naugmentation and optimized tokenization strategies, to improve accuracy and\ngeneralizability across different datasets. We conduct extensive experiments\nusing the A-MAPS, ASAP, GuitarSet, and Leduc datasets, comparing our model\nagainst state-of-the-art hidden Markov models (HMMs) and deep learning-based\nbeat tracking methods. The results demonstrate that our model outperforms\nexisting symbolic music beat tracking approaches, achieving competitive\nF1-scores across various musical styles and instruments. Our findings highlight\nthe potential of transformer architectures for symbolic beat tracking and\nsuggest future integration with automatic music transcription systems for\nenhanced music analysis and score generation.", "comment": "Accepted to the 22nd Sound and Music Computing Conference (SMC), 2025", "pdf_url": "http://arxiv.org/pdf/2507.00466v1", "categories": ["cs.SD", "cs.CL", "cs.MM", "eess.AS"], "cate": "cs.SD", "url": "http://arxiv.org/abs/2507.00466v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00108", "title": "Teaching Programming in the Age of Generative AI: Insights from Literature, Pedagogical Proposals, and Student Perspectives", "authors": ["Clemente Rubio-Manzano", "Jazna Meza", "Rodolfo Fernandez-Santibanez", "Christian Vidal-Castro"], "summary": "Computer programming is undergoing a true transformation driven by powerful\nnew tools for automatic source code generation based on large language models.\nThis transformation is also manifesting in introductory programming courses at\nuniversities around the world, generating an in-depth debate about how\nprogramming content should be taught, learned, and assessed in the context of\ngenerative artificial intelligence.\n  This article aims, on the one hand, to review the most relevant studies on\nthis issue, highlighting the advantages and disadvantages identified in the\nspecialized literature. On the other hand, it proposes enriching teaching and\nlearning methodologies by focusing on code comprehension and execution rather\nthan on mere coding or program functionality. In particular, it advocates for\nthe use of visual representations of code and visual simulations of its\nexecution as effective tools for teaching, learning, and assessing programming,\nthus fostering a deeper understanding among students.\n  Finally, the opinions of students who took the object-oriented programming\ncourse are presented to provide preliminary context supporting the\nincorporation of visual simulations in Java (or other languages) as part of the\ntraining process.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00108v1", "categories": ["cs.CY", "cs.AI", "cs.ET", "cs.PL"], "cate": "cs.CY", "url": "http://arxiv.org/abs/2507.00108v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00485", "title": "PNAct: Crafting Backdoor Attacks in Safe Reinforcement Learning", "authors": ["Weiran Guo", "Guanjun Liu", "Ziyuan Zhou", "Ling Wang"], "summary": "Reinforcement Learning (RL) is widely used in tasks where agents interact\nwith an environment to maximize rewards. Building on this foundation, Safe\nReinforcement Learning (Safe RL) incorporates a cost metric alongside the\nreward metric, ensuring that agents adhere to safety constraints during\ndecision-making. In this paper, we identify that Safe RL is vulnerable to\nbackdoor attacks, which can manipulate agents into performing unsafe actions.\nFirst, we introduce the relevant concepts and evaluation metrics for backdoor\nattacks in Safe RL. It is the first attack framework in the Safe RL field that\ninvolves both Positive and Negative Action sample (PNAct) is to implant\nbackdoors, where positive action samples provide reference actions and negative\naction samples indicate actions to be avoided. We theoretically point out the\nproperties of PNAct and design an attack algorithm. Finally, we conduct\nexperiments to evaluate the effectiveness of our proposed backdoor attack\nframework, evaluating it with the established metrics. This paper highlights\nthe potential risks associated with Safe RL and underscores the feasibility of\nsuch attacks. Our code and supplementary material are available at\nhttps://github.com/azure-123/PNAct.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00485v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00485v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00698", "title": "Rectifying Magnitude Neglect in Linear Attention", "authors": ["Qihang Fan", "Huaibo Huang", "Yuang Ai", "ran He"], "summary": "As the core operator of Transformers, Softmax Attention exhibits excellent\nglobal modeling capabilities. However, its quadratic complexity limits its\napplicability to vision tasks. In contrast, Linear Attention shares a similar\nformulation with Softmax Attention while achieving linear complexity, enabling\nefficient global information modeling. Nevertheless, Linear Attention suffers\nfrom a significant performance degradation compared to standard Softmax\nAttention. In this paper, we analyze the underlying causes of this issue based\non the formulation of Linear Attention. We find that, unlike Softmax Attention,\nLinear Attention entirely disregards the magnitude information of the Query.\nThis prevents the attention score distribution from dynamically adapting as the\nQuery scales. As a result, despite its structural similarity to Softmax\nAttention, Linear Attention exhibits a significantly different attention score\ndistribution. Based on this observation, we propose Magnitude-Aware Linear\nAttention (MALA), which modifies the computation of Linear Attention to fully\nincorporate the Query's magnitude. This adjustment allows MALA to generate an\nattention score distribution that closely resembles Softmax Attention while\nexhibiting a more well-balanced structure. We evaluate the effectiveness of\nMALA on multiple tasks, including image classification, object detection,\ninstance segmentation, semantic segmentation, natural language processing,\nspeech recognition, and image generation. Our MALA achieves strong results on\nall of these tasks. Code will be available at https://github.com/qhfan/MALA", "comment": "Accepted by ICCV2025", "pdf_url": "http://arxiv.org/pdf/2507.00698v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00698v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00487", "title": "MassTool: A Multi-Task Search-Based Tool Retrieval Framework for Large Language Models", "authors": ["Jianghao Lin", "Xinyuan Wang", "Xinyi Dai", "Menghui Zhu", "Bo Chen", "Ruiming Tang", "Yong Yu", "Weinan Zhang"], "summary": "Tool retrieval is a critical component in enabling large language models\n(LLMs) to interact effectively with external tools. It aims to precisely filter\nthe massive tools into a small set of candidates for the downstream\ntool-augmented LLMs. However, most existing approaches primarily focus on\noptimizing tool representations, often neglecting the importance of precise\nquery comprehension. To address this gap, we introduce MassTool, a multi-task\nsearch-based framework designed to enhance both query representation and tool\nretrieval accuracy. MassTool employs a two-tower architecture: a tool usage\ndetection tower that predicts the need for function calls, and a tool retrieval\ntower that leverages a query-centric graph convolution network (QC-GCN) for\neffective query-tool matching. It also incorporates search-based user intent\nmodeling (SUIM) to handle diverse and out-of-distribution queries, alongside an\nadaptive knowledge transfer (AdaKT) module for efficient multi-task learning.\nBy jointly optimizing tool usage detection loss, list-wise retrieval loss, and\ncontrastive regularization loss, MassTool establishes a robust dual-step\nsequential decision-making pipeline for precise query understanding. Extensive\nexperiments demonstrate its effectiveness in improving retrieval accuracy. Our\ncode is available at https://github.com/wxydada/MassTool.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00487v1", "categories": ["cs.IR", "cs.CL"], "cate": "cs.IR", "url": "http://arxiv.org/abs/2507.00487v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00145", "title": "AI-Hybrid TRNG: Kernel-Based Deep Learning for Near-Uniform Entropy Harvesting from Physical Noise", "authors": ["Hasan Yiğit"], "summary": "AI-Hybrid TRNG is a deep-learning framework that extracts near-uniform\nentropy directly from physical noise, eliminating the need for bulky quantum\ndevices or expensive laboratory-grade RF receivers. Instead, it relies on a\nlow-cost, thumb-sized RF front end, plus CPU-timing jitter, for training, and\nthen emits 32-bit high-entropy streams without any quantization step.\n  Unlike deterministic or trained artificial intelligence random number\ngenerators (RNGs), our dynamic inner-outer network couples adaptive natural\nsources and reseeding, yielding truly unpredictable and autonomous sequences.\nGenerated numbers pass the NIST SP 800-22 battery better than a CPU-based\nmethod. It also passes nineteen bespoke statistical tests for both bit- and\ninteger-level analysis. All results satisfy cryptographic standards, while\nforward and backward prediction experiments reveal no exploitable biases. The\nmodel's footprint is below 0.5 MB, making it deployable on MCUs and FPGA soft\ncores, as well as suitable for other resource-constrained platforms.\n  By detaching randomness quality from dedicated hardware, AI-Hybrid TRNG\nbroadens the reach of high-integrity random number generators across secure\nsystems, cryptographic protocols, embedded and edge devices, stochastic\nsimulations, and server applications that need randomness.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00145v1", "categories": ["cs.CR", "cs.AI", "cs.ET", "cs.IT", "eess.SP", "math.IT"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2507.00145v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00518", "title": "Exploring Large Action Sets with Hyperspherical Embeddings using von Mises-Fisher Sampling", "authors": ["Walid Bendada", "Guillaume Salha-Galvan", "Romain Hennequin", "Théo Bontempelli", "Thomas Bouabça", "Tristan Cazenave"], "summary": "This paper introduces von Mises-Fisher exploration (vMF-exp), a scalable\nmethod for exploring large action sets in reinforcement learning problems where\nhyperspherical embedding vectors represent these actions. vMF-exp involves\ninitially sampling a state embedding representation using a von Mises-Fisher\ndistribution, then exploring this representation's nearest neighbors, which\nscales to virtually unlimited numbers of candidate actions. We show that, under\ntheoretical assumptions, vMF-exp asymptotically maintains the same probability\nof exploring each action as Boltzmann Exploration (B-exp), a popular\nalternative that, nonetheless, suffers from scalability issues as it requires\ncomputing softmax values for each action. Consequently, vMF-exp serves as a\nscalable alternative to B-exp for exploring large action sets with\nhyperspherical embeddings. Experiments on simulated data, real-world public\ndata, and the successful large-scale deployment of vMF-exp on the recommender\nsystem of a global music streaming service empirically validate the key\nproperties of the proposed method.", "comment": "42nd International Conference on Machine Learning (ICML 2025)", "pdf_url": "http://arxiv.org/pdf/2507.00518v1", "categories": ["cs.LG", "cs.IR"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00518v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00707", "title": "BEV-VAE: Multi-view Image Generation with Spatial Consistency for Autonomous Driving", "authors": ["Zeming Chen", "Hang Zhao"], "summary": "Multi-view image generation in autonomous driving demands consistent 3D scene\nunderstanding across camera views. Most existing methods treat this problem as\na 2D image set generation task, lacking explicit 3D modeling. However, we argue\nthat a structured representation is crucial for scene generation, especially\nfor autonomous driving applications. This paper proposes BEV-VAE for consistent\nand controllable view synthesis. BEV-VAE first trains a multi-view image\nvariational autoencoder for a compact and unified BEV latent space and then\ngenerates the scene with a latent diffusion transformer. BEV-VAE supports\narbitrary view generation given camera configurations, and optionally 3D\nlayouts. Experiments on nuScenes and Argoverse 2 (AV2) show strong performance\nin both 3D consistent reconstruction and generation. The code is available at:\nhttps://github.com/Czm369/bev-vae.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00707v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00707v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00693", "title": "Leveraging Large Language Models for Spontaneous Speech-Based Suicide Risk Detection", "authors": ["Yifan Gao", "Jiao Fu", "Long Guo", "Hong Liu"], "summary": "Early identification of suicide risk is crucial for preventing suicidal\nbehaviors. As a result, the identification and study of patterns and markers\nrelated to suicide risk have become a key focus of current research. In this\npaper, we present the results of our work in the 1st SpeechWellness Challenge\n(SW1), which aims to explore speech as a non-invasive and easily accessible\nmental health indicator for identifying adolescents at risk of suicide.Our\napproach leverages large language model (LLM) as the primary tool for feature\nextraction, alongside conventional acoustic and semantic features. The proposed\nmethod achieves an accuracy of 74\\% on the test set, ranking first in the SW1\nchallenge. These findings demonstrate the potential of LLM-based methods for\nanalyzing speech in the context of suicide risk assessment.", "comment": "Accepted to Interspeech 2025", "pdf_url": "http://arxiv.org/pdf/2507.00693v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "cate": "cs.SD", "url": "http://arxiv.org/abs/2507.00693v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00161", "title": "Designing an Adaptive Storytelling Platform to Promote Civic Education in Politically Polarized Learning Environments", "authors": ["Christopher M. Wegemer", "Edward Halim", "Jeff Burke"], "summary": "Political polarization undermines democratic civic education by exacerbating\nidentity-based resistance to opposing viewpoints. Emerging AI technologies\noffer new opportunities to advance interventions that reduce polarization and\npromote political open-mindedness. We examined novel design strategies that\nleverage adaptive and emotionally-responsive civic narratives that may sustain\nstudents' emotional engagement in stories, and in turn, promote\nperspective-taking toward members of political out-groups. Drawing on theories\nfrom political psychology and narratology, we investigate how affective\ncomputing techniques can support three storytelling mechanisms: transportation\ninto a story world, identification with characters, and interaction with the\nstoryteller. Using a design-based research (DBR) approach, we iteratively\ndeveloped and refined an AI-mediated Digital Civic Storytelling (AI-DCS)\nplatform. Our prototype integrates facial emotion recognition and attention\ntracking to assess users' affective and attentional states in real time.\nNarrative content is organized around pre-structured story outlines, with\nbeat-by-beat language adaptation implemented via GPT-4, personalizing\nlinguistic tone to sustain students' emotional engagement in stories that\ncenter political perspectives different from their own. Our work offers a\nfoundation for AI-supported, emotionally-sensitive strategies that address\naffective polarization while preserving learner autonomy. We conclude with\nimplications for civic education interventions, algorithmic literacy, and HCI\nchallenges associated with AI dialogue management and affect-adaptive learning\nenvironments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00161v1", "categories": ["cs.HC", "cs.AI"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2507.00161v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00574", "title": "Foundation Models for Clinical Records at Health System Scale", "authors": ["Haresh Rengaraj Rajamohan", "Xiang Gao", "Weicheng Zhu", "Shih-Lun Huang", "Long Chen", "Kyunghyun Cho", "Cem M. Deniz", "Narges Razavian"], "summary": "Large-scale pretraining has transformed modeling of language and other data\ntypes, but its potential remains underexplored in healthcare with structured\nelectronic health records (EHRs). We present a novel generative pretraining\nstrategy for sequential EHR data using next-visit event prediction. Our model\nlearns to autoregressively generate various tokenized clinical events for the\nnext visit based on patient history and inherently handles the joint prediction\nof heterogeneous data types. Additionally, we introduce regularization on\npredicting repeated events and highlight a key pitfall in EHR-based foundation\nmodel evaluations: repeated event tokens can inflate performance metrics when\nnew onsets are not distinguished from subsequent occurrences. Our model is\nevaluated via zero-shot prediction for forecasting dementia and knee\nosteoarthritis incidence within 2 and 5 years, and the model performance rivals\na fully fine-tuned masked pretrained Transformer baseline, demonstrating that\nour approach captures complex clinical dependencies without requiring costly\ntask-specific fine-tuning.", "comment": "Accepted to ICML 2025 Workshop on Foundation Models for Structured\n  Data", "pdf_url": "http://arxiv.org/pdf/2507.00574v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00574v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00709", "title": "TopoStreamer: Temporal Lane Segment Topology Reasoning in Autonomous Driving", "authors": ["Yiming Yang", "Yueru Luo", "Bingkun He", "Hongbin Lin", "Suzhong Fu", "Chao Yan", "Kun Tang", "Xinrui Yan", "Chao Zheng", "Shuguang Cui", "Zhen Li"], "summary": "Lane segment topology reasoning constructs a comprehensive road network by\ncapturing the topological relationships between lane segments and their\nsemantic types. This enables end-to-end autonomous driving systems to perform\nroad-dependent maneuvers such as turning and lane changing. However, the\nlimitations in consistent positional embedding and temporal multiple attribute\nlearning in existing methods hinder accurate roadnet reconstruction. To address\nthese issues, we propose TopoStreamer, an end-to-end temporal perception model\nfor lane segment topology reasoning. Specifically, TopoStreamer introduces\nthree key improvements: streaming attribute constraints, dynamic lane boundary\npositional encoding, and lane segment denoising. The streaming attribute\nconstraints enforce temporal consistency in both centerline and boundary\ncoordinates, along with their classifications. Meanwhile, dynamic lane boundary\npositional encoding enhances the learning of up-to-date positional information\nwithin queries, while lane segment denoising helps capture diverse lane segment\npatterns, ultimately improving model performance. Additionally, we assess the\naccuracy of existing models using a lane boundary classification metric, which\nserves as a crucial measure for lane-changing scenarios in autonomous driving.\nOn the OpenLane-V2 dataset, TopoStreamer demonstrates significant improvements\nover state-of-the-art methods, achieving substantial performance gains of +3.4%\nmAP in lane segment perception and +2.1% OLS in centerline perception tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00709v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00709v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00740", "title": "Safe Low Bandwidth SPV: A Formal Treatment of Simplified Payment Verification Protocols and Security Bounds", "authors": ["Craig S Wright"], "summary": "This paper presents a complete formal specification, protocol description,\nand mathematical proof structure for Simplified Payment Verification (SPV) as\noriginally defined in the Bitcoin whitepaper \\cite{nakamoto2008}. In stark\ncontrast to the misrepresentations proliferated by popular implementations, we\nshow that SPV is not only secure under bounded adversarial assumptions but\nstrictly optimal for digital cash systems requiring scalable and verifiable\ntransaction inclusion. We reconstruct the SPV protocol from first principles,\ngrounding its verification model in symbolic automata, Merkle membership\nrelations, and chain-of-proof dominance predicates. Through rigorous\nprobabilistic and game-theoretic analysis, we derive the economic bounds within\nwhich the protocol operates securely and verify its liveness and safety\nproperties under partial connectivity, hostile relay networks, and adversarial\npropagation delay. Our specification further introduces low-bandwidth\noptimisations such as adaptive polling and compressed header synchronisation\nwhile preserving correctness. This document serves both as a blueprint for\nsecure SPV implementation and a rebuttal of common misconceptions surrounding\nnon-validating clients.", "comment": "56 pages 5 images", "pdf_url": "http://arxiv.org/pdf/2507.00740v1", "categories": ["cs.CR", "cs.CL", "cs.DC", "68Q85, 68M10, 94A60, 91A80, 68Q17, 68W10, 68R10", "C.2.2; F.2.2; D.4.6; K.6.5"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2507.00740v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00184", "title": "Text-to-Level Diffusion Models With Various Text Encoders for Super Mario Bros", "authors": ["Jacob Schrum", "Olivia Kilday", "Emilio Salas", "Bess Hagan", "Reid Williams"], "summary": "Recent research shows how diffusion models can unconditionally generate\ntile-based game levels, but use of diffusion models for text-to-level\ngeneration is underexplored. There are practical considerations for creating a\nusable model: caption/level pairs are needed, as is a text embedding model, and\na way of generating entire playable levels, rather than individual scenes. We\npresent strategies to automatically assign descriptive captions to an existing\nlevel dataset, and train diffusion models using both pretrained text encoders\nand simple transformer models trained from scratch. Captions are automatically\nassigned to generated levels so that the degree of overlap between input and\noutput captions can be compared. We also assess the diversity and playability\nof the resulting levels. Results are compared with an unconditional diffusion\nmodel and a generative adversarial network, as well as the text-to-level\napproaches Five-Dollar Model and MarioGPT. Notably, the best diffusion model\nuses a simple transformer model for text embedding, and takes less time to\ntrain than diffusion models employing more complex text encoders, indicating\nthat reliance on larger language models is not necessary. We also present a GUI\nallowing designers to construct long levels from model-generated scenes.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00184v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00184v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00589", "title": "Quantum Circuit Structure Optimization for Quantum Reinforcement Learning", "authors": ["Seok Bin Son", "Joongheon Kim"], "summary": "Reinforcement learning (RL) enables agents to learn optimal policies through\nenvironmental interaction. However, RL suffers from reduced learning efficiency\ndue to the curse of dimensionality in high-dimensional spaces. Quantum\nreinforcement learning (QRL) addresses this issue by leveraging superposition\nand entanglement in quantum computing, allowing efficient handling of\nhigh-dimensional problems with fewer resources. QRL combines quantum neural\nnetworks (QNNs) with RL, where the parameterized quantum circuit (PQC) acts as\nthe core computational module. The PQC performs linear and nonlinear\ntransformations through gate operations, similar to hidden layers in classical\nneural networks. Previous QRL studies, however, have used fixed PQC structures\nbased on empirical intuition without verifying their optimality. This paper\nproposes a QRL-NAS algorithm that integrates quantum neural architecture search\n(QNAS) to optimize PQC structures within QRL. Experiments demonstrate that\nQRL-NAS achieves higher rewards than QRL with fixed circuits, validating its\neffectiveness and practical utility.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00589v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00589v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00721", "title": "UPRE: Zero-Shot Domain Adaptation for Object Detection via Unified Prompt and Representation Enhancement", "authors": ["Xiao Zhang", "Fei Wei", "Yong Wang", "Wenda Zhao", "Feiyi Li", "Xiangxiang Chu"], "summary": "Zero-shot domain adaptation (ZSDA) presents substantial challenges due to the\nlack of images in the target domain. Previous approaches leverage\nVision-Language Models (VLMs) to tackle this challenge, exploiting their\nzero-shot learning capabilities. However, these methods primarily address\ndomain distribution shifts and overlook the misalignment between the detection\ntask and VLMs, which rely on manually crafted prompts. To overcome these\nlimitations, we propose the unified prompt and representation enhancement\n(UPRE) framework, which jointly optimizes both textual prompts and visual\nrepresentations. Specifically, our approach introduces a multi-view domain\nprompt that combines linguistic domain priors with detection-specific\nknowledge, and a visual representation enhancement module that produces domain\nstyle variations. Furthermore, we introduce multi-level enhancement strategies,\nincluding relative domain distance and positive-negative separation, which\nalign multi-modal representations at the image level and capture diverse visual\nrepresentations at the instance level, respectively. Extensive experiments\nconducted on nine benchmark datasets demonstrate the superior performance of\nour framework in ZSDA detection scenarios. Code is available at\nhttps://github.com/AMAP-ML/UPRE.", "comment": "ICCV2025", "pdf_url": "http://arxiv.org/pdf/2507.00721v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00721v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00808", "title": "Multi-interaction TTS toward professional recording reproduction", "authors": ["Hiroki Kanagawa", "Kenichi Fujita", "Aya Watanabe", "Yusuke Ijima"], "summary": "Voice directors often iteratively refine voice actors' performances by\nproviding feedback to achieve the desired outcome. While this iterative\nfeedback-based refinement process is important in actual recordings, it has\nbeen overlooked in text-to-speech synthesis (TTS). As a result, fine-grained\nstyle refinement after the initial synthesis is not possible, even though the\nsynthesized speech often deviates from the user's intended style. To address\nthis issue, we propose a TTS method with multi-step interaction that allows\nusers to intuitively and rapidly refine synthetized speech. Our approach models\nthe interaction between the TTS model and its user to emulate the relationship\nbetween voice actors and voice directors. Experiments show that the proposed\nmodel with its corresponding dataset enable iterative style refinements in\naccordance with users' directions, thus demonstrating its multi-interaction\ncapability. Sample audios are available: https://ntt-hilab-gensp.\ngithub.io/ssw13multiinteraction_tts/", "comment": "7 pages,6 figures, Accepted to Speech Synthesis Workshop 2025 (SSW13)", "pdf_url": "http://arxiv.org/pdf/2507.00808v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "cate": "cs.SD", "url": "http://arxiv.org/abs/2507.00808v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00185", "title": "Multimodal, Multi-Disease Medical Imaging Foundation Model (MerMED-FM)", "authors": ["Yang Zhou", "Chrystie Wan Ning Quek", "Jun Zhou", "Yan Wang", "Yang Bai", "Yuhe Ke", "Jie Yao", "Laura Gutierrez", "Zhen Ling Teo", "Darren Shu Jeng Ting", "Brian T. Soetikno", "Christopher S. Nielsen", "Tobias Elze", "Zengxiang Li", "Linh Le Dinh", "Lionel Tim-Ee Cheng", "Tran Nguyen Tuan Anh", "Chee Leong Cheng", "Tien Yin Wong", "Nan Liu", "Iain Beehuat Tan", "Tony Kiat Hon Lim", "Rick Siow Mong Goh", "Yong Liu", "Daniel Shu Wei Ting"], "summary": "Current artificial intelligence models for medical imaging are predominantly\nsingle modality and single disease. Attempts to create multimodal and\nmulti-disease models have resulted in inconsistent clinical accuracy.\nFurthermore, training these models typically requires large, labour-intensive,\nwell-labelled datasets. We developed MerMED-FM, a state-of-the-art multimodal,\nmulti-specialty foundation model trained using self-supervised learning and a\nmemory module. MerMED-FM was trained on 3.3 million medical images from over\nten specialties and seven modalities, including computed tomography (CT), chest\nX-rays (CXR), ultrasound (US), pathology patches, color fundus photography\n(CFP), optical coherence tomography (OCT) and dermatology images. MerMED-FM was\nevaluated across multiple diseases and compared against existing foundational\nmodels. Strong performance was achieved across all modalities, with AUROCs of\n0.988 (OCT); 0.982 (pathology); 0.951 (US); 0.943 (CT); 0.931 (skin); 0.894\n(CFP); 0.858 (CXR). MerMED-FM has the potential to be a highly adaptable,\nversatile, cross-specialty foundation model that enables robust medical imaging\ninterpretation across diverse medical disciplines.", "comment": "42 pages, 3 composite figures, 4 tables", "pdf_url": "http://arxiv.org/pdf/2507.00185v1", "categories": ["eess.IV", "cs.AI", "cs.CV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2507.00185v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00611", "title": "Residual Reward Models for Preference-based Reinforcement Learning", "authors": ["Chenyang Cao", "Miguel Rogel-García", "Mohamed Nabail", "Xueqian Wang", "Nicholas Rhinehart"], "summary": "Preference-based Reinforcement Learning (PbRL) provides a way to learn\nhigh-performance policies in environments where the reward signal is hard to\nspecify, avoiding heuristic and time-consuming reward design. However, PbRL can\nsuffer from slow convergence speed since it requires training in a reward\nmodel. Prior work has proposed learning a reward model from demonstrations and\nfine-tuning it using preferences. However, when the model is a neural network,\nusing different loss functions for pre-training and fine-tuning can pose\nchallenges to reliable optimization. In this paper, we propose a method to\neffectively leverage prior knowledge with a Residual Reward Model (RRM). An RRM\nassumes that the true reward of the environment can be split into a sum of two\nparts: a prior reward and a learned reward. The prior reward is a term\navailable before training, for example, a user's ``best guess'' reward\nfunction, or a reward function learned from inverse reinforcement learning\n(IRL), and the learned reward is trained with preferences. We introduce\nstate-based and image-based versions of RRM and evaluate them on several tasks\nin the Meta-World environment suite. Experimental results show that our method\nsubstantially improves the performance of a common PbRL method. Our method\nachieves performance improvements for a variety of different types of prior\nrewards, including proxy rewards, a reward obtained from IRL, and even a\nnegated version of the proxy reward. We also conduct experiments with a Franka\nPanda to show that our method leads to superior performance on a real robot. It\nsignificantly accelerates policy learning for different tasks, achieving\nsuccess in fewer steps than the baseline. The videos are presented at\nhttps://sunlighted.github.io/RRM-web/.", "comment": "26 pages, 22 figures", "pdf_url": "http://arxiv.org/pdf/2507.00611v1", "categories": ["cs.LG", "cs.AI", "cs.RO"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00611v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00724", "title": "Holmes: Towards Effective and Harmless Model Ownership Verification to Personalized Large Vision Models via Decoupling Common Features", "authors": ["Linghui Zhu", "Yiming Li", "Haiqin Weng", "Yan Liu", "Tianwei Zhang", "Shu-Tao Xia", "Zhi Wang"], "summary": "Large vision models achieve remarkable performance in various downstream\ntasks, primarily by personalizing pre-trained models through fine-tuning with\nprivate and valuable local data, which makes the personalized model a valuable\nintellectual property for its owner. Similar to the era of traditional DNNs,\nmodel stealing attacks also pose significant risks to these personalized\nmodels. However, in this paper, we reveal that most existing defense methods\n(developed for traditional DNNs), typically designed for models trained from\nscratch, either introduce additional security risks, are prone to misjudgment,\nor are even ineffective for fine-tuned models. To alleviate these problems,\nthis paper proposes a harmless model ownership verification method for\npersonalized models by decoupling similar common features. In general, our\nmethod consists of three main stages. In the first stage, we create shadow\nmodels that retain common features of the victim model while disrupting\ndataset-specific features. We represent the dataset-specific features of the\nvictim model by the output differences between the shadow and victim models.\nAfter that, a meta-classifier is trained to identify stolen models by\ndetermining whether suspicious models contain the dataset-specific features of\nthe victim. In the third stage, we conduct model ownership verification by\nhypothesis test to mitigate randomness and enhance robustness. Extensive\nexperiments on benchmark datasets verify the effectiveness of the proposed\nmethod in detecting different types of model stealing simultaneously.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00724v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00724v1", "date": "2025-06-24", "updated": "2025-06-24"}
{"id": "2507.00877", "title": "Verifiable Natural Language to Linear Temporal Logic Translation: A Benchmark Dataset and Evaluation Suite", "authors": ["William H English", "Chase Walker", "Dominic Simon", "Sumit Kumar Jha", "Rickard Ewetz"], "summary": "Empirical evaluation of state-of-the-art natural-language (NL) to\ntemporal-logic (TL) translation systems reveals near-perfect performance on\nexisting benchmarks. However, current studies measure only the accuracy of the\ntranslation of NL logic into formal TL, ignoring a system's capacity to ground\natomic propositions into new scenarios or environments. This is a critical\nfeature, necessary for the verification of resulting formulas in a concrete\nstate space. Consequently, most NL-to-TL translation frameworks propose their\nown bespoke dataset in which the correct grounding is known a-priori, inflating\nperformance metrics and neglecting the need for extensible, domain-general\nsystems. In this paper, we introduce the Verifiable Linear Temporal Logic\nBenchmark ( VLTL-Bench), a unifying benchmark that measures verification and\nverifiability of automated NL-to-LTL translation. The dataset consists of three\nunique state spaces and thousands of diverse natural language specifications\nand corresponding formal specifications in temporal logic. Moreover, the\nbenchmark contains sample traces to validate the temporal logic expressions.\nWhile the benchmark directly supports end-to-end evaluation, we observe that\nmany frameworks decompose the process into i) lifting, ii) grounding, iii)\ntranslation, and iv) verification. The benchmark provides ground truths after\neach of these steps to enable researches to improve and evaluate different\nsubsteps of the overall problem. To encourage methodologically sound advances\nin verifiable NL-to-LTL translation approaches, we release VLTL-Bench here:\nhttps://www.kaggle.com/datasets/dubascudes/vltl bench.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00877v1", "categories": ["eess.SY", "cs.CL", "cs.SY"], "cate": "eess.SY", "url": "http://arxiv.org/abs/2507.00877v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00191", "title": "Beyond Sensor Data: Foundation Models of Behavioral Data from Wearables Improve Health Predictions", "authors": ["Eray Erturk", "Fahad Kamran", "Salar Abbaspourazad", "Sean Jewell", "Harsh Sharma", "Yujie Li", "Sinead Williamson", "Nicholas J Foti", "Joseph Futoma"], "summary": "Wearable devices record physiological and behavioral signals that can improve\nhealth predictions. While foundation models are increasingly used for such\npredictions, they have been primarily applied to low-level sensor data, despite\nbehavioral data often being more informative due to their alignment with\nphysiologically relevant timescales and quantities. We develop foundation\nmodels of such behavioral signals using over 2.5B hours of wearable data from\n162K individuals, systematically optimizing architectures and tokenization\nstrategies for this unique dataset. Evaluated on 57 health-related tasks, our\nmodel shows strong performance across diverse real-world applications including\nindividual-level classification and time-varying health state prediction. The\nmodel excels in behavior-driven tasks like sleep prediction, and improves\nfurther when combined with representations of raw sensor data. These results\nunderscore the importance of tailoring foundation model design to wearables and\ndemonstrate the potential to enable new health applications.", "comment": "Accepted to ICML 2025", "pdf_url": "http://arxiv.org/pdf/2507.00191v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00191v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00647", "title": "Cooperative Sheaf Neural Networks", "authors": ["André Ribeiro", "Ana Luiza Tenório", "Juan Belieni", "Amauri H. Souza", "Diego Mesquita"], "summary": "Sheaf diffusion has recently emerged as a promising design pattern for graph\nrepresentation learning due to its inherent ability to handle heterophilic data\nand avoid oversmoothing. Meanwhile, cooperative message passing has also been\nproposed as a way to enhance the flexibility of information diffusion by\nallowing nodes to independently choose whether to propagate/gather information\nfrom/to neighbors. A natural question ensues: is sheaf diffusion capable of\nexhibiting this cooperative behavior? Here, we provide a negative answer to\nthis question. In particular, we show that existing sheaf diffusion methods\nfail to achieve cooperative behavior due to the lack of message directionality.\nTo circumvent this limitation, we introduce the notion of cellular sheaves over\ndirected graphs and characterize their in- and out-degree Laplacians. We\nleverage our construction to propose Cooperative Sheaf Neural Networks (CSNNs).\nTheoretically, we characterize the receptive field of CSNN and show it allows\nnodes to selectively attend (listen) to arbitrarily far nodes while ignoring\nall others in their path, potentially mitigating oversquashing. Our experiments\nshow that CSNN presents overall better performance compared to prior art on\nsheaf diffusion as well as cooperative graph neural networks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00647v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00647v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00739", "title": "Biorthogonal Tunable Wavelet Unit with Lifting Scheme in Convolutional Neural Network", "authors": ["An Le", "Hung Nguyen", "Sungbal Seo", "You-Suk Bae", "Truong Nguyen"], "summary": "This work introduces a novel biorthogonal tunable wavelet unit constructed\nusing a lifting scheme that relaxes both the orthogonality and equal filter\nlength constraints, providing greater flexibility in filter design. The\nproposed unit enhances convolution, pooling, and downsampling operations,\nleading to improved image classification and anomaly detection in convolutional\nneural networks (CNN). When integrated into an 18-layer residual neural network\n(ResNet-18), the approach improved classification accuracy on CIFAR-10 by 2.12%\nand on the Describable Textures Dataset (DTD) by 9.73%, demonstrating its\neffectiveness in capturing fine-grained details. Similar improvements were\nobserved in ResNet-34. For anomaly detection in the hazelnut category of the\nMVTec Anomaly Detection dataset, the proposed method achieved competitive and\nwellbalanced performance in both segmentation and detection tasks,\noutperforming existing approaches in terms of accuracy and robustness.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00739v1", "categories": ["cs.CV", "eess.IV", "eess.SP"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00739v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00898", "title": "ONLY: One-Layer Intervention Sufficiently Mitigates Hallucinations in Large Vision-Language Models", "authors": ["Zifu Wan", "Ce Zhang", "Silong Yong", "Martin Q. Ma", "Simon Stepputtis", "Louis-Philippe Morency", "Deva Ramanan", "Katia Sycara", "Yaqi Xie"], "summary": "Recent Large Vision-Language Models (LVLMs) have introduced a new paradigm\nfor understanding and reasoning about image input through textual responses.\nAlthough they have achieved remarkable performance across a range of\nmulti-modal tasks, they face the persistent challenge of hallucination, which\nintroduces practical weaknesses and raises concerns about their reliable\ndeployment in real-world applications. Existing work has explored contrastive\ndecoding approaches to mitigate this issue, where the output of the original\nLVLM is compared and contrasted with that of a perturbed version. However,\nthese methods require two or more queries that slow down LVLM response\ngeneration, making them less suitable for real-time applications. To overcome\nthis limitation, we propose ONLY, a training-free decoding approach that\nrequires only a single query and a one-layer intervention during decoding,\nenabling efficient real-time deployment. Specifically, we enhance textual\noutputs by selectively amplifying crucial textual information using a\ntext-to-visual entropy ratio for each token. Extensive experimental results\ndemonstrate that our proposed ONLY consistently outperforms state-of-the-art\nmethods across various benchmarks while requiring minimal implementation effort\nand computational cost. Code is available at https://github.com/zifuwan/ONLY.", "comment": "Accepted by ICCV 2025. Project page: https://zifuwan.github.io/ONLY/", "pdf_url": "http://arxiv.org/pdf/2507.00898v1", "categories": ["cs.CV", "cs.CL"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00898v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00195", "title": "What Makes Local Updates Effective: The Role of Data Heterogeneity and Smoothness", "authors": ["Kumar Kshitij Patel"], "summary": "This thesis contributes to the theoretical understanding of local update\nalgorithms, especially Local SGD, in distributed and federated optimization\nunder realistic models of data heterogeneity. A central focus is on the bounded\nsecond-order heterogeneity assumption, which is shown to be both necessary and\nsufficient for local updates to outperform centralized or mini-batch methods in\nconvex and non-convex settings. The thesis establishes tight upper and lower\nbounds in several regimes for various local update algorithms and characterizes\nthe min-max complexity of multiple problem classes. At its core is a\nfine-grained consensus-error-based analysis framework that yields sharper\nfinite-time convergence bounds under third-order smoothness and relaxed\nheterogeneity assumptions. The thesis also extends to online federated\nlearning, providing fundamental regret bounds under both first-order and bandit\nfeedback. Together, these results clarify when and why local updates offer\nprovable advantages, and the thesis serves as a self-contained guide for\nanalyzing Local SGD in heterogeneous environments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00195v1", "categories": ["cs.LG", "cs.AI", "cs.MA", "math.OC", "stat.ML"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00195v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00651", "title": "GANs Secretly Perform Approximate Bayesian Model Selection", "authors": ["Maurizio Filippone", "Marius P. Linhard"], "summary": "Generative Adversarial Networks (GANs) are popular and successful generative\nmodels. Despite their success, optimization is notoriously challenging and they\nrequire regularization against overfitting. In this work, we explain the\nsuccess and limitations of GANs by interpreting them as probabilistic\ngenerative models. This interpretation enables us to view GANs as Bayesian\nneural networks with partial stochasticity, allowing us to establish conditions\nof universal approximation. We can then cast the adversarial-style optimization\nof several variants of GANs as the optimization of a proxy for the marginal\nlikelihood. Taking advantage of the connection between marginal likelihood\noptimization and Occam's razor, we can define regularization and optimization\nstrategies to smooth the loss landscape and search for solutions with minimum\ndescription length, which are associated with flat minima and good\ngeneralization. The results on a wide range of experiments indicate that these\nstrategies lead to performance improvements and pave the way to a deeper\nunderstanding of regularization strategies for GANs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00651v1", "categories": ["cs.LG", "cs.CV", "stat.ML"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00651v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00748", "title": "Improving the Reasoning of Multi-Image Grounding in MLLMs via Reinforcement Learning", "authors": ["Bob Zhang", "Haoran Li", "Tao Zhang", "Cilin Yan", "Jiayin Cai", "Xiaolong Jiang", "Yanbin Hao"], "summary": "Recently, Multimodal Large Language Models (MLLMs) excel at visual grounding\nin single-image scenarios with textual references. However, their performance\ndegrades when handling real-world applications involving complex multi-image\ncompositions and multimodal instructions, which reveals limitations in\ncross-image reasoning and generalization. To address these challenges, we adopt\na Reinforcement Learning (RL) based post-training strategy to improve the\nreasoning performance of MLLMs in multi-image grounding tasks. Our approach\nbegins with synthesizing high-quality chain-of-thought (CoT) data for\ncold-start initialization, followed by supervised fine-tuning (SFT) using\nlow-rank adaptation (LoRA). The cold-start training stage enables the model to\nidentify correct solutions. Subsequently, we perform rejection sampling using\nthe merged SFT model to curate high-quality RL data and leverage rule-based RL\nto guide the model toward optimal reasoning paths. Extensive experimental\nresults demonstrate the effectiveness of our approach, achieving +9.04\\%\nimprovements on MIG-Bench and +4.98\\% improvements on several out-of-domain\nreasoning grounding benchmarks over the SFT baseline. Furthermore, our approach\nexhibits strong generalization in multi-image perception, with gains of +3.1\\%\nand +2.4\\% over the base model on subsets of the BLINK and MMIU benchmarks,\nrespectively.", "comment": "11 pages", "pdf_url": "http://arxiv.org/pdf/2507.00748v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00748v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00979", "title": "Enhancing LLM Agent Safety via Causal Influence Prompting", "authors": ["Dongyoon Hahm", "Woogyeol Jin", "June Suk Choi", "Sungsoo Ahn", "Kimin Lee"], "summary": "As autonomous agents powered by large language models (LLMs) continue to\ndemonstrate potential across various assistive tasks, ensuring their safe and\nreliable behavior is crucial for preventing unintended consequences. In this\nwork, we introduce CIP, a novel technique that leverages causal influence\ndiagrams (CIDs) to identify and mitigate risks arising from agent\ndecision-making. CIDs provide a structured representation of cause-and-effect\nrelationships, enabling agents to anticipate harmful outcomes and make safer\ndecisions. Our approach consists of three key steps: (1) initializing a CID\nbased on task specifications to outline the decision-making process, (2)\nguiding agent interactions with the environment using the CID, and (3)\niteratively refining the CID based on observed behaviors and outcomes.\nExperimental results demonstrate that our method effectively enhances safety in\nboth code execution and mobile device control tasks.", "comment": "Accepted at ACL 2025 Findings, Source code:\n  https://github.com/HahmDY/causal_influence_prompting.git", "pdf_url": "http://arxiv.org/pdf/2507.00979v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2507.00979v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00209", "title": "SurgiSR4K: A High-Resolution Endoscopic Video Dataset for Robotic-Assisted Minimally Invasive Procedures", "authors": ["Fengyi Jiang", "Xiaorui Zhang", "Lingbo Jin", "Ruixing Liang", "Yuxin Chen", "Adi Chola Venkatesh", "Jason Culman", "Tiantian Wu", "Lirong Shao", "Wenqing Sun", "Cong Gao", "Hallie McNamara", "Jingpei Lu", "Omid Mohareri"], "summary": "High-resolution imaging is crucial for enhancing visual clarity and enabling\nprecise computer-assisted guidance in minimally invasive surgery (MIS). Despite\nthe increasing adoption of 4K endoscopic systems, there remains a significant\ngap in publicly available native 4K datasets tailored specifically for\nrobotic-assisted MIS. We introduce SurgiSR4K, the first publicly accessible\nsurgical imaging and video dataset captured at a native 4K resolution,\nrepresenting realistic conditions of robotic-assisted procedures. SurgiSR4K\ncomprises diverse visual scenarios including specular reflections, tool\nocclusions, bleeding, and soft tissue deformations, meticulously designed to\nreflect common challenges faced during laparoscopic and robotic surgeries. This\ndataset opens up possibilities for a broad range of computer vision tasks that\nmight benefit from high resolution data, such as super resolution (SR), smoke\nremoval, surgical instrument detection, 3D tissue reconstruction, monocular\ndepth estimation, instance segmentation, novel view synthesis, and\nvision-language model (VLM) development. SurgiSR4K provides a robust foundation\nfor advancing research in high-resolution surgical imaging and fosters the\ndevelopment of intelligent imaging technologies aimed at enhancing performance,\nsafety, and usability in image-guided robotic surgeries.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00209v1", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.RO"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2507.00209v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00653", "title": "Cognitive Load-Aware Inference: A Neuro-Symbolic Framework for Optimizing the Token Economy of Large Language Models", "authors": ["Yilun Zhang"], "summary": "The escalating computational costs of Large Language Model (LLM) inference\nhave become a critical barrier to their widespread and sustainable deployment.\nWhile existing optimization strategies are effective, they are predominantly\nbased on statistical heuristics or architectural modifications, lacking a\nguiding cognitive theory to manage the inference process itself. This paper\naims to bridge this gap by introducing a novel paradigm: the Cognitive\nLoad-Aware Inference (CLAI) framework, which operationalizes principles from\nCognitive Load Theory (CLT) and neuroscience for LLM inference. We formalize\nthe concepts of Intrinsic Cognitive Load, Extraneous Cognitive Load, and\nGermane Cognitive Load into quantifiable LLM metrics ($ICL_{LLM}$, $ECL_{LLM}$,\nand $GCL_{LLM}$), thereby reframing the inference process as a cognitive\neconomics optimization problem: based on the intrinsic complexity of a problem\n($ICL_{LLM}$), minimize wasteful computation ($ECL_{LLM}$), and strategically\nallocate the token budget to productive reasoning ($GCL_{LLM}$). We propose two\nimplementation paths: CLAI-Prompt, a zero-shot method that guides a base LLM\nthrough cognitive control steps via a structured meta-prompt, and CLAI-Tune, a\nfine-tuned model that internalizes these principles for spontaneous cognitive\neconomy. Across a range of benchmarks in complex reasoning, long-context\nquestion answering, and code generation, our methods achieve significant\nreductions in token consumption (up to 45\\%) without sacrificing accuracy.\nFurthermore, CLAI-Tune exhibits an emergent ability to autonomously decompose\ndifficult problems, a key characteristic of human expert cognition. This work\ndemonstrates that by emulating the brain's resource management strategies, we\ncan build more efficient, robust, and capable artificial intelligence systems.", "comment": "23 pages", "pdf_url": "http://arxiv.org/pdf/2507.00653v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00653v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00752", "title": "Multi-Modal Graph Convolutional Network with Sinusoidal Encoding for Robust Human Action Segmentation", "authors": ["Hao Xing", "Kai Zhe Boey", "Yuankai Wu", "Darius Burschka", "Gordon Cheng"], "summary": "Accurate temporal segmentation of human actions is critical for intelligent\nrobots in collaborative settings, where a precise understanding of sub-activity\nlabels and their temporal structure is essential. However, the inherent noise\nin both human pose estimation and object detection often leads to\nover-segmentation errors, disrupting the coherence of action sequences. To\naddress this, we propose a Multi-Modal Graph Convolutional Network (MMGCN) that\nintegrates low-frame-rate (e.g., 1 fps) visual data with high-frame-rate (e.g.,\n30 fps) motion data (skeleton and object detections) to mitigate fragmentation.\nOur framework introduces three key contributions. First, a sinusoidal encoding\nstrategy that maps 3D skeleton coordinates into a continuous sin-cos space to\nenhance spatial representation robustness. Second, a temporal graph fusion\nmodule that aligns multi-modal inputs with differing resolutions via\nhierarchical feature aggregation, Third, inspired by the smooth transitions\ninherent to human actions, we design SmoothLabelMix, a data augmentation\ntechnique that mixes input sequences and labels to generate synthetic training\nexamples with gradual action transitions, enhancing temporal consistency in\npredictions and reducing over-segmentation artifacts.\n  Extensive experiments on the Bimanual Actions Dataset, a public benchmark for\nhuman-object interaction understanding, demonstrate that our approach\noutperforms state-of-the-art methods, especially in action segmentation\naccuracy, achieving F1@10: 94.5% and F1@25: 92.8%.", "comment": "7 pages, 4 figures, accepted in IROS25, Hangzhou, China", "pdf_url": "http://arxiv.org/pdf/2507.00752v1", "categories": ["cs.CV", "cs.RO"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00752v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00214", "title": "Two-Stage Reasoning-Infused Learning: Improving Classification with LLM-Generated Reasoning", "authors": ["Mads Henrichsen", "Rasmus Krebs"], "summary": "Standard classification models often map inputs directly to labels without\nexplicit reasoning, potentially limiting their performance, robustness, and\ninterpretability. This paper introduces a novel two-stage approach to enhance\ntext classification by leveraging Large Language Model (LLM)-generated\nreasonings. In the first stage, we fine-tune a Llama-3.2-1B-Instruct model\n(henceforth Llama-R-Gen) on a general-purpose reasoning dataset\n(syvai/reasoning-gen) to generate textual reasoning (R) given a question and\nits answer. In the second stage, this generally trained Llama-R-Gen is used\noffline to create an augmented training dataset for a downstream generative\nmodel. This downstream model, based on Llama-3.2-1B-Instruct, takes only the\ninput text (Q) and is trained to output the generated reasoning (R) immediately\nfollowed by the predicted emotion (A). We demonstrate this methodology on the\ndair-ai/emotion dataset for emotion classification. Our experiments show that\nthe generative model trained to output reasoning and the emotion (Classifier\nQ->RA) achieves a significant improvement of 8.7 percentage points in accuracy\n(for emotion prediction) compared to a baseline generative model trained solely\nto output the emotion (Classifier Q->A), highlighting the strong generalization\ncapabilities of the reasoning generation and the benefit of explicit reasoning\ntraining. This work underscores the potential of LLM-generated reasonings for\ncreating richer training datasets, thereby improving the performance of diverse\ndownstream NLP tasks and providing explicit explanations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00214v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2507.00214v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00654", "title": "Neural Augmented Kalman Filters for Road Network assisted GNSS positioning", "authors": ["Hans van Gorp", "Davide Belli", "Amir Jalalirad", "Bence Major"], "summary": "The Global Navigation Satellite System (GNSS) provides critical positioning\ninformation globally, but its accuracy in dense urban environments is often\ncompromised by multipath and non-line-of-sight errors. Road network data can be\nused to reduce the impact of these errors and enhance the accuracy of a\npositioning system. Previous works employing road network data are either\nlimited to offline applications, or rely on Kalman Filter (KF) heuristics with\nlittle flexibility and robustness. We instead propose training a Temporal Graph\nNeural Network (TGNN) to integrate road network information into a KF. The TGNN\nis designed to predict the correct road segment and its associated uncertainty\nto be used in the measurement update step of the KF. We validate our approach\nwith real-world GNSS data and open-source road networks, observing a 29%\ndecrease in positioning error for challenging scenarios compared to a GNSS-only\nKF. To the best of our knowledge, ours is the first deep learning-based\napproach jointly employing road network data and GNSS measurements to determine\nthe user position on Earth.", "comment": "Accepted to ICML 2025 workshop ML4Wireless", "pdf_url": "http://arxiv.org/pdf/2507.00654v1", "categories": ["cs.LG", "cs.SY", "eess.SP", "eess.SY"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00654v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00754", "title": "Language-Unlocked ViT (LUViT): Empowering Self-Supervised Vision Transformers with LLMs", "authors": ["Selim Kuzucu", "Muhammad Ferjad Naeem", "Anna Kukleva", "Federico Tombari", "Bernt Schiele"], "summary": "The integration of Large Language Model (LLMs) blocks with Vision\nTransformers (ViTs) holds immense promise for vision-only tasks by leveraging\nthe rich semantic knowledge and reasoning capabilities of LLMs. However, a\nfundamental challenge lies in the inherent modality mismatch between\ntext-centric pretraining of LLMs and vision-centric training of ViTs. Direct\nfusion often fails to fully exploit the LLM's potential and suffers from\nunstable finetuning. As a result, LLM blocks are kept frozen while only the\nvision components are learned. As a remedy to these challenges, we introduce\nLanguage-Unlocked Vision Transformers (LUViT), a novel approach that bridges\nthis modality mismatch through a synergistic pre-training strategy. LUViT\nco-adapts a ViT backbone and an LLM fusion block by (1) employing Masked\nAuto-Encoding (MAE) to pre-train the ViT for richer visual representations, and\n(2) concurrently training Low-Rank Adaptation (LoRA) layers within the LLM\nblock using the MAE objective. This joint optimization guides the ViT to\nproduce LLM-aligned features and the LLM to effectively interpret visual\ninformation. We demonstrate through extensive experiments that LUViT\nsignificantly improves performance on various downstream vision tasks,\nshowcasing a more effective and efficient pathway to harness LLM knowledge for\nvisual understanding.", "comment": "26 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2507.00754v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00754v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00225", "title": "Discovering the underlying analytic structure within Standard Model constants using artificial intelligence", "authors": ["S. V. Chekanov", "H. Kjellerstrand"], "summary": "This paper presents a search for underlying analytic structures among the\nfundamental parameters of the Standard Model (SM) using symbolic regression and\ngenetic programming. We identify the simplest analytic relationships connecting\npairs of these constants and report several notable observations based on about\na thousand expressions with relative precision better than 1%. These results\nmay serve as valuable inputs for model builders and artificial intelligence\nmethods aimed at uncovering hidden patterns among the SM constants, or\npotentially used as building blocks for a deeper underlying law that connects\nall parameters of the SM through a small set of fundamental constants.", "comment": "42 pages, 10 tables", "pdf_url": "http://arxiv.org/pdf/2507.00225v1", "categories": ["hep-ph", "cs.AI", "physics.data-an"], "cate": "hep-ph", "url": "http://arxiv.org/abs/2507.00225v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00669", "title": "Audio-3DVG: Unified Audio - Point Cloud Fusion for 3D Visual Grounding", "authors": ["Duc Cao-Dinh", "Khai Le-Duc", "Anh Dao", "Bach Phan Tat", "Chris Ngo", "Duy M. H. Nguyen", "Nguyen X. Khanh", "Thanh Nguyen-Tang"], "summary": "3D Visual Grounding (3DVG) involves localizing target objects in 3D point\nclouds based on natural language. While prior work has made strides using\ntextual descriptions, leveraging spoken language-known as Audio-based 3D Visual\nGrounding-remains underexplored and challenging. Motivated by advances in\nautomatic speech recognition (ASR) and speech representation learning, we\npropose Audio-3DVG, a simple yet effective framework that integrates audio and\nspatial information for enhanced grounding. Rather than treating speech as a\nmonolithic input, we decompose the task into two complementary components.\nFirst, we introduce Object Mention Detection, a multi-label classification task\nthat explicitly identifies which objects are referred to in the audio, enabling\nmore structured audio-scene reasoning. Second, we propose an Audio-Guided\nAttention module that captures interactions between candidate objects and\nrelational speech cues, improving target discrimination in cluttered scenes. To\nsupport benchmarking, we synthesize audio descriptions for standard 3DVG\ndatasets, including ScanRefer, Sr3D, and Nr3D. Experimental results demonstrate\nthat Audio-3DVG not only achieves new state-of-the-art performance in\naudio-based grounding, but also competes with text-based methods-highlighting\nthe promise of integrating spoken language into 3D vision tasks.", "comment": "Work in progress, 42 pages", "pdf_url": "http://arxiv.org/pdf/2507.00669v1", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.RO"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00669v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00756", "title": "Towards Open-World Human Action Segmentation Using Graph Convolutional Networks", "authors": ["Hao Xing", "Kai Zhe Boey", "Gordon Cheng"], "summary": "Human-object interaction segmentation is a fundamental task of daily activity\nunderstanding, which plays a crucial role in applications such as assistive\nrobotics, healthcare, and autonomous systems. Most existing learning-based\nmethods excel in closed-world action segmentation, they struggle to generalize\nto open-world scenarios where novel actions emerge. Collecting exhaustive\naction categories for training is impractical due to the dynamic diversity of\nhuman activities, necessitating models that detect and segment\nout-of-distribution actions without manual annotation. To address this issue,\nwe formally define the open-world action segmentation problem and propose a\nstructured framework for detecting and segmenting unseen actions. Our framework\nintroduces three key innovations: 1) an Enhanced Pyramid Graph Convolutional\nNetwork (EPGCN) with a novel decoder module for robust spatiotemporal feature\nupsampling. 2) Mixup-based training to synthesize out-of-distribution data,\neliminating reliance on manual annotations. 3) A novel Temporal Clustering loss\nthat groups in-distribution actions while distancing out-of-distribution\nsamples.\n  We evaluate our framework on two challenging human-object interaction\nrecognition datasets: Bimanual Actions and 2 Hands and Object (H2O) datasets.\nExperimental results demonstrate significant improvements over state-of-the-art\naction segmentation models across multiple open-set evaluation metrics,\nachieving 16.9% and 34.6% relative gains in open-set segmentation (F1@50) and\nout-of-distribution detection performances (AUROC), respectively. Additionally,\nwe conduct an in-depth ablation study to assess the impact of each proposed\ncomponent, identifying the optimal framework configuration for open-world\naction segmentation.", "comment": "8 pages, 3 figures, accepted in IROS25, Hangzhou, China", "pdf_url": "http://arxiv.org/pdf/2507.00756v1", "categories": ["cs.CV", "cs.RO"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00756v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00227", "title": "Investigating Stochastic Methods for Prosody Modeling in Speech Synthesis", "authors": ["Paul Mayer", "Florian Lux", "Alejandro Pérez-González-de-Martos", "Angelina Elizarova", "Lindsey Vanderlyn", "Dirk Väth", "Ngoc Thang Vu"], "summary": "While generative methods have progressed rapidly in recent years, generating\nexpressive prosody for an utterance remains a challenging task in\ntext-to-speech synthesis. This is particularly true for systems that model\nprosody explicitly through parameters such as pitch, energy, and duration,\nwhich is commonly done for the sake of interpretability and controllability. In\nthis work, we investigate the effectiveness of stochastic methods for this\ntask, including Normalizing Flows, Conditional Flow Matching, and Rectified\nFlows. We compare these methods to a traditional deterministic baseline, as\nwell as to real human realizations. Our extensive subjective and objective\nevaluations demonstrate that stochastic methods produce natural prosody on par\nwith human speakers by capturing the variability inherent in human speech.\nFurther, they open up additional controllability options by allowing the\nsampling temperature to be tuned.", "comment": "Accepted at Interspeech 2025", "pdf_url": "http://arxiv.org/pdf/2507.00227v1", "categories": ["eess.AS", "cs.AI"], "cate": "eess.AS", "url": "http://arxiv.org/abs/2507.00227v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00687", "title": "Diffusion Classifier Guidance for Non-robust Classifiers", "authors": ["Philipp Vaeth", "Dibyanshu Kumar", "Benjamin Paassen", "Magda Gregorová"], "summary": "Classifier guidance is intended to steer a diffusion process such that a\ngiven classifier reliably recognizes the generated data point as a certain\nclass. However, most classifier guidance approaches are restricted to robust\nclassifiers, which were specifically trained on the noise of the diffusion\nforward process. We extend classifier guidance to work with general,\nnon-robust, classifiers that were trained without noise. We analyze the\nsensitivity of both non-robust and robust classifiers to noise of the diffusion\nprocess on the standard CelebA data set, the specialized SportBalls data set\nand the high-dimensional real-world CelebA-HQ data set. Our findings reveal\nthat non-robust classifiers exhibit significant accuracy degradation under\nnoisy conditions, leading to unstable guidance gradients. To mitigate these\nissues, we propose a method that utilizes one-step denoised image predictions\nand implements stabilization techniques inspired by stochastic optimization\nmethods, such as exponential moving averages. Experimental results demonstrate\nthat our approach improves the stability of classifier guidance while\nmaintaining sample diversity and visual quality. This work contributes to\nadvancing conditional sampling techniques in generative models, enabling a\nbroader range of classifiers to be used as guidance classifiers.", "comment": "Accepted at ECML 2025", "pdf_url": "http://arxiv.org/pdf/2507.00687v1", "categories": ["cs.LG", "cs.CV"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00687v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00789", "title": "OptiPrune: Boosting Prompt-Image Consistency with Attention-Guided Noise and Dynamic Token Selection", "authors": ["Ziji Lu"], "summary": "Text-to-image diffusion models often struggle to achieve accurate semantic\nalignment between generated images and text prompts while maintaining\nefficiency for deployment on resource-constrained hardware. Existing approaches\neither incur substantial computational overhead through noise optimization or\ncompromise semantic fidelity by aggressively pruning tokens. In this work, we\npropose OptiPrune, a unified framework that combines distribution-aware initial\nnoise optimization with similarity-based token pruning to address both\nchallenges simultaneously. Specifically, (1) we introduce a distribution-aware\nnoise optimization module guided by attention scores to steer the initial\nlatent noise toward semantically meaningful regions, mitigating issues such as\nsubject neglect and feature entanglement; (2) we design a hardware-efficient\ntoken pruning strategy that selects representative base tokens via patch-wise\nsimilarity, injects randomness to enhance generalization, and recovers pruned\ntokens using maximum similarity copying before attention operations. Our method\npreserves the Gaussian prior during noise optimization and enables efficient\ninference without sacrificing alignment quality. Experiments on benchmark\ndatasets, including Animal-Animal, demonstrate that OptiPrune achieves\nstate-of-the-art prompt-image consistency with significantly reduced\ncomputational cost.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00789v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00789v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00229", "title": "A High-Fidelity Speech Super Resolution Network using a Complex Global Attention Module with Spectro-Temporal Loss", "authors": ["Tarikul Islam Tamiti", "Biraj Joshi", "Rida Hasan", "Rashedul Hasan", "Taieba Athay", "Nursad Mamun", "Anomadarshi Barua"], "summary": "Speech super-resolution (SSR) enhances low-resolution speech by increasing\nthe sampling rate. While most SSR methods focus on magnitude reconstruction,\nrecent research highlights the importance of phase reconstruction for improved\nperceptual quality. Therefore, we introduce CTFT-Net, a Complex Time-Frequency\nTransformation Network that reconstructs both magnitude and phase in complex\ndomains for improved SSR tasks. It incorporates a complex global attention\nblock to model inter-phoneme and inter-frequency dependencies and a complex\nconformer to capture long-range and local features, improving frequency\nreconstruction and noise robustness. CTFT-Net employs time-domain and\nmulti-resolution frequency-domain loss functions for better generalization.\nExperiments show CTFT-Net outperforms state-of-the-art models (NU-Wave,\nWSRGlow, NVSR, AERO) on the VCTK dataset, particularly for extreme upsampling\n(2 kHz to 48 kHz), reconstructing high frequencies effectively without noisy\nartifacts.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00229v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "cate": "cs.SD", "url": "http://arxiv.org/abs/2507.00229v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00695", "title": "A Test-Function Approach to Incremental Stability", "authors": ["Daniel Pfrommer", "Max Simchowitz", "Ali Jadbabaie"], "summary": "This paper presents a novel framework for analyzing\nIncremental-Input-to-State Stability ($\\delta$ISS) based on the idea of using\nrewards as \"test functions.\" Whereas control theory traditionally deals with\nLyapunov functions that satisfy a time-decrease condition, reinforcement\nlearning (RL) value functions are constructed by exponentially decaying a\nLipschitz reward function that may be non-smooth and unbounded on both sides.\nThus, these RL-style value functions cannot be directly understood as Lyapunov\ncertificates. We develop a new equivalence between a variant of incremental\ninput-to-state stability of a closed-loop system under given a policy, and the\nregularity of RL-style value functions under adversarial selection of a\nH\\\"older-continuous reward function. This result highlights that the regularity\nof value functions, and their connection to incremental stability, can be\nunderstood in a way that is distinct from the traditional Lyapunov-based\napproach to certifying stability in control theory.", "comment": "8 pages", "pdf_url": "http://arxiv.org/pdf/2507.00695v1", "categories": ["cs.LG", "cs.SY", "eess.SY"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00695v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00790", "title": "LD-RPS: Zero-Shot Unified Image Restoration via Latent Diffusion Recurrent Posterior Sampling", "authors": ["Huaqiu Li", "Yong Wang", "Tongwen Huang", "Hailang Huang", "Haoqian Wang", "Xiangxiang Chu"], "summary": "Unified image restoration is a significantly challenging task in low-level\nvision. Existing methods either make tailored designs for specific tasks,\nlimiting their generalizability across various types of degradation, or rely on\ntraining with paired datasets, thereby suffering from closed-set constraints.\nTo address these issues, we propose a novel, dataset-free, and unified approach\nthrough recurrent posterior sampling utilizing a pretrained latent diffusion\nmodel. Our method incorporates the multimodal understanding model to provide\nsematic priors for the generative model under a task-blind condition.\nFurthermore, it utilizes a lightweight module to align the degraded input with\nthe generated preference of the diffusion model, and employs recurrent\nrefinement for posterior sampling. Extensive experiments demonstrate that our\nmethod outperforms state-of-the-art methods, validating its effectiveness and\nrobustness. Our code and data will be available at\nhttps://github.com/AMAP-ML/LD-RPS.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00790v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00790v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00234", "title": "Interpretable AI for Time-Series: Multi-Model Heatmap Fusion with Global Attention and NLP-Generated Explanations", "authors": ["Jiztom Kavalakkatt Francis", "Matthew J Darr"], "summary": "In this paper, we present a novel framework for enhancing model\ninterpretability by integrating heatmaps produced separately by ResNet and a\nrestructured 2D Transformer with globally weighted input saliency. We address\nthe critical problem of spatial-temporal misalignment in existing\ninterpretability methods, where convolutional networks fail to capture global\ncontext and Transformers lack localized precision - a limitation that impedes\nactionable insights in safety-critical domains like healthcare and industrial\nmonitoring. Our method merges gradient-weighted activation maps (ResNet) and\nTransformer attention rollout into a unified visualization, achieving full\nspatial-temporal alignment while preserving real-time performance. Empirical\nevaluations on clinical (ECG arrhythmia detection) and industrial (energy\nconsumption prediction) datasets demonstrate significant improvements: the\nhybrid framework achieves 94.1% accuracy (F1 0.93) on the PhysioNet dataset and\nreduces regression error to RMSE = 0.28 kWh (R2 = 0.95) on the UCI Energy\nAppliance dataset-outperforming standalone ResNet, Transformer, and\nInceptionTime baselines by 3.8-12.4%. An NLP module translates fused heatmaps\ninto domain-specific narratives (e.g., \"Elevated ST-segment between 2-4 seconds\nsuggests myocardial ischemia\"), validated via BLEU-4 (0.586) and ROUGE-L\n(0.650) scores. By formalizing interpretability as causal fidelity and\nspatial-temporal alignment, our approach bridges the gap between technical\noutputs and stakeholder understanding, offering a scalable solution for\ntransparent, time-aware decision-making.", "comment": "13 pages", "pdf_url": "http://arxiv.org/pdf/2507.00234v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00234v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00701", "title": "SCAWaveNet: A Spatial-Channel Attention-based Network for Global Significant Wave Height Retrieval", "authors": ["Chong Zhang", "Xichao Liu", "Yibing Zhan", "Dapeng Tao", "Jun Ni"], "summary": "Recent advancements in spaceborne GNSS missions have produced extensive\nglobal datasets, providing a robust basis for deep learning-based significant\nwave height (SWH) retrieval. While existing deep learning models predominantly\nutilize CYGNSS data with four-channel information, they often adopt\nsingle-channel inputs or simple channel concatenation without leveraging the\nbenefits of cross-channel information interaction during training. To address\nthis limitation, a novel spatial-channel attention-based network, namely\nSCAWaveNet, is proposed for SWH retrieval. Specifically, features from each\nchannel of the DDMs are modeled as independent attention heads, enabling the\nfusion of spatial and channel-wise information. For auxiliary parameters, a\nlightweight attention mechanism is designed to assign weights along the spatial\nand channel dimensions. The final feature integrates both spatial and\nchannel-level characteristics. Model performance is evaluated using\nfour-channel CYGNSS data. When ERA5 is used as a reference, SCAWaveNet achieves\nan average RMSE of 0.438 m. When using buoy data from NDBC, the average RMSE\nreaches 0.432 m. Compared to state-of-the-art models, SCAWaveNet reduces the\naverage RMSE by at least 3.52% on the ERA5 dataset and by 5.47% on the NDBC\nbuoy observations. The code is available at\nhttps://github.com/Clifx9908/SCAWaveNet.", "comment": "16 pages,6 tables,11 figures", "pdf_url": "http://arxiv.org/pdf/2507.00701v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00701v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00792", "title": "Real-Time Inverse Kinematics for Generating Multi-Constrained Movements of Virtual Human Characters", "authors": ["Hendric Voss", "Stefan Kopp"], "summary": "Generating accurate and realistic virtual human movements in real-time is of\nhigh importance for a variety of applications in computer graphics, interactive\nvirtual environments, robotics, and biomechanics. This paper introduces a novel\nreal-time inverse kinematics (IK) solver specifically designed for realistic\nhuman-like movement generation. Leveraging the automatic differentiation and\njust-in-time compilation of TensorFlow, the proposed solver efficiently handles\ncomplex articulated human skeletons with high degrees of freedom. By treating\nforward and inverse kinematics as differentiable operations, our method\neffectively addresses common challenges such as error accumulation and\ncomplicated joint limits in multi-constrained problems, which are critical for\nrealistic human motion modeling. We demonstrate the solver's effectiveness on\nthe SMPLX human skeleton model, evaluating its performance against widely used\niterative-based IK algorithms, like Cyclic Coordinate Descent (CCD), FABRIK,\nand the nonlinear optimization algorithm IPOPT. Our experiments cover both\nsimple end-effector tasks and sophisticated, multi-constrained problems with\nrealistic joint limits. Results indicate that our IK solver achieves real-time\nperformance, exhibiting rapid convergence, minimal computational overhead per\niteration, and improved success rates compared to existing methods. The project\ncode is available at https://github.com/hvoss-techfak/TF-JAX-IK", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00792v1", "categories": ["cs.CV", "cs.HC"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00792v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00239", "title": "Linearly Decoding Refused Knowledge in Aligned Language Models", "authors": ["Aryan Shrivastava", "Ari Holtzman"], "summary": "Most commonly used language models (LMs) are instruction-tuned and aligned\nusing a combination of fine-tuning and reinforcement learning, causing them to\nrefuse users requests deemed harmful by the model. However, jailbreak prompts\ncan often bypass these refusal mechanisms and elicit harmful responses. In this\nwork, we study the extent to which information accessed via jailbreak prompts\nis decodable using linear probes trained on LM hidden states. We show that a\ngreat deal of initially refused information is linearly decodable. For example,\nacross models, the response of a jailbroken LM for the average IQ of a country\ncan be predicted by a linear probe with Pearson correlations exceeding $0.8$.\nSurprisingly, we find that probes trained on base models (which do not refuse)\nsometimes transfer to their instruction-tuned versions and are capable of\nrevealing information that jailbreaks decode generatively, suggesting that the\ninternal representations of many refused properties persist from base LMs\nthrough instruction-tuning. Importantly, we show that this information is not\nmerely \"leftover\" in instruction-tuned models, but is actively used by them: we\nfind that probe-predicted values correlate with LM generated pairwise\ncomparisons, indicating that the information decoded by our probes align with\nsuppressed generative behavior that may be expressed more subtly in other\ndownstream tasks. Overall, our results suggest that instruction-tuning does not\nwholly eliminate or even relocate harmful information in representation\nspace-they merely suppress its direct expression, leaving it both linearly\naccessible and indirectly influential in downstream behavior.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00239v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2507.00239v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00711", "title": "Large Reasoning Models are not thinking straight: on the unreliability of thinking trajectories", "authors": ["Jhouben Cuesta-Ramirez", "Samuel Beaussant", "Mehdi Mounsif"], "summary": "Large Language Models (LLMs) trained via Reinforcement Learning (RL) have\nrecently achieved impressive results on reasoning benchmarks. Yet, growing\nevidence shows that these models often generate longer but ineffective chains\nof thought (CoTs), calling into question whether benchmark gains reflect real\nreasoning improvements. We present new evidence of overthinking, where models\ndisregard correct solutions even when explicitly provided, instead continuing\nto generate unnecessary reasoning steps that often lead to incorrect\nconclusions. Experiments on three state-of-the-art models using the AIME2024\nmath benchmark reveal critical limitations in these models ability to integrate\ncorrective information, posing new challenges for achieving robust and\ninterpretable reasoning.", "comment": "Accepted to KONVENS 2025", "pdf_url": "http://arxiv.org/pdf/2507.00711v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00711v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00802", "title": "TRACE: Temporally Reliable Anatomically-Conditioned 3D CT Generation with Enhanced Efficiency", "authors": ["Minye Shao", "Xingyu Miao", "Haoran Duan", "Zeyu Wang", "Jingkun Chen", "Yawen Huang", "Xian Wu", "Jingjing Deng", "Yang Long", "Yefeng Zheng"], "summary": "3D medical image generation is essential for data augmentation and patient\nprivacy, calling for reliable and efficient models suited for clinical\npractice. However, current methods suffer from limited anatomical fidelity,\nrestricted axial length, and substantial computational cost, placing them\nbeyond reach for regions with limited resources and infrastructure. We\nintroduce TRACE, a framework that generates 3D medical images with\nspatiotemporal alignment using a 2D multimodal-conditioned diffusion approach.\nTRACE models sequential 2D slices as video frame pairs, combining segmentation\npriors and radiology reports for anatomical alignment, incorporating optical\nflow to sustain temporal coherence. During inference, an overlapping-frame\nstrategy links frame pairs into a flexible length sequence, reconstructed into\na spatiotemporally and anatomically aligned 3D volume. Experimental results\ndemonstrate that TRACE effectively balances computational efficiency with\npreserving anatomical fidelity and spatiotemporal consistency. Code is\navailable at: https://github.com/VinyehShaw/TRACE.", "comment": "Accepted to MICCAI 2025 (this version is not peer-reviewed; it is the\n  preprint version). MICCAI proceedings DOI will appear here", "pdf_url": "http://arxiv.org/pdf/2507.00802v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00802v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00248", "title": "Developing Lightweight DNN Models With Limited Data For Real-Time Sign Language Recognition", "authors": ["Nikita Nikitin", "Eugene Fomin"], "summary": "We present a novel framework for real-time sign language recognition using\nlightweight DNNs trained on limited data. Our system addresses key challenges\nin sign language recognition, including data scarcity, high computational\ncosts, and discrepancies in frame rates between training and inference\nenvironments. By encoding sign language specific parameters, such as handshape,\npalm orientation, movement, and location into vectorized inputs, and leveraging\nMediaPipe for landmark extraction, we achieve highly separable input data\nrepresentations. Our DNN architecture, optimized for sub 10MB deployment,\nenables accurate classification of 343 signs with less than 10ms latency on\nedge devices. The data annotation platform 'slait data' facilitates structured\nlabeling and vector extraction. Our model achieved 92% accuracy in isolated\nsign recognition and has been integrated into the 'slait ai' web application,\nwhere it demonstrates stable inference.", "comment": "7 pages, 2 figures, 2 tables, for associated mpeg file, see\n  https://slait.app/static/Screen_Recording.mp4", "pdf_url": "http://arxiv.org/pdf/2507.00248v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00248v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00733", "title": "Aleatoric and Epistemic Uncertainty Measures for Ordinal Classification through Binary Reduction", "authors": ["Stefan Haas", "Eyke Hüllermeier"], "summary": "Ordinal classification problems, where labels exhibit a natural order, are\nprevalent in high-stakes fields such as medicine and finance. Accurate\nuncertainty quantification, including the decomposition into aleatoric\n(inherent variability) and epistemic (lack of knowledge) components, is crucial\nfor reliable decision-making. However, existing research has primarily focused\non nominal classification and regression. In this paper, we introduce a novel\nclass of measures of aleatoric and epistemic uncertainty in ordinal\nclassification, which is based on a suitable reduction to (entropy- and\nvariance-based) measures for the binary case. These measures effectively\ncapture the trade-off in ordinal classification between exact hit-rate and\nminimial error distances. We demonstrate the effectiveness of our approach on\nvarious tabular ordinal benchmark datasets using ensembles of gradient-boosted\ntrees and multi-layer perceptrons for approximate Bayesian inference. Our\nmethod significantly outperforms standard and label-wise entropy and\nvariance-based measures in error detection, as indicated by misclassification\nrates and mean absolute error. Additionally, the ordinal measures show\ncompetitive performance in out-of-distribution (OOD) detection. Our findings\nhighlight the importance of considering the ordinal nature of classification\nproblems when assessing uncertainty.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00733v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00733v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00817", "title": "CAVALRY-V: A Large-Scale Generator Framework for Adversarial Attacks on Video MLLMs", "authors": ["Jiaming Zhang", "Rui Hu", "Qing Guo", "Wei Yang Bryan Lim"], "summary": "Video Multimodal Large Language Models (V-MLLMs) have shown impressive\ncapabilities in temporal reasoning and cross-modal understanding, yet their\nvulnerability to adversarial attacks remains underexplored due to unique\nchallenges: complex cross-modal reasoning mechanisms, temporal dependencies,\nand computational constraints. We present CAVALRY-V (Cross-modal\nLanguage-Vision Adversarial Yielding for Videos), a novel framework that\ndirectly targets the critical interface between visual perception and language\ngeneration in V-MLLMs. Our approach introduces two key innovations: (1) a\ndual-objective semantic-visual loss function that simultaneously disrupts the\nmodel's text generation logits and visual representations to undermine\ncross-modal integration, and (2) a computationally efficient two-stage\ngenerator framework that combines large-scale pre-training for cross-model\ntransferability with specialized fine-tuning for spatiotemporal coherence.\nEmpirical evaluation on comprehensive video understanding benchmarks\ndemonstrates that CAVALRY-V significantly outperforms existing attack methods,\nachieving 22.8% average improvement over the best baseline attacks on both\ncommercial systems (GPT-4.1, Gemini 2.0) and open-source models (QwenVL-2.5,\nInternVL-2.5, Llava-Video, Aria, MiniCPM-o-2.6). Our framework achieves\nflexibility through implicit temporal coherence modeling rather than explicit\nregularization, enabling significant performance improvements even on image\nunderstanding (34.4% average gain). This capability demonstrates CAVALRY-V's\npotential as a foundational approach for adversarial research across multimodal\nsystems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00817v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00817v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00257", "title": "Gym4ReaL: A Suite for Benchmarking Real-World Reinforcement Learning", "authors": ["Davide Salaorni", "Vincenzo De Paola", "Samuele Delpero", "Giovanni Dispoto", "Paolo Bonetti", "Alessio Russo", "Giuseppe Calcagno", "Francesco Trovò", "Matteo Papini", "Alberto Maria Metelli", "Marco Mussi", "Marcello Restelli"], "summary": "In recent years, \\emph{Reinforcement Learning} (RL) has made remarkable\nprogress, achieving superhuman performance in a wide range of simulated\nenvironments. As research moves toward deploying RL in real-world applications,\nthe field faces a new set of challenges inherent to real-world settings, such\nas large state-action spaces, non-stationarity, and partial observability.\nDespite their importance, these challenges are often underexplored in current\nbenchmarks, which tend to focus on idealized, fully observable, and stationary\nenvironments, often neglecting to incorporate real-world complexities\nexplicitly. In this paper, we introduce \\texttt{Gym4ReaL}, a comprehensive\nsuite of realistic environments designed to support the development and\nevaluation of RL algorithms that can operate in real-world scenarios. The suite\nincludes a diverse set of tasks that expose algorithms to a variety of\npractical challenges. Our experimental results show that, in these settings,\nstandard RL algorithms confirm their competitiveness against rule-based\nbenchmarks, motivating the development of new methods to fully exploit the\npotential of RL to tackle the complexities of real-world tasks.", "comment": "9 pages", "pdf_url": "http://arxiv.org/pdf/2507.00257v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00257v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00736", "title": "Ordinality in Discrete-level Question Difficulty Estimation: Introducing Balanced DRPS and OrderedLogitNN", "authors": ["Arthur Thuy", "Ekaterina Loginova", "Dries F. Benoit"], "summary": "Recent years have seen growing interest in Question Difficulty Estimation\n(QDE) using natural language processing techniques. Question difficulty is\noften represented using discrete levels, framing the task as ordinal regression\ndue to the inherent ordering from easiest to hardest. However, the literature\nhas neglected the ordinal nature of the task, relying on classification or\ndiscretized regression models, with specialized ordinal regression methods\nremaining unexplored. Furthermore, evaluation metrics are tightly coupled to\nthe modeling paradigm, hindering cross-study comparability. While some metrics\nfail to account for the ordinal structure of difficulty levels, none adequately\naddress class imbalance, resulting in biased performance assessments. This\nstudy addresses these limitations by benchmarking three types of model outputs\n-- discretized regression, classification, and ordinal regression -- using the\nbalanced Discrete Ranked Probability Score (DRPS), a novel metric that jointly\ncaptures ordinality and class imbalance. In addition to using popular ordinal\nregression methods, we propose OrderedLogitNN, extending the ordered logit\nmodel from econometrics to neural networks. We fine-tune BERT on the RACE++ and\nARC datasets and find that OrderedLogitNN performs considerably better on\ncomplex tasks. The balanced DRPS offers a robust and fair evaluation metric for\ndiscrete-level QDE, providing a principled foundation for future research.", "comment": "Published in the EvalLAC'25 workshop at AIED 2025", "pdf_url": "http://arxiv.org/pdf/2507.00736v1", "categories": ["cs.LG", "stat.ML"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00736v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00822", "title": "Instant Particle Size Distribution Measurement Using CNNs Trained on Synthetic Data", "authors": ["Yasser El Jarida", "Youssef Iraqi", "Loubna Mekouar"], "summary": "Accurate particle size distribution (PSD) measurement is important in\nindustries such as mining, pharmaceuticals, and fertilizer manufacturing,\nsignificantly influencing product quality and operational efficiency.\nTraditional PSD methods like sieve analysis and laser diffraction are manual,\ntime-consuming, and limited by particle overlap. Recent developments in\nconvolutional neural networks (CNNs) enable automated, real-time PSD estimation\ndirectly from particle images. In this work, we present a CNN-based methodology\ntrained on realistic synthetic particle imagery generated using Blender's\nadvanced rendering capabilities. Synthetic data sets using this method can\nreplicate various industrial scenarios by systematically varying particle\nshapes, textures, lighting, and spatial arrangements that closely resemble the\nactual configurations. We evaluated three CNN-based architectures, ResNet-50,\nInceptionV3, and EfficientNet-B0, for predicting critical PSD parameters (d10,\nd50, d90). Results demonstrated comparable accuracy across models, with\nEfficientNet-B0 achieving the best computational efficiency suitable for\nreal-time industrial deployment. This approach shows the effectiveness of\nrealistic synthetic data for robust CNN training, which offers significant\npotential for automated industrial PSD monitoring. The code is released at :\nhttps://github.com/YasserElj/Synthetic-Granular-Gen", "comment": "Accepted at the Synthetic Data for Computer Vision Workshop @ CVPR\n  2025. 10 pages, 5 figures. Code available at\n  https://github.com/YasserElj/Synthetic-Granular-Gen", "pdf_url": "http://arxiv.org/pdf/2507.00822v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00822v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00258", "title": "Impact of Fine-Tuning Methods on Memorization in Large Language Models", "authors": ["Jie Hou", "Chuxiong Wu", "Lannan Luo", "Qiang Zeng"], "summary": "As the capabilities of pre-trained large language models (LLMs) continue to\nadvance, the \"pre-train and fine-tune\" paradigm has become increasingly\nmainstream, leading to the development of various fine-tuning methods. However,\nthe privacy risks arising from memorization during fine-tuning have received\nrelatively little attention. To address this gap, we categorize popular\nfine-tuning approaches and assess their impact on memorization through the lens\nof membership inference attacks (MIAs). Our results show that, compared to\nparameter-based fine-tuning, prompt-based fine-tuning achieves competitive\nperformance while exhibiting lower vulnerability to MIAs. Furthermore,\nprompt-based methods maintain low memorization regardless of model scale. These\nfindings suggest that parameter-based fine-tuning is more prone to leaking\nprivate information, whereas prompt-based fine-tuning serves as a more\nprivacy-preserving option.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00258v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2507.00258v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00742", "title": "Evaluating LLMs and Prompting Strategies for Automated Hardware Diagnosis from Textual User-Reports", "authors": ["Carlos Caminha", "Maria de Lourdes M. Silva", "Iago C. Chaves", "Felipe T. Brito", "Victor A. E. Farias", "Javam C. Machado"], "summary": "Computer manufacturers offer platforms for users to describe device faults\nusing textual reports such as \"My screen is flickering\". Identifying the faulty\ncomponent from the report is essential for automating tests and improving user\nexperience. However, such reports are often ambiguous and lack detail, making\nthis task challenging. Large Language Models (LLMs) have shown promise in\naddressing such issues. This study evaluates 27 open-source models (1B-72B\nparameters) and 2 proprietary LLMs using four prompting strategies: Zero-Shot,\nFew-Shot, Chain-of-Thought (CoT), and CoT+Few-Shot (CoT+FS). We conducted\n98,948 inferences, processing over 51 million input tokens and generating 13\nmillion output tokens. We achieve f1-score up to 0.76. Results show that three\nmodels offer the best balance between size and performance:\nmistral-small-24b-instruct and two smaller models, llama-3.2-1b-instruct and\ngemma-2-2b-it, that offer competitive performance with lower VRAM usage,\nenabling efficient inference on end-user devices as modern laptops or\nsmartphones with NPUs.", "comment": "To be published in the Proceedings of the Brazilian Integrated\n  Software and Hardware Seminar 2025 (SEMISH 2025)", "pdf_url": "http://arxiv.org/pdf/2507.00742v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00742v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00825", "title": "High-Frequency Semantics and Geometric Priors for End-to-End Detection Transformers in Challenging UAV Imagery", "authors": ["Hongxing Peng", "Lide Chen", "Hui Zhu", "Yan Chen"], "summary": "Unmanned Aerial Vehicle-based Object Detection (UAV-OD) faces substantial\nchallenges, including small target sizes, high-density distributions, and\ncluttered backgrounds in UAV imagery. Current algorithms often depend on\nhand-crafted components like anchor boxes, which demand fine-tuning and exhibit\nlimited generalization, and Non-Maximum Suppression (NMS), which is\nthreshold-sensitive and prone to misclassifying dense objects. These generic\narchitectures thus struggle to adapt to aerial imaging characteristics,\nresulting in performance limitations. Moreover, emerging end-to-end frameworks\nhave yet to effectively mitigate these aerial-specific challenges.To address\nthese issues, we propose HEGS-DETR, a comprehensively enhanced, real-time\nDetection Transformer framework tailored for UAVs. First, we introduce the\nHigh-Frequency Enhanced Semantics Network (HFESNet) as a novel backbone.\nHFESNet preserves critical high-frequency spatial details to extract robust\nsemantic features, thereby improving discriminative capability for small and\noccluded targets in complex backgrounds. Second, our Efficient Small Object\nPyramid (ESOP) strategy strategically fuses high-resolution feature maps with\nminimal computational overhead, significantly boosting small object detection.\nFinally, the proposed Selective Query Recollection (SQR) and Geometry-Aware\nPositional Encoding (GAPE) modules enhance the detector's decoder stability and\nlocalization accuracy, effectively optimizing bounding boxes and providing\nexplicit spatial priors for dense scenes. Experiments on the VisDrone dataset\ndemonstrate that HEGS-DETR achieves a 5.1\\% AP$_{50}$ and 3.8\\% AP increase\nover the baseline, while maintaining real-time speed and reducing parameter\ncount by 4M.", "comment": "14 pages, 9 figures, to appear in KBS", "pdf_url": "http://arxiv.org/pdf/2507.00825v1", "categories": ["cs.CV", "I.2.10; I.4.8; I.5.1"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00825v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00268", "title": "Control-Optimized Deep Reinforcement Learning for Artificially Intelligent Autonomous Systems", "authors": ["Oren Fivel", "Matan Rudman", "Kobi Cohen"], "summary": "Deep reinforcement learning (DRL) has become a powerful tool for complex\ndecision-making in machine learning and AI. However, traditional methods often\nassume perfect action execution, overlooking the uncertainties and deviations\nbetween an agent's selected actions and the actual system response. In\nreal-world applications, such as robotics, mechatronics, and communication\nnetworks, execution mismatches arising from system dynamics, hardware\nconstraints, and latency can significantly degrade performance. This work\nadvances AI by developing a novel control-optimized DRL framework that\nexplicitly models and compensates for action execution mismatches, a challenge\nlargely overlooked in existing methods. Our approach establishes a structured\ntwo-stage process: determining the desired action and selecting the appropriate\ncontrol signal to ensure proper execution. It trains the agent while accounting\nfor action mismatches and controller corrections. By incorporating these\nfactors into the training process, the AI agent optimizes the desired action\nwith respect to both the actual control signal and the intended outcome,\nexplicitly considering execution errors. This approach enhances robustness,\nensuring that decision-making remains effective under real-world uncertainties.\nOur approach offers a substantial advancement for engineering practice by\nbridging the gap between idealized learning and real-world implementation. It\nequips intelligent agents operating in engineering environments with the\nability to anticipate and adjust for actuation errors and system disturbances\nduring training. We evaluate the framework in five widely used open-source\nmechanical simulation environments we restructured and developed to reflect\nreal-world operating conditions, showcasing its robustness against\nuncertainties and offering a highly practical and efficient solution for\ncontrol-oriented applications.", "comment": "27 pages, 10 figures", "pdf_url": "http://arxiv.org/pdf/2507.00268v1", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2507.00268v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00761", "title": "A Probabilistic Approach to Wildfire Spread Prediction Using a Denoising Diffusion Surrogate Model", "authors": ["Wenbo Yu", "Anirbit Ghosh", "Tobias Sebastian Finn", "Rossella Arcucci", "Marc Bocquet", "Sibo Cheng"], "summary": "Thanks to recent advances in generative AI, computers can now simulate\nrealistic and complex natural processes. We apply this capability to predict\nhow wildfires spread, a task made difficult by the unpredictable nature of fire\nand the variety of environmental conditions it depends on. In this study, We\npresent the first denoising diffusion model for predicting wildfire spread, a\nnew kind of AI framework that learns to simulate fires not just as one fixed\noutcome, but as a range of possible scenarios. By doing so, it accounts for the\ninherent uncertainty of wildfire dynamics, a feature that traditional models\ntypically fail to represent. Unlike deterministic approaches that generate a\nsingle prediction, our model produces ensembles of forecasts that reflect\nphysically meaningful distributions of where fire might go next. This\ntechnology could help us develop smarter, faster, and more reliable tools for\nanticipating wildfire behavior, aiding decision-makers in fire risk assessment\nand response planning.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00761v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00761v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00845", "title": "Do Echo Top Heights Improve Deep Learning Nowcasts?", "authors": ["Peter Pavlík", "Marc Schleiss", "Anna Bou Ezzeddine", "Viera Rozinajová"], "summary": "Precipitation nowcasting -- the short-term prediction of rainfall using\nrecent radar observations -- is critical for weather-sensitive sectors such as\ntransportation, agriculture, and disaster mitigation. While recent deep\nlearning models have shown promise in improving nowcasting skill, most\napproaches rely solely on 2D radar reflectivity fields, discarding valuable\nvertical information available in the full 3D radar volume. In this work, we\nexplore the use of Echo Top Height (ETH), a 2D projection indicating the\nmaximum altitude of radar reflectivity above a given threshold, as an auxiliary\ninput variable for deep learning-based nowcasting. We examine the relationship\nbetween ETH and radar reflectivity, confirming its relevance for predicting\nrainfall intensity. We implement a single-pass 3D U-Net that processes both the\nradar reflectivity and ETH as separate input channels. While our models are\nable to leverage ETH to improve skill at low rain-rate thresholds, results are\ninconsistent at higher intensities and the models with ETH systematically\nunderestimate precipitation intensity. Three case studies are used to\nillustrate how ETH can help in some cases, but also confuse the models and\nincrease the error variance. Nonetheless, the study serves as a foundation for\ncritically assessing the potential contribution of additional variables to\nnowcasting performance.", "comment": "Pre-review version of an article accepted at Transactions on\n  Large-Scale Data and Knowledge-Centered Systems", "pdf_url": "http://arxiv.org/pdf/2507.00845v1", "categories": ["cs.CV", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00845v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00269", "title": "Feature Integration Spaces: Joint Training Reveals Dual Encoding in Neural Network Representations", "authors": ["Omar Claflin"], "summary": "Current sparse autoencoder (SAE) approaches to neural network\ninterpretability assume that activations can be decomposed through linear\nsuperposition into sparse, interpretable features. Despite high reconstruction\nfidelity, SAEs consistently fail to eliminate polysemanticity and exhibit\npathological behavioral errors. We propose that neural networks encode\ninformation in two complementary spaces compressed into the same substrate:\nfeature identity and feature integration. To test this dual encoding\nhypothesis, we develop sequential and joint-training architectures to capture\nidentity and integration patterns simultaneously. Joint training achieves 41.3%\nreconstruction improvement and 51.6% reduction in KL divergence errors. This\narchitecture spontaneously develops bimodal feature organization: low squared\nnorm features contributing to integration pathways and the rest contributing\ndirectly to the residual. Small nonlinear components (3% of parameters) achieve\n16.5% standalone improvements, demonstrating parameter-efficient capture of\ncomputational relationships crucial for behavior. Additionally, intervention\nexperiments using 2x2 factorial stimulus designs demonstrated that integration\nfeatures exhibit selective sensitivity to experimental manipulations and\nproduce systematic behavioral effects on model outputs, including significant\ninteraction effects across semantic dimensions. This work provides systematic\nevidence for (1) dual encoding in neural representations, (2) meaningful\nnonlinearly encoded feature interactions, and (3) introduces an architectural\nparadigm shift from post-hoc feature analysis to integrated computational\ndesign, establishing foundations for next-generation SAEs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00269v1", "categories": ["q-bio.NC", "cs.AI"], "cate": "q-bio.NC", "url": "http://arxiv.org/abs/2507.00269v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00762", "title": "Leveraging Genetic Algorithms for Efficient Demonstration Generation in Real-World Reinforcement Learning Environments", "authors": ["Tom Maus", "Asma Atamna", "Tobias Glasmachers"], "summary": "Reinforcement Learning (RL) has demonstrated significant potential in certain\nreal-world industrial applications, yet its broader deployment remains limited\nby inherent challenges such as sample inefficiency and unstable learning\ndynamics. This study investigates the utilization of Genetic Algorithms (GAs)\nas a mechanism for improving RL performance in an industrially inspired sorting\nenvironment. We propose a novel approach in which GA-generated expert\ndemonstrations are used to enhance policy learning. These demonstrations are\nincorporated into a Deep Q-Network (DQN) replay buffer for experience-based\nlearning and utilized as warm-start trajectories for Proximal Policy\nOptimization (PPO) agents to accelerate training convergence. Our experiments\ncompare standard RL training with rule-based heuristics, brute-force\noptimization, and demonstration data, revealing that GA-derived demonstrations\nsignificantly improve RL performance. Notably, PPO agents initialized with\nGA-generated data achieved superior cumulative rewards, highlighting the\npotential of hybrid learning paradigms, where heuristic search methods\ncomplement data-driven RL. The utilized framework is publicly available and\nenables further research into adaptive RL strategies for real-world\napplications.", "comment": "This article has been submitted to and accepted for presentation at\n  the 11th International Conference on Machine Learning, Optimization, and Data\n  Science (LOD 2025). After publication, it will appear in the official LOD\n  2025 proceedings", "pdf_url": "http://arxiv.org/pdf/2507.00762v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00762v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00849", "title": "UAVD-Mamba: Deformable Token Fusion Vision Mamba for Multimodal UAV Detection", "authors": ["Wei Li", "Jiaman Tang", "Yang Li", "Beihao Xia", "Ligang Tan", "Hongmao Qin"], "summary": "Unmanned Aerial Vehicle (UAV) object detection has been widely used in\ntraffic management, agriculture, emergency rescue, etc. However, it faces\nsignificant challenges, including occlusions, small object sizes, and irregular\nshapes. These challenges highlight the necessity for a robust and efficient\nmultimodal UAV object detection method. Mamba has demonstrated considerable\npotential in multimodal image fusion. Leveraging this, we propose UAVD-Mamba, a\nmultimodal UAV object detection framework based on Mamba architectures. To\nimprove geometric adaptability, we propose the Deformable Token Mamba Block\n(DTMB) to generate deformable tokens by incorporating adaptive patches from\ndeformable convolutions alongside normal patches from normal convolutions,\nwhich serve as the inputs to the Mamba Block. To optimize the multimodal\nfeature complementarity, we design two separate DTMBs for the RGB and infrared\n(IR) modalities, with the outputs from both DTMBs integrated into the Mamba\nBlock for feature extraction and into the Fusion Mamba Block for feature\nfusion. Additionally, to improve multiscale object detection, especially for\nsmall objects, we stack four DTMBs at different scales to produce multiscale\nfeature representations, which are then sent to the Detection Neck for Mamba\n(DNM). The DNM module, inspired by the YOLO series, includes modifications to\nthe SPPF and C3K2 of YOLOv11 to better handle the multiscale features. In\nparticular, we employ cross-enhanced spatial attention before the DTMB and\ncross-channel attention after the Fusion Mamba Block to extract more\ndiscriminative features. Experimental results on the DroneVehicle dataset show\nthat our method outperforms the baseline OAFA method by 3.6% in the mAP metric.\nCodes will be released at https://github.com/GreatPlum-hnu/UAVD-Mamba.git.", "comment": "The paper was accepted by the 36th IEEE Intelligent Vehicles\n  Symposium (IEEE IV 2025)", "pdf_url": "http://arxiv.org/pdf/2507.00849v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00849v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00275", "title": "Double Q-learning for Value-based Deep Reinforcement Learning, Revisited", "authors": ["Prabhat Nagarajan", "Martha White", "Marlos C. Machado"], "summary": "Overestimation is pervasive in reinforcement learning (RL), including in\nQ-learning, which forms the algorithmic basis for many value-based deep RL\nalgorithms. Double Q-learning is an algorithm introduced to address\nQ-learning's overestimation by training two Q-functions and using both to\nde-correlate action-selection and action-evaluation in bootstrap targets.\nShortly after Q-learning was adapted to deep RL in the form of deep Q-networks\n(DQN), Double Q-learning was adapted to deep RL in the form of Double DQN.\nHowever, Double DQN only loosely adapts Double Q-learning, forgoing the\ntraining of two different Q-functions that bootstrap off one another. In this\npaper, we study algorithms that adapt this core idea of Double Q-learning for\nvalue-based deep RL. We term such algorithms Deep Double Q-learning (DDQL). Our\naim is to understand whether DDQL exhibits less overestimation than Double DQN\nand whether performant instantiations of DDQL exist. We answer both questions\naffirmatively, demonstrating that DDQL reduces overestimation and outperforms\nDouble DQN in aggregate across 57 Atari 2600 games, without requiring\nadditional hyperparameters. We also study several aspects of DDQL, including\nits network architecture, replay ratio, and minibatch sampling strategy.", "comment": "44 pages", "pdf_url": "http://arxiv.org/pdf/2507.00275v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00275v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00846", "title": "BoltzNCE: Learning Likelihoods for Boltzmann Generation with Stochastic Interpolants and Noise Contrastive Estimation", "authors": ["Rishal Aggrwal", "Jacky Chen", "Nicholas M. Boffi", "David Ryan Koes"], "summary": "Efficient sampling from the Boltzmann distribution defined by an energy\nfunction is a key challenge in modeling physical systems such as molecules.\nBoltzmann Generators tackle this by leveraging Continuous Normalizing Flows\nthat transform a simple prior into a distribution that can be reweighted to\nmatch the Boltzmann distribution using sample likelihoods. However, obtaining\nlikelihoods requires computing costly Jacobians during integration, making it\nimpractical for large molecular systems. To overcome this, we propose learning\nthe likelihood of the generated distribution via an energy-based model trained\nwith noise contrastive estimation and score matching. By using stochastic\ninterpolants to anneal between the prior and generated distributions, we\ncombine both the objective functions to efficiently learn the density function.\nOn the alanine dipeptide system, we demonstrate that our method yields free\nenergy profiles and energy distributions comparable to those obtained with\nexact likelihoods. Additionally, we show that free energy differences between\nmetastable states can be estimated accurately with orders-of-magnitude speedup.", "comment": "19 pages, 25 figures, submitted to NeurIPS 2025", "pdf_url": "http://arxiv.org/pdf/2507.00846v1", "categories": ["cs.LG", "physics.bio-ph"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00846v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00852", "title": "Robust Component Detection for Flexible Manufacturing: A Deep Learning Approach to Tray-Free Object Recognition under Variable Lighting", "authors": ["Fatemeh Sadat Daneshmand"], "summary": "Flexible manufacturing systems in Industry 4.0 require robots capable of\nhandling objects in unstructured environments without rigid positioning\nconstraints. This paper presents a computer vision system that enables\nindustrial robots to detect and grasp pen components in arbitrary orientations\nwithout requiring structured trays, while maintaining robust performance under\nvarying lighting conditions. We implement and evaluate a Mask R-CNN-based\napproach on a complete pen manufacturing line at ZHAW, addressing three\ncritical challenges: object detection without positional constraints,\nrobustness to extreme lighting variations, and reliable performance with\ncost-effective cameras. Our system achieves 95% detection accuracy across\ndiverse lighting conditions while eliminating the need for structured component\nplacement, demonstrating a 30% reduction in setup time and significant\nimprovement in manufacturing flexibility. The approach is validated through\nextensive testing under four distinct lighting scenarios, showing practical\napplicability for real-world industrial deployment.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00852v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00852v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00286", "title": "Visual Privacy Management with Generative AI for Blind and Low-Vision People", "authors": ["Tanusree Sharma", "Yu-Yun Tseng", "Lotus Zhang", "Ayae Ide", "Kelly Avery Mack", "Leah Findlater", "Danna Gurari", "Yang Wang"], "summary": "Blind and low vision (BLV) individuals use Generative AI (GenAI) tools to\ninterpret and manage visual content in their daily lives. While such tools can\nenhance the accessibility of visual content and so enable greater user\nindependence, they also introduce complex challenges around visual privacy. In\nthis paper, we investigate the current practices and future design preferences\nof blind and low vision individuals through an interview study with 21\nparticipants. Our findings reveal a range of current practices with GenAI that\nbalance privacy, efficiency, and emotional agency, with users accounting for\nprivacy risks across six key scenarios, such as self-presentation,\nindoor/outdoor spatial privacy, social sharing, and handling professional\ncontent. Our findings reveal design preferences, including on-device\nprocessing, zero-retention guarantees, sensitive content redaction,\nprivacy-aware appearance indicators, and multimodal tactile mirrored\ninteraction methods. We conclude with actionable design recommendations to\nsupport user-centered visual privacy through GenAI, expanding the notion of\nprivacy and responsible handling of others data.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00286v1", "categories": ["cs.HC", "cs.AI", "cs.ET"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2507.00286v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00848", "title": "Quantum Approximate Optimization Algorithm for Spatiotemporal Forecasting of HIV Clusters", "authors": ["Don Roosan", "Saif Nirzhor", "Rubayat Khan", "Fahmida Hai", "Mohammad Rifat Haidar"], "summary": "HIV epidemiological data is increasingly complex, requiring advanced\ncomputation for accurate cluster detection and forecasting. We employed\nquantum-accelerated machine learning to analyze HIV prevalence at the ZIP-code\nlevel using AIDSVu and synthetic SDoH data for 2022. Our approach compared\nclassical clustering (DBSCAN, HDBSCAN) with a quantum approximate optimization\nalgorithm (QAOA), developed a hybrid quantum-classical neural network for HIV\nprevalence forecasting, and used quantum Bayesian networks to explore causal\nlinks between SDoH factors and HIV incidence. The QAOA-based method achieved\n92% accuracy in cluster detection within 1.6 seconds, outperforming classical\nalgorithms. Meanwhile, the hybrid quantum-classical neural network predicted\nHIV prevalence with 94% accuracy, surpassing a purely classical counterpart.\nQuantum Bayesian analysis identified housing instability as a key driver of HIV\ncluster emergence and expansion, with stigma exerting a geographically variable\ninfluence. These quantum-enhanced methods deliver greater precision and\nefficiency in HIV surveillance while illuminating critical causal pathways.\nThis work can guide targeted interventions, optimize resource allocation for\nPrEP, and address structural inequities fueling HIV transmission.", "comment": "Conference details can be found here:\n  https://www.insticc.org/node/technicalprogram/DATA/2025", "pdf_url": "http://arxiv.org/pdf/2507.00848v1", "categories": ["cs.LG", "q-bio.MN"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00848v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00861", "title": "SafeMap: Robust HD Map Construction from Incomplete Observations", "authors": ["Xiaoshuai Hao", "Lingdong Kong", "Rong Yin", "Pengwei Wang", "Jing Zhang", "Yunfeng Diao", "Shu Zhao"], "summary": "Robust high-definition (HD) map construction is vital for autonomous driving,\nyet existing methods often struggle with incomplete multi-view camera data.\nThis paper presents SafeMap, a novel framework specifically designed to secure\naccuracy even when certain camera views are missing. SafeMap integrates two key\ncomponents: the Gaussian-based Perspective View Reconstruction (G-PVR) module\nand the Distillation-based Bird's-Eye-View (BEV) Correction (D-BEVC) module.\nG-PVR leverages prior knowledge of view importance to dynamically prioritize\nthe most informative regions based on the relationships among available camera\nviews. Furthermore, D-BEVC utilizes panoramic BEV features to correct the BEV\nrepresentations derived from incomplete observations. Together, these\ncomponents facilitate the end-to-end map reconstruction and robust HD map\ngeneration. SafeMap is easy to implement and integrates seamlessly into\nexisting systems, offering a plug-and-play solution for enhanced robustness.\nExperimental results demonstrate that SafeMap significantly outperforms\nprevious methods in both complete and incomplete scenarios, highlighting its\nsuperior performance and reliability.", "comment": "Accepted by ICML 2025", "pdf_url": "http://arxiv.org/pdf/2507.00861v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00861v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00287", "title": "Self-Supervised Multiview Xray Matching", "authors": ["Mohamad Dabboussi", "Malo Huard", "Yann Gousseau", "Pietro Gori"], "summary": "Accurate interpretation of multi-view radiographs is crucial for diagnosing\nfractures, muscular injuries, and other anomalies. While significant advances\nhave been made in AI-based analysis of single images, current methods often\nstruggle to establish robust correspondences between different X-ray views, an\nessential capability for precise clinical evaluations. In this work, we present\na novel self-supervised pipeline that eliminates the need for manual annotation\nby automatically generating a many-to-many correspondence matrix between\nsynthetic X-ray views. This is achieved using digitally reconstructed\nradiographs (DRR), which are automatically derived from unannotated CT volumes.\nOur approach incorporates a transformer-based training phase to accurately\npredict correspondences across two or more X-ray views. Furthermore, we\ndemonstrate that learning correspondences among synthetic X-ray views can be\nleveraged as a pretraining strategy to enhance automatic multi-view fracture\ndetection on real data. Extensive evaluations on both synthetic and real X-ray\ndatasets show that incorporating correspondences improves performance in\nmulti-view fracture classification.", "comment": "MICCAI 2025", "pdf_url": "http://arxiv.org/pdf/2507.00287v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00287v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00851", "title": "Aligning Learning and Endogenous Decision-Making", "authors": ["Rares Cristian", "Pavithra Harsha", "Georgia Perakis", "Brian Quanz"], "summary": "Many of the observations we make are biased by our decisions. For instance,\nthe demand of items is impacted by the prices set, and online checkout choices\nare influenced by the assortments presented. The challenge in decision-making\nunder this setting is the lack of counterfactual information, and the need to\nlearn it instead. We introduce an end-to-end method under endogenous\nuncertainty to train ML models to be aware of their downstream, enabling their\neffective use in the decision-making stage. We further introduce a robust\noptimization variant that accounts for uncertainty in ML models -- specifically\nby constructing uncertainty sets over the space of ML models and optimizing\nactions to protect against worst-case predictions. We prove guarantees that\nthis robust approach can capture near-optimal decisions with high probability\nas a function of data. Besides this, we also introduce a new class of two-stage\nstochastic optimization problems to the end-to-end learning framework that can\nnow be addressed through our framework. Here, the first stage is an\ninformation-gathering problem to decide which random variable to poll and gain\ninformation about before making a second-stage decision based off of it. We\npresent several computational experiments for pricing and inventory\nassortment/recommendation problems. We compare against existing methods in\nonline learning/bandits/offline reinforcement learning and show our approach\nhas consistent improved performance over these. Just as in the endogenous\nsetting, the model's prediction also depends on the first-stage decision made.\nWhile this decision does not affect the random variable in this setting, it\ndoes affect the correct point forecast that should be made.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00851v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00851v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00868", "title": "Is Visual in-Context Learning for Compositional Medical Tasks within Reach?", "authors": ["Simon Reiß", "Zdravko Marinov", "Alexander Jaus", "Constantin Seibold", "M. Saquib Sarfraz", "Erik Rodner", "Rainer Stiefelhagen"], "summary": "In this paper, we explore the potential of visual in-context learning to\nenable a single model to handle multiple tasks and adapt to new tasks during\ntest time without re-training. Unlike previous approaches, our focus is on\ntraining in-context learners to adapt to sequences of tasks, rather than\nindividual tasks. Our goal is to solve complex tasks that involve multiple\nintermediate steps using a single model, allowing users to define entire vision\npipelines flexibly at test time. To achieve this, we first examine the\nproperties and limitations of visual in-context learning architectures, with a\nparticular focus on the role of codebooks. We then introduce a novel method for\ntraining in-context learners using a synthetic compositional task generation\nengine. This engine bootstraps task sequences from arbitrary segmentation\ndatasets, enabling the training of visual in-context learners for compositional\ntasks. Additionally, we investigate different masking-based training objectives\nto gather insights into how to train models better for solving complex,\ncompositional tasks. Our exploration not only provides important insights\nespecially for multi-modal medical task sequences but also highlights\nchallenges that need to be addressed.", "comment": "Accepted to ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.00868v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00868v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00288", "title": "Reconfiguring Digital Accountability: AI-Powered Innovations and Transnational Governance in a Postnational Accounting Context", "authors": ["Claire Li", "David Freeborn"], "summary": "This study explores how AI-powered digital innovations are reshaping\norganisational accountability in a transnational governance context. As AI\nsystems increasingly mediate decision-making in domains such as auditing and\nfinancial reporting, traditional mechanisms of accountability, based on\ncontrol, transparency, and auditability, are being destabilised. We integrate\nthe Technology Acceptance Model (TAM), Actor-Network Theory (ANT), and\ninstitutional theory to examine how organisations adopt AI technologies in\nresponse to regulatory, ethical, and cultural pressures that transcend national\nboundaries. We argue that accountability is co-constructed within global\nsocio-technical networks, shaped not only by user perceptions but also by\ngovernance logics and normative expectations. Extending TAM, we incorporate\ncompliance and legitimacy as key factors in perceived usefulness and usability.\nDrawing on ANT, we reconceptualise accountability as a relational and emergent\nproperty of networked assemblages. We propose two organisational strategies\nincluding internal governance reconfiguration and external actor-network\nengagement to foster responsible, legitimate, and globally accepted AI adoption\nin the accounting domain.", "comment": "22 pages", "pdf_url": "http://arxiv.org/pdf/2507.00288v1", "categories": ["econ.TH", "cs.AI", "cs.ET"], "cate": "econ.TH", "url": "http://arxiv.org/abs/2507.00288v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00862", "title": "Machine Learning-based Early Detection of Potato Sprouting Using Electrophysiological Signals", "authors": ["Davide Andreoletti", "Aris Marcolongo", "Natasa Sarafijanovic Djukic", "Julien Roulet", "Stefano Billeter", "Andrzej Kurenda", "Margot Visse-Mansiaux", "Brice Dupuis", "Carrol Annette Plummer", "Beatrice Paoli", "Omran Ayoub"], "summary": "Accurately predicting potato sprouting before the emergence of any visual\nsigns is critical for effective storage management, as sprouting degrades both\nthe commercial and nutritional value of tubers. Effective forecasting allows\nfor the precise application of anti-sprouting chemicals (ASCs), minimizing\nwaste and reducing costs. This need has become even more pressing following the\nban on Isopropyl N-(3-chlorophenyl) carbamate (CIPC) or Chlorpropham due to\nhealth and environmental concerns, which has led to the adoption of\nsignificantly more expensive alternative ASCs. Existing approaches primarily\nrely on visual identification, which only detects sprouting after morphological\nchanges have occurred, limiting their effectiveness for proactive management. A\nreliable early prediction method is therefore essential to enable timely\nintervention and improve the efficiency of post-harvest storage strategies,\nwhere early refers to detecting sprouting before any visible signs appear. In\nthis work, we address the problem of early prediction of potato sprouting. To\nthis end, we propose a novel machine learning (ML)-based approach that enables\nearly prediction of potato sprouting using electrophysiological signals\nrecorded from tubers using proprietary sensors. Our approach preprocesses the\nrecorded signals, extracts relevant features from the wavelet domain, and\ntrains supervised ML models for early sprouting detection. Additionally, we\nincorporate uncertainty quantification techniques to enhance predictions.\nExperimental results demonstrate promising performance in the early detection\nof potato sprouting by accurately predicting the exact day of sprouting for a\nsubset of potatoes and while showing acceptable average error across all\npotatoes. Despite promising results, further refinements are necessary to\nminimize prediction errors, particularly in reducing the maximum observed\ndeviations.", "comment": "8 pages, 7 figures", "pdf_url": "http://arxiv.org/pdf/2507.00862v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00862v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00886", "title": "GaussianVLM: Scene-centric 3D Vision-Language Models using Language-aligned Gaussian Splats for Embodied Reasoning and Beyond", "authors": ["Anna-Maria Halacheva", "Jan-Nico Zaech", "Xi Wang", "Danda Pani Paudel", "Luc Van Gool"], "summary": "As multimodal language models advance, their application to 3D scene\nunderstanding is a fast-growing frontier, driving the development of 3D\nVision-Language Models (VLMs). Current methods show strong dependence on object\ndetectors, introducing processing bottlenecks and limitations in taxonomic\nflexibility. To address these limitations, we propose a scene-centric 3D VLM\nfor 3D Gaussian splat scenes that employs language- and task-aware scene\nrepresentations. Our approach directly embeds rich linguistic features into the\n3D scene representation by associating language with each Gaussian primitive,\nachieving early modality alignment. To process the resulting dense\nrepresentations, we introduce a dual sparsifier that distills them into\ncompact, task-relevant tokens via task-guided and location-guided pathways,\nproducing sparse, task-aware global and local scene tokens. Notably, we present\nthe first Gaussian splatting-based VLM, leveraging photorealistic 3D\nrepresentations derived from standard RGB images, demonstrating strong\ngeneralization: it improves performance of prior 3D VLM five folds, in\nout-of-the-domain settings.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00886v1", "categories": ["cs.CV", "cs.RO"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00886v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00292", "title": "Reducing Variability of Multiple Instance Learning Methods for Digital Pathology", "authors": ["Ali Mammadov", "Loïc Le Folgoc", "Guillaume Hocquet", "Pietro Gori"], "summary": "Digital pathology has revolutionized the field by enabling the digitization\nof tissue samples into whole slide images (WSIs). However, the high resolution\nand large size of WSIs present significant challenges when it comes to applying\nDeep Learning models. As a solution, WSIs are often divided into smaller\npatches with a global label (\\textit{i.e., diagnostic}) per slide, instead of a\n(too) costly pixel-wise annotation. By treating each slide as a bag of patches,\nMultiple Instance Learning (MIL) methods have emerged as a suitable solution\nfor WSI classification. A major drawback of MIL methods is their high\nvariability in performance across different runs, which can reach up to 10-15\nAUC points on the test set, making it difficult to compare different MIL\nmethods reliably. This variability mainly comes from three factors: i) weight\ninitialization, ii) batch (shuffling) ordering, iii) and learning rate. To\naddress that, we introduce a Multi-Fidelity, Model Fusion strategy for MIL\nmethods. We first train multiple models for a few epochs and average the most\nstable and promising ones based on validation scores. This approach can be\napplied to any existing MIL model to reduce performance variability. It also\nsimplifies hyperparameter tuning and improves reproducibility while maintaining\ncomputational efficiency. We extensively validate our approach on WSI\nclassification tasks using 2 different datasets, 3 initialization strategies\nand 5 MIL methods, for a total of more than 2000 experiments.", "comment": "MICCAI 2025", "pdf_url": "http://arxiv.org/pdf/2507.00292v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00292v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00880", "title": "NN-Former: Rethinking Graph Structure in Neural Architecture Representation", "authors": ["Ruihan Xu", "Haokui Zhang", "Yaowei Wang", "Wei Zeng", "Shiliang Zhang"], "summary": "The growing use of deep learning necessitates efficient network design and\ndeployment, making neural predictors vital for estimating attributes such as\naccuracy and latency. Recently, Graph Neural Networks (GNNs) and transformers\nhave shown promising performance in representing neural architectures. However,\neach of both methods has its disadvantages. GNNs lack the capabilities to\nrepresent complicated features, while transformers face poor generalization\nwhen the depth of architecture grows. To mitigate the above issues, we rethink\nneural architecture topology and show that sibling nodes are pivotal while\noverlooked in previous research. We thus propose a novel predictor leveraging\nthe strengths of GNNs and transformers to learn the enhanced topology. We\nintroduce a novel token mixer that considers siblings, and a new channel mixer\nnamed bidirectional graph isomorphism feed-forward network. Our approach\nconsistently achieves promising performance in both accuracy and latency\nprediction, providing valuable insights for learning Directed Acyclic Graph\n(DAG) topology. The code is available at https://github.com/XuRuihan/NNFormer.", "comment": "Accepted to CVPR 2025. Code is avaiable at\n  https://github.com/XuRuihan/NNFormer", "pdf_url": "http://arxiv.org/pdf/2507.00880v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00880v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00898", "title": "ONLY: One-Layer Intervention Sufficiently Mitigates Hallucinations in Large Vision-Language Models", "authors": ["Zifu Wan", "Ce Zhang", "Silong Yong", "Martin Q. Ma", "Simon Stepputtis", "Louis-Philippe Morency", "Deva Ramanan", "Katia Sycara", "Yaqi Xie"], "summary": "Recent Large Vision-Language Models (LVLMs) have introduced a new paradigm\nfor understanding and reasoning about image input through textual responses.\nAlthough they have achieved remarkable performance across a range of\nmulti-modal tasks, they face the persistent challenge of hallucination, which\nintroduces practical weaknesses and raises concerns about their reliable\ndeployment in real-world applications. Existing work has explored contrastive\ndecoding approaches to mitigate this issue, where the output of the original\nLVLM is compared and contrasted with that of a perturbed version. However,\nthese methods require two or more queries that slow down LVLM response\ngeneration, making them less suitable for real-time applications. To overcome\nthis limitation, we propose ONLY, a training-free decoding approach that\nrequires only a single query and a one-layer intervention during decoding,\nenabling efficient real-time deployment. Specifically, we enhance textual\noutputs by selectively amplifying crucial textual information using a\ntext-to-visual entropy ratio for each token. Extensive experimental results\ndemonstrate that our proposed ONLY consistently outperforms state-of-the-art\nmethods across various benchmarks while requiring minimal implementation effort\nand computational cost. Code is available at https://github.com/zifuwan/ONLY.", "comment": "Accepted by ICCV 2025. Project page: https://zifuwan.github.io/ONLY/", "pdf_url": "http://arxiv.org/pdf/2507.00898v1", "categories": ["cs.CV", "cs.CL"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00898v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00297", "title": "Natural language processing for African languages", "authors": ["David Ifeoluwa Adelani"], "summary": "Recent advances in word embeddings and language models use large-scale,\nunlabelled data and self-supervised learning to boost NLP performance.\nMultilingual models, often trained on web-sourced data like Wikipedia, face\nchallenges: few low-resource languages are included, their data is often noisy,\nand lack of labeled datasets makes it hard to evaluate performance outside\nhigh-resource languages like English. In this dissertation, we focus on\nlanguages spoken in Sub-Saharan Africa where all the indigenous languages in\nthis region can be regarded as low-resourced in terms of the availability of\nlabelled data for NLP tasks and unlabelled data found on the web. We analyse\nthe noise in the publicly available corpora, and curate a high-quality corpus,\ndemonstrating that the quality of semantic representations learned in word\nembeddings does not only depend on the amount of data but on the quality of\npre-training data. We demonstrate empirically the limitations of word\nembeddings, and the opportunities the multilingual pre-trained language model\n(PLM) offers especially for languages unseen during pre-training and\nlow-resource scenarios. We further study how to adapt and specialize\nmultilingual PLMs to unseen African languages using a small amount of\nmonolingual texts. To address the under-representation of the African languages\nin NLP research, we developed large scale human-annotated labelled datasets for\n21 African languages in two impactful NLP tasks: named entity recognition and\nmachine translation. We conduct an extensive empirical evaluation using\nstate-of-the-art methods across supervised, weakly-supervised, and transfer\nlearning settings.", "comment": "PhD thesis", "pdf_url": "http://arxiv.org/pdf/2507.00297v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2507.00297v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00899", "title": "TABASCO: A Fast, Simplified Model for Molecular Generation with Improved Physical Quality", "authors": ["Carlos Vonessen", "Charles Harris", "Miruna Cretu", "Pietro Liò"], "summary": "State-of-the-art models for 3D molecular generation are based on significant\ninductive biases, SE(3), permutation equivariance to respect symmetry and graph\nmessage-passing networks to capture local chemistry, yet the generated\nmolecules still struggle with physical plausibility. We introduce TABASCO which\nrelaxes these assumptions: The model has a standard non-equivariant transformer\narchitecture, treats atoms in a molecule as sequences and reconstructs bonds\ndeterministically after generation. The absence of equivariant layers and\nmessage passing allows us to significantly simplify the model architecture and\nscale data throughput. On the GEOM-Drugs benchmark TABASCO achieves\nstate-of-the-art PoseBusters validity and delivers inference roughly 10x faster\nthan the strongest baseline, while exhibiting emergent rotational equivariance\ndespite symmetry not being hard-coded. Our work offers a blueprint for training\nminimalist, high-throughput generative models suited to specialised tasks such\nas structure- and pharmacophore-based drug design. We provide a link to our\nimplementation at github.com/carlosinator/tabasco.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00899v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00899v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00916", "title": "Masks make discriminative models great again!", "authors": ["Tianshi Cao", "Marie-Julie Rakotosaona", "Ben Poole", "Federico Tombari", "Michael Niemeyer"], "summary": "We present Image2GS, a novel approach that addresses the challenging problem\nof reconstructing photorealistic 3D scenes from a single image by focusing\nspecifically on the image-to-3D lifting component of the reconstruction\nprocess. By decoupling the lifting problem (converting an image to a 3D model\nrepresenting what is visible) from the completion problem (hallucinating\ncontent not present in the input), we create a more deterministic task suitable\nfor discriminative models. Our method employs visibility masks derived from\noptimized 3D Gaussian splats to exclude areas not visible from the source view\nduring training. This masked training strategy significantly improves\nreconstruction quality in visible regions compared to strong baselines.\nNotably, despite being trained only on masked regions, Image2GS remains\ncompetitive with state-of-the-art discriminative models trained on full target\nimages when evaluated on complete scenes. Our findings highlight the\nfundamental struggle discriminative models face when fitting unseen regions and\ndemonstrate the advantages of addressing image-to-3D lifting as a distinct\nproblem with specialized techniques.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00916v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00916v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00310", "title": "Open-ended Scientific Discovery via Bayesian Surprise", "authors": ["Dhruv Agarwal", "Bodhisattwa Prasad Majumder", "Reece Adamson", "Megha Chakravorty", "Satvika Reddy Gavireddy", "Aditya Parashar", "Harshit Surana", "Bhavana Dalvi Mishra", "Andrew McCallum", "Ashish Sabharwal", "Peter Clark"], "summary": "The promise of autonomous scientific discovery (ASD) hinges not only on\nanswering questions, but also on knowing which questions to ask. Most recent\nworks in ASD explore the use of large language models (LLMs) in goal-driven\nsettings, relying on human-specified research questions to guide hypothesis\ngeneration. However, scientific discovery may be accelerated further by\nallowing the AI system to drive exploration by its own criteria. The few\nexisting approaches in open-ended ASD select hypotheses based on diversity\nheuristics or subjective proxies for human interestingness, but the former\nstruggles to meaningfully navigate the typically vast hypothesis space, and the\nlatter suffers from imprecise definitions. This paper presents AutoDS -- a\nmethod for open-ended ASD that instead drives scientific exploration using\nBayesian surprise. Here, we quantify the epistemic shift from the LLM's prior\nbeliefs about a hypothesis to its posterior beliefs after gathering\nexperimental results. To efficiently explore the space of nested hypotheses,\nour method employs a Monte Carlo tree search (MCTS) strategy with progressive\nwidening using surprisal as the reward function. We evaluate AutoDS in the\nsetting of data-driven discovery across 21 real-world datasets spanning domains\nsuch as biology, economics, finance, and behavioral science. Our results\ndemonstrate that under a fixed budget, AutoDS substantially outperforms\ncompetitors by producing 5--29\\% more discoveries deemed surprising by the LLM.\nOur human evaluation further finds that two-thirds of AutoDS discoveries are\nsurprising to the domain experts, suggesting this is an important step forward\ntowards building open-ended ASD systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00310v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00310v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00920", "title": "Privacy-Preserving Quantized Federated Learning with Diverse Precision", "authors": ["Dang Qua Nguyen", "Morteza Hashemi", "Erik Perrins", "Sergiy A. Vorobyov", "David J. Love", "Taejoon Kim"], "summary": "Federated learning (FL) has emerged as a promising paradigm for distributed\nmachine learning, enabling collaborative training of a global model across\nmultiple local devices without requiring them to share raw data. Despite its\nadvancements, FL is limited by factors such as: (i) privacy risks arising from\nthe unprotected transmission of local model updates to the fusion center (FC)\nand (ii) decreased learning utility caused by heterogeneity in model\nquantization resolution across participating devices. Prior work typically\naddresses only one of these challenges because maintaining learning utility\nunder both privacy risks and quantization heterogeneity is a non-trivial task.\nIn this paper, our aim is therefore to improve the learning utility of a\nprivacy-preserving FL that allows clusters of devices with different\nquantization resolutions to participate in each FL round. Specifically, we\nintroduce a novel stochastic quantizer (SQ) that is designed to simultaneously\nachieve differential privacy (DP) and minimum quantization error. Notably, the\nproposed SQ guarantees bounded distortion, unlike other DP approaches. To\naddress quantization heterogeneity, we introduce a cluster size optimization\ntechnique combined with a linear fusion approach to enhance model aggregation\naccuracy. Numerical simulations validate the benefits of our approach in terms\nof privacy protection and learning utility compared to the conventional\nLaplaceSQ-FL algorithm.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00920v1", "categories": ["cs.LG", "eess.SP"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00920v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00950", "title": "MVP: Winning Solution to SMP Challenge 2025 Video Track", "authors": ["Liliang Ye", "Yunyao Zhang", "Yafeng Wu", "Yi-Ping Phoebe Chen", "Junqing Yu", "Wei Yang", "Zikai Song"], "summary": "Social media platforms serve as central hubs for content dissemination,\nopinion expression, and public engagement across diverse modalities. Accurately\npredicting the popularity of social media videos enables valuable applications\nin content recommendation, trend detection, and audience engagement. In this\npaper, we present Multimodal Video Predictor (MVP), our winning solution to the\nVideo Track of the SMP Challenge 2025. MVP constructs expressive post\nrepresentations by integrating deep video features extracted from pretrained\nmodels with user metadata and contextual information. The framework applies\nsystematic preprocessing techniques, including log-transformations and outlier\nremoval, to improve model robustness. A gradient-boosted regression model is\ntrained to capture complex patterns across modalities. Our approach ranked\nfirst in the official evaluation of the Video Track, demonstrating its\neffectiveness and reliability for multimodal video popularity prediction on\nsocial platforms. The source code is available at\nhttps://anonymous.4open.science/r/SMPDVideo.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00950v1", "categories": ["cs.CV", "cs.LG", "cs.MM"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00950v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00322", "title": "Failure by Interference: Language Models Make Balanced Parentheses Errors When Faulty Mechanisms Overshadow Sound Ones", "authors": ["Daking Rai", "Samuel Miller", "Kevin Moran", "Ziyu Yao"], "summary": "Despite remarkable advances in coding capabilities, language models (LMs)\nstill struggle with simple syntactic tasks such as generating balanced\nparentheses. In this study, we investigate the underlying mechanisms behind the\npersistence of these errors across LMs of varying sizes (124M-7B) to both\nunderstand and mitigate the errors. Our study reveals that LMs rely on a number\nof components (attention heads and FF neurons) that independently make their\nown predictions. While some components reliably promote correct answers across\na generalized range of inputs (i.e., implementing \"sound mechanisms''), others\nare less reliable and introduce noise by promoting incorrect tokens (i.e.,\nimplementing \"faulty mechanisms''). Errors occur when the faulty mechanisms\novershadow the sound ones and dominantly affect the predictions. Motivated by\nthis insight, we introduce RASteer, a steering method to systematically\nidentify and increase the contribution of reliable components for improving\nmodel performance. RASteer substantially improves performance on balanced\nparentheses tasks, boosting accuracy of some models from $0$% to around $100$%\nwithout impairing the models' general coding ability. We further demonstrate\nits broader applicability in arithmetic reasoning tasks, achieving performance\ngains of up to around $20$%.", "comment": "23 pages, 10 figures, Preprint", "pdf_url": "http://arxiv.org/pdf/2507.00322v1", "categories": ["cs.CL", "cs.AI", "cs.SE", "I.2.7"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2507.00322v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00927", "title": "Understanding Generalization in Node and Link Prediction", "authors": ["Antonis Vasileiou", "Timo Stoll", "Christopher Morris"], "summary": "Using message-passing graph neural networks (MPNNs) for node and link\nprediction is crucial in various scientific and industrial domains, which has\nled to the development of diverse MPNN architectures. Besides working well in\npractical settings, their ability to generalize beyond the training set remains\npoorly understood. While some studies have explored MPNNs' generalization in\ngraph-level prediction tasks, much less attention has been given to node- and\nlink-level predictions. Existing works often rely on unrealistic i.i.d.\\@\nassumptions, overlooking possible correlations between nodes or links, and\nassuming fixed aggregation and impractical loss functions while neglecting the\ninfluence of graph structure. In this work, we introduce a unified framework to\nanalyze the generalization properties of MPNNs in inductive and transductive\nnode and link prediction settings, incorporating diverse architectural\nparameters and loss functions and quantifying the influence of graph structure.\nAdditionally, our proposed generalization framework can be applied beyond\ngraphs to any classification task under the inductive or transductive setting.\nOur empirical study supports our theoretical insights, deepening our\nunderstanding of MPNNs' generalization capabilities in these tasks.", "comment": "arXiv admin note: text overlap with arXiv:2412.07106", "pdf_url": "http://arxiv.org/pdf/2507.00927v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00927v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00969", "title": "Surgical Neural Radiance Fields from One Image", "authors": ["Alberto Neri", "Maximilan Fehrentz", "Veronica Penza", "Leonardo S. Mattos", "Nazim Haouchine"], "summary": "Purpose: Neural Radiance Fields (NeRF) offer exceptional capabilities for 3D\nreconstruction and view synthesis, yet their reliance on extensive multi-view\ndata limits their application in surgical intraoperative settings where only\nlimited data is available. In particular, collecting such extensive data\nintraoperatively is impractical due to time constraints. This work addresses\nthis challenge by leveraging a single intraoperative image and preoperative\ndata to train NeRF efficiently for surgical scenarios.\n  Methods: We leverage preoperative MRI data to define the set of camera\nviewpoints and images needed for robust and unobstructed training.\nIntraoperatively, the appearance of the surgical image is transferred to the\npre-constructed training set through neural style transfer, specifically\ncombining WTC2 and STROTSS to prevent over-stylization. This process enables\nthe creation of a dataset for instant and fast single-image NeRF training.\n  Results: The method is evaluated with four clinical neurosurgical cases.\nQuantitative comparisons to NeRF models trained on real surgical microscope\nimages demonstrate strong synthesis agreement, with similarity metrics\nindicating high reconstruction fidelity and stylistic alignment. When compared\nwith ground truth, our method demonstrates high structural similarity,\nconfirming good reconstruction quality and texture preservation.\n  Conclusion: Our approach demonstrates the feasibility of single-image NeRF\ntraining in surgical settings, overcoming the limitations of traditional\nmulti-view methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00969v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00969v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00339", "title": "Training for X-Ray Vision: Amodal Segmentation, Amodal Content Completion, and View-Invariant Object Representation from Multi-Camera Video", "authors": ["Alexander Moore", "Amar Saini", "Kylie Cancilla", "Doug Poland", "Carmen Carrano"], "summary": "Amodal segmentation and amodal content completion require using object priors\nto estimate occluded masks and features of objects in complex scenes. Until\nnow, no data has provided an additional dimension for object context: the\npossibility of multiple cameras sharing a view of a scene. We introduce\nMOVi-MC-AC: Multiple Object Video with Multi-Cameras and Amodal Content, the\nlargest amodal segmentation and first amodal content dataset to date. Cluttered\nscenes of generic household objects are simulated in multi-camera video.\nMOVi-MC-AC contributes to the growing literature of object detection, tracking,\nand segmentation by including two new contributions to the deep learning for\ncomputer vision world. Multiple Camera (MC) settings where objects can be\nidentified and tracked between various unique camera perspectives are rare in\nboth synthetic and real-world video. We introduce a new complexity to synthetic\nvideo by providing consistent object ids for detections and segmentations\nbetween both frames and multiple cameras each with unique features and motion\npatterns on a single scene. Amodal Content (AC) is a reconstructive task in\nwhich models predict the appearance of target objects through occlusions. In\nthe amodal segmentation literature, some datasets have been released with\namodal detection, tracking, and segmentation labels. While other methods rely\non slow cut-and-paste schemes to generate amodal content pseudo-labels, they do\nnot account for natural occlusions present in the modal masks. MOVi-MC-AC\nprovides labels for ~5.8 million object instances, setting a new maximum in the\namodal dataset literature, along with being the first to provide ground-truth\namodal content. The full dataset is available at\nhttps://huggingface.co/datasets/Amar-S/MOVi-MC-AC ,", "comment": "9 pages, 2 figures", "pdf_url": "http://arxiv.org/pdf/2507.00339v1", "categories": ["cs.CV", "cs.AI", "68T45, 68T07", "I.2.10; I.2.6; I.4.6"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00339v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00945", "title": "Time Series Foundation Models are Flow Predictors", "authors": ["Massimiliano Luca", "Ciro Beneduce", "Bruno Lepri"], "summary": "We investigate the effectiveness of time series foundation models (TSFMs) for\ncrowd flow prediction, focusing on Moirai and TimesFM. Evaluated on three\nreal-world mobility datasets-Bike NYC, Taxi Beijing, and Spanish national OD\nflows-these models are deployed in a strict zero-shot setting, using only the\ntemporal evolution of each OD flow and no explicit spatial information. Moirai\nand TimesFM outperform both statistical and deep learning baselines, achieving\nup to 33% lower RMSE, 39% lower MAE and up to 49% higher CPC compared to\nstate-of-the-art competitors. Our results highlight the practical value of\nTSFMs for accurate, scalable flow prediction, even in scenarios with limited\nannotated data or missing spatial context.", "comment": "arXiv admin note: text overlap with arXiv:2203.07372", "pdf_url": "http://arxiv.org/pdf/2507.00945v1", "categories": ["cs.LG", "cs.CY"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00945v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00980", "title": "RTMap: Real-Time Recursive Mapping with Change Detection and Localization", "authors": ["Yuheng Du", "Sheng Yang", "Lingxuan Wang", "Zhenghua Hou", "Chengying Cai", "Zhitao Tan", "Mingxia Chen", "Shi-Sheng Huang", "Qiang Li"], "summary": "While recent online HD mapping methods relieve burdened offline pipelines and\nsolve map freshness, they remain limited by perceptual inaccuracies, occlusion\nin dense traffic, and an inability to fuse multi-agent observations. We propose\nRTMap to enhance these single-traversal methods by persistently crowdsourcing a\nmulti-traversal HD map as a self-evolutional memory. On onboard agents, RTMap\nsimultaneously addresses three core challenges in an end-to-end fashion: (1)\nUncertainty-aware positional modeling for HD map elements, (2)\nprobabilistic-aware localization w.r.t. the crowdsourced prior-map, and (3)\nreal-time detection for possible road structural changes. Experiments on\nseveral public autonomous driving datasets demonstrate our solid performance on\nboth the prior-aided map quality and the localization accuracy, demonstrating\nour effectiveness of robustly serving downstream prediction and planning\nmodules while gradually improving the accuracy and freshness of the\ncrowdsourced prior-map asynchronously. Our source-code will be made publicly\navailable at https://github.com/CN-ADLab/RTMap (Camera ready version\nincorporating reviewer suggestions will be updated soon).", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00980v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00980v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00347", "title": "VTS-Guided AI Interaction Workflow for Business Insights", "authors": ["Sun Ding", "Ude Enebeli", "Atilhan", "Manay", "Ryan Pua", "Kamal Kotak"], "summary": "Modern firms face a flood of dense, unstructured reports. Turning these\ndocuments into usable insights takes heavy effort and is far from agile when\nquick answers are needed. VTS-AI tackles this gap. It integrates Visual\nThinking Strategies, which emphasize evidence-based observation, linking, and\nthinking, into AI agents, so the agents can extract business insights from\nunstructured text, tables, and images at scale. The system works in three tiers\n(micro, meso, macro). It tags issues, links them to source pages, and rolls\nthem into clear action levers stored in a searchable YAML file. In tests on an\n18-page business report, VTS-AI matched the speed of a one-shot ChatGPT prompt\nyet produced richer findings: page locations, verbatim excerpts, severity\nscores, and causal links. Analysts can accept or adjust these outputs in the\nsame IDE, keeping human judgment in the loop. Early results show VTS-AI spots\nthe direction of key metrics and flags where deeper number-crunching is needed.\nNext steps include mapping narrative tags to financial ratios, adding\nfinance-tuned language models through a Model-Context Protocol, and building a\nRisk & Safety Layer to stress-test models and secure data. These upgrades aim\nto make VTS-AI a production-ready, audit-friendly tool for rapid business\nanalysis.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00347v1", "categories": ["cs.SE", "cs.AI"], "cate": "cs.SE", "url": "http://arxiv.org/abs/2507.00347v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00964", "title": "Benchmarking the Discovery Engine", "authors": ["Jack Foxabbott", "Arush Tagade", "Andrew Cusick", "Robbie McCorkell", "Leo McKee-Reid", "Jugal Patel", "Jamie Rumbelow", "Jessica Rumbelow", "Zohreh Shams"], "summary": "The Discovery Engine is a general purpose automated system for scientific\ndiscovery, which combines machine learning with state-of-the-art ML\ninterpretability to enable rapid and robust scientific insight across diverse\ndatasets. In this paper, we benchmark the Discovery Engine against five recent\npeer-reviewed scientific publications applying machine learning across\nmedicine, materials science, social science, and environmental science. In each\ncase, the Discovery Engine matches or exceeds prior predictive performance\nwhile also generating deeper, more actionable insights through rich\ninterpretability artefacts. These results demonstrate its potential as a new\nstandard for automated, interpretable scientific modelling that enables complex\nknowledge discovery from data.", "comment": "16 pages, 8 figures, benchmarks Discovery Engine on five scientific\n  datasets (medicine, materials science, climate, air quality, social science)", "pdf_url": "http://arxiv.org/pdf/2507.00964v1", "categories": ["cs.LG", "I.2.6; I.2.3; I.5.1; H.2.8; J.2; J.3; J.4"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00964v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00981", "title": "Evaluating Robustness of Monocular Depth Estimation with Procedural Scene Perturbations", "authors": ["Jack Nugent", "Siyang Wu", "Zeyu Ma", "Beining Han", "Meenal Parakh", "Abhishek Joshi", "Lingjie Mei", "Alexander Raistrick", "Xinyuan Li", "Jia Deng"], "summary": "Recent years have witnessed substantial progress on monocular depth\nestimation, particularly as measured by the success of large models on standard\nbenchmarks. However, performance on standard benchmarks does not offer a\ncomplete assessment, because most evaluate accuracy but not robustness. In this\nwork, we introduce PDE (Procedural Depth Evaluation), a new benchmark which\nenables systematic robustness evaluation. PDE uses procedural generation to\ncreate 3D scenes that test robustness to various controlled perturbations,\nincluding object, camera, material and lighting changes. Our analysis yields\ninteresting findings on what perturbations are challenging for state-of-the-art\ndepth models, which we hope will inform further research. Code and data are\navailable at https://github.com/princeton-vl/proc-depth-eval.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00981v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00981v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00352", "title": "An AST-guided LLM Approach for SVRF Code Synthesis", "authors": ["Abanoub E. Abdelmalak", "Mohamed A. Elsayed", "David Abercrombie", "Ilhami Torunoglu"], "summary": "Standard Verification Rule Format (SVRF) is essential for semiconductor\napplications like Design Rule Check (DRC), Layout Versus Schematic (LVS), and\nOptical Proximity Correction (OPC) and it faces challenges as advancing nodes\ncreate complex design rules that renders traditional SVRF development\nineffective and highlight an expertise gap. This paper introduces a novel\nmethodology integrating Abstract Syntax Tree (AST) embedding and\nRetrieval-Augmented Generation (RAG) for enhanced SVRF code synthesis, ensuring\nsemantic accuracy and error minimization through structural validation with\ndomain-specific insights for precise code generation.\n  We evaluate different T5-based models and propose an innovative SVRF-specific\nscoring framework that complements standard metrics like BLEU and ROUGE-L. In\nour approach, AST provides rigorous structural validation, while RAG infuses\nrelevant domain knowledge, effectively enhancing the code generation workflow.\n  Testing on a comprehensive benchmark of 740 DRC rule implementations, our\nmethodology demonstrates up to a 40\\% improvement in code generation accuracy\ncompared to basic text-based fine-tuning process. This fusion of industry\nexpertise with advanced coding strategies not only optimizes SVRF development\nunder limited dataset constraints but also creates a more intuitive and\nefficient coding environment. Consequently, users can rapidly iterate through\ndesign cycles, reduce manual error correction, and significantly improve\noverall productivity.", "comment": "9 Pages, 5 Figures, 2 Tables", "pdf_url": "http://arxiv.org/pdf/2507.00352v1", "categories": ["cs.SE", "cs.AI", "cs.ET"], "cate": "cs.SE", "url": "http://arxiv.org/abs/2507.00352v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00965", "title": "Scalable Feature Learning on Huge Knowledge Graphs for Downstream Machine Learning", "authors": ["Félix Lefebvre", "Gaël Varoquaux"], "summary": "Many machine learning tasks can benefit from external knowledge. Large\nknowledge graphs store such knowledge, and embedding methods can be used to\ndistill it into ready-to-use vector representations for downstream\napplications. For this purpose, current models have however two limitations:\nthey are primarily optimized for link prediction, via local contrastive\nlearning, and they struggle to scale to the largest graphs due to GPU memory\nlimits. To address these, we introduce SEPAL: a Scalable Embedding Propagation\nALgorithm for large knowledge graphs designed to produce high-quality\nembeddings for downstream tasks at scale. The key idea of SEPAL is to enforce\nglobal embedding alignment by optimizing embeddings only on a small core of\nentities, and then propagating them to the rest of the graph via message\npassing. We evaluate SEPAL on 7 large-scale knowledge graphs and 46 downstream\nmachine learning tasks. Our results show that SEPAL significantly outperforms\nprevious methods on downstream tasks. In addition, SEPAL scales up its base\nembedding model, enabling fitting huge knowledge graphs on commodity hardware.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00965v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00965v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00992", "title": "UniGlyph: Unified Segmentation-Conditioned Diffusion for Precise Visual Text Synthesis", "authors": ["Yuanrui Wang", "Cong Han", "YafeiLi", "Zhipeng Jin", "Xiawei Li", "SiNan Du", "Wen Tao", "Yi Yang", "shuanglong li", "Chun Yuan", "Liu Lin"], "summary": "Text-to-image generation has greatly advanced content creation, yet\naccurately rendering visual text remains a key challenge due to blurred glyphs,\nsemantic drift, and limited style control. Existing methods often rely on\npre-rendered glyph images as conditions, but these struggle to retain original\nfont styles and color cues, necessitating complex multi-branch designs that\nincrease model overhead and reduce flexibility. To address these issues, we\npropose a segmentation-guided framework that uses pixel-level visual text masks\n-- rich in glyph shape, color, and spatial detail -- as unified conditional\ninputs. Our method introduces two core components: (1) a fine-tuned bilingual\nsegmentation model for precise text mask extraction, and (2) a streamlined\ndiffusion model augmented with adaptive glyph conditioning and a\nregion-specific loss to preserve textual fidelity in both content and style.\nOur approach achieves state-of-the-art performance on the AnyText benchmark,\nsignificantly surpassing prior methods in both Chinese and English settings. To\nenable more rigorous evaluation, we also introduce two new benchmarks:\nGlyphMM-benchmark for testing layout and glyph consistency in complex\ntypesetting, and MiniText-benchmark for assessing generation quality in\nsmall-scale text regions. Experimental results show that our model outperforms\nexisting methods by a large margin in both scenarios, particularly excelling at\nsmall text rendering and complex layout preservation, validating its strong\ngeneralization and deployment readiness.", "comment": "Accepted by ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.00992v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00992v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00356", "title": "CGEarthEye:A High-Resolution Remote Sensing Vision Foundation Model Based on the Jilin-1 Satellite Constellation", "authors": ["Zhiwei Yi", "Xin Cheng", "Jingyu Ma", "Ruifei Zhu", "Junwei Tian", "Yuanxiu Zhou", "Xinge Zhao", "Hongzhe Li"], "summary": "Deep learning methods have significantly advanced the development of\nintelligent rinterpretation in remote sensing (RS), with foundational model\nresearch based on large-scale pre-training paradigms rapidly reshaping various\ndomains of Earth Observation (EO). However, compared to the open accessibility\nand high spatiotemporal coverage of medium-resolution data, the limited\nacquisition channels for ultra-high-resolution optical RS imagery have\nconstrained the progress of high-resolution remote sensing vision foundation\nmodels (RSVFM). As the world's largest sub-meter-level commercial RS satellite\nconstellation, the Jilin-1 constellation possesses abundant sub-meter-level\nimage resources. This study proposes CGEarthEye, a RSVFM framework specifically\ndesigned for Jilin-1 satellite characteristics, comprising five backbones with\ndifferent parameter scales with totaling 2.1 billion parameters. To enhance the\nrepresentational capacity of the foundation model, we developed JLSSD, the\nfirst 15-million-scale multi-temporal self-supervised learning (SSL) dataset\nfeaturing global coverage with quarterly temporal sampling within a single\nyear, constructed through multi-level representation clustering and sampling\nstrategies. The framework integrates seasonal contrast, augmentation-based\ncontrast, and masked patch token contrastive strategies for pre-training.\nComprehensive evaluations across 10 benchmark datasets covering four typical RS\ntasks demonstrate that the CGEarthEye consistently achieves state-of-the-art\n(SOTA) performance. Further analysis reveals CGEarthEye's superior\ncharacteristics in feature visualization, model convergence, parameter\nefficiency, and practical mapping applications. This study anticipates that the\nexceptional representation capabilities of CGEarthEye will facilitate broader\nand more efficient applications of Jilin-1 data in traditional EO application.", "comment": "A Remote Sensing Fundation Model for Very High Resolution Images", "pdf_url": "http://arxiv.org/pdf/2507.00356v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00356v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00971", "title": "Reasoning as an Adaptive Defense for Safety", "authors": ["Taeyoun Kim", "Fahim Tajwar", "Aditi Raghunathan", "Aviral Kumar"], "summary": "Reasoning methods that adaptively allocate test-time compute have advanced\nLLM performance on easy to verify domains such as math and code. In this work,\nwe study how to utilize this approach to train models that exhibit a degree of\nrobustness to safety vulnerabilities, and show that doing so can provide\nbenefits. We build a recipe called $\\textit{TARS}$ (Training Adaptive Reasoners\nfor Safety), a reinforcement learning (RL) approach that trains models to\nreason about safety using chain-of-thought traces and a reward signal that\nbalances safety with task completion. To build TARS, we identify three critical\ndesign choices: (1) a \"lightweight\" warmstart SFT stage, (2) a mix of harmful,\nharmless, and ambiguous prompts to prevent shortcut behaviors such as too many\nrefusals, and (3) a reward function to prevent degeneration of reasoning\ncapabilities during training. Models trained with TARS exhibit adaptive\nbehaviors by spending more compute on ambiguous queries, leading to better\nsafety-refusal trade-offs. They also internally learn to better distinguish\nbetween safe and unsafe prompts and attain greater robustness to both white-box\n(e.g., GCG) and black-box attacks (e.g., PAIR). Overall, our work provides an\neffective, open recipe for training LLMs against jailbreaks and harmful\nrequests by reasoning per prompt.", "comment": "42 pages, 11 Figures, 7 Tables", "pdf_url": "http://arxiv.org/pdf/2507.00971v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00971v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.01006", "title": "GLM-4.1V-Thinking: Towards Versatile Multimodal Reasoning with Scalable Reinforcement Learning", "authors": ["Wenyi Hong", "Wenmeng Yu", "Xiaotao Gu", "Guo Wang", "Guobing Gan", "Haomiao Tang", "Jiale Cheng", "Ji Qi", "Junhui Ji", "Lihang Pan", "Shuaiqi Duan", "Weihan Wang", "Yan Wang", "Yean Cheng", "Zehai He", "Zhe Su", "Zhen Yang", "Ziyang Pan", "Aohan Zeng", "Baoxu Wang", "Boyan Shi", "Changyu Pang", "Chenhui Zhang", "Da Yin", "Fan Yang", "Guoqing Chen", "Jiazheng Xu", "Jiali Chen", "Jing Chen", "Jinhao Chen", "Jinghao Lin", "Jinjiang Wang", "Junjie Chen", "Leqi Lei", "Leyi Pan", "Mingzhi Zhang", "Qinkai Zheng", "Sheng Yang", "Shi Zhong", "Shiyu Huang", "Shuyuan Zhao", "Siyan Xue", "Shangqin Tu", "Shengbiao Meng", "Tianshu Zhang", "Tianwei Luo", "Tianxiang Hao", "Tianle Gong", "Wenkai Li", "Wei Jia", "Xin Lyu", "Xuancheng Huang", "Yanling Wang", "Yadong Xue", "Yanfeng Wang", "Yifan An", "Yifan Du", "Yiming Shi", "Yiheng Huang", "Yilin Niu", "Yuan Wang", "Yuanchang Yue", "Yuchen Li", "Yutao Zhang", "Yuxuan Zhang", "Zhanxiao Du", "Zhenyu Hou", "Zhao Xue", "Zhengxiao Du", "Zihan Wang", "Peng Zhang", "Debing Liu", "Bin Xu", "Juanzi Li", "Minlie Huang", "Yuxiao Dong", "Jie Tang"], "summary": "We present GLM-4.1V-Thinking, a vision-language model (VLM) designed to\nadvance general-purpose multimodal reasoning. In this report, we share our key\nfindings in the development of the reasoning-centric training framework. We\nfirst develop a capable vision foundation model with significant potential\nthrough large-scale pre-training, which arguably sets the upper bound for the\nfinal performance. Reinforcement Learning with Curriculum Sampling (RLCS) then\nunlocks the full potential of the model, leading to comprehensive capability\nenhancement across a diverse range of tasks, including STEM problem solving,\nvideo understanding, content recognition, coding, grounding, GUI-based agents,\nand long document understanding, among others. To facilitate research in this\nfield, we open-source GLM-4.1V-9B-Thinking, which achieves state-of-the-art\nperformance among models of comparable size. In a comprehensive evaluation\nacross 28 public benchmarks, our model outperforms Qwen2.5-VL-7B on nearly all\ntasks and achieves comparable or even superior performance on 18 benchmarks\nrelative to the significantly larger Qwen2.5-VL-72B. Notably,\nGLM-4.1V-9B-Thinking also demonstrates competitive or superior performance\ncompared to closed-source models such as GPT-4o on challenging tasks including\nlong document understanding and STEM reasoning, further underscoring its strong\ncapabilities. Code, models and more information are released at\nhttps://github.com/THUDM/GLM-4.1V-Thinking.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.01006v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.01006v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00358", "title": "Data-Driven Exploration for a Class of Continuous-Time Linear--Quadratic Reinforcement Learning Problems", "authors": ["Yilie Huang", "Xun Yu Zhou"], "summary": "We study reinforcement learning (RL) for the same class of continuous-time\nstochastic linear--quadratic (LQ) control problems as in\n\\cite{huang2024sublinear}, where volatilities depend on both states and\ncontrols while states are scalar-valued and running control rewards are absent.\nWe propose a model-free, data-driven exploration mechanism that adaptively\nadjusts entropy regularization by the critic and policy variance by the actor.\nUnlike the constant or deterministic exploration schedules employed in\n\\cite{huang2024sublinear}, which require extensive tuning for implementations\nand ignore learning progresses during iterations, our adaptive exploratory\napproach boosts learning efficiency with minimal tuning. Despite its\nflexibility, our method achieves a sublinear regret bound that matches the\nbest-known model-free results for this class of LQ problems, which were\npreviously derived only with fixed exploration schedules. Numerical experiments\ndemonstrate that adaptive explorations accelerate convergence and improve\nregret performance compared to the non-adaptive model-free and model-based\ncounterparts.", "comment": "36 pages, 10 figures", "pdf_url": "http://arxiv.org/pdf/2507.00358v1", "categories": ["cs.LG", "cs.AI", "cs.SY", "eess.SY", "math.OC"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00358v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.01003", "title": "Description of the Training Process of Neural Networks via Ergodic Theorem : Ghost nodes", "authors": ["Eun-Ji Park", "Sangwon Yun"], "summary": "Recent studies have proposed interpreting the training process from an\nergodic perspective. Building on this foundation we present a unified framework\nfor understanding and accelerating the training of deep neural networks via\nstochastic gradient descent. By analyzing the geometric landscape of the\nobjective function we introduce a practical diagnostic, the running estimate of\nthe largest Lyapunov exponent, which provably distinguishes genuine convergence\ntoward stable minimizers from mere statistical stabilization near saddle\npoints. We then propose a ghost category extension for standard classifiers\nthat adds auxiliary ghost output nodes so the model gains extra descent\ndirections that open a lateral corridor around narrow loss barriers and enable\nthe optimizer to bypass poor basins during the early training phase. We show\nthat this extension strictly reduces approximation error and that after\nsufficient convergence the ghost dimensions collapse and the extended model's\ninvariant law coincides with that of the original and there exists a path in\nthe enlarged parameter space along which the total loss does not increase while\nthe original loss decreases by an arbitrary margin. Taken together these\nresults provide a principled architecture level intervention that accelerates\nearly stage trainability while preserving asymptotic behavior.", "comment": "9 pages, 2 figures", "pdf_url": "http://arxiv.org/pdf/2507.01003v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.01003v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.01009", "title": "ShapeEmbed: a self-supervised learning framework for 2D contour quantification", "authors": ["Anna Foix Romero", "Craig Russell", "Alexander Krull", "Virginie Uhlmann"], "summary": "The shape of objects is an important source of visual information in a wide\nrange of applications. One of the core challenges of shape quantification is to\nensure that the extracted measurements remain invariant to transformations that\npreserve an object's intrinsic geometry, such as changing its size,\norientation, and position in the image. In this work, we introduce ShapeEmbed,\na self-supervised representation learning framework designed to encode the\ncontour of objects in 2D images, represented as a Euclidean distance matrix,\ninto a shape descriptor that is invariant to translation, scaling, rotation,\nreflection, and point indexing. Our approach overcomes the limitations of\ntraditional shape descriptors while improving upon existing state-of-the-art\nautoencoder-based approaches. We demonstrate that the descriptors learned by\nour framework outperform their competitors in shape classification tasks on\nnatural and biological images. We envision our approach to be of particular\nrelevance to biological imaging applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.01009v1", "categories": ["cs.CV", "q-bio.QM"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.01009v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00378", "title": "iPanda: An Intelligent Protocol Testing and Debugging Agent for Conformance Testing", "authors": ["Xikai Sun", "Fan Dang", "Kebin Liu", "Xin Miao", "Zihao Yang", "Haimo Lu", "Yawen Zheng", "Yunhao Liu"], "summary": "Conformance testing is essential for ensuring that protocol implementations\ncomply with their specifications. However, traditional testing approaches\ninvolve manually creating numerous test cases and scripts, making the process\nlabor-intensive and inefficient. Recently, Large Language Models (LLMs) have\ndemonstrated impressive text comprehension and code generation abilities,\nproviding promising opportunities for automation. In this paper, we propose\niPanda, the first end-to-end framework that leverages LLMs to automate protocol\nconformance testing. Given a protocol specification document and its\nimplementation, iPanda first employs a keyword-based method to automatically\ngenerate comprehensive test cases. Then, it utilizes a code-based\nretrieval-augmented generation approach to effectively interpret the\nimplementation and produce executable test code. To further enhance code\nquality, iPanda incorporates an iterative self-correction mechanism to refine\ngenerated test scripts interactively. Finally, by executing and analyzing the\ngenerated tests, iPanda systematically verifies compliance between\nimplementations and protocol specifications. Comprehensive experiments on\nvarious protocols show that iPanda significantly outperforms pure LLM-based\napproaches, improving the success rate (Pass@1) of test-code generation by\nfactors ranging from 4.675 times to 10.751 times.", "comment": "14 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2507.00378v1", "categories": ["cs.SE", "cs.AI"], "cate": "cs.SE", "url": "http://arxiv.org/abs/2507.00378v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.01004", "title": "ZeCO: Zero Communication Overhead Sequence Parallelism for Linear Attention", "authors": ["Yuhong Chou", "Zehao Liu", "Ruijie Zhu", "Xinyi Wan", "Tianjian Li", "Congying Chu", "Qian Liu", "Jibin Wu", "Zejun Ma"], "summary": "Linear attention mechanisms deliver significant advantages for Large Language\nModels (LLMs) by providing linear computational complexity, enabling efficient\nprocessing of ultra-long sequences (e.g., 1M context). However, existing\nSequence Parallelism (SP) methods, essential for distributing these workloads\nacross devices, become the primary bottleneck due to substantial communication\noverhead. In this paper, we introduce ZeCO (Zero Communication Overhead)\nsequence parallelism for linear attention models, a new SP method designed to\novercome these limitations and achieve end-to-end near-linear scalability for\nlong sequence training. For example, training a model with a 1M sequence length\nacross 64 devices using ZeCO takes roughly the same time as training with an\n16k sequence on a single device. At the heart of ZeCO lies All-Scan, a new\ncollective communication primitive. All-Scan provides each SP rank with\nprecisely the initial operator state it requires while maintaining a minimal\ncommunication footprint, effectively eliminating communication overhead.\nTheoretically, we prove the optimaity of ZeCO, showing that it introduces only\nnegligible time and space overhead. Empirically, we compare the communication\ncosts of different sequence parallelism strategies and demonstrate that\nAll-Scan achieves the fastest communication in SP scenarios. Specifically, on\n256 GPUs with an 8M sequence length, ZeCO achieves a 60\\% speedup compared to\nthe current state-of-the-art (SOTA) SP method. We believe ZeCO establishes a\nclear path toward efficiently training next-generation LLMs on previously\nintractable sequence lengths.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.01004v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.01004v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.01012", "title": "DAM-VSR: Disentanglement of Appearance and Motion for Video Super-Resolution", "authors": ["Zhe Kong", "Le Li", "Yong Zhang", "Feng Gao", "Shaoshu Yang", "Tao Wang", "Kaihao Zhang", "Zhuoliang Kang", "Xiaoming Wei", "Guanying Chen", "Wenhan Luo"], "summary": "Real-world video super-resolution (VSR) presents significant challenges due\nto complex and unpredictable degradations. Although some recent methods utilize\nimage diffusion models for VSR and have shown improved detail generation\ncapabilities, they still struggle to produce temporally consistent frames. We\nattempt to use Stable Video Diffusion (SVD) combined with ControlNet to address\nthis issue. However, due to the intrinsic image-animation characteristics of\nSVD, it is challenging to generate fine details using only low-quality videos.\nTo tackle this problem, we propose DAM-VSR, an appearance and motion\ndisentanglement framework for VSR. This framework disentangles VSR into\nappearance enhancement and motion control problems. Specifically, appearance\nenhancement is achieved through reference image super-resolution, while motion\ncontrol is achieved through video ControlNet. This disentanglement fully\nleverages the generative prior of video diffusion models and the detail\ngeneration capabilities of image super-resolution models. Furthermore, equipped\nwith the proposed motion-aligned bidirectional sampling strategy, DAM-VSR can\nconduct VSR on longer input videos. DAM-VSR achieves state-of-the-art\nperformance on real-world data and AIGC data, demonstrating its powerful detail\ngeneration capabilities.", "comment": "Accepted by ACM SIGGRAPH 2025, Homepage:\n  https://kongzhecn.github.io/projects/dam-vsr/ Github:\n  https://github.com/kongzhecn/DAM-VSR", "pdf_url": "http://arxiv.org/pdf/2507.01012v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.01012v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00407", "title": "Augmenting Molecular Graphs with Geometries via Machine Learning Interatomic Potentials", "authors": ["Cong Fu", "Yuchao Lin", "Zachary Krueger", "Haiyang Yu", "Maho Nakata", "Jianwen Xie", "Emine Kucukbenli", "Xiaofeng Qian", "Shuiwang Ji"], "summary": "Accurate molecular property predictions require 3D geometries, which are\ntypically obtained using expensive methods such as density functional theory\n(DFT). Here, we attempt to obtain molecular geometries by relying solely on\nmachine learning interatomic potential (MLIP) models. To this end, we first\ncurate a large-scale molecular relaxation dataset comprising 3.5 million\nmolecules and 300 million snapshots. Then MLIP foundation models are trained\nwith supervised learning to predict energy and forces given 3D molecular\nstructures. Once trained, we show that the foundation models can be used in\ndifferent ways to obtain geometries either explicitly or implicitly. First, it\ncan be used to obtain low-energy 3D geometries via geometry optimization,\nproviding relaxed 3D geometries for downstream molecular property predictions.\nTo mitigate potential biases and enhance downstream predictions, we introduce\ngeometry fine-tuning based on the relaxed 3D geometries. Second, the foundation\nmodels can be directly fine-tuned for property prediction when ground truth 3D\ngeometries are available. Our results demonstrate that MLIP foundation models\ntrained on relaxation data can provide valuable molecular geometries that\nbenefit property predictions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00407v1", "categories": ["physics.chem-ph", "cs.AI", "q-bio.QM"], "cate": "physics.chem-ph", "url": "http://arxiv.org/abs/2507.00407v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00005", "title": "SwarmFusion: Revolutionizing Disaster Response with Swarm Intelligence and Deep Learning", "authors": ["Vasavi Lankipalle"], "summary": "Disaster response requires rapid, adaptive decision-making in chaotic\nenvironments. SwarmFusion, a novel hybrid framework, integrates particle swarm\noptimization with convolutional neural networks to optimize real-time resource\nallocation and path planning. By processing live satellite, drone, and sensor\ndata, SwarmFusion enhances situational awareness and operational efficiency in\nflood and wildfire scenarios. Simulations using the DisasterSim2025 dataset\ndemonstrate up to 40 percentage faster response times and 90 percentage\nsurvivor coverage compared to baseline methods. This scalable, data-driven\napproach offers a transformative solution for time-critical disaster\nmanagement, with potential applications across diverse crisis scenarios.", "comment": "6", "pdf_url": "http://arxiv.org/pdf/2507.00005v1", "categories": ["cs.NE", "cs.LG"], "cate": "cs.NE", "url": "http://arxiv.org/abs/2507.00005v1", "date": "2025-06-10", "updated": "2025-06-10"}
{"id": "2507.00008", "title": "DiMo-GUI: Advancing Test-time Scaling in GUI Grounding via Modality-Aware Visual Reasoning", "authors": ["Hang Wu", "Hongkai Chen", "Yujun Cai", "Chang Liu", "Qingwen Ye", "Ming-Hsuan Yang", "Yiwei Wang"], "summary": "Grounding natural language queries in graphical user interfaces (GUIs) poses\nunique challenges due to the diversity of visual elements, spatial clutter, and\nthe ambiguity of language. In this paper, we introduce DiMo-GUI, a\ntraining-free framework for GUI grounding that leverages two core strategies:\ndynamic visual grounding and modality-aware optimization. Instead of treating\nthe GUI as a monolithic image, our method splits the input into textual\nelements and iconic elements, allowing the model to reason over each modality\nindependently using general-purpose vision-language models. When predictions\nare ambiguous or incorrect, DiMo-GUI dynamically focuses attention by\ngenerating candidate focal regions centered on the model's initial predictions\nand incrementally zooms into subregions to refine the grounding result. This\nhierarchical refinement process helps disambiguate visually crowded layouts\nwithout the need for additional training or annotations. We evaluate our\napproach on standard GUI grounding benchmarks and demonstrate consistent\nimprovements over baseline inference pipelines, highlighting the effectiveness\nof combining modality separation with region-focused reasoning.", "comment": "8 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2507.00008v1", "categories": ["cs.AI", "cs.CV", "cs.HC"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2507.00008v1", "date": "2025-06-12", "updated": "2025-06-12"}
{"id": "2507.00418", "title": "Serving LLMs in HPC Clusters: A Comparative Study of Qualcomm Cloud AI 100 Ultra and High-Performance GPUs", "authors": ["Mohammad Firas Sada", "John J. Graham", "Elham E Khoda", "Mahidhar Tatineni", "Dmitry Mishin", "Rajesh K. Gupta", "Rick Wagner", "Larry Smarr", "Thomas A. DeFanti", "Frank Würthwein"], "summary": "This study presents a benchmarking analysis of the Qualcomm Cloud AI 100\nUltra (QAic) accelerator for large language model (LLM) inference, evaluating\nits energy efficiency (throughput per watt) and performance against leading\nNVIDIA (A100, H200) and AMD (MI300A) GPUs within the National Research Platform\n(NRP) ecosystem. A total of 15 open-source LLMs, ranging from 117 million to 90\nbillion parameters, are served using the vLLM framework. The QAic inference\ncards appears to be energy efficient and performs well in the energy efficiency\nmetric in most cases. The findings offer insights into the potential of the\nQualcomm Cloud AI 100 Ultra for high-performance computing (HPC) applications\nwithin the National Research Platform (NRP).", "comment": "To appear in Proceedings of the Practice and Experience in Advanced\n  Research Computing (PEARC '25)", "pdf_url": "http://arxiv.org/pdf/2507.00418v1", "categories": ["cs.DC", "cs.AI"], "cate": "cs.DC", "url": "http://arxiv.org/abs/2507.00418v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00006", "title": "MVGBench: Comprehensive Benchmark for Multi-view Generation Models", "authors": ["Xianghui Xie", "Chuhang Zou", "Meher Gitika Karumuri", "Jan Eric Lenssen", "Gerard Pons-Moll"], "summary": "We propose MVGBench, a comprehensive benchmark for multi-view image\ngeneration models (MVGs) that evaluates 3D consistency in geometry and texture,\nimage quality, and semantics (using vision language models). Recently, MVGs\nhave been the main driving force in 3D object creation. However, existing\nmetrics compare generated images against ground truth target views, which is\nnot suitable for generative tasks where multiple solutions exist while\ndiffering from ground truth. Furthermore, different MVGs are trained on\ndifferent view angles, synthetic data and specific lightings -- robustness to\nthese factors and generalization to real data are rarely evaluated thoroughly.\nWithout a rigorous evaluation protocol, it is also unclear what design choices\ncontribute to the progress of MVGs. MVGBench evaluates three different aspects:\nbest setup performance, generalization to real data and robustness. Instead of\ncomparing against ground truth, we introduce a novel 3D self-consistency metric\nwhich compares 3D reconstructions from disjoint generated multi-views. We\nsystematically compare 12 existing MVGs on 4 different curated real and\nsynthetic datasets. With our analysis, we identify important limitations of\nexisting methods specially in terms of robustness and generalization, and we\nfind the most critical design choices. Using the discovered best practices, we\npropose ViFiGen, a method that outperforms all evaluated MVGs on 3D\nconsistency. Our code, model, and benchmark suite will be publicly released.", "comment": "17 pages, 11 figures, 9 tables, project page:\n  https://virtualhumans.mpi-inf.mpg.de/MVGBench/", "pdf_url": "http://arxiv.org/pdf/2507.00006v1", "categories": ["cs.GR", "cs.LG", "eess.IV"], "cate": "cs.GR", "url": "http://arxiv.org/abs/2507.00006v1", "date": "2025-06-11", "updated": "2025-06-11"}
{"id": "2507.00016", "title": "Gradient-based Fine-Tuning through Pre-trained Model Regularization", "authors": ["Xuanbo Liu", "Liu Liu", "Fuxiang Wu", "Fusheng Hao", "Xianglong Liu"], "summary": "Large pre-trained models have demonstrated extensive applications across\nvarious fields. However, fine-tuning these models for specific downstream tasks\ndemands significant computational resources and storage. One fine-tuning\nmethod, gradient-based parameter selection (GPS), focuses on fine-tuning only\nthe parameters with high gradients in each neuron, thereby reducing the number\nof training parameters. Nevertheless, this approach increases computational\nresource requirements and storage demands. In this paper, we propose an\nefficient gradient-based and regularized fine-tuning method (GRFT) that updates\nthe rows or columns of the weight matrix. We theoretically demonstrate that the\nrows or columns with the highest sum of squared gradients are optimal for\nupdating. This strategy effectively reduces storage overhead and improves the\nefficiency of parameter selection. Additionally, we incorporate regularization\nto enhance knowledge transfer from the pre-trained model. GRFT achieves\nstate-of-the-art performance, surpassing existing methods such as GPS, Adapter\nTuning, and LoRA. Notably, GRFT requires updating only 1.22% and 0.30% of the\ntotal parameters on FGVC and VTAB datasets, respectively, demonstrating its\nhigh efficiency and effectiveness. The source code will be released soon.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00016v1", "categories": ["cs.LG", "cs.AI", "cs.CV"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00016v1", "date": "2025-06-14", "updated": "2025-06-14"}
{"id": "2507.00419", "title": "Geological Everything Model 3D: A Promptable Foundation Model for Unified and Zero-Shot Subsurface Understanding", "authors": ["Yimin Dou", "Xinming Wu", "Nathan L Bangs", "Harpreet Singh Sethi", "Jintao Li", "Hang Gao", "Zhixiang Guo"], "summary": "Understanding Earth's subsurface is critical for energy transition, natural\nhazard mitigation, and planetary science. Yet subsurface analysis remains\nfragmented, with separate models required for structural interpretation,\nstratigraphic analysis, geobody segmentation, and property modeling-each\ntightly coupled to specific data distributions and task formulations. We\nintroduce the Geological Everything Model 3D (GEM), a unified generative\narchitecture that reformulates all these tasks as prompt-conditioned inference\nalong latent structural frameworks derived from subsurface imaging. This\nformulation moves beyond task-specific models by enabling a shared inference\nmechanism, where GEM propagates human-provided prompts-such as well logs,\nmasks, or structural sketches-along inferred structural frameworks to produce\ngeologically coherent outputs. Through this mechanism, GEM achieves zero-shot\ngeneralization across tasks with heterogeneous prompt types, without retraining\nfor new tasks or data sources. This capability emerges from a two-stage\ntraining process that combines self-supervised representation learning on\nlarge-scale field seismic data with adversarial fine-tuning using mixed prompts\nand labels across diverse subsurface tasks. GEM demonstrates broad\napplicability across surveys and tasks, including Martian radar stratigraphy\nanalysis, structural interpretation in subduction zones, full seismic\nstratigraphic interpretation, geobody delineation, and property modeling. By\nbridging expert knowledge with generative reasoning in a structurally aware\nmanner, GEM lays the foundation for scalable, human-in-the-loop geophysical\nAI-transitioning from fragmented pipelines to a vertically integrated,\npromptable reasoning system. Project page: https://douyimin.github.io/GEM", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00419v1", "categories": ["physics.geo-ph", "cs.AI"], "cate": "physics.geo-ph", "url": "http://arxiv.org/abs/2507.00419v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00007", "title": "Integrating Universal Generative AI Platforms in Educational Labs to Foster Critical Thinking and Digital Literacy", "authors": ["Vasiliy Znamenskiy", "Rafael Niyazov", "Joel Hernandez"], "summary": "This paper presents a new educational framework for integrating generative\nartificial intelligence (GenAI) platforms such as ChatGPT, Claude, and Gemini\ninto laboratory activities aimed at developing critical thinking and digital\nliteracy among undergraduate students. Recognizing the limitations and risks of\nuncritical reliance on large language models (LLMs), the proposed pedagogical\nmodel reframes GenAI as a research subject and cognitive tool. Students\nformulate discipline-specific prompts and evaluate GenAI-generated responses in\ntext, image, and video modalities. A pilot implementation in a general\nastronomy course for non-science majors demonstrated high levels of engagement\nand critical reflection, with many students continuing the activity after class\nand presenting results at a research symposium. The results highlight the\nimportance of structured AI interactions in education and suggest that GenAI\ncan improve learning outcomes when combined with reflective assessment methods.\nThe study proposes a replicable model for interdisciplinary AI-integrated lab\nwork, adaptable to scientific disciplines. See the guide to learning activities\nbased on Generative-Ai platforms: https://doi.org/10.5281/zenodo.15555802", "comment": "http://doi.org/10.5121/ijci.2025.140302", "pdf_url": "http://arxiv.org/pdf/2507.00007v1", "categories": ["cs.CY", "cs.AI", "cs.LG", "68T50, 68U20, 97U50, 97D40", "I.2.7; K.3.1; K.3.2; H.5.3"], "cate": "cs.CY", "url": "http://arxiv.org/abs/2507.00007v1", "date": "2025-06-11", "updated": "2025-06-11"}
{"id": "2507.00028", "title": "HiT-JEPA: A Hierarchical Self-supervised Trajectory Embedding Framework for Similarity Computation", "authors": ["Lihuan Li", "Hao Xue", "Shuang Ao", "Yang Song", "Flora Salim"], "summary": "The representation of urban trajectory data plays a critical role in\neffectively analyzing spatial movement patterns. Despite considerable progress,\nthe challenge of designing trajectory representations that can capture diverse\nand complementary information remains an open research problem. Existing\nmethods struggle in incorporating trajectory fine-grained details and\nhigh-level summary in a single model, limiting their ability to attend to both\nlong-term dependencies while preserving local nuances. To address this, we\npropose HiT-JEPA (Hierarchical Interactions of Trajectory Semantics via a Joint\nEmbedding Predictive Architecture), a unified framework for learning\nmulti-scale urban trajectory representations across semantic abstraction\nlevels. HiT-JEPA adopts a three-layer hierarchy that progressively captures\npoint-level fine-grained details, intermediate patterns, and high-level\ntrajectory abstractions, enabling the model to integrate both local dynamics\nand global semantics in one coherent structure. Extensive experiments on\nmultiple real-world datasets for trajectory similarity computation show that\nHiT-JEPA's hierarchical design yields richer, multi-scale representations. Code\nis available at: https://anonymous.4open.science/r/HiT-JEPA.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00028v1", "categories": ["cs.LG", "cs.AI", "cs.CV"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00028v1", "date": "2025-06-17", "updated": "2025-06-17"}
{"id": "2507.00435", "title": "RoboEval: Where Robotic Manipulation Meets Structured and Scalable Evaluation", "authors": ["Yi Ru Wang", "Carter Ung", "Grant Tannert", "Jiafei Duan", "Josephine Li", "Amy Le", "Rishabh Oswal", "Markus Grotz", "Wilbert Pumacay", "Yuquan Deng", "Ranjay Krishna", "Dieter Fox", "Siddhartha Srinivasa"], "summary": "We present RoboEval, a simulation benchmark and structured evaluation\nframework designed to reveal the limitations of current bimanual manipulation\npolicies. While prior benchmarks report only binary task success, we show that\nsuch metrics often conceal critical weaknesses in policy behavior -- such as\npoor coordination, slipping during grasping, or asymmetric arm usage. RoboEval\nintroduces a suite of tiered, semantically grounded tasks decomposed into\nskill-specific stages, with variations that systematically challenge spatial,\nphysical, and coordination capabilities. Tasks are paired with fine-grained\ndiagnostic metrics and 3000+ human demonstrations to support imitation\nlearning. Our experiments reveal that policies with similar success rates\ndiverge in how tasks are executed -- some struggle with alignment, others with\ntemporally consistent bimanual control. We find that behavioral metrics\ncorrelate with success in over half of task-metric pairs, and remain\ninformative even when binary success saturates. By pinpointing when and how\npolicies fail, RoboEval enables a deeper, more actionable understanding of\nrobotic manipulation -- and highlights the need for evaluation tools that go\nbeyond success alone.", "comment": "Project page: https://robo-eval.github.io", "pdf_url": "http://arxiv.org/pdf/2507.00435v1", "categories": ["cs.RO", "cs.AI", "cs.CV"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2507.00435v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00032", "title": "Ken Utilization Layer: Hebbian Replay Within a Student's Ken for Adaptive Knowledge Tracing", "authors": ["Grey Kuling", "Marinka Zitnik"], "summary": "We introduce KUL-KT, a biologically inspired architecture for knowledge\ntracing (KT), combining Hebbian memory encoding with gradient-based\nconsolidation in a scalable, input-agnostic framework. KUL-KT adapts the\nprinciple of memory consolidation in neural systems, to student modeling by\nintroducing two key innovations: (i) a time-decaying Hebbian memory update that\nenables graceful forgetting, and (ii) a novel Loss-aligned Internal Target\n(LIT) method to compute an ideal internal state, allowing continual learning\nwithout backpropagation through time. The architecture consists of a fast\nHebbian memory that captures each learner interaction via a single associative\nupdate, and a slower linear network that consolidates recalled samples through\ngradient descent. This design enables few-shot personalization and natural\nforgetting without storing raw data or relying on large cohort training.\nOperating entirely in embedding space, KUL-KT supports both structured\n(tabular) and unstructured (short-answer) inputs. Empirically, KUL-KT\noutperforms strong baselines on ten public KT benchmarks in rank-sensitive\nmetrics such as nDCG and Recall@10. In a classroom deployment, KUL-KT\npersonalized quizzes from short-answer data, leading to improved\nlearner-perceived helpfulness and reduced difficulty (p < 0.05). Ablation\nstudies confirm that Hebbian decay and LIT are critical for continual\nadaptation. Compared to a strong graph-based KT model, KUL-KT trains 1.75x\nfaster and uses 99.01\\% less memory. These results position KUL-KT as a\nbiologically grounded, memory-efficient, and input-flexible framework for\npersonalized learning at scale.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00032v1", "categories": ["cs.CY", "cs.AI", "cs.LG", "cs.NE"], "cate": "cs.CY", "url": "http://arxiv.org/abs/2507.00032v1", "date": "2025-06-18", "updated": "2025-06-18"}
{"id": "2507.00041", "title": "TalentMine: LLM-Based Extraction and Question-Answering from Multimodal Talent Tables", "authors": ["Varun Mannam", "Fang Wang", "Chaochun Liu", "Xin Chen"], "summary": "In talent management systems, critical information often resides in complex\ntabular formats, presenting significant retrieval challenges for conventional\nlanguage models. These challenges are pronounced when processing Talent\ndocumentation that requires precise interpretation of tabular relationships for\naccurate information retrieval and downstream decision-making. Current table\nextraction methods struggle with semantic understanding, resulting in poor\nperformance when integrated into retrieval-augmented chat applications. This\npaper identifies a key bottleneck - while structural table information can be\nextracted, the semantic relationships between tabular elements are lost,\ncausing downstream query failures. To address this, we introduce TalentMine, a\nnovel LLM-enhanced framework that transforms extracted tables into semantically\nenriched representations. Unlike conventional approaches relying on CSV or text\nlinearization, our method employs specialized multimodal reasoning to preserve\nboth structural and semantic dimensions of tabular data. Experimental\nevaluation across employee benefits document collections demonstrates\nTalentMine's superior performance, achieving 100% accuracy in query answering\ntasks compared to 0% for standard AWS Textract extraction and 40% for AWS\nTextract Visual Q&A capabilities. Our comparative analysis also reveals that\nthe Claude v3 Haiku model achieves optimal performance for talent management\napplications. The key contributions of this work include (1) a systematic\nanalysis of semantic information loss in current table extraction pipelines,\n(2) a novel LLM-based method for semantically enriched table representation,\n(3) an efficient integration framework for retrieval-augmented systems as\nend-to-end systems, and (4) comprehensive benchmarks on talent analytics tasks\nshowing substantial improvements across multiple categories.", "comment": "Submitted to KDD conference, workshop: Talent and Management\n  Computing (TMC 2025), https://tmcworkshop.github.io/2025/", "pdf_url": "http://arxiv.org/pdf/2507.00041v1", "categories": ["cs.AI", "cs.CV", "cs.IR"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2507.00041v1", "date": "2025-06-22", "updated": "2025-06-22"}
{"id": "2507.00440", "title": "A Recipe for Causal Graph Regression: Confounding Effects Revisited", "authors": ["Yujia Yin", "Tianyi Qu", "Zihao Wang", "Yifan Chen"], "summary": "Through recognizing causal subgraphs, causal graph learning (CGL) has risen\nto be a promising approach for improving the generalizability of graph neural\nnetworks under out-of-distribution (OOD) scenarios. However, the empirical\nsuccesses of CGL techniques are mostly exemplified in classification settings,\nwhile regression tasks, a more challenging setting in graph learning, are\noverlooked. We thus devote this work to tackling causal graph regression (CGR);\nto this end we reshape the processing of confounding effects in existing CGL\nstudies, which mainly deal with classification. Specifically, we reflect on the\npredictive power of confounders in graph-level regression, and generalize\nclassification-specific causal intervention techniques to regression through a\nlens of contrastive learning. Extensive experiments on graph OOD benchmarks\nvalidate the efficacy of our proposals for CGR. The model implementation and\nthe code are provided on https://github.com/causal-graph/CGR.", "comment": "ICML 2025 accepted", "pdf_url": "http://arxiv.org/pdf/2507.00440v1", "categories": ["cs.LG", "cs.AI", "stat.ME"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00440v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00042", "title": "Catastrophic Forgetting Mitigation via Discrepancy-Weighted Experience Replay", "authors": ["Xinrun Xu", "Jianwen Yang", "Qiuhong Zhang", "Zhanbiao Lian", "Zhiming Ding", "Shan Jiang"], "summary": "Continually adapting edge models in cloud-edge collaborative object detection\nfor traffic monitoring suffers from catastrophic forgetting, where models lose\npreviously learned knowledge when adapting to new data distributions. This is\nespecially problematic in dynamic traffic environments characterised by\nperiodic variations (e.g., day/night, peak hours), where past knowledge remains\nvaluable. Existing approaches like experience replay and visual prompts offer\nsome mitigation, but struggle to effectively prioritize and leverage historical\ndata for optimal knowledge retention and adaptation. Specifically, simply\nstoring and replaying all historical data can be inefficient, while treating\nall historical experiences as equally important overlooks their varying\nrelevance to the current domain. This paper proposes ER-EMU, an edge model\nupdate algorithm based on adaptive experience replay, to address these\nlimitations. ER-EMU utilizes a limited-size experience buffer managed using a\nFirst-In-First-Out (FIFO) principle, and a novel Domain Distance Metric-based\nExperience Selection (DDM-ES) algorithm. DDM-ES employs the multi-kernel\nmaximum mean discrepancy (MK-MMD) to quantify the dissimilarity between target\ndomains, prioritizing the selection of historical data that is most dissimilar\nto the current target domain. This ensures training diversity and facilitates\nthe retention of knowledge from a wider range of past experiences, while also\npreventing overfitting to the new domain. The experience buffer is also updated\nusing a simple random sampling strategy to maintain a balanced representation\nof previous domains. Experiments on the Bellevue traffic video dataset,\ninvolving repeated day/night cycles, demonstrate that ER-EMU consistently\nimproves the performance of several state-of-the-art cloud-edge collaborative\nobject detection frameworks.", "comment": "ICANN 2025", "pdf_url": "http://arxiv.org/pdf/2507.00042v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00042v1", "date": "2025-06-23", "updated": "2025-06-23"}
{"id": "2507.00051", "title": "Real-Time Guidewire Tip Tracking Using a Siamese Network for Image-Guided Endovascular Procedures", "authors": ["Tianliang Yao", "Zhiqiang Pei", "Yong Li", "Yixuan Yuan", "Peng Qi"], "summary": "An ever-growing incorporation of AI solutions into clinical practices\nenhances the efficiency and effectiveness of healthcare services. This paper\nfocuses on guidewire tip tracking tasks during image-guided therapy for\ncardiovascular diseases, aiding physicians in improving diagnostic and\ntherapeutic quality. A novel tracking framework based on a Siamese network with\ndual attention mechanisms combines self- and cross-attention strategies for\nrobust guidewire tip tracking. This design handles visual ambiguities, tissue\ndeformations, and imaging artifacts through enhanced spatial-temporal feature\nlearning. Validation occurred on 3 randomly selected clinical digital\nsubtraction angiography (DSA) sequences from a dataset of 15 sequences,\ncovering multiple interventional scenarios. The results indicate a mean\nlocalization error of 0.421 $\\pm$ 0.138 mm, with a maximum error of 1.736 mm,\nand a mean Intersection over Union (IoU) of 0.782. The framework maintains an\naverage processing speed of 57.2 frames per second, meeting the temporal\ndemands of endovascular imaging. Further validations with robotic platforms for\nautomating diagnostics and therapies in clinical routines yielded tracking\nerrors of 0.708 $\\pm$ 0.695 mm and 0.148 $\\pm$ 0.057 mm in two distinct\nexperimental scenarios.", "comment": "This paper has been accepted by Advanced Intelligent Systems", "pdf_url": "http://arxiv.org/pdf/2507.00051v1", "categories": ["eess.IV", "cs.CV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2507.00051v1", "date": "2025-06-25", "updated": "2025-06-25"}
{"id": "2507.00443", "title": "Novel Pigeon-inspired 3D Obstacle Detection and Avoidance Maneuver for Multi-UAV Systems", "authors": ["Reza Ahmadvand", "Sarah Safura Sharif", "Yaser Mike Banad"], "summary": "Recent advances in multi-agent systems manipulation have demonstrated a\nrising demand for the implementation of multi-UAV systems in urban areas, which\nare always subjected to the presence of static and dynamic obstacles. Inspired\nby the collective behavior of tilapia fish and pigeons, the focus of the\npresented research is on the introduction of a nature-inspired collision-free\nformation control for a multi-UAV system, considering the obstacle avoidance\nmaneuvers. The developed framework in this study utilizes a semi-distributed\ncontrol approach, in which, based on a probabilistic Lloyd's algorithm, a\ncentralized guidance algorithm works for optimal positioning of the UAVs, while\na distributed control approach has been used for the intervehicle collision and\nobstacle avoidance. Further, the presented framework has been extended to the\n3D space with a novel definition of 3D maneuvers. Finally, the presented\nframework has been applied to multi-UAV systems in 2D and 3D scenarios, and the\nobtained results demonstrated the validity of the presented method in dynamic\nenvironments with stationary and moving obstacles.", "comment": "11 Pages, 11 Pictures, 1 Table, 3 Algorithms", "pdf_url": "http://arxiv.org/pdf/2507.00443v1", "categories": ["cs.RO", "cs.AI", "cs.MA"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2507.00443v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00044", "title": "HistoART: Histopathology Artifact Detection and Reporting Tool", "authors": ["Seyed Kahaki", "Alexander R. Webber", "Ghada Zamzmi", "Adarsh Subbaswamy", "Rucha Deshpande", "Aldo Badano"], "summary": "In modern cancer diagnostics, Whole Slide Imaging (WSI) is widely used to\ndigitize tissue specimens for detailed, high-resolution examination; however,\nother diagnostic approaches, such as liquid biopsy and molecular testing, are\nalso utilized based on the cancer type and clinical context. While WSI has\nrevolutionized digital histopathology by enabling automated, precise analysis,\nit remains vulnerable to artifacts introduced during slide preparation and\nscanning. These artifacts can compromise downstream image analysis. To address\nthis challenge, we propose and compare three robust artifact detection\napproaches for WSIs: (1) a foundation model-based approach (FMA) using a\nfine-tuned Unified Neural Image (UNI) architecture, (2) a deep learning\napproach (DLA) built on a ResNet50 backbone, and (3) a knowledge-based approach\n(KBA) leveraging handcrafted features from texture, color, and frequency-based\nmetrics. The methods target six common artifact types: tissue folds,\nout-of-focus regions, air bubbles, tissue damage, marker traces, and blood\ncontamination. Evaluations were conducted on 50,000+ image patches from diverse\nscanners (Hamamatsu, Philips, Leica Aperio AT2) across multiple sites. The FMA\nachieved the highest patch-wise AUROC of 0.995 (95% CI [0.994, 0.995]),\noutperforming the ResNet50-based method (AUROC: 0.977, 95% CI [0.977, 0.978])\nand the KBA (AUROC: 0.940, 95% CI [0.933, 0.946]). To translate detection into\nactionable insights, we developed a quality report scorecard that quantifies\nhigh-quality patches and visualizes artifact distributions.", "comment": "14 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2507.00044v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00044v1", "date": "2025-06-23", "updated": "2025-06-23"}
{"id": "2507.00185", "title": "Multimodal, Multi-Disease Medical Imaging Foundation Model (MerMED-FM)", "authors": ["Yang Zhou", "Chrystie Wan Ning Quek", "Jun Zhou", "Yan Wang", "Yang Bai", "Yuhe Ke", "Jie Yao", "Laura Gutierrez", "Zhen Ling Teo", "Darren Shu Jeng Ting", "Brian T. Soetikno", "Christopher S. Nielsen", "Tobias Elze", "Zengxiang Li", "Linh Le Dinh", "Lionel Tim-Ee Cheng", "Tran Nguyen Tuan Anh", "Chee Leong Cheng", "Tien Yin Wong", "Nan Liu", "Iain Beehuat Tan", "Tony Kiat Hon Lim", "Rick Siow Mong Goh", "Yong Liu", "Daniel Shu Wei Ting"], "summary": "Current artificial intelligence models for medical imaging are predominantly\nsingle modality and single disease. Attempts to create multimodal and\nmulti-disease models have resulted in inconsistent clinical accuracy.\nFurthermore, training these models typically requires large, labour-intensive,\nwell-labelled datasets. We developed MerMED-FM, a state-of-the-art multimodal,\nmulti-specialty foundation model trained using self-supervised learning and a\nmemory module. MerMED-FM was trained on 3.3 million medical images from over\nten specialties and seven modalities, including computed tomography (CT), chest\nX-rays (CXR), ultrasound (US), pathology patches, color fundus photography\n(CFP), optical coherence tomography (OCT) and dermatology images. MerMED-FM was\nevaluated across multiple diseases and compared against existing foundational\nmodels. Strong performance was achieved across all modalities, with AUROCs of\n0.988 (OCT); 0.982 (pathology); 0.951 (US); 0.943 (CT); 0.931 (skin); 0.894\n(CFP); 0.858 (CXR). MerMED-FM has the potential to be a highly adaptable,\nversatile, cross-specialty foundation model that enables robust medical imaging\ninterpretation across diverse medical disciplines.", "comment": "42 pages, 3 composite figures, 4 tables", "pdf_url": "http://arxiv.org/pdf/2507.00185v1", "categories": ["eess.IV", "cs.AI", "cs.CV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2507.00185v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00445", "title": "Iterative Distillation for Reward-Guided Fine-Tuning of Diffusion Models in Biomolecular Design", "authors": ["Xingyu Su", "Xiner Li", "Masatoshi Uehara", "Sunwoo Kim", "Yulai Zhao", "Gabriele Scalia", "Ehsan Hajiramezanali", "Tommaso Biancalani", "Degui Zhi", "Shuiwang Ji"], "summary": "We address the problem of fine-tuning diffusion models for reward-guided\ngeneration in biomolecular design. While diffusion models have proven highly\neffective in modeling complex, high-dimensional data distributions, real-world\napplications often demand more than high-fidelity generation, requiring\noptimization with respect to potentially non-differentiable reward functions\nsuch as physics-based simulation or rewards based on scientific knowledge.\nAlthough RL methods have been explored to fine-tune diffusion models for such\nobjectives, they often suffer from instability, low sample efficiency, and mode\ncollapse due to their on-policy nature. In this work, we propose an iterative\ndistillation-based fine-tuning framework that enables diffusion models to\noptimize for arbitrary reward functions. Our method casts the problem as policy\ndistillation: it collects off-policy data during the roll-in phase, simulates\nreward-based soft-optimal policies during roll-out, and updates the model by\nminimizing the KL divergence between the simulated soft-optimal policy and the\ncurrent model policy. Our off-policy formulation, combined with KL divergence\nminimization, enhances training stability and sample efficiency compared to\nexisting RL-based methods. Empirical results demonstrate the effectiveness and\nsuperior reward optimization of our approach across diverse tasks in protein,\nsmall molecule, and regulatory DNA design.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00445v1", "categories": ["cs.LG", "cs.AI", "q-bio.QM"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00445v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00048", "title": "A collaborative digital twin built on FAIR data and compute infrastructure", "authors": ["Thomas M. Deucher", "Juan C. Verduzco", "Michael Titus", "Alejandro Strachan"], "summary": "The integration of machine learning with automated experimentation in\nself-driving laboratories (SDL) offers a powerful approach to accelerate\ndiscovery and optimization tasks in science and engineering applications. When\nsupported by findable, accessible, interoperable, and reusable (FAIR) data\ninfrastructure, SDLs with overlapping interests can collaborate more\neffectively. This work presents a distributed SDL implementation built on\nnanoHUB services for online simulation and FAIR data management. In this\nframework, geographically dispersed collaborators conducting independent\noptimization tasks contribute raw experimental data to a shared central\ndatabase. These researchers can then benefit from analysis tools and machine\nlearning models that automatically update as additional data become available.\nNew data points are submitted through a simple web interface and automatically\nprocessed using a nanoHUB Sim2L, which extracts derived quantities and indexes\nall inputs and outputs in a FAIR data repository called ResultsDB. A separate\nnanoHUB workflow enables sequential optimization using active learning, where\nresearchers define the optimization objective, and machine learning models are\ntrained on-the-fly with all existing data, guiding the selection of future\nexperiments. Inspired by the concept of ``frugal twin\", the optimization task\nseeks to find the optimal recipe to combine food dyes to achieve the desired\ntarget color. With easily accessible and inexpensive materials, researchers and\nstudents can set up their own experiments, share data with collaborators, and\nexplore the combination of FAIR data, predictive ML models, and sequential\noptimization. The tools introduced are generally applicable and can easily be\nextended to other optimization problems.", "comment": "10 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2507.00048v1", "categories": ["cs.AI", "cond-mat.mtrl-sci", "cs.CE", "cs.LG"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2507.00048v1", "date": "2025-06-24", "updated": "2025-06-24"}
{"id": "2507.00190", "title": "Rethink 3D Object Detection from Physical World", "authors": ["Satoshi Tanaka", "Koji Minoda", "Fumiya Watanabe", "Takamasa Horibe"], "summary": "High-accuracy and low-latency 3D object detection is essential for autonomous\ndriving systems. While previous studies on 3D object detection often evaluate\nperformance based on mean average precision (mAP) and latency, they typically\nfail to address the trade-off between speed and accuracy, such as 60.0 mAP at\n100 ms vs 61.0 mAP at 500 ms. A quantitative assessment of the trade-offs\nbetween different hardware devices and accelerators remains unexplored, despite\nbeing critical for real-time applications. Furthermore, they overlook the\nimpact on collision avoidance in motion planning, for example, 60.0 mAP leading\nto safer motion planning or 61.0 mAP leading to high-risk motion planning. In\nthis paper, we introduce latency-aware AP (L-AP) and planning-aware AP (P-AP)\nas new metrics, which consider the physical world such as the concept of time\nand physical constraints, offering a more comprehensive evaluation for\nreal-time 3D object detection. We demonstrate the effectiveness of our metrics\nfor the entire autonomous driving system using nuPlan dataset, and evaluate 3D\nobject detection models accounting for hardware differences and accelerators.\nWe also develop a state-of-the-art performance model for real-time 3D object\ndetection through latency-aware hyperparameter optimization (L-HPO) using our\nmetrics. Additionally, we quantitatively demonstrate that the assumption \"the\nmore point clouds, the better the recognition performance\" is incorrect for\nreal-time applications and optimize both hardware and model selection using our\nmetrics.", "comment": "15 pages, 10 figures", "pdf_url": "http://arxiv.org/pdf/2507.00190v1", "categories": ["cs.RO", "cs.CV"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2507.00190v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00451", "title": "Best Agent Identification for General Game Playing", "authors": ["Matthew Stephenson", "Alex Newcombe", "Eric Piette", "Dennis Soemers"], "summary": "We present an efficient and generalised procedure to accurately identify the\nbest performing algorithm for each sub-task in a multi-problem domain. Our\napproach treats this as a set of best arm identification problems for\nmulti-armed bandits, where each bandit corresponds to a specific task and each\narm corresponds to a specific algorithm or agent. We propose an optimistic\nselection process based on the Wilson score interval (Optimistic-WS) that ranks\neach arm across all bandits in terms of their potential regret reduction. We\nevaluate the performance of Optimistic-WS on two of the most popular general\ngame domains, the General Video Game AI (GVGAI) framework and the Ludii general\ngame playing system, with the goal of identifying the highest performing agent\nfor each game within a limited number of trials. Compared to previous best arm\nidentification algorithms for multi-armed bandits, our results demonstrate a\nsubstantial performance improvement in terms of average simple regret. This\nnovel approach can be used to significantly improve the quality and accuracy of\nagent evaluation procedures for general game frameworks, as well as other\nmulti-task domains with high algorithm runtimes.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00451v1", "categories": ["cs.LG", "cs.AI", "cs.DS", "cs.IT", "math.IT", "stat.ML"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00451v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00049", "title": "AdaDeDup: Adaptive Hybrid Data Pruning for Efficient Large-Scale Object Detection Training", "authors": ["Feiyang Kang", "Nadine Chang", "Maying Shen", "Marc T. Law", "Rafid Mahmood", "Ruoxi Jia", "Jose M. Alvarez"], "summary": "The computational burden and inherent redundancy of large-scale datasets\nchallenge the training of contemporary machine learning models. Data pruning\noffers a solution by selecting smaller, informative subsets, yet existing\nmethods struggle: density-based approaches can be task-agnostic, while\nmodel-based techniques may introduce redundancy or prove computationally\nprohibitive. We introduce Adaptive De-Duplication (AdaDeDup), a novel hybrid\nframework that synergistically integrates density-based pruning with\nmodel-informed feedback in a cluster-adaptive manner. AdaDeDup first partitions\ndata and applies an initial density-based pruning. It then employs a proxy\nmodel to evaluate the impact of this initial pruning within each cluster by\ncomparing losses on kept versus pruned samples. This task-aware signal\nadaptively adjusts cluster-specific pruning thresholds, enabling more\naggressive pruning in redundant clusters while preserving critical data in\ninformative ones. Extensive experiments on large-scale object detection\nbenchmarks (Waymo, COCO, nuScenes) using standard models (BEVFormer, Faster\nR-CNN) demonstrate AdaDeDup's advantages. It significantly outperforms\nprominent baselines, substantially reduces performance degradation (e.g., over\n54% versus random sampling on Waymo), and achieves near-original model\nperformance while pruning 20% of data, highlighting its efficacy in enhancing\ndata efficiency for large-scale model training. Code is open-sourced.", "comment": "Preprint", "pdf_url": "http://arxiv.org/pdf/2507.00049v1", "categories": ["cs.CV", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00049v1", "date": "2025-06-24", "updated": "2025-06-24"}
{"id": "2507.00206", "title": "Towards 3D Semantic Image Synthesis for Medical Imaging", "authors": ["Wenwu Tang", "Khaled Seyam", "Bin Yang"], "summary": "In the medical domain, acquiring large datasets is challenging due to both\naccessibility issues and stringent privacy regulations. Consequently, data\navailability and privacy protection are major obstacles to applying machine\nlearning in medical imaging. To address this, our study proposes the Med-LSDM\n(Latent Semantic Diffusion Model), which operates directly in the 3D domain and\nleverages de-identified semantic maps to generate synthetic data as a method of\nprivacy preservation and data augmentation. Unlike many existing methods that\nfocus on generating 2D slices, Med-LSDM is designed specifically for 3D\nsemantic image synthesis, making it well-suited for applications requiring full\nvolumetric data. Med-LSDM incorporates a guiding mechanism that controls the 3D\nimage generation process by applying a diffusion model within the latent space\nof a pre-trained VQ-GAN. By operating in the compressed latent space, the model\nsignificantly reduces computational complexity while still preserving critical\n3D spatial details. Our approach demonstrates strong performance in 3D semantic\nmedical image synthesis, achieving a 3D-FID score of 0.0054 on the conditional\nDuke Breast dataset and similar Dice scores (0.70964) to those of real images\n(0.71496). These results demonstrate that the synthetic data from our model\nhave a small domain gap with real data and are useful for data augmentation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00206v1", "categories": ["eess.IV", "cs.CV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2507.00206v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00454", "title": "ATSTrack: Enhancing Visual-Language Tracking by Aligning Temporal and Spatial Scales", "authors": ["Yihao Zhen", "Qiang Wang", "Yu Qiao", "Liangqiong Qu", "Huijie Fan"], "summary": "A main challenge of Visual-Language Tracking (VLT) is the misalignment\nbetween visual inputs and language descriptions caused by target movement.\nPrevious trackers have explored many effective feature modification methods to\npreserve more aligned features. However, an important yet unexplored factor\nultimately hinders their capability, which is the inherent differences in the\ntemporal and spatial scale of information between visual and language inputs.\nTo address this issue, we propose a novel visual-language tracker that enhances\nthe effect of feature modification by \\textbf{A}ligning \\textbf{T}emporal and\n\\textbf{S}patial scale of different input components, named as\n\\textbf{ATSTrack}. Specifically, we decompose each language description into\nphrases with different attributes based on their temporal and spatial\ncorrespondence with visual inputs, and modify their features in a fine-grained\nmanner. Moreover, we introduce a Visual-Language token that comprises modified\nlinguistic information from the previous frame to guide the model to extract\nvisual features that are more relevant to language description, thereby\nreducing the impact caused by the differences in spatial scale. Experimental\nresults show that our proposed ATSTrack achieves performance comparable to\nexisting methods. Our code will be released.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00454v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00454v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00050", "title": "SEZ-HARN: Self-Explainable Zero-shot Human Activity Recognition Network", "authors": ["Devin Y. De Silva", "Sandareka Wickramanayake", "Dulani Meedeniya", "Sanka Rasnayaka"], "summary": "Human Activity Recognition (HAR), which uses data from Inertial Measurement\nUnit (IMU) sensors, has many practical applications in healthcare and assisted\nliving environments. However, its use in real-world scenarios has been limited\nby the lack of comprehensive IMU-based HAR datasets that cover a wide range of\nactivities and the lack of transparency in existing HAR models. Zero-shot HAR\n(ZS-HAR) overcomes the data limitations, but current models struggle to explain\ntheir decisions, making them less transparent. This paper introduces a novel\nIMU-based ZS-HAR model called the Self-Explainable Zero-shot Human Activity\nRecognition Network (SEZ-HARN). It can recognize activities not encountered\nduring training and provide skeleton videos to explain its decision-making\nprocess. We evaluate the effectiveness of the proposed SEZ-HARN on four\nbenchmark datasets PAMAP2, DaLiAc, HTD-MHAD and MHealth and compare its\nperformance against three state-of-the-art black-box ZS-HAR models. The\nexperiment results demonstrate that SEZ-HARN produces realistic and\nunderstandable explanations while achieving competitive Zero-shot recognition\naccuracy. SEZ-HARN achieves a Zero-shot prediction accuracy within 3\\% of the\nbest-performing black-box model on PAMAP2 while maintaining comparable\nperformance on the other three datasets.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00050v1", "categories": ["cs.AI", "cs.HC", "cs.LG", "I.2.0"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2507.00050v1", "date": "2025-06-25", "updated": "2025-06-25"}
{"id": "2507.00209", "title": "SurgiSR4K: A High-Resolution Endoscopic Video Dataset for Robotic-Assisted Minimally Invasive Procedures", "authors": ["Fengyi Jiang", "Xiaorui Zhang", "Lingbo Jin", "Ruixing Liang", "Yuxin Chen", "Adi Chola Venkatesh", "Jason Culman", "Tiantian Wu", "Lirong Shao", "Wenqing Sun", "Cong Gao", "Hallie McNamara", "Jingpei Lu", "Omid Mohareri"], "summary": "High-resolution imaging is crucial for enhancing visual clarity and enabling\nprecise computer-assisted guidance in minimally invasive surgery (MIS). Despite\nthe increasing adoption of 4K endoscopic systems, there remains a significant\ngap in publicly available native 4K datasets tailored specifically for\nrobotic-assisted MIS. We introduce SurgiSR4K, the first publicly accessible\nsurgical imaging and video dataset captured at a native 4K resolution,\nrepresenting realistic conditions of robotic-assisted procedures. SurgiSR4K\ncomprises diverse visual scenarios including specular reflections, tool\nocclusions, bleeding, and soft tissue deformations, meticulously designed to\nreflect common challenges faced during laparoscopic and robotic surgeries. This\ndataset opens up possibilities for a broad range of computer vision tasks that\nmight benefit from high resolution data, such as super resolution (SR), smoke\nremoval, surgical instrument detection, 3D tissue reconstruction, monocular\ndepth estimation, instance segmentation, novel view synthesis, and\nvision-language model (VLM) development. SurgiSR4K provides a robust foundation\nfor advancing research in high-resolution surgical imaging and fosters the\ndevelopment of intelligent imaging technologies aimed at enhancing performance,\nsafety, and usability in image-guided robotic surgeries.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00209v1", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.RO"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2507.00209v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00459", "title": "Process-aware and high-fidelity microstructure generation using stable diffusion", "authors": ["Hoang Cuong Phan", "Minh Tien Tran", "Chihun Lee", "Hoheok Kim", "Sehyok Oh", "Dong-Kyu Kim", "Ho Won Lee"], "summary": "Synthesizing realistic microstructure images conditioned on processing\nparameters is crucial for understanding process-structure relationships in\nmaterials design. However, this task remains challenging due to limited\ntraining micrographs and the continuous nature of processing variables. To\novercome these challenges, we present a novel process-aware generative modeling\napproach based on Stable Diffusion 3.5 Large (SD3.5-Large), a state-of-the-art\ntext-to-image diffusion model adapted for microstructure generation. Our method\nintroduces numeric-aware embeddings that encode continuous variables (annealing\ntemperature, time, and magnification) directly into the model's conditioning,\nenabling controlled image generation under specified process conditions and\ncapturing process-driven microstructural variations. To address data scarcity\nand computational constraints, we fine-tune only a small fraction of the\nmodel's weights via DreamBooth and Low-Rank Adaptation (LoRA), efficiently\ntransferring the pre-trained model to the materials domain. We validate realism\nusing a semantic segmentation model based on a fine-tuned U-Net with a VGG16\nencoder on 24 labeled micrographs. It achieves 97.1% accuracy and 85.7% mean\nIoU, outperforming previous methods. Quantitative analyses using physical\ndescriptors and spatial statistics show strong agreement between synthetic and\nreal microstructures. Specifically, two-point correlation and lineal-path\nerrors remain below 2.1% and 0.6%, respectively. Our method represents the\nfirst adaptation of SD3.5-Large for process-aware microstructure generation,\noffering a scalable approach for data-driven materials design.", "comment": "46 pages, 13 figures, 5 tables, 3rd Word Congress on Artificial\n  Intelligence in Materials & Manufacturing 2025", "pdf_url": "http://arxiv.org/pdf/2507.00459v1", "categories": ["cond-mat.mtrl-sci", "cs.AI"], "cate": "cond-mat.mtrl-sci", "url": "http://arxiv.org/abs/2507.00459v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00054", "title": "Enhancing Reasoning Capabilities in SLMs with Reward Guided Dataset Distillation", "authors": ["Shreyansh Padarha"], "summary": "The push to compress and impart the proficiency of Large Language Models\n(LLMs) into more deployable and efficient Small Language Models (SLMs) has\nbenefited from improvements in knowledge distillation (KD) techniques. These\ntechniques allow a smaller student model to learn from a more capable and\nlarger teacher model's responses. However, distillation often revolves around\nthe student model merely copying the teacher's in-distribution responses,\nlimiting its generalisability. This limitation is amplified on reasoning tasks\nand can be computationally expensive. In this study, we propose AdvDistill, a\nreward-guided dataset distillation framework. We utilise multiple generations\n(responses) from a teacher for each prompt and assign rewards based on\nrule-based verifiers. These varying and normally distributed rewards serve as\nweights when training student models. Our methods and their subsequent\nbehavioural analysis demonstrate a significant improvement in student model\nperformance for mathematical and complex reasoning tasks, showcasing the\nefficacy and benefits of incorporating a rewarding mechanism in dataset\ndistillation processes.", "comment": "17 Pages, 7 figures", "pdf_url": "http://arxiv.org/pdf/2507.00054v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2507.00054v1", "date": "2025-06-25", "updated": "2025-06-25"}
{"id": "2507.00320", "title": "Exploring Theory-Laden Observations in the Brain Basis of Emotional Experience", "authors": ["Christiana Westlin", "Ashutosh Singh", "Deniz Erdogmus", "Georgios Stratis", "Lisa Feldman Barrett"], "summary": "In the science of emotion, it is widely assumed that folk emotion categories\nform a biological and psychological typology, and studies are routinely\ndesigned and analyzed to identify emotion-specific patterns. This approach\nshapes the observations that studies report, ultimately reinforcing the\nassumption that guided the investigation. Here, we reanalyzed data from one\nsuch typologically-guided study that reported mappings between individual brain\npatterns and group-averaged ratings of 34 emotion categories. Our reanalysis\nwas guided by an alternative view of emotion categories as populations of\nvariable, situated instances, and which predicts a priori that there will be\nsignificant variation in brain patterns within a category across instances.\nCorrespondingly, our analysis made minimal assumptions about the structure of\nthe variance present in the data. As predicted, we did not observe the original\nmappings and instead observed significant variation across individuals. These\nfindings demonstrate how starting assumptions can ultimately impact scientific\nconclusions and suggest that a hypothesis must be supported using multiple\nanalytic methods before it is taken seriously.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00320v1", "categories": ["cs.LG", "cs.CV", "q-bio.NC"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00320v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00461", "title": "Novel Complex-Valued Hopfield Neural Networks with Phase and Magnitude Quantization", "authors": ["Garimella Ramamurthy", "Marcos Eduardo Valle", "Tata Jagannadha Swamy"], "summary": "This research paper introduces two novel complex-valued Hopfield neural\nnetworks (CvHNNs) that incorporate phase and magnitude quantization. The first\nCvHNN employs a ceiling-type activation function that operates on the\nrectangular coordinate representation of the complex net contribution. The\nsecond CvHNN similarly incorporates phase and magnitude quantization but\nutilizes a ceiling-type activation function based on the polar coordinate\nrepresentation of the complex net contribution. The proposed CvHNNs, with their\nphase and magnitude quantization, significantly increase the number of states\ncompared to existing models in the literature, thereby expanding the range of\npotential applications for CvHNNs.", "comment": "Paper submitted to the Fifth International Conference on Emerging\n  Techniques in Computational Intelligence (ICETCI 2025)", "pdf_url": "http://arxiv.org/pdf/2507.00461v1", "categories": ["cs.NE", "cs.AI"], "cate": "cs.NE", "url": "http://arxiv.org/abs/2507.00461v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00057", "title": "Estimating Correctness Without Oracles in LLM-Based Code Generation", "authors": ["Thomas Valentin", "Ardi Madadi", "Gaetano Sapia", "Marcel Böhme"], "summary": "Generating code from natural language specifications is one of the most\nsuccessful applications of Large Language Models (LLMs). Yet, they hallucinate:\nLLMs produce outputs that may be grammatically correct but are factually\nincorrect. Without an existing, correct implementation (i.e., an oracle), can\nwe quantify how likely the generated program is correct?\n  In this paper, we propose a measure of incorrectness, called incoherence,\nthat can be estimated efficiently in the absence of an oracle and provides a\nlower bound on the error, i.e., the probability that the LLM-generated program\nfor that specification is incorrect. Our experiments demonstrate an\nextraordinary effectiveness. For the average code generation task, our\nincoherence-based methodology can automatically identify about two-thirds of\nincorrect programs without reports of false positives. In fact, an oracle-based\nevaluation of LLMs can be reliably replaced by an incoherence-based evaluation.\nIn particular, we find a very strong agreement between the ranking of LLMs by\nthe number of programs deemed correct via an oracle (pass@1) and the ranking of\nLLMs by the number of programs deemed correct via our incoherence.", "comment": "8 pages + refs and appendix", "pdf_url": "http://arxiv.org/pdf/2507.00057v1", "categories": ["cs.PL", "cs.AI", "cs.LG", "cs.SE"], "cate": "cs.PL", "url": "http://arxiv.org/abs/2507.00057v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2507.00333", "title": "Scope Meets Screen: Lessons Learned in Designing Composite Visualizations for Marksmanship Training Across Skill Levels", "authors": ["Emin Zerman", "Jonas Carlsson", "Mårten Sjöström"], "summary": "Marksmanship practices are required in various professions, including police,\nmilitary personnel, hunters, as well as sports shooters, such as Olympic\nshooting, biathlon, and modern pentathlon. The current form of training and\ncoaching is mostly based on repetition, where the coach does not see through\nthe eyes of the shooter, and analysis is limited to stance and accuracy\npost-session. In this study, we present a shooting visualization system and\nevaluate its perceived effectiveness for both novice and expert shooters. To\nachieve this, five composite visualizations were developed using first-person\nshooting video recordings enriched with overlaid metrics and graphical\nsummaries. These views were evaluated with 10 participants (5 expert marksmen,\n5 novices) through a mixed-methods study including shot-count and aiming\ninterpretation tasks, pairwise preference comparisons, and semi-structured\ninterviews. The results show that a dashboard-style composite view, combining\nraw video with a polar plot and selected graphs, was preferred in 9 of 10 cases\nand supported understanding across skill levels. The insights gained from this\ndesign study point to the broader value of integrating first-person video with\nvisual analytics for coaching, and we suggest directions for applying this\napproach to other precision-based sports.", "comment": "5 pages, accepted at IEEE VIS 2025", "pdf_url": "http://arxiv.org/pdf/2507.00333v1", "categories": ["cs.HC", "cs.CV", "cs.GR", "eess.IV"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2507.00333v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00467", "title": "Diversity Conscious Refined Random Forest", "authors": ["Sijan Bhattarai", "Saurav Bhandari", "Girija Bhusal", "Saroj Shakya", "Tapendra Pandey"], "summary": "Random Forest (RF) is a widely used ensemble learning technique known for its\nrobust classification performance across diverse domains. However, it often\nrelies on hundreds of trees and all input features, leading to high inference\ncost and model redundancy. In this work, our goal is to grow trees dynamically\nonly on informative features and then enforce maximal diversity by clustering\nand retaining uncorrelated trees. Therefore, we propose a Refined Random Forest\nClassifier that iteratively refines itself by first removing the least\ninformative features and then analytically determines how many new trees should\nbe grown, followed by correlation-based clustering to remove redundant trees.\nThe classification accuracy of our model was compared against the standard RF\non the same number of trees. Experiments on 8 multiple benchmark datasets,\nincluding binary and multiclass datasets, demonstrate that the proposed model\nachieves improved accuracy compared to standard RF.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00467v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00467v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00079", "title": "VoyagerVision: Investigating the Role of Multi-modal Information for Open-ended Learning Systems", "authors": ["Ethan Smyth", "Alessandro Suglia"], "summary": "Open-endedness is an active field of research in the pursuit of capable\nArtificial General Intelligence (AGI), allowing models to pursue tasks of their\nown choosing. Simultaneously, recent advancements in Large Language Models\n(LLMs) such as GPT-4o [9] have allowed such models to be capable of\ninterpreting image inputs. Implementations such as OMNI-EPIC [4] have made use\nof such features, providing an LLM with pixel data of an agent's POV to parse\nthe environment and allow it to solve tasks. This paper proposes that providing\nthese visual inputs to a model gives it greater ability to interpret spatial\nenvironments, and as such, can increase the number of tasks it can successfully\nperform, extending its open-ended potential. To this aim, this paper proposes\nVoyagerVision -- a multi-modal model capable of creating structures within\nMinecraft using screenshots as a form of visual feedback, building on the\nfoundation of Voyager. VoyagerVision was capable of creating an average of 2.75\nunique structures within fifty iterations of the system, as Voyager was\nincapable of this, it is an extension in an entirely new direction.\nAdditionally, in a set of building unit tests VoyagerVision was successful in\nhalf of all attempts in flat worlds, with most failures arising in more complex\nstructures. Project website is available at\nhttps://esmyth-dev.github.io/VoyagerVision.github.io/", "comment": "website: https://esmyth-dev.github.io/VoyagerVision.github.io/", "pdf_url": "http://arxiv.org/pdf/2507.00079v1", "categories": ["cs.AI", "cs.LG"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2507.00079v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2507.00398", "title": "Accurate and Efficient Fetal Birth Weight Estimation from 3D Ultrasound", "authors": ["Jian Wang", "Qiongying Ni", "Hongkui Yu", "Ruixuan Yao", "Jinqiao Ying", "Bin Zhang", "Xingyi Yang", "Jin Peng", "Jiongquan Chen", "Junxuan Yu", "Wenlong Shi", "Chaoyu Chen", "Zhongnuo Yan", "Mingyuan Luo", "Gaocheng Cai", "Dong Ni", "Jing Lu", "Xin Yang"], "summary": "Accurate fetal birth weight (FBW) estimation is essential for optimizing\ndelivery decisions and reducing perinatal mortality. However, clinical methods\nfor FBW estimation are inefficient, operator-dependent, and challenging to\napply in cases of complex fetal anatomy. Existing deep learning methods are\nbased on 2D standard ultrasound (US) images or videos that lack spatial\ninformation, limiting their prediction accuracy. In this study, we propose the\nfirst method for directly estimating FBW from 3D fetal US volumes. Our approach\nintegrates a multi-scale feature fusion network (MFFN) and a synthetic\nsample-based learning framework (SSLF). The MFFN effectively extracts and fuses\nmulti-scale features under sparse supervision by incorporating channel\nattention, spatial attention, and a ranking-based loss function. SSLF generates\nsynthetic samples by simply combining fetal head and abdomen data from\ndifferent fetuses, utilizing semi-supervised learning to improve prediction\nperformance. Experimental results demonstrate that our method achieves superior\nperformance, with a mean absolute error of $166.4\\pm155.9$ $g$ and a mean\nabsolute percentage error of $5.1\\pm4.6$%, outperforming existing methods and\napproaching the accuracy of a senior doctor. Code is available at:\nhttps://github.com/Qioy-i/EFW.", "comment": "Accepted by MICCAI 2025", "pdf_url": "http://arxiv.org/pdf/2507.00398v1", "categories": ["eess.IV", "cs.CV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2507.00398v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00482", "title": "Physics-Aware Style Transfer for Adaptive Holographic Reconstruction", "authors": ["Chanseok Lee", "Fakhriyya Mammadova", "Jiseong Barg", "Mooseok Jang"], "summary": "Inline holographic imaging presents an ill-posed inverse problem of\nreconstructing objects' complex amplitude from recorded diffraction patterns.\nAlthough recent deep learning approaches have shown promise over classical\nphase retrieval algorithms, they often require high-quality ground truth\ndatasets of complex amplitude maps to achieve a statistical inverse mapping\noperation between the two domains. Here, we present a physics-aware style\ntransfer approach that interprets the object-to-sensor distance as an implicit\nstyle within diffraction patterns. Using the style domain as the intermediate\ndomain to construct cyclic image translation, we show that the inverse mapping\noperation can be learned in an adaptive manner only with datasets composed of\nintensity measurements. We further demonstrate its biomedical applicability by\nreconstructing the morphology of dynamically flowing red blood cells,\nhighlighting its potential for real-time, label-free imaging. As a framework\nthat leverages physical cues inherently embedded in measurements, the presented\nmethod offers a practical learning strategy for imaging applications where\nground truth is difficult or impossible to obtain.", "comment": "Keywords: holographic imaging, style transfer, phase retrieval, deep\n  learning", "pdf_url": "http://arxiv.org/pdf/2507.00482v1", "categories": ["physics.optics", "cs.AI", "cs.LG"], "cate": "physics.optics", "url": "http://arxiv.org/abs/2507.00482v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00092", "title": "Thinking About Thinking: SAGE-nano's Inverse Reasoning for Self-Aware Language Models", "authors": ["Basab Jha", "Firoj Paudel", "Ujjwal Puri", "Zhang Yuting", "Choi Donghyuk", "Wang Junhao"], "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities at\nsolving complex reasoning tasks with Chain-of-Thought (CoT) prompting, but\ntheir decision-making processes remain somewhat blackbox. We introduce\ntextbfinverse reasoning, a novel paradigm enabling LLMs to decompose and\nexplain their own reasoning chains post-hoc. Our approach, used in SAGE-nano, a\n4-billion-parameter reasoning model, employs a metacognitive structure that\nreflects back via attention processes to identify major decision points and\ngenerate explanations of reasoning choices. While typical CoT approaches are\ndirected towards forward reasoning generation, inverse reasoning provides\ninsight into why specific reasoning chains were selected over others. Through\nthorough testing of logical reasoning puzzles, math problems and ethical\ndilemmas from AQUA-RAT, CommonsenseQA, and customized benchmarks, we\ndemonstrate that SAGE-nano is at the cutting edge both on reasoning accuracy\n(74.6% on AQUA-RAT) and explanation quality (92.1% human preference score) for\nits task, and offers performance almost on par with models like Claude-3.5\nSonnet or GPT-4o. Our contributions are: (i) the first rigorous framework for\nLLM self-reflection via inverse reasoning, (ii) a novel metalearning framework\nto reverse the attention flow, (iii) comprehensive evaluation frameworks for\nreasoning transparency, and (iv) evidence that increasing reasoning using\ninverse reasoning improves interpretability along with reasoning performance.\nOur work creates new avenues for transparent AI systems and closes significant\ngaps in AI safety, education, and scientific discovery.", "comment": "19 pages, 2 figures, 9 tables", "pdf_url": "http://arxiv.org/pdf/2507.00092v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2507.00092v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00416", "title": "Evo-0: Vision-Language-Action Model with Implicit Spatial Understanding", "authors": ["Tao Lin", "Gen Li", "Yilei Zhong", "Yanwen Zou", "Bo Zhao"], "summary": "Vision-Language-Action (VLA) models have emerged as a promising framework for\nenabling generalist robots capable of perceiving, reasoning, and acting in the\nreal world. These models usually build upon pretrained Vision-Language Models\n(VLMs), which excel at semantic understanding due to large-scale text\npretraining. However, VLMs typically lack precise spatial understanding\ncapabilities, as they are primarily tuned on 2D image-text pairs without 3D\nsupervision. To address this limitation, recent approaches have incorporated\nexplicit 3D inputs such as point clouds or depth maps, but this necessitates\nadditional depth sensors or defective estimation. In contrast, our work\nintroduces a plug-and-play module that implicitly injects 3D geometry features\ninto VLA models by leveraging an off-the-shelf visual geometry foundation\nmodels. We design five spatially challenging tasks that require precise spatial\nunderstanding ability to validate effectiveness of our method. Extensive\nevaluations show that our method significantly improves the performance of\nstate-of-the-art VLA models across diverse scenarios.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00416v1", "categories": ["cs.RO", "cs.CV"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2507.00416v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00485", "title": "PNAct: Crafting Backdoor Attacks in Safe Reinforcement Learning", "authors": ["Weiran Guo", "Guanjun Liu", "Ziyuan Zhou", "Ling Wang"], "summary": "Reinforcement Learning (RL) is widely used in tasks where agents interact\nwith an environment to maximize rewards. Building on this foundation, Safe\nReinforcement Learning (Safe RL) incorporates a cost metric alongside the\nreward metric, ensuring that agents adhere to safety constraints during\ndecision-making. In this paper, we identify that Safe RL is vulnerable to\nbackdoor attacks, which can manipulate agents into performing unsafe actions.\nFirst, we introduce the relevant concepts and evaluation metrics for backdoor\nattacks in Safe RL. It is the first attack framework in the Safe RL field that\ninvolves both Positive and Negative Action sample (PNAct) is to implant\nbackdoors, where positive action samples provide reference actions and negative\naction samples indicate actions to be avoided. We theoretically point out the\nproperties of PNAct and design an attack algorithm. Finally, we conduct\nexperiments to evaluate the effectiveness of our proposed backdoor attack\nframework, evaluating it with the established metrics. This paper highlights\nthe potential risks associated with Safe RL and underscores the feasibility of\nsuch attacks. Our code and supplementary material are available at\nhttps://github.com/azure-123/PNAct.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00485v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00485v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00180", "title": "BlackBoxToBlueprint: Extracting Interpretable Logic from Legacy Systems using Reinforcement Learning and Counterfactual Analysis", "authors": ["Vidhi Rathore"], "summary": "Modernizing legacy software systems is a critical but challenging task, often\nhampered by a lack of documentation and understanding of the original system's\nintricate decision logic. Traditional approaches like behavioral cloning merely\nreplicate input-output behavior without capturing the underlying intent. This\npaper proposes a novel pipeline to automatically extract interpretable decision\nlogic from legacy systems treated as black boxes. The approach uses a\nReinforcement Learning (RL) agent to explore the input space and identify\ncritical decision boundaries by rewarding actions that cause meaningful changes\nin the system's output. These counterfactual state transitions, where the\noutput changes, are collected and clustered using K-Means. Decision trees are\nthen trained on these clusters to extract human-readable rules that approximate\nthe system's decision logic near the identified boundaries. I demonstrated the\npipeline's effectiveness on three dummy legacy systems with varying complexity,\nincluding threshold-based, combined-conditional, and non-linear range logic.\nResults show that the RL agent successfully focuses exploration on relevant\nboundary regions, and the extracted rules accurately reflect the core logic of\nthe underlying dummy systems, providing a promising foundation for generating\nspecifications and test cases during legacy migration.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00180v1", "categories": ["cs.AI", "cs.LG"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2507.00180v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00435", "title": "RoboEval: Where Robotic Manipulation Meets Structured and Scalable Evaluation", "authors": ["Yi Ru Wang", "Carter Ung", "Grant Tannert", "Jiafei Duan", "Josephine Li", "Amy Le", "Rishabh Oswal", "Markus Grotz", "Wilbert Pumacay", "Yuquan Deng", "Ranjay Krishna", "Dieter Fox", "Siddhartha Srinivasa"], "summary": "We present RoboEval, a simulation benchmark and structured evaluation\nframework designed to reveal the limitations of current bimanual manipulation\npolicies. While prior benchmarks report only binary task success, we show that\nsuch metrics often conceal critical weaknesses in policy behavior -- such as\npoor coordination, slipping during grasping, or asymmetric arm usage. RoboEval\nintroduces a suite of tiered, semantically grounded tasks decomposed into\nskill-specific stages, with variations that systematically challenge spatial,\nphysical, and coordination capabilities. Tasks are paired with fine-grained\ndiagnostic metrics and 3000+ human demonstrations to support imitation\nlearning. Our experiments reveal that policies with similar success rates\ndiverge in how tasks are executed -- some struggle with alignment, others with\ntemporally consistent bimanual control. We find that behavioral metrics\ncorrelate with success in over half of task-metric pairs, and remain\ninformative even when binary success saturates. By pinpointing when and how\npolicies fail, RoboEval enables a deeper, more actionable understanding of\nrobotic manipulation -- and highlights the need for evaluation tools that go\nbeyond success alone.", "comment": "Project page: https://robo-eval.github.io", "pdf_url": "http://arxiv.org/pdf/2507.00435v1", "categories": ["cs.RO", "cs.AI", "cs.CV"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2507.00435v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00491", "title": "Twill: Scheduling Compound AI Systems on Heterogeneous Mobile Edge Platforms", "authors": ["Zain Taufique", "Aman Vyas", "Antonio Miele", "Pasi Liljeberg", "Anil Kanduri"], "summary": "Compound AI (cAI) systems chain multiple AI models to solve complex problems.\ncAI systems are typically composed of deep neural networks (DNNs),\ntransformers, and large language models (LLMs), exhibiting a high degree of\ncomputational diversity and dynamic workload variation. Deploying cAI services\non mobile edge platforms poses a significant challenge in scheduling concurrent\nDNN-transformer inference tasks, which arrive dynamically in an unknown\nsequence. Existing mobile edge AI inference strategies manage multi-DNN or\ntransformer-only workloads, relying on design-time profiling, and cannot handle\nconcurrent inference of DNNs and transformers required by cAI systems. In this\nwork, we address the challenge of scheduling cAI systems on heterogeneous\nmobile edge platforms. We present Twill, a run-time framework to handle\nconcurrent inference requests of cAI workloads through task affinity-aware\ncluster mapping and migration, priority-aware task freezing/unfreezing, and\nDVFS, while minimizing inference latency within power budgets. We implement and\ndeploy our Twill framework on the Nvidia Jetson Orin NX platform. We evaluate\nTwill against state-of-the-art edge AI inference techniques over contemporary\nDNNs and LLMs, reducing inference latency by 54% on average, while honoring\npower budgets.", "comment": "9 Pages, 9 Figures, Accepted in International Conference on\n  Computer-Aided Design (ICCAD) 2025", "pdf_url": "http://arxiv.org/pdf/2507.00491v1", "categories": ["cs.MA", "cs.AI", "cs.CV", "cs.PF"], "cate": "cs.MA", "url": "http://arxiv.org/abs/2507.00491v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00205", "title": "Holistic Artificial Intelligence in Medicine; improved performance and explainability", "authors": ["Periklis Petridis", "Georgios Margaritis", "Vasiliki Stoumpou", "Dimitris Bertsimas"], "summary": "With the increasing interest in deploying Artificial Intelligence in\nmedicine, we previously introduced HAIM (Holistic AI in Medicine), a framework\nthat fuses multimodal data to solve downstream clinical tasks. However, HAIM\nuses data in a task-agnostic manner and lacks explainability. To address these\nlimitations, we introduce xHAIM (Explainable HAIM), a novel framework\nleveraging Generative AI to enhance both prediction and explainability through\nfour structured steps: (1) automatically identifying task-relevant patient data\nacross modalities, (2) generating comprehensive patient summaries, (3) using\nthese summaries for improved predictive modeling, and (4) providing clinical\nexplanations by linking predictions to patient-specific medical knowledge.\nEvaluated on the HAIM-MIMIC-MM dataset, xHAIM improves average AUC from 79.9%\nto 90.3% across chest pathology and operative tasks. Importantly, xHAIM\ntransforms AI from a black-box predictor into an explainable decision support\nsystem, enabling clinicians to interactively trace predictions back to relevant\npatient data, bridging AI advancements with clinical utility.", "comment": "Submitted to npj Digital Medicine", "pdf_url": "http://arxiv.org/pdf/2507.00205v1", "categories": ["cs.AI", "cs.LG"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2507.00205v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00476", "title": "FreNBRDF: A Frequency-Rectified Neural Material Representation", "authors": ["Chenliang Zhou", "Zheyuan Hu", "Cengiz Oztireli"], "summary": "Accurate material modeling is crucial for achieving photorealistic rendering,\nbridging the gap between computer-generated imagery and real-world photographs.\nWhile traditional approaches rely on tabulated BRDF data, recent work has\nshifted towards implicit neural representations, which offer compact and\nflexible frameworks for a range of tasks. However, their behavior in the\nfrequency domain remains poorly understood. To address this, we introduce\nFreNBRDF, a frequency-rectified neural material representation. By leveraging\nspherical harmonics, we integrate frequency-domain considerations into neural\nBRDF modeling. We propose a novel frequency-rectified loss, derived from a\nfrequency analysis of neural materials, and incorporate it into a generalizable\nand adaptive reconstruction and editing pipeline. This framework enhances\nfidelity, adaptability, and efficiency. Extensive experiments demonstrate that\n\\ours improves the accuracy and robustness of material appearance\nreconstruction and editing compared to state-of-the-art baselines, enabling\nmore structured and interpretable downstream tasks and applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00476v1", "categories": ["cs.GR", "cs.CV"], "cate": "cs.GR", "url": "http://arxiv.org/abs/2507.00476v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00493", "title": "Visual Anagrams Reveal Hidden Differences in Holistic Shape Processing Across Vision Models", "authors": ["Fenil R. Doshi", "Thomas Fel", "Talia Konkle", "George Alvarez"], "summary": "Humans are able to recognize objects based on both local texture cues and the\nconfiguration of object parts, yet contemporary vision models primarily harvest\nlocal texture cues, yielding brittle, non-compositional features. Work on\nshape-vs-texture bias has pitted shape and texture representations in\nopposition, measuring shape relative to texture, ignoring the possibility that\nmodels (and humans) can simultaneously rely on both types of cues, and\nobscuring the absolute quality of both types of representation. We therefore\nrecast shape evaluation as a matter of absolute configural competence,\noperationalized by the Configural Shape Score (CSS), which (i) measures the\nability to recognize both images in Object-Anagram pairs that preserve local\ntexture while permuting global part arrangement to depict different object\ncategories. Across 86 convolutional, transformer, and hybrid models, CSS (ii)\nuncovers a broad spectrum of configural sensitivity with fully self-supervised\nand language-aligned transformers -- exemplified by DINOv2, SigLIP2 and\nEVA-CLIP -- occupying the top end of the CSS spectrum. Mechanistic probes\nreveal that (iii) high-CSS networks depend on long-range interactions:\nradius-controlled attention masks abolish performance showing a distinctive\nU-shaped integration profile, and representational-similarity analyses expose a\nmid-depth transition from local to global coding. A BagNet control remains at\nchance (iv), ruling out \"border-hacking\" strategies. Finally, (v) we show that\nconfigural shape score also predicts other shape-dependent evals. Overall, we\npropose that the path toward truly robust, generalizable, and human-like vision\nsystems may not lie in forcing an artificial choice between shape and texture,\nbut rather in architectural and learning frameworks that seamlessly integrate\nboth local-texture and global configural shape.", "comment": "Project page: https://www.fenildoshi.com/configural-shape/", "pdf_url": "http://arxiv.org/pdf/2507.00493v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00493v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00248", "title": "Developing Lightweight DNN Models With Limited Data For Real-Time Sign Language Recognition", "authors": ["Nikita Nikitin", "Eugene Fomin"], "summary": "We present a novel framework for real-time sign language recognition using\nlightweight DNNs trained on limited data. Our system addresses key challenges\nin sign language recognition, including data scarcity, high computational\ncosts, and discrepancies in frame rates between training and inference\nenvironments. By encoding sign language specific parameters, such as handshape,\npalm orientation, movement, and location into vectorized inputs, and leveraging\nMediaPipe for landmark extraction, we achieve highly separable input data\nrepresentations. Our DNN architecture, optimized for sub 10MB deployment,\nenables accurate classification of 343 signs with less than 10ms latency on\nedge devices. The data annotation platform 'slait data' facilitates structured\nlabeling and vector extraction. Our model achieved 92% accuracy in isolated\nsign recognition and has been integrated into the 'slait ai' web application,\nwhere it demonstrates stable inference.", "comment": "7 pages, 2 figures, 2 tables, for associated mpeg file, see\n  https://slait.app/static/Screen_Recording.mp4", "pdf_url": "http://arxiv.org/pdf/2507.00248v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00248v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00491", "title": "Twill: Scheduling Compound AI Systems on Heterogeneous Mobile Edge Platforms", "authors": ["Zain Taufique", "Aman Vyas", "Antonio Miele", "Pasi Liljeberg", "Anil Kanduri"], "summary": "Compound AI (cAI) systems chain multiple AI models to solve complex problems.\ncAI systems are typically composed of deep neural networks (DNNs),\ntransformers, and large language models (LLMs), exhibiting a high degree of\ncomputational diversity and dynamic workload variation. Deploying cAI services\non mobile edge platforms poses a significant challenge in scheduling concurrent\nDNN-transformer inference tasks, which arrive dynamically in an unknown\nsequence. Existing mobile edge AI inference strategies manage multi-DNN or\ntransformer-only workloads, relying on design-time profiling, and cannot handle\nconcurrent inference of DNNs and transformers required by cAI systems. In this\nwork, we address the challenge of scheduling cAI systems on heterogeneous\nmobile edge platforms. We present Twill, a run-time framework to handle\nconcurrent inference requests of cAI workloads through task affinity-aware\ncluster mapping and migration, priority-aware task freezing/unfreezing, and\nDVFS, while minimizing inference latency within power budgets. We implement and\ndeploy our Twill framework on the Nvidia Jetson Orin NX platform. We evaluate\nTwill against state-of-the-art edge AI inference techniques over contemporary\nDNNs and LLMs, reducing inference latency by 54% on average, while honoring\npower budgets.", "comment": "9 Pages, 9 Figures, Accepted in International Conference on\n  Computer-Aided Design (ICCAD) 2025", "pdf_url": "http://arxiv.org/pdf/2507.00491v1", "categories": ["cs.MA", "cs.AI", "cs.CV", "cs.PF"], "cate": "cs.MA", "url": "http://arxiv.org/abs/2507.00491v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00509", "title": "TeamCMU at Touché: Adversarial Co-Evolution for Advertisement Integration and Detection in Conversational Search", "authors": ["To Eun Kim", "João Coelho", "Gbemileke Onilude", "Jai Singh"], "summary": "As conversational search engines increasingly adopt generation-based\nparadigms powered by Large Language Models (LLMs) and Retrieval-Augmented\nGeneration (RAG), the integration of advertisements into generated responses\npresents both commercial opportunities and challenges for user experience.\nUnlike traditional search, where advertisements are clearly delineated,\ngenerative systems blur the boundary between informational content and\npromotional material, raising concerns around transparency and trust. In this\nwork, we propose a modular pipeline for advertisement management in RAG-based\nconversational systems, consisting of an ad-rewriter for seamless ad\nintegration and a robust ad-classifier for detection. We leverage synthetic\ndata to train high-performing classifiers, which are then used to guide two\ncomplementary ad-integration strategies: supervised fine-tuning of the\nad-rewriter and a best-of-N sampling approach that selects the least detectable\nad-integrated response among multiple candidates. Our evaluation focuses on two\ncore questions: the effectiveness of ad classifiers in detecting diverse ad\nintegration strategies, and the training methods that best support coherent,\nminimally intrusive ad insertion. Experimental results show that our\nad-classifier, trained on synthetic advertisement data inspired by marketing\nstrategies and enhanced through curriculum learning, achieves robust detection\nperformance. Additionally, we demonstrate that classifier-guided optimization,\nthrough both fine-tuning and best-of-N sampling, significantly improves ad\nstealth, enabling more seamless integration. These findings contribute an\nadversarial co-evolution framework for developing more sophisticated ad-aware\ngenerative search systems and robust ad classifiers.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00509v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2507.00509v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00260", "title": "Disentangled Feature Importance", "authors": ["Jin-Hong Du", "Kathryn Roeder", "Larry Wasserman"], "summary": "Feature importance quantification faces a fundamental challenge: when\npredictors are correlated, standard methods systematically underestimate their\ncontributions. We prove that major existing approaches target identical\npopulation functionals under squared-error loss, revealing why they share this\ncorrelation-induced bias.\n  To address this limitation, we introduce \\emph{Disentangled Feature\nImportance (DFI)}, a nonparametric generalization of the classical $R^2$\ndecomposition via optimal transport. DFI transforms correlated features into\nindependent latent variables using a transport map, eliminating correlation\ndistortion. Importance is computed in this disentangled space and attributed\nback through the transport map's sensitivity. DFI provides a principled\ndecomposition of importance scores that sum to the total predictive variability\nfor latent additive models and to interaction-weighted functional ANOVA\nvariances more generally, under arbitrary feature dependencies.\n  We develop a comprehensive semiparametric theory for DFI. For general\ntransport maps, we establish root-$n$ consistency and asymptotic normality of\nimportance estimators in the latent space, which extends to the original\nfeature space for the Bures-Wasserstein map. Notably, our estimators achieve\nsecond-order estimation error, which vanishes if both regression function and\ntransport map estimation errors are $o_{\\mathbb{P}}(n^{-1/4})$. By design, DFI\navoids the computational burden of repeated submodel refitting and the\nchallenges of conditional covariate distribution estimation, thereby achieving\ncomputational efficiency.", "comment": "26 main and 29 supplementary pages", "pdf_url": "http://arxiv.org/pdf/2507.00260v1", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.ME", "stat.TH"], "cate": "stat.ML", "url": "http://arxiv.org/abs/2507.00260v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00498", "title": "MuteSwap: Silent Face-based Voice Conversion", "authors": ["Yifan Liu", "Yu Fang", "Zhouhan Lin"], "summary": "Conventional voice conversion modifies voice characteristics from a source\nspeaker to a target speaker, relying on audio input from both sides. However,\nthis process becomes infeasible when clean audio is unavailable, such as in\nsilent videos or noisy environments. In this work, we focus on the task of\nSilent Face-based Voice Conversion (SFVC), which does voice conversion entirely\nfrom visual inputs. i.e., given images of a target speaker and a silent video\nof a source speaker containing lip motion, SFVC generates speech aligning the\nidentity of the target speaker while preserving the speech content in the\nsource silent video. As this task requires generating intelligible speech and\nconverting identity using only visual cues, it is particularly challenging. To\naddress this, we introduce MuteSwap, a novel framework that employs contrastive\nlearning to align cross-modality identities and minimize mutual information to\nseparate shared visual features. Experimental results show that MuteSwap\nachieves impressive performance in both speech synthesis and identity\nconversion, especially under noisy conditions where methods dependent on audio\ninput fail to produce intelligible results, demonstrating both the\neffectiveness of our training approach and the feasibility of SFVC.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00498v1", "categories": ["cs.SD", "cs.CV", "cs.LG", "cs.MM", "eess.AS"], "cate": "cs.SD", "url": "http://arxiv.org/abs/2507.00498v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00513", "title": "Customer Service Representative's Perception of the AI Assistant in an Organization's Call Center", "authors": ["Kai Qin", "Kexin Du", "Yimeng Chen", "Yueyan Liu", "Jie Cai", "Zhiqiang Nie", "Nan Gao", "Guohui Wei", "Shengzhu Wang", "Chun Yu"], "summary": "The integration of various AI tools creates a complex socio-technical\nenvironment where employee-customer interactions form the core of work\npractices. This study investigates how customer service representatives (CSRs)\nat the power grid service customer service call center perceive AI assistance\nin their interactions with customers. Through a field visit and semi-structured\ninterviews with 13 CSRs, we found that AI can alleviate some traditional\nburdens during the call (e.g., typing and memorizing) but also introduces new\nburdens (e.g., earning, compliance, psychological burdens). This research\ncontributes to a more nuanced understanding of AI integration in organizational\nsettings and highlights the efforts and burdens undertaken by CSRs to adapt to\nthe updated system.", "comment": "ACM CSCW Poster 2025", "pdf_url": "http://arxiv.org/pdf/2507.00513v1", "categories": ["cs.HC", "cs.AI", "cs.CY"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2507.00513v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00263", "title": "Room Scene Discovery and Grouping in Unstructured Vacation Rental Image Collections", "authors": ["Vignesh Ram Nithin Kappagantula", "Shayan Hassantabar"], "summary": "The rapid growth of vacation rental (VR) platforms has led to an increasing\nvolume of property images, often uploaded without structured categorization.\nThis lack of organization poses significant challenges for travelers attempting\nto understand the spatial layout of a property, particularly when multiple\nrooms of the same type are present. To address this issue, we introduce an\neffective approach for solving the room scene discovery and grouping problem,\nas well as identifying bed types within each bedroom group. This grouping is\nvaluable for travelers to comprehend the spatial organization, layout, and the\nsleeping configuration of the property. We propose a computationally efficient\nmachine learning pipeline characterized by low latency and the ability to\nperform effectively with sample-efficient learning, making it well-suited for\nreal-time and data-scarce environments. The pipeline integrates a supervised\nroom-type detection model, a supervised overlap detection model to identify the\noverlap similarity between two images, and a clustering algorithm to group the\nimages of the same space together using the similarity scores. Additionally,\nthe pipeline maps each bedroom group to the corresponding bed types specified\nin the property's metadata, based on the visual content present in the group's\nimages using a Multi-modal Large Language Model (MLLM) model. We evaluate the\naforementioned models individually and also assess the pipeline in its\nentirety, observing strong performance that significantly outperforms\nestablished approaches such as contrastive learning and clustering with\npretrained embeddings.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00263v1", "categories": ["cs.CV", "cs.LG", "cs.NE"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00263v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00511", "title": "Medical Image Segmentation Using Advanced Unet: VMSE-Unet and VM-Unet CBAM+", "authors": ["Sayandeep Kanrar", "Raja Piyush", "Qaiser Razi", "Debanshi Chakraborty", "Vikas Hassija", "GSS Chalapathi"], "summary": "In this paper, we present the VMSE U-Net and VM-Unet CBAM+ model, two\ncutting-edge deep learning architectures designed to enhance medical image\nsegmentation. Our approach integrates Squeeze-and-Excitation (SE) and\nConvolutional Block Attention Module (CBAM) techniques into the traditional VM\nU-Net framework, significantly improving segmentation accuracy, feature\nlocalization, and computational efficiency. Both models show superior\nperformance compared to the baseline VM-Unet across multiple datasets. Notably,\nVMSEUnet achieves the highest accuracy, IoU, precision, and recall while\nmaintaining low loss values. It also exhibits exceptional computational\nefficiency with faster inference times and lower memory usage on both GPU and\nCPU. Overall, the study suggests that the enhanced architecture VMSE-Unet is a\nvaluable tool for medical image analysis. These findings highlight its\npotential for real-world clinical applications, emphasizing the importance of\nfurther research to optimize accuracy, robustness, and computational\nefficiency.", "comment": "under review", "pdf_url": "http://arxiv.org/pdf/2507.00511v1", "categories": ["eess.IV", "cs.CV", "cs.LG"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2507.00511v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00525", "title": "Box-QAymo: Box-Referring VQA Dataset for Autonomous Driving", "authors": ["Djamahl Etchegaray", "Yuxia Fu", "Zi Huang", "Yadan Luo"], "summary": "Interpretable communication is essential for safe and trustworthy autonomous\ndriving, yet current vision-language models (VLMs) often operate under\nidealized assumptions and struggle to capture user intent in real-world\nscenarios. Existing driving-oriented VQA datasets are limited to full-scene\ndescriptions or waypoint prediction, preventing the assessment of whether VLMs\ncan respond to localized user-driven queries. We introduce Box-QAymo, a\nbox-referring dataset and benchmark designed to both evaluate and finetune VLMs\non spatial and temporal reasoning over user-specified objects. Users express\nintent by drawing bounding boxes, offering a fast and intuitive interface for\nfocused queries in complex scenes. Specifically, we propose a hierarchical\nevaluation protocol that begins with binary sanity-check questions to assess\nbasic model capacities, and progresses to (1) attribute prediction for\nbox-referred objects, (2) motion understanding of target instances, and (3)\nspatiotemporal motion reasoning over inter-object dynamics across frames. To\nsupport this, we crowd-sourced fine-grained object classes and visual\nattributes that reflect the complexity drivers encounter, and extract object\ntrajectories to construct temporally grounded QA pairs. Rigorous quality\ncontrol through negative sampling, temporal consistency checks, and\ndifficulty-aware balancing guarantee dataset robustness and diversity. Our\ncomprehensive evaluation reveals significant limitations in current VLMs when\nqueried about perception questions, highlighting the gap in achieving\nreal-world performance. This work provides a foundation for developing more\nrobust and interpretable autonomous driving systems that can communicate\neffectively with users under real-world conditions. Project page and dataset\nare available at https://djamahl99.github.io/qaymo-pages/.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00525v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00525v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00298", "title": "Enhancing Interpretability in Generative Modeling: Statistically Disentangled Latent Spaces Guided by Generative Factors in Scientific Datasets", "authors": ["Arkaprabha Ganguli", "Nesar Ramachandra", "Julie Bessac", "Emil Constantinescu"], "summary": "This study addresses the challenge of statistically extracting generative\nfactors from complex, high-dimensional datasets in unsupervised or\nsemi-supervised settings. We investigate encoder-decoder-based generative\nmodels for nonlinear dimensionality reduction, focusing on disentangling\nlow-dimensional latent variables corresponding to independent physical factors.\nIntroducing Aux-VAE, a novel architecture within the classical Variational\nAutoencoder framework, we achieve disentanglement with minimal modifications to\nthe standard VAE loss function by leveraging prior statistical knowledge\nthrough auxiliary variables. These variables guide the shaping of the latent\nspace by aligning latent factors with learned auxiliary variables. We validate\nthe efficacy of Aux-VAE through comparative assessments on multiple datasets,\nincluding astronomical simulations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00298v1", "categories": ["stat.ML", "cs.LG"], "cate": "stat.ML", "url": "http://arxiv.org/abs/2507.00298v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2507.00577", "title": "BadViM: Backdoor Attack against Vision Mamba", "authors": ["Yinghao Wu", "Liyan Zhang"], "summary": "Vision State Space Models (SSMs), particularly architectures like Vision\nMamba (ViM), have emerged as promising alternatives to Vision Transformers\n(ViTs). However, the security implications of this novel architecture,\nespecially their vulnerability to backdoor attacks, remain critically\nunderexplored. Backdoor attacks aim to embed hidden triggers into victim\nmodels, causing the model to misclassify inputs containing these triggers while\nmaintaining normal behavior on clean inputs. This paper investigates the\nsusceptibility of ViM to backdoor attacks by introducing BadViM, a novel\nbackdoor attack framework specifically designed for Vision Mamba. The proposed\nBadViM leverages a Resonant Frequency Trigger (RFT) that exploits the frequency\nsensitivity patterns of the victim model to create stealthy, distributed\ntriggers. To maximize attack efficacy, we propose a Hidden State Alignment loss\nthat strategically manipulates the internal representations of model by\naligning the hidden states of backdoor images with those of target classes.\nExtensive experimental results demonstrate that BadViM achieves superior attack\nsuccess rates while maintaining clean data accuracy. Meanwhile, BadViM exhibits\nremarkable resilience against common defensive measures, including PatchDrop,\nPatchShuffle and JPEG compression, which typically neutralize normal backdoor\nattacks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00577v1", "categories": ["cs.CR", "cs.AI", "cs.CV"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2507.00577v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00535", "title": "Rethinking Group Recommender Systems in the Era of Generative AI: From One-Shot Recommendations to Agentic Group Decision Support", "authors": ["Dietmar Jannach", "Amra Delić", "Francesco Ricci", "Markus Zanker"], "summary": "More than twenty-five years ago, first ideas were developed on how to design\na system that can provide recommendations to groups of users instead of\nindividual users. Since then, a rich variety of algorithmic proposals were\npublished, e.g., on how to acquire individual preferences, how to aggregate\nthem, and how to generate recommendations for groups of users. However, despite\nthe rich literature on the topic, barely any examples of real-world group\nrecommender systems can be found. This lets us question common assumptions in\nacademic research, in particular regarding communication processes in a group\nand how recommendation-supported decisions are made. In this essay, we argue\nthat these common assumptions and corresponding system designs often may not\nmatch the needs or expectations of users. We thus call for a reorientation in\nthis research area, leveraging the capabilities of modern Generative AI\nassistants like ChatGPT. Specifically, as one promising future direction, we\nenvision group recommender systems to be systems where human group members\ninteract in a chat and an AI-based group recommendation agent assists the\ndecision-making process in an agentic way. Ultimately, this shall lead to a\nmore natural group decision-making environment and finally to wider adoption of\ngroup recommendation systems in practice.", "comment": "Submitted for publication", "pdf_url": "http://arxiv.org/pdf/2507.00535v1", "categories": ["cs.IR", "cs.AI"], "cate": "cs.IR", "url": "http://arxiv.org/abs/2507.00535v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00353", "title": "Augmented Physics-Based Li-ion Battery Model via Adaptive Ensemble Sparse Learning and Conformal Prediction", "authors": ["Samuel Filgueira da Silva", "Mehmet Fatih Ozkan", "Faissal El Idrissi", "Marcello Canova"], "summary": "Accurate electrochemical models are essential for the safe and efficient\noperation of lithium-ion batteries in real-world applications such as\nelectrified vehicles and grid storage. Reduced-order models (ROM) offer a\nbalance between fidelity and computational efficiency but often struggle to\ncapture complex and nonlinear behaviors, such as the dynamics in the cell\nvoltage response under high C-rate conditions. To address these limitations,\nthis study proposes an Adaptive Ensemble Sparse Identification (AESI) framework\nthat enhances the accuracy of reduced-order li-ion battery models by\ncompensating for unpredictable dynamics. The approach integrates an Extended\nSingle Particle Model (ESPM) with an evolutionary ensemble sparse learning\nstrategy to construct a robust hybrid model. In addition, the AESI framework\nincorporates a conformal prediction method to provide theoretically guaranteed\nuncertainty quantification for voltage error dynamics, thereby improving the\nreliability of the model's predictions. Evaluation across diverse operating\nconditions shows that the hybrid model (ESPM + AESI) improves the voltage\nprediction accuracy, achieving mean squared error reductions of up to 46% on\nunseen data. Prediction reliability is further supported by conformal\nprediction, yielding statistically valid prediction intervals with coverage\nratios of 96.85% and 97.41% for the ensemble models based on bagging and\nstability selection, respectively.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00353v1", "categories": ["eess.SY", "cs.LG", "cs.SY"], "cate": "eess.SY", "url": "http://arxiv.org/abs/2507.00353v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00582", "title": "Bridging Classical and Learning-based Iterative Registration through Deep Equilibrium Models", "authors": ["Yi Zhang", "Yidong Zhao", "Qian Tao"], "summary": "Deformable medical image registration is traditionally formulated as an\noptimization problem. While classical methods solve this problem iteratively,\nrecent learning-based approaches use recurrent neural networks (RNNs) to mimic\nthis process by unrolling the prediction of deformation fields in a fixed\nnumber of steps. However, classical methods typically converge after sufficient\niterations, but learning-based unrolling methods lack a theoretical convergence\nguarantee and show instability empirically. In addition, unrolling methods have\na practical bottleneck at training time: GPU memory usage grows linearly with\nthe unrolling steps due to backpropagation through time (BPTT). To address both\ntheoretical and practical challenges, we propose DEQReg, a novel registration\nframework based on Deep Equilibrium Models (DEQ), which formulates registration\nas an equilibrium-seeking problem, establishing a natural connection between\nclassical optimization and learning-based unrolling methods. DEQReg maintains\nconstant memory usage, enabling theoretically unlimited iteration steps.\nThrough extensive evaluation on the public brain MRI and lung CT datasets, we\nshow that DEQReg can achieve competitive registration performance, while\nsubstantially reducing memory consumption compared to state-of-the-art\nunrolling methods. We also reveal an intriguing phenomenon: the performance of\nexisting unrolling methods first increases slightly then degrades irreversibly\nwhen the inference steps go beyond the training configuration. In contrast,\nDEQReg achieves stable convergence with its inbuilt equilibrium-seeking\nmechanism, bridging the gap between classical optimization-based and modern\nlearning-based registration methods.", "comment": "Submitted version. Accepted by MICCAI 2025", "pdf_url": "http://arxiv.org/pdf/2507.00582v1", "categories": ["eess.IV", "cs.CV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2507.00582v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00537", "title": "Not All Attention Heads Are What You Need: Refining CLIP's Image Representation with Attention Ablation", "authors": ["Feng Lin", "Marco Chen", "Haokui Zhang", "Xiaotian Yu", "Guangming Lu", "Rong Xiao"], "summary": "This paper studies the role of attention heads in CLIP's image encoder. While\nCLIP has exhibited robust performance across diverse applications, we\nhypothesize that certain attention heads negatively affect final\nrepresentations and that ablating them can improve performance in downstream\ntasks. To capitalize on this insight, we propose a simple yet effective method,\ncalled Attention Ablation Technique (AAT), to suppress the contribution of\nspecific heads by manipulating attention weights. By integrating two\nalternative strategies tailored for different application scenarios, AAT\nsystematically identifies and ablates detrimental attention heads to enhance\nrepresentation quality. Experiments demonstrate that AAT consistently improves\ndownstream task performance across various domains, boosting recall rate by up\nto 11.1% on CLIP-family models for cross-modal retrieval. The results highlight\nthe potential of AAT to effectively refine large-scale vision-language models\nwith virtually no increase in inference cost.", "comment": "21 pages, 7 figures", "pdf_url": "http://arxiv.org/pdf/2507.00537v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00537v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00401", "title": "Few-shot Classification as Multi-instance Verification: Effective Backbone-agnostic Transfer across Domains", "authors": ["Xin Xu", "Eibe Frank", "Geoffrey Holmes"], "summary": "We investigate cross-domain few-shot learning under the constraint that\nfine-tuning of backbones (i.e., feature extractors) is impossible or infeasible\n-- a scenario that is increasingly common in practical use cases. Handling the\nlow-quality and static embeddings produced by frozen, \"black-box\" backbones\nleads to a problem representation of few-shot classification as a series of\nmultiple instance verification (MIV) tasks. Inspired by this representation, we\nintroduce a novel approach to few-shot domain adaptation, named the \"MIV-head\",\nakin to a classification head that is agnostic to any pretrained backbone and\ncomputationally efficient. The core components designed for the MIV-head, when\ntrained on few-shot data from a target domain, collectively yield strong\nperformance on test data from that domain. Importantly, it does so without\nfine-tuning the backbone, and within the \"meta-testing\" phase. Experimenting\nunder various settings and on an extension of the Meta-dataset benchmark for\ncross-domain few-shot image classification, using representative off-the-shelf\nconvolutional neural network and vision transformer backbones pretrained on\nImageNet1K, we show that the MIV-head achieves highly competitive accuracy when\ncompared to state-of-the-art \"adapter\" (or partially fine-tuning) methods\napplied to the same backbones, while incurring substantially lower adaptation\ncost. We also find well-known \"classification head\" approaches lag far behind\nin terms of accuracy. Ablation study empirically justifies the core components\nof our approach. We share our code at https://github.com/xxweka/MIV-head.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00401v1", "categories": ["cs.CV", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00401v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00635", "title": "Stable Tracking of Eye Gaze Direction During Ophthalmic Surgery", "authors": ["Tinghe Hong", "Shenlin Cai", "Boyang Li", "Kai Huang"], "summary": "Ophthalmic surgical robots offer superior stability and precision by reducing\nthe natural hand tremors of human surgeons, enabling delicate operations in\nconfined surgical spaces. Despite the advancements in developing vision- and\nforce-based control methods for surgical robots, preoperative navigation\nremains heavily reliant on manual operation, limiting the consistency and\nincreasing the uncertainty. Existing eye gaze estimation techniques in the\nsurgery, whether traditional or deep learning-based, face challenges including\ndependence on additional sensors, occlusion issues in surgical environments,\nand the requirement for facial detection. To address these limitations, this\nstudy proposes an innovative eye localization and tracking method that combines\nmachine learning with traditional algorithms, eliminating the requirements of\nlandmarks and maintaining stable iris detection and gaze estimation under\nvarying lighting and shadow conditions. Extensive real-world experiment results\nshow that our proposed method has an average estimation error of 0.58 degrees\nfor eye orientation estimation and 2.08-degree average control error for the\nrobotic arm's movement based on the calculated orientation.", "comment": "Accepted by ICRA 2025", "pdf_url": "http://arxiv.org/pdf/2507.00635v1", "categories": ["cs.RO", "cs.CV", "cs.HC"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2507.00635v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00546", "title": "Inverse Design in Nanophotonics via Representation Learning", "authors": ["Reza Marzban", "Ali Adibi", "Raphael Pestourie"], "summary": "Inverse design in nanophotonics, the computational discovery of structures\nachieving targeted electromagnetic (EM) responses, has become a key tool for\nrecent optical advances. Traditional intuition-driven or iterative optimization\nmethods struggle with the inherently high-dimensional, non-convex design spaces\nand the substantial computational demands of EM simulations. Recently, machine\nlearning (ML) has emerged to address these bottlenecks effectively. This review\nframes ML-enhanced inverse design methodologies through the lens of\nrepresentation learning, classifying them into two categories: output-side and\ninput-side approaches. Output-side methods use ML to learn a representation in\nthe solution space to create a differentiable solver that accelerates\noptimization. Conversely, input-side techniques employ ML to learn compact,\nlatent-space representations of feasible device geometries, enabling efficient\nglobal exploration through generative models. Each strategy presents unique\ntrade-offs in data requirements, generalization capacity, and novel design\ndiscovery potentials. Hybrid frameworks that combine physics-based optimization\nwith data-driven representations help escape poor local optima, improve\nscalability, and facilitate knowledge transfer. We conclude by highlighting\nopen challenges and opportunities, emphasizing complexity management,\ngeometry-independent representations, integration of fabrication constraints,\nand advancements in multiphysics co-designs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00546v1", "categories": ["physics.app-ph", "cs.AI", "cs.LG", "physics.optics"], "cate": "physics.app-ph", "url": "http://arxiv.org/abs/2507.00546v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00402", "title": "GRAND: Graph Release with Assured Node Differential Privacy", "authors": ["Suqing Liu", "Xuan Bi", "Tianxi Li"], "summary": "Differential privacy is a well-established framework for safeguarding\nsensitive information in data. While extensively applied across various\ndomains, its application to network data -- particularly at the node level --\nremains underexplored. Existing methods for node-level privacy either focus\nexclusively on query-based approaches, which restrict output to pre-specified\nnetwork statistics, or fail to preserve key structural properties of the\nnetwork. In this work, we propose GRAND (Graph Release with Assured Node\nDifferential privacy), which is, to the best of our knowledge, the first\nnetwork release mechanism that releases entire networks while ensuring\nnode-level differential privacy and preserving structural properties. Under a\nbroad class of latent space models, we show that the released network\nasymptotically follows the same distribution as the original network. The\neffectiveness of the approach is evaluated through extensive experiments on\nboth synthetic and real-world datasets.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00402v1", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.ME", "stat.TH"], "cate": "stat.ML", "url": "http://arxiv.org/abs/2507.00402v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00651", "title": "GANs Secretly Perform Approximate Bayesian Model Selection", "authors": ["Maurizio Filippone", "Marius P. Linhard"], "summary": "Generative Adversarial Networks (GANs) are popular and successful generative\nmodels. Despite their success, optimization is notoriously challenging and they\nrequire regularization against overfitting. In this work, we explain the\nsuccess and limitations of GANs by interpreting them as probabilistic\ngenerative models. This interpretation enables us to view GANs as Bayesian\nneural networks with partial stochasticity, allowing us to establish conditions\nof universal approximation. We can then cast the adversarial-style optimization\nof several variants of GANs as the optimization of a proxy for the marginal\nlikelihood. Taking advantage of the connection between marginal likelihood\noptimization and Occam's razor, we can define regularization and optimization\nstrategies to smooth the loss landscape and search for solutions with minimum\ndescription length, which are associated with flat minima and good\ngeneralization. The results on a wide range of experiments indicate that these\nstrategies lead to performance improvements and pave the way to a deeper\nunderstanding of regularization strategies for GANs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00651v1", "categories": ["cs.LG", "cs.CV", "stat.ML"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00651v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00577", "title": "BadViM: Backdoor Attack against Vision Mamba", "authors": ["Yinghao Wu", "Liyan Zhang"], "summary": "Vision State Space Models (SSMs), particularly architectures like Vision\nMamba (ViM), have emerged as promising alternatives to Vision Transformers\n(ViTs). However, the security implications of this novel architecture,\nespecially their vulnerability to backdoor attacks, remain critically\nunderexplored. Backdoor attacks aim to embed hidden triggers into victim\nmodels, causing the model to misclassify inputs containing these triggers while\nmaintaining normal behavior on clean inputs. This paper investigates the\nsusceptibility of ViM to backdoor attacks by introducing BadViM, a novel\nbackdoor attack framework specifically designed for Vision Mamba. The proposed\nBadViM leverages a Resonant Frequency Trigger (RFT) that exploits the frequency\nsensitivity patterns of the victim model to create stealthy, distributed\ntriggers. To maximize attack efficacy, we propose a Hidden State Alignment loss\nthat strategically manipulates the internal representations of model by\naligning the hidden states of backdoor images with those of target classes.\nExtensive experimental results demonstrate that BadViM achieves superior attack\nsuccess rates while maintaining clean data accuracy. Meanwhile, BadViM exhibits\nremarkable resilience against common defensive measures, including PatchDrop,\nPatchShuffle and JPEG compression, which typically neutralize normal backdoor\nattacks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00577v1", "categories": ["cs.CR", "cs.AI", "cs.CV"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2507.00577v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00423", "title": "Find a Scapegoat: Poisoning Membership Inference Attack and Defense to Federated Learning", "authors": ["Wenjin Mo", "Zhiyuan Li", "Minghong Fang", "Mingwei Fang"], "summary": "Federated learning (FL) allows multiple clients to collaboratively train a\nglobal machine learning model with coordination from a central server, without\nneeding to share their raw data. This approach is particularly appealing in the\nera of privacy regulations like the GDPR, leading many prominent companies to\nadopt it. However, FL's distributed nature makes it susceptible to poisoning\nattacks, where malicious clients, controlled by an attacker, send harmful data\nto compromise the model. Most existing poisoning attacks in FL aim to degrade\nthe model's integrity, such as reducing its accuracy, with limited attention to\nprivacy concerns from these attacks. In this study, we introduce FedPoisonMIA,\na novel poisoning membership inference attack targeting FL. FedPoisonMIA\ninvolves malicious clients crafting local model updates to infer membership\ninformation. Additionally, we propose a robust defense mechanism to mitigate\nthe impact of FedPoisonMIA attacks. Extensive experiments across various\ndatasets demonstrate the attack's effectiveness, while our defense approach\nreduces its impact to a degree.", "comment": "To appear in ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.00423v1", "categories": ["cs.CR", "cs.DC", "cs.LG"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2507.00423v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00660", "title": "MTCNet: Motion and Topology Consistency Guided Learning for Mitral Valve Segmentationin 4D Ultrasound", "authors": ["Rusi Chen", "Yuanting Yang", "Jiezhi Yao", "Hongning Song", "Ji Zhang", "Yongsong Zhou", "Yuhao Huang", "Ronghao Yang", "Dan Jia", "Yuhan Zhang", "Xing Tao", "Haoran Dou", "Qing Zhou", "Xin Yang", "Dong Ni"], "summary": "Mitral regurgitation is one of the most prevalent cardiac disorders.\nFour-dimensional (4D) ultrasound has emerged as the primary imaging modality\nfor assessing dynamic valvular morphology. However, 4D mitral valve (MV)\nanalysis remains challenging due to limited phase annotations, severe motion\nartifacts, and poor imaging quality. Yet, the absence of inter-phase dependency\nin existing methods hinders 4D MV analysis. To bridge this gap, we propose a\nMotion-Topology guided consistency network (MTCNet) for accurate 4D MV\nultrasound segmentation in semi-supervised learning (SSL). MTCNet requires only\nsparse end-diastolic and end-systolic annotations. First, we design a\ncross-phase motion-guided consistency learning strategy, utilizing a\nbi-directional attention memory bank to propagate spatio-temporal features.\nThis enables MTCNet to achieve excellent performance both per- and inter-phase.\nSecond, we devise a novel topology-guided correlation regularization that\nexplores physical prior knowledge to maintain anatomically plausible.\nTherefore, MTCNet can effectively leverage structural correspondence between\nlabeled and unlabeled phases. Extensive evaluations on the first largest 4D MV\ndataset, with 1408 phases from 160 patients, show that MTCNet performs superior\ncross-phase consistency compared to other advanced methods (Dice: 87.30%, HD:\n1.75mm). Both the code and the dataset are available at\nhttps://github.com/crs524/MTCNet.", "comment": "Accepted by MICCAI 2025", "pdf_url": "http://arxiv.org/pdf/2507.00660v1", "categories": ["eess.IV", "cs.AI", "cs.CV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2507.00660v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00579", "title": "TUM-MiKaNi at SemEval-2025 Task 3: Towards Multilingual and Knowledge-Aware Non-factual Hallucination Identification", "authors": ["Miriam Anschütz", "Ekaterina Gikalo", "Niklas Herbster", "Georg Groh"], "summary": "Hallucinations are one of the major problems of LLMs, hindering their\ntrustworthiness and deployment to wider use cases. However, most of the\nresearch on hallucinations focuses on English data, neglecting the multilingual\nnature of LLMs. This paper describes our submission to the SemEval-2025 Task-3\n- Mu-SHROOM, the Multilingual Shared-task on Hallucinations and Related\nObservable Overgeneration Mistakes. We propose a two-part pipeline that\ncombines retrieval-based fact verification against Wikipedia with a BERT-based\nsystem fine-tuned to identify common hallucination patterns. Our system\nachieves competitive results across all languages, reaching top-10 results in\neight languages, including English. Moreover, it supports multiple languages\nbeyond the fourteen covered by the shared task. This multilingual hallucination\nidentifier can help to improve LLM outputs and their usefulness in the future.", "comment": "6 pages, 3 figures, SemEval-2025 Task 3, ACL", "pdf_url": "http://arxiv.org/pdf/2507.00579v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2507.00579v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00469", "title": "Bisecle: Binding and Separation in Continual Learning for Video Language Understanding", "authors": ["Yue Tan", "Xiaoqian Hu", "Hao Xue", "Celso De Melo", "Flora D. Salim"], "summary": "Frontier vision-language models (VLMs) have made remarkable improvements in\nvideo understanding tasks. However, real-world videos typically exist as\ncontinuously evolving data streams (e.g., dynamic scenes captured by wearable\nglasses), necessitating models to continually adapt to shifting data\ndistributions and novel scenarios. Considering the prohibitive computational\ncosts of fine-tuning models on new tasks, usually, a small subset of parameters\nis updated while the bulk of the model remains frozen. This poses new\nchallenges to existing continual learning frameworks in the context of large\nmultimodal foundation models, i.e., catastrophic forgetting and update\nconflict. While the foundation models struggle with parameter-efficient\ncontinual learning, the hippocampus in the human brain has evolved highly\nefficient mechanisms for memory formation and consolidation. Inspired by the\nrapid Binding and pattern separation mechanisms in the hippocampus, in this\nwork, we propose Bisecle for video-language continual learning, where a\nmulti-directional supervision module is used to capture more cross-modal\nrelationships and a contrastive prompt learning scheme is designed to isolate\ntask-specific knowledge to facilitate efficient memory storage. Binding and\nseparation processes further strengthen the ability of VLMs to retain complex\nexperiences, enabling robust and efficient continual learning in video\nunderstanding tasks. We perform a thorough evaluation of the proposed Bisecle,\ndemonstrating its ability to mitigate forgetting and enhance cross-task\ngeneralization on several VideoQA benchmarks.", "comment": "23 pages, 12 figures, 10 tables", "pdf_url": "http://arxiv.org/pdf/2507.00469v1", "categories": ["cs.CV", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00469v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00669", "title": "Audio-3DVG: Unified Audio - Point Cloud Fusion for 3D Visual Grounding", "authors": ["Duc Cao-Dinh", "Khai Le-Duc", "Anh Dao", "Bach Phan Tat", "Chris Ngo", "Duy M. H. Nguyen", "Nguyen X. Khanh", "Thanh Nguyen-Tang"], "summary": "3D Visual Grounding (3DVG) involves localizing target objects in 3D point\nclouds based on natural language. While prior work has made strides using\ntextual descriptions, leveraging spoken language-known as Audio-based 3D Visual\nGrounding-remains underexplored and challenging. Motivated by advances in\nautomatic speech recognition (ASR) and speech representation learning, we\npropose Audio-3DVG, a simple yet effective framework that integrates audio and\nspatial information for enhanced grounding. Rather than treating speech as a\nmonolithic input, we decompose the task into two complementary components.\nFirst, we introduce Object Mention Detection, a multi-label classification task\nthat explicitly identifies which objects are referred to in the audio, enabling\nmore structured audio-scene reasoning. Second, we propose an Audio-Guided\nAttention module that captures interactions between candidate objects and\nrelational speech cues, improving target discrimination in cluttered scenes. To\nsupport benchmarking, we synthesize audio descriptions for standard 3DVG\ndatasets, including ScanRefer, Sr3D, and Nr3D. Experimental results demonstrate\nthat Audio-3DVG not only achieves new state-of-the-art performance in\naudio-based grounding, but also competes with text-based methods-highlighting\nthe promise of integrating spoken language into 3D vision tasks.", "comment": "Work in progress, 42 pages", "pdf_url": "http://arxiv.org/pdf/2507.00669v1", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.RO"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00669v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00583", "title": "AI-Generated Video Detection via Perceptual Straightening", "authors": ["Christian Internò", "Robert Geirhos", "Markus Olhofer", "Sunny Liu", "Barbara Hammer", "David Klindt"], "summary": "The rapid advancement of generative AI enables highly realistic synthetic\nvideos, posing significant challenges for content authentication and raising\nurgent concerns about misuse. Existing detection methods often struggle with\ngeneralization and capturing subtle temporal inconsistencies. We propose\nReStraV(Representation Straightening Video), a novel approach to distinguish\nnatural from AI-generated videos. Inspired by the \"perceptual straightening\"\nhypothesis -- which suggests real-world video trajectories become more straight\nin neural representation domain -- we analyze deviations from this expected\ngeometric property. Using a pre-trained self-supervised vision transformer\n(DINOv2), we quantify the temporal curvature and stepwise distance in the\nmodel's representation domain. We aggregate statistics of these measures for\neach video and train a classifier. Our analysis shows that AI-generated videos\nexhibit significantly different curvature and distance patterns compared to\nreal videos. A lightweight classifier achieves state-of-the-art detection\nperformance (e.g., 97.17% accuracy and 98.63% AUROC on the VidProM benchmark),\nsubstantially outperforming existing image- and video-based methods. ReStraV is\ncomputationally efficient, it is offering a low-cost and effective detection\nsolution. This work provides new insights into using neural representation\ngeometry for AI-generated video detection.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00583v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00583v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00482", "title": "Physics-Aware Style Transfer for Adaptive Holographic Reconstruction", "authors": ["Chanseok Lee", "Fakhriyya Mammadova", "Jiseong Barg", "Mooseok Jang"], "summary": "Inline holographic imaging presents an ill-posed inverse problem of\nreconstructing objects' complex amplitude from recorded diffraction patterns.\nAlthough recent deep learning approaches have shown promise over classical\nphase retrieval algorithms, they often require high-quality ground truth\ndatasets of complex amplitude maps to achieve a statistical inverse mapping\noperation between the two domains. Here, we present a physics-aware style\ntransfer approach that interprets the object-to-sensor distance as an implicit\nstyle within diffraction patterns. Using the style domain as the intermediate\ndomain to construct cyclic image translation, we show that the inverse mapping\noperation can be learned in an adaptive manner only with datasets composed of\nintensity measurements. We further demonstrate its biomedical applicability by\nreconstructing the morphology of dynamically flowing red blood cells,\nhighlighting its potential for real-time, label-free imaging. As a framework\nthat leverages physical cues inherently embedded in measurements, the presented\nmethod offers a practical learning strategy for imaging applications where\nground truth is difficult or impossible to obtain.", "comment": "Keywords: holographic imaging, style transfer, phase retrieval, deep\n  learning", "pdf_url": "http://arxiv.org/pdf/2507.00482v1", "categories": ["physics.optics", "cs.AI", "cs.LG"], "cate": "physics.optics", "url": "http://arxiv.org/abs/2507.00482v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00670", "title": "Mind the Detail: Uncovering Clinically Relevant Image Details in Accelerated MRI with Semantically Diverse Reconstructions", "authors": ["Jan Nikolas Morshuis", "Christian Schlarmann", "Thomas Küstner", "Christian F. Baumgartner", "Matthias Hein"], "summary": "In recent years, accelerated MRI reconstruction based on deep learning has\nled to significant improvements in image quality with impressive results for\nhigh acceleration factors. However, from a clinical perspective image quality\nis only secondary; much more important is that all clinically relevant\ninformation is preserved in the reconstruction from heavily undersampled data.\nIn this paper, we show that existing techniques, even when considering\nresampling for diffusion-based reconstruction, can fail to reconstruct small\nand rare pathologies, thus leading to potentially wrong diagnosis decisions\n(false negatives). To uncover the potentially missing clinical information we\npropose ``Semantically Diverse Reconstructions'' (\\SDR), a method which, given\nan original reconstruction, generates novel reconstructions with enhanced\nsemantic variability while all of them are fully consistent with the measured\ndata. To evaluate \\SDR automatically we train an object detector on the\nfastMRI+ dataset. We show that \\SDR significantly reduces the chance of\nfalse-negative diagnoses (higher recall) and improves mean average precision\ncompared to the original reconstructions. The code is available on\nhttps://github.com/NikolasMorshuis/SDR", "comment": "MICCAI 2025", "pdf_url": "http://arxiv.org/pdf/2507.00670v1", "categories": ["eess.IV", "cs.CV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2507.00670v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00589", "title": "Quantum Circuit Structure Optimization for Quantum Reinforcement Learning", "authors": ["Seok Bin Son", "Joongheon Kim"], "summary": "Reinforcement learning (RL) enables agents to learn optimal policies through\nenvironmental interaction. However, RL suffers from reduced learning efficiency\ndue to the curse of dimensionality in high-dimensional spaces. Quantum\nreinforcement learning (QRL) addresses this issue by leveraging superposition\nand entanglement in quantum computing, allowing efficient handling of\nhigh-dimensional problems with fewer resources. QRL combines quantum neural\nnetworks (QNNs) with RL, where the parameterized quantum circuit (PQC) acts as\nthe core computational module. The PQC performs linear and nonlinear\ntransformations through gate operations, similar to hidden layers in classical\nneural networks. Previous QRL studies, however, have used fixed PQC structures\nbased on empirical intuition without verifying their optimality. This paper\nproposes a QRL-NAS algorithm that integrates quantum neural architecture search\n(QNAS) to optimize PQC structures within QRL. Experiments demonstrate that\nQRL-NAS achieves higher rewards than QRL with fixed circuits, validating its\neffectiveness and practical utility.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00589v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00589v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00498", "title": "MuteSwap: Silent Face-based Voice Conversion", "authors": ["Yifan Liu", "Yu Fang", "Zhouhan Lin"], "summary": "Conventional voice conversion modifies voice characteristics from a source\nspeaker to a target speaker, relying on audio input from both sides. However,\nthis process becomes infeasible when clean audio is unavailable, such as in\nsilent videos or noisy environments. In this work, we focus on the task of\nSilent Face-based Voice Conversion (SFVC), which does voice conversion entirely\nfrom visual inputs. i.e., given images of a target speaker and a silent video\nof a source speaker containing lip motion, SFVC generates speech aligning the\nidentity of the target speaker while preserving the speech content in the\nsource silent video. As this task requires generating intelligible speech and\nconverting identity using only visual cues, it is particularly challenging. To\naddress this, we introduce MuteSwap, a novel framework that employs contrastive\nlearning to align cross-modality identities and minimize mutual information to\nseparate shared visual features. Experimental results show that MuteSwap\nachieves impressive performance in both speech synthesis and identity\nconversion, especially under noisy conditions where methods dependent on audio\ninput fail to produce intelligible results, demonstrating both the\neffectiveness of our training approach and the feasibility of SFVC.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00498v1", "categories": ["cs.SD", "cs.CV", "cs.LG", "cs.MM", "eess.AS"], "cate": "cs.SD", "url": "http://arxiv.org/abs/2507.00498v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00673", "title": "Prompt2SegCXR:Prompt to Segment All Organs and Diseases in Chest X-rays", "authors": ["Abduz Zami", "Shadman Sobhan", "Rounaq Hossain", "Md. Sawran Sorker", "Mohiuddin Ahmed", "Md. Redwan Hossain"], "summary": "Image segmentation plays a vital role in the medical field by isolating\norgans or regions of interest from surrounding areas. Traditionally,\nsegmentation models are trained on a specific organ or a disease, limiting\ntheir ability to handle other organs and diseases. At present, few advanced\nmodels can perform multi-organ or multi-disease segmentation, offering greater\nflexibility. Also, recently, prompt-based image segmentation has gained\nattention as a more flexible approach. It allows models to segment areas based\non user-provided prompts. Despite these advances, there has been no dedicated\nwork on prompt-based interactive multi-organ and multi-disease segmentation,\nespecially for Chest X-rays. This work presents two main contributions: first,\ngenerating doodle prompts by medical experts of a collection of datasets from\nmultiple sources with 23 classes, including 6 organs and 17 diseases,\nspecifically designed for prompt-based Chest X-ray segmentation. Second, we\nintroduce Prompt2SegCXR, a lightweight model for accurately segmenting multiple\norgans and diseases from Chest X-rays. The model incorporates multi-stage\nfeature fusion, enabling it to combine features from various network layers for\nbetter spatial and semantic understanding, enhancing segmentation accuracy.\nCompared to existing pre-trained models for prompt-based image segmentation,\nour model scores well, providing a reliable solution for segmenting Chest\nX-rays based on user prompts.", "comment": "29 Pages", "pdf_url": "http://arxiv.org/pdf/2507.00673v1", "categories": ["eess.IV", "cs.CV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2507.00673v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00598", "title": "High-resolution spatial memory requires grid-cell-like neural codes", "authors": ["Madison Cotteret", "Christopher J. Kymn", "Hugh Greatorex", "Martin Ziegler", "Elisabetta Chicca", "Friedrich T. Sommer"], "summary": "Continuous attractor networks (CANs) are widely used to model how the brain\ntemporarily retains continuous behavioural variables via persistent recurrent\nactivity, such as an animal's position in an environment. However, this memory\nmechanism is very sensitive to even small imperfections, such as noise or\nheterogeneity, which are both common in biological systems. Previous work has\nshown that discretising the continuum into a finite set of discrete attractor\nstates provides robustness to these imperfections, but necessarily reduces the\nresolution of the represented variable, creating a dilemma between stability\nand resolution. We show that this stability-resolution dilemma is most severe\nfor CANs using unimodal bump-like codes, as in traditional models. To overcome\nthis, we investigate sparse binary distributed codes based on random feature\nembeddings, in which neurons have spatially-periodic receptive fields. We\ndemonstrate theoretically and with simulations that such grid-cell-like codes\nenable CANs to achieve both high stability and high resolution simultaneously.\nThe model extends to embedding arbitrary nonlinear manifolds into a CAN, such\nas spheres or tori, and generalises linear path integration to integration\nalong freely-programmable on-manifold vector fields. Together, this work\nprovides a theory of how the brain could robustly represent continuous\nvariables with high resolution and perform flexible computations over\ntask-relevant manifolds.", "comment": "14 pages, 4 figures. Supplementary material: 11 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2507.00598v1", "categories": ["cs.NE", "cs.AI", "cs.SC"], "cate": "cs.NE", "url": "http://arxiv.org/abs/2507.00598v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00511", "title": "Medical Image Segmentation Using Advanced Unet: VMSE-Unet and VM-Unet CBAM+", "authors": ["Sayandeep Kanrar", "Raja Piyush", "Qaiser Razi", "Debanshi Chakraborty", "Vikas Hassija", "GSS Chalapathi"], "summary": "In this paper, we present the VMSE U-Net and VM-Unet CBAM+ model, two\ncutting-edge deep learning architectures designed to enhance medical image\nsegmentation. Our approach integrates Squeeze-and-Excitation (SE) and\nConvolutional Block Attention Module (CBAM) techniques into the traditional VM\nU-Net framework, significantly improving segmentation accuracy, feature\nlocalization, and computational efficiency. Both models show superior\nperformance compared to the baseline VM-Unet across multiple datasets. Notably,\nVMSEUnet achieves the highest accuracy, IoU, precision, and recall while\nmaintaining low loss values. It also exhibits exceptional computational\nefficiency with faster inference times and lower memory usage on both GPU and\nCPU. Overall, the study suggests that the enhanced architecture VMSE-Unet is a\nvaluable tool for medical image analysis. These findings highlight its\npotential for real-world clinical applications, emphasizing the importance of\nfurther research to optimize accuracy, robustness, and computational\nefficiency.", "comment": "under review", "pdf_url": "http://arxiv.org/pdf/2507.00511v1", "categories": ["eess.IV", "cs.CV", "cs.LG"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2507.00511v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00687", "title": "Diffusion Classifier Guidance for Non-robust Classifiers", "authors": ["Philipp Vaeth", "Dibyanshu Kumar", "Benjamin Paassen", "Magda Gregorová"], "summary": "Classifier guidance is intended to steer a diffusion process such that a\ngiven classifier reliably recognizes the generated data point as a certain\nclass. However, most classifier guidance approaches are restricted to robust\nclassifiers, which were specifically trained on the noise of the diffusion\nforward process. We extend classifier guidance to work with general,\nnon-robust, classifiers that were trained without noise. We analyze the\nsensitivity of both non-robust and robust classifiers to noise of the diffusion\nprocess on the standard CelebA data set, the specialized SportBalls data set\nand the high-dimensional real-world CelebA-HQ data set. Our findings reveal\nthat non-robust classifiers exhibit significant accuracy degradation under\nnoisy conditions, leading to unstable guidance gradients. To mitigate these\nissues, we propose a method that utilizes one-step denoised image predictions\nand implements stabilization techniques inspired by stochastic optimization\nmethods, such as exponential moving averages. Experimental results demonstrate\nthat our approach improves the stability of classifier guidance while\nmaintaining sample diversity and visual quality. This work contributes to\nadvancing conditional sampling techniques in generative models, enabling a\nbroader range of classifiers to be used as guidance classifiers.", "comment": "Accepted at ECML 2025", "pdf_url": "http://arxiv.org/pdf/2507.00687v1", "categories": ["cs.LG", "cs.CV"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00687v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00606", "title": "Mixture of Reasonings: Teach Large Language Models to Reason with Adaptive Strategies", "authors": ["Tao Xiong", "Xavier Hu", "Wenyan Fan", "Shengyu Zhang"], "summary": "Large language models (LLMs) excel in complex tasks through advanced\nprompting techniques like Chain-of-Thought (CoT) and Tree-of-Thought (ToT), but\ntheir reliance on manually crafted, task-specific prompts limits adaptability\nand efficiency. We introduce Mixture of Reasoning (MoR), a training framework\nthat embeds diverse reasoning strategies into LLMs for autonomous,\ntask-adaptive reasoning without external prompt engineering. MoR has two\nphases: Thought Generation, creating reasoning chain templates with models like\nGPT-4o, and SFT Dataset Construction, pairing templates with benchmark datasets\nfor supervised fine-tuning.Our experiments show that MoR significantly enhances\nperformance, with MoR150 achieving 0.730 (2.2% improvement) using CoT prompting\nand 0.734 (13.5% improvement) compared to baselines. MoR eliminates the need\nfor task-specific prompts, offering a generalizable solution for robust\nreasoning across diverse tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00606v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2507.00606v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00514", "title": "Simulation-Efficient Cosmological Inference with Multi-Fidelity SBI", "authors": ["Leander Thiele", "Adrian E. Bayer", "Naoya Takeishi"], "summary": "The simulation cost for cosmological simulation-based inference can be\ndecreased by combining simulation sets of varying fidelity. We propose an\napproach to such multi-fidelity inference based on feature matching and\nknowledge distillation. Our method results in improved posterior quality,\nparticularly for small simulation budgets and difficult inference problems.", "comment": "5 pages, 4 figures; accepted at ICML-colocated ML4Astro 2025 workshop", "pdf_url": "http://arxiv.org/pdf/2507.00514v1", "categories": ["astro-ph.CO", "cs.LG"], "cate": "astro-ph.CO", "url": "http://arxiv.org/abs/2507.00514v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00743", "title": "Tunable Wavelet Unit based Convolutional Neural Network in Optical Coherence Tomography Analysis Enhancement for Classifying Type of Epiretinal Membrane Surgery", "authors": ["An Le", "Nehal Mehta", "William Freeman", "Ines Nagel", "Melanie Tran", "Anna Heinke", "Akshay Agnihotri", "Lingyun Cheng", "Dirk-Uwe Bartsch", "Hung Nguyen", "Truong Nguyen", "Cheolhong An"], "summary": "In this study, we developed deep learning-based method to classify the type\nof surgery performed for epiretinal membrane (ERM) removal, either internal\nlimiting membrane (ILM) removal or ERM-alone removal. Our model, based on the\nResNet18 convolutional neural network (CNN) architecture, utilizes\npostoperative optical coherence tomography (OCT) center scans as inputs. We\nevaluated the model using both original scans and scans preprocessed with\nenergy crop and wavelet denoising, achieving 72% accuracy on preprocessed\ninputs, outperforming the 66% accuracy achieved on original scans. To further\nimprove accuracy, we integrated tunable wavelet units with two key adaptations:\nOrthogonal Lattice-based Wavelet Units (OrthLatt-UwU) and Perfect\nReconstruction Relaxation-based Wavelet Units (PR-Relax-UwU). These units\nallowed the model to automatically adjust filter coefficients during training\nand were incorporated into downsampling, stride-two convolution, and pooling\nlayers, enhancing its ability to distinguish between ERM-ILM removal and\nERM-alone removal, with OrthLattUwU boosting accuracy to 76% and PR-Relax-UwU\nincreasing performance to 78%. Performance comparisons showed that our AI model\noutperformed a trained human grader, who achieved only 50% accuracy in\nclassifying the removal surgery types from postoperative OCT scans. These\nfindings highlight the potential of CNN based models to improve clinical\ndecision-making by providing more accurate and reliable classifications. To the\nbest of our knowledge, this is the first work to employ tunable wavelets for\nclassifying different types of ERM removal surgery.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00743v1", "categories": ["eess.IV", "cs.CV", "eess.SP"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2507.00743v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00611", "title": "Residual Reward Models for Preference-based Reinforcement Learning", "authors": ["Chenyang Cao", "Miguel Rogel-García", "Mohamed Nabail", "Xueqian Wang", "Nicholas Rhinehart"], "summary": "Preference-based Reinforcement Learning (PbRL) provides a way to learn\nhigh-performance policies in environments where the reward signal is hard to\nspecify, avoiding heuristic and time-consuming reward design. However, PbRL can\nsuffer from slow convergence speed since it requires training in a reward\nmodel. Prior work has proposed learning a reward model from demonstrations and\nfine-tuning it using preferences. However, when the model is a neural network,\nusing different loss functions for pre-training and fine-tuning can pose\nchallenges to reliable optimization. In this paper, we propose a method to\neffectively leverage prior knowledge with a Residual Reward Model (RRM). An RRM\nassumes that the true reward of the environment can be split into a sum of two\nparts: a prior reward and a learned reward. The prior reward is a term\navailable before training, for example, a user's ``best guess'' reward\nfunction, or a reward function learned from inverse reinforcement learning\n(IRL), and the learned reward is trained with preferences. We introduce\nstate-based and image-based versions of RRM and evaluate them on several tasks\nin the Meta-World environment suite. Experimental results show that our method\nsubstantially improves the performance of a common PbRL method. Our method\nachieves performance improvements for a variety of different types of prior\nrewards, including proxy rewards, a reward obtained from IRL, and even a\nnegated version of the proxy reward. We also conduct experiments with a Franka\nPanda to show that our method leads to superior performance on a real robot. It\nsignificantly accelerates policy learning for different tasks, achieving\nsuccess in fewer steps than the baseline. The videos are presented at\nhttps://sunlighted.github.io/RRM-web/.", "comment": "26 pages, 22 figures", "pdf_url": "http://arxiv.org/pdf/2507.00611v1", "categories": ["cs.LG", "cs.AI", "cs.RO"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00611v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00537", "title": "Not All Attention Heads Are What You Need: Refining CLIP's Image Representation with Attention Ablation", "authors": ["Feng Lin", "Marco Chen", "Haokui Zhang", "Xiaotian Yu", "Guangming Lu", "Rong Xiao"], "summary": "This paper studies the role of attention heads in CLIP's image encoder. While\nCLIP has exhibited robust performance across diverse applications, we\nhypothesize that certain attention heads negatively affect final\nrepresentations and that ablating them can improve performance in downstream\ntasks. To capitalize on this insight, we propose a simple yet effective method,\ncalled Attention Ablation Technique (AAT), to suppress the contribution of\nspecific heads by manipulating attention weights. By integrating two\nalternative strategies tailored for different application scenarios, AAT\nsystematically identifies and ablates detrimental attention heads to enhance\nrepresentation quality. Experiments demonstrate that AAT consistently improves\ndownstream task performance across various domains, boosting recall rate by up\nto 11.1% on CLIP-family models for cross-modal retrieval. The results highlight\nthe potential of AAT to effectively refine large-scale vision-language models\nwith virtually no increase in inference cost.", "comment": "21 pages, 7 figures", "pdf_url": "http://arxiv.org/pdf/2507.00537v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00537v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00780", "title": "Research on Improving the High Precision and Lightweight Diabetic Retinopathy Detection of YOLOv8n", "authors": ["Fei Yuhuan", "Sun Xufei", "Zang Ran", "Wang Gengchen", "Su Meng", "Liu Fenghao"], "summary": "Early detection and diagnosis of diabetic retinopathy is one of the current\nresearch focuses in ophthalmology. However, due to the subtle features of\nmicro-lesions and their susceptibility to background interference, ex-isting\ndetection methods still face many challenges in terms of accuracy and\nrobustness. To address these issues, a lightweight and high-precision detection\nmodel based on the improved YOLOv8n, named YOLO-KFG, is proposed. Firstly, a\nnew dynamic convolution KWConv and C2f-KW module are designed to improve the\nbackbone network, enhancing the model's ability to perceive micro-lesions.\nSecondly, a fea-ture-focused diffusion pyramid network FDPN is designed to\nfully integrate multi-scale context information, further improving the model's\nability to perceive micro-lesions. Finally, a lightweight shared detection head\nGSDHead is designed to reduce the model's parameter count, making it more\ndeployable on re-source-constrained devices. Experimental results show that\ncompared with the base model YOLOv8n, the improved model reduces the parameter\ncount by 20.7%, increases mAP@0.5 by 4.1%, and improves the recall rate by\n7.9%. Compared with single-stage mainstream algorithms such as YOLOv5n and\nYOLOv10n, YOLO-KFG demonstrates significant advantages in both detection\naccuracy and efficiency.", "comment": "in Chinese language", "pdf_url": "http://arxiv.org/pdf/2507.00780v1", "categories": ["eess.IV", "cs.CV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2507.00780v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00613", "title": "Physics-Informed Neural ODEs for Temporal Dynamics Modeling in Cardiac T1 Mapping", "authors": ["Nuno Capitão", "Yi Zhang", "Yidong Zhao", "Qian Tao"], "summary": "Spin-lattice relaxation time ($T_1$) is an important biomarker in cardiac\nparametric mapping for characterizing myocardial tissue and diagnosing\ncardiomyopathies. Conventional Modified Look-Locker Inversion Recovery (MOLLI)\nacquires 11 breath-hold baseline images with interleaved rest periods to ensure\nmapping accuracy. However, prolonged scanning can be challenging for patients\nwith poor breathholds, often leading to motion artifacts that degrade image\nquality. In addition, $T_1$ mapping requires voxel-wise nonlinear fitting to a\nsignal recovery model involving an iterative estimation process. Recent studies\nhave proposed deep-learning approaches for rapid $T_1$ mapping using shortened\nsequences to reduce acquisition time for patient comfort. Nevertheless,\nexisting methods overlook important physics constraints, limiting\ninterpretability and generalization. In this work, we present an accelerated,\nend-to-end $T_1$ mapping framework leveraging Physics-Informed Neural Ordinary\nDifferential Equations (ODEs) to model temporal dynamics and address these\nchallenges. Our method achieves high-accuracy $T_1$ estimation from a sparse\nsubset of baseline images and ensures efficient null index estimation at test\ntime. Specifically, we develop a continuous-time LSTM-ODE model to enable\nselective Look-Locker (LL) data acquisition with arbitrary time lags.\nExperimental results show superior performance in $T_1$ estimation for both\nnative and post-contrast sequences and demonstrate the strong benefit of our\nphysics-based formulation over direct data-driven $T_1$ priors.", "comment": "Submitted version. Accepted at MICCAI 2025", "pdf_url": "http://arxiv.org/pdf/2507.00613v1", "categories": ["eess.IV", "cs.AI"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2507.00613v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00546", "title": "Inverse Design in Nanophotonics via Representation Learning", "authors": ["Reza Marzban", "Ali Adibi", "Raphael Pestourie"], "summary": "Inverse design in nanophotonics, the computational discovery of structures\nachieving targeted electromagnetic (EM) responses, has become a key tool for\nrecent optical advances. Traditional intuition-driven or iterative optimization\nmethods struggle with the inherently high-dimensional, non-convex design spaces\nand the substantial computational demands of EM simulations. Recently, machine\nlearning (ML) has emerged to address these bottlenecks effectively. This review\nframes ML-enhanced inverse design methodologies through the lens of\nrepresentation learning, classifying them into two categories: output-side and\ninput-side approaches. Output-side methods use ML to learn a representation in\nthe solution space to create a differentiable solver that accelerates\noptimization. Conversely, input-side techniques employ ML to learn compact,\nlatent-space representations of feasible device geometries, enabling efficient\nglobal exploration through generative models. Each strategy presents unique\ntrade-offs in data requirements, generalization capacity, and novel design\ndiscovery potentials. Hybrid frameworks that combine physics-based optimization\nwith data-driven representations help escape poor local optima, improve\nscalability, and facilitate knowledge transfer. We conclude by highlighting\nopen challenges and opportunities, emphasizing complexity management,\ngeometry-independent representations, integration of fabrication constraints,\nand advancements in multiphysics co-designs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00546v1", "categories": ["physics.app-ph", "cs.AI", "cs.LG", "physics.optics"], "cate": "physics.app-ph", "url": "http://arxiv.org/abs/2507.00546v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00832", "title": "Automated anatomy-based post-processing reduces false positives and improved interpretability of deep learning intracranial aneurysm detection", "authors": ["Jisoo Kim", "Chu-Hsuan Lin", "Alberto Ceballos-Arroyo", "Ping Liu", "Huaizu Jiang", "Shrikanth Yadav", "Qi Wan", "Lei Qin", "Geoffrey S Young"], "summary": "Introduction: Deep learning (DL) models can help detect intracranial\naneurysms on CTA, but high false positive (FP) rates remain a barrier to\nclinical translation, despite improvement in model architectures and strategies\nlike detection threshold tuning. We employed an automated, anatomy-based,\nheuristic-learning hybrid artery-vein segmentation post-processing method to\nfurther reduce FPs. Methods: Two DL models, CPM-Net and a deformable 3D\nconvolutional neural network-transformer hybrid (3D-CNN-TR), were trained with\n1,186 open-source CTAs (1,373 annotated aneurysms), and evaluated with 143\nheld-out private CTAs (218 annotated aneurysms). Brain, artery, vein, and\ncavernous venous sinus (CVS) segmentation masks were applied to remove possible\nFPs in the DL outputs that overlapped with: (1) brain mask; (2) vein mask; (3)\nvein more than artery masks; (4) brain plus vein mask; (5) brain plus vein more\nthan artery masks. Results: CPM-Net yielded 139 true-positives (TP); 79\nfalse-negative (FN); 126 FP. 3D-CNN-TR yielded 179 TP; 39 FN; 182 FP. FPs were\ncommonly extracranial (CPM-Net 27.3%; 3D-CNN-TR 42.3%), venous (CPM-Net 56.3%;\n3D-CNN-TR 29.1%), arterial (CPM-Net 11.9%; 3D-CNN-TR 53.3%), and non-vascular\n(CPM-Net 25.4%; 3D-CNN-TR 9.3%) structures. Method 5 performed best, reducing\nCPM-Net FP by 70.6% (89/126) and 3D-CNN-TR FP by 51.6% (94/182), without\nreducing TP, lowering the FP/case rate from 0.88 to 0.26 for CPM-NET, and from\n1.27 to 0.62 for the 3D-CNN-TR. Conclusion: Anatomy-based, interpretable\npost-processing can improve DL-based aneurysm detection model performance. More\nbroadly, automated, domain-informed, hybrid heuristic-learning processing holds\npromise for improving the performance and clinical acceptance of aneurysm\ndetection models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00832v1", "categories": ["eess.IV", "cs.AI", "cs.CV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2507.00832v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00631", "title": "Horus: A Protocol for Trustless Delegation Under Uncertainty", "authors": ["David Shi", "Kevin Joo"], "summary": "Correctness is an emergent property of systems where exposing error is\ncheaper than committing it. In dynamic, low-trust environments, autonomous AI\nagents benefit from delegating work to sub-agents, yet correctness cannot be\nassured through upfront specification or centralized oversight. We propose a\nprotocol that enforces correctness through collateralized claims in a recursive\nverification game. Tasks are published as intents, and solvers compete to\nfulfill them. Selected solvers carry out tasks under risk, with correctness\nchecked post hoc by verifiers. Any challenger can challenge a result by staking\nagainst it to trigger the verification process. Incorrect agents are slashed\nand correct opposition is rewarded, with an escalation path that penalizes\nerroneous verifiers themselves. When incentives are aligned across solvers,\nchallengers, and verifiers, falsification conditions make correctness the Nash\nequilibrium.", "comment": "9 pages, 1 figure", "pdf_url": "http://arxiv.org/pdf/2507.00631v1", "categories": ["cs.GT", "cs.AI", "cs.MA", "I.2.11; F.2.2"], "cate": "cs.GT", "url": "http://arxiv.org/abs/2507.00631v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00583", "title": "AI-Generated Video Detection via Perceptual Straightening", "authors": ["Christian Internò", "Robert Geirhos", "Markus Olhofer", "Sunny Liu", "Barbara Hammer", "David Klindt"], "summary": "The rapid advancement of generative AI enables highly realistic synthetic\nvideos, posing significant challenges for content authentication and raising\nurgent concerns about misuse. Existing detection methods often struggle with\ngeneralization and capturing subtle temporal inconsistencies. We propose\nReStraV(Representation Straightening Video), a novel approach to distinguish\nnatural from AI-generated videos. Inspired by the \"perceptual straightening\"\nhypothesis -- which suggests real-world video trajectories become more straight\nin neural representation domain -- we analyze deviations from this expected\ngeometric property. Using a pre-trained self-supervised vision transformer\n(DINOv2), we quantify the temporal curvature and stepwise distance in the\nmodel's representation domain. We aggregate statistics of these measures for\neach video and train a classifier. Our analysis shows that AI-generated videos\nexhibit significantly different curvature and distance patterns compared to\nreal videos. A lightweight classifier achieves state-of-the-art detection\nperformance (e.g., 97.17% accuracy and 98.63% AUROC on the VidProM benchmark),\nsubstantially outperforming existing image- and video-based methods. ReStraV is\ncomputationally efficient, it is offering a low-cost and effective detection\nsolution. This work provides new insights into using neural representation\ngeometry for AI-generated video detection.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00583v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00583v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00903", "title": "Deep learning-based segmentation of T1 and T2 cardiac MRI maps for automated disease detection", "authors": ["Andreea Bianca Popescu", "Andreas Seitz", "Heiko Mahrholdt", "Jens Wetzl", "Athira Jacob", "Lucian Mihai Itu", "Constantin Suciu", "Teodora Chitiboi"], "summary": "Objectives Parametric tissue mapping enables quantitative cardiac tissue\ncharacterization but is limited by inter-observer variability during manual\ndelineation. Traditional approaches relying on average relaxation values and\nsingle cutoffs may oversimplify myocardial complexity. This study evaluates\nwhether deep learning (DL) can achieve segmentation accuracy comparable to\ninter-observer variability, explores the utility of statistical features beyond\nmean T1/T2 values, and assesses whether machine learning (ML) combining\nmultiple features enhances disease detection. Materials & Methods T1 and T2\nmaps were manually segmented. The test subset was independently annotated by\ntwo observers, and inter-observer variability was assessed. A DL model was\ntrained to segment left ventricle blood pool and myocardium. Average (A), lower\nquartile (LQ), median (M), and upper quartile (UQ) were computed for the\nmyocardial pixels and employed in classification by applying cutoffs or in ML.\nDice similarity coefficient (DICE) and mean absolute percentage error evaluated\nsegmentation performance. Bland-Altman plots assessed inter-user and\nmodel-observer agreement. Receiver operating characteristic analysis determined\noptimal cutoffs. Pearson correlation compared features from model and manual\nsegmentations. F1-score, precision, and recall evaluated classification\nperformance. Wilcoxon test assessed differences between classification methods,\nwith p < 0.05 considered statistically significant. Results 144 subjects were\nsplit into training (100), validation (15) and evaluation (29) subsets.\nSegmentation model achieved a DICE of 85.4%, surpassing inter-observer\nagreement. Random forest applied to all features increased F1-score (92.7%, p <\n0.001). Conclusion DL facilitates segmentation of T1/ T2 maps. Combining\nmultiple features with ML improves disease detection.", "comment": "This work has been submitted for consideration at European Radiology\n  (Springer). Upon acceptance, this preprint will be updated with the journal\n  reference", "pdf_url": "http://arxiv.org/pdf/2507.00903v1", "categories": ["eess.IV", "cs.AI", "cs.CV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2507.00903v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00653", "title": "Cognitive Load-Aware Inference: A Neuro-Symbolic Framework for Optimizing the Token Economy of Large Language Models", "authors": ["Yilun Zhang"], "summary": "The escalating computational costs of Large Language Model (LLM) inference\nhave become a critical barrier to their widespread and sustainable deployment.\nWhile existing optimization strategies are effective, they are predominantly\nbased on statistical heuristics or architectural modifications, lacking a\nguiding cognitive theory to manage the inference process itself. This paper\naims to bridge this gap by introducing a novel paradigm: the Cognitive\nLoad-Aware Inference (CLAI) framework, which operationalizes principles from\nCognitive Load Theory (CLT) and neuroscience for LLM inference. We formalize\nthe concepts of Intrinsic Cognitive Load, Extraneous Cognitive Load, and\nGermane Cognitive Load into quantifiable LLM metrics ($ICL_{LLM}$, $ECL_{LLM}$,\nand $GCL_{LLM}$), thereby reframing the inference process as a cognitive\neconomics optimization problem: based on the intrinsic complexity of a problem\n($ICL_{LLM}$), minimize wasteful computation ($ECL_{LLM}$), and strategically\nallocate the token budget to productive reasoning ($GCL_{LLM}$). We propose two\nimplementation paths: CLAI-Prompt, a zero-shot method that guides a base LLM\nthrough cognitive control steps via a structured meta-prompt, and CLAI-Tune, a\nfine-tuned model that internalizes these principles for spontaneous cognitive\neconomy. Across a range of benchmarks in complex reasoning, long-context\nquestion answering, and code generation, our methods achieve significant\nreductions in token consumption (up to 45\\%) without sacrificing accuracy.\nFurthermore, CLAI-Tune exhibits an emergent ability to autonomously decompose\ndifficult problems, a key characteristic of human expert cognition. This work\ndemonstrates that by emulating the brain's resource management strategies, we\ncan build more efficient, robust, and capable artificial intelligence systems.", "comment": "23 pages", "pdf_url": "http://arxiv.org/pdf/2507.00653v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00653v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00600", "title": "A Practical Guide to Interpretable Role-Based Clustering in Multi-Layer Financial Networks", "authors": ["Christian Franssen", "Iman van Lelyveld", "Bernd Heidergott"], "summary": "Understanding the functional roles of financial institutions within\ninterconnected markets is critical for effective supervision, systemic risk\nassessment, and resolution planning. We propose an interpretable role-based\nclustering approach for multi-layer financial networks, designed to identify\nthe functional positions of institutions across different market segments. Our\nmethod follows a general clustering framework defined by proximity measures,\ncluster evaluation criteria, and algorithm selection. We construct explainable\nnode embeddings based on egonet features that capture both direct and indirect\ntrading relationships within and across market layers. Using transaction-level\ndata from the ECB's Money Market Statistical Reporting (MMSR), we demonstrate\nhow the approach uncovers heterogeneous institutional roles such as market\nintermediaries, cross-segment connectors, and peripheral lenders or borrowers.\nThe results highlight the flexibility and practical value of role-based\nclustering in analyzing financial networks and understanding institutional\nbehavior in complex market structures.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00600v1", "categories": ["cs.SI", "cs.LG"], "cate": "cs.SI", "url": "http://arxiv.org/abs/2507.00600v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00937", "title": "RaGNNarok: A Light-Weight Graph Neural Network for Enhancing Radar Point Clouds on Unmanned Ground Vehicles", "authors": ["David Hunt", "Shaocheng Luo", "Spencer Hallyburton", "Shafii Nillongo", "Yi Li", "Tingjun Chen", "Miroslav Pajic"], "summary": "Low-cost indoor mobile robots have gained popularity with the increasing\nadoption of automation in homes and commercial spaces. However, existing lidar\nand camera-based solutions have limitations such as poor performance in\nvisually obscured environments, high computational overhead for data\nprocessing, and high costs for lidars. In contrast, mmWave radar sensors offer\na cost-effective and lightweight alternative, providing accurate ranging\nregardless of visibility. However, existing radar-based localization suffers\nfrom sparse point cloud generation, noise, and false detections. Thus, in this\nwork, we introduce RaGNNarok, a real-time, lightweight, and generalizable graph\nneural network (GNN)-based framework to enhance radar point clouds, even in\ncomplex and dynamic environments. With an inference time of just 7.3 ms on the\nlow-cost Raspberry Pi 5, RaGNNarok runs efficiently even on such\nresource-constrained devices, requiring no additional computational resources.\nWe evaluate its performance across key tasks, including localization, SLAM, and\nautonomous navigation, in three different environments. Our results demonstrate\nstrong reliability and generalizability, making RaGNNarok a robust solution for\nlow-cost indoor mobile robots.", "comment": "8 pages, accepted by IROS 2025", "pdf_url": "http://arxiv.org/pdf/2507.00937v1", "categories": ["cs.RO", "cs.AR", "cs.CV", "cs.LG"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2507.00937v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00657", "title": "Generative Exaggeration in LLM Social Agents: Consistency, Bias, and Toxicity", "authors": ["Jacopo Nudo", "Mario Edoardo Pandolfo", "Edoardo Loru", "Mattia Samory", "Matteo Cinelli", "Walter Quattrociocchi"], "summary": "We investigate how Large Language Models (LLMs) behave when simulating\npolitical discourse on social media. Leveraging 21 million interactions on X\nduring the 2024 U.S. presidential election, we construct LLM agents based on\n1,186 real users, prompting them to reply to politically salient tweets under\ncontrolled conditions. Agents are initialized either with minimal ideological\ncues (Zero Shot) or recent tweet history (Few Shot), allowing one-to-one\ncomparisons with human replies. We evaluate three model families (Gemini,\nMistral, and DeepSeek) across linguistic style, ideological consistency, and\ntoxicity. We find that richer contextualization improves internal consistency\nbut also amplifies polarization, stylized signals, and harmful language. We\nobserve an emergent distortion that we call \"generation exaggeration\": a\nsystematic amplification of salient traits beyond empirical baselines. Our\nanalysis shows that LLMs do not emulate users, they reconstruct them. Their\noutputs, indeed, reflect internal optimization dynamics more than observed\nbehavior, introducing structural biases that compromise their reliability as\nsocial proxies. This challenges their use in content moderation, deliberative\nsimulations, and policy modeling.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00657v1", "categories": ["cs.HC", "cs.AI", "cs.SI"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2507.00657v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00616", "title": "Geometric Gaussian Approximations of Probability Distributions", "authors": ["Nathaël Da Costa", "Bálint Mucsányi", "Philipp Hennig"], "summary": "Approximating complex probability distributions, such as Bayesian posterior\ndistributions, is of central interest in many applications. We study the\nexpressivity of geometric Gaussian approximations. These consist of\napproximations by Gaussian pushforwards through diffeomorphisms or Riemannian\nexponential maps. We first review these two different kinds of geometric\nGaussian approximations. Then we explore their relationship to one another. We\nfurther provide a constructive proof that such geometric Gaussian\napproximations are universal, in that they can capture any probability\ndistribution. Finally, we discuss whether, given a family of probability\ndistributions, a common diffeomorphism can be found to obtain uniformly\nhigh-quality geometric Gaussian approximations for that family.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00616v1", "categories": ["math.DG", "cs.LG", "math.PR", "math.ST", "stat.TH"], "cate": "math.DG", "url": "http://arxiv.org/abs/2507.00616v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00983", "title": "DMCIE: Diffusion Model with Concatenation of Inputs and Errors to Improve the Accuracy of the Segmentation of Brain Tumors in MRI Images", "authors": ["Sara Yavari", "Rahul Nitin Pandya", "Jacob Furst"], "summary": "Accurate segmentation of brain tumors in MRI scans is essential for reliable\nclinical diagnosis and effective treatment planning. Recently, diffusion models\nhave demonstrated remarkable effectiveness in image generation and segmentation\ntasks. This paper introduces a novel approach to corrective segmentation based\non diffusion models. We propose DMCIE (Diffusion Model with Concatenation of\nInputs and Errors), a novel framework for accurate brain tumor segmentation in\nmulti-modal MRI scans. We employ a 3D U-Net to generate an initial segmentation\nmask, from which an error map is generated by identifying the differences\nbetween the prediction and the ground truth. The error map, concatenated with\nthe original MRI images, are used to guide a diffusion model. Using multimodal\nMRI inputs (T1, T1ce, T2, FLAIR), DMCIE effectively enhances segmentation\naccuracy by focusing on misclassified regions, guided by the original inputs.\nEvaluated on the BraTS2020 dataset, DMCIE outperforms several state-of-the-art\ndiffusion-based segmentation methods, achieving a Dice Score of 93.46 and an\nHD95 of 5.94 mm. These results highlight the effectiveness of error-guided\ndiffusion in producing precise and reliable brain tumor segmentations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00983v1", "categories": ["eess.IV", "cs.CV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2507.00983v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00660", "title": "MTCNet: Motion and Topology Consistency Guided Learning for Mitral Valve Segmentationin 4D Ultrasound", "authors": ["Rusi Chen", "Yuanting Yang", "Jiezhi Yao", "Hongning Song", "Ji Zhang", "Yongsong Zhou", "Yuhao Huang", "Ronghao Yang", "Dan Jia", "Yuhan Zhang", "Xing Tao", "Haoran Dou", "Qing Zhou", "Xin Yang", "Dong Ni"], "summary": "Mitral regurgitation is one of the most prevalent cardiac disorders.\nFour-dimensional (4D) ultrasound has emerged as the primary imaging modality\nfor assessing dynamic valvular morphology. However, 4D mitral valve (MV)\nanalysis remains challenging due to limited phase annotations, severe motion\nartifacts, and poor imaging quality. Yet, the absence of inter-phase dependency\nin existing methods hinders 4D MV analysis. To bridge this gap, we propose a\nMotion-Topology guided consistency network (MTCNet) for accurate 4D MV\nultrasound segmentation in semi-supervised learning (SSL). MTCNet requires only\nsparse end-diastolic and end-systolic annotations. First, we design a\ncross-phase motion-guided consistency learning strategy, utilizing a\nbi-directional attention memory bank to propagate spatio-temporal features.\nThis enables MTCNet to achieve excellent performance both per- and inter-phase.\nSecond, we devise a novel topology-guided correlation regularization that\nexplores physical prior knowledge to maintain anatomically plausible.\nTherefore, MTCNet can effectively leverage structural correspondence between\nlabeled and unlabeled phases. Extensive evaluations on the first largest 4D MV\ndataset, with 1408 phases from 160 patients, show that MTCNet performs superior\ncross-phase consistency compared to other advanced methods (Dice: 87.30%, HD:\n1.75mm). Both the code and the dataset are available at\nhttps://github.com/crs524/MTCNet.", "comment": "Accepted by MICCAI 2025", "pdf_url": "http://arxiv.org/pdf/2507.00660v1", "categories": ["eess.IV", "cs.AI", "cs.CV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2507.00660v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00629", "title": "Generalization performance of narrow one-hidden layer networks in the teacher-student setting", "authors": ["Jean Barbier", "Federica Gerace", "Alessandro Ingrosso", "Clarissa Lauditi", "Enrico M. Malatesta", "Gibbs Nwemadji", "Rodrigo Pérez Ortiz"], "summary": "Understanding the generalization abilities of neural networks for simple\ninput-output distributions is crucial to account for their learning performance\non real datasets. The classical teacher-student setting, where a network is\ntrained from data obtained thanks to a label-generating teacher model, serves\nas a perfect theoretical test bed. In this context, a complete theoretical\naccount of the performance of fully connected one-hidden layer networks in the\npresence of generic activation functions is lacking. In this work, we develop\nsuch a general theory for narrow networks, i.e. networks with a large number of\nhidden units, yet much smaller than the input dimension. Using methods from\nstatistical physics, we provide closed-form expressions for the typical\nperformance of both finite temperature (Bayesian) and empirical risk\nminimization estimators, in terms of a small number of weight statistics. In\ndoing so, we highlight the presence of a transition where hidden neurons\nspecialize when the number of samples is sufficiently large and proportional to\nthe number of parameters of the network. Our theory accurately predicts the\ngeneralization error of neural networks trained on regression or classification\ntasks with either noisy full-batch gradient descent (Langevin dynamics) or\nfull-batch gradient descent.", "comment": "34 pages, figures", "pdf_url": "http://arxiv.org/pdf/2507.00629v1", "categories": ["cond-mat.dis-nn", "cs.LG", "math.PR", "math.ST", "stat.TH"], "cate": "cond-mat.dis-nn", "url": "http://arxiv.org/abs/2507.00629v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00984", "title": "Box Pose and Shape Estimation and Domain Adaptation for Large-Scale Warehouse Automation", "authors": ["Xihang Yu", "Rajat Talak", "Jingnan Shi", "Ulrich Viereck", "Igor Gilitschenski", "Luca Carlone"], "summary": "Modern warehouse automation systems rely on fleets of intelligent robots that\ngenerate vast amounts of data -- most of which remains unannotated. This paper\ndevelops a self-supervised domain adaptation pipeline that leverages\nreal-world, unlabeled data to improve perception models without requiring\nmanual annotations. Our work focuses specifically on estimating the pose and\nshape of boxes and presents a correct-and-certify pipeline for self-supervised\nbox pose and shape estimation. We extensively evaluate our approach across a\nrange of simulated and real industrial settings, including adaptation to a\nlarge-scale real-world dataset of 50,000 images. The self-supervised model\nsignificantly outperforms models trained solely in simulation and shows\nsubstantial improvements over a zero-shot 3D bounding box estimation baseline.", "comment": "12 pages, 6 figures. This work will be presented at the 19th\n  International Symposium on Experimental Robotics (ISER2025)", "pdf_url": "http://arxiv.org/pdf/2507.00984v1", "categories": ["cs.RO", "cs.CV", "cs.LG"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2507.00984v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00665", "title": "SAFER: Probing Safety in Reward Models with Sparse Autoencoder", "authors": ["Sihang Li", "Wei Shi", "Ziyuan Xie", "Tao Liang", "Guojun Ma", "Xiang Wang"], "summary": "Reinforcement learning from human feedback (RLHF) is a key paradigm for\naligning large language models (LLMs) with human values, yet the reward models\nat its core remain largely opaque. In this work, we present sparse Autoencoder\nFor Enhanced Reward model (\\textbf{SAFER}), a novel framework for interpreting\nand improving reward models through mechanistic analysis. Leveraging Sparse\nAutoencoders (SAEs), we uncover human-interpretable features in reward model\nactivations, enabling insight into safety-relevant decision-making. We apply\nSAFER to safety-oriented preference datasets and quantify the salience of\nindividual features by activation differences between chosen and rejected\nresponses. Using these feature-level signals, we design targeted data poisoning\nand denoising strategies. Experiments show that SAFER can precisely degrade or\nenhance safety alignment with minimal data modification, without sacrificing\ngeneral chat performance. Our approach contributes to interpreting, auditing\nand refining reward models in high-stakes LLM alignment tasks. Our codes are\navailable at https://github.com/xzy-101/SAFER-code. \\textit{This paper\ndiscusses topics related to large language model safety and may include\ndiscussions or examples that highlight potential risks or unsafe outcomes.}", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00665v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2507.00665v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00640", "title": "Forward Reverse Kernel Regression for the Schrödinger bridge problem", "authors": ["Denis Belomestny", "John. Schoenmakers"], "summary": "In this paper, we study the Schr\\\"odinger Bridge Problem (SBP), which is\ncentral to entropic optimal transport. For general reference processes and\nbegin--endpoint distributions, we propose a forward-reverse iterative Monte\nCarlo procedure to approximate the Schr\\\"odinger potentials in a nonparametric\nway. In particular, we use kernel based Monte Carlo regression in the context\nof Picard iteration of a corresponding fixed point problem. By preserving in\nthe iteration positivity and contractivity in a Hilbert metric sense, we\ndevelop a provably convergent algorithm. Furthermore, we provide convergence\nrates for the potential estimates and prove their optimality. Finally, as an\napplication, we propose a non-nested Monte Carlo procedure for the final\ndimensional distributions of the Schr\\\"odinger Bridge process, based on the\nconstructed potentials and the forward-reverse simulation method for\nconditional diffusions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00640v1", "categories": ["stat.ML", "cs.LG", "cs.NA", "math.NA", "90C40, 65C05, 62G08"], "cate": "stat.ML", "url": "http://arxiv.org/abs/2507.00640v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00990", "title": "Robotic Manipulation by Imitating Generated Videos Without Physical Demonstrations", "authors": ["Shivansh Patel", "Shraddhaa Mohan", "Hanlin Mai", "Unnat Jain", "Svetlana Lazebnik", "Yunzhu Li"], "summary": "This work introduces Robots Imitating Generated Videos (RIGVid), a system\nthat enables robots to perform complex manipulation tasks--such as pouring,\nwiping, and mixing--purely by imitating AI-generated videos, without requiring\nany physical demonstrations or robot-specific training. Given a language\ncommand and an initial scene image, a video diffusion model generates potential\ndemonstration videos, and a vision-language model (VLM) automatically filters\nout results that do not follow the command. A 6D pose tracker then extracts\nobject trajectories from the video, and the trajectories are retargeted to the\nrobot in an embodiment-agnostic fashion. Through extensive real-world\nevaluations, we show that filtered generated videos are as effective as real\ndemonstrations, and that performance improves with generation quality. We also\nshow that relying on generated videos outperforms more compact alternatives\nsuch as keypoint prediction using VLMs, and that strong 6D pose tracking\noutperforms other ways to extract trajectories, such as dense feature point\ntracking. These findings suggest that videos produced by a state-of-the-art\noff-the-shelf model can offer an effective source of supervision for robotic\nmanipulation.", "comment": "Project Page: https://rigvid-robot.github.io/", "pdf_url": "http://arxiv.org/pdf/2507.00990v1", "categories": ["cs.RO", "cs.AI", "cs.CV"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2507.00990v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00669", "title": "Audio-3DVG: Unified Audio - Point Cloud Fusion for 3D Visual Grounding", "authors": ["Duc Cao-Dinh", "Khai Le-Duc", "Anh Dao", "Bach Phan Tat", "Chris Ngo", "Duy M. H. Nguyen", "Nguyen X. Khanh", "Thanh Nguyen-Tang"], "summary": "3D Visual Grounding (3DVG) involves localizing target objects in 3D point\nclouds based on natural language. While prior work has made strides using\ntextual descriptions, leveraging spoken language-known as Audio-based 3D Visual\nGrounding-remains underexplored and challenging. Motivated by advances in\nautomatic speech recognition (ASR) and speech representation learning, we\npropose Audio-3DVG, a simple yet effective framework that integrates audio and\nspatial information for enhanced grounding. Rather than treating speech as a\nmonolithic input, we decompose the task into two complementary components.\nFirst, we introduce Object Mention Detection, a multi-label classification task\nthat explicitly identifies which objects are referred to in the audio, enabling\nmore structured audio-scene reasoning. Second, we propose an Audio-Guided\nAttention module that captures interactions between candidate objects and\nrelational speech cues, improving target discrimination in cluttered scenes. To\nsupport benchmarking, we synthesize audio descriptions for standard 3DVG\ndatasets, including ScanRefer, Sr3D, and Nr3D. Experimental results demonstrate\nthat Audio-3DVG not only achieves new state-of-the-art performance in\naudio-based grounding, but also competes with text-based methods-highlighting\nthe promise of integrating spoken language into 3D vision tasks.", "comment": "Work in progress, 42 pages", "pdf_url": "http://arxiv.org/pdf/2507.00669v1", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.RO"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00669v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00641", "title": "Hebbian Physics Networks: A Self-Organizing Computational Architecture Based on Local Physical Laws", "authors": ["Gunjan Auti", "Hirofumi Daiguji", "Gouhei Tanaka"], "summary": "Traditional machine learning approaches in physics rely on global\noptimization, limiting interpretability and enforcing physical constraints\nexternally. We introduce the Hebbian Physics Network (HPN), a self-organizing\ncomputational framework in which learning emerges from local Hebbian updates\ndriven by violations of conservation laws. Grounded in non-equilibrium\nthermodynamics and inspired by Prigogine/'s theory of dissipative structures,\nHPNs eliminate the need for global loss functions by encoding physical laws\ndirectly into the system/'s local dynamics. Residuals - quantified imbalances\nin continuity, momentum, or energy - serve as thermodynamic signals that drive\nweight adaptation through generalized Hebbian plasticity. We demonstrate this\napproach on incompressible fluid flow and continuum diffusion, where physically\nconsistent structures emerge from random initial conditions without\nsupervision. HPNs reframe computation as a residual-driven thermodynamic\nprocess, offering an interpretable, scalable, and physically grounded\nalternative for modeling complex dynamical systems.", "comment": "6 pages, 2 figures, 2 supplementary videos", "pdf_url": "http://arxiv.org/pdf/2507.00641v1", "categories": ["nlin.AO", "cs.LG", "stat.CO", "stat.ME"], "cate": "nlin.AO", "url": "http://arxiv.org/abs/2507.00641v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00993", "title": "Advancing Lung Disease Diagnosis in 3D CT Scans", "authors": ["Qingqiu Li", "Runtian Yuan", "Junlin Hou", "Jilan Xu", "Yuejie Zhang", "Rui Feng", "Hao Chen"], "summary": "To enable more accurate diagnosis of lung disease in chest CT scans, we\npropose a straightforward yet effective model. Firstly, we analyze the\ncharacteristics of 3D CT scans and remove non-lung regions, which helps the\nmodel focus on lesion-related areas and reduces computational cost. We adopt\nResNeSt50 as a strong feature extractor, and use a weighted cross-entropy loss\nto mitigate class imbalance, especially for the underrepresented squamous cell\ncarcinoma category. Our model achieves a Macro F1 Score of 0.80 on the\nvalidation set of the Fair Disease Diagnosis Challenge, demonstrating its\nstrong performance in distinguishing between different lung conditions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00993v1", "categories": ["eess.IV", "cs.CV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2507.00993v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00709", "title": "TopoStreamer: Temporal Lane Segment Topology Reasoning in Autonomous Driving", "authors": ["Yiming Yang", "Yueru Luo", "Bingkun He", "Hongbin Lin", "Suzhong Fu", "Chao Yan", "Kun Tang", "Xinrui Yan", "Chao Zheng", "Shuguang Cui", "Zhen Li"], "summary": "Lane segment topology reasoning constructs a comprehensive road network by\ncapturing the topological relationships between lane segments and their\nsemantic types. This enables end-to-end autonomous driving systems to perform\nroad-dependent maneuvers such as turning and lane changing. However, the\nlimitations in consistent positional embedding and temporal multiple attribute\nlearning in existing methods hinder accurate roadnet reconstruction. To address\nthese issues, we propose TopoStreamer, an end-to-end temporal perception model\nfor lane segment topology reasoning. Specifically, TopoStreamer introduces\nthree key improvements: streaming attribute constraints, dynamic lane boundary\npositional encoding, and lane segment denoising. The streaming attribute\nconstraints enforce temporal consistency in both centerline and boundary\ncoordinates, along with their classifications. Meanwhile, dynamic lane boundary\npositional encoding enhances the learning of up-to-date positional information\nwithin queries, while lane segment denoising helps capture diverse lane segment\npatterns, ultimately improving model performance. Additionally, we assess the\naccuracy of existing models using a lane boundary classification metric, which\nserves as a crucial measure for lane-changing scenarios in autonomous driving.\nOn the OpenLane-V2 dataset, TopoStreamer demonstrates significant improvements\nover state-of-the-art methods, achieving substantial performance gains of +3.4%\nmAP in lane segment perception and +2.1% OLS in centerline perception tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00709v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00709v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00671", "title": "Harnessing the Power of Reinforcement Learning for Adaptive MCMC", "authors": ["Congye Wang", "Matthew A. Fisher", "Heishiro Kanagawa", "Wilson Chen", "Chris. J. Oates"], "summary": "Sampling algorithms drive probabilistic machine learning, and recent years\nhave seen an explosion in the diversity of tools for this task. However, the\nincreasing sophistication of sampling algorithms is correlated with an increase\nin the tuning burden. There is now a greater need than ever to treat the tuning\nof samplers as a learning task in its own right. In a conceptual breakthrough,\nWang et al (2025) formulated Metropolis-Hastings as a Markov decision process,\nopening up the possibility for adaptive tuning using Reinforcement Learning\n(RL). Their emphasis was on theoretical foundations; realising the practical\nbenefit of Reinforcement Learning Metropolis-Hastings (RLMH) was left for\nsubsequent work. The purpose of this paper is twofold: First, we observe the\nsurprising result that natural choices of reward, such as the acceptance rate,\nor the expected squared jump distance, provide insufficient signal for training\nRLMH. Instead, we propose a novel reward based on the contrastive divergence,\nwhose superior performance in the context of RLMH is demonstrated. Second, we\nexplore the potential of RLMH and present adaptive gradient-based samplers that\nbalance flexibility of the Markov transition kernel with learnability of the\nassociated RL task. A comprehensive simulation study using the posteriordb\nbenchmark supports the practical effectiveness of RLMH.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00671v1", "categories": ["stat.CO", "cs.LG", "stat.ML"], "cate": "stat.CO", "url": "http://arxiv.org/abs/2507.00671v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.01016", "title": "VQ-VLA: Improving Vision-Language-Action Models via Scaling Vector-Quantized Action Tokenizers", "authors": ["Yating Wang", "Haoyi Zhu", "Mingyu Liu", "Jiange Yang", "Hao-Shu Fang", "Tong He"], "summary": "In this paper, we introduce an innovative vector quantization based action\ntokenizer built upon the largest-scale action trajectory dataset to date,\nleveraging over 100 times more data than previous approaches. This extensive\ndataset enables our tokenizer to capture rich spatiotemporal dynamics,\nresulting in a model that not only accelerates inference but also generates\nsmoother and more coherent action outputs. Once trained, the tokenizer can be\nseamlessly adapted to a wide range of downstream tasks in a zero-shot manner,\nfrom short-horizon reactive behaviors to long-horizon planning. A key finding\nof our work is that the domain gap between synthetic and real action\ntrajectories is marginal, allowing us to effectively utilize a vast amount of\nsynthetic data during training without compromising real-world performance. To\nvalidate our approach, we conducted extensive experiments in both simulated\nenvironments and on real robotic platforms. The results demonstrate that as the\nvolume of synthetic trajectory data increases, the performance of our tokenizer\non downstream tasks improves significantly-most notably, achieving up to a 30%\nhigher success rate on two real-world tasks in long-horizon scenarios. These\nfindings highlight the potential of our action tokenizer as a robust and\nscalable solution for real-time embodied intelligence systems, paving the way\nfor more efficient and reliable robotic control in diverse application\ndomains.Project website: https://xiaoxiao0406.github.io/vqvla.github.io", "comment": "Accepted by ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.01016v1", "categories": ["cs.RO", "cs.CV"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2507.01016v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00724", "title": "Holmes: Towards Effective and Harmless Model Ownership Verification to Personalized Large Vision Models via Decoupling Common Features", "authors": ["Linghui Zhu", "Yiming Li", "Haiqin Weng", "Yan Liu", "Tianwei Zhang", "Shu-Tao Xia", "Zhi Wang"], "summary": "Large vision models achieve remarkable performance in various downstream\ntasks, primarily by personalizing pre-trained models through fine-tuning with\nprivate and valuable local data, which makes the personalized model a valuable\nintellectual property for its owner. Similar to the era of traditional DNNs,\nmodel stealing attacks also pose significant risks to these personalized\nmodels. However, in this paper, we reveal that most existing defense methods\n(developed for traditional DNNs), typically designed for models trained from\nscratch, either introduce additional security risks, are prone to misjudgment,\nor are even ineffective for fine-tuned models. To alleviate these problems,\nthis paper proposes a harmless model ownership verification method for\npersonalized models by decoupling similar common features. In general, our\nmethod consists of three main stages. In the first stage, we create shadow\nmodels that retain common features of the victim model while disrupting\ndataset-specific features. We represent the dataset-specific features of the\nvictim model by the output differences between the shadow and victim models.\nAfter that, a meta-classifier is trained to identify stolen models by\ndetermining whether suspicious models contain the dataset-specific features of\nthe victim. In the third stage, we conduct model ownership verification by\nhypothesis test to mitigate randomness and enhance robustness. Extensive\nexperiments on benchmark datasets verify the effectiveness of the proposed\nmethod in detecting different types of model stealing simultaneously.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00724v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00724v1", "date": "2025-06-24", "updated": "2025-06-24"}
{"id": "2507.00683", "title": "Testing the spin-bath view of self-attention: A Hamiltonian analysis of GPT-2 Transformer", "authors": ["Satadeep Bhattacharjee", "Seung-Cheol Lee"], "summary": "The recently proposed physics-based framework by Huo and\nJohnson~\\cite{huo2024capturing} models the attention mechanism of Large\nLanguage Models (LLMs) as an interacting two-body spin system, offering a\nfirst-principles explanation for phenomena like repetition and bias. Building\non this hypothesis, we extract the complete Query-Key weight matrices from a\nproduction-grade GPT-2 model and derive the corresponding effective Hamiltonian\nfor every attention head. From these Hamiltonians we obtain analytic\n\\textit{phase boundaries} logit gap criteria that predict which token should\ndominate the next-token distribution for a given context. A systematic\nevaluation on 144 heads across 20 factual-recall prompts reveals a strong\nnegative correlation between the theoretical logit gaps and the model's\nempirical token rankings ($r\\approx-0.70$, $p<10^{-3}$).Targeted ablations\nfurther show that suppressing the heads most aligned with the spin-bath\npredictions induces the anticipated shifts in output probabilities, confirming\na causal link rather than a coincidental association. Taken together, our\nfindings provide the first strong empirical evidence for the spin-bath analogy\nin a production-grade model. This validation not only furnishes a tractable,\nphysics-inspired lens for interpretability but also provides the groundwork for\nnovel generative models, bridging the gap between theoretical condensed matter\nphysics and AI.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00683v1", "categories": ["cond-mat.mtrl-sci", "cs.LG"], "cate": "cond-mat.mtrl-sci", "url": "http://arxiv.org/abs/2507.00683v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00755", "title": "LearnAFE: Circuit-Algorithm Co-design Framework for Learnable Audio Analog Front-End", "authors": ["Jinhai Hu", "Zhongyi Zhang", "Cong Sheng Leow", "Wang Ling Goh", "Yuan Gao"], "summary": "This paper presents a circuit-algorithm co-design framework for learnable\nanalog front-end (AFE) in audio signal classification. Designing AFE and\nbackend classifiers separately is a common practice but non-ideal, as shown in\nthis paper. Instead, this paper proposes a joint optimization of the backend\nclassifier with the AFE's transfer function to achieve system-level optimum.\nMore specifically, the transfer function parameters of an analog bandpass\nfilter (BPF) bank are tuned in a signal-to-noise ratio (SNR)-aware training\nloop for the classifier. Using a co-design loss function LBPF, this work shows\nsuperior optimization of both the filter bank and the classifier. Implemented\nin open-source SKY130 130nm CMOS process, the optimized design achieved\n90.5%-94.2% accuracy for 10-keyword classification task across a wide range of\ninput signal SNR from 5 dB to 20 dB, with only 22k classifier parameters.\nCompared to conventional approach, the proposed audio AFE achieves 8.7% and\n12.9% reduction in power and capacitor area respectively.", "comment": "11 pages, 15 figures, accepted for publication on IEEE Transactions\n  on Circuits and Systems I: Regular Papers", "pdf_url": "http://arxiv.org/pdf/2507.00755v1", "categories": ["eess.AS", "cs.AI", "cs.SD"], "cate": "eess.AS", "url": "http://arxiv.org/abs/2507.00755v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00719", "title": "Guided Unconditional and Conditional Generative Models for Super-Resolution and Inference of Quasi-Geostrophic Turbulence", "authors": ["Anantha Narayanan Suresh Babu", "Akhil Sadam", "Pierre F. J. Lermusiaux"], "summary": "Typically, numerical simulations of the ocean, weather, and climate are\ncoarse, and observations are sparse and gappy. In this work, we apply four\ngenerative diffusion modeling approaches to super-resolution and inference of\nforced two-dimensional quasi-geostrophic turbulence on the beta-plane from\ncoarse, sparse, and gappy observations. Two guided approaches minimally adapt a\npre-trained unconditional model: SDEdit modifies the initial condition, and\nDiffusion Posterior Sampling (DPS) modifies the reverse diffusion process\nscore. The other two conditional approaches, a vanilla variant and\nclassifier-free guidance, require training with paired high-resolution and\nobservation data. We consider eight test cases spanning: two regimes, eddy and\nanisotropic-jet turbulence; two Reynolds numbers, 10^3 and 10^4; and two\nobservation types, 4x coarse-resolution fields and coarse, sparse and gappy\nobservations. Our comprehensive skill metrics include norms of the\nreconstructed vorticity fields, turbulence statistical quantities, and\nquantification of the super-resolved probabilistic ensembles and their errors.\nWe also study the sensitivity to tuning parameters such as guidance strength.\nResults show that SDEdit generates unphysical fields, while DPS generates\nreasonable reconstructions at low computational cost but with smoothed\nfine-scale features. Both conditional approaches require re-training, but they\nreconstruct missing fine-scale features, are cycle-consistent with\nobservations, and possess the correct statistics such as energy spectra.\nFurther, their mean model errors are highly correlated with and predictable\nfrom their ensemble standard deviations. Results highlight the trade-offs\nbetween ease of implementation, fidelity (sharpness), and cycle-consistency of\nthe diffusion models, and offer practical guidance for deployment in\ngeophysical inverse problems.", "comment": "56 pages, 23 figures, 7 tables", "pdf_url": "http://arxiv.org/pdf/2507.00719v1", "categories": ["physics.flu-dyn", "cs.LG", "physics.ao-ph", "physics.geo-ph"], "cate": "physics.flu-dyn", "url": "http://arxiv.org/abs/2507.00719v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00769", "title": "LitBench: A Benchmark and Dataset for Reliable Evaluation of Creative Writing", "authors": ["Daniel Fein", "Sebastian Russo", "Violet Xiang", "Kabir Jolly", "Rafael Rafailov", "Nick Haber"], "summary": "Evaluating creative writing generated by large language models (LLMs) remains\nchallenging because open-ended narratives lack ground truths. Without\nperformant automated evaluation methods, off-the-shelf (OTS) language models\nare employed as zero-shot judges, yet their reliability is unclear in this\ncontext. In pursuit of robust evaluation for creative writing, we introduce\nLitBench, the first standardized benchmark and paired dataset for creative\nwriting verification, comprising a held-out test set of 2,480 debiased,\nhuman-labeled story comparisons drawn from Reddit and a 43,827-pair training\ncorpus of human preference labels. Using LitBench, we (i) benchmark zero-shot\nLLM judges, (ii) train Bradley Terry and generative reward models, and (iii)\nconduct an online human study to validate reward model rankings on newly\nLLM-generated stories. Our benchmark identifies Claude-3.7-Sonnet as the\nstrongest off-the-shelf judge, reaching 73% agreement with human preferences;\namong trained reward models, Bradley-Terry and Generative reward models both\nattain an accuracy of 78%, outperforming all off-the-shelf judges. An online\nhuman study further confirms that our trained reward models consistently align\nwith human preferences in novel LLM-generated stories. We release LitBench and\nreward models at\nhttps://huggingface.co/collections/SAA-Lab/litbench-68267b5da3aafe58f9e43461,\nproviding a vetted resource for reliable, automated evaluation and optimization\nof creative writing systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00769v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2507.00769v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00726", "title": "Can Large Language Models Develop Strategic Reasoning? Post-training Insights from Learning Chess", "authors": ["Dongyoon Hwang", "Hojoon Lee", "Jaegul Choo", "Dongmin Park", "Jongho Park"], "summary": "While reinforcement learning (RL) for large language models (LLMs) has shown\npromise in mathematical reasoning, strategic reasoning for LLMs using RL\nremains largely unexplored. We investigate whether LLMs can develop strategic\nreasoning capabilities through RL in chess. To this end, we leverage a\nchess-pretrained action-value network to provide dense reward on the LLM's\noutput move quality, which can be seen as a form of knowledge distillation. Our\nexperiments show that our distillation-based dense rewards often outperform\nsparse binary rewards. However, surprisingly, all models plateau far below\nexpert levels. We provide SFT and RL ablations on chess reasoning training and\nfind evidence that this limitation stems from a deficit in the pretrained\nmodels' internal understanding of chess--a deficit which RL alone may not be\nable to fully overcome.", "comment": "27 pages", "pdf_url": "http://arxiv.org/pdf/2507.00726v1", "categories": ["cs.AI", "cs.LG"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2507.00726v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00788", "title": "Echoes of AI: Investigating the Downstream Effects of AI Assistants on Software Maintainability", "authors": ["Markus Borg", "Dave Hewett", "Nadim Hagatulah", "Noric Couderc", "Emma Söderberg", "Donald Graham", "Uttam Kini", "Dave Farley"], "summary": "[Context] AI assistants, like GitHub Copilot and Cursor, are transforming\nsoftware engineering. While several studies highlight productivity\nimprovements, their impact on maintainability requires further investigation.\n[Objective] This study investigates whether co-development with AI assistants\naffects software maintainability, specifically how easily other developers can\nevolve the resulting source code. [Method] We conducted a two-phase controlled\nexperiment involving 151 participants, 95% of whom were professional\ndevelopers. In Phase 1, participants added a new feature to a Java web\napplication, with or without AI assistance. In Phase 2, a randomized controlled\ntrial, new participants evolved these solutions without AI assistance.\n[Results] AI-assisted development in Phase 1 led to a modest speedup in\nsubsequent evolution and slightly higher average CodeHealth. Although neither\ndifference was significant overall, the increase in CodeHealth was\nstatistically significant when habitual AI users completed Phase 1. For Phase\n1, we also observed a significant effect that corroborates previous\nproductivity findings: using an AI assistant yielded a 30.7% median decrease in\ntask completion time. Moreover, for habitual AI users, the mean speedup was\n55.9%. [Conclusions] Our study adds to the growing evidence that AI assistants\ncan effectively accelerate development. Moreover, we did not observe warning\nsigns of degraded code-level maintainability. We recommend that future research\nfocus on risks such as code bloat from excessive code generation and the\nbuild-up of cognitive debt as developers invest less mental effort during\nimplementation.", "comment": "Preprint of study preregistered at ICSME 2025 with In-Principal\n  Acceptance.\n  https://conf.researchr.org/track/icsme-2024/icsme-2024-registered-reports-track", "pdf_url": "http://arxiv.org/pdf/2507.00788v1", "categories": ["cs.SE", "cs.AI"], "cate": "cs.SE", "url": "http://arxiv.org/abs/2507.00788v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00747", "title": "SINDy on slow manifolds", "authors": ["Diemen Delgado-Cano", "Erick Kracht", "Urban Fasel", "Benjamin Herrmann"], "summary": "The sparse identification of nonlinear dynamics (SINDy) has been established\nas an effective method to learn interpretable models of dynamical systems from\ndata. However, for high-dimensional slow-fast dynamical systems, the regression\nproblem becomes simultaneously computationally intractable and ill-conditioned.\nAlthough, in principle, modeling only the dynamics evolving on the underlying\nslow manifold addresses both of these challenges, the truncated fast variables\nhave to be compensated by including higher-order nonlinearities as candidate\nterms for the model, leading to an explosive growth in the size of the SINDy\nlibrary. In this work, we develop a SINDy variant that is able to robustly and\nefficiently identify slow-fast dynamics in two steps: (i) identify the slow\nmanifold, that is, an algebraic equation for the fast variables as functions of\nthe slow ones, and (ii) learn a model for the dynamics of the slow variables\nrestricted to the manifold. Critically, the equation learned in (i) is\nleveraged to build a manifold-informed function library for (ii) that contains\nonly essential higher-order nonlinearites as candidate terms. Rather than\ncontaining all monomials of up to a certain degree, the resulting custom\nlibrary is a sparse subset of the latter that is tailored to the specific\nproblem at hand. The approach is demonstrated on numerical examples of a\nsnap-through buckling beam and the flow over a NACA 0012 airfoil. We find that\nour method significantly reduces both the condition number and the size of the\nSINDy library, thus enabling accurate identification of the dynamics on slow\nmanifolds.", "comment": "18 pages, 6 figures, to be submitted to Nonlinear Dynamics (Springer)", "pdf_url": "http://arxiv.org/pdf/2507.00747v1", "categories": ["math.DS", "cs.LG", "physics.comp-ph"], "cate": "math.DS", "url": "http://arxiv.org/abs/2507.00747v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00790", "title": "LD-RPS: Zero-Shot Unified Image Restoration via Latent Diffusion Recurrent Posterior Sampling", "authors": ["Huaqiu Li", "Yong Wang", "Tongwen Huang", "Hailang Huang", "Haoqian Wang", "Xiangxiang Chu"], "summary": "Unified image restoration is a significantly challenging task in low-level\nvision. Existing methods either make tailored designs for specific tasks,\nlimiting their generalizability across various types of degradation, or rely on\ntraining with paired datasets, thereby suffering from closed-set constraints.\nTo address these issues, we propose a novel, dataset-free, and unified approach\nthrough recurrent posterior sampling utilizing a pretrained latent diffusion\nmodel. Our method incorporates the multimodal understanding model to provide\nsematic priors for the generative model under a task-blind condition.\nFurthermore, it utilizes a lightweight module to align the degraded input with\nthe generated preference of the diffusion model, and employs recurrent\nrefinement for posterior sampling. Extensive experiments demonstrate that our\nmethod outperforms state-of-the-art methods, validating its effectiveness and\nrobustness. Our code and data will be available at\nhttps://github.com/AMAP-ML/LD-RPS.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00790v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00790v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00838", "title": "Stylometry recognizes human and LLM-generated texts in short samples", "authors": ["Karol Przystalski", "Jan K. Argasiński", "Iwona Grabska-Gradzińska", "Jeremi K. Ochab"], "summary": "The paper explores stylometry as a method to distinguish between texts\ncreated by Large Language Models (LLMs) and humans, addressing issues of model\nattribution, intellectual property, and ethical AI use. Stylometry has been\nused extensively to characterise the style and attribute authorship of texts.\nBy applying it to LLM-generated texts, we identify their emergent writing\npatterns. The paper involves creating a benchmark dataset based on Wikipedia,\nwith (a) human-written term summaries, (b) texts generated purely by LLMs\n(GPT-3.5/4, LLaMa 2/3, Orca, and Falcon), (c) processed through multiple text\nsummarisation methods (T5, BART, Gensim, and Sumy), and (d) rephrasing methods\n(Dipper, T5). The 10-sentence long texts were classified by tree-based models\n(decision trees and LightGBM) using human-designed (StyloMetrix) and\nn-gram-based (our own pipeline) stylometric features that encode lexical,\ngrammatical, syntactic, and punctuation patterns. The cross-validated results\nreached a performance of up to .87 Matthews correlation coefficient in the\nmulticlass scenario with 7 classes, and accuracy between .79 and 1. in binary\nclassification, with the particular example of Wikipedia and GPT-4 reaching up\nto .98 accuracy on a balanced dataset. Shapley Additive Explanations pinpointed\nfeatures characteristic of the encyclopaedic text type, individual overused\nwords, as well as a greater grammatical standardisation of LLMs with respect to\nhuman-written texts. These results show -- crucially, in the context of the\nincreasingly sophisticated LLMs -- that it is possible to distinguish machine-\nfrom human-generated texts at least for a well-defined text type.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00838v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2507.00838v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00814", "title": "Many LLMs Are More Utilitarian Than One", "authors": ["Anita Keshmirian", "Razan Baltaji", "Babak Hemmatian", "Hadi Asghari", "Lav R. Varshney"], "summary": "Moral judgment is integral to large language model (LLM) alignment and social\nreasoning. As multi-agent systems gain prominence, it becomes crucial to\nunderstand how LLMs function collectively during collaboration, compared to\nindividual agents. In human moral judgment, group deliberation leads to a\nutilitarian boost: a tendency to endorse norm violations that maximize benefits\nfor the greatest number of people despite harms. We study whether a similar\ndynamic emerges in multi-agent LLM systems. We tested six models on\nwell-established sets of moral dilemmas across two conditions: (1) Solo, where\nmodels reasoned independently, and (2) Group, where they engaged in multi-turn\ndiscussions in pairs or triads. In personal moral dilemmas, where agents must\ndecide to directly harm one individual to maximize the utility for others, all\nmodels found moral violations to be more acceptable when part of a group than\nindividually, similar to human experiments. Some models endorsed actions that\nmaximized overall well-being, even if they benefited strangers over familiar\nindividuals. Others became more willing to violate moral norms in groups.\nHowever, while human groups show a similar action bias, the mechanism for their\nutilitarian boost differs from LLMs. Whereas the human shift comes from\nheightened sensitivity to decision outcomes, LLM groups show either reduced\nnorm sensitivity or enhanced impartiality. This suggests that while the surface\nbehavior of LLM collectives mimics human group reasoning, the underlying\ndrivers differ. We discuss the implications for AI alignment, multi-agent\ndesign, and artificial moral reasoning.", "comment": "9 pages, 8 Figures, 7 tables", "pdf_url": "http://arxiv.org/pdf/2507.00814v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "I.2.7; I.2.11"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2507.00814v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00845", "title": "Do Echo Top Heights Improve Deep Learning Nowcasts?", "authors": ["Peter Pavlík", "Marc Schleiss", "Anna Bou Ezzeddine", "Viera Rozinajová"], "summary": "Precipitation nowcasting -- the short-term prediction of rainfall using\nrecent radar observations -- is critical for weather-sensitive sectors such as\ntransportation, agriculture, and disaster mitigation. While recent deep\nlearning models have shown promise in improving nowcasting skill, most\napproaches rely solely on 2D radar reflectivity fields, discarding valuable\nvertical information available in the full 3D radar volume. In this work, we\nexplore the use of Echo Top Height (ETH), a 2D projection indicating the\nmaximum altitude of radar reflectivity above a given threshold, as an auxiliary\ninput variable for deep learning-based nowcasting. We examine the relationship\nbetween ETH and radar reflectivity, confirming its relevance for predicting\nrainfall intensity. We implement a single-pass 3D U-Net that processes both the\nradar reflectivity and ETH as separate input channels. While our models are\nable to leverage ETH to improve skill at low rain-rate thresholds, results are\ninconsistent at higher intensities and the models with ETH systematically\nunderestimate precipitation intensity. Three case studies are used to\nillustrate how ETH can help in some cases, but also confuse the models and\nincrease the error variance. Nonetheless, the study serves as a foundation for\ncritically assessing the potential contribution of additional variables to\nnowcasting performance.", "comment": "Pre-review version of an article accepted at Transactions on\n  Large-Scale Data and Knowledge-Centered Systems", "pdf_url": "http://arxiv.org/pdf/2507.00845v1", "categories": ["cs.CV", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00845v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00816", "title": "PI-WAN: A Physics-Informed Wind-Adaptive Network for Quadrotor Dynamics Prediction in Unknown Environments", "authors": ["Mengyun Wang", "Bo Wang", "Yifeng Niu", "Chang Wang"], "summary": "Accurate dynamics modeling is essential for quadrotors to achieve precise\ntrajectory tracking in various applications. Traditional physical\nknowledge-driven modeling methods face substantial limitations in unknown\nenvironments characterized by variable payloads, wind disturbances, and\nexternal perturbations. On the other hand, data-driven modeling methods suffer\nfrom poor generalization when handling out-of-distribution (OoD) data,\nrestricting their effectiveness in unknown scenarios. To address these\nchallenges, we introduce the Physics-Informed Wind-Adaptive Network (PI-WAN),\nwhich combines knowledge-driven and data-driven modeling methods by embedding\nphysical constraints directly into the training process for robust quadrotor\ndynamics learning. Specifically, PI-WAN employs a Temporal Convolutional\nNetwork (TCN) architecture that efficiently captures temporal dependencies from\nhistorical flight data, while a physics-informed loss function applies physical\nprinciples to improve model generalization and robustness across previously\nunseen conditions. By incorporating real-time prediction results into a model\npredictive control (MPC) framework, we achieve improvements in closed-loop\ntracking performance. Comprehensive simulations and real-world flight\nexperiments demonstrate that our approach outperforms baseline methods in terms\nof prediction accuracy, tracking precision, and robustness to unknown\nenvironments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00816v1", "categories": ["cs.RO", "cs.AI"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2507.00816v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00866", "title": "Template-Fitting Meets Deep Learning: Redshift Estimation Using Physics-Guided Neural Networks", "authors": ["Jonas Chris Ferrao", "Dickson Dias", "Pranav Naik", "Glory D'Cruz", "Anish Naik", "Siya Khandeparkar", "Manisha Gokuldas Fal Dessai"], "summary": "Accurate photometric redshift estimation is critical for observational\ncosmology, especially in large-scale surveys where spectroscopic measurements\nare impractical. Traditional approaches include template fitting and machine\nlearning, each with distinct strengths and limitations. We present a hybrid\nmethod that integrates template fitting with deep learning using physics-guided\nneural networks. By embedding spectral energy distribution templates into the\nnetwork architecture, our model encodes physical priors into the training\nprocess. The system employs a multimodal design, incorporating cross-attention\nmechanisms to fuse photometric and image data, along with Bayesian layers for\nuncertainty estimation. We evaluate our model on the publicly available PREML\ndataset, which includes approximately 400,000 galaxies from the Hyper\nSuprime-Cam PDR3 release, with 5-band photometry, multi-band imaging, and\nspectroscopic redshifts. Our approach achieves an RMS error of 0.0507, a\n3-sigma catastrophic outlier rate of 0.13%, and a bias of 0.0028. The model\nsatisfies two of the three LSST photometric redshift requirements for redshifts\nbelow 3. These results highlight the potential of combining physically\nmotivated templates with data-driven models for robust redshift estimation in\nupcoming cosmological surveys.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00866v1", "categories": ["astro-ph.IM", "cs.LG"], "cate": "astro-ph.IM", "url": "http://arxiv.org/abs/2507.00866v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00817", "title": "CAVALRY-V: A Large-Scale Generator Framework for Adversarial Attacks on Video MLLMs", "authors": ["Jiaming Zhang", "Rui Hu", "Qing Guo", "Wei Yang Bryan Lim"], "summary": "Video Multimodal Large Language Models (V-MLLMs) have shown impressive\ncapabilities in temporal reasoning and cross-modal understanding, yet their\nvulnerability to adversarial attacks remains underexplored due to unique\nchallenges: complex cross-modal reasoning mechanisms, temporal dependencies,\nand computational constraints. We present CAVALRY-V (Cross-modal\nLanguage-Vision Adversarial Yielding for Videos), a novel framework that\ndirectly targets the critical interface between visual perception and language\ngeneration in V-MLLMs. Our approach introduces two key innovations: (1) a\ndual-objective semantic-visual loss function that simultaneously disrupts the\nmodel's text generation logits and visual representations to undermine\ncross-modal integration, and (2) a computationally efficient two-stage\ngenerator framework that combines large-scale pre-training for cross-model\ntransferability with specialized fine-tuning for spatiotemporal coherence.\nEmpirical evaluation on comprehensive video understanding benchmarks\ndemonstrates that CAVALRY-V significantly outperforms existing attack methods,\nachieving 22.8% average improvement over the best baseline attacks on both\ncommercial systems (GPT-4.1, Gemini 2.0) and open-source models (QwenVL-2.5,\nInternVL-2.5, Llava-Video, Aria, MiniCPM-o-2.6). Our framework achieves\nflexibility through implicit temporal coherence modeling rather than explicit\nregularization, enabling significant performance improvements even on image\nunderstanding (34.4% average gain). This capability demonstrates CAVALRY-V's\npotential as a foundational approach for adversarial research across multimodal\nsystems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00817v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00817v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00885", "title": "Scaling Laws Are Unreliable for Downstream Tasks: A Reality Check", "authors": ["Nicholas Lourie", "Michael Y. Hu", "Kyunghyun Cho"], "summary": "Downstream scaling laws aim to predict task performance at larger scales from\npretraining losses at smaller scales. Whether this prediction should be\npossible is unclear: some works demonstrate that task performance follows clear\nlinear scaling trends under transformation, whereas others point out\nfundamental challenges to downstream scaling laws, such as emergence and\ninverse scaling. In this work, we conduct a meta-analysis of existing data on\ndownstream scaling laws, finding that close fit to linear scaling laws only\noccurs in a minority of cases: 39% of the time. Furthermore, seemingly benign\nchanges to the experimental setting can completely change the scaling trend.\nOur analysis underscores the need to understand the conditions under which\nscaling laws succeed. To fully model the relationship between pretraining loss\nand downstream task performance, we must embrace the cases in which scaling\nbehavior deviates from linear trends.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00885v1", "categories": ["cs.CL", "cs.LG"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2507.00885v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00832", "title": "Automated anatomy-based post-processing reduces false positives and improved interpretability of deep learning intracranial aneurysm detection", "authors": ["Jisoo Kim", "Chu-Hsuan Lin", "Alberto Ceballos-Arroyo", "Ping Liu", "Huaizu Jiang", "Shrikanth Yadav", "Qi Wan", "Lei Qin", "Geoffrey S Young"], "summary": "Introduction: Deep learning (DL) models can help detect intracranial\naneurysms on CTA, but high false positive (FP) rates remain a barrier to\nclinical translation, despite improvement in model architectures and strategies\nlike detection threshold tuning. We employed an automated, anatomy-based,\nheuristic-learning hybrid artery-vein segmentation post-processing method to\nfurther reduce FPs. Methods: Two DL models, CPM-Net and a deformable 3D\nconvolutional neural network-transformer hybrid (3D-CNN-TR), were trained with\n1,186 open-source CTAs (1,373 annotated aneurysms), and evaluated with 143\nheld-out private CTAs (218 annotated aneurysms). Brain, artery, vein, and\ncavernous venous sinus (CVS) segmentation masks were applied to remove possible\nFPs in the DL outputs that overlapped with: (1) brain mask; (2) vein mask; (3)\nvein more than artery masks; (4) brain plus vein mask; (5) brain plus vein more\nthan artery masks. Results: CPM-Net yielded 139 true-positives (TP); 79\nfalse-negative (FN); 126 FP. 3D-CNN-TR yielded 179 TP; 39 FN; 182 FP. FPs were\ncommonly extracranial (CPM-Net 27.3%; 3D-CNN-TR 42.3%), venous (CPM-Net 56.3%;\n3D-CNN-TR 29.1%), arterial (CPM-Net 11.9%; 3D-CNN-TR 53.3%), and non-vascular\n(CPM-Net 25.4%; 3D-CNN-TR 9.3%) structures. Method 5 performed best, reducing\nCPM-Net FP by 70.6% (89/126) and 3D-CNN-TR FP by 51.6% (94/182), without\nreducing TP, lowering the FP/case rate from 0.88 to 0.26 for CPM-NET, and from\n1.27 to 0.62 for the 3D-CNN-TR. Conclusion: Anatomy-based, interpretable\npost-processing can improve DL-based aneurysm detection model performance. More\nbroadly, automated, domain-informed, hybrid heuristic-learning processing holds\npromise for improving the performance and clinical acceptance of aneurysm\ndetection models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00832v1", "categories": ["eess.IV", "cs.AI", "cs.CV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2507.00832v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00894", "title": "An in depth look at the Procrustes-Wasserstein distance: properties and barycenters", "authors": ["Davide Adamo", "Marco Corneli", "Manon Vuillien", "Emmanuelle Vila"], "summary": "Due to its invariance to rigid transformations such as rotations and\nreflections, Procrustes-Wasserstein (PW) was introduced in the literature as an\noptimal transport (OT) distance, alternative to Wasserstein and more suited to\ntasks such as the alignment and comparison of point clouds. Having that\napplication in mind, we carefully build a space of discrete probability\nmeasures and show that over that space PW actually is a distance. Algorithms to\nsolve the PW problems already exist, however we extend the PW framework by\ndiscussing and testing several initialization strategies. We then introduce the\nnotion of PW barycenter and detail an algorithm to estimate it from the data.\nThe result is a new method to compute representative shapes from a collection\nof point clouds. We benchmark our method against existing OT approaches,\ndemonstrating superior performance in scenarios requiring precise alignment and\nshape preservation. We finally show the usefulness of the PW barycenters in an\narchaeological context. Our results highlight the potential of PW in boosting\n2D and 3D point cloud analysis for machine learning and computational geometry\napplications.", "comment": "16 pages", "pdf_url": "http://arxiv.org/pdf/2507.00894v1", "categories": ["stat.ML", "cs.LG"], "cate": "stat.ML", "url": "http://arxiv.org/abs/2507.00894v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00833", "title": "HumanoidGen: Data Generation for Bimanual Dexterous Manipulation via LLM Reasoning", "authors": ["Zhi Jing", "Siyuan Yang", "Jicong Ao", "Ting Xiao", "Yugang Jiang", "Chenjia Bai"], "summary": "For robotic manipulation, existing robotics datasets and simulation\nbenchmarks predominantly cater to robot-arm platforms. However, for humanoid\nrobots equipped with dual arms and dexterous hands, simulation tasks and\nhigh-quality demonstrations are notably lacking. Bimanual dexterous\nmanipulation is inherently more complex, as it requires coordinated arm\nmovements and hand operations, making autonomous data collection challenging.\nThis paper presents HumanoidGen, an automated task creation and demonstration\ncollection framework that leverages atomic dexterous operations and LLM\nreasoning to generate relational constraints. Specifically, we provide spatial\nannotations for both assets and dexterous hands based on the atomic operations,\nand perform an LLM planner to generate a chain of actionable spatial\nconstraints for arm movements based on object affordances and scenes. To\nfurther improve planning ability, we employ a variant of Monte Carlo tree\nsearch to enhance LLM reasoning for long-horizon tasks and insufficient\nannotation. In experiments, we create a novel benchmark with augmented\nscenarios to evaluate the quality of the collected data. The results show that\nthe performance of the 2D and 3D diffusion policies can scale with the\ngenerated dataset. Project page is https://openhumanoidgen.github.io.", "comment": "Project Page: https://openhumanoidgen.github.io", "pdf_url": "http://arxiv.org/pdf/2507.00833v1", "categories": ["cs.RO", "cs.AI"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2507.00833v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00926", "title": "HyperFusion: Hierarchical Multimodal Ensemble Learning for Social Media Popularity Prediction", "authors": ["Liliang Ye", "Yunyao Zhang", "Yafeng Wu", "Yi-Ping Phoebe Chen", "Junqing Yu", "Wei Yang", "Zikai Song"], "summary": "Social media popularity prediction plays a crucial role in content\noptimization, marketing strategies, and user engagement enhancement across\ndigital platforms. However, predicting post popularity remains challenging due\nto the complex interplay between visual, textual, temporal, and user behavioral\nfactors. This paper presents HyperFusion, a hierarchical multimodal ensemble\nlearning framework for social media popularity prediction. Our approach employs\na three-tier fusion architecture that progressively integrates features across\nabstraction levels: visual representations from CLIP encoders, textual\nembeddings from transformer models, and temporal-spatial metadata with user\ncharacteristics. The framework implements a hierarchical ensemble strategy\ncombining CatBoost, TabNet, and custom multi-layer perceptrons. To address\nlimited labeled data, we propose a two-stage training methodology with\npseudo-labeling and iterative refinement. We introduce novel cross-modal\nsimilarity measures and hierarchical clustering features that capture\ninter-modal dependencies. Experimental results demonstrate that HyperFusion\nachieves competitive performance on the SMP challenge dataset. Our team\nachieved third place in the SMP Challenge 2025 (Image Track). The source code\nis available at https://anonymous.4open.science/r/SMPDImage.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00926v1", "categories": ["cs.MM", "cs.LG"], "cate": "cs.MM", "url": "http://arxiv.org/abs/2507.00926v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00838", "title": "Stylometry recognizes human and LLM-generated texts in short samples", "authors": ["Karol Przystalski", "Jan K. Argasiński", "Iwona Grabska-Gradzińska", "Jeremi K. Ochab"], "summary": "The paper explores stylometry as a method to distinguish between texts\ncreated by Large Language Models (LLMs) and humans, addressing issues of model\nattribution, intellectual property, and ethical AI use. Stylometry has been\nused extensively to characterise the style and attribute authorship of texts.\nBy applying it to LLM-generated texts, we identify their emergent writing\npatterns. The paper involves creating a benchmark dataset based on Wikipedia,\nwith (a) human-written term summaries, (b) texts generated purely by LLMs\n(GPT-3.5/4, LLaMa 2/3, Orca, and Falcon), (c) processed through multiple text\nsummarisation methods (T5, BART, Gensim, and Sumy), and (d) rephrasing methods\n(Dipper, T5). The 10-sentence long texts were classified by tree-based models\n(decision trees and LightGBM) using human-designed (StyloMetrix) and\nn-gram-based (our own pipeline) stylometric features that encode lexical,\ngrammatical, syntactic, and punctuation patterns. The cross-validated results\nreached a performance of up to .87 Matthews correlation coefficient in the\nmulticlass scenario with 7 classes, and accuracy between .79 and 1. in binary\nclassification, with the particular example of Wikipedia and GPT-4 reaching up\nto .98 accuracy on a balanced dataset. Shapley Additive Explanations pinpointed\nfeatures characteristic of the encyclopaedic text type, individual overused\nwords, as well as a greater grammatical standardisation of LLMs with respect to\nhuman-written texts. These results show -- crucially, in the context of the\nincreasingly sophisticated LLMs -- that it is possible to distinguish machine-\nfrom human-generated texts at least for a well-defined text type.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00838v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2507.00838v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00937", "title": "RaGNNarok: A Light-Weight Graph Neural Network for Enhancing Radar Point Clouds on Unmanned Ground Vehicles", "authors": ["David Hunt", "Shaocheng Luo", "Spencer Hallyburton", "Shafii Nillongo", "Yi Li", "Tingjun Chen", "Miroslav Pajic"], "summary": "Low-cost indoor mobile robots have gained popularity with the increasing\nadoption of automation in homes and commercial spaces. However, existing lidar\nand camera-based solutions have limitations such as poor performance in\nvisually obscured environments, high computational overhead for data\nprocessing, and high costs for lidars. In contrast, mmWave radar sensors offer\na cost-effective and lightweight alternative, providing accurate ranging\nregardless of visibility. However, existing radar-based localization suffers\nfrom sparse point cloud generation, noise, and false detections. Thus, in this\nwork, we introduce RaGNNarok, a real-time, lightweight, and generalizable graph\nneural network (GNN)-based framework to enhance radar point clouds, even in\ncomplex and dynamic environments. With an inference time of just 7.3 ms on the\nlow-cost Raspberry Pi 5, RaGNNarok runs efficiently even on such\nresource-constrained devices, requiring no additional computational resources.\nWe evaluate its performance across key tasks, including localization, SLAM, and\nautonomous navigation, in three different environments. Our results demonstrate\nstrong reliability and generalizability, making RaGNNarok a robust solution for\nlow-cost indoor mobile robots.", "comment": "8 pages, accepted by IROS 2025", "pdf_url": "http://arxiv.org/pdf/2507.00937v1", "categories": ["cs.RO", "cs.AR", "cs.CV", "cs.LG"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2507.00937v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00880", "title": "NN-Former: Rethinking Graph Structure in Neural Architecture Representation", "authors": ["Ruihan Xu", "Haokui Zhang", "Yaowei Wang", "Wei Zeng", "Shiliang Zhang"], "summary": "The growing use of deep learning necessitates efficient network design and\ndeployment, making neural predictors vital for estimating attributes such as\naccuracy and latency. Recently, Graph Neural Networks (GNNs) and transformers\nhave shown promising performance in representing neural architectures. However,\neach of both methods has its disadvantages. GNNs lack the capabilities to\nrepresent complicated features, while transformers face poor generalization\nwhen the depth of architecture grows. To mitigate the above issues, we rethink\nneural architecture topology and show that sibling nodes are pivotal while\noverlooked in previous research. We thus propose a novel predictor leveraging\nthe strengths of GNNs and transformers to learn the enhanced topology. We\nintroduce a novel token mixer that considers siblings, and a new channel mixer\nnamed bidirectional graph isomorphism feed-forward network. Our approach\nconsistently achieves promising performance in both accuracy and latency\nprediction, providing valuable insights for learning Directed Acyclic Graph\n(DAG) topology. The code is available at https://github.com/XuRuihan/NNFormer.", "comment": "Accepted to CVPR 2025. Code is avaiable at\n  https://github.com/XuRuihan/NNFormer", "pdf_url": "http://arxiv.org/pdf/2507.00880v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00880v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00950", "title": "MVP: Winning Solution to SMP Challenge 2025 Video Track", "authors": ["Liliang Ye", "Yunyao Zhang", "Yafeng Wu", "Yi-Ping Phoebe Chen", "Junqing Yu", "Wei Yang", "Zikai Song"], "summary": "Social media platforms serve as central hubs for content dissemination,\nopinion expression, and public engagement across diverse modalities. Accurately\npredicting the popularity of social media videos enables valuable applications\nin content recommendation, trend detection, and audience engagement. In this\npaper, we present Multimodal Video Predictor (MVP), our winning solution to the\nVideo Track of the SMP Challenge 2025. MVP constructs expressive post\nrepresentations by integrating deep video features extracted from pretrained\nmodels with user metadata and contextual information. The framework applies\nsystematic preprocessing techniques, including log-transformations and outlier\nremoval, to improve model robustness. A gradient-boosted regression model is\ntrained to capture complex patterns across modalities. Our approach ranked\nfirst in the official evaluation of the Video Track, demonstrating its\neffectiveness and reliability for multimodal video popularity prediction on\nsocial platforms. The source code is available at\nhttps://anonymous.4open.science/r/SMPDVideo.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00950v1", "categories": ["cs.CV", "cs.LG", "cs.MM"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00950v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00891", "title": "MemeCMD: An Automatically Generated Chinese Multi-turn Dialogue Dataset with Contextually Retrieved Memes", "authors": ["Yuheng Wang", "Xianhe Tang", "Pufeng Huang"], "summary": "Memes are widely used in online social interactions, providing vivid,\nintuitive, and often humorous means to express intentions and emotions.\nExisting dialogue datasets are predominantly limited to either manually\nannotated or pure-text conversations, lacking the expressiveness and contextual\nnuance that multimodal interactions provide.To address these challenges, we\nintroduce MemeCMD, an automatically generated Chinese Multi-turn Dialogue\ndataset with contextually retrieved memes. Our dataset combines a large-scale,\nMLLM-annotated meme library with dialogues auto-generated by dual agents across\ndiverse scenarios. We introduce a retrieval framework and adaptive threshold to\nensure contextually relevant, naturally spaced meme usage. Experiments\ndemonstrate the effectiveness of our approach in generating contextually\nappropriate and diverse meme-incorporated dialogues, offering a scalable and\nprivacy-preserving resource for advancing multimodal conversational AI.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00891v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2507.00891v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00957", "title": "Atmospheric model-trained machine learning selection and classification of ultracool TY dwarfs", "authors": ["Ankit Biswas"], "summary": "The T and Y spectral classes represent the coolest and lowest-mass population\nof brown dwarfs, yet their census remains incomplete due to limited statistics.\nExisting detection frameworks are often constrained to identifying M, L, and\nearly T dwarfs, owing to the sparse observational sample of ultracool dwarfs\n(UCDs) at later types. This paper presents a novel machine learning framework\ncapable of detecting and classifying late-T and Y dwarfs, trained entirely on\nsynthetic photometry from atmospheric models. Utilizing grids from the ATMO\n2020 and Sonora Bobcat models, I produce a training dataset over two orders of\nmagnitude larger than any empirical set of >T6 UCDs. Polynomial color relations\nfitted to the model photometry are used to assign spectral types to these\nsynthetic models, which in turn train an ensemble of classifiers to identify\nand classify the spectral type of late UCDs. The model is highly performant\nwhen validating on both synthetic and empirical datasets, verifying catalogs of\nknown UCDs with object classification metrics >99% and an average spectral type\nprecision within 0.35 +/- 0.37 subtypes. Application of the model to a 1.5\ndegree region around Pisces and the UKIDSS UDS field results in the discovery\nof one previously uncatalogued T8.2 candidate, demonstrating the ability of\nthis model-trained approach in discovering faint, late-type UCDs from\nphotometric catalogs.", "comment": "12 pages, 9 figures, to be published in Monthly Notices of the Royal\n  Astronomical Society", "pdf_url": "http://arxiv.org/pdf/2507.00957v1", "categories": ["astro-ph.SR", "astro-ph.EP", "astro-ph.IM", "cs.LG"], "cate": "astro-ph.SR", "url": "http://arxiv.org/abs/2507.00957v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00902", "title": "Constellation as a Service: Tailored Connectivity Management in Direct-Satellite-to-Device Networks", "authors": ["Feng Wang", "Shengyu Zhang", "Een-Kee Hong", "Tony Q. S. Quek"], "summary": "Direct-satellite-to-device (DS2D) communication is emerging as a promising\nsolution for global mobile service extension, leveraging the deployment of\nsatellite constellations. However, the challenge of managing DS2D connectivity\nfor multi-constellations becomes outstanding, including high interference and\nfrequent handovers caused by multi-coverage overlap and rapid satellite\nmovement. Moreover, existing approaches primarily operate within\nsingle-constellation shell, which inherently limits the ability to exploit the\nvast potential of multi-constellation connectivity provision, resulting in\nsuboptimal DS2D service performances. To address these challenges, this article\nproposes a Constellation as a Service (CaaS) framework, which treats the entire\nmulti-constellation infrastructure as a shared resource pool and dynamically\nforms optimal sub-constellations (SCs) for each DS2D service region. The\nformation of each SC integrates satellites from various orbits to provide\ntailored connectivity based on user demands, guided by two innovative\nstrategies: predictive satellite beamforming using generative artificial\nintelligence (GenAI) and pre-configured handover path for efficient satellite\naccess and mobility management. Simulation results demonstrate that CaaS\nsignificantly improves satellite service rates while reducing handover\noverhead, making it an efficient and continuable solution for managing DS2D\nconnectivity in multi-constellation environments.", "comment": "To appear in IEEE Communications Magazine", "pdf_url": "http://arxiv.org/pdf/2507.00902v1", "categories": ["eess.SY", "cs.AI", "cs.SY", "eess.SP"], "cate": "eess.SY", "url": "http://arxiv.org/abs/2507.00902v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00979", "title": "Enhancing LLM Agent Safety via Causal Influence Prompting", "authors": ["Dongyoon Hahm", "Woogyeol Jin", "June Suk Choi", "Sungsoo Ahn", "Kimin Lee"], "summary": "As autonomous agents powered by large language models (LLMs) continue to\ndemonstrate potential across various assistive tasks, ensuring their safe and\nreliable behavior is crucial for preventing unintended consequences. In this\nwork, we introduce CIP, a novel technique that leverages causal influence\ndiagrams (CIDs) to identify and mitigate risks arising from agent\ndecision-making. CIDs provide a structured representation of cause-and-effect\nrelationships, enabling agents to anticipate harmful outcomes and make safer\ndecisions. Our approach consists of three key steps: (1) initializing a CID\nbased on task specifications to outline the decision-making process, (2)\nguiding agent interactions with the environment using the CID, and (3)\niteratively refining the CID based on observed behaviors and outcomes.\nExperimental results demonstrate that our method effectively enhances safety in\nboth code execution and mobile device control tasks.", "comment": "Accepted at ACL 2025 Findings, Source code:\n  https://github.com/HahmDY/causal_influence_prompting.git", "pdf_url": "http://arxiv.org/pdf/2507.00979v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2507.00979v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00903", "title": "Deep learning-based segmentation of T1 and T2 cardiac MRI maps for automated disease detection", "authors": ["Andreea Bianca Popescu", "Andreas Seitz", "Heiko Mahrholdt", "Jens Wetzl", "Athira Jacob", "Lucian Mihai Itu", "Constantin Suciu", "Teodora Chitiboi"], "summary": "Objectives Parametric tissue mapping enables quantitative cardiac tissue\ncharacterization but is limited by inter-observer variability during manual\ndelineation. Traditional approaches relying on average relaxation values and\nsingle cutoffs may oversimplify myocardial complexity. This study evaluates\nwhether deep learning (DL) can achieve segmentation accuracy comparable to\ninter-observer variability, explores the utility of statistical features beyond\nmean T1/T2 values, and assesses whether machine learning (ML) combining\nmultiple features enhances disease detection. Materials & Methods T1 and T2\nmaps were manually segmented. The test subset was independently annotated by\ntwo observers, and inter-observer variability was assessed. A DL model was\ntrained to segment left ventricle blood pool and myocardium. Average (A), lower\nquartile (LQ), median (M), and upper quartile (UQ) were computed for the\nmyocardial pixels and employed in classification by applying cutoffs or in ML.\nDice similarity coefficient (DICE) and mean absolute percentage error evaluated\nsegmentation performance. Bland-Altman plots assessed inter-user and\nmodel-observer agreement. Receiver operating characteristic analysis determined\noptimal cutoffs. Pearson correlation compared features from model and manual\nsegmentations. F1-score, precision, and recall evaluated classification\nperformance. Wilcoxon test assessed differences between classification methods,\nwith p < 0.05 considered statistically significant. Results 144 subjects were\nsplit into training (100), validation (15) and evaluation (29) subsets.\nSegmentation model achieved a DICE of 85.4%, surpassing inter-observer\nagreement. Random forest applied to all features increased F1-score (92.7%, p <\n0.001). Conclusion DL facilitates segmentation of T1/ T2 maps. Combining\nmultiple features with ML improves disease detection.", "comment": "This work has been submitted for consideration at European Radiology\n  (Springer). Upon acceptance, this preprint will be updated with the journal\n  reference", "pdf_url": "http://arxiv.org/pdf/2507.00903v1", "categories": ["eess.IV", "cs.AI", "cs.CV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2507.00903v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00984", "title": "Box Pose and Shape Estimation and Domain Adaptation for Large-Scale Warehouse Automation", "authors": ["Xihang Yu", "Rajat Talak", "Jingnan Shi", "Ulrich Viereck", "Igor Gilitschenski", "Luca Carlone"], "summary": "Modern warehouse automation systems rely on fleets of intelligent robots that\ngenerate vast amounts of data -- most of which remains unannotated. This paper\ndevelops a self-supervised domain adaptation pipeline that leverages\nreal-world, unlabeled data to improve perception models without requiring\nmanual annotations. Our work focuses specifically on estimating the pose and\nshape of boxes and presents a correct-and-certify pipeline for self-supervised\nbox pose and shape estimation. We extensively evaluate our approach across a\nrange of simulated and real industrial settings, including adaptation to a\nlarge-scale real-world dataset of 50,000 images. The self-supervised model\nsignificantly outperforms models trained solely in simulation and shows\nsubstantial improvements over a zero-shot 3D bounding box estimation baseline.", "comment": "12 pages, 6 figures. This work will be presented at the 19th\n  International Symposium on Experimental Robotics (ISER2025)", "pdf_url": "http://arxiv.org/pdf/2507.00984v1", "categories": ["cs.RO", "cs.CV", "cs.LG"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2507.00984v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00907", "title": "The Age of Sensorial Zero Trust: Why We Can No Longer Trust Our Senses", "authors": ["Fabio Correa Xavier"], "summary": "In a world where deepfakes and cloned voices are emerging as sophisticated\nattack vectors, organizations require a new security mindset: Sensorial Zero\nTrust [9]. This article presents a scientific analysis of the need to\nsystematically doubt information perceived through the senses, establishing\nrigorous verification protocols to mitigate the risks of fraud based on\ngenerative artificial intelligence. Key concepts, such as Out-of-Band\nverification, Vision-Language Models (VLMs) as forensic collaborators,\ncryptographic provenance, and human training, are integrated into a framework\nthat extends Zero Trust principles to human sensory information. The approach\nis grounded in empirical findings and academic research, emphasizing that in an\nera of AI-generated realities, even our eyes and ears can no longer be\nimplicitly trusted without verification. Leaders are called to foster a culture\nof methodological skepticism to protect organizational integrity in this new\nthreat landscape.", "comment": "14 pages", "pdf_url": "http://arxiv.org/pdf/2507.00907v1", "categories": ["cs.CR", "cs.AI", "68T07, 68T45, 94A60", "K.6.5; D.4.6; I.2.6"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2507.00907v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.01006", "title": "GLM-4.1V-Thinking: Towards Versatile Multimodal Reasoning with Scalable Reinforcement Learning", "authors": ["Wenyi Hong", "Wenmeng Yu", "Xiaotao Gu", "Guo Wang", "Guobing Gan", "Haomiao Tang", "Jiale Cheng", "Ji Qi", "Junhui Ji", "Lihang Pan", "Shuaiqi Duan", "Weihan Wang", "Yan Wang", "Yean Cheng", "Zehai He", "Zhe Su", "Zhen Yang", "Ziyang Pan", "Aohan Zeng", "Baoxu Wang", "Boyan Shi", "Changyu Pang", "Chenhui Zhang", "Da Yin", "Fan Yang", "Guoqing Chen", "Jiazheng Xu", "Jiali Chen", "Jing Chen", "Jinhao Chen", "Jinghao Lin", "Jinjiang Wang", "Junjie Chen", "Leqi Lei", "Leyi Pan", "Mingzhi Zhang", "Qinkai Zheng", "Sheng Yang", "Shi Zhong", "Shiyu Huang", "Shuyuan Zhao", "Siyan Xue", "Shangqin Tu", "Shengbiao Meng", "Tianshu Zhang", "Tianwei Luo", "Tianxiang Hao", "Tianle Gong", "Wenkai Li", "Wei Jia", "Xin Lyu", "Xuancheng Huang", "Yanling Wang", "Yadong Xue", "Yanfeng Wang", "Yifan An", "Yifan Du", "Yiming Shi", "Yiheng Huang", "Yilin Niu", "Yuan Wang", "Yuanchang Yue", "Yuchen Li", "Yutao Zhang", "Yuxuan Zhang", "Zhanxiao Du", "Zhenyu Hou", "Zhao Xue", "Zhengxiao Du", "Zihan Wang", "Peng Zhang", "Debing Liu", "Bin Xu", "Juanzi Li", "Minlie Huang", "Yuxiao Dong", "Jie Tang"], "summary": "We present GLM-4.1V-Thinking, a vision-language model (VLM) designed to\nadvance general-purpose multimodal reasoning. In this report, we share our key\nfindings in the development of the reasoning-centric training framework. We\nfirst develop a capable vision foundation model with significant potential\nthrough large-scale pre-training, which arguably sets the upper bound for the\nfinal performance. Reinforcement Learning with Curriculum Sampling (RLCS) then\nunlocks the full potential of the model, leading to comprehensive capability\nenhancement across a diverse range of tasks, including STEM problem solving,\nvideo understanding, content recognition, coding, grounding, GUI-based agents,\nand long document understanding, among others. To facilitate research in this\nfield, we open-source GLM-4.1V-9B-Thinking, which achieves state-of-the-art\nperformance among models of comparable size. In a comprehensive evaluation\nacross 28 public benchmarks, our model outperforms Qwen2.5-VL-7B on nearly all\ntasks and achieves comparable or even superior performance on 18 benchmarks\nrelative to the significantly larger Qwen2.5-VL-72B. Notably,\nGLM-4.1V-9B-Thinking also demonstrates competitive or superior performance\ncompared to closed-source models such as GPT-4o on challenging tasks including\nlong document understanding and STEM reasoning, further underscoring its strong\ncapabilities. Code, models and more information are released at\nhttps://github.com/THUDM/GLM-4.1V-Thinking.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.01006v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.01006v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00909", "title": "Turning AI Data Centers into Grid-Interactive Assets: Results from a Field Demonstration in Phoenix, Arizona", "authors": ["Philip Colangelo", "Ayse K. Coskun", "Jack Megrue", "Ciaran Roberts", "Shayan Sengupta", "Varun Sivaram", "Ethan Tiao", "Aroon Vijaykar", "Chris Williams", "Daniel C. Wilson", "Zack MacFarland", "Daniel Dreiling", "Nathan Morey", "Anuja Ratnayake", "Baskar Vairamohan"], "summary": "Artificial intelligence (AI) is fueling exponential electricity demand\ngrowth, threatening grid reliability, raising prices for communities paying for\nnew energy infrastructure, and stunting AI innovation as data centers wait for\ninterconnection to constrained grids. This paper presents the first field\ndemonstration, in collaboration with major corporate partners, of a\nsoftware-only approach--Emerald Conductor--that transforms AI data centers into\nflexible grid resources that can efficiently and immediately harness existing\npower systems without massive infrastructure buildout. Conducted at a 256-GPU\ncluster running representative AI workloads within a commercial, hyperscale\ncloud data center in Phoenix, Arizona, the trial achieved a 25% reduction in\ncluster power usage for three hours during peak grid events while maintaining\nAI quality of service (QoS) guarantees. By orchestrating AI workloads based on\nreal-time grid signals without hardware modifications or energy storage, this\nplatform reimagines data centers as grid-interactive assets that enhance grid\nreliability, advance affordability, and accelerate AI's development.", "comment": "10 pages, 6 figures, 1 table", "pdf_url": "http://arxiv.org/pdf/2507.00909v1", "categories": ["cs.DC", "cs.AI", "cs.PF", "cs.SY", "eess.SY"], "cate": "cs.DC", "url": "http://arxiv.org/abs/2507.00909v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00914", "title": "Large Language Model Powered Intelligent Urban Agents: Concepts, Capabilities, and Applications", "authors": ["Jindong Han", "Yansong Ning", "Zirui Yuan", "Hang Ni", "Fan Liu", "Tengfei Lyu", "Hao Liu"], "summary": "The long-standing vision of intelligent cities is to create efficient,\nlivable, and sustainable urban environments using big data and artificial\nintelligence technologies. Recently, the advent of Large Language Models (LLMs)\nhas opened new ways toward realizing this vision. With powerful semantic\nunderstanding and reasoning capabilities, LLMs can be deployed as intelligent\nagents capable of autonomously solving complex problems across domains. In this\narticle, we focus on Urban LLM Agents, which are LLM-powered agents that are\nsemi-embodied within the hybrid cyber-physical-social space of cities and used\nfor system-level urban decision-making. First, we introduce the concept of\nurban LLM agents, discussing their unique capabilities and features. Second, we\nsurvey the current research landscape from the perspective of agent workflows,\nencompassing urban sensing, memory management, reasoning, execution, and\nlearning. Third, we categorize the application domains of urban LLM agents into\nfive groups: urban planning, transportation, environment, public safety, and\nurban society, presenting representative works in each group. Finally, we\ndiscuss trustworthiness and evaluation issues that are critical for real-world\ndeployment, and identify several open problems for future research. This survey\naims to establish a foundation for the emerging field of urban LLM agents and\nto provide a roadmap for advancing the intersection of LLMs and urban\nintelligence. A curated list of relevant papers and open-source resources is\nmaintained and continuously updated at\nhttps://github.com/usail-hkust/Awesome-Urban-LLM-Agents.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00914v1", "categories": ["cs.MA", "cs.AI"], "cate": "cs.MA", "url": "http://arxiv.org/abs/2507.00914v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00938", "title": "WebArXiv: Evaluating Multimodal Agents on Time-Invariant arXiv Tasks", "authors": ["Zihao Sun", "Meng Fang", "Ling Chen"], "summary": "Recent progress in large language models (LLMs) has enabled the development\nof autonomous web agents capable of navigating and interacting with real\nwebsites. However, evaluating such agents remains challenging due to the\ninstability and inconsistency of existing benchmarks, which often rely on\ndynamic content or oversimplified simulations. In this work, we introduce\nWebArXiv, a static and time-invariant benchmark comprising 275 web-based tasks\ngrounded in the arXiv platform. WebArXiv ensures reproducible and reliable\nevaluation by anchoring tasks in fixed web snapshots with deterministic ground\ntruths and standardized action trajectories. Through behavioral analysis, we\nidentify a common failure mode, Rigid History Reflection, where agents\nover-rely on fixed interaction histories. To address this, we propose a\nlightweight dynamic reflection mechanism that allows agents to selectively\nretrieve relevant past steps during decision-making. We evaluate ten\nstate-of-the-art web agents on WebArXiv. Results demonstrate clear performance\ndifferences across agents and validate the effectiveness of our proposed\nreflection strategy.", "comment": "10 pages, 9 figures, 4 tables", "pdf_url": "http://arxiv.org/pdf/2507.00938v1", "categories": ["cs.IR", "cs.AI", "cs.DB", "F.2.2; I.2.7"], "cate": "cs.IR", "url": "http://arxiv.org/abs/2507.00938v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00953", "title": "From Sentences to Sequences: Rethinking Languages in Biological System", "authors": ["Ke Liu", "Shuanke Shen", "Hao Chen"], "summary": "The paradigm of large language models in natural language processing (NLP)\nhas also shown promise in modeling biological languages, including proteins,\nRNA, and DNA. Both the auto-regressive generation paradigm and evaluation\nmetrics have been transferred from NLP to biological sequence modeling.\nHowever, the intrinsic structural correlations in natural and biological\nlanguages differ fundamentally. Therefore, we revisit the notion of language in\nbiological systems to better understand how NLP successes can be effectively\ntranslated to biological domains. By treating the 3D structure of biomolecules\nas the semantic content of a sentence and accounting for the strong\ncorrelations between residues or bases, we highlight the importance of\nstructural evaluation and demonstrate the applicability of the auto-regressive\nparadigm in biological language modeling. Code can be found at\n\\href{https://github.com/zjuKeLiu/RiFold}{github.com/zjuKeLiu/RiFold}", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00953v1", "categories": ["q-bio.BM", "cs.AI"], "cate": "q-bio.BM", "url": "http://arxiv.org/abs/2507.00953v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00966", "title": "MambAttention: Mamba with Multi-Head Attention for Generalizable Single-Channel Speech Enhancement", "authors": ["Nikolai Lund Kühne", "Jesper Jensen", "Jan Østergaard", "Zheng-Hua Tan"], "summary": "With the advent of new sequence models like Mamba and xLSTM, several studies\nhave shown that these models match or outperform state-of-the-art models in\nsingle-channel speech enhancement, automatic speech recognition, and\nself-supervised audio representation learning. However, prior research has\ndemonstrated that sequence models like LSTM and Mamba tend to overfit to the\ntraining set. To address this issue, previous works have shown that adding\nself-attention to LSTMs substantially improves generalization performance for\nsingle-channel speech enhancement. Nevertheless, neither the concept of hybrid\nMamba and time-frequency attention models nor their generalization performance\nhave been explored for speech enhancement. In this paper, we propose a novel\nhybrid architecture, MambAttention, which combines Mamba and shared time- and\nfrequency-multi-head attention modules for generalizable single-channel speech\nenhancement. To train our model, we introduce VoiceBank+Demand Extended\n(VB-DemandEx), a dataset inspired by VoiceBank+Demand but with more challenging\nnoise types and lower signal-to-noise ratios. Trained on VB-DemandEx, our\nproposed MambAttention model significantly outperforms existing\nstate-of-the-art LSTM-, xLSTM-, Mamba-, and Conformer-based systems of similar\ncomplexity across all reported metrics on two out-of-domain datasets: DNS 2020\nand EARS-WHAM_v2, while matching their performance on the in-domain dataset\nVB-DemandEx. Ablation studies highlight the role of weight sharing between the\ntime- and frequency-multi-head attention modules for generalization\nperformance. Finally, we explore integrating the shared time- and\nfrequency-multi-head attention modules with LSTM and xLSTM, which yields a\nnotable performance improvement on the out-of-domain datasets. However, our\nMambAttention model remains superior on both out-of-domain datasets across all\nreported evaluation metrics.", "comment": "Submitted to IEEE/ACM Transactions on Audio, Speech, and Language\n  Processing for possible publication", "pdf_url": "http://arxiv.org/pdf/2507.00966v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "cate": "cs.SD", "url": "http://arxiv.org/abs/2507.00966v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00969", "title": "Surgical Neural Radiance Fields from One Image", "authors": ["Alberto Neri", "Maximilan Fehrentz", "Veronica Penza", "Leonardo S. Mattos", "Nazim Haouchine"], "summary": "Purpose: Neural Radiance Fields (NeRF) offer exceptional capabilities for 3D\nreconstruction and view synthesis, yet their reliance on extensive multi-view\ndata limits their application in surgical intraoperative settings where only\nlimited data is available. In particular, collecting such extensive data\nintraoperatively is impractical due to time constraints. This work addresses\nthis challenge by leveraging a single intraoperative image and preoperative\ndata to train NeRF efficiently for surgical scenarios.\n  Methods: We leverage preoperative MRI data to define the set of camera\nviewpoints and images needed for robust and unobstructed training.\nIntraoperatively, the appearance of the surgical image is transferred to the\npre-constructed training set through neural style transfer, specifically\ncombining WTC2 and STROTSS to prevent over-stylization. This process enables\nthe creation of a dataset for instant and fast single-image NeRF training.\n  Results: The method is evaluated with four clinical neurosurgical cases.\nQuantitative comparisons to NeRF models trained on real surgical microscope\nimages demonstrate strong synthesis agreement, with similarity metrics\nindicating high reconstruction fidelity and stylistic alignment. When compared\nwith ground truth, our method demonstrates high structural similarity,\nconfirming good reconstruction quality and texture preservation.\n  Conclusion: Our approach demonstrates the feasibility of single-image NeRF\ntraining in surgical settings, overcoming the limitations of traditional\nmulti-view methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00969v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.00969v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00971", "title": "Reasoning as an Adaptive Defense for Safety", "authors": ["Taeyoun Kim", "Fahim Tajwar", "Aditi Raghunathan", "Aviral Kumar"], "summary": "Reasoning methods that adaptively allocate test-time compute have advanced\nLLM performance on easy to verify domains such as math and code. In this work,\nwe study how to utilize this approach to train models that exhibit a degree of\nrobustness to safety vulnerabilities, and show that doing so can provide\nbenefits. We build a recipe called $\\textit{TARS}$ (Training Adaptive Reasoners\nfor Safety), a reinforcement learning (RL) approach that trains models to\nreason about safety using chain-of-thought traces and a reward signal that\nbalances safety with task completion. To build TARS, we identify three critical\ndesign choices: (1) a \"lightweight\" warmstart SFT stage, (2) a mix of harmful,\nharmless, and ambiguous prompts to prevent shortcut behaviors such as too many\nrefusals, and (3) a reward function to prevent degeneration of reasoning\ncapabilities during training. Models trained with TARS exhibit adaptive\nbehaviors by spending more compute on ambiguous queries, leading to better\nsafety-refusal trade-offs. They also internally learn to better distinguish\nbetween safe and unsafe prompts and attain greater robustness to both white-box\n(e.g., GCG) and black-box attacks (e.g., PAIR). Overall, our work provides an\neffective, open recipe for training LLMs against jailbreaks and harmful\nrequests by reasoning per prompt.", "comment": "42 pages, 11 Figures, 7 Tables", "pdf_url": "http://arxiv.org/pdf/2507.00971v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.00971v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.00990", "title": "Robotic Manipulation by Imitating Generated Videos Without Physical Demonstrations", "authors": ["Shivansh Patel", "Shraddhaa Mohan", "Hanlin Mai", "Unnat Jain", "Svetlana Lazebnik", "Yunzhu Li"], "summary": "This work introduces Robots Imitating Generated Videos (RIGVid), a system\nthat enables robots to perform complex manipulation tasks--such as pouring,\nwiping, and mixing--purely by imitating AI-generated videos, without requiring\nany physical demonstrations or robot-specific training. Given a language\ncommand and an initial scene image, a video diffusion model generates potential\ndemonstration videos, and a vision-language model (VLM) automatically filters\nout results that do not follow the command. A 6D pose tracker then extracts\nobject trajectories from the video, and the trajectories are retargeted to the\nrobot in an embodiment-agnostic fashion. Through extensive real-world\nevaluations, we show that filtered generated videos are as effective as real\ndemonstrations, and that performance improves with generation quality. We also\nshow that relying on generated videos outperforms more compact alternatives\nsuch as keypoint prediction using VLMs, and that strong 6D pose tracking\noutperforms other ways to extract trajectories, such as dense feature point\ntracking. These findings suggest that videos produced by a state-of-the-art\noff-the-shelf model can offer an effective source of supervision for robotic\nmanipulation.", "comment": "Project Page: https://rigvid-robot.github.io/", "pdf_url": "http://arxiv.org/pdf/2507.00990v1", "categories": ["cs.RO", "cs.AI", "cs.CV"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2507.00990v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.01001", "title": "SciArena: An Open Evaluation Platform for Foundation Models in Scientific Literature Tasks", "authors": ["Yilun Zhao", "Kaiyan Zhang", "Tiansheng Hu", "Sihong Wu", "Ronan Le Bras", "Taira Anderson", "Jonathan Bragg", "Joseph Chee Chang", "Jesse Dodge", "Matt Latzke", "Yixin Liu", "Charles McGrady", "Xiangru Tang", "Zihang Wang", "Chen Zhao", "Hannaneh Hajishirzi", "Doug Downey", "Arman Cohan"], "summary": "We present SciArena, an open and collaborative platform for evaluating\nfoundation models on scientific literature tasks. Unlike traditional benchmarks\nfor scientific literature understanding and synthesis, SciArena engages the\nresearch community directly, following the Chatbot Arena evaluation approach of\ncommunity voting on model comparisons. By leveraging collective intelligence,\nSciArena offers a community-driven evaluation of model performance on\nopen-ended scientific tasks that demand literature-grounded, long-form\nresponses. The platform currently supports 23 open-source and proprietary\nfoundation models and has collected over 13,000 votes from trusted researchers\nacross diverse scientific domains. We analyze the data collected so far and\nconfirm that the submitted questions are diverse, aligned with real-world\nliterature needs, and that participating researchers demonstrate strong\nself-consistency and inter-annotator agreement in their evaluations. We discuss\nthe results and insights based on the model ranking leaderboard. To further\npromote research in building model-based automated evaluation systems for\nliterature tasks, we release SciArena-Eval, a meta-evaluation benchmark based\non our collected preference data. The benchmark measures the accuracy of models\nin judging answer quality by comparing their pairwise assessments with human\nvotes. Our experiments highlight the benchmark's challenges and emphasize the\nneed for more reliable automated evaluation methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.01001v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2507.01001v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.01003", "title": "Description of the Training Process of Neural Networks via Ergodic Theorem : Ghost nodes", "authors": ["Eun-Ji Park", "Sangwon Yun"], "summary": "Recent studies have proposed interpreting the training process from an\nergodic perspective. Building on this foundation we present a unified framework\nfor understanding and accelerating the training of deep neural networks via\nstochastic gradient descent. By analyzing the geometric landscape of the\nobjective function we introduce a practical diagnostic, the running estimate of\nthe largest Lyapunov exponent, which provably distinguishes genuine convergence\ntoward stable minimizers from mere statistical stabilization near saddle\npoints. We then propose a ghost category extension for standard classifiers\nthat adds auxiliary ghost output nodes so the model gains extra descent\ndirections that open a lateral corridor around narrow loss barriers and enable\nthe optimizer to bypass poor basins during the early training phase. We show\nthat this extension strictly reduces approximation error and that after\nsufficient convergence the ghost dimensions collapse and the extended model's\ninvariant law coincides with that of the original and there exists a path in\nthe enlarged parameter space along which the total loss does not increase while\nthe original loss decreases by an arbitrary margin. Taken together these\nresults provide a principled architecture level intervention that accelerates\nearly stage trainability while preserving asymptotic behavior.", "comment": "9 pages, 2 figures", "pdf_url": "http://arxiv.org/pdf/2507.01003v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2507.01003v1", "date": "2025-07-01", "updated": "2025-07-01"}
{"id": "2507.01006", "title": "GLM-4.1V-Thinking: Towards Versatile Multimodal Reasoning with Scalable Reinforcement Learning", "authors": ["Wenyi Hong", "Wenmeng Yu", "Xiaotao Gu", "Guo Wang", "Guobing Gan", "Haomiao Tang", "Jiale Cheng", "Ji Qi", "Junhui Ji", "Lihang Pan", "Shuaiqi Duan", "Weihan Wang", "Yan Wang", "Yean Cheng", "Zehai He", "Zhe Su", "Zhen Yang", "Ziyang Pan", "Aohan Zeng", "Baoxu Wang", "Boyan Shi", "Changyu Pang", "Chenhui Zhang", "Da Yin", "Fan Yang", "Guoqing Chen", "Jiazheng Xu", "Jiali Chen", "Jing Chen", "Jinhao Chen", "Jinghao Lin", "Jinjiang Wang", "Junjie Chen", "Leqi Lei", "Leyi Pan", "Mingzhi Zhang", "Qinkai Zheng", "Sheng Yang", "Shi Zhong", "Shiyu Huang", "Shuyuan Zhao", "Siyan Xue", "Shangqin Tu", "Shengbiao Meng", "Tianshu Zhang", "Tianwei Luo", "Tianxiang Hao", "Tianle Gong", "Wenkai Li", "Wei Jia", "Xin Lyu", "Xuancheng Huang", "Yanling Wang", "Yadong Xue", "Yanfeng Wang", "Yifan An", "Yifan Du", "Yiming Shi", "Yiheng Huang", "Yilin Niu", "Yuan Wang", "Yuanchang Yue", "Yuchen Li", "Yutao Zhang", "Yuxuan Zhang", "Zhanxiao Du", "Zhenyu Hou", "Zhao Xue", "Zhengxiao Du", "Zihan Wang", "Peng Zhang", "Debing Liu", "Bin Xu", "Juanzi Li", "Minlie Huang", "Yuxiao Dong", "Jie Tang"], "summary": "We present GLM-4.1V-Thinking, a vision-language model (VLM) designed to\nadvance general-purpose multimodal reasoning. In this report, we share our key\nfindings in the development of the reasoning-centric training framework. We\nfirst develop a capable vision foundation model with significant potential\nthrough large-scale pre-training, which arguably sets the upper bound for the\nfinal performance. Reinforcement Learning with Curriculum Sampling (RLCS) then\nunlocks the full potential of the model, leading to comprehensive capability\nenhancement across a diverse range of tasks, including STEM problem solving,\nvideo understanding, content recognition, coding, grounding, GUI-based agents,\nand long document understanding, among others. To facilitate research in this\nfield, we open-source GLM-4.1V-9B-Thinking, which achieves state-of-the-art\nperformance among models of comparable size. In a comprehensive evaluation\nacross 28 public benchmarks, our model outperforms Qwen2.5-VL-7B on nearly all\ntasks and achieves comparable or even superior performance on 18 benchmarks\nrelative to the significantly larger Qwen2.5-VL-72B. Notably,\nGLM-4.1V-9B-Thinking also demonstrates competitive or superior performance\ncompared to closed-source models such as GPT-4o on challenging tasks including\nlong document understanding and STEM reasoning, further underscoring its strong\ncapabilities. Code, models and more information are released at\nhttps://github.com/THUDM/GLM-4.1V-Thinking.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.01006v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2507.01006v1", "date": "2025-07-01", "updated": "2025-07-01"}
