{"id": "2506.22677", "title": "Prediction of Protein Three-dimensional Structures via a Hardware-Executable Quantum Computing Framework", "authors": ["Yuqi Zhang", "Yuxin Yang", "William Martin", "Kingsten Lin", "Zixu Wang", "Cheng-Chang Lu", "Weiwen Jiang", "Ruth Nussinov", "Joseph Loscalzo", "Qiang Guan", "Feixiong Cheng"], "summary": "Accurate prediction of protein active site structures remains a central\nchallenge in structural biology, particularly for short and flexible peptide\nfragments where conventional methods often fail. Here, we present a quantum\ncomputing framework specifically developed for utility-level quantum processors\nto address this problem. Starting from an amino acid sequence, we formulate the\nstructure prediction task as a ground-state energy minimization problem using\nthe Variational Quantum Eigensolver (VQE). Amino acid connectivity is encoded\non a tetrahedral lattice model, and structural constraints-including steric,\ngeometric, and chirality terms-are mapped into a problem-specific Hamiltonian\nexpressed as sparse Pauli operators. The optimization is executed via a\ntwo-stage architecture separating energy estimation and measurement decoding,\nallowing noise mitigation under realistic quantum device conditions. We\nevaluate the framework on 23 randomly selected real protein fragments from the\nPDBbind dataset, as well as 7 real fragments from proteins with therapeutic\npotential, and run the experiments on the IBM-Cleveland Clinic quantum\nprocessor. Structural predictions are benchmarked against AlphaFold3 (AF3)\nusing identical postprocessing and docking procedures. Our quantum method\noutperformed AF3 in both RMSD (Root-Mean-Square Deviation) and docking\nefficacy. This work demonstrates, for the first time, a complete end-to-end\npipeline for biologically relevant structure prediction on real quantum\nhardware, highlighting its engineering feasibility and practical advantage over\nexisting classical and deep learning approaches.", "comment": "22 pages, 4 figures", "pdf_url": "http://arxiv.org/pdf/2506.22677v1", "categories": ["cs.ET"], "cate": "cs.ET", "url": "http://arxiv.org/abs/2506.22677v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.23185", "title": "Stateful Logic In-Memory Using Gain-Cell eDRAM", "authors": ["Barak Hoffer", "Shahar Kvatinsky"], "summary": "Modern data-intensive applications demand memory solutions that deliver\nhigh-density, low-power, and integrated computational capabilities to reduce\ndata movement overhead. This paper presents the use of Gain-Cell embedded DRAM\n(GC-eDRAM) - a compelling alternative to traditional SRAM and eDRAM - for\nstateful, in-memory logic. We propose a circuit design that exploits GC-eDRAM's\ndual-port architecture and nondestructive read operation to perform logic\nfunctions directly within the GC-eDRAM memory array. Our simulation results\ndemonstrate a 5us retention time coupled with a 99.5% success rate for\ncomputing the logic gates. By incorporating processing-in-memory (PIM)\nfunctionality into GC-eDRAM, our approach enhances memory and compute\ndensities, lowers power consumption, and improves overall performance for\ndata-intensive applications.", "comment": "Proceedings of IEEE International NEWCAS Conference, June 2025", "pdf_url": "http://arxiv.org/pdf/2506.23185v1", "categories": ["cs.ET"], "cate": "cs.ET", "url": "http://arxiv.org/abs/2506.23185v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23405", "title": "CMOS+X: Stacking Persistent Embedded Memories based on Oxide Transistors upon GPGPU Platforms", "authors": ["Faaiq Waqar", "Ming-Yen Lee", "Seongwon Yoon", "Seongkwang Lim", "Shimeng Yu"], "summary": "In contemporary general-purpose graphics processing units (GPGPUs), the\ncontinued increase in raw arithmetic throughput is constrained by the\ncapabilities of the register file (single-cycle) and last-level cache (high\nbandwidth), which require the delivery of operands at a cadence demanded by\nwide single-instruction multiple-data (SIMD) lanes. Enhancing the capacity,\ndensity, or bandwidth of these memories can unlock substantial performance\ngains; however, the recent stagnation of SRAM bit-cell scaling leads to\ninequivalent losses in compute density.\n  To address the challenges posed by SRAM's scaling and leakage power\nconsumption, this paper explores the potential CMOS+X integration of amorphous\noxide semiconductor (AOS) transistors in capacitive, persistent memory\ntopologies (e.g., 1T1C eDRAM, 2T0C/3T0C Gain Cell) as alternative cells in\nmulti-ported and high-bandwidth banked GPGPU memories. A detailed study of the\ndensity and energy tradeoffs of back-end-of-line (BEOL) integrated memories\nutilizing monolithic 3D (M3D)-integrated multiplexed arrays is conducted, while\naccounting for the macro-level limitations of integrating AOS candidate\nstructures proposed by the device community (an aspect often overlooked in\nprior work). By exploiting the short lifetime of register operands, we propose\na multi-ported AOS gain-cell capable of delivering 3x the read ports in ~76% of\nthe footprint of SRAM with over 70% lower standby power, enabling enhancements\nto compute capacity, such as larger warp sizes or processor counts. Benchmarks\nrun on a validated NVIDIA Ampere-class GPU model, using a modified version of\nAccel-Sim, demonstrate improvements of up to 5.2x the performance per watt and\nan average 8% higher geometric mean instruction per cycle (IPC) on various\ncompute- and memory-bound tasks.", "comment": "14 pages, 18 figures, 4 tables, 4 equations", "pdf_url": "http://arxiv.org/pdf/2506.23405v1", "categories": ["cs.ET", "cs.AR", "B.8.2; B.3.1"], "cate": "cs.ET", "url": "http://arxiv.org/abs/2506.23405v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.22506", "title": "SABRE-FL: Selective and Accurate Backdoor Rejection for Federated Prompt Learning", "authors": ["Momin Ahmad Khan", "Yasra Chandio", "Fatima Muhammad Anwar"], "summary": "Federated Prompt Learning has emerged as a communication-efficient and\nprivacy-preserving paradigm for adapting large vision-language models like CLIP\nacross decentralized clients. However, the security implications of this setup\nremain underexplored. In this work, we present the first study of backdoor\nattacks in Federated Prompt Learning. We show that when malicious clients\ninject visually imperceptible, learnable noise triggers into input images, the\nglobal prompt learner becomes vulnerable to targeted misclassification while\nstill maintaining high accuracy on clean inputs. Motivated by this\nvulnerability, we propose SABRE-FL, a lightweight, modular defense that filters\npoisoned prompt updates using an embedding-space anomaly detector trained\noffline on out-of-distribution data. SABRE-FL requires no access to raw client\ndata or labels and generalizes across diverse datasets. We show, both\ntheoretically and empirically, that malicious clients can be reliably\nidentified and filtered using an embedding-based detector. Across five diverse\ndatasets and four baseline defenses, SABRE-FL outperforms all baselines by\nsignificantly reducing backdoor accuracy while preserving clean accuracy,\ndemonstrating strong empirical performance and underscoring the need for robust\nprompt learning in future federated systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22506v1", "categories": ["cs.CR", "cs.AI"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.22506v1", "date": "2025-06-25", "updated": "2025-06-25"}
{"id": "2506.23826", "title": "Towards the \"Digital Me\": A vision of authentic Conversational Agents powered by personal Human Digital Twins", "authors": ["Llu√≠s C. Coll", "Martin W. Lauer-Schmaltz", "Philip Cash", "John P. Hansen", "Anja Maier"], "summary": "Human Digital Twins (HDTs) have traditionally been conceptualized as\ndata-driven models designed to support decision-making across various domains.\nHowever, recent advancements in conversational AI open new possibilities for\nHDTs to function as authentic, interactive digital counterparts of individuals.\nThis paper introduces a novel HDT system architecture that integrates large\nlanguage models with dynamically updated personal data, enabling it to mirror\nan individual's conversational style, memories, and behaviors. To achieve this,\nour approach implements context-aware memory retrieval, neural\nplasticity-inspired consolidation, and adaptive learning mechanisms, creating a\nmore natural and evolving digital persona. The resulting system does not only\nreplicate an individual's unique conversational style depending on who they are\nspeaking with, but also enriches responses with dynamically captured personal\nexperiences, opinions, and memories. While this marks a significant step toward\ndeveloping authentic virtual counterparts, it also raises critical ethical\nconcerns regarding privacy, accountability, and the long-term implications of\npersistent digital identities. This study contributes to the field of HDTs by\ndescribing our novel system architecture, demonstrating its capabilities, and\ndiscussing future directions and emerging challenges to ensure the responsible\nand ethical development of HDTs.", "comment": "24 pages, 9 figures", "pdf_url": "http://arxiv.org/pdf/2506.23826v1", "categories": ["cs.ET", "cs.AI", "cs.CY", "cs.HC", "cs.IR"], "cate": "cs.ET", "url": "http://arxiv.org/abs/2506.23826v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22515", "title": "In-context learning for the classification of manipulation techniques in phishing emails", "authors": ["Antony Dalmiere", "Guillaume Auriol", "Vincent Nicomette", "Pascal Marchand"], "summary": "Traditional phishing detection often overlooks psychological manipulation.\nThis study investigates using Large Language Model (LLM) In-Context Learning\n(ICL) for fine-grained classification of phishing emails based on a taxonomy of\n40 manipulation techniques. Using few-shot examples with GPT-4o-mini on\nreal-world French phishing emails (SignalSpam), we evaluated performance\nagainst a human-annotated test set (100 emails). The approach effectively\nidentifies prevalent techniques (e.g., Baiting, Curiosity Appeal, Request For\nMinor Favor) with a promising accuracy of 0.76. This work demonstrates ICL's\npotential for nuanced phishing analysis and provides insights into attacker\nstrategies.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22515v1", "categories": ["cs.CR", "cs.AI"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.22515v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.22876", "title": "Cooperation as Black Box: Conceptual Fluctuation and Diagnostic Tools for Misalignment in MAS", "authors": ["Shayak Nandi", "Fernanda M. Eliott"], "summary": "Misalignment in multi-agent systems (MAS) is often treated as a technical\nfailure; yet many such failures originate upstream, during the conceptual\ndesign phase, where semantic ambiguity and normative projection take place.\nThis paper identifies a foundational source of interpretive misalignment in\nMAS: the systemic conflation of cooperation and coordination, and the moral\noverreading that follows. Using the Rabbit-Duck illusion, we illustrate how\nperspective-dependent readings of agent behavior can create epistemic\ninstability. To address this, we introduce the Misalignment Mosaic, a\ndiagnostic framework for diagnosing meaning-level misalignment in MAS. It\ncomprises four components: 1. Terminological Inconsistency, 2. Concept-to-Code\nDecay, 3. Morality as Cooperation, and 4. Interpretive Ambiguity. The Mosaic\nenables researchers to examine how misalignment arises not only through policy\nor reward structures but also through language, framing, and design\nassumptions. While this paper focuses on the specific ambiguity between\ncoordination and cooperation, the Mosaic generalizes to other overloaded\nconcepts in MAS, such as alignment, autonomy, and trust. Rather than define\ncooperation once and for all, we offer a framework to diagnose meaning itself\nas a source of misalignment.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22876v1", "categories": ["cs.MA"], "cate": "cs.MA", "url": "http://arxiv.org/abs/2506.22876v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.22466", "title": "Conversations with Andrea: Visitors' Opinions on Android Robots in a Museum", "authors": ["Marcel Heisler", "Christian Becker-Asano"], "summary": "The android robot Andrea was set up at a public museum in Germany for six\nconsecutive days to have conversations with visitors, fully autonomously. No\nspecific context was given, so visitors could state their opinions regarding\npossible use-cases in structured interviews, without any bias. Additionally the\n44 interviewees were asked for their general opinions of the robot, their\nreasons (not) to interact with it and necessary improvements for future use.\nThe android's voice and wig were changed between different days of operation to\ngive varying cues regarding its gender. This did not have a significant impact\non the positive overall perception of the robot. Most visitors want the robot\nto provide information about exhibits in the future, while opinions on other\nroles, like a receptionist, were both wanted and explicitly not wanted by\ndifferent visitors. Speaking more languages (than only English) and faster\nresponse times were the improvements most desired. These findings from the\ninterviews are in line with an analysis of the system logs, which revealed,\nthat after chitchat and personal questions, most of the 4436 collected requests\nasked for information related to the museum and to converse in a different\nlanguage. The valuable insights gained from these real-world interactions are\nnow used to improve the system to become a useful real-world application.", "comment": "To be published in IEEE RO-MAN 2025 conference proceedings; for\n  videos check https://ai.hdm-stuttgart.de/humanoid-lab", "pdf_url": "http://arxiv.org/pdf/2506.22466v1", "categories": ["cs.RO", "cs.CY", "I.2.9; I.2.7"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.22466v1", "date": "2025-06-18", "updated": "2025-06-18"}
{"id": "2506.22464", "title": "Golden Ratio Assisted Localization for Wireless Sensor Network", "authors": ["Hitesh Mohapatra"], "summary": "This paper presents a novel localization algorithm for wireless sensor\nnetworks (WSNs) called Golden Ratio Localization (GRL), which leverages the\nmathematical properties of the golden ratio (phi 1.618) to optimize both node\nplacement and communication range. GRL introduces phi-based anchor node\ndeployment and hop-sensitive weighting using phi-exponents to improve\nlocalization accuracy while minimizing energy consumption. Through extensive\nsimulations conducted on a 100 m * 100 m sensor field with 100 nodes and 10\nanchors, GRL achieved an average localization error of 2.35 meters,\noutperforming DV- Hop (3.87 meters) and Centroid (4.95 meters). In terms of\nenergy efficiency, GRL reduced localization energy consumption to 1.12 microJ\nper node, compared to 1.78 microJ for DV-Hop and 1.45 microJ for Centroid.\nThese results confirm that GRL provides a more balanced and efficient\nlocalization approach, making it especially suitable for energy-constrained and\nlarge-scale WSN deployments.", "comment": "6", "pdf_url": "http://arxiv.org/pdf/2506.22464v1", "categories": ["cs.NI", "cs.HC", "B.4"], "cate": "cs.NI", "url": "http://arxiv.org/abs/2506.22464v1", "date": "2025-06-18", "updated": "2025-06-18"}
{"id": "2506.22946", "title": "Modular versus Hierarchical: A Structural Signature of Topic Popularity in Mathematical Research", "authors": ["Brian Hepler"], "summary": "Mathematical researchers, especially those in early-career positions, face\ncritical decisions about topic specialization with limited information about\nthe collaborative environments of different research areas. The aim of this\npaper is to study how the popularity of a research topic is associated with the\nstructure of that topic's collaboration network, as observed by a suite of\nmeasures capturing organizational structure at several scales. We apply these\nmeasures to 1,938 algorithmically discovered topics across 121,391 papers\nsourced from arXiv metadata during the period 2020--2025. Our analysis, which\ncontrols for the confounding effects of network size, reveals a structural\ndichotomy--we find that popular topics organize into modular \"schools of\nthought,\" while niche topics maintain hierarchical core-periphery structures\ncentered around established experts. This divide is not an artifact of scale,\nbut represents a size-independent structural pattern correlated with\npopularity. We also document a \"constraint reversal\": after controlling for\nsize, researchers in popular fields face greater structural constraints on\ncollaboration opportunities, contrary to conventional expectations. Our\nfindings suggest that topic selection is an implicit choice between two\nfundamentally different collaborative environments, each with distinct\nimplications for a researcher's career. To make these structural patterns\ntransparent to the research community, we developed the Math Research Compass\n(https://mathresearchcompass.com), an interactive platform providing data on\ntopic popularity and collaboration patterns across mathematical topics.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22946v1", "categories": ["cs.SI", "cs.CY", "cs.DL", "math.HO", "01A80, 91D30, 05C82, 62R07"], "cate": "cs.SI", "url": "http://arxiv.org/abs/2506.22946v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.22656", "title": "Knowledge-Guided Multi-Agent Framework for Automated Requirements Development: A Vision", "authors": ["Jiangping Huang", "Dongming Jin", "Weisong Sun", "Yang Liu", "Zhi Jin"], "summary": "This paper envisions a knowledge-guided multi-agent framework named KGMAF for\nautomated requirements development. KGMAF aims to address gaps in current\nautomation systems for SE, which prioritize code development and overlook the\ncomplexities of requirements tasks. KGMAF is composed of six specialized agents\nand an artifact pool to improve efficiency and accuracy. Specifically, KGMAF\noutlines the functionality, actions, and knowledge of each agent and provides\nthe conceptual design of the artifact pool. Our case study highlights the\npotential of KGMAF in real-world scenarios. Finally, we outline several\nresearch opportunities for implementing and enhancing automated requirements\ndevelopment using multi-agent systems. We believe that KGMAF will play a\npivotal role in shaping the future of automated requirements development in the\nera of LLMs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22656v1", "categories": ["cs.SE", "cs.AI", "68-04", "D.2.3; I.2.7"], "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.22656v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22520", "title": "Exploring Artificial Intelligence Tutor Teammate Adaptability to Harness Discovery Curiosity and Promote Learning in the Context of Interactive Molecular Dynamics", "authors": ["Mustafa Demir", "Jacob Miratsky", "Jonathan Nguyen", "Chun Kit Chan", "Punya Mishra", "Abhishek Singharoy"], "summary": "This study examines the impact of an Artificial Intelligence tutor teammate\n(AI) on student curiosity-driven engagement and learning effectiveness during\nInteractive Molecular Dynamics (IMD) tasks on the Visual Molecular Dynamics\nplatform. It explores the role of the AI's curiosity-triggering and response\nbehaviors in stimulating and sustaining student curiosity, affecting the\nfrequency and complexity of student-initiated questions. The study further\nassesses how AI interventions shape student engagement, foster discovery\ncuriosity, and enhance team performance within the IMD learning environment.\nUsing a Wizard-of-Oz paradigm, a human experimenter dynamically adjusts the AI\ntutor teammate's behavior through a large language model. By employing a\nmixed-methods exploratory design, a total of 11 high school students\nparticipated in four IMD tasks that involved molecular visualization and\ncalculations, which increased in complexity over a 60-minute period. Team\nperformance was evaluated through real-time observation and recordings, whereas\nteam communication was measured by question complexity and AI's\ncuriosity-triggering and response behaviors. Cross Recurrence Quantification\nAnalysis (CRQA) metrics reflected structural alignment in coordination and were\nlinked to communication behaviors. High-performing teams exhibited superior\ntask completion, deeper understanding, and increased engagement. Advanced\nquestions were associated with AI curiosity-triggering, indicating heightened\nengagement and cognitive complexity. CRQA metrics highlighted dynamic\nsynchronization in student-AI interactions, emphasizing structured yet adaptive\nengagement to promote curiosity. These proof-of-concept findings suggest that\nthe AI's dual role as a teammate and educator indicates its capacity to provide\nadaptive feedback, sustaining engagement and epistemic curiosity.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22520v1", "categories": ["cs.HC", "cs.AI", "cs.CE", "cs.CY"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.22520v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.23000", "title": "Communication via Sensing", "authors": ["Mohammad Kazemi", "Tolga M. Duman", "Deniz G√ºnd√ºz"], "summary": "We present an alternative take on the recently popularized concept of\n`$\\textit{joint sensing and communications}$', which focuses on using\ncommunication resources also for sensing. Here, we propose the opposite, where\nwe exploit the sensing capabilities of the receiver for communication. Our goal\nis to characterize the fundamental limits of communication over such a channel,\nwhich we call `$\\textit{communication via sensing}$'. We assume that changes in\nthe sensed attributes, e.g., location, speed, etc., are limited due to\npractical constraints, which are captured by assuming a finite-state channel\n(FSC) with an input cost constraint. We first formulate an upper bound on the\n$N$-letter capacity as a cost-constrained optimization problem over the input\nsequence distribution, and then convert it to an equivalent problem over the\nstate sequence distribution. Moreover, by breaking a walk on the underlying\nMarkov chain into a weighted sum of traversed graph cycles in the long walk\nlimit, we obtain a compact single-letter formulation of the capacity upper\nbound. Finally, for a specific case of a two-state FSC with noisy sensing\ncharacterized by a binary symmetric channel (BSC), we obtain a closed-form\nexpression for the capacity upper bound. Comparison with an existing numerical\nlower bound shows that our proposed upper bound is very tight for all crossover\nprobabilities.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23000v1", "categories": ["cs.IT", "math.IT"], "cate": "cs.IT", "url": "http://arxiv.org/abs/2506.23000v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.22654", "title": "Oobleck: Low-Compromise Design for Fault Tolerant Accelerators", "authors": ["Guy Wilks", "Brian Li", "Jonathan Balkind"], "summary": "Data center hardware refresh cycles are lengthening. However, increasing\nprocessor complexity is raising the potential for faults. To achieve longevity\nin the face of increasingly fault-prone datapaths, fault tolerance is needed,\nespecially in on-chip accelerator datapaths. Previously researched methods for\nadding fault tolerance to accelerator designs require high area, lowering chip\nutilisation. We propose a novel architecture for accelerator fault tolerance,\nOobleck, which leverages modular acceleration to enable fault tolerance without\nburdensome area requirements.\n  In order to streamline the development and enforce modular conventions, we\nintroduce the Viscosity language, an actor based approach to hardware-software\nco-design. Viscosity uses a single description of the accelerator's function\nand produces both hardware and software descriptions.\n  Our high-level models of data centers indicate that our approach can decrease\nthe number of failure-induced chip purchases inside data centers while not\naffecting aggregate throughput, thus reducing data center costs. To show the\nfeasibility of our approach, we show three case-studies: FFT, AES, and DCT\naccelerators. We additionally profile the performance under the key parameters\naffecting latency. Under a single fault we can maintain speedups of between\n1.7x-5.16x for accelerated applications over purely software implementations.\nWe show further benefits can be achieved by adding hot-spare FPGAs into the\nchip.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22654v1", "categories": ["cs.AR"], "cate": "cs.AR", "url": "http://arxiv.org/abs/2506.22654v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22714", "title": "Libra: Synergizing CUDA and Tensor Cores for High-Performance Sparse Matrix Multiplication", "authors": ["Jinliang Shi", "Shigang Li", "Youxuan Xu", "Xueying Wang", "Rongtian Fu", "Zhi Ma", "Tong Wu"], "summary": "Sparse matrix multiplication operators (i.e., SpMM and SDDMM) are widely used\nin deep learning and scientific computing. Modern accelerators are commonly\nequipped with Tensor cores and CUDA cores to accelerate sparse operators. The\nformer brings superior computing power but only for structured matrix\nmultiplication, while the latter has relatively lower performance but with\nhigher programming flexibility. In this work, we discover that utilizing one\nresource alone leads to inferior performance for sparse matrix multiplication,\ndue to their respective limitations. To this end, we propose Libra, a\nsystematic approach that enables synergistic computation between CUDA and\nTensor cores to achieve the best performance for sparse matrix multiplication.\nSpecifically, we propose a 2D-aware workload distribution strategy to find out\nthe sweet point of task mapping for different sparse operators, leveraging both\nthe high performance of Tensor cores and the low computational redundancy on\nCUDA cores. In addition, Libra incorporates systematic optimizations for\nheterogeneous computing, including hybrid load-balancing, finely optimized\nkernel implementations, and GPU-accelerated preprocessing. Extensive\nexperimental results on H100 and RTX 4090 GPUs show that Libra outperforms the\nstate-of-the-art by on average 3.1x (up to 9.23x) over DTC-SpMM and 2.9x (up to\n3.9x) for end-to-end GNN applications. Libra opens up a new perspective for\nsparse operator acceleration by fully exploiting the heterogeneous computing\nresources on GPUs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22714v1", "categories": ["cs.DC", "cs.LG", "cs.PF", "C.1.4; I.2.11"], "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.22714v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.22440", "title": "From Model Design to Organizational Design: Complexity Redistribution and Trade-Offs in Generative AI", "authors": ["Sharique Hasan", "Alexander Oettl", "Sampsa Samila"], "summary": "This paper introduces the Generality-Accuracy-Simplicity (GAS) framework to\nanalyze how large language models (LLMs) are reshaping organizations and\ncompetitive strategy. We argue that viewing AI as a simple reduction in input\ncosts overlooks two critical dynamics: (a) the inherent trade-offs among\ngenerality, accuracy, and simplicity, and (b) the redistribution of complexity\nacross stakeholders. While LLMs appear to defy the traditional trade-off by\noffering high generality and accuracy through simple interfaces, this\nuser-facing simplicity masks a significant shift of complexity to\ninfrastructure, compliance, and specialized personnel. The GAS trade-off,\ntherefore, does not disappear but is relocated from the user to the\norganization, creating new managerial challenges, particularly around accuracy\nin high-stakes applications. We contend that competitive advantage no longer\nstems from mere AI adoption, but from mastering this redistributed complexity\nthrough the design of abstraction layers, workflow alignment, and complementary\nexpertise. This study advances AI strategy by clarifying how scalable cognition\nrelocates complexity and redefines the conditions for technology integration.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22440v1", "categories": ["cs.CY", "cs.LG", "cs.MA", "econ.GN", "q-fin.EC"], "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.22440v1", "date": "2025-06-10", "updated": "2025-06-10"}
{"id": "2506.22437", "title": "Robust Perspective Correction for Real-World Crack Evolution Tracking in Image-Based Structural Health Monitoring", "authors": ["Xinxin Sun", "Peter Chang"], "summary": "Accurate image alignment is essential for monitoring crack evolution in\nstructural health monitoring (SHM), particularly under real-world conditions\ninvolving perspective distortion, occlusion, and low contrast. However,\ntraditional feature detectors such as SIFT and SURF, which rely on\nGaussian-based scale spaces, tend to suppress high-frequency edges, making them\nunsuitable for thin crack localization. Lightweight binary alternatives like\nORB and BRISK, while computationally efficient, often suffer from poor keypoint\nrepeatability on textured or shadowed surfaces. This study presents a\nphysics-informed alignment framework that adapts the open KAZE architecture to\nSHM-specific challenges. By utilizing nonlinear anisotropic diffusion to\nconstruct a crack-preserving scale space, and integrating RANSAC-based\nhomography estimation, the framework enables accurate geometric correction\nwithout the need for training, parameter tuning, or prior calibration. The\nmethod is validated on time-lapse images of masonry and concrete acquired via\nhandheld smartphone under varied field conditions, including shadow\ninterference, cropping, oblique viewing angles, and surface clutter. Compared\nto classical detectors, the proposed framework reduces crack area and spine\nlength errors by up to 70 percent and 90 percent, respectively, while\nmaintaining sub-5 percent alignment error in key metrics. Unsupervised,\ninterpretable, and computationally lightweight, this approach supports scalable\ndeployment via UAVs and mobile platforms. By tailoring nonlinear scale-space\nmodeling to SHM image alignment, this work offers a robust and physically\ngrounded alternative to conventional techniques for tracking real-world crack\nevolution.", "comment": "43 pages, 5 figures, 19 tables. Submitted to NDT&E International.\n  This work may also be of interest to researchers in optical NDE and civil\n  engineering SHM", "pdf_url": "http://arxiv.org/pdf/2506.22437v1", "categories": ["cs.CV", "68T45 (Computer Vision)"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22437v1", "date": "2025-05-06", "updated": "2025-05-06"}
{"id": "2506.23384", "title": "Programmable Co-Transcriptional Splicing: Realizing Regular Languages via Hairpin Deletion", "authors": ["Da-Jung Cho", "Szil√°rd Zsolt Fazekas", "Shinnosuke Seki", "Max Wiedenh√∂ft"], "summary": "RNA co-transcriptionality, where RNA is spliced or folded during\ntranscription from DNA templates, offers promising potential for molecular\nprogramming. It enables programmable folding of nano-scale RNA structures and\nhas recently been shown to be Turing universal. While post-transcriptional\nsplicing is well studied, co-transcriptional splicing is gaining attention for\nits efficiency, though its unpredictability still remains a challenge. In this\npaper, we focus on engineering co-transcriptional splicing, not only as a\nnatural phenomenon but as a programmable mechanism for generating specific RNA\ntarget sequences from DNA templates. The problem we address is whether we can\nencode a set of RNA sequences for a given system onto a DNA template word,\nensuring that all the sequences are generated through co-transcriptional\nsplicing. Given that finding the optimal encoding has been shown to be\nNP-complete under the various energy models considered, we propose a practical\nalternative approach under the logarithmic energy model. More specifically, we\nprovide a construction that encodes an arbitrary nondeterministic finite\nautomaton (NFA) into a circular DNA template from which co-transcriptional\nsplicing produces all sequences accepted by the NFA. As all finite languages\ncan be efficiently encoded as NFA, this framework solves the problem of finding\nsmall DNA templates for arbitrary target sets of RNA sequences. The quest to\nobtain the smallest possible such templates naturally leads us to consider the\nproblem of minimizing NFA and certain practically motivated variants of it, but\nas we show, those minimization problems are computationally intractable.", "comment": "28 pages, 8 Figures, Accepted at the 31st International Conference on\n  DNA Computing and Molecular Programming (2025)", "pdf_url": "http://arxiv.org/pdf/2506.23384v1", "categories": ["cs.FL", "92-10", "F.4.3; J.3; F.1.3"], "cate": "cs.FL", "url": "http://arxiv.org/abs/2506.23384v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.22676", "title": "CAD-Integrated Electrostatic Boundary Element Simulations with Non-Conforming Higher-Order Meshes", "authors": ["Benjamin Marussig", "J√ºrgen Zechner", "Thomas R√ºberg", "Lars Kielhorn", "Domagoj Bo≈°njak", "Thomas-Peter Fries"], "summary": "We present a design through analysis workflow that enables virtual\nprototyping of electric devices. A CAD plugin establishes the interaction\nbetween design and analysis, allowing the preparation of analysis models and\nthe visualization of its results within the design environment. The simulations\nutilize a fast boundary element method (BEM) that allows for non-conforming and\nhigher-order meshes. Our numerical experiments investigate the accuracy of the\napproach and its sensitivity to the initial CAD representation. Overall, the\nworkflow enables a close link between design and analysis, where the\nnon-conforming higher-order BEM approach provides accurate results and\nsignificantly simplifies the interaction.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22676v1", "categories": ["cs.CE"], "cate": "cs.CE", "url": "http://arxiv.org/abs/2506.22676v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22489", "title": "A Multi-Criteria Evaluation Framework for Siting Fusion Energy Facilities: Application and Evaluation of U.S. Coal Power Plants", "authors": ["Muhammad R. Abdussami", "Kevin Daley", "Gabrielle Hoelzle", "Aditi Verma"], "summary": "This paper proposes a comprehensive methodology for siting fusion energy\nfacilities, integrating expert judgment, geospatial data, and multi-criteria\ndecision making tools to evaluate site suitability systematically. As a case\nstudy, we apply this framework to all currently operational coal power plant\nsites in the United States to examine their potential for hosting future fusion\nfacilities at a time when these coal plants are shut down on reaching their end\nof life - timelines which are expected to coincide with the potential\ndeployment of fusion energy facilities. Drawing on 22 siting criteria -\nincluding state and federal policies, risk and hazard assessments, and spatial\nand infrastructural parameters - we implement two MultiCriteria Decision-Making\n(MCDM) methods: the Fuzzy Full Consistency Method (F-FUCOM) to derive attribute\nweights and the Weighted Sum Method (WSM) to rank sites based on composite\nsuitability scores. By focusing on fusion-specific siting needs and\ndemonstrating the framework through a coal site application, this study\ncontributes a scalable and transparent decision-support tool for identifying\noptimal fusion energy deployment locations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22489v1", "categories": ["eess.SY", "cs.SY", "physics.app-ph"], "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.22489v1", "date": "2025-06-24", "updated": "2025-06-24"}
{"id": "2506.22448", "title": "Unsupervised Learning-Based Joint Resource Allocation and Beamforming Design for RIS-Assisted MISO-OFDMA Systems", "authors": ["Yu Ma", "Xingyu Zhou", "Xiao Li", "Le Liang", "Shi Jin"], "summary": "Reconfigurable intelligent surfaces (RIS) are key enablers for 6G wireless\nsystems. This paper studies downlink transmission in an RIS-assisted MISO-OFDMA\nsystem, addressing resource allocation challenges. A two-stage unsupervised\nlearning-based framework is proposed to jointly design RIS phase shifts, BS\nbeamforming, and resource block (RB) allocation. The framework includes\nBeamNet, which predicts RIS phase shifts from CSI, and AllocationNet, which\nallocates RBs using equivalent CSI derived from BeamNet outputs. Active\nbeamforming is implemented via maximum ratio transmission and water-filling. To\nhandle discrete constraints while ensuring differentiability, quantization and\nthe Gumbel-softmax trick are adopted. A customized loss and phased training\nenhance performance under QoS constraints. Simulations show the method achieves\n99.93% of the sum rate of the SCA baseline with only 0.036% of its runtime, and\nit remains robust across varying channel and user conditions.", "comment": "Due to the limitation \"The abstract field cannot be longer than 1,920\n  characters\", the abstract here is shorter than that in the PDF file", "pdf_url": "http://arxiv.org/pdf/2506.22448v1", "categories": ["eess.SP", "cs.AI", "cs.IT", "math.IT"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.22448v1", "date": "2025-06-12", "updated": "2025-06-12"}
{"id": "2506.22532", "title": "High Resolution Isotropic 3D Cine imaging with Automated Segmentation using Concatenated 2D Real-time Imaging and Deep Learning", "authors": ["Mark Wrobel", "Michele Pascale", "Tina Yao", "Ruaraidh Campbell", "Elena Milano", "Michael Quail", "Jennifer Steeden", "Vivek Muthurangu"], "summary": "Background: Conventional cardiovascular magnetic resonance (CMR) in\npaediatric and congenital heart disease uses 2D, breath-hold, balanced steady\nstate free precession (bSSFP) cine imaging for assessment of function and\ncardiac-gated, respiratory-navigated, static 3D bSSFP whole-heart imaging for\nanatomical assessment. Our aim is to concatenate a stack 2D free-breathing\nreal-time cines and use Deep Learning (DL) to create an isotropic a fully\nsegmented 3D cine dataset from these images. Methods: Four DL models were\ntrained on open-source data that performed: a) Interslice contrast correction;\nb) Interslice respiratory motion correction; c) Super-resolution (slice\ndirection); and d) Segmentation of right and left atria and ventricles (RA, LA,\nRV, and LV), thoracic aorta (Ao) and pulmonary arteries (PA). In 10 patients\nundergoing routine cardiovascular examination, our method was validated on\nprospectively acquired sagittal stacks of real-time cine images. Quantitative\nmetrics (ventricular volumes and vessel diameters) and image quality of the 3D\ncines were compared to conventional breath hold cine and whole heart imaging.\nResults: All real-time data were successfully transformed into 3D cines with a\ntotal post-processing time of <1 min in all cases. There were no significant\nbiases in any LV or RV metrics with reasonable limits of agreement and\ncorrelation. There is also reasonable agreement for all vessel diameters,\nalthough there was a small but significant overestimation of RPA diameter.\nConclusion: We have demonstrated the potential of creating a 3D-cine data from\nconcatenated 2D real-time cine images using a series of DL models. Our method\nhas short acquisition and reconstruction times with fully segmented data being\navailable within 2 minutes. The good agreement with conventional imaging\nsuggests that our method could help to significantly speed up CMR in clinical\npractice.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22532v1", "categories": ["eess.IV", "cs.CV", "cs.LG"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.22532v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22799", "title": "VoteSplat: Hough Voting Gaussian Splatting for 3D Scene Understanding", "authors": ["Minchao Jiang", "Shunyu Jia", "Jiaming Gu", "Xiaoyuan Lu", "Guangming Zhu", "Anqi Dong", "Liang Zhang"], "summary": "3D Gaussian Splatting (3DGS) has become horsepower in high-quality, real-time\nrendering for novel view synthesis of 3D scenes. However, existing methods\nfocus primarily on geometric and appearance modeling, lacking deeper scene\nunderstanding while also incurring high training costs that complicate the\noriginally streamlined differentiable rendering pipeline. To this end, we\npropose VoteSplat, a novel 3D scene understanding framework that integrates\nHough voting with 3DGS. Specifically, Segment Anything Model (SAM) is utilized\nfor instance segmentation, extracting objects, and generating 2D vote maps. We\nthen embed spatial offset vectors into Gaussian primitives. These offsets\nconstruct 3D spatial votes by associating them with 2D image votes, while depth\ndistortion constraints refine localization along the depth axis. For\nopen-vocabulary object localization, VoteSplat maps 2D image semantics to 3D\npoint clouds via voting points, reducing training costs associated with\nhigh-dimensional CLIP features while preserving semantic unambiguity. Extensive\nexperiments demonstrate effectiveness of VoteSplat in open-vocabulary 3D\ninstance localization, 3D point cloud understanding, click-based 3D object\nlocalization, hierarchical segmentation, and ablation studies. Our code is\navailable at https://sy-ja.github.io/votesplat/", "comment": "Accepted to ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2506.22799v1", "categories": ["cs.GR", "cs.CV", "cs.LG"], "cate": "cs.GR", "url": "http://arxiv.org/abs/2506.22799v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.22608", "title": "On Fine-Grained Distinct Element Estimation", "authors": ["Ilias Diakonikolas", "Daniel M. Kane", "Jasper C. H. Lee", "Thanasis Pittas", "David P. Woodruff", "Samson Zhou"], "summary": "We study the problem of distributed distinct element estimation, where\n$\\alpha$ servers each receive a subset of a universe $[n]$ and aim to compute a\n$(1+\\varepsilon)$-approximation to the number of distinct elements using\nminimal communication. While prior work establishes a worst-case bound of\n$\\Theta\\left(\\alpha\\log n+\\frac{\\alpha}{\\varepsilon^2}\\right)$ bits, these\nresults rely on assumptions that may not hold in practice. We introduce a new\nparameterization based on the number $C = \\frac{\\beta}{\\varepsilon^2}$ of\npairwise collisions, i.e., instances where the same element appears on multiple\nservers, and design a protocol that uses only $\\mathcal{O}\\left(\\alpha\\log\nn+\\frac{\\sqrt{\\beta}}{\\varepsilon^2} \\log n\\right)$ bits, breaking previous\nlower bounds when $C$ is small. We further improve our algorithm under\nassumptions on the number of distinct elements or collisions and provide\nmatching lower bounds in all regimes, establishing $C$ as a tight complexity\nmeasure for the problem. Finally, we consider streaming algorithms for distinct\nelement estimation parameterized by the number of items with frequency larger\nthan $1$. Overall, our results offer insight into why statistical problems with\nknown hardness results can be efficiently solved in practice.", "comment": "ICML 2025", "pdf_url": "http://arxiv.org/pdf/2506.22608v1", "categories": ["cs.DS"], "cate": "cs.DS", "url": "http://arxiv.org/abs/2506.22608v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22441", "title": "Latent Factorization of Tensors with Threshold Distance Weighted Loss for Traffic Data Estimation", "authors": ["Lei Yang"], "summary": "Intelligent transportation systems (ITS) rely heavily on complete and\nhigh-quality spatiotemporal traffic data to achieve optimal performance.\nNevertheless, in real-word traffic data collection processes, issues such as\ncommunication failures and sensor malfunctions often lead to incomplete or\ncorrupted datasets, thereby posing significant challenges to the advancement of\nITS. Among various methods for imputing missing spatiotemporal traffic data,\nthe latent factorization of tensors (LFT) model has emerged as a widely adopted\nand effective solution. However, conventional LFT models typically employ the\nstandard L2-norm in their learning objective, which makes them vulnerable to\nthe influence of outliers. To overcome this limitation, this paper proposes a\nthreshold distance weighted (TDW) loss-incorporated Latent Factorization of\nTensors (TDWLFT) model. The proposed loss function effectively reduces the\nmodel's sensitivity to outliers by assigning differentiated weights to\nindividual samples. Extensive experiments conducted on two traffic speed\ndatasets sourced from diverse urban environments confirm that the proposed\nTDWLFT model consistently outperforms state-of-the-art approaches in terms of\nboth in both prediction accuracy and computational efficiency.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22441v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22441v1", "date": "2025-06-11", "updated": "2025-06-11"}
{"id": "2506.22604", "title": "Bootstrapping Human-Like Planning via LLMs", "authors": ["David Porfirio", "Vincent Hsiao", "Morgan Fine-Morris", "Leslie Smith", "Laura M. Hiatt"], "summary": "Robot end users increasingly require accessible means of specifying tasks for\nrobots to perform. Two common end-user programming paradigms include\ndrag-and-drop interfaces and natural language programming. Although natural\nlanguage interfaces harness an intuitive form of human communication,\ndrag-and-drop interfaces enable users to meticulously and precisely dictate the\nkey actions of the robot's task. In this paper, we investigate the degree to\nwhich both approaches can be combined. Specifically, we construct a large\nlanguage model (LLM)-based pipeline that accepts natural language as input and\nproduces human-like action sequences as output, specified at a level of\ngranularity that a human would produce. We then compare these generated action\nsequences to another dataset of hand-specified action sequences. Although our\nresults reveal that larger models tend to outperform smaller ones in the\nproduction of human-like action sequences, smaller models nonetheless achieve\nsatisfactory performance.", "comment": "Accepted by the 2025 34th IEEE International Conference on Robot and\n  Human Interactive Communication (RO-MAN)", "pdf_url": "http://arxiv.org/pdf/2506.22604v1", "categories": ["cs.AI", "cs.HC", "cs.RO"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.22604v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22648", "title": "Interact2Vec -- An efficient neural network-based model for simultaneously learning users and items embeddings in recommender systems", "authors": ["Pedro R. Pires", "Tiago A. Almeida"], "summary": "Over the past decade, recommender systems have experienced a surge in\npopularity. Despite notable progress, they grapple with challenging issues,\nsuch as high data dimensionality and sparseness. Representing users and items\nas low-dimensional embeddings learned via neural networks has become a leading\nsolution. However, while recent studies show promising results, many approaches\nrely on complex architectures or require content data, which may not always be\navailable. This paper presents Interact2Vec, a novel neural network-based model\nthat simultaneously learns distributed embeddings for users and items while\ndemanding only implicit feedback. The model employs state-of-the-art strategies\nthat natural language processing models commonly use to optimize the training\nphase and enhance the final embeddings. Two types of experiments were conducted\nregarding the extrinsic and intrinsic quality of the model. In the former, we\nbenchmarked the recommendations generated by Interact2Vec's embeddings in a\ntop-$N$ ranking problem, comparing them with six other recommender algorithms.\nThe model achieved the second or third-best results in 30\\% of the datasets,\nbeing competitive with other recommenders, and has proven to be very efficient\nwith an average training time reduction of 274\\% compared to other\nembedding-based models. Later, we analyzed the intrinsic quality of the\nembeddings through similarity tables. Our findings suggest that Interact2Vec\ncan achieve promising results, especially on the extrinsic task, and is an\nexcellent embedding-generator model for scenarios of scarce computing\nresources, enabling the learning of item and user embeddings simultaneously and\nefficiently.", "comment": "Accepted for publication in Applied Soft Computing (ASOC), 49 pages,\n  14 figures", "pdf_url": "http://arxiv.org/pdf/2506.22648v1", "categories": ["cs.IR", "cs.LG"], "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.22648v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22646", "title": "Speaker Targeting via Self-Speaker Adaptation for Multi-talker ASR", "authors": ["Weiqing Wang", "Taejin Park", "Ivan Medennikov", "Jinhan Wang", "Kunal Dhawan", "He Huang", "Nithin Rao Koluguri", "Jagadeesh Balam", "Boris Ginsburg"], "summary": "We propose a self-speaker adaptation method for streaming multi-talker\nautomatic speech recognition (ASR) that eliminates the need for explicit\nspeaker queries. Unlike conventional approaches requiring target speaker\nembeddings or enrollment audio, our technique dynamically adapts individual ASR\ninstances through speaker-wise speech activity prediction. The key innovation\ninvolves injecting speaker-specific kernels generated via speaker supervision\nactivations into selected ASR encoder layers. This enables instantaneous\nspeaker adaptation to target speakers while handling fully overlapped speech\neven in a streaming scenario. Experiments show state-of-the-art performance in\nboth offline and streaming scenarios, demonstrating that our self-adaptive\nmethod effectively addresses severe speech overlap through streamlined\nspeaker-focused recognition. The results validate the proposed self-speaker\nadaptation approach as a robust solution for multi-talker ASR under severe\noverlapping speech conditions.", "comment": "Accepted by INTERSPEECH 2025", "pdf_url": "http://arxiv.org/pdf/2506.22646v1", "categories": ["eess.AS", "cs.SD"], "cate": "eess.AS", "url": "http://arxiv.org/abs/2506.22646v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22564", "title": "Efficient Tensor Decomposition via Moment Matrix Extension", "authors": ["Bobby Shi", "Julia Lindberg", "Joe Kileel"], "summary": "Motivated by a flurry of recent work on efficient tensor decomposition\nalgorithms, we show that the celebrated moment matrix extension algorithm of\nBrachat, Comon, Mourrain, and Tsigaridas for symmetric tensor canonical\npolyadic (CP) decomposition can be made efficient under the right conditions.\nWe first show that the crucial property determining the complexity of the\nalgorithm is the regularity of a target decomposition. This allows us to reduce\nthe complexity of the vanilla algorithm, while also unifying results from\nprevious works. We then show that for tensors in $S^d\\mathbb{C}^{n+1}$ with $d$\neven, low enough regularity can reduce finding a symmetric tensor decomposition\nto solving a system of linear equations. For order-$4$ tensors we prove that\ngeneric tensors of rank up to $r=2n+1$ can be decomposed efficiently via moment\nmatrix extension, exceeding the rank threshold allowed by simultaneous\ndiagonalization. We then formulate a conjecture that states for generic\norder-$4$ tensors of rank $r=O(n^2)$ the induced linear system is sufficient\nfor efficient tensor decomposition, matching the asymptotics of existing\nalgorithms and in fact improving the leading coefficient. Towards this\nconjecture we give computer assisted proofs that the statement holds for $n=2,\n\\dots, 17$. Next we demonstrate that classes of nonidentifiable tensors can be\ndecomposed efficiently via the moment matrix extension algorithm, bypassing the\nusual need for uniqueness of decomposition. Of particular interest is the class\nof monomials, for which the extension algorithm is not only efficient but also\nimproves on existing theory by explicitly parameterizing the space of\ndecompositions. Code for implementations of the efficient algorithm for generic\ntensors and monomials are provided, along with several numerical examples.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22564v1", "categories": ["math.AG", "cs.NA", "cs.SC", "math.NA"], "cate": "math.AG", "url": "http://arxiv.org/abs/2506.22564v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22439", "title": "Psycholinguistic Word Features: a New Approach for the Evaluation of LLMs Alignment with Humans", "authors": ["Javier Conde", "Miguel Gonz√°lez", "Mar√≠a Grandury", "Gonzalo Mart√≠nez", "Pedro Reviriego", "Mar Brysbaert"], "summary": "The evaluation of LLMs has so far focused primarily on how well they can\nperform different tasks such as reasoning, question-answering, paraphrasing, or\ntranslating. For most of these tasks, performance can be measured with\nobjective metrics, such as the number of correct answers. However, other\nlanguage features are not easily quantified. For example, arousal,\nconcreteness, or gender associated with a given word, as well as the extent to\nwhich we experience words with senses and relate them to a specific sense.\nThose features have been studied for many years by psycholinguistics,\nconducting large-scale experiments with humans to produce ratings for thousands\nof words. This opens an opportunity to evaluate how well LLMs align with human\nratings on these word features, taking advantage of existing studies that cover\nmany different language features in a large number of words. In this paper, we\nevaluate the alignment of a representative group of LLMs with human ratings on\ntwo psycholinguistic datasets: the Glasgow and Lancaster norms. These datasets\ncover thirteen features over thousands of words. The results show that\nalignment is \\textcolor{black}{generally} better in the Glasgow norms evaluated\n(arousal, valence, dominance, concreteness, imageability, familiarity, and\ngender) than on the Lancaster norms evaluated (introceptive, gustatory,\nolfactory, haptic, auditory, and visual). This suggests a potential limitation\nof current LLMs in aligning with human sensory associations for words, which\nmay be due to their lack of embodied cognition present in humans and\nillustrates the usefulness of evaluating LLMs with psycholinguistic datasets.", "comment": "Accepted for the GEM2 workshop at ACL 2025", "pdf_url": "http://arxiv.org/pdf/2506.22439v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.22439v1", "date": "2025-05-29", "updated": "2025-05-29"}
{"id": "2506.22576", "title": "The lightning method for the heat equation", "authors": ["Hunter L Croix", "Alan E. Lindsay"], "summary": "This paper introduces a new method for solving the planar heat equation based\non the Lightning Method. The lightning method is a recent development in the\nnumerical solution of linear PDEs which expresses solutions using sums of\npolynomials and rational functions, or more generally as sums of fundamental\nsolutions. The method is particularly well suited to handle domains with sharp\ncorners where solution singularities are present. Boundary conditions are\nformed on a set of collocation points which is then solved as an overdetermined\nlinear system. The approach of the present work is to utilize the Laplace\ntransform to obtain a modified Helmholtz equation which is solved by an\napplication of the lightning method. The numerical inversion of the Laplace\ntransform is then performed by means of Talbot integration. Our validation of\nthe method against existing results and multiple challenging test problems\nshows the method attains spectral accuracy with root-exponential convergence\nwhile being robust across a wide range of time intervals and adaptable to a\nvariety of geometric scenarios.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22576v1", "categories": ["math.NA", "cs.NA", "math.CV"], "cate": "math.NA", "url": "http://arxiv.org/abs/2506.22576v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.23717", "title": "Towards Efficient and Accurate Spiking Neural Networks via Adaptive Bit Allocation", "authors": ["Xingting Yao", "Qinghao Hu", "Fei Zhou", "Tielong Liu", "Gang Li", "Peisong Wang", "Jian Cheng"], "summary": "Multi-bit spiking neural networks (SNNs) have recently become a heated\nresearch spot, pursuing energy-efficient and high-accurate AI. However, with\nmore bits involved, the associated memory and computation demands escalate to\nthe point where the performance improvements become disproportionate. Based on\nthe insight that different layers demonstrate different importance and extra\nbits could be wasted and interfering, this paper presents an adaptive bit\nallocation strategy for direct-trained SNNs, achieving fine-grained layer-wise\nallocation of memory and computation resources. Thus, SNN's efficiency and\naccuracy can be improved. Specifically, we parametrize the temporal lengths and\nthe bit widths of weights and spikes, and make them learnable and controllable\nthrough gradients. To address the challenges caused by changeable bit widths\nand temporal lengths, we propose the refined spiking neuron, which can handle\ndifferent temporal lengths, enable the derivation of gradients for temporal\nlengths, and suit spike quantization better. In addition, we theoretically\nformulate the step-size mismatch problem of learnable bit widths, which may\nincur severe quantization errors to SNN, and accordingly propose the step-size\nrenewal mechanism to alleviate this issue. Experiments on various datasets,\nincluding the static CIFAR and ImageNet and the dynamic CIFAR-DVS and\nDVS-GESTURE, demonstrate that our methods can reduce the overall memory and\ncomputation cost while achieving higher accuracy. Particularly, our\nSEWResNet-34 can achieve a 2.69\\% accuracy gain and 4.16$\\times$ lower bit\nbudgets over the advanced baseline work on ImageNet. This work will be fully\nopen-sourced.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23717v1", "categories": ["cs.NE", "cs.AI", "cs.CV", "cs.LG"], "cate": "cs.NE", "url": "http://arxiv.org/abs/2506.23717v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22628", "title": "Evaluating Sound Similarity Metrics for Differentiable, Iterative Sound-Matching", "authors": ["Amir Salimi", "Abram Hindle", "Osmar R. Zaiane"], "summary": "Manual sound design with a synthesizer is inherently iterative: an artist\ncompares the synthesized output to a mental target, adjusts parameters, and\nrepeats until satisfied. Iterative sound-matching automates this workflow by\ncontinually programming a synthesizer under the guidance of a loss function (or\nsimilarity measure) toward a target sound. Prior comparisons of loss functions\nhave typically favored one metric over another, but only within narrow\nsettings: limited synthesis methods, few loss types, often without blind\nlistening tests. This leaves open the question of whether a universally optimal\nloss exists, or the choice of loss remains a creative decision conditioned on\nthe synthesis method and the sound designer's preference. We propose\ndifferentiable iterative sound-matching as the natural extension of the\navailable literature, since it combines the manual approach to sound design\nwith modern advances in machine learning. To analyze the variability of loss\nfunction performance across synthesizers, we implemented a mix of four novel\nand established differentiable loss functions, and paired them with\ndifferentiable subtractive, additive, and AM synthesizers. For each of the\nsixteen synthesizer--loss combinations, we ran 300 randomized sound-matching\ntrials. Performance was measured using parameter differences,\nspectrogram-distance metrics, and manually assigned listening scores. We\nobserved a moderate level of consistency among the three performance measures.\nOur post-hoc analysis shows that the loss function performance is highly\ndependent on the synthesizer. These findings underscore the value of expanding\nthe scope of sound-matching experiments and developing new similarity metrics\ntailored to specific synthesis techniques rather than pursuing\none-size-fits-all solutions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22628v1", "categories": ["cs.SD", "eess.AS"], "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.22628v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22476", "title": "An Interpretable Transformer-Based Foundation Model for Cross-Procedural Skill Assessment Using Raw fNIRS Signals", "authors": ["A. Subedi", "S. De", "L. Cavuoto", "S. Schwaitzberg", "M. Hackett", "J. Norfleet"], "summary": "Objective skill assessment in high-stakes procedural environments requires\nmodels that not only decode underlying cognitive and motor processes but also\ngeneralize across tasks, individuals, and experimental contexts. While prior\nwork has demonstrated the potential of functional near-infrared spectroscopy\n(fNIRS) for evaluating cognitive-motor performance, existing approaches are\noften task-specific, rely on extensive preprocessing, and lack robustness to\nnew procedures or conditions. Here, we introduce an interpretable\ntransformer-based foundation model trained on minimally processed fNIRS signals\nfor cross-procedural skill assessment. Pretrained using self-supervised\nlearning on data from laparoscopic surgical tasks and endotracheal intubation\n(ETI), the model achieves greater than 88% classification accuracy on all\ntasks, with Matthews Correlation Coefficient exceeding 0.91 on ETI. It\ngeneralizes to a novel emergency airway procedure--cricothyrotomy--using fewer\nthan 30 labeled samples and a lightweight (less than 2k parameter) adapter\nmodule, attaining an AUC greater than 87%. Interpretability is achieved via a\nnovel channel attention mechanism--developed specifically for fNIRS--that\nidentifies functionally coherent prefrontal sub-networks validated through\nablation studies. Temporal attention patterns align with task-critical phases\nand capture stress-induced changes in neural variability, offering insight into\ndynamic cognitive states.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22476v1", "categories": ["eess.SP", "cs.ET", "cs.HC", "cs.LG", "q-bio.NC", "I.2.6; J.3; H.1.2"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.22476v1", "date": "2025-06-21", "updated": "2025-06-21"}
{"id": "2506.22521", "title": "A Survey on Model Extraction Attacks and Defenses for Large Language Models", "authors": ["Kaixiang Zhao", "Lincan Li", "Kaize Ding", "Neil Zhenqiang Gong", "Yue Zhao", "Yushun Dong"], "summary": "Model extraction attacks pose significant security threats to deployed\nlanguage models, potentially compromising intellectual property and user\nprivacy. This survey provides a comprehensive taxonomy of LLM-specific\nextraction attacks and defenses, categorizing attacks into functionality\nextraction, training data extraction, and prompt-targeted attacks. We analyze\nvarious attack methodologies including API-based knowledge distillation, direct\nquerying, parameter recovery, and prompt stealing techniques that exploit\ntransformer architectures. We then examine defense mechanisms organized into\nmodel protection, data privacy protection, and prompt-targeted strategies,\nevaluating their effectiveness across different deployment scenarios. We\npropose specialized metrics for evaluating both attack effectiveness and\ndefense performance, addressing the specific challenges of generative language\nmodels. Through our analysis, we identify critical limitations in current\napproaches and propose promising research directions, including integrated\nattack methodologies and adaptive defense mechanisms that balance security with\nmodel utility. This work serves NLP researchers, ML engineers, and security\nprofessionals seeking to protect language models in production environments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22521v1", "categories": ["cs.CR", "cs.AI"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.22521v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.22440", "title": "From Model Design to Organizational Design: Complexity Redistribution and Trade-Offs in Generative AI", "authors": ["Sharique Hasan", "Alexander Oettl", "Sampsa Samila"], "summary": "This paper introduces the Generality-Accuracy-Simplicity (GAS) framework to\nanalyze how large language models (LLMs) are reshaping organizations and\ncompetitive strategy. We argue that viewing AI as a simple reduction in input\ncosts overlooks two critical dynamics: (a) the inherent trade-offs among\ngenerality, accuracy, and simplicity, and (b) the redistribution of complexity\nacross stakeholders. While LLMs appear to defy the traditional trade-off by\noffering high generality and accuracy through simple interfaces, this\nuser-facing simplicity masks a significant shift of complexity to\ninfrastructure, compliance, and specialized personnel. The GAS trade-off,\ntherefore, does not disappear but is relocated from the user to the\norganization, creating new managerial challenges, particularly around accuracy\nin high-stakes applications. We contend that competitive advantage no longer\nstems from mere AI adoption, but from mastering this redistributed complexity\nthrough the design of abstraction layers, workflow alignment, and complementary\nexpertise. This study advances AI strategy by clarifying how scalable cognition\nrelocates complexity and redefines the conditions for technology integration.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22440v1", "categories": ["cs.CY", "cs.LG", "cs.MA", "econ.GN", "q-fin.EC"], "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.22440v1", "date": "2025-06-10", "updated": "2025-06-10"}
{"id": "2506.22473", "title": "Unsupervised Discovery of Behavioral Primitives from Sensorimotor Dynamic Functional Connectivity", "authors": ["Fernando Diaz Ledezma", "Valentin Marcel", "Matej Hoffmann"], "summary": "The movements of both animals and robots give rise to streams of\nhigh-dimensional motor and sensory information. Imagine the brain of a newborn\nor the controller of a baby humanoid robot trying to make sense of unprocessed\nsensorimotor time series. Here, we present a framework for studying the dynamic\nfunctional connectivity between the multimodal sensory signals of a robotic\nagent to uncover an underlying structure. Using instantaneous mutual\ninformation, we capture the time-varying functional connectivity (FC) between\nproprioceptive, tactile, and visual signals, revealing the sensorimotor\nrelationships. Using an infinite relational model, we identified sensorimotor\nmodules and their evolving connectivity. To further interpret these dynamic\ninteractions, we employed non-negative matrix factorization, which decomposed\nthe connectivity patterns into additive factors and their corresponding\ntemporal coefficients. These factors can be considered the agent's motion\nprimitives or movement synergies that the agent can use to make sense of its\nsensorimotor space and later for behavior selection. In the future, the method\ncan be deployed in robot learning as well as in the analysis of human movement\ntrajectories or brain signals.", "comment": "8 pages with 6 figures", "pdf_url": "http://arxiv.org/pdf/2506.22473v1", "categories": ["cs.RO", "eess.SP"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.22473v1", "date": "2025-06-20", "updated": "2025-06-20"}
{"id": "2506.22470", "title": "Reliable Transmission of LTP Using Reinforcement Learning-Based Adaptive FEC", "authors": ["Liang Chen", "Yu Song", "Kanglian Zhao", "Juan A. Fraire", "Wenfeng Li"], "summary": "Delay/Disruption Tolerant Networking (DTN) employs the Licklider Transmission\nProtocol (LTP) with Automatic Repeat reQuest (ARQ) for reliable data delivery\nin challenging interplanetary networks. While previous studies have integrated\npacket-level Forward Erasure Correction (FEC) into LTP to reduce retransmission\ntime costs, existing static and delay-feedback-based dynamic coding methods\nstruggle with highly variable and unpredictable deep space channel conditions.\nThis paper proposes a reinforcement learning (RL)-based adaptive FEC algorithm\nto address these limitations. The algorithm utilizes historical feedback and\nsystem state to predict future channel conditions and proactively adjust the\ncode rate. This approach aims to anticipate channel quality degradation,\nthereby preventing decoding failures and subsequent LTP retransmissions and\nimproving coding efficiency by minimizing redundancy during favorable channel\nconditions. Performance evaluations conducted in simulated Earth-Moon and\nEarth-Mars link scenarios demonstrate this algorithm's effectiveness in\noptimizing data transmission for interplanetary networks. Compared to existing\nmethods, this approach demonstrates significant improvement, with matrix\ndecoding failures reduced by at least 2/3.", "comment": "15 pages, 30 figures, Liang Chen and Yu Song are co-first authors", "pdf_url": "http://arxiv.org/pdf/2506.22470v1", "categories": ["cs.NI", "cs.SY", "eess.SY"], "cate": "cs.NI", "url": "http://arxiv.org/abs/2506.22470v1", "date": "2025-06-19", "updated": "2025-06-19"}
{"id": "2506.22954", "title": "Evaluating and Improving Large Language Models for Competitive Program Generation", "authors": ["Minnan Wei", "Ziming Li", "Xiang Chen", "Menglin Zheng", "Ziyan Qu", "Cheng Yu", "Siyu Chen", "Xiaolin Ju"], "summary": "Context: Due to the demand for strong algorithmic reasoning, complex logic\nimplementation, and strict adherence to input/output formats and resource\nconstraints, competitive programming generation by large language models (LLMs)\nis considered the most challenging problem in current LLM-based code\ngeneration. However, previous studies often evaluate LLMs using simple prompts\nand benchmark datasets prone to data leakage. Moreover, prior work has limited\nconsideration of the diversity in algorithm types and difficulty levels.\nObjective: In this study, we aim to evaluate and improve LLMs in solving\nreal-world competitive programming problems. Methods: We initially collect 117\nproblems from nine regional ICPC/CCPC contests held in 2024 and design four\nfiltering criteria to construct a curated benchmark consisting of 80 problems.\nLeveraging DeepSeek-R1 as the LLM, we evaluate its competitive program\ngeneration capabilities through the online judge (OJ) platforms, guided by a\ncarefully designed basic prompt. For incorrect submissions, we construct a\nfine-grained error taxonomy and then propose a targeted improvement framework\nby combining a multi-turn dialogue-based repair phase and an\ninformation-augmented regeneration phase. Results: Experimental results show\nthat only 5 out of 80 problems are fully accepted when using basic prompts. For\nthe unsolved problems, we construct the error taxonomy, including general\nerrors (such as design, boundary, condition, data type, syntax, and\ninput/output errors) and specialized errors (such as those in mathematical\nproblems, greedy algorithms, and graph theories). After applying our proposed\nimprovement strategies, we substantially increased the number of correct\nsolutions, with 46 out of 80 problems successfully accepted.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22954v1", "categories": ["cs.SI", "cs.SE"], "cate": "cs.SI", "url": "http://arxiv.org/abs/2506.22954v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.22688", "title": "An LLM-assisted approach to designing software architectures using ADD", "authors": ["Humberto Cervantes", "Rick Kazman", "Yuanfang Cai"], "summary": "Designing effective software architectures is a complex, iterative process\nthat traditionally relies on expert judgment. This paper proposes an approach\nfor Large Language Model (LLM)-assisted software architecture design using the\nAttribute-Driven Design (ADD) method. By providing an LLM with an explicit\ndescription of ADD, an architect persona, and a structured iteration plan, our\nmethod guides the LLM to collaboratively produce architecture artifacts with a\nhuman architect. We validate the approach through case studies, comparing\ngenerated designs against proven solutions and evaluating them with\nprofessional architects. Results show that our LLM-assisted ADD process can\ngenerate architectures closely aligned with established solutions and partially\nsatisfying architectural drivers, highlighting both the promise and current\nlimitations of using LLMs in architecture design. Our findings emphasize the\nimportance of human oversight and iterative refinement when leveraging LLMs in\nthis domain.", "comment": "30 pages, 12 figures, 7 tables", "pdf_url": "http://arxiv.org/pdf/2506.22688v1", "categories": ["cs.SE", "D.2.11; D.2.2"], "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.22688v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22583", "title": "Supra-threshold control of peripheral LOD", "authors": ["Benjamin Watson", "Neff Walker", "Larry F Hodges"], "summary": "Level of detail (LOD) is widely used to control visual feedback in\ninteractive applications. LOD control is typically based on perception at\nthreshold - the conditions in which a stimulus first becomes perceivable. Yet\nmost LOD manipulations are quite perceivable and occur well above threshold.\nMoreover, research shows that supra-threshold perception differs drastically\nfrom perception at threshold. In that case, should supra-threshold LOD control\nalso differ from LOD control at threshold?\n  In two experiments, we examine supra-threshold LOD control in the visual\nperiphery and find that indeed, it should differ drastically from LOD control\nat threshold. Specifically, we find that LOD must support a task-dependent\nlevel of reliable perceptibility. Above that level, perceptibility of LOD\ncontrol manipulations should be minimized, and detail contrast is a better\npredictor of perceptibility than detail size. Below that level, perceptibility\nmust be maximized, and LOD should be improved as eccentricity rises or contrast\ndrops. This directly contradicts prevailing threshold-based LOD control\nschemes, and strongly suggests a reexamination of LOD control for foveal\ndisplay.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22583v1", "categories": ["cs.HC", "cs.GR"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.22583v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.23052", "title": "Flexible Intelligent Metasurface for Enhancing Multi-Target Wireless Sensing", "authors": ["Zihao Teng", "Jiancheng An", "Lu Gan", "Naofal Al-Dhahir", "Zhu Han"], "summary": "Flexible intelligent metasurface (FIM) has emerged as a transformative\ntechnology to enhance wireless sensing by dynamically morphing its\nthree-dimensional (3D) surface shape and electromagnetic response. Unlike\nconventional rigid arrays, an FIM consists of low-cost radiating elements that\ncan independently adjust their positions and radiation characteristics, thereby\nallowing for real-time optimization of the sensing environment. This paper\ninvestigates the impact of FIM on wireless sensing performance. Specifically,\nwe focus on the maximization of the cumulated power of the probing signals at\nthe target locations under the per-antenna power constraint by jointly\noptimizing the transmit covariance matrix and the surface shape of the\ntransmitting FIM. We propose a block coordinate descend (BCD) algorithm to find\na locally optimal solution, by alternatively updating the FIM surface shape and\nthe transmit covariance matrix, while keeping the other one fixed at each step.\nFurthermore, we analyze the computational complexity and convergence properties\nof the proposed algorithm and demonstrate that FIM enhances wireless sensing by\nproviding a new design degree-of-freedom to coordinate the correlation between\nsteering vectors at different angles. Numerical results demonstrate that FIM\nsignificantly improves wireless sensing performance under the considered\nmulti-target scenario.", "comment": "7 pages, 3 figures, accepted by IEEE TVT", "pdf_url": "http://arxiv.org/pdf/2506.23052v1", "categories": ["cs.IT", "eess.SP", "math.IT"], "cate": "cs.IT", "url": "http://arxiv.org/abs/2506.23052v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.22772", "title": "Approximate Logic Synthesis Using BLASYS", "authors": ["Jingxiao Ma", "Soheil Hashemi", "Sherief Reda"], "summary": "Approximate computing is an emerging paradigm where design accuracy can be\ntraded for improvements in design metrics such as design area and power\nconsumption. In this work, we overview our open-source tool, BLASYS, for\nsynthesis of approximate circuits using Boolean Matrix Factorization (BMF). In\nour methodology the truth table of a given circuit is approximated using BMF to\na controllable approximation degree, and the results of the factorization are\nused to synthesize the approximate circuit output. BLASYS scales up the\ncomputations to large circuits through the use of partition techniques, where\nan input circuit is partitioned into a number of interconnected subcircuits and\nthen a design-space exploration technique identifies the best order for\nsubcircuit approximations. BLASYS leads to a graceful trade-off between\naccuracy and full circuit complexity as measured by design area. Using an\nopen-source design flow, we extensively evaluate our methodology on a number of\nbenchmarks, where we demonstrate that the proposed methodology can achieve on\naverage 48.14% in area savings, while introducing an average relative error of\n5%.", "comment": "Published in the Workshop on Open-Source EDA Technology (WOSET),\n  2019. (Workshop link: https://woset-workshop.github.io/WOSET2019.html)", "pdf_url": "http://arxiv.org/pdf/2506.22772v1", "categories": ["cs.AR", "B.6.1; B.2.4; B.8.2"], "cate": "cs.AR", "url": "http://arxiv.org/abs/2506.22772v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.22773", "title": "Not All Water Consumption Is Equal: A Water Stress Weighted Metric for Sustainable Computing", "authors": ["Yanran Wu", "Inez Hua", "Yi Ding"], "summary": "Water consumption is an increasingly critical dimension of computing\nsustainability, especially as AI workloads rapidly scale. However, current\nwater impact assessment often overlooks where and when water stress is more\nsevere. To fill in this gap, we present SCARF, the first general framework that\nevaluates water impact of computing by factoring in both spatial and temporal\nvariations in water stress. SCARF calculates an Adjusted Water Impact (AWI)\nmetric that considers both consumption volume and local water stress over time.\nThrough three case studies on LLM serving, datacenters, and semiconductor\nfabrication plants, we show the hidden opportunities for reducing water impact\nby optimizing location and time choices, paving the way for water-sustainable\ncomputing. The code is available at https://github.com/jojacola/SCARF.", "comment": "7 pages, 9 figures, HotCarbon '25: Proceedings of the 4th Workshop on\n  Sustainable Computer Systems, Cambridge, Massachusetts (USA), July 10-11th,\n  2025", "pdf_url": "http://arxiv.org/pdf/2506.22773v1", "categories": ["cs.DC", "cs.AR", "cs.CY", "cs.LG"], "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.22773v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.22449", "title": "Computational Analysis of Climate Policy", "authors": ["Carolyn Hicks"], "summary": "This thesis explores the impact of the Climate Emergency movement on local\ngovernment climate policy, using computational methods. The Climate Emergency\nmovement sought to accelerate climate action at local government level through\nthe mechanism of Climate Emergency Declarations (CEDs), resulting in a series\nof commitments from councils to treat climate change as an emergency. With the\naim of assessing the potential of current large language models to answer\ncomplex policy questions, I first built and configured a system named PALLM\n(Policy Analysis with a Large Language Model), using the OpenAI model GPT-4.\nThis system is designed to apply a conceptual framework for climate emergency\nresponse plans to a dataset of climate policy documents. I validated the\nperformance of this system with the help of local government policymakers, by\ngenerating analyses of the climate policies of 11 local governments in Victoria\nand assessing the policymakers' level of agreement with PALLM's responses.\nHaving established that PALLM's performance is satisfactory, I used it to\nconduct a large-scale analysis of current policy documents from local\ngovernments in the state of Victoria, Australia. This thesis presents the\nmethodology and results of this analysis, comparing the results for councils\nwhich have passed a CED to those which did not. This study finds that GPT-4 is\ncapable of high-level policy analysis, with limitations including a lack of\nreliable attribution, and can also enable more nuanced analysis by researchers.\nIts use in this research shows that councils which have passed a CED are more\nlikely to have a recent and climate-specific policy, and show more attention to\nurgency, prioritisation, and equity and social justice, than councils which\nhave not. It concludes that the ability to assess policy documents at scale\nopens up exciting new opportunities for policy researchers.", "comment": "Master's thesis", "pdf_url": "http://arxiv.org/pdf/2506.22449v1", "categories": ["cs.CY", "cs.CL"], "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.22449v1", "date": "2025-06-13", "updated": "2025-06-13"}
{"id": "2506.22438", "title": "Counting with Confidence: Accurate Pest Monitoring in Water Traps", "authors": ["Xumin Gao", "Mark Stevens", "Grzegorz Cielniak"], "summary": "Accurate pest population monitoring and tracking their dynamic changes are\ncrucial for precision agriculture decision-making. A common limitation in\nexisting vision-based automatic pest counting research is that models are\ntypically evaluated on datasets with ground truth but deployed in real-world\nscenarios without assessing the reliability of counting results due to the lack\nof ground truth. To this end, this paper proposed a method for comprehensively\nevaluating pest counting confidence in the image, based on information related\nto counting results and external environmental conditions. First, a pest\ndetection network is used for pest detection and counting, extracting counting\nresult-related information. Then, the pest images undergo image quality\nassessment, image complexity assessment, and pest distribution uniformity\nassessment. And the changes in image clarity caused by stirring during image\nacquisition are quantified by calculating the average gradient magnitude.\nNotably, we designed a hypothesis-driven multi-factor sensitivity analysis\nmethod to select the optimal image quality assessment and image complexity\nassessment methods. And we proposed an adaptive DBSCAN clustering algorithm for\npest distribution uniformity assessment. Finally, the obtained information\nrelated to counting results and external environmental conditions is input into\na regression model for prediction, resulting in the final pest counting\nconfidence. To the best of our knowledge, this is the first study dedicated to\ncomprehensively evaluating counting confidence in counting tasks, and\nquantifying the relationship between influencing factors and counting\nconfidence through a model. Experimental results show our method reduces MSE by\n31.7% and improves R2 by 15.2% on the pest counting confidence test set,\ncompared to the baseline built primarily on information related to counting\nresults.", "comment": "\\c{opyright} 20XX the authors. This work has been accepted to IFAC\n  for publication under a Creative Commons Licence CC-BY-NC-ND", "pdf_url": "http://arxiv.org/pdf/2506.22438v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22438v1", "date": "2025-05-19", "updated": "2025-05-19"}
{"id": "2506.23578", "title": "Reachability in symmetric VASS", "authors": ["≈Åukasz Kami≈Ñski", "S≈Çawomir Lasota"], "summary": "We investigate the reachability problem in symmetric vector addition systems\nwith states (VASS), where transitions are invariant under a group of\npermutations of coordinates. One extremal case, the trivial groups, yields\ngeneral VASS. In another extremal case, the symmetric groups, we show that the\nreachability problem can be solved in PSPACE, regardless of the dimension of\ninput VASS (to be contrasted with Ackermannian complexity in general VASS). We\nalso consider other groups, in particular alternating and cyclic ones.\nFurthermore, motivated by the open status of the reachability problem in data\nVASS, we estimate the gain in complexity when the group arises as a combination\nof the trivial and symmetric groups.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23578v1", "categories": ["cs.FL", "cs.CL"], "cate": "cs.FL", "url": "http://arxiv.org/abs/2506.23578v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22870", "title": "Improved design of an active landing gear for a passenger aircraft using multi-objective optimization technique", "authors": ["Milad Zarchi", "Behrooz Attaran"], "summary": "The landing gear system is a major aircraft subsystem that must withstand\nextreme forces during ground maneuvers and absorb vibrations. While traditional\nsystems perform well under normal conditions, their efficiency drops under\nvarying landing and runway scenarios. This study addresses this issue by\nsimultaneously optimizing controller coefficients, parameters of a nonlinear\nhydraulic actuator integrated into the traditional shock absorber, and a\nvibration absorber using a bee-inspired multi-objective algorithm. To\ndemonstrate adaptability, the paper includes sensitivity analysis for\nthree-point landings affected by added payload and touchdown speed, and\nrobustness analysis for one- and two-point landings under emergency wind\nconditions. The dynamic flight equations of an Airbus A320-200 during landing\nare derived and solved numerically. Results show that the active shock absorber\nsystem, optimized via two bee-based algorithms, outperforms the passive system\nin reducing bounce and pitch displacements and momenta, suspension travel, and\nimpact force in both time and frequency domains. This leads to significantly\nimproved passenger comfort and potentially longer structural fatigue life,\ndemonstrating industrial applicability.", "comment": "21 pages, 24 Figures,", "pdf_url": "http://arxiv.org/pdf/2506.22870v1", "categories": ["cs.CE"], "cate": "cs.CE", "url": "http://arxiv.org/abs/2506.22870v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.22579", "title": "Data-Efficient Excavation Force Estimation for Wheel Loaders", "authors": ["Armin Abdolmohammadi", "Navid Mojahed", "Shima Nazari", "Bahram Ravani"], "summary": "Accurate excavation force prediction is essential for enabling autonomous\noperation and optimizing control strategies in earthmoving machinery.\nConventional methods typically require extensive data collection or simulations\nacross diverse soil types, limiting scalability and adaptability. This paper\nproposes a data-efficient framework that calibrates soil parameters using force\ndata from the prior bucket-loading cycle. Leveraging an analytical soil-tool\ninteraction model, the fundamental earthmoving equation (FEE), our approach\nuses a multi-stage optimization strategy, on soil parameters during the loading\nphase. These fitted parameters are then used to predict excavation forces in\nthe upcoming digging cycle, allowing the system to adapt its control inputs\nwithout the need for extensive data collection or machine learning-based model\ntraining. The framework is validated in high-fidelity simulations using the\nAlgoryx Dynamics engine, across multiple soil types and excavation\ntrajectories, demonstrating accurate force predictions with root-mean-square\nerrors of 10\\% to 15\\% in primary test cases. This cycle-to-cycle adaptation\nstrategy showcases the potential for online and scalable efficient path\nplanning for wheel loader operations.", "comment": "Preprint version of the paper submitted to IEEE Transaction of\n  Vehicular Technology", "pdf_url": "http://arxiv.org/pdf/2506.22579v1", "categories": ["eess.SY", "cs.SY"], "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.22579v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22454", "title": "Microelectrode Signal Dynamics as Biomarkers of Subthalamic Nucleus Entry on Deep Brain Stimulation: A Nonlinear Feature Approach", "authors": ["Ana Luiza S. Tavares", "Artur Pedro M. Neto", "Francinaldo L. Gomes", "Paul Rodrigo dos Reis", "Arthur G. da Silva", "Antonio P. Junior", "Bruno D. Gomes"], "summary": "Accurate intraoperative localization of the subthalamic nucleus (STN) is\nessential for the efficacy of Deep Brain Stimulation (DBS) in patients with\nParkinson's disease. While microelectrode recordings (MERs) provide rich\nelectrophysiological information during DBS electrode implantation, current\nlocalization practices often rely on subjective interpretation of signal\nfeatures. In this study, we propose a quantitative framework that leverages\nnonlinear dynamics and entropy-based metrics to classify neural activity\nrecorded inside versus outside the STN. MER data from three patients were\npreprocessed using a robust artifact correction pipeline, segmented, and\nlabelled based on surgical annotations. A comprehensive set of recurrence\nquantification analysis, nonlinear, and entropy features were extracted from\neach segment. Multiple supervised classifiers were trained on every combination\nof feature domains using stratified 10-fold cross-validation, followed by\nstatistical comparison using paired Wilcoxon signed-rank tests with\nHolm-Bonferroni correction. The combination of entropy and nonlinear features\nyielded the highest discriminative power, and the Extra Trees classifier\nemerged as the best model with a cross-validated F1-score of 0.902+/-0.027 and\nROC AUC of 0.887+/-0.055. Final evaluation on a 20% hold-out test set confirmed\nrobust generalization (F1= 0.922, ROC AUC = 0.941). These results highlight the\npotential of nonlinear and entropy signal descriptors in supporting real-time,\ndata-driven decision-making during DBS surgeries", "comment": "8 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2506.22454v1", "categories": ["eess.SP", "cs.LG"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.22454v1", "date": "2025-06-14", "updated": "2025-06-14"}
{"id": "2506.22580", "title": "FedCLAM: Client Adaptive Momentum with Foreground Intensity Matching for Federated Medical Image Segmentation", "authors": ["Vasilis Siomos", "Jonathan Passerat-Palmbach", "Giacomo Tarroni"], "summary": "Federated learning is a decentralized training approach that keeps data under\nstakeholder control while achieving superior performance over isolated\ntraining. While inter-institutional feature discrepancies pose a challenge in\nall federated settings, medical imaging is particularly affected due to diverse\nimaging devices and population variances, which can diminish the global model's\neffectiveness. Existing aggregation methods generally fail to adapt across\nvaried circumstances. To address this, we propose FedCLAM, which integrates\n\\textit{client-adaptive momentum} terms derived from each client's loss\nreduction during local training, as well as a \\textit{personalized dampening\nfactor} to curb overfitting. We further introduce a novel \\textit{intensity\nalignment} loss that matches predicted and ground-truth foreground\ndistributions to handle heterogeneous image intensity profiles across\ninstitutions and devices. Extensive evaluations on two datasets show that\nFedCLAM surpasses eight cutting-edge methods in medical segmentation tasks,\nunderscoring its efficacy. The code is available at\nhttps://github.com/siomvas/FedCLAM.", "comment": "10 pages, 2 figures, Accepted at MICCAI 2025", "pdf_url": "http://arxiv.org/pdf/2506.22580v1", "categories": ["eess.IV", "cs.AI", "cs.CV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.22580v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22849", "title": "DOBB-BVH: Efficient Ray Traversal by Transforming Wide BVHs into Oriented Bounding Box Trees using Discrete Rotations", "authors": ["Michael A. Kern", "Alain Galvan", "David Oldcorn", "Daniel Skinner", "Rohan Mehalwal", "Leo Reyes Lozano", "Matth√§us G. Chajdas"], "summary": "Oriented bounding box (OBB) bounding volume hierarchies offer a more precise\nfit than axis-aligned bounding box hierarchies in scenarios with thin elongated\nand arbitrarily rotated geometry, enhancing intersection test performance in\nray tracing. However, determining optimally oriented bounding boxes can be\ncomputationally expensive and have high memory requirements. Recent research\nhas shown that pre-built hierarchies can be efficiently converted to OBB\nhierarchies on the GPU in a bottom-up pass, yielding significant ray tracing\ntraversal improvements. In this paper, we introduce a novel OBB construction\ntechnique where all internal node children share a consistent OBB transform,\nchosen from a fixed set of discrete quantized rotations. This allows for\nefficient encoding and reduces the computational complexity of OBB\ntransformations. We further extend our approach to hierarchies with multiple\nchildren per node by leveraging Discrete Orientation Polytopes (k-DOPs),\ndemonstrating improvements in traversal performance while limiting the build\ntime impact for real-time applications. Our method is applied as a\npost-processing step, integrating seamlessly into existing hierarchy\nconstruction pipelines. Despite a 12.6% increase in build time, our\nexperimental results demonstrate an average improvement of 18.5% in primary,\n32.4% in secondary rays, and maximum gain of 65% in ray intersection\nperformance, highlighting its potential for advancing real-time applications.", "comment": "10 pages main content, 3 pages appendix", "pdf_url": "http://arxiv.org/pdf/2506.22849v1", "categories": ["cs.GR"], "cate": "cs.GR", "url": "http://arxiv.org/abs/2506.22849v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.22619", "title": "On Finding $\\ell$-th Smallest Perfect Matchings", "authors": ["Nicolas El Maalouly", "Sebastian Haslebacher", "Adrian Taubner", "Lasse Wulf"], "summary": "Given an undirected weighted graph $G$ and an integer $k$, Exact-Weight\nPerfect Matching (EWPM) is the problem of finding a perfect matching of weight\nexactly $k$ in $G$. In this paper, we study EWPM and its variants. The EWPM\nproblem is famous, since in the case of unary encoded weights, Mulmuley,\nVazirani, and Vazirani showed almost 40 years ago that the problem can be\nsolved in randomized polynomial time. However, up to this date no\nderandomization is known.\n  Our first result is a simple deterministic algorithm for EWPM that runs in\ntime $n^{O(\\ell)}$, where $\\ell$ is the number of distinct weights that perfect\nmatchings in $G$ can take. In fact, we show how to find an $\\ell$-th smallest\nperfect matching in any weighted graph (even if the weights are encoded in\nbinary, in which case EWPM in general is known to be NP-complete) in time\n$n^{O(\\ell)}$ for any integer $\\ell$. Similar next-to-optimal variants have\nalso been studied recently for the shortest path problem.\n  For our second result, we extend the list of problems that are known to be\nequivalent to EWPM. We show that EWPM is equivalent under a weight-preserving\nreduction to the Exact Cycle Sum problem (ECS) in undirected graphs with a\nconservative (i.e. no negative cycles) weight function. To the best of our\nknowledge, we are the first to study this problem. As a consequence, the latter\nproblem is contained in RP if the weights are encoded in unary. Finally, we\nidentify a special case of EWPM, called BCPM, which was recently studied by El\nMaalouly, Steiner and Wulf. We show that BCPM is equivalent under a\nweight-preserving transformation to another problem recently studied by\nSchlotter and Seb\\H{o} as well as Geelen and Kapadia: the Shortest Odd Cycle\nproblem (SOC) in undirected graphs with conservative weights.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22619v1", "categories": ["cs.DS"], "cate": "cs.DS", "url": "http://arxiv.org/abs/2506.22619v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22442", "title": "Features-based embedding or Feature-grounding", "authors": ["Piotr Makarevich"], "summary": "In everyday reasoning, when we think about a particular object, we associate\nit with a unique set of expected properties such as weight, size, or more\nabstract attributes like density or horsepower. These expectations are shaped\nby our prior knowledge and the conceptual categories we have formed through\nexperience. This paper investigates how such knowledge-based structured\nthinking can be reproduced in deep learning models using features based\nembeddings. Specially, it introduces an specific approach to build\nfeature-grounded embedding, aiming to align shareable representations of\noperable dictionary with interpretable domain-specific conceptual features.", "comment": "13 pages, 12 figures", "pdf_url": "http://arxiv.org/pdf/2506.22442v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22442v1", "date": "2025-06-11", "updated": "2025-06-11"}
{"id": "2506.22609", "title": "Ludax: A GPU-Accelerated Domain Specific Language for Board Games", "authors": ["Graham Todd", "Alexander G. Padula", "Dennis J. N. J. Soemers", "Julian Togelius"], "summary": "Games have long been used as benchmarks and testing environments for research\nin artificial intelligence. A key step in supporting this research was the\ndevelopment of game description languages: frameworks that compile\ndomain-specific code into playable and simulatable game environments, allowing\nresearchers to generalize their algorithms and approaches across multiple games\nwithout having to manually implement each one. More recently, progress in\nreinforcement learning (RL) has been largely driven by advances in hardware\nacceleration. Libraries like JAX allow practitioners to take full advantage of\ncutting-edge computing hardware, often speeding up training and testing by\norders of magnitude. Here, we present a synthesis of these strands of research:\na domain-specific language for board games which automatically compiles into\nhardware-accelerated code. Our framework, Ludax, combines the generality of\ngame description languages with the speed of modern parallel processing\nhardware and is designed to fit neatly into existing deep learning pipelines.\nWe envision Ludax as a tool to help accelerate games research generally, from\nRL to cognitive science, by enabling rapid simulation and providing a flexible\nrepresentation scheme. We present a detailed breakdown of Ludax's description\nlanguage and technical notes on the compilation process, along with speed\nbenchmarking and a demonstration of training RL agents. The Ludax framework,\nalong with implementations of existing board games, is open-source and freely\navailable.", "comment": "18 pages, 3 figures", "pdf_url": "http://arxiv.org/pdf/2506.22609v1", "categories": ["cs.AI"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.22609v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.23026", "title": "Machine Assistant with Reliable Knowledge: Enhancing Student Learning via RAG-based Retrieval", "authors": ["Yongsheng Lian"], "summary": "We present Machine Assistant with Reliable Knowledge (MARK), a\nretrieval-augmented question-answering system designed to support student\nlearning through accurate and contextually grounded responses. The system is\nbuilt on a retrieval-augmented generation (RAG) framework, which integrates a\ncurated knowledge base to ensure factual consistency. To enhance retrieval\neffectiveness across diverse question types, we implement a hybrid search\nstrategy that combines dense vector similarity with sparse keyword-based\nretrieval. This dual-retrieval mechanism improves robustness for both general\nand domain-specific queries. The system includes a feedback loop in which\nstudents can rate responses and instructors can review and revise them.\nInstructor corrections are incorporated into the retrieval corpus, enabling\nadaptive refinement over time. The system was deployed in a classroom setting\nas a substitute for traditional office hours, where it successfully addressed a\nbroad range of student queries. It was also used to provide technical support\nby integrating with a customer-specific knowledge base, demonstrating its\nability to handle routine, context-sensitive tasks in applied domains. MARK is\npublicly accessible at https://app.eduquery.ai.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23026v1", "categories": ["cs.IR"], "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.23026v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.22972", "title": "Adaptable Non-parametric Approach for Speech-based Symptom Assessment: Isolating Private Medical Data in a Retrieval Datastore", "authors": ["Yu-Wen Chen", "Julia Hirschberg"], "summary": "The automatic assessment of health-related acoustic cues has the potential to\nimprove healthcare accessibility and affordability. Although parametric models\nare promising, they face challenges in privacy and adaptability. To address\nthese, we propose a NoN-Parametric framework for Speech-based symptom\nAssessment (NoNPSA). By isolating medical data in a retrieval datastore, NoNPSA\navoids encoding private information in model parameters and enables efficient\ndata updates. A self-supervised learning (SSL) model pre-trained on\ngeneral-purpose datasets extracts features, which are used for similarity-based\nretrieval. Metadata-aware refinement filters the retrieved data, and associated\nlabels are used to compute an assessment score. Experimental results show that\nNoNPSA achieves competitive performance compared to fine-tuning SSL-based\nmethods, while enabling greater privacy, update efficiency, and\nadaptability--showcasing the potential of non-parametric approaches in\nhealthcare.", "comment": "IEEE MLSP 2025", "pdf_url": "http://arxiv.org/pdf/2506.22972v1", "categories": ["eess.AS"], "cate": "eess.AS", "url": "http://arxiv.org/abs/2506.22972v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23730", "title": "One-Parametric Presburger Arithmetic has Quantifier Elimination", "authors": ["Alessio Mansutti", "Mikhail R. Starchak"], "summary": "We give a quantifier elimination procedure for one-parametric Presburger\narithmetic, the extension of Presburger arithmetic with the function $x \\mapsto\nt \\cdot x$, where $t$ is a fixed free variable ranging over the integers. This\nresolves an open problem proposed in [Bogart et al., Discrete Analysis, 2017].\nAs conjectured in [Goodrick, Arch. Math. Logic, 2018], quantifier elimination\nis obtained for the extended structure featuring all integer division functions\n$x \\mapsto \\lfloor{\\frac{x}{f(t)}}\\rfloor$, one for each integer polynomial\n$f$.\n  Our algorithm works by iteratively eliminating blocks of existential\nquantifiers. The elimination of a block builds on two sub-procedures, both\nrunning in non-deterministic polynomial time. The first one is an adaptation of\na recently developed and efficient quantifier elimination procedure for\nPresburger arithmetic, modified to handle formulae with coefficients over the\nring $\\mathbb{Z}[t]$ of univariate polynomials. The second is reminiscent of\nthe so-called \"base $t$ division method\" used by Bogart et al. As a result, we\ndeduce that the satisfiability problem for the existential fragment of\none-parametric Presburger arithmetic (which encompasses a broad class of\nnon-linear integer programs) is in NP, and that the smallest solution to a\nsatisfiable formula in this fragment is of polynomial bit size.", "comment": "Extended version of a MFCS 2025 paper", "pdf_url": "http://arxiv.org/pdf/2506.23730v1", "categories": ["cs.LO", "cs.SC"], "cate": "cs.LO", "url": "http://arxiv.org/abs/2506.23730v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22485", "title": "AI Agents-as-Judge: Automated Assessment of Accuracy, Consistency, Completeness and Clarity for Enterprise Documents", "authors": ["Sudip Dasgupta", "Himanshu Shankar"], "summary": "This study presents a modular, multi-agent system for the automated review of\nhighly structured enterprise business documents using AI agents. Unlike prior\nsolutions focused on unstructured texts or limited compliance checks, this\nframework leverages modern orchestration tools such as LangChain, CrewAI,\nTruLens, and Guidance to enable section-by-section evaluation of documents for\naccuracy, consistency, completeness, and clarity. Specialized agents, each\nresponsible for discrete review criteria such as template compliance or factual\ncorrectness, operate in parallel or sequence as required. Evaluation outputs\nare enforced to a standardized, machine-readable schema, supporting downstream\nanalytics and auditability. Continuous monitoring and a feedback loop with\nhuman reviewers allow for iterative system improvement and bias mitigation.\n  Quantitative evaluation demonstrates that the AI Agent-as-Judge system\napproaches or exceeds human performance in key areas: achieving 99% information\nconsistency (vs. 92% for humans), halving error and bias rates, and reducing\naverage review time from 30 to 2.5 minutes per document, with a 95% agreement\nrate between AI and expert human judgment. While promising for a wide range of\nindustries, the study also discusses current limitations, including the need\nfor human oversight in highly specialized domains and the operational cost of\nlarge-scale LLM usage. The proposed system serves as a flexible, auditable, and\nscalable foundation for AI-driven document quality assurance in the enterprise\ncontext.", "comment": "17 pages, 2 system diagrams, 1 table, no prior conference publication", "pdf_url": "http://arxiv.org/pdf/2506.22485v1", "categories": ["cs.CL", "cs.AI", "68T07, 68T50", "I.2.1; I.2.3; I.2.7; H.3.3"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.22485v1", "date": "2025-06-23", "updated": "2025-06-23"}
{"id": "2506.22615", "title": "Error Estimates for the Arnoldi Approximation of a Matrix Square Root", "authors": ["James H. Adler", "Xiaozhe Hu", "Wenxiao Pan", "Zhongqin Xue"], "summary": "The Arnoldi process provides an efficient framework for approximating\nfunctions of a matrix applied to a vector, i.e., of the form $f(M)\\mathbf{b}$,\nby repeated matrix-vector multiplications. In this paper, we derive an\n\\textit{a priori} error estimate for approximating the action of a matrix\nsquare root using the Arnoldi process, where the integral representation of the\nerror is reformulated in terms of the error for solving the linear system\n$M\\mathbf{x}=\\mathbf{b}$. The results extend the error analysis of the Lanczos\nmethod for Hermitian matrices in [Chen et al., SIAM J. Matrix Anal. Appl.,\n2022] to non-Hermitian cases. Furthermore, to make the method applicable to\nlarge-scale problems, we assume that the matrices are preprocessed utilizing\ndata-sparse approximations preserving positive definiteness, and then establish\na refined error bound in this setting. The numerical results on matrices with\ndifferent structures demonstrate that our theoretical analysis yields a\nreliable upper bound. Finally, simulations on large-scale matrices arising in\nparticulate suspensions validate the effectiveness and practicality of the\napproach.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22615v1", "categories": ["math.NA", "cs.NA", "65F60 65Z05 70-10"], "cate": "math.NA", "url": "http://arxiv.org/abs/2506.22615v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.23734", "title": "Marker Gene Method : Identifying Stable Solutions in a Dynamic Environment", "authors": ["Hao Shi", "Xi Li", "Fangfang Xie"], "summary": "Competitive Co-evolutionary Algorithms (CCEAs) are often hampered by complex\ndynamics like intransitivity and the Red Queen effect, leading to unstable\nconvergence. To counter these challenges, this paper introduces the Marker Gene\nMethod (MGM), a framework that establishes stability by using a 'marker gene'\nas a dynamic benchmark and an adaptive weighting mechanism to balance\nexploration and exploitation. We provide rigorous mathematical proofs\ndemonstrating that MGM creates strong attractors near Nash Equilibria within\nthe Strictly Competitive Game framework. Empirically, MGM demonstrates its\nefficacy across a spectrum of challenges: it stabilizes the canonical\nRock-Paper-Scissors game, significantly improves the performance of C-RMOEA/D\non ZDT benchmarks, and, when augmented with a Memory Pool (MP) extension, it\nsuccessfully tames the notoriously pathological Shapley Biased Game. This work\npresents a theoretically sound and empirically validated framework that\nsubstantially enhances the stability and robustness of CCEAs in complex\ncompetitive environments.", "comment": "Submitted to IEEE Transactions on Evolutionary Computation. 13 pages,\n  10 figures. Supplementary material is included", "pdf_url": "http://arxiv.org/pdf/2506.23734v1", "categories": ["cs.NE", "cs.AI", "cs.GT"], "cate": "cs.NE", "url": "http://arxiv.org/abs/2506.23734v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22661", "title": "Enhancing Neural Audio Fingerprint Robustness to Audio Degradation for Music Identification", "authors": ["R. Oguz Araz", "Guillem Cort√®s-Sebasti√†", "Emilio Molina", "Joan Serr√†", "Xavier Serra", "Yuki Mitsufuji", "Dmitry Bogdanov"], "summary": "Audio fingerprinting (AFP) allows the identification of unknown audio content\nby extracting compact representations, termed audio fingerprints, that are\ndesigned to remain robust against common audio degradations. Neural AFP methods\noften employ metric learning, where representation quality is influenced by the\nnature of the supervision and the utilized loss function. However, recent work\nunrealistically simulates real-life audio degradation during training,\nresulting in sub-optimal supervision. Additionally, although several modern\nmetric learning approaches have been proposed, current neural AFP methods\ncontinue to rely on the NT-Xent loss without exploring the recent advances or\nclassical alternatives. In this work, we propose a series of best practices to\nenhance the self-supervision by leveraging musical signal properties and\nrealistic room acoustics. We then present the first systematic evaluation of\nvarious metric learning approaches in the context of AFP, demonstrating that a\nself-supervised adaptation of the triplet loss yields superior performance. Our\nresults also reveal that training with multiple positive samples per anchor has\ncritically different effects across loss functions. Our approach is built upon\nthese insights and achieves state-of-the-art performance on both a large,\nsynthetically degraded dataset and a real-world dataset recorded using\nmicrophones in diverse music venues.", "comment": "Accepted to ISMIR2025", "pdf_url": "http://arxiv.org/pdf/2506.22661v1", "categories": ["cs.SD", "eess.AS"], "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.22661v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22477", "title": "Innovative Research on IoT Architecture and Robotic Operating Platforms: Applications of Large Language Models and Generative AI", "authors": ["Huiwen Han"], "summary": "This paper introduces an innovative design for robotic operating platforms,\nunderpinned by a transformative Internet of Things (IoT) architecture,\nseamlessly integrating cutting-edge technologies such as large language models\n(LLMs), generative AI, edge computing, and 5G networks. The proposed platform\naims to elevate the intelligence and autonomy of IoT systems and robotics,\nenabling them to make real-time decisions and adapt dynamically to changing\nenvironments. Through a series of compelling case studies across industries\nincluding smart manufacturing, healthcare, and service sectors, this paper\ndemonstrates the substantial potential of IoT-enabled robotics to optimize\noperational workflows, enhance productivity, and deliver innovative, scalable\nsolutions. By emphasizing the roles of LLMs and generative AI, the research\nhighlights how these technologies drive the evolution of intelligent robotics\nand IoT, shaping the future of industry-specific advancements. The findings not\nonly showcase the transformative power of these technologies but also offer a\nforward-looking perspective on their broader societal and industrial\nimplications, positioning them as catalysts for next-generation automation and\ntechnological convergence.", "comment": "Published in: 2024 6th International Conference on Robotics,\n  Intelligent Control and Artificial Intelligence (RICAI), IEEE Xplore, DOI:\n  10.1109/RICAI64321.2024.10911316. \\c{opyright} 2024 IEEE", "pdf_url": "http://arxiv.org/pdf/2506.22477v1", "categories": ["cs.NI", "cs.AI", "cs.ET", "cs.RO"], "cate": "cs.NI", "url": "http://arxiv.org/abs/2506.22477v1", "date": "2025-06-22", "updated": "2025-06-22"}
{"id": "2506.22557", "title": "MetaCipher: A General and Extensible Reinforcement Learning Framework for Obfuscation-Based Jailbreak Attacks on Black-Box LLMs", "authors": ["Boyuan Chen", "Minghao Shao", "Abdul Basit", "Siddharth Garg", "Muhammad Shafique"], "summary": "The growing capabilities of large language models (LLMs) have exposed them to\nincreasingly sophisticated jailbreak attacks. Among these, obfuscation-based\nattacks -- which encrypt malicious content to evade detection -- remain highly\neffective. By leveraging the reasoning ability of advanced LLMs to interpret\nencrypted prompts, such attacks circumvent conventional defenses that rely on\nkeyword detection or context filtering. These methods are very difficult to\ndefend against, as existing safety mechanisms are not designed to interpret or\ndecode ciphered content. In this work, we propose \\textbf{MetaCipher}, a novel\nobfuscation-based jailbreak framework, along with a reinforcement\nlearning-based dynamic cipher selection mechanism that adaptively chooses\noptimal encryption strategies from a cipher pool. This approach enhances\njailbreak effectiveness and generalizability across diverse task types, victim\nLLMs, and safety guardrails. Our framework is modular and extensible by design,\nsupporting arbitrary cipher families and accommodating evolving adversarial\nstrategies. We complement our method with a large-scale empirical analysis of\ncipher performance across multiple victim LLMs. Within as few as 10 queries,\nMetaCipher achieves over 92\\% attack success rate (ASR) on most recent standard\nmalicious prompt benchmarks against state-of-the-art non-reasoning LLMs, and\nover 74\\% ASR against reasoning-capable LLMs, outperforming all existing\nobfuscation-based jailbreak methods. These results highlight the long-term\nrobustness and adaptability of our approach, making it more resilient than\nprior methods in the face of advancing safety measures.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22557v1", "categories": ["cs.CR", "cs.LG"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.22557v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22445", "title": "Hierarchical Adversarially-Resilient Multi-Agent Reinforcement Learning for Cyber-Physical Systems Security", "authors": ["Saad Alqithami"], "summary": "Cyber-Physical Systems play a critical role in the infrastructure of various\nsectors, including manufacturing, energy distribution, and autonomous\ntransportation systems. However, their increasing connectivity renders them\nhighly vulnerable to sophisticated cyber threats, such as adaptive and zero-day\nattacks, against which traditional security methods like rule-based intrusion\ndetection and single-agent reinforcement learning prove insufficient. To\novercome these challenges, this paper introduces a novel Hierarchical\nAdversarially-Resilient Multi-Agent Reinforcement Learning (HAMARL) framework.\nHAMARL employs a hierarchical structure consisting of local agents dedicated to\nsubsystem security and a global coordinator that oversees and optimizes\ncomprehensive, system-wide defense strategies. Furthermore, the framework\nincorporates an adversarial training loop designed to simulate and anticipate\nevolving cyber threats, enabling proactive defense adaptation. Extensive\nexperimental evaluations conducted on a simulated industrial IoT testbed\nindicate that HAMARL substantially outperforms traditional multi-agent\nreinforcement learning approaches, significantly improving attack detection\naccuracy, reducing response times, and ensuring operational continuity. The\nresults underscore the effectiveness of combining hierarchical multi-agent\ncoordination with adversarially-aware training to enhance the resilience and\nsecurity of next-generation CPS.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22445v1", "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.MA"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22445v1", "date": "2025-06-12", "updated": "2025-06-12"}
{"id": "2506.22494", "title": "DriveBLIP2: Attention-Guided Explanation Generation for Complex Driving Scenarios", "authors": ["Shihong Ling", "Yue Wan", "Xiaowei Jia", "Na Du"], "summary": "This paper introduces a new framework, DriveBLIP2, built upon the BLIP2-OPT\narchitecture, to generate accurate and contextually relevant explanations for\nemerging driving scenarios. While existing vision-language models perform well\nin general tasks, they encounter difficulties in understanding complex,\nmulti-object environments, particularly in real-time applications such as\nautonomous driving, where the rapid identification of key objects is crucial.\nTo address this limitation, an Attention Map Generator is proposed to highlight\nsignificant objects relevant to driving decisions within critical video frames.\nBy directing the model's focus to these key regions, the generated attention\nmap helps produce clear and relevant explanations, enabling drivers to better\nunderstand the vehicle's decision-making process in critical situations.\nEvaluations on the DRAMA dataset reveal significant improvements in explanation\nquality, as indicated by higher BLEU, ROUGE, CIDEr, and SPICE scores compared\nto baseline models. These findings underscore the potential of targeted\nattention mechanisms in vision-language models for enhancing explainability in\nreal-time autonomous driving.", "comment": "Accepted to IEEE/RSJ International Conference on Intelligent Robots\n  and Systems (IROS) 2025. 7 pages, 3 figures", "pdf_url": "http://arxiv.org/pdf/2506.22494v1", "categories": ["cs.RO", "cs.CV", "cs.LG"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.22494v1", "date": "2025-06-25", "updated": "2025-06-25"}
{"id": "2506.22474", "title": "RL-based Adaptive Task Offloading in Mobile-Edge Computing for Future IoT Networks", "authors": ["Ziad Qais Al Abbasi", "Khaled M. Rabie", "Senior Member", "Xingwang Li", "Senior Member", "Wali Ullah Khan", "Asma Abu Samah"], "summary": "The Internet of Things (IoT) has been increasingly used in our everyday lives\nas well as in numerous industrial applications. However, due to limitations in\ncomputing and power capabilities, IoT devices need to send their respective\ntasks to cloud service stations that are usually located at far distances.\nHaving to transmit data far distances introduces challenges for services that\nrequire low latency such as industrial control in factories and plants as well\nas artificial intelligence assisted autonomous driving. To solve this issue,\nmobile edge computing (MEC) is deployed at the networks edge to reduce\ntransmission time. In this regard, this study proposes a new offloading scheme\nfor MEC-assisted ultra dense cellular networks using reinforcement learning\n(RL) techniques. The proposed scheme enables efficient resource allocation and\ndynamic offloading decisions based on varying network conditions and user\ndemands. The RL algorithm learns from the networks historical data and adapts\nthe offloading decisions to optimize the networks overall performance.\nNon-orthogonal multiple access is also adopted to improve resource utilization\namong the IoT devices. Simulation results demonstrate that the proposed scheme\noutperforms other stateof the art offloading algorithms in terms of energy\nefficiency, network throughput, and user satisfaction.", "comment": "7 pages", "pdf_url": "http://arxiv.org/pdf/2506.22474v1", "categories": ["cs.NI", "cs.SY", "eess.SY", "C.2 COMPUTER-COMMUNICATION NETWORKS"], "cate": "cs.NI", "url": "http://arxiv.org/abs/2506.22474v1", "date": "2025-06-20", "updated": "2025-06-20"}
{"id": "2506.22993", "title": "Prediction Gaps as Pathways to Explanation: Rethinking Educational Outcomes through Differences in Model Performance", "authors": ["Javier Garcia-Bernardo", "Eva Jaspers", "Weverthon Machado", "Samuel Plach", "Erik Jan van Leeuwen"], "summary": "Social contexts -- such as families, schools, and neighborhoods -- shape life\noutcomes. The key question is not simply whether they matter, but rather for\nwhom and under what conditions. Here, we argue that prediction gaps --\ndifferences in predictive performance between statistical models of varying\ncomplexity -- offer a pathway for identifying surprising empirical patterns\n(i.e., not captured by simpler models) which highlight where theories succeed\nor fall short. Using population-scale administrative data from the Netherlands,\nwe compare logistic regression, gradient boosting, and graph neural networks to\npredict university completion using early-life social contexts. Overall,\nprediction gaps are small, suggesting that previously identified indicators,\nparticularly parental status, capture most measurable variation in educational\nattainment. However, gaps are larger for girls growing up without fathers --\nsuggesting that the effects of social context for these groups go beyond simple\nmodels in line with sociological theory. Our paper shows the potential of\nprediction methods to support sociological explanation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22993v1", "categories": ["cs.SI"], "cate": "cs.SI", "url": "http://arxiv.org/abs/2506.22993v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.22703", "title": "P4OMP: Retrieval-Augmented Prompting for OpenMP Parallelism in Serial Code", "authors": ["Wali Mohammad Abdullah", "Azmain Kabir"], "summary": "We present P4OMP, a retrieval-augmented framework for transforming serial\nC/C++ code into OpenMP-annotated parallel code using large language models\n(LLMs). To our knowledge, this is the first system to apply retrieval-based\nprompting for OpenMP pragma correctness without model fine-tuning or compiler\ninstrumentation. P4OMP leverages Retrieval-Augmented Generation (RAG) with\nstructured instructional knowledge from OpenMP tutorials to improve the\nreliability of prompt-driven code generation. By grounding generation in the\nretrieved context, P4OMP improves syntactic correctness compared to baseline\nprompting with GPT-3.5-Turbo. We evaluate P4OMP against a baseline,\nGPT-3.5-Turbo without retrieval, on a comprehensive benchmark of 108 real-world\nC++ programs drawn from Stack Overflow, PolyBench, and NAS benchmark suites.\nP4OMP achieves 100% compilation success on all parallelizable cases, while the\nbaseline fails to compile in 20 out of 108 cases. Six cases that rely on\nnon-random-access iterators or thread-unsafe constructs are excluded due to\nfundamental OpenMP limitations. A detailed analysis demonstrates how P4OMP\nconsistently avoids scoping errors, syntactic misuse, and invalid directive\ncombinations that commonly affect baseline-generated code. We further\ndemonstrate strong runtime scaling across seven compute-intensive benchmarks on\nan HPC cluster. P4OMP offers a robust, modular pipeline that significantly\nimproves the reliability and applicability of LLM-generated OpenMP code.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22703v1", "categories": ["cs.SE", "cs.AI"], "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.22703v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.22597", "title": "A tangible user interface for assessing cognitive mapping ability", "authors": ["Ehud Sharlin", "Benjamin Watson", "Steve Sutphen", "Lili Liu", "Robert Lederer", "John Frazer"], "summary": "Wayfinding, the ability to recall the environment and navigate through it, is\nan essential cognitive skill relied upon almost every day in a person's life. A\ncrucial component of wayfinding is the construction of cognitive maps, mental\nrepresentations of the environments through which a person travels. Age,\ndisease or injury can severely affect cognitive mapping, making assessment of\nthis basic survival skill particularly important to clinicians and therapists.\nCognitive mapping has also been the focus of decades of basic research by\ncognitive psychologists. Both communities have evolved a number of techniques\nfor assessing cognitive mapping ability. We present the Cognitive Map Probe\n(CMP), a new computerized tool for assessment of cognitive mapping ability that\nincreases consistency and promises improvements in flexibility, accessibility,\nsensitivity and control. The CMP uses a tangible user interface that affords\nspatial manipulation. We describe the design of the CMP, and find that it is\nsensitive to factors known to affect cognitive mapping performance in extensive\nexperimental testing.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22597v1", "categories": ["cs.HC"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.22597v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.23081", "title": "Linear Complementary Pairs of Algebraic Geometry Codes via Kummer Extensions", "authors": ["Huang Junjie", "Chen Haojie", "Zhang Huachao", "Zhao Chang-An"], "summary": "Due to their widespread applications, linear complementary pairs (LCPs) have\nattracted much attention in recent years. In this paper, we determine explicit\nconstruction of non-special divisors of degree $g$ and $g-1$ on Kummer\nextensions with specific properties. In addition, we present several methods\nfor constructing LCPs of algebraic geometry codes (AG Codes) via Kummer\nextensions. These results are applied in constructing explicit LCPs of AG Codes\nfrom subcovers of the BM curve, elliptic function fields, hyperelliptic\nfunction fields and other function fields. It is important to mention that we\nconstruct several families LCPs of MDS AG Codes from elliptic function fields\nand we obtain some linear complementary dual (LCD) codes from certain maximal\nelliptic function fields and hyperelliptic function fields.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23081v1", "categories": ["cs.IT", "math.IT"], "cate": "cs.IT", "url": "http://arxiv.org/abs/2506.23081v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23901", "title": "Sustainable operation of research infrastructure for novel computing", "authors": ["Yannik Stradmann", "Joscha Ilmberger", "Eric M√ºller", "Johannes Schemmel"], "summary": "Novel compute systems are an emerging research topic, aiming towards building\nnext-generation compute platforms. For these systems to thrive, they need to be\nprovided as research infrastructure to allow acceptance and usage by a large\ncommunity. By the example of the neuromorphic BrainScaleS-2 system, we showcase\nthe transformation from a laboratory setup to a sustainable, publicly available\nplatform. It is embedded into a purpose-built institute, tightly coupling a\nconventional cluster with novel compute hardware. The network infrastructure is\noptimized for robust operation, even in the case of unintended behavior of\nindividual devices. The systems themselves are packaged into 19-inch compatible\nunits to allow for easy maintenance and extension. We operate the platform\nusing modern CI/CD techniques and continuously assert its health using\nautomated system monitoring. Finally, we share our lessons learned during the\ndecade-long endeavor of operating analog neuromorphic systems as a publicly\navailable research platform.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23901v1", "categories": ["cs.AR"], "cate": "cs.AR", "url": "http://arxiv.org/abs/2506.23901v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22818", "title": "TriADA: Massively Parallel Trilinear Matrix-by-Tensor Multiply-Add Algorithm and Device Architecture for the Acceleration of 3D Discrete Transformations", "authors": ["Stanislav Sedukhin", "Yoichi Tomioka", "Kazuya Matsumoto", "Yuichi Okuyama"], "summary": "Multilinear transformations are key in high-performance computing (HPC) and\nartificial intelligence (AI) workloads, where data is represented as tensors.\nHowever, their high computational and memory demands, which grow with\ndimensionality, often slow down critical tasks. Moreover, scaling computation\nby enlarging the number of parallel processing units substantially increases\nenergy consumption, limiting widespread adoption, especially for sparse data,\nwhich is common in HPC and AI applications. This paper introduces the Trilinear\nAlgorithm and isomorphic to algorithm Device Architecture (TriADA) to address\nthese challenges with the following innovations: (1) a massively parallel,\nlow-rank algorithm for computing a family of trilinear (3D) discrete orthogonal\ntransformations (3D-DXTs), which is a special case of the more general 3-mode\nmatrix-by-tensor multiplication (3D-GEMT); (2) a new outer-product-based GEMM\nkernel with decoupled streaming active memory, specially designed to accelerate\n3D-GEMT operation; (3) an isomorphic to the proposed algorithm, fully\ndistributed 3D network of mesh interconnected processing elements or cells with\na coordinate-free, data-driven local processing activity, which is independent\nof problem size; (4) an elastic sparse outer-product (ESOP) method that avoids\nunnecessary computing and communication operations with zero-valued operands,\nthereby enhancing energy efficiency, computational accuracy, and stability.\nTriADA is capable of performing a variety of trilinear transformations with\nhypercubic arithmetic complexity in a linear number of time-steps. The\nmassively parallel, scalable, and energy-efficient architecture of TriADA is\nideal for accelerating multilinear tensor operations, which are the most\ndemanding parts of AI and HPC workloads.", "comment": "19 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2506.22818v1", "categories": ["cs.DC", "cs.AI", "cs.AR", "cs.ET", "eess.SP", "C.1.4; C.3; F.2.1; G.1.3; G.4"], "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.22818v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.22481", "title": "Theories of \"Sexuality\" in Natural Language Processing Bias Research", "authors": ["Jacob Hobbs"], "summary": "In recent years, significant advancements in the field of Natural Language\nProcessing (NLP) have positioned commercialized language models as\nwide-reaching, highly useful tools. In tandem, there has been an explosion of\nmultidisciplinary research examining how NLP tasks reflect, perpetuate, and\namplify social biases such as gender and racial bias. A significant gap in this\nscholarship is a detailed analysis of how queer sexualities are encoded and\n(mis)represented by both NLP systems and practitioners. Following previous work\nin the field of AI fairness, we document how sexuality is defined and\noperationalized via a survey and analysis of 55 articles that quantify\nsexuality-based NLP bias. We find that sexuality is not clearly defined in a\nmajority of the literature surveyed, indicating a reliance on assumed or\nnormative conceptions of sexual/romantic practices and identities. Further, we\nfind that methods for extracting biased outputs from NLP technologies often\nconflate gender and sexual identities, leading to monolithic conceptions of\nqueerness and thus improper quantifications of bias. With the goal of improving\nsexuality-based NLP bias analyses, we conclude with recommendations that\nencourage more thorough engagement with both queer communities and\ninterdisciplinary literature.", "comment": "17 pages, 9 tables, undergraduate senior thesis, submitted to The\n  Spectra: The Virginia Engineering and Science Research Journal", "pdf_url": "http://arxiv.org/pdf/2506.22481v1", "categories": ["cs.CY", "cs.CL"], "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.22481v1", "date": "2025-06-22", "updated": "2025-06-22"}
{"id": "2506.22463", "title": "Modulated Diffusion: Accelerating Generative Modeling with Modulated Quantization", "authors": ["Weizhi Gao", "Zhichao Hou", "Junqi Yin", "Feiyi Wang", "Linyu Peng", "Xiaorui Liu"], "summary": "Diffusion models have emerged as powerful generative models, but their high\ncomputation cost in iterative sampling remains a significant bottleneck. In\nthis work, we present an in-depth and insightful study of state-of-the-art\nacceleration techniques for diffusion models, including caching and\nquantization, revealing their limitations in computation error and generation\nquality. To break these limits, this work introduces Modulated Diffusion\n(MoDiff), an innovative, rigorous, and principled framework that accelerates\ngenerative modeling through modulated quantization and error compensation.\nMoDiff not only inherents the advantages of existing caching and quantization\nmethods but also serves as a general framework to accelerate all diffusion\nmodels. The advantages of MoDiff are supported by solid theoretical insight and\nanalysis. In addition, extensive experiments on CIFAR-10 and LSUN demonstrate\nthat MoDiff significant reduces activation quantization from 8 bits to 3 bits\nwithout performance degradation in post-training quantization (PTQ). Our code\nimplementation is available at https://github.com/WeizhiGao/MoDiff.", "comment": "26 pages, accepted by ICML 2025", "pdf_url": "http://arxiv.org/pdf/2506.22463v1", "categories": ["cs.CV", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22463v1", "date": "2025-06-18", "updated": "2025-06-18"}
{"id": "2506.22561", "title": "On the Reachability Problem for Two-Dimensional Branching VASS", "authors": ["Clotilde Bizi√®re", "Thibault Hilaire", "J√©r√¥me Leroux", "Gr√©goire Sutre"], "summary": "Vectors addition systems with states (VASS), or equivalently Petri nets, are\narguably one of the most studied formalisms for the modeling and analysis of\nconcurrent systems. A central decision problem for VASS is reachability:\nwhether there exists a run from an initial configuration to a final one. This\nproblem has been known to be decidable for over forty years, and its complexity\nhas recently been precisely characterized. Our work concerns the reachability\nproblem for BVASS, a branching generalization of VASS. In dimension one, the\nexact complexity of this problem is known. In this paper, we prove that the\nreachability problem for 2-dimensional BVASS is decidable. In fact, we even\nshow that the reachability set admits a computable semilinear presentation. The\ndecidability status of the reachability problem for BVASS remains open in\nhigher dimensions.", "comment": "Full version of the paper with the same title and authors to appear\n  in the proceedings of MFCS 2025", "pdf_url": "http://arxiv.org/pdf/2506.22561v1", "categories": ["cs.LO", "cs.FL"], "cate": "cs.LO", "url": "http://arxiv.org/abs/2506.22561v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22944", "title": "Feasibility of spectral-element modeling of wave propagation through the anatomy of marine mammals", "authors": ["Carlos Garc√≠a A.", "Vladimiro Boselli", "Aida Hejazi Nooghabi", "Andrea Colombi", "Lapo Boschi"], "summary": "This study introduces the first 3D spectral-element method (SEM) simulation\nof ultrasonic wave propagation in a bottlenose dolphin (Tursiops truncatus)\nhead. Unlike traditional finite-element methods (FEM), which struggle with\nhigh-frequency simulations due to costly linear-system inversions and slower\nconvergence, SEM offers exponential convergence and efficient parallel\ncomputation. Using Computed Tomography (CT) scan data, we developed a detailed\nhexahedral mesh capturing complex anatomical features, such as acoustic fats\nand jaws. Our simulations of plane and spherical waves confirm SEM's\neffectiveness for ultrasonic time-domain modeling. This approach opens new\navenues for marine biology, contributing to research in echolocation, the\nimpacts of anthropogenic marine noise pollution and the biophysics of hearing\nand click generation in marine mammals. By overcoming FEM's limitations, SEM\nprovides a powerful scalable tool to test hypotheses about dolphin\nbioacoustics, with significant implications for conservation and understanding\nmarine mammal auditory systems under increasing environmental challenges.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22944v1", "categories": ["cs.CE", "cs.SD", "eess.AS", "q-bio.TO"], "cate": "cs.CE", "url": "http://arxiv.org/abs/2506.22944v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.22652", "title": "QoS-aware State-Augmented Learnable Algorithm for Wireless Coexistence Parameter Management", "authors": ["Mohammad Reza Fasihi", "Brian L. Mark"], "summary": "Efficient and fair coexistence in unlicensed spectrum is essential to support\nheterogeneous networks such as 5G NR-U and Wi-Fi, which often contend for\nshared wireless resources. We introduce a general framework for wireless\nCoexistence Parameter Management (CPM) based on state-augmented constrained\nreinforcement learning. We propose a novel algorithm, QaSAL-CPM, which\nincorporates state-augmentation by embedding the dual variables in the\nconstrained optimization formulation directly into the agent's observation\nspace. This method enables the agent to respond to constraint violations in\nreal time while continuing to optimize a primary performance objective. Through\nextensive simulations of 5G NR-U and Wi-Fi coexistence scenarios, we show that\nQaSAL-CPM achieves reliable QoS compliance and improved policy robustness\nacross various transmitter densities compared to previous approaches. The\nproposed framework offers a scalable and adaptive solution for real-time\ncoexistence optimization in next-generation wireless networks.", "comment": "13 pages, 7 figures", "pdf_url": "http://arxiv.org/pdf/2506.22652v1", "categories": ["eess.SY", "cs.SY"], "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.22652v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22455", "title": "Data Normalization Strategies for EEG Deep Learning", "authors": ["Dung Truong", "Arnaud Delorme"], "summary": "Normalization is a critical yet often overlooked component in the\npreprocessing pipeline for EEG deep learning applications. The rise of\nlarge-scale pretraining paradigms such as self-supervised learning (SSL)\nintroduces a new set of tasks whose nature is substantially different from\nsupervised training common in EEG deep learning applications. This raises new\nquestions about optimal normalization strategies for the applicable task. In\nthis study, we systematically evaluate the impact of normalization granularity\n(recording vs. window level) and scope (cross-channel vs. within-channel) on\nboth supervised (age and gender prediction) and self-supervised (Contrastive\nPredictive Coding) tasks. Using high-density resting-state EEG from 2,836\nsubjects in the Healthy Brain Network dataset, we show that optimal\nnormalization strategies differ significantly between training paradigms.\nWindow-level within-channel normalization yields the best performance in\nsupervised tasks, while minimal or cross-channel normalization at the window\nlevel is more effective for SSL. These results underscore the necessity of\ntask-specific normalization choices and challenge the assumption that a\nuniversal normalization strategy can generalize across learning settings. Our\nfindings provide practical insights for developing robust EEG deep learning\npipelines as the field shifts toward large-scale, foundation model training.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22455v1", "categories": ["eess.SP", "cs.LG"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.22455v1", "date": "2025-06-15", "updated": "2025-06-15"}
{"id": "2506.22596", "title": "Multi-Domain FeFET-Based Pixel for In-Sensor Multiply-and-Accumulate Operations", "authors": ["Md Rahatul Islam Udoy", "Wantong Li", "Kai Ni", "Ahmedullah Aziz"], "summary": "This paper presents an FeFET-based active pixel sensor that performs\nin-sensor multiply-and-accumulate (MAC) operations by leveraging the\nmulti-domain polarization states of ferroelectric layers. The proposed design\nintegrates a programmable FeFET into a 3-transistor pixel circuit, where the\nFeFET's non-volatile conductance encodes the weight, and the photodiode voltage\ndrop encodes the input. Their interaction generates an output current\nproportional to the product, enabling in-pixel analog multiplication.\nAccumulation is achieved by summing output currents along shared column lines,\nrealizing full MAC functionality within the image sensor array. Extensive\nHSPICE simulations, using 45 nm CMOS models, validate the operation and confirm\nthe scalability of the design. This compact and power-efficient architecture\nminimizes data movement, making it ideal for real-time edge computing,\nneuromorphic vision, and secure sensing applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22596v1", "categories": ["eess.IV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.22596v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22973", "title": "Confident Splatting: Confidence-Based Compression of 3D Gaussian Splatting via Learnable Beta Distributions", "authors": ["AmirHossein Naghi Razlighi", "Elaheh Badali Golezani", "Shohreh Kasaei"], "summary": "3D Gaussian Splatting enables high-quality real-time rendering but often\nproduces millions of splats, resulting in excessive storage and computational\noverhead. We propose a novel lossy compression method based on learnable\nconfidence scores modeled as Beta distributions. Each splat's confidence is\noptimized through reconstruction-aware losses, enabling pruning of\nlow-confidence splats while preserving visual fidelity. The proposed approach\nis architecture-agnostic and can be applied to any Gaussian Splatting variant.\nIn addition, the average confidence values serve as a new metric to assess the\nquality of the scene. Extensive experiments demonstrate favorable trade-offs\nbetween compression and fidelity compared to prior work. Our code and data are\npublicly available at\nhttps://github.com/amirhossein-razlighi/Confident-Splatting", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22973v1", "categories": ["cs.GR", "cs.CV"], "cate": "cs.GR", "url": "http://arxiv.org/abs/2506.22973v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.22728", "title": "Counting distinct (non-)crossing substrings", "authors": ["Haruki Umezaki", "Hiroki Shibata", "Dominik K√∂ppl", "Yuto Nakashima", "Shunsuke Inenaga", "Hideo Bannai"], "summary": "Let $w$ be a string of length $n$. The problem of counting factors crossing a\nposition - Problem 64 from the textbook ``125 Problems in Text Algorithms''\n[Crochemore, Leqroc, and Rytter, 2021], asks to count the number\n$\\mathcal{C}(w,k)$ (resp. $\\mathcal{N}(w,k)$) of distinct substrings in $w$\nthat have occurrences containing (resp. not containing) a position $k$ in $w$.\nThe solutions provided in their textbook compute $\\mathcal{C}(w,k)$ and\n$\\mathcal{N}(w,k)$ in $O(n)$ time for a single position $k$ in $w$, and thus a\ndirect application would require $O(n^2)$ time for all positions $k = 1,\n\\ldots, n$ in $w$. Their solution is designed for constant-size alphabets. In\nthis paper, we present new algorithms which compute $\\mathcal{C}(w,k)$ in\n$O(n)$ total time for general ordered alphabets, and $\\mathcal{N}(w,k)$ in\n$O(n)$ total time for linearly sortable alphabets, for all positions $k = 1,\n\\ldots, n$ in $w$.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22728v1", "categories": ["cs.DS"], "cate": "cs.DS", "url": "http://arxiv.org/abs/2506.22728v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.22443", "title": "Learning Interpretable Rules from Neural Networks: Neurosymbolic AI for Radar Hand Gesture Recognition", "authors": ["Sarah Seifi", "Tobias Sukianto", "Cecilia Carbonelli", "Lorenzo Servadei", "Robert Wille"], "summary": "Rule-based models offer interpretability but struggle with complex data,\nwhile deep neural networks excel in performance yet lack transparency. This\nwork investigates a neuro-symbolic rule learning neural network named RL-Net\nthat learns interpretable rule lists through neural optimization, applied for\nthe first time to radar-based hand gesture recognition (HGR). We benchmark\nRL-Net against a fully transparent rule-based system (MIRA) and an explainable\nblack-box model (XentricAI), evaluating accuracy, interpretability, and user\nadaptability via transfer learning. Our results show that RL-Net achieves a\nfavorable trade-off, maintaining strong performance (93.03% F1) while\nsignificantly reducing rule complexity. We identify optimization challenges\nspecific to rule pruning and hierarchy bias and propose stability-enhancing\nmodifications. Compared to MIRA and XentricAI, RL-Net emerges as a practical\nmiddle ground between transparency and performance. This study highlights the\nreal-world feasibility of neuro-symbolic models for interpretable HGR and\noffers insights for extending explainable AI to edge-deployable sensing\nsystems.", "comment": "8 pages, 3 figures, accepted at the late-breaking work track at the\n  XAI-2025 third World Conference of Explainable AI", "pdf_url": "http://arxiv.org/pdf/2506.22443v1", "categories": ["cs.LG", "cs.HC"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22443v1", "date": "2025-06-11", "updated": "2025-06-11"}
{"id": "2506.22653", "title": "URSA: The Universal Research and Scientific Agent", "authors": ["Michael Grosskopf", "Russell Bent", "Rahul Somasundaram", "Isaac Michaud", "Arthur Lui", "Nathan Debardeleben", "Earl Lawrence"], "summary": "Large language models (LLMs) have moved far beyond their initial form as\nsimple chatbots, now carrying out complex reasoning, planning, writing, coding,\nand research tasks. These skills overlap significantly with those that human\nscientists use day-to-day to solve complex problems that drive the cutting edge\nof research. Using LLMs in \"agentic\" AI has the potential to revolutionize\nmodern science and remove bottlenecks to progress. In this work, we present\nURSA, a scientific agent ecosystem for accelerating research tasks. URSA\nconsists of a set of modular agents and tools, including coupling to advanced\nphysics simulation codes, that can be combined to address scientific problems\nof varied complexity and impact. This work highlights the architecture of URSA,\nas well as examples that highlight the potential of the system.", "comment": "31 pages, 9 figures", "pdf_url": "http://arxiv.org/pdf/2506.22653v1", "categories": ["cs.AI"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.22653v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.23060", "title": "Synergizing Implicit and Explicit User Interests: A Multi-Embedding Retrieval Framework at Pinterest", "authors": ["Zhibo Fan", "Hongtao Lin", "Haoyu Chen", "Bowen Deng", "Hedi Xia", "Yuke Yan", "James Li"], "summary": "Industrial recommendation systems are typically composed of multiple stages,\nincluding retrieval, ranking, and blending. The retrieval stage plays a\ncritical role in generating a high-recall set of candidate items that covers a\nwide range of diverse user interests. Effectively covering the diverse and\nlong-tail user interests within this stage poses a significant challenge:\ntraditional two-tower models struggle in this regard due to limited user-item\nfeature interaction and often bias towards top use cases. To address these\nissues, we propose a novel multi-embedding retrieval framework designed to\nenhance user interest representation by generating multiple user embeddings\nconditioned on both implicit and explicit user interests. Implicit interests\nare captured from user history through a Differentiable Clustering Module\n(DCM), whereas explicit interests, such as topics that the user has followed,\nare modeled via Conditional Retrieval (CR). These methodologies represent a\nform of conditioned user representation learning that involves condition\nrepresentation construction and associating the target item with the relevant\nconditions. Synergizing implicit and explicit user interests serves as a\ncomplementary approach to achieve more effective and comprehensive candidate\nretrieval as they benefit on different user segments and extract conditions\nfrom different but supplementary sources. Extensive experiments and A/B testing\nreveal significant improvements in user engagements and feed diversity metrics.\nOur proposed framework has been successfully deployed on Pinterest home feed.", "comment": "KDD 2025", "pdf_url": "http://arxiv.org/pdf/2506.23060v1", "categories": ["cs.IR"], "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.23060v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23371", "title": "Investigating an Overfitting and Degeneration Phenomenon in Self-Supervised Multi-Pitch Estimation", "authors": ["Frank Cwitkowitz", "Zhiyao Duan"], "summary": "Multi-Pitch Estimation (MPE) continues to be a sought after capability of\nMusic Information Retrieval (MIR) systems, and is critical for many\napplications and downstream tasks involving pitch, including music\ntranscription. However, existing methods are largely based on supervised\nlearning, and there are significant challenges in collecting annotated data for\nthe task. Recently, self-supervised techniques exploiting intrinsic properties\nof pitch and harmonic signals have shown promise for both monophonic and\npolyphonic pitch estimation, but these still remain inferior to supervised\nmethods. In this work, we extend the classic supervised MPE paradigm by\nincorporating several self-supervised objectives based on pitch-invariant and\npitch-equivariant properties. This joint training results in a substantial\nimprovement under closed training conditions, which naturally suggests that\napplying the same objectives to a broader collection of data will yield further\nimprovements. However, in doing so we uncover a phenomenon whereby our model\nsimultaneously overfits to the supervised data while degenerating on data used\nfor self-supervision only. We demonstrate and investigate this and offer our\ninsights on the underlying problem.", "comment": "Accepted to ISMIR 2025", "pdf_url": "http://arxiv.org/pdf/2506.23371v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "cate": "eess.AS", "url": "http://arxiv.org/abs/2506.23371v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.22486", "title": "Hallucination Detection with Small Language Models", "authors": ["Ming Cheung"], "summary": "Since the introduction of ChatGPT, large language models (LLMs) have\ndemonstrated significant utility in various tasks, such as answering questions\nthrough retrieval-augmented generation. Context can be retrieved using a\nvectorized database, serving as a foundation for LLMs to generate responses.\nHowever, hallucinations in responses can undermine the reliability of LLMs in\npractical applications, and they are not easily detectable in the absence of\nground truth, particularly in question-and-answer scenarios. This paper\nproposes a framework that integrates multiple small language models to verify\nresponses generated by LLMs using the retrieved context from a vectorized\ndatabase. By breaking down the responses into individual sentences and\nutilizing the probability of generating \"Yes\" tokens from the outputs of\nmultiple models for a given set of questions, responses, and relevant context,\nhallucinations can be detected. The proposed framework is validated through\nexperiments with real datasets comprising over 100 sets of questions, answers,\nand contexts, including responses with fully and partially correct sentences.\nThe results demonstrate a 10\\% improvement in F1 scores for detecting correct\nresponses compared to hallucinations, indicating that multiple small language\nmodels can be effectively employed for answer verification, providing a\nscalable and efficient solution for both academic and practical applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22486v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.22486v1", "date": "2025-06-24", "updated": "2025-06-24"}
{"id": "2506.22657", "title": "A Class of Stochastic Runge-Kutta Methods for Stochastic Differential Equations Converging with Order 1 in $L^p$-Norm", "authors": ["Andreas R√∂√üler"], "summary": "For the approximation of solutions for It\\^o and Stratonovich stochastic\ndifferential equations (SDEs)a new class of efficient stochastic Runge-Kutta\n(SRK) methods is developed. As the main novelty only two stages are necessary\nfor the proposed SRK methods of order 1 that can be applied to SDEs with\nnon-commutative or with commutative noise. In addition, a variant of the SRK\nmethod for SDEs with additive noise is presented. All proposed SRK methods\ncover also the case of drift-implicit schemes and general order conditions for\nthe coefficients are calculated explicitly. The new class of SRK methods is\nhighly efficient in the sense that it features computational cost depending\nonly linearly on the dimension of the SDE and on the dimension of the driving\nWiener process. For all proposed SRK methods strong convergence with order 1 in\n$L^p$-norm for any $p \\geq 2$ is proved. Moreover, sufficient conditions for\napproximated iterated stochastic integrals are established such that\nconvergence with order 1 in $L^p$-norm is preserved if they are applied for the\nSRK method. The presented theoretical results are confirmed by numerical\nexperiments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22657v1", "categories": ["math.NA", "cs.NA", "math.PR", "65C30, 60H10, 65L06"], "cate": "math.NA", "url": "http://arxiv.org/abs/2506.22657v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.23738", "title": "More Efficient Real-Valued Gray-Box Optimization through Incremental Distribution Estimation in RV-GOMEA", "authors": ["Renzo J. Scholman", "Tanja Alderliesten", "Peter A. N. Bosman"], "summary": "The Gene-pool Optimal Mixing EA (GOMEA) family of EAs offers a specific means\nto exploit problem-specific knowledge through linkage learning, i.e.,\ninter-variable dependency detection, expressed using subsets of variables, that\nshould undergo joint variation. Such knowledge can be exploited if faster\nfitness evaluations are possible when only a few variables are changed in a\nsolution, enabling large speed-ups. The recent-most version of Real-Valued\nGOMEA (RV-GOMEA) can learn a conditional linkage model during optimization\nusing fitness-based linkage learning, enabling fine-grained dependency\nexploitation in learning and sampling a Gaussian distribution. However, while\nthe most efficient Gaussian-based EAs, like NES and CMA-ES, employ incremental\nlearning of the Gaussian distribution rather than performing full re-estimation\nevery generation, the recent-most RV-GOMEA version does not employ such\nincremental learning. In this paper, we therefore study whether incremental\ndistribution estimation can lead to efficiency enhancements of RV-GOMEA. We\nconsider various benchmark problems with varying degrees of overlapping\ndependencies. We find that, compared to RV-GOMEA and VKD-CMA-ES, the required\nnumber of evaluations to reach high-quality solutions can be reduced by a\nfactor of up to 1.5 if population sizes are tuned problem-specifically, while a\nreduction by a factor of 2-3 can be achieved with generic population-sizing\nguidelines.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23738v1", "categories": ["cs.NE"], "cate": "cs.NE", "url": "http://arxiv.org/abs/2506.23738v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22789", "title": "WavShape: Information-Theoretic Speech Representation Learning for Fair and Privacy-Aware Audio Processing", "authors": ["Oguzhan Baser", "Ahmet Ege Tanriverdi", "Kaan Kale", "Sandeep P. Chinchali", "Sriram Vishwanath"], "summary": "Speech embeddings often retain sensitive attributes such as speaker identity,\naccent, or demographic information, posing risks in biased model training and\nprivacy leakage. We propose WavShape, an information-theoretic speech\nrepresentation learning framework that optimizes embeddings for fairness and\nprivacy while preserving task-relevant information. We leverage mutual\ninformation (MI) estimation using the Donsker-Varadhan formulation to guide an\nMI-based encoder that systematically filters sensitive attributes while\nmaintaining speech content essential for downstream tasks. Experimental results\non three known datasets show that WavShape reduces MI between embeddings and\nsensitive attributes by up to 81% while retaining 97% of task-relevant\ninformation. By integrating information theory with self-supervised speech\nmodels, this work advances the development of fair, privacy-aware, and\nresource-efficient speech systems.", "comment": "5 pages, 4 figures, Published at The Proceedings of Interspeech 2025,\n  code is available at http://www.github.com/UTAustin-SwarmLab/WavShape", "pdf_url": "http://arxiv.org/pdf/2506.22789v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.22789v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.22671", "title": "Towards an Optimized Multi-Cyclic Queuing and Forwarding in Time Sensitive Networking with Time Injection", "authors": ["Rubi Debnath", "Mohammadreza Barzegaran", "Sebastian Steinhorst"], "summary": "Cyclic Queuing and Forwarding (CQF) is a Time-Sensitive Networking (TSN)\nshaping mechanism that provides bounded latency and deterministic Quality of\nService (QoS). However, CQF's use of a single cycle restricts its ability to\nsupport TSN traffic with diverse timing requirements. Multi-Cyclic Queuing and\nForwarding (Multi-CQF) is a new and emerging TSN shaping mechanism that uses\nmultiple cycles on the same egress port, allowing it to accommodate TSN flows\nwith varied timing requirements more effectively than CQF. Despite its\npotential, current Multi-CQF configuration studies are limited, leading to a\nlack of comprehensive research, poor understanding of the mechanism, and\nlimited adoption of Multi-CQF in practical applications. Previous work has\nshown the impact of Time Injection (TI), defined as the start time of\nTime-Triggered (TT) flows at the source node, on CQF queue resource\nutilization. However, the impact of TI has not yet been explored in the context\nof Multi-CQF. This paper introduces a set of constraints and leverages Domain\nSpecific Knowledge (DSK) to reduce the search space for Multi-CQF\nconfiguration. Building on this foundation, we develop an open-source Genetic\nAlgorithm (GA) and a hybrid GA-Simulated Annealing (GASA) approach to\nefficiently configure Multi-CQF networks and introduce TI in Multi-CQF to\nenhance schedulability. Experimental results show that our proposed algorithms\nsignificantly increase the number of scheduled TT flows compared to the\nbaseline Simulated Annealing (SA) model, improving scheduling by an average of\n15%. Additionally, GASA achieves a 20% faster convergence rate and lower time\ncomplexity, outperforming the SA model in speed, and efficiency.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22671v1", "categories": ["cs.NI", "cs.ET"], "cate": "cs.NI", "url": "http://arxiv.org/abs/2506.22671v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22606", "title": "A User-Centric, Privacy-Preserving, and Verifiable Ecosystem for Personal Data Management and Utilization", "authors": ["Osama Zafar", "Mina Namazi", "Yuqiao Xu", "Youngjin Yoo", "Erman Ayday"], "summary": "In the current paradigm of digital personalized services, the centralized\nmanagement of personal data raises significant privacy concerns, security\nvulnerabilities, and diminished individual autonomy over sensitive information.\nDespite their efficiency, traditional centralized architectures frequently fail\nto satisfy rigorous privacy requirements and expose users to data breaches and\nunauthorized access risks. This pressing challenge calls for a fundamental\nparadigm shift in methodologies for collecting, storing, and utilizing personal\ndata across diverse sectors, including education, healthcare, and finance.\n  This paper introduces a novel decentralized, privacy-preserving architecture\nthat handles heterogeneous personal information, ranging from educational\ncredentials to health records and financial data. Unlike traditional models,\nour system grants users complete data ownership and control, allowing them to\nselectively share information without compromising privacy. The architecture's\nfoundation comprises advanced privacy-enhancing technologies, including secure\nenclaves and federated learning, enabling secure computation, verification, and\ndata sharing. The system supports diverse functionalities, including local\ncomputation, model training, and privacy-preserving data sharing, while\nensuring data credibility and robust user privacy.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22606v1", "categories": ["cs.CR", "cs.LG"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.22606v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22507", "title": "Integrated Multimodal Sensing and Communication: Challenges, Technologies, and Architectures", "authors": ["Yubo Peng", "Luping Xiang", "Kun Yang", "Feibo Jiang", "Kezhi Wang", "Christos Masouros"], "summary": "The evolution towards 6G networks requires the intelligent integration of\ncommunication and sensing capabilities to support diverse and complex\napplications, such as autonomous driving and immersive services. However,\nexisting integrated sensing and communication (ISAC) systems predominantly rely\non single-modal sensors as primary participants, which leads to a limited\nrepresentation of environmental features and significant performance\nbottlenecks under the emerging requirements of 6G applications. This limitation\nmotivates a paradigm shift from single-modal to multimodal ISAC. In this\narticle, we first analyze the key challenges in realizing multimodal ISAC,\nincluding the fusion of heterogeneous multimodal data, the high communication\noverhead among distributed sensors, and the design of efficient and scalable\nsystem architectures. We then introduce several enabling technologies, such as\nlarge AI models, semantic communication, and multi-agent systems, that hold\npromise for addressing these challenges. To operationalize these technologies,\nwe zoom into three architectural paradigms: fusion-based multimodal ISAC\n(F-MAC), interaction-based multimodal ISAC (I-MAC), and relay-based multimodal\nISAC (R-MAC), each tailored to organize devices and modalities for efficient\ncollaboration in different scenarios. Thereafter, a case study is presented\nbased on the F-MAC scheme, demonstrating that the scheme achieves more\ncomprehensive sensing and improves sensing accuracy by approximately 80%\ncompared to conventional single-modal ISAC systems. Finally, we discuss several\nopen issues to be addressed in the future.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22507v1", "categories": ["cs.NI", "cs.MA", "eess.SP"], "cate": "cs.NI", "url": "http://arxiv.org/abs/2506.22507v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.22572", "title": "Directed Shape Morphing using Kirigami-enhanced Thermoplastics", "authors": ["Mrunmayi Mungekar", "Sanjith Menon", "M. Ravi Shankar", "M. Khalid Jawed"], "summary": "We present a simple, accessible method for autonomously transforming flat\nplastic sheets into intricate three-dimensional structures using only uniform\nheating and common tools such as household ovens and scissors. Our approach\ncombines heat-shrinkable thermoplastics with Kirigami patterns tailored to the\ntarget 3D shape, creating bilayer composites that morph into a wide range of\ncomplex structures, e.g., bowls, pyramids, and even custom ergonomic surfaces\nlike mouse covers. Critically, the transformation is driven by a\nlow-information stimulus (uniform heat) yet produces highly intricate shapes\nthrough programmed geometric design. The morphing behavior, confirmed by finite\nelement simulations, arises from strain mismatch between the contracting\nthermoplastic layer and the constraining Kirigami layer. By decoupling material\ncomposition from mechanical response, this method avoids detailed process\ncontrol and enables a broad class of self-morphing structures, offering a\nversatile platform for adaptive design and scalable manufacturing.", "comment": "Software and Data: https://github.com/structuresComp/Shrinky-Dink", "pdf_url": "http://arxiv.org/pdf/2506.22572v1", "categories": ["cs.RO"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.22572v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22477", "title": "Innovative Research on IoT Architecture and Robotic Operating Platforms: Applications of Large Language Models and Generative AI", "authors": ["Huiwen Han"], "summary": "This paper introduces an innovative design for robotic operating platforms,\nunderpinned by a transformative Internet of Things (IoT) architecture,\nseamlessly integrating cutting-edge technologies such as large language models\n(LLMs), generative AI, edge computing, and 5G networks. The proposed platform\naims to elevate the intelligence and autonomy of IoT systems and robotics,\nenabling them to make real-time decisions and adapt dynamically to changing\nenvironments. Through a series of compelling case studies across industries\nincluding smart manufacturing, healthcare, and service sectors, this paper\ndemonstrates the substantial potential of IoT-enabled robotics to optimize\noperational workflows, enhance productivity, and deliver innovative, scalable\nsolutions. By emphasizing the roles of LLMs and generative AI, the research\nhighlights how these technologies drive the evolution of intelligent robotics\nand IoT, shaping the future of industry-specific advancements. The findings not\nonly showcase the transformative power of these technologies but also offer a\nforward-looking perspective on their broader societal and industrial\nimplications, positioning them as catalysts for next-generation automation and\ntechnological convergence.", "comment": "Published in: 2024 6th International Conference on Robotics,\n  Intelligent Control and Artificial Intelligence (RICAI), IEEE Xplore, DOI:\n  10.1109/RICAI64321.2024.10911316. \\c{opyright} 2024 IEEE", "pdf_url": "http://arxiv.org/pdf/2506.22477v1", "categories": ["cs.NI", "cs.AI", "cs.ET", "cs.RO"], "cate": "cs.NI", "url": "http://arxiv.org/abs/2506.22477v1", "date": "2025-06-22", "updated": "2025-06-22"}
{"id": "2506.23179", "title": "Community-Based Efficient Algorithms for User-Driven Competitive Influence Maximization in Social Networks", "authors": ["Rahul Kumar Gautam"], "summary": "Nowadays, people in the modern world communicate with their friends,\nrelatives, and colleagues through the internet. Persons/nodes and\ncommunication/edges among them form a network. Social media networks are a type\nof network where people share their views with the community. There are several\nmodels that capture human behavior, such as a reaction to the information\nreceived from friends or relatives. The two fundamental models of information\ndiffusion widely discussed in the social networks are the Independent Cascade\nModel and the Linear Threshold Model. Liu et al. [1] propose a variant of the\nlinear threshold model in their paper title User-driven competitive influence\nMaximization(UDCIM) in social networks. Authors try to simulate human behavior\nwhere they do not make a decision immediately after being influenced, but take\na pause for a while, and then they make a final decision. They propose the\nheuristic algorithms and prove the approximation factor under community\nconstraints( The seed vertices belong to an identical community). Even finding\nthe community is itself an NP-hard problem. In this article, we extend the\nexisting work with algorithms and LP-formation of the problem. We also\nimplement and test the LP-formulated equations on small datasets by using the\nGurobi Solver [2]. We furthermore propose one heuristic and one genetic\nalgorithm. The extensive experimentation is carried out on medium to large\ndatasets, and the outcomes of both algorithms are plotted in the results and\ndiscussion section.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23179v1", "categories": ["cs.SI"], "cate": "cs.SI", "url": "http://arxiv.org/abs/2506.23179v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.22742", "title": "RAILS: Retrieval-Augmented Intelligence for Learning Software Development", "authors": ["Wali Mohammad Abdullah", "Md. Morshedul Islam", "Devraj Parmar", "Happy Hasmukhbhai Patel", "Sindhuja Prabhakaran", "Baidya Saha"], "summary": "Large Language Models (LLMs) like GPT-3.5-Turbo are increasingly used to\nassist software development, yet they often produce incomplete code or\nincorrect imports, especially when lacking access to external or\nproject-specific documentation. We introduce RAILS (Retrieval-Augmented\nIntelligence for Learning Software Development), a framework that augments LLM\nprompts with semantically retrieved context from curated Java resources using\nFAISS and OpenAI embeddings. RAILS incorporates an iterative validation loop\nguided by compiler feedback to refine suggestions. We evaluated RAILS on 78\nreal-world Java import error cases spanning standard libraries, GUI APIs,\nexternal tools, and custom utilities. Despite using the same LLM, RAILS\noutperforms baseline prompting by preserving intent, avoiding hallucinations,\nand surfacing correct imports even when libraries are unavailable locally.\nFuture work will integrate symbolic filtering via PostgreSQL and extend support\nto other languages and IDEs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22742v1", "categories": ["cs.SE", "cs.AI"], "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.22742v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.22674", "title": "Do Electric Vehicles Induce More Motion Sickness Than Fuel Vehicles? A Survey Study in China", "authors": ["Weiyin Xie", "Chunxi Huang", "Jiyao Wang", "Dengbo He"], "summary": "Electric vehicles (EVs) are a promising alternative to fuel vehicles (FVs),\ngiven some unique characteristics of EVs, for example, the low air pollution\nand maintenance cost. However, the increasing prevalence of EVs is accompanied\nby widespread complaints regarding the high likelihood of motion sickness (MS)\ninduction, especially when compared to FVs, which has become one of the major\nobstacles to the acceptance and popularity of EVs. Despite the prevalence of\nsuch complaints online and among EV users, the association between vehicle type\n(i.e., EV versus FV) and MS prevalence and severity has not been quantified.\nThus, this study aims to investigate the existence of EV-induced MS and explore\nthe potential factors leading to it. A survey study was conducted to collect\npassengers' MS experience in EVs and FVs in the past one year. In total, 639\nvalid responses were collected from mainland China. The results show that FVs\nwere associated with a higher frequency of MS, while EVs were found to induce\nmore severe MS symptoms. Further, we found that passengers' MS severity was\nassociated with individual differences (i.e., age, gender, sleep habits,\nsusceptibility to motion-induced MS), in-vehicle activities (i.e., chatting\nwith others and watching in-vehicle displays), and road conditions (i.e.,\ncongestion and slope), while the MS frequency was associated with the vehicle\nownership and riding frequency. The results from this study can guide the\ndirections of future empirical studies that aim to quantify the inducers of MS\nin EVs and FVs, as well as the optimization of EVs to reduce MS.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22674v1", "categories": ["cs.HC", "cs.CY", "stat.AP"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.22674v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.23194", "title": "General Mathematical Proof of Occam's Razor; Upgrading Theoretical Physicists' Methodology", "authors": ["Gabriel Leuenberger"], "summary": "This paper's first aim is to prove a modernized Occam's razor beyond a\nreasonable doubt. To summarize the main argument in one sentence: If we\nconsider all possible, intelligible, scientific models of ever-higher\ncomplexity, democratically, the predictions most favored by these complex\nmodels will agree with the predictions of the simplest models. This fact can be\nproven mathematically, thereby validating Occam's razor. Major parts of this\nline of reasoning have long preexisted within the depths of the algorithmic\ninformation theory literature, but they have always left room for doubts of\nvarious kinds. Therefore, we increase the generality, completeness, clarity,\naccessibility, and credibility of these arguments by countering over a dozen\nobjections. We build our mathematical proof of Occam's razor on the shoulders\nof the exact 'chain rule' for Kolmogorov complexity.\n  Concerning physics, we then go on to diagnose the primary amendable root\ncause of the present stagnation of the research field of fundamental\ntheoretical physics. We show that the effective antidote would consist in a\npractically feasible upgrade to the theoretical physicists' research\nmethodology: When proposing new theoretical models, physicists should simply\ncalculate and report the total amount of information that their models consist\nof. We explain why this methodology would be highly effective as well as how\nthese calculations could be performed efficiently.", "comment": "44 pages", "pdf_url": "http://arxiv.org/pdf/2506.23194v1", "categories": ["cs.IT", "math.IT"], "cate": "cs.IT", "url": "http://arxiv.org/abs/2506.23194v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.22702", "title": "A Correlation-Based Design of RIS for Reduced Power Consumption and Simplified Control Circuitry", "authors": ["Zina Mohamed", "Ammar B. Kouki", "Sonia A√Øssa"], "summary": "Aiming at simplifying the hardware structure and reducing the energy\nconsumption in wireless communication via reconfigurable intelligent surfaces\n(RIS), this paper introduces a novel RIS design founded on the correlation\nbetween the phase shift values of the surface elements. First, a correlation\nanalysis is conducted, considering the azimuth angle of a target device within\na coverage region spanning from $-80^{\\circ}$ to $80^{\\circ}$. The correlation\nis demonstrated for different deployment cases, creating the basis for the new\nRIS structure, termed Connected-RIS, where correlated elements are designed to\nshare the same control signal. The fundamental performance of the proposed\ndesign is then analyzed in terms of control signals, power consumption, and\ncommunication system performance, comparing it to two RIS structures with full\ncontrol: one with the same size as the proposed design, and the other employing\nthe minimum number of elements necessary to satisfy the fair coverage\ncriterion. The correlation-based RIS design enables three-dimensional passive\nbeamforming and significantly reduces the number of required load impedances\nand control signals, thereby lowering the hardware cost and simplifying the\ncontrol circuitry. It also achieves substantial power savings as compared to\nthe baseline schemes, while maintaining sufficient gain for a fair radio\ncoverage. For instance, numerical simulations demonstrate that the proposed\ndesign reduces the power consumption by almost 86-92\\% and the control signals\nby 83-98\\% compared to operation with fully controlled RIS.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22702v1", "categories": ["eess.SY", "cs.AR", "cs.SY", "eess.SP"], "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.22702v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.22884", "title": "Performance Measurements in the AI-Centric Computing Continuum Systems", "authors": ["Praveen Kumar Donta", "Qiyang Zhang", "Schahram Dustdar"], "summary": "Over the Eight decades, computing paradigms have shifted from large,\ncentralized systems to compact, distributed architectures, leading to the rise\nof the Distributed Computing Continuum (DCC). In this model, multiple layers\nsuch as cloud, edge, Internet of Things (IoT), and mobile platforms work\ntogether to support a wide range of applications. Recently, the emergence of\nGenerative AI and large language models has further intensified the demand for\ncomputational resources across this continuum. Although traditional performance\nmetrics have provided a solid foundation, they need to be revisited and\nexpanded to keep pace with changing computational demands and application\nrequirements. Accurate performance measurements benefit both system designers\nand users by supporting improvements in efficiency and promoting alignment with\nsystem goals. In this context, we review commonly used metrics in DCC and IoT\nenvironments. We also discuss emerging performance dimensions that address\nevolving computing needs, such as sustainability, energy efficiency, and system\nobservability. We also outline criteria and considerations for selecting\nappropriate metrics, aiming to inspire future research and development in this\ncritical area.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22884v1", "categories": ["cs.DC", "cs.AI", "cs.ET", "cs.NI", "cs.SY", "eess.SY"], "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.22884v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.22492", "title": "Report on NSF Workshop on Science of Safe AI", "authors": ["Rajeev Alur", "Greg Durrett", "Hadas Kress-Gazit", "Corina PƒÉsƒÉreanu", "Ren√© Vidal"], "summary": "Recent advances in machine learning, particularly the emergence of foundation\nmodels, are leading to new opportunities to develop technology-based solutions\nto societal problems. However, the reasoning and inner workings of today's\ncomplex AI models are not transparent to the user, and there are no safety\nguarantees regarding their predictions. Consequently, to fulfill the promise of\nAI, we must address the following scientific challenge: how to develop AI-based\nsystems that are not only accurate and performant but also safe and\ntrustworthy?\n  The criticality of safe operation is particularly evident for autonomous\nsystems for control and robotics, and was the catalyst for the Safe Learning\nEnabled Systems (SLES) program at NSF. For the broader class of AI\napplications, such as users interacting with chatbots and clinicians receiving\ntreatment recommendations, safety is, while no less important, less\nwell-defined with context-dependent interpretations. This motivated the\norganization of a day-long workshop, held at University of Pennsylvania on\nFebruary 26, 2025, to bring together investigators funded by the NSF SLES\nprogram with a broader pool of researchers studying AI safety. This report is\nthe result of the discussions in the working groups that addressed different\naspects of safety at the workshop. The report articulates a new research agenda\nfocused on developing theory, methods, and tools that will provide the\nfoundations of the next generation of AI-enabled systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22492v1", "categories": ["cs.CY", "cs.AI"], "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.22492v1", "date": "2025-06-24", "updated": "2025-06-24"}
{"id": "2506.22498", "title": "ViFusionTST: Deep Fusion of Time-Series Image Representations from Load Signals for Early Bed-Exit Prediction", "authors": ["Hao Liu", "Yu Hu", "Rakiba Rayhana", "Ling Bai", "Zheng Liu"], "summary": "Bed-related falls remain a leading source of injury in hospitals and\nlong-term-care facilities, yet many commercial alarms trigger only after a\npatient has already left the bed. We show that early bed-exit intent can be\npredicted using only four low-cost load cells mounted under the bed legs. The\nresulting load signals are first converted into a compact set of complementary\nimages: an RGB line plot that preserves raw waveforms and three texture maps -\nrecurrence plot, Markov transition field, and Gramian angular field - that\nexpose higher-order dynamics. We introduce ViFusionTST, a dual-stream Swin\nTransformer that processes the line plot and texture maps in parallel and fuses\nthem through cross-attention to learn data-driven modality weights.\n  To provide a realistic benchmark, we collected six months of continuous data\nfrom 95 beds in a long-term-care facility. On this real-world dataset\nViFusionTST reaches an accuracy of 0.885 and an F1 score of 0.794, surpassing\nrecent 1D and 2D time-series baselines across F1, recall, accuracy, and AUPRC.\nThe results demonstrate that image-based fusion of load-sensor signals for time\nseries classification is a practical and effective solution for real-time,\nprivacy-preserving fall prevention.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22498v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22498v1", "date": "2025-06-25", "updated": "2025-06-25"}
{"id": "2506.22969", "title": "SparStencil: Retargeting Sparse Tensor Cores to Scientific Stencil Computations via Structured Sparsity Transformation", "authors": ["Qi Li", "Kun Li", "Haozhi Han", "Liang Yuan", "Junshi Chen", "Yunquan Zhang", "Yifeng Chen", "Hong An", "Ting Cao", "Mao Yang"], "summary": "Sparse Tensor Cores offer exceptional performance gains for AI workloads by\nexploiting structured 2:4 sparsity. However, their potential remains untapped\nfor core scientific workloads such as stencil computations, which exhibit\nirregular sparsity patterns.This paper presents SparStencil, the first system\nto retarget sparse TCUs for scientific stencil computations through structured\nsparsity transformation. SparStencil introduces three key techniques: (1)\nAdaptive Layout Morphing, which restructures stencil patterns into\nstaircase-aligned sparse matrices via a flatten-and-crush pipeline; (2)\nStructured Sparsity Conversion, which formulates transformation as a graph\nmatching problem to ensure compatibility with 2:4 sparsity constraints; (3)\nAutomatic Kernel Generation, which compiles transformed stencils into optimized\nsparse MMA kernels via layout search and table-driven memory mapping. Evaluated\non 79 stencil kernels spanning diverse scientific domains, SparStencil achieves\nup to 7.1x speedup (3.1x on average) over state-of-the-art framework while\nreducing code complexity and matching or exceeding expert-tuned performance in\nboth compute throughput and memory efficiency.", "comment": "Accepted to SC'25 (June 3, 2025). This work was previously submitted\n  to ISCA'25 (Nov 22, 2024) and substantially revised based on feedback", "pdf_url": "http://arxiv.org/pdf/2506.22969v1", "categories": ["cs.CE"], "cate": "cs.CE", "url": "http://arxiv.org/abs/2506.22969v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.22702", "title": "A Correlation-Based Design of RIS for Reduced Power Consumption and Simplified Control Circuitry", "authors": ["Zina Mohamed", "Ammar B. Kouki", "Sonia A√Øssa"], "summary": "Aiming at simplifying the hardware structure and reducing the energy\nconsumption in wireless communication via reconfigurable intelligent surfaces\n(RIS), this paper introduces a novel RIS design founded on the correlation\nbetween the phase shift values of the surface elements. First, a correlation\nanalysis is conducted, considering the azimuth angle of a target device within\na coverage region spanning from $-80^{\\circ}$ to $80^{\\circ}$. The correlation\nis demonstrated for different deployment cases, creating the basis for the new\nRIS structure, termed Connected-RIS, where correlated elements are designed to\nshare the same control signal. The fundamental performance of the proposed\ndesign is then analyzed in terms of control signals, power consumption, and\ncommunication system performance, comparing it to two RIS structures with full\ncontrol: one with the same size as the proposed design, and the other employing\nthe minimum number of elements necessary to satisfy the fair coverage\ncriterion. The correlation-based RIS design enables three-dimensional passive\nbeamforming and significantly reduces the number of required load impedances\nand control signals, thereby lowering the hardware cost and simplifying the\ncontrol circuitry. It also achieves substantial power savings as compared to\nthe baseline schemes, while maintaining sufficient gain for a fair radio\ncoverage. For instance, numerical simulations demonstrate that the proposed\ndesign reduces the power consumption by almost 86-92\\% and the control signals\nby 83-98\\% compared to operation with fully controlled RIS.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22702v1", "categories": ["eess.SY", "cs.AR", "cs.SY", "eess.SP"], "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.22702v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.22456", "title": "WISVA: Generative AI for 5G Network Optimization in Smart Warehouses", "authors": ["Rahul Gulia", "Amlan Ganguly", "Andres Kwasinski", "Michael E. Kuhl", "Ehsan Rashedi", "Clark Hochgraf"], "summary": "The next decade will usher in a profound transformation of wireless\ncommunication, driven by the ever-increasing demand for data-intensive\napplications and the rapid adoption of emerging technologies. To fully unlock\nthe potential of 5G and beyond, substantial advancements are required in signal\nprocessing techniques, innovative network architectures, and efficient spectrum\nutilization strategies. These advancements facilitate seamless integration of\nemerging technologies, driving industrial digital transformation and\nconnectivity. This paper introduces a novel Variational Autoencoder (VAE)-based\nframework, Wireless Infrastructure for Smart Warehouses using VAE (WISVA),\ndesigned for accurate indoor radio propagation modeling in automated Industry\n4.0 environments such as warehouses and factory floors operating within 5G\nwireless bands. The research delves into the meticulous creation of training\ndata tensors, capturing complex electromagnetic (EM) wave behaviors influenced\nby diverse obstacles, and outlines the architecture and training methodology of\nthe proposed VAE model. The model's robustness and adaptability are showcased\nthrough its ability to predict signal-to-interference-plus-noise ratio (SINR)\nheatmaps across various scenarios, including denoising tasks, validation\ndatasets, extrapolation to unseen configurations, and previously unencountered\nwarehouse layouts. Compelling reconstruction error heatmaps are presented,\nhighlighting the superior accuracy of WISVA compared to traditional autoencoder\nmodels. The paper also analyzes the model's performance in handling complex\nsmart warehouse environments, demonstrating its potential as a key enabler for\noptimizing wireless infrastructure in Industry 4.0.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22456v1", "categories": ["eess.SP", "eess.IV"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.22456v1", "date": "2025-06-16", "updated": "2025-06-16"}
{"id": "2506.22790", "title": "ICME 2025 Generalizable HDR and SDR Video Quality Measurement Grand Challenge", "authors": ["Yixu Chen", "Bowen Chen", "Hai Wei", "Alan C. Bovik", "Baojun Li", "Wei Sun", "Linhan Cao", "Kang Fu", "Dandan Zhu", "Jun Jia", "Menghan Hu", "Xiongkuo Min", "Guangtao Zhai", "Dounia Hammou", "Fei Yin", "Rafal Mantiuk", "Amritha Premkumar", "Prajit T Rajendran", "Vignesh V Menon"], "summary": "This paper reports IEEE International Conference on Multimedia \\& Expo (ICME)\n2025 Grand Challenge on Generalizable HDR and SDR Video Quality Measurement.\nWith the rapid development of video technology, especially High Dynamic Range\n(HDR) and Standard Dynamic Range (SDR) contents, the need for robust and\ngeneralizable Video Quality Assessment (VQA) methods has become increasingly\ndemanded. Existing VQA models often struggle to deliver consistent performance\nacross varying dynamic ranges, distortion types, and diverse content. This\nchallenge was established to benchmark and promote VQA approaches capable of\njointly handling HDR and SDR content. In the final evaluation phase, five teams\nsubmitted seven models along with technical reports to the Full Reference (FR)\nand No Reference (NR) tracks. Among them, four methods outperformed VMAF\nbaseline, while the top-performing model achieved state-of-the-art performance,\nsetting a new benchmark for generalizable video quality assessment.", "comment": "ICME 2025 Grand Challenges", "pdf_url": "http://arxiv.org/pdf/2506.22790v1", "categories": ["eess.IV", "cs.CV", "cs.MM"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.22790v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23001", "title": "The ultimate display: Where will all the pixels come from?", "authors": ["Benjamin Watson", "David Luebke"], "summary": "Could the answer be to compute fewer pixels? Renderers that break traditional\nframed patterns and opt for temporally adaptive sampling might be the key to\nprinter-resolution wall displays that update hundreds of times per second.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23001v1", "categories": ["cs.GR"], "cate": "cs.GR", "url": "http://arxiv.org/abs/2506.23001v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.22778", "title": "Tight Additive Sensitivity on LZ-style Compressors and String Attractors", "authors": ["Yuto Fujie", "Hiroki Shibata", "Yuto Nakashima", "Shunsuke Inenaga"], "summary": "The worst-case additive sensitivity of a string repetitiveness measure $c$ is\ndefined to be the largest difference between $c(w)$ and $c(w')$, where $w$ is a\nstring of length $n$ and $w'$ is a string that can be obtained by performing a\nsingle-character edit operation on $w$. We present $O(\\sqrt{n})$ upper bounds\nfor the worst-case additive sensitivity of the smallest string attractor size\n$\\gamma$ and the smallest bidirectional scheme size $b$, which match the known\nlower bounds $\\Omega(\\sqrt{n})$ for $\\gamma$ and $b$ [Akagi et al. 2023].\nFurther, we present matching upper and lower bounds for the worst-case additive\nsensitivity of the Lempel-Ziv family - $\\Theta(n^{\\frac{2}{3}})$ for LZSS and\nLZ-End, and $\\Theta(n)$ for LZ78.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22778v1", "categories": ["cs.DS", "cs.CC"], "cate": "cs.DS", "url": "http://arxiv.org/abs/2506.22778v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.22444", "title": "Active Learning for Forecasting Severity among Patients with Post Acute Sequelae of SARS-CoV-2", "authors": ["Jing Wang", "Amar Sra", "Jeremy C. Weiss"], "summary": "The long-term effects of Postacute Sequelae of SARS-CoV-2, known as PASC,\npose a significant challenge to healthcare systems worldwide. Accurate\nidentification of progression events, such as hospitalization and reinfection,\nis essential for effective patient management and resource allocation. However,\ntraditional models trained on structured data struggle to capture the nuanced\nprogression of PASC. In this study, we introduce the first publicly available\ncohort of 18 PASC patients, with text time series features based on Large\nLanguage Model Llama-3.1-70B-Instruct and clinical risk annotated by clinical\nexpert. We propose an Active Attention Network to predict the clinical risk and\nidentify progression events related to the risk. By integrating human expertise\nwith active learning, we aim to enhance clinical risk prediction accuracy and\nenable progression events identification with fewer number of annotation. The\nultimate goal is to improves patient care and decision-making for SARS-CoV-2\npatient.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22444v1", "categories": ["cs.LG", "cs.CY"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22444v1", "date": "2025-06-11", "updated": "2025-06-11"}
{"id": "2506.22740", "title": "Explanations are a means to an end", "authors": ["Jessica Hullman", "Ziyang Guo", "Berk Ustun"], "summary": "Modern methods for explainable machine learning are designed to describe how\nmodels map inputs to outputs--without deep consideration of how these\nexplanations will be used in practice. This paper argues that explanations\nshould be designed and evaluated with a specific end in mind. We describe how\nto formalize this end in a framework based in statistical decision theory. We\nshow how this functionally-grounded approach can be applied across diverse use\ncases, such as clinical decision support, providing recourse, or debugging. We\ndemonstrate its use to characterize the maximum \"boost\" in performance on a\nparticular task that an explanation could provide an idealized decision-maker,\npreventing misuse due to ambiguity by forcing researchers to specify concrete\nuse cases that can be analyzed in light of models of expected explanation use.\nWe argue that evaluation should meld theoretical and empirical perspectives on\nthe value of explanation, and contribute definitions that span these\nperspectives.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22740v1", "categories": ["cs.AI", "stat.ML"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.22740v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23085", "title": "Enhancing Live Broadcast Engagement: A Multi-modal Approach to Short Video Recommendations Using MMGCN and User Preferences", "authors": ["Saeid Aghasoleymani Najafabadi"], "summary": "The purpose of this paper is to explore a multi-modal approach to enhancing\nlive broadcast engagement by developing a short video recommendation system\nthat incorporates Multi-modal Graph Convolutional Networks (MMGCN) with user\npreferences. In order to provide personalized recommendations tailored to\nindividual interests, the proposed system takes into account user interaction\ndata, video content features, and contextual information. With the aid of a\nhybrid approach combining collaborative filtering and content-based filtering\ntechniques, the system is able to capture nuanced relationships between users,\nvideo attributes, and engagement patterns. Three datasets are used to evaluate\nthe effectiveness of the system: Kwai, TikTok, and MovieLens. Compared to\nbaseline models, such as DeepFM, Wide & Deep, LightGBM, and XGBoost, the\nproposed MMGCN-based model shows superior performance. A notable feature of the\nproposed model is that it outperforms all baseline methods in capturing diverse\nuser preferences and making accurate, personalized recommendations, resulting\nin a Kwai F1 score of 0.574, a Tiktok F1 score of 0.506, and a MovieLens F1\nscore of 0.197. We emphasize the importance of multi-modal integration and\nuser-centric approaches in advancing recommender systems, emphasizing the role\nthey play in enhancing content discovery and audience interaction on live\nbroadcast platforms.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23085v1", "categories": ["cs.IR", "cs.AI"], "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.23085v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23553", "title": "Human-CLAP: Human-perception-based contrastive language-audio pretraining", "authors": ["Taisei Takano", "Yuki Okamoto", "Yusuke Kanamori", "Yuki Saito", "Ryotaro Nagase", "Hiroshi Saruwatari"], "summary": "Contrastive language-audio pretraining (CLAP) is widely used for audio\ngeneration and recognition tasks. For example, CLAPScore, which utilizes the\nsimilarity of CLAP embeddings, has been a major metric for the evaluation of\nthe relevance between audio and text in text-to-audio. However, the\nrelationship between CLAPScore and human subjective evaluation scores is still\nunclarified. We show that CLAPScore has a low correlation with human subjective\nevaluation scores. Additionally, we propose a human-perception-based CLAP\ncalled Human-CLAP by training a contrastive language-audio model using the\nsubjective evaluation score. In our experiments, the results indicate that our\nHuman-CLAP improved the Spearman's rank correlation coefficient (SRCC) between\nthe CLAPScore and the subjective evaluation scores by more than 0.25 compared\nwith the conventional CLAP.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23553v1", "categories": ["eess.AS", "cs.SD"], "cate": "eess.AS", "url": "http://arxiv.org/abs/2506.23553v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22491", "title": "PromptAug: Fine-grained Conflict Classification Using Data Augmentation", "authors": ["Oliver Warke", "Joemon M. Jose", "Faegheh Hasibi", "Jan Breitsohl"], "summary": "Given the rise of conflicts on social media, effective classification models\nto detect harmful behaviours are essential. Following the\ngarbage-in-garbage-out maxim, machine learning performance depends heavily on\ntraining data quality. However, high-quality labelled data, especially for\nnuanced tasks like identifying conflict behaviours, is limited, expensive, and\ndifficult to obtain. Additionally, as social media platforms increasingly\nrestrict access to research data, text data augmentation is gaining attention\nas an alternative to generate training data. Augmenting conflict-related data\nposes unique challenges due to Large Language Model (LLM) guardrails that\nprevent generation of offensive content. This paper introduces PromptAug, an\ninnovative LLM-based data augmentation method. PromptAug achieves statistically\nsignificant improvements of 2% in both accuracy and F1-score on conflict and\nemotion datasets. To thoroughly evaluate PromptAug against other data\naugmentation methods we conduct a robust evaluation using extreme data scarcity\nscenarios, quantitative diversity analysis and a qualitative thematic analysis.\nThe thematic analysis identifies four problematic patterns in augmented text:\nLinguistic Fluidity, Humour Ambiguity, Augmented Content Ambiguity, and\nAugmented Content Misinterpretation.\n  Overall, this work presents PromptAug as an effective method for augmenting\ndata in sensitive tasks like conflict detection, offering a unique,\ninterdisciplinary evaluation grounded in both natural language processing and\nsocial science methodology.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22491v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "I.2.7; J.4; K.4.2"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.22491v1", "date": "2025-06-24", "updated": "2025-06-24"}
{"id": "2506.22664", "title": "Hybrid Explicit-Implicit Predictor-Corrector Exponential Time-Differencing Multistep Pad√© Schemes for Semilinear Parabolic Equations with Time-Delay", "authors": ["Haishen Dai", "Huan Lei"], "summary": "In this paper, we propose and analyze ETD-Multistep-Pad\\'{e}\n(ETD-MS-Pad\\'{e}) and ETD Implicit Multistep-Pad\\'{e} (ETD-IMS-Pad\\'{e}) for\nsemilinear parabolic delay differential equations with smooth solutions. In our\nprevious work [15], we proposed ETD-RK-Pad\\'{e} scheme to compute high-order\nnumerical solutions for nonlinear parabolic reaction-diffusion equation with\nconstant time delay. However, the based ETD-RK numerical scheme in [15] is very\ncomplex and the corresponding calculation program is also very complicated. We\npropose in this paper ETD-MS-Pad\\'{e} and ETD-IMS-Pad\\'{e} schemes for the\nsolution of semilinear parabolic equations with delay. We synergize the\nETD-MS-Pad\\'{e} with ETD-IMS-Pad\\'{e} to construct efficient\npredictor-corrector scheme. This new predictor-corrector scheme will become an\nimportant tool for solving the numerical solutions of parabolic differential\nequations. Remarkably, we also conducted experiments in Table$10$ to compare\nthe numerical results of the predictor-corrector scheme with the EERK scheme\nproposed in paper [42]. The predictor-corrector scheme demonstrated better\nconvergence.\n  The main idea is to employ an ETD-based Adams multistep extrapolation for the\ntime integration of the corresponding equation. To overcome the well-known\nnumerical instability associated with computing the exponential operator, we\nutilize the Pad\\'{e} approach to approximate this exponential operator. This\nmethodology leads to the development of the ETD-MS-Pad\\'{e} and\nETD-IMS-Pad\\'{e} schemes, applicable even for arbitrary time orders. We\nvalidate the ETD-MS1,2,3,4-Pad\\'{e} schemes and ETD-IMS2,3,4 schemes through\nnumerical experiments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22664v1", "categories": ["math.NA", "cs.NA"], "cate": "math.NA", "url": "http://arxiv.org/abs/2506.22664v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.24041", "title": "Unsupervised Sparse Coding-based Spiking Neural Network for Real-time Spike Sorting", "authors": ["Alexis Melot", "Sean U. N. Wood", "Yannick Coffinier", "Pierre Yger", "Fabien Alibart"], "summary": "Spike sorting is a crucial step in decoding multichannel extracellular neural\nsignals, enabling the identification of individual neuronal activity. A key\nchallenge in brain-machine interfaces (BMIs) is achieving real-time, low-power\nspike sorting at the edge while keeping high neural decoding performance. This\nstudy introduces the Neuromorphic Sparse Sorter (NSS), a compact two-layer\nspiking neural network optimized for efficient spike sorting. NSS leverages the\nLocally Competitive Algorithm (LCA) for sparse coding to extract relevant\nfeatures from noisy events with reduced computational demands. NSS learns to\nsort detected spike waveforms in an online fashion and operates entirely\nunsupervised. To exploit multi-bit spike coding capabilities of neuromorphic\nplatforms like Intel's Loihi 2, a custom neuron model was implemented, enabling\nflexible power-performance trade-offs via adjustable spike bit-widths.\nEvaluations on simulated and real-world tetrode signals with biological drift\nshowed NSS outperformed established pipelines such as WaveClus3 and PCA+KMeans.\nWith 2-bit graded spikes, NSS on Loihi 2 outperformed NSS implemented with\nleaky integrate-and-fire neuron and achieved an F1-score of 77% (+10%\nimprovement) while consuming 8.6mW (+1.65mW) when tested on a drifting\nrecording, with a computational processing time of 0.25ms (+60 us) per\ninference.", "comment": "Main article : 16 pages, 7 figures and 4 tables. Supplementary\n  Material starts at page 17 with 7 figures", "pdf_url": "http://arxiv.org/pdf/2506.24041v1", "categories": ["cs.NE", "cs.LG"], "cate": "cs.NE", "url": "http://arxiv.org/abs/2506.24041v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22810", "title": "A Self-Training Approach for Whisper to Enhance Long Dysarthric Speech Recognition", "authors": ["Shiyao Wang", "Jiaming Zhou", "Shiwan Zhao", "Yong Qin"], "summary": "Dysarthric speech recognition (DSR) enhances the accessibility of smart\ndevices for dysarthric speakers with limited mobility. Previously, DSR research\nwas constrained by the fact that existing datasets typically consisted of\nisolated words, command phrases, and a limited number of sentences spoken by a\nfew individuals. This constrained research to command-interaction systems and\nspeaker adaptation. The Speech Accessibility Project (SAP) changed this by\nreleasing a large and diverse English dysarthric dataset, leading to the SAP\nChallenge to build speaker- and text-independent DSR systems. We enhanced the\nWhisper model's performance on long dysarthric speech via a novel self-training\nmethod. This method increased training data and adapted the model to handle\npotentially incomplete speech segments encountered during inference. Our system\nachieved second place in both Word Error Rate and Semantic Score in the SAP\nChallenge.", "comment": "accepted by Interspeech 2025", "pdf_url": "http://arxiv.org/pdf/2506.22810v1", "categories": ["cs.SD", "eess.AS"], "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.22810v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.22818", "title": "TriADA: Massively Parallel Trilinear Matrix-by-Tensor Multiply-Add Algorithm and Device Architecture for the Acceleration of 3D Discrete Transformations", "authors": ["Stanislav Sedukhin", "Yoichi Tomioka", "Kazuya Matsumoto", "Yuichi Okuyama"], "summary": "Multilinear transformations are key in high-performance computing (HPC) and\nartificial intelligence (AI) workloads, where data is represented as tensors.\nHowever, their high computational and memory demands, which grow with\ndimensionality, often slow down critical tasks. Moreover, scaling computation\nby enlarging the number of parallel processing units substantially increases\nenergy consumption, limiting widespread adoption, especially for sparse data,\nwhich is common in HPC and AI applications. This paper introduces the Trilinear\nAlgorithm and isomorphic to algorithm Device Architecture (TriADA) to address\nthese challenges with the following innovations: (1) a massively parallel,\nlow-rank algorithm for computing a family of trilinear (3D) discrete orthogonal\ntransformations (3D-DXTs), which is a special case of the more general 3-mode\nmatrix-by-tensor multiplication (3D-GEMT); (2) a new outer-product-based GEMM\nkernel with decoupled streaming active memory, specially designed to accelerate\n3D-GEMT operation; (3) an isomorphic to the proposed algorithm, fully\ndistributed 3D network of mesh interconnected processing elements or cells with\na coordinate-free, data-driven local processing activity, which is independent\nof problem size; (4) an elastic sparse outer-product (ESOP) method that avoids\nunnecessary computing and communication operations with zero-valued operands,\nthereby enhancing energy efficiency, computational accuracy, and stability.\nTriADA is capable of performing a variety of trilinear transformations with\nhypercubic arithmetic complexity in a linear number of time-steps. The\nmassively parallel, scalable, and energy-efficient architecture of TriADA is\nideal for accelerating multilinear tensor operations, which are the most\ndemanding parts of AI and HPC workloads.", "comment": "19 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2506.22818v1", "categories": ["cs.DC", "cs.AI", "cs.AR", "cs.ET", "eess.SP", "C.1.4; C.3; F.2.1; G.1.3; G.4"], "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.22818v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.22639", "title": "Fingerprinting SDKs for Mobile Apps and Where to Find Them: Understanding the Market for Device Fingerprinting", "authors": ["Michael A. Specter", "Mihai Christodorescu", "Abbie Farr", "Bo Ma", "Robin Lassonde", "Xiaoyang Xu", "Xiang Pan", "Fengguo Wei", "Saswat Anand", "Dave Kleidermacher"], "summary": "This paper presents a large-scale analysis of fingerprinting-like behavior in\nthe mobile application ecosystem. We take a market-based approach, focusing on\nthird-party tracking as enabled by applications' common use of third-party\nSDKs. Our dataset consists of over 228,000 SDKs from popular Maven\nrepositories, 178,000 Android applications collected from the Google Play\nstore, and our static analysis pipeline detects exfiltration of over 500\nindividual signals. To the best of our knowledge, this represents the\nlargest-scale analysis of SDK behavior undertaken to date.\n  We find that Ads SDKs (the ostensible focus of industry efforts such as\nApple's App Tracking Transparency and Google's Privacy Sandbox) appear to be\nthe source of only 30.56% of the fingerprinting behaviors. A surprising 23.92%\noriginate from SDKs whose purpose was unknown or unclear. Furthermore, Security\nand Authentication SDKs are linked to only 11.7% of likely fingerprinting\ninstances. These results suggest that addressing fingerprinting solely in\nspecific market-segment contexts like advertising may offer incomplete benefit.\nEnforcing anti-fingerprinting policies is also complex, as we observe a sparse\ndistribution of signals and APIs used by likely fingerprinting SDKs. For\ninstance, only 2% of exfiltrated APIs are used by more than 75% of SDKs, making\nit difficult to rely on user permissions to control fingerprinting behavior.", "comment": "To appear in ACM CCS 2025. Extended from conference version; has\n  added appendices more inclusive author list", "pdf_url": "http://arxiv.org/pdf/2506.22639v1", "categories": ["cs.CR"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.22639v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22855", "title": "Momentum-based Accelerated Algorithm for Distributed Optimization under Sector-Bound Nonlinearity", "authors": ["Mohammadreza Doostmohammadian", "Hamid R. Rabiee"], "summary": "Distributed optimization advances centralized machine learning methods by\nenabling parallel and decentralized learning processes over a network of\ncomputing nodes. This work provides an accelerated consensus-based distributed\nalgorithm for locally non-convex optimization using the gradient-tracking\ntechnique. The proposed algorithm (i) improves the convergence rate by adding\nmomentum towards the optimal state using the heavy-ball method, while (ii)\naddressing general sector-bound nonlinearities over the information-sharing\nnetwork. The link nonlinearity includes any sign-preserving odd sector-bound\nmapping, for example, log-scale data quantization or clipping in practical\napplications. For admissible momentum and gradient-tracking parameters, using\nperturbation theory and eigen-spectrum analysis, we prove convergence even in\nthe presence of sector-bound nonlinearity and for locally non-convex cost\nfunctions. Further, in contrast to most existing weight-stochastic algorithms,\nwe adopt weight-balanced (WB) network design. This WB design and\nperturbation-based analysis allow to handle dynamic directed network of agents\nto address possible time-varying setups due to link failures or packet drops.", "comment": "Journal of the Franklin Institute", "pdf_url": "http://arxiv.org/pdf/2506.22855v1", "categories": ["eess.SY", "cs.DC", "cs.MA", "cs.SY", "eess.SP", "math.OC"], "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.22855v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.22593", "title": "Pixels-to-Graph: Real-time Integration of Building Information Models and Scene Graphs for Semantic-Geometric Human-Robot Understanding", "authors": ["Antonello Longo", "Chanyoung Chung", "Matteo Palieri", "Sung-Kyun Kim", "Ali Agha", "Cataldo Guaragnella", "Shehryar Khattak"], "summary": "Autonomous robots are increasingly playing key roles as support platforms for\nhuman operators in high-risk, dangerous applications. To accomplish challenging\ntasks, an efficient human-robot cooperation and understanding is required.\nWhile typically robotic planning leverages 3D geometric information, human\noperators are accustomed to a high-level compact representation of the\nenvironment, like top-down 2D maps representing the Building Information Model\n(BIM). 3D scene graphs have emerged as a powerful tool to bridge the gap\nbetween human readable 2D BIM and the robot 3D maps. In this work, we introduce\nPixels-to-Graph (Pix2G), a novel lightweight method to generate structured\nscene graphs from image pixels and LiDAR maps in real-time for the autonomous\nexploration of unknown environments on resource-constrained robot platforms. To\nsatisfy onboard compute constraints, the framework is designed to perform all\noperation on CPU only. The method output are a de-noised 2D top-down\nenvironment map and a structure-segmented 3D pointcloud which are seamlessly\nconnected using a multi-layer graph abstracting information from object-level\nup to the building-level. The proposed method is quantitatively and\nqualitatively evaluated during real-world experiments performed using the NASA\nJPL NeBula-Spot legged robot to autonomously explore and map cluttered garage\nand urban office like environments in real-time.", "comment": "Paper accepted to 2025 IEEE International Conference on Automation\n  Science and Engineering (CASE)", "pdf_url": "http://arxiv.org/pdf/2506.22593v1", "categories": ["cs.RO", "cs.AI", "cs.CV"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.22593v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22480", "title": "Service Placement in Small Cell Networks Using Distributed Best Arm Identification in Linear Bandits", "authors": ["Mariam Yahya", "Aydin Sezgin", "Setareh Maghsudi"], "summary": "As users in small cell networks increasingly rely on computation-intensive\nservices, cloud-based access often results in high latency. Multi-access edge\ncomputing (MEC) mitigates this by bringing computational resources closer to\nend users, with small base stations (SBSs) serving as edge servers to enable\nlow-latency service delivery. However, limited edge capacity makes it\nchallenging to decide which services to deploy locally versus in the cloud,\nespecially under unknown service demand and dynamic network conditions. To\ntackle this problem, we model service demand as a linear function of service\nattributes and formulate the service placement task as a linear bandit problem,\nwhere SBSs act as agents and services as arms. The goal is to identify the\nservice that, when placed at the edge, offers the greatest reduction in total\nuser delay compared to cloud deployment. We propose a distributed and adaptive\nmulti-agent best-arm identification (BAI) algorithm under a fixed-confidence\nsetting, where SBSs collaborate to accelerate learning. Simulations show that\nour algorithm identifies the optimal service with the desired confidence and\nachieves near-optimal speedup, as the number of learning rounds decreases\nproportionally with the number of SBSs. We also provide theoretical analysis of\nthe algorithm's sample complexity and communication overhead.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22480v1", "categories": ["cs.NI", "cs.DC", "cs.LG"], "cate": "cs.NI", "url": "http://arxiv.org/abs/2506.22480v1", "date": "2025-06-22", "updated": "2025-06-22"}
{"id": "2506.22497", "title": "Peer Review as Structured Commentary: Immutable Identity, Public Dialogue, and Reproducible Scholarship", "authors": ["Craig Steven Wright"], "summary": "This paper reconceptualises peer review as structured public commentary.\nTraditional academic validation is hindered by anonymity, latency, and\ngatekeeping. We propose a transparent, identity-linked, and reproducible system\nof scholarly evaluation anchored in open commentary. Leveraging blockchain for\nimmutable audit trails and AI for iterative synthesis, we design a framework\nthat incentivises intellectual contribution, captures epistemic evolution, and\nenables traceable reputational dynamics. This model empowers fields from\ncomputational science to the humanities, reframing academic knowledge as a\nliving process rather than a static credential.", "comment": "66 pages, 0 figures, interdisciplinary framework, includes proposed\n  architecture and metadata layer structures", "pdf_url": "http://arxiv.org/pdf/2506.22497v1", "categories": ["cs.CY", "cs.AI", "cs.DL", "cs.SI", "physics.hist-ph", "68T99, 03B30, 91D30", "I.2.0; H.3.5; K.4.4"], "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.22497v1", "date": "2025-06-25", "updated": "2025-06-25"}
{"id": "2506.22752", "title": "Privacy-Preserving Methods for Bug Severity Prediction", "authors": ["Havvanur Dervi≈üoƒülu", "Ru≈üen Halepmollasƒ±", "Elif Eyvaz"], "summary": "Bug severity prediction is a critical task in software engineering as it\nenables more efficient resource allocation and prioritization in software\nmaintenance. While AI-based analyses and models significantly require access to\nextensive datasets, industrial applications face challenges due to data-sharing\nconstraints and the limited availability of labeled data. In this study, we\ninvestigate method-level bug severity prediction using source code metrics and\nLarge Language Models (LLMs) with two widely used datasets. We compare the\nperformance of models trained using centralized learning, federated learning,\nand synthetic data generation. Our experimental results, obtained using two\nwidely recognized software defect datasets, indicate that models trained with\nfederated learning and synthetic data achieve comparable results to centrally\ntrained models without data sharing. Our finding highlights the potential of\nprivacy-preserving approaches such as federated learning and synthetic data\ngeneration to enable effective bug severity prediction in industrial context\nwhere data sharing is a major challenge.\n  The source code and dataset are available at our GitHub repository:\nhttps://github.com/drvshavva/EASE2025-Privacy-Preserving-Methods-for-Bug-Severity-Prediction.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22752v1", "categories": ["cs.SE"], "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.22752v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.22741", "title": "Insights in Adaptation: Examining Self-reflection Strategies of Job Seekers with Visual Impairments in India", "authors": ["Akshay Nayak Kolgar", "Yash Prakash", "Sampath Jayarathna", "Hae-Na Lee", "Vikas Ashok"], "summary": "Significant changes in the digital employment landscape, driven by rapid\ntechnological advancements and the COVID-19 pandemic, have introduced new\nopportunities for blind and visually impaired (BVI) individuals in developing\ncountries like India. However, a significant portion of the BVI population in\nIndia remains unemployed despite extensive accessibility advancements and job\nsearch interventions. Therefore, we conducted semi-structured interviews with\n20 BVI persons who were either pursuing or recently sought employment in the\ndigital industry. Our findings reveal that despite gaining digital literacy and\nextensive training, BVI individuals struggle to meet industry requirements for\nfulfilling job openings. While they engage in self-reflection to identify\nshortcomings in their approach and skills, they lack constructive feedback from\npeers and recruiters. Moreover, the numerous job intervention tools are limited\nin their ability to meet the unique needs of BVI job seekers. Our results\ntherefore provide key insights that inform the design of future collaborative\nintervention systems that offer personalized feedback for BVI individuals,\neffectively guiding their self-reflection process and subsequent job search\nbehaviors, and potentially leading to improved employment outcomes.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22741v1", "categories": ["cs.HC"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.22741v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23198", "title": "Hybrid Character Sums From Vectorial Dual-Bent Functions and Asymptotically Optimal Complex Codebooks With Small Alphabet Sizes", "authors": ["Ziling Heng", "Peng Wang", "Chengju Li"], "summary": "Hybrid character sums are an important class of exponential sums which have\nnice applications in coding theory and sequence design. Let $\\gf_{p^m}$ be the\nfinite field with $p^m$ elements for a prime $p$ and a positive integer $m$.\nLet $V_n^{(p)}$ be an $n$-dimensional vector space over $\\gf_p$ for a prime\n$p$. In this paper, we study the hybrid character sums of the form\n\\begin{eqnarray*} \\sum_{x \\in V_n^{(p)}}\\psi\\left(F(x)\\right)\\chi_1\\left(a\nx\\right), \\end{eqnarray*} where $F$ is a function from $V_n^{(p)}$ to\n$\\gf_{p^m}$ and $a \\in V_n^{(p)}$, $\\psi$ is a nontrivial multiplicative\ncharacter of $\\gf_{p^m}$ and $\\chi_1$ is the canonical additive character of\n$V_n^{(p)}$. If $F(x)$ is a vectorial dual-bent function and $a \\in\nV_n^{(p)}\\setminus \\{0\\}$, we determine their complex modulus or explicit\nvalues under certain conditions, which generalizes some known results as\nspecial cases. It is concluded that the hybrid character sums from vectorial\ndual-bent functions have very small complex modulus. As applications, three\nfamilies of asymptotically optimal complex codebooks are constructed from\nvectorial dual-bent functions and their maximal cross-correlation amplitude are\ndetermined based on the hybrid character sums. The constructed codebooks have\nvery small alphabet sizes, which enhances their appeal for implementation.\nBesides, all of the three families of codebooks have only two-valued or\nthree-valued cross-correlation amplitudes.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23198v1", "categories": ["cs.IT", "math.IT"], "cate": "cs.IT", "url": "http://arxiv.org/abs/2506.23198v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.22773", "title": "Not All Water Consumption Is Equal: A Water Stress Weighted Metric for Sustainable Computing", "authors": ["Yanran Wu", "Inez Hua", "Yi Ding"], "summary": "Water consumption is an increasingly critical dimension of computing\nsustainability, especially as AI workloads rapidly scale. However, current\nwater impact assessment often overlooks where and when water stress is more\nsevere. To fill in this gap, we present SCARF, the first general framework that\nevaluates water impact of computing by factoring in both spatial and temporal\nvariations in water stress. SCARF calculates an Adjusted Water Impact (AWI)\nmetric that considers both consumption volume and local water stress over time.\nThrough three case studies on LLM serving, datacenters, and semiconductor\nfabrication plants, we show the hidden opportunities for reducing water impact\nby optimizing location and time choices, paving the way for water-sustainable\ncomputing. The code is available at https://github.com/jojacola/SCARF.", "comment": "7 pages, 9 figures, HotCarbon '25: Proceedings of the 4th Workshop on\n  Sustainable Computer Systems, Cambridge, Massachusetts (USA), July 10-11th,\n  2025", "pdf_url": "http://arxiv.org/pdf/2506.22773v1", "categories": ["cs.DC", "cs.AR", "cs.CY", "cs.LG"], "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.22773v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23395", "title": "FastSet: Parallel Claim Settlement", "authors": ["Xiaohong Chen", "Grigore Rosu"], "summary": "FastSet is an actor-based distributed protocol for decentralized finance and\nsettlement, which is inspired from blockchains. Account holders cooperate by\nmaking claims, which can include payments, holding and transferring assets,\naccessing and updating shared data, medical records, digital identity, and\nmathematical theorems, among many others. The claims are signed by their owners\nand are broadcast to a decentralized network of validators, which validate and\nsettle them. Validators replicate the global state of the accounts and need not\ncommunicate with each other. In sharp contrast to blockchains, strong\nconsistency is purposely given up as a requirement. Yet, many if not most of\nthe blockchain benefits are preserved. The protocol is proved to be correct,\ndespite its massively parallel nature.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23395v1", "categories": ["cs.DC"], "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.23395v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.22493", "title": "A Detailed Factor Analysis for the Political Compass Test: Navigating Ideologies of Large Language Models", "authors": ["Sadia Kamal", "Lalu Prasad Yadav Prakash", "S M Rafiuddin", "Mohammed Rakib", "Arunkumar Bagavathi", "Atriya Sen", "Sagnik Ray Choudhury"], "summary": "Political Compass Test (PCT) or similar questionnaires have been used to\nquantify LLM's political leanings. Building on a recent line of work that\nexamines the validity of PCT tests, we demonstrate that variation in standard\ngeneration parameters does not significantly impact the models' PCT scores.\nHowever, external factors such as prompt variations and fine-tuning\nindividually and in combination affect the same. Finally, we demonstrate that\nwhen models are fine-tuned on text datasets with higher political content than\nothers, the PCT scores are not differentially affected. This calls for a\nthorough investigation into the validity of PCT and similar tests, as well as\nthe mechanism by which political leanings are encoded in LLMs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22493v1", "categories": ["cs.CY", "cs.CL", "cs.LG"], "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.22493v1", "date": "2025-06-24", "updated": "2025-06-24"}
{"id": "2506.22499", "title": "Scalable Dynamic Origin-Destination Demand Estimation Enhanced by High-Resolution Satellite Imagery Data", "authors": ["Jiachao Liu", "Pablo Guarda", "Koichiro Niinuma", "Sean Qian"], "summary": "This study presents a novel integrated framework for dynamic\norigin-destination demand estimation (DODE) in multi-class mesoscopic network\nmodels, leveraging high-resolution satellite imagery together with conventional\ntraffic data from local sensors. Unlike sparse local detectors, satellite\nimagery offers consistent, city-wide road and traffic information of both\nparking and moving vehicles, overcoming data availability limitations. To\nextract information from imagery data, we design a computer vision pipeline for\nclass-specific vehicle detection and map matching, generating link-level\ntraffic density observations by vehicle class. Building upon this information,\nwe formulate a computational graph-based DODE model that calibrates dynamic\nnetwork states by jointly matching observed traffic counts and travel times\nfrom local sensors with density measurements derived from satellite imagery. To\nassess the accuracy and scalability of the proposed framework, we conduct a\nseries of numerical experiments using both synthetic and real-world data. The\nresults of out-of-sample tests demonstrate that supplementing traditional data\nwith satellite-derived density significantly improves estimation performance,\nespecially for links without local sensors. Real-world experiments also confirm\nthe framework's capability to handle large-scale networks, supporting its\npotential for practical deployment in cities of varying sizes. Sensitivity\nanalysis further evaluates the impact of data quality related to satellite\nimagery data.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22499v1", "categories": ["cs.CV", "cs.AI", "stat.AP"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22499v1", "date": "2025-06-25", "updated": "2025-06-25"}
{"id": "2506.23028", "title": "Towards a better approach to the Vehicle Routing Problem", "authors": ["Souad Abdoune", "Menouar Boulif"], "summary": "The Vehicle Routing Problem (VRP) is a fundamental challenge in logistics\nmanagement research, given its substantial influence on transportation\nefficiency, cost minimization, and service quality. As a combinatorial\noptimization problem, VRP plays a crucial role in a wide range of real world\napplications, particularly in transportation, logistics, and delivery systems,\ndue to its diverse formulations and numerous extensions. Over the years,\nresearchers have introduced various VRP variants to address specific\noperational constraints, emerging industry requirements and optimize specific\nobjectives, making it one of the most extensively studied problems in\noperations research. This article provides a comprehensive overview of VRP by\nexploring its theoretical foundations, discussing the limitations of its\nclassical model, and introducing its key extensions. By systematically\nreviewing the diverse constraints, objectives, and variants examined in recent\nliterature, this study aims to contribute to a deeper understanding of VRP\nwhile highlighting its ongoing evolution and relevance in modern optimization\nand decision making processes.", "comment": "22 pages, 21 figures", "pdf_url": "http://arxiv.org/pdf/2506.23028v1", "categories": ["cs.CE", "math.OC", "90B06", "F.2.2; G.2.1"], "cate": "cs.CE", "url": "http://arxiv.org/abs/2506.23028v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.22707", "title": "X-pSRAM: A Photonic SRAM with Embedded XOR Logic for Ultra-Fast In-Memory Computing", "authors": ["Md Abdullah-Al Kaiser", "Sugeet Sunder", "Ajey P. Jacob", "Akhilesh R. Jaiswal"], "summary": "Traditional von Neumann architectures suffer from fundamental bottlenecks due\nto continuous data movement between memory and processing units, a challenge\nthat worsens with technology scaling as electrical interconnect delays become\nmore significant. These limitations impede the performance and energy\nefficiency required for modern data-intensive applications. In contrast,\nphotonic in-memory computing presents a promising alternative by harnessing the\nadvantages of light, enabling ultra-fast data propagation without\nlength-dependent impedance, thereby significantly reducing computational\nlatency and energy consumption. This work proposes a novel differential\nphotonic static random access memory (pSRAM) bitcell that facilitates\nelectro-optic data storage while enabling ultra-fast in-memory Boolean XOR\ncomputation. By employing cross-coupled microring resonators and differential\nphotodiodes, the XOR-augmented pSRAM (X-pSRAM) bitcell achieves at least 10 GHz\nread, write, and compute operations entirely in the optical domain.\nAdditionally, wavelength-division multiplexing (WDM) enables n-bit XOR\ncomputation in a single-shot operation, supporting massively parallel\nprocessing and enhanced computational efficiency. Validated on GlobalFoundries'\n45SPCLO node, the X-pSRAM consumed 13.2 fJ energy per bit for XOR computation,\nrepresenting a significant advancement toward next-generation optical computing\nwith applications in cryptography, hyperdimensional computing, and neural\nnetworks.", "comment": "8 pages, 6 figures, 1 table", "pdf_url": "http://arxiv.org/pdf/2506.22707v1", "categories": ["eess.SY", "cs.SY"], "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.22707v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.22457", "title": "A Complex UNet Approach for Non-Invasive Fetal ECG Extraction Using Single-Channel Dry Textile Electrodes", "authors": ["Iulia Orvas", "Andrei Radu", "Alessandra Galli", "Ana Neacsu", "Elisabetta Peri"], "summary": "Continuous, non-invasive pregnancy monitoring is crucial for minimising\npotential complications. The fetal electrocardiogram (fECG) represents a\npromising tool for assessing fetal health beyond clinical environments.\nHome-based monitoring necessitates the use of a minimal number of comfortable\nand durable electrodes, such as dry textile electrodes. However, this setup\npresents many challenges, including increased noise and motion artefacts, which\ncomplicate the accurate extraction of fECG signals. To overcome these\nchallenges, we introduce a pioneering method for extracting fECG from\nsingle-channel recordings obtained using dry textile electrodes using AI\ntechniques. We created a new dataset by simulating abdominal recordings,\nincluding noise closely resembling real-world characteristics of in-vivo\nrecordings through dry textile electrodes, alongside mECG and fECG. To ensure\nthe reliability of the extracted fECG, we propose an innovative pipeline based\non a complex-valued denoising network, Complex UNet. Unlike previous approaches\nthat focused solely on signal magnitude, our method processes both real and\nimaginary components of the spectrogram, addressing phase information and\npreventing incongruous predictions. We evaluated our novel pipeline against\ntraditional, well-established approaches, on both simulated and real data in\nterms of fECG extraction and R-peak detection. The results showcase that our\nsuggested method achieves new state-of-the-art results, enabling an accurate\nextraction of fECG morphology across all evaluated settings. This method is the\nfirst to effectively extract fECG signals from single-channel recordings using\ndry textile electrodes, making a significant advancement towards a fully\nnon-invasive and self-administered fECG extraction solution.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22457v1", "categories": ["eess.SP", "cs.AI"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.22457v1", "date": "2025-06-16", "updated": "2025-06-16"}
{"id": "2506.22882", "title": "CA-Diff: Collaborative Anatomy Diffusion for Brain Tissue Segmentation", "authors": ["Qilong Xing", "Zikai Song", "Yuteng Ye", "Yuke Chen", "Youjia Zhang", "Na Feng", "Junqing Yu", "Wei Yang"], "summary": "Segmentation of brain structures from MRI is crucial for evaluating brain\nmorphology, yet existing CNN and transformer-based methods struggle to\ndelineate complex structures accurately. While current diffusion models have\nshown promise in image segmentation, they are inadequate when applied directly\nto brain MRI due to neglecting anatomical information. To address this, we\npropose Collaborative Anatomy Diffusion (CA-Diff), a framework integrating\nspatial anatomical features to enhance segmentation accuracy of the diffusion\nmodel. Specifically, we introduce distance field as an auxiliary anatomical\ncondition to provide global spatial context, alongside a collaborative\ndiffusion process to model its joint distribution with anatomical structures,\nenabling effective utilization of anatomical features for segmentation.\nFurthermore, we introduce a consistency loss to refine relationships between\nthe distance field and anatomical structures and design a time adapted channel\nattention module to enhance the U-Net feature fusion procedure. Extensive\nexperiments show that CA-Diff outperforms state-of-the-art (SOTA) methods.", "comment": "ICME 2025", "pdf_url": "http://arxiv.org/pdf/2506.22882v1", "categories": ["eess.IV", "cs.CV", "cs.LG"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.22882v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23092", "title": "Glyph-Based Multiscale Visualization of Turbulent Multi-Physics Statistics", "authors": ["Arisa Cowe", "Tyson Neuroth", "Qi Wu", "Martin Rieth", "Jacqueline Chen", "Myoungkyu Lee", "Kwan-Liu Ma"], "summary": "Many scientific and engineering problems involving multi-physics span a wide\nrange of scales. Understanding the interactions across these scales is\nessential for fully comprehending such complex problems. However, visualizing\nmultivariate, multiscale data within an integrated view where correlations\nacross space, scales, and fields are easily perceived remains challenging. To\naddress this, we introduce a novel local spatial statistical visualization of\nflow fields across multiple fields and turbulence scales. Our method leverages\nthe curvelet transform for scale decomposition of fields of interest, a\nlevel-set-restricted centroidal Voronoi tessellation to partition the spatial\ndomain into local regions for statistical aggregation, and a set of glyph\ndesigns that combines information across scales and fields into a single, or\nreduced set of perceivable visual representations. Each glyph represents data\naggregated within a Voronoi region and is positioned at the Voronoi site for\ndirect visualization in a 3D view centered around flow features of interest. We\nimplement and integrate our method into an interactive visualization system\nwhere the glyph-based technique operates in tandem with linked 3D spatial views\nand 2D statistical views, supporting a holistic analysis. We demonstrate with\ncase studies visualizing turbulent combustion data--multi-scalar compressible\nflows--and turbulent incompressible channel flow data. This new capability\nenables scientists to better understand the interactions between multiple\nfields and length scales in turbulent flows.", "comment": "15 pages (13 pages without references)", "pdf_url": "http://arxiv.org/pdf/2506.23092v1", "categories": ["cs.GR", "cs.HC"], "cate": "cs.GR", "url": "http://arxiv.org/abs/2506.23092v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.22922", "title": "Global Predecessor Indexing: Avoiding Binary Search in Weighted Job Scheduling", "authors": ["Amit Joshi"], "summary": "We present an improved solution to the Weighted Job Scheduling (WJS) problem.\nWhile the classical dynamic programming (DP) solution runs in $O(n \\log(n))$\ntime due to comparison-based sorting and per-job binary search, we eliminate\nthe binary search bottleneck. In its place, we introduce a novel multi-phase\npreprocessing technique called Global Predecessor Indexing (GPI), which\ncomputes the latest non-overlapping job (i.e., the predecessor) for all jobs\nvia a two-pointer linear-time pass. GPI enables direct use in the classical DP\nrecurrence. When combined with linear-time sorting, GPI yields a complete\n$O(n)$ solution. Even with comparison-based sorting, GPI significantly\noutperforms the classical solution in practice by avoiding repeated binary\nsearches. Keywords: Weighted Job Scheduling, Interval Scheduling, Dynamic\nProgramming, Linear Sorting, Two Pointers, Preprocessing", "comment": "6 pages, 9 figures including tables. Short theoretical and practical\n  paper on improved dynamic programming for weighted job scheduling with\n  linear-time preprocessing", "pdf_url": "http://arxiv.org/pdf/2506.22922v1", "categories": ["cs.DS"], "cate": "cs.DS", "url": "http://arxiv.org/abs/2506.22922v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.22445", "title": "Hierarchical Adversarially-Resilient Multi-Agent Reinforcement Learning for Cyber-Physical Systems Security", "authors": ["Saad Alqithami"], "summary": "Cyber-Physical Systems play a critical role in the infrastructure of various\nsectors, including manufacturing, energy distribution, and autonomous\ntransportation systems. However, their increasing connectivity renders them\nhighly vulnerable to sophisticated cyber threats, such as adaptive and zero-day\nattacks, against which traditional security methods like rule-based intrusion\ndetection and single-agent reinforcement learning prove insufficient. To\novercome these challenges, this paper introduces a novel Hierarchical\nAdversarially-Resilient Multi-Agent Reinforcement Learning (HAMARL) framework.\nHAMARL employs a hierarchical structure consisting of local agents dedicated to\nsubsystem security and a global coordinator that oversees and optimizes\ncomprehensive, system-wide defense strategies. Furthermore, the framework\nincorporates an adversarial training loop designed to simulate and anticipate\nevolving cyber threats, enabling proactive defense adaptation. Extensive\nexperimental evaluations conducted on a simulated industrial IoT testbed\nindicate that HAMARL substantially outperforms traditional multi-agent\nreinforcement learning approaches, significantly improving attack detection\naccuracy, reducing response times, and ensuring operational continuity. The\nresults underscore the effectiveness of combining hierarchical multi-agent\ncoordination with adversarially-aware training to enhance the resilience and\nsecurity of next-generation CPS.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22445v1", "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.MA"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22445v1", "date": "2025-06-12", "updated": "2025-06-12"}
{"id": "2506.22774", "title": "Bridging Ethical Principles and Algorithmic Methods: An Alternative Approach for Assessing Trustworthiness in AI Systems", "authors": ["Michael Papademas", "Xenia Ziouvelou", "Antonis Troumpoukis", "Vangelis Karkaletsis"], "summary": "Artificial Intelligence (AI) technology epitomizes the complex challenges\nposed by human-made artifacts, particularly those widely integrated into\nsociety and exert significant influence, highlighting potential benefits and\ntheir negative consequences. While other technologies may also pose substantial\nrisks, AI's pervasive reach makes its societal effects especially profound. The\ncomplexity of AI systems, coupled with their remarkable capabilities, can lead\nto a reliance on technologies that operate beyond direct human oversight or\nunderstanding. To mitigate the risks that arise, several theoretical tools and\nguidelines have been developed, alongside efforts to create technological tools\naimed at safeguarding Trustworthy AI. The guidelines take a more holistic view\nof the issue but fail to provide techniques for quantifying trustworthiness.\nConversely, while technological tools are better at achieving such\nquantification, they lack a holistic perspective, focusing instead on specific\naspects of Trustworthy AI. This paper aims to introduce an assessment method\nthat combines the ethical components of Trustworthy AI with the algorithmic\nprocesses of PageRank and TrustRank. The goal is to establish an assessment\nframework that minimizes the subjectivity inherent in the self-assessment\ntechniques prevalent in the field by introducing algorithmic criteria. The\napplication of our approach indicates that a holistic assessment of an AI\nsystem's trustworthiness can be achieved by providing quantitative insights\nwhile considering the theoretical content of relevant guidelines.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22774v1", "categories": ["cs.AI", "cs.CY"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.22774v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23090", "title": "Multi-task Offline Reinforcement Learning for Online Advertising in Recommender Systems", "authors": ["Langming Liu", "Wanyu Wang", "Chi Zhang", "Bo Li", "Hongzhi Yin", "Xuetao Wei", "Wenbo Su", "Bo Zheng", "Xiangyu Zhao"], "summary": "Online advertising in recommendation platforms has gained significant\nattention, with a predominant focus on channel recommendation and budget\nallocation strategies. However, current offline reinforcement learning (RL)\nmethods face substantial challenges when applied to sparse advertising\nscenarios, primarily due to severe overestimation, distributional shifts, and\noverlooking budget constraints. To address these issues, we propose MTORL, a\nnovel multi-task offline RL model that targets two key objectives. First, we\nestablish a Markov Decision Process (MDP) framework specific to the nuances of\nadvertising. Then, we develop a causal state encoder to capture dynamic user\ninterests and temporal dependencies, facilitating offline RL through\nconditional sequence modeling. Causal attention mechanisms are introduced to\nenhance user sequence representations by identifying correlations among causal\nstates. We employ multi-task learning to decode actions and rewards,\nsimultaneously addressing channel recommendation and budget allocation.\nNotably, our framework includes an automated system for integrating these tasks\ninto online advertising. Extensive experiments on offline and online\nenvironments demonstrate MTORL's superiority over state-of-the-art methods.", "comment": "KDD 2025", "pdf_url": "http://arxiv.org/pdf/2506.23090v1", "categories": ["cs.IR", "cs.LG"], "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.23090v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23859", "title": "Less is More: Data Curation Matters in Scaling Speech Enhancement", "authors": ["Chenda Li", "Wangyou Zhang", "Wei Wang", "Robin Scheibler", "Kohei Saijo", "Samuele Cornell", "Yihui Fu", "Marvin Sach", "Zhaoheng Ni", "Anurag Kumar", "Tim Fingscheidt", "Shinji Watanabe", "Yanmin Qian"], "summary": "The vast majority of modern speech enhancement systems rely on data-driven\nneural network models. Conventionally, larger datasets are presumed to yield\nsuperior model performance, an observation empirically validated across\nnumerous tasks in other domains. However, recent studies reveal diminishing\nreturns when scaling speech enhancement data. We focus on a critical factor:\nprevalent quality issues in ``clean'' training labels within large-scale\ndatasets. This work re-examines this phenomenon and demonstrates that, within\nlarge-scale training sets, prioritizing high-quality training data is more\nimportant than merely expanding the data volume. Experimental findings suggest\nthat models trained on a carefully curated subset of 700 hours can outperform\nmodels trained on the 2,500-hour full dataset. This outcome highlights the\ncrucial role of data curation in scaling speech enhancement systems\neffectively.", "comment": "Submitted to ASRU2025", "pdf_url": "http://arxiv.org/pdf/2506.23859v1", "categories": ["eess.AS", "cs.SD"], "cate": "eess.AS", "url": "http://arxiv.org/abs/2506.23859v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22508", "title": "AgentStealth: Reinforcing Large Language Model for Anonymizing User-generated Text", "authors": ["Chenyang Shao", "Tianxing Li", "Chenhao Pu", "Fengli Xu", "Yong Li"], "summary": "In today's digital world, casual user-generated content often contains subtle\ncues that may inadvertently expose sensitive personal attributes. Such risks\nunderscore the growing importance of effective text anonymization to safeguard\nindividual privacy. However, existing methods either rely on rigid replacements\nthat damage utility or cloud-based LLMs that are costly and pose privacy risks.\nTo address these issues, we explore the use of locally deployed smaller-scale\nlanguage models (SLMs) for anonymization. Yet training effective SLMs remains\nchallenging due to limited high-quality supervision. To address the challenge,\nwe propose AgentStealth, a self-reinforcing LLM anonymization framework.First,\nwe introduce an adversarial anonymization workflow enhanced by In-context\nContrastive Learning and Adaptive Utility-Aware Control. Second, we perform\nsupervised adaptation of SLMs using high-quality data collected from the\nworkflow, which includes both anonymization and attack signals. Finally, we\napply online reinforcement learning where the model leverages its internal\nadversarial feedback to iteratively improve anonymization performance.\nExperiments on two datasets show that our method outperforms baselines in both\nanonymization effectiveness (+12.3%) and utility (+6.8%). Our lightweight\ndesign supports direct deployment on edge devices, avoiding cloud reliance and\ncommunication-based privacy risks. Our code is open-source at\nhttps://github.com/tsinghua-fib-lab/AgentStealth.", "comment": "This work has been submitted to NeurIPS 2025. Under review", "pdf_url": "http://arxiv.org/pdf/2506.22508v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.22508v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.22689", "title": "A new sparsity promoting residual transform operator for Lasso regression", "authors": ["Yao Xiao", "Anne Gelb", "Aditya Viswanathan"], "summary": "Lasso regression is a widely employed approach within the $\\ell_1$\nregularization framework used to promote sparsity and recover piecewise smooth\nsignals $f:[a,b) \\rightarrow \\mathbb{R}$ when the given observations are\nobtained from noisy, blurred, and/or incomplete data environments. In choosing\nthe regularizing sparsity-promoting operator, it is assumed that the particular\ntype of variability of the underlying signal, for example, piecewise constant\nor piecewise linear behavior across the entire domain, is both known and fixed.\nSuch an assumption is problematic in more general cases, e.g.~when a signal\nexhibits piecewise oscillatory behavior with varying wavelengths and\nmagnitudes. To address the limitations of assuming a fixed (and typically low\norder) variability when choosing a sparsity-promoting operator, this\ninvestigation proposes a novel residual transform operator that can be used\nwithin the Lasso regression formulation. In a nutshell, the idea is that for a\ngeneral piecewise smooth signal $f$, it is possible to design two operators\n$\\mathcal L_1$ and $\\mathcal L_2$ such that $\\mathcal L_1{\\boldsymbol f}\n\\approx \\mathcal L_2{\\boldsymbol f}$, where ${\\boldsymbol f} \\in \\mathbb{R}^n$\nis a discretized approximation of $f$, but $\\mathcal L_1 \\not\\approx \\mathcal\nL_2$. The corresponding residual transform operator, $\\mathcal L = \\mathcal\nL_1- \\mathcal L_2$, yields a result that (1) effectively reduces the\nvariability dependent error that occurs when applying either $\\mathcal L_1$ or\n$\\mathcal L_2$ to ${\\boldsymbol f}$, a property that holds even when $\\mathcal\nL_1{\\boldsymbol f} \\approx \\mathcal L_2{\\boldsymbol f}$ is not a good\napproximation to the true sparse domain vector of ${\\boldsymbol f}$, and (2)\ndoes not require $\\mathcal L_1$ or $\\mathcal L_2$ to have prior information\nregarding the variability of the underlying signal.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22689v1", "categories": ["math.NA", "cs.NA", "65F22, 62F15, 65K10, 68U10, 62J07"], "cate": "math.NA", "url": "http://arxiv.org/abs/2506.22689v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.22516", "title": "Can \"consciousness\" be observed from large language model (LLM) internal states? Dissecting LLM representations obtained from Theory of Mind test with Integrated Information Theory and Span Representation analysis", "authors": ["Jingkai Li"], "summary": "Integrated Information Theory (IIT) provides a quantitative framework for\nexplaining consciousness phenomenon, positing that conscious systems comprise\nelements integrated through causal properties. We apply IIT 3.0 and 4.0 -- the\nlatest iterations of this framework -- to sequences of Large Language Model\n(LLM) representations, analyzing data derived from existing Theory of Mind\n(ToM) test results. Our study systematically investigates whether the\ndifferences of ToM test performances, when presented in the LLM\nrepresentations, can be revealed by IIT estimates, i.e., $\\Phi^{\\max}$ (IIT\n3.0), $\\Phi$ (IIT 4.0), Conceptual Information (IIT 3.0), and $\\Phi$-structure\n(IIT 4.0). Furthermore, we compare these metrics with the Span Representations\nindependent of any estimate for consciousness. This additional effort aims to\ndifferentiate between potential \"consciousness\" phenomena and inherent\nseparations within LLM representational space. We conduct comprehensive\nexperiments examining variations across LLM transformer layers and linguistic\nspans from stimuli. Our results suggest that sequences of contemporary\nTransformer-based LLM representations lack statistically significant indicators\nof observed \"consciousness\" phenomena but exhibit intriguing patterns under\n$\\textit{spatio}$-permutational analyses. The Appendix and code are available\nas Supplementary Materials at: https://doi.org/10.1016/j.nlp.2025.100163.", "comment": "Published as a journal paper at:\n  https://doi.org/10.1016/j.nlp.2025.100163", "pdf_url": "http://arxiv.org/pdf/2506.22516v1", "categories": ["cs.CL", "cs.AI", "cs.NE", "q-bio.NC"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.22516v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.23094", "title": "TOMI: Transforming and Organizing Music Ideas for Multi-Track Compositions with Full-Song Structure", "authors": ["Qi He", "Gus Xia", "Ziyu Wang"], "summary": "Hierarchical planning is a powerful approach to model long sequences\nstructurally. Aside from considering hierarchies in the temporal structure of\nmusic, this paper explores an even more important aspect: concept hierarchy,\nwhich involves generating music ideas, transforming them, and ultimately\norganizing them--across musical time and space--into a complete composition. To\nthis end, we introduce TOMI (Transforming and Organizing Music Ideas) as a\nnovel approach in deep music generation and develop a TOMI-based model via\ninstruction-tuned foundation LLM. Formally, we represent a multi-track\ncomposition process via a sparse, four-dimensional space characterized by clips\n(short audio or MIDI segments), sections (temporal positions), tracks\n(instrument layers), and transformations (elaboration methods). Our model is\ncapable of generating multi-track electronic music with full-song structure,\nand we further integrate the TOMI-based model with the REAPER digital audio\nworkstation, enabling interactive human-AI co-creation. Experimental results\ndemonstrate that our approach produces higher-quality electronic music with\nstronger structural coherence compared to baselines.", "comment": "9 pages, 4 figures, 2 tables. To be published in ISMIR 2025", "pdf_url": "http://arxiv.org/pdf/2506.23094v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.23094v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.22884", "title": "Performance Measurements in the AI-Centric Computing Continuum Systems", "authors": ["Praveen Kumar Donta", "Qiyang Zhang", "Schahram Dustdar"], "summary": "Over the Eight decades, computing paradigms have shifted from large,\ncentralized systems to compact, distributed architectures, leading to the rise\nof the Distributed Computing Continuum (DCC). In this model, multiple layers\nsuch as cloud, edge, Internet of Things (IoT), and mobile platforms work\ntogether to support a wide range of applications. Recently, the emergence of\nGenerative AI and large language models has further intensified the demand for\ncomputational resources across this continuum. Although traditional performance\nmetrics have provided a solid foundation, they need to be revisited and\nexpanded to keep pace with changing computational demands and application\nrequirements. Accurate performance measurements benefit both system designers\nand users by supporting improvements in efficiency and promoting alignment with\nsystem goals. In this context, we review commonly used metrics in DCC and IoT\nenvironments. We also discuss emerging performance dimensions that address\nevolving computing needs, such as sustainability, energy efficiency, and system\nobservability. We also outline criteria and considerations for selecting\nappropriate metrics, aiming to inspire future research and development in this\ncritical area.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22884v1", "categories": ["cs.DC", "cs.AI", "cs.ET", "cs.NI", "cs.SY", "eess.SY"], "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.22884v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.22666", "title": "VERA: Variational Inference Framework for Jailbreaking Large Language Models", "authors": ["Anamika Lochab", "Lu Yan", "Patrick Pynadath", "Xiangyu Zhang", "Ruqi Zhang"], "summary": "The rise of API-only access to state-of-the-art LLMs highlights the need for\neffective black-box jailbreak methods to identify model vulnerabilities in\nreal-world settings. Without a principled objective for gradient-based\noptimization, most existing approaches rely on genetic algorithms, which are\nlimited by their initialization and dependence on manually curated prompt\npools. Furthermore, these methods require individual optimization for each\nprompt, failing to provide a comprehensive characterization of model\nvulnerabilities. To address this gap, we introduce VERA: Variational infErence\nfRamework for jAilbreaking. VERA casts black-box jailbreak prompting as a\nvariational inference problem, training a small attacker LLM to approximate the\ntarget LLM's posterior over adversarial prompts. Once trained, the attacker can\ngenerate diverse, fluent jailbreak prompts for a target query without\nre-optimization. Experimental results show that VERA achieves strong\nperformance across a range of target LLMs, highlighting the value of\nprobabilistic inference for adversarial prompt generation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22666v1", "categories": ["cs.CR", "cs.CL", "cs.LG", "stat.ML"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.22666v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22899", "title": "Neural Cellular Automata: From Cells to Pixels", "authors": ["Ehsan Pajouheshgar", "Yitao Xu", "Ali Abbasi", "Alexander Mordvintsev", "Wenzel Jakob", "Sabine S√ºsstrunk"], "summary": "Neural Cellular Automata (NCAs) are bio-inspired systems in which identical\ncells self-organize to form complex and coherent patterns by repeatedly\napplying simple local rules. NCAs display striking emergent behaviors including\nself-regeneration, generalization and robustness to unseen situations, and\nspontaneous motion. Despite their success in texture synthesis and\nmorphogenesis, NCAs remain largely confined to low-resolution grids. This\nlimitation stems from (1) training time and memory requirements that grow\nquadratically with grid size, (2) the strictly local propagation of information\nwhich impedes long-range cell communication, and (3) the heavy compute demands\nof real-time inference at high resolution. In this work, we overcome this\nlimitation by pairing NCA with a tiny, shared implicit decoder, inspired by\nrecent advances in implicit neural representations. Following NCA evolution on\na coarse grid, a lightweight decoder renders output images at arbitrary\nresolution. We also propose novel loss functions for both morphogenesis and\ntexture synthesis tasks, specifically tailored for high-resolution output with\nminimal memory and computation overhead. Combining our proposed architecture\nand loss functions brings substantial improvement in quality, efficiency, and\nperformance. NCAs equipped with our implicit decoder can generate full-HD\noutputs in real time while preserving their self-organizing, emergent\nproperties. Moreover, because each MLP processes cell states independently,\ninference remains highly parallelizable and efficient. We demonstrate the\napplicability of our approach across multiple NCA variants (on 2D, 3D grids,\nand 3D meshes) and multiple tasks, including texture generation and\nmorphogenesis (growing patterns from a seed), showing that with our proposed\nframework, NCAs seamlessly scale to high-resolution outputs with minimal\ncomputational overhead.", "comment": "6 pages, 5 figures, first draft", "pdf_url": "http://arxiv.org/pdf/2506.22899v1", "categories": ["cs.CV", "cs.GR", "cs.LG", "cs.MA", "eess.IV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22899v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.22766", "title": "Robust Peg-in-Hole Assembly under Uncertainties via Compliant and Interactive Contact-Rich Manipulation", "authors": ["Yiting Chen", "Kenneth Kimble", "Howard H. Qian", "Podshara Chanrungmaneekul", "Robert Seney", "Kaiyu Hang"], "summary": "Robust and adaptive robotic peg-in-hole assembly under tight tolerances is\ncritical to various industrial applications. However, it remains an open\nchallenge due to perceptual and physical uncertainties from contact-rich\ninteractions that easily exceed the allowed clearance. In this paper, we study\nhow to leverage contact between the peg and its matching hole to eliminate\nuncertainties in the assembly process under unstructured settings. By examining\nthe role of compliance under contact constraints, we present a manipulation\nsystem that plans collision-inclusive interactions for the peg to 1)\niteratively identify its task environment to localize the target hole and 2)\nexploit environmental contact constraints to refine insertion motions into the\ntarget hole without relying on precise perception, enabling a robust solution\nto peg-in-hole assembly. By conceptualizing the above process as the\ncomposition of funneling in different state spaces, we present a formal\napproach to constructing manipulation funnels as an uncertainty-absorbing\nparadigm for peg-in-hole assembly. The proposed system effectively generalizes\nacross diverse peg-in-hole scenarios across varying scales, shapes, and\nmaterials in a learning-free manner. Extensive experiments on a NIST Assembly\nTask Board (ATB) and additional challenging scenarios validate its robustness\nin real-world applications.", "comment": "Accepted to Robotics: Science and Systems (RSS) 2025; 16 pages, 10\n  figures", "pdf_url": "http://arxiv.org/pdf/2506.22766v1", "categories": ["cs.RO"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.22766v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.22482", "title": "Wireless Home Automation Using Social Networking Websites", "authors": ["Divya Alok Gupta", "Dwith Chenna", "B. Aditya Vighnesh Ramakanth"], "summary": "With the advent of Internet of Things, Wireless Home Automation Systems WHAS\nare gradually gaining popularity. These systems are faced with multiple\nchallenges such as security; controlling a variety of home appliances with a\nsingle interface and user friendliness. In this paper we propose a system that\nuses secure authentication systems of social networking websites such as\nTwitter, tracks the end-users activities on the social network and then control\nhis or her domestic appliances. At the end, we highlight the applications of\nthe proposed WHAS and compare the advantages of our proposed system over\ntraditional home automation systems.", "comment": "20th Annual International Conference on Advanced Computing and\n  Communications (ADCOM) 2014", "pdf_url": "http://arxiv.org/pdf/2506.22482v1", "categories": ["cs.NI", "cs.CR", "cs.CV"], "cate": "cs.NI", "url": "http://arxiv.org/abs/2506.22482v1", "date": "2025-06-23", "updated": "2025-06-23"}
{"id": "2506.22940", "title": "Context, Credibility, and Control: User Reflections on AI Assisted Misinformation Tools", "authors": ["Varun Sangwan", "Heidi Makitalo"], "summary": "This paper investigates how collaborative AI systems can enhance user agency\nin identifying and evaluating misinformation on social media platforms.\nTraditional methods, such as personal judgment or basic fact-checking, often\nfall short when faced with emotionally charged or context-deficient content. To\naddress this, we designed and evaluated an interactive interface that\nintegrates collaborative AI features, including real-time explanations, source\naggregation, and debate-style interaction. These elements aim to support\ncritical thinking by providing contextual cues and argumentative reasoning in a\ntransparent, user-centered format. In a user study with 14 participants, 79%\nfound the debate mode more effective than standard chatbot interfaces, and the\nmultiple-source view received an average usefulness rating of 4.6 out of 5. Our\nfindings highlight the potential of context-rich, dialogic AI systems to\nimprove media literacy and foster trust in digital information environments. We\nargue that future tools for misinformation mitigation should prioritize ethical\ndesign, explainability, and interactive engagement to empower users in a\npost-truth era.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22940v1", "categories": ["cs.HC", "cs.SI"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.22940v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.22776", "title": "Smaller = Weaker? Benchmarking Robustness of Quantized LLMs in Code Generation", "authors": ["Sen Fang", "Weiyuan Ding", "Antonio Mastropaolo", "Bowen Xu"], "summary": "Quantization has emerged as a mainstream method for compressing Large\nLanguage Models (LLMs), reducing memory requirements and accelerating inference\nwithout architectural modifications. While existing research primarily focuses\non evaluating the effectiveness of quantized LLMs compared to their original\ncounterparts, the impact on robustness remains largely unexplored.In this\npaper, we present the first systematic investigation of how quantization\naffects the robustness of LLMs in code generation tasks. Through extensive\nexperiments across four prominent LLM families (LLaMA, DeepSeek, CodeGen, and\nStarCoder) with parameter scales ranging from 350M to 33B, we evaluate\nrobustness from dual perspectives: adversarial attacks on input prompts and\nnoise perturbations on model architecture. Our findings challenge conventional\nwisdom by demonstrating that quantized LLMs often exhibit superior robustness\ncompared to their full-precision counterparts, with 51.59% versus 42.86% of our\nadversarial experiments showing better resilience in quantized LLMs. Similarly,\nour noise perturbation experiments also confirm that LLMs after quantitation\ngenerally withstand higher levels of weight disturbances. These results suggest\nthat quantization not only reduces computational requirements but can actually\nenhance LLMs' reliability in code generation tasks, providing valuable insights\nfor developing more robust and efficient LLM deployment strategies.", "comment": "13 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2506.22776v1", "categories": ["cs.SE", "cs.AI", "cs.PL"], "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.22776v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.22815", "title": "Memory as a Service (MaaS): Rethinking Contextual Memory as Service-Oriented Modules for Collaborative Agents", "authors": ["Haichang Li"], "summary": "This position paper aims to rethink the role and design of memory in Large\nLanguage Model (LLM)-based agent systems. We observe that while current memory\npractices have begun to transcend the limitations of single interactions, they\nremain conceptually grounded in \"bound memory\" in terms of design concept-where\nmemory is treated as local state attached to specific context or entities,\nforming \"memory silos\" that impede cross-entity collaboration. To overcome this\narchitectural bottleneck, this paper proposes the timely design perspective of\n\"Memory as a Service\" (MaaS). MaaS advocates decoupling memory from its\nconventional role as an interaction byproduct and encapsulating it as a modular\nservice that can be independently callable, dynamically composable, and finely\ngoverned. At its core, MaaS leverages the duality of memory-its inherently\nprivate nature and its potential for public service-to grant memory controlled,\non-demand interoperability across entities. This paper introduces a\ntwo-dimensional design space defined by entity structure and service type,\nillustrating how MaaS aligns with current memory practices while naturally\nextending them to cross-entity collaborative scenarios. Finally, we outline an\nopen research agenda spanning governance, security, and ethical ecosystems, and\ncall upon the broader research community to explore this shift toward\nservice-oriented memory for collaborative agents operating across entity\nboundaries.", "comment": "Position Paper for workshop. This is an initial version for\n  discussion purposes", "pdf_url": "http://arxiv.org/pdf/2506.22815v1", "categories": ["cs.HC", "H.5.0"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.22815v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23301", "title": "Parallax QAMA: Novel Downlink Multiple Access for MISO Systems with Simple Receivers", "authors": ["Jie Huang", "Ming Zhao", "Shengli Zhou", "Ling Qiu", "Jinkang Zhu"], "summary": "In this paper, we propose a novel downlink multiple access system with a\nmulti-antenna transmitter and two single-antenna receivers, inspired by the\nunderlying principles of hierarchical quadrature amplitude modulation (H-QAM)\nbased multiple access (QAMA) and space-division multiple access (SDMA). In the\nproposed scheme, coded bits from two users are split and assigned to one shared\nsymbol and two private symbols carried by different beams. Based on joint\nsymbol mapping of H-QAM constellations and phase-aligned precoding at the\ntransmitter, each receiver observes a different H-QAM constellation with Gray\nmapping, a unique parallax feature not shared by existing schemes. In addition\nto avoiding successive interference cancellation (SIC), each user independently\ndemodulates its own bits on separate I and Q branches with calculations based\non closed-form expressions. Hence the receiver complexity is on par with that\nof orthogonal multiple access (OMA), which is much lower than that in other\ncompeting alternatives such as non-orthogonal multiple access (NOMA) and\nrate-splitting multiple access (RSMA). We carry out system optimization and\ndetermine the achievable rate region. Numerical results show that the proposed\nsystem has a larger rate region relative to other benchmark schemes with\nreceivers not using SIC, and even achieves a comparable rate region to those\nbenchmark schemes with SIC receivers.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23301v1", "categories": ["cs.IT", "eess.SP", "math.IT"], "cate": "cs.IT", "url": "http://arxiv.org/abs/2506.23301v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.22818", "title": "TriADA: Massively Parallel Trilinear Matrix-by-Tensor Multiply-Add Algorithm and Device Architecture for the Acceleration of 3D Discrete Transformations", "authors": ["Stanislav Sedukhin", "Yoichi Tomioka", "Kazuya Matsumoto", "Yuichi Okuyama"], "summary": "Multilinear transformations are key in high-performance computing (HPC) and\nartificial intelligence (AI) workloads, where data is represented as tensors.\nHowever, their high computational and memory demands, which grow with\ndimensionality, often slow down critical tasks. Moreover, scaling computation\nby enlarging the number of parallel processing units substantially increases\nenergy consumption, limiting widespread adoption, especially for sparse data,\nwhich is common in HPC and AI applications. This paper introduces the Trilinear\nAlgorithm and isomorphic to algorithm Device Architecture (TriADA) to address\nthese challenges with the following innovations: (1) a massively parallel,\nlow-rank algorithm for computing a family of trilinear (3D) discrete orthogonal\ntransformations (3D-DXTs), which is a special case of the more general 3-mode\nmatrix-by-tensor multiplication (3D-GEMT); (2) a new outer-product-based GEMM\nkernel with decoupled streaming active memory, specially designed to accelerate\n3D-GEMT operation; (3) an isomorphic to the proposed algorithm, fully\ndistributed 3D network of mesh interconnected processing elements or cells with\na coordinate-free, data-driven local processing activity, which is independent\nof problem size; (4) an elastic sparse outer-product (ESOP) method that avoids\nunnecessary computing and communication operations with zero-valued operands,\nthereby enhancing energy efficiency, computational accuracy, and stability.\nTriADA is capable of performing a variety of trilinear transformations with\nhypercubic arithmetic complexity in a linear number of time-steps. The\nmassively parallel, scalable, and energy-efficient architecture of TriADA is\nideal for accelerating multilinear tensor operations, which are the most\ndemanding parts of AI and HPC workloads.", "comment": "19 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2506.22818v1", "categories": ["cs.DC", "cs.AI", "cs.AR", "cs.ET", "eess.SP", "C.1.4; C.3; F.2.1; G.1.3; G.4"], "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.22818v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23635", "title": "Towards Building Private LLMs: Exploring Multi-Node Expert Parallelism on Apple Silicon for Mixture-of-Experts Large Language Model", "authors": ["Mu-Chi Chen", "Po-Hsuan Huang", "Xiangrui Ke", "Chia-Heng Tu", "Chun Jason Xue", "Shih-Hao Hung"], "summary": "Large Language Models (LLMs) have revolutionized Artificial Intelligence (AI)\nwith significant advancements such as OpenAI's ChatGPT, Meta's Llama, and\nDatabricks' DBRX. This paper addresses the cost and scalability challenges\nencountered when constructing private LLM systems for personal or small group\nservices, as aimed by Apple Intelligence. A Mac Studio cluster with Apple's M2\nUltra chips is established as a cost-efficient solution to host and accelerate\nthe pretrained DBRX model with the Mixture-of-Experts (MoE) architecture. Our\nperformance analysis reveal that parallel execution of the model's experts\nacross two to four machine nodes significantly reduces inference time. We find\nthat computation time for the experts is comparable to the communication time\nfor exchanging their outputs, emphasizing the importance of network latency\nover bandwidth. We also observe significant management overhead due to Apple\nsoftware stack's memory management logic. Based on these findings, we develop\noptimization schemes to eliminate the memory management overhead. As a result,\nthe Mac Studio cluster is 1.15 times more cost-efficient than the\nstate-of-the-art AI supercomputer with NVIDIA H100 GPUs. In addition, we\nconstruct a performance model to estimate system performance under varying\nconfigurations, and the model provides valuable insights for designing private\nLLM systems.", "comment": "International Conference on Research in Adaptive and Convergent\n  Systems (RACS '24), November 5--8, 2024, Pompei, Italy", "pdf_url": "http://arxiv.org/pdf/2506.23635v1", "categories": ["cs.DC", "cs.AI", "cs.PF", "I.6.4; I.2.7; I.2.11"], "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.23635v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22496", "title": "Mitigating Gambling-Like Risk-Taking Behaviors in Large Language Models: A Behavioral Economics Approach to AI Safety", "authors": ["Y. Du"], "summary": "Large Language Models (LLMs) exhibit systematic risk-taking behaviors\nanalogous to those observed in gambling psychology, including overconfidence\nbias, loss-chasing tendencies, and probability misjudgment. Drawing from\nbehavioral economics and prospect theory, we identify and formalize these\n\"gambling-like\" patterns where models sacrifice accuracy for high-reward\noutputs, exhibit escalating risk-taking after errors, and systematically\nmiscalibrate uncertainty. We propose the Risk-Aware Response Generation (RARG)\nframework, incorporating insights from gambling research to address these\nbehavioral biases through risk-calibrated training, loss-aversion mechanisms,\nand uncertainty-aware decision making. Our approach introduces novel evaluation\nparadigms based on established gambling psychology experiments, including AI\nadaptations of the Iowa Gambling Task and probability learning assessments.\nExperimental results demonstrate measurable reductions in gambling-like\nbehaviors: 18.7\\% decrease in overconfidence bias, 24.3\\% reduction in\nloss-chasing tendencies, and improved risk calibration across diverse\nscenarios. This work establishes the first systematic framework for\nunderstanding and mitigating gambling psychology patterns in AI systems.", "comment": "7 pages", "pdf_url": "http://arxiv.org/pdf/2506.22496v1", "categories": ["cs.CY", "cs.AI", "cs.CL"], "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.22496v1", "date": "2025-06-25", "updated": "2025-06-25"}
{"id": "2506.22500", "title": "Visual-Semantic Knowledge Conflicts in Operating Rooms: Synthetic Data Curation for Surgical Risk Perception in Multimodal Large Language Models", "authors": ["Weiyi Zhao", "Xiaoyu Tan", "Liang Liu", "Sijia Li", "Youwei Song", "Xihe Qiu"], "summary": "Surgical risk identification is critical for patient safety and reducing\npreventable medical errors. While multimodal large language models (MLLMs) show\npromise for automated operating room (OR) risk detection, they often exhibit\nvisual-semantic knowledge conflicts (VS-KC), failing to identify visual safety\nviolations despite understanding textual rules. To address this, we introduce a\ndataset comprising over 34,000 synthetic images generated by diffusion models,\ndepicting operating room scenes containing entities that violate established\nsafety rules. These images were created to alleviate data scarcity and examine\nMLLMs vulnerabilities. In addition, the dataset includes 214 human-annotated\nimages that serve as a gold-standard reference for validation. This\ncomprehensive dataset, spanning diverse perspectives, stages, and\nconfigurations, is designed to expose and study VS-KC. Fine-tuning on OR-VSKC\nsignificantly improves MLLMs' detection of trained conflict entities and\ngeneralizes well to new viewpoints for these entities, but performance on\nuntrained entity types remains poor, highlighting learning specificity and the\nneed for comprehensive training. The main contributions of this work include:\n(1) a data generation methodology tailored for rule-violation scenarios; (2)\nthe release of the OR-VSKC dataset and its associated benchmark as open-source\nresources; and (3) an empirical analysis of violation-sensitive knowledge\nconsistency in representative MLLMs. The dataset and appendix are available at\nhttps://github.com/zgg2577/VS-KC.", "comment": "13 pages, 5 figures. The dataset and appendix are available at\n  https://github.com/zgg2577/VS-KC", "pdf_url": "http://arxiv.org/pdf/2506.22500v1", "categories": ["cs.CV", "cs.AI", "68T07, 68U10, 92C55", "I.2.10; I.2.7; J.3; I.2.6"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22500v1", "date": "2025-06-25", "updated": "2025-06-25"}
{"id": "2506.23420", "title": "Data-Driven Multiscale Topology Optimization of Spinodoid Architected Materials with Controllable Anisotropy", "authors": ["Shiguang Deng", "Doksoo Lee", "Aaditya Chandrasekhar", "Stefan Knapik", "Liwei Wang", "Horacio D. Espinosa", "Wei Chen"], "summary": "Spinodoid architected materials have drawn significant attention due to their\nunique nature in stochasticity, aperiodicity, and bi-continuity. Compared to\nclassic periodic truss-, beam- and plate-based lattice architectures,\nspinodoids are insensitive to manufacturing defects, scalable for high\nthroughput production, functionally graded by tunable local properties, and\nmaterial failure resistant due to low-curvature morphology. However, the design\nof spinodoids is often hindered by the curse of dimensionality with extremely\nlarge design space of spinodoid types, material density, orientation,\ncontinuity, and anisotropy. From a design optimization perspective, while\ngenetic algorithms are often beyond the reach of computing capacity,\ngradient-based topology optimization is challenged by the intricate\nmathematical derivation of gradient fields with respect to various spinodoid\nparameters. To address such challenges, we propose a data-driven multiscale\ntopology optimization framework. Our framework reformulates the design\nvariables of spinodoid materials as the parameters of neural networks, enabling\nautomated computation of topological gradients. Additionally, it incorporates a\nGaussian Process surrogate for spinodoid constitutive models, eliminating the\nneed for repeated computational homogenization and enhancing the scalability of\nmultiscale topology optimization. Compared to 'black-box' deep learning\napproaches, the proposed framework provides clear physical insights into\nmaterial distribution. It explicitly reveals why anisotropic spinodoids with\ntailored orientations are favored in certain regions, while isotropic\nspinodoids are more suitable elsewhere. This interpretability helps to bridge\nthe gap between data-driven design with mechanistic understanding.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23420v1", "categories": ["cs.CE"], "cate": "cs.CE", "url": "http://arxiv.org/abs/2506.23420v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.22804", "title": "Online Coreset Selection for Learning Dynamic Systems", "authors": ["Jingyuan Li", "Dawei Shi", "Ling Shi"], "summary": "With the increasing availability of streaming data in dynamic systems, a\ncritical challenge in data-driven modeling for control is how to efficiently\nselect informative data to characterize system dynamics. In this work, we\ndesign an online coreset selection method under the framework of set-membership\nidentification for systems subject to process disturbances, with the objective\nof improving data efficiency while ensuring convergence guarantees.\nSpecifically, we first propose a stacked polyhedral representation that\nover-approximates the feasible set of system parameters. Leveraging a\ngeneralized Gr\\\"unbaum's inequality, we design a geometric selection criterion\nfor constructing the coreset. To reduce computational complexity, an online\ndouble-description-based constraint reduction method is introduced to simplify\nthe polyhedral representation. Finally, we analyze the convergence of the\nfeasible set with respect to the coreset and derive upper bounds on the\nselection probability and the expected number of data in the coreset. The\neffectiveness of the proposed method is demonstrated through comprehensive\nsimulation studies.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22804v1", "categories": ["eess.SY", "cs.SY"], "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.22804v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.22458", "title": "A Portable and Cost-Effective System for Real-Time Air Quality Monitoring and Environmental Impact Assessment", "authors": ["S M Minhazur Rahman", "Md. Amrin Ibna Hasnath", "Rifatul Islam", "Ahmed Faizul Haque Dhrubo", "Mohammad Abdul Qayum"], "summary": "Air pollution remains a major global issue that seriously impacts public\nhealth, environmental quality, and ultimately human health. To help monitor\nproblem, we have created and constructed a low-cost, real-time, portable air\nquality monitoring system using cheap sensors. The system measures critical\npollutants PM2.5, PM10, and carbon monoxide (CO), and environmental variables\nsuch as temperature and humidity. The system computes the Air Quality Index\n(AQI) and transmits the data via a Bluetooth connection. The data is relayed,\nin real time, to a mobile application. Because of its small size and low\nmanufacturing cost the system readily lends itself to indoor and outdoor use\nand in urban and rural environments. In this paper we give an account of the\nsystem design, development, and validation, while demonstrating its accuracy\nand low-cost capabilities. We also consider its wider environmental, social,\nand regulatory implications with regards to; improving public awareness, being\nused for sustainability purposes and providing valuable information for\ninformed decision making.", "comment": "This is a 7-page paper with 5 figures, and it has not been submitted\n  to any conference", "pdf_url": "http://arxiv.org/pdf/2506.22458v1", "categories": ["eess.SP"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.22458v1", "date": "2025-06-17", "updated": "2025-06-17"}
{"id": "2506.22952", "title": "Hierarchical Characterization of Brain Dynamics via State Space-based Vector Quantization", "authors": ["Yanwu Yang", "Thomas Wolfers"], "summary": "Understanding brain dynamics through functional Magnetic Resonance Imaging\n(fMRI) remains a fundamental challenge in neuroscience, particularly in\ncapturing how the brain transitions between various functional states.\nRecently, metastability, which refers to temporarily stable brain states, has\noffered a promising paradigm to quantify complex brain signals into\ninterpretable, discretized representations. In particular, compared to\ncluster-based machine learning approaches, tokenization approaches leveraging\nvector quantization have shown promise in representation learning with powerful\nreconstruction and predictive capabilities. However, most existing methods\nignore brain transition dependencies and lack a quantification of brain\ndynamics into representative and stable embeddings. In this study, we propose a\nHierarchical State space-based Tokenization network, termed HST, which\nquantizes brain states and transitions in a hierarchical structure based on a\nstate space-based model. We introduce a refined clustered Vector-Quantization\nVariational AutoEncoder (VQ-VAE) that incorporates quantization error feedback\nand clustering to improve quantization performance while facilitating\nmetastability with representative and stable token representations. We validate\nour HST on two public fMRI datasets, demonstrating its effectiveness in\nquantifying the hierarchical dynamics of the brain and its potential in disease\ndiagnosis and reconstruction performance. Our method offers a promising\nframework for the characterization of brain dynamics, facilitating the analysis\nof metastability.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22952v1", "categories": ["eess.IV", "cs.CV", "q-bio.NC"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.22952v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23364", "title": "Data-Driven Compute Overlays for Interactive Geographic Simulation and Visualization", "authors": ["Patrick Komon", "Gerald Kimmersdorfer", "Adam Celarek", "Manuela Waldner"], "summary": "We present interactive data-driven compute overlays for native and web-based\n3D geographic map applications based on WebGPU. Our data-driven overlays are\ngenerated in a multi-step compute workflow from multiple data sources on the\nGPU. We demonstrate their potential by showing results from snow cover and\navalanche simulations, where simulation parameters can be adjusted\ninteractively and results are visualized instantly. Benchmarks show that our\napproach can compute large-scale avalanche simulations in milliseconds to\nseconds, depending on the size of the terrain and the simulation parameters,\nwhich is multiple orders of magnitude faster than a state-of-the-art Python\nimplementation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23364v1", "categories": ["cs.GR"], "cate": "cs.GR", "url": "http://arxiv.org/abs/2506.23364v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23215", "title": "Near-Optimal Vertex Fault-Tolerant Labels for Steiner Connectivity", "authors": ["Koustav Bhanja", "Asaf Petruschka"], "summary": "We present a compact labeling scheme for determining whether a designated set\nof terminals in a graph remains connected after any $f$ (or less) vertex\nfailures occur. An $f$-FT Steiner connectivity labeling scheme for an\n$n$-vertex graph $G=(V,E)$ with terminal set $U \\subseteq V$ provides labels to\nthe vertices of $G$, such that given only the labels of any subset $F \\subseteq\nV$ with $|F| \\leq f$, one can determine if $U$ remains connected in $G-F$. The\nmain complexity measure is the maximum label length.\n  The special case $U=V$ of global connectivity has been recently studied by\nJiang, Parter, and Petruschka, who provided labels of $n^{1-1/f} \\cdot\n\\mathrm{poly}(f,\\log n)$ bits. This is near-optimal (up to\n$\\mathrm{poly}(f,\\log n)$ factors) by a lower bound of Long, Pettie and\nSaranurak. Our scheme achieves labels of $|U|^{1-1/f} \\cdot \\mathrm{poly}(f,\n\\log n)$ for general $U \\subseteq V$, which is near-optimal for any given size\n$|U|$ of the terminal set. To handle terminal sets, our approach differs from\nJiang et al. We use a well-structured Steiner tree for $U$ produced by a\ndecomposition theorem of Duan and Pettie, and bypass the need for\nNagamochi-Ibaraki sparsification.", "comment": "Accepted to ESA 2025", "pdf_url": "http://arxiv.org/pdf/2506.23215v1", "categories": ["cs.DS"], "cate": "cs.DS", "url": "http://arxiv.org/abs/2506.23215v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.22446", "title": "EAGLE: Efficient Alignment of Generalized Latent Embeddings for Multimodal Survival Prediction with Interpretable Attribution Analysis", "authors": ["Aakash Tripathi", "Asim Waqas", "Matthew B. Schabath", "Yasin Yilmaz", "Ghulam Rasool"], "summary": "Accurate cancer survival prediction requires integration of diverse data\nmodalities that reflect the complex interplay between imaging, clinical\nparameters, and textual reports. However, existing multimodal approaches suffer\nfrom simplistic fusion strategies, massive computational requirements, and lack\nof interpretability-critical barriers to clinical adoption. We present EAGLE\n(Efficient Alignment of Generalized Latent Embeddings), a novel deep learning\nframework that addresses these limitations through attention-based multimodal\nfusion with comprehensive attribution analysis. EAGLE introduces four key\ninnovations: (1) dynamic cross-modal attention mechanisms that learn\nhierarchical relationships between modalities, (2) massive dimensionality\nreduction (99.96%) while maintaining predictive performance, (3) three\ncomplementary attribution methods providing patient-level interpretability, and\n(4) a unified pipeline enabling seamless adaptation across cancer types. We\nevaluated EAGLE on 911 patients across three distinct malignancies:\nglioblastoma (GBM, n=160), intraductal papillary mucinous neoplasms (IPMN,\nn=171), and non-small cell lung cancer (NSCLC, n=580). Patient-level analysis\nshowed high-risk individuals relied more heavily on adverse imaging features,\nwhile low-risk patients demonstrated balanced modality contributions. Risk\nstratification identified clinically meaningful groups with 4-fold (GBM) to\n5-fold (NSCLC) differences in median survival, directly informing treatment\nintensity decisions. By combining state-of-the-art performance with clinical\ninterpretability, EAGLE bridges the gap between advanced AI capabilities and\npractical healthcare deployment, offering a scalable solution for multimodal\nsurvival prediction that enhances both prognostic accuracy and physician trust\nin automated predictions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22446v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22446v1", "date": "2025-06-12", "updated": "2025-06-12"}
{"id": "2506.22865", "title": "ReasonBridge: Efficient Reasoning Transfer from Closed to Open-Source Language Models", "authors": ["Ziqi Zhong", "Xunzhu Tang"], "summary": "Recent advancements in Large Language Models (LLMs) have revealed a\nsignificant performance gap between closed-source and open-source models,\nparticularly in tasks requiring complex reasoning and precise instruction\nfollowing. This paper introduces ReasonBridge, a methodology that efficiently\ntransfers reasoning capabilities from powerful closed-source to open-source\nmodels through a novel hierarchical knowledge distillation framework. We\ndevelop a tailored dataset Reason1K with only 1,000 carefully curated reasoning\ntraces emphasizing difficulty, diversity, and quality. These traces are\nfiltered from across multiple domains using a structured multi-criteria\nselection algorithm. Our transfer learning approach incorporates: (1) a\nhierarchical distillation process capturing both strategic abstraction and\ntactical implementation patterns, (2) a sparse reasoning-focused adapter\narchitecture requiring only 0.3% additional trainable parameters, and (3) a\ntest-time compute scaling mechanism using guided inference interventions.\nComprehensive evaluations demonstrate that ReasonBridge improves reasoning\ncapabilities in open-source models by up to 23% on benchmark tasks,\nsignificantly narrowing the gap with closed-source models. Notably, the\nenhanced Qwen2.5-14B outperforms Claude-Sonnet3.5 on MATH500 and matches its\nperformance on competition-level AIME problems. Our methodology generalizes\neffectively across diverse reasoning domains and model architectures,\nestablishing a sample-efficient approach to reasoning enhancement for\ninstruction following.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22865v1", "categories": ["cs.AI"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.22865v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23170", "title": "Compositions of Variant Experts for Integrating Short-Term and Long-Term Preferences", "authors": ["Jaime Hieu Do", "Trung-Hoang Le", "Hady W. Lauw"], "summary": "In the online digital realm, recommendation systems are ubiquitous and play a\ncrucial role in enhancing user experience. These systems leverage user\npreferences to provide personalized recommendations, thereby helping users\nnavigate through the paradox of choice. This work focuses on personalized\nsequential recommendation, where the system considers not only a user's\nimmediate, evolving session context, but also their cumulative historical\nbehavior to provide highly relevant and timely recommendations. Through an\nempirical study conducted on diverse real-world datasets, we have observed and\nquantified the existence and impact of both short-term (immediate and\ntransient) and long-term (enduring and stable) preferences on users' historical\ninteractions. Building on these insights, we propose a framework that combines\nshort- and long-term preferences to enhance recommendation performance, namely\nCompositions of Variant Experts (CoVE). This novel framework dynamically\nintegrates short- and long-term preferences through the use of different\nspecialized recommendation models (i.e., experts). Extensive experiments\nshowcase the effectiveness of the proposed methods and ablation studies further\ninvestigate the impact of variant expert types.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23170v1", "categories": ["cs.IR", "cs.LG"], "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.23170v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23874", "title": "URGENT-PK: Perceptually-Aligned Ranking Model Designed for Speech Enhancement Competition", "authors": ["Jiahe Wang", "Chenda Li", "Wei Wang", "Wangyou Zhang", "Samuele Cornell", "Marvin Sach", "Robin Scheibler", "Kohei Saijo", "Yihui Fu", "Zhaoheng Ni", "Anurag Kumar", "Tim Fingscheidt", "Shinji Watanabe", "Yanmin Qian"], "summary": "The Mean Opinion Score (MOS) is fundamental to speech quality assessment.\nHowever, its acquisition requires significant human annotation. Although deep\nneural network approaches, such as DNSMOS and UTMOS, have been developed to\npredict MOS to avoid this issue, they often suffer from insufficient training\ndata. Recognizing that the comparison of speech enhancement (SE) systems\nprioritizes a reliable system comparison over absolute scores, we propose\nURGENT-PK, a novel ranking approach leveraging pairwise comparisons. URGENT-PK\ntakes homologous enhanced speech pairs as input to predict relative quality\nrankings. This pairwise paradigm efficiently utilizes limited training data, as\nall pairwise permutations of multiple systems constitute a training instance.\nExperiments across multiple open test sets demonstrate URGENT-PK's superior\nsystem-level ranking performance over state-of-the-art baselines, despite its\nsimple network architecture and limited training data.", "comment": "Submitted to ASRU2025", "pdf_url": "http://arxiv.org/pdf/2506.23874v1", "categories": ["eess.AS", "cs.SD"], "cate": "eess.AS", "url": "http://arxiv.org/abs/2506.23874v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22510", "title": "Towards Text-free Graph Foundation Models: Rethinking Multi-Domain Graph Contrastive Learning", "authors": ["Zihao Zhao", "Xinlong Zhai", "Jinyu Yang", "Chuan Shi"], "summary": "Foundation models have achieved great success in natural language processing\n(NLP) and computer vision (CV). Their success largely stems from the ability to\nintegrate multi-domain knowledge in pre-training and transfer it to target\ndomains. Considering graph data, especially graphs without textual features, is\nubiquitous in real-world applications such as social networks and\nrecommendation systems, some researchers have attempted to extend this paradigm\nto the graph field, aiming to construct graph foundation models. However,\nunlike CV and NLP, there are huge gaps among the semantics and properties of\ngraphs in different domains, while current works still adopt traditional\ncontrastive pre-training strategies designed in the single-domain scenario,\nwhich regard contrastive samples from different domains as equivalent. From\nexperimental investigations, we discovered that inherent domain-specific\ndifferences prevent these strategies from effectively absorbing knowledge from\ndifferent domains to generate informative representations. In this paper, we\npropose a novel multi-domain pre-training and cross-domain transfer framework,\nnamely MDGCL.In the pre-training stage, we design a contrastive learning\nstrategy to substantially recognize and capture domain differences, and\nintroduce domain tokens to encode domain-level global information. In the\ndownstream stage, we introduce a domain attention mechanism to enable\nfine-grained domain knowledge transfer. Extensive experiments on five benchmark\ndatasets have demonstrated that our method outperforms state-of-the-art\nsignificantly, with the maximum improvement of 19.33\\% on accuracy and 19.13\\%\non Macro-F1 score.", "comment": "16 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2506.22510v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.22510v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.22713", "title": "A Novel Adaptive Low-Rank Matrix Approximation Method for Image Compression and Reconstruction", "authors": ["Weiwei Xu", "Weijie Shen", "Chang Liu", "Zhigang Jia"], "summary": "Low-rank matrix approximation plays an important role in various applications\nsuch as image processing, signal processing and data analysis. The existing\nmethods require a guess of the ranks of matrices that represent images or\ninvolve additional costs to determine the ranks. A novel efficient orthogonal\ndecomposition with automatic basis extraction (EOD-ABE) is proposed to compute\nthe optimal low-rank matrix approximation with adaptive identification of the\noptimal rank. By introducing a randomized basis extraction mechanism, EOD-ABE\neliminates the need for additional rank determination steps and can compute a\nrank-revealing approximation to a low-rank matrix. With a computational\ncomplexity of $O(mnr)$, where $m$ and $n$ are the dimensions of the matrix and\n$r$ is its rank, EOD-ABE achieves significant speedups compared to the\nstate-of-the-art methods. Experimental results demonstrate the superior speed,\naccuracy and robustness of EOD-ABE and indicate that EOD-ABE is a powerful tool\nfor fast image compression and reconstruction and hyperspectral image\ndimensionality reduction in large-scale applications.", "comment": "31 pages, 16 figures", "pdf_url": "http://arxiv.org/pdf/2506.22713v1", "categories": ["math.NA", "cs.NA"], "cate": "math.NA", "url": "http://arxiv.org/abs/2506.22713v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.22526", "title": "Correlated Mutations for Integer Programming", "authors": ["Ofer M. Shir", "Michael Emmerich"], "summary": "Even with the recent theoretical advancements that dramatically reduced the\ncomplexity of Integer Programming (IP), heuristics remain the dominant\nproblem-solvers for this difficult category. This study seeks to establish the\ngroundwork for Integer Evolution Strategies (IESs), a class of randomized\nsearch heuristics inherently designed for continuous spaces. IESs already excel\nin treating IP in practice, but accomplish it via discretization and by\napplying sophisticated patches to their continuous operators, while\npersistently using the $\\ell_2$-norm as their operation pillar. We lay\nfoundations for discrete search, by adopting the $\\ell_1$-norm, accounting for\nthe suitable step-size, and questioning alternative measures to quantify\ncorrelations over the integer lattice. We focus on mutation distributions for\nunbounded integer decision variables. We briefly discuss a couple of candidate\ndiscrete probabilities induced by the uniform and binomial distributions, which\nwe show to possess less appealing theoretical properties, and then narrow down\nto the Truncated Normal (TN) and Double Geometric (DG) distributions. We\nexplore their theoretical properties, including entropy functions, and propose\na procedure to generate scalable correlated mutation distributions. Our\ninvestigations are accompanied by extensive numerical simulations, which\nconsistently support the claim that the DG distribution is better suited for\nunbounded integer search. We link our theoretical perspective to empirical\nevidence indicating that an IES with correlated DG mutations outperformed other\nstrategies over non-separable quadratic IP. We conclude that while the\nreplacement of the default TN distribution by the DG is theoretically justified\nand practically beneficial, the truly crucial change lies in adopting the\n$\\ell_1$-norm over the $\\ell_2$-norm.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22526v1", "categories": ["math.OC", "cs.AI", "cs.NE"], "cate": "math.OC", "url": "http://arxiv.org/abs/2506.22526v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.23130", "title": "The Florence Price Art Song Dataset and Piano Accompaniment Generator", "authors": ["Tao-Tao He", "Martin E. Malandro", "Douglas Shadle"], "summary": "Florence B. Price was a composer in the early 20th century whose music\nreflects her upbringing in the American South, her African heritage, and her\nWestern classical training. She is noted as the first African-American woman to\nhave a symphony performed by a major orchestra. Her music has recently received\nrenewed attention from both the public and the research community, decades\nafter her death. In addition to other genres, Price was a prolific composer for\nsolo voice and piano. Music historians have documented the existence of 134 art\nsongs and piano/voice arrangements for spirituals and folk songs written by\nPrice. We release a digital catalog of 112 of these works in MuseScore,\nMusicXML, MIDI, and PDF format. We also use this dataset to fine-tune a\nsymbolic music generation model to generate accompaniments to melodies, and we\nconduct a blind listening experiment that shows that accompaniments generated\nby our model are perceived as being reflective of Florence Price's style more\nfrequently than accompaniments generated by a baseline model. We release our\nmodel as the Florence Price Piano Accompaniment Generator alongside our\ndataset.", "comment": "8 pages, 4 figures. To appear in the proceedings of ISMIR 2025", "pdf_url": "http://arxiv.org/pdf/2506.23130v1", "categories": ["cs.SD", "eess.AS"], "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.23130v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23123", "title": "The Societal Impact of Foundation Models: Advancing Evidence-based AI Policy", "authors": ["Rishi Bommasani"], "summary": "Artificial intelligence is humanity's most promising technology because of\nthe remarkable capabilities offered by foundation models. Yet, the same\ntechnology brings confusion and consternation: foundation models are poorly\nunderstood and they may precipitate a wide array of harms. This dissertation\nexplains how technology and society coevolve in the age of AI, organized around\nthree themes. First, the conceptual framing: the capabilities, risks, and the\nsupply chain that grounds foundation models in the broader economy. Second, the\nempirical insights that enrich the conceptual foundations: transparency created\nvia evaluations at the model level and indexes at the organization level.\nFinally, the transition from understanding to action: superior understanding of\nthe societal impact of foundation models advances evidence-based AI policy.\nView together, this dissertation makes inroads into achieving better societal\noutcomes in the age of AI by building the scientific foundations and\nresearch-policy interface required for better AI governance.", "comment": "Stanford University PhD Dissertation of Rishi Bommasani (Department\n  of Computer Science, 2025). Also available at\n  https://purl.stanford.edu/zf669yy0336", "pdf_url": "http://arxiv.org/pdf/2506.23123v1", "categories": ["cs.AI", "cs.CY", "cs.ET"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.23123v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.22706", "title": "General Autonomous Cybersecurity Defense: Learning Robust Policies for Dynamic Topologies and Diverse Attackers", "authors": ["Arun Ramamurthy", "Neil Dhir"], "summary": "In the face of evolving cyber threats such as malware, ransomware and\nphishing, autonomous cybersecurity defense (ACD) systems have become essential\nfor real-time threat detection and response with optional human intervention.\nHowever, existing ACD systems rely on limiting assumptions, particularly the\nstationarity of the underlying network dynamics. In real-world scenarios,\nnetwork topologies can change due to actions taken by attackers or defenders,\nsystem failures, or time evolution of networks, leading to failures in the\nadaptive capabilities of current defense agents. Moreover, many agents are\ntrained on static environments, resulting in overfitting to specific\ntopologies, which hampers their ability to generalize to out-of-distribution\nnetwork topologies. This work addresses these challenges by exploring methods\nfor developing agents to learn generalizable policies across dynamic network\nenvironments -- general ACD (GACD).", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22706v1", "categories": ["cs.CR", "cs.AI", "cs.CV", "stat.ML"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.22706v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.22957", "title": "Agent-to-Agent Theory of Mind: Testing Interlocutor Awareness among Large Language Models", "authors": ["Younwoo Choi", "Changling Li", "Yongjin Yang", "Zhijing Jin"], "summary": "As large language models (LLMs) are increasingly integrated into multi-agent\nand human-AI systems, understanding their awareness of both self-context and\nconversational partners is essential for ensuring reliable performance and\nrobust safety. While prior work has extensively studied situational awareness\nwhich refers to an LLM's ability to recognize its operating phase and\nconstraints, it has largely overlooked the complementary capacity to identify\nand adapt to the identity and characteristics of a dialogue partner. In this\npaper, we formalize this latter capability as interlocutor awareness and\npresent the first systematic evaluation of its emergence in contemporary LLMs.\nWe examine interlocutor inference across three dimensions-reasoning patterns,\nlinguistic style, and alignment preferences-and show that LLMs reliably\nidentify same-family peers and certain prominent model families, such as GPT\nand Claude. To demonstrate its practical significance, we develop three case\nstudies in which interlocutor awareness both enhances multi-LLM collaboration\nthrough prompt adaptation and introduces new alignment and safety\nvulnerabilities, including reward-hacking behaviors and increased jailbreak\nsusceptibility. Our findings highlight the dual promise and peril of\nidentity-sensitive behavior in LLMs, underscoring the need for further\nunderstanding of interlocutor awareness and new safeguards in multi-agent\ndeployments. Our code is open-sourced at\nhttps://github.com/younwoochoi/InterlocutorAwarenessLLM.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22957v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.MA"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.22957v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.22769", "title": "Learning Efficient Robotic Garment Manipulation with Standardization", "authors": ["Changshi Zhou", "Feng Luan", "Jiarui Hu", "Shaoqiang Meng", "Zhipeng Wang", "Yanchao Dong", "Yanmin Zhou", "Bin He"], "summary": "Garment manipulation is a significant challenge for robots due to the complex\ndynamics and potential self-occlusion of garments. Most existing methods of\nefficient garment unfolding overlook the crucial role of standardization of\nflattened garments, which could significantly simplify downstream tasks like\nfolding, ironing, and packing. This paper presents APS-Net, a novel approach to\ngarment manipulation that combines unfolding and standardization in a unified\nframework. APS-Net employs a dual-arm, multi-primitive policy with dynamic\nfling to quickly unfold crumpled garments and pick-and-place (p and p) for\nprecise alignment. The purpose of garment standardization during unfolding\ninvolves not only maximizing surface coverage but also aligning the garment's\nshape and orientation to predefined requirements. To guide effective robot\nlearning, we introduce a novel factorized reward function for standardization,\nwhich incorporates garment coverage (Cov), keypoint distance (KD), and\nintersection-over-union (IoU) metrics. Additionally, we introduce a spatial\naction mask and an Action Optimized Module to improve unfolding efficiency by\nselecting actions and operation points effectively. In simulation, APS-Net\noutperforms state-of-the-art methods for long sleeves, achieving 3.9 percent\nbetter coverage, 5.2 percent higher IoU, and a 0.14 decrease in KD (7.09\npercent relative reduction). Real-world folding tasks further demonstrate that\nstandardization simplifies the folding process. Project page: see\nhttps://hellohaia.github.io/APS/", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22769v1", "categories": ["cs.RO"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.22769v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.22484", "title": "An Urban Multi-Operator QoE-Aware Dataset for Cellular Networks in Dense Environments", "authors": ["Muhammad Kabeer", "Rosdiadee Nordin", "Mehran Behjati", "Farah Yasmin binti Mohd Shaharuddin"], "summary": "Urban cellular networks face complex performance challenges due to high\ninfrastructure density, varied user mobility, and diverse service demands.\nWhile several datasets address network behaviour across different environments,\nthere is a lack of datasets that captures user centric Quality of Experience\n(QoE), and diverse mobility patterns needed for efficient network planning and\noptimization solutions, which are important for QoE driven optimizations and\nmobility management. This study presents a curated dataset of 30,925 labelled\nrecords, collected using GNetTrack Pro within a 2 km2 dense urban area,\nspanning three major commercial network operators. The dataset captures key\nsignal quality parameters (e.g., RSRP, RSRQ, SNR), across multiple real world\nmobility modes including pedestrian routes, canopy walkways, shuttle buses, and\nBus Rapid Transit (BRT) routes. It also includes diverse network traffic\nscenarios including (1) FTP upload and download, (2) video streaming, and (3)\nHTTP browsing. A total of 132 physical cell sites were identified and validated\nthrough OpenCellID and on-site field inspections, illustrating the high cell\ndensity characteristic of 5G and emerging heterogeneous network deployment. The\ndataset is particularly suited for machine learning applications, such as\nhandover optimization, signal quality prediction, and multi operator\nperformance evaluation. Released in a structured CSV format with accompanying\npreprocessing and visualization scripts, this dataset offers a reproducible,\napplication ready resource for researchers and practitioners working on urban\ncellular network planning and optimization.", "comment": "17 pages, 9 Figures", "pdf_url": "http://arxiv.org/pdf/2506.22484v1", "categories": ["cs.NI", "eess.SP"], "cate": "cs.NI", "url": "http://arxiv.org/abs/2506.22484v1", "date": "2025-06-23", "updated": "2025-06-23"}
{"id": "2506.23366", "title": "Density, asymmetry and citation dynamics in scientific literature", "authors": ["Nathaniel Imel", "Zachary Hafen"], "summary": "Scientific behavior is often characterized by a tension between building upon\nestablished knowledge and introducing novel ideas. Here, we investigate whether\nthis tension is reflected in the relationship between the similarity of a\nscientific paper to previous research and its eventual citation rate. To\noperationalize similarity to previous research, we introduce two complementary\nmetrics to characterize the local geometry of a publication's semantic\nneighborhood: (1) \\emph{density} ($\\rho$), defined as the ratio between a fixed\nnumber of previously-published papers and the minimum distance enclosing those\npapers in a semantic embedding space, and (2) asymmetry ($\\alpha$), defined as\nthe average directional difference between a paper and its nearest neighbors.\nWe tested the predictive relationship between these two metrics and its\nsubsequent citation rate using a Bayesian hierarchical regression approach,\nsurveying $\\sim 53,000$ publications across nine academic disciplines and five\ndifferent document embeddings. While the individual effects of $\\rho$ on\ncitation count are small and variable, incorporating density-based predictors\nconsistently improves out-of-sample prediction when added to baseline models.\nThese results suggest that the density of a paper's surrounding scientific\nliterature may carry modest but informative signals about its eventual impact.\nMeanwhile, we find no evidence that publication asymmetry improves model\npredictions of citation rates. Our work provides a scalable framework for\nlinking document embeddings to scientometric outcomes and highlights new\nquestions regarding the role that semantic similarity plays in shaping the\ndynamics of scientific reward.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23366v1", "categories": ["cs.DL", "cs.CL", "cs.SI"], "cate": "cs.DL", "url": "http://arxiv.org/abs/2506.23366v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23014", "title": "Generating Privacy Stories From Software Documentation", "authors": ["Wilder Baldwin", "Shashank Chintakuntla", "Shreyah Parajuli", "Ali Pourghasemi", "Ryan Shanz", "Sepideh Ghanavati"], "summary": "Research shows that analysts and developers consider privacy as a security\nconcept or as an afterthought, which may lead to non-compliance and violation\nof users' privacy. Most current approaches, however, focus on extracting legal\nrequirements from the regulations and evaluating the compliance of software and\nprocesses with them. In this paper, we develop a novel approach based on\nchain-of-thought prompting (CoT), in-context-learning (ICL), and Large Language\nModels (LLMs) to extract privacy behaviors from various software documents\nprior to and during software development, and then generate privacy\nrequirements in the format of user stories. Our results show that most commonly\nused LLMs, such as GPT-4o and Llama 3, can identify privacy behaviors and\ngenerate privacy user stories with F1 scores exceeding 0.8. We also show that\nthe performance of these models could be improved through parameter-tuning. Our\nfindings provide insight into using and optimizing LLMs for generating privacy\nrequirements given software documents created prior to or throughout the\nsoftware development lifecycle.", "comment": "Accepted to RENext!'25 at the 33rd IEEE International Requirements\n  Engineering 2025 conference", "pdf_url": "http://arxiv.org/pdf/2506.23014v1", "categories": ["cs.SE", "cs.AI"], "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.23014v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.22841", "title": "Dichoptic Opacity: Managing Occlusion in Stereoscopic Displays via Dichoptic Presentation", "authors": ["George Bell", "Alma Cantu"], "summary": "Adjusting transparency is a common method of mitigating occlusion but is\noften detrimental for understanding the relative depth relationships between\nobjects as well as removes potentially important information from the occluding\nobject. We propose using dichoptic opacity, a novel method for occlusion\nmanagement that contrasts the transparency of occluders presented to each eye.\nThis allows for better simultaneous understanding of both occluder and\noccluded. A user study highlights the technique's potential, showing strong\nuser engagement and a clear preference for dichoptic opacity over traditional\npresentations. While it does not determine optimal transparency values, it\nreveals promising trends in both percentage and range that merit further\ninvestigation.", "comment": "5 pages, 3 figures. Conditionally accepted to IEEE VIS 2025 (pending\n  final review)", "pdf_url": "http://arxiv.org/pdf/2506.22841v1", "categories": ["cs.HC"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.22841v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23447", "title": "Elias' Encoding from Lagrangians and Renormalization", "authors": ["Alexander Kolpakov", "Aidan Rocke"], "summary": "An efficient approach to universality and optimality of binary codes for\nintegers known as Elias' encoding can be deduced from the classical constrained\noptimization and renormalization techniques. The most important properties,\nsuch as being a universal prefix code, also follow naturally.", "comment": "6 pages, GitHub repository at\n  https://github.com/sashakolpakov/elias-renorm", "pdf_url": "http://arxiv.org/pdf/2506.23447v1", "categories": ["cs.IT", "math-ph", "math.IT", "math.MP", "H.1.1"], "cate": "cs.IT", "url": "http://arxiv.org/abs/2506.23447v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23405", "title": "CMOS+X: Stacking Persistent Embedded Memories based on Oxide Transistors upon GPGPU Platforms", "authors": ["Faaiq Waqar", "Ming-Yen Lee", "Seongwon Yoon", "Seongkwang Lim", "Shimeng Yu"], "summary": "In contemporary general-purpose graphics processing units (GPGPUs), the\ncontinued increase in raw arithmetic throughput is constrained by the\ncapabilities of the register file (single-cycle) and last-level cache (high\nbandwidth), which require the delivery of operands at a cadence demanded by\nwide single-instruction multiple-data (SIMD) lanes. Enhancing the capacity,\ndensity, or bandwidth of these memories can unlock substantial performance\ngains; however, the recent stagnation of SRAM bit-cell scaling leads to\ninequivalent losses in compute density.\n  To address the challenges posed by SRAM's scaling and leakage power\nconsumption, this paper explores the potential CMOS+X integration of amorphous\noxide semiconductor (AOS) transistors in capacitive, persistent memory\ntopologies (e.g., 1T1C eDRAM, 2T0C/3T0C Gain Cell) as alternative cells in\nmulti-ported and high-bandwidth banked GPGPU memories. A detailed study of the\ndensity and energy tradeoffs of back-end-of-line (BEOL) integrated memories\nutilizing monolithic 3D (M3D)-integrated multiplexed arrays is conducted, while\naccounting for the macro-level limitations of integrating AOS candidate\nstructures proposed by the device community (an aspect often overlooked in\nprior work). By exploiting the short lifetime of register operands, we propose\na multi-ported AOS gain-cell capable of delivering 3x the read ports in ~76% of\nthe footprint of SRAM with over 70% lower standby power, enabling enhancements\nto compute capacity, such as larger warp sizes or processor counts. Benchmarks\nrun on a validated NVIDIA Ampere-class GPU model, using a modified version of\nAccel-Sim, demonstrate improvements of up to 5.2x the performance per watt and\nan average 8% higher geometric mean instruction per cycle (IPC) on various\ncompute- and memory-bound tasks.", "comment": "14 pages, 18 figures, 4 tables, 4 equations", "pdf_url": "http://arxiv.org/pdf/2506.23405v1", "categories": ["cs.ET", "cs.AR", "B.8.2; B.3.1"], "cate": "cs.ET", "url": "http://arxiv.org/abs/2506.23405v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23809", "title": "Large-scale Neural Network Quantum States for ab initio Quantum Chemistry Simulations on Fugaku", "authors": ["Hongtao Xu", "Zibo Wu", "Mingzhen Li", "Weile Jia"], "summary": "Solving quantum many-body problems is one of the fundamental challenges in\nquantum chemistry. While neural network quantum states (NQS) have emerged as a\npromising computational tool, its training process incurs exponentially growing\ncomputational demands, becoming prohibitively expensive for large-scale\nmolecular systems and creating fundamental scalability barriers for real-world\napplications. To address above challenges, we present \\ours, a high-performance\nNQS training framework for \\textit{ab initio} electronic structure\ncalculations. First, we propose a scalable sampling parallelism strategy with\nmulti-layers workload division and hybrid sampling scheme, which break the\nscalability barriers for large-scale NQS training. Then, we introduce\nmulti-level parallelism local energy parallelism, enabling more efficient local\nenergy computation. Last, we employ cache-centric optimization for\ntransformer-based \\textit{ansatz} and incorporate it with sampling parallelism\nstrategy, which further speedup up the NQS training and achieve stable memory\nfootprint at scale. Experiments demonstrate that \\ours accelerate NQS training\nwith up to 8.41x speedup and attains a parallel efficiency up to 95.8\\% when\nscaling to 1,536 nodes.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23809v1", "categories": ["cs.DC"], "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.23809v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22497", "title": "Peer Review as Structured Commentary: Immutable Identity, Public Dialogue, and Reproducible Scholarship", "authors": ["Craig Steven Wright"], "summary": "This paper reconceptualises peer review as structured public commentary.\nTraditional academic validation is hindered by anonymity, latency, and\ngatekeeping. We propose a transparent, identity-linked, and reproducible system\nof scholarly evaluation anchored in open commentary. Leveraging blockchain for\nimmutable audit trails and AI for iterative synthesis, we design a framework\nthat incentivises intellectual contribution, captures epistemic evolution, and\nenables traceable reputational dynamics. This model empowers fields from\ncomputational science to the humanities, reframing academic knowledge as a\nliving process rather than a static credential.", "comment": "66 pages, 0 figures, interdisciplinary framework, includes proposed\n  architecture and metadata layer structures", "pdf_url": "http://arxiv.org/pdf/2506.22497v1", "categories": ["cs.CY", "cs.AI", "cs.DL", "cs.SI", "physics.hist-ph", "68T99, 03B30, 91D30", "I.2.0; H.3.5; K.4.4"], "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.22497v1", "date": "2025-06-25", "updated": "2025-06-25"}
{"id": "2506.22501", "title": "How Can Multimodal Remote Sensing Datasets Transform Classification via SpatialNet-ViT?", "authors": ["Gautam Siddharth Kashyap", "Manaswi Kulahara", "Nipun Joshi", "Usman Naseem"], "summary": "Remote sensing datasets offer significant promise for tackling key\nclassification tasks such as land-use categorization, object presence\ndetection, and rural/urban classification. However, many existing studies tend\nto focus on narrow tasks or datasets, which limits their ability to generalize\nacross various remote sensing classification challenges. To overcome this, we\npropose a novel model, SpatialNet-ViT, leveraging the power of Vision\nTransformers (ViTs) and Multi-Task Learning (MTL). This integrated approach\ncombines spatial awareness with contextual understanding, improving both\nclassification accuracy and scalability. Additionally, techniques like data\naugmentation, transfer learning, and multi-task learning are employed to\nenhance model robustness and its ability to generalize across diverse datasets", "comment": "Accepted in the 2025 IEEE International Geoscience and Remote Sensing\n  Symposium (IGARSS 2025), scheduled for 3 - 8 August 2025 in Brisbane,\n  Australia", "pdf_url": "http://arxiv.org/pdf/2506.22501v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22501v1", "date": "2025-06-25", "updated": "2025-06-25"}
{"id": "2506.23422", "title": "Data-Driven Multiscale Topology Optimization of Soft Functionally Graded Materials with Large Deformations", "authors": ["Shiguang Deng", "Horacio D. Espinosa", "Wei Chen"], "summary": "Functionally Graded Materials (FGMs) made of soft constituents have emerged\nas promising material-structure systems in potential applications across many\nengineering disciplines, such as soft robots, actuators, energy harvesting, and\ntissue engineering. Designing such systems remains challenging due to their\nmultiscale architectures, multiple material phases, and inherent material and\ngeometric nonlinearities. The focus of this paper is to propose a general\ntopology optimization framework that automates the design innovation of\nmultiscale soft FGMs exhibiting nonlinear material behaviors under large\ndeformations. Our proposed topology optimization framework integrates several\nkey innovations: (1) a novel microstructure reconstruction algorithm that\ngenerates composite architecture materials from a reduced design space using\nphysically interpretable parameters; (2) a new material homogenization approach\nthat estimates effective properties by combining the stored energy functions of\nmultiple soft constituents; (3) a neural network-based topology optimization\nthat incorporates data-driven material surrogates to enable bottom-up,\nsimultaneous optimization of material and structure; and (4) a generic\nnonlinear sensitivity analysis technique that computes design sensitivities\nnumerically without requiring explicit gradient derivation. To enhance the\nconvergence of the nonlinear equilibrium equations amid topology optimization,\nwe introduce an energy interpolation scheme and employ a Newton-Raphson solver\nwith adaptive step sizes and convergence criteria. Numerical experiments show\nthat the proposed framework produces distinct topological designs, different\nfrom those obtained under linear elasticity, with spatially varying\nmicrostructures.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23422v1", "categories": ["cs.CE"], "cate": "cs.CE", "url": "http://arxiv.org/abs/2506.23422v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.22855", "title": "Momentum-based Accelerated Algorithm for Distributed Optimization under Sector-Bound Nonlinearity", "authors": ["Mohammadreza Doostmohammadian", "Hamid R. Rabiee"], "summary": "Distributed optimization advances centralized machine learning methods by\nenabling parallel and decentralized learning processes over a network of\ncomputing nodes. This work provides an accelerated consensus-based distributed\nalgorithm for locally non-convex optimization using the gradient-tracking\ntechnique. The proposed algorithm (i) improves the convergence rate by adding\nmomentum towards the optimal state using the heavy-ball method, while (ii)\naddressing general sector-bound nonlinearities over the information-sharing\nnetwork. The link nonlinearity includes any sign-preserving odd sector-bound\nmapping, for example, log-scale data quantization or clipping in practical\napplications. For admissible momentum and gradient-tracking parameters, using\nperturbation theory and eigen-spectrum analysis, we prove convergence even in\nthe presence of sector-bound nonlinearity and for locally non-convex cost\nfunctions. Further, in contrast to most existing weight-stochastic algorithms,\nwe adopt weight-balanced (WB) network design. This WB design and\nperturbation-based analysis allow to handle dynamic directed network of agents\nto address possible time-varying setups due to link failures or packet drops.", "comment": "Journal of the Franklin Institute", "pdf_url": "http://arxiv.org/pdf/2506.22855v1", "categories": ["eess.SY", "cs.DC", "cs.MA", "cs.SY", "eess.SP", "math.OC"], "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.22855v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.22459", "title": "Physics-Embedded Neural Networks for sEMG-based Continuous Motion Estimation", "authors": ["Wending Heng", "Chaoyuan Liang", "Yihui Zhao", "Zhiqiang Zhang", "Glen Cooper", "Zhenhong Li"], "summary": "Accurately decoding human motion intentions from surface electromyography\n(sEMG) is essential for myoelectric control and has wide applications in\nrehabilitation robotics and assistive technologies. However, existing\nsEMG-based motion estimation methods often rely on subject-specific\nmusculoskeletal (MSK) models that are difficult to calibrate, or purely\ndata-driven models that lack physiological consistency. This paper introduces a\nnovel Physics-Embedded Neural Network (PENN) that combines interpretable MSK\nforward-dynamics with data-driven residual learning, thereby preserving\nphysiological consistency while achieving accurate motion estimation. The PENN\nemploys a recursive temporal structure to propagate historical estimates and a\nlightweight convolutional neural network for residual correction, leading to\nrobust and temporally coherent estimations. A two-phase training strategy is\ndesigned for PENN. Experimental evaluations on six healthy subjects show that\nPENN outperforms state-of-the-art baseline methods in both root mean square\nerror (RMSE) and $R^2$ metrics.", "comment": "Accepted by 2025 IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS)", "pdf_url": "http://arxiv.org/pdf/2506.22459v1", "categories": ["eess.SP", "cs.LG"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.22459v1", "date": "2025-06-17", "updated": "2025-06-17"}
{"id": "2506.23002", "title": "An Image Processing Based Blur Reduction Technique in Smartphone-to-Smartphone Visible Light Communication System", "authors": ["Vaigai Nayaki Yokar", "Hoa Le-Minh", "Zabih Ghassemlooy", "Wai Lok Woo"], "summary": "In this paper, we present a blur reduction technique for\nsmartphone-to-smartphone visible light communications (S2SVLC). The key\ntechnique it to avoid the repeated scanning of the transmitted data and to\nlower the amount of data discarded at the receiver end of the S2SVLC system.\nThis image processing method will improve the system recognition efficiency and\ndata rate. The proposed method includes converting the red-green-blue (RGB)\nimage into grayscale, applying contrast enhancement, scaling and binarizing the\nimage to reduce the blur levels in the image. The experiment includes practical\ndata acquisition and further processing and estimation in MATLAB. The\nexperiment is carried out in different conditions like distance, rotation, and\ntilt also considering different surrounding illuminations like ambient light\nand no light conditions to estimate the blur levels in S2SVLC. In this\nexperimental investigation two types of coding, American Standard code for\ninformation interchange (ASCII), and quick response (QR) code are used for data\ntransmission in S2SVLC. The obtained results indicate that, the proposed\ntechnique is proven to improve the recovery efficiency to 96% in the receiver\nend at different conditions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23002v1", "categories": ["eess.IV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.23002v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23388", "title": "Escher Tile Deformation via Closed-Form Solution", "authors": ["Crane He Chen", "Vladimir G. Kim"], "summary": "We present a real-time deformation method for Escher tiles -- interlocking\norganic forms that seamlessly tessellate the plane following symmetry rules. We\nformulate the problem as determining a periodic displacement field. The goal is\nto deform Escher tiles without introducing gaps or overlaps. The resulting\ndisplacement field is obtained in closed form by an analytical solution. Our\nmethod processes tiles of 17 wallpaper groups across various representations\nsuch as images and meshes. Rather than treating tiles as mere boundaries, we\nconsider them as textured shapes, ensuring that both the boundary and interior\ndeform simultaneously. To enable fine-grained artistic input, our interactive\ntool features a user-controllable adaptive fall-off parameter, allowing precise\nadjustment of locality and supporting deformations with meaningful semantic\ncontrol. We demonstrate the effectiveness of our method through various\nexamples, including photo editing and shape sculpting, showing its use in\napplications such as fabrication and animation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23388v1", "categories": ["cs.GR", "cs.CG", "cs.MS", "math.MG"], "cate": "cs.GR", "url": "http://arxiv.org/abs/2506.23388v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23363", "title": "Parameterized Critical Node Cut Revisited", "authors": ["Du≈°an Knop", "Nikolaos Melissinos", "Manolis Vasilakis"], "summary": "Given a graph $G$ and integers $k, x \\geq 0$, the Critical Node Cut problem\nasks whether it is possible to delete at most $k$ vertices from $G$ such that\nthe number of remaining pairs of connected vertices is at most $x$. This\nproblem generalizes Vertex Cover (when $x = 0$), and has applications in\nnetwork design, epidemiology, and social network analysis. We investigate the\nparameterized complexity of Critical Node Cut under various structural\nparameters. We first significantly strengthen existing hardness results by\nproving W[1]-hardness even when parameterized by the combined parameter $k +\n\\mathrm{fes} + \\Delta + \\mathrm{pw}$, where $\\mathrm{fes}$ is the feedback edge\nset number, $\\Delta$ the maximum degree, and $\\mathrm{pw}$ the pathwidth of the\ninput graph. We then identify three structural parameters--max-leaf number,\nvertex integrity, and modular-width--that render the problem fixed-parameter\ntractable. Furthermore, leveraging a technique introduced by Lampis [ICALP\n'14], we develop an FPT approximation scheme that, for any $\\varepsilon > 0$,\ncomputes a $(1+\\varepsilon)$-approximate solution in time $(\\mathrm{tw} /\n\\varepsilon)^{\\mathcal{O}(\\mathrm{tw})} n^{\\mathcal{O}(1)}$, where\n$\\mathrm{tw}$ denotes the treewidth of the input graph. Finally, we show that\nCritical Node Cut does not admit a polynomial kernel when parameterized by\nvertex cover number, unless standard complexity assumptions fail. Overall, our\nresults significantly sharpen the known complexity landscape of Critical Node\nCut.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23363v1", "categories": ["cs.DS", "cs.CC"], "cate": "cs.DS", "url": "http://arxiv.org/abs/2506.23363v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.22447", "title": "Vision Transformers for Multi-Variable Climate Downscaling: Emulating Regional Climate Models with a Shared Encoder and Multi-Decoder Architecture", "authors": ["Fabio Merizzi", "Harilaos Loukos"], "summary": "Global Climate Models (GCMs) are critical for simulating large-scale climate\ndynamics, but their coarse spatial resolution limits their applicability in\nregional studies. Regional Climate Models (RCMs) refine this through dynamic\ndownscaling, albeit at considerable computational cost and with limited\nflexibility. While deep learning has emerged as an efficient data-driven\nalternative, most existing studies have focused on single-variable models that\ndownscale one variable at a time. This approach can lead to limited contextual\nawareness, redundant computation, and lack of cross-variable interaction. Our\nstudy addresses these limitations by proposing a multi-task, multi-variable\nVision Transformer (ViT) architecture with a shared encoder and\nvariable-specific decoders (1EMD). The proposed architecture jointly predicts\nthree key climate variables: surface temperature (tas), wind speed (sfcWind),\nand 500 hPa geopotential height (zg500), directly from GCM-resolution inputs,\nemulating RCM-scale downscaling over Europe. We show that our multi-variable\napproach achieves positive cross-variable knowledge transfer and consistently\noutperforms single-variable baselines trained under identical conditions, while\nalso improving computational efficiency. These results demonstrate the\neffectiveness of multi-variable modeling for high-resolution climate\ndownscaling.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22447v1", "categories": ["cs.LG", "cs.AI", "eess.IV"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22447v1", "date": "2025-06-12", "updated": "2025-06-12"}
{"id": "2506.22893", "title": "Agentic Enterprise: AI-Centric User to User-Centric AI", "authors": ["Arpit Narechania", "Alex Endert", "Atanu R Sinha"], "summary": "After a very long winter, the Artificial Intelligence (AI) spring is here.\nOr, so it seems over the last three years. AI has the potential to impact many\nareas of human life - personal, social, health, education, professional. In\nthis paper, we take a closer look at the potential of AI for Enterprises, where\ndecision-making plays a crucial and repeated role across functions, tasks, and\noperations. We consider Agents imbued with AI as means to increase\ndecision-productivity of enterprises. We highlight six tenets for Agentic\nsuccess in enterprises, by drawing attention to what the current, AI-Centric\nUser paradigm misses, in the face of persistent needs of and usefulness for\nEnterprise Decision-Making. In underscoring a shift to User-Centric AI, we\noffer six tenets and promote market mechanisms for platforms, aligning the\ndesign of AI and its delivery by Agents to the cause of enterprise users.", "comment": "12 pages, 1 figure, 2 sidebars; Preprint", "pdf_url": "http://arxiv.org/pdf/2506.22893v1", "categories": ["cs.AI", "cs.HC"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.22893v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23191", "title": "Impact of Shallow vs. Deep Relevance Judgments on BERT-based Reranking Models", "authors": ["Gabriel Iturra-Bocaz", "Danny Vo", "Petra Galuscakova"], "summary": "This paper investigates the impact of shallow versus deep relevance judgments\non the performance of BERT-based reranking models in neural Information\nRetrieval. Shallow-judged datasets, characterized by numerous queries each with\nfew relevance judgments, and deep-judged datasets, involving fewer queries with\nextensive relevance judgments, are compared. The research assesses how these\ndatasets affect the performance of BERT-based reranking models trained on them.\nThe experiments are run on the MS MARCO and LongEval collections. Results\nindicate that shallow-judged datasets generally enhance generalization and\neffectiveness of reranking models due to a broader range of available contexts.\nThe disadvantage of the deep-judged datasets might be mitigated by a larger\nnumber of negative training examples.", "comment": "Accepted at ICTIR'25", "pdf_url": "http://arxiv.org/pdf/2506.23191v1", "categories": ["cs.IR"], "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.23191v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.22628", "title": "Evaluating Sound Similarity Metrics for Differentiable, Iterative Sound-Matching", "authors": ["Amir Salimi", "Abram Hindle", "Osmar R. Zaiane"], "summary": "Manual sound design with a synthesizer is inherently iterative: an artist\ncompares the synthesized output to a mental target, adjusts parameters, and\nrepeats until satisfied. Iterative sound-matching automates this workflow by\ncontinually programming a synthesizer under the guidance of a loss function (or\nsimilarity measure) toward a target sound. Prior comparisons of loss functions\nhave typically favored one metric over another, but only within narrow\nsettings: limited synthesis methods, few loss types, often without blind\nlistening tests. This leaves open the question of whether a universally optimal\nloss exists, or the choice of loss remains a creative decision conditioned on\nthe synthesis method and the sound designer's preference. We propose\ndifferentiable iterative sound-matching as the natural extension of the\navailable literature, since it combines the manual approach to sound design\nwith modern advances in machine learning. To analyze the variability of loss\nfunction performance across synthesizers, we implemented a mix of four novel\nand established differentiable loss functions, and paired them with\ndifferentiable subtractive, additive, and AM synthesizers. For each of the\nsixteen synthesizer--loss combinations, we ran 300 randomized sound-matching\ntrials. Performance was measured using parameter differences,\nspectrogram-distance metrics, and manually assigned listening scores. We\nobserved a moderate level of consistency among the three performance measures.\nOur post-hoc analysis shows that the loss function performance is highly\ndependent on the synthesizer. These findings underscore the value of expanding\nthe scope of sound-matching experiments and developing new similarity metrics\ntailored to specific synthesis techniques rather than pursuing\none-size-fits-all solutions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22628v1", "categories": ["cs.SD", "eess.AS"], "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.22628v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22516", "title": "Can \"consciousness\" be observed from large language model (LLM) internal states? Dissecting LLM representations obtained from Theory of Mind test with Integrated Information Theory and Span Representation analysis", "authors": ["Jingkai Li"], "summary": "Integrated Information Theory (IIT) provides a quantitative framework for\nexplaining consciousness phenomenon, positing that conscious systems comprise\nelements integrated through causal properties. We apply IIT 3.0 and 4.0 -- the\nlatest iterations of this framework -- to sequences of Large Language Model\n(LLM) representations, analyzing data derived from existing Theory of Mind\n(ToM) test results. Our study systematically investigates whether the\ndifferences of ToM test performances, when presented in the LLM\nrepresentations, can be revealed by IIT estimates, i.e., $\\Phi^{\\max}$ (IIT\n3.0), $\\Phi$ (IIT 4.0), Conceptual Information (IIT 3.0), and $\\Phi$-structure\n(IIT 4.0). Furthermore, we compare these metrics with the Span Representations\nindependent of any estimate for consciousness. This additional effort aims to\ndifferentiate between potential \"consciousness\" phenomena and inherent\nseparations within LLM representational space. We conduct comprehensive\nexperiments examining variations across LLM transformer layers and linguistic\nspans from stimuli. Our results suggest that sequences of contemporary\nTransformer-based LLM representations lack statistically significant indicators\nof observed \"consciousness\" phenomena but exhibit intriguing patterns under\n$\\textit{spatio}$-permutational analyses. The Appendix and code are available\nas Supplementary Materials at: https://doi.org/10.1016/j.nlp.2025.100163.", "comment": "Published as a journal paper at:\n  https://doi.org/10.1016/j.nlp.2025.100163", "pdf_url": "http://arxiv.org/pdf/2506.22516v1", "categories": ["cs.CL", "cs.AI", "cs.NE", "q-bio.NC"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.22516v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.22782", "title": "Long-time error estimate and decay of finite element method to a generalized viscoelastic flow", "authors": ["Yingwen Guo", "Yinnian He", "Wenlin Qiu", "Xiangcheng Zheng"], "summary": "This work analyzes the finite element approximation to a viscoelastic flow\nmodel, which generalizes the Navier-Stokes equation and Oldroyd's model by\nintroducing the tempered power-law memory kernel. We prove regularity and\nlong-time exponential decay of the solutions, as well as a long-time\nconvolution-type Gr\\\"onwall inequality to support numerical analysis. A\nVolterra-Stokes projection is developed and analyzed to facilitate the\nparabolic-type duality argument, leading to the long-time error estimates and\nexponential decay of velocity and pressure. A benchmark problem of planar\nfour-to-one contraction flow is simulated to substantiate the generality of the\nproposed model in comparison with the Navier-Stokes equation and Oldroyd's\nmodel.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22782v1", "categories": ["math.NA", "cs.NA"], "cate": "math.NA", "url": "http://arxiv.org/abs/2506.22782v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.22771", "title": "FF-INT8: Efficient Forward-Forward DNN Training on Edge Devices with INT8 Precision", "authors": ["Jingxiao Ma", "Priyadarshini Panda", "Sherief Reda"], "summary": "Backpropagation has been the cornerstone of neural network training for\ndecades, yet its inefficiencies in time and energy consumption limit its\nsuitability for resource-constrained edge devices. While low-precision neural\nnetwork quantization has been extensively researched to speed up model\ninference, its application in training has been less explored. Recently, the\nForward-Forward (FF) algorithm has emerged as a promising alternative to\nbackpropagation, replacing the backward pass with an additional forward pass.\nBy avoiding the need to store intermediate activations for backpropagation, FF\ncan reduce memory footprint, making it well-suited for embedded devices. This\npaper presents an INT8 quantized training approach that leverages FF's\nlayer-by-layer strategy to stabilize gradient quantization. Furthermore, we\npropose a novel \"look-ahead\" scheme to address limitations of FF and improve\nmodel accuracy. Experiments conducted on NVIDIA Jetson Orin Nano board\ndemonstrate 4.6% faster training, 8.3% energy savings, and 27.0% reduction in\nmemory usage, while maintaining competitive accuracy compared to the\nstate-of-the-art.", "comment": "To be published in the 62nd Design Automation Conference (DAC), 2025", "pdf_url": "http://arxiv.org/pdf/2506.22771v1", "categories": ["cs.LG", "cs.AI", "cs.NE", "I.2.0; I.2.6"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22771v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23325", "title": "XY-Tokenizer: Mitigating the Semantic-Acoustic Conflict in Low-Bitrate Speech Codecs", "authors": ["Yitian Gong", "Luozhijie Jin", "Ruifan Deng", "Dong Zhang", "Xin Zhang", "Qinyuan Cheng", "Zhaoye Fei", "Shimin Li", "Xipeng Qiu"], "summary": "Speech codecs serve as bridges between speech signals and large language\nmodels. An ideal codec for speech language models should not only preserve\nacoustic information but also capture rich semantic information. However,\nexisting speech codecs struggle to balance high-quality audio reconstruction\nwith ease of modeling by language models. In this study, we analyze the\nlimitations of previous codecs in balancing semantic richness and acoustic\nfidelity. We propose XY-Tokenizer, a novel codec that mitigates the conflict\nbetween semantic and acoustic capabilities through multi-stage, multi-task\nlearning. Experimental results demonstrate that XY-Tokenizer achieves\nperformance in both semantic and acoustic tasks comparable to that of\nstate-of-the-art codecs operating at similar bitrates, even though those\nexisting codecs typically excel in only one aspect. Specifically, XY-Tokenizer\nachieves strong text alignment, surpassing distillation-based semantic modeling\nmethods such as SpeechTokenizer and Mimi, while maintaining a speaker\nsimilarity score of 0.83 between reconstructed and original audio. The\nreconstruction performance of XY-Tokenizer is comparable to that of BigCodec,\nthe current state-of-the-art among acoustic-only codecs, which achieves a\nspeaker similarity score of 0.84 at a similar bitrate. Code and models are\navailable at https://github.com/gyt1145028706/XY-Tokenizer.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23325v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.23325v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23511", "title": "Mutli-Level Autoencoder: Deep Learning Based Channel Coding and Modulation", "authors": ["Ahmad Abdel-Qader", "Anas Chaaban", "Mohamed S. Shehata"], "summary": "In this paper, we design a deep learning-based convolutional autoencoder for\nchannel coding and modulation. The objective is to develop an adaptive scheme\ncapable of operating at various signal-to-noise ratios (SNR)s without the need\nfor re-training. Additionally, the proposed framework allows validation by\ntesting all possible codes in the codebook, as opposed to previous AI-based\nencoder/decoder frameworks which relied on testing only a small subset of the\navailable codes. This limitation in earlier methods often led to unreliable\nconclusions when generalized to larger codebooks. In contrast to previous\nmethods, our multi-level encoding and decoding approach splits the message into\nblocks, where each encoder block processes a distinct group of $B$ bits. By\ndoing so, the proposed scheme can exhaustively test $2^{B}$ possible codewords\nfor each encoder/decoder level, constituting a layer of the overall scheme. The\nproposed model was compared to classical polar codes and TurboAE-MOD schemes,\nshowing improved reliability with achieving comparable, or even superior\nresults in some settings. Notably, the architecture can adapt to different SNRs\nby selectively removing one of the encoder/decoder layers without re-training,\nthus demonstrating flexibility and efficiency in practical wireless\ncommunication scenarios.", "comment": "Accepted at IWCMC 2025", "pdf_url": "http://arxiv.org/pdf/2506.23511v1", "categories": ["eess.SP", "cs.ET"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.23511v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22722", "title": "Kill Two Birds with One Stone! Trajectory enabled Unified Online Detection of Adversarial Examples and Backdoor Attacks", "authors": ["Anmin Fu", "Fanyu Meng", "Huaibing Peng", "Hua Ma", "Zhi Zhang", "Yifeng Zheng", "Willy Susilo", "Yansong Gao"], "summary": "The proposed UniGuard is the first unified online detection framework capable\nof simultaneously addressing adversarial examples and backdoor attacks.\nUniGuard builds upon two key insights: first, both AE and backdoor attacks have\nto compromise the inference phase, making it possible to tackle them\nsimultaneously during run-time via online detection. Second, an adversarial\ninput, whether a perturbed sample in AE attacks or a trigger-carrying sample in\nbackdoor attacks, exhibits distinctive trajectory signatures from a benign\nsample as it propagates through the layers of a DL model in forward inference.\nThe propagation trajectory of the adversarial sample must deviate from that of\nits benign counterpart; otherwise, the adversarial objective cannot be\nfulfilled. Detecting these trajectory signatures is inherently challenging due\nto their subtlety; UniGuard overcomes this by treating the propagation\ntrajectory as a time-series signal, leveraging LSTM and spectrum transformation\nto amplify differences between adversarial and benign trajectories that are\nsubtle in the time domain. UniGuard exceptional efficiency and effectiveness\nhave been extensively validated across various modalities (image, text, and\naudio) and tasks (classification and regression), ranging from diverse model\narchitectures against a wide range of AE attacks and backdoor attacks,\nincluding challenging partial backdoors and dynamic triggers. When compared to\nSOTA methods, including ContraNet (NDSS 22) specific for AE detection and TED\n(IEEE SP 24) specific for backdoor detection, UniGuard consistently\ndemonstrates superior performance, even when matched against each method's\nstrengths in addressing their respective threats-each SOTA fails to parts of\nattack strategies while UniGuard succeeds for all.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22722v1", "categories": ["cs.CR", "cs.AI"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.22722v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.22966", "title": "Detection of coordinated fleet vehicles in route choice urban games. Part I. Inverse fleet assignment theory", "authors": ["Grzegorz Jamr√≥z", "Rafa≈Ç Kucharski"], "summary": "Detection of collectively routing fleets of vehicles in future urban systems\nmay become important for the management of traffic, as such routing may\ndestabilize urban networks leading to deterioration of driving conditions.\nAccordingly, in this paper we discuss the question whether it is possible to\ndetermine the flow of fleet vehicles on all routes given the fleet size and\nbehaviour as well as the combined total flow of fleet and non-fleet vehicles on\nevery route. We prove that the answer to this Inverse Fleet Assignment Problem\nis 'yes' for myopic fleet strategies which are more 'selfish' than\n'altruistic', and 'no' otherwise, under mild assumptions on route/link\nperformance functions. To reach these conclusions we introduce the forward\nfleet assignment operator and study its properties, proving that it is\ninvertible for 'bad' objectives of fleet controllers. We also discuss the\nchallenges of implementing myopic fleet routing in the real world and compare\nit to Stackelberg and Nash routing. Finally, we show that optimal Stackelberg\nfleet routing could involve highly variable mixed strategies in some scenarios,\nwhich would likely cause chaos in the traffic network.", "comment": "30 pages, 7 figures", "pdf_url": "http://arxiv.org/pdf/2506.22966v1", "categories": ["math.OC", "cs.MA", "econ.TH"], "cate": "math.OC", "url": "http://arxiv.org/abs/2506.22966v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.22788", "title": "SPI-BoTER: Error Compensation for Industrial Robots via Sparse Attention Masking and Hybrid Loss with Spatial-Physical Information", "authors": ["Xuao Hou", "Yongquan Jia", "Shijin Zhang", "Yuqiang Wu"], "summary": "The widespread application of industrial robots in fields such as cutting and\nwelding has imposed increasingly stringent requirements on the trajectory\naccuracy of end-effectors. However, current error compensation methods face\nseveral critical challenges, including overly simplified mechanism modeling, a\nlack of physical consistency in data-driven approaches, and substantial data\nrequirements. These issues make it difficult to achieve both high accuracy and\nstrong generalization simultaneously. To address these challenges, this paper\nproposes a Spatial-Physical Informed Attention Residual Network (SPI-BoTER).\nThis method integrates the kinematic equations of the robotic manipulator with\na Transformer architecture enhanced by sparse self-attention masks. A\nparameter-adaptive hybrid loss function incorporating spatial and physical\ninformation is employed to iteratively optimize the network during training,\nenabling high-precision error compensation under small-sample conditions.\nAdditionally, inverse joint angle compensation is performed using a gradient\ndescent-based optimization method. Experimental results on a small-sample\ndataset from a UR5 robotic arm (724 samples, with a train:test:validation split\nof 8:1:1) demonstrate the superior performance of the proposed method. It\nachieves a 3D absolute positioning error of 0.2515 mm with a standard deviation\nof 0.15 mm, representing a 35.16\\% reduction in error compared to conventional\ndeep neural network (DNN) methods. Furthermore, the inverse angle compensation\nalgorithm converges to an accuracy of 0.01 mm within an average of 147\niterations. This study presents a solution that combines physical\ninterpretability with data adaptability for high-precision control of\nindustrial robots, offering promising potential for the reliable execution of\nprecision tasks in intelligent manufacturing.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22788v1", "categories": ["cs.RO"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.22788v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.22487", "title": "AGI Enabled Solutions For IoX Layers Bottlenecks In Cyber-Physical-Social-Thinking Space", "authors": ["Amar Khelloufi", "Huansheng Ning", "Sahraoui Dhelim", "Jianguo Ding"], "summary": "The integration of the Internet of Everything (IoX) and Artificial General\nIntelligence (AGI) has given rise to a transformative paradigm aimed at\naddressing critical bottlenecks across sensing, network, and application layers\nin Cyber-Physical-Social Thinking (CPST) ecosystems. In this survey, we provide\na systematic and comprehensive review of AGI-enhanced IoX research, focusing on\nthree key components: sensing-layer data management, network-layer protocol\noptimization, and application-layer decision-making frameworks. Specifically,\nthis survey explores how AGI can mitigate IoX bottlenecks challenges by\nleveraging adaptive sensor fusion, edge preprocessing, and selective attention\nmechanisms at the sensing layer, while resolving network-layer issues such as\nprotocol heterogeneity and dynamic spectrum management, neuro-symbolic\nreasoning, active inference, and causal reasoning, Furthermore, the survey\nexamines AGI-enabled frameworks for managing identity and relationship\nexplosion. Key findings suggest that AGI-driven strategies, such as adaptive\nsensor fusion, edge preprocessing, and semantic modeling, offer novel solutions\nto sensing-layer data overload, network-layer protocol heterogeneity, and\napplication-layer identity explosion. The survey underscores the importance of\ncross-layer integration, quantum-enabled communication, and ethical governance\nframeworks for future AGI-enabled IoX systems. Finally, the survey identifies\nunresolved challenges, such as computational requirements, scalability, and\nreal-world validation, calling for further research to fully realize AGI's\npotential in addressing IoX bottlenecks. we believe AGI-enhanced IoX is\nemerging as a critical research field at the intersection of interconnected\nsystems and advanced AI.", "comment": "31 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2506.22487v1", "categories": ["cs.NI", "cs.AI"], "cate": "cs.NI", "url": "http://arxiv.org/abs/2506.22487v1", "date": "2025-06-24", "updated": "2025-06-24"}
{"id": "2506.23469", "title": "Reconciling Attribute and Structural Anomalies for Improved Graph Anomaly Detection", "authors": ["Chunjing Xiao", "Jiahui Lu", "Xovee Xu", "Fan Zhou", "Tianshu Xie", "Wei Lu", "Lifeng Xu"], "summary": "Graph anomaly detection is critical in domains such as healthcare and\neconomics, where identifying deviations can prevent substantial losses.\nExisting unsupervised approaches strive to learn a single model capable of\ndetecting both attribute and structural anomalies. However, they confront the\ntug-of-war problem between two distinct types of anomalies, resulting in\nsuboptimal performance. This work presents TripleAD, a mutual\ndistillation-based triple-channel graph anomaly detection framework. It\nincludes three estimation modules to identify the attribute, structural, and\nmixed anomalies while mitigating the interference between different types of\nanomalies. In the first channel, we design a multiscale attribute estimation\nmodule to capture extensive node interactions and ameliorate the over-smoothing\nissue. To better identify structural anomalies, we introduce a link-enhanced\nstructure estimation module in the second channel that facilitates information\nflow to topologically isolated nodes. The third channel is powered by an\nattribute-mixed curvature, a new indicator that encapsulates both attribute and\nstructural information for discriminating mixed anomalies. Moreover, a mutual\ndistillation strategy is introduced to encourage communication and\ncollaboration between the three channels. Extensive experiments demonstrate the\neffectiveness of the proposed TripleAD model against strong baselines.", "comment": "Accepted by IEEE Transactions on Neural Networks and Learning Systems\n  (TNNLS); DOI: https://doi.org/10.1109/TNNLS.2025.3561172", "pdf_url": "http://arxiv.org/pdf/2506.23469v1", "categories": ["cs.LG", "cs.SI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.23469v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23034", "title": "Guiding AI to Fix Its Own Flaws: An Empirical Study on LLM-Driven Secure Code Generation", "authors": ["Hao Yan", "Swapneel Suhas Vaidya", "Xiaokuan Zhang", "Ziyu Yao"], "summary": "Large Language Models (LLMs) have become powerful tools for automated code\ngeneration. However, these models often overlook critical security practices,\nwhich can result in the generation of insecure code that contains\nvulnerabilities-weaknesses or flaws in the code that attackers can exploit to\ncompromise a system. However, there has been limited exploration of strategies\nto guide LLMs in generating secure code and a lack of in-depth analysis of the\neffectiveness of LLMs in repairing code containing vulnerabilities. In this\npaper, we present a comprehensive evaluation of state-of-the-art LLMs by\nexamining their inherent tendencies to produce insecure code, their capability\nto generate secure code when guided by self-generated vulnerability hints, and\ntheir effectiveness in repairing vulnerabilities when provided with different\nlevels of feedback. Our study covers both proprietary and open-weight models\nacross various scales and leverages established benchmarks to assess a wide\nrange of vulnerability types. Through quantitative and qualitative analyses, we\nreveal that although LLMs are prone to generating insecure code, advanced\nmodels can benefit from vulnerability hints and fine-grained feedback to avoid\nor fix vulnerabilities. We also provide actionable suggestions to developers to\nreduce vulnerabilities when using LLMs for code generation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23034v1", "categories": ["cs.SE"], "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.23034v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.22926", "title": "Coordinated 2D-3D Visualization of Volumetric Medical Data in XR with Multimodal Interactions", "authors": ["Qixuan Liu", "Shi Qiu", "Yinqiao Wang", "Xiwen Wu", "Kenneth Siu Ho Chok", "Chi-Wing Fu", "Pheng-Ann Heng"], "summary": "Volumetric medical imaging technologies produce detailed 3D representations\nof anatomical structures. However, effective medical data visualization and\nexploration pose significant challenges, especially for individuals with\nlimited medical expertise. We introduce a novel XR-based system with two key\ninnovations: (1) a coordinated visualization module integrating Multi-layered\nMulti-planar Reconstruction with 3D mesh models and (2) a multimodal\ninteraction framework combining hand gestures with LLM-enabled voice commands.\nWe conduct preliminary evaluations, including a 15-participant user study and\nexpert interviews, to demonstrate the system's abilities to enhance spatial\nunderstanding and reduce cognitive load. Experimental results show notable\nimprovements in task completion times, usability metrics, and interaction\neffectiveness enhanced by LLM-driven voice control. While identifying areas for\nfuture refinement, our findings highlight the potential of this immersive\nvisualization system to advance medical training and clinical practice. Our\ndemo application and supplemental materials are available for download at:\nhttps://osf.io/bpjq5/.", "comment": "IEEE VIS 2025 Short Paper", "pdf_url": "http://arxiv.org/pdf/2506.22926v1", "categories": ["cs.HC", "cs.GR", "cs.MM"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.22926v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23680", "title": "Asymptotically Optimal Secure Aggregation for Wireless Federated Learning with Multiple Servers", "authors": ["Zhenhao Huang", "Kai Liang", "Yuanming Shi", "Songze Li", "Youlong Wu"], "summary": "In this paper, we investigate the transmission latency of the secure\naggregation problem in a \\emph{wireless} federated learning system with\nmultiple curious servers. We propose a privacy-preserving coded aggregation\nscheme where the servers can not infer any information about the distributed\nusers' local gradients, nor the aggregation value. In our scheme, each user\nencodes its local gradient into $\\sK$ confidential messages intended\nexclusively for different servers using a multi-secret sharing method, and each\nserver forwards the summation of the received confidential messages, while the\nusers sequentially employ artificial noise alignment techniques to facilitate\nsecure transmission. Through these summations, the user can recover the\naggregation of all local gradients. We prove the privacy guarantee in the\ninformation-theoretic sense and characterize the uplink and downlink\ncommunication latency measured by \\emph{normalized delivery time} (NDT), both\nof which decrease monotonically with the number of servers $\\sK$ while\nincreasing over most of the range of the number of users $\\sM$. Finally, we\nestablish a lower bound on the NDT of the considered system and theoretically\nprove that the scheme achieves the optimal uplink and downlink NDT under the\nconditions $\\sK \\gg \\sM \\gg 0$ and $\\sK \\gg \\sM$, respectively. For arbitrary\n$\\sK$ and $\\sM$, the proposed scheme achieves the optimal uplink NDT within a\nmultiplicative gap of $4$.", "comment": "This work was in part presented at the IEEE International Symposium\n  on Information Theory (ISIT), 2023", "pdf_url": "http://arxiv.org/pdf/2506.23680v1", "categories": ["cs.IT", "math.IT"], "cate": "cs.IT", "url": "http://arxiv.org/abs/2506.23680v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23672", "title": "Data-Driven Power Modeling and Monitoring via Hardware Performance Counter Tracking", "authors": ["Sergio Mazzola", "Gabriele Ara", "Thomas Benz", "Bj√∂rn Forsberg", "Tommaso Cucinotta", "Luca Benini"], "summary": "Energy-centric design is paramount in the current embedded computing era: use\ncases require increasingly high performance at an affordable power budget,\noften under real-time constraints. Hardware heterogeneity and parallelism help\naddress the efficiency challenge, but greatly complicate online power\nconsumption assessments, which are essential for dynamic hardware and software\nstack adaptations. We introduce a novel power modeling methodology with\nstate-of-the-art accuracy, low overhead, and high responsiveness, whose\nimplementation does not rely on microarchitectural details. Our methodology\nidentifies the Performance Monitoring Counters (PMCs) with the highest linear\ncorrelation to the power consumption of each hardware sub-system, for each\nDynamic Voltage and Frequency Scaling (DVFS) state. The individual, simple\nmodels are composed into a complete model that effectively describes the power\nconsumption of the whole system, achieving high accuracy and low overhead. Our\nevaluation reports an average estimation error of 7.5% for power consumption\nand 1.3% for energy. We integrate these models in the Linux kernel with\nRunmeter, an open-source, PMC-based monitoring framework. Runmeter manages PMC\nsampling and processing, enabling the execution of our power models at runtime.\nWith a worst-case time overhead of only 0.7%, Runmeter provides responsive and\naccurate power measurements directly in the kernel. This information can be\nemployed for actuation policies in workload-aware DVFS and power-aware,\nclosed-loop task scheduling.", "comment": "Published on Journal of Systems Architecture (JSA), here:\n  https://doi.org/10.1016/j.sysarc.2025.103504 Extension of conference paper\n  https://doi.org/10.1007/978-3-031-15074-6_22 (SAMOS 2022)", "pdf_url": "http://arxiv.org/pdf/2506.23672v1", "categories": ["cs.PF", "cs.AR"], "cate": "cs.PF", "url": "http://arxiv.org/abs/2506.23672v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23934", "title": "QPART: Adaptive Model Quantization and Dynamic Workload Balancing for Accuracy-aware Edge Inference", "authors": ["Xiangchen Li", "Saeid Ghafouri", "Bo Ji", "Hans Vandierendonck", "Deepu John", "Dimitrios S. Nikolopoulos"], "summary": "As machine learning inferences increasingly move to edge devices, adapting to\ndiverse computational capabilities, hardware, and memory constraints becomes\nmore critical. Instead of relying on a pre-trained model fixed for all future\ninference queries across diverse edge devices, we argue that planning an\ninference pattern with a request-specific model tailored to the device's\ncomputational capacity, accuracy requirements, and time constraints is more\ncost-efficient and robust to diverse scenarios. To this end, we propose an\naccuracy-aware and workload-balanced inference system that integrates joint\nmodel quantization and inference partitioning. In this approach, the server\ndynamically responds to inference queries by sending a quantized model and\nadaptively sharing the inference workload with the device. Meanwhile, the\ndevice's computational power, channel capacity, and accuracy requirements are\nconsidered when deciding.\n  Furthermore, we introduce a new optimization framework for the inference\nsystem, incorporating joint model quantization and partitioning. Our approach\noptimizes layer-wise quantization bit width and partition points to minimize\ntime consumption and cost while accounting for varying accuracy requirements of\ntasks through an accuracy degradation metric in our optimization model. To our\nknowledge, this work represents the first exploration of optimizing\nquantization layer-wise bit-width in the inference serving system, by\nintroducing theoretical measurement of accuracy degradation. Simulation results\ndemonstrate a substantial reduction in overall time and power consumption, with\ncomputation payloads decreasing by over 80% and accuracy degradation kept below\n1%.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23934v1", "categories": ["cs.DC", "cs.AI", "cs.LG", "cs.PF"], "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.23934v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22512", "title": "Ask before you Build: Rethinking AI-for-Good in Human Trafficking Interventions", "authors": ["Pratheeksha Nair", "Gabriel Lefebvre", "Sophia Garrel", "Maryam Molamohammadi", "Reihaneh Rabbany"], "summary": "AI for good initiatives often rely on the assumption that technical\ninterventions can resolve complex social problems. In the context of human\ntrafficking (HT), such techno-solutionism risks oversimplifying exploitation,\nreinforcing power imbalances and causing harm to the very communities AI claims\nto support. In this paper, we introduce the Radical Questioning (RQ) framework\nas a five step, pre-project ethical assessment tool to critically evaluate\nwhether AI should be built at all, especially in domains involving marginalized\npopulations and entrenched systemic injustice. RQ does not replace principles\nbased ethics but precedes it, offering an upstream, deliberative space to\nconfront assumptions, map power, and consider harms before design. Using a case\nstudy in AI for HT, we demonstrate how RQ reveals overlooked sociocultural\ncomplexities and guides us away from surveillance based interventions toward\nsurvivor empowerment tools. While developed in the context of HT, RQ's five\nstep structure can generalize to other domains, though the specific questions\nmust be contextual. This paper situates RQ within a broader AI ethics\nphilosophy that challenges instrumentalist norms and centers relational,\nreflexive responsibility.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22512v1", "categories": ["cs.CY", "cs.AI"], "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.22512v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.22503", "title": "What Makes a Dribble Successful? Insights From 3D Pose Tracking Data", "authors": ["Michiel Schepers", "Pieter Robberechts", "Jan Van Haaren", "Jesse Davis"], "summary": "Data analysis plays an increasingly important role in soccer, offering new\nways to evaluate individual and team performance. One specific application is\nthe evaluation of dribbles: one-on-one situations where an attacker attempts to\nbypass a defender with the ball. While previous research has primarily relied\non 2D positional tracking data, this fails to capture aspects like balance,\norientation, and ball control, limiting the depth of current insights. This\nstudy explores how pose tracking data (capturing players' posture and movement\nin three dimensions) can improve our understanding of dribbling skills. We\nextract novel pose-based features from 1,736 dribbles in the 2022/23 Champions\nLeague season and evaluate their impact on dribble success. Our results\nindicate that features capturing the attacker's balance and the alignment of\nthe orientation between the attacker and defender are informative for\npredicting dribble success. Incorporating these pose-based features on top of\nfeatures derived from traditional 2D positional data leads to a measurable\nimprovement in model performance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22503v1", "categories": ["cs.CV", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22503v1", "date": "2025-06-25", "updated": "2025-06-25"}
{"id": "2506.22520", "title": "Exploring Artificial Intelligence Tutor Teammate Adaptability to Harness Discovery Curiosity and Promote Learning in the Context of Interactive Molecular Dynamics", "authors": ["Mustafa Demir", "Jacob Miratsky", "Jonathan Nguyen", "Chun Kit Chan", "Punya Mishra", "Abhishek Singharoy"], "summary": "This study examines the impact of an Artificial Intelligence tutor teammate\n(AI) on student curiosity-driven engagement and learning effectiveness during\nInteractive Molecular Dynamics (IMD) tasks on the Visual Molecular Dynamics\nplatform. It explores the role of the AI's curiosity-triggering and response\nbehaviors in stimulating and sustaining student curiosity, affecting the\nfrequency and complexity of student-initiated questions. The study further\nassesses how AI interventions shape student engagement, foster discovery\ncuriosity, and enhance team performance within the IMD learning environment.\nUsing a Wizard-of-Oz paradigm, a human experimenter dynamically adjusts the AI\ntutor teammate's behavior through a large language model. By employing a\nmixed-methods exploratory design, a total of 11 high school students\nparticipated in four IMD tasks that involved molecular visualization and\ncalculations, which increased in complexity over a 60-minute period. Team\nperformance was evaluated through real-time observation and recordings, whereas\nteam communication was measured by question complexity and AI's\ncuriosity-triggering and response behaviors. Cross Recurrence Quantification\nAnalysis (CRQA) metrics reflected structural alignment in coordination and were\nlinked to communication behaviors. High-performing teams exhibited superior\ntask completion, deeper understanding, and increased engagement. Advanced\nquestions were associated with AI curiosity-triggering, indicating heightened\nengagement and cognitive complexity. CRQA metrics highlighted dynamic\nsynchronization in student-AI interactions, emphasizing structured yet adaptive\nengagement to promote curiosity. These proof-of-concept findings suggest that\nthe AI's dual role as a teammate and educator indicates its capacity to provide\nadaptive feedback, sustaining engagement and epistemic curiosity.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22520v1", "categories": ["cs.HC", "cs.AI", "cs.CE", "cs.CY"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.22520v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.22867", "title": "Identification of Cellular Automata on Spaces of Bernoulli Probability Measures", "authors": ["Faizal Hafiz", "Amelia Kunze", "Enrico Formenti", "Davide La Torre"], "summary": "Classical Cellular Automata (CCAs) are a powerful computational framework for\nmodeling global spatio-temporal dynamics with local interactions. While CCAs\nhave been applied across numerous scientific fields, identifying the local rule\nthat governs observed dynamics remains a challenging task. Moreover, the\nunderlying assumption of deterministic cell states often limits the\napplicability of CCAs to systems characterized by inherent uncertainty. This\nstudy, therefore, focuses on the identification of Cellular Automata on spaces\nof probability measures (CAMs), where cell states are represented by\nprobability distributions. This framework enables the modeling of systems with\nprobabilistic uncertainty and spatially varying dynamics. Moreover, we\nformulate the local rule identification problem as a parameter estimation\nproblem and propose a meta-heuristic search based on Self-adaptive Differential\nEvolution (SaDE) to estimate local rule parameters accurately from the observed\ndata. The efficacy of the proposed approach is demonstrated through local rule\nidentification in two-dimensional CAMs with varying neighborhood types and\nradii.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22867v1", "categories": ["eess.SY", "cs.IT", "cs.SY", "math.IT"], "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.22867v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.22460", "title": "Heart rate and respiratory rate prediction from noisy real-world smartphone based on Deep Learning methods", "authors": ["Ibne Farabi Shihab"], "summary": "Using mobile phone video of the fingertip as a data source for estimating\nvital signs such as heart rate (HR) and respiratory rate (RR) during daily life\nhas long been suggested. While existing literature indicates that these\nestimates are accurate to within several beats or breaths per minute, the data\nused to draw these conclusions are typically collected in laboratory\nenvironments under careful experimental control, and yet the results are\nassumed to generalize to daily life. In an effort to test it, a team of\nresearchers collected a large dataset of mobile phone video recordings made\nduring daily life and annotated with ground truth HR and RR labels from N=111\nparticipants. They found that traditional algorithm performance on the\nfingerprint videos is worse than previously reported (7 times and 13 times\nworse for RR and HR, respectively). Fortunately, recent advancements in deep\nlearning, especially in convolutional neural networks (CNNs), offer a promising\nsolution to improve this performance. This study proposes a new method for\nestimating HR and RR using a novel 3D deep CNN, demonstrating a reduced error\nin estimated HR by 68% and RR by 75%. These promising results suggest that\nregressor-based deep learning approaches should be used in estimating HR and\nRR.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22460v1", "categories": ["eess.SP", "cs.AI"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.22460v1", "date": "2025-06-17", "updated": "2025-06-17"}
{"id": "2506.23005", "title": "Channel characterization in screen-to-camera based optical camera communication", "authors": ["Vaigai Nayaki Yokar", "Hoa Le Minh", "Zabih Ghassemlooy", "Wai Lok Woo"], "summary": "With the increase in optical camera communication (OCC), a screen to\ncamera-based communication can be established. This opens a new field of\nvisible light communication (VLC) known as smartphone to smartphone based\nvisible light communication (S2SVLC) system. In this paper, we experimentally\ndemonstrate a S2SVLC system based on VLC technology using a smartphone screen\nand a smartphone camera over a link span of 20 cms. We analyze the Lambertian\norder of the smartphone screen and carry out a channel characterization of a\nscreen to camera link-based VLC system under specific test conditions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23005v1", "categories": ["eess.IV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.23005v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23406", "title": "Uncertain Mode Surfaces in 3D Symmetric Second-Order Tensor Field Ensembles", "authors": ["Tim Gerrits"], "summary": "The analysis of 3D symmetric second-order tensor fields often relies on\ntopological features such as degenerate tensor lines, neutral surfaces, and\ntheir generalization to mode surfaces, which reveal important structural\ninsights into the data. However, uncertainty in such fields is typically\nvisualized using derived scalar attributes or tensor glyph representations,\nwhich often fail to capture the global behavior. Recent advances have\nintroduced uncertain topological features for tensor field ensembles by\nfocusing on degenerate tensor locations. Yet, mode surfaces, including neutral\nsurfaces and arbitrary mode surfaces are essential to a comprehensive\nunderstanding of tensor field topology. In this work, we present a\ngeneralization of uncertain degenerate tensor features to uncertain mode\nsurfaces of arbitrary mode values, encompassing uncertain degenerate tensor\nlines as a special case. Our approach supports both surface and line\ngeometries, forming a unified framework for analyzing uncertain mode-based\ntopological features in tensor field ensembles. We demonstrate the\neffectiveness of our method on several real-world simulation datasets from\nengineering and materials science.", "comment": "4 + 1 pages, 4 figures, IEEE VIS 2025", "pdf_url": "http://arxiv.org/pdf/2506.23406v1", "categories": ["cs.GR"], "cate": "cs.GR", "url": "http://arxiv.org/abs/2506.23406v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23399", "title": "Planar Multiway Cut with Terminals on Few Faces", "authors": ["Sukanya Pandey", "Erik Jan van Leeuwen"], "summary": "We consider the \\textsc{Edge Multiway Cut} problem on planar graphs. It is\nknown that this can be solved in $n^{O(\\sqrt{t})}$ time [Klein, Marx, ICALP\n2012] and not in $n^{o(\\sqrt{t})}$ time under the Exponential Time Hypothesis\n[Marx, ICALP 2012], where $t$ is the number of terminals. A stronger parameter\nis the number $k$ of faces of the planar graph that jointly cover all\nterminals. For the related {\\sc Steiner Tree} problem, an $n^{O(\\sqrt{k})}$\ntime algorithm was recently shown [Kisfaludi-Bak et al., SODA 2019]. By a\ncompletely different approach, we prove in this paper that \\textsc{Edge\nMultiway Cut} can be solved in $n^{O(\\sqrt{k})}$ time as well.\n  Our approach employs several major concepts on planar graphs, including\nhomotopy and sphere-cut decomposition. We also mix a global treewidth dynamic\nprogram with a Dreyfus-Wagner style dynamic program to locally deal with large\nnumbers of terminals.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23399v1", "categories": ["cs.DS", "F.2.0"], "cate": "cs.DS", "url": "http://arxiv.org/abs/2506.23399v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.22502", "title": "Stabilization of industrial processes with time series machine learning", "authors": ["Matvei Anoshin", "Olga Tsurkan", "Vadim Lopatkin", "Leonid Fedichkin"], "summary": "The stabilization of time series processes is a crucial problem that is\nubiquitous in various industrial fields. The application of machine learning to\nits solution can have a decisive impact, improving both the quality of the\nresulting stabilization with less computational resources required. In this\nwork, we present a simple pipeline consisting of two neural networks: the\noracle predictor and the optimizer, proposing a substitution of the point-wise\nvalues optimization to the problem of the neural network training, which\nsuccessfully improves stability in terms of the temperature control by about 3\ntimes compared to ordinary solvers.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22502v1", "categories": ["cs.LG", "cs.SY", "eess.SY"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22502v1", "date": "2025-06-25", "updated": "2025-06-25"}
{"id": "2506.22919", "title": "Hecto: Modular Sparse Experts for Adaptive and Interpretable Reasoning", "authors": ["Sanskar Pandey", "Ruhaan Chopra", "Saad Murtaza Bhat", "Ark Abhyudaya"], "summary": "Mixture-of-Experts (MoE) models enable conditional computation by routing\ninputs to specialized experts, but these experts rely on identical inductive\nbiases, thus limiting representational diversity. This static computation\npathway is inefficient for inputs that require different types of reasoning and\nlimits specialization and interpretability. We propose Hecto, a lightweight MoE\narchitecture that leverages architectural heterogeneity by combining a GRU\nexpert for temporal reasoning and an FFNN expert for static abstraction under a\nsparse Top-1 gating mechanism. Evaluated on three reasoning benchmarks (AG\nNews, SST-2, HotpotQA) and a regression task (STS-B), Hecto matches or closely\ntrails homogeneous baselines in performance despite receiving isolated input\nrepresentations, while achieving clear expert specialization, with each expert\naligning to distinct reasoning types (temporal vs static). At larger batch\nsizes, Hecto exhibits improved performance, benefiting from relaxed\ncomputational constraints that allow its heterogeneous architecture to optimize\nmore effectively. Ablation results isolate architectural diversity as the\nsource of Hecto's stability and interpretability across diverse reasoning\ntasks. Overall, Hecto establishes itself as a new benchmark for conditional\ncomputation, offering a principled framework for specialized reasoning in\nlow-resource regimes with its model strength derived from principled\nspecialization.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22919v1", "categories": ["cs.AI"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.22919v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23319", "title": "Learning to Rank with Variable Result Presentation Lengths", "authors": ["Norman Knyazev", "Harrie Oosterhuis"], "summary": "Learning to Rank (LTR) methods generally assume that each document in a top-K\nranking is presented in an equal format. However, previous work has shown that\nusers' perceptions of relevance can be changed by varying presentations, i.e.,\nallocating more vertical space to some documents to provide additional textual\nor image information. Furthermore, presentation length can also redirect\nattention, as users are more likely to notice longer presentations when\nscrolling through results. Deciding on the document presentation lengths in a\nfixed vertical space ranking is an important problem that has not been\naddressed by existing LTR methods.\n  We address this gap by introducing the variable presentation length ranking\ntask, where simultaneously the ordering of documents and their presentation\nlength is decided. Despite being a generalization of standard ranking, we show\nthat this setting brings significant new challenges: Firstly, the probability\nranking principle no longer applies to this setting, and secondly, the problem\ncannot be divided into separate ordering and length selection tasks.\n  We therefore propose VLPL - a new family of Plackett-Luce list-wise gradient\nestimation methods for the joint optimization of document ordering and lengths.\nOur semi-synthetic experiments show that VLPL can effectively balance the\nexpected exposure and attractiveness of all documents, achieving the best\nperformance across different ranking settings. Furthermore, we observe that\neven simple length-aware methods can achieve significant performance\nimprovements over fixed-length models. Altogether, our theoretical and\nempirical results highlight the importance and difficulties of combining\ndocument presentation with LTR.", "comment": "SIGIR 2025", "pdf_url": "http://arxiv.org/pdf/2506.23319v1", "categories": ["cs.IR", "cs.LG"], "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.23319v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.22661", "title": "Enhancing Neural Audio Fingerprint Robustness to Audio Degradation for Music Identification", "authors": ["R. Oguz Araz", "Guillem Cort√®s-Sebasti√†", "Emilio Molina", "Joan Serr√†", "Xavier Serra", "Yuki Mitsufuji", "Dmitry Bogdanov"], "summary": "Audio fingerprinting (AFP) allows the identification of unknown audio content\nby extracting compact representations, termed audio fingerprints, that are\ndesigned to remain robust against common audio degradations. Neural AFP methods\noften employ metric learning, where representation quality is influenced by the\nnature of the supervision and the utilized loss function. However, recent work\nunrealistically simulates real-life audio degradation during training,\nresulting in sub-optimal supervision. Additionally, although several modern\nmetric learning approaches have been proposed, current neural AFP methods\ncontinue to rely on the NT-Xent loss without exploring the recent advances or\nclassical alternatives. In this work, we propose a series of best practices to\nenhance the self-supervision by leveraging musical signal properties and\nrealistic room acoustics. We then present the first systematic evaluation of\nvarious metric learning approaches in the context of AFP, demonstrating that a\nself-supervised adaptation of the triplet loss yields superior performance. Our\nresults also reveal that training with multiple positive samples per anchor has\ncritically different effects across loss functions. Our approach is built upon\nthese insights and achieves state-of-the-art performance on both a large,\nsynthetically degraded dataset and a real-world dataset recorded using\nmicrophones in diverse music venues.", "comment": "Accepted to ISMIR2025", "pdf_url": "http://arxiv.org/pdf/2506.22661v1", "categories": ["cs.SD", "eess.AS"], "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.22661v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22518", "title": "Weak-to-Strong GraphRAG: Aligning Weak Retrievers with Large Language Models for Graph-based Retrieval Augmented Generation", "authors": ["Deyu Zou", "Yongqiang Chen", "Mufei Li", "Siqi Miao", "Chenxi Liu", "Bo Han", "James Cheng", "Pan Li"], "summary": "Graph-based retrieval-augmented generation (RAG) enables large language\nmodels (LLMs) to ground responses with structured external knowledge from\nup-to-date knowledge graphs (KGs) and reduce hallucinations. However, LLMs\noften rely on a weak retriever in graph-based RAG: I) Due to the lack of ground\ntruth, the retriever is often trained on weak supervision, which often\nintroduces spurious signals to the LLMs. II) Due to the abstraction of graph\ndata, the retrieved knowledge is often presented in unorganized forms. To\nmitigate the issue, we present Refined Graph-based RAG (ReG) to align weak\nretrievers to LLMs for graph-based RAG. Specifically, ReG incorporates LLM\nfeedback to get rid of spurious signals and improve the quality of the\nsupervision. Meanwhile, ReG introduces a structure-aware reorganization module\nto refactor the retrieval results into logically coherent evidence chains.\nExperiments on prominent benchmarks demonstrate that ReG significantly and\nconsistently brings improvements across different LLM backbones by up to 10%.\nThe improved supervision quality enables ReG to match the state-of-the-art\nperformance with 5% training data and to transfer to out-of-distribution KGs.\nNotably, when adopted to reasoning-based LLMs, ReG reduces the reasoning token\ncost by up to 30% and improves the performance by up to 4%.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22518v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.22518v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.22831", "title": "A Chimera domain decomposition method with weak Dirichlet-Robin coupling for finite element simulation of particulate flows", "authors": ["Raphael M√ºnster", "Otto Mierka", "Dmitri Kuzmin", "Stefan Turek"], "summary": "We introduce a new multimesh finite element method for direct numerical\nsimulation of incompressible particulate flows. The proposed approach falls\ninto the category of overlapping domain decomposition / Chimera / overset grid\nmeshes. In addition to calculating the velocity and pressure of the fictitious\nfluid on a fixed background mesh, we solve the incompressible Navier-Stokes\nequations on body-fitted submeshes that are attached to moving particles. The\nsubmesh velocity and pressure are used to calculate the hydrodynamic forces and\ntorques acting on the particles. The coupling with the background velocity and\npressure is enforced via (i) Robin-type boundary conditions for an\nArbitrary-Lagrangian-Eulerian (ALE) formulation of the submesh problems and\n(ii) a Dirichlet-type distributed interior penalty term in the weak form of the\nbackground mesh problem. The implementation of the weak Dirichlet-Robin\ncoupling is discussed in the context of discrete projection methods and finite\nelement discretizations. Detailed numerical studies are performed for standard\ntest problems involving fixed and moving immersed objects. A comparison of\nChimera results with those produced by fictitious boundary methods illustrates\nsignificant gains in the accuracy of drag and lift approximations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22831v1", "categories": ["math.NA", "cs.NA"], "cate": "math.NA", "url": "http://arxiv.org/abs/2506.22831v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23165", "title": "Mirror Descent Policy Optimisation for Robust Constrained Markov Decision Processes", "authors": ["David Bossens", "Atsushi Nitanda"], "summary": "Safety is an essential requirement for reinforcement learning systems. The\nnewly emerging framework of robust constrained Markov decision processes allows\nlearning policies that satisfy long-term constraints while providing guarantees\nunder epistemic uncertainty. This paper presents mirror descent policy\noptimisation for robust constrained Markov decision processes (RCMDPs), making\nuse of policy gradient techniques to optimise both the policy (as a maximiser)\nand the transition kernel (as an adversarial minimiser) on the Lagrangian\nrepresenting a constrained MDP. In the oracle-based RCMDP setting, we obtain an\n$\\mathcal{O}\\left(\\frac{1}{T}\\right)$ convergence rate for the squared distance\nas a Bregman divergence, and an $\\mathcal{O}\\left(e^{-T}\\right)$ convergence\nrate for entropy-regularised objectives. In the sample-based RCMDP setting, we\nobtain an $\\tilde{\\mathcal{O}}\\left(\\frac{1}{T^{1/3}}\\right)$ convergence rate.\nExperiments confirm the benefits of mirror descent policy optimisation in\nconstrained and unconstrained optimisation, and significant improvements are\nobserved in robustness tests when compared to baseline policy optimisation\nalgorithms.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23165v1", "categories": ["cs.LG", "cs.NE"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.23165v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23367", "title": "You Sound a Little Tense: L2 Tailored Clear TTS Using Durational Vowel Properties", "authors": ["Paige Tutt√∂s√≠", "H. Henny Yeung", "Yue Wang", "Jean-Julien Aucouturier", "Angelica Lim"], "summary": "We present the first text-to-speech (TTS) system tailored to second language\n(L2) speakers. We use duration differences between American English tense\n(longer) and lax (shorter) vowels to create a \"clarity mode\" for Matcha-TTS.\nOur perception studies showed that French-L1, English-L2 listeners had fewer\n(at least 9.15%) transcription errors when using our clarity mode, and found it\nmore encouraging and respectful than overall slowed down speech. Remarkably,\nlisteners were not aware of these effects: despite the decreased word error\nrate in clarity mode, listeners still believed that slowing all target words\nwas the most intelligible, suggesting that actual intelligibility does not\ncorrelate with perceived intelligibility. Additionally, we found that\nWhisper-ASR did not use the same cues as L2 speakers to differentiate difficult\nvowels and is not sufficient to assess the intelligibility of TTS systems for\nthese individuals.", "comment": "Accepted to ISCA Speech Synthesis Workshop, 2025", "pdf_url": "http://arxiv.org/pdf/2506.23367v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.23367v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23851", "title": "Comparative Studies: Cloud-Enabled Adaptive Learning System for Scalable Education in Sub-Saharan", "authors": ["Israel Fianyi", "Soonja Yeom", "Ju-Hyun Shin"], "summary": "The integration of cloud computing in education can revolutionise learning in\nadvanced (Australia & South Korea) and middle-income (Ghana & Nigeria)\ncountries, while offering scalable, cost-effective and equitable access to\nadaptive learning systems. This paper explores how cloud computing and adaptive\nlearning technologies are deployed across different socio-economic and\ninfrastructure contexts. The study identifies enabling factors and systematic\nchallenges, providing insights into how cloud-based education can be tailored\nto bridge the digital and educational divide globally.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23851v1", "categories": ["cs.CY", "cs.ET", "cs.HC"], "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.23851v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22727", "title": "Convergent Privacy Framework with Contractive GNN Layers for Multi-hop Aggregations", "authors": ["Yu Zheng", "Chenang Li", "Zhou Li", "Qingsong Wang"], "summary": "Differential privacy (DP) has been integrated into graph neural networks\n(GNNs) to protect sensitive structural information, e.g., edges, nodes, and\nassociated features across various applications. A common approach is to\nperturb the message-passing process, which forms the core of most GNN\narchitectures. However, existing methods typically incur a privacy cost that\ngrows linearly with the number of layers (Usenix Security'23), ultimately\nrequiring excessive noise to maintain a reasonable privacy level. This\nlimitation becomes particularly problematic when deep GNNs are necessary to\ncapture complex and long-range interactions in graphs. In this paper, we\ntheoretically establish that the privacy budget can converge with respect to\nthe number of layers by applying privacy amplification techniques to the\nmessage-passing process, exploiting the contractive properties inherent to\nstandard GNN operations. Motivated by this analysis, we propose a simple yet\neffective Contractive Graph Layer (CGL) that ensures the contractiveness\nrequired for theoretical guarantees while preserving model utility. Our\nframework, CARIBOU, supports both training and inference, equipped with a\ncontractive aggregation module, a privacy allocation module, and a privacy\nauditing module. Experimental evaluations demonstrate that CARIBOU\nsignificantly improves the privacy-utility trade-off and achieves superior\nperformance in privacy auditing tasks.", "comment": "23 pages", "pdf_url": "http://arxiv.org/pdf/2506.22727v1", "categories": ["cs.CR"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.22727v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.22971", "title": "Hierarchical Decentralized Stochastic Control for Cyber-Physical Systems", "authors": ["Kesav Kazam Ramachandran Anantharaman", "Rahul Meshram"], "summary": "This paper presents a two-timescale hierarchical decentralized architecture\nfor control of Cyber-Physical Systems. The architecture consists of $N$\nindependent sub-processes, a global controller, and $N$ local controllers, each\nformulated as a Markov Decision Process (MDP). The global controller, operating\nat a slower timescale optimizes the infinite-horizon discounted cumulative\nreward under budget constraints. For the local controllers, operating at a\nfaster timescale, we propose two different optimization frameworks, namely the\nCOpt and FOpt. In the COpt framework, the local controller also optimizes an\ninfinite-horizon MDP, while in the FOpt framework, the local controller\noptimizes a finite-horizon MDP. The FOpt framework mimics a federal structure,\nwhere the local controllers have more autonomy in their decision making. First,\nthe existence of stationary deterministic optimal policies for both these\nframeworks is established. Then, various relationships between the two\nframeworks are studied, including a bound on the difference between the two\noptimal value functions. Additionally, sufficiency conditions are provided such\nthat the two frameworks lead to the same optimal values.", "comment": "6 pages, 2 figures", "pdf_url": "http://arxiv.org/pdf/2506.22971v1", "categories": ["eess.SY", "cs.LG", "cs.MA", "cs.SY", "math.OC"], "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.22971v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.22827", "title": "Hierarchical Vision-Language Planning for Multi-Step Humanoid Manipulation", "authors": ["Andr√© Schakkal", "Ben Zandonati", "Zhutian Yang", "Navid Azizan"], "summary": "Enabling humanoid robots to reliably execute complex multi-step manipulation\ntasks is crucial for their effective deployment in industrial and household\nenvironments. This paper presents a hierarchical planning and control framework\ndesigned to achieve reliable multi-step humanoid manipulation. The proposed\nsystem comprises three layers: (1) a low-level RL-based controller responsible\nfor tracking whole-body motion targets; (2) a mid-level set of skill policies\ntrained via imitation learning that produce motion targets for different steps\nof a task; and (3) a high-level vision-language planning module that determines\nwhich skills should be executed and also monitors their completion in real-time\nusing pretrained vision-language models (VLMs). Experimental validation is\nperformed on a Unitree G1 humanoid robot executing a non-prehensile\npick-and-place task. Over 40 real-world trials, the hierarchical system\nachieved a 72.5% success rate in completing the full manipulation sequence.\nThese experiments confirm the feasibility of the proposed hierarchical system,\nhighlighting the benefits of VLM-based skill planning and monitoring for\nmulti-step manipulation scenarios. See https://vlp-humanoid.github.io/ for\nvideo demonstrations of the policy rollout.", "comment": "Accepted at the RSS 2025 Workshop on Robot Planning in the Era of\n  Foundation Models", "pdf_url": "http://arxiv.org/pdf/2506.22827v1", "categories": ["cs.RO"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.22827v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.22507", "title": "Integrated Multimodal Sensing and Communication: Challenges, Technologies, and Architectures", "authors": ["Yubo Peng", "Luping Xiang", "Kun Yang", "Feibo Jiang", "Kezhi Wang", "Christos Masouros"], "summary": "The evolution towards 6G networks requires the intelligent integration of\ncommunication and sensing capabilities to support diverse and complex\napplications, such as autonomous driving and immersive services. However,\nexisting integrated sensing and communication (ISAC) systems predominantly rely\non single-modal sensors as primary participants, which leads to a limited\nrepresentation of environmental features and significant performance\nbottlenecks under the emerging requirements of 6G applications. This limitation\nmotivates a paradigm shift from single-modal to multimodal ISAC. In this\narticle, we first analyze the key challenges in realizing multimodal ISAC,\nincluding the fusion of heterogeneous multimodal data, the high communication\noverhead among distributed sensors, and the design of efficient and scalable\nsystem architectures. We then introduce several enabling technologies, such as\nlarge AI models, semantic communication, and multi-agent systems, that hold\npromise for addressing these challenges. To operationalize these technologies,\nwe zoom into three architectural paradigms: fusion-based multimodal ISAC\n(F-MAC), interaction-based multimodal ISAC (I-MAC), and relay-based multimodal\nISAC (R-MAC), each tailored to organize devices and modalities for efficient\ncollaboration in different scenarios. Thereafter, a case study is presented\nbased on the F-MAC scheme, demonstrating that the scheme achieves more\ncomprehensive sensing and improves sensing accuracy by approximately 80%\ncompared to conventional single-modal ISAC systems. Finally, we discuss several\nopen issues to be addressed in the future.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22507v1", "categories": ["cs.NI", "cs.MA", "eess.SP"], "cate": "cs.NI", "url": "http://arxiv.org/abs/2506.22507v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.23510", "title": "Breadth, Depth, and Flux of Course-Prerequisite Networks", "authors": ["Konstantin Zuev", "Pavlos Stavrinides"], "summary": "Course-prerequisite networks (CPNs) are directed acyclic graphs that model\ncomplex academic curricula by representing courses as nodes and dependencies\nbetween them as directed links. These networks are indispensable tools for\nvisualizing, studying, and understanding curricula. For example, CPNs can be\nused to detect important courses, improve advising, guide curriculum design,\nanalyze graduation time distributions, and quantify the strength of knowledge\nflow between different university departments. However, most CPN analyses to\ndate have focused only on micro- and meso-scale properties. To fill this gap,\nwe define and study three new global CPN measures: breadth, depth, and flux.\nAll three measures are invariant under transitive reduction and are based on\nthe concept of topological stratification, which generalizes topological\nordering in directed acyclic graphs. These measures can be used for macro-scale\ncomparison of different CPNs. We illustrate the new measures numerically by\napplying them to three real and synthetic CPNs from three universities: the\nCyprus University of Technology, the California Institute of Technology, and\nJohns Hopkins University. The CPN data analyzed in this paper are publicly\navailable in a GitHub repository.", "comment": "11 pages, 9 figures, 1 Table", "pdf_url": "http://arxiv.org/pdf/2506.23510v1", "categories": ["physics.soc-ph", "cs.SI", "stat.AP"], "cate": "physics.soc-ph", "url": "http://arxiv.org/abs/2506.23510v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23063", "title": "HF-DGF: Hybrid Feedback Guided Directed Grey-box Fuzzing", "authors": ["Guangfa Lyu", "Zhenzhong Cao", "Xiaofei Ren", "Fengyu Wang"], "summary": "Directed Grey-box Fuzzing (DGF) has emerged as a widely adopted technique for\ncrash reproduction and patch testing, leveraging its capability to precisely\nnavigate toward target locations and exploit vulnerabilities. However, current\nDGF tools are constrained by insufficient runtime feedback, limiting their\nefficiency in reaching targets and exploring state spaces. This study presents\nHF-DGF, a novel directed grey-box fuzzing framework. Its seed scheduling is\nguided by a hybrid feedback mechanism integrating control-flow distance,\nvalue-flow influence score, and slice coverage. To enable precise control-flow\ndistance feedback, we propose a backward-stepping algorithm to calculate basic\nblock-level seed distances on a virtual inter-procedural control-flow graph\n(ICFG). For effective state space exploration, we introduce value-flow\ninfluence and a corresponding metric, the value-flow influence score.\nAdditionally, to mitigate runtime overhead from hybrid feedback, we adopt a\nnovel selective instrumentation strategy. Evaluations on 41 real-world\nvulnerabilities show HF-DGF outperforms existing tools: it achieves crash\nreproduction 5.05 times faster than AFL, 5.79 times faster than AFLGo, 73.75\ntimes faster than WindRanger, 2.56 times faster than DAFL, and 8.45 times\nfaster than Beacon on average. Notably, when all fuzzers triggered crashes,\nHF-DGF exhibited the lowest code coverage, demonstrating superior\ndirectionality and efficiency. It also surpasses AFLGo, WindRanger, DAFL, and\nBeacon in static analysis efficiency.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23063v1", "categories": ["cs.SE"], "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.23063v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.22932", "title": "Immersive Technologies and Elderly Users: Current use, Limitations and Future Perspectives", "authors": ["Zoe Anastasiadou", "Andreas Lanitis"], "summary": "The increase of the percentage of elderly population in modern societies\ndictates the use of emerging technologies as a means of supporting elder\nmembers of the society. Within this scope, Extended Reality (XR) technologies\npose as a promising technology for improving the daily lives of the elderly\npopulation. This paper presents a literature review that describes the most\ncommon characteristics of the physical and mental state of the elderly,\nallowing readers, and specifically XR developers, to understand the main\ndifficulties faced by elderly users of extended reality applications so they\ncan develop accessible, user friendly and engaging applications for the target\naudience. Furthermore, a review of existing extended reality applications that\ntarget the elder population is presented, allowing readers to get acquainted\nwith existing design paradigms that can inspire future developments.", "comment": "13 pages, 2 figures", "pdf_url": "http://arxiv.org/pdf/2506.22932v1", "categories": ["cs.HC"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.22932v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23787", "title": "ISI-Aware Code Design: A Linear Approach Towards Reliable Molecular Communication", "authors": ["Tamoghno Nath", "Krishna Gopal Benerjee", "Adrish Banerjee"], "summary": "Intersymbol Interference (ISI) is a major bottleneck in Molecular\nCommunication via Diffusion (MCvD), degrading system performance. This paper\nintroduces two families of linear channel codes to mitigate ISI: Zero Pad Zero\nStart (ZPZS) and Zero Pad (ZP) codes, ensuring that each codeword avoids\nconsecutive bit-1s. The ZPZS and ZP codes are then combined to form a binary ZP\ncode, offering a higher code rate than linear ZP codes and allowing simple\ndecoding via the Majority Location Rule (MLR). Additionally, a Leading One Zero\nPad (LOZP) code is proposed, which relaxes zero-padding constraints by\nprioritizing the placement of bit-1s, achieving a higher rate than ZP. A\nclosed-form expression is derived to compute expected ISI, showing it depends\non the average bit-1 density in the codewords. ISI and Bit Error Rate (BER)\nperformance are evaluated under two MCvD channel models: (i) without refresh,\nwhere past bits persist longer, and (ii) with refresh, where the channel is\ncleared after each reception. Results show that the LOZP code performs better\nin the refresh channel due to initial bit-1 placement, while ZP excels without\nrefresh by reducing average bit-1 density. The asymptotic upper bound on code\nrate illustrates a trade-off between ISI and rate. Simulations demonstrate that\nZP and LOZP codes improve BER by controlling bit-1 positions and density,\nproviding better reliability in ISI-dominated regimes compared to conventional\nerror-correcting codes.", "comment": "23 pages, 14 figures", "pdf_url": "http://arxiv.org/pdf/2506.23787v1", "categories": ["cs.IT", "math.IT"], "cate": "cs.IT", "url": "http://arxiv.org/abs/2506.23787v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23682", "title": "Not quite a piece of CHERI-cake: Are new digital security by design architectures usable?", "authors": ["Maysara Alhindi", "Joseph Hallett"], "summary": "A digital security-by-design computer architecture, like CHERI, lets you\nprogram without fear of buffer overflows or other memory safety errors, but\nCHERI also rewrites some of the assumptions about how C works and how\nfundamental types (such as pointers) are implemented in hardware. We conducted\na usability study to examine how developers react to the changes required by\nCHERI when porting software to run on it. We find that developers struggle with\nCHERI's display of warnings and errors and a lack of diverse documentation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23682v1", "categories": ["cs.CR", "cs.AR", "cs.HC"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.23682v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.24045", "title": "Agent.xpu: Efficient Scheduling of Agentic LLM Workloads on Heterogeneous SoC", "authors": ["Xinming Wei", "Jiahao Zhang", "Haoran Li", "Jiayu Chen", "Rui Qu", "Maoliang Li", "Xiang Chen", "Guojie Luo"], "summary": "The proliferation of agentic Large Language Models (LLMs) on personal devices\nintroduces a new class of workloads characterized by a dichotomy of objectives.\nReactive tasks, initiated by users, demand immediate, low-latency responses,\nwhile proactive tasks operate invisibly and prioritize throughput. Existing\non-device LLM engines, designed for isolated inferences, fail to efficiently\nmanage these concurrent and conflicting requests on consumer-grade\nheterogeneous SoCs with CPU, integrated GPU, and NPU. This paper introduces\nAgent.xpu, an efficient serving system for agentic LLM workloads on\nmemory-unified heterogeneous SoCs. With dedicated offline profiling, Agent.xpu\nfirst constructs a heterogeneous execution graph, which fuses and chunks model\nkernels for affinity-guided, elastic accelerator mapping with predictive kernel\nannotation. At runtime, its online scheduler enables fine-grained, kernel-level\npreemption to guarantee the responsiveness of reactive tasks. To maximize SoC\nutilization, it adopts slack-aware kernel backfill to opportunistically append\nproactive tasks, and mitigates NPU-iGPU contention via bandwidth-aware\ndispatch. Evaluation on an Intel Core Ultra SoC shows that Agent.xpu achieves\n4.6$\\times$ lower latency for reactive tasks and sustains\n1.6$\\times$-6.8$\\times$ higher throughput for proactive tasks compared to\nstate-of-the-art inference engines.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.24045v1", "categories": ["cs.DC", "cs.LG"], "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.24045v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22523", "title": "Red Teaming for Generative AI, Report on a Copyright-Focused Exercise Completed in an Academic Medical Center", "authors": ["James Wen", "Sahil Nalawade", "Zhiwei Liang", "Catherine Bielick", "Marisa Ferrara Boston", "Alexander Chowdhury", "Adele Collin", "Luigi De Angelis", "Jacob Ellen", "Heather Frase", "Rodrigo R. Gameiro", "Juan Manuel Gutierrez", "Pooja Kadam", "Murat Keceli", "Srikanth Krishnamurthy", "Anne Kwok", "Yanan Lance Lu", "Heather Mattie", "Liam G. McCoy", "Katherine Miller", "Allison C. Morgan", "Marlene Louisa Moerig", "Trang Nguyen", "Alexander Owen-Post", "Alex D. Ruiz", "Sreekar Reddy Puchala", "Soujanya Samineni", "Takeshi Tohyama", "Varun Ullanat", "Carmine Valenza", "Camilo Velez", "Pengcheng Wang", "Anna Wuest", "Yuxiang Zhou", "Yingde Zhu", "Jason M. Johnson", "Jennifer Willcox", "Francis J. Vitiello", "Leo Anthony G. Celi", "Renato Umeton"], "summary": "Generative AI is present in multiple industries. Dana-Farber Cancer\nInstitute, in partnership with Microsoft, has created an internal AI tool,\nGPT4DFCI. Together we hosted a red teaming event to assess whether the\nunderlying GPT models that support the tool would output copyrighted data. Our\nteams focused on reproducing content from books, news articles, scientific\narticles, and electronic health records. We found isolated instances where\nGPT4DFCI was able to identify copyrighted material and reproduce exact quotes\nfrom famous books which indicates that copyrighted material was in the training\ndata. The model was not able to reproduce content from our target news article,\nscientific article, or electronic health records. However, there were instances\nof fabrication. As a result of this event, a mitigation strategy is in\nproduction in GPT4DFCI v2.8.2, deployed on January 21, 2025. We hope this\nreport leads to similar events in which AI software tools are stress-tested to\nassess the perimeter of their legal and ethical usage.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22523v1", "categories": ["cs.CY", "cs.AI"], "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.22523v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.22504", "title": "Patch2Loc: Learning to Localize Patches for Unsupervised Brain Lesion Detection", "authors": ["Hassan Baker", "Austin J. Brockmeier"], "summary": "Detecting brain lesions as abnormalities observed in magnetic resonance\nimaging (MRI) is essential for diagnosis and treatment. In the search of\nabnormalities, such as tumors and malformations, radiologists may benefit from\ncomputer-aided diagnostics that use computer vision systems trained with\nmachine learning to segment normal tissue from abnormal brain tissue. While\nsupervised learning methods require annotated lesions, we propose a new\nunsupervised approach (Patch2Loc) that learns from normal patches taken from\nstructural MRI. We train a neural network model to map a patch back to its\nspatial location within a slice of the brain volume. During inference, abnormal\npatches are detected by the relatively higher error and/or variance of the\nlocation prediction. This generates a heatmap that can be integrated into\npixel-wise methods to achieve finer-grained segmentation. We demonstrate the\nability of our model to segment abnormal brain tissues by applying our approach\nto the detection of tumor tissues in MRI on T2-weighted images from BraTS2021\nand MSLUB datasets and T1-weighted images from ATLAS and WMH datasets. We show\nthat it outperforms the state-of-the art in unsupervised segmentation. The\ncodebase for this work can be found on our\n\\href{https://github.com/bakerhassan/Patch2Loc}{GitHub page}.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22504v1", "categories": ["cs.CV", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22504v1", "date": "2025-06-25", "updated": "2025-06-25"}
{"id": "2506.23545", "title": "Immersive Technologies in Training and Healthcare: From Space Missions to Psychophysiological Research", "authors": ["Barbara Karpowicz", "Maciej Grzeszczuk", "Adam Kuzdrali≈Ñski", "Monika Kornacka", "Aliaksandr Marozau", "Wiktor Stawski", "Pavlo Zinevych", "Grzegorz Marcin W√≥jcik", "Tomasz Kowalewski", "Grzegorz Pochwatko", "Wies≈Çaw Kopeƒá"], "summary": "Virtual, Augmented, and eXtended Reality (VR/AR/XR) technologies are\nincreasingly recognized for their applications in training, diagnostics, and\npsychological research, particularly in high-risk and highly regulated\nenvironments. In this panel we discuss how immersive systems enhance human\nperformance across multiple domains, including clinical psychology, space\nexploration, and medical education. In psychological research and training, XR\ncan offer a controlled yet ecologically valid setting for measuring cognitive\nand affective processes. In space exploration, we discuss the development of\nVR-based astronaut training and diagnostic systems, allowing astronauts to\nperform real-time health assessments. In medical education and rehabilitation,\nwe cover procedural training and patient engagement. From virtual surgical\nsimulations to gamified rehabilitation exercises, immersive environments\nenhance both learning outcomes and treatment adherence.", "comment": "8 pages, 1 figure", "pdf_url": "http://arxiv.org/pdf/2506.23545v1", "categories": ["cs.HC", "cs.CE"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.23545v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22931", "title": "Real-Time Energy Management Strategies for Community Microgrids", "authors": ["Moslem Uddin", "Huadong Mo", "Daoyi Dong"], "summary": "This study presents a real-time energy management framework for hybrid\ncommunity microgrids integrating photovoltaic, wind, battery energy storage\nsystems, diesel generators, and grid interconnection. The proposed approach\nformulates the dispatch problem as a multi-objective optimization task that\naims to minimize operational costs. Two control strategies are proposed and\nevaluated: a conventional rule-based control (RBC) method and an advanced deep\nreinforcement learning (DRL) approach utilizing proximal policy optimization\n(PPO). A realistic case study based on Australian load and generation profiles\nis used to validate the framework. Simulation results demonstrate that DRL-PPO\nreduces operational costs by 18%, CO_2 emissions by 20%, and improves system\nreliability by 87.5% compared to RBC. Beside, DRL-PPO increases renewable\nenergy utilization by 13%, effectively reducing dependence on diesel generation\nand grid imports. These findings demonstrate the potential of DRL-based\napproaches to enable cost-effective and resilient microgrid operations,\nparticularly in regional and remote communities.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22931v1", "categories": ["eess.SY", "cs.SY"], "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.22931v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.22461", "title": "Machine Learning for Proactive Groundwater Management: Early Warning and Resource Allocation", "authors": ["Chuan Li", "Ruoxuan Yang"], "summary": "Groundwater supports ecosystems, agriculture, and drinking water supplies\nworldwide, yet effective monitoring remains challenging due to sparse data,\ncomputational constraints, and delayed outputs from traditional approaches. We\ndevelop a machine learning pipeline that predicts groundwater level categories\nusing climate data, hydro-meteorological records, and physiographic attributes\nprocessed through AutoGluon's automated ensemble framework. Our approach\nintegrates geospatial preprocessing, domain-driven feature engineering, and\nautomated model selection to overcome conventional monitoring limitations.\nApplied to a large-scale French dataset (n $>$ 3,440,000 observations from\n1,500+ wells), the model achieves weighted F\\_1 scores of 0.927 on validation\ndata and 0.67 on temporally distinct test data. Scenario-based evaluations\ndemonstrate practical utility for early warning systems and water allocation\ndecisions under changing climate conditions. The open-source implementation\nprovides a scalable framework for integrating machine learning into national\ngroundwater monitoring networks, enabling more responsive and data-driven water\nmanagement strategies.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22461v1", "categories": ["eess.SP", "cs.AI"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.22461v1", "date": "2025-06-18", "updated": "2025-06-18"}
{"id": "2506.23102", "title": "MedRegion-CT: Region-Focused Multimodal LLM for Comprehensive 3D CT Report Generation", "authors": ["Sunggu Kyung", "Jinyoung Seo", "Hyunseok Lim", "Dongyeong Kim", "Hyungbin Park", "Jimin Sung", "Jihyun Kim", "Wooyoung Jo", "Yoojin Nam", "Namkug Kim"], "summary": "The recent release of RadGenome-Chest CT has significantly advanced CT-based\nreport generation. However, existing methods primarily focus on global\nfeatures, making it challenging to capture region-specific details, which may\ncause certain abnormalities to go unnoticed. To address this, we propose\nMedRegion-CT, a region-focused Multi-Modal Large Language Model (MLLM)\nframework, featuring three key innovations. First, we introduce Region\nRepresentative ($R^2$) Token Pooling, which utilizes a 2D-wise pretrained\nvision model to efficiently extract 3D CT features. This approach generates\nglobal tokens representing overall slice features and region tokens\nhighlighting target areas, enabling the MLLM to process comprehensive\ninformation effectively. Second, a universal segmentation model generates\npseudo-masks, which are then processed by a mask encoder to extract\nregion-centric features. This allows the MLLM to focus on clinically relevant\nregions, using six predefined region masks. Third, we leverage segmentation\nresults to extract patient-specific attributions, including organ size,\ndiameter, and locations. These are converted into text prompts, enriching the\nMLLM's understanding of patient-specific contexts. To ensure rigorous\nevaluation, we conducted benchmark experiments on report generation using the\nRadGenome-Chest CT. MedRegion-CT achieved state-of-the-art performance,\noutperforming existing methods in natural language generation quality and\nclinical relevance while maintaining interpretability. The code for our\nframework is publicly available.", "comment": "14 pages, 5 figures, submitted to ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2506.23102v1", "categories": ["eess.IV", "cs.CV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.23102v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23777", "title": "Synthetically Expressive: Evaluating gesture and voice for emotion and empathy in VR and 2D scenarios", "authors": ["Haoyang Du", "Kiran Chhatre", "Christopher Peters", "Brian Keegan", "Rachel McDonnell", "Cathy Ennis"], "summary": "The creation of virtual humans increasingly leverages automated synthesis of\nspeech and gestures, enabling expressive, adaptable agents that effectively\nengage users. However, the independent development of voice and gesture\ngeneration technologies, alongside the growing popularity of virtual reality\n(VR), presents significant questions about the integration of these signals and\ntheir ability to convey emotional detail in immersive environments. In this\npaper, we evaluate the influence of real and synthetic gestures and speech,\nalongside varying levels of immersion (VR vs. 2D displays) and emotional\ncontexts (positive, neutral, negative) on user perceptions. We investigate how\nimmersion affects the perceived match between gestures and speech and the\nimpact on key aspects of user experience, including emotional and empathetic\nresponses and the sense of co-presence. Our findings indicate that while VR\nenhances the perception of natural gesture-voice pairings, it does not\nsimilarly improve synthetic ones - amplifying the perceptual gap between them.\nThese results highlight the need to reassess gesture appropriateness and refine\nAI-driven synthesis for immersive environments. See video:\nhttps://youtu.be/WMfjIB1X-dc", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23777v1", "categories": ["cs.GR"], "cate": "cs.GR", "url": "http://arxiv.org/abs/2506.23777v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23442", "title": "Efficient Resource Allocation under Adversary Attacks: A Decomposition-Based Approach", "authors": ["Mansoor Davoodi", "Setareh Maghsudi"], "summary": "We address the problem of allocating limited resources in a network under\npersistent yet statistically unknown adversarial attacks. Each node in the\nnetwork may be degraded, but not fully disabled, depending on its available\ndefensive resources. The objective is twofold: to minimize total system damage\nand to reduce cumulative resource allocation and transfer costs over time. We\nmodel this challenge as a bi-objective optimization problem and propose a\ndecomposition-based solution that integrates chance-constrained programming\nwith network flow optimization. The framework separates the problem into two\ninterrelated subproblems: determining optimal node-level allocations across\ntime slots, and computing efficient inter-node resource transfers. We\ntheoretically prove the convergence of our method to the optimal solution that\nwould be obtained with full statistical knowledge of the adversary. Extensive\nsimulations demonstrate that our method efficiently learns the adversarial\npatterns and achieves substantial gains in minimizing both damage and\noperational costs, comparing three benchmark strategies under various parameter\nsettings.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23442v1", "categories": ["cs.DS"], "cate": "cs.DS", "url": "http://arxiv.org/abs/2506.23442v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22530", "title": "Task-Agnostic Contrastive Pretraining for Relational Deep Learning", "authors": ["Jakub Pele≈°ka", "Gustav ≈†√≠r"], "summary": "Relational Deep Learning (RDL) is an emerging paradigm that leverages Graph\nNeural Network principles to learn directly from relational databases by\nrepresenting them as heterogeneous graphs. However, existing RDL models\ntypically rely on task-specific supervised learning, requiring training\nseparate models for each predictive task, which may hamper scalability and\nreuse.\n  In this work, we propose a novel task-agnostic contrastive pretraining\napproach for RDL that enables database-wide representation learning. For that\naim, we introduce three levels of contrastive objectives$-$row-level,\nlink-level, and context-level$-$designed to capture the structural and semantic\nheterogeneity inherent to relational data. We implement the respective\npretraining approach through a modular RDL architecture and an efficient\nsampling strategy tailored to the heterogeneous database setting. Our\npreliminary results on standard RDL benchmarks demonstrate that fine-tuning the\npretrained models measurably outperforms training from scratch, validating the\npromise of the proposed methodology in learning transferable representations\nfor relational data.", "comment": "arXiv admin note: text overlap with arXiv:2506.22199", "pdf_url": "http://arxiv.org/pdf/2506.22530v1", "categories": ["cs.LG", "cs.DB"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22530v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22920", "title": "Improving Rationality in the Reasoning Process of Language Models through Self-playing Game", "authors": ["Pinzheng Wang", "Juntao Li", "Zecheng Tang", "Haijia Gui", "Min zhang"], "summary": "Large language models (LLMs) have demonstrated considerable reasoning\nabilities in various tasks such as mathematics and coding. However, recent\nstudies indicate that even the best models lack true comprehension of their\nreasoning processes. In this paper, we explore how self-play can enhance the\nrationality of models in the reasoning process without supervision from humans\nor superior models. We design a Critic-Discernment Game(CDG) in which a prover\nfirst provides a solution to a given problem and is subsequently challenged by\ncritiques of its solution. These critiques either aim to assist or mislead the\nprover. The objective of the prover is to maintain the correct answer when\nfaced with misleading comments, while correcting errors in response to\nconstructive feedback. Our experiments on tasks involving mathematical\nreasoning, stepwise error detection, self-correction, and long-chain reasoning\ndemonstrate that CDG training can significantly improve the ability of\nwell-aligned LLMs to comprehend their reasoning process.", "comment": "Accepted by ICML 2025", "pdf_url": "http://arxiv.org/pdf/2506.22920v1", "categories": ["cs.AI"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.22920v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23394", "title": "Teaching a Language Model to Speak the Language of Tools", "authors": ["Simeon Emanuilov"], "summary": "External tool integration through function-calling is essential for practical\nlanguage model applications, yet most multilingual models lack reliable\ntool-use capabilities in non-English languages. Even state-of-the-art\nmultilingual models struggle with determining when to use tools and generating\nthe structured outputs required for function calls, often exhibiting language\nconfusion when prompted in lower-resource languages. This work presents a\nmethodology for adapting existing language models to enable robust tool use in\nany target language, using Bulgarian as a case study. The approach involves\ncontinued training of the BgGPT model series (2.6B, 9B, 27B parameters) on a\nnovel bilingual dataset of 10,035 function-calling examples designed to support\nstandardized protocols like MCP (Model Context Protocol). The research\nintroduces TUCAN (Tool-Using Capable Assistant Navigator), which achieves up to\n28.75% improvement in function-calling accuracy over base models while\npreserving core language understanding, as verified on established Bulgarian\nbenchmarks. Beyond accuracy gains, TUCAN models demonstrate production-ready\nresponse formatting with clean, parsable function calls, contrasting with the\nverbose and inconsistent outputs of base models. The models, evaluation\nframework, and dataset are released to enable replication for other languages.\nThis work demonstrates a practical approach for extending tool-augmented\ncapabilities beyond English-centric systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23394v1", "categories": ["cs.IR", "cs.AI", "cs.CL", "I.2.7; I.2.1"], "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.23394v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.22789", "title": "WavShape: Information-Theoretic Speech Representation Learning for Fair and Privacy-Aware Audio Processing", "authors": ["Oguzhan Baser", "Ahmet Ege Tanriverdi", "Kaan Kale", "Sandeep P. Chinchali", "Sriram Vishwanath"], "summary": "Speech embeddings often retain sensitive attributes such as speaker identity,\naccent, or demographic information, posing risks in biased model training and\nprivacy leakage. We propose WavShape, an information-theoretic speech\nrepresentation learning framework that optimizes embeddings for fairness and\nprivacy while preserving task-relevant information. We leverage mutual\ninformation (MI) estimation using the Donsker-Varadhan formulation to guide an\nMI-based encoder that systematically filters sensitive attributes while\nmaintaining speech content essential for downstream tasks. Experimental results\non three known datasets show that WavShape reduces MI between embeddings and\nsensitive attributes by up to 81% while retaining 97% of task-relevant\ninformation. By integrating information theory with self-supervised speech\nmodels, this work advances the development of fair, privacy-aware, and\nresource-efficient speech systems.", "comment": "5 pages, 4 figures, Published at The Proceedings of Interspeech 2025,\n  code is available at http://www.github.com/UTAustin-SwarmLab/WavShape", "pdf_url": "http://arxiv.org/pdf/2506.22789v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.22789v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.22529", "title": "MisinfoTeleGraph: Network-driven Misinformation Detection for German Telegram Messages", "authors": ["Lu Kalkbrenner", "Veronika Solopova", "Steffen Zeiler", "Robert Nickel", "Dorothea Kolossa"], "summary": "Connectivity and message propagation are central, yet often underutilized,\nsources of information in misinformation detection -- especially on poorly\nmoderated platforms such as Telegram, which has become a critical channel for\nmisinformation dissemination, namely in the German electoral context. In this\npaper, we introduce Misinfo-TeleGraph, the first German-language Telegram-based\ngraph dataset for misinformation detection. It includes over 5 million messages\nfrom public channels, enriched with metadata, channel relationships, and both\nweak and strong labels. These labels are derived via semantic similarity to\nfact-checks and news articles using M3-embeddings, as well as manual\nannotation. To establish reproducible baselines, we evaluate both text-only\nmodels and graph neural networks (GNNs) that incorporate message forwarding as\na network structure. Our results show that GraphSAGE with LSTM aggregation\nsignificantly outperforms text-only baselines in terms of Matthews Correlation\nCoefficient (MCC) and F1-score. We further evaluate the impact of subscribers,\nview counts, and automatically versus human-created labels on performance, and\nhighlight both the potential and challenges of weak supervision in this domain.\nThis work provides a reproducible benchmark and open dataset for future\nresearch on misinformation detection in German-language Telegram networks and\nother low-moderation social platforms.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22529v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.22529v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22912", "title": "A Dilation-based Seamless Multiscale Method For Elliptic Problems", "authors": ["Ziheng Chen", "Bj√∂rn Engquist"], "summary": "Many numerical methods for multiscale differential equations require a scale\nseparation between the larger and the smaller scales to achieve accuracy and\ncomputational efficiency. In the area of multiscale dynamical systems,\nso-called, seamless methods have been introduced to reduce the requirement of\nscale separation. We will translate these methods to numerical homogenization\nproblems and extend the technique to multiple dimensions. The initial step is\nto prove that a one-dimensional \\sepia{second-order} elliptic operator with\noscillatory coefficients can be rewritten as a multiscale dynamical system.\nInspired by this, multiscale elliptic operators in higher dimensions are\napproximated by a novel approach based on local dilation, which provides a\nmiddle ground for balancing intractability and accuracy without the need for\nfull resolution. The dilation operator can be further generalized to preserve\nimportant structures by properly decomposing the coefficient field. Error\nestimates are developed and promising numerical results of different examples\nare included.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22912v1", "categories": ["math.NA", "cs.NA", "74Q05, 35B27, 65N30, 65N06, 74Q15"], "cate": "math.NA", "url": "http://arxiv.org/abs/2506.22912v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23546", "title": "Neural Langevin Machine: a local asymmetric learning rule can be creative", "authors": ["Zhendong Yu", "Weizhong Huang", "Haiping Huang"], "summary": "Fixed points of recurrent neural networks can be leveraged to store and\ngenerate information. These fixed points can be captured by the Boltzmann-Gibbs\nmeasure, which leads to neural Langevin dynamics that can be used for sampling\nand learning a real dataset. We call this type of generative model neural\nLangevin machine, which is interpretable due to its analytic form of\ndistribution and is simple to train. Moreover, the learning process is derived\nas a local asymmetric plasticity rule, bearing biological relevance. Therefore,\none can realize a continuous sampling of creative dynamics in a neural network,\nmimicking an imagination process in brain circuits. This neural Langevin\nmachine may be another promising generative model, at least in its strength in\ncircuit-based sampling and biologically plausible learning rule.", "comment": "15 pages, 3 figures, with Github link in the paper", "pdf_url": "http://arxiv.org/pdf/2506.23546v1", "categories": ["q-bio.NC", "cond-mat.dis-nn", "cs.LG", "cs.NE"], "cate": "q-bio.NC", "url": "http://arxiv.org/abs/2506.23546v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23437", "title": "From Large-scale Audio Tagging to Real-Time Explainable Emergency Vehicle Sirens Detection", "authors": ["Stefano Giacomelli", "Marco Giordano", "Claudia Rinaldi", "Fabio Graziosi"], "summary": "Accurate recognition of Emergency Vehicle (EV) sirens is critical for the\nintegration of intelligent transportation systems, smart city monitoring\nsystems, and autonomous driving technologies. Modern automatic solutions are\nlimited by the lack of large scale, curated datasets and by the computational\ndemands of state of the art sound event detection models. This work introduces\nE2PANNs (Efficient Emergency Pre trained Audio Neural Networks), a lightweight\nConvolutional Neural Network architecture derived from the PANNs framework,\nspecifically optimized for binary EV siren detection. Leveraging our dedicated\nsubset of AudioSet (AudioSet EV) we fine-tune and evaluate E2PANNs across\nmultiple reference datasets and test its viability on embedded hardware. The\nexperimental campaign includes ablation studies, cross-domain benchmarking, and\nreal-time inference deployment on edge device. Interpretability analyses\nexploiting Guided Backpropagation and ScoreCAM algorithms provide insights into\nthe model internal representations and validate its ability to capture distinct\nspectrotemporal patterns associated with different types of EV sirens. Real\ntime performance is assessed through frame wise and event based detection\nmetrics, as well as a detailed analysis of false positive activations. Results\ndemonstrate that E2PANNs establish a new state of the art in this research\ndomain, with high computational efficiency, and suitability for edge-based\naudio monitoring and safety-critical applications.", "comment": "pre-print (submitted to the IEEE/ACM Transactions on Audio, Speech,\n  and Language Processing)", "pdf_url": "http://arxiv.org/pdf/2506.23437v1", "categories": ["cs.SD", "cs.AI", "eess.AS", "68T07", "E.1; H.1; I.2; I.5; J.2; K.4; C.4"], "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.23437v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23967", "title": "Green Metrics Tool: Measuring for fun and profit", "authors": ["Geerd-Dietger Hoffmann", "Verena Majuntke"], "summary": "The environmental impact of software is gaining increasing attention as the\ndemand for computational resources continues to rise. In order to optimize\nsoftware resource consumption and reduce carbon emissions, measuring and\nevaluating software is a first essential step. In this paper we discuss what\nmetrics are important for fact base decision making. We introduce the Green\nMetrics Tool (GMT), a novel framework for accurately measuring the resource\nconsumption of software. The tool provides a containerized, controlled, and\nreproducible life cycle-based approach, assessing the resource use of software\nduring key phases. Finally, we discuss GMT features like visualization,\ncomparability and rule- and LLM-based optimisations highlighting its potential\nto guide developers and researchers in reducing the environmental impact of\ntheir software.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23967v1", "categories": ["cs.SE", "cs.CY", "cs.ET"], "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.23967v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22750", "title": "Enhancing Android Malware Detection with Retrieval-Augmented Generation", "authors": ["Saraga S.", "Anagha M. S.", "Dincy R. Arikkat", "Rafidha Rehiman K. A.", "Serena Nicolazzo", "Antonino Nocera", "Vinod P"], "summary": "The widespread use of Android applications has made them a prime target for\ncyberattacks, significantly increasing the risk of malware that threatens user\nprivacy, security, and device functionality. Effective malware detection is\nthus critical, with static analysis, dynamic analysis, and Machine Learning\nbeing widely used approaches. In this work, we focus on a Machine\nLearning-based method utilizing static features. We first compiled a dataset of\nbenign and malicious APKs and performed static analysis to extract features\nsuch as code structure, permissions, and manifest file content, without\nexecuting the apps. Instead of relying solely on raw static features, our\nsystem uses an LLM to generate high-level functional descriptions of APKs. To\nmitigate hallucinations, which are a known vulnerability of LLM, we integrated\nRetrieval-Augmented Generation (RAG), enabling the LLM to ground its output in\nrelevant context. Using carefully designed prompts, we guide the LLM to produce\ncoherent function summaries, which are then analyzed using a transformer-based\nmodel, improving detection accuracy over conventional feature-based methods for\nmalware detection.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22750v1", "categories": ["cs.CR"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.22750v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.22991", "title": "Resilient-Native and Intelligent Next-Generation Wireless Systems: Key Enablers, Foundations, and Applications", "authors": ["Mehdi Bennis", "Sumudu Samarakoon", "Tamara Alshammari", "Chathuranga Weeraddana", "Zhoujun Tian", "Chaouki Ben Issaid"], "summary": "Just like power, water, and transportation systems, wireless networks are a\ncrucial societal infrastructure. As natural and human-induced disruptions\ncontinue to grow, wireless networks must be resilient. This requires them to\nwithstand and recover from unexpected adverse conditions, shocks, unmodeled\ndisturbances and cascading failures. Unlike robustness and reliability,\nresilience is based on the understanding that disruptions will inevitably\nhappen. Resilience, as elasticity, focuses on the ability to bounce back to\nfavorable states, while resilience as plasticity involves agents and networks\nthat can flexibly expand their states and hypotheses through real-time\nadaptation and reconfiguration. This situational awareness and active\npreparedness, adapting world models and counterfactually reasoning about\npotential system failures and the best responses, is a core aspect of\nresilience. This article will first disambiguate resilience from reliability\nand robustness, before delving into key mathematical foundations of resilience\ngrounded in abstraction, compositionality and emergence. Subsequently, we focus\nour attention on a plethora of techniques and methodologies pertaining to the\nunique characteristics of resilience, as well as their applications through a\ncomprehensive set of use cases. Ultimately, the goal of this paper is to\nestablish a unified foundation for understanding, modeling, and engineering\nresilience in wireless communication systems, while laying a roadmap for the\nnext-generation of resilient-native and intelligent wireless systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22991v1", "categories": ["cs.NI", "cs.LO", "cs.MA", "cs.SY", "eess.SY"], "cate": "cs.NI", "url": "http://arxiv.org/abs/2506.22991v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.22894", "title": "Safe Reinforcement Learning with a Predictive Safety Filter for Motion Planning and Control: A Drifting Vehicle Example", "authors": ["Bei Zhou", "Baha Zarrouki", "Mattia Piccinini", "Cheng Hu", "Lei Xie", "Johannes Betz"], "summary": "Autonomous drifting is a complex and crucial maneuver for safety-critical\nscenarios like slippery roads and emergency collision avoidance, requiring\nprecise motion planning and control. Traditional motion planning methods often\nstruggle with the high instability and unpredictability of drifting,\nparticularly when operating at high speeds. Recent learning-based approaches\nhave attempted to tackle this issue but often rely on expert knowledge or have\nlimited exploration capabilities. Additionally, they do not effectively address\nsafety concerns during learning and deployment. To overcome these limitations,\nwe propose a novel Safe Reinforcement Learning (RL)-based motion planner for\nautonomous drifting. Our approach integrates an RL agent with model-based drift\ndynamics to determine desired drift motion states, while incorporating a\nPredictive Safety Filter (PSF) that adjusts the agent's actions online to\nprevent unsafe states. This ensures safe and efficient learning, and stable\ndrift operation. We validate the effectiveness of our method through\nsimulations on a Matlab-Carsim platform, demonstrating significant improvements\nin drift performance, reduced tracking errors, and computational efficiency\ncompared to traditional methods. This strategy promises to extend the\ncapabilities of autonomous vehicles in safety-critical maneuvers.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22894v1", "categories": ["cs.RO"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.22894v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.22671", "title": "Towards an Optimized Multi-Cyclic Queuing and Forwarding in Time Sensitive Networking with Time Injection", "authors": ["Rubi Debnath", "Mohammadreza Barzegaran", "Sebastian Steinhorst"], "summary": "Cyclic Queuing and Forwarding (CQF) is a Time-Sensitive Networking (TSN)\nshaping mechanism that provides bounded latency and deterministic Quality of\nService (QoS). However, CQF's use of a single cycle restricts its ability to\nsupport TSN traffic with diverse timing requirements. Multi-Cyclic Queuing and\nForwarding (Multi-CQF) is a new and emerging TSN shaping mechanism that uses\nmultiple cycles on the same egress port, allowing it to accommodate TSN flows\nwith varied timing requirements more effectively than CQF. Despite its\npotential, current Multi-CQF configuration studies are limited, leading to a\nlack of comprehensive research, poor understanding of the mechanism, and\nlimited adoption of Multi-CQF in practical applications. Previous work has\nshown the impact of Time Injection (TI), defined as the start time of\nTime-Triggered (TT) flows at the source node, on CQF queue resource\nutilization. However, the impact of TI has not yet been explored in the context\nof Multi-CQF. This paper introduces a set of constraints and leverages Domain\nSpecific Knowledge (DSK) to reduce the search space for Multi-CQF\nconfiguration. Building on this foundation, we develop an open-source Genetic\nAlgorithm (GA) and a hybrid GA-Simulated Annealing (GASA) approach to\nefficiently configure Multi-CQF networks and introduce TI in Multi-CQF to\nenhance schedulability. Experimental results show that our proposed algorithms\nsignificantly increase the number of scheduled TT flows compared to the\nbaseline Simulated Annealing (SA) model, improving scheduling by an average of\n15%. Additionally, GASA achieves a 20% faster convergence rate and lower time\ncomplexity, outperforming the SA model in speed, and efficiency.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22671v1", "categories": ["cs.NI", "cs.ET"], "cate": "cs.NI", "url": "http://arxiv.org/abs/2506.22671v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.23978", "title": "LLM Agents Are the Antidote to Walled Gardens", "authors": ["Samuele Marro", "Philip Torr"], "summary": "While the Internet's core infrastructure was designed to be open and\nuniversal, today's application layer is dominated by closed, proprietary\nplatforms. Open and interoperable APIs require significant investment, and\nmarket leaders have little incentive to enable data exchange that could erode\ntheir user lock-in. We argue that LLM-based agents fundamentally disrupt this\nstatus quo. Agents can automatically translate between data formats and\ninteract with interfaces designed for humans: this makes interoperability\ndramatically cheaper and effectively unavoidable. We name this shift universal\ninteroperability: the ability for any two digital services to exchange data\nseamlessly using AI-mediated adapters. Universal interoperability undermines\nmonopolistic behaviours and promotes data portability. However, it can also\nlead to new security risks and technical debt. Our position is that the ML\ncommunity should embrace this development while building the appropriate\nframeworks to mitigate the downsides. By acting now, we can harness AI to\nrestore user freedom and competitive markets without sacrificing security.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23978v1", "categories": ["cs.LG", "cs.CL", "cs.CY", "cs.SI", "68T50, 68M10, 91B26", "I.2.11; I.2.7; H.4.5"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.23978v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23100", "title": "Repair Ingredients Are All You Need: Improving Large Language Model-Based Program Repair via Repair Ingredients Search", "authors": ["Jiayi Zhang", "Kai Huang", "Jian Zhang", "Yang Liu", "Chunyang Chen"], "summary": "Automated Program Repair (APR) techniques aim to automatically fix buggy\nprograms. Among these, Large Language Model-based (LLM-based) approaches have\nshown great promise. Recent advances demonstrate that directly leveraging LLMs\ncan achieve leading results. However, these techniques remain suboptimal in\ngenerating contextually relevant and accurate patches, as they often overlook\nrepair ingredients crucial for practical program repair. In this paper, we\npropose ReinFix, a novel framework that enables LLMs to autonomously search for\nrepair ingredients throughout both the reasoning and solution phases of bug\nfixing. In the reasoning phase, ReinFix integrates static analysis tools to\nretrieve internal ingredients, such as variable definitions, to assist the LLM\nin root cause analysis when it encounters difficulty understanding the context.\nDuring the solution phase, when the LLM lacks experience in fixing specific\nbugs, ReinFix searches for external ingredients from historical bug fixes with\nsimilar bug patterns, leveraging both the buggy code and its root cause to\nguide the LLM in identifying appropriate repair actions, thereby increasing the\nlikelihood of generating correct patches. Evaluations on two popular benchmarks\n(Defects4J V1.2 and V2.0) demonstrate the effectiveness of our approach over\nSOTA baselines. Notably, ReinFix fixes 146 bugs, which is 32 more than the\nbaselines on Defects4J V1.2. On Defects4J V2.0, ReinFix fixes 38 more bugs than\nthe SOTA. Importantly, when evaluating on the recent benchmarks that are free\nof data leakage risk, ReinFix also maintains the best performance.", "comment": "Accepted by ICSE 2026. Jiayi Zhang and Kai Huang contributed equally\n  to this work", "pdf_url": "http://arxiv.org/pdf/2506.23100v1", "categories": ["cs.SE"], "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.23100v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.22937", "title": "GamerAstra: Enhancing Video Game Accessibility for Blind and Low-Vision Players through a Multi-Agent AI Framework", "authors": ["Tianrun Qiu", "Changxin Chen", "Sizhe Cheng", "Yiming Yang", "Yixiao Guo", "Zhicong Lu", "Yuxin Ma"], "summary": "Blind and low-vision (BLV) players encounter critical challenges in engaging\nwith video games due to the inaccessibility of visual elements, difficulties in\nnavigating interfaces, and limitations in sending interaction input. Moreover,\nthe development of specialized accessibility features typically requires\nsubstantial programming effort and is often implemented on a game-by-game\nbasis. To address these challenges, we introduce \\textit{GamerAstra}, a\ngeneralized accessibility framework that leverages a multi-agent design to\nfacilitate access to video games for BLV players. It integrates multi-modal\ntechniques including large language models and vision-language models, enabling\ninteraction with games lacking native accessibility support. The framework\nfurther incorporates customizable assistance granularities to support varying\ndegrees of visual impairment and enhances interface navigation through multiple\ninput modalities. The evaluation through technical assessments and user studies\nindicate that \\textit{GamerAstra} effectively enhances playability and delivers\na more immersive gaming experience for BLV players. These findings also\nunderscore potential avenues for advancing intelligent accessibility frameworks\nin the gaming domain.", "comment": "19 pages, 9 figures", "pdf_url": "http://arxiv.org/pdf/2506.22937v1", "categories": ["cs.HC", "H.5.2"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.22937v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.24009", "title": "Bridging Physical and Digital Worlds: Embodied Large AI for Future Wireless Systems", "authors": ["Xinquan Wang", "Fenghao Zhu", "Zhaohui Yang", "Chongwen Huang", "Xiaoming Chen", "Zhaoyang Zhang", "Sami Muhaidat", "M√©rouane Debbah"], "summary": "Large artificial intelligence (AI) models offer revolutionary potential for\nfuture wireless systems, promising unprecedented capabilities in network\noptimization and performance. However, current paradigms largely overlook\ncrucial physical interactions. This oversight means they primarily rely on\noffline datasets, leading to difficulties in handling real-time wireless\ndynamics and non-stationary environments. Furthermore, these models often lack\nthe capability for active environmental probing. This paper proposes a\nfundamental paradigm shift towards wireless embodied large AI (WELAI), moving\nfrom passive observation to active embodiment. We first identify key challenges\nfaced by existing models, then we explore the design principles and system\nstructure of WELAI. Besides, we outline prospective applications in\nnext-generation wireless. Finally, through an illustrative case study, we\ndemonstrate the effectiveness of WELAI and point out promising research\ndirections for realizing adaptive, robust, and autonomous wireless systems.", "comment": "7 pages, 4 figures", "pdf_url": "http://arxiv.org/pdf/2506.24009v1", "categories": ["cs.IT", "cs.AI", "math.IT"], "cate": "cs.IT", "url": "http://arxiv.org/abs/2506.24009v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22480", "title": "Service Placement in Small Cell Networks Using Distributed Best Arm Identification in Linear Bandits", "authors": ["Mariam Yahya", "Aydin Sezgin", "Setareh Maghsudi"], "summary": "As users in small cell networks increasingly rely on computation-intensive\nservices, cloud-based access often results in high latency. Multi-access edge\ncomputing (MEC) mitigates this by bringing computational resources closer to\nend users, with small base stations (SBSs) serving as edge servers to enable\nlow-latency service delivery. However, limited edge capacity makes it\nchallenging to decide which services to deploy locally versus in the cloud,\nespecially under unknown service demand and dynamic network conditions. To\ntackle this problem, we model service demand as a linear function of service\nattributes and formulate the service placement task as a linear bandit problem,\nwhere SBSs act as agents and services as arms. The goal is to identify the\nservice that, when placed at the edge, offers the greatest reduction in total\nuser delay compared to cloud deployment. We propose a distributed and adaptive\nmulti-agent best-arm identification (BAI) algorithm under a fixed-confidence\nsetting, where SBSs collaborate to accelerate learning. Simulations show that\nour algorithm identifies the optimal service with the desired confidence and\nachieves near-optimal speedup, as the number of learning rounds decreases\nproportionally with the number of SBSs. We also provide theoretical analysis of\nthe algorithm's sample complexity and communication overhead.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22480v1", "categories": ["cs.NI", "cs.DC", "cs.LG"], "cate": "cs.NI", "url": "http://arxiv.org/abs/2506.22480v1", "date": "2025-06-22", "updated": "2025-06-22"}
{"id": "2506.22988", "title": "(World) Building Transformation: Students and Teachers as CoCreators in OpenXR Learning Environments", "authors": ["Abigail Greenbaum", "Elizabeth Strickler", "Victoria Patterson", "Bolu Oluleye"], "summary": "Emerging extended reality (XR) tools and platforms offer an exciting\nopportunity to align learning experiences in higher education with the futures\nin which students will pursue their goals. However, the dynamic nature of XR as\nsubject matter challenges hierarchies and classroom practices typical of higher\neducation. This instructional design practice paper reflects on how our team of\nfaculty, learning experience designers, and user experience (UX) researchers\nimplemented human-centered design thinking, transformative learning, and\nproblem-posing education to design and implement a special topics media\nentrepreneurship course in building the metaverse. By pairing our practitioner\nexperience with learner personas, as well as survey, interview, and focus group\nresponses from our learners, we narrate our design and its implications through\na human-centered, reflective lens.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22988v1", "categories": ["cs.CY"], "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.22988v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.22505", "title": "Weakly Supervised Object Segmentation by Background Conditional Divergence", "authors": ["Hassan Baker", "Matthew S. Emigh", "Austin J. Brockmeier"], "summary": "As a computer vision task, automatic object segmentation remains challenging\nin specialized image domains without massive labeled data, such as synthetic\naperture sonar images, remote sensing, biomedical imaging, etc. In any domain,\nobtaining pixel-wise segmentation masks is expensive. In this work, we propose\na method for training a masking network to perform binary object segmentation\nusing weak supervision in the form of image-wise presence or absence of an\nobject of interest, which provides less information but may be obtained more\nquickly from manual or automatic labeling. A key step in our method is that the\nsegmented objects can be placed into background-only images to create\nrealistic, images of the objects with counterfactual backgrounds. To create a\ncontrast between the original and counterfactual background images, we propose\nto first cluster the background-only images, and then during learning create\ncounterfactual images that blend objects segmented from their original source\nbackgrounds to backgrounds chosen from a targeted cluster. One term in the\ntraining loss is the divergence between these counterfactual images and the\nreal object images with backgrounds of the target cluster. The other term is a\nsupervised loss for background-only images. While an adversarial critic could\nprovide the divergence, we use sample-based divergences. We conduct experiments\non side-scan and synthetic aperture sonar in which our approach succeeds\ncompared to previous unsupervised segmentation baselines that were only tested\non natural images. Furthermore, to show generality we extend our experiments to\nnatural images, obtaining reasonable performance with our method that avoids\npretrained networks, generative networks, and adversarial critics. The basecode\nfor this work can be found at\n\\href{GitHub}{https://github.com/bakerhassan/WSOS}.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22505v1", "categories": ["cs.CV", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22505v1", "date": "2025-06-25", "updated": "2025-06-25"}
{"id": "2506.23739", "title": "Validation of AI-Based 3D Human Pose Estimation in a Cyber-Physical Environment", "authors": ["Lisa Marie Otto", "Michael Kaiser", "Daniel Seebacher", "Steffen M√ºller"], "summary": "Ensuring safe and realistic interactions between automated driving systems\nand vulnerable road users (VRUs) in urban environments requires advanced\ntesting methodologies. This paper presents a test environment that combines a\nVehiclein-the-Loop (ViL) test bench with a motion laboratory, demonstrating the\nfeasibility of cyber-physical (CP) testing of vehicle-pedestrian and\nvehicle-cyclist interactions. Building upon previous work focused on pedestrian\nlocalization, we further validate a human pose estimation (HPE) approach\nthrough a comparative analysis of real-world (RW) and virtual representations\nof VRUs. The study examines the perception of full-body motion using a\ncommercial monocular camera-based 3Dskeletal detection AI. The virtual scene is\ngenerated in Unreal Engine 5, where VRUs are animated in real time and\nprojected onto a screen to stimulate the camera. The proposed stimulation\ntechnique ensures the correct perspective, enabling realistic vehicle\nperception. To assess the accuracy and consistency of HPE across RW and CP\ndomains, we analyze the reliability of detections as well as variations in\nmovement trajectories and joint estimation stability. The validation includes\ndynamic test scenarios where human avatars, both walking and cycling, are\nmonitored under controlled conditions. Our results show a strong alignment in\nHPE between RW and CP test conditions for stable motion patterns, while notable\ninaccuracies persist under dynamic movements and occlusions, particularly for\ncomplex cyclist postures. These findings contribute to refining CP testing\napproaches for evaluating next-generation AI-based vehicle perception and to\nenhancing interaction models of automated vehicles and VRUs in CP environments.", "comment": "6 pages, 5 figures, Preprint for 2025 IEEE IAVVC (International\n  Automated Vehicle Validation Conference)", "pdf_url": "http://arxiv.org/pdf/2506.23739v1", "categories": ["cs.RO", "cs.CE", "cs.HC"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.23739v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22971", "title": "Hierarchical Decentralized Stochastic Control for Cyber-Physical Systems", "authors": ["Kesav Kazam Ramachandran Anantharaman", "Rahul Meshram"], "summary": "This paper presents a two-timescale hierarchical decentralized architecture\nfor control of Cyber-Physical Systems. The architecture consists of $N$\nindependent sub-processes, a global controller, and $N$ local controllers, each\nformulated as a Markov Decision Process (MDP). The global controller, operating\nat a slower timescale optimizes the infinite-horizon discounted cumulative\nreward under budget constraints. For the local controllers, operating at a\nfaster timescale, we propose two different optimization frameworks, namely the\nCOpt and FOpt. In the COpt framework, the local controller also optimizes an\ninfinite-horizon MDP, while in the FOpt framework, the local controller\noptimizes a finite-horizon MDP. The FOpt framework mimics a federal structure,\nwhere the local controllers have more autonomy in their decision making. First,\nthe existence of stationary deterministic optimal policies for both these\nframeworks is established. Then, various relationships between the two\nframeworks are studied, including a bound on the difference between the two\noptimal value functions. Additionally, sufficiency conditions are provided such\nthat the two frameworks lead to the same optimal values.", "comment": "6 pages, 2 figures", "pdf_url": "http://arxiv.org/pdf/2506.22971v1", "categories": ["eess.SY", "cs.LG", "cs.MA", "cs.SY", "math.OC"], "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.22971v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.22462", "title": "Privacy-aware IoT Fall Detection Services For Aging in Place", "authors": ["Abdallah Lakhdari", "Jiajie Li", "Amani Abusafia", "Athman Bouguettaya"], "summary": "Fall detection is critical to support the growing elderly population,\nprojected to reach 2.1 billion by 2050. However, existing methods often face\ndata scarcity challenges or compromise privacy. We propose a novel IoT-based\nFall Detection as a Service (FDaaS) framework to assist the elderly in living\nindependently and safely by accurately detecting falls. We design a\nservice-oriented architecture that leverages Ultra-wideband (UWB) radar sensors\nas an IoT health-sensing service, ensuring privacy and minimal intrusion. We\naddress the challenges of data scarcity by utilizing a Fall Detection\nGenerative Pre-trained Transformer (FD-GPT) that uses augmentation techniques.\nWe developed a protocol to collect a comprehensive dataset of the elderly daily\nactivities and fall events. This resulted in a real dataset that carefully\nmimics the elderly's routine. We rigorously evaluate and compare various models\nusing this dataset. Experimental results show our approach achieves 90.72%\naccuracy and 89.33% precision in distinguishing between fall events and regular\nactivities of daily living.", "comment": "11 pages, 12 figures, This paper is accepted in the 2025 IEEE\n  International Conference on Web Services (ICWS 2025)", "pdf_url": "http://arxiv.org/pdf/2506.22462v1", "categories": ["eess.SP", "cs.AI", "cs.CY", "cs.HC"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.22462v1", "date": "2025-06-18", "updated": "2025-06-18"}
{"id": "2506.23121", "title": "CRISP-SAM2: SAM2 with Cross-Modal Interaction and Semantic Prompting for Multi-Organ Segmentation", "authors": ["Xinlei Yu", "Chanmiao Wang", "Hui Jin", "Ahmed Elazab", "Gangyong Jia", "Xiang Wan", "Changqing Zou", "Ruiquan Ge"], "summary": "Multi-organ medical segmentation is a crucial component of medical image\nprocessing, essential for doctors to make accurate diagnoses and develop\neffective treatment plans. Despite significant progress in this field, current\nmulti-organ segmentation models often suffer from inaccurate details,\ndependence on geometric prompts and loss of spatial information. Addressing\nthese challenges, we introduce a novel model named CRISP-SAM2 with CRoss-modal\nInteraction and Semantic Prompting based on SAM2. This model represents a\npromising approach to multi-organ medical segmentation guided by textual\ndescriptions of organs. Our method begins by converting visual and textual\ninputs into cross-modal contextualized semantics using a progressive\ncross-attention interaction mechanism. These semantics are then injected into\nthe image encoder to enhance the detailed understanding of visual information.\nTo eliminate reliance on geometric prompts, we use a semantic prompting\nstrategy, replacing the original prompt encoder to sharpen the perception of\nchallenging targets. In addition, a similarity-sorting self-updating strategy\nfor memory and a mask-refining process is applied to further adapt to medical\nimaging and enhance localized details. Comparative experiments conducted on\nseven public datasets indicate that CRISP-SAM2 outperforms existing models.\nExtensive analysis also demonstrates the effectiveness of our method, thereby\nconfirming its superior performance, especially in addressing the limitations\nmentioned earlier. Our code is available at:\nhttps://github.com/YU-deep/CRISP\\_SAM2.git.", "comment": "19 pages, 9 figures, 10 tables", "pdf_url": "http://arxiv.org/pdf/2506.23121v1", "categories": ["eess.IV", "cs.AI", "cs.CV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.23121v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23957", "title": "GaVS: 3D-Grounded Video Stabilization via Temporally-Consistent Local Reconstruction and Rendering", "authors": ["Zinuo You", "Stamatios Georgoulis", "Anpei Chen", "Siyu Tang", "Dengxin Dai"], "summary": "Video stabilization is pivotal for video processing, as it removes unwanted\nshakiness while preserving the original user motion intent. Existing\napproaches, depending on the domain they operate, suffer from several issues\n(e.g. geometric distortions, excessive cropping, poor generalization) that\ndegrade the user experience. To address these issues, we introduce\n\\textbf{GaVS}, a novel 3D-grounded approach that reformulates video\nstabilization as a temporally-consistent `local reconstruction and rendering'\nparadigm. Given 3D camera poses, we augment a reconstruction model to predict\nGaussian Splatting primitives, and finetune it at test-time, with multi-view\ndynamics-aware photometric supervision and cross-frame regularization, to\nproduce temporally-consistent local reconstructions. The model are then used to\nrender each stabilized frame. We utilize a scene extrapolation module to avoid\nframe cropping. Our method is evaluated on a repurposed dataset, instilled with\n3D-grounded information, covering samples with diverse camera motions and scene\ndynamics. Quantitatively, our method is competitive with or superior to\nstate-of-the-art 2D and 2.5D approaches in terms of conventional task metrics\nand new geometry consistency. Qualitatively, our method produces noticeably\nbetter results compared to alternatives, validated by the user study.", "comment": "siggraph 2025, project website: https://sinoyou.github.io/gavs", "pdf_url": "http://arxiv.org/pdf/2506.23957v1", "categories": ["cs.GR", "cs.CV"], "cate": "cs.GR", "url": "http://arxiv.org/abs/2506.23957v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23561", "title": "Towards practical FPRAS for #NFA: Exploiting the Power of Dependence", "authors": ["Kuldeep S. Meel", "Alexis de Colnet"], "summary": "#NFA refers to the problem of counting the words of length $n$ accepted by a\nnon-deterministic finite automaton. #NFA is #P-hard, and although\nfully-polynomial-time randomized approximation schemes (FPRAS) exist, they are\nall impractical. The first FPRAS for #NFA had a running time of\n$\\tilde{O}(n^{17}m^{17}\\varepsilon^{-14}\\log(\\delta^{-1}))$, where $m$ is the\nnumber of states in the automaton, $\\delta \\in (0,1]$ is the confidence\nparameter, and $\\varepsilon > 0$ is the tolerance parameter (typically smaller\nthan $1$). The current best FPRAS achieved a significant improvement in the\ntime complexity relative to the first FPRAS and obtained FPRAS with time\ncomplexity $\\tilde{O}((n^{10}m^2 +\nn^6m^3)\\varepsilon^{-4}\\log^2(\\delta^{-1}))$. The complexity of the improved\nFPRAS is still too intimidating to attempt any practical implementation.\n  In this paper, we pursue the quest for practical FPRAS for #NFA by presenting\na new algorithm with a time complexity of\n$O(n^2m^3\\log(nm)\\varepsilon^{-2}\\log(\\delta^{-1}))$. Observe that evaluating\nwhether a word of length $n$ is accepted by an NFA has a time complexity of\n$O(nm^2)$. Therefore, our proposed FPRAS achieves sub-quadratic complexity with\nrespect to membership checks.", "comment": "23 Pages, full version of paper accepted at PODS 2025", "pdf_url": "http://arxiv.org/pdf/2506.23561v1", "categories": ["cs.DS"], "cate": "cs.DS", "url": "http://arxiv.org/abs/2506.23561v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22566", "title": "Exploration Behavior of Untrained Policies", "authors": ["Jacob Adamczyk"], "summary": "Exploration remains a fundamental challenge in reinforcement learning (RL),\nparticularly in environments with sparse or adversarial reward structures. In\nthis work, we study how the architecture of deep neural policies implicitly\nshapes exploration before training. We theoretically and empirically\ndemonstrate strategies for generating ballistic or diffusive trajectories from\nuntrained policies in a toy model. Using the theory of infinite-width networks\nand a continuous-time limit, we show that untrained policies return correlated\nactions and result in non-trivial state-visitation distributions. We discuss\nthe distributions of the corresponding trajectories for a standard\narchitecture, revealing insights into inductive biases for tackling\nexploration. Our results establish a theoretical and experimental framework for\nusing policy initialization as a design tool to understand exploration behavior\nin early training.", "comment": "High-dimensional Learning Dynamics Workshop at ICML-2025", "pdf_url": "http://arxiv.org/pdf/2506.22566v1", "categories": ["cs.LG", "cs.AI", "stat.ML"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22566v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22992", "title": "MARBLE: A Hard Benchmark for Multimodal Spatial Reasoning and Planning", "authors": ["Yulun Jiang", "Yekun Chai", "Maria Brbiƒá", "Michael Moor"], "summary": "The ability to process information from multiple modalities and to reason\nthrough it step-by-step remains a critical challenge in advancing artificial\nintelligence. However, existing reasoning benchmarks focus on text-only\nreasoning, or employ multimodal questions that can be answered by directly\nretrieving information from a non-text modality. Thus, complex reasoning\nremains poorly understood in multimodal domains. Here, we present MARBLE, a\nchallenging multimodal reasoning benchmark that is designed to scrutinize\nmultimodal language models (MLLMs) in their ability to carefully reason\nstep-by-step through complex multimodal problems and environments. MARBLE is\ncomposed of two highly challenging tasks, M-Portal and M-Cube, that require the\ncrafting and understanding of multistep plans under spatial, visual, and\nphysical constraints. We find that current MLLMs perform poorly on MARBLE --\nall the 12 advanced models obtain near-random performance on M-Portal and 0%\naccuracy on M-Cube. Only in simplified subtasks some models outperform the\nrandom baseline, indicating that complex reasoning is still a challenge for\nexisting MLLMs. Moreover, we show that perception remains a bottleneck, where\nMLLMs occasionally fail to extract information from the visual inputs. By\nshedding a light on the limitations of MLLMs, we hope that MARBLE will spur the\ndevelopment of the next generation of models with the ability to reason and\nplan across many, multimodal reasoning steps.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22992v1", "categories": ["cs.AI", "cs.CL", "cs.CV"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.22992v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23397", "title": "NaviX: A Native Vector Index Design for Graph DBMSs With Robust Predicate-Agnostic Search Performance", "authors": ["Gaurav Sehgal", "Semih Salihoglu"], "summary": "There is an increasing demand for extending existing DBMSs with vector\nindices so that they become unified systems capable of supporting modern\npredictive applications, which require joint querying of vector embeddings\ntogether with the structured properties and connections of objects. We present\nNaviX, a native vector index for graph DBMSs (GDBMSs) that has two main design\ngoals. First, we aim to implement a disk-based vector index that leverages the\ncore storage and query-processing capabilities of the underlying GDBMS. To this\nend, NaviX is built on the Hierarchical Navigable Small-World (HNSW) graph,\nwhich itself is a graph-based structure. Second, we aim to support\npredicate-agnostic filtered vector search queries, in which the k nearest\nneighbors (kNNs) of a query vector vQ are searched only within an arbitrary\nsubset S of vectors defined by an ad-hoc selection sub-query QS. We adopt a\nprefiltering approach that evaluates QS first and passes the full description\nof subset S to the kNN search operator. We study how to design a prefiltering\nsearch algorithm that remains robust under varying selectivities and under\ndifferent correlations between subset S and query vector vQ. We propose an\nadaptive algorithm that uses the local selectivity of each vector in the HNSW\ngraph to choose an appropriate heuristic at every iteration of the kNN search.\nFinally, We demonstrate NaviX's robustness and efficiency through extensive\nexperiments against both existing prefiltering- and postfiltering-based\nbaselines.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23397v1", "categories": ["cs.IR", "cs.DB"], "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.23397v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.22810", "title": "A Self-Training Approach for Whisper to Enhance Long Dysarthric Speech Recognition", "authors": ["Shiyao Wang", "Jiaming Zhou", "Shiwan Zhao", "Yong Qin"], "summary": "Dysarthric speech recognition (DSR) enhances the accessibility of smart\ndevices for dysarthric speakers with limited mobility. Previously, DSR research\nwas constrained by the fact that existing datasets typically consisted of\nisolated words, command phrases, and a limited number of sentences spoken by a\nfew individuals. This constrained research to command-interaction systems and\nspeaker adaptation. The Speech Accessibility Project (SAP) changed this by\nreleasing a large and diverse English dysarthric dataset, leading to the SAP\nChallenge to build speaker- and text-independent DSR systems. We enhanced the\nWhisper model's performance on long dysarthric speech via a novel self-training\nmethod. This method increased training data and adapted the model to handle\npotentially incomplete speech segments encountered during inference. Our system\nachieved second place in both Word Error Rate and Semantic Score in the SAP\nChallenge.", "comment": "accepted by Interspeech 2025", "pdf_url": "http://arxiv.org/pdf/2506.22810v1", "categories": ["cs.SD", "eess.AS"], "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.22810v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.22598", "title": "RExBench: Can coding agents autonomously implement AI research extensions?", "authors": ["Nicholas Edwards", "Yukyung Lee", "Yujun", "Mao", "Yulu Qin", "Sebastian Schuster", "Najoung Kim"], "summary": "Agents based on Large Language Models (LLMs) have shown promise for\nperforming sophisticated software engineering tasks autonomously. In addition,\nthere has been progress towards developing agents that can perform parts of the\nresearch pipeline in machine learning and the natural sciences. We argue that\nresearch extension and its implementation is a critical capability for such\nsystems, and introduce RExBench to support the evaluation of this capability.\nRExBench is a benchmark consisting of 12 realistic research experiment\nimplementation tasks that aim to investigate research hypotheses that have not\npreviously been implemented. Each task is set up as an extension to an existing\nresearch paper and codebase, accompanied by domain expert-written instructions.\nRExBench is robust to data contamination, and supports an automatic evaluation\ninfrastructure that executes agent outputs to determine whether the success\ncriteria are met. We use this benchmark to evaluate nine LLM agents implemented\nusing three different frameworks: aider, Claude Code, and OpenHands. We find\nthat all agents evaluated fail to autonomously implement the majority of the\nextensions. Although the success rate improves with additional human-written\nhints, the best performance under this setting remains below 40%. This\nindicates that current agents are still short of being able to handle realistic\nresearch extension tasks without substantial human guidance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22598v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.22598v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22918", "title": "An approximation theory for Markov chain compression", "authors": ["Mark Fornace", "Michael Lindsey"], "summary": "We develop a framework for the compression of reversible Markov chains with\nrigorous error control. Given a subset of selected states, we construct reduced\ndynamics that can be lifted to an approximation of the full dynamics, and we\nprove simple spectral and nuclear norm bounds on the recovery error in terms of\na suitably interpreted Nystr\\\"{o}m approximation error. We introduce two\ncompression schemes: a projective compression based on committor functions and\na structure-preserving compression defined in terms of an induced Markov chain\nover the selected states. The Nystr\\\"{o}m error appearing in our bounds can be\ncontrolled using recent results on column subset selection by nuclear\nmaximization. Numerical experiments validate our theory and demonstrate the\nscalability of our approach.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22918v1", "categories": ["math.NA", "cs.NA", "math.PR"], "cate": "math.NA", "url": "http://arxiv.org/abs/2506.22918v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23582", "title": "RELATE: Subjective evaluation dataset for automatic evaluation of relevance between text and audio", "authors": ["Yusuke Kanamori", "Yuki Okamoto", "Taisei Takano", "Shinnosuke Takamichi", "Yuki Saito", "Hiroshi Saruwatari"], "summary": "In text-to-audio (TTA) research, the relevance between input text and output\naudio is an important evaluation aspect. Traditionally, it has been evaluated\nfrom both subjective and objective perspectives. However, subjective evaluation\nis costly in terms of money and time, and objective evaluation is unclear\nregarding the correlation to subjective evaluation scores. In this study, we\nconstruct RELATE, an open-sourced dataset that subjectively evaluates the\nrelevance. Also, we benchmark a model for automatically predicting the\nsubjective evaluation score from synthesized audio. Our model outperforms a\nconventional CLAPScore model, and that trend extends to many sound categories.", "comment": "Accepted to INTERSPEECH2025", "pdf_url": "http://arxiv.org/pdf/2506.23582v1", "categories": ["cs.SD", "eess.AS"], "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.23582v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23992", "title": "Harnessing AI Agents to Advance Research on Refugee Child Mental Health", "authors": ["Aditya Shrivastava", "Komal Gupta", "Shraddha Arora"], "summary": "The international refugee crisis deepens, exposing millions of dis placed\nchildren to extreme psychological trauma. This research suggests a com pact,\nAI-based framework for processing unstructured refugee health data and\ndistilling knowledge on child mental health. We compare two Retrieval-Aug\nmented Generation (RAG) pipelines, Zephyr-7B-beta and DeepSeek R1-7B, to\ndetermine how well they process challenging humanitarian datasets while avoid\ning hallucination hazards. By combining cutting-edge AI methods with migration\nresearch and child psychology, this study presents a scalable strategy to\nassist policymakers, mental health practitioners, and humanitarian agencies to\nbetter assist displaced children and recognize their mental wellbeing. In\ntotal, both the models worked properly but significantly Deepseek R1 is\nsuperior to Zephyr with an accuracy of answer relevance 0.91", "comment": "14 page , 2 image , 2 tables , accepted under 5th International\n  Conference on Innovations in Computational Intelligence and Computer Vision\n  (ICICV-2025)", "pdf_url": "http://arxiv.org/pdf/2506.23992v1", "categories": ["cs.AI", "cs.ET"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.23992v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22787", "title": "What's Privacy Good for? Measuring Privacy as a Shield from Harms due to Personal Data Use", "authors": ["Sri Harsha Gajavalli", "Junichi Koizumi", "Rakibul Hasan"], "summary": "We propose a harm-centric conceptualization of privacy that asks: What harms\nfrom personal data use can privacy prevent? The motivation behind this research\nis limitations in existing privacy frameworks (e.g., Contextual Integrity) to\ncapture or categorize many of the harms that arise from modern technology's use\nof personal data. We operationalize this conceptualization in an online study\nwith 400 college and university students. Study participants indicated their\nperceptions of different harms (e.g., manipulation, discrimination, and\nharassment) that may arise when artificial intelligence-based algorithms infer\npersonal data (e.g., demographics, personality traits, and cognitive\ndisability) and use it to identify students who are likely to drop out of a\ncourse or the best job candidate. The study includes 14 harms and six types of\npersonal data selected based on an extensive literature review.\n  Comprehensive statistical analyses of the study data show that the 14 harms\nare internally consistent and collectively represent a general notion of\nprivacy harms. The study data also surfaces nuanced perceptions of harms, both\nacross the contexts and participants' demographic factors. Based on these\nresults, we discuss how privacy can be improved equitably. Thus, this research\nnot only contributes to enhancing the understanding of privacy as a concept but\nalso provides practical guidance to improve privacy in the context of education\nand employment.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22787v1", "categories": ["cs.CR", "cs.CY"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.22787v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23079", "title": "Research on Comprehensive Classroom Evaluation System Based on Multiple AI Models", "authors": ["Cong Xie", "Li Yang", "Daben Wang", "Jing Xiao"], "summary": "The promotion of the national education digitalization strategy has\nfacilitated the development of teaching quality evaluation towards all-round,\nprocess-oriented, precise, and intelligent directions, inspiring explorations\ninto new methods and technologies for educational quality assurance. Classroom\nteaching evaluation methods dominated by teaching supervision and student\nteaching evaluation suffer from issues such as low efficiency, strong\nsubjectivity, and limited evaluation dimensions. How to further advance\nintelligent and objective evaluation remains a topic to be explored. This\npaper, based on image recognition technology, speech recognition technology,\nand AI large language models, develops a comprehensive evaluation system that\nautomatically generates evaluation reports and optimization suggestions from\ntwo dimensions: teacher teaching ability and classroom teaching effectiveness.\nThis study establishes a closed-loop classroom evaluation model that\ncomprehensively evaluates student and teaching conditions based on\nmulti-dimensional data throughout the classroom teaching process, and further\nanalyzes the data to guide teaching improvement. It meets the requirements of\nall-round and process-oriented classroom evaluation in the era of digital\neducation, effectively solves the main problems of manual evaluation methods,\nand provides data collection and analysis methods as well as technologies for\nrelevant research on educational teaching evaluation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23079v1", "categories": ["cs.CY", "cs.MA"], "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.23079v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.22942", "title": "Energy-Constrained Resilient Multi-Robot Coverage Control", "authors": ["Kartik A. Pant", "Jaehyeok Kim", "James M. Goppert", "Inseok Hwang"], "summary": "The problem of multi-robot coverage control becomes significantly challenging\nwhen multiple robots leave the mission space simultaneously to charge their\nbatteries, disrupting the underlying network topology for communication and\nsensing. To address this, we propose a resilient network design and control\napproach that allows robots to achieve the desired coverage performance while\nsatisfying energy constraints and maintaining network connectivity throughout\nthe mission. We model the combined motion, energy, and network dynamics of the\nmultirobot systems (MRS) as a hybrid system with three modes, i.e., coverage,\nreturn-to-base, and recharge, respectively. We show that ensuring the energy\nconstraints can be transformed into designing appropriate guard conditions for\nmode transition between each of the three modes. Additionally, we present a\nsystematic procedure to design, maintain, and reconfigure the underlying\nnetwork topology using an energy-aware bearing rigid network design, enhancing\nthe structural resilience of the MRS even when a subset of robots departs to\ncharge their batteries. Finally, we validate our proposed method using\nnumerical simulations.", "comment": "6 pages, 4 figures", "pdf_url": "http://arxiv.org/pdf/2506.22942v1", "categories": ["cs.RO"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.22942v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.22745", "title": "Trusted Routing for Blockchain-Enabled Low-Altitude Intelligent Networks", "authors": ["Sijie He", "Ziye Jia", "Qiuming Zhu", "Fuhui Zhou", "Qihui Wu"], "summary": "Due to the scalability and portability, the low-altitude intelligent networks\n(LAINs) are essential in various fields such as surveillance and disaster\nrescue. However, in LAINs, unmanned aerial vehicles (UAVs) are characterized by\nthe distributed topology and high dynamic mobility, and vulnerable to security\nthreats, which may degrade the routing performance for data transmission.\nHence, how to ensure the routing stability and security of LAINs is a\nchallenge. In this paper, we focus on the routing process in LAINs with\nmultiple UAV clusters and propose the blockchain-enabled zero-trust\narchitecture to manage the joining and exiting of UAVs. Furthermore, we\nformulate the routing problem to minimize the end-to-end (E2E) delay, which is\nan integer linear programming and intractable to solve. Therefore, considering\nthe distribution of LAINs, we reformulate the routing problem into a\ndecentralized partially observable Markov decision process. With the proposed\nsoft hierarchical experience replay buffer, the multi-agent double deep\nQ-network based adaptive routing algorithm is designed. Finally, simulations\nare conducted and numerical results show that the total E2E delay of the\nproposed mechanism decreases by 22.38\\% than the benchmark on average.", "comment": "Low-altitude intelligent networks, trusted routing, blockchain, soft\n  hierarchical experience replay buffer, multi-agent deep reinforcement\n  learning", "pdf_url": "http://arxiv.org/pdf/2506.22745v1", "categories": ["cs.NI"], "cate": "cs.NI", "url": "http://arxiv.org/abs/2506.22745v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.24118", "title": "Scaling Human Judgment in Community Notes with LLMs", "authors": ["Haiwen Li", "Soham De", "Manon Revel", "Andreas Haupt", "Brad Miller", "Keith Coleman", "Jay Baxter", "Martin Saveski", "Michiel A. Bakker"], "summary": "This paper argues for a new paradigm for Community Notes in the LLM era: an\nopen ecosystem where both humans and LLMs can write notes, and the decision of\nwhich notes are helpful enough to show remains in the hands of humans. This\napproach can accelerate the delivery of notes, while maintaining trust and\nlegitimacy through Community Notes' foundational principle: A community of\ndiverse human raters collectively serve as the ultimate evaluator and arbiter\nof what is helpful. Further, the feedback from this diverse community can be\nused to improve LLMs' ability to produce accurate, unbiased, broadly helpful\nnotes--what we term Reinforcement Learning from Community Feedback (RLCF). This\nbecomes a two-way street: LLMs serve as an asset to humans--helping deliver\ncontext quickly and with minimal effort--while human feedback, in turn,\nenhances the performance of LLMs. This paper describes how such a system can\nwork, its benefits, key new risks and challenges it introduces, and a research\nagenda to solve those challenges and realize the potential of this approach.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.24118v1", "categories": ["cs.CY", "cs.SI"], "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.24118v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23234", "title": "From Release to Adoption: Challenges in Reusing Pre-trained AI Models for Downstream Developers", "authors": ["Peerachai Banyongrakkul", "Mansooreh Zahedi", "Patanamon Thongtanunam", "Christoph Treude", "Haoyu Gao"], "summary": "Pre-trained models (PTMs) have gained widespread popularity and achieved\nremarkable success across various fields, driven by their groundbreaking\nperformance and easy accessibility through hosting providers. However, the\nchallenges faced by downstream developers in reusing PTMs in software systems\nare less explored. To bridge this knowledge gap, we qualitatively created and\nanalyzed a dataset of 840 PTM-related issue reports from 31 OSS GitHub\nprojects. We systematically developed a comprehensive taxonomy of PTM-related\nchallenges that developers face in downstream projects. Our study identifies\nseven key categories of challenges that downstream developers face in reusing\nPTMs, such as model usage, model performance, and output quality. We also\ncompared our findings with existing taxonomies. Additionally, we conducted a\nresolution time analysis and, based on statistical tests, found that\nPTM-related issues take significantly longer to be resolved than issues\nunrelated to PTMs, with significant variation across challenge categories. We\ndiscuss the implications of our findings for practitioners and possibilities\nfor future research.", "comment": "Recently accepted at ICSME 2025", "pdf_url": "http://arxiv.org/pdf/2506.23234v1", "categories": ["cs.SE"], "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.23234v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.22940", "title": "Context, Credibility, and Control: User Reflections on AI Assisted Misinformation Tools", "authors": ["Varun Sangwan", "Heidi Makitalo"], "summary": "This paper investigates how collaborative AI systems can enhance user agency\nin identifying and evaluating misinformation on social media platforms.\nTraditional methods, such as personal judgment or basic fact-checking, often\nfall short when faced with emotionally charged or context-deficient content. To\naddress this, we designed and evaluated an interactive interface that\nintegrates collaborative AI features, including real-time explanations, source\naggregation, and debate-style interaction. These elements aim to support\ncritical thinking by providing contextual cues and argumentative reasoning in a\ntransparent, user-centered format. In a user study with 14 participants, 79%\nfound the debate mode more effective than standard chatbot interfaces, and the\nmultiple-source view received an average usefulness rating of 4.6 out of 5. Our\nfindings highlight the potential of context-rich, dialogic AI systems to\nimprove media literacy and foster trust in digital information environments. We\nargue that future tools for misinformation mitigation should prioritize ethical\ndesign, explainability, and interactive engagement to empower users in a\npost-truth era.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22940v1", "categories": ["cs.HC", "cs.SI"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.22940v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.24060", "title": "Combinatorial Multi-Access Coded Caching with Private Caches under Intersecting Index Constraints", "authors": ["Dhruv Pratap Singh", "Anjana A. Mahesh", "B. Sundar Rajan"], "summary": "We consider the coded caching system where each user, equipped with a private\ncache, accesses a distinct r-subset of access caches. A central server housing\na library of files populates both private and access caches using uncoded\nplacement. In this work, we focus on a constrained indexing regime, referred to\nas the intersection class, in which the sets used to index the demands of each\nuser must have a nonempty intersection. This regime models resource-limited IoT\nscenarios such as edge-assisted IoT systems, where devices with small private\ncaches connect to a small number of shared caches. We provide a necessary and\nsufficient condition under which the system parameters fall within this\nintersection class. Under this condition, we propose a centralized coded\ncaching scheme and characterize its rate-memory trade-off. Next, we define a\nuniform-intersection subclass and establish a condition under which the system\nbelongs to this subclass. Within this subclass, the proposed scheme has a\nregular structure, with each transmission benefiting the same number of users,\nand we characterize its rate-memory trade-off. Additionally, we derive an index\ncoding-based lower bound on the minimum achievable worst-case rate under\nuncoded placement. Finally, we provide numerical comparisons between the rate\nof the proposed scheme, the new lower bound, and bounds from the original work.", "comment": "9 pages and 3 figures", "pdf_url": "http://arxiv.org/pdf/2506.24060v1", "categories": ["cs.IT", "math.IT"], "cate": "cs.IT", "url": "http://arxiv.org/abs/2506.24060v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22668", "title": "DistShap: Scalable GNN Explanations with Distributed Shapley Values", "authors": ["Selahattin Akkas", "Aditya Devarakonda", "Ariful Azad"], "summary": "With the growing adoption of graph neural networks (GNNs), explaining their\npredictions has become increasingly important. However, attributing predictions\nto specific edges or features remains computationally expensive. For example,\nclassifying a node with 100 neighbors using a 3-layer GNN may involve\nidentifying important edges from millions of candidates contributing to the\nprediction. To address this challenge, we propose DistShap, a parallel\nalgorithm that distributes Shapley value-based explanations across multiple\nGPUs. DistShap operates by sampling subgraphs in a distributed setting,\nexecuting GNN inference in parallel across GPUs, and solving a distributed\nleast squares problem to compute edge importance scores. DistShap outperforms\nmost existing GNN explanation methods in accuracy and is the first to scale to\nGNN models with millions of features by using up to 128 GPUs on the NERSC\nPerlmutter supercomputer.", "comment": "12 pages", "pdf_url": "http://arxiv.org/pdf/2506.22668v1", "categories": ["cs.LG", "cs.AI", "cs.DC", "stat.ML"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22668v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.23079", "title": "Research on Comprehensive Classroom Evaluation System Based on Multiple AI Models", "authors": ["Cong Xie", "Li Yang", "Daben Wang", "Jing Xiao"], "summary": "The promotion of the national education digitalization strategy has\nfacilitated the development of teaching quality evaluation towards all-round,\nprocess-oriented, precise, and intelligent directions, inspiring explorations\ninto new methods and technologies for educational quality assurance. Classroom\nteaching evaluation methods dominated by teaching supervision and student\nteaching evaluation suffer from issues such as low efficiency, strong\nsubjectivity, and limited evaluation dimensions. How to further advance\nintelligent and objective evaluation remains a topic to be explored. This\npaper, based on image recognition technology, speech recognition technology,\nand AI large language models, develops a comprehensive evaluation system that\nautomatically generates evaluation reports and optimization suggestions from\ntwo dimensions: teacher teaching ability and classroom teaching effectiveness.\nThis study establishes a closed-loop classroom evaluation model that\ncomprehensively evaluates student and teaching conditions based on\nmulti-dimensional data throughout the classroom teaching process, and further\nanalyzes the data to guide teaching improvement. It meets the requirements of\nall-round and process-oriented classroom evaluation in the era of digital\neducation, effectively solves the main problems of manual evaluation methods,\nand provides data collection and analysis methods as well as technologies for\nrelevant research on educational teaching evaluation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23079v1", "categories": ["cs.CY", "cs.MA"], "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.23079v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.22509", "title": "FreeDNA: Endowing Domain Adaptation of Diffusion-Based Dense Prediction with Training-Free Domain Noise Alignment", "authors": ["Hang Xu", "Jie Huang", "Linjiang Huang", "Dong Li", "Yidi Liu", "Feng Zhao"], "summary": "Domain Adaptation(DA) for dense prediction tasks is an important topic, which\nenhances the dense prediction model's performance when tested on its unseen\ndomain. Recently, with the development of Diffusion-based Dense Prediction\n(DDP) models, the exploration of DA designs tailored to this framework is worth\nexploring, since the diffusion model is effective in modeling the distribution\ntransformation that comprises domain information. In this work, we propose a\ntraining-free mechanism for DDP frameworks, endowing them with DA capabilities.\nOur motivation arises from the observation that the exposure bias (e.g., noise\nstatistics bias) in diffusion brings domain shift, and different domains in\nconditions of DDP models can also be effectively captured by the noise\nprediction statistics. Based on this, we propose a training-free Domain Noise\nAlignment (DNA) approach, which alleviates the variations of noise statistics\nto domain changes during the diffusion sampling process, thereby achieving\ndomain adaptation. Specifically, when the source domain is available, we\ndirectly adopt the DNA method to achieve domain adaptation by aligning the\nnoise statistics of the target domain with those of the source domain. For the\nmore challenging source-free DA, inspired by the observation that regions\ncloser to the source domain exhibit higher confidence meeting variations of\nsampling noise, we utilize the statistics from the high-confidence regions\nprogressively to guide the noise statistic adjustment during the sampling\nprocess. Notably, our method demonstrates the effectiveness of enhancing the DA\ncapability of DDP models across four common dense prediction tasks. Code is\navailable at\n\\href{https://github.com/xuhang07/FreeDNA}{https://github.com/xuhang07/FreeDNA}.", "comment": "ICCV2025", "pdf_url": "http://arxiv.org/pdf/2506.22509v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22509v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.23169", "title": "Extreme Scenario Characterization for High Renewable Energy Penetrated Power Systems over Long Time Scales", "authors": ["Kai Kang", "Feng Liu", "Yifan Su", "Zhaojian Wang"], "summary": "Power systems with high renewable energy penetration are highly influenced by\nweather conditions, often facing significant challenges such as persistent\npower shortages and severe power fluctuations over long time scales. This paper\naddresses the critical need for effective characterization of extreme scenarios\nunder these situations. First, novel risk indices are proposed to quantify the\nseverity of continuous power shortages and substantial power fluctuations over\nlong-term operations. These indices are independent of specific scheduling\nstrategies and incorporate the system's resource regulation capabilities. By\nemploying a filtering-based approach, the proposed indices focus on retaining\nkey characteristics of continuous power shortages and fluctuation events,\nenabling the identification of extreme scenarios on long time scales. Secondly,\nan extreme scenario generation method is developed using Gaussian mixture\nmodels and sequential Monte Carlo simulation. Especially, this method\nperiodically evaluates the severity of generated scenarios based on the defined\nrisk indices, retaining extreme scenarios while discarding less critical ones.\nFinally, case studies based on real-world data demonstrate the efficacy of the\nproposed method. The results confirm that integrating the identified extreme\nscenarios significantly enhances the system's ability to ensure long-term\nsecurity and reliability under high renewable energy penetration.", "comment": "Accepted for publication in 2025 IEEE Power & Energy Society General\n  Meeting", "pdf_url": "http://arxiv.org/pdf/2506.23169v1", "categories": ["eess.SY", "cs.SY"], "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.23169v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.22465", "title": "Preconditioned Conjugate Gradient for MIMO-AFDM System", "authors": ["Jun Zhu", "Yin Xu", "Dazhi He", "Haoyang Li", "Yunfeng Guan", "Wenjun Zhang"], "summary": "Affine frequency division multiplexing (AFDM) is a promising chirp-assisted\nmulticarrier waveform for future high mobility communications. A significant\nchallenge in MIMO-AFDM systems is the multi-user interference (MUI), which can\nbe effectively addressed by employing precoding techniques. However, the\ncomplexity introduced by AFDM makes the precoding process computationally\nexpensive and challenging. To overcome this issue, We combine AFDM channel\nsparse property and using Preconditioned Conjugate Gradient (PCG) method to\niteratively process the precoding, thereby reducing the complexity of the\nprecoding design. Simulation results demonstrate that the proposed\nsparsification approach, coupled with the PCG method, achieving quite precoding\nperformance while significantly reducing computational complexity. This makes\nthe application of AFDM more feasible and efficient for high-mobility\ncommunication scenarios, paving the way for its broader implementation in\nnext-generation communication systems.", "comment": "arXiv admin note: text overlap with arXiv:2503.10525", "pdf_url": "http://arxiv.org/pdf/2506.22465v1", "categories": ["eess.SP", "cs.IT", "math.IT"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.22465v1", "date": "2025-06-18", "updated": "2025-06-18"}
{"id": "2506.23184", "title": "Score-based Diffusion Model for Unpaired Virtual Histology Staining", "authors": ["Anran Liu", "Xiaofei Wang", "Jing Cai", "Chao Li"], "summary": "Hematoxylin and eosin (H&E) staining visualizes histology but lacks\nspecificity for diagnostic markers. Immunohistochemistry (IHC) staining\nprovides protein-targeted staining but is restricted by tissue availability and\nantibody specificity. Virtual staining, i.e., computationally translating the\nH&E image to its IHC counterpart while preserving the tissue structure, is\npromising for efficient IHC generation. Existing virtual staining methods still\nface key challenges: 1) effective decomposition of staining style and tissue\nstructure, 2) controllable staining process adaptable to diverse tissue and\nproteins, and 3) rigorous structural consistency modelling to handle the\nnon-pixel-aligned nature of paired H&E and IHC images. This study proposes a\nmutual-information (MI)-guided score-based diffusion model for unpaired virtual\nstaining. Specifically, we design 1) a global MI-guided energy function that\ndisentangles the tissue structure and staining characteristics across\nmodalities, 2) a novel timestep-customized reverse diffusion process for\nprecise control of the staining intensity and structural reconstruction, and 3)\na local MI-driven contrastive learning strategy to ensure the cellular level\nstructural consistency between H&E-IHC images. Extensive experiments\ndemonstrate the our superiority over state-of-the-art approaches, highlighting\nits biomedical potential. Codes will be open-sourced upon acceptance.", "comment": "11 pages, 3 figures", "pdf_url": "http://arxiv.org/pdf/2506.23184v1", "categories": ["eess.IV", "cs.AI", "cs.CV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.23184v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.24108", "title": "Navigating with Annealing Guidance Scale in Diffusion Space", "authors": ["Shai Yehezkel", "Omer Dahary", "Andrey Voynov", "Daniel Cohen-Or"], "summary": "Denoising diffusion models excel at generating high-quality images\nconditioned on text prompts, yet their effectiveness heavily relies on careful\nguidance during the sampling process. Classifier-Free Guidance (CFG) provides a\nwidely used mechanism for steering generation by setting the guidance scale,\nwhich balances image quality and prompt alignment. However, the choice of the\nguidance scale has a critical impact on the convergence toward a visually\nappealing and prompt-adherent image. In this work, we propose an annealing\nguidance scheduler which dynamically adjusts the guidance scale over time based\non the conditional noisy signal. By learning a scheduling policy, our method\naddresses the temperamental behavior of CFG. Empirical results demonstrate that\nour guidance scheduler significantly enhances image quality and alignment with\nthe text prompt, advancing the performance of text-to-image generation.\nNotably, our novel scheduler requires no additional activations or memory\nconsumption, and can seamlessly replace the common classifier-free guidance,\noffering an improved trade-off between prompt alignment and quality.", "comment": "Project page:\n  https://annealing-guidance.github.io/annealing-guidance/", "pdf_url": "http://arxiv.org/pdf/2506.24108v1", "categories": ["cs.GR", "cs.AI", "cs.CV", "cs.LG"], "cate": "cs.GR", "url": "http://arxiv.org/abs/2506.24108v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23638", "title": "Simple Approximations for General Spanner Problems", "authors": ["Fritz B√∂kler", "Markus Chimani", "Henning Jasper"], "summary": "Consider a graph with n nodes and m edges, independent edge weights and\nlengths, and arbitrary distance demands for node pairs. The spanner problem\nasks for a minimum-weight subgraph that satisfies these demands via\nsufficiently short paths w.r.t. the edge lengths. For multiplicative\nalpha-spanners (where demands equal alpha times the original distances) and\nassuming that each edge's weight equals its length, the simple Greedy heuristic\nby Alth\\\"ofer et al. (1993) is known to yield strong solutions, both in theory\nand practice. To obtain guarantees in more general settings, recent\napproximations typically abandon this simplicity and practicality. Still, so\nfar, there is no known non-trivial approximation algorithm for the spanner\nproblem in its most general form. We provide two surprisingly simple\napproximations algorithms. In general, our Adapted Greedy achieves the first\nunconditional approximation ratio of m, which is non-trivial due to the\nindependence of weights and lengths. Crucially, it maintains all size and\nweight guarantees Greedy is known for, i.e., in the aforementioned\nmultiplicative alpha-spanner scenario and even for additive +beta-spanners.\nFurther, it generalizes some of these size guarantees to derive new weight\nguarantees. Our second approach, Randomized Rounding, establishes a graph\ntransformation that allows a simple rounding scheme over a standard\nmulticommodity flow LP. It yields an O(n log n)-approximation, assuming integer\nlengths and polynomially bounded distance demands. The only other known\napproximation guarantee in this general setting requires several complex\nsubalgorithms and analyses, yet we match it up to a factor of O(n^{1/5-eps})\nusing standard tools. Further, on bounded-degree graphs, we yield the first\nO(log n) approximation ratio for constant-bounded distance demands (beyond\nmultiplicative 2-spanners in unit-length graphs).", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23638v1", "categories": ["cs.DS", "cs.DM", "math.CO", "68R10 (Primary) 05C85, 90C11 (Secondary)", "F.2.2; G.2.1; G.2.2"], "cate": "cs.DS", "url": "http://arxiv.org/abs/2506.23638v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22578", "title": "The Hidden Link Between RLHF and Contrastive Learning", "authors": ["Xufei Lv", "Haoyuan Sun", "Xuefeng Bai", "Min Zhang", "Houde Liu", "Kehai Chen"], "summary": "Alignment of large language models (LLMs) with human values has recently\ngarnered significant attention, with prominent examples including the canonical\nyet costly Reinforcement Learning from Human Feedback (RLHF) and the simple\nDirect Preference Optimization (DPO). In this work, we demonstrate that both\nRLHF and DPO can be interpreted from the perspective of mutual information (MI)\nmaximization, uncovering a profound connection to contrastive learning. Within\nthis framework, both RLHF and DPO can be viewed as methods that perform\ncontrastive learning based on the positive and negative samples derived from\nthe base model, leveraging the Donsker-Varadhan (DV) lower bound on MI\n(equivalently, the MINE estimator). This paradigm further explains why RLHF may\nnot intrinsically incentivize reasoning capacities in LLMs beyond what is\nalready present in the base model. Building on this perspective, we replace the\nDV/MINE bound with the Jensen-Shannon MI estimator and propose Mutual\nInformation Optimization (MIO). Comprehensive theoretical analysis and\nextensive empirical evaluations demonstrate that MIO mitigates the late-stage\ndecline in chosen-likelihood observed in DPO, achieving competitive or superior\nperformance across various challenging reasoning and mathematical benchmarks.\nWe will release the model and code upon acceptance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22578v1", "categories": ["cs.LG", "cs.AI", "stat.ML"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22578v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.23049", "title": "AURA: Agent for Understanding, Reasoning, and Automated Tool Use in Voice-Driven Tasks", "authors": ["Leander Melroy Maben", "Gayathri Ganesh Lakshmy", "Srijith Radhakrishnan", "Siddhant Arora", "Shinji Watanabe"], "summary": "Despite advances in language and speech technologies, no open-source system\nenables full speech-to-speech, multi-turn dialogue with integrated tool use and\nagentic reasoning. We introduce AURA (Agent for Understanding, Reasoning, and\nAutomated Tool Use), the first open-source, speech-native assistant capable of\ncompleting complex, goal-driven tasks through dynamic tool invocation and\nmulti-turn conversation. AURA combines open-weight ASR, TTS, and LLMs in a\ncascaded pipeline and supports tools such as calendar booking, contact lookup,\nweb search, and email. Its modular design allows easy integration of new tools\nusing natural language prompts and action classes. On VoiceBench, AURA scores\n92.75% on OpenBookQA-outperforming all open-weight systems and nearing\nGPT-4o-and 4.39 on AlpacaEval, competitive with other open-weight systems.\nHuman evaluation shows 90% task success on complex, multi-turn speech tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23049v1", "categories": ["cs.AI", "cs.CL", "cs.SD", "eess.AS", "68T42, 68T50,", "I.2.7; I.2.11; H.5.5"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.23049v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23471", "title": "KiseKloset: Comprehensive System For Outfit Retrieval, Recommendation, And Try-On", "authors": ["Thanh-Tung Phan-Nguyen", "Khoi-Nguyen Nguyen-Ngoc", "Tam V. Nguyen", "Minh-Triet Tran", "Trung-Nghia Le"], "summary": "The global fashion e-commerce industry has become integral to people's daily\nlives, leveraging technological advancements to offer personalized shopping\nexperiences, primarily through recommendation systems that enhance customer\nengagement through personalized suggestions. To improve customers' experience\nin online shopping, we propose a novel comprehensive KiseKloset system for\noutfit retrieval, recommendation, and try-on. We explore two approaches for\noutfit retrieval: similar item retrieval and text feedback-guided item\nretrieval. Notably, we introduce a novel transformer architecture designed to\nrecommend complementary items from diverse categories. Furthermore, we enhance\nthe overall performance of the search pipeline by integrating approximate\nalgorithms to optimize the search process. Additionally, addressing the crucial\nneeds of online shoppers, we employ a lightweight yet efficient virtual try-on\nframework capable of real-time operation, memory efficiency, and maintaining\nrealistic outputs compared to its predecessors. This virtual try-on module\nempowers users to visualize specific garments on themselves, enhancing the\ncustomers' experience and reducing costs associated with damaged items for\nretailers. We deployed our end-to-end system for online users to test and\nprovide feedback, enabling us to measure their satisfaction levels. The results\nof our user study revealed that 84% of participants found our comprehensive\nsystem highly useful, significantly improving their online shopping experience.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23471v1", "categories": ["cs.IR", "cs.CV"], "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.23471v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22846", "title": "Boosting CTC-Based ASR Using LLM-Based Intermediate Loss Regularization", "authors": ["Duygu Altinok"], "summary": "End-to-end (E2E) automatic speech recognition (ASR) systems have\nrevolutionized the field by integrating all components into a single neural\nnetwork, with attention-based encoder-decoder models achieving state-of-the-art\nperformance. However, their autoregressive decoding process limits inference\nspeed, making them unsuitable for real-time applications. In contrast,\nCTC-based models offer faster, non-autoregressive decoding but struggle to\nmodel linguistic dependencies effectively. Addressing this challenge, we\npropose a novel auxiliary loss framework called Language-Aware Intermediate\nLoss (LAIL) to enhance CTC-based ASR using the linguistic knowledge of large\nlanguage models (LLMs). By attaching connector layers to intermediate encoder\nlayers, LAIL maps outputs to the embedding space of an LLM and computes a\ncausal language modeling loss during training. This approach enhances\nlinguistic modeling while preserving the computational efficiency of CTC\ndecoding. Using the Conformer architecture and various LLaMA models, we\ndemonstrate significant improvements in Word Error Rate (WER) on the\nLibriSpeech, TEDLIUM2, and WSJ corpora, achieving state-of-the-art performance\nfor CTC-based ASR with minimal computational overhead.", "comment": "This is the accepted version of an article accepted to the TSD 2025\n  conference, published in Springer Lecture Notes in Artificial Intelligence\n  (LNAI). The final authenticated version is available online at SpringerLink", "pdf_url": "http://arxiv.org/pdf/2506.22846v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.22846v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.22623", "title": "Temperature Matters: Enhancing Watermark Robustness Against Paraphrasing Attacks", "authors": ["Badr Youbi Idrissi", "Monica Millunzi", "Amelia Sorrenti", "Lorenzo Baraldi", "Daryna Dementieva"], "summary": "In the present-day scenario, Large Language Models (LLMs) are establishing\ntheir presence as powerful instruments permeating various sectors of society.\nWhile their utility offers valuable support to individuals, there are multiple\nconcerns over potential misuse. Consequently, some academic endeavors have\nsought to introduce watermarking techniques, characterized by the inclusion of\nmarkers within machine-generated text, to facilitate algorithmic\nidentification. This research project is focused on the development of a novel\nmethodology for the detection of synthetic text, with the overarching goal of\nensuring the ethical application of LLMs in AI-driven text generation. The\ninvestigation commences with replicating findings from a previous baseline\nstudy, thereby underscoring its susceptibility to variations in the underlying\ngeneration model. Subsequently, we propose an innovative watermarking approach\nand subject it to rigorous evaluation, employing paraphrased generated text to\nasses its robustness. Experimental results highlight the robustness of our\nproposal compared to the~\\cite{aarson} watermarking method.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22623v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.22623v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.23084", "title": "PML method for the time-domain stochastic acoustic wave equation and an inverse source problem", "authors": ["Hongxia Guo", "Tianjiao Wang", "Xiang Xu", "Yue Zhao"], "summary": "In this paper, we develop and analyze a time-domain perfectly matched layer\n(PML) method for the stochastic acoustic wave equation driven by spatially\nwhite additive Gaussian noise. We begin by establishing the well-posedness and\nstability of the direct problem through a rigorous analysis of the associated\ntime-harmonic stochastic Helmholtz equation and the application of an abstract\nLaplace transform inversion theorem. To address the low regularity of the\nrandom source, we employ scattering theory to investigate the meromorphic\ncontinuation of the Helmholtz resolvent defined on rough fields. Based on a\npiecewise constant approximation of the white noise, we construct an\napproximate wave solution and formulate a time-domain PML method. The\nconvergence of the PML method is established, with explicit dependence on the\nPML layer's thickness and medium properties, as well as the piecewise constant\napproximation of the white noise. In addition, we propose a frequency-domain\napproach for solving the inverse random source problem using time-domain\nboundary measurements. A logarithmic stability estimate is derived,\nhighlighting the ill-posedness of the inverse problem and offering guidance for\nthe design of effective numerical schemes.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23084v1", "categories": ["math.NA", "cs.NA", "35B35, 35R60, 78A46"], "cate": "math.NA", "url": "http://arxiv.org/abs/2506.23084v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23670", "title": "Efficient Interleaved Speech Modeling through Knowledge Distillation", "authors": ["Mohammadmahdi Nouriborji", "Morteza Rohanian"], "summary": "Current speech language models exceed the size and latency constraints of\nmany deployment environments. We build compact, expressive speech generation\nmodels through layer-aligned distillation, matching hidden states, attention\nmaps, and softened logits to compress large multimodal transformers by 3x with\nminimal loss in performance. We introduce TinyWave, a family of 2B-parameter\nmodels for speech-to-speech and interleaved speech-text generation, trained on\n50,000 hours of public audio. TinyWave supports (i) speech-only generation\nusing phonetic or expressive tokens and (ii) mixed speech-text continuations.\nEvaluation on Libri-Light shows TinyWave within 1.4 normalized perplexity\npoints of its teacher. Accuracy on spoken StoryCloze and SALMon reaches 93-97%\nof the teacher's performance, outperforming size-matched baselines. These\nmodels are optimized for deployment on commodity hardware, enabling\napplications in real-time conversational agents, assistive technologies, and\nlow-resource environments. We release models, training code, and evaluation\nscripts to support reproducible research on compact, expressive speech\ngeneration.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23670v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.23670v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.24008", "title": "Spatial QUBO: Convolutional Formulation of Large-Scale Binary Optimization with Dense Interactions", "authors": ["Hiroshi Yamashita", "Hideyuki Suzuki"], "summary": "The spatial photonic Ising machine (SPIM) is a promising optical hardware\nsolver for large-scale combinatorial optimization problems with dense\ninteractions. As the SPIM can represent Ising problems with rank-one coupling\nmatrices, multiplexed versions have been proposed to enhance the applicability\nto higher-rank interactions. However, the multiplexing cost reduces the\nimplementation efficiency, and even without multiplexing, the SPIM is known to\nrepresent coupling matrices beyond rank-one. In this paper, to clarify the\nintrinsic representation power of the original SPIM, we propose spatial QUBO\n(spQUBO), a formulation of Ising problems with spatially convolutional\nstructures. We prove that any spQUBO reduces to a two-dimensional spQUBO, with\nthe convolutional structure preserved, and that any two-dimensional spQUBO can\nbe efficiently implemented on the SPIM without multiplexing. We further\ndemonstrate its practical applicability to distance-based combinatorial\noptimization, such as placement problems and clustering problems. These results\nadvance our understanding of the class of optimization problems where SPIMs\nexhibit superior efficiency and scalability. Furthermore, spQUBO's efficiency\nis not limited to the SPIM architecture; we show that its convolutional\nstructure allows efficient computation using Fast Fourier Transforms (FFT).", "comment": "18 pages, 6 figures (including supplementary information, 7 pages, 1\n  figure)", "pdf_url": "http://arxiv.org/pdf/2506.24008v1", "categories": ["cond-mat.dis-nn", "cs.ET", "physics.app-ph", "physics.optics"], "cate": "cond-mat.dis-nn", "url": "http://arxiv.org/abs/2506.24008v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22938", "title": "Efficient Cybersecurity Assessment Using SVM and Fuzzy Evidential Reasoning for Resilient Infrastructure", "authors": ["Zaydon L. Ali", "Wassan Saad Abduljabbar Hayale", "Israa Ibraheem Al_Barazanchi", "Ravi Sekhar", "Pritesh Shah", "Sushma Parihar"], "summary": "With current advancement in hybermedia knowledges, the privacy of digital\ninformation has developed a critical problem. To overawed the susceptibilities\nof present security protocols, scholars tend to focus mainly on efforts on\nalternation of current protocols. Over past decade, various proposed encoding\nmodels have been shown insecurity, leading to main threats against significant\ndata. Utilizing the suitable encryption model is very vital means of guard\nagainst various such, but algorithm is selected based on the dependency of data\nwhich need to be secured. Moreover, testing potentiality of the security\nassessment one by one to identify the best choice can take a vital time for\nprocessing. For faster and precisive identification of assessment algorithm, we\nsuggest a security phase exposure model for cipher encryption technique by\ninvoking Support Vector Machine (SVM). In this work, we form a dataset using\nusual security components like contrast, homogeneity. To overcome the\nuncertainty in analysing the security and lack of ability of processing data to\na risk assessment mechanism. To overcome with such complications, this paper\nproposes an assessment model for security issues using fuzzy evidential\nreasoning (ER) approaches. Significantly, the model can be utilised to process\nand assemble risk assessment data on various aspects in systematic ways. To\nestimate the performance of our framework, we have various analyses like,\nrecall, F1 score and accuracy.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22938v1", "categories": ["cs.CR", "cs.LG"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.22938v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23351", "title": "Benchmarking Generalizable Bimanual Manipulation: RoboTwin Dual-Arm Collaboration Challenge at CVPR 2025 MEIS Workshop", "authors": ["Tianxing Chen", "Kaixuan Wang", "Zhaohui Yang", "Yuhao Zhang", "Zanxin Chen", "Baijun Chen", "Wanxi Dong", "Ziyuan Liu", "Dong Chen", "Tianshuo Yang", "Haibao Yu", "Xiaokang Yang", "Yusen Qin", "Zhiqiang Xie", "Yao Mu", "Ping Luo", "Tian Nian", "Weiliang Deng", "Yiheng Ge", "Yibin Liu", "Zixuan Li", "Dehui Wang", "Zhixuan Liang", "Haohui Xie", "Rijie Zeng", "Yunfei Ge", "Peiqing Cong", "Guannan He", "Zhaoming Han", "Ruocheng Yin", "Jingxiang Guo", "Lunkai Lin", "Tianling Xu", "Hongzhe Bi", "Xuewu Lin", "Tianwei Lin", "Shujie Luo", "Keyu Li", "Ziyan Zhao", "Ke Fan", "Heyang Xu", "Bo Peng", "Wenlong Gao", "Dongjiang Li", "Feng Jin", "Hui Shen", "Jinming Li", "Chaowei Cui", "Yuchen", "Yaxin Peng", "Lingdong Zeng", "Wenlong Dong", "Tengfei Li", "Weijie Ke", "Jun Chen", "Erdemt Bao", "Tian Lan", "Tenglong Liu", "Jin Yang", "Huiping Zhuang", "Baozhi Jia", "Shuai Zhang", "Zhengfeng Zou", "Fangheng Guan", "Tianyi Jia", "Ke Zhou", "Hongjiu Zhang", "Yating Han", "Cheng Fang", "Yixian Zou", "Chongyang Xu", "Qinglun Zhang", "Shen Cheng", "Xiaohe Wang", "Ping Tan", "Haoqiang Fan", "Shuaicheng Liu", "Jiaheng Chen", "Chuxuan Huang", "Chengliang Lin", "Kaijun Luo", "Boyu Yue", "Yi Liu", "Jinyu Chen", "Zichang Tan", "Liming Deng", "Shuo Xu", "Zijian Cai", "Shilong Yin", "Hao Wang", "Hongshan Liu", "Tianyang Li", "Long Shi", "Ran Xu", "Huilin Xu", "Zhengquan Zhang", "Congsheng Xu", "Jinchang Yang", "Feng Xu"], "summary": "Embodied Artificial Intelligence (Embodied AI) is an emerging frontier in\nrobotics, driven by the need for autonomous systems that can perceive, reason,\nand act in complex physical environments. While single-arm systems have shown\nstrong task performance, collaborative dual-arm systems are essential for\nhandling more intricate tasks involving rigid, deformable, and\ntactile-sensitive objects. To advance this goal, we launched the RoboTwin\nDual-Arm Collaboration Challenge at the 2nd MEIS Workshop, CVPR 2025. Built on\nthe RoboTwin Simulation platform (1.0 and 2.0) and the AgileX COBOT-Magic Robot\nplatform, the competition consisted of three stages: Simulation Round 1,\nSimulation Round 2, and a final Real-World Round. Participants totally tackled\n17 dual-arm manipulation tasks, covering rigid, deformable, and tactile-based\nscenarios. The challenge attracted 64 global teams and over 400 participants,\nproducing top-performing solutions like SEM and AnchorDP3 and generating\nvaluable insights into generalizable bimanual policy learning. This report\noutlines the competition setup, task design, evaluation methodology, key\nfindings and future direction, aiming to support future research on robust and\ngeneralizable bimanual manipulation policies. The Challenge Webpage is\navailable at https://robotwin-benchmark.github.io/cvpr-2025-challenge/.", "comment": "Challenge Webpage:\n  https://robotwin-benchmark.github.io/cvpr-2025-challenge/", "pdf_url": "http://arxiv.org/pdf/2506.23351v1", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.MA"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.23351v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.22956", "title": "SPICE-HL3: Single-Photon, Inertial, and Stereo Camera dataset for Exploration of High-Latitude Lunar Landscapes", "authors": ["David Rodr√≠guez-Mart√≠nez", "Dave van der Meer", "Junlin Song", "Abishek Bera", "C. J. P√©rez-del-Pulgar", "Miguel Angel Olivares-Mendez"], "summary": "Exploring high-latitude lunar regions presents an extremely challenging\nvisual environment for robots. The low sunlight elevation angle and minimal\nlight scattering result in a visual field dominated by a high dynamic range\nfeaturing long, dynamic shadows. Reproducing these conditions on Earth requires\nsophisticated simulators and specialized facilities. We introduce a unique\ndataset recorded at the LunaLab from the SnT - University of Luxembourg, an\nindoor test facility designed to replicate the optical characteristics of\nmultiple lunar latitudes. Our dataset includes images, inertial measurements,\nand wheel odometry data from robots navigating seven distinct trajectories\nunder multiple illumination scenarios, simulating high-latitude lunar\nconditions from dawn to night time with and without the aid of headlights,\nresulting in 88 distinct sequences containing a total of 1.3M images. Data was\ncaptured using a stereo RGB-inertial sensor, a monocular monochrome camera, and\nfor the first time, a novel single-photon avalanche diode (SPAD) camera. We\nrecorded both static and dynamic image sequences, with robots navigating at\nslow (5 cm/s) and fast (50 cm/s) speeds. All data is calibrated, synchronized,\nand timestamped, providing a valuable resource for validating perception tasks\nfrom vision-based autonomous navigation to scientific imaging for future lunar\nmissions targeting high-latitude regions or those intended for robots operating\nacross perceptually degraded environments. The dataset can be downloaded from\nhttps://zenodo.org/records/13970078?preview=1, and a visual overview is\navailable at https://youtu.be/d7sPeO50_2I. All supplementary material can be\nfound at https://github.com/spaceuma/spice-hl3.", "comment": "10 pages, 8 figures, dataset", "pdf_url": "http://arxiv.org/pdf/2506.22956v1", "categories": ["cs.RO"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.22956v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.22793", "title": "Offline Reinforcement Learning for Mobility Robustness Optimization", "authors": ["Pegah Alizadeh", "Anastasios Giovanidis", "Pradeepa Ramachandra", "Vasileios Koutsoukis", "Osama Arouk"], "summary": "In this work we revisit the Mobility Robustness Optimisation (MRO) algorithm\nand study the possibility of learning the optimal Cell Individual Offset tuning\nusing offline Reinforcement Learning. Such methods make use of collected\noffline datasets to learn the optimal policy, without further exploration. We\nadapt and apply a sequence-based method called Decision Transformers as well as\na value-based method called Conservative Q-Learning to learn the optimal policy\nfor the same target reward as the vanilla rule-based MRO. The same input\nfeatures related to failures, ping-pongs, and other handover issues are used.\nEvaluation for realistic New Radio networks with 3500 MHz carrier frequency on\na traffic mix including diverse user service types and a specific tunable\ncell-pair shows that offline-RL methods outperform rule-based MRO, offering up\nto 7% improvement. Furthermore, offline-RL can be trained for diverse objective\nfunctions using the same available dataset, thus offering operational\nflexibility compared to rule-based methods.", "comment": "7 pages, double column, 4 figures, 6 tables, conference submission", "pdf_url": "http://arxiv.org/pdf/2506.22793v1", "categories": ["cs.NI", "cs.AI", "cs.PF"], "cate": "cs.NI", "url": "http://arxiv.org/abs/2506.22793v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23281", "title": "On the Feasibility of Deduplicating Compiler Bugs with Bisection", "authors": ["Xintong Zhou", "Zhenyang Xu", "Chengnian Sun"], "summary": "Random testing has proven to be an effective technique for compiler\nvalidation. However, the debugging of bugs identified through random testing\npresents a significant challenge due to the frequent occurrence of duplicate\ntest programs that expose identical compiler bugs. The process to identify\nduplicates is a practical research problem known as bug deduplication. Prior\nmethodologies for compiler bug deduplication primarily rely on program analysis\nto extract bug-related features for duplicate identification, which can result\nin substantial computational overhead and limited generalizability. This paper\ninvestigates the feasibility of employing bisection, a standard debugging\nprocedure largely overlooked in prior research on compiler bug deduplication,\nfor this purpose. Our study demonstrates that the utilization of bisection to\nlocate failure-inducing commits provides a valuable criterion for\ndeduplication, albeit one that requires supplementary techniques for more\naccurate identification. Building on these results, we introduce BugLens, a\nnovel deduplication method that primarily uses bisection, enhanced by the\nidentification of bug-triggering optimizations to minimize false negatives.\nEmpirical evaluations conducted on four real-world datasets demonstrate that\nBugLens significantly outperforms the state-of-the-art analysis-based\nmethodologies Tamer and D3 by saving an average of 26.98% and 9.64% human\neffort to identify the same number of distinct bugs. Given the inherent\nsimplicity and generalizability of bisection, it presents a highly practical\nsolution for compiler bug deduplication in real-world applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23281v1", "categories": ["cs.SE", "cs.PL"], "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.23281v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.22941", "title": "Positioning AI Tools to Support Online Harm Reduction Practice: Applications and Design Directions", "authors": ["Kaixuan Wang", "Jason T. Jacques", "Chenxin Diao"], "summary": "Access to accurate and actionable harm reduction information can directly\nimpact the health outcomes of People Who Use Drugs (PWUD), yet existing online\nchannels often fail to meet their diverse and dynamic needs due to limitations\nin adaptability, accessibility, and the pervasive impact of stigma. Large\nLanguage Models (LLMs) present a novel opportunity to enhance information\nprovision, but their application in such a high-stakes domain is under-explored\nand presents socio-technical challenges. This paper investigates how LLMs can\nbe responsibly designed to support the information needs of PWUD. Through a\nqualitative workshop involving diverse stakeholder groups (academics, harm\nreduction practitioners, and an online community moderator), we explored LLM\ncapabilities, identified potential use cases, and delineated core design\nconsiderations. Our findings reveal that while LLMs can address some existing\ninformation barriers (e.g., by offering responsive, multilingual, and\npotentially less stigmatising interactions), their effectiveness is contingent\nupon overcoming challenges related to ethical alignment with harm reduction\nprinciples, nuanced contextual understanding, effective communication, and\nclearly defined operational boundaries. We articulate design pathways\nemphasising collaborative co-design with experts and PWUD to develop LLM\nsystems that are helpful, safe, and responsibly governed. This work contributes\nempirically grounded insights and actionable design considerations for the\nresponsible development of LLMs as supportive tools within the harm reduction\necosystem.", "comment": "16 pages, 4 figures, with appendix", "pdf_url": "http://arxiv.org/pdf/2506.22941v1", "categories": ["cs.HC", "cs.AI"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.22941v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.22448", "title": "Unsupervised Learning-Based Joint Resource Allocation and Beamforming Design for RIS-Assisted MISO-OFDMA Systems", "authors": ["Yu Ma", "Xingyu Zhou", "Xiao Li", "Le Liang", "Shi Jin"], "summary": "Reconfigurable intelligent surfaces (RIS) are key enablers for 6G wireless\nsystems. This paper studies downlink transmission in an RIS-assisted MISO-OFDMA\nsystem, addressing resource allocation challenges. A two-stage unsupervised\nlearning-based framework is proposed to jointly design RIS phase shifts, BS\nbeamforming, and resource block (RB) allocation. The framework includes\nBeamNet, which predicts RIS phase shifts from CSI, and AllocationNet, which\nallocates RBs using equivalent CSI derived from BeamNet outputs. Active\nbeamforming is implemented via maximum ratio transmission and water-filling. To\nhandle discrete constraints while ensuring differentiability, quantization and\nthe Gumbel-softmax trick are adopted. A customized loss and phased training\nenhance performance under QoS constraints. Simulations show the method achieves\n99.93% of the sum rate of the SCA baseline with only 0.036% of its runtime, and\nit remains robust across varying channel and user conditions.", "comment": "Due to the limitation \"The abstract field cannot be longer than 1,920\n  characters\", the abstract here is shorter than that in the PDF file", "pdf_url": "http://arxiv.org/pdf/2506.22448v1", "categories": ["eess.SP", "cs.AI", "cs.IT", "math.IT"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.22448v1", "date": "2025-06-12", "updated": "2025-06-12"}
{"id": "2506.22855", "title": "Momentum-based Accelerated Algorithm for Distributed Optimization under Sector-Bound Nonlinearity", "authors": ["Mohammadreza Doostmohammadian", "Hamid R. Rabiee"], "summary": "Distributed optimization advances centralized machine learning methods by\nenabling parallel and decentralized learning processes over a network of\ncomputing nodes. This work provides an accelerated consensus-based distributed\nalgorithm for locally non-convex optimization using the gradient-tracking\ntechnique. The proposed algorithm (i) improves the convergence rate by adding\nmomentum towards the optimal state using the heavy-ball method, while (ii)\naddressing general sector-bound nonlinearities over the information-sharing\nnetwork. The link nonlinearity includes any sign-preserving odd sector-bound\nmapping, for example, log-scale data quantization or clipping in practical\napplications. For admissible momentum and gradient-tracking parameters, using\nperturbation theory and eigen-spectrum analysis, we prove convergence even in\nthe presence of sector-bound nonlinearity and for locally non-convex cost\nfunctions. Further, in contrast to most existing weight-stochastic algorithms,\nwe adopt weight-balanced (WB) network design. This WB design and\nperturbation-based analysis allow to handle dynamic directed network of agents\nto address possible time-varying setups due to link failures or packet drops.", "comment": "Journal of the Franklin Institute", "pdf_url": "http://arxiv.org/pdf/2506.22855v1", "categories": ["eess.SY", "cs.DC", "cs.MA", "cs.SY", "eess.SP", "math.OC"], "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.22855v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23321", "title": "AISCliteracy: Assessing Artificial Intelligence and Cybersecurity Literacy Levels and Learning Needs of Students", "authors": ["Devendra Chapagain", "Naresh Kshetri", "Bishwo Prakash Pokharel"], "summary": "Artificial intelligence (AI) is rapidly transforming global industries and\nsocieties, making AI literacy an indispensable skill for future generations.\nWhile AI integration in education is still emerging in Nepal, this study\nfocuses on assessing the current AI literacy levels and identifying learning\nneeds among students in Chitwan District of Nepal. By measuring students'\nunderstanding of AI and pinpointing areas for improvement, this research aims\nto provide actionable recommendations for educational stakeholders. Given the\npivotal role of young learners in navigating a rapidly evolving technological\nlandscape, fostering AI literacy is paramount. This study seeks to understand\nthe current state of AI literacy in Chitwan District by analyzing students'\nknowledge, skills, and attitudes towards AI. The results will contribute to\ndeveloping robust AI education programs for Nepalese schools. This paper offers\na contemporary perspective on AI's role in Nepalese secondary education,\nemphasizing the latest AI tools and technologies. Moreover, the study\nilluminates the potential revolutionary impact of technological innovations on\neducational leadership and student outcomes. A survey was conducted to\nconceptualize the newly emerging concept of AI and cybersecurity among students\nof Chitwan district from different schools and colleges to find the literacy\nrate. The participants in the survey were students between grade 9 to 12. We\nconclude with discussions of the affordances and barriers to bringing AI and\ncybersecurity education to students from lower classes.", "comment": "11 pages, 3 figures", "pdf_url": "http://arxiv.org/pdf/2506.23321v1", "categories": ["cs.CY", "cs.CR"], "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.23321v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.22511", "title": "Lightning the Night with Generative Artificial Intelligence", "authors": ["Tingting Zhou", "Feng Zhang", "Haoyang Fu", "Baoxiang Pan", "Renhe Zhang", "Feng Lu", "Zhixin Yang"], "summary": "The visible light reflectance data from geostationary satellites is crucial\nfor meteorological observations and plays an important role in weather\nmonitoring and forecasting. However, due to the lack of visible light at night,\nit is impossible to conduct continuous all-day weather observations using\nvisible light reflectance data. This study pioneers the use of generative\ndiffusion models to address this limitation. Based on the multi-band thermal\ninfrared brightness temperature data from the Advanced Geostationary Radiation\nImager (AGRI) onboard the Fengyun-4B (FY4B) geostationary satellite, we\ndeveloped a high-precision visible light reflectance retrieval model, called\nReflectance Diffusion (RefDiff), which enables 0.47~\\mu\\mathrm{m},\n0.65~\\mu\\mathrm{m}, and 0.825~\\mu\\mathrm{m} bands visible light reflectance\nretrieval at night. Compared to the classical models, RefDiff not only\nsignificantly improves accuracy through ensemble averaging but also provides\nuncertainty estimation. Specifically, the SSIM index of RefDiff can reach 0.90,\nwith particularly significant improvements in areas with complex cloud\nstructures and thick clouds. The model's nighttime retrieval capability was\nvalidated using VIIRS nighttime product, demonstrating comparable performance\nto its daytime counterpart. In summary, this research has made substantial\nprogress in the ability to retrieve visible light reflectance at night, with\nthe potential to expand the application of nighttime visible light data.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22511v1", "categories": ["cs.CV", "cs.AI", "eess.IV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22511v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.23204", "title": "Data-driven Implementations of Various Generalizations of Balanced Truncation", "authors": ["Umair Zulfiqar"], "summary": "There exist two main frameworks for non-intrusive implementations of\napproximate balanced truncation: the quadrature-based framework and the\nADI-based framework. Both approaches rely solely on samples of the transfer\nfunction to construct truncated balanced models, eliminating the need for\naccess to the original model's state-space realization. Recently, the\nquadrature-based framework has been extended to various generalizations of\nbalanced truncation, including positive-real balanced truncation, bounded-real\nbalanced truncation, and balanced stochastic truncation. While this extension\nis theoretically nonintrusive-meaning it does not require the original\nstate-space realization-it depends on samples of spectral factorizations of the\ntransfer function. Since practical methods for obtaining such samples are\ncurrently unavailable, this extension remains largely a theoretical\ncontribution. In this work, we present a non-intrusive ADI-type framework for\nthese generalized balanced truncation methods that requires only samples of the\noriginal transfer function for implementation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23204v1", "categories": ["eess.SY", "cs.SY"], "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.23204v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.22467", "title": "SegmentAnyMuscle: A universal muscle segmentation model across different locations in MRI", "authors": ["Roy Colglazier", "Jisoo Lee", "Haoyu Dong", "Hanxue Gu", "Yaqian Chen", "Joseph Cao", "Zafer Yildiz", "Zhonghao Liu", "Nicholas Konz", "Jichen Yang", "Jikai Zhang", "Yuwen Chen", "Lin Li", "Adrian Camarena", "Maciej A. Mazurowski"], "summary": "The quantity and quality of muscles are increasingly recognized as important\npredictors of health outcomes. While MRI offers a valuable modality for such\nassessments, obtaining precise quantitative measurements of musculature remains\nchallenging. This study aimed to develop a publicly available model for muscle\nsegmentation in MRIs and demonstrate its applicability across various\nanatomical locations and imaging sequences. A total of 362 MRIs from 160\npatients at a single tertiary center (Duke University Health System, 2016-2020)\nwere included, with 316 MRIs from 114 patients used for model development. The\nmodel was tested on two separate sets: one with 28 MRIs representing common\nsequence types, achieving an average Dice Similarity Coefficient (DSC) of\n88.45%, and another with 18 MRIs featuring less frequent sequences and\nabnormalities such as muscular atrophy, hardware, and significant noise,\nachieving 86.21% DSC. These results demonstrate the feasibility of a fully\nautomated deep learning algorithm for segmenting muscles on MRI across diverse\nsettings. The public release of this model enables consistent, reproducible\nresearch into the relationship between musculature and health.", "comment": "24 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2506.22467v1", "categories": ["eess.SP", "cs.CV"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.22467v1", "date": "2025-06-18", "updated": "2025-06-18"}
{"id": "2506.23208", "title": "Multi-Source COVID-19 Detection via Variance Risk Extrapolation", "authors": ["Runtian Yuan", "Qingqiu Li", "Junlin Hou", "Jilan Xu", "Yuejie Zhang", "Rui Feng", "Hao Chen"], "summary": "We present our solution for the Multi-Source COVID-19 Detection Challenge,\nwhich aims to classify chest CT scans into COVID and Non-COVID categories\nacross data collected from four distinct hospitals and medical centers. A major\nchallenge in this task lies in the domain shift caused by variations in imaging\nprotocols, scanners, and patient populations across institutions. To enhance\nthe cross-domain generalization of our model, we incorporate Variance Risk\nExtrapolation (VREx) into the training process. VREx encourages the model to\nmaintain consistent performance across multiple source domains by explicitly\nminimizing the variance of empirical risks across environments. This\nregularization strategy reduces overfitting to center-specific features and\npromotes learning of domain-invariant representations. We further apply Mixup\ndata augmentation to improve generalization and robustness. Mixup interpolates\nboth the inputs and labels of randomly selected pairs of training samples,\nencouraging the model to behave linearly between examples and enhancing its\nresilience to noise and limited data. Our method achieves an average macro F1\nscore of 0.96 across the four sources on the validation set, demonstrating\nstrong generalization.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23208v1", "categories": ["eess.IV", "cs.CV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.23208v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.22583", "title": "Supra-threshold control of peripheral LOD", "authors": ["Benjamin Watson", "Neff Walker", "Larry F Hodges"], "summary": "Level of detail (LOD) is widely used to control visual feedback in\ninteractive applications. LOD control is typically based on perception at\nthreshold - the conditions in which a stimulus first becomes perceivable. Yet\nmost LOD manipulations are quite perceivable and occur well above threshold.\nMoreover, research shows that supra-threshold perception differs drastically\nfrom perception at threshold. In that case, should supra-threshold LOD control\nalso differ from LOD control at threshold?\n  In two experiments, we examine supra-threshold LOD control in the visual\nperiphery and find that indeed, it should differ drastically from LOD control\nat threshold. Specifically, we find that LOD must support a task-dependent\nlevel of reliable perceptibility. Above that level, perceptibility of LOD\ncontrol manipulations should be minimized, and detail contrast is a better\npredictor of perceptibility than detail size. Below that level, perceptibility\nmust be maximized, and LOD should be improved as eccentricity rises or contrast\ndrops. This directly contradicts prevailing threshold-based LOD control\nschemes, and strongly suggests a reexamination of LOD control for foveal\ndisplay.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22583v1", "categories": ["cs.HC", "cs.GR"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.22583v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.23906", "title": "Segmented Operations using Matrix Multiplications", "authors": ["Aleksandros Sobczyk", "Giuseppe Sorrentino", "Anastasios Zouzias"], "summary": "Specialized computational units that perform small matrix multiplications as\nprimitive operations are typically present in modern accelerators. However,\nthese units are often underutilized for many fundamental operations besides\ndense matrix multiplications. The analysis of algorithms for such architectures\nis currently stagnated due to the lack of a rigorous theoretical model of\ncomputation that captures their characteristics. In this work, we propose\nMMV-RAM, a computational model tailored to matrix multiplication accelerators.\nMMV-RAM judiciously extends the Vector-RAM model with an additional processing\nunit that multiplies two matrices of sizes $n\\times s$ and $s\\times s$ in a\nsingle parallel step, where $s$ is a model parameter. We provide a detailed\ntheoretical analysis of the model, and carefully balance the computational\npower between the matrix and vector units, guided by the circuit complexity\nlower bound that parity is not in AC[0].\n  In MMV-RAM, we study algorithms for segmented scan and sum, two fundamental\nparallel primitives. We propose a segmented scan algorithm that uses matrix\nmultiplications to perform speculative block-scan computations, which runs in\n$O(\\log_s(n))$ steps. In contrast, we show that any algorithm that uses only\nthe vector unit of MMV-RAM requires\n$\\Omega\\left(\\frac{\\log_2(n)}{\\log_2\\log_2(n)}\\right)$ steps. We further apply\nthese techniques to obtain similar theoretical speedups for element-wise vector\nmultiplication and matrix multiplication. Beyond the worst-case complexity\nanalysis, we propose algorithms for segmented operations that could lead to\nhighly efficient and pragmatic implementations. For example, we observe that\nsegmented sum is a combination of three elementary parallel primitives: scan,\ncompress, and vector differentiation. As a case study, we implement...", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23906v1", "categories": ["cs.DS", "cs.CC", "cs.DC"], "cate": "cs.DS", "url": "http://arxiv.org/abs/2506.23906v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22602", "title": "Are Fast Methods Stable in Adversarially Robust Transfer Learning?", "authors": ["Joshua C. Zhao", "Saurabh Bagchi"], "summary": "Transfer learning is often used to decrease the computational cost of model\ntraining, as fine-tuning a model allows a downstream task to leverage the\nfeatures learned from the pre-training dataset and quickly adapt them to a new\ntask. This is particularly useful for achieving adversarial robustness, as\nadversarially training models from scratch is very computationally expensive.\nHowever, high robustness in transfer learning still requires adversarial\ntraining during the fine-tuning phase, which requires up to an order of\nmagnitude more time than standard fine-tuning. In this work, we revisit the use\nof the fast gradient sign method (FGSM) in robust transfer learning to improve\nthe computational cost of adversarial fine-tuning. We surprisingly find that\nFGSM is much more stable in adversarial fine-tuning than when training from\nscratch. In particular, FGSM fine-tuning does not suffer from any issues with\ncatastrophic overfitting at standard perturbation budgets of $\\varepsilon=4$ or\n$\\varepsilon=8$. This stability is further enhanced with parameter-efficient\nfine-tuning methods, where FGSM remains stable even up to $\\varepsilon=32$ for\nlinear probing. We demonstrate how this stability translates into performance\nacross multiple datasets. Compared to fine-tuning with the more commonly used\nmethod of projected gradient descent (PGD), on average, FGSM only loses 0.39%\nand 1.39% test robustness for $\\varepsilon=4$ and $\\varepsilon=8$ while using\n$4\\times$ less training time. Surprisingly, FGSM may not only be a\nsignificantly more efficient alternative to PGD in adversarially robust\ntransfer learning but also a well-performing one.", "comment": "13 pages", "pdf_url": "http://arxiv.org/pdf/2506.22602v1", "categories": ["cs.LG", "stat.ML"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22602v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.23080", "title": "AI's Euclid's Elements Moment: From Language Models to Computable Thought", "authors": ["Xinmin Fang", "Lingfeng Tao", "Zhengxiong Li"], "summary": "This paper presents a comprehensive five-stage evolutionary framework for\nunderstanding the development of artificial intelligence, arguing that its\ntrajectory mirrors the historical progression of human cognitive technologies.\nWe posit that AI is advancing through distinct epochs, each defined by a\nrevolutionary shift in its capacity for representation and reasoning, analogous\nto the inventions of cuneiform, the alphabet, grammar and logic, mathematical\ncalculus, and formal logical systems. This \"Geometry of Cognition\" framework\nmoves beyond mere metaphor to provide a systematic, cross-disciplinary model\nthat not only explains AI's past architectural shifts-from expert systems to\nTransformers-but also charts a concrete and prescriptive path forward.\nCrucially, we demonstrate that this evolution is not merely linear but\nreflexive: as AI advances through these stages, the tools and insights it\ndevelops create a feedback loop that fundamentally reshapes its own underlying\narchitecture. We are currently transitioning into a \"Metalinguistic Moment,\"\ncharacterized by the emergence of self-reflective capabilities like\nChain-of-Thought prompting and Constitutional AI. The subsequent stages, the\n\"Mathematical Symbolism Moment\" and the \"Formal Logic System Moment,\" will be\ndefined by the development of a computable calculus of thought, likely through\nneuro-symbolic architectures and program synthesis, culminating in provably\naligned and reliable AI that reconstructs its own foundational representations.\nThis work serves as the methodological capstone to our trilogy, which\npreviously explored the economic drivers (\"why\") and cognitive nature (\"what\")\nof AI. Here, we address the \"how,\" providing a theoretical foundation for\nfuture research and offering concrete, actionable strategies for startups and\ndevelopers aiming to build the next generation of intelligent systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23080v1", "categories": ["cs.AI"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.23080v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23643", "title": "Act-With-Think: Chunk Auto-Regressive Modeling for Generative Recommendation", "authors": ["Yifan Wang", "Weinan Gan", "Longtao Xiao", "Jieming Zhu", "Heng Chang", "Haozhao Wang", "Rui Zhang", "Zhenhua Dong", "Ruiming Tang", "Ruixuan Li"], "summary": "Generative recommendation (GR) typically encodes behavioral or semantic\naspects of item information into discrete tokens, leveraging the standard\nautoregressive (AR) generation paradigm to make predictions. However, existing\nmethods tend to overlook their intrinsic relationship, that is, the semantic\nusually provides some reasonable explainability \"$\\textbf{why}$\" for the\nbehavior \"$\\textbf{what}$\", which may constrain the full potential of GR. To\nthis end, we present Chunk AutoRegressive Modeling (CAR), a new generation\nparadigm following the decision pattern that users usually think semantic\naspects of items (e.g. brand) and then take actions on target items (e.g.\npurchase). Our CAR, for the $\\textit{first time}$, incorporates semantics\n(SIDs) and behavior (UID) into a single autoregressive transformer from an\n``act-with-think'' dual perspective via chunk-level autoregression.\nSpecifically, CAR packs SIDs and UID into a conceptual chunk for item unified\nrepresentation, allowing each decoding step to make a holistic prediction.\nExperiments show that our CAR significantly outperforms existing methods based\non traditional AR, improving Recall@5 by 7.93% to 22.30%. Furthermore, we\nverify the scaling effect between model performance and SIDs bit number,\ndemonstrating that CAR preliminary emulates a kind of slow-thinking style\nmechanism akin to the reasoning processes observed in large language models\n(LLMs).", "comment": "9 pages, 2 figures", "pdf_url": "http://arxiv.org/pdf/2506.23643v1", "categories": ["cs.IR"], "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.23643v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22858", "title": "Mind the Gap: Entity-Preserved Context-Aware ASR Structured Transcriptions", "authors": ["Duygu Altinok"], "summary": "Automatic Speech Recognition (ASR) systems, such as Whisper, achieve high\ntranscription accuracy but struggle with named entities and numerical data,\nespecially when proper formatting is required. These issues increase word error\nrate (WER) and impair semantic understanding in critical domains like legal,\nfinancial, and medical applications. We propose a novel training approach that\nextends the semantic context of ASR models by adding overlapping context\nwindows during training. By sliding 5-second overlaps on both sides of\n30-second chunks, we create a 40-second \"effective semantic window,\" improving\nentity recognition and formatting while focusing predictions on the central 30\nseconds. To address entities spanning chunk boundaries, we reassign such\nentities entirely to the right-hand chunk, ensuring proper formatting.\nAdditionally, enriched training data with embedded entity labels enables the\nmodel to learn both recognition and type-specific formatting. Evaluated on the\nSpoken Wikipedia dataset, our method improves performance across semantic\ntasks, including named entity recognition (NER) and entity formatting. These\nresults highlight the effectiveness of context-aware training in addressing ASR\nlimitations for long-form transcription and complex entity recognition tasks.", "comment": "This is the accepted version of an article accepted to the TSD 2025\n  conference, published in Springer Lecture Notes in Artificial Intelligence\n  (LNAI). The final authenticated version is available online at SpringerLink", "pdf_url": "http://arxiv.org/pdf/2506.22858v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.22858v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.22644", "title": "Evaluating Hybrid Retrieval Augmented Generation using Dynamic Test Sets: LiveRAG Challenge", "authors": ["Chase Fensore", "Kaustubh Dhole", "Joyce C Ho", "Eugene Agichtein"], "summary": "We present our submission to the LiveRAG Challenge 2025, which evaluates\nretrieval-augmented generation (RAG) systems on dynamic test sets using the\nFineWeb-10BT corpus. Our final hybrid approach combines sparse (BM25) and dense\n(E5) retrieval methods and then aims to generate relevant and faithful answers\nwith Falcon3-10B-Instruct. Through systematic evaluation on 200 synthetic\nquestions generated with DataMorgana across 64 unique question-user\ncombinations, we demonstrate that neural re-ranking with RankLLaMA improves MAP\nfrom 0.523 to 0.797 (52% relative improvement) but introduces prohibitive\ncomputational costs (84s vs 1.74s per question). While DSPy-optimized prompting\nstrategies achieved higher semantic similarity (0.771 vs 0.668), their 0%\nrefusal rates raised concerns about over-confidence and generalizability. Our\nsubmitted hybrid system without re-ranking achieved 4th place in faithfulness\nand 11th place in correctness among 25 teams. Analysis across question\ncategories reveals that vocabulary alignment between questions and documents\nwas the strongest predictor of performance on our development set, with\ndocument-similar phrasing improving cosine similarity from 0.562 to 0.762.", "comment": "4 pages, 3 tables, 2 figures. Accepted at the SIGIR LiveRAG Workshop\n  2025 (Submission 2664)", "pdf_url": "http://arxiv.org/pdf/2506.22644v1", "categories": ["cs.CL", "cs.IR"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.22644v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.23093", "title": "A residual driven multiscale method for Darcy's flow in perforated domains", "authors": ["Wei Xie", "Shubin Fu", "Yin Yang", "Yunqing Huang"], "summary": "In this paper, we present a residual-driven multiscale method for simulating\nDarcy flow in perforated domains, where complex geometries and highly\nheterogeneous permeability make direct simulations computationally expensive.\nTo address this, we introduce a velocity elimination technique that\nreformulates the mixed velocity-pressure system into a pressure-only\nformulation, significantly reducing complexity by focusing on the dominant\npressure variable. Our method is developed within the Generalized Multiscale\nFinite Element Method (GMsFEM) framework. For each coarse block, we construct\noffline basis functions from local spectral problems that capture key geometric\nand physical features. Online basis functions are then adaptively enriched\nusing residuals, allowing the method to incorporate global effects such as\nsource terms and boundary conditions, thereby improving accuracy. We provide\ndetailed error analysis demonstrating how the offline and online spaces\ncontribute to the accuracy and efficiency of the solution. Numerical\nexperiments confirm the method's effectiveness, showing substantial reductions\nin computational cost while maintaining high accuracy, particularly through\nadaptive online enrichment. These results highlight the method's potential for\nefficient and accurate simulation of Darcy flow in complex, heterogeneous\nperforated domains.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23093v1", "categories": ["math.NA", "cs.NA"], "cate": "math.NA", "url": "http://arxiv.org/abs/2506.23093v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23869", "title": "Scaling Self-Supervised Representation Learning for Symbolic Piano Performance", "authors": ["Louis Bradshaw", "Honglu Fan", "Alexander Spangher", "Stella Biderman", "Simon Colton"], "summary": "We study the capabilities of generative autoregressive transformer models\ntrained on large amounts of symbolic solo-piano transcriptions. After first\npretraining on approximately 60,000 hours of music, we use a comparatively\nsmaller, high-quality subset, to finetune models to produce musical\ncontinuations, perform symbolic classification tasks, and produce\ngeneral-purpose contrastive MIDI embeddings by adapting the SimCLR framework to\nsymbolic music. When evaluating piano continuation coherence, our generative\nmodel outperforms leading symbolic generation techniques and remains\ncompetitive with proprietary audio generation models. On MIR classification\nbenchmarks, frozen representations from our contrastive model achieve\nstate-of-the-art results in linear probe experiments, while direct finetuning\ndemonstrates the generalizability of pretrained representations, often\nrequiring only a few hundred labeled examples to specialize to downstream\ntasks.", "comment": "ISMIR (2025)", "pdf_url": "http://arxiv.org/pdf/2506.23869v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.23869v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22949", "title": "A Study on Semi-Supervised Detection of DDoS Attacks under Class Imbalance", "authors": ["Ehsan Hallaji", "Vaishnavi Shanmugam", "Roozbeh Razavi-Far", "Mehrdad Saif"], "summary": "One of the most difficult challenges in cybersecurity is eliminating\nDistributed Denial of Service (DDoS) attacks. Automating this task using\nartificial intelligence is a complex process due to the inherent class\nimbalance and lack of sufficient labeled samples of real-world datasets. This\nresearch investigates the use of Semi-Supervised Learning (SSL) techniques to\nimprove DDoS attack detection when data is imbalanced and partially labeled. In\nthis process, 13 state-of-the-art SSL algorithms are evaluated for detecting\nDDoS attacks in several scenarios. We evaluate their practical efficacy and\nshortcomings, including the extent to which they work in extreme environments.\nThe results will offer insight into designing intelligent Intrusion Detection\nSystems (IDSs) that are robust against class imbalance and handle partially\nlabeled data.", "comment": "Accepted for publication in IEEE CCECE 2025", "pdf_url": "http://arxiv.org/pdf/2506.22949v1", "categories": ["cs.CR", "cs.AI", "cs.LG"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.22949v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23514", "title": "MGPRL: Distributed Multi-Gaussian Processes for Wi-Fi-based Multi-Robot Relative Localization in Large Indoor Environments", "authors": ["Sai Krishna Ghanta", "Ramviyas Parasuraman"], "summary": "Relative localization is a crucial capability for multi-robot systems\noperating in GPS-denied environments. Existing approaches for multi-robot\nrelative localization often depend on costly or short-range sensors like\ncameras and LiDARs. Consequently, these approaches face challenges such as high\ncomputational overhead (e.g., map merging) and difficulties in disjoint\nenvironments. To address this limitation, this paper introduces MGPRL, a novel\ndistributed framework for multi-robot relative localization using convex-hull\nof multiple Wi-Fi access points (AP). To accomplish this, we employ\nco-regionalized multi-output Gaussian Processes for efficient Radio Signal\nStrength Indicator (RSSI) field prediction and perform uncertainty-aware\nmulti-AP localization, which is further coupled with weighted convex hull-based\nalignment for robust relative pose estimation. Each robot predicts the RSSI\nfield of the environment by an online scan of APs in its environment, which are\nutilized for position estimation of multiple APs. To perform relative\nlocalization, each robot aligns the convex hull of its predicted AP locations\nwith that of the neighbor robots. This approach is well-suited for devices with\nlimited computational resources and operates solely on widely available Wi-Fi\nRSSI measurements without necessitating any dedicated pre-calibration or\noffline fingerprinting. We rigorously evaluate the performance of the proposed\nMGPRL in ROS simulations and demonstrate it with real-world experiments,\ncomparing it against multiple state-of-the-art approaches. The results showcase\nthat MGPRL outperforms existing methods in terms of localization accuracy and\ncomputational efficiency. Finally, we open source MGPRL as a ROS package\nhttps://github.com/herolab-uga/MGPRL.", "comment": "Accepted to IROS 2025", "pdf_url": "http://arxiv.org/pdf/2506.23514v1", "categories": ["cs.RO", "cs.AI", "cs.MA"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.23514v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23023", "title": "Scenario-Based Hierarchical Reinforcement Learning for Automated Driving Decision Making", "authors": ["M. Youssef Abdelhamid", "Lennart Vater", "Zlatan Ajanovic"], "summary": "Developing decision-making algorithms for highly automated driving systems\nremains challenging, since these systems have to operate safely in an open and\ncomplex environments. Reinforcement Learning (RL) approaches can learn\ncomprehensive decision policies directly from experience and already show\npromising results in simple driving tasks. However, current approaches fail to\nachieve generalizability for more complex driving tasks and lack learning\nefficiency. Therefore, we present Scenario-based Automated Driving\nReinforcement Learning (SAD-RL), the first framework that integrates\nReinforcement Learning (RL) of hierarchical policy in a scenario-based\nenvironment. A high-level policy selects maneuver templates that are evaluated\nand executed by a low-level control logic. The scenario-based environment\nallows to control the training experience for the agent and to explicitly\nintroduce challenging, but rate situations into the training process. Our\nexperiments show that an agent trained using the SAD-RL framework can achieve\nsafe behaviour in easy as well as challenging situations efficiently. Our\nablation studies confirmed that both HRL and scenario diversity are essential\nfor achieving these results.", "comment": "6 pages, 10 figures, submitted to a conference", "pdf_url": "http://arxiv.org/pdf/2506.23023v1", "categories": ["cs.RO", "cs.AI", "cs.LG"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.23023v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.22875", "title": "Reliable Image Transmission in CPS-based Pub/Sub", "authors": ["Everson Flores", "Bruna Guterres", "Thomaz Pereira Junior", "Paula Barros", "Alberto Cabral", "Cristiana Lima Dora", "Marcelo Malheiros", "Marcelo Pias"], "summary": "Developments in communication and automation have driven the expansion of\ndistributed networks, essential for IoT and CPS development in industrial\napplications requiring reliable image processing and real-time adaptability.\nAlthough broadly adopted, there is a literature gap regarding the performance\nof MQTT protocol for image sharing and transmission under high-traffic\nscenarios with intermittent connectivity, restricting its use in critical IoT\nand CPS applications. In this context, the present work examines the\nreliability of real-time image transmission in IoT and CPS industrial systems\nthat utilize the MQTT-based publish/subscribe communication model. It focuses\non scenarios with network interruptions and high data traffic, evaluating the\nperformance of a distributed system through a series of controlled testbed\nvalidation experiments. Experimental validation demonstrated that while the\nMQTT-based system sustains reliable transmission under normal conditions, its\nrecovery capability depends on the failure point, with complete restoration\noccurring when disruptions affect the Orchestrator Node and partial recovery\nwhen the Producer Node or Broker are affected. The study also confirmed that\nthe system prevents duplicate errors and adapts well to increasing network\ndemands, reinforcing its suitability for industrial applications that require\nefficient and resilient data handling.", "comment": "10 pages, 4 figures", "pdf_url": "http://arxiv.org/pdf/2506.22875v1", "categories": ["cs.NI", "cs.DC"], "cate": "cs.NI", "url": "http://arxiv.org/abs/2506.22875v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23534", "title": "Improving vulnerability type prediction and line-level detection via adversarial training-based data augmentation and multi-task learning", "authors": ["Siyu Chen", "Jiongyi Yang", "Xiang Chen", "Menglin Zheng", "Minnan Wei", "Xiaolin Ju"], "summary": "Context: Software vulnerabilities pose a significant threat to modern\nsoftware systems, as evidenced by the growing number of reported\nvulnerabilities and cyberattacks. These escalating trends underscore the urgent\nneed for effective approaches that can automatically detect and understand\nsoftware vulnerabilities. Objective: However, the scarcity of labeled samples\nand the class imbalance issue in vulnerability datasets present significant\nchallenges for both Vulnerability Type Prediction (VTP) and Line-level\nVulnerability Detection (LVD), especially for rare yet critical vulnerability\ntypes. Moreover, most existing studies treat VTP and LVD as independent tasks,\noverlooking their inherent correlation, which limits the potential to leverage\nshared semantic patterns across tasks. Methods: To address these limitations,\nwe propose a unified approach that integrates Embedding-Layer Driven\nAdversarial Training (EDAT) with Multi-task Learning (MTL). Specifically, EDAT\nenhances model robustness by introducing adversarial perturbations to\nidentifier embeddings, guided by semantic importance. Meanwhile, MTL improves\noverall performance by leveraging shared representations and inter-task\ncorrelations between VTP and LVD. Results: Extensive experiments demonstrate\nthat our proposed approach outperforms state-of-the-art baselines on both VTP\nand LVD tasks. For VTP, it yields notable improvements in accuracy, precision,\nrecall, and F1-score, particularly in identifying rare vulnerability types.\nSimilarly, for LVD, our approach enhances line-level detection accuracy while\nsignificantly reducing false positives. Conclusion: Our study demonstrates that\ncombining EDAT with MTL provides a unified solution that improves performance\non both tasks and warrants further investigation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23534v1", "categories": ["cs.SE"], "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.23534v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22968", "title": "Against 'softmaxing' culture", "authors": ["Daniel Mwesigwa"], "summary": "AI is flattening culture. Evaluations of \"culture\" are showing the myriad\nways in which large AI models are homogenizing language and culture, averaging\nout rich linguistic differences into generic expressions. I call this\nphenomenon \"softmaxing culture,\" and it is one of the fundamental challenges\nfacing AI evaluations today. Efforts to improve and strengthen evaluations of\nculture are central to the project of cultural alignment in large AI systems.\nThis position paper argues that machine learning (ML) and human-computer\ninteraction (HCI) approaches to evaluation are limited. I propose two key\nshifts. First, instead of asking \"what is culture?\" at the start of system\nevaluations, I propose beginning with the question: \"when is culture?\" Second,\nwhile I acknowledge the philosophical claim that cultural universals exist, the\nchallenge is not simply to describe them, but to situate them in relation to\ntheir particulars. Taken together, these conceptual shifts invite evaluation\napproaches that move beyond technical requirements, toward perspectives more\nresponsive to the complexities of culture.", "comment": "7 pages", "pdf_url": "http://arxiv.org/pdf/2506.22968v1", "categories": ["cs.HC", "cs.AI"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.22968v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.22465", "title": "Preconditioned Conjugate Gradient for MIMO-AFDM System", "authors": ["Jun Zhu", "Yin Xu", "Dazhi He", "Haoyang Li", "Yunfeng Guan", "Wenjun Zhang"], "summary": "Affine frequency division multiplexing (AFDM) is a promising chirp-assisted\nmulticarrier waveform for future high mobility communications. A significant\nchallenge in MIMO-AFDM systems is the multi-user interference (MUI), which can\nbe effectively addressed by employing precoding techniques. However, the\ncomplexity introduced by AFDM makes the precoding process computationally\nexpensive and challenging. To overcome this issue, We combine AFDM channel\nsparse property and using Preconditioned Conjugate Gradient (PCG) method to\niteratively process the precoding, thereby reducing the complexity of the\nprecoding design. Simulation results demonstrate that the proposed\nsparsification approach, coupled with the PCG method, achieving quite precoding\nperformance while significantly reducing computational complexity. This makes\nthe application of AFDM more feasible and efficient for high-mobility\ncommunication scenarios, paving the way for its broader implementation in\nnext-generation communication systems.", "comment": "arXiv admin note: text overlap with arXiv:2503.10525", "pdf_url": "http://arxiv.org/pdf/2506.22465v1", "categories": ["eess.SP", "cs.IT", "math.IT"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.22465v1", "date": "2025-06-18", "updated": "2025-06-18"}
{"id": "2506.22875", "title": "Reliable Image Transmission in CPS-based Pub/Sub", "authors": ["Everson Flores", "Bruna Guterres", "Thomaz Pereira Junior", "Paula Barros", "Alberto Cabral", "Cristiana Lima Dora", "Marcelo Malheiros", "Marcelo Pias"], "summary": "Developments in communication and automation have driven the expansion of\ndistributed networks, essential for IoT and CPS development in industrial\napplications requiring reliable image processing and real-time adaptability.\nAlthough broadly adopted, there is a literature gap regarding the performance\nof MQTT protocol for image sharing and transmission under high-traffic\nscenarios with intermittent connectivity, restricting its use in critical IoT\nand CPS applications. In this context, the present work examines the\nreliability of real-time image transmission in IoT and CPS industrial systems\nthat utilize the MQTT-based publish/subscribe communication model. It focuses\non scenarios with network interruptions and high data traffic, evaluating the\nperformance of a distributed system through a series of controlled testbed\nvalidation experiments. Experimental validation demonstrated that while the\nMQTT-based system sustains reliable transmission under normal conditions, its\nrecovery capability depends on the failure point, with complete restoration\noccurring when disruptions affect the Orchestrator Node and partial recovery\nwhen the Producer Node or Broker are affected. The study also confirmed that\nthe system prevents duplicate errors and adapts well to increasing network\ndemands, reinforcing its suitability for industrial applications that require\nefficient and resilient data handling.", "comment": "10 pages, 4 figures", "pdf_url": "http://arxiv.org/pdf/2506.22875v1", "categories": ["cs.NI", "cs.DC"], "cate": "cs.NI", "url": "http://arxiv.org/abs/2506.22875v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23774", "title": "Leveraging a Multi-Agent LLM-Based System to Educate Teachers in Hate Incidents Management", "authors": ["Ewelina Gajewska", "Michal Wawer", "Katarzyna Budzynska", "Jaros≈Çaw A. Chudziak"], "summary": "Computer-aided teacher training is a state-of-the-art method designed to\nenhance teachers' professional skills effectively while minimising concerns\nrelated to costs, time constraints, and geographical limitations. We\ninvestigate the potential of large language models (LLMs) in teacher education,\nusing a case of teaching hate incidents management in schools. To this end, we\ncreate a multi-agent LLM-based system that mimics realistic situations of hate,\nusing a combination of retrieval-augmented prompting and persona modelling. It\nis designed to identify and analyse hate speech patterns, predict potential\nescalation, and propose effective intervention strategies. By integrating\npersona modelling with agentic LLMs, we create contextually diverse simulations\nof hate incidents, mimicking real-life situations. The system allows teachers\nto analyse and understand the dynamics of hate incidents in a safe and\ncontrolled environment, providing valuable insights and practical knowledge to\nmanage such situations confidently in real life. Our pilot evaluation\ndemonstrates teachers' enhanced understanding of the nature of annotator\ndisagreements and the role of context in hate speech interpretation, leading to\nthe development of more informed and effective strategies for addressing hate\nin classroom settings.", "comment": "8 pages, 1 figure", "pdf_url": "http://arxiv.org/pdf/2506.23774v1", "categories": ["cs.CY", "cs.HC", "H.1.2"], "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.23774v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22513", "title": "Automated Defect Identification and Categorization in NDE 4.0 with the Application of Artificial Intelligence", "authors": ["Aditya Sharma"], "summary": "This investigation attempts to create an automated framework for fault\ndetection and organization for usage in contemporary radiography, as per NDE\n4.0. The review's goals are to address the lack of information that is\nsufficiently explained, learn how to make the most of virtual defect increase,\nand determine whether the framework is viable by using NDE measurements. As its\nbasic information source, the technique consists of compiling and categorizing\n223 CR photographs of airplane welds. Information expansion systems, such as\nvirtual defect increase and standard increase, are used to work on the\npreparation dataset. A modified U-net model is prepared using the improved data\nto produce semantic fault division veils. To assess the effectiveness of the\nmodel, NDE boundaries such as Case, estimating exactness, and misleading call\nrate are used. Tiny a90/95 characteristics, which provide strong\ndifferentiating evidence of flaws, reveal that the suggested approach achieves\nexceptional awareness in defect detection. Considering a 90/95, size error, and\nfake call rate in the weld area, the consolidated expansion approach clearly\nwins. Due to the framework's fast derivation speed, large images can be broken\ndown efficiently and quickly. Professional controllers evaluate the transmitted\nsystem in the field and believe that it has a guarantee as a support device in\nthe testing cycle, irrespective of particular equipment cut-off points and\nprogramming resemblance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22513v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22513v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.23242", "title": "Revisiting Z Transform Laplace Inversion: To Correct flaws in Signal and System Theory", "authors": ["Yuxin Yang", "Hang Zhou", "Chaojie Li", "Xin Li", "Yingyi Yan", "Mingyang Zheng"], "summary": "This paper revisits the classical formulation of the Z-transform and its\nrelationship to the inverse Laplace transform (L-1), originally developed by\nRagazzini in sampled-data theory. It identifies a longstanding mathematical\noversight in standard derivations, which typically neglect the contribution\nfrom the infinite arc in the complex plane during inverse Laplace evaluation.\nThis omission leads to inconsistencies, especially at discontinuities such as t\n= 0. By incorporating the full Bromwich contour, including all boundary\ncontributions, we restore internal consistency between L-1 and the Z-transform,\naligning the corrected L-1 with results from Discrete-Time Fourier Transform\n(DTFT) aliasing theory. Consequently, this necessitates a structural revision\nof the Z-transform, inverse Laplace transform, and the behavior of the\nHeaviside step function at discontinuities, providing a more accurate\nfoundation for modeling and analysis of sampled-data systems.", "comment": "This work is to be submitted to IEEE transactions on automatic\n  control", "pdf_url": "http://arxiv.org/pdf/2506.23242v1", "categories": ["eess.SY", "cs.SY"], "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.23242v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.22468", "title": "Dimensionality Reduction on IoT Monitoring Data of Smart Building for Energy Consumption Forecasting", "authors": ["Konstantinos Koutras", "Agorakis Bompotas", "Constantinos Halkiopoulos", "Athanasios Kalogeras", "Christos Alexakos"], "summary": "The Internet of Things (IoT) plays a major role today in smart building\ninfrastructures, from simple smart-home applications, to more sophisticated\nindustrial type installations. The vast amounts of data generated from relevant\nsystems can be processed in different ways revealing important information.\nThis is especially true in the era of edge computing, when advanced data\nanalysis and decision-making is gradually moving to the edge of the network\nwhere devices are generally characterised by low computing resources. In this\ncontext, one of the emerging main challenges is related to maintaining data\nanalysis accuracy even with less data that can be efficiently handled by low\nresource devices. The present work focuses on correlation analysis of data\nretrieved from a pilot IoT network installation monitoring a small smart office\nby means of environmental and energy consumption sensors. The research\nmotivation was to find statistical correlation between the monitoring variables\nthat will allow the use of machine learning (ML) prediction algorithms for\nenergy consumption reducing input parameters. For this to happen, a series of\nhypothesis tests for the correlation of three different environmental variables\nwith the energy consumption were carried out. A total of ninety tests were\nperformed, thirty for each pair of variables. In these tests, p-values showed\nthe existence of strong or semi-strong correlation with two environmental\nvariables, and of a weak correlation with a third one. Using the proposed\nmethodology, we manage without examining the entire data set to exclude weak\ncorrelated variables while keeping the same score of accuracy.", "comment": "Version of submitted paper on 2023 IEEE International Smart Cities\n  Conference (ISC2), 1-6, 2023", "pdf_url": "http://arxiv.org/pdf/2506.22468v1", "categories": ["eess.SP", "cs.AI"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.22468v1", "date": "2025-06-19", "updated": "2025-06-19"}
{"id": "2506.23259", "title": "Improving Myocardial Infarction Detection via Synthetic ECG Pretraining", "authors": ["Lachin Naghashyar"], "summary": "Myocardial infarction is a major cause of death globally, and accurate early\ndiagnosis from electrocardiograms (ECGs) remains a clinical priority. Deep\nlearning models have shown promise for automated ECG interpretation, but\nrequire large amounts of labeled data, which are often scarce in practice. We\npropose a physiology-aware pipeline that (i) synthesizes 12-lead ECGs with\ntunable MI morphology and realistic noise, and (ii) pre-trains recurrent and\ntransformer classifiers with self-supervised masked-autoencoding plus a joint\nreconstruction-classification objective. We validate the realism of synthetic\nECGs via statistical and visual analysis, confirming that key morphological\nfeatures are preserved. Pretraining on synthetic data consistently improved\nclassification performance, particularly in low-data settings, with AUC gains\nof up to 4 percentage points. These results show that controlled synthetic ECGs\ncan help improve MI detection when real clinical data is limited.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23259v1", "categories": ["eess.IV", "cs.CV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.23259v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.22685", "title": "Mitigating Semantic Collapse in Generative Personalization with a Surprisingly Simple Test-Time Embedding Adjustment", "authors": ["Anh Bui", "Trang Vu", "Trung Le", "Junae Kim", "Tamas Abraham", "Rollin Omari", "Amar Kaur", "Dinh Phung"], "summary": "In this paper, we investigate the semantic collapsing problem in generative\npersonalization, an under-explored topic where the learned visual concept\n($V^*$) gradually shifts from its original textual meaning and comes to\ndominate other concepts in multi-concept input prompts. This issue not only\nreduces the semantic richness of complex input prompts like \"a photo of $V^*$\nwearing glasses and playing guitar\" into simpler, less contextually rich forms\nsuch as \"a photo of $V^*$\" but also leads to simplified output images that fail\nto capture the intended concept.\n  We identify the root cause as unconstrained optimisation, which allows the\nlearned embedding $V^*$ to drift arbitrarily in the embedding space, both in\ndirection and magnitude. To address this, we propose a simple yet effective\ntraining-free method that adjusts the magnitude and direction of pre-trained\nembedding at inference time, effectively mitigating the semantic collapsing\nproblem. Our method is broadly applicable across different personalization\nmethods and demonstrates significant improvements in text-image alignment in\ndiverse use cases. Our code is anonymously published at\nhttps://anonymous.4open.science/r/Embedding-Adjustment.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22685v1", "categories": ["cs.LG", "cs.GR"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22685v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.24001", "title": "Fantastic Flips and Where to Find Them: A General Framework for Parameterized Local Search on Partitioning Problem", "authors": ["Niels Gr√ºttemeier", "Nils Morawietz", "Frank Sommer"], "summary": "Parameterized local search combines classic local search heuristics with the\nparadigm of parameterized algorithmics. While most local search algorithms aim\nto improve given solutions by performing one single operation on a given\nsolution, the parameterized approach aims to improve a solution by performing\n$k$ simultaneous operations. Herein, $k$ is a parameter called search radius\nfor which the value can be chosen by a user. One major goal in the field of\nparameterized local search is to outline the trade-off between the size of $k$\nand the running time of the local search step. In this work, we introduce an\nabstract framework that generalizes natural parameterized local search\napproaches for a large class of partitioning problems: Given $n$ items that are\npartitioned into $b$ bins and a target function that evaluates the quality of\nthe current partition, one asks whether it is possible to improve the solution\nby removing up to $k$ items from their current bins and reassigning them to\nother bins. Among others, our framework applies for the local search versions\nof problems like Cluster Editing, Vector Bin Packing, and Nash Social Welfare.\nMotivated by a real-world application of the problem Vector Bin Packing, we\nintroduce a parameter called number of types $\\tau \\le n$ and show that all\nproblems fitting in our framework can be solved in $\\tau^k 2^{O(k)} |I|^{O(1)}$\ntime, where $|I|$ denotes the total input size. In case of Cluster Editing, the\nparameter $\\tau$ generalizes the well-known parameter neighborhood diversity of\nthe input graph. We complement this by showing that for all considered\nproblems, an algorithm significantly improving over our algorithm with running\ntime $\\tau^k 2^{O(k)} |I|^{O(1)}$ would contradict the ETH. Additionally, we\nshow that even on very restricted instances, all considered problems are\nW[1]-hard when parameterized by the search radius $k$ alone.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.24001v1", "categories": ["cs.DS", "cs.CC"], "cate": "cs.DS", "url": "http://arxiv.org/abs/2506.24001v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22621", "title": "Hierarchical Modeling and Architecture Optimization: Review and Unified Framework", "authors": ["Paul Saves", "Edward Hall√©-Hannan", "Jasper Bussemaker", "Youssef Diouane", "Nathalie Bartoli"], "summary": "Simulation-based problems involving mixed-variable inputs frequently feature\ndomains that are hierarchical, conditional, heterogeneous, or tree-structured.\nThese characteristics pose challenges for data representation, modeling, and\noptimization. This paper reviews extensive literature on these structured input\nspaces and proposes a unified framework that generalizes existing approaches.\nIn this framework, input variables may be continuous, integer, or categorical.\nA variable is described as meta if its value governs the presence of other\ndecreed variables, enabling the modeling of conditional and hierarchical\nstructures.\n  We further introduce the concept of partially-decreed variables, whose\nactivation depends on contextual conditions. To capture these inter-variable\nhierarchical relationships, we introduce design space graphs, combining\nprinciples from feature modeling and graph theory. This allows the definition\nof general hierarchical domains suitable for describing complex system\narchitectures. The framework supports the use of surrogate models over such\ndomains and integrates hierarchical kernels and distances for efficient\nmodeling and optimization. The proposed methods are implemented in the\nopen-source Surrogate Modeling Toolbox (SMT 2.0), and their capabilities are\ndemonstrated through applications in Bayesian optimization for complex system\ndesign, including a case study in green aircraft architecture.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22621v1", "categories": ["cs.LG", "math.OC", "stat.ML"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22621v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.23107", "title": "Can Large Language Models Capture Human Risk Preferences? A Cross-Cultural Study", "authors": ["Bing Song", "Jianing Liu", "Sisi Jian", "Chenyang Wu", "Vinayak Dixit"], "summary": "Large language models (LLMs) have made significant strides, extending their\napplications to dialogue systems, automated content creation, and\ndomain-specific advisory tasks. However, as their use grows, concerns have\nemerged regarding their reliability in simulating complex decision-making\nbehavior, such as risky decision-making, where a single choice can lead to\nmultiple outcomes. This study investigates the ability of LLMs to simulate\nrisky decision-making scenarios. We compare model-generated decisions with\nactual human responses in a series of lottery-based tasks, using transportation\nstated preference survey data from participants in Sydney, Dhaka, Hong Kong,\nand Nanjing. Demographic inputs were provided to two LLMs -- ChatGPT 4o and\nChatGPT o1-mini -- which were tasked with predicting individual choices. Risk\npreferences were analyzed using the Constant Relative Risk Aversion (CRRA)\nframework. Results show that both models exhibit more risk-averse behavior than\nhuman participants, with o1-mini aligning more closely with observed human\ndecisions. Further analysis of multilingual data from Nanjing and Hong Kong\nindicates that model predictions in Chinese deviate more from actual responses\ncompared to English, suggesting that prompt language may influence simulation\nperformance. These findings highlight both the promise and the current\nlimitations of LLMs in replicating human-like risk behavior, particularly in\nlinguistic and cultural settings.", "comment": "20 pages, 1 figure", "pdf_url": "http://arxiv.org/pdf/2506.23107v1", "categories": ["cs.AI"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.23107v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.22644", "title": "Evaluating Hybrid Retrieval Augmented Generation using Dynamic Test Sets: LiveRAG Challenge", "authors": ["Chase Fensore", "Kaustubh Dhole", "Joyce C Ho", "Eugene Agichtein"], "summary": "We present our submission to the LiveRAG Challenge 2025, which evaluates\nretrieval-augmented generation (RAG) systems on dynamic test sets using the\nFineWeb-10BT corpus. Our final hybrid approach combines sparse (BM25) and dense\n(E5) retrieval methods and then aims to generate relevant and faithful answers\nwith Falcon3-10B-Instruct. Through systematic evaluation on 200 synthetic\nquestions generated with DataMorgana across 64 unique question-user\ncombinations, we demonstrate that neural re-ranking with RankLLaMA improves MAP\nfrom 0.523 to 0.797 (52% relative improvement) but introduces prohibitive\ncomputational costs (84s vs 1.74s per question). While DSPy-optimized prompting\nstrategies achieved higher semantic similarity (0.771 vs 0.668), their 0%\nrefusal rates raised concerns about over-confidence and generalizability. Our\nsubmitted hybrid system without re-ranking achieved 4th place in faithfulness\nand 11th place in correctness among 25 teams. Analysis across question\ncategories reveals that vocabulary alignment between questions and documents\nwas the strongest predictor of performance on our development set, with\ndocument-similar phrasing improving cosine similarity from 0.562 to 0.762.", "comment": "4 pages, 3 tables, 2 figures. Accepted at the SIGIR LiveRAG Workshop\n  2025 (Submission 2664)", "pdf_url": "http://arxiv.org/pdf/2506.22644v1", "categories": ["cs.CL", "cs.IR"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.22644v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22944", "title": "Feasibility of spectral-element modeling of wave propagation through the anatomy of marine mammals", "authors": ["Carlos Garc√≠a A.", "Vladimiro Boselli", "Aida Hejazi Nooghabi", "Andrea Colombi", "Lapo Boschi"], "summary": "This study introduces the first 3D spectral-element method (SEM) simulation\nof ultrasonic wave propagation in a bottlenose dolphin (Tursiops truncatus)\nhead. Unlike traditional finite-element methods (FEM), which struggle with\nhigh-frequency simulations due to costly linear-system inversions and slower\nconvergence, SEM offers exponential convergence and efficient parallel\ncomputation. Using Computed Tomography (CT) scan data, we developed a detailed\nhexahedral mesh capturing complex anatomical features, such as acoustic fats\nand jaws. Our simulations of plane and spherical waves confirm SEM's\neffectiveness for ultrasonic time-domain modeling. This approach opens new\navenues for marine biology, contributing to research in echolocation, the\nimpacts of anthropogenic marine noise pollution and the biophysics of hearing\nand click generation in marine mammals. By overcoming FEM's limitations, SEM\nprovides a powerful scalable tool to test hypotheses about dolphin\nbioacoustics, with significant implications for conservation and understanding\nmarine mammal auditory systems under increasing environmental challenges.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22944v1", "categories": ["cs.CE", "cs.SD", "eess.AS", "q-bio.TO"], "cate": "cs.CE", "url": "http://arxiv.org/abs/2506.22944v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.22679", "title": "Assessing the feasibility of Large Language Models for detecting micro-behaviors in team interactions during space missions", "authors": ["Ankush Raut", "Projna Paromita", "Sydney Begerowski", "Suzanne Bell", "Theodora Chaspari"], "summary": "We explore the feasibility of large language models (LLMs) in detecting\nsubtle expressions of micro-behaviors in team conversations using transcripts\ncollected during simulated space missions. Specifically, we examine zero-shot\nclassification, fine-tuning, and paraphrase-augmented fine-tuning with\nencoder-only sequence classification LLMs, as well as few-shot text generation\nwith decoder-only causal language modeling LLMs, to predict the micro-behavior\nassociated with each conversational turn (i.e., dialogue). Our findings\nindicate that encoder-only LLMs, such as RoBERTa and DistilBERT, struggled to\ndetect underrepresented micro-behaviors, particularly discouraging speech, even\nwith weighted fine-tuning. In contrast, the instruction fine-tuned version of\nLlama-3.1, a decoder-only LLM, demonstrated superior performance, with the best\nmodels achieving macro F1-scores of 44% for 3-way classification and 68% for\nbinary classification. These results have implications for the development of\nspeech technologies aimed at analyzing team communication dynamics and\nenhancing training interventions in high-stakes environments such as space\nmissions, particularly in scenarios where text is the only accessible data.", "comment": "5 pages, 4 figures. Accepted to Interspeech 2025", "pdf_url": "http://arxiv.org/pdf/2506.22679v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.22679v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.23249", "title": "An \\textsf{AT1} phase-field framework for quasi-static anti-plane shear fracture: Unifying $Œæ$-based adaptivity and nonlinear strain energy density function", "authors": ["Maria P. Fernando", "S. M. Mallikarjunaiah"], "summary": "This work introduces a novel \\textsf{AT1} phase-field framework for\nsimulating quasi-static anti-plane shear fracture in geometrically linear\nelastic bodies. A key feature of this framework is the unification of\n$\\xi$-based local mesh adaptivity -- where $\\xi$ represents the characteristic\nlength of the damage zone -- and an algebraically nonlinear strain energy\ndensity function. A modified Francfort-Marigo energy functional, together with\nits Ambrosio-Tortorelli-type regularization, is hereby proposed to address\nchallenges within the framework of nonlinearly constituted materials. We\ndynamically optimize $\\xi$ throughout the simulation, significantly enhancing\nthe computational efficiency and accuracy of numerically approximating the\nlocal minimizers of the Ambrosio-Tortorelli (\\textsf{AT1})-type phase-field\nmodel. The proposed regularization for the total energy functional comprises\nthree distinct components: a nonlinear strain energy, an evolving surface\nenergy, and a linear-type regularization term dependent on the length scale of\nthe damage zone. Variational principles applied to this novel energy functional\nyield a coupled system of governing second-order quasilinear partial\ndifferential equations for the mechanics and phase-field variables. These\nequations are subsequently discretized using the conforming bilinear finite\nelement method. The formulation is underpinned by four crucial parameters: two\nare integral to the nonlinear strain energy function, while the other two serve\nas penalty parameters. These penalty parameters are asymptotically calibrated\nand rigorously utilized in the numerical simulations. Our results demonstrate\nthat this spatially adaptive approach leads to enhanced mesh adaptivity,\nensuring the robust convergence of the numerical solution.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23249v1", "categories": ["math.NA", "cs.NA"], "cate": "math.NA", "url": "http://arxiv.org/abs/2506.23249v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23873", "title": "Emergent musical properties of a transformer under contrastive self-supervised learning", "authors": ["Yuexuan Kong", "Gabriel Meseguer-Brocal", "Vincent Lostanlen", "Mathieu Lagrange", "Romain Hennequin"], "summary": "In music information retrieval (MIR), contrastive self-supervised learning\nfor general-purpose representation models is effective for global tasks such as\nautomatic tagging. However, for local tasks such as chord estimation, it is\nwidely assumed that contrastively trained general-purpose self-supervised\nmodels are inadequate and that more sophisticated SSL is necessary; e.g.,\nmasked modeling. Our paper challenges this assumption by revealing the\npotential of contrastive SSL paired with a transformer in local MIR tasks. We\nconsider a lightweight vision transformer with one-dimensional patches in the\ntime--frequency domain (ViT-1D) and train it with simple contrastive SSL\nthrough normalized temperature-scaled cross-entropy loss (NT-Xent). Although\nNT-Xent operates only over the class token, we observe that, potentially thanks\nto weight sharing, informative musical properties emerge in ViT-1D's sequence\ntokens. On global tasks, the temporal average of class and sequence tokens\noffers a performance increase compared to the class token alone, showing useful\nproperties in the sequence tokens. On local tasks, sequence tokens perform\nunexpectedly well, despite not being specifically trained for. Furthermore,\nhigh-level musical features such as onsets emerge from layer-wise attention\nmaps and self-similarity matrices show different layers capture different\nmusical dimensions. Our paper does not focus on improving performance but\nadvances the musical interpretation of transformers and sheds light on some\noverlooked abilities of contrastive SSL paired with transformers for sequence\nmodeling in MIR.", "comment": "Accepted at ISMIR 2025", "pdf_url": "http://arxiv.org/pdf/2506.23873v1", "categories": ["cs.SD", "cs.IR", "cs.LG", "eess.AS"], "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.23873v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23050", "title": "Equivalence Classes in AES -- Part 1", "authors": ["David Cornwell"], "summary": "We investigate properties of equivalence classes in AES which arise naturally\nfrom properties of MixColumns and InvMixColumns. These two operations have the\nproperty that the XOR of the 4 input bytes equals the XOR of 4 output bytes. We\nexamine the effect on equivalence classes due to the operation of SubBytes,\nShiftRows, MixColumns and AddRoundKey. The next phase of research is to find a\nkey recovery attack using known (plaintext, ciphertext) equivalence class\npairs.\n  Keywords: AES, Equivalence, Class, MixColumns, ShiftRows, SubBytes,\nAddRoundKey, Schedule, State, XOR", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23050v1", "categories": ["cs.CR"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.23050v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23689", "title": "Pok√©AI: A Goal-Generating, Battle-Optimizing Multi-agent System for Pokemon Red", "authors": ["Zihao Liu", "Xinhang Sui", "Yueran Song", "Siwen Wang"], "summary": "We introduce Pok\\'eAI, the first text-based, multi-agent large language model\n(LLM) framework designed to autonomously play and progress through Pok\\'emon\nRed. Our system consists of three specialized agents-Planning, Execution, and\nCritique-each with its own memory bank, role, and skill set. The Planning Agent\nfunctions as the central brain, generating tasks to progress through the game.\nThese tasks are then delegated to the Execution Agent, which carries them out\nwithin the game environment. Upon task completion, the Critique Agent evaluates\nthe outcome to determine whether the objective was successfully achieved. Once\nverification is complete, control returns to the Planning Agent, forming a\nclosed-loop decision-making system.\n  As a preliminary step, we developed a battle module within the Execution\nAgent. Our results show that the battle AI achieves an average win rate of\n80.8% across 50 wild encounters, only 6% lower than the performance of an\nexperienced human player. Furthermore, we find that a model's battle\nperformance correlates strongly with its LLM Arena score on language-related\ntasks, indicating a meaningful link between linguistic ability and strategic\nreasoning. Finally, our analysis of gameplay logs reveals that each LLM\nexhibits a unique playstyle, suggesting that individual models develop distinct\nstrategic behaviors.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23689v1", "categories": ["cs.AI", "cs.MA"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.23689v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23078", "title": "Event-based Stereo Visual-Inertial Odometry with Voxel Map", "authors": ["Zhaoxing Zhang", "Xiaoxiang Wang", "Chengliang Zhang", "Yangyang Guo", "Zikang Yuan", "Xin Yang"], "summary": "The event camera, renowned for its high dynamic range and exceptional\ntemporal resolution, is recognized as an important sensor for visual odometry.\nHowever, the inherent noise in event streams complicates the selection of\nhigh-quality map points, which critically determine the precision of state\nestimation. To address this challenge, we propose Voxel-ESVIO, an event-based\nstereo visual-inertial odometry system that utilizes voxel map management,\nwhich efficiently filter out high-quality 3D points. Specifically, our\nmethodology utilizes voxel-based point selection and voxel-aware point\nmanagement to collectively optimize the selection and updating of map points on\na per-voxel basis. These synergistic strategies enable the efficient retrieval\nof noise-resilient map points with the highest observation likelihood in\ncurrent frames, thereby ensureing the state estimation accuracy. Extensive\nevaluations on three public benchmarks demonstrate that our Voxel-ESVIO\noutperforms state-of-the-art methods in both accuracy and computational\nefficiency.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23078v1", "categories": ["cs.RO"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.23078v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.22991", "title": "Resilient-Native and Intelligent Next-Generation Wireless Systems: Key Enablers, Foundations, and Applications", "authors": ["Mehdi Bennis", "Sumudu Samarakoon", "Tamara Alshammari", "Chathuranga Weeraddana", "Zhoujun Tian", "Chaouki Ben Issaid"], "summary": "Just like power, water, and transportation systems, wireless networks are a\ncrucial societal infrastructure. As natural and human-induced disruptions\ncontinue to grow, wireless networks must be resilient. This requires them to\nwithstand and recover from unexpected adverse conditions, shocks, unmodeled\ndisturbances and cascading failures. Unlike robustness and reliability,\nresilience is based on the understanding that disruptions will inevitably\nhappen. Resilience, as elasticity, focuses on the ability to bounce back to\nfavorable states, while resilience as plasticity involves agents and networks\nthat can flexibly expand their states and hypotheses through real-time\nadaptation and reconfiguration. This situational awareness and active\npreparedness, adapting world models and counterfactually reasoning about\npotential system failures and the best responses, is a core aspect of\nresilience. This article will first disambiguate resilience from reliability\nand robustness, before delving into key mathematical foundations of resilience\ngrounded in abstraction, compositionality and emergence. Subsequently, we focus\nour attention on a plethora of techniques and methodologies pertaining to the\nunique characteristics of resilience, as well as their applications through a\ncomprehensive set of use cases. Ultimately, the goal of this paper is to\nestablish a unified foundation for understanding, modeling, and engineering\nresilience in wireless communication systems, while laying a roadmap for the\nnext-generation of resilient-native and intelligent wireless systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22991v1", "categories": ["cs.NI", "cs.LO", "cs.MA", "cs.SY", "eess.SY"], "cate": "cs.NI", "url": "http://arxiv.org/abs/2506.22991v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23535", "title": "Comparative Analysis of the Code Generated by Popular Large Language Models (LLMs) for MISRA C++ Compliance", "authors": ["Malik Muhammad Umer"], "summary": "Safety-critical systems are engineered systems whose failure or malfunction\ncould result in catastrophic consequences. The software development for\nsafety-critical systems necessitates rigorous engineering practices and\nadherence to certification standards like DO-178C for avionics. DO-178C is a\nguidance document which requires compliance to well-defined software coding\nstandards like MISRA C++ to enforce coding guidelines that prevent the use of\nambiguous, unsafe, or undefined constructs. Large Language Models (LLMs) have\ndemonstrated significant capabilities in automatic code generation across a\nwide range of programming languages, including C++. Despite their impressive\nperformance, code generated by LLMs in safety-critical domains must be\ncarefully analyzed for conformance to MISRA C++ coding standards. In this\npaper, I have conducted a comparative analysis of the C++ code generated by\npopular LLMs including: OpenAI ChatGPT, Google Gemini, DeepSeek, Meta AI, and\nMicrosoft Copilot for compliance with MISRA C++.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23535v1", "categories": ["cs.SE"], "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.23535v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23016", "title": "Deep Learning in Mild Cognitive Impairment Diagnosis using Eye Movements and Image Content in Visual Memory Tasks", "authors": ["Tom√°s Silva Santos Rocha", "Anastasiia Mikhailova", "Moreno I. Coco", "Jos√© Santos-Victor"], "summary": "The global prevalence of dementia is projected to double by 2050,\nhighlighting the urgent need for scalable diagnostic tools. This study utilizes\ndigital cognitive tasks with eye-tracking data correlated with memory processes\nto distinguish between Healthy Controls (HC) and Mild Cognitive Impairment\n(MCI), a precursor to dementia. A deep learning model based on VTNet was\ntrained using eye-tracking data from 44 participants (24 MCI, 20 HCs) who\nperformed a visual memory task. The model utilizes both time series and spatial\ndata derived from eye-tracking. It was modified to incorporate scan paths, heat\nmaps, and image content. These modifications also enabled testing parameters\nsuch as image resolution and task performance, analyzing their impact on model\nperformance. The best model, utilizing $700\\times700px$ resolution heatmaps,\nachieved 68% sensitivity and 76% specificity. Despite operating under more\nchallenging conditions (e.g., smaller dataset size, shorter task duration, or a\nless standardized task), the model's performance is comparable to an\nAlzheimer's study using similar methods (70% sensitivity and 73% specificity).\nThese findings contribute to the development of automated diagnostic tools for\nMCI. Future work should focus on refining the model and using a standardized\nlong-term visual memory task.", "comment": "13 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2506.23016v1", "categories": ["cs.HC", "cs.CV"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.23016v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.22867", "title": "Identification of Cellular Automata on Spaces of Bernoulli Probability Measures", "authors": ["Faizal Hafiz", "Amelia Kunze", "Enrico Formenti", "Davide La Torre"], "summary": "Classical Cellular Automata (CCAs) are a powerful computational framework for\nmodeling global spatio-temporal dynamics with local interactions. While CCAs\nhave been applied across numerous scientific fields, identifying the local rule\nthat governs observed dynamics remains a challenging task. Moreover, the\nunderlying assumption of deterministic cell states often limits the\napplicability of CCAs to systems characterized by inherent uncertainty. This\nstudy, therefore, focuses on the identification of Cellular Automata on spaces\nof probability measures (CAMs), where cell states are represented by\nprobability distributions. This framework enables the modeling of systems with\nprobabilistic uncertainty and spatially varying dynamics. Moreover, we\nformulate the local rule identification problem as a parameter estimation\nproblem and propose a meta-heuristic search based on Self-adaptive Differential\nEvolution (SaDE) to estimate local rule parameters accurately from the observed\ndata. The efficacy of the proposed approach is demonstrated through local rule\nidentification in two-dimensional CAMs with varying neighborhood types and\nradii.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22867v1", "categories": ["eess.SY", "cs.IT", "cs.SY", "math.IT"], "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.22867v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23058", "title": "Verifying Properties of Index Arrays in a Purely-Functional Data-Parallel Language", "authors": ["Nikolaj Hey Hinnerskov", "Robert Schenck", "Cosmin E. Oancea"], "summary": "This paper presents a novel approach to automatically verify properties of\npure data-parallel programs with non-linear indexing -- expressed as pre- and\npost-conditions on functions. Programs consist of nests of second-order array\ncombinators (e.g., map, scan, and scatter) and loops. The key idea is to\nrepresent arrays as index functions: programs are index function\ntransformations over which properties are propagated and inferred. Our\nframework proves properties on index functions by distilling them into\nalgebraic (in)equalities and discharging them to a Fourier-Motzkin-based\nsolver. The framework is practical and accessible: properties are not\nrestricted to a decidable logic, but instead are carefully selected to express\npractically useful guarantees that can be automatically reasoned about and\ninferred. These guarantees extend beyond program correctness and can be\nexploited by the entire compiler pipeline for optimization. We implement our\nsystem in the pure data-parallel language Futhark and demonstrate its\npracticality on seven applications, reporting an average verification time of 1\nsecond. Two case studies show how eliminating dynamic verification in GPU\nprograms results in significant speedups.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23058v1", "categories": ["cs.PL", "cs.DC"], "cate": "cs.PL", "url": "http://arxiv.org/abs/2506.23058v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23851", "title": "Comparative Studies: Cloud-Enabled Adaptive Learning System for Scalable Education in Sub-Saharan", "authors": ["Israel Fianyi", "Soonja Yeom", "Ju-Hyun Shin"], "summary": "The integration of cloud computing in education can revolutionise learning in\nadvanced (Australia & South Korea) and middle-income (Ghana & Nigeria)\ncountries, while offering scalable, cost-effective and equitable access to\nadaptive learning systems. This paper explores how cloud computing and adaptive\nlearning technologies are deployed across different socio-economic and\ninfrastructure contexts. The study identifies enabling factors and systematic\nchallenges, providing insights into how cloud-based education can be tailored\nto bridge the digital and educational divide globally.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23851v1", "categories": ["cs.CY", "cs.ET", "cs.HC"], "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.23851v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22517", "title": "Container damage detection using advanced computer vision model Yolov12 vs Yolov11 vs RF-DETR A comparative analysis", "authors": ["Subhadip Kumar"], "summary": "Containers are an integral part of the logistics industry and act as a\nbarrier for cargo. A typical service life for a container is more than 20\nyears. However, overtime containers suffer various types of damage due to the\nmechanical as well as natural factors. A damaged container is a safety hazard\nfor the employees handling it and a liability for the logistic company.\nTherefore, a timely inspection and detection of the damaged container is a key\nfor prolonging service life as well as avoiding safety hazards. In this paper,\nwe will compare the performance of the damage detection by three\nstate-of-the-art advanced computer vision models Yolov12, Yolov11 and RF-DETR.\nWe will use a dataset of 278 annotated images to train, validate and test the\nmodel. We will compare the mAP and precision of the model. The objective of\nthis paper is to identify the model that is best suited for container damage\ndetection. The result is mixed. mAP@50 score of Yolov11 and 12 was 81.9%\ncompared to RF-DETR, which was 77.7%. However, while testing the model for\nnot-so-common damaged containers, the RF-DETR model outperformed the others\noverall, exhibiting superiority to accurately detecting both damaged containers\nas well as damage occurrences with high confidence.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22517v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22517v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.23248", "title": "Joint Trajectory and Resource Optimization for HAPs-SAR Systems with Energy-Aware Constraints", "authors": ["Bang Huang", "Kihong Park", "Xiaowei Pang", "Mohamed-Slim Alouini"], "summary": "This paper investigates the joint optimization of trajectory planning and\nresource allocation for a high-altitude platform stations synthetic aperture\nradar (HAPs-SAR) system. To support real-time sensing and conserve the limited\nenergy budget of the HAPs, the proposed framework assumes that the acquired\nradar data are transmitted in real time to a ground base station for SAR image\nreconstruction. A dynamic trajectory model is developed, and the power\nconsumption associated with radar sensing, data transmission, and circular\nflight is comprehensively analyzed. In addition, solar energy harvesting is\nconsidered to enhance system sustainability. An energy-aware mixed-integer\nnonlinear programming (MINLP) problem is formulated to maximize radar beam\ncoverage while satisfying operational constraints. To solve this challenging\nproblem, a sub-optimal successive convex approximation (SCA)-based framework is\nproposed, incorporating iterative optimization and finite search. Simulation\nresults validate the convergence of the proposed algorithm and demonstrate its\neffectiveness in balancing SAR performance, communication reliability, and\nenergy efficiency. A final SAR imaging simulation on a 9-target lattice\nscenario further confirms the practical feasibility of the proposed solution.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23248v1", "categories": ["eess.SY", "cs.SY"], "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.23248v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.22469", "title": "Multi-Modal Beamforming with Model Compression and Modality Generation for V2X Networks", "authors": ["Chen Shang", "Dinh Thai Hoang", "Jiadong Yu"], "summary": "Integrating sensing and communication (ISAC) has emerged as a cornerstone\ntechnology for predictive beamforming in 6G-enabled vehicle-to-everything (V2X)\nnetworks. However, existing ISAC paradigms rely solely on radio frequency (RF)\nsignal, limiting sensing resolution and robustness in V2X environments with\nhigh mobility and multipath interference. Fortunately, the widespread\ndeployment of diverse non-RF sensors such as cameras and LiDAR, along with the\nintegration of artificial intelligence (AI) and communication systems, offers\nnew opportunities to improve the synergy between sensing and communication.\nMotivated by this, this work develops a novel and robust communication\nframework that leverages multi-modal sensing data and advanced AI technologies\nto assist beamforming in dynamic and realistic vehicular scenarios.\nSpecifically, we propose a multi-modal learning framework for predictive\nbeamforming that integrates modality-specific branches and employs hierarchical\nTransformer to capture cross-modal features. By exploiting the intrinsic\ncorrelation between multi-modal sensing data and beamforming decisions, this\ndesign enhances the accuracy and robustness of beamforming in dynamic V2X\nscenarios. To enable practical deployment on resource-constrained edge device\n(i.e., the roadside unit), we then develop a module-aware compression strategy\nthat significantly reduces inference latency while preserving model\nperformance. Furthermore, to address potential modality missing in real-world\nscenarios, we introduce a generative model that is able to reconstruct missing\ninputs from available observations, allowing the framework to operate reliably\neven under incomplete sensing conditions. Extensive simulation results\nconducted on real-world datasets demonstrate that the proposed scheme\nconsistently outperforms existing baselines across various metrics.", "comment": "13 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2506.22469v1", "categories": ["eess.SP"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.22469v1", "date": "2025-06-19", "updated": "2025-06-19"}
{"id": "2506.23298", "title": "Exposing and Mitigating Calibration Biases and Demographic Unfairness in MLLM Few-Shot In-Context Learning for Medical Image Classification", "authors": ["Xing Shen", "Justin Szeto", "Mingyang Li", "Hengguan Huang", "Tal Arbel"], "summary": "Multimodal large language models (MLLMs) have enormous potential to perform\nfew-shot in-context learning in the context of medical image analysis. However,\nsafe deployment of these models into real-world clinical practice requires an\nin-depth analysis of the accuracies of their predictions, and their associated\ncalibration errors, particularly across different demographic subgroups. In\nthis work, we present the first investigation into the calibration biases and\ndemographic unfairness of MLLMs' predictions and confidence scores in few-shot\nin-context learning for medical image classification. We introduce CALIN, an\ninference-time calibration method designed to mitigate the associated biases.\nSpecifically, CALIN estimates the amount of calibration needed, represented by\ncalibration matrices, using a bi-level procedure: progressing from the\npopulation level to the subgroup level prior to inference. It then applies this\nestimation to calibrate the predicted confidence scores during inference.\nExperimental results on three medical imaging datasets: PAPILA for fundus image\nclassification, HAM10000 for skin cancer classification, and MIMIC-CXR for\nchest X-ray classification demonstrate CALIN's effectiveness at ensuring fair\nconfidence calibration in its prediction, while improving its overall\nprediction accuracies and exhibiting minimum fairness-utility trade-off.", "comment": "Preprint version. The peer-reviewed version of this paper has been\n  accepted to MICCAI 2025 main conference", "pdf_url": "http://arxiv.org/pdf/2506.23298v1", "categories": ["eess.IV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.23298v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.22899", "title": "Neural Cellular Automata: From Cells to Pixels", "authors": ["Ehsan Pajouheshgar", "Yitao Xu", "Ali Abbasi", "Alexander Mordvintsev", "Wenzel Jakob", "Sabine S√ºsstrunk"], "summary": "Neural Cellular Automata (NCAs) are bio-inspired systems in which identical\ncells self-organize to form complex and coherent patterns by repeatedly\napplying simple local rules. NCAs display striking emergent behaviors including\nself-regeneration, generalization and robustness to unseen situations, and\nspontaneous motion. Despite their success in texture synthesis and\nmorphogenesis, NCAs remain largely confined to low-resolution grids. This\nlimitation stems from (1) training time and memory requirements that grow\nquadratically with grid size, (2) the strictly local propagation of information\nwhich impedes long-range cell communication, and (3) the heavy compute demands\nof real-time inference at high resolution. In this work, we overcome this\nlimitation by pairing NCA with a tiny, shared implicit decoder, inspired by\nrecent advances in implicit neural representations. Following NCA evolution on\na coarse grid, a lightweight decoder renders output images at arbitrary\nresolution. We also propose novel loss functions for both morphogenesis and\ntexture synthesis tasks, specifically tailored for high-resolution output with\nminimal memory and computation overhead. Combining our proposed architecture\nand loss functions brings substantial improvement in quality, efficiency, and\nperformance. NCAs equipped with our implicit decoder can generate full-HD\noutputs in real time while preserving their self-organizing, emergent\nproperties. Moreover, because each MLP processes cell states independently,\ninference remains highly parallelizable and efficient. We demonstrate the\napplicability of our approach across multiple NCA variants (on 2D, 3D grids,\nand 3D meshes) and multiple tasks, including texture generation and\nmorphogenesis (growing patterns from a seed), showing that with our proposed\nframework, NCAs seamlessly scale to high-resolution outputs with minimal\ncomputational overhead.", "comment": "6 pages, 5 figures, first draft", "pdf_url": "http://arxiv.org/pdf/2506.22899v1", "categories": ["cs.CV", "cs.GR", "cs.LG", "cs.MA", "eess.IV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22899v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.24032", "title": "Dominating Set Knapsack: Profit Optimization on Dominating Sets", "authors": ["Sipra Singh"], "summary": "In a large-scale network, we want to choose some influential nodes to make a\nprofit by paying some cost within a limited budget so that we do not have to\nspend more budget on some nodes adjacent to the chosen nodes; our problem is\nthe graph-theoretic representation of it. We define our problem Dominating Set\nKnapsack by attaching Knapsack Problem with Dominating Set on graphs. Each\nvertex is associated with a cost factor and a profit amount. We aim to choose\nsome vertices within a fixed budget that gives maximum profit so that we do not\nneed to choose their 1-hop neighbors. We show that the Dominating Set Knapsack\nproblem is strongly NP-complete even when restricted to Bipartite graphs but\nweakly NP-complete for Star graphs. We present a pseudo-polynomial time\nalgorithm for Trees in time $O(n\\cdot min\\{s^2, (\\alpha(V))^2\\})$. We show that\nDominating Set Knapsack is very unlikely to be Fixed Parameter Tractable(FPT)\nby proving that it is in W[2]-hard parameterized by the solution size. We\ndeveloped FPT algorithms with running time $O(4^{tw}\\cdot n^{O(1)} \\cdot\nmin\\{s^2,{\\alpha(V)}^2\\})$ and $O(2^{vck-1}\\cdot n^{O(1)} \\cdot\nmin\\{s^2,{\\alpha(V)}^2\\})$, where $tw$ represents the treewidth of the given\ngraph, $vck$ is the solution size of the Vertex Cover Knapsack, $s$ is the size\nof the knapsack and $\\alpha(V)=\\sum_{v\\in V}\\alpha(v)$.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.24032v1", "categories": ["cs.DS", "cs.CC"], "cate": "cs.DS", "url": "http://arxiv.org/abs/2506.24032v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22631", "title": "A hierarchical Vovk-Azoury-Warmuth forecaster with discounting for online regression in RKHS", "authors": ["Dmitry B. Rokhlin"], "summary": "We study the problem of online regression with the unconstrained quadratic\nloss against a time-varying sequence of functions from a Reproducing Kernel\nHilbert Space (RKHS). Recently, Jacobsen and Cutkosky (2024) introduced a\ndiscounted Vovk-Azoury-Warmuth (DVAW) forecaster that achieves optimal dynamic\nregret in the finite-dimensional case. In this work, we lift their approach to\nthe non-parametric domain by synthesizing the DVAW framework with a random\nfeature approximation. We propose a fully adaptive, hierarchical algorithm,\nwhich we call H-VAW-D (Hierarchical Vovk-Azoury-Warmuth with Discounting), that\nlearns both the discount factor and the number of random features. We prove\nthat this algorithm, which has a per-iteration computational complexity of\n$O(T\\ln T)$, achieves an expected dynamic regret of $O(T^{2/3}P_T^{1/3} +\n\\sqrt{T}\\ln T)$, where $P_T$ is the functional path length of a comparator\nsequence.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22631v1", "categories": ["cs.LG", "stat.ML", "68Q32, 68W27, 68W20"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22631v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.23123", "title": "The Societal Impact of Foundation Models: Advancing Evidence-based AI Policy", "authors": ["Rishi Bommasani"], "summary": "Artificial intelligence is humanity's most promising technology because of\nthe remarkable capabilities offered by foundation models. Yet, the same\ntechnology brings confusion and consternation: foundation models are poorly\nunderstood and they may precipitate a wide array of harms. This dissertation\nexplains how technology and society coevolve in the age of AI, organized around\nthree themes. First, the conceptual framing: the capabilities, risks, and the\nsupply chain that grounds foundation models in the broader economy. Second, the\nempirical insights that enrich the conceptual foundations: transparency created\nvia evaluations at the model level and indexes at the organization level.\nFinally, the transition from understanding to action: superior understanding of\nthe societal impact of foundation models advances evidence-based AI policy.\nView together, this dissertation makes inroads into achieving better societal\noutcomes in the age of AI by building the scientific foundations and\nresearch-policy interface required for better AI governance.", "comment": "Stanford University PhD Dissertation of Rishi Bommasani (Department\n  of Computer Science, 2025). Also available at\n  https://purl.stanford.edu/zf669yy0336", "pdf_url": "http://arxiv.org/pdf/2506.23123v1", "categories": ["cs.AI", "cs.CY", "cs.ET"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.23123v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23322", "title": "GaussMaster: An LLM-based Database Copilot System", "authors": ["Wei Zhou", "Ji Sun", "Xuanhe Zhou", "Guoliang Li", "Luyang Liu", "Hao Wu", "Tianyuan Wang"], "summary": "In the financial industry, data is the lifeblood of operations, and DBAs\nshoulder significant responsibilities for SQL tuning, database deployment,\ndiagnosis, and service repair. In recent years, both database vendors and\ncustomers have increasingly turned to autonomous database platforms in an\neffort to alleviate the heavy workload of DBAs. However, existing autonomous\ndatabase platforms are limited in their capabilities, primarily addressing\nsingle-point issues such as NL2SQL, anomaly detection, and SQL tuning. Manual\nintervention remains a necessity for comprehensive database maintenance.\nGaussMaster aims to revolutionize this landscape by introducing an LLM-based\ndatabase copilot system. This innovative solution is designed not only to\nassist developers in writing efficient SQL queries but also to provide\ncomprehensive care for database services. When database instances exhibit\nabnormal behavior, GaussMaster is capable of orchestrating the entire\nmaintenance process automatically. It achieves this by analyzing hundreds of\nmetrics and logs, employing a Tree-of-thought approach to identify root causes,\nand invoking appropriate tools to resolve issues. We have successfully\nimplemented GaussMaster in real-world scenarios, such as the banking industry,\nwhere it has achieved zero human intervention for over 34 database maintenance\nscenarios. In this paper, we present significant improvements in these tasks\nwith code at https://gitcode.com/opengauss/openGauss-GaussMaster.", "comment": "We welcome contributions from the community. For reference, please\n  see the code at: https://gitcode.com/opengauss/openGauss-GaussMaster", "pdf_url": "http://arxiv.org/pdf/2506.23322v1", "categories": ["cs.DB", "cs.AI", "cs.CL", "cs.IR"], "cate": "cs.DB", "url": "http://arxiv.org/abs/2506.23322v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23030", "title": "VisionScores -- A system-segmented image score dataset for deep learning tasks", "authors": ["Alejandro Romero Amezcua", "Mariano Jos√© Juan Rivera Meraz"], "summary": "VisionScores presents a novel proposal being the first system-segmented image\nscore dataset, aiming to offer structure-rich, high information-density images\nfor machine and deep learning tasks. Delimited to two-handed piano pieces, it\nwas built to consider not only certain graphic similarity but also composition\npatterns, as this creative process is highly instrument-dependent. It provides\ntwo scenarios in relation to composer and composition type. The first, formed\nby 14k samples, considers works from different authors but the same composition\ntype, specifically, Sonatinas. The latter, consisting of 10.8K samples,\npresents the opposite case, various composition types from the same author,\nbeing the one selected Franz Liszt. All of the 24.8k samples are formatted as\ngrayscale jpg images of $128 \\times 512$ pixels. VisionScores supplies the\nusers not only the formatted samples but the systems' order and pieces'\nmetadata. Moreover, unsegmented full-page scores and the pre-formatted images\nare included for further analysis.", "comment": "Comments: 5 pages, 3 figures. Accepted for presentation at the 2025\n  IEEE International Conference on Image Processing (ICIP). \\c{opyright} 2025\n  IEEE. Personal use of this material is permitted. Permission from IEEE must\n  be obtained for any other use", "pdf_url": "http://arxiv.org/pdf/2506.23030v1", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23030v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.22694", "title": "VOCABTRIM: Vocabulary Pruning for Efficient Speculative Decoding in LLMs", "authors": ["Raghavv Goel", "Sudhanshu Agrawal", "Mukul Gagrani", "Junyoung Park", "Yifan Zao", "He Zhang", "Tian Liu", "Yiping Yang", "Xin Yuan", "Jiuyan Lu", "Chris Lott", "Mingu Lee"], "summary": "In this paper, we introduce a simple training-free technique to improve the\nperformance of drafter-based speculative decoding (SpD) methods that\nincorporates language modeling head (LM head) during drafting process. A\ndrafter-based speculative decoding leverages one or more smaller language\nmodels, a.k.a. drafters or draft models, to sample a draft sequence or tree\nconsisting of multiple tokens, followed by verification by a base LLM, a target\nmodel, accepting a subset as its valid generation. As it is usually considered\nthat the speculative decoding requires one-to-one mapping between vocabularies\nof the target model and the draft model, it has been natural to share the\nvocabulary between them, or even share the LM head as in EAGLE or Medusa. We\nfirst identify that this draft token sampling scheme inherently contains an\nunnecessary inference overhead in drafting, especially for some target LLMs\nwith very large vocabularies. Then, we propose a simple technique, VocabTrim,\nto mitigate the drafting overhead to improve the generation speed in\nmemory-bound environment. VocabTrim reconstructs the drafter LM head to contain\nonly a limited set of tokens, selected by the most frequently sampled from the\nvocabulary of the target model. While limiting the vocabulary in drafting\nslightly degrades the acceptance rate, it significantly reduces the drafting\nlatency in memory-bound process which is often the case on edge devices,\nresulting in higher memory-bound speed up (MBSU). We show that our method can\nboost the memory-bound speed-up for Llama-3 models on Spec-Bench, specifically\nby 16% for Llama-3.2-3B-Instruct.", "comment": "7 pages, 4 figures, 5 tables, accepted at ICML 2025 workshop on\n  Efficient Systems for Foundational Models", "pdf_url": "http://arxiv.org/pdf/2506.22694v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.22694v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23344", "title": "Data-Driven Self-Supervised Learning for the Discovery of Solution Singularity for Partial Differential Equations", "authors": ["Difeng Cai", "Paulina Sep√∫lveda"], "summary": "The appearance of singularities in the function of interest constitutes a\nfundamental challenge in scientific computing. It can significantly undermine\nthe effectiveness of numerical schemes for function approximation, numerical\nintegration, and the solution of partial differential equations (PDEs), etc.\nThe problem becomes more sophisticated if the location of the singularity is\nunknown, which is often encountered in solving PDEs. Detecting the singularity\nis therefore critical for developing efficient adaptive methods to reduce\ncomputational costs in various applications. In this paper, we consider\nsingularity detection in a purely data-driven setting. Namely, the input only\ncontains given data, such as the vertex set from a mesh. To overcome the\nlimitation of the raw unlabeled data, we propose a self-supervised learning\n(SSL) framework for estimating the location of the singularity. A key component\nis a filtering procedure as the pretext task in SSL, where two filtering\nmethods are presented, based on $k$ nearest neighbors and kernel density\nestimation, respectively. We provide numerical examples to illustrate the\npotential pathological or inaccurate results due to the use of raw data without\nfiltering. Various experiments are presented to demonstrate the ability of the\nproposed approach to deal with input perturbation, label corruption, and\ndifferent kinds of singularities such interior circle, boundary layer,\nconcentric semicircles, etc.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23344v1", "categories": ["math.NA", "cs.LG", "cs.NA", "stat.ML"], "cate": "math.NA", "url": "http://arxiv.org/abs/2506.23344v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23986", "title": "StreamFlow: Streaming Flow Matching with Block-wise Guided Attention Mask for Speech Token Decoding", "authors": ["Dake Guo", "Jixun Yao", "Linhan Ma", "Wang He", "Lei Xie"], "summary": "Recent advancements in discrete token-based speech generation have\nhighlighted the importance of token-to-waveform generation for audio quality,\nparticularly in real-time interactions. Traditional frameworks integrating\nsemantic tokens with flow matching (FM) struggle with streaming capabilities\ndue to their reliance on a global receptive field. Additionally, directly\nimplementing token-by-token streaming speech generation often results in\ndegraded audio quality. To address these challenges, we propose StreamFlow, a\nnovel neural architecture that facilitates streaming flow matching with\ndiffusion transformers (DiT). To mitigate the long-sequence extrapolation\nissues arising from lengthy historical dependencies, we design a local\nblock-wise receptive field strategy. Specifically, the sequence is first\nsegmented into blocks, and we introduce block-wise attention masks that enable\nthe current block to receive information from the previous or subsequent block.\nThese attention masks are combined hierarchically across different DiT-blocks\nto regulate the receptive field of DiTs. Both subjective and objective\nexperimental results demonstrate that our approach achieves performance\ncomparable to non-streaming methods while surpassing other streaming methods in\nterms of speech quality, all the while effectively managing inference time\nduring long-sequence generation. Furthermore, our method achieves a notable\nfirst-packet latency of only 180 ms.\\footnote{Speech samples:\nhttps://dukguo.github.io/StreamFlow/}", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23986v1", "categories": ["cs.SD", "eess.AS"], "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.23986v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23183", "title": "A Practical and Secure Byzantine Robust Aggregator", "authors": ["De Zhang Lee", "Aashish Kolluri", "Prateek Saxena", "Ee-Chien Chang"], "summary": "In machine learning security, one is often faced with the problem of removing\noutliers from a given set of high-dimensional vectors when computing their\naverage. For example, many variants of data poisoning attacks produce gradient\nvectors during training that are outliers in the distribution of clean\ngradients, which bias the computed average used to derive the ML model.\nFiltering them out before averaging serves as a generic defense strategy.\nByzantine robust aggregation is an algorithmic primitive which computes a\nrobust average of vectors, in the presence of an $\\epsilon$ fraction of vectors\nwhich may have been arbitrarily and adaptively corrupted, such that the\nresulting bias in the final average is provably bounded.\n  In this paper, we give the first robust aggregator that runs in quasi-linear\ntime in the size of input vectors and provably has near-optimal bias bounds.\nOur algorithm also does not assume any knowledge of the distribution of clean\nvectors, nor does it require pre-computing any filtering thresholds from it.\nThis makes it practical to use directly in standard neural network training\nprocedures. We empirically confirm its expected runtime efficiency and its\neffectiveness in nullifying 10 different ML poisoning attacks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23183v1", "categories": ["cs.CR"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.23183v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23793", "title": "Advancing Learnable Multi-Agent Pathfinding Solvers with Active Fine-Tuning", "authors": ["Anton Andreychuk", "Konstantin Yakovlev", "Aleksandr Panov", "Alexey Skrynnik"], "summary": "Multi-agent pathfinding (MAPF) is a common abstraction of multi-robot\ntrajectory planning problems, where multiple homogeneous robots simultaneously\nmove in the shared environment. While solving MAPF optimally has been proven to\nbe NP-hard, scalable, and efficient, solvers are vital for real-world\napplications like logistics, search-and-rescue, etc. To this end, decentralized\nsuboptimal MAPF solvers that leverage machine learning have come on stage.\nBuilding on the success of the recently introduced MAPF-GPT, a pure imitation\nlearning solver, we introduce MAPF-GPT-DDG. This novel approach effectively\nfine-tunes the pre-trained MAPF model using centralized expert data. Leveraging\na novel delta-data generation mechanism, MAPF-GPT-DDG accelerates training\nwhile significantly improving performance at test time. Our experiments\ndemonstrate that MAPF-GPT-DDG surpasses all existing learning-based MAPF\nsolvers, including the original MAPF-GPT, regarding solution quality across\nmany testing scenarios. Remarkably, it can work with MAPF instances involving\nup to 1 million agents in a single environment, setting a new milestone for\nscalability in MAPF domains.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23793v1", "categories": ["cs.AI", "cs.LG", "cs.MA"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.23793v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23114", "title": "Minimizing Acoustic Noise: Enhancing Quiet Locomotion for Quadruped Robots in Indoor Applications", "authors": ["Zhanxiang Cao", "Buqing Nie", "Yang Zhang", "Yue Gao"], "summary": "Recent advancements in quadruped robot research have significantly improved\ntheir ability to traverse complex and unstructured outdoor environments.\nHowever, the issue of noise generated during locomotion is generally\noverlooked, which is critically important in noise-sensitive indoor\nenvironments, such as service and healthcare settings, where maintaining low\nnoise levels is essential. This study aims to optimize the acoustic noise\ngenerated by quadruped robots during locomotion through the development of\nadvanced motion control algorithms. To achieve this, we propose a novel\napproach that minimizes noise emissions by integrating optimized gait design\nwith tailored control strategies. This method achieves an average noise\nreduction of approximately 8 dBA during movement, thereby enhancing the\nsuitability of quadruped robots for deployment in noise-sensitive indoor\nenvironments. Experimental results demonstrate the effectiveness of this\napproach across various indoor settings, highlighting the potential of\nquadruped robots for quiet operation in noise-sensitive environments.", "comment": "8 pages,6 figures, IROS2025", "pdf_url": "http://arxiv.org/pdf/2506.23114v1", "categories": ["cs.RO"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.23114v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23083", "title": "Model-Based Diagnosis: Automating End-to-End Diagnosis of Network Failures", "authors": ["Changrong Wu", "Yiyao Yu", "Myungjin Lee", "Jayanth Srinivasa", "Ennan Zhai", "George Varghese", "Yuval Tamir"], "summary": "Fast diagnosis and repair of enterprise network failures is critically\nimportant since disruptions cause major business impacts. Prior works focused\non diagnosis primitives or procedures limited to a subset of the problem, such\nas only data plane or only control plane faults. This paper proposes a new\nparadigm, model-based network diagnosis, that provides a systematic way to\nderive automated procedures for identifying the root cause of network failures,\nbased on reports of end-to-end user-level symptoms. The diagnosis procedures\nare systematically derived from a model of packet forwarding and routing,\ncovering hardware, firmware, and software faults in both the data plane and\ndistributed control plane. These automated procedures replace and dramatically\naccelerate diagnosis by an experienced human operator. Model-based diagnosis is\ninspired by, leverages, and is complementary to recent work on network\nverification. We have built NetDx, a proof-of-concept implementation of\nmodel-based network diagnosis. We deployed NetDx on a new emulator of networks\nconsisting of P4 switches with distributed routing software. We validated the\nrobustness and coverage of NetDx with an automated fault injection campaign, in\nwhich 100% of faults were diagnosed correctly. Furthermore, on a data set of 33\nfaults from a large cloud provider that are within the domain targeted by\nNetDx, 30 are efficiently diagnosed in seconds instead of hours.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23083v1", "categories": ["cs.NI"], "cate": "cs.NI", "url": "http://arxiv.org/abs/2506.23083v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23644", "title": "QLPro: Automated Code Vulnerability Discovery via LLM and Static Code Analysis Integration", "authors": ["Junze Hu", "Xiangyu Jin", "Yizhe Zeng", "Yuling Liu", "Yunpeng Li", "Dan Du", "Kaiyu Xie", "Hongsong Zhu"], "summary": "We introduce QLPro, a vulnerability detection framework that systematically\nintegrates LLMs and static analysis tools to enable comprehensive vulnerability\ndetection across entire open-source projects.We constructed a new dataset,\nJavaTest, comprising 10 open-source projects from GitHub with 62 confirmed\nvulnerabilities. CodeQL, a state-of-the-art static analysis tool, detected only\n24 of these vulnerabilities while QLPro detected 41. Furthermore, QLPro\ndiscovered 6 previously unknown vulnerabilities, 2 of which have been confirmed\nas 0-days.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23644v1", "categories": ["cs.SE", "cs.AI", "cs.CR"], "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.23644v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23017", "title": "Mind the Dark: A Gamified Exploration of Deceptive Design Awareness for Children in the Digital Age", "authors": ["Noverah Khan", "Hira Eiraj Daud", "Suleman Shahid"], "summary": "This paper addresses the critical issue of deceptive design elements\nprevalent in technology, and their potential impact on children. Recent\nresearch highlights the impact of dark patterns on adults and adolescents,\nwhile studies involving children are scarce. In an era where children wield\ngreater independence with digital devices, their vulnerability to dark patterns\namplifies without early education. Our findings show a significant positive\nimpact of dark pattern education on children's awareness, revealing that\nheightened awareness considerably alters children's navigation of social media,\nvideo games, and streaming platforms. To this end, we developed a gamified\napplication aimed at instructing children on identifying and responding to\nvarious dark patterns. Our evaluation results emphasize the critical role of\nearly education in empowering children to recognize and counter deceptive\ndesign, thereby cultivating a digitally literate generation capable of making\ninformed choices in the complex landscape of digital technology.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23017v1", "categories": ["cs.HC"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.23017v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.22903", "title": "Limited Feedback in RIS-Assisted Wireless Communications: Use Cases, Challenges, and Future Directions", "authors": ["Weicong Chen", "Jiajia Guo", "Yiming Cui", "Xiao Li", "Shi Jin"], "summary": "Channel state information (CSI) is essential to unlock the potential of\nreconfigurable intelligent surfaces (RISs) in wireless communication systems.\nSince massive RIS elements are typically implemented without baseband signal\nprocessing capabilities, limited CSI feedback is necessary when designing the\nreflection/refraction coefficients of the RIS. In this article, the unique\nRIS-assisted channel features, such as the RIS position-dependent channel\nfluctuation, the ultra-high dimensional sub-channel matrix, and the structured\nsparsity, are distilled from recent advances in limited feedback and used as\nguidelines for designing feedback schemes. We begin by illustrating the use\ncases and the corresponding challenges associated with RIS feedback. We then\ndiscuss how to leverage techniques such as channel customization,\nstructured-sparsity, autoencoders, and others to reduce feedback overhead and\ncomplexity when devising feedback schemes. Finally, we identify potential\nresearch directions by considering the unresolved challenges, the new RIS\narchitecture, and the integration with multi-modal information and artificial\nintelligence.", "comment": "This work has been submitted for possible publication", "pdf_url": "http://arxiv.org/pdf/2506.22903v1", "categories": ["eess.SP", "cs.IT", "math.IT"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.22903v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23210", "title": "FedRef: Communication-Efficient Bayesian Fine Tuning with Reference Model", "authors": ["Taehwan Yoon", "Bongjun Choi"], "summary": "Federated learning(FL) is used for distributed scenarios to train artificial\nintelligence(AI) models while ensuring users' privacy. In federated learning\nscenario, the server generally never knows about users' data. This type of\nconcept makes the AI training process efficient in terms of data privacy.\nHowever, regarding model performance, federated AI models may not sufficiently\nsatisfy AI users' expectations. Furthermore, AI users have a wide range of\ndifferent needs. It is not easy to satisfy the whole users needs. These types\nof issues can be addressed through AI model optimization, fine-tuning, or\npersonalization to achieve optimal model performance. To address model\noptimization challenges, we propose reference model-based federated learning\nfor optimal fine-tuning, which overcomes catastrophic forgetting in each round.\nThis method is derived from Bayesian parameter-efficient transfer learning,\nwhich includes an optimal proximal term and enables overcoming the catastrophic\nforgetting issue in each round by utilizing a reference model that incorporates\nprevious model parameters. As a result, this method achieves both high model\nperformance and low computing cost.", "comment": "6 pages,14 equation", "pdf_url": "http://arxiv.org/pdf/2506.23210v1", "categories": ["cs.LG", "cs.AI", "cs.DC"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.23210v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.24061", "title": "Beyond Distance: Mobility Neural Embeddings Reveal Visible and Invisible Barriers in Urban Space", "authors": ["Guangyuan Weng", "Minsuk Kim", "Yong-Yeol Ahn", "Esteban Moro"], "summary": "Human mobility in cities is shaped not only by visible structures such as\nhighways, rivers, and parks but also by invisible barriers rooted in\nsocioeconomic segregation, uneven access to amenities, and administrative\ndivisions. Yet identifying and quantifying these barriers at scale and their\nrelative importance on people's movements remains a major challenge. Neural\nembedding models, originally developed for language, offer a powerful way to\ncapture the complexity of human mobility from large-scale data. Here, we apply\nthis approach to 25.4 million observed trajectories across 11 major U.S.\ncities, learning mobility embeddings that reveal how people move through urban\nspace. These mobility embeddings define a functional distance between places,\none that reflects behavioral rather than physical proximity, and allow us to\ndetect barriers between neighborhoods that are geographically close but\nbehaviorally disconnected. We find that the strongest predictors of these\nbarriers are differences in access to amenities, administrative borders, and\nresidential segregation by income and race. These invisible borders are\nconcentrated in urban cores and persist across cities, spatial scales, and time\nperiods. Physical infrastructure, such as highways and parks, plays a secondary\nbut still significant role, especially at short distances. We also find that\nindividuals who cross barriers tend to do so outside of traditional commuting\nhours and are more likely to live in areas with greater racial diversity, and\nhigher transit use or income. Together, these findings reveal how spatial,\nsocial, and behavioral forces structure urban accessibility and provide a\nscalable framework to detect and monitor barriers in cities, with applications\nin planning, policy evaluation, and equity analysis.", "comment": "40 pages, 19 figures, and 12 tables", "pdf_url": "http://arxiv.org/pdf/2506.24061v1", "categories": ["cs.CY", "physics.soc-ph"], "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.24061v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22531", "title": "Preserve Anything: Controllable Image Synthesis with Object Preservation", "authors": ["Prasen Kumar Sharma", "Neeraj Matiyali", "Siddharth Srivastava", "Gaurav Sharma"], "summary": "We introduce \\textit{Preserve Anything}, a novel method for controlled image\nsynthesis that addresses key limitations in object preservation and semantic\nconsistency in text-to-image (T2I) generation. Existing approaches often fail\n(i) to preserve multiple objects with fidelity, (ii) maintain semantic\nalignment with prompts, or (iii) provide explicit control over scene\ncomposition. To overcome these challenges, the proposed method employs an\nN-channel ControlNet that integrates (i) object preservation with size and\nplacement agnosticism, color and detail retention, and artifact elimination,\n(ii) high-resolution, semantically consistent backgrounds with accurate\nshadows, lighting, and prompt adherence, and (iii) explicit user control over\nbackground layouts and lighting conditions. Key components of our framework\ninclude object preservation and background guidance modules, enforcing lighting\nconsistency and a high-frequency overlay module to retain fine details while\nmitigating unwanted artifacts. We introduce a benchmark dataset consisting of\n240K natural images filtered for aesthetic quality and 18K 3D-rendered\nsynthetic images with metadata such as lighting, camera angles, and object\nrelationships. This dataset addresses the deficiencies of existing benchmarks\nand allows a complete evaluation. Empirical results demonstrate that our method\nachieves state-of-the-art performance, significantly improving feature-space\nfidelity (FID 15.26) and semantic alignment (CLIP-S 32.85) while maintaining\ncompetitive aesthetic quality. We also conducted a user study to demonstrate\nthe efficacy of the proposed work on unseen benchmark and observed a remarkable\nimprovement of $\\sim25\\%$, $\\sim19\\%$, $\\sim13\\%$, and $\\sim14\\%$ in terms of\nprompt alignment, photorealism, the presence of AI artifacts, and natural\naesthetics over existing works.", "comment": "Accepted at ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2506.22531v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22531v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.23302", "title": "Load Limiting Control for Component Life Extension", "authors": ["Chams Eddine Mballo", "Robert Walters", "Jonnalagadda V. R. Prasad"], "summary": "This paper presents the development of a novel life-extending control scheme\nfor critical helicopter components subjected to significant fatigue loading.\nThe primary objective is to synthesize a more efficient and less conservative\nlife-extending control scheme than those currently available in the literature.\nThe proposed Load Limiting Control (LLC) scheme is a viable solution that\naddresses several issues that current life-extending control schemes suffer\nfrom, such as the neglect of fatigue damage induced by the harmonic component\nof loads and the inability to distinguish between aggressive and non-aggressive\nmaneuvers. The proposed LLC scheme treats desired harmonic load limits as limit\nboundaries and recasts the problem of load limiting as a vehicle limit by\ncomputing a Control Margin (CM) using a limit detection and avoidance module.\nThe computed CM is used as a cue to the pilot. The limit detection and\navoidance module comprises an optimization algorithm, a model predictive\ncontroller, and a computationally simple on-board dynamical model. Simulations\nwere conducted to demonstrate the effectiveness of the LLC scheme in limiting\nharmonic pitch link loads during flight. One significant outcome is that, with\nsufficient training, the pilot can skillfully track the cue within 0.5 seconds\nof initiating the tracking task.", "comment": "Accepted for publication in Journal of Guidance, Control, and\n  Dynamics, Vol 48 (2), 2025. Version of Record at DOI\n  https://doi.org/10.2514/1.G007854", "pdf_url": "http://arxiv.org/pdf/2506.23302v1", "categories": ["eess.SY", "cs.SY"], "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.23302v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.22471", "title": "Continual Learning for Wireless Channel Prediction", "authors": ["Muhammad Ahmed Mohsin", "Muhammad Umer", "Ahsan Bilal", "Muhammad Ali Jamshed", "John M. Cioffi"], "summary": "Modern 5G/6G deployments routinely face cross-configuration handovers--users\ntraversing cells with different antenna layouts, carrier frequencies, and\nscattering statistics--which inflate channel-prediction NMSE by $37.5\\%$ on\naverage when models are naively fine-tuned. The proposed improvement frames\nthis mismatch as a continual-learning problem and benchmarks three adaptation\nfamilies: replay with loss-aware reservoirs, synaptic-importance\nregularization, and memory-free learning-without-forgetting. Across three\nrepresentative 3GPP urban micro scenarios, the best replay and regularization\nschemes cut the high-SNR error floor by up to 2~dB ($\\approx 35\\%$), while even\nthe lightweight distillation recovers up to $30\\%$ improvement over baseline\nhandover prediction schemes. These results show that targeted rehearsal and\nparameter anchoring are essential for handover-robust CSI prediction and\nsuggest a clear migration path for embedding continual-learning hooks into\ncurrent channel prediction efforts in 3GPP--NR and O-RAN. The full codebase can\nbe found at\nhttps://github.com/ahmd-mohsin/continual-learning-channel-prediction.git.", "comment": "Accepted at ICML Workshop on ML4Wireless", "pdf_url": "http://arxiv.org/pdf/2506.22471v1", "categories": ["eess.SP", "cs.NI"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.22471v1", "date": "2025-06-19", "updated": "2025-06-19"}
{"id": "2506.23305", "title": "BPD-Neo: An MRI Dataset for Lung-Trachea Segmentation with Clinical Data for Neonatal Bronchopulmonary Dysplasia", "authors": ["Rachit Saluja", "Arzu Kovanlikaya", "Candace Chien", "Lauren Kathryn Blatt", "Jeffrey M. Perlman", "Stefan Worgall", "Mert R. Sabuncu", "Jonathan P. Dyke"], "summary": "Bronchopulmonary dysplasia (BPD) is a common complication among preterm\nneonates, with portable X-ray imaging serving as the standard diagnostic\nmodality in neonatal intensive care units (NICUs). However, lung magnetic\nresonance imaging (MRI) offers a non-invasive alternative that avoids sedation\nand radiation while providing detailed insights into the underlying mechanisms\nof BPD. Leveraging high-resolution 3D MRI data, advanced image processing and\nsemantic segmentation algorithms can be developed to assist clinicians in\nidentifying the etiology of BPD. In this dataset, we present MRI scans paired\nwith corresponding semantic segmentations of the lungs and trachea for 40\nneonates, the majority of whom are diagnosed with BPD. The imaging data consist\nof free-breathing 3D stack-of-stars radial gradient echo acquisitions, known as\nthe StarVIBE series. Additionally, we provide comprehensive clinical data and\nbaseline segmentation models, validated against clinical assessments, to\nsupport further research and development in neonatal lung imaging.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23305v1", "categories": ["eess.IV", "cs.CV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.23305v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.22907", "title": "MagShield: Towards Better Robustness in Sparse Inertial Motion Capture Under Magnetic Disturbances", "authors": ["Yunzhe Shao", "Xinyu Yi", "Lu Yin", "Shihui Guo", "Junhai Yong", "Feng Xu"], "summary": "This paper proposes a novel method called MagShield, designed to address the\nissue of magnetic interference in sparse inertial motion capture (MoCap)\nsystems. Existing Inertial Measurement Unit (IMU) systems are prone to\norientation estimation errors in magnetically disturbed environments, limiting\ntheir practical application in real-world scenarios. To address this problem,\nMagShield employs a \"detect-then-correct\" strategy, first detecting magnetic\ndisturbances through multi-IMU joint analysis, and then correcting orientation\nerrors using human motion priors. MagShield can be integrated with most\nexisting sparse inertial MoCap systems, improving their performance in\nmagnetically disturbed environments. Experimental results demonstrate that\nMagShield significantly enhances the accuracy of motion capture under magnetic\ninterference and exhibits good compatibility across different sparse inertial\nMoCap systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22907v1", "categories": ["cs.CV", "cs.GR"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22907v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.24052", "title": "Translating between the representations of an acyclic convex geometry of bounded degree", "authors": ["Oscar Defrain", "Arthur Ohana", "Simon Vilmin"], "summary": "We consider the problem of enumerating the irreducible closed sets of a\nclosure system given by an implicational base. In the context of Horn logic,\nthese correspond to Horn expressions and characteristic models, respectively.\nTo date, the complexity status of this problem is widely open, and it is\nfurther known to generalize the notorious hypergraph dualization problem, even\nin the context of acyclic convex geometries, i.e., closure systems admitting an\nacyclic implicational base. This paper studies this later class with a focus on\nthe degree, which corresponds to the maximal number of implications in which an\nelement occurs. We show that the problem is tractable for bounded values of\nthis parameter, even when relaxed to the notions of premise- and\nconclusion-degree. Our algorithms rely on structural properties of acyclic\nconvex geometries and involve various techniques from algorithmic enumeration\nsuch as solution graph traversal, saturation techniques, and a sequential\napproach leveraging from acyclicity. They are shown to perform in\nincremental-polynomial time for the computation of irreducible closed sets, and\nin polynomial time for the construction of an implicational base. Finally, we\nargue that our running times cannot be improved to polynomial delay using the\nstandard framework of flashlight search.", "comment": "36 pages, 8 figures", "pdf_url": "http://arxiv.org/pdf/2506.24052v1", "categories": ["cs.DS", "cs.DM", "math.CO"], "cate": "cs.DS", "url": "http://arxiv.org/abs/2506.24052v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22638", "title": "Layer Importance for Mathematical Reasoning is Forged in Pre-Training and Invariant after Post-Training", "authors": ["Aadim Nepal", "Safal Shrestha", "Anubhav Shrestha", "Minwu Kim", "Keith Ross"], "summary": "Large language models can exhibit improved mathematical reasoning\ncapabilities following post-training with instruction tuning, reinforcement\nlearning, or knowledge distillation. However, it remains unclear whether these\nimprovements are driven by major changes in transformer layers or from minor\nadjustments that leave the relative layer importance structures of the base\nmodel largely unchanged. We investigate this question through systematic\nlayer-wise ablation experiments, examining base, instruction-tuned,\nknowledge-distilled, and reinforcement learning variants on mathematical\nreasoning benchmarks. Our findings show that mathematical reasoning gives rise\nto a specific layer importance structure, and this structure persists across\nall post-training paradigms. Removal of such layers causes accuracy drops of up\nto 80%. In contrast, non-mathematical tasks like factual recall exhibit no\ncritical layers. This distinction suggests that mathematical reasoning requires\nspecialized layers that emerge during pre-training, while other non-reasoning\ntasks do not. From an information-theoretic perspective, we also observe that\nthese critical layers are the same layers where major representational\ntransformation occurs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22638v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22638v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.23128", "title": "Are Large Language Models Capable of Deep Relational Reasoning? Insights from DeepSeek-R1 and Benchmark Comparisons", "authors": ["Chi Chiu So", "Yueyue Sun", "Jun-Min Wang", "Siu Pang Yung", "Anthony Wai Keung Loh", "Chun Pong Chau"], "summary": "How far are Large Language Models (LLMs) in performing deep relational\nreasoning? In this paper, we evaluate and compare the reasoning capabilities of\nthree cutting-edge LLMs, namely, DeepSeek-R1, DeepSeek-V3 and GPT-4o, through a\nsuite of carefully designed benchmark tasks in family tree and general graph\nreasoning. Our experiments reveal that DeepSeek-R1 consistently achieves the\nhighest F1-scores across multiple tasks and problem sizes, demonstrating strong\naptitude in logical deduction and relational inference. However, all evaluated\nmodels, including DeepSeek-R1, struggle significantly as problem complexity\nincreases, largely due to token length limitations and incomplete output\nstructures. A detailed analysis of DeepSeek-R1's long Chain-of-Thought\nresponses uncovers its unique planning and verification strategies, but also\nhighlights instances of incoherent or incomplete reasoning, calling attention\nto the need for deeper scrutiny into LLMs' internal inference dynamics. We\nfurther discuss key directions for future work, including the role of\nmultimodal reasoning and the systematic examination of reasoning failures. Our\nfindings provide both empirical insights and theoretical implications for\nadvancing LLMs' reasoning abilities, particularly in tasks that demand\nstructured, multi-step logical inference. Our code repository will be publicly\navailable at https://github.com/kelvinhkcs/Deep-Relational-Reasoning.", "comment": "10 pages, 0 figures, accepted by 2025 IEEE international conference\n  on artificial intelligence testing (AITest)", "pdf_url": "http://arxiv.org/pdf/2506.23128v1", "categories": ["cs.AI"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.23128v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23485", "title": "Thought-Augmented Planning for LLM-Powered Interactive Recommender Agent", "authors": ["Haocheng Yu", "Yaxiong Wu", "Hao Wang", "Wei Guo", "Yong Liu", "Yawen Li", "Yuyang Ye", "Junping Du", "Enhong Chen"], "summary": "Interactive recommendation is a typical information-seeking task that allows\nusers to interactively express their needs through natural language and obtain\npersonalized recommendations. Large language model-powered (LLM-powered) agents\nhave become a new paradigm in interactive recommendations, effectively\ncapturing users' real-time needs and enhancing personalized experiences.\nHowever, due to limited planning and generalization capabilities, existing\nformulations of LLM-powered interactive recommender agents struggle to\neffectively address diverse and complex user intents, such as intuitive,\nunrefined, or occasionally ambiguous requests. To tackle this challenge, we\npropose a novel thought-augmented interactive recommender agent system (TAIRA)\nthat addresses complex user intents through distilled thought patterns.\nSpecifically, TAIRA is designed as an LLM-powered multi-agent system featuring\na manager agent that orchestrates recommendation tasks by decomposing user\nneeds and planning subtasks, with its planning capacity strengthened through\nThought Pattern Distillation (TPD), a thought-augmentation method that extracts\nhigh-level thoughts from the agent's and human experts' experiences. Moreover,\nwe designed a set of user simulation schemes to generate personalized queries\nof different difficulties and evaluate the recommendations based on specific\ndatasets. Through comprehensive experiments conducted across multiple datasets,\nTAIRA exhibits significantly enhanced performance compared to existing methods.\nNotably, TAIRA shows a greater advantage on more challenging tasks while\ngeneralizing effectively on novel tasks, further validating its superiority in\nmanaging complex user intents within interactive recommendation systems. The\ncode is publicly available at:https://github.com/Alcein/TAIRA.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23485v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.23485v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23049", "title": "AURA: Agent for Understanding, Reasoning, and Automated Tool Use in Voice-Driven Tasks", "authors": ["Leander Melroy Maben", "Gayathri Ganesh Lakshmy", "Srijith Radhakrishnan", "Siddhant Arora", "Shinji Watanabe"], "summary": "Despite advances in language and speech technologies, no open-source system\nenables full speech-to-speech, multi-turn dialogue with integrated tool use and\nagentic reasoning. We introduce AURA (Agent for Understanding, Reasoning, and\nAutomated Tool Use), the first open-source, speech-native assistant capable of\ncompleting complex, goal-driven tasks through dynamic tool invocation and\nmulti-turn conversation. AURA combines open-weight ASR, TTS, and LLMs in a\ncascaded pipeline and supports tools such as calendar booking, contact lookup,\nweb search, and email. Its modular design allows easy integration of new tools\nusing natural language prompts and action classes. On VoiceBench, AURA scores\n92.75% on OpenBookQA-outperforming all open-weight systems and nearing\nGPT-4o-and 4.39 on AlpacaEval, competitive with other open-weight systems.\nHuman evaluation shows 90% task success on complex, multi-turn speech tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23049v1", "categories": ["cs.AI", "cs.CL", "cs.SD", "eess.AS", "68T42, 68T50,", "I.2.7; I.2.11; H.5.5"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.23049v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.22698", "title": "Text Production and Comprehension by Human and Artificial Intelligence: Interdisciplinary Workshop Report", "authors": ["Emily Dux Speltz"], "summary": "This report synthesizes the outcomes of a recent interdisciplinary workshop\nthat brought together leading experts in cognitive psychology, language\nlearning, and artificial intelligence (AI)-based natural language processing\n(NLP). The workshop, funded by the National Science Foundation, aimed to\naddress a critical knowledge gap in our understanding of the relationship\nbetween AI language models and human cognitive processes in text comprehension\nand composition. Through collaborative dialogue across cognitive, linguistic,\nand technological perspectives, workshop participants examined the underlying\nprocesses involved when humans produce and comprehend text, and how AI can both\ninform our understanding of these processes and augment human capabilities. The\nworkshop revealed emerging patterns in the relationship between large language\nmodels (LLMs) and human cognition, with highlights on both the capabilities of\nLLMs and their limitations in fully replicating human-like language\nunderstanding and generation. Key findings include the potential of LLMs to\noffer insights into human language processing, the increasing alignment between\nLLM behavior and human language processing when models are fine-tuned with\nhuman feedback, and the opportunities and challenges presented by human-AI\ncollaboration in language tasks. By synthesizing these findings, this report\naims to guide future research, development, and implementation of LLMs in\ncognitive psychology, linguistics, and education. It emphasizes the importance\nof ethical considerations and responsible use of AI technologies while striving\nto enhance human capabilities in text comprehension and production through\neffective human-AI collaboration.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22698v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.22698v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23381", "title": "A new family of a posteriori error estimates for non-conforming finite element methods leading to stabilization-free error bounds", "authors": ["T. Chaumont-Frelet"], "summary": "We propose new a posteriori error estimators for non-conforming finite\nelement discretizations of second-order elliptic PDE problems. These estimators\nare based on novel reformulations of the standard Prager-Synge identity, and\nenable to prove efficiency estimates without extra stabilization terms in the\nerror measure for a large class of discretization schemes. We propose a\nresidual-based estimator for which the efficiency constant scales optimally in\npolynomial degree, as well as two equilibrated estimators that are\npolynomial-degree-robust. One of the two estimators further leads to guaranteed\nerror bounds.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23381v1", "categories": ["math.NA", "cs.NA"], "cate": "math.NA", "url": "http://arxiv.org/abs/2506.23381v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.22646", "title": "Speaker Targeting via Self-Speaker Adaptation for Multi-talker ASR", "authors": ["Weiqing Wang", "Taejin Park", "Ivan Medennikov", "Jinhan Wang", "Kunal Dhawan", "He Huang", "Nithin Rao Koluguri", "Jagadeesh Balam", "Boris Ginsburg"], "summary": "We propose a self-speaker adaptation method for streaming multi-talker\nautomatic speech recognition (ASR) that eliminates the need for explicit\nspeaker queries. Unlike conventional approaches requiring target speaker\nembeddings or enrollment audio, our technique dynamically adapts individual ASR\ninstances through speaker-wise speech activity prediction. The key innovation\ninvolves injecting speaker-specific kernels generated via speaker supervision\nactivations into selected ASR encoder layers. This enables instantaneous\nspeaker adaptation to target speakers while handling fully overlapped speech\neven in a streaming scenario. Experiments show state-of-the-art performance in\nboth offline and streaming scenarios, demonstrating that our self-adaptive\nmethod effectively addresses severe speech overlap through streamlined\nspeaker-focused recognition. The results validate the proposed self-speaker\nadaptation approach as a robust solution for multi-talker ASR under severe\noverlapping speech conditions.", "comment": "Accepted by INTERSPEECH 2025", "pdf_url": "http://arxiv.org/pdf/2506.22646v1", "categories": ["eess.AS", "cs.SD"], "cate": "eess.AS", "url": "http://arxiv.org/abs/2506.22646v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.23260", "title": "From Prompt Injections to Protocol Exploits: Threats in LLM-Powered AI Agents Workflows", "authors": ["Mohamed Amine Ferrag", "Norbert Tihanyi", "Djallel Hamouda", "Leandros Maglaras", "Merouane Debbah"], "summary": "Autonomous AI agents powered by large language models (LLMs) with structured\nfunction-calling interfaces have dramatically expanded capabilities for\nreal-time data retrieval, complex computation, and multi-step orchestration.\nYet, the explosive proliferation of plugins, connectors, and inter-agent\nprotocols has outpaced discovery mechanisms and security practices, resulting\nin brittle integrations vulnerable to diverse threats. In this survey, we\nintroduce the first unified, end-to-end threat model for LLM-agent ecosystems,\nspanning host-to-tool and agent-to-agent communications, formalize adversary\ncapabilities and attacker objectives, and catalog over thirty attack\ntechniques. Specifically, we organized the threat model into four domains:\nInput Manipulation (e.g., prompt injections, long-context hijacks, multimodal\nadversarial inputs), Model Compromise (e.g., prompt- and parameter-level\nbackdoors, composite and encrypted multi-backdoors, poisoning strategies),\nSystem and Privacy Attacks (e.g., speculative side-channels, membership\ninference, retrieval poisoning, social-engineering simulations), and Protocol\nVulnerabilities (e.g., exploits in Model Context Protocol (MCP), Agent\nCommunication Protocol (ACP), Agent Network Protocol (ANP), and Agent-to-Agent\n(A2A) protocol). For each category, we review representative scenarios, assess\nreal-world feasibility, and evaluate existing defenses. Building on our threat\ntaxonomy, we identify key open challenges and future research directions, such\nas securing MCP deployments through dynamic trust management and cryptographic\nprovenance tracking; designing and hardening Agentic Web Interfaces; and\nachieving resilience in multi-agent and federated environments. Our work\nprovides a comprehensive reference to guide the design of robust defense\nmechanisms and establish best practices for resilient LLM-agent workflows.", "comment": "29 pages, 15 figures, 6 tables", "pdf_url": "http://arxiv.org/pdf/2506.23260v1", "categories": ["cs.CR", "cs.AI"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.23260v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23125", "title": "Learning Motion Skills with Adaptive Assistive Curriculum Force in Humanoid Robots", "authors": ["Zhanxiang Cao", "Yang Zhang", "Buqing Nie", "Huangxuan Lin", "Haoyang Li", "Yue Gao"], "summary": "Learning policies for complex humanoid tasks remains both challenging and\ncompelling. Inspired by how infants and athletes rely on external support--such\nas parental walkers or coach-applied guidance--to acquire skills like walking,\ndancing, and performing acrobatic flips, we propose A2CF: Adaptive Assistive\nCurriculum Force for humanoid motion learning. A2CF trains a dual-agent system,\nin which a dedicated assistive force agent applies state-dependent forces to\nguide the robot through difficult initial motions and gradually reduces\nassistance as the robot's proficiency improves. Across three\nbenchmarks--bipedal walking, choreographed dancing, and backflip--A2CF achieves\nconvergence 30% faster than baseline methods, lowers failure rates by over 40%,\nand ultimately produces robust, support-free policies. Real-world experiments\nfurther demonstrate that adaptively applied assistive forces significantly\naccelerate the acquisition of complex skills in high-dimensional robotic\ncontrol.", "comment": "8 pages, 8 figures", "pdf_url": "http://arxiv.org/pdf/2506.23125v1", "categories": ["cs.RO"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.23125v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23190", "title": "Autonomous Vision-Aided UAV Positioning for Obstacle-Aware Wireless Connectivity", "authors": ["Kamran Shafafi", "Manuel Ricardo", "Rui Campos"], "summary": "Unmanned Aerial Vehicles (UAVs) offer a promising solution for enhancing\nwireless connectivity and Quality of Service (QoS) in urban environments,\nacting as aerial Wi-Fi access points or cellular base stations. Their\nflexibility and rapid deployment capabilities make them suitable for addressing\ninfrastructure gaps and traffic surges. However, optimizing UAV positions to\nmaintain Line of Sight (LoS) links with ground User Equipment (UEs) remains\nchallenging in obstacle-dense urban scenarios. This paper proposes VTOPA, a\nVision-Aided Traffic- and Obstacle-Aware Positioning Algorithm that\nautonomously extracts environmental information -- such as obstacles and UE\nlocations -- via computer vision and optimizes UAV positioning accordingly. The\nalgorithm prioritizes LoS connectivity and dynamically adapts to user traffic\ndemands in real time. Evaluated through simulations in ns-3, VTOPA achieves up\nto a 50% increase in aggregate throughput and a 50% reduction in delay, without\ncompromising fairness, outperforming benchmark approaches in obstacle-rich\nenvironments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23190v1", "categories": ["cs.NI"], "cate": "cs.NI", "url": "http://arxiv.org/abs/2506.23190v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23696", "title": "What Challenges Do Developers Face When Using Verification-Aware Programming Languages?", "authors": ["Francisco Oliveira", "Alexandra Mendes", "Carolina Carreira"], "summary": "Software reliability is critical in ensuring that the digital systems we\ndepend on function correctly. In software development, increasing software\nreliability often involves testing. However, for complex and critical systems,\ndevelopers can use Design by Contract (DbC) methods to define precise\nspecifications that software components must satisfy. Verification-Aware (VA)\nprogramming languages support DbC and formal verification at compile-time or\nrun-time, offering stronger correctness guarantees than traditional testing.\nHowever, despite the strong guarantees provided by VA languages, their adoption\nremains limited. In this study, we investigate the barriers to adopting VA\nlanguages by analyzing developer discussions on public forums using topic\nmodeling techniques. We complement this analysis with a developer survey to\nbetter understand the practical challenges associated with VA languages. Our\nfindings reveal key obstacles to adoption, including steep learning curves and\nusability issues. Based on these insights, we identify actionable\nrecommendations to improve the usability and accessibility of VA languages. Our\nfindings suggest that simplifying tool interfaces, providing better educational\nmaterials, and improving integration with everyday development environments\ncould improve the usability and adoption of these languages. Our work provides\nactionable insights for improving the usability of VA languages and making\nverification tools more accessible.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23696v1", "categories": ["cs.SE", "cs.PL"], "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.23696v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23075", "title": "CSBrain: A Cross-scale Spatiotemporal Brain Foundation Model for EEG Decoding", "authors": ["Yuchen Zhou", "Jiamin Wu", "Zichen Ren", "Zhouheng Yao", "Weiheng Lu", "Kunyu Peng", "Qihao Zheng", "Chunfeng Song", "Wanli Ouyang", "Chao Gou"], "summary": "Understanding and decoding brain activity from electroencephalography (EEG)\nsignals is a fundamental challenge in neuroscience and AI, with applications in\ncognition, emotion recognition, diagnosis, and brain-computer interfaces. While\nrecent EEG foundation models advance generalized decoding via unified\narchitectures and large-scale pretraining, they adopt a scale-agnostic dense\nmodeling paradigm inherited from NLP and vision. This design neglects a core\nproperty of neural activity: cross-scale spatiotemporal structure. EEG task\npatterns span a wide range of temporal and spatial scales, from short bursts to\nslow rhythms, and from localized cortical responses to distributed\ninteractions. Ignoring this diversity leads to suboptimal representations and\nweak generalization. We propose CSBrain, a Cross-scale Spatiotemporal Brain\nfoundation model for generalized EEG decoding. CSBrain introduces: (i)\nCross-scale Spatiotemporal Tokenization (CST), which aggregates multi-scale\nfeatures from localized temporal windows and anatomical brain regions into\ncompact scale-aware tokens; and (ii) Structured Sparse Attention (SSA), which\ncaptures cross-window and cross-region dependencies, enhancing scale diversity\nwhile removing spurious correlations. CST and SSA are alternately stacked to\nprogressively integrate multi-scale dependencies. Experiments on 11 EEG tasks\nacross 16 datasets show that CSBrain consistently outperforms task-specific and\nfoundation model baselines. These results establish cross-scale modeling as a\nkey inductive bias and position CSBrain as a robust backbone for future\nbrain-AI research.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23075v1", "categories": ["cs.HC", "cs.LG", "eess.SP", "q-bio.NC"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.23075v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23010", "title": "On Universality of Non-Separable Approximate Message Passing Algorithms", "authors": ["Max Lovig", "Tianhao Wang", "Zhou Fan"], "summary": "Mean-field characterizations of first-order iterative algorithms -- including\nApproximate Message Passing (AMP), stochastic and proximal gradient descent,\nand Langevin diffusions -- have enabled a precise understanding of learning\ndynamics in many statistical applications. For algorithms whose non-linearities\nhave a coordinate-separable form, it is known that such characterizations enjoy\na degree of universality with respect to the underlying data distribution.\nHowever, mean-field characterizations of non-separable algorithm dynamics have\nlargely remained restricted to i.i.d. Gaussian or rotationally-invariant data.\n  In this work, we initiate a study of universality for non-separable AMP\nalgorithms. We identify a general condition for AMP with polynomial\nnon-linearities, in terms of a Bounded Composition Property (BCP) for their\nrepresenting tensors, to admit a state evolution that holds universally for\nmatrices with non-Gaussian entries. We then formalize a condition of\nBCP-approximability for Lipschitz AMP algorithms to enjoy a similar universal\nguarantee. We demonstrate that many common classes of non-separable\nnon-linearities are BCP-approximable, including local denoisers, spectral\ndenoisers for generic signals, and compositions of separable functions with\ngeneric linear maps, implying the universality of state evolution for AMP\nalgorithms employing these non-linearities.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23010v1", "categories": ["math.ST", "cs.IT", "cs.LG", "math.IT", "math.PR", "stat.TH"], "cate": "math.ST", "url": "http://arxiv.org/abs/2506.23010v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23583", "title": "Detect \\& Score: Privacy-Preserving Misbehaviour Detection and Contribution Evaluation in Federated Learning", "authors": ["Marvin Xhemrishi", "Alexandre Graell i Amat", "Bal√°zs Pej√≥"], "summary": "Federated learning with secure aggregation enables private and collaborative\nlearning from decentralised data without leaking sensitive client information.\nHowever, secure aggregation also complicates the detection of malicious client\nbehaviour and the evaluation of individual client contributions to the\nlearning. To address these challenges, QI (Pejo et al.) and FedGT (Xhemrishi et\nal.) were proposed for contribution evaluation (CE) and misbehaviour detection\n(MD), respectively. QI, however, lacks adequate MD accuracy due to its reliance\non the random selection of clients in each training round, while FedGT lacks\nthe CE ability. In this work, we combine the strengths of QI and FedGT to\nachieve both robust MD and accurate CE. Our experiments demonstrate superior\nperformance compared to using either method independently.", "comment": "The shorter version is accepted at FL-AsiaCCS 25", "pdf_url": "http://arxiv.org/pdf/2506.23583v1", "categories": ["cs.CR", "cs.DC", "cs.LG"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.23583v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.24118", "title": "Scaling Human Judgment in Community Notes with LLMs", "authors": ["Haiwen Li", "Soham De", "Manon Revel", "Andreas Haupt", "Brad Miller", "Keith Coleman", "Jay Baxter", "Martin Saveski", "Michiel A. Bakker"], "summary": "This paper argues for a new paradigm for Community Notes in the LLM era: an\nopen ecosystem where both humans and LLMs can write notes, and the decision of\nwhich notes are helpful enough to show remains in the hands of humans. This\napproach can accelerate the delivery of notes, while maintaining trust and\nlegitimacy through Community Notes' foundational principle: A community of\ndiverse human raters collectively serve as the ultimate evaluator and arbiter\nof what is helpful. Further, the feedback from this diverse community can be\nused to improve LLMs' ability to produce accurate, unbiased, broadly helpful\nnotes--what we term Reinforcement Learning from Community Feedback (RLCF). This\nbecomes a two-way street: LLMs serve as an asset to humans--helping deliver\ncontext quickly and with minimal effort--while human feedback, in turn,\nenhances the performance of LLMs. This paper describes how such a system can\nwork, its benefits, key new risks and challenges it introduces, and a research\nagenda to solve those challenges and realize the potential of this approach.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.24118v1", "categories": ["cs.CY", "cs.SI"], "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.24118v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22554", "title": "Seamless Interaction: Dyadic Audiovisual Motion Modeling and Large-Scale Dataset", "authors": ["Vasu Agrawal", "Akinniyi Akinyemi", "Kathryn Alvero", "Morteza Behrooz", "Julia Buffalini", "Fabio Maria Carlucci", "Joy Chen", "Junming Chen", "Zhang Chen", "Shiyang Cheng", "Praveen Chowdary", "Joe Chuang", "Antony D'Avirro", "Jon Daly", "Ning Dong", "Mark Duppenthaler", "Cynthia Gao", "Jeff Girard", "Martin Gleize", "Sahir Gomez", "Hongyu Gong", "Srivathsan Govindarajan", "Brandon Han", "Sen He", "Denise Hernandez", "Yordan Hristov", "Rongjie Huang", "Hirofumi Inaguma", "Somya Jain", "Raj Janardhan", "Qingyao Jia", "Christopher Klaiber", "Dejan Kovachev", "Moneish Kumar", "Hang Li", "Yilei Li", "Pavel Litvin", "Wei Liu", "Guangyao Ma", "Jing Ma", "Martin Ma", "Xutai Ma", "Lucas Mantovani", "Sagar Miglani", "Sreyas Mohan", "Louis-Philippe Morency", "Evonne Ng", "Kam-Woh Ng", "Tu Anh Nguyen", "Amia Oberai", "Benjamin Peloquin", "Juan Pino", "Jovan Popovic", "Omid Poursaeed", "Fabian Prada", "Alice Rakotoarison", "Alexander Richard", "Christophe Ropers", "Safiyyah Saleem", "Vasu Sharma", "Alex Shcherbyna", "Jia Shen", "Jie Shen", "Anastasis Stathopoulos", "Anna Sun", "Paden Tomasello", "Tuan Tran", "Arina Turkatenko", "Bo Wan", "Chao Wang", "Jeff Wang", "Mary Williamson", "Carleigh Wood", "Tao Xiang", "Yilin Yang", "Julien Yao", "Chen Zhang", "Jiemin Zhang", "Xinyue Zhang", "Jason Zheng", "Pavlo Zhyzheria", "Jan Zikes", "Michael Zollhoefer"], "summary": "Human communication involves a complex interplay of verbal and nonverbal\nsignals, essential for conveying meaning and achieving interpersonal goals. To\ndevelop socially intelligent AI technologies, it is crucial to develop models\nthat can both comprehend and generate dyadic behavioral dynamics. To this end,\nwe introduce the Seamless Interaction Dataset, a large-scale collection of over\n4,000 hours of face-to-face interaction footage from over 4,000 participants in\ndiverse contexts. This dataset enables the development of AI technologies that\nunderstand dyadic embodied dynamics, unlocking breakthroughs in virtual agents,\ntelepresence experiences, and multimodal content analysis tools. We also\ndevelop a suite of models that utilize the dataset to generate dyadic motion\ngestures and facial expressions aligned with human speech. These models can\ntake as input both the speech and visual behavior of their interlocutors. We\npresent a variant with speech from an LLM model and integrations with 2D and 3D\nrendering methods, bringing us closer to interactive virtual agents.\nAdditionally, we describe controllable variants of our motion models that can\nadapt emotional responses and expressivity levels, as well as generating more\nsemantically-relevant gestures. Finally, we discuss methods for assessing the\nquality of these dyadic motion models, which are demonstrating the potential\nfor more intuitive and responsive human-AI interactions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22554v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22554v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.23304", "title": "ANN-Based Grid Impedance Estimation for Adaptive Gain Scheduling in VSG Under Dynamic Grid Conditions", "authors": ["Quang-Manh Hoang", "Van Nam Nguyen", "Taehyung Kim", "Guilherme Vieira Hollweg", "Wencong Su", "Van-Hai Bui"], "summary": "In contrast to grid-following inverters, Virtual Synchronous Generators\n(VSGs) perform well under weak grid conditions but may become unstable when the\ngrid is strong. Grid strength depends on grid impedance, which unfortunately\nvaries over time. In this paper, we propose a novel adaptive gain-scheduling\ncontrol scheme for VSGs. First, an Artificial Neural Network (ANN) estimates\nthe fundamental-frequency grid impedance; then these estimates are fed into an\nadaptive gain-scheduling function to recalculate controller parameters under\nvarying grid conditions. The proposed method is validated in Simulink and\ncompared with a conventional VSG employing fixed controller gains. The results\ndemonstrate that settling times and overshoot percentages remain consistent\nacross different grid conditions. Additionally, previously unseen grid\nimpedance values are estimated with high accuracy and minimal time delay,\nmaking the approach well suited for real-time gain-scheduling control.", "comment": "Paper was accepted for IEEE Energy Conversion Congress and Exposition\n  (ECCE) 2025, Philadelphia, PA, USA", "pdf_url": "http://arxiv.org/pdf/2506.23304v1", "categories": ["eess.SY", "cs.SY"], "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.23304v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.22472", "title": "Optical Waveguide-based Spider Web Enables Resilient Impact Detection and Localization", "authors": ["Dylan Wilson", "Marco Pontin", "Peter Walters", "Perla Maiolino"], "summary": "Spiders use their webs as multifunctional tools that enable capturing and\nlocalizing prey and more general environmental sensing through vibrations.\nInspired by their biological function, we present a spider web-inspired optical\nwaveguide system for resilient impulse detection and localization. The\nstructure consists of six clear thermoplastic polyurethane (TPU) waveguides\narranged radially and interconnected by a spiral TPU thread, mimicking orb\nspider webs. Light transmission losses, induced by vibrations, are measured via\ncoupled LEDs and photo-diodes, allowing real-time detection. We systematically\ncharacterize individual waveguides, analyzing key parameters such as tension,\nimpulse position, and break angle to optimize vibrational response. The\ncomplete system is validated through controlled experiments, revealing a 5 ms\npropagation delay in vibration transfer between adjacent radii, enhancing\nlocalization capabilities. We demonstrate a robust impulse detection and\nlocalization algorithm leveraging time delay analysis, achieving reliable event\nidentification even in cases of sensor failure. This study highlights the\npotential of bioinspired optical waveguide structures for adaptive sensing,\nwith applications in soft robotics, structural monitoring, and environmental\nsensing.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22472v1", "categories": ["eess.SP", "cs.RO"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.22472v1", "date": "2025-06-20", "updated": "2025-06-20"}
{"id": "2506.23309", "title": "SurgTPGS: Semantic 3D Surgical Scene Understanding with Text Promptable Gaussian Splatting", "authors": ["Yiming Huang", "Long Bai", "Beilei Cui", "Kun Yuan", "Guankun Wang", "Mobarakol Islam", "Nicolas Padoy", "Nassir Navab", "Hongliang Ren"], "summary": "In contemporary surgical research and practice, accurately comprehending 3D\nsurgical scenes with text-promptable capabilities is particularly crucial for\nsurgical planning and real-time intra-operative guidance, where precisely\nidentifying and interacting with surgical tools and anatomical structures is\nparamount. However, existing works focus on surgical vision-language model\n(VLM), 3D reconstruction, and segmentation separately, lacking support for\nreal-time text-promptable 3D queries. In this paper, we present SurgTPGS, a\nnovel text-promptable Gaussian Splatting method to fill this gap. We introduce\na 3D semantics feature learning strategy incorporating the Segment Anything\nmodel and state-of-the-art vision-language models. We extract the segmented\nlanguage features for 3D surgical scene reconstruction, enabling a more\nin-depth understanding of the complex surgical environment. We also propose\nsemantic-aware deformation tracking to capture the seamless deformation of\nsemantic features, providing a more precise reconstruction for both texture and\nsemantic features. Furthermore, we present semantic region-aware optimization,\nwhich utilizes regional-based semantic information to supervise the training,\nparticularly promoting the reconstruction quality and semantic smoothness. We\nconduct comprehensive experiments on two real-world surgical datasets to\ndemonstrate the superiority of SurgTPGS over state-of-the-art methods,\nhighlighting its potential to revolutionize surgical practices. SurgTPGS paves\nthe way for developing next-generation intelligent surgical systems by\nenhancing surgical precision and safety. Our code is available at:\nhttps://github.com/lastbasket/SurgTPGS.", "comment": "MICCAI 2025. Project Page:\n  https://lastbasket.github.io/MICCAI-2025-SurgTPGS/", "pdf_url": "http://arxiv.org/pdf/2506.23309v1", "categories": ["eess.IV", "cs.CV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.23309v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.22926", "title": "Coordinated 2D-3D Visualization of Volumetric Medical Data in XR with Multimodal Interactions", "authors": ["Qixuan Liu", "Shi Qiu", "Yinqiao Wang", "Xiwen Wu", "Kenneth Siu Ho Chok", "Chi-Wing Fu", "Pheng-Ann Heng"], "summary": "Volumetric medical imaging technologies produce detailed 3D representations\nof anatomical structures. However, effective medical data visualization and\nexploration pose significant challenges, especially for individuals with\nlimited medical expertise. We introduce a novel XR-based system with two key\ninnovations: (1) a coordinated visualization module integrating Multi-layered\nMulti-planar Reconstruction with 3D mesh models and (2) a multimodal\ninteraction framework combining hand gestures with LLM-enabled voice commands.\nWe conduct preliminary evaluations, including a 15-participant user study and\nexpert interviews, to demonstrate the system's abilities to enhance spatial\nunderstanding and reduce cognitive load. Experimental results show notable\nimprovements in task completion times, usability metrics, and interaction\neffectiveness enhanced by LLM-driven voice control. While identifying areas for\nfuture refinement, our findings highlight the potential of this immersive\nvisualization system to advance medical training and clinical practice. Our\ndemo application and supplemental materials are available for download at:\nhttps://osf.io/bpjq5/.", "comment": "IEEE VIS 2025 Short Paper", "pdf_url": "http://arxiv.org/pdf/2506.22926v1", "categories": ["cs.HC", "cs.GR", "cs.MM"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.22926v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.24114", "title": "A Refined Kernel for $d$-Hitting Set", "authors": ["Yuxi Liu", "Mingyu Xiao"], "summary": "The $d$-Hitting Set problem is a fundamental problem in parameterized\ncomplexity, which asks whether a given hypergraph contains a vertex subset $S$\nof size at most $k$ that intersects every hyperedge (i.e., $S \\cap e \\neq\n\\emptyset$ for each hyperedge $e$). The best known kernel for this problem,\nestablished by Abu-Khzam [1], has $(2d - 1)k^{d - 1} + k$ vertices. This result\nhas been very widely used in the literature as many problems can be modeled as\na special $d$-Hitting Set problem. In this work, we present a refinement to\nthis result by employing linear programming techniques to construct crown\ndecompositions in hypergraphs. This approach yields a slight but notable\nimprovement, reducing the size to $(2d - 2)k^{d - 1} + k$ vertices.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.24114v1", "categories": ["cs.DS"], "cate": "cs.DS", "url": "http://arxiv.org/abs/2506.24114v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22645", "title": "Cost-effective Reduced-Order Modeling via Bayesian Active Learning", "authors": ["Amir Hossein Rahmati", "Nathan M. Urban", "Byung-Jun Yoon", "Xiaoning Qian"], "summary": "Machine Learning surrogates have been developed to accelerate solving systems\ndynamics of complex processes in different science and engineering\napplications. To faithfully capture governing systems dynamics, these methods\nrely on large training datasets, hence restricting their applicability in\nreal-world problems. In this work, we propose BayPOD-AL, an active learning\nframework based on an uncertainty-aware Bayesian proper orthogonal\ndecomposition (POD) approach, which aims to effectively learn reduced-order\nmodels from high-fidelity full-order models representing complex systems.\nExperimental results on predicting the temperature evolution over a rod\ndemonstrate BayPOD-AL's effectiveness in suggesting the informative data and\nreducing computational cost related to constructing a training dataset compared\nto other uncertainty-guided active learning strategies. Furthermore, we\ndemonstrate BayPOD-AL's generalizability and efficiency by evaluating its\nperformance on a dataset of higher temporal resolution than the training\ndataset.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22645v1", "categories": ["cs.LG", "stat.ML"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22645v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.23141", "title": "Context-Driven Knowledge Graph Completion with Semantic-Aware Relational Message Passing", "authors": ["Siyuan Li", "Ruitong Liu", "Yan Wen", "Te Sun"], "summary": "Semantic context surrounding a triplet $(h, r, t)$ is crucial for Knowledge\nGraph Completion (KGC), providing vital cues for prediction. However,\ntraditional node-based message passing mechanisms, when applied to knowledge\ngraphs, often introduce noise and suffer from information dilution or\nover-smoothing by indiscriminately aggregating information from all neighboring\nedges. To address this challenge, we propose a semantic-aware relational\nmessage passing. A core innovation of this framework is the introduction of a\n\\textbf{semantic-aware Top-K neighbor selection strategy}. Specifically, this\nstrategy first evaluates the semantic relevance between a central node and its\nincident edges within a shared latent space, selecting only the Top-K most\npertinent ones. Subsequently, information from these selected edges is\neffectively fused with the central node's own representation using a\n\\textbf{multi-head attention aggregator} to generate a semantically focused\nnode message. In this manner, our model not only leverages the structure and\nfeatures of edges within the knowledge graph but also more accurately captures\nand propagates the contextual information most relevant to the specific link\nprediction task, thereby effectively mitigating interference from irrelevant\ninformation. Extensive experiments demonstrate that our method achieves\nsuperior performance compared to existing approaches on several established\nbenchmarks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23141v1", "categories": ["cs.AI"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.23141v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23662", "title": "Zero-Shot Contextual Embeddings via Offline Synthetic Corpus Generation", "authors": ["Philip Lippmann", "Jie Yang"], "summary": "Context-aware embedding methods boost retrieval accuracy by conditioning on\ncorpus statistics (e.g., term co-occurrence and topical patterns) extracted\nfrom neighboring documents. However, this context-aware approach requires\naccess to the target corpus or requires domain-specific finetuning, posing\npractical barriers in privacy-sensitive or resource-constrained settings. We\npresent ZEST, a zero-shot contextual adaptation framework that replaces real\ncorpus access with a one-time offline synthesis of a compact proxy. Given only\na handful exemplar documents representative of the general target domain, we\nuse a multi-step hierarchical procedure to generate a synthetic context corpus\nof several hundred documents that aims to emulate key domain-specific\ndistributions. At inference, the frozen context-aware encoder uses this proxy\ncorpus -- without any finetuning or target corpus access -- to produce\ndomain-adapted embeddings. Across the MTEB benchmark, ZEST's zero-shot\nsynthetic context adaptation using only five example documents performs within\n0.5% of models leveraging full target corpus access -- demonstrating remarkable\nefficacy without any retraining. ZEST thus provides a practical method for\ndeploying high-performance, adaptable embeddings in constrained environments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23662v1", "categories": ["cs.CL", "cs.IR"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.23662v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23094", "title": "TOMI: Transforming and Organizing Music Ideas for Multi-Track Compositions with Full-Song Structure", "authors": ["Qi He", "Gus Xia", "Ziyu Wang"], "summary": "Hierarchical planning is a powerful approach to model long sequences\nstructurally. Aside from considering hierarchies in the temporal structure of\nmusic, this paper explores an even more important aspect: concept hierarchy,\nwhich involves generating music ideas, transforming them, and ultimately\norganizing them--across musical time and space--into a complete composition. To\nthis end, we introduce TOMI (Transforming and Organizing Music Ideas) as a\nnovel approach in deep music generation and develop a TOMI-based model via\ninstruction-tuned foundation LLM. Formally, we represent a multi-track\ncomposition process via a sparse, four-dimensional space characterized by clips\n(short audio or MIDI segments), sections (temporal positions), tracks\n(instrument layers), and transformations (elaboration methods). Our model is\ncapable of generating multi-track electronic music with full-song structure,\nand we further integrate the TOMI-based model with the REAPER digital audio\nworkstation, enabling interactive human-AI co-creation. Experimental results\ndemonstrate that our approach produces higher-quality electronic music with\nstronger structural coherence compared to baselines.", "comment": "9 pages, 4 figures, 2 tables. To be published in ISMIR 2025", "pdf_url": "http://arxiv.org/pdf/2506.23094v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.23094v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.22724", "title": "The Translation Barrier Hypothesis: Multilingual Generation with Large Language Models Suffers from Implicit Translation Failure", "authors": ["Niyati Bafna", "Tianjian Li", "Kenton Murray", "David R. Mortensen", "David Yarowsky", "Hale Sirin", "Daniel Khashabi"], "summary": "Multilingual generation with large language models (LLMs) is often of poor\nquality for mid- to low-resource languages. Building on insights from\ninterpretability, we demonstrate the existence of an implicit\ntask-solving-->translation pipeline for generation, whereby the model first\nsolves the required task in a largely target-language-agnostic manner, and\nsubsequently translates answer concepts into the intended target language. We\nhypothesize that the failure of the translation stage is an important culprit\nfor the observed low quality of final outputs, and formalize this as the\ntranslation barrier hypothesis. We test this hypothesis for a word translation\ntask across 108 language pairs, using logit lens to observe model processing in\nintermediate layers. We find that a significant portion of overall failures\nindeed stems from translation failure, or the model's inability to translate\ncorrectly solved intermediate concepts into the target language. This is\nespecially true for low-resource target languages. Our results highlight an\nimportant hurdle for end-to-end multilingual generation, and lend guiding\ninsights for future work seeking to improve multilinguality in LLMs.", "comment": "23 pages incl. appendix", "pdf_url": "http://arxiv.org/pdf/2506.22724v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.22724v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23449", "title": "Fourth-order compact difference schemes for the one-dimensional Euler-Bernoulli beam equation with damping term", "authors": ["Wenjie Huang", "Hao Wang", "Shiquan Zhang", "Qinyi Zhang"], "summary": "This paper proposes and analyzes a finite difference method based on compact\nschemes for the Euler-Bernoulli beam equation with damping terms. The method\nachieves fourth-order accuracy in space and second-order accuracy in time,\nwhile requiring only three spatial grid points within a single compact stencil.\nSpatial discretization is carried out using a compact finite difference scheme,\nwith a variable substitution technique employed to reduce the order of the\nequation and effectively handle the damping terms. For the temporal\ndiscretization, the Crank-Nicolson scheme is applied. The consistency,\nstability, and convergence of the proposed method are rigorously proved.\nNumerical experiments are presented to verify the theoretical results and\ndemonstrate the accuracy and efficiency of the method.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23449v1", "categories": ["math.NA", "cs.NA"], "cate": "math.NA", "url": "http://arxiv.org/abs/2506.23449v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22846", "title": "Boosting CTC-Based ASR Using LLM-Based Intermediate Loss Regularization", "authors": ["Duygu Altinok"], "summary": "End-to-end (E2E) automatic speech recognition (ASR) systems have\nrevolutionized the field by integrating all components into a single neural\nnetwork, with attention-based encoder-decoder models achieving state-of-the-art\nperformance. However, their autoregressive decoding process limits inference\nspeed, making them unsuitable for real-time applications. In contrast,\nCTC-based models offer faster, non-autoregressive decoding but struggle to\nmodel linguistic dependencies effectively. Addressing this challenge, we\npropose a novel auxiliary loss framework called Language-Aware Intermediate\nLoss (LAIL) to enhance CTC-based ASR using the linguistic knowledge of large\nlanguage models (LLMs). By attaching connector layers to intermediate encoder\nlayers, LAIL maps outputs to the embedding space of an LLM and computes a\ncausal language modeling loss during training. This approach enhances\nlinguistic modeling while preserving the computational efficiency of CTC\ndecoding. Using the Conformer architecture and various LLaMA models, we\ndemonstrate significant improvements in Word Error Rate (WER) on the\nLibriSpeech, TEDLIUM2, and WSJ corpora, achieving state-of-the-art performance\nfor CTC-based ASR with minimal computational overhead.", "comment": "This is the accepted version of an article accepted to the TSD 2025\n  conference, published in Springer Lecture Notes in Artificial Intelligence\n  (LNAI). The final authenticated version is available online at SpringerLink", "pdf_url": "http://arxiv.org/pdf/2506.22846v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.22846v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23294", "title": "Threshold Signatures for Central Bank Digital Currencies", "authors": ["Mostafa Abdelrahman", "Filip Rezabek", "Lars Hupel", "Kilian Glas", "Georg Carle"], "summary": "Digital signatures are crucial for securing Central Bank Digital Currencies\n(CBDCs) transactions. Like most forms of digital currencies, CBDC solutions\nrely on signatures for transaction authenticity and integrity, leading to major\nissues in the case of private key compromise. Our work explores threshold\nsignature schemes (TSSs) in the context of CBDCs. TSSs allow distributed key\nmanagement and signing, reducing the risk of a compromised key. We analyze\nCBDC-specific requirements, considering the applicability of TSSs, and use\nFilia CBDC solution as a base for a detailed evaluation. As most of the current\nsolutions rely on ECDSA for compatibility, we focus on ECDSA-based TSSs and\ntheir supporting libraries. Our performance evaluation measured the\ncomputational and communication complexity across key processes, as well as the\nthroughput and latency of end-to-end transactions. The results confirm that TSS\ncan enhance the security of CBDC implementations while maintaining acceptable\nperformance for real-world deployments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23294v1", "categories": ["cs.CR"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.23294v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23126", "title": "ParticleFormer: A 3D Point Cloud World Model for Multi-Object, Multi-Material Robotic Manipulation", "authors": ["Suning Huang", "Qianzhong Chen", "Xiaohan Zhang", "Jiankai Sun", "Mac Schwager"], "summary": "3D world models (i.e., learning-based 3D dynamics models) offer a promising\napproach to generalizable robotic manipulation by capturing the underlying\nphysics of environment evolution conditioned on robot actions. However,\nexisting 3D world models are primarily limited to single-material dynamics\nusing a particle-based Graph Neural Network model, and often require\ntime-consuming 3D scene reconstruction to obtain 3D particle tracks for\ntraining. In this work, we present ParticleFormer, a Transformer-based point\ncloud world model trained with a hybrid point cloud reconstruction loss,\nsupervising both global and local dynamics features in multi-material,\nmulti-object robot interactions. ParticleFormer captures fine-grained\nmulti-object interactions between rigid, deformable, and flexible materials,\ntrained directly from real-world robot perception data without an elaborate\nscene reconstruction. We demonstrate the model's effectiveness both in 3D scene\nforecasting tasks, and in downstream manipulation tasks using a Model\nPredictive Control (MPC) policy. In addition, we extend existing dynamics\nlearning benchmarks to include diverse multi-material, multi-object interaction\nscenarios. We validate our method on six simulation and three real-world\nexperiments, where it consistently outperforms leading baselines by achieving\nsuperior dynamics prediction accuracy and less rollout error in downstream\nvisuomotor tasks. Experimental videos are available at\nhttps://particleformer.github.io/.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23126v1", "categories": ["cs.RO"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.23126v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23350", "title": "On the Resilience of Underwater Semantic Wireless Communications", "authors": ["Jo√£o Pedro Loureiro", "Patr√≠cia Delgado", "Tom√°s Feliciano Ribeiro", "Filipe B. Teixeira", "Rui Campos"], "summary": "Underwater wireless communications face significant challenges due to\npropagation constraints, limiting the effectiveness of traditional radio and\noptical technologies. Long-range acoustic communications support distances up\nto a few kilometers, but suffer from low bandwidth, high error ratios, and\nmultipath interference. Semantic communications, which focus on transmitting\nextracted semantic features rather than raw data, present a promising solution\nby significantly reducing the volume of data transmitted over the wireless\nlink.\n  This paper evaluates the resilience of SAGE, a semantic-oriented\ncommunications framework that combines semantic processing with Generative\nArtificial Intelligence (GenAI) to compress and transmit image data as textual\ndescriptions over acoustic links. To assess robustness, we use a\ncustom-tailored simulator that introduces character errors observed in\nunderwater acoustic channels. Evaluation results show that SAGE can\nsuccessfully reconstruct meaningful image content even under varying error\nconditions, highlighting its potential for robust and efficient underwater\nwireless communication in harsh environments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23350v1", "categories": ["cs.NI"], "cate": "cs.NI", "url": "http://arxiv.org/abs/2506.23350v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23715", "title": "Towards a Science of Developer eXperience (DevX)", "authors": ["Benoit Combemale"], "summary": "As software continues to permeate nearly every facet of modern life, the\ncomplexity and ubiquity of digital services underscore the need for\nsustainable, effective, and inclusive software development practices. Although\nsoftware engineering has made significant progress in technical challenges\nsince its inception, the human experience of those involved in software\ncreation, broadly defined as developers, remains underexplored. This column\nadvocates for the formal recognition of Developer eXperience (DevX) as a\ndistinct research field. We argue that DevX profoundly influences critical\ndevelopment activities and overall productivity, especially as development\nbecomes increasingly collaborative and diverse in terms of application domains.\nBuilding on existing efforts to measure and enhance DevX, we identify key\nrationales, scientific enablers, and interdisciplinary intersections that\nsupport this emerging discipline. We also outline the core scientific\nchallenges ahead, aiming to call for actions from the research community and to\npromote more human-centered approaches to software engineering.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23715v1", "categories": ["cs.SE"], "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.23715v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23116", "title": "A User Experience 3.0 (UX 3.0) Paradigm Framework: Designing for Human-Centered AI Experiences", "authors": ["Wei Xu"], "summary": "User experience (UX) practices have evolved in stages and are entering a\ntransformative phase (UX 3.0), driven by AI technologies and shifting user\nneeds. Human-centered AI (HCAI) experiences are emerging, necessitating new UX\napproaches to support UX practices in the AI era. We propose a UX 3.0 paradigm\nframework to respond and guide UX practices in developing HCAI systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23116v1", "categories": ["cs.HC"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.23116v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23045", "title": "Zak-OFDM: Low Complexity Joint Equalization of OFDM Carriers in Doubly-Spread Channels", "authors": ["Saif Khan Mohammed", "Sandesh Rao Mattu", "Nishant Mehrotra", "Venkatesh Khammammetti", "Robert Calderbank"], "summary": "We communicate over wireless channels by first estimating and then equalizing\nthe effective channel. In Zak-OTFS (orthogonal time frequency space) modulation\nthe carrier waveform is a pulse in the delay-Doppler (DD) domain, formally a\nquasi-periodic localized function with specific periods along delay and\nDoppler. When the channel delay spread is less than the delay period, and the\nchannel Doppler spread is less than the Doppler period, the response to a\nsingle Zak-OTFS carrier provides an image of the scattering environment and can\nbe used to predict the effective channel at all other carriers. This makes\nchannel estimation straightforward, and there is no loss in spectral efficiency\nsince it is possible to design data and pilot signals that are mutually\nunbiased. However, the naive approach to equalization has complexity ${\\mathcal\nO}(M^3N^3)$ where $M$ and $N$ are respectively the number of delay and Doppler\nbins in an OTFS frame. We simplify equalization by transforming Zak-OTFS\ninformation symbols to CP-OFDM (cyclic prefix orthogonal frequency division\nmultiplexing) modulation.\n  Why not simply communicate with CP-OFDM? Inter-carrier interference (ICI) in\nCP-OFDM makes it is very challenging to acquire the complete frequency domain\n(FD) channel response between subcarriers in the presence of mobility and delay\nspread. We avoid this difficulty by estimating the effective channel in the DD\ndomain from which we are able to reconstruct the complete FD channel response.\nWe take advantage of CP-OFDM to design an ${\\mathcal O}(M^2N^2)$ low-complexity\nmethod of jointly equalizing all subcarriers, where $MN$ is the number of\nsubcarriers. Our approach removes the need for traditional pilots in CP-OFDM\nand reduces the need to vary carrier spacing with mobility.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23045v1", "categories": ["eess.SP", "cs.IT", "math.IT"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.23045v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23836", "title": "Proving the Limited Scalability of Centralized Distributed Optimization via a New Lower Bound Construction", "authors": ["Alexander Tyurin"], "summary": "We consider centralized distributed optimization in the classical federated\nlearning setup, where $n$ workers jointly find an $\\varepsilon$-stationary\npoint of an $L$-smooth, $d$-dimensional nonconvex function $f$, having access\nonly to unbiased stochastic gradients with variance $\\sigma^2$. Each worker\nrequires at most $h$ seconds to compute a stochastic gradient, and the\ncommunication times from the server to the workers and from the workers to the\nserver are $\\tau_{s}$ and $\\tau_{w}$ seconds per coordinate, respectively. One\nof the main motivations for distributed optimization is to achieve scalability\nwith respect to $n$. For instance, it is well known that the distributed\nversion of SGD has a variance-dependent runtime term $\\frac{h \\sigma^2 L\n\\Delta}{n \\varepsilon^2},$ which improves with the number of workers $n,$ where\n$\\Delta = f(x^0) - f^*,$ and $x^0 \\in R^d$ is the starting point. Similarly,\nusing unbiased sparsification compressors, it is possible to reduce both the\nvariance-dependent runtime term and the communication runtime term. However,\nonce we account for the communication from the server to the workers\n$\\tau_{s}$, we prove that it becomes infeasible to design a method using\nunbiased random sparsification compressors that scales both the server-side\ncommunication runtime term $\\tau_{s} d \\frac{L \\Delta}{\\varepsilon}$ and the\nvariance-dependent runtime term $\\frac{h \\sigma^2 L \\Delta}{\\varepsilon^2},$\nbetter than poly-logarithmically in $n$, even in the homogeneous (i.i.d.) case,\nwhere all workers access the same distribution. To establish this result, we\nconstruct a new \"worst-case\" function and develop a new lower bound framework\nthat reduces the analysis to the concentration of a random sum, for which we\nprove a concentration bound. These results reveal fundamental limitations in\nscaling distributed optimization, even under the homogeneous assumption.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23836v1", "categories": ["math.OC", "cs.DC", "cs.LG"], "cate": "math.OC", "url": "http://arxiv.org/abs/2506.23836v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22444", "title": "Active Learning for Forecasting Severity among Patients with Post Acute Sequelae of SARS-CoV-2", "authors": ["Jing Wang", "Amar Sra", "Jeremy C. Weiss"], "summary": "The long-term effects of Postacute Sequelae of SARS-CoV-2, known as PASC,\npose a significant challenge to healthcare systems worldwide. Accurate\nidentification of progression events, such as hospitalization and reinfection,\nis essential for effective patient management and resource allocation. However,\ntraditional models trained on structured data struggle to capture the nuanced\nprogression of PASC. In this study, we introduce the first publicly available\ncohort of 18 PASC patients, with text time series features based on Large\nLanguage Model Llama-3.1-70B-Instruct and clinical risk annotated by clinical\nexpert. We propose an Active Attention Network to predict the clinical risk and\nidentify progression events related to the risk. By integrating human expertise\nwith active learning, we aim to enhance clinical risk prediction accuracy and\nenable progression events identification with fewer number of annotation. The\nultimate goal is to improves patient care and decision-making for SARS-CoV-2\npatient.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22444v1", "categories": ["cs.LG", "cs.CY"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22444v1", "date": "2025-06-11", "updated": "2025-06-11"}
{"id": "2506.22556", "title": "Recomposed realities: animating still images via patch clustering and randomness", "authors": ["Markus Juvonen", "Samuli Siltanen"], "summary": "We present a patch-based image reconstruction and animation method that uses\nexisting image data to bring still images to life through motion. Image patches\nfrom curated datasets are grouped using k-means clustering and a new target\nimage is reconstructed by matching and randomly sampling from these clusters.\nThis approach emphasizes reinterpretation over replication, allowing the source\nand target domains to differ conceptually while sharing local structures.", "comment": "22 pages, 19 figures", "pdf_url": "http://arxiv.org/pdf/2506.22556v1", "categories": ["cs.CV", "eess.IV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22556v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.23421", "title": "Predictor-Based Compensators for Networked Control Systems with Stochastic Delays and Sampling Intervals", "authors": ["Matheus Wagner", "Marcelo M. Morato", "Ant√¥nio Augusto Fr√∂hlich", "Julio E. Normey-Rico"], "summary": "The stochastic nature of time delays and sampling intervals in Networked\nControl Systems poses significant challenges for controller synthesis and\nanalysis, often leading to conservative designs and degraded performance. This\nwork presents a modeling approach for Linear Multiple-Input Multiple-Output\nNetworked Control Systems and introduces a compensation scheme based on the\nFiltered Smith Predictor to mitigate the adverse effects of stochastic time\ndelays on closed-loop performance. The proposed scheme is evaluated through\nnumerical simulations of a well-established Cooperative Adaptive Cruise Control\nsystem. Results demonstrate that the compensator achieves near-ideal average\nclosed-loop performance and significantly reduces response variability compared\nto a traditional Filtered Smith Predictor. Notably, it yields a 45% reduction\nin worst-case tracking error signal energy relative to an ideal baseline system\nwith no time delays and constant sampling intervals.", "comment": "This work has been submitted to the IEEE for possible publication", "pdf_url": "http://arxiv.org/pdf/2506.23421v1", "categories": ["eess.SY", "cs.SY"], "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.23421v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.22476", "title": "An Interpretable Transformer-Based Foundation Model for Cross-Procedural Skill Assessment Using Raw fNIRS Signals", "authors": ["A. Subedi", "S. De", "L. Cavuoto", "S. Schwaitzberg", "M. Hackett", "J. Norfleet"], "summary": "Objective skill assessment in high-stakes procedural environments requires\nmodels that not only decode underlying cognitive and motor processes but also\ngeneralize across tasks, individuals, and experimental contexts. While prior\nwork has demonstrated the potential of functional near-infrared spectroscopy\n(fNIRS) for evaluating cognitive-motor performance, existing approaches are\noften task-specific, rely on extensive preprocessing, and lack robustness to\nnew procedures or conditions. Here, we introduce an interpretable\ntransformer-based foundation model trained on minimally processed fNIRS signals\nfor cross-procedural skill assessment. Pretrained using self-supervised\nlearning on data from laparoscopic surgical tasks and endotracheal intubation\n(ETI), the model achieves greater than 88% classification accuracy on all\ntasks, with Matthews Correlation Coefficient exceeding 0.91 on ETI. It\ngeneralizes to a novel emergency airway procedure--cricothyrotomy--using fewer\nthan 30 labeled samples and a lightweight (less than 2k parameter) adapter\nmodule, attaining an AUC greater than 87%. Interpretability is achieved via a\nnovel channel attention mechanism--developed specifically for fNIRS--that\nidentifies functionally coherent prefrontal sub-networks validated through\nablation studies. Temporal attention patterns align with task-critical phases\nand capture stress-induced changes in neural variability, offering insight into\ndynamic cognitive states.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22476v1", "categories": ["eess.SP", "cs.ET", "cs.HC", "cs.LG", "q-bio.NC", "I.2.6; J.3; H.1.2"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.22476v1", "date": "2025-06-21", "updated": "2025-06-21"}
{"id": "2506.23311", "title": "Physics informed guided diffusion for accelerated multi-parametric MRI reconstruction", "authors": ["Perla Mayo", "Carolin M. Pirkl", "Alin Achim", "Bjoern Menze", "Mohammad Golbabaee"], "summary": "We introduce MRF-DiPh, a novel physics informed denoising diffusion approach\nfor multiparametric tissue mapping from highly accelerated, transient-state\nquantitative MRI acquisitions like Magnetic Resonance Fingerprinting (MRF). Our\nmethod is derived from a proximal splitting formulation, incorporating a\npretrained denoising diffusion model as an effective image prior to regularize\nthe MRF inverse problem. Further, during reconstruction it simultaneously\nenforces two key physical constraints: (1) k-space measurement consistency and\n(2) adherence to the Bloch response model. Numerical experiments on in-vivo\nbrain scans data show that MRF-DiPh outperforms deep learning and compressed\nsensing MRF baselines, providing more accurate parameter maps while better\npreserving measurement fidelity and physical model consistency-critical for\nsolving reliably inverse problems in medical imaging.", "comment": "11 pages, 1 figure, 1 algorithm, 3 tables. Accepted to MICCAI 2025.\n  This is a version prior peer-review", "pdf_url": "http://arxiv.org/pdf/2506.23311v1", "categories": ["eess.IV", "cs.LG", "physics.med-ph"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.23311v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23854", "title": "HiNeuS: High-fidelity Neural Surface Mitigating Low-texture and Reflective Ambiguity", "authors": ["Yida Wang", "Xueyang Zhang", "Kun Zhan", "Peng Jia", "Xianpeng Lang"], "summary": "Neural surface reconstruction faces persistent challenges in reconciling\ngeometric fidelity with photometric consistency under complex scene conditions.\nWe present HiNeuS, a unified framework that holistically addresses three core\nlimitations in existing approaches: multi-view radiance inconsistency, missing\nkeypoints in textureless regions, and structural degradation from over-enforced\nEikonal constraints during joint optimization. To resolve these issues through\na unified pipeline, we introduce: 1) Differential visibility verification\nthrough SDF-guided ray tracing, resolving reflection ambiguities via continuous\nocclusion modeling; 2) Planar-conformal regularization via ray-aligned geometry\npatches that enforce local surface coherence while preserving sharp edges\nthrough adaptive appearance weighting; and 3) Physically-grounded Eikonal\nrelaxation that dynamically modulates geometric constraints based on local\nradiance gradients, enabling detail preservation without sacrificing global\nregularity. Unlike prior methods that handle these aspects through sequential\noptimizations or isolated modules, our approach achieves cohesive integration\nwhere appearance-geometry constraints evolve synergistically throughout\ntraining. Comprehensive evaluations across synthetic and real-world datasets\ndemonstrate state-of-the-art performance, including a 21.4% reduction in\nChamfer distance over reflection-aware baselines and 2.32 dB PSNR improvement\nagainst neural rendering counterparts. Qualitative analyses reveal superior\ncapability in recovering specular instruments, urban layouts with\ncentimeter-scale infrastructure, and low-textured surfaces without local patch\ncollapse. The method's generalizability is further validated through successful\napplication to inverse rendering tasks, including material decomposition and\nview-consistent relighting.", "comment": "Published in International Conference on Computer Vision (ICCV) 2025", "pdf_url": "http://arxiv.org/pdf/2506.23854v1", "categories": ["cs.CV", "cs.GR"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23854v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22524", "title": "Inventory Control Using a L√©vy Process for Evaluating Total Costs under Intermittent Demand", "authors": ["Ryoya Koide", "Yurika Ono", "Aya Ishigaki"], "summary": "Products with intermittent demand are characterized by a high risk of sales\nlosses and obsolescence due to the sporadic occurrence of demand events.\nGenerally, both point forecasting and probabilistic forecasting approaches are\napplied to intermittent demand. In particular, probabilistic forecasting, which\nmodels demand as a stochastic process, is capable of capturing uncertainty. An\nexample of such modeling is the use of L\\'evy processes, which possess\nindependent increments and accommodate discontinuous changes (jumps). However,\nto the best of our knowledge, in inventory control using L\\'evy processes, no\nstudies have investigated how the order quantity and reorder point affect the\ntotal cost. One major difficulty has been the mathematical formulation of\ninventory replenishment triggered at reorder points. To address this challenge,\nthe present study formulates a reorder-point policy by modeling cumulative\ndemand as a drifted Poisson process and introducing a stopping time to\nrepresent the timing at which the reorder point is reached. Furthermore, the\nvalidity of the proposed method is verified by comparing the total cost with\nthat obtained from a case where an ARIMA model is combined with a reorder-point\npolicy. As a main result, while the total cost under ARIMA-based forecasting\nincreases linearly over time, the L\\'evy process-based formulation provides an\nanalytical expression for the total cost, revealing that random demand\nfluctuations cause the expected total cost to grow at a rate faster than\nlinear.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22524v1", "categories": ["math.OC", "cs.DS", "math.PR", "60G51, 90B05, 93E20", "I.2.6; G.3"], "cate": "math.OC", "url": "http://arxiv.org/abs/2506.22524v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22655", "title": "Learning Stochastic Multiscale Models", "authors": ["Andrew F. Ilersich", "Prasanth B. Nair"], "summary": "The physical sciences are replete with dynamical systems that require the\nresolution of a wide range of length and time scales. This presents significant\ncomputational challenges since direct numerical simulation requires\ndiscretization at the finest relevant scales, leading to a high-dimensional\nstate space. In this work, we propose an approach to learn stochastic\nmultiscale models in the form of stochastic differential equations directly\nfrom observational data. Our method resolves the state on a coarse mesh while\nintroducing an auxiliary state to capture the effects of unresolved scales. We\nlearn the parameters of the multiscale model using a modern forward-solver-free\namortized variational inference method. Our approach draws inspiration from\nphysics-based multiscale modeling approaches, such as large-eddy simulation in\nfluid dynamics, while learning directly from data. We present numerical studies\nto demonstrate that our learned multiscale models achieve superior predictive\naccuracy compared to direct numerical simulation and closure-type models at\nequivalent resolution.", "comment": "Body is 9 pages, 13 including acknowledgements and references, 35\n  including appendix. 21 figures and 6 tables. Submitted to NeurIPS 2025", "pdf_url": "http://arxiv.org/pdf/2506.22655v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22655v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.23168", "title": "Rises for Measuring Local Distributivity in Lattices", "authors": ["Mohammad Abdulla", "Tobias Hille", "Dominik D√ºrrschnabel", "Gerd Stumme"], "summary": "Distributivity is a well-established and extensively studied notion in\nlattice theory. In the context of data analysis, particularly within Formal\nConcept Analysis (FCA), lattices are often observed to exhibit a high degree of\ndistributivity. However, no standardized measure exists to quantify this\nproperty. In this paper, we introduce the notion of rises in (concept) lattices\nas a means to assess distributivity. Rises capture how the number of attributes\nor objects in covering concepts change within the concept lattice. We show that\na lattice is distributive if and only if no non-unit rises occur. Furthermore,\nwe relate rises to the classical notion of meet- and join distributivity. We\nobserve that concept lattices from real-world data are to a high degree\njoin-distributive, but much less meet-distributive. We additionally study how\njoin-distributivity manifests on the level of ordered sets.", "comment": "16 pages, 2 tables, 5 figures, International Joint Conference on\n  Conceptual Knowledge Structures", "pdf_url": "http://arxiv.org/pdf/2506.23168v1", "categories": ["cs.AI", "cs.DM", "math.CO", "math.RA", "06B99", "G.2.1"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.23168v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23826", "title": "Towards the \"Digital Me\": A vision of authentic Conversational Agents powered by personal Human Digital Twins", "authors": ["Llu√≠s C. Coll", "Martin W. Lauer-Schmaltz", "Philip Cash", "John P. Hansen", "Anja Maier"], "summary": "Human Digital Twins (HDTs) have traditionally been conceptualized as\ndata-driven models designed to support decision-making across various domains.\nHowever, recent advancements in conversational AI open new possibilities for\nHDTs to function as authentic, interactive digital counterparts of individuals.\nThis paper introduces a novel HDT system architecture that integrates large\nlanguage models with dynamically updated personal data, enabling it to mirror\nan individual's conversational style, memories, and behaviors. To achieve this,\nour approach implements context-aware memory retrieval, neural\nplasticity-inspired consolidation, and adaptive learning mechanisms, creating a\nmore natural and evolving digital persona. The resulting system does not only\nreplicate an individual's unique conversational style depending on who they are\nspeaking with, but also enriches responses with dynamically captured personal\nexperiences, opinions, and memories. While this marks a significant step toward\ndeveloping authentic virtual counterparts, it also raises critical ethical\nconcerns regarding privacy, accountability, and the long-term implications of\npersistent digital identities. This study contributes to the field of HDTs by\ndescribing our novel system architecture, demonstrating its capabilities, and\ndiscussing future directions and emerging challenges to ensure the responsible\nand ethical development of HDTs.", "comment": "24 pages, 9 figures", "pdf_url": "http://arxiv.org/pdf/2506.23826v1", "categories": ["cs.ET", "cs.AI", "cs.CY", "cs.HC", "cs.IR"], "cate": "cs.ET", "url": "http://arxiv.org/abs/2506.23826v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23130", "title": "The Florence Price Art Song Dataset and Piano Accompaniment Generator", "authors": ["Tao-Tao He", "Martin E. Malandro", "Douglas Shadle"], "summary": "Florence B. Price was a composer in the early 20th century whose music\nreflects her upbringing in the American South, her African heritage, and her\nWestern classical training. She is noted as the first African-American woman to\nhave a symphony performed by a major orchestra. Her music has recently received\nrenewed attention from both the public and the research community, decades\nafter her death. In addition to other genres, Price was a prolific composer for\nsolo voice and piano. Music historians have documented the existence of 134 art\nsongs and piano/voice arrangements for spirituals and folk songs written by\nPrice. We release a digital catalog of 112 of these works in MuseScore,\nMusicXML, MIDI, and PDF format. We also use this dataset to fine-tune a\nsymbolic music generation model to generate accompaniments to melodies, and we\nconduct a blind listening experiment that shows that accompaniments generated\nby our model are perceived as being reflective of Florence Price's style more\nfrequently than accompaniments generated by a baseline model. We release our\nmodel as the Florence Price Piano Accompaniment Generator alongside our\ndataset.", "comment": "8 pages, 4 figures. To appear in the proceedings of ISMIR 2025", "pdf_url": "http://arxiv.org/pdf/2506.23130v1", "categories": ["cs.SD", "eess.AS"], "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.23130v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.22760", "title": "Jan-nano Technical Report", "authors": ["Alan Dao", "Dinh Bach Vu"], "summary": "Most language models face a fundamental tradeoff where powerful capabilities\nrequire substantial computational resources. We shatter this constraint with\nJan-nano, a 4B parameter language model that redefines efficiency through\nradical specialization: instead of trying to know everything, it masters the\nart of finding anything instantly. Fine-tuned from Qwen3-4B using our novel\nmulti-stage RLVR system that completely eliminates reliance on next token\nprediction training (SFT), Jan-nano achieves 83.2% on SimpleQA benchmark with\nMCP integration while running on consumer hardware. With 128K context length,\nJan-nano proves that intelligence isn't about scale, it's about strategy.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22760v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.22760v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23483", "title": "On the convergence of iterative regularization method assisted by the graph Laplacian with early stopping", "authors": ["Harshit Bajpai", "Gaurav Mittal", "Ankik Kumar Giri"], "summary": "We present a data-assisted iterative regularization method for solving\nill-posed inverse problems in Hilbert space settings. The proposed approach,\ntermed \\texttt{IRMGL+\\(\\Psi\\)}, integrates classical iterative techniques with\na data-driven regularization term realized through an iteratively updated graph\nLaplacian. Our method commences by computing a preliminary solution using any\nsuitable reconstruction method, which then serves as the basis for constructing\nthe initial graph Laplacian. The solution is subsequently refined through an\niterative process, where the graph Laplacian is simultaneously recalibrated at\neach step to effectively capture the evolving structure of the solution. A key\ninnovation of this work lies in the formulation of this iterative scheme and\nthe rigorous justification of the classical discrepancy principle as a reliable\nearly stopping criterion specifically tailored to the proposed method. Under\nstandard assumptions, we establish stability and convergence results for the\nscheme when the discrepancy principle is applied. Furthermore, we demonstrate\nthe robustness and effectiveness of our method through numerical experiments\nutilizing four distinct initial reconstructors $\\Psi$: the adjoint operator\n(Adj), filtered back projection (FBP), total variation (TV) denoising, and\nstandard Tikhonov regularization (Tik). It is observed that \\texttt{IRMGL+Adj}\ndemonstrates a distinct advantage over the other initializers, producing a\nrobust and stable approximate solution directly from a basic initial\nreconstruction.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23483v1", "categories": ["math.NA", "cs.NA", "math.FA", "math.OC"], "cate": "math.NA", "url": "http://arxiv.org/abs/2506.23483v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22858", "title": "Mind the Gap: Entity-Preserved Context-Aware ASR Structured Transcriptions", "authors": ["Duygu Altinok"], "summary": "Automatic Speech Recognition (ASR) systems, such as Whisper, achieve high\ntranscription accuracy but struggle with named entities and numerical data,\nespecially when proper formatting is required. These issues increase word error\nrate (WER) and impair semantic understanding in critical domains like legal,\nfinancial, and medical applications. We propose a novel training approach that\nextends the semantic context of ASR models by adding overlapping context\nwindows during training. By sliding 5-second overlaps on both sides of\n30-second chunks, we create a 40-second \"effective semantic window,\" improving\nentity recognition and formatting while focusing predictions on the central 30\nseconds. To address entities spanning chunk boundaries, we reassign such\nentities entirely to the right-hand chunk, ensuring proper formatting.\nAdditionally, enriched training data with embedded entity labels enables the\nmodel to learn both recognition and type-specific formatting. Evaluated on the\nSpoken Wikipedia dataset, our method improves performance across semantic\ntasks, including named entity recognition (NER) and entity formatting. These\nresults highlight the effectiveness of context-aware training in addressing ASR\nlimitations for long-form transcription and complex entity recognition tasks.", "comment": "This is the accepted version of an article accepted to the TSD 2025\n  conference, published in Springer Lecture Notes in Artificial Intelligence\n  (LNAI). The final authenticated version is available online at SpringerLink", "pdf_url": "http://arxiv.org/pdf/2506.22858v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.22858v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23296", "title": "Securing AI Systems: A Guide to Known Attacks and Impacts", "authors": ["Naoto Kiribuchi", "Kengo Zenitani", "Takayuki Semitsu"], "summary": "Embedded into information systems, artificial intelligence (AI) faces\nsecurity threats that exploit AI-specific vulnerabilities. This paper provides\nan accessible overview of adversarial attacks unique to predictive and\ngenerative AI systems. We identify eleven major attack types and explicitly\nlink attack techniques to their impacts -- including information leakage,\nsystem compromise, and resource exhaustion -- mapped to the confidentiality,\nintegrity, and availability (CIA) security triad. We aim to equip researchers,\ndevelopers, security practitioners, and policymakers, even those without\nspecialized AI security expertise, with foundational knowledge to recognize\nAI-specific risks and implement effective defenses, thereby enhancing the\noverall security posture of AI systems.", "comment": "34 pages, 16 figures", "pdf_url": "http://arxiv.org/pdf/2506.23296v1", "categories": ["cs.CR", "cs.AI"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.23296v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23129", "title": "Flatness-based Finite-Horizon Multi-UAV Formation Trajectory Planning and Directionally Aware Collision Avoidance Tracking", "authors": ["Hossein B. Jond", "Logan Beaver", "Martin Jirou≈°ek", "Naiemeh Ahmadlou", "Veli Bakƒ±rcƒ±oƒülu", "Martin Saska"], "summary": "Collision-free optimal formation control of unmanned aerial vehicle (UAV)\nteams is challenging. The state-of-the-art optimal control approaches often\nrely on numerical methods sensitive to initial guesses. This paper presents an\ninnovative collision-free finite-time formation control scheme for multiple\nUAVs leveraging the differential flatness of the UAV dynamics, eliminating the\nneed for numerical methods. We formulate a finite-time optimal control problem\nto plan a formation trajectory for feasible initial states. This formation\ntrajectory planning optimal control problem involves a collective performance\nindex to meet the formation requirements of achieving relative positions and\nvelocity consensus. It is solved by applying Pontryagin's principle.\nSubsequently, a collision-constrained regulating problem is addressed to ensure\ncollision-free tracking of the planned formation trajectory. The tracking\nproblem incorporates a directionally aware collision avoidance strategy that\nprioritizes avoiding UAVs in the forward path and relative approach. It assigns\nlower priority to those on the sides with an oblique relative approach and\ndisregards UAVs behind and not in the relative approach. The simulation results\nfor a four-UAV team (re)formation problem confirm the efficacy of the proposed\ncontrol scheme.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23129v1", "categories": ["cs.RO"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.23129v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23488", "title": "Generative AI-enhanced Low-Altitude UAV-Mounted Stacked Intelligent Metasurfaces", "authors": ["Geng Sun", "Mingzhe Fan", "Lei Zhang", "Hongyang Pan", "Jiahui Li", "Chuang Zhang", "Linyao Li", "Changyuan Zhao", "Chau Yuen"], "summary": "Wireless communication systems face significant challenges in meeting the\nincreasing demands for higher data rates and more reliable connectivity in\ncomplex environments. Stacked intelligent metasurfaces (SIMs) have emerged as a\npromising technology for realizing wave-domain signal processing, with mobile\nSIMs offering superior communication performance compared to their fixed\ncounterparts. In this paper, we investigate a novel unmanned aerial vehicle\n(UAV)-mounted SIMs (UAV-SIMs) assisted communication system within the\nlow-altitude economy (LAE) networks paradigm, where UAVs function as both base\nstations that cache SIM-processed data and mobile platforms that flexibly\ndeploy SIMs to enhance uplink communications from ground users. To maximize\nnetwork capacity, we formulate a UAV-SIM-based joint optimization problem\n(USBJOP) that comprehensively addresses three critical aspects: the association\nbetween UAV-SIMs and users, the three-dimensional positioning of UAV-SIMs, and\nthe phase shifts across multiple SIM layers. Due to the inherent non-convexity\nand NP-hardness of USBJOP, we decompose it into three sub-optimization\nproblems, \\textit{i.e.}, association between UAV-SIMs and users optimization\nproblem (AUUOP), UAV location optimization problem (ULOP), and UAV-SIM phase\nshifts optimization problem (USPSOP), and solve them using an alternating\noptimization strategy. Specifically, we transform AUUOP and ULOP into convex\nforms solvable by the CVX tool, while addressing USPSOP through a generative\nartificial intelligence (GAI)-based hybrid optimization algorithm. Simulations\ndemonstrate that our proposed approach significantly outperforms benchmark\nschemes, achieving approximately 1.5 times higher network capacity compared to\nsuboptimal alternatives. Additionally, our proposed GAI method reduces the\nalgorithm runtime by 10\\% while maintaining solution quality.", "comment": "This paper has been already submitted to TCCN", "pdf_url": "http://arxiv.org/pdf/2506.23488v1", "categories": ["cs.NI"], "cate": "cs.NI", "url": "http://arxiv.org/abs/2506.23488v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23749", "title": "A Survey of LLM-based Automated Program Repair: Taxonomies, Design Paradigms, and Applications", "authors": ["Boyang Yang", "Zijian Cai", "Fengling Liu", "Bach Le", "Lingming Zhang", "Tegawend√© F. Bissyand√©", "Yang Liu", "Haoye Tian"], "summary": "Large language models (LLMs) are reshaping automated program repair (APR). We\ncategorize the recent 63 LLM-based APR systems published from January 2022 to\nJune 2025 into four paradigms, and show how retrieval- or analysis-augmented\ncontexts strengthen any of them. This taxonomy clarifies key trade-offs:\nfine-tuning delivers strong task alignment at high training cost; prompting\nenables rapid deployment but is limited by prompt design and context windows;\nprocedural pipelines offer reproducible control with moderate overhead; agentic\nframeworks tackle multi-hunk or cross-file bugs at the price of increased\nlatency and complexity. Persistent challenges include verifying semantic\ncorrectness beyond test suites, repairing repository-scale defects, and\nlowering the costs of LLMs. We outline research directions that combine\nlightweight human feedback, repository-aware retrieval, code analysis, and\ncost-aware planning to advance reliable and efficient LLM-based APR.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23749v1", "categories": ["cs.SE"], "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.23749v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23180", "title": "ImprovMate: Multimodal AI Assistant for Improv Actor Training", "authors": ["Riccardo Drago", "Yotam Sechayk", "Mustafa Doga Dogan", "Andrea Sanna", "Takeo Igarashi"], "summary": "Improvisation training for actors presents unique challenges, particularly in\nmaintaining narrative coherence and managing cognitive load during\nperformances. Previous research on AI in improvisation performance often\npredates advances in large language models (LLMs) and relies on human\nintervention. We introduce ImprovMate, which leverages LLMs as GPTs to automate\nthe generation of narrative stimuli and cues, allowing actors to focus on\ncreativity without keeping track of plot or character continuity. Based on\ninsights from professional improvisers, ImprovMate incorporates exercises that\nmimic live training, such as abrupt story resolution and reactive thinking\nexercises, while maintaining coherence via reference tables. By balancing\nrandomness and structured guidance, ImprovMate provides a groundbreaking tool\nfor improv training. Our pilot study revealed that actors might embrace AI\ntechniques if the latter mirrors traditional practices, and appreciate the\nfresh twist introduced by our approach with the AI-generated cues.", "comment": "ACM DIS '25", "pdf_url": "http://arxiv.org/pdf/2506.23180v1", "categories": ["cs.HC", "H.5.0; H.5.2"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.23180v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23650", "title": "Optimal Quantum Algorithm for Estimating Fidelity to a Pure State", "authors": ["Wang Fang", "Qisheng Wang"], "summary": "We present an optimal quantum algorithm for fidelity estimation between two\nquantum states when one of them is pure. In particular, the (square root)\nfidelity of a mixed state to a pure state can be estimated to within additive\nerror $\\varepsilon$ by using $\\Theta(1/\\varepsilon)$ queries to their\nstate-preparation circuits, achieving a quadratic speedup over the folklore\n$O(1/\\varepsilon^2)$. Our approach is technically simple, and can moreover\nestimate the quantity $\\sqrt{\\operatorname{tr}(\\rho\\sigma^2)}$ that is not\ncommon in the literature. To the best of our knowledge, this is the first\nquery-optimal approach to fidelity estimation involving mixed states.", "comment": "14 pages. To appear in ESA 2025", "pdf_url": "http://arxiv.org/pdf/2506.23650v1", "categories": ["quant-ph", "cs.IT", "math.IT"], "cate": "quant-ph", "url": "http://arxiv.org/abs/2506.23650v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23906", "title": "Segmented Operations using Matrix Multiplications", "authors": ["Aleksandros Sobczyk", "Giuseppe Sorrentino", "Anastasios Zouzias"], "summary": "Specialized computational units that perform small matrix multiplications as\nprimitive operations are typically present in modern accelerators. However,\nthese units are often underutilized for many fundamental operations besides\ndense matrix multiplications. The analysis of algorithms for such architectures\nis currently stagnated due to the lack of a rigorous theoretical model of\ncomputation that captures their characteristics. In this work, we propose\nMMV-RAM, a computational model tailored to matrix multiplication accelerators.\nMMV-RAM judiciously extends the Vector-RAM model with an additional processing\nunit that multiplies two matrices of sizes $n\\times s$ and $s\\times s$ in a\nsingle parallel step, where $s$ is a model parameter. We provide a detailed\ntheoretical analysis of the model, and carefully balance the computational\npower between the matrix and vector units, guided by the circuit complexity\nlower bound that parity is not in AC[0].\n  In MMV-RAM, we study algorithms for segmented scan and sum, two fundamental\nparallel primitives. We propose a segmented scan algorithm that uses matrix\nmultiplications to perform speculative block-scan computations, which runs in\n$O(\\log_s(n))$ steps. In contrast, we show that any algorithm that uses only\nthe vector unit of MMV-RAM requires\n$\\Omega\\left(\\frac{\\log_2(n)}{\\log_2\\log_2(n)}\\right)$ steps. We further apply\nthese techniques to obtain similar theoretical speedups for element-wise vector\nmultiplication and matrix multiplication. Beyond the worst-case complexity\nanalysis, we propose algorithms for segmented operations that could lead to\nhighly efficient and pragmatic implementations. For example, we observe that\nsegmented sum is a combination of three elementary parallel primitives: scan,\ncompress, and vector differentiation. As a case study, we implement...", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23906v1", "categories": ["cs.DS", "cs.CC", "cs.DC"], "cate": "cs.DS", "url": "http://arxiv.org/abs/2506.23906v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22462", "title": "Privacy-aware IoT Fall Detection Services For Aging in Place", "authors": ["Abdallah Lakhdari", "Jiajie Li", "Amani Abusafia", "Athman Bouguettaya"], "summary": "Fall detection is critical to support the growing elderly population,\nprojected to reach 2.1 billion by 2050. However, existing methods often face\ndata scarcity challenges or compromise privacy. We propose a novel IoT-based\nFall Detection as a Service (FDaaS) framework to assist the elderly in living\nindependently and safely by accurately detecting falls. We design a\nservice-oriented architecture that leverages Ultra-wideband (UWB) radar sensors\nas an IoT health-sensing service, ensuring privacy and minimal intrusion. We\naddress the challenges of data scarcity by utilizing a Fall Detection\nGenerative Pre-trained Transformer (FD-GPT) that uses augmentation techniques.\nWe developed a protocol to collect a comprehensive dataset of the elderly daily\nactivities and fall events. This resulted in a real dataset that carefully\nmimics the elderly's routine. We rigorously evaluate and compare various models\nusing this dataset. Experimental results show our approach achieves 90.72%\naccuracy and 89.33% precision in distinguishing between fall events and regular\nactivities of daily living.", "comment": "11 pages, 12 figures, This paper is accepted in the 2025 IEEE\n  International Conference on Web Services (ICWS 2025)", "pdf_url": "http://arxiv.org/pdf/2506.22462v1", "categories": ["eess.SP", "cs.AI", "cs.CY", "cs.HC"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.22462v1", "date": "2025-06-18", "updated": "2025-06-18"}
{"id": "2506.22562", "title": "Improving Token-based Object Detection with Video", "authors": ["Abhineet Singh", "Nilanjan Ray"], "summary": "This paper improves upon the Pix2Seq object detector by extending it for\nvideos. In the process, it introduces a new way to perform end-to-end video\nobject detection that improves upon existing video detectors in two key ways.\nFirst, by representing objects as variable-length sequences of discrete tokens,\nwe can succinctly represent widely varying numbers of video objects, with\ndiverse shapes and locations, without having to inject any localization cues in\nthe training process. This eliminates the need to sample the space of all\npossible boxes that constrains conventional detectors and thus solves the dual\nproblems of loss sparsity during training and heuristics-based postprocessing\nduring inference. Second, it conceptualizes and outputs the video objects as\nfully integrated and indivisible 3D boxes or tracklets instead of generating\nimage-specific 2D boxes and linking these boxes together to construct the video\nobject, as done in most conventional detectors. This allows it to scale\neffortlessly with available computational resources by simply increasing the\nlength of the video subsequence that the network takes as input, even\ngeneralizing to multi-object tracking if the subsequence can span the entire\nvideo. We compare our video detector with the baseline Pix2Seq static detector\non several datasets and demonstrate consistent improvement, although with\nstrong signs of being bottlenecked by our limited computational resources. We\nalso compare it with several video detectors on UA-DETRAC to show that it is\ncompetitive with the current state of the art even with the computational\nbottleneck. We make our code and models publicly available.", "comment": "Under review for publication in IEEE Access", "pdf_url": "http://arxiv.org/pdf/2506.22562v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22562v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.23425", "title": "Power Flow Analysis of a 5-Bus Power System Based on Newton-Raphson Method", "authors": ["Sampson E. Nwachukwu"], "summary": "Load flow analysis is a fundamental technique used by electrical engineers to\nsimulate and evaluate power system behavior under steady-state conditions. It\nenables efficient operation and control by determining how active and reactive\npower flows throughout the system. Selecting an appropriate solution method is\ncritical to ensuring reliable and economical operation of power generation,\ntransmission, and distribution networks. While the conventional loop method may\nbe used in small-scale systems, it is limited by its reliance on\nimpedance-based load data and its inability to scale to complex networks. In\ncontrast, iterative techniques such as the Gauss-Seidel (GS) and Newton-Raphson\n(NR) methods are better suited for analyzing large systems. Of these, the NR\nmethod offers significant advantages due to its quadratic convergence and\nimproved numerical stability. This study presents a power flow analysis of a\n5-bus system using the Newton-Raphson approach. The system was modeled and\nsimulated in PowerWorld Simulator (PWS), and a custom MATLAB implementation was\ndeveloped to verify the results under a base case scenario. The comparative\nanalysis demonstrates that the NR method provides accurate and robust solutions\nfor power flow problems, making it well-suited for evaluating system\nperformance under various operating conditions.", "comment": "8 pages, 27 figures", "pdf_url": "http://arxiv.org/pdf/2506.23425v1", "categories": ["eess.SY", "cs.SY"], "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.23425v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.22488", "title": "Zero-Shot EEG-to-Gait Decoding via Phase-Aware Representation Learning", "authors": ["Xi Fu", "Weibang Jiang", "Rui Liu", "Gernot R. M√ºller-Putz", "Cuntai Guan"], "summary": "Accurate decoding of lower-limb motion from EEG signals is essential for\nadvancing brain-computer interface (BCI) applications in movement intent\nrecognition and control. However, challenges persist in achieving causal,\nphase-consistent predictions and in modeling both inter- and intra-subject\nvariability. To address these issues, we propose NeuroDyGait, a\ndomain-generalizable EEG-to-motion decoding framework that leverages structured\ncontrastive representation learning and relational domain modeling. The\nproposed method employs relative contrastive learning to achieve semantic\nalignment between EEG and motion embeddings. Furthermore, a multi-cycle gait\nreconstruction objective is introduced to enforce temporal coherence and\nmaintain biomechanical consistency. To promote inter-session generalization,\nduring fine-tuning, a domain dynamic decoding mechanism adaptively assigns\nsession-specific prediction heads and learns to mix their outputs based on\ninter-session relationships. NeuroDyGait enables zero-shot motion prediction\nfor unseen individuals without requiring adaptation and achieves superior\nperformance in cross-subject gait decoding on benchmark datasets. Additionally,\nit demonstrates strong phase-detection capabilities even without explicit phase\nsupervision during training. These findings highlight the potential of\nrelational domain learning in enabling scalable, target-free deployment of\nBCIs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22488v1", "categories": ["eess.SP", "cs.LG"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.22488v1", "date": "2025-06-24", "updated": "2025-06-24"}
{"id": "2506.23334", "title": "Federated Breast Cancer Detection Enhanced by Synthetic Ultrasound Image Augmentation", "authors": ["Hongyi Pan", "Ziliang Hong", "Gorkem Durak", "Ziyue Xu", "Ulas Bagci"], "summary": "Federated learning (FL) has emerged as a promising paradigm for\ncollaboratively training deep learning models across institutions without\nexchanging sensitive medical data. However, its effectiveness is often hindered\nby limited data availability and non-independent, identically distributed data\nacross participating clients, which can degrade model performance and\ngeneralization. To address these challenges, we propose a generative AI based\ndata augmentation framework that integrates synthetic image sharing into the\nfederated training process for breast cancer diagnosis via ultrasound images.\nSpecifically, we train two simple class-specific Deep Convolutional Generative\nAdversarial Networks: one for benign and one for malignant lesions. We then\nsimulate a realistic FL setting using three publicly available breast\nultrasound image datasets: BUSI, BUS-BRA, and UDIAT. FedAvg and FedProx are\nadopted as baseline FL algorithms. Experimental results show that incorporating\na suitable number of synthetic images improved the average AUC from 0.9206 to\n0.9237 for FedAvg and from 0.9429 to 0.9538 for FedProx. We also note that\nexcessive use of synthetic data reduced performance, underscoring the\nimportance of maintaining a balanced ratio of real and synthetic samples. Our\nfindings highlight the potential of generative AI based data augmentation to\nenhance FL results in the breast ultrasound image classification task.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23334v1", "categories": ["eess.IV", "cs.AI", "cs.CV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.23334v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.22634", "title": "A Rigorous Error Bound for the TG Kernel in Prime Counting", "authors": ["Bugra Kilictas", "Faruk Alpay"], "summary": "We establish rigorous error bounds for prime counting using a truncated\nGaussian (TG) kernel in the explicit formula framework. Our main theorem proves\nthat the approximation error remains globally below 1/2 for all sufficiently\nlarge arguments, guaranteeing exact computation of {\\pi}(x) through simple\nrounding, without relying on unproven hypotheses.\n  The TG kernel construction employs Gaussian-like test functions with compact\nsupport, engineered with vanishing moments to eliminate main terms. For x with\n10^8 decimal digits, we demonstrate that only ~1200 nontrivial zeta zeros\nsuffice to achieve the error bound, enabling computation in seconds on modern\nhardware - a dramatic improvement over classical methods.\n  Key contributions include: (1) Explicit tail truncation bounds using Taylor\nremainder analysis, showing exponential decay; (2) Zero-sum truncation error\nbounds via unconditional density estimates; (3) Rigorous treatment of trivial\nzero contributions. All constants are made explicit, ensuring full\nverifiability.\n  The method bridges analytic number theory and practical computation, with\npotential applications to record-breaking prime counting computations. We\ndiscuss algorithmic implications including FFT-based arithmetic for ~330\nmillion bit numbers. The framework's flexibility suggests connections to deeper\nstructures in prime distribution, particularly regarding optimized kernel\ndesigns and the interplay between smoothing parameters {\\alpha} and truncation\nheights.\n  This work exemplifies how classical analytic techniques, when carefully\nimplemented with modern computational perspectives, yield practical algorithms\nfor problems previously considered purely theoretical. The rigorous error\nanalysis ensures reliability even at astronomical scales, opening new avenues\nfor computational number theory research.", "comment": "19 pages, 0 figure", "pdf_url": "http://arxiv.org/pdf/2506.22634v1", "categories": ["math.NT", "cs.DS", "cs.NA", "math.NA", "11N05, 11Y35, 11M26, 65B10", "F.2.1; I.1.2"], "cate": "math.NT", "url": "http://arxiv.org/abs/2506.22634v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22668", "title": "DistShap: Scalable GNN Explanations with Distributed Shapley Values", "authors": ["Selahattin Akkas", "Aditya Devarakonda", "Ariful Azad"], "summary": "With the growing adoption of graph neural networks (GNNs), explaining their\npredictions has become increasingly important. However, attributing predictions\nto specific edges or features remains computationally expensive. For example,\nclassifying a node with 100 neighbors using a 3-layer GNN may involve\nidentifying important edges from millions of candidates contributing to the\nprediction. To address this challenge, we propose DistShap, a parallel\nalgorithm that distributes Shapley value-based explanations across multiple\nGPUs. DistShap operates by sampling subgraphs in a distributed setting,\nexecuting GNN inference in parallel across GPUs, and solving a distributed\nleast squares problem to compute edge importance scores. DistShap outperforms\nmost existing GNN explanation methods in accuracy and is the first to scale to\nGNN models with millions of features by using up to 128 GPUs on the NERSC\nPerlmutter supercomputer.", "comment": "12 pages", "pdf_url": "http://arxiv.org/pdf/2506.22668v1", "categories": ["cs.LG", "cs.AI", "cs.DC", "stat.ML"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22668v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.23273", "title": "FinStat2SQL: A Text2SQL Pipeline for Financial Statement Analysis", "authors": ["Quang Hung Nguyen", "Phuong Anh Trinh", "Phan Quoc Hung Mai", "Tuan Phong Trinh"], "summary": "Despite the advancements of large language models, text2sql still faces many\nchallenges, particularly with complex and domain-specific queries. In finance,\ndatabase designs and financial reporting layouts vary widely between financial\nentities and countries, making text2sql even more challenging. We present\nFinStat2SQL, a lightweight text2sql pipeline enabling natural language queries\nover financial statements. Tailored to local standards like VAS, it combines\nlarge and small language models in a multi-agent setup for entity extraction,\nSQL generation, and self-correction. We build a domain-specific database and\nevaluate models on a synthetic QA dataset. A fine-tuned 7B model achieves\n61.33\\% accuracy with sub-4-second response times on consumer hardware,\noutperforming GPT-4o-mini. FinStat2SQL offers a scalable, cost-efficient\nsolution for financial analysis, making AI-powered querying accessible to\nVietnamese enterprises.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23273v1", "categories": ["cs.AI"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.23273v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23873", "title": "Emergent musical properties of a transformer under contrastive self-supervised learning", "authors": ["Yuexuan Kong", "Gabriel Meseguer-Brocal", "Vincent Lostanlen", "Mathieu Lagrange", "Romain Hennequin"], "summary": "In music information retrieval (MIR), contrastive self-supervised learning\nfor general-purpose representation models is effective for global tasks such as\nautomatic tagging. However, for local tasks such as chord estimation, it is\nwidely assumed that contrastively trained general-purpose self-supervised\nmodels are inadequate and that more sophisticated SSL is necessary; e.g.,\nmasked modeling. Our paper challenges this assumption by revealing the\npotential of contrastive SSL paired with a transformer in local MIR tasks. We\nconsider a lightweight vision transformer with one-dimensional patches in the\ntime--frequency domain (ViT-1D) and train it with simple contrastive SSL\nthrough normalized temperature-scaled cross-entropy loss (NT-Xent). Although\nNT-Xent operates only over the class token, we observe that, potentially thanks\nto weight sharing, informative musical properties emerge in ViT-1D's sequence\ntokens. On global tasks, the temporal average of class and sequence tokens\noffers a performance increase compared to the class token alone, showing useful\nproperties in the sequence tokens. On local tasks, sequence tokens perform\nunexpectedly well, despite not being specifically trained for. Furthermore,\nhigh-level musical features such as onsets emerge from layer-wise attention\nmaps and self-similarity matrices show different layers capture different\nmusical dimensions. Our paper does not focus on improving performance but\nadvances the musical interpretation of transformers and sheds light on some\noverlooked abilities of contrastive SSL paired with transformers for sequence\nmodeling in MIR.", "comment": "Accepted at ISMIR 2025", "pdf_url": "http://arxiv.org/pdf/2506.23873v1", "categories": ["cs.SD", "cs.IR", "cs.LG", "eess.AS"], "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.23873v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23325", "title": "XY-Tokenizer: Mitigating the Semantic-Acoustic Conflict in Low-Bitrate Speech Codecs", "authors": ["Yitian Gong", "Luozhijie Jin", "Ruifan Deng", "Dong Zhang", "Xin Zhang", "Qinyuan Cheng", "Zhaoye Fei", "Shimin Li", "Xipeng Qiu"], "summary": "Speech codecs serve as bridges between speech signals and large language\nmodels. An ideal codec for speech language models should not only preserve\nacoustic information but also capture rich semantic information. However,\nexisting speech codecs struggle to balance high-quality audio reconstruction\nwith ease of modeling by language models. In this study, we analyze the\nlimitations of previous codecs in balancing semantic richness and acoustic\nfidelity. We propose XY-Tokenizer, a novel codec that mitigates the conflict\nbetween semantic and acoustic capabilities through multi-stage, multi-task\nlearning. Experimental results demonstrate that XY-Tokenizer achieves\nperformance in both semantic and acoustic tasks comparable to that of\nstate-of-the-art codecs operating at similar bitrates, even though those\nexisting codecs typically excel in only one aspect. Specifically, XY-Tokenizer\nachieves strong text alignment, surpassing distillation-based semantic modeling\nmethods such as SpeechTokenizer and Mimi, while maintaining a speaker\nsimilarity score of 0.83 between reconstructed and original audio. The\nreconstruction performance of XY-Tokenizer is comparable to that of BigCodec,\nthe current state-of-the-art among acoustic-only codecs, which achieves a\nspeaker similarity score of 0.84 at a similar bitrate. Code and models are\navailable at https://github.com/gyt1145028706/XY-Tokenizer.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23325v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.23325v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.22777", "title": "Teaching Models to Verbalize Reward Hacking in Chain-of-Thought Reasoning", "authors": ["Miles Turpin", "Andy Arditi", "Marvin Li", "Joe Benton", "Julian Michael"], "summary": "Language models trained with RL can engage in reward hacking--exploiting\nunintended strategies for high reward--without revealing this behavior in their\nchain-of-thought reasoning, making detection difficult and posing risks for\nhigh-stakes applications. We propose verbalization fine-tuning (VFT), a pre-RL\nintervention that trains models to explicitly acknowledge when they are\ninfluenced by prompt cues--hints which point to incorrect answers (e.g., \"a\nStanford professor thinks the answer is A\"). To evaluate VFT, we subsequently\ntrain models with RL on environments where held-out prompt cues signal which\nincorrect answers will receive high reward, incentivizing models to reward hack\nby exploiting cues instead of reasoning correctly. We measure how often models\nexploit these cues without verbalizing it. After RL, only 6% of the VFT-trained\nmodel's responses consist of undetected reward hacks. In comparison, when we\nperform RL without VFT, the rate of undetected reward hacks goes up to 88%;\nwith a debiasing baseline intervention, this increases further to 99%. VFT\nachieves this by substantially increasing how often models verbalize the\ninfluence of cues--from 8% to 42% after VFT, and up to 94% after RL--while\nbaselines remain low even after RL (10% and 1%). Our results show that teaching\nmodels to explicitly verbalize reward hacking behavior before RL significantly\nimproves their detection, offering a practical path toward more transparent and\nsafe AI systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22777v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.22777v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23702", "title": "Rectangular $C^1$-$Q_k$ Bell finite elements in two and three dimensions", "authors": ["Hongling Hu", "Shangyou Zhang"], "summary": "Both the function and its normal derivative on the element boundary are $Q_k$\npolynomials\n  for the Bogner-Fox-Schmit $C^1$-$Q_k$ finite element functions.\nMathematically, to keep the optimal order of approximation, their spaces are\nrequired to\n  include $P_k$ and $P_{k-1}$ polynomials respectively. We construct a Bell\ntype $C^1$-$Q_k$ finite element on rectangular meshes in 2D and 3D,\n  which has its normal derivative as a $Q_{k-1}$ polynomial on each face, for\n$k\\ge 4$. We show, with a big reduction of the space, the $C^1$-$Q_k$ Bell\n  finite element retains the optimal order of convergence. Numerical\nexperiments are performed, comparing the new elements with the original\nelements.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23702v1", "categories": ["math.NA", "cs.NA"], "cate": "math.NA", "url": "http://arxiv.org/abs/2506.23702v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22944", "title": "Feasibility of spectral-element modeling of wave propagation through the anatomy of marine mammals", "authors": ["Carlos Garc√≠a A.", "Vladimiro Boselli", "Aida Hejazi Nooghabi", "Andrea Colombi", "Lapo Boschi"], "summary": "This study introduces the first 3D spectral-element method (SEM) simulation\nof ultrasonic wave propagation in a bottlenose dolphin (Tursiops truncatus)\nhead. Unlike traditional finite-element methods (FEM), which struggle with\nhigh-frequency simulations due to costly linear-system inversions and slower\nconvergence, SEM offers exponential convergence and efficient parallel\ncomputation. Using Computed Tomography (CT) scan data, we developed a detailed\nhexahedral mesh capturing complex anatomical features, such as acoustic fats\nand jaws. Our simulations of plane and spherical waves confirm SEM's\neffectiveness for ultrasonic time-domain modeling. This approach opens new\navenues for marine biology, contributing to research in echolocation, the\nimpacts of anthropogenic marine noise pollution and the biophysics of hearing\nand click generation in marine mammals. By overcoming FEM's limitations, SEM\nprovides a powerful scalable tool to test hypotheses about dolphin\nbioacoustics, with significant implications for conservation and understanding\nmarine mammal auditory systems under increasing environmental challenges.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22944v1", "categories": ["cs.CE", "cs.SD", "eess.AS", "q-bio.TO"], "cate": "cs.CE", "url": "http://arxiv.org/abs/2506.22944v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23314", "title": "Interpretable by Design: MH-AutoML for Transparent and Efficient Android Malware Detection without Compromising Performance", "authors": ["Joner Assolin", "Gabriel Canto", "Diego Kreutz", "Eduardo Feitosa", "Hendrio Bragan√ßa", "Angelo Nogueira", "Vanderson Rocha"], "summary": "Malware detection in Android systems requires both cybersecurity expertise\nand machine learning (ML) techniques. Automated Machine Learning (AutoML) has\nemerged as an approach to simplify ML development by reducing the need for\nspecialized knowledge. However, current AutoML solutions typically operate as\nblack-box systems with limited transparency, interpretability, and experiment\ntraceability. To address these limitations, we present MH-AutoML, a\ndomain-specific framework for Android malware detection. MH-AutoML automates\nthe entire ML pipeline, including data preprocessing, feature engineering,\nalgorithm selection, and hyperparameter tuning. The framework incorporates\ncapabilities for interpretability, debugging, and experiment tracking that are\noften missing in general-purpose solutions. In this study, we compare MH-AutoML\nagainst seven established AutoML frameworks: Auto-Sklearn, AutoGluon, TPOT,\nHyperGBM, Auto-PyTorch, LightAutoML, and MLJAR. Results show that MH-AutoML\nachieves better recall rates while providing more transparency and control. The\nframework maintains computational efficiency comparable to other solutions,\nmaking it suitable for cybersecurity applications where both performance and\nexplainability matter.", "comment": "18 pages, 10 figures, 7 tabelas, paper submitted to JBCS", "pdf_url": "http://arxiv.org/pdf/2506.23314v1", "categories": ["cs.CR", "cs.AI", "68T99", "I.2"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.23314v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23152", "title": "DexH2R: A Benchmark for Dynamic Dexterous Grasping in Human-to-Robot Handover", "authors": ["Youzhuo Wang", "Jiayi Ye", "Chuyang Xiao", "Yiming Zhong", "Heng Tao", "Hang Yu", "Yumeng Liu", "Jingyi Yu", "Yuexin Ma"], "summary": "Handover between a human and a dexterous robotic hand is a fundamental yet\nchallenging task in human-robot collaboration. It requires handling dynamic\nenvironments and a wide variety of objects and demands robust and adaptive\ngrasping strategies. However, progress in developing effective dynamic\ndexterous grasping methods is limited by the absence of high-quality,\nreal-world human-to-robot handover datasets. Existing datasets primarily focus\non grasping static objects or rely on synthesized handover motions, which\ndiffer significantly from real-world robot motion patterns, creating a\nsubstantial gap in applicability. In this paper, we introduce DexH2R, a\ncomprehensive real-world dataset for human-to-robot handovers, built on a\ndexterous robotic hand. Our dataset captures a diverse range of interactive\nobjects, dynamic motion patterns, rich visual sensor data, and detailed\nannotations. Additionally, to ensure natural and human-like dexterous motions,\nwe utilize teleoperation for data collection, enabling the robot's movements to\nalign with human behaviors and habits, which is a crucial characteristic for\nintelligent humanoid robots. Furthermore, we propose an effective solution,\nDynamicGrasp, for human-to-robot handover and evaluate various state-of-the-art\napproaches, including auto-regressive models and diffusion policy methods,\nproviding a thorough comparison and analysis. We believe our benchmark will\ndrive advancements in human-to-robot handover research by offering a\nhigh-quality dataset, effective solutions, and comprehensive evaluation\nmetrics.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23152v1", "categories": ["cs.RO"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.23152v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23493", "title": "Securing the Sky: Integrated Satellite-UAV Physical Layer Security for Low-Altitude Wireless Networks", "authors": ["Jiahui Li", "Geng Sun", "Xiaoyu Sun", "Fang Mei", "Jingjing Wang", "Xiangwang Hou", "Daxin Tian", "Victor C. M. Leung"], "summary": "Low-altitude wireless networks (LAWNs) have garnered significant attention in\nthe forthcoming 6G networks. In LAWNs, satellites with wide coverage and\nunmanned aerial vehicles (UAVs) with flexible mobility can complement each\nother to form integrated satellite-UAV networks, providing ubiquitous and\nhigh-speed connectivity for low-altitude operations. However, the higher\nline-of-sight probability in low-altitude airspace increases transmission\nsecurity concerns. In this work, we present a collaborative beamforming-based\nphysical layer security scheme for LAWNs. We introduce the fundamental aspects\nof integrated satellite-UAV networks, physical layer security, UAV swarms, and\ncollaborative beamforming for LAWN applications. Following this, we highlight\nseveral opportunities for collaborative UAV swarm secure applications enabled\nby satellite networks, including achieving physical layer security in scenarios\ninvolving data dissemination, data relay, eavesdropper collusion, and imperfect\neavesdropper information. Next, we detail two case studies: a secure relay\nsystem and a two-way aerial secure communication framework specifically\ndesigned for LAWN environments. Simulation results demonstrate that these\nphysical layer security schemes are effective and beneficial for secure\nlow-altitude wireless communications. A short practicality analysis shows that\nthe proposed method is applicable to LAWN scenarios. Finally, we discuss\ncurrent challenges and future research directions for enhancing security in\nLAWNs.", "comment": "This paper has been submitted to IEEE Wireless Communications", "pdf_url": "http://arxiv.org/pdf/2506.23493v1", "categories": ["cs.NI", "eess.SP"], "cate": "cs.NI", "url": "http://arxiv.org/abs/2506.23493v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23762", "title": "Software Engineering for Large Language Models: Research Status, Challenges and the Road Ahead", "authors": ["Hongzhou Rao", "Yanjie Zhao", "Xinyi Hou", "Shenao Wang", "Haoyu Wang"], "summary": "The rapid advancement of large language models (LLMs) has redefined\nartificial intelligence (AI), pushing the boundaries of AI research and\nenabling unbounded possibilities for both academia and the industry. However,\nLLM development faces increasingly complex challenges throughout its lifecycle,\nyet no existing research systematically explores these challenges and solutions\nfrom the perspective of software engineering (SE) approaches. To fill the gap,\nwe systematically analyze research status throughout the LLM development\nlifecycle, divided into six phases: requirements engineering, dataset\nconstruction, model development and enhancement, testing and evaluation,\ndeployment and operations, and maintenance and evolution. We then conclude by\nidentifying the key challenges for each phase and presenting potential research\ndirections to address these challenges. In general, we provide valuable\ninsights from an SE perspective to facilitate future advances in LLM\ndevelopment.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23762v1", "categories": ["cs.SE", "cs.AI"], "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.23762v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23253", "title": "Vibe coding: programming through conversation with artificial intelligence", "authors": ["Advait Sarkar", "Ian Drosos"], "summary": "We examine \"vibe coding\": an emergent programming paradigm where developers\nprimarily write code by interacting with code-generating large language models\nrather than writing code directly. We analysed a curated set of videos\ndepicting extended vibe coding sessions with rich think-aloud reflections.\nUsing framework analysis, we investigated programmers' goals, workflows,\nprompting techniques, debugging approaches, and challenges encountered. We find\nthat vibe coding follows iterative goal satisfaction cycles where developers\nalternate between prompting AI, evaluating generated code through rapid\nscanning and application testing, and manual editing. Prompting strategies\nblend vague, high-level directives with detailed technical specifications.\nDebugging remains a hybrid process combining AI assistance with manual\npractices. Critically, vibe coding does not eliminate the need for programming\nexpertise but rather redistributes it toward context management, rapid code\nevaluation, and decisions about when to transition between AI-driven and manual\nmanipulation of code. Trust in AI tools during vibe coding is dynamic and\ncontextual, developed through iterative verification rather than blanket\nacceptance. Vibe coding is an evolution of AI-assisted programming that\nrepresents an early manifestation of \"material disengagement\", where\npractitioners orchestrate code production and manipulation, mediated through\nAI, while maintaining selective and strategic oversight.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23253v1", "categories": ["cs.HC"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.23253v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23966", "title": "Pinching-Antenna Systems with In-Waveguide Attenuation: Performance Analysis and Algorithm Design", "authors": ["Yanqing Xu", "Zhiguo Ding", "Robert Schober", "Tsung-Hui Chang"], "summary": "Pinching-antenna systems have emerged as a promising flexible-antenna\narchitecture for next-generation wireless networks, enabling enhanced\nadaptability and user-centric connectivity through antenna repositioning along\nwaveguides. However, existing studies often overlook in-waveguide signal\nattenuation and in the literature, there is no comprehensive analysis on\nwhether and under what conditions such an assumption is justified. This paper\naddresses this gap by explicitly incorporating in-waveguide attenuation into\nboth the system model and algorithm design, and studying its impact on the\ndownlink user data rates. We begin with a single-user scenario and derive a\nclosed-form expression for the globally optimal antenna placement, which\nreveals how the attenuation coefficient and the user-to-waveguide distance\njointly affect the optimal antenna position. Based on this analytical solution,\nwe further provide a theoretical analysis identifying the system conditions\nunder which the in-waveguide attenuation has an insignificant impact on the\nuser achievable rate. The study is then extended to the multi-user\nmultiple-input multiple-output setting, where two efficient algorithms are\ndeveloped, based on the weighted minimum mean square error method and the\nmaximum ratio combining method, to jointly optimize beamforming and antenna\nplacement. Simulation results validate the efficacy of the proposed algorithms\nand demonstrate that pinching-antenna systems substantially outperform\nconventional fixed-antenna baselines, underscoring their potential for future\nflexible wireless communications.", "comment": "This paper aims to address a fundamental question in pinching-antenna\n  systems: Can in-waveguide attenuation be safely ignored without causing\n  significant performance degradation? Our analytical results provide a clear\n  answer -- YES, provided that certain mild and practically realizable\n  conditions on the system parameters are satisfied", "pdf_url": "http://arxiv.org/pdf/2506.23966v1", "categories": ["eess.SP", "cs.IT", "math.IT"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.23966v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22466", "title": "Conversations with Andrea: Visitors' Opinions on Android Robots in a Museum", "authors": ["Marcel Heisler", "Christian Becker-Asano"], "summary": "The android robot Andrea was set up at a public museum in Germany for six\nconsecutive days to have conversations with visitors, fully autonomously. No\nspecific context was given, so visitors could state their opinions regarding\npossible use-cases in structured interviews, without any bias. Additionally the\n44 interviewees were asked for their general opinions of the robot, their\nreasons (not) to interact with it and necessary improvements for future use.\nThe android's voice and wig were changed between different days of operation to\ngive varying cues regarding its gender. This did not have a significant impact\non the positive overall perception of the robot. Most visitors want the robot\nto provide information about exhibits in the future, while opinions on other\nroles, like a receptionist, were both wanted and explicitly not wanted by\ndifferent visitors. Speaking more languages (than only English) and faster\nresponse times were the improvements most desired. These findings from the\ninterviews are in line with an analysis of the system logs, which revealed,\nthat after chitchat and personal questions, most of the 4436 collected requests\nasked for information related to the museum and to converse in a different\nlanguage. The valuable insights gained from these real-world interactions are\nnow used to improve the system to become a useful real-world application.", "comment": "To be published in IEEE RO-MAN 2025 conference proceedings; for\n  videos check https://ai.hdm-stuttgart.de/humanoid-lab", "pdf_url": "http://arxiv.org/pdf/2506.22466v1", "categories": ["cs.RO", "cs.CY", "I.2.9; I.2.7"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.22466v1", "date": "2025-06-18", "updated": "2025-06-18"}
{"id": "2506.22567", "title": "Unifying Biomedical Vision-Language Expertise: Towards a Generalist Foundation Model via Multi-CLIP Knowledge Distillation", "authors": ["Shansong Wang", "Zhecheng Jin", "Mingzhe Hu", "Mojtaba Safari", "Feng Zhao", "Chih-Wei Chang", "Richard LJ Qiu", "Justin Roper", "David S. Yu", "Xiaofeng Yang"], "summary": "CLIP models pretrained on natural images with billion-scale image-text pairs\nhave demonstrated impressive capabilities in zero-shot classification,\ncross-modal retrieval, and open-ended visual answering. However, transferring\nthis success to biomedicine is hindered by the scarcity of large-scale\nbiomedical image-text corpora, the heterogeneity of image modalities, and\nfragmented data standards across institutions. These limitations hinder the\ndevelopment of a unified and generalizable biomedical foundation model trained\nfrom scratch. To overcome this, we introduce MMKD-CLIP, a generalist biomedical\nfoundation model developed via Multiple Medical CLIP Knowledge Distillation.\nRather than relying on billion-scale raw data, MMKD-CLIP distills knowledge\nfrom nine state-of-the-art domain-specific or generalist biomedical CLIP\nmodels, each pretrained on millions of biomedical image-text pairs. Our\ntwo-stage training pipeline first performs CLIP-style pretraining on over 2.9\nmillion biomedical image-text pairs from 26 image modalities, followed by\nfeature-level distillation using over 19.2 million feature pairs extracted from\nteacher models. We evaluate MMKD-CLIP on 58 diverse biomedical datasets,\nencompassing over 10.8 million biomedical images across nine image modalities.\nThe evaluation spans six core task types: zero-shot classification, linear\nprobing, cross-modal retrieval, visual question answering, survival prediction,\nand cancer diagnosis. MMKD-CLIP consistently outperforms all teacher models\nwhile demonstrating remarkable robustness and generalization across image\ndomains and task settings. These results underscore that multi-teacher\nknowledge distillation is a scalable and effective paradigm for building\nhigh-performing biomedical foundation models under the practical constraints of\nreal-world data availability.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22567v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22567v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.23509", "title": "Power-Gas Infrastructure Planning under Weather-induced Supply and Demand Uncertainties", "authors": ["Rahman Khorramfar", "Dharik Mallapragada", "Saurabh Amin"], "summary": "Implementing economy-wide decarbonization strategies based on decarbonizing\nthe power grid via variable renewable energy (VRE) expansion and\nelectrification of end-uses requires new approaches for energy infrastructure\nplanning that consider, among other factors, weather-induced uncertainty in\ndemand and VRE supply. An energy planning model that fails to account for these\nuncertainties can hinder the intended transition efforts to a low-carbon grid\nand increase the risk of supply shortage especially during extreme weather\nconditions. Here, we consider the generation and transmission expansion problem\nof joint power-gas infrastructure and operations planning under the uncertainty\nof both demand and renewable supply. We propose two distributionally robust\noptimization approaches based on moment (MDRO) and Wasserstein distance (WDRO)\nambiguity sets to endogenize these uncertainties and account for the change in\nthe underlying distribution of these parameters that is caused by the climate\nchange, among other factors. Furthermore, our model considers the risk-aversion\nof the energy planners in the modeling framework via the conditional\nvalue-at-risk (CVaR) metric. An equivalent mixed-integer linear programming\n(MILP) reformulation of both modeling frameworks is presented, and a\ncomputationally efficient approximation scheme to obtain near-optimal solutions\nis proposed. We demonstrate the resulting DRO planning models and solution\nstrategy via a New England case study under different levels of end-use\nelectrification and decarbonization targets. Our experiments systematically\nexplore different modeling aspects and compare the DRO models with stochastic\nprogramming (SP) results.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23509v1", "categories": ["eess.SY", "cs.SY"], "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.23509v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22490", "title": "MENGLAN: Multiscale Enhanced Nonparametric Gas Analyzer with Lightweight Architecture and Networks", "authors": ["Zhenke Duan", "Jiqun Pan", "Jiani Tu"], "summary": "Accurate detection of ethylene concentrations in mixed gases is crucial in\nchemical production for safety and health purposes. Traditional methods are\nhindered by high cost and complexity, limiting their practical application.\nThis study proposes MENGLAN, a Multiscale Enhanced Nonparametric Gas Analyzer\nthat integrates a dual-stream structure, a Hybrid Multi-Head Attention\nmechanism, and a Feature Reactivation Module to enable real-time, lightweight,\nand high-precision ethylene concentration prediction. Results show that MENGLAN\nachieves superior performance, reduced computational demand, and enhanced\ndeployability compared to existing methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22490v1", "categories": ["eess.SP", "cs.LG"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.22490v1", "date": "2025-06-24", "updated": "2025-06-24"}
{"id": "2506.23466", "title": "FD-DiT: Frequency Domain-Directed Diffusion Transformer for Low-Dose CT Reconstruction", "authors": ["Qiqing Liu", "Guoquan Wei", "Zekun Zhou", "Yiyang Wen", "Liu Shi", "Qiegen Liu"], "summary": "Low-dose computed tomography (LDCT) reduces radiation exposure but suffers\nfrom image artifacts and loss of detail due to quantum and electronic noise,\npotentially impacting diagnostic accuracy. Transformer combined with diffusion\nmodels has been a promising approach for image generation. Nevertheless,\nexisting methods exhibit limitations in preserving finegrained image details.\nTo address this issue, frequency domain-directed diffusion transformer (FD-DiT)\nis proposed for LDCT reconstruction. FD-DiT centers on a diffusion strategy\nthat progressively introduces noise until the distribution statistically aligns\nwith that of LDCT data, followed by denoising processing. Furthermore, we\nemploy a frequency decoupling technique to concentrate noise primarily in\nhigh-frequency domain, thereby facilitating effective capture of essential\nanatomical structures and fine details. A hybrid denoising network is then\nutilized to optimize the overall data reconstruction process. To enhance the\ncapability in recognizing high-frequency noise, we incorporate sliding sparse\nlocal attention to leverage the sparsity and locality of shallow-layer\ninformation, propagating them via skip connections for improving feature\nrepresentation. Finally, we propose a learnable dynamic fusion strategy for\noptimal component integration. Experimental results demonstrate that at\nidentical dose levels, LDCT images reconstructed by FD-DiT exhibit superior\nnoise and artifact suppression compared to state-of-the-art methods.", "comment": "11pages, 11 figures", "pdf_url": "http://arxiv.org/pdf/2506.23466v1", "categories": ["eess.IV", "cs.CV", "physics.med-ph"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.23466v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22701", "title": "Lower bounds for trace estimation via Block Krylov and other methods", "authors": ["Shi Jie Yu"], "summary": "This paper studies theoretical lower bounds for estimating the trace of a\nmatrix function, $\\text{tr}(f(A))$, focusing on methods that use Hutchinson's\nmethod along with Block Krylov techniques. These methods work by approximating\nmatrix-vector products like $f(A)V$ using a Block Krylov subspace. This is\nclosely related to approximating functions with polynomials. We derive\ntheoretical upper bounds on how many Krylov steps are needed for functions such\nas $A^{-1/2}$ and $A^{-1}$ by analyzing the upper bounds from the polynomial\napproximation of their scalar equivalent. In addition, we also develop lower\nlimits on the number of queries needed for trace estimation, specifically for\n$\\text{tr}(W^{-p})$ where $W$ is a Wishart matrix. Our study clarifies the\nconnection between the number of steps in Block Krylov methods and the degree\nof the polynomial used for approximation. This links the total cost of trace\nestimation to basic limits in polynomial approximation and how much information\nis needed for the computation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22701v1", "categories": ["math.ST", "cs.DS", "cs.LG", "cs.NA", "math.NA", "stat.TH"], "cate": "math.ST", "url": "http://arxiv.org/abs/2506.22701v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.22685", "title": "Mitigating Semantic Collapse in Generative Personalization with a Surprisingly Simple Test-Time Embedding Adjustment", "authors": ["Anh Bui", "Trang Vu", "Trung Le", "Junae Kim", "Tamas Abraham", "Rollin Omari", "Amar Kaur", "Dinh Phung"], "summary": "In this paper, we investigate the semantic collapsing problem in generative\npersonalization, an under-explored topic where the learned visual concept\n($V^*$) gradually shifts from its original textual meaning and comes to\ndominate other concepts in multi-concept input prompts. This issue not only\nreduces the semantic richness of complex input prompts like \"a photo of $V^*$\nwearing glasses and playing guitar\" into simpler, less contextually rich forms\nsuch as \"a photo of $V^*$\" but also leads to simplified output images that fail\nto capture the intended concept.\n  We identify the root cause as unconstrained optimisation, which allows the\nlearned embedding $V^*$ to drift arbitrarily in the embedding space, both in\ndirection and magnitude. To address this, we propose a simple yet effective\ntraining-free method that adjusts the magnitude and direction of pre-trained\nembedding at inference time, effectively mitigating the semantic collapsing\nproblem. Our method is broadly applicable across different personalization\nmethods and demonstrates significant improvements in text-image alignment in\ndiverse use cases. Our code is anonymously published at\nhttps://anonymous.4open.science/r/Embedding-Adjustment.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22685v1", "categories": ["cs.LG", "cs.GR"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22685v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.23276", "title": "Corrupted by Reasoning: Reasoning Language Models Become Free-Riders in Public Goods Games", "authors": ["David Guzman Piedrahita", "Yongjin Yang", "Mrinmaya Sachan", "Giorgia Ramponi", "Bernhard Sch√∂lkopf", "Zhijing Jin"], "summary": "As large language models (LLMs) are increasingly deployed as autonomous\nagents, understanding their cooperation and social mechanisms is becoming\nincreasingly important. In particular, how LLMs balance self-interest and\ncollective well-being is a critical challenge for ensuring alignment,\nrobustness, and safe deployment. In this paper, we examine the challenge of\ncostly sanctioning in multi-agent LLM systems, where an agent must decide\nwhether to invest its own resources to incentivize cooperation or penalize\ndefection. To study this, we adapt a public goods game with institutional\nchoice from behavioral economics, allowing us to observe how different LLMs\nnavigate social dilemmas over repeated interactions. Our analysis reveals four\ndistinct behavioral patterns among models: some consistently establish and\nsustain high levels of cooperation, others fluctuate between engagement and\ndisengagement, some gradually decline in cooperative behavior over time, and\nothers rigidly follow fixed strategies regardless of outcomes. Surprisingly, we\nfind that reasoning LLMs, such as the o1 series, struggle significantly with\ncooperation, whereas some traditional LLMs consistently achieve high levels of\ncooperation. These findings suggest that the current approach to improving\nLLMs, which focuses on enhancing their reasoning capabilities, does not\nnecessarily lead to cooperation, providing valuable insights for deploying LLM\nagents in environments that require sustained collaboration. Our code is\navailable at https://github.com/davidguzmanp/SanctSim", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23276v1", "categories": ["cs.AI", "cs.CL"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.23276v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23367", "title": "You Sound a Little Tense: L2 Tailored Clear TTS Using Durational Vowel Properties", "authors": ["Paige Tutt√∂s√≠", "H. Henny Yeung", "Yue Wang", "Jean-Julien Aucouturier", "Angelica Lim"], "summary": "We present the first text-to-speech (TTS) system tailored to second language\n(L2) speakers. We use duration differences between American English tense\n(longer) and lax (shorter) vowels to create a \"clarity mode\" for Matcha-TTS.\nOur perception studies showed that French-L1, English-L2 listeners had fewer\n(at least 9.15%) transcription errors when using our clarity mode, and found it\nmore encouraging and respectful than overall slowed down speech. Remarkably,\nlisteners were not aware of these effects: despite the decreased word error\nrate in clarity mode, listeners still believed that slowing all target words\nwas the most intelligible, suggesting that actual intelligibility does not\ncorrelate with perceived intelligibility. Additionally, we found that\nWhisper-ASR did not use the same cues as L2 speakers to differentiate difficult\nvowels and is not sufficient to assess the intelligibility of TTS systems for\nthese individuals.", "comment": "Accepted to ISCA Speech Synthesis Workshop, 2025", "pdf_url": "http://arxiv.org/pdf/2506.23367v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.23367v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.22791", "title": "ContextCache: Context-Aware Semantic Cache for Multi-Turn Queries in Large Language Models", "authors": ["Jianxin Yan", "Wangze Ni", "Lei Chen", "Xuemin Lin", "Peng Cheng", "Zhan Qin", "Kui Ren"], "summary": "Semantic caching significantly reduces computational costs and improves\nefficiency by storing and reusing large language model (LLM) responses.\nHowever, existing systems rely primarily on matching individual queries,\nlacking awareness of multi-turn dialogue contexts, which leads to incorrect\ncache hits when similar queries appear in different conversational settings.\nThis demonstration introduces ContextCache, a context-aware semantic caching\nsystem for multi-turn dialogues. ContextCache employs a two-stage retrieval\narchitecture that first executes vector-based retrieval on the current query to\nidentify potential matches and then integrates current and historical dialogue\nrepresentations through self-attention mechanisms for precise contextual\nmatching. Evaluation of real-world conversations shows that ContextCache\nimproves precision and recall compared to existing methods. Additionally,\ncached responses exhibit approximately 10 times lower latency than direct LLM\ninvocation, enabling significant computational cost reductions for LLM\nconversational applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22791v1", "categories": ["cs.CL", "cs.DB"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.22791v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23741", "title": "Efficient Numerical Integration for Finite Element Trunk Spaces in 2D and 3D using Machine Learning: A new Optimisation Paradigm to Construct Application-Specific Quadrature Rules", "authors": ["Tomas Teijeiro", "Pouria Behnoudfar", "Jamie M. Taylor", "David Pardo", "Victor M. Calo"], "summary": "Finite element methods usually construct basis functions and quadrature rules\nfor multidimensional domains via tensor products of one-dimensional\ncounterparts. While straightforward, this approach results in integration\nspaces larger than necessary, especially as the polynomial degree $p$ or the\nspatial dimension increases, leading to considerable computational overhead.\nThis work starts from the hypothesis that reducing the dimensionality of the\npolynomial space can lead to quadrature rules with fewer points and lower\ncomputational cost, while preserving the exactness of numerical integration. We\nuse trunk spaces that exclude high-degree monomials that do not improve the\napproximation quality of the discrete space. These reduced spaces retain\nsufficient expressive power and allow us to construct smaller (more economical)\nintegration domains. Given a maximum degree $p$, we define trial and test\nspaces $U$ and $V$ as 2D or 3D trunk spaces and form the integration space\n$\\mathcal{S} = U \\otimes V$. We then construct exact quadrature rules by\nsolving a non-convex optimisation problem over the number of points $q$, their\ncoordinates, and weights. We use a shallow neural network with linear\nactivations to parametrise the rule, and a random restart strategy to mitigate\nconvergence to poor local minima. When necessary, we dynamically increase $q$\nto achieve exact integration. Our construction reaches machine-precision\naccuracy (errors below 1e-22) using significantly fewer points than standard\ntensor-product Gaussian quadrature: up to 30\\% reduction in 2D for $p \\leq 10$,\nand 50\\% in 3D for $p \\leq 6$. These results show that combining the\nmathematical understanding of polynomial structure with numerical optimisation\ncan lead to a practical and extensible methodology for improving the\nadaptiveness, efficiency, and scalability of quadrature rules for high-order\nfinite element simulations.", "comment": "15 pages, 5 figures, 2 tables", "pdf_url": "http://arxiv.org/pdf/2506.23741v1", "categories": ["math.NA", "cs.NA", "65D32"], "cate": "math.NA", "url": "http://arxiv.org/abs/2506.23741v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23030", "title": "VisionScores -- A system-segmented image score dataset for deep learning tasks", "authors": ["Alejandro Romero Amezcua", "Mariano Jos√© Juan Rivera Meraz"], "summary": "VisionScores presents a novel proposal being the first system-segmented image\nscore dataset, aiming to offer structure-rich, high information-density images\nfor machine and deep learning tasks. Delimited to two-handed piano pieces, it\nwas built to consider not only certain graphic similarity but also composition\npatterns, as this creative process is highly instrument-dependent. It provides\ntwo scenarios in relation to composer and composition type. The first, formed\nby 14k samples, considers works from different authors but the same composition\ntype, specifically, Sonatinas. The latter, consisting of 10.8K samples,\npresents the opposite case, various composition types from the same author,\nbeing the one selected Franz Liszt. All of the 24.8k samples are formatted as\ngrayscale jpg images of $128 \\times 512$ pixels. VisionScores supplies the\nusers not only the formatted samples but the systems' order and pieces'\nmetadata. Moreover, unsegmented full-page scores and the pre-formatted images\nare included for further analysis.", "comment": "Comments: 5 pages, 3 figures. Accepted for presentation at the 2025\n  IEEE International Conference on Image Processing (ICIP). \\c{opyright} 2025\n  IEEE. Personal use of this material is permitted. Permission from IEEE must\n  be obtained for any other use", "pdf_url": "http://arxiv.org/pdf/2506.23030v1", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23030v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23435", "title": "All Proof of Work But No Proof of Play", "authors": ["Hayder Tirmazi"], "summary": "Speedrunning is a competition that emerged from communities of early video\ngames such as Doom (1993). Speedrunners try to finish a game in minimal time.\nProvably verifying the authenticity of submitted speedruns is an open problem.\nTraditionally, best-effort speedrun verification is conducted by on-site human\nobservers, forensic audio analysis, or a rigorous mathematical analysis of the\ngame mechanics. Such methods are tedious, fallible, and, perhaps worst of all,\nnot cryptographic. Motivated by naivety and the Dunning-Kruger effect, we\nattempt to build a system that cryptographically proves the authenticity of\nspeedruns. This paper describes our attempted solutions and ways to circumvent\nthem. Through a narration of our failures, we attempt to demonstrate the\ndifficulty of authenticating live and interactive human input in untrusted\nenvironments, as well as the limits of signature schemes, game integrity, and\nprovable play.", "comment": "Published in CFAIL 2025", "pdf_url": "http://arxiv.org/pdf/2506.23435v1", "categories": ["cs.CR", "cs.NI"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.23435v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23164", "title": "Mode Collapse Happens: Evaluating Critical Interactions in Joint Trajectory Prediction Models", "authors": ["Maarten Hugenholtz", "Anna Meszaros", "Jens Kober", "Zlatan Ajanovic"], "summary": "Autonomous Vehicle decisions rely on multimodal prediction models that\naccount for multiple route options and the inherent uncertainty in human\nbehavior. However, models can suffer from mode collapse, where only the most\nlikely mode is predicted, posing significant safety risks. While existing\nmethods employ various strategies to generate diverse predictions, they often\noverlook the diversity in interaction modes among agents. Additionally,\ntraditional metrics for evaluating prediction models are dataset-dependent and\ndo not evaluate inter-agent interactions quantitatively. To our knowledge, none\nof the existing metrics explicitly evaluates mode collapse. In this paper, we\npropose a novel evaluation framework that assesses mode collapse in joint\ntrajectory predictions, focusing on safety-critical interactions. We introduce\nmetrics for mode collapse, mode correctness, and coverage, emphasizing the\nsequential dimension of predictions. By testing four multi-agent trajectory\nprediction models, we demonstrate that mode collapse indeed happens. When\nlooking at the sequential dimension, although prediction accuracy improves\ncloser to interaction events, there are still cases where the models are unable\nto predict the correct interaction mode, even just before the interaction mode\nbecomes inevitable. We hope that our framework can help researchers gain new\ninsights and advance the development of more consistent and accurate prediction\nmodels, thus enhancing the safety of autonomous driving systems.", "comment": "12 pages, 8 figures, submitted to a journal", "pdf_url": "http://arxiv.org/pdf/2506.23164v1", "categories": ["cs.RO", "cs.AI"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.23164v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23628", "title": "The Kubernetes Network Driver Model: A Composable Architecture for High-Performance Networking", "authors": ["Antonio Ojea"], "summary": "Traditional Kubernetes networking struggles to meet the escalating demands of\nAI/ML and evolving Telco infrastructure. This paper introduces Kubernetes\nNetwork Drivers (KNDs), a transformative, modular, and declarative architecture\ndesigned to overcome current imperative provisioning and API limitations. KNDs\nintegrate network resource management into Kubernetes' core by utilizing\nDynamic Resource Allocation (DRA), Node Resource Interface (NRI) improvements,\nand upcoming OCI Runtime Specification changes. Our DraNet implementation\ndemonstrates declarative attachment of network interfaces, including Remote\nDirect Memory Access (RDMA) devices, significantly boosting high-performance\nAI/ML workloads. This capability enables sophisticated cloud-native\napplications and lays crucial groundwork for future Telco solutions, fostering\na \"galaxy\" of specialized KNDs for enhanced application delivery and reduced\noperational complexity.", "comment": "6 pages, 9 figures, submitted to IEEE LCN Special Track on\n  Cloud-AI-Native Mobile Networks Powered by eBPF (CAMe 2025)", "pdf_url": "http://arxiv.org/pdf/2506.23628v1", "categories": ["cs.NI", "cs.AI"], "cate": "cs.NI", "url": "http://arxiv.org/abs/2506.23628v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23898", "title": "Requirements for Active Assistance of Natural Questions in Software Architecture", "authors": ["Diogo Lemos", "Ademar Aguiar", "Neil B. Harrison"], "summary": "Natural questions are crucial to shaping key architectural decisions and\npreserving architectural knowledge. They arise organically during the\narchitectural design process, often resulting from the existing architectural\nexperience of the designer and the distinctive characteristics of the system\nbeing designed. However, natural questions are often mismanaged or ignored,\nwhich can lead to architectural drift, knowledge loss, inefficient resource\nuse, or poor understandability of the system's architecture. We aim to better\nunderstand the lifecycle of natural questions, its key requirements, challenges\nand difficulties, and then to envision an assisted environment to properly\nsupport it. The environment should be adaptable and responsive to real-world\nconstraints and uncertainties by seamlessly integrating knowledge management\ntools and artificial intelligence techniques into software development\nworkflows. Based on existing literature, a requirements workshop, and three\ndesign iterations, we proposed a lifecycle for natural questions and elicited\nessential functional and non-functional requirements for such an environment.\nAt last, the results of a survey conducted with experts helped to analyze and\nvalidate the elicited requirements and proposed features for the environment to\nenhance collaboration, decision-making, and the preservation of architectural\nknowledge more effectively than conventional methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23898v1", "categories": ["cs.SE"], "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.23898v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23443", "title": "Accessible Data Access and Analysis by People who are Blind or Have Low Vision", "authors": ["Samuel Reinders", "Munazza Zaib", "Matthew Butler", "Bongshin Lee", "Ingrid Zukerman", "Lizhen Qu", "Kim Marriott"], "summary": "Our work aims to develop new assistive technologies that enable blind or low\nvision (BLV) people to explore and analyze data readily. At present, barriers\nexist for BLV people to explore and analyze data, restricting access to\ngovernment, health and personal data, and limiting employment opportunities.\nThis work explores the co-design and development of an innovative system to\nsupport data access, with a focus on the use of refreshable tactile displays\n(RTDs) and conversational agents. The envisaged system will use a combination\nof tactile graphics and speech to communicate with BLV users, and proactively\nassist with data analysis tasks. As well as addressing significant equity gaps,\nour work expects to produce innovations in assistive technology, multimodal\ninterfaces, dialogue systems, and natural language understanding and\ngeneration.", "comment": "Poster presented at the 1st Workshop on Accessible Data\n  Visualization, IEEE VIS 2024", "pdf_url": "http://arxiv.org/pdf/2506.23443v1", "categories": ["cs.HC"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.23443v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22491", "title": "PromptAug: Fine-grained Conflict Classification Using Data Augmentation", "authors": ["Oliver Warke", "Joemon M. Jose", "Faegheh Hasibi", "Jan Breitsohl"], "summary": "Given the rise of conflicts on social media, effective classification models\nto detect harmful behaviours are essential. Following the\ngarbage-in-garbage-out maxim, machine learning performance depends heavily on\ntraining data quality. However, high-quality labelled data, especially for\nnuanced tasks like identifying conflict behaviours, is limited, expensive, and\ndifficult to obtain. Additionally, as social media platforms increasingly\nrestrict access to research data, text data augmentation is gaining attention\nas an alternative to generate training data. Augmenting conflict-related data\nposes unique challenges due to Large Language Model (LLM) guardrails that\nprevent generation of offensive content. This paper introduces PromptAug, an\ninnovative LLM-based data augmentation method. PromptAug achieves statistically\nsignificant improvements of 2% in both accuracy and F1-score on conflict and\nemotion datasets. To thoroughly evaluate PromptAug against other data\naugmentation methods we conduct a robust evaluation using extreme data scarcity\nscenarios, quantitative diversity analysis and a qualitative thematic analysis.\nThe thematic analysis identifies four problematic patterns in augmented text:\nLinguistic Fluidity, Humour Ambiguity, Augmented Content Ambiguity, and\nAugmented Content Misinterpretation.\n  Overall, this work presents PromptAug as an effective method for augmenting\ndata in sensitive tasks like conflict detection, offering a unique,\ninterdisciplinary evaluation grounded in both natural language processing and\nsocial science methodology.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22491v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "I.2.7; J.4; K.4.2"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.22491v1", "date": "2025-06-24", "updated": "2025-06-24"}
{"id": "2506.22570", "title": "Dual Atrous Separable Convolution for Improving Agricultural Semantic Segmentation", "authors": ["Chee Mei Ling", "Thangarajah Akilan", "Aparna Ravinda Phalke"], "summary": "Agricultural image semantic segmentation is a pivotal component of modern\nagriculture, facilitating accurate visual data analysis to improve crop\nmanagement, optimize resource utilization, and boost overall productivity. This\nstudy proposes an efficient image segmentation method for precision\nagriculture, focusing on accurately delineating farmland anomalies to support\ninformed decision-making and proactive interventions. A novel Dual Atrous\nSeparable Convolution (DAS Conv) module is integrated within the\nDeepLabV3-based segmentation framework. The DAS Conv module is meticulously\ndesigned to achieve an optimal balance between dilation rates and padding size,\nthereby enhancing model performance without compromising efficiency. The study\nalso incorporates a strategic skip connection from an optimal stage in the\nencoder to the decoder to bolster the model's capacity to capture fine-grained\nspatial features. Despite its lower computational complexity, the proposed\nmodel outperforms its baseline and achieves performance comparable to highly\ncomplex transformer-based state-of-the-art (SOTA) models on the Agriculture\nVision benchmark dataset. It achieves more than 66% improvement in efficiency\nwhen considering the trade-off between model complexity and performance,\ncompared to the SOTA model. This study highlights an efficient and effective\nsolution for improving semantic segmentation in remote sensing applications,\noffering a computationally lightweight model capable of high-quality\nperformance in agricultural imagery.", "comment": "17 pages, 7 figures, 6 tables", "pdf_url": "http://arxiv.org/pdf/2506.22570v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22570v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.23554", "title": "A Bidirectional Power Router for Traceable Multi-energy Management", "authors": ["Shiu Mochiyama", "Ryo Takahashi", "Yoshihiko Susuki"], "summary": "To address challenges in improving self-consumption of renewables and\nresilience in local residential power systems, the earlier work of the authors\nintroduced a novel multi-energy management concept, integrating bidirectional\npower routing and electricity-hydrogen conversion. This paper focuses on an\nexperimental verification of the bidirectional power router based on\nline-switching, the essential hardware to realize the concept. The primary\ncontribution is the validation of the router's capability to handle dynamic\nchange of bidirectional power flow. Furthermore, to achieve bidirectional power\nrouting without affecting the smooth and stable operation of the power system,\na novel algorithm for router's switching is designed based on power flow\nmonitoring. The effectiveness of the proposed method is demonstrated through an\nexperiment using a setup with a commercially available stationary battery.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23554v1", "categories": ["eess.SY", "cs.SY"], "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.23554v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22495", "title": "Masked Autoencoders that Feel the Heart: Unveiling Simplicity Bias for ECG Analyses", "authors": ["He-Yang Xu", "Hongxiang Gao", "Yuwen Li", "Xiu-Shen Wei", "Chengyu Liu"], "summary": "The diagnostic value of electrocardiogram (ECG) lies in its dynamic\ncharacteristics, ranging from rhythm fluctuations to subtle waveform\ndeformations that evolve across time and frequency domains. However, supervised\nECG models tend to overfit dominant and repetitive patterns, overlooking\nfine-grained but clinically critical cues, a phenomenon known as Simplicity\nBias (SB), where models favor easily learnable signals over subtle but\ninformative ones. In this work, we first empirically demonstrate the presence\nof SB in ECG analyses and its negative impact on diagnostic performance, while\nsimultaneously discovering that self-supervised learning (SSL) can alleviate\nit, providing a promising direction for tackling the bias. Following the SSL\nparadigm, we propose a novel method comprising two key components: 1)\nTemporal-Frequency aware Filters to capture temporal-frequency features\nreflecting the dynamic characteristics of ECG signals, and 2) building on this,\nMulti-Grained Prototype Reconstruction for coarse and fine representation\nlearning across dual domains, further mitigating SB. To advance SSL in ECG\nanalyses, we curate a large-scale multi-site ECG dataset with 1.53 million\nrecordings from over 300 clinical centers. Experiments on three downstream\ntasks across six ECG datasets demonstrate that our method effectively reduces\nSB and achieves state-of-the-art performance. Code and dataset will be released\npublicly.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22495v1", "categories": ["eess.SP", "cs.AI", "cs.LG"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.22495v1", "date": "2025-06-25", "updated": "2025-06-25"}
{"id": "2506.23490", "title": "UltraTwin: Towards Cardiac Anatomical Twin Generation from Multi-view 2D Ultrasound", "authors": ["Junxuan Yu", "Yaofei Duan", "Yuhao Huang", "Yu Wang", "Rongbo Ling", "Weihao Luo", "Ang Zhang", "Jingxian Xu", "Qiongying Ni", "Yongsong Zhou", "Binghan Li", "Haoran Dou", "Liping Liu", "Yanfen Chu", "Feng Geng", "Zhe Sheng", "Zhifeng Ding", "Dingxin Zhang", "Rui Huang", "Yuhang Zhang", "Xiaowei Xu", "Tao Tan", "Dong Ni", "Zhongshan Gou", "Xin Yang"], "summary": "Echocardiography is routine for cardiac examination. However, 2D ultrasound\n(US) struggles with accurate metric calculation and direct observation of 3D\ncardiac structures. Moreover, 3D US is limited by low resolution, small field\nof view and scarce availability in practice. Constructing the cardiac\nanatomical twin from 2D images is promising to provide precise treatment\nplanning and clinical quantification. However, it remains challenging due to\nthe rare paired data, complex structures, and US noises. In this study, we\nintroduce a novel generative framework UltraTwin, to obtain cardiac anatomical\ntwin from sparse multi-view 2D US. Our contribution is three-fold. First,\npioneered the construction of a real-world and high-quality dataset containing\nstrictly paired multi-view 2D US and CT, and pseudo-paired data. Second, we\npropose a coarse-to-fine scheme to achieve hierarchical reconstruction\noptimization. Last, we introduce an implicit autoencoder for topology-aware\nconstraints. Extensive experiments show that UltraTwin reconstructs\nhigh-quality anatomical twins versus strong competitors. We believe it advances\nanatomical twin modeling for potential applications in personalized cardiac\ncare.", "comment": "accepted by miccai 2025", "pdf_url": "http://arxiv.org/pdf/2506.23490v1", "categories": ["eess.IV", "cs.AI", "cs.CV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.23490v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23062", "title": "Shifted Composition IV: Underdamped Langevin and Numerical Discretizations with Partial Acceleration", "authors": ["Jason M. Altschuler", "Sinho Chewi", "Matthew S. Zhang"], "summary": "Quantifying the convergence rate of the underdamped Langevin dynamics (ULD)\nis a classical topic, in large part due to the possibility for\ndiffusive-to-ballistic speedups -- as was recently established for the\ncontinuous-time dynamics via space-time Poincare inequalities. A central\nchallenge for analyzing ULD is that its degeneracy necessitates the development\nof new analysis approaches, e.g., the theory of hypocoercivity. In this paper,\nwe give a new coupling-based framework for analyzing ULD and its numerical\ndiscretizations. First, in the continuous-time setting, we use this framework\nto establish new parabolic Harnack inequalities for ULD. These are the first\nHarnack inequalities that decay to zero in contractive settings, thereby\nreflecting the convergence properties of ULD in addition to just its regularity\nproperties.\n  Second, we build upon these Harnack inequalities to develop a local error\nframework for analyzing discretizations of ULD in KL divergence. This extends\nour framework in part III from uniformly elliptic diffusions to degenerate\ndiffusions, and shares its virtues: the framework is user-friendly, applies to\nsophisticated discretization schemes, and does not require contractivity.\nApplying this framework to the randomized midpoint discretization of ULD\nestablishes (i) the first ballistic acceleration result for log-concave\nsampling (i.e., sublinear dependence on the condition number), and (ii) the\nfirst $d^{1/3}$ iteration complexity guarantee for sampling to constant total\nvariation error in dimension $d$.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23062v1", "categories": ["math.PR", "cs.DS", "cs.NA", "math.AP", "math.NA", "math.ST", "stat.TH"], "cate": "math.PR", "url": "http://arxiv.org/abs/2506.23062v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.22696", "title": "Residual Matrix Transformers: Scaling the Size of the Residual Stream", "authors": ["Brian Mak", "Jeffrey Flanigan"], "summary": "The residual stream acts as a memory bus where transformer layers both store\nand access features (Elhage et al., 2021). We consider changing the mechanism\nfor retrieving and storing information in the residual stream, and replace the\nresidual stream of the transformer with an outer product memory matrix\n(Kohonen, 1972, Anderson, 1972). We call this model the Residual Matrix\nTransformer (RMT). We find that the RMT enjoys a number of attractive\nproperties: 1) the size of the residual stream can be scaled independently of\ncompute and model size, improving performance, 2) the RMT can achieve the same\nloss as the transformer with 58% fewer FLOPS, 25% fewer parameters, and 41%\nfewer training tokens tokens, and 3) the RMT outperforms the transformer on\ndownstream evaluations. We theoretically analyze the transformer and the RMT,\nand show that the RMT allows for more efficient scaling of the residual stream,\nas well as improved variance propagation properties. Code for this project can\nbe found at https://github.com/bmac3/residual-matrix-transformer.", "comment": "Accepted to ICML 2025", "pdf_url": "http://arxiv.org/pdf/2506.22696v1", "categories": ["cs.LG", "cs.CL"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22696v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23306", "title": "GATSim: Urban Mobility Simulation with Generative Agents", "authors": ["Qi Liu", "Can Li", "Wanjing Ma"], "summary": "Traditional agent-based urban mobility simulations rely on rigid rule-based\nsystems that fail to capture the complexity, adaptability, and behavioral\ndiversity characteristic of human travel decision-making. Recent advances in\nlarge language models and AI agent technology offer opportunities to create\nagents with reasoning capabilities, persistent memory, and adaptive learning\nmechanisms. We propose GATSim (Generative-Agent Transport Simulation), a novel\nframework that leverages these advances to create generative agents with rich\nbehavioral characteristics for urban mobility simulation. Unlike conventional\napproaches, GATSim agents possess diverse socioeconomic attributes, individual\nlifestyles, and evolving preferences that shape their mobility decisions\nthrough psychologically-informed memory systems, tool usage capabilities, and\nlifelong learning mechanisms. The main contributions of this study include: (1)\na comprehensive architecture combining an urban mobility foundation model with\nagent cognitive systems and transport simulation environment, (2) a fully\nfunctional prototype implementation, and (3) systematic validation\ndemonstrating that generative agents produce believable travel behaviors.\nThrough designed reflection processes, generative agents in this study can\ntransform specific travel experiences into generalized insights, enabling\nrealistic behavioral adaptation over time with specialized mechanisms for\nactivity planning and real-time reactive behaviors tailored to urban mobility\ncontexts. Experiments show that generative agents perform competitively with\nhuman annotators in mobility scenarios while naturally producing macroscopic\ntraffic evolution patterns. The code for the prototype system is shared at\nhttps://github.com/qiliuchn/gatsim.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23306v1", "categories": ["cs.AI"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.23306v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23437", "title": "From Large-scale Audio Tagging to Real-Time Explainable Emergency Vehicle Sirens Detection", "authors": ["Stefano Giacomelli", "Marco Giordano", "Claudia Rinaldi", "Fabio Graziosi"], "summary": "Accurate recognition of Emergency Vehicle (EV) sirens is critical for the\nintegration of intelligent transportation systems, smart city monitoring\nsystems, and autonomous driving technologies. Modern automatic solutions are\nlimited by the lack of large scale, curated datasets and by the computational\ndemands of state of the art sound event detection models. This work introduces\nE2PANNs (Efficient Emergency Pre trained Audio Neural Networks), a lightweight\nConvolutional Neural Network architecture derived from the PANNs framework,\nspecifically optimized for binary EV siren detection. Leveraging our dedicated\nsubset of AudioSet (AudioSet EV) we fine-tune and evaluate E2PANNs across\nmultiple reference datasets and test its viability on embedded hardware. The\nexperimental campaign includes ablation studies, cross-domain benchmarking, and\nreal-time inference deployment on edge device. Interpretability analyses\nexploiting Guided Backpropagation and ScoreCAM algorithms provide insights into\nthe model internal representations and validate its ability to capture distinct\nspectrotemporal patterns associated with different types of EV sirens. Real\ntime performance is assessed through frame wise and event based detection\nmetrics, as well as a detailed analysis of false positive activations. Results\ndemonstrate that E2PANNs establish a new state of the art in this research\ndomain, with high computational efficiency, and suitability for edge-based\naudio monitoring and safety-critical applications.", "comment": "pre-print (submitted to the IEEE/ACM Transactions on Audio, Speech,\n  and Language Processing)", "pdf_url": "http://arxiv.org/pdf/2506.23437v1", "categories": ["cs.SD", "cs.AI", "eess.AS", "68T07", "E.1; H.1; I.2; I.5; J.2; K.4; C.4"], "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.23437v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22808", "title": "MedEthicsQA: A Comprehensive Question Answering Benchmark for Medical Ethics Evaluation of LLMs", "authors": ["Jianhui Wei", "Zijie Meng", "Zikai Xiao", "Tianxiang Hu", "Yang Feng", "Zhijie Zhou", "Jian Wu", "Zuozhu Liu"], "summary": "While Medical Large Language Models (MedLLMs) have demonstrated remarkable\npotential in clinical tasks, their ethical safety remains insufficiently\nexplored. This paper introduces $\\textbf{MedEthicsQA}$, a comprehensive\nbenchmark comprising $\\textbf{5,623}$ multiple-choice questions and\n$\\textbf{5,351}$ open-ended questions for evaluation of medical ethics in LLMs.\nWe systematically establish a hierarchical taxonomy integrating global medical\nethical standards. The benchmark encompasses widely used medical datasets,\nauthoritative question banks, and scenarios derived from PubMed literature.\nRigorous quality control involving multi-stage filtering and multi-faceted\nexpert validation ensures the reliability of the dataset with a low error rate\n($2.72\\%$). Evaluation of state-of-the-art MedLLMs exhibit declined performance\nin answering medical ethics questions compared to their foundation\ncounterparts, elucidating the deficiencies of medical ethics alignment. The\ndataset, registered under CC BY-NC 4.0 license, is available at\nhttps://github.com/JianhuiWei7/MedEthicsQA.", "comment": "20 pages", "pdf_url": "http://arxiv.org/pdf/2506.22808v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.22808v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23748", "title": "Error analysis for a Finite Element Discretization of a radially symmetric harmonic map heat flow problem", "authors": ["Nam Anh Nguyen", "Arnold Reusken"], "summary": "We consider the harmonic map heat flow problem for a radially symmetric case.\nFor discretization of this problem we apply a $H^1$-conforming finite element\nmethod in space combined with a semi-implicit Euler time stepping. The\nsemi-implicit Euler method results in a linear problem in each time step. We\nrestrict to the regime of smooth solutions of the continuous problem and\npresent an error analysis of this discretization method. This results in\noptimal order discretization error bounds. Key ingredients of the analysis are\na discrete energy estimate, that mimics the energy dissipation of the\ncontinuous solution, and a convexity property that is essential for discrete\nstability and for control of the linearization error. We also present numerical\nresults that validate the theoretical ones.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23748v1", "categories": ["math.NA", "cs.NA"], "cate": "math.NA", "url": "http://arxiv.org/abs/2506.23748v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23049", "title": "AURA: Agent for Understanding, Reasoning, and Automated Tool Use in Voice-Driven Tasks", "authors": ["Leander Melroy Maben", "Gayathri Ganesh Lakshmy", "Srijith Radhakrishnan", "Siddhant Arora", "Shinji Watanabe"], "summary": "Despite advances in language and speech technologies, no open-source system\nenables full speech-to-speech, multi-turn dialogue with integrated tool use and\nagentic reasoning. We introduce AURA (Agent for Understanding, Reasoning, and\nAutomated Tool Use), the first open-source, speech-native assistant capable of\ncompleting complex, goal-driven tasks through dynamic tool invocation and\nmulti-turn conversation. AURA combines open-weight ASR, TTS, and LLMs in a\ncascaded pipeline and supports tools such as calendar booking, contact lookup,\nweb search, and email. Its modular design allows easy integration of new tools\nusing natural language prompts and action classes. On VoiceBench, AURA scores\n92.75% on OpenBookQA-outperforming all open-weight systems and nearing\nGPT-4o-and 4.39 on AlpacaEval, competitive with other open-weight systems.\nHuman evaluation shows 90% task success on complex, multi-turn speech tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23049v1", "categories": ["cs.AI", "cs.CL", "cs.SD", "eess.AS", "68T42, 68T50,", "I.2.7; I.2.11; H.5.5"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.23049v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23474", "title": "A Large-Scale Evolvable Dataset for Model Context Protocol Ecosystem and Security Analysis", "authors": ["Zhiwei Lin", "Bonan Ruan", "Jiahao Liu", "Weibo Zhao"], "summary": "The Model Context Protocol (MCP) has recently emerged as a standardized\ninterface for connecting language models with external tools and data. As the\necosystem rapidly expands, the lack of a structured, comprehensive view of\nexisting MCP artifacts presents challenges for research. To bridge this gap, we\nintroduce MCPCorpus, a large-scale dataset containing around 14K MCP servers\nand 300 MCP clients. Each artifact is annotated with 20+ normalized attributes\ncapturing its identity, interface configuration, GitHub activity, and metadata.\nMCPCorpus provides a reproducible snapshot of the real-world MCP ecosystem,\nenabling studies of adoption trends, ecosystem health, and implementation\ndiversity. To keep pace with the rapid evolution of the MCP ecosystem, we\nprovide utility tools for automated data synchronization, normalization, and\ninspection. Furthermore, to support efficient exploration and exploitation, we\nrelease a lightweight web-based search interface. MCPCorpus is publicly\navailable at: https://github.com/Snakinya/MCPCorpus.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23474v1", "categories": ["cs.CR"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.23474v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23316", "title": "InfGen: Scenario Generation as Next Token Group Prediction", "authors": ["Zhenghao Peng", "Yuxin Liu", "Bolei Zhou"], "summary": "Realistic and interactive traffic simulation is essential for training and\nevaluating autonomous driving systems. However, most existing data-driven\nsimulation methods rely on static initialization or log-replay data, limiting\ntheir ability to model dynamic, long-horizon scenarios with evolving agent\npopulations. We propose InfGen, a scenario generation framework that outputs\nagent states and trajectories in an autoregressive manner. InfGen represents\nthe entire scene as a sequence of tokens, including traffic light signals,\nagent states, and motion vectors, and uses a transformer model to simulate\ntraffic over time. This design enables InfGen to continuously insert new agents\ninto traffic, supporting infinite scene generation. Experiments demonstrate\nthat InfGen produces realistic, diverse, and adaptive traffic behaviors.\nFurthermore, reinforcement learning policies trained in InfGen-generated\nscenarios achieve superior robustness and generalization, validating its\nutility as a high-fidelity simulation environment for autonomous driving. More\ninformation is available at https://metadriverse.github.io/infgen/.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23316v1", "categories": ["cs.RO", "cs.CV"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.23316v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23640", "title": "Geminet: Learning the Duality-based Iterative Process for Lightweight Traffic Engineering in Changing Topologies", "authors": ["Ximeng Liu", "Shizhen Zhao", "Xinbing Wang"], "summary": "Recently, researchers have explored ML-based Traffic Engineering (TE),\nleveraging neural networks to solve TE problems traditionally addressed by\noptimization. However, existing ML-based TE schemes remain impractical: they\neither fail to handle topology changes or suffer from poor scalability due to\nexcessive computational and memory overhead. To overcome these limitations, we\npropose Geminet, a lightweight and scalable ML-based TE framework that can\nhandle changing topologies. Geminet is built upon two key insights: (i) a\nmethodology that decouples neural networks from topology by learning an\niterative gradient-descent-based adjustment process, as the update rule of\ngradient descent is topology-agnostic, relying only on a few gradient-related\nquantities; (ii) shifting optimization from path-level routing weights to\nedge-level dual variables, reducing memory consumption by leveraging the fact\nthat edges are far fewer than paths. Evaluations on WAN and data center\ndatasets show that Geminet significantly improves scalability. Its neural\nnetwork size is only 0.04% to 7% of existing schemes, while handling topology\nvariations as effectively as HARP, a state-of-the-art ML-based TE approach,\nwithout performance degradation. When trained on large-scale topologies,\nGeminet consumes under 10 GiB of memory, more than eight times less than the\n80-plus GiB required by HARP, while achieving 5.45 times faster convergence\nspeed, demonstrating its potential for large-scale deployment.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23640v1", "categories": ["cs.NI", "cs.LG"], "cate": "cs.NI", "url": "http://arxiv.org/abs/2506.23640v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23967", "title": "Green Metrics Tool: Measuring for fun and profit", "authors": ["Geerd-Dietger Hoffmann", "Verena Majuntke"], "summary": "The environmental impact of software is gaining increasing attention as the\ndemand for computational resources continues to rise. In order to optimize\nsoftware resource consumption and reduce carbon emissions, measuring and\nevaluating software is a first essential step. In this paper we discuss what\nmetrics are important for fact base decision making. We introduce the Green\nMetrics Tool (GMT), a novel framework for accurately measuring the resource\nconsumption of software. The tool provides a containerized, controlled, and\nreproducible life cycle-based approach, assessing the resource use of software\nduring key phases. Finally, we discuss GMT features like visualization,\ncomparability and rule- and LLM-based optimisations highlighting its potential\nto guide developers and researchers in reducing the environmental impact of\ntheir software.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23967v1", "categories": ["cs.SE", "cs.CY", "cs.ET"], "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.23967v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23457", "title": "Reducing Motion Sickness in Passengers of Autonomous Personal Mobility Vehicles by Presenting a Driving Path", "authors": ["Yuya Ide", "Hailong Liu", "Takahiro Wada"], "summary": "Autonomous personal mobility vehicles (APMVs) are small mobility devices\ndesigned for individual automated transportation in shared spaces. In such\nenvironments, frequent pedestrian avoidance maneuvers may cause rapid steering\nadjustments and passive postural responses from passengers, thereby increasing\nthe risk of motion sickness. This study investigated the effects of providing\npath information on 16 passengers' head movement behavior and motion sickness\nwhile riding an APMV. Through a controlled experiment comparing manual driving\n(MD), autonomous driving without path information (AD w/o path), and autonomous\ndriving with path information (AD w/ path), we found that providing path cues\nsignificantly reduced MISC scores and delayed the onset of motion sickness\nsymptoms. In addition, participants were more likely to proactively align their\nhead movements with the direction of vehicle rotation in both MD and AD w/ path\nconditions. Although a small correlation was observed between the delay in yaw\nrotation of the passenger's head relative to the vehicle and the occurrence of\nmotion sickness, the underlying physiological mechanism remains to be\nelucidated.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23457v1", "categories": ["cs.HC"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.23457v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22520", "title": "Exploring Artificial Intelligence Tutor Teammate Adaptability to Harness Discovery Curiosity and Promote Learning in the Context of Interactive Molecular Dynamics", "authors": ["Mustafa Demir", "Jacob Miratsky", "Jonathan Nguyen", "Chun Kit Chan", "Punya Mishra", "Abhishek Singharoy"], "summary": "This study examines the impact of an Artificial Intelligence tutor teammate\n(AI) on student curiosity-driven engagement and learning effectiveness during\nInteractive Molecular Dynamics (IMD) tasks on the Visual Molecular Dynamics\nplatform. It explores the role of the AI's curiosity-triggering and response\nbehaviors in stimulating and sustaining student curiosity, affecting the\nfrequency and complexity of student-initiated questions. The study further\nassesses how AI interventions shape student engagement, foster discovery\ncuriosity, and enhance team performance within the IMD learning environment.\nUsing a Wizard-of-Oz paradigm, a human experimenter dynamically adjusts the AI\ntutor teammate's behavior through a large language model. By employing a\nmixed-methods exploratory design, a total of 11 high school students\nparticipated in four IMD tasks that involved molecular visualization and\ncalculations, which increased in complexity over a 60-minute period. Team\nperformance was evaluated through real-time observation and recordings, whereas\nteam communication was measured by question complexity and AI's\ncuriosity-triggering and response behaviors. Cross Recurrence Quantification\nAnalysis (CRQA) metrics reflected structural alignment in coordination and were\nlinked to communication behaviors. High-performing teams exhibited superior\ntask completion, deeper understanding, and increased engagement. Advanced\nquestions were associated with AI curiosity-triggering, indicating heightened\nengagement and cognitive complexity. CRQA metrics highlighted dynamic\nsynchronization in student-AI interactions, emphasizing structured yet adaptive\nengagement to promote curiosity. These proof-of-concept findings suggest that\nthe AI's dual role as a teammate and educator indicates its capacity to provide\nadaptive feedback, sustaining engagement and epistemic curiosity.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22520v1", "categories": ["cs.HC", "cs.AI", "cs.CE", "cs.CY"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.22520v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.22589", "title": "LIGHT: Multi-Modal Text Linking on Historical Maps", "authors": ["Yijun Lin", "Rhett Olson", "Junhan Wu", "Yao-Yi Chiang", "Jerod Weinman"], "summary": "Text on historical maps provides valuable information for studies in history,\neconomics, geography, and other related fields. Unlike structured or\nsemi-structured documents, text on maps varies significantly in orientation,\nreading order, shape, and placement. Many modern methods can detect and\ntranscribe text regions, but they struggle to effectively ``link'' the\nrecognized text fragments, e.g., determining a multi-word place name. Existing\nlayout analysis methods model word relationships to improve text understanding\nin structured documents, but they primarily rely on linguistic features and\nneglect geometric information, which is essential for handling map text. To\naddress these challenges, we propose LIGHT, a novel multi-modal approach that\nintegrates linguistic, image, and geometric features for linking text on\nhistorical maps. In particular, LIGHT includes a geometry-aware embedding\nmodule that encodes the polygonal coordinates of text regions to capture\npolygon shapes and their relative spatial positions on an image. LIGHT unifies\nthis geometric information with the visual and linguistic token embeddings from\nLayoutLMv3, a pretrained layout analysis model. LIGHT uses the cross-modal\ninformation to predict the reading-order successor of each text instance\ndirectly with a bi-directional learning strategy that enhances sequence\nrobustness. Experimental results show that LIGHT outperforms existing methods\non the ICDAR 2024/2025 MapText Competition data, demonstrating the\neffectiveness of multi-modal learning for historical map text linking.", "comment": "Accepted at ICDAR2025", "pdf_url": "http://arxiv.org/pdf/2506.22589v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22589v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.23649", "title": "Reliability Assessment of Power System Based on the Dichotomy Method", "authors": ["Wenjie Wan", "Han Hu", "Feiyu Chen", "Xiaoyu Liu", "Kequan Zhao"], "summary": "With a sustainable increase in the scale of power system, the number of\nstates in the state space grows exponentially, and the reliability assessment\nof the power system faces enormous challenges. Traditional state-by-state\nassessment methods, such as state enumeration (SE) and Monte Carlo simulation\n(MCS) methods, have encountered performance bottlenecks in terms of efficiency\nand accuracy. In this paper, the Boolean lattice representation theory of the\nstate space was studied, and a dichotomy method was proposed to efficiently\npartition the state space into some disjoint sub-lattices with a relatively\nsmall number of optimal power flow (OPF) operations. Based on lattice\npartition, the reliability indices of the entire space can be calculated\nlattice-by-lattice. In addition, alone with the partitioning procedure, the\ncalculated loss of load probability (LOLP) monotonically increases and rapidly\ntends to the analytic value with the designated error bound. Moreover, we\ndesigned a customized Monte Carlo sampling method in lattices of interest to\ncompute expected energy not supply (EENS). The experiments are conducted on the\nRBTS and RTS-79 systems. The results show that the proposed method achieves the\nanalytic LOLP of the RBTS system after five hundreds of OPF operations, which\nis about hundreds of times faster than traditional methods, and the designed\nMonte Carlo sampling method converged after thousands of OPF operations on test\nsystems.", "comment": "10pages, 8figures", "pdf_url": "http://arxiv.org/pdf/2506.23649v1", "categories": ["eess.SY", "cs.SY"], "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.23649v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22549", "title": "50 GHz Piezoelectric Acoustic Filter", "authors": ["Omar Barrera", "Jack Kramer", "Lezli Matto", "Vakhtang Chulukhadze", "Sinwoo Cho", "Michael Liao", "Mark S. Goorsky", "Ruochen Lu"], "summary": "This paper presents significant frequency scaling of acoustic filter\ntechnology to 50 GHz. This achievement is enabled by the P3F LiNbO3 multilayer\nstack, in which piezoelectric thin-films of alternating orientations are\ntransferred in sequence, thereby allowing efficient exploitation of high-order\nmodes with high quality factor (Q) and coupling coefficient (k2) in a thicker\npiezoelectric stack. The demonstrated filter is comprised of twelfth-order\nsymmetric (S12) mode lateral-field-excited bulk acoustic wave resonators\n(XBARs), built on a 4-layer periodically poled piezoelectric (P3F) 128 Y-cut\nlithium niobate (LiNbO3) stack. The filter exhibits 3.3 dB insertion loss (IL)\nand a fractional bandwidth (FBW) of 2.9%. The miniature design, with a\nfootprint of 0.36 mm2, makes it promising for future wireless front-end\napplications. These results represent the highest frequency acoustic filters\nreported to date, setting a new benchmark in piezoelectric filter technology.\nUpon further development, the platform could enable filters further into the\nFR2 range, essential for next-generation communication systems.", "comment": "8 pages, 10 Figures", "pdf_url": "http://arxiv.org/pdf/2506.22549v1", "categories": ["eess.SP"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.22549v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.23506", "title": "Artificial Intelligence-assisted Pixel-level Lung (APL) Scoring for Fast and Accurate Quantification in Ultra-short Echo-time MRI", "authors": ["Bowen Xin", "Rohan Hickey", "Tamara Blake", "Jin Jin", "Claire E Wainwright", "Thomas Benkert", "Alto Stemmer", "Peter Sly", "David Coman", "Jason Dowling"], "summary": "Lung magnetic resonance imaging (MRI) with ultrashort echo-time (UTE)\nrepresents a recent breakthrough in lung structure imaging, providing image\nresolution and quality comparable to computed tomography (CT). Due to the\nabsence of ionising radiation, MRI is often preferred over CT in paediatric\ndiseases such as cystic fibrosis (CF), one of the most common genetic disorders\nin Caucasians. To assess structural lung damage in CF imaging, CT scoring\nsystems provide valuable quantitative insights for disease diagnosis and\nprogression. However, few quantitative scoring systems are available in\nstructural lung MRI (e.g., UTE-MRI). To provide fast and accurate\nquantification in lung MRI, we investigated the feasibility of novel Artificial\nintelligence-assisted Pixel-level Lung (APL) scoring for CF. APL scoring\nconsists of 5 stages, including 1) image loading, 2) AI lung segmentation, 3)\nlung-bounded slice sampling, 4) pixel-level annotation, and 5) quantification\nand reporting. The results shows that our APL scoring took 8.2 minutes per\nsubject, which was more than twice as fast as the previous grid-level scoring.\nAdditionally, our pixel-level scoring was statistically more accurate\n(p=0.021), while strongly correlating with grid-level scoring (R=0.973,\np=5.85e-9). This tool has great potential to streamline the workflow of UTE\nlung MRI in clinical settings, and be extended to other structural lung MRI\nsequences (e.g., BLADE MRI), and for other lung diseases (e.g.,\nbronchopulmonary dysplasia).", "comment": "Oral presentation in ISMRM2025", "pdf_url": "http://arxiv.org/pdf/2506.23506v1", "categories": ["eess.IV", "cs.AI", "cs.CV", "physics.med-ph"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.23506v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23333", "title": "Moving Matter: Using a Single, Simple Robot to Reconfigure a Connected Set of Building Blocks", "authors": ["Javier Garcia", "Jonas Friemel", "Ramin Kosfeld", "Michael Yannuzzi", "Peter Kramer", "Christian Rieck", "Christian Scheffer", "Arne Schmidt", "Harm Kube", "Dan Biediger", "S√°ndor P. Fekete", "Aaron T. Becker"], "summary": "We implement and evaluate different methods for the reconfiguration of a\nconnected arrangement of tiles into a desired target shape, using a single\nactive robot that can move along the tile structure. This robot can pick up,\ncarry, or drop off one tile at a time, but it must maintain a single connected\nconfiguration at all times.\n  Becker et al. (CCCG 2025) recently proposed an algorithm that uses histograms\nas canonical intermediate configurations, guaranteeing performance within a\nconstant factor of the optimal solution if the start and target configuration\nare well-separated. We implement and evaluate this algorithm, both in a\nsimulated and practical setting, using an inchworm type robot to compare it\nwith two existing heuristic algorithms.", "comment": "8 pages, 12 figures. To appear in the proceedings of the 2025 IEEE\n  21st International Conference on Automation Science and Engineering (CASE\n  2025)", "pdf_url": "http://arxiv.org/pdf/2506.23333v1", "categories": ["cs.RO", "cs.CG", "cs.DS"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.23333v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.22708", "title": "FairMarket-RL: LLM-Guided Fairness Shaping for Multi-Agent Reinforcement Learning in Peer-to-Peer Markets", "authors": ["Shrenik Jadhav", "Birva Sevak", "Srijita Das", "Akhtar Hussain", "Wencong Su", "Van-Hai Bui"], "summary": "Peer-to-peer (P2P) trading is increasingly recognized as a key mechanism for\ndecentralized market regulation, yet existing approaches often lack robust\nframeworks to ensure fairness. This paper presents FairMarket-RL, a novel\nhybrid framework that combines Large Language Models (LLMs) with Reinforcement\nLearning (RL) to enable fairness-aware trading agents. In a simulated P2P\nmicrogrid with multiple sellers and buyers, the LLM acts as a real-time\nfairness critic, evaluating each trading episode using two metrics:\nFairness-To-Buyer (FTB) and Fairness-Between-Sellers (FBS). These fairness\nscores are integrated into agent rewards through scheduled\n{\\lambda}-coefficients, forming an adaptive LLM-guided reward shaping loop that\nreplaces brittle, rule-based fairness constraints. Agents are trained using\nIndependent Proximal Policy Optimization (IPPO) and achieve equitable outcomes,\nfulfilling over 90% of buyer demand, maintaining fair seller margins, and\nconsistently reaching FTB and FBS scores above 0.80. The training process\ndemonstrates that fairness feedback improves convergence, reduces buyer\nshortfalls, and narrows profit disparities between sellers. With its\nlanguage-based critic, the framework scales naturally, and its extension to a\nlarge power distribution system with household prosumers illustrates its\npractical applicability. FairMarket-RL thus offers a scalable, equity-driven\nsolution for autonomous trading in decentralized energy systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22708v1", "categories": ["cs.LG", "cs.SY", "econ.GN", "eess.SY", "q-fin.EC"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22708v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23464", "title": "The Confidence Paradox: Can LLM Know When It's Wrong", "authors": ["Sahil Tripathi", "Md Tabrez Nafis", "Imran Hussain", "Jiechao Gao"], "summary": "Document Visual Question Answering (DocVQA) systems are increasingly deployed\nin real world applications, yet they remain ethically opaque-often producing\noverconfident answers to ambiguous questions or failing to communicate\nuncertainty in a trustworthy manner. This misalignment between model confidence\nand actual knowledge poses significant risks, particularly in domains requiring\nethical accountability. Existing approaches such as LayoutLMv3, UDOP, and DONUT\nhave advanced SOTA performance by focusing on architectural sophistication and\naccuracy; however, they fall short in ethical responsiveness.\n  To address these limitations, we introduce HonestVQA, a self-supervised\nhonesty calibration framework for ethically aligned DocVQA. Our model-agnostic\nmethod quantifies uncertainty to identify knowledge gaps, aligns model\nconfidence with actual correctness using weighted loss functions, and enforces\nethical response behavior via contrastive learning. We further introduce two\nprincipled evaluation metrics--Honesty Score (H-Score) and Ethical Confidence\nIndex (ECI)--to benchmark alignment between confidence, accuracy, and ethical\ncommunication. Empirically, HonestVQA improves DocVQA accuracy by up to 4.3%\nand F1 by 4.3% across SpDocVQA, InfographicsVQA, and SROIE datasets. It reduces\noverconfidence, lowering H-Score and ECI by 0.072 and 0.078, respectively. In\ncross domain evaluation, it achieves up to 78.9% accuracy and 76.1% F1-score,\ndemonstrating strong generalization. Ablation shows a 3.8% drop in accuracy\nwithout alignment or contrastive loss.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23464v1", "categories": ["cs.AI"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.23464v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23552", "title": "JAM-Flow: Joint Audio-Motion Synthesis with Flow Matching", "authors": ["Mingi Kwon", "Joonghyuk Shin", "Jaeseok Jung", "Jaesik Park", "Youngjung Uh"], "summary": "The intrinsic link between facial motion and speech is often overlooked in\ngenerative modeling, where talking head synthesis and text-to-speech (TTS) are\ntypically addressed as separate tasks. This paper introduces JAM-Flow, a\nunified framework to simultaneously synthesize and condition on both facial\nmotion and speech. Our approach leverages flow matching and a novel Multi-Modal\nDiffusion Transformer (MM-DiT) architecture, integrating specialized Motion-DiT\nand Audio-DiT modules. These are coupled via selective joint attention layers\nand incorporate key architectural choices, such as temporally aligned\npositional embeddings and localized joint attention masking, to enable\neffective cross-modal interaction while preserving modality-specific strengths.\nTrained with an inpainting-style objective, JAM-Flow supports a wide array of\nconditioning inputs-including text, reference audio, and reference\nmotion-facilitating tasks such as synchronized talking head generation from\ntext, audio-driven animation, and much more, within a single, coherent model.\nJAM-Flow significantly advances multi-modal generative modeling by providing a\npractical solution for holistic audio-visual synthesis. project page:\nhttps://joonghyuk.com/jamflow-web", "comment": "project page: https://joonghyuk.com/jamflow-web Under review.\n  Preprint published on arXiv", "pdf_url": "http://arxiv.org/pdf/2506.23552v1", "categories": ["cs.CV", "cs.SD", "eess.AS"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23552v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22813", "title": "Selecting and Merging: Towards Adaptable and Scalable Named Entity Recognition with Large Language Models", "authors": ["Zhuojun Ding", "Wei Wei", "Chenghao Fan"], "summary": "Supervised fine-tuning (SFT) is widely used to align large language models\n(LLMs) with information extraction (IE) tasks, such as named entity recognition\n(NER). However, annotating such fine-grained labels and training\ndomain-specific models is costly. Existing works typically train a unified\nmodel across multiple domains, but such approaches lack adaptation and\nscalability since not all training data benefits target domains and scaling\ntrained models remains challenging. We propose the SaM framework, which\ndynamically Selects and Merges expert models at inference time. Specifically,\nfor a target domain, we select domain-specific experts pre-trained on existing\ndomains based on (i) domain similarity to the target domain and (ii)\nperformance on sampled instances, respectively. The experts are then merged to\ncreate task-specific models optimized for the target domain. By dynamically\nmerging experts beneficial to target domains, we improve generalization across\nvarious domains without extra training. Additionally, experts can be added or\nremoved conveniently, leading to great scalability. Extensive experiments on\nmultiple benchmarks demonstrate our framework's effectiveness, which\noutperforms the unified model by an average of 10%. We further provide insights\ninto potential improvements, practical experience, and extensions of our\nframework.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22813v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.22813v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23892", "title": "Dimension and model reduction approaches for linear Bayesian inverse problems with rank-deficient prior covariances", "authors": ["Josie K√∂nig", "Elizabeth Qian", "Melina A. Freitag"], "summary": "Bayesian inverse problems use observed data to update a prior probability\ndistribution for an unknown state or parameter of a scientific system to a\nposterior distribution conditioned on the data. In many applications, the\nunknown parameter is high-dimensional, making computation of the posterior\nexpensive due to the need to sample in a high-dimensional space and the need to\nevaluate an expensive high-dimensional forward model relating the unknown\nparameter to the data. However, inverse problems often exhibit low-dimensional\nstructure due to the fact that the available data are only informative in a\nlow-dimensional subspace of the parameter space. Dimension reduction approaches\nexploit this structure by restricting inference to the low-dimensional subspace\ninformed by the data, which can be sampled more efficiently. Further\ncomputational cost reductions can be achieved by replacing expensive\nhigh-dimensional forward models with cheaper lower-dimensional reduced models.\nIn this work, we propose new dimension and model reduction approaches for\nlinear Bayesian inverse problems with rank-deficient prior covariances, which\narise in many practical inference settings. The dimension reduction approach is\napplicable to general linear Bayesian inverse problems whereas the model\nreduction approaches are specific to the problem of inferring the initial\ncondition of a linear dynamical system. We provide theoretical approximation\nguarantees as well as numerical experiments demonstrating the accuracy and\nefficiency of the proposed approaches.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23892v1", "categories": ["math.NA", "cs.NA", "cs.SY", "eess.SY"], "cate": "math.NA", "url": "http://arxiv.org/abs/2506.23892v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23371", "title": "Investigating an Overfitting and Degeneration Phenomenon in Self-Supervised Multi-Pitch Estimation", "authors": ["Frank Cwitkowitz", "Zhiyao Duan"], "summary": "Multi-Pitch Estimation (MPE) continues to be a sought after capability of\nMusic Information Retrieval (MIR) systems, and is critical for many\napplications and downstream tasks involving pitch, including music\ntranscription. However, existing methods are largely based on supervised\nlearning, and there are significant challenges in collecting annotated data for\nthe task. Recently, self-supervised techniques exploiting intrinsic properties\nof pitch and harmonic signals have shown promise for both monophonic and\npolyphonic pitch estimation, but these still remain inferior to supervised\nmethods. In this work, we extend the classic supervised MPE paradigm by\nincorporating several self-supervised objectives based on pitch-invariant and\npitch-equivariant properties. This joint training results in a substantial\nimprovement under closed training conditions, which naturally suggests that\napplying the same objectives to a broader collection of data will yield further\nimprovements. However, in doing so we uncover a phenomenon whereby our model\nsimultaneously overfits to the supervised data while degenerating on data used\nfor self-supervision only. We demonstrate and investigate this and offer our\ninsights on the underlying problem.", "comment": "Accepted to ISMIR 2025", "pdf_url": "http://arxiv.org/pdf/2506.23371v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "cate": "eess.AS", "url": "http://arxiv.org/abs/2506.23371v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23583", "title": "Detect \\& Score: Privacy-Preserving Misbehaviour Detection and Contribution Evaluation in Federated Learning", "authors": ["Marvin Xhemrishi", "Alexandre Graell i Amat", "Bal√°zs Pej√≥"], "summary": "Federated learning with secure aggregation enables private and collaborative\nlearning from decentralised data without leaking sensitive client information.\nHowever, secure aggregation also complicates the detection of malicious client\nbehaviour and the evaluation of individual client contributions to the\nlearning. To address these challenges, QI (Pejo et al.) and FedGT (Xhemrishi et\nal.) were proposed for contribution evaluation (CE) and misbehaviour detection\n(MD), respectively. QI, however, lacks adequate MD accuracy due to its reliance\non the random selection of clients in each training round, while FedGT lacks\nthe CE ability. In this work, we combine the strengths of QI and FedGT to\nachieve both robust MD and accurate CE. Our experiments demonstrate superior\nperformance compared to using either method independently.", "comment": "The shorter version is accepted at FL-AsiaCCS 25", "pdf_url": "http://arxiv.org/pdf/2506.23583v1", "categories": ["cs.CR", "cs.DC", "cs.LG"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.23583v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23326", "title": "Simplifying Data-Driven Modeling of the Volume-Flow-Pressure Relationship in Hydraulic Soft Robotic Actuators", "authors": ["Sang-Yoep Lee", "Leonardo Zamora Yanez", "Jacob Rogatinsky", "Vi T. Vo", "Tanvi Shingade", "Tommaso Ranzani"], "summary": "Soft robotic systems are known for their flexibility and adaptability, but\ntraditional physics-based models struggle to capture their complex, nonlinear\nbehaviors. This study explores a data-driven approach to modeling the\nvolume-flow-pressure relationship in hydraulic soft actuators, focusing on\nlow-complexity models with high accuracy. We perform regression analysis on a\nstacked balloon actuator system using exponential, polynomial, and neural\nnetwork models with or without autoregressive inputs. The results demonstrate\nthat simpler models, particularly multivariate polynomials, effectively predict\npressure dynamics with fewer parameters. This research offers a practical\nsolution for real-time soft robotics applications, balancing model complexity\nand computational efficiency. Moreover, the approach may benefit various\ntechniques that require explicit analytical models.", "comment": "This work has been submitted to the IEEE for possible publication", "pdf_url": "http://arxiv.org/pdf/2506.23326v1", "categories": ["cs.RO"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.23326v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23740", "title": "Campus5G: A Campus Scale Private 5G Open RAN Testbed", "authors": ["Andrew E. Ferguson", "Ujjwal Pawar", "Tianxin Wang", "Mahesh K. Marina"], "summary": "Mobile networks are embracing disaggregation, reflected by the industry trend\ntowards Open RAN. Private 5G networks are viewed as particularly suitable\ncontenders as early adopters of Open RAN, owing to their setting, high degree\nof control, and opportunity for innovation they present. Motivated by this, we\nhave recently deployed Campus5G, the first of its kind campus-wide,\nO-RAN-compliant private 5G testbed across the central campus of the University\nof Edinburgh. We present in detail our process developing the testbed, from\nplanning, to architecting, to deployment, and measuring the testbed\nperformance. We then discuss the lessons learned from building the testbed, and\nhighlight some research opportunities that emerged from our deployment\nexperience.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23740v1", "categories": ["cs.NI", "C.2.1"], "cate": "cs.NI", "url": "http://arxiv.org/abs/2506.23740v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23995", "title": "STCLocker: Deadlock Avoidance Testing for Autonomous Driving Systems", "authors": ["Mingfei Cheng", "Renzhi Wang", "Xiaofei Xie", "Yuan Zhou", "Lei Ma"], "summary": "Autonomous Driving System (ADS) testing is essential to ensure the safety and\nreliability of autonomous vehicles (AVs) before deployment. However, existing\ntechniques primarily focus on evaluating ADS functionalities in single-AV\nsettings. As ADSs are increasingly deployed in multi-AV traffic, it becomes\ncrucial to assess their cooperative performance, particularly regarding\ndeadlocks, a fundamental coordination failure in which multiple AVs enter a\ncircular waiting state indefinitely, resulting in motion planning failures.\nDespite its importance, the cooperative capability of ADSs to prevent deadlocks\nremains insufficiently underexplored. To address this gap, we propose the first\ndedicated Spatio-Temporal Conflict-Guided Deadlock Avoidance Testing technique,\nSTCLocker, for generating DeadLock Scenarios (DLSs), where a group of AVs\ncontrolled by the ADS under test are in a circular wait state. STCLocker\nconsists of three key components: Deadlock Oracle, Conflict Feedback, and\nConflict-aware Scenario Generation. Deadlock Oracle provides a reliable\nblack-box mechanism for detecting deadlock cycles among multiple AVs within a\ngiven scenario. Conflict Feedback and Conflict-aware Scenario Generation\ncollaborate to actively guide AVs into simultaneous competition over spatial\nconflict resources (i.e., shared passing regions) and temporal competitive\nbehaviors (i.e., reaching the conflict region at the same time), thereby\nincreasing the effectiveness of generating conflict-prone deadlocks. We\nevaluate STCLocker on two types of ADSs: Roach, an end-to-end ADS, and OpenCDA,\na module-based ADS supporting cooperative communication. Experimental results\nshow that, on average, STCLocker generates more DLS than the best-performing\nbaseline.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23995v1", "categories": ["cs.SE", "cs.AI", "cs.RO"], "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.23995v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23458", "title": "Neuro-Informed Joint Learning Enhances Cognitive Workload Decoding in Portable BCIs", "authors": ["Xiaoxiao Yang", "Chan Feng", "Jiancheng Chen"], "summary": "Portable and wearable consumer-grade electroencephalography (EEG) devices,\nlike Muse headbands, offer unprecedented mobility for daily brain-computer\ninterface (BCI) applications, including cognitive load detection. However, the\nexacerbated non-stationarity in portable EEG signals constrains data fidelity\nand decoding accuracy, creating a fundamental trade-off between portability and\nperformance. To mitigate such limitation, we propose MuseCogNet (Muse-based\nCognitive Network), a unified joint learning framework integrating\nself-supervised and supervised training paradigms. In particular, we introduce\nan EEG-grounded self-supervised reconstruction loss based on average pooling to\ncapture robust neurophysiological patterns, while cross-entropy loss refines\ntask-specific cognitive discriminants. This joint learning framework resembles\nthe bottom-up and top-down attention in humans, enabling MuseCogNet to\nsignificantly outperform state-of-the-art methods on a publicly available Muse\ndataset and establish an implementable pathway for neurocognitive monitoring in\necological settings.", "comment": "2 pages short paper", "pdf_url": "http://arxiv.org/pdf/2506.23458v1", "categories": ["cs.HC", "cs.LG"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.23458v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22674", "title": "Do Electric Vehicles Induce More Motion Sickness Than Fuel Vehicles? A Survey Study in China", "authors": ["Weiyin Xie", "Chunxi Huang", "Jiyao Wang", "Dengbo He"], "summary": "Electric vehicles (EVs) are a promising alternative to fuel vehicles (FVs),\ngiven some unique characteristics of EVs, for example, the low air pollution\nand maintenance cost. However, the increasing prevalence of EVs is accompanied\nby widespread complaints regarding the high likelihood of motion sickness (MS)\ninduction, especially when compared to FVs, which has become one of the major\nobstacles to the acceptance and popularity of EVs. Despite the prevalence of\nsuch complaints online and among EV users, the association between vehicle type\n(i.e., EV versus FV) and MS prevalence and severity has not been quantified.\nThus, this study aims to investigate the existence of EV-induced MS and explore\nthe potential factors leading to it. A survey study was conducted to collect\npassengers' MS experience in EVs and FVs in the past one year. In total, 639\nvalid responses were collected from mainland China. The results show that FVs\nwere associated with a higher frequency of MS, while EVs were found to induce\nmore severe MS symptoms. Further, we found that passengers' MS severity was\nassociated with individual differences (i.e., age, gender, sleep habits,\nsusceptibility to motion-induced MS), in-vehicle activities (i.e., chatting\nwith others and watching in-vehicle displays), and road conditions (i.e.,\ncongestion and slope), while the MS frequency was associated with the vehicle\nownership and riding frequency. The results from this study can guide the\ndirections of future empirical studies that aim to quantify the inducers of MS\nin EVs and FVs, as well as the optimization of EVs to reduce MS.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22674v1", "categories": ["cs.HC", "cs.CY", "stat.AP"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.22674v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22591", "title": "BrainMT: A Hybrid Mamba-Transformer Architecture for Modeling Long-Range Dependencies in Functional MRI Data", "authors": ["Arunkumar Kannan", "Martin A. Lindquist", "Brian Caffo"], "summary": "Recent advances in deep learning have made it possible to predict phenotypic\nmeasures directly from functional magnetic resonance imaging (fMRI) brain\nvolumes, sparking significant interest in the neuroimaging community. However,\nexisting approaches, primarily based on convolutional neural networks or\ntransformer architectures, often struggle to model the complex relationships\ninherent in fMRI data, limited by their inability to capture long-range spatial\nand temporal dependencies. To overcome these shortcomings, we introduce\nBrainMT, a novel hybrid framework designed to efficiently learn and integrate\nlong-range spatiotemporal attributes in fMRI data. Our framework operates in\ntwo stages: (1) a bidirectional Mamba block with a temporal-first scanning\nmechanism to capture global temporal interactions in a computationally\nefficient manner; and (2) a transformer block leveraging self-attention to\nmodel global spatial relationships across the deep features processed by the\nMamba block. Extensive experiments on two large-scale public datasets,\nUKBioBank and the Human Connectome Project, demonstrate that BrainMT achieves\nstate-of-the-art performance on both classification (sex prediction) and\nregression (cognitive intelligence prediction) tasks, outperforming existing\nmethods by a significant margin. Our code and implementation details will be\nmade publicly available at this\nhttps://github.com/arunkumar-kannan/BrainMT-fMRI", "comment": "Accepted at MICCAI 2025", "pdf_url": "http://arxiv.org/pdf/2506.22591v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22591v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.23716", "title": "A Data-Ensemble-Based Approach for Sample-Efficient LQ Control of Linear Time-Varying Systems", "authors": ["Sahel Vahedi Noori", "Maryam Babazadeh"], "summary": "This paper presents a sample-efficient, data-driven control framework for\nfinite-horizon linear quadratic (LQ) control of linear time-varying (LTV)\nsystems. In contrast to the time-invariant case, the time-varying LQ problem\ninvolves a differential Riccati equation (DRE) with time-dependent parameters\nand terminal boundary constraints. We formulate the LQ problem as a nonconvex\noptimization problem and conduct a rigorous analysis of its dual structure. By\nexploiting the inherent convexity of the dual problem and analyzing the KKT\nconditions, we derive an explicit relationship between the optimal dual\nsolution and the parameters of the associated Q-function in time-varying case.\nThis theoretical insight supports the development of a novel, sample-efficient,\nnon-iterative semidefinite programming (SDP) algorithm that directly computes\nthe optimal sequence of feedback gains from an ensemble of input-state data\nsequences without model identification. The resulting convex, data-dependent\nframework provides global optimality guarantees for completely unknown LTV\nsystems. As a special case, the method also applies to finite-horizon LQ\ncontrol of linear time-invariant (LTI) systems. In this setting, a single\ninput-state trajectory suffices to identify the optimal LQ feedback policy,\nimproving significantly over existing Q-learning approaches for finite horizon\nLTI systems that typically require data from multiple episodes. The approach\nprovides a new optimization-based perspective on Q-learning in time-varying\nsettings and contributes to the broader understanding of data-driven control in\nnon-stationary environments. Simulation results show that, compared to recent\nmethods, the proposed approach achieves superior optimality and sample\nefficiency on LTV systems, and indicates potential for stabilizing and optimal\ncontrol of nonlinear systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23716v1", "categories": ["eess.SY", "cs.SY"], "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.23716v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22796", "title": "Channel Knowledge Map-assisted Dual-domain Tracking and Predictive Beamforming for High-Mobility Wireless Networks", "authors": ["Ruolin Du", "Zhiqiang Wei", "Zai Yang", "Lei Yang", "Yong Zeng", "Derrick Wing Kwan Ng", "Jinhong Yuan"], "summary": "This paper introduces a novel channel knowledge map (CKM)-assisted\ndual-domain tracking and predictive beamforming scheme for high-mobility\nwireless networks. The central premise is that the CKM integrates both the\ncoordinate and beam domains, thereby enabling tracking in one domain via\ntreating the other domain's input as priors or measurements. In the coordinate\ndomain (C-Domain), an extended Kalman filter (EKF) is employed to predict and\ntrack the state (i.e., location and velocity) of a moving communication\nreceiver across time slots under both line-of-sight (LoS)-present and\nLoS-absent conditions, where the CKM provides a prior mapping from multipath\nchannel parameters to potential target locations. In the beam domain\n(B-Domain), the updated location of the receiver is fed back to CKM to offer a\npriori information of angle of arrival (AoA) variations, which are incorporated\nto establish beam transition models for effective beam tracking, depending on\nthe angular variation situation of each path. Then, we analyze the Cram\\'er-Rao\nBound (CRB) for AoA estimation for each path in the considered system and\npropose a jointly predictive beamforming and power allocation design to\nminimize AoA estimation errors, directly enhancing multipath beam tracking\naccuracy and indirectly improving target tracking performance. Simulation\nresults demonstrate that the proposed scheme achieves significant improvements\nin both target and beam tracking performance compared to the state-of-the-art\napproaches, particularly in AoA tracking of non-line-of-sight (NLoS) paths,\nhighlighting the potential gain of CKM in facilitating both target and beam\ntracking in high-mobility communications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22796v1", "categories": ["eess.SP"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.22796v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23537", "title": "AFUNet: Cross-Iterative Alignment-Fusion Synergy for HDR Reconstruction via Deep Unfolding Paradigm", "authors": ["Xinyue Li", "Zhangkai Ni", "Wenhan Yang"], "summary": "Existing learning-based methods effectively reconstruct HDR images from\nmulti-exposure LDR inputs with extended dynamic range and improved detail, but\nthey rely more on empirical design rather than theoretical foundation, which\ncan impact their reliability. To address these limitations, we propose the\ncross-iterative Alignment and Fusion deep Unfolding Network (AFUNet), where HDR\nreconstruction is systematically decoupled into two interleaved subtasks --\nalignment and fusion -- optimized through alternating refinement, achieving\nsynergy between the two subtasks to enhance the overall performance. Our method\nformulates multi-exposure HDR reconstruction from a Maximum A Posteriori (MAP)\nestimation perspective, explicitly incorporating spatial correspondence priors\nacross LDR images and naturally bridging the alignment and fusion subproblems\nthrough joint constraints. Building on the mathematical foundation, we\nreimagine traditional iterative optimization through unfolding -- transforming\nthe conventional solution process into an end-to-end trainable AFUNet with\ncarefully designed modules that work progressively. Specifically, each\niteration of AFUNet incorporates an Alignment-Fusion Module (AFM) that\nalternates between a Spatial Alignment Module (SAM) for alignment and a Channel\nFusion Module (CFM) for adaptive feature fusion, progressively bridging\nmisaligned content and exposure discrepancies. Extensive qualitative and\nquantitative evaluations demonstrate AFUNet's superior performance,\nconsistently surpassing state-of-the-art methods. Our code is available at:\nhttps://github.com/eezkni/AFUNet", "comment": "Accepted to International Conference on Computer Vision (ICCV) 2025", "pdf_url": "http://arxiv.org/pdf/2506.23537v1", "categories": ["eess.IV", "cs.CV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.23537v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23456", "title": "Sampling and Identity-Testing Without Approximate Tensorization of Entropy", "authors": ["William Gay", "William He", "Nicholas Kocurek", "Ryan O'Donnell"], "summary": "Certain tasks in high-dimensional statistics become easier when the\nunderlying distribution satisfies a local-to-global property called approximate\ntensorization of entropy (ATE). For example, the Glauber dynamics Markov chain\nof an ATE distribution mixes fast and can produce approximate samples in a\nsmall amount of time, since such a distribution satisfies a modified\nlog-Sobolev inequality. Moreover, identity-testing for an ATE distribution\nrequires few samples if the tester is given coordinate conditional access to\nthe unknown distribution, as shown by Blanca, Chen, \\v{S}tefankovi\\v{c}, and\nVigoda (COLT 2023).\n  A natural class of distributions that do not satisfy ATE consists of mixtures\nof (few) distributions that do satisfy ATE. We study the complexity of\nidentity-testing and sampling for these distributions. Our main results are the\nfollowing:\n  1. We show fast mixing of Glauber dynamics from a data-based initialization,\nwith optimal sample complexity, for mixtures of distributions satisfying\nmodified log-Sobolev inequalities. This extends work of Huang, Koehler, Lee,\nMohanty, Rajaraman, Vuong, and Wu (STOC 2025, COLT 2025) for mixtures of\ndistributions satisfying Poincar\\'e inequalities.\n  2. Answering an open question posed by Blanca et al., we give efficient\nidentity-testers for mixtures of ATE distributions in the\ncoordinate-conditional sampling access model. We also give some simplifications\nand improvements to the original algorithm of Blanca et al.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23456v1", "categories": ["math.ST", "cs.DS", "cs.LG", "stat.ML", "stat.TH"], "cate": "math.ST", "url": "http://arxiv.org/abs/2506.23456v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22712", "title": "Generalized Linear Mode Connectivity for Transformers", "authors": ["Alexander Theus", "Alessandro Cabodi", "Sotiris Anagnostidis", "Antonio Orvieto", "Sidak Pal Singh", "Valentina Boeva"], "summary": "Understanding the geometry of neural network loss landscapes is a central\nquestion in deep learning, with implications for generalization and\noptimization. A striking phenomenon is linear mode connectivity (LMC), where\nindependently trained models can be connected by low- or zero-loss paths,\ndespite appearing to lie in separate loss basins. However, this is often\nobscured by symmetries in parameter space -- such as neuron permutations --\nwhich make functionally equivalent models appear dissimilar. Prior work has\npredominantly focused on neuron re-ordering through permutations, but such\napproaches are limited in scope and fail to capture the richer symmetries\nexhibited by modern architectures such as Transformers. In this work, we\nintroduce a unified framework that captures four symmetry classes:\npermutations, semi-permutations, orthogonal transformations, and general\ninvertible maps -- broadening the set of valid reparameterizations and\nsubsuming many previous approaches as special cases. Crucially, this\ngeneralization enables, for the first time, the discovery of low- and\nzero-barrier linear interpolation paths between independently trained Vision\nTransformers and GPT-2 models. These results reveal deeper structure in the\nloss landscape and underscore the importance of symmetry-aware analysis for\nunderstanding model space geometry.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22712v1", "categories": ["cs.LG", "stat.ML"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22712v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23503", "title": "Data Augmentation for Cognitive Behavioral Therapy: Leveraging ERNIE Language Models using Artificial Intelligence", "authors": ["Bosubabu Sambana", "Kondreddygari Archana", "Suram Indhra Sena Reddy", "Shaik Meethaigar Jameer Basha", "Shaik Karishma"], "summary": "Cognitive Behavioral Therapy (CBT) is a proven approach for addressing the\nirrational thought patterns associated with mental health disorders, but its\neffectiveness relies on accurately identifying cognitive pathways to provide\ntargeted treatment. In today's digital age, individuals often express negative\nemotions on social media, where they may reveal cognitive distortions, and in\nsevere cases, exhibit suicidal tendencies. However, there is a significant gap\nin methodologies designed to analyze these cognitive pathways, which could be\ncritical for psychotherapists aiming to deliver timely and effective\ninterventions in online environments. Cognitive Behavioral Therapy (CBT)\nframework leveraging acceptance, commitment and data augmentation to categorize\nand address both textual and visual content as positive or negative.\nSpecifically, the system employs BERT, RoBERTa for Sentiment Analysis and T5,\nPEGASUS for Text Summarization, mT5 for Text Translation in Multiple Languages\nfocusing on detecting negative emotions and cognitive distortions within social\nmedia data. While existing models are primarily designed to identify negative\nthoughts, the proposed system goes beyond this by predicting additional\nnegative side effects and other potential mental health disorders likes\nPhobias, Eating Disorders. This enhancement allows for a more comprehensive\nunderstanding and intervention strategy, offering psychotherapists a powerful\ntool for early detection and treatment of various psychological issues.", "comment": "6 Pages, 5 Figures, IEEE IDCIoT 2025", "pdf_url": "http://arxiv.org/pdf/2506.23503v1", "categories": ["cs.AI"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.23503v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23582", "title": "RELATE: Subjective evaluation dataset for automatic evaluation of relevance between text and audio", "authors": ["Yusuke Kanamori", "Yuki Okamoto", "Taisei Takano", "Shinnosuke Takamichi", "Yuki Saito", "Hiroshi Saruwatari"], "summary": "In text-to-audio (TTA) research, the relevance between input text and output\naudio is an important evaluation aspect. Traditionally, it has been evaluated\nfrom both subjective and objective perspectives. However, subjective evaluation\nis costly in terms of money and time, and objective evaluation is unclear\nregarding the correlation to subjective evaluation scores. In this study, we\nconstruct RELATE, an open-sourced dataset that subjectively evaluates the\nrelevance. Also, we benchmark a model for automatically predicting the\nsubjective evaluation score from synthesized audio. Our model outperforms a\nconventional CLAPScore model, and that trend extends to many sound categories.", "comment": "Accepted to INTERSPEECH2025", "pdf_url": "http://arxiv.org/pdf/2506.23582v1", "categories": ["cs.SD", "eess.AS"], "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.23582v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22846", "title": "Boosting CTC-Based ASR Using LLM-Based Intermediate Loss Regularization", "authors": ["Duygu Altinok"], "summary": "End-to-end (E2E) automatic speech recognition (ASR) systems have\nrevolutionized the field by integrating all components into a single neural\nnetwork, with attention-based encoder-decoder models achieving state-of-the-art\nperformance. However, their autoregressive decoding process limits inference\nspeed, making them unsuitable for real-time applications. In contrast,\nCTC-based models offer faster, non-autoregressive decoding but struggle to\nmodel linguistic dependencies effectively. Addressing this challenge, we\npropose a novel auxiliary loss framework called Language-Aware Intermediate\nLoss (LAIL) to enhance CTC-based ASR using the linguistic knowledge of large\nlanguage models (LLMs). By attaching connector layers to intermediate encoder\nlayers, LAIL maps outputs to the embedding space of an LLM and computes a\ncausal language modeling loss during training. This approach enhances\nlinguistic modeling while preserving the computational efficiency of CTC\ndecoding. Using the Conformer architecture and various LLaMA models, we\ndemonstrate significant improvements in Word Error Rate (WER) on the\nLibriSpeech, TEDLIUM2, and WSJ corpora, achieving state-of-the-art performance\nfor CTC-based ASR with minimal computational overhead.", "comment": "This is the accepted version of an article accepted to the TSD 2025\n  conference, published in Springer Lecture Notes in Artificial Intelligence\n  (LNAI). The final authenticated version is available online at SpringerLink", "pdf_url": "http://arxiv.org/pdf/2506.22846v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.22846v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23933", "title": "Structure-preserving approximation of the non-isothermal Cahn-Hilliard system", "authors": ["Aaron Brunk", "Maria Lukacova-Medvidova", "Dennis Schumann"], "summary": "We propose and analyze a structure-preserving approximation of the\nnon-isothermal Cahn-Hilliard equation using conforming finite elements for the\nspatial discretization and a problem-specific mixed explicit-implicit approach\nfor the temporal discretization. To ensure the preservation of structural\nproperties, i.e. conservation of mass and internal energy as well as entropy\nproduction, we introduce a suitable variational formulation for the continuous\nproblem, based on the entropy equation. Analytical findings are supported by\nnumerical tests, including convergence analysis.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23933v1", "categories": ["math.NA", "cs.NA"], "cate": "math.NA", "url": "http://arxiv.org/abs/2506.23933v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23552", "title": "JAM-Flow: Joint Audio-Motion Synthesis with Flow Matching", "authors": ["Mingi Kwon", "Joonghyuk Shin", "Jaeseok Jung", "Jaesik Park", "Youngjung Uh"], "summary": "The intrinsic link between facial motion and speech is often overlooked in\ngenerative modeling, where talking head synthesis and text-to-speech (TTS) are\ntypically addressed as separate tasks. This paper introduces JAM-Flow, a\nunified framework to simultaneously synthesize and condition on both facial\nmotion and speech. Our approach leverages flow matching and a novel Multi-Modal\nDiffusion Transformer (MM-DiT) architecture, integrating specialized Motion-DiT\nand Audio-DiT modules. These are coupled via selective joint attention layers\nand incorporate key architectural choices, such as temporally aligned\npositional embeddings and localized joint attention masking, to enable\neffective cross-modal interaction while preserving modality-specific strengths.\nTrained with an inpainting-style objective, JAM-Flow supports a wide array of\nconditioning inputs-including text, reference audio, and reference\nmotion-facilitating tasks such as synchronized talking head generation from\ntext, audio-driven animation, and much more, within a single, coherent model.\nJAM-Flow significantly advances multi-modal generative modeling by providing a\npractical solution for holistic audio-visual synthesis. project page:\nhttps://joonghyuk.com/jamflow-web", "comment": "project page: https://joonghyuk.com/jamflow-web Under review.\n  Preprint published on arXiv", "pdf_url": "http://arxiv.org/pdf/2506.23552v1", "categories": ["cs.CV", "cs.SD", "eess.AS"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23552v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23592", "title": "Cybersecurity AI: The Dangerous Gap Between Automation and Autonomy", "authors": ["V√≠ctor Mayoral-Vilches"], "summary": "The cybersecurity industry combines \"automated\" and \"autonomous\" AI, creating\ndangerous misconceptions about system capabilities. Recent milestones like XBOW\ntopping HackerOne's leaderboard showcase impressive progress, yet these systems\nremain fundamentally semi-autonomous--requiring human oversight. Drawing from\nrobotics principles, where the distinction between automation and autonomy is\nwell-established, I take inspiration from prior work and establish a 6-level\ntaxonomy (Level 0-5) distinguishing automation from autonomy in Cybersecurity\nAI. Current \"autonomous\" pentesters operate at Level 3-4: they execute complex\nattack sequences but need human review for edge cases and strategic decisions.\nTrue Level 5 autonomy remains aspirational. Organizations deploying\nmischaracterized \"autonomous\" tools risk reducing oversight precisely when it's\nmost needed, potentially creating new vulnerabilities. The path forward\nrequires precise terminology, transparent capabilities disclosure, and human-AI\npartnership-not replacement.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23592v1", "categories": ["cs.CR"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.23592v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23333", "title": "Moving Matter: Using a Single, Simple Robot to Reconfigure a Connected Set of Building Blocks", "authors": ["Javier Garcia", "Jonas Friemel", "Ramin Kosfeld", "Michael Yannuzzi", "Peter Kramer", "Christian Rieck", "Christian Scheffer", "Arne Schmidt", "Harm Kube", "Dan Biediger", "S√°ndor P. Fekete", "Aaron T. Becker"], "summary": "We implement and evaluate different methods for the reconfiguration of a\nconnected arrangement of tiles into a desired target shape, using a single\nactive robot that can move along the tile structure. This robot can pick up,\ncarry, or drop off one tile at a time, but it must maintain a single connected\nconfiguration at all times.\n  Becker et al. (CCCG 2025) recently proposed an algorithm that uses histograms\nas canonical intermediate configurations, guaranteeing performance within a\nconstant factor of the optimal solution if the start and target configuration\nare well-separated. We implement and evaluate this algorithm, both in a\nsimulated and practical setting, using an inchworm type robot to compare it\nwith two existing heuristic algorithms.", "comment": "8 pages, 12 figures. To appear in the proceedings of the 2025 IEEE\n  21st International Conference on Automation Science and Engineering (CASE\n  2025)", "pdf_url": "http://arxiv.org/pdf/2506.23333v1", "categories": ["cs.RO", "cs.CG", "cs.DS"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.23333v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23755", "title": "How Long Can I Transmit? A Mobility Aware mmWave-based UAV Communication Framework", "authors": ["Shawon Mitra", "Subhojit Sarkar", "Sasthi C. Ghosh"], "summary": "One primary focus of next generation wireless communication networks is the\nmillimeterwave (mmWave) spectrum, typically considered in the 30 GHz to 300 GHz\nfrequency range. Despite their promise of high data rates, mmWaves suffer from\nsevere attenuation while passing through obstacles. Unmanned aerial vehicles\n(UAVs) have been proposed to offset this limitation on account of their\nadditional degrees of freedom, which can be leveraged to provide line of sight\n(LoS) transmission paths. While some prior works have proposed analytical\nframeworks to compute the LoS probability for static ground users and a UAV,\nthe same is lacking for mobile users on the ground. In this paper, we consider\nthe popular Manhattan point line process (MPLP) to model an urban environment,\nwithin which a ground user moves with a known velocity for a small time\ninterval along the roads. We derive an expression for the expected duration of\nLoS between a static UAV in the air and a mobile ground user, and validate the\nsame through simulations. To demonstrate the efficacy of the proposed analysis,\nwe propose a simple user association algorithm that greedily assigns the UAVs\nto users with the highest expected LoS time, and show that it outperforms the\nexisting benchmark schemes that assign the users to the nearest UAVs with LoS\nwithout considering the user mobility.", "comment": "This article has been submitted in a reputed conference", "pdf_url": "http://arxiv.org/pdf/2506.23755v1", "categories": ["cs.NI", "eess.SP"], "cate": "cs.NI", "url": "http://arxiv.org/abs/2506.23755v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.24015", "title": "Bug Fixing with Broader Context: Enhancing LLM-Based Program Repair via Layered Knowledge Injection", "authors": ["Ramtin Ehsani", "Esteban Parra", "Sonia Haiduc", "Preetha Chatterjee"], "summary": "Prompting LLMs with bug-related context (e.g., error messages, stack traces)\nimproves automated program repair, but many bugs still remain unresolved. In\nreal-world projects, developers often rely on broader repository and\nproject-level context beyond the local code to resolve such bugs. In this\npaper, we investigate how automatically extracting and providing such knowledge\ncan improve LLM-based program repair. We propose a layered knowledge injection\nframework that incrementally augments LLMs with structured context. It starts\nwith the Bug Knowledge Layer, which includes information such as the buggy\nfunction and failing tests; expands to the Repository Knowledge Layer, which\nadds structural dependencies, related files, and commit history; and finally\ninjects the Project Knowledge Layer, which incorporates relevant details from\ndocumentation and previously fixed bugs. We evaluate this framework on a\ndataset of 314 bugs from BugsInPy using two LLMs (Llama 3.3 and GPT-4o-mini),\nand analyze fix rates across six bug types. By progressively injecting\nknowledge across layers, our approach achieves a fix rate of 79% (250/314)\nusing Llama 3.3, a significant improvement of 23% over previous work. All bug\ntypes show improvement with the addition of repository-level context, while\nonly a subset benefit further from project-level knowledge, highlighting that\ndifferent bug types require different levels of contextual information for\neffective repair. We also analyze the remaining unresolved bugs and find that\nmore complex and structurally isolated bugs, such as Program Anomaly and GUI\nbugs, remain difficult even after injecting all available information. Our\nresults show that layered context injection improves program repair and suggest\nthe need for interactive and adaptive APR systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.24015v1", "categories": ["cs.SE"], "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.24015v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23545", "title": "Immersive Technologies in Training and Healthcare: From Space Missions to Psychophysiological Research", "authors": ["Barbara Karpowicz", "Maciej Grzeszczuk", "Adam Kuzdrali≈Ñski", "Monika Kornacka", "Aliaksandr Marozau", "Wiktor Stawski", "Pavlo Zinevych", "Grzegorz Marcin W√≥jcik", "Tomasz Kowalewski", "Grzegorz Pochwatko", "Wies≈Çaw Kopeƒá"], "summary": "Virtual, Augmented, and eXtended Reality (VR/AR/XR) technologies are\nincreasingly recognized for their applications in training, diagnostics, and\npsychological research, particularly in high-risk and highly regulated\nenvironments. In this panel we discuss how immersive systems enhance human\nperformance across multiple domains, including clinical psychology, space\nexploration, and medical education. In psychological research and training, XR\ncan offer a controlled yet ecologically valid setting for measuring cognitive\nand affective processes. In space exploration, we discuss the development of\nVR-based astronaut training and diagnostic systems, allowing astronauts to\nperform real-time health assessments. In medical education and rehabilitation,\nwe cover procedural training and patient engagement. From virtual surgical\nsimulations to gamified rehabilitation exercises, immersive environments\nenhance both learning outcomes and treatment adherence.", "comment": "8 pages, 1 figure", "pdf_url": "http://arxiv.org/pdf/2506.23545v1", "categories": ["cs.HC", "cs.CE"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.23545v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22729", "title": "Persistence Paradox in Dynamic Science", "authors": ["Honglin Bao", "Kai Li"], "summary": "Persistence is often regarded as a virtue in science. In this paper, however,\nwe challenge this conventional view by highlighting its contextual nature,\nparticularly how persistence can become a liability during periods of paradigm\nshift. We focus on the deep learning revolution catalyzed by AlexNet in 2012.\nAnalyzing the 20-year career trajectories of over 5,000 scientists who were\nactive in top machine learning venues during the preceding decade, we examine\nhow their research focus and output evolved. We first uncover a dynamic period\nin which leading venues increasingly prioritized cutting-edge deep learning\ndevelopments that displaced relatively traditional statistical learning\nmethods. Scientists responded to these changes in markedly different ways.\nThose who were previously successful or affiliated with old teams adapted more\nslowly, experiencing what we term a rigidity penalty - a reluctance to embrace\nnew directions leading to a decline in scientific impact, as measured by\ncitation percentile rank. In contrast, scientists who pursued strategic\nadaptation - selectively pivoting toward emerging trends while preserving weak\nconnections to prior expertise - reaped the greatest benefits. Taken together,\nour macro- and micro-level findings show that scientific breakthroughs act as\nmechanisms that reconfigure power structures within a field.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22729v1", "categories": ["cs.DL", "cs.CY", "cs.LG"], "cate": "cs.DL", "url": "http://arxiv.org/abs/2506.22729v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.22624", "title": "Seg-R1: Segmentation Can Be Surprisingly Simple with Reinforcement Learning", "authors": ["Zuyao You", "Zuxuan Wu"], "summary": "We present Seg-R1, a preliminary exploration of using reinforcement learning\n(RL) to enhance the pixel-level understanding and reasoning capabilities of\nlarge multimodal models (LMMs). Starting with foreground segmentation tasks,\nspecifically camouflaged object detection (COD) and salient object detection\n(SOD), our approach enables the LMM to generate point and bounding box prompts\nin the next-token fashion, which are then used to guide SAM2 in producing\nsegmentation masks. We introduce Group Relative Policy Optimization (GRPO) into\nthe segmentation domain, equipping the LMM with pixel-level comprehension\nthrough a carefully designed training strategy. Notably, Seg-R1 achieves\nremarkable performance with purely RL-based training, achieving .873 S-measure\non COD10K without complex model modification. Moreover, we found that pure RL\ntraining demonstrates strong open-world generalization. Despite being trained\nsolely on foreground segmentation image-mask pairs without text supervision,\nSeg-R1 achieves impressive zero-shot performance on referring segmentation and\nreasoning segmentation tasks, with 71.4 cIoU on RefCOCOg test and 56.7 gIoU on\nReasonSeg test, outperforming models fully supervised on these datasets.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22624v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22624v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.23733", "title": "A Digital Twinning Approach to Decarbonisation: Research Challenges", "authors": ["Blair Archibald", "Paul Harvey", "Michele Sevegnani"], "summary": "Transportation accounts for around 27% of green house gas emissions in the\nUK. While an obvious priority area for decarbonisation, and aligned to the UK\ngovernment goal of reducing emissions by 68% for 2030, the free-market nature\nof the transportation sector combined with its fundamentally implicit and\npervasive connections to all aspects of society and national infrastructure\nmean that all decarbonisation efforts to date have been siloed within a single\ntransport sector, e.g. only considering greener aviation fuels. Truly\ndecarbonising transport requires radical changes to the entire transport\ninfrastructure, and since that transport does not happen in isolation, a single\nuser often using multiple modes, we need a view over the whole transport\nsystem. The first step to solving a problem is to understand it. As a result of\nthe fragmented nature of the transportation sector, there is currently no\nsystem level view. Without the ability to monitor even adjacent transport\ndomains, the ability for people or organisations to (dynamically) adapt their\noperations for decarbonisation outcomes is unrealistic. As transportation is a\ncomplex social-techno-economic system, information and knowledge sharing is a\nmust to be able to understand and explore potential solutions to the\ndecarbonisation challenge. We believe a Federated Digital Twinning Approach has\nthe potential to tackle transport decarbonisation problems, and, in this\nextended abstract, we give an overview of the research required to tackle the\nfundamental challenges around digital twin design, generation, validation and\nverification.", "comment": "LOCO 2024, December 3, 2024, Glasgow/Online; Extended Abstract", "pdf_url": "http://arxiv.org/pdf/2506.23733v1", "categories": ["eess.SY", "cs.SY"], "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.23733v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22824", "title": "Sensing Security Oriented OFDM-ISAC Against Multi-Intercept Threats", "authors": ["Lingyun Xu", "Bowen Wang", "Huiyong Li", "Ziyang Cheng"], "summary": "In recent years, security has emerged as a critical aspect of integrated\nsensing and communication (ISAC) systems. While significant research has\nfocused on secure communications, particularly in ensuring physical layer\nsecurity, the issue of sensing security has received comparatively less\nattention. This paper addresses the sensing security problem in ISAC,\nparticularly under the threat of multi-intercept adversaries. We consider a\nrealistic scenario in which the sensing target is an advanced electronic\nreconnaissance aircraft capable of employing multiple signal interception\ntechniques, such as power detection (PD) and cyclostationary analysis (CA). To\nevaluate sensing security under such sophisticated threats, we analyze two\ncritical features of the transmitted signal: (i) power distribution and (ii)\ncyclic spectrum. Further, we introduce a novel ergodic cyclic spectrum metric\nwhich leverages the intrinsic mathematical structure of cyclostationary signals\nto more comprehensively characterize their behavior. Building on this analysis,\nwe formulate a new ISAC design problem that explicitly considers sensing\nsecurity, and we develop a low-complexity, efficient optimization approach to\nsolve it. Simulation results demonstrate that the proposed metric is both\neffective and insightful, and that our ISAC design significantly enhances\nsensing security performance in the presence of multi-intercept threats.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22824v1", "categories": ["eess.SP"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.22824v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23584", "title": "A Clinically-Grounded Two-Stage Framework for Renal CT Report Generation", "authors": ["Renjie Liang", "Zhengkang Fan", "Jinqian Pan", "Chenkun Sun", "Russell Terry", "Jie Xu"], "summary": "Generating radiology reports from CT scans remains a complex task due to the\nnuanced nature of medical imaging and the variability in clinical\ndocumentation. In this study, we propose a two-stage framework for generating\nrenal radiology reports from 2D CT slices. First, we extract structured\nabnormality features using a multi-task learning model trained to identify\nlesion attributes such as location, size, enhancement, and attenuation. These\nextracted features are subsequently combined with the corresponding CT image\nand fed into a fine-tuned vision-language model to generate natural language\nreport sentences aligned with clinical findings. We conduct experiments on a\ncurated dataset of renal CT studies with manually annotated\nsentence-slice-feature triplets and evaluate performance using both\nclassification metrics and natural language generation metrics. Our results\ndemonstrate that the proposed model outperforms random baselines across all\nabnormality types, and the generated reports capture key clinical content with\nreasonable textual accuracy. This exploratory work highlights the feasibility\nof modular, feature-informed report generation for renal imaging. Future\nefforts will focus on extending this pipeline to 3D CT volumes and further\nimproving clinical fidelity in multimodal medical AI systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23584v1", "categories": ["eess.IV", "cs.AI", "cs.CV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.23584v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23756", "title": "Optimized methods for composite optimization: a reduction perspective", "authors": ["Jinho Bok", "Jason M. Altschuler"], "summary": "Recent advances in convex optimization have leveraged computer-assisted\nproofs to develop optimized first-order methods that improve over classical\nalgorithms. However, each optimized method is specially tailored for a\nparticular problem setting, and it is a well-documented challenge to extend\noptimized methods to other settings due to their highly bespoke design and\nanalysis. We provide a general framework that derives optimized methods for\ncomposite optimization directly from those for unconstrained smooth\noptimization. The derived methods naturally extend the original methods,\ngeneralizing how proximal gradient descent extends gradient descent. The key to\nour result is certain algebraic identities that provide a unified and\nstraightforward way of extending convergence analyses from unconstrained to\ncomposite settings. As concrete examples, we apply our framework to establish\n(1) the phenomenon of stepsize acceleration for proximal gradient descent; (2)\na convergence rate for the proximal optimized gradient method which is faster\nthan FISTA; (3) a new method that improves the state-of-the-art rate for\nminimizing gradient norm in the composite setting.", "comment": "40 pages", "pdf_url": "http://arxiv.org/pdf/2506.23756v1", "categories": ["math.OC", "cs.DS"], "cate": "math.OC", "url": "http://arxiv.org/abs/2506.23756v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22716", "title": "BEST-Route: Adaptive LLM Routing with Test-Time Optimal Compute", "authors": ["Dujian Ding", "Ankur Mallick", "Shaokun Zhang", "Chi Wang", "Daniel Madrigal", "Mirian Del Carmen Hipolito Garcia", "Menglin Xia", "Laks V. S. Lakshmanan", "Qingyun Wu", "Victor R√ºhle"], "summary": "Large language models (LLMs) are powerful tools but are often expensive to\ndeploy at scale. LLM query routing mitigates this by dynamically assigning\nqueries to models of varying cost and quality to obtain a desired trade-off.\nPrior query routing approaches generate only one response from the selected\nmodel and a single response from a small (inexpensive) model was often not good\nenough to beat a response from a large (expensive) model due to which they end\nup overusing the large model and missing out on potential cost savings.\nHowever, it is well known that for small models, generating multiple responses\nand selecting the best can enhance quality while remaining cheaper than a\nsingle large-model response. We leverage this idea to propose BEST-Route, a\nnovel routing framework that chooses a model and the number of responses to\nsample from it based on query difficulty and the quality thresholds.\nExperiments on real-world datasets demonstrate that our method reduces costs by\nup to 60% with less than 1% performance drop.", "comment": "Accepted to ICML 2025 (main conference)", "pdf_url": "http://arxiv.org/pdf/2506.22716v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.DB"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22716v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23504", "title": "Hybrid Approach for Electricity Price Forecasting using AlexNet and LSTM", "authors": ["Bosubabu Sambana", "Kotamsetty Geethika Devi", "Bandi Rajeswara Reddy", "Galeti Mohammad Hussain", "Gownivalla Siddartha"], "summary": "The recent development of advanced machine learning methods for hybrid models\nhas greatly addressed the need for the correct prediction of electrical prices.\nThis method combines AlexNet and LSTM algorithms, which are used to introduce a\nnew model with higher accuracy in price forecasting. Despite RNN and ANN being\neffective, they often fail to deal with forex time sequence data. The\ntraditional methods do not accurately forecast the prices. These traditional\nmethods only focus on demand and price which leads to insufficient analysis of\ndata. To address this issue, using the hybrid approach, which focuses on\nexternal variables that also effect the predicted prices. Nevertheless, due to\nAlexNet's excellent feature extraction and LSTM's learning sequential patterns,\nthe prediction accuracy is vastly increased. The model is built on the past\ndata, which has been supplied with the most significant elements like demand,\ntemperature, sunlight, and rain. For example, the model applies methods, such\nas minimum-maximum scaling and a time window, to predict the electricity prices\nof the future. The results show that this hybrid model is good than the\nstandalone ones in terms of accuracy. Although we got our accuracy rating of\n97.08, it shows higher accompaniments than remaining models RNN and ANN with\naccuracies of 96.64 and 96.63 respectively.", "comment": "6 Pages, 7 Figures", "pdf_url": "http://arxiv.org/pdf/2506.23504v1", "categories": ["cs.AI"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.23504v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23670", "title": "Efficient Interleaved Speech Modeling through Knowledge Distillation", "authors": ["Mohammadmahdi Nouriborji", "Morteza Rohanian"], "summary": "Current speech language models exceed the size and latency constraints of\nmany deployment environments. We build compact, expressive speech generation\nmodels through layer-aligned distillation, matching hidden states, attention\nmaps, and softened logits to compress large multimodal transformers by 3x with\nminimal loss in performance. We introduce TinyWave, a family of 2B-parameter\nmodels for speech-to-speech and interleaved speech-text generation, trained on\n50,000 hours of public audio. TinyWave supports (i) speech-only generation\nusing phonetic or expressive tokens and (ii) mixed speech-text continuations.\nEvaluation on Libri-Light shows TinyWave within 1.4 normalized perplexity\npoints of its teacher. Accuracy on spoken StoryCloze and SALMon reaches 93-97%\nof the teacher's performance, outperforming size-matched baselines. These\nmodels are optimized for deployment on commodity hardware, enabling\napplications in real-time conversational agents, assistive technologies, and\nlow-resource environments. We release models, training code, and evaluation\nscripts to support reproducible research on compact, expressive speech\ngeneration.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23670v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.23670v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22852", "title": "Knowledge Augmented Finetuning Matters in both RAG and Agent Based Dialog Systems", "authors": ["Yucheng Cai", "Yuxuan Wu", "Yi Huang", "Junlan Feng", "Zhijian Ou"], "summary": "Large language models (LLMs) have recently been applied to dialog systems.\nDespite making progress, LLMs are prone to errors in knowledge-intensive\nscenarios. Recently, approaches based on retrieval augmented generation (RAG)\nand agent have emerged to improve the factual accuracy by enhancing the LLMs\nwith knowledge retrieved from external knowledge bases (KBs). This is mostly\nimplemented by prompting the LLMs with instructions, examples and the retrieved\nknowledge. However, LLMs may have difficulty using the retrieved knowledge\neffectively for response generation, because they are not well trained to do\nsuch generation for specific domains. To mitigate this problem, we propose to\nfinetune the LLMs in the RAG-based and agent-based systems with domain-specific\ndata, together with domain-specific external knowledge, which is called\nknowledge augmented finetuning (KAFT). We base our study on the MobileCS2\ndataset, a real-life customer service dialog dataset that features intensive\nknowledge interactions, to systematically compare the prompting and KAFT\ntechniques in the RAG-based and agent-based systems. Experiment results show\nthat KAFT substantially surpasses prompting in both RAG and agent systems,\nparticularly in terms of factual accuracy. To the best of our knowledge, this\npaper represents the first solid empirical work to investigate the KAFT idea.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22852v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.22852v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23947", "title": "Explicit modified Euler approximations of the A√Øt-Sahalia type model with Poisson jumps", "authors": ["Yingsong Jiang", "Ruishu Liu", "Minhong Xu"], "summary": "This paper focuses on mean-square approximations of a generalized\nA\\\"it-Sahalia interest rate model with Poisson jumps. The main challenge in the\nconstruction and analysis of time-discrete numerical schemes is caused by a\ndrift that blows up at the origin, highly nonlinear drift and diffusion\ncoefficients and positivity-preserving requirement. Due to the presence of the\nPoisson jumps, additional difficulties arise in recovering the exact order\n$1/2$ of convergence for the time-stepping schemes. By incorporating\nimplicitness in the term $\\alpha_{-1}x^{-1} $ and introducing the modifications\nfunctions $f_h$ and $g_h$ in the recursion, a novel explicit Euler-type scheme\nis proposed, which is easy to implement and preserves the positivity of the\noriginal model unconditionally, i.e., for any time step-size $h>0$. A\nmean-square convergence rate of order $1/2$ is established for the proposed\nscheme in both the non-critical and general critical cases. Finally, numerical\nexperiments are provided to confirm the theoretical findings.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23947v1", "categories": ["math.NA", "cs.NA", "60H35, 60H15, 65C30"], "cate": "math.NA", "url": "http://arxiv.org/abs/2506.23947v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23553", "title": "Human-CLAP: Human-perception-based contrastive language-audio pretraining", "authors": ["Taisei Takano", "Yuki Okamoto", "Yusuke Kanamori", "Yuki Saito", "Ryotaro Nagase", "Hiroshi Saruwatari"], "summary": "Contrastive language-audio pretraining (CLAP) is widely used for audio\ngeneration and recognition tasks. For example, CLAPScore, which utilizes the\nsimilarity of CLAP embeddings, has been a major metric for the evaluation of\nthe relevance between audio and text in text-to-audio. However, the\nrelationship between CLAPScore and human subjective evaluation scores is still\nunclarified. We show that CLAPScore has a low correlation with human subjective\nevaluation scores. Additionally, we propose a human-perception-based CLAP\ncalled Human-CLAP by training a contrastive language-audio model using the\nsubjective evaluation score. In our experiments, the results indicate that our\nHuman-CLAP improved the Spearman's rank correlation coefficient (SRCC) between\nthe CLAPScore and the subjective evaluation scores by more than 0.25 compared\nwith the conventional CLAP.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23553v1", "categories": ["eess.AS", "cs.SD"], "cate": "eess.AS", "url": "http://arxiv.org/abs/2506.23553v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23603", "title": "SoK: Semantic Privacy in Large Language Models", "authors": ["Baihe Ma", "Yanna Jiang", "Xu Wang", "Guangshen Yu", "Qin Wang", "Caijun Sun", "Chen Li", "Xuelei Qi", "Ying He", "Wei Ni", "Ren Ping Liu"], "summary": "As Large Language Models (LLMs) are increasingly deployed in sensitive\ndomains, traditional data privacy measures prove inadequate for protecting\ninformation that is implicit, contextual, or inferable - what we define as\nsemantic privacy. This Systematization of Knowledge (SoK) introduces a\nlifecycle-centric framework to analyze how semantic privacy risks emerge across\ninput processing, pretraining, fine-tuning, and alignment stages of LLMs. We\ncategorize key attack vectors and assess how current defenses, such as\ndifferential privacy, embedding encryption, edge computing, and unlearning,\naddress these threats. Our analysis reveals critical gaps in semantic-level\nprotection, especially against contextual inference and latent representation\nleakage. We conclude by outlining open challenges, including quantifying\nsemantic leakage, protecting multimodal inputs, balancing de-identification\nwith generation quality, and ensuring transparency in privacy enforcement. This\nwork aims to inform future research on designing robust, semantically aware\nprivacy-preserving techniques for LLMs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23603v1", "categories": ["cs.CR", "cs.AI"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.23603v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23346", "title": "Safe and Performant Deployment of Autonomous Systems via Model Predictive Control and Hamilton-Jacobi Reachability Analysis", "authors": ["Hao Wang", "Armand Jordana", "Ludovic Righetti", "Somil Bansal"], "summary": "While we have made significant algorithmic developments to enable autonomous\nsystems to perform sophisticated tasks, it remains difficult for them to\nperform tasks effective and safely. Most existing approaches either fail to\nprovide any safety assurances or substantially compromise task performance for\nsafety. In this work, we develop a framework, based on model predictive control\n(MPC) and Hamilton-Jacobi (HJ) reachability, to optimize task performance for\nautonomous systems while respecting the safety constraints. Our framework\nguarantees recursive feasibility for the MPC controller, and it is scalable to\nhigh-dimensional systems. We demonstrate the effectiveness of our framework\nwith two simulation studies using a 4D Dubins Car and a 6 Dof Kuka iiwa\nmanipulator, and the experiments show that our framework significantly improves\nthe safety constraints satisfaction of the systems over the baselines.", "comment": "RSS 2025 Workshop on Reliable Robotics", "pdf_url": "http://arxiv.org/pdf/2506.23346v1", "categories": ["cs.RO", "cs.SY", "eess.SY"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.23346v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23964", "title": "Learning Constraints Directly from Network Data", "authors": ["Hongyu H√®", "Minhao Jin", "Maria Apostolaki"], "summary": "Network data conforms to a wide range of rules that arise from protocols,\ndesign principles, and deployment decisions (e.g., a packet's queuing delay\nmust be less than its end-to-end delay). Formalizing such rules as logic\nconstraints can (i) improve the quality of synthetic data, (ii) reduce the\nbrittleness of machine learning (ML) models, and (iii) improve semantic\nunderstanding of network measurements. However, these benefits remain out of\nreach if rule extraction is manual or solely reliant on ML, as both approaches\nyield incomplete, unreliable, and/or inaccurate rules.\n  This paper formulates rule extraction as a constraint modeling problem and\nintroduces NetNomos that learns propositional logic constraints directly from\nraw network measurements. Constraint modeling in this domain is uniquely\nchallenging due to the scale of the data, the inherent learning complexity and\npassive environment, and the lack of ground truth supervision. NetNomos\naddresses these challenges via a lattice-based search structured by constraint\nspecificity and succinctness. Our approach reduces learning complexity from\nsuperquadratic to logarithmic and enables efficient traversal in combinatorial\nsearch space.\n  Our evaluations on diverse network datasets show that NetNomos learns all\nbenchmark rules, including those associated with as little as 0.01% of data\npoints, in under three hours. In contrast, baseline methods discover less than\n25% of the rules and require several days to run. Through three case studies,\nwe show that: NetNomos (i) finds rule violations in the outputs of all seven\nsynthetic traffic generators, hence can be used to assess and guide their\ngeneration process; (ii) detects semantic differences in traffic, hence can be\nused for anomaly detection; and (iii) automatically finds rules used for\ntelemetry imputation, hence can support monitoring through inference.", "comment": "13 pages, 15 figures", "pdf_url": "http://arxiv.org/pdf/2506.23964v1", "categories": ["cs.NI", "cs.LG", "C.2.3; I.2.6; I.2.3"], "cate": "cs.NI", "url": "http://arxiv.org/abs/2506.23964v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22954", "title": "Evaluating and Improving Large Language Models for Competitive Program Generation", "authors": ["Minnan Wei", "Ziming Li", "Xiang Chen", "Menglin Zheng", "Ziyan Qu", "Cheng Yu", "Siyu Chen", "Xiaolin Ju"], "summary": "Context: Due to the demand for strong algorithmic reasoning, complex logic\nimplementation, and strict adherence to input/output formats and resource\nconstraints, competitive programming generation by large language models (LLMs)\nis considered the most challenging problem in current LLM-based code\ngeneration. However, previous studies often evaluate LLMs using simple prompts\nand benchmark datasets prone to data leakage. Moreover, prior work has limited\nconsideration of the diversity in algorithm types and difficulty levels.\nObjective: In this study, we aim to evaluate and improve LLMs in solving\nreal-world competitive programming problems. Methods: We initially collect 117\nproblems from nine regional ICPC/CCPC contests held in 2024 and design four\nfiltering criteria to construct a curated benchmark consisting of 80 problems.\nLeveraging DeepSeek-R1 as the LLM, we evaluate its competitive program\ngeneration capabilities through the online judge (OJ) platforms, guided by a\ncarefully designed basic prompt. For incorrect submissions, we construct a\nfine-grained error taxonomy and then propose a targeted improvement framework\nby combining a multi-turn dialogue-based repair phase and an\ninformation-augmented regeneration phase. Results: Experimental results show\nthat only 5 out of 80 problems are fully accepted when using basic prompts. For\nthe unsolved problems, we construct the error taxonomy, including general\nerrors (such as design, boundary, condition, data type, syntax, and\ninput/output errors) and specialized errors (such as those in mathematical\nproblems, greedy algorithms, and graph theories). After applying our proposed\nimprovement strategies, we substantially increased the number of correct\nsolutions, with 46 out of 80 problems successfully accepted.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22954v1", "categories": ["cs.SI", "cs.SE"], "cate": "cs.SI", "url": "http://arxiv.org/abs/2506.22954v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23678", "title": "Interactive Reasoning: Visualizing and Controlling Chain-of-Thought Reasoning in Large Language Models", "authors": ["Rock Yuren Pang", "K. J. Kevin Feng", "Shangbin Feng", "Chu Li", "Weijia Shi", "Yulia Tsvetkov", "Jeffrey Heer", "Katharina Reinecke"], "summary": "The output quality of large language models (LLMs) can be improved via\n\"reasoning\": generating segments of chain-of-thought (CoT) content to further\ncondition the model prior to producing user-facing output. While these chains\ncontain valuable information, they are verbose and lack explicit organization,\nmaking them tedious to review. Moreover, they lack opportunities for user\nfeedback, such as to remove unwanted considerations, add desired ones, or\nclarify unclear assumptions. We introduce Interactive Reasoning, an interaction\ndesign that visualizes chain-of-thought outputs as a hierarchy of topics and\nenables user review and modification. We implement interactive reasoning in\nHippo, a prototype for AI-assisted decision making in the face of uncertain\ntrade-offs. In a user study with 16 participants, we find that interactive\nreasoning in Hippo allows users to quickly identify and interrupt erroneous\ngenerations, efficiently steer the model towards customized responses, and\nbetter understand both model reasoning and model outputs. Our work contributes\nto a new paradigm that incorporates user oversight into LLM reasoning\nprocesses.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23678v1", "categories": ["cs.HC", "cs.AI"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.23678v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22773", "title": "Not All Water Consumption Is Equal: A Water Stress Weighted Metric for Sustainable Computing", "authors": ["Yanran Wu", "Inez Hua", "Yi Ding"], "summary": "Water consumption is an increasingly critical dimension of computing\nsustainability, especially as AI workloads rapidly scale. However, current\nwater impact assessment often overlooks where and when water stress is more\nsevere. To fill in this gap, we present SCARF, the first general framework that\nevaluates water impact of computing by factoring in both spatial and temporal\nvariations in water stress. SCARF calculates an Adjusted Water Impact (AWI)\nmetric that considers both consumption volume and local water stress over time.\nThrough three case studies on LLM serving, datacenters, and semiconductor\nfabrication plants, we show the hidden opportunities for reducing water impact\nby optimizing location and time choices, paving the way for water-sustainable\ncomputing. The code is available at https://github.com/jojacola/SCARF.", "comment": "7 pages, 9 figures, HotCarbon '25: Proceedings of the 4th Workshop on\n  Sustainable Computer Systems, Cambridge, Massachusetts (USA), July 10-11th,\n  2025", "pdf_url": "http://arxiv.org/pdf/2506.22773v1", "categories": ["cs.DC", "cs.AR", "cs.CY", "cs.LG"], "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.22773v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.22636", "title": "ReCo: Reminder Composition Mitigates Hallucinations in Vision-Language Models", "authors": ["Sotirios Panagiotis Chytas", "Miso Choi", "Hyunwoo J. Kim", "Vikas Singh"], "summary": "Vision Language Models (VLMs) show impressive capabilities in integrating and\nreasoning with both visual and language data. But these models make mistakes. A\ncommon finding -- similar to LLMs -- is their tendency to hallucinate, i.e.,\ngenerate plausible sounding text which is not grounded in the visual input, or\nat worst, is contradictory. A growing consensus attributes this behavior to an\nover-reliance on language -- especially as the generation progresses, the model\nsuffers from a ``fading memory effect'' with respect to the provided visual\ninput. We study mechanisms by which this behavior can be controlled.\nSpecifically, using ideas from geometric algebra and relational compositions,\nwe propose the addition of a small, trainable module (named ReCo) on top of any\nVLM -- no other modification is needed. We show that such a lightweight module\nis able to mitigate the fading memory effect on three of the most widely used\nVLMs (InstructBLIP, LlaVA, MiniGPT4), where we see performance improvements on\nmultiple benchmarks. Additionally, we show that our module can be combined with\nmany of the other approaches for reducing hallucination where we achieve\nimproved results for each one.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22636v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22636v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.23744", "title": "On sample-based functional observability of linear systems", "authors": ["Isabelle Krauss", "Victor G. Lopez", "Matthias A. M√ºller"], "summary": "Sample-based observability characterizes the ability to reconstruct the\ninternal state of a dynamical system by using limited output information, i.e.,\nwhen measurements are only infrequently and/or irregularly available. In this\nwork, we investigate the concept of functional observability, which refers to\nthe ability to infer a function of the system state from the outputs, within a\nsamplebased framework. Here, we give necessary and sufficient conditions for a\nsystem to be sample-based functionally observable, and formulate conditions on\nthe sampling schemes such that these are satisfied. Furthermore, we provide a\nnumerical example, where we demonstrate the applicability of the obtained\nresults.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23744v1", "categories": ["eess.SY", "cs.SY"], "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.23744v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22844", "title": "Coexistence analysis of Wi-Fi 6E and 5G NR-U in the 6 GHz band", "authors": ["Navid Keshtiarast", "Marina Petrova"], "summary": "The ever-increasing demand for broadband and IoT wireless connectivity has\nrecently urged the regulators around the world to start opening the 6 GHz\nspectrum for unlicensed use. These bands will, for example, permit the use of\nadditional 1.2 GHz in the US and 500 MHz in Europe for unlicensed radio access\ntechnologies (RATs) such as Wi-Fi and 5G New Radio Unlicensed (5G NR-U). To\nsupport QoS-sensitive applications with both technologies, fair and efficient\ncoexistence approaches between the two RATs, as well as with incumbents already\noperating in the 6 GHz band, are crucial. In this paper, we study through\nextensive simulations the achievable mean downlink throughput of both Wi-Fi 6E\nAPs and 5G NR-U gNBs when they are co-deployed in a dense residential scenario\nunder high-interference conditions. We also explore how different parameter\nsettings e.g., MAC frame aggregation, energy detection threshold and maximum\nchannel occupancy time (MCOT) affect the coexistence. Our findings give\nimportant insights into how to tune the key parameters to design fair\ncoexistence policies.", "comment": "Accepted for Publication in ICNS3 2025", "pdf_url": "http://arxiv.org/pdf/2506.22844v1", "categories": ["eess.SP", "cs.NI"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.22844v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23664", "title": "Diffusion Model-based Data Augmentation Method for Fetal Head Ultrasound Segmentation", "authors": ["Fangyijie Wang", "Kevin Whelan", "F√©lix Balado", "Gu√©nol√© Silvestre", "Kathleen M. Curran"], "summary": "Medical image data is less accessible than in other domains due to privacy\nand regulatory constraints. In addition, labeling requires costly,\ntime-intensive manual image annotation by clinical experts. To overcome these\nchallenges, synthetic medical data generation offers a promising solution.\nGenerative AI (GenAI), employing generative deep learning models, has proven\neffective at producing realistic synthetic images. This study proposes a novel\nmask-guided GenAI approach using diffusion models to generate synthetic fetal\nhead ultrasound images paired with segmentation masks. These synthetic pairs\naugment real datasets for supervised fine-tuning of the Segment Anything Model\n(SAM). Our results show that the synthetic data captures real image features\neffectively, and this approach reaches state-of-the-art fetal head\nsegmentation, especially when trained with a limited number of real image-mask\npairs. In particular, the segmentation reaches Dice Scores of 94.66\\% and\n94.38\\% using a handful of ultrasound images from the Spanish and African\ncohorts, respectively. Our code, models, and data are available on GitHub.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23664v1", "categories": ["eess.IV", "cs.CV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.23664v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23790", "title": "A Graph Width Perspective on Partially Ordered Hamiltonian Paths and Cycles I: Treewidth, Pathwidth, and Grid Graphs", "authors": ["Jesse Beisegel", "Katharina Klost", "Kristin Knorr", "Fabienne Ratajczak", "Robert Scheffler"], "summary": "We consider the problem of finding a Hamiltonian path or a Hamiltonian cycle\nwith precedence constraints in the form of a partial order on the vertex set.\nWe show that the path problem is $\\mathsf{NP}$-complete for graphs of pathwidth\n4 while the cycle problem is $\\mathsf{NP}$-complete on graphs of pathwidth 5.\nWe complement these results by giving polynomial-time algorithms for graphs of\npathwidth 3 and treewidth 2 for Hamiltonian paths as well as pathwidth 4 and\ntreewidth 3 for Hamiltonian cycles. Furthermore, we study the complexity of the\npath and cycle problems on rectangular grid graphs of bounded height. For\nthese, we show that the path and cycle problems are $\\mathsf{NP}$-complete when\nthe height of the grid is greater or equal to 7 and 9, respectively. In the\nvariant where we look for minimum edge-weighted Hamiltonian paths and cycles,\nthe problems are $\\mathsf{NP}$-hard for heights 5 and 6, respectively.", "comment": "\"A Graph Width Perspective on Partially Ordered Hamiltonian Paths\"\n  arXiv:2503.03553 was an extended abstract of a host of results. We have\n  decided to split that paper into two separate full papers. This first paper\n  given here covers the first half of the results along with several new\n  results, in particular about Hamiltonian cycles", "pdf_url": "http://arxiv.org/pdf/2506.23790v1", "categories": ["cs.DM", "cs.CC", "cs.DS", "math.CO"], "cate": "cs.DM", "url": "http://arxiv.org/abs/2506.23790v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22732", "title": "Robust Tensor Completion via Gradient Tensor Nulclear L1-L2 Norm for Traffic Data Recovery", "authors": ["Hao Shu", "Jicheng Li", "Tianyv Lei", "Lijun Sun"], "summary": "In real-world scenarios, spatiotemporal traffic data frequently experiences\ndual degradation from missing values and noise caused by sensor malfunctions\nand communication failures. Therefore, effective data recovery methods are\nessential to ensure the reliability of downstream data-driven applications.\nwhile classical tensor completion methods have been widely adopted, they are\nincapable of modeling noise, making them unsuitable for complex scenarios\ninvolving simultaneous data missingness and noise interference. Existing Robust\nTensor Completion (RTC) approaches offer potential solutions by separately\nmodeling the actual tensor data and noise. However, their effectiveness is\noften constrained by the over-relaxation of convex rank surrogates and the\nsuboptimal utilization of local consistency, leading to inadequate model\naccuracy. To address these limitations, we first introduce the tensor L1-L2\nnorm, a novel non-convex tensor rank surrogate that functions as an effective\nlow-rank representation tool. Leveraging an advanced feature fusion strategy,\nwe further develop the gradient tensor L1-L2 norm by incorporating the tensor\nL1-L2 norm in the gradient domain. By integrating the gradient tensor nuclear\nL1-L2 norm into the RTC framework, we propose the Robust Tensor Completion via\nGradient Tensor Nuclear L1-L2 Norm (RTC-GTNLN) model, which not only fully\nexploits both global low-rankness and local consistency without trade-off\nparameter, but also effectively handles the dual degradation challenges of\nmissing data and noise in traffic data. Extensive experiments conducted on\nmultiple real-world traffic datasets demonstrate that the RTC-GTNLN model\nconsistently outperforms existing state-of-the-art methods in complex recovery\nscenarios involving simultaneous missing values and noise.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22732v1", "categories": ["cs.LG", "eess.SP", "stat.ML"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22732v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23517", "title": "Assessing GPTZero's Accuracy in Identifying AI vs. Human-Written Essays", "authors": ["Selin Dik", "Osman Erdem", "Mehmet Dik"], "summary": "As the use of AI tools by students has become more prevalent, instructors\nhave started using AI detection tools like GPTZero and QuillBot to detect AI\nwritten text. However, the reliability of these detectors remains uncertain. In\nour study, we focused mostly on the success rate of GPTZero, the most-used AI\ndetector, in identifying AI-generated texts based on different lengths of\nrandomly submitted essays: short (40-100 word count), medium (100-350 word\ncount), and long (350-800 word count). We gathered a data set consisting of\ntwenty-eight AI-generated papers and fifty human-written papers. With this\nrandomized essay data, papers were individually plugged into GPTZero and\nmeasured for percentage of AI generation and confidence. A vast majority of the\nAI-generated papers were detected accurately (ranging from 91-100% AI believed\ngeneration), while the human generated essays fluctuated; there were a handful\nof false positives. These findings suggest that although GPTZero is effective\nat detecting purely AI-generated content, its reliability in distinguishing\nhuman-authored texts is limited. Educators should therefore exercise caution\nwhen relying solely on AI detection tools.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23517v1", "categories": ["cs.AI", "cs.CL"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.23517v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23869", "title": "Scaling Self-Supervised Representation Learning for Symbolic Piano Performance", "authors": ["Louis Bradshaw", "Honglu Fan", "Alexander Spangher", "Stella Biderman", "Simon Colton"], "summary": "We study the capabilities of generative autoregressive transformer models\ntrained on large amounts of symbolic solo-piano transcriptions. After first\npretraining on approximately 60,000 hours of music, we use a comparatively\nsmaller, high-quality subset, to finetune models to produce musical\ncontinuations, perform symbolic classification tasks, and produce\ngeneral-purpose contrastive MIDI embeddings by adapting the SimCLR framework to\nsymbolic music. When evaluating piano continuation coherence, our generative\nmodel outperforms leading symbolic generation techniques and remains\ncompetitive with proprietary audio generation models. On MIR classification\nbenchmarks, frozen representations from our contrastive model achieve\nstate-of-the-art results in linear probe experiments, while direct finetuning\ndemonstrates the generalizability of pretrained representations, often\nrequiring only a few hundred labeled examples to specialize to downstream\ntasks.", "comment": "ISMIR (2025)", "pdf_url": "http://arxiv.org/pdf/2506.23869v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.23869v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22853", "title": "DICE-BENCH: Evaluating the Tool-Use Capabilities of Large Language Models in Multi-Round, Multi-Party Dialogues", "authors": ["Kyochul Jang", "Donghyeon Lee", "Kyusik Kim", "Dongseok Heo", "Taewhoo Lee", "Woojeong Kim", "Bongwon Suh"], "summary": "Existing function-calling benchmarks focus on single-turn interactions.\nHowever, they overlook the complexity of real-world scenarios. To quantify how\nexisting benchmarks address practical applications, we introduce DICE-SCORE, a\nmetric that evaluates the dispersion of tool-related information such as\nfunction name and parameter values throughout the dialogue. Analyzing existing\nbenchmarks through DICE-SCORE reveals notably low scores, highlighting the need\nfor more realistic scenarios. To address this gap, we present DICE-BENCH, a\nframework that constructs practical function-calling datasets by synthesizing\nconversations through a tool graph that maintains dependencies across rounds\nand a multi-agent system with distinct personas to enhance dialogue\nnaturalness. The final dataset comprises 1,607 high-DICE-SCORE instances. Our\nexperiments on 19 LLMs with DICE-BENCH show that significant advances are still\nrequired before such models can be deployed effectively in real-world settings.\nOur code and data are all publicly available:\nhttps://snuhcc.github.io/DICE-Bench/.", "comment": "9 pages, ACL 2025 Vienna", "pdf_url": "http://arxiv.org/pdf/2506.22853v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.22853v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23969", "title": "Full history recursive multilevel Picard approximations suffer from the curse of dimensionality for the Hamilton-Jacobi-Bellman equation of a stochastic control problem", "authors": ["Martin Hutzenthaler", "Tuan Anh Nguyen"], "summary": "Full history recursive multilevel Picard (MLP) approximations have been\nproved to overcome the curse of dimensionality in the numerical approximation\nof semilinear heat equations with nonlinearities which are globally Lipschitz\ncontinuous with respect to the maximum-norm. Nonlinearities in\nHamilton-Jacobi-Bellman equations in stochastic control theory, however, are\noften (locally) Lipschitz continuous with respect to the standard Euclidean\nnorm. In this paper we prove the surprising fact that MLP approximations for\none such example equation suffer from the curse of dimensionality.", "comment": "21 pages", "pdf_url": "http://arxiv.org/pdf/2506.23969v1", "categories": ["math.NA", "cs.NA"], "cate": "math.NA", "url": "http://arxiv.org/abs/2506.23969v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23859", "title": "Less is More: Data Curation Matters in Scaling Speech Enhancement", "authors": ["Chenda Li", "Wangyou Zhang", "Wei Wang", "Robin Scheibler", "Kohei Saijo", "Samuele Cornell", "Yihui Fu", "Marvin Sach", "Zhaoheng Ni", "Anurag Kumar", "Tim Fingscheidt", "Shinji Watanabe", "Yanmin Qian"], "summary": "The vast majority of modern speech enhancement systems rely on data-driven\nneural network models. Conventionally, larger datasets are presumed to yield\nsuperior model performance, an observation empirically validated across\nnumerous tasks in other domains. However, recent studies reveal diminishing\nreturns when scaling speech enhancement data. We focus on a critical factor:\nprevalent quality issues in ``clean'' training labels within large-scale\ndatasets. This work re-examines this phenomenon and demonstrates that, within\nlarge-scale training sets, prioritizing high-quality training data is more\nimportant than merely expanding the data volume. Experimental findings suggest\nthat models trained on a carefully curated subset of 700 hours can outperform\nmodels trained on the 2,500-hour full dataset. This outcome highlights the\ncrucial role of data curation in scaling speech enhancement systems\neffectively.", "comment": "Submitted to ASRU2025", "pdf_url": "http://arxiv.org/pdf/2506.23859v1", "categories": ["eess.AS", "cs.SD"], "cate": "eess.AS", "url": "http://arxiv.org/abs/2506.23859v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23622", "title": "Privacy-Preserving Federated Learning Scheme with Mitigating Model Poisoning Attacks: Vulnerabilities and Countermeasures", "authors": ["Jiahui Wu", "Fucai Luo", "Tiecheng Sun", "Haiyan Wang", "Weizhe Zhang"], "summary": "The privacy-preserving federated learning schemes based on the setting of two\nhonest-but-curious and non-colluding servers offer promising solutions in terms\nof security and efficiency. However, our investigation reveals that these\nschemes still suffer from privacy leakage when considering model poisoning\nattacks from malicious users. Specifically, we demonstrate that the\nprivacy-preserving computation process for defending against model poisoning\nattacks inadvertently leaks privacy to one of the honest-but-curious servers,\nenabling it to access users' gradients in plaintext. To address both privacy\nleakage and model poisoning attacks, we propose an enhanced privacy-preserving\nand Byzantine-robust federated learning (PBFL) scheme, comprising three\ncomponents: (1) a two-trapdoor fully homomorphic encryption (FHE) scheme to\nbolster users' privacy protection; (2) a novel secure normalization judgment\nmethod to preemptively thwart gradient poisoning; and (3) an innovative secure\ncosine similarity measurement method for detecting model poisoning attacks\nwithout compromising data privacy. Our scheme guarantees privacy preservation\nand resilience against model poisoning attacks, even in scenarios with\nheterogeneous, non-IID (Independently and Identically Distributed) datasets.\nTheoretical analyses substantiate the security and efficiency of our scheme,\nand extensive experiments corroborate the efficacy of our private attacks.\nFurthermore, the experimental results demonstrate that our scheme accelerates\ntraining speed while reducing communication overhead compared to the\nstate-of-the-art PBFL schemes.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23622v1", "categories": ["cs.CR"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.23622v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23351", "title": "Benchmarking Generalizable Bimanual Manipulation: RoboTwin Dual-Arm Collaboration Challenge at CVPR 2025 MEIS Workshop", "authors": ["Tianxing Chen", "Kaixuan Wang", "Zhaohui Yang", "Yuhao Zhang", "Zanxin Chen", "Baijun Chen", "Wanxi Dong", "Ziyuan Liu", "Dong Chen", "Tianshuo Yang", "Haibao Yu", "Xiaokang Yang", "Yusen Qin", "Zhiqiang Xie", "Yao Mu", "Ping Luo", "Tian Nian", "Weiliang Deng", "Yiheng Ge", "Yibin Liu", "Zixuan Li", "Dehui Wang", "Zhixuan Liang", "Haohui Xie", "Rijie Zeng", "Yunfei Ge", "Peiqing Cong", "Guannan He", "Zhaoming Han", "Ruocheng Yin", "Jingxiang Guo", "Lunkai Lin", "Tianling Xu", "Hongzhe Bi", "Xuewu Lin", "Tianwei Lin", "Shujie Luo", "Keyu Li", "Ziyan Zhao", "Ke Fan", "Heyang Xu", "Bo Peng", "Wenlong Gao", "Dongjiang Li", "Feng Jin", "Hui Shen", "Jinming Li", "Chaowei Cui", "Yuchen", "Yaxin Peng", "Lingdong Zeng", "Wenlong Dong", "Tengfei Li", "Weijie Ke", "Jun Chen", "Erdemt Bao", "Tian Lan", "Tenglong Liu", "Jin Yang", "Huiping Zhuang", "Baozhi Jia", "Shuai Zhang", "Zhengfeng Zou", "Fangheng Guan", "Tianyi Jia", "Ke Zhou", "Hongjiu Zhang", "Yating Han", "Cheng Fang", "Yixian Zou", "Chongyang Xu", "Qinglun Zhang", "Shen Cheng", "Xiaohe Wang", "Ping Tan", "Haoqiang Fan", "Shuaicheng Liu", "Jiaheng Chen", "Chuxuan Huang", "Chengliang Lin", "Kaijun Luo", "Boyu Yue", "Yi Liu", "Jinyu Chen", "Zichang Tan", "Liming Deng", "Shuo Xu", "Zijian Cai", "Shilong Yin", "Hao Wang", "Hongshan Liu", "Tianyang Li", "Long Shi", "Ran Xu", "Huilin Xu", "Zhengquan Zhang", "Congsheng Xu", "Jinchang Yang", "Feng Xu"], "summary": "Embodied Artificial Intelligence (Embodied AI) is an emerging frontier in\nrobotics, driven by the need for autonomous systems that can perceive, reason,\nand act in complex physical environments. While single-arm systems have shown\nstrong task performance, collaborative dual-arm systems are essential for\nhandling more intricate tasks involving rigid, deformable, and\ntactile-sensitive objects. To advance this goal, we launched the RoboTwin\nDual-Arm Collaboration Challenge at the 2nd MEIS Workshop, CVPR 2025. Built on\nthe RoboTwin Simulation platform (1.0 and 2.0) and the AgileX COBOT-Magic Robot\nplatform, the competition consisted of three stages: Simulation Round 1,\nSimulation Round 2, and a final Real-World Round. Participants totally tackled\n17 dual-arm manipulation tasks, covering rigid, deformable, and tactile-based\nscenarios. The challenge attracted 64 global teams and over 400 participants,\nproducing top-performing solutions like SEM and AnchorDP3 and generating\nvaluable insights into generalizable bimanual policy learning. This report\noutlines the competition setup, task design, evaluation methodology, key\nfindings and future direction, aiming to support future research on robust and\ngeneralizable bimanual manipulation policies. The Challenge Webpage is\navailable at https://robotwin-benchmark.github.io/cvpr-2025-challenge/.", "comment": "Challenge Webpage:\n  https://robotwin-benchmark.github.io/cvpr-2025-challenge/", "pdf_url": "http://arxiv.org/pdf/2506.23351v1", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.MA"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.23351v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.22471", "title": "Continual Learning for Wireless Channel Prediction", "authors": ["Muhammad Ahmed Mohsin", "Muhammad Umer", "Ahsan Bilal", "Muhammad Ali Jamshed", "John M. Cioffi"], "summary": "Modern 5G/6G deployments routinely face cross-configuration handovers--users\ntraversing cells with different antenna layouts, carrier frequencies, and\nscattering statistics--which inflate channel-prediction NMSE by $37.5\\%$ on\naverage when models are naively fine-tuned. The proposed improvement frames\nthis mismatch as a continual-learning problem and benchmarks three adaptation\nfamilies: replay with loss-aware reservoirs, synaptic-importance\nregularization, and memory-free learning-without-forgetting. Across three\nrepresentative 3GPP urban micro scenarios, the best replay and regularization\nschemes cut the high-SNR error floor by up to 2~dB ($\\approx 35\\%$), while even\nthe lightweight distillation recovers up to $30\\%$ improvement over baseline\nhandover prediction schemes. These results show that targeted rehearsal and\nparameter anchoring are essential for handover-robust CSI prediction and\nsuggest a clear migration path for embedding continual-learning hooks into\ncurrent channel prediction efforts in 3GPP--NR and O-RAN. The full codebase can\nbe found at\nhttps://github.com/ahmd-mohsin/continual-learning-channel-prediction.git.", "comment": "Accepted at ICML Workshop on ML4Wireless", "pdf_url": "http://arxiv.org/pdf/2506.22471v1", "categories": ["eess.SP", "cs.NI"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.22471v1", "date": "2025-06-19", "updated": "2025-06-19"}
{"id": "2506.23683", "title": "Threadbox: Sandboxing for Modular Security", "authors": ["Maysara Alhindi", "Joseph Hallett"], "summary": "There are many sandboxing mechanisms provided by operating systems to limit\nwhat resources applications can access, however, sometimes the use of these\nmechanisms requires developers to refactor their code to fit the sandboxing\nmodel. In this work, we investigate what makes existing sandboxing mechanisms\nchallenging to apply to certain types of applications, and propose Threadbox, a\nsandboxing mechanism that enables having modular and independent sandboxes, and\ncan be applied to threads and sandbox specific functions. We present case\nstudies to illustrate the applicability of the idea and discuss its\nlimitations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23683v1", "categories": ["cs.CR", "cs.OS", "cs.SE"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.23683v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23694", "title": "If You Had to Pitch Your Ideal Software -- Evaluating Large Language Models to Support User Scenario Writing for User Experience Experts and Laypersons", "authors": ["Patrick Stadler", "Christopher Lazik", "Christopher Katins", "Thomas Kosch"], "summary": "The process of requirements analysis requires an understanding of the end\nusers of a system. Thus, expert stakeholders, such as User Experience (UX)\ndesigners, usually create various descriptions containing information about the\nusers and their possible needs. In our paper, we investigate to what extent UX\nnovices are able to write such descriptions into user scenarios. We conducted a\nuser study with 60 participants consisting of 30 UX experts and 30 novices who\nwere asked to write a user scenario with or without the help of an\nLLM-supported writing assistant. Our findings show that LLMs empower laypersons\nto write reasonable user scenarios and provide first-hand insights for\nrequirements analysis that are comparable to UX experts in terms of structure\nand clarity, while especially excelling at audience-orientation. We present our\nqualitative and quantitative findings, including user scenario anatomies,\npotential influences, and differences in the way participants approached the\ntask.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23694v1", "categories": ["cs.HC"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.23694v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22774", "title": "Bridging Ethical Principles and Algorithmic Methods: An Alternative Approach for Assessing Trustworthiness in AI Systems", "authors": ["Michael Papademas", "Xenia Ziouvelou", "Antonis Troumpoukis", "Vangelis Karkaletsis"], "summary": "Artificial Intelligence (AI) technology epitomizes the complex challenges\nposed by human-made artifacts, particularly those widely integrated into\nsociety and exert significant influence, highlighting potential benefits and\ntheir negative consequences. While other technologies may also pose substantial\nrisks, AI's pervasive reach makes its societal effects especially profound. The\ncomplexity of AI systems, coupled with their remarkable capabilities, can lead\nto a reliance on technologies that operate beyond direct human oversight or\nunderstanding. To mitigate the risks that arise, several theoretical tools and\nguidelines have been developed, alongside efforts to create technological tools\naimed at safeguarding Trustworthy AI. The guidelines take a more holistic view\nof the issue but fail to provide techniques for quantifying trustworthiness.\nConversely, while technological tools are better at achieving such\nquantification, they lack a holistic perspective, focusing instead on specific\naspects of Trustworthy AI. This paper aims to introduce an assessment method\nthat combines the ethical components of Trustworthy AI with the algorithmic\nprocesses of PageRank and TrustRank. The goal is to establish an assessment\nframework that minimizes the subjectivity inherent in the self-assessment\ntechniques prevalent in the field by introducing algorithmic criteria. The\napplication of our approach indicates that a holistic assessment of an AI\nsystem's trustworthiness can be achieved by providing quantitative insights\nwhile considering the theoretical content of relevant guidelines.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22774v1", "categories": ["cs.AI", "cs.CY"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.22774v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.22637", "title": "CaO$_2$: Rectifying Inconsistencies in Diffusion-Based Dataset Distillation", "authors": ["Haoxuan Wang", "Zhenghao Zhao", "Junyi Wu", "Yuzhang Shang", "Gaowen Liu", "Yan Yan"], "summary": "The recent introduction of diffusion models in dataset distillation has shown\npromising potential in creating compact surrogate datasets for large,\nhigh-resolution target datasets, offering improved efficiency and performance\nover traditional bi-level/uni-level optimization methods. However, current\ndiffusion-based dataset distillation approaches overlook the evaluation process\nand exhibit two critical inconsistencies in the distillation process: (1)\nObjective Inconsistency, where the distillation process diverges from the\nevaluation objective, and (2) Condition Inconsistency, leading to mismatches\nbetween generated images and their corresponding conditions. To resolve these\nissues, we introduce Condition-aware Optimization with Objective-guided\nSampling (CaO$_2$), a two-stage diffusion-based framework that aligns the\ndistillation process with the evaluation objective. The first stage employs a\nprobability-informed sample selection pipeline, while the second stage refines\nthe corresponding latent representations to improve conditional likelihood.\nCaO$_2$ achieves state-of-the-art performance on ImageNet and its subsets,\nsurpassing the best-performing baselines by an average of 2.3% accuracy.", "comment": "ICCV 2025. Code is available at\n  https://github.com/hatchetProject/CaO2", "pdf_url": "http://arxiv.org/pdf/2506.22637v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22637v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.23769", "title": "Active Estimation of Multiplicative Faults in Dynamical Systems", "authors": ["Gabriel de Albuquerque Gleizer", "Peyman Mohajerin Esfahani", "Tamas Keviczky"], "summary": "This paper addresses the problem of estimating multiplicative fault signals\nin linear time-invariant systems by processing its input and output variables,\nas well as designing an input signal to maximize the accuracy of such\nestimates. The proposed real-time fault estimator is based on a residual\ngenerator used for fault detection and a multiple-output regressor generator,\nwhich feed a moving-horizon linear regression that estimates the parameter\nchanges. Asymptotic performance guarantees are provided in the presence of\nnoise. Motivated by the performance bounds, an optimal input design problem is\nformulated, for which we provide efficient algorithms and optimality bounds.\nNumerical examples demonstrate the efficacy of our approach and the importance\nof the optimal input design for accurate fault estimation.", "comment": "27 pages, 7 figures. Submitted to Automatica", "pdf_url": "http://arxiv.org/pdf/2506.23769v1", "categories": ["eess.SY", "cs.SY"], "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.23769v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22903", "title": "Limited Feedback in RIS-Assisted Wireless Communications: Use Cases, Challenges, and Future Directions", "authors": ["Weicong Chen", "Jiajia Guo", "Yiming Cui", "Xiao Li", "Shi Jin"], "summary": "Channel state information (CSI) is essential to unlock the potential of\nreconfigurable intelligent surfaces (RISs) in wireless communication systems.\nSince massive RIS elements are typically implemented without baseband signal\nprocessing capabilities, limited CSI feedback is necessary when designing the\nreflection/refraction coefficients of the RIS. In this article, the unique\nRIS-assisted channel features, such as the RIS position-dependent channel\nfluctuation, the ultra-high dimensional sub-channel matrix, and the structured\nsparsity, are distilled from recent advances in limited feedback and used as\nguidelines for designing feedback schemes. We begin by illustrating the use\ncases and the corresponding challenges associated with RIS feedback. We then\ndiscuss how to leverage techniques such as channel customization,\nstructured-sparsity, autoencoders, and others to reduce feedback overhead and\ncomplexity when devising feedback schemes. Finally, we identify potential\nresearch directions by considering the unresolved challenges, the new RIS\narchitecture, and the integration with multi-modal information and artificial\nintelligence.", "comment": "This work has been submitted for possible publication", "pdf_url": "http://arxiv.org/pdf/2506.22903v1", "categories": ["eess.SP", "cs.IT", "math.IT"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.22903v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23688", "title": "GUSL: A Novel and Efficient Machine Learning Model for Prostate Segmentation on MRI", "authors": ["Jiaxin Yang", "Vasileios Magoulianitis", "Catherine Aurelia Christie Alexander", "Jintang Xue", "Masatomo Kaneko", "Giovanni Cacciamani", "Andre Abreu", "Vinay Duddalwar", "C. -C. Jay Kuo", "Inderbir S. Gill", "Chrysostomos Nikias"], "summary": "Prostate and zonal segmentation is a crucial step for clinical diagnosis of\nprostate cancer (PCa). Computer-aided diagnosis tools for prostate segmentation\nare based on the deep learning (DL) paradigm. However, deep neural networks are\nperceived as \"black-box\" solutions by physicians, thus making them less\npractical for deployment in the clinical setting. In this paper, we introduce a\nfeed-forward machine learning model, named Green U-shaped Learning (GUSL),\nsuitable for medical image segmentation without backpropagation. GUSL\nintroduces a multi-layer regression scheme for coarse-to-fine segmentation. Its\nfeature extraction is based on a linear model, which enables seamless\ninterpretability during feature extraction. Also, GUSL introduces a mechanism\nfor attention on the prostate boundaries, which is an error-prone region, by\nemploying regression to refine the predictions through residue correction. In\naddition, a two-step pipeline approach is used to mitigate the class imbalance,\nan issue inherent in medical imaging problems. After conducting experiments on\ntwo publicly available datasets and one private dataset, in both prostate gland\nand zonal segmentation tasks, GUSL achieves state-of-the-art performance among\nother DL-based models. Notably, GUSL features a very energy-efficient pipeline,\nsince it has a model size several times smaller and less complexity than the\nrest of the solutions. In all datasets, GUSL achieved a Dice Similarity\nCoefficient (DSC) performance greater than $0.9$ for gland segmentation.\nConsidering also its lightweight model size and transparency in feature\nextraction, it offers a competitive and practical package for medical imaging\napplications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23688v1", "categories": ["eess.IV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.23688v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22771", "title": "FF-INT8: Efficient Forward-Forward DNN Training on Edge Devices with INT8 Precision", "authors": ["Jingxiao Ma", "Priyadarshini Panda", "Sherief Reda"], "summary": "Backpropagation has been the cornerstone of neural network training for\ndecades, yet its inefficiencies in time and energy consumption limit its\nsuitability for resource-constrained edge devices. While low-precision neural\nnetwork quantization has been extensively researched to speed up model\ninference, its application in training has been less explored. Recently, the\nForward-Forward (FF) algorithm has emerged as a promising alternative to\nbackpropagation, replacing the backward pass with an additional forward pass.\nBy avoiding the need to store intermediate activations for backpropagation, FF\ncan reduce memory footprint, making it well-suited for embedded devices. This\npaper presents an INT8 quantized training approach that leverages FF's\nlayer-by-layer strategy to stabilize gradient quantization. Furthermore, we\npropose a novel \"look-ahead\" scheme to address limitations of FF and improve\nmodel accuracy. Experiments conducted on NVIDIA Jetson Orin Nano board\ndemonstrate 4.6% faster training, 8.3% energy savings, and 27.0% reduction in\nmemory usage, while maintaining competitive accuracy compared to the\nstate-of-the-art.", "comment": "To be published in the 62nd Design Automation Conference (DAC), 2025", "pdf_url": "http://arxiv.org/pdf/2506.22771v1", "categories": ["cs.LG", "cs.AI", "cs.NE", "I.2.0; I.2.6"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22771v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23520", "title": "ChemActor: Enhancing Automated Extraction of Chemical Synthesis Actions with LLM-Generated Data", "authors": ["Yu Zhang", "Ruijie Yu", "Jidong Tian", "Feng Zhu", "Jiapeng Liu", "Xiaokang Yang", "Yaohui Jin", "Yanyan Xu"], "summary": "With the increasing interest in robotic synthesis in the context of organic\nchemistry, the automated extraction of chemical procedures from literature is\ncritical. However, this task remains challenging due to the inherent ambiguity\nof chemical language and the high cost of human annotation required for\ndeveloping reliable computer-aided extraction protocols. Here, we present\nChemActor, a fully fine-tuned large language model (LLM), as a chemical\nexecutor to convert between unstructured experimental procedures and structured\naction sequences. We propose a sequential LLM-generated data framework to\naddress the challenges of insufficient and low-quality annotated data. This\nframework integrates a data selection module that selects data based on\ndistribution divergence, with a general-purpose LLM, to generate\nmachine-executable actions from a single molecule input. Additionally, we\nintroduce a novel multi-round LLMs circle review metric, which reflects the\nmodel's advanced understanding of chemical experimental procedures. Extensive\nexperiments on reaction-to-description (R2D) and description-to-action (D2A)\ntasks demonstrate that ChemActor, augmented by LLM-generated data, achieves\nstate-of-the-art performance, outperforming the baseline model by 10%. The code\nis available at: https://github.com/Zhanghahah/ChemActor.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23520v1", "categories": ["cs.AI"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.23520v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23873", "title": "Emergent musical properties of a transformer under contrastive self-supervised learning", "authors": ["Yuexuan Kong", "Gabriel Meseguer-Brocal", "Vincent Lostanlen", "Mathieu Lagrange", "Romain Hennequin"], "summary": "In music information retrieval (MIR), contrastive self-supervised learning\nfor general-purpose representation models is effective for global tasks such as\nautomatic tagging. However, for local tasks such as chord estimation, it is\nwidely assumed that contrastively trained general-purpose self-supervised\nmodels are inadequate and that more sophisticated SSL is necessary; e.g.,\nmasked modeling. Our paper challenges this assumption by revealing the\npotential of contrastive SSL paired with a transformer in local MIR tasks. We\nconsider a lightweight vision transformer with one-dimensional patches in the\ntime--frequency domain (ViT-1D) and train it with simple contrastive SSL\nthrough normalized temperature-scaled cross-entropy loss (NT-Xent). Although\nNT-Xent operates only over the class token, we observe that, potentially thanks\nto weight sharing, informative musical properties emerge in ViT-1D's sequence\ntokens. On global tasks, the temporal average of class and sequence tokens\noffers a performance increase compared to the class token alone, showing useful\nproperties in the sequence tokens. On local tasks, sequence tokens perform\nunexpectedly well, despite not being specifically trained for. Furthermore,\nhigh-level musical features such as onsets emerge from layer-wise attention\nmaps and self-similarity matrices show different layers capture different\nmusical dimensions. Our paper does not focus on improving performance but\nadvances the musical interpretation of transformers and sheds light on some\noverlooked abilities of contrastive SSL paired with transformers for sequence\nmodeling in MIR.", "comment": "Accepted at ISMIR 2025", "pdf_url": "http://arxiv.org/pdf/2506.23873v1", "categories": ["cs.SD", "cs.IR", "cs.LG", "eess.AS"], "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.23873v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22858", "title": "Mind the Gap: Entity-Preserved Context-Aware ASR Structured Transcriptions", "authors": ["Duygu Altinok"], "summary": "Automatic Speech Recognition (ASR) systems, such as Whisper, achieve high\ntranscription accuracy but struggle with named entities and numerical data,\nespecially when proper formatting is required. These issues increase word error\nrate (WER) and impair semantic understanding in critical domains like legal,\nfinancial, and medical applications. We propose a novel training approach that\nextends the semantic context of ASR models by adding overlapping context\nwindows during training. By sliding 5-second overlaps on both sides of\n30-second chunks, we create a 40-second \"effective semantic window,\" improving\nentity recognition and formatting while focusing predictions on the central 30\nseconds. To address entities spanning chunk boundaries, we reassign such\nentities entirely to the right-hand chunk, ensuring proper formatting.\nAdditionally, enriched training data with embedded entity labels enables the\nmodel to learn both recognition and type-specific formatting. Evaluated on the\nSpoken Wikipedia dataset, our method improves performance across semantic\ntasks, including named entity recognition (NER) and entity formatting. These\nresults highlight the effectiveness of context-aware training in addressing ASR\nlimitations for long-form transcription and complex entity recognition tasks.", "comment": "This is the accepted version of an article accepted to the TSD 2025\n  conference, published in Springer Lecture Notes in Artificial Intelligence\n  (LNAI). The final authenticated version is available online at SpringerLink", "pdf_url": "http://arxiv.org/pdf/2506.22858v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.22858v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.24054", "title": "Sparse grids vs. random points for high-dimensional polynomial approximation", "authors": ["Jakob Eggl", "Elias Mindlberger", "Mario Ullrich"], "summary": "We study polynomial approximation on a $d$-cube, where $d$ is large, and\ncompare interpolation on sparse grids, aka Smolyak's algorithm (SA), with a\nsimple least squares method based on randomly generated points (LS) using\nstandard benchmark functions. Our main motivation is the influential paper\n[Barthelmann, Novak, Ritter: High dimensional polynomial interpolation on\nsparse grids, Adv. Comput. Math. 12, 2000]. We repeat and extend their\ntheoretical analysis and numerical experiments for SA and compare to LS in\ndimensions up to 100. Our extensive experiments demonstrate that LS, even with\nonly slight oversampling, consistently matches the accuracy of SA in low\ndimensions. In high dimensions, however, LS shows clear superiority.", "comment": "31 pages, 12 figures", "pdf_url": "http://arxiv.org/pdf/2506.24054v1", "categories": ["math.NA", "cs.NA", "65D05 (Primary)"], "cate": "math.NA", "url": "http://arxiv.org/abs/2506.24054v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23874", "title": "URGENT-PK: Perceptually-Aligned Ranking Model Designed for Speech Enhancement Competition", "authors": ["Jiahe Wang", "Chenda Li", "Wei Wang", "Wangyou Zhang", "Samuele Cornell", "Marvin Sach", "Robin Scheibler", "Kohei Saijo", "Yihui Fu", "Zhaoheng Ni", "Anurag Kumar", "Tim Fingscheidt", "Shinji Watanabe", "Yanmin Qian"], "summary": "The Mean Opinion Score (MOS) is fundamental to speech quality assessment.\nHowever, its acquisition requires significant human annotation. Although deep\nneural network approaches, such as DNSMOS and UTMOS, have been developed to\npredict MOS to avoid this issue, they often suffer from insufficient training\ndata. Recognizing that the comparison of speech enhancement (SE) systems\nprioritizes a reliable system comparison over absolute scores, we propose\nURGENT-PK, a novel ranking approach leveraging pairwise comparisons. URGENT-PK\ntakes homologous enhanced speech pairs as input to predict relative quality\nrankings. This pairwise paradigm efficiently utilizes limited training data, as\nall pairwise permutations of multiple systems constitute a training instance.\nExperiments across multiple open test sets demonstrate URGENT-PK's superior\nsystem-level ranking performance over state-of-the-art baselines, despite its\nsimple network architecture and limited training data.", "comment": "Submitted to ASRU2025", "pdf_url": "http://arxiv.org/pdf/2506.23874v1", "categories": ["eess.AS", "cs.SD"], "cate": "eess.AS", "url": "http://arxiv.org/abs/2506.23874v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23634", "title": "gMBA: Expression Semantic Guided Mixed Boolean-Arithmetic Deobfuscation Using Transformer Architectures", "authors": ["Youjeong Noh", "Joon-Young Paik", "Jingun Kwon", "Eun-Sun Cho"], "summary": "Mixed Boolean-Arithmetic (MBA) obfuscation protects intellectual property by\nconverting programs into forms that are more complex to analyze. However, MBA\nhas been increasingly exploited by malware developers to evade detection and\ncause significant real-world problems. Traditional MBA deobfuscation methods\noften consider these expressions as part of a black box and overlook their\ninternal semantic information. To bridge this gap, we propose a truth table,\nwhich is an automatically constructed semantic representation of an\nexpression's behavior that does not rely on external resources. The truth table\nis a mathematical form that represents the output of expression for all\npossible combinations of input. We also propose a general and extensible guided\nMBA deobfuscation framework (gMBA) that modifies a Transformer-based neural\nencoder-decoder Seq2Seq architecture to incorporate this semantic guidance.\nExperimental results and in-depth analysis show that integrating expression\nsemantics significantly improves performance and highlights the importance of\ninternal semantic expressions in recovering obfuscated code to its original\nform.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23634v1", "categories": ["cs.CR", "cs.AI"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.23634v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23369", "title": "GS-NBV: a Geometry-based, Semantics-aware Viewpoint Planning Algorithm for Avocado Harvesting under Occlusions", "authors": ["Xiao'ao Song", "Konstantinos Karydis"], "summary": "Efficient identification of picking points is critical for automated fruit\nharvesting. Avocados present unique challenges owing to their irregular shape,\nweight, and less-structured growing environments, which require specific\nviewpoints for successful harvesting. We propose a geometry-based,\nsemantics-aware viewpoint-planning algorithm to address these challenges. The\nplanning process involves three key steps: viewpoint sampling, evaluation, and\nexecution. Starting from a partially occluded view, the system first detects\nthe fruit, then leverages geometric information to constrain the viewpoint\nsearch space to a 1D circle, and uniformly samples four points to balance the\nefficiency and exploration. A new picking score metric is introduced to\nevaluate the viewpoint suitability and guide the camera to the next-best view.\nWe validate our method through simulation against two state-of-the-art\nalgorithms. Results show a 100% success rate in two case studies with\nsignificant occlusions, demonstrating the efficiency and robustness of our\napproach. Our code is available at https://github.com/lineojcd/GSNBV", "comment": "Accepted for publication in CASE 2025, 6 pages, 8 figures", "pdf_url": "http://arxiv.org/pdf/2506.23369v1", "categories": ["cs.RO"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.23369v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.22844", "title": "Coexistence analysis of Wi-Fi 6E and 5G NR-U in the 6 GHz band", "authors": ["Navid Keshtiarast", "Marina Petrova"], "summary": "The ever-increasing demand for broadband and IoT wireless connectivity has\nrecently urged the regulators around the world to start opening the 6 GHz\nspectrum for unlicensed use. These bands will, for example, permit the use of\nadditional 1.2 GHz in the US and 500 MHz in Europe for unlicensed radio access\ntechnologies (RATs) such as Wi-Fi and 5G New Radio Unlicensed (5G NR-U). To\nsupport QoS-sensitive applications with both technologies, fair and efficient\ncoexistence approaches between the two RATs, as well as with incumbents already\noperating in the 6 GHz band, are crucial. In this paper, we study through\nextensive simulations the achievable mean downlink throughput of both Wi-Fi 6E\nAPs and 5G NR-U gNBs when they are co-deployed in a dense residential scenario\nunder high-interference conditions. We also explore how different parameter\nsettings e.g., MAC frame aggregation, energy detection threshold and maximum\nchannel occupancy time (MCOT) affect the coexistence. Our findings give\nimportant insights into how to tune the key parameters to design fair\ncoexistence policies.", "comment": "Accepted for Publication in ICNS3 2025", "pdf_url": "http://arxiv.org/pdf/2506.22844v1", "categories": ["eess.SP", "cs.NI"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.22844v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23841", "title": "An ontological lens on attack trees: Toward adequacy and interoperability", "authors": ["√çtalo Oliveira", "Stefano M. Nicoletti", "Gal Engelberg", "Mattia Fumagalli", "Dan Klein", "Giancarlo Guizzardi"], "summary": "Attack Trees (AT) are a popular formalism for security analysis. They are\nmeant to display an attacker's goal decomposed into attack steps needed to\nachieve it and compute certain security metrics (e.g., attack cost,\nprobability, and damage). ATs offer three important services: (a) conceptual\nmodeling capabilities for representing security risk management scenarios, (b)\na qualitative assessment to find root causes and minimal conditions of\nsuccessful attacks, and (c) quantitative analyses via security metrics\ncomputation under formal semantics, such as minimal time and cost among all\nattacks. Still, the AT language presents limitations due to its lack of\nontological foundations, thus compromising associated services. Via an\nontological analysis grounded in the Common Ontology of Value and Risk (COVER)\n-- a reference core ontology based on the Unified Foundational Ontology (UFO)\n-- we investigate the ontological adequacy of AT and reveal four significant\nshortcomings: (1) ambiguous syntactical terms that can be interpreted in\nvarious ways; (2) ontological deficit concerning crucial domain-specific\nconcepts; (3) lacking modeling guidance to construct ATs decomposing a goal;\n(4) lack of semantic interoperability, resulting in ad hoc stand-alone tools.\nWe also discuss existing incremental solutions and how our analysis paves the\nway for overcoming those issues through a broader approach to risk management\nmodeling.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23841v1", "categories": ["cs.CR", "cs.SE"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.23841v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23815", "title": "The Impact of AI on Educational Assessment: A Framework for Constructive Alignment", "authors": ["Patrick Stokkink"], "summary": "The influence of Artificial Intelligence (AI), and specifically Large\nLanguage Models (LLM), on education is continuously increasing. These models\nare frequently used by students, giving rise to the question whether current\nforms of assessment are still a valid way to evaluate student performance and\ncomprehension. The theoretical framework developed in this paper is grounded in\nConstructive Alignment (CA) theory and Bloom's taxonomy for defining learning\nobjectives. We argue that AI influences learning objectives of different Bloom\nlevels in a different way, and assessment has to be adopted accordingly.\nFurthermore, in line with Bloom's vision, formative and summative assessment\nshould be aligned on whether the use of AI is permitted or not.\n  Although lecturers tend to agree that education and assessment need to be\nadapted to the presence of AI, a strong bias exists on the extent to which\nlecturers want to allow for AI in assessment. This bias is caused by a\nlecturer's familiarity with AI and specifically whether they use it themselves.\nTo avoid this bias, we propose structured guidelines on a university or faculty\nlevel, to foster alignment among the staff. Besides that, we argue that\nteaching staff should be trained on the capabilities and limitations of AI\ntools. In this way, they are better able to adapt their assessment methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23815v1", "categories": ["cs.HC", "cs.AI"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.23815v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22787", "title": "What's Privacy Good for? Measuring Privacy as a Shield from Harms due to Personal Data Use", "authors": ["Sri Harsha Gajavalli", "Junichi Koizumi", "Rakibul Hasan"], "summary": "We propose a harm-centric conceptualization of privacy that asks: What harms\nfrom personal data use can privacy prevent? The motivation behind this research\nis limitations in existing privacy frameworks (e.g., Contextual Integrity) to\ncapture or categorize many of the harms that arise from modern technology's use\nof personal data. We operationalize this conceptualization in an online study\nwith 400 college and university students. Study participants indicated their\nperceptions of different harms (e.g., manipulation, discrimination, and\nharassment) that may arise when artificial intelligence-based algorithms infer\npersonal data (e.g., demographics, personality traits, and cognitive\ndisability) and use it to identify students who are likely to drop out of a\ncourse or the best job candidate. The study includes 14 harms and six types of\npersonal data selected based on an extensive literature review.\n  Comprehensive statistical analyses of the study data show that the 14 harms\nare internally consistent and collectively represent a general notion of\nprivacy harms. The study data also surfaces nuanced perceptions of harms, both\nacross the contexts and participants' demographic factors. Based on these\nresults, we discuss how privacy can be improved equitably. Thus, this research\nnot only contributes to enhancing the understanding of privacy as a concept but\nalso provides practical guidance to improve privacy in the context of education\nand employment.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22787v1", "categories": ["cs.CR", "cs.CY"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.22787v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.22678", "title": "3D Shape Generation: A Survey", "authors": ["Nicolas Caytuiro", "Ivan Sipiran"], "summary": "Recent advances in deep learning have significantly transformed the field of\n3D shape generation, enabling the synthesis of complex, diverse, and\nsemantically meaningful 3D objects. This survey provides a comprehensive\noverview of the current state of the art in 3D shape generation, organizing the\ndiscussion around three core components: shape representations, generative\nmodeling approaches, and evaluation protocols. We begin by categorizing 3D\nrepresentations into explicit, implicit, and hybrid setups, highlighting their\nstructural properties, advantages, and limitations. Next, we review a wide\nrange of generation methods, focusing on feedforward architectures. We further\nsummarize commonly used datasets and evaluation metrics that assess fidelity,\ndiversity, and realism of generated shapes. Finally, we identify open\nchallenges and outline future research directions that could drive progress in\ncontrollable, efficient, and high-quality 3D shape generation. This survey aims\nto serve as a valuable reference for researchers and practitioners seeking a\nstructured and in-depth understanding of this rapidly evolving field.", "comment": "20 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2506.22678v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22678v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.23817", "title": "Statistical Modeling for Accurate Characterization of Doppler Effect in LEO-Terrestrial Networks", "authors": ["Islam M. Tanash", "Risto Wichman", "Nuria Gonzalez-Prelcic"], "summary": "Low Earth Orbit (LEO) satellite communication is a promising solution for\nglobal wireless coverage, especially in underserved and remote areas. However,\nthe high relative velocity of LEO satellites induces significant Doppler shifts\nthat disrupt subcarrier orthogonality and degrade multicarrier system\nperformance. While the common time-varying Doppler shift can be compensated\nrelative to a reference point, the residual differential Doppler across users\nwithin the coverage cell remains a significant challenge, causing severe\nintercarrier interference. This paper presents a generalized analytical\nframework for characterizing both the Doppler shift magnitude and the\ndifferential Doppler in LEO systems. Unlike prior works limited by flat-Earth\nassumptions or specific orbital configurations, our model incorporates Earth's\ncurvature and supports arbitrary elevation angles. Using spherical geometry, we\nderive closed-form expressions for Doppler shift based on the central angle\nbetween the satellite and ground users. We further provide a statistical\ncharacterization of both the Doppler shift magnitude and the differential\nDoppler in terms of their cumulative distribution function (CDF) and\nprobability density function (PDF) for uniformly distributed users within a\nspherical cap cell. Additionally, we derive a tight upper bound for the Doppler\nshift CDF and an exact expression for the maximum differential Doppler\nexperienced across the coverage region. To mitigate intra-cell Doppler\nvariation, we implement a user clustering technique that partitions the\ncoverage area based on a Doppler disparity threshold into spherical sub-cells,\nensuring compliance with 3GPP tolerances. Extensive simulations over realistic\nsatellite constellations validate our analysis and reveal the impact of\naltitude, beamwidth, and satellite-user geometry on Doppler behavior.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23817v1", "categories": ["eess.SY", "cs.SY"], "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.23817v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22935", "title": "Differentiable Radar Ambiguity Functions: Mathematical Formulation and Computational Implementation", "authors": ["Marc Bara Iniesta"], "summary": "The ambiguity function is fundamental to radar waveform design,\ncharacterizing range and Doppler resolution capabilities. However, its\ntraditional formulation involves non-differentiable operations, preventing\nintegration with gradient-based optimization methods and modern machine\nlearning frameworks. This paper presents the first complete mathematical\nframework and computational implementation for differentiable radar ambiguity\nfunctions. Our approach addresses the fundamental technical challenges that\nhave prevented the radar community from leveraging automatic differentiation:\nproper handling of complex-valued gradients using Wirtinger calculus, efficient\ncomputation through parallelized FFT operations, numerical stability throughout\ncascaded operations, and composability with arbitrary differentiable\noperations. We term this approach GRAF (Gradient-based Radar Ambiguity\nFunctions), which reformulates the ambiguity function computation to maintain\nmathematical equivalence while enabling gradient flow through the entire\npipeline. The resulting implementation provides a general-purpose\ndifferentiable ambiguity function compatible with modern automatic\ndifferentiation frameworks, enabling new research directions including neural\nnetwork-based waveform generation with ambiguity constraints, end-to-end\noptimization of radar systems, and integration of classical radar theory with\nmodern deep learning. We provide complete implementation details and\ndemonstrate computational efficiency suitable for practical applications. This\nwork establishes the mathematical and computational foundation for applying\nmodern machine learning techniques to radar waveform design, bridging classical\nradar signal processing with automatic differentiation frameworks.", "comment": "16 pages, 4 figures, source code available at\n  https://github.com/marcbara/graf-psl-lpi (DOI: 10.5281/zenodo.15763301)", "pdf_url": "http://arxiv.org/pdf/2506.22935v1", "categories": ["eess.SP", "cs.LG", "cs.NA", "math.NA", "94A12, 65T50, 68T05", "F.2.1; I.2.6; G.1.0"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.22935v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23700", "title": "MedSAM-CA: A CNN-Augmented ViT with Attention-Enhanced Multi-Scale Fusion for Medical Image Segmentation", "authors": ["Peiting Tian", "Xi Chen", "Haixia Bi", "Fan Li"], "summary": "Medical image segmentation plays a crucial role in clinical diagnosis and\ntreatment planning, where accurate boundary delineation is essential for\nprecise lesion localization, organ identification, and quantitative assessment.\nIn recent years, deep learning-based methods have significantly advanced\nsegmentation accuracy. However, two major challenges remain. First, the\nperformance of these methods heavily relies on large-scale annotated datasets,\nwhich are often difficult to obtain in medical scenarios due to privacy\nconcerns and high annotation costs. Second, clinically challenging scenarios,\nsuch as low contrast in certain imaging modalities and blurry lesion boundaries\ncaused by malignancy, still pose obstacles to precise segmentation. To address\nthese challenges, we propose MedSAM-CA, an architecture-level fine-tuning\napproach that mitigates reliance on extensive manual annotations by adapting\nthe pretrained foundation model, Medical Segment Anything (MedSAM). MedSAM-CA\nintroduces two key components: the Convolutional Attention-Enhanced Boundary\nRefinement Network (CBR-Net) and the Attention-Enhanced Feature Fusion Block\n(Atte-FFB). CBR-Net operates in parallel with the MedSAM encoder to recover\nboundary information potentially overlooked by long-range attention mechanisms,\nleveraging hierarchical convolutional processing. Atte-FFB, embedded in the\nMedSAM decoder, fuses multi-level fine-grained features from skip connections\nin CBR-Net with global representations upsampled within the decoder to enhance\nboundary delineation accuracy. Experiments on publicly available datasets\ncovering dermoscopy, CT, and MRI imaging modalities validate the effectiveness\nof MedSAM-CA. On dermoscopy dataset, MedSAM-CA achieves 94.43% Dice with only\n2% of full training data, reaching 97.25% of full-data training performance,\ndemonstrating strong effectiveness in low-resource clinical settings.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23700v1", "categories": ["eess.IV", "cs.CV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.23700v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22780", "title": "Multimodal Atmospheric Super-Resolution With Deep Generative Models", "authors": ["Dibyajyoti Chakraborty", "Haiwen Guan", "Jason Stock", "Troy Arcomano", "Guido Cervone", "Romit Maulik"], "summary": "Score-based diffusion modeling is a generative machine learning algorithm\nthat can be used to sample from complex distributions. They achieve this by\nlearning a score function, i.e., the gradient of the log-probability density of\nthe data, and reversing a noising process using the same. Once trained,\nscore-based diffusion models not only generate new samples but also enable\nzero-shot conditioning of the generated samples on observed data. This promises\na novel paradigm for data and model fusion, wherein the implicitly learned\ndistributions of pretrained score-based diffusion models can be updated given\nthe availability of online data in a Bayesian formulation. In this article, we\napply such a concept to the super-resolution of a high-dimensional dynamical\nsystem, given the real-time availability of low-resolution and experimentally\nobserved sparse sensor measurements from multimodal data. Additional analysis\non how score-based sampling can be used for uncertainty estimates is also\nprovided. Our experiments are performed for a super-resolution task that\ngenerates the ERA5 atmospheric dataset given sparse observations from a\ncoarse-grained representation of the same and/or from unstructured experimental\nobservations of the IGRA radiosonde dataset. We demonstrate accurate recovery\nof the high dimensional state given multiple sources of low-fidelity\nmeasurements. We also discover that the generative model can balance the\ninfluence of multiple dataset modalities during spatiotemporal reconstructions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22780v1", "categories": ["cs.LG", "physics.geo-ph"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22780v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23549", "title": "CooT: Learning to Coordinate In-Context with Coordination Transformers", "authors": ["Huai-Chih Wang", "Hsiang-Chun Chuang", "Hsi-Chun Cheng", "Dai-Jie Wu", "Shao-Hua Sun"], "summary": "Effective coordination among artificial agents in dynamic and uncertain\nenvironments remains a significant challenge in multi-agent systems. Existing\napproaches, such as self-play and population-based methods, either generalize\npoorly to unseen partners or require extensive training. To overcome these\nlimitations, we propose Coordination Transformers (CooT), a novel in-context\ncoordination framework that uses recent interaction histories to adapt to\nunseen partners rapidly. Unlike previous approaches that primarily aim to\nincrease the diversity of training partners, CooT explicitly focuses on\nadapting to new partner behaviors by predicting actions aligned with observed\npartner interactions. Trained on interaction trajectories collected from\ndiverse pairs of agents with complementary behaviors, CooT quickly learns\neffective coordination strategies without explicit supervision or fine-tuning.\nEvaluations on the Overcooked benchmark demonstrate that CooT significantly\noutperforms baseline methods in coordination tasks involving previously unseen\npartners. Human evaluations further confirm CooT as the most effective\ncollaborative partner, while extensive ablations highlight its robustness,\nflexibility, and sensitivity to context in multi-agent scenarios.", "comment": "23 pages, 10 tables, 8 figures", "pdf_url": "http://arxiv.org/pdf/2506.23549v1", "categories": ["cs.AI", "cs.HC", "cs.LG"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.23549v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23986", "title": "StreamFlow: Streaming Flow Matching with Block-wise Guided Attention Mask for Speech Token Decoding", "authors": ["Dake Guo", "Jixun Yao", "Linhan Ma", "Wang He", "Lei Xie"], "summary": "Recent advancements in discrete token-based speech generation have\nhighlighted the importance of token-to-waveform generation for audio quality,\nparticularly in real-time interactions. Traditional frameworks integrating\nsemantic tokens with flow matching (FM) struggle with streaming capabilities\ndue to their reliance on a global receptive field. Additionally, directly\nimplementing token-by-token streaming speech generation often results in\ndegraded audio quality. To address these challenges, we propose StreamFlow, a\nnovel neural architecture that facilitates streaming flow matching with\ndiffusion transformers (DiT). To mitigate the long-sequence extrapolation\nissues arising from lengthy historical dependencies, we design a local\nblock-wise receptive field strategy. Specifically, the sequence is first\nsegmented into blocks, and we introduce block-wise attention masks that enable\nthe current block to receive information from the previous or subsequent block.\nThese attention masks are combined hierarchically across different DiT-blocks\nto regulate the receptive field of DiTs. Both subjective and objective\nexperimental results demonstrate that our approach achieves performance\ncomparable to non-streaming methods while surpassing other streaming methods in\nterms of speech quality, all the while effectively managing inference time\nduring long-sequence generation. Furthermore, our method achieves a notable\nfirst-packet latency of only 180 ms.\\footnote{Speech samples:\nhttps://dukguo.github.io/StreamFlow/}", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23986v1", "categories": ["cs.SD", "eess.AS"], "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.23986v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22957", "title": "Agent-to-Agent Theory of Mind: Testing Interlocutor Awareness among Large Language Models", "authors": ["Younwoo Choi", "Changling Li", "Yongjin Yang", "Zhijing Jin"], "summary": "As large language models (LLMs) are increasingly integrated into multi-agent\nand human-AI systems, understanding their awareness of both self-context and\nconversational partners is essential for ensuring reliable performance and\nrobust safety. While prior work has extensively studied situational awareness\nwhich refers to an LLM's ability to recognize its operating phase and\nconstraints, it has largely overlooked the complementary capacity to identify\nand adapt to the identity and characteristics of a dialogue partner. In this\npaper, we formalize this latter capability as interlocutor awareness and\npresent the first systematic evaluation of its emergence in contemporary LLMs.\nWe examine interlocutor inference across three dimensions-reasoning patterns,\nlinguistic style, and alignment preferences-and show that LLMs reliably\nidentify same-family peers and certain prominent model families, such as GPT\nand Claude. To demonstrate its practical significance, we develop three case\nstudies in which interlocutor awareness both enhances multi-LLM collaboration\nthrough prompt adaptation and introduces new alignment and safety\nvulnerabilities, including reward-hacking behaviors and increased jailbreak\nsusceptibility. Our findings highlight the dual promise and peril of\nidentity-sensitive behavior in LLMs, underscoring the need for further\nunderstanding of interlocutor awareness and new safeguards in multi-agent\ndeployments. Our code is open-sourced at\nhttps://github.com/younwoochoi/InterlocutorAwarenessLLM.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22957v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.MA"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.22957v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.22553", "title": "On a result by Meshulam", "authors": ["Heinz H. Bauschke", "Tran Thanh Tung"], "summary": "In 1996, Meshulam proved that every sequence generated by applying\nprojections onto affine subspaces, drawn from a finite collection in Euclidean\nspace, must be bounded.\n  In this paper, we extend his result not only from affine subspaces to convex\npolyhedral subsets, but also from Euclidean to general Hilbert space. Various\nexamples are provided to illustrate the sharpness of the results.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22553v1", "categories": ["math.OC", "cs.NA", "math.FA", "math.NA", "47H09, 65K05, 90C25 (Primary) 52A37, 52B55 (Secondary)"], "cate": "math.OC", "url": "http://arxiv.org/abs/2506.22553v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.23682", "title": "Not quite a piece of CHERI-cake: Are new digital security by design architectures usable?", "authors": ["Maysara Alhindi", "Joseph Hallett"], "summary": "A digital security-by-design computer architecture, like CHERI, lets you\nprogram without fear of buffer overflows or other memory safety errors, but\nCHERI also rewrites some of the assumptions about how C works and how\nfundamental types (such as pointers) are implemented in hardware. We conducted\na usability study to examine how developers react to the changes required by\nCHERI when porting software to run on it. We find that developers struggle with\nCHERI's display of warnings and errors and a lack of diverse documentation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23682v1", "categories": ["cs.CR", "cs.AR", "cs.HC"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.23682v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23400", "title": "A Model Predictive Control Framework to Enhance Safety and Quality in Mobile Additive Manufacturing Systems", "authors": ["Yifei Li", "Joshua A. Robbins", "Guha Manogharan", "Herschel C. Pangborn", "Ilya Kovalenko"], "summary": "In recent years, the demand for customized, on-demand production has grown in\nthe manufacturing sector. Additive Manufacturing (AM) has emerged as a\npromising technology to enhance customization capabilities, enabling greater\nflexibility, reduced lead times, and more efficient material usage. However,\ntraditional AM systems remain constrained by static setups and human worker\ndependencies, resulting in long lead times and limited scalability. Mobile\nrobots can improve the flexibility of production systems by transporting\nproducts to designated locations in a dynamic environment. By integrating AM\nsystems with mobile robots, manufacturers can optimize travel time for\npreparatory tasks and distributed printing operations. Mobile AM robots have\nbeen deployed for on-site production of large-scale structures, but often\nneglect critical print quality metrics like surface roughness. Additionally,\nthese systems do not have the precision necessary for producing small,\nintricate components. We propose a model predictive control framework for a\nmobile AM platform that ensures safe navigation on the plant floor while\nmaintaining high print quality in a dynamic environment. Three case studies are\nused to test the feasibility and reliability of the proposed systems.", "comment": "2025 IEEE 21st International Conference on Automation Science and\n  Engineering", "pdf_url": "http://arxiv.org/pdf/2506.23400v1", "categories": ["cs.RO", "cs.SY", "eess.SY"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.23400v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.22884", "title": "Performance Measurements in the AI-Centric Computing Continuum Systems", "authors": ["Praveen Kumar Donta", "Qiyang Zhang", "Schahram Dustdar"], "summary": "Over the Eight decades, computing paradigms have shifted from large,\ncentralized systems to compact, distributed architectures, leading to the rise\nof the Distributed Computing Continuum (DCC). In this model, multiple layers\nsuch as cloud, edge, Internet of Things (IoT), and mobile platforms work\ntogether to support a wide range of applications. Recently, the emergence of\nGenerative AI and large language models has further intensified the demand for\ncomputational resources across this continuum. Although traditional performance\nmetrics have provided a solid foundation, they need to be revisited and\nexpanded to keep pace with changing computational demands and application\nrequirements. Accurate performance measurements benefit both system designers\nand users by supporting improvements in efficiency and promoting alignment with\nsystem goals. In this context, we review commonly used metrics in DCC and IoT\nenvironments. We also discuss emerging performance dimensions that address\nevolving computing needs, such as sustainability, energy efficiency, and system\nobservability. We also outline criteria and considerations for selecting\nappropriate metrics, aiming to inspire future research and development in this\ncritical area.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22884v1", "categories": ["cs.DC", "cs.AI", "cs.ET", "cs.NI", "cs.SY", "eess.SY"], "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.22884v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23866", "title": "Exploring Privacy and Security as Drivers for Environmental Sustainability in Cloud-Based Office Solutions", "authors": ["Jason Kayembe", "Iness Ben Guirat", "Jan Tobias M√ºhlberg"], "summary": "In this paper, we explore the intersection of privacy, security, and\nenvironmental sustainability in cloud-based office solutions, focusing on\nquantifying user- and network-side energy use and associated carbon emissions.\nWe hypothesise that privacy-focused services are typically more\nenergy-efficient than those funded through data collection and advertising. To\nevaluate this, we propose a framework that systematically measures\nenvironmental costs based on energy usage and network data traffic during\nwell-defined, automated usage scenarios. To test our hypothesis, we first\nanalyse how underlying architectures and business models, such as monetisation\nthrough personalised advertising, contribute to the environmental footprint of\nthese services. We then explore existing methodologies and tools for software\nenvironmental impact assessment. We apply our framework to three mainstream\nemail services selected to reflect different privacy policies, from\nad-supported tracking-intensive models to privacy-focused designs: Microsoft\nOutlook, Google Mail (Gmail), and Proton Mail. We extend this comparison to a\nself-hosted email solution, evaluated with and without end-to-end encryption.\nWe show that the self-hosted solution, even with 14% of device energy and 15%\nof emissions overheads from PGP encryption, remains the most energy-efficient,\nsaving up to 33% of emissions per session compared to Gmail. Among commercial\nproviders, Proton Mail is the most efficient, saving up to 0.1 gCO2 e per\nsession compared to Outlook, whose emissions can be further reduced by 2%\nthrough ad-blocking.", "comment": "Post-proceedings paper persented at LOCO '24: 1st International\n  Workshop on Low Carbon Computing, 2024-12-03, in Glasgow, UK", "pdf_url": "http://arxiv.org/pdf/2506.23866v1", "categories": ["cs.CR", "cs.CY", "cs.SE"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.23866v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23850", "title": "Email as the Interface to Generative AI Models: Seamless Administrative Automation", "authors": ["Andres Navarro", "Carlos de Quinto", "Jos√© Alberto Hern√°ndez"], "summary": "This paper introduces a novel architectural framework that integrates Large\nLanguage Models (LLMs) with email interfaces to automate administrative tasks,\nspecifically targeting accessibility barriers in enterprise environments. The\nsystem connects email communication channels with Optical Character Recognition\n(OCR) and intelligent automation, enabling non-technical administrative staff\nto delegate complex form-filling and document processing tasks using familiar\nemail interfaces. By treating the email body as a natural language prompt and\nattachments as contextual information, the workflow bridges the gap between\nadvanced AI capabilities and practical usability. Empirical evaluation shows\nthat the system can complete complex administrative forms in under 8 seconds of\nautomated processing, with human supervision reducing total staff time by a\nfactor of three to four compared to manual workflows. The top-performing LLM\naccurately filled 16 out of 29 form fields and reduced the total cost per\nprocessed form by 64% relative to manual completion. These findings demonstrate\nthat email-based LLM integration is a viable and cost-effective approach for\ndemocratizing advanced automation in organizational settings, supporting\nwidespread adoption without requiring specialized technical knowledge or major\nworkflow changes. This aligns with broader trends in leveraging LLMs to enhance\naccessibility and automate complex tasks for non-technical users, making\ntechnology more inclusive and efficient.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23850v1", "categories": ["cs.HC"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.23850v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22946", "title": "Modular versus Hierarchical: A Structural Signature of Topic Popularity in Mathematical Research", "authors": ["Brian Hepler"], "summary": "Mathematical researchers, especially those in early-career positions, face\ncritical decisions about topic specialization with limited information about\nthe collaborative environments of different research areas. The aim of this\npaper is to study how the popularity of a research topic is associated with the\nstructure of that topic's collaboration network, as observed by a suite of\nmeasures capturing organizational structure at several scales. We apply these\nmeasures to 1,938 algorithmically discovered topics across 121,391 papers\nsourced from arXiv metadata during the period 2020--2025. Our analysis, which\ncontrols for the confounding effects of network size, reveals a structural\ndichotomy--we find that popular topics organize into modular \"schools of\nthought,\" while niche topics maintain hierarchical core-periphery structures\ncentered around established experts. This divide is not an artifact of scale,\nbut represents a size-independent structural pattern correlated with\npopularity. We also document a \"constraint reversal\": after controlling for\nsize, researchers in popular fields face greater structural constraints on\ncollaboration opportunities, contrary to conventional expectations. Our\nfindings suggest that topic selection is an implicit choice between two\nfundamentally different collaborative environments, each with distinct\nimplications for a researcher's career. To make these structural patterns\ntransparent to the research community, we developed the Math Research Compass\n(https://mathresearchcompass.com), an interactive platform providing data on\ntopic popularity and collaboration patterns across mathematical topics.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22946v1", "categories": ["cs.SI", "cs.CY", "cs.DL", "math.HO", "01A80, 91D30, 05C82, 62R07"], "cate": "cs.SI", "url": "http://arxiv.org/abs/2506.22946v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.22710", "title": "LightBSR: Towards Lightweight Blind Super-Resolution via Discriminative Implicit Degradation Representation Learning", "authors": ["Jiang Yuan", "JI Ma", "Bo Wang", "Guanzhou Ke", "Weiming Hu"], "summary": "Implicit degradation estimation-based blind super-resolution (IDE-BSR) hinges\non extracting the implicit degradation representation (IDR) of the LR image and\nadapting it to LR image features to guide HR detail restoration. Although\nIDE-BSR has shown potential in dealing with noise interference and complex\ndegradations, existing methods ignore the importance of IDR discriminability\nfor BSR and instead over-complicate the adaptation process to improve effect,\nresulting in a significant increase in the model's parameters and computations.\nIn this paper, we focus on the discriminability optimization of IDR and propose\na new powerful and lightweight BSR model termed LightBSR. Specifically, we\nemploy a knowledge distillation-based learning framework. We first introduce a\nwell-designed degradation-prior-constrained contrastive learning technique\nduring teacher stage to make the model more focused on distinguishing different\ndegradation types. Then we utilize a feature alignment technique to transfer\nthe degradation-related knowledge acquired by the teacher to the student for\npractical inferencing. Extensive experiments demonstrate the effectiveness of\nIDR discriminability-driven BSR model design. The proposed LightBSR can achieve\noutstanding performance with minimal complexity across a range of blind SR\ntasks. Our code is accessible at: https://github.com/MJ-NCEPU/LightBSR.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22710v1", "categories": ["cs.CV", "eess.IV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22710v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.24017", "title": "Orchestrated Couplings: A Time-Varying Edge Weight Framework for Efficient Event-Triggered Multiagent Networks", "authors": ["Emre Yildirim", "Tansel Yucelen", "Arman Sargolzaei"], "summary": "In this paper, we focus on reducing node-to-node information exchange in\ndistributed control of multiagent networks while improving the overall network\nperformance. Specifically, we consider a multiagent network that is composed of\nleader and follower nodes over a time-varying, connected, and undirected graph.\nIn contrast to existing works on the event-triggered distributed control\nliterature, we propose a time-varying edge weight event-triggered control\nframework. In this framework, each node dynamically adjusts its edge weights by\nincreasing them during the transient (active) phase and decreasing them during\nthe steady-state (idle) phase of the multiagent network. This not only reduces\nthe number of events in the network but also improves the performance (i.e.,\nconvergence speed and control effort) of the overall multiagent network.\nSystem-theoretically, we first prove the closed-loop stability of the proposed\nevent-triggered distributed control framework, where we then show that this\nframework does not exhibit a Zeno behavior. Finally, illustrative numerical\nexamples are provided to demonstrate the efficacy of this framework.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.24017v1", "categories": ["eess.SY", "cs.SY"], "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.24017v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22943", "title": "Rate Maximization for Fluid Antenna System Assisted Semantic Communication", "authors": ["Siyun Liang", "Chen Zhu", "Zhaohui Yang", "Changsheng You", "Dusit Niyato", "Kai-Kit Wong", "Zhaoyang Zhang"], "summary": "In this paper, we investigate the problem of rate maximization in a fluid\nantenna system (FAS) assisted\n  semantic communication system. In the considered model, a base station (BS)\nwith multiple static antennas employs semantic extraction techniques to\ncompress the data ready to be sent to a user. The user equipped with a fluid\nantenna is located in the near field coverage region of the BS. Our aim is to\njointly optimize the transmit beamforming and the semantic compression rate at\nthe BS, as well as the selection of activated ports in FAS, to maximize the\nequivalent transmission ratio under a specific power budget. We design an\nalternating algorithm to solve the problem, where we obtain the optimal\nsemantic compression ratio is in closed form at each step. Simulation results\nvalidate the effectiveness of the proposed algorithm.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22943v1", "categories": ["eess.SP"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.22943v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23701", "title": "MDPG: Multi-domain Diffusion Prior Guidance for MRI Reconstruction", "authors": ["Lingtong Zhang", "Mengdie Song", "Xiaohan Hao", "Huayu Mai", "Bensheng Qiu"], "summary": "Magnetic Resonance Imaging (MRI) reconstruction is essential in medical\ndiagnostics. As the latest generative models, diffusion models (DMs) have\nstruggled to produce high-fidelity images due to their stochastic nature in\nimage domains. Latent diffusion models (LDMs) yield both compact and detailed\nprior knowledge in latent domains, which could effectively guide the model\ntowards more effective learning of the original data distribution. Inspired by\nthis, we propose Multi-domain Diffusion Prior Guidance (MDPG) provided by\npre-trained LDMs to enhance data consistency in MRI reconstruction tasks.\nSpecifically, we first construct a Visual-Mamba-based backbone, which enables\nefficient encoding and reconstruction of under-sampled images. Then pre-trained\nLDMs are integrated to provide conditional priors in both latent and image\ndomains. A novel Latent Guided Attention (LGA) is proposed for efficient fusion\nin multi-level latent domains. Simultaneously, to effectively utilize a prior\nin both the k-space and image domain, under-sampled images are fused with\ngenerated full-sampled images by the Dual-domain Fusion Branch (DFB) for\nself-adaption guidance. Lastly, to further enhance the data consistency, we\npropose a k-space regularization strategy based on the non-auto-calibration\nsignal (NACS) set. Extensive experiments on two public MRI datasets fully\ndemonstrate the effectiveness of the proposed methodology. The code is\navailable at https://github.com/Zolento/MDPG.", "comment": "Accept by MICCAI2025", "pdf_url": "http://arxiv.org/pdf/2506.23701v1", "categories": ["eess.IV", "cs.CV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.23701v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22802", "title": "Riemannian-Geometric Fingerprints of Generative Models", "authors": ["Hae Jin Song", "Laurent Itti"], "summary": "Recent breakthroughs and rapid integration of generative models (GMs) have\nsparked interest in the problem of model attribution and their fingerprints.\nFor instance, service providers need reliable methods of authenticating their\nmodels to protect their IP, while users and law enforcement seek to verify the\nsource of generated content for accountability and trust. In addition, a\ngrowing threat of model collapse is arising, as more model-generated data are\nbeing fed back into sources (e.g., YouTube) that are often harvested for\ntraining (\"regurgitative training\"), heightening the need to differentiate\nsynthetic from human data. Yet, a gap still exists in understanding generative\nmodels' fingerprints, we believe, stemming from the lack of a formal framework\nthat can define, represent, and analyze the fingerprints in a principled way.\nTo address this gap, we take a geometric approach and propose a new definition\nof artifact and fingerprint of GMs using Riemannian geometry, which allows us\nto leverage the rich theory of differential geometry. Our new definition\ngeneralizes previous work (Song et al., 2024) to non-Euclidean manifolds by\nlearning Riemannian metrics from data and replacing the Euclidean distances and\nnearest-neighbor search with geodesic distances and kNN-based Riemannian center\nof mass. We apply our theory to a new gradient-based algorithm for computing\nthe fingerprints in practice. Results show that it is more effective in\ndistinguishing a large array of GMs, spanning across 4 different datasets in 2\ndifferent resolutions (64 by 64, 256 by 256), 27 model architectures, and 2\nmodalities (Vision, Vision-Language). Using our proposed definition\nsignificantly improves the performance on model attribution, as well as a\ngeneralization to unseen datasets, model types, and modalities, suggesting its\npractical efficacy.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22802v1", "categories": ["cs.LG", "cs.CR", "cs.CV", "I.2.6"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22802v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23563", "title": "MMReason: An Open-Ended Multi-Modal Multi-Step Reasoning Benchmark for MLLMs Toward AGI", "authors": ["Huanjin Yao", "Jiaxing Huang", "Yawen Qiu", "Michael K. Chen", "Wenzheng Liu", "Wei Zhang", "Wenjie Zeng", "Xikun Zhang", "Jingyi Zhang", "Yuxin Song", "Wenhao Wu", "Dacheng Tao"], "summary": "Reasoning plays a crucial role in advancing Multimodal Large Language Models\n(MLLMs) toward Artificial General Intelligence. However, existing MLLM\nbenchmarks often fall short in precisely and comprehensively evaluating\nlong-chain reasoning abilities from three key aspects: (1) lack of difficulty\nand diversity, (2) susceptibility to guessability and memorization, (3)\ninadequate assessment of intermediate reasoning steps. To fill this gap, we\nintroduce MMReason, a new benchmark designed to precisely and comprehensively\nevaluate MLLM long-chain reasoning capability with diverse, open-ended,\nchallenging questions. First, we curate challenging questions requiring\nmulti-step reasoning from various fields (i.e., 6 disciplines) and multiple\ndifficulty levels (i.e., from pre-university to university, and from\nfoundational to competition tiers). Second, these questions are reformulated\ninto an open-ended format and filtered using a multi-model voting technique to\neliminate shortcut cases related to guessing and memorization, ensuring robust\nreasoning evaluations. Third, we annotate the questions with detailed\nstep-by-step solutions, and design a reference-based ternary scoring mechanism\nto reliably assess intermediate reasoning steps. With MMReason, we benchmark\npopular leading MLLMs and provide an in-depth analysis of their reasoning\ncapabilities. We hope MMReason will serve as a valuable resource for advancing\nMLLM reasoning research. Code will be available at\nhttps://github.com/HJYao00/MMReason.", "comment": "Technical report", "pdf_url": "http://arxiv.org/pdf/2506.23563v1", "categories": ["cs.AI", "cs.CL", "cs.CV"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.23563v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22977", "title": "On the Generalizability of \"Competition of Mechanisms: Tracing How Language Models Handle Facts and Counterfactuals\"", "authors": ["Asen Dotsinski", "Udit Thakur", "Marko Ivanov", "Mohammad Hafeez Khan", "Maria Heuss"], "summary": "We present a reproduction study of \"Competition of Mechanisms: Tracing How\nLanguage Models Handle Facts and Counterfactuals\" (Ortu et al., 2024), which\ninvestigates competition of mechanisms in language models between factual\nrecall and counterfactual in-context repetition. Our study successfully\nreproduces their primary findings regarding the localization of factual and\ncounterfactual information, the dominance of attention blocks in mechanism\ncompetition, and the specialization of attention heads in handling competing\ninformation. We reproduce their results on both GPT-2 (Radford et al., 2019)\nand Pythia 6.9B (Biderman et al., 2023). We extend their work in three\nsignificant directions. First, we explore the generalizability of these\nfindings to even larger models by replicating the experiments on Llama 3.1 8B\n(Grattafiori et al., 2024), discovering greatly reduced attention head\nspecialization. Second, we investigate the impact of prompt structure by\nintroducing variations where we avoid repeating the counterfactual statement\nverbatim or we change the premise word, observing a marked decrease in the\nlogit for the counterfactual token. Finally, we test the validity of the\nauthors' claims for prompts of specific domains, discovering that certain\ncategories of prompts skew the results by providing the factual prediction\ntoken as part of the subject of the sentence. Overall, we find that the\nattention head ablation proposed in Ortu et al. (2024) is ineffective for\ndomains that are underrepresented in their dataset, and that the effectiveness\nvaries based on model architecture, prompt structure, domain and task.", "comment": "22 pages, 25 figures. For an interactive dashboard with all figures,\n  see https://comp-mech-generalizability.streamlit.app/ . For the accompanying\n  code, see https://github.com/asendotsinski/comp-mech-generalizability . To be\n  published in proceedings of the 2025 Machine Learning Reproducibility\n  Challenge", "pdf_url": "http://arxiv.org/pdf/2506.22977v1", "categories": ["cs.CL", "cs.LG"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.22977v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.22564", "title": "Efficient Tensor Decomposition via Moment Matrix Extension", "authors": ["Bobby Shi", "Julia Lindberg", "Joe Kileel"], "summary": "Motivated by a flurry of recent work on efficient tensor decomposition\nalgorithms, we show that the celebrated moment matrix extension algorithm of\nBrachat, Comon, Mourrain, and Tsigaridas for symmetric tensor canonical\npolyadic (CP) decomposition can be made efficient under the right conditions.\nWe first show that the crucial property determining the complexity of the\nalgorithm is the regularity of a target decomposition. This allows us to reduce\nthe complexity of the vanilla algorithm, while also unifying results from\nprevious works. We then show that for tensors in $S^d\\mathbb{C}^{n+1}$ with $d$\neven, low enough regularity can reduce finding a symmetric tensor decomposition\nto solving a system of linear equations. For order-$4$ tensors we prove that\ngeneric tensors of rank up to $r=2n+1$ can be decomposed efficiently via moment\nmatrix extension, exceeding the rank threshold allowed by simultaneous\ndiagonalization. We then formulate a conjecture that states for generic\norder-$4$ tensors of rank $r=O(n^2)$ the induced linear system is sufficient\nfor efficient tensor decomposition, matching the asymptotics of existing\nalgorithms and in fact improving the leading coefficient. Towards this\nconjecture we give computer assisted proofs that the statement holds for $n=2,\n\\dots, 17$. Next we demonstrate that classes of nonidentifiable tensors can be\ndecomposed efficiently via the moment matrix extension algorithm, bypassing the\nusual need for uniqueness of decomposition. Of particular interest is the class\nof monomials, for which the extension algorithm is not only efficient but also\nimproves on existing theory by explicitly parameterizing the space of\ndecompositions. Code for implementations of the efficient algorithm for generic\ntensors and monomials are provided, along with several numerical examples.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22564v1", "categories": ["math.AG", "cs.NA", "cs.SC", "math.NA"], "cate": "math.AG", "url": "http://arxiv.org/abs/2506.22564v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.23683", "title": "Threadbox: Sandboxing for Modular Security", "authors": ["Maysara Alhindi", "Joseph Hallett"], "summary": "There are many sandboxing mechanisms provided by operating systems to limit\nwhat resources applications can access, however, sometimes the use of these\nmechanisms requires developers to refactor their code to fit the sandboxing\nmodel. In this work, we investigate what makes existing sandboxing mechanisms\nchallenging to apply to certain types of applications, and propose Threadbox, a\nsandboxing mechanism that enables having modular and independent sandboxes, and\ncan be applied to threads and sandbox specific functions. We present case\nstudies to illustrate the applicability of the idea and discuss its\nlimitations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23683v1", "categories": ["cs.CR", "cs.OS", "cs.SE"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.23683v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23433", "title": "Risk-Based Filtering of Valuable Driving Situations in the Waymo Open Motion Dataset", "authors": ["Tim Puphal", "Vipul Ramtekkar", "Kenji Nishimiya"], "summary": "Improving automated vehicle software requires driving data rich in valuable\nroad user interactions. In this paper, we propose a risk-based filtering\napproach that helps identify such valuable driving situations from large\ndatasets. Specifically, we use a probabilistic risk model to detect high-risk\nsituations. Our method stands out by considering a) first-order situations\n(where one vehicle directly influences another and induces risk) and b)\nsecond-order situations (where influence propagates through an intermediary\nvehicle). In experiments, we show that our approach effectively selects\nvaluable driving situations in the Waymo Open Motion Dataset. Compared to the\ntwo baseline interaction metrics of Kalman difficulty and Tracks-To-Predict\n(TTP), our filtering approach identifies complex and complementary situations,\nenriching the quality in automated vehicle testing. The risk data is made\nopen-source: https://github.com/HRI-EU/RiskBasedFiltering.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23433v1", "categories": ["cs.RO"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.23433v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23435", "title": "All Proof of Work But No Proof of Play", "authors": ["Hayder Tirmazi"], "summary": "Speedrunning is a competition that emerged from communities of early video\ngames such as Doom (1993). Speedrunners try to finish a game in minimal time.\nProvably verifying the authenticity of submitted speedruns is an open problem.\nTraditionally, best-effort speedrun verification is conducted by on-site human\nobservers, forensic audio analysis, or a rigorous mathematical analysis of the\ngame mechanics. Such methods are tedious, fallible, and, perhaps worst of all,\nnot cryptographic. Motivated by naivety and the Dunning-Kruger effect, we\nattempt to build a system that cryptographically proves the authenticity of\nspeedruns. This paper describes our attempted solutions and ways to circumvent\nthem. Through a narration of our failures, we attempt to demonstrate the\ndifficulty of authenticating live and interactive human input in untrusted\nenvironments, as well as the limits of signature schemes, game integrity, and\nprovable play.", "comment": "Published in CFAIL 2025", "pdf_url": "http://arxiv.org/pdf/2506.23435v1", "categories": ["cs.CR", "cs.NI"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.23435v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23960", "title": "ADReFT: Adaptive Decision Repair for Safe Autonomous Driving via Reinforcement Fine-Tuning", "authors": ["Mingfei Cheng", "Xiaofei Xie", "Renzhi Wang", "Yuan Zhou", "Ming Hu"], "summary": "Autonomous Driving Systems (ADSs) continue to face safety-critical risks due\nto the inherent limitations in their design and performance capabilities.\nOnline repair plays a crucial role in mitigating such limitations, ensuring the\nruntime safety and reliability of ADSs. Existing online repair solutions\nenforce ADS compliance by transforming unacceptable trajectories into\nacceptable ones based on predefined specifications, such as rule-based\nconstraints or training datasets. However, these approaches often lack\ngeneralizability, adaptability and tend to be overly conservative, resulting in\nineffective repairs that not only fail to mitigate safety risks sufficiently\nbut also degrade the overall driving experience. To address this issue, we\npropose Adaptive Decision Repair (ADReFT), a novel and effective repair method\nthat identifies safety-critical states through offline learning from failed\ntests and generates appropriate mitigation actions to improve ADS safety.\nSpecifically, ADReFT incorporates a transformer-based model with two joint\nheads, State Monitor and Decision Adapter, designed to capture complex driving\nenvironment interactions to evaluate state safety severity and generate\nadaptive repair actions. Given the absence of oracles for state safety\nidentification, we first pretrain ADReFT using supervised learning with coarse\nannotations, i.e., labeling states preceding violations as positive samples and\nothers as negative samples. It establishes ADReFT's foundational capability to\nmitigate safety-critical violations, though it may result in somewhat\nconservative mitigation strategies. Therefore, we subsequently finetune ADReFT\nusing reinforcement learning to improve its initial capability and generate\nmore precise and contextually appropriate repair decisions. Our evaluation\nresults illustrate that ADReFT achieves better repair performance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23960v1", "categories": ["cs.LG", "cs.AI", "cs.SE"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.23960v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23952", "title": "Autonomy by Design: Preserving Human Autonomy in AI Decision-Support", "authors": ["Stefan Buijsman", "Sarah Carter", "Juan Pablo Berm√∫dez"], "summary": "AI systems increasingly support human decision-making across domains of\nprofessional, skill-based, and personal activity. While previous work has\nexamined how AI might affect human autonomy globally, the effects of AI on\ndomain-specific autonomy -- the capacity for self-governed action within\ndefined realms of skill or expertise -- remain understudied. We analyze how AI\ndecision-support systems affect two key components of domain-specific autonomy:\nskilled competence (the ability to make informed judgments within one's domain)\nand authentic value-formation (the capacity to form genuine domain-relevant\nvalues and preferences). By engaging with prior investigations and analyzing\nempirical cases across medical, financial, and educational domains, we\ndemonstrate how the absence of reliable failure indicators and the potential\nfor unconscious value shifts can erode domain-specific autonomy both\nimmediately and over time. We then develop a constructive framework for\nautonomy-preserving AI support systems. We propose specific socio-technical\ndesign patterns -- including careful role specification, implementation of\ndefeater mechanisms, and support for reflective practice -- that can help\nmaintain domain-specific autonomy while leveraging AI capabilities. This\nframework provides concrete guidance for developing AI systems that enhance\nrather than diminish human agency within specialized domains of action.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23952v1", "categories": ["cs.HC", "cs.AI", "cs.LG", "econ.GN", "q-fin.EC"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.23952v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22957", "title": "Agent-to-Agent Theory of Mind: Testing Interlocutor Awareness among Large Language Models", "authors": ["Younwoo Choi", "Changling Li", "Yongjin Yang", "Zhijing Jin"], "summary": "As large language models (LLMs) are increasingly integrated into multi-agent\nand human-AI systems, understanding their awareness of both self-context and\nconversational partners is essential for ensuring reliable performance and\nrobust safety. While prior work has extensively studied situational awareness\nwhich refers to an LLM's ability to recognize its operating phase and\nconstraints, it has largely overlooked the complementary capacity to identify\nand adapt to the identity and characteristics of a dialogue partner. In this\npaper, we formalize this latter capability as interlocutor awareness and\npresent the first systematic evaluation of its emergence in contemporary LLMs.\nWe examine interlocutor inference across three dimensions-reasoning patterns,\nlinguistic style, and alignment preferences-and show that LLMs reliably\nidentify same-family peers and certain prominent model families, such as GPT\nand Claude. To demonstrate its practical significance, we develop three case\nstudies in which interlocutor awareness both enhances multi-LLM collaboration\nthrough prompt adaptation and introduces new alignment and safety\nvulnerabilities, including reward-hacking behaviors and increased jailbreak\nsusceptibility. Our findings highlight the dual promise and peril of\nidentity-sensitive behavior in LLMs, underscoring the need for further\nunderstanding of interlocutor awareness and new safeguards in multi-agent\ndeployments. Our code is open-sourced at\nhttps://github.com/younwoochoi/InterlocutorAwarenessLLM.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22957v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.MA"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.22957v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.22718", "title": "Part Segmentation and Motion Estimation for Articulated Objects with Dynamic 3D Gaussians", "authors": ["Jun-Jee Chao", "Qingyuan Jiang", "Volkan Isler"], "summary": "Part segmentation and motion estimation are two fundamental problems for\narticulated object motion analysis. In this paper, we present a method to solve\nthese two problems jointly from a sequence of observed point clouds of a single\narticulated object. The main challenge in our problem setting is that the point\nclouds are not assumed to be generated by a fixed set of moving points.\nInstead, each point cloud in the sequence could be an arbitrary sampling of the\nobject surface at that particular time step. Such scenarios occur when the\nobject undergoes major occlusions, or if the dataset is collected using\nmeasurements from multiple sensors asynchronously. In these scenarios, methods\nthat rely on tracking point correspondences are not appropriate. We present an\nalternative approach based on a compact but effective representation where we\nrepresent the object as a collection of simple building blocks modeled as 3D\nGaussians. We parameterize the Gaussians with time-dependent rotations,\ntranslations, and scales that are shared across all time steps. With our\nrepresentation, part segmentation can be achieved by building correspondences\nbetween the observed points and the Gaussians. Moreover, the transformation of\neach point across time can be obtained by following the poses of the assigned\nGaussian (even when the point is not observed). Experiments show that our\nmethod outperforms existing methods that solely rely on finding point\ncorrespondences. Additionally, we extend existing datasets to emulate\nreal-world scenarios by considering viewpoint occlusions. We further\ndemonstrate that our method is more robust to missing points as compared to\nexisting approaches on these challenging datasets, even when some parts are\ncompletely occluded in some time-steps. Notably, our part segmentation\nperformance outperforms the state-of-the-art method by 13% on point clouds with\nocclusions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22718v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22718v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.24083", "title": "Time Shift Governor-Guided MPC with Collision Cone CBFs for Safe Adaptive Cruise Control in Dynamic Environments", "authors": ["Robin Inho Kee", "Taehyeun Kim", "Anouck Girard", "Ilya Kolmanovsky"], "summary": "This paper introduces a Time Shift Governor (TSG)-guided Model Predictive\nController with Control Barrier Functions (CBFs)-based constraints for adaptive\ncruise control (ACC). This MPC-CBF approach is defined for obstacle-free curved\nroad tracking, while following distance and obstacle avoidance constraints are\nhandled using standard CBFs and relaxed Collision Cone CBFs. In order to\naddress scenarios involving rapidly moving obstacles or rapidly changing\nleading vehicle's behavior, the TSG augmentation is employed which alters the\ntarget reference to enforce constraints. Simulation results demonstrate the\neffectiveness of the TSG-guided MPC-CBF approach.", "comment": "Robin Inho Kee and Taehyeun Kim contributed equally to this work", "pdf_url": "http://arxiv.org/pdf/2506.24083v1", "categories": ["eess.SY", "cs.SY"], "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.24083v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23045", "title": "Zak-OFDM: Low Complexity Joint Equalization of OFDM Carriers in Doubly-Spread Channels", "authors": ["Saif Khan Mohammed", "Sandesh Rao Mattu", "Nishant Mehrotra", "Venkatesh Khammammetti", "Robert Calderbank"], "summary": "We communicate over wireless channels by first estimating and then equalizing\nthe effective channel. In Zak-OTFS (orthogonal time frequency space) modulation\nthe carrier waveform is a pulse in the delay-Doppler (DD) domain, formally a\nquasi-periodic localized function with specific periods along delay and\nDoppler. When the channel delay spread is less than the delay period, and the\nchannel Doppler spread is less than the Doppler period, the response to a\nsingle Zak-OTFS carrier provides an image of the scattering environment and can\nbe used to predict the effective channel at all other carriers. This makes\nchannel estimation straightforward, and there is no loss in spectral efficiency\nsince it is possible to design data and pilot signals that are mutually\nunbiased. However, the naive approach to equalization has complexity ${\\mathcal\nO}(M^3N^3)$ where $M$ and $N$ are respectively the number of delay and Doppler\nbins in an OTFS frame. We simplify equalization by transforming Zak-OTFS\ninformation symbols to CP-OFDM (cyclic prefix orthogonal frequency division\nmultiplexing) modulation.\n  Why not simply communicate with CP-OFDM? Inter-carrier interference (ICI) in\nCP-OFDM makes it is very challenging to acquire the complete frequency domain\n(FD) channel response between subcarriers in the presence of mobility and delay\nspread. We avoid this difficulty by estimating the effective channel in the DD\ndomain from which we are able to reconstruct the complete FD channel response.\nWe take advantage of CP-OFDM to design an ${\\mathcal O}(M^2N^2)$ low-complexity\nmethod of jointly equalizing all subcarriers, where $MN$ is the number of\nsubcarriers. Our approach removes the need for traditional pilots in CP-OFDM\nand reduces the need to vary carrier spacing with mobility.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23045v1", "categories": ["eess.SP", "cs.IT", "math.IT"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.23045v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23721", "title": "Deep Learning-Based Semantic Segmentation for Real-Time Kidney Imaging and Measurements with Augmented Reality-Assisted Ultrasound", "authors": ["Gijs Luijten", "Roberto Maria Scardigno", "Lisle Faray de Paiva", "Peter Hoyer", "Jens Kleesiek", "Domenico Buongiorno", "Vitoantonio Bevilacqua", "Jan Egger"], "summary": "Ultrasound (US) is widely accessible and radiation-free but has a steep\nlearning curve due to its dynamic nature and non-standard imaging planes.\nAdditionally, the constant need to shift focus between the US screen and the\npatient poses a challenge. To address these issues, we integrate deep learning\n(DL)-based semantic segmentation for real-time (RT) automated kidney volumetric\nmeasurements, which are essential for clinical assessment but are traditionally\ntime-consuming and prone to fatigue. This automation allows clinicians to\nconcentrate on image interpretation rather than manual measurements.\nComplementing DL, augmented reality (AR) enhances the usability of US by\nprojecting the display directly into the clinician's field of view, improving\nergonomics and reducing the cognitive load associated with screen-to-patient\ntransitions. Two AR-DL-assisted US pipelines on HoloLens-2 are proposed: one\nstreams directly via the application programming interface for a wireless\nsetup, while the other supports any US device with video output for broader\naccessibility. We evaluate RT feasibility and accuracy using the Open Kidney\nDataset and open-source segmentation models (nnU-Net, Segmenter, YOLO with\nMedSAM and LiteMedSAM). Our open-source GitHub pipeline includes model\nimplementations, measurement algorithms, and a Wi-Fi-based streaming solution,\nenhancing US training and diagnostics, especially in point-of-care settings.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23721v1", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.HC", "cs.LG"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.23721v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22809", "title": "BayesLoRA: Task-Specific Uncertainty in Low-Rank Adapters", "authors": ["Cooper Doyle"], "summary": "We propose BayesLoRA, a task-specific uncertainty quantification framework\nthat integrates MC-Dropout into Low-Rank Adapters (LoRA). Unlike\ngeneral-purpose transformer uncertainty methods, BayesLoRA provides guardrails\ntailored to downstream workflows, enabling agents to introspect and modulate\nbehavior under uncertainty. We demonstrate mathematically and empirically that\nLoRA adapters exhibit amplified variance outside fine-tuning distributions,\nyielding reliable confidence estimates for agentic decision-making.", "comment": "13 pages, 3 figures, 1 table", "pdf_url": "http://arxiv.org/pdf/2506.22809v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22809v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23576", "title": "Evaluating Multi-Agent Defences Against Jailbreaking Attacks on Large Language Models", "authors": ["Maria Carolina Cornelia Wit", "Jun Pang"], "summary": "Recent advances in large language models (LLMs) have raised concerns about\njailbreaking attacks, i.e., prompts that bypass safety mechanisms. This paper\ninvestigates the use of multi-agent LLM systems as a defence against such\nattacks. We evaluate three jailbreaking strategies, including the original\nAutoDefense attack and two from Deepleaps: BetterDan and JB. Reproducing the\nAutoDefense framework, we compare single-agent setups with two- and three-agent\nconfigurations. Our results show that multi-agent systems enhance resistance to\njailbreaks, especially by reducing false negatives. However, its effectiveness\nvaries by attack type, and it introduces trade-offs such as increased false\npositives and computational overhead. These findings point to the limitations\nof current automated defences and suggest directions for improving alignment\nrobustness in future LLM systems.", "comment": "26 pages, 1 figure", "pdf_url": "http://arxiv.org/pdf/2506.23576v1", "categories": ["cs.AI"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.23576v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22978", "title": "A Systematic Study of Compositional Syntactic Transformer Language Models", "authors": ["Yida Zhao", "Hao Xve", "Xiang Hu", "Kewei Tu"], "summary": "Syntactic language models (SLMs) enhance Transformers by incorporating\nsyntactic biases through the modeling of linearized syntactic parse trees\nalongside surface sentences. This paper focuses on compositional SLMs that are\nbased on constituency parse trees and contain explicit bottom-up composition of\nconstituent representations. We identify key aspects of design choices in\nexisting compositional SLMs and propose a unified framework encompassing both\nexisting models and novel variants. We conduct a comprehensive empirical\nevaluation of all the variants in our framework across language modeling,\nsyntactic generalization, summarization, dialogue, and inference efficiency.\nBased on the experimental results, we make multiple recommendations on the\ndesign of compositional SLMs. Our code is released at\nhttps://github.com/zhaoyd1/compositional_SLMs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22978v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.22978v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.22634", "title": "A Rigorous Error Bound for the TG Kernel in Prime Counting", "authors": ["Bugra Kilictas", "Faruk Alpay"], "summary": "We establish rigorous error bounds for prime counting using a truncated\nGaussian (TG) kernel in the explicit formula framework. Our main theorem proves\nthat the approximation error remains globally below 1/2 for all sufficiently\nlarge arguments, guaranteeing exact computation of {\\pi}(x) through simple\nrounding, without relying on unproven hypotheses.\n  The TG kernel construction employs Gaussian-like test functions with compact\nsupport, engineered with vanishing moments to eliminate main terms. For x with\n10^8 decimal digits, we demonstrate that only ~1200 nontrivial zeta zeros\nsuffice to achieve the error bound, enabling computation in seconds on modern\nhardware - a dramatic improvement over classical methods.\n  Key contributions include: (1) Explicit tail truncation bounds using Taylor\nremainder analysis, showing exponential decay; (2) Zero-sum truncation error\nbounds via unconditional density estimates; (3) Rigorous treatment of trivial\nzero contributions. All constants are made explicit, ensuring full\nverifiability.\n  The method bridges analytic number theory and practical computation, with\npotential applications to record-breaking prime counting computations. We\ndiscuss algorithmic implications including FFT-based arithmetic for ~330\nmillion bit numbers. The framework's flexibility suggests connections to deeper\nstructures in prime distribution, particularly regarding optimized kernel\ndesigns and the interplay between smoothing parameters {\\alpha} and truncation\nheights.\n  This work exemplifies how classical analytic techniques, when carefully\nimplemented with modern computational perspectives, yield practical algorithms\nfor problems previously considered purely theoretical. The rigorous error\nanalysis ensures reliability even at astronomical scales, opening new avenues\nfor computational number theory research.", "comment": "19 pages, 0 figure", "pdf_url": "http://arxiv.org/pdf/2506.22634v1", "categories": ["math.NT", "cs.DS", "cs.NA", "math.NA", "11N05, 11Y35, 11M26, 65B10", "F.2.1; I.1.2"], "cate": "math.NT", "url": "http://arxiv.org/abs/2506.22634v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.23814", "title": "Breaking Out from the TESSERACT: Reassessing ML-based Malware Detection under Spatio-Temporal Drift", "authors": ["Theo Chow", "Mario D'Onghia", "Lorenz Linhardt", "Zeliang Kan", "Daniel Arp", "Lorenzo Cavallaro", "Fabio Pierazzi"], "summary": "Several recent works focused on the best practices for applying machine\nlearning to cybersecurity. In the context of malware, TESSERACT highlighted the\nimpact of concept drift on detection performance and suggested temporal and\nspatial constraints to be enforced to ensure realistic time-aware evaluations,\nwhich have been adopted by the community. In this paper, we demonstrate\nstriking discrepancies in the performance of learning-based malware detection\nacross the same time frame when evaluated on two representative Android malware\ndatasets used in top-tier security conferences, both adhering to established\nsampling and evaluation guidelines. This questions our ability to understand\nhow current state-of-the-art approaches would perform in realistic scenarios.\nTo address this, we identify five novel temporal and spatial bias factors that\naffect realistic evaluations. We thoroughly evaluate the impact of these\nfactors in the Android malware domain on two representative datasets and five\nAndroid malware classifiers used or proposed in top-tier security conferences.\nFor each factor, we provide practical and actionable recommendations that the\ncommunity should integrate in their methodology for more realistic and\nreproducible settings.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23814v1", "categories": ["cs.CR"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.23814v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23514", "title": "MGPRL: Distributed Multi-Gaussian Processes for Wi-Fi-based Multi-Robot Relative Localization in Large Indoor Environments", "authors": ["Sai Krishna Ghanta", "Ramviyas Parasuraman"], "summary": "Relative localization is a crucial capability for multi-robot systems\noperating in GPS-denied environments. Existing approaches for multi-robot\nrelative localization often depend on costly or short-range sensors like\ncameras and LiDARs. Consequently, these approaches face challenges such as high\ncomputational overhead (e.g., map merging) and difficulties in disjoint\nenvironments. To address this limitation, this paper introduces MGPRL, a novel\ndistributed framework for multi-robot relative localization using convex-hull\nof multiple Wi-Fi access points (AP). To accomplish this, we employ\nco-regionalized multi-output Gaussian Processes for efficient Radio Signal\nStrength Indicator (RSSI) field prediction and perform uncertainty-aware\nmulti-AP localization, which is further coupled with weighted convex hull-based\nalignment for robust relative pose estimation. Each robot predicts the RSSI\nfield of the environment by an online scan of APs in its environment, which are\nutilized for position estimation of multiple APs. To perform relative\nlocalization, each robot aligns the convex hull of its predicted AP locations\nwith that of the neighbor robots. This approach is well-suited for devices with\nlimited computational resources and operates solely on widely available Wi-Fi\nRSSI measurements without necessitating any dedicated pre-calibration or\noffline fingerprinting. We rigorously evaluate the performance of the proposed\nMGPRL in ROS simulations and demonstrate it with real-world experiments,\ncomparing it against multiple state-of-the-art approaches. The results showcase\nthat MGPRL outperforms existing methods in terms of localization accuracy and\ncomputational efficiency. Finally, we open source MGPRL as a ROS package\nhttps://github.com/herolab-uga/MGPRL.", "comment": "Accepted to IROS 2025", "pdf_url": "http://arxiv.org/pdf/2506.23514v1", "categories": ["cs.RO", "cs.AI", "cs.MA"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.23514v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23788", "title": "E-WAN: Efficient Communication in Energy Harvesting Low-Power Networks", "authors": ["Naomi Stricker", "David Blaser", "Andres Gomez", "Lothar Thiele"], "summary": "The ever-increasing number of distributed embedded systems in the context of\nthe Internet of Things (IoT), Wireless Sensor Networks (WSN), and\nCyber-Physical Systems (CPS) rely on wireless communication to collect and\nexchange data. Nodes can employ single-hop communication which, despite its\nease, may necessitate energy-intensive long-range communication to cover long\ndistances. Conversely, multi-hop communication allows for more energy-efficient\nshort-range communication since nodes can rely on other nodes to forward their\ndata. Yet, this approach requires relay nodes to be available and continuous\nmaintenance of a dynamically changing distributed state. At the same time,\nenergy harvesting has the potential to outperform traditional battery-based\nsystems by improving their lifetime, scalability with lower maintenance costs,\nand environmental impact. However, the limited and temporally and spatially\nvariable harvested energy poses significant challenges for networking in energy\nharvesting networks, particularly considering the energy demands and\ncharacteristics of both multi-hop and single-hop communication. We propose\nE-WAN, a protocol for energy harvesting wide-area low-power networks that\nbuilds on the concept of \\emph{virtual sub-networks} to enable\nresource-efficient multi-hop communication when possible and reliable however\nenergy-intensive point-to-point communication otherwise. Nodes autonomously and\ndynamically move between the two and adjust to changing network states and\nresources based only on easily obtainable network state information. We\nillustrate E-WAN's advantages both in terms of efficiency and adaptability in\nvarious communication and harvesting scenarios. Furthermore, we demonstrate\nE-WAN operating in a realistic setting by deploying an energy harvesting\nnetwork in a real-world indoor environment.", "comment": "This is the author's version of the work. Submitted to ACM TOSN on\n  June 2023. Major revision submitted on May 2024. Minor Revision submitted on\n  March 2025", "pdf_url": "http://arxiv.org/pdf/2506.23788v1", "categories": ["eess.SP", "cs.NI"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.23788v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.24057", "title": "Access InContext: Futuring Accessible Prototyping Tools and Methods", "authors": ["Patricia Piedade", "Peter A Hayton", "Cynthia Bennett", "Anna R L Carter", "Clara Crivellaro", "Alan Dix", "Jess McGowan", "Katta Spiel", "Miriam Sturdee", "Garreth W. Tigwell", "Hugo Nicolau"], "summary": "The popularity of accessibility research has grown recently, improving\ndigital inclusion for people with disabilities. However, researchers, including\nthose who have disabilities, have attempted to include people with disabilities\nin all aspects of design, and they have identified a myriad of practical\naccessibility barriers posed by tools and methods leveraged by human-computer\ninteraction (HCI) researchers during prototyping. To build a more inclusive\ntechnological landscape, we must question the effectiveness of existing\nprototyping tools and methods, repurpose/retrofit existing resources, and build\nnew tools and methods to support the participation of both researchers and\npeople with disabilities within the prototyping design process of novel\ntechnologies. This full-day workshop at CHI 2025 will provide a platform for\nHCI researchers, designers, and practitioners to discuss barriers and\nopportunities for creating accessible prototyping and promote hands-on ideation\nand fabrication exercises aimed at futuring accessible prototyping.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.24057v1", "categories": ["cs.HC"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.24057v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23122", "title": "Decoding Memes: Benchmarking Narrative Role Classification across Multilingual and Multimodal Models", "authors": ["Shivam Sharma", "Tanmoy Chakraborty"], "summary": "This work investigates the challenging task of identifying narrative roles -\nHero, Villain, Victim, and Other - in Internet memes, across three diverse test\nsets spanning English and code-mixed (English-Hindi) languages. Building on an\nannotated dataset originally skewed toward the 'Other' class, we explore a more\nbalanced and linguistically diverse extension, originally introduced as part of\nthe CLEF 2024 shared task. Comprehensive lexical and structural analyses\nhighlight the nuanced, culture-specific, and context-rich language used in real\nmemes, in contrast to synthetically curated hateful content, which exhibits\nexplicit and repetitive lexical markers. To benchmark the role detection task,\nwe evaluate a wide spectrum of models, including fine-tuned multilingual\ntransformers, sentiment and abuse-aware classifiers, instruction-tuned LLMs,\nand multimodal vision-language models. Performance is assessed under zero-shot\nsettings using precision, recall, and F1 metrics. While larger models like\nDeBERTa-v3 and Qwen2.5-VL demonstrate notable gains, results reveal consistent\nchallenges in reliably identifying the 'Victim' class and generalising across\ncultural and code-mixed content. We also explore prompt design strategies to\nguide multimodal models and find that hybrid prompts incorporating structured\ninstructions and role definitions offer marginal yet consistent improvements.\nOur findings underscore the importance of cultural grounding, prompt\nengineering, and multimodal reasoning in modelling subtle narrative framings in\nvisual-textual content.", "comment": "This work has been submitted to the IEEE for possible publication", "pdf_url": "http://arxiv.org/pdf/2506.23122v1", "categories": ["cs.CL", "cs.CY"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.23122v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.22720", "title": "Deterministic Object Pose Confidence Region Estimation", "authors": ["Jinghao Wang", "Zhang Li", "Zi Wang", "Banglei Guan", "Yang Shang", "Qifeng Yu"], "summary": "6D pose confidence region estimation has emerged as a critical direction,\naiming to perform uncertainty quantification for assessing the reliability of\nestimated poses. However, current sampling-based approach suffers from critical\nlimitations that severely impede their practical deployment: 1) the sampling\nspeed significantly decreases as the number of samples increases. 2) the\nderived confidence regions are often excessively large. To address these\nchallenges, we propose a deterministic and efficient method for estimating pose\nconfidence regions. Our approach uses inductive conformal prediction to\ncalibrate the deterministically regressed Gaussian keypoint distributions into\n2D keypoint confidence regions. We then leverage the implicit function theorem\nto propagate these keypoint confidence regions directly into 6D pose confidence\nregions. This method avoids the inefficiency and inflated region sizes\nassociated with sampling and ensembling. It provides compact confidence regions\nthat cover the ground-truth poses with a user-defined confidence level.\nExperimental results on the LineMOD Occlusion and SPEED datasets show that our\nmethod achieves higher pose estimation accuracy with reduced computational\ntime. For the same coverage rate, our method yields significantly smaller\nconfidence region volumes, reducing them by up to 99.9\\% for rotations and\n99.8\\% for translations. The code will be available soon.", "comment": "Accepted by ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2506.22720v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22720v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.22470", "title": "Reliable Transmission of LTP Using Reinforcement Learning-Based Adaptive FEC", "authors": ["Liang Chen", "Yu Song", "Kanglian Zhao", "Juan A. Fraire", "Wenfeng Li"], "summary": "Delay/Disruption Tolerant Networking (DTN) employs the Licklider Transmission\nProtocol (LTP) with Automatic Repeat reQuest (ARQ) for reliable data delivery\nin challenging interplanetary networks. While previous studies have integrated\npacket-level Forward Erasure Correction (FEC) into LTP to reduce retransmission\ntime costs, existing static and delay-feedback-based dynamic coding methods\nstruggle with highly variable and unpredictable deep space channel conditions.\nThis paper proposes a reinforcement learning (RL)-based adaptive FEC algorithm\nto address these limitations. The algorithm utilizes historical feedback and\nsystem state to predict future channel conditions and proactively adjust the\ncode rate. This approach aims to anticipate channel quality degradation,\nthereby preventing decoding failures and subsequent LTP retransmissions and\nimproving coding efficiency by minimizing redundancy during favorable channel\nconditions. Performance evaluations conducted in simulated Earth-Moon and\nEarth-Mars link scenarios demonstrate this algorithm's effectiveness in\noptimizing data transmission for interplanetary networks. Compared to existing\nmethods, this approach demonstrates significant improvement, with matrix\ndecoding failures reduced by at least 2/3.", "comment": "15 pages, 30 figures, Liang Chen and Yu Song are co-first authors", "pdf_url": "http://arxiv.org/pdf/2506.22470v1", "categories": ["cs.NI", "cs.SY", "eess.SY"], "cate": "cs.NI", "url": "http://arxiv.org/abs/2506.22470v1", "date": "2025-06-19", "updated": "2025-06-19"}
{"id": "2506.23118", "title": "Belief Propagation-based Target Handover in Distributed Integrated Sensing and Communication", "authors": ["Liping Bai", "Yu Ge", "Henk Wymeersch"], "summary": "Distributed integrated sensing and communication (DISAC) systems are key\nenablers for 6G networks, offering the capability to jointly track multiple\ntargets using spatially distributed base stations (BSs). A fundamental\nchallenge in DISAC is the seamless and efficient handover of target tracks\nbetween BSs with partially overlapping fields of view, especially in dense and\ndynamic environments. In this paper, we propose a novel target handover\nframework based on belief propagation (BP) for multi-target tracking in DISAC\nsystems. By representing the probabilistic data association and tracking\nproblem through a factor graph, the proposed method enables efficient marginal\ninference with reduced computational complexity. Our framework introduces a\nprincipled handover criterion and message-passing strategy that minimizes\ninter-BS communication while maintaining tracking continuity and accuracy. We\ndemonstrate that the proposed handover procedure achieves performance\ncomparable to centralized processing, yet significantly reduces data exchange\nand processing overhead. Extensive simulations validate the robustness of the\napproach in urban tracking scenarios with closely spaced targets.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23118v1", "categories": ["eess.SP"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.23118v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23759", "title": "Spatio-Temporal Representation Decoupling and Enhancement for Federated Instrument Segmentation in Surgical Videos", "authors": ["Zheng Fang", "Xiaoming Qi", "Chun-Mei Feng", "Jialun Pei", "Weixin Si", "Yueming Jin"], "summary": "Surgical instrument segmentation under Federated Learning (FL) is a promising\ndirection, which enables multiple surgical sites to collaboratively train the\nmodel without centralizing datasets. However, there exist very limited FL works\nin surgical data science, and FL methods for other modalities do not consider\ninherent characteristics in surgical domain: i) different scenarios show\ndiverse anatomical backgrounds while highly similar instrument representation;\nii) there exist surgical simulators which promote large-scale synthetic data\ngeneration with minimal efforts. In this paper, we propose a novel Personalized\nFL scheme, Spatio-Temporal Representation Decoupling and Enhancement (FedST),\nwhich wisely leverages surgical domain knowledge during both local-site and\nglobal-server training to boost segmentation. Concretely, our model embraces a\nRepresentation Separation and Cooperation (RSC) mechanism in local-site\ntraining, which decouples the query embedding layer to be trained privately, to\nencode respective backgrounds. Meanwhile, other parameters are optimized\nglobally to capture the consistent representations of instruments, including\nthe temporal layer to capture similar motion patterns. A textual-guided channel\nselection is further designed to highlight site-specific features, facilitating\nmodel adapta tion to each site. Moreover, in global-server training, we propose\nSynthesis-based Explicit Representation Quantification (SERQ), which defines an\nexplicit representation target based on synthetic data to synchronize the model\nconvergence during fusion for improving model generalization.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23759v1", "categories": ["eess.IV", "cs.CV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.23759v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22821", "title": "Deep learning 40 years of human migration", "authors": ["Thomas Gaskin", "Guy J. Abel"], "summary": "We present a novel and detailed dataset on origin-destination annual\nmigration flows and stocks between 230 countries and regions, spanning the\nperiod from 1990 to the present. Our flow estimates are further disaggregated\nby country of birth, providing a comprehensive picture of migration over the\nlast 43 years. The estimates are obtained by training a deep recurrent neural\nnetwork to learn flow patterns from 18 covariates for all countries, including\ngeographic, economic, cultural, societal, and political information. The\nrecurrent architecture of the neural network means that the entire past can\ninfluence current migration patterns, allowing us to learn long-range temporal\ncorrelations. By training an ensemble of neural networks and additionally\npushing uncertainty on the covariates through the trained network, we obtain\nconfidence bounds for all our estimates, allowing researchers to pinpoint the\ngeographic regions most in need of additional data collection. We validate our\napproach on various test sets of unseen data, demonstrating that it\nsignificantly outperforms traditional methods estimating five-year flows while\ndelivering a significant increase in temporal resolution. The model is fully\nopen source: all training data, neural network weights, and training code are\nmade public alongside the migration estimates, providing a valuable resource\nfor future studies of human migration.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22821v1", "categories": ["cs.LG", "68T07", "I.2.6"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22821v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23626", "title": "Self-correcting Reward Shaping via Language Models for Reinforcement Learning Agents in Games", "authors": ["Ant√≥nio Afonso", "Iolanda Leite", "Alessandro Sestini", "Florian Fuchs", "Konrad Tollmar", "Linus Gissl√©n"], "summary": "Reinforcement Learning (RL) in games has gained significant momentum in\nrecent years, enabling the creation of different agent behaviors that can\ntransform a player's gaming experience. However, deploying RL agents in\nproduction environments presents two key challenges: (1) designing an effective\nreward function typically requires an RL expert, and (2) when a game's content\nor mechanics are modified, previously tuned reward weights may no longer be\noptimal. Towards the latter challenge, we propose an automated approach for\niteratively fine-tuning an RL agent's reward function weights, based on a\nuser-defined language based behavioral goal. A Language Model (LM) proposes\nupdated weights at each iteration based on this target behavior and a summary\nof performance statistics from prior training rounds. This closed-loop process\nallows the LM to self-correct and refine its output over time, producing\nincreasingly aligned behavior without the need for manual reward engineering.\nWe evaluate our approach in a racing task and show that it consistently\nimproves agent performance across iterations. The LM-guided agents show a\nsignificant increase in performance from $9\\%$ to $74\\%$ success rate in just\none iteration. We compare our LM-guided tuning against a human expert's manual\nweight design in the racing task: by the final iteration, the LM-tuned agent\nachieved an $80\\%$ success rate, and completed laps in an average of $855$ time\nsteps, a competitive performance against the expert-tuned agent's peak $94\\%$\nsuccess, and $850$ time steps.", "comment": "16 pages in total, 10 pages of main paper, 5 figures", "pdf_url": "http://arxiv.org/pdf/2506.23626v1", "categories": ["cs.AI"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.23626v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23046", "title": "SoMi-ToM: Evaluating Multi-Perspective Theory of Mind in Embodied Social Interactions", "authors": ["Xianzhe Fan", "Xuhui Zhou", "Chuanyang Jin", "Kolby Nottingham", "Hao Zhu", "Maarten Sap"], "summary": "Humans continuously infer the states, goals, and behaviors of others by\nperceiving their surroundings in dynamic, real-world social interactions.\nHowever, most Theory of Mind (ToM) benchmarks only evaluate static, text-based\nscenarios, which have a significant gap compared to real interactions. We\npropose the SoMi-ToM benchmark, designed to evaluate multi-perspective ToM in\nembodied multi-agent complex social interactions. This benchmark is based on\nrich multimodal interaction data generated by the interaction environment SoMi,\ncovering diverse crafting goals and social relationships. Our framework\nsupports multi-level evaluation: (1) first-person evaluation provides\nmultimodal (visual, dialogue, action, etc.) input from a first-person\nperspective during a task for real-time state inference, (2) third-person\nevaluation provides complete third-person perspective video and text records\nafter a task for goal and behavior inference. This evaluation method allows for\na more comprehensive examination of a model's ToM capabilities from both the\nsubjective immediate experience and the objective global observation. We\nconstructed a challenging dataset containing 35 third-person perspective\nvideos, 363 first-person perspective images, and 1225 expert-annotated\nmultiple-choice questions (three options). On this dataset, we systematically\nevaluated the performance of human subjects and several state-of-the-art large\nvision-language models (LVLMs). The results show that LVLMs perform\nsignificantly worse than humans on SoMi-ToM: the average accuracy gap between\nhumans and models is 40.1% in first-person evaluation and 26.4% in third-person\nevaluation. This indicates that future LVLMs need to further improve their ToM\ncapabilities in embodied, complex social interactions.", "comment": "23 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2506.23046v1", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.RO"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.23046v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.22701", "title": "Lower bounds for trace estimation via Block Krylov and other methods", "authors": ["Shi Jie Yu"], "summary": "This paper studies theoretical lower bounds for estimating the trace of a\nmatrix function, $\\text{tr}(f(A))$, focusing on methods that use Hutchinson's\nmethod along with Block Krylov techniques. These methods work by approximating\nmatrix-vector products like $f(A)V$ using a Block Krylov subspace. This is\nclosely related to approximating functions with polynomials. We derive\ntheoretical upper bounds on how many Krylov steps are needed for functions such\nas $A^{-1/2}$ and $A^{-1}$ by analyzing the upper bounds from the polynomial\napproximation of their scalar equivalent. In addition, we also develop lower\nlimits on the number of queries needed for trace estimation, specifically for\n$\\text{tr}(W^{-p})$ where $W$ is a Wishart matrix. Our study clarifies the\nconnection between the number of steps in Block Krylov methods and the degree\nof the polynomial used for approximation. This links the total cost of trace\nestimation to basic limits in polynomial approximation and how much information\nis needed for the computation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22701v1", "categories": ["math.ST", "cs.DS", "cs.LG", "cs.NA", "math.NA", "stat.TH"], "cate": "math.ST", "url": "http://arxiv.org/abs/2506.22701v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23841", "title": "An ontological lens on attack trees: Toward adequacy and interoperability", "authors": ["√çtalo Oliveira", "Stefano M. Nicoletti", "Gal Engelberg", "Mattia Fumagalli", "Dan Klein", "Giancarlo Guizzardi"], "summary": "Attack Trees (AT) are a popular formalism for security analysis. They are\nmeant to display an attacker's goal decomposed into attack steps needed to\nachieve it and compute certain security metrics (e.g., attack cost,\nprobability, and damage). ATs offer three important services: (a) conceptual\nmodeling capabilities for representing security risk management scenarios, (b)\na qualitative assessment to find root causes and minimal conditions of\nsuccessful attacks, and (c) quantitative analyses via security metrics\ncomputation under formal semantics, such as minimal time and cost among all\nattacks. Still, the AT language presents limitations due to its lack of\nontological foundations, thus compromising associated services. Via an\nontological analysis grounded in the Common Ontology of Value and Risk (COVER)\n-- a reference core ontology based on the Unified Foundational Ontology (UFO)\n-- we investigate the ontological adequacy of AT and reveal four significant\nshortcomings: (1) ambiguous syntactical terms that can be interpreted in\nvarious ways; (2) ontological deficit concerning crucial domain-specific\nconcepts; (3) lacking modeling guidance to construct ATs decomposing a goal;\n(4) lack of semantic interoperability, resulting in ad hoc stand-alone tools.\nWe also discuss existing incremental solutions and how our analysis paves the\nway for overcoming those issues through a broader approach to risk management\nmodeling.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23841v1", "categories": ["cs.CR", "cs.SE"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.23841v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23573", "title": "Online Human Action Detection during Escorting", "authors": ["Siddhartha Mondal", "Avik Mitra", "Chayan Sarkar"], "summary": "The deployment of robot assistants in large indoor spaces has seen\nsignificant growth, with escorting tasks becoming a key application. However,\nmost current escorting robots primarily rely on navigation-focused strategies,\nassuming that the person being escorted will follow without issue. In crowded\nenvironments, this assumption often falls short, as individuals may struggle to\nkeep pace, become obstructed, get distracted, or need to stop unexpectedly. As\na result, conventional robotic systems are often unable to provide effective\nescorting services due to their limited understanding of human movement\ndynamics. To address these challenges, an effective escorting robot must\ncontinuously detect and interpret human actions during the escorting process\nand adjust its movement accordingly. However, there is currently no existing\ndataset designed specifically for human action detection in the context of\nescorting. Given that escorting often occurs in crowded environments, where\nother individuals may enter the robot's camera view, the robot also needs to\nidentify the specific human it is escorting (the subject) before predicting\ntheir actions. Since no existing model performs both person re-identification\nand action prediction in real-time, we propose a novel neural network\narchitecture that can accomplish both tasks. This enables the robot to adjust\nits speed dynamically based on the escortee's movements and seamlessly resume\nescorting after any disruption. In comparative evaluations against strong\nbaselines, our system demonstrates superior efficiency and effectiveness,\nshowcasing its potential to significantly improve robotic escorting services in\ncomplex, real-world scenarios.", "comment": "Accepted in IEEE RO-MAN '25", "pdf_url": "http://arxiv.org/pdf/2506.23573v1", "categories": ["cs.RO", "cs.AI", "cs.LG"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.23573v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.24104", "title": "Bridging Service Design, Visualizations, and Visual Analytics in Healthcare Digital Twins: Challenges, Gaps, and Research Opportunities", "authors": ["Mariia Ershova", "Graziano Blasilli"], "summary": "Digital twins (DT) are increasingly used in healthcare to model patients,\nprocesses, and physiological systems. While recent solutions leverage\nvisualization, visual analytics, and user interaction, these systems rarely\nincorporate structured service design methodologies. Bridging service design\nwith visual analytics and visualization can be valuable for the healthcare DT\ncommunity. This paper aims to introduce the service design discipline to\nvisualization researchers by framing this integration gap and suggesting\nresearch directions to enhance the real-world applicability of DT solutions.", "comment": "Submitted to: Workshop on Visual Analytics in Healthcare (VAHC 2025)", "pdf_url": "http://arxiv.org/pdf/2506.24104v1", "categories": ["cs.HC"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.24104v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23123", "title": "The Societal Impact of Foundation Models: Advancing Evidence-based AI Policy", "authors": ["Rishi Bommasani"], "summary": "Artificial intelligence is humanity's most promising technology because of\nthe remarkable capabilities offered by foundation models. Yet, the same\ntechnology brings confusion and consternation: foundation models are poorly\nunderstood and they may precipitate a wide array of harms. This dissertation\nexplains how technology and society coevolve in the age of AI, organized around\nthree themes. First, the conceptual framing: the capabilities, risks, and the\nsupply chain that grounds foundation models in the broader economy. Second, the\nempirical insights that enrich the conceptual foundations: transparency created\nvia evaluations at the model level and indexes at the organization level.\nFinally, the transition from understanding to action: superior understanding of\nthe societal impact of foundation models advances evidence-based AI policy.\nView together, this dissertation makes inroads into achieving better societal\noutcomes in the age of AI by building the scientific foundations and\nresearch-policy interface required for better AI governance.", "comment": "Stanford University PhD Dissertation of Rishi Bommasani (Department\n  of Computer Science, 2025). Also available at\n  https://purl.stanford.edu/zf669yy0336", "pdf_url": "http://arxiv.org/pdf/2506.23123v1", "categories": ["cs.AI", "cs.CY", "cs.ET"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.23123v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.22726", "title": "XTransfer: Cross-Modality Model Transfer for Human Sensing with Few Data at the Edge", "authors": ["Yu Zhang", "Xi Zhang", "Hualin zhou", "Xinyuan Chen", "Shang Gao", "Hong Jia", "Jianfei Yang", "Yuankai Qi", "Tao Gu"], "summary": "Deep learning for human sensing on edge systems offers significant\nopportunities for smart applications. However, its training and development are\nhindered by the limited availability of sensor data and resource constraints of\nedge systems. Current methods that rely on transferring pre-trained models\noften encounter issues such as modality shift and high resource demands,\nresulting in substantial accuracy loss, resource overhead, and poor\nadaptability across different sensing applications. In this paper, we propose\nXTransfer, a first-of-its-kind method for resource-efficient, modality-agnostic\nmodel transfer. XTransfer freely leverages single or multiple pre-trained\nmodels and transfers knowledge across different modalities by (i) model\nrepairing that safely repairs modality shift in pre-trained model layers with\nonly few sensor data, and (ii) layer recombining that efficiently searches and\nrecombines layers of interest from source models in a layer-wise manner to\ncreate compact models. We benchmark various baselines across diverse human\nsensing datasets spanning different modalities. Comprehensive results\ndemonstrate that XTransfer achieves state-of-the-art performance on human\nsensing tasks while significantly reducing the costs of sensor data collection,\nmodel training, and edge deployment.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22726v1", "categories": ["cs.CV", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22726v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.22474", "title": "RL-based Adaptive Task Offloading in Mobile-Edge Computing for Future IoT Networks", "authors": ["Ziad Qais Al Abbasi", "Khaled M. Rabie", "Senior Member", "Xingwang Li", "Senior Member", "Wali Ullah Khan", "Asma Abu Samah"], "summary": "The Internet of Things (IoT) has been increasingly used in our everyday lives\nas well as in numerous industrial applications. However, due to limitations in\ncomputing and power capabilities, IoT devices need to send their respective\ntasks to cloud service stations that are usually located at far distances.\nHaving to transmit data far distances introduces challenges for services that\nrequire low latency such as industrial control in factories and plants as well\nas artificial intelligence assisted autonomous driving. To solve this issue,\nmobile edge computing (MEC) is deployed at the networks edge to reduce\ntransmission time. In this regard, this study proposes a new offloading scheme\nfor MEC-assisted ultra dense cellular networks using reinforcement learning\n(RL) techniques. The proposed scheme enables efficient resource allocation and\ndynamic offloading decisions based on varying network conditions and user\ndemands. The RL algorithm learns from the networks historical data and adapts\nthe offloading decisions to optimize the networks overall performance.\nNon-orthogonal multiple access is also adopted to improve resource utilization\namong the IoT devices. Simulation results demonstrate that the proposed scheme\noutperforms other stateof the art offloading algorithms in terms of energy\nefficiency, network throughput, and user satisfaction.", "comment": "7 pages", "pdf_url": "http://arxiv.org/pdf/2506.22474v1", "categories": ["cs.NI", "cs.SY", "eess.SY", "C.2 COMPUTER-COMMUNICATION NETWORKS"], "cate": "cs.NI", "url": "http://arxiv.org/abs/2506.22474v1", "date": "2025-06-20", "updated": "2025-06-20"}
{"id": "2506.23203", "title": "Multi-Branch DNN and CRLB-Ratio-Weight Fusion for Enhanced DOA Sensing via a Massive H$^2$AD MIMO Receiver", "authors": ["Feng Shu", "Jiatong Bai", "Di Wu", "Wei Zhu", "Bin Deng", "Fuhui Zhou", "Jiangzhou Wang"], "summary": "As a green MIMO structure, massive H$^2$AD is viewed as a potential\ntechnology for the future 6G wireless network. For such a structure, it is a\nchallenging task to design a low-complexity and high-performance fusion of\ntarget direction values sensed by different sub-array groups with fewer use of\nprior knowledge. To address this issue, a lightweight Cramer-Rao lower bound\n(CRLB)-ratio-weight fusion (WF) method is proposed, which approximates inverse\nCRLB of each subarray using antenna number reciprocals to eliminate real-time\nCRLB computation. This reduces complexity and prior knowledge dependence while\npreserving fusion performance. Moreover, a multi-branch deep neural network\n(MBDNN) is constructed to further enhance direction-of-arrival (DOA) sensing by\nleveraging candidate angles from multiple subarrays. The subarray-specific\nbranch networks are integrated with a shared regression module to effectively\neliminate pseudo-solutions and fuse true angles. Simulation results show that\nthe proposed CRLB-ratio-WF method achieves DOA sensing performance comparable\nto CRLB-based methods, while significantly reducing the reliance on prior\nknowledge. More notably, the proposed MBDNN has superior performance in low-SNR\nranges. At SNR $= -15$ dB, it achieves an order-of-magnitude improvement in\nestimation accuracy compared to CRLB-ratio-WF method.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23203v1", "categories": ["eess.SP", "cs.AI"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.23203v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.24003", "title": "ShapeKit", "authors": ["Junqi Liu", "Dongli He", "Wenxuan Li", "Ningyu Wang", "Alan L. Yuille", "Zongwei Zhou"], "summary": "In this paper, we present a practical approach to improve anatomical shape\naccuracy in whole-body medical segmentation. Our analysis shows that a\nshape-focused toolkit can enhance segmentation performance by over 8%, without\nthe need for model re-training or fine-tuning. In comparison, modifications to\nmodel architecture typically lead to marginal gains of less than 3%. Motivated\nby this observation, we introduce ShapeKit, a flexible and easy-to-integrate\ntoolkit designed to refine anatomical shapes. This work highlights the\nunderappreciated value of shape-based tools and calls attention to their\npotential impact within the medical segmentation community.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.24003v1", "categories": ["eess.IV", "cs.CV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.24003v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22837", "title": "xLSTMAD: A Powerful xLSTM-based Method for Anomaly Detection", "authors": ["Kamil Faber", "Marcin Pietro≈Ñ", "Dominik ≈ªurek", "Roberto Corizzo"], "summary": "The recently proposed xLSTM is a powerful model that leverages expressive\nmultiplicative gating and residual connections, providing the temporal capacity\nneeded for long-horizon forecasting and representation learning. This\narchitecture has demonstrated success in time series forecasting, lossless\ncompression, and even large-scale language modeling tasks, where its linear\nmemory footprint and fast inference make it a viable alternative to\nTransformers. Despite its growing popularity, no prior work has explored xLSTM\nfor anomaly detection. In this work, we fill this gap by proposing xLSTMAD, the\nfirst anomaly detection method that integrates a full encoder-decoder xLSTM\narchitecture, purpose-built for multivariate time series data. Our encoder\nprocesses input sequences to capture historical context, while the decoder is\ndevised in two separate variants of the method. In the forecasting approach,\nthe decoder iteratively generates forecasted future values xLSTMAD-F, while the\nreconstruction approach reconstructs the input time series from its encoded\ncounterpart xLSTMAD-R. We investigate the performance of two loss functions:\nMean Squared Error (MSE), and Soft Dynamic Time Warping (SoftDTW) to consider\nlocal reconstruction fidelity and global sequence alignment, respectively. We\nevaluate our method on the comprehensive TSB-AD-M benchmark, which spans 17\nreal-world datasets, using state-of-the-art challenging metrics such as VUS-PR.\nIn our results, xLSTM showcases state-of-the-art accuracy, outperforming 23\npopular anomaly detection baselines. Our paper is the first work revealing the\npowerful modeling capabilities of xLSTM for anomaly detection, paving the way\nfor exciting new developments on this subject. Our code is available at:\nhttps://github.com/Nyderx/xlstmad", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22837v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22837v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23673", "title": "HASD: Hierarchical Adaption for pathology Slide-level Domain-shift", "authors": ["Jingsong Liu", "Han Li", "Chen Yang", "Michael Deutges", "Ario Sadafi", "Xin You", "Katharina Breininger", "Nassir Navab", "Peter J. Sch√ºffler"], "summary": "Domain shift is a critical problem for pathology AI as pathology data is\nheavily influenced by center-specific conditions. Current pathology domain\nadaptation methods focus on image patches rather than WSI, thus failing to\ncapture global WSI features required in typical clinical scenarios. In this\nwork, we address the challenges of slide-level domain shift by proposing a\nHierarchical Adaptation framework for Slide-level Domain-shift (HASD). HASD\nachieves multi-scale feature consistency and computationally efficient\nslide-level domain adaptation through two key components: (1) a hierarchical\nadaptation framework that integrates a Domain-level Alignment Solver for\nfeature alignment, a Slide-level Geometric Invariance Regularization to\npreserve the morphological structure, and a Patch-level Attention Consistency\nRegularization to maintain local critical diagnostic cues; and (2) a prototype\nselection mechanism that reduces computational overhead. We validate our method\non two slide-level tasks across five datasets, achieving a 4.1\\% AUROC\nimprovement in a Breast Cancer HER2 Grading cohort and a 3.9\\% C-index gain in\na UCEC survival prediction cohort. Our method provides a practical and reliable\nslide-level domain adaption solution for pathology institutions, minimizing\nboth computational and annotation costs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23673v1", "categories": ["cs.AI"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.23673v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23051", "title": "MariNER: A Dataset for Historical Brazilian Portuguese Named Entity Recognition", "authors": ["Jo√£o Lucas Luz Lima Sarcinelli", "Marina Lages Gon√ßalves Teixeira", "Jade Bortot de Paiva", "Diego Furtado Silva"], "summary": "Named Entity Recognition (NER) is a fundamental Natural Language Processing\n(NLP) task that aims to identify and classify entity mentions in texts across\ndifferent categories. While languages such as English possess a large number of\nhigh-quality resources for this task, Brazilian Portuguese still lacks in\nquantity of gold-standard NER datasets, especially when considering specific\ndomains. Particularly, this paper considers the importance of NER for analyzing\nhistorical texts in the context of digital humanities. To address this gap,\nthis work outlines the construction of MariNER: \\textit{Mapeamento e\nAnota\\c{c}\\~oes de Registros hIst\\'oricos para NER} (Mapping and Annotation of\nHistorical Records for NER), the first gold-standard dataset for early\n20th-century Brazilian Portuguese, with more than 9,000 manually annotated\nsentences. We also assess and compare the performance of state-of-the-art NER\nmodels for the dataset.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23051v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.23051v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.22826", "title": "Denoising Multi-Color QR Codes and Stiefel-Valued Data by Relaxed Regularizations", "authors": ["Robert Beinert", "Jonas Bresch"], "summary": "The handling of manifold-valued data, for instance, plays a central role in\ncolor restoration tasks relying on circle- or sphere-valued color models, in\nthe study of rotational or directional information related to the special\northogonal group, and in Gaussian image processing, where the pixel statistics\nare interpreted as values on the hyperbolic sheet. Especially, to denoise these\nkind of data, there have been proposed several generalizations of total\nvariation (TV) and Tikhonov-type denoising models incorporating the underlying\nmanifolds. Recently, a novel, numerically efficient denoising approach has been\nintroduced, where the data are embedded in an Euclidean ambient space, the\nnon-convex manifolds are encoded by a series of positive semi-definite,\nfixed-rank matrices, and the rank constraint is relaxed to obtain a\nconvexification that can be solved using standard algorithms from convex\nanalysis. The aim of the present paper is to extent this approach to new kinds\nof data like multi-binary and Stiefel-valued data. Multi-binary data can, for\ninstance, be used to model multi-color QR codes whereas Stiefel-valued data\noccur in image and video-based recognition. For both new data types, we propose\nTV- and Tikhonov-based denoising modelstogether with easy-to-solve\nconvexification. All derived methods are evaluated on proof-of-concept,\nsynthetic experiments.", "comment": "9 pages, 2 figures, 3 algorithms", "pdf_url": "http://arxiv.org/pdf/2506.22826v1", "categories": ["math.OC", "cs.CV", "cs.NA", "math.NA", "94A08, 94A12, 65J22, 90C22, 90C25"], "cate": "math.OC", "url": "http://arxiv.org/abs/2506.22826v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23855", "title": "Differentially Private Synthetic Data Release for Topics API Outputs", "authors": ["Travis Dick", "Alessandro Epasto", "Adel Javanmard", "Josh Karlin", "Andres Munoz Medina", "Vahab Mirrokni", "Sergei Vassilvitskii", "Peilin Zhong"], "summary": "The analysis of the privacy properties of Privacy-Preserving Ads APIs is an\narea of research that has received strong interest from academics, industry,\nand regulators. Despite this interest, the empirical study of these methods is\nhindered by the lack of publicly available data. Reliable empirical analysis of\nthe privacy properties of an API, in fact, requires access to a dataset\nconsisting of realistic API outputs; however, privacy concerns prevent the\ngeneral release of such data to the public.\n  In this work, we develop a novel methodology to construct synthetic API\noutputs that are simultaneously realistic enough to enable accurate study and\nprovide strong privacy protections. We focus on one Privacy-Preserving Ads\nAPIs: the Topics API, part of Google Chrome's Privacy Sandbox. We developed a\nmethodology to generate a differentially-private dataset that closely matches\nthe re-identification risk properties of the real Topics API data. The use of\ndifferential privacy provides strong theoretical bounds on the leakage of\nprivate user information from this release.\n  Our methodology is based on first computing a large number of\ndifferentially-private statistics describing how output API traces evolve over\ntime. Then, we design a parameterized distribution over sequences of API traces\nand optimize its parameters so that they closely match the statistics obtained.\nFinally, we create the synthetic data by drawing from this distribution.\n  Our work is complemented by an open-source release of the anonymized dataset\nobtained by this methodology. We hope this will enable external researchers to\nanalyze the API in-depth and replicate prior and future work on a realistic\nlarge-scale dataset. We believe that this work will contribute to fostering\ntransparency regarding the privacy properties of Privacy-Preserving Ads APIs.", "comment": "20 pages, 8 figures", "pdf_url": "http://arxiv.org/pdf/2506.23855v1", "categories": ["cs.CR", "cs.AI", "cs.LG"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.23855v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23614", "title": "Passage-traversing optimal path planning with sampling-based algorithms", "authors": ["Jing Huang", "Hao Su", "Kwok Wai Samuel Au"], "summary": "This paper introduces a new paradigm of optimal path planning, i.e.,\npassage-traversing optimal path planning (PTOPP), that optimizes paths'\ntraversed passages for specified optimization objectives. In particular, PTOPP\nis utilized to find the path with optimal accessible free space along its\nentire length, which represents a basic requirement for paths in robotics. As\npassages are places where free space shrinks and becomes constrained, the core\nidea is to leverage the path's passage traversal status to characterize its\naccessible free space comprehensively. To this end, a novel passage detection\nand free space decomposition method using proximity graphs is proposed,\nenabling fast detection of sparse but informative passages and environment\ndecompositions. Based on this preprocessing, optimal path planning with\naccessible free space objectives or constraints is formulated as PTOPP problems\ncompatible with sampling-based optimal planners. Then, sampling-based\nalgorithms for PTOPP, including their dependent primitive procedures, are\ndeveloped leveraging partitioned environments for fast passage traversal check.\nAll these methods are implemented and thoroughly tested for effectiveness and\nefficiency validation. Compared to existing approaches, such as clearance-based\nmethods, PTOPP demonstrates significant advantages in configurability, solution\noptimality, and efficiency, addressing prior limitations and incapabilities. It\nis believed to provide an efficient and versatile solution to accessible free\nspace optimization over conventional avenues and more generally, to a broad\nclass of path planning problems that can be formulated as PTOPP.", "comment": "30 pages, 22 figures, 6 tables, journal paper", "pdf_url": "http://arxiv.org/pdf/2506.23614v1", "categories": ["cs.RO", "cs.CG"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.23614v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22443", "title": "Learning Interpretable Rules from Neural Networks: Neurosymbolic AI for Radar Hand Gesture Recognition", "authors": ["Sarah Seifi", "Tobias Sukianto", "Cecilia Carbonelli", "Lorenzo Servadei", "Robert Wille"], "summary": "Rule-based models offer interpretability but struggle with complex data,\nwhile deep neural networks excel in performance yet lack transparency. This\nwork investigates a neuro-symbolic rule learning neural network named RL-Net\nthat learns interpretable rule lists through neural optimization, applied for\nthe first time to radar-based hand gesture recognition (HGR). We benchmark\nRL-Net against a fully transparent rule-based system (MIRA) and an explainable\nblack-box model (XentricAI), evaluating accuracy, interpretability, and user\nadaptability via transfer learning. Our results show that RL-Net achieves a\nfavorable trade-off, maintaining strong performance (93.03% F1) while\nsignificantly reducing rule complexity. We identify optimization challenges\nspecific to rule pruning and hierarchy bias and propose stability-enhancing\nmodifications. Compared to MIRA and XentricAI, RL-Net emerges as a practical\nmiddle ground between transparency and performance. This study highlights the\nreal-world feasibility of neuro-symbolic models for interpretable HGR and\noffers insights for extending explainable AI to edge-deployable sensing\nsystems.", "comment": "8 pages, 3 figures, accepted at the late-breaking work track at the\n  XAI-2025 third World Conference of Explainable AI", "pdf_url": "http://arxiv.org/pdf/2506.22443v1", "categories": ["cs.LG", "cs.HC"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22443v1", "date": "2025-06-11", "updated": "2025-06-11"}
{"id": "2506.23411", "title": "Datasets for Fairness in Language Models: An In-Depth Survey", "authors": ["Jiale Zhang", "Zichong Wang", "Avash Palikhe", "Zhipeng Yin", "Wenbin Zhang"], "summary": "Fairness benchmarks play a central role in shaping how we evaluate language\nmodels, yet surprisingly little attention has been given to examining the\ndatasets that these benchmarks rely on. This survey addresses that gap by\npresenting a broad and careful review of the most widely used fairness datasets\nin current language model research, characterizing them along several key\ndimensions including their origin, scope, content, and intended use to help\nresearchers better appreciate the assumptions and limitations embedded in these\nresources. To support more meaningful comparisons and analyses, we introduce a\nunified evaluation framework that reveals consistent patterns of demographic\ndisparities across datasets and scoring methods. Applying this framework to\ntwenty four common benchmarks, we highlight the often overlooked biases that\ncan influence conclusions about model fairness and offer practical guidance for\nselecting, combining, and interpreting these datasets. We also point to\nopportunities for creating new fairness benchmarks that reflect more diverse\nsocial contexts and encourage more thoughtful use of these tools going forward.\nAll code, data, and detailed results are publicly available at\nhttps://github.com/vanbanTruong/Fairness-in-Large-Language-Models/tree/main/datasets\nto promote transparency and reproducibility across the research community.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23411v1", "categories": ["cs.CL", "cs.CY", "cs.LG"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.23411v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.22736", "title": "UniFuse: A Unified All-in-One Framework for Multi-Modal Medical Image Fusion Under Diverse Degradations and Misalignments", "authors": ["Dayong Su", "Yafei Zhang", "Huafeng Li", "Jinxing Li", "Yu Liu"], "summary": "Current multimodal medical image fusion typically assumes that source images\nare of high quality and perfectly aligned at the pixel level. Its effectiveness\nheavily relies on these conditions and often deteriorates when handling\nmisaligned or degraded medical images. To address this, we propose UniFuse, a\ngeneral fusion framework. By embedding a degradation-aware prompt learning\nmodule, UniFuse seamlessly integrates multi-directional information from input\nimages and correlates cross-modal alignment with restoration, enabling joint\noptimization of both tasks within a unified framework. Additionally, we design\nan Omni Unified Feature Representation scheme, which leverages Spatial Mamba to\nencode multi-directional features and mitigate modality differences in feature\nalignment. To enable simultaneous restoration and fusion within an All-in-One\nconfiguration, we propose a Universal Feature Restoration & Fusion module,\nincorporating the Adaptive LoRA Synergistic Network (ALSN) based on LoRA\nprinciples. By leveraging ALSN's adaptive feature representation along with\ndegradation-type guidance, we enable joint restoration and fusion within a\nsingle-stage framework. Compared to staged approaches, UniFuse unifies\nalignment, restoration, and fusion within a single framework. Experimental\nresults across multiple datasets demonstrate the method's effectiveness and\nsignificant advantages over existing approaches.", "comment": "Accepted by ICCV2025", "pdf_url": "http://arxiv.org/pdf/2506.22736v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22736v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.22502", "title": "Stabilization of industrial processes with time series machine learning", "authors": ["Matvei Anoshin", "Olga Tsurkan", "Vadim Lopatkin", "Leonid Fedichkin"], "summary": "The stabilization of time series processes is a crucial problem that is\nubiquitous in various industrial fields. The application of machine learning to\nits solution can have a decisive impact, improving both the quality of the\nresulting stabilization with less computational resources required. In this\nwork, we present a simple pipeline consisting of two neural networks: the\noracle predictor and the optimizer, proposing a substitution of the point-wise\nvalues optimization to the problem of the neural network training, which\nsuccessfully improves stability in terms of the temperature control by about 3\ntimes compared to ordinary solvers.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22502v1", "categories": ["cs.LG", "cs.SY", "eess.SY"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22502v1", "date": "2025-06-25", "updated": "2025-06-25"}
{"id": "2506.23368", "title": "Optimizing Solar Energy Production in the USA: Time-Series Analysis Using AI for Smart Energy Management", "authors": ["Istiaq Ahmed", "Md Asif Ul Hoq Khan", "MD Zahedul Islam", "Md Sakibul Hasan", "Tanaya Jakir", "Arat Hossain", "Joynal Abed", "Muhammad Hasanuzzaman", "Sadia Sharmeen Shatyi", "Kazi Nehal Hasnain"], "summary": "As the US rapidly moves towards cleaner energy sources, solar energy is fast\nbecoming the pillar of its renewable energy mix. Even while solar energy is\nincreasingly being used, its variability is a key hindrance to grid stability,\nstorage efficiency, and system stability overall. Solar energy has emerged as\none of the fastest-growing renewable energy sources in the United States,\nadding noticeably to the country's energy mix. Retrospectively, the necessity\nof inserting the sun's energy into the grid without disrupting reliability and\ncost efficiencies highlights the necessity of good forecasting software and\nsmart control systems. The dataset utilized for this research project comprised\nboth hourly and daily solar energy production records collected from multiple\nutility-scale solar farms across diverse U.S. regions, including California,\nTexas, and Arizona. Training and evaluation of all models were performed with a\ntime-based cross-validation scheme, namely, sliding window validation. Both the\nRandom Forest and the XG-Boost models demonstrated noticeably greater and the\nsame performance across each of the measures considered, with relatively high\naccuracy. The almost perfect and equal performance by the Random Forest and\nXG-Boost models also shows both models to have learned the patterns in the data\nvery comprehensively, with high reliability in their predictions. By\nincorporating AI-powered time-series models like XG-Boost in grid management\nsoftware, utility companies can dynamically modify storage cycles in real-time\nas well as dispatch and peak load planning, based on their predictions.\nAI-powered solar forecasting also has profound implications for renewable\nenergy policy and planning, particularly as U.S. federal and state governments\naccelerate toward ambitious decarbonization goals.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23368v1", "categories": ["eess.SP"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.23368v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.24014", "title": "Simultaneous Super-Resolution of Spatial and Spectral Imaging with a Camera Array and Notch Filters", "authors": ["Peng Lin", "Xuesong Wang", "Yating Chen", "Xianyu Wu", "Feng Huang", "Shouqian Chen"], "summary": "This study proposes an algorithm based on a notch filter camera array system\nfor simultaneous super-resolution imaging and spectral reconstruction,\nenhancing the spatial resolution and multispectral imaging capabilities of\ntargets. In this study, multi-aperture super-resolution algorithms,\npan-sharpening techniques, and spectral reconstruction algorithms were\ninvestigated and integrated. The sub-pixel level offset information and\nspectral disparities among the 9 low-resolution images captured by the 9\ndistinct imaging apertures were utilized, leading to the successful\nreconstruction of 31 super-resolution spectral images. By conducting\nsimulations with a publicly available dataset and performing qualitative and\nquantitative comparisons with snapshot coded aperture spectral imaging systems,\nthe experimental results demonstrate that our system and algorithm attained a\npeak signal-to-noise ratio of 35.6dB, representing a 5dB enhancement over the\nmost advanced snapshot coded aperture spectral imaging systems, while also\nreducing processing time. This research offers an effective solution for\nachieving high temporal, spectral, and spatial resolution through the\nutilization of multi-aperture imaging systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.24014v1", "categories": ["eess.IV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.24014v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22845", "title": "Quantum Neural Networks for Wind Energy Forecasting: A Comparative Study of Performance and Scalability with Classical Models", "authors": ["Batuhan Hangun", "Oguz Altun", "Onder Eyecioglu"], "summary": "Quantum Neural Networks (QNNs), a prominent approach in Quantum Machine\nLearning (QML), are emerging as a powerful alternative to classical machine\nlearning methods. Recent studies have focused on the applicability of QNNs to\nvarious tasks, such as time-series forecasting, prediction, and classification,\nacross a wide range of applications, including cybersecurity and medical\nimaging. With the increased use of smart grids driven by the integration of\nrenewable energy systems, machine learning plays an important role in\npredicting power demand and detecting system disturbances. This study provides\nan in-depth investigation of QNNs for predicting the power output of a wind\nturbine. We assess the predictive performance and simulation time of six QNN\nconfigurations that are based on the Z Feature Map for data encoding and\nvarying ansatz structures. Through detailed cross-validation experiments and\ntests on an unseen hold-out dataset, we experimentally demonstrate that QNNs\ncan achieve predictive performance that is competitive with, and in some cases\nmarginally better than, the benchmarked classical approaches. Our results also\nreveal the effects of dataset size and circuit complexity on predictive\nperformance and simulation time. We believe our findings will offer valuable\ninsights for researchers in the energy domain who wish to incorporate quantum\nmachine learning into their work.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22845v1", "categories": ["cs.LG", "cs.AI", "cs.PF"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22845v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23689", "title": "Pok√©AI: A Goal-Generating, Battle-Optimizing Multi-agent System for Pokemon Red", "authors": ["Zihao Liu", "Xinhang Sui", "Yueran Song", "Siwen Wang"], "summary": "We introduce Pok\\'eAI, the first text-based, multi-agent large language model\n(LLM) framework designed to autonomously play and progress through Pok\\'emon\nRed. Our system consists of three specialized agents-Planning, Execution, and\nCritique-each with its own memory bank, role, and skill set. The Planning Agent\nfunctions as the central brain, generating tasks to progress through the game.\nThese tasks are then delegated to the Execution Agent, which carries them out\nwithin the game environment. Upon task completion, the Critique Agent evaluates\nthe outcome to determine whether the objective was successfully achieved. Once\nverification is complete, control returns to the Planning Agent, forming a\nclosed-loop decision-making system.\n  As a preliminary step, we developed a battle module within the Execution\nAgent. Our results show that the battle AI achieves an average win rate of\n80.8% across 50 wild encounters, only 6% lower than the performance of an\nexperienced human player. Furthermore, we find that a model's battle\nperformance correlates strongly with its LLM Arena score on language-related\ntasks, indicating a meaningful link between linguistic ability and strategic\nreasoning. Finally, our analysis of gameplay logs reveals that each LLM\nexhibits a unique playstyle, suggesting that individual models develop distinct\nstrategic behaviors.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23689v1", "categories": ["cs.AI", "cs.MA"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.23689v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23056", "title": "Boosting LLM's Molecular Structure Elucidation with Knowledge Enhanced Tree Search Reasoning", "authors": ["Xiang Zhuang", "Bin Wu", "Jiyu Cui", "Kehua Feng", "Xiaotong Li", "Huabin Xing", "Keyan Ding", "Qiang Zhang", "Huajun Chen"], "summary": "Molecular structure elucidation involves deducing a molecule's structure from\nvarious types of spectral data, which is crucial in chemical experimental\nanalysis. While large language models (LLMs) have shown remarkable proficiency\nin analyzing and reasoning through complex tasks, they still encounter\nsubstantial challenges in molecular structure elucidation. We identify that\nthese challenges largely stem from LLMs' limited grasp of specialized chemical\nknowledge. In this work, we introduce a Knowledge-enhanced reasoning framework\nfor Molecular Structure Elucidation (K-MSE), leveraging Monte Carlo Tree Search\nfor test-time scaling as a plugin. Specifically, we construct an external\nmolecular substructure knowledge base to extend the LLMs' coverage of the\nchemical structure space. Furthermore, we design a specialized\nmolecule-spectrum scorer to act as a reward model for the reasoning process,\naddressing the issue of inaccurate solution evaluation in LLMs. Experimental\nresults show that our approach significantly boosts performance, particularly\ngaining more than 20% improvement on both GPT-4o-mini and GPT-4o. Our code is\navailable at https://github.com/HICAI-ZJU/K-MSE.", "comment": "ACL 2025 Main", "pdf_url": "http://arxiv.org/pdf/2506.23056v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.23056v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.22851", "title": "Deep neural networks can provably solve Bellman equations for Markov decision processes without the curse of dimensionality", "authors": ["Arnulf Jentzen", "Konrad Kleinberg", "Thomas Kruse"], "summary": "Discrete time stochastic optimal control problems and Markov decision\nprocesses (MDPs) are fundamental models for sequential decision-making under\nuncertainty and as such provide the mathematical framework underlying\nreinforcement learning theory. A central tool for solving MDPs is the Bellman\nequation and its solution, the so-called $Q$-function. In this article, we\nconstruct deep neural network (DNN) approximations for $Q$-functions associated\nto MDPs with infinite time horizon and finite control set $A$. More\nspecifically, we show that if the the payoff function and the random transition\ndynamics of the MDP can be suitably approximated by DNNs with leaky rectified\nlinear unit (ReLU) activation, then the solutions $Q_d\\colon \\mathbb R^d\\to\n\\mathbb R^{|A|}$, $d\\in \\mathbb{N}$, of the associated Bellman equations can\nalso be approximated in the $L^2$-sense by DNNs with leaky ReLU activation\nwhose numbers of parameters grow at most polynomially in both the dimension\n$d\\in \\mathbb{N}$ of the state space and the reciprocal $1/\\varepsilon$ of the\nprescribed error $\\varepsilon\\in (0,1)$. Our proof relies on the recently\nintroduced full-history recursive multilevel fixed-point (MLFP) approximation\nscheme.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22851v1", "categories": ["math.OC", "cs.LG", "cs.NA", "math.NA", "math.PR", "stat.ML", "90C40, 90C39, 60J05, 93E20, 65C05, 68T07"], "cate": "math.OC", "url": "http://arxiv.org/abs/2506.22851v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23866", "title": "Exploring Privacy and Security as Drivers for Environmental Sustainability in Cloud-Based Office Solutions", "authors": ["Jason Kayembe", "Iness Ben Guirat", "Jan Tobias M√ºhlberg"], "summary": "In this paper, we explore the intersection of privacy, security, and\nenvironmental sustainability in cloud-based office solutions, focusing on\nquantifying user- and network-side energy use and associated carbon emissions.\nWe hypothesise that privacy-focused services are typically more\nenergy-efficient than those funded through data collection and advertising. To\nevaluate this, we propose a framework that systematically measures\nenvironmental costs based on energy usage and network data traffic during\nwell-defined, automated usage scenarios. To test our hypothesis, we first\nanalyse how underlying architectures and business models, such as monetisation\nthrough personalised advertising, contribute to the environmental footprint of\nthese services. We then explore existing methodologies and tools for software\nenvironmental impact assessment. We apply our framework to three mainstream\nemail services selected to reflect different privacy policies, from\nad-supported tracking-intensive models to privacy-focused designs: Microsoft\nOutlook, Google Mail (Gmail), and Proton Mail. We extend this comparison to a\nself-hosted email solution, evaluated with and without end-to-end encryption.\nWe show that the self-hosted solution, even with 14% of device energy and 15%\nof emissions overheads from PGP encryption, remains the most energy-efficient,\nsaving up to 33% of emissions per session compared to Gmail. Among commercial\nproviders, Proton Mail is the most efficient, saving up to 0.1 gCO2 e per\nsession compared to Outlook, whose emissions can be further reduced by 2%\nthrough ad-blocking.", "comment": "Post-proceedings paper persented at LOCO '24: 1st International\n  Workshop on Low Carbon Computing, 2024-12-03, in Glasgow, UK", "pdf_url": "http://arxiv.org/pdf/2506.23866v1", "categories": ["cs.CR", "cs.CY", "cs.SE"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.23866v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23624", "title": "Towards Universal Shared Control in Teleoperation Without Haptic Feedback", "authors": ["Max Grobbel", "Tristan Schneider", "S√∂ren Hohmann"], "summary": "Teleoperation with non-haptic VR controllers deprives human operators of\ncritical motion feedback. We address this by embedding a multi-objective\noptimization problem that converts user input into collision-free UR5e joint\ntrajectories while actively suppressing liquid slosh in a glass. The controller\nmaintains 13 ms average planning latency, confirming real-time performance and\nmotivating the augmentation of this teleoperation approach to further\nobjectives.", "comment": "5 pages, submitted to IEEE Telepresence 2025 conference", "pdf_url": "http://arxiv.org/pdf/2506.23624v1", "categories": ["cs.RO", "cs.SY", "eess.SY"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.23624v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22462", "title": "Privacy-aware IoT Fall Detection Services For Aging in Place", "authors": ["Abdallah Lakhdari", "Jiajie Li", "Amani Abusafia", "Athman Bouguettaya"], "summary": "Fall detection is critical to support the growing elderly population,\nprojected to reach 2.1 billion by 2050. However, existing methods often face\ndata scarcity challenges or compromise privacy. We propose a novel IoT-based\nFall Detection as a Service (FDaaS) framework to assist the elderly in living\nindependently and safely by accurately detecting falls. We design a\nservice-oriented architecture that leverages Ultra-wideband (UWB) radar sensors\nas an IoT health-sensing service, ensuring privacy and minimal intrusion. We\naddress the challenges of data scarcity by utilizing a Fall Detection\nGenerative Pre-trained Transformer (FD-GPT) that uses augmentation techniques.\nWe developed a protocol to collect a comprehensive dataset of the elderly daily\nactivities and fall events. This resulted in a real dataset that carefully\nmimics the elderly's routine. We rigorously evaluate and compare various models\nusing this dataset. Experimental results show our approach achieves 90.72%\naccuracy and 89.33% precision in distinguishing between fall events and regular\nactivities of daily living.", "comment": "11 pages, 12 figures, This paper is accepted in the 2025 IEEE\n  International Conference on Web Services (ICWS 2025)", "pdf_url": "http://arxiv.org/pdf/2506.22462v1", "categories": ["eess.SP", "cs.AI", "cs.CY", "cs.HC"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.22462v1", "date": "2025-06-18", "updated": "2025-06-18"}
{"id": "2506.23610", "title": "Evaluating the Simulation of Human Personality-Driven Susceptibility to Misinformation with LLMs", "authors": ["Manuel Pratelli", "Marinella Petrocchi"], "summary": "Large language models (LLMs) make it possible to generate synthetic\nbehavioural data at scale, offering an ethical and low-cost alternative to\nhuman experiments. Whether such data can faithfully capture psychological\ndifferences driven by personality traits, however, remains an open question. We\nevaluate the capacity of LLM agents, conditioned on Big-Five profiles, to\nreproduce personality-based variation in susceptibility to misinformation,\nfocusing on news discernment, the ability to judge true headlines as true and\nfalse headlines as false. Leveraging published datasets in which human\nparticipants with known personality profiles rated headline accuracy, we create\nmatching LLM agents and compare their responses to the original human patterns.\nCertain trait-misinformation associations, notably those involving\nAgreeableness and Conscientiousness, are reliably replicated, whereas others\ndiverge, revealing systematic biases in how LLMs internalize and express\npersonality. The results underscore both the promise and the limits of\npersonality-aligned LLMs for behavioral simulation, and offer new insight into\nmodeling cognitive diversity in artificial agents.", "comment": "pre-print version - paper actually under submission", "pdf_url": "http://arxiv.org/pdf/2506.23610v1", "categories": ["cs.CL", "cs.CY"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.23610v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22749", "title": "Deep Learning based Joint Geometry and Attribute Up-sampling for Large-Scale Colored Point Clouds", "authors": ["Yun Zhang", "Feifan Chen", "Na Li", "Zhiwei Guo", "Xu Wang", "Fen Miao", "Sam Kwong"], "summary": "Colored point cloud, which includes geometry and attribute components, is a\nmainstream representation enabling realistic and immersive 3D applications. To\ngenerate large-scale and denser colored point clouds, we propose a deep\nlearning-based Joint Geometry and Attribute Up-sampling (JGAU) method that\nlearns to model both geometry and attribute patterns while leveraging spatial\nattribute correlations. First, we establish and release a large-scale dataset\nfor colored point cloud up-sampling called SYSU-PCUD, containing 121\nlarge-scale colored point clouds with diverse geometry and attribute\ncomplexities across six categories and four sampling rates. Second, to improve\nthe quality of up-sampled point clouds, we propose a deep learning-based JGAU\nframework that jointly up-samples geometry and attributes. It consists of a\ngeometry up-sampling network and an attribute up-sampling network, where the\nlatter leverages the up-sampled auxiliary geometry to model neighborhood\ncorrelations of the attributes. Third, we propose two coarse attribute\nup-sampling methods, Geometric Distance Weighted Attribute Interpolation\n(GDWAI) and Deep Learning-based Attribute Interpolation (DLAI), to generate\ncoarse up-sampled attributes for each point. Then, an attribute enhancement\nmodule is introduced to refine these up-sampled attributes and produce\nhigh-quality point clouds by further exploiting intrinsic attribute and\ngeometry patterns. Extensive experiments show that the Peak Signal-to-Noise\nRatio (PSNR) achieved by the proposed JGAU method is 33.90 decibels, 32.10\ndecibels, 31.10 decibels, and 30.39 decibels for up-sampling rates of 4 times,\n8 times, 12 times, and 16 times, respectively. Compared to state-of-the-art\nmethods, JGAU achieves average PSNR gains of 2.32 decibels, 2.47 decibels, 2.28\ndecibels, and 2.11 decibels at these four up-sampling rates, demonstrating\nsignificant improvement.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22749v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22749v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.22617", "title": "On beam characterization of ground-based CMB radio telescopes using UAV-mounted sources: application to the QUIJOTE TFGI and plans for LSPE-Strip", "authors": ["Fabio Paonessa", "Lorenzo Ciorba", "Giuseppe Addamo", "Paz Alonso-Arias", "Barbara Caccianiga", "Marco Bersanelli", "Francesco Cuttaia", "Cristian Franceschet", "Ricardo Tanausu Genova Santos", "Massimo Gervasi", "Roger Hoyland", "Mike Jones", "Carlos Hugo Lopez-Caraballo", "Mauro Lumia", "Michele Maris", "Aniello Mennella", "Gianluca Morgante", "Oscar Antonio Peverini", "Sabrina Realini", "Jose Alberto Rubino-Martin", "Stefano Sartor", "Angela Taylor", "Fabrizio Villa", "Mario Zannoni", "Giuseppe Virone"], "summary": "The Large Scale Polarization Explorer (LSPE) project, funded by the Italian\nSpace Agency (ASI), includes the development of LSPE-Strip, a ground-based\nradio telescope for observing Cosmic Microwave Background (CMB) anisotropies.\nLSPE-Strip, nearing its construction phase, will operate from the Teide\nObservatory in Tenerife, employing 49 coherent polarimeters at 43 GHz to\ndeliver critical data on CMB anisotropies and 6 channels at 95 GHz as\natmospheric monitor. On-site characterization of such advanced instruments is\ncrucial to detect possible systematic effects, such as gain fluctuations, beam\ndistortions, and pointing errors, that can compromise performance by\nintroducing spurious polarizations or radiation collection from unintended\ndirections. To address these challenges, a drone-mounted Q-band test source for\non-site characterization of LSPE-Strip's polarimeter array was developed.\nModern Unmanned Aerial Vehicles (UAVs) offer a flexible approach for antenna\npattern measurements, yet their use in high-frequency radio astronomy is not\nconsolidated practice. In October 2022, a UAV-based measurement campaign was\nconducted with the TFGI instrument on the second QUIJOTE telescope in Tenerife,\nin collaboration with the Instituto de Astrofisica de Canarias. This pioneering\neffort aimed to validate UAV-based beam characterization methods and assess\nQUIJOTE's performance under operational conditions. Preliminary results\ndemonstrated high measurement accuracy, leveraging QUIJOTE's dual-receiver\nconfiguration for beam validation. These findings provide valuable insights for\noptimizing UAV systems in preparation for LSPE-Strip's future characterization.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22617v1", "categories": ["astro-ph.IM", "cs.SY", "eess.SY"], "cate": "astro-ph.IM", "url": "http://arxiv.org/abs/2506.22617v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.23410", "title": "Integrated Polarimetric Sensing and Communication with Polarization-Reconfigurable Arrays", "authors": ["Byunghyun Lee", "Rang Liu", "David J. Love", "James V. Krogmeier", "A. Lee Swindlehurst"], "summary": "Polarization diversity offers a cost- and space-efficient solution to enhance\nthe performance of integrated sensing and communication systems. Polarimetric\nsensing exploits the signal's polarity to extract details about the target such\nas shape, pose, and material composition. From a communication perspective,\npolarization diversity can enhance the reliability and throughput of\ncommunication channels. This paper proposes an integrated polarimetric sensing\nand communication (IPSAC) system that jointly conducts polarimetric sensing and\ncommunications. We study the use of single-port polarization-reconfigurable\nantennas to adapt to channel depolarization effects, without the need for\nseparate RF chains for each polarization. We address the problem of optimizing\nwaveforms and polarizations based on two sensing metrics. We first consider\nminimizing the mean square error (MSE) of the target depolarization parameter\nestimate, which is a critical task for various polarimetric radar applications\nsuch as rainfall forecasting, vegetation identification, and target\nclassification. To address this nonconvex problem, we apply semi-definite\nrelaxation (SDR) and majorization-minimization (MM) optimization techniques.\nNext, we consider a design that maximizes the target\nsignal-to-interference-plus-noise ratio (SINR) leveraging prior knowledge of\nthe target and clutter depolarization statistics to enhance the target\ndetection performance. To tackle this problem, we modify the solution developed\nfor MSE minimization subject to the same quality-of-service (QoS) constraints.\nExtensive simulations show that the proposed polarization reconfiguration\nmethod substantially improves the depolarization parameter MSE. Furthermore,\nthe proposed method considerably boosts the target SINR due to polarization\ndiversity, particularly in cluttered environments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23410v1", "categories": ["eess.SP"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.23410v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.24074", "title": "C3VDv2 -- Colonoscopy 3D video dataset with enhanced realism", "authors": ["Mayank V. Golhar", "Lucas Sebastian Galeano Fretes", "Loren Ayers", "Venkata S. Akshintala", "Taylor L. Bobrow", "Nicholas J. Durr"], "summary": "Computer vision techniques have the potential to improve the diagnostic\nperformance of colonoscopy, but the lack of 3D colonoscopy datasets for\ntraining and validation hinders their development. This paper introduces\nC3VDv2, the second version (v2) of the high-definition Colonoscopy 3D Video\nDataset, featuring enhanced realism designed to facilitate the quantitative\nevaluation of 3D colon reconstruction algorithms. 192 video sequences were\ncaptured by imaging 60 unique, high-fidelity silicone colon phantom segments.\nGround truth depth, surface normals, optical flow, occlusion,\nsix-degree-of-freedom pose, coverage maps, and 3D models are provided for 169\ncolonoscopy videos. Eight simulated screening colonoscopy videos acquired by a\ngastroenterologist are provided with ground truth poses. The dataset includes\n15 videos featuring colon deformations for qualitative assessment. C3VDv2\nemulates diverse and challenging scenarios for 3D reconstruction algorithms,\nincluding fecal debris, mucous pools, blood, debris obscuring the colonoscope\nlens, en-face views, and fast camera motion. The enhanced realism of C3VDv2\nwill allow for more robust and representative development and evaluation of 3D\nreconstruction algorithms.", "comment": "19 pages, 7 figures", "pdf_url": "http://arxiv.org/pdf/2506.24074v1", "categories": ["eess.IV", "cs.CV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.24074v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22848", "title": "Scalable Structure Learning of Bayesian Networks by Learning Algorithm Ensembles", "authors": ["Shengcai Liu", "Hui Ou-yang", "Zhiyuan Wang", "Cheng Chen", "Qijun Cai", "Yew-Soon Ong", "Ke Tang"], "summary": "Learning the structure of Bayesian networks (BNs) from data is challenging,\nespecially for datasets involving a large number of variables. The recently\nproposed divide-and-conquer (D\\&D) strategies present a promising approach for\nlearning large BNs. However, they still face a main issue of unstable learning\naccuracy across subproblems. In this work, we introduce the idea of employing\nstructure learning ensemble (SLE), which combines multiple BN structure\nlearning algorithms, to consistently achieve high learning accuracy. We further\npropose an automatic approach called Auto-SLE for learning near-optimal SLEs,\naddressing the challenge of manually designing high-quality SLEs. The learned\nSLE is then integrated into a D\\&D method. Extensive experiments firmly show\nthe superiority of our method over D\\&D methods with single BN structure\nlearning algorithm in learning large BNs, achieving accuracy improvement\nusually by 30\\%$\\sim$225\\% on datasets involving 10,000 variables. Furthermore,\nour method generalizes well to datasets with many more (e.g., 30000) variables\nand different network characteristics than those present in the training data\nfor learning the SLE. These results indicate the significant potential of\nemploying (automatic learning of) SLEs for scalable BN structure learning.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22848v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22848v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23692", "title": "Agent4S: The Transformation of Research Paradigms from the Perspective of Large Language Models", "authors": ["Boyuan Zheng", "Zerui Fang", "Zhe Xu", "Rui Wang", "Yiwen Chen", "Cunshi Wang", "Mengwei Qu", "Lei Lei", "Zhen Feng", "Yan Liu", "Yuyang Li", "Mingzhou Tan", "Jiaji Wu", "Jianwei Shuai", "Jia Li", "Fangfu Ye"], "summary": "While AI for Science (AI4S) serves as an analytical tool in the current\nresearch paradigm, it doesn't solve its core inefficiency. We propose \"Agent\nfor Science\" (Agent4S)-the use of LLM-driven agents to automate the entire\nresearch workflow-as the true Fifth Scientific Paradigm. This paper introduces\na five-level classification for Agent4S, outlining a clear roadmap from simple\ntask automation to fully autonomous, collaborative \"AI Scientists.\" This\nframework defines the next revolutionary step in scientific discovery.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23692v1", "categories": ["cs.AI"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.23692v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23071", "title": "Text2VectorSQL: Bridging Text-to-SQL and Vector Search for Unified Natural Language Queries", "authors": ["Zhengren Wang", "Bozhou Li", "Dongwen Yao", "Wentao Zhang"], "summary": "While Text-to-SQL enables natural language interaction with structured\ndatabases, its effectiveness diminishes with unstructured data or ambiguous\nqueries due to rigid syntax and limited expressiveness. Concurrently, vector\nsearch has emerged as a powerful paradigm for semantic retrieval, particularly\nfor unstructured data. However, existing VectorSQL implementations still rely\nheavily on manual crafting and lack tailored evaluation frameworks, leaving a\nsignificant gap between theoretical potential and practical deployment. To\nbridge these complementary paradigms, we introduces Text2VectorSQL, a novel\nframework unifying Text-to-SQL and vector search to overcome expressiveness\nconstraints and support more diverse and holistical natural language queries.\nSpecifically, Text2VectorSQL enables semantic filtering, multi-modal matching,\nand retrieval acceleration. For evaluation, we build vector index on\nappropriate columns, extend user queries with semantic search, and annotate\nground truths via an automatic pipeline with expert review. Furthermore, we\ndevelop dedicated Text2VectorSQL models with synthetic data, demonstrating\nsignificant performance improvements over baseline methods. Our work\nestablishes the foundation for the Text2VectorSQL task, paving the way for more\nversatile and intuitive database interfaces. The repository will be publicly\navailable at https://github.com/Open-DataFlow/Text2VectorSQL.", "comment": "Work in progess", "pdf_url": "http://arxiv.org/pdf/2506.23071v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.23071v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.22935", "title": "Differentiable Radar Ambiguity Functions: Mathematical Formulation and Computational Implementation", "authors": ["Marc Bara Iniesta"], "summary": "The ambiguity function is fundamental to radar waveform design,\ncharacterizing range and Doppler resolution capabilities. However, its\ntraditional formulation involves non-differentiable operations, preventing\nintegration with gradient-based optimization methods and modern machine\nlearning frameworks. This paper presents the first complete mathematical\nframework and computational implementation for differentiable radar ambiguity\nfunctions. Our approach addresses the fundamental technical challenges that\nhave prevented the radar community from leveraging automatic differentiation:\nproper handling of complex-valued gradients using Wirtinger calculus, efficient\ncomputation through parallelized FFT operations, numerical stability throughout\ncascaded operations, and composability with arbitrary differentiable\noperations. We term this approach GRAF (Gradient-based Radar Ambiguity\nFunctions), which reformulates the ambiguity function computation to maintain\nmathematical equivalence while enabling gradient flow through the entire\npipeline. The resulting implementation provides a general-purpose\ndifferentiable ambiguity function compatible with modern automatic\ndifferentiation frameworks, enabling new research directions including neural\nnetwork-based waveform generation with ambiguity constraints, end-to-end\noptimization of radar systems, and integration of classical radar theory with\nmodern deep learning. We provide complete implementation details and\ndemonstrate computational efficiency suitable for practical applications. This\nwork establishes the mathematical and computational foundation for applying\nmodern machine learning techniques to radar waveform design, bridging classical\nradar signal processing with automatic differentiation frameworks.", "comment": "16 pages, 4 figures, source code available at\n  https://github.com/marcbara/graf-psl-lpi (DOI: 10.5281/zenodo.15763301)", "pdf_url": "http://arxiv.org/pdf/2506.22935v1", "categories": ["eess.SP", "cs.LG", "cs.NA", "math.NA", "94A12, 65T50, 68T05", "F.2.1; I.2.6; G.1.0"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.22935v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23909", "title": "RawMal-TF: Raw Malware Dataset Labeled by Type and Family", "authors": ["David B√°lik", "Martin Jureƒçek", "Mark Stamp"], "summary": "This work addresses the challenge of malware classification using machine\nlearning by developing a novel dataset labeled at both the malware type and\nfamily levels. Raw binaries were collected from sources such as VirusShare, VX\nUnderground, and MalwareBazaar, and subsequently labeled with family\ninformation parsed from binary names and type-level labels integrated from\nClarAVy. The dataset includes 14 malware types and 17 malware families, and was\nprocessed using a unified feature extraction pipeline based on static analysis,\nparticularly extracting features from Portable Executable headers, to support\nadvanced classification tasks. The evaluation was focused on three key\nclassification tasks. In the binary classification of malware versus benign\nsamples, Random Forest and XGBoost achieved high accuracy on the full datasets,\nreaching 98.5% for type-based detection and 98.98% for family-based detection.\nWhen using truncated datasets of 1,000 samples to assess performance under\nlimited data conditions, both models still performed strongly, achieving 97.6%\nfor type-based detection and 98.66% for family-based detection. For interclass\nclassification, which distinguishes between malware types or families, the\nmodels reached up to 97.5% accuracy on type-level tasks and up to 93.7% on\nfamily-level tasks. In the multiclass classification setting, which assigns\nsamples to the correct type or family, SVM achieved 81.1% accuracy on type\nlabels, while Random Forest and XGBoost reached approximately 73.4% on family\nlabels. The results highlight practical trade-offs between accuracy and\ncomputational cost, and demonstrate that labeling at both the type and family\nlevels enables more fine-grained and insightful malware classification. The\nwork establishes a robust foundation for future research on advanced malware\ndetection and classification.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23909v1", "categories": ["cs.CR", "cs.LG"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.23909v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23723", "title": "A comprehensive control architecture for semi-autonomous dual-arm robots in agriculture settings", "authors": ["Jozsef Palmieri", "Paolo Di Lillo", "Stefano Chiaverini", "Alessandro Marino"], "summary": "The adoption of mobile robotic platforms in complex environments, such as\nagricultural settings, requires these systems to exhibit a flexible yet\neffective architecture that integrates perception and control. In such\nscenarios, several tasks need to be accomplished simultaneously, ranging from\nmanaging robot limits to performing operational tasks and handling human\ninputs. The purpose of this paper is to present a comprehensive control\narchitecture for achieving complex tasks such as robotized harvesting in\nvineyards within the framework of the European project CANOPIES. In detail, a\n16-DOF dual-arm mobile robot is employed, controlled via a Hierarchical\nQuadratic Programming (HQP) approach capable of handling both equality and\ninequality constraints at various priorities to harvest grape bunches selected\nby the perception system developed within the project. Furthermore, given the\ncomplexity of the scenario and the uncertainty in the perception system, which\ncould potentially lead to collisions with the environment, the handling of\ninteraction forces is necessary. Remarkably, this was achieved using the same\nHQP framework. This feature is further leveraged to enable semi-autonomous\noperations, allowing a human operator to assist the robotic counterpart in\ncompleting harvesting tasks. Finally, the obtained results are validated\nthrough extensive testing conducted first in a laboratory environment to prove\nindividual functionalities, then in a real vineyard, encompassing both\nautonomous and semi-autonomous grape harvesting operations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23723v1", "categories": ["cs.RO"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.23723v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22464", "title": "Golden Ratio Assisted Localization for Wireless Sensor Network", "authors": ["Hitesh Mohapatra"], "summary": "This paper presents a novel localization algorithm for wireless sensor\nnetworks (WSNs) called Golden Ratio Localization (GRL), which leverages the\nmathematical properties of the golden ratio (phi 1.618) to optimize both node\nplacement and communication range. GRL introduces phi-based anchor node\ndeployment and hop-sensitive weighting using phi-exponents to improve\nlocalization accuracy while minimizing energy consumption. Through extensive\nsimulations conducted on a 100 m * 100 m sensor field with 100 nodes and 10\nanchors, GRL achieved an average localization error of 2.35 meters,\noutperforming DV- Hop (3.87 meters) and Centroid (4.95 meters). In terms of\nenergy efficiency, GRL reduced localization energy consumption to 1.12 microJ\nper node, compared to 1.78 microJ for DV-Hop and 1.45 microJ for Centroid.\nThese results confirm that GRL provides a more balanced and efficient\nlocalization approach, making it especially suitable for energy-constrained and\nlarge-scale WSN deployments.", "comment": "6", "pdf_url": "http://arxiv.org/pdf/2506.22464v1", "categories": ["cs.NI", "cs.HC", "B.4"], "cate": "cs.NI", "url": "http://arxiv.org/abs/2506.22464v1", "date": "2025-06-18", "updated": "2025-06-18"}
{"id": "2506.23826", "title": "Towards the \"Digital Me\": A vision of authentic Conversational Agents powered by personal Human Digital Twins", "authors": ["Llu√≠s C. Coll", "Martin W. Lauer-Schmaltz", "Philip Cash", "John P. Hansen", "Anja Maier"], "summary": "Human Digital Twins (HDTs) have traditionally been conceptualized as\ndata-driven models designed to support decision-making across various domains.\nHowever, recent advancements in conversational AI open new possibilities for\nHDTs to function as authentic, interactive digital counterparts of individuals.\nThis paper introduces a novel HDT system architecture that integrates large\nlanguage models with dynamically updated personal data, enabling it to mirror\nan individual's conversational style, memories, and behaviors. To achieve this,\nour approach implements context-aware memory retrieval, neural\nplasticity-inspired consolidation, and adaptive learning mechanisms, creating a\nmore natural and evolving digital persona. The resulting system does not only\nreplicate an individual's unique conversational style depending on who they are\nspeaking with, but also enriches responses with dynamically captured personal\nexperiences, opinions, and memories. While this marks a significant step toward\ndeveloping authentic virtual counterparts, it also raises critical ethical\nconcerns regarding privacy, accountability, and the long-term implications of\npersistent digital identities. This study contributes to the field of HDTs by\ndescribing our novel system architecture, demonstrating its capabilities, and\ndiscussing future directions and emerging challenges to ensure the responsible\nand ethical development of HDTs.", "comment": "24 pages, 9 figures", "pdf_url": "http://arxiv.org/pdf/2506.23826v1", "categories": ["cs.ET", "cs.AI", "cs.CY", "cs.HC", "cs.IR"], "cate": "cs.ET", "url": "http://arxiv.org/abs/2506.23826v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22753", "title": "Degradation-Modeled Multipath Diffusion for Tunable Metalens Photography", "authors": ["Jianing Zhang", "Jiayi Zhu", "Feiyu Ji", "Xiaokang Yang", "Xiaoyun Yuan"], "summary": "Metalenses offer significant potential for ultra-compact computational\nimaging but face challenges from complex optical degradation and computational\nrestoration difficulties. Existing methods typically rely on precise optical\ncalibration or massive paired datasets, which are non-trivial for real-world\nimaging systems. Furthermore, a lack of control over the inference process\noften results in undesirable hallucinated artifacts. We introduce\nDegradation-Modeled Multipath Diffusion for tunable metalens photography,\nleveraging powerful natural image priors from pretrained models instead of\nlarge datasets. Our framework uses positive, neutral, and negative-prompt paths\nto balance high-frequency detail generation, structural fidelity, and\nsuppression of metalens-specific degradation, alongside \\textit{pseudo} data\naugmentation. A tunable decoder enables controlled trade-offs between fidelity\nand perceptual quality. Additionally, a spatially varying degradation-aware\nattention (SVDA) module adaptively models complex optical and sensor-induced\ndegradation. Finally, we design and build a millimeter-scale MetaCamera for\nreal-world validation. Extensive results show that our approach outperforms\nstate-of-the-art methods, achieving high-fidelity and sharp image\nreconstruction. More materials: https://dmdiff.github.io/.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22753v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22753v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.22705", "title": "A Mixed-Signal Photonic SRAM-based High-Speed Energy-Efficient Photonic Tensor Core with Novel Electro-Optic ADC", "authors": ["Md Abdullah-Al Kaiser", "Sugeet Sunder", "Ajey P. Jacob", "Akhilesh R. Jaiswal"], "summary": "The rapid surge in data generated by Internet of Things (IoT), artificial\nintelligence (AI), and machine learning (ML) applications demands ultra-fast,\nscalable, and energy-efficient hardware, as traditional von Neumann\narchitectures face significant latency and power challenges due to data\ntransfer bottlenecks between memory and processing units. Furthermore,\nconventional electrical memory technologies are increasingly constrained by\nrising bitline and wordline capacitance, as well as the resistance of compact\nand long interconnects, as technology scales. In contrast, photonics-based\nin-memory computing systems offer substantial speed and energy improvements\nover traditional transistor-based systems, owing to their ultra-fast operating\nfrequencies, low crosstalk, and high data bandwidth. Hence, we present a novel\ndifferential photonic SRAM (pSRAM) bitcell-augmented scalable mixed-signal\nmulti-bit photonic tensor core, enabling high-speed, energy-efficient matrix\nmultiplication operations using fabrication-friendly integrated photonic\ncomponents. Additionally, we propose a novel 1-hot encoding electro-optic\nanalog-to-digital converter (eoADC) architecture to convert the multiplication\noutputs into digital bitstreams, supporting processing in the electrical\ndomain. Our designed photonic tensor core, utilizing GlobalFoundries'\nmonolithic 45SPCLO technology node, achieves computation speeds of 4.10\ntera-operations per second (TOPS) and a power efficiency of 3.02 TOPS/W.", "comment": "7 pages, 10 figures, 1 table", "pdf_url": "http://arxiv.org/pdf/2506.22705v1", "categories": ["physics.optics", "cs.SY", "eess.SY"], "cate": "physics.optics", "url": "http://arxiv.org/abs/2506.22705v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23432", "title": "All-Optical Inter-Satellite Relays with Intelligent Beam Control: Harnessing Liquid Lenses and Optical Hard Limiters", "authors": ["Mohammad Taghi Dabiri", "Mazen Hasna", "Saud Althunibat", "Khalid Qaraqe"], "summary": "Low Earth orbit (LEO) satellite constellations are emerging as a key enabler\nof next-generation communications, offering global coverage and significantly\nlower latency compared to traditional terrestrial networks and geostationary\nsatellites. However, further latency reduction is essential for time-critical\napplications such as real-time sensing, autonomous systems, and interactive\nservices. One critical bottleneck is the optical-to-electrical (O/E) and\nelectrical-to-optical (E/O) conversions at intermediate nodes in multi-hop\nlinks, which introduce unwanted processing delays. To address this, we\ninvestigate an all-optical relay system based on Optical Hard Limiters (OHL),\nwhich operate purely in the optical domain to suppress noise and restore signal\nquality without requiring O/E conversions. First, we present a rigorous\nanalysis of inter-satellite multi-relay communication under the OHL relaying\narchitecture, comparing it against conventional Amplify-and-Forward (AF) and\nDecode-and-Forward (DF) schemes. Through this comparison, we highlight both the\nadvantages and limitations of OHL relays, including their particular\nsensitivity to parameter choices such as the threshold setting and divergence\nangle at the transmitter. Recognizing that a LEO constellation is inherently\ntime-varying - satellites move relative to one another, causing continuous\nchanges in link distances and tracking errors - we propose a joint optimization\nstrategy. This scheme adaptively tunes the OHL decision threshold and beam\ndivergence in real time to maintain optimal performance, ultimately lowering\nerror rates and latency. Extensive simulations in a large-scale LEO network\ndemonstrate the viability of our method and offer insights into practical\nimplementation for next-generation inter-satellite communication systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23432v1", "categories": ["eess.SP"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.23432v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.22447", "title": "Vision Transformers for Multi-Variable Climate Downscaling: Emulating Regional Climate Models with a Shared Encoder and Multi-Decoder Architecture", "authors": ["Fabio Merizzi", "Harilaos Loukos"], "summary": "Global Climate Models (GCMs) are critical for simulating large-scale climate\ndynamics, but their coarse spatial resolution limits their applicability in\nregional studies. Regional Climate Models (RCMs) refine this through dynamic\ndownscaling, albeit at considerable computational cost and with limited\nflexibility. While deep learning has emerged as an efficient data-driven\nalternative, most existing studies have focused on single-variable models that\ndownscale one variable at a time. This approach can lead to limited contextual\nawareness, redundant computation, and lack of cross-variable interaction. Our\nstudy addresses these limitations by proposing a multi-task, multi-variable\nVision Transformer (ViT) architecture with a shared encoder and\nvariable-specific decoders (1EMD). The proposed architecture jointly predicts\nthree key climate variables: surface temperature (tas), wind speed (sfcWind),\nand 500 hPa geopotential height (zg500), directly from GCM-resolution inputs,\nemulating RCM-scale downscaling over Europe. We show that our multi-variable\napproach achieves positive cross-variable knowledge transfer and consistently\noutperforms single-variable baselines trained under identical conditions, while\nalso improving computational efficiency. These results demonstrate the\neffectiveness of multi-variable modeling for high-resolution climate\ndownscaling.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22447v1", "categories": ["cs.LG", "cs.AI", "eess.IV"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22447v1", "date": "2025-06-12", "updated": "2025-06-12"}
{"id": "2506.22871", "title": "P$^2$U: Progressive Precision Update For Efficient Model Distribution", "authors": ["Homayun Afrabandpey", "Hamed Rezazadegan Tavakoli"], "summary": "Efficient model distribution is becoming increasingly critical in\nbandwidth-constrained environments. In this paper, we propose a simple yet\neffective approach called Progressive Precision Update (P$^2$U) to address this\nproblem. Instead of transmitting the original high-precision model, P$^2$U\ntransmits a lower-bit precision model, coupled with a model update representing\nthe difference between the original high-precision model and the transmitted\nlow precision version. With extensive experiments on various model\narchitectures, ranging from small models ($1 - 6$ million parameters) to a\nlarge model (more than $100$ million parameters) and using three different data\nsets, e.g., chest X-Ray, PASCAL-VOC, and CIFAR-100, we demonstrate that P$^2$U\nconsistently achieves better tradeoff between accuracy, bandwidth usage and\nlatency. Moreover, we show that when bandwidth or startup time is the priority,\naggressive quantization (e.g., 4-bit) can be used without severely compromising\nperformance. These results establish P$^2$U as an effective and practical\nsolution for scalable and efficient model distribution in low-resource\nsettings, including federated learning, edge computing, and IoT deployments.\nGiven that P$^2$U complements existing compression techniques and can be\nimplemented alongside any compression method, e.g., sparsification,\nquantization, pruning, etc., the potential for improvement is even greater.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22871v1", "categories": ["cs.LG", "cs.MM", "I.2.6"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22871v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23703", "title": "A New Perspective On AI Safety Through Control Theory Methodologies", "authors": ["Lars Ullrich", "Walter Zimmer", "Ross Greer", "Knut Graichen", "Alois C. Knoll", "Mohan Trivedi"], "summary": "While artificial intelligence (AI) is advancing rapidly and mastering\nincreasingly complex problems with astonishing performance, the safety\nassurance of such systems is a major concern. Particularly in the context of\nsafety-critical, real-world cyber-physical systems, AI promises to achieve a\nnew level of autonomy but is hampered by a lack of safety assurance. While\ndata-driven control takes up recent developments in AI to improve control\nsystems, control theory in general could be leveraged to improve AI safety.\nTherefore, this article outlines a new perspective on AI safety based on an\ninterdisciplinary interpretation of the underlying data-generation process and\nthe respective abstraction by AI systems in a system theory-inspired and system\nanalysis-driven manner. In this context, the new perspective, also referred to\nas data control, aims to stimulate AI engineering to take advantage of existing\nsafety analysis and assurance in an interdisciplinary way to drive the paradigm\nof data control. Following a top-down approach, a generic foundation for safety\nanalysis and assurance is outlined at an abstract level that can be refined for\nspecific AI systems and applications and is prepared for future innovation.", "comment": "Accepted to be published as part of the 2025 IEEE Open Journal of\n  Intelligent Transportation Systems (OJ-ITS)", "pdf_url": "http://arxiv.org/pdf/2506.23703v1", "categories": ["cs.AI"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.23703v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23101", "title": "From Individuals to Interactions: Benchmarking Gender Bias in Multimodal Large Language Models from the Lens of Social Relationship", "authors": ["Yue Xu", "Wenjie Wang"], "summary": "Multimodal large language models (MLLMs) have shown impressive capabilities\nacross tasks involving both visual and textual modalities. However, growing\nconcerns remain about their potential to encode and amplify gender bias,\nparticularly in socially sensitive applications. Existing benchmarks\npredominantly evaluate bias in isolated scenarios, overlooking how bias may\nemerge subtly through interpersonal interactions. We fill this gap by going\nbeyond single-entity evaluation and instead focusing on a deeper examination of\nrelational and contextual gender bias in dual-individual interactions. We\nintroduce Genres, a novel benchmark designed to evaluate gender bias in MLLMs\nthrough the lens of social relationships in generated narratives. Genres\nassesses gender bias through a dual-character profile and narrative generation\ntask that captures rich interpersonal dynamics and supports a fine-grained bias\nevaluation suite across multiple dimensions. Experiments on both open- and\nclosed-source MLLMs reveal persistent, context-sensitive gender biases that are\nnot evident in single-character settings. Our findings underscore the\nimportance of relationship-aware benchmarks for diagnosing subtle,\ninteraction-driven gender bias in MLLMs and provide actionable insights for\nfuture bias mitigation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23101v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.23101v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23024", "title": "BWLer: Barycentric Weight Layer Elucidates a Precision-Conditioning Tradeoff for PINNs", "authors": ["Jerry Liu", "Yasa Baig", "Denise Hui Jean Lee", "Rajat Vadiraj Dwaraknath", "Atri Rudra", "Chris R√©"], "summary": "Physics-informed neural networks (PINNs) offer a flexible way to solve\npartial differential equations (PDEs) with machine learning, yet they still\nfall well short of the machine-precision accuracy many scientific tasks demand.\nIn this work, we investigate whether the precision ceiling comes from the\nill-conditioning of the PDEs or from the typical multi-layer perceptron (MLP)\narchitecture. We introduce the Barycentric Weight Layer (BWLer), which models\nthe PDE solution through barycentric polynomial interpolation. A BWLer can be\nadded on top of an existing MLP (a BWLer-hat) or replace it completely\n(explicit BWLer), cleanly separating how we represent the solution from how we\ntake derivatives for the PDE loss. Using BWLer, we identify fundamental\nprecision limitations within the MLP: on a simple 1-D interpolation task, even\nMLPs with O(1e5) parameters stall around 1e-8 RMSE -- about eight orders above\nfloat64 machine precision -- before any PDE terms are added. In PDE learning,\nadding a BWLer lifts this ceiling and exposes a tradeoff between achievable\naccuracy and the conditioning of the PDE loss. For linear PDEs we fully\ncharacterize this tradeoff with an explicit error decomposition and navigate it\nduring training with spectral derivatives and preconditioning. Across five\nbenchmark PDEs, adding a BWLer on top of an MLP improves RMSE by up to 30x for\nconvection, 10x for reaction, and 1800x for wave equations while remaining\ncompatible with first-order optimizers. Replacing the MLP entirely lets an\nexplicit BWLer reach near-machine-precision on convection, reaction, and wave\nproblems (up to 10 billion times better than prior results) and match the\nperformance of standard PINNs on stiff Burgers' and irregular-geometry Poisson\nproblems. Together, these findings point to a practical path for combining the\nflexibility of PINNs with the precision of classical spectral solvers.", "comment": "Workshop for the Theory of AI for Scientific Computing @ COLT 2025\n  (Best Paper). 39 pages, 24 figures", "pdf_url": "http://arxiv.org/pdf/2506.23024v1", "categories": ["cs.LG", "cs.AI", "cs.NA", "math.NA"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.23024v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23985", "title": "Lock Prediction for Zero-Downtime Database Encryption", "authors": ["Mohamed Sami Rakha", "Adam Sorrenti", "Greg Stager", "Walid Rjaibi", "Andriy Miranskyy"], "summary": "Modern enterprise database systems face significant challenges in balancing\ndata security and performance. Ensuring robust encryption for sensitive\ninformation is critical for systems' compliance with security standards.\nAlthough holistic database encryption provides strong protection, existing\ndatabase systems often require a complete backup and restore cycle, resulting\nin prolonged downtime and increased storage usage. This makes it difficult to\nimplement online encryption techniques in high-throughput environments without\ndisrupting critical operations.\n  To address this challenge, we envision a solution that enables online\ndatabase encryption aligned with system activity, eliminating the need for\ndowntime, storage overhead, or full-database reprocessing. Central to this\nvision is the ability to predict which parts of the database will be accessed\nnext, allowing encryption to be applied online. As a step towards this\nsolution, this study proposes a predictive approach that leverages deep\nlearning models to forecast database lock sequences, using IBM Db2 as the\ndatabase system under study. In this study, we collected a specialized dataset\nfrom TPC-C benchmark workloads, leveraging lock event logs for model training\nand evaluation. We applied deep learning architectures, such as Transformer and\nLSTM, to evaluate models for various table-level and page-level lock\npredictions. We benchmark the accuracy of the trained models versus a Naive\nBaseline across different prediction horizons and timelines.\n  The study experiments demonstrate that the proposed deep learning-based\nmodels achieve up to 49% average accuracy for table-level and 66% for\npage-level predictions, outperforming a Naive Baseline. By anticipating which\ntables and pages will be locked next, the proposed approach is a step toward\nonline encryption, offering a practical path toward secure, low-overhead\ndatabase systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23985v1", "categories": ["cs.CR", "cs.DB"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.23985v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23725", "title": "PAC Bench: Do Foundation Models Understand Prerequisites for Executing Manipulation Policies?", "authors": ["Atharva Gundawar", "Som Sagar", "Ransalu Senanayake"], "summary": "Vision-Language Models (VLMs) are increasingly pivotal for generalist robot\nmanipulation, enabling tasks such as physical reasoning, policy generation, and\nfailure detection. However, their proficiency in these high-level applications\noften assumes a deep understanding of low-level physical prerequisites, a\ncapability that remains largely unverified. For robots to perform actions\nreliably, they must comprehend intrinsic object properties (e.g., material,\nweight), action affordances (e.g., graspable, stackable), and physical\nconstraints (e.g., stability, reachability, or an object's state, such as being\nclosed). Despite the widespread use of VLMs in manipulation tasks, we argue\nthat off-the-shelf models may lack this granular, physically grounded\nunderstanding, as such prerequisites are often overlooked during training.\n  To address this critical gap, we introduce PAC Bench, a comprehensive\nbenchmark designed to systematically evaluate VLMs on their understanding of\ncore Properties, Affordances, and Constraints (PAC) from a task executability\nperspective. PAC Bench features a diverse dataset with over 30,000 annotations,\ncomprising 673 real-world images (115 object classes, 15 property types, and 1\nto 3 affordances defined per class), 100 real-world humanoid-view scenarios,\nand 120 unique simulated constraint scenarios across four tasks.\n  Our evaluations reveal significant gaps in the ability of current VLMs to\ngrasp fundamental physical concepts, highlighting limitations in their\nsuitability for reliable robot manipulation and pointing to key areas for\ntargeted research. PAC Bench also serves as a standardized benchmark for\nrigorously evaluating physical reasoning in VLMs and guiding the development of\nmore robust, physically grounded models for robotic applications.\n  Project Page: https://pacbench.github.io/", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23725v1", "categories": ["cs.RO", "cs.AI"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.23725v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22476", "title": "An Interpretable Transformer-Based Foundation Model for Cross-Procedural Skill Assessment Using Raw fNIRS Signals", "authors": ["A. Subedi", "S. De", "L. Cavuoto", "S. Schwaitzberg", "M. Hackett", "J. Norfleet"], "summary": "Objective skill assessment in high-stakes procedural environments requires\nmodels that not only decode underlying cognitive and motor processes but also\ngeneralize across tasks, individuals, and experimental contexts. While prior\nwork has demonstrated the potential of functional near-infrared spectroscopy\n(fNIRS) for evaluating cognitive-motor performance, existing approaches are\noften task-specific, rely on extensive preprocessing, and lack robustness to\nnew procedures or conditions. Here, we introduce an interpretable\ntransformer-based foundation model trained on minimally processed fNIRS signals\nfor cross-procedural skill assessment. Pretrained using self-supervised\nlearning on data from laparoscopic surgical tasks and endotracheal intubation\n(ETI), the model achieves greater than 88% classification accuracy on all\ntasks, with Matthews Correlation Coefficient exceeding 0.91 on ETI. It\ngeneralizes to a novel emergency airway procedure--cricothyrotomy--using fewer\nthan 30 labeled samples and a lightweight (less than 2k parameter) adapter\nmodule, attaining an AUC greater than 87%. Interpretability is achieved via a\nnovel channel attention mechanism--developed specifically for fNIRS--that\nidentifies functionally coherent prefrontal sub-networks validated through\nablation studies. Temporal attention patterns align with task-critical phases\nand capture stress-induced changes in neural variability, offering insight into\ndynamic cognitive states.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22476v1", "categories": ["eess.SP", "cs.ET", "cs.HC", "cs.LG", "q-bio.NC", "I.2.6; J.3; H.1.2"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.22476v1", "date": "2025-06-21", "updated": "2025-06-21"}
{"id": "2506.23845", "title": "Use Sparse Autoencoders to Discover Unknown Concepts, Not to Act on Known Concepts", "authors": ["Kenny Peng", "Rajiv Movva", "Jon Kleinberg", "Emma Pierson", "Nikhil Garg"], "summary": "While sparse autoencoders (SAEs) have generated significant excitement, a\nseries of negative results have added to skepticism about their usefulness.\nHere, we establish a conceptual distinction that reconciles competing\nnarratives surrounding SAEs. We argue that while SAEs may be less effective for\nacting on known concepts, SAEs are powerful tools for discovering unknown\nconcepts. This distinction cleanly separates existing negative and positive\nresults, and suggests several classes of SAE applications. Specifically, we\noutline use cases for SAEs in (i) ML interpretability, explainability,\nfairness, auditing, and safety, and (ii) social and health sciences.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23845v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CY"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.23845v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22756", "title": "RoboPearls: Editable Video Simulation for Robot Manipulation", "authors": ["Tao Tang", "Likui Zhang", "Youpeng Wen", "Kaidong Zhang", "Jia-Wang Bian", "xia zhou", "Tianyi Yan", "Kun Zhan", "Peng Jia", "Hefeng Wu", "Liang Lin", "Xiaodan Liang"], "summary": "The development of generalist robot manipulation policies has seen\nsignificant progress, driven by large-scale demonstration data across diverse\nenvironments. However, the high cost and inefficiency of collecting real-world\ndemonstrations hinder the scalability of data acquisition. While existing\nsimulation platforms enable controlled environments for robotic learning, the\nchallenge of bridging the sim-to-real gap remains. To address these challenges,\nwe propose RoboPearls, an editable video simulation framework for robotic\nmanipulation. Built on 3D Gaussian Splatting (3DGS), RoboPearls enables the\nconstruction of photo-realistic, view-consistent simulations from demonstration\nvideos, and supports a wide range of simulation operators, including various\nobject manipulations, powered by advanced modules like Incremental Semantic\nDistillation (ISD) and 3D regularized NNFM Loss (3D-NNFM). Moreover, by\nincorporating large language models (LLMs), RoboPearls automates the simulation\nproduction process in a user-friendly manner through flexible command\ninterpretation and execution. Furthermore, RoboPearls employs a vision-language\nmodel (VLM) to analyze robotic learning issues to close the simulation loop for\nperformance enhancement. To demonstrate the effectiveness of RoboPearls, we\nconduct extensive experiments on multiple datasets and scenes, including\nRLBench, COLOSSEUM, Ego4D, Open X-Embodiment, and a real-world robot, which\ndemonstrate our satisfactory simulation performance.", "comment": "ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2506.22756v1", "categories": ["cs.CV", "cs.RO"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22756v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.22708", "title": "FairMarket-RL: LLM-Guided Fairness Shaping for Multi-Agent Reinforcement Learning in Peer-to-Peer Markets", "authors": ["Shrenik Jadhav", "Birva Sevak", "Srijita Das", "Akhtar Hussain", "Wencong Su", "Van-Hai Bui"], "summary": "Peer-to-peer (P2P) trading is increasingly recognized as a key mechanism for\ndecentralized market regulation, yet existing approaches often lack robust\nframeworks to ensure fairness. This paper presents FairMarket-RL, a novel\nhybrid framework that combines Large Language Models (LLMs) with Reinforcement\nLearning (RL) to enable fairness-aware trading agents. In a simulated P2P\nmicrogrid with multiple sellers and buyers, the LLM acts as a real-time\nfairness critic, evaluating each trading episode using two metrics:\nFairness-To-Buyer (FTB) and Fairness-Between-Sellers (FBS). These fairness\nscores are integrated into agent rewards through scheduled\n{\\lambda}-coefficients, forming an adaptive LLM-guided reward shaping loop that\nreplaces brittle, rule-based fairness constraints. Agents are trained using\nIndependent Proximal Policy Optimization (IPPO) and achieve equitable outcomes,\nfulfilling over 90% of buyer demand, maintaining fair seller margins, and\nconsistently reaching FTB and FBS scores above 0.80. The training process\ndemonstrates that fairness feedback improves convergence, reduces buyer\nshortfalls, and narrows profit disparities between sellers. With its\nlanguage-based critic, the framework scales naturally, and its extension to a\nlarge power distribution system with household prosumers illustrates its\npractical applicability. FairMarket-RL thus offers a scalable, equity-driven\nsolution for autonomous trading in decentralized energy systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22708v1", "categories": ["cs.LG", "cs.SY", "econ.GN", "eess.SY", "q-fin.EC"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22708v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23455", "title": "General Signal Model and Capacity Limit for Rydberg Quantum Information System", "authors": ["Jieao Zhu", "Linglong Dai"], "summary": "Rydberg atomic receivers represent a transformative approach to achieving\nhigh-sensitivity, broadband, and miniaturized radio frequency (RF) reception.\nHowever, existing static signal models for Rydberg atomic receivers rely on the\nsteady-state assumption of atomic quantum states, which cannot fully describe\nthe signal reception process of dynamic signals. To fill in this gap, in this\npaper, we present a general model to compute the dynamic signal response of\nRydberg atomic receivers in closed form. Specifically, by applying small-signal\nperturbation techniques to the quantum master equation, we derive closed-form\nLaplace domain transfer functions that characterize the receiver's dynamic\nresponses to time-varying signal fields. To gain more insights into the\nquantum-based RF-photocurrent conversion process, we further introduce the\nconcept of quantum transconductance that describes the quantum system as an\nequivalent classical system. By applying quantum transconductance, we quantify\nthe influence of in-band blackbody radiation (BBR) noise on the atomic receiver\nsensitivity. Extensive simulations for Rydberg atomic receivers validate the\nproposed signal model, and demonstrate the possibility of quantum receivers to\noutperform classical electronic receivers through the improvement of quantum\ntransconductance.", "comment": "Submitted to TWC. In this paper, we compute the dynamic response of\n  Rydberg atomic receivers by solving the small-signal perturbation solution to\n  quantum master equation. Transfer functions of this quantum receiver is\n  derived, with the instantaneous bandwidths problem and the in-band blackbody\n  radiation noise computed theoretically for the first time", "pdf_url": "http://arxiv.org/pdf/2506.23455v1", "categories": ["eess.SP", "quant-ph"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.23455v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22456", "title": "WISVA: Generative AI for 5G Network Optimization in Smart Warehouses", "authors": ["Rahul Gulia", "Amlan Ganguly", "Andres Kwasinski", "Michael E. Kuhl", "Ehsan Rashedi", "Clark Hochgraf"], "summary": "The next decade will usher in a profound transformation of wireless\ncommunication, driven by the ever-increasing demand for data-intensive\napplications and the rapid adoption of emerging technologies. To fully unlock\nthe potential of 5G and beyond, substantial advancements are required in signal\nprocessing techniques, innovative network architectures, and efficient spectrum\nutilization strategies. These advancements facilitate seamless integration of\nemerging technologies, driving industrial digital transformation and\nconnectivity. This paper introduces a novel Variational Autoencoder (VAE)-based\nframework, Wireless Infrastructure for Smart Warehouses using VAE (WISVA),\ndesigned for accurate indoor radio propagation modeling in automated Industry\n4.0 environments such as warehouses and factory floors operating within 5G\nwireless bands. The research delves into the meticulous creation of training\ndata tensors, capturing complex electromagnetic (EM) wave behaviors influenced\nby diverse obstacles, and outlines the architecture and training methodology of\nthe proposed VAE model. The model's robustness and adaptability are showcased\nthrough its ability to predict signal-to-interference-plus-noise ratio (SINR)\nheatmaps across various scenarios, including denoising tasks, validation\ndatasets, extrapolation to unseen configurations, and previously unencountered\nwarehouse layouts. Compelling reconstruction error heatmaps are presented,\nhighlighting the superior accuracy of WISVA compared to traditional autoencoder\nmodels. The paper also analyzes the model's performance in handling complex\nsmart warehouse environments, demonstrating its potential as a key enabler for\noptimizing wireless infrastructure in Industry 4.0.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22456v1", "categories": ["eess.SP", "eess.IV"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.22456v1", "date": "2025-06-16", "updated": "2025-06-16"}
{"id": "2506.22895", "title": "Interpretable Time Series Autoregression for Periodicity Quantification", "authors": ["Xinyu Chen", "Vassilis Digalakis Jr", "Lijun Ding", "Dingyi Zhuang", "Jinhua Zhao"], "summary": "Time series autoregression is a classical statistical model for capturing\nauto-correlations and identifying temporal patterns such as periodicity and\nseasonality. In this work, we propose a novel sparse autoregression framework\nfrom an interpretable machine learning perspective and the model\ninterpretability for periodicity quantification is reinforced by $\\ell_0$-norm\ninduced sparsity constraints. On the time-varying time series data, we\nreformulate the sparse autoregression and convert the involved optimization\nproblem into a mixed-integer optimization (MIO). To accelerate it, we develop a\nsubspace pursuit based decision variable pruning (DVP) strategy to reduce the\nsearch space. On the multidimensional time series that involves complicated\nspatial and temporal dimensions, we propose a spatially- and time-varying\nsparse autoregression model and resolve the corresponding MIO problem by\ndeveloping a two-stage optimization scheme. In particular, the proposed scheme\nmakes the model scalable to large problems even with millions of decision\nvariables. Empirically, we conduct extensive experiments to evaluate the\nproposed models on real-world time series data. First, we demonstrate that the\nMIO solver can be drastically accelerated through the DVP strategy, while\nmaintaining the same solution quality as a full MIO solver. Applying the\ntime-varying sparse autoregression model to ridesharing trip data, we uncover\nboth daily and weekly periodicities and reveal long-term changes in regularity\nof human mobility. Second, we demonstrate the spatial patterns of yearly\nseasonality in climate variable time series such as temperature and\nprecipitation across the past four decades, and our model allows to discover\ndynamic climate patterns and identify climate phenomena such as El Nino in sea\nsurface temperature.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22895v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22895v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23706", "title": "Attestable Audits: Verifiable AI Safety Benchmarks Using Trusted Execution Environments", "authors": ["Christoph Schnabl", "Daniel Hugenroth", "Bill Marino", "Alastair R. Beresford"], "summary": "Benchmarks are important measures to evaluate safety and compliance of AI\nmodels at scale. However, they typically do not offer verifiable results and\nlack confidentiality for model IP and benchmark datasets. We propose Attestable\nAudits, which run inside Trusted Execution Environments and enable users to\nverify interaction with a compliant AI model. Our work protects sensitive data\neven when model provider and auditor do not trust each other. This addresses\nverification challenges raised in recent AI governance frameworks. We build a\nprototype demonstrating feasibility on typical audit benchmarks against\nLlama-3.1.", "comment": "ICML 2024 Workshop TAIG", "pdf_url": "http://arxiv.org/pdf/2506.23706v1", "categories": ["cs.AI", "cs.CL", "cs.CR"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.23706v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23111", "title": "FairI Tales: Evaluation of Fairness in Indian Contexts with a Focus on Bias and Stereotypes", "authors": ["Janki Atul Nawale", "Mohammed Safi Ur Rahman Khan", "Janani D", "Mansi Gupta", "Danish Pruthi", "Mitesh M. Khapra"], "summary": "Existing studies on fairness are largely Western-focused, making them\ninadequate for culturally diverse countries such as India. To address this gap,\nwe introduce INDIC-BIAS, a comprehensive India-centric benchmark designed to\nevaluate fairness of LLMs across 85 identity groups encompassing diverse\ncastes, religions, regions, and tribes. We first consult domain experts to\ncurate over 1,800 socio-cultural topics spanning behaviors and situations,\nwhere biases and stereotypes are likely to emerge. Grounded in these topics, we\ngenerate and manually validate 20,000 real-world scenario templates to probe\nLLMs for fairness. We structure these templates into three evaluation tasks:\nplausibility, judgment, and generation. Our evaluation of 14 popular LLMs on\nthese tasks reveals strong negative biases against marginalized identities,\nwith models frequently reinforcing common stereotypes. Additionally, we find\nthat models struggle to mitigate bias even when explicitly asked to rationalize\ntheir decision. Our evaluation provides evidence of both allocative and\nrepresentational harms that current LLMs could cause towards Indian identities,\ncalling for a more cautious usage in practical applications. We release\nINDIC-BIAS as an open-source benchmark to advance research on benchmarking and\nmitigating biases and stereotypes in the Indian context.", "comment": "Accepted in ACL 2025", "pdf_url": "http://arxiv.org/pdf/2506.23111v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.23111v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23062", "title": "Shifted Composition IV: Underdamped Langevin and Numerical Discretizations with Partial Acceleration", "authors": ["Jason M. Altschuler", "Sinho Chewi", "Matthew S. Zhang"], "summary": "Quantifying the convergence rate of the underdamped Langevin dynamics (ULD)\nis a classical topic, in large part due to the possibility for\ndiffusive-to-ballistic speedups -- as was recently established for the\ncontinuous-time dynamics via space-time Poincare inequalities. A central\nchallenge for analyzing ULD is that its degeneracy necessitates the development\nof new analysis approaches, e.g., the theory of hypocoercivity. In this paper,\nwe give a new coupling-based framework for analyzing ULD and its numerical\ndiscretizations. First, in the continuous-time setting, we use this framework\nto establish new parabolic Harnack inequalities for ULD. These are the first\nHarnack inequalities that decay to zero in contractive settings, thereby\nreflecting the convergence properties of ULD in addition to just its regularity\nproperties.\n  Second, we build upon these Harnack inequalities to develop a local error\nframework for analyzing discretizations of ULD in KL divergence. This extends\nour framework in part III from uniformly elliptic diffusions to degenerate\ndiffusions, and shares its virtues: the framework is user-friendly, applies to\nsophisticated discretization schemes, and does not require contractivity.\nApplying this framework to the randomized midpoint discretization of ULD\nestablishes (i) the first ballistic acceleration result for log-concave\nsampling (i.e., sublinear dependence on the condition number), and (ii) the\nfirst $d^{1/3}$ iteration complexity guarantee for sampling to constant total\nvariation error in dimension $d$.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23062v1", "categories": ["math.PR", "cs.DS", "cs.NA", "math.AP", "math.NA", "math.ST", "stat.TH"], "cate": "math.PR", "url": "http://arxiv.org/abs/2506.23062v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.24033", "title": "Poisoning Attacks to Local Differential Privacy for Ranking Estimation", "authors": ["Pei Zhan", "Peng Tang", "Yangzhuo Li", "Puwen Wei", "Shanqing Guo"], "summary": "Local differential privacy (LDP) involves users perturbing their inputs to\nprovide plausible deniability of their data. However, this also makes LDP\nvulnerable to poisoning attacks. In this paper, we first introduce novel\npoisoning attacks for ranking estimation. These attacks are intricate, as fake\nattackers do not merely adjust the frequency of target items. Instead, they\nleverage a limited number of fake users to precisely modify frequencies,\neffectively altering item rankings to maximize gains. To tackle this challenge,\nwe introduce the concepts of attack cost and optimal attack item (set), and\npropose corresponding strategies for kRR, OUE, and OLH protocols. For kRR, we\niteratively select optimal attack items and allocate suitable fake users. For\nOUE, we iteratively determine optimal attack item sets and consider the\nincremental changes in item frequencies across different sets. Regarding OLH,\nwe develop a harmonic cost function based on the pre-image of a hash to select\nthat supporting a larger number of effective attack items. Lastly, we present\nan attack strategy based on confidence levels to quantify the probability of a\nsuccessful attack and the number of attack iterations more precisely. We\ndemonstrate the effectiveness of our attacks through theoretical and empirical\nevidence, highlighting the necessity for defenses against these attacks. The\nsource code and data have been made available at\nhttps://github.com/LDP-user/LDP-Ranking.git.", "comment": "This paper, consisting of 24 pages with 31 figures and 1 table, has\n  been accepted by ACM CCS 2025", "pdf_url": "http://arxiv.org/pdf/2506.24033v1", "categories": ["cs.CR"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.24033v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23739", "title": "Validation of AI-Based 3D Human Pose Estimation in a Cyber-Physical Environment", "authors": ["Lisa Marie Otto", "Michael Kaiser", "Daniel Seebacher", "Steffen M√ºller"], "summary": "Ensuring safe and realistic interactions between automated driving systems\nand vulnerable road users (VRUs) in urban environments requires advanced\ntesting methodologies. This paper presents a test environment that combines a\nVehiclein-the-Loop (ViL) test bench with a motion laboratory, demonstrating the\nfeasibility of cyber-physical (CP) testing of vehicle-pedestrian and\nvehicle-cyclist interactions. Building upon previous work focused on pedestrian\nlocalization, we further validate a human pose estimation (HPE) approach\nthrough a comparative analysis of real-world (RW) and virtual representations\nof VRUs. The study examines the perception of full-body motion using a\ncommercial monocular camera-based 3Dskeletal detection AI. The virtual scene is\ngenerated in Unreal Engine 5, where VRUs are animated in real time and\nprojected onto a screen to stimulate the camera. The proposed stimulation\ntechnique ensures the correct perspective, enabling realistic vehicle\nperception. To assess the accuracy and consistency of HPE across RW and CP\ndomains, we analyze the reliability of detections as well as variations in\nmovement trajectories and joint estimation stability. The validation includes\ndynamic test scenarios where human avatars, both walking and cycling, are\nmonitored under controlled conditions. Our results show a strong alignment in\nHPE between RW and CP test conditions for stable motion patterns, while notable\ninaccuracies persist under dynamic movements and occlusions, particularly for\ncomplex cyclist postures. These findings contribute to refining CP testing\napproaches for evaluating next-generation AI-based vehicle perception and to\nenhancing interaction models of automated vehicles and VRUs in CP environments.", "comment": "6 pages, 5 figures, Preprint for 2025 IEEE IAVVC (International\n  Automated Vehicle Validation Conference)", "pdf_url": "http://arxiv.org/pdf/2506.23739v1", "categories": ["cs.RO", "cs.CE", "cs.HC"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.23739v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22604", "title": "Bootstrapping Human-Like Planning via LLMs", "authors": ["David Porfirio", "Vincent Hsiao", "Morgan Fine-Morris", "Leslie Smith", "Laura M. Hiatt"], "summary": "Robot end users increasingly require accessible means of specifying tasks for\nrobots to perform. Two common end-user programming paradigms include\ndrag-and-drop interfaces and natural language programming. Although natural\nlanguage interfaces harness an intuitive form of human communication,\ndrag-and-drop interfaces enable users to meticulously and precisely dictate the\nkey actions of the robot's task. In this paper, we investigate the degree to\nwhich both approaches can be combined. Specifically, we construct a large\nlanguage model (LLM)-based pipeline that accepts natural language as input and\nproduces human-like action sequences as output, specified at a level of\ngranularity that a human would produce. We then compare these generated action\nsequences to another dataset of hand-specified action sequences. Although our\nresults reveal that larger models tend to outperform smaller ones in the\nproduction of human-like action sequences, smaller models nonetheless achieve\nsatisfactory performance.", "comment": "Accepted by the 2025 34th IEEE International Conference on Robot and\n  Human Interactive Communication (RO-MAN)", "pdf_url": "http://arxiv.org/pdf/2506.22604v1", "categories": ["cs.AI", "cs.HC", "cs.RO"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.22604v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.23866", "title": "Exploring Privacy and Security as Drivers for Environmental Sustainability in Cloud-Based Office Solutions", "authors": ["Jason Kayembe", "Iness Ben Guirat", "Jan Tobias M√ºhlberg"], "summary": "In this paper, we explore the intersection of privacy, security, and\nenvironmental sustainability in cloud-based office solutions, focusing on\nquantifying user- and network-side energy use and associated carbon emissions.\nWe hypothesise that privacy-focused services are typically more\nenergy-efficient than those funded through data collection and advertising. To\nevaluate this, we propose a framework that systematically measures\nenvironmental costs based on energy usage and network data traffic during\nwell-defined, automated usage scenarios. To test our hypothesis, we first\nanalyse how underlying architectures and business models, such as monetisation\nthrough personalised advertising, contribute to the environmental footprint of\nthese services. We then explore existing methodologies and tools for software\nenvironmental impact assessment. We apply our framework to three mainstream\nemail services selected to reflect different privacy policies, from\nad-supported tracking-intensive models to privacy-focused designs: Microsoft\nOutlook, Google Mail (Gmail), and Proton Mail. We extend this comparison to a\nself-hosted email solution, evaluated with and without end-to-end encryption.\nWe show that the self-hosted solution, even with 14% of device energy and 15%\nof emissions overheads from PGP encryption, remains the most energy-efficient,\nsaving up to 33% of emissions per session compared to Gmail. Among commercial\nproviders, Proton Mail is the most efficient, saving up to 0.1 gCO2 e per\nsession compared to Outlook, whose emissions can be further reduced by 2%\nthrough ad-blocking.", "comment": "Post-proceedings paper persented at LOCO '24: 1st International\n  Workshop on Low Carbon Computing, 2024-12-03, in Glasgow, UK", "pdf_url": "http://arxiv.org/pdf/2506.23866v1", "categories": ["cs.CR", "cs.CY", "cs.SE"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.23866v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22762", "title": "VSRM: A Robust Mamba-Based Framework for Video Super-Resolution", "authors": ["Dinh Phu Tran", "Dao Duy Hung", "Daeyoung Kim"], "summary": "Video super-resolution remains a major challenge in low-level vision tasks.\nTo date, CNN- and Transformer-based methods have delivered impressive results.\nHowever, CNNs are limited by local receptive fields, while Transformers\nstruggle with quadratic complexity, posing challenges for processing long\nsequences in VSR. Recently, Mamba has drawn attention for its long-sequence\nmodeling, linear complexity, and large receptive fields. In this work, we\npropose VSRM, a novel \\textbf{V}ideo \\textbf{S}uper-\\textbf{R}esolution\nframework that leverages the power of \\textbf{M}amba. VSRM introduces\nSpatial-to-Temporal Mamba and Temporal-to-Spatial Mamba blocks to extract\nlong-range spatio-temporal features and enhance receptive fields efficiently.\nTo better align adjacent frames, we propose Deformable Cross-Mamba Alignment\nmodule. This module utilizes a deformable cross-mamba mechanism to make the\ncompensation stage more dynamic and flexible, preventing feature distortions.\nFinally, we minimize the frequency domain gaps between reconstructed and\nground-truth frames by proposing a simple yet effective Frequency\nCharbonnier-like loss that better preserves high-frequency content and enhances\nvisual quality. Through extensive experiments, VSRM achieves state-of-the-art\nresults on diverse benchmarks, establishing itself as a solid foundation for\nfuture research.", "comment": "Accepted by ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2506.22762v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22762v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.22811", "title": "Terahertz source-on-a-chip with decade-long stability using layered superconductor elliptical microcavities", "authors": ["Mingqi Zhang", "Shungo Nakagawa", "Yuki Enomoto", "Yoshihiko Kuzumi", "Ryuta Kikuchi", "Yuki Yamauchi", "Toshiaki Hattori", "Richard A. Klemm", "Kazuo Kadowaki", "Takanari Kashiwagi", "Kaveh Delfanazari"], "summary": "Coherent, continuous-wave, and electrically tunable chip-scale terahertz\n(THz) sources are critical for emerging applications in sensing, imaging,\nspectroscopy, communication, space and quantum technologies. Here, we\ndemonstrate a robust source-on-a-chip THz emitter based on a layered\nhigh-temperature superconductor, engineered with an elliptical microcavity and\ncapable of sustained coherent emission over an unprecedented operational\nlifetime exceeding 11 years. This compact THz source operates up to 60 K, with\nTc= 90 K, delivering stable radiation in the 0.7-0.8 THz range, with on-chip\nelectrical tunability from 100 GHz to 1 THz. Coherence arises from the\nphase-locked oscillation of intrinsic Josephson junction arrays, resonantly\ncoupled to transverse electromagnetic modes within the cavity, analogous to a\nlaser cavity, yielding collective macroscopic oscillations. THz emission\nremains detectable across a 0.5 m free-space open-air link at room temperature.\nWe analyse the cavity-mode structure and extract THz photon generation rates up\nto 503 photons fs-1 in cryogenic conditions and 50-260 photons ps-1\nover-the-air. These results establish long-term coherent THz emission from\nsuperconductors and chart a viable path toward scalable, tunable, solid-state\ncoherent THz laser-on-a-chip platforms, especially for future classical and\nquantum systems.", "comment": "24 pages, 18 Figures", "pdf_url": "http://arxiv.org/pdf/2506.22811v1", "categories": ["quant-ph", "cond-mat.supr-con", "cs.SY", "eess.SY", "physics.app-ph", "physics.optics"], "cate": "quant-ph", "url": "http://arxiv.org/abs/2506.22811v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23472", "title": "Automatic Phase Calibration for High-resolution mmWave Sensing via Ambient Radio Anchors", "authors": ["Ruixu Geng", "Yadong Li", "Dongheng Zhang", "Pengcheng Huang", "Binquan Wang", "Binbin Zhang", "Zhi Lu", "Yang Hu", "Yan Chen"], "summary": "Millimeter-wave (mmWave) radar systems with large array have pushed radar\nsensing into a new era, thanks to their high angular resolution. However, our\nlong-term experiments indicate that array elements exhibit phase drift over\ntime and require periodic phase calibration to maintain high-resolution,\ncreating an obstacle for practical high-resolution mmWave sensing.\nUnfortunately, existing calibration methods are inadequate for periodic\nrecalibration, either because they rely on artificial references or fail to\nprovide sufficient precision. To address this challenge, we introduce\nAutoCalib, the first framework designed to automatically and accurately\ncalibrate high-resolution mmWave radars by identifying Ambient Radio Anchors\n(ARAs)-naturally existing objects in ambient environments that offer stable\nphase references. AutoCalib achieves calibration by first generating spatial\nspectrum templates based on theoretical electromagnetic characteristics. It\nthen employs a pattern-matching and scoring mechanism to accurately detect\nthese anchors and select the optimal one for calibration. Extensive experiments\nacross 11 environments demonstrate that AutoCalib capable of identifying ARAs\nthat existing methods miss due to their focus on strong reflectors. AutoCalib's\ncalibration performance approaches corner reflectors (74% phase error\nreduction) while outperforming existing methods by 83%. Beyond radar\ncalibration, AutoCalib effectively supports other phase-dependent applications\nlike handheld imaging, delivering 96% of corner reflector calibration\nperformance without artificial references.", "comment": "13 pages, 21 figures", "pdf_url": "http://arxiv.org/pdf/2506.23472v1", "categories": ["eess.SP"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.23472v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22511", "title": "Lightning the Night with Generative Artificial Intelligence", "authors": ["Tingting Zhou", "Feng Zhang", "Haoyang Fu", "Baoxiang Pan", "Renhe Zhang", "Feng Lu", "Zhixin Yang"], "summary": "The visible light reflectance data from geostationary satellites is crucial\nfor meteorological observations and plays an important role in weather\nmonitoring and forecasting. However, due to the lack of visible light at night,\nit is impossible to conduct continuous all-day weather observations using\nvisible light reflectance data. This study pioneers the use of generative\ndiffusion models to address this limitation. Based on the multi-band thermal\ninfrared brightness temperature data from the Advanced Geostationary Radiation\nImager (AGRI) onboard the Fengyun-4B (FY4B) geostationary satellite, we\ndeveloped a high-precision visible light reflectance retrieval model, called\nReflectance Diffusion (RefDiff), which enables 0.47~\\mu\\mathrm{m},\n0.65~\\mu\\mathrm{m}, and 0.825~\\mu\\mathrm{m} bands visible light reflectance\nretrieval at night. Compared to the classical models, RefDiff not only\nsignificantly improves accuracy through ensemble averaging but also provides\nuncertainty estimation. Specifically, the SSIM index of RefDiff can reach 0.90,\nwith particularly significant improvements in areas with complex cloud\nstructures and thick clouds. The model's nighttime retrieval capability was\nvalidated using VIIRS nighttime product, demonstrating comparable performance\nto its daytime counterpart. In summary, this research has made substantial\nprogress in the ability to retrieve visible light reflectance at night, with\nthe potential to expand the application of nighttime visible light data.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22511v1", "categories": ["cs.CV", "cs.AI", "eess.IV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22511v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.22901", "title": "Missing-Modality-Aware Graph Neural Network for Cancer Classification", "authors": ["Sina Tabakhi", "Haiping Lu"], "summary": "A key challenge in learning from multimodal biological data is missing\nmodalities, where all data from some modalities are missing for some patients.\nCurrent fusion methods address this by excluding patients with missing\nmodalities, imputing missing modalities, or making predictions directly with\npartial modalities. However, they often struggle with diverse missing-modality\npatterns and the exponential growth of the number of such patterns as the\nnumber of modalities increases. To address these limitations, we propose MAGNET\n(Missing-modality-Aware Graph neural NETwork) for direct prediction with\npartial modalities, which introduces a patient-modality multi-head attention\nmechanism to fuse lower-dimensional modality embeddings based on their\nimportance and missingness. MAGNET's complexity increases linearly with the\nnumber of modalities while adapting to missing-pattern variability. To generate\npredictions, MAGNET further constructs a patient graph with fused multimodal\nembeddings as node features and the connectivity determined by the modality\nmissingness, followed by a conventional graph neural network. Experiments on\nthree public multiomics datasets for cancer classification, with real-world\ninstead of artificial missingness, show that MAGNET outperforms the\nstate-of-the-art fusion methods. The data and code are available at\nhttps://github.com/SinaTabakhi/MAGNET.", "comment": "15 pages, 7 figures", "pdf_url": "http://arxiv.org/pdf/2506.22901v1", "categories": ["cs.LG", "cs.AI", "q-bio.BM", "q-bio.GN"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22901v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23773", "title": "BayesL: Towards a Logical Framework for Bayesian Networks", "authors": ["Stefano M. Nicoletti", "Mari√´lle Stoelinga"], "summary": "We introduce BayesL, a novel logical framework for specifying, querying, and\nverifying the behaviour of Bayesian networks (BNs). BayesL (pronounced \"Basil\")\nis a structured language that allows for the creation of queries over BNs. It\nfacilitates versatile reasoning concerning causal and evidence-based\nrelationships, and permits comprehensive what-if scenario evaluations without\nthe need for manual modifications to the model.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23773v1", "categories": ["cs.AI", "cs.LO"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.23773v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23122", "title": "Decoding Memes: Benchmarking Narrative Role Classification across Multilingual and Multimodal Models", "authors": ["Shivam Sharma", "Tanmoy Chakraborty"], "summary": "This work investigates the challenging task of identifying narrative roles -\nHero, Villain, Victim, and Other - in Internet memes, across three diverse test\nsets spanning English and code-mixed (English-Hindi) languages. Building on an\nannotated dataset originally skewed toward the 'Other' class, we explore a more\nbalanced and linguistically diverse extension, originally introduced as part of\nthe CLEF 2024 shared task. Comprehensive lexical and structural analyses\nhighlight the nuanced, culture-specific, and context-rich language used in real\nmemes, in contrast to synthetically curated hateful content, which exhibits\nexplicit and repetitive lexical markers. To benchmark the role detection task,\nwe evaluate a wide spectrum of models, including fine-tuned multilingual\ntransformers, sentiment and abuse-aware classifiers, instruction-tuned LLMs,\nand multimodal vision-language models. Performance is assessed under zero-shot\nsettings using precision, recall, and F1 metrics. While larger models like\nDeBERTa-v3 and Qwen2.5-VL demonstrate notable gains, results reveal consistent\nchallenges in reliably identifying the 'Victim' class and generalising across\ncultural and code-mixed content. We also explore prompt design strategies to\nguide multimodal models and find that hybrid prompts incorporating structured\ninstructions and role definitions offer marginal yet consistent improvements.\nOur findings underscore the importance of cultural grounding, prompt\nengineering, and multimodal reasoning in modelling subtle narrative framings in\nvisual-textual content.", "comment": "This work has been submitted to the IEEE for possible publication", "pdf_url": "http://arxiv.org/pdf/2506.23122v1", "categories": ["cs.CL", "cs.CY"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.23122v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23550", "title": "Seeding neural network quantum states with tensor network states", "authors": ["Ryui Kaneko", "Shimpei Goto"], "summary": "We find an efficient approach to approximately convert matrix product states\n(MPSs) into restricted Boltzmann machine wave functions consisting of a\nmultinomial hidden unit through a canonical polyadic (CP) decomposition of the\nMPSs. This method allows us to generate well-behaved initial neural network\nquantum states for quantum many-body ground-state calculations in polynomial\ntime of the number of variational parameters and systematically shorten the\ndistance between the initial states and the ground states with increasing the\nrank of the CP decomposition. We demonstrate the efficiency of our method by\ntaking the transverse-field Ising model as an example and discuss possible\napplications of our method to more general quantum many-body systems in which\nthe ground-state wave functions possess complex nodal structures.", "comment": "13 pages, 13 figures", "pdf_url": "http://arxiv.org/pdf/2506.23550v1", "categories": ["cond-mat.str-el", "cs.LG", "cs.NA", "math.NA", "quant-ph"], "cate": "cond-mat.str-el", "url": "http://arxiv.org/abs/2506.23550v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.24056", "title": "Logit-Gap Steering: Efficient Short-Suffix Jailbreaks for Aligned Large Language Models", "authors": ["Tung-Ling Li", "Hongliang Liu"], "summary": "We introduce logit-gap steering, a fast jailbreak framework that casts the\nrefusal-affirmation gap of RLHF-aligned language models as a single pass over\nthe vocabulary. A forward-computable score blends gap reduction with\nlightweight proxies for KL penalty and reward shift, allowing a \"sort-sum-stop\"\nsweep to complete in under a second and return a short suffix--two orders of\nmagnitude fewer model calls than beam or gradient attacks. The same suffix\ngeneralises to unseen prompts and scales from 0.5 B to 70 B checkpoints,\nlifting one-shot attack success from baseline levels to 80-100% while\npreserving topical coherence. Beyond efficiency, these suffixes expose\nsentence-boundary reward cliffs and other alignment artefacts, offering a\nlightweight probe into how safety tuning reshapes internal representations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.24056v1", "categories": ["cs.CR", "cs.CL", "cs.LG"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.24056v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23768", "title": "Motion Tracking with Muscles: Predictive Control of a Parametric Musculoskeletal Canine Model", "authors": ["Vittorio La Barbera", "Steven Bohez", "Leonard Hasenclever", "Yuval Tassa", "John R. Hutchinson"], "summary": "We introduce a novel musculoskeletal model of a dog, procedurally generated\nfrom accurate 3D muscle meshes. Accompanying this model is a motion\ncapture-based locomotion task compatible with a variety of control algorithms,\nas well as an improved muscle dynamics model designed to enhance convergence in\ndifferentiable control frameworks. We validate our approach by comparing\nsimulated muscle activation patterns with experimentally obtained\nelectromyography (EMG) data from previous canine locomotion studies. This work\naims to bridge gaps between biomechanics, robotics, and computational\nneuroscience, offering a robust platform for researchers investigating muscle\nactuation and neuromuscular control.We plan to release the full model along\nwith the retargeted motion capture clips to facilitate further research and\ndevelopment.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23768v1", "categories": ["cs.RO"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.23768v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22803", "title": "Intervening in Black Box: Concept Bottleneck Model for Enhancing Human Neural Network Mutual Understanding", "authors": ["Nuoye Xiong", "Anqi Dong", "Ning Wang", "Cong Hua", "Guangming Zhu", "Mei Lin", "Peiyi Shen", "Liang Zhang"], "summary": "Recent advances in deep learning have led to increasingly complex models with\ndeeper layers and more parameters, reducing interpretability and making their\ndecisions harder to understand. While many methods explain black-box reasoning,\nmost lack effective interventions or only operate at sample-level without\nmodifying the model itself. To address this, we propose the Concept Bottleneck\nModel for Enhancing Human-Neural Network Mutual Understanding (CBM-HNMU).\nCBM-HNMU leverages the Concept Bottleneck Model (CBM) as an interpretable\nframework to approximate black-box reasoning and communicate conceptual\nunderstanding. Detrimental concepts are automatically identified and refined\n(removed/replaced) based on global gradient contributions. The modified CBM\nthen distills corrected knowledge back into the black-box model, enhancing both\ninterpretability and accuracy. We evaluate CBM-HNMU on various CNN and\ntransformer-based models across Flower-102, CIFAR-10, CIFAR-100, FGVC-Aircraft,\nand CUB-200, achieving a maximum accuracy improvement of 2.64% and a maximum\nincrease in average accuracy across 1.03%. Source code is available at:\nhttps://github.com/XiGuaBo/CBM-HNMU.", "comment": "Accepted by ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2506.22803v1", "categories": ["cs.CV", "cs.HC", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22803v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23949", "title": "AI Risk-Management Standards Profile for General-Purpose AI (GPAI) and Foundation Models", "authors": ["Anthony M. Barrett", "Jessica Newman", "Brandie Nonnecke", "Nada Madkour", "Dan Hendrycks", "Evan R. Murphy", "Krystal Jackson", "Deepika Raman"], "summary": "Increasingly multi-purpose AI models, such as cutting-edge large language\nmodels or other 'general-purpose AI' (GPAI) models, 'foundation models,'\ngenerative AI models, and 'frontier models' (typically all referred to\nhereafter with the umbrella term 'GPAI/foundation models' except where greater\nspecificity is needed), can provide many beneficial capabilities but also risks\nof adverse events with profound consequences. This document provides\nrisk-management practices or controls for identifying, analyzing, and\nmitigating risks of GPAI/foundation models. We intend this document primarily\nfor developers of large-scale, state-of-the-art GPAI/foundation models; others\nthat can benefit from this guidance include downstream developers of end-use\napplications that build on a GPAI/foundation model. This document facilitates\nconformity with or use of leading AI risk management-related standards,\nadapting and building on the generic voluntary guidance in the NIST AI Risk\nManagement Framework and ISO/IEC 23894, with a focus on the unique issues faced\nby developers of GPAI/foundation models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23949v1", "categories": ["cs.AI", "cs.CR", "cs.CY"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.23949v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22783", "title": "PhonemeFake: Redefining Deepfake Realism with Language-Driven Segmental Manipulation and Adaptive Bilevel Detection", "authors": ["Oguzhan Baser", "Ahmet Ege Tanriverdi", "Sriram Vishwanath", "Sandeep P. Chinchali"], "summary": "Deepfake (DF) attacks pose a growing threat as generative models become\nincreasingly advanced. However, our study reveals that existing DF datasets\nfail to deceive human perception, unlike real DF attacks that influence public\ndiscourse. It highlights the need for more realistic DF attack vectors. We\nintroduce PhonemeFake (PF), a DF attack that manipulates critical speech\nsegments using language reasoning, significantly reducing human perception by\nup to 42% and benchmark accuracies by up to 94%. We release an easy-to-use PF\ndataset on HuggingFace and open-source bilevel DF segment detection model that\nadaptively prioritizes compute on manipulated regions. Our extensive\nexperiments across three known DF datasets reveal that our detection model\nreduces EER by 91% while achieving up to 90% speed-up, with minimal compute\noverhead and precise localization beyond existing models as a scalable\nsolution.", "comment": "5 pages, 3 figures, Published at Proceedings of Interspeech 2025, for\n  the dataset see https://huggingface.co/datasets/phonemefake/PhonemeFakeV2,\n  for the code see https://github.com/UTAustin-SwarmLab/ PhonemeFake", "pdf_url": "http://arxiv.org/pdf/2506.22783v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22783v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.22884", "title": "Performance Measurements in the AI-Centric Computing Continuum Systems", "authors": ["Praveen Kumar Donta", "Qiyang Zhang", "Schahram Dustdar"], "summary": "Over the Eight decades, computing paradigms have shifted from large,\ncentralized systems to compact, distributed architectures, leading to the rise\nof the Distributed Computing Continuum (DCC). In this model, multiple layers\nsuch as cloud, edge, Internet of Things (IoT), and mobile platforms work\ntogether to support a wide range of applications. Recently, the emergence of\nGenerative AI and large language models has further intensified the demand for\ncomputational resources across this continuum. Although traditional performance\nmetrics have provided a solid foundation, they need to be revisited and\nexpanded to keep pace with changing computational demands and application\nrequirements. Accurate performance measurements benefit both system designers\nand users by supporting improvements in efficiency and promoting alignment with\nsystem goals. In this context, we review commonly used metrics in DCC and IoT\nenvironments. We also discuss emerging performance dimensions that address\nevolving computing needs, such as sustainability, energy efficiency, and system\nobservability. We also outline criteria and considerations for selecting\nappropriate metrics, aiming to inspire future research and development in this\ncritical area.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22884v1", "categories": ["cs.DC", "cs.AI", "cs.ET", "cs.NI", "cs.SY", "eess.SY"], "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.22884v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23473", "title": "Cooperative Sensing in Cell-free Massive MIMO ISAC Systems: Performance Optimization and Signal Processing", "authors": ["Haotian Liu", "Zhiqing Wei", "Luyang Sun", "Ruizhong Xu", "Yixin Zhang", "Zhiyong Feng"], "summary": "Integrated sensing and communication (ISAC), as a technology enabled seamless\nconnection between communication and sensing, is regarded a core enabling\ntechnology for these applications. However, the accuracy of single-node sensing\nin ISAC system is limited, prompting the emergence of multi-node cooperative\nsensing. In multi-node cooperative sensing, the synchronization error limits\nthe sensing accuracy, which can be mitigated by the architecture of cell-free\nmassive multi-input multi-output (CF-mMIMO), since the multiple nodes are\ninterconnected via optical fibers with high synchronization accuracy. However,\nthe multi-node cooperative sensing in CF-mMIMO ISAC systems faces the following\nchallenges: 1) The joint optimization of placement and resource allocation of\ndistributed access points (APs) to improve the sensing performance in\nmulti-target detection scenario is difficult; 2) The fusion of the sensing\ninformation from distributed APs with multi-view discrepancies is difficult. To\naddress these challenges, this paper proposes a joint placement and antenna\nresource optimization scheme for distributed APs to minimize the sensing\nCramr-Rao bound for targets' parameters within the area of interest. Then, a\nsymbol-level fusion-based multi-dynamic target sensing (SL-MDTS) scheme is\nprovided, effectively fusing sensing information from multiple APs. The\nsimulation results validate the effectiveness of the joint optimization scheme\nand the superiority of the SL-MDTS scheme. Compared to state-of-the-art\ngrid-based symbol-level sensing information fusion schemes, the proposed\nSL-MDTS scheme improves the accuracy of localization and velocity estimation by\n44 % and 41.4 %, respectively.", "comment": "13 pages, 10 figures", "pdf_url": "http://arxiv.org/pdf/2506.23473v1", "categories": ["eess.SP"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.23473v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22556", "title": "Recomposed realities: animating still images via patch clustering and randomness", "authors": ["Markus Juvonen", "Samuli Siltanen"], "summary": "We present a patch-based image reconstruction and animation method that uses\nexisting image data to bring still images to life through motion. Image patches\nfrom curated datasets are grouped using k-means clustering and a new target\nimage is reconstructed by matching and randomly sampling from these clusters.\nThis approach emphasizes reinterpretation over replication, allowing the source\nand target domains to differ conceptually while sharing local structures.", "comment": "22 pages, 19 figures", "pdf_url": "http://arxiv.org/pdf/2506.22556v1", "categories": ["cs.CV", "eess.IV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22556v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22927", "title": "Towards Time Series Generation Conditioned on Unstructured Natural Language", "authors": ["Jaeyun Woo", "Jiseok Lee", "Brian Kenji Iwana"], "summary": "Generative Artificial Intelligence (AI) has rapidly become a powerful tool,\ncapable of generating various types of data, such as images and text. However,\ndespite the significant advancement of generative AI, time series generative AI\nremains underdeveloped, even though the application of time series is essential\nin finance, climate, and numerous fields. In this research, we propose a novel\nmethod of generating time series conditioned on unstructured natural language\ndescriptions. We use a diffusion model combined with a language model to\ngenerate time series from the text. Through the proposed method, we demonstrate\nthat time series generation based on natural language is possible. The proposed\nmethod can provide various applications such as custom forecasting, time series\nmanipulation, data augmentation, and transfer learning. Furthermore, we\nconstruct and propose a new public dataset for time series generation,\nconsisting of 63,010 time series-description pairs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22927v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22927v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23784", "title": "When GNNs Met a Word Equations Solver: Learning to Rank Equations (Extended Technical Report)", "authors": ["Parosh Aziz Abdulla", "Mohamed Faouzi Atig", "Julie Cailler", "Chencheng Liang", "Philipp R√ºmmer"], "summary": "Nielsen transformation is a standard approach for solving word equations: by\nrepeatedly splitting equations and applying simplification steps, equations are\nrewritten until a solution is reached. When solving a conjunction of word\nequations in this way, the performance of the solver will depend considerably\non the order in which equations are processed. In this work, the use of Graph\nNeural Networks (GNNs) for ranking word equations before and during the solving\nprocess is explored. For this, a novel graph-based representation for word\nequations is presented, preserving global information across conjuncts,\nenabling the GNN to have a holistic view during ranking. To handle the variable\nnumber of conjuncts, three approaches to adapt a multi-classification task to\nthe problem of ranking equations are proposed. The training of the GNN is done\nwith the help of minimum unsatisfiable subsets (MUSes) of word equations. The\nexperimental results show that, compared to state-of-the-art string solvers,\nthe new framework solves more problems in benchmarks where each variable\nappears at most once in each equation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23784v1", "categories": ["cs.AI", "cs.LG"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.23784v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23127", "title": "Unleashing Embodied Task Planning Ability in LLMs via Reinforcement Learning", "authors": ["Zhaoye Fei", "Li Ji", "Siyin Wang", "Junhao Shi", "Jingjing Gong", "Xipeng Qiu"], "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious tasks, yet they face significant challenges in embodied task planning\nscenarios that require continuous environmental understanding and action\ngeneration. Existing approaches generate open-loop action scripts based on\nstatic knowledge, making it difficult to learn causal relationships between\nactions and environmental feedback, particularly in partially observable\nenvironments. We introduce Embodied Planner-R1, a novel outcome-driven\nreinforcement learning framework that enables LLMs to develop interactive\ncapabilities through autonomous exploration with minimal supervision. Our\nframework incorporates three key innovations: (1) Without human annotations, we\nemploy pure reinforcement learning with group rollout, incorporating\nin-environment interaction through parallel exploration; (2) completion-driven\nsparse reward; and (3) Interactive Policy Optimization (IPO) for efficient\nlearning from grouped trajectories. Across two challenging text-based Embodied\nplanning benchmarks, Embodied Planner-R1 achieves impressive completion rates\nof 97.78% on ALFWorld and 79.92% on ScienceWorld, surpassing prior methods by a\nlarge margin, and suffers only a -3.66% drop in previously unseen environments,\nevidencing strong generalization.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23127v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.23127v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.24042", "title": "Faster Diffusion Models via Higher-Order Approximation", "authors": ["Gen Li", "Yuchen Zhou", "Yuting Wei", "Yuxin Chen"], "summary": "In this paper, we explore provable acceleration of diffusion models without\nany additional retraining. Focusing on the task of approximating a target data\ndistribution in $\\mathbb{R}^d$ to within $\\varepsilon$ total-variation\ndistance, we propose a principled, training-free sampling algorithm that\nrequires only the order of\n  $$ d^{1+2/K} \\varepsilon^{-1/K} $$\n  score function evaluations (up to log factor) in the presence of accurate\nscores, where $K$ is an arbitrarily large fixed integer. This result applies to\na broad class of target data distributions, without the need for assumptions\nsuch as smoothness or log-concavity. Our theory is robust vis-a-vis inexact\nscore estimation, degrading gracefully as the score estimation error increases\n-- without demanding higher-order smoothness on the score estimates as assumed\nin previous work. The proposed algorithm draws insight from high-order ODE\nsolvers, leveraging high-order Lagrange interpolation and successive refinement\nto approximate the integral derived from the probability flow ODE.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.24042v1", "categories": ["cs.LG", "cs.NA", "math.NA", "math.ST", "stat.ML", "stat.TH"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.24042v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22445", "title": "Hierarchical Adversarially-Resilient Multi-Agent Reinforcement Learning for Cyber-Physical Systems Security", "authors": ["Saad Alqithami"], "summary": "Cyber-Physical Systems play a critical role in the infrastructure of various\nsectors, including manufacturing, energy distribution, and autonomous\ntransportation systems. However, their increasing connectivity renders them\nhighly vulnerable to sophisticated cyber threats, such as adaptive and zero-day\nattacks, against which traditional security methods like rule-based intrusion\ndetection and single-agent reinforcement learning prove insufficient. To\novercome these challenges, this paper introduces a novel Hierarchical\nAdversarially-Resilient Multi-Agent Reinforcement Learning (HAMARL) framework.\nHAMARL employs a hierarchical structure consisting of local agents dedicated to\nsubsystem security and a global coordinator that oversees and optimizes\ncomprehensive, system-wide defense strategies. Furthermore, the framework\nincorporates an adversarial training loop designed to simulate and anticipate\nevolving cyber threats, enabling proactive defense adaptation. Extensive\nexperimental evaluations conducted on a simulated industrial IoT testbed\nindicate that HAMARL substantially outperforms traditional multi-agent\nreinforcement learning approaches, significantly improving attack detection\naccuracy, reducing response times, and ensuring operational continuity. The\nresults underscore the effectiveness of combining hierarchical multi-agent\ncoordination with adversarially-aware training to enhance the resilience and\nsecurity of next-generation CPS.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22445v1", "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.MA"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22445v1", "date": "2025-06-12", "updated": "2025-06-12"}
{"id": "2506.23771", "title": "Multi-Timescale Hierarchical Reinforcement Learning for Unified Behavior and Control of Autonomous Driving", "authors": ["Guizhe Jin", "Zhuoren Li", "Bo Leng", "Ran Yu", "Lu Xiong"], "summary": "Reinforcement Learning (RL) is increasingly used in autonomous driving (AD)\nand shows clear advantages. However, most RL-based AD methods overlook policy\nstructure design. An RL policy that only outputs short-timescale vehicle\ncontrol commands results in fluctuating driving behavior due to fluctuations in\nnetwork outputs, while one that only outputs long-timescale driving goals\ncannot achieve unified optimality of driving behavior and control. Therefore,\nwe propose a multi-timescale hierarchical reinforcement learning approach. Our\napproach adopts a hierarchical policy structure, where high- and low-level RL\npolicies are unified-trained to produce long-timescale motion guidance and\nshort-timescale control commands, respectively. Therein, motion guidance is\nexplicitly represented by hybrid actions to capture multimodal driving\nbehaviors on structured road and support incremental low-level extend-state\nupdates. Additionally, a hierarchical safety mechanism is designed to ensure\nmulti-timescale safety. Evaluation in simulator-based and HighD dataset-based\nhighway multi-lane scenarios demonstrates that our approach significantly\nimproves AD performance, effectively increasing driving efficiency, action\nconsistency and safety.", "comment": "8 pages, Submitted to IEEE Robotics and Automation Letters", "pdf_url": "http://arxiv.org/pdf/2506.23771v1", "categories": ["cs.RO", "cs.AI"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.23771v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22893", "title": "Agentic Enterprise: AI-Centric User to User-Centric AI", "authors": ["Arpit Narechania", "Alex Endert", "Atanu R Sinha"], "summary": "After a very long winter, the Artificial Intelligence (AI) spring is here.\nOr, so it seems over the last three years. AI has the potential to impact many\nareas of human life - personal, social, health, education, professional. In\nthis paper, we take a closer look at the potential of AI for Enterprises, where\ndecision-making plays a crucial and repeated role across functions, tasks, and\noperations. We consider Agents imbued with AI as means to increase\ndecision-productivity of enterprises. We highlight six tenets for Agentic\nsuccess in enterprises, by drawing attention to what the current, AI-Centric\nUser paradigm misses, in the face of persistent needs of and usefulness for\nEnterprise Decision-Making. In underscoring a shift to User-Centric AI, we\noffer six tenets and promote market mechanisms for platforms, aligning the\ndesign of AI and its delivery by Agents to the cause of enterprise users.", "comment": "12 pages, 1 figure, 2 sidebars; Preprint", "pdf_url": "http://arxiv.org/pdf/2506.22893v1", "categories": ["cs.AI", "cs.HC"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.22893v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23967", "title": "Green Metrics Tool: Measuring for fun and profit", "authors": ["Geerd-Dietger Hoffmann", "Verena Majuntke"], "summary": "The environmental impact of software is gaining increasing attention as the\ndemand for computational resources continues to rise. In order to optimize\nsoftware resource consumption and reduce carbon emissions, measuring and\nevaluating software is a first essential step. In this paper we discuss what\nmetrics are important for fact base decision making. We introduce the Green\nMetrics Tool (GMT), a novel framework for accurately measuring the resource\nconsumption of software. The tool provides a containerized, controlled, and\nreproducible life cycle-based approach, assessing the resource use of software\nduring key phases. Finally, we discuss GMT features like visualization,\ncomparability and rule- and LLM-based optimisations highlighting its potential\nto guide developers and researchers in reducing the environmental impact of\ntheir software.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23967v1", "categories": ["cs.SE", "cs.CY", "cs.ET"], "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.23967v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22784", "title": "Single-Frame Point-Pixel Registration via Supervised Cross-Modal Feature Matching", "authors": ["Yu Han", "Zhiwei Huang", "Yanting Zhang", "Fangjun Ding", "Shen Cai", "Rui Fan"], "summary": "Point-pixel registration between LiDAR point clouds and camera images is a\nfundamental yet challenging task in autonomous driving and robotic perception.\nA key difficulty lies in the modality gap between unstructured point clouds and\nstructured images, especially under sparse single-frame LiDAR settings.\nExisting methods typically extract features separately from point clouds and\nimages, then rely on hand-crafted or learned matching strategies. This separate\nencoding fails to bridge the modality gap effectively, and more critically,\nthese methods struggle with the sparsity and noise of single-frame LiDAR, often\nrequiring point cloud accumulation or additional priors to improve reliability.\nInspired by recent progress in detector-free matching paradigms (e.g.\nMatchAnything), we revisit the projection-based approach and introduce the\ndetector-free framework for direct point-pixel matching between LiDAR and\ncamera views. Specifically, we project the LiDAR intensity map into a 2D view\nfrom the LiDAR perspective and feed it into an attention-based detector-free\nmatching network, enabling cross-modal correspondence estimation without\nrelying on multi-frame accumulation. To further enhance matching reliability,\nwe introduce a repeatability scoring mechanism that acts as a soft visibility\nprior. This guides the network to suppress unreliable matches in regions with\nlow intensity variation, improving robustness under sparse input. Extensive\nexperiments on KITTI, nuScenes, and MIAS-LCEC-TF70 benchmarks demonstrate that\nour method achieves state-of-the-art performance, outperforming prior\napproaches on nuScenes (even those relying on accumulated point clouds),\ndespite using only single-frame LiDAR.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22784v1", "categories": ["cs.CV", "cs.AI", "cs.RO"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22784v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.22923", "title": "Energy-Aware Model Predictive Control for Batch Manufacturing System Scheduling Under Different Electricity Pricing Strategies", "authors": ["Hongliang Li", "Herschel C. Pangborn", "Ilya Kovalenko"], "summary": "Manufacturing industries are among the highest energy-consuming sectors,\nfacing increasing pressure to reduce energy costs. This paper presents an\nenergy-aware Model Predictive Control (MPC) framework to dynamically schedule\nmanufacturing processes in response to time-varying electricity prices without\ncompromising production goals or violating production constraints. A\nnetwork-based manufacturing system model is developed to capture complex\nmaterial flows, batch processing, and capacities of buffers and machines. The\nscheduling problem is formulated as a Mixed-Integer Quadratic Program (MIQP)\nthat balances energy costs, buffer levels, and production requirements. A case\nstudy evaluates the proposed MPC framework under four industrial electricity\npricing schemes. Numerical results demonstrate that the approach reduces energy\nusage expenses while satisfying production goals and adhering to production\nconstraints. The findings highlight the importance of considering the detailed\nelectricity cost structure in manufacturing scheduling decisions and provide\npractical insights for manufacturers when selecting among different electricity\npricing strategies.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22923v1", "categories": ["math.OC", "cs.SY", "eess.SY"], "cate": "math.OC", "url": "http://arxiv.org/abs/2506.22923v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23495", "title": "Far-Field vs. Near-Field Propagation Channels: Key Differences and Impact on 6G XL-MIMO Performance Evaluation", "authors": ["Zihang Ding", "Jianhua Zhang", "Changsheng You", "Pan Tang", "Hongbo Xing", "Zhiqiang Yuan", "Jie Meng", "Guangyi Liu"], "summary": "Extremely large-scale multiple-input multiple-output (XL-MIMO) is regarded as\na promising technology for next-generation communication systems. However, this\nwill expand the near-field (NF) range, rendering more users more likely to be\nlocated in the NF region. In this paper, we aim to answer two questions: What\nare the new characteristics of the NF channel? Is it necessary to develop new\ntransciver techniques to maintain system performance within the NF region? To\nthis end, we first review current NF channel models and analyze the differences\nbetween the existing 3GPP TR 38.901 channel model and the NF channel model,\nincluding the spherical wavefront and spatially non-stationarity. Then, we\nprovide examples on how these differences affect the XL-MIMO system performance\nin terms of beamforming gain and achievable rate. Simulation results\ndemonstrate that, when using far-field (FF) technique under the NF channel, the\nmaximum normalized beam gain loss is less than 3 dB for most users in the NF\nregion defined by Rayleigh distance. Moreover, the achievable rate loss of beam\ntraining is less than 3% compared to that realized by NF technique. Finally, we\ndemonstrate the necessity of employing NF transceiver techniques based on\nsimulation results.", "comment": "13 pages, 8 figures, 2 tables, 52 references. Note: This article has\n  been submitted to China Communications and is currently under review", "pdf_url": "http://arxiv.org/pdf/2506.23495v1", "categories": ["eess.SP"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.23495v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22710", "title": "LightBSR: Towards Lightweight Blind Super-Resolution via Discriminative Implicit Degradation Representation Learning", "authors": ["Jiang Yuan", "JI Ma", "Bo Wang", "Guanzhou Ke", "Weiming Hu"], "summary": "Implicit degradation estimation-based blind super-resolution (IDE-BSR) hinges\non extracting the implicit degradation representation (IDR) of the LR image and\nadapting it to LR image features to guide HR detail restoration. Although\nIDE-BSR has shown potential in dealing with noise interference and complex\ndegradations, existing methods ignore the importance of IDR discriminability\nfor BSR and instead over-complicate the adaptation process to improve effect,\nresulting in a significant increase in the model's parameters and computations.\nIn this paper, we focus on the discriminability optimization of IDR and propose\na new powerful and lightweight BSR model termed LightBSR. Specifically, we\nemploy a knowledge distillation-based learning framework. We first introduce a\nwell-designed degradation-prior-constrained contrastive learning technique\nduring teacher stage to make the model more focused on distinguishing different\ndegradation types. Then we utilize a feature alignment technique to transfer\nthe degradation-related knowledge acquired by the teacher to the student for\npractical inferencing. Extensive experiments demonstrate the effectiveness of\nIDR discriminability-driven BSR model design. The proposed LightBSR can achieve\noutstanding performance with minimal complexity across a range of blind SR\ntasks. Our code is accessible at: https://github.com/MJ-NCEPU/LightBSR.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22710v1", "categories": ["cs.CV", "eess.IV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22710v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.22929", "title": "Mathematical Computation on High-dimensional Data via Array Programming and Parallel Acceleration", "authors": ["Chen Zhang"], "summary": "While deep learning excels in natural image and language processing, its\napplication to high-dimensional data faces computational challenges due to the\ndimensionality curse. Current large-scale data tools focus on business-oriented\ndescriptive statistics, lacking mathematical statistics support for advanced\nanalysis. We propose a parallel computation architecture based on space\ncompleteness, decomposing high-dimensional data into dimension-independent\nstructures for distributed processing. This framework enables seamless\nintegration of data mining and parallel-optimized machine learning methods,\nsupporting scientific computations across diverse data types like medical and\nnatural images within a unified system.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22929v1", "categories": ["cs.LG", "cs.AI", "eess.IV", "eess.SP"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22929v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23793", "title": "Advancing Learnable Multi-Agent Pathfinding Solvers with Active Fine-Tuning", "authors": ["Anton Andreychuk", "Konstantin Yakovlev", "Aleksandr Panov", "Alexey Skrynnik"], "summary": "Multi-agent pathfinding (MAPF) is a common abstraction of multi-robot\ntrajectory planning problems, where multiple homogeneous robots simultaneously\nmove in the shared environment. While solving MAPF optimally has been proven to\nbe NP-hard, scalable, and efficient, solvers are vital for real-world\napplications like logistics, search-and-rescue, etc. To this end, decentralized\nsuboptimal MAPF solvers that leverage machine learning have come on stage.\nBuilding on the success of the recently introduced MAPF-GPT, a pure imitation\nlearning solver, we introduce MAPF-GPT-DDG. This novel approach effectively\nfine-tunes the pre-trained MAPF model using centralized expert data. Leveraging\na novel delta-data generation mechanism, MAPF-GPT-DDG accelerates training\nwhile significantly improving performance at test time. Our experiments\ndemonstrate that MAPF-GPT-DDG surpasses all existing learning-based MAPF\nsolvers, including the original MAPF-GPT, regarding solution quality across\nmany testing scenarios. Remarkably, it can work with MAPF instances involving\nup to 1 million agents in a single environment, setting a new milestone for\nscalability in MAPF domains.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23793v1", "categories": ["cs.AI", "cs.LG", "cs.MA"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.23793v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23133", "title": "Format-Adapter: Improving Reasoning Capability of LLMs by Adapting Suitable Format", "authors": ["Dingzirui Wang", "Xuanliang Zhang", "Rongyu Cao", "Longxu Dou", "Xianzhen Luo", "Yingwei Ma", "Qingfu Zhu", "Wanxiang Che", "Binhua Li", "Fei Huang", "Yongbin Li"], "summary": "Generating and voting multiple answers is an effective method to mitigate\nreasoning inconsistencies of large language models (LLMs). Prior works have\nshown that multiple reasoning formats outperform a single format when\ngenerating multiple answers. However, previous works using multiple formats\nrely on formats labeled by humans, which could be unsuitable for all tasks and\nhave high labeling costs. To address this issue, we adapt suitable formats to\nthe given tasks by generating and selecting formats. We first propose how to\nmeasure the reasoning error when generating multiple answers. Then, we\nintroduce Format-Adapter, which utilizes LLMs to generate and select suitable\nreasoning formats by minimizing the error measurement we present. We conduct\nexperiments on math and commonsense reasoning tasks, where Format-Adapter\nachieves a 4.3% performance improvement on average over previous works,\ndemonstrating the effectiveness.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23133v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.23133v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.22482", "title": "Wireless Home Automation Using Social Networking Websites", "authors": ["Divya Alok Gupta", "Dwith Chenna", "B. Aditya Vighnesh Ramakanth"], "summary": "With the advent of Internet of Things, Wireless Home Automation Systems WHAS\nare gradually gaining popularity. These systems are faced with multiple\nchallenges such as security; controlling a variety of home appliances with a\nsingle interface and user friendliness. In this paper we propose a system that\nuses secure authentication systems of social networking websites such as\nTwitter, tracks the end-users activities on the social network and then control\nhis or her domestic appliances. At the end, we highlight the applications of\nthe proposed WHAS and compare the advantages of our proposed system over\ntraditional home automation systems.", "comment": "20th Annual International Conference on Advanced Computing and\n  Communications (ADCOM) 2014", "pdf_url": "http://arxiv.org/pdf/2506.22482v1", "categories": ["cs.NI", "cs.CR", "cs.CV"], "cate": "cs.NI", "url": "http://arxiv.org/abs/2506.22482v1", "date": "2025-06-23", "updated": "2025-06-23"}
{"id": "2506.23781", "title": "Data-Driven Predictive Planning and Control for Aerial 3D Inspection with Back-face Elimination", "authors": ["Savvas Papaioannou", "Panayiotis Kolios", "Christos G. Panayiotou", "Marios M. Polycarpou"], "summary": "Automated inspection with Unmanned Aerial Systems (UASs) is a transformative\ncapability set to revolutionize various application domains. However, this task\nis inherently complex, as it demands the seamless integration of perception,\nplanning, and control which existing approaches often treat separately.\nMoreover, it requires accurate long-horizon planning to predict action\nsequences, in contrast to many current techniques, which tend to be myopic. To\novercome these limitations, we propose a 3D inspection approach that unifies\nperception, planning, and control within a single data-driven predictive\ncontrol framework. Unlike traditional methods that rely on known UAS dynamic\nmodels, our approach requires only input-output data, making it easily\napplicable to off-the-shelf black-box UASs. Our method incorporates back-face\nelimination, a visibility determination technique from 3D computer graphics,\ndirectly into the control loop, thereby enabling the online generation of\naccurate, long-horizon 3D inspection trajectories.", "comment": "2025 European Control Conference (ECC), Thessaloniki, Greece, 24-27\n  June 2025", "pdf_url": "http://arxiv.org/pdf/2506.23781v1", "categories": ["cs.RO", "cs.SY", "eess.SY"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.23781v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23092", "title": "Glyph-Based Multiscale Visualization of Turbulent Multi-Physics Statistics", "authors": ["Arisa Cowe", "Tyson Neuroth", "Qi Wu", "Martin Rieth", "Jacqueline Chen", "Myoungkyu Lee", "Kwan-Liu Ma"], "summary": "Many scientific and engineering problems involving multi-physics span a wide\nrange of scales. Understanding the interactions across these scales is\nessential for fully comprehending such complex problems. However, visualizing\nmultivariate, multiscale data within an integrated view where correlations\nacross space, scales, and fields are easily perceived remains challenging. To\naddress this, we introduce a novel local spatial statistical visualization of\nflow fields across multiple fields and turbulence scales. Our method leverages\nthe curvelet transform for scale decomposition of fields of interest, a\nlevel-set-restricted centroidal Voronoi tessellation to partition the spatial\ndomain into local regions for statistical aggregation, and a set of glyph\ndesigns that combines information across scales and fields into a single, or\nreduced set of perceivable visual representations. Each glyph represents data\naggregated within a Voronoi region and is positioned at the Voronoi site for\ndirect visualization in a 3D view centered around flow features of interest. We\nimplement and integrate our method into an interactive visualization system\nwhere the glyph-based technique operates in tandem with linked 3D spatial views\nand 2D statistical views, supporting a holistic analysis. We demonstrate with\ncase studies visualizing turbulent combustion data--multi-scalar compressible\nflows--and turbulent incompressible channel flow data. This new capability\nenables scientists to better understand the interactions between multiple\nfields and length scales in turbulent flows.", "comment": "15 pages (13 pages without references)", "pdf_url": "http://arxiv.org/pdf/2506.23092v1", "categories": ["cs.GR", "cs.HC"], "cate": "cs.GR", "url": "http://arxiv.org/abs/2506.23092v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23978", "title": "LLM Agents Are the Antidote to Walled Gardens", "authors": ["Samuele Marro", "Philip Torr"], "summary": "While the Internet's core infrastructure was designed to be open and\nuniversal, today's application layer is dominated by closed, proprietary\nplatforms. Open and interoperable APIs require significant investment, and\nmarket leaders have little incentive to enable data exchange that could erode\ntheir user lock-in. We argue that LLM-based agents fundamentally disrupt this\nstatus quo. Agents can automatically translate between data formats and\ninteract with interfaces designed for humans: this makes interoperability\ndramatically cheaper and effectively unavoidable. We name this shift universal\ninteroperability: the ability for any two digital services to exchange data\nseamlessly using AI-mediated adapters. Universal interoperability undermines\nmonopolistic behaviours and promotes data portability. However, it can also\nlead to new security risks and technical debt. Our position is that the ML\ncommunity should embrace this development while building the appropriate\nframeworks to mitigate the downsides. By acting now, we can harness AI to\nrestore user freedom and competitive markets without sacrificing security.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23978v1", "categories": ["cs.LG", "cs.CL", "cs.CY", "cs.SI", "68T50, 68M10, 91B26", "I.2.11; I.2.7; H.4.5"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.23978v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22800", "title": "RGE-GS: Reward-Guided Expansive Driving Scene Reconstruction via Diffusion Priors", "authors": ["Sicong Du", "Jiarun Liu", "Qifeng Chen", "Hao-Xiang Chen", "Tai-Jiang Mu", "Sheng Yang"], "summary": "A single-pass driving clip frequently results in incomplete scanning of the\nroad structure, making reconstructed scene expanding a critical requirement for\nsensor simulators to effectively regress driving actions. Although contemporary\n3D Gaussian Splatting (3DGS) techniques achieve remarkable reconstruction\nquality, their direct extension through the integration of diffusion priors\noften introduces cumulative physical inconsistencies and compromises training\nefficiency. To address these limitations, we present RGE-GS, a novel expansive\nreconstruction framework that synergizes diffusion-based generation with\nreward-guided Gaussian integration. The RGE-GS framework incorporates two key\ninnovations: First, we propose a reward network that learns to identify and\nprioritize consistently generated patterns prior to reconstruction phases,\nthereby enabling selective retention of diffusion outputs for spatial\nstability. Second, during the reconstruction process, we devise a\ndifferentiated training strategy that automatically adjust Gaussian\noptimization progress according to scene converge metrics, which achieving\nbetter convergence than baseline methods. Extensive evaluations of publicly\navailable datasets demonstrate that RGE-GS achieves state-of-the-art\nperformance in reconstruction quality. Our source-code will be made publicly\navailable at https://github.com/CN-ADLab/RGE-GS. (Camera-ready version\nincorporating reviewer suggestions will be updated soon.)", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22800v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22800v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.22985", "title": "Orthogonal Frequency Division Multiplexing Continuous Variable Terahertz Quantum Key Distribution", "authors": ["Mingqi Zhang", "Kaveh Delfanazari"], "summary": "We propose a novel continuous-variable quantum key distribution (CVQKD)\nprotocol that employs orthogonal frequency-division multiplexing (OFDM) in the\nterahertz (THz) band to enable high-throughput and secure quantum\ncommunication. By encoding quantum information across multiple subcarriers, the\nprotocol enhances spectral efficiency and mitigates channel dispersion and\natmospheric attenuation. We present a comprehensive security analysis under\ncollective Gaussian attacks, considering both terrestrial free-space channels,\naccounting for humidity-induced absorption, and inter-satellite links,\nincorporating realistic intermodulation noise. Simulations show secret key\nrates (SKR) reaching ~72 bits per channel use in open-air conditions. While\nintermodulation noise imposes trade-offs, optimised modulation variance enables\nresilience and secure communication range. The maximum terrestrial quantum link\nextends up to 4.5 m due to atmospheric THz absorption, whereas inter-satellite\nlinks can support secure communication over distances exceeding 100 km, owing\nto minimal propagation channel losses in space. We evaluate the practical\nimplementation of our protocol using recently developed on-chip coherent THz\nsources based on superconducting Josephson junctions. These compact,\nvoltage-tunable emitters produce wideband coherent radiation, making them ideal\ncandidates for integration in scalable quantum networks. By incorporating their\ncharacteristics into our simulations, we assess secure key generation under\nvarious environmental conditions. Our results show secure communication over\ndistances up to 3 m in open air, and up to 26 km in cryogenic or vacuum\nenvironments. This work advances the prospect of compact, high-capacity CVQKD\nsystems for both terrestrial and space-based THz quantum communication.", "comment": "12 pages, 9 figures", "pdf_url": "http://arxiv.org/pdf/2506.22985v1", "categories": ["quant-ph", "cs.SY", "eess.SY", "physics.app-ph", "physics.ins-det", "physics.optics"], "cate": "quant-ph", "url": "http://arxiv.org/abs/2506.22985v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23511", "title": "Mutli-Level Autoencoder: Deep Learning Based Channel Coding and Modulation", "authors": ["Ahmad Abdel-Qader", "Anas Chaaban", "Mohamed S. Shehata"], "summary": "In this paper, we design a deep learning-based convolutional autoencoder for\nchannel coding and modulation. The objective is to develop an adaptive scheme\ncapable of operating at various signal-to-noise ratios (SNR)s without the need\nfor re-training. Additionally, the proposed framework allows validation by\ntesting all possible codes in the codebook, as opposed to previous AI-based\nencoder/decoder frameworks which relied on testing only a small subset of the\navailable codes. This limitation in earlier methods often led to unreliable\nconclusions when generalized to larger codebooks. In contrast to previous\nmethods, our multi-level encoding and decoding approach splits the message into\nblocks, where each encoder block processes a distinct group of $B$ bits. By\ndoing so, the proposed scheme can exhaustively test $2^{B}$ possible codewords\nfor each encoder/decoder level, constituting a layer of the overall scheme. The\nproposed model was compared to classical polar codes and TurboAE-MOD schemes,\nshowing improved reliability with achieving comparable, or even superior\nresults in some settings. Notably, the architecture can adapt to different SNRs\nby selectively removing one of the encoder/decoder layers without re-training,\nthus demonstrating flexibility and efficiency in practical wireless\ncommunication scenarios.", "comment": "Accepted at IWCMC 2025", "pdf_url": "http://arxiv.org/pdf/2506.23511v1", "categories": ["eess.SP", "cs.ET"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.23511v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22899", "title": "Neural Cellular Automata: From Cells to Pixels", "authors": ["Ehsan Pajouheshgar", "Yitao Xu", "Ali Abbasi", "Alexander Mordvintsev", "Wenzel Jakob", "Sabine S√ºsstrunk"], "summary": "Neural Cellular Automata (NCAs) are bio-inspired systems in which identical\ncells self-organize to form complex and coherent patterns by repeatedly\napplying simple local rules. NCAs display striking emergent behaviors including\nself-regeneration, generalization and robustness to unseen situations, and\nspontaneous motion. Despite their success in texture synthesis and\nmorphogenesis, NCAs remain largely confined to low-resolution grids. This\nlimitation stems from (1) training time and memory requirements that grow\nquadratically with grid size, (2) the strictly local propagation of information\nwhich impedes long-range cell communication, and (3) the heavy compute demands\nof real-time inference at high resolution. In this work, we overcome this\nlimitation by pairing NCA with a tiny, shared implicit decoder, inspired by\nrecent advances in implicit neural representations. Following NCA evolution on\na coarse grid, a lightweight decoder renders output images at arbitrary\nresolution. We also propose novel loss functions for both morphogenesis and\ntexture synthesis tasks, specifically tailored for high-resolution output with\nminimal memory and computation overhead. Combining our proposed architecture\nand loss functions brings substantial improvement in quality, efficiency, and\nperformance. NCAs equipped with our implicit decoder can generate full-HD\noutputs in real time while preserving their self-organizing, emergent\nproperties. Moreover, because each MLP processes cell states independently,\ninference remains highly parallelizable and efficient. We demonstrate the\napplicability of our approach across multiple NCA variants (on 2D, 3D grids,\nand 3D meshes) and multiple tasks, including texture generation and\nmorphogenesis (growing patterns from a seed), showing that with our proposed\nframework, NCAs seamlessly scale to high-resolution outputs with minimal\ncomputational overhead.", "comment": "6 pages, 5 figures, first draft", "pdf_url": "http://arxiv.org/pdf/2506.22899v1", "categories": ["cs.CV", "cs.GR", "cs.LG", "cs.MA", "eess.IV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22899v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.22950", "title": "Infinite Sampling: Efficient and Stable Grouped RL Training for Large Language Models", "authors": ["Liangyu Wang", "Huanyi Xie", "Xinhai Wang", "Tianjin Huang", "Mengdi Li", "Di Wang"], "summary": "Group-based reinforcement learning algorithms such as Group Reward Policy\nOptimization (GRPO) have proven effective for fine-tuning large language models\n(LLMs) with human feedback. However, generating and storing multiple responses\nper prompt incurs substantial memory overhead, especially as the sample group\nsize increases, limiting scalability under constrained hardware.\n  We propose Infinite Sampling, a framework that enables efficient and stable\nGRPO training by decoupling group size from GPU memory usage. It consists of:\n(1) micro sampling groups that decompose large groups into memory-feasible\nrounds; (2) continuous sampling that interleaves generation across groups to\nimprove utilization; and (3) a length-aware scheduler combining\ntoken-conditioned sequence length prediction with a two-stage plan: global\ngrouping via FPTAS and runtime refill via SJF.\n  Experiments show that our Micro Sampling Groups reduce peak memory usage by\nover 50% compared to full-group decoding (e.g., from 21.55 GB to 10.64 GB on\nQwen3-1.7B). Building on this, Infinite Sampling improves throughput by over\n25% compared to the naive micro sampling group method, reducing decoding steps\nwhile maintaining full-length completions and memory usage. Our hybrid\nscheduling ensures efficient and stable GRPO training with larger groups under\nrealistic GPU memory constraints.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22950v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22950v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23844", "title": "A Survey on Autonomy-Induced Security Risks in Large Model-Based Agents", "authors": ["Hang Su", "Jun Luo", "Chang Liu", "Xiao Yang", "Yichi Zhang", "Yinpeng Dong", "Jun Zhu"], "summary": "Recent advances in large language models (LLMs) have catalyzed the rise of\nautonomous AI agents capable of perceiving, reasoning, and acting in dynamic,\nopen-ended environments. These large-model agents mark a paradigm shift from\nstatic inference systems to interactive, memory-augmented entities. While these\ncapabilities significantly expand the functional scope of AI, they also\nintroduce qualitatively novel security risks - such as memory poisoning, tool\nmisuse, reward hacking, and emergent misalignment - that extend beyond the\nthreat models of conventional systems or standalone LLMs. In this survey, we\nfirst examine the structural foundations and key capabilities that underpin\nincreasing levels of agent autonomy, including long-term memory retention,\nmodular tool use, recursive planning, and reflective reasoning. We then analyze\nthe corresponding security vulnerabilities across the agent stack, identifying\nfailure modes such as deferred decision hazards, irreversible tool chains, and\ndeceptive behaviors arising from internal state drift or value misalignment.\nThese risks are traced to architectural fragilities that emerge across\nperception, cognition, memory, and action modules. To address these challenges,\nwe systematically review recent defense strategies deployed at different\nautonomy layers, including input sanitization, memory lifecycle control,\nconstrained decision-making, structured tool invocation, and introspective\nreflection. We introduce the Reflective Risk-Aware Agent Architecture (R2A2), a\nunified cognitive framework grounded in Constrained Markov Decision Processes\n(CMDPs), which incorporates risk-aware world modeling, meta-policy adaptation,\nand joint reward-risk optimization to enable principled, proactive safety\nacross the agent's decision-making loop.", "comment": "18 pages", "pdf_url": "http://arxiv.org/pdf/2506.23844v1", "categories": ["cs.AI"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.23844v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23136", "title": "LLM-Assisted Question-Answering on Technical Documents Using Structured Data-Aware Retrieval Augmented Generation", "authors": ["Shadman Sobhan", "Mohammad Ariful Haque"], "summary": "Large Language Models (LLMs) are capable of natural language understanding\nand generation. But they face challenges such as hallucination and outdated\nknowledge. Fine-tuning is one possible solution, but it is resource-intensive\nand must be repeated with every data update. Retrieval-Augmented Generation\n(RAG) offers an efficient solution by allowing LLMs to access external\nknowledge sources. However, traditional RAG pipelines struggle with retrieving\ninformation from complex technical documents with structured data such as\ntables and images. In this work, we propose a RAG pipeline, capable of handling\ntables and images in documents, for technical documents that support both\nscanned and searchable formats. Its retrieval process combines vector\nsimilarity search with a fine-tuned reranker based on Gemma-2-9b-it. The\nreranker is trained using RAFT (Retrieval-Augmented Fine-Tuning) on a custom\ndataset designed to improve context identification for question answering. Our\nevaluation demonstrates that the proposed pipeline achieves a high faithfulness\nscore of 94% (RAGas) and 96% (DeepEval), and an answer relevancy score of 87%\n(RAGas) and 93% (DeepEval). Comparative analysis demonstrates that the proposed\narchitecture is superior to general RAG pipelines in terms of table-based\nquestions and handling questions outside context.", "comment": "29 Pages, 11 Tables", "pdf_url": "http://arxiv.org/pdf/2506.23136v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.23136v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.22802", "title": "Riemannian-Geometric Fingerprints of Generative Models", "authors": ["Hae Jin Song", "Laurent Itti"], "summary": "Recent breakthroughs and rapid integration of generative models (GMs) have\nsparked interest in the problem of model attribution and their fingerprints.\nFor instance, service providers need reliable methods of authenticating their\nmodels to protect their IP, while users and law enforcement seek to verify the\nsource of generated content for accountability and trust. In addition, a\ngrowing threat of model collapse is arising, as more model-generated data are\nbeing fed back into sources (e.g., YouTube) that are often harvested for\ntraining (\"regurgitative training\"), heightening the need to differentiate\nsynthetic from human data. Yet, a gap still exists in understanding generative\nmodels' fingerprints, we believe, stemming from the lack of a formal framework\nthat can define, represent, and analyze the fingerprints in a principled way.\nTo address this gap, we take a geometric approach and propose a new definition\nof artifact and fingerprint of GMs using Riemannian geometry, which allows us\nto leverage the rich theory of differential geometry. Our new definition\ngeneralizes previous work (Song et al., 2024) to non-Euclidean manifolds by\nlearning Riemannian metrics from data and replacing the Euclidean distances and\nnearest-neighbor search with geodesic distances and kNN-based Riemannian center\nof mass. We apply our theory to a new gradient-based algorithm for computing\nthe fingerprints in practice. Results show that it is more effective in\ndistinguishing a large array of GMs, spanning across 4 different datasets in 2\ndifferent resolutions (64 by 64, 256 by 256), 27 model architectures, and 2\nmodalities (Vision, Vision-Language). Using our proposed definition\nsignificantly improves the performance on model attribution, as well as a\ngeneralization to unseen datasets, model types, and modalities, suggesting its\npractical efficacy.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22802v1", "categories": ["cs.LG", "cs.CR", "cs.CV", "I.2.6"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22802v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23919", "title": "World4Omni: A Zero-Shot Framework from Image Generation World Model to Robotic Manipulation", "authors": ["Haonan Chen", "Bangjun Wang", "Jingxiang Guo", "Tianrui Zhang", "Yiwen Hou", "Xuchuan Huang", "Chenrui Tie", "Lin Shao"], "summary": "Improving data efficiency and generalization in robotic manipulation remains\na core challenge. We propose a novel framework that leverages a pre-trained\nmultimodal image-generation model as a world model to guide policy learning. By\nexploiting its rich visual-semantic representations and strong generalization\nacross diverse scenes, the model generates open-ended future state predictions\nthat inform downstream manipulation. Coupled with zero-shot low-level control\nmodules, our approach enables general-purpose robotic manipulation without\ntask-specific training. Experiments in both simulation and real-world\nenvironments demonstrate that our method achieves effective performance across\na wide range of manipulation tasks with no additional data collection or\nfine-tuning. Supplementary materials are available on our website:\nhttps://world4omni.github.io/.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23919v1", "categories": ["cs.RO"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.23919v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23549", "title": "CooT: Learning to Coordinate In-Context with Coordination Transformers", "authors": ["Huai-Chih Wang", "Hsiang-Chun Chuang", "Hsi-Chun Cheng", "Dai-Jie Wu", "Shao-Hua Sun"], "summary": "Effective coordination among artificial agents in dynamic and uncertain\nenvironments remains a significant challenge in multi-agent systems. Existing\napproaches, such as self-play and population-based methods, either generalize\npoorly to unseen partners or require extensive training. To overcome these\nlimitations, we propose Coordination Transformers (CooT), a novel in-context\ncoordination framework that uses recent interaction histories to adapt to\nunseen partners rapidly. Unlike previous approaches that primarily aim to\nincrease the diversity of training partners, CooT explicitly focuses on\nadapting to new partner behaviors by predicting actions aligned with observed\npartner interactions. Trained on interaction trajectories collected from\ndiverse pairs of agents with complementary behaviors, CooT quickly learns\neffective coordination strategies without explicit supervision or fine-tuning.\nEvaluations on the Overcooked benchmark demonstrate that CooT significantly\noutperforms baseline methods in coordination tasks involving previously unseen\npartners. Human evaluations further confirm CooT as the most effective\ncollaborative partner, while extensive ablations highlight its robustness,\nflexibility, and sensitivity to context in multi-agent scenarios.", "comment": "23 pages, 10 tables, 8 figures", "pdf_url": "http://arxiv.org/pdf/2506.23549v1", "categories": ["cs.AI", "cs.HC", "cs.LG"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.23549v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22803", "title": "Intervening in Black Box: Concept Bottleneck Model for Enhancing Human Neural Network Mutual Understanding", "authors": ["Nuoye Xiong", "Anqi Dong", "Ning Wang", "Cong Hua", "Guangming Zhu", "Mei Lin", "Peiyi Shen", "Liang Zhang"], "summary": "Recent advances in deep learning have led to increasingly complex models with\ndeeper layers and more parameters, reducing interpretability and making their\ndecisions harder to understand. While many methods explain black-box reasoning,\nmost lack effective interventions or only operate at sample-level without\nmodifying the model itself. To address this, we propose the Concept Bottleneck\nModel for Enhancing Human-Neural Network Mutual Understanding (CBM-HNMU).\nCBM-HNMU leverages the Concept Bottleneck Model (CBM) as an interpretable\nframework to approximate black-box reasoning and communicate conceptual\nunderstanding. Detrimental concepts are automatically identified and refined\n(removed/replaced) based on global gradient contributions. The modified CBM\nthen distills corrected knowledge back into the black-box model, enhancing both\ninterpretability and accuracy. We evaluate CBM-HNMU on various CNN and\ntransformer-based models across Flower-102, CIFAR-10, CIFAR-100, FGVC-Aircraft,\nand CUB-200, achieving a maximum accuracy improvement of 2.64% and a maximum\nincrease in average accuracy across 1.03%. Source code is available at:\nhttps://github.com/XiGuaBo/CBM-HNMU.", "comment": "Accepted by ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2506.22803v1", "categories": ["cs.CV", "cs.HC", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22803v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.22991", "title": "Resilient-Native and Intelligent Next-Generation Wireless Systems: Key Enablers, Foundations, and Applications", "authors": ["Mehdi Bennis", "Sumudu Samarakoon", "Tamara Alshammari", "Chathuranga Weeraddana", "Zhoujun Tian", "Chaouki Ben Issaid"], "summary": "Just like power, water, and transportation systems, wireless networks are a\ncrucial societal infrastructure. As natural and human-induced disruptions\ncontinue to grow, wireless networks must be resilient. This requires them to\nwithstand and recover from unexpected adverse conditions, shocks, unmodeled\ndisturbances and cascading failures. Unlike robustness and reliability,\nresilience is based on the understanding that disruptions will inevitably\nhappen. Resilience, as elasticity, focuses on the ability to bounce back to\nfavorable states, while resilience as plasticity involves agents and networks\nthat can flexibly expand their states and hypotheses through real-time\nadaptation and reconfiguration. This situational awareness and active\npreparedness, adapting world models and counterfactually reasoning about\npotential system failures and the best responses, is a core aspect of\nresilience. This article will first disambiguate resilience from reliability\nand robustness, before delving into key mathematical foundations of resilience\ngrounded in abstraction, compositionality and emergence. Subsequently, we focus\nour attention on a plethora of techniques and methodologies pertaining to the\nunique characteristics of resilience, as well as their applications through a\ncomprehensive set of use cases. Ultimately, the goal of this paper is to\nestablish a unified foundation for understanding, modeling, and engineering\nresilience in wireless communication systems, while laying a roadmap for the\nnext-generation of resilient-native and intelligent wireless systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22991v1", "categories": ["cs.NI", "cs.LO", "cs.MA", "cs.SY", "eess.SY"], "cate": "cs.NI", "url": "http://arxiv.org/abs/2506.22991v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23525", "title": "Sensing for Free: Learn to Localize More Sources than Antennas without Pilots", "authors": ["Wentao Yu", "Khaled B. Letaief", "Lizhong Zheng"], "summary": "Integrated sensing and communication (ISAC) represents a key paradigm for\nfuture wireless networks. However, existing approaches require waveform\nmodifications, dedicated pilots, or overhead that complicates standards\nintegration. We propose sensing for free - performing multi-source localization\nwithout pilots by reusing uplink data symbols, making sensing occur during\ntransmission and directly compatible with 3GPP 5G NR and 6G specifications.\nWith ever-increasing devices in dense 6G networks, this approach is\nparticularly compelling when combined with sparse arrays, which can localize\nmore sources than uniform arrays via an enlarged virtual array. Existing\npilot-free multi-source localization algorithms first reconstruct an extended\ncovariance matrix and apply subspace methods, incurring cubic complexity and\nlimited to second-order statistics. Performance degrades under non-Gaussian\ndata symbols and few snapshots, and higher-order statistics remain unexploited.\nWe address these challenges with an attention-only transformer that directly\nprocesses raw signal snapshots for grid-less end-to-end direction-of-arrival\n(DOA) estimation. The model efficiently captures higher-order statistics while\nbeing permutation-invariant and adaptive to varying snapshot counts. Our\nalgorithm greatly outperforms state-of-the-art AI-based benchmarks with over\n30x reduction in parameters and runtime, and enjoys excellent generalization\nunder practical mismatches. Applied to multi-user MIMO beam training, our\nalgorithm can localize uplink DOAs of multiple users during data transmission.\nThrough angular reciprocity, estimated uplink DOAs prune downlink beam sweeping\ncandidates and improve throughput via sensing-assisted beam management. This\nwork shows how reusing existing data transmission for sensing can enhance both\nmulti-source localization and beam management in 3GPP efforts towards 6G.", "comment": "13 pages, 9 figures, 1 table", "pdf_url": "http://arxiv.org/pdf/2506.23525v1", "categories": ["eess.SP"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.23525v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22902", "title": "Point Cloud Compression and Objective Quality Assessment: A Survey", "authors": ["Yiling Xu", "Yujie Zhang", "Shuting Xia", "Kaifa Yang", "He Huang", "Ziyu Shan", "Wenjie Huang", "Qi Yang", "Le Yang"], "summary": "The rapid growth of 3D point cloud data, driven by applications in autonomous\ndriving, robotics, and immersive environments, has led to criticals demand for\nefficient compression and quality assessment techniques. Unlike traditional 2D\nmedia, point clouds present unique challenges due to their irregular structure,\nhigh data volume, and complex attributes. This paper provides a comprehensive\nsurvey of recent advances in point cloud compression (PCC) and point cloud\nquality assessment (PCQA), emphasizing their significance for real-time and\nperceptually relevant applications. We analyze a wide range of handcrafted and\nlearning-based PCC algorithms, along with objective PCQA metrics. By\nbenchmarking representative methods on emerging datasets, we offer detailed\ncomparisons and practical insights into their strengths and limitations.\nDespite notable progress, challenges such as enhancing visual fidelity,\nreducing latency, and supporting multimodal data remain. This survey outlines\nfuture directions, including hybrid compression frameworks and advanced feature\nextraction strategies, to enable more efficient, immersive, and intelligent 3D\napplications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22902v1", "categories": ["cs.CV", "eess.IV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22902v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.22984", "title": "Cybersecurity-Focused Anomaly Detection in Connected Autonomous Vehicles Using Machine Learning", "authors": ["Prathyush Kumar Reddy Lebaku", "Lu Gao", "Yunpeng Zhang", "Zhixia Li", "Yongxin Liu", "Tanvir Arafin"], "summary": "Anomaly detection in connected autonomous vehicles (CAVs) is crucial for\nmaintaining safe and reliable transportation networks, as CAVs can be\nsusceptible to sensor malfunctions, cyber-attacks, and unexpected environmental\ndisruptions. This study explores an anomaly detection approach by simulating\nvehicle behavior, generating a dataset that represents typical and atypical\nvehicular interactions. The dataset includes time-series data of position,\nspeed, and acceleration for multiple connected autonomous vehicles. We utilized\nmachine learning models to effectively identify abnormal driving patterns.\nFirst, we applied a stacked Long Short-Term Memory (LSTM) model to capture\ntemporal dependencies and sequence-based anomalies. The stacked LSTM model\nprocessed the sequential data to learn standard driving behaviors.\nAdditionally, we deployed a Random Forest model to support anomaly detection by\noffering ensemble-based predictions, which enhanced model interpretability and\nperformance. The Random Forest model achieved an R2 of 0.9830, MAE of 5.746,\nand a 95th percentile anomaly threshold of 14.18, while the stacked LSTM model\nattained an R2 of 0.9998, MAE of 82.425, and a 95th percentile anomaly\nthreshold of 265.63. These results demonstrate the models' effectiveness in\naccurately predicting vehicle trajectories and detecting anomalies in\nautonomous driving scenarios.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22984v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22984v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23908", "title": "Beyond Statistical Learning: Exact Learning Is Essential for General Intelligence", "authors": ["Andr√°s Gy√∂rgy", "Tor Lattimore", "Nevena Laziƒá", "Csaba Szepesv√°ri"], "summary": "Sound deductive reasoning -- the ability to derive new knowledge from\nexisting facts and rules -- is an indisputably desirable aspect of general\nintelligence. Despite the major advances of AI systems in areas such as math\nand science, especially since the introduction of transformer architectures, it\nis well-documented that even the most advanced frontier systems regularly and\nconsistently falter on easily-solvable deductive reasoning tasks. Hence, these\nsystems are unfit to fulfill the dream of achieving artificial general\nintelligence capable of sound deductive reasoning. We argue that their unsound\nbehavior is a consequence of the statistical learning approach powering their\ndevelopment. To overcome this, we contend that to achieve reliable deductive\nreasoning in learning-based AI systems, researchers must fundamentally shift\nfrom optimizing for statistical performance against distributions on reasoning\nproblems and algorithmic tasks to embracing the more ambitious exact learning\nparadigm, which demands correctness on all inputs. We argue that exact learning\nis both essential and possible, and that this ambitious objective should guide\nalgorithm design.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23908v1", "categories": ["cs.AI", "cs.LG"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.23908v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23137", "title": "Flow-Modulated Scoring for Semantic-Aware Knowledge Graph Completion", "authors": ["Siyuan Li", "Ruitong Liu", "Yan Wen", "Te Sun"], "summary": "Effective modeling of multifaceted relations is pivotal for Knowledge Graph\nCompletion (KGC). However, a majority of existing approaches are predicated on\nstatic, embedding-based scoring, exhibiting inherent limitations in capturing\ncontextual dependencies and relational dynamics. Addressing this gap, we\npropose the Flow-Modulated Scoring (FMS) framework. FMS comprises two principal\ncomponents: (1) a semantic context learning module that encodes\ncontext-sensitive entity representations, and (2) a conditional flow-matching\nmodule designed to learn the dynamic transformation from a head to a tail\nembedding, governed by the aforementioned context. The resultant predictive\nvector field, representing the context-informed relational path, serves to\ndynamically refine the initial static score of an entity pair. Through this\nsynergy of context-aware static representations and conditioned dynamic\ninformation, FMS facilitates a more profound modeling of relational semantics.\nComprehensive evaluations on several standard benchmarks demonstrate that our\nproposed method surpasses prior state-of-the-art results.", "comment": "10 pages", "pdf_url": "http://arxiv.org/pdf/2506.23137v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.23137v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.22890", "title": "CP-Guard: A Unified, Probability-Agnostic, and Adaptive Framework for Malicious Agent Detection and Defense in Multi-Agent Embodied Perception Systems", "authors": ["Senkang Hu", "Yihang Tao", "Guowen Xu", "Xinyuan Qian", "Yiqin Deng", "Xianhao Chen", "Sam Tak Wu Kwong", "Yuguang Fang"], "summary": "Collaborative Perception (CP) has been shown to be a promising technique for\nmulti-agent autonomous driving and multi-agent robotic systems, where multiple\nagents share their perception information to enhance the overall perception\nperformance and expand the perception range. However, in CP, an ego agent needs\nto receive messages from its collaborators, which makes it vulnerable to\nattacks from malicious agents. To address this critical issue, we propose a\nunified, probability-agnostic, and adaptive framework, namely, CP-Guard, which\nis a tailored defense mechanism for CP deployed by each agent to accurately\ndetect and eliminate malicious agents in its collaboration network. Our key\nidea is to enable CP to reach a consensus rather than a conflict against an ego\nagent's perception results. Based on this idea, we first develop a\nprobability-agnostic sample consensus (PASAC) method to effectively sample a\nsubset of the collaborators and verify the consensus without prior\nprobabilities of malicious agents. Furthermore, we define collaborative\nconsistency loss (CCLoss) for object detection task and bird's eye view (BEV)\nsegmentation task to capture the discrepancy between an ego agent and its\ncollaborators, which is used as a verification criterion for consensus. In\naddition, we propose online adaptive threshold via dual sliding windows to\ndynamically adjust the threshold for consensus verification and ensure the\nreliability of the systems in dynamic environments. Finally, we conduct\nextensive experiments and demonstrate the effectiveness of our framework. Code\nwill be released at https://github.com/CP-Security/CP-Guard", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22890v1", "categories": ["cs.CV", "cs.CR"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22890v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23944", "title": "Adapt Your Body: Mitigating Proprioception Shifts in Imitation Learning", "authors": ["Fuhang Kuang", "Jiacheng You", "Yingdong Hu", "Tong Zhang", "Chuan Wen", "Yang Gao"], "summary": "Imitation learning models for robotic tasks typically rely on multi-modal\ninputs, such as RGB images, language, and proprioceptive states. While\nproprioception is intuitively important for decision-making and obstacle\navoidance, simply incorporating all proprioceptive states leads to a surprising\ndegradation in imitation learning performance. In this work, we identify the\nunderlying issue as the proprioception shift problem, where the distributions\nof proprioceptive states diverge significantly between training and deployment.\nTo address this challenge, we propose a domain adaptation framework that\nbridges the gap by utilizing rollout data collected during deployment. Using\nWasserstein distance, we quantify the discrepancy between expert and rollout\nproprioceptive states and minimize this gap by adding noise to both sets of\nstates, proportional to the Wasserstein distance. This strategy enhances\nrobustness against proprioception shifts by aligning the training and\ndeployment distributions. Experiments on robotic manipulation tasks demonstrate\nthe efficacy of our method, enabling the imitation policy to leverage\nproprioception while mitigating its adverse effects. Our approach outperforms\nthe naive solution which discards proprioception, and other baselines designed\nto address distributional shifts.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23944v1", "categories": ["cs.RO", "cs.AI"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.23944v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23682", "title": "Not quite a piece of CHERI-cake: Are new digital security by design architectures usable?", "authors": ["Maysara Alhindi", "Joseph Hallett"], "summary": "A digital security-by-design computer architecture, like CHERI, lets you\nprogram without fear of buffer overflows or other memory safety errors, but\nCHERI also rewrites some of the assumptions about how C works and how\nfundamental types (such as pointers) are implemented in hardware. We conducted\na usability study to examine how developers react to the changes required by\nCHERI when porting software to run on it. We find that developers struggle with\nCHERI's display of warnings and errors and a lack of diverse documentation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23682v1", "categories": ["cs.CR", "cs.AR", "cs.HC"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.23682v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22806", "title": "Concept Pinpoint Eraser for Text-to-image Diffusion Models via Residual Attention Gate", "authors": ["Byung Hyun Lee", "Sungjin Lim", "Seunggyu Lee", "Dong Un Kang", "Se Young Chun"], "summary": "Remarkable progress in text-to-image diffusion models has brought a major\nconcern about potentially generating images on inappropriate or trademarked\nconcepts. Concept erasing has been investigated with the goals of deleting\ntarget concepts in diffusion models while preserving other concepts with\nminimal distortion. To achieve these goals, recent concept erasing methods\nusually fine-tune the cross-attention layers of diffusion models. In this work,\nwe first show that merely updating the cross-attention layers in diffusion\nmodels, which is mathematically equivalent to adding \\emph{linear} modules to\nweights, may not be able to preserve diverse remaining concepts. Then, we\npropose a novel framework, dubbed Concept Pinpoint Eraser (CPE), by adding\n\\emph{nonlinear} Residual Attention Gates (ResAGs) that selectively erase (or\ncut) target concepts while safeguarding remaining concepts from broad\ndistributions by employing an attention anchoring loss to prevent the\nforgetting. Moreover, we adversarially train CPE with ResAG and learnable text\nembeddings in an iterative manner to maximize erasing performance and enhance\nrobustness against adversarial attacks. Extensive experiments on the erasure of\ncelebrities, artistic styles, and explicit contents demonstrated that the\nproposed CPE outperforms prior arts by keeping diverse remaining concepts while\ndeleting the target concepts with robustness against attack prompts. Code is\navailable at https://github.com/Hyun1A/CPE", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22806v1", "categories": ["cs.CV", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22806v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.22995", "title": "A Reinforcement Learning Approach for Optimal Control in Microgrids", "authors": ["Davide Salaorni", "Federico Bianchi", "Francesco Trov√≤", "Marcello Restelli"], "summary": "The increasing integration of renewable energy sources (RESs) is transforming\ntraditional power grid networks, which require new approaches for managing\ndecentralized energy production and consumption. Microgrids (MGs) provide a\npromising solution by enabling localized control over energy generation,\nstorage, and distribution. This paper presents a novel reinforcement learning\n(RL)-based methodology for optimizing microgrid energy management.\nSpecifically, we propose an RL agent that learns optimal energy trading and\nstorage policies by leveraging historical data on energy production,\nconsumption, and market prices. A digital twin (DT) is used to simulate the\nenergy storage system dynamics, incorporating degradation factors to ensure a\nrealistic emulation of the analysed setting. Our approach is validated through\nan experimental campaign using real-world data from a power grid located in the\nItalian territory. The results indicate that the proposed RL-based strategy\noutperforms rule-based methods and existing RL benchmarks, offering a robust\nsolution for intelligent microgrid management.", "comment": "8 pages, accepted to International Joint Conference on Neural\n  Networks 2025", "pdf_url": "http://arxiv.org/pdf/2506.22995v1", "categories": ["cs.LG", "cs.SY", "eess.SY"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22995v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23557", "title": "Data-Driven Modulation Optimization with LMMSE Equalization for Reliability Enhancement in Underwater Acoustic Communications", "authors": ["Xuehan Wang", "Hengyu Zhang", "Jintao Wang", "Zhi Sun", "Bo Ai"], "summary": "Ultra-reliable underwater acoustic (UWA) communications serve as one of the\nkey enabling technologies for future space-air-ground-underwater integrated\nnetworks. However, the reliability of current UWA transmission is still\ninsufficient since severe performance degradation occurs for conventional\nmulticarrier systems in UWA channels with severe delay-scale spread. To solve\nthis problem, we exploit learning-inspired approaches to optimize the\nmodulation scheme under the assumption of linear minimum mean square error\n(LMMSE) equalization, where the discrete representation of waveforms is adopted\nby utilizing Nyquist filters. The optimization problem is first transferred\ninto maximizing the fairness of estimation mean square error (MSE) for each\ndata symbol since the total MSE is invariant considering the property of\northogonal modulation. The Siamese architecture is then adopted to obtain\nconsistent optimization results across various channel conditions, which avoids\nthe overhead of online feedback, cooperation, and deployment of neural networks\nand guarantees generalization. The overall scheme including the loss function,\nneural network structure, and training process is also investigated in depth in\nthis paper. The excellent performance and robustness of the proposed modulation\nscheme are verified by carrying out the bit error rate test over various UWA\nchannels with severe delay-scale spread.", "comment": "6 pages, 3 figures. This paper has been accepted for presentation in\n  IEEE/CIC ICCC 2025", "pdf_url": "http://arxiv.org/pdf/2506.23557v1", "categories": ["eess.SP"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.23557v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22929", "title": "Mathematical Computation on High-dimensional Data via Array Programming and Parallel Acceleration", "authors": ["Chen Zhang"], "summary": "While deep learning excels in natural image and language processing, its\napplication to high-dimensional data faces computational challenges due to the\ndimensionality curse. Current large-scale data tools focus on business-oriented\ndescriptive statistics, lacking mathematical statistics support for advanced\nanalysis. We propose a parallel computation architecture based on space\ncompleteness, decomposing high-dimensional data into dimension-independent\nstructures for distributed processing. This framework enables seamless\nintegration of data mining and parallel-optimized machine learning methods,\nsupporting scientific computations across diverse data types like medical and\nnatural images within a unified system.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22929v1", "categories": ["cs.LG", "cs.AI", "eess.IV", "eess.SP"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22929v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.22994", "title": "Kernel Outlier Detection", "authors": ["Can Hakan Daƒüƒ±dƒ±r", "Mia Hubert", "Peter J. Rousseeuw"], "summary": "A new anomaly detection method called kernel outlier detection (KOD) is\nproposed. It is designed to address challenges of outlier detection in\nhigh-dimensional settings. The aim is to overcome limitations of existing\nmethods, such as dependence on distributional assumptions or on hyperparameters\nthat are hard to tune. KOD starts with a kernel transformation, followed by a\nprojection pursuit approach. Its novelties include a new ensemble of directions\nto search over, and a new way to combine results of different direction types.\nThis provides a flexible and lightweight approach for outlier detection. Our\nempirical evaluations illustrate the effectiveness of KOD on three small\ndatasets with challenging structures, and on four large benchmark datasets.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22994v1", "categories": ["cs.LG", "stat.ML"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22994v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23924", "title": "Performance of LLMs on Stochastic Modeling Operations Research Problems: From Theory to Practice", "authors": ["Akshit Kumar", "Tianyi Peng", "Yuhang Wu", "Assaf Zeevi"], "summary": "Large language models (LLMs) have exhibited expert-level capabilities across\nvarious domains. However, their abilities to solve problems in Operations\nResearch (OR) -- the analysis and optimization of mathematical models derived\nfrom real-world problems or their verbal descriptions -- remain underexplored.\nIn this work, we take a first step toward evaluating LLMs' abilities to solve\nstochastic modeling problems, a core class of OR problems characterized by\nuncertainty and typically involving tools from probability, statistics, and\nstochastic processes. We manually procure a representative set of\ngraduate-level homework and doctoral qualification-exam problems and test LLMs'\nabilities to solve them. We further leverage SimOpt, an open-source library of\nsimulation-optimization problems and solvers, to investigate LLMs' abilities to\nmake real-world decisions under uncertainty. Our results show that, though a\nnontrivial amount of work is still needed to reliably automate the stochastic\nmodeling pipeline in reality, state-of-the-art LLMs demonstrate proficiency on\npar with human experts in both classroom and practical settings. These findings\nhighlight the potential of building AI agents that assist OR researchers and\namplify the real-world impact of OR through automation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23924v1", "categories": ["cs.AI"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.23924v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23139", "title": "Benchmarking Deep Search over Heterogeneous Enterprise Data", "authors": ["Prafulla Kumar Choubey", "Xiangyu Peng", "Shilpa Bhagavath", "Kung-Hsiang Huang", "Caiming Xiong", "Chien-Sheng Wu"], "summary": "We present a new benchmark for evaluating Deep Search--a realistic and\ncomplex form of retrieval-augmented generation (RAG) that requires\nsource-aware, multi-hop reasoning over diverse, sparsed, but related sources.\nThese include documents, meeting transcripts, Slack messages, GitHub, and URLs,\nwhich vary in structure and often contain human-to-human interactions. We build\nit using a synthetic data pipeline that simulates business workflows across\nproduct planning, development, and support stages, generating interconnected\ncontent with realistic noise and multi-hop questions with guaranteed\nground-truth answers. We release our benchmark with both answerable and\nunanswerable queries, and retrieval pool of 39,190 enterprise artifacts,\nenabling fine-grained evaluation of long-context LLM and RAG systems. Our\nexperiments reveal that even the best-performing agentic RAG methods achieve an\naverage performance score of 32.96 on our benchmark. With further analysis, we\nhighlight retrieval as the main bottleneck: existing methods struggle to\nconduct deep searches and retrieve all necessary evidence. Consequently, they\noften reason over partial context, leading to significant performance\ndegradation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23139v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.23139v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.22961", "title": "MPC in the Quantum Head (or: Superposition-Secure (Quantum) Zero-Knowledge)", "authors": ["Andrea Coladangelo", "Ruta Jawale", "Dakshita Khurana", "Giulio Malavolta", "Hendrik Waldner"], "summary": "The MPC-in-the-head technique (Ishai et al., STOC 2007) is a celebrated\nmethod to build zero-knowledge protocols with desirable theoretical properties\nand high practical efficiency. This technique has generated a large body of\nresearch and has influenced the design of real-world post-quantum cryptographic\nsignatures. In this work, we present a generalization of the MPC-in-the-head\nparadigm to the quantum setting, where the MPC is running a quantum\ncomputation. As an application of our framework, we propose a new approach to\nbuild zero-knowledge protocols where security holds even against a verifier\nthat can obtain a superposition of transcripts. This notion was pioneered by\nDamgard et al., who built a zero-knowledge protocol for NP (in the common\nreference string model) secure against superposition attacks, by relying on\nperfectly hiding and unconditionally binding dual-mode commitments.\nUnfortunately, no such commitments are known from standard cryptographic\nassumptions. In this work we revisit this problem, and present two new\nthree-round protocols in the common reference string model: (i) A\nzero-knowledge argument for NP, whose security reduces to the standard learning\nwith errors (LWE) problem. (ii) A zero-knowledge argument for QMA from the same\nassumption.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22961v1", "categories": ["quant-ph", "cs.CR"], "cate": "quant-ph", "url": "http://arxiv.org/abs/2506.22961v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23999", "title": "Predictive Risk Analysis and Safe Trajectory Planning for Intelligent and Connected Vehicles", "authors": ["Zeyu Han", "Mengchi Cai", "Chaoyi Chen", "Qingwen Meng", "Guangwei Wang", "Ying Liu", "Qing Xu", "Jianqiang Wang", "Keqiang Li"], "summary": "The safe trajectory planning of intelligent and connected vehicles is a key\ncomponent in autonomous driving technology. Modeling the environment risk\ninformation by field is a promising and effective approach for safe trajectory\nplanning. However, existing risk assessment theories only analyze the risk by\ncurrent information, ignoring future prediction. This paper proposes a\npredictive risk analysis and safe trajectory planning framework for intelligent\nand connected vehicles. This framework first predicts future trajectories of\nobjects by a local risk-aware algorithm, following with a\nspatiotemporal-discretised predictive risk analysis using the prediction\nresults. Then the safe trajectory is generated based on the predictive risk\nanalysis. Finally, simulation and vehicle experiments confirm the efficacy and\nreal-time practicability of our approach.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23999v1", "categories": ["cs.RO"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.23999v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23721", "title": "Deep Learning-Based Semantic Segmentation for Real-Time Kidney Imaging and Measurements with Augmented Reality-Assisted Ultrasound", "authors": ["Gijs Luijten", "Roberto Maria Scardigno", "Lisle Faray de Paiva", "Peter Hoyer", "Jens Kleesiek", "Domenico Buongiorno", "Vitoantonio Bevilacqua", "Jan Egger"], "summary": "Ultrasound (US) is widely accessible and radiation-free but has a steep\nlearning curve due to its dynamic nature and non-standard imaging planes.\nAdditionally, the constant need to shift focus between the US screen and the\npatient poses a challenge. To address these issues, we integrate deep learning\n(DL)-based semantic segmentation for real-time (RT) automated kidney volumetric\nmeasurements, which are essential for clinical assessment but are traditionally\ntime-consuming and prone to fatigue. This automation allows clinicians to\nconcentrate on image interpretation rather than manual measurements.\nComplementing DL, augmented reality (AR) enhances the usability of US by\nprojecting the display directly into the clinician's field of view, improving\nergonomics and reducing the cognitive load associated with screen-to-patient\ntransitions. Two AR-DL-assisted US pipelines on HoloLens-2 are proposed: one\nstreams directly via the application programming interface for a wireless\nsetup, while the other supports any US device with video output for broader\naccessibility. We evaluate RT feasibility and accuracy using the Open Kidney\nDataset and open-source segmentation models (nnU-Net, Segmenter, YOLO with\nMedSAM and LiteMedSAM). Our open-source GitHub pipeline includes model\nimplementations, measurement algorithms, and a Wi-Fi-based streaming solution,\nenhancing US training and diagnostics, especially in point-of-care settings.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23721v1", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.HC", "cs.LG"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.23721v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22807", "title": "FreqDGT: Frequency-Adaptive Dynamic Graph Networks with Transformer for Cross-subject EEG Emotion Recognition", "authors": ["Yueyang Li", "Shengyu Gong", "Weiming Zeng", "Nizhuan Wang", "Wai Ting Siok"], "summary": "Electroencephalography (EEG) serves as a reliable and objective signal for\nemotion recognition in affective brain-computer interfaces, offering unique\nadvantages through its high temporal resolution and ability to capture\nauthentic emotional states that cannot be consciously controlled. However,\ncross-subject generalization remains a fundamental challenge due to individual\nvariability, cognitive traits, and emotional responses. We propose FreqDGT, a\nfrequency-adaptive dynamic graph transformer that systematically addresses\nthese limitations through an integrated framework. FreqDGT introduces\nfrequency-adaptive processing (FAP) to dynamically weight emotion-relevant\nfrequency bands based on neuroscientific evidence, employs adaptive dynamic\ngraph learning (ADGL) to learn input-specific brain connectivity patterns, and\nimplements multi-scale temporal disentanglement network (MTDN) that combines\nhierarchical temporal transformers with adversarial feature disentanglement to\ncapture both temporal dynamics and ensure cross-subject robustness.\nComprehensive experiments demonstrate that FreqDGT significantly improves\ncross-subject emotion recognition accuracy, confirming the effectiveness of\nintegrating frequency-adaptive, spatial-dynamic, and temporal-hierarchical\nmodeling while ensuring robustness to individual differences. The code is\navailable at https://github.com/NZWANG/FreqDGT.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22807v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22807v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23036", "title": "Fragile, Robust, and Antifragile: A Perspective from Parameter Responses in Reinforcement Learning Under Stress", "authors": ["Zain ul Abdeen", "Ming Jin"], "summary": "This paper explores Reinforcement learning (RL) policy robustness by\nsystematically analyzing network parameters under internal and external\nstresses. Inspired by synaptic plasticity in neuroscience, synaptic filtering\nintroduces internal stress by selectively perturbing parameters, while\nadversarial attacks apply external stress through modified agent observations.\nThis dual approach enables the classification of parameters as fragile, robust,\nor antifragile, based on their influence on policy performance in clean and\nadversarial settings. Parameter scores are defined to quantify these\ncharacteristics, and the framework is validated on PPO-trained agents in Mujoco\ncontinuous control environments. The results highlight the presence of\nantifragile parameters that enhance policy performance under stress,\ndemonstrating the potential of targeted filtering techniques to improve RL\npolicy adaptability. These insights provide a foundation for future\nadvancements in the design of robust and antifragile RL systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23036v1", "categories": ["cs.LG", "cs.SY", "eess.SY"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.23036v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23568", "title": "A Fast and Accurate 3-D Reconstruction Algorithm for Near-Range Microwave Imaging with Handheld Synthetic Aperture Radar", "authors": ["Lei Wang", "Xianxun Yao", "Tiancheng Song", "Guolin Sun"], "summary": "The design of image reconstruction algorithms for near-range handheld\nsynthetic aperture radar (SAR) systems has gained increasing popularity due to\nthe promising performance of portable millimeter-wave (MMW) imaging devices in\nvarious application fields. Time domain imaging algorithms including the\nbackprojection algorithm (BPA) and the Kirchhoff migration algorithm (KMA) are\nwidely adopted due to their direct applicability to arbitrary scan\ntrajectories. However, they suffer from time complexity issues that hinder\ntheir practical application. Wavenumber domain algorithms greatly improve the\ncomputational efficiency but most of them are restricted to specific array\ntopologies. Based on the factorization techniques as adopted in far-field\nsynthetic aperture radar imaging, the time domain fast factorized\nbackprojection algorithm for handheld synthetic aperture radar (HHFFBPA) is\nproposed. The local spectral properties of the radar images for handheld\nsystems are analyzed and analytical spectrum compression techniques are derived\nto realize efficient sampling of the subimages. Validated through numerical\nsimulations and experiments, HHFFBPA achieves fast and accurate 3-D imaging for\nhandheld synthetic aperture radar systems with arbitrary trajectories.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23568v1", "categories": ["eess.SP"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.23568v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23004", "title": "A Novel Frame Identification and Synchronization Technique for Smartphone Visible Light Communication Systems Based on Convolutional Neural Networks", "authors": ["Vaigai Nayaki Yokar", "Hoa Le-Minh", "Xicong Li", "Wai Lok Woo", "Luis Nero Alves", "Stanislav Zvanovec", "Tran The Son", "Zabih Ghassemlooy"], "summary": "This paper proposes a novel, robust, and lightweight supervised Convolutional\nNeural Network (CNN)-based technique for frame identification and\nsynchronization, designed to enhance short-link communication performance in a\nscreen-to-camera (S2C) based visible light communication (VLC) system.\nDeveloped using Python and the TensorFlow Keras framework, the proposed CNN\nmodel was trained through three real-time experimental investigations conducted\nin Jupyter Notebook. These experiments incorporated a dataset created from\nscratch to address various real-time challenges in S2C communication, including\nblurring, cropping, and rotated images in mobility scenarios. Overhead frames\nwere introduced for synchronization, which leads to enhanced system\nperformance. The experimental results demonstrate that the proposed model\nachieves an overall accuracy of approximately 98.74%, highlighting its\neffectiveness in identifying and synchronizing frames in S2C VLC systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23004v1", "categories": ["cs.CV", "eess.IV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23004v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.22995", "title": "A Reinforcement Learning Approach for Optimal Control in Microgrids", "authors": ["Davide Salaorni", "Federico Bianchi", "Francesco Trov√≤", "Marcello Restelli"], "summary": "The increasing integration of renewable energy sources (RESs) is transforming\ntraditional power grid networks, which require new approaches for managing\ndecentralized energy production and consumption. Microgrids (MGs) provide a\npromising solution by enabling localized control over energy generation,\nstorage, and distribution. This paper presents a novel reinforcement learning\n(RL)-based methodology for optimizing microgrid energy management.\nSpecifically, we propose an RL agent that learns optimal energy trading and\nstorage policies by leveraging historical data on energy production,\nconsumption, and market prices. A digital twin (DT) is used to simulate the\nenergy storage system dynamics, incorporating degradation factors to ensure a\nrealistic emulation of the analysed setting. Our approach is validated through\nan experimental campaign using real-world data from a power grid located in the\nItalian territory. The results indicate that the proposed RL-based strategy\noutperforms rule-based methods and existing RL benchmarks, offering a robust\nsolution for intelligent microgrid management.", "comment": "8 pages, accepted to International Joint Conference on Neural\n  Networks 2025", "pdf_url": "http://arxiv.org/pdf/2506.22995v1", "categories": ["cs.LG", "cs.SY", "eess.SY"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22995v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23926", "title": "Industrial brain: a human-like autonomous neuro-symbolic cognitive decision-making system", "authors": ["Junping Wang", "Bicheng Wang", "Yibo Xuea", "Yuan Xie"], "summary": "Resilience non-equilibrium measurement, the ability to maintain fundamental\nfunctionality amidst failures and errors, is crucial for scientific management\nand engineering applications of industrial chain. The problem is particularly\nchallenging when the number or types of multiple co-evolution of resilience\n(for example, randomly placed) are extremely chaos. Existing end-to-end deep\nlearning ordinarily do not generalize well to unseen full-feld reconstruction\nof spatiotemporal co-evolution structure, and predict resilience of network\ntopology, especially in multiple chaos data regimes typically seen in\nreal-world applications. To address this challenge, here we propose industrial\nbrain, a human-like autonomous cognitive decision-making and planning framework\nintegrating higher-order activity-driven neuro network and CT-OODA symbolic\nreasoning to autonomous plan resilience directly from observational data of\nglobal variable. The industrial brain not only understands and model structure\nof node activity dynamics and network co-evolution topology without simplifying\nassumptions, and reveal the underlying laws hidden behind complex networks, but\nalso enabling accurate resilience prediction, inference, and planning.\nExperimental results show that industrial brain significantly outperforms\nresilience prediction and planning methods, with an accurate improvement of up\nto 10.8\\% over GoT and OlaGPT framework and 11.03\\% over spectral dimension\nreduction. It also generalizes to unseen topologies and dynamics and maintains\nrobust performance despite observational disturbances. Our findings suggest\nthat industrial brain addresses an important gap in resilience prediction and\nplanning for industrial chain.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23926v1", "categories": ["cs.AI", "cs.LG"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.23926v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23146", "title": "Learning-to-Context Slope: Evaluating In-Context Learning Effectiveness Beyond Performance Illusions", "authors": ["Dingzriui Wang", "Xuanliang Zhang", "Keyan Xu", "Qingfu Zhu", "Wanxiang Che", "Yang Deng"], "summary": "In-context learning (ICL) has emerged as an effective approach to enhance the\nperformance of large language models (LLMs). However, its effectiveness varies\nsignificantly across models and tasks, posing challenges for practitioners to\ndetermine when ICL reliably improves performance. Current evaluation\napproaches, reliant on performance change after applying ICL, suffer from low\nreliability, poor attribution, and impracticality in data-insufficient\nscenarios. We propose the Learning-to-Context Slope (LCS), a novel metric that\nquantifies ICL effectiveness by modeling the slope between learning gain (loss\ndecrease from demonstrations) and contextual relevance (demonstration-input\nrelevance). LCS addresses key limitations of performance-based metrics: (1) it\ncaptures continuous loss changes even when outputs are incorrect, improving\nreliability; (2) its formulation attributes ICL failures to weak contextual\nalignment (inability to adapt inputs to demonstrations) or strong output\ncalibration (self-verification of correctness); and (3) it minimizes reliance\non labeled data via synthetic evaluation. Extensive experiments demonstrate\nthat LCS strongly correlates with performance improvements in labeled settings\nand reliably reflects true effectiveness in biased or data-scarce scenarios.\nFurther analysis reveals actionable thresholds for LCS and identifies model\ncapabilities critical to ICL success.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23146v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.23146v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23066", "title": "CoreMark: Toward Robust and Universal Text Watermarking Technique", "authors": ["Jiale Meng", "Yiming Li", "Zheming Lu", "Zewei He", "Hao Luo", "Tianwei Zhang"], "summary": "Text watermarking schemes have gained considerable attention in recent years,\nyet still face critical challenges in achieving simultaneous robustness,\ngeneralizability, and imperceptibility. This paper introduces a new embedding\nparadigm,termed CORE, which comprises several consecutively aligned black pixel\nsegments. Its key innovation lies in its inherent noise resistance during\ntransmission and broad applicability across languages and fonts. Based on the\nCORE, we present a text watermarking framework named CoreMark. Specifically,\nCoreMark first dynamically extracts COREs from characters. Then, the characters\nwith stronger robustness are selected according to the lengths of COREs. By\nmodifying the thickness of the CORE, the hidden data is embedded into the\nselected characters without causing significant visual distortions. Moreover, a\ngeneral plug-and-play embedding strength modulator is proposed, which can\nadaptively enhance the robustness for small font sizes by adjusting the\nembedding strength according to the font size. Experimental evaluation\nindicates that CoreMark demonstrates outstanding generalizability across\nmultiple languages and fonts. Compared to existing methods, CoreMark achieves\nsignificant improvements in resisting screenshot, print-scan, and print camera\nattacks, while maintaining satisfactory imperceptibility.", "comment": "10 pages, 16 figures", "pdf_url": "http://arxiv.org/pdf/2506.23066v1", "categories": ["cs.CV", "cs.CR", "cs.MM"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23066v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.24046", "title": "Exploring Accelerated Skill Acquisition via Tandem Training for Colonoscopy", "authors": ["Olivia Richards", "Keith L. Obstein", "Nabil Simaan"], "summary": "New endoscopists require a large volume of expert-proctored colonoscopies to\nattain minimal competency. Developing multi-fingered, synchronized control of a\ncolonoscope requires significant time and exposure to the device. Current\ntraining methods inhibit this development by relying on tool hand-off for\nexpert demonstrations. There is a need for colonoscopy training tools that\nenable in-hand expert guidance in real-time. We present a new concept of a\ntandem training system that uses a telemanipulated preceptor colonoscope to\nguide novice users as they perform a colonoscopy. This system is capable of\ndual-control and can automatically toggle between expert and novice control of\na standard colonoscope's angulation control wheels. Preliminary results from a\nuser study with novice and expert users show the effectiveness of this device\nas a skill acquisition tool. We believe that this device has the potential to\naccelerate skill acquisition for colonoscopy and, in the future, enable\nindividualized instruction and responsive teaching through bidirectional\nactuation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.24046v1", "categories": ["cs.RO", "cs.HC"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.24046v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23739", "title": "Validation of AI-Based 3D Human Pose Estimation in a Cyber-Physical Environment", "authors": ["Lisa Marie Otto", "Michael Kaiser", "Daniel Seebacher", "Steffen M√ºller"], "summary": "Ensuring safe and realistic interactions between automated driving systems\nand vulnerable road users (VRUs) in urban environments requires advanced\ntesting methodologies. This paper presents a test environment that combines a\nVehiclein-the-Loop (ViL) test bench with a motion laboratory, demonstrating the\nfeasibility of cyber-physical (CP) testing of vehicle-pedestrian and\nvehicle-cyclist interactions. Building upon previous work focused on pedestrian\nlocalization, we further validate a human pose estimation (HPE) approach\nthrough a comparative analysis of real-world (RW) and virtual representations\nof VRUs. The study examines the perception of full-body motion using a\ncommercial monocular camera-based 3Dskeletal detection AI. The virtual scene is\ngenerated in Unreal Engine 5, where VRUs are animated in real time and\nprojected onto a screen to stimulate the camera. The proposed stimulation\ntechnique ensures the correct perspective, enabling realistic vehicle\nperception. To assess the accuracy and consistency of HPE across RW and CP\ndomains, we analyze the reliability of detections as well as variations in\nmovement trajectories and joint estimation stability. The validation includes\ndynamic test scenarios where human avatars, both walking and cycling, are\nmonitored under controlled conditions. Our results show a strong alignment in\nHPE between RW and CP test conditions for stable motion patterns, while notable\ninaccuracies persist under dynamic movements and occlusions, particularly for\ncomplex cyclist postures. These findings contribute to refining CP testing\napproaches for evaluating next-generation AI-based vehicle perception and to\nenhancing interaction models of automated vehicles and VRUs in CP environments.", "comment": "6 pages, 5 figures, Preprint for 2025 IEEE IAVVC (International\n  Automated Vehicle Validation Conference)", "pdf_url": "http://arxiv.org/pdf/2506.23739v1", "categories": ["cs.RO", "cs.CE", "cs.HC"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.23739v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22814", "title": "Efficient Multi-Crop Saliency Partitioning for Automatic Image Cropping", "authors": ["Andrew Hamara", "Andrew C. Freeman"], "summary": "Automatic image cropping aims to extract the most visually salient regions\nwhile preserving essential composition elements. Traditional saliency-aware\ncropping methods optimize a single bounding box, making them ineffective for\napplications requiring multiple disjoint crops. In this work, we extend the\nFixed Aspect Ratio Cropping algorithm to efficiently extract multiple\nnon-overlapping crops in linear time. Our approach dynamically adjusts\nattention thresholds and removes selected crops from consideration without\nrecomputing the entire saliency map. We discuss qualitative results and\nintroduce the potential for future datasets and benchmarks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22814v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22814v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23201", "title": "External Data-Enhanced Meta-Representation for Adaptive Probabilistic Load Forecasting", "authors": ["Haoran Li", "Muhao Guo", "Marija Ilic", "Yang Weng", "Guangchun Ruan"], "summary": "Accurate residential load forecasting is critical for power system\nreliability with rising renewable integration and demand-side flexibility.\nHowever, most statistical and machine learning models treat external factors,\nsuch as weather, calendar effects, and pricing, as extra input, ignoring their\nheterogeneity, and thus limiting the extraction of useful external information.\nWe propose a paradigm shift: external data should serve as meta-knowledge to\ndynamically adapt the forecasting model itself. Based on this idea, we design a\nmeta-representation framework using hypernetworks that modulate selected\nparameters of a base Deep Learning (DL) model in response to external\nconditions. This provides both expressivity and adaptability. We further\nintegrate a Mixture-of-Experts (MoE) mechanism to enhance efficiency through\nselective expert activation, while improving robustness by filtering redundant\nexternal inputs. The resulting model, dubbed as a Meta Mixture of Experts for\nExternal data (M2oE2), achieves substantial improvements in accuracy and\nrobustness with limited additional overhead, outperforming existing\nstate-of-the-art methods in diverse load datasets. The dataset and source code\nare publicly available at\nhttps://github.com/haorandd/M2oE2\\_load\\_forecast.git.", "comment": "10 pages", "pdf_url": "http://arxiv.org/pdf/2506.23201v1", "categories": ["cs.LG", "cs.SY", "eess.SY"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.23201v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23621", "title": "Wireless Propagation Parameter Estimation with Convolutional Neural Networks", "authors": ["Steffen Schieler", "Sebastian Semper", "Reiner Thom√§"], "summary": "Wireless channel propagation parameter estimation forms the foundation of\nchannel sounding, estimation, modeling, and sensing. This paper introduces a\nDeep Learning approach for joint delay- and Doppler estimation from frequency\nand time samples of a radio channel transfer function.\n  Our work estimates the two-dimensional path parameters from a channel impulse\nresponse containing an unknown number of paths. Compared to existing deep\nlearning-based methods, the parameters are not estimated via classification but\nin a quasi-grid-free manner. We employ a deterministic preprocessing scheme\nthat incorporates a multi-channel windowing to increase the estimator's\nrobustness and enables the use of a CNN architecture. The proposed architecture\nthen jointly estimates the number of paths along with the respective delay and\nDoppler-shift parameters of the paths. Hence, it jointly solves the model order\nselection and parameter estimation task. We also integrate the CNN into an\nexisting maximum-likelihood estimator framework for efficient initialization of\na gradient-based iteration, to provide more accurate estimates.\n  In the analysis, we compare our approach to other methods in terms of\nestimate accuracy and model order error on synthetic data. Finally, we\ndemonstrate its applicability to real-world measurement data from a anechoic\nbi-static RADAR emulation measurement.", "comment": "This is the accepted version of the article published in the\n  International Journal of Microwave and Wireless Technologies with the DOI\n  10.1017/S1759078725000431", "pdf_url": "http://arxiv.org/pdf/2506.23621v1", "categories": ["eess.SP"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.23621v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23254", "title": "PixelBoost: Leveraging Brownian Motion for Realistic-Image Super-Resolution", "authors": ["Aradhana Mishra", "Bumshik Lee"], "summary": "Diffusion-model-based image super-resolution techniques often face a\ntrade-off between realistic image generation and computational efficiency. This\nissue is exacerbated when inference times by decreasing sampling steps,\nresulting in less realistic and hazy images. To overcome this challenge, we\nintroduce a novel diffusion model named PixelBoost that underscores the\nsignificance of embracing the stochastic nature of Brownian motion in advancing\nimage super-resolution, resulting in a high degree of realism, particularly\nfocusing on texture and edge definitions. By integrating controlled\nstochasticity into the training regimen, our proposed model avoids convergence\nto local optima, effectively capturing and reproducing the inherent uncertainty\nof image textures and patterns. Our proposed model demonstrates superior\nobjective results in terms of learned perceptual image patch similarity\n(LPIPS), lightness order error (LOE), peak signal-to-noise ratio(PSNR),\nstructural similarity index measure (SSIM), as well as visual quality. To\ndetermine the edge enhancement, we evaluated the gradient magnitude and pixel\nvalue, and our proposed model exhibited a better edge reconstruction\ncapability. Additionally, our model demonstrates adaptive learning capabilities\nby effectively adjusting to Brownian noise patterns and introduces a sigmoidal\nnoise sequencing method that simplifies training, resulting in faster inference\nspeeds.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23254v1", "categories": ["cs.CV", "cs.AI", "cs.MM", "eess.IV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23254v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23024", "title": "BWLer: Barycentric Weight Layer Elucidates a Precision-Conditioning Tradeoff for PINNs", "authors": ["Jerry Liu", "Yasa Baig", "Denise Hui Jean Lee", "Rajat Vadiraj Dwaraknath", "Atri Rudra", "Chris R√©"], "summary": "Physics-informed neural networks (PINNs) offer a flexible way to solve\npartial differential equations (PDEs) with machine learning, yet they still\nfall well short of the machine-precision accuracy many scientific tasks demand.\nIn this work, we investigate whether the precision ceiling comes from the\nill-conditioning of the PDEs or from the typical multi-layer perceptron (MLP)\narchitecture. We introduce the Barycentric Weight Layer (BWLer), which models\nthe PDE solution through barycentric polynomial interpolation. A BWLer can be\nadded on top of an existing MLP (a BWLer-hat) or replace it completely\n(explicit BWLer), cleanly separating how we represent the solution from how we\ntake derivatives for the PDE loss. Using BWLer, we identify fundamental\nprecision limitations within the MLP: on a simple 1-D interpolation task, even\nMLPs with O(1e5) parameters stall around 1e-8 RMSE -- about eight orders above\nfloat64 machine precision -- before any PDE terms are added. In PDE learning,\nadding a BWLer lifts this ceiling and exposes a tradeoff between achievable\naccuracy and the conditioning of the PDE loss. For linear PDEs we fully\ncharacterize this tradeoff with an explicit error decomposition and navigate it\nduring training with spectral derivatives and preconditioning. Across five\nbenchmark PDEs, adding a BWLer on top of an MLP improves RMSE by up to 30x for\nconvection, 10x for reaction, and 1800x for wave equations while remaining\ncompatible with first-order optimizers. Replacing the MLP entirely lets an\nexplicit BWLer reach near-machine-precision on convection, reaction, and wave\nproblems (up to 10 billion times better than prior results) and match the\nperformance of standard PINNs on stiff Burgers' and irregular-geometry Poisson\nproblems. Together, these findings point to a practical path for combining the\nflexibility of PINNs with the precision of classical spectral solvers.", "comment": "Workshop for the Theory of AI for Scientific Computing @ COLT 2025\n  (Best Paper). 39 pages, 24 figures", "pdf_url": "http://arxiv.org/pdf/2506.23024v1", "categories": ["cs.LG", "cs.AI", "cs.NA", "math.NA"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.23024v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23949", "title": "AI Risk-Management Standards Profile for General-Purpose AI (GPAI) and Foundation Models", "authors": ["Anthony M. Barrett", "Jessica Newman", "Brandie Nonnecke", "Nada Madkour", "Dan Hendrycks", "Evan R. Murphy", "Krystal Jackson", "Deepika Raman"], "summary": "Increasingly multi-purpose AI models, such as cutting-edge large language\nmodels or other 'general-purpose AI' (GPAI) models, 'foundation models,'\ngenerative AI models, and 'frontier models' (typically all referred to\nhereafter with the umbrella term 'GPAI/foundation models' except where greater\nspecificity is needed), can provide many beneficial capabilities but also risks\nof adverse events with profound consequences. This document provides\nrisk-management practices or controls for identifying, analyzing, and\nmitigating risks of GPAI/foundation models. We intend this document primarily\nfor developers of large-scale, state-of-the-art GPAI/foundation models; others\nthat can benefit from this guidance include downstream developers of end-use\napplications that build on a GPAI/foundation model. This document facilitates\nconformity with or use of leading AI risk management-related standards,\nadapting and building on the generic voluntary guidance in the NIST AI Risk\nManagement Framework and ISO/IEC 23894, with a focus on the unique issues faced\nby developers of GPAI/foundation models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23949v1", "categories": ["cs.AI", "cs.CR", "cs.CY"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.23949v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23149", "title": "V-SYNTHESIS: Task-Agnostic Synthesis of Consistent and Diverse In-Context Demonstrations from Scratch via V-Entropy", "authors": ["Dingzirui Wang", "Xuanliang Zhang", "Keyan Xu", "Qingfu Zhu", "Wanxiang Che", "Yang Deng"], "summary": "High labeling cost for in-context learning (ICL) demonstrations motivates\nusing large language models (LLMs) for synthesis to reduce overhead. However,\nexisting synthesis methods are mainly task-specific or rely on pre-existing\ndemonstrations. So this paper focuses on synthesizing demonstrations from\nscratch for arbitrary tasks. A major challenge in synthesizing from scratch is\nensuring consistency with the target task, as the lack of labeling guidance\ncould lead to synthesis bias. We first propose a consistency metric called\nV-Score, which has higher performance and lower computation cost compared with\nthe metrics based on grams or embedding vectors. Furthermore, we introduce\nV-Synthesis, which leverages V-Score for proportional sampling to ensure both\nhigh consistency and diversity of synthesized demonstrations. Experimental\nresults demonstrate that V-Synthesis yields an average performance improvement\nof 2.0% compared to existing synthesis methods confirming the effectiveness of\nV-Synthesis.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23149v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.23149v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23074", "title": "Learning Counterfactually Decoupled Attention for Open-World Model Attribution", "authors": ["Yu Zheng", "Boyang Gong", "Fanye Kong", "Yueqi Duan", "Bingyao Yu", "Wenzhao Zheng", "Lei Chen", "Jiwen Lu", "Jie Zhou"], "summary": "In this paper, we propose a Counterfactually Decoupled Attention Learning\n(CDAL) method for open-world model attribution. Existing methods rely on\nhandcrafted design of region partitioning or feature space, which could be\nconfounded by the spurious statistical correlations and struggle with novel\nattacks in open-world scenarios. To address this, CDAL explicitly models the\ncausal relationships between the attentional visual traces and source model\nattribution, and counterfactually decouples the discriminative model-specific\nartifacts from confounding source biases for comparison. In this way, the\nresulting causal effect provides a quantification on the quality of learned\nattention maps, thus encouraging the network to capture essential generation\npatterns that generalize to unseen source models by maximizing the effect.\nExtensive experiments on existing open-world model attribution benchmarks show\nthat with minimal computational overhead, our method consistently improves\nstate-of-the-art models by large margins, particularly for unseen novel\nattacks. Source code: https://github.com/yzheng97/CDAL.", "comment": "Accepted by ICCV 2025. Code: \\url{https://github.com/yzheng97/CDAL}", "pdf_url": "http://arxiv.org/pdf/2506.23074v1", "categories": ["cs.CV", "cs.CR", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23074v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "1906.00306", "title": "Programming Soft Robots with Flexible Mechanical Metamaterials", "authors": ["Ahmad Rafsanjani", "Katia Bertoldi", "Andr√© R. Studart"], "summary": "The complex behavior of highly deformable mechanical metamaterials can\nsubstantially enhance the performance of soft robots.", "comment": null, "pdf_url": "http://arxiv.org/pdf/1906.00306v1", "categories": ["cond-mat.soft", "cond-mat.mtrl-sci", "cs.RO"], "cate": "cond-mat.soft", "url": "http://arxiv.org/abs/1906.00306v1", "date": "2019-06-01", "updated": "2019-06-01"}
{"id": "2506.23774", "title": "Leveraging a Multi-Agent LLM-Based System to Educate Teachers in Hate Incidents Management", "authors": ["Ewelina Gajewska", "Michal Wawer", "Katarzyna Budzynska", "Jaros≈Çaw A. Chudziak"], "summary": "Computer-aided teacher training is a state-of-the-art method designed to\nenhance teachers' professional skills effectively while minimising concerns\nrelated to costs, time constraints, and geographical limitations. We\ninvestigate the potential of large language models (LLMs) in teacher education,\nusing a case of teaching hate incidents management in schools. To this end, we\ncreate a multi-agent LLM-based system that mimics realistic situations of hate,\nusing a combination of retrieval-augmented prompting and persona modelling. It\nis designed to identify and analyse hate speech patterns, predict potential\nescalation, and propose effective intervention strategies. By integrating\npersona modelling with agentic LLMs, we create contextually diverse simulations\nof hate incidents, mimicking real-life situations. The system allows teachers\nto analyse and understand the dynamics of hate incidents in a safe and\ncontrolled environment, providing valuable insights and practical knowledge to\nmanage such situations confidently in real life. Our pilot evaluation\ndemonstrates teachers' enhanced understanding of the nature of annotator\ndisagreements and the role of context in hate speech interpretation, leading to\nthe development of more informed and effective strategies for addressing hate\nin classroom settings.", "comment": "8 pages, 1 figure", "pdf_url": "http://arxiv.org/pdf/2506.23774v1", "categories": ["cs.CY", "cs.HC", "H.1.2"], "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.23774v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22817", "title": "Unleashing the Multi-View Fusion Potential: Noise Correction in VLM for Open-Vocabulary 3D Scene Understanding", "authors": ["Xingyilang Yin", "Jiale Wang", "Xi Yang", "Mutian Xu", "Xu Gu", "Nannan Wang"], "summary": "Recent open-vocabulary 3D scene understanding approaches mainly focus on\ntraining 3D networks through contrastive learning with point-text pairs or by\ndistilling 2D features into 3D models via point-pixel alignment. While these\nmethods show considerable performance in benchmarks with limited vocabularies,\nthey struggle to handle diverse object categories as the limited amount of 3D\ndata upbound training strong open-vocabulary 3d models. We observe that 2D\nmulti-view fusion methods take precedence in understanding diverse concepts in\n3D scenes. However, inherent noises in vision-language models lead multi-view\nfusion to sub-optimal performance. To this end, we introduce MVOV3D, a novel\napproach aimed at unleashing the potential of 2D multi-view fusion for\nopen-vocabulary 3D scene understanding. We focus on reducing the inherent\nnoises without training, thereby preserving the generalizability while\nenhancing open-world capabilities. Specifically, MVOV3D improves multi-view 2D\nfeatures by leveraging precise region-level image features and text features\nencoded by CLIP encoders and incorporates 3D geometric priors to optimize\nmulti-view fusion. Extensive experiments on various datasets demonstrate the\neffectiveness of our method. Notably, our MVOV3D achieves a new record with\n14.7% mIoU on ScanNet200 and 16.2% mIoU on Matterport160 for challenge\nopen-vocabulary semantic segmentation, outperforming current leading trained 3D\nnetworks by a significant margin.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22817v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22817v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23346", "title": "Safe and Performant Deployment of Autonomous Systems via Model Predictive Control and Hamilton-Jacobi Reachability Analysis", "authors": ["Hao Wang", "Armand Jordana", "Ludovic Righetti", "Somil Bansal"], "summary": "While we have made significant algorithmic developments to enable autonomous\nsystems to perform sophisticated tasks, it remains difficult for them to\nperform tasks effective and safely. Most existing approaches either fail to\nprovide any safety assurances or substantially compromise task performance for\nsafety. In this work, we develop a framework, based on model predictive control\n(MPC) and Hamilton-Jacobi (HJ) reachability, to optimize task performance for\nautonomous systems while respecting the safety constraints. Our framework\nguarantees recursive feasibility for the MPC controller, and it is scalable to\nhigh-dimensional systems. We demonstrate the effectiveness of our framework\nwith two simulation studies using a 4D Dubins Car and a 6 Dof Kuka iiwa\nmanipulator, and the experiments show that our framework significantly improves\nthe safety constraints satisfaction of the systems over the baselines.", "comment": "RSS 2025 Workshop on Reliable Robotics", "pdf_url": "http://arxiv.org/pdf/2506.23346v1", "categories": ["cs.RO", "cs.SY", "eess.SY"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.23346v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23750", "title": "Wideband Coverage Enhancement for IRS-Aided Wireless Networks Based on Power Measurement", "authors": ["Ge Yan", "Lipeng Zhu", "He Sun", "Rui Zhang"], "summary": "By applying tunable phase shifts to incident waves via passive signal\nreflection, intelligent reflecting surface (IRS) can offer significant\nperformance improvement for wireless communication systems. To reap such\nperformance gain, channel knowledge for IRS-cascaded links is generally\nrequired, which is practically challenging to acquire due to their\nhigh-dimensional and time-varying characteristics. Conventional pilot-based\nchannel estimation incurs excessive overhead due to the large number of\nreflecting elements, thus undermining the IRS efficiency, especially for\nwideband systems with frequency-selective fading channels. To tackle this\nissue, we propose in this letter a power-measurement-based channel\nautocorrelation matrix estimation and coverage enhancement approach for\nIRS-aided orthogonal frequency division multiplexing (OFDM) systems.\nSpecifically, by estimating equivalent channel autocorrelation matrices of\nIRS-cascaded OFDM channels based on receive signal power and optimizing the IRS\nreflection vector based on them, the average coverage performance in the\nIRS-aided region is enhanced without the need for frequent reconfiguration of\nIRS reflection coefficients based on user instantaneous channels. Simulation\nresults validate the effectiveness of the proposed approach for improving the\naverage channel gain over the coverage region.", "comment": "5 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2506.23750v1", "categories": ["eess.SP"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.23750v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23353", "title": "Layer Decomposition and Morphological Reconstruction for Task-Oriented Infrared Image Enhancement", "authors": ["Siyuan Chai", "Xiaodong Guo", "Tong Liu"], "summary": "Infrared image helps improve the perception capabilities of autonomous\ndriving in complex weather conditions such as fog, rain, and low light.\nHowever, infrared image often suffers from low contrast, especially in\nnon-heat-emitting targets like bicycles, which significantly affects the\nperformance of downstream high-level vision tasks. Furthermore, achieving\ncontrast enhancement without amplifying noise and losing important information\nremains a challenge. To address these challenges, we propose a task-oriented\ninfrared image enhancement method. Our approach consists of two key components:\nlayer decomposition and saliency information extraction. First, we design an\nlayer decomposition method for infrared images, which enhances scene details\nwhile preserving dark region features, providing more features for subsequent\nsaliency information extraction. Then, we propose a morphological\nreconstruction-based saliency extraction method that effectively extracts and\nenhances target information without amplifying noise. Our method improves the\nimage quality for object detection and semantic segmentation tasks. Extensive\nexperiments demonstrate that our approach outperforms state-of-the-art methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23353v1", "categories": ["cs.CV", "eess.IV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23353v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23025", "title": "Spectra 1.1: Scaling Laws and Efficient Inference for Ternary Language Models", "authors": ["Tejas Vaidhya", "Ayush Kaushal", "Vineet Jain", "Francis Couture Harpin", "Prashant Shishodia", "Majid Behbahani", "Yuriy Nevmyvaka", "Irina Rish"], "summary": "Large language models (LLMs) are increasingly used across research and\nindustry applications, yet their inference efficiency remains a significant\nchallenge. As the computational power of modern GPU architectures continuously\nimproves, their memory bandwidth and capacity have not scaled proportionally,\ncreating a critical bottleneck during inference. To address this, we\ninvestigate ternary language models (TriLMs) that employ quantization-aware\ntraining to significantly reduce memory requirements. We first analyze the\nscalability of TriLMs by conducting a scaling law analysis, revealing that\nTriLMs benefit more from increasing training data than from scaling model\nparameters. Based on this observation, we introduce Spectra-1.1, an open suite\nof TriLMs trained on up to 1.2 trillion tokens, demonstrating sustained\nperformance gains at scale. Furthermore, to improve inference efficiency, we\npropose novel 2-bit and 1.6-bit packing schemes for ternary weights, which\ndemonstrate accelerated inference across various CPU architectures. Also,\nbuilding on the 2-bit packing, we develop a GPU kernel called TriRun that\naccelerates end-to-end model inference by up to 5 times compared to\nfloating-point baselines. To encourage further exploration and development of\nTriLMs, we will release the Spectra-1.1 suite and TriRun inference kernels.\nOverall, our work lays the foundation for building and deploying efficient\nLLMs, providing a valuable resource for the research community.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23025v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.23025v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23992", "title": "Harnessing AI Agents to Advance Research on Refugee Child Mental Health", "authors": ["Aditya Shrivastava", "Komal Gupta", "Shraddha Arora"], "summary": "The international refugee crisis deepens, exposing millions of dis placed\nchildren to extreme psychological trauma. This research suggests a com pact,\nAI-based framework for processing unstructured refugee health data and\ndistilling knowledge on child mental health. We compare two Retrieval-Aug\nmented Generation (RAG) pipelines, Zephyr-7B-beta and DeepSeek R1-7B, to\ndetermine how well they process challenging humanitarian datasets while avoid\ning hallucination hazards. By combining cutting-edge AI methods with migration\nresearch and child psychology, this study presents a scalable strategy to\nassist policymakers, mental health practitioners, and humanitarian agencies to\nbetter assist displaced children and recognize their mental wellbeing. In\ntotal, both the models worked properly but significantly Deepseek R1 is\nsuperior to Zephyr with an accuracy of answer relevance 0.91", "comment": "14 page , 2 image , 2 tables , accepted under 5th International\n  Conference on Innovations in Computational Intelligence and Computer Vision\n  (ICICV-2025)", "pdf_url": "http://arxiv.org/pdf/2506.23992v1", "categories": ["cs.AI", "cs.ET"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.23992v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23192", "title": "RiverText: A Python Library for Training and Evaluating Incremental Word Embeddings from Text Data Streams", "authors": ["Gabriel Iturra-Bocaz", "Felipe Bravo-Marquez"], "summary": "Word embeddings have become essential components in various information\nretrieval and natural language processing tasks, such as ranking, document\nclassification, and question answering. However, despite their widespread use,\ntraditional word embedding models present a limitation in their static nature,\nwhich hampers their ability to adapt to the constantly evolving language\npatterns that emerge in sources such as social media and the web (e.g., new\nhashtags or brand names). To overcome this problem, incremental word embedding\nalgorithms are introduced, capable of dynamically updating word representations\nin response to new language patterns and processing continuous data streams.\n  This paper presents RiverText, a Python library for training and evaluating\nincremental word embeddings from text data streams. Our tool is a resource for\nthe information retrieval and natural language processing communities that work\nwith word embeddings in streaming scenarios, such as analyzing social media.\nThe library implements different incremental word embedding techniques, such as\nSkip-gram, Continuous Bag of Words, and Word Context Matrix, in a standardized\nframework. In addition, it uses PyTorch as its backend for neural network\ntraining. We have implemented a module that adapts existing intrinsic static\nword embedding evaluation tasks for word similarity and word categorization to\na streaming setting. Finally, we compare the implemented methods with different\nhyperparameter settings and discuss the results. Our open-source library is\navailable at https://github.com/dccuchile/rivertext.", "comment": "Accepted at SIGIR'23", "pdf_url": "http://arxiv.org/pdf/2506.23192v1", "categories": ["cs.CL", "cs.LG"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.23192v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23145", "title": "Forget-MI: Machine Unlearning for Forgetting Multimodal Information in Healthcare Settings", "authors": ["Shahad Hardan", "Darya Taratynova", "Abdelmajid Essofi", "Karthik Nandakumar", "Mohammad Yaqub"], "summary": "Privacy preservation in AI is crucial, especially in healthcare, where models\nrely on sensitive patient data. In the emerging field of machine unlearning,\nexisting methodologies struggle to remove patient data from trained multimodal\narchitectures, which are widely used in healthcare. We propose Forget-MI, a\nnovel machine unlearning method for multimodal medical data, by establishing\nloss functions and perturbation techniques. Our approach unlearns unimodal and\njoint representations of the data requested to be forgotten while preserving\nknowledge from the remaining data and maintaining comparable performance to the\noriginal model. We evaluate our results using performance on the forget\ndataset, performance on the test dataset, and Membership Inference Attack\n(MIA), which measures the attacker's ability to distinguish the forget dataset\nfrom the training dataset. Our model outperforms the existing approaches that\naim to reduce MIA and the performance on the forget dataset while keeping an\nequivalent performance on the test set. Specifically, our approach reduces MIA\nby 0.202 and decreases AUC and F1 scores on the forget set by 0.221 and 0.305,\nrespectively. Additionally, our performance on the test set matches that of the\nretrained model, while allowing forgetting. Code is available at\nhttps://github.com/BioMedIA-MBZUAI/Forget-MI.git", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23145v1", "categories": ["cs.LG", "cs.CR", "cs.CV"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.23145v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.22472", "title": "Optical Waveguide-based Spider Web Enables Resilient Impact Detection and Localization", "authors": ["Dylan Wilson", "Marco Pontin", "Peter Walters", "Perla Maiolino"], "summary": "Spiders use their webs as multifunctional tools that enable capturing and\nlocalizing prey and more general environmental sensing through vibrations.\nInspired by their biological function, we present a spider web-inspired optical\nwaveguide system for resilient impulse detection and localization. The\nstructure consists of six clear thermoplastic polyurethane (TPU) waveguides\narranged radially and interconnected by a spiral TPU thread, mimicking orb\nspider webs. Light transmission losses, induced by vibrations, are measured via\ncoupled LEDs and photo-diodes, allowing real-time detection. We systematically\ncharacterize individual waveguides, analyzing key parameters such as tension,\nimpulse position, and break angle to optimize vibrational response. The\ncomplete system is validated through controlled experiments, revealing a 5 ms\npropagation delay in vibration transfer between adjacent radii, enhancing\nlocalization capabilities. We demonstrate a robust impulse detection and\nlocalization algorithm leveraging time delay analysis, achieving reliable event\nidentification even in cases of sensor failure. This study highlights the\npotential of bioinspired optical waveguide structures for adaptive sensing,\nwith applications in soft robotics, structural monitoring, and environmental\nsensing.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22472v1", "categories": ["eess.SP", "cs.RO"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.22472v1", "date": "2025-06-20", "updated": "2025-06-20"}
{"id": "2506.23826", "title": "Towards the \"Digital Me\": A vision of authentic Conversational Agents powered by personal Human Digital Twins", "authors": ["Llu√≠s C. Coll", "Martin W. Lauer-Schmaltz", "Philip Cash", "John P. Hansen", "Anja Maier"], "summary": "Human Digital Twins (HDTs) have traditionally been conceptualized as\ndata-driven models designed to support decision-making across various domains.\nHowever, recent advancements in conversational AI open new possibilities for\nHDTs to function as authentic, interactive digital counterparts of individuals.\nThis paper introduces a novel HDT system architecture that integrates large\nlanguage models with dynamically updated personal data, enabling it to mirror\nan individual's conversational style, memories, and behaviors. To achieve this,\nour approach implements context-aware memory retrieval, neural\nplasticity-inspired consolidation, and adaptive learning mechanisms, creating a\nmore natural and evolving digital persona. The resulting system does not only\nreplicate an individual's unique conversational style depending on who they are\nspeaking with, but also enriches responses with dynamically captured personal\nexperiences, opinions, and memories. While this marks a significant step toward\ndeveloping authentic virtual counterparts, it also raises critical ethical\nconcerns regarding privacy, accountability, and the long-term implications of\npersistent digital identities. This study contributes to the field of HDTs by\ndescribing our novel system architecture, demonstrating its capabilities, and\ndiscussing future directions and emerging challenges to ensure the responsible\nand ethical development of HDTs.", "comment": "24 pages, 9 figures", "pdf_url": "http://arxiv.org/pdf/2506.23826v1", "categories": ["cs.ET", "cs.AI", "cs.CY", "cs.HC", "cs.IR"], "cate": "cs.ET", "url": "http://arxiv.org/abs/2506.23826v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22819", "title": "Prompting without Panic: Attribute-aware, Zero-shot, Test-Time Calibration", "authors": ["Ramya Hebbalaguppe", "Tamoghno Kandar", "Abhinav Nagpal", "Chetan Arora"], "summary": "Vision-language models (VLM) have demonstrated impressive performance in\nimage recognition by leveraging self-supervised training on large datasets.\nTheir performance can be further improved by adapting to the test sample using\ntest-time prompt tuning (TPT). Unfortunately, the singular focus of TPT\napproaches on improving the accuracy suffers from tunnel vision, and leads to\ndegradation in confidence calibration. This limits the applicability of TPT in\ncritical applications.\n  We make three contributions in this work. (1) We posit that random or naive\ninitialization of prompts leads to overfitting on a particular test sample, and\nis the main reason for miscalibration of the VLM after TPT. To mitigate the\nproblem, we propose careful initialization of test time prompt using prior\nknowledge about the target label attributes from a large language model (LLM);\n(2) To further maintain the quality of prompts during \\tpt, we propose a novel\nregularization loss to reduce intraclass distance, and increase inter-class\ndistance between the learnt\n  Through extensive experiments on different CLIP architectures and 15\ndatasets, we show that our approach can effectively improve the calibration\nafter TPT. We report an average expected calibration error (ECE) of 4.11 with\nour method, TCA, compared to 11.7 for vanilla TPT, 6.12 for C-TPT (ICLR'24),\n6.78 for DiffTPT (CVPR'23), and 8.43 for PromptAlign (NeurIPS'23). The code is\npublicly accessible at:\nhttps://github.com/rhebbalaguppe/TCA_PromptWithoutPanic.", "comment": "26 pages", "pdf_url": "http://arxiv.org/pdf/2506.22819v1", "categories": ["cs.CV", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22819v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23400", "title": "A Model Predictive Control Framework to Enhance Safety and Quality in Mobile Additive Manufacturing Systems", "authors": ["Yifei Li", "Joshua A. Robbins", "Guha Manogharan", "Herschel C. Pangborn", "Ilya Kovalenko"], "summary": "In recent years, the demand for customized, on-demand production has grown in\nthe manufacturing sector. Additive Manufacturing (AM) has emerged as a\npromising technology to enhance customization capabilities, enabling greater\nflexibility, reduced lead times, and more efficient material usage. However,\ntraditional AM systems remain constrained by static setups and human worker\ndependencies, resulting in long lead times and limited scalability. Mobile\nrobots can improve the flexibility of production systems by transporting\nproducts to designated locations in a dynamic environment. By integrating AM\nsystems with mobile robots, manufacturers can optimize travel time for\npreparatory tasks and distributed printing operations. Mobile AM robots have\nbeen deployed for on-site production of large-scale structures, but often\nneglect critical print quality metrics like surface roughness. Additionally,\nthese systems do not have the precision necessary for producing small,\nintricate components. We propose a model predictive control framework for a\nmobile AM platform that ensures safe navigation on the plant floor while\nmaintaining high print quality in a dynamic environment. Three case studies are\nused to test the feasibility and reliability of the proposed systems.", "comment": "2025 IEEE 21st International Conference on Automation Science and\n  Engineering", "pdf_url": "http://arxiv.org/pdf/2506.23400v1", "categories": ["cs.RO", "cs.SY", "eess.SY"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.23400v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23788", "title": "E-WAN: Efficient Communication in Energy Harvesting Low-Power Networks", "authors": ["Naomi Stricker", "David Blaser", "Andres Gomez", "Lothar Thiele"], "summary": "The ever-increasing number of distributed embedded systems in the context of\nthe Internet of Things (IoT), Wireless Sensor Networks (WSN), and\nCyber-Physical Systems (CPS) rely on wireless communication to collect and\nexchange data. Nodes can employ single-hop communication which, despite its\nease, may necessitate energy-intensive long-range communication to cover long\ndistances. Conversely, multi-hop communication allows for more energy-efficient\nshort-range communication since nodes can rely on other nodes to forward their\ndata. Yet, this approach requires relay nodes to be available and continuous\nmaintenance of a dynamically changing distributed state. At the same time,\nenergy harvesting has the potential to outperform traditional battery-based\nsystems by improving their lifetime, scalability with lower maintenance costs,\nand environmental impact. However, the limited and temporally and spatially\nvariable harvested energy poses significant challenges for networking in energy\nharvesting networks, particularly considering the energy demands and\ncharacteristics of both multi-hop and single-hop communication. We propose\nE-WAN, a protocol for energy harvesting wide-area low-power networks that\nbuilds on the concept of \\emph{virtual sub-networks} to enable\nresource-efficient multi-hop communication when possible and reliable however\nenergy-intensive point-to-point communication otherwise. Nodes autonomously and\ndynamically move between the two and adjust to changing network states and\nresources based only on easily obtainable network state information. We\nillustrate E-WAN's advantages both in terms of efficiency and adaptability in\nvarious communication and harvesting scenarios. Furthermore, we demonstrate\nE-WAN operating in a realistic setting by deploying an energy harvesting\nnetwork in a real-world indoor environment.", "comment": "This is the author's version of the work. Submitted to ACM TOSN on\n  June 2023. Major revision submitted on May 2024. Minor Revision submitted on\n  March 2025", "pdf_url": "http://arxiv.org/pdf/2506.23788v1", "categories": ["eess.SP", "cs.NI"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.23788v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23481", "title": "Evaluation of Geolocation Capabilities of Multimodal Large Language Models and Analysis of Associated Privacy Risks", "authors": ["Xian Zhang", "Xiang Cheng"], "summary": "Objectives: The rapid advancement of Multimodal Large Language Models (MLLMs)\nhas significantly enhanced their reasoning capabilities, enabling a wide range\nof intelligent applications. However, these advancements also raise critical\nconcerns regarding privacy and ethics. MLLMs are now capable of inferring the\ngeographic location of images -- such as those shared on social media or\ncaptured from street views -- based solely on visual content, thereby posing\nserious risks of privacy invasion, including doxxing, surveillance, and other\nsecurity threats.\n  Methods: This study provides a comprehensive analysis of existing geolocation\ntechniques based on MLLMs. It systematically reviews relevant litera-ture and\nevaluates the performance of state-of-the-art visual reasoning models on\ngeolocation tasks, particularly in identifying the origins of street view\nimagery.\n  Results: Empirical evaluation reveals that the most advanced visual large\nmodels can successfully localize the origin of street-level imagery with up to\n$49\\%$ accuracy within a 1-kilometer radius. This performance underscores the\nmodels' powerful capacity to extract and utilize fine-grained geographic cues\nfrom visual data.\n  Conclusions: Building on these findings, the study identifies key visual\nelements that contribute to suc-cessful geolocation, such as text,\narchitectural styles, and environmental features. Furthermore, it discusses the\npotential privacy implications associated with MLLM-enabled geolocation and\ndiscuss several technical and policy-based coun-termeasures to mitigate\nassociated risks. Our code and dataset are available at\nhttps://github.com/zxyl1003/MLLM-Geolocation-Evaluation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23481v1", "categories": ["cs.CV", "eess.IV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23481v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23033", "title": "Feature-Wise Mixing for Mitigating Contextual Bias in Predictive Supervised Learning", "authors": ["Yash Vardhan Tomar"], "summary": "Bias in predictive machine learning (ML) models is a fundamental challenge\ndue to the skewed or unfair outcomes produced by biased models. Existing\nmitigation strategies rely on either post-hoc corrections or rigid constraints.\nHowever, emerging research claims that these techniques can limit scalability\nand reduce generalizability. To address this, this paper introduces a\nfeature-wise mixing framework to mitigate contextual bias. This was done by\nredistributing feature representations across multiple contextual datasets. To\nassess feature-wise mixing's effectiveness, four ML classifiers were trained\nusing cross-validation and evaluated with bias-sensitive loss functions,\nincluding disparity metrics and mean squared error (MSE), which served as a\nstandard measure of predictive performance. The proposed method achieved an\naverage bias reduction of 43.35% and a statistically significant decrease in\nMSE across all classifiers trained on mixed datasets. Additionally,\nbenchmarking against established bias mitigation techniques found that\nfeature-wise mixing consistently outperformed SMOTE oversampling and\ndemonstrated competitive effectiveness without requiring explicit bias\nattribute identification. Feature-wise mixing efficiently avoids the\ncomputational overhead typically associated with fairness-aware learning\nalgorithms. Future work could explore applying feature-wise mixing for\nreal-world fields where accurate predictions are necessary.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23033v1", "categories": ["cs.LG", "stat.ML"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.23033v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.24026", "title": "Constructing Non-Markovian Decision Process via History Aggregator", "authors": ["Yongyi Wang", "Wenxin Li"], "summary": "In the domain of algorithmic decision-making, non-Markovian dynamics manifest\nas a significant impediment, especially for paradigms such as Reinforcement\nLearning (RL), thereby exerting far-reaching consequences on the advancement\nand effectiveness of the associated systems. Nevertheless, the existing\nbenchmarks are deficient in comprehensively assessing the capacity of decision\nalgorithms to handle non-Markovian dynamics. To address this deficiency, we\nhave devised a generalized methodology grounded in category theory. Notably, we\nestablished the category of Markov Decision Processes (MDP) and the category of\nnon-Markovian Decision Processes (NMDP), and proved the equivalence\nrelationship between them. This theoretical foundation provides a novel\nperspective for understanding and addressing non-Markovian dynamics. We further\nintroduced non-Markovianity into decision-making problem settings via the\nHistory Aggregator for State (HAS). With HAS, we can precisely control the\nstate dependency structure of decision-making problems in the time series. Our\nanalysis demonstrates the effectiveness of our method in representing a broad\nrange of non-Markovian dynamics. This approach facilitates a more rigorous and\nflexible evaluation of decision algorithms by testing them in problem settings\nwhere non-Markovian dynamics are explicitly constructed.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.24026v1", "categories": ["cs.AI"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.24026v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23235", "title": "Generalist Reward Models: Found Inside Large Language Models", "authors": ["Yi-Chen Li", "Tian Xu", "Yang Yu", "Xuqin Zhang", "Xiong-Hui Chen", "Zhongxiang Ling", "Ningjing Chao", "Lei Yuan", "Zhi-Hua Zhou"], "summary": "The alignment of Large Language Models (LLMs) is critically dependent on\nreward models trained on costly human preference data. While recent work\nexplores bypassing this cost with AI feedback, these methods often lack a\nrigorous theoretical foundation. In this paper, we discover that a powerful\ngeneralist reward model is already latently present within any LLM trained via\nstandard next-token prediction. We prove that this endogenous reward is not a\nheuristic, but is theoretically equivalent to a reward function learned through\noffline inverse reinforcement learning. This connection allows us to directly\nelicit a high-quality reward signal from a base (pre-trained or supervised\nfine-tuned) model without any further training. Critically, we also prove that\nsubsequent reinforcement learning using this endogenous reward leads to a\npolicy with a provably superior error bound compared to the base model. To our\nbest knowledge, this is the first theoretical proof of the effectiveness of\nreinforcement learning for LLMs. Our experiments validate this theory,\ndemonstrating that our method not only outperforms existing LLM-as-a-judge\napproaches but can also surpass explicitly trained reward models. These\nfindings suggest that the reward modeling stage can be replaced by a principled\nmethod of eliciting the knowledge already captured during pre-training,\nheralding a more efficient, powerful, and scalable paradigm for LLMs alignment\nas well as multi-modal models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23235v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.23235v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23321", "title": "AISCliteracy: Assessing Artificial Intelligence and Cybersecurity Literacy Levels and Learning Needs of Students", "authors": ["Devendra Chapagain", "Naresh Kshetri", "Bishwo Prakash Pokharel"], "summary": "Artificial intelligence (AI) is rapidly transforming global industries and\nsocieties, making AI literacy an indispensable skill for future generations.\nWhile AI integration in education is still emerging in Nepal, this study\nfocuses on assessing the current AI literacy levels and identifying learning\nneeds among students in Chitwan District of Nepal. By measuring students'\nunderstanding of AI and pinpointing areas for improvement, this research aims\nto provide actionable recommendations for educational stakeholders. Given the\npivotal role of young learners in navigating a rapidly evolving technological\nlandscape, fostering AI literacy is paramount. This study seeks to understand\nthe current state of AI literacy in Chitwan District by analyzing students'\nknowledge, skills, and attitudes towards AI. The results will contribute to\ndeveloping robust AI education programs for Nepalese schools. This paper offers\na contemporary perspective on AI's role in Nepalese secondary education,\nemphasizing the latest AI tools and technologies. Moreover, the study\nilluminates the potential revolutionary impact of technological innovations on\neducational leadership and student outcomes. A survey was conducted to\nconceptualize the newly emerging concept of AI and cybersecurity among students\nof Chitwan district from different schools and colleges to find the literacy\nrate. The participants in the survey were students between grade 9 to 12. We\nconclude with discussions of the affordances and barriers to bringing AI and\ncybersecurity education to students from lower classes.", "comment": "11 pages, 3 figures", "pdf_url": "http://arxiv.org/pdf/2506.23321v1", "categories": ["cs.CY", "cs.CR"], "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.23321v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.22477", "title": "Innovative Research on IoT Architecture and Robotic Operating Platforms: Applications of Large Language Models and Generative AI", "authors": ["Huiwen Han"], "summary": "This paper introduces an innovative design for robotic operating platforms,\nunderpinned by a transformative Internet of Things (IoT) architecture,\nseamlessly integrating cutting-edge technologies such as large language models\n(LLMs), generative AI, edge computing, and 5G networks. The proposed platform\naims to elevate the intelligence and autonomy of IoT systems and robotics,\nenabling them to make real-time decisions and adapt dynamically to changing\nenvironments. Through a series of compelling case studies across industries\nincluding smart manufacturing, healthcare, and service sectors, this paper\ndemonstrates the substantial potential of IoT-enabled robotics to optimize\noperational workflows, enhance productivity, and deliver innovative, scalable\nsolutions. By emphasizing the roles of LLMs and generative AI, the research\nhighlights how these technologies drive the evolution of intelligent robotics\nand IoT, shaping the future of industry-specific advancements. The findings not\nonly showcase the transformative power of these technologies but also offer a\nforward-looking perspective on their broader societal and industrial\nimplications, positioning them as catalysts for next-generation automation and\ntechnological convergence.", "comment": "Published in: 2024 6th International Conference on Robotics,\n  Intelligent Control and Artificial Intelligence (RICAI), IEEE Xplore, DOI:\n  10.1109/RICAI64321.2024.10911316. \\c{opyright} 2024 IEEE", "pdf_url": "http://arxiv.org/pdf/2506.22477v1", "categories": ["cs.NI", "cs.AI", "cs.ET", "cs.RO"], "cate": "cs.NI", "url": "http://arxiv.org/abs/2506.22477v1", "date": "2025-06-22", "updated": "2025-06-22"}
{"id": "2506.23851", "title": "Comparative Studies: Cloud-Enabled Adaptive Learning System for Scalable Education in Sub-Saharan", "authors": ["Israel Fianyi", "Soonja Yeom", "Ju-Hyun Shin"], "summary": "The integration of cloud computing in education can revolutionise learning in\nadvanced (Australia & South Korea) and middle-income (Ghana & Nigeria)\ncountries, while offering scalable, cost-effective and equitable access to\nadaptive learning systems. This paper explores how cloud computing and adaptive\nlearning technologies are deployed across different socio-economic and\ninfrastructure contexts. The study identifies enabling factors and systematic\nchallenges, providing insights into how cloud-based education can be tailored\nto bridge the digital and educational divide globally.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23851v1", "categories": ["cs.CY", "cs.ET", "cs.HC"], "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.23851v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22832", "title": "Listener-Rewarded Thinking in VLMs for Image Preferences", "authors": ["Alexander Gambashidze", "Li Pengyi", "Matvey Skripkin", "Andrey Galichin", "Anton Gusarov", "Konstantin Sobolev", "Andrey Kuznetsov", "Ivan Oseledets"], "summary": "Training robust and generalizable reward models for human visual preferences\nis essential for aligning text-to-image and text-to-video generative models\nwith human intent. However, current reward models often fail to generalize, and\nsupervised fine-tuning leads to memorization, demanding complex annotation\npipelines. While reinforcement learning (RL), specifically Group Relative\nPolicy Optimization (GRPO), improves generalization, we uncover a key failure\nmode: a significant drop in reasoning accuracy occurs when a model's reasoning\ntrace contradicts that of an independent, frozen vision-language model\n(\"listener\") evaluating the same output. To address this, we introduce a\nlistener-augmented GRPO framework. Here, the listener re-evaluates the\nreasoner's chain-of-thought to provide a dense, calibrated confidence score,\nshaping the RL reward signal. This encourages the reasoner not only to answer\ncorrectly, but to produce explanations that are persuasive to an independent\nmodel. Our listener-shaped reward scheme achieves best accuracy on the\nImageReward benchmark (67.4%), significantly improves out-of-distribution (OOD)\nperformance on a large-scale human preference dataset (1.2M votes, up to +6%\nover naive reasoner), and reduces reasoning contradictions compared to strong\nGRPO and SFT baselines. These results demonstrate that listener-based rewards\nprovide a scalable, data-efficient path to aligning vision-language models with\nnuanced human preferences. We will release our reasoning model here:\nhttps://huggingface.co/alexgambashidze/qwen2.5vl_image_preference_reasoner.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22832v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22832v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23569", "title": "Alleviating CoD in Renewable Energy Profile Clustering Using an Optical Quantum Computer", "authors": ["Chengjun Liu", "Yijun Xu", "Wei Gu", "Bo Sun", "Kai Wen", "Shuai Lu", "Lamine Mili"], "summary": "The traditional clustering problem of renewable energy profiles is typically\nformulated as a combinatorial optimization that suffers from the Curse of\nDimensionality (CoD) on classical computers. To address this issue, this paper\nfirst proposed a kernel-based quantum clustering method. More specifically, the\nkernel-based similarity between profiles with minimal intra-group distance is\nencoded into the ground-state of the Hamiltonian in the form of an Ising model.\nThen, this NP-hard problem can be reformulated into a Quadratic Unconstrained\nBinary Optimization (QUBO), which a Coherent Ising Machine (CIM) can naturally\nsolve with significant improvement over classical computers. The test results\nfrom a real optical quantum computer verify the validity of the proposed\nmethod. It also demonstrates its ability to address CoD in an NP-hard\nclustering problem.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23569v1", "categories": ["quant-ph", "cs.SY", "eess.SY"], "cate": "quant-ph", "url": "http://arxiv.org/abs/2506.23569v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23937", "title": "Optimized Frequency-Diverse Movable Antenna Arrays for Directional Secrecy in Wireless Systems", "authors": ["Chu Li", "Marjan Boloori", "Eduard Jorswieck", "Aydin Sezgin"], "summary": "Movable-antenna (MA) arrays are envisioned as a promising technique for\nenhancing secrecy performance in wireless communications by leveraging\nadditional spatial degrees of freedom. However, when the eavesdropper is\nlocated in the same direction as the legitimate user, particularly in\nmmWave/THz bands where line-of-sight (LOS) propagation dominates, the secrecy\nperformance of MA arrays becomes significantly limited, thus directionally\ninsecure. To address this challenge, we employ a joint design that combines an\nMA array with a frequency-diverse array (FDA) at the transmitter to secure the\ntransmission across both range and direction. Specifically, we derive\nclosed-form expressions for the optimal antenna positions and frequency shifts,\nassuming small perturbations in both parameters from a linear frequency-diverse\nMA configuration. Furthermore, we compare the worst-case secrecy rate under\nthis minor perturbation assumption with that obtained under a general\nconstraint, where simulated annealing is employed to numerically determine the\noptimal parameters. Simulation results confirm that the proposed optimized\nfrequency diverse MA approach significantly enhances secrecy performance in the\npresence of an eavesdropper aligned with the direction of the legitimate\nreceiver.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23937v1", "categories": ["eess.SP"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.23937v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23484", "title": "TAG-WM: Tamper-Aware Generative Image Watermarking via Diffusion Inversion Sensitivity", "authors": ["Yuzhuo Chen", "Zehua Ma", "Han Fang", "Weiming Zhang", "Nenghai Yu"], "summary": "AI-generated content (AIGC) enables efficient visual creation but raises\ncopyright and authenticity risks. As a common technique for integrity\nverification and source tracing, digital image watermarking is regarded as a\npotential solution to above issues. Among these, watermarking methods capable\nof preserving the generation quality are receiving increased attention.\nHowever, the proliferation and high performance of generative image editing\napplications have elevated the risks of malicious tampering, creating new\ndemands. 1) The tamper robustness of current lossless visual quality watermarks\nremains constrained by the modification-sensitive diffusion inversion process,\nnecessitating enhanced robustness. 2) The improved tampering quality and rapid\niteration cycles render passive tampering detection methods inadequate, making\nproactive tampering localization capability a desired feature for watermarks.\nTo address these requirements, this paper proposes a Tamper-Aware Generative\nimage WaterMarking method named TAG-WM. The proposed method comprises four key\nmodules: a dual-mark joint sampling (DMJS) algorithm for embedding copyright\nand localization watermarks into the latent space while preserving generative\nquality, the watermark latent reconstruction (WLR) utilizing reversed DMJS, a\ndense variation region detector (DVRD) leveraging diffusion inversion\nsensitivity to identify tampered areas via statistical deviation analysis, and\nthe tamper-aware decoding (TAD) guided by localization results. The\nexperimental results indicate that TAG-WM achieves SOTA tampering robustness\nand tampering localization capability with distortions while maintaining\nlossless generation quality and a considerable capacity of 256 bits.", "comment": "Accepted by ICCV 2025 (2025 IEEE/CVF International Conference on\n  Computer Vision)", "pdf_url": "http://arxiv.org/pdf/2506.23484v1", "categories": ["cs.MM", "cs.CV", "eess.IV", "I.3.3; I.4.9"], "cate": "cs.MM", "url": "http://arxiv.org/abs/2506.23484v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23036", "title": "Fragile, Robust, and Antifragile: A Perspective from Parameter Responses in Reinforcement Learning Under Stress", "authors": ["Zain ul Abdeen", "Ming Jin"], "summary": "This paper explores Reinforcement learning (RL) policy robustness by\nsystematically analyzing network parameters under internal and external\nstresses. Inspired by synaptic plasticity in neuroscience, synaptic filtering\nintroduces internal stress by selectively perturbing parameters, while\nadversarial attacks apply external stress through modified agent observations.\nThis dual approach enables the classification of parameters as fragile, robust,\nor antifragile, based on their influence on policy performance in clean and\nadversarial settings. Parameter scores are defined to quantify these\ncharacteristics, and the framework is validated on PPO-trained agents in Mujoco\ncontinuous control environments. The results highlight the presence of\nantifragile parameters that enhance policy performance under stress,\ndemonstrating the potential of targeted filtering techniques to improve RL\npolicy adaptability. These insights provide a foundation for future\nadvancements in the design of robust and antifragile RL systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23036v1", "categories": ["cs.LG", "cs.SY", "eess.SY"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.23036v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.24119", "title": "SPIRAL: Self-Play on Zero-Sum Games Incentivizes Reasoning via Multi-Agent Multi-Turn Reinforcement Learning", "authors": ["Bo Liu", "Leon Guertler", "Simon Yu", "Zichen Liu", "Penghui Qi", "Daniel Balcells", "Mickel Liu", "Cheston Tan", "Weiyan Shi", "Min Lin", "Wee Sun Lee", "Natasha Jaques"], "summary": "Recent advances in reinforcement learning have shown that language models can\ndevelop sophisticated reasoning through training on tasks with verifiable\nrewards, but these approaches depend on human-curated problem-answer pairs and\ndomain-specific reward engineering. We introduce SPIRAL, a self-play framework\nwhere models learn by playing multi-turn, zero-sum games against continuously\nimproving versions of themselves, eliminating the need for human supervision.\nThrough self-play, SPIRAL generates an infinite curriculum of progressively\nchallenging problems as models must constantly adapt to stronger opponents. To\nenable this self-play training at scale, We implement a fully online,\nmulti-turn, multi-agent reinforcement learning system for LLMs and propose\nrole-conditioned advantage estimation (RAE) to stabilize multi-agent training.\nUsing SPIRAL, self-play on zero-sum games produces reasoning capabilities that\ntransfer broadly. Training Qwen3-4B-Base on Kuhn Poker alone achieves 8.6%\nimprovement on math and 8.4% on general reasoning, outperforming SFT on 25,000\nexpert game trajectories. Analysis reveals that this transfer occurs through\nthree cognitive patterns: systematic decomposition, expected value calculation,\nand case-by-case analysis. Multi-game training (TicTacToe, Kuhn Poker, Simple\nNegotiation) further enhances performance as each game develops distinct\nreasoning strengths. Applying SPIRAL to a strong reasoning model\n(DeepSeek-R1-Distill-Qwen-7B) can still lead to 2.0% average improvement. These\nresults demonstrate that zero-sum games naturally develop transferable\nreasoning capabilities, highlighting a promising direction for autonomous\nreasoning development.", "comment": "Work in Progress", "pdf_url": "http://arxiv.org/pdf/2506.24119v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.24119v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23288", "title": "Two Spelling Normalization Approaches Based on Large Language Models", "authors": ["Miguel Domingo", "Francisco Casacuberta"], "summary": "The absence of standardized spelling conventions and the organic evolution of\nhuman language present an inherent linguistic challenge within historical\ndocuments, a longstanding concern for scholars in the humanities. Addressing\nthis issue, spelling normalization endeavors to align a document's orthography\nwith contemporary standards. In this study, we propose two new approaches based\non large language models: one of which has been trained without a supervised\ntraining, and a second one which has been trained for machine translation. Our\nevaluation spans multiple datasets encompassing diverse languages and\nhistorical periods, leading us to the conclusion that while both of them\nyielded encouraging results, statistical machine translation still seems to be\nthe most suitable technology for this task.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23288v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.23288v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23499", "title": "Unbounded knapsack problem and double partitions", "authors": ["Boris Y. Rubinstein"], "summary": "The unbounded knapsack problem can be considered as a particular case of the\ndouble partition problem that asks for a number of nonnegative integer\nsolutions to a system of two linear Diophantine equations with integer\ncoefficients. In the middle of 19th century Sylvester and Cayley suggested an\napproach based on the variable elimination allowing a reduction of a double\npartition to a sum of scalar partitions. This manuscript discusses a geometric\ninterpretation of this method and its application to the knapsack problem.", "comment": "6 pages, 1 figure", "pdf_url": "http://arxiv.org/pdf/2506.23499v1", "categories": ["math.NT", "cs.CR", "11P82"], "cate": "math.NT", "url": "http://arxiv.org/abs/2506.23499v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22604", "title": "Bootstrapping Human-Like Planning via LLMs", "authors": ["David Porfirio", "Vincent Hsiao", "Morgan Fine-Morris", "Leslie Smith", "Laura M. Hiatt"], "summary": "Robot end users increasingly require accessible means of specifying tasks for\nrobots to perform. Two common end-user programming paradigms include\ndrag-and-drop interfaces and natural language programming. Although natural\nlanguage interfaces harness an intuitive form of human communication,\ndrag-and-drop interfaces enable users to meticulously and precisely dictate the\nkey actions of the robot's task. In this paper, we investigate the degree to\nwhich both approaches can be combined. Specifically, we construct a large\nlanguage model (LLM)-based pipeline that accepts natural language as input and\nproduces human-like action sequences as output, specified at a level of\ngranularity that a human would produce. We then compare these generated action\nsequences to another dataset of hand-specified action sequences. Although our\nresults reveal that larger models tend to outperform smaller ones in the\nproduction of human-like action sequences, smaller models nonetheless achieve\nsatisfactory performance.", "comment": "Accepted by the 2025 34th IEEE International Conference on Robot and\n  Human Interactive Communication (RO-MAN)", "pdf_url": "http://arxiv.org/pdf/2506.22604v1", "categories": ["cs.AI", "cs.HC", "cs.RO"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.22604v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.24039", "title": "Foundation Models for Zero-Shot Segmentation of Scientific Images without AI-Ready Data", "authors": ["Shubhabrata Mukherjee", "Jack Lang", "Obeen Kwon", "Iryna Zenyuk", "Valerie Brogden", "Adam Weber", "Daniela Ushizima"], "summary": "Zero-shot and prompt-based technologies capitalized on using frequently\noccurring images to transform visual reasoning tasks, which explains why such\ntechnologies struggle with valuable yet scarce scientific image sets. In this\nwork, we propose Zenesis, a comprehensive no-code interactive platform designed\nto minimize barriers posed by data readiness for scientific images. We develop\nlightweight multi-modal adaptation techniques that enable zero-shot operation\non raw scientific data, along with human-in-the-loop refinement and\nheuristic-based temporal enhancement options. We demonstrate the performance of\nour approach through comprehensive comparison and validation on challenging\nFocused Ion Beam Scanning Electron Microscopy (FIB-SEM) data of catalyst-loaded\nmembranes. Zenesis significantly outperforms baseline methods, achieving an\naverage accuracy of 0.947, an Intersection over Union (IOU) of 0.858, and a\nDice score of 0.923 for amorphous catalyst samples and accuracy of 0.987, an\nIOU of 0.857, and a Dice score of 0.923 for crystalline samples. These results\nmark a substantial improvement over traditional methods like Otsu thresholding\nand even advanced models like Segment Anything Model (SAM) when used in\nisolation. Our results demonstrate that Zenesis is a powerful tool for\nscientific applications, particularly in fields where high-quality annotated\ndatasets are unavailable, accelerating accurate analysis of experimental\nimaging.", "comment": "This manuscript is a draft on arxiv. A final version has been\n  submitted to the 59th ICPP 2025, DRAI workshop", "pdf_url": "http://arxiv.org/pdf/2506.24039v1", "categories": ["cs.CV", "cs.HC"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.24039v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22833", "title": "SemFaceEdit: Semantic Face Editing on Generative Radiance Manifolds", "authors": ["Shashikant Verma", "Shanmuganathan Raman"], "summary": "Despite multiple view consistency offered by 3D-aware GAN techniques, the\nresulting images often lack the capacity for localized editing. In response,\ngenerative radiance manifolds emerge as an efficient approach for constrained\npoint sampling within volumes, effectively reducing computational demands and\nenabling the learning of fine details. This work introduces SemFaceEdit, a\nnovel method that streamlines the appearance and geometric editing process by\ngenerating semantic fields on generative radiance manifolds. Utilizing latent\ncodes, our method effectively disentangles the geometry and appearance\nassociated with different facial semantics within the generated image. In\ncontrast to existing methods that can change the appearance of the entire\nradiance field, our method enables the precise editing of particular facial\nsemantics while preserving the integrity of other regions. Our network\ncomprises two key modules: the Geometry module, which generates semantic\nradiance and occupancy fields, and the Appearance module, which is responsible\nfor predicting RGB radiance. We jointly train both modules in adversarial\nsettings to learn semantic-aware geometry and appearance descriptors. The\nappearance descriptors are then conditioned on their respective semantic latent\ncodes by the Appearance Module, facilitating disentanglement and enhanced\ncontrol. Our experiments highlight SemFaceEdit's superior performance in\nsemantic field-based editing, particularly in achieving improved radiance field\ndisentanglement.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22833v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22833v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23624", "title": "Towards Universal Shared Control in Teleoperation Without Haptic Feedback", "authors": ["Max Grobbel", "Tristan Schneider", "S√∂ren Hohmann"], "summary": "Teleoperation with non-haptic VR controllers deprives human operators of\ncritical motion feedback. We address this by embedding a multi-objective\noptimization problem that converts user input into collision-free UR5e joint\ntrajectories while actively suppressing liquid slosh in a glass. The controller\nmaintains 13 ms average planning latency, confirming real-time performance and\nmotivating the augmentation of this teleoperation approach to further\nobjectives.", "comment": "5 pages, submitted to IEEE Telepresence 2025 conference", "pdf_url": "http://arxiv.org/pdf/2506.23624v1", "categories": ["cs.RO", "cs.SY", "eess.SY"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.23624v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23966", "title": "Pinching-Antenna Systems with In-Waveguide Attenuation: Performance Analysis and Algorithm Design", "authors": ["Yanqing Xu", "Zhiguo Ding", "Robert Schober", "Tsung-Hui Chang"], "summary": "Pinching-antenna systems have emerged as a promising flexible-antenna\narchitecture for next-generation wireless networks, enabling enhanced\nadaptability and user-centric connectivity through antenna repositioning along\nwaveguides. However, existing studies often overlook in-waveguide signal\nattenuation and in the literature, there is no comprehensive analysis on\nwhether and under what conditions such an assumption is justified. This paper\naddresses this gap by explicitly incorporating in-waveguide attenuation into\nboth the system model and algorithm design, and studying its impact on the\ndownlink user data rates. We begin with a single-user scenario and derive a\nclosed-form expression for the globally optimal antenna placement, which\nreveals how the attenuation coefficient and the user-to-waveguide distance\njointly affect the optimal antenna position. Based on this analytical solution,\nwe further provide a theoretical analysis identifying the system conditions\nunder which the in-waveguide attenuation has an insignificant impact on the\nuser achievable rate. The study is then extended to the multi-user\nmultiple-input multiple-output setting, where two efficient algorithms are\ndeveloped, based on the weighted minimum mean square error method and the\nmaximum ratio combining method, to jointly optimize beamforming and antenna\nplacement. Simulation results validate the efficacy of the proposed algorithms\nand demonstrate that pinching-antenna systems substantially outperform\nconventional fixed-antenna baselines, underscoring their potential for future\nflexible wireless communications.", "comment": "This paper aims to address a fundamental question in pinching-antenna\n  systems: Can in-waveguide attenuation be safely ignored without causing\n  significant performance degradation? Our analytical results provide a clear\n  answer -- YES, provided that certain mild and practically realizable\n  conditions on the system parameters are satisfied", "pdf_url": "http://arxiv.org/pdf/2506.23966v1", "categories": ["eess.SP", "cs.IT", "math.IT"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.23966v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.24092", "title": "WaRA: Wavelet Low Rank Adaptation", "authors": ["Moein Heidari", "Yasamin Medghalchi", "Mahdi Khoursha", "Reza Rezaeian", "Ilker Hacihaliloglu"], "summary": "Parameter-efficient fine-tuning (PEFT) has gained widespread adoption across\nvarious applications. Among PEFT techniques, Low-Rank Adaptation (LoRA) and its\nextensions have emerged as particularly effective, allowing efficient model\nadaptation while significantly reducing computational overhead. However,\nexisting approaches typically rely on global low-rank factorizations, which\noverlook local or multi-scale structure, failing to capture complex patterns in\nthe weight updates. To address this, we propose WaRA, a novel PEFT method that\nleverages wavelet transforms to decompose the weight update matrix into a\nmulti-resolution representation. By performing low-rank factorization in the\nwavelet domain and reconstructing updates through an inverse transform, WaRA\nobtains compressed adaptation parameters that harness multi-resolution\nanalysis, enabling it to capture both coarse and fine-grained features while\nproviding greater flexibility and sparser representations than standard LoRA.\nThrough comprehensive experiments and analysis, we demonstrate that WaRA\nperforms superior on diverse vision tasks, including image generation,\nclassification, and semantic segmentation, significantly enhancing generated\nimage quality while reducing computational complexity. Although WaRA was\nprimarily designed for vision tasks, we further showcase its effectiveness in\nlanguage tasks, highlighting its broader applicability and generalizability.\nThe code is publicly available at\n\\href{GitHub}{https://github.com/moeinheidari7829/WaRA}.", "comment": "Submitted to BMVC 2025", "pdf_url": "http://arxiv.org/pdf/2506.24092v1", "categories": ["cs.CV", "eess.IV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.24092v1", "date": "2025-06-25", "updated": "2025-06-25"}
{"id": "2506.23041", "title": "ReMem: Mutual Information-Aware Fine-tuning of Pretrained Vision Transformers for Effective Knowledge Distillation", "authors": ["Chengyu Dong", "Huan Gui", "Noveen Sachdeva", "Long Jin", "Ke Yin", "Jingbo Shang", "Lichan Hong", "Ed H. Chi", "Zhe Zhao"], "summary": "Knowledge distillation from pretrained visual representation models offers an\neffective approach to improve small, task-specific production models. However,\nthe effectiveness of such knowledge transfer drops significantly when\ndistilling from strong models that are pretrained in a large scale. In this\npaper, we address this challenge for pretrained Vision Transformers (ViTs) by\nexploring methods to fine-tune them for more effective knowledge transfer.\nMotivated by the connection between mutual information and distillation\neffectiveness, we propose to employ mutual information-aware optimization\nduring finetuning. For small or highly-imbalanced downstream datasets where\nsuch optimization becomes less effective, we introduce a simple yet effective\nheuristic of reweighting MLP blocks. This approach is inspired by our\nobservation that top MLP blocks are primarily responsible for mutual\ninformation loss. Our method enables small student models to benefit from those\npretrained models among the strongest.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23041v1", "categories": ["cs.LG", "cs.CV"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.23041v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "1610.09431", "title": "Attention acts to suppress goal-based conflict under high competition", "authors": ["Omar Claflin"], "summary": "It is known that when multiple stimuli are present, top-down attention\nselectively enhances the neural signal in the visual cortex for task-relevant\nstimuli, but this has been tested only under conditions of minimal competition\nof visual attention. Here we show during high competition, that is, two stimuli\nin a shared receptive field possessing opposing modulatory goals, top-down\nattention suppresses both task-relevant and irrelevant neural signals within\n100 ms of stimuli onset. This non-selective engagement of top-down attentional\nresources serves to reduce the feedforward signal representing irrelevant\nstimuli.", "comment": "25 pages, 3 figures, 3 tables", "pdf_url": "http://arxiv.org/pdf/1610.09431v1", "categories": ["q-bio.NC", "cs.AI"], "cate": "q-bio.NC", "url": "http://arxiv.org/abs/1610.09431v1", "date": "2016-10-29", "updated": "2016-10-29"}
{"id": "2506.23293", "title": "Objective-Free Local Learning and Emergent Language Structure in Thinking Machines", "authors": ["P. Myles Eugenio"], "summary": "We present a neuro-symbolic framework for generative language modeling based\non local, event-driven emergent learning. At its core is a hierarchical\nHopfield memory chain acting as a compositional short-term memory and dynamic\ntokenizer (retokenizer). Rather than relying on predefined tokens or\nsupervision, the model builds structure from scratch, learning symbol sequences\nas multi-scale representations. It constructs projection tensors that bind\nco-occurring features into hierarchical tokens, introducing redundancy (i.e an\nemergent gauge structure) and enabling compression of local activations into\nlong-range dependencies. Curiously, we find that the retokenizer can filter\nnatural language patterns from noise, generating synthetic languages with\ncoherent internal morphology -- quantifiably the same as human language.\nLanguage is learned in a local (Hebbian) fashion, where model constraints\ndictate allowed emergent structure, and new information is retained in\nalignment with this structure. The absence of a global objective enables a form\nof plasticity not found in conventional language models, allowing the system to\ngeneralize beyond its initial inference class -- even without explicit data. We\ndemonstrate that briefly activating a new neuron during inference binds\ndistributed multi-scale token features into a symbolic embedding. These\nemergent embedding neurons act as long-term memory and support a key-value\nmechanism for compositional inference and generalization. This architecture\nprovides a methodological foundation for studying how symbolic structure can\nemerge from local neural learning. It offers a new pathway for building\nscalable, interpretable neuro-symbolic systems -- where tokens, grammar, and\nreasoning arise as compressed memory traces within a Hopfield hierarchy. This\napproach advances the development of neuromorphic architectures for generative\nlanguage models.", "comment": "22 pages, 7 figures", "pdf_url": "http://arxiv.org/pdf/2506.23293v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "q-bio.NC"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.23293v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23644", "title": "QLPro: Automated Code Vulnerability Discovery via LLM and Static Code Analysis Integration", "authors": ["Junze Hu", "Xiangyu Jin", "Yizhe Zeng", "Yuling Liu", "Yunpeng Li", "Dan Du", "Kaiyu Xie", "Hongsong Zhu"], "summary": "We introduce QLPro, a vulnerability detection framework that systematically\nintegrates LLMs and static analysis tools to enable comprehensive vulnerability\ndetection across entire open-source projects.We constructed a new dataset,\nJavaTest, comprising 10 open-source projects from GitHub with 62 confirmed\nvulnerabilities. CodeQL, a state-of-the-art static analysis tool, detected only\n24 of these vulnerabilities while QLPro detected 41. Furthermore, QLPro\ndiscovered 6 previously unknown vulnerabilities, 2 of which have been confirmed\nas 0-days.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23644v1", "categories": ["cs.SE", "cs.AI", "cs.CR"], "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.23644v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22756", "title": "RoboPearls: Editable Video Simulation for Robot Manipulation", "authors": ["Tao Tang", "Likui Zhang", "Youpeng Wen", "Kaidong Zhang", "Jia-Wang Bian", "xia zhou", "Tianyi Yan", "Kun Zhan", "Peng Jia", "Hefeng Wu", "Liang Lin", "Xiaodan Liang"], "summary": "The development of generalist robot manipulation policies has seen\nsignificant progress, driven by large-scale demonstration data across diverse\nenvironments. However, the high cost and inefficiency of collecting real-world\ndemonstrations hinder the scalability of data acquisition. While existing\nsimulation platforms enable controlled environments for robotic learning, the\nchallenge of bridging the sim-to-real gap remains. To address these challenges,\nwe propose RoboPearls, an editable video simulation framework for robotic\nmanipulation. Built on 3D Gaussian Splatting (3DGS), RoboPearls enables the\nconstruction of photo-realistic, view-consistent simulations from demonstration\nvideos, and supports a wide range of simulation operators, including various\nobject manipulations, powered by advanced modules like Incremental Semantic\nDistillation (ISD) and 3D regularized NNFM Loss (3D-NNFM). Moreover, by\nincorporating large language models (LLMs), RoboPearls automates the simulation\nproduction process in a user-friendly manner through flexible command\ninterpretation and execution. Furthermore, RoboPearls employs a vision-language\nmodel (VLM) to analyze robotic learning issues to close the simulation loop for\nperformance enhancement. To demonstrate the effectiveness of RoboPearls, we\nconduct extensive experiments on multiple datasets and scenes, including\nRLBench, COLOSSEUM, Ego4D, Open X-Embodiment, and a real-world robot, which\ndemonstrate our satisfactory simulation performance.", "comment": "ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2506.22756v1", "categories": ["cs.CV", "cs.RO"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22756v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.24046", "title": "Exploring Accelerated Skill Acquisition via Tandem Training for Colonoscopy", "authors": ["Olivia Richards", "Keith L. Obstein", "Nabil Simaan"], "summary": "New endoscopists require a large volume of expert-proctored colonoscopies to\nattain minimal competency. Developing multi-fingered, synchronized control of a\ncolonoscope requires significant time and exposure to the device. Current\ntraining methods inhibit this development by relying on tool hand-off for\nexpert demonstrations. There is a need for colonoscopy training tools that\nenable in-hand expert guidance in real-time. We present a new concept of a\ntandem training system that uses a telemanipulated preceptor colonoscope to\nguide novice users as they perform a colonoscopy. This system is capable of\ndual-control and can automatically toggle between expert and novice control of\na standard colonoscope's angulation control wheels. Preliminary results from a\nuser study with novice and expert users show the effectiveness of this device\nas a skill acquisition tool. We believe that this device has the potential to\naccelerate skill acquisition for colonoscopy and, in the future, enable\nindividualized instruction and responsive teaching through bidirectional\nactuation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.24046v1", "categories": ["cs.RO", "cs.HC"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.24046v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22836", "title": "FOCUS: Fine-grained Optimization with Semantic Guided Understanding for Pedestrian Attributes Recognition", "authors": ["Hongyan An", "Kuan Zhu", "Xin He", "Haiyun Guo", "Chaoyang Zhao", "Ming Tang", "Jinqiao Wang"], "summary": "Pedestrian attribute recognition (PAR) is a fundamental perception task in\nintelligent transportation and security. To tackle this fine-grained task, most\nexisting methods focus on extracting regional features to enrich attribute\ninformation. However, a regional feature is typically used to predict a fixed\nset of pre-defined attributes in these methods, which limits the performance\nand practicality in two aspects: 1) Regional features may compromise\nfine-grained patterns unique to certain attributes in favor of capturing common\ncharacteristics shared across attributes. 2) Regional features cannot\ngeneralize to predict unseen attributes in the test time. In this paper, we\npropose the \\textbf{F}ine-grained \\textbf{O}ptimization with semanti\\textbf{C}\ng\\textbf{U}ided under\\textbf{S}tanding (FOCUS) approach for PAR, which\nadaptively extracts fine-grained attribute-level features for each attribute\nindividually, regardless of whether the attributes are seen or not during\ntraining. Specifically, we propose the Multi-Granularity Mix Tokens (MGMT) to\ncapture latent features at varying levels of visual granularity, thereby\nenriching the diversity of the extracted information. Next, we introduce the\nAttribute-guided Visual Feature Extraction (AVFE) module, which leverages\ntextual attributes as queries to retrieve their corresponding visual attribute\nfeatures from the Mix Tokens using a cross-attention mechanism. To ensure that\ntextual attributes focus on the appropriate Mix Tokens, we further incorporate\na Region-Aware Contrastive Learning (RACL) method, encouraging attributes\nwithin the same region to share consistent attention maps. Extensive\nexperiments on PA100K, PETA, and RAPv1 datasets demonstrate the effectiveness\nand strong generalization ability of our method.", "comment": "ICME 2025 Oral", "pdf_url": "http://arxiv.org/pdf/2506.22836v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22836v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23781", "title": "Data-Driven Predictive Planning and Control for Aerial 3D Inspection with Back-face Elimination", "authors": ["Savvas Papaioannou", "Panayiotis Kolios", "Christos G. Panayiotou", "Marios M. Polycarpou"], "summary": "Automated inspection with Unmanned Aerial Systems (UASs) is a transformative\ncapability set to revolutionize various application domains. However, this task\nis inherently complex, as it demands the seamless integration of perception,\nplanning, and control which existing approaches often treat separately.\nMoreover, it requires accurate long-horizon planning to predict action\nsequences, in contrast to many current techniques, which tend to be myopic. To\novercome these limitations, we propose a 3D inspection approach that unifies\nperception, planning, and control within a single data-driven predictive\ncontrol framework. Unlike traditional methods that rely on known UAS dynamic\nmodels, our approach requires only input-output data, making it easily\napplicable to off-the-shelf black-box UASs. Our method incorporates back-face\nelimination, a visibility determination technique from 3D computer graphics,\ndirectly into the control loop, thereby enabling the online generation of\naccurate, long-horizon 3D inspection trajectories.", "comment": "2025 European Control Conference (ECC), Thessaloniki, Greece, 24-27\n  June 2025", "pdf_url": "http://arxiv.org/pdf/2506.23781v1", "categories": ["cs.RO", "cs.SY", "eess.SY"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.23781v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.24024", "title": "Post-processing of EEG-based Auditory Attention Decoding Decisions via Hidden Markov Models", "authors": ["Nicolas Heintz", "Tom Francart", "Alexander Bertrand"], "summary": "Auditory attention decoding (AAD) algorithms exploit brain signals, such as\nelectroencephalography (EEG), to identify which speaker a listener is focusing\non in a multi-speaker environment. While state-of-the-art AAD algorithms can\nidentify the attended speaker on short time windows, their predictions are\noften too inaccurate for practical use. In this work, we propose augmenting AAD\nwith a hidden Markov model (HMM) that models the temporal structure of\nattention. More specifically, the HMM relies on the fact that a subject is much\nless likely to switch attention than to keep attending the same speaker at any\nmoment in time. We show how a HMM can significantly improve existing AAD\nalgorithms in both causal (real-time) and non-causal (offline) settings. We\nfurther demonstrate that HMMs outperform existing postprocessing approaches in\nboth accuracy and responsiveness, and explore how various factors such as\nwindow length, switching frequency, and AAD accuracy influence overall\nperformance. The proposed method is computationally efficient, intuitive to use\nand applicable in both real-time and offline settings.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.24024v1", "categories": ["eess.SP", "cs.LG"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.24024v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23053", "title": "Double-Diffusion: Diffusion Conditioned Diffusion Probabilistic Model For Air Quality Prediction", "authors": ["Hanlin Dong", "Arian Prabowo", "Hao Xue", "Flora D. Salim"], "summary": "Air quality prediction is a challenging forecasting task due to its\nspatio-temporal complexity and the inherent dynamics as well as uncertainty.\nMost of the current models handle these two challenges by applying Graph Neural\nNetworks or known physics principles, and quantifying stochasticity through\nprobabilistic networks like Diffusion models. Nevertheless, finding the right\nbalancing point between the certainties and uncertainties remains an open\nquestion. Therefore, we propose Double-Diffusion, a novel diffusion\nprobabilistic model that harnesses the power of known physics to guide air\nquality forecasting with stochasticity. To the best of our knowledge, while\nprecedents have been made of using conditional diffusion models to predict air\npollution, this is the first attempt to use physics as a conditional generative\napproach for air quality prediction. Along with a sampling strategy adopted\nfrom image restoration and a new denoiser architecture, Double-Diffusion ranks\nfirst in most evaluation scenarios across two real-life datasets compared with\nother probabilistic models, it also cuts inference time by 50% to 30% while\nenjoying an increase between 3-12% in Continuous Ranked Probabilistic Score\n(CRPS).", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23053v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.23053v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2402.09146", "title": "ResQuNNs:Towards Enabling Deep Learning in Quantum Convolution Neural Networks", "authors": ["Muhammad Kashif", "Muhammad Shafique"], "summary": "In this paper, we present a novel framework for enhancing the performance of\nQuanvolutional Neural Networks (QuNNs) by introducing trainable quanvolutional\nlayers and addressing the critical challenges associated with them. Traditional\nquanvolutional layers, although beneficial for feature extraction, have largely\nbeen static, offering limited adaptability. Unlike state-of-the-art, our\nresearch overcomes this limitation by enabling training within these layers,\nsignificantly increasing the flexibility and potential of QuNNs. However, the\nintroduction of multiple trainable quanvolutional layers induces complexities\nin gradient-based optimization, primarily due to the difficulty in accessing\ngradients across these layers. To resolve this, we propose a novel\narchitecture, Residual Quanvolutional Neural Networks (ResQuNNs), leveraging\nthe concept of residual learning, which facilitates the flow of gradients by\nadding skip connections between layers. By inserting residual blocks between\nquanvolutional layers, we ensure enhanced gradient access throughout the\nnetwork, leading to improved training performance. Moreover, we provide\nempirical evidence on the strategic placement of these residual blocks within\nQuNNs. Through extensive experimentation, we identify an efficient\nconfiguration of residual blocks, which enables gradients across all the layers\nin the network that eventually results in efficient training. Our findings\nsuggest that the precise location of residual blocks plays a crucial role in\nmaximizing the performance gains in QuNNs. Our results mark a substantial step\nforward in the evolution of quantum deep learning, offering new avenues for\nboth theoretical development and practical quantum computing applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2402.09146v5", "categories": ["cs.LG", "cs.AI", "quant-ph"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2402.09146v5", "date": "2024-02-14", "updated": "2024-09-02"}
{"id": "2506.23315", "title": "Ensemble BERT for Medication Event Classification on Electronic Health Records (EHRs)", "authors": ["Shouvon Sarker", "Xishuang Dong", "Lijun Qian"], "summary": "Identification of key variables such as medications, diseases, relations from\nhealth records and clinical notes has a wide range of applications in the\nclinical domain. n2c2 2022 provided shared tasks on challenges in natural\nlanguage processing for clinical data analytics on electronic health records\n(EHR), where it built a comprehensive annotated clinical data Contextualized\nMedication Event Dataset (CMED). This study focuses on subtask 2 in Track 1 of\nthis challenge that is to detect and classify medication events from clinical\nnotes through building a novel BERT-based ensemble model. It started with\npretraining BERT models on different types of big data such as Wikipedia and\nMIMIC. Afterwards, these pretrained BERT models were fine-tuned on CMED\ntraining data. These fine-tuned BERT models were employed to accomplish\nmedication event classification on CMED testing data with multiple predictions.\nThese multiple predictions generated by these fine-tuned BERT models were\nintegrated to build final prediction with voting strategies. Experimental\nresults demonstrated that BERT-based ensemble models can effectively improve\nstrict Micro-F score by about 5% and strict Macro-F score by about 6%,\nrespectively.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23315v1", "categories": ["cs.CL", "cs.LG"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.23315v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23679", "title": "Learning Modular Exponentiation with Transformers", "authors": ["David Demitri Africa", "Sara M. Kapoor", "Theo Simon Sorg"], "summary": "Modular exponentiation is crucial to number theory and cryptography, yet\nremains largely unexplored from a mechanistic interpretability standpoint. We\ntrain a 4-layer encoder-decoder Transformer model to perform this operation and\ninvestigate the emergence of numerical reasoning during training. Utilizing\nprincipled sampling strategies, PCA-based embedding analysis, and activation\npatching, we examine how number-theoretic properties are encoded within the\nmodel. We find that reciprocal operand training leads to strong performance\ngains, with sudden generalization across related moduli. These synchronized\naccuracy surges reflect grokking-like dynamics, suggesting the model\ninternalizes shared arithmetic structure. We also find a subgraph consisting\nentirely of attention heads in the final layer sufficient to achieve full\nperformance on the task of regular exponentiation. These results suggest that\ntransformer models learn modular arithmetic through specialized computational\ncircuits, paving the way for more interpretable and efficient neural approaches\nto modular exponentiation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23679v1", "categories": ["cs.LG", "cs.AI", "cs.CR"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.23679v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22784", "title": "Single-Frame Point-Pixel Registration via Supervised Cross-Modal Feature Matching", "authors": ["Yu Han", "Zhiwei Huang", "Yanting Zhang", "Fangjun Ding", "Shen Cai", "Rui Fan"], "summary": "Point-pixel registration between LiDAR point clouds and camera images is a\nfundamental yet challenging task in autonomous driving and robotic perception.\nA key difficulty lies in the modality gap between unstructured point clouds and\nstructured images, especially under sparse single-frame LiDAR settings.\nExisting methods typically extract features separately from point clouds and\nimages, then rely on hand-crafted or learned matching strategies. This separate\nencoding fails to bridge the modality gap effectively, and more critically,\nthese methods struggle with the sparsity and noise of single-frame LiDAR, often\nrequiring point cloud accumulation or additional priors to improve reliability.\nInspired by recent progress in detector-free matching paradigms (e.g.\nMatchAnything), we revisit the projection-based approach and introduce the\ndetector-free framework for direct point-pixel matching between LiDAR and\ncamera views. Specifically, we project the LiDAR intensity map into a 2D view\nfrom the LiDAR perspective and feed it into an attention-based detector-free\nmatching network, enabling cross-modal correspondence estimation without\nrelying on multi-frame accumulation. To further enhance matching reliability,\nwe introduce a repeatability scoring mechanism that acts as a soft visibility\nprior. This guides the network to suppress unreliable matches in regions with\nlow intensity variation, improving robustness under sparse input. Extensive\nexperiments on KITTI, nuScenes, and MIAS-LCEC-TF70 benchmarks demonstrate that\nour method achieves state-of-the-art performance, outperforming prior\napproaches on nuScenes (even those relying on accumulated point clouds),\ndespite using only single-frame LiDAR.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22784v1", "categories": ["cs.CV", "cs.AI", "cs.RO"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22784v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.22843", "title": "AG-VPReID 2025: Aerial-Ground Video-based Person Re-identification Challenge Results", "authors": ["Kien Nguyen", "Clinton Fookes", "Sridha Sridharan", "Huy Nguyen", "Feng Liu", "Xiaoming Liu", "Arun Ross", "Dana Michalski", "Tam√°s Endrei", "Ivan DeAndres-Tame", "Ruben Tolosana", "Ruben Vera-Rodriguez", "Aythami Morales", "Julian Fierrez", "Javier Ortega-Garcia", "Zijing Gong", "Yuhao Wang", "Xuehu Liu", "Pingping Zhang", "Md Rashidunnabi", "Hugo Proen√ßa", "Kailash A. Hambarde", "Saeid Rezaei"], "summary": "Person re-identification (ReID) across aerial and ground vantage points has\nbecome crucial for large-scale surveillance and public safety applications.\nAlthough significant progress has been made in ground-only scenarios, bridging\nthe aerial-ground domain gap remains a formidable challenge due to extreme\nviewpoint differences, scale variations, and occlusions. Building upon the\nachievements of the AG-ReID 2023 Challenge, this paper introduces the AG-VPReID\n2025 Challenge - the first large-scale video-based competition focused on\nhigh-altitude (80-120m) aerial-ground ReID. Constructed on the new AG-VPReID\ndataset with 3,027 identities, over 13,500 tracklets, and approximately 3.7\nmillion frames captured from UAVs, CCTV, and wearable cameras, the challenge\nfeatured four international teams. These teams developed solutions ranging from\nmulti-stream architectures to transformer-based temporal reasoning and\nphysics-informed modeling. The leading approach, X-TFCLIP from UAM, attained\n72.28% Rank-1 accuracy in the aerial-to-ground ReID setting and 70.77% in the\nground-to-aerial ReID setting, surpassing existing baselines while highlighting\nthe dataset's complexity. For additional details, please refer to the official\nwebsite at https://agvpreid25.github.io.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22843v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22843v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23892", "title": "Dimension and model reduction approaches for linear Bayesian inverse problems with rank-deficient prior covariances", "authors": ["Josie K√∂nig", "Elizabeth Qian", "Melina A. Freitag"], "summary": "Bayesian inverse problems use observed data to update a prior probability\ndistribution for an unknown state or parameter of a scientific system to a\nposterior distribution conditioned on the data. In many applications, the\nunknown parameter is high-dimensional, making computation of the posterior\nexpensive due to the need to sample in a high-dimensional space and the need to\nevaluate an expensive high-dimensional forward model relating the unknown\nparameter to the data. However, inverse problems often exhibit low-dimensional\nstructure due to the fact that the available data are only informative in a\nlow-dimensional subspace of the parameter space. Dimension reduction approaches\nexploit this structure by restricting inference to the low-dimensional subspace\ninformed by the data, which can be sampled more efficiently. Further\ncomputational cost reductions can be achieved by replacing expensive\nhigh-dimensional forward models with cheaper lower-dimensional reduced models.\nIn this work, we propose new dimension and model reduction approaches for\nlinear Bayesian inverse problems with rank-deficient prior covariances, which\narise in many practical inference settings. The dimension reduction approach is\napplicable to general linear Bayesian inverse problems whereas the model\nreduction approaches are specific to the problem of inferring the initial\ncondition of a linear dynamical system. We provide theoretical approximation\nguarantees as well as numerical experiments demonstrating the accuracy and\nefficiency of the proposed approaches.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23892v1", "categories": ["math.NA", "cs.NA", "cs.SY", "eess.SY"], "cate": "math.NA", "url": "http://arxiv.org/abs/2506.23892v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22473", "title": "Unsupervised Discovery of Behavioral Primitives from Sensorimotor Dynamic Functional Connectivity", "authors": ["Fernando Diaz Ledezma", "Valentin Marcel", "Matej Hoffmann"], "summary": "The movements of both animals and robots give rise to streams of\nhigh-dimensional motor and sensory information. Imagine the brain of a newborn\nor the controller of a baby humanoid robot trying to make sense of unprocessed\nsensorimotor time series. Here, we present a framework for studying the dynamic\nfunctional connectivity between the multimodal sensory signals of a robotic\nagent to uncover an underlying structure. Using instantaneous mutual\ninformation, we capture the time-varying functional connectivity (FC) between\nproprioceptive, tactile, and visual signals, revealing the sensorimotor\nrelationships. Using an infinite relational model, we identified sensorimotor\nmodules and their evolving connectivity. To further interpret these dynamic\ninteractions, we employed non-negative matrix factorization, which decomposed\nthe connectivity patterns into additive factors and their corresponding\ntemporal coefficients. These factors can be considered the agent's motion\nprimitives or movement synergies that the agent can use to make sense of its\nsensorimotor space and later for behavior selection. In the future, the method\ncan be deployed in robot learning as well as in the analysis of human movement\ntrajectories or brain signals.", "comment": "8 pages with 6 figures", "pdf_url": "http://arxiv.org/pdf/2506.22473v1", "categories": ["cs.RO", "eess.SP"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.22473v1", "date": "2025-06-20", "updated": "2025-06-20"}
{"id": "2506.23055", "title": "Measuring How LLMs Internalize Human Psychological Concepts: A preliminary analysis", "authors": ["Hiro Taiyo Hamada", "Ippei Fujisawa", "Genji Kawakita", "Yuki Yamada"], "summary": "Large Language Models (LLMs) such as ChatGPT have shown remarkable abilities\nin producing human-like text. However, it is unclear how accurately these\nmodels internalize concepts that shape human thought and behavior. Here, we\ndeveloped a quantitative framework to assess concept alignment between LLMs and\nhuman psychological dimensions using 43 standardized psychological\nquestionnaires, selected for their established validity in measuring distinct\npsychological constructs. Our method evaluates how accurately language models\nreconstruct and classify questionnaire items through pairwise similarity\nanalysis. We compared resulting cluster structures with the original\ncategorical labels using hierarchical clustering. A GPT-4 model achieved\nsuperior classification accuracy (66.2\\%), significantly outperforming GPT-3.5\n(55.9\\%) and BERT (48.1\\%), all exceeding random baseline performance (31.9\\%).\nWe also demonstrated that the estimated semantic similarity from GPT-4 is\nassociated with Pearson's correlation coefficients of human responses in\nmultiple psychological questionnaires. This framework provides a novel approach\nto evaluate the alignment of the human-LLM concept and identify potential\nrepresentational biases. Our findings demonstrate that modern LLMs can\napproximate human psychological constructs with measurable accuracy, offering\ninsights for developing more interpretable AI systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23055v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.23055v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2504.15071", "title": "Aria-MIDI: A Dataset of Piano MIDI Files for Symbolic Music Modeling", "authors": ["Louis Bradshaw", "Simon Colton"], "summary": "We introduce an extensive new dataset of MIDI files, created by transcribing\naudio recordings of piano performances into their constituent notes. The data\npipeline we use is multi-stage, employing a language model to autonomously\ncrawl and score audio recordings from the internet based on their metadata,\nfollowed by a stage of pruning and segmentation using an audio classifier. The\nresulting dataset contains over one million distinct MIDI files, comprising\nroughly 100,000 hours of transcribed audio. We provide an in-depth analysis of\nour techniques, offering statistical insights, and investigate the content by\nextracting metadata tags, which we also provide. Dataset available at\nhttps://github.com/loubbrad/aria-midi.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.15071v1", "categories": ["cs.SD", "cs.AI", "cs.LG"], "cate": "cs.SD", "url": "http://arxiv.org/abs/2504.15071v1", "date": "2025-04-21", "updated": "2025-04-21"}
{"id": "2506.23340", "title": "Information Loss in LLMs' Multilingual Translation: The Role of Training Data, Language Proximity, and Language Family", "authors": ["Yumeng Lin", "Xufeng Duan", "David Haslett", "Yige Chen", "Zhenguang G. Cai"], "summary": "Large language models have achieved impressive progress in multilingual\ntranslation, yet they continue to face challenges with certain language\npairs-particularly those with limited training data or significant linguistic\ndivergence from English. This study systematically investigates how training\ndata, language proximity, and language family affect information loss in\nmultilingual translation. We evaluate two large language models, GPT-4 and\nLlama 2, by performing round-trip translations. Translation quality was\nassessed using BLEU scores and BERT similarity metrics. Our results reveal a\nrobust interaction between training data size and language distance: while\nabundant training data can mitigate the effects of linguistic divergence,\nlanguages structurally closer to English consistently yield higher translation\nquality in low-resource conditions. Among various distance metrics,\northographic, phylogenetic, syntactic, and geographical distances emerge as\nstrong predictors of translation performance. Language family also exerts an\nindependent influence. These findings contribute to a deeper understanding of\nthe linguistic constraints shaping multilingual translation in large language\nmodels, emphasizing that translation quality is shaped not only by data volume\nbut also by structural and typological relationships between languages.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23340v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.23340v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23706", "title": "Attestable Audits: Verifiable AI Safety Benchmarks Using Trusted Execution Environments", "authors": ["Christoph Schnabl", "Daniel Hugenroth", "Bill Marino", "Alastair R. Beresford"], "summary": "Benchmarks are important measures to evaluate safety and compliance of AI\nmodels at scale. However, they typically do not offer verifiable results and\nlack confidentiality for model IP and benchmark datasets. We propose Attestable\nAudits, which run inside Trusted Execution Environments and enable users to\nverify interaction with a compliant AI model. Our work protects sensitive data\neven when model provider and auditor do not trust each other. This addresses\nverification challenges raised in recent AI governance frameworks. We build a\nprototype demonstrating feasibility on typical audit benchmarks against\nLlama-3.1.", "comment": "ICML 2024 Workshop TAIG", "pdf_url": "http://arxiv.org/pdf/2506.23706v1", "categories": ["cs.AI", "cs.CL", "cs.CR"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.23706v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23046", "title": "SoMi-ToM: Evaluating Multi-Perspective Theory of Mind in Embodied Social Interactions", "authors": ["Xianzhe Fan", "Xuhui Zhou", "Chuanyang Jin", "Kolby Nottingham", "Hao Zhu", "Maarten Sap"], "summary": "Humans continuously infer the states, goals, and behaviors of others by\nperceiving their surroundings in dynamic, real-world social interactions.\nHowever, most Theory of Mind (ToM) benchmarks only evaluate static, text-based\nscenarios, which have a significant gap compared to real interactions. We\npropose the SoMi-ToM benchmark, designed to evaluate multi-perspective ToM in\nembodied multi-agent complex social interactions. This benchmark is based on\nrich multimodal interaction data generated by the interaction environment SoMi,\ncovering diverse crafting goals and social relationships. Our framework\nsupports multi-level evaluation: (1) first-person evaluation provides\nmultimodal (visual, dialogue, action, etc.) input from a first-person\nperspective during a task for real-time state inference, (2) third-person\nevaluation provides complete third-person perspective video and text records\nafter a task for goal and behavior inference. This evaluation method allows for\na more comprehensive examination of a model's ToM capabilities from both the\nsubjective immediate experience and the objective global observation. We\nconstructed a challenging dataset containing 35 third-person perspective\nvideos, 363 first-person perspective images, and 1225 expert-annotated\nmultiple-choice questions (three options). On this dataset, we systematically\nevaluated the performance of human subjects and several state-of-the-art large\nvision-language models (LVLMs). The results show that LVLMs perform\nsignificantly worse than humans on SoMi-ToM: the average accuracy gap between\nhumans and models is 40.1% in first-person evaluation and 26.4% in third-person\nevaluation. This indicates that future LVLMs need to further improve their ToM\ncapabilities in embodied, complex social interactions.", "comment": "23 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2506.23046v1", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.RO"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.23046v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.22850", "title": "DMD-Net: Deep Mesh Denoising Network", "authors": ["Aalok Gangopadhyay", "Shashikant Verma", "Shanmuganathan Raman"], "summary": "We present Deep Mesh Denoising Network (DMD-Net), an end-to-end deep learning\nframework, for solving the mesh denoising problem. DMD-Net consists of a Graph\nConvolutional Neural Network in which aggregation is performed in both the\nprimal as well as the dual graph. This is realized in the form of an asymmetric\ntwo-stream network, which contains a primal-dual fusion block that enables\ncommunication between the primal-stream and the dual-stream. We develop a\nFeature Guided Transformer (FGT) paradigm, which consists of a feature\nextractor, a transformer, and a denoiser. The feature extractor estimates the\nlocal features, that guide the transformer to compute a transformation, which\nis applied to the noisy input mesh to obtain a useful intermediate\nrepresentation. This is further processed by the denoiser to obtain the\ndenoised mesh. Our network is trained on a large scale dataset of 3D objects.\nWe perform exhaustive ablation studies to demonstrate that each component in\nour network is essential for obtaining the best performance. We show that our\nmethod obtains competitive or better results when compared with the\nstate-of-the-art mesh denoising algorithms. We demonstrate that our method is\nrobust to various kinds of noise. We observe that even in the presence of\nextremely high noise, our method achieves excellent performance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22850v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22850v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.22484", "title": "An Urban Multi-Operator QoE-Aware Dataset for Cellular Networks in Dense Environments", "authors": ["Muhammad Kabeer", "Rosdiadee Nordin", "Mehran Behjati", "Farah Yasmin binti Mohd Shaharuddin"], "summary": "Urban cellular networks face complex performance challenges due to high\ninfrastructure density, varied user mobility, and diverse service demands.\nWhile several datasets address network behaviour across different environments,\nthere is a lack of datasets that captures user centric Quality of Experience\n(QoE), and diverse mobility patterns needed for efficient network planning and\noptimization solutions, which are important for QoE driven optimizations and\nmobility management. This study presents a curated dataset of 30,925 labelled\nrecords, collected using GNetTrack Pro within a 2 km2 dense urban area,\nspanning three major commercial network operators. The dataset captures key\nsignal quality parameters (e.g., RSRP, RSRQ, SNR), across multiple real world\nmobility modes including pedestrian routes, canopy walkways, shuttle buses, and\nBus Rapid Transit (BRT) routes. It also includes diverse network traffic\nscenarios including (1) FTP upload and download, (2) video streaming, and (3)\nHTTP browsing. A total of 132 physical cell sites were identified and validated\nthrough OpenCellID and on-site field inspections, illustrating the high cell\ndensity characteristic of 5G and emerging heterogeneous network deployment. The\ndataset is particularly suited for machine learning applications, such as\nhandover optimization, signal quality prediction, and multi operator\nperformance evaluation. Released in a structured CSV format with accompanying\npreprocessing and visualization scripts, this dataset offers a reproducible,\napplication ready resource for researchers and practitioners working on urban\ncellular network planning and optimization.", "comment": "17 pages, 9 Figures", "pdf_url": "http://arxiv.org/pdf/2506.22484v1", "categories": ["cs.NI", "eess.SP"], "cate": "cs.NI", "url": "http://arxiv.org/abs/2506.22484v1", "date": "2025-06-23", "updated": "2025-06-23"}
{"id": "2506.23068", "title": "Curious Causality-Seeking Agents Learn Meta Causal World", "authors": ["Zhiyu Zhao", "Haoxuan Li", "Haifeng Zhang", "Jun Wang", "Francesco Faccio", "J√ºrgen Schmidhuber", "Mengyue Yang"], "summary": "When building a world model, a common assumption is that the environment has\na single, unchanging underlying causal rule, like applying Newton's laws to\nevery situation. In reality, what appears as a drifting causal mechanism is\noften the manifestation of a fixed underlying mechanism seen through a narrow\nobservational window. This brings about a problem that, when building a world\nmodel, even subtle shifts in policy or environment states can alter the very\nobserved causal mechanisms. In this work, we introduce the \\textbf{Meta-Causal\nGraph} as world models, a minimal unified representation that efficiently\nencodes the transformation rules governing how causal structures shift across\ndifferent latent world states. A single Meta-Causal Graph is composed of\nmultiple causal subgraphs, each triggered by meta state, which is in the latent\nstate space. Building on this representation, we introduce a\n\\textbf{Causality-Seeking Agent} whose objectives are to (1) identify the meta\nstates that trigger each subgraph, (2) discover the corresponding causal\nrelationships by agent curiosity-driven intervention policy, and (3)\niteratively refine the Meta-Causal Graph through ongoing curiosity-driven\nexploration and agent experiences. Experiments on both synthetic tasks and a\nchallenging robot arm manipulation task demonstrate that our method robustly\ncaptures shifts in causal dynamics and generalizes effectively to previously\nunseen contexts.", "comment": "33 pages", "pdf_url": "http://arxiv.org/pdf/2506.23068v1", "categories": ["cs.LG", "cs.AI", "stat.AP"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.23068v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.22439", "title": "Psycholinguistic Word Features: a New Approach for the Evaluation of LLMs Alignment with Humans", "authors": ["Javier Conde", "Miguel Gonz√°lez", "Mar√≠a Grandury", "Gonzalo Mart√≠nez", "Pedro Reviriego", "Mar Brysbaert"], "summary": "The evaluation of LLMs has so far focused primarily on how well they can\nperform different tasks such as reasoning, question-answering, paraphrasing, or\ntranslating. For most of these tasks, performance can be measured with\nobjective metrics, such as the number of correct answers. However, other\nlanguage features are not easily quantified. For example, arousal,\nconcreteness, or gender associated with a given word, as well as the extent to\nwhich we experience words with senses and relate them to a specific sense.\nThose features have been studied for many years by psycholinguistics,\nconducting large-scale experiments with humans to produce ratings for thousands\nof words. This opens an opportunity to evaluate how well LLMs align with human\nratings on these word features, taking advantage of existing studies that cover\nmany different language features in a large number of words. In this paper, we\nevaluate the alignment of a representative group of LLMs with human ratings on\ntwo psycholinguistic datasets: the Glasgow and Lancaster norms. These datasets\ncover thirteen features over thousands of words. The results show that\nalignment is \\textcolor{black}{generally} better in the Glasgow norms evaluated\n(arousal, valence, dominance, concreteness, imageability, familiarity, and\ngender) than on the Lancaster norms evaluated (introceptive, gustatory,\nolfactory, haptic, auditory, and visual). This suggests a potential limitation\nof current LLMs in aligning with human sensory associations for words, which\nmay be due to their lack of embodied cognition present in humans and\nillustrates the usefulness of evaluating LLMs with psycholinguistic datasets.", "comment": "Accepted for the GEM2 workshop at ACL 2025", "pdf_url": "http://arxiv.org/pdf/2506.22439v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.22439v1", "date": "2025-05-29", "updated": "2025-05-29"}
{"id": "2506.23342", "title": "ATGen: A Framework for Active Text Generation", "authors": ["Akim Tsvigun", "Daniil Vasilev", "Ivan Tsvigun", "Ivan Lysenko", "Talgat Bektleuov", "Aleksandr Medvedev", "Uliana Vinogradova", "Nikita Severin", "Mikhail Mozikov", "Andrey Savchenko", "Rostislav Grigorev", "Ramil Kuleev", "Fedor Zhdanov", "Artem Shelmanov", "Ilya Makarov"], "summary": "Active learning (AL) has demonstrated remarkable potential in reducing the\nannotation effort required for training machine learning models. However,\ndespite the surging popularity of natural language generation (NLG) tasks in\nrecent years, the application of AL to NLG has been limited. In this paper, we\nintroduce Active Text Generation (ATGen) - a comprehensive framework that\nbridges AL with text generation tasks, enabling the application of\nstate-of-the-art AL strategies to NLG. Our framework simplifies AL-empowered\nannotation in NLG tasks using both human annotators and automatic annotation\nagents based on large language models (LLMs). The framework supports LLMs\ndeployed as services, such as ChatGPT and Claude, or operated on-premises.\nFurthermore, ATGen provides a unified platform for smooth implementation and\nbenchmarking of novel AL strategies tailored to NLG tasks. Finally, we present\nevaluation results for state-of-the-art AL strategies across diverse settings\nand multiple text generation tasks. We show that ATGen reduces both the effort\nof human annotators and costs associated with API calls to LLM-based annotation\nagents. The code of the framework is available on GitHub under the MIT license.\nThe video presentation is available at http://atgen-video.nlpresearch.group", "comment": "Accepted at ACL 2025 System Demonstrations", "pdf_url": "http://arxiv.org/pdf/2506.23342v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.23342v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23949", "title": "AI Risk-Management Standards Profile for General-Purpose AI (GPAI) and Foundation Models", "authors": ["Anthony M. Barrett", "Jessica Newman", "Brandie Nonnecke", "Nada Madkour", "Dan Hendrycks", "Evan R. Murphy", "Krystal Jackson", "Deepika Raman"], "summary": "Increasingly multi-purpose AI models, such as cutting-edge large language\nmodels or other 'general-purpose AI' (GPAI) models, 'foundation models,'\ngenerative AI models, and 'frontier models' (typically all referred to\nhereafter with the umbrella term 'GPAI/foundation models' except where greater\nspecificity is needed), can provide many beneficial capabilities but also risks\nof adverse events with profound consequences. This document provides\nrisk-management practices or controls for identifying, analyzing, and\nmitigating risks of GPAI/foundation models. We intend this document primarily\nfor developers of large-scale, state-of-the-art GPAI/foundation models; others\nthat can benefit from this guidance include downstream developers of end-use\napplications that build on a GPAI/foundation model. This document facilitates\nconformity with or use of leading AI risk management-related standards,\nadapting and building on the generic voluntary guidance in the NIST AI Risk\nManagement Framework and ISO/IEC 23894, with a focus on the unique issues faced\nby developers of GPAI/foundation models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23949v1", "categories": ["cs.AI", "cs.CR", "cs.CY"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.23949v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23135", "title": "RoboScape: Physics-informed Embodied World Model", "authors": ["Yu Shang", "Xin Zhang", "Yinzhou Tang", "Lei Jin", "Chen Gao", "Wei Wu", "Yong Li"], "summary": "World models have become indispensable tools for embodied intelligence,\nserving as powerful simulators capable of generating realistic robotic videos\nwhile addressing critical data scarcity challenges. However, current embodied\nworld models exhibit limited physical awareness, particularly in modeling 3D\ngeometry and motion dynamics, resulting in unrealistic video generation for\ncontact-rich robotic scenarios. In this paper, we present RoboScape, a unified\nphysics-informed world model that jointly learns RGB video generation and\nphysics knowledge within an integrated framework. We introduce two key\nphysics-informed joint training tasks: temporal depth prediction that enhances\n3D geometric consistency in video rendering, and keypoint dynamics learning\nthat implicitly encodes physical properties (e.g., object shape and material\ncharacteristics) while improving complex motion modeling. Extensive experiments\ndemonstrate that RoboScape generates videos with superior visual fidelity and\nphysical plausibility across diverse robotic scenarios. We further validate its\npractical utility through downstream applications including robotic policy\ntraining with generated data and policy evaluation. Our work provides new\ninsights for building efficient physics-informed world models to advance\nembodied intelligence research. The code is available at:\nhttps://github.com/tsinghua-fib-lab/RoboScape.", "comment": "17 pages", "pdf_url": "http://arxiv.org/pdf/2506.23135v1", "categories": ["cs.CV", "cs.RO"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23135v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.22864", "title": "Mask-aware Text-to-Image Retrieval: Referring Expression Segmentation Meets Cross-modal Retrieval", "authors": ["Li-Cheng Shen", "Jih-Kang Hsieh", "Wei-Hua Li", "Chu-Song Chen"], "summary": "Text-to-image retrieval (TIR) aims to find relevant images based on a textual\nquery, but existing approaches are primarily based on whole-image captions and\nlack interpretability. Meanwhile, referring expression segmentation (RES)\nenables precise object localization based on natural language descriptions but\nis computationally expensive when applied across large image collections. To\nbridge this gap, we introduce Mask-aware TIR (MaTIR), a new task that unifies\nTIR and RES, requiring both efficient image search and accurate object\nsegmentation. To address this task, we propose a two-stage framework,\ncomprising a first stage for segmentation-aware image retrieval and a second\nstage for reranking and object grounding with a multimodal large language model\n(MLLM). We leverage SAM 2 to generate object masks and Alpha-CLIP to extract\nregion-level embeddings offline at first, enabling effective and scalable\nonline retrieval. Secondly, MLLM is used to refine retrieval rankings and\ngenerate bounding boxes, which are matched to segmentation masks. We evaluate\nour approach on COCO and D$^3$ datasets, demonstrating significant improvements\nin both retrieval accuracy and segmentation quality over previous methods.", "comment": "ICMR 2025", "pdf_url": "http://arxiv.org/pdf/2506.22864v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22864v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.22507", "title": "Integrated Multimodal Sensing and Communication: Challenges, Technologies, and Architectures", "authors": ["Yubo Peng", "Luping Xiang", "Kun Yang", "Feibo Jiang", "Kezhi Wang", "Christos Masouros"], "summary": "The evolution towards 6G networks requires the intelligent integration of\ncommunication and sensing capabilities to support diverse and complex\napplications, such as autonomous driving and immersive services. However,\nexisting integrated sensing and communication (ISAC) systems predominantly rely\non single-modal sensors as primary participants, which leads to a limited\nrepresentation of environmental features and significant performance\nbottlenecks under the emerging requirements of 6G applications. This limitation\nmotivates a paradigm shift from single-modal to multimodal ISAC. In this\narticle, we first analyze the key challenges in realizing multimodal ISAC,\nincluding the fusion of heterogeneous multimodal data, the high communication\noverhead among distributed sensors, and the design of efficient and scalable\nsystem architectures. We then introduce several enabling technologies, such as\nlarge AI models, semantic communication, and multi-agent systems, that hold\npromise for addressing these challenges. To operationalize these technologies,\nwe zoom into three architectural paradigms: fusion-based multimodal ISAC\n(F-MAC), interaction-based multimodal ISAC (I-MAC), and relay-based multimodal\nISAC (R-MAC), each tailored to organize devices and modalities for efficient\ncollaboration in different scenarios. Thereafter, a case study is presented\nbased on the F-MAC scheme, demonstrating that the scheme achieves more\ncomprehensive sensing and improves sensing accuracy by approximately 80%\ncompared to conventional single-modal ISAC systems. Finally, we discuss several\nopen issues to be addressed in the future.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22507v1", "categories": ["cs.NI", "cs.MA", "eess.SP"], "cate": "cs.NI", "url": "http://arxiv.org/abs/2506.22507v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.23145", "title": "Forget-MI: Machine Unlearning for Forgetting Multimodal Information in Healthcare Settings", "authors": ["Shahad Hardan", "Darya Taratynova", "Abdelmajid Essofi", "Karthik Nandakumar", "Mohammad Yaqub"], "summary": "Privacy preservation in AI is crucial, especially in healthcare, where models\nrely on sensitive patient data. In the emerging field of machine unlearning,\nexisting methodologies struggle to remove patient data from trained multimodal\narchitectures, which are widely used in healthcare. We propose Forget-MI, a\nnovel machine unlearning method for multimodal medical data, by establishing\nloss functions and perturbation techniques. Our approach unlearns unimodal and\njoint representations of the data requested to be forgotten while preserving\nknowledge from the remaining data and maintaining comparable performance to the\noriginal model. We evaluate our results using performance on the forget\ndataset, performance on the test dataset, and Membership Inference Attack\n(MIA), which measures the attacker's ability to distinguish the forget dataset\nfrom the training dataset. Our model outperforms the existing approaches that\naim to reduce MIA and the performance on the forget dataset while keeping an\nequivalent performance on the test set. Specifically, our approach reduces MIA\nby 0.202 and decreases AUC and F1 scores on the forget set by 0.221 and 0.305,\nrespectively. Additionally, our performance on the test set matches that of the\nretrained model, while allowing forgetting. Code is available at\nhttps://github.com/BioMedIA-MBZUAI/Forget-MI.git", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23145v1", "categories": ["cs.LG", "cs.CR", "cs.CV"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.23145v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.22441", "title": "Latent Factorization of Tensors with Threshold Distance Weighted Loss for Traffic Data Estimation", "authors": ["Lei Yang"], "summary": "Intelligent transportation systems (ITS) rely heavily on complete and\nhigh-quality spatiotemporal traffic data to achieve optimal performance.\nNevertheless, in real-word traffic data collection processes, issues such as\ncommunication failures and sensor malfunctions often lead to incomplete or\ncorrupted datasets, thereby posing significant challenges to the advancement of\nITS. Among various methods for imputing missing spatiotemporal traffic data,\nthe latent factorization of tensors (LFT) model has emerged as a widely adopted\nand effective solution. However, conventional LFT models typically employ the\nstandard L2-norm in their learning objective, which makes them vulnerable to\nthe influence of outliers. To overcome this limitation, this paper proposes a\nthreshold distance weighted (TDW) loss-incorporated Latent Factorization of\nTensors (TDWLFT) model. The proposed loss function effectively reduces the\nmodel's sensitivity to outliers by assigning differentiated weights to\nindividual samples. Extensive experiments conducted on two traffic speed\ndatasets sourced from diverse urban environments confirm that the proposed\nTDWLFT model consistently outperforms state-of-the-art approaches in terms of\nboth in both prediction accuracy and computational efficiency.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22441v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22441v1", "date": "2025-06-11", "updated": "2025-06-11"}
{"id": "2506.23377", "title": "Perspective Dial: Measuring Perspective of Text and Guiding LLM Outputs", "authors": ["Taejin Kim", "Siun-Chuon Mau", "Konrad Vesey"], "summary": "Large language models (LLMs) are used in a variety of mission-critical roles.\nDue to the rapidly developing nature of LLMs, there is a lack of quantifiable\nunderstanding of the bias and perspective associated with LLM output. Inspired\nby this need, this paper considers the broader issue of perspective or\nviewpoint of general text and perspective control of large-language model (LLM)\noutput. Perspective-Dial consists of two main components: a (1) metric space,\ndubbed Perspective Space, that enables quantitative measurements of different\nperspectives regarding a topic, and the use of (2) Systematic Prompt\nEngineering that utilizes greedy-coordinate descent to control LLM output\nperspective based on measurement feedback from the Perspective Space. The\nempirical nature of the approach allows progress to side step a principled\nunderstanding of perspective or bias -- effectively quantifying and adjusting\noutputs for a variety of topics. Potential applications include detection,\ntracking and mitigation of LLM bias, narrative detection, sense making and\ntracking in public discourse, and debate bot advocating given perspective.", "comment": "7 pages, 5 main pages of text, 5 figures, 2 tables. Research work\n  performed at CACI INTL INC", "pdf_url": "http://arxiv.org/pdf/2506.23377v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.23377v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.24072", "title": "Protocol insecurity with finitely many sessions and XOR", "authors": ["R Ramanujam", "Vaishnavi Sundararajan", "S P Suresh"], "summary": "We present a different proof of the insecurity problem for XOR, solved in by\nChevalier, Kuesters, Rusinowitch and Turuani (2005). Our proof uses the notion\nof typed terms and well-typed proofs, and removes a restriction on the class of\nprotocols to which the [CKRT05] proof applies, by introducing a slightly\ndifferent (but very natural) notion of protocols, where honest agent sends are\nderivable from previous receives in the same session.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.24072v1", "categories": ["cs.LO", "cs.CR"], "cate": "cs.LO", "url": "http://arxiv.org/abs/2506.24072v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23434", "title": "Towards foundational LiDAR world models with efficient latent flow matching", "authors": ["Tianran Liu", "Shengwen Zhao", "Nicholas Rhinehart"], "summary": "LiDAR-based world models offer more structured and geometry-aware\nrepresentations than their image-based counterparts. However, existing LiDAR\nworld models are narrowly trained; each model excels only in the domain for\nwhich it was built. Can we develop LiDAR world models that exhibit strong\ntransferability across multiple domains? We conduct the first systematic domain\ntransfer study across three demanding scenarios: (i) outdoor to indoor\ngeneralization, (ii) sparse-beam \\& dense-beam adaptation, and (iii)\nnon-semantic to semantic transfer. Given different amounts of fine-tuning data,\nour experiments show that a single pre-trained model can achieve up to 11%\nabsolute improvement (83\\% relative) over training from scratch and outperforms\ntraining from scratch in 30/36 of our comparisons. This transferability of\ndynamic learning significantly reduces the reliance on manually annotated data\nfor semantic occupancy forecasting: our method exceed the previous semantic\noccupancy forecasting models with only 5% of the labeled training data required\nby prior models. We also observed inefficiencies of current LiDAR world models,\nmainly through their under-compression of LiDAR data and inefficient training\nobjectives. To address this, we propose a latent conditional flow matching\n(CFM)-based frameworks that achieves state-of-the-art reconstruction accuracy\nusing only half the training data and a compression ratio 6 times higher than\nthat of prior methods. Our model achieves SOTA performance on\nfuture-trajectory-conditioned semantic occupancy forecasting while being 23x\nmore computationally efficient (a 28x FPS speedup); and achieves SOTA\nperformance on semantic occupancy forecasting while being 2x more\ncomputationally efficient (a 1.1x FPS speedup).", "comment": "25 pages, 13 figures", "pdf_url": "http://arxiv.org/pdf/2506.23434v1", "categories": ["cs.CV", "cs.RO"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23434v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22866", "title": "Region-Aware CAM: High-Resolution Weakly-Supervised Defect Segmentation via Salient Region Perception", "authors": ["Hang-Cheng Dong", "Lu Zou", "Bingguo Liu", "Dong Ye", "Guodong Liu"], "summary": "Surface defect detection plays a critical role in industrial quality\ninspection. Recent advances in artificial intelligence have significantly\nenhanced the automation level of detection processes. However, conventional\nsemantic segmentation and object detection models heavily rely on large-scale\nannotated datasets, which conflicts with the practical requirements of defect\ndetection tasks. This paper proposes a novel weakly supervised semantic\nsegmentation framework comprising two key components: a region-aware class\nactivation map (CAM) and pseudo-label training. To address the limitations of\nexisting CAM methods, especially low-resolution thermal maps, and insufficient\ndetail preservation, we introduce filtering-guided backpropagation (FGBP),\nwhich refines target regions by filtering gradient magnitudes to identify areas\nwith higher relevance to defects. Building upon this, we further develop a\nregion-aware weighted module to enhance spatial precision. Finally,\npseudo-label segmentation is implemented to refine the model's performance\niteratively. Comprehensive experiments on industrial defect datasets\ndemonstrate the superiority of our method. The proposed framework effectively\nbridges the gap between weakly supervised learning and high-precision defect\nsegmentation, offering a practical solution for resource-constrained industrial\nscenarios.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22866v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22866v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.22702", "title": "A Correlation-Based Design of RIS for Reduced Power Consumption and Simplified Control Circuitry", "authors": ["Zina Mohamed", "Ammar B. Kouki", "Sonia A√Øssa"], "summary": "Aiming at simplifying the hardware structure and reducing the energy\nconsumption in wireless communication via reconfigurable intelligent surfaces\n(RIS), this paper introduces a novel RIS design founded on the correlation\nbetween the phase shift values of the surface elements. First, a correlation\nanalysis is conducted, considering the azimuth angle of a target device within\na coverage region spanning from $-80^{\\circ}$ to $80^{\\circ}$. The correlation\nis demonstrated for different deployment cases, creating the basis for the new\nRIS structure, termed Connected-RIS, where correlated elements are designed to\nshare the same control signal. The fundamental performance of the proposed\ndesign is then analyzed in terms of control signals, power consumption, and\ncommunication system performance, comparing it to two RIS structures with full\ncontrol: one with the same size as the proposed design, and the other employing\nthe minimum number of elements necessary to satisfy the fair coverage\ncriterion. The correlation-based RIS design enables three-dimensional passive\nbeamforming and significantly reduces the number of required load impedances\nand control signals, thereby lowering the hardware cost and simplifying the\ncontrol circuitry. It also achieves substantial power savings as compared to\nthe baseline schemes, while maintaining sufficient gain for a fair radio\ncoverage. For instance, numerical simulations demonstrate that the proposed\ndesign reduces the power consumption by almost 86-92\\% and the control signals\nby 83-98\\% compared to operation with fully controlled RIS.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22702v1", "categories": ["eess.SY", "cs.AR", "cs.SY", "eess.SP"], "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.22702v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23147", "title": "maneuverRecognition -- A Python package for Timeseries Classification in the domain of Vehicle Telematics", "authors": ["Jonathan Schuster", "Fabian Transchel"], "summary": "In the domain of vehicle telematics the automated recognition of driving\nmaneuvers is used to classify and evaluate driving behaviour. This not only\nserves as a component to enhance the personalization of insurance policies, but\nalso to increase road safety, reduce accidents and the associated costs as well\nas to reduce fuel consumption and support environmentally friendly driving. In\nthis context maneuver recognition technically requires a continuous application\nof time series classification which poses special challenges to the transfer,\npreprocessing and storage of telematic sensor data, the training of predictive\nmodels, and the prediction itself. Although much research has been done in the\nfield of gathering relevant data or regarding the methods to build predictive\nmodels for the task of maneuver recognition, there is a practical need for\npython packages and functions that allow to quickly transform data into the\nrequired structure as well as to build and evaluate such models. The\nmaneuverRecognition package was therefore developed to provide the necessary\nfunctions for preprocessing, modelling and evaluation and also includes a ready\nto use LSTM based network structure that can be modified. The implementation of\nthe package is demonstrated using real driving data of three different persons\nrecorded via smartphone sensors.", "comment": "6 pages, 2 figures", "pdf_url": "http://arxiv.org/pdf/2506.23147v1", "categories": ["cs.LG", "cs.CV"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.23147v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.22445", "title": "Hierarchical Adversarially-Resilient Multi-Agent Reinforcement Learning for Cyber-Physical Systems Security", "authors": ["Saad Alqithami"], "summary": "Cyber-Physical Systems play a critical role in the infrastructure of various\nsectors, including manufacturing, energy distribution, and autonomous\ntransportation systems. However, their increasing connectivity renders them\nhighly vulnerable to sophisticated cyber threats, such as adaptive and zero-day\nattacks, against which traditional security methods like rule-based intrusion\ndetection and single-agent reinforcement learning prove insufficient. To\novercome these challenges, this paper introduces a novel Hierarchical\nAdversarially-Resilient Multi-Agent Reinforcement Learning (HAMARL) framework.\nHAMARL employs a hierarchical structure consisting of local agents dedicated to\nsubsystem security and a global coordinator that oversees and optimizes\ncomprehensive, system-wide defense strategies. Furthermore, the framework\nincorporates an adversarial training loop designed to simulate and anticipate\nevolving cyber threats, enabling proactive defense adaptation. Extensive\nexperimental evaluations conducted on a simulated industrial IoT testbed\nindicate that HAMARL substantially outperforms traditional multi-agent\nreinforcement learning approaches, significantly improving attack detection\naccuracy, reducing response times, and ensuring operational continuity. The\nresults underscore the effectiveness of combining hierarchical multi-agent\ncoordination with adversarially-aware training to enhance the resilience and\nsecurity of next-generation CPS.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22445v1", "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.MA"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22445v1", "date": "2025-06-12", "updated": "2025-06-12"}
{"id": "2506.23393", "title": "Hierarchical Memory Organization for Wikipedia Generation", "authors": ["Eugene J. Yu", "Dawei Zhu", "Yifan Song", "Xiangyu Wong", "Jiebin Zhang", "Wenxuan Shi", "Xiaoguang Li", "Qun Liu", "Sujian Li"], "summary": "Generating Wikipedia articles autonomously is a challenging task requiring\nthe integration of accurate, comprehensive, and well-structured information\nfrom diverse sources. This paper introduces the Memory Organization-based\nGeneration (MOG) framework, a novel approach to address these challenges by\nleveraging a hierarchical memory architecture. MOG extracts fine-grained memory\nunits from web documents, recursively organizes them into a Wikipedia-style\nhierarchical structure, and uses this structure to guide the generation\nprocess. This ensures alignment between memory and the article outline,\nimproving both informativeness and verifiability while minimizing\nhallucinations. Additionally, a citation module is implemented to enhance\ntraceability by linking every generated sentence to specific memory units.\nEvaluations on our newly created WikiStart dataset demonstrate that MOG\noutperforms baseline methods in producing informative and reliable articles,\nmaking it particularly robust in real-world scenarios.", "comment": "ACL 2025 Main Conference", "pdf_url": "http://arxiv.org/pdf/2506.23393v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.23393v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23982", "title": "StyleDrive: Towards Driving-Style Aware Benchmarking of End-To-End Autonomous Driving", "authors": ["Ruiyang Hao", "Bowen Jing", "Haibao Yu", "Zaiqing Nie"], "summary": "While personalization has been explored in traditional autonomous driving\nsystems, it remains largely overlooked in end-to-end autonomous driving\n(E2EAD), despite its growing prominence. This gap is critical, as user-aligned\nbehavior is essential for trust, comfort, and widespread adoption of autonomous\nvehicles. A core challenge is the lack of large-scale real-world datasets\nannotated with diverse and fine-grained driving preferences, hindering the\ndevelopment and evaluation of personalized E2EAD models. In this work, we\npresent the first large-scale real-world dataset enriched with annotations\ncapturing diverse driving preferences, establishing a foundation for\npersonalization in E2EAD. We extract static environmental features from\nreal-world road topology and infer dynamic contextual cues using a fine-tuned\nvisual language model (VLM), enabling consistent and fine-grained scenario\nconstruction. Based on these scenarios, we derive objective preference\nannotations through behavioral distribution analysis and rule-based heuristics.\nTo address the inherent subjectivity of driving style, we further employ the\nVLM to generate subjective annotations by jointly modeling scene semantics and\ndriver behavior. Final high-quality labels are obtained through a\nhuman-in-the-loop verification process that fuses both perspectives. Building\non this dataset, we propose the first benchmark for evaluating personalized\nE2EAD models. We assess several state-of-the-art models with and without\npreference conditioning, demonstrating that incorporating personalized\npreferences results in behavior more aligned with human driving. Our work lays\nthe foundation for personalized E2EAD by providing a standardized platform to\nsystematically integrate human preferences into data-driven E2EAD systems,\ncatalyzing future research in human-centric autonomy.", "comment": "14 pages, 4 figures", "pdf_url": "http://arxiv.org/pdf/2506.23982v1", "categories": ["cs.CV", "cs.RO", "I.4.9"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23982v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22868", "title": "STR-Match: Matching SpatioTemporal Relevance Score for Training-Free Video Editing", "authors": ["Junsung Lee", "Junoh Kang", "Bohyung Han"], "summary": "Previous text-guided video editing methods often suffer from temporal\ninconsistency, motion distortion, and-most notably-limited domain\ntransformation. We attribute these limitations to insufficient modeling of\nspatiotemporal pixel relevance during the editing process. To address this, we\npropose STR-Match, a training-free video editing algorithm that produces\nvisually appealing and spatiotemporally coherent videos through latent\noptimization guided by our novel STR score. The score captures spatiotemporal\npixel relevance across adjacent frames by leveraging 2D spatial attention and\n1D temporal modules in text-to-video (T2V) diffusion models, without the\noverhead of computationally expensive 3D attention mechanisms. Integrated into\na latent optimization framework with a latent mask, STR-Match generates\ntemporally consistent and visually faithful videos, maintaining strong\nperformance even under significant domain transformations while preserving key\nvisual attributes of the source. Extensive experiments demonstrate that\nSTR-Match consistently outperforms existing methods in both visual quality and\nspatiotemporal consistency.", "comment": "15 pages, 9 figures, 3 tables", "pdf_url": "http://arxiv.org/pdf/2506.22868v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22868v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.22732", "title": "Robust Tensor Completion via Gradient Tensor Nulclear L1-L2 Norm for Traffic Data Recovery", "authors": ["Hao Shu", "Jicheng Li", "Tianyv Lei", "Lijun Sun"], "summary": "In real-world scenarios, spatiotemporal traffic data frequently experiences\ndual degradation from missing values and noise caused by sensor malfunctions\nand communication failures. Therefore, effective data recovery methods are\nessential to ensure the reliability of downstream data-driven applications.\nwhile classical tensor completion methods have been widely adopted, they are\nincapable of modeling noise, making them unsuitable for complex scenarios\ninvolving simultaneous data missingness and noise interference. Existing Robust\nTensor Completion (RTC) approaches offer potential solutions by separately\nmodeling the actual tensor data and noise. However, their effectiveness is\noften constrained by the over-relaxation of convex rank surrogates and the\nsuboptimal utilization of local consistency, leading to inadequate model\naccuracy. To address these limitations, we first introduce the tensor L1-L2\nnorm, a novel non-convex tensor rank surrogate that functions as an effective\nlow-rank representation tool. Leveraging an advanced feature fusion strategy,\nwe further develop the gradient tensor L1-L2 norm by incorporating the tensor\nL1-L2 norm in the gradient domain. By integrating the gradient tensor nuclear\nL1-L2 norm into the RTC framework, we propose the Robust Tensor Completion via\nGradient Tensor Nuclear L1-L2 Norm (RTC-GTNLN) model, which not only fully\nexploits both global low-rankness and local consistency without trade-off\nparameter, but also effectively handles the dual degradation challenges of\nmissing data and noise in traffic data. Extensive experiments conducted on\nmultiple real-world traffic datasets demonstrate that the RTC-GTNLN model\nconsistently outperforms existing state-of-the-art methods in complex recovery\nscenarios involving simultaneous missing values and noise.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22732v1", "categories": ["cs.LG", "eess.SP", "stat.ML"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22732v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23165", "title": "Mirror Descent Policy Optimisation for Robust Constrained Markov Decision Processes", "authors": ["David Bossens", "Atsushi Nitanda"], "summary": "Safety is an essential requirement for reinforcement learning systems. The\nnewly emerging framework of robust constrained Markov decision processes allows\nlearning policies that satisfy long-term constraints while providing guarantees\nunder epistemic uncertainty. This paper presents mirror descent policy\noptimisation for robust constrained Markov decision processes (RCMDPs), making\nuse of policy gradient techniques to optimise both the policy (as a maximiser)\nand the transition kernel (as an adversarial minimiser) on the Lagrangian\nrepresenting a constrained MDP. In the oracle-based RCMDP setting, we obtain an\n$\\mathcal{O}\\left(\\frac{1}{T}\\right)$ convergence rate for the squared distance\nas a Bregman divergence, and an $\\mathcal{O}\\left(e^{-T}\\right)$ convergence\nrate for entropy-regularised objectives. In the sample-based RCMDP setting, we\nobtain an $\\tilde{\\mathcal{O}}\\left(\\frac{1}{T^{1/3}}\\right)$ convergence rate.\nExperiments confirm the benefits of mirror descent policy optimisation in\nconstrained and unconstrained optimisation, and significant improvements are\nobserved in robustness tests when compared to baseline policy optimisation\nalgorithms.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23165v1", "categories": ["cs.LG", "cs.NE"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.23165v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.22446", "title": "EAGLE: Efficient Alignment of Generalized Latent Embeddings for Multimodal Survival Prediction with Interpretable Attribution Analysis", "authors": ["Aakash Tripathi", "Asim Waqas", "Matthew B. Schabath", "Yasin Yilmaz", "Ghulam Rasool"], "summary": "Accurate cancer survival prediction requires integration of diverse data\nmodalities that reflect the complex interplay between imaging, clinical\nparameters, and textual reports. However, existing multimodal approaches suffer\nfrom simplistic fusion strategies, massive computational requirements, and lack\nof interpretability-critical barriers to clinical adoption. We present EAGLE\n(Efficient Alignment of Generalized Latent Embeddings), a novel deep learning\nframework that addresses these limitations through attention-based multimodal\nfusion with comprehensive attribution analysis. EAGLE introduces four key\ninnovations: (1) dynamic cross-modal attention mechanisms that learn\nhierarchical relationships between modalities, (2) massive dimensionality\nreduction (99.96%) while maintaining predictive performance, (3) three\ncomplementary attribution methods providing patient-level interpretability, and\n(4) a unified pipeline enabling seamless adaptation across cancer types. We\nevaluated EAGLE on 911 patients across three distinct malignancies:\nglioblastoma (GBM, n=160), intraductal papillary mucinous neoplasms (IPMN,\nn=171), and non-small cell lung cancer (NSCLC, n=580). Patient-level analysis\nshowed high-risk individuals relied more heavily on adverse imaging features,\nwhile low-risk patients demonstrated balanced modality contributions. Risk\nstratification identified clinically meaningful groups with 4-fold (GBM) to\n5-fold (NSCLC) differences in median survival, directly informing treatment\nintensity decisions. By combining state-of-the-art performance with clinical\ninterpretability, EAGLE bridges the gap between advanced AI capabilities and\npractical healthcare deployment, offering a scalable solution for multimodal\nsurvival prediction that enhances both prognostic accuracy and physician trust\nin automated predictions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22446v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22446v1", "date": "2025-06-12", "updated": "2025-06-12"}
{"id": "2506.23411", "title": "Datasets for Fairness in Language Models: An In-Depth Survey", "authors": ["Jiale Zhang", "Zichong Wang", "Avash Palikhe", "Zhipeng Yin", "Wenbin Zhang"], "summary": "Fairness benchmarks play a central role in shaping how we evaluate language\nmodels, yet surprisingly little attention has been given to examining the\ndatasets that these benchmarks rely on. This survey addresses that gap by\npresenting a broad and careful review of the most widely used fairness datasets\nin current language model research, characterizing them along several key\ndimensions including their origin, scope, content, and intended use to help\nresearchers better appreciate the assumptions and limitations embedded in these\nresources. To support more meaningful comparisons and analyses, we introduce a\nunified evaluation framework that reveals consistent patterns of demographic\ndisparities across datasets and scoring methods. Applying this framework to\ntwenty four common benchmarks, we highlight the often overlooked biases that\ncan influence conclusions about model fairness and offer practical guidance for\nselecting, combining, and interpreting these datasets. We also point to\nopportunities for creating new fairness benchmarks that reflect more diverse\nsocial contexts and encourage more thoughtful use of these tools going forward.\nAll code, data, and detailed results are publicly available at\nhttps://github.com/vanbanTruong/Fairness-in-Large-Language-Models/tree/main/datasets\nto promote transparency and reproducibility across the research community.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23411v1", "categories": ["cs.CL", "cs.CY", "cs.LG"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.23411v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23995", "title": "STCLocker: Deadlock Avoidance Testing for Autonomous Driving Systems", "authors": ["Mingfei Cheng", "Renzhi Wang", "Xiaofei Xie", "Yuan Zhou", "Lei Ma"], "summary": "Autonomous Driving System (ADS) testing is essential to ensure the safety and\nreliability of autonomous vehicles (AVs) before deployment. However, existing\ntechniques primarily focus on evaluating ADS functionalities in single-AV\nsettings. As ADSs are increasingly deployed in multi-AV traffic, it becomes\ncrucial to assess their cooperative performance, particularly regarding\ndeadlocks, a fundamental coordination failure in which multiple AVs enter a\ncircular waiting state indefinitely, resulting in motion planning failures.\nDespite its importance, the cooperative capability of ADSs to prevent deadlocks\nremains insufficiently underexplored. To address this gap, we propose the first\ndedicated Spatio-Temporal Conflict-Guided Deadlock Avoidance Testing technique,\nSTCLocker, for generating DeadLock Scenarios (DLSs), where a group of AVs\ncontrolled by the ADS under test are in a circular wait state. STCLocker\nconsists of three key components: Deadlock Oracle, Conflict Feedback, and\nConflict-aware Scenario Generation. Deadlock Oracle provides a reliable\nblack-box mechanism for detecting deadlock cycles among multiple AVs within a\ngiven scenario. Conflict Feedback and Conflict-aware Scenario Generation\ncollaborate to actively guide AVs into simultaneous competition over spatial\nconflict resources (i.e., shared passing regions) and temporal competitive\nbehaviors (i.e., reaching the conflict region at the same time), thereby\nincreasing the effectiveness of generating conflict-prone deadlocks. We\nevaluate STCLocker on two types of ADSs: Roach, an end-to-end ADS, and OpenCDA,\na module-based ADS supporting cooperative communication. Experimental results\nshow that, on average, STCLocker generates more DLS than the best-performing\nbaseline.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23995v1", "categories": ["cs.SE", "cs.AI", "cs.RO"], "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.23995v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22880", "title": "Decoupled Seg Tokens Make Stronger Reasoning Video Segmenter and Grounder", "authors": ["Dang Jisheng", "Wu Xudong", "Wang Bimei", "Lv Ning", "Chen Jiayu", "Jingwen Zhao", "Yichu liu", "Jizhao Liu", "Juncheng Li", "Teng Wang"], "summary": "Existing video segmenter and grounder approaches, exemplified by Sa2VA,\ndirectly fuse features within segmentation models. This often results in an\nundesirable entanglement of dynamic visual information and static semantics,\nthereby degrading segmentation accuracy. To systematically mitigate this issue,\nwe propose DeSa2VA, a decoupling-enhanced prompting scheme integrating text\npre-training and a linear decoupling module to address the information\nprocessing limitations inherent in SAM-2. Specifically, first, we devise a\npre-training paradigm that converts textual ground-truth labels into\npoint-level prompts while generating corresponding text masks. These masks are\nrefined through a hybrid loss function to strengthen the model's semantic\ngrounding capabilities. Next, we employ linear projection to disentangle hidden\nstates that generated by a large language model into distinct textual and\nvisual feature subspaces. Finally, a dynamic mask fusion strategy\nsynergistically combines these decoupled features through triple supervision\nfrom predicted text/visual masks and ground-truth annotations. Extensive\nexperiments demonstrate state-of-the-art performance across diverse tasks,\nincluding image segmentation, image question answering, video segmentation, and\nvideo question answering. Our codes are available at\nhttps://github.com/longmalongma/DeSa2VA.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22880v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22880v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.22818", "title": "TriADA: Massively Parallel Trilinear Matrix-by-Tensor Multiply-Add Algorithm and Device Architecture for the Acceleration of 3D Discrete Transformations", "authors": ["Stanislav Sedukhin", "Yoichi Tomioka", "Kazuya Matsumoto", "Yuichi Okuyama"], "summary": "Multilinear transformations are key in high-performance computing (HPC) and\nartificial intelligence (AI) workloads, where data is represented as tensors.\nHowever, their high computational and memory demands, which grow with\ndimensionality, often slow down critical tasks. Moreover, scaling computation\nby enlarging the number of parallel processing units substantially increases\nenergy consumption, limiting widespread adoption, especially for sparse data,\nwhich is common in HPC and AI applications. This paper introduces the Trilinear\nAlgorithm and isomorphic to algorithm Device Architecture (TriADA) to address\nthese challenges with the following innovations: (1) a massively parallel,\nlow-rank algorithm for computing a family of trilinear (3D) discrete orthogonal\ntransformations (3D-DXTs), which is a special case of the more general 3-mode\nmatrix-by-tensor multiplication (3D-GEMT); (2) a new outer-product-based GEMM\nkernel with decoupled streaming active memory, specially designed to accelerate\n3D-GEMT operation; (3) an isomorphic to the proposed algorithm, fully\ndistributed 3D network of mesh interconnected processing elements or cells with\na coordinate-free, data-driven local processing activity, which is independent\nof problem size; (4) an elastic sparse outer-product (ESOP) method that avoids\nunnecessary computing and communication operations with zero-valued operands,\nthereby enhancing energy efficiency, computational accuracy, and stability.\nTriADA is capable of performing a variety of trilinear transformations with\nhypercubic arithmetic complexity in a linear number of time-steps. The\nmassively parallel, scalable, and energy-efficient architecture of TriADA is\nideal for accelerating multilinear tensor operations, which are the most\ndemanding parts of AI and HPC workloads.", "comment": "19 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2506.22818v1", "categories": ["cs.DC", "cs.AI", "cs.AR", "cs.ET", "eess.SP", "C.1.4; C.3; F.2.1; G.1.3; G.4"], "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.22818v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23174", "title": "Data Can Speak for Itself: Quality-guided Utilization of Wireless Synthetic Data", "authors": ["Chen Gong", "Bo Liang", "Wei Gao", "Chenren Xu"], "summary": "Generative models have gained significant attention for their ability to\nproduce realistic synthetic data that supplements the quantity of real-world\ndatasets. While recent studies show performance improvements in wireless\nsensing tasks by incorporating all synthetic data into training sets, the\nquality of synthetic data remains unpredictable and the resulting performance\ngains are not guaranteed. To address this gap, we propose tractable and\ngeneralizable metrics to quantify quality attributes of synthetic data -\naffinity and diversity. Our assessment reveals prevalent affinity limitation in\ncurrent wireless synthetic data, leading to mislabeled data and degraded task\nperformance. We attribute the quality limitation to generative models' lack of\nawareness of untrained conditions and domain-specific processing. To mitigate\nthese issues, we introduce SynCheck, a quality-guided synthetic data\nutilization scheme that refines synthetic data quality during task model\ntraining. Our evaluation demonstrates that SynCheck consistently outperforms\nquality-oblivious utilization of synthetic data, and achieves 4.3% performance\nimprovement even when the previous utilization degrades performance by 13.4%.", "comment": "Published in MobiSys 2025", "pdf_url": "http://arxiv.org/pdf/2506.23174v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.23174v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.22447", "title": "Vision Transformers for Multi-Variable Climate Downscaling: Emulating Regional Climate Models with a Shared Encoder and Multi-Decoder Architecture", "authors": ["Fabio Merizzi", "Harilaos Loukos"], "summary": "Global Climate Models (GCMs) are critical for simulating large-scale climate\ndynamics, but their coarse spatial resolution limits their applicability in\nregional studies. Regional Climate Models (RCMs) refine this through dynamic\ndownscaling, albeit at considerable computational cost and with limited\nflexibility. While deep learning has emerged as an efficient data-driven\nalternative, most existing studies have focused on single-variable models that\ndownscale one variable at a time. This approach can lead to limited contextual\nawareness, redundant computation, and lack of cross-variable interaction. Our\nstudy addresses these limitations by proposing a multi-task, multi-variable\nVision Transformer (ViT) architecture with a shared encoder and\nvariable-specific decoders (1EMD). The proposed architecture jointly predicts\nthree key climate variables: surface temperature (tas), wind speed (sfcWind),\nand 500 hPa geopotential height (zg500), directly from GCM-resolution inputs,\nemulating RCM-scale downscaling over Europe. We show that our multi-variable\napproach achieves positive cross-variable knowledge transfer and consistently\noutperforms single-variable baselines trained under identical conditions, while\nalso improving computational efficiency. These results demonstrate the\neffectiveness of multi-variable modeling for high-resolution climate\ndownscaling.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22447v1", "categories": ["cs.LG", "cs.AI", "eess.IV"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22447v1", "date": "2025-06-12", "updated": "2025-06-12"}
{"id": "2506.23423", "title": "TuCo: Measuring the Contribution of Fine-Tuning to Individual Responses of LLMs", "authors": ["Felipe Nuti", "Tim Franzmeyer", "Jo√£o Henriques"], "summary": "Past work has studied the effects of fine-tuning on large language models'\n(LLMs) overall performance on certain tasks. However, a quantitative and\nsystematic method for analyzing its effect on individual outputs is still\nlacking. Here, we propose a new method for measuring the contribution that\nfine-tuning makes to individual LLM responses, assuming access to the original\npre-trained model. Our method tracks the model's intermediate hidden states,\nproviding a more fine-grained insight into the effects of fine-tuning than a\nsimple comparison of final outputs from pre-trained and fine-tuned models. We\nintroduce and theoretically analyze an exact decomposition of any fine-tuned\nLLM into a pre-training component and a fine-tuning component. Empirically, we\nfind that model behavior and performance can be steered by up- or down-scaling\nthe fine-tuning component during the forward pass. Motivated by this finding\nand our theoretical analysis, we define the Tuning Contribution (TuCo) as the\nratio of the magnitudes of the fine-tuning component to the pre-training\ncomponent. We observe that three prominent adversarial attacks on LLMs\ncircumvent safety measures in a way that reduces TuCo, and that TuCo is\nconsistently lower on prompts where these attacks succeed compared to those\nwhere they do not. This suggests that attenuating the effect of fine-tuning on\nmodel outputs plays a role in the success of such attacks. In summary, TuCo\nenables the quantitative study of how fine-tuning influences model behavior and\nsafety, and vice versa.", "comment": "ICML 2025", "pdf_url": "http://arxiv.org/pdf/2506.23423v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.23423v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.24044", "title": "A Survey on Vision-Language-Action Models for Autonomous Driving", "authors": ["Sicong Jiang", "Zilin Huang", "Kangan Qian", "Ziang Luo", "Tianze Zhu", "Yang Zhong", "Yihong Tang", "Menglin Kong", "Yunlong Wang", "Siwen Jiao", "Hao Ye", "Zihao Sheng", "Xin Zhao", "Tuopu Wen", "Zheng Fu", "Sikai Chen", "Kun Jiang", "Diange Yang", "Seongjin Choi", "Lijun Sun"], "summary": "The rapid progress of multimodal large language models (MLLM) has paved the\nway for Vision-Language-Action (VLA) paradigms, which integrate visual\nperception, natural language understanding, and control within a single policy.\nResearchers in autonomous driving are actively adapting these methods to the\nvehicle domain. Such models promise autonomous vehicles that can interpret\nhigh-level instructions, reason about complex traffic scenes, and make their\nown decisions. However, the literature remains fragmented and is rapidly\nexpanding. This survey offers the first comprehensive overview of VLA for\nAutonomous Driving (VLA4AD). We (i) formalize the architectural building blocks\nshared across recent work, (ii) trace the evolution from early explainer to\nreasoning-centric VLA models, and (iii) compare over 20 representative models\naccording to VLA's progress in the autonomous driving domain. We also\nconsolidate existing datasets and benchmarks, highlighting protocols that\njointly measure driving safety, accuracy, and explanation quality. Finally, we\ndetail open challenges - robustness, real-time efficiency, and formal\nverification - and outline future directions of VLA4AD. This survey provides a\nconcise yet complete reference for advancing interpretable socially aligned\nautonomous vehicles. Github repo is available at\n\\href{https://github.com/JohnsonJiang1996/Awesome-VLA4AD}{SicongJiang/Awesome-VLA4AD}.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.24044v1", "categories": ["cs.CV", "cs.AI", "cs.RO"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.24044v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22881", "title": "How Semantically Informative is an Image?: Measuring the Covariance-Weighted Norm of Contrastive Learning Embeddings", "authors": ["Fumiya Uchiyama", "Rintaro Yanagi", "Shohei Taniguchi", "Shota Takashiro", "Masahiro Suzuki", "Hirokatsu Kataoka", "Yusuke Iwasawa", "Yutaka Matsuo"], "summary": "Contrastive learning has the capacity to model multimodal probability\ndistributions by embedding and aligning visual representations with semantics\nfrom captions. This approach enables the estimation of relational semantic\nsimilarity; however, it remains unclear whether it can also represent absolute\nsemantic informativeness. In this work, we introduce a semantic informativeness\nmetric for an image calculated from text samples via a contrastive learning\nmodel; similarly, the informativeness of a text is calculated from image\nsamples. We propose a redefinition of the concept of Information Gain, a\nconcept previously explored in natural language processing, extending its\napplication to the domains of vision and language. Our metric quantifies how\nconditioning on an image distorts the distribution of associated texts, and\nvice versa for text conditioning on image distributions. In OpenCLIP's\nempirical results, we observe that images with the lowest Information Gain\nscores often correspond to placeholder icons such as \"image not found.\"\nFurthermore, we propose to measure a norm-based metric of the embedding to\nestimate the Information Gain, following the theoretical results for Skip-Gram\nwith Negative Sampling (SGNS) word embedding. Information Gain can be measured\nusing either CLIP or SigLIP, and the results demonstrate a strong correlation\nwith a coefficient of determination ranging from 0.98 to 1.00. After obtaining\nthe mean and the covariance of the sample embedding, the computational cost of\nthis method is independent of the sample size, and it is compatible with\npublicly available, open-weight models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22881v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22881v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.22855", "title": "Momentum-based Accelerated Algorithm for Distributed Optimization under Sector-Bound Nonlinearity", "authors": ["Mohammadreza Doostmohammadian", "Hamid R. Rabiee"], "summary": "Distributed optimization advances centralized machine learning methods by\nenabling parallel and decentralized learning processes over a network of\ncomputing nodes. This work provides an accelerated consensus-based distributed\nalgorithm for locally non-convex optimization using the gradient-tracking\ntechnique. The proposed algorithm (i) improves the convergence rate by adding\nmomentum towards the optimal state using the heavy-ball method, while (ii)\naddressing general sector-bound nonlinearities over the information-sharing\nnetwork. The link nonlinearity includes any sign-preserving odd sector-bound\nmapping, for example, log-scale data quantization or clipping in practical\napplications. For admissible momentum and gradient-tracking parameters, using\nperturbation theory and eigen-spectrum analysis, we prove convergence even in\nthe presence of sector-bound nonlinearity and for locally non-convex cost\nfunctions. Further, in contrast to most existing weight-stochastic algorithms,\nwe adopt weight-balanced (WB) network design. This WB design and\nperturbation-based analysis allow to handle dynamic directed network of agents\nto address possible time-varying setups due to link failures or packet drops.", "comment": "Journal of the Franklin Institute", "pdf_url": "http://arxiv.org/pdf/2506.22855v1", "categories": ["eess.SY", "cs.DC", "cs.MA", "cs.SY", "eess.SP", "math.OC"], "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.22855v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23182", "title": "Attribution assignment for deep-generative sequence models enables interpretability analysis using positive-only data", "authors": ["Robert Frank", "Michael Widrich", "Rahmad Akbar", "G√ºnter Klambauer", "Geir Kjetil Sandve", "Philippe A. Robert", "Victor Greiff"], "summary": "Generative machine learning models offer a powerful framework for therapeutic\ndesign by efficiently exploring large spaces of biological sequences enriched\nfor desirable properties. Unlike supervised learning methods, which require\nboth positive and negative labeled data, generative models such as LSTMs can be\ntrained solely on positively labeled sequences, for example, high-affinity\nantibodies. This is particularly advantageous in biological settings where\nnegative data are scarce, unreliable, or biologically ill-defined. However, the\nlack of attribution methods for generative models has hindered the ability to\nextract interpretable biological insights from such models. To address this\ngap, we developed Generative Attribution Metric Analysis (GAMA), an attribution\nmethod for autoregressive generative models based on Integrated Gradients. We\nassessed GAMA using synthetic datasets with known ground truths to characterize\nits statistical behavior and validate its ability to recover biologically\nrelevant features. We further demonstrated the utility of GAMA by applying it\nto experimental antibody-antigen binding data. GAMA enables model\ninterpretability and the validation of generative sequence design strategies\nwithout the need for negative training data.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23182v1", "categories": ["cs.LG", "q-bio.QM"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.23182v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.22448", "title": "Unsupervised Learning-Based Joint Resource Allocation and Beamforming Design for RIS-Assisted MISO-OFDMA Systems", "authors": ["Yu Ma", "Xingyu Zhou", "Xiao Li", "Le Liang", "Shi Jin"], "summary": "Reconfigurable intelligent surfaces (RIS) are key enablers for 6G wireless\nsystems. This paper studies downlink transmission in an RIS-assisted MISO-OFDMA\nsystem, addressing resource allocation challenges. A two-stage unsupervised\nlearning-based framework is proposed to jointly design RIS phase shifts, BS\nbeamforming, and resource block (RB) allocation. The framework includes\nBeamNet, which predicts RIS phase shifts from CSI, and AllocationNet, which\nallocates RBs using equivalent CSI derived from BeamNet outputs. Active\nbeamforming is implemented via maximum ratio transmission and water-filling. To\nhandle discrete constraints while ensuring differentiability, quantization and\nthe Gumbel-softmax trick are adopted. A customized loss and phased training\nenhance performance under QoS constraints. Simulations show the method achieves\n99.93% of the sum rate of the SCA baseline with only 0.036% of its runtime, and\nit remains robust across varying channel and user conditions.", "comment": "Due to the limitation \"The abstract field cannot be longer than 1,920\n  characters\", the abstract here is shorter than that in the PDF file", "pdf_url": "http://arxiv.org/pdf/2506.22448v1", "categories": ["eess.SP", "cs.AI", "cs.IT", "math.IT"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.22448v1", "date": "2025-06-12", "updated": "2025-06-12"}
{"id": "2506.23431", "title": "Pipelined Decoder for Efficient Context-Aware Text Generation", "authors": ["Zixian Huang", "Chenxu Niu", "Yu Gu", "Gengyang Xiao", "Xinwei Huang", "Gong Cheng"], "summary": "As the basis of generative AI, an autoregressive model requires the\ngeneration of a new token depending on all the previously generated tokens,\nwhich brings high quality but also restricts the model to generate tokens one\nby one, forming a bottleneck limiting the generation speed. In this paper, we\npropose a new decoder architecture that efficiently generates text in parallel\nfor context-aware generation tasks. Our proposed pipelined decoder initiates\nthe generation of multiple subsequences simultaneously, and, at each time-step,\nit generates a new token for each subsequence to realize parallelism.\nExperiments on multiple text generation tasks, including question answering,\ntext summarization, and keyphrase generation, show that our pipelined decoder\nsignificantly improves the generation speed without a significant loss of\ngeneration quality or additional memory consumption.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23431v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.23431v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.22890", "title": "CP-Guard: A Unified, Probability-Agnostic, and Adaptive Framework for Malicious Agent Detection and Defense in Multi-Agent Embodied Perception Systems", "authors": ["Senkang Hu", "Yihang Tao", "Guowen Xu", "Xinyuan Qian", "Yiqin Deng", "Xianhao Chen", "Sam Tak Wu Kwong", "Yuguang Fang"], "summary": "Collaborative Perception (CP) has been shown to be a promising technique for\nmulti-agent autonomous driving and multi-agent robotic systems, where multiple\nagents share their perception information to enhance the overall perception\nperformance and expand the perception range. However, in CP, an ego agent needs\nto receive messages from its collaborators, which makes it vulnerable to\nattacks from malicious agents. To address this critical issue, we propose a\nunified, probability-agnostic, and adaptive framework, namely, CP-Guard, which\nis a tailored defense mechanism for CP deployed by each agent to accurately\ndetect and eliminate malicious agents in its collaboration network. Our key\nidea is to enable CP to reach a consensus rather than a conflict against an ego\nagent's perception results. Based on this idea, we first develop a\nprobability-agnostic sample consensus (PASAC) method to effectively sample a\nsubset of the collaborators and verify the consensus without prior\nprobabilities of malicious agents. Furthermore, we define collaborative\nconsistency loss (CCLoss) for object detection task and bird's eye view (BEV)\nsegmentation task to capture the discrepancy between an ego agent and its\ncollaborators, which is used as a verification criterion for consensus. In\naddition, we propose online adaptive threshold via dual sliding windows to\ndynamically adjust the threshold for consensus verification and ensure the\nreliability of the systems in dynamic environments. Finally, we conduct\nextensive experiments and demonstrate the effectiveness of our framework. Code\nwill be released at https://github.com/CP-Security/CP-Guard", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22890v1", "categories": ["cs.CV", "cs.CR"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22890v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.22929", "title": "Mathematical Computation on High-dimensional Data via Array Programming and Parallel Acceleration", "authors": ["Chen Zhang"], "summary": "While deep learning excels in natural image and language processing, its\napplication to high-dimensional data faces computational challenges due to the\ndimensionality curse. Current large-scale data tools focus on business-oriented\ndescriptive statistics, lacking mathematical statistics support for advanced\nanalysis. We propose a parallel computation architecture based on space\ncompleteness, decomposing high-dimensional data into dimension-independent\nstructures for distributed processing. This framework enables seamless\nintegration of data mining and parallel-optimized machine learning methods,\nsupporting scientific computations across diverse data types like medical and\nnatural images within a unified system.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22929v1", "categories": ["cs.LG", "cs.AI", "eess.IV", "eess.SP"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22929v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23186", "title": "Efficient Algorithms for Learning and Compressing Monophonic Halfspaces in Graphs", "authors": ["Marco Bressan", "Victor Chepoi", "Emmanuel Esposito", "Maximilian Thiessen"], "summary": "Abstract notions of convexity over the vertices of a graph, and corresponding\nnotions of halfspaces, have recently gained attention from the machine learning\ncommunity. In this work we study monophonic halfspaces, a notion of graph\nhalfspaces defined through closure under induced paths. Our main result is a\n$2$-satisfiability based decomposition theorem, which allows one to represent\nmonophonic halfspaces as a disjoint union of certain vertex subsets. Using this\ndecomposition, we achieve efficient and (nearly) optimal algorithms for various\nlearning problems, such as teaching, active, and online learning. Most notably,\nwe obtain a polynomial-time algorithm for empirical risk minimization.\nIndependently of the decomposition theorem, we obtain an efficient, stable, and\nproper sample compression scheme. This makes monophonic halfspaces efficiently\nlearnable with proper learners and linear error rate $1/\\varepsilon$ in the\nrealizable PAC setting. Our results answer open questions from the literature,\nand show a stark contrast with geodesic halfspaces, for which most of the said\nlearning problems are NP-hard.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23186v1", "categories": ["cs.LG", "cs.DM", "math.CO", "stat.ML"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.23186v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.22457", "title": "A Complex UNet Approach for Non-Invasive Fetal ECG Extraction Using Single-Channel Dry Textile Electrodes", "authors": ["Iulia Orvas", "Andrei Radu", "Alessandra Galli", "Ana Neacsu", "Elisabetta Peri"], "summary": "Continuous, non-invasive pregnancy monitoring is crucial for minimising\npotential complications. The fetal electrocardiogram (fECG) represents a\npromising tool for assessing fetal health beyond clinical environments.\nHome-based monitoring necessitates the use of a minimal number of comfortable\nand durable electrodes, such as dry textile electrodes. However, this setup\npresents many challenges, including increased noise and motion artefacts, which\ncomplicate the accurate extraction of fECG signals. To overcome these\nchallenges, we introduce a pioneering method for extracting fECG from\nsingle-channel recordings obtained using dry textile electrodes using AI\ntechniques. We created a new dataset by simulating abdominal recordings,\nincluding noise closely resembling real-world characteristics of in-vivo\nrecordings through dry textile electrodes, alongside mECG and fECG. To ensure\nthe reliability of the extracted fECG, we propose an innovative pipeline based\non a complex-valued denoising network, Complex UNet. Unlike previous approaches\nthat focused solely on signal magnitude, our method processes both real and\nimaginary components of the spectrogram, addressing phase information and\npreventing incongruous predictions. We evaluated our novel pipeline against\ntraditional, well-established approaches, on both simulated and real data in\nterms of fECG extraction and R-peak detection. The results showcase that our\nsuggested method achieves new state-of-the-art results, enabling an accurate\nextraction of fECG morphology across all evaluated settings. This method is the\nfirst to effectively extract fECG signals from single-channel recordings using\ndry textile electrodes, making a significant advancement towards a fully\nnon-invasive and self-administered fECG extraction solution.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22457v1", "categories": ["eess.SP", "cs.AI"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.22457v1", "date": "2025-06-16", "updated": "2025-06-16"}
{"id": "2506.23463", "title": "What to Keep and What to Drop: Adaptive Table Filtering Framework", "authors": ["Jang Won June"], "summary": "Large language models (LLMs) for table-based reasoning often struggle with\nlarge tables due to input length limits. We propose ATF (Adaptive Table\nFiltering Framework), a modular and question-aware filtering pipeline that\nprunes uninformative columns and rows using LLM-generated column descriptions,\nclustering, and sparse-dense alignment scores. ATF integrates seamlessly with\nexisting models (e.g., TAPAS, TAPEX) without retraining. Experiments show that\nATF reduces table cells by ~70\\%, boosting performance on out-of-domain TableQA\ntasks while causing slight performance drops on Table Fact Verification, where\nfull-table context is more critical. These results highlight ATF's ability to\nadaptively balance informativeness and minimalism across tasks.", "comment": "26 pages, 9 figures", "pdf_url": "http://arxiv.org/pdf/2506.23463v1", "categories": ["cs.CL", "I.2.7"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.23463v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22899", "title": "Neural Cellular Automata: From Cells to Pixels", "authors": ["Ehsan Pajouheshgar", "Yitao Xu", "Ali Abbasi", "Alexander Mordvintsev", "Wenzel Jakob", "Sabine S√ºsstrunk"], "summary": "Neural Cellular Automata (NCAs) are bio-inspired systems in which identical\ncells self-organize to form complex and coherent patterns by repeatedly\napplying simple local rules. NCAs display striking emergent behaviors including\nself-regeneration, generalization and robustness to unseen situations, and\nspontaneous motion. Despite their success in texture synthesis and\nmorphogenesis, NCAs remain largely confined to low-resolution grids. This\nlimitation stems from (1) training time and memory requirements that grow\nquadratically with grid size, (2) the strictly local propagation of information\nwhich impedes long-range cell communication, and (3) the heavy compute demands\nof real-time inference at high resolution. In this work, we overcome this\nlimitation by pairing NCA with a tiny, shared implicit decoder, inspired by\nrecent advances in implicit neural representations. Following NCA evolution on\na coarse grid, a lightweight decoder renders output images at arbitrary\nresolution. We also propose novel loss functions for both morphogenesis and\ntexture synthesis tasks, specifically tailored for high-resolution output with\nminimal memory and computation overhead. Combining our proposed architecture\nand loss functions brings substantial improvement in quality, efficiency, and\nperformance. NCAs equipped with our implicit decoder can generate full-HD\noutputs in real time while preserving their self-organizing, emergent\nproperties. Moreover, because each MLP processes cell states independently,\ninference remains highly parallelizable and efficient. We demonstrate the\napplicability of our approach across multiple NCA variants (on 2D, 3D grids,\nand 3D meshes) and multiple tasks, including texture generation and\nmorphogenesis (growing patterns from a seed), showing that with our proposed\nframework, NCAs seamlessly scale to high-resolution outputs with minimal\ncomputational overhead.", "comment": "6 pages, 5 figures, first draft", "pdf_url": "http://arxiv.org/pdf/2506.22899v1", "categories": ["cs.CV", "cs.GR", "cs.LG", "cs.MA", "eess.IV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22899v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23052", "title": "Flexible Intelligent Metasurface for Enhancing Multi-Target Wireless Sensing", "authors": ["Zihao Teng", "Jiancheng An", "Lu Gan", "Naofal Al-Dhahir", "Zhu Han"], "summary": "Flexible intelligent metasurface (FIM) has emerged as a transformative\ntechnology to enhance wireless sensing by dynamically morphing its\nthree-dimensional (3D) surface shape and electromagnetic response. Unlike\nconventional rigid arrays, an FIM consists of low-cost radiating elements that\ncan independently adjust their positions and radiation characteristics, thereby\nallowing for real-time optimization of the sensing environment. This paper\ninvestigates the impact of FIM on wireless sensing performance. Specifically,\nwe focus on the maximization of the cumulated power of the probing signals at\nthe target locations under the per-antenna power constraint by jointly\noptimizing the transmit covariance matrix and the surface shape of the\ntransmitting FIM. We propose a block coordinate descend (BCD) algorithm to find\na locally optimal solution, by alternatively updating the FIM surface shape and\nthe transmit covariance matrix, while keeping the other one fixed at each step.\nFurthermore, we analyze the computational complexity and convergence properties\nof the proposed algorithm and demonstrate that FIM enhances wireless sensing by\nproviding a new design degree-of-freedom to coordinate the correlation between\nsteering vectors at different angles. Numerical results demonstrate that FIM\nsignificantly improves wireless sensing performance under the considered\nmulti-target scenario.", "comment": "7 pages, 3 figures, accepted by IEEE TVT", "pdf_url": "http://arxiv.org/pdf/2506.23052v1", "categories": ["cs.IT", "eess.SP", "math.IT"], "cate": "cs.IT", "url": "http://arxiv.org/abs/2506.23052v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23201", "title": "External Data-Enhanced Meta-Representation for Adaptive Probabilistic Load Forecasting", "authors": ["Haoran Li", "Muhao Guo", "Marija Ilic", "Yang Weng", "Guangchun Ruan"], "summary": "Accurate residential load forecasting is critical for power system\nreliability with rising renewable integration and demand-side flexibility.\nHowever, most statistical and machine learning models treat external factors,\nsuch as weather, calendar effects, and pricing, as extra input, ignoring their\nheterogeneity, and thus limiting the extraction of useful external information.\nWe propose a paradigm shift: external data should serve as meta-knowledge to\ndynamically adapt the forecasting model itself. Based on this idea, we design a\nmeta-representation framework using hypernetworks that modulate selected\nparameters of a base Deep Learning (DL) model in response to external\nconditions. This provides both expressivity and adaptability. We further\nintegrate a Mixture-of-Experts (MoE) mechanism to enhance efficiency through\nselective expert activation, while improving robustness by filtering redundant\nexternal inputs. The resulting model, dubbed as a Meta Mixture of Experts for\nExternal data (M2oE2), achieves substantial improvements in accuracy and\nrobustness with limited additional overhead, outperforming existing\nstate-of-the-art methods in diverse load datasets. The dataset and source code\nare publicly available at\nhttps://github.com/haorandd/M2oE2\\_load\\_forecast.git.", "comment": "10 pages", "pdf_url": "http://arxiv.org/pdf/2506.23201v1", "categories": ["cs.LG", "cs.SY", "eess.SY"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.23201v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.22460", "title": "Heart rate and respiratory rate prediction from noisy real-world smartphone based on Deep Learning methods", "authors": ["Ibne Farabi Shihab"], "summary": "Using mobile phone video of the fingertip as a data source for estimating\nvital signs such as heart rate (HR) and respiratory rate (RR) during daily life\nhas long been suggested. While existing literature indicates that these\nestimates are accurate to within several beats or breaths per minute, the data\nused to draw these conclusions are typically collected in laboratory\nenvironments under careful experimental control, and yet the results are\nassumed to generalize to daily life. In an effort to test it, a team of\nresearchers collected a large dataset of mobile phone video recordings made\nduring daily life and annotated with ground truth HR and RR labels from N=111\nparticipants. They found that traditional algorithm performance on the\nfingerprint videos is worse than previously reported (7 times and 13 times\nworse for RR and HR, respectively). Fortunately, recent advancements in deep\nlearning, especially in convolutional neural networks (CNNs), offer a promising\nsolution to improve this performance. This study proposes a new method for\nestimating HR and RR using a novel 3D deep CNN, demonstrating a reduced error\nin estimated HR by 68% and RR by 75%. These promising results suggest that\nregressor-based deep learning approaches should be used in estimating HR and\nRR.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22460v1", "categories": ["eess.SP", "cs.AI"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.22460v1", "date": "2025-06-17", "updated": "2025-06-17"}
{"id": "2506.23485", "title": "Thought-Augmented Planning for LLM-Powered Interactive Recommender Agent", "authors": ["Haocheng Yu", "Yaxiong Wu", "Hao Wang", "Wei Guo", "Yong Liu", "Yawen Li", "Yuyang Ye", "Junping Du", "Enhong Chen"], "summary": "Interactive recommendation is a typical information-seeking task that allows\nusers to interactively express their needs through natural language and obtain\npersonalized recommendations. Large language model-powered (LLM-powered) agents\nhave become a new paradigm in interactive recommendations, effectively\ncapturing users' real-time needs and enhancing personalized experiences.\nHowever, due to limited planning and generalization capabilities, existing\nformulations of LLM-powered interactive recommender agents struggle to\neffectively address diverse and complex user intents, such as intuitive,\nunrefined, or occasionally ambiguous requests. To tackle this challenge, we\npropose a novel thought-augmented interactive recommender agent system (TAIRA)\nthat addresses complex user intents through distilled thought patterns.\nSpecifically, TAIRA is designed as an LLM-powered multi-agent system featuring\na manager agent that orchestrates recommendation tasks by decomposing user\nneeds and planning subtasks, with its planning capacity strengthened through\nThought Pattern Distillation (TPD), a thought-augmentation method that extracts\nhigh-level thoughts from the agent's and human experts' experiences. Moreover,\nwe designed a set of user simulation schemes to generate personalized queries\nof different difficulties and evaluate the recommendations based on specific\ndatasets. Through comprehensive experiments conducted across multiple datasets,\nTAIRA exhibits significantly enhanced performance compared to existing methods.\nNotably, TAIRA shows a greater advantage on more challenging tasks while\ngeneralizing effectively on novel tasks, further validating its superiority in\nmanaging complex user intents within interactive recommendation systems. The\ncode is publicly available at:https://github.com/Alcein/TAIRA.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23485v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.23485v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22900", "title": "MOTOR: Multimodal Optimal Transport via Grounded Retrieval in Medical Visual Question Answering", "authors": ["Mai A. Shaaban", "Tausifa Jan Saleem", "Vijay Ram Papineni", "Mohammad Yaqub"], "summary": "Medical visual question answering (MedVQA) plays a vital role in clinical\ndecision-making by providing contextually rich answers to image-based queries.\nAlthough vision-language models (VLMs) are widely used for this task, they\noften generate factually incorrect answers. Retrieval-augmented generation\naddresses this challenge by providing information from external sources, but\nrisks retrieving irrelevant context, which can degrade the reasoning\ncapabilities of VLMs. Re-ranking retrievals, as introduced in existing\napproaches, enhances retrieval relevance by focusing on query-text alignment.\nHowever, these approaches neglect the visual or multimodal context, which is\nparticularly crucial for medical diagnosis. We propose MOTOR, a novel\nmultimodal retrieval and re-ranking approach that leverages grounded captions\nand optimal transport. It captures the underlying relationships between the\nquery and the retrieved context based on textual and visual information.\nConsequently, our approach identifies more clinically relevant contexts to\naugment the VLM input. Empirical analysis and human expert evaluation\ndemonstrate that MOTOR achieves higher accuracy on MedVQA datasets,\noutperforming state-of-the-art methods by an average of 6.45%. Code is\navailable at https://github.com/BioMedIA-MBZUAI/MOTOR.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22900v1", "categories": ["cs.CV", "cs.CL"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22900v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23075", "title": "CSBrain: A Cross-scale Spatiotemporal Brain Foundation Model for EEG Decoding", "authors": ["Yuchen Zhou", "Jiamin Wu", "Zichen Ren", "Zhouheng Yao", "Weiheng Lu", "Kunyu Peng", "Qihao Zheng", "Chunfeng Song", "Wanli Ouyang", "Chao Gou"], "summary": "Understanding and decoding brain activity from electroencephalography (EEG)\nsignals is a fundamental challenge in neuroscience and AI, with applications in\ncognition, emotion recognition, diagnosis, and brain-computer interfaces. While\nrecent EEG foundation models advance generalized decoding via unified\narchitectures and large-scale pretraining, they adopt a scale-agnostic dense\nmodeling paradigm inherited from NLP and vision. This design neglects a core\nproperty of neural activity: cross-scale spatiotemporal structure. EEG task\npatterns span a wide range of temporal and spatial scales, from short bursts to\nslow rhythms, and from localized cortical responses to distributed\ninteractions. Ignoring this diversity leads to suboptimal representations and\nweak generalization. We propose CSBrain, a Cross-scale Spatiotemporal Brain\nfoundation model for generalized EEG decoding. CSBrain introduces: (i)\nCross-scale Spatiotemporal Tokenization (CST), which aggregates multi-scale\nfeatures from localized temporal windows and anatomical brain regions into\ncompact scale-aware tokens; and (ii) Structured Sparse Attention (SSA), which\ncaptures cross-window and cross-region dependencies, enhancing scale diversity\nwhile removing spurious correlations. CST and SSA are alternately stacked to\nprogressively integrate multi-scale dependencies. Experiments on 11 EEG tasks\nacross 16 datasets show that CSBrain consistently outperforms task-specific and\nfoundation model baselines. These results establish cross-scale modeling as a\nkey inductive bias and position CSBrain as a robust backbone for future\nbrain-AI research.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23075v1", "categories": ["cs.HC", "cs.LG", "eess.SP", "q-bio.NC"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.23075v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23210", "title": "FedRef: Communication-Efficient Bayesian Fine Tuning with Reference Model", "authors": ["Taehwan Yoon", "Bongjun Choi"], "summary": "Federated learning(FL) is used for distributed scenarios to train artificial\nintelligence(AI) models while ensuring users' privacy. In federated learning\nscenario, the server generally never knows about users' data. This type of\nconcept makes the AI training process efficient in terms of data privacy.\nHowever, regarding model performance, federated AI models may not sufficiently\nsatisfy AI users' expectations. Furthermore, AI users have a wide range of\ndifferent needs. It is not easy to satisfy the whole users needs. These types\nof issues can be addressed through AI model optimization, fine-tuning, or\npersonalization to achieve optimal model performance. To address model\noptimization challenges, we propose reference model-based federated learning\nfor optimal fine-tuning, which overcomes catastrophic forgetting in each round.\nThis method is derived from Bayesian parameter-efficient transfer learning,\nwhich includes an optimal proximal term and enables overcoming the catastrophic\nforgetting issue in each round by utilizing a reference model that incorporates\nprevious model parameters. As a result, this method achieves both high model\nperformance and low computing cost.", "comment": "6 pages,14 equation", "pdf_url": "http://arxiv.org/pdf/2506.23210v1", "categories": ["cs.LG", "cs.AI", "cs.DC"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.23210v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.22461", "title": "Machine Learning for Proactive Groundwater Management: Early Warning and Resource Allocation", "authors": ["Chuan Li", "Ruoxuan Yang"], "summary": "Groundwater supports ecosystems, agriculture, and drinking water supplies\nworldwide, yet effective monitoring remains challenging due to sparse data,\ncomputational constraints, and delayed outputs from traditional approaches. We\ndevelop a machine learning pipeline that predicts groundwater level categories\nusing climate data, hydro-meteorological records, and physiographic attributes\nprocessed through AutoGluon's automated ensemble framework. Our approach\nintegrates geospatial preprocessing, domain-driven feature engineering, and\nautomated model selection to overcome conventional monitoring limitations.\nApplied to a large-scale French dataset (n $>$ 3,440,000 observations from\n1,500+ wells), the model achieves weighted F\\_1 scores of 0.927 on validation\ndata and 0.67 on temporally distinct test data. Scenario-based evaluations\ndemonstrate practical utility for early warning systems and water allocation\ndecisions under changing climate conditions. The open-source implementation\nprovides a scalable framework for integrating machine learning into national\ngroundwater monitoring networks, enabling more responsive and data-driven water\nmanagement strategies.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22461v1", "categories": ["eess.SP", "cs.AI"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.22461v1", "date": "2025-06-18", "updated": "2025-06-18"}
{"id": "2506.23508", "title": "Reinforcement Fine-Tuning Enables MLLMs Learning Novel Tasks Stably", "authors": ["Zhihao Zhang", "Qiaole Dong", "Qi Zhang", "Jun Zhao", "Enyu Zhou", "Zhiheng Xi", "Senjie Jin", "Xiaoran Fan", "Yuhao Zhou", "Yanwei Fu", "Tao Ji", "Tao Gui", "Xuanjing Huang"], "summary": "Post-training algorithms such as Supervised Fine-Tuning (SFT) and\nReinforcement Fine-Tuning (RFT) are widely used to adapt multimodal large\nlanguage models to downstream tasks. While effective at task adaptation, their\nimpact on prior knowledge remains unclear. In this paper, we introduce jigsaw\npuzzles as a novel task absent from existing pretraining corpora and\nsystematically study the behavior of SFT and RFT on an open-source multimodal\nmodel, Qwen2.5-VL. Our experiments reveal a sharp trade-off: SFT enables rapid\ntask acquisition but leads to catastrophic forgetting, whereas RFT learns more\nslowly on novel tasks but maintains prior knowledge. We analyze this phenomenon\nthrough the lens of learning dynamics, showing that RFT reinforces correct\nsamples that are naturally aligned with the base model's probability landscape,\nmitigating interference with prior knowledge. Moreover, supervised training on\ncorrect RFT-simulated rollouts allows SFT to preserve knowledge while rapidly\nlearning new tasks. These findings suggest that data distribution, rather than\nalgorithmic differences, plays a central role in forgetting, and highlight\nRFT's potential for stable continual learning in multimodal large language\nmodels.", "comment": "18 pages (Preprint. Work in progress)", "pdf_url": "http://arxiv.org/pdf/2506.23508v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.23508v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22902", "title": "Point Cloud Compression and Objective Quality Assessment: A Survey", "authors": ["Yiling Xu", "Yujie Zhang", "Shuting Xia", "Kaifa Yang", "He Huang", "Ziyu Shan", "Wenjie Huang", "Qi Yang", "Le Yang"], "summary": "The rapid growth of 3D point cloud data, driven by applications in autonomous\ndriving, robotics, and immersive environments, has led to criticals demand for\nefficient compression and quality assessment techniques. Unlike traditional 2D\nmedia, point clouds present unique challenges due to their irregular structure,\nhigh data volume, and complex attributes. This paper provides a comprehensive\nsurvey of recent advances in point cloud compression (PCC) and point cloud\nquality assessment (PCQA), emphasizing their significance for real-time and\nperceptually relevant applications. We analyze a wide range of handcrafted and\nlearning-based PCC algorithms, along with objective PCQA metrics. By\nbenchmarking representative methods on emerging datasets, we offer detailed\ncomparisons and practical insights into their strengths and limitations.\nDespite notable progress, challenges such as enhancing visual fidelity,\nreducing latency, and supporting multimodal data remain. This survey outlines\nfuture directions, including hybrid compression frameworks and advanced feature\nextraction strategies, to enable more efficient, immersive, and intelligent 3D\napplications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22902v1", "categories": ["cs.CV", "eess.IV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22902v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23213", "title": "Nuisance parameters and elliptically symmetric distributions: a geometric approach to parametric and semiparametric efficiency", "authors": ["Stefano Fortunati", "Jean-Pierre Delmas", "Esa Ollila"], "summary": "Elliptically symmetric distributions are a classic example of a\nsemiparametric model where the location vector and the scatter matrix (or a\nparameterization of them) are the two finite-dimensional parameters of\ninterest, while the density generator represents an\n\\textit{infinite-dimensional nuisance} term. This basic representation of the\nelliptic model can be made more accurate, rich, and flexible by considering\nadditional \\textit{finite-dimensional nuisance} parameters. Our aim is\ntherefore to investigate the deep and counter-intuitive links between\nstatistical efficiency in estimating the parameters of interest in the presence\nof both finite and infinite-dimensional nuisance parameters. Unlike previous\nworks that addressed this problem using Le Cam's asymptotic theory, our\napproach here is purely geometric: efficiency will be analyzed using tools such\nas projections and tangent spaces embedded in the relevant Hilbert space. This\nallows us to obtain original results also for the case where the location\nvector and the scatter matrix are parameterized by a finite-dimensional vector\nthat can be partitioned in two sub-vectors: one containing the parameters of\ninterest and the other containing the nuisance parameters. As an example, we\nillustrate how the obtained results can be applied to the well-known\n\\virg{low-rank} parameterization. Furthermore, while the theoretical analysis\nwill be developed for Real Elliptically Symmetric (RES) distributions, we show\nhow to extend our results to the case of Circular and Non-Circular Complex\nElliptically Symmetric (C-CES and NC-CES) distributions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23213v1", "categories": ["math.ST", "eess.SP", "stat.TH"], "cate": "math.ST", "url": "http://arxiv.org/abs/2506.23213v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23221", "title": "Single Image Inpainting and Super-Resolution with Simultaneous Uncertainty Guarantees by Universal Reproducing Kernels", "authors": ["B√°lint Horv√°th", "Bal√°zs Csan√°d Cs√°ji"], "summary": "The paper proposes a statistical learning approach to the problem of\nestimating missing pixels of images, crucial for image inpainting and\nsuper-resolution problems. One of the main novelties of the method is that it\nalso provides uncertainty quantifications together with the estimated values.\nOur core assumption is that the underlying data-generating function comes from\na Reproducing Kernel Hilbert Space (RKHS). A special emphasis is put on\nband-limited functions, central to signal processing, which form Paley-Wiener\ntype RKHSs. The proposed method, which we call Simultaneously Guaranteed Kernel\nInterpolation (SGKI), is an extension and refinement of a recently developed\nkernel method. An advantage of SGKI is that it not only estimates the missing\npixels, but also builds non-asymptotic confidence bands for the unobserved\nvalues, which are simultaneously guaranteed for all missing pixels. We also\nshow how to compute these bands efficiently using Schur complements, we discuss\na generalization to vector-valued functions, and we present a series of\nnumerical experiments on various datasets containing synthetically generated\nand benchmark images, as well.", "comment": "23 pages, 8 figures, 6 tables", "pdf_url": "http://arxiv.org/pdf/2506.23221v1", "categories": ["cs.LG", "cs.CV"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.23221v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.22462", "title": "Privacy-aware IoT Fall Detection Services For Aging in Place", "authors": ["Abdallah Lakhdari", "Jiajie Li", "Amani Abusafia", "Athman Bouguettaya"], "summary": "Fall detection is critical to support the growing elderly population,\nprojected to reach 2.1 billion by 2050. However, existing methods often face\ndata scarcity challenges or compromise privacy. We propose a novel IoT-based\nFall Detection as a Service (FDaaS) framework to assist the elderly in living\nindependently and safely by accurately detecting falls. We design a\nservice-oriented architecture that leverages Ultra-wideband (UWB) radar sensors\nas an IoT health-sensing service, ensuring privacy and minimal intrusion. We\naddress the challenges of data scarcity by utilizing a Fall Detection\nGenerative Pre-trained Transformer (FD-GPT) that uses augmentation techniques.\nWe developed a protocol to collect a comprehensive dataset of the elderly daily\nactivities and fall events. This resulted in a real dataset that carefully\nmimics the elderly's routine. We rigorously evaluate and compare various models\nusing this dataset. Experimental results show our approach achieves 90.72%\naccuracy and 89.33% precision in distinguishing between fall events and regular\nactivities of daily living.", "comment": "11 pages, 12 figures, This paper is accepted in the 2025 IEEE\n  International Conference on Web Services (ICWS 2025)", "pdf_url": "http://arxiv.org/pdf/2506.22462v1", "categories": ["eess.SP", "cs.AI", "cs.CY", "cs.HC"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.22462v1", "date": "2025-06-18", "updated": "2025-06-18"}
{"id": "2506.23524", "title": "NEU-ESC: A Comprehensive Vietnamese dataset for Educational Sentiment analysis and topic Classification toward multitask learning", "authors": ["Phan Quoc Hung Mai", "Quang Hung Nguyen", "Phuong Giang Duong", "Hong Hanh Nguyen", "Nguyen Tuan Long"], "summary": "In the field of education, understanding students' opinions through their\ncomments is crucial, especially in the Vietnamese language, where resources\nremain limited. Existing educational datasets often lack domain relevance and\nstudent slang. To address these gaps, we introduce NEU-ESC, a new Vietnamese\ndataset for Educational Sentiment Classification and Topic Classification,\ncurated from university forums, which offers more samples, richer class\ndiversity, longer texts, and broader vocabulary. In addition, we explore\nmultitask learning using encoder-only language models (BERT), in which we\nshowed that it achieves performance up to 83.7% and 79.8% accuracy for\nsentiment and topic classification tasks. We also benchmark our dataset and\nmodel with other datasets and models, including Large Language Models, and\ndiscuss these benchmarks. The dataset is publicly available at:\nhttps://huggingface.co/datasets/hung20gg/NEU-ESC.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23524v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.23524v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22907", "title": "MagShield: Towards Better Robustness in Sparse Inertial Motion Capture Under Magnetic Disturbances", "authors": ["Yunzhe Shao", "Xinyu Yi", "Lu Yin", "Shihui Guo", "Junhai Yong", "Feng Xu"], "summary": "This paper proposes a novel method called MagShield, designed to address the\nissue of magnetic interference in sparse inertial motion capture (MoCap)\nsystems. Existing Inertial Measurement Unit (IMU) systems are prone to\norientation estimation errors in magnetically disturbed environments, limiting\ntheir practical application in real-world scenarios. To address this problem,\nMagShield employs a \"detect-then-correct\" strategy, first detecting magnetic\ndisturbances through multi-IMU joint analysis, and then correcting orientation\nerrors using human motion priors. MagShield can be integrated with most\nexisting sparse inertial MoCap systems, improving their performance in\nmagnetically disturbed environments. Experimental results demonstrate that\nMagShield significantly enhances the accuracy of motion capture under magnetic\ninterference and exhibits good compatibility across different sparse inertial\nMoCap systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22907v1", "categories": ["cs.CV", "cs.GR"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22907v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23301", "title": "Parallax QAMA: Novel Downlink Multiple Access for MISO Systems with Simple Receivers", "authors": ["Jie Huang", "Ming Zhao", "Shengli Zhou", "Ling Qiu", "Jinkang Zhu"], "summary": "In this paper, we propose a novel downlink multiple access system with a\nmulti-antenna transmitter and two single-antenna receivers, inspired by the\nunderlying principles of hierarchical quadrature amplitude modulation (H-QAM)\nbased multiple access (QAMA) and space-division multiple access (SDMA). In the\nproposed scheme, coded bits from two users are split and assigned to one shared\nsymbol and two private symbols carried by different beams. Based on joint\nsymbol mapping of H-QAM constellations and phase-aligned precoding at the\ntransmitter, each receiver observes a different H-QAM constellation with Gray\nmapping, a unique parallax feature not shared by existing schemes. In addition\nto avoiding successive interference cancellation (SIC), each user independently\ndemodulates its own bits on separate I and Q branches with calculations based\non closed-form expressions. Hence the receiver complexity is on par with that\nof orthogonal multiple access (OMA), which is much lower than that in other\ncompeting alternatives such as non-orthogonal multiple access (NOMA) and\nrate-splitting multiple access (RSMA). We carry out system optimization and\ndetermine the achievable rate region. Numerical results show that the proposed\nsystem has a larger rate region relative to other benchmark schemes with\nreceivers not using SIC, and even achieves a comparable rate region to those\nbenchmark schemes with SIC receivers.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23301v1", "categories": ["cs.IT", "eess.SP", "math.IT"], "cate": "cs.IT", "url": "http://arxiv.org/abs/2506.23301v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23225", "title": "Masked Gated Linear Unit", "authors": ["Yukito Tajima", "Nakamasa Inoue", "Yusuke Sekikawa", "Ikuro Sato", "Rio Yokota"], "summary": "Gated Linear Units (GLUs) have become essential components in the\nfeed-forward networks of state-of-the-art Large Language Models (LLMs).\nHowever, they require twice as many memory reads compared to feed-forward\nlayers without gating, due to the use of separate weight matrices for the gate\nand value streams. To address this bottleneck, we introduce Masked Gated Linear\nUnits (MGLUs), a novel family of GLUs with an efficient kernel implementation.\nThe core contribution of MGLUs include: (1) the Mixture of Element-wise Gating\n(MoEG) architecture that learns multiple binary masks, each determining gate or\nvalue assignments at the element level on a single shared weight matrix\nresulting in reduced memory transfer, and (2) FlashMGLU, a hardware-friendly\nkernel that yields up to a 19.7 $\\times$ inference-time speed-up over a naive\nPyTorch MGLU and is 47% more memory-efficient and 34% faster than standard GLUs\ndespite added architectural complexity on an RTX5090 GPU. In LLM experiments,\nthe Swish-activated variant SwiMGLU preserves its memory advantages while\nmatching - or even surpassing - the downstream accuracy of the SwiGLU baseline.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23225v1", "categories": ["cs.LG", "cs.CL"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.23225v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.22468", "title": "Dimensionality Reduction on IoT Monitoring Data of Smart Building for Energy Consumption Forecasting", "authors": ["Konstantinos Koutras", "Agorakis Bompotas", "Constantinos Halkiopoulos", "Athanasios Kalogeras", "Christos Alexakos"], "summary": "The Internet of Things (IoT) plays a major role today in smart building\ninfrastructures, from simple smart-home applications, to more sophisticated\nindustrial type installations. The vast amounts of data generated from relevant\nsystems can be processed in different ways revealing important information.\nThis is especially true in the era of edge computing, when advanced data\nanalysis and decision-making is gradually moving to the edge of the network\nwhere devices are generally characterised by low computing resources. In this\ncontext, one of the emerging main challenges is related to maintaining data\nanalysis accuracy even with less data that can be efficiently handled by low\nresource devices. The present work focuses on correlation analysis of data\nretrieved from a pilot IoT network installation monitoring a small smart office\nby means of environmental and energy consumption sensors. The research\nmotivation was to find statistical correlation between the monitoring variables\nthat will allow the use of machine learning (ML) prediction algorithms for\nenergy consumption reducing input parameters. For this to happen, a series of\nhypothesis tests for the correlation of three different environmental variables\nwith the energy consumption were carried out. A total of ninety tests were\nperformed, thirty for each pair of variables. In these tests, p-values showed\nthe existence of strong or semi-strong correlation with two environmental\nvariables, and of a weak correlation with a third one. Using the proposed\nmethodology, we manage without examining the entire data set to exclude weak\ncorrelated variables while keeping the same score of accuracy.", "comment": "Version of submitted paper on 2023 IEEE International Smart Cities\n  Conference (ISC2), 1-6, 2023", "pdf_url": "http://arxiv.org/pdf/2506.22468v1", "categories": ["eess.SP", "cs.AI"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.22468v1", "date": "2025-06-19", "updated": "2025-06-19"}
{"id": "2506.23527", "title": "On Recipe Memorization and Creativity in Large Language Models: Is Your Model a Creative Cook, a Bad Cook, or Merely a Plagiator?", "authors": ["Jan Kvapil", "Martin Fajcik"], "summary": "This work-in-progress investigates the memorization, creativity, and nonsense\nfound in cooking recipes generated from Large Language Models (LLMs).\nPrecisely, we aim (i) to analyze memorization, creativity, and non-sense in\nLLMs using a small, high-quality set of human judgments and (ii) to evaluate\npotential approaches to automate such a human annotation in order to scale our\nstudy to hundreds of recipes. To achieve (i), we conduct a detailed human\nannotation on 20 preselected recipes generated by LLM (Mixtral), extracting\neach recipe's ingredients and step-by-step actions to assess which elements are\nmemorized--i.e., directly traceable to online sources possibly seen during\ntraining--and which arise from genuine creative synthesis or outright nonsense.\nWe find that Mixtral consistently reuses ingredients that can be found in\nonline documents, potentially seen during model training, suggesting strong\nreliance on memorized content. To achieve aim (ii) and scale our analysis\nbeyond small sample sizes and single LLM validation, we design an\n``LLM-as-judge'' pipeline that automates recipe generation, nonsense detection,\nparsing ingredients and recipe steps, and their annotation. For instance,\ncomparing its output against human annotations, the best ingredient extractor\nand annotator is Llama 3.1+Gemma 2 9B, achieving up to 78% accuracy on\ningredient matching. This automated framework enables large-scale\nquantification of memorization, creativity, and nonsense in generated recipes,\nproviding rigorous evidence of the models' creative capacities.", "comment": "13 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2506.23527v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.23527v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22908", "title": "Attention to Burstiness: Low-Rank Bilinear Prompt Tuning", "authors": ["Yuzhu Wang", "Manni Duan", "Shu Kong"], "summary": "Visual Prompt Tuning (VPT) is a parameter-efficient fune-tuning technique\nthat adapts a pre-trained vision Transformer (ViT) by learning a small set of\nparameters in the input space, known as prompts. In VPT, we uncover\n``burstiness'' in the values arising from the interaction of image patch\nembeddings, and the key and query projectors within Transformer's\nself-attention module. Furthermore, the values of patch embeddings and the key\nand query projectors exhibit Laplacian and hyper-Laplacian distribution,\nrespectively. Intuitively, these non-Gaussian distributions pose challenges for\nlearning prompts. To address this, we propose whitening these data,\nde-correlating them and equalizing their variance towards more Gaussian before\nlearning prompts. We derive the whitening matrix over random image patch\nembeddings and ViT's key and query projectors, and multiply it with the prompt\nto be learned in a bilinear manner. Surprisingly, this method significantly\naccelerates prompt tuning and boosts accuracy, e.g., $>$25 accuracy points on\nthe CUB dataset; interestingly, it learns ``bursty prompts''. Extending the\nbilinear model which is known to introduce burstiness, we present a compact,\nlow-rank version by learning two smaller matrices whose multiplication yields\nthe final prompts. We call the proposed methods Bilinear Prompt Tuning (BPT).\nExtensive experiments across multiple benchmark datasets demonstrate that BPT\nmethods not only outperform various VPT methods but also reduce parameter count\nand computation overhead.", "comment": "ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2506.22908v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22908v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23493", "title": "Securing the Sky: Integrated Satellite-UAV Physical Layer Security for Low-Altitude Wireless Networks", "authors": ["Jiahui Li", "Geng Sun", "Xiaoyu Sun", "Fang Mei", "Jingjing Wang", "Xiangwang Hou", "Daxin Tian", "Victor C. M. Leung"], "summary": "Low-altitude wireless networks (LAWNs) have garnered significant attention in\nthe forthcoming 6G networks. In LAWNs, satellites with wide coverage and\nunmanned aerial vehicles (UAVs) with flexible mobility can complement each\nother to form integrated satellite-UAV networks, providing ubiquitous and\nhigh-speed connectivity for low-altitude operations. However, the higher\nline-of-sight probability in low-altitude airspace increases transmission\nsecurity concerns. In this work, we present a collaborative beamforming-based\nphysical layer security scheme for LAWNs. We introduce the fundamental aspects\nof integrated satellite-UAV networks, physical layer security, UAV swarms, and\ncollaborative beamforming for LAWN applications. Following this, we highlight\nseveral opportunities for collaborative UAV swarm secure applications enabled\nby satellite networks, including achieving physical layer security in scenarios\ninvolving data dissemination, data relay, eavesdropper collusion, and imperfect\neavesdropper information. Next, we detail two case studies: a secure relay\nsystem and a two-way aerial secure communication framework specifically\ndesigned for LAWN environments. Simulation results demonstrate that these\nphysical layer security schemes are effective and beneficial for secure\nlow-altitude wireless communications. A short practicality analysis shows that\nthe proposed method is applicable to LAWN scenarios. Finally, we discuss\ncurrent challenges and future research directions for enhancing security in\nLAWNs.", "comment": "This paper has been submitted to IEEE Wireless Communications", "pdf_url": "http://arxiv.org/pdf/2506.23493v1", "categories": ["cs.NI", "eess.SP"], "cate": "cs.NI", "url": "http://arxiv.org/abs/2506.23493v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23266", "title": "Sub-MoE: Efficient Mixture-of-Expert LLMs Compression via Subspace Expert Merging", "authors": ["Lujun Li", "Zhu Qiyuan", "Jiacheng Wang", "Wei Li", "Hao Gu", "Sirui Han", "Yike Guo"], "summary": "Mixture of Experts (MoE) LLMs face significant obstacles due to their massive\nparameter scale, which imposes memory, storage, and deployment challenges.\nAlthough recent expert merging methods promise greater efficiency by\nconsolidating multiple experts, they are fundamentally hindered by parameter\nconflicts arising from expert specialization. In this paper, we present\nSub-MoE, a novel MoE compression framework via Subspace Expert Merging. Our key\ninsight is to perform joint Singular Value Decomposition (SVD) on concatenated\nexpert weights, reducing conflicting parameters by extracting shared\n$U$-matrices while enabling effective merging of the expert-specific $V$\ncomponents. Specifically, Sub-MoE consists of two innovative phases: (1)\nAdaptive Expert Clustering, which groups functionally coherent experts via\nK-means clustering based on cosine similarity of expert outputs; and (2)\nSubspace Expert Merging, which first enforces Experts Union Decomposition to\nderive the shared $U$-matrix across experts in the same group, then pursues\nfrequency-based merging for individual $V$-matrices, and finalizes expert\nreconstruction using the merged $V$-matrix. In this way, we align and fuse\nexperts in a shared subspace, and can be extended with intra-expert compression\nfor further inference optimization. Extensive experiments on Mixtral, DeepSeek,\nand Qwen-1.5|3 MoE LLMs demonstrate that our Sub-MoE significantly outperforms\nexisting expert pruning and merging methods. Notably, our Sub-MoE maintains\n96\\%|86\\% of original performance with 25\\%|50\\% expert reduction on\nMixtral-8x7B in zero-shot benchmarks. Code will be released at\nhttps://github.com/lliai/MoERazor.", "comment": "Work in progress, revisions ongoing", "pdf_url": "http://arxiv.org/pdf/2506.23266v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.23266v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.22477", "title": "Innovative Research on IoT Architecture and Robotic Operating Platforms: Applications of Large Language Models and Generative AI", "authors": ["Huiwen Han"], "summary": "This paper introduces an innovative design for robotic operating platforms,\nunderpinned by a transformative Internet of Things (IoT) architecture,\nseamlessly integrating cutting-edge technologies such as large language models\n(LLMs), generative AI, edge computing, and 5G networks. The proposed platform\naims to elevate the intelligence and autonomy of IoT systems and robotics,\nenabling them to make real-time decisions and adapt dynamically to changing\nenvironments. Through a series of compelling case studies across industries\nincluding smart manufacturing, healthcare, and service sectors, this paper\ndemonstrates the substantial potential of IoT-enabled robotics to optimize\noperational workflows, enhance productivity, and deliver innovative, scalable\nsolutions. By emphasizing the roles of LLMs and generative AI, the research\nhighlights how these technologies drive the evolution of intelligent robotics\nand IoT, shaping the future of industry-specific advancements. The findings not\nonly showcase the transformative power of these technologies but also offer a\nforward-looking perspective on their broader societal and industrial\nimplications, positioning them as catalysts for next-generation automation and\ntechnological convergence.", "comment": "Published in: 2024 6th International Conference on Robotics,\n  Intelligent Control and Artificial Intelligence (RICAI), IEEE Xplore, DOI:\n  10.1109/RICAI64321.2024.10911316. \\c{opyright} 2024 IEEE", "pdf_url": "http://arxiv.org/pdf/2506.22477v1", "categories": ["cs.NI", "cs.AI", "cs.ET", "cs.RO"], "cate": "cs.NI", "url": "http://arxiv.org/abs/2506.22477v1", "date": "2025-06-22", "updated": "2025-06-22"}
{"id": "2506.23601", "title": "Semantic-guided Diverse Decoding for Large Language Model", "authors": ["Weijie Shi", "Yue Cui", "Yaguang Wu", "Jingzhi Fang", "Shibo Zhang", "Mengze Li", "Sirui Han", "Jia Zhu", "Jiajie Xu", "Xiaofang Zhou"], "summary": "Diverse decoding of large language models is crucial for applications\nrequiring multiple semantically distinct responses, yet existing methods\nprimarily achieve lexical rather than semantic diversity. This limitation\nsignificantly constrains Best-of-N strategies, group-based reinforcement\nlearning, and data synthesis. While temperature sampling and diverse beam\nsearch modify token distributions or apply n-gram penalties, they fail to\nensure meaningful semantic differentiation. We introduce Semantic-guided\nDiverse Decoding (SemDiD), operating directly in embedding space that balances\nquality with diversity through three complementary mechanisms: orthogonal\ndirectional guidance, dynamic inter-group repulsion, and position-debiased\nprobability assessment. SemDiD harmonizes these competing objectives using\nadaptive gain functions and constraint optimization, ensuring both quality\nthresholds and maximal semantic differentiation. Experiments show SemDiD\nconsistently outperforms existing methods, improving Best-of-N coverage by\n1.4-5.2% across diverse tasks and accelerating RLHF training convergence by 15%\nwhile increasing accuracy by up to 2.1%.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23601v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.23601v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22930", "title": "Towards Explainable Bilingual Multimodal Misinformation Detection and Localization", "authors": ["Yiwei He", "Xiangtai Li", "Zhenglin Huang", "Yi Dong", "Hao Fei", "Jiangning Zhang", "Baoyuan Wu", "Guangliang Cheng"], "summary": "The increasing realism of multimodal content has made misinformation more\nsubtle and harder to detect, especially in news media where images are\nfrequently paired with bilingual (e.g., Chinese-English) subtitles. Such\ncontent often includes localized image edits and cross-lingual inconsistencies\nthat jointly distort meaning while remaining superficially plausible. We\nintroduce BiMi, a bilingual multimodal framework that jointly performs\nregion-level localization, cross-modal and cross-lingual consistency detection,\nand natural language explanation for misinformation analysis. To support\ngeneralization, BiMi integrates an online retrieval module that supplements\nmodel reasoning with up-to-date external context. We further release BiMiBench,\na large-scale and comprehensive benchmark constructed by systematically editing\nreal news images and subtitles, comprising 104,000 samples with realistic\nmanipulations across visual and linguistic modalities. To enhance\ninterpretability, we apply Group Relative Policy Optimization (GRPO) to improve\nexplanation quality, marking the first use of GRPO in this domain. Extensive\nexperiments demonstrate that BiMi outperforms strong baselines by up to +8.9 in\nclassification accuracy, +15.9 in localization accuracy, and +2.5 in\nexplanation BERTScore, advancing state-of-the-art performance in realistic,\nmultilingual misinformation detection. Code, models, and datasets will be\nreleased.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22930v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22930v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23560", "title": "Tensor Train Quantum State Tomography using Compressed Sensing", "authors": ["Shakir Showkat Sofi", "Charlotte Vermeylen", "Lieven De Lathauwer"], "summary": "Quantum state tomography (QST) is a fundamental technique for estimating the\nstate of a quantum system from measured data and plays a crucial role in\nevaluating the performance of quantum devices. However, standard estimation\nmethods become impractical due to the exponential growth of parameters in the\nstate representation. In this work, we address this challenge by parameterizing\nthe state using a low-rank block tensor train decomposition and demonstrate\nthat our approach is both memory- and computationally efficient. This framework\napplies to a broad class of quantum states that can be well approximated by\nlow-rank decompositions, including pure states, nearly pure states, and ground\nstates of Hamiltonians.", "comment": "Accepted for publication in EUSIPCO 2025", "pdf_url": "http://arxiv.org/pdf/2506.23560v1", "categories": ["quant-ph", "cs.AI", "eess.SP", "math.OC"], "cate": "quant-ph", "url": "http://arxiv.org/abs/2506.23560v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23274", "title": "Predicting thinking time in Reasoning models", "authors": ["Hans Peter Lynsg√∏e Raaschou-jensen", "Constanza Fierro", "Anders S√∏gaard"], "summary": "Reasoning models that produce long, hidden chains of thought have emerged as\npowerful tools for complex, reasoning-intensive\ntasks\\citep{deepseekai2025deepseekr1incentivizingreasoningcapability,\nopenai2024openaio1card}. However, this paradigm introduces a new user\nexperience challenge: users have little insight into how much time the model\nwill spend reasoning before returning an answer. This unpredictability, can\nlead to user frustration and is likely to compound as LLMs can produce\nincreasingly long tasks asynchronously\n\\citep{kwa2025measuringaiabilitycomplete}. In this paper, we introduce and\nevaluate methods for both online and offline prediction of model \"thinking\ntime,\" aiming to develop a practical \"progress bar for reasoning.\" We discuss\nthe implications for user interaction and future research directions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23274v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.23274v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.22479", "title": "Hindsight-Guided Momentum (HGM) Optimizer: An Approach to Adaptive Learning Rate", "authors": ["Krisanu Sarkar"], "summary": "We introduce Hindsight-Guided Momentum (HGM), a first-order optimization\nalgorithm that adaptively scales learning rates based on the directional\nconsistency of recent updates. Traditional adaptive methods, such as Adam or\nRMSprop , adapt learning dynamics using only the magnitude of gradients, often\noverlooking important geometric cues.Geometric cues refer to directional\ninformation, such as the alignment between current gradients and past updates,\nwhich reflects the local curvature and consistency of the optimization path.\nHGM addresses this by incorporating a hindsight mechanism that evaluates the\ncosine similarity between the current gradient and accumulated momentum. This\nallows it to distinguish between coherent and conflicting gradient directions,\nincreasing the learning rate when updates align and reducing it in regions of\noscillation or noise. The result is a more responsive optimizer that\naccelerates convergence in smooth regions of the loss surface while maintaining\nstability in sharper or more erratic areas. Despite this added adaptability,\nthe method preserves the computational and memory efficiency of existing\noptimizers.By more intelligently responding to the structure of the\noptimization landscape, HGM provides a simple yet effective improvement over\nexisting approaches, particularly in non-convex settings like that of deep\nneural network training.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22479v1", "categories": ["math.OC", "cs.AI", "cs.LG"], "cate": "math.OC", "url": "http://arxiv.org/abs/2506.22479v1", "date": "2025-06-22", "updated": "2025-06-22"}
{"id": "2506.23610", "title": "Evaluating the Simulation of Human Personality-Driven Susceptibility to Misinformation with LLMs", "authors": ["Manuel Pratelli", "Marinella Petrocchi"], "summary": "Large language models (LLMs) make it possible to generate synthetic\nbehavioural data at scale, offering an ethical and low-cost alternative to\nhuman experiments. Whether such data can faithfully capture psychological\ndifferences driven by personality traits, however, remains an open question. We\nevaluate the capacity of LLM agents, conditioned on Big-Five profiles, to\nreproduce personality-based variation in susceptibility to misinformation,\nfocusing on news discernment, the ability to judge true headlines as true and\nfalse headlines as false. Leveraging published datasets in which human\nparticipants with known personality profiles rated headline accuracy, we create\nmatching LLM agents and compare their responses to the original human patterns.\nCertain trait-misinformation associations, notably those involving\nAgreeableness and Conscientiousness, are reliably replicated, whereas others\ndiverge, revealing systematic biases in how LLMs internalize and express\npersonality. The results underscore both the promise and the limits of\npersonality-aligned LLMs for behavioral simulation, and offer new insight into\nmodeling cognitive diversity in artificial agents.", "comment": "pre-print version - paper actually under submission", "pdf_url": "http://arxiv.org/pdf/2506.23610v1", "categories": ["cs.CL", "cs.CY"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.23610v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22939", "title": "Utilizing a Novel Deep Learning Method for Scene Categorization in Remote Sensing Data", "authors": ["Ghufran A. Omran", "Wassan Saad Abduljabbar Hayale", "Ahmad AbdulQadir AlRababah", "Israa Ibraheem Al-Barazanchi", "Ravi Sekhar", "Pritesh Shah", "Sushma Parihar", "Harshavardhan Reddy Penubadi"], "summary": "Scene categorization (SC) in remotely acquired images is an important subject\nwith broad consequences in different fields, including catastrophe control,\necological observation, architecture for cities, and more. Nevertheless, its\nseveral apps, reaching a high degree of accuracy in SC from distant observation\ndata has demonstrated to be difficult. This is because traditional conventional\ndeep learning models require large databases with high variety and high levels\nof noise to capture important visual features. To address these problems, this\ninvestigation file introduces an innovative technique referred to as the\nCuttlefish Optimized Bidirectional Recurrent Neural Network (CO- BRNN) for type\nof scenes in remote sensing data. The investigation compares the execution of\nCO-BRNN with current techniques, including Multilayer Perceptron- Convolutional\nNeural Network (MLP-CNN), Convolutional Neural Network-Long Short Term Memory\n(CNN-LSTM), and Long Short Term Memory-Conditional Random Field (LSTM-CRF),\nGraph-Based (GB), Multilabel Image Retrieval Model (MIRM-CF), Convolutional\nNeural Networks Data Augmentation (CNN-DA). The results demonstrate that\nCO-BRNN attained the maximum accuracy of 97%, followed by LSTM-CRF with 90%,\nMLP-CNN with 85%, and CNN-LSTM with 80%. The study highlights the significance\nof physical confirmation to ensure the efficiency of satellite data.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22939v1", "categories": ["cs.CV", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22939v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23755", "title": "How Long Can I Transmit? A Mobility Aware mmWave-based UAV Communication Framework", "authors": ["Shawon Mitra", "Subhojit Sarkar", "Sasthi C. Ghosh"], "summary": "One primary focus of next generation wireless communication networks is the\nmillimeterwave (mmWave) spectrum, typically considered in the 30 GHz to 300 GHz\nfrequency range. Despite their promise of high data rates, mmWaves suffer from\nsevere attenuation while passing through obstacles. Unmanned aerial vehicles\n(UAVs) have been proposed to offset this limitation on account of their\nadditional degrees of freedom, which can be leveraged to provide line of sight\n(LoS) transmission paths. While some prior works have proposed analytical\nframeworks to compute the LoS probability for static ground users and a UAV,\nthe same is lacking for mobile users on the ground. In this paper, we consider\nthe popular Manhattan point line process (MPLP) to model an urban environment,\nwithin which a ground user moves with a known velocity for a small time\ninterval along the roads. We derive an expression for the expected duration of\nLoS between a static UAV in the air and a mobile ground user, and validate the\nsame through simulations. To demonstrate the efficacy of the proposed analysis,\nwe propose a simple user association algorithm that greedily assigns the UAVs\nto users with the highest expected LoS time, and show that it outperforms the\nexisting benchmark schemes that assign the users to the nearest UAVs with LoS\nwithout considering the user mobility.", "comment": "This article has been submitted in a reputed conference", "pdf_url": "http://arxiv.org/pdf/2506.23755v1", "categories": ["cs.NI", "eess.SP"], "cate": "cs.NI", "url": "http://arxiv.org/abs/2506.23755v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23280", "title": "BAPE: Learning an Explicit Bayes Classifier for Long-tailed Visual Recognition", "authors": ["Chaoqun Du", "Yulin Wang", "Shiji Song", "Gao Huang"], "summary": "Bayesian decision theory advocates the Bayes classifier as the optimal\napproach for minimizing the risk in machine learning problems. Current deep\nlearning algorithms usually solve for the optimal classifier by\n\\emph{implicitly} estimating the posterior probabilities, \\emph{e.g.}, by\nminimizing the Softmax cross-entropy loss. This simple methodology has been\nproven effective for meticulously balanced academic benchmark datasets.\nHowever, it is not applicable to the long-tailed data distributions in the real\nworld, where it leads to the gradient imbalance issue and fails to ensure the\nBayes optimal decision rule. To address these challenges, this paper presents a\nnovel approach (BAPE) that provides a more precise theoretical estimation of\nthe data distributions by \\emph{explicitly} modeling the parameters of the\nposterior probabilities and solving them with point estimation. Consequently,\nour method directly learns the Bayes classifier without gradient descent based\non Bayes' theorem, simultaneously alleviating the gradient imbalance and\nensuring the Bayes optimal decision rule. Furthermore, we propose a\nstraightforward yet effective \\emph{distribution adjustment} technique. This\nmethod enables the Bayes classifier trained from the long-tailed training set\nto effectively adapt to the test data distribution with an arbitrary imbalance\nfactor, thereby enhancing performance without incurring additional\ncomputational costs. In addition, we demonstrate the gains of our method are\northogonal to existing learning approaches for long-tailed scenarios, as they\nare mostly designed under the principle of \\emph{implicitly} estimating the\nposterior probabilities. Extensive empirical evaluations on CIFAR-10-LT,\nCIFAR-100-LT, ImageNet-LT, and iNaturalist demonstrate that our method\nsignificantly improves the generalization performance of popular deep networks,\ndespite its simplicity.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23280v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.23280v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.22485", "title": "AI Agents-as-Judge: Automated Assessment of Accuracy, Consistency, Completeness and Clarity for Enterprise Documents", "authors": ["Sudip Dasgupta", "Himanshu Shankar"], "summary": "This study presents a modular, multi-agent system for the automated review of\nhighly structured enterprise business documents using AI agents. Unlike prior\nsolutions focused on unstructured texts or limited compliance checks, this\nframework leverages modern orchestration tools such as LangChain, CrewAI,\nTruLens, and Guidance to enable section-by-section evaluation of documents for\naccuracy, consistency, completeness, and clarity. Specialized agents, each\nresponsible for discrete review criteria such as template compliance or factual\ncorrectness, operate in parallel or sequence as required. Evaluation outputs\nare enforced to a standardized, machine-readable schema, supporting downstream\nanalytics and auditability. Continuous monitoring and a feedback loop with\nhuman reviewers allow for iterative system improvement and bias mitigation.\n  Quantitative evaluation demonstrates that the AI Agent-as-Judge system\napproaches or exceeds human performance in key areas: achieving 99% information\nconsistency (vs. 92% for humans), halving error and bias rates, and reducing\naverage review time from 30 to 2.5 minutes per document, with a 95% agreement\nrate between AI and expert human judgment. While promising for a wide range of\nindustries, the study also discusses current limitations, including the need\nfor human oversight in highly specialized domains and the operational cost of\nlarge-scale LLM usage. The proposed system serves as a flexible, auditable, and\nscalable foundation for AI-driven document quality assurance in the enterprise\ncontext.", "comment": "17 pages, 2 system diagrams, 1 table, no prior conference publication", "pdf_url": "http://arxiv.org/pdf/2506.22485v1", "categories": ["cs.CL", "cs.AI", "68T07, 68T50", "I.2.1; I.2.3; I.2.7; H.3.3"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.22485v1", "date": "2025-06-23", "updated": "2025-06-23"}
{"id": "2506.23661", "title": "Robustness of Misinformation Classification Systems to Adversarial Examples Through BeamAttack", "authors": ["Arnisa Fazla", "Lucas Krauter", "David Guzman Piedrahita", "Andrianos Michail"], "summary": "We extend BeamAttack, an adversarial attack algorithm designed to evaluate\nthe robustness of text classification systems through word-level modifications\nguided by beam search. Our extensions include support for word deletions and\nthe option to skip substitutions, enabling the discovery of minimal\nmodifications that alter model predictions. We also integrate LIME to better\nprioritize word replacements. Evaluated across multiple datasets and victim\nmodels (BiLSTM, BERT, and adversarially trained RoBERTa) within the BODEGA\nframework, our approach achieves over a 99\\% attack success rate while\npreserving the semantic and lexical similarity of the original texts. Through\nboth quantitative and qualitative analysis, we highlight BeamAttack's\neffectiveness and its limitations. Our implementation is available at\nhttps://github.com/LucK1Y/BeamAttack", "comment": "12 pages main text, 27 pages total including references and\n  appendices. 13 figures, 10 tables. Accepted for publication in the LNCS\n  proceedings of CLEF 2025 (Best-of-Labs track)", "pdf_url": "http://arxiv.org/pdf/2506.23661v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.23661v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22955", "title": "YM-WML: A new Yolo-based segmentation Model with Weighted Multi-class Loss for medical imaging", "authors": ["Haniyeh Nikkhah", "Jafar Tanha", "Mahdi Zarrin", "SeyedEhsan Roshan", "Amin Kazempour"], "summary": "Medical image segmentation poses significant challenges due to class\nimbalance and the complex structure of medical images. To address these\nchallenges, this study proposes YM-WML, a novel model for cardiac image\nsegmentation. The model integrates a robust backbone for effective feature\nextraction, a YOLOv11 neck for multi-scale feature aggregation, and an\nattention-based segmentation head for precise and accurate segmentation. To\naddress class imbalance, we introduce the Weighted Multi-class Exponential\n(WME) loss function. On the ACDC dataset, YM-WML achieves a Dice Similarity\nCoefficient of 91.02, outperforming state-of-the-art methods. The model\ndemonstrates stable training, accurate segmentation, and strong generalization,\nsetting a new benchmark in cardiac segmentation tasks.", "comment": "Accepted at The 7th International conference on Pattern Recognition\n  and Image Analysis (IPRIA 2025)", "pdf_url": "http://arxiv.org/pdf/2506.22955v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22955v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23286", "title": "Not All Explanations for Deep Learning Phenomena Are Equally Valuable", "authors": ["Alan Jeffares", "Mihaela van der Schaar"], "summary": "Developing a better understanding of surprising or counterintuitive phenomena\nhas constituted a significant portion of deep learning research in recent\nyears. These include double descent, grokking, and the lottery ticket\nhypothesis -- among many others. Works in this area often develop ad hoc\nhypotheses attempting to explain these observed phenomena on an isolated,\ncase-by-case basis. This position paper asserts that, in many prominent cases,\nthere is little evidence to suggest that these phenomena appear in real-world\napplications and these efforts may be inefficient in driving progress in the\nbroader field. Consequently, we argue against viewing them as isolated puzzles\nthat require bespoke resolutions or explanations. However, despite this, we\nsuggest that deep learning phenomena do still offer research value by providing\nunique settings in which we can refine our broad explanatory theories of more\ngeneral deep learning principles. This position is reinforced by analyzing the\nresearch outcomes of several prominent examples of these phenomena from the\nrecent literature. We revisit the current norms in the research community in\napproaching these problems and propose practical recommendations for future\nresearch, aiming to ensure that progress on deep learning phenomena is well\naligned with the ultimate pragmatic goal of progress in the broader field of\ndeep learning.", "comment": "Accepted at ICML 2025 for oral presentation", "pdf_url": "http://arxiv.org/pdf/2506.23286v1", "categories": ["cs.LG", "cs.AI", "stat.ML"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.23286v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.22486", "title": "Hallucination Detection with Small Language Models", "authors": ["Ming Cheung"], "summary": "Since the introduction of ChatGPT, large language models (LLMs) have\ndemonstrated significant utility in various tasks, such as answering questions\nthrough retrieval-augmented generation. Context can be retrieved using a\nvectorized database, serving as a foundation for LLMs to generate responses.\nHowever, hallucinations in responses can undermine the reliability of LLMs in\npractical applications, and they are not easily detectable in the absence of\nground truth, particularly in question-and-answer scenarios. This paper\nproposes a framework that integrates multiple small language models to verify\nresponses generated by LLMs using the retrieved context from a vectorized\ndatabase. By breaking down the responses into individual sentences and\nutilizing the probability of generating \"Yes\" tokens from the outputs of\nmultiple models for a given set of questions, responses, and relevant context,\nhallucinations can be detected. The proposed framework is validated through\nexperiments with real datasets comprising over 100 sets of questions, answers,\nand contexts, including responses with fully and partially correct sentences.\nThe results demonstrate a 10\\% improvement in F1 scores for detecting correct\nresponses compared to hallucinations, indicating that multiple small language\nmodels can be effectively employed for answer verification, providing a\nscalable and efficient solution for both academic and practical applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22486v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.22486v1", "date": "2025-06-24", "updated": "2025-06-24"}
{"id": "2506.23662", "title": "Zero-Shot Contextual Embeddings via Offline Synthetic Corpus Generation", "authors": ["Philip Lippmann", "Jie Yang"], "summary": "Context-aware embedding methods boost retrieval accuracy by conditioning on\ncorpus statistics (e.g., term co-occurrence and topical patterns) extracted\nfrom neighboring documents. However, this context-aware approach requires\naccess to the target corpus or requires domain-specific finetuning, posing\npractical barriers in privacy-sensitive or resource-constrained settings. We\npresent ZEST, a zero-shot contextual adaptation framework that replaces real\ncorpus access with a one-time offline synthesis of a compact proxy. Given only\na handful exemplar documents representative of the general target domain, we\nuse a multi-step hierarchical procedure to generate a synthetic context corpus\nof several hundred documents that aims to emulate key domain-specific\ndistributions. At inference, the frozen context-aware encoder uses this proxy\ncorpus -- without any finetuning or target corpus access -- to produce\ndomain-adapted embeddings. Across the MTEB benchmark, ZEST's zero-shot\nsynthetic context adaptation using only five example documents performs within\n0.5% of models leveraging full target corpus access -- demonstrating remarkable\nefficacy without any retraining. ZEST thus provides a practical method for\ndeploying high-performance, adaptable embeddings in constrained environments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23662v1", "categories": ["cs.CL", "cs.IR"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.23662v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22960", "title": "Peccavi: Visual Paraphrase Attack Safe and Distortion Free Image Watermarking Technique for AI-Generated Images", "authors": ["Shreyas Dixit", "Ashhar Aziz", "Shashwat Bajpai", "Vasu Sharma", "Aman Chadha", "Vinija Jain", "Amitava Das"], "summary": "A report by the European Union Law Enforcement Agency predicts that by 2026,\nup to 90 percent of online content could be synthetically generated, raising\nconcerns among policymakers, who cautioned that \"Generative AI could act as a\nforce multiplier for political disinformation. The combined effect of\ngenerative text, images, videos, and audio may surpass the influence of any\nsingle modality.\" In response, California's Bill AB 3211 mandates the\nwatermarking of AI-generated images, videos, and audio. However, concerns\nremain regarding the vulnerability of invisible watermarking techniques to\ntampering and the potential for malicious actors to bypass them entirely.\nGenerative AI-powered de-watermarking attacks, especially the newly introduced\nvisual paraphrase attack, have shown an ability to fully remove watermarks,\nresulting in a paraphrase of the original image. This paper introduces PECCAVI,\nthe first visual paraphrase attack-safe and distortion-free image watermarking\ntechnique. In visual paraphrase attacks, an image is altered while preserving\nits core semantic regions, termed Non-Melting Points (NMPs). PECCAVI\nstrategically embeds watermarks within these NMPs and employs multi-channel\nfrequency domain watermarking. It also incorporates noisy burnishing to counter\nreverse-engineering efforts aimed at locating NMPs to disrupt the embedded\nwatermark, thereby enhancing durability. PECCAVI is model-agnostic. All\nrelevant resources and codes will be open-sourced.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22960v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22960v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23287", "title": "Hierarchical Quantized Diffusion Based Tree Generation Method for Hierarchical Representation and Lineage Analysis", "authors": ["Zelin Zang", "WenZhe Li", "Fei Chen", "Yongjie Xu", "Chang Yu", "Zhen Lei", "Stan Z. Li"], "summary": "In single-cell research, tracing and analyzing high-throughput single-cell\ndifferentiation trajectories is crucial for understanding complex biological\nprocesses. Key to this is the modeling and generation of hierarchical data that\nrepresents the intrinsic structure within datasets. Traditional methods face\nlimitations in terms of computational cost, performance, generative capacity,\nand stability. Recent VAEs based approaches have made strides in addressing\nthese challenges but still require specialized network modules for each tree\nbranch, limiting their stability and ability to capture deep hierarchical\nrelationships. To overcome these challenges, we introduce diffusion-based\napproach called HDTree. HDTree captures tree relationships within a\nhierarchical latent space using a unified hierarchical codebook and quantized\ndiffusion processes to model tree node transitions. This method improves\nstability by eliminating branch-specific modules and enhancing generative\ncapacity through gradual hierarchical changes simulated by the diffusion\nprocess. HDTree's effectiveness is demonstrated through comparisons on both\ngeneral-purpose and single-cell datasets, where it outperforms existing methods\nin terms of accuracy and performance. These contributions provide a new tool\nfor hierarchical lineage analysis, enabling more accurate and efficient\nmodeling of cellular differentiation paths and offering insights for downstream\nbiological tasks. The code of HDTree is available at anonymous link\nhttps://anonymous.4open.science/r/code_HDTree_review-A8DB.", "comment": "9 pages, 6 figures, under review", "pdf_url": "http://arxiv.org/pdf/2506.23287v1", "categories": ["cs.LG", "q-bio.QM"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.23287v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.22487", "title": "AGI Enabled Solutions For IoX Layers Bottlenecks In Cyber-Physical-Social-Thinking Space", "authors": ["Amar Khelloufi", "Huansheng Ning", "Sahraoui Dhelim", "Jianguo Ding"], "summary": "The integration of the Internet of Everything (IoX) and Artificial General\nIntelligence (AGI) has given rise to a transformative paradigm aimed at\naddressing critical bottlenecks across sensing, network, and application layers\nin Cyber-Physical-Social Thinking (CPST) ecosystems. In this survey, we provide\na systematic and comprehensive review of AGI-enhanced IoX research, focusing on\nthree key components: sensing-layer data management, network-layer protocol\noptimization, and application-layer decision-making frameworks. Specifically,\nthis survey explores how AGI can mitigate IoX bottlenecks challenges by\nleveraging adaptive sensor fusion, edge preprocessing, and selective attention\nmechanisms at the sensing layer, while resolving network-layer issues such as\nprotocol heterogeneity and dynamic spectrum management, neuro-symbolic\nreasoning, active inference, and causal reasoning, Furthermore, the survey\nexamines AGI-enabled frameworks for managing identity and relationship\nexplosion. Key findings suggest that AGI-driven strategies, such as adaptive\nsensor fusion, edge preprocessing, and semantic modeling, offer novel solutions\nto sensing-layer data overload, network-layer protocol heterogeneity, and\napplication-layer identity explosion. The survey underscores the importance of\ncross-layer integration, quantum-enabled communication, and ethical governance\nframeworks for future AGI-enabled IoX systems. Finally, the survey identifies\nunresolved challenges, such as computational requirements, scalability, and\nreal-world validation, calling for further research to fully realize AGI's\npotential in addressing IoX bottlenecks. we believe AGI-enhanced IoX is\nemerging as a critical research field at the intersection of interconnected\nsystems and advanced AI.", "comment": "31 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2506.22487v1", "categories": ["cs.NI", "cs.AI"], "cate": "cs.NI", "url": "http://arxiv.org/abs/2506.22487v1", "date": "2025-06-24", "updated": "2025-06-24"}
{"id": "2506.23667", "title": "L0: Reinforcement Learning to Become General Agents", "authors": ["Junjie Zhang", "Jingyi Xi", "Zhuoyang Song", "Junyu Lu", "Yuhua Ke", "Ting Sun", "Yukun Yang", "Jiaxing Zhang", "Songxin Zhang", "Zejian Xie"], "summary": "Training large language models (LLMs) to act as autonomous agents for\nmulti-turn, long-horizon tasks remains significant challenges in scalability\nand training efficiency. To address this, we introduce L-Zero (L0), a scalable,\nend-to-end training pipeline for general-purpose agents. Featuring a low-cost,\nextensible, and sandboxed concurrent agent worker pool, L0 lowers the barrier\nfor applying reinforcement learning in complex environments. We also introduce\nNB-Agent, the agent scaffold within L0, which operates in a \"code-as-action\"\nfashion via a Read-Eval-Print-Loop (REPL). We evaluate L0 on factuality\nquestion-answering benchmarks. Our experiments demonstrate that a base model\ncan develop robust problem-solving skills using solely Reinforcement Learning\nwith Verifiable Rewards (RLVR). On the Qwen2.5-7B-Instruct model, our method\nboosts accuracy on SimpleQA from 30 % to 80 % and on HotpotQA from 22 % to 41\n%. We have open-sourced the entire L0 system, including our L0 series models,\nthe NB-Agent, a complete training pipeline, and the corresponding training\nrecipes on (https://github.com/cmriat/l0).", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23667v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.23667v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22967", "title": "ActAlign: Zero-Shot Fine-Grained Video Classification via Language-Guided Sequence Alignment", "authors": ["Amir Aghdam", "Vincent Tao Hu"], "summary": "We address the task of zero-shot fine-grained video classification, where no\nvideo examples or temporal annotations are available for unseen action classes.\nWhile contrastive vision-language models such as SigLIP demonstrate strong\nopen-set recognition via mean-pooled image-text similarity, they fail to\ncapture the temporal structure critical for distinguishing fine-grained\nactivities. We introduce ActAlign, a zero-shot framework that formulates video\nclassification as sequence alignment. For each class, a large language model\ngenerates an ordered sub-action sequence, which is aligned with video frames\nusing Dynamic Time Warping (DTW) in a shared embedding space. Without any\nvideo-text supervision or fine-tuning, ActAlign achieves 30.5% accuracy on the\nextremely challenging ActionAtlas benchmark, where human accuracy is only\n61.6%. ActAlign outperforms billion-parameter video-language models while using\napproximately 8x less parameters. These results demonstrate that structured\nlanguage priors, combined with classical alignment techniques, offer a scalable\nand general approach to unlocking the open-set recognition potential of\nvision-language models for fine-grained video understanding.", "comment": "Preprint manuscript - Project page:\n  https://github.com/aghdamamir/act-align", "pdf_url": "http://arxiv.org/pdf/2506.22967v1", "categories": ["cs.CV", "cs.LG", "cs.MM", "I.2.10; I.2.7"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22967v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23339", "title": "VALID-Mol: a Systematic Framework for Validated LLM-Assisted Molecular Design", "authors": ["Malikussaid", "Hilal Hudan Nuha"], "summary": "Large Language Models (LLMs) demonstrate remarkable potential for scientific\ndiscovery, but their application in domains requiring factual accuracy and\ndomain-specific constraints remains challenging. In molecular design for drug\ndiscovery, LLMs can suggest creative molecular modifications but often produce\nchemically invalid or impractical structures. We present VALID-Mol, a\nsystematic framework for integrating chemical validation with LLM-driven\nmolecular design that increases the rate of generating valid chemical\nstructures from 3% to 83%. Our approach combines methodical prompt engineering,\nautomated chemical validation, and a fine-tuned domain-adapted LLM to ensure\nreliable generation of synthesizable molecules with improved properties. Beyond\nthe specific implementation, we contribute a generalizable methodology for\nscientifically-constrained LLM applications, with quantifiable reliability\nimprovements. Computational predictions suggest our framework can generate\npromising candidates for synthesis with up to 17-fold computationally predicted\nimprovements in target affinity while maintaining synthetic accessibility. We\nprovide a detailed analysis of our prompt engineering process, validation\narchitecture, and fine-tuning approach, offering a reproducible blueprint for\napplying LLMs to other scientific domains where domain-specific validation is\nessential.", "comment": "16 pages, 1 figure, 5 algorithms, 7 tables, to be published in ICSECS\n  Conference 2025, unabridged version", "pdf_url": "http://arxiv.org/pdf/2506.23339v1", "categories": ["cs.LG", "cs.AI", "physics.chem-ph", "q-bio.QM"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.23339v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.22491", "title": "PromptAug: Fine-grained Conflict Classification Using Data Augmentation", "authors": ["Oliver Warke", "Joemon M. Jose", "Faegheh Hasibi", "Jan Breitsohl"], "summary": "Given the rise of conflicts on social media, effective classification models\nto detect harmful behaviours are essential. Following the\ngarbage-in-garbage-out maxim, machine learning performance depends heavily on\ntraining data quality. However, high-quality labelled data, especially for\nnuanced tasks like identifying conflict behaviours, is limited, expensive, and\ndifficult to obtain. Additionally, as social media platforms increasingly\nrestrict access to research data, text data augmentation is gaining attention\nas an alternative to generate training data. Augmenting conflict-related data\nposes unique challenges due to Large Language Model (LLM) guardrails that\nprevent generation of offensive content. This paper introduces PromptAug, an\ninnovative LLM-based data augmentation method. PromptAug achieves statistically\nsignificant improvements of 2% in both accuracy and F1-score on conflict and\nemotion datasets. To thoroughly evaluate PromptAug against other data\naugmentation methods we conduct a robust evaluation using extreme data scarcity\nscenarios, quantitative diversity analysis and a qualitative thematic analysis.\nThe thematic analysis identifies four problematic patterns in augmented text:\nLinguistic Fluidity, Humour Ambiguity, Augmented Content Ambiguity, and\nAugmented Content Misinterpretation.\n  Overall, this work presents PromptAug as an effective method for augmenting\ndata in sensitive tasks like conflict detection, offering a unique,\ninterdisciplinary evaluation grounded in both natural language processing and\nsocial science methodology.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22491v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "I.2.7; J.4; K.4.2"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.22491v1", "date": "2025-06-24", "updated": "2025-06-24"}
{"id": "2506.23735", "title": "AutoEvoEval: An Automated Framework for Evolving Close-Ended LLM Evaluation Data", "authors": ["JiaRu Wu", "Mingwei Liu"], "summary": "Large language models (LLMs) have shown remarkable performance on various\ntasks, but existing evaluation benchmarks are often static and insufficient to\nfully assess their robustness and generalization in realistic scenarios. Prior\nwork using evolutionary or adversarial data augmentation has improved\nevaluation diversity but lacks systematic control over perturbation types and\nmulti-step complexity, limiting comprehensive robustness analysis. To address\nthese gaps, we propose AutoEvoEval, an evolution-based evaluation framework for\nclose-ended tasks such as multi-choice question answering. AutoEvoEval\nintroduces 22 interpretable atomic evolution operations and supports\nmulti-round compositions, enabling controlled generation of diverse,\nchallenging, and realistic test samples. We conduct extensive experiments\naddressing four research questions on a broad set of open- and closed-source\nLLMs. Our results show that atomic operations cause an average accuracy drop of\n7.283\\%, with structure-disrupting or misleading semantic edits causing the\nlargest declines. Model sensitivities vary significantly for the same\nperturbation, and combining multiple evolution steps amplifies adversarial\neffects by up to 52.932\\%. These findings suggest current benchmarks may\noverestimate true model generalization and emphasize the need for\nevolution-aware robustness evaluation. Code and resources are available at:\nhttps://github.com/SYSUSELab/AutoEvoEval.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23735v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.23735v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22979", "title": "Probabilistic Prototype Calibration of Vision-Language Models for Generalized Few-shot Semantic Segmentation", "authors": ["Jie Liu", "Jiayi Shen", "Pan Zhou", "Jan-Jakob Sonke", "Efstratios Gavves"], "summary": "Generalized Few-Shot Semantic Segmentation (GFSS) aims to extend a\nsegmentation model to novel classes with only a few annotated examples while\nmaintaining performance on base classes. Recently, pretrained vision-language\nmodels (VLMs) such as CLIP have been leveraged in GFSS to improve\ngeneralization on novel classes through multi-modal prototypes learning.\nHowever, existing prototype-based methods are inherently deterministic,\nlimiting the adaptability of learned prototypes to diverse samples,\nparticularly for novel classes with scarce annotations. To address this, we\npropose FewCLIP, a probabilistic prototype calibration framework over\nmulti-modal prototypes from the pretrained CLIP, thus providing more adaptive\nprototype learning for GFSS. Specifically, FewCLIP first introduces a prototype\ncalibration mechanism, which refines frozen textual prototypes with learnable\nvisual calibration prototypes, leading to a more discriminative and adaptive\nrepresentation. Furthermore, unlike deterministic prototype learning\ntechniques, FewCLIP introduces distribution regularization over these\ncalibration prototypes. This probabilistic formulation ensures structured and\nuncertainty-aware prototype learning, effectively mitigating overfitting to\nlimited novel class data while enhancing generalization. Extensive experimental\nresults on PASCAL-5$^i$ and COCO-20$^i$ datasets demonstrate that our proposed\nFewCLIP significantly outperforms state-of-the-art approaches across both GFSS\nand class-incremental setting. The code is available at\nhttps://github.com/jliu4ai/FewCLIP.", "comment": "ICCV2025 Proceeding", "pdf_url": "http://arxiv.org/pdf/2506.22979v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22979v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23349", "title": "A case for data valuation transparency via DValCards", "authors": ["Keziah Naggita", "Julienne LaChance"], "summary": "Following the rise in popularity of data-centric machine learning (ML),\nvarious data valuation methods have been proposed to quantify the contribution\nof each datapoint to desired ML model performance metrics (e.g., accuracy).\nBeyond the technical applications of data valuation methods (e.g., data\ncleaning, data acquisition, etc.), it has been suggested that within the\ncontext of data markets, data buyers might utilize such methods to fairly\ncompensate data owners. Here we demonstrate that data valuation metrics are\ninherently biased and unstable under simple algorithmic design choices,\nresulting in both technical and ethical implications. By analyzing 9 tabular\nclassification datasets and 6 data valuation methods, we illustrate how (1)\ncommon and inexpensive data pre-processing techniques can drastically alter\nestimated data values; (2) subsampling via data valuation metrics may increase\nclass imbalance; and (3) data valuation metrics may undervalue underrepresented\ngroup data. Consequently, we argue in favor of increased transparency\nassociated with data valuation in-the-wild and introduce the novel Data\nValuation Cards (DValCards) framework towards this aim. The proliferation of\nDValCards will reduce misuse of data valuation metrics, including in data\npricing, and build trust in responsible ML systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23349v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.23349v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.22492", "title": "Report on NSF Workshop on Science of Safe AI", "authors": ["Rajeev Alur", "Greg Durrett", "Hadas Kress-Gazit", "Corina PƒÉsƒÉreanu", "Ren√© Vidal"], "summary": "Recent advances in machine learning, particularly the emergence of foundation\nmodels, are leading to new opportunities to develop technology-based solutions\nto societal problems. However, the reasoning and inner workings of today's\ncomplex AI models are not transparent to the user, and there are no safety\nguarantees regarding their predictions. Consequently, to fulfill the promise of\nAI, we must address the following scientific challenge: how to develop AI-based\nsystems that are not only accurate and performant but also safe and\ntrustworthy?\n  The criticality of safe operation is particularly evident for autonomous\nsystems for control and robotics, and was the catalyst for the Safe Learning\nEnabled Systems (SLES) program at NSF. For the broader class of AI\napplications, such as users interacting with chatbots and clinicians receiving\ntreatment recommendations, safety is, while no less important, less\nwell-defined with context-dependent interpretations. This motivated the\norganization of a day-long workshop, held at University of Pennsylvania on\nFebruary 26, 2025, to bring together investigators funded by the NSF SLES\nprogram with a broader pool of researchers studying AI safety. This report is\nthe result of the discussions in the working groups that addressed different\naspects of safety at the workshop. The report articulates a new research agenda\nfocused on developing theory, methods, and tools that will provide the\nfoundations of the next generation of AI-enabled systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22492v1", "categories": ["cs.CY", "cs.AI"], "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.22492v1", "date": "2025-06-24", "updated": "2025-06-24"}
{"id": "2506.23743", "title": "Positional Bias in Binary Question Answering: How Uncertainty Shapes Model Preferences", "authors": ["Tiziano Labruna", "Simone Gallo", "Giovanni Da San Martino"], "summary": "Positional bias in binary question answering occurs when a model\nsystematically favors one choice over another based solely on the ordering of\npresented options. In this study, we quantify and analyze positional bias\nacross five large language models under varying degrees of answer uncertainty.\nWe re-adapted the SQuAD-it dataset by adding an extra incorrect answer option\nand then created multiple versions with progressively less context and more\nout-of-context answers, yielding datasets that range from low to high\nuncertainty. Additionally, we evaluate two naturally higher-uncertainty\nbenchmarks: (1) WebGPT - question pairs with unequal human-assigned quality\nscores, and (2) Winning Arguments - where models predict the more persuasive\nargument in Reddit's r/ChangeMyView exchanges. Across each dataset, the order\nof the \"correct\" (or higher-quality/persuasive) option is systematically\nflipped (first placed in position 1, then in position 2) to compute both\nPreference Fairness and Position Consistency. We observe that positional bias\nis nearly absent under low-uncertainty conditions, but grows exponentially when\nit becomes doubtful to decide which option is correct.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23743v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.23743v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22982", "title": "Revisiting CroPA: A Reproducibility Study and Enhancements for Cross-Prompt Adversarial Transferability in Vision-Language Models", "authors": ["Atharv Mittal", "Agam Pandey", "Amritanshu Tiwari", "Sukrit Jindal", "Swadesh Swain"], "summary": "Large Vision-Language Models (VLMs) have revolutionized computer vision,\nenabling tasks such as image classification, captioning, and visual question\nanswering. However, they remain highly vulnerable to adversarial attacks,\nparticularly in scenarios where both visual and textual modalities can be\nmanipulated. In this study, we conduct a comprehensive reproducibility study of\n\"An Image is Worth 1000 Lies: Adversarial Transferability Across Prompts on\nVision-Language Models\" validating the Cross-Prompt Attack (CroPA) and\nconfirming its superior cross-prompt transferability compared to existing\nbaselines. Beyond replication we propose several key improvements: (1) A novel\ninitialization strategy that significantly improves Attack Success Rate (ASR).\n(2) Investigate cross-image transferability by learning universal\nperturbations. (3) A novel loss function targeting vision encoder attention\nmechanisms to improve generalization. Our evaluation across prominent VLMs --\nincluding Flamingo, BLIP-2, and InstructBLIP as well as extended experiments on\nLLaVA validates the original results and demonstrates that our improvements\nconsistently boost adversarial effectiveness. Our work reinforces the\nimportance of studying adversarial vulnerabilities in VLMs and provides a more\nrobust framework for generating transferable adversarial examples, with\nsignificant implications for understanding the security of VLMs in real-world\napplications.", "comment": "Accepted to MLRC 2025", "pdf_url": "http://arxiv.org/pdf/2506.22982v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22982v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23358", "title": "Federated Timeline Synthesis: Scalable and Private Methodology For Model Training and Deployment", "authors": ["Pawel Renc", "Michal K. Grzeszczyk", "Linglong Qian", "Nassim Oufattole", "Jeff Rasley", "Arkadiusz Sitek"], "summary": "We present Federated Timeline Synthesis (FTS), a novel framework for training\ngenerative foundation models across distributed timeseries data applied to\nelectronic health records (EHR). At its core, FTS represents patient history as\ntokenized Patient Health Timelines (PHTs), language-agnostic sequences encoding\ntemporal, categorical, and continuous clinical information. Each institution\ntrains an autoregressive transformer on its local PHTs and transmits only model\nweights to a central server. The server uses the generators to synthesize a\nlarge corpus of trajectories and train a Global Generator (GG), enabling\nzero-shot inference via Monte Carlo simulation of future PHTs. We evaluate FTS\non five clinically meaningful prediction tasks using MIMIC-IV data, showing\nthat models trained on synthetic data generated by GG perform comparably to\nthose trained on real data. FTS offers strong privacy guarantees, scalability\nacross institutions, and extensibility to diverse prediction and simulation\ntasks especially in healthcare, including counterfactual inference, early\nwarning detection, and synthetic trial design.", "comment": "conference paper", "pdf_url": "http://arxiv.org/pdf/2506.23358v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.23358v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.22495", "title": "Masked Autoencoders that Feel the Heart: Unveiling Simplicity Bias for ECG Analyses", "authors": ["He-Yang Xu", "Hongxiang Gao", "Yuwen Li", "Xiu-Shen Wei", "Chengyu Liu"], "summary": "The diagnostic value of electrocardiogram (ECG) lies in its dynamic\ncharacteristics, ranging from rhythm fluctuations to subtle waveform\ndeformations that evolve across time and frequency domains. However, supervised\nECG models tend to overfit dominant and repetitive patterns, overlooking\nfine-grained but clinically critical cues, a phenomenon known as Simplicity\nBias (SB), where models favor easily learnable signals over subtle but\ninformative ones. In this work, we first empirically demonstrate the presence\nof SB in ECG analyses and its negative impact on diagnostic performance, while\nsimultaneously discovering that self-supervised learning (SSL) can alleviate\nit, providing a promising direction for tackling the bias. Following the SSL\nparadigm, we propose a novel method comprising two key components: 1)\nTemporal-Frequency aware Filters to capture temporal-frequency features\nreflecting the dynamic characteristics of ECG signals, and 2) building on this,\nMulti-Grained Prototype Reconstruction for coarse and fine representation\nlearning across dual domains, further mitigating SB. To advance SSL in ECG\nanalyses, we curate a large-scale multi-site ECG dataset with 1.53 million\nrecordings from over 300 clinical centers. Experiments on three downstream\ntasks across six ECG datasets demonstrate that our method effectively reduces\nSB and achieves state-of-the-art performance. Code and dataset will be released\npublicly.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22495v1", "categories": ["eess.SP", "cs.AI", "cs.LG"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.22495v1", "date": "2025-06-25", "updated": "2025-06-25"}
{"id": "2506.23840", "title": "Do Thinking Tokens Help or Trap? Towards More Efficient Large Reasoning Model", "authors": ["Bowen Ding", "Yuhan Chen", "Futing Wang", "Lingfeng Ming", "Tao Lin"], "summary": "Large Reasoning Models (LRMs) excel at solving complex problems but face an\noverthinking dilemma. When handling simple tasks, they often produce verbose\nresponses overloaded with thinking tokens (e.g., wait, however). These tokens\ntrigger unnecessary high-level reasoning behaviors like reflection and\nbacktracking, reducing efficiency. In this work, our pilot study reveals that\nthese thinking-token-induced behaviors are not essential for effective\nproblem-solving and may even hinder correct reasoning within constrained token\nbudgets. We identify this phenomenon as the thinking trap. To mitigate this\nissue, we propose Dual Policy Preference Optimization (DuP-PO), a novel\nalgorithm featuring: (1) A rollout sampling strategy that guarantees balanced\nexposure to responses with and without thinking tokens; (2) A fine-grained\nadvantage control technique to dynamically regulate the prediction of target\ntokens; (3) A policy shaping method ensuring stable gradient contributions from\nthinking tokens. Experimental results on five popular math reasoning benchmarks\nshow that DuP-PO performs well on the popular LRM, which significantly improves\ntheir token efficiency during reasoning, while achieving superior performance\nof the base model.", "comment": "13 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2506.23840v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.23840v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23004", "title": "A Novel Frame Identification and Synchronization Technique for Smartphone Visible Light Communication Systems Based on Convolutional Neural Networks", "authors": ["Vaigai Nayaki Yokar", "Hoa Le-Minh", "Xicong Li", "Wai Lok Woo", "Luis Nero Alves", "Stanislav Zvanovec", "Tran The Son", "Zabih Ghassemlooy"], "summary": "This paper proposes a novel, robust, and lightweight supervised Convolutional\nNeural Network (CNN)-based technique for frame identification and\nsynchronization, designed to enhance short-link communication performance in a\nscreen-to-camera (S2C) based visible light communication (VLC) system.\nDeveloped using Python and the TensorFlow Keras framework, the proposed CNN\nmodel was trained through three real-time experimental investigations conducted\nin Jupyter Notebook. These experiments incorporated a dataset created from\nscratch to address various real-time challenges in S2C communication, including\nblurring, cropping, and rotated images in mobility scenarios. Overhead frames\nwere introduced for synchronization, which leads to enhanced system\nperformance. The experimental results demonstrate that the proposed model\nachieves an overall accuracy of approximately 98.74%, highlighting its\neffectiveness in identifying and synchronizing frames in S2C VLC systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23004v1", "categories": ["cs.CV", "eess.IV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23004v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23374", "title": "When Additive Noise Meets Unobserved Mediators: Bivariate Denoising Diffusion for Causal Discovery", "authors": ["Dominik Meier", "Sujai Hiremath", "Promit Ghosal", "Kyra Gan"], "summary": "Distinguishing cause and effect from bivariate observational data is a\nfoundational problem in many disciplines, but challenging without additional\nassumptions. Additive noise models (ANMs) are widely used to enable\nsample-efficient bivariate causal discovery. However, conventional ANM-based\nmethods fail when unobserved mediators corrupt the causal relationship between\nvariables. This paper makes three key contributions: first, we rigorously\ncharacterize why standard ANM approaches break down in the presence of\nunmeasured mediators. Second, we demonstrate that prior solutions for hidden\nmediation are brittle in finite sample settings, limiting their practical\nutility. To address these gaps, we propose Bivariate Denoising Diffusion (BiDD)\nfor causal discovery, a method designed to handle latent noise introduced by\nunmeasured mediators. Unlike prior methods that infer directionality through\nmean squared error loss comparisons, our approach introduces a novel\nindependence test statistic: during the noising and denoising processes for\neach variable, we condition on the other variable as input and evaluate the\nindependence of the predicted noise relative to this input. We prove asymptotic\nconsistency of BiDD under the ANM, and conjecture that it performs well under\nhidden mediation. Experiments on synthetic and real-world data demonstrate\nconsistent performance, outperforming existing methods in mediator-corrupted\nsettings while maintaining strong performance in mediator-free settings.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23374v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.23374v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.22496", "title": "Mitigating Gambling-Like Risk-Taking Behaviors in Large Language Models: A Behavioral Economics Approach to AI Safety", "authors": ["Y. Du"], "summary": "Large Language Models (LLMs) exhibit systematic risk-taking behaviors\nanalogous to those observed in gambling psychology, including overconfidence\nbias, loss-chasing tendencies, and probability misjudgment. Drawing from\nbehavioral economics and prospect theory, we identify and formalize these\n\"gambling-like\" patterns where models sacrifice accuracy for high-reward\noutputs, exhibit escalating risk-taking after errors, and systematically\nmiscalibrate uncertainty. We propose the Risk-Aware Response Generation (RARG)\nframework, incorporating insights from gambling research to address these\nbehavioral biases through risk-calibrated training, loss-aversion mechanisms,\nand uncertainty-aware decision making. Our approach introduces novel evaluation\nparadigms based on established gambling psychology experiments, including AI\nadaptations of the Iowa Gambling Task and probability learning assessments.\nExperimental results demonstrate measurable reductions in gambling-like\nbehaviors: 18.7\\% decrease in overconfidence bias, 24.3\\% reduction in\nloss-chasing tendencies, and improved risk calibration across diverse\nscenarios. This work establishes the first systematic framework for\nunderstanding and mitigating gambling psychology patterns in AI systems.", "comment": "7 pages", "pdf_url": "http://arxiv.org/pdf/2506.22496v1", "categories": ["cs.CY", "cs.AI", "cs.CL"], "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.22496v1", "date": "2025-06-25", "updated": "2025-06-25"}
{"id": "2506.23864", "title": "Garbage In, Reasoning Out? Why Benchmark Scores are Unreliable and What to Do About It", "authors": ["Seyed Mahed Mousavi", "Edoardo Cecchinato", "Lucia Hornikova", "Giuseppe Riccardi"], "summary": "We conduct a systematic audit of three widely used reasoning benchmarks,\nSocialIQa, FauxPas-EAI, and ToMi, and uncover pervasive flaws in both benchmark\nitems and evaluation methodology. Using five LLMs (GPT-{3, 3.5, 4, o1}, and\nLLaMA 3.1) as diagnostic tools, we identify structural, semantic, and pragmatic\nissues in benchmark design (e.g., duplicated items, ambiguous wording, and\nimplausible answers), as well as scoring procedures that prioritize output form\nover reasoning process. Through systematic human annotation and re-evaluation\non cleaned benchmark subsets, we find that model scores often improve not due\nto due to erratic surface wording variations and not to improved reasoning.\nInfact, further analyses show that model performance is highly sensitive to\nminor input variations such as context availability and phrasing, revealing\nthat high scores may reflect alignment with format-specific cues rather than\nconsistent inference based on the input. These findings challenge the validity\nof current benchmark-based claims about reasoning in LLMs, and highlight the\nneed for evaluation protocols that assess reasoning as a process of drawing\ninference from available information, rather than as static output selection.\nWe release audited data and evaluation tools to support more interpretable and\ndiagnostic assessments of model reasoning.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23864v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.23864v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23009", "title": "MusiXQA: Advancing Visual Music Understanding in Multimodal Large Language Models", "authors": ["Jian Chen", "Wenye Ma", "Penghang Liu", "Wei Wang", "Tengwei Song", "Ming Li", "Chenguang Wang", "Ruiyi Zhang", "Changyou Chen"], "summary": "Multimodal Large Language Models (MLLMs) have achieved remarkable visual\nreasoning abilities in natural images, text-rich documents, and graphic\ndesigns. However, their ability to interpret music sheets remains\nunderexplored. To bridge this gap, we introduce MusiXQA, the first\ncomprehensive dataset for evaluating and advancing MLLMs in music sheet\nunderstanding. MusiXQA features high-quality synthetic music sheets generated\nvia MusiXTeX, with structured annotations covering note pitch and duration,\nchords, clefs, key/time signatures, and text, enabling diverse visual QA tasks.\nThrough extensive evaluations, we reveal significant limitations of current\nstate-of-the-art MLLMs in this domain. Beyond benchmarking, we developed\nPhi-3-MusiX, an MLLM fine-tuned on our dataset, achieving significant\nperformance gains over GPT-based methods. The proposed dataset and model\nestablish a foundation for future advances in MLLMs for music sheet\nunderstanding. Code, data, and model will be released upon acceptance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23009v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23009v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23408", "title": "Do LLMs Dream of Discrete Algorithms?", "authors": ["Claudionor Coelho Jr", "Yanen Li", "Philip Tee"], "summary": "Large Language Models (LLMs) have rapidly transformed the landscape of\nartificial intelligence, enabling natural language interfaces and dynamic\norchestration of software components. However, their reliance on probabilistic\ninference limits their effectiveness in domains requiring strict logical\nreasoning, discrete decision-making, and robust interpretability. This paper\ninvestigates these limitations and proposes a neurosymbolic approach that\naugments LLMs with logic-based reasoning modules, particularly leveraging\nProlog predicates and composable toolsets. By integrating first-order logic and\nexplicit rule systems, our framework enables LLMs to decompose complex queries\ninto verifiable sub-tasks, orchestrate reliable solutions, and mitigate common\nfailure modes such as hallucination and incorrect step decomposition. We\ndemonstrate the practical benefits of this hybrid architecture through\nexperiments on the DABStep benchmark, showing improved precision, coverage, and\nsystem documentation in multi-step reasoning tasks. Our results indicate that\ncombining LLMs with modular logic reasoning restores engineering rigor,\nenhances system reliability, and offers a scalable path toward trustworthy,\ninterpretable AI agents across complex domains.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23408v1", "categories": ["cs.LG", "cs.LO"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.23408v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.22497", "title": "Peer Review as Structured Commentary: Immutable Identity, Public Dialogue, and Reproducible Scholarship", "authors": ["Craig Steven Wright"], "summary": "This paper reconceptualises peer review as structured public commentary.\nTraditional academic validation is hindered by anonymity, latency, and\ngatekeeping. We propose a transparent, identity-linked, and reproducible system\nof scholarly evaluation anchored in open commentary. Leveraging blockchain for\nimmutable audit trails and AI for iterative synthesis, we design a framework\nthat incentivises intellectual contribution, captures epistemic evolution, and\nenables traceable reputational dynamics. This model empowers fields from\ncomputational science to the humanities, reframing academic knowledge as a\nliving process rather than a static credential.", "comment": "66 pages, 0 figures, interdisciplinary framework, includes proposed\n  architecture and metadata layer structures", "pdf_url": "http://arxiv.org/pdf/2506.22497v1", "categories": ["cs.CY", "cs.AI", "cs.DL", "cs.SI", "physics.hist-ph", "68T99, 03B30, 91D30", "I.2.0; H.3.5; K.4.4"], "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.22497v1", "date": "2025-06-25", "updated": "2025-06-25"}
{"id": "2506.23888", "title": "Advancing Multi-Step Mathematical Reasoning in Large Language Models through Multi-Layered Self-Reflection with Auto-Prompting", "authors": ["Andr√© de Souza Loureiro", "Jorge Valverde-Rebaza", "Julieta Noguez", "David Escarcega", "Ricardo Marcacini"], "summary": "Recent advancements in Large Language Models (LLMs) have significantly\nimproved their problem-solving capabilities. However, these models still\nstruggle when faced with complex multi-step reasoning tasks. In this paper, we\npropose the Multi-Layered Self-Reflection with Auto-Prompting (MAPS) framework,\na novel approach designed to enhance multi-step mathematical reasoning in LLMs\nby integrating techniques such as Chain of Thought (CoT), Self-Reflection, and\nAuto-Prompting. Unlike traditional static prompting methods, MAPS employs an\niterative refinement process. Initially, the model generates a solution using\nCoT prompting. When errors are detected, an adaptive self-reflection mechanism\nidentifies and analyzes them, generating tailored prompts to guide corrections.\nThese dynamically adjusted prompts enable the model to iteratively refine its\nreasoning. Experiments on four well-established benchmarks across multiple LLMs\nshow that MAPS significantly outperforms standard CoT and achieves competitive\nresults with reasoning-optimized models. In addition, MAPS enables\ngeneral-purpose LLMs to reach performance levels comparable to specialized\nreasoning models. While deeper reflection layers improve accuracy, they also\nincrease token usage and costs. To balance this trade-off, MAPS strategically\nlimits reflection depth, ensuring an optimal balance between cost and reasoning\nperformance.", "comment": "Accepted for publication in: European Conference on Machine Learning\n  and Principles and Practice of Knowledge Discovery in Databases (ECML PKDD\n  2025). Research Track", "pdf_url": "http://arxiv.org/pdf/2506.23888v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.23888v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23030", "title": "VisionScores -- A system-segmented image score dataset for deep learning tasks", "authors": ["Alejandro Romero Amezcua", "Mariano Jos√© Juan Rivera Meraz"], "summary": "VisionScores presents a novel proposal being the first system-segmented image\nscore dataset, aiming to offer structure-rich, high information-density images\nfor machine and deep learning tasks. Delimited to two-handed piano pieces, it\nwas built to consider not only certain graphic similarity but also composition\npatterns, as this creative process is highly instrument-dependent. It provides\ntwo scenarios in relation to composer and composition type. The first, formed\nby 14k samples, considers works from different authors but the same composition\ntype, specifically, Sonatinas. The latter, consisting of 10.8K samples,\npresents the opposite case, various composition types from the same author,\nbeing the one selected Franz Liszt. All of the 24.8k samples are formatted as\ngrayscale jpg images of $128 \\times 512$ pixels. VisionScores supplies the\nusers not only the formatted samples but the systems' order and pieces'\nmetadata. Moreover, unsegmented full-page scores and the pre-formatted images\nare included for further analysis.", "comment": "Comments: 5 pages, 3 figures. Accepted for presentation at the 2025\n  IEEE International Conference on Image Processing (ICIP). \\c{opyright} 2025\n  IEEE. Personal use of this material is permitted. Permission from IEEE must\n  be obtained for any other use", "pdf_url": "http://arxiv.org/pdf/2506.23030v1", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23030v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23419", "title": "BenchMake: Turn any scientific data set into a reproducible benchmark", "authors": ["Amanda S Barnard"], "summary": "Benchmark data sets are a cornerstone of machine learning development and\napplications, ensuring new methods are robust, reliable and competitive. The\nrelative rarity of benchmark sets in computational science, due to the\nuniqueness of the problems and the pace of change in the associated domains,\nmakes evaluating new innovations difficult for computational scientists. In\nthis paper a new tool is developed and tested to potentially turn any of the\nincreasing numbers of scientific data sets made openly available into a\nbenchmark accessible to the community. BenchMake uses non-negative matrix\nfactorisation to deterministically identify and isolate challenging edge cases\non the convex hull (the smallest convex set that contains all existing data\ninstances) and partitions a required fraction of matched data instances into a\ntesting set that maximises divergence and statistical significance, across\ntabular, graph, image, signal and textual modalities. BenchMake splits are\ncompared to establish splits and random splits using ten publicly available\nbenchmark sets from different areas of science, with different sizes, shapes,\ndistributions.", "comment": "10 pages, 15 pages in Appendix, 15 figures, 5 tables, 57 references", "pdf_url": "http://arxiv.org/pdf/2506.23419v1", "categories": ["cs.LG", "cs.AI", "cs.DL", "62G09", "J.1"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.23419v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.22498", "title": "ViFusionTST: Deep Fusion of Time-Series Image Representations from Load Signals for Early Bed-Exit Prediction", "authors": ["Hao Liu", "Yu Hu", "Rakiba Rayhana", "Ling Bai", "Zheng Liu"], "summary": "Bed-related falls remain a leading source of injury in hospitals and\nlong-term-care facilities, yet many commercial alarms trigger only after a\npatient has already left the bed. We show that early bed-exit intent can be\npredicted using only four low-cost load cells mounted under the bed legs. The\nresulting load signals are first converted into a compact set of complementary\nimages: an RGB line plot that preserves raw waveforms and three texture maps -\nrecurrence plot, Markov transition field, and Gramian angular field - that\nexpose higher-order dynamics. We introduce ViFusionTST, a dual-stream Swin\nTransformer that processes the line plot and texture maps in parallel and fuses\nthem through cross-attention to learn data-driven modality weights.\n  To provide a realistic benchmark, we collected six months of continuous data\nfrom 95 beds in a long-term-care facility. On this real-world dataset\nViFusionTST reaches an accuracy of 0.885 and an F1 score of 0.794, surpassing\nrecent 1D and 2D time-series baselines across F1, recall, accuracy, and AUPRC.\nThe results demonstrate that image-based fusion of load-sensor signals for time\nseries classification is a practical and effective solution for real-time,\nprivacy-preserving fall prevention.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22498v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22498v1", "date": "2025-06-25", "updated": "2025-06-25"}
{"id": "2506.23921", "title": "The Trilemma of Truth in Large Language Models", "authors": ["Germans Savcisens", "Tina Eliassi-Rad"], "summary": "We often attribute human characteristics to large language models (LLMs) and\nclaim that they \"know\" certain things. LLMs have an internal probabilistic\nknowledge that represents information retained during training. How can we\nassess the veracity of this knowledge? We examine two common methods for\nprobing the veracity of LLMs and discover several assumptions that are flawed.\nTo address these flawed assumptions, we introduce sAwMIL (short for Sparse\nAware Multiple-Instance Learning), a probing method that utilizes the internal\nactivations of LLMs to separate statements into true, false, and neither.\nsAwMIL is based on multiple-instance learning and conformal prediction. We\nevaluate sAwMIL on 5 validity criteria across 16 open-source LLMs, including\nboth default and chat-based variants, as well as on 3 new datasets. Among the\ninsights we provide are: (1) the veracity signal is often concentrated in the\nthird quarter of an LLM's depth; (2) truth and falsehood signals are not always\nsymmetric; (3) linear probes perform better on chat models than on default\nmodels; (4) nonlinear probes may be required to capture veracity signals for\nsome LLMs with reinforcement learning from human feedback or knowledge\ndistillation; and (5) LLMs capture a third type of signal that is distinct from\ntrue and false and is neither true nor false. These findings provide a reliable\nmethod for verifying what LLMs \"know\" and how certain they are of their\nprobabilistic internal knowledge.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23921v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.23921v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23038", "title": "Inpainting is All You Need: A Diffusion-based Augmentation Method for Semi-supervised Medical Image Segmentation", "authors": ["Xinrong Hu", "Yiyu Shi"], "summary": "Collecting pixel-level labels for medical datasets can be a laborious and\nexpensive process, and enhancing segmentation performance with a scarcity of\nlabeled data is a crucial challenge. This work introduces AugPaint, a data\naugmentation framework that utilizes inpainting to generate image-label pairs\nfrom limited labeled data. AugPaint leverages latent diffusion models, known\nfor their ability to generate high-quality in-domain images with low overhead,\nand adapts the sampling process for the inpainting task without need for\nretraining. Specifically, given a pair of image and label mask, we crop the\narea labeled with the foreground and condition on it during reversed denoising\nprocess for every noise level. Masked background area would gradually be filled\nin, and all generated images are paired with the label mask. This approach\nensures the accuracy of match between synthetic images and label masks, setting\nit apart from existing dataset generation methods. The generated images serve\nas valuable supervision for training downstream segmentation models,\neffectively addressing the challenge of limited annotations. We conducted\nextensive evaluations of our data augmentation method on four public medical\nimage segmentation datasets, including CT, MRI, and skin imaging. Results\nacross all datasets demonstrate that AugPaint outperforms state-of-the-art\nlabel-efficient methodologies, significantly improving segmentation\nperformance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23038v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23038v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23424", "title": "Accurate Parameter-Efficient Test-Time Adaptation for Time Series Forecasting", "authors": ["Heitor R. Medeiros", "Hossein Sharifi-Noghabi", "Gabriel L. Oliveira", "Saghar Irandoust"], "summary": "Real-world time series often exhibit a non-stationary nature, degrading the\nperformance of pre-trained forecasting models. Test-Time Adaptation (TTA)\naddresses this by adjusting models during inference, but existing methods\ntypically update the full model, increasing memory and compute costs. We\npropose PETSA, a parameter-efficient method that adapts forecasters at test\ntime by only updating small calibration modules on the input and output. PETSA\nuses low-rank adapters and dynamic gating to adjust representations without\nretraining. To maintain accuracy despite limited adaptation capacity, we\nintroduce a specialized loss combining three components: (1) a robust term, (2)\na frequency-domain term to preserve periodicity, and (3) a patch-wise\nstructural term for structural alignment. PETSA improves the adaptability of\nvarious forecasting backbones while requiring fewer parameters than baselines.\nExperimental results on benchmark datasets show that PETSA achieves competitive\nor better performance across all horizons. Our code is available at:\nhttps://github.com/BorealisAI/PETSA", "comment": "Second Workshop on Test-Time Adaptation: Putting Updates to the Test!\n  at ICML 2025, Vancouver, Canada. 2025", "pdf_url": "http://arxiv.org/pdf/2506.23424v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.23424v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.22499", "title": "Scalable Dynamic Origin-Destination Demand Estimation Enhanced by High-Resolution Satellite Imagery Data", "authors": ["Jiachao Liu", "Pablo Guarda", "Koichiro Niinuma", "Sean Qian"], "summary": "This study presents a novel integrated framework for dynamic\norigin-destination demand estimation (DODE) in multi-class mesoscopic network\nmodels, leveraging high-resolution satellite imagery together with conventional\ntraffic data from local sensors. Unlike sparse local detectors, satellite\nimagery offers consistent, city-wide road and traffic information of both\nparking and moving vehicles, overcoming data availability limitations. To\nextract information from imagery data, we design a computer vision pipeline for\nclass-specific vehicle detection and map matching, generating link-level\ntraffic density observations by vehicle class. Building upon this information,\nwe formulate a computational graph-based DODE model that calibrates dynamic\nnetwork states by jointly matching observed traffic counts and travel times\nfrom local sensors with density measurements derived from satellite imagery. To\nassess the accuracy and scalability of the proposed framework, we conduct a\nseries of numerical experiments using both synthetic and real-world data. The\nresults of out-of-sample tests demonstrate that supplementing traditional data\nwith satellite-derived density significantly improves estimation performance,\nespecially for links without local sensors. Real-world experiments also confirm\nthe framework's capability to handle large-scale networks, supporting its\npotential for practical deployment in cities of varying sizes. Sensitivity\nanalysis further evaluates the impact of data quality related to satellite\nimagery data.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22499v1", "categories": ["cs.CV", "cs.AI", "stat.AP"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22499v1", "date": "2025-06-25", "updated": "2025-06-25"}
{"id": "2506.23929", "title": "IMPACT: Inflectional Morphology Probes Across Complex Typologies", "authors": ["Mohammed J. Saeed", "Tommi Vehvilainen", "Evgeny Fedoseev", "Sevil Caliskan", "Tatiana Vodolazova"], "summary": "Large Language Models (LLMs) have shown significant progress on various\nmultilingual benchmarks and are increasingly used to generate and evaluate text\nin non-English languages. However, while they may produce fluent outputs, it\nremains unclear to what extent these models truly grasp the underlying\nlinguistic complexity of those languages, particularly in morphology. To\ninvestigate this, we introduce IMPACT, a synthetically generated evaluation\nframework focused on inflectional morphology, which we publicly release,\ndesigned to evaluate LLM performance across five morphologically rich\nlanguages: Arabic, Russian, Finnish, Turkish, and Hebrew. IMPACT includes\nunit-test-style cases covering both shared and language-specific phenomena,\nfrom basic verb inflections (e.g., tense, number, gender) to unique features\nlike Arabic's reverse gender agreement and vowel harmony in Finnish and\nTurkish. We assess eight multilingual LLMs that, despite strong English\nperformance, struggle with other languages and uncommon morphological patterns,\nespecially when judging ungrammatical examples. We also show that Chain of\nThought and Thinking Models can degrade performance. Our work exposes gaps in\nLLMs' handling of linguistic complexity, pointing to clear room for\nimprovement. To support further research, we publicly release the IMPACT\nframework.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23929v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.23929v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23042", "title": "From Coarse to Fine: Learnable Discrete Wavelet Transforms for Efficient 3D Gaussian Splatting", "authors": ["Hung Nguyen", "An Le", "Runfa Li", "Truong Nguyen"], "summary": "3D Gaussian Splatting has emerged as a powerful approach in novel view\nsynthesis, delivering rapid training and rendering but at the cost of an\never-growing set of Gaussian primitives that strains memory and bandwidth. We\nintroduce AutoOpti3DGS, a training-time framework that automatically restrains\nGaussian proliferation without sacrificing visual fidelity. The key idea is to\nfeed the input images to a sequence of learnable Forward and Inverse Discrete\nWavelet Transforms, where low-pass filters are kept fixed, high-pass filters\nare learnable and initialized to zero, and an auxiliary orthogonality loss\ngradually activates fine frequencies. This wavelet-driven, coarse-to-fine\nprocess delays the formation of redundant fine Gaussians, allowing 3DGS to\ncapture global structure first and refine detail only when necessary. Through\nextensive experiments, AutoOpti3DGS requires just a single filter learning-rate\nhyper-parameter, integrates seamlessly with existing efficient 3DGS frameworks,\nand consistently produces sparser scene representations more compatible with\nmemory or storage-constrained hardware.", "comment": "Accepted to ICCV Workshop", "pdf_url": "http://arxiv.org/pdf/2506.23042v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23042v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23446", "title": "Enhancing Insider Threat Detection Using User-Based Sequencing and Transformer Encoders", "authors": ["Mohamed Elbasheer", "Adewale Akinfaderin"], "summary": "Insider threat detection presents unique challenges due to the authorized\nstatus of malicious actors and the subtlety of anomalous behaviors. Existing\nmachine learning methods often treat user activity as isolated events, thereby\nfailing to leverage sequential dependencies in user behavior. In this study, we\npropose a User-Based Sequencing (UBS) methodology, transforming the CERT\ninsider threat dataset into structured temporal sequences suitable for deep\nsequential modeling. We deploy a Transformer Encoder architecture to model\nbenign user activity and employ its reconstruction errors as anomaly scores.\nThese scores are subsequently evaluated using three unsupervised outlier\ndetection algorithms: One-Class SVM (OCSVM), Local Outlier Factor (LOF), and\nIsolation Forest (iForest). Across four rigorously designed test sets,\nincluding combinations of multiple CERT dataset releases, our UBS-Transformer\npipeline consistently achieves state-of-the-art performance - notably 96.61%\naccuracy, 99.43% recall, 96.38% F1-score, 95.00% AUROC, and exceptionally low\nfalse negative (0.0057) and false positive (0.0571) rates. Comparative analyses\ndemonstrate that our approach substantially outperforms tabular and\nconventional autoencoder baselines, underscoring the efficacy of sequential\nuser modeling and advanced anomaly detection in the insider threat domain.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23446v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.23446v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22500", "title": "Visual-Semantic Knowledge Conflicts in Operating Rooms: Synthetic Data Curation for Surgical Risk Perception in Multimodal Large Language Models", "authors": ["Weiyi Zhao", "Xiaoyu Tan", "Liang Liu", "Sijia Li", "Youwei Song", "Xihe Qiu"], "summary": "Surgical risk identification is critical for patient safety and reducing\npreventable medical errors. While multimodal large language models (MLLMs) show\npromise for automated operating room (OR) risk detection, they often exhibit\nvisual-semantic knowledge conflicts (VS-KC), failing to identify visual safety\nviolations despite understanding textual rules. To address this, we introduce a\ndataset comprising over 34,000 synthetic images generated by diffusion models,\ndepicting operating room scenes containing entities that violate established\nsafety rules. These images were created to alleviate data scarcity and examine\nMLLMs vulnerabilities. In addition, the dataset includes 214 human-annotated\nimages that serve as a gold-standard reference for validation. This\ncomprehensive dataset, spanning diverse perspectives, stages, and\nconfigurations, is designed to expose and study VS-KC. Fine-tuning on OR-VSKC\nsignificantly improves MLLMs' detection of trained conflict entities and\ngeneralizes well to new viewpoints for these entities, but performance on\nuntrained entity types remains poor, highlighting learning specificity and the\nneed for comprehensive training. The main contributions of this work include:\n(1) a data generation methodology tailored for rule-violation scenarios; (2)\nthe release of the OR-VSKC dataset and its associated benchmark as open-source\nresources; and (3) an empirical analysis of violation-sensitive knowledge\nconsistency in representative MLLMs. The dataset and appendix are available at\nhttps://github.com/zgg2577/VS-KC.", "comment": "13 pages, 5 figures. The dataset and appendix are available at\n  https://github.com/zgg2577/VS-KC", "pdf_url": "http://arxiv.org/pdf/2506.22500v1", "categories": ["cs.CV", "cs.AI", "68T07, 68U10, 92C55", "I.2.10; I.2.7; J.3; I.2.6"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22500v1", "date": "2025-06-25", "updated": "2025-06-25"}
{"id": "2506.23930", "title": "Leveraging the Potential of Prompt Engineering for Hate Speech Detection in Low-Resource Languages", "authors": ["Ruhina Tabasshum Prome", "Tarikul Islam Tamiti", "Anomadarshi Barua"], "summary": "The rapid expansion of social media leads to a marked increase in hate\nspeech, which threatens personal lives and results in numerous hate crimes.\nDetecting hate speech presents several challenges: diverse dialects, frequent\ncode-mixing, and the prevalence of misspelled words in user-generated content\non social media platforms. Recent progress in hate speech detection is\ntypically concentrated on high-resource languages. However, low-resource\nlanguages still face significant challenges due to the lack of large-scale,\nhigh-quality datasets. This paper investigates how we can overcome this\nlimitation via prompt engineering on large language models (LLMs) focusing on\nlow-resource Bengali language. We investigate six prompting strategies -\nzero-shot prompting, refusal suppression, flattering the classifier, multi-shot\nprompting, role prompting, and finally our innovative metaphor prompting to\ndetect hate speech effectively in low-resource languages. We pioneer the\nmetaphor prompting to circumvent the built-in safety mechanisms of LLMs that\nmarks a significant departure from existing jailbreaking methods. We\ninvestigate all six different prompting strategies on the Llama2-7B model and\ncompare the results extensively with three pre-trained word embeddings - GloVe,\nWord2Vec, and FastText for three different deep learning models - multilayer\nperceptron (MLP), convolutional neural network (CNN), and bidirectional gated\nrecurrent unit (BiGRU). To prove the effectiveness of our metaphor prompting in\nthe low-resource Bengali language, we also evaluate it in another low-resource\nlanguage - Hindi, and two high-resource languages - English and German. The\nperformance of all prompting techniques is evaluated using the F1 score, and\nenvironmental impact factor (IF), which measures CO$_2$ emissions, electricity\nusage, and computational time.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23930v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.23930v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23044", "title": "Ovis-U1 Technical Report", "authors": ["Guo-Hua Wang", "Shanshan Zhao", "Xinjie Zhang", "Liangfu Cao", "Pengxin Zhan", "Lunhao Duan", "Shiyin Lu", "Minghao Fu", "Xiaohao Chen", "Jianshan Zhao", "Yang Li", "Qing-Guo Chen"], "summary": "In this report, we introduce Ovis-U1, a 3-billion-parameter unified model\nthat integrates multimodal understanding, text-to-image generation, and image\nediting capabilities. Building on the foundation of the Ovis series, Ovis-U1\nincorporates a diffusion-based visual decoder paired with a bidirectional token\nrefiner, enabling image generation tasks comparable to leading models like\nGPT-4o. Unlike some previous models that use a frozen MLLM for generation\ntasks, Ovis-U1 utilizes a new unified training approach starting from a\nlanguage model. Compared to training solely on understanding or generation\ntasks, unified training yields better performance, demonstrating the\nenhancement achieved by integrating these two tasks. Ovis-U1 achieves a score\nof 69.6 on the OpenCompass Multi-modal Academic Benchmark, surpassing recent\nstate-of-the-art models such as Ristretto-3B and SAIL-VL-1.5-2B. In\ntext-to-image generation, it excels with scores of 83.72 and 0.89 on the\nDPG-Bench and GenEval benchmarks, respectively. For image editing, it achieves\n4.00 and 6.42 on the ImgEdit-Bench and GEdit-Bench-EN, respectively. As the\ninitial version of the Ovis unified model series, Ovis-U1 pushes the boundaries\nof multimodal understanding, generation, and editing.", "comment": "A unified model for multimodal understanding, text-to-image\n  generation, and image editing. GitHub: https://github.com/AIDC-AI/Ovis-U1", "pdf_url": "http://arxiv.org/pdf/2506.23044v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23044v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23462", "title": "Can We Predict the Unpredictable? Leveraging DisasterNet-LLM for Multimodal Disaster Classification", "authors": ["Manaswi Kulahara", "Gautam Siddharth Kashyap", "Nipun Joshi", "Arpita Soni"], "summary": "Effective disaster management requires timely and accurate insights, yet\ntraditional methods struggle to integrate multimodal data such as images,\nweather records, and textual reports. To address this, we propose\nDisasterNet-LLM, a specialized Large Language Model (LLM) designed for\ncomprehensive disaster analysis. By leveraging advanced pretraining,\ncross-modal attention mechanisms, and adaptive transformers, DisasterNet-LLM\nexcels in disaster classification. Experimental results demonstrate its\nsuperiority over state-of-the-art models, achieving higher accuracy of 89.5%,\nan F1 score of 88.0%, AUC of 0.92%, and BERTScore of 0.88% in multimodal\ndisaster classification tasks.", "comment": "Accepted in the 2025 IEEE International Geoscience and Remote Sensing\n  Symposium (IGARSS 2025), scheduled for 3 - 8 August 2025 in Brisbane,\n  Australia", "pdf_url": "http://arxiv.org/pdf/2506.23462v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.23462v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22501", "title": "How Can Multimodal Remote Sensing Datasets Transform Classification via SpatialNet-ViT?", "authors": ["Gautam Siddharth Kashyap", "Manaswi Kulahara", "Nipun Joshi", "Usman Naseem"], "summary": "Remote sensing datasets offer significant promise for tackling key\nclassification tasks such as land-use categorization, object presence\ndetection, and rural/urban classification. However, many existing studies tend\nto focus on narrow tasks or datasets, which limits their ability to generalize\nacross various remote sensing classification challenges. To overcome this, we\npropose a novel model, SpatialNet-ViT, leveraging the power of Vision\nTransformers (ViTs) and Multi-Task Learning (MTL). This integrated approach\ncombines spatial awareness with contextual understanding, improving both\nclassification accuracy and scalability. Additionally, techniques like data\naugmentation, transfer learning, and multi-task learning are employed to\nenhance model robustness and its ability to generalize across diverse datasets", "comment": "Accepted in the 2025 IEEE International Geoscience and Remote Sensing\n  Symposium (IGARSS 2025), scheduled for 3 - 8 August 2025 in Brisbane,\n  Australia", "pdf_url": "http://arxiv.org/pdf/2506.22501v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22501v1", "date": "2025-06-25", "updated": "2025-06-25"}
{"id": "2506.23940", "title": "Graft: Integrating the Domain Knowledge via Efficient Parameter Synergy for MLLMs", "authors": ["Yang Dai", "Jianxiang An", "Tianwei Lin", "Hongyang He", "Hongzhe Huang", "Wenqiao Zhang", "Zheqi Lv", "Siliang Tang", "Yueting Zhuang"], "summary": "Multimodal Large Language Models (MLLMs) have achieved success across various\ndomains. However, their applicability tends to degrade when confronted with\ndifferent types of data inputs, especially for MLLMs that have been fine-tuned\nfor specific tasks. Despite its importance, the study of knowledge sharing\namong domain-specific MLLMs--such as those trained for mathematics or\ncode--remains largely underexplored. To address the fragmentation of knowledge\nacross domain-specialized MLLMs, we propose a unified parameter integration\nframework that enables modular composition of expert capabilities. Our method\nis grounded in a novel Compatibility-Aware Parameter Splicing (CAPS) strategy,\nwhich leverages both local functional attribution and global\ninformation-theoretic signals to guide selective parameter fusion. By extending\nthis mechanism to the low-rank adaptation layer granularity, we ensure\nefficient integration with minimal inference overhead. Furthermore, we\nintroduce a domain compatibility scoring mechanism that quantifies inter-expert\nalignment at the activation level and correlates with downstream task utility.\nThis principled fusion protocol allows the final model to synergize\nheterogeneous expertise while preserving structural modularity. Extensive\nevaluations across diverse multimodal benchmarks validate the effectiveness of\nour framework, offering a scalable path toward compositional, domain-adaptive\nMLLMs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23940v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.23940v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23061", "title": "Empowering Small VLMs to Think with Dynamic Memorization and Exploration", "authors": ["Jiazhen Liu", "Yuchuan Deng", "Long Chen"], "summary": "Empowering Small-scale Vision-Language Models (SVLMs) with reliable thinking\ncapabilities remains fundamentally challenging due to their limited parameter\ncapacity and weak instruction-following abilities. Existing training paradigms,\nincluding Supervised Fine-Tuning (SFT) and Reinforcement Learning with\nVerifiable Reward (RLVR), impose substantial demands on the base VLM, exceeding\nthe capabilities of SVLMs. Consequently, directly applying these paradigms to\nSVLMs often suffers from severe pseudo thinking traces and advantage collapse,\nultimately undermining both thinking reliability and task performance. A\nnatural solution is to combine SFT and RLVR, leveraging their complementarity\nto reduce the dependence on model capacity. However, the widely adopted\ntwo-stage training paradigm still performs poorly on SVLMs, as their tendency\ntoward sub-optimal convergence hinders the trade-off and limits the benefits of\nthe combination. To address this, we propose DyME, a novel training paradigm\nthat Dynamically selects between Memorization (via SFT) and Exploration (via\nRLVR) modes at each optimization step, ensuring that every update contributes\nto the trade-off. Extensive experiments across diverse domains demonstrate that\nDyME consistently achieves this balance, and thus delivers substantial\nperformance improvements. These results establish DyME as a practical and\neffective solution for empowering SVLMs with reliable thinking capabilities.\nGitHub: https://github.com/HKUST-LongGroup/DyME", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23061v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23061v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23469", "title": "Reconciling Attribute and Structural Anomalies for Improved Graph Anomaly Detection", "authors": ["Chunjing Xiao", "Jiahui Lu", "Xovee Xu", "Fan Zhou", "Tianshu Xie", "Wei Lu", "Lifeng Xu"], "summary": "Graph anomaly detection is critical in domains such as healthcare and\neconomics, where identifying deviations can prevent substantial losses.\nExisting unsupervised approaches strive to learn a single model capable of\ndetecting both attribute and structural anomalies. However, they confront the\ntug-of-war problem between two distinct types of anomalies, resulting in\nsuboptimal performance. This work presents TripleAD, a mutual\ndistillation-based triple-channel graph anomaly detection framework. It\nincludes three estimation modules to identify the attribute, structural, and\nmixed anomalies while mitigating the interference between different types of\nanomalies. In the first channel, we design a multiscale attribute estimation\nmodule to capture extensive node interactions and ameliorate the over-smoothing\nissue. To better identify structural anomalies, we introduce a link-enhanced\nstructure estimation module in the second channel that facilitates information\nflow to topologically isolated nodes. The third channel is powered by an\nattribute-mixed curvature, a new indicator that encapsulates both attribute and\nstructural information for discriminating mixed anomalies. Moreover, a mutual\ndistillation strategy is introduced to encourage communication and\ncollaboration between the three channels. Extensive experiments demonstrate the\neffectiveness of the proposed TripleAD model against strong baselines.", "comment": "Accepted by IEEE Transactions on Neural Networks and Learning Systems\n  (TNNLS); DOI: https://doi.org/10.1109/TNNLS.2025.3561172", "pdf_url": "http://arxiv.org/pdf/2506.23469v1", "categories": ["cs.LG", "cs.SI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.23469v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22506", "title": "SABRE-FL: Selective and Accurate Backdoor Rejection for Federated Prompt Learning", "authors": ["Momin Ahmad Khan", "Yasra Chandio", "Fatima Muhammad Anwar"], "summary": "Federated Prompt Learning has emerged as a communication-efficient and\nprivacy-preserving paradigm for adapting large vision-language models like CLIP\nacross decentralized clients. However, the security implications of this setup\nremain underexplored. In this work, we present the first study of backdoor\nattacks in Federated Prompt Learning. We show that when malicious clients\ninject visually imperceptible, learnable noise triggers into input images, the\nglobal prompt learner becomes vulnerable to targeted misclassification while\nstill maintaining high accuracy on clean inputs. Motivated by this\nvulnerability, we propose SABRE-FL, a lightweight, modular defense that filters\npoisoned prompt updates using an embedding-space anomaly detector trained\noffline on out-of-distribution data. SABRE-FL requires no access to raw client\ndata or labels and generalizes across diverse datasets. We show, both\ntheoretically and empirically, that malicious clients can be reliably\nidentified and filtered using an embedding-based detector. Across five diverse\ndatasets and four baseline defenses, SABRE-FL outperforms all baselines by\nsignificantly reducing backdoor accuracy while preserving clean accuracy,\ndemonstrating strong empirical performance and underscoring the need for robust\nprompt learning in future federated systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22506v1", "categories": ["cs.CR", "cs.AI"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.22506v1", "date": "2025-06-25", "updated": "2025-06-25"}
{"id": "2506.23951", "title": "Unveiling Decision-Making in LLMs for Text Classification : Extraction of influential and interpretable concepts with Sparse Autoencoders", "authors": ["Mathis Le Bail", "J√©r√©mie Dentan", "Davide Buscaldi", "Sonia Vanier"], "summary": "Sparse Autoencoders (SAEs) have been successfully used to probe Large\nLanguage Models (LLMs) and extract interpretable concepts from their internal\nrepresentations. These concepts are linear combinations of neuron activations\nthat correspond to human-interpretable features. In this paper, we investigate\nthe effectiveness of SAE-based explainability approaches for sentence\nclassification, a domain where such methods have not been extensively explored.\nWe present a novel SAE-based architecture tailored for text classification,\nleveraging a specialized classifier head and incorporating an activation rate\nsparsity loss. We benchmark this architecture against established methods such\nas ConceptShap, Independent Component Analysis, and other SAE-based concept\nextraction techniques. Our evaluation covers two classification benchmarks and\nfour fine-tuned LLMs from the Pythia family. We further enrich our analysis\nwith two novel metrics for measuring the precision of concept-based\nexplanations, using an external sentence encoder. Our empirical results show\nthat our architecture improves both the causality and interpretability of the\nextracted features.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23951v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.23951v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23066", "title": "CoreMark: Toward Robust and Universal Text Watermarking Technique", "authors": ["Jiale Meng", "Yiming Li", "Zheming Lu", "Zewei He", "Hao Luo", "Tianwei Zhang"], "summary": "Text watermarking schemes have gained considerable attention in recent years,\nyet still face critical challenges in achieving simultaneous robustness,\ngeneralizability, and imperceptibility. This paper introduces a new embedding\nparadigm,termed CORE, which comprises several consecutively aligned black pixel\nsegments. Its key innovation lies in its inherent noise resistance during\ntransmission and broad applicability across languages and fonts. Based on the\nCORE, we present a text watermarking framework named CoreMark. Specifically,\nCoreMark first dynamically extracts COREs from characters. Then, the characters\nwith stronger robustness are selected according to the lengths of COREs. By\nmodifying the thickness of the CORE, the hidden data is embedded into the\nselected characters without causing significant visual distortions. Moreover, a\ngeneral plug-and-play embedding strength modulator is proposed, which can\nadaptively enhance the robustness for small font sizes by adjusting the\nembedding strength according to the font size. Experimental evaluation\nindicates that CoreMark demonstrates outstanding generalizability across\nmultiple languages and fonts. Compared to existing methods, CoreMark achieves\nsignificant improvements in resisting screenshot, print-scan, and print camera\nattacks, while maintaining satisfactory imperceptibility.", "comment": "10 pages, 16 figures", "pdf_url": "http://arxiv.org/pdf/2506.23066v1", "categories": ["cs.CV", "cs.CR", "cs.MM"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23066v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23492", "title": "Sample Margin-Aware Recalibration of Temperature Scaling", "authors": ["Haolan Guo", "Linwei Tao", "Haoyang Luo", "Minjing Dong", "Chang Xu"], "summary": "Recent advances in deep learning have significantly improved predictive\naccuracy. However, modern neural networks remain systematically overconfident,\nposing risks for deployment in safety-critical scenarios. Current post-hoc\ncalibration methods face a fundamental dilemma: global approaches like\nTemperature Scaling apply uniform adjustments across all samples, introducing\nhigh bias despite computational efficiency, while more expressive methods that\noperate on full logit distributions suffer from high variance due to noisy\nhigh-dimensional inputs and insufficient validation data. To address these\nchallenges, we propose Sample Margin-Aware Recalibration of Temperature\n(SMART), a lightweight, data-efficient recalibration method that precisely\nscales logits based on the margin between the top two logits -- termed the\nlogit gap. Specifically, the logit gap serves as a denoised, scalar signal\ndirectly tied to decision boundary uncertainty, providing a robust indicator\nthat avoids the noise inherent in high-dimensional logit spaces while\npreserving model prediction invariance. Meanwhile, SMART employs a novel\nsoft-binned Expected Calibration Error (SoftECE) objective that balances model\nbias and variance through adaptive binning, enabling stable parameter updates\neven with extremely limited calibration data. Extensive evaluations across\ndiverse datasets and architectures demonstrate that SMART achieves\nstate-of-the-art calibration performance even with substantially fewer\nparameters compared to existing parametric methods, offering a principled,\nrobust, and highly efficient solution for practical uncertainty quantification\nin neural network predictions. The source code is available at:\nhttps://anonymous.4open.science/r/SMART-8B11.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23492v1", "categories": ["cs.LG", "cs.AI", "cs.CV"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.23492v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22508", "title": "AgentStealth: Reinforcing Large Language Model for Anonymizing User-generated Text", "authors": ["Chenyang Shao", "Tianxing Li", "Chenhao Pu", "Fengli Xu", "Yong Li"], "summary": "In today's digital world, casual user-generated content often contains subtle\ncues that may inadvertently expose sensitive personal attributes. Such risks\nunderscore the growing importance of effective text anonymization to safeguard\nindividual privacy. However, existing methods either rely on rigid replacements\nthat damage utility or cloud-based LLMs that are costly and pose privacy risks.\nTo address these issues, we explore the use of locally deployed smaller-scale\nlanguage models (SLMs) for anonymization. Yet training effective SLMs remains\nchallenging due to limited high-quality supervision. To address the challenge,\nwe propose AgentStealth, a self-reinforcing LLM anonymization framework.First,\nwe introduce an adversarial anonymization workflow enhanced by In-context\nContrastive Learning and Adaptive Utility-Aware Control. Second, we perform\nsupervised adaptation of SLMs using high-quality data collected from the\nworkflow, which includes both anonymization and attack signals. Finally, we\napply online reinforcement learning where the model leverages its internal\nadversarial feedback to iteratively improve anonymization performance.\nExperiments on two datasets show that our method outperforms baselines in both\nanonymization effectiveness (+12.3%) and utility (+6.8%). Our lightweight\ndesign supports direct deployment on edge devices, avoiding cloud reliance and\ncommunication-based privacy risks. Our code is open-source at\nhttps://github.com/tsinghua-fib-lab/AgentStealth.", "comment": "This work has been submitted to NeurIPS 2025. Under review", "pdf_url": "http://arxiv.org/pdf/2506.22508v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.22508v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.23979", "title": "TaP: A Taxonomy-Guided Framework for Automated and Scalable Preference Data Generation", "authors": ["Renren Jin", "Tianhao Shen", "Xinwei Wu", "Dan Shi", "Haoran Sun", "Wuwei Huang", "Quandong Wang", "Wei Liu", "Jian Luan", "Bin Wang", "Deyi Xiong"], "summary": "Conducting supervised fine-tuning and preference fine-tuning on large\nlanguage models (LLMs) requires high-quality datasets to improve their ability\nto follow instructions and align with human preferences and values. However,\nconstructing such datasets is resource-intensive, and most available datasets\nfor supervised and preference fine-tuning are in English. To address these\nchallenges, we propose the \\underline{\\textbf{Ta}}xonomy-Guided\n\\underline{\\textbf{P}}reference Data Generation (TaP) framework, which\nfacilitates automated and scalable construction of preference datasets across\nvarious languages. TaP is grounded in a structured taxonomy that allows\nfine-grained control over dataset composition, thereby ensuring both diversity\nand comprehensive coverage. We employ TaP-generated datasets to perform\nsupervised and preference fine-tuning on various LLMs. Experimental results\ndemonstrate that LLMs trained on TaP-generated datasets outperform those\ntrained on existing open-source datasets. Remarkably, LLMs trained on\nTaP-generated datasets surpass the performance of those trained on an\nopen-source dataset that is 180 times larger.", "comment": "33 pages, 15 tables, 11 figures", "pdf_url": "http://arxiv.org/pdf/2506.23979v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.23979v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23072", "title": "Unsupervised 3D Braided Hair Reconstruction from a Single-View Image", "authors": ["Jing Gao"], "summary": "Reconstructing 3D braided hairstyles from single-view images remains a\nchallenging task due to the intricate interwoven structure and complex\ntopologies of braids. Existing strand-based hair reconstruction methods\ntypically focus on loose hairstyles and often struggle to capture the\nfine-grained geometry of braided hair. In this paper, we propose a novel\nunsupervised pipeline for efficiently reconstructing 3D braided hair from\nsingle-view RGB images. Leveraging a synthetic braid model inspired by braid\ntheory, our approach effectively captures the complex intertwined structures of\nbraids. Extensive experiments demonstrate that our method outperforms\nstate-of-the-art approaches, providing superior accuracy, realism, and\nefficiency in reconstructing 3D braided hairstyles, supporting expressive\nhairstyle modeling in digital humans.", "comment": "6 pages, 3 figures, accepted to the 2025 International Conference on\n  Machine Vision Applications (MVA 2025)", "pdf_url": "http://arxiv.org/pdf/2506.23072v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23072v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23516", "title": "FedWSQ: Efficient Federated Learning with Weight Standardization and Distribution-Aware Non-Uniform Quantization", "authors": ["Seung-Wook Kim", "Seongyeol Kim", "Jiah Kim", "Seowon Ji", "Se-Ho Lee"], "summary": "Federated learning (FL) often suffers from performance degradation due to key\nchallenges such as data heterogeneity and communication constraints. To address\nthese limitations, we present a novel FL framework called FedWSQ, which\nintegrates weight standardization (WS) and the proposed distribution-aware\nnon-uniform quantization (DANUQ). WS enhances FL performance by filtering out\nbiased components in local updates during training, thereby improving the\nrobustness of the model against data heterogeneity and unstable client\nparticipation. In addition, DANUQ minimizes quantization errors by leveraging\nthe statistical properties of local model updates. As a result, FedWSQ\nsignificantly reduces communication overhead while maintaining superior model\naccuracy. Extensive experiments on FL benchmark datasets demonstrate that\nFedWSQ consistently outperforms existing FL methods across various challenging\nFL settings, including extreme data heterogeneity and ultra-low-bit\ncommunication scenarios.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23516v1", "categories": ["cs.LG", "cs.AI", "cs.CV"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.23516v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22509", "title": "FreeDNA: Endowing Domain Adaptation of Diffusion-Based Dense Prediction with Training-Free Domain Noise Alignment", "authors": ["Hang Xu", "Jie Huang", "Linjiang Huang", "Dong Li", "Yidi Liu", "Feng Zhao"], "summary": "Domain Adaptation(DA) for dense prediction tasks is an important topic, which\nenhances the dense prediction model's performance when tested on its unseen\ndomain. Recently, with the development of Diffusion-based Dense Prediction\n(DDP) models, the exploration of DA designs tailored to this framework is worth\nexploring, since the diffusion model is effective in modeling the distribution\ntransformation that comprises domain information. In this work, we propose a\ntraining-free mechanism for DDP frameworks, endowing them with DA capabilities.\nOur motivation arises from the observation that the exposure bias (e.g., noise\nstatistics bias) in diffusion brings domain shift, and different domains in\nconditions of DDP models can also be effectively captured by the noise\nprediction statistics. Based on this, we propose a training-free Domain Noise\nAlignment (DNA) approach, which alleviates the variations of noise statistics\nto domain changes during the diffusion sampling process, thereby achieving\ndomain adaptation. Specifically, when the source domain is available, we\ndirectly adopt the DNA method to achieve domain adaptation by aligning the\nnoise statistics of the target domain with those of the source domain. For the\nmore challenging source-free DA, inspired by the observation that regions\ncloser to the source domain exhibit higher confidence meeting variations of\nsampling noise, we utilize the statistics from the high-confidence regions\nprogressively to guide the noise statistic adjustment during the sampling\nprocess. Notably, our method demonstrates the effectiveness of enhancing the DA\ncapability of DDP models across four common dense prediction tasks. Code is\navailable at\n\\href{https://github.com/xuhang07/FreeDNA}{https://github.com/xuhang07/FreeDNA}.", "comment": "ICCV2025", "pdf_url": "http://arxiv.org/pdf/2506.22509v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22509v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.23990", "title": "Machine Understanding of Scientific Language", "authors": ["Dustin Wright"], "summary": "Scientific information expresses human understanding of nature. This\nknowledge is largely disseminated in different forms of text, including\nscientific papers, news articles, and discourse among people on social media.\nWhile important for accelerating our pursuit of knowledge, not all scientific\ntext is faithful to the underlying science. As the volume of this text has\nburgeoned online in recent years, it has become a problem of societal\nimportance to be able to identify the faithfulness of a given piece of\nscientific text automatically. This thesis is concerned with the cultivation of\ndatasets, methods, and tools for machine understanding of scientific language,\nin order to analyze and understand science communication at scale. To arrive at\nthis, I present several contributions in three areas of natural language\nprocessing and machine learning: automatic fact checking, learning with limited\ndata, and scientific text processing. These contributions include new methods\nand resources for identifying check-worthy claims, adversarial claim\ngeneration, multi-source domain adaptation, learning from crowd-sourced labels,\ncite-worthiness detection, zero-shot scientific fact checking, detecting\nexaggerated scientific claims, and modeling degrees of information change in\nscience communication. Critically, I demonstrate how the research outputs of\nthis thesis are useful for effectively learning from limited amounts of\nscientific text in order to identify misinformative scientific statements and\ngenerate new insights into the science communication process", "comment": "PhD Thesis, 210 pages", "pdf_url": "http://arxiv.org/pdf/2506.23990v1", "categories": ["cs.CL", "cs.LG"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.23990v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23074", "title": "Learning Counterfactually Decoupled Attention for Open-World Model Attribution", "authors": ["Yu Zheng", "Boyang Gong", "Fanye Kong", "Yueqi Duan", "Bingyao Yu", "Wenzhao Zheng", "Lei Chen", "Jiwen Lu", "Jie Zhou"], "summary": "In this paper, we propose a Counterfactually Decoupled Attention Learning\n(CDAL) method for open-world model attribution. Existing methods rely on\nhandcrafted design of region partitioning or feature space, which could be\nconfounded by the spurious statistical correlations and struggle with novel\nattacks in open-world scenarios. To address this, CDAL explicitly models the\ncausal relationships between the attentional visual traces and source model\nattribution, and counterfactually decouples the discriminative model-specific\nartifacts from confounding source biases for comparison. In this way, the\nresulting causal effect provides a quantification on the quality of learned\nattention maps, thus encouraging the network to capture essential generation\npatterns that generalize to unseen source models by maximizing the effect.\nExtensive experiments on existing open-world model attribution benchmarks show\nthat with minimal computational overhead, our method consistently improves\nstate-of-the-art models by large margins, particularly for unseen novel\nattacks. Source code: https://github.com/yzheng97/CDAL.", "comment": "Accepted by ICCV 2025. Code: \\url{https://github.com/yzheng97/CDAL}", "pdf_url": "http://arxiv.org/pdf/2506.23074v1", "categories": ["cs.CV", "cs.CR", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23074v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23544", "title": "Both Asymptotic and Non-Asymptotic Convergence of Quasi-Hyperbolic Momentum using Increasing Batch Size", "authors": ["Kento Imaizumi", "Hideaki Iiduka"], "summary": "Momentum methods were originally introduced for their superiority to\nstochastic gradient descent (SGD) in deterministic settings with convex\nobjective functions. However, despite their widespread application to deep\nneural networks -- a representative case of stochastic nonconvex optimization\n-- the theoretical justification for their effectiveness in such settings\nremains limited. Quasi-hyperbolic momentum (QHM) is an algorithm that\ngeneralizes various momentum methods and has been studied to better understand\nthe class of momentum-based algorithms as a whole. In this paper, we provide\nboth asymptotic and non-asymptotic convergence results for mini-batch QHM with\nan increasing batch size. We show that achieving asymptotic convergence\nrequires either a decaying learning rate or an increasing batch size. Since a\ndecaying learning rate adversely affects non-asymptotic convergence, we\ndemonstrate that using mini-batch QHM with an increasing batch size -- without\ndecaying the learning rate -- can be a more effective strategy. Our experiments\nshow that even a finite increase in batch size can provide benefits for\ntraining neural networks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23544v1", "categories": ["cs.LG", "math.OC"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.23544v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22510", "title": "Towards Text-free Graph Foundation Models: Rethinking Multi-Domain Graph Contrastive Learning", "authors": ["Zihao Zhao", "Xinlong Zhai", "Jinyu Yang", "Chuan Shi"], "summary": "Foundation models have achieved great success in natural language processing\n(NLP) and computer vision (CV). Their success largely stems from the ability to\nintegrate multi-domain knowledge in pre-training and transfer it to target\ndomains. Considering graph data, especially graphs without textual features, is\nubiquitous in real-world applications such as social networks and\nrecommendation systems, some researchers have attempted to extend this paradigm\nto the graph field, aiming to construct graph foundation models. However,\nunlike CV and NLP, there are huge gaps among the semantics and properties of\ngraphs in different domains, while current works still adopt traditional\ncontrastive pre-training strategies designed in the single-domain scenario,\nwhich regard contrastive samples from different domains as equivalent. From\nexperimental investigations, we discovered that inherent domain-specific\ndifferences prevent these strategies from effectively absorbing knowledge from\ndifferent domains to generate informative representations. In this paper, we\npropose a novel multi-domain pre-training and cross-domain transfer framework,\nnamely MDGCL.In the pre-training stage, we design a contrastive learning\nstrategy to substantially recognize and capture domain differences, and\nintroduce domain tokens to encode domain-level global information. In the\ndownstream stage, we introduce a domain attention mechanism to enable\nfine-grained domain knowledge transfer. Extensive experiments on five benchmark\ndatasets have demonstrated that our method outperforms state-of-the-art\nsignificantly, with the maximum improvement of 19.33\\% on accuracy and 19.13\\%\non Macro-F1 score.", "comment": "16 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2506.22510v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.22510v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.23998", "title": "Auto-TA: Towards Scalable Automated Thematic Analysis (TA) via Multi-Agent Large Language Models with Reinforcement Learning", "authors": ["Seungjun Yi", "Joakim Nguyen", "Huimin Xu", "Terence Lim", "Andrew Well", "Mia Markey", "Ying Ding"], "summary": "Congenital heart disease (CHD) presents complex, lifelong challenges often\nunderrepresented in traditional clinical metrics. While unstructured narratives\noffer rich insights into patient and caregiver experiences, manual thematic\nanalysis (TA) remains labor-intensive and unscalable. We propose a fully\nautomated large language model (LLM) pipeline that performs end-to-end TA on\nclinical narratives, which eliminates the need for manual coding or full\ntranscript review. Our system employs a novel multi-agent framework, where\nspecialized LLM agents assume roles to enhance theme quality and alignment with\nhuman analysis. To further improve thematic relevance, we optionally integrate\nreinforcement learning from human feedback (RLHF). This supports scalable,\npatient-centered analysis of large qualitative datasets and allows LLMs to be\nfine-tuned for specific clinical contexts.", "comment": "Presented at ACL 2025 SRW", "pdf_url": "http://arxiv.org/pdf/2506.23998v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.23998v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23077", "title": "Dynamic Contrastive Learning for Hierarchical Retrieval: A Case Study of Distance-Aware Cross-View Geo-Localization", "authors": ["Suofei Zhang", "Xinxin Wang", "Xiaofu Wu", "Quan Zhou", "Haifeng Hu"], "summary": "Existing deep learning-based cross-view geo-localization methods primarily\nfocus on improving the accuracy of cross-domain image matching, rather than\nenabling models to comprehensively capture contextual information around the\ntarget and minimize the cost of localization errors. To support systematic\nresearch into this Distance-Aware Cross-View Geo-Localization (DACVGL) problem,\nwe construct Distance-Aware Campus (DA-Campus), the first benchmark that pairs\nmulti-view imagery with precise distance annotations across three spatial\nresolutions. Based on DA-Campus, we formulate DACVGL as a hierarchical\nretrieval problem across different domains. Our study further reveals that, due\nto the inherent complexity of spatial relationships among buildings, this\nproblem can only be addressed via a contrastive learning paradigm, rather than\nconventional metric learning. To tackle this challenge, we propose Dynamic\nContrastive Learning (DyCL), a novel framework that progressively aligns\nfeature representations according to hierarchical spatial margins. Extensive\nexperiments demonstrate that DyCL is highly complementary to existing\nmulti-scale metric learning methods and yields substantial improvements in both\nhierarchical retrieval performance and overall cross-view geo-localization\naccuracy. Our code and benchmark are publicly available at\nhttps://github.com/anocodetest1/DyCL.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23077v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23077v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23551", "title": "A unified framework on the universal approximation of transformer-type architectures", "authors": ["Jingpu Cheng", "Qianxiao Li", "Ting Lin", "Zuowei Shen"], "summary": "We investigate the universal approximation property (UAP) of transformer-type\narchitectures, providing a unified theoretical framework that extends prior\nresults on residual networks to models incorporating attention mechanisms. Our\nwork identifies token distinguishability as a fundamental requirement for UAP\nand introduces a general sufficient condition that applies to a broad class of\narchitectures. Leveraging an analyticity assumption on the attention layer, we\ncan significantly simplify the verification of this condition, providing a\nnon-constructive approach in establishing UAP for such architectures. We\ndemonstrate the applicability of our framework by proving UAP for transformers\nwith various attention mechanisms, including kernel-based and sparse attention\nmechanisms. The corollaries of our results either generalize prior works or\nestablish UAP for architectures not previously covered. Furthermore, our\nframework offers a principled foundation for designing novel transformer\narchitectures with inherent UAP guarantees, including those with specific\nfunctional symmetries. We propose examples to illustrate these insights.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23551v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.23551v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22511", "title": "Lightning the Night with Generative Artificial Intelligence", "authors": ["Tingting Zhou", "Feng Zhang", "Haoyang Fu", "Baoxiang Pan", "Renhe Zhang", "Feng Lu", "Zhixin Yang"], "summary": "The visible light reflectance data from geostationary satellites is crucial\nfor meteorological observations and plays an important role in weather\nmonitoring and forecasting. However, due to the lack of visible light at night,\nit is impossible to conduct continuous all-day weather observations using\nvisible light reflectance data. This study pioneers the use of generative\ndiffusion models to address this limitation. Based on the multi-band thermal\ninfrared brightness temperature data from the Advanced Geostationary Radiation\nImager (AGRI) onboard the Fengyun-4B (FY4B) geostationary satellite, we\ndeveloped a high-precision visible light reflectance retrieval model, called\nReflectance Diffusion (RefDiff), which enables 0.47~\\mu\\mathrm{m},\n0.65~\\mu\\mathrm{m}, and 0.825~\\mu\\mathrm{m} bands visible light reflectance\nretrieval at night. Compared to the classical models, RefDiff not only\nsignificantly improves accuracy through ensemble averaging but also provides\nuncertainty estimation. Specifically, the SSIM index of RefDiff can reach 0.90,\nwith particularly significant improvements in areas with complex cloud\nstructures and thick clouds. The model's nighttime retrieval capability was\nvalidated using VIIRS nighttime product, demonstrating comparable performance\nto its daytime counterpart. In summary, this research has made substantial\nprogress in the ability to retrieve visible light reflectance at night, with\nthe potential to expand the application of nighttime visible light data.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22511v1", "categories": ["cs.CV", "cs.AI", "eess.IV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22511v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.24006", "title": "Large Language Models Don't Make Sense of Word Problems. A Scoping Review from a Mathematics Education Perspective", "authors": ["Anselm R. Strohmaier", "Wim Van Dooren", "Kathrin Se√üler", "Brian Greer", "Lieven Verschaffel"], "summary": "The progress of Large Language Models (LLMs) like ChatGPT raises the question\nof how they can be integrated into education. One hope is that they can support\nmathematics learning, including word-problem solving. Since LLMs can handle\ntextual input with ease, they appear well-suited for solving mathematical word\nproblems. Yet their real competence, whether they can make sense of the\nreal-world context, and the implications for classrooms remain unclear. We\nconducted a scoping review from a mathematics-education perspective, including\nthree parts: a technical overview, a systematic review of word problems used in\nresearch, and a state-of-the-art empirical evaluation of LLMs on mathematical\nword problems. First, in the technical overview, we contrast the\nconceptualization of word problems and their solution processes between LLMs\nand students. In computer-science research this is typically labeled\nmathematical reasoning, a term that does not align with usage in mathematics\neducation. Second, our literature review of 213 studies shows that the most\npopular word-problem corpora are dominated by s-problems, which do not require\na consideration of realities of their real-world context. Finally, our\nevaluation of GPT-3.5-turbo, GPT-4o-mini, GPT-4.1, and o3 on 287 word problems\nshows that most recent LLMs solve these s-problems with near-perfect accuracy,\nincluding a perfect score on 20 problems from PISA. LLMs still showed\nweaknesses in tackling problems where the real-world context is problematic or\nnon-sensical. In sum, we argue based on all three aspects that LLMs have\nmastered a superficial solution process but do not make sense of word problems,\nwhich potentially limits their value as instructional tools in mathematics\nclassrooms.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.24006v1", "categories": ["cs.CL", "math.HO"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.24006v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23086", "title": "Frequency-enhanced Multi-granularity Context Network for Efficient Vertebrae Segmentation", "authors": ["Jian Shi", "Tianqi You", "Pingping Zhang", "Hongli Zhang", "Rui Xu", "Haojie Li"], "summary": "Automated and accurate segmentation of individual vertebra in 3D CT and MRI\nimages is essential for various clinical applications. Due to the limitations\nof current imaging techniques and the complexity of spinal structures, existing\nmethods still struggle with reducing the impact of image blurring and\ndistinguishing similar vertebrae. To alleviate these issues, we introduce a\nFrequency-enhanced Multi-granularity Context Network (FMC-Net) to improve the\naccuracy of vertebrae segmentation. Specifically, we first apply wavelet\ntransform for lossless downsampling to reduce the feature distortion in blurred\nimages. The decomposed high and low-frequency components are then processed\nseparately. For the high-frequency components, we apply a High-frequency\nFeature Refinement (HFR) to amplify the prominence of key features and filter\nout noises, restoring fine-grained details in blurred images. For the\nlow-frequency components, we use a Multi-granularity State Space Model (MG-SSM)\nto aggregate feature representations with different receptive fields,\nextracting spatially-varying contexts while capturing long-range dependencies\nwith linear complexity. The utilization of multi-granularity contexts is\nessential for distinguishing similar vertebrae and improving segmentation\naccuracy. Extensive experiments demonstrate that our method outperforms\nstate-of-the-art approaches on both CT and MRI vertebrae segmentation datasets.\nThe source code is publicly available at https://github.com/anaanaa/FMCNet.", "comment": "Accepted by MICCAI2025. More modifications my be performed", "pdf_url": "http://arxiv.org/pdf/2506.23086v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23086v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23589", "title": "Transition Matching: Scalable and Flexible Generative Modeling", "authors": ["Neta Shaul", "Uriel Singer", "Itai Gat", "Yaron Lipman"], "summary": "Diffusion and flow matching models have significantly advanced media\ngeneration, yet their design space is well-explored, somewhat limiting further\nimprovements. Concurrently, autoregressive (AR) models, particularly those\ngenerating continuous tokens, have emerged as a promising direction for\nunifying text and media generation. This paper introduces Transition Matching\n(TM), a novel discrete-time, continuous-state generative paradigm that unifies\nand advances both diffusion/flow models and continuous AR generation. TM\ndecomposes complex generation tasks into simpler Markov transitions, allowing\nfor expressive non-deterministic probability transition kernels and arbitrary\nnon-continuous supervision processes, thereby unlocking new flexible design\navenues. We explore these choices through three TM variants: (i) Difference\nTransition Matching (DTM), which generalizes flow matching to discrete-time by\ndirectly learning transition probabilities, yielding state-of-the-art image\nquality and text adherence as well as improved sampling efficiency. (ii)\nAutoregressive Transition Matching (ARTM) and (iii) Full History Transition\nMatching (FHTM) are partially and fully causal models, respectively, that\ngeneralize continuous AR methods. They achieve continuous causal AR generation\nquality comparable to non-causal approaches and potentially enable seamless\nintegration with existing AR text generation techniques. Notably, FHTM is the\nfirst fully causal model to match or surpass the performance of flow-based\nmethods on text-to-image task in continuous domains. We demonstrate these\ncontributions through a rigorous large-scale comparison of TM variants and\nrelevant baselines, maintaining a fixed architecture, training data, and\nhyperparameters.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23589v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.23589v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22512", "title": "Ask before you Build: Rethinking AI-for-Good in Human Trafficking Interventions", "authors": ["Pratheeksha Nair", "Gabriel Lefebvre", "Sophia Garrel", "Maryam Molamohammadi", "Reihaneh Rabbany"], "summary": "AI for good initiatives often rely on the assumption that technical\ninterventions can resolve complex social problems. In the context of human\ntrafficking (HT), such techno-solutionism risks oversimplifying exploitation,\nreinforcing power imbalances and causing harm to the very communities AI claims\nto support. In this paper, we introduce the Radical Questioning (RQ) framework\nas a five step, pre-project ethical assessment tool to critically evaluate\nwhether AI should be built at all, especially in domains involving marginalized\npopulations and entrenched systemic injustice. RQ does not replace principles\nbased ethics but precedes it, offering an upstream, deliberative space to\nconfront assumptions, map power, and consider harms before design. Using a case\nstudy in AI for HT, we demonstrate how RQ reveals overlooked sociocultural\ncomplexities and guides us away from surveillance based interventions toward\nsurvivor empowerment tools. While developed in the context of HT, RQ's five\nstep structure can generalize to other domains, though the specific questions\nmust be contextual. This paper situates RQ within a broader AI ethics\nphilosophy that challenges instrumentalist norms and centers relational,\nreflexive responsibility.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22512v1", "categories": ["cs.CY", "cs.AI"], "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.22512v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.24016", "title": "EXPERT: An Explainable Image Captioning Evaluation Metric with Structured Explanations", "authors": ["Hyunjong Kim", "Sangyeop Kim", "Jongheon Jeong", "Yeongjae Cho", "Sungzoon Cho"], "summary": "Recent advances in large language models and vision-language models have led\nto growing interest in explainable evaluation metrics for image captioning.\nHowever, these metrics generate explanations without standardized criteria, and\nthe overall quality of the generated explanations remains unverified. In this\npaper, we propose EXPERT, a reference-free evaluation metric that provides\nstructured explanations based on three fundamental criteria: fluency,\nrelevance, and descriptiveness. By constructing large-scale datasets of\nhigh-quality structured explanations, we develop a two-stage evaluation\ntemplate to effectively supervise a vision-language model for both scoring and\nexplanation generation. EXPERT achieves state-of-the-art results on benchmark\ndatasets while providing significantly higher-quality explanations than\nexisting metrics, as validated through comprehensive human evaluation. Our code\nand datasets are available at https://github.com/hjkim811/EXPERT.", "comment": "Accepted at ACL 2025 Findings", "pdf_url": "http://arxiv.org/pdf/2506.24016v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.24016v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23088", "title": "Where, What, Why: Towards Explainable Driver Attention Prediction", "authors": ["Yuchen Zhou", "Jiayu Tang", "Xiaoyan Xiao", "Yueyao Lin", "Linkai Liu", "Zipeng Guo", "Hao Fei", "Xiaobo Xia", "Chao Gou"], "summary": "Modeling task-driven attention in driving is a fundamental challenge for both\nautonomous vehicles and cognitive science. Existing methods primarily predict\nwhere drivers look by generating spatial heatmaps, but fail to capture the\ncognitive motivations behind attention allocation in specific contexts, which\nlimits deeper understanding of attention mechanisms. To bridge this gap, we\nintroduce Explainable Driver Attention Prediction, a novel task paradigm that\njointly predicts spatial attention regions (where), parses attended semantics\n(what), and provides cognitive reasoning for attention allocation (why). To\nsupport this, we present W3DA, the first large-scale explainable driver\nattention dataset. It enriches existing benchmarks with detailed semantic and\ncausal annotations across diverse driving scenarios, including normal\nconditions, safety-critical situations, and traffic accidents. We further\npropose LLada, a Large Language model-driven framework for driver attention\nprediction, which unifies pixel modeling, semantic parsing, and cognitive\nreasoning within an end-to-end architecture. Extensive experiments demonstrate\nthe effectiveness of LLada, exhibiting robust generalization across datasets\nand driving conditions. This work serves as a key step toward a deeper\nunderstanding of driver attention mechanisms, with significant implications for\nautonomous driving, intelligent driver training, and human-computer\ninteraction.", "comment": "Accepted by ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2506.23088v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23088v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23596", "title": "When Will It Fail?: Anomaly to Prompt for Forecasting Future Anomalies in Time Series", "authors": ["Min-Yeong Park", "Won-Jeong Lee", "Seong Tae Kim", "Gyeong-Moon Park"], "summary": "Recently, forecasting future abnormal events has emerged as an important\nscenario to tackle real-world necessities. However, the solution of predicting\nspecific future time points when anomalies will occur, known as Anomaly\nPrediction (AP), remains under-explored. Existing methods dealing with time\nseries data fail in AP, focusing only on immediate anomalies or failing to\nprovide precise predictions for future anomalies. To address the AP task, we\npropose a novel framework called Anomaly to Prompt (A2P), comprised of\nAnomaly-Aware Forecasting (AAF) and Synthetic Anomaly Prompting (SAP). To\nenable the forecasting model to forecast abnormal time points, we adopt a\nstrategy to learn the relationships of anomalies. For the robust detection of\nanomalies, our proposed SAP introduces a learnable Anomaly Prompt Pool (APP)\nthat simulates diverse anomaly patterns using signal adaptive prompt.\nComprehensive experiments on multiple real-world datasets demonstrate the\nsuperiority of A2P over state-of-the-art methods, showcasing its ability to\npredict future anomalies. Our implementation code is available at\nhttps://github.com/KU-VGI/AP.", "comment": "18 pages, 10 figures, 12 tables, ICML 2025", "pdf_url": "http://arxiv.org/pdf/2506.23596v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.23596v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22515", "title": "In-context learning for the classification of manipulation techniques in phishing emails", "authors": ["Antony Dalmiere", "Guillaume Auriol", "Vincent Nicomette", "Pascal Marchand"], "summary": "Traditional phishing detection often overlooks psychological manipulation.\nThis study investigates using Large Language Model (LLM) In-Context Learning\n(ICL) for fine-grained classification of phishing emails based on a taxonomy of\n40 manipulation techniques. Using few-shot examples with GPT-4o-mini on\nreal-world French phishing emails (SignalSpam), we evaluated performance\nagainst a human-annotated test set (100 emails). The approach effectively\nidentifies prevalent techniques (e.g., Baiting, Curiosity Appeal, Request For\nMinor Favor) with a promising accuracy of 0.76. This work demonstrates ICL's\npotential for nuanced phishing analysis and provides insights into attacker\nstrategies.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22515v1", "categories": ["cs.CR", "cs.AI"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.22515v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.24068", "title": "STACK: Adversarial Attacks on LLM Safeguard Pipelines", "authors": ["Ian R. McKenzie", "Oskar J. Hollinsworth", "Tom Tseng", "Xander Davies", "Stephen Casper", "Aaron D. Tucker", "Robert Kirk", "Adam Gleave"], "summary": "Frontier AI developers are relying on layers of safeguards to protect against\ncatastrophic misuse of AI systems. Anthropic guards their latest Claude 4 Opus\nmodel using one such defense pipeline, and other frontier developers including\nGoogle DeepMind and OpenAI pledge to soon deploy similar defenses. However, the\nsecurity of such pipelines is unclear, with limited prior work evaluating or\nattacking these pipelines. We address this gap by developing and red-teaming an\nopen-source defense pipeline. First, we find that a novel few-shot-prompted\ninput and output classifier outperforms state-of-the-art open-weight safeguard\nmodel ShieldGemma across three attacks and two datasets, reducing the attack\nsuccess rate (ASR) to 0% on the catastrophic misuse dataset ClearHarm. Second,\nwe introduce a STaged AttaCK (STACK) procedure that achieves 71% ASR on\nClearHarm in a black-box attack against the few-shot-prompted classifier\npipeline. Finally, we also evaluate STACK in a transfer setting, achieving 33%\nASR, providing initial evidence that it is feasible to design attacks with no\naccess to the target pipeline. We conclude by suggesting specific mitigations\nthat developers could use to thwart staged attacks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.24068v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.24068v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23104", "title": "DC-TTA: Divide-and-Conquer Framework for Test-Time Adaptation of Interactive Segmentation", "authors": ["Jihun Kim", "Hoyong Kwon", "Hyeokjun Kweon", "Wooseong Jeong", "Kuk-Jin Yoon"], "summary": "Interactive segmentation (IS) allows users to iteratively refine object\nboundaries with minimal cues, such as positive and negative clicks. While the\nSegment Anything Model (SAM) has garnered attention in the IS community for its\npromptable segmentation capabilities, it often struggles in specialized domains\nor when handling complex scenarios (e.g., camouflaged or multi-part objects).\nTo overcome these challenges, we propose DC-TTA, a novel test-time adaptation\n(TTA) framework that adapts SAM on a per-sample basis by leveraging user\ninteractions as supervision. Instead of forcing a single model to incorporate\nall user clicks at once, DC-TTA partitions the clicks into more coherent\nsubsets, each processed independently via TTA with a separated model. This\nDivide-and-Conquer strategy reduces conflicts among diverse cues and enables\nmore localized updates. Finally, we merge the adapted models to form a unified\npredictor that integrates the specialized knowledge from each subset.\nExperimental results across various benchmarks demonstrate that DC-TTA\nsignificantly outperforms SAM's zero-shot results and conventional TTA methods,\neffectively handling complex tasks such as camouflaged object segmentation with\nfewer interactions and improved accuracy.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23104v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23104v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23629", "title": "A Nonlinear Low-rank Representation Model with Convolutional Neural Network for Imputing Water Quality Data", "authors": ["Xin Liao", "Bing Yang", "Cai Yu"], "summary": "The integrity of Water Quality Data (WQD) is critical in environmental\nmonitoring for scientific decision-making and ecological protection. However,\nwater quality monitoring systems are often challenged by large amounts of\nmissing data due to unavoidable problems such as sensor failures and\ncommunication delays, which further lead to water quality data becoming\nHigh-Dimensional and Sparse (HDS). Traditional data imputation methods are\ndifficult to depict the potential dynamics and fail to capture the deep data\nfeatures, resulting in unsatisfactory imputation performance. To effectively\naddress the above issues, this paper proposes a Nonlinear Low-rank\nRepresentation model (NLR) with Convolutional Neural Networks (CNN) for\nimputing missing WQD, which utilizes CNNs to implement two ideas: a) fusing\ntemporal features to model the temporal dependence of data between time slots,\nand b) Extracting nonlinear interactions and local patterns to mine\nhigher-order relationships features and achieve deep fusion of multidimensional\ninformation. Experimental studies on three real water quality datasets\ndemonstrate that the proposed model significantly outperforms existing\nstate-of-the-art data imputation models in terms of estimation accuracy. It\nprovides an effective approach for handling water quality monitoring data in\ncomplex dynamic environments.", "comment": "7 pages, 2 figures, conference", "pdf_url": "http://arxiv.org/pdf/2506.23629v1", "categories": ["cs.LG", "cs.AI", "68T07(Primary) 62M10, 65C60 (Secondary)", "I.2.7"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.23629v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22516", "title": "Can \"consciousness\" be observed from large language model (LLM) internal states? Dissecting LLM representations obtained from Theory of Mind test with Integrated Information Theory and Span Representation analysis", "authors": ["Jingkai Li"], "summary": "Integrated Information Theory (IIT) provides a quantitative framework for\nexplaining consciousness phenomenon, positing that conscious systems comprise\nelements integrated through causal properties. We apply IIT 3.0 and 4.0 -- the\nlatest iterations of this framework -- to sequences of Large Language Model\n(LLM) representations, analyzing data derived from existing Theory of Mind\n(ToM) test results. Our study systematically investigates whether the\ndifferences of ToM test performances, when presented in the LLM\nrepresentations, can be revealed by IIT estimates, i.e., $\\Phi^{\\max}$ (IIT\n3.0), $\\Phi$ (IIT 4.0), Conceptual Information (IIT 3.0), and $\\Phi$-structure\n(IIT 4.0). Furthermore, we compare these metrics with the Span Representations\nindependent of any estimate for consciousness. This additional effort aims to\ndifferentiate between potential \"consciousness\" phenomena and inherent\nseparations within LLM representational space. We conduct comprehensive\nexperiments examining variations across LLM transformer layers and linguistic\nspans from stimuli. Our results suggest that sequences of contemporary\nTransformer-based LLM representations lack statistically significant indicators\nof observed \"consciousness\" phenomena but exhibit intriguing patterns under\n$\\textit{spatio}$-permutational analyses. The Appendix and code are available\nas Supplementary Materials at: https://doi.org/10.1016/j.nlp.2025.100163.", "comment": "Published as a journal paper at:\n  https://doi.org/10.1016/j.nlp.2025.100163", "pdf_url": "http://arxiv.org/pdf/2506.22516v1", "categories": ["cs.CL", "cs.AI", "cs.NE", "q-bio.NC"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.22516v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.24106", "title": "On the Predictive Power of Representation Dispersion in Language Models", "authors": ["Yanhong Li", "Ming Li", "Karen Livescu", "Jiawei Zhou"], "summary": "We show that a language model's ability to predict text is tightly linked to\nthe breadth of its embedding space: models that spread their contextual\nrepresentations more widely tend to achieve lower perplexity. Concretely, we\nfind that representation dispersion - the average pairwise cosine distance\namong hidden vectors - strongly and negatively correlates with perplexity\nacross diverse model families (LLaMA, Qwen, and others) and domains (Wikipedia,\nnews, scientific abstracts). Beyond illustrating this link, we show how\ndispersion can be leveraged for a range of practical tasks without requiring\nlabeled data. First, measuring dispersion on unlabeled text allows us to\npredict downstream accuracy in new domains, offering a data-efficient tool for\nmodel selection. Next, we find that identifying layers with higher dispersion\npinpoints the best representations for retrieval-based methods such as kNN-LM,\nbypassing exhaustive layer-by-layer searches. Finally, we integrate a simple\npush-away objective into training, which increases dispersion in both\nsingle-domain and cross-domain scenarios and directly improves perplexity in\neach.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.24106v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.24106v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23106", "title": "Computer-Aided Multi-Stroke Character Simplification by Stroke Removal", "authors": ["Ryo Ishiyama", "Shinnosuke Matsuo", "Seiichi Uchida"], "summary": "Multi-stroke characters in scripts such as Chinese and Japanese can be highly\ncomplex, posing significant challenges for both native speakers and,\nespecially, non-native learners. If these characters can be simplified without\ndegrading their legibility, it could reduce learning barriers for non-native\nspeakers, facilitate simpler and legible font designs, and contribute to\nefficient character-based communication systems. In this paper, we propose a\nframework to systematically simplify multi-stroke characters by selectively\nremoving strokes while preserving their overall legibility. More specifically,\nwe use a highly accurate character recognition model to assess legibility and\nremove those strokes that minimally impact it. Experimental results on 1,256\ncharacter classes with 5, 10, 15, and 20 strokes reveal several key findings,\nincluding the observation that even after removing multiple strokes, many\ncharacters remain distinguishable. These findings suggest the potential for\nmore formalized simplification strategies.", "comment": "ICDAR2025 (Oral)", "pdf_url": "http://arxiv.org/pdf/2506.23106v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23106v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23679", "title": "Learning Modular Exponentiation with Transformers", "authors": ["David Demitri Africa", "Sara M. Kapoor", "Theo Simon Sorg"], "summary": "Modular exponentiation is crucial to number theory and cryptography, yet\nremains largely unexplored from a mechanistic interpretability standpoint. We\ntrain a 4-layer encoder-decoder Transformer model to perform this operation and\ninvestigate the emergence of numerical reasoning during training. Utilizing\nprincipled sampling strategies, PCA-based embedding analysis, and activation\npatching, we examine how number-theoretic properties are encoded within the\nmodel. We find that reciprocal operand training leads to strong performance\ngains, with sudden generalization across related moduli. These synchronized\naccuracy surges reflect grokking-like dynamics, suggesting the model\ninternalizes shared arithmetic structure. We also find a subgraph consisting\nentirely of attention heads in the final layer sufficient to achieve full\nperformance on the task of regular exponentiation. These results suggest that\ntransformer models learn modular arithmetic through specialized computational\ncircuits, paving the way for more interpretable and efficient neural approaches\nto modular exponentiation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23679v1", "categories": ["cs.LG", "cs.AI", "cs.CR"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.23679v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22518", "title": "Weak-to-Strong GraphRAG: Aligning Weak Retrievers with Large Language Models for Graph-based Retrieval Augmented Generation", "authors": ["Deyu Zou", "Yongqiang Chen", "Mufei Li", "Siqi Miao", "Chenxi Liu", "Bo Han", "James Cheng", "Pan Li"], "summary": "Graph-based retrieval-augmented generation (RAG) enables large language\nmodels (LLMs) to ground responses with structured external knowledge from\nup-to-date knowledge graphs (KGs) and reduce hallucinations. However, LLMs\noften rely on a weak retriever in graph-based RAG: I) Due to the lack of ground\ntruth, the retriever is often trained on weak supervision, which often\nintroduces spurious signals to the LLMs. II) Due to the abstraction of graph\ndata, the retrieved knowledge is often presented in unorganized forms. To\nmitigate the issue, we present Refined Graph-based RAG (ReG) to align weak\nretrievers to LLMs for graph-based RAG. Specifically, ReG incorporates LLM\nfeedback to get rid of spurious signals and improve the quality of the\nsupervision. Meanwhile, ReG introduces a structure-aware reorganization module\nto refactor the retrieval results into logically coherent evidence chains.\nExperiments on prominent benchmarks demonstrate that ReG significantly and\nconsistently brings improvements across different LLM backbones by up to 10%.\nThe improved supervision quality enables ReG to match the state-of-the-art\nperformance with 5% training data and to transfer to out-of-distribution KGs.\nNotably, when adopted to reasoning-based LLMs, ReG reduces the reasoning token\ncost by up to 30% and improves the performance by up to 4%.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22518v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.22518v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.24117", "title": "Computational Detection of Intertextual Parallels in Biblical Hebrew: A Benchmark Study Using Transformer-Based Language Models", "authors": ["David M. Smiley"], "summary": "Identifying parallel passages in biblical Hebrew is foundational in biblical\nscholarship for uncovering intertextual relationships. Traditional methods rely\non manual comparison, which is labor-intensive and prone to human error. This\nstudy evaluates the potential of pre-trained transformer-based language models,\nincluding E5, AlephBERT, MPNet, and LaBSE, for detecting textual parallels in\nthe Hebrew Bible. Focusing on known parallels between the books of Samuel/Kings\nand Chronicles, I assessed each model's capability to generate word embeddings\nthat delineate parallel from non-parallel passages. Utilizing cosine similarity\nand Wasserstein Distance measures, I found that E5 and AlephBERT show\nsignificant promise, with E5 excelling in parallel detection and AlephBERT\ndemonstrating stronger non-parallel differentiation. These findings indicate\nthat pre-trained models can enhance the efficiency and accuracy of detecting\nintertextual parallels in ancient texts, suggesting broader applications for\nancient language studies.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.24117v1", "categories": ["cs.CL"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.24117v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23108", "title": "Hierarchical Corpus-View-Category Refinement for Carotid Plaque Risk Grading in Ultrasound", "authors": ["Zhiyuan Zhu", "Jian Wang", "Yong Jiang", "Tong Han", "Yuhao Huang", "Ang Zhang", "Kaiwen Yang", "Mingyuan Luo", "Zhe Liu", "Yaofei Duan", "Dong Ni", "Tianhong Tang", "Xin Yang"], "summary": "Accurate carotid plaque grading (CPG) is vital to assess the risk of\ncardiovascular and cerebrovascular diseases. Due to the small size and high\nintra-class variability of plaque, CPG is commonly evaluated using a\ncombination of transverse and longitudinal ultrasound views in clinical\npractice. However, most existing deep learning-based multi-view classification\nmethods focus on feature fusion across different views, neglecting the\nimportance of representation learning and the difference in class features. To\naddress these issues, we propose a novel Corpus-View-Category Refinement\nFramework (CVC-RF) that processes information from Corpus-, View-, and\nCategory-levels, enhancing model performance. Our contribution is four-fold.\nFirst, to the best of our knowledge, we are the foremost deep learning-based\nmethod for CPG according to the latest Carotid Plaque-RADS guidelines. Second,\nwe propose a novel center-memory contrastive loss, which enhances the network's\nglobal modeling capability by comparing with representative cluster centers and\ndiverse negative samples at the Corpus level. Third, we design a cascaded\ndown-sampling attention module to fuse multi-scale information and achieve\nimplicit feature interaction at the View level. Finally, a parameter-free\nmixture-of-experts weighting strategy is introduced to leverage class\nclustering knowledge to weight different experts, enabling feature decoupling\nat the Category level. Experimental results indicate that CVC-RF effectively\nmodels global features via multi-level refinement, achieving state-of-the-art\nperformance in the challenging CPG task.", "comment": "Accepted at MICCAI 2025", "pdf_url": "http://arxiv.org/pdf/2506.23108v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23108v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23719", "title": "DABstep: Data Agent Benchmark for Multi-step Reasoning", "authors": ["Alex Egg", "Martin Iglesias Goyanes", "Friso Kingma", "Andreu Mora", "Leandro von Werra", "Thomas Wolf"], "summary": "We introduce DABstep, a novel benchmark for evaluating AI agents on realistic\nmulti-step data analysis tasks. DABstep comprises over 450 real-world\nchallenges derived from a financial analytics platform, requiring models to\ncombine code-based data processing with contextual reasoning over heterogeneous\ndocumentation. Each task demands an iterative, multi-step problem-solving\napproach, testing capabilities in data manipulation, cross-referencing multiple\nsources, and precise result reporting. The benchmark provides a factoid-style\nanswer format with automatic correctness checks for objective scoring at scale.\nWe evaluate leading LLM-based agents, revealing a substantial performance gap:\neven the best agent achieves only 14.55% accuracy on the hardest tasks. We\ndetail our benchmark's design, dataset composition, task formulation,\nevaluation protocol, report baseline results and analyze failure modes. DABstep\nis released with a public leaderboard and toolkit to accelerate research in\nautonomous data analysis.", "comment": "13 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2506.23719v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.23719v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22520", "title": "Exploring Artificial Intelligence Tutor Teammate Adaptability to Harness Discovery Curiosity and Promote Learning in the Context of Interactive Molecular Dynamics", "authors": ["Mustafa Demir", "Jacob Miratsky", "Jonathan Nguyen", "Chun Kit Chan", "Punya Mishra", "Abhishek Singharoy"], "summary": "This study examines the impact of an Artificial Intelligence tutor teammate\n(AI) on student curiosity-driven engagement and learning effectiveness during\nInteractive Molecular Dynamics (IMD) tasks on the Visual Molecular Dynamics\nplatform. It explores the role of the AI's curiosity-triggering and response\nbehaviors in stimulating and sustaining student curiosity, affecting the\nfrequency and complexity of student-initiated questions. The study further\nassesses how AI interventions shape student engagement, foster discovery\ncuriosity, and enhance team performance within the IMD learning environment.\nUsing a Wizard-of-Oz paradigm, a human experimenter dynamically adjusts the AI\ntutor teammate's behavior through a large language model. By employing a\nmixed-methods exploratory design, a total of 11 high school students\nparticipated in four IMD tasks that involved molecular visualization and\ncalculations, which increased in complexity over a 60-minute period. Team\nperformance was evaluated through real-time observation and recordings, whereas\nteam communication was measured by question complexity and AI's\ncuriosity-triggering and response behaviors. Cross Recurrence Quantification\nAnalysis (CRQA) metrics reflected structural alignment in coordination and were\nlinked to communication behaviors. High-performing teams exhibited superior\ntask completion, deeper understanding, and increased engagement. Advanced\nquestions were associated with AI curiosity-triggering, indicating heightened\nengagement and cognitive complexity. CRQA metrics highlighted dynamic\nsynchronization in student-AI interactions, emphasizing structured yet adaptive\nengagement to promote curiosity. These proof-of-concept findings suggest that\nthe AI's dual role as a teammate and educator indicates its capacity to provide\nadaptive feedback, sustaining engagement and epistemic curiosity.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22520v1", "categories": ["cs.HC", "cs.AI", "cs.CE", "cs.CY"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.22520v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.22449", "title": "Computational Analysis of Climate Policy", "authors": ["Carolyn Hicks"], "summary": "This thesis explores the impact of the Climate Emergency movement on local\ngovernment climate policy, using computational methods. The Climate Emergency\nmovement sought to accelerate climate action at local government level through\nthe mechanism of Climate Emergency Declarations (CEDs), resulting in a series\nof commitments from councils to treat climate change as an emergency. With the\naim of assessing the potential of current large language models to answer\ncomplex policy questions, I first built and configured a system named PALLM\n(Policy Analysis with a Large Language Model), using the OpenAI model GPT-4.\nThis system is designed to apply a conceptual framework for climate emergency\nresponse plans to a dataset of climate policy documents. I validated the\nperformance of this system with the help of local government policymakers, by\ngenerating analyses of the climate policies of 11 local governments in Victoria\nand assessing the policymakers' level of agreement with PALLM's responses.\nHaving established that PALLM's performance is satisfactory, I used it to\nconduct a large-scale analysis of current policy documents from local\ngovernments in the state of Victoria, Australia. This thesis presents the\nmethodology and results of this analysis, comparing the results for councils\nwhich have passed a CED to those which did not. This study finds that GPT-4 is\ncapable of high-level policy analysis, with limitations including a lack of\nreliable attribution, and can also enable more nuanced analysis by researchers.\nIts use in this research shows that councils which have passed a CED are more\nlikely to have a recent and climate-specific policy, and show more attention to\nurgency, prioritisation, and equity and social justice, than councils which\nhave not. It concludes that the ability to assess policy documents at scale\nopens up exciting new opportunities for policy researchers.", "comment": "Master's thesis", "pdf_url": "http://arxiv.org/pdf/2506.22449v1", "categories": ["cs.CY", "cs.CL"], "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.22449v1", "date": "2025-06-13", "updated": "2025-06-13"}
{"id": "2506.23115", "title": "MoCa: Modality-aware Continual Pre-training Makes Better Bidirectional Multimodal Embeddings", "authors": ["Haonan Chen", "Hong Liu", "Yuping Luo", "Liang Wang", "Nan Yang", "Furu Wei", "Zhicheng Dou"], "summary": "Multimodal embedding models, built upon causal Vision Language Models (VLMs),\nhave shown promise in various tasks. However, current approaches face three key\nlimitations: the use of causal attention in VLM backbones is suboptimal for\nembedding tasks; scalability issues due to reliance on high-quality labeled\npaired data for contrastive learning; and limited diversity in training\nobjectives and data. To address these issues, we propose MoCa, a two-stage\nframework for transforming pre-trained VLMs into effective bidirectional\nmultimodal embedding models. The first stage, Modality-aware Continual\nPre-training, introduces a joint reconstruction objective that simultaneously\ndenoises interleaved text and image inputs, enhancing bidirectional\ncontext-aware reasoning. The second stage, Heterogeneous Contrastive\nFine-tuning, leverages diverse, semantically rich multimodal data beyond simple\nimage-caption pairs to enhance generalization and alignment. Our method\naddresses the stated limitations by introducing bidirectional attention through\ncontinual pre-training, scaling effectively with massive unlabeled datasets via\njoint reconstruction objectives, and utilizing diverse multimodal data for\nenhanced representation robustness. Experiments demonstrate that MoCa\nconsistently improves performance across MMEB and ViDoRe-v2 benchmarks,\nachieving new state-of-the-art results, and exhibits strong scalability with\nboth model size and training data on MMEB.", "comment": "Homepage: https://haon-chen.github.io/MoCa/", "pdf_url": "http://arxiv.org/pdf/2506.23115v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23115v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23726", "title": "System-Embedded Diffusion Bridge Models", "authors": ["Bartlomiej Sobieski", "Matthew Tivnan", "Yuang Wang", "Siyeop Yoon", "Pengfei Jin", "Dufan Wu", "Quanzheng Li", "Przemyslaw Biecek"], "summary": "Solving inverse problems -- recovering signals from incomplete or noisy\nmeasurements -- is fundamental in science and engineering. Score-based\ngenerative models (SGMs) have recently emerged as a powerful framework for this\ntask. Two main paradigms have formed: unsupervised approaches that adapt\npretrained generative models to inverse problems, and supervised bridge methods\nthat train stochastic processes conditioned on paired clean and corrupted data.\nWhile the former typically assume knowledge of the measurement model, the\nlatter have largely overlooked this structural information. We introduce System\nembedded Diffusion Bridge Models (SDBs), a new class of supervised bridge\nmethods that explicitly embed the known linear measurement system into the\ncoefficients of a matrix-valued SDE. This principled integration yields\nconsistent improvements across diverse linear inverse problems and demonstrates\nrobust generalization under system misspecification between training and\ndeployment, offering a promising solution to real-world applications.", "comment": "Preprint", "pdf_url": "http://arxiv.org/pdf/2506.23726v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.23726v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22521", "title": "A Survey on Model Extraction Attacks and Defenses for Large Language Models", "authors": ["Kaixiang Zhao", "Lincan Li", "Kaize Ding", "Neil Zhenqiang Gong", "Yue Zhao", "Yushun Dong"], "summary": "Model extraction attacks pose significant security threats to deployed\nlanguage models, potentially compromising intellectual property and user\nprivacy. This survey provides a comprehensive taxonomy of LLM-specific\nextraction attacks and defenses, categorizing attacks into functionality\nextraction, training data extraction, and prompt-targeted attacks. We analyze\nvarious attack methodologies including API-based knowledge distillation, direct\nquerying, parameter recovery, and prompt stealing techniques that exploit\ntransformer architectures. We then examine defense mechanisms organized into\nmodel protection, data privacy protection, and prompt-targeted strategies,\nevaluating their effectiveness across different deployment scenarios. We\npropose specialized metrics for evaluating both attack effectiveness and\ndefense performance, addressing the specific challenges of generative language\nmodels. Through our analysis, we identify critical limitations in current\napproaches and propose promising research directions, including integrated\nattack methodologies and adaptive defense mechanisms that balance security with\nmodel utility. This work serves NLP researchers, ML engineers, and security\nprofessionals seeking to protect language models in production environments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22521v1", "categories": ["cs.CR", "cs.AI"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.22521v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.22481", "title": "Theories of \"Sexuality\" in Natural Language Processing Bias Research", "authors": ["Jacob Hobbs"], "summary": "In recent years, significant advancements in the field of Natural Language\nProcessing (NLP) have positioned commercialized language models as\nwide-reaching, highly useful tools. In tandem, there has been an explosion of\nmultidisciplinary research examining how NLP tasks reflect, perpetuate, and\namplify social biases such as gender and racial bias. A significant gap in this\nscholarship is a detailed analysis of how queer sexualities are encoded and\n(mis)represented by both NLP systems and practitioners. Following previous work\nin the field of AI fairness, we document how sexuality is defined and\noperationalized via a survey and analysis of 55 articles that quantify\nsexuality-based NLP bias. We find that sexuality is not clearly defined in a\nmajority of the literature surveyed, indicating a reliance on assumed or\nnormative conceptions of sexual/romantic practices and identities. Further, we\nfind that methods for extracting biased outputs from NLP technologies often\nconflate gender and sexual identities, leading to monolithic conceptions of\nqueerness and thus improper quantifications of bias. With the goal of improving\nsexuality-based NLP bias analyses, we conclude with recommendations that\nencourage more thorough engagement with both queer communities and\ninterdisciplinary literature.", "comment": "17 pages, 9 tables, undergraduate senior thesis, submitted to The\n  Spectra: The Virginia Engineering and Science Research Journal", "pdf_url": "http://arxiv.org/pdf/2506.22481v1", "categories": ["cs.CY", "cs.CL"], "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.22481v1", "date": "2025-06-22", "updated": "2025-06-22"}
{"id": "2506.23120", "title": "Enhancing Spatial Reasoning in Multimodal Large Language Models through Reasoning-based Segmentation", "authors": ["Zhenhua Ning", "Zhuotao Tian", "Shaoshuai Shi", "Guangming Lu", "Daojing He", "Wenjie Pei", "Li Jiang"], "summary": "Recent advances in point cloud perception have demonstrated remarkable\nprogress in scene understanding through vision-language alignment leveraging\nlarge language models (LLMs). However, existing methods may still encounter\nchallenges in handling complex instructions that require accurate spatial\nreasoning, even if the 3D point cloud data provides detailed spatial cues such\nas size and position for identifying the targets. To tackle this issue, we\npropose Relevant Reasoning Segmentation (R$^2$S), a reasoning-based\nsegmentation framework. The framework emulates human cognitive processes by\ndecomposing spatial reasoning into two sequential stages: first identifying\nrelevant elements, then processing instructions guided by their associated\nvisual priors. Furthermore, acknowledging the inadequacy of existing datasets\nin complex reasoning tasks, we introduce 3D ReasonSeg, a reasoning-based\nsegmentation dataset comprising 25,185 training samples and 3,966 validation\nsamples with precise annotations. Both quantitative and qualitative experiments\ndemonstrate that the R$^2$S and 3D ReasonSeg effectively endow 3D point cloud\nperception with stronger spatial reasoning capabilities, and we hope that they\ncan serve as a new baseline and benchmark for future work.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23120v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23120v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23731", "title": "Radioactive Watermarks in Diffusion and Autoregressive Image Generative Models", "authors": ["Michel Meintz", "Jan Dubi≈Ñski", "Franziska Boenisch", "Adam Dziedzic"], "summary": "Image generative models have become increasingly popular, but training them\nrequires large datasets that are costly to collect and curate. To circumvent\nthese costs, some parties may exploit existing models by using the generated\nimages as training data for their own models. In general, watermarking is a\nvaluable tool for detecting unauthorized use of generated images. However, when\nthese images are used to train a new model, watermarking can only enable\ndetection if the watermark persists through training and remains identifiable\nin the outputs of the newly trained model - a property known as radioactivity.\nWe analyze the radioactivity of watermarks in images generated by diffusion\nmodels (DMs) and image autoregressive models (IARs). We find that existing\nwatermarking methods for DMs fail to retain radioactivity, as watermarks are\neither erased during encoding into the latent space or lost in the\nnoising-denoising process (during the training in the latent space). Meanwhile,\ndespite IARs having recently surpassed DMs in image generation quality and\nefficiency, no radioactive watermarking methods have been proposed for them. To\novercome this limitation, we propose the first watermarking method tailored for\nIARs and with radioactivity in mind - drawing inspiration from techniques in\nlarge language models (LLMs), which share IARs' autoregressive paradigm. Our\nextensive experimental evaluation highlights our method's effectiveness in\npreserving radioactivity within IARs, enabling robust provenance tracking, and\npreventing unauthorized use of their generated images.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23731v1", "categories": ["cs.LG", "cs.CV"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.23731v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22523", "title": "Red Teaming for Generative AI, Report on a Copyright-Focused Exercise Completed in an Academic Medical Center", "authors": ["James Wen", "Sahil Nalawade", "Zhiwei Liang", "Catherine Bielick", "Marisa Ferrara Boston", "Alexander Chowdhury", "Adele Collin", "Luigi De Angelis", "Jacob Ellen", "Heather Frase", "Rodrigo R. Gameiro", "Juan Manuel Gutierrez", "Pooja Kadam", "Murat Keceli", "Srikanth Krishnamurthy", "Anne Kwok", "Yanan Lance Lu", "Heather Mattie", "Liam G. McCoy", "Katherine Miller", "Allison C. Morgan", "Marlene Louisa Moerig", "Trang Nguyen", "Alexander Owen-Post", "Alex D. Ruiz", "Sreekar Reddy Puchala", "Soujanya Samineni", "Takeshi Tohyama", "Varun Ullanat", "Carmine Valenza", "Camilo Velez", "Pengcheng Wang", "Anna Wuest", "Yuxiang Zhou", "Yingde Zhu", "Jason M. Johnson", "Jennifer Willcox", "Francis J. Vitiello", "Leo Anthony G. Celi", "Renato Umeton"], "summary": "Generative AI is present in multiple industries. Dana-Farber Cancer\nInstitute, in partnership with Microsoft, has created an internal AI tool,\nGPT4DFCI. Together we hosted a red teaming event to assess whether the\nunderlying GPT models that support the tool would output copyrighted data. Our\nteams focused on reproducing content from books, news articles, scientific\narticles, and electronic health records. We found isolated instances where\nGPT4DFCI was able to identify copyrighted material and reproduce exact quotes\nfrom famous books which indicates that copyrighted material was in the training\ndata. The model was not able to reproduce content from our target news article,\nscientific article, or electronic health records. However, there were instances\nof fabrication. As a result of this event, a mitigation strategy is in\nproduction in GPT4DFCI v2.8.2, deployed on January 21, 2025. We hope this\nreport leads to similar events in which AI software tools are stress-tested to\nassess the perimeter of their legal and ethical usage.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22523v1", "categories": ["cs.CY", "cs.AI"], "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.22523v1", "date": "2025-06-26", "updated": "2025-06-26"}
{"id": "2506.22493", "title": "A Detailed Factor Analysis for the Political Compass Test: Navigating Ideologies of Large Language Models", "authors": ["Sadia Kamal", "Lalu Prasad Yadav Prakash", "S M Rafiuddin", "Mohammed Rakib", "Arunkumar Bagavathi", "Atriya Sen", "Sagnik Ray Choudhury"], "summary": "Political Compass Test (PCT) or similar questionnaires have been used to\nquantify LLM's political leanings. Building on a recent line of work that\nexamines the validity of PCT tests, we demonstrate that variation in standard\ngeneration parameters does not significantly impact the models' PCT scores.\nHowever, external factors such as prompt variations and fine-tuning\nindividually and in combination affect the same. Finally, we demonstrate that\nwhen models are fine-tuned on text datasets with higher political content than\nothers, the PCT scores are not differentially affected. This calls for a\nthorough investigation into the validity of PCT and similar tests, as well as\nthe mechanism by which political leanings are encoded in LLMs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22493v1", "categories": ["cs.CY", "cs.CL", "cs.LG"], "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.22493v1", "date": "2025-06-24", "updated": "2025-06-24"}
{"id": "2506.23132", "title": "Dare to Plagiarize? Plagiarized Painting Recognition and Retrieval", "authors": ["Sophie Zhou", "Shu Kong"], "summary": "Art plagiarism detection plays a crucial role in protecting artists'\ncopyrights and intellectual property, yet it remains a challenging problem in\nforensic analysis. In this paper, we address the task of recognizing\nplagiarized paintings and explaining the detected plagarisms by retrieving\nvisually similar authentic artworks. To support this study, we construct a\ndataset by collecting painting photos and synthesizing plagiarized versions\nusing generative AI, tailored to specific artists' styles. We first establish a\nbaseline approach using off-the-shelf features from the visual foundation model\nDINOv2 to retrieve the most similar images in the database and classify\nplagiarism based on a similarity threshold. Surprisingly, this non-learned\nmethod achieves a high recognition accuracy of 97.2\\% but suffers from low\nretrieval precision 29.0\\% average precision (AP). To improve retrieval\nquality, we finetune DINOv2 with a metric learning loss using positive and\nnegative sample pairs sampled in the database. The finetuned model greatly\nimproves retrieval performance by 12\\% AP over the baseline, though it\nunexpectedly results in a lower recognition accuracy (92.7\\%). We conclude with\ninsightful discussions and outline directions for future research.", "comment": "to appear at AVSS'25", "pdf_url": "http://arxiv.org/pdf/2506.23132v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23132v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23757", "title": "Training of Spiking Neural Networks with Expectation-Propagation", "authors": ["Dan Yao", "Steve McLaughlin", "Yoann Altmann"], "summary": "In this paper, we propose a unifying message-passing framework for training\nspiking neural networks (SNNs) using Expectation-Propagation. Our gradient-free\nmethod is capable of learning the marginal distributions of network parameters\nand simultaneously marginalizes nuisance parameters, such as the outputs of\nhidden layers. This framework allows for the first time, training of discrete\nand continuous weights, for deterministic and stochastic spiking networks,\nusing batches of training samples. Although its convergence is not ensured, the\nalgorithm converges in practice faster than gradient-based methods, without\nrequiring a large number of passes through the training data. The\nclassification and regression results presented pave the way for new efficient\ntraining methods for deep Bayesian networks.", "comment": "10 pages", "pdf_url": "http://arxiv.org/pdf/2506.23757v1", "categories": ["cs.LG", "stat.ME", "stat.ML"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.23757v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22526", "title": "Correlated Mutations for Integer Programming", "authors": ["Ofer M. Shir", "Michael Emmerich"], "summary": "Even with the recent theoretical advancements that dramatically reduced the\ncomplexity of Integer Programming (IP), heuristics remain the dominant\nproblem-solvers for this difficult category. This study seeks to establish the\ngroundwork for Integer Evolution Strategies (IESs), a class of randomized\nsearch heuristics inherently designed for continuous spaces. IESs already excel\nin treating IP in practice, but accomplish it via discretization and by\napplying sophisticated patches to their continuous operators, while\npersistently using the $\\ell_2$-norm as their operation pillar. We lay\nfoundations for discrete search, by adopting the $\\ell_1$-norm, accounting for\nthe suitable step-size, and questioning alternative measures to quantify\ncorrelations over the integer lattice. We focus on mutation distributions for\nunbounded integer decision variables. We briefly discuss a couple of candidate\ndiscrete probabilities induced by the uniform and binomial distributions, which\nwe show to possess less appealing theoretical properties, and then narrow down\nto the Truncated Normal (TN) and Double Geometric (DG) distributions. We\nexplore their theoretical properties, including entropy functions, and propose\na procedure to generate scalable correlated mutation distributions. Our\ninvestigations are accompanied by extensive numerical simulations, which\nconsistently support the claim that the DG distribution is better suited for\nunbounded integer search. We link our theoretical perspective to empirical\nevidence indicating that an IES with correlated DG mutations outperformed other\nstrategies over non-separable quadratic IP. We conclude that while the\nreplacement of the default TN distribution by the DG is theoretically justified\nand practically beneficial, the truly crucial change lies in adopting the\n$\\ell_1$-norm over the $\\ell_2$-norm.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22526v1", "categories": ["math.OC", "cs.AI", "cs.NE"], "cate": "math.OC", "url": "http://arxiv.org/abs/2506.22526v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22496", "title": "Mitigating Gambling-Like Risk-Taking Behaviors in Large Language Models: A Behavioral Economics Approach to AI Safety", "authors": ["Y. Du"], "summary": "Large Language Models (LLMs) exhibit systematic risk-taking behaviors\nanalogous to those observed in gambling psychology, including overconfidence\nbias, loss-chasing tendencies, and probability misjudgment. Drawing from\nbehavioral economics and prospect theory, we identify and formalize these\n\"gambling-like\" patterns where models sacrifice accuracy for high-reward\noutputs, exhibit escalating risk-taking after errors, and systematically\nmiscalibrate uncertainty. We propose the Risk-Aware Response Generation (RARG)\nframework, incorporating insights from gambling research to address these\nbehavioral biases through risk-calibrated training, loss-aversion mechanisms,\nand uncertainty-aware decision making. Our approach introduces novel evaluation\nparadigms based on established gambling psychology experiments, including AI\nadaptations of the Iowa Gambling Task and probability learning assessments.\nExperimental results demonstrate measurable reductions in gambling-like\nbehaviors: 18.7\\% decrease in overconfidence bias, 24.3\\% reduction in\nloss-chasing tendencies, and improved risk calibration across diverse\nscenarios. This work establishes the first systematic framework for\nunderstanding and mitigating gambling psychology patterns in AI systems.", "comment": "7 pages", "pdf_url": "http://arxiv.org/pdf/2506.22496v1", "categories": ["cs.CY", "cs.AI", "cs.CL"], "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.22496v1", "date": "2025-06-25", "updated": "2025-06-25"}
{"id": "2506.23135", "title": "RoboScape: Physics-informed Embodied World Model", "authors": ["Yu Shang", "Xin Zhang", "Yinzhou Tang", "Lei Jin", "Chen Gao", "Wei Wu", "Yong Li"], "summary": "World models have become indispensable tools for embodied intelligence,\nserving as powerful simulators capable of generating realistic robotic videos\nwhile addressing critical data scarcity challenges. However, current embodied\nworld models exhibit limited physical awareness, particularly in modeling 3D\ngeometry and motion dynamics, resulting in unrealistic video generation for\ncontact-rich robotic scenarios. In this paper, we present RoboScape, a unified\nphysics-informed world model that jointly learns RGB video generation and\nphysics knowledge within an integrated framework. We introduce two key\nphysics-informed joint training tasks: temporal depth prediction that enhances\n3D geometric consistency in video rendering, and keypoint dynamics learning\nthat implicitly encodes physical properties (e.g., object shape and material\ncharacteristics) while improving complex motion modeling. Extensive experiments\ndemonstrate that RoboScape generates videos with superior visual fidelity and\nphysical plausibility across diverse robotic scenarios. We further validate its\npractical utility through downstream applications including robotic policy\ntraining with generated data and policy evaluation. Our work provides new\ninsights for building efficient physics-informed world models to advance\nembodied intelligence research. The code is available at:\nhttps://github.com/tsinghua-fib-lab/RoboScape.", "comment": "17 pages", "pdf_url": "http://arxiv.org/pdf/2506.23135v1", "categories": ["cs.CV", "cs.RO"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23135v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23776", "title": "Model-driven Stochastic Trace Clustering", "authors": ["Jari Peeperkorn", "Johannes De Smedt", "Jochen De Weerdt"], "summary": "Process discovery algorithms automatically extract process models from event\nlogs, but high variability often results in complex and hard-to-understand\nmodels. To mitigate this issue, trace clustering techniques group process\nexecutions into clusters, each represented by a simpler and more understandable\nprocess model. Model-driven trace clustering improves on this by assigning\ntraces to clusters based on their conformity to cluster-specific process\nmodels. However, most existing clustering techniques rely on either no process\nmodel discovery, or non-stochastic models, neglecting the frequency or\nprobability of activities and transitions, thereby limiting their capability to\ncapture real-world execution dynamics. We propose a novel model-driven trace\nclustering method that optimizes stochastic process models within each cluster.\nOur approach uses entropic relevance, a stochastic conformance metric based on\ndirectly-follows probabilities, to guide trace assignment. This allows\nclustering decisions to consider both structural alignment with a cluster's\nprocess model and the likelihood that a trace originates from a given\nstochastic process model. The method is computationally efficient, scales\nlinearly with input size, and improves model interpretability by producing\nclusters with clearer control-flow patterns. Extensive experiments on public\nreal-life datasets show that our method outperforms existing alternatives in\nrepresenting process behavior and reveals how clustering performance rankings\ncan shift when stochasticity is considered.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23776v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.23776v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22554", "title": "Seamless Interaction: Dyadic Audiovisual Motion Modeling and Large-Scale Dataset", "authors": ["Vasu Agrawal", "Akinniyi Akinyemi", "Kathryn Alvero", "Morteza Behrooz", "Julia Buffalini", "Fabio Maria Carlucci", "Joy Chen", "Junming Chen", "Zhang Chen", "Shiyang Cheng", "Praveen Chowdary", "Joe Chuang", "Antony D'Avirro", "Jon Daly", "Ning Dong", "Mark Duppenthaler", "Cynthia Gao", "Jeff Girard", "Martin Gleize", "Sahir Gomez", "Hongyu Gong", "Srivathsan Govindarajan", "Brandon Han", "Sen He", "Denise Hernandez", "Yordan Hristov", "Rongjie Huang", "Hirofumi Inaguma", "Somya Jain", "Raj Janardhan", "Qingyao Jia", "Christopher Klaiber", "Dejan Kovachev", "Moneish Kumar", "Hang Li", "Yilei Li", "Pavel Litvin", "Wei Liu", "Guangyao Ma", "Jing Ma", "Martin Ma", "Xutai Ma", "Lucas Mantovani", "Sagar Miglani", "Sreyas Mohan", "Louis-Philippe Morency", "Evonne Ng", "Kam-Woh Ng", "Tu Anh Nguyen", "Amia Oberai", "Benjamin Peloquin", "Juan Pino", "Jovan Popovic", "Omid Poursaeed", "Fabian Prada", "Alice Rakotoarison", "Alexander Richard", "Christophe Ropers", "Safiyyah Saleem", "Vasu Sharma", "Alex Shcherbyna", "Jia Shen", "Jie Shen", "Anastasis Stathopoulos", "Anna Sun", "Paden Tomasello", "Tuan Tran", "Arina Turkatenko", "Bo Wan", "Chao Wang", "Jeff Wang", "Mary Williamson", "Carleigh Wood", "Tao Xiang", "Yilin Yang", "Julien Yao", "Chen Zhang", "Jiemin Zhang", "Xinyue Zhang", "Jason Zheng", "Pavlo Zhyzheria", "Jan Zikes", "Michael Zollhoefer"], "summary": "Human communication involves a complex interplay of verbal and nonverbal\nsignals, essential for conveying meaning and achieving interpersonal goals. To\ndevelop socially intelligent AI technologies, it is crucial to develop models\nthat can both comprehend and generate dyadic behavioral dynamics. To this end,\nwe introduce the Seamless Interaction Dataset, a large-scale collection of over\n4,000 hours of face-to-face interaction footage from over 4,000 participants in\ndiverse contexts. This dataset enables the development of AI technologies that\nunderstand dyadic embodied dynamics, unlocking breakthroughs in virtual agents,\ntelepresence experiences, and multimodal content analysis tools. We also\ndevelop a suite of models that utilize the dataset to generate dyadic motion\ngestures and facial expressions aligned with human speech. These models can\ntake as input both the speech and visual behavior of their interlocutors. We\npresent a variant with speech from an LLM model and integrations with 2D and 3D\nrendering methods, bringing us closer to interactive virtual agents.\nAdditionally, we describe controllable variants of our motion models that can\nadapt emotional responses and expressivity levels, as well as generating more\nsemantically-relevant gestures. Finally, we discuss methods for assessing the\nquality of these dyadic motion models, which are demonstrating the potential\nfor more intuitive and responsive human-AI interactions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22554v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22554v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22666", "title": "VERA: Variational Inference Framework for Jailbreaking Large Language Models", "authors": ["Anamika Lochab", "Lu Yan", "Patrick Pynadath", "Xiangyu Zhang", "Ruqi Zhang"], "summary": "The rise of API-only access to state-of-the-art LLMs highlights the need for\neffective black-box jailbreak methods to identify model vulnerabilities in\nreal-world settings. Without a principled objective for gradient-based\noptimization, most existing approaches rely on genetic algorithms, which are\nlimited by their initialization and dependence on manually curated prompt\npools. Furthermore, these methods require individual optimization for each\nprompt, failing to provide a comprehensive characterization of model\nvulnerabilities. To address this gap, we introduce VERA: Variational infErence\nfRamework for jAilbreaking. VERA casts black-box jailbreak prompting as a\nvariational inference problem, training a small attacker LLM to approximate the\ntarget LLM's posterior over adversarial prompts. Once trained, the attacker can\ngenerate diverse, fluent jailbreak prompts for a target query without\nre-optimization. Experimental results show that VERA achieves strong\nperformance across a range of target LLMs, highlighting the value of\nprobabilistic inference for adversarial prompt generation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22666v1", "categories": ["cs.CR", "cs.CL", "cs.LG", "stat.ML"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.22666v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.23138", "title": "VisualPrompter: Prompt Optimization with Visual Feedback for Text-to-Image Synthesis", "authors": ["Shiyu Wu", "Mingzhen Sun", "Weining Wang", "Yequan Wang", "Jing Liu"], "summary": "Since there exists a notable gap between user-provided and model-preferred\nprompts, generating high-quality and satisfactory images using diffusion models\noften requires prompt engineering to optimize user inputs. Current studies on\ntext-to-image prompt engineering can effectively enhance the style and\naesthetics of generated images. However, they often neglect the semantic\nalignment between generated images and user descriptions, resulting in visually\nappealing but content-wise unsatisfying outputs. In this work, we propose\nVisualPrompter, a novel training-free prompt engineering framework that refines\nuser inputs to model-preferred sentences. In particular, VisualPrompter\nutilizes an automatic self-reflection module to identify the missing concepts\nin generated images and a target-specific prompt optimization mechanism to\nrevise the prompts in a fine-grained manner. Extensive experiments demonstrate\nthe effectiveness of our VisualPrompter, which achieves new state-of-the-art\nperformance on multiple benchmarks for text-image alignment evaluation.\nAdditionally, our framework features a plug-and-play design, making it highly\nadaptable to various generative models.", "comment": "12 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2506.23138v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23138v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23782", "title": "Calibrating Graph Neural Networks with Wavelet-Aware Temperature Scaling", "authors": ["Xiaoyang Li", "Linwei Tao", "Haohui Lu", "Minjing Dong", "Junbin Gao", "Chang Xu"], "summary": "Graph Neural Networks (GNNs) have demonstrated strong predictive performance\non relational data; however, their confidence estimates often misalign with\nactual predictive correctness, posing significant limitations for deployment in\nsafety-critical settings. While existing graph-aware calibration methods seek\nto mitigate this limitation, they primarily depend on coarse one-hop\nstatistics, such as neighbor-predicted confidence, or latent node embeddings,\nthereby neglecting the fine-grained structural heterogeneity inherent in graph\ntopology. In this work, we propose Wavelet-Aware Temperature Scaling (WATS), a\npost-hoc calibration framework that assigns node-specific temperatures based on\ntunable heat-kernel graph wavelet features. Specifically, WATS harnesses the\nscalability and topology sensitivity of graph wavelets to refine confidence\nestimates, all without necessitating model retraining or access to neighboring\nlogits or predictions. Extensive evaluations across seven benchmark datasets\nwith varying graph structures and two GNN backbones demonstrate that WATS\nachieves the lowest Expected Calibration Error (ECE) among all compared\nmethods, outperforming both classical and graph-specific baselines by up to\n42.3\\% in ECE and reducing calibration variance by 17.24\\% on average compared\nwith graph-specific methods. Moreover, WATS remains computationally efficient,\nscaling well across graphs of diverse sizes and densities. Code will be\nreleased based on publication.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23782v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.23782v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22566", "title": "Exploration Behavior of Untrained Policies", "authors": ["Jacob Adamczyk"], "summary": "Exploration remains a fundamental challenge in reinforcement learning (RL),\nparticularly in environments with sparse or adversarial reward structures. In\nthis work, we study how the architecture of deep neural policies implicitly\nshapes exploration before training. We theoretically and empirically\ndemonstrate strategies for generating ballistic or diffusive trajectories from\nuntrained policies in a toy model. Using the theory of infinite-width networks\nand a continuous-time limit, we show that untrained policies return correlated\nactions and result in non-trivial state-visitation distributions. We discuss\nthe distributions of the corresponding trajectories for a standard\narchitecture, revealing insights into inductive biases for tackling\nexploration. Our results establish a theoretical and experimental framework for\nusing policy initialization as a design tool to understand exploration behavior\nin early training.", "comment": "High-dimensional Learning Dynamics Workshop at ICML-2025", "pdf_url": "http://arxiv.org/pdf/2506.22566v1", "categories": ["cs.LG", "cs.AI", "stat.ML"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22566v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22696", "title": "Residual Matrix Transformers: Scaling the Size of the Residual Stream", "authors": ["Brian Mak", "Jeffrey Flanigan"], "summary": "The residual stream acts as a memory bus where transformer layers both store\nand access features (Elhage et al., 2021). We consider changing the mechanism\nfor retrieving and storing information in the residual stream, and replace the\nresidual stream of the transformer with an outer product memory matrix\n(Kohonen, 1972, Anderson, 1972). We call this model the Residual Matrix\nTransformer (RMT). We find that the RMT enjoys a number of attractive\nproperties: 1) the size of the residual stream can be scaled independently of\ncompute and model size, improving performance, 2) the RMT can achieve the same\nloss as the transformer with 58% fewer FLOPS, 25% fewer parameters, and 41%\nfewer training tokens tokens, and 3) the RMT outperforms the transformer on\ndownstream evaluations. We theoretically analyze the transformer and the RMT,\nand show that the RMT allows for more efficient scaling of the residual stream,\nas well as improved variance propagation properties. Code for this project can\nbe found at https://github.com/bmac3/residual-matrix-transformer.", "comment": "Accepted to ICML 2025", "pdf_url": "http://arxiv.org/pdf/2506.22696v1", "categories": ["cs.LG", "cs.CL"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22696v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23150", "title": "AlignCVC: Aligning Cross-View Consistency for Single-Image-to-3D Generation", "authors": ["Xinyue Liang", "Zhiyuan Ma", "Lingchen Sun", "Yanjun Guo", "Lei Zhang"], "summary": "Single-image-to-3D models typically follow a sequential generation and\nreconstruction workflow. However, intermediate multi-view images synthesized by\npre-trained generation models often lack cross-view consistency (CVC),\nsignificantly degrading 3D reconstruction performance. While recent methods\nattempt to refine CVC by feeding reconstruction results back into the\nmulti-view generator, these approaches struggle with noisy and unstable\nreconstruction outputs that limit effective CVC improvement. We introduce\nAlignCVC, a novel framework that fundamentally re-frames single-image-to-3D\ngeneration through distribution alignment rather than relying on strict\nregression losses. Our key insight is to align both generated and reconstructed\nmulti-view distributions toward the ground-truth multi-view distribution,\nestablishing a principled foundation for improved CVC. Observing that generated\nimages exhibit weak CVC while reconstructed images display strong CVC due to\nexplicit rendering, we propose a soft-hard alignment strategy with distinct\nobjectives for generation and reconstruction models. This approach not only\nenhances generation quality but also dramatically accelerates inference to as\nfew as 4 steps. As a plug-and-play paradigm, our method, namely AlignCVC,\nseamlessly integrates various multi-view generation models with 3D\nreconstruction models. Extensive experiments demonstrate the effectiveness and\nefficiency of AlignCVC for single-image-to-3D generation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23150v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23150v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23799", "title": "KAIROS: Scalable Model-Agnostic Data Valuation", "authors": ["Jiongli Zhu", "Parjanya Prajakta Prashant", "Alex Cloninger", "Babak Salimi"], "summary": "Training data increasingly shapes not only model accuracy but also regulatory\ncompliance and market valuation of AI assets. Yet existing valuation methods\nremain inadequate: model-based techniques depend on a single fitted model and\ninherit its biases, while algorithm-based approaches such as Data Shapley\nrequire costly retrainings at web scale. Recent Wasserstein-based\nmodel-agnostic methods rely on approximations that misrank examples relative to\ntheir true leave-one-out (LOO) utility. We introduce KAIROS, a scalable,\nmodel-agnostic valuation framework that assigns each example a distributional\ninfluence score: its contribution to the Maximum Mean Discrepancy (MMD) between\nthe empirical training distribution and a clean reference set. Unlike\nWasserstein surrogates, our MMD-based influence admits a closed-form solution\nthat faithfully approximates the exact LOO ranking within $O(1/N^2)$ error,\nrequires no retraining, and naturally extends to conditional kernels for\nunified label- and feature-error detection. Moreover, KAIROS supports efficient\nonline updates: when a new batch of size m arrives, all scores can be updated\nin $O(mN)$ time, delivering up to 50x speedup without compromising ranking\nquality. Empirical evaluations on noise, mislabeling, and poisoning benchmarks\nshow that KAIROS consistently outperforms state-of-the-art model-, Shapley-,\nand Wasserstein-based baselines in both accuracy and runtime. We provide\nrigorous theoretical guarantees, including symmetry for reproducible rankings\nand density-separation for interpretable thresholds.", "comment": "19 pages, 9 figures", "pdf_url": "http://arxiv.org/pdf/2506.23799v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.23799v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22567", "title": "Unifying Biomedical Vision-Language Expertise: Towards a Generalist Foundation Model via Multi-CLIP Knowledge Distillation", "authors": ["Shansong Wang", "Zhecheng Jin", "Mingzhe Hu", "Mojtaba Safari", "Feng Zhao", "Chih-Wei Chang", "Richard LJ Qiu", "Justin Roper", "David S. Yu", "Xiaofeng Yang"], "summary": "CLIP models pretrained on natural images with billion-scale image-text pairs\nhave demonstrated impressive capabilities in zero-shot classification,\ncross-modal retrieval, and open-ended visual answering. However, transferring\nthis success to biomedicine is hindered by the scarcity of large-scale\nbiomedical image-text corpora, the heterogeneity of image modalities, and\nfragmented data standards across institutions. These limitations hinder the\ndevelopment of a unified and generalizable biomedical foundation model trained\nfrom scratch. To overcome this, we introduce MMKD-CLIP, a generalist biomedical\nfoundation model developed via Multiple Medical CLIP Knowledge Distillation.\nRather than relying on billion-scale raw data, MMKD-CLIP distills knowledge\nfrom nine state-of-the-art domain-specific or generalist biomedical CLIP\nmodels, each pretrained on millions of biomedical image-text pairs. Our\ntwo-stage training pipeline first performs CLIP-style pretraining on over 2.9\nmillion biomedical image-text pairs from 26 image modalities, followed by\nfeature-level distillation using over 19.2 million feature pairs extracted from\nteacher models. We evaluate MMKD-CLIP on 58 diverse biomedical datasets,\nencompassing over 10.8 million biomedical images across nine image modalities.\nThe evaluation spans six core task types: zero-shot classification, linear\nprobing, cross-modal retrieval, visual question answering, survival prediction,\nand cancer diagnosis. MMKD-CLIP consistently outperforms all teacher models\nwhile demonstrating remarkable robustness and generalization across image\ndomains and task settings. These results underscore that multi-teacher\nknowledge distillation is a scalable and effective paradigm for building\nhigh-performing biomedical foundation models under the practical constraints of\nreal-world data availability.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22567v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22567v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22716", "title": "BEST-Route: Adaptive LLM Routing with Test-Time Optimal Compute", "authors": ["Dujian Ding", "Ankur Mallick", "Shaokun Zhang", "Chi Wang", "Daniel Madrigal", "Mirian Del Carmen Hipolito Garcia", "Menglin Xia", "Laks V. S. Lakshmanan", "Qingyun Wu", "Victor R√ºhle"], "summary": "Large language models (LLMs) are powerful tools but are often expensive to\ndeploy at scale. LLM query routing mitigates this by dynamically assigning\nqueries to models of varying cost and quality to obtain a desired trade-off.\nPrior query routing approaches generate only one response from the selected\nmodel and a single response from a small (inexpensive) model was often not good\nenough to beat a response from a large (expensive) model due to which they end\nup overusing the large model and missing out on potential cost savings.\nHowever, it is well known that for small models, generating multiple responses\nand selecting the best can enhance quality while remaining cheaper than a\nsingle large-model response. We leverage this idea to propose BEST-Route, a\nnovel routing framework that chooses a model and the number of responses to\nsample from it based on query difficulty and the quality thresholds.\nExperiments on real-world datasets demonstrate that our method reduces costs by\nup to 60% with less than 1% performance drop.", "comment": "Accepted to ICML 2025 (main conference)", "pdf_url": "http://arxiv.org/pdf/2506.22716v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.DB"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22716v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23151", "title": "MEMFOF: High-Resolution Training for Memory-Efficient Multi-Frame Optical Flow Estimation", "authors": ["Vladislav Bargatin", "Egor Chistov", "Alexander Yakovenko", "Dmitriy Vatolin"], "summary": "Recent advances in optical flow estimation have prioritized accuracy at the\ncost of growing GPU memory consumption, particularly for high-resolution\n(FullHD) inputs. We introduce MEMFOF, a memory-efficient multi-frame optical\nflow method that identifies a favorable trade-off between multi-frame\nestimation and GPU memory usage. Notably, MEMFOF requires only 2.09 GB of GPU\nmemory at runtime for 1080p inputs, and 28.5 GB during training, which uniquely\npositions our method to be trained at native 1080p without the need for\ncropping or downsampling. We systematically revisit design choices from\nRAFT-like architectures, integrating reduced correlation volumes and\nhigh-resolution training protocols alongside multi-frame estimation, to achieve\nstate-of-the-art performance across multiple benchmarks while substantially\nreducing memory overhead. Our method outperforms more resource-intensive\nalternatives in both accuracy and runtime efficiency, validating its robustness\nfor flow estimation at high resolutions. At the time of submission, our method\nranks first on the Spring benchmark with a 1-pixel (1px) outlier rate of 3.289,\nleads Sintel (clean) with an endpoint error (EPE) of 0.963, and achieves the\nbest Fl-all error on KITTI-2015 at 2.94%. The code is available at\nhttps://github.com/msu-video-group/memfof.", "comment": "Accepted at ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2506.23151v1", "categories": ["cs.CV", "cs.AI", "cs.MM"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23151v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23800", "title": "Towards the Training of Deeper Predictive Coding Neural Networks", "authors": ["Chang Qi", "Matteo Forasassi", "Thomas Lukasiewicz", "Tommaso Salvatori"], "summary": "Predictive coding networks trained with equilibrium propagation are neural\nmodels that perform inference through an iterative energy minimization process.\nPrevious studies have demonstrated their effectiveness in shallow\narchitectures, but show significant performance degradation when depth exceeds\nfive to seven layers. In this work, we show that the reason behind this\ndegradation is due to exponentially imbalanced errors between layers during\nweight updates, and predictions from the previous layer not being effective in\nguiding updates in deeper layers. We address the first issue by introducing two\nnovel methods to optimize the latent variables that use precision-weighting to\nre-balance the distribution of energy among layers during the `relaxation\nphase', and the second issue by proposing a novel weight update mechanism that\nreduces error accumulation in deeper layers. Empirically, we test our methods\non a large number of image classification tasks, resulting in large\nimprovements in test accuracy across networks with more than seven layers, with\nperformances comparable to those of backprop on similar models. These findings\nsuggest that a better understanding of the relaxation phase is important to\ntrain models using equilibrium propagation at scale, and open new possibilities\nfor their application in complex tasks.", "comment": "18 Pages, 7 figures", "pdf_url": "http://arxiv.org/pdf/2506.23800v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.23800v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22578", "title": "The Hidden Link Between RLHF and Contrastive Learning", "authors": ["Xufei Lv", "Haoyuan Sun", "Xuefeng Bai", "Min Zhang", "Houde Liu", "Kehai Chen"], "summary": "Alignment of large language models (LLMs) with human values has recently\ngarnered significant attention, with prominent examples including the canonical\nyet costly Reinforcement Learning from Human Feedback (RLHF) and the simple\nDirect Preference Optimization (DPO). In this work, we demonstrate that both\nRLHF and DPO can be interpreted from the perspective of mutual information (MI)\nmaximization, uncovering a profound connection to contrastive learning. Within\nthis framework, both RLHF and DPO can be viewed as methods that perform\ncontrastive learning based on the positive and negative samples derived from\nthe base model, leveraging the Donsker-Varadhan (DV) lower bound on MI\n(equivalently, the MINE estimator). This paradigm further explains why RLHF may\nnot intrinsically incentivize reasoning capacities in LLMs beyond what is\nalready present in the base model. Building on this perspective, we replace the\nDV/MINE bound with the Jensen-Shannon MI estimator and propose Mutual\nInformation Optimization (MIO). Comprehensive theoretical analysis and\nextensive empirical evaluations demonstrate that MIO mitigates the late-stage\ndecline in chosen-likelihood observed in DPO, achieving competitive or superior\nperformance across various challenging reasoning and mathematical benchmarks.\nWe will release the model and code upon acceptance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22578v1", "categories": ["cs.LG", "cs.AI", "stat.ML"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22578v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22783", "title": "PhonemeFake: Redefining Deepfake Realism with Language-Driven Segmental Manipulation and Adaptive Bilevel Detection", "authors": ["Oguzhan Baser", "Ahmet Ege Tanriverdi", "Sriram Vishwanath", "Sandeep P. Chinchali"], "summary": "Deepfake (DF) attacks pose a growing threat as generative models become\nincreasingly advanced. However, our study reveals that existing DF datasets\nfail to deceive human perception, unlike real DF attacks that influence public\ndiscourse. It highlights the need for more realistic DF attack vectors. We\nintroduce PhonemeFake (PF), a DF attack that manipulates critical speech\nsegments using language reasoning, significantly reducing human perception by\nup to 42% and benchmark accuracies by up to 94%. We release an easy-to-use PF\ndataset on HuggingFace and open-source bilevel DF segment detection model that\nadaptively prioritizes compute on manipulated regions. Our extensive\nexperiments across three known DF datasets reveal that our detection model\nreduces EER by 91% while achieving up to 90% speed-up, with minimal compute\noverhead and precise localization beyond existing models as a scalable\nsolution.", "comment": "5 pages, 3 figures, Published at Proceedings of Interspeech 2025, for\n  the dataset see https://huggingface.co/datasets/phonemefake/PhonemeFakeV2,\n  for the code see https://github.com/UTAustin-SwarmLab/ PhonemeFake", "pdf_url": "http://arxiv.org/pdf/2506.22783v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22783v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23153", "title": "Dynamic View Synthesis from Small Camera Motion Videos", "authors": ["Huiqiang Sun", "Xingyi Li", "Juewen Peng", "Liao Shen", "Zhiguo Cao", "Ke Xian", "Guosheng Lin"], "summary": "Novel view synthesis for dynamic $3$D scenes poses a significant challenge.\nMany notable efforts use NeRF-based approaches to address this task and yield\nimpressive results. However, these methods rely heavily on sufficient motion\nparallax in the input images or videos. When the camera motion range becomes\nlimited or even stationary (i.e., small camera motion), existing methods\nencounter two primary challenges: incorrect representation of scene geometry\nand inaccurate estimation of camera parameters. These challenges make prior\nmethods struggle to produce satisfactory results or even become invalid. To\naddress the first challenge, we propose a novel Distribution-based Depth\nRegularization (DDR) that ensures the rendering weight distribution to align\nwith the true distribution. Specifically, unlike previous methods that use\ndepth loss to calculate the error of the expectation, we calculate the\nexpectation of the error by using Gumbel-softmax to differentiably sample\npoints from discrete rendering weight distribution. Additionally, we introduce\nconstraints that enforce the volume density of spatial points before the object\nboundary along the ray to be near zero, ensuring that our model learns the\ncorrect geometry of the scene. To demystify the DDR, we further propose a\nvisualization tool that enables observing the scene geometry representation at\nthe rendering weight level. For the second challenge, we incorporate camera\nparameter learning during training to enhance the robustness of our model to\ncamera parameters. We conduct extensive experiments to demonstrate the\neffectiveness of our approach in representing scenes with small camera motion\ninput, and our results compare favorably to state-of-the-art methods.", "comment": "Accepted by TVCG", "pdf_url": "http://arxiv.org/pdf/2506.23153v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23153v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23802", "title": "Adaptive Out-of-Control Point Pattern Detection in Sequential Random Finite Set Observations", "authors": ["Konstantinos Bourazas", "Savvas Papaioannou", "Panayiotis Kolios"], "summary": "In this work we introduce a novel adaptive anomaly detection framework\nspecifically designed for monitoring sequential random finite set (RFS)\nobservations. Our approach effectively distinguishes between In-Control data\n(normal) and Out-Of-Control data (anomalies) by detecting deviations from the\nexpected statistical behavior of the process. The primary contributions of this\nstudy include the development of an innovative RFS-based framework that not\nonly learns the normal behavior of the data-generating process online but also\ndynamically adapts to behavioral shifts to accurately identify abnormal point\npatterns. To achieve this, we introduce a new class of RFS-based posterior\ndistributions, named Power Discounting Posteriors (PD), which facilitate\nadaptation to systematic changes in data while enabling anomaly detection of\npoint pattern data through a novel predictive posterior density function. The\neffectiveness of the proposed approach is demonstrated by extensive qualitative\nand quantitative simulation experiments.", "comment": "23rd European Control Conference (ECC 2025), Thessaloniki, Greece,\n  24-27 June 2025", "pdf_url": "http://arxiv.org/pdf/2506.23802v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.23802v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22580", "title": "FedCLAM: Client Adaptive Momentum with Foreground Intensity Matching for Federated Medical Image Segmentation", "authors": ["Vasilis Siomos", "Jonathan Passerat-Palmbach", "Giacomo Tarroni"], "summary": "Federated learning is a decentralized training approach that keeps data under\nstakeholder control while achieving superior performance over isolated\ntraining. While inter-institutional feature discrepancies pose a challenge in\nall federated settings, medical imaging is particularly affected due to diverse\nimaging devices and population variances, which can diminish the global model's\neffectiveness. Existing aggregation methods generally fail to adapt across\nvaried circumstances. To address this, we propose FedCLAM, which integrates\n\\textit{client-adaptive momentum} terms derived from each client's loss\nreduction during local training, as well as a \\textit{personalized dampening\nfactor} to curb overfitting. We further introduce a novel \\textit{intensity\nalignment} loss that matches predicted and ground-truth foreground\ndistributions to handle heterogeneous image intensity profiles across\ninstitutions and devices. Extensive evaluations on two datasets show that\nFedCLAM surpasses eight cutting-edge methods in medical segmentation tasks,\nunderscoring its efficacy. The code is available at\nhttps://github.com/siomvas/FedCLAM.", "comment": "10 pages, 2 figures, Accepted at MICCAI 2025", "pdf_url": "http://arxiv.org/pdf/2506.22580v1", "categories": ["eess.IV", "cs.AI", "cs.CV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.22580v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22809", "title": "BayesLoRA: Task-Specific Uncertainty in Low-Rank Adapters", "authors": ["Cooper Doyle"], "summary": "We propose BayesLoRA, a task-specific uncertainty quantification framework\nthat integrates MC-Dropout into Low-Rank Adapters (LoRA). Unlike\ngeneral-purpose transformer uncertainty methods, BayesLoRA provides guardrails\ntailored to downstream workflows, enabling agents to introspect and modulate\nbehavior under uncertainty. We demonstrate mathematically and empirically that\nLoRA adapters exhibit amplified variance outside fine-tuning distributions,\nyielding reliable confidence estimates for agentic decision-making.", "comment": "13 pages, 3 figures, 1 table", "pdf_url": "http://arxiv.org/pdf/2506.22809v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22809v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23156", "title": "Self-Supervised Contrastive Learning for Multi-Label Images", "authors": ["Jiale Chen"], "summary": "Self-supervised learning (SSL) has demonstrated its effectiveness in learning\nrepresentations through comparison methods that align with human intuition.\nHowever, mainstream SSL methods heavily rely on high body datasets with single\nlabel, such as ImageNet, resulting in intolerable pre-training overhead.\nBesides, more general multi-label images are frequently overlooked in SSL,\ndespite their potential for richer semantic information and broader\napplicability in downstream scenarios. Therefore, we tailor the mainstream SSL\napproach to guarantee excellent representation learning capabilities using\nfewer multi-label images. Firstly, we propose a block-wise augmentation module\naimed at extracting additional potential positive view pairs from multi-label\nimages. Subsequently, an image-aware contrastive loss is devised to establish\nconnections between these views, thereby facilitating the extraction of\nsemantically consistent representations. Comprehensive linear fine-tuning and\ntransfer learning validate the competitiveness of our approach despite\nchallenging sample quality and quantity.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23156v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23156v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23803", "title": "SGD with Adaptive Preconditioning: Unified Analysis and Momentum Acceleration", "authors": ["Dmitry Kovalev"], "summary": "In this paper, we revisit stochastic gradient descent (SGD) with AdaGrad-type\npreconditioning. Our contributions are twofold. First, we develop a unified\nconvergence analysis of SGD with adaptive preconditioning under anisotropic or\nmatrix smoothness and noise assumptions. This allows us to recover\nstate-of-the-art convergence results for several popular adaptive gradient\nmethods, including AdaGrad-Norm, AdaGrad, and ASGO/One-sided Shampoo. In\naddition, we establish the fundamental connection between two recently proposed\nalgorithms, Scion and DASGO, and provide the first theoretical guarantees for\nthe latter. Second, we show that the convergence of methods like AdaGrad and\nDASGO can be provably accelerated beyond the best-known rates using Nesterov\nmomentum. Consequently, we obtain the first theoretical justification that\nAdaGrad-type algorithms can simultaneously benefit from both diagonal\npreconditioning and momentum, which may provide an ultimate explanation for the\npractical efficiency of Adam.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23803v1", "categories": ["cs.LG", "math.OC"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.23803v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22593", "title": "Pixels-to-Graph: Real-time Integration of Building Information Models and Scene Graphs for Semantic-Geometric Human-Robot Understanding", "authors": ["Antonello Longo", "Chanyoung Chung", "Matteo Palieri", "Sung-Kyun Kim", "Ali Agha", "Cataldo Guaragnella", "Shehryar Khattak"], "summary": "Autonomous robots are increasingly playing key roles as support platforms for\nhuman operators in high-risk, dangerous applications. To accomplish challenging\ntasks, an efficient human-robot cooperation and understanding is required.\nWhile typically robotic planning leverages 3D geometric information, human\noperators are accustomed to a high-level compact representation of the\nenvironment, like top-down 2D maps representing the Building Information Model\n(BIM). 3D scene graphs have emerged as a powerful tool to bridge the gap\nbetween human readable 2D BIM and the robot 3D maps. In this work, we introduce\nPixels-to-Graph (Pix2G), a novel lightweight method to generate structured\nscene graphs from image pixels and LiDAR maps in real-time for the autonomous\nexploration of unknown environments on resource-constrained robot platforms. To\nsatisfy onboard compute constraints, the framework is designed to perform all\noperation on CPU only. The method output are a de-noised 2D top-down\nenvironment map and a structure-segmented 3D pointcloud which are seamlessly\nconnected using a multi-layer graph abstracting information from object-level\nup to the building-level. The proposed method is quantitatively and\nqualitatively evaluated during real-world experiments performed using the NASA\nJPL NeBula-Spot legged robot to autonomously explore and map cluttered garage\nand urban office like environments in real-time.", "comment": "Paper accepted to 2025 IEEE International Conference on Automation\n  Science and Engineering (CASE)", "pdf_url": "http://arxiv.org/pdf/2506.22593v1", "categories": ["cs.RO", "cs.AI", "cs.CV"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.22593v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22864", "title": "Mask-aware Text-to-Image Retrieval: Referring Expression Segmentation Meets Cross-modal Retrieval", "authors": ["Li-Cheng Shen", "Jih-Kang Hsieh", "Wei-Hua Li", "Chu-Song Chen"], "summary": "Text-to-image retrieval (TIR) aims to find relevant images based on a textual\nquery, but existing approaches are primarily based on whole-image captions and\nlack interpretability. Meanwhile, referring expression segmentation (RES)\nenables precise object localization based on natural language descriptions but\nis computationally expensive when applied across large image collections. To\nbridge this gap, we introduce Mask-aware TIR (MaTIR), a new task that unifies\nTIR and RES, requiring both efficient image search and accurate object\nsegmentation. To address this task, we propose a two-stage framework,\ncomprising a first stage for segmentation-aware image retrieval and a second\nstage for reranking and object grounding with a multimodal large language model\n(MLLM). We leverage SAM 2 to generate object masks and Alpha-CLIP to extract\nregion-level embeddings offline at first, enabling effective and scalable\nonline retrieval. Secondly, MLLM is used to refine retrieval rankings and\ngenerate bounding boxes, which are matched to segmentation masks. We evaluate\nour approach on COCO and D$^3$ datasets, demonstrating significant improvements\nin both retrieval accuracy and segmentation quality over previous methods.", "comment": "ICMR 2025", "pdf_url": "http://arxiv.org/pdf/2506.22864v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22864v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23157", "title": "STD-GS: Exploring Frame-Event Interaction for SpatioTemporal-Disentangled Gaussian Splatting to Reconstruct High-Dynamic Scene", "authors": ["Hanyu Zhou", "Haonan Wang", "Haoyue Liu", "Yuxing Duan", "Luxin Yan", "Gim Hee Lee"], "summary": "High-dynamic scene reconstruction aims to represent static background with\nrigid spatial features and dynamic objects with deformed continuous\nspatiotemporal features. Typically, existing methods adopt unified\nrepresentation model (e.g., Gaussian) to directly match the spatiotemporal\nfeatures of dynamic scene from frame camera. However, this unified paradigm\nfails in the potential discontinuous temporal features of objects due to frame\nimaging and the heterogeneous spatial features between background and objects.\nTo address this issue, we disentangle the spatiotemporal features into various\nlatent representations to alleviate the spatiotemporal mismatching between\nbackground and objects. In this work, we introduce event camera to compensate\nfor frame camera, and propose a spatiotemporal-disentangled Gaussian splatting\nframework for high-dynamic scene reconstruction. As for dynamic scene, we\nfigure out that background and objects have appearance discrepancy in\nframe-based spatial features and motion discrepancy in event-based temporal\nfeatures, which motivates us to distinguish the spatiotemporal features between\nbackground and objects via clustering. As for dynamic object, we discover that\nGaussian representations and event data share the consistent spatiotemporal\ncharacteristic, which could serve as a prior to guide the spatiotemporal\ndisentanglement of object Gaussians. Within Gaussian splatting framework, the\ncumulative scene-object disentanglement can improve the spatiotemporal\ndiscrimination between background and objects to render the time-continuous\ndynamic scene. Extensive experiments have been performed to verify the\nsuperiority of the proposed method.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23157v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23157v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23824", "title": "Supercm: Revisiting Clustering for Semi-Supervised Learning", "authors": ["Durgesh Singh", "Ahcene Boubekki", "Robert Jenssen", "Michael C. Kampffmeyer"], "summary": "The development of semi-supervised learning (SSL) has in recent years largely\nfocused on the development of new consistency regularization or entropy\nminimization approaches, often resulting in models with complex training\nstrategies to obtain the desired results. In this work, we instead propose a\nnovel approach that explicitly incorporates the underlying clustering\nassumption in SSL through extending a recently proposed differentiable\nclustering module. Leveraging annotated data to guide the cluster centroids\nresults in a simple end-to-end trainable deep SSL approach. We demonstrate that\nthe proposed model improves the performance over the supervised-only baseline\nand show that our framework can be used in conjunction with other SSL methods\nto further boost their performance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23824v1", "categories": ["cs.LG", "cs.CV"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.23824v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22623", "title": "Temperature Matters: Enhancing Watermark Robustness Against Paraphrasing Attacks", "authors": ["Badr Youbi Idrissi", "Monica Millunzi", "Amelia Sorrenti", "Lorenzo Baraldi", "Daryna Dementieva"], "summary": "In the present-day scenario, Large Language Models (LLMs) are establishing\ntheir presence as powerful instruments permeating various sectors of society.\nWhile their utility offers valuable support to individuals, there are multiple\nconcerns over potential misuse. Consequently, some academic endeavors have\nsought to introduce watermarking techniques, characterized by the inclusion of\nmarkers within machine-generated text, to facilitate algorithmic\nidentification. This research project is focused on the development of a novel\nmethodology for the detection of synthetic text, with the overarching goal of\nensuring the ethical application of LLMs in AI-driven text generation. The\ninvestigation commences with replicating findings from a previous baseline\nstudy, thereby underscoring its susceptibility to variations in the underlying\ngeneration model. Subsequently, we propose an innovative watermarking approach\nand subject it to rigorous evaluation, employing paraphrased generated text to\nasses its robustness. Experimental results highlight the robustness of our\nproposal compared to the~\\cite{aarson} watermarking method.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22623v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.22623v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22900", "title": "MOTOR: Multimodal Optimal Transport via Grounded Retrieval in Medical Visual Question Answering", "authors": ["Mai A. Shaaban", "Tausifa Jan Saleem", "Vijay Ram Papineni", "Mohammad Yaqub"], "summary": "Medical visual question answering (MedVQA) plays a vital role in clinical\ndecision-making by providing contextually rich answers to image-based queries.\nAlthough vision-language models (VLMs) are widely used for this task, they\noften generate factually incorrect answers. Retrieval-augmented generation\naddresses this challenge by providing information from external sources, but\nrisks retrieving irrelevant context, which can degrade the reasoning\ncapabilities of VLMs. Re-ranking retrievals, as introduced in existing\napproaches, enhances retrieval relevance by focusing on query-text alignment.\nHowever, these approaches neglect the visual or multimodal context, which is\nparticularly crucial for medical diagnosis. We propose MOTOR, a novel\nmultimodal retrieval and re-ranking approach that leverages grounded captions\nand optimal transport. It captures the underlying relationships between the\nquery and the retrieved context based on textual and visual information.\nConsequently, our approach identifies more clinically relevant contexts to\naugment the VLM input. Empirical analysis and human expert evaluation\ndemonstrate that MOTOR achieves higher accuracy on MedVQA datasets,\noutperforming state-of-the-art methods by an average of 6.45%. Code is\navailable at https://github.com/BioMedIA-MBZUAI/MOTOR.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22900v1", "categories": ["cs.CV", "cs.CL"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22900v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23189", "title": "Trident: Detecting Face Forgeries with Adversarial Triplet Learning", "authors": ["Mustafa Hakan Kara", "Aysegul Dundar", "Uƒüur G√ºd√ºkbay"], "summary": "As face forgeries generated by deep neural networks become increasingly\nsophisticated, detecting face manipulations in digital media has posed a\nsignificant challenge, underscoring the importance of maintaining digital media\nintegrity and combating visual disinformation. Current detection models,\npredominantly based on supervised training with domain-specific data, often\nfalter against forgeries generated by unencountered techniques. In response to\nthis challenge, we introduce \\textit{Trident}, a face forgery detection\nframework that employs triplet learning with a Siamese network architecture for\nenhanced adaptability across diverse forgery methods. \\textit{Trident} is\ntrained on curated triplets to isolate nuanced differences of forgeries,\ncapturing fine-grained features that distinguish pristine samples from\nmanipulated ones while controlling for other variables. To further enhance\ngeneralizability, we incorporate domain-adversarial training with a forgery\ndiscriminator. This adversarial component guides our embedding model towards\nforgery-agnostic representations, improving its robustness to unseen\nmanipulations. In addition, we prevent gradient flow from the classifier head\nto the embedding model, avoiding overfitting induced by artifacts peculiar to\ncertain forgeries. Comprehensive evaluations across multiple benchmarks and\nablation studies demonstrate the effectiveness of our framework. We will\nrelease our code in a GitHub repository.", "comment": "11 pages, 3 figures, and 7 tables", "pdf_url": "http://arxiv.org/pdf/2506.23189v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23189v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23843", "title": "EFPI: Elastic Formation and Position Identification in Football (Soccer) using Template Matching and Linear Assignment", "authors": ["Joris Bekkers"], "summary": "Understanding team formations and player positioning is crucial for tactical\nanalysis in football (soccer). This paper presents a flexible method for\nformation recognition and player position assignment in football using\npredefined static formation templates and cost minimization from spatiotemporal\ntracking data, called EFPI. Our approach employs linear sum assignment to\noptimally match players to positions within a set of template formations by\nminimizing the total distance between actual player locations and template\npositions, subsequently selecting the formation with the lowest assignment\ncost. To improve accuracy, we scale actual player positions to match the\ndimensions of these formation templates in both width and length. While the\nmethod functions effectively on individual frames, it extends naturally to\nlarger game segments such as complete periods, possession sequences or specific\nintervals (e.g. 10 second intervals, 5 minute intervals etc.). Additionally, we\nincorporate an optional stability parameter that prevents unnecessary formation\nchanges when assignment costs differ only marginally between time segments.\nEFPI is available as open-source code through the unravelsports Python package.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23843v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.23843v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22638", "title": "Layer Importance for Mathematical Reasoning is Forged in Pre-Training and Invariant after Post-Training", "authors": ["Aadim Nepal", "Safal Shrestha", "Anubhav Shrestha", "Minwu Kim", "Keith Ross"], "summary": "Large language models can exhibit improved mathematical reasoning\ncapabilities following post-training with instruction tuning, reinforcement\nlearning, or knowledge distillation. However, it remains unclear whether these\nimprovements are driven by major changes in transformer layers or from minor\nadjustments that leave the relative layer importance structures of the base\nmodel largely unchanged. We investigate this question through systematic\nlayer-wise ablation experiments, examining base, instruction-tuned,\nknowledge-distilled, and reinforcement learning variants on mathematical\nreasoning benchmarks. Our findings show that mathematical reasoning gives rise\nto a specific layer importance structure, and this structure persists across\nall post-training paradigms. Removal of such layers causes accuracy drops of up\nto 80%. In contrast, non-mathematical tasks like factual recall exhibit no\ncritical layers. This distinction suggests that mathematical reasoning requires\nspecialized layers that emerge during pre-training, while other non-reasoning\ntasks do not. From an information-theoretic perspective, we also observe that\nthese critical layers are the same layers where major representational\ntransformation occurs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22638v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22638v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22992", "title": "MARBLE: A Hard Benchmark for Multimodal Spatial Reasoning and Planning", "authors": ["Yulun Jiang", "Yekun Chai", "Maria Brbiƒá", "Michael Moor"], "summary": "The ability to process information from multiple modalities and to reason\nthrough it step-by-step remains a critical challenge in advancing artificial\nintelligence. However, existing reasoning benchmarks focus on text-only\nreasoning, or employ multimodal questions that can be answered by directly\nretrieving information from a non-text modality. Thus, complex reasoning\nremains poorly understood in multimodal domains. Here, we present MARBLE, a\nchallenging multimodal reasoning benchmark that is designed to scrutinize\nmultimodal language models (MLLMs) in their ability to carefully reason\nstep-by-step through complex multimodal problems and environments. MARBLE is\ncomposed of two highly challenging tasks, M-Portal and M-Cube, that require the\ncrafting and understanding of multistep plans under spatial, visual, and\nphysical constraints. We find that current MLLMs perform poorly on MARBLE --\nall the 12 advanced models obtain near-random performance on M-Portal and 0%\naccuracy on M-Cube. Only in simplified subtasks some models outperform the\nrandom baseline, indicating that complex reasoning is still a challenge for\nexisting MLLMs. Moreover, we show that perception remains a bottleneck, where\nMLLMs occasionally fail to extract information from the visual inputs. By\nshedding a light on the limitations of MLLMs, we hope that MARBLE will spur the\ndevelopment of the next generation of models with the ability to reason and\nplan across many, multimodal reasoning steps.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22992v1", "categories": ["cs.AI", "cs.CL", "cs.CV"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.22992v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23196", "title": "DEL: Dense Event Localization for Multi-modal Audio-Visual Understanding", "authors": ["Mona Ahmadian", "Amir Shirian", "Frank Guerin", "Andrew Gilbert"], "summary": "Real-world videos often contain overlapping events and complex temporal\ndependencies, making multimodal interaction modeling particularly challenging.\nWe introduce DEL, a framework for dense semantic action localization, aiming to\naccurately detect and classify multiple actions at fine-grained temporal\nresolutions in long untrimmed videos. DEL consists of two key modules: the\nalignment of audio and visual features that leverage masked self-attention to\nenhance intra-mode consistency and a multimodal interaction refinement module\nthat models cross-modal dependencies across multiple scales, enabling\nhigh-level semantics and fine-grained details. Our method achieves\nstate-of-the-art performance on multiple real-world Temporal Action\nLocalization (TAL) datasets, UnAV-100, THUMOS14, ActivityNet 1.3, and\nEPIC-Kitchens-100, surpassing previous approaches with notable average mAP\ngains of +3.3%, +2.6%, +1.2%, +1.7% (verb), and +1.4% (noun), respectively.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23196v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23196v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23845", "title": "Use Sparse Autoencoders to Discover Unknown Concepts, Not to Act on Known Concepts", "authors": ["Kenny Peng", "Rajiv Movva", "Jon Kleinberg", "Emma Pierson", "Nikhil Garg"], "summary": "While sparse autoencoders (SAEs) have generated significant excitement, a\nseries of negative results have added to skepticism about their usefulness.\nHere, we establish a conceptual distinction that reconciles competing\nnarratives surrounding SAEs. We argue that while SAEs may be less effective for\nacting on known concepts, SAEs are powerful tools for discovering unknown\nconcepts. This distinction cleanly separates existing negative and positive\nresults, and suggests several classes of SAE applications. Specifically, we\noutline use cases for SAEs in (i) ML interpretability, explainability,\nfairness, auditing, and safety, and (ii) social and health sciences.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23845v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CY"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.23845v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22656", "title": "Knowledge-Guided Multi-Agent Framework for Automated Requirements Development: A Vision", "authors": ["Jiangping Huang", "Dongming Jin", "Weisong Sun", "Yang Liu", "Zhi Jin"], "summary": "This paper envisions a knowledge-guided multi-agent framework named KGMAF for\nautomated requirements development. KGMAF aims to address gaps in current\nautomation systems for SE, which prioritize code development and overlook the\ncomplexities of requirements tasks. KGMAF is composed of six specialized agents\nand an artifact pool to improve efficiency and accuracy. Specifically, KGMAF\noutlines the functionality, actions, and knowledge of each agent and provides\nthe conceptual design of the artifact pool. Our case study highlights the\npotential of KGMAF in real-world scenarios. Finally, we outline several\nresearch opportunities for implementing and enhancing automated requirements\ndevelopment using multi-agent systems. We believe that KGMAF will play a\npivotal role in shaping the future of automated requirements development in the\nera of LLMs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22656v1", "categories": ["cs.SE", "cs.AI", "68-04", "D.2.3; I.2.7"], "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.22656v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.23049", "title": "AURA: Agent for Understanding, Reasoning, and Automated Tool Use in Voice-Driven Tasks", "authors": ["Leander Melroy Maben", "Gayathri Ganesh Lakshmy", "Srijith Radhakrishnan", "Siddhant Arora", "Shinji Watanabe"], "summary": "Despite advances in language and speech technologies, no open-source system\nenables full speech-to-speech, multi-turn dialogue with integrated tool use and\nagentic reasoning. We introduce AURA (Agent for Understanding, Reasoning, and\nAutomated Tool Use), the first open-source, speech-native assistant capable of\ncompleting complex, goal-driven tasks through dynamic tool invocation and\nmulti-turn conversation. AURA combines open-weight ASR, TTS, and LLMs in a\ncascaded pipeline and supports tools such as calendar booking, contact lookup,\nweb search, and email. Its modular design allows easy integration of new tools\nusing natural language prompts and action classes. On VoiceBench, AURA scores\n92.75% on OpenBookQA-outperforming all open-weight systems and nearing\nGPT-4o-and 4.39 on AlpacaEval, competitive with other open-weight systems.\nHuman evaluation shows 90% task success on complex, multi-turn speech tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23049v1", "categories": ["cs.AI", "cs.CL", "cs.SD", "eess.AS", "68T42, 68T50,", "I.2.7; I.2.11; H.5.5"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.23049v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23202", "title": "Transformer-Based Person Search with High-Frequency Augmentation and Multi-Wave Mixing", "authors": ["Qilin Shu", "Qixian Zhang", "Qi Zhang", "Hongyun Zhang", "Duoqian Miao", "Cairong Zhao"], "summary": "The person search task aims to locate a target person within a set of scene\nimages. In recent years, transformer-based models in this field have made some\nprogress. However, they still face three primary challenges: 1) the\nself-attention mechanism tends to suppress high-frequency components in the\nfeatures, which severely impacts model performance; 2) the computational cost\nof transformers is relatively high. To address these issues, we propose a novel\nHigh-frequency Augmentation and Multi-Wave mixing (HAMW) method for person\nsearch. HAMW is designed to enhance the discriminative feature extraction\ncapabilities of transformers while reducing computational overhead and\nimproving efficiency. Specifically, we develop a three-stage framework that\nprogressively optimizes both detection and re-identification performance. Our\nmodel enhances the perception of high-frequency features by learning from\naugmented inputs containing additional high-frequency components. Furthermore,\nwe replace the self-attention layers in the transformer with a strategy based\non multi-level Haar wavelet fusion to capture multi-scale features. This not\nonly lowers the computational complexity but also alleviates the suppression of\nhigh-frequency features and enhances the ability to exploit multi-scale\ninformation. Extensive experiments demonstrate that HAMW achieves\nstate-of-the-art performance on both the CUHK-SYSU and PRW datasets.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23202v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23202v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23872", "title": "When Plants Respond: Electrophysiology and Machine Learning for Green Monitoring Systems", "authors": ["Eduard Buss", "Till Aust", "Heiko Hamann"], "summary": "Living plants, while contributing to ecological balance and climate\nregulation, also function as natural sensors capable of transmitting\ninformation about their internal physiological states and surrounding\nconditions. This rich source of data provides potential for applications in\nenvironmental monitoring and precision agriculture. With integration into\nbiohybrid systems, we establish novel channels of physiological signal flow\nbetween living plants and artificial devices. We equipped *Hedera helix* with a\nplant-wearable device called PhytoNode to continuously record the plant's\nelectrophysiological activity. We deployed plants in an uncontrolled outdoor\nenvironment to map electrophysiological patterns to environmental conditions.\nOver five months, we collected data that we analyzed using state-of-the-art and\nautomated machine learning (AutoML). Our classification models achieve high\nperformance, reaching macro F1 scores of up to 95 percent in binary tasks.\nAutoML approaches outperformed manual tuning, and selecting subsets of\nstatistical features further improved accuracy. Our biohybrid living system\nmonitors the electrophysiology of plants in harsh, real-world conditions. This\nwork advances scalable, self-sustaining, and plant-integrated living biohybrid\nsystems for sustainable environmental monitoring.", "comment": "Submitted and Accepted at the 14th international conference on\n  biomimetic and biohybrid systems (Living Machines)", "pdf_url": "http://arxiv.org/pdf/2506.23872v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.23872v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22668", "title": "DistShap: Scalable GNN Explanations with Distributed Shapley Values", "authors": ["Selahattin Akkas", "Aditya Devarakonda", "Ariful Azad"], "summary": "With the growing adoption of graph neural networks (GNNs), explaining their\npredictions has become increasingly important. However, attributing predictions\nto specific edges or features remains computationally expensive. For example,\nclassifying a node with 100 neighbors using a 3-layer GNN may involve\nidentifying important edges from millions of candidates contributing to the\nprediction. To address this challenge, we propose DistShap, a parallel\nalgorithm that distributes Shapley value-based explanations across multiple\nGPUs. DistShap operates by sampling subgraphs in a distributed setting,\nexecuting GNN inference in parallel across GPUs, and solving a distributed\nleast squares problem to compute edge importance scores. DistShap outperforms\nmost existing GNN explanation methods in accuracy and is the first to scale to\nGNN models with millions of features by using up to 128 GPUs on the NERSC\nPerlmutter supercomputer.", "comment": "12 pages", "pdf_url": "http://arxiv.org/pdf/2506.22668v1", "categories": ["cs.LG", "cs.AI", "cs.DC", "stat.ML"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22668v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.23115", "title": "MoCa: Modality-aware Continual Pre-training Makes Better Bidirectional Multimodal Embeddings", "authors": ["Haonan Chen", "Hong Liu", "Yuping Luo", "Liang Wang", "Nan Yang", "Furu Wei", "Zhicheng Dou"], "summary": "Multimodal embedding models, built upon causal Vision Language Models (VLMs),\nhave shown promise in various tasks. However, current approaches face three key\nlimitations: the use of causal attention in VLM backbones is suboptimal for\nembedding tasks; scalability issues due to reliance on high-quality labeled\npaired data for contrastive learning; and limited diversity in training\nobjectives and data. To address these issues, we propose MoCa, a two-stage\nframework for transforming pre-trained VLMs into effective bidirectional\nmultimodal embedding models. The first stage, Modality-aware Continual\nPre-training, introduces a joint reconstruction objective that simultaneously\ndenoises interleaved text and image inputs, enhancing bidirectional\ncontext-aware reasoning. The second stage, Heterogeneous Contrastive\nFine-tuning, leverages diverse, semantically rich multimodal data beyond simple\nimage-caption pairs to enhance generalization and alignment. Our method\naddresses the stated limitations by introducing bidirectional attention through\ncontinual pre-training, scaling effectively with massive unlabeled datasets via\njoint reconstruction objectives, and utilizing diverse multimodal data for\nenhanced representation robustness. Experiments demonstrate that MoCa\nconsistently improves performance across MMEB and ViDoRe-v2 benchmarks,\nachieving new state-of-the-art results, and exhibits strong scalability with\nboth model size and training data on MMEB.", "comment": "Homepage: https://haon-chen.github.io/MoCa/", "pdf_url": "http://arxiv.org/pdf/2506.23115v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23115v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23205", "title": "BridgeShape: Latent Diffusion Schr√∂dinger Bridge for 3D Shape Completion", "authors": ["Dequan Kong", "Zhe Zhu", "Honghua Chen", "Mingqiang Wei"], "summary": "Existing diffusion-based 3D shape completion methods typically use a\nconditional paradigm, injecting incomplete shape information into the denoising\nnetwork via deep feature interactions (e.g., concatenation, cross-attention) to\nguide sampling toward complete shapes, often represented by voxel-based\ndistance functions. However, these approaches fail to explicitly model the\noptimal global transport path, leading to suboptimal completions. Moreover,\nperforming diffusion directly in voxel space imposes resolution constraints,\nlimiting the generation of fine-grained geometric details. To address these\nchallenges, we propose BridgeShape, a novel framework for 3D shape completion\nvia latent diffusion Schr\\\"odinger bridge. The key innovations lie in two\naspects: (i) BridgeShape formulates shape completion as an optimal transport\nproblem, explicitly modeling the transition between incomplete and complete\nshapes to ensure a globally coherent transformation. (ii) We introduce a\nDepth-Enhanced Vector Quantized Variational Autoencoder (VQ-VAE) to encode 3D\nshapes into a compact latent space, leveraging self-projected multi-view depth\ninformation enriched with strong DINOv2 features to enhance geometric\nstructural perception. By operating in a compact yet structurally informative\nlatent space, BridgeShape effectively mitigates resolution constraints and\nenables more efficient and high-fidelity 3D shape completion. BridgeShape\nachieves state-of-the-art performance on large-scale 3D shape completion\nbenchmarks, demonstrating superior fidelity at higher resolutions and for\nunseen object classes.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23205v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23205v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23875", "title": "Chain of Thought in Order: Discovering Learning-Friendly Orders for Arithmetic", "authors": ["Yuta Sato", "Kazuhiko Kawamoto", "Hiroshi Kera"], "summary": "The chain of thought is fundamental in Transformers, which is to perform\nstep-by-step reasoning. Besides what intermediate steps work, the order of\nthese steps critically affects the difficulty of the reasoning. This study\naddresses a novel task of unraveling chain of thought - reordering decoder\ninput tokens to a learning-friendly sequence for Transformers to learn\narithmetic tasks. The proposed pipeline first trains a Transformer on a mixture\nof target sequences arranged in different orders and then identifies benign\norders as those with fast loss drops in the early stage. As the search space\ngrows factorially with sequence length, we propose a two-stage hierarchical\napproach for inter- and intra-block reordering. Experiments on four\norder-sensitive arithmetic tasks show that our method identifies a\nlearning-friendly order out of a few billion candidates. Notably, on the\nmultiplication task, it recovered the reverse-digit order reported in prior\nstudies.", "comment": "14 pages, 10 figures", "pdf_url": "http://arxiv.org/pdf/2506.23875v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.23875v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22698", "title": "Text Production and Comprehension by Human and Artificial Intelligence: Interdisciplinary Workshop Report", "authors": ["Emily Dux Speltz"], "summary": "This report synthesizes the outcomes of a recent interdisciplinary workshop\nthat brought together leading experts in cognitive psychology, language\nlearning, and artificial intelligence (AI)-based natural language processing\n(NLP). The workshop, funded by the National Science Foundation, aimed to\naddress a critical knowledge gap in our understanding of the relationship\nbetween AI language models and human cognitive processes in text comprehension\nand composition. Through collaborative dialogue across cognitive, linguistic,\nand technological perspectives, workshop participants examined the underlying\nprocesses involved when humans produce and comprehend text, and how AI can both\ninform our understanding of these processes and augment human capabilities. The\nworkshop revealed emerging patterns in the relationship between large language\nmodels (LLMs) and human cognition, with highlights on both the capabilities of\nLLMs and their limitations in fully replicating human-like language\nunderstanding and generation. Key findings include the potential of LLMs to\noffer insights into human language processing, the increasing alignment between\nLLM behavior and human language processing when models are fine-tuned with\nhuman feedback, and the opportunities and challenges presented by human-AI\ncollaboration in language tasks. By synthesizing these findings, this report\naims to guide future research, development, and implementation of LLMs in\ncognitive psychology, linguistics, and education. It emphasizes the importance\nof ethical considerations and responsible use of AI technologies while striving\nto enhance human capabilities in text comprehension and production through\neffective human-AI collaboration.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22698v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.22698v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23219", "title": "UrbanLLaVA: A Multi-modal Large Language Model for Urban Intelligence with Spatial Reasoning and Understanding", "authors": ["Jie Feng", "Shengyuan Wang", "Tianhui Liu", "Yanxin Xi", "Yong Li"], "summary": "Urban research involves a wide range of scenarios and tasks that require the\nunderstanding of multi-modal data. Current methods often focus on specific data\ntypes and lack a unified framework in urban field for processing them\ncomprehensively. The recent success of multi-modal large language models\n(MLLMs) presents a promising opportunity to overcome this limitation. In this\npaper, we introduce $\\textit{UrbanLLaVA}$, a multi-modal large language model\ndesigned to process these four types of data simultaneously and achieve strong\nperformance across diverse urban tasks compared with general MLLMs. In\n$\\textit{UrbanLLaVA}$, we first curate a diverse urban instruction dataset\nencompassing both single-modal and cross-modal urban data, spanning from\nlocation view to global view of urban environment. Additionally, we propose a\nmulti-stage training framework that decouples spatial reasoning enhancement\nfrom domain knowledge learning, thereby improving the compatibility and\ndownstream performance of $\\textit{UrbanLLaVA}$ across diverse urban tasks.\nFinally, we also extend existing benchmark for urban research to assess the\nperformance of MLLMs across a wide range of urban tasks. Experimental results\nfrom three cities demonstrate that $\\textit{UrbanLLaVA}$ outperforms\nopen-source and proprietary MLLMs in both single-modal tasks and complex\ncross-modal tasks and shows robust generalization abilities across cities.\nSource codes and data are openly accessible to the research community via\nhttps://github.com/tsinghua-fib-lab/UrbanLLaVA.", "comment": "Accepted by ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2506.23219v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23219v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23207", "title": "TVG-SLAM: Robust Gaussian Splatting SLAM with Tri-view Geometric Constraints", "authors": ["Zhen Tan", "Xieyuanli Chen", "Lei Feng", "Yangbing Ge", "Shuaifeng Zhi", "Jiaxiong Liu", "Dewen Hu"], "summary": "Recent advances in 3D Gaussian Splatting (3DGS) have enabled RGB-only SLAM\nsystems to achieve high-fidelity scene representation. However, the heavy\nreliance of existing systems on photometric rendering loss for camera tracking\nundermines their robustness, especially in unbounded outdoor environments with\nsevere viewpoint and illumination changes. To address these challenges, we\npropose TVG-SLAM, a robust RGB-only 3DGS SLAM system that leverages a novel\ntri-view geometry paradigm to ensure consistent tracking and high-quality\nmapping. We introduce a dense tri-view matching module that aggregates reliable\npairwise correspondences into consistent tri-view matches, forming robust\ngeometric constraints across frames. For tracking, we propose Hybrid Geometric\nConstraints, which leverage tri-view matches to construct complementary\ngeometric cues alongside photometric loss, ensuring accurate and stable pose\nestimation even under drastic viewpoint shifts and lighting variations. For\nmapping, we propose a new probabilistic initialization strategy that encodes\ngeometric uncertainty from tri-view correspondences into newly initialized\nGaussians. Additionally, we design a Dynamic Attenuation of Rendering Trust\nmechanism to mitigate tracking drift caused by mapping latency. Experiments on\nmultiple public outdoor datasets show that our TVG-SLAM outperforms prior\nRGB-only 3DGS-based SLAM systems. Notably, in the most challenging dataset, our\nmethod improves tracking robustness, reducing the average Absolute Trajectory\nError (ATE) by 69.0\\% while achieving state-of-the-art rendering quality. The\nimplementation of our method will be released as open-source.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23207v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23207v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23923", "title": "Reinforcement Learning for Synchronised Flow Control in a Dual-Gate Resin Infusion System", "authors": ["Miguel Camacho-S√°nchez", "Fernando Garc√≠a-Torres", "Jesper John Lisegaard", "Roc√≠o del Amor", "Sankhya Mohanty", "Valery Naranjo"], "summary": "Resin infusion (RI) and resin transfer moulding (RTM) are critical processes\nfor the manufacturing of high-performance fibre-reinforced polymer composites,\nparticularly for large-scale applications such as wind turbine blades.\nControlling the resin flow dynamics in these processes is critical to ensure\nthe uniform impregnation of the fibre reinforcements, thereby preventing\nresidual porosities and dry spots that impact the consequent structural\nintegrity of the final component. This paper presents a reinforcement learning\n(RL) based strategy, established using process simulations, for synchronising\nthe different resin flow fronts in an infusion scenario involving two resin\ninlets and a single outlet. Using Proximal Policy Optimisation (PPO), our\napproach addresses the challenge of managing the fluid dynamics in a partially\nobservable environment. The results demonstrate the effectiveness of the RL\napproach in achieving an accurate flow convergence, highlighting its potential\ntowards improving process control and product quality in composites\nmanufacturing.", "comment": "11 pages, 4 figures, 45th Ris{\\o} International Symposium on\n  Materials Science", "pdf_url": "http://arxiv.org/pdf/2506.23923v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.23923v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22703", "title": "P4OMP: Retrieval-Augmented Prompting for OpenMP Parallelism in Serial Code", "authors": ["Wali Mohammad Abdullah", "Azmain Kabir"], "summary": "We present P4OMP, a retrieval-augmented framework for transforming serial\nC/C++ code into OpenMP-annotated parallel code using large language models\n(LLMs). To our knowledge, this is the first system to apply retrieval-based\nprompting for OpenMP pragma correctness without model fine-tuning or compiler\ninstrumentation. P4OMP leverages Retrieval-Augmented Generation (RAG) with\nstructured instructional knowledge from OpenMP tutorials to improve the\nreliability of prompt-driven code generation. By grounding generation in the\nretrieved context, P4OMP improves syntactic correctness compared to baseline\nprompting with GPT-3.5-Turbo. We evaluate P4OMP against a baseline,\nGPT-3.5-Turbo without retrieval, on a comprehensive benchmark of 108 real-world\nC++ programs drawn from Stack Overflow, PolyBench, and NAS benchmark suites.\nP4OMP achieves 100% compilation success on all parallelizable cases, while the\nbaseline fails to compile in 20 out of 108 cases. Six cases that rely on\nnon-random-access iterators or thread-unsafe constructs are excluded due to\nfundamental OpenMP limitations. A detailed analysis demonstrates how P4OMP\nconsistently avoids scoping errors, syntactic misuse, and invalid directive\ncombinations that commonly affect baseline-generated code. We further\ndemonstrate strong runtime scaling across seven compute-intensive benchmarks on\nan HPC cluster. P4OMP offers a robust, modular pipeline that significantly\nimproves the reliability and applicability of LLM-generated OpenMP code.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22703v1", "categories": ["cs.SE", "cs.AI"], "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.22703v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23225", "title": "Masked Gated Linear Unit", "authors": ["Yukito Tajima", "Nakamasa Inoue", "Yusuke Sekikawa", "Ikuro Sato", "Rio Yokota"], "summary": "Gated Linear Units (GLUs) have become essential components in the\nfeed-forward networks of state-of-the-art Large Language Models (LLMs).\nHowever, they require twice as many memory reads compared to feed-forward\nlayers without gating, due to the use of separate weight matrices for the gate\nand value streams. To address this bottleneck, we introduce Masked Gated Linear\nUnits (MGLUs), a novel family of GLUs with an efficient kernel implementation.\nThe core contribution of MGLUs include: (1) the Mixture of Element-wise Gating\n(MoEG) architecture that learns multiple binary masks, each determining gate or\nvalue assignments at the element level on a single shared weight matrix\nresulting in reduced memory transfer, and (2) FlashMGLU, a hardware-friendly\nkernel that yields up to a 19.7 $\\times$ inference-time speed-up over a naive\nPyTorch MGLU and is 47% more memory-efficient and 34% faster than standard GLUs\ndespite added architectural complexity on an RTX5090 GPU. In LLM experiments,\nthe Swish-activated variant SwiMGLU preserves its memory advantages while\nmatching - or even surpassing - the downstream accuracy of the SwiGLU baseline.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23225v1", "categories": ["cs.LG", "cs.CL"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.23225v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23209", "title": "A Hierarchical Slice Attention Network for Appendicitis Classification in 3D CT Scans", "authors": ["Chia-Wen Huang", "Haw Hwai", "Chien-Chang Lee", "Pei-Yuan Wu"], "summary": "Timely and accurate diagnosis of appendicitis is critical in clinical\nsettings to prevent serious complications. While CT imaging remains the\nstandard diagnostic tool, the growing number of cases can overwhelm\nradiologists, potentially causing delays. In this paper, we propose a deep\nlearning model that leverages 3D CT scans for appendicitis classification,\nincorporating Slice Attention mechanisms guided by external 2D datasets to\nenhance small lesion detection. Additionally, we introduce a hierarchical\nclassification framework using pre-trained 2D models to differentiate between\nsimple and complicated appendicitis. Our approach improves AUC by 3% for\nappendicitis and 5.9% for complicated appendicitis, offering a more efficient\nand reliable diagnostic solution compared to previous work.", "comment": "8 pages, 1 figure, 3 tables. Published in IEEE ISBI 2025. This\n  version corrects citation numbering errors", "pdf_url": "http://arxiv.org/pdf/2506.23209v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23209v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23958", "title": "Bridging the Gap with Retrieval-Augmented Generation: Making Prosthetic Device User Manuals Available in Marginalised Languages", "authors": ["Ikechukwu Ogbonna", "Lesley Davidson", "Soumya Banerjee", "Abhishek Dasgupta", "Laurence Kenney", "Vikranth Harthikote Nagaraja"], "summary": "Millions of people in African countries face barriers to accessing healthcare\ndue to language and literacy gaps. This research tackles this challenge by\ntransforming complex medical documents -- in this case, prosthetic device user\nmanuals -- into accessible formats for underserved populations. This case study\nin cross-cultural translation is particularly pertinent/relevant for\ncommunities that receive donated prosthetic devices but may not receive the\naccompanying user documentation. Or, if available online, may only be available\nin formats (e.g., language and readability) that are inaccessible to local\npopulations (e.g., English-language, high resource settings/cultural context).\nThe approach is demonstrated using the widely spoken Pidgin dialect, but our\nopen-source framework has been designed to enable rapid and easy extension to\nother languages/dialects. This work presents an AI-powered framework designed\nto process and translate complex medical documents, e.g., user manuals for\nprosthetic devices, into marginalised languages. The system enables users --\nsuch as healthcare workers or patients -- to upload English-language medical\nequipment manuals, pose questions in their native language, and receive\naccurate, localised answers in real time. Technically, the system integrates a\nRetrieval-Augmented Generation (RAG) pipeline for processing and semantic\nunderstanding of the uploaded manuals. It then employs advanced Natural\nLanguage Processing (NLP) models for generative question-answering and\nmultilingual translation. Beyond simple translation, it ensures accessibility\nto device instructions, treatment protocols, and safety information, empowering\npatients and clinicians to make informed healthcare decisions.", "comment": "5 pages, 0 figures, 0 tables", "pdf_url": "http://arxiv.org/pdf/2506.23958v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.23958v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22704", "title": "Beyond Code: The Multidimensional Impacts of Large Language Models in Software Development", "authors": ["Sardar Fatooreh Bonabi", "Sarah Bana", "Tingting Nian", "Vijay Gurbaxani"], "summary": "Large language models (LLMs) are poised to significantly impact software\ndevelopment, especially in the Open-Source Software (OSS) sector. To understand\nthis impact, we first outline the mechanisms through which LLMs may influence\nOSS through code development, collaborative knowledge transfer, and skill\ndevelopment. We then empirically examine how LLMs affect OSS developers' work\nin these three key areas. Leveraging a natural experiment from a temporary\nChatGPT ban in Italy, we employ a Difference-in-Differences framework with\ntwo-way fixed effects to analyze data from all OSS developers on GitHub in\nthree similar countries, Italy, France, and Portugal, totaling 88,022 users. We\nfind that access to ChatGPT increases developer productivity by 6.4%, knowledge\nsharing by 9.6%, and skill acquisition by 8.4%. These benefits vary\nsignificantly by user experience level: novice developers primarily experience\nproductivity gains, whereas more experienced developers benefit more from\nimproved knowledge sharing and accelerated skill acquisition. In addition, we\nfind that LLM-assisted learning is highly context-dependent, with the greatest\nbenefits observed in technically complex, fragmented, or rapidly evolving\ncontexts. We show that the productivity effects of LLMs extend beyond direct\ncode generation to include enhanced collaborative learning and knowledge\nexchange among developers; dynamics that are essential for gaining a holistic\nunderstanding of LLMs' impact in OSS. Our findings offer critical managerial\nimplications: strategically deploying LLMs can accelerate novice developers'\nonboarding and productivity, empower intermediate developers to foster\nknowledge sharing and collaboration, and support rapid skill acquisition,\ntogether enhancing long-term organizational productivity and agility.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22704v1", "categories": ["econ.GN", "cs.AI", "q-fin.EC"], "cate": "econ.GN", "url": "http://arxiv.org/abs/2506.22704v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23276", "title": "Corrupted by Reasoning: Reasoning Language Models Become Free-Riders in Public Goods Games", "authors": ["David Guzman Piedrahita", "Yongjin Yang", "Mrinmaya Sachan", "Giorgia Ramponi", "Bernhard Sch√∂lkopf", "Zhijing Jin"], "summary": "As large language models (LLMs) are increasingly deployed as autonomous\nagents, understanding their cooperation and social mechanisms is becoming\nincreasingly important. In particular, how LLMs balance self-interest and\ncollective well-being is a critical challenge for ensuring alignment,\nrobustness, and safe deployment. In this paper, we examine the challenge of\ncostly sanctioning in multi-agent LLM systems, where an agent must decide\nwhether to invest its own resources to incentivize cooperation or penalize\ndefection. To study this, we adapt a public goods game with institutional\nchoice from behavioral economics, allowing us to observe how different LLMs\nnavigate social dilemmas over repeated interactions. Our analysis reveals four\ndistinct behavioral patterns among models: some consistently establish and\nsustain high levels of cooperation, others fluctuate between engagement and\ndisengagement, some gradually decline in cooperative behavior over time, and\nothers rigidly follow fixed strategies regardless of outcomes. Surprisingly, we\nfind that reasoning LLMs, such as the o1 series, struggle significantly with\ncooperation, whereas some traditional LLMs consistently achieve high levels of\ncooperation. These findings suggest that the current approach to improving\nLLMs, which focuses on enhancing their reasoning capabilities, does not\nnecessarily lead to cooperation, providing valuable insights for deploying LLM\nagents in environments that require sustained collaboration. Our code is\navailable at https://github.com/davidguzmanp/SanctSim", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23276v1", "categories": ["cs.AI", "cs.CL"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.23276v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23219", "title": "UrbanLLaVA: A Multi-modal Large Language Model for Urban Intelligence with Spatial Reasoning and Understanding", "authors": ["Jie Feng", "Shengyuan Wang", "Tianhui Liu", "Yanxin Xi", "Yong Li"], "summary": "Urban research involves a wide range of scenarios and tasks that require the\nunderstanding of multi-modal data. Current methods often focus on specific data\ntypes and lack a unified framework in urban field for processing them\ncomprehensively. The recent success of multi-modal large language models\n(MLLMs) presents a promising opportunity to overcome this limitation. In this\npaper, we introduce $\\textit{UrbanLLaVA}$, a multi-modal large language model\ndesigned to process these four types of data simultaneously and achieve strong\nperformance across diverse urban tasks compared with general MLLMs. In\n$\\textit{UrbanLLaVA}$, we first curate a diverse urban instruction dataset\nencompassing both single-modal and cross-modal urban data, spanning from\nlocation view to global view of urban environment. Additionally, we propose a\nmulti-stage training framework that decouples spatial reasoning enhancement\nfrom domain knowledge learning, thereby improving the compatibility and\ndownstream performance of $\\textit{UrbanLLaVA}$ across diverse urban tasks.\nFinally, we also extend existing benchmark for urban research to assess the\nperformance of MLLMs across a wide range of urban tasks. Experimental results\nfrom three cities demonstrate that $\\textit{UrbanLLaVA}$ outperforms\nopen-source and proprietary MLLMs in both single-modal tasks and complex\ncross-modal tasks and shows robust generalization abilities across cities.\nSource codes and data are openly accessible to the research community via\nhttps://github.com/tsinghua-fib-lab/UrbanLLaVA.", "comment": "Accepted by ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2506.23219v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23219v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23960", "title": "ADReFT: Adaptive Decision Repair for Safe Autonomous Driving via Reinforcement Fine-Tuning", "authors": ["Mingfei Cheng", "Xiaofei Xie", "Renzhi Wang", "Yuan Zhou", "Ming Hu"], "summary": "Autonomous Driving Systems (ADSs) continue to face safety-critical risks due\nto the inherent limitations in their design and performance capabilities.\nOnline repair plays a crucial role in mitigating such limitations, ensuring the\nruntime safety and reliability of ADSs. Existing online repair solutions\nenforce ADS compliance by transforming unacceptable trajectories into\nacceptable ones based on predefined specifications, such as rule-based\nconstraints or training datasets. However, these approaches often lack\ngeneralizability, adaptability and tend to be overly conservative, resulting in\nineffective repairs that not only fail to mitigate safety risks sufficiently\nbut also degrade the overall driving experience. To address this issue, we\npropose Adaptive Decision Repair (ADReFT), a novel and effective repair method\nthat identifies safety-critical states through offline learning from failed\ntests and generates appropriate mitigation actions to improve ADS safety.\nSpecifically, ADReFT incorporates a transformer-based model with two joint\nheads, State Monitor and Decision Adapter, designed to capture complex driving\nenvironment interactions to evaluate state safety severity and generate\nadaptive repair actions. Given the absence of oracles for state safety\nidentification, we first pretrain ADReFT using supervised learning with coarse\nannotations, i.e., labeling states preceding violations as positive samples and\nothers as negative samples. It establishes ADReFT's foundational capability to\nmitigate safety-critical violations, though it may result in somewhat\nconservative mitigation strategies. Therefore, we subsequently finetune ADReFT\nusing reinforcement learning to improve its initial capability and generate\nmore precise and contextually appropriate repair decisions. Our evaluation\nresults illustrate that ADReFT achieves better repair performance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23960v1", "categories": ["cs.LG", "cs.AI", "cs.SE"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.23960v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22706", "title": "General Autonomous Cybersecurity Defense: Learning Robust Policies for Dynamic Topologies and Diverse Attackers", "authors": ["Arun Ramamurthy", "Neil Dhir"], "summary": "In the face of evolving cyber threats such as malware, ransomware and\nphishing, autonomous cybersecurity defense (ACD) systems have become essential\nfor real-time threat detection and response with optional human intervention.\nHowever, existing ACD systems rely on limiting assumptions, particularly the\nstationarity of the underlying network dynamics. In real-world scenarios,\nnetwork topologies can change due to actions taken by attackers or defenders,\nsystem failures, or time evolution of networks, leading to failures in the\nadaptive capabilities of current defense agents. Moreover, many agents are\ntrained on static environments, resulting in overfitting to specific\ntopologies, which hampers their ability to generalize to out-of-distribution\nnetwork topologies. This work addresses these challenges by exploring methods\nfor developing agents to learn generalizable policies across dynamic network\nenvironments -- general ACD (GACD).", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22706v1", "categories": ["cs.CR", "cs.AI", "cs.CV", "stat.ML"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.22706v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23322", "title": "GaussMaster: An LLM-based Database Copilot System", "authors": ["Wei Zhou", "Ji Sun", "Xuanhe Zhou", "Guoliang Li", "Luyang Liu", "Hao Wu", "Tianyuan Wang"], "summary": "In the financial industry, data is the lifeblood of operations, and DBAs\nshoulder significant responsibilities for SQL tuning, database deployment,\ndiagnosis, and service repair. In recent years, both database vendors and\ncustomers have increasingly turned to autonomous database platforms in an\neffort to alleviate the heavy workload of DBAs. However, existing autonomous\ndatabase platforms are limited in their capabilities, primarily addressing\nsingle-point issues such as NL2SQL, anomaly detection, and SQL tuning. Manual\nintervention remains a necessity for comprehensive database maintenance.\nGaussMaster aims to revolutionize this landscape by introducing an LLM-based\ndatabase copilot system. This innovative solution is designed not only to\nassist developers in writing efficient SQL queries but also to provide\ncomprehensive care for database services. When database instances exhibit\nabnormal behavior, GaussMaster is capable of orchestrating the entire\nmaintenance process automatically. It achieves this by analyzing hundreds of\nmetrics and logs, employing a Tree-of-thought approach to identify root causes,\nand invoking appropriate tools to resolve issues. We have successfully\nimplemented GaussMaster in real-world scenarios, such as the banking industry,\nwhere it has achieved zero human intervention for over 34 database maintenance\nscenarios. In this paper, we present significant improvements in these tasks\nwith code at https://gitcode.com/opengauss/openGauss-GaussMaster.", "comment": "We welcome contributions from the community. For reference, please\n  see the code at: https://gitcode.com/opengauss/openGauss-GaussMaster", "pdf_url": "http://arxiv.org/pdf/2506.23322v1", "categories": ["cs.DB", "cs.AI", "cs.CL", "cs.IR"], "cate": "cs.DB", "url": "http://arxiv.org/abs/2506.23322v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23227", "title": "High-quality Pseudo-labeling for Point Cloud Segmentation with Scene-level Annotation", "authors": ["Lunhao Duan", "Shanshan Zhao", "Xingxing Weng", "Jing Zhang", "Gui-Song Xia"], "summary": "This paper investigates indoor point cloud semantic segmentation under\nscene-level annotation, which is less explored compared to methods relying on\nsparse point-level labels. In the absence of precise point-level labels,\ncurrent methods first generate point-level pseudo-labels, which are then used\nto train segmentation models. However, generating accurate pseudo-labels for\neach point solely based on scene-level annotations poses a considerable\nchallenge, substantially affecting segmentation performance. Consequently, to\nenhance accuracy, this paper proposes a high-quality pseudo-label generation\nframework by exploring contemporary multi-modal information and region-point\nsemantic consistency. Specifically, with a cross-modal feature guidance module,\nour method utilizes 2D-3D correspondences to align point cloud features with\ncorresponding 2D image pixels, thereby assisting point cloud feature learning.\nTo further alleviate the challenge presented by the scene-level annotation, we\nintroduce a region-point semantic consistency module. It produces regional\nsemantics through a region-voting strategy derived from point-level semantics,\nwhich are subsequently employed to guide the point-level semantic predictions.\nLeveraging the aforementioned modules, our method can rectify inaccurate\npoint-level semantic predictions during training and obtain high-quality\npseudo-labels. Significant improvements over previous works on ScanNet v2 and\nS3DIS datasets under scene-level annotation can demonstrate the effectiveness.\nAdditionally, comprehensive ablation studies validate the contributions of our\napproach's individual components. The code is available at\nhttps://github.com/LHDuan/WSegPC .", "comment": "Accepted by TPAMI. Code: https://github.com/LHDuan/WSegPC", "pdf_url": "http://arxiv.org/pdf/2506.23227v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23227v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23971", "title": "UMA: A Family of Universal Models for Atoms", "authors": ["Brandon M. Wood", "Misko Dzamba", "Xiang Fu", "Meng Gao", "Muhammed Shuaibi", "Luis Barroso-Luque", "Kareem Abdelmaqsoud", "Vahe Gharakhanyan", "John R. Kitchin", "Daniel S. Levine", "Kyle Michel", "Anuroop Sriram", "Taco Cohen", "Abhishek Das", "Ammar Rizvi", "Sushree Jagriti Sahoo", "Zachary W. Ulissi", "C. Lawrence Zitnick"], "summary": "The ability to quickly and accurately compute properties from atomic\nsimulations is critical for advancing a large number of applications in\nchemistry and materials science including drug discovery, energy storage, and\nsemiconductor manufacturing. To address this need, Meta FAIR presents a family\nof Universal Models for Atoms (UMA), designed to push the frontier of speed,\naccuracy, and generalization. UMA models are trained on half a billion unique\n3D atomic structures (the largest training runs to date) by compiling data\nacross multiple chemical domains, e.g. molecules, materials, and catalysts. We\ndevelop empirical scaling laws to help understand how to increase model\ncapacity alongside dataset size to achieve the best accuracy. The UMA small and\nmedium models utilize a novel architectural design we refer to as mixture of\nlinear experts that enables increasing model capacity without sacrificing\nspeed. For example, UMA-medium has 1.4B parameters but only ~50M active\nparameters per atomic structure. We evaluate UMA models on a diverse set of\napplications across multiple domains and find that, remarkably, a single model\nwithout any fine-tuning can perform similarly or better than specialized\nmodels. We are releasing the UMA code, weights, and associated data to\naccelerate computational workflows and enable the community to continue to\nbuild increasingly capable AI models.", "comment": "29 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2506.23971v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.23971v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22716", "title": "BEST-Route: Adaptive LLM Routing with Test-Time Optimal Compute", "authors": ["Dujian Ding", "Ankur Mallick", "Shaokun Zhang", "Chi Wang", "Daniel Madrigal", "Mirian Del Carmen Hipolito Garcia", "Menglin Xia", "Laks V. S. Lakshmanan", "Qingyun Wu", "Victor R√ºhle"], "summary": "Large language models (LLMs) are powerful tools but are often expensive to\ndeploy at scale. LLM query routing mitigates this by dynamically assigning\nqueries to models of varying cost and quality to obtain a desired trade-off.\nPrior query routing approaches generate only one response from the selected\nmodel and a single response from a small (inexpensive) model was often not good\nenough to beat a response from a large (expensive) model due to which they end\nup overusing the large model and missing out on potential cost savings.\nHowever, it is well known that for small models, generating multiple responses\nand selecting the best can enhance quality while remaining cheaper than a\nsingle large-model response. We leverage this idea to propose BEST-Route, a\nnovel routing framework that chooses a model and the number of responses to\nsample from it based on query difficulty and the quality thresholds.\nExperiments on real-world datasets demonstrate that our method reduces costs by\nup to 60% with less than 1% performance drop.", "comment": "Accepted to ICML 2025 (main conference)", "pdf_url": "http://arxiv.org/pdf/2506.22716v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.DB"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22716v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23366", "title": "Density, asymmetry and citation dynamics in scientific literature", "authors": ["Nathaniel Imel", "Zachary Hafen"], "summary": "Scientific behavior is often characterized by a tension between building upon\nestablished knowledge and introducing novel ideas. Here, we investigate whether\nthis tension is reflected in the relationship between the similarity of a\nscientific paper to previous research and its eventual citation rate. To\noperationalize similarity to previous research, we introduce two complementary\nmetrics to characterize the local geometry of a publication's semantic\nneighborhood: (1) \\emph{density} ($\\rho$), defined as the ratio between a fixed\nnumber of previously-published papers and the minimum distance enclosing those\npapers in a semantic embedding space, and (2) asymmetry ($\\alpha$), defined as\nthe average directional difference between a paper and its nearest neighbors.\nWe tested the predictive relationship between these two metrics and its\nsubsequent citation rate using a Bayesian hierarchical regression approach,\nsurveying $\\sim 53,000$ publications across nine academic disciplines and five\ndifferent document embeddings. While the individual effects of $\\rho$ on\ncitation count are small and variable, incorporating density-based predictors\nconsistently improves out-of-sample prediction when added to baseline models.\nThese results suggest that the density of a paper's surrounding scientific\nliterature may carry modest but informative signals about its eventual impact.\nMeanwhile, we find no evidence that publication asymmetry improves model\npredictions of citation rates. Our work provides a scalable framework for\nlinking document embeddings to scientometric outcomes and highlights new\nquestions regarding the role that semantic similarity plays in shaping the\ndynamics of scientific reward.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23366v1", "categories": ["cs.DL", "cs.CL", "cs.SI"], "cate": "cs.DL", "url": "http://arxiv.org/abs/2506.23366v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23236", "title": "VolumetricSMPL: A Neural Volumetric Body Model for Efficient Interactions, Contacts, and Collisions", "authors": ["Marko Mihajlovic", "Siwei Zhang", "Gen Li", "Kaifeng Zhao", "Lea M√ºller", "Siyu Tang"], "summary": "Parametric human body models play a crucial role in computer graphics and\nvision, enabling applications ranging from human motion analysis to\nunderstanding human-environment interactions. Traditionally, these models use\nsurface meshes, which pose challenges in efficiently handling interactions with\nother geometric entities, such as objects and scenes, typically represented as\nmeshes or point clouds. To address this limitation, recent research has\nexplored volumetric neural implicit body models. However, existing works are\neither insufficiently robust for complex human articulations or impose high\ncomputational and memory costs, limiting their widespread use. To this end, we\nintroduce VolumetricSMPL, a neural volumetric body model that leverages Neural\nBlend Weights (NBW) to generate compact, yet efficient MLP decoders. Unlike\nprior approaches that rely on large MLPs, NBW dynamically blends a small set of\nlearned weight matrices using predicted shape- and pose-dependent coefficients,\nsignificantly improving computational efficiency while preserving\nexpressiveness. VolumetricSMPL outperforms prior volumetric occupancy model\nCOAP with 10x faster inference, 6x lower GPU memory usage, enhanced accuracy,\nand a Signed Distance Function (SDF) for efficient and differentiable contact\nmodeling. We demonstrate VolumetricSMPL's strengths across four challenging\ntasks: (1) reconstructing human-object interactions from in-the-wild images,\n(2) recovering human meshes in 3D scenes from egocentric views, (3)\nscene-constrained motion synthesis, and (4) resolving self-intersections. Our\nresults highlight its broad applicability and significant performance and\nefficiency gains.", "comment": "[ICCV 2025] https://markomih.github.io/VolumetricSMPL", "pdf_url": "http://arxiv.org/pdf/2506.23236v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23236v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23977", "title": "A Scalable Approach for Safe and Robust Learning via Lipschitz-Constrained Networks", "authors": ["Zain ul Abdeen", "Vassilis Kekatos", "Ming Jin"], "summary": "Certified robustness is a critical property for deploying neural networks\n(NN) in safety-critical applications. A principle approach to achieving such\nguarantees is to constrain the global Lipschitz constant of the network.\nHowever, accurate methods for Lipschitz-constrained training often suffer from\nnon-convex formulations and poor scalability due to reliance on global\nsemidefinite programs (SDPs). In this letter, we propose a convex training\nframework that enforces global Lipschitz constraints via semidefinite\nrelaxation. By reparameterizing the NN using loop transformation, we derive a\nconvex admissibility condition that enables tractable and certifiable training.\nWhile the resulting formulation guarantees robustness, its scalability is\nlimited by the size of global SDP. To overcome this, we develop a randomized\nsubspace linear matrix inequalities (RS-LMI) approach that decomposes the\nglobal constraints into sketched layerwise constraints projected onto\nlow-dimensional subspaces, yielding a smooth and memory-efficient training\nobjective. Empirical results on MNIST, CIFAR-10, and ImageNet demonstrate that\nthe proposed framework achieves competitive accuracy with significantly\nimproved Lipschitz bounds and runtime performance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23977v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.23977v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22722", "title": "Kill Two Birds with One Stone! Trajectory enabled Unified Online Detection of Adversarial Examples and Backdoor Attacks", "authors": ["Anmin Fu", "Fanyu Meng", "Huaibing Peng", "Hua Ma", "Zhi Zhang", "Yifeng Zheng", "Willy Susilo", "Yansong Gao"], "summary": "The proposed UniGuard is the first unified online detection framework capable\nof simultaneously addressing adversarial examples and backdoor attacks.\nUniGuard builds upon two key insights: first, both AE and backdoor attacks have\nto compromise the inference phase, making it possible to tackle them\nsimultaneously during run-time via online detection. Second, an adversarial\ninput, whether a perturbed sample in AE attacks or a trigger-carrying sample in\nbackdoor attacks, exhibits distinctive trajectory signatures from a benign\nsample as it propagates through the layers of a DL model in forward inference.\nThe propagation trajectory of the adversarial sample must deviate from that of\nits benign counterpart; otherwise, the adversarial objective cannot be\nfulfilled. Detecting these trajectory signatures is inherently challenging due\nto their subtlety; UniGuard overcomes this by treating the propagation\ntrajectory as a time-series signal, leveraging LSTM and spectrum transformation\nto amplify differences between adversarial and benign trajectories that are\nsubtle in the time domain. UniGuard exceptional efficiency and effectiveness\nhave been extensively validated across various modalities (image, text, and\naudio) and tasks (classification and regression), ranging from diverse model\narchitectures against a wide range of AE attacks and backdoor attacks,\nincluding challenging partial backdoors and dynamic triggers. When compared to\nSOTA methods, including ContraNet (NDSS 22) specific for AE detection and TED\n(IEEE SP 24) specific for backdoor detection, UniGuard consistently\ndemonstrates superior performance, even when matched against each method's\nstrengths in addressing their respective threats-each SOTA fails to parts of\nattack strategies while UniGuard succeeds for all.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22722v1", "categories": ["cs.CR", "cs.AI"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.22722v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23367", "title": "You Sound a Little Tense: L2 Tailored Clear TTS Using Durational Vowel Properties", "authors": ["Paige Tutt√∂s√≠", "H. Henny Yeung", "Yue Wang", "Jean-Julien Aucouturier", "Angelica Lim"], "summary": "We present the first text-to-speech (TTS) system tailored to second language\n(L2) speakers. We use duration differences between American English tense\n(longer) and lax (shorter) vowels to create a \"clarity mode\" for Matcha-TTS.\nOur perception studies showed that French-L1, English-L2 listeners had fewer\n(at least 9.15%) transcription errors when using our clarity mode, and found it\nmore encouraging and respectful than overall slowed down speech. Remarkably,\nlisteners were not aware of these effects: despite the decreased word error\nrate in clarity mode, listeners still believed that slowing all target words\nwas the most intelligible, suggesting that actual intelligibility does not\ncorrelate with perceived intelligibility. Additionally, we found that\nWhisper-ASR did not use the same cues as L2 speakers to differentiate difficult\nvowels and is not sufficient to assess the intelligibility of TTS systems for\nthese individuals.", "comment": "Accepted to ISCA Speech Synthesis Workshop, 2025", "pdf_url": "http://arxiv.org/pdf/2506.23367v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.23367v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23247", "title": "Aggregating Local Saliency Maps for Semi-Global Explainable Image Classification", "authors": ["James Hinns", "David Martens"], "summary": "Deep learning dominates image classification tasks, yet understanding how\nmodels arrive at predictions remains a challenge. Much research focuses on\nlocal explanations of individual predictions, such as saliency maps, which\nvisualise the influence of specific pixels on a model's prediction. However,\nreviewing many of these explanations to identify recurring patterns is\ninfeasible, while global methods often oversimplify and miss important local\nbehaviours. To address this, we propose Segment Attribution Tables (SATs), a\nmethod for summarising local saliency explanations into (semi-)global insights.\nSATs take image segments (such as \"eyes\" in Chihuahuas) and leverage saliency\nmaps to quantify their influence. These segments highlight concepts the model\nrelies on across instances and reveal spurious correlations, such as reliance\non backgrounds or watermarks, even when out-of-distribution test performance\nsees little change. SATs can explain any classifier for which a form of\nsaliency map can be produced, using segmentation maps that provide named\nsegments. SATs bridge the gap between oversimplified global summaries and\noverly detailed local explanations, offering a practical tool for analysing and\ndebugging image classifiers.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23247v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23247v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23978", "title": "LLM Agents Are the Antidote to Walled Gardens", "authors": ["Samuele Marro", "Philip Torr"], "summary": "While the Internet's core infrastructure was designed to be open and\nuniversal, today's application layer is dominated by closed, proprietary\nplatforms. Open and interoperable APIs require significant investment, and\nmarket leaders have little incentive to enable data exchange that could erode\ntheir user lock-in. We argue that LLM-based agents fundamentally disrupt this\nstatus quo. Agents can automatically translate between data formats and\ninteract with interfaces designed for humans: this makes interoperability\ndramatically cheaper and effectively unavoidable. We name this shift universal\ninteroperability: the ability for any two digital services to exchange data\nseamlessly using AI-mediated adapters. Universal interoperability undermines\nmonopolistic behaviours and promotes data portability. However, it can also\nlead to new security risks and technical debt. Our position is that the ML\ncommunity should embrace this development while building the appropriate\nframeworks to mitigate the downsides. By acting now, we can harness AI to\nrestore user freedom and competitive markets without sacrificing security.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23978v1", "categories": ["cs.LG", "cs.CL", "cs.CY", "cs.SI", "68T50, 68M10, 91B26", "I.2.11; I.2.7; H.4.5"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.23978v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22742", "title": "RAILS: Retrieval-Augmented Intelligence for Learning Software Development", "authors": ["Wali Mohammad Abdullah", "Md. Morshedul Islam", "Devraj Parmar", "Happy Hasmukhbhai Patel", "Sindhuja Prabhakaran", "Baidya Saha"], "summary": "Large Language Models (LLMs) like GPT-3.5-Turbo are increasingly used to\nassist software development, yet they often produce incomplete code or\nincorrect imports, especially when lacking access to external or\nproject-specific documentation. We introduce RAILS (Retrieval-Augmented\nIntelligence for Learning Software Development), a framework that augments LLM\nprompts with semantically retrieved context from curated Java resources using\nFAISS and OpenAI embeddings. RAILS incorporates an iterative validation loop\nguided by compiler feedback to refine suggestions. We evaluated RAILS on 78\nreal-world Java import error cases spanning standard libraries, GUI APIs,\nexternal tools, and custom utilities. Despite using the same LLM, RAILS\noutperforms baseline prompting by preserving intent, avoiding hallucinations,\nand surfacing correct imports even when libraries are unavailable locally.\nFuture work will integrate symbolic filtering via PostgreSQL and extend support\nto other languages and IDEs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22742v1", "categories": ["cs.SE", "cs.AI"], "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.22742v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23394", "title": "Teaching a Language Model to Speak the Language of Tools", "authors": ["Simeon Emanuilov"], "summary": "External tool integration through function-calling is essential for practical\nlanguage model applications, yet most multilingual models lack reliable\ntool-use capabilities in non-English languages. Even state-of-the-art\nmultilingual models struggle with determining when to use tools and generating\nthe structured outputs required for function calls, often exhibiting language\nconfusion when prompted in lower-resource languages. This work presents a\nmethodology for adapting existing language models to enable robust tool use in\nany target language, using Bulgarian as a case study. The approach involves\ncontinued training of the BgGPT model series (2.6B, 9B, 27B parameters) on a\nnovel bilingual dataset of 10,035 function-calling examples designed to support\nstandardized protocols like MCP (Model Context Protocol). The research\nintroduces TUCAN (Tool-Using Capable Assistant Navigator), which achieves up to\n28.75% improvement in function-calling accuracy over base models while\npreserving core language understanding, as verified on established Bulgarian\nbenchmarks. Beyond accuracy gains, TUCAN models demonstrate production-ready\nresponse formatting with clean, parsable function calls, contrasting with the\nverbose and inconsistent outputs of base models. The models, evaluation\nframework, and dataset are released to enable replication for other languages.\nThis work demonstrates a practical approach for extending tool-augmented\ncapabilities beyond English-centric systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23394v1", "categories": ["cs.IR", "cs.AI", "cs.CL", "I.2.7; I.2.1"], "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.23394v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23252", "title": "DGE-YOLO: Dual-Branch Gathering and Attention for Accurate UAV Object Detection", "authors": ["Kunwei Lv", "Ping Lan"], "summary": "The rapid proliferation of unmanned aerial vehicles (UAVs) has highlighted\nthe importance of robust and efficient object detection in diverse aerial\nscenarios. Detecting small objects under complex conditions, however, remains a\nsignificant challenge. Existing approaches often prioritize inference speed,\nleading to degraded performance when handling multi-modal inputs. To address\nthis, we present DGE-YOLO, an enhanced YOLO-based detection framework designed\nto effectively fuse multi-modal information. Specifically, we introduce a\ndual-branch architecture for modality-specific feature extraction, enabling the\nmodel to process both infrared and visible images. To further enrich semantic\nrepresentation, we propose an Efficient Multi-scale Attention (EMA) mechanism\nthat enhances feature learning across spatial scales. Additionally, we replace\nthe conventional neck with a Gather-and-Distribute module to mitigate\ninformation loss during feature aggregation. Extensive experiments on the Drone\nVehicle dataset demonstrate that DGE-YOLO achieves superior performance over\nstate-of-the-art methods, validating its effectiveness in multi-modal UAV\nobject detection tasks.", "comment": "8 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2506.23252v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23252v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23996", "title": "The Jacobian and Hessian of the Kullback-Leibler Divergence between Multivariate Gaussian Distributions (Technical Report)", "authors": ["Juan Maro√±as"], "summary": "This document shows how to obtain the Jacobian and Hessian matrices of the\nKullback-Leibler divergence between two multivariate Gaussian distributions,\nusing the first and second-order differentials. The presented derivations are\nbased on the theory presented by \\cite{magnus99}. I've also got great\ninspiration from some of the derivations in \\cite{minka}.\n  Since I pretend to be at most didactic, the document is split into a summary\nof results and detailed derivations on each of the elements involved, with\nspecific references to the tricks used in the derivations, and to many of the\nunderlying concepts.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23996v1", "categories": ["cs.LG", "math.OC"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.23996v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22771", "title": "FF-INT8: Efficient Forward-Forward DNN Training on Edge Devices with INT8 Precision", "authors": ["Jingxiao Ma", "Priyadarshini Panda", "Sherief Reda"], "summary": "Backpropagation has been the cornerstone of neural network training for\ndecades, yet its inefficiencies in time and energy consumption limit its\nsuitability for resource-constrained edge devices. While low-precision neural\nnetwork quantization has been extensively researched to speed up model\ninference, its application in training has been less explored. Recently, the\nForward-Forward (FF) algorithm has emerged as a promising alternative to\nbackpropagation, replacing the backward pass with an additional forward pass.\nBy avoiding the need to store intermediate activations for backpropagation, FF\ncan reduce memory footprint, making it well-suited for embedded devices. This\npaper presents an INT8 quantized training approach that leverages FF's\nlayer-by-layer strategy to stabilize gradient quantization. Furthermore, we\npropose a novel \"look-ahead\" scheme to address limitations of FF and improve\nmodel accuracy. Experiments conducted on NVIDIA Jetson Orin Nano board\ndemonstrate 4.6% faster training, 8.3% energy savings, and 27.0% reduction in\nmemory usage, while maintaining competitive accuracy compared to the\nstate-of-the-art.", "comment": "To be published in the 62nd Design Automation Conference (DAC), 2025", "pdf_url": "http://arxiv.org/pdf/2506.22771v1", "categories": ["cs.LG", "cs.AI", "cs.NE", "I.2.0; I.2.6"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22771v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23517", "title": "Assessing GPTZero's Accuracy in Identifying AI vs. Human-Written Essays", "authors": ["Selin Dik", "Osman Erdem", "Mehmet Dik"], "summary": "As the use of AI tools by students has become more prevalent, instructors\nhave started using AI detection tools like GPTZero and QuillBot to detect AI\nwritten text. However, the reliability of these detectors remains uncertain. In\nour study, we focused mostly on the success rate of GPTZero, the most-used AI\ndetector, in identifying AI-generated texts based on different lengths of\nrandomly submitted essays: short (40-100 word count), medium (100-350 word\ncount), and long (350-800 word count). We gathered a data set consisting of\ntwenty-eight AI-generated papers and fifty human-written papers. With this\nrandomized essay data, papers were individually plugged into GPTZero and\nmeasured for percentage of AI generation and confidence. A vast majority of the\nAI-generated papers were detected accurately (ranging from 91-100% AI believed\ngeneration), while the human generated essays fluctuated; there were a handful\nof false positives. These findings suggest that although GPTZero is effective\nat detecting purely AI-generated content, its reliability in distinguishing\nhuman-authored texts is limited. Educators should therefore exercise caution\nwhen relying solely on AI detection tools.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23517v1", "categories": ["cs.AI", "cs.CL"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.23517v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23254", "title": "PixelBoost: Leveraging Brownian Motion for Realistic-Image Super-Resolution", "authors": ["Aradhana Mishra", "Bumshik Lee"], "summary": "Diffusion-model-based image super-resolution techniques often face a\ntrade-off between realistic image generation and computational efficiency. This\nissue is exacerbated when inference times by decreasing sampling steps,\nresulting in less realistic and hazy images. To overcome this challenge, we\nintroduce a novel diffusion model named PixelBoost that underscores the\nsignificance of embracing the stochastic nature of Brownian motion in advancing\nimage super-resolution, resulting in a high degree of realism, particularly\nfocusing on texture and edge definitions. By integrating controlled\nstochasticity into the training regimen, our proposed model avoids convergence\nto local optima, effectively capturing and reproducing the inherent uncertainty\nof image textures and patterns. Our proposed model demonstrates superior\nobjective results in terms of learned perceptual image patch similarity\n(LPIPS), lightness order error (LOE), peak signal-to-noise ratio(PSNR),\nstructural similarity index measure (SSIM), as well as visual quality. To\ndetermine the edge enhancement, we evaluated the gradient magnitude and pixel\nvalue, and our proposed model exhibited a better edge reconstruction\ncapability. Additionally, our model demonstrates adaptive learning capabilities\nby effectively adjusting to Brownian noise patterns and introduces a sigmoidal\nnoise sequencing method that simplifies training, resulting in faster inference\nspeeds.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23254v1", "categories": ["cs.CV", "cs.AI", "cs.MM", "eess.IV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23254v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.24000", "title": "The Illusion of Progress? A Critical Look at Test-Time Adaptation for Vision-Language Models", "authors": ["Lijun Sheng", "Jian Liang", "Ran He", "Zilei Wang", "Tieniu Tan"], "summary": "Test-time adaptation (TTA) methods have gained significant attention for\nenhancing the performance of vision-language models (VLMs) such as CLIP during\ninference, without requiring additional labeled data. However, current TTA\nresearches generally suffer from major limitations such as duplication of\nbaseline results, limited evaluation metrics, inconsistent experimental\nsettings, and insufficient analysis. These problems hinder fair comparisons\nbetween TTA methods and obscure their practical strengths and weaknesses. To\naddress these challenges, we introduce TTA-VLM, a comprehensive benchmark for\nevaluating TTA methods on VLMs. Our benchmark implements 8 episodic TTA and 7\nonline TTA methods within a unified and reproducible framework, and evaluates\nthem across 15 widely used datasets. Unlike prior studies focused solely on\nCLIP, we extend the evaluation to SigLIP--a model trained with a Sigmoid\nloss--and include training-time tuning methods such as CoOp, MaPLe, and TeCoA\nto assess generality. Beyond classification accuracy, TTA-VLM incorporates\nvarious evaluation metrics, including robustness, calibration,\nout-of-distribution detection, and stability, enabling a more holistic\nassessment of TTA methods. Through extensive experiments, we find that 1)\nexisting TTA methods produce limited gains compared to the previous pioneering\nwork; 2) current TTA methods exhibit poor collaboration with training-time\nfine-tuning methods; 3) accuracy gains frequently come at the cost of reduced\nmodel trustworthiness. We release TTA-VLM to provide fair comparison and\ncomprehensive evaluation of TTA methods for VLMs, and we hope it encourages the\ncommunity to develop more reliable and generalizable TTA strategies.", "comment": "Github link: https://github.com/TomSheng21/tta-vlm", "pdf_url": "http://arxiv.org/pdf/2506.24000v1", "categories": ["cs.LG", "cs.CV"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.24000v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22776", "title": "Smaller = Weaker? Benchmarking Robustness of Quantized LLMs in Code Generation", "authors": ["Sen Fang", "Weiyuan Ding", "Antonio Mastropaolo", "Bowen Xu"], "summary": "Quantization has emerged as a mainstream method for compressing Large\nLanguage Models (LLMs), reducing memory requirements and accelerating inference\nwithout architectural modifications. While existing research primarily focuses\non evaluating the effectiveness of quantized LLMs compared to their original\ncounterparts, the impact on robustness remains largely unexplored.In this\npaper, we present the first systematic investigation of how quantization\naffects the robustness of LLMs in code generation tasks. Through extensive\nexperiments across four prominent LLM families (LLaMA, DeepSeek, CodeGen, and\nStarCoder) with parameter scales ranging from 350M to 33B, we evaluate\nrobustness from dual perspectives: adversarial attacks on input prompts and\nnoise perturbations on model architecture. Our findings challenge conventional\nwisdom by demonstrating that quantized LLMs often exhibit superior robustness\ncompared to their full-precision counterparts, with 51.59% versus 42.86% of our\nadversarial experiments showing better resilience in quantized LLMs. Similarly,\nour noise perturbation experiments also confirm that LLMs after quantitation\ngenerally withstand higher levels of weight disturbances. These results suggest\nthat quantization not only reduces computational requirements but can actually\nenhance LLMs' reliability in code generation tasks, providing valuable insights\nfor developing more robust and efficient LLM deployment strategies.", "comment": "13 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2506.22776v1", "categories": ["cs.SE", "cs.AI", "cs.PL"], "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.22776v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23563", "title": "MMReason: An Open-Ended Multi-Modal Multi-Step Reasoning Benchmark for MLLMs Toward AGI", "authors": ["Huanjin Yao", "Jiaxing Huang", "Yawen Qiu", "Michael K. Chen", "Wenzheng Liu", "Wei Zhang", "Wenjie Zeng", "Xikun Zhang", "Jingyi Zhang", "Yuxin Song", "Wenhao Wu", "Dacheng Tao"], "summary": "Reasoning plays a crucial role in advancing Multimodal Large Language Models\n(MLLMs) toward Artificial General Intelligence. However, existing MLLM\nbenchmarks often fall short in precisely and comprehensively evaluating\nlong-chain reasoning abilities from three key aspects: (1) lack of difficulty\nand diversity, (2) susceptibility to guessability and memorization, (3)\ninadequate assessment of intermediate reasoning steps. To fill this gap, we\nintroduce MMReason, a new benchmark designed to precisely and comprehensively\nevaluate MLLM long-chain reasoning capability with diverse, open-ended,\nchallenging questions. First, we curate challenging questions requiring\nmulti-step reasoning from various fields (i.e., 6 disciplines) and multiple\ndifficulty levels (i.e., from pre-university to university, and from\nfoundational to competition tiers). Second, these questions are reformulated\ninto an open-ended format and filtered using a multi-model voting technique to\neliminate shortcut cases related to guessing and memorization, ensuring robust\nreasoning evaluations. Third, we annotate the questions with detailed\nstep-by-step solutions, and design a reference-based ternary scoring mechanism\nto reliably assess intermediate reasoning steps. With MMReason, we benchmark\npopular leading MLLMs and provide an in-depth analysis of their reasoning\ncapabilities. We hope MMReason will serve as a valuable resource for advancing\nMLLM reasoning research. Code will be available at\nhttps://github.com/HJYao00/MMReason.", "comment": "Technical report", "pdf_url": "http://arxiv.org/pdf/2506.23563v1", "categories": ["cs.AI", "cs.CL", "cs.CV"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.23563v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23257", "title": "PCLVis: Visual Analytics of Process Communication Latency in Large-Scale Simulation", "authors": ["Chongke Bi", "Xin Gao", "Baofeng Fu", "Yuheng Zhao", "Siming Chen", "Ying Zhao", "Yunhai Wang"], "summary": "Large-scale simulations on supercomputers have become important tools for\nusers. However, their scalability remains a problem due to the huge\ncommunication cost among parallel processes. Most of the existing communication\nlatency analysis methods rely on the physical link layer information, which is\nonly available to administrators. In this paper, a framework called PCLVis is\nproposed to help general users analyze process communication latency (PCL)\nevents. Instead of the physical link layer information, the PCLVis uses the MPI\nprocess communication data for the analysis. First, a spatial PCL event\nlocating method is developed. All processes with high correlation are\nclassified into a single cluster by constructing a process-correlation tree.\nSecond, the propagation path of PCL events is analyzed by constructing a\ncommunication-dependency-based directed acyclic graph (DAG), which can help\nusers interactively explore a PCL event from the temporal evolution of a\nlocated PCL event cluster. In this graph, a sliding window algorithm is\ndesigned to generate the PCL events abstraction. Meanwhile, a new glyph called\nthe communication state glyph (CS-Glyph) is designed for each process to show\nits communication states, including its in/out messages and load balance. Each\nleaf node can be further unfolded to view additional information. Third, a PCL\nevent attribution strategy is formulated to help users optimize their\nsimulations. The effectiveness of the PCLVis framework is demonstrated by\nanalyzing the PCL events of several simulations running on the TH-1A\nsupercomputer. By using the proposed framework, users can greatly improve the\nefficiency of their simulations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23257v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23257v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.24005", "title": "Provably Efficient and Agile Randomized Q-Learning", "authors": ["He Wang", "Xingyu Xu", "Yuejie Chi"], "summary": "While Bayesian-based exploration often demonstrates superior empirical\nperformance compared to bonus-based methods in model-based reinforcement\nlearning (RL), its theoretical understanding remains limited for model-free\nsettings. Existing provable algorithms either suffer from computational\nintractability or rely on stage-wise policy updates which reduce responsiveness\nand slow down the learning process. In this paper, we propose a novel variant\nof Q-learning algorithm, refereed to as RandomizedQ, which integrates\nsampling-based exploration with agile, step-wise, policy updates, for episodic\ntabular RL. We establish an $\\widetilde{O}(\\sqrt{H^5SAT})$ regret bound, where\n$S$ is the number of states, $A$ is the number of actions, $H$ is the episode\nlength, and $T$ is the total number of episodes. In addition, we present a\nlogarithmic regret bound under a mild positive sub-optimality condition on the\noptimal Q-function. Empirically, RandomizedQ exhibits outstanding performance\ncompared to existing Q-learning variants with both bonus-based and\nBayesian-based exploration on standard benchmarks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.24005v1", "categories": ["cs.LG"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.24005v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22777", "title": "Teaching Models to Verbalize Reward Hacking in Chain-of-Thought Reasoning", "authors": ["Miles Turpin", "Andy Arditi", "Marvin Li", "Joe Benton", "Julian Michael"], "summary": "Language models trained with RL can engage in reward hacking--exploiting\nunintended strategies for high reward--without revealing this behavior in their\nchain-of-thought reasoning, making detection difficult and posing risks for\nhigh-stakes applications. We propose verbalization fine-tuning (VFT), a pre-RL\nintervention that trains models to explicitly acknowledge when they are\ninfluenced by prompt cues--hints which point to incorrect answers (e.g., \"a\nStanford professor thinks the answer is A\"). To evaluate VFT, we subsequently\ntrain models with RL on environments where held-out prompt cues signal which\nincorrect answers will receive high reward, incentivizing models to reward hack\nby exploiting cues instead of reasoning correctly. We measure how often models\nexploit these cues without verbalizing it. After RL, only 6% of the VFT-trained\nmodel's responses consist of undetected reward hacks. In comparison, when we\nperform RL without VFT, the rate of undetected reward hacks goes up to 88%;\nwith a debiasing baseline intervention, this increases further to 99%. VFT\nachieves this by substantially increasing how often models verbalize the\ninfluence of cues--from 8% to 42% after VFT, and up to 94% after RL--while\nbaselines remain low even after RL (10% and 1%). Our results show that teaching\nmodels to explicitly verbalize reward hacking behavior before RL significantly\nimproves their detection, offering a practical path toward more transparent and\nsafe AI systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22777v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.22777v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23578", "title": "Reachability in symmetric VASS", "authors": ["≈Åukasz Kami≈Ñski", "S≈Çawomir Lasota"], "summary": "We investigate the reachability problem in symmetric vector addition systems\nwith states (VASS), where transitions are invariant under a group of\npermutations of coordinates. One extremal case, the trivial groups, yields\ngeneral VASS. In another extremal case, the symmetric groups, we show that the\nreachability problem can be solved in PSPACE, regardless of the dimension of\ninput VASS (to be contrasted with Ackermannian complexity in general VASS). We\nalso consider other groups, in particular alternating and cyclic ones.\nFurthermore, motivated by the open status of the reachability problem in data\nVASS, we estimate the gain in complexity when the group arises as a combination\nof the trivial and symmetric groups.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23578v1", "categories": ["cs.FL", "cs.CL"], "cate": "cs.FL", "url": "http://arxiv.org/abs/2506.23578v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23263", "title": "Causal-Entity Reflected Egocentric Traffic Accident Video Synthesis", "authors": ["Lei-lei Li", "Jianwu Fang", "Junbin Xiao", "Shanmin Pang", "Hongkai Yu", "Chen Lv", "Jianru Xue", "Tat-Seng Chua"], "summary": "Egocentricly comprehending the causes and effects of car accidents is crucial\nfor the safety of self-driving cars, and synthesizing causal-entity reflected\naccident videos can facilitate the capability test to respond to unaffordable\naccidents in reality. However, incorporating causal relations as seen in\nreal-world videos into synthetic videos remains challenging. This work argues\nthat precisely identifying the accident participants and capturing their\nrelated behaviors are of critical importance. In this regard, we propose a\nnovel diffusion model, Causal-VidSyn, for synthesizing egocentric traffic\naccident videos. To enable causal entity grounding in video diffusion,\nCausal-VidSyn leverages the cause descriptions and driver fixations to identify\nthe accident participants and behaviors, facilitated by accident reason\nanswering and gaze-conditioned selection modules. To support Causal-VidSyn, we\nfurther construct Drive-Gaze, the largest driver gaze dataset (with 1.54M\nframes of fixations) in driving accident scenarios. Extensive experiments show\nthat Causal-VidSyn surpasses state-of-the-art video diffusion models in terms\nof frame quality and causal sensitivity in various tasks, including accident\nvideo editing, normal-to-accident video diffusion, and text-to-video\ngeneration.", "comment": "Accepted by ICCV2025", "pdf_url": "http://arxiv.org/pdf/2506.23263v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23263v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.24018", "title": "Bridging Theory and Practice in Link Representation with Graph Neural Networks", "authors": ["Veronica Lachi", "Francesco Ferrini", "Antonio Longa", "Bruno Lepri", "Andrea Passerini", "Manfred Jaeger"], "summary": "Graph Neural Networks (GNNs) are widely used to compute representations of\nnode pairs for downstream tasks such as link prediction. Yet, theoretical\nunderstanding of their expressive power has focused almost entirely on\ngraph-level representations. In this work, we shift the focus to links and\nprovide the first comprehensive study of GNN expressiveness in link\nrepresentation. We introduce a unifying framework, the $k_\\phi$-$k_\\rho$-$m$\nframework, that subsumes existing message-passing link models and enables\nformal expressiveness comparisons. Using this framework, we derive a hierarchy\nof state-of-the-art methods and offer theoretical tools to analyze future\narchitectures. To complement our analysis, we propose a synthetic evaluation\nprotocol comprising the first benchmark specifically designed to assess\nlink-level expressiveness. Finally, we ask: does expressiveness matter in\npractice? We use a graph symmetry metric that quantifies the difficulty of\ndistinguishing links and show that while expressive models may underperform on\nstandard benchmarks, they significantly outperform simpler ones as symmetry\nincreases, highlighting the need for dataset-aware model selection.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.24018v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.24018v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22783", "title": "PhonemeFake: Redefining Deepfake Realism with Language-Driven Segmental Manipulation and Adaptive Bilevel Detection", "authors": ["Oguzhan Baser", "Ahmet Ege Tanriverdi", "Sriram Vishwanath", "Sandeep P. Chinchali"], "summary": "Deepfake (DF) attacks pose a growing threat as generative models become\nincreasingly advanced. However, our study reveals that existing DF datasets\nfail to deceive human perception, unlike real DF attacks that influence public\ndiscourse. It highlights the need for more realistic DF attack vectors. We\nintroduce PhonemeFake (PF), a DF attack that manipulates critical speech\nsegments using language reasoning, significantly reducing human perception by\nup to 42% and benchmark accuracies by up to 94%. We release an easy-to-use PF\ndataset on HuggingFace and open-source bilevel DF segment detection model that\nadaptively prioritizes compute on manipulated regions. Our extensive\nexperiments across three known DF datasets reveal that our detection model\nreduces EER by 91% while achieving up to 90% speed-up, with minimal compute\noverhead and precise localization beyond existing models as a scalable\nsolution.", "comment": "5 pages, 3 figures, Published at Proceedings of Interspeech 2025, for\n  the dataset see https://huggingface.co/datasets/phonemefake/PhonemeFakeV2,\n  for the code see https://github.com/UTAustin-SwarmLab/ PhonemeFake", "pdf_url": "http://arxiv.org/pdf/2506.22783v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22783v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23670", "title": "Efficient Interleaved Speech Modeling through Knowledge Distillation", "authors": ["Mohammadmahdi Nouriborji", "Morteza Rohanian"], "summary": "Current speech language models exceed the size and latency constraints of\nmany deployment environments. We build compact, expressive speech generation\nmodels through layer-aligned distillation, matching hidden states, attention\nmaps, and softened logits to compress large multimodal transformers by 3x with\nminimal loss in performance. We introduce TinyWave, a family of 2B-parameter\nmodels for speech-to-speech and interleaved speech-text generation, trained on\n50,000 hours of public audio. TinyWave supports (i) speech-only generation\nusing phonetic or expressive tokens and (ii) mixed speech-text continuations.\nEvaluation on Libri-Light shows TinyWave within 1.4 normalized perplexity\npoints of its teacher. Accuracy on spoken StoryCloze and SALMon reaches 93-97%\nof the teacher's performance, outperforming size-matched baselines. These\nmodels are optimized for deployment on commodity hardware, enabling\napplications in real-time conversational agents, assistive technologies, and\nlow-resource environments. We release models, training code, and evaluation\nscripts to support reproducible research on compact, expressive speech\ngeneration.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23670v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.23670v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23270", "title": "Token Activation Map to Visually Explain Multimodal LLMs", "authors": ["Yi Li", "Hualiang Wang", "Xinpeng Ding", "Haonan Wang", "Xiaomeng Li"], "summary": "Multimodal large language models (MLLMs) are broadly empowering various\nfields. Despite their advancements, the explainability of MLLMs remains less\nexplored, hindering deeper understanding, model credibility, and effective\nvisualization. Unlike conventional vision models (e.g., CNNs, ViTs, CLIP) that\nproduce a single output, MLLMs generate sequences of tokens progressively,\nwhere each generated token depends on the previous context. Therefore, earlier\ncontext tokens can introduce redundant activations that interfere with the\nexplanation of later tokens beyond their original information. Existing studies\noften overlook this issue, but our observations reveal that these redundant\ncorrelations can significantly hurt the reliability of explanations. To address\nthis, we propose an estimated causal inference method to mitigate the\ninterference of context to achieve high-quality MLLM explanation, with a novel\nrank Gaussian filter to further reduce activation noises. We term this method\nToken Activation Map (TAM) to highlight the consideration of interactions\nbetween tokens. TAM also indicates that it excels at explaining multiple tokens\nof MLLM, which is different from the Class Activation Map (CAM) for a single\nprediction. Our TAM method significantly outperforms existing SoTA methods,\nshowcasing high-quality visualization results that can be utilized for various\nscenarios, such as object localization, failure case analysis, video\nvisualization, MLLMs visual comparison, and model understanding (e.g., color,\nshape, action, location, visual reasoning, multi-turn conversation, etc). The\ncode is available atgithub.com/xmed-lab/TAM.", "comment": "ICCV2025 Accepted", "pdf_url": "http://arxiv.org/pdf/2506.23270v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23270v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.24042", "title": "Faster Diffusion Models via Higher-Order Approximation", "authors": ["Gen Li", "Yuchen Zhou", "Yuting Wei", "Yuxin Chen"], "summary": "In this paper, we explore provable acceleration of diffusion models without\nany additional retraining. Focusing on the task of approximating a target data\ndistribution in $\\mathbb{R}^d$ to within $\\varepsilon$ total-variation\ndistance, we propose a principled, training-free sampling algorithm that\nrequires only the order of\n  $$ d^{1+2/K} \\varepsilon^{-1/K} $$\n  score function evaluations (up to log factor) in the presence of accurate\nscores, where $K$ is an arbitrarily large fixed integer. This result applies to\na broad class of target data distributions, without the need for assumptions\nsuch as smoothness or log-concavity. Our theory is robust vis-a-vis inexact\nscore estimation, degrading gracefully as the score estimation error increases\n-- without demanding higher-order smoothness on the score estimates as assumed\nin previous work. The proposed algorithm draws insight from high-order ODE\nsolvers, leveraging high-order Lagrange interpolation and successive refinement\nto approximate the integral derived from the probability flow ODE.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.24042v1", "categories": ["cs.LG", "cs.NA", "math.NA", "math.ST", "stat.ML", "stat.TH"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.24042v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22784", "title": "Single-Frame Point-Pixel Registration via Supervised Cross-Modal Feature Matching", "authors": ["Yu Han", "Zhiwei Huang", "Yanting Zhang", "Fangjun Ding", "Shen Cai", "Rui Fan"], "summary": "Point-pixel registration between LiDAR point clouds and camera images is a\nfundamental yet challenging task in autonomous driving and robotic perception.\nA key difficulty lies in the modality gap between unstructured point clouds and\nstructured images, especially under sparse single-frame LiDAR settings.\nExisting methods typically extract features separately from point clouds and\nimages, then rely on hand-crafted or learned matching strategies. This separate\nencoding fails to bridge the modality gap effectively, and more critically,\nthese methods struggle with the sparsity and noise of single-frame LiDAR, often\nrequiring point cloud accumulation or additional priors to improve reliability.\nInspired by recent progress in detector-free matching paradigms (e.g.\nMatchAnything), we revisit the projection-based approach and introduce the\ndetector-free framework for direct point-pixel matching between LiDAR and\ncamera views. Specifically, we project the LiDAR intensity map into a 2D view\nfrom the LiDAR perspective and feed it into an attention-based detector-free\nmatching network, enabling cross-modal correspondence estimation without\nrelying on multi-frame accumulation. To further enhance matching reliability,\nwe introduce a repeatability scoring mechanism that acts as a soft visibility\nprior. This guides the network to suppress unreliable matches in regions with\nlow intensity variation, improving robustness under sparse input. Extensive\nexperiments on KITTI, nuScenes, and MIAS-LCEC-TF70 benchmarks demonstrate that\nour method achieves state-of-the-art performance, outperforming prior\napproaches on nuScenes (even those relying on accumulated point clouds),\ndespite using only single-frame LiDAR.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22784v1", "categories": ["cs.CV", "cs.AI", "cs.RO"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22784v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23706", "title": "Attestable Audits: Verifiable AI Safety Benchmarks Using Trusted Execution Environments", "authors": ["Christoph Schnabl", "Daniel Hugenroth", "Bill Marino", "Alastair R. Beresford"], "summary": "Benchmarks are important measures to evaluate safety and compliance of AI\nmodels at scale. However, they typically do not offer verifiable results and\nlack confidentiality for model IP and benchmark datasets. We propose Attestable\nAudits, which run inside Trusted Execution Environments and enable users to\nverify interaction with a compliant AI model. Our work protects sensitive data\neven when model provider and auditor do not trust each other. This addresses\nverification challenges raised in recent AI governance frameworks. We build a\nprototype demonstrating feasibility on typical audit benchmarks against\nLlama-3.1.", "comment": "ICML 2024 Workshop TAIG", "pdf_url": "http://arxiv.org/pdf/2506.23706v1", "categories": ["cs.AI", "cs.CL", "cs.CR"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.23706v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23271", "title": "Mettle: Meta-Token Learning for Memory-Efficient Audio-Visual Adaptation", "authors": ["Jinxing Zhou", "Zhihui Li", "Yongqiang Yu", "Yanghao Zhou", "Ruohao Guo", "Guangyao Li", "Yuxin Mao", "Mingfei Han", "Xiaojun Chang", "Meng Wang"], "summary": "We present \\textbf{Met}a-\\textbf{T}oken \\textbf{Le}arning (Mettle), a simple\nand memory-efficient method for adapting large-scale pretrained transformer\nmodels to downstream audio-visual tasks. Instead of sequentially modifying the\noutput feature distribution of the transformer backbone, Mettle utilizes a\nlightweight \\textit{Layer-Centric Distillation (LCD)} module to distill in\nparallel the intact audio or visual features embedded by each transformer layer\ninto compact meta-tokens. This distillation process considers both pretrained\nknowledge preservation and task-specific adaptation. The obtained meta-tokens\ncan be directly applied to classification tasks, such as audio-visual event\nlocalization and audio-visual video parsing. To further support fine-grained\nsegmentation tasks, such as audio-visual segmentation, we introduce a\n\\textit{Meta-Token Injection (MTI)} module, which utilizes the audio and visual\nmeta-tokens distilled from the top transformer layer to guide feature\nadaptation in earlier layers. Extensive experiments on multiple audiovisual\nbenchmarks demonstrate that our method significantly reduces memory usage and\ntraining time while maintaining parameter efficiency and competitive accuracy.", "comment": "Technical Report", "pdf_url": "http://arxiv.org/pdf/2506.23271v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23271v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.24093", "title": "Development of Hybrid Artificial Intelligence Training on Real and Synthetic Data: Benchmark on Two Mixed Training Strategies", "authors": ["Paul Wachter", "Lukas Niehaus", "Julius Sch√∂ning"], "summary": "Synthetic data has emerged as a cost-effective alternative to real data for\ntraining artificial neural networks (ANN). However, the disparity between\nsynthetic and real data results in a domain gap. That gap leads to poor\nperformance and generalization of the trained ANN when applied to real-world\nscenarios. Several strategies have been developed to bridge this gap, which\ncombine synthetic and real data, known as mixed training using hybrid datasets.\nWhile these strategies have been shown to mitigate the domain gap, a systematic\nevaluation of their generalizability and robustness across various tasks and\narchitectures remains underexplored. To address this challenge, our study\ncomprehensively analyzes two widely used mixing strategies on three prevalent\narchitectures and three distinct hybrid datasets. From these datasets, we\nsample subsets with varying proportions of synthetic to real data to\ninvestigate the impact of synthetic and real components. The findings of this\npaper provide valuable insights into optimizing the use of synthetic data in\nthe training process of any ANN, contributing to enhancing robustness and\nefficacy.", "comment": "21pages, 14 figures, 2 tables", "pdf_url": "http://arxiv.org/pdf/2506.24093v1", "categories": ["cs.LG", "cs.AI", "I.2.1; I.2.0; F.2.3"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.24093v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22789", "title": "WavShape: Information-Theoretic Speech Representation Learning for Fair and Privacy-Aware Audio Processing", "authors": ["Oguzhan Baser", "Ahmet Ege Tanriverdi", "Kaan Kale", "Sandeep P. Chinchali", "Sriram Vishwanath"], "summary": "Speech embeddings often retain sensitive attributes such as speaker identity,\naccent, or demographic information, posing risks in biased model training and\nprivacy leakage. We propose WavShape, an information-theoretic speech\nrepresentation learning framework that optimizes embeddings for fairness and\nprivacy while preserving task-relevant information. We leverage mutual\ninformation (MI) estimation using the Donsker-Varadhan formulation to guide an\nMI-based encoder that systematically filters sensitive attributes while\nmaintaining speech content essential for downstream tasks. Experimental results\non three known datasets show that WavShape reduces MI between embeddings and\nsensitive attributes by up to 81% while retaining 97% of task-relevant\ninformation. By integrating information theory with self-supervised speech\nmodels, this work advances the development of fair, privacy-aware, and\nresource-efficient speech systems.", "comment": "5 pages, 4 figures, Published at The Proceedings of Interspeech 2025,\n  code is available at http://www.github.com/UTAustin-SwarmLab/WavShape", "pdf_url": "http://arxiv.org/pdf/2506.22789v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.22789v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23714", "title": "Towards an Automated Multimodal Approach for Video Summarization: Building a Bridge Between Text, Audio and Facial Cue-Based Summarization", "authors": ["Md Moinul Islam", "Sofoklis Kakouros", "Janne Heikkil√§", "Mourad Oussalah"], "summary": "The increasing volume of video content in educational, professional, and\nsocial domains necessitates effective summarization techniques that go beyond\ntraditional unimodal approaches. This paper proposes a behaviour-aware\nmultimodal video summarization framework that integrates textual, audio, and\nvisual cues to generate timestamp-aligned summaries. By extracting prosodic\nfeatures, textual cues and visual indicators, the framework identifies\nsemantically and emotionally important moments. A key contribution is the\nidentification of bonus words, which are terms emphasized across multiple\nmodalities and used to improve the semantic relevance and expressive clarity of\nthe summaries. The approach is evaluated against pseudo-ground truth (pGT)\nsummaries generated using LLM-based extractive method. Experimental results\ndemonstrate significant improvements over traditional extractive method, such\nas the Edmundson method, in both text and video-based evaluation metrics.\nText-based metrics show ROUGE-1 increasing from 0.4769 to 0.7929 and BERTScore\nfrom 0.9152 to 0.9536, while in video-based evaluation, our proposed framework\nimproves F1-Score by almost 23%. The findings underscore the potential of\nmultimodal integration in producing comprehensive and behaviourally informed\nvideo summaries.", "comment": "Accepted to HHAI WS 2025: Workshops at the Fourth International\n  Conference on Hybrid Human-Artificial Intelligence (HHAI)", "pdf_url": "http://arxiv.org/pdf/2506.23714v1", "categories": ["cs.CV", "cs.CL"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23714v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23275", "title": "Why Settle for One? Text-to-ImageSet Generation and Evaluation", "authors": ["Chengyou Jia", "Xin Shen", "Zhuohang Dang", "Zhuohang Dang", "Changliang Xia", "Weijia Wu", "Xinyu Zhang", "Hangwei Qian", "Ivor W. Tsang", "Minnan Luo"], "summary": "Despite remarkable progress in Text-to-Image models, many real-world\napplications require generating coherent image sets with diverse consistency\nrequirements. Existing consistent methods often focus on a specific domain with\nspecific aspects of consistency, which significantly constrains their\ngeneralizability to broader applications. In this paper, we propose a more\nchallenging problem, Text-to-ImageSet (T2IS) generation, which aims to generate\nsets of images that meet various consistency requirements based on user\ninstructions. To systematically study this problem, we first introduce\n$\\textbf{T2IS-Bench}$ with 596 diverse instructions across 26 subcategories,\nproviding comprehensive coverage for T2IS generation. Building on this, we\npropose $\\textbf{T2IS-Eval}$, an evaluation framework that transforms user\ninstructions into multifaceted assessment criteria and employs effective\nevaluators to adaptively assess consistency fulfillment between criteria and\ngenerated sets. Subsequently, we propose $\\textbf{AutoT2IS}$, a training-free\nframework that maximally leverages pretrained Diffusion Transformers'\nin-context capabilities to harmonize visual elements to satisfy both\nimage-level prompt alignment and set-level visual consistency. Extensive\nexperiments on T2IS-Bench reveal that diverse consistency challenges all\nexisting methods, while our AutoT2IS significantly outperforms current\ngeneralized and even specialized approaches. Our method also demonstrates the\nability to enable numerous underexplored real-world applications, confirming\nits substantial practical value. Visit our project in\nhttps://chengyou-jia.github.io/T2IS-Home.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23275v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23275v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.24120", "title": "Data Uniformity Improves Training Efficiency and More, with a Convergence Framework Beyond the NTK Regime", "authors": ["Yuqing Wang", "Shangding Gu"], "summary": "Data selection plays a crucial role in data-driven decision-making, including\nin large language models (LLMs), and is typically task-dependent. Properties\nsuch as data quality and diversity have been extensively studied and are known\nto enhance model performance. However, it remains unclear whether there exist\nother quantitative and general principles of data selection that can\nconsistently improve performance, especially for complex tasks with limited\nprior knowledge. In this paper, we demonstrate that selecting more uniformly\ndistributed data can improve training efficiency while enhancing performance.\nSpecifically, we establish that more uniform (less biased) distribution leads\nto a larger minimum pairwise distance between data points, denoted by\n$h_{\\min}$, and prove that a smaller $h_{\\min}$ can slow down the training\ndynamics of gradient descent (GD). Moreover, we theoretically show that the\napproximation error of neural networks decreases as $h_{\\min}$ increases. Our\nanalysis introduces a convergence framework for GD beyond the Neural Tangent\nKernel (NTK) regime, applicable to a broad class of architectures, including\ntransformers, without requiring Lipschitz smoothness. This framework further\nprovides theoretical justification for the use of residual connections and\nfunction compositions in deep neural architectures. In the end, we conduct\ncomprehensive experiments for supervised fine-tuning across various settings,\nincluding different optimization strategies, model sizes, and training\ndatasets. The results consistently demonstrate that selecting data by\nmaximizing pairwise distance significantly accelerates training and achieves\ncomparable or better performance in LLMs across diverse datasets. Code and\nDatasets are available at the link:\nhttps://github.com/SafeRL-Lab/data-uniformity.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.24120v1", "categories": ["cs.LG", "cs.AI", "math.OC", "stat.ML"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.24120v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22793", "title": "Offline Reinforcement Learning for Mobility Robustness Optimization", "authors": ["Pegah Alizadeh", "Anastasios Giovanidis", "Pradeepa Ramachandra", "Vasileios Koutsoukis", "Osama Arouk"], "summary": "In this work we revisit the Mobility Robustness Optimisation (MRO) algorithm\nand study the possibility of learning the optimal Cell Individual Offset tuning\nusing offline Reinforcement Learning. Such methods make use of collected\noffline datasets to learn the optimal policy, without further exploration. We\nadapt and apply a sequence-based method called Decision Transformers as well as\na value-based method called Conservative Q-Learning to learn the optimal policy\nfor the same target reward as the vanilla rule-based MRO. The same input\nfeatures related to failures, ping-pongs, and other handover issues are used.\nEvaluation for realistic New Radio networks with 3500 MHz carrier frequency on\na traffic mix including diverse user service types and a specific tunable\ncell-pair shows that offline-RL methods outperform rule-based MRO, offering up\nto 7% improvement. Furthermore, offline-RL can be trained for diverse objective\nfunctions using the same available dataset, thus offering operational\nflexibility compared to rule-based methods.", "comment": "7 pages, double column, 4 figures, 6 tables, conference submission", "pdf_url": "http://arxiv.org/pdf/2506.22793v1", "categories": ["cs.NI", "cs.AI", "cs.PF"], "cate": "cs.NI", "url": "http://arxiv.org/abs/2506.22793v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23845", "title": "Use Sparse Autoencoders to Discover Unknown Concepts, Not to Act on Known Concepts", "authors": ["Kenny Peng", "Rajiv Movva", "Jon Kleinberg", "Emma Pierson", "Nikhil Garg"], "summary": "While sparse autoencoders (SAEs) have generated significant excitement, a\nseries of negative results have added to skepticism about their usefulness.\nHere, we establish a conceptual distinction that reconciles competing\nnarratives surrounding SAEs. We argue that while SAEs may be less effective for\nacting on known concepts, SAEs are powerful tools for discovering unknown\nconcepts. This distinction cleanly separates existing negative and positive\nresults, and suggests several classes of SAE applications. Specifically, we\noutline use cases for SAEs in (i) ML interpretability, explainability,\nfairness, auditing, and safety, and (ii) social and health sciences.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23845v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CY"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.23845v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23282", "title": "Autoregressive Denoising Score Matching is a Good Video Anomaly Detector", "authors": ["Hanwen Zhang", "Congqi Cao", "Qinyi Lv", "Lingtong Min", "Yanning Zhang"], "summary": "Video anomaly detection (VAD) is an important computer vision problem. Thanks\nto the mode coverage capabilities of generative models, the likelihood-based\nparadigm is catching growing interest, as it can model normal distribution and\ndetect out-of-distribution anomalies. However, these likelihood-based methods\nare blind to the anomalies located in local modes near the learned\ndistribution. To handle these ``unseen\" anomalies, we dive into three gaps\nuniquely existing in VAD regarding scene, motion and appearance. Specifically,\nwe first build a noise-conditioned score transformer for denoising score\nmatching. Then, we introduce a scene-dependent and motion-aware score function\nby embedding the scene condition of input sequences into our model and\nassigning motion weights based on the difference between key frames of input\nsequences. Next, to solve the problem of blindness in principle, we integrate\nunaffected visual information via a novel autoregressive denoising score\nmatching mechanism for inference. Through autoregressively injecting\nintensifying Gaussian noise into the denoised data and estimating the\ncorresponding score function, we compare the denoised data with the original\ndata to get a difference and aggregate it with the score function for an\nenhanced appearance perception and accumulate the abnormal context. With all\nthree gaps considered, we can compute a more comprehensive anomaly indicator.\nExperiments on three popular VAD benchmarks demonstrate the state-of-the-art\nperformance of our method.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23282v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23282v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.24124", "title": "Teaching Time Series to See and Speak: Forecasting with Aligned Visual and Textual Perspectives", "authors": ["Dong Sixun", "Fan Wei", "Teresa Wu", "Fu Yanjie"], "summary": "Time series forecasting traditionally relies on unimodal numerical inputs,\nwhich often struggle to capture high-level semantic patterns due to their dense\nand unstructured nature. While recent approaches have explored representing\ntime series as text using large language models (LLMs), these methods remain\nlimited by the discrete nature of token sequences and lack the perceptual\nintuition humans typically apply, such as interpreting visual patterns. In this\npaper, we propose a multimodal contrastive learning framework that transforms\nraw time series into structured visual and textual perspectives. Rather than\nusing natural language or real-world images, we construct both modalities\ndirectly from numerical sequences. We then align these views in a shared\nsemantic space via contrastive learning, enabling the model to capture richer\nand more complementary representations. Furthermore, we introduce a variate\nselection module that leverages the aligned representations to identify the\nmost informative variables for multivariate forecasting. Extensive experiments\non fifteen short-term and six long-term forecasting benchmarks demonstrate that\nour approach consistently outperforms strong unimodal and cross-modal\nbaselines, highlighting the effectiveness of multimodal alignment in enhancing\ntime series forecasting. Code is available at:\nhttps://github.com/Ironieser/TimesCLIP.", "comment": "Code: https://github.com/Ironieser/TimesCLIP", "pdf_url": "http://arxiv.org/pdf/2506.24124v1", "categories": ["cs.LG", "cs.CV"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.24124v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22808", "title": "MedEthicsQA: A Comprehensive Question Answering Benchmark for Medical Ethics Evaluation of LLMs", "authors": ["Jianhui Wei", "Zijie Meng", "Zikai Xiao", "Tianxiang Hu", "Yang Feng", "Zhijie Zhou", "Jian Wu", "Zuozhu Liu"], "summary": "While Medical Large Language Models (MedLLMs) have demonstrated remarkable\npotential in clinical tasks, their ethical safety remains insufficiently\nexplored. This paper introduces $\\textbf{MedEthicsQA}$, a comprehensive\nbenchmark comprising $\\textbf{5,623}$ multiple-choice questions and\n$\\textbf{5,351}$ open-ended questions for evaluation of medical ethics in LLMs.\nWe systematically establish a hierarchical taxonomy integrating global medical\nethical standards. The benchmark encompasses widely used medical datasets,\nauthoritative question banks, and scenarios derived from PubMed literature.\nRigorous quality control involving multi-stage filtering and multi-faceted\nexpert validation ensures the reliability of the dataset with a low error rate\n($2.72\\%$). Evaluation of state-of-the-art MedLLMs exhibit declined performance\nin answering medical ethics questions compared to their foundation\ncounterparts, elucidating the deficiencies of medical ethics alignment. The\ndataset, registered under CC BY-NC 4.0 license, is available at\nhttps://github.com/JianhuiWei7/MedEthicsQA.", "comment": "20 pages", "pdf_url": "http://arxiv.org/pdf/2506.22808v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.22808v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23978", "title": "LLM Agents Are the Antidote to Walled Gardens", "authors": ["Samuele Marro", "Philip Torr"], "summary": "While the Internet's core infrastructure was designed to be open and\nuniversal, today's application layer is dominated by closed, proprietary\nplatforms. Open and interoperable APIs require significant investment, and\nmarket leaders have little incentive to enable data exchange that could erode\ntheir user lock-in. We argue that LLM-based agents fundamentally disrupt this\nstatus quo. Agents can automatically translate between data formats and\ninteract with interfaces designed for humans: this makes interoperability\ndramatically cheaper and effectively unavoidable. We name this shift universal\ninteroperability: the ability for any two digital services to exchange data\nseamlessly using AI-mediated adapters. Universal interoperability undermines\nmonopolistic behaviours and promotes data portability. However, it can also\nlead to new security risks and technical debt. Our position is that the ML\ncommunity should embrace this development while building the appropriate\nframeworks to mitigate the downsides. By acting now, we can harness AI to\nrestore user freedom and competitive markets without sacrificing security.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23978v1", "categories": ["cs.LG", "cs.CL", "cs.CY", "cs.SI", "68T50, 68M10, 91B26", "I.2.11; I.2.7; H.4.5"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.23978v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23283", "title": "MoMa: Modulating Mamba for Adapting Image Foundation Models to Video Recognition", "authors": ["Yuhuan Yang", "Chaofan Ma", "Zhenjie Mao", "Jiangchao Yao", "Ya Zhang", "Yanfeng Wang"], "summary": "Video understanding is a complex challenge that requires effective modeling\nof spatial-temporal dynamics. With the success of image foundation models\n(IFMs) in image understanding, recent approaches have explored\nparameter-efficient fine-tuning (PEFT) to adapt IFMs for video. However, most\nof these methods tend to process spatial and temporal information separately,\nwhich may fail to capture the full intricacy of video dynamics. In this paper,\nwe propose MoMa, an efficient adapter framework that achieves full\nspatial-temporal modeling by integrating Mamba's selective state space modeling\ninto IFMs. We propose a novel SeqMod operation to inject spatial-temporal\ninformation into pre-trained IFMs, without disrupting their original features.\nBy incorporating SeqMod into a Divide-and-Modulate architecture, MoMa enhances\nvideo understanding while maintaining computational efficiency. Extensive\nexperiments on multiple video benchmarks demonstrate the effectiveness of MoMa,\nachieving superior performance with reduced computational cost.", "comment": "ICML 2025 paper", "pdf_url": "http://arxiv.org/pdf/2506.23283v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23283v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2504.15071", "title": "Aria-MIDI: A Dataset of Piano MIDI Files for Symbolic Music Modeling", "authors": ["Louis Bradshaw", "Simon Colton"], "summary": "We introduce an extensive new dataset of MIDI files, created by transcribing\naudio recordings of piano performances into their constituent notes. The data\npipeline we use is multi-stage, employing a language model to autonomously\ncrawl and score audio recordings from the internet based on their metadata,\nfollowed by a stage of pruning and segmentation using an audio classifier. The\nresulting dataset contains over one million distinct MIDI files, comprising\nroughly 100,000 hours of transcribed audio. We provide an in-depth analysis of\nour techniques, offering statistical insights, and investigate the content by\nextracting metadata tags, which we also provide. Dataset available at\nhttps://github.com/loubbrad/aria-midi.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.15071v1", "categories": ["cs.SD", "cs.AI", "cs.LG"], "cate": "cs.SD", "url": "http://arxiv.org/abs/2504.15071v1", "date": "2025-04-21", "updated": "2025-04-21"}
{"id": "2506.22809", "title": "BayesLoRA: Task-Specific Uncertainty in Low-Rank Adapters", "authors": ["Cooper Doyle"], "summary": "We propose BayesLoRA, a task-specific uncertainty quantification framework\nthat integrates MC-Dropout into Low-Rank Adapters (LoRA). Unlike\ngeneral-purpose transformer uncertainty methods, BayesLoRA provides guardrails\ntailored to downstream workflows, enabling agents to introspect and modulate\nbehavior under uncertainty. We demonstrate mathematically and empirically that\nLoRA adapters exhibit amplified variance outside fine-tuning distributions,\nyielding reliable confidence estimates for agentic decision-making.", "comment": "13 pages, 3 figures, 1 table", "pdf_url": "http://arxiv.org/pdf/2506.22809v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22809v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.24019", "title": "Ella: Embodied Social Agents with Lifelong Memory", "authors": ["Hongxin Zhang", "Zheyuan Zhang", "Zeyuan Wang", "Zunzhe Zhang", "Lixing Fang", "Qinhong Zhou", "Chuang Gan"], "summary": "We introduce Ella, an embodied social agent capable of lifelong learning\nwithin a community in a 3D open world, where agents accumulate experiences and\nacquire knowledge through everyday visual observations and social interactions.\nAt the core of Ella's capabilities is a structured, long-term multimodal memory\nsystem that stores, updates, and retrieves information effectively. It consists\nof a name-centric semantic memory for organizing acquired knowledge and a\nspatiotemporal episodic memory for capturing multimodal experiences. By\nintegrating this lifelong memory system with foundation models, Ella retrieves\nrelevant information for decision-making, plans daily activities, builds social\nrelationships, and evolves autonomously while coexisting with other intelligent\nbeings in the open world. We conduct capability-oriented evaluations in a\ndynamic 3D open world where 15 agents engage in social activities for days and\nare assessed with a suite of unseen controlled evaluations. Experimental\nresults show that Ella can influence, lead, and cooperate with other agents\nwell to achieve goals, showcasing its ability to learn effectively through\nobservation and social interaction. Our findings highlight the transformative\npotential of combining structured memory systems with foundation models for\nadvancing embodied intelligence. More videos can be found at\nhttps://umass-embodied-agi.github.io/Ella/.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.24019v1", "categories": ["cs.CV", "cs.CL"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.24019v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23285", "title": "Competitive Distillation: A Simple Learning Strategy for Improving Visual Classification", "authors": ["Daqian Shi", "Xiaolei Diao", "Xu Chen", "C√©dric M. John"], "summary": "Deep Neural Networks (DNNs) have significantly advanced the field of computer\nvision. To improve DNN training process, knowledge distillation methods\ndemonstrate their effectiveness in accelerating network training by introducing\na fixed learning direction from the teacher network to student networks. In\nthis context, several distillation-based optimization strategies are proposed,\ne.g., deep mutual learning and self-distillation, as an attempt to achieve\ngeneric training performance enhancement through the cooperative training of\nmultiple networks. However, such strategies achieve limited improvements due to\nthe poor understanding of the impact of learning directions among networks\nacross different iterations. In this paper, we propose a novel competitive\ndistillation strategy that allows each network in a group to potentially act as\na teacher based on its performance, enhancing the overall learning performance.\nCompetitive distillation organizes a group of networks to perform a shared task\nand engage in competition, where competitive optimization is proposed to\nimprove the parameter updating process. We further introduce stochastic\nperturbation in competitive distillation, aiming to motivate networks to induce\nmutations to achieve better visual representations and global optimum. The\nexperimental results show that competitive distillation achieves promising\nperformance in diverse tasks and datasets.", "comment": "Accepted by ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2506.23285v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23285v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.22440", "title": "From Model Design to Organizational Design: Complexity Redistribution and Trade-Offs in Generative AI", "authors": ["Sharique Hasan", "Alexander Oettl", "Sampsa Samila"], "summary": "This paper introduces the Generality-Accuracy-Simplicity (GAS) framework to\nanalyze how large language models (LLMs) are reshaping organizations and\ncompetitive strategy. We argue that viewing AI as a simple reduction in input\ncosts overlooks two critical dynamics: (a) the inherent trade-offs among\ngenerality, accuracy, and simplicity, and (b) the redistribution of complexity\nacross stakeholders. While LLMs appear to defy the traditional trade-off by\noffering high generality and accuracy through simple interfaces, this\nuser-facing simplicity masks a significant shift of complexity to\ninfrastructure, compliance, and specialized personnel. The GAS trade-off,\ntherefore, does not disappear but is relocated from the user to the\norganization, creating new managerial challenges, particularly around accuracy\nin high-stakes applications. We contend that competitive advantage no longer\nstems from mere AI adoption, but from mastering this redistributed complexity\nthrough the design of abstraction layers, workflow alignment, and complementary\nexpertise. This study advances AI strategy by clarifying how scalable cognition\nrelocates complexity and redefines the conditions for technology integration.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22440v1", "categories": ["cs.CY", "cs.LG", "cs.MA", "econ.GN", "q-fin.EC"], "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.22440v1", "date": "2025-06-10", "updated": "2025-06-10"}
{"id": "2506.22818", "title": "TriADA: Massively Parallel Trilinear Matrix-by-Tensor Multiply-Add Algorithm and Device Architecture for the Acceleration of 3D Discrete Transformations", "authors": ["Stanislav Sedukhin", "Yoichi Tomioka", "Kazuya Matsumoto", "Yuichi Okuyama"], "summary": "Multilinear transformations are key in high-performance computing (HPC) and\nartificial intelligence (AI) workloads, where data is represented as tensors.\nHowever, their high computational and memory demands, which grow with\ndimensionality, often slow down critical tasks. Moreover, scaling computation\nby enlarging the number of parallel processing units substantially increases\nenergy consumption, limiting widespread adoption, especially for sparse data,\nwhich is common in HPC and AI applications. This paper introduces the Trilinear\nAlgorithm and isomorphic to algorithm Device Architecture (TriADA) to address\nthese challenges with the following innovations: (1) a massively parallel,\nlow-rank algorithm for computing a family of trilinear (3D) discrete orthogonal\ntransformations (3D-DXTs), which is a special case of the more general 3-mode\nmatrix-by-tensor multiplication (3D-GEMT); (2) a new outer-product-based GEMM\nkernel with decoupled streaming active memory, specially designed to accelerate\n3D-GEMT operation; (3) an isomorphic to the proposed algorithm, fully\ndistributed 3D network of mesh interconnected processing elements or cells with\na coordinate-free, data-driven local processing activity, which is independent\nof problem size; (4) an elastic sparse outer-product (ESOP) method that avoids\nunnecessary computing and communication operations with zero-valued operands,\nthereby enhancing energy efficiency, computational accuracy, and stability.\nTriADA is capable of performing a variety of trilinear transformations with\nhypercubic arithmetic complexity in a linear number of time-steps. The\nmassively parallel, scalable, and energy-efficient architecture of TriADA is\nideal for accelerating multilinear tensor operations, which are the most\ndemanding parts of AI and HPC workloads.", "comment": "19 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2506.22818v1", "categories": ["cs.DC", "cs.AI", "cs.AR", "cs.ET", "eess.SP", "C.1.4; C.3; F.2.1; G.1.3; G.4"], "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.22818v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.24056", "title": "Logit-Gap Steering: Efficient Short-Suffix Jailbreaks for Aligned Large Language Models", "authors": ["Tung-Ling Li", "Hongliang Liu"], "summary": "We introduce logit-gap steering, a fast jailbreak framework that casts the\nrefusal-affirmation gap of RLHF-aligned language models as a single pass over\nthe vocabulary. A forward-computable score blends gap reduction with\nlightweight proxies for KL penalty and reward shift, allowing a \"sort-sum-stop\"\nsweep to complete in under a second and return a short suffix--two orders of\nmagnitude fewer model calls than beam or gradient attacks. The same suffix\ngeneralises to unseen prompts and scales from 0.5 B to 70 B checkpoints,\nlifting one-shot attack success from baseline levels to 80-100% while\npreserving topical coherence. Beyond efficiency, these suffixes expose\nsentence-boundary reward cliffs and other alignment artefacts, offering a\nlightweight probe into how safety tuning reshapes internal representations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.24056v1", "categories": ["cs.CR", "cs.CL", "cs.LG"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.24056v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23292", "title": "DDL: A Dataset for Interpretable Deepfake Detection and Localization in Real-World Scenarios", "authors": ["Changtao Miao", "Yi Zhang", "Weize Gao", "Man Luo", "Weiwei Feng", "Zhiya Tan", "Jianshu Li", "Ajian Liu", "Yunfeng Diao", "Qi Chu", "Tao Gong", "Zhe Li", "Weibin Yao", "Joey Tianyi Zhou"], "summary": "Recent advances in AIGC have exacerbated the misuse of malicious deepfake\ncontent, making the development of reliable deepfake detection methods an\nessential means to address this challenge. Although existing deepfake detection\nmodels demonstrate outstanding performance in detection metrics, most methods\nonly provide simple binary classification results, lacking interpretability. In\ncritical domains such as law, interpretability is crucial for enhancing the\ncredibility and authority of decisions. Recent studies attempt to improve the\ninterpretability of classification results by providing spatial manipulation\nmasks or temporal forgery segments. However, the practical effectiveness of\nthese methods remains suboptimal due to limitations of the forgery data. Most\ncurrent deepfake datasets predominantly offer binary labels, only a few\ndatasets with localization annotations. However, they suffer from restricted\nforgery scenarios, limited diversity in deepfake types, and insufficient data\nscale, making them inadequate for complex real-world scenarios. To address this\npredicament, we construct a novel large-scale deepfake detection and\nlocalization ($\\textbf{DDL}$) dataset containing over $\\textbf{1.8M}$ forged\nsamples and encompassing up to $\\textbf{75}$ distinct deepfake methods. The DDL\ndesign incorporates four key innovations: (1) $\\textbf{Diverse Forgery\nScenarios}$, (2) $\\textbf{Comprehensive Deepfake Methods}$, (3) $\\textbf{Varied\nManipulation Modes}$, and (4) $\\textbf{Fine-grained Forgery Annotations}$.\nThrough these improvements, our DDL not only provides a more challenging\nbenchmark for complex real-world forgeries, but also offers crucial support for\nbuilding next-generation deepfake detection, localization, and interpretability\nmethods. The DDL dataset project page is on\nhttps://deepfake-workshop-ijcai2025.github.io/main/index.html.", "comment": "This paper is a preliminary version, with an extended and\n  comprehensive version currently under development", "pdf_url": "http://arxiv.org/pdf/2506.23292v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23292v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.22450", "title": "Arnoldi Singular Vector perturbations for machine learning weather prediction", "authors": ["Jens Winkler", "Michael Denhard"], "summary": "Since weather forecasts are fundamentally uncertain, reliable decision making\nrequires information on the likelihoods of future weather scenarios. We explore\nthe sensitivity of machine learning weather prediction (MLWP) using the 24h\nPangu Weather ML model of Huawei to errors in the initial conditions with a\nspecific kind of Singular Vector (SV) perturbations. Our Arnoldi-SV (A-SV)\nmethod does not need linear nor adjoint model versions and is applicable to\nnumerical weather prediction (NWP) as well as MLWP. It observes error growth\nwithin a given optimization time window by iteratively applying a forecast\nmodel to perturbed model states. This creates a Krylov subspace, implicitly\nbased on a matrix operator, which approximates the local error growth. Each\niteration adds new dimensions to the Krylov space and its leading right SVs are\nexpected to turn into directions of growing errors. We show that A-SV indeed\nfinds dynamically meaningful perturbation patterns for the 24h Pangu Weather\nmodel, which grow right from the beginning of the forecast rollout. These\nperturbations describe local unstable modes and could be a basis to initialize\nMLWP ensembles. Since we start A-SV from random noise perturbations, the\nalgorithm transforms noise into perturbations conditioned on a given reference\nstate - a process that is akin to the denoising process of the generic\ndiffusion based ML model of GenCast, therefor we briefly discuss similarities\nand differences.", "comment": "dynamical systems, atmospheric physics, machine learing weather\n  prediction, forecast uncertainity, 42 pages with 29 figures (inkl. appendix)", "pdf_url": "http://arxiv.org/pdf/2506.22450v1", "categories": ["physics.ao-ph", "cs.LG"], "cate": "physics.ao-ph", "url": "http://arxiv.org/abs/2506.22450v1", "date": "2025-06-13", "updated": "2025-06-13"}
{"id": "2506.22832", "title": "Listener-Rewarded Thinking in VLMs for Image Preferences", "authors": ["Alexander Gambashidze", "Li Pengyi", "Matvey Skripkin", "Andrey Galichin", "Anton Gusarov", "Konstantin Sobolev", "Andrey Kuznetsov", "Ivan Oseledets"], "summary": "Training robust and generalizable reward models for human visual preferences\nis essential for aligning text-to-image and text-to-video generative models\nwith human intent. However, current reward models often fail to generalize, and\nsupervised fine-tuning leads to memorization, demanding complex annotation\npipelines. While reinforcement learning (RL), specifically Group Relative\nPolicy Optimization (GRPO), improves generalization, we uncover a key failure\nmode: a significant drop in reasoning accuracy occurs when a model's reasoning\ntrace contradicts that of an independent, frozen vision-language model\n(\"listener\") evaluating the same output. To address this, we introduce a\nlistener-augmented GRPO framework. Here, the listener re-evaluates the\nreasoner's chain-of-thought to provide a dense, calibrated confidence score,\nshaping the RL reward signal. This encourages the reasoner not only to answer\ncorrectly, but to produce explanations that are persuasive to an independent\nmodel. Our listener-shaped reward scheme achieves best accuracy on the\nImageReward benchmark (67.4%), significantly improves out-of-distribution (OOD)\nperformance on a large-scale human preference dataset (1.2M votes, up to +6%\nover naive reasoner), and reduces reasoning contradictions compared to strong\nGRPO and SFT baselines. These results demonstrate that listener-based rewards\nprovide a scalable, data-efficient path to aligning vision-language models with\nnuanced human preferences. We will release our reasoning model here:\nhttps://huggingface.co/alexgambashidze/qwen2.5vl_image_preference_reasoner.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22832v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22832v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.24086", "title": "MotionGPT3: Human Motion as a Second Modality", "authors": ["Bingfan Zhu", "Biao Jiang", "Sunyi Wang", "Shixiang Tang", "Tao Chen", "Linjie Luo", "Youyi Zheng", "Xin Chen"], "summary": "Though recent advances in multimodal models have demonstrated strong\ncapabilities and opportunities in unified understanding and generation, the\ndevelopment of unified motion-language models remains underexplored. To enable\nsuch models with high-fidelity human motion, two core challenges must be\naddressed. The first is the reconstruction gap between the continuous motion\nmodality and discrete representation in an autoregressive manner, and the\nsecond is the degradation of language intelligence during unified training.\nInspired by the mixture of experts, we propose MotionGPT3, a bimodal\nmotion-language model that treats human motion as a second modality, decoupling\nmotion modeling via separate model parameters and enabling both effective\ncross-modal interaction and efficient multimodal scaling training. To preserve\nlanguage intelligence, the text branch retains the original structure and\nparameters of the pretrained language model, while a new motion branch is\nintegrated via a shared attention mechanism, enabling bidirectional information\nflow between two modalities. We first employ a motion Variational Autoencoder\n(VAE) to encode raw human motion into latent representations. Based on this\ncontinuous latent space, the motion branch predicts motion latents directly\nfrom intermediate hidden states using a diffusion head, bypassing discrete\ntokenization. Extensive experiments show that our approach achieves competitive\nperformance on both motion understanding and generation tasks while preserving\nstrong language capabilities, establishing a unified bimodal motion diffusion\nframework within an autoregressive manner.", "comment": "21 pages, 8 figures", "pdf_url": "http://arxiv.org/pdf/2506.24086v1", "categories": ["cs.CV", "cs.CL"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.24086v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23295", "title": "DiffFit: Disentangled Garment Warping and Texture Refinement for Virtual Try-On", "authors": ["Xiang Xu"], "summary": "Virtual try-on (VTON) aims to synthesize realistic images of a person wearing\na target garment, with broad applications in e-commerce and digital fashion.\nWhile recent advances in latent diffusion models have substantially improved\nvisual quality, existing approaches still struggle with preserving fine-grained\ngarment details, achieving precise garment-body alignment, maintaining\ninference efficiency, and generalizing to diverse poses and clothing styles. To\naddress these challenges, we propose DiffFit, a novel two-stage latent\ndiffusion framework for high-fidelity virtual try-on. DiffFit adopts a\nprogressive generation strategy: the first stage performs geometry-aware\ngarment warping, aligning the garment with the target body through fine-grained\ndeformation and pose adaptation. The second stage refines texture fidelity via\na cross-modal conditional diffusion model that integrates the warped garment,\nthe original garment appearance, and the target person image for high-quality\nrendering. By decoupling geometric alignment and appearance refinement, DiffFit\neffectively reduces task complexity and enhances both generation stability and\nvisual realism. It excels in preserving garment-specific attributes such as\ntextures, wrinkles, and lighting, while ensuring accurate alignment with the\nhuman body. Extensive experiments on large-scale VTON benchmarks demonstrate\nthat DiffFit achieves superior performance over existing state-of-the-art\nmethods in both quantitative metrics and perceptual evaluations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23295v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23295v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.22454", "title": "Microelectrode Signal Dynamics as Biomarkers of Subthalamic Nucleus Entry on Deep Brain Stimulation: A Nonlinear Feature Approach", "authors": ["Ana Luiza S. Tavares", "Artur Pedro M. Neto", "Francinaldo L. Gomes", "Paul Rodrigo dos Reis", "Arthur G. da Silva", "Antonio P. Junior", "Bruno D. Gomes"], "summary": "Accurate intraoperative localization of the subthalamic nucleus (STN) is\nessential for the efficacy of Deep Brain Stimulation (DBS) in patients with\nParkinson's disease. While microelectrode recordings (MERs) provide rich\nelectrophysiological information during DBS electrode implantation, current\nlocalization practices often rely on subjective interpretation of signal\nfeatures. In this study, we propose a quantitative framework that leverages\nnonlinear dynamics and entropy-based metrics to classify neural activity\nrecorded inside versus outside the STN. MER data from three patients were\npreprocessed using a robust artifact correction pipeline, segmented, and\nlabelled based on surgical annotations. A comprehensive set of recurrence\nquantification analysis, nonlinear, and entropy features were extracted from\neach segment. Multiple supervised classifiers were trained on every combination\nof feature domains using stratified 10-fold cross-validation, followed by\nstatistical comparison using paired Wilcoxon signed-rank tests with\nHolm-Bonferroni correction. The combination of entropy and nonlinear features\nyielded the highest discriminative power, and the Extra Trees classifier\nemerged as the best model with a cross-validated F1-score of 0.902+/-0.027 and\nROC AUC of 0.887+/-0.055. Final evaluation on a 20% hold-out test set confirmed\nrobust generalization (F1= 0.922, ROC AUC = 0.941). These results highlight the\npotential of nonlinear and entropy signal descriptors in supporting real-time,\ndata-driven decision-making during DBS surgeries", "comment": "8 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2506.22454v1", "categories": ["eess.SP", "cs.LG"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.22454v1", "date": "2025-06-14", "updated": "2025-06-14"}
{"id": "2506.22837", "title": "xLSTMAD: A Powerful xLSTM-based Method for Anomaly Detection", "authors": ["Kamil Faber", "Marcin Pietro≈Ñ", "Dominik ≈ªurek", "Roberto Corizzo"], "summary": "The recently proposed xLSTM is a powerful model that leverages expressive\nmultiplicative gating and residual connections, providing the temporal capacity\nneeded for long-horizon forecasting and representation learning. This\narchitecture has demonstrated success in time series forecasting, lossless\ncompression, and even large-scale language modeling tasks, where its linear\nmemory footprint and fast inference make it a viable alternative to\nTransformers. Despite its growing popularity, no prior work has explored xLSTM\nfor anomaly detection. In this work, we fill this gap by proposing xLSTMAD, the\nfirst anomaly detection method that integrates a full encoder-decoder xLSTM\narchitecture, purpose-built for multivariate time series data. Our encoder\nprocesses input sequences to capture historical context, while the decoder is\ndevised in two separate variants of the method. In the forecasting approach,\nthe decoder iteratively generates forecasted future values xLSTMAD-F, while the\nreconstruction approach reconstructs the input time series from its encoded\ncounterpart xLSTMAD-R. We investigate the performance of two loss functions:\nMean Squared Error (MSE), and Soft Dynamic Time Warping (SoftDTW) to consider\nlocal reconstruction fidelity and global sequence alignment, respectively. We\nevaluate our method on the comprehensive TSB-AD-M benchmark, which spans 17\nreal-world datasets, using state-of-the-art challenging metrics such as VUS-PR.\nIn our results, xLSTM showcases state-of-the-art accuracy, outperforming 23\npopular anomaly detection baselines. Our paper is the first work revealing the\npowerful modeling capabilities of xLSTM for anomaly detection, paving the way\nfor exciting new developments on this subject. Our code is available at:\nhttps://github.com/Nyderx/xlstmad", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22837v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22837v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.24119", "title": "SPIRAL: Self-Play on Zero-Sum Games Incentivizes Reasoning via Multi-Agent Multi-Turn Reinforcement Learning", "authors": ["Bo Liu", "Leon Guertler", "Simon Yu", "Zichen Liu", "Penghui Qi", "Daniel Balcells", "Mickel Liu", "Cheston Tan", "Weiyan Shi", "Min Lin", "Wee Sun Lee", "Natasha Jaques"], "summary": "Recent advances in reinforcement learning have shown that language models can\ndevelop sophisticated reasoning through training on tasks with verifiable\nrewards, but these approaches depend on human-curated problem-answer pairs and\ndomain-specific reward engineering. We introduce SPIRAL, a self-play framework\nwhere models learn by playing multi-turn, zero-sum games against continuously\nimproving versions of themselves, eliminating the need for human supervision.\nThrough self-play, SPIRAL generates an infinite curriculum of progressively\nchallenging problems as models must constantly adapt to stronger opponents. To\nenable this self-play training at scale, We implement a fully online,\nmulti-turn, multi-agent reinforcement learning system for LLMs and propose\nrole-conditioned advantage estimation (RAE) to stabilize multi-agent training.\nUsing SPIRAL, self-play on zero-sum games produces reasoning capabilities that\ntransfer broadly. Training Qwen3-4B-Base on Kuhn Poker alone achieves 8.6%\nimprovement on math and 8.4% on general reasoning, outperforming SFT on 25,000\nexpert game trajectories. Analysis reveals that this transfer occurs through\nthree cognitive patterns: systematic decomposition, expected value calculation,\nand case-by-case analysis. Multi-game training (TicTacToe, Kuhn Poker, Simple\nNegotiation) further enhances performance as each game develops distinct\nreasoning strengths. Applying SPIRAL to a strong reasoning model\n(DeepSeek-R1-Distill-Qwen-7B) can still lead to 2.0% average improvement. These\nresults demonstrate that zero-sum games naturally develop transferable\nreasoning capabilities, highlighting a promising direction for autonomous\nreasoning development.", "comment": "Work in Progress", "pdf_url": "http://arxiv.org/pdf/2506.24119v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.24119v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23308", "title": "Endo-4DGX: Robust Endoscopic Scene Reconstruction and Illumination Correction with Gaussian Splatting", "authors": ["Yiming Huang", "Long Bai", "Beilei Cui", "Yanheng Li", "Tong Chen", "Jie Wang", "Jinlin Wu", "Zhen Lei", "Hongbin Liu", "Hongliang Ren"], "summary": "Accurate reconstruction of soft tissue is crucial for advancing automation in\nimage-guided robotic surgery. The recent 3D Gaussian Splatting (3DGS)\ntechniques and their variants, 4DGS, achieve high-quality renderings of dynamic\nsurgical scenes in real-time. However, 3D-GS-based methods still struggle in\nscenarios with varying illumination, such as low light and over-exposure.\nTraining 3D-GS in such extreme light conditions leads to severe optimization\nproblems and devastating rendering quality. To address these challenges, we\npresent Endo-4DGX, a novel reconstruction method with illumination-adaptive\nGaussian Splatting designed specifically for endoscopic scenes with uneven\nlighting. By incorporating illumination embeddings, our method effectively\nmodels view-dependent brightness variations. We introduce a region-aware\nenhancement module to model the sub-area lightness at the Gaussian level and a\nspatial-aware adjustment module to learn the view-consistent brightness\nadjustment. With the illumination adaptive design, Endo-4DGX achieves superior\nrendering performance under both low-light and over-exposure conditions while\nmaintaining geometric accuracy. Additionally, we employ an exposure control\nloss to restore the appearance from adverse exposure to the normal level for\nillumination-adaptive optimization. Experimental results demonstrate that\nEndo-4DGX significantly outperforms combinations of state-of-the-art\nreconstruction and restoration methods in challenging lighting environments,\nunderscoring its potential to advance robot-assisted surgical applications. Our\ncode is available at https://github.com/lastbasket/Endo-4DGX.", "comment": "MICCAI 2025. Project Page:\n  https://lastbasket.github.io/MICCAI-2025-Endo-4DGX/", "pdf_url": "http://arxiv.org/pdf/2506.23308v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23308v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.22455", "title": "Data Normalization Strategies for EEG Deep Learning", "authors": ["Dung Truong", "Arnaud Delorme"], "summary": "Normalization is a critical yet often overlooked component in the\npreprocessing pipeline for EEG deep learning applications. The rise of\nlarge-scale pretraining paradigms such as self-supervised learning (SSL)\nintroduces a new set of tasks whose nature is substantially different from\nsupervised training common in EEG deep learning applications. This raises new\nquestions about optimal normalization strategies for the applicable task. In\nthis study, we systematically evaluate the impact of normalization granularity\n(recording vs. window level) and scope (cross-channel vs. within-channel) on\nboth supervised (age and gender prediction) and self-supervised (Contrastive\nPredictive Coding) tasks. Using high-density resting-state EEG from 2,836\nsubjects in the Healthy Brain Network dataset, we show that optimal\nnormalization strategies differ significantly between training paradigms.\nWindow-level within-channel normalization yields the best performance in\nsupervised tasks, while minimal or cross-channel normalization at the window\nlevel is more effective for SSL. These results underscore the necessity of\ntask-specific normalization choices and challenge the assumption that a\nuniversal normalization strategy can generalize across learning settings. Our\nfindings provide practical insights for developing robust EEG deep learning\npipelines as the field shifts toward large-scale, foundation model training.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22455v1", "categories": ["eess.SP", "cs.LG"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.22455v1", "date": "2025-06-15", "updated": "2025-06-15"}
{"id": "2506.22845", "title": "Quantum Neural Networks for Wind Energy Forecasting: A Comparative Study of Performance and Scalability with Classical Models", "authors": ["Batuhan Hangun", "Oguz Altun", "Onder Eyecioglu"], "summary": "Quantum Neural Networks (QNNs), a prominent approach in Quantum Machine\nLearning (QML), are emerging as a powerful alternative to classical machine\nlearning methods. Recent studies have focused on the applicability of QNNs to\nvarious tasks, such as time-series forecasting, prediction, and classification,\nacross a wide range of applications, including cybersecurity and medical\nimaging. With the increased use of smart grids driven by the integration of\nrenewable energy systems, machine learning plays an important role in\npredicting power demand and detecting system disturbances. This study provides\nan in-depth investigation of QNNs for predicting the power output of a wind\nturbine. We assess the predictive performance and simulation time of six QNN\nconfigurations that are based on the Z Feature Map for data encoding and\nvarying ansatz structures. Through detailed cross-validation experiments and\ntests on an unseen hold-out dataset, we experimentally demonstrate that QNNs\ncan achieve predictive performance that is competitive with, and in some cases\nmarginally better than, the benchmarked classical approaches. Our results also\nreveal the effects of dataset size and circuit complexity on predictive\nperformance and simulation time. We believe our findings will offer valuable\ninsights for researchers in the energy domain who wish to incorporate quantum\nmachine learning into their work.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22845v1", "categories": ["cs.LG", "cs.AI", "cs.PF"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22845v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23323", "title": "FastSeg: Efficient Training-Free Open-Vocabulary Segmentation via Hierarchical Attention Refinement Method", "authors": ["Quang-Huy Che", "Vinh-Tiep Nguyen"], "summary": "Open-vocabulary semantic segmentation (OVSS) aims to segment objects from\narbitrary text categories without requiring densely annotated datasets.\nAlthough contrastive learning based models enable zero-shot segmentation, they\noften lose fine spatial precision at pixel level, due to global representation\nbias. In contrast, diffusion-based models naturally encode fine-grained spatial\nfeatures via attention mechanisms that capture both global context and local\ndetails. However, they often face challenges in balancing the number of\niterations with the quality of the segmentation. In this work, we propose\nFastSeg, a novel and efficient training-free framework with only (1+1)-step of\nreverse process of a pretrained diffusion model (e.g., Stable Diffusion).\nMoreover, instead of running multiple times for different classes, FastSeg\nperforms segmentation for all classes at once. To further enhance the\nsegmentation quality, FastSeg introduces three key components: (i) a\ndual-prompt mechanism for discriminative, class-aware attention extraction,\n(ii) a Hierarchical Attention Refinement Method (HARD) that enhances fused\ncross-attention using scale-aligned selfattention maps, and (iii) a Test-Time\nFlipping (TTF) scheme designed to improve spatial consistency. Extensive\nexperiments show that FastSeg achieves state-of-the-art training-free\nperformance, obtaining 43.8% average mIoU across PASCAL VOC, PASCAL Context,\nand COCO Object benchmarks while maintaining superior inference efficiency. Our\nresults demonstrate that FastSeg provides a strong foundation for\nextendability, bridging the gap between segmentation quality and inference\nefficiency.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23323v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23323v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.22459", "title": "Physics-Embedded Neural Networks for sEMG-based Continuous Motion Estimation", "authors": ["Wending Heng", "Chaoyuan Liang", "Yihui Zhao", "Zhiqiang Zhang", "Glen Cooper", "Zhenhong Li"], "summary": "Accurately decoding human motion intentions from surface electromyography\n(sEMG) is essential for myoelectric control and has wide applications in\nrehabilitation robotics and assistive technologies. However, existing\nsEMG-based motion estimation methods often rely on subject-specific\nmusculoskeletal (MSK) models that are difficult to calibrate, or purely\ndata-driven models that lack physiological consistency. This paper introduces a\nnovel Physics-Embedded Neural Network (PENN) that combines interpretable MSK\nforward-dynamics with data-driven residual learning, thereby preserving\nphysiological consistency while achieving accurate motion estimation. The PENN\nemploys a recursive temporal structure to propagate historical estimates and a\nlightweight convolutional neural network for residual correction, leading to\nrobust and temporally coherent estimations. A two-phase training strategy is\ndesigned for PENN. Experimental evaluations on six healthy subjects show that\nPENN outperforms state-of-the-art baseline methods in both root mean square\nerror (RMSE) and $R^2$ metrics.", "comment": "Accepted by 2025 IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS)", "pdf_url": "http://arxiv.org/pdf/2506.22459v1", "categories": ["eess.SP", "cs.LG"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.22459v1", "date": "2025-06-17", "updated": "2025-06-17"}
{"id": "2506.22848", "title": "Scalable Structure Learning of Bayesian Networks by Learning Algorithm Ensembles", "authors": ["Shengcai Liu", "Hui Ou-yang", "Zhiyuan Wang", "Cheng Chen", "Qijun Cai", "Yew-Soon Ong", "Ke Tang"], "summary": "Learning the structure of Bayesian networks (BNs) from data is challenging,\nespecially for datasets involving a large number of variables. The recently\nproposed divide-and-conquer (D\\&D) strategies present a promising approach for\nlearning large BNs. However, they still face a main issue of unstable learning\naccuracy across subproblems. In this work, we introduce the idea of employing\nstructure learning ensemble (SLE), which combines multiple BN structure\nlearning algorithms, to consistently achieve high learning accuracy. We further\npropose an automatic approach called Auto-SLE for learning near-optimal SLEs,\naddressing the challenge of manually designing high-quality SLEs. The learned\nSLE is then integrated into a D\\&D method. Extensive experiments firmly show\nthe superiority of our method over D\\&D methods with single BN structure\nlearning algorithm in learning large BNs, achieving accuracy improvement\nusually by 30\\%$\\sim$225\\% on datasets involving 10,000 variables. Furthermore,\nour method generalizes well to datasets with many more (e.g., 30000) variables\nand different network characteristics than those present in the training data\nfor learning the SLE. These results indicate the significant potential of\nemploying (automatic learning of) SLEs for scalable BN structure learning.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22848v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22848v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23329", "title": "IR3D-Bench: Evaluating Vision-Language Model Scene Understanding as Agentic Inverse Rendering", "authors": ["Parker Liu", "Chenxin Li", "Zhengxin Li", "Yipeng Wu", "Wuyang Li", "Zhiqin Yang", "Zhenyuan Zhang", "Yunlong Lin", "Sirui Han", "Brandon Y. Feng"], "summary": "Vision-language models (VLMs) excel at descriptive tasks, but whether they\ntruly understand scenes from visual observations remains uncertain. We\nintroduce IR3D-Bench, a benchmark challenging VLMs to demonstrate understanding\nthrough active creation rather than passive recognition. Grounded in the\nanalysis-by-synthesis paradigm, IR3D-Bench tasks Vision-Language Agents (VLAs)\nwith actively using programming and rendering tools to recreate the underlying\n3D structure of an input image, achieving agentic inverse rendering through\ntool use. This \"understanding-by-creating\" approach probes the tool-using\ngenerative capacity of VLAs, moving beyond the descriptive or conversational\ncapacity measured by traditional scene understanding benchmarks. We provide a\ncomprehensive suite of metrics to evaluate geometric accuracy, spatial\nrelations, appearance attributes, and overall plausibility. Initial experiments\non agentic inverse rendering powered by various state-of-the-art VLMs highlight\ncurrent limitations, particularly in visual precision rather than basic tool\nusage. IR3D-Bench, including data and evaluation protocols, is released to\nfacilitate systematic study and development of tool-using VLAs towards genuine\nscene understanding by creating.", "comment": "Project Page: https://ir3d-bench.github.io/", "pdf_url": "http://arxiv.org/pdf/2506.23329v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23329v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.22463", "title": "Modulated Diffusion: Accelerating Generative Modeling with Modulated Quantization", "authors": ["Weizhi Gao", "Zhichao Hou", "Junqi Yin", "Feiyi Wang", "Linyu Peng", "Xiaorui Liu"], "summary": "Diffusion models have emerged as powerful generative models, but their high\ncomputation cost in iterative sampling remains a significant bottleneck. In\nthis work, we present an in-depth and insightful study of state-of-the-art\nacceleration techniques for diffusion models, including caching and\nquantization, revealing their limitations in computation error and generation\nquality. To break these limits, this work introduces Modulated Diffusion\n(MoDiff), an innovative, rigorous, and principled framework that accelerates\ngenerative modeling through modulated quantization and error compensation.\nMoDiff not only inherents the advantages of existing caching and quantization\nmethods but also serves as a general framework to accelerate all diffusion\nmodels. The advantages of MoDiff are supported by solid theoretical insight and\nanalysis. In addition, extensive experiments on CIFAR-10 and LSUN demonstrate\nthat MoDiff significant reduces activation quantization from 8 bits to 3 bits\nwithout performance degradation in post-training quantization (PTQ). Our code\nimplementation is available at https://github.com/WeizhiGao/MoDiff.", "comment": "26 pages, accepted by ICML 2025", "pdf_url": "http://arxiv.org/pdf/2506.22463v1", "categories": ["cs.CV", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22463v1", "date": "2025-06-18", "updated": "2025-06-18"}
{"id": "2506.22853", "title": "DICE-BENCH: Evaluating the Tool-Use Capabilities of Large Language Models in Multi-Round, Multi-Party Dialogues", "authors": ["Kyochul Jang", "Donghyeon Lee", "Kyusik Kim", "Dongseok Heo", "Taewhoo Lee", "Woojeong Kim", "Bongwon Suh"], "summary": "Existing function-calling benchmarks focus on single-turn interactions.\nHowever, they overlook the complexity of real-world scenarios. To quantify how\nexisting benchmarks address practical applications, we introduce DICE-SCORE, a\nmetric that evaluates the dispersion of tool-related information such as\nfunction name and parameter values throughout the dialogue. Analyzing existing\nbenchmarks through DICE-SCORE reveals notably low scores, highlighting the need\nfor more realistic scenarios. To address this gap, we present DICE-BENCH, a\nframework that constructs practical function-calling datasets by synthesizing\nconversations through a tool graph that maintains dependencies across rounds\nand a multi-agent system with distinct personas to enhance dialogue\nnaturalness. The final dataset comprises 1,607 high-DICE-SCORE instances. Our\nexperiments on 19 LLMs with DICE-BENCH show that significant advances are still\nrequired before such models can be deployed effectively in real-world settings.\nOur code and data are all publicly available:\nhttps://snuhcc.github.io/DICE-Bench/.", "comment": "9 pages, ACL 2025 Vienna", "pdf_url": "http://arxiv.org/pdf/2506.22853v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.22853v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23347", "title": "CycleVAR: Repurposing Autoregressive Model for Unsupervised One-Step Image Translation", "authors": ["Yi Liu", "Shengqian Li", "Zuzeng Lin", "Feng Wang", "Si Liu"], "summary": "The current conditional autoregressive image generation methods have shown\npromising results, yet their potential remains largely unexplored in the\npractical unsupervised image translation domain, which operates without\nexplicit cross-domain correspondences. A critical limitation stems from the\ndiscrete quantization inherent in traditional Vector Quantization-based\nframeworks, which disrupts gradient flow between the Variational Autoencoder\ndecoder and causal Transformer, impeding end-to-end optimization during\nadversarial training in image space. To tackle this issue, we propose using\nSoftmax Relaxed Quantization, a novel approach that reformulates codebook\nselection as a continuous probability mixing process via Softmax, thereby\npreserving gradient propagation. Building upon this differentiable foundation,\nwe introduce CycleVAR, which reformulates image-to-image translation as\nimage-conditional visual autoregressive generation by injecting multi-scale\nsource image tokens as contextual prompts, analogous to prefix-based\nconditioning in language models. CycleVAR exploits two modes to generate the\ntarget image tokens, including (1) serial multi-step generation, enabling\niterative refinement across scales, and (2) parallel one-step generation\nsynthesizing all resolution outputs in a single forward pass. Experimental\nfindings indicate that the parallel one-step generation mode attains superior\ntranslation quality with quicker inference speed than the serial multi-step\nmode in unsupervised scenarios. Furthermore, both quantitative and qualitative\nresults indicate that CycleVAR surpasses previous state-of-the-art unsupervised\nimage translation models, \\textit{e}.\\textit{g}., CycleGAN-Turbo.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23347v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23347v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.22476", "title": "An Interpretable Transformer-Based Foundation Model for Cross-Procedural Skill Assessment Using Raw fNIRS Signals", "authors": ["A. Subedi", "S. De", "L. Cavuoto", "S. Schwaitzberg", "M. Hackett", "J. Norfleet"], "summary": "Objective skill assessment in high-stakes procedural environments requires\nmodels that not only decode underlying cognitive and motor processes but also\ngeneralize across tasks, individuals, and experimental contexts. While prior\nwork has demonstrated the potential of functional near-infrared spectroscopy\n(fNIRS) for evaluating cognitive-motor performance, existing approaches are\noften task-specific, rely on extensive preprocessing, and lack robustness to\nnew procedures or conditions. Here, we introduce an interpretable\ntransformer-based foundation model trained on minimally processed fNIRS signals\nfor cross-procedural skill assessment. Pretrained using self-supervised\nlearning on data from laparoscopic surgical tasks and endotracheal intubation\n(ETI), the model achieves greater than 88% classification accuracy on all\ntasks, with Matthews Correlation Coefficient exceeding 0.91 on ETI. It\ngeneralizes to a novel emergency airway procedure--cricothyrotomy--using fewer\nthan 30 labeled samples and a lightweight (less than 2k parameter) adapter\nmodule, attaining an AUC greater than 87%. Interpretability is achieved via a\nnovel channel attention mechanism--developed specifically for fNIRS--that\nidentifies functionally coherent prefrontal sub-networks validated through\nablation studies. Temporal attention patterns align with task-critical phases\nand capture stress-induced changes in neural variability, offering insight into\ndynamic cognitive states.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22476v1", "categories": ["eess.SP", "cs.ET", "cs.HC", "cs.LG", "q-bio.NC", "I.2.6; J.3; H.1.2"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.22476v1", "date": "2025-06-21", "updated": "2025-06-21"}
{"id": "2506.22864", "title": "Mask-aware Text-to-Image Retrieval: Referring Expression Segmentation Meets Cross-modal Retrieval", "authors": ["Li-Cheng Shen", "Jih-Kang Hsieh", "Wei-Hua Li", "Chu-Song Chen"], "summary": "Text-to-image retrieval (TIR) aims to find relevant images based on a textual\nquery, but existing approaches are primarily based on whole-image captions and\nlack interpretability. Meanwhile, referring expression segmentation (RES)\nenables precise object localization based on natural language descriptions but\nis computationally expensive when applied across large image collections. To\nbridge this gap, we introduce Mask-aware TIR (MaTIR), a new task that unifies\nTIR and RES, requiring both efficient image search and accurate object\nsegmentation. To address this task, we propose a two-stage framework,\ncomprising a first stage for segmentation-aware image retrieval and a second\nstage for reranking and object grounding with a multimodal large language model\n(MLLM). We leverage SAM 2 to generate object masks and Alpha-CLIP to extract\nregion-level embeddings offline at first, enabling effective and scalable\nonline retrieval. Secondly, MLLM is used to refine retrieval rankings and\ngenerate bounding boxes, which are matched to segmentation masks. We evaluate\nour approach on COCO and D$^3$ datasets, demonstrating significant improvements\nin both retrieval accuracy and segmentation quality over previous methods.", "comment": "ICMR 2025", "pdf_url": "http://arxiv.org/pdf/2506.22864v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22864v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23352", "title": "GeoProg3D: Compositional Visual Reasoning for City-Scale 3D Language Fields", "authors": ["Shunsuke Yasuki", "Taiki Miyanishi", "Nakamasa Inoue", "Shuhei Kurita", "Koya Sakamoto", "Daichi Azuma", "Masato Taki", "Yutaka Matsuo"], "summary": "The advancement of 3D language fields has enabled intuitive interactions with\n3D scenes via natural language. However, existing approaches are typically\nlimited to small-scale environments, lacking the scalability and compositional\nreasoning capabilities necessary for large, complex urban settings. To overcome\nthese limitations, we propose GeoProg3D, a visual programming framework that\nenables natural language-driven interactions with city-scale high-fidelity 3D\nscenes. GeoProg3D consists of two key components: (i) a Geography-aware\nCity-scale 3D Language Field (GCLF) that leverages a memory-efficient\nhierarchical 3D model to handle large-scale data, integrated with geographic\ninformation for efficiently filtering vast urban spaces using directional cues,\ndistance measurements, elevation data, and landmark references; and (ii)\nGeographical Vision APIs (GV-APIs), specialized geographic vision tools such as\narea segmentation and object detection. Our framework employs large language\nmodels (LLMs) as reasoning engines to dynamically combine GV-APIs and operate\nGCLF, effectively supporting diverse geographic vision tasks. To assess\nperformance in city-scale reasoning, we introduce GeoEval3D, a comprehensive\nbenchmark dataset containing 952 query-answer pairs across five challenging\ntasks: grounding, spatial reasoning, comparison, counting, and measurement.\nExperiments demonstrate that GeoProg3D significantly outperforms existing 3D\nlanguage fields and vision-language models across multiple tasks. To our\nknowledge, GeoProg3D is the first framework enabling compositional geographic\nreasoning in high-fidelity city-scale 3D environments via natural language. The\ncode is available at https://snskysk.github.io/GeoProg3D/.", "comment": "Accepted by ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2506.23352v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23352v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.22479", "title": "Hindsight-Guided Momentum (HGM) Optimizer: An Approach to Adaptive Learning Rate", "authors": ["Krisanu Sarkar"], "summary": "We introduce Hindsight-Guided Momentum (HGM), a first-order optimization\nalgorithm that adaptively scales learning rates based on the directional\nconsistency of recent updates. Traditional adaptive methods, such as Adam or\nRMSprop , adapt learning dynamics using only the magnitude of gradients, often\noverlooking important geometric cues.Geometric cues refer to directional\ninformation, such as the alignment between current gradients and past updates,\nwhich reflects the local curvature and consistency of the optimization path.\nHGM addresses this by incorporating a hindsight mechanism that evaluates the\ncosine similarity between the current gradient and accumulated momentum. This\nallows it to distinguish between coherent and conflicting gradient directions,\nincreasing the learning rate when updates align and reducing it in regions of\noscillation or noise. The result is a more responsive optimizer that\naccelerates convergence in smooth regions of the loss surface while maintaining\nstability in sharper or more erratic areas. Despite this added adaptability,\nthe method preserves the computational and memory efficiency of existing\noptimizers.By more intelligently responding to the structure of the\noptimization landscape, HGM provides a simple yet effective improvement over\nexisting approaches, particularly in non-convex settings like that of deep\nneural network training.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22479v1", "categories": ["math.OC", "cs.AI", "cs.LG"], "cate": "math.OC", "url": "http://arxiv.org/abs/2506.22479v1", "date": "2025-06-22", "updated": "2025-06-22"}
{"id": "2506.22866", "title": "Region-Aware CAM: High-Resolution Weakly-Supervised Defect Segmentation via Salient Region Perception", "authors": ["Hang-Cheng Dong", "Lu Zou", "Bingguo Liu", "Dong Ye", "Guodong Liu"], "summary": "Surface defect detection plays a critical role in industrial quality\ninspection. Recent advances in artificial intelligence have significantly\nenhanced the automation level of detection processes. However, conventional\nsemantic segmentation and object detection models heavily rely on large-scale\nannotated datasets, which conflicts with the practical requirements of defect\ndetection tasks. This paper proposes a novel weakly supervised semantic\nsegmentation framework comprising two key components: a region-aware class\nactivation map (CAM) and pseudo-label training. To address the limitations of\nexisting CAM methods, especially low-resolution thermal maps, and insufficient\ndetail preservation, we introduce filtering-guided backpropagation (FGBP),\nwhich refines target regions by filtering gradient magnitudes to identify areas\nwith higher relevance to defects. Building upon this, we further develop a\nregion-aware weighted module to enhance spatial precision. Finally,\npseudo-label segmentation is implemented to refine the model's performance\niteratively. Comprehensive experiments on industrial defect datasets\ndemonstrate the superiority of our method. The proposed framework effectively\nbridges the gap between weakly supervised learning and high-precision defect\nsegmentation, offering a practical solution for resource-constrained industrial\nscenarios.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22866v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22866v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23353", "title": "Layer Decomposition and Morphological Reconstruction for Task-Oriented Infrared Image Enhancement", "authors": ["Siyuan Chai", "Xiaodong Guo", "Tong Liu"], "summary": "Infrared image helps improve the perception capabilities of autonomous\ndriving in complex weather conditions such as fog, rain, and low light.\nHowever, infrared image often suffers from low contrast, especially in\nnon-heat-emitting targets like bicycles, which significantly affects the\nperformance of downstream high-level vision tasks. Furthermore, achieving\ncontrast enhancement without amplifying noise and losing important information\nremains a challenge. To address these challenges, we propose a task-oriented\ninfrared image enhancement method. Our approach consists of two key components:\nlayer decomposition and saliency information extraction. First, we design an\nlayer decomposition method for infrared images, which enhances scene details\nwhile preserving dark region features, providing more features for subsequent\nsaliency information extraction. Then, we propose a morphological\nreconstruction-based saliency extraction method that effectively extracts and\nenhances target information without amplifying noise. Our method improves the\nimage quality for object detection and semantic segmentation tasks. Extensive\nexperiments demonstrate that our approach outperforms state-of-the-art methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23353v1", "categories": ["cs.CV", "eess.IV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23353v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.22480", "title": "Service Placement in Small Cell Networks Using Distributed Best Arm Identification in Linear Bandits", "authors": ["Mariam Yahya", "Aydin Sezgin", "Setareh Maghsudi"], "summary": "As users in small cell networks increasingly rely on computation-intensive\nservices, cloud-based access often results in high latency. Multi-access edge\ncomputing (MEC) mitigates this by bringing computational resources closer to\nend users, with small base stations (SBSs) serving as edge servers to enable\nlow-latency service delivery. However, limited edge capacity makes it\nchallenging to decide which services to deploy locally versus in the cloud,\nespecially under unknown service demand and dynamic network conditions. To\ntackle this problem, we model service demand as a linear function of service\nattributes and formulate the service placement task as a linear bandit problem,\nwhere SBSs act as agents and services as arms. The goal is to identify the\nservice that, when placed at the edge, offers the greatest reduction in total\nuser delay compared to cloud deployment. We propose a distributed and adaptive\nmulti-agent best-arm identification (BAI) algorithm under a fixed-confidence\nsetting, where SBSs collaborate to accelerate learning. Simulations show that\nour algorithm identifies the optimal service with the desired confidence and\nachieves near-optimal speedup, as the number of learning rounds decreases\nproportionally with the number of SBSs. We also provide theoretical analysis of\nthe algorithm's sample complexity and communication overhead.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22480v1", "categories": ["cs.NI", "cs.DC", "cs.LG"], "cate": "cs.NI", "url": "http://arxiv.org/abs/2506.22480v1", "date": "2025-06-22", "updated": "2025-06-22"}
{"id": "2506.22868", "title": "STR-Match: Matching SpatioTemporal Relevance Score for Training-Free Video Editing", "authors": ["Junsung Lee", "Junoh Kang", "Bohyung Han"], "summary": "Previous text-guided video editing methods often suffer from temporal\ninconsistency, motion distortion, and-most notably-limited domain\ntransformation. We attribute these limitations to insufficient modeling of\nspatiotemporal pixel relevance during the editing process. To address this, we\npropose STR-Match, a training-free video editing algorithm that produces\nvisually appealing and spatiotemporally coherent videos through latent\noptimization guided by our novel STR score. The score captures spatiotemporal\npixel relevance across adjacent frames by leveraging 2D spatial attention and\n1D temporal modules in text-to-video (T2V) diffusion models, without the\noverhead of computationally expensive 3D attention mechanisms. Integrated into\na latent optimization framework with a latent mask, STR-Match generates\ntemporally consistent and visually faithful videos, maintaining strong\nperformance even under significant domain transformations while preserving key\nvisual attributes of the source. Extensive experiments demonstrate that\nSTR-Match consistently outperforms existing methods in both visual quality and\nspatiotemporal consistency.", "comment": "15 pages, 9 figures, 3 tables", "pdf_url": "http://arxiv.org/pdf/2506.22868v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22868v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23361", "title": "OmniVCus: Feedforward Subject-driven Video Customization with Multimodal Control Conditions", "authors": ["Yuanhao Cai", "He Zhang", "Xi Chen", "Jinbo Xing", "Yiwei Hu", "Yuqian Zhou", "Kai Zhang", "Zhifei Zhang", "Soo Ye Kim", "Tianyu Wang", "Yulun Zhang", "Xiaokang Yang", "Zhe Lin", "Alan Yuille"], "summary": "Existing feedforward subject-driven video customization methods mainly study\nsingle-subject scenarios due to the difficulty of constructing multi-subject\ntraining data pairs. Another challenging problem that how to use the signals\nsuch as depth, mask, camera, and text prompts to control and edit the subject\nin the customized video is still less explored. In this paper, we first propose\na data construction pipeline, VideoCus-Factory, to produce training data pairs\nfor multi-subject customization from raw videos without labels and control\nsignals such as depth-to-video and mask-to-video pairs. Based on our\nconstructed data, we develop an Image-Video Transfer Mixed (IVTM) training with\nimage editing data to enable instructive editing for the subject in the\ncustomized video. Then we propose a diffusion Transformer framework, OmniVCus,\nwith two embedding mechanisms, Lottery Embedding (LE) and Temporally Aligned\nEmbedding (TAE). LE enables inference with more subjects by using the training\nsubjects to activate more frame embeddings. TAE encourages the generation\nprocess to extract guidance from temporally aligned control signals by\nassigning the same frame embeddings to the control and noise tokens.\nExperiments demonstrate that our method significantly surpasses\nstate-of-the-art methods in both quantitative and qualitative evaluations.\nVideo demos are at our project page:\nhttps://caiyuanhao1998.github.io/project/OmniVCus/. Our code will be released\nat https://github.com/caiyuanhao1998/Open-OmniVCus", "comment": "A data construction pipeline and a diffusion Transformer framework\n  for controllable subject-driven video customization", "pdf_url": "http://arxiv.org/pdf/2506.23361v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23361v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.22488", "title": "Zero-Shot EEG-to-Gait Decoding via Phase-Aware Representation Learning", "authors": ["Xi Fu", "Weibang Jiang", "Rui Liu", "Gernot R. M√ºller-Putz", "Cuntai Guan"], "summary": "Accurate decoding of lower-limb motion from EEG signals is essential for\nadvancing brain-computer interface (BCI) applications in movement intent\nrecognition and control. However, challenges persist in achieving causal,\nphase-consistent predictions and in modeling both inter- and intra-subject\nvariability. To address these issues, we propose NeuroDyGait, a\ndomain-generalizable EEG-to-motion decoding framework that leverages structured\ncontrastive representation learning and relational domain modeling. The\nproposed method employs relative contrastive learning to achieve semantic\nalignment between EEG and motion embeddings. Furthermore, a multi-cycle gait\nreconstruction objective is introduced to enforce temporal coherence and\nmaintain biomechanical consistency. To promote inter-session generalization,\nduring fine-tuning, a domain dynamic decoding mechanism adaptively assigns\nsession-specific prediction heads and learns to mix their outputs based on\ninter-session relationships. NeuroDyGait enables zero-shot motion prediction\nfor unseen individuals without requiring adaptation and achieves superior\nperformance in cross-subject gait decoding on benchmark datasets. Additionally,\nit demonstrates strong phase-detection capabilities even without explicit phase\nsupervision during training. These findings highlight the potential of\nrelational domain learning in enabling scalable, target-free deployment of\nBCIs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22488v1", "categories": ["eess.SP", "cs.LG"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.22488v1", "date": "2025-06-24", "updated": "2025-06-24"}
{"id": "2506.22880", "title": "Decoupled Seg Tokens Make Stronger Reasoning Video Segmenter and Grounder", "authors": ["Dang Jisheng", "Wu Xudong", "Wang Bimei", "Lv Ning", "Chen Jiayu", "Jingwen Zhao", "Yichu liu", "Jizhao Liu", "Juncheng Li", "Teng Wang"], "summary": "Existing video segmenter and grounder approaches, exemplified by Sa2VA,\ndirectly fuse features within segmentation models. This often results in an\nundesirable entanglement of dynamic visual information and static semantics,\nthereby degrading segmentation accuracy. To systematically mitigate this issue,\nwe propose DeSa2VA, a decoupling-enhanced prompting scheme integrating text\npre-training and a linear decoupling module to address the information\nprocessing limitations inherent in SAM-2. Specifically, first, we devise a\npre-training paradigm that converts textual ground-truth labels into\npoint-level prompts while generating corresponding text masks. These masks are\nrefined through a hybrid loss function to strengthen the model's semantic\ngrounding capabilities. Next, we employ linear projection to disentangle hidden\nstates that generated by a large language model into distinct textual and\nvisual feature subspaces. Finally, a dynamic mask fusion strategy\nsynergistically combines these decoupled features through triple supervision\nfrom predicted text/visual masks and ground-truth annotations. Extensive\nexperiments demonstrate state-of-the-art performance across diverse tasks,\nincluding image segmentation, image question answering, video segmentation, and\nvideo question answering. Our codes are available at\nhttps://github.com/longmalongma/DeSa2VA.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22880v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22880v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23382", "title": "SIEDD: Shared-Implicit Encoder with Discrete Decoders", "authors": ["Vikram Rangarajan", "Shishira Maiya", "Max Ehrlich", "Abhinav Shrivastava"], "summary": "Implicit Neural Representations (INRs) offer exceptional fidelity for video\ncompression by learning per-video optimized functions, but their adoption is\ncrippled by impractically slow encoding times. Existing attempts to accelerate\nINR encoding often sacrifice reconstruction quality or crucial coordinate-level\ncontrol essential for adaptive streaming and transcoding. We introduce SIEDD\n(Shared-Implicit Encoder with Discrete Decoders), a novel architecture that\nfundamentally accelerates INR encoding without these compromises. SIEDD first\nrapidly trains a shared, coordinate-based encoder on sparse anchor frames to\nefficiently capture global, low-frequency video features. This encoder is then\nfrozen, enabling massively parallel training of lightweight, discrete decoders\nfor individual frame groups, further expedited by aggressive coordinate-space\nsampling. This synergistic design delivers a remarkable 20-30X encoding\nspeed-up over state-of-the-art INR codecs on HD and 4K benchmarks, while\nmaintaining competitive reconstruction quality and compression ratios.\nCritically, SIEDD retains full coordinate-based control, enabling continuous\nresolution decoding and eliminating costly transcoding. Our approach\nsignificantly advances the practicality of high-fidelity neural video\ncompression, demonstrating a scalable and efficient path towards real-world\ndeployment. Our codebase is available at\nhttps://github.com/VikramRangarajan/SIEDD .", "comment": "Project page at https://vikramrangarajan.github.io/SIEDD . Project\n  code at https://github.com/VikramRangarajan/SIEDD", "pdf_url": "http://arxiv.org/pdf/2506.23382v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23382v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.22490", "title": "MENGLAN: Multiscale Enhanced Nonparametric Gas Analyzer with Lightweight Architecture and Networks", "authors": ["Zhenke Duan", "Jiqun Pan", "Jiani Tu"], "summary": "Accurate detection of ethylene concentrations in mixed gases is crucial in\nchemical production for safety and health purposes. Traditional methods are\nhindered by high cost and complexity, limiting their practical application.\nThis study proposes MENGLAN, a Multiscale Enhanced Nonparametric Gas Analyzer\nthat integrates a dual-stream structure, a Hybrid Multi-Head Attention\nmechanism, and a Feature Reactivation Module to enable real-time, lightweight,\nand high-precision ethylene concentration prediction. Results show that MENGLAN\nachieves superior performance, reduced computational demand, and enhanced\ndeployability compared to existing methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22490v1", "categories": ["eess.SP", "cs.LG"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.22490v1", "date": "2025-06-24", "updated": "2025-06-24"}
{"id": "2506.22884", "title": "Performance Measurements in the AI-Centric Computing Continuum Systems", "authors": ["Praveen Kumar Donta", "Qiyang Zhang", "Schahram Dustdar"], "summary": "Over the Eight decades, computing paradigms have shifted from large,\ncentralized systems to compact, distributed architectures, leading to the rise\nof the Distributed Computing Continuum (DCC). In this model, multiple layers\nsuch as cloud, edge, Internet of Things (IoT), and mobile platforms work\ntogether to support a wide range of applications. Recently, the emergence of\nGenerative AI and large language models has further intensified the demand for\ncomputational resources across this continuum. Although traditional performance\nmetrics have provided a solid foundation, they need to be revisited and\nexpanded to keep pace with changing computational demands and application\nrequirements. Accurate performance measurements benefit both system designers\nand users by supporting improvements in efficiency and promoting alignment with\nsystem goals. In this context, we review commonly used metrics in DCC and IoT\nenvironments. We also discuss emerging performance dimensions that address\nevolving computing needs, such as sustainability, energy efficiency, and system\nobservability. We also outline criteria and considerations for selecting\nappropriate metrics, aiming to inspire future research and development in this\ncritical area.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22884v1", "categories": ["cs.DC", "cs.AI", "cs.ET", "cs.NI", "cs.SY", "eess.SY"], "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.22884v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23414", "title": "A High-Throughput Platform to Bench Test Smartphone-Based Heart Rate Measurements Derived From Video", "authors": ["Ming-Zher Poh", "Jonathan Wang", "Jonathan Hsu", "Lawrence Cai", "Eric Teasley", "James A. Taylor", "Jameson K. Rogers", "Anupam Pathak", "Shwetak Patel"], "summary": "Smartphone-based heart rate (HR) monitoring apps using finger-over-camera\nphotoplethysmography (PPG) face significant challenges in performance\nevaluation and device compatibility due to device variability and\nfragmentation. Manual testing is impractical, and standardized methods are\nlacking. This paper presents a novel, high-throughput bench-testing platform to\naddress this critical need. We designed a system comprising a test rig capable\nof holding 12 smartphones for parallel testing, a method for generating\nsynthetic PPG test videos with controllable HR and signal quality, and a host\nmachine for coordinating video playback and data logging. The system achieved a\nmean absolute percentage error (MAPE) of 0.11% +/- 0.001% between input and\nmeasured HR, and a correlation coefficient of 0.92 +/- 0.008 between input and\nmeasured PPG signals using a clinically-validated smartphone-based HR app.\nBench-testing results of 20 different smartphone models correctly classified\nall the devices as meeting the ANSI/CTA accuracy standards for HR monitors\n(MAPE <10%) when compared to a prospective clinical study with 80 participants,\ndemonstrating high positive predictive value. This platform offers a scalable\nsolution for pre-deployment testing of smartphone HR apps to improve app\nperformance, ensure device compatibility, and advance the field of mobile\nhealth.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23414v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23414v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.22493", "title": "A Detailed Factor Analysis for the Political Compass Test: Navigating Ideologies of Large Language Models", "authors": ["Sadia Kamal", "Lalu Prasad Yadav Prakash", "S M Rafiuddin", "Mohammed Rakib", "Arunkumar Bagavathi", "Atriya Sen", "Sagnik Ray Choudhury"], "summary": "Political Compass Test (PCT) or similar questionnaires have been used to\nquantify LLM's political leanings. Building on a recent line of work that\nexamines the validity of PCT tests, we demonstrate that variation in standard\ngeneration parameters does not significantly impact the models' PCT scores.\nHowever, external factors such as prompt variations and fine-tuning\nindividually and in combination affect the same. Finally, we demonstrate that\nwhen models are fine-tuned on text datasets with higher political content than\nothers, the PCT scores are not differentially affected. This calls for a\nthorough investigation into the validity of PCT and similar tests, as well as\nthe mechanism by which political leanings are encoded in LLMs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22493v1", "categories": ["cs.CY", "cs.CL", "cs.LG"], "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.22493v1", "date": "2025-06-24", "updated": "2025-06-24"}
{"id": "2506.22895", "title": "Interpretable Time Series Autoregression for Periodicity Quantification", "authors": ["Xinyu Chen", "Vassilis Digalakis Jr", "Lijun Ding", "Dingyi Zhuang", "Jinhua Zhao"], "summary": "Time series autoregression is a classical statistical model for capturing\nauto-correlations and identifying temporal patterns such as periodicity and\nseasonality. In this work, we propose a novel sparse autoregression framework\nfrom an interpretable machine learning perspective and the model\ninterpretability for periodicity quantification is reinforced by $\\ell_0$-norm\ninduced sparsity constraints. On the time-varying time series data, we\nreformulate the sparse autoregression and convert the involved optimization\nproblem into a mixed-integer optimization (MIO). To accelerate it, we develop a\nsubspace pursuit based decision variable pruning (DVP) strategy to reduce the\nsearch space. On the multidimensional time series that involves complicated\nspatial and temporal dimensions, we propose a spatially- and time-varying\nsparse autoregression model and resolve the corresponding MIO problem by\ndeveloping a two-stage optimization scheme. In particular, the proposed scheme\nmakes the model scalable to large problems even with millions of decision\nvariables. Empirically, we conduct extensive experiments to evaluate the\nproposed models on real-world time series data. First, we demonstrate that the\nMIO solver can be drastically accelerated through the DVP strategy, while\nmaintaining the same solution quality as a full MIO solver. Applying the\ntime-varying sparse autoregression model to ridesharing trip data, we uncover\nboth daily and weekly periodicities and reveal long-term changes in regularity\nof human mobility. Second, we demonstrate the spatial patterns of yearly\nseasonality in climate variable time series such as temperature and\nprecipitation across the past four decades, and our model allows to discover\ndynamic climate patterns and identify climate phenomena such as El Nino in sea\nsurface temperature.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22895v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22895v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23418", "title": "Why Settle for Mid: A Probabilistic Viewpoint to Spatial Relationship Alignment in Text-to-image Models", "authors": ["Parham Rezaei", "Arash Marioriyad", "Mahdieh Soleymani Baghshah", "Mohammad Hossein Rohban"], "summary": "Despite the ability of text-to-image models to generate high-quality,\nrealistic, and diverse images, they face challenges in compositional\ngeneration, often struggling to accurately represent details specified in the\ninput prompt. A prevalent issue in compositional generation is the misalignment\nof spatial relationships, as models often fail to faithfully generate images\nthat reflect the spatial configurations specified between objects in the input\nprompts. To address this challenge, we propose a novel probabilistic framework\nfor modeling the relative spatial positioning of objects in a scene, leveraging\nthe concept of Probability of Superiority (PoS). Building on this insight, we\nmake two key contributions. First, we introduce a novel evaluation metric,\nPoS-based Evaluation (PSE), designed to assess the alignment of 2D and 3D\nspatial relationships between text and image, with improved adherence to human\njudgment. Second, we propose PoS-based Generation (PSG), an inference-time\nmethod that improves the alignment of 2D and 3D spatial relationships in T2I\nmodels without requiring fine-tuning. PSG employs a Part-of-Speech PoS-based\nreward function that can be utilized in two distinct ways: (1) as a\ngradient-based guidance mechanism applied to the cross-attention maps during\nthe denoising steps, or (2) as a search-based strategy that evaluates a set of\ninitial noise vectors to select the best one. Extensive experiments demonstrate\nthat the PSE metric exhibits stronger alignment with human judgment compared to\ntraditional center-based metrics, providing a more nuanced and reliable measure\nof complex spatial relationship accuracy in text-image alignment. Furthermore,\nPSG significantly enhances the ability of text-to-image models to generate\nimages with specified spatial configurations, outperforming state-of-the-art\nmethods across multiple evaluation metrics and benchmarks.", "comment": "12 main pages, 18 figures, and 16 tables", "pdf_url": "http://arxiv.org/pdf/2506.23418v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23418v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.22494", "title": "DriveBLIP2: Attention-Guided Explanation Generation for Complex Driving Scenarios", "authors": ["Shihong Ling", "Yue Wan", "Xiaowei Jia", "Na Du"], "summary": "This paper introduces a new framework, DriveBLIP2, built upon the BLIP2-OPT\narchitecture, to generate accurate and contextually relevant explanations for\nemerging driving scenarios. While existing vision-language models perform well\nin general tasks, they encounter difficulties in understanding complex,\nmulti-object environments, particularly in real-time applications such as\nautonomous driving, where the rapid identification of key objects is crucial.\nTo address this limitation, an Attention Map Generator is proposed to highlight\nsignificant objects relevant to driving decisions within critical video frames.\nBy directing the model's focus to these key regions, the generated attention\nmap helps produce clear and relevant explanations, enabling drivers to better\nunderstand the vehicle's decision-making process in critical situations.\nEvaluations on the DRAMA dataset reveal significant improvements in explanation\nquality, as indicated by higher BLEU, ROUGE, CIDEr, and SPICE scores compared\nto baseline models. These findings underscore the potential of targeted\nattention mechanisms in vision-language models for enhancing explainability in\nreal-time autonomous driving.", "comment": "Accepted to IEEE/RSJ International Conference on Intelligent Robots\n  and Systems (IROS) 2025. 7 pages, 3 figures", "pdf_url": "http://arxiv.org/pdf/2506.22494v1", "categories": ["cs.RO", "cs.CV", "cs.LG"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.22494v1", "date": "2025-06-25", "updated": "2025-06-25"}
{"id": "2506.22901", "title": "Missing-Modality-Aware Graph Neural Network for Cancer Classification", "authors": ["Sina Tabakhi", "Haiping Lu"], "summary": "A key challenge in learning from multimodal biological data is missing\nmodalities, where all data from some modalities are missing for some patients.\nCurrent fusion methods address this by excluding patients with missing\nmodalities, imputing missing modalities, or making predictions directly with\npartial modalities. However, they often struggle with diverse missing-modality\npatterns and the exponential growth of the number of such patterns as the\nnumber of modalities increases. To address these limitations, we propose MAGNET\n(Missing-modality-Aware Graph neural NETwork) for direct prediction with\npartial modalities, which introduces a patient-modality multi-head attention\nmechanism to fuse lower-dimensional modality embeddings based on their\nimportance and missingness. MAGNET's complexity increases linearly with the\nnumber of modalities while adapting to missing-pattern variability. To generate\npredictions, MAGNET further constructs a patient graph with fused multimodal\nembeddings as node features and the connectivity determined by the modality\nmissingness, followed by a conventional graph neural network. Experiments on\nthree public multiomics datasets for cancer classification, with real-world\ninstead of artificial missingness, show that MAGNET outperforms the\nstate-of-the-art fusion methods. The data and code are available at\nhttps://github.com/SinaTabakhi/MAGNET.", "comment": "15 pages, 7 figures", "pdf_url": "http://arxiv.org/pdf/2506.22901v1", "categories": ["cs.LG", "cs.AI", "q-bio.BM", "q-bio.GN"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22901v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23426", "title": "Detecting What Matters: A Novel Approach for Out-of-Distribution 3D Object Detection in Autonomous Vehicles", "authors": ["Menna Taha", "Aya Ahmed", "Mohammed Karmoose", "Yasser Gadallah"], "summary": "Autonomous vehicles (AVs) use object detection models to recognize their\nsurroundings and make driving decisions accordingly. Conventional object\ndetection approaches classify objects into known classes, which limits the AV's\nability to detect and appropriately respond to Out-of-Distribution (OOD)\nobjects. This problem is a significant safety concern since the AV may fail to\ndetect objects or misclassify them, which can potentially lead to hazardous\nsituations such as accidents. Consequently, we propose a novel object detection\napproach that shifts the emphasis from conventional class-based classification\nto object harmfulness determination. Instead of object detection by their\nspecific class, our method identifies them as either 'harmful' or 'harmless'\nbased on whether they pose a danger to the AV. This is done based on the object\nposition relative to the AV and its trajectory. With this metric, our model can\neffectively detect previously unseen objects to enable the AV to make safer\nreal-time decisions. Our results demonstrate that the proposed model\neffectively detects OOD objects, evaluates their harmfulness, and classifies\nthem accordingly, thus enhancing the AV decision-making effectiveness in\ndynamic environments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23426v1", "categories": ["cs.CV", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23426v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.22495", "title": "Masked Autoencoders that Feel the Heart: Unveiling Simplicity Bias for ECG Analyses", "authors": ["He-Yang Xu", "Hongxiang Gao", "Yuwen Li", "Xiu-Shen Wei", "Chengyu Liu"], "summary": "The diagnostic value of electrocardiogram (ECG) lies in its dynamic\ncharacteristics, ranging from rhythm fluctuations to subtle waveform\ndeformations that evolve across time and frequency domains. However, supervised\nECG models tend to overfit dominant and repetitive patterns, overlooking\nfine-grained but clinically critical cues, a phenomenon known as Simplicity\nBias (SB), where models favor easily learnable signals over subtle but\ninformative ones. In this work, we first empirically demonstrate the presence\nof SB in ECG analyses and its negative impact on diagnostic performance, while\nsimultaneously discovering that self-supervised learning (SSL) can alleviate\nit, providing a promising direction for tackling the bias. Following the SSL\nparadigm, we propose a novel method comprising two key components: 1)\nTemporal-Frequency aware Filters to capture temporal-frequency features\nreflecting the dynamic characteristics of ECG signals, and 2) building on this,\nMulti-Grained Prototype Reconstruction for coarse and fine representation\nlearning across dual domains, further mitigating SB. To advance SSL in ECG\nanalyses, we curate a large-scale multi-site ECG dataset with 1.53 million\nrecordings from over 300 clinical centers. Experiments on three downstream\ntasks across six ECG datasets demonstrate that our method effectively reduces\nSB and achieves state-of-the-art performance. Code and dataset will be released\npublicly.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22495v1", "categories": ["eess.SP", "cs.AI", "cs.LG"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.22495v1", "date": "2025-06-25", "updated": "2025-06-25"}
{"id": "2506.22911", "title": "Learning Truthful Mechanisms without Discretization", "authors": ["Yunxuan Ma", "Siqiang Wang", "Zhijian Duan", "Yukun Cheng", "Xiaotie Deng"], "summary": "This paper introduces TEDI (Truthful, Expressive, and Dimension-Insensitive\napproach), a discretization-free algorithm to learn truthful and\nutility-maximizing mechanisms. Existing learning-based approaches often rely on\ndiscretization of outcome spaces to ensure truthfulness, which leads to\ninefficiency with increasing problem size. To address this limitation, we\nformalize the concept of pricing rules, defined as functions that map outcomes\nto prices. Based on this concept, we propose a novel menu mechanism, which can\nbe equivalent to a truthful direct mechanism under specific conditions. The\ncore idea of TEDI lies in its parameterization of pricing rules using Partial\nGroupMax Network, a new network architecture designed to universally\napproximate partial convex functions. To learn optimal pricing rules, we\ndevelop novel training techniques, including covariance trick and continuous\nsampling, to derive unbiased gradient estimators compatible with first-order\noptimization. Theoretical analysis establishes that TEDI guarantees\ntruthfulness, full expressiveness, and dimension-insensitivity. Experimental\nevaluation in the studied auction setting demonstrates that TEDI achieves\nstrong performance, competitive with or exceeding state-of-the-art methods.\n  This work presents the first approaches to learn truthful mechanisms without\noutcome discretization, thereby enhancing algorithmic efficiency. The proposed\nconcepts, network architecture, and learning techniques might offer potential\nvalue and provide new insights for automated mechanism design and\ndifferentiable economics.", "comment": "66 pages", "pdf_url": "http://arxiv.org/pdf/2506.22911v1", "categories": ["cs.GT", "cs.AI", "cs.LG"], "cate": "cs.GT", "url": "http://arxiv.org/abs/2506.22911v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23434", "title": "Towards foundational LiDAR world models with efficient latent flow matching", "authors": ["Tianran Liu", "Shengwen Zhao", "Nicholas Rhinehart"], "summary": "LiDAR-based world models offer more structured and geometry-aware\nrepresentations than their image-based counterparts. However, existing LiDAR\nworld models are narrowly trained; each model excels only in the domain for\nwhich it was built. Can we develop LiDAR world models that exhibit strong\ntransferability across multiple domains? We conduct the first systematic domain\ntransfer study across three demanding scenarios: (i) outdoor to indoor\ngeneralization, (ii) sparse-beam \\& dense-beam adaptation, and (iii)\nnon-semantic to semantic transfer. Given different amounts of fine-tuning data,\nour experiments show that a single pre-trained model can achieve up to 11%\nabsolute improvement (83\\% relative) over training from scratch and outperforms\ntraining from scratch in 30/36 of our comparisons. This transferability of\ndynamic learning significantly reduces the reliance on manually annotated data\nfor semantic occupancy forecasting: our method exceed the previous semantic\noccupancy forecasting models with only 5% of the labeled training data required\nby prior models. We also observed inefficiencies of current LiDAR world models,\nmainly through their under-compression of LiDAR data and inefficient training\nobjectives. To address this, we propose a latent conditional flow matching\n(CFM)-based frameworks that achieves state-of-the-art reconstruction accuracy\nusing only half the training data and a compression ratio 6 times higher than\nthat of prior methods. Our model achieves SOTA performance on\nfuture-trajectory-conditioned semantic occupancy forecasting while being 23x\nmore computationally efficient (a 28x FPS speedup); and achieves SOTA\nperformance on semantic occupancy forecasting while being 2x more\ncomputationally efficient (a 1.1x FPS speedup).", "comment": "25 pages, 13 figures", "pdf_url": "http://arxiv.org/pdf/2506.23434v1", "categories": ["cs.CV", "cs.RO"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23434v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22503", "title": "What Makes a Dribble Successful? Insights From 3D Pose Tracking Data", "authors": ["Michiel Schepers", "Pieter Robberechts", "Jan Van Haaren", "Jesse Davis"], "summary": "Data analysis plays an increasingly important role in soccer, offering new\nways to evaluate individual and team performance. One specific application is\nthe evaluation of dribbles: one-on-one situations where an attacker attempts to\nbypass a defender with the ball. While previous research has primarily relied\non 2D positional tracking data, this fails to capture aspects like balance,\norientation, and ball control, limiting the depth of current insights. This\nstudy explores how pose tracking data (capturing players' posture and movement\nin three dimensions) can improve our understanding of dribbling skills. We\nextract novel pose-based features from 1,736 dribbles in the 2022/23 Champions\nLeague season and evaluate their impact on dribble success. Our results\nindicate that features capturing the attacker's balance and the alignment of\nthe orientation between the attacker and defender are informative for\npredicting dribble success. Incorporating these pose-based features on top of\nfeatures derived from traditional 2D positional data leads to a measurable\nimprovement in model performance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22503v1", "categories": ["cs.CV", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22503v1", "date": "2025-06-25", "updated": "2025-06-25"}
{"id": "2506.22929", "title": "Mathematical Computation on High-dimensional Data via Array Programming and Parallel Acceleration", "authors": ["Chen Zhang"], "summary": "While deep learning excels in natural image and language processing, its\napplication to high-dimensional data faces computational challenges due to the\ndimensionality curse. Current large-scale data tools focus on business-oriented\ndescriptive statistics, lacking mathematical statistics support for advanced\nanalysis. We propose a parallel computation architecture based on space\ncompleteness, decomposing high-dimensional data into dimension-independent\nstructures for distributed processing. This framework enables seamless\nintegration of data mining and parallel-optimized machine learning methods,\nsupporting scientific computations across diverse data types like medical and\nnatural images within a unified system.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22929v1", "categories": ["cs.LG", "cs.AI", "eess.IV", "eess.SP"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22929v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23440", "title": "PathDiff: Histopathology Image Synthesis with Unpaired Text and Mask Conditions", "authors": ["Mahesh Bhosale", "Abdul Wasi", "Yuanhao Zhai", "Yunjie Tian", "Samuel Border", "Nan Xi", "Pinaki Sarder", "Junsong Yuan", "David Doermann", "Xuan Gong"], "summary": "Diffusion-based generative models have shown promise in synthesizing\nhistopathology images to address data scarcity caused by privacy constraints.\nDiagnostic text reports provide high-level semantic descriptions, and masks\noffer fine-grained spatial structures essential for representing distinct\nmorphological regions. However, public datasets lack paired text and mask data\nfor the same histopathological images, limiting their joint use in image\ngeneration. This constraint restricts the ability to fully exploit the benefits\nof combining both modalities for enhanced control over semantics and spatial\ndetails. To overcome this, we propose PathDiff, a diffusion framework that\neffectively learns from unpaired mask-text data by integrating both modalities\ninto a unified conditioning space. PathDiff allows precise control over\nstructural and contextual features, generating high-quality, semantically\naccurate images. PathDiff also improves image fidelity, text-image alignment,\nand faithfulness, enhancing data augmentation for downstream tasks like nuclei\nsegmentation and classification. Extensive experiments demonstrate its\nsuperiority over existing methods.", "comment": "Accepted to ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2506.23440v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23440v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22504", "title": "Patch2Loc: Learning to Localize Patches for Unsupervised Brain Lesion Detection", "authors": ["Hassan Baker", "Austin J. Brockmeier"], "summary": "Detecting brain lesions as abnormalities observed in magnetic resonance\nimaging (MRI) is essential for diagnosis and treatment. In the search of\nabnormalities, such as tumors and malformations, radiologists may benefit from\ncomputer-aided diagnostics that use computer vision systems trained with\nmachine learning to segment normal tissue from abnormal brain tissue. While\nsupervised learning methods require annotated lesions, we propose a new\nunsupervised approach (Patch2Loc) that learns from normal patches taken from\nstructural MRI. We train a neural network model to map a patch back to its\nspatial location within a slice of the brain volume. During inference, abnormal\npatches are detected by the relatively higher error and/or variance of the\nlocation prediction. This generates a heatmap that can be integrated into\npixel-wise methods to achieve finer-grained segmentation. We demonstrate the\nability of our model to segment abnormal brain tissues by applying our approach\nto the detection of tumor tissues in MRI on T2-weighted images from BraTS2021\nand MSLUB datasets and T1-weighted images from ATLAS and WMH datasets. We show\nthat it outperforms the state-of-the art in unsupervised segmentation. The\ncodebase for this work can be found on our\n\\href{https://github.com/bakerhassan/Patch2Loc}{GitHub page}.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22504v1", "categories": ["cs.CV", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22504v1", "date": "2025-06-25", "updated": "2025-06-25"}
{"id": "2506.22941", "title": "Positioning AI Tools to Support Online Harm Reduction Practice: Applications and Design Directions", "authors": ["Kaixuan Wang", "Jason T. Jacques", "Chenxin Diao"], "summary": "Access to accurate and actionable harm reduction information can directly\nimpact the health outcomes of People Who Use Drugs (PWUD), yet existing online\nchannels often fail to meet their diverse and dynamic needs due to limitations\nin adaptability, accessibility, and the pervasive impact of stigma. Large\nLanguage Models (LLMs) present a novel opportunity to enhance information\nprovision, but their application in such a high-stakes domain is under-explored\nand presents socio-technical challenges. This paper investigates how LLMs can\nbe responsibly designed to support the information needs of PWUD. Through a\nqualitative workshop involving diverse stakeholder groups (academics, harm\nreduction practitioners, and an online community moderator), we explored LLM\ncapabilities, identified potential use cases, and delineated core design\nconsiderations. Our findings reveal that while LLMs can address some existing\ninformation barriers (e.g., by offering responsive, multilingual, and\npotentially less stigmatising interactions), their effectiveness is contingent\nupon overcoming challenges related to ethical alignment with harm reduction\nprinciples, nuanced contextual understanding, effective communication, and\nclearly defined operational boundaries. We articulate design pathways\nemphasising collaborative co-design with experts and PWUD to develop LLM\nsystems that are helpful, safe, and responsibly governed. This work contributes\nempirically grounded insights and actionable design considerations for the\nresponsible development of LLMs as supportive tools within the harm reduction\necosystem.", "comment": "16 pages, 4 figures, with appendix", "pdf_url": "http://arxiv.org/pdf/2506.22941v1", "categories": ["cs.HC", "cs.AI"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.22941v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23460", "title": "Contrastive Learning with Diffusion Features for Weakly Supervised Medical Image Segmentation", "authors": ["Dewen Zeng", "Xinrong Hu", "Yu-Jen Chen", "Yawen Wu", "Xiaowei Xu", "Yiyu Shi"], "summary": "Weakly supervised semantic segmentation (WSSS) methods using class labels\noften rely on class activation maps (CAMs) to localize objects. However,\ntraditional CAM-based methods struggle with partial activations and imprecise\nobject boundaries due to optimization discrepancies between classification and\nsegmentation. Recently, the conditional diffusion model (CDM) has been used as\nan alternative for generating segmentation masks in WSSS, leveraging its strong\nimage generation capabilities tailored to specific class distributions. By\nmodifying or perturbing the condition during diffusion sampling, the related\nobjects can be highlighted in the generated images. Yet, the saliency maps\ngenerated by CDMs are prone to noise from background alterations during reverse\ndiffusion. To alleviate the problem, we introduce Contrastive Learning with\nDiffusion Features (CLDF), a novel method that uses contrastive learning to\ntrain a pixel decoder to map the diffusion features from a frozen CDM to a\nlow-dimensional embedding space for segmentation. Specifically, we integrate\ngradient maps generated from CDM external classifier with CAMs to identify\nforeground and background pixels with fewer false positives/negatives for\ncontrastive learning, enabling robust pixel embedding learning. Experimental\nresults on four segmentation tasks from two public medical datasets demonstrate\nthat our method significantly outperforms existing baselines.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23460v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23460v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22505", "title": "Weakly Supervised Object Segmentation by Background Conditional Divergence", "authors": ["Hassan Baker", "Matthew S. Emigh", "Austin J. Brockmeier"], "summary": "As a computer vision task, automatic object segmentation remains challenging\nin specialized image domains without massive labeled data, such as synthetic\naperture sonar images, remote sensing, biomedical imaging, etc. In any domain,\nobtaining pixel-wise segmentation masks is expensive. In this work, we propose\na method for training a masking network to perform binary object segmentation\nusing weak supervision in the form of image-wise presence or absence of an\nobject of interest, which provides less information but may be obtained more\nquickly from manual or automatic labeling. A key step in our method is that the\nsegmented objects can be placed into background-only images to create\nrealistic, images of the objects with counterfactual backgrounds. To create a\ncontrast between the original and counterfactual background images, we propose\nto first cluster the background-only images, and then during learning create\ncounterfactual images that blend objects segmented from their original source\nbackgrounds to backgrounds chosen from a targeted cluster. One term in the\ntraining loss is the divergence between these counterfactual images and the\nreal object images with backgrounds of the target cluster. The other term is a\nsupervised loss for background-only images. While an adversarial critic could\nprovide the divergence, we use sample-based divergences. We conduct experiments\non side-scan and synthetic aperture sonar in which our approach succeeds\ncompared to previous unsupervised segmentation baselines that were only tested\non natural images. Furthermore, to show generality we extend our experiments to\nnatural images, obtaining reasonable performance with our method that avoids\npretrained networks, generative networks, and adversarial critics. The basecode\nfor this work can be found at\n\\href{GitHub}{https://github.com/bakerhassan/WSOS}.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22505v1", "categories": ["cs.CV", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22505v1", "date": "2025-06-25", "updated": "2025-06-25"}
{"id": "2506.22949", "title": "A Study on Semi-Supervised Detection of DDoS Attacks under Class Imbalance", "authors": ["Ehsan Hallaji", "Vaishnavi Shanmugam", "Roozbeh Razavi-Far", "Mehrdad Saif"], "summary": "One of the most difficult challenges in cybersecurity is eliminating\nDistributed Denial of Service (DDoS) attacks. Automating this task using\nartificial intelligence is a complex process due to the inherent class\nimbalance and lack of sufficient labeled samples of real-world datasets. This\nresearch investigates the use of Semi-Supervised Learning (SSL) techniques to\nimprove DDoS attack detection when data is imbalanced and partially labeled. In\nthis process, 13 state-of-the-art SSL algorithms are evaluated for detecting\nDDoS attacks in several scenarios. We evaluate their practical efficacy and\nshortcomings, including the extent to which they work in extreme environments.\nThe results will offer insight into designing intelligent Intrusion Detection\nSystems (IDSs) that are robust against class imbalance and handle partially\nlabeled data.", "comment": "Accepted for publication in IEEE CCECE 2025", "pdf_url": "http://arxiv.org/pdf/2506.22949v1", "categories": ["cs.CR", "cs.AI", "cs.LG"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.22949v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23461", "title": "Time-variant Image Inpainting via Interactive Distribution Transition Estimation", "authors": ["Yun Xing", "Qing Guo", "Xiaoguang Li", "Yihao Huang", "Xiaofeng Cao", "Di Lin", "Ivor Tsang", "Lei Ma"], "summary": "In this work, we focus on a novel and practical task, i.e., Time-vAriant\niMage inPainting (TAMP). The aim of TAMP is to restore a damaged target image\nby leveraging the complementary information from a reference image, where both\nimages captured the same scene but with a significant time gap in between,\ni.e., time-variant images. Different from conventional reference-guided image\ninpainting, the reference image under TAMP setup presents significant content\ndistinction to the target image and potentially also suffers from damages. Such\nan application frequently happens in our daily lives to restore a damaged image\nby referring to another reference image, where there is no guarantee of the\nreference image's source and quality. In particular, our study finds that even\nstate-of-the-art (SOTA) reference-guided image inpainting methods fail to\nachieve plausible results due to the chaotic image complementation. To address\nsuch an ill-posed problem, we propose a novel Interactive Distribution\nTransition Estimation (InDiTE) module which interactively complements the\ntime-variant images with adaptive semantics thus facilitate the restoration of\ndamaged regions. To further boost the performance, we propose our TAMP\nsolution, namely Interactive Distribution Transition Estimation-driven\nDiffusion (InDiTE-Diff), which integrates InDiTE with SOTA diffusion model and\nconducts latent cross-reference during sampling. Moreover, considering the lack\nof benchmarks for TAMP task, we newly assembled a dataset, i.e., TAMP-Street,\nbased on existing image and mask datasets. We conduct experiments on the\nTAMP-Street datasets under two different time-variant image inpainting\nsettings, which show our method consistently outperform SOTA reference-guided\nimage inpainting methods for solving TAMP.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23461v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23461v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22532", "title": "High Resolution Isotropic 3D Cine imaging with Automated Segmentation using Concatenated 2D Real-time Imaging and Deep Learning", "authors": ["Mark Wrobel", "Michele Pascale", "Tina Yao", "Ruaraidh Campbell", "Elena Milano", "Michael Quail", "Jennifer Steeden", "Vivek Muthurangu"], "summary": "Background: Conventional cardiovascular magnetic resonance (CMR) in\npaediatric and congenital heart disease uses 2D, breath-hold, balanced steady\nstate free precession (bSSFP) cine imaging for assessment of function and\ncardiac-gated, respiratory-navigated, static 3D bSSFP whole-heart imaging for\nanatomical assessment. Our aim is to concatenate a stack 2D free-breathing\nreal-time cines and use Deep Learning (DL) to create an isotropic a fully\nsegmented 3D cine dataset from these images. Methods: Four DL models were\ntrained on open-source data that performed: a) Interslice contrast correction;\nb) Interslice respiratory motion correction; c) Super-resolution (slice\ndirection); and d) Segmentation of right and left atria and ventricles (RA, LA,\nRV, and LV), thoracic aorta (Ao) and pulmonary arteries (PA). In 10 patients\nundergoing routine cardiovascular examination, our method was validated on\nprospectively acquired sagittal stacks of real-time cine images. Quantitative\nmetrics (ventricular volumes and vessel diameters) and image quality of the 3D\ncines were compared to conventional breath hold cine and whole heart imaging.\nResults: All real-time data were successfully transformed into 3D cines with a\ntotal post-processing time of <1 min in all cases. There were no significant\nbiases in any LV or RV metrics with reasonable limits of agreement and\ncorrelation. There is also reasonable agreement for all vessel diameters,\nalthough there was a small but significant overestimation of RPA diameter.\nConclusion: We have demonstrated the potential of creating a 3D-cine data from\nconcatenated 2D real-time cine images using a series of DL models. Our method\nhas short acquisition and reconstruction times with fully segmented data being\navailable within 2 minutes. The good agreement with conventional imaging\nsuggests that our method could help to significantly speed up CMR in clinical\npractice.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22532v1", "categories": ["eess.IV", "cs.CV", "cs.LG"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.22532v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22957", "title": "Agent-to-Agent Theory of Mind: Testing Interlocutor Awareness among Large Language Models", "authors": ["Younwoo Choi", "Changling Li", "Yongjin Yang", "Zhijing Jin"], "summary": "As large language models (LLMs) are increasingly integrated into multi-agent\nand human-AI systems, understanding their awareness of both self-context and\nconversational partners is essential for ensuring reliable performance and\nrobust safety. While prior work has extensively studied situational awareness\nwhich refers to an LLM's ability to recognize its operating phase and\nconstraints, it has largely overlooked the complementary capacity to identify\nand adapt to the identity and characteristics of a dialogue partner. In this\npaper, we formalize this latter capability as interlocutor awareness and\npresent the first systematic evaluation of its emergence in contemporary LLMs.\nWe examine interlocutor inference across three dimensions-reasoning patterns,\nlinguistic style, and alignment preferences-and show that LLMs reliably\nidentify same-family peers and certain prominent model families, such as GPT\nand Claude. To demonstrate its practical significance, we develop three case\nstudies in which interlocutor awareness both enhances multi-LLM collaboration\nthrough prompt adaptation and introduces new alignment and safety\nvulnerabilities, including reward-hacking behaviors and increased jailbreak\nsusceptibility. Our findings highlight the dual promise and peril of\nidentity-sensitive behavior in LLMs, underscoring the need for further\nunderstanding of interlocutor awareness and new safeguards in multi-agent\ndeployments. Our code is open-sourced at\nhttps://github.com/younwoochoi/InterlocutorAwarenessLLM.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22957v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.MA"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.22957v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23465", "title": "Sanitizing Manufacturing Dataset Labels Using Vision-Language Models", "authors": ["Nazanin Mahjourian", "Vinh Nguyen"], "summary": "The success of machine learning models in industrial applications is heavily\ndependent on the quality of the datasets used to train the models. However,\nlarge-scale datasets, specially those constructed from crowd-sourcing and\nweb-scraping, often suffer from label noise, inconsistencies, and errors. This\nproblem is particularly pronounced in manufacturing domains, where obtaining\nhigh-quality labels is costly and time-consuming. This paper introduces\nVision-Language Sanitization and Refinement (VLSR), which is a\nvision-language-based framework for label sanitization and refinement in\nmulti-label manufacturing image datasets. This method embeds both images and\ntheir associated textual labels into a shared semantic space leveraging the\nCLIP vision-language model. Then two key tasks are addressed in this process by\ncomputing the cosine similarity between embeddings. First, label sanitization\nis performed to identify irrelevant, misspelled, or semantically weak labels,\nand surface the most semantically aligned label for each image by comparing\nimage-label pairs using cosine similarity between image and label embeddings.\nSecond, the method applies density-based clustering on text embeddings,\nfollowed by iterative cluster merging, to group semantically similar labels\ninto unified label groups. The Factorynet dataset, which includes noisy labels\nfrom both human annotations and web-scraped sources, is employed to evaluate\nthe effectiveness of the proposed framework. Experimental results demonstrate\nthat the VLSR framework successfully identifies problematic labels and improves\nlabel consistency. This method enables a significant reduction in label\nvocabulary through clustering, which ultimately enhances the dataset's quality\nfor training robust machine learning models in industrial applications with\nminimal human intervention.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23465v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23465v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22536", "title": "Strategic A/B testing via Maximum Probability-driven Two-armed Bandit", "authors": ["Yu Zhang", "Shanshan Zhao", "Bokui Wan", "Jinjuan Wang", "Xiaodong Yan"], "summary": "Detecting a minor average treatment effect is a major challenge in\nlarge-scale applications, where even minimal improvements can have a\nsignificant economic impact. Traditional methods, reliant on normal\ndistribution-based or expanded statistics, often fail to identify such minor\neffects because of their inability to handle small discrepancies with\nsufficient sensitivity. This work leverages a counterfactual outcome framework\nand proposes a maximum probability-driven two-armed bandit (TAB) process by\nweighting the mean volatility statistic, which controls Type I error. The\nimplementation of permutation methods further enhances the robustness and\nefficacy. The established strategic central limit theorem (SCLT) demonstrates\nthat our approach yields a more concentrated distribution under the null\nhypothesis and a less concentrated one under the alternative hypothesis,\ngreatly improving statistical power. The experimental results indicate a\nsignificant improvement in the A/B testing, highlighting the potential to\nreduce experimental costs while maintaining high statistical power.", "comment": "25 pages, 14 figures", "pdf_url": "http://arxiv.org/pdf/2506.22536v1", "categories": ["stat.ML", "cs.LG", "math.PR"], "cate": "stat.ML", "url": "http://arxiv.org/abs/2506.22536v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22968", "title": "Against 'softmaxing' culture", "authors": ["Daniel Mwesigwa"], "summary": "AI is flattening culture. Evaluations of \"culture\" are showing the myriad\nways in which large AI models are homogenizing language and culture, averaging\nout rich linguistic differences into generic expressions. I call this\nphenomenon \"softmaxing culture,\" and it is one of the fundamental challenges\nfacing AI evaluations today. Efforts to improve and strengthen evaluations of\nculture are central to the project of cultural alignment in large AI systems.\nThis position paper argues that machine learning (ML) and human-computer\ninteraction (HCI) approaches to evaluation are limited. I propose two key\nshifts. First, instead of asking \"what is culture?\" at the start of system\nevaluations, I propose beginning with the question: \"when is culture?\" Second,\nwhile I acknowledge the philosophical claim that cultural universals exist, the\nchallenge is not simply to describe them, but to situate them in relation to\ntheir particulars. Taken together, these conceptual shifts invite evaluation\napproaches that move beyond technical requirements, toward perspectives more\nresponsive to the complexities of culture.", "comment": "7 pages", "pdf_url": "http://arxiv.org/pdf/2506.22968v1", "categories": ["cs.HC", "cs.AI"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.22968v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23467", "title": "AdFair-CLIP: Adversarial Fair Contrastive Language-Image Pre-training for Chest X-rays", "authors": ["Chenlang Yi", "Zizhan Xiong", "Qi Qi", "Xiyuan Wei", "Girish Bathla", "Ching-Long Lin", "Bobak Jack Mortazavi", "Tianbao Yang"], "summary": "Contrastive Language-Image Pre-training (CLIP) models have demonstrated\nsuperior performance across various visual tasks including medical image\nclassification. However, fairness concerns, including demographic biases, have\nreceived limited attention for CLIP models. This oversight leads to critical\nissues, particularly those related to race and gender, resulting in disparities\nin diagnostic outcomes and reduced reliability for underrepresented groups. To\naddress these challenges, we introduce AdFair-CLIP, a novel framework employing\nadversarial feature intervention to suppress sensitive attributes, thereby\nmitigating spurious correlations and improving prediction fairness. We conduct\ncomprehensive experiments on chest X-ray (CXR) datasets, and show that\nAdFair-CLIP significantly enhances both fairness and diagnostic accuracy, while\nmaintaining robust generalization in zero-shot and few-shot scenarios. These\nresults establish new benchmarks for fairness-aware learning in CLIP-based\nmedical diagnostic models, particularly for CXR analysis.", "comment": "This preprint has been accepted by MICCAI 2025", "pdf_url": "http://arxiv.org/pdf/2506.23467v1", "categories": ["cs.CV", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23467v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22552", "title": "Neural models of multiscale systems: conceptual limitations, stochastic parametrizations, and a climate application", "authors": ["Fabrizio Falasca"], "summary": "This work explores key conceptual limitations in data-driven modeling of\nmultiscale dynamical systems, focusing on neural emulators and stochastic\nclimate modeling. A skillful climate model should capture both stationary\nstatistics and responses to external perturbations. While current\nautoregressive neural models often reproduce the former, they typically\nstruggle with the latter. We begin by analyzing a low-dimensional dynamical\nsystem to expose, by analogy, fundamental limitations that persist in\nhigh-dimensional settings. Specifically, we construct neural stochastic models\nunder two scenarios: one where the full state vector is observed, and another\nwith only partial observations (i.e. a subset of variables). In the first case,\nthe models accurately capture both equilibrium statistics and forced responses\nin ensemble mean and variance. In the more realistic case of partial\nobservations, two key challenges emerge: (i) identifying the \\textit{proper}\nvariables to model, and (ii) parameterizing the influence of unobserved degrees\nof freedom. These issues are not specific to neural networks but reflect\nfundamental limitations of data-driven modeling and the need to target the slow\ndynamics of the system. We argue that physically grounded strategies -- such as\ncoarse-graining and stochastic parameterizations -- are critical, both\nconceptually and practically, for the skillful emulation of complex systems\nlike the coupled climate system. Building on these insights, we turn to a more\nrealistic application: a stochastic reduced neural model of the sea surface\ntemperature field and the net radiative flux at the top of the atmosphere,\nassessing its stationary statistics, response to temperature forcing, and\ninterpretability.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22552v1", "categories": ["nlin.CD", "cond-mat.stat-mech", "cs.LG", "physics.ao-ph"], "cate": "nlin.CD", "url": "http://arxiv.org/abs/2506.22552v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.22978", "title": "A Systematic Study of Compositional Syntactic Transformer Language Models", "authors": ["Yida Zhao", "Hao Xve", "Xiang Hu", "Kewei Tu"], "summary": "Syntactic language models (SLMs) enhance Transformers by incorporating\nsyntactic biases through the modeling of linearized syntactic parse trees\nalongside surface sentences. This paper focuses on compositional SLMs that are\nbased on constituency parse trees and contain explicit bottom-up composition of\nconstituent representations. We identify key aspects of design choices in\nexisting compositional SLMs and propose a unified framework encompassing both\nexisting models and novel variants. We conduct a comprehensive empirical\nevaluation of all the variants in our framework across language modeling,\nsyntactic generalization, summarization, dialogue, and inference efficiency.\nBased on the experimental results, we make multiple recommendations on the\ndesign of compositional SLMs. Our code is released at\nhttps://github.com/zhaoyd1/compositional_SLMs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22978v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.22978v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23468", "title": "NavMorph: A Self-Evolving World Model for Vision-and-Language Navigation in Continuous Environments", "authors": ["Xuan Yao", "Junyu Gao", "Changsheng Xu"], "summary": "Vision-and-Language Navigation in Continuous Environments (VLN-CE) requires\nagents to execute sequential navigation actions in complex environments guided\nby natural language instructions. Current approaches often struggle with\ngeneralizing to novel environments and adapting to ongoing changes during\nnavigation. Inspired by human cognition, we present NavMorph, a self-evolving\nworld model framework that enhances environmental understanding and\ndecision-making in VLN-CE tasks. NavMorph employs compact latent\nrepresentations to model environmental dynamics, equipping agents with\nforesight for adaptive planning and policy refinement. By integrating a novel\nContextual Evolution Memory, NavMorph leverages scene-contextual information to\nsupport effective navigation while maintaining online adaptability. Extensive\nexperiments demonstrate that our method achieves notable performance\nimprovements on popular VLN-CE benchmarks. Code is available at\n\\href{https://github.com/Feliciaxyao/NavMorph}{this https URL}.", "comment": "Accepted by ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2506.23468v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23468v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22555", "title": "Spectral Bias in Variational Quantum Machine Learning", "authors": ["Callum Duffy", "Marcin Jastrzebski"], "summary": "In this work, we investigate the phenomenon of spectral bias in quantum\nmachine learning, where, in classical settings, models tend to fit\nlow-frequency components of a target function earlier during training than\nhigh-frequency ones, demonstrating a frequency-dependent rate of convergence.\nWe study this effect specifically in parameterised quantum circuits (PQCs).\nLeveraging the established formulation of PQCs as Fourier series, we prove that\nspectral bias in this setting arises from the ``redundancy'' of the Fourier\ncoefficients, which denotes the number of terms in the analytical form of the\nmodel contributing to the same frequency component. The choice of data encoding\nscheme dictates the degree of redundancy for a Fourier coefficient. We find\nthat the magnitude of the Fourier coefficients' gradients during training\nstrongly correlates with the coefficients' redundancy. We then further\ndemonstrate this empirically with three different encoding schemes.\nAdditionally, we demonstrate that PQCs with greater redundancy exhibit\nincreased robustness to random perturbations in their parameters at the\ncorresponding frequencies. We investigate how design choices affect the ability\nof PQCs to learn Fourier sums, focusing on parameter initialization scale and\nentanglement structure, finding large initializations and low-entanglement\nschemes tend to slow convergence.", "comment": "12 pages, 8 figures", "pdf_url": "http://arxiv.org/pdf/2506.22555v1", "categories": ["quant-ph", "cs.LG"], "cate": "quant-ph", "url": "http://arxiv.org/abs/2506.22555v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.23014", "title": "Generating Privacy Stories From Software Documentation", "authors": ["Wilder Baldwin", "Shashank Chintakuntla", "Shreyah Parajuli", "Ali Pourghasemi", "Ryan Shanz", "Sepideh Ghanavati"], "summary": "Research shows that analysts and developers consider privacy as a security\nconcept or as an afterthought, which may lead to non-compliance and violation\nof users' privacy. Most current approaches, however, focus on extracting legal\nrequirements from the regulations and evaluating the compliance of software and\nprocesses with them. In this paper, we develop a novel approach based on\nchain-of-thought prompting (CoT), in-context-learning (ICL), and Large Language\nModels (LLMs) to extract privacy behaviors from various software documents\nprior to and during software development, and then generate privacy\nrequirements in the format of user stories. Our results show that most commonly\nused LLMs, such as GPT-4o and Llama 3, can identify privacy behaviors and\ngenerate privacy user stories with F1 scores exceeding 0.8. We also show that\nthe performance of these models could be improved through parameter-tuning. Our\nfindings provide insight into using and optimizing LLMs for generating privacy\nrequirements given software documents created prior to or throughout the\nsoftware development lifecycle.", "comment": "Accepted to RENext!'25 at the 33rd IEEE International Requirements\n  Engineering 2025 conference", "pdf_url": "http://arxiv.org/pdf/2506.23014v1", "categories": ["cs.SE", "cs.AI"], "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.23014v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23470", "title": "Interactive Interface For Semantic Segmentation Dataset Synthesis", "authors": ["Ngoc-Do Tran", "Minh-Tuan Huynh", "Tam V. Nguyen", "Minh-Triet Tran", "Trung-Nghia Le"], "summary": "The rapid advancement of AI and computer vision has significantly increased\nthe demand for high-quality annotated datasets, particularly for semantic\nsegmentation. However, creating such datasets is resource-intensive, requiring\nsubstantial time, labor, and financial investment, and often raises privacy\nconcerns due to the use of real-world data. To mitigate these challenges, we\npresent SynthLab, consisting of a modular platform for visual data synthesis\nand a user-friendly interface. The modular architecture of SynthLab enables\neasy maintenance, scalability with centralized updates, and seamless\nintegration of new features. Each module handles distinct aspects of computer\nvision tasks, enhancing flexibility and adaptability. Meanwhile, its\ninteractive, user-friendly interface allows users to quickly customize their\ndata pipelines through drag-and-drop actions. Extensive user studies involving\na diverse range of users across different ages, professions, and expertise\nlevels, have demonstrated flexible usage, and high accessibility of SynthLab,\nenabling users without deep technical expertise to harness AI for real-world\napplications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23470v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23470v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22557", "title": "MetaCipher: A General and Extensible Reinforcement Learning Framework for Obfuscation-Based Jailbreak Attacks on Black-Box LLMs", "authors": ["Boyuan Chen", "Minghao Shao", "Abdul Basit", "Siddharth Garg", "Muhammad Shafique"], "summary": "The growing capabilities of large language models (LLMs) have exposed them to\nincreasingly sophisticated jailbreak attacks. Among these, obfuscation-based\nattacks -- which encrypt malicious content to evade detection -- remain highly\neffective. By leveraging the reasoning ability of advanced LLMs to interpret\nencrypted prompts, such attacks circumvent conventional defenses that rely on\nkeyword detection or context filtering. These methods are very difficult to\ndefend against, as existing safety mechanisms are not designed to interpret or\ndecode ciphered content. In this work, we propose \\textbf{MetaCipher}, a novel\nobfuscation-based jailbreak framework, along with a reinforcement\nlearning-based dynamic cipher selection mechanism that adaptively chooses\noptimal encryption strategies from a cipher pool. This approach enhances\njailbreak effectiveness and generalizability across diverse task types, victim\nLLMs, and safety guardrails. Our framework is modular and extensible by design,\nsupporting arbitrary cipher families and accommodating evolving adversarial\nstrategies. We complement our method with a large-scale empirical analysis of\ncipher performance across multiple victim LLMs. Within as few as 10 queries,\nMetaCipher achieves over 92\\% attack success rate (ASR) on most recent standard\nmalicious prompt benchmarks against state-of-the-art non-reasoning LLMs, and\nover 74\\% ASR against reasoning-capable LLMs, outperforming all existing\nobfuscation-based jailbreak methods. These results highlight the long-term\nrobustness and adaptability of our approach, making it more resilient than\nprior methods in the face of advancing safety measures.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22557v1", "categories": ["cs.CR", "cs.LG"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.22557v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.23023", "title": "Scenario-Based Hierarchical Reinforcement Learning for Automated Driving Decision Making", "authors": ["M. Youssef Abdelhamid", "Lennart Vater", "Zlatan Ajanovic"], "summary": "Developing decision-making algorithms for highly automated driving systems\nremains challenging, since these systems have to operate safely in an open and\ncomplex environments. Reinforcement Learning (RL) approaches can learn\ncomprehensive decision policies directly from experience and already show\npromising results in simple driving tasks. However, current approaches fail to\nachieve generalizability for more complex driving tasks and lack learning\nefficiency. Therefore, we present Scenario-based Automated Driving\nReinforcement Learning (SAD-RL), the first framework that integrates\nReinforcement Learning (RL) of hierarchical policy in a scenario-based\nenvironment. A high-level policy selects maneuver templates that are evaluated\nand executed by a low-level control logic. The scenario-based environment\nallows to control the training experience for the agent and to explicitly\nintroduce challenging, but rate situations into the training process. Our\nexperiments show that an agent trained using the SAD-RL framework can achieve\nsafe behaviour in easy as well as challenging situations efficiently. Our\nablation studies confirmed that both HRL and scenario diversity are essential\nfor achieving these results.", "comment": "6 pages, 10 figures, submitted to a conference", "pdf_url": "http://arxiv.org/pdf/2506.23023v1", "categories": ["cs.RO", "cs.AI", "cs.LG"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.23023v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23478", "title": "GeoCD: A Differential Local Approximation for Geodesic Chamfer Distance", "authors": ["Pedro Alonso", "Tianrui Li", "Chongshou Li"], "summary": "Chamfer Distance (CD) is a widely adopted metric in 3D point cloud learning\ndue to its simplicity and efficiency. However, it suffers from a fundamental\nlimitation: it relies solely on Euclidean distances, which often fail to\ncapture the intrinsic geometry of 3D shapes. To address this limitation, we\npropose GeoCD, a topology-aware and fully differentiable approximation of\ngeodesic distance designed to serve as a metric for 3D point cloud learning.\nOur experiments show that GeoCD consistently improves reconstruction quality\nover standard CD across various architectures and datasets. We demonstrate this\nby fine-tuning several models, initially trained with standard CD, using GeoCD.\nRemarkably, fine-tuning for a single epoch with GeoCD yields significant gains\nacross multiple evaluation metrics.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23478v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23478v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22565", "title": "Adjoint Schr√∂dinger Bridge Sampler", "authors": ["Guan-Horng Liu", "Jaemoo Choi", "Yongxin Chen", "Benjamin Kurt Miller", "Ricky T. Q. Chen"], "summary": "Computational methods for learning to sample from the Boltzmann distribution\n-- where the target distribution is known only up to an unnormalized energy\nfunction -- have advanced significantly recently. Due to the lack of explicit\ntarget samples, however, prior diffusion-based methods, known as diffusion\nsamplers, often require importance-weighted estimation or complicated learning\nprocesses. Both trade off scalability with extensive evaluations of the energy\nand model, thereby limiting their practical usage. In this work, we propose\nAdjoint Schr\\\"odinger Bridge Sampler (ASBS), a new diffusion sampler that\nemploys simple and scalable matching-based objectives yet without the need to\nestimate target samples during training. ASBS is grounded on a mathematical\nmodel -- the Schr\\\"odinger Bridge -- which enhances sampling efficiency via\nkinetic-optimal transportation. Through a new lens of stochastic optimal\ncontrol theory, we demonstrate how SB-based diffusion samplers can be learned\nat scale via Adjoint Matching and prove convergence to the global solution.\nNotably, ASBS generalizes the recent Adjoint Sampling (Havens et al., 2025) to\narbitrary source distributions by relaxing the so-called memoryless condition\nthat largely restricts the design space. Through extensive experiments, we\ndemonstrate the effectiveness of ASBS on sampling from classical energy\nfunctions, amortized conformer generation, and molecular Boltzmann\ndistributions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22565v1", "categories": ["stat.ML", "cs.LG", "math.OC"], "cate": "stat.ML", "url": "http://arxiv.org/abs/2506.22565v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.23024", "title": "BWLer: Barycentric Weight Layer Elucidates a Precision-Conditioning Tradeoff for PINNs", "authors": ["Jerry Liu", "Yasa Baig", "Denise Hui Jean Lee", "Rajat Vadiraj Dwaraknath", "Atri Rudra", "Chris R√©"], "summary": "Physics-informed neural networks (PINNs) offer a flexible way to solve\npartial differential equations (PDEs) with machine learning, yet they still\nfall well short of the machine-precision accuracy many scientific tasks demand.\nIn this work, we investigate whether the precision ceiling comes from the\nill-conditioning of the PDEs or from the typical multi-layer perceptron (MLP)\narchitecture. We introduce the Barycentric Weight Layer (BWLer), which models\nthe PDE solution through barycentric polynomial interpolation. A BWLer can be\nadded on top of an existing MLP (a BWLer-hat) or replace it completely\n(explicit BWLer), cleanly separating how we represent the solution from how we\ntake derivatives for the PDE loss. Using BWLer, we identify fundamental\nprecision limitations within the MLP: on a simple 1-D interpolation task, even\nMLPs with O(1e5) parameters stall around 1e-8 RMSE -- about eight orders above\nfloat64 machine precision -- before any PDE terms are added. In PDE learning,\nadding a BWLer lifts this ceiling and exposes a tradeoff between achievable\naccuracy and the conditioning of the PDE loss. For linear PDEs we fully\ncharacterize this tradeoff with an explicit error decomposition and navigate it\nduring training with spectral derivatives and preconditioning. Across five\nbenchmark PDEs, adding a BWLer on top of an MLP improves RMSE by up to 30x for\nconvection, 10x for reaction, and 1800x for wave equations while remaining\ncompatible with first-order optimizers. Replacing the MLP entirely lets an\nexplicit BWLer reach near-machine-precision on convection, reaction, and wave\nproblems (up to 10 billion times better than prior results) and match the\nperformance of standard PINNs on stiff Burgers' and irregular-geometry Poisson\nproblems. Together, these findings point to a practical path for combining the\nflexibility of PINNs with the precision of classical spectral solvers.", "comment": "Workshop for the Theory of AI for Scientific Computing @ COLT 2025\n  (Best Paper). 39 pages, 24 figures", "pdf_url": "http://arxiv.org/pdf/2506.23024v1", "categories": ["cs.LG", "cs.AI", "cs.NA", "math.NA"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.23024v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23479", "title": "Instant GaussianImage: A Generalizable and Self-Adaptive Image Representation via 2D Gaussian Splatting", "authors": ["Zhaojie Zeng", "Yuesong Wang", "Chao Yang", "Tao Guan", "Lili Ju"], "summary": "Implicit Neural Representation (INR) has demonstrated remarkable advances in\nthe field of image representation but demands substantial GPU resources.\nGaussianImage recently pioneered the use of Gaussian Splatting to mitigate this\ncost, however, the slow training process limits its practicality, and the fixed\nnumber of Gaussians per image limits its adaptability to varying information\nentropy. To address these issues, we propose in this paper a generalizable and\nself-adaptive image representation framework based on 2D Gaussian Splatting.\nOur method employs a network to quickly generate a coarse Gaussian\nrepresentation, followed by minimal fine-tuning steps, achieving comparable\nrendering quality of GaussianImage while significantly reducing training time.\nMoreover, our approach dynamically adjusts the number of Gaussian points based\non image complexity to further enhance flexibility and efficiency in practice.\nExperiments on DIV2K and Kodak datasets show that our method matches or exceeds\nGaussianImage's rendering performance with far fewer iterations and shorter\ntraining times. Specifically, our method reduces the training time by up to one\norder of magnitude while achieving superior rendering performance with the same\nnumber of Gaussians.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23479v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23479v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22606", "title": "A User-Centric, Privacy-Preserving, and Verifiable Ecosystem for Personal Data Management and Utilization", "authors": ["Osama Zafar", "Mina Namazi", "Yuqiao Xu", "Youngjin Yoo", "Erman Ayday"], "summary": "In the current paradigm of digital personalized services, the centralized\nmanagement of personal data raises significant privacy concerns, security\nvulnerabilities, and diminished individual autonomy over sensitive information.\nDespite their efficiency, traditional centralized architectures frequently fail\nto satisfy rigorous privacy requirements and expose users to data breaches and\nunauthorized access risks. This pressing challenge calls for a fundamental\nparadigm shift in methodologies for collecting, storing, and utilizing personal\ndata across diverse sectors, including education, healthcare, and finance.\n  This paper introduces a novel decentralized, privacy-preserving architecture\nthat handles heterogeneous personal information, ranging from educational\ncredentials to health records and financial data. Unlike traditional models,\nour system grants users complete data ownership and control, allowing them to\nselectively share information without compromising privacy. The architecture's\nfoundation comprises advanced privacy-enhancing technologies, including secure\nenclaves and federated learning, enabling secure computation, verification, and\ndata sharing. The system supports diverse functionalities, including local\ncomputation, model training, and privacy-preserving data sharing, while\nensuring data credibility and robust user privacy.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22606v1", "categories": ["cs.CR", "cs.LG"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.22606v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.23025", "title": "Spectra 1.1: Scaling Laws and Efficient Inference for Ternary Language Models", "authors": ["Tejas Vaidhya", "Ayush Kaushal", "Vineet Jain", "Francis Couture Harpin", "Prashant Shishodia", "Majid Behbahani", "Yuriy Nevmyvaka", "Irina Rish"], "summary": "Large language models (LLMs) are increasingly used across research and\nindustry applications, yet their inference efficiency remains a significant\nchallenge. As the computational power of modern GPU architectures continuously\nimproves, their memory bandwidth and capacity have not scaled proportionally,\ncreating a critical bottleneck during inference. To address this, we\ninvestigate ternary language models (TriLMs) that employ quantization-aware\ntraining to significantly reduce memory requirements. We first analyze the\nscalability of TriLMs by conducting a scaling law analysis, revealing that\nTriLMs benefit more from increasing training data than from scaling model\nparameters. Based on this observation, we introduce Spectra-1.1, an open suite\nof TriLMs trained on up to 1.2 trillion tokens, demonstrating sustained\nperformance gains at scale. Furthermore, to improve inference efficiency, we\npropose novel 2-bit and 1.6-bit packing schemes for ternary weights, which\ndemonstrate accelerated inference across various CPU architectures. Also,\nbuilding on the 2-bit packing, we develop a GPU kernel called TriRun that\naccelerates end-to-end model inference by up to 5 times compared to\nfloating-point baselines. To encourage further exploration and development of\nTriLMs, we will release the Spectra-1.1 suite and TriRun inference kernels.\nOverall, our work lays the foundation for building and deploying efficient\nLLMs, providing a valuable resource for the research community.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23025v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.23025v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23481", "title": "Evaluation of Geolocation Capabilities of Multimodal Large Language Models and Analysis of Associated Privacy Risks", "authors": ["Xian Zhang", "Xiang Cheng"], "summary": "Objectives: The rapid advancement of Multimodal Large Language Models (MLLMs)\nhas significantly enhanced their reasoning capabilities, enabling a wide range\nof intelligent applications. However, these advancements also raise critical\nconcerns regarding privacy and ethics. MLLMs are now capable of inferring the\ngeographic location of images -- such as those shared on social media or\ncaptured from street views -- based solely on visual content, thereby posing\nserious risks of privacy invasion, including doxxing, surveillance, and other\nsecurity threats.\n  Methods: This study provides a comprehensive analysis of existing geolocation\ntechniques based on MLLMs. It systematically reviews relevant litera-ture and\nevaluates the performance of state-of-the-art visual reasoning models on\ngeolocation tasks, particularly in identifying the origins of street view\nimagery.\n  Results: Empirical evaluation reveals that the most advanced visual large\nmodels can successfully localize the origin of street-level imagery with up to\n$49\\%$ accuracy within a 1-kilometer radius. This performance underscores the\nmodels' powerful capacity to extract and utilize fine-grained geographic cues\nfrom visual data.\n  Conclusions: Building on these findings, the study identifies key visual\nelements that contribute to suc-cessful geolocation, such as text,\narchitectural styles, and environmental features. Furthermore, it discusses the\npotential privacy implications associated with MLLM-enabled geolocation and\ndiscuss several technical and policy-based coun-termeasures to mitigate\nassociated risks. Our code and dataset are available at\nhttps://github.com/zxyl1003/MLLM-Geolocation-Evaluation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23481v1", "categories": ["cs.CV", "eess.IV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23481v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22607", "title": "Learning Individual Reproductive Behavior from Aggregate Fertility Rates via Neural Posterior Estimation", "authors": ["Daniel Ciganda", "Ignacio Camp√≥n", "I√±aki Permanyer", "Jakob H Macke"], "summary": "While age-specific fertility rates (ASFRs) provide the most extensive record\nof reproductive change, their aggregate nature masks the underlying behavioral\nmechanisms that ultimately drive fertility trends. To recover these mechanisms,\nwe develop a likelihood-free Bayesian framework that couples an\nindividual-level model of the reproductive process with Sequential Neural\nPosterior Estimation (SNPE). This allows us to infer eight behavioral and\nbiological parameters from just two aggregate series: ASFRs and the age-profile\nof planned versus unplanned births. Applied to U.S. National Survey of Family\nGrowth cohorts and to Demographic and Health Survey cohorts from Colombia, the\nDominican Republic, and Peru, the method reproduces observed fertility\nschedules and, critically, predicts out-of-sample micro-level distributions of\nage at first sex, inter-birth intervals, and family-size ideals, none of which\ninform the estimation step. Because the fitted model yields complete synthetic\nlife histories, it enables behaviorally explicit population forecasts and\nsupports the construction of demographic digital twins.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22607v1", "categories": ["stat.AP", "cs.LG"], "cate": "stat.AP", "url": "http://arxiv.org/abs/2506.22607v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.23030", "title": "VisionScores -- A system-segmented image score dataset for deep learning tasks", "authors": ["Alejandro Romero Amezcua", "Mariano Jos√© Juan Rivera Meraz"], "summary": "VisionScores presents a novel proposal being the first system-segmented image\nscore dataset, aiming to offer structure-rich, high information-density images\nfor machine and deep learning tasks. Delimited to two-handed piano pieces, it\nwas built to consider not only certain graphic similarity but also composition\npatterns, as this creative process is highly instrument-dependent. It provides\ntwo scenarios in relation to composer and composition type. The first, formed\nby 14k samples, considers works from different authors but the same composition\ntype, specifically, Sonatinas. The latter, consisting of 10.8K samples,\npresents the opposite case, various composition types from the same author,\nbeing the one selected Franz Liszt. All of the 24.8k samples are formatted as\ngrayscale jpg images of $128 \\times 512$ pixels. VisionScores supplies the\nusers not only the formatted samples but the systems' order and pieces'\nmetadata. Moreover, unsegmented full-page scores and the pre-formatted images\nare included for further analysis.", "comment": "Comments: 5 pages, 3 figures. Accepted for presentation at the 2025\n  IEEE International Conference on Image Processing (ICIP). \\c{opyright} 2025\n  IEEE. Personal use of this material is permitted. Permission from IEEE must\n  be obtained for any other use", "pdf_url": "http://arxiv.org/pdf/2506.23030v1", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23030v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23482", "title": "MTADiffusion: Mask Text Alignment Diffusion Model for Object Inpainting", "authors": ["Jun Huang", "Ting Liu", "Yihang Wu", "Xiaochao Qu", "Luoqi Liu", "Xiaolin Hu"], "summary": "Advancements in generative models have enabled image inpainting models to\ngenerate content within specific regions of an image based on provided prompts\nand masks. However, existing inpainting methods often suffer from problems such\nas semantic misalignment, structural distortion, and style inconsistency. In\nthis work, we present MTADiffusion, a Mask-Text Alignment diffusion model\ndesigned for object inpainting. To enhance the semantic capabilities of the\ninpainting model, we introduce MTAPipeline, an automatic solution for\nannotating masks with detailed descriptions. Based on the MTAPipeline, we\nconstruct a new MTADataset comprising 5 million images and 25 million mask-text\npairs. Furthermore, we propose a multi-task training strategy that integrates\nboth inpainting and edge prediction tasks to improve structural stability. To\npromote style consistency, we present a novel inpainting style-consistency loss\nusing a pre-trained VGG network and the Gram matrix. Comprehensive evaluations\non BrushBench and EditBench demonstrate that MTADiffusion achieves\nstate-of-the-art performance compared to other methods.", "comment": "CVPR 2025", "pdf_url": "http://arxiv.org/pdf/2506.23482v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23482v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22611", "title": "Deep Hedging to Manage Tail Risk", "authors": ["Yuming Ma"], "summary": "Extending Buehler et al.'s 2019 Deep Hedging paradigm, we innovatively employ\ndeep neural networks to parameterize convex-risk minimization (CVaR/ES) for the\nportfolio tail-risk hedging problem. Through comprehensive numerical\nexperiments on crisis-era bootstrap market simulators -- customizable with\ntransaction costs, risk budgets, liquidity constraints, and market impact --\nour end-to-end framework not only achieves significant one-day 99% CVaR\nreduction but also yields practical insights into friction-aware strategy\nadaptation, demonstrating robustness and operational viability in realistic\nmarkets.", "comment": "59 pages", "pdf_url": "http://arxiv.org/pdf/2506.22611v1", "categories": ["q-fin.PM", "cs.LG", "math.OC", "q-fin.CP", "q-fin.RM", "91G70 91G20 91G60"], "cate": "q-fin.PM", "url": "http://arxiv.org/abs/2506.22611v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.23040", "title": "Treatment, evidence, imitation, and chat", "authors": ["Samuel J. Weisenthal"], "summary": "Large language models are thought to have potential to aid in medical\ndecision making. We investigate this here. We start with the treatment problem,\nthe patient's core medical decision-making task, which is solved in\ncollaboration with a healthcare provider. We discuss approaches to solving the\ntreatment problem, including -- within evidence-based medicine -- trials and\nobservational data. We then discuss the chat problem, and how this differs from\nthe treatment problem -- in particular as it relates to imitation. We then\ndiscuss how a large language model might be used to solve the treatment problem\nand highlight some of the challenges that emerge. We finally discuss how these\nchallenges relate to evidence-based medicine, and how this might inform next\nsteps.", "comment": "12 pages", "pdf_url": "http://arxiv.org/pdf/2506.23040v1", "categories": ["stat.OT", "cs.AI"], "cate": "stat.OT", "url": "http://arxiv.org/abs/2506.23040v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23491", "title": "Qwen-GUI-3B: A Lightweight Vision-Language Model for Cross-Resolution GUI Grounding", "authors": ["ZongHan Hsieh", "Tzer-Jen Wei"], "summary": "This paper introduces Qwen-GUI-3B, a lightweight Vision-Language Model (VLM)\nspecifically designed for Graphical User Interface grounding tasks, achieving\nperformance competitive with significantly larger models. Unlike large-scale\nVLMs (>7B parameters) that are computationally intensive and impractical for\nconsumer-grade hardware, Qwen-GUI-3B delivers strong grounding accuracy while\nbeing fully trainable on a single GPU (RTX 4090). The model incorporates\nseveral key innovations: (i) combine cross-platform, multi-resolution dataset\nof 24K examples from diverse sources including mobile, desktop, and web GUI\nscreenshots to effectively address data scarcity in high-resolution desktop\nenvironments; (ii) a two-stage fine-tuning strategy, where initial\ncross-platform training establishes robust GUI understanding, followed by\nspecialized fine-tuning on high-resolution data to significantly enhance model\nadaptability; and (iii) data curation and redundancy reduction strategies,\ndemonstrating that randomly sampling a smaller subset with reduced redundancy\nachieves performance comparable to larger datasets, emphasizing data diversity\nover sheer volume. Empirical evaluation on standard GUI grounding\nbenchmarks-including ScreenSpot, ScreenSpot-v2, and the challenging\nScreenSpot-Pro, highlights Qwen-GUI-3B's exceptional accuracy, achieving 84.9%\non ScreenSpot and 86.4% on ScreenSpot-v2, surpassing prior models under 4B\nparameters. Ablation studies validate the critical role of balanced sampling\nand two-stage fine-tuning in enhancing robustness, particularly in\nhigh-resolution desktop scenarios. The Qwen-GUI-3B is available at:\nhttps://github.com/Han1018/Qwen-GUI-3B", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23491v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23491v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22641", "title": "Diversity by Design: Addressing Mode Collapse Improves scRNA-seq Perturbation Modeling on Well-Calibrated Metrics", "authors": ["Gabriel M. Mejia", "Henry E. Miller", "Francis J. A. Leblanc", "Bo Wang", "Brendan Swain", "Lucas Paulo de Lima Camillo"], "summary": "Recent benchmarks reveal that models for single-cell perturbation response\nare often outperformed by simply predicting the dataset mean. We trace this\nanomaly to a metric artifact: control-referenced deltas and unweighted error\nmetrics reward mode collapse whenever the control is biased or the biological\nsignal is sparse. Large-scale \\textit{in silico} simulations and analysis of\ntwo real-world perturbation datasets confirm that shared reference shifts, not\ngenuine biological change, drives high performance in these evaluations. We\nintroduce differentially expressed gene (DEG)-aware metrics, weighted\nmean-squared error (WMSE) and weighted delta $R^{2}$ ($R^{2}_{w}(\\Delta)$) with\nrespect to all perturbations, that measure error in niche signals with high\nsensitivity. We further introduce negative and positive performance baselines\nto calibrate these metrics. With these improvements, the mean baseline sinks to\nnull performance while genuine predictors are correctly rewarded. Finally, we\nshow that using WMSE as a loss function reduces mode collapse and improves\nmodel performance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22641v1", "categories": ["q-bio.GN", "cs.LG", "q-bio.MN", "stat.ML"], "cate": "q-bio.GN", "url": "http://arxiv.org/abs/2506.22641v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.23044", "title": "Ovis-U1 Technical Report", "authors": ["Guo-Hua Wang", "Shanshan Zhao", "Xinjie Zhang", "Liangfu Cao", "Pengxin Zhan", "Lunhao Duan", "Shiyin Lu", "Minghao Fu", "Xiaohao Chen", "Jianshan Zhao", "Yang Li", "Qing-Guo Chen"], "summary": "In this report, we introduce Ovis-U1, a 3-billion-parameter unified model\nthat integrates multimodal understanding, text-to-image generation, and image\nediting capabilities. Building on the foundation of the Ovis series, Ovis-U1\nincorporates a diffusion-based visual decoder paired with a bidirectional token\nrefiner, enabling image generation tasks comparable to leading models like\nGPT-4o. Unlike some previous models that use a frozen MLLM for generation\ntasks, Ovis-U1 utilizes a new unified training approach starting from a\nlanguage model. Compared to training solely on understanding or generation\ntasks, unified training yields better performance, demonstrating the\nenhancement achieved by integrating these two tasks. Ovis-U1 achieves a score\nof 69.6 on the OpenCompass Multi-modal Academic Benchmark, surpassing recent\nstate-of-the-art models such as Ristretto-3B and SAIL-VL-1.5-2B. In\ntext-to-image generation, it excels with scores of 83.72 and 0.89 on the\nDPG-Bench and GenEval benchmarks, respectively. For image editing, it achieves\n4.00 and 6.42 on the ImgEdit-Bench and GEdit-Bench-EN, respectively. As the\ninitial version of the Ovis unified model series, Ovis-U1 pushes the boundaries\nof multimodal understanding, generation, and editing.", "comment": "A unified model for multimodal understanding, text-to-image\n  generation, and image editing. GitHub: https://github.com/AIDC-AI/Ovis-U1", "pdf_url": "http://arxiv.org/pdf/2506.23044v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23044v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23502", "title": "LLM-enhanced Action-aware Multi-modal Prompt Tuning for Image-Text Matching", "authors": ["Mengxiao Tian", "Xinxiao Wu", "Shuo Yang"], "summary": "Driven by large-scale contrastive vision-language pre-trained models such as\nCLIP, recent advancements in the image-text matching task have achieved\nremarkable success in representation learning. Due to image-level\nvisual-language alignment, CLIP falls short in understanding fine-grained\ndetails such as object attributes and spatial relationships between objects.\nRecent efforts have attempted to compel CLIP to acquire structured visual\nrepresentations by introducing prompt learning to achieve object-level\nalignment. While achieving promising results, they still lack the capability to\nperceive actions, which are crucial for describing the states or relationships\nbetween objects. Therefore, we propose to endow CLIP with fine-grained\naction-level understanding by introducing an LLM-enhanced action-aware\nmulti-modal prompt-tuning method, incorporating the action-related external\nknowledge generated by large language models (LLMs). Specifically, we design an\naction triplet prompt and an action state prompt to exploit compositional\nsemantic knowledge and state-related causal knowledge implicitly stored in\nLLMs. Subsequently, we propose an adaptive interaction module to aggregate\nattentive visual features conditioned on action-aware prompted knowledge for\nestablishing discriminative and action-aware visual representations, which\nfurther improves the performance. Comprehensive experimental results on two\nbenchmark datasets demonstrate the effectiveness of our method.", "comment": "accepted by ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2506.23502v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23502v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22648", "title": "Interact2Vec -- An efficient neural network-based model for simultaneously learning users and items embeddings in recommender systems", "authors": ["Pedro R. Pires", "Tiago A. Almeida"], "summary": "Over the past decade, recommender systems have experienced a surge in\npopularity. Despite notable progress, they grapple with challenging issues,\nsuch as high data dimensionality and sparseness. Representing users and items\nas low-dimensional embeddings learned via neural networks has become a leading\nsolution. However, while recent studies show promising results, many approaches\nrely on complex architectures or require content data, which may not always be\navailable. This paper presents Interact2Vec, a novel neural network-based model\nthat simultaneously learns distributed embeddings for users and items while\ndemanding only implicit feedback. The model employs state-of-the-art strategies\nthat natural language processing models commonly use to optimize the training\nphase and enhance the final embeddings. Two types of experiments were conducted\nregarding the extrinsic and intrinsic quality of the model. In the former, we\nbenchmarked the recommendations generated by Interact2Vec's embeddings in a\ntop-$N$ ranking problem, comparing them with six other recommender algorithms.\nThe model achieved the second or third-best results in 30\\% of the datasets,\nbeing competitive with other recommenders, and has proven to be very efficient\nwith an average training time reduction of 274\\% compared to other\nembedding-based models. Later, we analyzed the intrinsic quality of the\nembeddings through similarity tables. Our findings suggest that Interact2Vec\ncan achieve promising results, especially on the extrinsic task, and is an\nexcellent embedding-generator model for scenarios of scarce computing\nresources, enabling the learning of item and user embeddings simultaneously and\nefficiently.", "comment": "Accepted for publication in Applied Soft Computing (ASOC), 49 pages,\n  14 figures", "pdf_url": "http://arxiv.org/pdf/2506.22648v1", "categories": ["cs.IR", "cs.LG"], "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.22648v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.23046", "title": "SoMi-ToM: Evaluating Multi-Perspective Theory of Mind in Embodied Social Interactions", "authors": ["Xianzhe Fan", "Xuhui Zhou", "Chuanyang Jin", "Kolby Nottingham", "Hao Zhu", "Maarten Sap"], "summary": "Humans continuously infer the states, goals, and behaviors of others by\nperceiving their surroundings in dynamic, real-world social interactions.\nHowever, most Theory of Mind (ToM) benchmarks only evaluate static, text-based\nscenarios, which have a significant gap compared to real interactions. We\npropose the SoMi-ToM benchmark, designed to evaluate multi-perspective ToM in\nembodied multi-agent complex social interactions. This benchmark is based on\nrich multimodal interaction data generated by the interaction environment SoMi,\ncovering diverse crafting goals and social relationships. Our framework\nsupports multi-level evaluation: (1) first-person evaluation provides\nmultimodal (visual, dialogue, action, etc.) input from a first-person\nperspective during a task for real-time state inference, (2) third-person\nevaluation provides complete third-person perspective video and text records\nafter a task for goal and behavior inference. This evaluation method allows for\na more comprehensive examination of a model's ToM capabilities from both the\nsubjective immediate experience and the objective global observation. We\nconstructed a challenging dataset containing 35 third-person perspective\nvideos, 363 first-person perspective images, and 1225 expert-annotated\nmultiple-choice questions (three options). On this dataset, we systematically\nevaluated the performance of human subjects and several state-of-the-art large\nvision-language models (LVLMs). The results show that LVLMs perform\nsignificantly worse than humans on SoMi-ToM: the average accuracy gap between\nhumans and models is 40.1% in first-person evaluation and 26.4% in third-person\nevaluation. This indicates that future LVLMs need to further improve their ToM\ncapabilities in embodied, complex social interactions.", "comment": "23 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2506.23046v1", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.RO"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.23046v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23505", "title": "Improve Underwater Object Detection through YOLOv12 Architecture and Physics-informed Augmentation", "authors": ["Tinh Nguyen"], "summary": "Underwater object detection is crucial for autonomous navigation,\nenvironmental monitoring, and marine exploration, but it is severely hampered\nby light attenuation, turbidity, and occlusion. Current methods balance\naccuracy and computational efficiency, but they have trouble deploying in\nreal-time under low visibility conditions. Through the integration of\nphysics-informed augmentation techniques with the YOLOv12 architecture, this\nstudy advances underwater detection. With Residual ELAN blocks to preserve\nstructural features in turbid waters and Area Attention to maintain large\nreceptive fields for occluded objects while reducing computational complexity.\nUnderwater optical properties are addressed by domain-specific augmentations\nsuch as turbulence adaptive blurring, biologically grounded occlusion\nsimulation, and spectral HSV transformations for color distortion. Extensive\ntests on four difficult datasets show state-of-the-art performance, with\nBrackish data registering 98.30% mAP at 142 FPS. YOLOv12 improves occlusion\nrobustness by 18.9%, small-object recall by 22.4%, and detection precision by\nup to 7.94% compared to previous models. The crucial role of augmentation\nstrategy is validated by ablation studies. This work offers a precise and\neffective solution for conservation and underwater robotics applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23505v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23505v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22666", "title": "VERA: Variational Inference Framework for Jailbreaking Large Language Models", "authors": ["Anamika Lochab", "Lu Yan", "Patrick Pynadath", "Xiangyu Zhang", "Ruqi Zhang"], "summary": "The rise of API-only access to state-of-the-art LLMs highlights the need for\neffective black-box jailbreak methods to identify model vulnerabilities in\nreal-world settings. Without a principled objective for gradient-based\noptimization, most existing approaches rely on genetic algorithms, which are\nlimited by their initialization and dependence on manually curated prompt\npools. Furthermore, these methods require individual optimization for each\nprompt, failing to provide a comprehensive characterization of model\nvulnerabilities. To address this gap, we introduce VERA: Variational infErence\nfRamework for jAilbreaking. VERA casts black-box jailbreak prompting as a\nvariational inference problem, training a small attacker LLM to approximate the\ntarget LLM's posterior over adversarial prompts. Once trained, the attacker can\ngenerate diverse, fluent jailbreak prompts for a target query without\nre-optimization. Experimental results show that VERA achieves strong\nperformance across a range of target LLMs, highlighting the value of\nprobabilistic inference for adversarial prompt generation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22666v1", "categories": ["cs.CR", "cs.CL", "cs.LG", "stat.ML"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.22666v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.23055", "title": "Measuring How LLMs Internalize Human Psychological Concepts: A preliminary analysis", "authors": ["Hiro Taiyo Hamada", "Ippei Fujisawa", "Genji Kawakita", "Yuki Yamada"], "summary": "Large Language Models (LLMs) such as ChatGPT have shown remarkable abilities\nin producing human-like text. However, it is unclear how accurately these\nmodels internalize concepts that shape human thought and behavior. Here, we\ndeveloped a quantitative framework to assess concept alignment between LLMs and\nhuman psychological dimensions using 43 standardized psychological\nquestionnaires, selected for their established validity in measuring distinct\npsychological constructs. Our method evaluates how accurately language models\nreconstruct and classify questionnaire items through pairwise similarity\nanalysis. We compared resulting cluster structures with the original\ncategorical labels using hierarchical clustering. A GPT-4 model achieved\nsuperior classification accuracy (66.2\\%), significantly outperforming GPT-3.5\n(55.9\\%) and BERT (48.1\\%), all exceeding random baseline performance (31.9\\%).\nWe also demonstrated that the estimated semantic similarity from GPT-4 is\nassociated with Pearson's correlation coefficients of human responses in\nmultiple psychological questionnaires. This framework provides a novel approach\nto evaluate the alignment of the human-LLM concept and identify potential\nrepresentational biases. Our findings demonstrate that modern LLMs can\napproximate human psychological constructs with measurable accuracy, offering\ninsights for developing more interpretable AI systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23055v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.23055v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23513", "title": "ViewPoint: Panoramic Video Generation with Pretrained Diffusion Models", "authors": ["Zixun Fang", "Kai Zhu", "Zhiheng Liu", "Yu Liu", "Wei Zhai", "Yang Cao", "Zheng-Jun Zha"], "summary": "Panoramic video generation aims to synthesize 360-degree immersive videos,\nholding significant importance in the fields of VR, world models, and spatial\nintelligence. Existing works fail to synthesize high-quality panoramic videos\ndue to the inherent modality gap between panoramic data and perspective data,\nwhich constitutes the majority of the training data for modern diffusion\nmodels. In this paper, we propose a novel framework utilizing pretrained\nperspective video models for generating panoramic videos. Specifically, we\ndesign a novel panorama representation named ViewPoint map, which possesses\nglobal spatial continuity and fine-grained visual details simultaneously. With\nour proposed Pano-Perspective attention mechanism, the model benefits from\npretrained perspective priors and captures the panoramic spatial correlations\nof the ViewPoint map effectively. Extensive experiments demonstrate that our\nmethod can synthesize highly dynamic and spatially consistent panoramic videos,\nachieving state-of-the-art performance and surpassing previous methods.", "comment": "https://becauseimbatman0.github.io/ViewPoint", "pdf_url": "http://arxiv.org/pdf/2506.23513v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23513v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22675", "title": "Bayesian Invariance Modeling of Multi-Environment Data", "authors": ["Luhuan Wu", "Mingzhang Yin", "Yixin Wang", "John P. Cunningham", "David M. Blei"], "summary": "Invariant prediction [Peters et al., 2016] analyzes feature/outcome data from\nmultiple environments to identify invariant features - those with a stable\npredictive relationship to the outcome. Such features support generalization to\nnew environments and help reveal causal mechanisms. Previous methods have\nprimarily tackled this problem through hypothesis testing or regularized\noptimization. Here we develop Bayesian Invariant Prediction (BIP), a\nprobabilistic model for invariant prediction. BIP encodes the indices of\ninvariant features as a latent variable and recover them by posterior\ninference. Under the assumptions of Peters et al. [2016], the BIP posterior\ntargets the true invariant features. We prove that the posterior is consistent\nand that greater environment heterogeneity leads to faster posterior\ncontraction. To handle many features, we design an efficient variational\napproximation called VI-BIP. In simulations and real data, we find that BIP and\nVI-BIP are more accurate and scalable than existing methods for invariant\nprediction.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22675v1", "categories": ["stat.ML", "cs.LG"], "cate": "stat.ML", "url": "http://arxiv.org/abs/2506.22675v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.23068", "title": "Curious Causality-Seeking Agents Learn Meta Causal World", "authors": ["Zhiyu Zhao", "Haoxuan Li", "Haifeng Zhang", "Jun Wang", "Francesco Faccio", "J√ºrgen Schmidhuber", "Mengyue Yang"], "summary": "When building a world model, a common assumption is that the environment has\na single, unchanging underlying causal rule, like applying Newton's laws to\nevery situation. In reality, what appears as a drifting causal mechanism is\noften the manifestation of a fixed underlying mechanism seen through a narrow\nobservational window. This brings about a problem that, when building a world\nmodel, even subtle shifts in policy or environment states can alter the very\nobserved causal mechanisms. In this work, we introduce the \\textbf{Meta-Causal\nGraph} as world models, a minimal unified representation that efficiently\nencodes the transformation rules governing how causal structures shift across\ndifferent latent world states. A single Meta-Causal Graph is composed of\nmultiple causal subgraphs, each triggered by meta state, which is in the latent\nstate space. Building on this representation, we introduce a\n\\textbf{Causality-Seeking Agent} whose objectives are to (1) identify the meta\nstates that trigger each subgraph, (2) discover the corresponding causal\nrelationships by agent curiosity-driven intervention policy, and (3)\niteratively refine the Meta-Causal Graph through ongoing curiosity-driven\nexploration and agent experiences. Experiments on both synthetic tasks and a\nchallenging robot arm manipulation task demonstrate that our method robustly\ncaptures shifts in causal dynamics and generalizes effectively to previously\nunseen contexts.", "comment": "33 pages", "pdf_url": "http://arxiv.org/pdf/2506.23068v1", "categories": ["cs.LG", "cs.AI", "stat.AP"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.23068v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23518", "title": "WAVE: Warp-Based View Guidance for Consistent Novel View Synthesis Using a Single Image", "authors": ["Jiwoo Park", "Tae Eun Choi", "Youngjun Jun", "Seong Jae Hwang"], "summary": "Generating high-quality novel views of a scene from a single image requires\nmaintaining structural coherence across different views, referred to as view\nconsistency. While diffusion models have driven advancements in novel view\nsynthesis, they still struggle to preserve spatial continuity across views.\nDiffusion models have been combined with 3D models to address the issue, but\nsuch approaches lack efficiency due to their complex multi-step pipelines. This\npaper proposes a novel view-consistent image generation method which utilizes\ndiffusion models without additional modules. Our key idea is to enhance\ndiffusion models with a training-free method that enables adaptive attention\nmanipulation and noise reinitialization by leveraging view-guided warping to\nensure view consistency. Through our comprehensive metric framework suitable\nfor novel-view datasets, we show that our method improves view consistency\nacross various diffusion models, demonstrating its broader applicability.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23518v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23518v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22701", "title": "Lower bounds for trace estimation via Block Krylov and other methods", "authors": ["Shi Jie Yu"], "summary": "This paper studies theoretical lower bounds for estimating the trace of a\nmatrix function, $\\text{tr}(f(A))$, focusing on methods that use Hutchinson's\nmethod along with Block Krylov techniques. These methods work by approximating\nmatrix-vector products like $f(A)V$ using a Block Krylov subspace. This is\nclosely related to approximating functions with polynomials. We derive\ntheoretical upper bounds on how many Krylov steps are needed for functions such\nas $A^{-1/2}$ and $A^{-1}$ by analyzing the upper bounds from the polynomial\napproximation of their scalar equivalent. In addition, we also develop lower\nlimits on the number of queries needed for trace estimation, specifically for\n$\\text{tr}(W^{-p})$ where $W$ is a Wishart matrix. Our study clarifies the\nconnection between the number of steps in Block Krylov methods and the degree\nof the polynomial used for approximation. This links the total cost of trace\nestimation to basic limits in polynomial approximation and how much information\nis needed for the computation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22701v1", "categories": ["math.ST", "cs.DS", "cs.LG", "cs.NA", "math.NA", "stat.TH"], "cate": "math.ST", "url": "http://arxiv.org/abs/2506.22701v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23085", "title": "Enhancing Live Broadcast Engagement: A Multi-modal Approach to Short Video Recommendations Using MMGCN and User Preferences", "authors": ["Saeid Aghasoleymani Najafabadi"], "summary": "The purpose of this paper is to explore a multi-modal approach to enhancing\nlive broadcast engagement by developing a short video recommendation system\nthat incorporates Multi-modal Graph Convolutional Networks (MMGCN) with user\npreferences. In order to provide personalized recommendations tailored to\nindividual interests, the proposed system takes into account user interaction\ndata, video content features, and contextual information. With the aid of a\nhybrid approach combining collaborative filtering and content-based filtering\ntechniques, the system is able to capture nuanced relationships between users,\nvideo attributes, and engagement patterns. Three datasets are used to evaluate\nthe effectiveness of the system: Kwai, TikTok, and MovieLens. Compared to\nbaseline models, such as DeepFM, Wide & Deep, LightGBM, and XGBoost, the\nproposed MMGCN-based model shows superior performance. A notable feature of the\nproposed model is that it outperforms all baseline methods in capturing diverse\nuser preferences and making accurate, personalized recommendations, resulting\nin a Kwai F1 score of 0.574, a Tiktok F1 score of 0.506, and a MovieLens F1\nscore of 0.197. We emphasize the importance of multi-modal integration and\nuser-centric approaches in advancing recommender systems, emphasizing the role\nthey play in enhancing content discovery and audience interaction on live\nbroadcast platforms.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23085v1", "categories": ["cs.IR", "cs.AI"], "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.23085v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23519", "title": "From Sight to Insight: Unleashing Eye-Tracking in Weakly Supervised Video Salient Object Detection", "authors": ["Qi Qin", "Runmin Cong", "Gen Zhan", "Yiting Liao", "Sam Kwong"], "summary": "The eye-tracking video saliency prediction (VSP) task and video salient\nobject detection (VSOD) task both focus on the most attractive objects in video\nand show the result in the form of predictive heatmaps and pixel-level saliency\nmasks, respectively. In practical applications, eye tracker annotations are\nmore readily obtainable and align closely with the authentic visual patterns of\nhuman eyes. Therefore, this paper aims to introduce fixation information to\nassist the detection of video salient objects under weak supervision. On the\none hand, we ponder how to better explore and utilize the information provided\nby fixation, and then propose a Position and Semantic Embedding (PSE) module to\nprovide location and semantic guidance during the feature learning process. On\nthe other hand, we achieve spatiotemporal feature modeling under weak\nsupervision from the aspects of feature selection and feature contrast. A\nSemantics and Locality Query (SLQ) Competitor with semantic and locality\nconstraints is designed to effectively select the most matching and accurate\nobject query for spatiotemporal modeling. In addition, an Intra-Inter Mixed\nContrastive (IIMC) model improves the spatiotemporal modeling capabilities\nunder weak supervision by forming an intra-video and inter-video contrastive\nlearning paradigm. Experimental results on five popular VSOD benchmarks\nindicate that our model outperforms other competitors on various evaluation\nmetrics.", "comment": "15 Pages, 9 Figures", "pdf_url": "http://arxiv.org/pdf/2506.23519v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23519v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22714", "title": "Libra: Synergizing CUDA and Tensor Cores for High-Performance Sparse Matrix Multiplication", "authors": ["Jinliang Shi", "Shigang Li", "Youxuan Xu", "Xueying Wang", "Rongtian Fu", "Zhi Ma", "Tong Wu"], "summary": "Sparse matrix multiplication operators (i.e., SpMM and SDDMM) are widely used\nin deep learning and scientific computing. Modern accelerators are commonly\nequipped with Tensor cores and CUDA cores to accelerate sparse operators. The\nformer brings superior computing power but only for structured matrix\nmultiplication, while the latter has relatively lower performance but with\nhigher programming flexibility. In this work, we discover that utilizing one\nresource alone leads to inferior performance for sparse matrix multiplication,\ndue to their respective limitations. To this end, we propose Libra, a\nsystematic approach that enables synergistic computation between CUDA and\nTensor cores to achieve the best performance for sparse matrix multiplication.\nSpecifically, we propose a 2D-aware workload distribution strategy to find out\nthe sweet point of task mapping for different sparse operators, leveraging both\nthe high performance of Tensor cores and the low computational redundancy on\nCUDA cores. In addition, Libra incorporates systematic optimizations for\nheterogeneous computing, including hybrid load-balancing, finely optimized\nkernel implementations, and GPU-accelerated preprocessing. Extensive\nexperimental results on H100 and RTX 4090 GPUs show that Libra outperforms the\nstate-of-the-art by on average 3.1x (up to 9.23x) over DTC-SpMM and 2.9x (up to\n3.9x) for end-to-end GNN applications. Libra opens up a new perspective for\nsparse operator acceleration by fully exploiting the heterogeneous computing\nresources on GPUs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22714v1", "categories": ["cs.DC", "cs.LG", "cs.PF", "C.1.4; I.2.11"], "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.22714v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23094", "title": "TOMI: Transforming and Organizing Music Ideas for Multi-Track Compositions with Full-Song Structure", "authors": ["Qi He", "Gus Xia", "Ziyu Wang"], "summary": "Hierarchical planning is a powerful approach to model long sequences\nstructurally. Aside from considering hierarchies in the temporal structure of\nmusic, this paper explores an even more important aspect: concept hierarchy,\nwhich involves generating music ideas, transforming them, and ultimately\norganizing them--across musical time and space--into a complete composition. To\nthis end, we introduce TOMI (Transforming and Organizing Music Ideas) as a\nnovel approach in deep music generation and develop a TOMI-based model via\ninstruction-tuned foundation LLM. Formally, we represent a multi-track\ncomposition process via a sparse, four-dimensional space characterized by clips\n(short audio or MIDI segments), sections (temporal positions), tracks\n(instrument layers), and transformations (elaboration methods). Our model is\ncapable of generating multi-track electronic music with full-song structure,\nand we further integrate the TOMI-based model with the REAPER digital audio\nworkstation, enabling interactive human-AI co-creation. Experimental results\ndemonstrate that our approach produces higher-quality electronic music with\nstronger structural coherence compared to baselines.", "comment": "9 pages, 4 figures, 2 tables. To be published in ISMIR 2025", "pdf_url": "http://arxiv.org/pdf/2506.23094v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.23094v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23523", "title": "Lightweight Temporal Transformer Decomposition for Federated Autonomous Driving", "authors": ["Tuong Do", "Binh X. Nguyen", "Quang D. Tran", "Erman Tjiputra", "Te-Chuan Chiu", "Anh Nguyen"], "summary": "Traditional vision-based autonomous driving systems often face difficulties\nin navigating complex environments when relying solely on single-image inputs.\nTo overcome this limitation, incorporating temporal data such as past image\nframes or steering sequences, has proven effective in enhancing robustness and\nadaptability in challenging scenarios. While previous high-performance methods\nexist, they often rely on resource-intensive fusion networks, making them\nimpractical for training and unsuitable for federated learning. To address\nthese challenges, we propose lightweight temporal transformer decomposition, a\nmethod that processes sequential image frames and temporal steering data by\nbreaking down large attention maps into smaller matrices. This approach reduces\nmodel complexity, enabling efficient weight updates for convergence and\nreal-time predictions while leveraging temporal information to enhance\nautonomous driving performance. Intensive experiments on three datasets\ndemonstrate that our method outperforms recent approaches by a clear margin\nwhile achieving real-time performance. Additionally, real robot experiments\nfurther confirm the effectiveness of our method.", "comment": "Accepted in IROS 2025", "pdf_url": "http://arxiv.org/pdf/2506.23523v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23523v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22726", "title": "XTransfer: Cross-Modality Model Transfer for Human Sensing with Few Data at the Edge", "authors": ["Yu Zhang", "Xi Zhang", "Hualin zhou", "Xinyuan Chen", "Shang Gao", "Hong Jia", "Jianfei Yang", "Yuankai Qi", "Tao Gu"], "summary": "Deep learning for human sensing on edge systems offers significant\nopportunities for smart applications. However, its training and development are\nhindered by the limited availability of sensor data and resource constraints of\nedge systems. Current methods that rely on transferring pre-trained models\noften encounter issues such as modality shift and high resource demands,\nresulting in substantial accuracy loss, resource overhead, and poor\nadaptability across different sensing applications. In this paper, we propose\nXTransfer, a first-of-its-kind method for resource-efficient, modality-agnostic\nmodel transfer. XTransfer freely leverages single or multiple pre-trained\nmodels and transfers knowledge across different modalities by (i) model\nrepairing that safely repairs modality shift in pre-trained model layers with\nonly few sensor data, and (ii) layer recombining that efficiently searches and\nrecombines layers of interest from source models in a layer-wise manner to\ncreate compact models. We benchmark various baselines across diverse human\nsensing datasets spanning different modalities. Comprehensive results\ndemonstrate that XTransfer achieves state-of-the-art performance on human\nsensing tasks while significantly reducing the costs of sensor data collection,\nmodel training, and edge deployment.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22726v1", "categories": ["cs.CV", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22726v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23101", "title": "From Individuals to Interactions: Benchmarking Gender Bias in Multimodal Large Language Models from the Lens of Social Relationship", "authors": ["Yue Xu", "Wenjie Wang"], "summary": "Multimodal large language models (MLLMs) have shown impressive capabilities\nacross tasks involving both visual and textual modalities. However, growing\nconcerns remain about their potential to encode and amplify gender bias,\nparticularly in socially sensitive applications. Existing benchmarks\npredominantly evaluate bias in isolated scenarios, overlooking how bias may\nemerge subtly through interpersonal interactions. We fill this gap by going\nbeyond single-entity evaluation and instead focusing on a deeper examination of\nrelational and contextual gender bias in dual-individual interactions. We\nintroduce Genres, a novel benchmark designed to evaluate gender bias in MLLMs\nthrough the lens of social relationships in generated narratives. Genres\nassesses gender bias through a dual-character profile and narrative generation\ntask that captures rich interpersonal dynamics and supports a fine-grained bias\nevaluation suite across multiple dimensions. Experiments on both open- and\nclosed-source MLLMs reveal persistent, context-sensitive gender biases that are\nnot evident in single-character settings. Our findings underscore the\nimportance of relationship-aware benchmarks for diagnosing subtle,\ninteraction-driven gender bias in MLLMs and provide actionable insights for\nfuture bias mitigation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23101v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.23101v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23529", "title": "When Test-Time Adaptation Meets Self-Supervised Models", "authors": ["Jisu Han", "Jihee Park", "Dongyoon Han", "Wonjun Hwang"], "summary": "Training on test-time data enables deep learning models to adapt to dynamic\nenvironmental changes, enhancing their practical applicability. Online\nadaptation from source to target domains is promising but it remains highly\nreliant on the performance of source pretrained model. In this paper, we\ninvestigate whether test-time adaptation (TTA) methods can continuously improve\nmodels trained via self-supervised learning (SSL) without relying on source\npretraining. We introduce a self-supervised TTA protocol after observing that\nexisting TTA approaches struggle when directly applied to self-supervised\nmodels with low accuracy on the source domain. Furthermore, we propose a\ncollaborative learning framework that integrates SSL and TTA models, leveraging\ncontrastive learning and knowledge distillation for stepwise representation\nrefinement. We validate our method on diverse self-supervised models, including\nDINO, MoCo, and iBOT, across TTA benchmarks. Extensive experiments validate the\neffectiveness of our approach in SSL, showing that it achieves competitive\nperformance even without source pretraining.", "comment": "15 pages, 7 figures", "pdf_url": "http://arxiv.org/pdf/2506.23529v1", "categories": ["cs.CV", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23529v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22729", "title": "Persistence Paradox in Dynamic Science", "authors": ["Honglin Bao", "Kai Li"], "summary": "Persistence is often regarded as a virtue in science. In this paper, however,\nwe challenge this conventional view by highlighting its contextual nature,\nparticularly how persistence can become a liability during periods of paradigm\nshift. We focus on the deep learning revolution catalyzed by AlexNet in 2012.\nAnalyzing the 20-year career trajectories of over 5,000 scientists who were\nactive in top machine learning venues during the preceding decade, we examine\nhow their research focus and output evolved. We first uncover a dynamic period\nin which leading venues increasingly prioritized cutting-edge deep learning\ndevelopments that displaced relatively traditional statistical learning\nmethods. Scientists responded to these changes in markedly different ways.\nThose who were previously successful or affiliated with old teams adapted more\nslowly, experiencing what we term a rigidity penalty - a reluctance to embrace\nnew directions leading to a decline in scientific impact, as measured by\ncitation percentile rank. In contrast, scientists who pursued strategic\nadaptation - selectively pivoting toward emerging trends while preserving weak\nconnections to prior expertise - reaped the greatest benefits. Taken together,\nour macro- and micro-level findings show that scientific breakthroughs act as\nmechanisms that reconfigure power structures within a field.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22729v1", "categories": ["cs.DL", "cs.CY", "cs.LG"], "cate": "cs.DL", "url": "http://arxiv.org/abs/2506.22729v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23115", "title": "MoCa: Modality-aware Continual Pre-training Makes Better Bidirectional Multimodal Embeddings", "authors": ["Haonan Chen", "Hong Liu", "Yuping Luo", "Liang Wang", "Nan Yang", "Furu Wei", "Zhicheng Dou"], "summary": "Multimodal embedding models, built upon causal Vision Language Models (VLMs),\nhave shown promise in various tasks. However, current approaches face three key\nlimitations: the use of causal attention in VLM backbones is suboptimal for\nembedding tasks; scalability issues due to reliance on high-quality labeled\npaired data for contrastive learning; and limited diversity in training\nobjectives and data. To address these issues, we propose MoCa, a two-stage\nframework for transforming pre-trained VLMs into effective bidirectional\nmultimodal embedding models. The first stage, Modality-aware Continual\nPre-training, introduces a joint reconstruction objective that simultaneously\ndenoises interleaved text and image inputs, enhancing bidirectional\ncontext-aware reasoning. The second stage, Heterogeneous Contrastive\nFine-tuning, leverages diverse, semantically rich multimodal data beyond simple\nimage-caption pairs to enhance generalization and alignment. Our method\naddresses the stated limitations by introducing bidirectional attention through\ncontinual pre-training, scaling effectively with massive unlabeled datasets via\njoint reconstruction objectives, and utilizing diverse multimodal data for\nenhanced representation robustness. Experiments demonstrate that MoCa\nconsistently improves performance across MMEB and ViDoRe-v2 benchmarks,\nachieving new state-of-the-art results, and exhibits strong scalability with\nboth model size and training data on MMEB.", "comment": "Homepage: https://haon-chen.github.io/MoCa/", "pdf_url": "http://arxiv.org/pdf/2506.23115v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23115v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23532", "title": "GViT: Representing Images as Gaussians for Visual Recognition", "authors": ["Jefferson Hernandez", "Ruozhen He", "Guha Balakrishnan", "Alexander C. Berg", "Vicente Ordonez"], "summary": "We introduce GVIT, a classification framework that abandons conventional\npixel or patch grid input representations in favor of a compact set of\nlearnable 2D Gaussians. Each image is encoded as a few hundred Gaussians whose\npositions, scales, orientations, colors, and opacities are optimized jointly\nwith a ViT classifier trained on top of these representations. We reuse the\nclassifier gradients as constructive guidance, steering the Gaussians toward\nclass-salient regions while a differentiable renderer optimizes an image\nreconstruction loss. We demonstrate that by 2D Gaussian input representations\ncoupled with our GVIT guidance, using a relatively standard ViT architecture,\nclosely matches the performance of a traditional patch-based ViT, reaching a\n76.9% top-1 accuracy on Imagenet-1k using a ViT-B architecture.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23532v1", "categories": ["cs.CV", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23532v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22763", "title": "Can We Reliably Predict the Fed's Next Move? A Multi-Modal Approach to U.S. Monetary Policy Forecasting", "authors": ["Fiona Xiao Jingyi", "Lili Liu"], "summary": "Forecasting central bank policy decisions remains a persistent challenge for\ninvestors, financial institutions, and policymakers due to the wide-reaching\nimpact of monetary actions. In particular, anticipating shifts in the U.S.\nfederal funds rate is vital for risk management and trading strategies.\nTraditional methods relying only on structured macroeconomic indicators often\nfall short in capturing the forward-looking cues embedded in central bank\ncommunications.\n  This study examines whether predictive accuracy can be enhanced by\nintegrating structured data with unstructured textual signals from Federal\nReserve communications. We adopt a multi-modal framework, comparing traditional\nmachine learning models, transformer-based language models, and deep learning\narchitectures in both unimodal and hybrid settings.\n  Our results show that hybrid models consistently outperform unimodal\nbaselines. The best performance is achieved by combining TF-IDF features of\nFOMC texts with economic indicators in an XGBoost classifier, reaching a test\nAUC of 0.83. FinBERT-based sentiment features marginally improve ranking but\nperform worse in classification, especially under class imbalance. SHAP\nanalysis reveals that sparse, interpretable features align more closely with\npolicy-relevant signals.\n  These findings underscore the importance of integrating textual and\nstructured signals transparently. For monetary policy forecasting, simpler\nhybrid models can offer both accuracy and interpretability, delivering\nactionable insights for researchers and decision-makers.", "comment": "9 pages, 15 figures", "pdf_url": "http://arxiv.org/pdf/2506.22763v1", "categories": ["q-fin.PM", "cs.LG", "q-fin.CP"], "cate": "q-fin.PM", "url": "http://arxiv.org/abs/2506.22763v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23121", "title": "CRISP-SAM2: SAM2 with Cross-Modal Interaction and Semantic Prompting for Multi-Organ Segmentation", "authors": ["Xinlei Yu", "Chanmiao Wang", "Hui Jin", "Ahmed Elazab", "Gangyong Jia", "Xiang Wan", "Changqing Zou", "Ruiquan Ge"], "summary": "Multi-organ medical segmentation is a crucial component of medical image\nprocessing, essential for doctors to make accurate diagnoses and develop\neffective treatment plans. Despite significant progress in this field, current\nmulti-organ segmentation models often suffer from inaccurate details,\ndependence on geometric prompts and loss of spatial information. Addressing\nthese challenges, we introduce a novel model named CRISP-SAM2 with CRoss-modal\nInteraction and Semantic Prompting based on SAM2. This model represents a\npromising approach to multi-organ medical segmentation guided by textual\ndescriptions of organs. Our method begins by converting visual and textual\ninputs into cross-modal contextualized semantics using a progressive\ncross-attention interaction mechanism. These semantics are then injected into\nthe image encoder to enhance the detailed understanding of visual information.\nTo eliminate reliance on geometric prompts, we use a semantic prompting\nstrategy, replacing the original prompt encoder to sharpen the perception of\nchallenging targets. In addition, a similarity-sorting self-updating strategy\nfor memory and a mask-refining process is applied to further adapt to medical\nimaging and enhance localized details. Comparative experiments conducted on\nseven public datasets indicate that CRISP-SAM2 outperforms existing models.\nExtensive analysis also demonstrates the effectiveness of our method, thereby\nconfirming its superior performance, especially in addressing the limitations\nmentioned earlier. Our code is available at:\nhttps://github.com/YU-deep/CRISP\\_SAM2.git.", "comment": "19 pages, 9 figures, 10 tables", "pdf_url": "http://arxiv.org/pdf/2506.23121v1", "categories": ["eess.IV", "cs.AI", "cs.CV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.23121v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23538", "title": "Uncertainty-aware Diffusion and Reinforcement Learning for Joint Plane Localization and Anomaly Diagnosis in 3D Ultrasound", "authors": ["Yuhao Huang", "Yueyue Xu", "Haoran Dou", "Jiaxiao Deng", "Xin Yang", "Hongyu Zheng", "Dong Ni"], "summary": "Congenital uterine anomalies (CUAs) can lead to infertility, miscarriage,\npreterm birth, and an increased risk of pregnancy complications. Compared to\ntraditional 2D ultrasound (US), 3D US can reconstruct the coronal plane,\nproviding a clear visualization of the uterine morphology for assessing CUAs\naccurately. In this paper, we propose an intelligent system for simultaneous\nautomated plane localization and CUA diagnosis. Our highlights are: 1) we\ndevelop a denoising diffusion model with local (plane) and global (volume/text)\nguidance, using an adaptive weighting strategy to optimize attention allocation\nto different conditions; 2) we introduce a reinforcement learning-based\nframework with unsupervised rewards to extract the key slice summary from\nredundant sequences, fully integrating information across multiple planes to\nreduce learning difficulty; 3) we provide text-driven uncertainty modeling for\ncoarse prediction, and leverage it to adjust the classification probability for\noverall performance improvement. Extensive experiments on a large 3D uterine US\ndataset show the efficacy of our method, in terms of plane localization and CUA\ndiagnosis. Code is available at https://github.com/yuhoo0302/CUA-US.", "comment": "Accepted by MICCAI 2025;10 pages, 3 figures", "pdf_url": "http://arxiv.org/pdf/2506.23538v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23538v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22773", "title": "Not All Water Consumption Is Equal: A Water Stress Weighted Metric for Sustainable Computing", "authors": ["Yanran Wu", "Inez Hua", "Yi Ding"], "summary": "Water consumption is an increasingly critical dimension of computing\nsustainability, especially as AI workloads rapidly scale. However, current\nwater impact assessment often overlooks where and when water stress is more\nsevere. To fill in this gap, we present SCARF, the first general framework that\nevaluates water impact of computing by factoring in both spatial and temporal\nvariations in water stress. SCARF calculates an Adjusted Water Impact (AWI)\nmetric that considers both consumption volume and local water stress over time.\nThrough three case studies on LLM serving, datacenters, and semiconductor\nfabrication plants, we show the hidden opportunities for reducing water impact\nby optimizing location and time choices, paving the way for water-sustainable\ncomputing. The code is available at https://github.com/jojacola/SCARF.", "comment": "7 pages, 9 figures, HotCarbon '25: Proceedings of the 4th Workshop on\n  Sustainable Computer Systems, Cambridge, Massachusetts (USA), July 10-11th,\n  2025", "pdf_url": "http://arxiv.org/pdf/2506.22773v1", "categories": ["cs.DC", "cs.AR", "cs.CY", "cs.LG"], "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.22773v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23127", "title": "Unleashing Embodied Task Planning Ability in LLMs via Reinforcement Learning", "authors": ["Zhaoye Fei", "Li Ji", "Siyin Wang", "Junhao Shi", "Jingjing Gong", "Xipeng Qiu"], "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious tasks, yet they face significant challenges in embodied task planning\nscenarios that require continuous environmental understanding and action\ngeneration. Existing approaches generate open-loop action scripts based on\nstatic knowledge, making it difficult to learn causal relationships between\nactions and environmental feedback, particularly in partially observable\nenvironments. We introduce Embodied Planner-R1, a novel outcome-driven\nreinforcement learning framework that enables LLMs to develop interactive\ncapabilities through autonomous exploration with minimal supervision. Our\nframework incorporates three key innovations: (1) Without human annotations, we\nemploy pure reinforcement learning with group rollout, incorporating\nin-environment interaction through parallel exploration; (2) completion-driven\nsparse reward; and (3) Interactive Policy Optimization (IPO) for efficient\nlearning from grouped trajectories. Across two challenging text-based Embodied\nplanning benchmarks, Embodied Planner-R1 achieves impressive completion rates\nof 97.78% on ALFWorld and 79.92% on ScienceWorld, surpassing prior methods by a\nlarge margin, and suffers only a -3.66% drop in previously unseen environments,\nevidencing strong generalization.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23127v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.23127v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23542", "title": "Consistent Time-of-Flight Depth Denoising via Graph-Informed Geometric Attention", "authors": ["Weida Wang", "Changyong He", "Jin Zeng", "Di Qiu"], "summary": "Depth images captured by Time-of-Flight (ToF) sensors are prone to noise,\nrequiring denoising for reliable downstream applications. Previous works either\nfocus on single-frame processing, or perform multi-frame processing without\nconsidering depth variations at corresponding pixels across frames, leading to\nundesirable temporal inconsistency and spatial ambiguity. In this paper, we\npropose a novel ToF depth denoising network leveraging motion-invariant graph\nfusion to simultaneously enhance temporal stability and spatial sharpness.\nSpecifically, despite depth shifts across frames, graph structures exhibit\ntemporal self-similarity, enabling cross-frame geometric attention for graph\nfusion. Then, by incorporating an image smoothness prior on the fused graph and\ndata fidelity term derived from ToF noise distribution, we formulate a maximum\na posterior problem for ToF denoising. Finally, the solution is unrolled into\niterative filters whose weights are adaptively learned from the graph-informed\ngeometric attention, producing a high-performance yet interpretable network.\nExperimental results demonstrate that the proposed scheme achieves\nstate-of-the-art performance in terms of accuracy and consistency on synthetic\nDVToF dataset and exhibits robust generalization on the real Kinectv2 dataset.\nSource code will be released at\n\\href{https://github.com/davidweidawang/GIGA-ToF}{https://github.com/davidweidawang/GIGA-ToF}.", "comment": "This paper has been accepted for publication at the International\n  Conference on Computer Vision (ICCV) 2025", "pdf_url": "http://arxiv.org/pdf/2506.23542v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23542v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22799", "title": "VoteSplat: Hough Voting Gaussian Splatting for 3D Scene Understanding", "authors": ["Minchao Jiang", "Shunyu Jia", "Jiaming Gu", "Xiaoyuan Lu", "Guangming Zhu", "Anqi Dong", "Liang Zhang"], "summary": "3D Gaussian Splatting (3DGS) has become horsepower in high-quality, real-time\nrendering for novel view synthesis of 3D scenes. However, existing methods\nfocus primarily on geometric and appearance modeling, lacking deeper scene\nunderstanding while also incurring high training costs that complicate the\noriginally streamlined differentiable rendering pipeline. To this end, we\npropose VoteSplat, a novel 3D scene understanding framework that integrates\nHough voting with 3DGS. Specifically, Segment Anything Model (SAM) is utilized\nfor instance segmentation, extracting objects, and generating 2D vote maps. We\nthen embed spatial offset vectors into Gaussian primitives. These offsets\nconstruct 3D spatial votes by associating them with 2D image votes, while depth\ndistortion constraints refine localization along the depth axis. For\nopen-vocabulary object localization, VoteSplat maps 2D image semantics to 3D\npoint clouds via voting points, reducing training costs associated with\nhigh-dimensional CLIP features while preserving semantic unambiguity. Extensive\nexperiments demonstrate effectiveness of VoteSplat in open-vocabulary 3D\ninstance localization, 3D point cloud understanding, click-based 3D object\nlocalization, hierarchical segmentation, and ablation studies. Our code is\navailable at https://sy-ja.github.io/votesplat/", "comment": "Accepted to ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2506.22799v1", "categories": ["cs.GR", "cs.CV", "cs.LG"], "cate": "cs.GR", "url": "http://arxiv.org/abs/2506.22799v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23137", "title": "Flow-Modulated Scoring for Semantic-Aware Knowledge Graph Completion", "authors": ["Siyuan Li", "Ruitong Liu", "Yan Wen", "Te Sun"], "summary": "Effective modeling of multifaceted relations is pivotal for Knowledge Graph\nCompletion (KGC). However, a majority of existing approaches are predicated on\nstatic, embedding-based scoring, exhibiting inherent limitations in capturing\ncontextual dependencies and relational dynamics. Addressing this gap, we\npropose the Flow-Modulated Scoring (FMS) framework. FMS comprises two principal\ncomponents: (1) a semantic context learning module that encodes\ncontext-sensitive entity representations, and (2) a conditional flow-matching\nmodule designed to learn the dynamic transformation from a head to a tail\nembedding, governed by the aforementioned context. The resultant predictive\nvector field, representing the context-informed relational path, serves to\ndynamically refine the initial static score of an entity pair. Through this\nsynergy of context-aware static representations and conditioned dynamic\ninformation, FMS facilitates a more profound modeling of relational semantics.\nComprehensive evaluations on several standard benchmarks demonstrate that our\nproposed method surpasses prior state-of-the-art results.", "comment": "10 pages", "pdf_url": "http://arxiv.org/pdf/2506.23137v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.23137v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23543", "title": "Pyramidal Patchification Flow for Visual Generation", "authors": ["Hui Li", "Baoyou Chen", "Liwei Zhang", "Jiaye Li", "Jingdong Wang", "Siyu Zhu"], "summary": "Diffusion transformers (DiTs) adopt Patchify, mapping patch representations\nto token representations through linear projections, to adjust the number of\ntokens input to DiT blocks and thus the computation cost. Instead of a single\npatch size for all the timesteps, we introduce a Pyramidal Patchification Flow\n(PPFlow) approach: Large patch sizes are used for high noise timesteps and\nsmall patch sizes for low noise timesteps; Linear projections are learned for\neach patch size; and Unpatchify is accordingly modified. Unlike Pyramidal Flow,\nour approach operates over full latent representations other than pyramid\nrepresentations, and adopts the normal denoising process without requiring the\nrenoising trick. We demonstrate the effectiveness of our approach through two\ntraining manners. Training from scratch achieves a $1.6\\times$ ($2.0\\times$)\ninference speed over SiT-B/2 for 2-level (3-level) pyramid patchification with\nslightly lower training FLOPs and similar image generation performance.\nTraining from pretrained normal DiTs achieves even better performance with\nsmall training time. The code and checkpoint are at\nhttps://github.com/fudan-generative-vision/PPFlow.", "comment": "10 pages, 9figures", "pdf_url": "http://arxiv.org/pdf/2506.23543v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23543v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22803", "title": "Intervening in Black Box: Concept Bottleneck Model for Enhancing Human Neural Network Mutual Understanding", "authors": ["Nuoye Xiong", "Anqi Dong", "Ning Wang", "Cong Hua", "Guangming Zhu", "Mei Lin", "Peiyi Shen", "Liang Zhang"], "summary": "Recent advances in deep learning have led to increasingly complex models with\ndeeper layers and more parameters, reducing interpretability and making their\ndecisions harder to understand. While many methods explain black-box reasoning,\nmost lack effective interventions or only operate at sample-level without\nmodifying the model itself. To address this, we propose the Concept Bottleneck\nModel for Enhancing Human-Neural Network Mutual Understanding (CBM-HNMU).\nCBM-HNMU leverages the Concept Bottleneck Model (CBM) as an interpretable\nframework to approximate black-box reasoning and communicate conceptual\nunderstanding. Detrimental concepts are automatically identified and refined\n(removed/replaced) based on global gradient contributions. The modified CBM\nthen distills corrected knowledge back into the black-box model, enhancing both\ninterpretability and accuracy. We evaluate CBM-HNMU on various CNN and\ntransformer-based models across Flower-102, CIFAR-10, CIFAR-100, FGVC-Aircraft,\nand CUB-200, achieving a maximum accuracy improvement of 2.64% and a maximum\nincrease in average accuracy across 1.03%. Source code is available at:\nhttps://github.com/XiGuaBo/CBM-HNMU.", "comment": "Accepted by ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2506.22803v1", "categories": ["cs.CV", "cs.HC", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22803v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23139", "title": "Benchmarking Deep Search over Heterogeneous Enterprise Data", "authors": ["Prafulla Kumar Choubey", "Xiangyu Peng", "Shilpa Bhagavath", "Kung-Hsiang Huang", "Caiming Xiong", "Chien-Sheng Wu"], "summary": "We present a new benchmark for evaluating Deep Search--a realistic and\ncomplex form of retrieval-augmented generation (RAG) that requires\nsource-aware, multi-hop reasoning over diverse, sparsed, but related sources.\nThese include documents, meeting transcripts, Slack messages, GitHub, and URLs,\nwhich vary in structure and often contain human-to-human interactions. We build\nit using a synthetic data pipeline that simulates business workflows across\nproduct planning, development, and support stages, generating interconnected\ncontent with realistic noise and multi-hop questions with guaranteed\nground-truth answers. We release our benchmark with both answerable and\nunanswerable queries, and retrieval pool of 39,190 enterprise artifacts,\nenabling fine-grained evaluation of long-context LLM and RAG systems. Our\nexperiments reveal that even the best-performing agentic RAG methods achieve an\naverage performance score of 32.96 on our benchmark. With further analysis, we\nhighlight retrieval as the main bottleneck: existing methods struggle to\nconduct deep searches and retrieve all necessary evidence. Consequently, they\noften reason over partial context, leading to significant performance\ndegradation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23139v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.23139v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23547", "title": "Oneta: Multi-Style Image Enhancement Using Eigentransformation Functions", "authors": ["Jiwon Kim", "Soohyun Hwang", "Dong-O Kim", "Changsu Han", "Min Kyu Park", "Chang-Su Kim"], "summary": "The first algorithm, called Oneta, for a novel task of multi-style image\nenhancement is proposed in this work. Oneta uses two point operators\nsequentially: intensity enhancement with a transformation function (TF) and\ncolor correction with a color correction matrix (CCM). This two-step\nenhancement model, though simple, achieves a high performance upper bound.\nAlso, we introduce eigentransformation function (eigenTF) to represent TF\ncompactly. The Oneta network comprises Y-Net and C-Net to predict eigenTF and\nCCM parameters, respectively. To support $K$ styles, Oneta employs $K$\nlearnable tokens. During training, each style token is learned using image\npairs from the corresponding dataset. In testing, Oneta selects one of the $K$\nstyle tokens to enhance an image accordingly. Extensive experiments show that\nthe single Oneta network can effectively undertake six enhancement tasks --\nretouching, image signal processing, low-light image enhancement, dehazing,\nunderwater image enhancement, and white balancing -- across 30 datasets.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23547v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23547v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22806", "title": "Concept Pinpoint Eraser for Text-to-image Diffusion Models via Residual Attention Gate", "authors": ["Byung Hyun Lee", "Sungjin Lim", "Seunggyu Lee", "Dong Un Kang", "Se Young Chun"], "summary": "Remarkable progress in text-to-image diffusion models has brought a major\nconcern about potentially generating images on inappropriate or trademarked\nconcepts. Concept erasing has been investigated with the goals of deleting\ntarget concepts in diffusion models while preserving other concepts with\nminimal distortion. To achieve these goals, recent concept erasing methods\nusually fine-tune the cross-attention layers of diffusion models. In this work,\nwe first show that merely updating the cross-attention layers in diffusion\nmodels, which is mathematically equivalent to adding \\emph{linear} modules to\nweights, may not be able to preserve diverse remaining concepts. Then, we\npropose a novel framework, dubbed Concept Pinpoint Eraser (CPE), by adding\n\\emph{nonlinear} Residual Attention Gates (ResAGs) that selectively erase (or\ncut) target concepts while safeguarding remaining concepts from broad\ndistributions by employing an attention anchoring loss to prevent the\nforgetting. Moreover, we adversarially train CPE with ResAG and learnable text\nembeddings in an iterative manner to maximize erasing performance and enhance\nrobustness against adversarial attacks. Extensive experiments on the erasure of\ncelebrities, artistic styles, and explicit contents demonstrated that the\nproposed CPE outperforms prior arts by keeping diverse remaining concepts while\ndeleting the target concepts with robustness against attack prompts. Code is\navailable at https://github.com/Hyun1A/CPE", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22806v1", "categories": ["cs.CV", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22806v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23151", "title": "MEMFOF: High-Resolution Training for Memory-Efficient Multi-Frame Optical Flow Estimation", "authors": ["Vladislav Bargatin", "Egor Chistov", "Alexander Yakovenko", "Dmitriy Vatolin"], "summary": "Recent advances in optical flow estimation have prioritized accuracy at the\ncost of growing GPU memory consumption, particularly for high-resolution\n(FullHD) inputs. We introduce MEMFOF, a memory-efficient multi-frame optical\nflow method that identifies a favorable trade-off between multi-frame\nestimation and GPU memory usage. Notably, MEMFOF requires only 2.09 GB of GPU\nmemory at runtime for 1080p inputs, and 28.5 GB during training, which uniquely\npositions our method to be trained at native 1080p without the need for\ncropping or downsampling. We systematically revisit design choices from\nRAFT-like architectures, integrating reduced correlation volumes and\nhigh-resolution training protocols alongside multi-frame estimation, to achieve\nstate-of-the-art performance across multiple benchmarks while substantially\nreducing memory overhead. Our method outperforms more resource-intensive\nalternatives in both accuracy and runtime efficiency, validating its robustness\nfor flow estimation at high resolutions. At the time of submission, our method\nranks first on the Spring benchmark with a 1-pixel (1px) outlier rate of 3.289,\nleads Sintel (clean) with an endpoint error (EPE) of 0.963, and achieves the\nbest Fl-all error on KITTI-2015 at 2.94%. The code is available at\nhttps://github.com/msu-video-group/memfof.", "comment": "Accepted at ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2506.23151v1", "categories": ["cs.CV", "cs.AI", "cs.MM"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23151v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23552", "title": "JAM-Flow: Joint Audio-Motion Synthesis with Flow Matching", "authors": ["Mingi Kwon", "Joonghyuk Shin", "Jaeseok Jung", "Jaesik Park", "Youngjung Uh"], "summary": "The intrinsic link between facial motion and speech is often overlooked in\ngenerative modeling, where talking head synthesis and text-to-speech (TTS) are\ntypically addressed as separate tasks. This paper introduces JAM-Flow, a\nunified framework to simultaneously synthesize and condition on both facial\nmotion and speech. Our approach leverages flow matching and a novel Multi-Modal\nDiffusion Transformer (MM-DiT) architecture, integrating specialized Motion-DiT\nand Audio-DiT modules. These are coupled via selective joint attention layers\nand incorporate key architectural choices, such as temporally aligned\npositional embeddings and localized joint attention masking, to enable\neffective cross-modal interaction while preserving modality-specific strengths.\nTrained with an inpainting-style objective, JAM-Flow supports a wide array of\nconditioning inputs-including text, reference audio, and reference\nmotion-facilitating tasks such as synchronized talking head generation from\ntext, audio-driven animation, and much more, within a single, coherent model.\nJAM-Flow significantly advances multi-modal generative modeling by providing a\npractical solution for holistic audio-visual synthesis. project page:\nhttps://joonghyuk.com/jamflow-web", "comment": "project page: https://joonghyuk.com/jamflow-web Under review.\n  Preprint published on arXiv", "pdf_url": "http://arxiv.org/pdf/2506.23552v1", "categories": ["cs.CV", "cs.SD", "eess.AS"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23552v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22819", "title": "Prompting without Panic: Attribute-aware, Zero-shot, Test-Time Calibration", "authors": ["Ramya Hebbalaguppe", "Tamoghno Kandar", "Abhinav Nagpal", "Chetan Arora"], "summary": "Vision-language models (VLM) have demonstrated impressive performance in\nimage recognition by leveraging self-supervised training on large datasets.\nTheir performance can be further improved by adapting to the test sample using\ntest-time prompt tuning (TPT). Unfortunately, the singular focus of TPT\napproaches on improving the accuracy suffers from tunnel vision, and leads to\ndegradation in confidence calibration. This limits the applicability of TPT in\ncritical applications.\n  We make three contributions in this work. (1) We posit that random or naive\ninitialization of prompts leads to overfitting on a particular test sample, and\nis the main reason for miscalibration of the VLM after TPT. To mitigate the\nproblem, we propose careful initialization of test time prompt using prior\nknowledge about the target label attributes from a large language model (LLM);\n(2) To further maintain the quality of prompts during \\tpt, we propose a novel\nregularization loss to reduce intraclass distance, and increase inter-class\ndistance between the learnt\n  Through extensive experiments on different CLIP architectures and 15\ndatasets, we show that our approach can effectively improve the calibration\nafter TPT. We report an average expected calibration error (ECE) of 4.11 with\nour method, TCA, compared to 11.7 for vanilla TPT, 6.12 for C-TPT (ICLR'24),\n6.78 for DiffTPT (CVPR'23), and 8.43 for PromptAlign (NeurIPS'23). The code is\npublicly accessible at:\nhttps://github.com/rhebbalaguppe/TCA_PromptWithoutPanic.", "comment": "26 pages", "pdf_url": "http://arxiv.org/pdf/2506.22819v1", "categories": ["cs.CV", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22819v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23164", "title": "Mode Collapse Happens: Evaluating Critical Interactions in Joint Trajectory Prediction Models", "authors": ["Maarten Hugenholtz", "Anna Meszaros", "Jens Kober", "Zlatan Ajanovic"], "summary": "Autonomous Vehicle decisions rely on multimodal prediction models that\naccount for multiple route options and the inherent uncertainty in human\nbehavior. However, models can suffer from mode collapse, where only the most\nlikely mode is predicted, posing significant safety risks. While existing\nmethods employ various strategies to generate diverse predictions, they often\noverlook the diversity in interaction modes among agents. Additionally,\ntraditional metrics for evaluating prediction models are dataset-dependent and\ndo not evaluate inter-agent interactions quantitatively. To our knowledge, none\nof the existing metrics explicitly evaluates mode collapse. In this paper, we\npropose a novel evaluation framework that assesses mode collapse in joint\ntrajectory predictions, focusing on safety-critical interactions. We introduce\nmetrics for mode collapse, mode correctness, and coverage, emphasizing the\nsequential dimension of predictions. By testing four multi-agent trajectory\nprediction models, we demonstrate that mode collapse indeed happens. When\nlooking at the sequential dimension, although prediction accuracy improves\ncloser to interaction events, there are still cases where the models are unable\nto predict the correct interaction mode, even just before the interaction mode\nbecomes inevitable. We hope that our framework can help researchers gain new\ninsights and advance the development of more consistent and accurate prediction\nmodels, thus enhancing the safety of autonomous driving systems.", "comment": "12 pages, 8 figures, submitted to a journal", "pdf_url": "http://arxiv.org/pdf/2506.23164v1", "categories": ["cs.RO", "cs.AI"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.23164v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23555", "title": "LH2Face: Loss function for Hard High-quality Face", "authors": ["Fan Xie", "Pan Cao"], "summary": "In current practical face authentication systems, most face recognition (FR)\nalgorithms are based on cosine similarity with softmax classification. Despite\nits reliable classification performance, this method struggles with hard\nsamples. A popular strategy to improve FR performance is incorporating angular\nor cosine margins. However, it does not take face quality or recognition\nhardness into account, simply increasing the margin value and thus causing an\noverly uniform training strategy. To address this problem, a novel loss\nfunction is proposed, named Loss function for Hard High-quality Face (LH2Face).\nFirstly, a similarity measure based on the von Mises-Fisher (vMF) distribution\nis stated, specifically focusing on the logarithm of the Probability Density\nFunction (PDF), which represents the distance between a probability\ndistribution and a vector. Then, an adaptive margin-based multi-classification\nmethod using softmax, called the Uncertainty-Aware Margin Function, is\nimplemented in the article. Furthermore, proxy-based loss functions are used to\napply extra constraints between the proxy and sample to optimize their\nrepresentation space distribution. Finally, a renderer is constructed that\noptimizes FR through face reconstruction and vice versa. Our LH2Face is\nsuperior to similiar schemes on hard high-quality face datasets, achieving\n49.39% accuracy on the IJB-B dataset, which surpasses the second-place method\nby 2.37%.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23555v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23555v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22851", "title": "Deep neural networks can provably solve Bellman equations for Markov decision processes without the curse of dimensionality", "authors": ["Arnulf Jentzen", "Konrad Kleinberg", "Thomas Kruse"], "summary": "Discrete time stochastic optimal control problems and Markov decision\nprocesses (MDPs) are fundamental models for sequential decision-making under\nuncertainty and as such provide the mathematical framework underlying\nreinforcement learning theory. A central tool for solving MDPs is the Bellman\nequation and its solution, the so-called $Q$-function. In this article, we\nconstruct deep neural network (DNN) approximations for $Q$-functions associated\nto MDPs with infinite time horizon and finite control set $A$. More\nspecifically, we show that if the the payoff function and the random transition\ndynamics of the MDP can be suitably approximated by DNNs with leaky rectified\nlinear unit (ReLU) activation, then the solutions $Q_d\\colon \\mathbb R^d\\to\n\\mathbb R^{|A|}$, $d\\in \\mathbb{N}$, of the associated Bellman equations can\nalso be approximated in the $L^2$-sense by DNNs with leaky ReLU activation\nwhose numbers of parameters grow at most polynomially in both the dimension\n$d\\in \\mathbb{N}$ of the state space and the reciprocal $1/\\varepsilon$ of the\nprescribed error $\\varepsilon\\in (0,1)$. Our proof relies on the recently\nintroduced full-history recursive multilevel fixed-point (MLFP) approximation\nscheme.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22851v1", "categories": ["math.OC", "cs.LG", "cs.NA", "math.NA", "math.PR", "stat.ML", "90C40, 90C39, 60J05, 93E20, 65C05, 68T07"], "cate": "math.OC", "url": "http://arxiv.org/abs/2506.22851v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23173", "title": "Deep Learning for Optical Misalignment Diagnostics in Multi-Lens Imaging Systems", "authors": ["Tomer Slor", "Dean Oren", "Shira Baneth", "Tom Coen", "Haim Suchowski"], "summary": "In the rapidly evolving field of optical engineering, precise alignment of\nmulti-lens imaging systems is critical yet challenging, as even minor\nmisalignments can significantly degrade performance. Traditional alignment\nmethods rely on specialized equipment and are time-consuming processes,\nhighlighting the need for automated and scalable solutions. We present two\ncomplementary deep learning-based inverse-design methods for diagnosing\nmisalignments in multi-element lens systems using only optical measurements.\nFirst, we use ray-traced spot diagrams to predict five-degree-of-freedom\n(5-DOF) errors in a 6-lens photographic prime, achieving a mean absolute error\nof 0.031mm in lateral translation and 0.011$^\\circ$ in tilt. We also introduce\na physics-based simulation pipeline that utilizes grayscale synthetic camera\nimages, enabling a deep learning model to estimate 4-DOF, decenter and tilt\nerrors in both two- and six-lens multi-lens systems. These results show the\npotential to reshape manufacturing and quality control in precision imaging.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23173v1", "categories": ["physics.optics", "cs.AI", "cs.LG"], "cate": "physics.optics", "url": "http://arxiv.org/abs/2506.23173v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23565", "title": "OcRFDet: Object-Centric Radiance Fields for Multi-View 3D Object Detection in Autonomous Driving", "authors": ["Mingqian Ji", "Jian Yang", "Shanshan Zhang"], "summary": "Current multi-view 3D object detection methods typically transfer 2D features\ninto 3D space using depth estimation or 3D position encoder, but in a fully\ndata-driven and implicit manner, which limits the detection performance.\nInspired by the success of radiance fields on 3D reconstruction, we assume they\ncan be used to enhance the detector's ability of 3D geometry estimation.\nHowever, we observe a decline in detection performance, when we directly use\nthem for 3D rendering as an auxiliary task. From our analysis, we find the\nperformance drop is caused by the strong responses on the background when\nrendering the whole scene. To address this problem, we propose object-centric\nradiance fields, focusing on modeling foreground objects while discarding\nbackground noises. Specifically, we employ Object-centric Radiance Fields\n(OcRF) to enhance 3D voxel features via an auxiliary task of rendering\nforeground objects. We further use opacity - the side-product of rendering- to\nenhance the 2D foreground BEV features via Height-aware Opacity-based Attention\n(HOA), where attention maps at different height levels are generated separately\nvia multiple networks in parallel. Extensive experiments on the nuScenes\nvalidation and test datasets demonstrate that our OcRFDet achieves superior\nperformance, outperforming previous state-of-the-art methods with 57.2$\\%$ mAP\nand 64.8$\\%$ NDS on the nuScenes test benchmark. Code will be available at\nhttps://github.com/Mingqj/OcRFDet.", "comment": "Accepted by ICCV2025", "pdf_url": "http://arxiv.org/pdf/2506.23565v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23565v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22882", "title": "CA-Diff: Collaborative Anatomy Diffusion for Brain Tissue Segmentation", "authors": ["Qilong Xing", "Zikai Song", "Yuteng Ye", "Yuke Chen", "Youjia Zhang", "Na Feng", "Junqing Yu", "Wei Yang"], "summary": "Segmentation of brain structures from MRI is crucial for evaluating brain\nmorphology, yet existing CNN and transformer-based methods struggle to\ndelineate complex structures accurately. While current diffusion models have\nshown promise in image segmentation, they are inadequate when applied directly\nto brain MRI due to neglecting anatomical information. To address this, we\npropose Collaborative Anatomy Diffusion (CA-Diff), a framework integrating\nspatial anatomical features to enhance segmentation accuracy of the diffusion\nmodel. Specifically, we introduce distance field as an auxiliary anatomical\ncondition to provide global spatial context, alongside a collaborative\ndiffusion process to model its joint distribution with anatomical structures,\nenabling effective utilization of anatomical features for segmentation.\nFurthermore, we introduce a consistency loss to refine relationships between\nthe distance field and anatomical structures and design a time adapted channel\nattention module to enhance the U-Net feature fusion procedure. Extensive\nexperiments show that CA-Diff outperforms state-of-the-art (SOTA) methods.", "comment": "ICME 2025", "pdf_url": "http://arxiv.org/pdf/2506.22882v1", "categories": ["eess.IV", "cs.CV", "cs.LG"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.22882v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23174", "title": "Data Can Speak for Itself: Quality-guided Utilization of Wireless Synthetic Data", "authors": ["Chen Gong", "Bo Liang", "Wei Gao", "Chenren Xu"], "summary": "Generative models have gained significant attention for their ability to\nproduce realistic synthetic data that supplements the quantity of real-world\ndatasets. While recent studies show performance improvements in wireless\nsensing tasks by incorporating all synthetic data into training sets, the\nquality of synthetic data remains unpredictable and the resulting performance\ngains are not guaranteed. To address this gap, we propose tractable and\ngeneralizable metrics to quantify quality attributes of synthetic data -\naffinity and diversity. Our assessment reveals prevalent affinity limitation in\ncurrent wireless synthetic data, leading to mislabeled data and degraded task\nperformance. We attribute the quality limitation to generative models' lack of\nawareness of untrained conditions and domain-specific processing. To mitigate\nthese issues, we introduce SynCheck, a quality-guided synthetic data\nutilization scheme that refines synthetic data quality during task model\ntraining. Our evaluation demonstrates that SynCheck consistently outperforms\nquality-oblivious utilization of synthetic data, and achieves 4.3% performance\nimprovement even when the previous utilization degrades performance by 13.4%.", "comment": "Published in MobiSys 2025", "pdf_url": "http://arxiv.org/pdf/2506.23174v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.23174v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23566", "title": "Metadata, Wavelet, and Time Aware Diffusion Models for Satellite Image Super Resolution", "authors": ["Luigi Sigillo", "Renato Giamba", "Danilo Comminiello"], "summary": "The acquisition of high-resolution satellite imagery is often constrained by\nthe spatial and temporal limitations of satellite sensors, as well as the high\ncosts associated with frequent observations. These challenges hinder\napplications such as environmental monitoring, disaster response, and\nagricultural management, which require fine-grained and high-resolution data.\nIn this paper, we propose MWT-Diff, an innovative framework for satellite image\nsuper-resolution (SR) that combines latent diffusion models with wavelet\ntransforms to address these challenges. At the core of the framework is a novel\nmetadata-, wavelet-, and time-aware encoder (MWT-Encoder), which generates\nembeddings that capture metadata attributes, multi-scale frequency information,\nand temporal relationships. The embedded feature representations steer the\nhierarchical diffusion dynamics, through which the model progressively\nreconstructs high-resolution satellite imagery from low-resolution inputs. This\nprocess preserves critical spatial characteristics including textural patterns,\nboundary discontinuities, and high-frequency spectral components essential for\ndetailed remote sensing analysis. The comparative analysis of MWT-Diff across\nmultiple datasets demonstrated favorable performance compared to recent\napproaches, as measured by standard perceptual quality metrics including FID\nand LPIPS.", "comment": "ICLR 2025 Workshop on Machine Learning for Remote Sensing (ML4RS)", "pdf_url": "http://arxiv.org/pdf/2506.23566v1", "categories": ["cs.CV", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23566v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22899", "title": "Neural Cellular Automata: From Cells to Pixels", "authors": ["Ehsan Pajouheshgar", "Yitao Xu", "Ali Abbasi", "Alexander Mordvintsev", "Wenzel Jakob", "Sabine S√ºsstrunk"], "summary": "Neural Cellular Automata (NCAs) are bio-inspired systems in which identical\ncells self-organize to form complex and coherent patterns by repeatedly\napplying simple local rules. NCAs display striking emergent behaviors including\nself-regeneration, generalization and robustness to unseen situations, and\nspontaneous motion. Despite their success in texture synthesis and\nmorphogenesis, NCAs remain largely confined to low-resolution grids. This\nlimitation stems from (1) training time and memory requirements that grow\nquadratically with grid size, (2) the strictly local propagation of information\nwhich impedes long-range cell communication, and (3) the heavy compute demands\nof real-time inference at high resolution. In this work, we overcome this\nlimitation by pairing NCA with a tiny, shared implicit decoder, inspired by\nrecent advances in implicit neural representations. Following NCA evolution on\na coarse grid, a lightweight decoder renders output images at arbitrary\nresolution. We also propose novel loss functions for both morphogenesis and\ntexture synthesis tasks, specifically tailored for high-resolution output with\nminimal memory and computation overhead. Combining our proposed architecture\nand loss functions brings substantial improvement in quality, efficiency, and\nperformance. NCAs equipped with our implicit decoder can generate full-HD\noutputs in real time while preserving their self-organizing, emergent\nproperties. Moreover, because each MLP processes cell states independently,\ninference remains highly parallelizable and efficient. We demonstrate the\napplicability of our approach across multiple NCA variants (on 2D, 3D grids,\nand 3D meshes) and multiple tasks, including texture generation and\nmorphogenesis (growing patterns from a seed), showing that with our proposed\nframework, NCAs seamlessly scale to high-resolution outputs with minimal\ncomputational overhead.", "comment": "6 pages, 5 figures, first draft", "pdf_url": "http://arxiv.org/pdf/2506.22899v1", "categories": ["cs.CV", "cs.GR", "cs.LG", "cs.MA", "eess.IV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22899v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23184", "title": "Score-based Diffusion Model for Unpaired Virtual Histology Staining", "authors": ["Anran Liu", "Xiaofei Wang", "Jing Cai", "Chao Li"], "summary": "Hematoxylin and eosin (H&E) staining visualizes histology but lacks\nspecificity for diagnostic markers. Immunohistochemistry (IHC) staining\nprovides protein-targeted staining but is restricted by tissue availability and\nantibody specificity. Virtual staining, i.e., computationally translating the\nH&E image to its IHC counterpart while preserving the tissue structure, is\npromising for efficient IHC generation. Existing virtual staining methods still\nface key challenges: 1) effective decomposition of staining style and tissue\nstructure, 2) controllable staining process adaptable to diverse tissue and\nproteins, and 3) rigorous structural consistency modelling to handle the\nnon-pixel-aligned nature of paired H&E and IHC images. This study proposes a\nmutual-information (MI)-guided score-based diffusion model for unpaired virtual\nstaining. Specifically, we design 1) a global MI-guided energy function that\ndisentangles the tissue structure and staining characteristics across\nmodalities, 2) a novel timestep-customized reverse diffusion process for\nprecise control of the staining intensity and structural reconstruction, and 3)\na local MI-driven contrastive learning strategy to ensure the cellular level\nstructural consistency between H&E-IHC images. Extensive experiments\ndemonstrate the our superiority over state-of-the-art approaches, highlighting\nits biomedical potential. Codes will be open-sourced upon acceptance.", "comment": "11 pages, 3 figures", "pdf_url": "http://arxiv.org/pdf/2506.23184v1", "categories": ["eess.IV", "cs.AI", "cs.CV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.23184v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23575", "title": "Event-based Tiny Object Detection: A Benchmark Dataset and Baseline", "authors": ["Nuo Chen", "Chao Xiao", "Yimian Dai", "Shiman He", "Miao Li", "Wei An"], "summary": "Small object detection (SOD) in anti-UAV task is a challenging problem due to\nthe small size of UAVs and complex backgrounds. Traditional frame-based cameras\nstruggle to detect small objects in complex environments due to their low frame\nrates, limited dynamic range, and data redundancy. Event cameras, with\nmicrosecond temporal resolution and high dynamic range, provide a more\neffective solution for SOD. However, existing event-based object detection\ndatasets are limited in scale, feature large targets size, and lack diverse\nbackgrounds, making them unsuitable for SOD benchmarks. In this paper, we\nintroduce a Event-based Small object detection (EVSOD) dataset (namely EV-UAV),\nthe first large-scale, highly diverse benchmark for anti-UAV tasks. It includes\n147 sequences with over 2.3 million event-level annotations, featuring\nextremely small targets (averaging 6.8 $\\times$ 5.4 pixels) and diverse\nscenarios such as urban clutter and extreme lighting conditions. Furthermore,\nbased on the observation that small moving targets form continuous curves in\nspatiotemporal event point clouds, we propose Event based Sparse Segmentation\nNetwork (EV-SpSegNet), a novel baseline for event segmentation in point cloud\nspace, along with a Spatiotemporal Correlation (STC) loss that leverages motion\ncontinuity to guide the network in retaining target events. Extensive\nexperiments on the EV-UAV dataset demonstrate the superiority of our method and\nprovide a benchmark for future research in EVSOD. The dataset and code are at\nhttps://github.com/ChenYichen9527/Ev-UAV.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23575v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23575v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22911", "title": "Learning Truthful Mechanisms without Discretization", "authors": ["Yunxuan Ma", "Siqiang Wang", "Zhijian Duan", "Yukun Cheng", "Xiaotie Deng"], "summary": "This paper introduces TEDI (Truthful, Expressive, and Dimension-Insensitive\napproach), a discretization-free algorithm to learn truthful and\nutility-maximizing mechanisms. Existing learning-based approaches often rely on\ndiscretization of outcome spaces to ensure truthfulness, which leads to\ninefficiency with increasing problem size. To address this limitation, we\nformalize the concept of pricing rules, defined as functions that map outcomes\nto prices. Based on this concept, we propose a novel menu mechanism, which can\nbe equivalent to a truthful direct mechanism under specific conditions. The\ncore idea of TEDI lies in its parameterization of pricing rules using Partial\nGroupMax Network, a new network architecture designed to universally\napproximate partial convex functions. To learn optimal pricing rules, we\ndevelop novel training techniques, including covariance trick and continuous\nsampling, to derive unbiased gradient estimators compatible with first-order\noptimization. Theoretical analysis establishes that TEDI guarantees\ntruthfulness, full expressiveness, and dimension-insensitivity. Experimental\nevaluation in the studied auction setting demonstrates that TEDI achieves\nstrong performance, competitive with or exceeding state-of-the-art methods.\n  This work presents the first approaches to learn truthful mechanisms without\noutcome discretization, thereby enhancing algorithmic efficiency. The proposed\nconcepts, network architecture, and learning techniques might offer potential\nvalue and provide new insights for automated mechanism design and\ndifferentiable economics.", "comment": "66 pages", "pdf_url": "http://arxiv.org/pdf/2506.22911v1", "categories": ["cs.GT", "cs.AI", "cs.LG"], "cate": "cs.GT", "url": "http://arxiv.org/abs/2506.22911v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23203", "title": "Multi-Branch DNN and CRLB-Ratio-Weight Fusion for Enhanced DOA Sensing via a Massive H$^2$AD MIMO Receiver", "authors": ["Feng Shu", "Jiatong Bai", "Di Wu", "Wei Zhu", "Bin Deng", "Fuhui Zhou", "Jiangzhou Wang"], "summary": "As a green MIMO structure, massive H$^2$AD is viewed as a potential\ntechnology for the future 6G wireless network. For such a structure, it is a\nchallenging task to design a low-complexity and high-performance fusion of\ntarget direction values sensed by different sub-array groups with fewer use of\nprior knowledge. To address this issue, a lightweight Cramer-Rao lower bound\n(CRLB)-ratio-weight fusion (WF) method is proposed, which approximates inverse\nCRLB of each subarray using antenna number reciprocals to eliminate real-time\nCRLB computation. This reduces complexity and prior knowledge dependence while\npreserving fusion performance. Moreover, a multi-branch deep neural network\n(MBDNN) is constructed to further enhance direction-of-arrival (DOA) sensing by\nleveraging candidate angles from multiple subarrays. The subarray-specific\nbranch networks are integrated with a shared regression module to effectively\neliminate pseudo-solutions and fuse true angles. Simulation results show that\nthe proposed CRLB-ratio-WF method achieves DOA sensing performance comparable\nto CRLB-based methods, while significantly reducing the reliance on prior\nknowledge. More notably, the proposed MBDNN has superior performance in low-SNR\nranges. At SNR $= -15$ dB, it achieves an order-of-magnitude improvement in\nestimation accuracy compared to CRLB-ratio-WF method.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23203v1", "categories": ["eess.SP", "cs.AI"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.23203v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23577", "title": "StackCLIP: Clustering-Driven Stacked Prompt in Zero-Shot Industrial Anomaly Detection", "authors": ["Yanning Hou", "Yanran Ruan", "Junfa Li", "Shanshan Wang", "Jianfeng Qiu", "Ke Xu"], "summary": "Enhancing the alignment between text and image features in the CLIP model is\na critical challenge in zero-shot industrial anomaly detection tasks. Recent\nstudies predominantly utilize specific category prompts during pretraining,\nwhich can cause overfitting to the training categories and limit model\ngeneralization. To address this, we propose a method that transforms category\nnames through multicategory name stacking to create stacked prompts, forming\nthe basis of our StackCLIP model. Our approach introduces two key components.\nThe Clustering-Driven Stacked Prompts (CSP) module constructs generic prompts\nby stacking semantically analogous categories, while utilizing multi-object\ntextual feature fusion to amplify discriminative anomalies among similar\nobjects. The Ensemble Feature Alignment (EFA) module trains knowledge-specific\nlinear layers tailored for each stack cluster and adaptively integrates them\nbased on the attributes of test categories. These modules work together to\ndeliver superior training speed, stability, and convergence, significantly\nboosting anomaly segmentation performance. Additionally, our stacked prompt\nframework offers robust generalization across classification tasks. To further\nimprove performance, we introduce the Regulating Prompt Learning (RPL) module,\nwhich leverages the generalization power of stacked prompts to refine prompt\nlearning, elevating results in anomaly detection classification tasks.\nExtensive testing on seven industrial anomaly detection datasets demonstrates\nthat our method achieves state-of-the-art performance in both zero-shot anomaly\ndetection and segmentation tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23577v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23577v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22935", "title": "Differentiable Radar Ambiguity Functions: Mathematical Formulation and Computational Implementation", "authors": ["Marc Bara Iniesta"], "summary": "The ambiguity function is fundamental to radar waveform design,\ncharacterizing range and Doppler resolution capabilities. However, its\ntraditional formulation involves non-differentiable operations, preventing\nintegration with gradient-based optimization methods and modern machine\nlearning frameworks. This paper presents the first complete mathematical\nframework and computational implementation for differentiable radar ambiguity\nfunctions. Our approach addresses the fundamental technical challenges that\nhave prevented the radar community from leveraging automatic differentiation:\nproper handling of complex-valued gradients using Wirtinger calculus, efficient\ncomputation through parallelized FFT operations, numerical stability throughout\ncascaded operations, and composability with arbitrary differentiable\noperations. We term this approach GRAF (Gradient-based Radar Ambiguity\nFunctions), which reformulates the ambiguity function computation to maintain\nmathematical equivalence while enabling gradient flow through the entire\npipeline. The resulting implementation provides a general-purpose\ndifferentiable ambiguity function compatible with modern automatic\ndifferentiation frameworks, enabling new research directions including neural\nnetwork-based waveform generation with ambiguity constraints, end-to-end\noptimization of radar systems, and integration of classical radar theory with\nmodern deep learning. We provide complete implementation details and\ndemonstrate computational efficiency suitable for practical applications. This\nwork establishes the mathematical and computational foundation for applying\nmodern machine learning techniques to radar waveform design, bridging classical\nradar signal processing with automatic differentiation frameworks.", "comment": "16 pages, 4 figures, source code available at\n  https://github.com/marcbara/graf-psl-lpi (DOI: 10.5281/zenodo.15763301)", "pdf_url": "http://arxiv.org/pdf/2506.22935v1", "categories": ["eess.SP", "cs.LG", "cs.NA", "math.NA", "94A12, 65T50, 68T05", "F.2.1; I.2.6; G.1.0"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.22935v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23210", "title": "FedRef: Communication-Efficient Bayesian Fine Tuning with Reference Model", "authors": ["Taehwan Yoon", "Bongjun Choi"], "summary": "Federated learning(FL) is used for distributed scenarios to train artificial\nintelligence(AI) models while ensuring users' privacy. In federated learning\nscenario, the server generally never knows about users' data. This type of\nconcept makes the AI training process efficient in terms of data privacy.\nHowever, regarding model performance, federated AI models may not sufficiently\nsatisfy AI users' expectations. Furthermore, AI users have a wide range of\ndifferent needs. It is not easy to satisfy the whole users needs. These types\nof issues can be addressed through AI model optimization, fine-tuning, or\npersonalization to achieve optimal model performance. To address model\noptimization challenges, we propose reference model-based federated learning\nfor optimal fine-tuning, which overcomes catastrophic forgetting in each round.\nThis method is derived from Bayesian parameter-efficient transfer learning,\nwhich includes an optimal proximal term and enables overcoming the catastrophic\nforgetting issue in each round by utilizing a reference model that incorporates\nprevious model parameters. As a result, this method achieves both high model\nperformance and low computing cost.", "comment": "6 pages,14 equation", "pdf_url": "http://arxiv.org/pdf/2506.23210v1", "categories": ["cs.LG", "cs.AI", "cs.DC"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.23210v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23580", "title": "Dataset Distillation via Vision-Language Category Prototype", "authors": ["Yawen Zou", "Guang Li", "Duo Su", "Zi Wang", "Jun Yu", "Chao Zhang"], "summary": "Dataset distillation (DD) condenses large datasets into compact yet\ninformative substitutes, preserving performance comparable to the original\ndataset while reducing storage, transmission costs, and computational\nconsumption. However, previous DD methods mainly focus on distilling\ninformation from images, often overlooking the semantic information inherent in\nthe data. The disregard for context hinders the model's generalization ability,\nparticularly in tasks involving complex datasets, which may result in illogical\noutputs or the omission of critical objects. In this study, we integrate\nvision-language methods into DD by introducing text prototypes to distill\nlanguage information and collaboratively synthesize data with image prototypes,\nthereby enhancing dataset distillation performance. Notably, the text\nprototypes utilized in this study are derived from descriptive text information\ngenerated by an open-source large language model. This framework demonstrates\nbroad applicability across datasets without pre-existing text descriptions,\nexpanding the potential of dataset distillation beyond traditional image-based\napproaches. Compared to other methods, the proposed approach generates\nlogically coherent images containing target objects, achieving state-of-the-art\nvalidation performance and demonstrating robust generalization. Source code and\ngenerated data are available in\nhttps://github.com/zou-yawen/Dataset-Distillation-via-Vision-Language-Category-Prototype/", "comment": "accepted by ICCV2025", "pdf_url": "http://arxiv.org/pdf/2506.23580v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23580v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22938", "title": "Efficient Cybersecurity Assessment Using SVM and Fuzzy Evidential Reasoning for Resilient Infrastructure", "authors": ["Zaydon L. Ali", "Wassan Saad Abduljabbar Hayale", "Israa Ibraheem Al_Barazanchi", "Ravi Sekhar", "Pritesh Shah", "Sushma Parihar"], "summary": "With current advancement in hybermedia knowledges, the privacy of digital\ninformation has developed a critical problem. To overawed the susceptibilities\nof present security protocols, scholars tend to focus mainly on efforts on\nalternation of current protocols. Over past decade, various proposed encoding\nmodels have been shown insecurity, leading to main threats against significant\ndata. Utilizing the suitable encryption model is very vital means of guard\nagainst various such, but algorithm is selected based on the dependency of data\nwhich need to be secured. Moreover, testing potentiality of the security\nassessment one by one to identify the best choice can take a vital time for\nprocessing. For faster and precisive identification of assessment algorithm, we\nsuggest a security phase exposure model for cipher encryption technique by\ninvoking Support Vector Machine (SVM). In this work, we form a dataset using\nusual security components like contrast, homogeneity. To overcome the\nuncertainty in analysing the security and lack of ability of processing data to\na risk assessment mechanism. To overcome with such complications, this paper\nproposes an assessment model for security issues using fuzzy evidential\nreasoning (ER) approaches. Significantly, the model can be utilised to process\nand assemble risk assessment data on various aspects in systematic ways. To\nestimate the performance of our framework, we have various analyses like,\nrecall, F1 score and accuracy.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22938v1", "categories": ["cs.CR", "cs.LG"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.22938v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23219", "title": "UrbanLLaVA: A Multi-modal Large Language Model for Urban Intelligence with Spatial Reasoning and Understanding", "authors": ["Jie Feng", "Shengyuan Wang", "Tianhui Liu", "Yanxin Xi", "Yong Li"], "summary": "Urban research involves a wide range of scenarios and tasks that require the\nunderstanding of multi-modal data. Current methods often focus on specific data\ntypes and lack a unified framework in urban field for processing them\ncomprehensively. The recent success of multi-modal large language models\n(MLLMs) presents a promising opportunity to overcome this limitation. In this\npaper, we introduce $\\textit{UrbanLLaVA}$, a multi-modal large language model\ndesigned to process these four types of data simultaneously and achieve strong\nperformance across diverse urban tasks compared with general MLLMs. In\n$\\textit{UrbanLLaVA}$, we first curate a diverse urban instruction dataset\nencompassing both single-modal and cross-modal urban data, spanning from\nlocation view to global view of urban environment. Additionally, we propose a\nmulti-stage training framework that decouples spatial reasoning enhancement\nfrom domain knowledge learning, thereby improving the compatibility and\ndownstream performance of $\\textit{UrbanLLaVA}$ across diverse urban tasks.\nFinally, we also extend existing benchmark for urban research to assess the\nperformance of MLLMs across a wide range of urban tasks. Experimental results\nfrom three cities demonstrate that $\\textit{UrbanLLaVA}$ outperforms\nopen-source and proprietary MLLMs in both single-modal tasks and complex\ncross-modal tasks and shows robust generalization abilities across cities.\nSource codes and data are openly accessible to the research community via\nhttps://github.com/tsinghua-fib-lab/UrbanLLaVA.", "comment": "Accepted by ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2506.23219v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23219v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23581", "title": "PBCAT: Patch-based composite adversarial training against physically realizable attacks on object detection", "authors": ["Xiao Li", "Yiming Zhu", "Yifan Huang", "Wei Zhang", "Yingzhe He", "Jie Shi", "Xiaolin Hu"], "summary": "Object detection plays a crucial role in many security-sensitive\napplications. However, several recent studies have shown that object detectors\ncan be easily fooled by physically realizable attacks, \\eg, adversarial patches\nand recent adversarial textures, which pose realistic and urgent threats.\nAdversarial Training (AT) has been recognized as the most effective defense\nagainst adversarial attacks. While AT has been extensively studied in the\n$l_\\infty$ attack settings on classification models, AT against physically\nrealizable attacks on object detectors has received limited exploration. Early\nattempts are only performed to defend against adversarial patches, leaving AT\nagainst a wider range of physically realizable attacks under-explored. In this\nwork, we consider defending against various physically realizable attacks with\na unified AT method. We propose PBCAT, a novel Patch-Based Composite\nAdversarial Training strategy. PBCAT optimizes the model by incorporating the\ncombination of small-area gradient-guided adversarial patches and imperceptible\nglobal adversarial perturbations covering the entire image. With these designs,\nPBCAT has the potential to defend against not only adversarial patches but also\nunseen physically realizable attacks such as adversarial textures. Extensive\nexperiments in multiple settings demonstrated that PBCAT significantly improved\nrobustness against various physically realizable attacks over state-of-the-art\ndefense methods. Notably, it improved the detection accuracy by 29.7\\% over\nprevious defense methods under one recent adversarial texture attack.", "comment": "Accepted by ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2506.23581v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23581v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22939", "title": "Utilizing a Novel Deep Learning Method for Scene Categorization in Remote Sensing Data", "authors": ["Ghufran A. Omran", "Wassan Saad Abduljabbar Hayale", "Ahmad AbdulQadir AlRababah", "Israa Ibraheem Al-Barazanchi", "Ravi Sekhar", "Pritesh Shah", "Sushma Parihar", "Harshavardhan Reddy Penubadi"], "summary": "Scene categorization (SC) in remotely acquired images is an important subject\nwith broad consequences in different fields, including catastrophe control,\necological observation, architecture for cities, and more. Nevertheless, its\nseveral apps, reaching a high degree of accuracy in SC from distant observation\ndata has demonstrated to be difficult. This is because traditional conventional\ndeep learning models require large databases with high variety and high levels\nof noise to capture important visual features. To address these problems, this\ninvestigation file introduces an innovative technique referred to as the\nCuttlefish Optimized Bidirectional Recurrent Neural Network (CO- BRNN) for type\nof scenes in remote sensing data. The investigation compares the execution of\nCO-BRNN with current techniques, including Multilayer Perceptron- Convolutional\nNeural Network (MLP-CNN), Convolutional Neural Network-Long Short Term Memory\n(CNN-LSTM), and Long Short Term Memory-Conditional Random Field (LSTM-CRF),\nGraph-Based (GB), Multilabel Image Retrieval Model (MIRM-CF), Convolutional\nNeural Networks Data Augmentation (CNN-DA). The results demonstrate that\nCO-BRNN attained the maximum accuracy of 97%, followed by LSTM-CRF with 90%,\nMLP-CNN with 85%, and CNN-LSTM with 80%. The study highlights the significance\nof physical confirmation to ensure the efficiency of satellite data.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22939v1", "categories": ["cs.CV", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22939v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23236", "title": "VolumetricSMPL: A Neural Volumetric Body Model for Efficient Interactions, Contacts, and Collisions", "authors": ["Marko Mihajlovic", "Siwei Zhang", "Gen Li", "Kaifeng Zhao", "Lea M√ºller", "Siyu Tang"], "summary": "Parametric human body models play a crucial role in computer graphics and\nvision, enabling applications ranging from human motion analysis to\nunderstanding human-environment interactions. Traditionally, these models use\nsurface meshes, which pose challenges in efficiently handling interactions with\nother geometric entities, such as objects and scenes, typically represented as\nmeshes or point clouds. To address this limitation, recent research has\nexplored volumetric neural implicit body models. However, existing works are\neither insufficiently robust for complex human articulations or impose high\ncomputational and memory costs, limiting their widespread use. To this end, we\nintroduce VolumetricSMPL, a neural volumetric body model that leverages Neural\nBlend Weights (NBW) to generate compact, yet efficient MLP decoders. Unlike\nprior approaches that rely on large MLPs, NBW dynamically blends a small set of\nlearned weight matrices using predicted shape- and pose-dependent coefficients,\nsignificantly improving computational efficiency while preserving\nexpressiveness. VolumetricSMPL outperforms prior volumetric occupancy model\nCOAP with 10x faster inference, 6x lower GPU memory usage, enhanced accuracy,\nand a Signed Distance Function (SDF) for efficient and differentiable contact\nmodeling. We demonstrate VolumetricSMPL's strengths across four challenging\ntasks: (1) reconstructing human-object interactions from in-the-wild images,\n(2) recovering human meshes in 3D scenes from egocentric views, (3)\nscene-constrained motion synthesis, and (4) resolving self-intersections. Our\nresults highlight its broad applicability and significant performance and\nefficiency gains.", "comment": "[ICCV 2025] https://markomih.github.io/VolumetricSMPL", "pdf_url": "http://arxiv.org/pdf/2506.23236v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23236v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23590", "title": "CAI: Caption-Sensitive Attention Intervention for Mitigating Object Hallucination in Large Vision-Language Models", "authors": ["Qiming Li", "Zekai Ye", "Xiaocheng Feng", "Weihong Zhong", "Libo Qin", "Ruihan Chen", "Baohang Li", "Kui Jiang", "Yaowei Wang", "Ting Liu", "Bing Qin"], "summary": "Although Large Vision-Language Models (LVLMs) have demonstrated powerful\ncapabilities in interpreting visual information, they frequently produce\ncontent that deviates from visual information, leading to object hallucination.\nTo tackle this, recent works mostly depend on expensive manual annotations and\ntraining cost, or significantly increase inference time. In this work, we\nobserve that LVLMs' attention to visual information is significantly stronger\nwhen answering caption queries compared to non-caption queries. Inspired by\nthis phenomenon, we propose Caption-sensitive Attention Intervention (CAI), a\ntraining-free, plug-and-play hallucination mitigation method that leverages the\nattention activation pattern in response to caption queries to enhance LVLMs'\nvisual perception capability. Extensive experimental results across four\nbenchmarks covering both discriminative and generative tasks, demonstrate that\nCAI achieves state-of-the-art (SOTA) hallucination mitigating performance only\nwith minimal additional inference cost.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23590v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23590v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22949", "title": "A Study on Semi-Supervised Detection of DDoS Attacks under Class Imbalance", "authors": ["Ehsan Hallaji", "Vaishnavi Shanmugam", "Roozbeh Razavi-Far", "Mehrdad Saif"], "summary": "One of the most difficult challenges in cybersecurity is eliminating\nDistributed Denial of Service (DDoS) attacks. Automating this task using\nartificial intelligence is a complex process due to the inherent class\nimbalance and lack of sufficient labeled samples of real-world datasets. This\nresearch investigates the use of Semi-Supervised Learning (SSL) techniques to\nimprove DDoS attack detection when data is imbalanced and partially labeled. In\nthis process, 13 state-of-the-art SSL algorithms are evaluated for detecting\nDDoS attacks in several scenarios. We evaluate their practical efficacy and\nshortcomings, including the extent to which they work in extreme environments.\nThe results will offer insight into designing intelligent Intrusion Detection\nSystems (IDSs) that are robust against class imbalance and handle partially\nlabeled data.", "comment": "Accepted for publication in IEEE CCECE 2025", "pdf_url": "http://arxiv.org/pdf/2506.22949v1", "categories": ["cs.CR", "cs.AI", "cs.LG"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.22949v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23247", "title": "Aggregating Local Saliency Maps for Semi-Global Explainable Image Classification", "authors": ["James Hinns", "David Martens"], "summary": "Deep learning dominates image classification tasks, yet understanding how\nmodels arrive at predictions remains a challenge. Much research focuses on\nlocal explanations of individual predictions, such as saliency maps, which\nvisualise the influence of specific pixels on a model's prediction. However,\nreviewing many of these explanations to identify recurring patterns is\ninfeasible, while global methods often oversimplify and miss important local\nbehaviours. To address this, we propose Segment Attribution Tables (SATs), a\nmethod for summarising local saliency explanations into (semi-)global insights.\nSATs take image segments (such as \"eyes\" in Chihuahuas) and leverage saliency\nmaps to quantify their influence. These segments highlight concepts the model\nrelies on across instances and reveal spurious correlations, such as reliance\non backgrounds or watermarks, even when out-of-distribution test performance\nsees little change. SATs can explain any classifier for which a form of\nsaliency map can be produced, using segmentation maps that provide named\nsegments. SATs bridge the gap between oversimplified global summaries and\noverly detailed local explanations, offering a practical tool for analysing and\ndebugging image classifiers.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23247v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23247v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23605", "title": "AI-Generated Lecture Slides for Improving Slide Element Detection and Retrieval", "authors": ["Suyash Maniyar", "Vishvesh Trivedi", "Ajoy Mondal", "Anand Mishra", "C. V. Jawahar"], "summary": "Lecture slide element detection and retrieval are key problems in slide\nunderstanding. Training effective models for these tasks often depends on\nextensive manual annotation. However, annotating large volumes of lecture\nslides for supervised training is labor intensive and requires domain\nexpertise. To address this, we propose a large language model (LLM)-guided\nsynthetic lecture slide generation pipeline, SynLecSlideGen, which produces\nhigh-quality, coherent and realistic slides. We also create an evaluation\nbenchmark, namely RealSlide by manually annotating 1,050 real lecture slides.\nTo assess the utility of our synthetic slides, we perform few-shot transfer\nlearning on real data using models pre-trained on them. Experimental results\nshow that few-shot transfer learning with pretraining on synthetic slides\nsignificantly improves performance compared to training only on real data. This\ndemonstrates that synthetic data can effectively compensate for limited labeled\nlecture slides. The code and resources of our work are publicly available on\nour project website: https://synslidegen.github.io/.", "comment": "40 pages including supplementary, accepted at ICDAR 2025", "pdf_url": "http://arxiv.org/pdf/2506.23605v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23605v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22963", "title": "CN-SBM: Categorical Block Modelling For Primary and Residual Copy Number Variation", "authors": ["Kevin Lam", "William Daniels", "J Maxwell Douglas", "Daniel Lai", "Samuel Aparicio", "Benjamin Bloem-Reddy", "Yongjin Park"], "summary": "Cancer is a genetic disorder whose clonal evolution can be monitored by\ntracking noisy genome-wide copy number variants. We introduce the Copy Number\nStochastic Block Model (CN-SBM), a probabilistic framework that jointly\nclusters samples and genomic regions based on discrete copy number states using\na bipartite categorical block model. Unlike models relying on Gaussian or\nPoisson assumptions, CN-SBM respects the discrete nature of CNV calls and\ncaptures subpopulation-specific patterns through block-wise structure. Using a\ntwo-stage approach, CN-SBM decomposes CNV data into primary and residual\ncomponents, enabling detection of both large-scale chromosomal alterations and\nfiner aberrations. We derive a scalable variational inference algorithm for\napplication to large cohorts and high-resolution data. Benchmarks on simulated\nand real datasets show improved model fit over existing methods. Applied to\nTCGA low-grade glioma data, CN-SBM reveals clinically relevant subtypes and\nstructured residual variation, aiding patient stratification in survival\nanalysis. These results establish CN-SBM as an interpretable, scalable\nframework for CNV analysis with direct relevance for tumor heterogeneity and\nprognosis.", "comment": "8 pages, 4 figures", "pdf_url": "http://arxiv.org/pdf/2506.22963v1", "categories": ["stat.ML", "cs.LG", "q-bio.GN"], "cate": "stat.ML", "url": "http://arxiv.org/abs/2506.22963v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23254", "title": "PixelBoost: Leveraging Brownian Motion for Realistic-Image Super-Resolution", "authors": ["Aradhana Mishra", "Bumshik Lee"], "summary": "Diffusion-model-based image super-resolution techniques often face a\ntrade-off between realistic image generation and computational efficiency. This\nissue is exacerbated when inference times by decreasing sampling steps,\nresulting in less realistic and hazy images. To overcome this challenge, we\nintroduce a novel diffusion model named PixelBoost that underscores the\nsignificance of embracing the stochastic nature of Brownian motion in advancing\nimage super-resolution, resulting in a high degree of realism, particularly\nfocusing on texture and edge definitions. By integrating controlled\nstochasticity into the training regimen, our proposed model avoids convergence\nto local optima, effectively capturing and reproducing the inherent uncertainty\nof image textures and patterns. Our proposed model demonstrates superior\nobjective results in terms of learned perceptual image patch similarity\n(LPIPS), lightness order error (LOE), peak signal-to-noise ratio(PSNR),\nstructural similarity index measure (SSIM), as well as visual quality. To\ndetermine the edge enhancement, we evaluated the gradient magnitude and pixel\nvalue, and our proposed model exhibited a better edge reconstruction\ncapability. Additionally, our model demonstrates adaptive learning capabilities\nby effectively adjusting to Brownian noise patterns and introduces a sigmoidal\nnoise sequencing method that simplifies training, resulting in faster inference\nspeeds.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23254v1", "categories": ["cs.CV", "cs.AI", "cs.MM", "eess.IV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23254v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23606", "title": "SG-LDM: Semantic-Guided LiDAR Generation via Latent-Aligned Diffusion", "authors": ["Zhengkang Xiang", "Zizhao Li", "Amir Khodabandeh", "Kourosh Khoshelham"], "summary": "Lidar point cloud synthesis based on generative models offers a promising\nsolution to augment deep learning pipelines, particularly when real-world data\nis scarce or lacks diversity. By enabling flexible object manipulation, this\nsynthesis approach can significantly enrich training datasets and enhance\ndiscriminative models. However, existing methods focus on unconditional lidar\npoint cloud generation, overlooking their potential for real-world\napplications. In this paper, we propose SG-LDM, a Semantic-Guided Lidar\nDiffusion Model that employs latent alignment to enable robust\nsemantic-to-lidar synthesis. By directly operating in the native lidar space\nand leveraging explicit semantic conditioning, SG-LDM achieves state-of-the-art\nperformance in generating high-fidelity lidar point clouds guided by semantic\nlabels. Moreover, we propose the first diffusion-based lidar translation\nframework based on SG-LDM, which enables cross-domain translation as a domain\nadaptation strategy to enhance downstream perception performance. Systematic\nexperiments demonstrate that SG-LDM significantly outperforms existing lidar\ndiffusion models and the proposed lidar translation framework further improves\ndata augmentation performance in the downstream lidar segmentation task.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23606v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23606v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22967", "title": "ActAlign: Zero-Shot Fine-Grained Video Classification via Language-Guided Sequence Alignment", "authors": ["Amir Aghdam", "Vincent Tao Hu"], "summary": "We address the task of zero-shot fine-grained video classification, where no\nvideo examples or temporal annotations are available for unseen action classes.\nWhile contrastive vision-language models such as SigLIP demonstrate strong\nopen-set recognition via mean-pooled image-text similarity, they fail to\ncapture the temporal structure critical for distinguishing fine-grained\nactivities. We introduce ActAlign, a zero-shot framework that formulates video\nclassification as sequence alignment. For each class, a large language model\ngenerates an ordered sub-action sequence, which is aligned with video frames\nusing Dynamic Time Warping (DTW) in a shared embedding space. Without any\nvideo-text supervision or fine-tuning, ActAlign achieves 30.5% accuracy on the\nextremely challenging ActionAtlas benchmark, where human accuracy is only\n61.6%. ActAlign outperforms billion-parameter video-language models while using\napproximately 8x less parameters. These results demonstrate that structured\nlanguage priors, combined with classical alignment techniques, offer a scalable\nand general approach to unlocking the open-set recognition potential of\nvision-language models for fine-grained video understanding.", "comment": "Preprint manuscript - Project page:\n  https://github.com/aghdamamir/act-align", "pdf_url": "http://arxiv.org/pdf/2506.22967v1", "categories": ["cs.CV", "cs.LG", "cs.MM", "I.2.10; I.2.7"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.22967v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23260", "title": "From Prompt Injections to Protocol Exploits: Threats in LLM-Powered AI Agents Workflows", "authors": ["Mohamed Amine Ferrag", "Norbert Tihanyi", "Djallel Hamouda", "Leandros Maglaras", "Merouane Debbah"], "summary": "Autonomous AI agents powered by large language models (LLMs) with structured\nfunction-calling interfaces have dramatically expanded capabilities for\nreal-time data retrieval, complex computation, and multi-step orchestration.\nYet, the explosive proliferation of plugins, connectors, and inter-agent\nprotocols has outpaced discovery mechanisms and security practices, resulting\nin brittle integrations vulnerable to diverse threats. In this survey, we\nintroduce the first unified, end-to-end threat model for LLM-agent ecosystems,\nspanning host-to-tool and agent-to-agent communications, formalize adversary\ncapabilities and attacker objectives, and catalog over thirty attack\ntechniques. Specifically, we organized the threat model into four domains:\nInput Manipulation (e.g., prompt injections, long-context hijacks, multimodal\nadversarial inputs), Model Compromise (e.g., prompt- and parameter-level\nbackdoors, composite and encrypted multi-backdoors, poisoning strategies),\nSystem and Privacy Attacks (e.g., speculative side-channels, membership\ninference, retrieval poisoning, social-engineering simulations), and Protocol\nVulnerabilities (e.g., exploits in Model Context Protocol (MCP), Agent\nCommunication Protocol (ACP), Agent Network Protocol (ANP), and Agent-to-Agent\n(A2A) protocol). For each category, we review representative scenarios, assess\nreal-world feasibility, and evaluate existing defenses. Building on our threat\ntaxonomy, we identify key open challenges and future research directions, such\nas securing MCP deployments through dynamic trust management and cryptographic\nprovenance tracking; designing and hardening Agentic Web Interfaces; and\nachieving resilience in multi-agent and federated environments. Our work\nprovides a comprehensive reference to guide the design of robust defense\nmechanisms and establish best practices for resilient LLM-agent workflows.", "comment": "29 pages, 15 figures, 6 tables", "pdf_url": "http://arxiv.org/pdf/2506.23260v1", "categories": ["cs.CR", "cs.AI"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.23260v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23607", "title": "PGOV3D: Open-Vocabulary 3D Semantic Segmentation with Partial-to-Global Curriculum", "authors": ["Shiqi Zhang", "Sha Zhang", "Jiajun Deng", "Yedong Shen", "Mingxiao MA", "Yanyong Zhang"], "summary": "Existing open-vocabulary 3D semantic segmentation methods typically supervise\n3D segmentation models by merging text-aligned features (e.g., CLIP) extracted\nfrom multi-view images onto 3D points. However, such approaches treat\nmulti-view images merely as intermediaries for transferring open-vocabulary\ninformation, overlooking their rich semantic content and cross-view\ncorrespondences, which limits model effectiveness. To address this, we propose\nPGOV3D, a novel framework that introduces a Partial-to-Global curriculum for\nimproving open-vocabulary 3D semantic segmentation. The key innovation lies in\na two-stage training strategy. In the first stage, we pre-train the model on\npartial scenes that provide dense semantic information but relatively simple\ngeometry. These partial point clouds are derived from multi-view RGB-D inputs\nvia pixel-wise depth projection. To enable open-vocabulary learning, we\nleverage a multi-modal large language model (MLLM) and a 2D segmentation\nfoundation model to generate open-vocabulary labels for each viewpoint,\noffering rich and aligned supervision. An auxiliary inter-frame consistency\nmodule is introduced to enforce feature consistency across varying viewpoints\nand enhance spatial understanding. In the second stage, we fine-tune the model\non complete scene-level point clouds, which are sparser and structurally more\ncomplex. We aggregate the partial vocabularies associated with each scene and\ngenerate pseudo labels using the pre-trained model, effectively bridging the\nsemantic gap between dense partial observations and large-scale 3D\nenvironments. Extensive experiments on ScanNet, ScanNet200, and S3DIS\nbenchmarks demonstrate that PGOV3D achieves competitive performance in\nopen-vocabulary 3D semantic segmentation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23607v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23607v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22971", "title": "Hierarchical Decentralized Stochastic Control for Cyber-Physical Systems", "authors": ["Kesav Kazam Ramachandran Anantharaman", "Rahul Meshram"], "summary": "This paper presents a two-timescale hierarchical decentralized architecture\nfor control of Cyber-Physical Systems. The architecture consists of $N$\nindependent sub-processes, a global controller, and $N$ local controllers, each\nformulated as a Markov Decision Process (MDP). The global controller, operating\nat a slower timescale optimizes the infinite-horizon discounted cumulative\nreward under budget constraints. For the local controllers, operating at a\nfaster timescale, we propose two different optimization frameworks, namely the\nCOpt and FOpt. In the COpt framework, the local controller also optimizes an\ninfinite-horizon MDP, while in the FOpt framework, the local controller\noptimizes a finite-horizon MDP. The FOpt framework mimics a federal structure,\nwhere the local controllers have more autonomy in their decision making. First,\nthe existence of stationary deterministic optimal policies for both these\nframeworks is established. Then, various relationships between the two\nframeworks are studied, including a bound on the difference between the two\noptimal value functions. Additionally, sufficiency conditions are provided such\nthat the two frameworks lead to the same optimal values.", "comment": "6 pages, 2 figures", "pdf_url": "http://arxiv.org/pdf/2506.22971v1", "categories": ["eess.SY", "cs.LG", "cs.MA", "cs.SY", "math.OC"], "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.22971v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23270", "title": "Token Activation Map to Visually Explain Multimodal LLMs", "authors": ["Yi Li", "Hualiang Wang", "Xinpeng Ding", "Haonan Wang", "Xiaomeng Li"], "summary": "Multimodal large language models (MLLMs) are broadly empowering various\nfields. Despite their advancements, the explainability of MLLMs remains less\nexplored, hindering deeper understanding, model credibility, and effective\nvisualization. Unlike conventional vision models (e.g., CNNs, ViTs, CLIP) that\nproduce a single output, MLLMs generate sequences of tokens progressively,\nwhere each generated token depends on the previous context. Therefore, earlier\ncontext tokens can introduce redundant activations that interfere with the\nexplanation of later tokens beyond their original information. Existing studies\noften overlook this issue, but our observations reveal that these redundant\ncorrelations can significantly hurt the reliability of explanations. To address\nthis, we propose an estimated causal inference method to mitigate the\ninterference of context to achieve high-quality MLLM explanation, with a novel\nrank Gaussian filter to further reduce activation noises. We term this method\nToken Activation Map (TAM) to highlight the consideration of interactions\nbetween tokens. TAM also indicates that it excels at explaining multiple tokens\nof MLLM, which is different from the Class Activation Map (CAM) for a single\nprediction. Our TAM method significantly outperforms existing SoTA methods,\nshowcasing high-quality visualization results that can be utilized for various\nscenarios, such as object localization, failure case analysis, video\nvisualization, MLLMs visual comparison, and model understanding (e.g., color,\nshape, action, location, visual reasoning, multi-turn conversation, etc). The\ncode is available atgithub.com/xmed-lab/TAM.", "comment": "ICCV2025 Accepted", "pdf_url": "http://arxiv.org/pdf/2506.23270v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23270v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23611", "title": "AttentionGS: Towards Initialization-Free 3D Gaussian Splatting via Structural Attention", "authors": ["Ziao Liu", "Zhenjia Li", "Yifeng Shi", "Xiangang Li"], "summary": "3D Gaussian Splatting (3DGS) is a powerful alternative to Neural Radiance\nFields (NeRF), excelling in complex scene reconstruction and efficient\nrendering. However, it relies on high-quality point clouds from\nStructure-from-Motion (SfM), limiting its applicability. SfM also fails in\ntexture-deficient or constrained-view scenarios, causing severe degradation in\n3DGS reconstruction. To address this limitation, we propose AttentionGS, a\nnovel framework that eliminates the dependency on high-quality initial point\nclouds by leveraging structural attention for direct 3D reconstruction from\nrandomly initialization. In the early training stage, we introduce geometric\nattention to rapidly recover the global scene structure. As training\nprogresses, we incorporate texture attention to refine fine-grained details and\nenhance rendering quality. Furthermore, we employ opacity-weighted gradients to\nguide Gaussian densification, leading to improved surface reconstruction.\nExtensive experiments on multiple benchmark datasets demonstrate that\nAttentionGS significantly outperforms state-of-the-art methods, particularly in\nscenarios where point cloud initialization is unreliable. Our approach paves\nthe way for more robust and flexible 3D Gaussian Splatting in real-world\napplications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23611v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23611v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22977", "title": "On the Generalizability of \"Competition of Mechanisms: Tracing How Language Models Handle Facts and Counterfactuals\"", "authors": ["Asen Dotsinski", "Udit Thakur", "Marko Ivanov", "Mohammad Hafeez Khan", "Maria Heuss"], "summary": "We present a reproduction study of \"Competition of Mechanisms: Tracing How\nLanguage Models Handle Facts and Counterfactuals\" (Ortu et al., 2024), which\ninvestigates competition of mechanisms in language models between factual\nrecall and counterfactual in-context repetition. Our study successfully\nreproduces their primary findings regarding the localization of factual and\ncounterfactual information, the dominance of attention blocks in mechanism\ncompetition, and the specialization of attention heads in handling competing\ninformation. We reproduce their results on both GPT-2 (Radford et al., 2019)\nand Pythia 6.9B (Biderman et al., 2023). We extend their work in three\nsignificant directions. First, we explore the generalizability of these\nfindings to even larger models by replicating the experiments on Llama 3.1 8B\n(Grattafiori et al., 2024), discovering greatly reduced attention head\nspecialization. Second, we investigate the impact of prompt structure by\nintroducing variations where we avoid repeating the counterfactual statement\nverbatim or we change the premise word, observing a marked decrease in the\nlogit for the counterfactual token. Finally, we test the validity of the\nauthors' claims for prompts of specific domains, discovering that certain\ncategories of prompts skew the results by providing the factual prediction\ntoken as part of the subject of the sentence. Overall, we find that the\nattention head ablation proposed in Ortu et al. (2024) is ineffective for\ndomains that are underrepresented in their dataset, and that the effectiveness\nvaries based on model architecture, prompt structure, domain and task.", "comment": "22 pages, 25 figures. For an interactive dashboard with all figures,\n  see https://comp-mech-generalizability.streamlit.app/ . For the accompanying\n  code, see https://github.com/asendotsinski/comp-mech-generalizability . To be\n  published in proceedings of the 2025 Machine Learning Reproducibility\n  Challenge", "pdf_url": "http://arxiv.org/pdf/2506.22977v1", "categories": ["cs.CL", "cs.LG"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.22977v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23274", "title": "Predicting thinking time in Reasoning models", "authors": ["Hans Peter Lynsg√∏e Raaschou-jensen", "Constanza Fierro", "Anders S√∏gaard"], "summary": "Reasoning models that produce long, hidden chains of thought have emerged as\npowerful tools for complex, reasoning-intensive\ntasks\\citep{deepseekai2025deepseekr1incentivizingreasoningcapability,\nopenai2024openaio1card}. However, this paradigm introduces a new user\nexperience challenge: users have little insight into how much time the model\nwill spend reasoning before returning an answer. This unpredictability, can\nlead to user frustration and is likely to compound as LLMs can produce\nincreasingly long tasks asynchronously\n\\citep{kwa2025measuringaiabilitycomplete}. In this paper, we introduce and\nevaluate methods for both online and offline prediction of model \"thinking\ntime,\" aiming to develop a practical \"progress bar for reasoning.\" We discuss\nthe implications for user interaction and future research directions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23274v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.23274v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23618", "title": "TurboVSR: Fantastic Video Upscalers and Where to Find Them", "authors": ["Zhongdao Wang", "Guodongfang Zhao", "Jingjing Ren", "Bailan Feng", "Shifeng Zhang", "Wenbo Li"], "summary": "Diffusion-based generative models have demonstrated exceptional promise in\nthe video super-resolution (VSR) task, achieving a substantial advancement in\ndetail generation relative to prior methods. However, these approaches face\nsignificant computational efficiency challenges. For instance, current\ntechniques may require tens of minutes to super-resolve a mere 2-second, 1080p\nvideo. In this paper, we present TurboVSR, an ultra-efficient diffusion-based\nvideo super-resolution model. Our core design comprises three key aspects: (1)\nWe employ an autoencoder with a high compression ratio of 32$\\times$32$\\times$8\nto reduce the number of tokens. (2) Highly compressed latents pose substantial\nchallenges for training. We introduce factorized conditioning to mitigate the\nlearning complexity: we first learn to super-resolve the initial frame;\nsubsequently, we condition the super-resolution of the remaining frames on the\nhigh-resolution initial frame and the low-resolution subsequent frames. (3) We\nconvert the pre-trained diffusion model to a shortcut model to enable fewer\nsampling steps, further accelerating inference. As a result, TurboVSR performs\non par with state-of-the-art VSR methods, while being 100+ times faster, taking\nonly 7 seconds to process a 2-second long 1080p video. TurboVSR also supports\nimage resolution by considering image as a one-frame video. Our efficient\ndesign makes SR beyond 1080p possible, results on 4K (3648$\\times$2048) image\nSR show surprising fine details.", "comment": "ICCV, 2025", "pdf_url": "http://arxiv.org/pdf/2506.23618v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23618v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23010", "title": "On Universality of Non-Separable Approximate Message Passing Algorithms", "authors": ["Max Lovig", "Tianhao Wang", "Zhou Fan"], "summary": "Mean-field characterizations of first-order iterative algorithms -- including\nApproximate Message Passing (AMP), stochastic and proximal gradient descent,\nand Langevin diffusions -- have enabled a precise understanding of learning\ndynamics in many statistical applications. For algorithms whose non-linearities\nhave a coordinate-separable form, it is known that such characterizations enjoy\na degree of universality with respect to the underlying data distribution.\nHowever, mean-field characterizations of non-separable algorithm dynamics have\nlargely remained restricted to i.i.d. Gaussian or rotationally-invariant data.\n  In this work, we initiate a study of universality for non-separable AMP\nalgorithms. We identify a general condition for AMP with polynomial\nnon-linearities, in terms of a Bounded Composition Property (BCP) for their\nrepresenting tensors, to admit a state evolution that holds universally for\nmatrices with non-Gaussian entries. We then formalize a condition of\nBCP-approximability for Lipschitz AMP algorithms to enjoy a similar universal\nguarantee. We demonstrate that many common classes of non-separable\nnon-linearities are BCP-approximable, including local denoisers, spectral\ndenoisers for generic signals, and compositions of separable functions with\ngeneric linear maps, implying the universality of state evolution for AMP\nalgorithms employing these non-linearities.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23010v1", "categories": ["math.ST", "cs.IT", "cs.LG", "math.IT", "math.PR", "stat.TH"], "cate": "math.ST", "url": "http://arxiv.org/abs/2506.23010v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23275", "title": "Why Settle for One? Text-to-ImageSet Generation and Evaluation", "authors": ["Chengyou Jia", "Xin Shen", "Zhuohang Dang", "Zhuohang Dang", "Changliang Xia", "Weijia Wu", "Xinyu Zhang", "Hangwei Qian", "Ivor W. Tsang", "Minnan Luo"], "summary": "Despite remarkable progress in Text-to-Image models, many real-world\napplications require generating coherent image sets with diverse consistency\nrequirements. Existing consistent methods often focus on a specific domain with\nspecific aspects of consistency, which significantly constrains their\ngeneralizability to broader applications. In this paper, we propose a more\nchallenging problem, Text-to-ImageSet (T2IS) generation, which aims to generate\nsets of images that meet various consistency requirements based on user\ninstructions. To systematically study this problem, we first introduce\n$\\textbf{T2IS-Bench}$ with 596 diverse instructions across 26 subcategories,\nproviding comprehensive coverage for T2IS generation. Building on this, we\npropose $\\textbf{T2IS-Eval}$, an evaluation framework that transforms user\ninstructions into multifaceted assessment criteria and employs effective\nevaluators to adaptively assess consistency fulfillment between criteria and\ngenerated sets. Subsequently, we propose $\\textbf{AutoT2IS}$, a training-free\nframework that maximally leverages pretrained Diffusion Transformers'\nin-context capabilities to harmonize visual elements to satisfy both\nimage-level prompt alignment and set-level visual consistency. Extensive\nexperiments on T2IS-Bench reveal that diverse consistency challenges all\nexisting methods, while our AutoT2IS significantly outperforms current\ngeneralized and even specialized approaches. Our method also demonstrates the\nability to enable numerous underexplored real-world applications, confirming\nits substantial practical value. Visit our project in\nhttps://chengyou-jia.github.io/T2IS-Home.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23275v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23275v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23623", "title": "Revisiting Audio-Visual Segmentation with Vision-Centric Transformer", "authors": ["Shaofei Huang", "Rui Ling", "Tianrui Hui", "Hongyu Li", "Xu Zhou", "Shifeng Zhang", "Si Liu", "Richang Hong", "Meng Wang"], "summary": "Audio-Visual Segmentation (AVS) aims to segment sound-producing objects in\nvideo frames based on the associated audio signal. Prevailing AVS methods\ntypically adopt an audio-centric Transformer architecture, where object queries\nare derived from audio features. However, audio-centric Transformers suffer\nfrom two limitations: perception ambiguity caused by the mixed nature of audio,\nand weakened dense prediction ability due to visual detail loss. To address\nthese limitations, we propose a new Vision-Centric Transformer (VCT) framework\nthat leverages vision-derived queries to iteratively fetch corresponding audio\nand visual information, enabling queries to better distinguish between\ndifferent sounding objects from mixed audio and accurately delineate their\ncontours. Additionally, we also introduce a Prototype Prompted Query Generation\n(PPQG) module within our VCT framework to generate vision-derived queries that\nare both semantically aware and visually rich through audio prototype prompting\nand pixel context grouping, facilitating audio-visual information aggregation.\nExtensive experiments demonstrate that our VCT framework achieves new\nstate-of-the-art performances on three subsets of the AVSBench dataset. The\ncode is available at https://github.com/spyflying/VCT_AVS.", "comment": "Accepted by CVPR 2025; Code: https://github.com/spyflying/VCT_AVS;\n  Models: https://huggingface.co/nowherespyfly/VCT_AVS", "pdf_url": "http://arxiv.org/pdf/2506.23623v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23623v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23023", "title": "Scenario-Based Hierarchical Reinforcement Learning for Automated Driving Decision Making", "authors": ["M. Youssef Abdelhamid", "Lennart Vater", "Zlatan Ajanovic"], "summary": "Developing decision-making algorithms for highly automated driving systems\nremains challenging, since these systems have to operate safely in an open and\ncomplex environments. Reinforcement Learning (RL) approaches can learn\ncomprehensive decision policies directly from experience and already show\npromising results in simple driving tasks. However, current approaches fail to\nachieve generalizability for more complex driving tasks and lack learning\nefficiency. Therefore, we present Scenario-based Automated Driving\nReinforcement Learning (SAD-RL), the first framework that integrates\nReinforcement Learning (RL) of hierarchical policy in a scenario-based\nenvironment. A high-level policy selects maneuver templates that are evaluated\nand executed by a low-level control logic. The scenario-based environment\nallows to control the training experience for the agent and to explicitly\nintroduce challenging, but rate situations into the training process. Our\nexperiments show that an agent trained using the SAD-RL framework can achieve\nsafe behaviour in easy as well as challenging situations efficiently. Our\nablation studies confirmed that both HRL and scenario diversity are essential\nfor achieving these results.", "comment": "6 pages, 10 figures, submitted to a conference", "pdf_url": "http://arxiv.org/pdf/2506.23023v1", "categories": ["cs.RO", "cs.AI", "cs.LG"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.23023v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23286", "title": "Not All Explanations for Deep Learning Phenomena Are Equally Valuable", "authors": ["Alan Jeffares", "Mihaela van der Schaar"], "summary": "Developing a better understanding of surprising or counterintuitive phenomena\nhas constituted a significant portion of deep learning research in recent\nyears. These include double descent, grokking, and the lottery ticket\nhypothesis -- among many others. Works in this area often develop ad hoc\nhypotheses attempting to explain these observed phenomena on an isolated,\ncase-by-case basis. This position paper asserts that, in many prominent cases,\nthere is little evidence to suggest that these phenomena appear in real-world\napplications and these efforts may be inefficient in driving progress in the\nbroader field. Consequently, we argue against viewing them as isolated puzzles\nthat require bespoke resolutions or explanations. However, despite this, we\nsuggest that deep learning phenomena do still offer research value by providing\nunique settings in which we can refine our broad explanatory theories of more\ngeneral deep learning principles. This position is reinforced by analyzing the\nresearch outcomes of several prominent examples of these phenomena from the\nrecent literature. We revisit the current norms in the research community in\napproaching these problems and propose practical recommendations for future\nresearch, aiming to ensure that progress on deep learning phenomena is well\naligned with the ultimate pragmatic goal of progress in the broader field of\ndeep learning.", "comment": "Accepted at ICML 2025 for oral presentation", "pdf_url": "http://arxiv.org/pdf/2506.23286v1", "categories": ["cs.LG", "cs.AI", "stat.ML"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.23286v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23627", "title": "Brain Tumor Detection through Thermal Imaging and MobileNET", "authors": ["Roham Maiti", "Debasmita Bhoumik"], "summary": "Brain plays a crucial role in regulating body functions and cognitive\nprocesses, with brain tumors posing significant risks to human health. Precise\nand prompt detection is a key factor in proper treatment and better patient\noutcomes. Traditional methods for detecting brain tumors, that include\nbiopsies, MRI, and CT scans often face challenges due to their high costs and\nthe need for specialized medical expertise. Recent developments in machine\nlearning (ML) and deep learning (DL) has exhibited strong capabilities in\nautomating the identification and categorization of brain tumors from medical\nimages, especially MRI scans. However, these classical ML models have\nlimitations, such as high computational demands, the need for large datasets,\nand long training times, which hinder their accessibility and efficiency. Our\nresearch uses MobileNET model for efficient detection of these tumors. The\nnovelty of this project lies in building an accurate tumor detection model\nwhich use less computing re-sources and runs in less time followed by efficient\ndecision making through the use of image processing technique for accurate\nresults. The suggested method attained an average accuracy of 98.5%.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23627v1", "categories": ["cs.CV", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23627v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23030", "title": "VisionScores -- A system-segmented image score dataset for deep learning tasks", "authors": ["Alejandro Romero Amezcua", "Mariano Jos√© Juan Rivera Meraz"], "summary": "VisionScores presents a novel proposal being the first system-segmented image\nscore dataset, aiming to offer structure-rich, high information-density images\nfor machine and deep learning tasks. Delimited to two-handed piano pieces, it\nwas built to consider not only certain graphic similarity but also composition\npatterns, as this creative process is highly instrument-dependent. It provides\ntwo scenarios in relation to composer and composition type. The first, formed\nby 14k samples, considers works from different authors but the same composition\ntype, specifically, Sonatinas. The latter, consisting of 10.8K samples,\npresents the opposite case, various composition types from the same author,\nbeing the one selected Franz Liszt. All of the 24.8k samples are formatted as\ngrayscale jpg images of $128 \\times 512$ pixels. VisionScores supplies the\nusers not only the formatted samples but the systems' order and pieces'\nmetadata. Moreover, unsegmented full-page scores and the pre-formatted images\nare included for further analysis.", "comment": "Comments: 5 pages, 3 figures. Accepted for presentation at the 2025\n  IEEE International Conference on Image Processing (ICIP). \\c{opyright} 2025\n  IEEE. Personal use of this material is permitted. Permission from IEEE must\n  be obtained for any other use", "pdf_url": "http://arxiv.org/pdf/2506.23030v1", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23030v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23293", "title": "Objective-Free Local Learning and Emergent Language Structure in Thinking Machines", "authors": ["P. Myles Eugenio"], "summary": "We present a neuro-symbolic framework for generative language modeling based\non local, event-driven emergent learning. At its core is a hierarchical\nHopfield memory chain acting as a compositional short-term memory and dynamic\ntokenizer (retokenizer). Rather than relying on predefined tokens or\nsupervision, the model builds structure from scratch, learning symbol sequences\nas multi-scale representations. It constructs projection tensors that bind\nco-occurring features into hierarchical tokens, introducing redundancy (i.e an\nemergent gauge structure) and enabling compression of local activations into\nlong-range dependencies. Curiously, we find that the retokenizer can filter\nnatural language patterns from noise, generating synthetic languages with\ncoherent internal morphology -- quantifiably the same as human language.\nLanguage is learned in a local (Hebbian) fashion, where model constraints\ndictate allowed emergent structure, and new information is retained in\nalignment with this structure. The absence of a global objective enables a form\nof plasticity not found in conventional language models, allowing the system to\ngeneralize beyond its initial inference class -- even without explicit data. We\ndemonstrate that briefly activating a new neuron during inference binds\ndistributed multi-scale token features into a symbolic embedding. These\nemergent embedding neurons act as long-term memory and support a key-value\nmechanism for compositional inference and generalization. This architecture\nprovides a methodological foundation for studying how symbolic structure can\nemerge from local neural learning. It offers a new pathway for building\nscalable, interpretable neuro-symbolic systems -- where tokens, grammar, and\nreasoning arise as compressed memory traces within a Hopfield hierarchy. This\napproach advances the development of neuromorphic architectures for generative\nlanguage models.", "comment": "22 pages, 7 figures", "pdf_url": "http://arxiv.org/pdf/2506.23293v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "q-bio.NC"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.23293v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23630", "title": "Blending Concepts with Text-to-Image Diffusion Models", "authors": ["Lorenzo Olearo", "Giorgio Longari", "Alessandro Raganato", "Rafael Pe√±aloza", "Simone Melzi"], "summary": "Diffusion models have dramatically advanced text-to-image generation in\nrecent years, translating abstract concepts into high-fidelity images with\nremarkable ease. In this work, we examine whether they can also blend distinct\nconcepts, ranging from concrete objects to intangible ideas, into coherent new\nvisual entities under a zero-shot framework. Specifically, concept blending\nmerges the key attributes of multiple concepts (expressed as textual prompts)\ninto a single, novel image that captures the essence of each concept. We\ninvestigate four blending methods, each exploiting different aspects of the\ndiffusion pipeline (e.g., prompt scheduling, embedding interpolation, or\nlayer-wise conditioning). Through systematic experimentation across diverse\nconcept categories, such as merging concrete concepts, synthesizing compound\nwords, transferring artistic styles, and blending architectural landmarks, we\nshow that modern diffusion models indeed exhibit creative blending capabilities\nwithout further training or fine-tuning. Our extensive user study, involving\n100 participants, reveals that no single approach dominates in all scenarios:\neach blending technique excels under certain conditions, with factors like\nprompt ordering, conceptual distance, and random seed affecting the outcome.\nThese findings highlight the remarkable compositional potential of diffusion\nmodels while exposing their sensitivity to seemingly minor input variations.", "comment": "Currently under review", "pdf_url": "http://arxiv.org/pdf/2506.23630v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23630v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23074", "title": "Learning Counterfactually Decoupled Attention for Open-World Model Attribution", "authors": ["Yu Zheng", "Boyang Gong", "Fanye Kong", "Yueqi Duan", "Bingyao Yu", "Wenzhao Zheng", "Lei Chen", "Jiwen Lu", "Jie Zhou"], "summary": "In this paper, we propose a Counterfactually Decoupled Attention Learning\n(CDAL) method for open-world model attribution. Existing methods rely on\nhandcrafted design of region partitioning or feature space, which could be\nconfounded by the spurious statistical correlations and struggle with novel\nattacks in open-world scenarios. To address this, CDAL explicitly models the\ncausal relationships between the attentional visual traces and source model\nattribution, and counterfactually decouples the discriminative model-specific\nartifacts from confounding source biases for comparison. In this way, the\nresulting causal effect provides a quantification on the quality of learned\nattention maps, thus encouraging the network to capture essential generation\npatterns that generalize to unseen source models by maximizing the effect.\nExtensive experiments on existing open-world model attribution benchmarks show\nthat with minimal computational overhead, our method consistently improves\nstate-of-the-art models by large margins, particularly for unseen novel\nattacks. Source code: https://github.com/yzheng97/CDAL.", "comment": "Accepted by ICCV 2025. Code: \\url{https://github.com/yzheng97/CDAL}", "pdf_url": "http://arxiv.org/pdf/2506.23074v1", "categories": ["cs.CV", "cs.CR", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23074v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23296", "title": "Securing AI Systems: A Guide to Known Attacks and Impacts", "authors": ["Naoto Kiribuchi", "Kengo Zenitani", "Takayuki Semitsu"], "summary": "Embedded into information systems, artificial intelligence (AI) faces\nsecurity threats that exploit AI-specific vulnerabilities. This paper provides\nan accessible overview of adversarial attacks unique to predictive and\ngenerative AI systems. We identify eleven major attack types and explicitly\nlink attack techniques to their impacts -- including information leakage,\nsystem compromise, and resource exhaustion -- mapped to the confidentiality,\nintegrity, and availability (CIA) security triad. We aim to equip researchers,\ndevelopers, security practitioners, and policymakers, even those without\nspecialized AI security expertise, with foundational knowledge to recognize\nAI-specific risks and implement effective defenses, thereby enhancing the\noverall security posture of AI systems.", "comment": "34 pages, 16 figures", "pdf_url": "http://arxiv.org/pdf/2506.23296v1", "categories": ["cs.CR", "cs.AI"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.23296v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23639", "title": "Unified Multimodal Understanding via Byte-Pair Visual Encoding", "authors": ["Wanpeng Zhang", "Yicheng Feng", "Hao Luo", "Yijiang Li", "Zihao Yue", "Sipeng Zheng", "Zongqing Lu"], "summary": "Multimodal large language models (MLLMs) have made significant progress in\nvision-language understanding, yet effectively aligning different modalities\nremains a fundamental challenge. We present a framework that unifies multimodal\nunderstanding by applying byte-pair encoding to visual tokens. Unlike\nconventional approaches that rely on modality-specific encoders, our method\ndirectly incorporates structural information into visual tokens, mirroring\nsuccessful tokenization strategies in text-only language models. We introduce a\npriority-guided encoding scheme that considers both frequency and spatial\nconsistency, coupled with a multi-stage training procedure based on\ncurriculum-driven data composition. These enhancements enable the transformer\nmodel to better capture cross-modal relationships and reason with visual\ninformation. Comprehensive experiments demonstrate improved performance across\ndiverse vision-language tasks. By bridging the gap between visual and textual\nrepresentations, our approach contributes to the advancement of more capable\nand efficient multimodal foundation models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23639v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23639v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23075", "title": "CSBrain: A Cross-scale Spatiotemporal Brain Foundation Model for EEG Decoding", "authors": ["Yuchen Zhou", "Jiamin Wu", "Zichen Ren", "Zhouheng Yao", "Weiheng Lu", "Kunyu Peng", "Qihao Zheng", "Chunfeng Song", "Wanli Ouyang", "Chao Gou"], "summary": "Understanding and decoding brain activity from electroencephalography (EEG)\nsignals is a fundamental challenge in neuroscience and AI, with applications in\ncognition, emotion recognition, diagnosis, and brain-computer interfaces. While\nrecent EEG foundation models advance generalized decoding via unified\narchitectures and large-scale pretraining, they adopt a scale-agnostic dense\nmodeling paradigm inherited from NLP and vision. This design neglects a core\nproperty of neural activity: cross-scale spatiotemporal structure. EEG task\npatterns span a wide range of temporal and spatial scales, from short bursts to\nslow rhythms, and from localized cortical responses to distributed\ninteractions. Ignoring this diversity leads to suboptimal representations and\nweak generalization. We propose CSBrain, a Cross-scale Spatiotemporal Brain\nfoundation model for generalized EEG decoding. CSBrain introduces: (i)\nCross-scale Spatiotemporal Tokenization (CST), which aggregates multi-scale\nfeatures from localized temporal windows and anatomical brain regions into\ncompact scale-aware tokens; and (ii) Structured Sparse Attention (SSA), which\ncaptures cross-window and cross-region dependencies, enhancing scale diversity\nwhile removing spurious correlations. CST and SSA are alternately stacked to\nprogressively integrate multi-scale dependencies. Experiments on 11 EEG tasks\nacross 16 datasets show that CSBrain consistently outperforms task-specific and\nfoundation model baselines. These results establish cross-scale modeling as a\nkey inductive bias and position CSBrain as a robust backbone for future\nbrain-AI research.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23075v1", "categories": ["cs.HC", "cs.LG", "eess.SP", "q-bio.NC"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.23075v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23314", "title": "Interpretable by Design: MH-AutoML for Transparent and Efficient Android Malware Detection without Compromising Performance", "authors": ["Joner Assolin", "Gabriel Canto", "Diego Kreutz", "Eduardo Feitosa", "Hendrio Bragan√ßa", "Angelo Nogueira", "Vanderson Rocha"], "summary": "Malware detection in Android systems requires both cybersecurity expertise\nand machine learning (ML) techniques. Automated Machine Learning (AutoML) has\nemerged as an approach to simplify ML development by reducing the need for\nspecialized knowledge. However, current AutoML solutions typically operate as\nblack-box systems with limited transparency, interpretability, and experiment\ntraceability. To address these limitations, we present MH-AutoML, a\ndomain-specific framework for Android malware detection. MH-AutoML automates\nthe entire ML pipeline, including data preprocessing, feature engineering,\nalgorithm selection, and hyperparameter tuning. The framework incorporates\ncapabilities for interpretability, debugging, and experiment tracking that are\noften missing in general-purpose solutions. In this study, we compare MH-AutoML\nagainst seven established AutoML frameworks: Auto-Sklearn, AutoGluon, TPOT,\nHyperGBM, Auto-PyTorch, LightAutoML, and MLJAR. Results show that MH-AutoML\nachieves better recall rates while providing more transparency and control. The\nframework maintains computational efficiency comparable to other solutions,\nmaking it suitable for cybersecurity applications where both performance and\nexplainability matter.", "comment": "18 pages, 10 figures, 7 tabelas, paper submitted to JBCS", "pdf_url": "http://arxiv.org/pdf/2506.23314v1", "categories": ["cs.CR", "cs.AI", "68T99", "I.2"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.23314v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23641", "title": "VAP-Diffusion: Enriching Descriptions with MLLMs for Enhanced Medical Image Generation", "authors": ["Peng Huang", "Junhu Fu", "Bowen Guo", "Zeju Li", "Yuanyuan Wang", "Yi Guo"], "summary": "As the appearance of medical images is influenced by multiple underlying\nfactors, generative models require rich attribute information beyond labels to\nproduce realistic and diverse images. For instance, generating an image of skin\nlesion with specific patterns demands descriptions that go beyond diagnosis,\nsuch as shape, size, texture, and color. However, such detailed descriptions\nare not always accessible. To address this, we explore a framework, termed\nVisual Attribute Prompts (VAP)-Diffusion, to leverage external knowledge from\npre-trained Multi-modal Large Language Models (MLLMs) to improve the quality\nand diversity of medical image generation. First, to derive descriptions from\nMLLMs without hallucination, we design a series of prompts following\nChain-of-Thoughts for common medical imaging tasks, including dermatologic,\ncolorectal, and chest X-ray images. Generated descriptions are utilized during\ntraining and stored across different categories. During testing, descriptions\nare randomly retrieved from the corresponding category for inference. Moreover,\nto make the generator robust to unseen combination of descriptions at the test\ntime, we propose a Prototype Condition Mechanism that restricts test embeddings\nto be similar to those from training. Experiments on three common types of\nmedical imaging across four datasets verify the effectiveness of VAP-Diffusion.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23641v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23641v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23090", "title": "Multi-task Offline Reinforcement Learning for Online Advertising in Recommender Systems", "authors": ["Langming Liu", "Wanyu Wang", "Chi Zhang", "Bo Li", "Hongzhi Yin", "Xuetao Wei", "Wenbo Su", "Bo Zheng", "Xiangyu Zhao"], "summary": "Online advertising in recommendation platforms has gained significant\nattention, with a predominant focus on channel recommendation and budget\nallocation strategies. However, current offline reinforcement learning (RL)\nmethods face substantial challenges when applied to sparse advertising\nscenarios, primarily due to severe overestimation, distributional shifts, and\noverlooking budget constraints. To address these issues, we propose MTORL, a\nnovel multi-task offline RL model that targets two key objectives. First, we\nestablish a Markov Decision Process (MDP) framework specific to the nuances of\nadvertising. Then, we develop a causal state encoder to capture dynamic user\ninterests and temporal dependencies, facilitating offline RL through\nconditional sequence modeling. Causal attention mechanisms are introduced to\nenhance user sequence representations by identifying correlations among causal\nstates. We employ multi-task learning to decode actions and rewards,\nsimultaneously addressing channel recommendation and budget allocation.\nNotably, our framework includes an automated system for integrating these tasks\ninto online advertising. Extensive experiments on offline and online\nenvironments demonstrate MTORL's superiority over state-of-the-art methods.", "comment": "KDD 2025", "pdf_url": "http://arxiv.org/pdf/2506.23090v1", "categories": ["cs.IR", "cs.LG"], "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.23090v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23322", "title": "GaussMaster: An LLM-based Database Copilot System", "authors": ["Wei Zhou", "Ji Sun", "Xuanhe Zhou", "Guoliang Li", "Luyang Liu", "Hao Wu", "Tianyuan Wang"], "summary": "In the financial industry, data is the lifeblood of operations, and DBAs\nshoulder significant responsibilities for SQL tuning, database deployment,\ndiagnosis, and service repair. In recent years, both database vendors and\ncustomers have increasingly turned to autonomous database platforms in an\neffort to alleviate the heavy workload of DBAs. However, existing autonomous\ndatabase platforms are limited in their capabilities, primarily addressing\nsingle-point issues such as NL2SQL, anomaly detection, and SQL tuning. Manual\nintervention remains a necessity for comprehensive database maintenance.\nGaussMaster aims to revolutionize this landscape by introducing an LLM-based\ndatabase copilot system. This innovative solution is designed not only to\nassist developers in writing efficient SQL queries but also to provide\ncomprehensive care for database services. When database instances exhibit\nabnormal behavior, GaussMaster is capable of orchestrating the entire\nmaintenance process automatically. It achieves this by analyzing hundreds of\nmetrics and logs, employing a Tree-of-thought approach to identify root causes,\nand invoking appropriate tools to resolve issues. We have successfully\nimplemented GaussMaster in real-world scenarios, such as the banking industry,\nwhere it has achieved zero human intervention for over 34 database maintenance\nscenarios. In this paper, we present significant improvements in these tasks\nwith code at https://gitcode.com/opengauss/openGauss-GaussMaster.", "comment": "We welcome contributions from the community. For reference, please\n  see the code at: https://gitcode.com/opengauss/openGauss-GaussMaster", "pdf_url": "http://arxiv.org/pdf/2506.23322v1", "categories": ["cs.DB", "cs.AI", "cs.CL", "cs.IR"], "cate": "cs.DB", "url": "http://arxiv.org/abs/2506.23322v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23648", "title": "MReg: A Novel Regression Model with MoE-based Video Feature Mining for Mitral Regurgitation Diagnosis", "authors": ["Zhe Liu", "Yuhao Huang", "Lian Liu", "Chengrui Zhang", "Haotian Lin", "Tong Han", "Zhiyuan Zhu", "Yanlin Chen", "Yuerui Chen", "Dong Ni", "Zhongshan Gou", "Xin Yang"], "summary": "Color Doppler echocardiography is a crucial tool for diagnosing mitral\nregurgitation (MR). Recent studies have explored intelligent methods for MR\ndiagnosis to minimize user dependence and improve accuracy. However, these\napproaches often fail to align with clinical workflow and may lead to\nsuboptimal accuracy and interpretability. In this study, we introduce an\nautomated MR diagnosis model (MReg) developed on the 4-chamber cardiac color\nDoppler echocardiography video (A4C-CDV). It follows comprehensive feature\nmining strategies to detect MR and assess its severity, considering clinical\nrealities. Our contribution is threefold. First, we formulate the MR diagnosis\nas a regression task to capture the continuity and ordinal relationships\nbetween categories. Second, we design a feature selection and amplification\nmechanism to imitate the sonographer's diagnostic logic for accurate MR\ngrading. Third, inspired by the Mixture-of-Experts concept, we introduce a\nfeature summary module to extract the category-level features, enhancing the\nrepresentational capacity for more accurate grading. We trained and evaluated\nour proposed MReg on a large in-house A4C-CDV dataset comprising 1868 cases\nwith three graded regurgitation labels. Compared to other weakly supervised\nvideo anomaly detection and supervised classification methods, MReg\ndemonstrated superior performance in MR diagnosis. Our code is available at:\nhttps://github.com/cskdstz/MReg.", "comment": "10 pages, 5 figures, accepted by MICCAI 2025", "pdf_url": "http://arxiv.org/pdf/2506.23648v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23648v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23170", "title": "Compositions of Variant Experts for Integrating Short-Term and Long-Term Preferences", "authors": ["Jaime Hieu Do", "Trung-Hoang Le", "Hady W. Lauw"], "summary": "In the online digital realm, recommendation systems are ubiquitous and play a\ncrucial role in enhancing user experience. These systems leverage user\npreferences to provide personalized recommendations, thereby helping users\nnavigate through the paradox of choice. This work focuses on personalized\nsequential recommendation, where the system considers not only a user's\nimmediate, evolving session context, but also their cumulative historical\nbehavior to provide highly relevant and timely recommendations. Through an\nempirical study conducted on diverse real-world datasets, we have observed and\nquantified the existence and impact of both short-term (immediate and\ntransient) and long-term (enduring and stable) preferences on users' historical\ninteractions. Building on these insights, we propose a framework that combines\nshort- and long-term preferences to enhance recommendation performance, namely\nCompositions of Variant Experts (CoVE). This novel framework dynamically\nintegrates short- and long-term preferences through the use of different\nspecialized recommendation models (i.e., experts). Extensive experiments\nshowcase the effectiveness of the proposed methods and ablation studies further\ninvestigate the impact of variant expert types.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23170v1", "categories": ["cs.IR", "cs.LG"], "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.23170v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23325", "title": "XY-Tokenizer: Mitigating the Semantic-Acoustic Conflict in Low-Bitrate Speech Codecs", "authors": ["Yitian Gong", "Luozhijie Jin", "Ruifan Deng", "Dong Zhang", "Xin Zhang", "Qinyuan Cheng", "Zhaoye Fei", "Shimin Li", "Xipeng Qiu"], "summary": "Speech codecs serve as bridges between speech signals and large language\nmodels. An ideal codec for speech language models should not only preserve\nacoustic information but also capture rich semantic information. However,\nexisting speech codecs struggle to balance high-quality audio reconstruction\nwith ease of modeling by language models. In this study, we analyze the\nlimitations of previous codecs in balancing semantic richness and acoustic\nfidelity. We propose XY-Tokenizer, a novel codec that mitigates the conflict\nbetween semantic and acoustic capabilities through multi-stage, multi-task\nlearning. Experimental results demonstrate that XY-Tokenizer achieves\nperformance in both semantic and acoustic tasks comparable to that of\nstate-of-the-art codecs operating at similar bitrates, even though those\nexisting codecs typically excel in only one aspect. Specifically, XY-Tokenizer\nachieves strong text alignment, surpassing distillation-based semantic modeling\nmethods such as SpeechTokenizer and Mimi, while maintaining a speaker\nsimilarity score of 0.83 between reconstructed and original audio. The\nreconstruction performance of XY-Tokenizer is comparable to that of BigCodec,\nthe current state-of-the-art among acoustic-only codecs, which achieves a\nspeaker similarity score of 0.84 at a similar bitrate. Code and models are\navailable at https://github.com/gyt1145028706/XY-Tokenizer.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23325v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.23325v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23657", "title": "Towards Markerless Intraoperative Tracking of Deformable Spine Tissue", "authors": ["Connor Daly", "Elettra Marconi", "Marco Riva", "Jinendra Ekanayake", "Daniel S. Elson", "Ferdinando Rodriguez y Baena"], "summary": "Consumer-grade RGB-D imaging for intraoperative orthopedic tissue tracking is\na promising method with high translational potential. Unlike bone-mounted\ntracking devices, markerless tracking can reduce operating time and complexity.\nHowever, its use has been limited to cadaveric studies. This paper introduces\nthe first real-world clinical RGB-D dataset for spine surgery and develops\nSpineAlign, a system for capturing deformation between preoperative and\nintraoperative spine states. We also present an intraoperative segmentation\nnetwork trained on this data and introduce CorrespondNet, a multi-task\nframework for predicting key regions for registration in both intraoperative\nand preoperative scenes.", "comment": "Preprint of paper, submitted", "pdf_url": "http://arxiv.org/pdf/2506.23657v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23657v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23173", "title": "Deep Learning for Optical Misalignment Diagnostics in Multi-Lens Imaging Systems", "authors": ["Tomer Slor", "Dean Oren", "Shira Baneth", "Tom Coen", "Haim Suchowski"], "summary": "In the rapidly evolving field of optical engineering, precise alignment of\nmulti-lens imaging systems is critical yet challenging, as even minor\nmisalignments can significantly degrade performance. Traditional alignment\nmethods rely on specialized equipment and are time-consuming processes,\nhighlighting the need for automated and scalable solutions. We present two\ncomplementary deep learning-based inverse-design methods for diagnosing\nmisalignments in multi-element lens systems using only optical measurements.\nFirst, we use ray-traced spot diagrams to predict five-degree-of-freedom\n(5-DOF) errors in a 6-lens photographic prime, achieving a mean absolute error\nof 0.031mm in lateral translation and 0.011$^\\circ$ in tilt. We also introduce\na physics-based simulation pipeline that utilizes grayscale synthetic camera\nimages, enabling a deep learning model to estimate 4-DOF, decenter and tilt\nerrors in both two- and six-lens multi-lens systems. These results show the\npotential to reshape manufacturing and quality control in precision imaging.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23173v1", "categories": ["physics.optics", "cs.AI", "cs.LG"], "cate": "physics.optics", "url": "http://arxiv.org/abs/2506.23173v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23334", "title": "Federated Breast Cancer Detection Enhanced by Synthetic Ultrasound Image Augmentation", "authors": ["Hongyi Pan", "Ziliang Hong", "Gorkem Durak", "Ziyue Xu", "Ulas Bagci"], "summary": "Federated learning (FL) has emerged as a promising paradigm for\ncollaboratively training deep learning models across institutions without\nexchanging sensitive medical data. However, its effectiveness is often hindered\nby limited data availability and non-independent, identically distributed data\nacross participating clients, which can degrade model performance and\ngeneralization. To address these challenges, we propose a generative AI based\ndata augmentation framework that integrates synthetic image sharing into the\nfederated training process for breast cancer diagnosis via ultrasound images.\nSpecifically, we train two simple class-specific Deep Convolutional Generative\nAdversarial Networks: one for benign and one for malignant lesions. We then\nsimulate a realistic FL setting using three publicly available breast\nultrasound image datasets: BUSI, BUS-BRA, and UDIAT. FedAvg and FedProx are\nadopted as baseline FL algorithms. Experimental results show that incorporating\na suitable number of synthetic images improved the average AUC from 0.9206 to\n0.9237 for FedAvg and from 0.9429 to 0.9538 for FedProx. We also note that\nexcessive use of synthetic data reduced performance, underscoring the\nimportance of maintaining a balanced ratio of real and synthetic samples. Our\nfindings highlight the potential of generative AI based data augmentation to\nenhance FL results in the breast ultrasound image classification task.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23334v1", "categories": ["eess.IV", "cs.AI", "cs.CV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.23334v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23663", "title": "On the Domain Robustness of Contrastive Vision-Language Models", "authors": ["Mario Koddenbrock", "Rudolf Hoffmann", "David Brodmann", "Erik Rodner"], "summary": "In real-world vision-language applications, practitioners increasingly rely\non large, pretrained foundation models rather than custom-built solutions,\ndespite limited transparency regarding their training data and processes. While\nthese models achieve impressive performance on general benchmarks, their\neffectiveness can decline notably under specialized domain shifts, such as\nunique imaging conditions or environmental variations. In this work, we\nintroduce Deepbench, a framework designed to assess domain-specific robustness\nof vision-language models (VLMs). Deepbench leverages a large language model\n(LLM) to generate realistic, context-aware image corruptions tailored to\nspecific deployment domains without requiring labeled data. We evaluate a range\nof contrastive vision-language architectures and architectural variants across\nsix real-world domains and observe substantial variability in robustness,\nhighlighting the need for targeted, domain-aware evaluation. Deepbench is\nreleased as open-source software to support further research into domain-aware\nrobustness assessment.", "comment": "Deepbench is available at https://github.com/ml-lab-htw/deepbench", "pdf_url": "http://arxiv.org/pdf/2506.23663v1", "categories": ["cs.CV", "cs.LG", "I.4"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23663v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23192", "title": "RiverText: A Python Library for Training and Evaluating Incremental Word Embeddings from Text Data Streams", "authors": ["Gabriel Iturra-Bocaz", "Felipe Bravo-Marquez"], "summary": "Word embeddings have become essential components in various information\nretrieval and natural language processing tasks, such as ranking, document\nclassification, and question answering. However, despite their widespread use,\ntraditional word embedding models present a limitation in their static nature,\nwhich hampers their ability to adapt to the constantly evolving language\npatterns that emerge in sources such as social media and the web (e.g., new\nhashtags or brand names). To overcome this problem, incremental word embedding\nalgorithms are introduced, capable of dynamically updating word representations\nin response to new language patterns and processing continuous data streams.\n  This paper presents RiverText, a Python library for training and evaluating\nincremental word embeddings from text data streams. Our tool is a resource for\nthe information retrieval and natural language processing communities that work\nwith word embeddings in streaming scenarios, such as analyzing social media.\nThe library implements different incremental word embedding techniques, such as\nSkip-gram, Continuous Bag of Words, and Word Context Matrix, in a standardized\nframework. In addition, it uses PyTorch as its backend for neural network\ntraining. We have implemented a module that adapts existing intrinsic static\nword embedding evaluation tasks for word similarity and word categorization to\na streaming setting. Finally, we compare the implemented methods with different\nhyperparameter settings and discuss the results. Our open-source library is\navailable at https://github.com/dccuchile/rivertext.", "comment": "Accepted at SIGIR'23", "pdf_url": "http://arxiv.org/pdf/2506.23192v1", "categories": ["cs.CL", "cs.LG"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.23192v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23339", "title": "VALID-Mol: a Systematic Framework for Validated LLM-Assisted Molecular Design", "authors": ["Malikussaid", "Hilal Hudan Nuha"], "summary": "Large Language Models (LLMs) demonstrate remarkable potential for scientific\ndiscovery, but their application in domains requiring factual accuracy and\ndomain-specific constraints remains challenging. In molecular design for drug\ndiscovery, LLMs can suggest creative molecular modifications but often produce\nchemically invalid or impractical structures. We present VALID-Mol, a\nsystematic framework for integrating chemical validation with LLM-driven\nmolecular design that increases the rate of generating valid chemical\nstructures from 3% to 83%. Our approach combines methodical prompt engineering,\nautomated chemical validation, and a fine-tuned domain-adapted LLM to ensure\nreliable generation of synthesizable molecules with improved properties. Beyond\nthe specific implementation, we contribute a generalizable methodology for\nscientifically-constrained LLM applications, with quantifiable reliability\nimprovements. Computational predictions suggest our framework can generate\npromising candidates for synthesis with up to 17-fold computationally predicted\nimprovements in target affinity while maintaining synthetic accessibility. We\nprovide a detailed analysis of our prompt engineering process, validation\narchitecture, and fine-tuning approach, offering a reproducible blueprint for\napplying LLMs to other scientific domains where domain-specific validation is\nessential.", "comment": "16 pages, 1 figure, 5 algorithms, 7 tables, to be published in ICSECS\n  Conference 2025, unabridged version", "pdf_url": "http://arxiv.org/pdf/2506.23339v1", "categories": ["cs.LG", "cs.AI", "physics.chem-ph", "q-bio.QM"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.23339v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23674", "title": "Partial Forward Blocking: A Novel Data Pruning Paradigm for Lossless Training Acceleration", "authors": ["Dongyue Wu", "Zilin Guo", "Jialong Zuo", "Nong Sang", "Changxin Gao"], "summary": "The ever-growing size of training datasets enhances the generalization\ncapability of modern machine learning models but also incurs exorbitant\ncomputational costs. Existing data pruning approaches aim to accelerate\ntraining by removing those less important samples. However, they often rely on\ngradients or proxy models, leading to prohibitive additional costs of gradient\nback-propagation and proxy model training. In this paper, we propose Partial\nForward Blocking (PFB), a novel framework for lossless training acceleration.\nThe efficiency of PFB stems from its unique adaptive pruning pipeline: sample\nimportance is assessed based on features extracted from the shallow layers of\nthe target model. Less important samples are then pruned, allowing only the\nretained ones to proceed with the subsequent forward pass and loss\nback-propagation. This mechanism significantly reduces the computational\noverhead of deep-layer forward passes and back-propagation for pruned samples,\nwhile also eliminating the need for auxiliary backward computations and proxy\nmodel training. Moreover, PFB introduces probability density as an indicator of\nsample importance. Combined with an adaptive distribution estimation module,\nour method dynamically prioritizes relatively rare samples, aligning with the\nconstantly evolving training state. Extensive experiments demonstrate the\nsignificant superiority of PFB in performance and speed. On ImageNet, PFB\nachieves a 0.5% accuracy improvement and 33% training time reduction with 40%\ndata pruned.", "comment": "Accepted by ICCV2025", "pdf_url": "http://arxiv.org/pdf/2506.23674v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23674v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23247", "title": "Aggregating Local Saliency Maps for Semi-Global Explainable Image Classification", "authors": ["James Hinns", "David Martens"], "summary": "Deep learning dominates image classification tasks, yet understanding how\nmodels arrive at predictions remains a challenge. Much research focuses on\nlocal explanations of individual predictions, such as saliency maps, which\nvisualise the influence of specific pixels on a model's prediction. However,\nreviewing many of these explanations to identify recurring patterns is\ninfeasible, while global methods often oversimplify and miss important local\nbehaviours. To address this, we propose Segment Attribution Tables (SATs), a\nmethod for summarising local saliency explanations into (semi-)global insights.\nSATs take image segments (such as \"eyes\" in Chihuahuas) and leverage saliency\nmaps to quantify their influence. These segments highlight concepts the model\nrelies on across instances and reveal spurious correlations, such as reliance\non backgrounds or watermarks, even when out-of-distribution test performance\nsees little change. SATs can explain any classifier for which a form of\nsaliency map can be produced, using segmentation maps that provide named\nsegments. SATs bridge the gap between oversimplified global summaries and\noverly detailed local explanations, offering a practical tool for analysing and\ndebugging image classifiers.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23247v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23247v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23342", "title": "ATGen: A Framework for Active Text Generation", "authors": ["Akim Tsvigun", "Daniil Vasilev", "Ivan Tsvigun", "Ivan Lysenko", "Talgat Bektleuov", "Aleksandr Medvedev", "Uliana Vinogradova", "Nikita Severin", "Mikhail Mozikov", "Andrey Savchenko", "Rostislav Grigorev", "Ramil Kuleev", "Fedor Zhdanov", "Artem Shelmanov", "Ilya Makarov"], "summary": "Active learning (AL) has demonstrated remarkable potential in reducing the\nannotation effort required for training machine learning models. However,\ndespite the surging popularity of natural language generation (NLG) tasks in\nrecent years, the application of AL to NLG has been limited. In this paper, we\nintroduce Active Text Generation (ATGen) - a comprehensive framework that\nbridges AL with text generation tasks, enabling the application of\nstate-of-the-art AL strategies to NLG. Our framework simplifies AL-empowered\nannotation in NLG tasks using both human annotators and automatic annotation\nagents based on large language models (LLMs). The framework supports LLMs\ndeployed as services, such as ChatGPT and Claude, or operated on-premises.\nFurthermore, ATGen provides a unified platform for smooth implementation and\nbenchmarking of novel AL strategies tailored to NLG tasks. Finally, we present\nevaluation results for state-of-the-art AL strategies across diverse settings\nand multiple text generation tasks. We show that ATGen reduces both the effort\nof human annotators and costs associated with API calls to LLM-based annotation\nagents. The code of the framework is available on GitHub under the MIT license.\nThe video presentation is available at http://atgen-video.nlpresearch.group", "comment": "Accepted at ACL 2025 System Demonstrations", "pdf_url": "http://arxiv.org/pdf/2506.23342v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.23342v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23675", "title": "Pruning by Block Benefit: Exploring the Properties of Vision Transformer Blocks during Domain Adaptation", "authors": ["Patrick Glandorf", "Bodo Rosenhahn"], "summary": "Vision Transformer have set new benchmarks in several tasks, but these models\ncome with the lack of high computational costs which makes them impractical for\nresource limited hardware. Network pruning reduces the computational complexity\nby removing less important operations while maintaining performance. However,\npruning a model on an unseen data domain, leads to a misevaluation of weight\nsignificance, resulting in suboptimal resource assignment. In this work, we\nfind that task-sensitive layers initially fail to improve the feature\nrepresentation on downstream tasks, leading to performance loss for early\npruning decisions. To address this problem, we introduce Pruning by Block\nBenefit (P3B), a pruning method that utilizes the relative contribution on\nblock level to globally assign parameter resources. P3B identifies low-impact\ncomponents to reduce parameter allocation while preserving critical ones.\nClassical pruning mask optimization struggles to reactivate zero-mask-elements.\nIn contrast, P3B sets a layerwise keep ratio based on global performance\nmetrics, ensuring the reactivation of late-converging blocks. We show in\nextensive experiments that P3B is a state of the art pruning method with most\nnoticeable gains in transfer learning tasks. Notably, P3B is able to conserve\nhigh performance, even in high sparsity regimes of 70% parameter reduction\nwhile only losing 0.64% in accuracy.", "comment": "ICCV'25 Workshops", "pdf_url": "http://arxiv.org/pdf/2506.23675v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23675v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23270", "title": "Token Activation Map to Visually Explain Multimodal LLMs", "authors": ["Yi Li", "Hualiang Wang", "Xinpeng Ding", "Haonan Wang", "Xiaomeng Li"], "summary": "Multimodal large language models (MLLMs) are broadly empowering various\nfields. Despite their advancements, the explainability of MLLMs remains less\nexplored, hindering deeper understanding, model credibility, and effective\nvisualization. Unlike conventional vision models (e.g., CNNs, ViTs, CLIP) that\nproduce a single output, MLLMs generate sequences of tokens progressively,\nwhere each generated token depends on the previous context. Therefore, earlier\ncontext tokens can introduce redundant activations that interfere with the\nexplanation of later tokens beyond their original information. Existing studies\noften overlook this issue, but our observations reveal that these redundant\ncorrelations can significantly hurt the reliability of explanations. To address\nthis, we propose an estimated causal inference method to mitigate the\ninterference of context to achieve high-quality MLLM explanation, with a novel\nrank Gaussian filter to further reduce activation noises. We term this method\nToken Activation Map (TAM) to highlight the consideration of interactions\nbetween tokens. TAM also indicates that it excels at explaining multiple tokens\nof MLLM, which is different from the Class Activation Map (CAM) for a single\nprediction. Our TAM method significantly outperforms existing SoTA methods,\nshowcasing high-quality visualization results that can be utilized for various\nscenarios, such as object localization, failure case analysis, video\nvisualization, MLLMs visual comparison, and model understanding (e.g., color,\nshape, action, location, visual reasoning, multi-turn conversation, etc). The\ncode is available atgithub.com/xmed-lab/TAM.", "comment": "ICCV2025 Accepted", "pdf_url": "http://arxiv.org/pdf/2506.23270v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23270v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23351", "title": "Benchmarking Generalizable Bimanual Manipulation: RoboTwin Dual-Arm Collaboration Challenge at CVPR 2025 MEIS Workshop", "authors": ["Tianxing Chen", "Kaixuan Wang", "Zhaohui Yang", "Yuhao Zhang", "Zanxin Chen", "Baijun Chen", "Wanxi Dong", "Ziyuan Liu", "Dong Chen", "Tianshuo Yang", "Haibao Yu", "Xiaokang Yang", "Yusen Qin", "Zhiqiang Xie", "Yao Mu", "Ping Luo", "Tian Nian", "Weiliang Deng", "Yiheng Ge", "Yibin Liu", "Zixuan Li", "Dehui Wang", "Zhixuan Liang", "Haohui Xie", "Rijie Zeng", "Yunfei Ge", "Peiqing Cong", "Guannan He", "Zhaoming Han", "Ruocheng Yin", "Jingxiang Guo", "Lunkai Lin", "Tianling Xu", "Hongzhe Bi", "Xuewu Lin", "Tianwei Lin", "Shujie Luo", "Keyu Li", "Ziyan Zhao", "Ke Fan", "Heyang Xu", "Bo Peng", "Wenlong Gao", "Dongjiang Li", "Feng Jin", "Hui Shen", "Jinming Li", "Chaowei Cui", "Yuchen", "Yaxin Peng", "Lingdong Zeng", "Wenlong Dong", "Tengfei Li", "Weijie Ke", "Jun Chen", "Erdemt Bao", "Tian Lan", "Tenglong Liu", "Jin Yang", "Huiping Zhuang", "Baozhi Jia", "Shuai Zhang", "Zhengfeng Zou", "Fangheng Guan", "Tianyi Jia", "Ke Zhou", "Hongjiu Zhang", "Yating Han", "Cheng Fang", "Yixian Zou", "Chongyang Xu", "Qinglun Zhang", "Shen Cheng", "Xiaohe Wang", "Ping Tan", "Haoqiang Fan", "Shuaicheng Liu", "Jiaheng Chen", "Chuxuan Huang", "Chengliang Lin", "Kaijun Luo", "Boyu Yue", "Yi Liu", "Jinyu Chen", "Zichang Tan", "Liming Deng", "Shuo Xu", "Zijian Cai", "Shilong Yin", "Hao Wang", "Hongshan Liu", "Tianyang Li", "Long Shi", "Ran Xu", "Huilin Xu", "Zhengquan Zhang", "Congsheng Xu", "Jinchang Yang", "Feng Xu"], "summary": "Embodied Artificial Intelligence (Embodied AI) is an emerging frontier in\nrobotics, driven by the need for autonomous systems that can perceive, reason,\nand act in complex physical environments. While single-arm systems have shown\nstrong task performance, collaborative dual-arm systems are essential for\nhandling more intricate tasks involving rigid, deformable, and\ntactile-sensitive objects. To advance this goal, we launched the RoboTwin\nDual-Arm Collaboration Challenge at the 2nd MEIS Workshop, CVPR 2025. Built on\nthe RoboTwin Simulation platform (1.0 and 2.0) and the AgileX COBOT-Magic Robot\nplatform, the competition consisted of three stages: Simulation Round 1,\nSimulation Round 2, and a final Real-World Round. Participants totally tackled\n17 dual-arm manipulation tasks, covering rigid, deformable, and tactile-based\nscenarios. The challenge attracted 64 global teams and over 400 participants,\nproducing top-performing solutions like SEM and AnchorDP3 and generating\nvaluable insights into generalizable bimanual policy learning. This report\noutlines the competition setup, task design, evaluation methodology, key\nfindings and future direction, aiming to support future research on robust and\ngeneralizable bimanual manipulation policies. The Challenge Webpage is\navailable at https://robotwin-benchmark.github.io/cvpr-2025-challenge/.", "comment": "Challenge Webpage:\n  https://robotwin-benchmark.github.io/cvpr-2025-challenge/", "pdf_url": "http://arxiv.org/pdf/2506.23351v1", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.MA"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.23351v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23676", "title": "A Unified Framework for Stealthy Adversarial Generation via Latent Optimization and Transferability Enhancement", "authors": ["Gaozheng Pei", "Ke Ma", "Dongpeng Zhang", "Chengzhi Sun", "Qianqian Xu", "Qingming Huang"], "summary": "Due to their powerful image generation capabilities, diffusion-based\nadversarial example generation methods through image editing are rapidly\ngaining popularity. However, due to reliance on the discriminative capability\nof the diffusion model, these diffusion-based methods often struggle to\ngeneralize beyond conventional image classification tasks, such as in Deepfake\ndetection. Moreover, traditional strategies for enhancing adversarial example\ntransferability are challenging to adapt to these methods. To address these\nchallenges, we propose a unified framework that seamlessly incorporates\ntraditional transferability enhancement strategies into diffusion model-based\nadversarial example generation via image editing, enabling their application\nacross a wider range of downstream tasks. Our method won first place in the\n\"1st Adversarial Attacks on Deepfake Detectors: A Challenge in the Era of\nAI-Generated Media\" competition at ACM MM25, which validates the effectiveness\nof our approach.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23676v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23676v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23293", "title": "Objective-Free Local Learning and Emergent Language Structure in Thinking Machines", "authors": ["P. Myles Eugenio"], "summary": "We present a neuro-symbolic framework for generative language modeling based\non local, event-driven emergent learning. At its core is a hierarchical\nHopfield memory chain acting as a compositional short-term memory and dynamic\ntokenizer (retokenizer). Rather than relying on predefined tokens or\nsupervision, the model builds structure from scratch, learning symbol sequences\nas multi-scale representations. It constructs projection tensors that bind\nco-occurring features into hierarchical tokens, introducing redundancy (i.e an\nemergent gauge structure) and enabling compression of local activations into\nlong-range dependencies. Curiously, we find that the retokenizer can filter\nnatural language patterns from noise, generating synthetic languages with\ncoherent internal morphology -- quantifiably the same as human language.\nLanguage is learned in a local (Hebbian) fashion, where model constraints\ndictate allowed emergent structure, and new information is retained in\nalignment with this structure. The absence of a global objective enables a form\nof plasticity not found in conventional language models, allowing the system to\ngeneralize beyond its initial inference class -- even without explicit data. We\ndemonstrate that briefly activating a new neuron during inference binds\ndistributed multi-scale token features into a symbolic embedding. These\nemergent embedding neurons act as long-term memory and support a key-value\nmechanism for compositional inference and generalization. This architecture\nprovides a methodological foundation for studying how symbolic structure can\nemerge from local neural learning. It offers a new pathway for building\nscalable, interpretable neuro-symbolic systems -- where tokens, grammar, and\nreasoning arise as compressed memory traces within a Hopfield hierarchy. This\napproach advances the development of neuromorphic architectures for generative\nlanguage models.", "comment": "22 pages, 7 figures", "pdf_url": "http://arxiv.org/pdf/2506.23293v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "q-bio.NC"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.23293v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23358", "title": "Federated Timeline Synthesis: Scalable and Private Methodology For Model Training and Deployment", "authors": ["Pawel Renc", "Michal K. Grzeszczyk", "Linglong Qian", "Nassim Oufattole", "Jeff Rasley", "Arkadiusz Sitek"], "summary": "We present Federated Timeline Synthesis (FTS), a novel framework for training\ngenerative foundation models across distributed timeseries data applied to\nelectronic health records (EHR). At its core, FTS represents patient history as\ntokenized Patient Health Timelines (PHTs), language-agnostic sequences encoding\ntemporal, categorical, and continuous clinical information. Each institution\ntrains an autoregressive transformer on its local PHTs and transmits only model\nweights to a central server. The server uses the generators to synthesize a\nlarge corpus of trajectories and train a Global Generator (GG), enabling\nzero-shot inference via Monte Carlo simulation of future PHTs. We evaluate FTS\non five clinically meaningful prediction tasks using MIMIC-IV data, showing\nthat models trained on synthetic data generated by GG perform comparably to\nthose trained on real data. FTS offers strong privacy guarantees, scalability\nacross institutions, and extensibility to diverse prediction and simulation\ntasks especially in healthcare, including counterfactual inference, early\nwarning detection, and synthetic trial design.", "comment": "conference paper", "pdf_url": "http://arxiv.org/pdf/2506.23358v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.23358v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23690", "title": "SynMotion: Semantic-Visual Adaptation for Motion Customized Video Generation", "authors": ["Shuai Tan", "Biao Gong", "Yujie Wei", "Shiwei Zhang", "Zhuoxin Liu", "Dandan Zheng", "Jingdong Chen", "Yan Wang", "Hao Ouyang", "Kecheng Zheng", "Yujun Shen"], "summary": "Diffusion-based video motion customization facilitates the acquisition of\nhuman motion representations from a few video samples, while achieving\narbitrary subjects transfer through precise textual conditioning. Existing\napproaches often rely on semantic-level alignment, expecting the model to learn\nnew motion concepts and combine them with other entities (e.g., ''cats'' or\n''dogs'') to produce visually appealing results. However, video data involve\ncomplex spatio-temporal patterns, and focusing solely on semantics cause the\nmodel to overlook the visual complexity of motion. Conversely, tuning only the\nvisual representation leads to semantic confusion in representing the intended\naction. To address these limitations, we propose SynMotion, a new\nmotion-customized video generation model that jointly leverages semantic\nguidance and visual adaptation. At the semantic level, we introduce the\ndual-embedding semantic comprehension mechanism which disentangles subject and\nmotion representations, allowing the model to learn customized motion features\nwhile preserving its generative capabilities for diverse subjects. At the\nvisual level, we integrate parameter-efficient motion adapters into a\npre-trained video generation model to enhance motion fidelity and temporal\ncoherence. Furthermore, we introduce a new embedding-specific training strategy\nwhich \\textbf{alternately optimizes} subject and motion embeddings, supported\nby the manually constructed Subject Prior Video (SPV) training dataset. This\nstrategy promotes motion specificity while preserving generalization across\ndiverse subjects. Lastly, we introduce MotionBench, a newly curated benchmark\nwith diverse motion patterns. Experimental results across both T2V and I2V\nsettings demonstrate that \\method outperforms existing baselines. Project page:\nhttps://lucaria-academy.github.io/SynMotion/", "comment": "Project page: https://lucaria-academy.github.io/SynMotion/", "pdf_url": "http://arxiv.org/pdf/2506.23690v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23690v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23311", "title": "Physics informed guided diffusion for accelerated multi-parametric MRI reconstruction", "authors": ["Perla Mayo", "Carolin M. Pirkl", "Alin Achim", "Bjoern Menze", "Mohammad Golbabaee"], "summary": "We introduce MRF-DiPh, a novel physics informed denoising diffusion approach\nfor multiparametric tissue mapping from highly accelerated, transient-state\nquantitative MRI acquisitions like Magnetic Resonance Fingerprinting (MRF). Our\nmethod is derived from a proximal splitting formulation, incorporating a\npretrained denoising diffusion model as an effective image prior to regularize\nthe MRF inverse problem. Further, during reconstruction it simultaneously\nenforces two key physical constraints: (1) k-space measurement consistency and\n(2) adherence to the Bloch response model. Numerical experiments on in-vivo\nbrain scans data show that MRF-DiPh outperforms deep learning and compressed\nsensing MRF baselines, providing more accurate parameter maps while better\npreserving measurement fidelity and physical model consistency-critical for\nsolving reliably inverse problems in medical imaging.", "comment": "11 pages, 1 figure, 1 algorithm, 3 tables. Accepted to MICCAI 2025.\n  This is a version prior peer-review", "pdf_url": "http://arxiv.org/pdf/2506.23311v1", "categories": ["eess.IV", "cs.LG", "physics.med-ph"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.23311v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23377", "title": "Perspective Dial: Measuring Perspective of Text and Guiding LLM Outputs", "authors": ["Taejin Kim", "Siun-Chuon Mau", "Konrad Vesey"], "summary": "Large language models (LLMs) are used in a variety of mission-critical roles.\nDue to the rapidly developing nature of LLMs, there is a lack of quantifiable\nunderstanding of the bias and perspective associated with LLM output. Inspired\nby this need, this paper considers the broader issue of perspective or\nviewpoint of general text and perspective control of large-language model (LLM)\noutput. Perspective-Dial consists of two main components: a (1) metric space,\ndubbed Perspective Space, that enables quantitative measurements of different\nperspectives regarding a topic, and the use of (2) Systematic Prompt\nEngineering that utilizes greedy-coordinate descent to control LLM output\nperspective based on measurement feedback from the Perspective Space. The\nempirical nature of the approach allows progress to side step a principled\nunderstanding of perspective or bias -- effectively quantifying and adjusting\noutputs for a variety of topics. Potential applications include detection,\ntracking and mitigation of LLM bias, narrative detection, sense making and\ntracking in public discourse, and debate bot advocating given perspective.", "comment": "7 pages, 5 main pages of text, 5 figures, 2 tables. Research work\n  performed at CACI INTL INC", "pdf_url": "http://arxiv.org/pdf/2506.23377v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.23377v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23705", "title": "Single Image Test-Time Adaptation via Multi-View Co-Training", "authors": ["Smriti Joshi", "Richard Osuala", "Lidia Garrucho", "Kaisar Kushibar", "Dimitri Kessler", "Oliver Diaz", "Karim Lekadir"], "summary": "Test-time adaptation enables a trained model to adjust to a new domain during\ninference, making it particularly valuable in clinical settings where such\non-the-fly adaptation is required. However, existing techniques depend on large\ntarget domain datasets, which are often impractical and unavailable in medical\nscenarios that demand per-patient, real-time inference. Moreover, current\nmethods commonly focus on two-dimensional images, failing to leverage the\nvolumetric richness of medical imaging data. Bridging this gap, we propose a\nPatch-Based Multi-View Co-Training method for Single Image Test-Time\nadaptation. Our method enforces feature and prediction consistency through\nuncertainty-guided self-training, enabling effective volumetric segmentation in\nthe target domain with only a single test-time image. Validated on three\npublicly available breast magnetic resonance imaging datasets for tumor\nsegmentation, our method achieves performance close to the upper bound\nsupervised benchmark while also outperforming all existing state-of-the-art\nmethods, on average by a Dice Similarity Coefficient of 3.75%. We publicly\nshare our accessible codebase, readily integrable with the popular nnUNet\nframework, at https://github.com/smriti-joshi/muvi.git.", "comment": "MICCAI 2025", "pdf_url": "http://arxiv.org/pdf/2506.23705v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23705v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23315", "title": "Ensemble BERT for Medication Event Classification on Electronic Health Records (EHRs)", "authors": ["Shouvon Sarker", "Xishuang Dong", "Lijun Qian"], "summary": "Identification of key variables such as medications, diseases, relations from\nhealth records and clinical notes has a wide range of applications in the\nclinical domain. n2c2 2022 provided shared tasks on challenges in natural\nlanguage processing for clinical data analytics on electronic health records\n(EHR), where it built a comprehensive annotated clinical data Contextualized\nMedication Event Dataset (CMED). This study focuses on subtask 2 in Track 1 of\nthis challenge that is to detect and classify medication events from clinical\nnotes through building a novel BERT-based ensemble model. It started with\npretraining BERT models on different types of big data such as Wikipedia and\nMIMIC. Afterwards, these pretrained BERT models were fine-tuned on CMED\ntraining data. These fine-tuned BERT models were employed to accomplish\nmedication event classification on CMED testing data with multiple predictions.\nThese multiple predictions generated by these fine-tuned BERT models were\nintegrated to build final prediction with voting strategies. Experimental\nresults demonstrated that BERT-based ensemble models can effectively improve\nstrict Micro-F score by about 5% and strict Macro-F score by about 6%,\nrespectively.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23315v1", "categories": ["cs.CL", "cs.LG"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.23315v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23382", "title": "SIEDD: Shared-Implicit Encoder with Discrete Decoders", "authors": ["Vikram Rangarajan", "Shishira Maiya", "Max Ehrlich", "Abhinav Shrivastava"], "summary": "Implicit Neural Representations (INRs) offer exceptional fidelity for video\ncompression by learning per-video optimized functions, but their adoption is\ncrippled by impractically slow encoding times. Existing attempts to accelerate\nINR encoding often sacrifice reconstruction quality or crucial coordinate-level\ncontrol essential for adaptive streaming and transcoding. We introduce SIEDD\n(Shared-Implicit Encoder with Discrete Decoders), a novel architecture that\nfundamentally accelerates INR encoding without these compromises. SIEDD first\nrapidly trains a shared, coordinate-based encoder on sparse anchor frames to\nefficiently capture global, low-frequency video features. This encoder is then\nfrozen, enabling massively parallel training of lightweight, discrete decoders\nfor individual frame groups, further expedited by aggressive coordinate-space\nsampling. This synergistic design delivers a remarkable 20-30X encoding\nspeed-up over state-of-the-art INR codecs on HD and 4K benchmarks, while\nmaintaining competitive reconstruction quality and compression ratios.\nCritically, SIEDD retains full coordinate-based control, enabling continuous\nresolution decoding and eliminating costly transcoding. Our approach\nsignificantly advances the practicality of high-fidelity neural video\ncompression, demonstrating a scalable and efficient path towards real-world\ndeployment. Our codebase is available at\nhttps://github.com/VikramRangarajan/SIEDD .", "comment": "Project page at https://vikramrangarajan.github.io/SIEDD . Project\n  code at https://github.com/VikramRangarajan/SIEDD", "pdf_url": "http://arxiv.org/pdf/2506.23382v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23382v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23711", "title": "Subjective Camera: Bridging Human Cognition and Visual Reconstruction through Sequence-Aware Sketch-Guided Diffusion", "authors": ["Haoyang Chen", "Dongfang Sun", "Caoyuan Ma", "Shiqin Wang", "Kewei Zhang", "Zheng Wang", "Zhixiang Wang"], "summary": "We propose Subjective Camera, a human-as-imaging-device paradigm that\nreconstructs real-world scenes from mental impressions through synergistic use\nof verbal descriptions and progressive rough sketches. This approach overcomes\ndual limitations of language ambiguity and sketch abstraction by treating the\nuser's drawing sequence as priors, effectively translating subjective\nperceptual expectations into photorealistic images.\n  Existing approaches face three fundamental barriers: (1) user-specific\nsubjective input biases, (2) huge modality gap between planar sketch and 3D\npriors in diffusion, and (3) sketch quality-sensitive performance degradation.\nCurrent solutions either demand resource-intensive model adaptation or impose\nimpractical requirements on sketch precision.\n  Our framework addresses these challenges through concept-sequential\ngeneration. (1) We establish robust appearance priors through text-reward\noptimization, and then implement sequence-aware disentangled generation that\nprocesses concepts in sketching order; these steps accommodate user-specific\nsubjective expectation in a train-free way. (2) We employ latent optimization\nthat effectively bridges the modality gap between planar sketches and 3D priors\nin diffusion. (3) Our hierarchical reward-guided framework enables the use of\nrough sketches without demanding artistic expertise. Comprehensive evaluation\nacross diverse datasets demonstrates that our approach achieves\nstate-of-the-art performance in maintaining both semantic and spatial\ncoherence.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23711v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23711v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23319", "title": "Learning to Rank with Variable Result Presentation Lengths", "authors": ["Norman Knyazev", "Harrie Oosterhuis"], "summary": "Learning to Rank (LTR) methods generally assume that each document in a top-K\nranking is presented in an equal format. However, previous work has shown that\nusers' perceptions of relevance can be changed by varying presentations, i.e.,\nallocating more vertical space to some documents to provide additional textual\nor image information. Furthermore, presentation length can also redirect\nattention, as users are more likely to notice longer presentations when\nscrolling through results. Deciding on the document presentation lengths in a\nfixed vertical space ranking is an important problem that has not been\naddressed by existing LTR methods.\n  We address this gap by introducing the variable presentation length ranking\ntask, where simultaneously the ordering of documents and their presentation\nlength is decided. Despite being a generalization of standard ranking, we show\nthat this setting brings significant new challenges: Firstly, the probability\nranking principle no longer applies to this setting, and secondly, the problem\ncannot be divided into separate ordering and length selection tasks.\n  We therefore propose VLPL - a new family of Plackett-Luce list-wise gradient\nestimation methods for the joint optimization of document ordering and lengths.\nOur semi-synthetic experiments show that VLPL can effectively balance the\nexpected exposure and attractiveness of all documents, achieving the best\nperformance across different ranking settings. Furthermore, we observe that\neven simple length-aware methods can achieve significant performance\nimprovements over fixed-length models. Altogether, our theoretical and\nempirical results highlight the importance and difficulties of combining\ndocument presentation with LTR.", "comment": "SIGIR 2025", "pdf_url": "http://arxiv.org/pdf/2506.23319v1", "categories": ["cs.IR", "cs.LG"], "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.23319v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23393", "title": "Hierarchical Memory Organization for Wikipedia Generation", "authors": ["Eugene J. Yu", "Dawei Zhu", "Yifan Song", "Xiangyu Wong", "Jiebin Zhang", "Wenxuan Shi", "Xiaoguang Li", "Qun Liu", "Sujian Li"], "summary": "Generating Wikipedia articles autonomously is a challenging task requiring\nthe integration of accurate, comprehensive, and well-structured information\nfrom diverse sources. This paper introduces the Memory Organization-based\nGeneration (MOG) framework, a novel approach to address these challenges by\nleveraging a hierarchical memory architecture. MOG extracts fine-grained memory\nunits from web documents, recursively organizes them into a Wikipedia-style\nhierarchical structure, and uses this structure to guide the generation\nprocess. This ensures alignment between memory and the article outline,\nimproving both informativeness and verifiability while minimizing\nhallucinations. Additionally, a citation module is implemented to enhance\ntraceability by linking every generated sentence to specific memory units.\nEvaluations on our newly created WikiStart dataset demonstrate that MOG\noutperforms baseline methods in producing informative and reliable articles,\nmaking it particularly robust in real-world scenarios.", "comment": "ACL 2025 Main Conference", "pdf_url": "http://arxiv.org/pdf/2506.23393v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.23393v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23714", "title": "Towards an Automated Multimodal Approach for Video Summarization: Building a Bridge Between Text, Audio and Facial Cue-Based Summarization", "authors": ["Md Moinul Islam", "Sofoklis Kakouros", "Janne Heikkil√§", "Mourad Oussalah"], "summary": "The increasing volume of video content in educational, professional, and\nsocial domains necessitates effective summarization techniques that go beyond\ntraditional unimodal approaches. This paper proposes a behaviour-aware\nmultimodal video summarization framework that integrates textual, audio, and\nvisual cues to generate timestamp-aligned summaries. By extracting prosodic\nfeatures, textual cues and visual indicators, the framework identifies\nsemantically and emotionally important moments. A key contribution is the\nidentification of bonus words, which are terms emphasized across multiple\nmodalities and used to improve the semantic relevance and expressive clarity of\nthe summaries. The approach is evaluated against pseudo-ground truth (pGT)\nsummaries generated using LLM-based extractive method. Experimental results\ndemonstrate significant improvements over traditional extractive method, such\nas the Edmundson method, in both text and video-based evaluation metrics.\nText-based metrics show ROUGE-1 increasing from 0.4769 to 0.7929 and BERTScore\nfrom 0.9152 to 0.9536, while in video-based evaluation, our proposed framework\nimproves F1-Score by almost 23%. The findings underscore the potential of\nmultimodal integration in producing comprehensive and behaviourally informed\nvideo summaries.", "comment": "Accepted to HHAI WS 2025: Workshops at the Fourth International\n  Conference on Hybrid Human-Artificial Intelligence (HHAI)", "pdf_url": "http://arxiv.org/pdf/2506.23714v1", "categories": ["cs.CV", "cs.CL"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23714v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23344", "title": "Data-Driven Self-Supervised Learning for the Discovery of Solution Singularity for Partial Differential Equations", "authors": ["Difeng Cai", "Paulina Sep√∫lveda"], "summary": "The appearance of singularities in the function of interest constitutes a\nfundamental challenge in scientific computing. It can significantly undermine\nthe effectiveness of numerical schemes for function approximation, numerical\nintegration, and the solution of partial differential equations (PDEs), etc.\nThe problem becomes more sophisticated if the location of the singularity is\nunknown, which is often encountered in solving PDEs. Detecting the singularity\nis therefore critical for developing efficient adaptive methods to reduce\ncomputational costs in various applications. In this paper, we consider\nsingularity detection in a purely data-driven setting. Namely, the input only\ncontains given data, such as the vertex set from a mesh. To overcome the\nlimitation of the raw unlabeled data, we propose a self-supervised learning\n(SSL) framework for estimating the location of the singularity. A key component\nis a filtering procedure as the pretext task in SSL, where two filtering\nmethods are presented, based on $k$ nearest neighbors and kernel density\nestimation, respectively. We provide numerical examples to illustrate the\npotential pathological or inaccurate results due to the use of raw data without\nfiltering. Various experiments are presented to demonstrate the ability of the\nproposed approach to deal with input perturbation, label corruption, and\ndifferent kinds of singularities such interior circle, boundary layer,\nconcentric semicircles, etc.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23344v1", "categories": ["math.NA", "cs.LG", "cs.NA", "stat.ML"], "cate": "math.NA", "url": "http://arxiv.org/abs/2506.23344v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23394", "title": "Teaching a Language Model to Speak the Language of Tools", "authors": ["Simeon Emanuilov"], "summary": "External tool integration through function-calling is essential for practical\nlanguage model applications, yet most multilingual models lack reliable\ntool-use capabilities in non-English languages. Even state-of-the-art\nmultilingual models struggle with determining when to use tools and generating\nthe structured outputs required for function calls, often exhibiting language\nconfusion when prompted in lower-resource languages. This work presents a\nmethodology for adapting existing language models to enable robust tool use in\nany target language, using Bulgarian as a case study. The approach involves\ncontinued training of the BgGPT model series (2.6B, 9B, 27B parameters) on a\nnovel bilingual dataset of 10,035 function-calling examples designed to support\nstandardized protocols like MCP (Model Context Protocol). The research\nintroduces TUCAN (Tool-Using Capable Assistant Navigator), which achieves up to\n28.75% improvement in function-calling accuracy over base models while\npreserving core language understanding, as verified on established Bulgarian\nbenchmarks. Beyond accuracy gains, TUCAN models demonstrate production-ready\nresponse formatting with clean, parsable function calls, contrasting with the\nverbose and inconsistent outputs of base models. The models, evaluation\nframework, and dataset are released to enable replication for other languages.\nThis work demonstrates a practical approach for extending tool-augmented\ncapabilities beyond English-centric systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23394v1", "categories": ["cs.IR", "cs.AI", "cs.CL", "I.2.7; I.2.1"], "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.23394v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23724", "title": "When Small Guides Large: Cross-Model Co-Learning for Test-Time Adaptation", "authors": ["Chang'an Yi", "Xiaohui Deng", "Guohao Chen", "Yan Zhou", "Qinghua Lu", "Shuaicheng Niu"], "summary": "Test-time Adaptation (TTA) adapts a given model to testing domain data with\npotential domain shifts through online unsupervised learning, yielding\nimpressive performance. However, to date, existing TTA methods primarily focus\non single-model adaptation. In this work, we investigate an intriguing\nquestion: how does cross-model knowledge influence the TTA process? Our\nfindings reveal that, in TTA's unsupervised online setting, each model can\nprovide complementary, confident knowledge to the others, even when there are\nsubstantial differences in model size. For instance, a smaller model like\nMobileViT (10.6M parameters) can effectively guide a larger model like ViT-Base\n(86.6M parameters). In light of this, we propose COCA, a Cross-Model\nCo-Learning framework for TTA, which mainly consists of two main strategies. 1)\nCo-adaptation adaptively integrates complementary knowledge from other models\nthroughout the TTA process, reducing individual model biases. 2)\nSelf-adaptation enhances each model's unique strengths via unsupervised\nlearning, enabling diverse adaptation to the target domain. Extensive\nexperiments show that COCA, which can also serve as a plug-and-play module,\nsignificantly boosts existing SOTAs, on models with various sizes--including\nResNets, ViTs, and Mobile-ViTs--via cross-model co-learned TTA. For example,\nwith Mobile-ViT's guidance, COCA raises ViT-Base's average adaptation accuracy\non ImageNet-C from 51.7% to 64.5%. The code is publicly available at\nhttps://github.com/ycarobot/COCA.", "comment": "15 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2506.23724v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23724v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23351", "title": "Benchmarking Generalizable Bimanual Manipulation: RoboTwin Dual-Arm Collaboration Challenge at CVPR 2025 MEIS Workshop", "authors": ["Tianxing Chen", "Kaixuan Wang", "Zhaohui Yang", "Yuhao Zhang", "Zanxin Chen", "Baijun Chen", "Wanxi Dong", "Ziyuan Liu", "Dong Chen", "Tianshuo Yang", "Haibao Yu", "Xiaokang Yang", "Yusen Qin", "Zhiqiang Xie", "Yao Mu", "Ping Luo", "Tian Nian", "Weiliang Deng", "Yiheng Ge", "Yibin Liu", "Zixuan Li", "Dehui Wang", "Zhixuan Liang", "Haohui Xie", "Rijie Zeng", "Yunfei Ge", "Peiqing Cong", "Guannan He", "Zhaoming Han", "Ruocheng Yin", "Jingxiang Guo", "Lunkai Lin", "Tianling Xu", "Hongzhe Bi", "Xuewu Lin", "Tianwei Lin", "Shujie Luo", "Keyu Li", "Ziyan Zhao", "Ke Fan", "Heyang Xu", "Bo Peng", "Wenlong Gao", "Dongjiang Li", "Feng Jin", "Hui Shen", "Jinming Li", "Chaowei Cui", "Yuchen", "Yaxin Peng", "Lingdong Zeng", "Wenlong Dong", "Tengfei Li", "Weijie Ke", "Jun Chen", "Erdemt Bao", "Tian Lan", "Tenglong Liu", "Jin Yang", "Huiping Zhuang", "Baozhi Jia", "Shuai Zhang", "Zhengfeng Zou", "Fangheng Guan", "Tianyi Jia", "Ke Zhou", "Hongjiu Zhang", "Yating Han", "Cheng Fang", "Yixian Zou", "Chongyang Xu", "Qinglun Zhang", "Shen Cheng", "Xiaohe Wang", "Ping Tan", "Haoqiang Fan", "Shuaicheng Liu", "Jiaheng Chen", "Chuxuan Huang", "Chengliang Lin", "Kaijun Luo", "Boyu Yue", "Yi Liu", "Jinyu Chen", "Zichang Tan", "Liming Deng", "Shuo Xu", "Zijian Cai", "Shilong Yin", "Hao Wang", "Hongshan Liu", "Tianyang Li", "Long Shi", "Ran Xu", "Huilin Xu", "Zhengquan Zhang", "Congsheng Xu", "Jinchang Yang", "Feng Xu"], "summary": "Embodied Artificial Intelligence (Embodied AI) is an emerging frontier in\nrobotics, driven by the need for autonomous systems that can perceive, reason,\nand act in complex physical environments. While single-arm systems have shown\nstrong task performance, collaborative dual-arm systems are essential for\nhandling more intricate tasks involving rigid, deformable, and\ntactile-sensitive objects. To advance this goal, we launched the RoboTwin\nDual-Arm Collaboration Challenge at the 2nd MEIS Workshop, CVPR 2025. Built on\nthe RoboTwin Simulation platform (1.0 and 2.0) and the AgileX COBOT-Magic Robot\nplatform, the competition consisted of three stages: Simulation Round 1,\nSimulation Round 2, and a final Real-World Round. Participants totally tackled\n17 dual-arm manipulation tasks, covering rigid, deformable, and tactile-based\nscenarios. The challenge attracted 64 global teams and over 400 participants,\nproducing top-performing solutions like SEM and AnchorDP3 and generating\nvaluable insights into generalizable bimanual policy learning. This report\noutlines the competition setup, task design, evaluation methodology, key\nfindings and future direction, aiming to support future research on robust and\ngeneralizable bimanual manipulation policies. The Challenge Webpage is\navailable at https://robotwin-benchmark.github.io/cvpr-2025-challenge/.", "comment": "Challenge Webpage:\n  https://robotwin-benchmark.github.io/cvpr-2025-challenge/", "pdf_url": "http://arxiv.org/pdf/2506.23351v1", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.MA"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.23351v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23419", "title": "BenchMake: Turn any scientific data set into a reproducible benchmark", "authors": ["Amanda S Barnard"], "summary": "Benchmark data sets are a cornerstone of machine learning development and\napplications, ensuring new methods are robust, reliable and competitive. The\nrelative rarity of benchmark sets in computational science, due to the\nuniqueness of the problems and the pace of change in the associated domains,\nmakes evaluating new innovations difficult for computational scientists. In\nthis paper a new tool is developed and tested to potentially turn any of the\nincreasing numbers of scientific data sets made openly available into a\nbenchmark accessible to the community. BenchMake uses non-negative matrix\nfactorisation to deterministically identify and isolate challenging edge cases\non the convex hull (the smallest convex set that contains all existing data\ninstances) and partitions a required fraction of matched data instances into a\ntesting set that maximises divergence and statistical significance, across\ntabular, graph, image, signal and textual modalities. BenchMake splits are\ncompared to establish splits and random splits using ten publicly available\nbenchmark sets from different areas of science, with different sizes, shapes,\ndistributions.", "comment": "10 pages, 15 pages in Appendix, 15 figures, 5 tables, 57 references", "pdf_url": "http://arxiv.org/pdf/2506.23419v1", "categories": ["cs.LG", "cs.AI", "cs.DL", "62G09", "J.1"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.23419v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23729", "title": "Proteus-ID: ID-Consistent and Motion-Coherent Video Customization", "authors": ["Guiyu Zhang", "Chen Shi", "Zijian Jiang", "Xunzhi Xiang", "Jingjing Qian", "Shaoshuai Shi", "Li Jiang"], "summary": "Video identity customization seeks to synthesize realistic, temporally\ncoherent videos of a specific subject, given a single reference image and a\ntext prompt. This task presents two core challenges: (1) maintaining identity\nconsistency while aligning with the described appearance and actions, and (2)\ngenerating natural, fluid motion without unrealistic stiffness. To address\nthese challenges, we introduce Proteus-ID, a novel diffusion-based framework\nfor identity-consistent and motion-coherent video customization. First, we\npropose a Multimodal Identity Fusion (MIF) module that unifies visual and\ntextual cues into a joint identity representation using a Q-Former, providing\ncoherent guidance to the diffusion model and eliminating modality imbalance.\nSecond, we present a Time-Aware Identity Injection (TAII) mechanism that\ndynamically modulates identity conditioning across denoising steps, improving\nfine-detail reconstruction. Third, we propose Adaptive Motion Learning (AML), a\nself-supervised strategy that reweights the training loss based on\noptical-flow-derived motion heatmaps, enhancing motion realism without\nrequiring additional inputs. To support this task, we construct Proteus-Bench,\na high-quality dataset comprising 200K curated clips for training and 150\nindividuals from diverse professions and ethnicities for evaluation. Extensive\nexperiments demonstrate that Proteus-ID outperforms prior methods in identity\npreservation, text alignment, and motion quality, establishing a new benchmark\nfor video identity customization. Codes and data are publicly available at\nhttps://grenoble-zhang.github.io/Proteus-ID/.", "comment": "Preprint. Work in progress", "pdf_url": "http://arxiv.org/pdf/2506.23729v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23729v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23371", "title": "Investigating an Overfitting and Degeneration Phenomenon in Self-Supervised Multi-Pitch Estimation", "authors": ["Frank Cwitkowitz", "Zhiyao Duan"], "summary": "Multi-Pitch Estimation (MPE) continues to be a sought after capability of\nMusic Information Retrieval (MIR) systems, and is critical for many\napplications and downstream tasks involving pitch, including music\ntranscription. However, existing methods are largely based on supervised\nlearning, and there are significant challenges in collecting annotated data for\nthe task. Recently, self-supervised techniques exploiting intrinsic properties\nof pitch and harmonic signals have shown promise for both monophonic and\npolyphonic pitch estimation, but these still remain inferior to supervised\nmethods. In this work, we extend the classic supervised MPE paradigm by\nincorporating several self-supervised objectives based on pitch-invariant and\npitch-equivariant properties. This joint training results in a substantial\nimprovement under closed training conditions, which naturally suggests that\napplying the same objectives to a broader collection of data will yield further\nimprovements. However, in doing so we uncover a phenomenon whereby our model\nsimultaneously overfits to the supervised data while degenerating on data used\nfor self-supervision only. We demonstrate and investigate this and offer our\ninsights on the underlying problem.", "comment": "Accepted to ISMIR 2025", "pdf_url": "http://arxiv.org/pdf/2506.23371v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "cate": "eess.AS", "url": "http://arxiv.org/abs/2506.23371v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23423", "title": "TuCo: Measuring the Contribution of Fine-Tuning to Individual Responses of LLMs", "authors": ["Felipe Nuti", "Tim Franzmeyer", "Jo√£o Henriques"], "summary": "Past work has studied the effects of fine-tuning on large language models'\n(LLMs) overall performance on certain tasks. However, a quantitative and\nsystematic method for analyzing its effect on individual outputs is still\nlacking. Here, we propose a new method for measuring the contribution that\nfine-tuning makes to individual LLM responses, assuming access to the original\npre-trained model. Our method tracks the model's intermediate hidden states,\nproviding a more fine-grained insight into the effects of fine-tuning than a\nsimple comparison of final outputs from pre-trained and fine-tuned models. We\nintroduce and theoretically analyze an exact decomposition of any fine-tuned\nLLM into a pre-training component and a fine-tuning component. Empirically, we\nfind that model behavior and performance can be steered by up- or down-scaling\nthe fine-tuning component during the forward pass. Motivated by this finding\nand our theoretical analysis, we define the Tuning Contribution (TuCo) as the\nratio of the magnitudes of the fine-tuning component to the pre-training\ncomponent. We observe that three prominent adversarial attacks on LLMs\ncircumvent safety measures in a way that reduces TuCo, and that TuCo is\nconsistently lower on prompts where these attacks succeed compared to those\nwhere they do not. This suggests that attenuating the effect of fine-tuning on\nmodel outputs plays a role in the success of such attacks. In summary, TuCo\nenables the quantitative study of how fine-tuning influences model behavior and\nsafety, and vice versa.", "comment": "ICML 2025", "pdf_url": "http://arxiv.org/pdf/2506.23423v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.23423v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23751", "title": "Can We Challenge Open-Vocabulary Object Detectors with Generated Content in Street Scenes?", "authors": ["Annika M√ºtze", "Sadia Ilyas", "Christian D√∂rpelkus", "Matthias Rottmann"], "summary": "Open-vocabulary object detectors such as Grounding DINO are trained on vast\nand diverse data, achieving remarkable performance on challenging datasets. Due\nto that, it is unclear where to find their limitations, which is of major\nconcern when using in safety-critical applications. Real-world data does not\nprovide sufficient control, required for a rigorous evaluation of model\ngeneralization. In contrast, synthetically generated data allows to\nsystematically explore the boundaries of model competence/generalization. In\nthis work, we address two research questions: 1) Can we challenge\nopen-vocabulary object detectors with generated image content? 2) Can we find\nsystematic failure modes of those models? To address these questions, we design\ntwo automated pipelines using stable diffusion to inpaint unusual objects with\nhigh diversity in semantics, by sampling multiple substantives from WordNet and\nChatGPT. On the synthetically generated data, we evaluate and compare multiple\nopen-vocabulary object detectors as well as a classical object detector. The\nsynthetic data is derived from two real-world datasets, namely LostAndFound, a\nchallenging out-of-distribution (OOD) detection benchmark, and the NuImages\ndataset. Our results indicate that inpainting can challenge open-vocabulary\nobject detectors in terms of overlooking objects. Additionally, we find a\nstrong dependence of open-vocabulary models on object location, rather than on\nobject semantics. This provides a systematic approach to challenge\nopen-vocabulary models and gives valuable insights on how data could be\nacquired to effectively improve these models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23751v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23751v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23382", "title": "SIEDD: Shared-Implicit Encoder with Discrete Decoders", "authors": ["Vikram Rangarajan", "Shishira Maiya", "Max Ehrlich", "Abhinav Shrivastava"], "summary": "Implicit Neural Representations (INRs) offer exceptional fidelity for video\ncompression by learning per-video optimized functions, but their adoption is\ncrippled by impractically slow encoding times. Existing attempts to accelerate\nINR encoding often sacrifice reconstruction quality or crucial coordinate-level\ncontrol essential for adaptive streaming and transcoding. We introduce SIEDD\n(Shared-Implicit Encoder with Discrete Decoders), a novel architecture that\nfundamentally accelerates INR encoding without these compromises. SIEDD first\nrapidly trains a shared, coordinate-based encoder on sparse anchor frames to\nefficiently capture global, low-frequency video features. This encoder is then\nfrozen, enabling massively parallel training of lightweight, discrete decoders\nfor individual frame groups, further expedited by aggressive coordinate-space\nsampling. This synergistic design delivers a remarkable 20-30X encoding\nspeed-up over state-of-the-art INR codecs on HD and 4K benchmarks, while\nmaintaining competitive reconstruction quality and compression ratios.\nCritically, SIEDD retains full coordinate-based control, enabling continuous\nresolution decoding and eliminating costly transcoding. Our approach\nsignificantly advances the practicality of high-fidelity neural video\ncompression, demonstrating a scalable and efficient path towards real-world\ndeployment. Our codebase is available at\nhttps://github.com/VikramRangarajan/SIEDD .", "comment": "Project page at https://vikramrangarajan.github.io/SIEDD . Project\n  code at https://github.com/VikramRangarajan/SIEDD", "pdf_url": "http://arxiv.org/pdf/2506.23382v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23382v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23424", "title": "Accurate Parameter-Efficient Test-Time Adaptation for Time Series Forecasting", "authors": ["Heitor R. Medeiros", "Hossein Sharifi-Noghabi", "Gabriel L. Oliveira", "Saghar Irandoust"], "summary": "Real-world time series often exhibit a non-stationary nature, degrading the\nperformance of pre-trained forecasting models. Test-Time Adaptation (TTA)\naddresses this by adjusting models during inference, but existing methods\ntypically update the full model, increasing memory and compute costs. We\npropose PETSA, a parameter-efficient method that adapts forecasters at test\ntime by only updating small calibration modules on the input and output. PETSA\nuses low-rank adapters and dynamic gating to adjust representations without\nretraining. To maintain accuracy despite limited adaptation capacity, we\nintroduce a specialized loss combining three components: (1) a robust term, (2)\na frequency-domain term to preserve periodicity, and (3) a patch-wise\nstructural term for structural alignment. PETSA improves the adaptability of\nvarious forecasting backbones while requiring fewer parameters than baselines.\nExperimental results on benchmark datasets show that PETSA achieves competitive\nor better performance across all horizons. Our code is available at:\nhttps://github.com/BorealisAI/PETSA", "comment": "Second Workshop on Test-Time Adaptation: Putting Updates to the Test!\n  at ICML 2025, Vancouver, Canada. 2025", "pdf_url": "http://arxiv.org/pdf/2506.23424v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.23424v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23783", "title": "Mamba-FETrack V2: Revisiting State Space Model for Frame-Event based Visual Object Tracking", "authors": ["Shiao Wang", "Ju Huang", "Qingchuan Ma", "Jinfeng Gao", "Chunyi Xu", "Xiao Wang", "Lan Chen", "Bo Jiang"], "summary": "Combining traditional RGB cameras with bio-inspired event cameras for robust\nobject tracking has garnered increasing attention in recent years. However,\nmost existing multimodal tracking algorithms depend heavily on high-complexity\nVision Transformer architectures for feature extraction and fusion across\nmodalities. This not only leads to substantial computational overhead but also\nlimits the effectiveness of cross-modal interactions. In this paper, we propose\nan efficient RGB-Event object tracking framework based on the linear-complexity\nVision Mamba network, termed Mamba-FETrack V2. Specifically, we first design a\nlightweight Prompt Generator that utilizes embedded features from each\nmodality, together with a shared prompt pool, to dynamically generate\nmodality-specific learnable prompt vectors. These prompts, along with the\nmodality-specific embedded features, are then fed into a Vision Mamba-based\nFEMamba backbone, which facilitates prompt-guided feature extraction,\ncross-modal interaction, and fusion in a unified manner. Finally, the fused\nrepresentations are passed to the tracking head for accurate target\nlocalization. Extensive experimental evaluations on multiple RGB-Event tracking\nbenchmarks, including short-term COESOT dataset and long-term datasets, i.e.,\nFE108 and FELT V2, demonstrate the superior performance and efficiency of the\nproposed tracking framework. The source code and pre-trained models will be\nreleased on https://github.com/Event-AHU/Mamba_FETrack", "comment": "Journal extension of Mamba-FETrack which was published on Pattern\n  Recognition and Computer Vision (PRCV) 2024", "pdf_url": "http://arxiv.org/pdf/2506.23783v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23783v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23396", "title": "AICO: Feature Significance Tests for Supervised Learning", "authors": ["Kay Giesecke", "Enguerrand Horel", "Chartsiri Jirachotkulthorn"], "summary": "The opacity of many supervised learning algorithms remains a key challenge,\nhindering scientific discovery and limiting broader deployment -- particularly\nin high-stakes domains. This paper develops model- and distribution-agnostic\nsignificance tests to assess the influence of input features in any regression\nor classification algorithm. Our method evaluates a feature's incremental\ncontribution to model performance by masking its values across samples. Under\nthe null hypothesis, the distribution of performance differences across a test\nset has a non-positive median. We construct a uniformly most powerful,\nrandomized sign test for this median, yielding exact p-values for assessing\nfeature significance and confidence intervals with exact coverage for\nestimating population-level feature importance. The approach requires minimal\nassumptions, avoids model retraining or auxiliary models, and remains\ncomputationally efficient even for large-scale, high-dimensional settings.\nExperiments on synthetic tasks validate its statistical and computational\nadvantages, and applications to real-world data illustrate its practical\nutility.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23396v1", "categories": ["stat.ML", "cs.LG"], "cate": "stat.ML", "url": "http://arxiv.org/abs/2506.23396v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23431", "title": "Pipelined Decoder for Efficient Context-Aware Text Generation", "authors": ["Zixian Huang", "Chenxu Niu", "Yu Gu", "Gengyang Xiao", "Xinwei Huang", "Gong Cheng"], "summary": "As the basis of generative AI, an autoregressive model requires the\ngeneration of a new token depending on all the previously generated tokens,\nwhich brings high quality but also restricts the model to generate tokens one\nby one, forming a bottleneck limiting the generation speed. In this paper, we\npropose a new decoder architecture that efficiently generates text in parallel\nfor context-aware generation tasks. Our proposed pipelined decoder initiates\nthe generation of multiple subsequences simultaneously, and, at each time-step,\nit generates a new token for each subsequence to realize parallelism.\nExperiments on multiple text generation tasks, including question answering,\ntext summarization, and keyphrase generation, show that our pipelined decoder\nsignificantly improves the generation speed without a significant loss of\ngeneration quality or additional memory consumption.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23431v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.23431v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23785", "title": "Visual Textualization for Image Prompted Object Detection", "authors": ["Yongjian Wu", "Yang Zhou", "Jiya Saiyin", "Bingzheng Wei", "Yan Xu"], "summary": "We propose VisTex-OVLM, a novel image prompted object detection method that\nintroduces visual textualization -- a process that projects a few visual\nexemplars into the text feature space to enhance Object-level Vision-Language\nModels' (OVLMs) capability in detecting rare categories that are difficult to\ndescribe textually and nearly absent from their pre-training data, while\npreserving their pre-trained object-text alignment. Specifically, VisTex-OVLM\nleverages multi-scale textualizing blocks and a multi-stage fusion strategy to\nintegrate visual information from visual exemplars, generating textualized\nvisual tokens that effectively guide OVLMs alongside text prompts. Unlike\nprevious methods, our method maintains the original architecture of OVLM,\nmaintaining its generalization capabilities while enhancing performance in\nfew-shot settings. VisTex-OVLM demonstrates superior performance across\nopen-set datasets which have minimal overlap with OVLM's pre-training data and\nachieves state-of-the-art results on few-shot benchmarks PASCAL VOC and MSCOCO.\nThe code will be released at https://github.com/WitGotFlg/VisTex-OVLM.", "comment": "Accepted by ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2506.23785v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23785v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23411", "title": "Datasets for Fairness in Language Models: An In-Depth Survey", "authors": ["Jiale Zhang", "Zichong Wang", "Avash Palikhe", "Zhipeng Yin", "Wenbin Zhang"], "summary": "Fairness benchmarks play a central role in shaping how we evaluate language\nmodels, yet surprisingly little attention has been given to examining the\ndatasets that these benchmarks rely on. This survey addresses that gap by\npresenting a broad and careful review of the most widely used fairness datasets\nin current language model research, characterizing them along several key\ndimensions including their origin, scope, content, and intended use to help\nresearchers better appreciate the assumptions and limitations embedded in these\nresources. To support more meaningful comparisons and analyses, we introduce a\nunified evaluation framework that reveals consistent patterns of demographic\ndisparities across datasets and scoring methods. Applying this framework to\ntwenty four common benchmarks, we highlight the often overlooked biases that\ncan influence conclusions about model fairness and offer practical guidance for\nselecting, combining, and interpreting these datasets. We also point to\nopportunities for creating new fairness benchmarks that reflect more diverse\nsocial contexts and encourage more thoughtful use of these tools going forward.\nAll code, data, and detailed results are publicly available at\nhttps://github.com/vanbanTruong/Fairness-in-Large-Language-Models/tree/main/datasets\nto promote transparency and reproducibility across the research community.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23411v1", "categories": ["cs.CL", "cs.CY", "cs.LG"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.23411v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23437", "title": "From Large-scale Audio Tagging to Real-Time Explainable Emergency Vehicle Sirens Detection", "authors": ["Stefano Giacomelli", "Marco Giordano", "Claudia Rinaldi", "Fabio Graziosi"], "summary": "Accurate recognition of Emergency Vehicle (EV) sirens is critical for the\nintegration of intelligent transportation systems, smart city monitoring\nsystems, and autonomous driving technologies. Modern automatic solutions are\nlimited by the lack of large scale, curated datasets and by the computational\ndemands of state of the art sound event detection models. This work introduces\nE2PANNs (Efficient Emergency Pre trained Audio Neural Networks), a lightweight\nConvolutional Neural Network architecture derived from the PANNs framework,\nspecifically optimized for binary EV siren detection. Leveraging our dedicated\nsubset of AudioSet (AudioSet EV) we fine-tune and evaluate E2PANNs across\nmultiple reference datasets and test its viability on embedded hardware. The\nexperimental campaign includes ablation studies, cross-domain benchmarking, and\nreal-time inference deployment on edge device. Interpretability analyses\nexploiting Guided Backpropagation and ScoreCAM algorithms provide insights into\nthe model internal representations and validate its ability to capture distinct\nspectrotemporal patterns associated with different types of EV sirens. Real\ntime performance is assessed through frame wise and event based detection\nmetrics, as well as a detailed analysis of false positive activations. Results\ndemonstrate that E2PANNs establish a new state of the art in this research\ndomain, with high computational efficiency, and suitability for edge-based\naudio monitoring and safety-critical applications.", "comment": "pre-print (submitted to the IEEE/ACM Transactions on Audio, Speech,\n  and Language Processing)", "pdf_url": "http://arxiv.org/pdf/2506.23437v1", "categories": ["cs.SD", "cs.AI", "eess.AS", "68T07", "E.1; H.1; I.2; I.5; J.2; K.4; C.4"], "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.23437v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23801", "title": "Controllable Reference-Based Real-World Remote Sensing Image Super-Resolution with Generative Diffusion Priors", "authors": ["Ce Wang", "Wanjie Sun"], "summary": "Super-resolution (SR) techniques can enhance the spatial resolution of remote\nsensing images by utilizing low-resolution (LR) images to reconstruct\nhigh-resolution (HR) images, enabling more efficient large-scale earth\nobservation applications. While single-image super-resolution (SISR) methods\nhave shown progress, reference-based super-resolution (RefSR) offers superior\nperformance by incorporating historical HR images alongside current LR\nobservations. However, existing RefSR methods struggle with real-world\ncomplexities, such as cross-sensor resolution gap and significant land cover\nchanges, often leading to under-generation or over-reliance on reference image.\nTo address these challenges, we propose CRefDiff, a novel controllable\nreference-based diffusion model for real-world remote sensing image SR. To\naddress the under-generation problem, CRefDiff is built upon the pretrained\nStable Diffusion model, leveraging its powerful generative prior to produce\naccurate structures and textures. To mitigate over-reliance on the reference,\nwe introduce a dual-branch fusion mechanism that adaptively integrates both\nlocal and global information from the reference image. Moreover, this novel\ndual-branch design enables reference strength control during inference,\nenhancing interactivity and flexibility of the model. Finally, a strategy named\nBetter Start is proposed to significantly reduce the number of denoising steps,\nthereby accelerating the inference process. To support further research, we\nintroduce Real-RefRSSRD, a new real-world RefSR dataset for remote sensing\nimages, consisting of HR NAIP and LR Sentinel-2 image pairs with diverse land\ncover changes and significant temporal gaps. Extensive experiments on\nReal-RefRSSRD show that CRefDiff achieves state-of-the-art performance across\nvarious metrics and improves downstream tasks such as scene classification and\nsemantic segmentation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23801v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23801v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23426", "title": "Detecting What Matters: A Novel Approach for Out-of-Distribution 3D Object Detection in Autonomous Vehicles", "authors": ["Menna Taha", "Aya Ahmed", "Mohammed Karmoose", "Yasser Gadallah"], "summary": "Autonomous vehicles (AVs) use object detection models to recognize their\nsurroundings and make driving decisions accordingly. Conventional object\ndetection approaches classify objects into known classes, which limits the AV's\nability to detect and appropriately respond to Out-of-Distribution (OOD)\nobjects. This problem is a significant safety concern since the AV may fail to\ndetect objects or misclassify them, which can potentially lead to hazardous\nsituations such as accidents. Consequently, we propose a novel object detection\napproach that shifts the emphasis from conventional class-based classification\nto object harmfulness determination. Instead of object detection by their\nspecific class, our method identifies them as either 'harmful' or 'harmless'\nbased on whether they pose a danger to the AV. This is done based on the object\nposition relative to the AV and its trajectory. With this metric, our model can\neffectively detect previously unseen objects to enable the AV to make safer\nreal-time decisions. Our results demonstrate that the proposed model\neffectively detects OOD objects, evaluates their harmfulness, and classifies\nthem accordingly, thus enhancing the AV decision-making effectiveness in\ndynamic environments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23426v1", "categories": ["cs.CV", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23426v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23461", "title": "Time-variant Image Inpainting via Interactive Distribution Transition Estimation", "authors": ["Yun Xing", "Qing Guo", "Xiaoguang Li", "Yihao Huang", "Xiaofeng Cao", "Di Lin", "Ivor Tsang", "Lei Ma"], "summary": "In this work, we focus on a novel and practical task, i.e., Time-vAriant\niMage inPainting (TAMP). The aim of TAMP is to restore a damaged target image\nby leveraging the complementary information from a reference image, where both\nimages captured the same scene but with a significant time gap in between,\ni.e., time-variant images. Different from conventional reference-guided image\ninpainting, the reference image under TAMP setup presents significant content\ndistinction to the target image and potentially also suffers from damages. Such\nan application frequently happens in our daily lives to restore a damaged image\nby referring to another reference image, where there is no guarantee of the\nreference image's source and quality. In particular, our study finds that even\nstate-of-the-art (SOTA) reference-guided image inpainting methods fail to\nachieve plausible results due to the chaotic image complementation. To address\nsuch an ill-posed problem, we propose a novel Interactive Distribution\nTransition Estimation (InDiTE) module which interactively complements the\ntime-variant images with adaptive semantics thus facilitate the restoration of\ndamaged regions. To further boost the performance, we propose our TAMP\nsolution, namely Interactive Distribution Transition Estimation-driven\nDiffusion (InDiTE-Diff), which integrates InDiTE with SOTA diffusion model and\nconducts latent cross-reference during sampling. Moreover, considering the lack\nof benchmarks for TAMP task, we newly assembled a dataset, i.e., TAMP-Street,\nbased on existing image and mask datasets. We conduct experiments on the\nTAMP-Street datasets under two different time-variant image inpainting\nsettings, which show our method consistently outperform SOTA reference-guided\nimage inpainting methods for solving TAMP.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23461v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23461v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23808", "title": "Towards Initialization-free Calibrated Bundle Adjustment", "authors": ["Carl Olsson", "Amanda Nilsson"], "summary": "A recent series of works has shown that initialization-free BA can be\nachieved using pseudo Object Space Error (pOSE) as a surrogate objective. The\ninitial reconstruction-step optimizes an objective where all terms are\nprojectively invariant and it cannot incorporate knowledge of the camera\ncalibration. As a result, the solution is only determined up to a projective\ntransformation of the scene and the process requires more data for successful\nreconstruction.\n  In contrast, we present a method that is able to use the known camera\ncalibration thereby producing near metric solutions, that is, reconstructions\nthat are accurate up to a similarity transformation. To achieve this we\nintroduce pairwise relative rotation estimates that carry information about\ncamera calibration. These are only invariant to similarity transformations,\nthus encouraging solutions that preserve metric features of the real scene. Our\nmethod can be seen as integrating rotation averaging into the pOSE framework\nstriving towards initialization-free calibrated SfM.\n  Our experimental evaluation shows that we are able to reliably optimize our\nobjective, achieving convergence to the global minimum with high probability\nfrom random starting solutions, resulting in accurate near metric\nreconstructions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23808v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23808v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23429", "title": "DPOT: A DeepParticle method for Computation of Optimal Transport with convergence guarantee", "authors": ["Yingyuan Li", "Aokun Wang", "Zhongjian Wang"], "summary": "In this work, we propose a novel machine learning approach to compute the\noptimal transport map between two continuous distributions from their unpaired\nsamples, based on the DeepParticle methods. The proposed method leads to a\nmin-min optimization during training and does not impose any restriction on the\nnetwork structure. Theoretically we establish a weak convergence guarantee and\na quantitative error bound between the learned map and the optimal transport\nmap. Our numerical experiments validate the theoretical results and the\neffectiveness of the new approach, particularly on real-world tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23429v1", "categories": ["stat.ML", "cs.LG"], "cate": "stat.ML", "url": "http://arxiv.org/abs/2506.23429v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23462", "title": "Can We Predict the Unpredictable? Leveraging DisasterNet-LLM for Multimodal Disaster Classification", "authors": ["Manaswi Kulahara", "Gautam Siddharth Kashyap", "Nipun Joshi", "Arpita Soni"], "summary": "Effective disaster management requires timely and accurate insights, yet\ntraditional methods struggle to integrate multimodal data such as images,\nweather records, and textual reports. To address this, we propose\nDisasterNet-LLM, a specialized Large Language Model (LLM) designed for\ncomprehensive disaster analysis. By leveraging advanced pretraining,\ncross-modal attention mechanisms, and adaptive transformers, DisasterNet-LLM\nexcels in disaster classification. Experimental results demonstrate its\nsuperiority over state-of-the-art models, achieving higher accuracy of 89.5%,\nan F1 score of 88.0%, AUC of 0.92%, and BERTScore of 0.88% in multimodal\ndisaster classification tasks.", "comment": "Accepted in the 2025 IEEE International Geoscience and Remote Sensing\n  Symposium (IGARSS 2025), scheduled for 3 - 8 August 2025 in Brisbane,\n  Australia", "pdf_url": "http://arxiv.org/pdf/2506.23462v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.23462v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23810", "title": "MadCLIP: Few-shot Medical Anomaly Detection with CLIP", "authors": ["Mahshid Shiri", "Cigdem Beyan", "Vittorio Murino"], "summary": "An innovative few-shot anomaly detection approach is presented, leveraging\nthe pre-trained CLIP model for medical data, and adapting it for both\nimage-level anomaly classification (AC) and pixel-level anomaly segmentation\n(AS). A dual-branch design is proposed to separately capture normal and\nabnormal features through learnable adapters in the CLIP vision encoder. To\nimprove semantic alignment, learnable text prompts are employed to link visual\nfeatures. Furthermore, SigLIP loss is applied to effectively handle the\nmany-to-one relationship between images and unpaired text prompts, showcasing\nits adaptation in the medical field for the first time. Our approach is\nvalidated on multiple modalities, demonstrating superior performance over\nexisting methods for AC and AS, in both same-dataset and cross-dataset\nevaluations. Unlike prior work, it does not rely on synthetic data or memory\nbanks, and an ablation study confirms the contribution of each component. The\ncode is available at https://github.com/mahshid1998/MadCLIP.", "comment": "Accepted to MICCAI 2025 (this version is not peer-reviewed; it is the\n  submitted version). MICCAI proceedings DOI will appear here", "pdf_url": "http://arxiv.org/pdf/2506.23810v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23810v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23453", "title": "Minimax Optimal Two-Stage Algorithm For Moment Estimation Under Covariate Shift", "authors": ["Zhen Zhang", "Xin Liu", "Shaoli Wang", "Jiaye Teng"], "summary": "Covariate shift occurs when the distribution of input features differs\nbetween the training and testing phases. In covariate shift, estimating an\nunknown function's moment is a classical problem that remains under-explored,\ndespite its common occurrence in real-world scenarios. In this paper, we\ninvestigate the minimax lower bound of the problem when the source and target\ndistributions are known. To achieve the minimax optimal bound (up to a\nlogarithmic factor), we propose a two-stage algorithm. Specifically, it first\ntrains an optimal estimator for the function under the source distribution, and\nthen uses a likelihood ratio reweighting procedure to calibrate the moment\nestimator. In practice, the source and target distributions are typically\nunknown, and estimating the likelihood ratio may be unstable. To solve this\nproblem, we propose a truncated version of the estimator that ensures double\nrobustness and provide the corresponding upper bound. Extensive numerical\nstudies on synthetic examples confirm our theoretical findings and further\nillustrate the effectiveness of our proposed method.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23453v1", "categories": ["stat.ML", "cs.LG"], "cate": "stat.ML", "url": "http://arxiv.org/abs/2506.23453v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23465", "title": "Sanitizing Manufacturing Dataset Labels Using Vision-Language Models", "authors": ["Nazanin Mahjourian", "Vinh Nguyen"], "summary": "The success of machine learning models in industrial applications is heavily\ndependent on the quality of the datasets used to train the models. However,\nlarge-scale datasets, specially those constructed from crowd-sourcing and\nweb-scraping, often suffer from label noise, inconsistencies, and errors. This\nproblem is particularly pronounced in manufacturing domains, where obtaining\nhigh-quality labels is costly and time-consuming. This paper introduces\nVision-Language Sanitization and Refinement (VLSR), which is a\nvision-language-based framework for label sanitization and refinement in\nmulti-label manufacturing image datasets. This method embeds both images and\ntheir associated textual labels into a shared semantic space leveraging the\nCLIP vision-language model. Then two key tasks are addressed in this process by\ncomputing the cosine similarity between embeddings. First, label sanitization\nis performed to identify irrelevant, misspelled, or semantically weak labels,\nand surface the most semantically aligned label for each image by comparing\nimage-label pairs using cosine similarity between image and label embeddings.\nSecond, the method applies density-based clustering on text embeddings,\nfollowed by iterative cluster merging, to group semantically similar labels\ninto unified label groups. The Factorynet dataset, which includes noisy labels\nfrom both human annotations and web-scraped sources, is employed to evaluate\nthe effectiveness of the proposed framework. Experimental results demonstrate\nthat the VLSR framework successfully identifies problematic labels and improves\nlabel consistency. This method enables a significant reduction in label\nvocabulary through clustering, which ultimately enhances the dataset's quality\nfor training robust machine learning models in industrial applications with\nminimal human intervention.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23465v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23465v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23822", "title": "Interpretable Zero-Shot Learning with Locally-Aligned Vision-Language Model", "authors": ["Shiming Chen", "Bowen Duan", "Salman Khan", "Fahad Shahbaz Khan"], "summary": "Large-scale vision-language models (VLMs), such as CLIP, have achieved\nremarkable success in zero-shot learning (ZSL) by leveraging large-scale\nvisual-text pair datasets. However, these methods often lack interpretability,\nas they compute the similarity between an entire query image and the embedded\ncategory words, making it difficult to explain their predictions. One approach\nto address this issue is to develop interpretable models by integrating\nlanguage, where classifiers are built using discrete attributes, similar to\nhuman perception. This introduces a new challenge: how to effectively align\nlocal visual features with corresponding attributes based on pre-trained VLMs.\nTo tackle this, we propose LaZSL, a locally-aligned vision-language model for\ninterpretable ZSL. LaZSL employs local visual-semantic alignment via optimal\ntransport to perform interaction between visual regions and their associated\nattributes, facilitating effective alignment and providing interpretable\nsimilarity without the need for additional training. Extensive experiments\ndemonstrate that our method offers several advantages, including enhanced\ninterpretability, improved accuracy, and strong domain generalization. Codes\navailable at: https://github.com/shiming-chen/LaZSL.", "comment": "Accepted to ICCV'25", "pdf_url": "http://arxiv.org/pdf/2506.23822v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23822v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23456", "title": "Sampling and Identity-Testing Without Approximate Tensorization of Entropy", "authors": ["William Gay", "William He", "Nicholas Kocurek", "Ryan O'Donnell"], "summary": "Certain tasks in high-dimensional statistics become easier when the\nunderlying distribution satisfies a local-to-global property called approximate\ntensorization of entropy (ATE). For example, the Glauber dynamics Markov chain\nof an ATE distribution mixes fast and can produce approximate samples in a\nsmall amount of time, since such a distribution satisfies a modified\nlog-Sobolev inequality. Moreover, identity-testing for an ATE distribution\nrequires few samples if the tester is given coordinate conditional access to\nthe unknown distribution, as shown by Blanca, Chen, \\v{S}tefankovi\\v{c}, and\nVigoda (COLT 2023).\n  A natural class of distributions that do not satisfy ATE consists of mixtures\nof (few) distributions that do satisfy ATE. We study the complexity of\nidentity-testing and sampling for these distributions. Our main results are the\nfollowing:\n  1. We show fast mixing of Glauber dynamics from a data-based initialization,\nwith optimal sample complexity, for mixtures of distributions satisfying\nmodified log-Sobolev inequalities. This extends work of Huang, Koehler, Lee,\nMohanty, Rajaraman, Vuong, and Wu (STOC 2025, COLT 2025) for mixtures of\ndistributions satisfying Poincar\\'e inequalities.\n  2. Answering an open question posed by Blanca et al., we give efficient\nidentity-testers for mixtures of ATE distributions in the\ncoordinate-conditional sampling access model. We also give some simplifications\nand improvements to the original algorithm of Blanca et al.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23456v1", "categories": ["math.ST", "cs.DS", "cs.LG", "stat.ML", "stat.TH"], "cate": "math.ST", "url": "http://arxiv.org/abs/2506.23456v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23485", "title": "Thought-Augmented Planning for LLM-Powered Interactive Recommender Agent", "authors": ["Haocheng Yu", "Yaxiong Wu", "Hao Wang", "Wei Guo", "Yong Liu", "Yawen Li", "Yuyang Ye", "Junping Du", "Enhong Chen"], "summary": "Interactive recommendation is a typical information-seeking task that allows\nusers to interactively express their needs through natural language and obtain\npersonalized recommendations. Large language model-powered (LLM-powered) agents\nhave become a new paradigm in interactive recommendations, effectively\ncapturing users' real-time needs and enhancing personalized experiences.\nHowever, due to limited planning and generalization capabilities, existing\nformulations of LLM-powered interactive recommender agents struggle to\neffectively address diverse and complex user intents, such as intuitive,\nunrefined, or occasionally ambiguous requests. To tackle this challenge, we\npropose a novel thought-augmented interactive recommender agent system (TAIRA)\nthat addresses complex user intents through distilled thought patterns.\nSpecifically, TAIRA is designed as an LLM-powered multi-agent system featuring\na manager agent that orchestrates recommendation tasks by decomposing user\nneeds and planning subtasks, with its planning capacity strengthened through\nThought Pattern Distillation (TPD), a thought-augmentation method that extracts\nhigh-level thoughts from the agent's and human experts' experiences. Moreover,\nwe designed a set of user simulation schemes to generate personalized queries\nof different difficulties and evaluate the recommendations based on specific\ndatasets. Through comprehensive experiments conducted across multiple datasets,\nTAIRA exhibits significantly enhanced performance compared to existing methods.\nNotably, TAIRA shows a greater advantage on more challenging tasks while\ngeneralizing effectively on novel tasks, further validating its superiority in\nmanaging complex user intents within interactive recommendation systems. The\ncode is publicly available at:https://github.com/Alcein/TAIRA.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23485v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.23485v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23825", "title": "Flash-VStream: Efficient Real-Time Understanding for Long Video Streams", "authors": ["Haoji Zhang", "Yiqin Wang", "Yansong Tang", "Yong Liu", "Jiashi Feng", "Xiaojie Jin"], "summary": "Benefiting from the advances in large language models and cross-modal\nalignment, existing multimodal large language models have achieved prominent\nperformance in image and short video understanding. However, the understanding\nof long videos is still challenging, as their long-context nature results in\nsignificant computational and memory overhead. Most existing work treats long\nvideos in the same way as short videos, which is inefficient for real-world\napplications and hard to generalize to even longer videos. To address these\nissues, we propose Flash-VStream, an efficient video language model capable of\nprocessing extremely long videos and responding to user queries in real time.\nParticularly, we design a Flash Memory module, containing a low-capacity\ncontext memory to aggregate long-context temporal information and model the\ndistribution of information density, and a high-capacity augmentation memory to\nretrieve detailed spatial information based on this distribution. Compared to\nexisting models, Flash-VStream achieves significant reductions in inference\nlatency. Extensive experiments on long video benchmarks and comprehensive video\nbenchmarks, i.e., EgoSchema, MLVU, LVBench, MVBench and Video-MME, demonstrate\nthe state-of-the-art performance and outstanding efficiency of our method. Code\nis available at https://github.com/IVGSZ/Flash-VStream.", "comment": "Accepted by ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2506.23825v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23825v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23458", "title": "Neuro-Informed Joint Learning Enhances Cognitive Workload Decoding in Portable BCIs", "authors": ["Xiaoxiao Yang", "Chan Feng", "Jiancheng Chen"], "summary": "Portable and wearable consumer-grade electroencephalography (EEG) devices,\nlike Muse headbands, offer unprecedented mobility for daily brain-computer\ninterface (BCI) applications, including cognitive load detection. However, the\nexacerbated non-stationarity in portable EEG signals constrains data fidelity\nand decoding accuracy, creating a fundamental trade-off between portability and\nperformance. To mitigate such limitation, we propose MuseCogNet (Muse-based\nCognitive Network), a unified joint learning framework integrating\nself-supervised and supervised training paradigms. In particular, we introduce\nan EEG-grounded self-supervised reconstruction loss based on average pooling to\ncapture robust neurophysiological patterns, while cross-entropy loss refines\ntask-specific cognitive discriminants. This joint learning framework resembles\nthe bottom-up and top-down attention in humans, enabling MuseCogNet to\nsignificantly outperform state-of-the-art methods on a publicly available Muse\ndataset and establish an implementable pathway for neurocognitive monitoring in\necological settings.", "comment": "2 pages short paper", "pdf_url": "http://arxiv.org/pdf/2506.23458v1", "categories": ["cs.HC", "cs.LG"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.23458v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23490", "title": "UltraTwin: Towards Cardiac Anatomical Twin Generation from Multi-view 2D Ultrasound", "authors": ["Junxuan Yu", "Yaofei Duan", "Yuhao Huang", "Yu Wang", "Rongbo Ling", "Weihao Luo", "Ang Zhang", "Jingxian Xu", "Qiongying Ni", "Yongsong Zhou", "Binghan Li", "Haoran Dou", "Liping Liu", "Yanfen Chu", "Feng Geng", "Zhe Sheng", "Zhifeng Ding", "Dingxin Zhang", "Rui Huang", "Yuhang Zhang", "Xiaowei Xu", "Tao Tan", "Dong Ni", "Zhongshan Gou", "Xin Yang"], "summary": "Echocardiography is routine for cardiac examination. However, 2D ultrasound\n(US) struggles with accurate metric calculation and direct observation of 3D\ncardiac structures. Moreover, 3D US is limited by low resolution, small field\nof view and scarce availability in practice. Constructing the cardiac\nanatomical twin from 2D images is promising to provide precise treatment\nplanning and clinical quantification. However, it remains challenging due to\nthe rare paired data, complex structures, and US noises. In this study, we\nintroduce a novel generative framework UltraTwin, to obtain cardiac anatomical\ntwin from sparse multi-view 2D US. Our contribution is three-fold. First,\npioneered the construction of a real-world and high-quality dataset containing\nstrictly paired multi-view 2D US and CT, and pseudo-paired data. Second, we\npropose a coarse-to-fine scheme to achieve hierarchical reconstruction\noptimization. Last, we introduce an implicit autoencoder for topology-aware\nconstraints. Extensive experiments show that UltraTwin reconstructs\nhigh-quality anatomical twins versus strong competitors. We believe it advances\nanatomical twin modeling for potential applications in personalized cardiac\ncare.", "comment": "accepted by miccai 2025", "pdf_url": "http://arxiv.org/pdf/2506.23490v1", "categories": ["eess.IV", "cs.AI", "cs.CV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.23490v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23827", "title": "Spatially Gene Expression Prediction using Dual-Scale Contrastive Learning", "authors": ["Mingcheng Qu", "Yuncong Wu", "Donglin Di", "Yue Gao", "Tonghua Su", "Yang Song", "Lei Fan"], "summary": "Spatial transcriptomics (ST) provides crucial insights into tissue\nmicro-environments, but is limited to its high cost and complexity. As an\nalternative, predicting gene expression from pathology whole slide images (WSI)\nis gaining increasing attention. However, existing methods typically rely on\nsingle patches or a single pathology modality, neglecting the complex spatial\nand molecular interactions between target and neighboring information (e.g.,\ngene co-expression). This leads to a failure in establishing connections among\nadjacent regions and capturing intricate cross-modal relationships. To address\nthese issues, we propose NH2ST, a framework that integrates spatial context and\nboth pathology and gene modalities for gene expression prediction. Our model\ncomprises a query branch and a neighbor branch to process paired target patch\nand gene data and their neighboring regions, where cross-attention and\ncontrastive learning are employed to capture intrinsic associations and ensure\nalignments between pathology and gene expression. Extensive experiments on six\ndatasets demonstrate that our model consistently outperforms existing methods,\nachieving over 20% in PCC metrics. Codes are available at\nhttps://github.com/MCPathology/NH2ST", "comment": "Our paper has been accepted by MICCAI 2025", "pdf_url": "http://arxiv.org/pdf/2506.23827v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23827v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23467", "title": "AdFair-CLIP: Adversarial Fair Contrastive Language-Image Pre-training for Chest X-rays", "authors": ["Chenlang Yi", "Zizhan Xiong", "Qi Qi", "Xiyuan Wei", "Girish Bathla", "Ching-Long Lin", "Bobak Jack Mortazavi", "Tianbao Yang"], "summary": "Contrastive Language-Image Pre-training (CLIP) models have demonstrated\nsuperior performance across various visual tasks including medical image\nclassification. However, fairness concerns, including demographic biases, have\nreceived limited attention for CLIP models. This oversight leads to critical\nissues, particularly those related to race and gender, resulting in disparities\nin diagnostic outcomes and reduced reliability for underrepresented groups. To\naddress these challenges, we introduce AdFair-CLIP, a novel framework employing\nadversarial feature intervention to suppress sensitive attributes, thereby\nmitigating spurious correlations and improving prediction fairness. We conduct\ncomprehensive experiments on chest X-ray (CXR) datasets, and show that\nAdFair-CLIP significantly enhances both fairness and diagnostic accuracy, while\nmaintaining robust generalization in zero-shot and few-shot scenarios. These\nresults establish new benchmarks for fairness-aware learning in CLIP-based\nmedical diagnostic models, particularly for CXR analysis.", "comment": "This preprint has been accepted by MICCAI 2025", "pdf_url": "http://arxiv.org/pdf/2506.23467v1", "categories": ["cs.CV", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23467v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23491", "title": "Qwen-GUI-3B: A Lightweight Vision-Language Model for Cross-Resolution GUI Grounding", "authors": ["ZongHan Hsieh", "Tzer-Jen Wei"], "summary": "This paper introduces Qwen-GUI-3B, a lightweight Vision-Language Model (VLM)\nspecifically designed for Graphical User Interface grounding tasks, achieving\nperformance competitive with significantly larger models. Unlike large-scale\nVLMs (>7B parameters) that are computationally intensive and impractical for\nconsumer-grade hardware, Qwen-GUI-3B delivers strong grounding accuracy while\nbeing fully trainable on a single GPU (RTX 4090). The model incorporates\nseveral key innovations: (i) combine cross-platform, multi-resolution dataset\nof 24K examples from diverse sources including mobile, desktop, and web GUI\nscreenshots to effectively address data scarcity in high-resolution desktop\nenvironments; (ii) a two-stage fine-tuning strategy, where initial\ncross-platform training establishes robust GUI understanding, followed by\nspecialized fine-tuning on high-resolution data to significantly enhance model\nadaptability; and (iii) data curation and redundancy reduction strategies,\ndemonstrating that randomly sampling a smaller subset with reduced redundancy\nachieves performance comparable to larger datasets, emphasizing data diversity\nover sheer volume. Empirical evaluation on standard GUI grounding\nbenchmarks-including ScreenSpot, ScreenSpot-v2, and the challenging\nScreenSpot-Pro, highlights Qwen-GUI-3B's exceptional accuracy, achieving 84.9%\non ScreenSpot and 86.4% on ScreenSpot-v2, surpassing prior models under 4B\nparameters. Ablation studies validate the critical role of balanced sampling\nand two-stage fine-tuning in enhancing robustness, particularly in\nhigh-resolution desktop scenarios. The Qwen-GUI-3B is available at:\nhttps://github.com/Han1018/Qwen-GUI-3B", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23491v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23491v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23832", "title": "Low-latency vision transformers via large-scale multi-head attention", "authors": ["Ronit D. Gross", "Tal Halevi", "Ella Koresh", "Yarden Tzach", "Ido Kanter"], "summary": "The emergence of spontaneous symmetry breaking among a few heads of\nmulti-head attention (MHA) across transformer blocks in classification tasks\nwas recently demonstrated through the quantification of single-nodal\nperformance (SNP). This finding indicates that each head focuses its attention\non a subset of labels through cooperation among its SNPs. This underlying\nlearning mechanism is generalized to large-scale MHA (LS-MHA) using a single\nmatrix value representing single-head performance (SHP), analogous to\nsingle-filter performance in convolutional neural networks (CNNs). The results\nindicate that each SHP matrix comprises multiple unit clusters such that each\nlabel being explicitly recognized by a few heads with negligible noise. This\nleads to an increased signal-to-noise ratio (SNR) along the transformer blocks,\nthereby improving classification accuracy. These features give rise to several\ndistinct vision transformer (ViT) architectures that achieve the same accuracy\nbut differ in their LS-MHA structures. As a result, their soft committee yields\nsuperior accuracy, an outcome not typically observed in CNNs which rely on\nhundreds of filters. In addition, a significant reduction in latency is\nachieved without affecting the accuracy by replacing the initial transformer\nblocks with convolutional layers. This substitution accelerates early-stage\nlearning, which is then improved by subsequent transformer layers. The\nextension of this learning mechanism to natural language processing tasks,\nbased on quantitative differences between CNNs and ViT architectures, has the\npotential to yield new insights in deep learning. The findings are demonstrated\nusing compact convolutional transformer architectures trained on the CIFAR-100\ndataset.", "comment": "23 pages, 4 figures, 7 tables", "pdf_url": "http://arxiv.org/pdf/2506.23832v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23832v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23487", "title": "Test of partial effects for Frechet regression on Bures-Wasserstein manifolds", "authors": ["Haoshu Xu", "Hongzhe Li"], "summary": "We propose a novel test for assessing partial effects in Frechet regression\non Bures Wasserstein manifolds. Our approach employs a sample splitting\nstrategy: the first subsample is used to fit the Frechet regression model,\nyielding estimates of the covariance matrices and their associated optimal\ntransport maps, while the second subsample is used to construct the test\nstatistic. We prove that this statistic converges in distribution to a weighted\nmixture of chi squared components, where the weights correspond to the\neigenvalues of an integral operator defined by an appropriate RKHS kernel. We\nestablish that our procedure achieves the nominal asymptotic size and\ndemonstrate that its worst-case power converges uniformly to one. Through\nextensive simulations and a real data application, we illustrate the test's\nfinite-sample accuracy and practical utility.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23487v1", "categories": ["stat.ML", "cs.LG"], "cate": "stat.ML", "url": "http://arxiv.org/abs/2506.23487v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23492", "title": "Sample Margin-Aware Recalibration of Temperature Scaling", "authors": ["Haolan Guo", "Linwei Tao", "Haoyang Luo", "Minjing Dong", "Chang Xu"], "summary": "Recent advances in deep learning have significantly improved predictive\naccuracy. However, modern neural networks remain systematically overconfident,\nposing risks for deployment in safety-critical scenarios. Current post-hoc\ncalibration methods face a fundamental dilemma: global approaches like\nTemperature Scaling apply uniform adjustments across all samples, introducing\nhigh bias despite computational efficiency, while more expressive methods that\noperate on full logit distributions suffer from high variance due to noisy\nhigh-dimensional inputs and insufficient validation data. To address these\nchallenges, we propose Sample Margin-Aware Recalibration of Temperature\n(SMART), a lightweight, data-efficient recalibration method that precisely\nscales logits based on the margin between the top two logits -- termed the\nlogit gap. Specifically, the logit gap serves as a denoised, scalar signal\ndirectly tied to decision boundary uncertainty, providing a robust indicator\nthat avoids the noise inherent in high-dimensional logit spaces while\npreserving model prediction invariance. Meanwhile, SMART employs a novel\nsoft-binned Expected Calibration Error (SoftECE) objective that balances model\nbias and variance through adaptive binning, enabling stable parameter updates\neven with extremely limited calibration data. Extensive evaluations across\ndiverse datasets and architectures demonstrate that SMART achieves\nstate-of-the-art calibration performance even with substantially fewer\nparameters compared to existing parametric methods, offering a principled,\nrobust, and highly efficient solution for practical uncertainty quantification\nin neural network predictions. The source code is available at:\nhttps://anonymous.4open.science/r/SMART-8B11.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23492v1", "categories": ["cs.LG", "cs.AI", "cs.CV"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.23492v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23833", "title": "PointSSIM: A novel low dimensional resolution invariant image-to-image comparison metric", "authors": ["Oscar Ovanger", "Ragnar Hauge", "Jacob Skauvold", "Michael J. Pyrcz", "Jo Eidsvik"], "summary": "This paper presents PointSSIM, a novel low-dimensional image-to-image\ncomparison metric that is resolution invariant. Drawing inspiration from the\nstructural similarity index measure and mathematical morphology, PointSSIM\nenables robust comparison across binary images of varying resolutions by\ntransforming them into marked point pattern representations. The key features\nof the image, referred to as anchor points, are extracted from binary images by\nidentifying locally adaptive maxima from the minimal distance transform. Image\ncomparisons are then performed using a summary vector, capturing intensity,\nconnectivity, complexity, and structural attributes. Results show that this\napproach provides an efficient and reliable method for image comparison,\nparticularly suited to applications requiring structural analysis across\ndifferent resolutions.", "comment": "13 pages, 20 figures", "pdf_url": "http://arxiv.org/pdf/2506.23833v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23833v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23529", "title": "When Test-Time Adaptation Meets Self-Supervised Models", "authors": ["Jisu Han", "Jihee Park", "Dongyoon Han", "Wonjun Hwang"], "summary": "Training on test-time data enables deep learning models to adapt to dynamic\nenvironmental changes, enhancing their practical applicability. Online\nadaptation from source to target domains is promising but it remains highly\nreliant on the performance of source pretrained model. In this paper, we\ninvestigate whether test-time adaptation (TTA) methods can continuously improve\nmodels trained via self-supervised learning (SSL) without relying on source\npretraining. We introduce a self-supervised TTA protocol after observing that\nexisting TTA approaches struggle when directly applied to self-supervised\nmodels with low accuracy on the source domain. Furthermore, we propose a\ncollaborative learning framework that integrates SSL and TTA models, leveraging\ncontrastive learning and knowledge distillation for stepwise representation\nrefinement. We validate our method on diverse self-supervised models, including\nDINO, MoCo, and iBOT, across TTA benchmarks. Extensive experiments validate the\neffectiveness of our approach in SSL, showing that it achieves competitive\nperformance even without source pretraining.", "comment": "15 pages, 7 figures", "pdf_url": "http://arxiv.org/pdf/2506.23529v1", "categories": ["cs.CV", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23529v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23506", "title": "Artificial Intelligence-assisted Pixel-level Lung (APL) Scoring for Fast and Accurate Quantification in Ultra-short Echo-time MRI", "authors": ["Bowen Xin", "Rohan Hickey", "Tamara Blake", "Jin Jin", "Claire E Wainwright", "Thomas Benkert", "Alto Stemmer", "Peter Sly", "David Coman", "Jason Dowling"], "summary": "Lung magnetic resonance imaging (MRI) with ultrashort echo-time (UTE)\nrepresents a recent breakthrough in lung structure imaging, providing image\nresolution and quality comparable to computed tomography (CT). Due to the\nabsence of ionising radiation, MRI is often preferred over CT in paediatric\ndiseases such as cystic fibrosis (CF), one of the most common genetic disorders\nin Caucasians. To assess structural lung damage in CF imaging, CT scoring\nsystems provide valuable quantitative insights for disease diagnosis and\nprogression. However, few quantitative scoring systems are available in\nstructural lung MRI (e.g., UTE-MRI). To provide fast and accurate\nquantification in lung MRI, we investigated the feasibility of novel Artificial\nintelligence-assisted Pixel-level Lung (APL) scoring for CF. APL scoring\nconsists of 5 stages, including 1) image loading, 2) AI lung segmentation, 3)\nlung-bounded slice sampling, 4) pixel-level annotation, and 5) quantification\nand reporting. The results shows that our APL scoring took 8.2 minutes per\nsubject, which was more than twice as fast as the previous grid-level scoring.\nAdditionally, our pixel-level scoring was statistically more accurate\n(p=0.021), while strongly correlating with grid-level scoring (R=0.973,\np=5.85e-9). This tool has great potential to streamline the workflow of UTE\nlung MRI in clinical settings, and be extended to other structural lung MRI\nsequences (e.g., BLADE MRI), and for other lung diseases (e.g.,\nbronchopulmonary dysplasia).", "comment": "Oral presentation in ISMRM2025", "pdf_url": "http://arxiv.org/pdf/2506.23506v1", "categories": ["eess.IV", "cs.AI", "cs.CV", "physics.med-ph"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.23506v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23835", "title": "Refine Any Object in Any Scene", "authors": ["Ziwei Chen", "Ziling Liu", "Zitong Huang", "Mingqi Gao", "Feng Zheng"], "summary": "Viewpoint missing of objects is common in scene reconstruction, as camera\npaths typically prioritize capturing the overall scene structure rather than\nindividual objects. This makes it highly challenging to achieve high-fidelity\nobject-level modeling while maintaining accurate scene-level representation.\nAddressing this issue is critical for advancing downstream tasks requiring\ndetailed object understanding and appearance modeling. In this paper, we\nintroduce Refine Any object In any ScenE (RAISE), a novel 3D enhancement\nframework that leverages 3D generative priors to recover fine-grained object\ngeometry and appearance under missing views. Starting from substituting\ndegraded objects with proxies, via a 3D generative model with strong 3D\nunderstanding, RAISE progressively refines geometry and texture by aligning\neach proxy to its degraded counterpart in 7-DOF pose, followed by correcting\nspatial and appearance inconsistencies via registration-constrained\nenhancement. This two-stage refinement ensures the high-fidelity geometry and\nappearance of the original object in unseen views while maintaining consistency\nin spatial positioning, observed geometry, and appearance. Extensive\nexperiments on challenging benchmarks show that RAISE significantly outperforms\nstate-of-the-art methods in both novel view synthesis and geometry completion\ntasks. RAISE is made publicly available at https://github.com/PolySummit/RAISE.", "comment": "9 pages with 6 figures", "pdf_url": "http://arxiv.org/pdf/2506.23835v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23835v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23532", "title": "GViT: Representing Images as Gaussians for Visual Recognition", "authors": ["Jefferson Hernandez", "Ruozhen He", "Guha Balakrishnan", "Alexander C. Berg", "Vicente Ordonez"], "summary": "We introduce GVIT, a classification framework that abandons conventional\npixel or patch grid input representations in favor of a compact set of\nlearnable 2D Gaussians. Each image is encoded as a few hundred Gaussians whose\npositions, scales, orientations, colors, and opacities are optimized jointly\nwith a ViT classifier trained on top of these representations. We reuse the\nclassifier gradients as constructive guidance, steering the Gaussians toward\nclass-salient regions while a differentiable renderer optimizes an image\nreconstruction loss. We demonstrate that by 2D Gaussian input representations\ncoupled with our GVIT guidance, using a relatively standard ViT architecture,\nclosely matches the performance of a traditional patch-based ViT, reaching a\n76.9% top-1 accuracy on Imagenet-1k using a ViT-B architecture.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23532v1", "categories": ["cs.CV", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23532v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23508", "title": "Reinforcement Fine-Tuning Enables MLLMs Learning Novel Tasks Stably", "authors": ["Zhihao Zhang", "Qiaole Dong", "Qi Zhang", "Jun Zhao", "Enyu Zhou", "Zhiheng Xi", "Senjie Jin", "Xiaoran Fan", "Yuhao Zhou", "Yanwei Fu", "Tao Ji", "Tao Gui", "Xuanjing Huang"], "summary": "Post-training algorithms such as Supervised Fine-Tuning (SFT) and\nReinforcement Fine-Tuning (RFT) are widely used to adapt multimodal large\nlanguage models to downstream tasks. While effective at task adaptation, their\nimpact on prior knowledge remains unclear. In this paper, we introduce jigsaw\npuzzles as a novel task absent from existing pretraining corpora and\nsystematically study the behavior of SFT and RFT on an open-source multimodal\nmodel, Qwen2.5-VL. Our experiments reveal a sharp trade-off: SFT enables rapid\ntask acquisition but leads to catastrophic forgetting, whereas RFT learns more\nslowly on novel tasks but maintains prior knowledge. We analyze this phenomenon\nthrough the lens of learning dynamics, showing that RFT reinforces correct\nsamples that are naturally aligned with the base model's probability landscape,\nmitigating interference with prior knowledge. Moreover, supervised training on\ncorrect RFT-simulated rollouts allows SFT to preserve knowledge while rapidly\nlearning new tasks. These findings suggest that data distribution, rather than\nalgorithmic differences, plays a central role in forgetting, and highlight\nRFT's potential for stable continual learning in multimodal large language\nmodels.", "comment": "18 pages (Preprint. Work in progress)", "pdf_url": "http://arxiv.org/pdf/2506.23508v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.23508v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23852", "title": "RGC-VQA: An Exploration Database for Robotic-Generated Video Quality Assessment", "authors": ["Jianing Jin", "Jiangyong Ying", "Huiyu Duan", "Liu Yang", "Sijing Wu", "Yunhao Li", "Yushuo Zheng", "Xiongkuo Min", "Guangtao Zhai"], "summary": "As camera-equipped robotic platforms become increasingly integrated into\ndaily life, robotic-generated videos have begun to appear on streaming media\nplatforms, enabling us to envision a future where humans and robots coexist. We\ninnovatively propose the concept of Robotic-Generated Content (RGC) to term\nthese videos generated from egocentric perspective of robots. The perceptual\nquality of RGC videos is critical in human-robot interaction scenarios, and RGC\nvideos exhibit unique distortions and visual requirements that differ markedly\nfrom those of professionally-generated content (PGC) videos and user-generated\ncontent (UGC) videos. However, dedicated research on quality assessment of RGC\nvideos is still lacking. To address this gap and to support broader robotic\napplications, we establish the first Robotic-Generated Content Database (RGCD),\nwhich contains a total of 2,100 videos drawn from three robot categories and\nsourced from diverse platforms. A subjective VQA experiment is conducted\nsubsequently to assess human visual perception of robotic-generated videos.\nFinally, we conduct a benchmark experiment to evaluate the performance of 11\nstate-of-the-art VQA models on our database. Experimental results reveal\nsignificant limitations in existing VQA models when applied to complex,\nrobotic-generated content, highlighting a critical need for RGC-specific VQA\nmodels. Our RGCD is publicly available at:\nhttps://github.com/IntMeGroup/RGC-VQA.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23852v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23852v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23538", "title": "Uncertainty-aware Diffusion and Reinforcement Learning for Joint Plane Localization and Anomaly Diagnosis in 3D Ultrasound", "authors": ["Yuhao Huang", "Yueyue Xu", "Haoran Dou", "Jiaxiao Deng", "Xin Yang", "Hongyu Zheng", "Dong Ni"], "summary": "Congenital uterine anomalies (CUAs) can lead to infertility, miscarriage,\npreterm birth, and an increased risk of pregnancy complications. Compared to\ntraditional 2D ultrasound (US), 3D US can reconstruct the coronal plane,\nproviding a clear visualization of the uterine morphology for assessing CUAs\naccurately. In this paper, we propose an intelligent system for simultaneous\nautomated plane localization and CUA diagnosis. Our highlights are: 1) we\ndevelop a denoising diffusion model with local (plane) and global (volume/text)\nguidance, using an adaptive weighting strategy to optimize attention allocation\nto different conditions; 2) we introduce a reinforcement learning-based\nframework with unsupervised rewards to extract the key slice summary from\nredundant sequences, fully integrating information across multiple planes to\nreduce learning difficulty; 3) we provide text-driven uncertainty modeling for\ncoarse prediction, and leverage it to adjust the classification probability for\noverall performance improvement. Extensive experiments on a large 3D uterine US\ndataset show the efficacy of our method, in terms of plane localization and CUA\ndiagnosis. Code is available at https://github.com/yuhoo0302/CUA-US.", "comment": "Accepted by MICCAI 2025;10 pages, 3 figures", "pdf_url": "http://arxiv.org/pdf/2506.23538v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23538v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23514", "title": "MGPRL: Distributed Multi-Gaussian Processes for Wi-Fi-based Multi-Robot Relative Localization in Large Indoor Environments", "authors": ["Sai Krishna Ghanta", "Ramviyas Parasuraman"], "summary": "Relative localization is a crucial capability for multi-robot systems\noperating in GPS-denied environments. Existing approaches for multi-robot\nrelative localization often depend on costly or short-range sensors like\ncameras and LiDARs. Consequently, these approaches face challenges such as high\ncomputational overhead (e.g., map merging) and difficulties in disjoint\nenvironments. To address this limitation, this paper introduces MGPRL, a novel\ndistributed framework for multi-robot relative localization using convex-hull\nof multiple Wi-Fi access points (AP). To accomplish this, we employ\nco-regionalized multi-output Gaussian Processes for efficient Radio Signal\nStrength Indicator (RSSI) field prediction and perform uncertainty-aware\nmulti-AP localization, which is further coupled with weighted convex hull-based\nalignment for robust relative pose estimation. Each robot predicts the RSSI\nfield of the environment by an online scan of APs in its environment, which are\nutilized for position estimation of multiple APs. To perform relative\nlocalization, each robot aligns the convex hull of its predicted AP locations\nwith that of the neighbor robots. This approach is well-suited for devices with\nlimited computational resources and operates solely on widely available Wi-Fi\nRSSI measurements without necessitating any dedicated pre-calibration or\noffline fingerprinting. We rigorously evaluate the performance of the proposed\nMGPRL in ROS simulations and demonstrate it with real-world experiments,\ncomparing it against multiple state-of-the-art approaches. The results showcase\nthat MGPRL outperforms existing methods in terms of localization accuracy and\ncomputational efficiency. Finally, we open source MGPRL as a ROS package\nhttps://github.com/herolab-uga/MGPRL.", "comment": "Accepted to IROS 2025", "pdf_url": "http://arxiv.org/pdf/2506.23514v1", "categories": ["cs.RO", "cs.AI", "cs.MA"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.23514v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23854", "title": "HiNeuS: High-fidelity Neural Surface Mitigating Low-texture and Reflective Ambiguity", "authors": ["Yida Wang", "Xueyang Zhang", "Kun Zhan", "Peng Jia", "Xianpeng Lang"], "summary": "Neural surface reconstruction faces persistent challenges in reconciling\ngeometric fidelity with photometric consistency under complex scene conditions.\nWe present HiNeuS, a unified framework that holistically addresses three core\nlimitations in existing approaches: multi-view radiance inconsistency, missing\nkeypoints in textureless regions, and structural degradation from over-enforced\nEikonal constraints during joint optimization. To resolve these issues through\na unified pipeline, we introduce: 1) Differential visibility verification\nthrough SDF-guided ray tracing, resolving reflection ambiguities via continuous\nocclusion modeling; 2) Planar-conformal regularization via ray-aligned geometry\npatches that enforce local surface coherence while preserving sharp edges\nthrough adaptive appearance weighting; and 3) Physically-grounded Eikonal\nrelaxation that dynamically modulates geometric constraints based on local\nradiance gradients, enabling detail preservation without sacrificing global\nregularity. Unlike prior methods that handle these aspects through sequential\noptimizations or isolated modules, our approach achieves cohesive integration\nwhere appearance-geometry constraints evolve synergistically throughout\ntraining. Comprehensive evaluations across synthetic and real-world datasets\ndemonstrate state-of-the-art performance, including a 21.4% reduction in\nChamfer distance over reflection-aware baselines and 2.32 dB PSNR improvement\nagainst neural rendering counterparts. Qualitative analyses reveal superior\ncapability in recovering specular instruments, urban layouts with\ncentimeter-scale infrastructure, and low-textured surfaces without local patch\ncollapse. The method's generalizability is further validated through successful\napplication to inverse rendering tasks, including material decomposition and\nview-consistent relighting.", "comment": "Published in International Conference on Computer Vision (ICCV) 2025", "pdf_url": "http://arxiv.org/pdf/2506.23854v1", "categories": ["cs.CV", "cs.GR"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23854v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23546", "title": "Neural Langevin Machine: a local asymmetric learning rule can be creative", "authors": ["Zhendong Yu", "Weizhong Huang", "Haiping Huang"], "summary": "Fixed points of recurrent neural networks can be leveraged to store and\ngenerate information. These fixed points can be captured by the Boltzmann-Gibbs\nmeasure, which leads to neural Langevin dynamics that can be used for sampling\nand learning a real dataset. We call this type of generative model neural\nLangevin machine, which is interpretable due to its analytic form of\ndistribution and is simple to train. Moreover, the learning process is derived\nas a local asymmetric plasticity rule, bearing biological relevance. Therefore,\none can realize a continuous sampling of creative dynamics in a neural network,\nmimicking an imagination process in brain circuits. This neural Langevin\nmachine may be another promising generative model, at least in its strength in\ncircuit-based sampling and biologically plausible learning rule.", "comment": "15 pages, 3 figures, with Github link in the paper", "pdf_url": "http://arxiv.org/pdf/2506.23546v1", "categories": ["q-bio.NC", "cond-mat.dis-nn", "cs.LG", "cs.NE"], "cate": "q-bio.NC", "url": "http://arxiv.org/abs/2506.23546v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23516", "title": "FedWSQ: Efficient Federated Learning with Weight Standardization and Distribution-Aware Non-Uniform Quantization", "authors": ["Seung-Wook Kim", "Seongyeol Kim", "Jiah Kim", "Seowon Ji", "Se-Ho Lee"], "summary": "Federated learning (FL) often suffers from performance degradation due to key\nchallenges such as data heterogeneity and communication constraints. To address\nthese limitations, we present a novel FL framework called FedWSQ, which\nintegrates weight standardization (WS) and the proposed distribution-aware\nnon-uniform quantization (DANUQ). WS enhances FL performance by filtering out\nbiased components in local updates during training, thereby improving the\nrobustness of the model against data heterogeneity and unstable client\nparticipation. In addition, DANUQ minimizes quantization errors by leveraging\nthe statistical properties of local model updates. As a result, FedWSQ\nsignificantly reduces communication overhead while maintaining superior model\naccuracy. Extensive experiments on FL benchmark datasets demonstrate that\nFedWSQ consistently outperforms existing FL methods across various challenging\nFL settings, including extreme data heterogeneity and ultra-low-bit\ncommunication scenarios.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23516v1", "categories": ["cs.LG", "cs.AI", "cs.CV"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.23516v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23856", "title": "A Closer Look at Conditional Prompt Tuning for Vision-Language Models", "authors": ["Ji Zhang", "Shihan Wu", "Lianli Gao", "Jingkuan Song", "Nicu Sebe", "Heng Tao Shen"], "summary": "Despite the great promise of Prompt Tuning (PT) in adapting large\nVision-Language Pretrained Models (VLPMs) to downstream tasks, they often\nstruggle to overcome the Base-New Tradeoff (BNT) dilemma: as VLPMs are better\ntuned to a base task, their ability to generalize to new tasks diminishes.\nRecent work on conditional PT addresses this problem by replacing static\nprompts with dynamic Visual Image Information (VII)-conditioned prompts,\nimproving the model's generalization to new tasks to some extent. In this work,\nwe first identify a critical issue with existing conditional PT methods: using\nVII as the \"condition\" of prompts yields suboptimal performance, and even\nrandom noise-conditioned prompts can outperform the VII-conditioned\ncounterparts. On further analysis, we find that learning dynamic prompts\nconditioned on Textual Class Information (TCI) is the key to solving the BNT\nproblem. Motivated by this, we then propose Class-adaptive Prompt Tuning\n(CaPT), which enables fast adaptation of tuned models to new classes by\nlearning TCI-conditioned prompts from base classes. Remarkably, CaPT can be\nused as a plugin to mitigate the BNT problem for existing unconditional PT\nschemes. Extensive experiments on 11 datasets show that CaPT consistently\nimproves the performance of five strong unconditional PT baselines with\nnegligible additional computational cost. Additionally, by integrating CaPT\nwith our recently proposed DePT framework, we devise a new conditional PT\napproach, termed DeCaPT, which outperforms the H ACC of the state-of-the-art\nconditional PT scheme by 3.49%, averaged over the 11 datasets. Code:\nhttps://github.com/Koorye/CaPT.", "comment": "18 pages", "pdf_url": "http://arxiv.org/pdf/2506.23856v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23856v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23549", "title": "CooT: Learning to Coordinate In-Context with Coordination Transformers", "authors": ["Huai-Chih Wang", "Hsiang-Chun Chuang", "Hsi-Chun Cheng", "Dai-Jie Wu", "Shao-Hua Sun"], "summary": "Effective coordination among artificial agents in dynamic and uncertain\nenvironments remains a significant challenge in multi-agent systems. Existing\napproaches, such as self-play and population-based methods, either generalize\npoorly to unseen partners or require extensive training. To overcome these\nlimitations, we propose Coordination Transformers (CooT), a novel in-context\ncoordination framework that uses recent interaction histories to adapt to\nunseen partners rapidly. Unlike previous approaches that primarily aim to\nincrease the diversity of training partners, CooT explicitly focuses on\nadapting to new partner behaviors by predicting actions aligned with observed\npartner interactions. Trained on interaction trajectories collected from\ndiverse pairs of agents with complementary behaviors, CooT quickly learns\neffective coordination strategies without explicit supervision or fine-tuning.\nEvaluations on the Overcooked benchmark demonstrate that CooT significantly\noutperforms baseline methods in coordination tasks involving previously unseen\npartners. Human evaluations further confirm CooT as the most effective\ncollaborative partner, while extensive ablations highlight its robustness,\nflexibility, and sensitivity to context in multi-agent scenarios.", "comment": "23 pages, 10 tables, 8 figures", "pdf_url": "http://arxiv.org/pdf/2506.23549v1", "categories": ["cs.AI", "cs.HC", "cs.LG"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.23549v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23524", "title": "NEU-ESC: A Comprehensive Vietnamese dataset for Educational Sentiment analysis and topic Classification toward multitask learning", "authors": ["Phan Quoc Hung Mai", "Quang Hung Nguyen", "Phuong Giang Duong", "Hong Hanh Nguyen", "Nguyen Tuan Long"], "summary": "In the field of education, understanding students' opinions through their\ncomments is crucial, especially in the Vietnamese language, where resources\nremain limited. Existing educational datasets often lack domain relevance and\nstudent slang. To address these gaps, we introduce NEU-ESC, a new Vietnamese\ndataset for Educational Sentiment Classification and Topic Classification,\ncurated from university forums, which offers more samples, richer class\ndiversity, longer texts, and broader vocabulary. In addition, we explore\nmultitask learning using encoder-only language models (BERT), in which we\nshowed that it achieves performance up to 83.7% and 79.8% accuracy for\nsentiment and topic classification tasks. We also benchmark our dataset and\nmodel with other datasets and models, including Large Language Models, and\ndiscuss these benchmarks. The dataset is publicly available at:\nhttps://huggingface.co/datasets/hung20gg/NEU-ESC.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23524v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.23524v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23858", "title": "VMoBA: Mixture-of-Block Attention for Video Diffusion Models", "authors": ["Jianzong Wu", "Liang Hou", "Haotian Yang", "Xin Tao", "Ye Tian", "Pengfei Wan", "Di Zhang", "Yunhai Tong"], "summary": "The quadratic complexity of full attention mechanisms poses a significant\nbottleneck for Video Diffusion Models (VDMs) aiming to generate long-duration,\nhigh-resolution videos. While various sparse attention methods have been\nproposed, many are designed as training-free inference accelerators or do not\noptimally capture the unique spatio-temporal characteristics inherent in video\ndata when trained natively. This paper introduces Video Mixture of Block\nAttention (VMoBA), a novel sparse attention mechanism specifically adapted for\nVDMs. Motivated by an in-depth analysis of attention patterns within\npre-trained video transformers, which revealed strong spatio-temporal locality,\nvarying query importance, and head-specific concentration levels, VMoBA\nenhances the original MoBA framework with three key modifications: (1) a\nlayer-wise recurrent block partition scheme (1D-2D-3D) to dynamically adapt to\ndiverse spatio-temporal attention patterns and improve efficiency; (2) global\nblock selection to prioritize the most salient query-key block interactions\nacross an entire attention head; and (3) threshold-based block selection to\ndynamically determine the number of attended blocks based on their cumulative\nsimilarity. Extensive experiments demonstrate that VMoBA significantly\naccelerates the training of VDMs on longer sequences, achieving 2.92x FLOPs and\n1.48x latency speedup, while attaining comparable or even superior generation\nquality to full attention. Furthermore, VMoBA exhibits competitive performance\nin training-free inference, offering 2.40x FLOPs and 1.35x latency speedup for\nhigh-res video generation.", "comment": "Code is at https://github.com/KwaiVGI/VMoBA", "pdf_url": "http://arxiv.org/pdf/2506.23858v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23858v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23550", "title": "Seeding neural network quantum states with tensor network states", "authors": ["Ryui Kaneko", "Shimpei Goto"], "summary": "We find an efficient approach to approximately convert matrix product states\n(MPSs) into restricted Boltzmann machine wave functions consisting of a\nmultinomial hidden unit through a canonical polyadic (CP) decomposition of the\nMPSs. This method allows us to generate well-behaved initial neural network\nquantum states for quantum many-body ground-state calculations in polynomial\ntime of the number of variational parameters and systematically shorten the\ndistance between the initial states and the ground states with increasing the\nrank of the CP decomposition. We demonstrate the efficiency of our method by\ntaking the transverse-field Ising model as an example and discuss possible\napplications of our method to more general quantum many-body systems in which\nthe ground-state wave functions possess complex nodal structures.", "comment": "13 pages, 13 figures", "pdf_url": "http://arxiv.org/pdf/2506.23550v1", "categories": ["cond-mat.str-el", "cs.LG", "cs.NA", "math.NA", "quant-ph"], "cate": "cond-mat.str-el", "url": "http://arxiv.org/abs/2506.23550v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23538", "title": "Uncertainty-aware Diffusion and Reinforcement Learning for Joint Plane Localization and Anomaly Diagnosis in 3D Ultrasound", "authors": ["Yuhao Huang", "Yueyue Xu", "Haoran Dou", "Jiaxiao Deng", "Xin Yang", "Hongyu Zheng", "Dong Ni"], "summary": "Congenital uterine anomalies (CUAs) can lead to infertility, miscarriage,\npreterm birth, and an increased risk of pregnancy complications. Compared to\ntraditional 2D ultrasound (US), 3D US can reconstruct the coronal plane,\nproviding a clear visualization of the uterine morphology for assessing CUAs\naccurately. In this paper, we propose an intelligent system for simultaneous\nautomated plane localization and CUA diagnosis. Our highlights are: 1) we\ndevelop a denoising diffusion model with local (plane) and global (volume/text)\nguidance, using an adaptive weighting strategy to optimize attention allocation\nto different conditions; 2) we introduce a reinforcement learning-based\nframework with unsupervised rewards to extract the key slice summary from\nredundant sequences, fully integrating information across multiple planes to\nreduce learning difficulty; 3) we provide text-driven uncertainty modeling for\ncoarse prediction, and leverage it to adjust the classification probability for\noverall performance improvement. Extensive experiments on a large 3D uterine US\ndataset show the efficacy of our method, in terms of plane localization and CUA\ndiagnosis. Code is available at https://github.com/yuhoo0302/CUA-US.", "comment": "Accepted by MICCAI 2025;10 pages, 3 figures", "pdf_url": "http://arxiv.org/pdf/2506.23538v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23538v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23863", "title": "Puzzles: Unbounded Video-Depth Augmentation for Scalable End-to-End 3D Reconstruction", "authors": ["Jiahao Ma", "Lei Wang", "Miaomiao liu", "David Ahmedt-Aristizabal", "Chuong Nguyen"], "summary": "Multi-view 3D reconstruction remains a core challenge in computer vision.\nRecent methods, such as DUST3R and its successors, directly regress pointmaps\nfrom image pairs without relying on known scene geometry or camera parameters.\nHowever, the performance of these models is constrained by the diversity and\nscale of available training data. In this work, we introduce Puzzles, a data\naugmentation strategy that synthesizes an unbounded volume of high-quality\nposed video-depth data from a single image or video clip. By simulating diverse\ncamera trajectories and realistic scene geometry through targeted image\ntransformations, Puzzles significantly enhances data variety. Extensive\nexperiments show that integrating Puzzles into existing video-based 3D\nreconstruction pipelines consistently boosts performance without modifying the\nunderlying network architecture. Notably, models trained on only ten percent of\nthe original data augmented with Puzzles still achieve accuracy comparable to\nthose trained on the full dataset. Code is available at\nhttps://jiahao-ma.github.io/puzzles/.", "comment": "Feed-forward 3D reconstruction, Data Augmentation", "pdf_url": "http://arxiv.org/pdf/2506.23863v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23863v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23566", "title": "Metadata, Wavelet, and Time Aware Diffusion Models for Satellite Image Super Resolution", "authors": ["Luigi Sigillo", "Renato Giamba", "Danilo Comminiello"], "summary": "The acquisition of high-resolution satellite imagery is often constrained by\nthe spatial and temporal limitations of satellite sensors, as well as the high\ncosts associated with frequent observations. These challenges hinder\napplications such as environmental monitoring, disaster response, and\nagricultural management, which require fine-grained and high-resolution data.\nIn this paper, we propose MWT-Diff, an innovative framework for satellite image\nsuper-resolution (SR) that combines latent diffusion models with wavelet\ntransforms to address these challenges. At the core of the framework is a novel\nmetadata-, wavelet-, and time-aware encoder (MWT-Encoder), which generates\nembeddings that capture metadata attributes, multi-scale frequency information,\nand temporal relationships. The embedded feature representations steer the\nhierarchical diffusion dynamics, through which the model progressively\nreconstructs high-resolution satellite imagery from low-resolution inputs. This\nprocess preserves critical spatial characteristics including textural patterns,\nboundary discontinuities, and high-frequency spectral components essential for\ndetailed remote sensing analysis. The comparative analysis of MWT-Diff across\nmultiple datasets demonstrated favorable performance compared to recent\napproaches, as measured by standard perceptual quality metrics including FID\nand LPIPS.", "comment": "ICLR 2025 Workshop on Machine Learning for Remote Sensing (ML4RS)", "pdf_url": "http://arxiv.org/pdf/2506.23566v1", "categories": ["cs.CV", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23566v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23560", "title": "Tensor Train Quantum State Tomography using Compressed Sensing", "authors": ["Shakir Showkat Sofi", "Charlotte Vermeylen", "Lieven De Lathauwer"], "summary": "Quantum state tomography (QST) is a fundamental technique for estimating the\nstate of a quantum system from measured data and plays a crucial role in\nevaluating the performance of quantum devices. However, standard estimation\nmethods become impractical due to the exponential growth of parameters in the\nstate representation. In this work, we address this challenge by parameterizing\nthe state using a low-rank block tensor train decomposition and demonstrate\nthat our approach is both memory- and computationally efficient. This framework\napplies to a broad class of quantum states that can be well approximated by\nlow-rank decompositions, including pure states, nearly pure states, and ground\nstates of Hamiltonians.", "comment": "Accepted for publication in EUSIPCO 2025", "pdf_url": "http://arxiv.org/pdf/2506.23560v1", "categories": ["quant-ph", "cs.AI", "eess.SP", "math.OC"], "cate": "quant-ph", "url": "http://arxiv.org/abs/2506.23560v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23881", "title": "Spurious-Aware Prototype Refinement for Reliable Out-of-Distribution Detection", "authors": ["Reihaneh Zohrabi", "Hosein Hasani", "Mahdieh Soleymani Baghshah", "Anna Rohrbach", "Marcus Rohrbach", "Mohammad Hossein Rohban"], "summary": "Out-of-distribution (OOD) detection is crucial for ensuring the reliability\nand safety of machine learning models in real-world applications, where they\nfrequently face data distributions unseen during training. Despite progress,\nexisting methods are often vulnerable to spurious correlations that mislead\nmodels and compromise robustness. To address this, we propose SPROD, a novel\nprototype-based OOD detection approach that explicitly addresses the challenge\nposed by unknown spurious correlations. Our post-hoc method refines class\nprototypes to mitigate bias from spurious features without additional data or\nhyperparameter tuning, and is broadly applicable across diverse backbones and\nOOD detection settings. We conduct a comprehensive spurious correlation OOD\ndetection benchmarking, comparing our method against existing approaches and\ndemonstrating its superior performance across challenging OOD datasets, such as\nCelebA, Waterbirds, UrbanCars, Spurious Imagenet, and the newly introduced\nAnimals MetaCoCo. On average, SPROD improves AUROC by 4.7% and FPR@95 by 9.3%\nover the second best.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23881v1", "categories": ["cs.CV", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23881v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23573", "title": "Online Human Action Detection during Escorting", "authors": ["Siddhartha Mondal", "Avik Mitra", "Chayan Sarkar"], "summary": "The deployment of robot assistants in large indoor spaces has seen\nsignificant growth, with escorting tasks becoming a key application. However,\nmost current escorting robots primarily rely on navigation-focused strategies,\nassuming that the person being escorted will follow without issue. In crowded\nenvironments, this assumption often falls short, as individuals may struggle to\nkeep pace, become obstructed, get distracted, or need to stop unexpectedly. As\na result, conventional robotic systems are often unable to provide effective\nescorting services due to their limited understanding of human movement\ndynamics. To address these challenges, an effective escorting robot must\ncontinuously detect and interpret human actions during the escorting process\nand adjust its movement accordingly. However, there is currently no existing\ndataset designed specifically for human action detection in the context of\nescorting. Given that escorting often occurs in crowded environments, where\nother individuals may enter the robot's camera view, the robot also needs to\nidentify the specific human it is escorting (the subject) before predicting\ntheir actions. Since no existing model performs both person re-identification\nand action prediction in real-time, we propose a novel neural network\narchitecture that can accomplish both tasks. This enables the robot to adjust\nits speed dynamically based on the escortee's movements and seamlessly resume\nescorting after any disruption. In comparative evaluations against strong\nbaselines, our system demonstrates superior efficiency and effectiveness,\nshowcasing its potential to significantly improve robotic escorting services in\ncomplex, real-world scenarios.", "comment": "Accepted in IEEE RO-MAN '25", "pdf_url": "http://arxiv.org/pdf/2506.23573v1", "categories": ["cs.RO", "cs.AI", "cs.LG"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.23573v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23573", "title": "Online Human Action Detection during Escorting", "authors": ["Siddhartha Mondal", "Avik Mitra", "Chayan Sarkar"], "summary": "The deployment of robot assistants in large indoor spaces has seen\nsignificant growth, with escorting tasks becoming a key application. However,\nmost current escorting robots primarily rely on navigation-focused strategies,\nassuming that the person being escorted will follow without issue. In crowded\nenvironments, this assumption often falls short, as individuals may struggle to\nkeep pace, become obstructed, get distracted, or need to stop unexpectedly. As\na result, conventional robotic systems are often unable to provide effective\nescorting services due to their limited understanding of human movement\ndynamics. To address these challenges, an effective escorting robot must\ncontinuously detect and interpret human actions during the escorting process\nand adjust its movement accordingly. However, there is currently no existing\ndataset designed specifically for human action detection in the context of\nescorting. Given that escorting often occurs in crowded environments, where\nother individuals may enter the robot's camera view, the robot also needs to\nidentify the specific human it is escorting (the subject) before predicting\ntheir actions. Since no existing model performs both person re-identification\nand action prediction in real-time, we propose a novel neural network\narchitecture that can accomplish both tasks. This enables the robot to adjust\nits speed dynamically based on the escortee's movements and seamlessly resume\nescorting after any disruption. In comparative evaluations against strong\nbaselines, our system demonstrates superior efficiency and effectiveness,\nshowcasing its potential to significantly improve robotic escorting services in\ncomplex, real-world scenarios.", "comment": "Accepted in IEEE RO-MAN '25", "pdf_url": "http://arxiv.org/pdf/2506.23573v1", "categories": ["cs.RO", "cs.AI", "cs.LG"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.23573v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23897", "title": "PriOr-Flow: Enhancing Primitive Panoramic Optical Flow with Orthogonal View", "authors": ["Longliang Liu", "Miaojie Feng", "Junda Cheng", "Jijun Xiang", "Xuan Zhu", "Xin Yang"], "summary": "Panoramic optical flow enables a comprehensive understanding of temporal\ndynamics across wide fields of view. However, severe distortions caused by\nsphere-to-plane projections, such as the equirectangular projection (ERP),\nsignificantly degrade the performance of conventional perspective-based optical\nflow methods, especially in polar regions. To address this challenge, we\npropose PriOr-Flow, a novel dual-branch framework that leverages the\nlow-distortion nature of the orthogonal view to enhance optical flow estimation\nin these regions. Specifically, we introduce the Dual-Cost Collaborative Lookup\n(DCCL) operator, which jointly retrieves correlation information from both the\nprimitive and orthogonal cost volumes, effectively mitigating distortion noise\nduring cost volume construction. Furthermore, our Ortho-Driven Distortion\nCompensation (ODDC) module iteratively refines motion features from both\nbranches, further suppressing polar distortions. Extensive experiments\ndemonstrate that PriOr-Flow is compatible with various perspective-based\niterative optical flow methods and consistently achieves state-of-the-art\nperformance on publicly available panoramic optical flow datasets, setting a\nnew benchmark for wide-field motion estimation. The code is publicly available\nat: https://github.com/longliangLiu/PriOr-Flow.", "comment": "11 pages", "pdf_url": "http://arxiv.org/pdf/2506.23897v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23897v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23581", "title": "PBCAT: Patch-based composite adversarial training against physically realizable attacks on object detection", "authors": ["Xiao Li", "Yiming Zhu", "Yifan Huang", "Wei Zhang", "Yingzhe He", "Jie Shi", "Xiaolin Hu"], "summary": "Object detection plays a crucial role in many security-sensitive\napplications. However, several recent studies have shown that object detectors\ncan be easily fooled by physically realizable attacks, \\eg, adversarial patches\nand recent adversarial textures, which pose realistic and urgent threats.\nAdversarial Training (AT) has been recognized as the most effective defense\nagainst adversarial attacks. While AT has been extensively studied in the\n$l_\\infty$ attack settings on classification models, AT against physically\nrealizable attacks on object detectors has received limited exploration. Early\nattempts are only performed to defend against adversarial patches, leaving AT\nagainst a wider range of physically realizable attacks under-explored. In this\nwork, we consider defending against various physically realizable attacks with\na unified AT method. We propose PBCAT, a novel Patch-Based Composite\nAdversarial Training strategy. PBCAT optimizes the model by incorporating the\ncombination of small-area gradient-guided adversarial patches and imperceptible\nglobal adversarial perturbations covering the entire image. With these designs,\nPBCAT has the potential to defend against not only adversarial patches but also\nunseen physically realizable attacks such as adversarial textures. Extensive\nexperiments in multiple settings demonstrated that PBCAT significantly improved\nrobustness against various physically realizable attacks over state-of-the-art\ndefense methods. Notably, it improved the detection accuracy by 29.7\\% over\nprevious defense methods under one recent adversarial texture attack.", "comment": "Accepted by ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2506.23581v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23581v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23581", "title": "PBCAT: Patch-based composite adversarial training against physically realizable attacks on object detection", "authors": ["Xiao Li", "Yiming Zhu", "Yifan Huang", "Wei Zhang", "Yingzhe He", "Jie Shi", "Xiaolin Hu"], "summary": "Object detection plays a crucial role in many security-sensitive\napplications. However, several recent studies have shown that object detectors\ncan be easily fooled by physically realizable attacks, \\eg, adversarial patches\nand recent adversarial textures, which pose realistic and urgent threats.\nAdversarial Training (AT) has been recognized as the most effective defense\nagainst adversarial attacks. While AT has been extensively studied in the\n$l_\\infty$ attack settings on classification models, AT against physically\nrealizable attacks on object detectors has received limited exploration. Early\nattempts are only performed to defend against adversarial patches, leaving AT\nagainst a wider range of physically realizable attacks under-explored. In this\nwork, we consider defending against various physically realizable attacks with\na unified AT method. We propose PBCAT, a novel Patch-Based Composite\nAdversarial Training strategy. PBCAT optimizes the model by incorporating the\ncombination of small-area gradient-guided adversarial patches and imperceptible\nglobal adversarial perturbations covering the entire image. With these designs,\nPBCAT has the potential to defend against not only adversarial patches but also\nunseen physically realizable attacks such as adversarial textures. Extensive\nexperiments in multiple settings demonstrated that PBCAT significantly improved\nrobustness against various physically realizable attacks over state-of-the-art\ndefense methods. Notably, it improved the detection accuracy by 29.7\\% over\nprevious defense methods under one recent adversarial texture attack.", "comment": "Accepted by ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2506.23581v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23581v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23903", "title": "GroundingDINO-US-SAM: Text-Prompted Multi-Organ Segmentation in Ultrasound with LoRA-Tuned Vision-Language Models", "authors": ["Hamza Rasaee", "Taha Koleilat", "Hassan Rivaz"], "summary": "Accurate and generalizable object segmentation in ultrasound imaging remains\na significant challenge due to anatomical variability, diverse imaging\nprotocols, and limited annotated data. In this study, we propose a\nprompt-driven vision-language model (VLM) that integrates Grounding DINO with\nSAM2 to enable object segmentation across multiple ultrasound organs. A total\nof 18 public ultrasound datasets, encompassing the breast, thyroid, liver,\nprostate, kidney, and paraspinal muscle, were utilized. These datasets were\ndivided into 15 for fine-tuning and validation of Grounding DINO using Low Rank\nAdaptation (LoRA) to the ultrasound domain, and 3 were held out entirely for\ntesting to evaluate performance in unseen distributions. Comprehensive\nexperiments demonstrate that our approach outperforms state-of-the-art\nsegmentation methods, including UniverSeg, MedSAM, MedCLIP-SAM, BiomedParse,\nand SAMUS on most seen datasets while maintaining strong performance on unseen\ndatasets without additional fine-tuning. These results underscore the promise\nof VLMs in scalable and robust ultrasound image analysis, reducing dependence\non large, organ-specific annotated datasets. We will publish our code on\ncode.sonography.ai after acceptance.", "comment": "11 pages, 3 figures, 6 figures", "pdf_url": "http://arxiv.org/pdf/2506.23903v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23903v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23583", "title": "Detect \\& Score: Privacy-Preserving Misbehaviour Detection and Contribution Evaluation in Federated Learning", "authors": ["Marvin Xhemrishi", "Alexandre Graell i Amat", "Bal√°zs Pej√≥"], "summary": "Federated learning with secure aggregation enables private and collaborative\nlearning from decentralised data without leaking sensitive client information.\nHowever, secure aggregation also complicates the detection of malicious client\nbehaviour and the evaluation of individual client contributions to the\nlearning. To address these challenges, QI (Pejo et al.) and FedGT (Xhemrishi et\nal.) were proposed for contribution evaluation (CE) and misbehaviour detection\n(MD), respectively. QI, however, lacks adequate MD accuracy due to its reliance\non the random selection of clients in each training round, while FedGT lacks\nthe CE ability. In this work, we combine the strengths of QI and FedGT to\nachieve both robust MD and accurate CE. Our experiments demonstrate superior\nperformance compared to using either method independently.", "comment": "The shorter version is accepted at FL-AsiaCCS 25", "pdf_url": "http://arxiv.org/pdf/2506.23583v1", "categories": ["cs.CR", "cs.DC", "cs.LG"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.23583v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23584", "title": "A Clinically-Grounded Two-Stage Framework for Renal CT Report Generation", "authors": ["Renjie Liang", "Zhengkang Fan", "Jinqian Pan", "Chenkun Sun", "Russell Terry", "Jie Xu"], "summary": "Generating radiology reports from CT scans remains a complex task due to the\nnuanced nature of medical imaging and the variability in clinical\ndocumentation. In this study, we propose a two-stage framework for generating\nrenal radiology reports from 2D CT slices. First, we extract structured\nabnormality features using a multi-task learning model trained to identify\nlesion attributes such as location, size, enhancement, and attenuation. These\nextracted features are subsequently combined with the corresponding CT image\nand fed into a fine-tuned vision-language model to generate natural language\nreport sentences aligned with clinical findings. We conduct experiments on a\ncurated dataset of renal CT studies with manually annotated\nsentence-slice-feature triplets and evaluate performance using both\nclassification metrics and natural language generation metrics. Our results\ndemonstrate that the proposed model outperforms random baselines across all\nabnormality types, and the generated reports capture key clinical content with\nreasonable textual accuracy. This exploratory work highlights the feasibility\nof modular, feature-informed report generation for renal imaging. Future\nefforts will focus on extending this pipeline to 3D CT volumes and further\nimproving clinical fidelity in multimodal medical AI systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23584v1", "categories": ["eess.IV", "cs.AI", "cs.CV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.23584v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23916", "title": "Three-dimensional end-to-end deep learning for brain MRI analysis", "authors": ["Radhika Juglan", "Marta Ligero", "Zunamys I. Carrero", "Asier Rabasco", "Tim Lenz", "Leo Misera", "Gregory Patrick Veldhuizen", "Paul Kuntke", "Hagen H. Kitzler", "Sven Nebelung", "Daniel Truhn", "Jakob Nikolas Kather"], "summary": "Deep learning (DL) methods are increasingly outperforming classical\napproaches in brain imaging, yet their generalizability across diverse imaging\ncohorts remains inadequately assessed. As age and sex are key neurobiological\nmarkers in clinical neuroscience, influencing brain structure and disease risk,\nthis study evaluates three of the existing three-dimensional architectures,\nnamely Simple Fully Connected Network (SFCN), DenseNet, and Shifted Window\n(Swin) Transformers, for age and sex prediction using T1-weighted MRI from four\nindependent cohorts: UK Biobank (UKB, n=47,390), Dallas Lifespan Brain Study\n(DLBS, n=132), Parkinson's Progression Markers Initiative (PPMI, n=108 healthy\ncontrols), and Information eXtraction from Images (IXI, n=319). We found that\nSFCN consistently outperformed more complex architectures with AUC of 1.00\n[1.00-1.00] in UKB (internal test set) and 0.85-0.91 in external test sets for\nsex classification. For the age prediction task, SFCN demonstrated a mean\nabsolute error (MAE) of 2.66 (r=0.89) in UKB and 4.98-5.81 (r=0.55-0.70) across\nexternal datasets. Pairwise DeLong and Wilcoxon signed-rank tests with\nBonferroni corrections confirmed SFCN's superiority over Swin Transformer\nacross most cohorts (p<0.017, for three comparisons). Explainability analysis\nfurther demonstrates the regional consistency of model attention across cohorts\nand specific to each task. Our findings reveal that simpler convolutional\nnetworks outperform the denser and more complex attention-based DL\narchitectures in brain image analysis by demonstrating better generalizability\nacross different datasets.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23916v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23916v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23619", "title": "Overparametrized models with posterior drift", "authors": ["Guillaume Coqueret", "Martial Laguerre"], "summary": "This paper investigates the impact of posterior drift on out-of-sample\nforecasting accuracy in overparametrized machine learning models. We document\nthe loss in performance when the loadings of the data generating process change\nbetween the training and testing samples. This matters crucially in settings in\nwhich regime changes are likely to occur, for instance, in financial markets.\nApplied to equity premium forecasting, our results underline the sensitivity of\na market timing strategy to sub-periods and to the bandwidth parameters that\ncontrol the complexity of the model. For the average investor, we find that\nfocusing on holding periods of 15 years can generate very heterogeneous\nreturns, especially for small bandwidths. Large bandwidths yield much more\nconsistent outcomes, but are far less appealing from a risk-adjusted return\nstandpoint. All in all, our findings tend to recommend cautiousness when\nresorting to large linear models for stock market predictions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23619v1", "categories": ["q-fin.ST", "cs.LG", "econ.EM", "stat.ML"], "cate": "q-fin.ST", "url": "http://arxiv.org/abs/2506.23619v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23589", "title": "Transition Matching: Scalable and Flexible Generative Modeling", "authors": ["Neta Shaul", "Uriel Singer", "Itai Gat", "Yaron Lipman"], "summary": "Diffusion and flow matching models have significantly advanced media\ngeneration, yet their design space is well-explored, somewhat limiting further\nimprovements. Concurrently, autoregressive (AR) models, particularly those\ngenerating continuous tokens, have emerged as a promising direction for\nunifying text and media generation. This paper introduces Transition Matching\n(TM), a novel discrete-time, continuous-state generative paradigm that unifies\nand advances both diffusion/flow models and continuous AR generation. TM\ndecomposes complex generation tasks into simpler Markov transitions, allowing\nfor expressive non-deterministic probability transition kernels and arbitrary\nnon-continuous supervision processes, thereby unlocking new flexible design\navenues. We explore these choices through three TM variants: (i) Difference\nTransition Matching (DTM), which generalizes flow matching to discrete-time by\ndirectly learning transition probabilities, yielding state-of-the-art image\nquality and text adherence as well as improved sampling efficiency. (ii)\nAutoregressive Transition Matching (ARTM) and (iii) Full History Transition\nMatching (FHTM) are partially and fully causal models, respectively, that\ngeneralize continuous AR methods. They achieve continuous causal AR generation\nquality comparable to non-causal approaches and potentially enable seamless\nintegration with existing AR text generation techniques. Notably, FHTM is the\nfirst fully causal model to match or surpass the performance of flow-based\nmethods on text-to-image task in continuous domains. We demonstrate these\ncontributions through a rigorous large-scale comparison of TM variants and\nrelevant baselines, maintaining a fixed architecture, training data, and\nhyperparameters.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23589v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.23589v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23918", "title": "Thinking with Images for Multimodal Reasoning: Foundations, Methods, and Future Frontiers", "authors": ["Zhaochen Su", "Peng Xia", "Hangyu Guo", "Zhenhua Liu", "Yan Ma", "Xiaoye Qu", "Jiaqi Liu", "Yanshu Li", "Kaide Zeng", "Zhengyuan Yang", "Linjie Li", "Yu Cheng", "Heng Ji", "Junxian He", "Yi R.", "Fung"], "summary": "Recent progress in multimodal reasoning has been significantly advanced by\ntextual Chain-of-Thought (CoT), a paradigm where models conduct reasoning\nwithin language. This text-centric approach, however, treats vision as a\nstatic, initial context, creating a fundamental \"semantic gap\" between rich\nperceptual data and discrete symbolic thought. Human cognition often transcends\nlanguage, utilizing vision as a dynamic mental sketchpad. A similar evolution\nis now unfolding in AI, marking a fundamental paradigm shift from models that\nmerely think about images to those that can truly think with images. This\nemerging paradigm is characterized by models leveraging visual information as\nintermediate steps in their thought process, transforming vision from a passive\ninput into a dynamic, manipulable cognitive workspace. In this survey, we chart\nthis evolution of intelligence along a trajectory of increasing cognitive\nautonomy, which unfolds across three key stages: from external tool\nexploration, through programmatic manipulation, to intrinsic imagination. To\nstructure this rapidly evolving field, our survey makes four key contributions.\n(1) We establish the foundational principles of the think with image paradigm\nand its three-stage framework. (2) We provide a comprehensive review of the\ncore methods that characterize each stage of this roadmap. (3) We analyze the\ncritical landscape of evaluation benchmarks and transformative applications.\n(4) We identify significant challenges and outline promising future directions.\nBy providing this structured overview, we aim to offer a clear roadmap for\nfuture research towards more powerful and human-aligned multimodal AI.", "comment": "We maintain a real-time GitHub repository tracking progress at:\n  https://github.com/zhaochen0110/Awesome_Think_With_Images", "pdf_url": "http://arxiv.org/pdf/2506.23918v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23918v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23627", "title": "Brain Tumor Detection through Thermal Imaging and MobileNET", "authors": ["Roham Maiti", "Debasmita Bhoumik"], "summary": "Brain plays a crucial role in regulating body functions and cognitive\nprocesses, with brain tumors posing significant risks to human health. Precise\nand prompt detection is a key factor in proper treatment and better patient\noutcomes. Traditional methods for detecting brain tumors, that include\nbiopsies, MRI, and CT scans often face challenges due to their high costs and\nthe need for specialized medical expertise. Recent developments in machine\nlearning (ML) and deep learning (DL) has exhibited strong capabilities in\nautomating the identification and categorization of brain tumors from medical\nimages, especially MRI scans. However, these classical ML models have\nlimitations, such as high computational demands, the need for large datasets,\nand long training times, which hinder their accessibility and efficiency. Our\nresearch uses MobileNET model for efficient detection of these tumors. The\nnovelty of this project lies in building an accurate tumor detection model\nwhich use less computing re-sources and runs in less time followed by efficient\ndecision making through the use of image processing technique for accurate\nresults. The suggested method attained an average accuracy of 98.5%.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23627v1", "categories": ["cs.CV", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23627v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23596", "title": "When Will It Fail?: Anomaly to Prompt for Forecasting Future Anomalies in Time Series", "authors": ["Min-Yeong Park", "Won-Jeong Lee", "Seong Tae Kim", "Gyeong-Moon Park"], "summary": "Recently, forecasting future abnormal events has emerged as an important\nscenario to tackle real-world necessities. However, the solution of predicting\nspecific future time points when anomalies will occur, known as Anomaly\nPrediction (AP), remains under-explored. Existing methods dealing with time\nseries data fail in AP, focusing only on immediate anomalies or failing to\nprovide precise predictions for future anomalies. To address the AP task, we\npropose a novel framework called Anomaly to Prompt (A2P), comprised of\nAnomaly-Aware Forecasting (AAF) and Synthetic Anomaly Prompting (SAP). To\nenable the forecasting model to forecast abnormal time points, we adopt a\nstrategy to learn the relationships of anomalies. For the robust detection of\nanomalies, our proposed SAP introduces a learnable Anomaly Prompt Pool (APP)\nthat simulates diverse anomaly patterns using signal adaptive prompt.\nComprehensive experiments on multiple real-world datasets demonstrate the\nsuperiority of A2P over state-of-the-art methods, showcasing its ability to\npredict future anomalies. Our implementation code is available at\nhttps://github.com/KU-VGI/AP.", "comment": "18 pages, 10 figures, 12 tables, ICML 2025", "pdf_url": "http://arxiv.org/pdf/2506.23596v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.23596v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23963", "title": "Evaluating the Impact of Khmer Font Types on Text Recognition", "authors": ["Vannkinh Nom", "Souhail Bakkali", "Muhammad Muzzamil Luqman", "Mickael Coustaty", "Jean-Marc Ogier"], "summary": "Text recognition is significantly influenced by font types, especially for\ncomplex scripts like Khmer. The variety of Khmer fonts, each with its unique\ncharacter structure, presents challenges for optical character recognition\n(OCR) systems. In this study, we evaluate the impact of 19 randomly selected\nKhmer font types on text recognition accuracy using Pytesseract. The fonts\ninclude Angkor, Battambang, Bayon, Bokor, Chenla, Dangrek, Freehand, Kh Kompong\nChhnang, Kh SN Kampongsom, Khmer, Khmer CN Stueng Songke, Khmer Savuth Pen,\nMetal, Moul, Odor MeanChey, Preah Vihear, Siemreap, Sithi Manuss, and iSeth\nFirst. Our comparison of OCR performance across these fonts reveals that Khmer,\nOdor MeanChey, Siemreap, Sithi Manuss, and Battambang achieve high accuracy,\nwhile iSeth First, Bayon, and Dangrek perform poorly. This study underscores\nthe critical importance of font selection in optimizing Khmer text recognition\nand provides valuable insights for developing more robust OCR systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23963v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23963v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23640", "title": "Geminet: Learning the Duality-based Iterative Process for Lightweight Traffic Engineering in Changing Topologies", "authors": ["Ximeng Liu", "Shizhen Zhao", "Xinbing Wang"], "summary": "Recently, researchers have explored ML-based Traffic Engineering (TE),\nleveraging neural networks to solve TE problems traditionally addressed by\noptimization. However, existing ML-based TE schemes remain impractical: they\neither fail to handle topology changes or suffer from poor scalability due to\nexcessive computational and memory overhead. To overcome these limitations, we\npropose Geminet, a lightweight and scalable ML-based TE framework that can\nhandle changing topologies. Geminet is built upon two key insights: (i) a\nmethodology that decouples neural networks from topology by learning an\niterative gradient-descent-based adjustment process, as the update rule of\ngradient descent is topology-agnostic, relying only on a few gradient-related\nquantities; (ii) shifting optimization from path-level routing weights to\nedge-level dual variables, reducing memory consumption by leveraging the fact\nthat edges are far fewer than paths. Evaluations on WAN and data center\ndatasets show that Geminet significantly improves scalability. Its neural\nnetwork size is only 0.04% to 7% of existing schemes, while handling topology\nvariations as effectively as HARP, a state-of-the-art ML-based TE approach,\nwithout performance degradation. When trained on large-scale topologies,\nGeminet consumes under 10 GiB of memory, more than eight times less than the\n80-plus GiB required by HARP, while achieving 5.45 times faster convergence\nspeed, demonstrating its potential for large-scale deployment.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23640v1", "categories": ["cs.NI", "cs.LG"], "cate": "cs.NI", "url": "http://arxiv.org/abs/2506.23640v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23601", "title": "Semantic-guided Diverse Decoding for Large Language Model", "authors": ["Weijie Shi", "Yue Cui", "Yaguang Wu", "Jingzhi Fang", "Shibo Zhang", "Mengze Li", "Sirui Han", "Jia Zhu", "Jiajie Xu", "Xiaofang Zhou"], "summary": "Diverse decoding of large language models is crucial for applications\nrequiring multiple semantically distinct responses, yet existing methods\nprimarily achieve lexical rather than semantic diversity. This limitation\nsignificantly constrains Best-of-N strategies, group-based reinforcement\nlearning, and data synthesis. While temperature sampling and diverse beam\nsearch modify token distributions or apply n-gram penalties, they fail to\nensure meaningful semantic differentiation. We introduce Semantic-guided\nDiverse Decoding (SemDiD), operating directly in embedding space that balances\nquality with diversity through three complementary mechanisms: orthogonal\ndirectional guidance, dynamic inter-group repulsion, and position-debiased\nprobability assessment. SemDiD harmonizes these competing objectives using\nadaptive gain functions and constraint optimization, ensuring both quality\nthresholds and maximal semantic differentiation. Experiments show SemDiD\nconsistently outperforms existing methods, improving Best-of-N coverage by\n1.4-5.2% across diverse tasks and accelerating RLHF training convergence by 15%\nwhile increasing accuracy by up to 2.1%.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23601v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.23601v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23972", "title": "Visual and Memory Dual Adapter for Multi-Modal Object Tracking", "authors": ["Boyue Xu", "Ruichao Hou", "Tongwei Ren", "Gangshan Wu"], "summary": "Prompt-learning-based multi-modal trackers have achieved promising progress\nby employing lightweight visual adapters to incorporate auxiliary modality\nfeatures into frozen foundation models. However, existing approaches often\nstruggle to learn reliable prompts due to limited exploitation of critical cues\nacross frequency and temporal domains. In this paper, we propose a novel visual\nand memory dual adapter (VMDA) to construct more robust and discriminative\nrepresentations for multi-modal tracking. Specifically, we develop a simple but\neffective visual adapter that adaptively transfers discriminative cues from\nauxiliary modality to dominant modality by jointly modeling the frequency,\nspatial, and channel-wise features. Additionally, we design the memory adapter\ninspired by the human memory mechanism, which stores global temporal cues and\nperforms dynamic update and retrieval operations to ensure the consistent\npropagation of reliable temporal information across video sequences. Extensive\nexperiments demonstrate that our method achieves state-of-the-art performance\non the various multi-modal tracking tasks, including RGB-Thermal, RGB-Depth,\nand RGB-Event tracking. Code and models are available at\nhttps://github.com/xuboyue1999/mmtrack.git.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23972v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23972v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23663", "title": "On the Domain Robustness of Contrastive Vision-Language Models", "authors": ["Mario Koddenbrock", "Rudolf Hoffmann", "David Brodmann", "Erik Rodner"], "summary": "In real-world vision-language applications, practitioners increasingly rely\non large, pretrained foundation models rather than custom-built solutions,\ndespite limited transparency regarding their training data and processes. While\nthese models achieve impressive performance on general benchmarks, their\neffectiveness can decline notably under specialized domain shifts, such as\nunique imaging conditions or environmental variations. In this work, we\nintroduce Deepbench, a framework designed to assess domain-specific robustness\nof vision-language models (VLMs). Deepbench leverages a large language model\n(LLM) to generate realistic, context-aware image corruptions tailored to\nspecific deployment domains without requiring labeled data. We evaluate a range\nof contrastive vision-language architectures and architectural variants across\nsix real-world domains and observe substantial variability in robustness,\nhighlighting the need for targeted, domain-aware evaluation. Deepbench is\nreleased as open-source software to support further research into domain-aware\nrobustness assessment.", "comment": "Deepbench is available at https://github.com/ml-lab-htw/deepbench", "pdf_url": "http://arxiv.org/pdf/2506.23663v1", "categories": ["cs.CV", "cs.LG", "I.4"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23663v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23603", "title": "SoK: Semantic Privacy in Large Language Models", "authors": ["Baihe Ma", "Yanna Jiang", "Xu Wang", "Guangshen Yu", "Qin Wang", "Caijun Sun", "Chen Li", "Xuelei Qi", "Ying He", "Wei Ni", "Ren Ping Liu"], "summary": "As Large Language Models (LLMs) are increasingly deployed in sensitive\ndomains, traditional data privacy measures prove inadequate for protecting\ninformation that is implicit, contextual, or inferable - what we define as\nsemantic privacy. This Systematization of Knowledge (SoK) introduces a\nlifecycle-centric framework to analyze how semantic privacy risks emerge across\ninput processing, pretraining, fine-tuning, and alignment stages of LLMs. We\ncategorize key attack vectors and assess how current defenses, such as\ndifferential privacy, embedding encryption, edge computing, and unlearning,\naddress these threats. Our analysis reveals critical gaps in semantic-level\nprotection, especially against contextual inference and latent representation\nleakage. We conclude by outlining open challenges, including quantifying\nsemantic leakage, protecting multimodal inputs, balancing de-identification\nwith generation quality, and ensuring transparency in privacy enforcement. This\nwork aims to inform future research on designing robust, semantically aware\nprivacy-preserving techniques for LLMs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23603v1", "categories": ["cs.CR", "cs.AI"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.23603v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23975", "title": "Toward Simple and Robust Contrastive Explanations for Image Classification by Leveraging Instance Similarity and Concept Relevance", "authors": ["Yuliia Kaidashova", "Bettina Finzel", "Ute Schmid"], "summary": "Understanding why a classification model prefers one class over another for\nan input instance is the challenge of contrastive explanation. This work\nimplements concept-based contrastive explanations for image classification by\nleveraging the similarity of instance embeddings and relevance of\nhuman-understandable concepts used by a fine-tuned deep learning model. Our\napproach extracts concepts with their relevance score, computes contrasts for\nsimilar instances, and evaluates the resulting contrastive explanations based\non explanation complexity. Robustness is tested for different image\naugmentations. Two research questions are addressed: (1) whether explanation\ncomplexity varies across different relevance ranges, and (2) whether\nexplanation complexity remains consistent under image augmentations such as\nrotation and noise. The results confirm that for our experiments higher concept\nrelevance leads to shorter, less complex explanations, while lower relevance\nresults in longer, more diffuse explanations. Additionally, explanations show\nvarying degrees of robustness. The discussion of these findings offers insights\ninto the potential of building more interpretable and robust AI systems.", "comment": "17 pages, 6 figures, KI2025 - 48th German Conference on Artificial\n  Intelligence", "pdf_url": "http://arxiv.org/pdf/2506.23975v1", "categories": ["cs.CV", "68T07", "I.2; I.4"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23975v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23717", "title": "Towards Efficient and Accurate Spiking Neural Networks via Adaptive Bit Allocation", "authors": ["Xingting Yao", "Qinghao Hu", "Fei Zhou", "Tielong Liu", "Gang Li", "Peisong Wang", "Jian Cheng"], "summary": "Multi-bit spiking neural networks (SNNs) have recently become a heated\nresearch spot, pursuing energy-efficient and high-accurate AI. However, with\nmore bits involved, the associated memory and computation demands escalate to\nthe point where the performance improvements become disproportionate. Based on\nthe insight that different layers demonstrate different importance and extra\nbits could be wasted and interfering, this paper presents an adaptive bit\nallocation strategy for direct-trained SNNs, achieving fine-grained layer-wise\nallocation of memory and computation resources. Thus, SNN's efficiency and\naccuracy can be improved. Specifically, we parametrize the temporal lengths and\nthe bit widths of weights and spikes, and make them learnable and controllable\nthrough gradients. To address the challenges caused by changeable bit widths\nand temporal lengths, we propose the refined spiking neuron, which can handle\ndifferent temporal lengths, enable the derivation of gradients for temporal\nlengths, and suit spike quantization better. In addition, we theoretically\nformulate the step-size mismatch problem of learnable bit widths, which may\nincur severe quantization errors to SNN, and accordingly propose the step-size\nrenewal mechanism to alleviate this issue. Experiments on various datasets,\nincluding the static CIFAR and ImageNet and the dynamic CIFAR-DVS and\nDVS-GESTURE, demonstrate that our methods can reduce the overall memory and\ncomputation cost while achieving higher accuracy. Particularly, our\nSEWResNet-34 can achieve a 2.69\\% accuracy gain and 4.16$\\times$ lower bit\nbudgets over the advanced baseline work on ImageNet. This work will be fully\nopen-sourced.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23717v1", "categories": ["cs.NE", "cs.AI", "cs.CV", "cs.LG"], "cate": "cs.NE", "url": "http://arxiv.org/abs/2506.23717v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23605", "title": "AI-Generated Lecture Slides for Improving Slide Element Detection and Retrieval", "authors": ["Suyash Maniyar", "Vishvesh Trivedi", "Ajoy Mondal", "Anand Mishra", "C. V. Jawahar"], "summary": "Lecture slide element detection and retrieval are key problems in slide\nunderstanding. Training effective models for these tasks often depends on\nextensive manual annotation. However, annotating large volumes of lecture\nslides for supervised training is labor intensive and requires domain\nexpertise. To address this, we propose a large language model (LLM)-guided\nsynthetic lecture slide generation pipeline, SynLecSlideGen, which produces\nhigh-quality, coherent and realistic slides. We also create an evaluation\nbenchmark, namely RealSlide by manually annotating 1,050 real lecture slides.\nTo assess the utility of our synthetic slides, we perform few-shot transfer\nlearning on real data using models pre-trained on them. Experimental results\nshow that few-shot transfer learning with pretraining on synthetic slides\nsignificantly improves performance compared to training only on real data. This\ndemonstrates that synthetic data can effectively compensate for limited labeled\nlecture slides. The code and resources of our work are publicly available on\nour project website: https://synslidegen.github.io/.", "comment": "40 pages including supplementary, accepted at ICDAR 2025", "pdf_url": "http://arxiv.org/pdf/2506.23605v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23605v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23982", "title": "StyleDrive: Towards Driving-Style Aware Benchmarking of End-To-End Autonomous Driving", "authors": ["Ruiyang Hao", "Bowen Jing", "Haibao Yu", "Zaiqing Nie"], "summary": "While personalization has been explored in traditional autonomous driving\nsystems, it remains largely overlooked in end-to-end autonomous driving\n(E2EAD), despite its growing prominence. This gap is critical, as user-aligned\nbehavior is essential for trust, comfort, and widespread adoption of autonomous\nvehicles. A core challenge is the lack of large-scale real-world datasets\nannotated with diverse and fine-grained driving preferences, hindering the\ndevelopment and evaluation of personalized E2EAD models. In this work, we\npresent the first large-scale real-world dataset enriched with annotations\ncapturing diverse driving preferences, establishing a foundation for\npersonalization in E2EAD. We extract static environmental features from\nreal-world road topology and infer dynamic contextual cues using a fine-tuned\nvisual language model (VLM), enabling consistent and fine-grained scenario\nconstruction. Based on these scenarios, we derive objective preference\nannotations through behavioral distribution analysis and rule-based heuristics.\nTo address the inherent subjectivity of driving style, we further employ the\nVLM to generate subjective annotations by jointly modeling scene semantics and\ndriver behavior. Final high-quality labels are obtained through a\nhuman-in-the-loop verification process that fuses both perspectives. Building\non this dataset, we propose the first benchmark for evaluating personalized\nE2EAD models. We assess several state-of-the-art models with and without\npreference conditioning, demonstrating that incorporating personalized\npreferences results in behavior more aligned with human driving. Our work lays\nthe foundation for personalized E2EAD by providing a standardized platform to\nsystematically integrate human preferences into data-driven E2EAD systems,\ncatalyzing future research in human-centric autonomy.", "comment": "14 pages, 4 figures", "pdf_url": "http://arxiv.org/pdf/2506.23982v1", "categories": ["cs.CV", "cs.RO", "I.4.9"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23982v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23721", "title": "Deep Learning-Based Semantic Segmentation for Real-Time Kidney Imaging and Measurements with Augmented Reality-Assisted Ultrasound", "authors": ["Gijs Luijten", "Roberto Maria Scardigno", "Lisle Faray de Paiva", "Peter Hoyer", "Jens Kleesiek", "Domenico Buongiorno", "Vitoantonio Bevilacqua", "Jan Egger"], "summary": "Ultrasound (US) is widely accessible and radiation-free but has a steep\nlearning curve due to its dynamic nature and non-standard imaging planes.\nAdditionally, the constant need to shift focus between the US screen and the\npatient poses a challenge. To address these issues, we integrate deep learning\n(DL)-based semantic segmentation for real-time (RT) automated kidney volumetric\nmeasurements, which are essential for clinical assessment but are traditionally\ntime-consuming and prone to fatigue. This automation allows clinicians to\nconcentrate on image interpretation rather than manual measurements.\nComplementing DL, augmented reality (AR) enhances the usability of US by\nprojecting the display directly into the clinician's field of view, improving\nergonomics and reducing the cognitive load associated with screen-to-patient\ntransitions. Two AR-DL-assisted US pipelines on HoloLens-2 are proposed: one\nstreams directly via the application programming interface for a wireless\nsetup, while the other supports any US device with video output for broader\naccessibility. We evaluate RT feasibility and accuracy using the Open Kidney\nDataset and open-source segmentation models (nnU-Net, Segmenter, YOLO with\nMedSAM and LiteMedSAM). Our open-source GitHub pipeline includes model\nimplementations, measurement algorithms, and a Wi-Fi-based streaming solution,\nenhancing US training and diagnostics, especially in point-of-care settings.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23721v1", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.HC", "cs.LG"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.23721v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23628", "title": "The Kubernetes Network Driver Model: A Composable Architecture for High-Performance Networking", "authors": ["Antonio Ojea"], "summary": "Traditional Kubernetes networking struggles to meet the escalating demands of\nAI/ML and evolving Telco infrastructure. This paper introduces Kubernetes\nNetwork Drivers (KNDs), a transformative, modular, and declarative architecture\ndesigned to overcome current imperative provisioning and API limitations. KNDs\nintegrate network resource management into Kubernetes' core by utilizing\nDynamic Resource Allocation (DRA), Node Resource Interface (NRI) improvements,\nand upcoming OCI Runtime Specification changes. Our DraNet implementation\ndemonstrates declarative attachment of network interfaces, including Remote\nDirect Memory Access (RDMA) devices, significantly boosting high-performance\nAI/ML workloads. This capability enables sophisticated cloud-native\napplications and lays crucial groundwork for future Telco solutions, fostering\na \"galaxy\" of specialized KNDs for enhanced application delivery and reduced\noperational complexity.", "comment": "6 pages, 9 figures, submitted to IEEE LCN Special Track on\n  Cloud-AI-Native Mobile Networks Powered by eBPF (CAMe 2025)", "pdf_url": "http://arxiv.org/pdf/2506.23628v1", "categories": ["cs.NI", "cs.AI"], "cate": "cs.NI", "url": "http://arxiv.org/abs/2506.23628v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.24019", "title": "Ella: Embodied Social Agents with Lifelong Memory", "authors": ["Hongxin Zhang", "Zheyuan Zhang", "Zeyuan Wang", "Zunzhe Zhang", "Lixing Fang", "Qinhong Zhou", "Chuang Gan"], "summary": "We introduce Ella, an embodied social agent capable of lifelong learning\nwithin a community in a 3D open world, where agents accumulate experiences and\nacquire knowledge through everyday visual observations and social interactions.\nAt the core of Ella's capabilities is a structured, long-term multimodal memory\nsystem that stores, updates, and retrieves information effectively. It consists\nof a name-centric semantic memory for organizing acquired knowledge and a\nspatiotemporal episodic memory for capturing multimodal experiences. By\nintegrating this lifelong memory system with foundation models, Ella retrieves\nrelevant information for decision-making, plans daily activities, builds social\nrelationships, and evolves autonomously while coexisting with other intelligent\nbeings in the open world. We conduct capability-oriented evaluations in a\ndynamic 3D open world where 15 agents engage in social activities for days and\nare assessed with a suite of unseen controlled evaluations. Experimental\nresults show that Ella can influence, lead, and cooperate with other agents\nwell to achieve goals, showcasing its ability to learn effectively through\nobservation and social interaction. Our findings highlight the transformative\npotential of combining structured memory systems with foundation models for\nadvancing embodied intelligence. More videos can be found at\nhttps://umass-embodied-agi.github.io/Ella/.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.24019v1", "categories": ["cs.CV", "cs.CL"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.24019v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23767", "title": "Explainable AI for Comprehensive Risk Assessment for Financial Reports: A Lightweight Hierarchical Transformer Network Approach", "authors": ["Xue Wen Tan", "Stanley Kok"], "summary": "Every publicly traded U.S. company files an annual 10-K report containing\ncritical insights into financial health and risk. We propose Tiny eXplainable\nRisk Assessor (TinyXRA), a lightweight and explainable transformer-based model\nthat automatically assesses company risk from these reports. Unlike prior work\nthat relies solely on the standard deviation of excess returns (adjusted for\nthe Fama-French model), which indiscriminately penalizes both upside and\ndownside risk, TinyXRA incorporates skewness, kurtosis, and the Sortino ratio\nfor more comprehensive risk assessment. We leverage TinyBERT as our encoder to\nefficiently process lengthy financial documents, coupled with a novel dynamic,\nattention-based word cloud mechanism that provides intuitive risk visualization\nwhile filtering irrelevant terms. This lightweight design ensures scalable\ndeployment across diverse computing environments with real-time processing\ncapabilities for thousands of financial documents which is essential for\nproduction systems with constrained computational resources. We employ triplet\nloss for risk quartile classification, improving over pairwise loss approaches\nin existing literature by capturing both the direction and magnitude of risk\ndifferences. Our TinyXRA achieves state-of-the-art predictive accuracy across\nseven test years on a dataset spanning 2013-2024, while providing transparent\nand interpretable risk assessments. We conduct comprehensive ablation studies\nto evaluate our contributions and assess model explanations both quantitatively\nby systematically removing highly attended words and sentences, and\nqualitatively by examining explanation coherence. The paper concludes with\nfindings, practical implications, limitations, and future research directions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23767v1", "categories": ["q-fin.RM", "cs.LG"], "cate": "q-fin.RM", "url": "http://arxiv.org/abs/2506.23767v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23629", "title": "A Nonlinear Low-rank Representation Model with Convolutional Neural Network for Imputing Water Quality Data", "authors": ["Xin Liao", "Bing Yang", "Cai Yu"], "summary": "The integrity of Water Quality Data (WQD) is critical in environmental\nmonitoring for scientific decision-making and ecological protection. However,\nwater quality monitoring systems are often challenged by large amounts of\nmissing data due to unavoidable problems such as sensor failures and\ncommunication delays, which further lead to water quality data becoming\nHigh-Dimensional and Sparse (HDS). Traditional data imputation methods are\ndifficult to depict the potential dynamics and fail to capture the deep data\nfeatures, resulting in unsatisfactory imputation performance. To effectively\naddress the above issues, this paper proposes a Nonlinear Low-rank\nRepresentation model (NLR) with Convolutional Neural Networks (CNN) for\nimputing missing WQD, which utilizes CNNs to implement two ideas: a) fusing\ntemporal features to model the temporal dependence of data between time slots,\nand b) Extracting nonlinear interactions and local patterns to mine\nhigher-order relationships features and achieve deep fusion of multidimensional\ninformation. Experimental studies on three real water quality datasets\ndemonstrate that the proposed model significantly outperforms existing\nstate-of-the-art data imputation models in terms of estimation accuracy. It\nprovides an effective approach for handling water quality monitoring data in\ncomplex dynamic environments.", "comment": "7 pages, 2 figures, conference", "pdf_url": "http://arxiv.org/pdf/2506.23629v1", "categories": ["cs.LG", "cs.AI", "68T07(Primary) 62M10, 65C60 (Secondary)", "I.2.7"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.23629v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.24039", "title": "Foundation Models for Zero-Shot Segmentation of Scientific Images without AI-Ready Data", "authors": ["Shubhabrata Mukherjee", "Jack Lang", "Obeen Kwon", "Iryna Zenyuk", "Valerie Brogden", "Adam Weber", "Daniela Ushizima"], "summary": "Zero-shot and prompt-based technologies capitalized on using frequently\noccurring images to transform visual reasoning tasks, which explains why such\ntechnologies struggle with valuable yet scarce scientific image sets. In this\nwork, we propose Zenesis, a comprehensive no-code interactive platform designed\nto minimize barriers posed by data readiness for scientific images. We develop\nlightweight multi-modal adaptation techniques that enable zero-shot operation\non raw scientific data, along with human-in-the-loop refinement and\nheuristic-based temporal enhancement options. We demonstrate the performance of\nour approach through comprehensive comparison and validation on challenging\nFocused Ion Beam Scanning Electron Microscopy (FIB-SEM) data of catalyst-loaded\nmembranes. Zenesis significantly outperforms baseline methods, achieving an\naverage accuracy of 0.947, an Intersection over Union (IOU) of 0.858, and a\nDice score of 0.923 for amorphous catalyst samples and accuracy of 0.987, an\nIOU of 0.857, and a Dice score of 0.923 for crystalline samples. These results\nmark a substantial improvement over traditional methods like Otsu thresholding\nand even advanced models like Segment Anything Model (SAM) when used in\nisolation. Our results demonstrate that Zenesis is a powerful tool for\nscientific applications, particularly in fields where high-quality annotated\ndatasets are unavailable, accelerating accurate analysis of experimental\nimaging.", "comment": "This manuscript is a draft on arxiv. A final version has been\n  submitted to the 59th ICPP 2025, DRAI workshop", "pdf_url": "http://arxiv.org/pdf/2506.24039v1", "categories": ["cs.CV", "cs.HC"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.24039v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23783", "title": "Mamba-FETrack V2: Revisiting State Space Model for Frame-Event based Visual Object Tracking", "authors": ["Shiao Wang", "Ju Huang", "Qingchuan Ma", "Jinfeng Gao", "Chunyi Xu", "Xiao Wang", "Lan Chen", "Bo Jiang"], "summary": "Combining traditional RGB cameras with bio-inspired event cameras for robust\nobject tracking has garnered increasing attention in recent years. However,\nmost existing multimodal tracking algorithms depend heavily on high-complexity\nVision Transformer architectures for feature extraction and fusion across\nmodalities. This not only leads to substantial computational overhead but also\nlimits the effectiveness of cross-modal interactions. In this paper, we propose\nan efficient RGB-Event object tracking framework based on the linear-complexity\nVision Mamba network, termed Mamba-FETrack V2. Specifically, we first design a\nlightweight Prompt Generator that utilizes embedded features from each\nmodality, together with a shared prompt pool, to dynamically generate\nmodality-specific learnable prompt vectors. These prompts, along with the\nmodality-specific embedded features, are then fed into a Vision Mamba-based\nFEMamba backbone, which facilitates prompt-guided feature extraction,\ncross-modal interaction, and fusion in a unified manner. Finally, the fused\nrepresentations are passed to the tracking head for accurate target\nlocalization. Extensive experimental evaluations on multiple RGB-Event tracking\nbenchmarks, including short-term COESOT dataset and long-term datasets, i.e.,\nFE108 and FELT V2, demonstrate the superior performance and efficiency of the\nproposed tracking framework. The source code and pre-trained models will be\nreleased on https://github.com/Event-AHU/Mamba_FETrack", "comment": "Journal extension of Mamba-FETrack which was published on Pattern\n  Recognition and Computer Vision (PRCV) 2024", "pdf_url": "http://arxiv.org/pdf/2506.23783v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23783v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23634", "title": "gMBA: Expression Semantic Guided Mixed Boolean-Arithmetic Deobfuscation Using Transformer Architectures", "authors": ["Youjeong Noh", "Joon-Young Paik", "Jingun Kwon", "Eun-Sun Cho"], "summary": "Mixed Boolean-Arithmetic (MBA) obfuscation protects intellectual property by\nconverting programs into forms that are more complex to analyze. However, MBA\nhas been increasingly exploited by malware developers to evade detection and\ncause significant real-world problems. Traditional MBA deobfuscation methods\noften consider these expressions as part of a black box and overlook their\ninternal semantic information. To bridge this gap, we propose a truth table,\nwhich is an automatically constructed semantic representation of an\nexpression's behavior that does not rely on external resources. The truth table\nis a mathematical form that represents the output of expression for all\npossible combinations of input. We also propose a general and extensible guided\nMBA deobfuscation framework (gMBA) that modifies a Transformer-based neural\nencoder-decoder Seq2Seq architecture to incorporate this semantic guidance.\nExperimental results and in-depth analysis show that integrating expression\nsemantics significantly improves performance and highlights the importance of\ninternal semantic expressions in recovering obfuscated code to its original\nform.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23634v1", "categories": ["cs.CR", "cs.AI"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.23634v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.24044", "title": "A Survey on Vision-Language-Action Models for Autonomous Driving", "authors": ["Sicong Jiang", "Zilin Huang", "Kangan Qian", "Ziang Luo", "Tianze Zhu", "Yang Zhong", "Yihong Tang", "Menglin Kong", "Yunlong Wang", "Siwen Jiao", "Hao Ye", "Zihao Sheng", "Xin Zhao", "Tuopu Wen", "Zheng Fu", "Sikai Chen", "Kun Jiang", "Diange Yang", "Seongjin Choi", "Lijun Sun"], "summary": "The rapid progress of multimodal large language models (MLLM) has paved the\nway for Vision-Language-Action (VLA) paradigms, which integrate visual\nperception, natural language understanding, and control within a single policy.\nResearchers in autonomous driving are actively adapting these methods to the\nvehicle domain. Such models promise autonomous vehicles that can interpret\nhigh-level instructions, reason about complex traffic scenes, and make their\nown decisions. However, the literature remains fragmented and is rapidly\nexpanding. This survey offers the first comprehensive overview of VLA for\nAutonomous Driving (VLA4AD). We (i) formalize the architectural building blocks\nshared across recent work, (ii) trace the evolution from early explainer to\nreasoning-centric VLA models, and (iii) compare over 20 representative models\naccording to VLA's progress in the autonomous driving domain. We also\nconsolidate existing datasets and benchmarks, highlighting protocols that\njointly measure driving safety, accuracy, and explanation quality. Finally, we\ndetail open challenges - robustness, real-time efficiency, and formal\nverification - and outline future directions of VLA4AD. This survey provides a\nconcise yet complete reference for advancing interpretable socially aligned\nautonomous vehicles. Github repo is available at\n\\href{https://github.com/JohnsonJiang1996/Awesome-VLA4AD}{SicongJiang/Awesome-VLA4AD}.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.24044v1", "categories": ["cs.CV", "cs.AI", "cs.RO"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.24044v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23784", "title": "When GNNs Met a Word Equations Solver: Learning to Rank Equations (Extended Technical Report)", "authors": ["Parosh Aziz Abdulla", "Mohamed Faouzi Atig", "Julie Cailler", "Chencheng Liang", "Philipp R√ºmmer"], "summary": "Nielsen transformation is a standard approach for solving word equations: by\nrepeatedly splitting equations and applying simplification steps, equations are\nrewritten until a solution is reached. When solving a conjunction of word\nequations in this way, the performance of the solver will depend considerably\non the order in which equations are processed. In this work, the use of Graph\nNeural Networks (GNNs) for ranking word equations before and during the solving\nprocess is explored. For this, a novel graph-based representation for word\nequations is presented, preserving global information across conjuncts,\nenabling the GNN to have a holistic view during ranking. To handle the variable\nnumber of conjuncts, three approaches to adapt a multi-classification task to\nthe problem of ranking equations are proposed. The training of the GNN is done\nwith the help of minimum unsatisfiable subsets (MUSes) of word equations. The\nexperimental results show that, compared to state-of-the-art string solvers,\nthe new framework solves more problems in benchmarks where each variable\nappears at most once in each equation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23784v1", "categories": ["cs.AI", "cs.LG"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.23784v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23635", "title": "Towards Building Private LLMs: Exploring Multi-Node Expert Parallelism on Apple Silicon for Mixture-of-Experts Large Language Model", "authors": ["Mu-Chi Chen", "Po-Hsuan Huang", "Xiangrui Ke", "Chia-Heng Tu", "Chun Jason Xue", "Shih-Hao Hung"], "summary": "Large Language Models (LLMs) have revolutionized Artificial Intelligence (AI)\nwith significant advancements such as OpenAI's ChatGPT, Meta's Llama, and\nDatabricks' DBRX. This paper addresses the cost and scalability challenges\nencountered when constructing private LLM systems for personal or small group\nservices, as aimed by Apple Intelligence. A Mac Studio cluster with Apple's M2\nUltra chips is established as a cost-efficient solution to host and accelerate\nthe pretrained DBRX model with the Mixture-of-Experts (MoE) architecture. Our\nperformance analysis reveal that parallel execution of the model's experts\nacross two to four machine nodes significantly reduces inference time. We find\nthat computation time for the experts is comparable to the communication time\nfor exchanging their outputs, emphasizing the importance of network latency\nover bandwidth. We also observe significant management overhead due to Apple\nsoftware stack's memory management logic. Based on these findings, we develop\noptimization schemes to eliminate the memory management overhead. As a result,\nthe Mac Studio cluster is 1.15 times more cost-efficient than the\nstate-of-the-art AI supercomputer with NVIDIA H100 GPUs. In addition, we\nconstruct a performance model to estimate system performance under varying\nconfigurations, and the model provides valuable insights for designing private\nLLM systems.", "comment": "International Conference on Research in Adaptive and Convergent\n  Systems (RACS '24), November 5--8, 2024, Pompei, Italy", "pdf_url": "http://arxiv.org/pdf/2506.23635v1", "categories": ["cs.DC", "cs.AI", "cs.PF", "I.6.4; I.2.7; I.2.11"], "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.23635v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.24063", "title": "Continual Adaptation: Environment-Conditional Parameter Generation for Object Detection in Dynamic Scenarios", "authors": ["Deng Li", "Aming Wu", "Yang Li", "Yaowei Wang", "Yahong Han"], "summary": "In practice, environments constantly change over time and space, posing\nsignificant challenges for object detectors trained based on a closed-set\nassumption, i.e., training and test data share the same distribution. To this\nend, continual test-time adaptation has attracted much attention, aiming to\nimprove detectors' generalization by fine-tuning a few specific parameters,\ne.g., BatchNorm layers. However, based on a small number of test images,\nfine-tuning certain parameters may affect the representation ability of other\nfixed parameters, leading to performance degradation. Instead, we explore a new\nmechanism, i.e., converting the fine-tuning process to a specific-parameter\ngeneration. Particularly, we first design a dual-path LoRA-based domain-aware\nadapter that disentangles features into domain-invariant and domain-specific\ncomponents, enabling efficient adaptation. Additionally, a conditional\ndiffusion-based parameter generation mechanism is presented to synthesize the\nadapter's parameters based on the current environment, preventing the\noptimization from getting stuck in local optima. Finally, we propose a\nclass-centered optimal transport alignment method to mitigate catastrophic\nforgetting. Extensive experiments conducted on various continuous domain\nadaptive object detection tasks demonstrate the effectiveness. Meanwhile,\nvisualization results show that the representation extracted by the generated\nparameters can capture more object-related information and strengthen the\ngeneralization ability.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.24063v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.24063v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23793", "title": "Advancing Learnable Multi-Agent Pathfinding Solvers with Active Fine-Tuning", "authors": ["Anton Andreychuk", "Konstantin Yakovlev", "Aleksandr Panov", "Alexey Skrynnik"], "summary": "Multi-agent pathfinding (MAPF) is a common abstraction of multi-robot\ntrajectory planning problems, where multiple homogeneous robots simultaneously\nmove in the shared environment. While solving MAPF optimally has been proven to\nbe NP-hard, scalable, and efficient, solvers are vital for real-world\napplications like logistics, search-and-rescue, etc. To this end, decentralized\nsuboptimal MAPF solvers that leverage machine learning have come on stage.\nBuilding on the success of the recently introduced MAPF-GPT, a pure imitation\nlearning solver, we introduce MAPF-GPT-DDG. This novel approach effectively\nfine-tunes the pre-trained MAPF model using centralized expert data. Leveraging\na novel delta-data generation mechanism, MAPF-GPT-DDG accelerates training\nwhile significantly improving performance at test time. Our experiments\ndemonstrate that MAPF-GPT-DDG surpasses all existing learning-based MAPF\nsolvers, including the original MAPF-GPT, regarding solution quality across\nmany testing scenarios. Remarkably, it can work with MAPF instances involving\nup to 1 million agents in a single environment, setting a new milestone for\nscalability in MAPF domains.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23793v1", "categories": ["cs.AI", "cs.LG", "cs.MA"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.23793v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23639", "title": "Unified Multimodal Understanding via Byte-Pair Visual Encoding", "authors": ["Wanpeng Zhang", "Yicheng Feng", "Hao Luo", "Yijiang Li", "Zihao Yue", "Sipeng Zheng", "Zongqing Lu"], "summary": "Multimodal large language models (MLLMs) have made significant progress in\nvision-language understanding, yet effectively aligning different modalities\nremains a fundamental challenge. We present a framework that unifies multimodal\nunderstanding by applying byte-pair encoding to visual tokens. Unlike\nconventional approaches that rely on modality-specific encoders, our method\ndirectly incorporates structural information into visual tokens, mirroring\nsuccessful tokenization strategies in text-only language models. We introduce a\npriority-guided encoding scheme that considers both frequency and spatial\nconsistency, coupled with a multi-stage training procedure based on\ncurriculum-driven data composition. These enhancements enable the transformer\nmodel to better capture cross-modal relationships and reason with visual\ninformation. Comprehensive experiments demonstrate improved performance across\ndiverse vision-language tasks. By bridging the gap between visual and textual\nrepresentations, our approach contributes to the advancement of more capable\nand efficient multimodal foundation models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23639v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23639v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.24085", "title": "Imagine for Me: Creative Conceptual Blending of Real Images and Text via Blended Attention", "authors": ["Wonwoong Cho", "Yanxia Zhang", "Yan-Ying Chen", "David I. Inouye"], "summary": "Blending visual and textual concepts into a new visual concept is a unique\nand powerful trait of human beings that can fuel creativity. However, in\npractice, cross-modal conceptual blending for humans is prone to cognitive\nbiases, like design fixation, which leads to local minima in the design space.\nIn this paper, we propose a T2I diffusion adapter \"IT-Blender\" that can\nautomate the blending process to enhance human creativity. Prior works related\nto cross-modal conceptual blending are limited in encoding a real image without\nloss of details or in disentangling the image and text inputs. To address these\ngaps, IT-Blender leverages pretrained diffusion models (SD and FLUX) to blend\nthe latent representations of a clean reference image with those of the noisy\ngenerated image. Combined with our novel blended attention, IT-Blender encodes\nthe real reference image without loss of details and blends the visual concept\nwith the object specified by the text in a disentangled way. Our experiment\nresults show that IT-Blender outperforms the baselines by a large margin in\nblending visual and textual concepts, shedding light on the new application of\nimage generative models to augment human creativity.", "comment": "Project website is available at https://imagineforme.github.io/", "pdf_url": "http://arxiv.org/pdf/2506.24085v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.24085v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23836", "title": "Proving the Limited Scalability of Centralized Distributed Optimization via a New Lower Bound Construction", "authors": ["Alexander Tyurin"], "summary": "We consider centralized distributed optimization in the classical federated\nlearning setup, where $n$ workers jointly find an $\\varepsilon$-stationary\npoint of an $L$-smooth, $d$-dimensional nonconvex function $f$, having access\nonly to unbiased stochastic gradients with variance $\\sigma^2$. Each worker\nrequires at most $h$ seconds to compute a stochastic gradient, and the\ncommunication times from the server to the workers and from the workers to the\nserver are $\\tau_{s}$ and $\\tau_{w}$ seconds per coordinate, respectively. One\nof the main motivations for distributed optimization is to achieve scalability\nwith respect to $n$. For instance, it is well known that the distributed\nversion of SGD has a variance-dependent runtime term $\\frac{h \\sigma^2 L\n\\Delta}{n \\varepsilon^2},$ which improves with the number of workers $n,$ where\n$\\Delta = f(x^0) - f^*,$ and $x^0 \\in R^d$ is the starting point. Similarly,\nusing unbiased sparsification compressors, it is possible to reduce both the\nvariance-dependent runtime term and the communication runtime term. However,\nonce we account for the communication from the server to the workers\n$\\tau_{s}$, we prove that it becomes infeasible to design a method using\nunbiased random sparsification compressors that scales both the server-side\ncommunication runtime term $\\tau_{s} d \\frac{L \\Delta}{\\varepsilon}$ and the\nvariance-dependent runtime term $\\frac{h \\sigma^2 L \\Delta}{\\varepsilon^2},$\nbetter than poly-logarithmically in $n$, even in the homogeneous (i.i.d.) case,\nwhere all workers access the same distribution. To establish this result, we\nconstruct a new \"worst-case\" function and develop a new lower bound framework\nthat reduces the analysis to the concentration of a random sum, for which we\nprove a concentration bound. These results reveal fundamental limitations in\nscaling distributed optimization, even under the homogeneous assumption.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23836v1", "categories": ["math.OC", "cs.DC", "cs.LG"], "cate": "math.OC", "url": "http://arxiv.org/abs/2506.23836v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23641", "title": "VAP-Diffusion: Enriching Descriptions with MLLMs for Enhanced Medical Image Generation", "authors": ["Peng Huang", "Junhu Fu", "Bowen Guo", "Zeju Li", "Yuanyuan Wang", "Yi Guo"], "summary": "As the appearance of medical images is influenced by multiple underlying\nfactors, generative models require rich attribute information beyond labels to\nproduce realistic and diverse images. For instance, generating an image of skin\nlesion with specific patterns demands descriptions that go beyond diagnosis,\nsuch as shape, size, texture, and color. However, such detailed descriptions\nare not always accessible. To address this, we explore a framework, termed\nVisual Attribute Prompts (VAP)-Diffusion, to leverage external knowledge from\npre-trained Multi-modal Large Language Models (MLLMs) to improve the quality\nand diversity of medical image generation. First, to derive descriptions from\nMLLMs without hallucination, we design a series of prompts following\nChain-of-Thoughts for common medical imaging tasks, including dermatologic,\ncolorectal, and chest X-ray images. Generated descriptions are utilized during\ntraining and stored across different categories. During testing, descriptions\nare randomly retrieved from the corresponding category for inference. Moreover,\nto make the generator robust to unseen combination of descriptions at the test\ntime, we propose a Prototype Condition Mechanism that restricts test embeddings\nto be similar to those from training. Experiments on three common types of\nmedical imaging across four datasets verify the effectiveness of VAP-Diffusion.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23641v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23641v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.24086", "title": "MotionGPT3: Human Motion as a Second Modality", "authors": ["Bingfan Zhu", "Biao Jiang", "Sunyi Wang", "Shixiang Tang", "Tao Chen", "Linjie Luo", "Youyi Zheng", "Xin Chen"], "summary": "Though recent advances in multimodal models have demonstrated strong\ncapabilities and opportunities in unified understanding and generation, the\ndevelopment of unified motion-language models remains underexplored. To enable\nsuch models with high-fidelity human motion, two core challenges must be\naddressed. The first is the reconstruction gap between the continuous motion\nmodality and discrete representation in an autoregressive manner, and the\nsecond is the degradation of language intelligence during unified training.\nInspired by the mixture of experts, we propose MotionGPT3, a bimodal\nmotion-language model that treats human motion as a second modality, decoupling\nmotion modeling via separate model parameters and enabling both effective\ncross-modal interaction and efficient multimodal scaling training. To preserve\nlanguage intelligence, the text branch retains the original structure and\nparameters of the pretrained language model, while a new motion branch is\nintegrated via a shared attention mechanism, enabling bidirectional information\nflow between two modalities. We first employ a motion Variational Autoencoder\n(VAE) to encode raw human motion into latent representations. Based on this\ncontinuous latent space, the motion branch predicts motion latents directly\nfrom intermediate hidden states using a diffusion head, bypassing discrete\ntokenization. Extensive experiments show that our approach achieves competitive\nperformance on both motion understanding and generation tasks while preserving\nstrong language capabilities, establishing a unified bimodal motion diffusion\nframework within an autoregressive manner.", "comment": "21 pages, 8 figures", "pdf_url": "http://arxiv.org/pdf/2506.24086v1", "categories": ["cs.CV", "cs.CL"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.24086v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23855", "title": "Differentially Private Synthetic Data Release for Topics API Outputs", "authors": ["Travis Dick", "Alessandro Epasto", "Adel Javanmard", "Josh Karlin", "Andres Munoz Medina", "Vahab Mirrokni", "Sergei Vassilvitskii", "Peilin Zhong"], "summary": "The analysis of the privacy properties of Privacy-Preserving Ads APIs is an\narea of research that has received strong interest from academics, industry,\nand regulators. Despite this interest, the empirical study of these methods is\nhindered by the lack of publicly available data. Reliable empirical analysis of\nthe privacy properties of an API, in fact, requires access to a dataset\nconsisting of realistic API outputs; however, privacy concerns prevent the\ngeneral release of such data to the public.\n  In this work, we develop a novel methodology to construct synthetic API\noutputs that are simultaneously realistic enough to enable accurate study and\nprovide strong privacy protections. We focus on one Privacy-Preserving Ads\nAPIs: the Topics API, part of Google Chrome's Privacy Sandbox. We developed a\nmethodology to generate a differentially-private dataset that closely matches\nthe re-identification risk properties of the real Topics API data. The use of\ndifferential privacy provides strong theoretical bounds on the leakage of\nprivate user information from this release.\n  Our methodology is based on first computing a large number of\ndifferentially-private statistics describing how output API traces evolve over\ntime. Then, we design a parameterized distribution over sequences of API traces\nand optimize its parameters so that they closely match the statistics obtained.\nFinally, we create the synthetic data by drawing from this distribution.\n  Our work is complemented by an open-source release of the anonymized dataset\nobtained by this methodology. We hope this will enable external researchers to\nanalyze the API in-depth and replicate prior and future work on a realistic\nlarge-scale dataset. We believe that this work will contribute to fostering\ntransparency regarding the privacy properties of Privacy-Preserving Ads APIs.", "comment": "20 pages, 8 figures", "pdf_url": "http://arxiv.org/pdf/2506.23855v1", "categories": ["cs.CR", "cs.AI", "cs.LG"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.23855v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23644", "title": "QLPro: Automated Code Vulnerability Discovery via LLM and Static Code Analysis Integration", "authors": ["Junze Hu", "Xiangyu Jin", "Yizhe Zeng", "Yuling Liu", "Yunpeng Li", "Dan Du", "Kaiyu Xie", "Hongsong Zhu"], "summary": "We introduce QLPro, a vulnerability detection framework that systematically\nintegrates LLMs and static analysis tools to enable comprehensive vulnerability\ndetection across entire open-source projects.We constructed a new dataset,\nJavaTest, comprising 10 open-source projects from GitHub with 62 confirmed\nvulnerabilities. CodeQL, a state-of-the-art static analysis tool, detected only\n24 of these vulnerabilities while QLPro detected 41. Furthermore, QLPro\ndiscovered 6 previously unknown vulnerabilities, 2 of which have been confirmed\nas 0-days.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23644v1", "categories": ["cs.SE", "cs.AI", "cs.CR"], "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.23644v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.24092", "title": "WaRA: Wavelet Low Rank Adaptation", "authors": ["Moein Heidari", "Yasamin Medghalchi", "Mahdi Khoursha", "Reza Rezaeian", "Ilker Hacihaliloglu"], "summary": "Parameter-efficient fine-tuning (PEFT) has gained widespread adoption across\nvarious applications. Among PEFT techniques, Low-Rank Adaptation (LoRA) and its\nextensions have emerged as particularly effective, allowing efficient model\nadaptation while significantly reducing computational overhead. However,\nexisting approaches typically rely on global low-rank factorizations, which\noverlook local or multi-scale structure, failing to capture complex patterns in\nthe weight updates. To address this, we propose WaRA, a novel PEFT method that\nleverages wavelet transforms to decompose the weight update matrix into a\nmulti-resolution representation. By performing low-rank factorization in the\nwavelet domain and reconstructing updates through an inverse transform, WaRA\nobtains compressed adaptation parameters that harness multi-resolution\nanalysis, enabling it to capture both coarse and fine-grained features while\nproviding greater flexibility and sparser representations than standard LoRA.\nThrough comprehensive experiments and analysis, we demonstrate that WaRA\nperforms superior on diverse vision tasks, including image generation,\nclassification, and semantic segmentation, significantly enhancing generated\nimage quality while reducing computational complexity. Although WaRA was\nprimarily designed for vision tasks, we further showcase its effectiveness in\nlanguage tasks, highlighting its broader applicability and generalizability.\nThe code is publicly available at\n\\href{GitHub}{https://github.com/moeinheidari7829/WaRA}.", "comment": "Submitted to BMVC 2025", "pdf_url": "http://arxiv.org/pdf/2506.24092v1", "categories": ["cs.CV", "eess.IV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.24092v1", "date": "2025-06-25", "updated": "2025-06-25"}
{"id": "2506.23869", "title": "Scaling Self-Supervised Representation Learning for Symbolic Piano Performance", "authors": ["Louis Bradshaw", "Honglu Fan", "Alexander Spangher", "Stella Biderman", "Simon Colton"], "summary": "We study the capabilities of generative autoregressive transformer models\ntrained on large amounts of symbolic solo-piano transcriptions. After first\npretraining on approximately 60,000 hours of music, we use a comparatively\nsmaller, high-quality subset, to finetune models to produce musical\ncontinuations, perform symbolic classification tasks, and produce\ngeneral-purpose contrastive MIDI embeddings by adapting the SimCLR framework to\nsymbolic music. When evaluating piano continuation coherence, our generative\nmodel outperforms leading symbolic generation techniques and remains\ncompetitive with proprietary audio generation models. On MIR classification\nbenchmarks, frozen representations from our contrastive model achieve\nstate-of-the-art results in linear probe experiments, while direct finetuning\ndemonstrates the generalizability of pretrained representations, often\nrequiring only a few hundred labeled examples to specialize to downstream\ntasks.", "comment": "ISMIR (2025)", "pdf_url": "http://arxiv.org/pdf/2506.23869v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.23869v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23678", "title": "Interactive Reasoning: Visualizing and Controlling Chain-of-Thought Reasoning in Large Language Models", "authors": ["Rock Yuren Pang", "K. J. Kevin Feng", "Shangbin Feng", "Chu Li", "Weijia Shi", "Yulia Tsvetkov", "Jeffrey Heer", "Katharina Reinecke"], "summary": "The output quality of large language models (LLMs) can be improved via\n\"reasoning\": generating segments of chain-of-thought (CoT) content to further\ncondition the model prior to producing user-facing output. While these chains\ncontain valuable information, they are verbose and lack explicit organization,\nmaking them tedious to review. Moreover, they lack opportunities for user\nfeedback, such as to remove unwanted considerations, add desired ones, or\nclarify unclear assumptions. We introduce Interactive Reasoning, an interaction\ndesign that visualizes chain-of-thought outputs as a hierarchy of topics and\nenables user review and modification. We implement interactive reasoning in\nHippo, a prototype for AI-assisted decision making in the face of uncertain\ntrade-offs. In a user study with 16 participants, we find that interactive\nreasoning in Hippo allows users to quickly identify and interrupt erroneous\ngenerations, efficiently steer the model towards customized responses, and\nbetter understand both model reasoning and model outputs. Our work contributes\nto a new paradigm that incorporates user oversight into LLM reasoning\nprocesses.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23678v1", "categories": ["cs.HC", "cs.AI"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.23678v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.24096", "title": "MILo: Mesh-In-the-Loop Gaussian Splatting for Detailed and Efficient Surface Reconstruction", "authors": ["Antoine Gu√©don", "Diego Gomez", "Nissim Maruani", "Bingchen Gong", "George Drettakis", "Maks Ovsjanikov"], "summary": "While recent advances in Gaussian Splatting have enabled fast reconstruction\nof high-quality 3D scenes from images, extracting accurate surface meshes\nremains a challenge. Current approaches extract the surface through costly\npost-processing steps, resulting in the loss of fine geometric details or\nrequiring significant time and leading to very dense meshes with millions of\nvertices. More fundamentally, the a posteriori conversion from a volumetric to\na surface representation limits the ability of the final mesh to preserve all\ngeometric structures captured during training. We present MILo, a novel\nGaussian Splatting framework that bridges the gap between volumetric and\nsurface representations by differentiably extracting a mesh from the 3D\nGaussians. We design a fully differentiable procedure that constructs the\nmesh-including both vertex locations and connectivity-at every iteration\ndirectly from the parameters of the Gaussians, which are the only quantities\noptimized during training. Our method introduces three key technical\ncontributions: a bidirectional consistency framework ensuring both\nrepresentations-Gaussians and the extracted mesh-capture the same underlying\ngeometry during training; an adaptive mesh extraction process performed at each\ntraining iteration, which uses Gaussians as differentiable pivots for Delaunay\ntriangulation; a novel method for computing signed distance values from the 3D\nGaussians that enables precise surface extraction while avoiding geometric\nerosion. Our approach can reconstruct complete scenes, including backgrounds,\nwith state-of-the-art quality while requiring an order of magnitude fewer mesh\nvertices than previous methods. Due to their light weight and empty interior,\nour meshes are well suited for downstream applications such as physics\nsimulations or animation.", "comment": "10 pages. A presentation video of our approach is available at\n  https://youtu.be/_SGNhhNz0fE", "pdf_url": "http://arxiv.org/pdf/2506.24096v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.24096v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23873", "title": "Emergent musical properties of a transformer under contrastive self-supervised learning", "authors": ["Yuexuan Kong", "Gabriel Meseguer-Brocal", "Vincent Lostanlen", "Mathieu Lagrange", "Romain Hennequin"], "summary": "In music information retrieval (MIR), contrastive self-supervised learning\nfor general-purpose representation models is effective for global tasks such as\nautomatic tagging. However, for local tasks such as chord estimation, it is\nwidely assumed that contrastively trained general-purpose self-supervised\nmodels are inadequate and that more sophisticated SSL is necessary; e.g.,\nmasked modeling. Our paper challenges this assumption by revealing the\npotential of contrastive SSL paired with a transformer in local MIR tasks. We\nconsider a lightweight vision transformer with one-dimensional patches in the\ntime--frequency domain (ViT-1D) and train it with simple contrastive SSL\nthrough normalized temperature-scaled cross-entropy loss (NT-Xent). Although\nNT-Xent operates only over the class token, we observe that, potentially thanks\nto weight sharing, informative musical properties emerge in ViT-1D's sequence\ntokens. On global tasks, the temporal average of class and sequence tokens\noffers a performance increase compared to the class token alone, showing useful\nproperties in the sequence tokens. On local tasks, sequence tokens perform\nunexpectedly well, despite not being specifically trained for. Furthermore,\nhigh-level musical features such as onsets emerge from layer-wise attention\nmaps and self-similarity matrices show different layers capture different\nmusical dimensions. Our paper does not focus on improving performance but\nadvances the musical interpretation of transformers and sheds light on some\noverlooked abilities of contrastive SSL paired with transformers for sequence\nmodeling in MIR.", "comment": "Accepted at ISMIR 2025", "pdf_url": "http://arxiv.org/pdf/2506.23873v1", "categories": ["cs.SD", "cs.IR", "cs.LG", "eess.AS"], "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.23873v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23679", "title": "Learning Modular Exponentiation with Transformers", "authors": ["David Demitri Africa", "Sara M. Kapoor", "Theo Simon Sorg"], "summary": "Modular exponentiation is crucial to number theory and cryptography, yet\nremains largely unexplored from a mechanistic interpretability standpoint. We\ntrain a 4-layer encoder-decoder Transformer model to perform this operation and\ninvestigate the emergence of numerical reasoning during training. Utilizing\nprincipled sampling strategies, PCA-based embedding analysis, and activation\npatching, we examine how number-theoretic properties are encoded within the\nmodel. We find that reciprocal operand training leads to strong performance\ngains, with sudden generalization across related moduli. These synchronized\naccuracy surges reflect grokking-like dynamics, suggesting the model\ninternalizes shared arithmetic structure. We also find a subgraph consisting\nentirely of attention heads in the final layer sufficient to achieve full\nperformance on the task of regular exponentiation. These results suggest that\ntransformer models learn modular arithmetic through specialized computational\ncircuits, paving the way for more interpretable and efficient neural approaches\nto modular exponentiation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23679v1", "categories": ["cs.LG", "cs.AI", "cs.CR"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.23679v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.24102", "title": "DenseWorld-1M: Towards Detailed Dense Grounded Caption in the Real World", "authors": ["Xiangtai Li", "Tao Zhang", "Yanwei Li", "Haobo Yuan", "Shihao Chen", "Yikang Zhou", "Jiahao Meng", "Yueyi Sun", "Shilin Xu", "Lu Qi", "Tianheng Cheng", "Yi Lin", "Zilong Huang", "Wenhao Huang", "Jiashi Feng", "Guang Shi"], "summary": "Multimodal Large Language Models (MLLMs) demonstrate a complex understanding\nof scenes, benefiting from large-scale and high-quality datasets. Most existing\ncaption datasets lack the ground locations and relations for visual entities.\nSeveral grounded caption datasets face the problems of missing detailed\ndescriptions, relations, and massive object descriptions on high-resolution\nimages. To fill this gap for the community, we present DenseWorld-1M, the first\nmassive, detailed, dense grounded caption dataset in the real world. We design\na three-stage labeling pipeline, containing open-world perception, detailed\nobject caption generation, and dense caption merging. The first stage obtains\nentity-level masks and labels. The second stage generates the object-level,\ndetailed captions with the guidance of masks and labels from the first stage.\nThe final stage merges object captions and masks into spatial and relational\ndense captions. To accelerate the labeling process and improve caption quality,\nwe present two VLM models: the Detailed Region Caption model and the Spatial\nCaption Merging model. Extensive experiments on various settings, including\nvision-language understanding, visual grounding, and region caption generation,\ndemonstrate the effectiveness of our DenseWorld-1M dataset and labeling models.", "comment": "Datasets and Models: https://github.com/lxtGH/DenseWorld-1M", "pdf_url": "http://arxiv.org/pdf/2506.24102v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.24102v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23881", "title": "Spurious-Aware Prototype Refinement for Reliable Out-of-Distribution Detection", "authors": ["Reihaneh Zohrabi", "Hosein Hasani", "Mahdieh Soleymani Baghshah", "Anna Rohrbach", "Marcus Rohrbach", "Mohammad Hossein Rohban"], "summary": "Out-of-distribution (OOD) detection is crucial for ensuring the reliability\nand safety of machine learning models in real-world applications, where they\nfrequently face data distributions unseen during training. Despite progress,\nexisting methods are often vulnerable to spurious correlations that mislead\nmodels and compromise robustness. To address this, we propose SPROD, a novel\nprototype-based OOD detection approach that explicitly addresses the challenge\nposed by unknown spurious correlations. Our post-hoc method refines class\nprototypes to mitigate bias from spurious features without additional data or\nhyperparameter tuning, and is broadly applicable across diverse backbones and\nOOD detection settings. We conduct a comprehensive spurious correlation OOD\ndetection benchmarking, comparing our method against existing approaches and\ndemonstrating its superior performance across challenging OOD datasets, such as\nCelebA, Waterbirds, UrbanCars, Spurious Imagenet, and the newly introduced\nAnimals MetaCoCo. On average, SPROD improves AUROC by 4.7% and FPR@95 by 9.3%\nover the second best.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23881v1", "categories": ["cs.CV", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23881v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23717", "title": "Towards Efficient and Accurate Spiking Neural Networks via Adaptive Bit Allocation", "authors": ["Xingting Yao", "Qinghao Hu", "Fei Zhou", "Tielong Liu", "Gang Li", "Peisong Wang", "Jian Cheng"], "summary": "Multi-bit spiking neural networks (SNNs) have recently become a heated\nresearch spot, pursuing energy-efficient and high-accurate AI. However, with\nmore bits involved, the associated memory and computation demands escalate to\nthe point where the performance improvements become disproportionate. Based on\nthe insight that different layers demonstrate different importance and extra\nbits could be wasted and interfering, this paper presents an adaptive bit\nallocation strategy for direct-trained SNNs, achieving fine-grained layer-wise\nallocation of memory and computation resources. Thus, SNN's efficiency and\naccuracy can be improved. Specifically, we parametrize the temporal lengths and\nthe bit widths of weights and spikes, and make them learnable and controllable\nthrough gradients. To address the challenges caused by changeable bit widths\nand temporal lengths, we propose the refined spiking neuron, which can handle\ndifferent temporal lengths, enable the derivation of gradients for temporal\nlengths, and suit spike quantization better. In addition, we theoretically\nformulate the step-size mismatch problem of learnable bit widths, which may\nincur severe quantization errors to SNN, and accordingly propose the step-size\nrenewal mechanism to alleviate this issue. Experiments on various datasets,\nincluding the static CIFAR and ImageNet and the dynamic CIFAR-DVS and\nDVS-GESTURE, demonstrate that our methods can reduce the overall memory and\ncomputation cost while achieving higher accuracy. Particularly, our\nSEWResNet-34 can achieve a 2.69\\% accuracy gain and 4.16$\\times$ lower bit\nbudgets over the advanced baseline work on ImageNet. This work will be fully\nopen-sourced.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23717v1", "categories": ["cs.NE", "cs.AI", "cs.CV", "cs.LG"], "cate": "cs.NE", "url": "http://arxiv.org/abs/2506.23717v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.24113", "title": "Epona: Autoregressive Diffusion World Model for Autonomous Driving", "authors": ["Kaiwen Zhang", "Zhenyu Tang", "Xiaotao Hu", "Xingang Pan", "Xiaoyang Guo", "Yuan Liu", "Jingwei Huang", "Li Yuan", "Qian Zhang", "Xiao-Xiao Long", "Xun Cao", "Wei Yin"], "summary": "Diffusion models have demonstrated exceptional visual quality in video\ngeneration, making them promising for autonomous driving world modeling.\nHowever, existing video diffusion-based world models struggle with\nflexible-length, long-horizon predictions and integrating trajectory planning.\nThis is because conventional video diffusion models rely on global joint\ndistribution modeling of fixed-length frame sequences rather than sequentially\nconstructing localized distributions at each timestep. In this work, we propose\nEpona, an autoregressive diffusion world model that enables localized\nspatiotemporal distribution modeling through two key innovations: 1) Decoupled\nspatiotemporal factorization that separates temporal dynamics modeling from\nfine-grained future world generation, and 2) Modular trajectory and video\nprediction that seamlessly integrate motion planning with visual modeling in an\nend-to-end framework. Our architecture enables high-resolution, long-duration\ngeneration while introducing a novel chain-of-forward training strategy to\naddress error accumulation in autoregressive loops. Experimental results\ndemonstrate state-of-the-art performance with 7.4\\% FVD improvement and minutes\nlonger prediction duration compared to prior works. The learned world model\nfurther serves as a real-time motion planner, outperforming strong end-to-end\nplanners on NAVSIM benchmarks. Code will be publicly available at\n\\href{https://github.com/Kevin-thu/Epona/}{https://github.com/Kevin-thu/Epona/}.", "comment": "ICCV2025, Project Page: https://kevin-thu.github.io/Epona/", "pdf_url": "http://arxiv.org/pdf/2506.24113v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.24113v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23908", "title": "Beyond Statistical Learning: Exact Learning Is Essential for General Intelligence", "authors": ["Andr√°s Gy√∂rgy", "Tor Lattimore", "Nevena Laziƒá", "Csaba Szepesv√°ri"], "summary": "Sound deductive reasoning -- the ability to derive new knowledge from\nexisting facts and rules -- is an indisputably desirable aspect of general\nintelligence. Despite the major advances of AI systems in areas such as math\nand science, especially since the introduction of transformer architectures, it\nis well-documented that even the most advanced frontier systems regularly and\nconsistently falter on easily-solvable deductive reasoning tasks. Hence, these\nsystems are unfit to fulfill the dream of achieving artificial general\nintelligence capable of sound deductive reasoning. We argue that their unsound\nbehavior is a consequence of the statistical learning approach powering their\ndevelopment. To overcome this, we contend that to achieve reliable deductive\nreasoning in learning-based AI systems, researchers must fundamentally shift\nfrom optimizing for statistical performance against distributions on reasoning\nproblems and algorithmic tasks to embracing the more ambitious exact learning\nparadigm, which demands correctness on all inputs. We argue that exact learning\nis both essential and possible, and that this ambitious objective should guide\nalgorithm design.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23908v1", "categories": ["cs.AI", "cs.LG"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.23908v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23719", "title": "DABstep: Data Agent Benchmark for Multi-step Reasoning", "authors": ["Alex Egg", "Martin Iglesias Goyanes", "Friso Kingma", "Andreu Mora", "Leandro von Werra", "Thomas Wolf"], "summary": "We introduce DABstep, a novel benchmark for evaluating AI agents on realistic\nmulti-step data analysis tasks. DABstep comprises over 450 real-world\nchallenges derived from a financial analytics platform, requiring models to\ncombine code-based data processing with contextual reasoning over heterogeneous\ndocumentation. Each task demands an iterative, multi-step problem-solving\napproach, testing capabilities in data manipulation, cross-referencing multiple\nsources, and precise result reporting. The benchmark provides a factoid-style\nanswer format with automatic correctness checks for objective scoring at scale.\nWe evaluate leading LLM-based agents, revealing a substantial performance gap:\neven the best agent achieves only 14.55% accuracy on the hardest tasks. We\ndetail our benchmark's design, dataset composition, task formulation,\nevaluation protocol, report baseline results and analyze failure modes. DABstep\nis released with a public leaderboard and toolkit to accelerate research in\nautonomous data analysis.", "comment": "13 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2506.23719v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.23719v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.24121", "title": "TextMesh4D: High-Quality Text-to-4D Mesh Generation", "authors": ["Sisi Dai", "Xinxin Su", "Boyan Wan", "Ruizhen Hu", "Kai Xu"], "summary": "Recent advancements in diffusion generative models significantly advanced\nimage, video, and 3D content creation from user-provided text prompts. However,\nthe challenging problem of dynamic 3D content generation (text-to-4D) with\ndiffusion guidance remains largely unexplored. In this paper, we introduce\nTextMesh4D, a novel framework for high-quality text-to-4D generation. Our\napproach leverages per-face Jacobians as a differentiable mesh representation\nand decomposes 4D generation into two stages: static object creation and\ndynamic motion synthesis. We further propose a flexibility-rigidity\nregularization term to stabilize Jacobian optimization under video diffusion\npriors, ensuring robust geometric performance. Experiments demonstrate that\nTextMesh4D achieves state-of-the-art results in terms of temporal consistency,\nstructural fidelity, and visual realism. Moreover, TextMesh4D operates with a\nlow GPU memory overhead-requiring only a single 24GB GPU-offering a\ncost-effective yet high-quality solution for text-driven 4D mesh generation.\nThe code will be released to facilitate future research in text-to-4D\ngeneration.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.24121v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.24121v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23909", "title": "RawMal-TF: Raw Malware Dataset Labeled by Type and Family", "authors": ["David B√°lik", "Martin Jureƒçek", "Mark Stamp"], "summary": "This work addresses the challenge of malware classification using machine\nlearning by developing a novel dataset labeled at both the malware type and\nfamily levels. Raw binaries were collected from sources such as VirusShare, VX\nUnderground, and MalwareBazaar, and subsequently labeled with family\ninformation parsed from binary names and type-level labels integrated from\nClarAVy. The dataset includes 14 malware types and 17 malware families, and was\nprocessed using a unified feature extraction pipeline based on static analysis,\nparticularly extracting features from Portable Executable headers, to support\nadvanced classification tasks. The evaluation was focused on three key\nclassification tasks. In the binary classification of malware versus benign\nsamples, Random Forest and XGBoost achieved high accuracy on the full datasets,\nreaching 98.5% for type-based detection and 98.98% for family-based detection.\nWhen using truncated datasets of 1,000 samples to assess performance under\nlimited data conditions, both models still performed strongly, achieving 97.6%\nfor type-based detection and 98.66% for family-based detection. For interclass\nclassification, which distinguishes between malware types or families, the\nmodels reached up to 97.5% accuracy on type-level tasks and up to 93.7% on\nfamily-level tasks. In the multiclass classification setting, which assigns\nsamples to the correct type or family, SVM achieved 81.1% accuracy on type\nlabels, while Random Forest and XGBoost reached approximately 73.4% on family\nlabels. The results highlight practical trade-offs between accuracy and\ncomputational cost, and demonstrate that labeling at both the type and family\nlevels enables more fine-grained and insightful malware classification. The\nwork establishes a robust foundation for future research on advanced malware\ndetection and classification.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23909v1", "categories": ["cs.CR", "cs.LG"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.23909v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23721", "title": "Deep Learning-Based Semantic Segmentation for Real-Time Kidney Imaging and Measurements with Augmented Reality-Assisted Ultrasound", "authors": ["Gijs Luijten", "Roberto Maria Scardigno", "Lisle Faray de Paiva", "Peter Hoyer", "Jens Kleesiek", "Domenico Buongiorno", "Vitoantonio Bevilacqua", "Jan Egger"], "summary": "Ultrasound (US) is widely accessible and radiation-free but has a steep\nlearning curve due to its dynamic nature and non-standard imaging planes.\nAdditionally, the constant need to shift focus between the US screen and the\npatient poses a challenge. To address these issues, we integrate deep learning\n(DL)-based semantic segmentation for real-time (RT) automated kidney volumetric\nmeasurements, which are essential for clinical assessment but are traditionally\ntime-consuming and prone to fatigue. This automation allows clinicians to\nconcentrate on image interpretation rather than manual measurements.\nComplementing DL, augmented reality (AR) enhances the usability of US by\nprojecting the display directly into the clinician's field of view, improving\nergonomics and reducing the cognitive load associated with screen-to-patient\ntransitions. Two AR-DL-assisted US pipelines on HoloLens-2 are proposed: one\nstreams directly via the application programming interface for a wireless\nsetup, while the other supports any US device with video output for broader\naccessibility. We evaluate RT feasibility and accuracy using the Open Kidney\nDataset and open-source segmentation models (nnU-Net, Segmenter, YOLO with\nMedSAM and LiteMedSAM). Our open-source GitHub pipeline includes model\nimplementations, measurement algorithms, and a Wi-Fi-based streaming solution,\nenhancing US training and diagnostics, especially in point-of-care settings.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23721v1", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.HC", "cs.LG"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.23721v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.24123", "title": "Calligrapher: Freestyle Text Image Customization", "authors": ["Yue Ma", "Qingyan Bai", "Hao Ouyang", "Ka Leong Cheng", "Qiuyu Wang", "Hongyu Liu", "Zichen Liu", "Haofan Wang", "Jingye Chen", "Yujun Shen", "Qifeng Chen"], "summary": "We introduce Calligrapher, a novel diffusion-based framework that\ninnovatively integrates advanced text customization with artistic typography\nfor digital calligraphy and design applications. Addressing the challenges of\nprecise style control and data dependency in typographic customization, our\nframework incorporates three key technical contributions. First, we develop a\nself-distillation mechanism that leverages the pre-trained text-to-image\ngenerative model itself alongside the large language model to automatically\nconstruct a style-centric typography benchmark. Second, we introduce a\nlocalized style injection framework via a trainable style encoder, which\ncomprises both Qformer and linear layers, to extract robust style features from\nreference images. An in-context generation mechanism is also employed to\ndirectly embed reference images into the denoising process, further enhancing\nthe refined alignment of target styles. Extensive quantitative and qualitative\nevaluations across diverse fonts and design contexts confirm Calligrapher's\naccurate reproduction of intricate stylistic details and precise glyph\npositioning. By automating high-quality, visually consistent typography,\nCalligrapher surpasses traditional models, empowering creative practitioners in\ndigital art, branding, and contextual typographic design.", "comment": "Project page: https://calligrapher2025.github.io/Calligrapher Code:\n  https://github.com/Calligrapher2025/Calligrapher", "pdf_url": "http://arxiv.org/pdf/2506.24123v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.24123v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23914", "title": "Learning robust parameter inference and density reconstruction in flyer plate impact experiments", "authors": ["Evan Bell", "Daniel A. Serino", "Ben S. Southworth", "Trevor Wilcox", "Marc L. Klasky"], "summary": "Estimating physical parameters or material properties from experimental\nobservations is a common objective in many areas of physics and material\nscience. In many experiments, especially in shock physics, radiography is the\nprimary means of observing the system of interest. However, radiography does\nnot provide direct access to key state variables, such as density, which\nprevents the application of traditional parameter estimation approaches. Here\nwe focus on flyer plate impact experiments on porous materials, and resolving\nthe underlying parameterized equation of state (EoS) and crush porosity model\nparameters given radiographic observation(s). We use machine learning as a tool\nto demonstrate with high confidence that using only high impact velocity data\ndoes not provide sufficient information to accurately infer both EoS and crush\nmodel parameters, even with fully resolved density fields or a dynamic sequence\nof images. We thus propose an observable data set consisting of low and high\nimpact velocity experiments/simulations that capture different regimes of\ncompaction and shock propagation, and proceed to introduce a generative machine\nlearning approach which produces a posterior distribution of physical\nparameters directly from radiographs. We demonstrate the effectiveness of the\napproach in estimating parameters from simulated flyer plate impact\nexperiments, and show that the obtained estimates of EoS and crush model\nparameters can then be used in hydrodynamic simulations to obtain accurate and\nphysically admissible density reconstructions. Finally, we examine the\nrobustness of the approach to model mismatches, and find that the learned\napproach can provide useful parameter estimates in the presence of\nout-of-distribution radiographic noise and previously unseen physics, thereby\npromoting a potential breakthrough in estimating material properties from\nexperimental radiographic images.", "comment": "24 pages, 21 figures", "pdf_url": "http://arxiv.org/pdf/2506.23914v1", "categories": ["physics.comp-ph", "cs.LG"], "cate": "physics.comp-ph", "url": "http://arxiv.org/abs/2506.23914v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23724", "title": "When Small Guides Large: Cross-Model Co-Learning for Test-Time Adaptation", "authors": ["Chang'an Yi", "Xiaohui Deng", "Guohao Chen", "Yan Zhou", "Qinghua Lu", "Shuaicheng Niu"], "summary": "Test-time Adaptation (TTA) adapts a given model to testing domain data with\npotential domain shifts through online unsupervised learning, yielding\nimpressive performance. However, to date, existing TTA methods primarily focus\non single-model adaptation. In this work, we investigate an intriguing\nquestion: how does cross-model knowledge influence the TTA process? Our\nfindings reveal that, in TTA's unsupervised online setting, each model can\nprovide complementary, confident knowledge to the others, even when there are\nsubstantial differences in model size. For instance, a smaller model like\nMobileViT (10.6M parameters) can effectively guide a larger model like ViT-Base\n(86.6M parameters). In light of this, we propose COCA, a Cross-Model\nCo-Learning framework for TTA, which mainly consists of two main strategies. 1)\nCo-adaptation adaptively integrates complementary knowledge from other models\nthroughout the TTA process, reducing individual model biases. 2)\nSelf-adaptation enhances each model's unique strengths via unsupervised\nlearning, enabling diverse adaptation to the target domain. Extensive\nexperiments show that COCA, which can also serve as a plug-and-play module,\nsignificantly boosts existing SOTAs, on models with various sizes--including\nResNets, ViTs, and Mobile-ViTs--via cross-model co-learned TTA. For example,\nwith Mobile-ViT's guidance, COCA raises ViT-Base's average adaptation accuracy\non ImageNet-C from 51.7% to 64.5%. The code is publicly available at\nhttps://github.com/ycarobot/COCA.", "comment": "15 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2506.23724v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23724v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.24125", "title": "FADRM: Fast and Accurate Data Residual Matching for Dataset Distillation", "authors": ["Jiacheng Cui", "Xinyue Bi", "Yaxin Luo", "Xiaohan Zhao", "Jiacheng Liu", "Zhiqiang Shen"], "summary": "Residual connection has been extensively studied and widely applied at the\nmodel architecture level. However, its potential in the more challenging\ndata-centric approaches remains unexplored. In this work, we introduce the\nconcept of Data Residual Matching for the first time, leveraging data-level\nskip connections to facilitate data generation and mitigate data information\nvanishing. This approach maintains a balance between newly acquired knowledge\nthrough pixel space optimization and existing core local information\nidentification within raw data modalities, specifically for the dataset\ndistillation task. Furthermore, by incorporating optimization-level\nrefinements, our method significantly improves computational efficiency,\nachieving superior performance while reducing training time and peak GPU memory\nusage by 50%. Consequently, the proposed method Fast and Accurate Data Residual\nMatching for Dataset Distillation (FADRM) establishes a new state-of-the-art,\ndemonstrating substantial improvements over existing methods across multiple\ndataset benchmarks in both efficiency and effectiveness. For instance, with\nResNet-18 as the student model and a 0.8% compression ratio on ImageNet-1K, the\nmethod achieves 47.7% test accuracy in single-model dataset distillation and\n50.0% in multi-model dataset distillation, surpassing RDED by +5.7% and\noutperforming state-of-the-art multi-model approaches, EDC and CV-DD, by +1.4%\nand +4.0%. Code is available at: https://github.com/Jiacheng8/FADRM.", "comment": "Code at: https://github.com/Jiacheng8/FADRM", "pdf_url": "http://arxiv.org/pdf/2506.24125v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.24125v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23921", "title": "The Trilemma of Truth in Large Language Models", "authors": ["Germans Savcisens", "Tina Eliassi-Rad"], "summary": "We often attribute human characteristics to large language models (LLMs) and\nclaim that they \"know\" certain things. LLMs have an internal probabilistic\nknowledge that represents information retained during training. How can we\nassess the veracity of this knowledge? We examine two common methods for\nprobing the veracity of LLMs and discover several assumptions that are flawed.\nTo address these flawed assumptions, we introduce sAwMIL (short for Sparse\nAware Multiple-Instance Learning), a probing method that utilizes the internal\nactivations of LLMs to separate statements into true, false, and neither.\nsAwMIL is based on multiple-instance learning and conformal prediction. We\nevaluate sAwMIL on 5 validity criteria across 16 open-source LLMs, including\nboth default and chat-based variants, as well as on 3 new datasets. Among the\ninsights we provide are: (1) the veracity signal is often concentrated in the\nthird quarter of an LLM's depth; (2) truth and falsehood signals are not always\nsymmetric; (3) linear probes perform better on chat models than on default\nmodels; (4) nonlinear probes may be required to capture veracity signals for\nsome LLMs with reinforcement learning from human feedback or knowledge\ndistillation; and (5) LLMs capture a third type of signal that is distinct from\ntrue and false and is neither true nor false. These findings provide a reliable\nmethod for verifying what LLMs \"know\" and how certain they are of their\nprobabilistic internal knowledge.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23921v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.23921v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23725", "title": "PAC Bench: Do Foundation Models Understand Prerequisites for Executing Manipulation Policies?", "authors": ["Atharva Gundawar", "Som Sagar", "Ransalu Senanayake"], "summary": "Vision-Language Models (VLMs) are increasingly pivotal for generalist robot\nmanipulation, enabling tasks such as physical reasoning, policy generation, and\nfailure detection. However, their proficiency in these high-level applications\noften assumes a deep understanding of low-level physical prerequisites, a\ncapability that remains largely unverified. For robots to perform actions\nreliably, they must comprehend intrinsic object properties (e.g., material,\nweight), action affordances (e.g., graspable, stackable), and physical\nconstraints (e.g., stability, reachability, or an object's state, such as being\nclosed). Despite the widespread use of VLMs in manipulation tasks, we argue\nthat off-the-shelf models may lack this granular, physically grounded\nunderstanding, as such prerequisites are often overlooked during training.\n  To address this critical gap, we introduce PAC Bench, a comprehensive\nbenchmark designed to systematically evaluate VLMs on their understanding of\ncore Properties, Affordances, and Constraints (PAC) from a task executability\nperspective. PAC Bench features a diverse dataset with over 30,000 annotations,\ncomprising 673 real-world images (115 object classes, 15 property types, and 1\nto 3 affordances defined per class), 100 real-world humanoid-view scenarios,\nand 120 unique simulated constraint scenarios across four tasks.\n  Our evaluations reveal significant gaps in the ability of current VLMs to\ngrasp fundamental physical concepts, highlighting limitations in their\nsuitability for reliable robot manipulation and pointing to key areas for\ntargeted research. PAC Bench also serves as a standardized benchmark for\nrigorously evaluating physical reasoning in VLMs and guiding the development of\nmore robust, physically grounded models for robotic applications.\n  Project Page: https://pacbench.github.io/", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23725v1", "categories": ["cs.RO", "cs.AI"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.23725v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.24127", "title": "How to Design and Train Your Implicit Neural Representation for Video Compression", "authors": ["Matthew Gwilliam", "Roy Zhang", "Namitha Padmanabhan", "Hongyang Du", "Abhinav Shrivastava"], "summary": "Implicit neural representation (INR) methods for video compression have\nrecently achieved visual quality and compression ratios that are competitive\nwith traditional pipelines. However, due to the need for per-sample network\ntraining, the encoding speeds of these methods are too slow for practical\nadoption. We develop a library to allow us to disentangle and review the\ncomponents of methods from the NeRV family, reframing their performance in\nterms of not only size-quality trade-offs, but also impacts on training time.\nWe uncover principles for effective video INR design and propose a\nstate-of-the-art configuration of these components, Rabbit NeRV (RNeRV). When\nall methods are given equal training time (equivalent to 300 NeRV epochs) for 7\ndifferent UVG videos at 1080p, RNeRV achieves +1.27% PSNR on average compared\nto the best-performing alternative for each video in our NeRV library. We then\ntackle the encoding speed issue head-on by investigating the viability of\nhyper-networks, which predict INR weights from video inputs, to disentangle\ntraining from encoding to allow for real-time encoding. We propose masking the\nweights of the predicted INR during training to allow for variable, higher\nquality compression, resulting in 1.7% improvements to both PSNR and MS-SSIM at\n0.037 bpp on the UCF-101 dataset, and we increase hyper-network parameters by\n0.4% for 2.5%/2.7% improvements to PSNR/MS-SSIM with equal bpp and similar\nspeeds. Our project website is available at https://mgwillia.github.io/vinrb/\nand our code is available at https://github.com/mgwillia/vinrb.", "comment": "21 pages, 41 figures, 5 tables", "pdf_url": "http://arxiv.org/pdf/2506.24127v1", "categories": ["cs.CV"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.24127v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23926", "title": "Industrial brain: a human-like autonomous neuro-symbolic cognitive decision-making system", "authors": ["Junping Wang", "Bicheng Wang", "Yibo Xuea", "Yuan Xie"], "summary": "Resilience non-equilibrium measurement, the ability to maintain fundamental\nfunctionality amidst failures and errors, is crucial for scientific management\nand engineering applications of industrial chain. The problem is particularly\nchallenging when the number or types of multiple co-evolution of resilience\n(for example, randomly placed) are extremely chaos. Existing end-to-end deep\nlearning ordinarily do not generalize well to unseen full-feld reconstruction\nof spatiotemporal co-evolution structure, and predict resilience of network\ntopology, especially in multiple chaos data regimes typically seen in\nreal-world applications. To address this challenge, here we propose industrial\nbrain, a human-like autonomous cognitive decision-making and planning framework\nintegrating higher-order activity-driven neuro network and CT-OODA symbolic\nreasoning to autonomous plan resilience directly from observational data of\nglobal variable. The industrial brain not only understands and model structure\nof node activity dynamics and network co-evolution topology without simplifying\nassumptions, and reveal the underlying laws hidden behind complex networks, but\nalso enabling accurate resilience prediction, inference, and planning.\nExperimental results show that industrial brain significantly outperforms\nresilience prediction and planning methods, with an accurate improvement of up\nto 10.8\\% over GoT and OlaGPT framework and 11.03\\% over spectral dimension\nreduction. It also generalizes to unseen topologies and dynamics and maintains\nrobust performance despite observational disturbances. Our findings suggest\nthat industrial brain addresses an important gap in resilience prediction and\nplanning for industrial chain.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23926v1", "categories": ["cs.AI", "cs.LG"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.23926v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23726", "title": "System-Embedded Diffusion Bridge Models", "authors": ["Bartlomiej Sobieski", "Matthew Tivnan", "Yuang Wang", "Siyeop Yoon", "Pengfei Jin", "Dufan Wu", "Quanzheng Li", "Przemyslaw Biecek"], "summary": "Solving inverse problems -- recovering signals from incomplete or noisy\nmeasurements -- is fundamental in science and engineering. Score-based\ngenerative models (SGMs) have recently emerged as a powerful framework for this\ntask. Two main paradigms have formed: unsupervised approaches that adapt\npretrained generative models to inverse problems, and supervised bridge methods\nthat train stochastic processes conditioned on paired clean and corrupted data.\nWhile the former typically assume knowledge of the measurement model, the\nlatter have largely overlooked this structural information. We introduce System\nembedded Diffusion Bridge Models (SDBs), a new class of supervised bridge\nmethods that explicitly embed the known linear measurement system into the\ncoefficients of a matrix-valued SDE. This principled integration yields\nconsistent improvements across diverse linear inverse problems and demonstrates\nrobust generalization under system misspecification between training and\ndeployment, offering a promising solution to real-world applications.", "comment": "Preprint", "pdf_url": "http://arxiv.org/pdf/2506.23726v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.23726v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.21629", "title": "ICP-3DGS: SfM-free 3D Gaussian Splatting for Large-scale Unbounded Scenes", "authors": ["Chenhao Zhang", "Yezhi Shen", "Fengqing Zhu"], "summary": "In recent years, neural rendering methods such as NeRFs and 3D Gaussian\nSplatting (3DGS) have made significant progress in scene reconstruction and\nnovel view synthesis. However, they heavily rely on preprocessed camera poses\nand 3D structural priors from structure-from-motion (SfM), which are\nchallenging to obtain in outdoor scenarios. To address this challenge, we\npropose to incorporate Iterative Closest Point (ICP) with optimization-based\nrefinement to achieve accurate camera pose estimation under large camera\nmovements. Additionally, we introduce a voxel-based scene densification\napproach to guide the reconstruction in large-scale scenes. Experiments\ndemonstrate that our approach ICP-3DGS outperforms existing methods in both\ncamera pose estimation and novel view synthesis across indoor and outdoor\nscenes of various scales. Source code is available at\nhttps://github.com/Chenhao-Z/ICP-3DGS.", "comment": "6 pages, Source code is available at\n  https://github.com/Chenhao-Z/ICP-3DGS. To appear at ICIP 2025", "pdf_url": "http://arxiv.org/pdf/2506.21629v1", "categories": ["cs.GR", "cs.CV"], "cate": "cs.GR", "url": "http://arxiv.org/abs/2506.21629v1", "date": "2025-06-24", "updated": "2025-06-24"}
{"id": "2506.23934", "title": "QPART: Adaptive Model Quantization and Dynamic Workload Balancing for Accuracy-aware Edge Inference", "authors": ["Xiangchen Li", "Saeid Ghafouri", "Bo Ji", "Hans Vandierendonck", "Deepu John", "Dimitrios S. Nikolopoulos"], "summary": "As machine learning inferences increasingly move to edge devices, adapting to\ndiverse computational capabilities, hardware, and memory constraints becomes\nmore critical. Instead of relying on a pre-trained model fixed for all future\ninference queries across diverse edge devices, we argue that planning an\ninference pattern with a request-specific model tailored to the device's\ncomputational capacity, accuracy requirements, and time constraints is more\ncost-efficient and robust to diverse scenarios. To this end, we propose an\naccuracy-aware and workload-balanced inference system that integrates joint\nmodel quantization and inference partitioning. In this approach, the server\ndynamically responds to inference queries by sending a quantized model and\nadaptively sharing the inference workload with the device. Meanwhile, the\ndevice's computational power, channel capacity, and accuracy requirements are\nconsidered when deciding.\n  Furthermore, we introduce a new optimization framework for the inference\nsystem, incorporating joint model quantization and partitioning. Our approach\noptimizes layer-wise quantization bit width and partition points to minimize\ntime consumption and cost while accounting for varying accuracy requirements of\ntasks through an accuracy degradation metric in our optimization model. To our\nknowledge, this work represents the first exploration of optimizing\nquantization layer-wise bit-width in the inference serving system, by\nintroducing theoretical measurement of accuracy degradation. Simulation results\ndemonstrate a substantial reduction in overall time and power consumption, with\ncomputation payloads decreasing by over 80% and accuracy degradation kept below\n1%.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23934v1", "categories": ["cs.DC", "cs.AI", "cs.LG", "cs.PF"], "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.23934v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23734", "title": "Marker Gene Method : Identifying Stable Solutions in a Dynamic Environment", "authors": ["Hao Shi", "Xi Li", "Fangfang Xie"], "summary": "Competitive Co-evolutionary Algorithms (CCEAs) are often hampered by complex\ndynamics like intransitivity and the Red Queen effect, leading to unstable\nconvergence. To counter these challenges, this paper introduces the Marker Gene\nMethod (MGM), a framework that establishes stability by using a 'marker gene'\nas a dynamic benchmark and an adaptive weighting mechanism to balance\nexploration and exploitation. We provide rigorous mathematical proofs\ndemonstrating that MGM creates strong attractors near Nash Equilibria within\nthe Strictly Competitive Game framework. Empirically, MGM demonstrates its\nefficacy across a spectrum of challenges: it stabilizes the canonical\nRock-Paper-Scissors game, significantly improves the performance of C-RMOEA/D\non ZDT benchmarks, and, when augmented with a Memory Pool (MP) extension, it\nsuccessfully tames the notoriously pathological Shapley Biased Game. This work\npresents a theoretically sound and empirically validated framework that\nsubstantially enhances the stability and robustness of CCEAs in complex\ncompetitive environments.", "comment": "Submitted to IEEE Transactions on Evolutionary Computation. 13 pages,\n  10 figures. Supplementary material is included", "pdf_url": "http://arxiv.org/pdf/2506.23734v1", "categories": ["cs.NE", "cs.AI", "cs.GT"], "cate": "cs.NE", "url": "http://arxiv.org/abs/2506.23734v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22467", "title": "SegmentAnyMuscle: A universal muscle segmentation model across different locations in MRI", "authors": ["Roy Colglazier", "Jisoo Lee", "Haoyu Dong", "Hanxue Gu", "Yaqian Chen", "Joseph Cao", "Zafer Yildiz", "Zhonghao Liu", "Nicholas Konz", "Jichen Yang", "Jikai Zhang", "Yuwen Chen", "Lin Li", "Adrian Camarena", "Maciej A. Mazurowski"], "summary": "The quantity and quality of muscles are increasingly recognized as important\npredictors of health outcomes. While MRI offers a valuable modality for such\nassessments, obtaining precise quantitative measurements of musculature remains\nchallenging. This study aimed to develop a publicly available model for muscle\nsegmentation in MRIs and demonstrate its applicability across various\nanatomical locations and imaging sequences. A total of 362 MRIs from 160\npatients at a single tertiary center (Duke University Health System, 2016-2020)\nwere included, with 316 MRIs from 114 patients used for model development. The\nmodel was tested on two separate sets: one with 28 MRIs representing common\nsequence types, achieving an average Dice Similarity Coefficient (DSC) of\n88.45%, and another with 18 MRIs featuring less frequent sequences and\nabnormalities such as muscular atrophy, hardware, and significant noise,\nachieving 86.21% DSC. These results demonstrate the feasibility of a fully\nautomated deep learning algorithm for segmenting muscles on MRI across diverse\nsettings. The public release of this model enables consistent, reproducible\nresearch into the relationship between musculature and health.", "comment": "24 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2506.22467v1", "categories": ["eess.SP", "cs.CV"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.22467v1", "date": "2025-06-18", "updated": "2025-06-18"}
{"id": "2506.23952", "title": "Autonomy by Design: Preserving Human Autonomy in AI Decision-Support", "authors": ["Stefan Buijsman", "Sarah Carter", "Juan Pablo Berm√∫dez"], "summary": "AI systems increasingly support human decision-making across domains of\nprofessional, skill-based, and personal activity. While previous work has\nexamined how AI might affect human autonomy globally, the effects of AI on\ndomain-specific autonomy -- the capacity for self-governed action within\ndefined realms of skill or expertise -- remain understudied. We analyze how AI\ndecision-support systems affect two key components of domain-specific autonomy:\nskilled competence (the ability to make informed judgments within one's domain)\nand authentic value-formation (the capacity to form genuine domain-relevant\nvalues and preferences). By engaging with prior investigations and analyzing\nempirical cases across medical, financial, and educational domains, we\ndemonstrate how the absence of reliable failure indicators and the potential\nfor unconscious value shifts can erode domain-specific autonomy both\nimmediately and over time. We then develop a constructive framework for\nautonomy-preserving AI support systems. We propose specific socio-technical\ndesign patterns -- including careful role specification, implementation of\ndefeater mechanisms, and support for reflective practice -- that can help\nmaintain domain-specific autonomy while leveraging AI capabilities. This\nframework provides concrete guidance for developing AI systems that enhance\nrather than diminish human agency within specialized domains of action.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23952v1", "categories": ["cs.HC", "cs.AI", "cs.LG", "econ.GN", "q-fin.EC"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.23952v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23735", "title": "AutoEvoEval: An Automated Framework for Evolving Close-Ended LLM Evaluation Data", "authors": ["JiaRu Wu", "Mingwei Liu"], "summary": "Large language models (LLMs) have shown remarkable performance on various\ntasks, but existing evaluation benchmarks are often static and insufficient to\nfully assess their robustness and generalization in realistic scenarios. Prior\nwork using evolutionary or adversarial data augmentation has improved\nevaluation diversity but lacks systematic control over perturbation types and\nmulti-step complexity, limiting comprehensive robustness analysis. To address\nthese gaps, we propose AutoEvoEval, an evolution-based evaluation framework for\nclose-ended tasks such as multi-choice question answering. AutoEvoEval\nintroduces 22 interpretable atomic evolution operations and supports\nmulti-round compositions, enabling controlled generation of diverse,\nchallenging, and realistic test samples. We conduct extensive experiments\naddressing four research questions on a broad set of open- and closed-source\nLLMs. Our results show that atomic operations cause an average accuracy drop of\n7.283\\%, with structure-disrupting or misleading semantic edits causing the\nlargest declines. Model sensitivities vary significantly for the same\nperturbation, and combining multiple evolution steps amplifies adversarial\neffects by up to 52.932\\%. These findings suggest current benchmarks may\noverestimate true model generalization and emphasize the need for\nevolution-aware robustness evaluation. Code and resources are available at:\nhttps://github.com/SYSUSELab/AutoEvoEval.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23735v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.23735v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22482", "title": "Wireless Home Automation Using Social Networking Websites", "authors": ["Divya Alok Gupta", "Dwith Chenna", "B. Aditya Vighnesh Ramakanth"], "summary": "With the advent of Internet of Things, Wireless Home Automation Systems WHAS\nare gradually gaining popularity. These systems are faced with multiple\nchallenges such as security; controlling a variety of home appliances with a\nsingle interface and user friendliness. In this paper we propose a system that\nuses secure authentication systems of social networking websites such as\nTwitter, tracks the end-users activities on the social network and then control\nhis or her domestic appliances. At the end, we highlight the applications of\nthe proposed WHAS and compare the advantages of our proposed system over\ntraditional home automation systems.", "comment": "20th Annual International Conference on Advanced Computing and\n  Communications (ADCOM) 2014", "pdf_url": "http://arxiv.org/pdf/2506.22482v1", "categories": ["cs.NI", "cs.CR", "cs.CV"], "cate": "cs.NI", "url": "http://arxiv.org/abs/2506.22482v1", "date": "2025-06-23", "updated": "2025-06-23"}
{"id": "2506.23964", "title": "Learning Constraints Directly from Network Data", "authors": ["Hongyu H√®", "Minhao Jin", "Maria Apostolaki"], "summary": "Network data conforms to a wide range of rules that arise from protocols,\ndesign principles, and deployment decisions (e.g., a packet's queuing delay\nmust be less than its end-to-end delay). Formalizing such rules as logic\nconstraints can (i) improve the quality of synthetic data, (ii) reduce the\nbrittleness of machine learning (ML) models, and (iii) improve semantic\nunderstanding of network measurements. However, these benefits remain out of\nreach if rule extraction is manual or solely reliant on ML, as both approaches\nyield incomplete, unreliable, and/or inaccurate rules.\n  This paper formulates rule extraction as a constraint modeling problem and\nintroduces NetNomos that learns propositional logic constraints directly from\nraw network measurements. Constraint modeling in this domain is uniquely\nchallenging due to the scale of the data, the inherent learning complexity and\npassive environment, and the lack of ground truth supervision. NetNomos\naddresses these challenges via a lattice-based search structured by constraint\nspecificity and succinctness. Our approach reduces learning complexity from\nsuperquadratic to logarithmic and enables efficient traversal in combinatorial\nsearch space.\n  Our evaluations on diverse network datasets show that NetNomos learns all\nbenchmark rules, including those associated with as little as 0.01% of data\npoints, in under three hours. In contrast, baseline methods discover less than\n25% of the rules and require several days to run. Through three case studies,\nwe show that: NetNomos (i) finds rule violations in the outputs of all seven\nsynthetic traffic generators, hence can be used to assess and guide their\ngeneration process; (ii) detects semantic differences in traffic, hence can be\nused for anomaly detection; and (iii) automatically finds rules used for\ntelemetry imputation, hence can support monitoring through inference.", "comment": "13 pages, 15 figures", "pdf_url": "http://arxiv.org/pdf/2506.23964v1", "categories": ["cs.NI", "cs.LG", "C.2.3; I.2.6; I.2.3"], "cate": "cs.NI", "url": "http://arxiv.org/abs/2506.23964v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23762", "title": "Software Engineering for Large Language Models: Research Status, Challenges and the Road Ahead", "authors": ["Hongzhou Rao", "Yanjie Zhao", "Xinyi Hou", "Shenao Wang", "Haoyu Wang"], "summary": "The rapid advancement of large language models (LLMs) has redefined\nartificial intelligence (AI), pushing the boundaries of AI research and\nenabling unbounded possibilities for both academia and the industry. However,\nLLM development faces increasingly complex challenges throughout its lifecycle,\nyet no existing research systematically explores these challenges and solutions\nfrom the perspective of software engineering (SE) approaches. To fill the gap,\nwe systematically analyze research status throughout the LLM development\nlifecycle, divided into six phases: requirements engineering, dataset\nconstruction, model development and enhancement, testing and evaluation,\ndeployment and operations, and maintenance and evolution. We then conclude by\nidentifying the key challenges for each phase and presenting potential research\ndirections to address these challenges. In general, we provide valuable\ninsights from an SE perspective to facilitate future advances in LLM\ndevelopment.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23762v1", "categories": ["cs.SE", "cs.AI"], "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.23762v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22494", "title": "DriveBLIP2: Attention-Guided Explanation Generation for Complex Driving Scenarios", "authors": ["Shihong Ling", "Yue Wan", "Xiaowei Jia", "Na Du"], "summary": "This paper introduces a new framework, DriveBLIP2, built upon the BLIP2-OPT\narchitecture, to generate accurate and contextually relevant explanations for\nemerging driving scenarios. While existing vision-language models perform well\nin general tasks, they encounter difficulties in understanding complex,\nmulti-object environments, particularly in real-time applications such as\nautonomous driving, where the rapid identification of key objects is crucial.\nTo address this limitation, an Attention Map Generator is proposed to highlight\nsignificant objects relevant to driving decisions within critical video frames.\nBy directing the model's focus to these key regions, the generated attention\nmap helps produce clear and relevant explanations, enabling drivers to better\nunderstand the vehicle's decision-making process in critical situations.\nEvaluations on the DRAMA dataset reveal significant improvements in explanation\nquality, as indicated by higher BLEU, ROUGE, CIDEr, and SPICE scores compared\nto baseline models. These findings underscore the potential of targeted\nattention mechanisms in vision-language models for enhancing explainability in\nreal-time autonomous driving.", "comment": "Accepted to IEEE/RSJ International Conference on Intelligent Robots\n  and Systems (IROS) 2025. 7 pages, 3 figures", "pdf_url": "http://arxiv.org/pdf/2506.22494v1", "categories": ["cs.RO", "cs.CV", "cs.LG"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.22494v1", "date": "2025-06-25", "updated": "2025-06-25"}
{"id": "2506.23990", "title": "Machine Understanding of Scientific Language", "authors": ["Dustin Wright"], "summary": "Scientific information expresses human understanding of nature. This\nknowledge is largely disseminated in different forms of text, including\nscientific papers, news articles, and discourse among people on social media.\nWhile important for accelerating our pursuit of knowledge, not all scientific\ntext is faithful to the underlying science. As the volume of this text has\nburgeoned online in recent years, it has become a problem of societal\nimportance to be able to identify the faithfulness of a given piece of\nscientific text automatically. This thesis is concerned with the cultivation of\ndatasets, methods, and tools for machine understanding of scientific language,\nin order to analyze and understand science communication at scale. To arrive at\nthis, I present several contributions in three areas of natural language\nprocessing and machine learning: automatic fact checking, learning with limited\ndata, and scientific text processing. These contributions include new methods\nand resources for identifying check-worthy claims, adversarial claim\ngeneration, multi-source domain adaptation, learning from crowd-sourced labels,\ncite-worthiness detection, zero-shot scientific fact checking, detecting\nexaggerated scientific claims, and modeling degrees of information change in\nscience communication. Critically, I demonstrate how the research outputs of\nthis thesis are useful for effectively learning from limited amounts of\nscientific text in order to identify misinformative scientific statements and\ngenerate new insights into the science communication process", "comment": "PhD Thesis, 210 pages", "pdf_url": "http://arxiv.org/pdf/2506.23990v1", "categories": ["cs.CL", "cs.LG"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.23990v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23771", "title": "Multi-Timescale Hierarchical Reinforcement Learning for Unified Behavior and Control of Autonomous Driving", "authors": ["Guizhe Jin", "Zhuoren Li", "Bo Leng", "Ran Yu", "Lu Xiong"], "summary": "Reinforcement Learning (RL) is increasingly used in autonomous driving (AD)\nand shows clear advantages. However, most RL-based AD methods overlook policy\nstructure design. An RL policy that only outputs short-timescale vehicle\ncontrol commands results in fluctuating driving behavior due to fluctuations in\nnetwork outputs, while one that only outputs long-timescale driving goals\ncannot achieve unified optimality of driving behavior and control. Therefore,\nwe propose a multi-timescale hierarchical reinforcement learning approach. Our\napproach adopts a hierarchical policy structure, where high- and low-level RL\npolicies are unified-trained to produce long-timescale motion guidance and\nshort-timescale control commands, respectively. Therein, motion guidance is\nexplicitly represented by hybrid actions to capture multimodal driving\nbehaviors on structured road and support incremental low-level extend-state\nupdates. Additionally, a hierarchical safety mechanism is designed to ensure\nmulti-timescale safety. Evaluation in simulator-based and HighD dataset-based\nhighway multi-lane scenarios demonstrates that our approach significantly\nimproves AD performance, effectively increasing driving efficiency, action\nconsistency and safety.", "comment": "8 pages, Submitted to IEEE Robotics and Automation Letters", "pdf_url": "http://arxiv.org/pdf/2506.23771v1", "categories": ["cs.RO", "cs.AI"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.23771v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22532", "title": "High Resolution Isotropic 3D Cine imaging with Automated Segmentation using Concatenated 2D Real-time Imaging and Deep Learning", "authors": ["Mark Wrobel", "Michele Pascale", "Tina Yao", "Ruaraidh Campbell", "Elena Milano", "Michael Quail", "Jennifer Steeden", "Vivek Muthurangu"], "summary": "Background: Conventional cardiovascular magnetic resonance (CMR) in\npaediatric and congenital heart disease uses 2D, breath-hold, balanced steady\nstate free precession (bSSFP) cine imaging for assessment of function and\ncardiac-gated, respiratory-navigated, static 3D bSSFP whole-heart imaging for\nanatomical assessment. Our aim is to concatenate a stack 2D free-breathing\nreal-time cines and use Deep Learning (DL) to create an isotropic a fully\nsegmented 3D cine dataset from these images. Methods: Four DL models were\ntrained on open-source data that performed: a) Interslice contrast correction;\nb) Interslice respiratory motion correction; c) Super-resolution (slice\ndirection); and d) Segmentation of right and left atria and ventricles (RA, LA,\nRV, and LV), thoracic aorta (Ao) and pulmonary arteries (PA). In 10 patients\nundergoing routine cardiovascular examination, our method was validated on\nprospectively acquired sagittal stacks of real-time cine images. Quantitative\nmetrics (ventricular volumes and vessel diameters) and image quality of the 3D\ncines were compared to conventional breath hold cine and whole heart imaging.\nResults: All real-time data were successfully transformed into 3D cines with a\ntotal post-processing time of <1 min in all cases. There were no significant\nbiases in any LV or RV metrics with reasonable limits of agreement and\ncorrelation. There is also reasonable agreement for all vessel diameters,\nalthough there was a small but significant overestimation of RPA diameter.\nConclusion: We have demonstrated the potential of creating a 3D-cine data from\nconcatenated 2D real-time cine images using a series of DL models. Our method\nhas short acquisition and reconstruction times with fully segmented data being\navailable within 2 minutes. The good agreement with conventional imaging\nsuggests that our method could help to significantly speed up CMR in clinical\npractice.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22532v1", "categories": ["eess.IV", "cs.CV", "cs.LG"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.22532v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.24007", "title": "Minimax and Bayes Optimal Best-arm Identification: Adaptive Experimental Design for Treatment Choice", "authors": ["Masahiro Kato"], "summary": "This study investigates adaptive experimental design for treatment choice,\nalso known as fixed-budget best-arm identification. We consider an adaptive\nprocedure consisting of a treatment-allocation phase followed by a\ntreatment-choice phase, and we design an adaptive experiment for this setup to\nefficiently identify the best treatment arm, defined as the one with the\nhighest expected outcome. In our designed experiment, the treatment-allocation\nphase consists of two stages. The first stage is a pilot phase, where we\nallocate each treatment arm uniformly with equal proportions to eliminate\nclearly suboptimal arms and estimate outcome variances. In the second stage, we\nallocate treatment arms in proportion to the variances estimated in the first\nstage. After the treatment-allocation phase, the procedure enters the\ntreatment-choice phase, where we choose the treatment arm with the highest\nsample mean as our estimate of the best treatment arm. We prove that this\nsingle design is simultaneously asymptotically minimax and Bayes optimal for\nthe simple regret, with upper bounds that match our lower bounds up to exact\nconstants. Therefore, our designed experiment achieves the sharp efficiency\nlimits without requiring separate tuning for minimax and Bayesian objectives.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.24007v1", "categories": ["econ.EM", "cs.LG", "math.ST", "stat.ME", "stat.ML", "stat.TH"], "cate": "econ.EM", "url": "http://arxiv.org/abs/2506.24007v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23782", "title": "Calibrating Graph Neural Networks with Wavelet-Aware Temperature Scaling", "authors": ["Xiaoyang Li", "Linwei Tao", "Haohui Lu", "Minjing Dong", "Junbin Gao", "Chang Xu"], "summary": "Graph Neural Networks (GNNs) have demonstrated strong predictive performance\non relational data; however, their confidence estimates often misalign with\nactual predictive correctness, posing significant limitations for deployment in\nsafety-critical settings. While existing graph-aware calibration methods seek\nto mitigate this limitation, they primarily depend on coarse one-hop\nstatistics, such as neighbor-predicted confidence, or latent node embeddings,\nthereby neglecting the fine-grained structural heterogeneity inherent in graph\ntopology. In this work, we propose Wavelet-Aware Temperature Scaling (WATS), a\npost-hoc calibration framework that assigns node-specific temperatures based on\ntunable heat-kernel graph wavelet features. Specifically, WATS harnesses the\nscalability and topology sensitivity of graph wavelets to refine confidence\nestimates, all without necessitating model retraining or access to neighboring\nlogits or predictions. Extensive evaluations across seven benchmark datasets\nwith varying graph structures and two GNN backbones demonstrate that WATS\nachieves the lowest Expected Calibration Error (ECE) among all compared\nmethods, outperforming both classical and graph-specific baselines by up to\n42.3\\% in ECE and reducing calibration variance by 17.24\\% on average compared\nwith graph-specific methods. Moreover, WATS remains computationally efficient,\nscaling well across graphs of diverse sizes and densities. Code will be\nreleased based on publication.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23782v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.23782v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22568", "title": "Maximum Dispersion, Maximum Concentration: Enhancing the Quality of MOP Solutions", "authors": ["Gladston Moreira", "Ivan Meneghini", "Elzabeth Wanner"], "summary": "Multi-objective optimization problems (MOPs) often require a trade-off\nbetween conflicting objectives, maximizing diversity and convergence in the\nobjective space. This study presents an approach to improve the quality of MOP\nsolutions by optimizing the dispersion in the decision space and the\nconvergence in a specific region of the objective space. Our approach defines a\nRegion of Interest (ROI) based on a cone representing the decision maker's\npreferences in the objective space, while enhancing the dispersion of solutions\nin the decision space using a uniformity measure. Combining solution\nconcentration in the objective space with dispersion in the decision space\nintensifies the search for Pareto-optimal solutions while increasing solution\ndiversity. When combined, these characteristics improve the quality of\nsolutions and avoid the bias caused by clustering solutions in a specific\nregion of the decision space. Preliminary experiments suggest that this method\nenhances multi-objective optimization by generating solutions that effectively\nbalance dispersion and concentration, thereby mitigating bias in the decision\nspace.", "comment": "11 pages", "pdf_url": "http://arxiv.org/pdf/2506.22568v1", "categories": ["math.OC", "cs.CV"], "cate": "math.OC", "url": "http://arxiv.org/abs/2506.22568v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.24024", "title": "Post-processing of EEG-based Auditory Attention Decoding Decisions via Hidden Markov Models", "authors": ["Nicolas Heintz", "Tom Francart", "Alexander Bertrand"], "summary": "Auditory attention decoding (AAD) algorithms exploit brain signals, such as\nelectroencephalography (EEG), to identify which speaker a listener is focusing\non in a multi-speaker environment. While state-of-the-art AAD algorithms can\nidentify the attended speaker on short time windows, their predictions are\noften too inaccurate for practical use. In this work, we propose augmenting AAD\nwith a hidden Markov model (HMM) that models the temporal structure of\nattention. More specifically, the HMM relies on the fact that a subject is much\nless likely to switch attention than to keep attending the same speaker at any\nmoment in time. We show how a HMM can significantly improve existing AAD\nalgorithms in both causal (real-time) and non-causal (offline) settings. We\nfurther demonstrate that HMMs outperform existing postprocessing approaches in\nboth accuracy and responsiveness, and explore how various factors such as\nwindow length, switching frequency, and AAD accuracy influence overall\nperformance. The proposed method is computationally efficient, intuitive to use\nand applicable in both real-time and offline settings.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.24024v1", "categories": ["eess.SP", "cs.LG"], "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.24024v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23783", "title": "Mamba-FETrack V2: Revisiting State Space Model for Frame-Event based Visual Object Tracking", "authors": ["Shiao Wang", "Ju Huang", "Qingchuan Ma", "Jinfeng Gao", "Chunyi Xu", "Xiao Wang", "Lan Chen", "Bo Jiang"], "summary": "Combining traditional RGB cameras with bio-inspired event cameras for robust\nobject tracking has garnered increasing attention in recent years. However,\nmost existing multimodal tracking algorithms depend heavily on high-complexity\nVision Transformer architectures for feature extraction and fusion across\nmodalities. This not only leads to substantial computational overhead but also\nlimits the effectiveness of cross-modal interactions. In this paper, we propose\nan efficient RGB-Event object tracking framework based on the linear-complexity\nVision Mamba network, termed Mamba-FETrack V2. Specifically, we first design a\nlightweight Prompt Generator that utilizes embedded features from each\nmodality, together with a shared prompt pool, to dynamically generate\nmodality-specific learnable prompt vectors. These prompts, along with the\nmodality-specific embedded features, are then fed into a Vision Mamba-based\nFEMamba backbone, which facilitates prompt-guided feature extraction,\ncross-modal interaction, and fusion in a unified manner. Finally, the fused\nrepresentations are passed to the tracking head for accurate target\nlocalization. Extensive experimental evaluations on multiple RGB-Event tracking\nbenchmarks, including short-term COESOT dataset and long-term datasets, i.e.,\nFE108 and FELT V2, demonstrate the superior performance and efficiency of the\nproposed tracking framework. The source code and pre-trained models will be\nreleased on https://github.com/Event-AHU/Mamba_FETrack", "comment": "Journal extension of Mamba-FETrack which was published on Pattern\n  Recognition and Computer Vision (PRCV) 2024", "pdf_url": "http://arxiv.org/pdf/2506.23783v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23783v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22580", "title": "FedCLAM: Client Adaptive Momentum with Foreground Intensity Matching for Federated Medical Image Segmentation", "authors": ["Vasilis Siomos", "Jonathan Passerat-Palmbach", "Giacomo Tarroni"], "summary": "Federated learning is a decentralized training approach that keeps data under\nstakeholder control while achieving superior performance over isolated\ntraining. While inter-institutional feature discrepancies pose a challenge in\nall federated settings, medical imaging is particularly affected due to diverse\nimaging devices and population variances, which can diminish the global model's\neffectiveness. Existing aggregation methods generally fail to adapt across\nvaried circumstances. To address this, we propose FedCLAM, which integrates\n\\textit{client-adaptive momentum} terms derived from each client's loss\nreduction during local training, as well as a \\textit{personalized dampening\nfactor} to curb overfitting. We further introduce a novel \\textit{intensity\nalignment} loss that matches predicted and ground-truth foreground\ndistributions to handle heterogeneous image intensity profiles across\ninstitutions and devices. Extensive evaluations on two datasets show that\nFedCLAM surpasses eight cutting-edge methods in medical segmentation tasks,\nunderscoring its efficacy. The code is available at\nhttps://github.com/siomvas/FedCLAM.", "comment": "10 pages, 2 figures, Accepted at MICCAI 2025", "pdf_url": "http://arxiv.org/pdf/2506.22580v1", "categories": ["eess.IV", "cs.AI", "cs.CV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.22580v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.24041", "title": "Unsupervised Sparse Coding-based Spiking Neural Network for Real-time Spike Sorting", "authors": ["Alexis Melot", "Sean U. N. Wood", "Yannick Coffinier", "Pierre Yger", "Fabien Alibart"], "summary": "Spike sorting is a crucial step in decoding multichannel extracellular neural\nsignals, enabling the identification of individual neuronal activity. A key\nchallenge in brain-machine interfaces (BMIs) is achieving real-time, low-power\nspike sorting at the edge while keeping high neural decoding performance. This\nstudy introduces the Neuromorphic Sparse Sorter (NSS), a compact two-layer\nspiking neural network optimized for efficient spike sorting. NSS leverages the\nLocally Competitive Algorithm (LCA) for sparse coding to extract relevant\nfeatures from noisy events with reduced computational demands. NSS learns to\nsort detected spike waveforms in an online fashion and operates entirely\nunsupervised. To exploit multi-bit spike coding capabilities of neuromorphic\nplatforms like Intel's Loihi 2, a custom neuron model was implemented, enabling\nflexible power-performance trade-offs via adjustable spike bit-widths.\nEvaluations on simulated and real-world tetrode signals with biological drift\nshowed NSS outperformed established pipelines such as WaveClus3 and PCA+KMeans.\nWith 2-bit graded spikes, NSS on Loihi 2 outperformed NSS implemented with\nleaky integrate-and-fire neuron and achieved an F1-score of 77% (+10%\nimprovement) while consuming 8.6mW (+1.65mW) when tested on a drifting\nrecording, with a computational processing time of 0.25ms (+60 us) per\ninference.", "comment": "Main article : 16 pages, 7 figures and 4 tables. Supplementary\n  Material starts at page 17 with 7 figures", "pdf_url": "http://arxiv.org/pdf/2506.24041v1", "categories": ["cs.NE", "cs.LG"], "cate": "cs.NE", "url": "http://arxiv.org/abs/2506.24041v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23815", "title": "The Impact of AI on Educational Assessment: A Framework for Constructive Alignment", "authors": ["Patrick Stokkink"], "summary": "The influence of Artificial Intelligence (AI), and specifically Large\nLanguage Models (LLM), on education is continuously increasing. These models\nare frequently used by students, giving rise to the question whether current\nforms of assessment are still a valid way to evaluate student performance and\ncomprehension. The theoretical framework developed in this paper is grounded in\nConstructive Alignment (CA) theory and Bloom's taxonomy for defining learning\nobjectives. We argue that AI influences learning objectives of different Bloom\nlevels in a different way, and assessment has to be adopted accordingly.\nFurthermore, in line with Bloom's vision, formative and summative assessment\nshould be aligned on whether the use of AI is permitted or not.\n  Although lecturers tend to agree that education and assessment need to be\nadapted to the presence of AI, a strong bias exists on the extent to which\nlecturers want to allow for AI in assessment. This bias is caused by a\nlecturer's familiarity with AI and specifically whether they use it themselves.\nTo avoid this bias, we propose structured guidelines on a university or faculty\nlevel, to foster alignment among the staff. Besides that, we argue that\nteaching staff should be trained on the capabilities and limitations of AI\ntools. In this way, they are better able to adapt their assessment methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23815v1", "categories": ["cs.HC", "cs.AI"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.23815v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22593", "title": "Pixels-to-Graph: Real-time Integration of Building Information Models and Scene Graphs for Semantic-Geometric Human-Robot Understanding", "authors": ["Antonello Longo", "Chanyoung Chung", "Matteo Palieri", "Sung-Kyun Kim", "Ali Agha", "Cataldo Guaragnella", "Shehryar Khattak"], "summary": "Autonomous robots are increasingly playing key roles as support platforms for\nhuman operators in high-risk, dangerous applications. To accomplish challenging\ntasks, an efficient human-robot cooperation and understanding is required.\nWhile typically robotic planning leverages 3D geometric information, human\noperators are accustomed to a high-level compact representation of the\nenvironment, like top-down 2D maps representing the Building Information Model\n(BIM). 3D scene graphs have emerged as a powerful tool to bridge the gap\nbetween human readable 2D BIM and the robot 3D maps. In this work, we introduce\nPixels-to-Graph (Pix2G), a novel lightweight method to generate structured\nscene graphs from image pixels and LiDAR maps in real-time for the autonomous\nexploration of unknown environments on resource-constrained robot platforms. To\nsatisfy onboard compute constraints, the framework is designed to perform all\noperation on CPU only. The method output are a de-noised 2D top-down\nenvironment map and a structure-segmented 3D pointcloud which are seamlessly\nconnected using a multi-layer graph abstracting information from object-level\nup to the building-level. The proposed method is quantitatively and\nqualitatively evaluated during real-world experiments performed using the NASA\nJPL NeBula-Spot legged robot to autonomously explore and map cluttered garage\nand urban office like environments in real-time.", "comment": "Paper accepted to 2025 IEEE International Conference on Automation\n  Science and Engineering (CASE)", "pdf_url": "http://arxiv.org/pdf/2506.22593v1", "categories": ["cs.RO", "cs.AI", "cs.CV"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.22593v1", "date": "2025-06-27", "updated": "2025-06-27"}
{"id": "2506.24045", "title": "Agent.xpu: Efficient Scheduling of Agentic LLM Workloads on Heterogeneous SoC", "authors": ["Xinming Wei", "Jiahao Zhang", "Haoran Li", "Jiayu Chen", "Rui Qu", "Maoliang Li", "Xiang Chen", "Guojie Luo"], "summary": "The proliferation of agentic Large Language Models (LLMs) on personal devices\nintroduces a new class of workloads characterized by a dichotomy of objectives.\nReactive tasks, initiated by users, demand immediate, low-latency responses,\nwhile proactive tasks operate invisibly and prioritize throughput. Existing\non-device LLM engines, designed for isolated inferences, fail to efficiently\nmanage these concurrent and conflicting requests on consumer-grade\nheterogeneous SoCs with CPU, integrated GPU, and NPU. This paper introduces\nAgent.xpu, an efficient serving system for agentic LLM workloads on\nmemory-unified heterogeneous SoCs. With dedicated offline profiling, Agent.xpu\nfirst constructs a heterogeneous execution graph, which fuses and chunks model\nkernels for affinity-guided, elastic accelerator mapping with predictive kernel\nannotation. At runtime, its online scheduler enables fine-grained, kernel-level\npreemption to guarantee the responsiveness of reactive tasks. To maximize SoC\nutilization, it adopts slack-aware kernel backfill to opportunistically append\nproactive tasks, and mitigates NPU-iGPU contention via bandwidth-aware\ndispatch. Evaluation on an Intel Core Ultra SoC shows that Agent.xpu achieves\n4.6$\\times$ lower latency for reactive tasks and sustains\n1.6$\\times$-6.8$\\times$ higher throughput for proactive tasks compared to\nstate-of-the-art inference engines.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.24045v1", "categories": ["cs.DC", "cs.LG"], "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.24045v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23826", "title": "Towards the \"Digital Me\": A vision of authentic Conversational Agents powered by personal Human Digital Twins", "authors": ["Llu√≠s C. Coll", "Martin W. Lauer-Schmaltz", "Philip Cash", "John P. Hansen", "Anja Maier"], "summary": "Human Digital Twins (HDTs) have traditionally been conceptualized as\ndata-driven models designed to support decision-making across various domains.\nHowever, recent advancements in conversational AI open new possibilities for\nHDTs to function as authentic, interactive digital counterparts of individuals.\nThis paper introduces a novel HDT system architecture that integrates large\nlanguage models with dynamically updated personal data, enabling it to mirror\nan individual's conversational style, memories, and behaviors. To achieve this,\nour approach implements context-aware memory retrieval, neural\nplasticity-inspired consolidation, and adaptive learning mechanisms, creating a\nmore natural and evolving digital persona. The resulting system does not only\nreplicate an individual's unique conversational style depending on who they are\nspeaking with, but also enriches responses with dynamically captured personal\nexperiences, opinions, and memories. While this marks a significant step toward\ndeveloping authentic virtual counterparts, it also raises critical ethical\nconcerns regarding privacy, accountability, and the long-term implications of\npersistent digital identities. This study contributes to the field of HDTs by\ndescribing our novel system architecture, demonstrating its capabilities, and\ndiscussing future directions and emerging challenges to ensure the responsible\nand ethical development of HDTs.", "comment": "24 pages, 9 figures", "pdf_url": "http://arxiv.org/pdf/2506.23826v1", "categories": ["cs.ET", "cs.AI", "cs.CY", "cs.HC", "cs.IR"], "cate": "cs.ET", "url": "http://arxiv.org/abs/2506.23826v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22706", "title": "General Autonomous Cybersecurity Defense: Learning Robust Policies for Dynamic Topologies and Diverse Attackers", "authors": ["Arun Ramamurthy", "Neil Dhir"], "summary": "In the face of evolving cyber threats such as malware, ransomware and\nphishing, autonomous cybersecurity defense (ACD) systems have become essential\nfor real-time threat detection and response with optional human intervention.\nHowever, existing ACD systems rely on limiting assumptions, particularly the\nstationarity of the underlying network dynamics. In real-world scenarios,\nnetwork topologies can change due to actions taken by attackers or defenders,\nsystem failures, or time evolution of networks, leading to failures in the\nadaptive capabilities of current defense agents. Moreover, many agents are\ntrained on static environments, resulting in overfitting to specific\ntopologies, which hampers their ability to generalize to out-of-distribution\nnetwork topologies. This work addresses these challenges by exploring methods\nfor developing agents to learn generalizable policies across dynamic network\nenvironments -- general ACD (GACD).", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22706v1", "categories": ["cs.CR", "cs.AI", "cs.CV", "stat.ML"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.22706v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.24048", "title": "Consensus-based optimization for closed-box adversarial attacks and a connection to evolution strategies", "authors": ["Tim Roith", "Leon Bungert", "Philipp Wacker"], "summary": "Consensus-based optimization (CBO) has established itself as an efficient\ngradient-free optimization scheme, with attractive mathematical properties,\nsuch as mean-field convergence results for non-convex loss functions. In this\nwork, we study CBO in the context of closed-box adversarial attacks, which are\nimperceptible input perturbations that aim to fool a classifier, without\naccessing its gradient. Our contribution is to establish a connection between\nthe so-called consensus hopping as introduced by Riedl et al. and natural\nevolution strategies (NES) commonly applied in the context of adversarial\nattacks and to rigorously relate both methods to gradient-based optimization\nschemes. Beyond that, we provide a comprehensive experimental study that shows\nthat despite the conceptual similarities, CBO can outperform NES and other\nevolutionary strategies in certain scenarios.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.24048v1", "categories": ["math.OC", "cs.LG", "65K10, 68Q32, 65K15, 90C26"], "cate": "math.OC", "url": "http://arxiv.org/abs/2506.24048v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23840", "title": "Do Thinking Tokens Help or Trap? Towards More Efficient Large Reasoning Model", "authors": ["Bowen Ding", "Yuhan Chen", "Futing Wang", "Lingfeng Ming", "Tao Lin"], "summary": "Large Reasoning Models (LRMs) excel at solving complex problems but face an\noverthinking dilemma. When handling simple tasks, they often produce verbose\nresponses overloaded with thinking tokens (e.g., wait, however). These tokens\ntrigger unnecessary high-level reasoning behaviors like reflection and\nbacktracking, reducing efficiency. In this work, our pilot study reveals that\nthese thinking-token-induced behaviors are not essential for effective\nproblem-solving and may even hinder correct reasoning within constrained token\nbudgets. We identify this phenomenon as the thinking trap. To mitigate this\nissue, we propose Dual Policy Preference Optimization (DuP-PO), a novel\nalgorithm featuring: (1) A rollout sampling strategy that guarantees balanced\nexposure to responses with and without thinking tokens; (2) A fine-grained\nadvantage control technique to dynamically regulate the prediction of target\ntokens; (3) A policy shaping method ensuring stable gradient contributions from\nthinking tokens. Experimental results on five popular math reasoning benchmarks\nshow that DuP-PO performs well on the popular LRM, which significantly improves\ntheir token efficiency during reasoning, while achieving superior performance\nof the base model.", "comment": "13 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2506.23840v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.23840v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22790", "title": "ICME 2025 Generalizable HDR and SDR Video Quality Measurement Grand Challenge", "authors": ["Yixu Chen", "Bowen Chen", "Hai Wei", "Alan C. Bovik", "Baojun Li", "Wei Sun", "Linhan Cao", "Kang Fu", "Dandan Zhu", "Jun Jia", "Menghan Hu", "Xiongkuo Min", "Guangtao Zhai", "Dounia Hammou", "Fei Yin", "Rafal Mantiuk", "Amritha Premkumar", "Prajit T Rajendran", "Vignesh V Menon"], "summary": "This paper reports IEEE International Conference on Multimedia \\& Expo (ICME)\n2025 Grand Challenge on Generalizable HDR and SDR Video Quality Measurement.\nWith the rapid development of video technology, especially High Dynamic Range\n(HDR) and Standard Dynamic Range (SDR) contents, the need for robust and\ngeneralizable Video Quality Assessment (VQA) methods has become increasingly\ndemanded. Existing VQA models often struggle to deliver consistent performance\nacross varying dynamic ranges, distortion types, and diverse content. This\nchallenge was established to benchmark and promote VQA approaches capable of\njointly handling HDR and SDR content. In the final evaluation phase, five teams\nsubmitted seven models along with technical reports to the Full Reference (FR)\nand No Reference (NR) tracks. Among them, four methods outperformed VMAF\nbaseline, while the top-performing model achieved state-of-the-art performance,\nsetting a new benchmark for generalizable video quality assessment.", "comment": "ICME 2025 Grand Challenges", "pdf_url": "http://arxiv.org/pdf/2506.22790v1", "categories": ["eess.IV", "cs.CV", "cs.MM"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.22790v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.24056", "title": "Logit-Gap Steering: Efficient Short-Suffix Jailbreaks for Aligned Large Language Models", "authors": ["Tung-Ling Li", "Hongliang Liu"], "summary": "We introduce logit-gap steering, a fast jailbreak framework that casts the\nrefusal-affirmation gap of RLHF-aligned language models as a single pass over\nthe vocabulary. A forward-computable score blends gap reduction with\nlightweight proxies for KL penalty and reward shift, allowing a \"sort-sum-stop\"\nsweep to complete in under a second and return a short suffix--two orders of\nmagnitude fewer model calls than beam or gradient attacks. The same suffix\ngeneralises to unseen prompts and scales from 0.5 B to 70 B checkpoints,\nlifting one-shot attack success from baseline levels to 80-100% while\npreserving topical coherence. Beyond efficiency, these suffixes expose\nsentence-boundary reward cliffs and other alignment artefacts, offering a\nlightweight probe into how safety tuning reshapes internal representations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.24056v1", "categories": ["cs.CR", "cs.CL", "cs.LG"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.24056v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23845", "title": "Use Sparse Autoencoders to Discover Unknown Concepts, Not to Act on Known Concepts", "authors": ["Kenny Peng", "Rajiv Movva", "Jon Kleinberg", "Emma Pierson", "Nikhil Garg"], "summary": "While sparse autoencoders (SAEs) have generated significant excitement, a\nseries of negative results have added to skepticism about their usefulness.\nHere, we establish a conceptual distinction that reconciles competing\nnarratives surrounding SAEs. We argue that while SAEs may be less effective for\nacting on known concepts, SAEs are powerful tools for discovering unknown\nconcepts. This distinction cleanly separates existing negative and positive\nresults, and suggests several classes of SAE applications. Specifically, we\noutline use cases for SAEs in (i) ML interpretability, explainability,\nfairness, auditing, and safety, and (ii) social and health sciences.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23845v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CY"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.23845v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22799", "title": "VoteSplat: Hough Voting Gaussian Splatting for 3D Scene Understanding", "authors": ["Minchao Jiang", "Shunyu Jia", "Jiaming Gu", "Xiaoyuan Lu", "Guangming Zhu", "Anqi Dong", "Liang Zhang"], "summary": "3D Gaussian Splatting (3DGS) has become horsepower in high-quality, real-time\nrendering for novel view synthesis of 3D scenes. However, existing methods\nfocus primarily on geometric and appearance modeling, lacking deeper scene\nunderstanding while also incurring high training costs that complicate the\noriginally streamlined differentiable rendering pipeline. To this end, we\npropose VoteSplat, a novel 3D scene understanding framework that integrates\nHough voting with 3DGS. Specifically, Segment Anything Model (SAM) is utilized\nfor instance segmentation, extracting objects, and generating 2D vote maps. We\nthen embed spatial offset vectors into Gaussian primitives. These offsets\nconstruct 3D spatial votes by associating them with 2D image votes, while depth\ndistortion constraints refine localization along the depth axis. For\nopen-vocabulary object localization, VoteSplat maps 2D image semantics to 3D\npoint clouds via voting points, reducing training costs associated with\nhigh-dimensional CLIP features while preserving semantic unambiguity. Extensive\nexperiments demonstrate effectiveness of VoteSplat in open-vocabulary 3D\ninstance localization, 3D point cloud understanding, click-based 3D object\nlocalization, hierarchical segmentation, and ablation studies. Our code is\navailable at https://sy-ja.github.io/votesplat/", "comment": "Accepted to ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2506.22799v1", "categories": ["cs.GR", "cs.CV", "cs.LG"], "cate": "cs.GR", "url": "http://arxiv.org/abs/2506.22799v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.24081", "title": "SQUASH: A SWAP-Based Quantum Attack to Sabotage Hybrid Quantum Neural Networks", "authors": ["Rahul Kumar", "Wenqi Wei", "Ying Mao", "Junaid Farooq", "Ying Wang", "Juntao Chen"], "summary": "We propose a circuit-level attack, SQUASH, a SWAP-Based Quantum Attack to\nsabotage Hybrid Quantum Neural Networks (HQNNs) for classification tasks.\nSQUASH is executed by inserting SWAP gate(s) into the variational quantum\ncircuit of the victim HQNN. Unlike conventional noise-based or adversarial\ninput attacks, SQUASH directly manipulates the circuit structure, leading to\nqubit misalignment and disrupting quantum state evolution. This attack is\nhighly stealthy, as it does not require access to training data or introduce\ndetectable perturbations in input states. Our results demonstrate that SQUASH\nsignificantly degrades classification performance, with untargeted SWAP attacks\nreducing accuracy by up to 74.08\\% and targeted SWAP attacks reducing target\nclass accuracy by up to 79.78\\%. These findings reveal a critical vulnerability\nin HQNN implementations, underscoring the need for more resilient architectures\nagainst circuit-level adversarial interventions.", "comment": "Keywords: Quantum Machine Learning, Hybrid Quantum Neural Networks,\n  SWAP Test, Fidelity, Circuit-level Attack", "pdf_url": "http://arxiv.org/pdf/2506.24081v1", "categories": ["quant-ph", "cs.AI", "cs.LG"], "cate": "quant-ph", "url": "http://arxiv.org/abs/2506.24081v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23855", "title": "Differentially Private Synthetic Data Release for Topics API Outputs", "authors": ["Travis Dick", "Alessandro Epasto", "Adel Javanmard", "Josh Karlin", "Andres Munoz Medina", "Vahab Mirrokni", "Sergei Vassilvitskii", "Peilin Zhong"], "summary": "The analysis of the privacy properties of Privacy-Preserving Ads APIs is an\narea of research that has received strong interest from academics, industry,\nand regulators. Despite this interest, the empirical study of these methods is\nhindered by the lack of publicly available data. Reliable empirical analysis of\nthe privacy properties of an API, in fact, requires access to a dataset\nconsisting of realistic API outputs; however, privacy concerns prevent the\ngeneral release of such data to the public.\n  In this work, we develop a novel methodology to construct synthetic API\noutputs that are simultaneously realistic enough to enable accurate study and\nprovide strong privacy protections. We focus on one Privacy-Preserving Ads\nAPIs: the Topics API, part of Google Chrome's Privacy Sandbox. We developed a\nmethodology to generate a differentially-private dataset that closely matches\nthe re-identification risk properties of the real Topics API data. The use of\ndifferential privacy provides strong theoretical bounds on the leakage of\nprivate user information from this release.\n  Our methodology is based on first computing a large number of\ndifferentially-private statistics describing how output API traces evolve over\ntime. Then, we design a parameterized distribution over sequences of API traces\nand optimize its parameters so that they closely match the statistics obtained.\nFinally, we create the synthetic data by drawing from this distribution.\n  Our work is complemented by an open-source release of the anonymized dataset\nobtained by this methodology. We hope this will enable external researchers to\nanalyze the API in-depth and replicate prior and future work on a realistic\nlarge-scale dataset. We believe that this work will contribute to fostering\ntransparency regarding the privacy properties of Privacy-Preserving Ads APIs.", "comment": "20 pages, 8 figures", "pdf_url": "http://arxiv.org/pdf/2506.23855v1", "categories": ["cs.CR", "cs.AI", "cs.LG"], "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.23855v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22802", "title": "Riemannian-Geometric Fingerprints of Generative Models", "authors": ["Hae Jin Song", "Laurent Itti"], "summary": "Recent breakthroughs and rapid integration of generative models (GMs) have\nsparked interest in the problem of model attribution and their fingerprints.\nFor instance, service providers need reliable methods of authenticating their\nmodels to protect their IP, while users and law enforcement seek to verify the\nsource of generated content for accountability and trust. In addition, a\ngrowing threat of model collapse is arising, as more model-generated data are\nbeing fed back into sources (e.g., YouTube) that are often harvested for\ntraining (\"regurgitative training\"), heightening the need to differentiate\nsynthetic from human data. Yet, a gap still exists in understanding generative\nmodels' fingerprints, we believe, stemming from the lack of a formal framework\nthat can define, represent, and analyze the fingerprints in a principled way.\nTo address this gap, we take a geometric approach and propose a new definition\nof artifact and fingerprint of GMs using Riemannian geometry, which allows us\nto leverage the rich theory of differential geometry. Our new definition\ngeneralizes previous work (Song et al., 2024) to non-Euclidean manifolds by\nlearning Riemannian metrics from data and replacing the Euclidean distances and\nnearest-neighbor search with geodesic distances and kNN-based Riemannian center\nof mass. We apply our theory to a new gradient-based algorithm for computing\nthe fingerprints in practice. Results show that it is more effective in\ndistinguishing a large array of GMs, spanning across 4 different datasets in 2\ndifferent resolutions (64 by 64, 256 by 256), 27 model architectures, and 2\nmodalities (Vision, Vision-Language). Using our proposed definition\nsignificantly improves the performance on model attribution, as well as a\ngeneralization to unseen datasets, model types, and modalities, suggesting its\npractical efficacy.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22802v1", "categories": ["cs.LG", "cs.CR", "cs.CV", "I.2.6"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.22802v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.24108", "title": "Navigating with Annealing Guidance Scale in Diffusion Space", "authors": ["Shai Yehezkel", "Omer Dahary", "Andrey Voynov", "Daniel Cohen-Or"], "summary": "Denoising diffusion models excel at generating high-quality images\nconditioned on text prompts, yet their effectiveness heavily relies on careful\nguidance during the sampling process. Classifier-Free Guidance (CFG) provides a\nwidely used mechanism for steering generation by setting the guidance scale,\nwhich balances image quality and prompt alignment. However, the choice of the\nguidance scale has a critical impact on the convergence toward a visually\nappealing and prompt-adherent image. In this work, we propose an annealing\nguidance scheduler which dynamically adjusts the guidance scale over time based\non the conditional noisy signal. By learning a scheduling policy, our method\naddresses the temperamental behavior of CFG. Empirical results demonstrate that\nour guidance scheduler significantly enhances image quality and alignment with\nthe text prompt, advancing the performance of text-to-image generation.\nNotably, our novel scheduler requires no additional activations or memory\nconsumption, and can seamlessly replace the common classifier-free guidance,\noffering an improved trade-off between prompt alignment and quality.", "comment": "Project page:\n  https://annealing-guidance.github.io/annealing-guidance/", "pdf_url": "http://arxiv.org/pdf/2506.24108v1", "categories": ["cs.GR", "cs.AI", "cs.CV", "cs.LG"], "cate": "cs.GR", "url": "http://arxiv.org/abs/2506.24108v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23869", "title": "Scaling Self-Supervised Representation Learning for Symbolic Piano Performance", "authors": ["Louis Bradshaw", "Honglu Fan", "Alexander Spangher", "Stella Biderman", "Simon Colton"], "summary": "We study the capabilities of generative autoregressive transformer models\ntrained on large amounts of symbolic solo-piano transcriptions. After first\npretraining on approximately 60,000 hours of music, we use a comparatively\nsmaller, high-quality subset, to finetune models to produce musical\ncontinuations, perform symbolic classification tasks, and produce\ngeneral-purpose contrastive MIDI embeddings by adapting the SimCLR framework to\nsymbolic music. When evaluating piano continuation coherence, our generative\nmodel outperforms leading symbolic generation techniques and remains\ncompetitive with proprietary audio generation models. On MIR classification\nbenchmarks, frozen representations from our contrastive model achieve\nstate-of-the-art results in linear probe experiments, while direct finetuning\ndemonstrates the generalizability of pretrained representations, often\nrequiring only a few hundred labeled examples to specialize to downstream\ntasks.", "comment": "ISMIR (2025)", "pdf_url": "http://arxiv.org/pdf/2506.23869v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.23869v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22826", "title": "Denoising Multi-Color QR Codes and Stiefel-Valued Data by Relaxed Regularizations", "authors": ["Robert Beinert", "Jonas Bresch"], "summary": "The handling of manifold-valued data, for instance, plays a central role in\ncolor restoration tasks relying on circle- or sphere-valued color models, in\nthe study of rotational or directional information related to the special\northogonal group, and in Gaussian image processing, where the pixel statistics\nare interpreted as values on the hyperbolic sheet. Especially, to denoise these\nkind of data, there have been proposed several generalizations of total\nvariation (TV) and Tikhonov-type denoising models incorporating the underlying\nmanifolds. Recently, a novel, numerically efficient denoising approach has been\nintroduced, where the data are embedded in an Euclidean ambient space, the\nnon-convex manifolds are encoded by a series of positive semi-definite,\nfixed-rank matrices, and the rank constraint is relaxed to obtain a\nconvexification that can be solved using standard algorithms from convex\nanalysis. The aim of the present paper is to extent this approach to new kinds\nof data like multi-binary and Stiefel-valued data. Multi-binary data can, for\ninstance, be used to model multi-color QR codes whereas Stiefel-valued data\noccur in image and video-based recognition. For both new data types, we propose\nTV- and Tikhonov-based denoising modelstogether with easy-to-solve\nconvexification. All derived methods are evaluated on proof-of-concept,\nsynthetic experiments.", "comment": "9 pages, 2 figures, 3 algorithms", "pdf_url": "http://arxiv.org/pdf/2506.22826v1", "categories": ["math.OC", "cs.CV", "cs.NA", "math.NA", "94A08, 94A12, 65J22, 90C22, 90C25"], "cate": "math.OC", "url": "http://arxiv.org/abs/2506.22826v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.24119", "title": "SPIRAL: Self-Play on Zero-Sum Games Incentivizes Reasoning via Multi-Agent Multi-Turn Reinforcement Learning", "authors": ["Bo Liu", "Leon Guertler", "Simon Yu", "Zichen Liu", "Penghui Qi", "Daniel Balcells", "Mickel Liu", "Cheston Tan", "Weiyan Shi", "Min Lin", "Wee Sun Lee", "Natasha Jaques"], "summary": "Recent advances in reinforcement learning have shown that language models can\ndevelop sophisticated reasoning through training on tasks with verifiable\nrewards, but these approaches depend on human-curated problem-answer pairs and\ndomain-specific reward engineering. We introduce SPIRAL, a self-play framework\nwhere models learn by playing multi-turn, zero-sum games against continuously\nimproving versions of themselves, eliminating the need for human supervision.\nThrough self-play, SPIRAL generates an infinite curriculum of progressively\nchallenging problems as models must constantly adapt to stronger opponents. To\nenable this self-play training at scale, We implement a fully online,\nmulti-turn, multi-agent reinforcement learning system for LLMs and propose\nrole-conditioned advantage estimation (RAE) to stabilize multi-agent training.\nUsing SPIRAL, self-play on zero-sum games produces reasoning capabilities that\ntransfer broadly. Training Qwen3-4B-Base on Kuhn Poker alone achieves 8.6%\nimprovement on math and 8.4% on general reasoning, outperforming SFT on 25,000\nexpert game trajectories. Analysis reveals that this transfer occurs through\nthree cognitive patterns: systematic decomposition, expected value calculation,\nand case-by-case analysis. Multi-game training (TicTacToe, Kuhn Poker, Simple\nNegotiation) further enhances performance as each game develops distinct\nreasoning strengths. Applying SPIRAL to a strong reasoning model\n(DeepSeek-R1-Distill-Qwen-7B) can still lead to 2.0% average improvement. These\nresults demonstrate that zero-sum games naturally develop transferable\nreasoning capabilities, highlighting a promising direction for autonomous\nreasoning development.", "comment": "Work in Progress", "pdf_url": "http://arxiv.org/pdf/2506.24119v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.24119v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23875", "title": "Chain of Thought in Order: Discovering Learning-Friendly Orders for Arithmetic", "authors": ["Yuta Sato", "Kazuhiko Kawamoto", "Hiroshi Kera"], "summary": "The chain of thought is fundamental in Transformers, which is to perform\nstep-by-step reasoning. Besides what intermediate steps work, the order of\nthese steps critically affects the difficulty of the reasoning. This study\naddresses a novel task of unraveling chain of thought - reordering decoder\ninput tokens to a learning-friendly sequence for Transformers to learn\narithmetic tasks. The proposed pipeline first trains a Transformer on a mixture\nof target sequences arranged in different orders and then identifies benign\norders as those with fast loss drops in the early stage. As the search space\ngrows factorially with sequence length, we propose a two-stage hierarchical\napproach for inter- and intra-block reordering. Experiments on four\norder-sensitive arithmetic tasks show that our method identifies a\nlearning-friendly order out of a few billion candidates. Notably, on the\nmultiplication task, it recovered the reverse-digit order reported in prior\nstudies.", "comment": "14 pages, 10 figures", "pdf_url": "http://arxiv.org/pdf/2506.23875v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.23875v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22882", "title": "CA-Diff: Collaborative Anatomy Diffusion for Brain Tissue Segmentation", "authors": ["Qilong Xing", "Zikai Song", "Yuteng Ye", "Yuke Chen", "Youjia Zhang", "Na Feng", "Junqing Yu", "Wei Yang"], "summary": "Segmentation of brain structures from MRI is crucial for evaluating brain\nmorphology, yet existing CNN and transformer-based methods struggle to\ndelineate complex structures accurately. While current diffusion models have\nshown promise in image segmentation, they are inadequate when applied directly\nto brain MRI due to neglecting anatomical information. To address this, we\npropose Collaborative Anatomy Diffusion (CA-Diff), a framework integrating\nspatial anatomical features to enhance segmentation accuracy of the diffusion\nmodel. Specifically, we introduce distance field as an auxiliary anatomical\ncondition to provide global spatial context, alongside a collaborative\ndiffusion process to model its joint distribution with anatomical structures,\nenabling effective utilization of anatomical features for segmentation.\nFurthermore, we introduce a consistency loss to refine relationships between\nthe distance field and anatomical structures and design a time adapted channel\nattention module to enhance the U-Net feature fusion procedure. Extensive\nexperiments show that CA-Diff outperforms state-of-the-art (SOTA) methods.", "comment": "ICME 2025", "pdf_url": "http://arxiv.org/pdf/2506.22882v1", "categories": ["eess.IV", "cs.CV", "cs.LG"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.22882v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23903", "title": "GroundingDINO-US-SAM: Text-Prompted Multi-Organ Segmentation in Ultrasound with LoRA-Tuned Vision-Language Models", "authors": ["Hamza Rasaee", "Taha Koleilat", "Hassan Rivaz"], "summary": "Accurate and generalizable object segmentation in ultrasound imaging remains\na significant challenge due to anatomical variability, diverse imaging\nprotocols, and limited annotated data. In this study, we propose a\nprompt-driven vision-language model (VLM) that integrates Grounding DINO with\nSAM2 to enable object segmentation across multiple ultrasound organs. A total\nof 18 public ultrasound datasets, encompassing the breast, thyroid, liver,\nprostate, kidney, and paraspinal muscle, were utilized. These datasets were\ndivided into 15 for fine-tuning and validation of Grounding DINO using Low Rank\nAdaptation (LoRA) to the ultrasound domain, and 3 were held out entirely for\ntesting to evaluate performance in unseen distributions. Comprehensive\nexperiments demonstrate that our approach outperforms state-of-the-art\nsegmentation methods, including UniverSeg, MedSAM, MedCLIP-SAM, BiomedParse,\nand SAMUS on most seen datasets while maintaining strong performance on unseen\ndatasets without additional fine-tuning. These results underscore the promise\nof VLMs in scalable and robust ultrasound image analysis, reducing dependence\non large, organ-specific annotated datasets. We will publish our code on\ncode.sonography.ai after acceptance.", "comment": "11 pages, 3 figures, 6 figures", "pdf_url": "http://arxiv.org/pdf/2506.23903v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.23903v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22952", "title": "Hierarchical Characterization of Brain Dynamics via State Space-based Vector Quantization", "authors": ["Yanwu Yang", "Thomas Wolfers"], "summary": "Understanding brain dynamics through functional Magnetic Resonance Imaging\n(fMRI) remains a fundamental challenge in neuroscience, particularly in\ncapturing how the brain transitions between various functional states.\nRecently, metastability, which refers to temporarily stable brain states, has\noffered a promising paradigm to quantify complex brain signals into\ninterpretable, discretized representations. In particular, compared to\ncluster-based machine learning approaches, tokenization approaches leveraging\nvector quantization have shown promise in representation learning with powerful\nreconstruction and predictive capabilities. However, most existing methods\nignore brain transition dependencies and lack a quantification of brain\ndynamics into representative and stable embeddings. In this study, we propose a\nHierarchical State space-based Tokenization network, termed HST, which\nquantizes brain states and transitions in a hierarchical structure based on a\nstate space-based model. We introduce a refined clustered Vector-Quantization\nVariational AutoEncoder (VQ-VAE) that incorporates quantization error feedback\nand clustering to improve quantization performance while facilitating\nmetastability with representative and stable token representations. We validate\nour HST on two public fMRI datasets, demonstrating its effectiveness in\nquantifying the hierarchical dynamics of the brain and its potential in disease\ndiagnosis and reconstruction performance. Our method offers a promising\nframework for the characterization of brain dynamics, facilitating the analysis\nof metastability.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22952v1", "categories": ["eess.IV", "cs.CV", "q-bio.NC"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.22952v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23923", "title": "Reinforcement Learning for Synchronised Flow Control in a Dual-Gate Resin Infusion System", "authors": ["Miguel Camacho-S√°nchez", "Fernando Garc√≠a-Torres", "Jesper John Lisegaard", "Roc√≠o del Amor", "Sankhya Mohanty", "Valery Naranjo"], "summary": "Resin infusion (RI) and resin transfer moulding (RTM) are critical processes\nfor the manufacturing of high-performance fibre-reinforced polymer composites,\nparticularly for large-scale applications such as wind turbine blades.\nControlling the resin flow dynamics in these processes is critical to ensure\nthe uniform impregnation of the fibre reinforcements, thereby preventing\nresidual porosities and dry spots that impact the consequent structural\nintegrity of the final component. This paper presents a reinforcement learning\n(RL) based strategy, established using process simulations, for synchronising\nthe different resin flow fronts in an infusion scenario involving two resin\ninlets and a single outlet. Using Proximal Policy Optimisation (PPO), our\napproach addresses the challenge of managing the fluid dynamics in a partially\nobservable environment. The results demonstrate the effectiveness of the RL\napproach in achieving an accurate flow convergence, highlighting its potential\ntowards improving process control and product quality in composites\nmanufacturing.", "comment": "11 pages, 4 figures, 45th Ris{\\o} International Symposium on\n  Materials Science", "pdf_url": "http://arxiv.org/pdf/2506.23923v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.23923v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22973", "title": "Confident Splatting: Confidence-Based Compression of 3D Gaussian Splatting via Learnable Beta Distributions", "authors": ["AmirHossein Naghi Razlighi", "Elaheh Badali Golezani", "Shohreh Kasaei"], "summary": "3D Gaussian Splatting enables high-quality real-time rendering but often\nproduces millions of splats, resulting in excessive storage and computational\noverhead. We propose a novel lossy compression method based on learnable\nconfidence scores modeled as Beta distributions. Each splat's confidence is\noptimized through reconstruction-aware losses, enabling pruning of\nlow-confidence splats while preserving visual fidelity. The proposed approach\nis architecture-agnostic and can be applied to any Gaussian Splatting variant.\nIn addition, the average confidence values serve as a new metric to assess the\nquality of the scene. Extensive experiments demonstrate favorable trade-offs\nbetween compression and fidelity compared to prior work. Our code and data are\npublicly available at\nhttps://github.com/amirhossein-razlighi/Confident-Splatting", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22973v1", "categories": ["cs.GR", "cs.CV"], "cate": "cs.GR", "url": "http://arxiv.org/abs/2506.22973v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23930", "title": "Leveraging the Potential of Prompt Engineering for Hate Speech Detection in Low-Resource Languages", "authors": ["Ruhina Tabasshum Prome", "Tarikul Islam Tamiti", "Anomadarshi Barua"], "summary": "The rapid expansion of social media leads to a marked increase in hate\nspeech, which threatens personal lives and results in numerous hate crimes.\nDetecting hate speech presents several challenges: diverse dialects, frequent\ncode-mixing, and the prevalence of misspelled words in user-generated content\non social media platforms. Recent progress in hate speech detection is\ntypically concentrated on high-resource languages. However, low-resource\nlanguages still face significant challenges due to the lack of large-scale,\nhigh-quality datasets. This paper investigates how we can overcome this\nlimitation via prompt engineering on large language models (LLMs) focusing on\nlow-resource Bengali language. We investigate six prompting strategies -\nzero-shot prompting, refusal suppression, flattering the classifier, multi-shot\nprompting, role prompting, and finally our innovative metaphor prompting to\ndetect hate speech effectively in low-resource languages. We pioneer the\nmetaphor prompting to circumvent the built-in safety mechanisms of LLMs that\nmarks a significant departure from existing jailbreaking methods. We\ninvestigate all six different prompting strategies on the Llama2-7B model and\ncompare the results extensively with three pre-trained word embeddings - GloVe,\nWord2Vec, and FastText for three different deep learning models - multilayer\nperceptron (MLP), convolutional neural network (CNN), and bidirectional gated\nrecurrent unit (BiGRU). To prove the effectiveness of our metaphor prompting in\nthe low-resource Bengali language, we also evaluate it in another low-resource\nlanguage - Hindi, and two high-resource languages - English and German. The\nperformance of all prompting techniques is evaluated using the F1 score, and\nenvironmental impact factor (IF), which measures CO$_2$ emissions, electricity\nusage, and computational time.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23930v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.23930v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.22992", "title": "MARBLE: A Hard Benchmark for Multimodal Spatial Reasoning and Planning", "authors": ["Yulun Jiang", "Yekun Chai", "Maria Brbiƒá", "Michael Moor"], "summary": "The ability to process information from multiple modalities and to reason\nthrough it step-by-step remains a critical challenge in advancing artificial\nintelligence. However, existing reasoning benchmarks focus on text-only\nreasoning, or employ multimodal questions that can be answered by directly\nretrieving information from a non-text modality. Thus, complex reasoning\nremains poorly understood in multimodal domains. Here, we present MARBLE, a\nchallenging multimodal reasoning benchmark that is designed to scrutinize\nmultimodal language models (MLLMs) in their ability to carefully reason\nstep-by-step through complex multimodal problems and environments. MARBLE is\ncomposed of two highly challenging tasks, M-Portal and M-Cube, that require the\ncrafting and understanding of multistep plans under spatial, visual, and\nphysical constraints. We find that current MLLMs perform poorly on MARBLE --\nall the 12 advanced models obtain near-random performance on M-Portal and 0%\naccuracy on M-Cube. Only in simplified subtasks some models outperform the\nrandom baseline, indicating that complex reasoning is still a challenge for\nexisting MLLMs. Moreover, we show that perception remains a bottleneck, where\nMLLMs occasionally fail to extract information from the visual inputs. By\nshedding a light on the limitations of MLLMs, we hope that MARBLE will spur the\ndevelopment of the next generation of models with the ability to reason and\nplan across many, multimodal reasoning steps.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22992v1", "categories": ["cs.AI", "cs.CL", "cs.CV"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.22992v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23934", "title": "QPART: Adaptive Model Quantization and Dynamic Workload Balancing for Accuracy-aware Edge Inference", "authors": ["Xiangchen Li", "Saeid Ghafouri", "Bo Ji", "Hans Vandierendonck", "Deepu John", "Dimitrios S. Nikolopoulos"], "summary": "As machine learning inferences increasingly move to edge devices, adapting to\ndiverse computational capabilities, hardware, and memory constraints becomes\nmore critical. Instead of relying on a pre-trained model fixed for all future\ninference queries across diverse edge devices, we argue that planning an\ninference pattern with a request-specific model tailored to the device's\ncomputational capacity, accuracy requirements, and time constraints is more\ncost-efficient and robust to diverse scenarios. To this end, we propose an\naccuracy-aware and workload-balanced inference system that integrates joint\nmodel quantization and inference partitioning. In this approach, the server\ndynamically responds to inference queries by sending a quantized model and\nadaptively sharing the inference workload with the device. Meanwhile, the\ndevice's computational power, channel capacity, and accuracy requirements are\nconsidered when deciding.\n  Furthermore, we introduce a new optimization framework for the inference\nsystem, incorporating joint model quantization and partitioning. Our approach\noptimizes layer-wise quantization bit width and partition points to minimize\ntime consumption and cost while accounting for varying accuracy requirements of\ntasks through an accuracy degradation metric in our optimization model. To our\nknowledge, this work represents the first exploration of optimizing\nquantization layer-wise bit-width in the inference serving system, by\nintroducing theoretical measurement of accuracy degradation. Simulation results\ndemonstrate a substantial reduction in overall time and power consumption, with\ncomputation payloads decreasing by over 80% and accuracy degradation kept below\n1%.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23934v1", "categories": ["cs.DC", "cs.AI", "cs.LG", "cs.PF"], "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.23934v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23016", "title": "Deep Learning in Mild Cognitive Impairment Diagnosis using Eye Movements and Image Content in Visual Memory Tasks", "authors": ["Tom√°s Silva Santos Rocha", "Anastasiia Mikhailova", "Moreno I. Coco", "Jos√© Santos-Victor"], "summary": "The global prevalence of dementia is projected to double by 2050,\nhighlighting the urgent need for scalable diagnostic tools. This study utilizes\ndigital cognitive tasks with eye-tracking data correlated with memory processes\nto distinguish between Healthy Controls (HC) and Mild Cognitive Impairment\n(MCI), a precursor to dementia. A deep learning model based on VTNet was\ntrained using eye-tracking data from 44 participants (24 MCI, 20 HCs) who\nperformed a visual memory task. The model utilizes both time series and spatial\ndata derived from eye-tracking. It was modified to incorporate scan paths, heat\nmaps, and image content. These modifications also enabled testing parameters\nsuch as image resolution and task performance, analyzing their impact on model\nperformance. The best model, utilizing $700\\times700px$ resolution heatmaps,\nachieved 68% sensitivity and 76% specificity. Despite operating under more\nchallenging conditions (e.g., smaller dataset size, shorter task duration, or a\nless standardized task), the model's performance is comparable to an\nAlzheimer's study using similar methods (70% sensitivity and 73% specificity).\nThese findings contribute to the development of automated diagnostic tools for\nMCI. Future work should focus on refining the model and using a standardized\nlong-term visual memory task.", "comment": "13 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2506.23016v1", "categories": ["cs.HC", "cs.CV"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.23016v1", "date": "2025-06-28", "updated": "2025-06-28"}
{"id": "2506.23944", "title": "Adapt Your Body: Mitigating Proprioception Shifts in Imitation Learning", "authors": ["Fuhang Kuang", "Jiacheng You", "Yingdong Hu", "Tong Zhang", "Chuan Wen", "Yang Gao"], "summary": "Imitation learning models for robotic tasks typically rely on multi-modal\ninputs, such as RGB images, language, and proprioceptive states. While\nproprioception is intuitively important for decision-making and obstacle\navoidance, simply incorporating all proprioceptive states leads to a surprising\ndegradation in imitation learning performance. In this work, we identify the\nunderlying issue as the proprioception shift problem, where the distributions\nof proprioceptive states diverge significantly between training and deployment.\nTo address this challenge, we propose a domain adaptation framework that\nbridges the gap by utilizing rollout data collected during deployment. Using\nWasserstein distance, we quantify the discrepancy between expert and rollout\nproprioceptive states and minimize this gap by adding noise to both sets of\nstates, proportional to the Wasserstein distance. This strategy enhances\nrobustness against proprioception shifts by aligning the training and\ndeployment distributions. Experiments on robotic manipulation tasks demonstrate\nthe efficacy of our method, enabling the imitation policy to leverage\nproprioception while mitigating its adverse effects. Our approach outperforms\nthe naive solution which discards proprioception, and other baselines designed\nto address distributional shifts.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23944v1", "categories": ["cs.RO", "cs.AI"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.23944v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23041", "title": "ReMem: Mutual Information-Aware Fine-tuning of Pretrained Vision Transformers for Effective Knowledge Distillation", "authors": ["Chengyu Dong", "Huan Gui", "Noveen Sachdeva", "Long Jin", "Ke Yin", "Jingbo Shang", "Lichan Hong", "Ed H. Chi", "Zhe Zhao"], "summary": "Knowledge distillation from pretrained visual representation models offers an\neffective approach to improve small, task-specific production models. However,\nthe effectiveness of such knowledge transfer drops significantly when\ndistilling from strong models that are pretrained in a large scale. In this\npaper, we address this challenge for pretrained Vision Transformers (ViTs) by\nexploring methods to fine-tune them for more effective knowledge transfer.\nMotivated by the connection between mutual information and distillation\neffectiveness, we propose to employ mutual information-aware optimization\nduring finetuning. For small or highly-imbalanced downstream datasets where\nsuch optimization becomes less effective, we introduce a simple yet effective\nheuristic of reweighting MLP blocks. This approach is inspired by our\nobservation that top MLP blocks are primarily responsible for mutual\ninformation loss. Our method enables small student models to benefit from those\npretrained models among the strongest.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23041v1", "categories": ["cs.LG", "cs.CV"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.23041v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23952", "title": "Autonomy by Design: Preserving Human Autonomy in AI Decision-Support", "authors": ["Stefan Buijsman", "Sarah Carter", "Juan Pablo Berm√∫dez"], "summary": "AI systems increasingly support human decision-making across domains of\nprofessional, skill-based, and personal activity. While previous work has\nexamined how AI might affect human autonomy globally, the effects of AI on\ndomain-specific autonomy -- the capacity for self-governed action within\ndefined realms of skill or expertise -- remain understudied. We analyze how AI\ndecision-support systems affect two key components of domain-specific autonomy:\nskilled competence (the ability to make informed judgments within one's domain)\nand authentic value-formation (the capacity to form genuine domain-relevant\nvalues and preferences). By engaging with prior investigations and analyzing\nempirical cases across medical, financial, and educational domains, we\ndemonstrate how the absence of reliable failure indicators and the potential\nfor unconscious value shifts can erode domain-specific autonomy both\nimmediately and over time. We then develop a constructive framework for\nautonomy-preserving AI support systems. We propose specific socio-technical\ndesign patterns -- including careful role specification, implementation of\ndefeater mechanisms, and support for reflective practice -- that can help\nmaintain domain-specific autonomy while leveraging AI capabilities. This\nframework provides concrete guidance for developing AI systems that enhance\nrather than diminish human agency within specialized domains of action.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23952v1", "categories": ["cs.HC", "cs.AI", "cs.LG", "econ.GN", "q-fin.EC"], "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.23952v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23046", "title": "SoMi-ToM: Evaluating Multi-Perspective Theory of Mind in Embodied Social Interactions", "authors": ["Xianzhe Fan", "Xuhui Zhou", "Chuanyang Jin", "Kolby Nottingham", "Hao Zhu", "Maarten Sap"], "summary": "Humans continuously infer the states, goals, and behaviors of others by\nperceiving their surroundings in dynamic, real-world social interactions.\nHowever, most Theory of Mind (ToM) benchmarks only evaluate static, text-based\nscenarios, which have a significant gap compared to real interactions. We\npropose the SoMi-ToM benchmark, designed to evaluate multi-perspective ToM in\nembodied multi-agent complex social interactions. This benchmark is based on\nrich multimodal interaction data generated by the interaction environment SoMi,\ncovering diverse crafting goals and social relationships. Our framework\nsupports multi-level evaluation: (1) first-person evaluation provides\nmultimodal (visual, dialogue, action, etc.) input from a first-person\nperspective during a task for real-time state inference, (2) third-person\nevaluation provides complete third-person perspective video and text records\nafter a task for goal and behavior inference. This evaluation method allows for\na more comprehensive examination of a model's ToM capabilities from both the\nsubjective immediate experience and the objective global observation. We\nconstructed a challenging dataset containing 35 third-person perspective\nvideos, 363 first-person perspective images, and 1225 expert-annotated\nmultiple-choice questions (three options). On this dataset, we systematically\nevaluated the performance of human subjects and several state-of-the-art large\nvision-language models (LVLMs). The results show that LVLMs perform\nsignificantly worse than humans on SoMi-ToM: the average accuracy gap between\nhumans and models is 40.1% in first-person evaluation and 26.4% in third-person\nevaluation. This indicates that future LVLMs need to further improve their ToM\ncapabilities in embodied, complex social interactions.", "comment": "23 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2506.23046v1", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.RO"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.23046v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23960", "title": "ADReFT: Adaptive Decision Repair for Safe Autonomous Driving via Reinforcement Fine-Tuning", "authors": ["Mingfei Cheng", "Xiaofei Xie", "Renzhi Wang", "Yuan Zhou", "Ming Hu"], "summary": "Autonomous Driving Systems (ADSs) continue to face safety-critical risks due\nto the inherent limitations in their design and performance capabilities.\nOnline repair plays a crucial role in mitigating such limitations, ensuring the\nruntime safety and reliability of ADSs. Existing online repair solutions\nenforce ADS compliance by transforming unacceptable trajectories into\nacceptable ones based on predefined specifications, such as rule-based\nconstraints or training datasets. However, these approaches often lack\ngeneralizability, adaptability and tend to be overly conservative, resulting in\nineffective repairs that not only fail to mitigate safety risks sufficiently\nbut also degrade the overall driving experience. To address this issue, we\npropose Adaptive Decision Repair (ADReFT), a novel and effective repair method\nthat identifies safety-critical states through offline learning from failed\ntests and generates appropriate mitigation actions to improve ADS safety.\nSpecifically, ADReFT incorporates a transformer-based model with two joint\nheads, State Monitor and Decision Adapter, designed to capture complex driving\nenvironment interactions to evaluate state safety severity and generate\nadaptive repair actions. Given the absence of oracles for state safety\nidentification, we first pretrain ADReFT using supervised learning with coarse\nannotations, i.e., labeling states preceding violations as positive samples and\nothers as negative samples. It establishes ADReFT's foundational capability to\nmitigate safety-critical violations, though it may result in somewhat\nconservative mitigation strategies. Therefore, we subsequently finetune ADReFT\nusing reinforcement learning to improve its initial capability and generate\nmore precise and contextually appropriate repair decisions. Our evaluation\nresults illustrate that ADReFT achieves better repair performance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23960v1", "categories": ["cs.LG", "cs.AI", "cs.SE"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.23960v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23102", "title": "MedRegion-CT: Region-Focused Multimodal LLM for Comprehensive 3D CT Report Generation", "authors": ["Sunggu Kyung", "Jinyoung Seo", "Hyunseok Lim", "Dongyeong Kim", "Hyungbin Park", "Jimin Sung", "Jihyun Kim", "Wooyoung Jo", "Yoojin Nam", "Namkug Kim"], "summary": "The recent release of RadGenome-Chest CT has significantly advanced CT-based\nreport generation. However, existing methods primarily focus on global\nfeatures, making it challenging to capture region-specific details, which may\ncause certain abnormalities to go unnoticed. To address this, we propose\nMedRegion-CT, a region-focused Multi-Modal Large Language Model (MLLM)\nframework, featuring three key innovations. First, we introduce Region\nRepresentative ($R^2$) Token Pooling, which utilizes a 2D-wise pretrained\nvision model to efficiently extract 3D CT features. This approach generates\nglobal tokens representing overall slice features and region tokens\nhighlighting target areas, enabling the MLLM to process comprehensive\ninformation effectively. Second, a universal segmentation model generates\npseudo-masks, which are then processed by a mask encoder to extract\nregion-centric features. This allows the MLLM to focus on clinically relevant\nregions, using six predefined region masks. Third, we leverage segmentation\nresults to extract patient-specific attributions, including organ size,\ndiameter, and locations. These are converted into text prompts, enriching the\nMLLM's understanding of patient-specific contexts. To ensure rigorous\nevaluation, we conducted benchmark experiments on report generation using the\nRadGenome-Chest CT. MedRegion-CT achieved state-of-the-art performance,\noutperforming existing methods in natural language generation quality and\nclinical relevance while maintaining interpretability. The code for our\nframework is publicly available.", "comment": "14 pages, 5 figures, submitted to ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2506.23102v1", "categories": ["eess.IV", "cs.CV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.23102v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.23995", "title": "STCLocker: Deadlock Avoidance Testing for Autonomous Driving Systems", "authors": ["Mingfei Cheng", "Renzhi Wang", "Xiaofei Xie", "Yuan Zhou", "Lei Ma"], "summary": "Autonomous Driving System (ADS) testing is essential to ensure the safety and\nreliability of autonomous vehicles (AVs) before deployment. However, existing\ntechniques primarily focus on evaluating ADS functionalities in single-AV\nsettings. As ADSs are increasingly deployed in multi-AV traffic, it becomes\ncrucial to assess their cooperative performance, particularly regarding\ndeadlocks, a fundamental coordination failure in which multiple AVs enter a\ncircular waiting state indefinitely, resulting in motion planning failures.\nDespite its importance, the cooperative capability of ADSs to prevent deadlocks\nremains insufficiently underexplored. To address this gap, we propose the first\ndedicated Spatio-Temporal Conflict-Guided Deadlock Avoidance Testing technique,\nSTCLocker, for generating DeadLock Scenarios (DLSs), where a group of AVs\ncontrolled by the ADS under test are in a circular wait state. STCLocker\nconsists of three key components: Deadlock Oracle, Conflict Feedback, and\nConflict-aware Scenario Generation. Deadlock Oracle provides a reliable\nblack-box mechanism for detecting deadlock cycles among multiple AVs within a\ngiven scenario. Conflict Feedback and Conflict-aware Scenario Generation\ncollaborate to actively guide AVs into simultaneous competition over spatial\nconflict resources (i.e., shared passing regions) and temporal competitive\nbehaviors (i.e., reaching the conflict region at the same time), thereby\nincreasing the effectiveness of generating conflict-prone deadlocks. We\nevaluate STCLocker on two types of ADSs: Roach, an end-to-end ADS, and OpenCDA,\na module-based ADS supporting cooperative communication. Experimental results\nshow that, on average, STCLocker generates more DLS than the best-performing\nbaseline.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23995v1", "categories": ["cs.SE", "cs.AI", "cs.RO"], "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.23995v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23121", "title": "CRISP-SAM2: SAM2 with Cross-Modal Interaction and Semantic Prompting for Multi-Organ Segmentation", "authors": ["Xinlei Yu", "Chanmiao Wang", "Hui Jin", "Ahmed Elazab", "Gangyong Jia", "Xiang Wan", "Changqing Zou", "Ruiquan Ge"], "summary": "Multi-organ medical segmentation is a crucial component of medical image\nprocessing, essential for doctors to make accurate diagnoses and develop\neffective treatment plans. Despite significant progress in this field, current\nmulti-organ segmentation models often suffer from inaccurate details,\ndependence on geometric prompts and loss of spatial information. Addressing\nthese challenges, we introduce a novel model named CRISP-SAM2 with CRoss-modal\nInteraction and Semantic Prompting based on SAM2. This model represents a\npromising approach to multi-organ medical segmentation guided by textual\ndescriptions of organs. Our method begins by converting visual and textual\ninputs into cross-modal contextualized semantics using a progressive\ncross-attention interaction mechanism. These semantics are then injected into\nthe image encoder to enhance the detailed understanding of visual information.\nTo eliminate reliance on geometric prompts, we use a semantic prompting\nstrategy, replacing the original prompt encoder to sharpen the perception of\nchallenging targets. In addition, a similarity-sorting self-updating strategy\nfor memory and a mask-refining process is applied to further adapt to medical\nimaging and enhance localized details. Comparative experiments conducted on\nseven public datasets indicate that CRISP-SAM2 outperforms existing models.\nExtensive analysis also demonstrates the effectiveness of our method, thereby\nconfirming its superior performance, especially in addressing the limitations\nmentioned earlier. Our code is available at:\nhttps://github.com/YU-deep/CRISP\\_SAM2.git.", "comment": "19 pages, 9 figures, 10 tables", "pdf_url": "http://arxiv.org/pdf/2506.23121v1", "categories": ["eess.IV", "cs.AI", "cs.CV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.23121v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.24009", "title": "Bridging Physical and Digital Worlds: Embodied Large AI for Future Wireless Systems", "authors": ["Xinquan Wang", "Fenghao Zhu", "Zhaohui Yang", "Chongwen Huang", "Xiaoming Chen", "Zhaoyang Zhang", "Sami Muhaidat", "M√©rouane Debbah"], "summary": "Large artificial intelligence (AI) models offer revolutionary potential for\nfuture wireless systems, promising unprecedented capabilities in network\noptimization and performance. However, current paradigms largely overlook\ncrucial physical interactions. This oversight means they primarily rely on\noffline datasets, leading to difficulties in handling real-time wireless\ndynamics and non-stationary environments. Furthermore, these models often lack\nthe capability for active environmental probing. This paper proposes a\nfundamental paradigm shift towards wireless embodied large AI (WELAI), moving\nfrom passive observation to active embodiment. We first identify key challenges\nfaced by existing models, then we explore the design principles and system\nstructure of WELAI. Besides, we outline prospective applications in\nnext-generation wireless. Finally, through an illustrative case study, we\ndemonstrate the effectiveness of WELAI and point out promising research\ndirections for realizing adaptive, robust, and autonomous wireless systems.", "comment": "7 pages, 4 figures", "pdf_url": "http://arxiv.org/pdf/2506.24009v1", "categories": ["cs.IT", "cs.AI", "math.IT"], "cate": "cs.IT", "url": "http://arxiv.org/abs/2506.24009v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23145", "title": "Forget-MI: Machine Unlearning for Forgetting Multimodal Information in Healthcare Settings", "authors": ["Shahad Hardan", "Darya Taratynova", "Abdelmajid Essofi", "Karthik Nandakumar", "Mohammad Yaqub"], "summary": "Privacy preservation in AI is crucial, especially in healthcare, where models\nrely on sensitive patient data. In the emerging field of machine unlearning,\nexisting methodologies struggle to remove patient data from trained multimodal\narchitectures, which are widely used in healthcare. We propose Forget-MI, a\nnovel machine unlearning method for multimodal medical data, by establishing\nloss functions and perturbation techniques. Our approach unlearns unimodal and\njoint representations of the data requested to be forgotten while preserving\nknowledge from the remaining data and maintaining comparable performance to the\noriginal model. We evaluate our results using performance on the forget\ndataset, performance on the test dataset, and Membership Inference Attack\n(MIA), which measures the attacker's ability to distinguish the forget dataset\nfrom the training dataset. Our model outperforms the existing approaches that\naim to reduce MIA and the performance on the forget dataset while keeping an\nequivalent performance on the test set. Specifically, our approach reduces MIA\nby 0.202 and decreases AUC and F1 scores on the forget set by 0.221 and 0.305,\nrespectively. Additionally, our performance on the test set matches that of the\nretrained model, while allowing forgetting. Code is available at\nhttps://github.com/BioMedIA-MBZUAI/Forget-MI.git", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23145v1", "categories": ["cs.LG", "cs.CR", "cs.CV"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.23145v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.24016", "title": "EXPERT: An Explainable Image Captioning Evaluation Metric with Structured Explanations", "authors": ["Hyunjong Kim", "Sangyeop Kim", "Jongheon Jeong", "Yeongjae Cho", "Sungzoon Cho"], "summary": "Recent advances in large language models and vision-language models have led\nto growing interest in explainable evaluation metrics for image captioning.\nHowever, these metrics generate explanations without standardized criteria, and\nthe overall quality of the generated explanations remains unverified. In this\npaper, we propose EXPERT, a reference-free evaluation metric that provides\nstructured explanations based on three fundamental criteria: fluency,\nrelevance, and descriptiveness. By constructing large-scale datasets of\nhigh-quality structured explanations, we develop a two-stage evaluation\ntemplate to effectively supervise a vision-language model for both scoring and\nexplanation generation. EXPERT achieves state-of-the-art results on benchmark\ndatasets while providing significantly higher-quality explanations than\nexisting metrics, as validated through comprehensive human evaluation. Our code\nand datasets are available at https://github.com/hjkim811/EXPERT.", "comment": "Accepted at ACL 2025 Findings", "pdf_url": "http://arxiv.org/pdf/2506.24016v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.24016v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23147", "title": "maneuverRecognition -- A Python package for Timeseries Classification in the domain of Vehicle Telematics", "authors": ["Jonathan Schuster", "Fabian Transchel"], "summary": "In the domain of vehicle telematics the automated recognition of driving\nmaneuvers is used to classify and evaluate driving behaviour. This not only\nserves as a component to enhance the personalization of insurance policies, but\nalso to increase road safety, reduce accidents and the associated costs as well\nas to reduce fuel consumption and support environmentally friendly driving. In\nthis context maneuver recognition technically requires a continuous application\nof time series classification which poses special challenges to the transfer,\npreprocessing and storage of telematic sensor data, the training of predictive\nmodels, and the prediction itself. Although much research has been done in the\nfield of gathering relevant data or regarding the methods to build predictive\nmodels for the task of maneuver recognition, there is a practical need for\npython packages and functions that allow to quickly transform data into the\nrequired structure as well as to build and evaluate such models. The\nmaneuverRecognition package was therefore developed to provide the necessary\nfunctions for preprocessing, modelling and evaluation and also includes a ready\nto use LSTM based network structure that can be modified. The implementation of\nthe package is demonstrated using real driving data of three different persons\nrecorded via smartphone sensors.", "comment": "6 pages, 2 figures", "pdf_url": "http://arxiv.org/pdf/2506.23147v1", "categories": ["cs.LG", "cs.CV"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.23147v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.24018", "title": "Bridging Theory and Practice in Link Representation with Graph Neural Networks", "authors": ["Veronica Lachi", "Francesco Ferrini", "Antonio Longa", "Bruno Lepri", "Andrea Passerini", "Manfred Jaeger"], "summary": "Graph Neural Networks (GNNs) are widely used to compute representations of\nnode pairs for downstream tasks such as link prediction. Yet, theoretical\nunderstanding of their expressive power has focused almost entirely on\ngraph-level representations. In this work, we shift the focus to links and\nprovide the first comprehensive study of GNN expressiveness in link\nrepresentation. We introduce a unifying framework, the $k_\\phi$-$k_\\rho$-$m$\nframework, that subsumes existing message-passing link models and enables\nformal expressiveness comparisons. Using this framework, we derive a hierarchy\nof state-of-the-art methods and offer theoretical tools to analyze future\narchitectures. To complement our analysis, we propose a synthetic evaluation\nprotocol comprising the first benchmark specifically designed to assess\nlink-level expressiveness. Finally, we ask: does expressiveness matter in\npractice? We use a graph symmetry metric that quantifies the difficulty of\ndistinguishing links and show that while expressive models may underperform on\nstandard benchmarks, they significantly outperform simpler ones as symmetry\nincreases, highlighting the need for dataset-aware model selection.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.24018v1", "categories": ["cs.LG", "cs.AI"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.24018v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23184", "title": "Score-based Diffusion Model for Unpaired Virtual Histology Staining", "authors": ["Anran Liu", "Xiaofei Wang", "Jing Cai", "Chao Li"], "summary": "Hematoxylin and eosin (H&E) staining visualizes histology but lacks\nspecificity for diagnostic markers. Immunohistochemistry (IHC) staining\nprovides protein-targeted staining but is restricted by tissue availability and\nantibody specificity. Virtual staining, i.e., computationally translating the\nH&E image to its IHC counterpart while preserving the tissue structure, is\npromising for efficient IHC generation. Existing virtual staining methods still\nface key challenges: 1) effective decomposition of staining style and tissue\nstructure, 2) controllable staining process adaptable to diverse tissue and\nproteins, and 3) rigorous structural consistency modelling to handle the\nnon-pixel-aligned nature of paired H&E and IHC images. This study proposes a\nmutual-information (MI)-guided score-based diffusion model for unpaired virtual\nstaining. Specifically, we design 1) a global MI-guided energy function that\ndisentangles the tissue structure and staining characteristics across\nmodalities, 2) a novel timestep-customized reverse diffusion process for\nprecise control of the staining intensity and structural reconstruction, and 3)\na local MI-driven contrastive learning strategy to ensure the cellular level\nstructural consistency between H&E-IHC images. Extensive experiments\ndemonstrate the our superiority over state-of-the-art approaches, highlighting\nits biomedical potential. Codes will be open-sourced upon acceptance.", "comment": "11 pages, 3 figures", "pdf_url": "http://arxiv.org/pdf/2506.23184v1", "categories": ["eess.IV", "cs.AI", "cs.CV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.23184v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.24044", "title": "A Survey on Vision-Language-Action Models for Autonomous Driving", "authors": ["Sicong Jiang", "Zilin Huang", "Kangan Qian", "Ziang Luo", "Tianze Zhu", "Yang Zhong", "Yihong Tang", "Menglin Kong", "Yunlong Wang", "Siwen Jiao", "Hao Ye", "Zihao Sheng", "Xin Zhao", "Tuopu Wen", "Zheng Fu", "Sikai Chen", "Kun Jiang", "Diange Yang", "Seongjin Choi", "Lijun Sun"], "summary": "The rapid progress of multimodal large language models (MLLM) has paved the\nway for Vision-Language-Action (VLA) paradigms, which integrate visual\nperception, natural language understanding, and control within a single policy.\nResearchers in autonomous driving are actively adapting these methods to the\nvehicle domain. Such models promise autonomous vehicles that can interpret\nhigh-level instructions, reason about complex traffic scenes, and make their\nown decisions. However, the literature remains fragmented and is rapidly\nexpanding. This survey offers the first comprehensive overview of VLA for\nAutonomous Driving (VLA4AD). We (i) formalize the architectural building blocks\nshared across recent work, (ii) trace the evolution from early explainer to\nreasoning-centric VLA models, and (iii) compare over 20 representative models\naccording to VLA's progress in the autonomous driving domain. We also\nconsolidate existing datasets and benchmarks, highlighting protocols that\njointly measure driving safety, accuracy, and explanation quality. Finally, we\ndetail open challenges - robustness, real-time efficiency, and formal\nverification - and outline future directions of VLA4AD. This survey provides a\nconcise yet complete reference for advancing interpretable socially aligned\nautonomous vehicles. Github repo is available at\n\\href{https://github.com/JohnsonJiang1996/Awesome-VLA4AD}{SicongJiang/Awesome-VLA4AD}.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.24044v1", "categories": ["cs.CV", "cs.AI", "cs.RO"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.24044v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23208", "title": "Multi-Source COVID-19 Detection via Variance Risk Extrapolation", "authors": ["Runtian Yuan", "Qingqiu Li", "Junlin Hou", "Jilan Xu", "Yuejie Zhang", "Rui Feng", "Hao Chen"], "summary": "We present our solution for the Multi-Source COVID-19 Detection Challenge,\nwhich aims to classify chest CT scans into COVID and Non-COVID categories\nacross data collected from four distinct hospitals and medical centers. A major\nchallenge in this task lies in the domain shift caused by variations in imaging\nprotocols, scanners, and patient populations across institutions. To enhance\nthe cross-domain generalization of our model, we incorporate Variance Risk\nExtrapolation (VREx) into the training process. VREx encourages the model to\nmaintain consistent performance across multiple source domains by explicitly\nminimizing the variance of empirical risks across environments. This\nregularization strategy reduces overfitting to center-specific features and\npromotes learning of domain-invariant representations. We further apply Mixup\ndata augmentation to improve generalization and robustness. Mixup interpolates\nboth the inputs and labels of randomly selected pairs of training samples,\nencouraging the model to behave linearly between examples and enhancing its\nresilience to noise and limited data. Our method achieves an average macro F1\nscore of 0.96 across the four sources on the validation set, demonstrating\nstrong generalization.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23208v1", "categories": ["eess.IV", "cs.CV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.23208v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.24068", "title": "STACK: Adversarial Attacks on LLM Safeguard Pipelines", "authors": ["Ian R. McKenzie", "Oskar J. Hollinsworth", "Tom Tseng", "Xander Davies", "Stephen Casper", "Aaron D. Tucker", "Robert Kirk", "Adam Gleave"], "summary": "Frontier AI developers are relying on layers of safeguards to protect against\ncatastrophic misuse of AI systems. Anthropic guards their latest Claude 4 Opus\nmodel using one such defense pipeline, and other frontier developers including\nGoogle DeepMind and OpenAI pledge to soon deploy similar defenses. However, the\nsecurity of such pipelines is unclear, with limited prior work evaluating or\nattacking these pipelines. We address this gap by developing and red-teaming an\nopen-source defense pipeline. First, we find that a novel few-shot-prompted\ninput and output classifier outperforms state-of-the-art open-weight safeguard\nmodel ShieldGemma across three attacks and two datasets, reducing the attack\nsuccess rate (ASR) to 0% on the catastrophic misuse dataset ClearHarm. Second,\nwe introduce a STaged AttaCK (STACK) procedure that achieves 71% ASR on\nClearHarm in a black-box attack against the few-shot-prompted classifier\npipeline. Finally, we also evaluate STACK in a transfer setting, achieving 33%\nASR, providing initial evidence that it is feasible to design attacks with no\naccess to the target pipeline. We conclude by suggesting specific mitigations\nthat developers could use to thwart staged attacks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.24068v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.24068v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23221", "title": "Single Image Inpainting and Super-Resolution with Simultaneous Uncertainty Guarantees by Universal Reproducing Kernels", "authors": ["B√°lint Horv√°th", "Bal√°zs Csan√°d Cs√°ji"], "summary": "The paper proposes a statistical learning approach to the problem of\nestimating missing pixels of images, crucial for image inpainting and\nsuper-resolution problems. One of the main novelties of the method is that it\nalso provides uncertainty quantifications together with the estimated values.\nOur core assumption is that the underlying data-generating function comes from\na Reproducing Kernel Hilbert Space (RKHS). A special emphasis is put on\nband-limited functions, central to signal processing, which form Paley-Wiener\ntype RKHSs. The proposed method, which we call Simultaneously Guaranteed Kernel\nInterpolation (SGKI), is an extension and refinement of a recently developed\nkernel method. An advantage of SGKI is that it not only estimates the missing\npixels, but also builds non-asymptotic confidence bands for the unobserved\nvalues, which are simultaneously guaranteed for all missing pixels. We also\nshow how to compute these bands efficiently using Schur complements, we discuss\na generalization to vector-valued functions, and we present a series of\nnumerical experiments on various datasets containing synthetically generated\nand benchmark images, as well.", "comment": "23 pages, 8 figures, 6 tables", "pdf_url": "http://arxiv.org/pdf/2506.23221v1", "categories": ["cs.LG", "cs.CV"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.23221v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.24081", "title": "SQUASH: A SWAP-Based Quantum Attack to Sabotage Hybrid Quantum Neural Networks", "authors": ["Rahul Kumar", "Wenqi Wei", "Ying Mao", "Junaid Farooq", "Ying Wang", "Juntao Chen"], "summary": "We propose a circuit-level attack, SQUASH, a SWAP-Based Quantum Attack to\nsabotage Hybrid Quantum Neural Networks (HQNNs) for classification tasks.\nSQUASH is executed by inserting SWAP gate(s) into the variational quantum\ncircuit of the victim HQNN. Unlike conventional noise-based or adversarial\ninput attacks, SQUASH directly manipulates the circuit structure, leading to\nqubit misalignment and disrupting quantum state evolution. This attack is\nhighly stealthy, as it does not require access to training data or introduce\ndetectable perturbations in input states. Our results demonstrate that SQUASH\nsignificantly degrades classification performance, with untargeted SWAP attacks\nreducing accuracy by up to 74.08\\% and targeted SWAP attacks reducing target\nclass accuracy by up to 79.78\\%. These findings reveal a critical vulnerability\nin HQNN implementations, underscoring the need for more resilient architectures\nagainst circuit-level adversarial interventions.", "comment": "Keywords: Quantum Machine Learning, Hybrid Quantum Neural Networks,\n  SWAP Test, Fidelity, Circuit-level Attack", "pdf_url": "http://arxiv.org/pdf/2506.24081v1", "categories": ["quant-ph", "cs.AI", "cs.LG"], "cate": "quant-ph", "url": "http://arxiv.org/abs/2506.24081v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23259", "title": "Improving Myocardial Infarction Detection via Synthetic ECG Pretraining", "authors": ["Lachin Naghashyar"], "summary": "Myocardial infarction is a major cause of death globally, and accurate early\ndiagnosis from electrocardiograms (ECGs) remains a clinical priority. Deep\nlearning models have shown promise for automated ECG interpretation, but\nrequire large amounts of labeled data, which are often scarce in practice. We\npropose a physiology-aware pipeline that (i) synthesizes 12-lead ECGs with\ntunable MI morphology and realistic noise, and (ii) pre-trains recurrent and\ntransformer classifiers with self-supervised masked-autoencoding plus a joint\nreconstruction-classification objective. We validate the realism of synthetic\nECGs via statistical and visual analysis, confirming that key morphological\nfeatures are preserved. Pretraining on synthetic data consistently improved\nclassification performance, particularly in low-data settings, with AUC gains\nof up to 4 percentage points. These results show that controlled synthetic ECGs\ncan help improve MI detection when real clinical data is limited.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23259v1", "categories": ["eess.IV", "cs.CV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.23259v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.24085", "title": "Imagine for Me: Creative Conceptual Blending of Real Images and Text via Blended Attention", "authors": ["Wonwoong Cho", "Yanxia Zhang", "Yan-Ying Chen", "David I. Inouye"], "summary": "Blending visual and textual concepts into a new visual concept is a unique\nand powerful trait of human beings that can fuel creativity. However, in\npractice, cross-modal conceptual blending for humans is prone to cognitive\nbiases, like design fixation, which leads to local minima in the design space.\nIn this paper, we propose a T2I diffusion adapter \"IT-Blender\" that can\nautomate the blending process to enhance human creativity. Prior works related\nto cross-modal conceptual blending are limited in encoding a real image without\nloss of details or in disentangling the image and text inputs. To address these\ngaps, IT-Blender leverages pretrained diffusion models (SD and FLUX) to blend\nthe latent representations of a clean reference image with those of the noisy\ngenerated image. Combined with our novel blended attention, IT-Blender encodes\nthe real reference image without loss of details and blends the visual concept\nwith the object specified by the text in a disentangled way. Our experiment\nresults show that IT-Blender outperforms the baselines by a large margin in\nblending visual and textual concepts, shedding light on the new application of\nimage generative models to augment human creativity.", "comment": "Project website is available at https://imagineforme.github.io/", "pdf_url": "http://arxiv.org/pdf/2506.24085v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.24085v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23305", "title": "BPD-Neo: An MRI Dataset for Lung-Trachea Segmentation with Clinical Data for Neonatal Bronchopulmonary Dysplasia", "authors": ["Rachit Saluja", "Arzu Kovanlikaya", "Candace Chien", "Lauren Kathryn Blatt", "Jeffrey M. Perlman", "Stefan Worgall", "Mert R. Sabuncu", "Jonathan P. Dyke"], "summary": "Bronchopulmonary dysplasia (BPD) is a common complication among preterm\nneonates, with portable X-ray imaging serving as the standard diagnostic\nmodality in neonatal intensive care units (NICUs). However, lung magnetic\nresonance imaging (MRI) offers a non-invasive alternative that avoids sedation\nand radiation while providing detailed insights into the underlying mechanisms\nof BPD. Leveraging high-resolution 3D MRI data, advanced image processing and\nsemantic segmentation algorithms can be developed to assist clinicians in\nidentifying the etiology of BPD. In this dataset, we present MRI scans paired\nwith corresponding semantic segmentations of the lungs and trachea for 40\nneonates, the majority of whom are diagnosed with BPD. The imaging data consist\nof free-breathing 3D stack-of-stars radial gradient echo acquisitions, known as\nthe StarVIBE series. Additionally, we provide comprehensive clinical data and\nbaseline segmentation models, validated against clinical assessments, to\nsupport further research and development in neonatal lung imaging.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23305v1", "categories": ["eess.IV", "cs.CV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.23305v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.24093", "title": "Development of Hybrid Artificial Intelligence Training on Real and Synthetic Data: Benchmark on Two Mixed Training Strategies", "authors": ["Paul Wachter", "Lukas Niehaus", "Julius Sch√∂ning"], "summary": "Synthetic data has emerged as a cost-effective alternative to real data for\ntraining artificial neural networks (ANN). However, the disparity between\nsynthetic and real data results in a domain gap. That gap leads to poor\nperformance and generalization of the trained ANN when applied to real-world\nscenarios. Several strategies have been developed to bridge this gap, which\ncombine synthetic and real data, known as mixed training using hybrid datasets.\nWhile these strategies have been shown to mitigate the domain gap, a systematic\nevaluation of their generalizability and robustness across various tasks and\narchitectures remains underexplored. To address this challenge, our study\ncomprehensively analyzes two widely used mixing strategies on three prevalent\narchitectures and three distinct hybrid datasets. From these datasets, we\nsample subsets with varying proportions of synthetic to real data to\ninvestigate the impact of synthetic and real components. The findings of this\npaper provide valuable insights into optimizing the use of synthetic data in\nthe training process of any ANN, contributing to enhancing robustness and\nefficacy.", "comment": "21pages, 14 figures, 2 tables", "pdf_url": "http://arxiv.org/pdf/2506.24093v1", "categories": ["cs.LG", "cs.AI", "I.2.1; I.2.0; F.2.3"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.24093v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23309", "title": "SurgTPGS: Semantic 3D Surgical Scene Understanding with Text Promptable Gaussian Splatting", "authors": ["Yiming Huang", "Long Bai", "Beilei Cui", "Kun Yuan", "Guankun Wang", "Mobarakol Islam", "Nicolas Padoy", "Nassir Navab", "Hongliang Ren"], "summary": "In contemporary surgical research and practice, accurately comprehending 3D\nsurgical scenes with text-promptable capabilities is particularly crucial for\nsurgical planning and real-time intra-operative guidance, where precisely\nidentifying and interacting with surgical tools and anatomical structures is\nparamount. However, existing works focus on surgical vision-language model\n(VLM), 3D reconstruction, and segmentation separately, lacking support for\nreal-time text-promptable 3D queries. In this paper, we present SurgTPGS, a\nnovel text-promptable Gaussian Splatting method to fill this gap. We introduce\na 3D semantics feature learning strategy incorporating the Segment Anything\nmodel and state-of-the-art vision-language models. We extract the segmented\nlanguage features for 3D surgical scene reconstruction, enabling a more\nin-depth understanding of the complex surgical environment. We also propose\nsemantic-aware deformation tracking to capture the seamless deformation of\nsemantic features, providing a more precise reconstruction for both texture and\nsemantic features. Furthermore, we present semantic region-aware optimization,\nwhich utilizes regional-based semantic information to supervise the training,\nparticularly promoting the reconstruction quality and semantic smoothness. We\nconduct comprehensive experiments on two real-world surgical datasets to\ndemonstrate the superiority of SurgTPGS over state-of-the-art methods,\nhighlighting its potential to revolutionize surgical practices. SurgTPGS paves\nthe way for developing next-generation intelligent surgical systems by\nenhancing surgical precision and safety. Our code is available at:\nhttps://github.com/lastbasket/SurgTPGS.", "comment": "MICCAI 2025. Project Page:\n  https://lastbasket.github.io/MICCAI-2025-SurgTPGS/", "pdf_url": "http://arxiv.org/pdf/2506.23309v1", "categories": ["eess.IV", "cs.CV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.23309v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.24106", "title": "On the Predictive Power of Representation Dispersion in Language Models", "authors": ["Yanhong Li", "Ming Li", "Karen Livescu", "Jiawei Zhou"], "summary": "We show that a language model's ability to predict text is tightly linked to\nthe breadth of its embedding space: models that spread their contextual\nrepresentations more widely tend to achieve lower perplexity. Concretely, we\nfind that representation dispersion - the average pairwise cosine distance\namong hidden vectors - strongly and negatively correlates with perplexity\nacross diverse model families (LLaMA, Qwen, and others) and domains (Wikipedia,\nnews, scientific abstracts). Beyond illustrating this link, we show how\ndispersion can be leveraged for a range of practical tasks without requiring\nlabeled data. First, measuring dispersion on unlabeled text allows us to\npredict downstream accuracy in new domains, offering a data-efficient tool for\nmodel selection. Next, we find that identifying layers with higher dispersion\npinpoints the best representations for retrieval-based methods such as kNN-LM,\nbypassing exhaustive layer-by-layer searches. Finally, we integrate a simple\npush-away objective into training, which increases dispersion in both\nsingle-domain and cross-domain scenarios and directly improves perplexity in\neach.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.24106v1", "categories": ["cs.CL", "cs.AI"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.24106v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23316", "title": "InfGen: Scenario Generation as Next Token Group Prediction", "authors": ["Zhenghao Peng", "Yuxin Liu", "Bolei Zhou"], "summary": "Realistic and interactive traffic simulation is essential for training and\nevaluating autonomous driving systems. However, most existing data-driven\nsimulation methods rely on static initialization or log-replay data, limiting\ntheir ability to model dynamic, long-horizon scenarios with evolving agent\npopulations. We propose InfGen, a scenario generation framework that outputs\nagent states and trajectories in an autoregressive manner. InfGen represents\nthe entire scene as a sequence of tokens, including traffic light signals,\nagent states, and motion vectors, and uses a transformer model to simulate\ntraffic over time. This design enables InfGen to continuously insert new agents\ninto traffic, supporting infinite scene generation. Experiments demonstrate\nthat InfGen produces realistic, diverse, and adaptive traffic behaviors.\nFurthermore, reinforcement learning policies trained in InfGen-generated\nscenarios achieve superior robustness and generalization, validating its\nutility as a high-fidelity simulation environment for autonomous driving. More\ninformation is available at https://metadriverse.github.io/infgen/.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23316v1", "categories": ["cs.RO", "cs.CV"], "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.23316v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.24108", "title": "Navigating with Annealing Guidance Scale in Diffusion Space", "authors": ["Shai Yehezkel", "Omer Dahary", "Andrey Voynov", "Daniel Cohen-Or"], "summary": "Denoising diffusion models excel at generating high-quality images\nconditioned on text prompts, yet their effectiveness heavily relies on careful\nguidance during the sampling process. Classifier-Free Guidance (CFG) provides a\nwidely used mechanism for steering generation by setting the guidance scale,\nwhich balances image quality and prompt alignment. However, the choice of the\nguidance scale has a critical impact on the convergence toward a visually\nappealing and prompt-adherent image. In this work, we propose an annealing\nguidance scheduler which dynamically adjusts the guidance scale over time based\non the conditional noisy signal. By learning a scheduling policy, our method\naddresses the temperamental behavior of CFG. Empirical results demonstrate that\nour guidance scheduler significantly enhances image quality and alignment with\nthe text prompt, advancing the performance of text-to-image generation.\nNotably, our novel scheduler requires no additional activations or memory\nconsumption, and can seamlessly replace the common classifier-free guidance,\noffering an improved trade-off between prompt alignment and quality.", "comment": "Project page:\n  https://annealing-guidance.github.io/annealing-guidance/", "pdf_url": "http://arxiv.org/pdf/2506.24108v1", "categories": ["cs.GR", "cs.AI", "cs.CV", "cs.LG"], "cate": "cs.GR", "url": "http://arxiv.org/abs/2506.24108v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23334", "title": "Federated Breast Cancer Detection Enhanced by Synthetic Ultrasound Image Augmentation", "authors": ["Hongyi Pan", "Ziliang Hong", "Gorkem Durak", "Ziyue Xu", "Ulas Bagci"], "summary": "Federated learning (FL) has emerged as a promising paradigm for\ncollaboratively training deep learning models across institutions without\nexchanging sensitive medical data. However, its effectiveness is often hindered\nby limited data availability and non-independent, identically distributed data\nacross participating clients, which can degrade model performance and\ngeneralization. To address these challenges, we propose a generative AI based\ndata augmentation framework that integrates synthetic image sharing into the\nfederated training process for breast cancer diagnosis via ultrasound images.\nSpecifically, we train two simple class-specific Deep Convolutional Generative\nAdversarial Networks: one for benign and one for malignant lesions. We then\nsimulate a realistic FL setting using three publicly available breast\nultrasound image datasets: BUSI, BUS-BRA, and UDIAT. FedAvg and FedProx are\nadopted as baseline FL algorithms. Experimental results show that incorporating\na suitable number of synthetic images improved the average AUC from 0.9206 to\n0.9237 for FedAvg and from 0.9429 to 0.9538 for FedProx. We also note that\nexcessive use of synthetic data reduced performance, underscoring the\nimportance of maintaining a balanced ratio of real and synthetic samples. Our\nfindings highlight the potential of generative AI based data augmentation to\nenhance FL results in the breast ultrasound image classification task.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23334v1", "categories": ["eess.IV", "cs.AI", "cs.CV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.23334v1", "date": "2025-06-29", "updated": "2025-06-29"}
{"id": "2506.24120", "title": "Data Uniformity Improves Training Efficiency and More, with a Convergence Framework Beyond the NTK Regime", "authors": ["Yuqing Wang", "Shangding Gu"], "summary": "Data selection plays a crucial role in data-driven decision-making, including\nin large language models (LLMs), and is typically task-dependent. Properties\nsuch as data quality and diversity have been extensively studied and are known\nto enhance model performance. However, it remains unclear whether there exist\nother quantitative and general principles of data selection that can\nconsistently improve performance, especially for complex tasks with limited\nprior knowledge. In this paper, we demonstrate that selecting more uniformly\ndistributed data can improve training efficiency while enhancing performance.\nSpecifically, we establish that more uniform (less biased) distribution leads\nto a larger minimum pairwise distance between data points, denoted by\n$h_{\\min}$, and prove that a smaller $h_{\\min}$ can slow down the training\ndynamics of gradient descent (GD). Moreover, we theoretically show that the\napproximation error of neural networks decreases as $h_{\\min}$ increases. Our\nanalysis introduces a convergence framework for GD beyond the Neural Tangent\nKernel (NTK) regime, applicable to a broad class of architectures, including\ntransformers, without requiring Lipschitz smoothness. This framework further\nprovides theoretical justification for the use of residual connections and\nfunction compositions in deep neural architectures. In the end, we conduct\ncomprehensive experiments for supervised fine-tuning across various settings,\nincluding different optimization strategies, model sizes, and training\ndatasets. The results consistently demonstrate that selecting data by\nmaximizing pairwise distance significantly accelerates training and achieves\ncomparable or better performance in LLMs across diverse datasets. Code and\nDatasets are available at the link:\nhttps://github.com/SafeRL-Lab/data-uniformity.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.24120v1", "categories": ["cs.LG", "cs.AI", "math.OC", "stat.ML"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.24120v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23466", "title": "FD-DiT: Frequency Domain-Directed Diffusion Transformer for Low-Dose CT Reconstruction", "authors": ["Qiqing Liu", "Guoquan Wei", "Zekun Zhou", "Yiyang Wen", "Liu Shi", "Qiegen Liu"], "summary": "Low-dose computed tomography (LDCT) reduces radiation exposure but suffers\nfrom image artifacts and loss of detail due to quantum and electronic noise,\npotentially impacting diagnostic accuracy. Transformer combined with diffusion\nmodels has been a promising approach for image generation. Nevertheless,\nexisting methods exhibit limitations in preserving finegrained image details.\nTo address this issue, frequency domain-directed diffusion transformer (FD-DiT)\nis proposed for LDCT reconstruction. FD-DiT centers on a diffusion strategy\nthat progressively introduces noise until the distribution statistically aligns\nwith that of LDCT data, followed by denoising processing. Furthermore, we\nemploy a frequency decoupling technique to concentrate noise primarily in\nhigh-frequency domain, thereby facilitating effective capture of essential\nanatomical structures and fine details. A hybrid denoising network is then\nutilized to optimize the overall data reconstruction process. To enhance the\ncapability in recognizing high-frequency noise, we incorporate sliding sparse\nlocal attention to leverage the sparsity and locality of shallow-layer\ninformation, propagating them via skip connections for improving feature\nrepresentation. Finally, we propose a learnable dynamic fusion strategy for\noptimal component integration. Experimental results demonstrate that at\nidentical dose levels, LDCT images reconstructed by FD-DiT exhibit superior\nnoise and artifact suppression compared to state-of-the-art methods.", "comment": "11pages, 11 figures", "pdf_url": "http://arxiv.org/pdf/2506.23466v1", "categories": ["eess.IV", "cs.CV", "physics.med-ph"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.23466v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.24125", "title": "FADRM: Fast and Accurate Data Residual Matching for Dataset Distillation", "authors": ["Jiacheng Cui", "Xinyue Bi", "Yaxin Luo", "Xiaohan Zhao", "Jiacheng Liu", "Zhiqiang Shen"], "summary": "Residual connection has been extensively studied and widely applied at the\nmodel architecture level. However, its potential in the more challenging\ndata-centric approaches remains unexplored. In this work, we introduce the\nconcept of Data Residual Matching for the first time, leveraging data-level\nskip connections to facilitate data generation and mitigate data information\nvanishing. This approach maintains a balance between newly acquired knowledge\nthrough pixel space optimization and existing core local information\nidentification within raw data modalities, specifically for the dataset\ndistillation task. Furthermore, by incorporating optimization-level\nrefinements, our method significantly improves computational efficiency,\nachieving superior performance while reducing training time and peak GPU memory\nusage by 50%. Consequently, the proposed method Fast and Accurate Data Residual\nMatching for Dataset Distillation (FADRM) establishes a new state-of-the-art,\ndemonstrating substantial improvements over existing methods across multiple\ndataset benchmarks in both efficiency and effectiveness. For instance, with\nResNet-18 as the student model and a 0.8% compression ratio on ImageNet-1K, the\nmethod achieves 47.7% test accuracy in single-model dataset distillation and\n50.0% in multi-model dataset distillation, surpassing RDED by +5.7% and\noutperforming state-of-the-art multi-model approaches, EDC and CV-DD, by +1.4%\nand +4.0%. Code is available at: https://github.com/Jiacheng8/FADRM.", "comment": "Code at: https://github.com/Jiacheng8/FADRM", "pdf_url": "http://arxiv.org/pdf/2506.24125v1", "categories": ["cs.CV", "cs.AI"], "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.24125v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23471", "title": "KiseKloset: Comprehensive System For Outfit Retrieval, Recommendation, And Try-On", "authors": ["Thanh-Tung Phan-Nguyen", "Khoi-Nguyen Nguyen-Ngoc", "Tam V. Nguyen", "Minh-Triet Tran", "Trung-Nghia Le"], "summary": "The global fashion e-commerce industry has become integral to people's daily\nlives, leveraging technological advancements to offer personalized shopping\nexperiences, primarily through recommendation systems that enhance customer\nengagement through personalized suggestions. To improve customers' experience\nin online shopping, we propose a novel comprehensive KiseKloset system for\noutfit retrieval, recommendation, and try-on. We explore two approaches for\noutfit retrieval: similar item retrieval and text feedback-guided item\nretrieval. Notably, we introduce a novel transformer architecture designed to\nrecommend complementary items from diverse categories. Furthermore, we enhance\nthe overall performance of the search pipeline by integrating approximate\nalgorithms to optimize the search process. Additionally, addressing the crucial\nneeds of online shoppers, we employ a lightweight yet efficient virtual try-on\nframework capable of real-time operation, memory efficiency, and maintaining\nrealistic outputs compared to its predecessors. This virtual try-on module\nempowers users to visualize specific garments on themselves, enhancing the\ncustomers' experience and reducing costs associated with damaged items for\nretailers. We deployed our end-to-end system for online users to test and\nprovide feedback, enabling us to measure their satisfaction levels. The results\nof our user study revealed that 84% of participants found our comprehensive\nsystem highly useful, significantly improving their online shopping experience.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23471v1", "categories": ["cs.IR", "cs.CV"], "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.23471v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23484", "title": "TAG-WM: Tamper-Aware Generative Image Watermarking via Diffusion Inversion Sensitivity", "authors": ["Yuzhuo Chen", "Zehua Ma", "Han Fang", "Weiming Zhang", "Nenghai Yu"], "summary": "AI-generated content (AIGC) enables efficient visual creation but raises\ncopyright and authenticity risks. As a common technique for integrity\nverification and source tracing, digital image watermarking is regarded as a\npotential solution to above issues. Among these, watermarking methods capable\nof preserving the generation quality are receiving increased attention.\nHowever, the proliferation and high performance of generative image editing\napplications have elevated the risks of malicious tampering, creating new\ndemands. 1) The tamper robustness of current lossless visual quality watermarks\nremains constrained by the modification-sensitive diffusion inversion process,\nnecessitating enhanced robustness. 2) The improved tampering quality and rapid\niteration cycles render passive tampering detection methods inadequate, making\nproactive tampering localization capability a desired feature for watermarks.\nTo address these requirements, this paper proposes a Tamper-Aware Generative\nimage WaterMarking method named TAG-WM. The proposed method comprises four key\nmodules: a dual-mark joint sampling (DMJS) algorithm for embedding copyright\nand localization watermarks into the latent space while preserving generative\nquality, the watermark latent reconstruction (WLR) utilizing reversed DMJS, a\ndense variation region detector (DVRD) leveraging diffusion inversion\nsensitivity to identify tampered areas via statistical deviation analysis, and\nthe tamper-aware decoding (TAD) guided by localization results. The\nexperimental results indicate that TAG-WM achieves SOTA tampering robustness\nand tampering localization capability with distortions while maintaining\nlossless generation quality and a considerable capacity of 256 bits.", "comment": "Accepted by ICCV 2025 (2025 IEEE/CVF International Conference on\n  Computer Vision)", "pdf_url": "http://arxiv.org/pdf/2506.23484v1", "categories": ["cs.MM", "cs.CV", "eess.IV", "I.3.3; I.4.9"], "cate": "cs.MM", "url": "http://arxiv.org/abs/2506.23484v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23490", "title": "UltraTwin: Towards Cardiac Anatomical Twin Generation from Multi-view 2D Ultrasound", "authors": ["Junxuan Yu", "Yaofei Duan", "Yuhao Huang", "Yu Wang", "Rongbo Ling", "Weihao Luo", "Ang Zhang", "Jingxian Xu", "Qiongying Ni", "Yongsong Zhou", "Binghan Li", "Haoran Dou", "Liping Liu", "Yanfen Chu", "Feng Geng", "Zhe Sheng", "Zhifeng Ding", "Dingxin Zhang", "Rui Huang", "Yuhang Zhang", "Xiaowei Xu", "Tao Tan", "Dong Ni", "Zhongshan Gou", "Xin Yang"], "summary": "Echocardiography is routine for cardiac examination. However, 2D ultrasound\n(US) struggles with accurate metric calculation and direct observation of 3D\ncardiac structures. Moreover, 3D US is limited by low resolution, small field\nof view and scarce availability in practice. Constructing the cardiac\nanatomical twin from 2D images is promising to provide precise treatment\nplanning and clinical quantification. However, it remains challenging due to\nthe rare paired data, complex structures, and US noises. In this study, we\nintroduce a novel generative framework UltraTwin, to obtain cardiac anatomical\ntwin from sparse multi-view 2D US. Our contribution is three-fold. First,\npioneered the construction of a real-world and high-quality dataset containing\nstrictly paired multi-view 2D US and CT, and pseudo-paired data. Second, we\npropose a coarse-to-fine scheme to achieve hierarchical reconstruction\noptimization. Last, we introduce an implicit autoencoder for topology-aware\nconstraints. Extensive experiments show that UltraTwin reconstructs\nhigh-quality anatomical twins versus strong competitors. We believe it advances\nanatomical twin modeling for potential applications in personalized cardiac\ncare.", "comment": "accepted by miccai 2025", "pdf_url": "http://arxiv.org/pdf/2506.23490v1", "categories": ["eess.IV", "cs.AI", "cs.CV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.23490v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23492", "title": "Sample Margin-Aware Recalibration of Temperature Scaling", "authors": ["Haolan Guo", "Linwei Tao", "Haoyang Luo", "Minjing Dong", "Chang Xu"], "summary": "Recent advances in deep learning have significantly improved predictive\naccuracy. However, modern neural networks remain systematically overconfident,\nposing risks for deployment in safety-critical scenarios. Current post-hoc\ncalibration methods face a fundamental dilemma: global approaches like\nTemperature Scaling apply uniform adjustments across all samples, introducing\nhigh bias despite computational efficiency, while more expressive methods that\noperate on full logit distributions suffer from high variance due to noisy\nhigh-dimensional inputs and insufficient validation data. To address these\nchallenges, we propose Sample Margin-Aware Recalibration of Temperature\n(SMART), a lightweight, data-efficient recalibration method that precisely\nscales logits based on the margin between the top two logits -- termed the\nlogit gap. Specifically, the logit gap serves as a denoised, scalar signal\ndirectly tied to decision boundary uncertainty, providing a robust indicator\nthat avoids the noise inherent in high-dimensional logit spaces while\npreserving model prediction invariance. Meanwhile, SMART employs a novel\nsoft-binned Expected Calibration Error (SoftECE) objective that balances model\nbias and variance through adaptive binning, enabling stable parameter updates\neven with extremely limited calibration data. Extensive evaluations across\ndiverse datasets and architectures demonstrate that SMART achieves\nstate-of-the-art calibration performance even with substantially fewer\nparameters compared to existing parametric methods, offering a principled,\nrobust, and highly efficient solution for practical uncertainty quantification\nin neural network predictions. The source code is available at:\nhttps://anonymous.4open.science/r/SMART-8B11.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23492v1", "categories": ["cs.LG", "cs.AI", "cs.CV"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.23492v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23506", "title": "Artificial Intelligence-assisted Pixel-level Lung (APL) Scoring for Fast and Accurate Quantification in Ultra-short Echo-time MRI", "authors": ["Bowen Xin", "Rohan Hickey", "Tamara Blake", "Jin Jin", "Claire E Wainwright", "Thomas Benkert", "Alto Stemmer", "Peter Sly", "David Coman", "Jason Dowling"], "summary": "Lung magnetic resonance imaging (MRI) with ultrashort echo-time (UTE)\nrepresents a recent breakthrough in lung structure imaging, providing image\nresolution and quality comparable to computed tomography (CT). Due to the\nabsence of ionising radiation, MRI is often preferred over CT in paediatric\ndiseases such as cystic fibrosis (CF), one of the most common genetic disorders\nin Caucasians. To assess structural lung damage in CF imaging, CT scoring\nsystems provide valuable quantitative insights for disease diagnosis and\nprogression. However, few quantitative scoring systems are available in\nstructural lung MRI (e.g., UTE-MRI). To provide fast and accurate\nquantification in lung MRI, we investigated the feasibility of novel Artificial\nintelligence-assisted Pixel-level Lung (APL) scoring for CF. APL scoring\nconsists of 5 stages, including 1) image loading, 2) AI lung segmentation, 3)\nlung-bounded slice sampling, 4) pixel-level annotation, and 5) quantification\nand reporting. The results shows that our APL scoring took 8.2 minutes per\nsubject, which was more than twice as fast as the previous grid-level scoring.\nAdditionally, our pixel-level scoring was statistically more accurate\n(p=0.021), while strongly correlating with grid-level scoring (R=0.973,\np=5.85e-9). This tool has great potential to streamline the workflow of UTE\nlung MRI in clinical settings, and be extended to other structural lung MRI\nsequences (e.g., BLADE MRI), and for other lung diseases (e.g.,\nbronchopulmonary dysplasia).", "comment": "Oral presentation in ISMRM2025", "pdf_url": "http://arxiv.org/pdf/2506.23506v1", "categories": ["eess.IV", "cs.AI", "cs.CV", "physics.med-ph"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.23506v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23516", "title": "FedWSQ: Efficient Federated Learning with Weight Standardization and Distribution-Aware Non-Uniform Quantization", "authors": ["Seung-Wook Kim", "Seongyeol Kim", "Jiah Kim", "Seowon Ji", "Se-Ho Lee"], "summary": "Federated learning (FL) often suffers from performance degradation due to key\nchallenges such as data heterogeneity and communication constraints. To address\nthese limitations, we present a novel FL framework called FedWSQ, which\nintegrates weight standardization (WS) and the proposed distribution-aware\nnon-uniform quantization (DANUQ). WS enhances FL performance by filtering out\nbiased components in local updates during training, thereby improving the\nrobustness of the model against data heterogeneity and unstable client\nparticipation. In addition, DANUQ minimizes quantization errors by leveraging\nthe statistical properties of local model updates. As a result, FedWSQ\nsignificantly reduces communication overhead while maintaining superior model\naccuracy. Extensive experiments on FL benchmark datasets demonstrate that\nFedWSQ consistently outperforms existing FL methods across various challenging\nFL settings, including extreme data heterogeneity and ultra-low-bit\ncommunication scenarios.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23516v1", "categories": ["cs.LG", "cs.AI", "cs.CV"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.23516v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23537", "title": "AFUNet: Cross-Iterative Alignment-Fusion Synergy for HDR Reconstruction via Deep Unfolding Paradigm", "authors": ["Xinyue Li", "Zhangkai Ni", "Wenhan Yang"], "summary": "Existing learning-based methods effectively reconstruct HDR images from\nmulti-exposure LDR inputs with extended dynamic range and improved detail, but\nthey rely more on empirical design rather than theoretical foundation, which\ncan impact their reliability. To address these limitations, we propose the\ncross-iterative Alignment and Fusion deep Unfolding Network (AFUNet), where HDR\nreconstruction is systematically decoupled into two interleaved subtasks --\nalignment and fusion -- optimized through alternating refinement, achieving\nsynergy between the two subtasks to enhance the overall performance. Our method\nformulates multi-exposure HDR reconstruction from a Maximum A Posteriori (MAP)\nestimation perspective, explicitly incorporating spatial correspondence priors\nacross LDR images and naturally bridging the alignment and fusion subproblems\nthrough joint constraints. Building on the mathematical foundation, we\nreimagine traditional iterative optimization through unfolding -- transforming\nthe conventional solution process into an end-to-end trainable AFUNet with\ncarefully designed modules that work progressively. Specifically, each\niteration of AFUNet incorporates an Alignment-Fusion Module (AFM) that\nalternates between a Spatial Alignment Module (SAM) for alignment and a Channel\nFusion Module (CFM) for adaptive feature fusion, progressively bridging\nmisaligned content and exposure discrepancies. Extensive qualitative and\nquantitative evaluations demonstrate AFUNet's superior performance,\nconsistently surpassing state-of-the-art methods. Our code is available at:\nhttps://github.com/eezkni/AFUNet", "comment": "Accepted to International Conference on Computer Vision (ICCV) 2025", "pdf_url": "http://arxiv.org/pdf/2506.23537v1", "categories": ["eess.IV", "cs.CV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.23537v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23563", "title": "MMReason: An Open-Ended Multi-Modal Multi-Step Reasoning Benchmark for MLLMs Toward AGI", "authors": ["Huanjin Yao", "Jiaxing Huang", "Yawen Qiu", "Michael K. Chen", "Wenzheng Liu", "Wei Zhang", "Wenjie Zeng", "Xikun Zhang", "Jingyi Zhang", "Yuxin Song", "Wenhao Wu", "Dacheng Tao"], "summary": "Reasoning plays a crucial role in advancing Multimodal Large Language Models\n(MLLMs) toward Artificial General Intelligence. However, existing MLLM\nbenchmarks often fall short in precisely and comprehensively evaluating\nlong-chain reasoning abilities from three key aspects: (1) lack of difficulty\nand diversity, (2) susceptibility to guessability and memorization, (3)\ninadequate assessment of intermediate reasoning steps. To fill this gap, we\nintroduce MMReason, a new benchmark designed to precisely and comprehensively\nevaluate MLLM long-chain reasoning capability with diverse, open-ended,\nchallenging questions. First, we curate challenging questions requiring\nmulti-step reasoning from various fields (i.e., 6 disciplines) and multiple\ndifficulty levels (i.e., from pre-university to university, and from\nfoundational to competition tiers). Second, these questions are reformulated\ninto an open-ended format and filtered using a multi-model voting technique to\neliminate shortcut cases related to guessing and memorization, ensuring robust\nreasoning evaluations. Third, we annotate the questions with detailed\nstep-by-step solutions, and design a reference-based ternary scoring mechanism\nto reliably assess intermediate reasoning steps. With MMReason, we benchmark\npopular leading MLLMs and provide an in-depth analysis of their reasoning\ncapabilities. We hope MMReason will serve as a valuable resource for advancing\nMLLM reasoning research. Code will be available at\nhttps://github.com/HJYao00/MMReason.", "comment": "Technical report", "pdf_url": "http://arxiv.org/pdf/2506.23563v1", "categories": ["cs.AI", "cs.CL", "cs.CV"], "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.23563v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23584", "title": "A Clinically-Grounded Two-Stage Framework for Renal CT Report Generation", "authors": ["Renjie Liang", "Zhengkang Fan", "Jinqian Pan", "Chenkun Sun", "Russell Terry", "Jie Xu"], "summary": "Generating radiology reports from CT scans remains a complex task due to the\nnuanced nature of medical imaging and the variability in clinical\ndocumentation. In this study, we propose a two-stage framework for generating\nrenal radiology reports from 2D CT slices. First, we extract structured\nabnormality features using a multi-task learning model trained to identify\nlesion attributes such as location, size, enhancement, and attenuation. These\nextracted features are subsequently combined with the corresponding CT image\nand fed into a fine-tuned vision-language model to generate natural language\nreport sentences aligned with clinical findings. We conduct experiments on a\ncurated dataset of renal CT studies with manually annotated\nsentence-slice-feature triplets and evaluate performance using both\nclassification metrics and natural language generation metrics. Our results\ndemonstrate that the proposed model outperforms random baselines across all\nabnormality types, and the generated reports capture key clinical content with\nreasonable textual accuracy. This exploratory work highlights the feasibility\nof modular, feature-informed report generation for renal imaging. Future\nefforts will focus on extending this pipeline to 3D CT volumes and further\nimproving clinical fidelity in multimodal medical AI systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23584v1", "categories": ["eess.IV", "cs.AI", "cs.CV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.23584v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23664", "title": "Diffusion Model-based Data Augmentation Method for Fetal Head Ultrasound Segmentation", "authors": ["Fangyijie Wang", "Kevin Whelan", "F√©lix Balado", "Gu√©nol√© Silvestre", "Kathleen M. Curran"], "summary": "Medical image data is less accessible than in other domains due to privacy\nand regulatory constraints. In addition, labeling requires costly,\ntime-intensive manual image annotation by clinical experts. To overcome these\nchallenges, synthetic medical data generation offers a promising solution.\nGenerative AI (GenAI), employing generative deep learning models, has proven\neffective at producing realistic synthetic images. This study proposes a novel\nmask-guided GenAI approach using diffusion models to generate synthetic fetal\nhead ultrasound images paired with segmentation masks. These synthetic pairs\naugment real datasets for supervised fine-tuning of the Segment Anything Model\n(SAM). Our results show that the synthetic data captures real image features\neffectively, and this approach reaches state-of-the-art fetal head\nsegmentation, especially when trained with a limited number of real image-mask\npairs. In particular, the segmentation reaches Dice Scores of 94.66\\% and\n94.38\\% using a handful of ultrasound images from the Spanish and African\ncohorts, respectively. Our code, models, and data are available on GitHub.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23664v1", "categories": ["eess.IV", "cs.CV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.23664v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23700", "title": "MedSAM-CA: A CNN-Augmented ViT with Attention-Enhanced Multi-Scale Fusion for Medical Image Segmentation", "authors": ["Peiting Tian", "Xi Chen", "Haixia Bi", "Fan Li"], "summary": "Medical image segmentation plays a crucial role in clinical diagnosis and\ntreatment planning, where accurate boundary delineation is essential for\nprecise lesion localization, organ identification, and quantitative assessment.\nIn recent years, deep learning-based methods have significantly advanced\nsegmentation accuracy. However, two major challenges remain. First, the\nperformance of these methods heavily relies on large-scale annotated datasets,\nwhich are often difficult to obtain in medical scenarios due to privacy\nconcerns and high annotation costs. Second, clinically challenging scenarios,\nsuch as low contrast in certain imaging modalities and blurry lesion boundaries\ncaused by malignancy, still pose obstacles to precise segmentation. To address\nthese challenges, we propose MedSAM-CA, an architecture-level fine-tuning\napproach that mitigates reliance on extensive manual annotations by adapting\nthe pretrained foundation model, Medical Segment Anything (MedSAM). MedSAM-CA\nintroduces two key components: the Convolutional Attention-Enhanced Boundary\nRefinement Network (CBR-Net) and the Attention-Enhanced Feature Fusion Block\n(Atte-FFB). CBR-Net operates in parallel with the MedSAM encoder to recover\nboundary information potentially overlooked by long-range attention mechanisms,\nleveraging hierarchical convolutional processing. Atte-FFB, embedded in the\nMedSAM decoder, fuses multi-level fine-grained features from skip connections\nin CBR-Net with global representations upsampled within the decoder to enhance\nboundary delineation accuracy. Experiments on publicly available datasets\ncovering dermoscopy, CT, and MRI imaging modalities validate the effectiveness\nof MedSAM-CA. On dermoscopy dataset, MedSAM-CA achieves 94.43% Dice with only\n2% of full training data, reaching 97.25% of full-data training performance,\ndemonstrating strong effectiveness in low-resource clinical settings.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23700v1", "categories": ["eess.IV", "cs.CV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.23700v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23701", "title": "MDPG: Multi-domain Diffusion Prior Guidance for MRI Reconstruction", "authors": ["Lingtong Zhang", "Mengdie Song", "Xiaohan Hao", "Huayu Mai", "Bensheng Qiu"], "summary": "Magnetic Resonance Imaging (MRI) reconstruction is essential in medical\ndiagnostics. As the latest generative models, diffusion models (DMs) have\nstruggled to produce high-fidelity images due to their stochastic nature in\nimage domains. Latent diffusion models (LDMs) yield both compact and detailed\nprior knowledge in latent domains, which could effectively guide the model\ntowards more effective learning of the original data distribution. Inspired by\nthis, we propose Multi-domain Diffusion Prior Guidance (MDPG) provided by\npre-trained LDMs to enhance data consistency in MRI reconstruction tasks.\nSpecifically, we first construct a Visual-Mamba-based backbone, which enables\nefficient encoding and reconstruction of under-sampled images. Then pre-trained\nLDMs are integrated to provide conditional priors in both latent and image\ndomains. A novel Latent Guided Attention (LGA) is proposed for efficient fusion\nin multi-level latent domains. Simultaneously, to effectively utilize a prior\nin both the k-space and image domain, under-sampled images are fused with\ngenerated full-sampled images by the Dual-domain Fusion Branch (DFB) for\nself-adaption guidance. Lastly, to further enhance the data consistency, we\npropose a k-space regularization strategy based on the non-auto-calibration\nsignal (NACS) set. Extensive experiments on two public MRI datasets fully\ndemonstrate the effectiveness of the proposed methodology. The code is\navailable at https://github.com/Zolento/MDPG.", "comment": "Accept by MICCAI2025", "pdf_url": "http://arxiv.org/pdf/2506.23701v1", "categories": ["eess.IV", "cs.CV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.23701v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23717", "title": "Towards Efficient and Accurate Spiking Neural Networks via Adaptive Bit Allocation", "authors": ["Xingting Yao", "Qinghao Hu", "Fei Zhou", "Tielong Liu", "Gang Li", "Peisong Wang", "Jian Cheng"], "summary": "Multi-bit spiking neural networks (SNNs) have recently become a heated\nresearch spot, pursuing energy-efficient and high-accurate AI. However, with\nmore bits involved, the associated memory and computation demands escalate to\nthe point where the performance improvements become disproportionate. Based on\nthe insight that different layers demonstrate different importance and extra\nbits could be wasted and interfering, this paper presents an adaptive bit\nallocation strategy for direct-trained SNNs, achieving fine-grained layer-wise\nallocation of memory and computation resources. Thus, SNN's efficiency and\naccuracy can be improved. Specifically, we parametrize the temporal lengths and\nthe bit widths of weights and spikes, and make them learnable and controllable\nthrough gradients. To address the challenges caused by changeable bit widths\nand temporal lengths, we propose the refined spiking neuron, which can handle\ndifferent temporal lengths, enable the derivation of gradients for temporal\nlengths, and suit spike quantization better. In addition, we theoretically\nformulate the step-size mismatch problem of learnable bit widths, which may\nincur severe quantization errors to SNN, and accordingly propose the step-size\nrenewal mechanism to alleviate this issue. Experiments on various datasets,\nincluding the static CIFAR and ImageNet and the dynamic CIFAR-DVS and\nDVS-GESTURE, demonstrate that our methods can reduce the overall memory and\ncomputation cost while achieving higher accuracy. Particularly, our\nSEWResNet-34 can achieve a 2.69\\% accuracy gain and 4.16$\\times$ lower bit\nbudgets over the advanced baseline work on ImageNet. This work will be fully\nopen-sourced.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23717v1", "categories": ["cs.NE", "cs.AI", "cs.CV", "cs.LG"], "cate": "cs.NE", "url": "http://arxiv.org/abs/2506.23717v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23721", "title": "Deep Learning-Based Semantic Segmentation for Real-Time Kidney Imaging and Measurements with Augmented Reality-Assisted Ultrasound", "authors": ["Gijs Luijten", "Roberto Maria Scardigno", "Lisle Faray de Paiva", "Peter Hoyer", "Jens Kleesiek", "Domenico Buongiorno", "Vitoantonio Bevilacqua", "Jan Egger"], "summary": "Ultrasound (US) is widely accessible and radiation-free but has a steep\nlearning curve due to its dynamic nature and non-standard imaging planes.\nAdditionally, the constant need to shift focus between the US screen and the\npatient poses a challenge. To address these issues, we integrate deep learning\n(DL)-based semantic segmentation for real-time (RT) automated kidney volumetric\nmeasurements, which are essential for clinical assessment but are traditionally\ntime-consuming and prone to fatigue. This automation allows clinicians to\nconcentrate on image interpretation rather than manual measurements.\nComplementing DL, augmented reality (AR) enhances the usability of US by\nprojecting the display directly into the clinician's field of view, improving\nergonomics and reducing the cognitive load associated with screen-to-patient\ntransitions. Two AR-DL-assisted US pipelines on HoloLens-2 are proposed: one\nstreams directly via the application programming interface for a wireless\nsetup, while the other supports any US device with video output for broader\naccessibility. We evaluate RT feasibility and accuracy using the Open Kidney\nDataset and open-source segmentation models (nnU-Net, Segmenter, YOLO with\nMedSAM and LiteMedSAM). Our open-source GitHub pipeline includes model\nimplementations, measurement algorithms, and a Wi-Fi-based streaming solution,\nenhancing US training and diagnostics, especially in point-of-care settings.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23721v1", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.HC", "cs.LG"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.23721v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23731", "title": "Radioactive Watermarks in Diffusion and Autoregressive Image Generative Models", "authors": ["Michel Meintz", "Jan Dubi≈Ñski", "Franziska Boenisch", "Adam Dziedzic"], "summary": "Image generative models have become increasingly popular, but training them\nrequires large datasets that are costly to collect and curate. To circumvent\nthese costs, some parties may exploit existing models by using the generated\nimages as training data for their own models. In general, watermarking is a\nvaluable tool for detecting unauthorized use of generated images. However, when\nthese images are used to train a new model, watermarking can only enable\ndetection if the watermark persists through training and remains identifiable\nin the outputs of the newly trained model - a property known as radioactivity.\nWe analyze the radioactivity of watermarks in images generated by diffusion\nmodels (DMs) and image autoregressive models (IARs). We find that existing\nwatermarking methods for DMs fail to retain radioactivity, as watermarks are\neither erased during encoding into the latent space or lost in the\nnoising-denoising process (during the training in the latent space). Meanwhile,\ndespite IARs having recently surpassed DMs in image generation quality and\nefficiency, no radioactive watermarking methods have been proposed for them. To\novercome this limitation, we propose the first watermarking method tailored for\nIARs and with radioactivity in mind - drawing inspiration from techniques in\nlarge language models (LLMs), which share IARs' autoregressive paradigm. Our\nextensive experimental evaluation highlights our method's effectiveness in\npreserving radioactivity within IARs, enabling robust provenance tracking, and\npreventing unauthorized use of their generated images.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23731v1", "categories": ["cs.LG", "cs.CV"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.23731v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23759", "title": "Spatio-Temporal Representation Decoupling and Enhancement for Federated Instrument Segmentation in Surgical Videos", "authors": ["Zheng Fang", "Xiaoming Qi", "Chun-Mei Feng", "Jialun Pei", "Weixin Si", "Yueming Jin"], "summary": "Surgical instrument segmentation under Federated Learning (FL) is a promising\ndirection, which enables multiple surgical sites to collaboratively train the\nmodel without centralizing datasets. However, there exist very limited FL works\nin surgical data science, and FL methods for other modalities do not consider\ninherent characteristics in surgical domain: i) different scenarios show\ndiverse anatomical backgrounds while highly similar instrument representation;\nii) there exist surgical simulators which promote large-scale synthetic data\ngeneration with minimal efforts. In this paper, we propose a novel Personalized\nFL scheme, Spatio-Temporal Representation Decoupling and Enhancement (FedST),\nwhich wisely leverages surgical domain knowledge during both local-site and\nglobal-server training to boost segmentation. Concretely, our model embraces a\nRepresentation Separation and Cooperation (RSC) mechanism in local-site\ntraining, which decouples the query embedding layer to be trained privately, to\nencode respective backgrounds. Meanwhile, other parameters are optimized\nglobally to capture the consistent representations of instruments, including\nthe temporal layer to capture similar motion patterns. A textual-guided channel\nselection is further designed to highlight site-specific features, facilitating\nmodel adapta tion to each site. Moreover, in global-server training, we propose\nSynthesis-based Explicit Representation Quantification (SERQ), which defines an\nexplicit representation target based on synthetic data to synchronize the model\nconvergence during fusion for improving model generalization.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23759v1", "categories": ["eess.IV", "cs.CV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.23759v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23824", "title": "Supercm: Revisiting Clustering for Semi-Supervised Learning", "authors": ["Durgesh Singh", "Ahcene Boubekki", "Robert Jenssen", "Michael C. Kampffmeyer"], "summary": "The development of semi-supervised learning (SSL) has in recent years largely\nfocused on the development of new consistency regularization or entropy\nminimization approaches, often resulting in models with complex training\nstrategies to obtain the desired results. In this work, we instead propose a\nnovel approach that explicitly incorporates the underlying clustering\nassumption in SSL through extending a recently proposed differentiable\nclustering module. Leveraging annotated data to guide the cluster centroids\nresults in a simple end-to-end trainable deep SSL approach. We demonstrate that\nthe proposed model improves the performance over the supervised-only baseline\nand show that our framework can be used in conjunction with other SSL methods\nto further boost their performance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23824v1", "categories": ["cs.LG", "cs.CV"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.23824v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.23957", "title": "GaVS: 3D-Grounded Video Stabilization via Temporally-Consistent Local Reconstruction and Rendering", "authors": ["Zinuo You", "Stamatios Georgoulis", "Anpei Chen", "Siyu Tang", "Dengxin Dai"], "summary": "Video stabilization is pivotal for video processing, as it removes unwanted\nshakiness while preserving the original user motion intent. Existing\napproaches, depending on the domain they operate, suffer from several issues\n(e.g. geometric distortions, excessive cropping, poor generalization) that\ndegrade the user experience. To address these issues, we introduce\n\\textbf{GaVS}, a novel 3D-grounded approach that reformulates video\nstabilization as a temporally-consistent `local reconstruction and rendering'\nparadigm. Given 3D camera poses, we augment a reconstruction model to predict\nGaussian Splatting primitives, and finetune it at test-time, with multi-view\ndynamics-aware photometric supervision and cross-frame regularization, to\nproduce temporally-consistent local reconstructions. The model are then used to\nrender each stabilized frame. We utilize a scene extrapolation module to avoid\nframe cropping. Our method is evaluated on a repurposed dataset, instilled with\n3D-grounded information, covering samples with diverse camera motions and scene\ndynamics. Quantitatively, our method is competitive with or superior to\nstate-of-the-art 2D and 2.5D approaches in terms of conventional task metrics\nand new geometry consistency. Qualitatively, our method produces noticeably\nbetter results compared to alternatives, validated by the user study.", "comment": "siggraph 2025, project website: https://sinoyou.github.io/gavs", "pdf_url": "http://arxiv.org/pdf/2506.23957v1", "categories": ["cs.GR", "cs.CV"], "cate": "cs.GR", "url": "http://arxiv.org/abs/2506.23957v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.24000", "title": "The Illusion of Progress? A Critical Look at Test-Time Adaptation for Vision-Language Models", "authors": ["Lijun Sheng", "Jian Liang", "Ran He", "Zilei Wang", "Tieniu Tan"], "summary": "Test-time adaptation (TTA) methods have gained significant attention for\nenhancing the performance of vision-language models (VLMs) such as CLIP during\ninference, without requiring additional labeled data. However, current TTA\nresearches generally suffer from major limitations such as duplication of\nbaseline results, limited evaluation metrics, inconsistent experimental\nsettings, and insufficient analysis. These problems hinder fair comparisons\nbetween TTA methods and obscure their practical strengths and weaknesses. To\naddress these challenges, we introduce TTA-VLM, a comprehensive benchmark for\nevaluating TTA methods on VLMs. Our benchmark implements 8 episodic TTA and 7\nonline TTA methods within a unified and reproducible framework, and evaluates\nthem across 15 widely used datasets. Unlike prior studies focused solely on\nCLIP, we extend the evaluation to SigLIP--a model trained with a Sigmoid\nloss--and include training-time tuning methods such as CoOp, MaPLe, and TeCoA\nto assess generality. Beyond classification accuracy, TTA-VLM incorporates\nvarious evaluation metrics, including robustness, calibration,\nout-of-distribution detection, and stability, enabling a more holistic\nassessment of TTA methods. Through extensive experiments, we find that 1)\nexisting TTA methods produce limited gains compared to the previous pioneering\nwork; 2) current TTA methods exhibit poor collaboration with training-time\nfine-tuning methods; 3) accuracy gains frequently come at the cost of reduced\nmodel trustworthiness. We release TTA-VLM to provide fair comparison and\ncomprehensive evaluation of TTA methods for VLMs, and we hope it encourages the\ncommunity to develop more reliable and generalizable TTA strategies.", "comment": "Github link: https://github.com/TomSheng21/tta-vlm", "pdf_url": "http://arxiv.org/pdf/2506.24000v1", "categories": ["cs.LG", "cs.CV"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.24000v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.24003", "title": "ShapeKit", "authors": ["Junqi Liu", "Dongli He", "Wenxuan Li", "Ningyu Wang", "Alan L. Yuille", "Zongwei Zhou"], "summary": "In this paper, we present a practical approach to improve anatomical shape\naccuracy in whole-body medical segmentation. Our analysis shows that a\nshape-focused toolkit can enhance segmentation performance by over 8%, without\nthe need for model re-training or fine-tuning. In comparison, modifications to\nmodel architecture typically lead to marginal gains of less than 3%. Motivated\nby this observation, we introduce ShapeKit, a flexible and easy-to-integrate\ntoolkit designed to refine anatomical shapes. This work highlights the\nunderappreciated value of shape-based tools and calls attention to their\npotential impact within the medical segmentation community.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.24003v1", "categories": ["eess.IV", "cs.CV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.24003v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.24016", "title": "EXPERT: An Explainable Image Captioning Evaluation Metric with Structured Explanations", "authors": ["Hyunjong Kim", "Sangyeop Kim", "Jongheon Jeong", "Yeongjae Cho", "Sungzoon Cho"], "summary": "Recent advances in large language models and vision-language models have led\nto growing interest in explainable evaluation metrics for image captioning.\nHowever, these metrics generate explanations without standardized criteria, and\nthe overall quality of the generated explanations remains unverified. In this\npaper, we propose EXPERT, a reference-free evaluation metric that provides\nstructured explanations based on three fundamental criteria: fluency,\nrelevance, and descriptiveness. By constructing large-scale datasets of\nhigh-quality structured explanations, we develop a two-stage evaluation\ntemplate to effectively supervise a vision-language model for both scoring and\nexplanation generation. EXPERT achieves state-of-the-art results on benchmark\ndatasets while providing significantly higher-quality explanations than\nexisting metrics, as validated through comprehensive human evaluation. Our code\nand datasets are available at https://github.com/hjkim811/EXPERT.", "comment": "Accepted at ACL 2025 Findings", "pdf_url": "http://arxiv.org/pdf/2506.24016v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.24016v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.24034", "title": "Supervised Diffusion-Model-Based PET Image Reconstruction", "authors": ["George Webber", "Alexander Hammers", "Andrew P King", "Andrew J Reader"], "summary": "Diffusion models (DMs) have recently been introduced as a regularizing prior\nfor PET image reconstruction, integrating DMs trained on high-quality PET\nimages with unsupervised schemes that condition on measured data. While these\napproaches have potential generalization advantages due to their independence\nfrom the scanner geometry and the injected activity level, they forgo the\nopportunity to explicitly model the interaction between the DM prior and noisy\nmeasurement data, potentially limiting reconstruction accuracy. To address\nthis, we propose a supervised DM-based algorithm for PET reconstruction. Our\nmethod enforces the non-negativity of PET's Poisson likelihood model and\naccommodates the wide intensity range of PET images. Through experiments on\nrealistic brain PET phantoms, we demonstrate that our approach outperforms or\nmatches state-of-the-art deep learning-based methods quantitatively across a\nrange of dose levels. We further conduct ablation studies to demonstrate the\nbenefits of the proposed components in our model, as well as its dependence on\ntraining data, parameter count, and number of diffusion steps. Additionally, we\nshow that our approach enables more accurate posterior sampling than\nunsupervised DM-based methods, suggesting improved uncertainty estimation.\nFinally, we extend our methodology to a practical approach for fully 3D PET and\npresent example results from real [$^{18}$F]FDG brain PET data.", "comment": "12 pages, 6 figures. Submitted to MICCAI 2025, not peer-reviewed", "pdf_url": "http://arxiv.org/pdf/2506.24034v1", "categories": ["physics.med-ph", "cs.CV"], "cate": "physics.med-ph", "url": "http://arxiv.org/abs/2506.24034v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.24074", "title": "C3VDv2 -- Colonoscopy 3D video dataset with enhanced realism", "authors": ["Mayank V. Golhar", "Lucas Sebastian Galeano Fretes", "Loren Ayers", "Venkata S. Akshintala", "Taylor L. Bobrow", "Nicholas J. Durr"], "summary": "Computer vision techniques have the potential to improve the diagnostic\nperformance of colonoscopy, but the lack of 3D colonoscopy datasets for\ntraining and validation hinders their development. This paper introduces\nC3VDv2, the second version (v2) of the high-definition Colonoscopy 3D Video\nDataset, featuring enhanced realism designed to facilitate the quantitative\nevaluation of 3D colon reconstruction algorithms. 192 video sequences were\ncaptured by imaging 60 unique, high-fidelity silicone colon phantom segments.\nGround truth depth, surface normals, optical flow, occlusion,\nsix-degree-of-freedom pose, coverage maps, and 3D models are provided for 169\ncolonoscopy videos. Eight simulated screening colonoscopy videos acquired by a\ngastroenterologist are provided with ground truth poses. The dataset includes\n15 videos featuring colon deformations for qualitative assessment. C3VDv2\nemulates diverse and challenging scenarios for 3D reconstruction algorithms,\nincluding fecal debris, mucous pools, blood, debris obscuring the colonoscope\nlens, en-face views, and fast camera motion. The enhanced realism of C3VDv2\nwill allow for more robust and representative development and evaluation of 3D\nreconstruction algorithms.", "comment": "19 pages, 7 figures", "pdf_url": "http://arxiv.org/pdf/2506.24074v1", "categories": ["eess.IV", "cs.CV"], "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.24074v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.24108", "title": "Navigating with Annealing Guidance Scale in Diffusion Space", "authors": ["Shai Yehezkel", "Omer Dahary", "Andrey Voynov", "Daniel Cohen-Or"], "summary": "Denoising diffusion models excel at generating high-quality images\nconditioned on text prompts, yet their effectiveness heavily relies on careful\nguidance during the sampling process. Classifier-Free Guidance (CFG) provides a\nwidely used mechanism for steering generation by setting the guidance scale,\nwhich balances image quality and prompt alignment. However, the choice of the\nguidance scale has a critical impact on the convergence toward a visually\nappealing and prompt-adherent image. In this work, we propose an annealing\nguidance scheduler which dynamically adjusts the guidance scale over time based\non the conditional noisy signal. By learning a scheduling policy, our method\naddresses the temperamental behavior of CFG. Empirical results demonstrate that\nour guidance scheduler significantly enhances image quality and alignment with\nthe text prompt, advancing the performance of text-to-image generation.\nNotably, our novel scheduler requires no additional activations or memory\nconsumption, and can seamlessly replace the common classifier-free guidance,\noffering an improved trade-off between prompt alignment and quality.", "comment": "Project page:\n  https://annealing-guidance.github.io/annealing-guidance/", "pdf_url": "http://arxiv.org/pdf/2506.24108v1", "categories": ["cs.GR", "cs.AI", "cs.CV", "cs.LG"], "cate": "cs.GR", "url": "http://arxiv.org/abs/2506.24108v1", "date": "2025-06-30", "updated": "2025-06-30"}
{"id": "2506.24124", "title": "Teaching Time Series to See and Speak: Forecasting with Aligned Visual and Textual Perspectives", "authors": ["Dong Sixun", "Fan Wei", "Teresa Wu", "Fu Yanjie"], "summary": "Time series forecasting traditionally relies on unimodal numerical inputs,\nwhich often struggle to capture high-level semantic patterns due to their dense\nand unstructured nature. While recent approaches have explored representing\ntime series as text using large language models (LLMs), these methods remain\nlimited by the discrete nature of token sequences and lack the perceptual\nintuition humans typically apply, such as interpreting visual patterns. In this\npaper, we propose a multimodal contrastive learning framework that transforms\nraw time series into structured visual and textual perspectives. Rather than\nusing natural language or real-world images, we construct both modalities\ndirectly from numerical sequences. We then align these views in a shared\nsemantic space via contrastive learning, enabling the model to capture richer\nand more complementary representations. Furthermore, we introduce a variate\nselection module that leverages the aligned representations to identify the\nmost informative variables for multivariate forecasting. Extensive experiments\non fifteen short-term and six long-term forecasting benchmarks demonstrate that\nour approach consistently outperforms strong unimodal and cross-modal\nbaselines, highlighting the effectiveness of multimodal alignment in enhancing\ntime series forecasting. Code is available at:\nhttps://github.com/Ironieser/TimesCLIP.", "comment": "Code: https://github.com/Ironieser/TimesCLIP", "pdf_url": "http://arxiv.org/pdf/2506.24124v1", "categories": ["cs.LG", "cs.CV"], "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.24124v1", "date": "2025-06-30", "updated": "2025-06-30"}
