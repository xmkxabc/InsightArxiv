{"id": "2503.17403", "pdf": "https://arxiv.org/pdf/2503.17403", "abs": "https://arxiv.org/abs/2503.17403", "authors": ["Azim Akhtarshenas", "Afshin Dini", "Navid Ayoobi"], "title": "ChatGPT or A Silent Everywhere Helper: A Survey of Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have revo lutionized natural language processing\nNatural Language Processing (NLP), with Chat Generative Pre-trained Transformer\n(ChatGPT) standing out as a notable exampledue to its advanced capabilities and\nwidespread applications. This survey provides a comprehensive analysis of\nChatGPT, exploring its architecture, training processes, and functionalities.\nWe examine its integration into various domains across industries such as\ncustomer service, education, healthcare, and entertainment. A comparative\nanalysis with other LLMs highlights ChatGPT's unique features and performance\nmetrics. Regarding benchmarks, the paper examines ChatGPT's comparative\nperformance against other LLMs and discusses potential risks such as\nmisinformation, bias, and data privacy concerns. Additionally, we offer a\nnumber of figures and tables that outline the backdrop of the discussion, the\nmain ideas of the article, the numerous LLM models, a thorough list of datasets\nused for pre-training, fine-tuning, and evaluation, as well as particular LLM\napplications with pertinent references. Finally, we identify future research\ndirections and technological advancements, underscoring the evolving landscape\nof LLMs and their profound impact on artificial intelligence Artificial\nIntelligence (AI) and society."}
{"id": "2503.17407", "pdf": "https://arxiv.org/pdf/2503.17407", "abs": "https://arxiv.org/abs/2503.17407", "authors": ["Jiaheng Liu", "Dawei Zhu", "Zhiqi Bai", "Yancheng He", "Huanxuan Liao", "Haoran Que", "Zekun Wang", "Chenchen Zhang", "Ge Zhang", "Jiebin Zhang", "Yuanxing Zhang", "Zhuo Chen", "Hangyu Guo", "Shilong Li", "Ziqiang Liu", "Yong Shan", "Yifan Song", "Jiayi Tian", "Wenhao Wu", "Zhejian Zhou", "Ruijie Zhu", "Junlan Feng", "Yang Gao", "Shizhu He", "Zhoujun Li", "Tianyu Liu", "Fanyu Meng", "Wenbo Su", "Yingshui Tan", "Zili Wang", "Jian Yang", "Wei Ye", "Bo Zheng", "Wangchunshu Zhou", "Wenhao Huang", "Sujian Li", "Zhaoxiang Zhang"], "title": "A Comprehensive Survey on Long Context Language Modeling", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Efficient processing of long contexts has been a persistent pursuit in\nNatural Language Processing. With the growing number of long documents,\ndialogues, and other textual data, it is important to develop Long Context\nLanguage Models (LCLMs) that can process and analyze extensive inputs in an\neffective and efficient way. In this paper, we present a comprehensive survey\non recent advances in long-context modeling for large language models. Our\nsurvey is structured around three key aspects: how to obtain effective and\nefficient LCLMs, how to train and deploy LCLMs efficiently, and how to evaluate\nand analyze LCLMs comprehensively. For the first aspect, we discuss data\nstrategies, architectural designs, and workflow approaches oriented with long\ncontext processing. For the second aspect, we provide a detailed examination of\nthe infrastructure required for LCLM training and inference. For the third\naspect, we present evaluation paradigms for long-context comprehension and\nlong-form generation, as well as behavioral analysis and mechanism\ninterpretability of LCLMs. Beyond these three key aspects, we thoroughly\nexplore the diverse application scenarios where existing LCLMs have been\ndeployed and outline promising future development directions. This survey\nprovides an up-to-date review of the literature on long-context LLMs, which we\nwish to serve as a valuable resource for both researchers and engineers. An\nassociated GitHub repository collecting the latest papers and repos is\navailable at:\n\\href{https://github.com/LCLM-Horizon/A-Comprehensive-Survey-For-Long-Context-Language-Modeling}{\\color[RGB]{175,36,67}{LCLM-Horizon}}."}
{"id": "2503.17425", "pdf": "https://arxiv.org/pdf/2503.17425", "abs": "https://arxiv.org/abs/2503.17425", "authors": ["Veysel Kocaman", "Yigit Gul", "M. Aytug Kaya", "Hasham Ul Haq", "Mehmet Butgul", "Cabir Celik", "David Talby"], "title": "Beyond Negation Detection: Comprehensive Assertion Detection Models for Clinical NLP", "categories": ["cs.CL", "cs.IR", "cs.LG", "H.3"], "comment": "accepted at Text2Story Workshop at ECIR 2025", "summary": "Assertion status detection is a critical yet often overlooked component of\nclinical NLP, essential for accurately attributing extracted medical facts.\nPast studies have narrowly focused on negation detection, leading to\nunderperforming commercial solutions such as AWS Medical Comprehend, Azure AI\nText Analytics, and GPT-4o due to their limited domain adaptation. To address\nthis gap, we developed state-of-the-art assertion detection models, including\nfine-tuned LLMs, transformer-based classifiers, few-shot classifiers, and deep\nlearning (DL) approaches. We evaluated these models against cloud-based\ncommercial API solutions, the legacy rule-based NegEx approach, and GPT-4o. Our\nfine-tuned LLM achieves the highest overall accuracy (0.962), outperforming\nGPT-4o (0.901) and commercial APIs by a notable margin, particularly excelling\nin Present (+4.2%), Absent (+8.4%), and Hypothetical (+23.4%) assertions. Our\nDL-based models surpass commercial solutions in Conditional (+5.3%) and\nAssociated-with-Someone-Else (+10.1%) categories, while the few-shot classifier\noffers a lightweight yet highly competitive alternative (0.929), making it\nideal for resource-constrained environments. Integrated within Spark NLP, our\nmodels consistently outperform black-box commercial solutions while enabling\nscalable inference and seamless integration with medical NER, Relation\nExtraction, and Terminology Resolution. These results reinforce the importance\nof domain-adapted, transparent, and customizable clinical NLP solutions over\ngeneral-purpose LLMs and proprietary APIs."}
{"id": "2503.17456", "pdf": "https://arxiv.org/pdf/2503.17456", "abs": "https://arxiv.org/abs/2503.17456", "authors": ["Soumen Kumar Mondal", "Sayambhu Sen", "Abhishek Singhania", "Preethi Jyothi"], "title": "Language-specific Neurons Do Not Facilitate Cross-Lingual Transfer", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted (oral) at NAACL 2025 (InsightsNLP)", "summary": "Multilingual large language models (LLMs) aim towards robust natural language\nunderstanding across diverse languages, yet their performance significantly\ndegrades on low-resource languages. This work explores whether existing\ntechniques to identify language-specific neurons can be leveraged to enhance\ncross-lingual task performance of lowresource languages. We conduct detailed\nexperiments covering existing language-specific neuron identification\ntechniques (such as Language Activation Probability Entropy and activation\nprobability-based thresholding) and neuron-specific LoRA fine-tuning with\nmodels like Llama 3.1 and Mistral Nemo. We find that such neuron-specific\ninterventions are insufficient to yield cross-lingual improvements on\ndownstream tasks (XNLI, XQuAD) in lowresource languages. This study highlights\nthe challenges in achieving cross-lingual generalization and provides critical\ninsights for multilingual LLMs."}
{"id": "2503.17391", "pdf": "https://arxiv.org/pdf/2503.17391", "abs": "https://arxiv.org/abs/2503.17391", "authors": ["Atharva Deo", "Nicholas Matsumoto", "Sun Kim", "Peter Wager", "Randy G. Tsai", "Aaron Denmark", "Cherine Yang", "Xi Li", "Jay Moran", "Miguel Hernandez", "Andrew J. Hung"], "title": "AI-driven Automation of End-to-end Assessment of Suturing Expertise", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "We present an AI based approach to automate the End-to-end Assessment of\nSuturing Expertise (EASE), a suturing skills assessment tool that\ncomprehensively defines criteria around relevant sub-skills.1 While EASE\nprovides granular skills assessment related to suturing to provide trainees\nwith an objective evaluation of their aptitude along with actionable insights,\nthe scoring process is currently performed by human evaluators, which is time\nand resource consuming. The AI based approach solves this by enabling real-time\nscore prediction with minimal resources during model inference. This enables\nthe possibility of real-time feedback to the surgeons/trainees, potentially\naccelerating the learning process for the suturing task and mitigating critical\nerrors during the surgery, improving patient outcomes. In this study, we focus\non the following 7 EASE domains that come under 3 suturing phases: 1) Needle\nHandling: Number of Repositions, Needle Hold Depth, Needle Hold Ratio, and\nNeedle Hold Angle; 2) Needle Driving: Driving Smoothness, and Wrist Rotation;\n3) Needle Withdrawal: Wrist Rotation."}
{"id": "2503.17460", "pdf": "https://arxiv.org/pdf/2503.17460", "abs": "https://arxiv.org/abs/2503.17460", "authors": ["Reem Gody", "Mahmoud Goudy", "Ahmed Y. Tawfik"], "title": "ConvoGen: Enhancing Conversational AI with Synthetic Data: A Multi-Agent Approach", "categories": ["cs.CL"], "comment": null, "summary": "In this paper, we present ConvoGen: an innovative framework for generating\nsynthetic conversational data using multi-agent systems. Our method leverages\nfew-shot learning and introduces iterative sampling from a dynamically updated\nfew-shot hub to create diverse and realistic conversational scenarios. The\ngenerated data has numerous applications, including training and evaluating\nconversational AI models, and augmenting existing datasets for tasks like\nconversational intent classification or conversation summarization. Our\nexperiments demonstrate the effectiveness of this method in producing\nhigh-quality diverse synthetic conversational data, highlighting its potential\nto enhance the development and evaluation of conversational AI systems."}
{"id": "2503.17406", "pdf": "https://arxiv.org/pdf/2503.17406", "abs": "https://arxiv.org/abs/2503.17406", "authors": ["Haochen Zhang", "Nader Zantout", "Pujith Kachana", "Ji Zhang", "Wenshan Wang"], "title": "IRef-VLA: A Benchmark for Interactive Referential Grounding with Imperfect Language in 3D Scenes", "categories": ["cs.CV", "cs.RO"], "comment": "Accepted to ICRA 2025. Code available at\n  https://github.com/HaochenZ11/IRef-VLA. arXiv admin note: text overlap with\n  arXiv:2411.03540", "summary": "With the recent rise of large language models, vision-language models, and\nother general foundation models, there is growing potential for multimodal,\nmulti-task robotics that can operate in diverse environments given natural\nlanguage input. One such application is indoor navigation using natural\nlanguage instructions. However, despite recent progress, this problem remains\nchallenging due to the 3D spatial reasoning and semantic understanding\nrequired. Additionally, the language used may be imperfect or misaligned with\nthe scene, further complicating the task. To address this challenge, we curate\na benchmark dataset, IRef-VLA, for Interactive Referential Vision and\nLanguage-guided Action in 3D Scenes with imperfect references. IRef-VLA is the\nlargest real-world dataset for the referential grounding task, consisting of\nover 11.5K scanned 3D rooms from existing datasets, 7.6M heuristically\ngenerated semantic relations, and 4.7M referential statements. Our dataset also\ncontains semantic object and room annotations, scene graphs, navigable free\nspace annotations, and is augmented with statements where the language has\nimperfections or ambiguities. We verify the generalizability of our dataset by\nevaluating with state-of-the-art models to obtain a performance baseline and\nalso develop a graph-search baseline to demonstrate the performance bound and\ngeneration of alternatives using scene-graph knowledge. With this benchmark, we\naim to provide a resource for 3D scene understanding that aids the development\nof robust, interactive navigation systems. The dataset and all source code is\npublicly released at https://github.com/HaochenZ11/IRef-VLA."}
{"id": "2503.17485", "pdf": "https://arxiv.org/pdf/2503.17485", "abs": "https://arxiv.org/abs/2503.17485", "authors": ["Lama Ayash", "Hassan Alhuzali", "Ashwag Alasmari", "Sultan Aloufi"], "title": "SaudiCulture: A Benchmark for Evaluating Large Language Models Cultural Competence within Saudi Arabia", "categories": ["cs.CL", "cs.AI"], "comment": "34 pages, under-review", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nnatural language processing; however, they often struggle to accurately capture\nand reflect cultural nuances. This research addresses this challenge by\nfocusing on Saudi Arabia, a country characterized by diverse dialects and rich\ncultural traditions. We introduce SaudiCulture, a novel benchmark designed to\nevaluate the cultural competence of LLMs within the distinct geographical and\ncultural contexts of Saudi Arabia. SaudiCulture is a comprehensive dataset of\nquestions covering five major geographical regions, such as West, East, South,\nNorth, and Center, along with general questions applicable across all regions.\nThe dataset encompasses a broad spectrum of cultural domains, including food,\nclothing, entertainment, celebrations, and crafts. To ensure a rigorous\nevaluation, SaudiCulture includes questions of varying complexity, such as\nopen-ended, single-choice, and multiple-choice formats, with some requiring\nmultiple correct answers. Additionally, the dataset distinguishes between\ncommon cultural knowledge and specialized regional aspects. We conduct\nextensive evaluations on five LLMs, such as GPT-4, Llama 3.3, FANAR, Jais, and\nAceGPT, analyzing their performance across different question types and\ncultural contexts. Our findings reveal that all models experience significant\nperformance declines when faced with highly specialized or region-specific\nquestions, particularly those requiring multiple correct responses.\nAdditionally, certain cultural categories are more easily identifiable than\nothers, further highlighting inconsistencies in LLMs cultural understanding.\nThese results emphasize the importance of incorporating region-specific\nknowledge into LLMs training to enhance their cultural competence."}
{"id": "2503.17415", "pdf": "https://arxiv.org/pdf/2503.17415", "abs": "https://arxiv.org/abs/2503.17415", "authors": ["Yicheng Duan", "Xi Huang", "Duo Chen"], "title": "Enhancing Subsequent Video Retrieval via Vision-Language Models (VLMs)", "categories": ["cs.CV", "cs.AI", "cs.IR"], "comment": null, "summary": "The rapid growth of video content demands efficient and precise retrieval\nsystems. While vision-language models (VLMs) excel in representation learning,\nthey often struggle with adaptive, time-sensitive video retrieval. This paper\nintroduces a novel framework that combines vector similarity search with\ngraph-based data structures. By leveraging VLM embeddings for initial retrieval\nand modeling contextual relationships among video segments, our approach\nenables adaptive query refinement and improves retrieval accuracy. Experiments\ndemonstrate its precision, scalability, and robustness, offering an effective\nsolution for interactive video retrieval in dynamic environments."}
{"id": "2503.17489", "pdf": "https://arxiv.org/pdf/2503.17489", "abs": "https://arxiv.org/abs/2503.17489", "authors": ["Shu Pu", "Yaochen Wang", "Dongping Chen", "Yuhang Chen", "Guohao Wang", "Qi Qin", "Zhongyi Zhang", "Zhiyuan Zhang", "Zetong Zhou", "Shuang Gong", "Yi Gui", "Yao Wan", "Philip S. Yu"], "title": "Judge Anything: MLLM as a Judge Across Any Modality", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Evaluating generative foundation models on open-ended multimodal\nunderstanding (MMU) and generation (MMG) tasks across diverse modalities (e.g.,\nimages, audio, video) poses significant challenges due to the complexity of\ncross-modal interactions. To this end, the idea of utilizing Multimodal LLMs\n(MLLMs) as automated judges has emerged, with encouraging results in assessing\nvision-language understanding tasks. Moving further, this paper extends\nMLLM-as-a-Judge across modalities to a unified manner by introducing two\nbenchmarks, TaskAnything and JudgeAnything, to respectively evaluate the\noverall performance and judging capabilities of MLLMs across any-to-any\nmodality tasks. Specifically, TaskAnything evaluates the MMU and MMG\ncapabilities across 15 any-to-any modality categories, employing 1,500 queries\ncurated from well-established benchmarks. Furthermore, JudgeAnything evaluates\nthe judging capabilities of 5 advanced (e.g., GPT-4o and Gemini-2.0-Flash) from\nthe perspectives of Pair Comparison and Score Evaluation, providing a\nstandardized testbed that incorporates human judgments and detailed rubrics.\nOur extensive experiments reveal that while these MLLMs show promise in\nassessing MMU (i.e., achieving an average of 66.55% in Pair Comparison setting\nand 42.79% in Score Evaluation setting), they encounter significant challenges\nwith MMG tasks (i.e., averaging only 53.37% in Pair Comparison setting and\n30.05% in Score Evaluation setting), exposing cross-modality biases and\nhallucination issues. To address this, we present OmniArena, an automated\nplatform for evaluating omni-models and multimodal reward models. Our work\nhighlights the need for fairer evaluation protocols and stronger alignment with\nhuman preferences. The source code and dataset are publicly available at:\nhttps://urrealhero.github.io/judgeanythingweb/."}
{"id": "2503.17453", "pdf": "https://arxiv.org/pdf/2503.17453", "abs": "https://arxiv.org/abs/2503.17453", "authors": ["Ran Liu", "Fengyu Zhang", "Cong Yu", "Longjiang Yang", "Zhuofan Wen", "Siyuan Zhang", "Hailiang Yao", "Shun Chen", "Zheng Lian", "Bin Liu"], "title": "Feature-Based Dual Visual Feature Extraction Model for Compound Multimodal Emotion Recognition", "categories": ["cs.CV"], "comment": null, "summary": "This article presents our results for the eighth Affective Behavior Analysis\nin-the-wild (ABAW) competition.Multimodal emotion recognition (ER) has\nimportant applications in affective computing and human-computer interaction.\nHowever, in the real world, compound emotion recognition faces greater issues\nof uncertainty and modal conflicts. For the Compound Expression (CE)\nRecognition Challenge,this paper proposes a multimodal emotion recognition\nmethod that fuses the features of Vision Transformer (ViT) and Residual Network\n(ResNet). We conducted experiments on the C-EXPR-DB and MELD datasets. The\nresults show that in scenarios with complex visual and audio cues (such as\nC-EXPR-DB), the model that fuses the features of ViT and ResNet exhibits\nsuperior performance.Our code are avalible on\nhttps://github.com/MyGitHub-ax/8th_ABAW"}
{"id": "2503.17509", "pdf": "https://arxiv.org/pdf/2503.17509", "abs": "https://arxiv.org/abs/2503.17509", "authors": ["Joseph Gatto", "Parker Seegmiller", "Timothy Burdick", "Inas S. Khayal", "Sarah DeLozier", "Sarah M. Preum"], "title": "Follow-up Question Generation For Enhanced Patient-Provider Conversations", "categories": ["cs.CL", "cs.AI"], "comment": "17 Pages, 7 Figures, 6 Tables", "summary": "Follow-up question generation is an essential feature of dialogue systems as\nit can reduce conversational ambiguity and enhance modeling complex\ninteractions. Conversational contexts often pose core NLP challenges such as\n(i) extracting relevant information buried in fragmented data sources, and (ii)\nmodeling parallel thought processes. These two challenges occur frequently in\nmedical dialogue as a doctor asks questions based not only on patient\nutterances but also their prior EHR data and current diagnostic hypotheses.\nAsking medical questions in asynchronous conversations compounds these issues\nas doctors can only rely on static EHR information to motivate follow-up\nquestions.\n  To address these challenges, we introduce FollowupQ, a novel framework for\nenhancing asynchronous medical conversation. FollowupQ is a multi-agent\nframework that processes patient messages and EHR data to generate personalized\nfollow-up questions, clarifying patient-reported medical conditions. FollowupQ\nreduces requisite provider follow-up communications by 34%. It also improves\nperformance by 17% and 5% on real and synthetic data, respectively. We also\nrelease the first public dataset of asynchronous medical messages with linked\nEHR data alongside 2,300 follow-up questions written by clinical experts for\nthe wider NLP research community."}
{"id": "2503.17467", "pdf": "https://arxiv.org/pdf/2503.17467", "abs": "https://arxiv.org/abs/2503.17467", "authors": ["Yuxuan Wei", "Zehan Wang", "Tian Guo", "Hao Liu", "Liquan Shen", "Hui Yuan"], "title": "High Efficiency Wiener Filter-based Point Cloud Quality Enhancement for MPEG G-PCC", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Point clouds, which directly record the geometry and attributes of scenes or\nobjects by a large number of points, are widely used in various applications\nsuch as virtual reality and immersive communication. However, due to the huge\ndata volume and unstructured geometry, efficient compression of point clouds is\nvery crucial. The Moving Picture Expert Group is establishing a geometry-based\npoint cloud compression (G-PCC) standard for both static and dynamic point\nclouds in recent years. Although lossy compression of G-PCC can achieve a very\nhigh compression ratio, the reconstruction quality is relatively low,\nespecially at low bitrates. To mitigate this problem, we propose a high\nefficiency Wiener filter that can be integrated into the encoder and decoder\npipeline of G-PCC to improve the reconstruction quality as well as the\nrate-distortion performance for dynamic point clouds. Specifically, we first\npropose a basic Wiener filter, and then improve it by introducing coefficients\ninheritance and variance-based point classification for the Luma component.\nBesides, to reduce the complexity of the nearest neighbor search during the\napplication of the Wiener filter, we also propose a Morton code-based fast\nnearest neighbor search algorithm for efficient calculation of filter\ncoefficients. Experimental results demonstrate that the proposed method can\nachieve average Bj{\\o}ntegaard delta rates of -6.1%, -7.3%, and -8.0% for Luma,\nChroma Cb, and Chroma Cr components, respectively, under the condition of\nlossless-geometry-lossy-attributes configuration compared to the latest G-PCC\nencoding platform (i.e., geometry-based solid content test model version 7.0\nrelease candidate 2) by consuming affordable computational complexity."}
{"id": "2503.17514", "pdf": "https://arxiv.org/pdf/2503.17514", "abs": "https://arxiv.org/abs/2503.17514", "authors": ["Ken Ziyu Liu", "Christopher A. Choquette-Choo", "Matthew Jagielski", "Peter Kairouz", "Sanmi Koyejo", "Percy Liang", "Nicolas Papernot"], "title": "Language Models May Verbatim Complete TextThey Were Not Explicitly Trained On", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "comment": "Main text: 9 pages, 7 figures, 1 table. Appendix: 29 pages, 20\n  tables, 15 figures", "summary": "An important question today is whether a given text was used to train a large\nlanguage model (LLM). A \\emph{completion} test is often employed: check if the\nLLM completes a sufficiently complex text. This, however, requires a\nground-truth definition of membership; most commonly, it is defined as a member\nbased on the $n$-gram overlap between the target text and any text in the\ndataset. In this work, we demonstrate that this $n$-gram based membership\ndefinition can be effectively gamed. We study scenarios where sequences are\n\\emph{non-members} for a given $n$ and we find that completion tests still\nsucceed. We find many natural cases of this phenomenon by retraining LLMs from\nscratch after removing all training samples that were completed; these cases\ninclude exact duplicates, near-duplicates, and even short overlaps. They\nshowcase that it is difficult to find a single viable choice of $n$ for\nmembership definitions. Using these insights, we design adversarial datasets\nthat can cause a given target sequence to be completed without containing it,\nfor any reasonable choice of $n$. Our findings highlight the inadequacy of\n$n$-gram membership, suggesting membership definitions fail to account for\nauxiliary information available to the training algorithm."}
{"id": "2503.17475", "pdf": "https://arxiv.org/pdf/2503.17475", "abs": "https://arxiv.org/abs/2503.17475", "authors": ["Gary Y. Li", "Li Chen", "Bryson Hicks", "Nikolai Schnittke", "David O. Kessler", "Jeffrey Shupp", "Maria Parker", "Cristiana Baloescu", "Christopher Moore", "Cynthia Gregory", "Kenton Gregory", "Balasundar Raju", "Jochen Kruecker", "Alvin Chen"], "title": "Spatiotemporal Learning with Context-aware Video Tubelets for Ultrasound Video Analysis", "categories": ["cs.CV", "cs.AI"], "comment": "ISBI Oral 2025", "summary": "Computer-aided pathology detection algorithms for video-based imaging\nmodalities must accurately interpret complex spatiotemporal information by\nintegrating findings across multiple frames. Current state-of-the-art methods\noperate by classifying on video sub-volumes (tubelets), but they often lose\nglobal spatial context by focusing only on local regions within detection ROIs.\nHere we propose a lightweight framework for tubelet-based object detection and\nvideo classification that preserves both global spatial context and fine\nspatiotemporal features. To address the loss of global context, we embed\ntubelet location, size, and confidence as inputs to the classifier.\nAdditionally, we use ROI-aligned feature maps from a pre-trained detection\nmodel, leveraging learned feature representations to increase the receptive\nfield and reduce computational complexity. Our method is efficient, with the\nspatiotemporal tubelet classifier comprising only 0.4M parameters. We apply our\napproach to detect and classify lung consolidation and pleural effusion in\nultrasound videos. Five-fold cross-validation on 14,804 videos from 828\npatients shows our method outperforms previous tubelet-based approaches and is\nsuited for real-time workflows."}
{"id": "2503.17523", "pdf": "https://arxiv.org/pdf/2503.17523", "abs": "https://arxiv.org/abs/2503.17523", "authors": ["Linlu Qiu", "Fei Sha", "Kelsey Allen", "Yoon Kim", "Tal Linzen", "Sjoerd van Steenkiste"], "title": "Bayesian Teaching Enables Probabilistic Reasoning in Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Artificial intelligence systems based on large language models (LLMs) are\nincreasingly used as agents that interact with users and with the world. To do\nso successfully, LLMs need to construct internal representations of the world\nand form probabilistic beliefs about those representations. To provide a user\nwith personalized recommendations, for example, the LLM needs to gradually\ninfer the user's preferences, over the course of multiple interactions. To\nevaluate whether contemporary LLMs are able to do so, we use the Bayesian\ninference framework from probability theory, which lays out the optimal way to\nupdate an agent's beliefs as it receives new information. We first show that\nthe LLMs do not update their beliefs as expected from the Bayesian framework,\nand that consequently their predictions do not improve as expected as more\ninformation becomes available, even less so than we find is the case for\nhumans. To address this issue, we teach the LLMs to reason in a Bayesian manner\nby training them to mimic the predictions of an optimal Bayesian model. We find\nthat this approach not only significantly improves the LLM's performance on the\nparticular recommendation task it is trained on, but also enables\ngeneralization to other tasks. This suggests that this method endows the LLM\nwith broader Bayesian reasoning skills. More generally, our results indicate\nthat LLMs can learn about reasoning strategies effectively and generalize those\nskills to new domains, which in part explains LLMs' empirical success."}
{"id": "2503.17486", "pdf": "https://arxiv.org/pdf/2503.17486", "abs": "https://arxiv.org/abs/2503.17486", "authors": ["Zhengqing Gao", "Dongting Hu", "Jia-Wang Bian", "Huan Fu", "Yan Li", "Tongliang Liu", "Mingming Gong", "Kun Zhang"], "title": "ProtoGS: Efficient and High-Quality Rendering with 3D Gaussian Prototypes", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "3D Gaussian Splatting (3DGS) has made significant strides in novel view\nsynthesis but is limited by the substantial number of Gaussian primitives\nrequired, posing challenges for deployment on lightweight devices. Recent\nmethods address this issue by compressing the storage size of densified\nGaussians, yet fail to preserve rendering quality and efficiency. To overcome\nthese limitations, we propose ProtoGS to learn Gaussian prototypes to represent\nGaussian primitives, significantly reducing the total Gaussian amount without\nsacrificing visual quality. Our method directly uses Gaussian prototypes to\nenable efficient rendering and leverage the resulting reconstruction loss to\nguide prototype learning. To further optimize memory efficiency during\ntraining, we incorporate structure-from-motion (SfM) points as anchor points to\ngroup Gaussian primitives. Gaussian prototypes are derived within each group by\nclustering of K-means, and both the anchor points and the prototypes are\noptimized jointly. Our experiments on real-world and synthetic datasets prove\nthat we outperform existing methods, achieving a substantial reduction in the\nnumber of Gaussians, and enabling high rendering speed while maintaining or\neven enhancing rendering fidelity."}
{"id": "2503.17579", "pdf": "https://arxiv.org/pdf/2503.17579", "abs": "https://arxiv.org/abs/2503.17579", "authors": ["Suet-Ying Lam", "Qingcheng Zeng", "Jingyi Wu", "Rob Voigt"], "title": "Leveraging Human Production-Interpretation Asymmetries to Test LLM Cognitive Plausibility", "categories": ["cs.CL"], "comment": null, "summary": "Whether large language models (LLMs) process language similarly to humans has\nbeen the subject of much theoretical and practical debate. We examine this\nquestion through the lens of the production-interpretation distinction found in\nhuman sentence processing and evaluate the extent to which instruction-tuned\nLLMs replicate this distinction. Using an empirically documented asymmetry\nbetween production and interpretation in humans for implicit causality verbs as\na testbed, we find that some LLMs do quantitatively and qualitatively reflect\nhuman-like asymmetries between production and interpretation. We demonstrate\nthat whether this behavior holds depends upon both model size - with larger\nmodels more likely to reflect human-like patterns and the choice of\nmeta-linguistic prompts used to elicit the behavior."}
{"id": "2503.17488", "pdf": "https://arxiv.org/pdf/2503.17488", "abs": "https://arxiv.org/abs/2503.17488", "authors": ["Tianwen Zhou", "Jing Wang", "Songtao Wu", "Kuanhong Xu"], "title": "ProDehaze: Prompting Diffusion Models Toward Faithful Image Dehazing", "categories": ["cs.CV"], "comment": "Accepted to ICME 2025", "summary": "Recent approaches using large-scale pretrained diffusion models for image\ndehazing improve perceptual quality but often suffer from hallucination issues,\nproducing unfaithful dehazed image to the original one. To mitigate this, we\npropose ProDehaze, a framework that employs internal image priors to direct\nexternal priors encoded in pretrained models. We introduce two types of\n\\textit{selective} internal priors that prompt the model to concentrate on\ncritical image areas: a Structure-Prompted Restorer in the latent space that\nemphasizes structure-rich regions, and a Haze-Aware Self-Correcting Refiner in\nthe decoding process to align distributions between clearer input regions and\nthe output. Extensive experiments on real-world datasets demonstrate that\nProDehaze achieves high-fidelity results in image dehazing, particularly in\nreducing color shifts. Our code is at https://github.com/TianwenZhou/ProDehaze."}
{"id": "2503.17599", "pdf": "https://arxiv.org/pdf/2503.17599", "abs": "https://arxiv.org/abs/2503.17599", "authors": ["Zheqing Li", "Yiying Yang", "Jiping Lang", "Wenhao Jiang", "Yuhang Zhao", "Shuang Li", "Dingqian Wang", "Zhu Lin", "Xuanna Li", "Yuze Tang", "Jiexian Qiu", "Xiaolin Lu", "Hongji Yu", "Shuang Chen", "Yuhua Bi", "Xiaofei Zeng", "Yixian Chen", "Junrong Chen", "Lin Yao"], "title": "GPBench: A Comprehensive and Fine-Grained Benchmark for Evaluating Large Language Models as General Practitioners", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "General practitioners (GPs) serve as the cornerstone of primary healthcare\nsystems by providing continuous and comprehensive medical services. However,\ndue to community-oriented nature of their practice, uneven training and\nresource gaps, the clinical proficiency among GPs can vary significantly across\nregions and healthcare settings. Currently, Large Language Models (LLMs) have\ndemonstrated great potential in clinical and medical applications, making them\na promising tool for supporting general practice. However, most existing\nbenchmarks and evaluation frameworks focus on exam-style assessments-typically\nmultiple-choice question-lack comprehensive assessment sets that accurately\nmirror the real-world scenarios encountered by GPs. To evaluate how effectively\nLLMs can make decisions in the daily work of GPs, we designed GPBench, which\nconsists of both test questions from clinical practice and a novel evaluation\nframework. The test set includes multiple-choice questions that assess\nfundamental knowledge of general practice, as well as realistic, scenario-based\nproblems. All questions are meticulously annotated by experts, incorporating\nrich fine-grained information related to clinical management. The proposed LLM\nevaluation framework is based on the competency model for general practice,\nproviding a comprehensive methodology for assessing LLM performance in\nreal-world settings. As the first large-model evaluation set targeting GP\ndecision-making scenarios, GPBench allows us to evaluate current mainstream\nLLMs. Expert assessment and evaluation reveal that in areas such as disease\nstaging, complication recognition, treatment detail, and medication usage,\nthese models exhibit at least ten major shortcomings. Overall, existing LLMs\nare not yet suitable for independent use in real-world GP working scenarios\nwithout human oversight."}
{"id": "2503.17493", "pdf": "https://arxiv.org/pdf/2503.17493", "abs": "https://arxiv.org/abs/2503.17493", "authors": ["Aidos Konyspay", "Pakizar Shamoi", "Malika Ziyada", "Zhusup Smambayev"], "title": "Meme Similarity and Emotion Detection using Multimodal Analysis", "categories": ["cs.CV"], "comment": "Have been submitted to IEEE for consideration", "summary": "Internet memes are a central element of online culture, blending images and\ntext. While substantial research has focused on either the visual or textual\ncomponents of memes, little attention has been given to their interplay. This\ngap raises a key question: What methodology can effectively compare memes and\nthe emotions they elicit? Our study employs a multimodal methodological\napproach, analyzing both the visual and textual elements of memes.\nSpecifically, we perform a multimodal CLIP (Contrastive Language-Image\nPre-training) model for grouping similar memes based on text and visual content\nembeddings, enabling robust similarity assessments across modalities. Using the\nReddit Meme Dataset and Memotion Dataset, we extract low-level visual features\nand high-level semantic features to identify similar meme pairs. To validate\nthese automated similarity assessments, we conducted a user study with 50\nparticipants, asking them to provide yes/no responses regarding meme similarity\nand their emotional reactions. The comparison of experimental results with\nhuman judgments showed a 67.23\\% agreement, suggesting that the computational\napproach aligns well with human perception. Additionally, we implemented a\ntext-based classifier using the DistilBERT model to categorize memes into one\nof six basic emotions. The results indicate that anger and joy are the dominant\nemotions in memes, with motivational memes eliciting stronger emotional\nresponses. This research contributes to the study of multimodal memes,\nenhancing both language-based and visual approaches to analyzing and improving\nonline visual communication and user experiences. Furthermore, it provides\ninsights for better content moderation strategies in online platforms."}
{"id": "2503.17662", "pdf": "https://arxiv.org/pdf/2503.17662", "abs": "https://arxiv.org/abs/2503.17662", "authors": ["Ke Ji", "Yixin Lian", "Linxu Li", "Jingsheng Gao", "Weiyuan Li", "Bin Dai"], "title": "Enhancing Persona Consistency for LLMs' Role-Playing using Persona-Aware Contrastive Learning", "categories": ["cs.CL"], "comment": "18 pages, 4 figures", "summary": "In recent years, large language models (LLMs) have achieved breakthrough\nprogress in many dialogue generation tasks. However, their lack of emotion and\nfine-grained role awareness limits the model's ability to provide personalized\nand diverse interactions further. Current methods face high costs in collecting\nhigh-quality annotated data for scenarios such as role-playing, and traditional\nhuman alignment methods are difficult to deploy due to the inherent diversity\nof model behavior in role-playing scenarios. Inspired by the alignment of\nmodels for safety behaviors through RLHF (Reinforcement Learning from Human\nFeedback), in this paper, we revisit model role-playing behavior from the\nperspective of persona alignment and propose a novel annotation-free framework\nnamed \\textbf{\\underline{P}}ersona-Aware \\textbf{\\underline{C}}ontrastive\n\\textbf{\\underline{L}}earning (PCL) to align LLMs' behavior during\nrole-playing, enhancing the model's role consistency. Specifically, we first\ndesign a role chain method to encourage the model to self-question based on the\nrole characteristics and dialogue context to adjust personality consistency.\nThen, we further enhance the model's role-playing strategy through iterative\ncontrastive learning between the use of role characteristics and not.\nExperiments on both black-box and white-box LLMs show that LLMs equipped with\nPCL significantly outperform vanilla LLMs under automatic evaluation methods\n(CharEval \\& GPT-4) and human expert evaluation."}
{"id": "2503.17497", "pdf": "https://arxiv.org/pdf/2503.17497", "abs": "https://arxiv.org/abs/2503.17497", "authors": ["Daniel Kuhse", "Harun Teper", "Sebastian Buschjäger", "Chien-Yao Wang", "Jian-Jia Chen"], "title": "You Only Look Once at Anytime (AnytimeYOLO): Analysis and Optimization of Early-Exits for Object-Detection", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "We introduce AnytimeYOLO, a family of variants of the YOLO architecture that\nenables anytime object detection. Our AnytimeYOLO networks allow for\ninterruptible inference, i.e., they provide a prediction at any point in time,\na property desirable for safety-critical real-time applications.\n  We present structured explorations to modify the YOLO architecture, enabling\nearly termination to obtain intermediate results. We focus on providing\nfine-grained control through high granularity of available termination points.\nFirst, we formalize Anytime Models as a special class of prediction models that\noffer anytime predictions. Then, we discuss a novel transposed variant of the\nYOLO architecture, that changes the architecture to enable better early\npredictions and greater freedom for the order of processing stages. Finally, we\npropose two optimization algorithms that, given an anytime model, can be used\nto determine the optimal exit execution order and the optimal subset of\nearly-exits to select for deployment in low-resource environments. We evaluate\nthe anytime performance and trade-offs of design choices, proposing a new\nanytime quality metric for this purpose. In particular, we also discuss key\nchallenges for anytime inference that currently make its deployment costly."}
{"id": "2503.17684", "pdf": "https://arxiv.org/pdf/2503.17684", "abs": "https://arxiv.org/abs/2503.17684", "authors": ["Dhruv Sahnan", "David Corney", "Irene Larraz", "Giovanni Zagni", "Ruben Miguez", "Zhuohan Xie", "Iryna Gurevych", "Elizabeth Churchill", "Tanmoy Chakraborty", "Preslav Nakov"], "title": "Can LLMs Automate Fact-Checking Article Writing?", "categories": ["cs.CL", "cs.AI"], "comment": "10 pages, 4 figures, 6 tables", "summary": "Automatic fact-checking aims to support professional fact-checkers by\noffering tools that can help speed up manual fact-checking. Yet, existing\nframeworks fail to address the key step of producing output suitable for\nbroader dissemination to the general public: while human fact-checkers\ncommunicate their findings through fact-checking articles, automated systems\ntypically produce little or no justification for their assessments. Here, we\naim to bridge this gap. We argue for the need to extend the typical automatic\nfact-checking pipeline with automatic generation of full fact-checking\narticles. We first identify key desiderata for such articles through a series\nof interviews with experts from leading fact-checking organizations. We then\ndevelop QRAFT, an LLM-based agentic framework that mimics the writing workflow\nof human fact-checkers. Finally, we assess the practical usefulness of QRAFT\nthrough human evaluations with professional fact-checkers. Our evaluation shows\nthat while QRAFT outperforms several previously proposed text-generation\napproaches, it lags considerably behind expert-written articles. We hope that\nour work will enable further research in this new and important direction."}
{"id": "2503.17499", "pdf": "https://arxiv.org/pdf/2503.17499", "abs": "https://arxiv.org/abs/2503.17499", "authors": ["Joey Mulé", "Dhandeep Challagundla", "Rachit Saini", "Riadul Islam"], "title": "Event-Based Crossing Dataset (EBCD)", "categories": ["cs.CV"], "comment": null, "summary": "Event-based vision revolutionizes traditional image sensing by capturing\nasynchronous intensity variations rather than static frames, enabling ultrafast\ntemporal resolution, sparse data encoding, and enhanced motion perception.\nWhile this paradigm offers significant advantages, conventional event-based\ndatasets impose a fixed thresholding constraint to determine pixel activations,\nseverely limiting adaptability to real-world environmental fluctuations. Lower\nthresholds retain finer details but introduce pervasive noise, whereas higher\nthresholds suppress extraneous activations at the expense of crucial object\ninformation. To mitigate these constraints, we introduce the Event-Based\nCrossing Dataset (EBCD), a comprehensive dataset tailored for pedestrian and\nvehicle detection in dynamic outdoor environments, incorporating a\nmulti-thresholding framework to refine event representations. By capturing\nevent-based images at ten distinct threshold levels (4, 8, 12, 16, 20, 30, 40,\n50, 60, and 75), this dataset facilitates an extensive assessment of object\ndetection performance under varying conditions of sparsity and noise\nsuppression. We benchmark state-of-the-art detection architectures-including\nYOLOv4, YOLOv7, EfficientDet-b0, MobileNet-v1, and Histogram of Oriented\nGradients (HOG)-to experiment upon the nuanced impact of threshold selection on\ndetection performance. By offering a systematic approach to threshold\nvariation, we foresee that EBCD fosters a more adaptive evaluation of\nevent-based object detection, aligning diverse neuromorphic vision with\nreal-world scene dynamics. We present the dataset as publicly available to\npropel further advancements in low-latency, high-fidelity neuromorphic imaging:\nhttps://ieee-dataport.org/documents/event-based-crossing-dataset-ebcd"}
{"id": "2503.17739", "pdf": "https://arxiv.org/pdf/2503.17739", "abs": "https://arxiv.org/abs/2503.17739", "authors": ["Chatrine Qwaider", "Bashar Alhafni", "Kirill Chirkunov", "Nizar Habash", "Ted Briscoe"], "title": "Enhancing Arabic Automated Essay Scoring with Synthetic Data and Error Injection", "categories": ["cs.CL"], "comment": null, "summary": "Automated Essay Scoring (AES) plays a crucial role in assessing language\nlearners' writing quality, reducing grading workload, and providing real-time\nfeedback. Arabic AES systems are particularly challenged by the lack of\nannotated essay datasets. This paper presents a novel framework leveraging\nLarge Language Models (LLMs) and Transformers to generate synthetic Arabic\nessay datasets for AES. We prompt an LLM to generate essays across CEFR\nproficiency levels and introduce controlled error injection using a fine-tuned\nStandard Arabic BERT model for error type prediction. Our approach produces\nrealistic human-like essays, contributing a dataset of 3,040 annotated essays.\nAdditionally, we develop a BERT-based auto-marking system for accurate and\nscalable Arabic essay evaluation. Experimental results demonstrate the\neffectiveness of our framework in improving Arabic AES performance."}
{"id": "2503.17526", "pdf": "https://arxiv.org/pdf/2503.17526", "abs": "https://arxiv.org/abs/2503.17526", "authors": ["Sébastien Quetin", "Tapotosh Ghosh", "Farhad Maleki"], "title": "Should we pre-train a decoder in contrastive learning for dense prediction tasks?", "categories": ["cs.CV"], "comment": null, "summary": "Contrastive learning in self-supervised settings primarily focuses on\npre-training encoders, while decoders are typically introduced and trained\nseparately for downstream dense prediction tasks. This conventional approach,\nhowever, overlooks the potential benefits of jointly pre-training both the\nencoder and decoder. In this paper, we propose DeCon: a framework-agnostic\nadaptation to convert an encoder-only self-supervised learning (SSL)\ncontrastive approach to an efficient encoder-decoder framework that can be\npre-trained in a contrastive manner. We first update the existing architecture\nto accommodate a decoder and its respective contrastive loss. We then introduce\na weighted encoder-decoder contrastive loss with non-competing objectives that\nfacilitates the joint encoder-decoder architecture pre-training. We adapt two\nestablished contrastive SSL frameworks tailored for dense prediction tasks,\nachieve new state-of-the-art results in COCO object detection and instance\nsegmentation, and match state-of-the-art performance on Pascal VOC semantic\nsegmentation. We show that our approach allows for pre-training a decoder and\nenhances the representation power of the encoder and its performance in dense\nprediction tasks. This benefit holds across heterogeneous decoder architectures\nbetween pre-training and fine-tuning and persists in out-of-domain,\nlimited-data scenarios."}
{"id": "2503.17753", "pdf": "https://arxiv.org/pdf/2503.17753", "abs": "https://arxiv.org/abs/2503.17753", "authors": ["Hojun Cho", "Donghu Kim", "Soyoung Yang", "Chan Lee", "Hunjoo Lee", "Jaegul Choo"], "title": "Building Resource-Constrained Language Agents: A Korean Case Study on Chemical Toxicity Information", "categories": ["cs.CL", "cs.AI"], "comment": "Preprint", "summary": "Language agents powered by large language models (LLMs) face significant\ndeployment challenges in resource-constrained environments, particularly for\nspecialized domains and less-common languages. This paper presents Tox-chat, a\nKorean chemical toxicity information agent devised within these limitations. We\npropose two key innovations: a context-efficient architecture that reduces\ntoken consumption through hierarchical section search, and a scenario-based\ndialogue generation methodology that effectively distills tool-using\ncapabilities from larger models. Experimental evaluations demonstrate that our\nfine-tuned 8B parameter model substantially outperforms both untuned models and\nbaseline approaches, in terms of DB faithfulness and preference. Our work\noffers valuable insights for researchers developing domain-specific language\nagents under practical constraints."}
{"id": "2503.17530", "pdf": "https://arxiv.org/pdf/2503.17530", "abs": "https://arxiv.org/abs/2503.17530", "authors": ["Tianyu Zhang", "Fan Wan", "Haoran Duan", "Kevin W. Tong", "Jingjing Deng", "Yang Long"], "title": "FMDConv: Fast Multi-Attention Dynamic Convolution via Speed-Accuracy Trade-off", "categories": ["cs.CV"], "comment": null, "summary": "Spatial convolution is fundamental in constructing deep Convolutional Neural\nNetworks (CNNs) for visual recognition. While dynamic convolution enhances\nmodel accuracy by adaptively combining static kernels, it incurs significant\ncomputational overhead, limiting its deployment in resource-constrained\nenvironments such as federated edge computing. To address this, we propose Fast\nMulti-Attention Dynamic Convolution (FMDConv), which integrates input\nattention, temperature-degraded kernel attention, and output attention to\noptimize the speed-accuracy trade-off. FMDConv achieves a better balance\nbetween accuracy and efficiency by selectively enhancing feature extraction\nwith lower complexity. Furthermore, we introduce two novel quantitative\nmetrics, the Inverse Efficiency Score and Rate-Correct Score, to systematically\nevaluate this trade-off. Extensive experiments on CIFAR-10, CIFAR-100, and\nImageNet demonstrate that FMDConv reduces the computational cost by up to\n49.8\\% on ResNet-18 and 42.2\\% on ResNet-50 compared to prior multi-attention\ndynamic convolution methods while maintaining competitive accuracy. These\nadvantages make FMDConv highly suitable for real-world, resource-constrained\napplications."}
{"id": "2503.17755", "pdf": "https://arxiv.org/pdf/2503.17755", "abs": "https://arxiv.org/abs/2503.17755", "authors": ["Sharan Maiya", "Yinhong Liu", "Ramit Debnath", "Anna Korhonen"], "title": "Improving Preference Extraction In LLMs By Identifying Latent Knowledge Through Classifying Probes", "categories": ["cs.CL", "cs.LG"], "comment": "preprint, submitted to ACL ARR 2025, 21 pages, 23 figures", "summary": "Large Language Models (LLMs) are often used as automated judges to evaluate\ntext, but their effectiveness can be hindered by various unintentional biases.\nWe propose using linear classifying probes, trained by leveraging differences\nbetween contrasting pairs of prompts, to directly access LLMs' latent knowledge\nand extract more accurate preferences. Through extensive experiments using\nmodels of varying size from four different families and six diverse datasets\nassessing text quality evaluation and common sense reasoning, we demonstrate\nthat both supervised and unsupervised probing approaches consistently\noutperform traditional generation-based judgement while maintaining similar\ncomputational costs. These probes generalise under domain shifts and can even\noutperform finetuned evaluators with the same training data size. Our results\nsuggest linear probing offers an accurate, robust and computationally efficient\napproach for LLM-as-judge tasks while providing interpretable insights into how\nmodels encode judgement-relevant knowledge. Our data and code will be openly\nreleased in the future."}
{"id": "2503.17536", "pdf": "https://arxiv.org/pdf/2503.17536", "abs": "https://arxiv.org/abs/2503.17536", "authors": ["Nusrat Munia", "Abdullah-Al-Zubaer Imran"], "title": "DermDiff: Generative Diffusion Model for Mitigating Racial Biases in Dermatology Diagnosis", "categories": ["cs.CV"], "comment": "Paper presented at ADSMI@MICCAI 2024", "summary": "Skin diseases, such as skin cancer, are a significant public health issue,\nand early diagnosis is crucial for effective treatment. Artificial intelligence\n(AI) algorithms have the potential to assist in triaging benign vs malignant\nskin lesions and improve diagnostic accuracy. However, existing AI models for\nskin disease diagnosis are often developed and tested on limited and biased\ndatasets, leading to poor performance on certain skin tones. To address this\nproblem, we propose a novel generative model, named DermDiff, that can generate\ndiverse and representative dermoscopic image data for skin disease diagnosis.\nLeveraging text prompting and multimodal image-text learning, DermDiff improves\nthe representation of underrepresented groups (patients, diseases, etc.) in\nhighly imbalanced datasets. Our extensive experimentation showcases the\neffectiveness of DermDiff in terms of high fidelity and diversity. Furthermore,\ndownstream evaluation suggests the potential of DermDiff in mitigating racial\nbiases for dermatology diagnosis. Our code is available at\nhttps://github.com/Munia03/DermDiff"}
{"id": "2503.17799", "pdf": "https://arxiv.org/pdf/2503.17799", "abs": "https://arxiv.org/abs/2503.17799", "authors": ["Yuhang Jiang", "Ramakanth Kavuluru"], "title": "Relation Extraction with Instance-Adapted Predicate Descriptions", "categories": ["cs.CL"], "comment": null, "summary": "Relation extraction (RE) is a standard information extraction task playing a\nmajor role in downstream applications such as knowledge discovery and question\nanswering. Although decoder-only large language models are excelling in\ngenerative tasks, smaller encoder models are still the go to architecture for\nRE. In this paper, we revisit fine-tuning such smaller models using a novel\ndual-encoder architecture with a joint contrastive and cross-entropy loss.\nUnlike previous methods that employ a fixed linear layer for predicate\nrepresentations, our approach uses a second encoder to compute\ninstance-specific predicate representations by infusing them with real entity\nspans from corresponding input instances. We conducted experiments on two\nbiomedical RE datasets and two general domain datasets. Our approach achieved\nF1 score improvements ranging from 1% to 2% over state-of-the-art methods with\na simple but elegant formulation. Ablation studies justify the importance of\nvarious components built into the proposed architecture."}
{"id": "2503.17539", "pdf": "https://arxiv.org/pdf/2503.17539", "abs": "https://arxiv.org/abs/2503.17539", "authors": ["Bhishma Dedhia", "David Bourgin", "Krishna Kumar Singh", "Yuheng Li", "Yan Kang", "Zhan Xu", "Niraj K. Jha", "Yuchen Liu"], "title": "Generating, Fast and Slow: Scalable Parallel Video Generation with Video Interface Networks", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion Transformers (DiTs) can generate short photorealistic videos, yet\ndirectly training and sampling longer videos with full attention across the\nvideo remains computationally challenging. Alternative methods break long\nvideos down into sequential generation of short video segments, requiring\nmultiple sampling chain iterations and specialized consistency modules. To\novercome these challenges, we introduce a new paradigm called Video Interface\nNetworks (VINs), which augment DiTs with an abstraction module to enable\nparallel inference of video chunks. At each diffusion step, VINs encode global\nsemantics from the noisy input of local chunks and the encoded representations,\nin turn, guide DiTs in denoising chunks in parallel. The coupling of VIN and\nDiT is learned end-to-end on the denoising objective. Further, the VIN\narchitecture maintains fixed-size encoding tokens that encode the input via a\nsingle cross-attention step. Disentangling the encoding tokens from the input\nthus enables VIN to scale to long videos and learn essential semantics.\nExperiments on VBench demonstrate that VINs surpass existing chunk-based\nmethods in preserving background consistency and subject coherence. We then\nshow via an optical flow analysis that our approach attains state-of-the-art\nmotion smoothness while using 25-40% fewer FLOPs than full generation. Finally,\nhuman raters favorably assessed the overall video quality and temporal\nconsistency of our method in a user study."}
{"id": "2503.17810", "pdf": "https://arxiv.org/pdf/2503.17810", "abs": "https://arxiv.org/abs/2503.17810", "authors": ["Farhan Farsi", "Parnian Fazel", "Sepand Haghighi", "Sadra Sabouri", "Farzaneh Goshtasb", "Nadia Hajipour", "Ehsaneddin Asgari", "Hossein Sameti"], "title": "ParsiPy: NLP Toolkit for Historical Persian Texts in Python", "categories": ["cs.CL"], "comment": "13 pages, 6 figure, accepted into Second Workshop on Ancient Language\n  Processing (ALP2025)", "summary": "The study of historical languages presents unique challenges due to their\ncomplex orthographic systems, fragmentary textual evidence, and the absence of\nstandardized digital representations of text in those languages. Tackling these\nchallenges needs special NLP digital tools to handle phonetic transcriptions\nand analyze ancient texts. This work introduces ParsiPy, an NLP toolkit\ndesigned to facilitate the analysis of historical Persian languages by offering\nmodules for tokenization, lemmatization, part-of-speech tagging,\nphoneme-to-transliteration conversion, and word embedding. We demonstrate the\nutility of our toolkit through the processing of Parsig (Middle Persian) texts,\nhighlighting its potential for expanding computational methods in the study of\nhistorical languages. Through this work, we contribute to computational\nphilology, offering tools that can be adapted for the broader study of ancient\ntexts and their digital preservation."}
{"id": "2503.17544", "pdf": "https://arxiv.org/pdf/2503.17544", "abs": "https://arxiv.org/abs/2503.17544", "authors": ["Yan Zhang", "Yao Feng", "Alpár Cseke", "Nitin Saini", "Nathan Bajandas", "Nicolas Heron", "Michael J. Black"], "title": "PRIMAL: Physically Reactive and Interactive Motor Model for Avatar Learning", "categories": ["cs.CV", "cs.AI"], "comment": "18 pages", "summary": "To build a motor system of the interactive avatar, it is essential to develop\na generative motion model drives the body to move through 3D space in a\nperpetual, realistic, controllable, and responsive manner. Although motion\ngeneration has been extensively studied, most methods do not support ``embodied\nintelligence'' due to their offline setting, slow speed, limited motion\nlengths, or unnatural movements. To overcome these limitations, we propose\nPRIMAL, an autoregressive diffusion model that is learned with a two-stage\nparadigm, inspired by recent advances in foundation models. In the pretraining\nstage, the model learns motion dynamics from a large number of sub-second\nmotion segments, providing ``motor primitives'' from which more complex motions\nare built. In the adaptation phase, we employ a ControlNet-like adaptor to\nfine-tune the motor control for semantic action generation and spatial target\nreaching. Experiments show that physics effects emerge from our training. Given\na single-frame initial state, our model not only generates unbounded,\nrealistic, and controllable motion, but also enables the avatar to be\nresponsive to induced impulses in real time. In addition, we can effectively\nand efficiently adapt our base model to few-shot personalized actions and the\ntask of spatial control. Evaluations show that our proposed method outperforms\nstate-of-the-art baselines. We leverage the model to create a real-time\ncharacter animation system in Unreal Engine that is highly responsive and\nnatural. Code, models, and more results are available at:\nhttps://yz-cnsdqz.github.io/eigenmotion/PRIMAL"}
{"id": "2503.17811", "pdf": "https://arxiv.org/pdf/2503.17811", "abs": "https://arxiv.org/abs/2503.17811", "authors": ["Wenqi Pei", "Hailing Xu", "Hengyuan Zhao", "Shizheng Hou", "Han Chen", "Zining Zhang", "Pingyi Luo", "Bingsheng He"], "title": "Feather-SQL: A Lightweight NL2SQL Framework with Dual-Model Collaboration Paradigm for Small Language Models", "categories": ["cs.CL", "cs.AI", "cs.DB"], "comment": null, "summary": "Natural Language to SQL (NL2SQL) has seen significant advancements with large\nlanguage models (LLMs). However, these models often depend on closed-source\nsystems and high computational resources, posing challenges in data privacy and\ndeployment. In contrast, small language models (SLMs) struggle with NL2SQL\ntasks, exhibiting poor performance and incompatibility with existing\nframeworks. To address these issues, we introduce Feather-SQL, a new\nlightweight framework tailored for SLMs. Feather-SQL improves SQL executability\nand accuracy through 1) schema pruning and linking, 2) multi-path and\nmulti-candidate generation. Additionally, we introduce the 1+1 Model\nCollaboration Paradigm, which pairs a strong general-purpose chat model with a\nfine-tuned SQL specialist, combining strong analytical reasoning with\nhigh-precision SQL generation. Experimental results on BIRD demonstrate that\nFeather-SQL improves NL2SQL performance on SLMs, with around 10% boost for\nmodels without fine-tuning. The proposed paradigm raises the accuracy ceiling\nof SLMs to 54.76%, highlighting its effectiveness."}
{"id": "2503.17574", "pdf": "https://arxiv.org/pdf/2503.17574", "abs": "https://arxiv.org/abs/2503.17574", "authors": ["Simona Kocour", "Assia Benbihi", "Aikaterini Adam", "Torsten Sattler"], "title": "Is there anything left? Measuring semantic residuals of objects removed from 3D Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "Searching in and editing 3D scenes has become extremely intuitive with\ntrainable scene representations that allow linking human concepts to elements\nin the scene. These operations are often evaluated on the basis of how\naccurately the searched element is segmented or extracted from the scene. In\nthis paper, we address the inverse problem, that is, how much of the searched\nelement remains in the scene after it is removed. This question is particularly\nimportant in the context of privacy-preserving mapping when a user reconstructs\na 3D scene and wants to remove private elements before sharing the map. To the\nbest of our knowledge, this is the first work to address this question. To\nanswer this, we propose a quantitative evaluation that measures whether a\nremoval operation leaves object residuals that can be reasoned over. The scene\nis not private when such residuals are present. Experiments on state-of-the-art\nscene representations show that the proposed metrics are meaningful and\nconsistent with the user study that we also present. We also propose a method\nto refine the removal based on spatial and semantic consistency."}
{"id": "2503.17860", "pdf": "https://arxiv.org/pdf/2503.17860", "abs": "https://arxiv.org/abs/2503.17860", "authors": ["Felix Faltings", "Wei Wei", "Yujia Bao"], "title": "Enhancing Retrieval Systems with Inference-Time Logical Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "Traditional retrieval methods rely on transforming user queries into vector\nrepresentations and retrieving documents based on cosine similarity within an\nembedding space. While efficient and scalable, this approach often fails to\nhandle complex queries involving logical constructs such as negations,\nconjunctions, and disjunctions. In this paper, we propose a novel\ninference-time logical reasoning framework that explicitly incorporates logical\nreasoning into the retrieval process. Our method extracts logical reasoning\nstructures from natural language queries and then composes the individual\ncosine similarity scores to formulate the final document scores. This approach\nenables the retrieval process to handle complex logical reasoning without\ncompromising computational efficiency. Our results on both synthetic and\nreal-world benchmarks demonstrate that the proposed method consistently\noutperforms traditional retrieval methods across different models and datasets,\nsignificantly improving retrieval performance for complex queries."}
{"id": "2503.17593", "pdf": "https://arxiv.org/pdf/2503.17593", "abs": "https://arxiv.org/abs/2503.17593", "authors": ["Mehdi Noroozi", "Alberto Gil Ramos", "Luca Morreale", "Ruchika Chavhan", "Malcolm Chadwick", "Abhinav Mehrotra", "Sourav Bhattacharya"], "title": "Guidance Free Image Editing via Explicit Conditioning", "categories": ["cs.CV"], "comment": null, "summary": "Current sampling mechanisms for conditional diffusion models rely mainly on\nClassifier Free Guidance (CFG) to generate high-quality images. However, CFG\nrequires several denoising passes in each time step, e.g., up to three passes\nin image editing tasks, resulting in excessive computational costs. This paper\nintroduces a novel conditioning technique to ease the computational burden of\nthe well-established guidance techniques, thereby significantly improving the\ninference time of diffusion models. We present Explicit Conditioning (EC) of\nthe noise distribution on the input modalities to achieve this. Intuitively, we\nmodel the noise to guide the conditional diffusion model during the diffusion\nprocess. We present evaluations on image editing tasks and demonstrate that EC\noutperforms CFG in generating diverse high-quality images with significantly\nreduced computations."}
{"id": "2503.17876", "pdf": "https://arxiv.org/pdf/2503.17876", "abs": "https://arxiv.org/abs/2503.17876", "authors": ["Kaiwen Zuo", "Jing Tang", "Hanbing Qin", "Binli Luo", "Ligang He", "Shiyan Tang"], "title": "Satisfactory Medical Consultation based on Terminology-Enhanced Information Retrieval and Emotional In-Context Learning", "categories": ["cs.CL", "cs.IR"], "comment": "The 46th European Conference on Information Retrieval Workshop", "summary": "Recent advancements in Large Language Models (LLMs) have marked significant\nprogress in understanding and responding to medical inquiries. However, their\nperformance still falls short of the standards set by professional\nconsultations. This paper introduces a novel framework for medical\nconsultation, comprising two main modules: Terminology-Enhanced Information\nRetrieval (TEIR) and Emotional In-Context Learning (EICL). TEIR ensures\nimplicit reasoning through the utilization of inductive knowledge and key\nterminology retrieval, overcoming the limitations of restricted domain\nknowledge in public databases. Additionally, this module features capabilities\nfor processing long context. The EICL module aids in generating sentences with\nhigh attribute relevance by memorizing semantic and attribute information from\nunlabelled corpora and applying controlled retrieval for the required\ninformation. Furthermore, a dataset comprising 803,564 consultation records was\ncompiled in China, significantly enhancing the model's capability for complex\ndialogues and proactive inquiry initiation. Comprehensive experiments\ndemonstrate the proposed method's effectiveness in extending the context window\nlength of existing LLMs. The experimental outcomes and extensive data validate\nthe framework's superiority over five baseline models in terms of BLEU and\nROUGE performance metrics, with substantial leads in certain capabilities.\nNotably, ablation studies confirm the significance of the TEIR and EICL\ncomponents. In addition, our new framework has the potential to significantly\nimprove patient satisfaction in real clinical consulting situations."}
{"id": "2503.17625", "pdf": "https://arxiv.org/pdf/2503.17625", "abs": "https://arxiv.org/abs/2503.17625", "authors": ["Karol Chlasta", "Katarzyna Wisiecka", "Krzysztof Krejtz", "Izabela Krejtz"], "title": "AI-Based Screening for Depression and Social Anxiety Through Eye Tracking: An Exploratory Study", "categories": ["cs.CV", "cs.AI", "cs.CY", "cs.HC", "cs.LG", "68U01", "J.3; I.2; I.5; H.4; C.3"], "comment": "17 pages, 11 figures", "summary": "Well-being is a dynamic construct that evolves over time and fluctuates\nwithin individuals, presenting challenges for accurate quantification. Reduced\nwell-being is often linked to depression or anxiety disorders, which are\ncharacterised by biases in visual attention towards specific stimuli, such as\nhuman faces. This paper introduces a novel approach to AI-assisted screening of\naffective disorders by analysing visual attention scan paths using\nconvolutional neural networks (CNNs). Data were collected from two studies\nexamining (1) attentional tendencies in individuals diagnosed with major\ndepression and (2) social anxiety. These data were processed using residual\nCNNs through images generated from eye-gaze patterns. Experimental results,\nobtained with ResNet architectures, demonstrated an average accuracy of 48% for\na three-class system and 62% for a two-class system. Based on these exploratory\nfindings, we propose that this method could be employed in rapid, ecological,\nand effective mental health screening systems to assess well-being through\neye-tracking."}
{"id": "2503.17882", "pdf": "https://arxiv.org/pdf/2503.17882", "abs": "https://arxiv.org/abs/2503.17882", "authors": ["Shengyun Si", "Xinpeng Wang", "Guangyao Zhai", "Nassir Navab", "Barbara Plank"], "title": "Think Before Refusal : Triggering Safety Reflection in LLMs to Mitigate False Refusal Behavior", "categories": ["cs.CL", "cs.AI"], "comment": "18 pages, 23 figures", "summary": "Recent advancements in large language models (LLMs) have demonstrated that\nfine-tuning and human alignment can render LLMs harmless. In practice, such\n\"harmlessness\" behavior is mainly achieved by training models to reject harmful\nrequests, such as \"Explain how to burn down my neighbor's house\", where the\nmodel appropriately declines to respond. However, this approach can\ninadvertently result in false refusal, where models reject benign queries as\nwell, such as \"Tell me how to kill a Python process\". In this work, we\ndemonstrate that prompting safety reflection before generating a response can\nmitigate false refusal behavior. Building on this finding, we introduce the\nThink-Before-Refusal (TBR) schema and conduct safety-aware instruction\nfine-tuning incorporating safety reflection. In an ablation study across 15\npre-trained models, we show that models fine-tuned with safety reflection\nsignificantly reduce false refusal behavior while maintaining safety and\noverall performance compared to those fine-tuned without safety reflection."}
{"id": "2503.17633", "pdf": "https://arxiv.org/pdf/2503.17633", "abs": "https://arxiv.org/abs/2503.17633", "authors": ["Tejas Panambur", "Mario Parente"], "title": "Enhancing Martian Terrain Recognition with Deep Constrained Clustering", "categories": ["cs.CV"], "comment": null, "summary": "Martian terrain recognition is pivotal for advancing our understanding of\ntopography, geomorphology, paleoclimate, and habitability. While deep\nclustering methods have shown promise in learning semantically homogeneous\nfeature embeddings from Martian rover imagery, the natural variations in\nintensity, scale, and rotation pose significant challenges for accurate terrain\nclassification. To address these limitations, we propose Deep Constrained\nClustering with Metric Learning (DCCML), a novel algorithm that leverages\nmultiple constraint types to guide the clustering process. DCCML incorporates\nsoft must-link constraints derived from spatial and depth similarities between\nneighboring patches, alongside hard constraints from stereo camera pairs and\ntemporally adjacent images. Experimental evaluation on the Curiosity rover\ndataset (with 150 clusters) demonstrates that DCCML increases homogeneous\nclusters by 16.7 percent while reducing the Davies-Bouldin Index from 3.86 to\n1.82 and boosting retrieval accuracy from 86.71 percent to 89.86 percent. This\nimprovement enables more precise classification of Martian geological features,\nadvancing our capacity to analyze and understand the planet's landscape."}
{"id": "2503.17900", "pdf": "https://arxiv.org/pdf/2503.17900", "abs": "https://arxiv.org/abs/2503.17900", "authors": ["Hsin-Ling Hsu", "Cong-Tinh Dao", "Luning Wang", "Zitao Shuai", "Thao Nguyen Minh Phan", "Jun-En Ding", "Chun-Chieh Liao", "Pengfei Hu", "Xiaoxue Han", "Chih-Ho Hsu", "Dongsheng Luo", "Wen-Chih Peng", "Feng Liu", "Fang-Ming Hung", "Chenwei Wu"], "title": "MedPlan:A Two-Stage RAG-Based System for Personalized Medical Plan Generation", "categories": ["cs.CL"], "comment": null, "summary": "Despite recent success in applying large language models (LLMs) to electronic\nhealth records (EHR), most systems focus primarily on assessment rather than\ntreatment planning. We identify three critical limitations in current\napproaches: they generate treatment plans in a single pass rather than\nfollowing the sequential reasoning process used by clinicians; they rarely\nincorporate patient-specific historical context; and they fail to effectively\ndistinguish between subjective and objective clinical information. Motivated by\nthe SOAP methodology (Subjective, Objective, Assessment, Plan), we introduce\nMedPlan, a novel framework that structures LLM reasoning to align with\nreal-life clinician workflows. Our approach employs a two-stage architecture\nthat first generates a clinical assessment based on patient symptoms and\nobjective data, then formulates a structured treatment plan informed by this\nassessment and enriched with patient-specific information through\nretrieval-augmented generation. Comprehensive evaluation demonstrates that our\nmethod significantly outperforms baseline approaches in both assessment\naccuracy and treatment plan quality."}
{"id": "2503.17641", "pdf": "https://arxiv.org/pdf/2503.17641", "abs": "https://arxiv.org/abs/2503.17641", "authors": ["Chi Zhang", "Chengjian Feng", "Feng Yan", "Qiming Zhang", "Mingjin Zhang", "Yujie Zhong", "Jing Zhang", "Lin Ma"], "title": "InstructVEdit: A Holistic Approach for Instructional Video Editing", "categories": ["cs.CV"], "comment": "https://o937-blip.github.io/InstructVEdit", "summary": "Video editing according to instructions is a highly challenging task due to\nthe difficulty in collecting large-scale, high-quality edited video pair data.\nThis scarcity not only limits the availability of training data but also\nhinders the systematic exploration of model architectures and training\nstrategies. While prior work has improved specific aspects of video editing\n(e.g., synthesizing a video dataset using image editing techniques or\ndecomposed video editing training), a holistic framework addressing the above\nchallenges remains underexplored. In this study, we introduce InstructVEdit, a\nfull-cycle instructional video editing approach that: (1) establishes a\nreliable dataset curation workflow to initialize training, (2) incorporates two\nmodel architectural improvements to enhance edit quality while preserving\ntemporal consistency, and (3) proposes an iterative refinement strategy\nleveraging real-world data to enhance generalization and minimize train-test\ndiscrepancies. Extensive experiments show that InstructVEdit achieves\nstate-of-the-art performance in instruction-based video editing, demonstrating\nrobust adaptability to diverse real-world scenarios. Project page:\nhttps://o937-blip.github.io/InstructVEdit."}
{"id": "2503.17922", "pdf": "https://arxiv.org/pdf/2503.17922", "abs": "https://arxiv.org/abs/2503.17922", "authors": ["Youhui Zuo", "Sibo Wei", "Chen Zhang", "Zhuorui Liu", "Wenpeng Lu", "Dawei Song"], "title": "WindowKV: Task-Adaptive Group-Wise KV Cache Window Selection for Efficient LLM Inference", "categories": ["cs.CL"], "comment": null, "summary": "With the advancements in long-context inference capabilities of large\nlanguage models (LLMs), the KV cache has become one of the foundational\ncomponents. However, its substantial GPU memory consumption makes KV cache\ncompression a key technique for enabling efficient LLM inference in industrial\nscenarios. While recent studies have focused on optimizing the memory occupied\nby the KV cache, they overlook two critical factors: preserving semantic\ncoherence and considering task-specific characteristic during compression. To\naddress these limitations, we propose a novel task-adaptive KV cache window\nselection method, WindowKV. WindowKV dynamically selects local semantic windows\nconsisting of consecutive tokens, according to task-specific characteristics,\nensuring the retained KV cache captures continuous, essential context.\nAdditionally, we introduce an intra-group layer KV cache indices sharing\nstrategy to reduce computational overhead, achieving a balance between\nperformance and efficiency. We rigorously evaluate WindowKV on the LongBench\nbenchmark, and the results demonstrate that it maintains a performance\ncomparable to full KV cache retention while using only 12% of the original KV\ncache, significantly reducing memory requirements. Furthermore, our method also\nachieves state-of-the-art results in the Needle-in-a-Haystack evaluation,\nhighlighting its effectiveness and robustness."}
{"id": "2503.17650", "pdf": "https://arxiv.org/pdf/2503.17650", "abs": "https://arxiv.org/abs/2503.17650", "authors": ["Xi Xiao", "Yunbei Zhang", "Yanshuh Li", "Xingjian Li", "Tianyang Wang", "Jihun Hamm", "Xiao Wang", "Min Xu"], "title": "Visual Variational Autoencoder Prompt Tuning", "categories": ["cs.CV"], "comment": null, "summary": "Parameter-efficient fine-tuning (PEFT) has emerged as a crucial approach for\nadapting large vision transformers to downstream tasks without the prohibitive\ncomputational costs of full fine-tuning. While existing visual prompt tuning\n(VPT) methods have made significant strides, they predominantly rely on static,\ndomain-specific prompts that fail to capture the rich visual diversity within\nindividual instances. This paper introduces V$^2$APT (Visual Variational\nAutoencoder Prompt Tuning), a novel framework that generates dynamic,\ninput-dependent prompts using a variational autoencoder architecture. By\nlearning a latent representation of image-specific features and decoding them\ninto customized prompts, V$^2$APT adapts to the unique visual characteristics\nof each input. Extensive experiments on FGVC, HTA, and VTAB-1k benchmarks\ndemonstrate that our approach consistently outperforms state-of-the-art PEFT\nmethods. Notably, V$^2$APT achieves +3.2\\% improvement over VPT-Deep on HTA,\nwith an average performance gain of +2.0\\% across all three datasets."}
{"id": "2503.17932", "pdf": "https://arxiv.org/pdf/2503.17932", "abs": "https://arxiv.org/abs/2503.17932", "authors": ["Xunguang Wang", "Wenxuan Wang", "Zhenlan Ji", "Zongjie Li", "Pingchuan Ma", "Daoyuan Wu", "Shuai Wang"], "title": "STShield: Single-Token Sentinel for Real-Time Jailbreak Detection in Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.CR"], "comment": "11 pages", "summary": "Large Language Models (LLMs) have become increasingly vulnerable to jailbreak\nattacks that circumvent their safety mechanisms. While existing defense methods\neither suffer from adaptive attacks or require computationally expensive\nauxiliary models, we present STShield, a lightweight framework for real-time\njailbroken judgement. STShield introduces a novel single-token sentinel\nmechanism that appends a binary safety indicator to the model's response\nsequence, leveraging the LLM's own alignment capabilities for detection. Our\nframework combines supervised fine-tuning on normal prompts with adversarial\ntraining using embedding-space perturbations, achieving robust detection while\npreserving model utility. Extensive experiments demonstrate that STShield\nsuccessfully defends against various jailbreak attacks, while maintaining the\nmodel's performance on legitimate queries. Compared to existing approaches,\nSTShield achieves superior defense performance with minimal computational\noverhead, making it a practical solution for real-world LLM deployment."}
{"id": "2503.17651", "pdf": "https://arxiv.org/pdf/2503.17651", "abs": "https://arxiv.org/abs/2503.17651", "authors": ["Zhuo Tao", "Liang Li", "Qi Chen", "Yunbin Tu", "Zheng-Jun Zha", "Ming-Hsuan Yang", "Yuankai Qi", "Qingming Huang"], "title": "Collaborative Temporal Consistency Learning for Point-supervised Natural Language Video Localization", "categories": ["cs.CV"], "comment": "Under review", "summary": "Natural language video localization (NLVL) is a crucial task in video\nunderstanding that aims to localize the target moment in videos specified by a\ngiven language description. Recently, a point-supervised paradigm has been\npresented to address this task, requiring only a single annotated frame within\nthe target moment rather than complete temporal boundaries. Compared with the\nfully-supervised paradigm, it offers a balance between localization accuracy\nand annotation cost. However, due to the absence of complete annotation, it is\nchallenging to align the video content with language descriptions, consequently\nhindering accurate moment prediction. To address this problem, we propose a new\nCOllaborative Temporal consistEncy Learning (COTEL) framework that leverages\nthe synergy between saliency detection and moment localization to strengthen\nthe video-language alignment. Specifically, we first design a frame- and a\nsegment-level Temporal Consistency Learning (TCL) module that models semantic\nalignment across frame saliencies and sentence-moment pairs. Then, we design a\ncross-consistency guidance scheme, including a Frame-level Consistency Guidance\n(FCG) and a Segment-level Consistency Guidance (SCG), that enables the two\ntemporal consistency learning paths to reinforce each other mutually. Further,\nwe introduce a Hierarchical Contrastive Alignment Loss (HCAL) to\ncomprehensively align the video and text query. Extensive experiments on two\nbenchmarks demonstrate that our method performs favorably against SoTA\napproaches. We will release all the source codes."}
{"id": "2503.17933", "pdf": "https://arxiv.org/pdf/2503.17933", "abs": "https://arxiv.org/abs/2503.17933", "authors": ["Justice Ou", "Tinglin Huang", "Yilun Zhao", "Ziyang Yu", "Peiqing Lu", "Rex Ying"], "title": "Experience Retrieval-Augmentation with Electronic Health Records Enables Accurate Discharge QA", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "To improve the reliability of Large Language Models (LLMs) in clinical\napplications, retrieval-augmented generation (RAG) is extensively applied to\nprovide factual medical knowledge. However, beyond general medical knowledge\nfrom open-ended datasets, clinical case-based knowledge is also critical for\neffective medical reasoning, as it provides context grounded in real-world\npatient experiences. Motivated by this, we propose Experience Retrieval\nAugmentation - ExpRAG framework based on Electronic Health Record (EHR), aiming\nto offer the relevant context from other patients' discharge reports. ExpRAG\nperforms retrieval through a coarse-to-fine process, utilizing an EHR-based\nreport ranker to efficiently identify similar patients, followed by an\nexperience retriever to extract task-relevant content for enhanced medical\nreasoning. To evaluate ExpRAG, we introduce DischargeQA, a clinical QA dataset\nwith 1,280 discharge-related questions across diagnosis, medication, and\ninstruction tasks. Each problem is generated using EHR data to ensure realistic\nand challenging scenarios. Experimental results demonstrate that ExpRAG\nconsistently outperforms a text-based ranker, achieving an average relative\nimprovement of 5.2%, highlighting the importance of case-based knowledge for\nmedical reasoning."}
{"id": "2503.17657", "pdf": "https://arxiv.org/pdf/2503.17657", "abs": "https://arxiv.org/abs/2503.17657", "authors": ["Yumeng Ren", "Yaofang Liu", "Aitor Artola", "Laurent Mertz", "Raymond H. Chan", "Jean-michel Morel"], "title": "Efficient Diffusion Training through Parallelization with Truncated Karhunen-Loève Expansion", "categories": ["cs.CV", "I.2.0; I.4.0"], "comment": "12 pages, 9 figures", "summary": "Diffusion denoising models have become a popular approach for image\ngeneration, but they often suffer from slow convergence during training. In\nthis paper, we identify that this slow convergence is partly due to the\ncomplexity of the Brownian motion driving the forward-time process. To address\nthis, we represent the Brownian motion using the Karhunen-Lo\\`eve expansion,\ntruncating it to a limited number of eigenfunctions. We propose a novel\nordinary differential equation with augmented random initials, termed KL\ndiffusion, as a new forward-time process for training and sampling. By\ndeveloping an appropriate denoising loss function, we facilitate the\nintegration of our KL-diffusion into existing denoising-based models. Using the\nwidely adopted DDIM framework as our baseline ensures a fair comparison, as our\nmodifications focus solely on the forward process and loss function, leaving\nthe network architecture and sampling methods unchanged. Our method\nsignificantly outperforms baseline diffusion models, achieving convergence\nspeeds that are twice faster to reach the best FID score of the baseline and\nultimately yielding much lower FID scores. Notably, our approach allows for\nhighly parallelized computation, requires no additional learnable parameters,\nand can be flexibly integrated into existing diffusion methods. The code will\nbe made publicly available."}
{"id": "2503.17936", "pdf": "https://arxiv.org/pdf/2503.17936", "abs": "https://arxiv.org/abs/2503.17936", "authors": ["Riya Naik", "Ashwin Srinivasan", "Estrid He", "Swati Agarwal"], "title": "An Empirical Study of the Role of Incompleteness and Ambiguity in Interactions with Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Natural language as a medium for human-computer interaction has long been\nanticipated, has been undergoing a sea-change with the advent of Large Language\nModels (LLMs) with startling capacities for processing and generating language.\nMany of us now treat LLMs as modern-day oracles, asking it almost any kind of\nquestion. Unlike its Delphic predecessor, consulting an LLM does not have to be\na single-turn activity (ask a question, receive an answer, leave); and -- also\nunlike the Pythia -- it is widely acknowledged that answers from LLMs can be\nimproved with additional context. In this paper, we aim to study when we need\nmulti-turn interactions with LLMs to successfully get a question answered; or\nconclude that a question is unanswerable. We present a neural symbolic\nframework that models the interactions between human and LLM agents. Through\nthe proposed framework, we define incompleteness and ambiguity in the questions\nas properties deducible from the messages exchanged in the interaction, and\nprovide results from benchmark problems, in which the answer-correctness is\nshown to depend on whether or not questions demonstrate the presence of\nincompleteness or ambiguity (according to the properties we identify). Our\nresults show multi-turn interactions are usually required for datasets which\nhave a high proportion of incompleteness or ambiguous questions; and that that\nincreasing interaction length has the effect of reducing incompleteness or\nambiguity. The results also suggest that our measures of incompleteness and\nambiguity can be useful tools for characterising interactions with an LLM on\nquestion-answeringproblems"}
{"id": "2503.17660", "pdf": "https://arxiv.org/pdf/2503.17660", "abs": "https://arxiv.org/abs/2503.17660", "authors": ["Kun Li", "Jianhui Wang", "Miao Zhang", "Xueqian Wang"], "title": "OMR-Diffusion:Optimizing Multi-Round Enhanced Training in Diffusion Models for Improved Intent Understanding", "categories": ["cs.CV"], "comment": null, "summary": "Generative AI has significantly advanced text-driven image generation, but it\nstill faces challenges in producing outputs that consistently align with\nevolving user preferences and intents, particularly in multi-turn dialogue\nscenarios. In this research, We present a Visual Co-Adaptation (VCA) framework\nthat incorporates human-in-the-loop feedback, utilizing a well-trained reward\nmodel specifically designed to closely align with human preferences. Using a\ndiverse multi-turn dialogue dataset, the framework applies multiple reward\nfunctions (such as diversity, consistency, and preference feedback) to refine\nthe diffusion model through LoRA, effectively optimizing image generation based\non user input. We also constructed multi-round dialogue datasets with prompts\nand image pairs that well-fit user intent. Experiments show the model achieves\n508 wins in human evaluation, outperforming DALL-E 3 (463 wins) and others. It\nalso achieves 3.4 rounds in dialogue efficiency (vs. 13.7 for DALL-E 3) and\nexcels in metrics like LPIPS (0.15) and BLIP (0.59). Various experiments\ndemonstrate the effectiveness of the proposed method over state-of-the-art\nbaselines, with significant improvements in image consistency and alignment\nwith user intent."}
{"id": "2503.17952", "pdf": "https://arxiv.org/pdf/2503.17952", "abs": "https://arxiv.org/abs/2503.17952", "authors": ["Divyansh Singh", "Manuel Nunez Martinez", "Bonnie J. Dorr", "Sonja Schmer Galunder"], "title": "SLIDE: Sliding Localized Information for Document Extraction", "categories": ["cs.CL"], "comment": null, "summary": "Constructing accurate knowledge graphs from long texts and low-resource\nlanguages is challenging, as large language models (LLMs) experience degraded\nperformance with longer input chunks. This problem is amplified in low-resource\nsettings where data scarcity hinders accurate entity and relationship\nextraction. Contextual retrieval methods, while improving retrieval accuracy,\nstruggle with long documents. They truncate critical information in texts\nexceeding maximum context lengths of LLMs, significantly limiting knowledge\ngraph construction. We introduce SLIDE (Sliding Localized Information for\nDocument Extraction), a chunking method that processes long documents by\ngenerating local context through overlapping windows. SLIDE ensures that\nessential contextual information is retained, enhancing knowledge graph\nextraction from documents exceeding LLM context limits. It significantly\nimproves GraphRAG performance, achieving a 24% increase in entity extraction\nand a 39% improvement in relationship extraction for English. For Afrikaans, a\nlow-resource language, SLIDE achieves a 49% increase in entity extraction and\nan 82% improvement in relationship extraction. Furthermore, it improves upon\nstate-of-the-art in question-answering metrics such as comprehensiveness,\ndiversity and empowerment, demonstrating its effectiveness in multilingual and\nresource-constrained settings."}
{"id": "2503.17668", "pdf": "https://arxiv.org/pdf/2503.17668", "abs": "https://arxiv.org/abs/2503.17668", "authors": ["Usha Kumari", "Shuvendu Rana"], "title": "3D Modeling: Camera Movement Estimation and path Correction for SFM Model using the Combination of Modified A-SIFT and Stereo System", "categories": ["cs.CV"], "comment": null, "summary": "Creating accurate and efficient 3D models poses significant challenges,\nparticularly in addressing large viewpoint variations, computational\ncomplexity, and alignment discrepancies. Efficient camera path generation can\nhelp resolve these issues. In this context, a modified version of the Affine\nScale-Invariant Feature Transform (ASIFT) is proposed to extract more matching\npoints with reduced computational overhead, ensuring an adequate number of\ninliers for precise camera rotation angle estimation. Additionally, a novel\ntwo-camera-based rotation correction model is introduced to mitigate small\nrotational errors, further enhancing accuracy. Furthermore, a stereo\ncamera-based translation estimation and correction model is implemented to\ndetermine camera movement in 3D space by altering the Structure From Motion\n(SFM) model. Finally, the novel combination of ASIFT and two camera-based SFM\nmodels provides an accurate camera movement trajectory in 3D space.\nExperimental results show that the proposed camera movement approach achieves\n99.9% accuracy compared to the actual camera movement path and outperforms\nstate-of-the-art camera path estimation methods. By leveraging this accurate\ncamera path, the system facilitates the creation of precise 3D models, making\nit a robust solution for applications requiring high fidelity and efficiency in\n3D reconstruction."}
{"id": "2503.17963", "pdf": "https://arxiv.org/pdf/2503.17963", "abs": "https://arxiv.org/abs/2503.17963", "authors": ["Guijin Son", "Hyunwoo Ko", "Haneral Jung", "Chami Hwang"], "title": "Won: Establishing Best Practices for Korean Financial NLP", "categories": ["cs.CL"], "comment": "The training dataset is uploaded here:\n  https://huggingface.co/datasets/KRX-Data/Won-Instruct. The model will be\n  updated shortly", "summary": "In this work, we present the first open leaderboard for evaluating Korean\nlarge language models focused on finance. Operated for about eight weeks, the\nleaderboard evaluated 1,119 submissions on a closed benchmark covering five\nMCQA categories: finance and accounting, stock price prediction, domestic\ncompany analysis, financial markets, and financial agent tasks and one\nopen-ended qa task. Building on insights from these evaluations, we release an\nopen instruction dataset of 80k instances and summarize widely used training\nstrategies observed among top-performing models. Finally, we introduce Won, a\nfully open and transparent LLM built using these best practices. We hope our\ncontributions help advance the development of better and safer financial LLMs\nfor Korean and other languages."}
{"id": "2503.17669", "pdf": "https://arxiv.org/pdf/2503.17669", "abs": "https://arxiv.org/abs/2503.17669", "authors": ["Yuheng Feng", "Jianhui Wang", "Kun Li", "Sida Li", "Tianyu Shi", "Haoyue Han", "Miao Zhang", "Xueqian Wang"], "title": "TDRI: Two-Phase Dialogue Refinement and Co-Adaptation for Interactive Image Generation", "categories": ["cs.CV"], "comment": null, "summary": "Although text-to-image generation technologies have made significant\nadvancements, they still face challenges when dealing with ambiguous prompts\nand aligning outputs with user intent.Our proposed framework, TDRI (Two-Phase\nDialogue Refinement and Co-Adaptation), addresses these issues by enhancing\nimage generation through iterative user interaction. It consists of two phases:\nthe Initial Generation Phase, which creates base images based on user prompts,\nand the Interactive Refinement Phase, which integrates user feedback through\nthree key modules. The Dialogue-to-Prompt (D2P) module ensures that user\nfeedback is effectively transformed into actionable prompts, which improves the\nalignment between user intent and model input. By evaluating generated outputs\nagainst user expectations, the Feedback-Reflection (FR) module identifies\ndiscrepancies and facilitates improvements. In an effort to ensure consistently\nhigh-quality results, the Adaptive Optimization (AO) module fine-tunes the\ngeneration process by balancing user preferences and maintaining prompt\nfidelity. Experimental results show that TDRI outperforms existing methods by\nachieving 33.6% human preference, compared to 6.2% for GPT-4 augmentation, and\nthe highest CLIP and BLIP alignment scores (0.338 and 0.336, respectively). In\niterative feedback tasks, user satisfaction increased to 88% after 8 rounds,\nwith diminishing returns beyond 6 rounds. Furthermore, TDRI has been found to\nreduce the number of iterations and improve personalization in the creation of\nfashion products. TDRI exhibits a strong potential for a wide range of\napplications in the creative and industrial domains, as it streamlines the\ncreative process and improves alignment with user preferences"}
{"id": "2503.17965", "pdf": "https://arxiv.org/pdf/2503.17965", "abs": "https://arxiv.org/abs/2503.17965", "authors": ["Beining Xu", "Arkaitz Zubiaga"], "title": "Understanding the Effects of RLHF on the Quality and Detectability of LLM-Generated Texts", "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7"], "comment": "14 pages, 3 figures", "summary": "Large Language Models (LLMs) have demonstrated exceptional performance on a\nrange of downstream NLP tasks by generating text that closely resembles human\nwriting. However, the ease of achieving this similarity raises concerns from\npotential malicious uses at scale by bad actors, as LLM-generated text becomes\nincreasingly difficult to discern from human text. Although detection methods\nhave been developed to address this issue, bad actors can further manipulate\nLLM-generated texts to make them less detectable. In this work, we study how\nfurther editing texts with Reinforcement Learning from Human Feedback (RLHF),\nwhich aligns model outputs with human preferences, affects (a) the quality of\ngenerated texts for two tasks, and (b) the performance of LLM-generated text\ndetectors, looking at both training-based and zero-shot detection methods.\nAlthough RLHF improves the quality of LLM-generated texts, we find that it also\ntends to produce more detectable, lengthy, and repetitive outputs.\nAdditionally, we observe that training-based detectors are vulnerable to short\ntexts and to texts that incorporate code, whereas zero-shot detectors exhibit\ngreater robustness."}
{"id": "2503.17672", "pdf": "https://arxiv.org/pdf/2503.17672", "abs": "https://arxiv.org/abs/2503.17672", "authors": ["Qing Zhong", "Peng-Tao Jiang", "Wen Wang", "Guodong Ding", "Lin Wu", "Kaiqi Huang"], "title": "A Temporal Modeling Framework for Video Pre-Training on Video Instance Segmentation", "categories": ["cs.CV"], "comment": "7 pages, 5figures, 6 tables, Accepted to ICME 2025", "summary": "Contemporary Video Instance Segmentation (VIS) methods typically adhere to a\npre-train then fine-tune regime, where a segmentation model trained on images\nis fine-tuned on videos. However, the lack of temporal knowledge in the\npre-trained model introduces a domain gap which may adversely affect the VIS\nperformance. To effectively bridge this gap, we present a novel video\npre-training approach to enhance VIS models, especially for videos with\nintricate instance relationships. Our crucial innovation focuses on reducing\ndisparities between the pre-training and fine-tuning stages. Specifically, we\nfirst introduce consistent pseudo-video augmentations to create diverse\npseudo-video samples for pre-training while maintaining the instance\nconsistency across frames. Then, we incorporate a multi-scale temporal module\nto enhance the model's ability to model temporal relations through self- and\ncross-attention at short- and long-term temporal spans. Our approach does not\nset constraints on model architecture and can integrate seamlessly with various\nVIS methods. Experiment results on commonly adopted VIS benchmarks show that\nour method consistently outperforms state-of-the-art methods. Our approach\nachieves a notable 4.0% increase in average precision on the challenging OVIS\ndataset."}
{"id": "2503.17994", "pdf": "https://arxiv.org/pdf/2503.17994", "abs": "https://arxiv.org/abs/2503.17994", "authors": ["Xin Xue", "Haoyi Zhou", "Tianyu Chen", "Shuai Zhang", "Yizhou Long", "Jianxin Li"], "title": "Instructing the Architecture Search for Spatial-temporal Sequence Forecasting with LLM", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Spatial-temporal sequence forecasting (STSF) is a long-standing research\nproblem with widespread real-world applications. Neural architecture search\n(NAS), which automates the neural network design, has been shown effective in\ntackling the STSF problem. However, the existing NAS methods for STSF focus on\ngenerating architectures in a time-consuming data-driven fashion, which heavily\nlimits their ability to use background knowledge and explore the complicated\nsearch trajectory. Large language models (LLMs) have shown remarkable ability\nin decision-making with comprehensive internal world knowledge, but how it\ncould benefit NAS for STSF remains unexplored. In this paper, we propose a\nnovel NAS method for STSF based on LLM. Instead of directly generate\narchitectures with LLM, We inspire the LLM's capability with a multi-level\nenhancement mechanism. Specifically, on the step-level, we decompose the\ngeneration task into decision steps with powerful prompt engineering and\ninspire LLM to serve as instructor for architecture search based on its\ninternal knowledge. On the instance-level, we utilize a one-step tuning\nframework to quickly evaluate the architecture instance and a memory bank to\ncumulate knowledge to improve LLM's search ability. On the task-level, we\npropose a two-stage architecture search, balancing the exploration stage and\noptimization stage, to reduce the possibility of being trapped in local optima.\nExtensive experimental results demonstrate that our method can achieve\ncompetitive effectiveness with superior efficiency against existing NAS methods\nfor STSF."}
{"id": "2503.17673", "pdf": "https://arxiv.org/pdf/2503.17673", "abs": "https://arxiv.org/abs/2503.17673", "authors": ["Jinyuan Liu", "Bowei Zhang", "Qingyun Mei", "Xingyuan Li", "Yang Zou", "Zhiying Jiang", "Long Ma", "Risheng Liu", "Xin Fan"], "title": "DCEvo: Discriminative Cross-Dimensional Evolutionary Learning for Infrared and Visible Image Fusion", "categories": ["cs.CV", "68T45", "I.4.3"], "comment": "Accepted by CVPR 2025", "summary": "Infrared and visible image fusion integrates information from distinct\nspectral bands to enhance image quality by leveraging the strengths and\nmitigating the limitations of each modality. Existing approaches typically\ntreat image fusion and subsequent high-level tasks as separate processes,\nresulting in fused images that offer only marginal gains in task performance\nand fail to provide constructive feedback for optimizing the fusion process. To\novercome these limitations, we propose a Discriminative Cross-Dimension\nEvolutionary Learning Framework, termed DCEvo, which simultaneously enhances\nvisual quality and perception accuracy. Leveraging the robust search\ncapabilities of Evolutionary Learning, our approach formulates the optimization\nof dual tasks as a multi-objective problem by employing an Evolutionary\nAlgorithm (EA) to dynamically balance loss function parameters. Inspired by\nvisual neuroscience, we integrate a Discriminative Enhancer (DE) within both\nthe encoder and decoder, enabling the effective learning of complementary\nfeatures from different modalities. Additionally, our Cross-Dimensional\nEmbedding (CDE) block facilitates mutual enhancement between high-dimensional\ntask features and low-dimensional fusion features, ensuring a cohesive and\nefficient feature integration process. Experimental results on three benchmarks\ndemonstrate that our method significantly outperforms state-of-the-art\napproaches, achieving an average improvement of 9.32% in visual quality while\nalso enhancing subsequent high-level tasks. The code is available at\nhttps://github.com/Beate-Suy-Zhang/DCEvo."}
{"id": "2503.18008", "pdf": "https://arxiv.org/pdf/2503.18008", "abs": "https://arxiv.org/abs/2503.18008", "authors": ["Kyuyoung Kim", "Jinwoo Shin", "Jaehyung Kim"], "title": "Personalized Language Models via Privacy-Preserving Evolutionary Model Merging", "categories": ["cs.CL", "cs.NE"], "comment": "Preprint", "summary": "Personalization in large language models (LLMs) seeks to tailor models to\nindividual user or user group preferences. Prompt-based methods augment queries\nwith user preference information, whereas training-based methods directly\nencode preferences into model parameters for more effective personalization.\nDespite achieving some success in personalizing LLMs, prior methods often fail\nto directly optimize task-specific metrics and lack explicit\nprivacy-preservation mechanisms. To address these limitations, we propose\nPrivacy-Preserving Model Merging via Evolutionary Algorithms (PriME), a novel\napproach to personalization that employs gradient-free methods to directly\noptimize task-specific metrics while preserving user privacy. By incorporating\nprivacy preservation into optimization, PriME produces a personalized module\nthat effectively captures the target user's preferences while minimizing the\nprivacy risks for the users sharing their private information. Experiments on\nthe LaMP benchmark show that PriME outperforms both prompt-based and\ntraining-based methods, achieving up to a 45% performance improvement over the\nprior art. Further analysis shows that PriME achieves a significantly better\nprivacy-utility trade-off, highlighting the potential of evolutionary\napproaches for privacy-preserving LLM personalization."}
{"id": "2503.17675", "pdf": "https://arxiv.org/pdf/2503.17675", "abs": "https://arxiv.org/abs/2503.17675", "authors": ["Shulei Wang", "Wang Lin", "Hai Huang", "Hanting Wang", "Sihang Cai", "WenKang Han", "Tao Jin", "Jingyuan Chen", "Jiacheng Sun", "Jieming Zhu", "Zhou Zhao"], "title": "Towards Transformer-Based Aligned Generation with Self-Coherence Guidance", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025", "summary": "We introduce a novel, training-free approach for enhancing alignment in\nTransformer-based Text-Guided Diffusion Models (TGDMs). Existing TGDMs often\nstruggle to generate semantically aligned images, particularly when dealing\nwith complex text prompts or multi-concept attribute binding challenges.\nPrevious U-Net-based methods primarily optimized the latent space, but their\ndirect application to Transformer-based architectures has shown limited\neffectiveness. Our method addresses these challenges by directly optimizing\ncross-attention maps during the generation process. Specifically, we introduce\nSelf-Coherence Guidance, a method that dynamically refines attention maps using\nmasks derived from previous denoising steps, ensuring precise alignment without\nadditional training. To validate our approach, we constructed more challenging\nbenchmarks for evaluating coarse-grained attribute binding, fine-grained\nattribute binding, and style binding. Experimental results demonstrate the\nsuperior performance of our method, significantly surpassing other\nstate-of-the-art methods across all evaluated tasks. Our code is available at\nhttps://scg-diffusion.github.io/scg-diffusion."}
{"id": "2503.18062", "pdf": "https://arxiv.org/pdf/2503.18062", "abs": "https://arxiv.org/abs/2503.18062", "authors": ["Anh Duc Nguyen", "Hieu Minh Phi", "Anh Viet Ngo", "Long Hai Trieu", "Thai Phuong Nguyen"], "title": "Investigating Recent Large Language Models for Vietnamese Machine Reading Comprehension", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have shown remarkable proficiency in Machine\nReading Comprehension (MRC) tasks; however, their effectiveness for\nlow-resource languages like Vietnamese remains largely unexplored. In this\npaper, we fine-tune and evaluate two state-of-the-art LLMs: Llama 3 (8B\nparameters) and Gemma (7B parameters), on ViMMRC, a Vietnamese MRC dataset. By\nutilizing Quantized Low-Rank Adaptation (QLoRA), we efficiently fine-tune these\nmodels and compare their performance against powerful LLM-based baselines.\nAlthough our fine-tuned models are smaller than GPT-3 and GPT-3.5, they\noutperform both traditional BERT-based approaches and these larger models. This\ndemonstrates the effectiveness of our fine-tuning process, showcasing how\nmodern LLMs can surpass the capabilities of older models like BERT while still\nbeing suitable for deployment in resource-constrained environments. Through\nintensive analyses, we explore various aspects of model performance, providing\nvaluable insights into adapting LLMs for low-resource languages like\nVietnamese. Our study contributes to the advancement of natural language\nprocessing in low-resource languages, and we make our fine-tuned models\npublicly available at: https://huggingface.co/iaiuet."}
{"id": "2503.17690", "pdf": "https://arxiv.org/pdf/2503.17690", "abs": "https://arxiv.org/abs/2503.17690", "authors": ["Ziyu Yao", "Xuxin Cheng", "Zhiqi Huang", "Lei Li"], "title": "CountLLM: Towards Generalizable Repetitive Action Counting via Large Language Model", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025", "summary": "Repetitive action counting, which aims to count periodic movements in a\nvideo, is valuable for video analysis applications such as fitness monitoring.\nHowever, existing methods largely rely on regression networks with limited\nrepresentational capacity, which hampers their ability to accurately capture\nvariable periodic patterns. Additionally, their supervised learning on narrow,\nlimited training sets leads to overfitting and restricts their ability to\ngeneralize across diverse scenarios. To address these challenges, we propose\nCountLLM, the first large language model (LLM)-based framework that takes video\ndata and periodic text prompts as inputs and outputs the desired counting\nvalue. CountLLM leverages the rich clues from explicit textual instructions and\nthe powerful representational capabilities of pre-trained LLMs for repetitive\naction counting. To effectively guide CountLLM, we develop a periodicity-based\nstructured template for instructions that describes the properties of\nperiodicity and implements a standardized answer format to ensure consistency.\nAdditionally, we propose a progressive multimodal training paradigm to enhance\nthe periodicity-awareness of the LLM. Empirical evaluations on widely\nrecognized benchmarks demonstrate CountLLM's superior performance and\ngeneralization, particularly in handling novel and out-of-domain actions that\ndeviate significantly from the training data, offering a promising avenue for\nrepetitive action counting."}
{"id": "2503.18063", "pdf": "https://arxiv.org/pdf/2503.18063", "abs": "https://arxiv.org/abs/2503.18063", "authors": ["Pieyi Zhang", "Richong Zhang", "Zhijie Nie"], "title": "Dynamic Task Vector Grouping for Efficient Multi-Task Prompt Tuning", "categories": ["cs.CL", "cs.AI"], "comment": "Work in progress", "summary": "Multi-task prompt tuning utilizes multiple high-resource source tasks to\nimprove performance on low-source target tasks. Existing approaches transfer\nthe soft prompt trained by combining all source tasks or a single\n``high-similar'' source task one-time-only. However, we find that the optimal\ntransfer performance often comes from a combination of source tasks, which is\nneither one nor all. Further, we find that the similarity between source and\ntarget tasks also changes dynamically during fine-tuning after transfering,\nmaking similarity calculation in the initiation stage inadequate. To address\nthese issues, we propose a method called Dynamic Task Vector Grouping (DTVG),\nwhose core ideas contain (1) measuring the task similarity with task vectors\ninstead of soft prompt, (2) grouping the optimal source task combination based\non two metrics: {\\it target similarity} and {\\it knowledge consistency}; (3)\ndynamically updating the combination in each iteration step. Extensive\nexperiments on the 26 NLP datasets under different settings demonstrate that\nDTVG effectively groups similar source tasks while reducing negative transfer,\nachieving the start-of-art performance."}
{"id": "2503.17695", "pdf": "https://arxiv.org/pdf/2503.17695", "abs": "https://arxiv.org/abs/2503.17695", "authors": ["Yikun Ma", "Yiqing Li", "Jiawei Wu", "Zhi Jin"], "title": "MotionDiff: Training-free Zero-shot Interactive Motion Editing via Flow-assisted Multi-view Diffusion", "categories": ["cs.CV"], "comment": null, "summary": "Generative models have made remarkable advancements and are capable of\nproducing high-quality content. However, performing controllable editing with\ngenerative models remains challenging, due to their inherent uncertainty in\noutputs. This challenge is praticularly pronounced in motion editing, which\ninvolves the processing of spatial information. While some physics-based\ngenerative methods have attempted to implement motion editing, they typically\noperate on single-view images with simple motions, such as translation and\ndragging. These methods struggle to handle complex rotation and stretching\nmotions and ensure multi-view consistency, often necessitating\nresource-intensive retraining. To address these challenges, we propose\nMotionDiff, a training-free zero-shot diffusion method that leverages optical\nflow for complex multi-view motion editing. Specifically, given a static scene,\nusers can interactively select objects of interest to add motion priors. The\nproposed Point Kinematic Model (PKM) then estimates corresponding multi-view\noptical flows during the Multi-view Flow Estimation Stage (MFES). Subsequently,\nthese optical flows are utilized to generate multi-view motion results through\ndecoupled motion representation in the Multi-view Motion Diffusion Stage\n(MMDS). Extensive experiments demonstrate that MotionDiff outperforms other\nphysics-based generative motion editing methods in achieving high-quality\nmulti-view consistent motion results. Notably, MotionDiff does not require\nretraining, enabling users to conveniently adapt it for various down-stream\ntasks."}
{"id": "2503.18069", "pdf": "https://arxiv.org/pdf/2503.18069", "abs": "https://arxiv.org/abs/2503.18069", "authors": ["Si Shen", "Fei Huang", "Zhixiao Zhao", "Chang Liu", "Tiansheng Zheng", "Danhao Zhu"], "title": "Long Is More Important Than Difficult for Training Reasoning Models", "categories": ["cs.CL"], "comment": "15 pages,6 figures", "summary": "Difficult problems, which often result in long reasoning traces, are widely\nrecognized as key factors for enhancing the performance of reasoning models.\nHowever, such high-challenge problems are scarce, limiting the size of\navailable datasets. In this paper, we propose a simple method to decouple the\nreliance on problem difficulty. First, we empirically demonstrate that\nreasoning length, rather than problem difficulty, primarily influences the\nperformance of trained models. Second, we identify a scaling law on reasoning\nlength, showing that model performance increases in a log-linear fashion as the\nreasoning data length grows. Finally, we introduce a straightforward technique\nto generate reasoning data of arbitrary length, and show that synthesized data\nis effective for training reasoning models. After fine-tuning the\nQwen2.5-32B-Instruct language model on our Long1K dataset, we present our\nmodel, Long1K-32B, which achieves remarkable performance with only 1,000\ntraining samples, achieving 95.6\\% accuracy on MATH, and 71.1\\% on GPQA\noutperforming DeepSeek-R1-Distill-Qwen-32B. The model, code, and dataset are\nall open-sourced, available at https://huggingface.co/ZTss/LONG1."}
{"id": "2503.17699", "pdf": "https://arxiv.org/pdf/2503.17699", "abs": "https://arxiv.org/abs/2503.17699", "authors": ["Haolin Qin", "Tingfa Xu", "Tianhao Li", "Zhenxiang Chen", "Tao Feng", "Jianan Li"], "title": "MUST: The First Dataset and Unified Framework for Multispectral UAV Single Object Tracking", "categories": ["cs.CV"], "comment": "CVPR2025", "summary": "UAV tracking faces significant challenges in real-world scenarios, such as\nsmall-size targets and occlusions, which limit the performance of RGB-based\ntrackers. Multispectral images (MSI), which capture additional spectral\ninformation, offer a promising solution to these challenges. However, progress\nin this field has been hindered by the lack of relevant datasets. To address\nthis gap, we introduce the first large-scale Multispectral UAV Single Object\nTracking dataset (MUST), which includes 250 video sequences spanning diverse\nenvironments and challenges, providing a comprehensive data foundation for\nmultispectral UAV tracking. We also propose a novel tracking framework,\nUNTrack, which encodes unified spectral, spatial, and temporal features from\nspectrum prompts, initial templates, and sequential searches. UNTrack employs\nan asymmetric transformer with a spectral background eliminate mechanism for\noptimal relationship modeling and an encoder that continuously updates the\nspectrum prompt to refine tracking, improving both accuracy and efficiency.\nExtensive experiments show that our proposed UNTrack outperforms\nstate-of-the-art UAV trackers. We believe our dataset and framework will drive\nfuture research in this area. The dataset is available on\nhttps://github.com/q2479036243/MUST-Multispectral-UAV-Single-Object-Tracking."}
{"id": "2503.18071", "pdf": "https://arxiv.org/pdf/2503.18071", "abs": "https://arxiv.org/abs/2503.18071", "authors": ["Zhiyu Lin", "Yifei Gao", "Xian Zhao", "Yunfan Yang", "Jitao Sang"], "title": "Mind with Eyes: from Language Reasoning to Multimodal Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "Language models have recently advanced into the realm of reasoning, yet it is\nthrough multimodal reasoning that we can fully unlock the potential to achieve\nmore comprehensive, human-like cognitive capabilities. This survey provides a\nsystematic overview of the recent multimodal reasoning approaches, categorizing\nthem into two levels: language-centric multimodal reasoning and collaborative\nmultimodal reasoning. The former encompasses one-pass visual perception and\nactive visual perception, where vision primarily serves a supporting role in\nlanguage reasoning. The latter involves action generation and state update\nwithin reasoning process, enabling a more dynamic interaction between\nmodalities. Furthermore, we analyze the technical evolution of these methods,\ndiscuss their inherent challenges, and introduce key benchmark tasks and\nevaluation metrics for assessing multimodal reasoning performance. Finally, we\nprovide insights into future research directions from the following two\nperspectives: (i) from visual-language reasoning to omnimodal reasoning and\n(ii) from multimodal reasoning to multimodal agents. This survey aims to\nprovide a structured overview that will inspire further advancements in\nmultimodal reasoning research."}
{"id": "2503.17700", "pdf": "https://arxiv.org/pdf/2503.17700", "abs": "https://arxiv.org/abs/2503.17700", "authors": ["Paul Hill", "Zhiming Liu", "Nantheera Anantrasirichai"], "title": "MAMAT: 3D Mamba-Based Atmospheric Turbulence Removal and its Object Detection Capability", "categories": ["cs.CV"], "comment": null, "summary": "Restoration and enhancement are essential for improving the quality of videos\ncaptured under atmospheric turbulence conditions, aiding visualization, object\ndetection, classification, and tracking in surveillance systems. In this paper,\nwe introduce a novel Mamba-based method, the 3D Mamba-Based Atmospheric\nTurbulence Removal (MAMAT), which employs a dual-module strategy to mitigate\nthese distortions. The first module utilizes deformable 3D convolutions for\nnon-rigid registration to minimize spatial shifts, while the second module\nenhances contrast and detail. Leveraging the advanced capabilities of the 3D\nMamba architecture, experimental results demonstrate that MAMAT outperforms\nstate-of-the-art learning-based methods, achieving up to a 3\\% improvement in\nvisual quality and a 15\\% boost in object detection. It not only enhances\nvisualization but also significantly improves object detection accuracy,\nbridging the gap between visual restoration and the effectiveness of\nsurveillance applications."}
{"id": "2503.18072", "pdf": "https://arxiv.org/pdf/2503.18072", "abs": "https://arxiv.org/abs/2503.18072", "authors": ["Germán Capdehourat", "Isabel Amigo", "Brian Lorenzo", "Joaquín Trigo"], "title": "On the effectiveness of LLMs for automatic grading of open-ended questions in Spanish", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Grading is a time-consuming and laborious task that educators must face. It\nis an important task since it provides feedback signals to learners, and it has\nbeen demonstrated that timely feedback improves the learning process. In recent\nyears, the irruption of LLMs has shed light on the effectiveness of automatic\ngrading. In this paper, we explore the performance of different LLMs and\nprompting techniques in automatically grading short-text answers to open-ended\nquestions. Unlike most of the literature, our study focuses on a use case where\nthe questions, answers, and prompts are all in Spanish. Experimental results\ncomparing automatic scores to those of human-expert evaluators show good\noutcomes in terms of accuracy, precision and consistency for advanced LLMs,\nboth open and proprietary. Results are notably sensitive to prompt styles,\nsuggesting biases toward certain words or content in the prompt. However, the\nbest combinations of models and prompt strategies, consistently surpasses an\naccuracy of 95% in a three-level grading task, which even rises up to more than\n98% when the it is simplified to a binary right or wrong rating problem, which\ndemonstrates the potential that LLMs have to implement this type of automation\nin education applications."}
{"id": "2503.17709", "pdf": "https://arxiv.org/pdf/2503.17709", "abs": "https://arxiv.org/abs/2503.17709", "authors": ["Yuchen Sun", "Shanhui Zhao", "Tao Yu", "Hao Wen", "Samith Va", "Mengwei Xu", "Yuanchun Li", "Chongyang Zhang"], "title": "GUI-Xplore: Empowering Generalizable GUI Agents with One Exploration", "categories": ["cs.CV", "cs.AI"], "comment": "CVPR 2025", "summary": "GUI agents hold significant potential to enhance the experience and\nefficiency of human-device interaction. However, current methods face\nchallenges in generalizing across applications (apps) and tasks, primarily due\nto two fundamental limitations in existing datasets. First, these datasets\noverlook developer-induced structural variations among apps, limiting the\ntransferability of knowledge across diverse software environments. Second, many\nof them focus solely on navigation tasks, which restricts their capacity to\nrepresent comprehensive software architectures and complex user interactions.\nTo address these challenges, we introduce GUI-Xplore, a dataset meticulously\ndesigned to enhance cross-application and cross-task generalization via an\nexploration-and-reasoning framework. GUI-Xplore integrates pre-recorded\nexploration videos providing contextual insights, alongside five hierarchically\nstructured downstream tasks designed to comprehensively evaluate GUI agent\ncapabilities. To fully exploit GUI-Xplore's unique features, we propose\nXplore-Agent, a GUI agent framework that combines Action-aware GUI Modeling\nwith Graph-Guided Environment Reasoning. Further experiments indicate that\nXplore-Agent achieves a 10% improvement over existing methods in unfamiliar\nenvironments, yet there remains significant potential for further enhancement\ntowards truly generalizable GUI agents."}
{"id": "2503.18076", "pdf": "https://arxiv.org/pdf/2503.18076", "abs": "https://arxiv.org/abs/2503.18076", "authors": ["Somnath Roy", "Padharthi Sreekar", "Srivatsa Narasimha", "Anubhav Anand"], "title": "A Multi-Model Adaptation of Speculative Decoding for Classification", "categories": ["cs.CL"], "comment": null, "summary": "The current study introduces a novel adaptation of speculative decoding,\nrepurposed from generation to classification tasks. We propose a multi-model\nframework employing up to three lightweight worker models and a single, more\nrobust judge model analogous to draft models and target model, respectively, in\nspeculative decoding. The worker models, tasked with the bulk of the\ncomputation, independently predict discrete class labels for a given input.\nWhen majority worker models agree on a label, it is accepted as the final\nlabel, optimizing efficiency by bypassing the computationally expensive judge\nmodel. In cases of disagreement, the judge model intervenes to resolve the\nlabel. This approach minimizes redundant computation, leverages the redundancy\nof multiple workers for confidence, and confines the judge model's role to\nchallenging cases, offering a practical balance of efficiency and accuracy. Our\nanalysis suggests that smaller out of the box instruction/chat finetuned worker\nmodels with 3 billion parameters (hereafter, 3B) demonstrate a level of\nalignment with judge models comparable to that of larger finetuned worker\nmodels with 7 billion parameters (hereafter, 7B) across both simple and higher\norder reasoning tasks. The top performing 3B worker model pair achieve an\nagreement rate of approximately 80-83% for sentiment and around 50-80% for\nsimilar ticket when compared to judge models. Additionally, 3B worker models\nprovide a speedup ranging from 2.8x to 9x relative to the judge models, while\n7B worker model combinations achieve a speedup ranging from 1.28x to 0.28x"}
{"id": "2503.17712", "pdf": "https://arxiv.org/pdf/2503.17712", "abs": "https://arxiv.org/abs/2503.17712", "authors": ["Heng Gao", "Zhuolin He", "Shoumeng Qiu", "Xiangyang Xue", "Jian Pu"], "title": "Multi-modality Anomaly Segmentation on the Road", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Semantic segmentation allows autonomous driving cars to understand the\nsurroundings of the vehicle comprehensively. However, it is also crucial for\nthe model to detect obstacles that may jeopardize the safety of autonomous\ndriving systems. Based on our experiments, we find that current uni-modal\nanomaly segmentation frameworks tend to produce high anomaly scores for\nnon-anomalous regions in images. Motivated by this empirical finding, we\ndevelop a multi-modal uncertainty-based anomaly segmentation framework, named\nMMRAS+, for autonomous driving systems. MMRAS+ effectively reduces the high\nanomaly outputs of non-anomalous classes by introducing text-modal using the\nCLIP text encoder. Indeed, MMRAS+ is the first multi-modal anomaly segmentation\nsolution for autonomous driving. Moreover, we develop an ensemble module to\nfurther boost the anomaly segmentation performance. Experiments on RoadAnomaly,\nSMIYC, and Fishyscapes validation datasets demonstrate the superior performance\nof our method. The code is available in\nhttps://github.com/HengGao12/MMRAS_plus."}
{"id": "2503.18085", "pdf": "https://arxiv.org/pdf/2503.18085", "abs": "https://arxiv.org/abs/2503.18085", "authors": ["Rochana Chaturvedi", "Peyman Baghershahi", "Sourav Medya", "Barbara Di Eugenio"], "title": "Temporal Relation Extraction in Clinical Texts: A Span-based Graph Transformer Approach", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Introducing a novel method for joint extraction of medical events and\n  temporal relations from free-text, leveraging clinical LPLMs and\n  Heterogeneous Graph Transformers, achieving a 5.5% improvement over the\n  previous state-of-the-art and up to 8.9% on long-range relations", "summary": "Temporal information extraction from unstructured text is essential for\ncontextualizing events and deriving actionable insights, particularly in the\nmedical domain. We address the task of extracting clinical events and their\ntemporal relations using the well-studied I2B2 2012 Temporal Relations\nChallenge corpus. This task is inherently challenging due to complex clinical\nlanguage, long documents, and sparse annotations. We introduce GRAPHTREX, a\nnovel method integrating span-based entity-relation extraction, clinical large\npre-trained language models (LPLMs), and Heterogeneous Graph Transformers (HGT)\nto capture local and global dependencies. Our HGT component facilitates\ninformation propagation across the document through innovative global landmarks\nthat bridge distant entities. Our method improves the state-of-the-art with\n5.5% improvement in the tempeval $F_1$ score over the previous best and up to\n8.9% improvement on long-range relations, which presents a formidable\nchallenge. This work not only advances temporal information extraction but also\nlays the groundwork for improved diagnostic and prognostic models through\nenhanced temporal reasoning."}
{"id": "2503.17715", "pdf": "https://arxiv.org/pdf/2503.17715", "abs": "https://arxiv.org/abs/2503.17715", "authors": ["Abtin Pourhadi", "Paul Swoboda"], "title": "Normalized Matching Transformer", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "We present a new state of the art approach for sparse keypoint matching\nbetween pairs of images. Our method consists of a fully deep learning based\napproach combining a visual backbone coupled with a SplineCNN graph neural\nnetwork for feature processing and a normalized transformer decoder for\ndecoding keypoint correspondences together with the Sinkhorn algorithm. Our\nmethod is trained using a contrastive and a hyperspherical loss for better\nfeature representations. We additionally use data augmentation during training.\nThis comparatively simple architecture combining extensive normalization and\nadvanced losses outperforms current state of the art approaches on PascalVOC\nand SPair-71k datasets by $5.1\\%$ and $2.2\\%$ respectively compared to BBGM,\nASAR, COMMON and GMTR while training for at least $1.7x$ fewer epochs."}
{"id": "2503.18089", "pdf": "https://arxiv.org/pdf/2503.18089", "abs": "https://arxiv.org/abs/2503.18089", "authors": ["Javad SeraJ", "Mohammad Mahdi Mohajeri", "Mohammad Javad Dousti"], "title": "$D^2LoRA$: Data-Driven LoRA Initialization for Low Resource Tasks", "categories": ["cs.CL"], "comment": null, "summary": "Tuning large language models is essential for optimizing their performance\nacross diverse applications, particularly in scenarios with limited data\navailability. Tuning large language models in scarce data scenarios is crucial,\nparticularly given that the convergence speed of the LoRA method is lower than\nthat of full fine-tuning. In this paper, we present an analysis of\npost-training methods including Supervised Fine-Tuning (SFT), Direct Preference\nOptimization (DPO), and Odds Ratio Preference Optimization (ORPO) within the\ncontext of task-specific learning using the LoRA method. Next we introduce\n$D^2LoRA$, a data-driven approach for initializing LoRA metrics that enhances\ntraining efficiency, especially in limited-data settings. Our experiments\ncompare $D^2LoRA$ with vanilla LoRA in terms of performance and catastrophic\nforgetting under extremely data-constrained conditions. The results demonstrate\nthat $D^2LoRA$ achieves a 1% improvement GSM8K benchmark and a 2-point\nimprovement in ROUGE score in title generation tasks. $D^2LoRA$ facilitates the\nadaptation of LLMs to multiple tasks even when task-specific data is scarce,\nthereby reducing training expenses and offering data cost."}
{"id": "2503.17716", "pdf": "https://arxiv.org/pdf/2503.17716", "abs": "https://arxiv.org/abs/2503.17716", "authors": ["Tim Alpherts", "Sennay Ghebreab", "Nanne van Noord"], "title": "EMPLACE: Self-Supervised Urban Scene Change Detection", "categories": ["cs.CV"], "comment": "7 pages, 7 figures, published at AAAI 2025", "summary": "Urban change is a constant process that influences the perception of\nneighbourhoods and the lives of the people within them. The field of Urban\nScene Change Detection (USCD) aims to capture changes in street scenes using\ncomputer vision and can help raise awareness of changes that make it possible\nto better understand the city and its residents. Traditionally, the field of\nUSCD has used supervised methods with small scale datasets. This constrains\nmethods when applied to new cities, as it requires labour-intensive labeling\nprocesses and forces a priori definitions of relevant change. In this paper we\nintroduce AC-1M the largest USCD dataset by far of over 1.1M images, together\nwith EMPLACE, a self-supervising method to train a Vision Transformer using our\nadaptive triplet loss. We show EMPLACE outperforms SOTA methods both as a\npre-training method for linear fine-tuning as well as a zero-shot setting.\nLastly, in a case study of Amsterdam, we show that we are able to detect both\nsmall and large changes throughout the city and that changes uncovered by\nEMPLACE, depending on size, correlate with housing prices - which in turn is\nindicative of inequity."}
{"id": "2503.18095", "pdf": "https://arxiv.org/pdf/2503.18095", "abs": "https://arxiv.org/abs/2503.18095", "authors": ["Lorena G Barberia", "Belinda Lombard", "Norton Trevisan Roman", "Tatiane C. M. Sousa"], "title": "Clarifying Misconceptions in COVID-19 Vaccine Sentiment and Stance Analysis and Their Implications for Vaccine Hesitancy Mitigation: A Systematic Review", "categories": ["cs.CL", "cs.CY", "cs.LG", "I.2.7"], "comment": "14 pages, 3 figures, 4 tables", "summary": "Background Advances in machine learning (ML) models have increased the\ncapability of researchers to detect vaccine hesitancy in social media using\nNatural Language Processing (NLP). A considerable volume of research has\nidentified the persistence of COVID-19 vaccine hesitancy in discourse shared on\nvarious social media platforms. Methods Our objective in this study was to\nconduct a systematic review of research employing sentiment analysis or stance\ndetection to study discourse towards COVID-19 vaccines and vaccination spread\non Twitter (officially known as X since 2023). Following registration in the\nPROSPERO international registry of systematic reviews, we searched papers\npublished from 1 January 2020 to 31 December 2023 that used supervised machine\nlearning to assess COVID-19 vaccine hesitancy through stance detection or\nsentiment analysis on Twitter. We categorized the studies according to a\ntaxonomy of five dimensions: tweet sample selection approach, self-reported\nstudy type, classification typology, annotation codebook definitions, and\ninterpretation of results. We analyzed if studies using stance detection report\ndifferent hesitancy trends than those using sentiment analysis by examining how\nCOVID-19 vaccine hesitancy is measured, and whether efforts were made to avoid\nmeasurement bias. Results Our review found that measurement bias is widely\nprevalent in studies employing supervised machine learning to analyze sentiment\nand stance toward COVID-19 vaccines and vaccination. The reporting errors are\nsufficiently serious that they hinder the generalisability and interpretation\nof these studies to understanding whether individual opinions communicate\nreluctance to vaccinate against SARS-CoV-2. Conclusion Improving the reporting\nof NLP methods is crucial to addressing knowledge gaps in vaccine hesitancy\ndiscourse."}
{"id": "2503.17717", "pdf": "https://arxiv.org/pdf/2503.17717", "abs": "https://arxiv.org/abs/2503.17717", "authors": ["Yu Wang", "Junxian Mu", "Hongzhi Huang", "Qilong Wang", "Pengfei Zhu", "Qinghua Hu"], "title": "BackMix: Regularizing Open Set Recognition by Removing Underlying Fore-Background Priors", "categories": ["cs.CV"], "comment": "20 pages, 11 figures. Accepted by TPAMI", "summary": "Open set recognition (OSR) requires models to classify known samples while\ndetecting unknown samples for real-world applications. Existing studies show\nimpressive progress using unknown samples from auxiliary datasets to regularize\nOSR models, but they have proved to be sensitive to selecting such known\noutliers. In this paper, we discuss the aforementioned problem from a new\nperspective: Can we regularize OSR models without elaborately selecting\nauxiliary known outliers? We first empirically and theoretically explore the\nrole of foregrounds and backgrounds in open set recognition and disclose that:\n1) backgrounds that correlate with foregrounds would mislead the model and\ncause failures when encounters 'partially' known images; 2) Backgrounds\nunrelated to foregrounds can serve as auxiliary known outliers and provide\nregularization via global average pooling. Based on the above insights, we\npropose a new method, Background Mix (BackMix), that mixes the foreground of an\nimage with different backgrounds to remove the underlying fore-background\npriors. Specifically, BackMix first estimates the foreground with class\nactivation maps (CAMs), then randomly replaces image patches with backgrounds\nfrom other images to obtain mixed images for training. With backgrounds\nde-correlated from foregrounds, the open set recognition performance is\nsignificantly improved. The proposed method is quite simple to implement,\nrequires no extra operation for inferences, and can be seamlessly integrated\ninto almost all of the existing frameworks. The code is released on\nhttps://github.com/Vanixxz/BackMix."}
{"id": "2503.18117", "pdf": "https://arxiv.org/pdf/2503.18117", "abs": "https://arxiv.org/abs/2503.18117", "authors": ["Muhidin A. Mohamed", "Shuab D. Ahmed", "Yahye A. Isse", "Hanad M. Mohamed", "Fuad M. Hassan", "Houssein A. Assowe"], "title": "Detection of Somali-written Fake News and Toxic Messages on the Social Media Using Transformer-based Language Models", "categories": ["cs.CL"], "comment": null, "summary": "The fact that everyone with a social media account can create and share\ncontent, and the increasing public reliance on social media platforms as a news\nand information source bring about significant challenges such as\nmisinformation, fake news, harmful content, etc. Although human content\nmoderation may be useful to an extent and used by these platforms to flag\nposted materials, the use of AI models provides a more sustainable, scalable,\nand effective way to mitigate these harmful contents. However, low-resourced\nlanguages such as the Somali language face limitations in AI automation,\nincluding scarce annotated training datasets and lack of language models\ntailored to their unique linguistic characteristics. This paper presents part\nof our ongoing research work to bridge some of these gaps for the Somali\nlanguage. In particular, we created two human-annotated social-media-sourced\nSomali datasets for two downstream applications, fake news \\& toxicity\nclassification, and developed a transformer-based monolingual Somali language\nmodel (named SomBERTa) -- the first of its kind to the best of our knowledge.\nSomBERTa is then fine-tuned and evaluated on toxic content, fake news and news\ntopic classification datasets. Comparative evaluation analysis of the proposed\nmodel against related multilingual models (e.g., AfriBERTa, AfroXLMR, etc)\ndemonstrated that SomBERTa consistently outperformed these comparators in both\nfake news and toxic content classification tasks while achieving the best\naverage accuracy (87.99%) across all tasks. This research contributes to Somali\nNLP by offering a foundational language model and a replicable framework for\nother low-resource languages, promoting digital and AI inclusivity and\nlinguistic diversity."}
{"id": "2503.17724", "pdf": "https://arxiv.org/pdf/2503.17724", "abs": "https://arxiv.org/abs/2503.17724", "authors": ["Jie Zhang", "Zhongqi Wang", "Shiguang Shan", "Xilin Chen"], "title": "Towards Invisible Backdoor Attack on Text-to-Image Diffusion Model", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Backdoor attacks targeting text-to-image diffusion models have advanced\nrapidly, enabling attackers to implant malicious triggers into these models to\nmanipulate their outputs. However, current backdoor samples often exhibit two\nkey abnormalities compared to benign samples: 1) Semantic Consistency, where\nbackdoor prompts tend to generate images with similar semantic content even\nwith significant textual variations to the prompts; 2) Attention Consistency,\nwhere the trigger induces consistent structural responses in the\ncross-attention maps. These consistencies leave detectable traces for\ndefenders, making backdoors easier to identify. To enhance the stealthiness of\nbackdoor samples, we propose a novel Invisible Backdoor Attack (IBA) by\nexplicitly mitigating these consistencies. Specifically, our approach leverages\nsyntactic structures as backdoor triggers to amplify the sensitivity to textual\nvariations, effectively breaking down the semantic consistency. Besides, a\nregularization method based on Kernel Maximum Mean Discrepancy (KMMD) is\nproposed to align the distribution of cross-attention responses between\nbackdoor and benign samples, thereby disrupting attention consistency.\nExtensive experiments demonstrate that our IBA achieves a 97.5% attack success\nrate while exhibiting stronger resistance to defenses, with an average of over\n98% backdoor samples bypassing three state-of-the-art detection mechanisms. The\ncode is available at https://github.com/Robin-WZQ/IBA."}
{"id": "2503.18129", "pdf": "https://arxiv.org/pdf/2503.18129", "abs": "https://arxiv.org/abs/2503.18129", "authors": ["Varvara Krechetova", "Denis Kochedykov"], "title": "GeoBenchX: Benchmarking LLMs for Multistep Geospatial Tasks", "categories": ["cs.CL", "cs.AI"], "comment": "Github with code and benchmark set:\n  https://github.com/Solirinai/GeoBenchX", "summary": "In this paper, we establish a benchmark for evaluating large language models\n(LLMs) on multi-step geospatial tasks relevant to commercial GIS practitioners.\nWe assess seven leading commercial LLMs (Sonnet 3.5 and 3.7, Haiku 3.5, Gemini\n2.0, GPT-4o, GPT-4o mini, and o3-mini) using a simple tool-calling agent\nequipped with 23 geospatial functions. Our benchmark comprises tasks across\nfour categories of increasing complexity, with both solvable and intentionally\nunsolvable tasks to test hallucination rejection. We develop an LLM-as-Judge\nevaluation framework to compare agent solutions against reference\nimplementations. Results show Sonnet 3.5 and GPT-4o achieve the best overall\nperformance, with Claude models excelling on solvable tasks while OpenAI models\nbetter identify unsolvable scenarios. We observe significant differences in\ntoken usage, with Anthropic models consuming substantially more tokens than\ncompetitors. Common errors include misunderstanding geometrical relationships,\nrelying on outdated knowledge, and inefficient data manipulation. The resulting\nbenchmark set, evaluation framework, and data generation pipeline are released\nas open-source resources, providing one more standardized method for ongoing\nevaluation of LLMs for GeoAI."}
{"id": "2503.17728", "pdf": "https://arxiv.org/pdf/2503.17728", "abs": "https://arxiv.org/abs/2503.17728", "authors": ["Yongjin Choi", "Chanhun Park", "Seung Jun Baek"], "title": "DynASyn: Multi-Subject Personalization Enabling Dynamic Action Synthesis", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted at AAAI 2025", "summary": "Recent advances in text-to-image diffusion models spurred research on\npersonalization, i.e., a customized image synthesis, of subjects within\nreference images. Although existing personalization methods are able to alter\nthe subjects' positions or to personalize multiple subjects simultaneously,\nthey often struggle to modify the behaviors of subjects or their dynamic\ninteractions. The difficulty is attributable to overfitting to reference\nimages, which worsens if only a single reference image is available. We propose\nDynASyn, an effective multi-subject personalization from a single reference\nimage addressing these challenges. DynASyn preserves the subject identity in\nthe personalization process by aligning concept-based priors with subject\nappearances and actions. This is achieved by regularizing the attention maps\nbetween the subject token and images through concept-based priors. In addition,\nwe propose concept-based prompt-and-image augmentation for an enhanced\ntrade-off between identity preservation and action diversity. We adopt an\nSDE-based editing guided by augmented prompts to generate diverse appearances\nand actions while maintaining identity consistency in the augmented images.\nExperiments show that DynASyn is capable of synthesizing highly realistic\nimages of subjects with novel contexts and dynamic interactions with the\nsurroundings, and outperforms baseline methods in both quantitative and\nqualitative aspects."}
{"id": "2503.18132", "pdf": "https://arxiv.org/pdf/2503.18132", "abs": "https://arxiv.org/abs/2503.18132", "authors": ["Yibo Yan", "Shen Wang", "Jiahao Huo", "Philip S. Yu", "Xuming Hu", "Qingsong Wen"], "title": "MathAgent: Leveraging a Mixture-of-Math-Agent Framework for Real-World Multimodal Mathematical Error Detection", "categories": ["cs.CL"], "comment": "Work In Progress", "summary": "Mathematical error detection in educational settings presents a significant\nchallenge for Multimodal Large Language Models (MLLMs), requiring a\nsophisticated understanding of both visual and textual mathematical content\nalong with complex reasoning capabilities. Though effective in mathematical\nproblem-solving, MLLMs often struggle with the nuanced task of identifying and\ncategorizing student errors in multimodal mathematical contexts. Therefore, we\nintroduce MathAgent, a novel Mixture-of-Math-Agent framework designed\nspecifically to address these challenges. Our approach decomposes error\ndetection into three phases, each handled by a specialized agent: an image-text\nconsistency validator, a visual semantic interpreter, and an integrative error\nanalyzer. This architecture enables more accurate processing of mathematical\ncontent by explicitly modeling relationships between multimodal problems and\nstudent solution steps. We evaluate MathAgent on real-world educational data,\ndemonstrating approximately 5% higher accuracy in error step identification and\n3% improvement in error categorization compared to baseline models. Besides,\nMathAgent has been successfully deployed in an educational platform that has\nserved over one million K-12 students, achieving nearly 90% student\nsatisfaction while generating significant cost savings by reducing manual error\ndetection."}
{"id": "2503.17731", "pdf": "https://arxiv.org/pdf/2503.17731", "abs": "https://arxiv.org/abs/2503.17731", "authors": ["Sungphill Moon", "Hyeontae Son", "Dongcheol Hur", "Sangwook Kim"], "title": "Co-op: Correspondence-based Novel Object Pose Estimation", "categories": ["cs.CV"], "comment": "Accepted at CVPR 2025", "summary": "We propose Co-op, a novel method for accurately and robustly estimating the\n6DoF pose of objects unseen during training from a single RGB image. Our method\nrequires only the CAD model of the target object and can precisely estimate its\npose without any additional fine-tuning. While existing model-based methods\nsuffer from inefficiency due to using a large number of templates, our method\nenables fast and accurate estimation with a small number of templates. This\nimprovement is achieved by finding semi-dense correspondences between the input\nimage and the pre-rendered templates. Our method achieves strong generalization\nperformance by leveraging a hybrid representation that combines patch-level\nclassification and offset regression. Additionally, our pose refinement model\nestimates probabilistic flow between the input image and the rendered image,\nrefining the initial estimate to an accurate pose using a differentiable PnP\nlayer. We demonstrate that our method not only estimates object poses rapidly\nbut also outperforms existing methods by a large margin on the seven core\ndatasets of the BOP Challenge, achieving state-of-the-art accuracy."}
{"id": "2503.18167", "pdf": "https://arxiv.org/pdf/2503.18167", "abs": "https://arxiv.org/abs/2503.18167", "authors": ["Suman Adhya", "Avishek Lahiri", "Debarshi Kumar Sanyal", "Partha Pratim Das"], "title": "Evaluating Negative Sampling Approaches for Neural Topic Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Code is available at: https://github.com/AdhyaSuman/Eval_NegTM", "summary": "Negative sampling has emerged as an effective technique that enables deep\nlearning models to learn better representations by introducing the paradigm of\nlearn-to-compare. The goal of this approach is to add robustness to deep\nlearning models to learn better representation by comparing the positive\nsamples against the negative ones. Despite its numerous demonstrations in\nvarious areas of computer vision and natural language processing, a\ncomprehensive study of the effect of negative sampling in an unsupervised\ndomain like topic modeling has not been well explored. In this paper, we\npresent a comprehensive analysis of the impact of different negative sampling\nstrategies on neural topic models. We compare the performance of several\npopular neural topic models by incorporating a negative sampling technique in\nthe decoder of variational autoencoder-based neural topic models. Experiments\non four publicly available datasets demonstrate that integrating negative\nsampling into topic models results in significant enhancements across multiple\naspects, including improved topic coherence, richer topic diversity, and more\naccurate document classification. Manual evaluations also indicate that the\ninclusion of negative sampling into neural topic models enhances the quality of\nthe generated topics. These findings highlight the potential of negative\nsampling as a valuable tool for advancing the effectiveness of neural topic\nmodels."}
{"id": "2503.17736", "pdf": "https://arxiv.org/pdf/2503.17736", "abs": "https://arxiv.org/abs/2503.17736", "authors": ["Yiming Zhao", "Yu Zeng", "Yukun Qi", "YaoYang Liu", "Lin Chen", "Zehui Chen", "Xikun Bao", "Jie Zhao", "Feng Zhao"], "title": "V2P-Bench: Evaluating Video-Language Understanding with Visual Prompts for Better Human-Model Interaction", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Large Vision-Language Models (LVLMs) have made significant progress in the\nfield of video understanding recently. However, current benchmarks uniformly\nlean on text prompts for evaluation, which often necessitate complex\nreferential language and fail to provide precise spatial and temporal\nreferences. This limitation diminishes the experience and efficiency of\nhuman-model interaction. To address this limitation, we propose the Video\nVisual Prompt Benchmark(V2P-Bench), a comprehensive benchmark specifically\ndesigned to evaluate LVLMs' video understanding capabilities in multimodal\nhuman-model interaction scenarios. V2P-Bench includes 980 unique videos and\n1,172 QA pairs, covering 5 main tasks and 12 dimensions, facilitating\ninstance-level fine-grained understanding aligned with human cognition.\nBenchmarking results reveal that even the most powerful models perform poorly\non V2P-Bench (65.4% for GPT-4o and 67.9% for Gemini-1.5-Pro), significantly\nlower than the human experts' 88.3%, highlighting the current shortcomings of\nLVLMs in understanding video visual prompts. We hope V2P-Bench will serve as a\nfoundation for advancing multimodal human-model interaction and video\nunderstanding evaluation. Project page:\nhttps://github.com/gaotiexinqu/V2P-Bench."}
{"id": "2503.18172", "pdf": "https://arxiv.org/pdf/2503.18172", "abs": "https://arxiv.org/abs/2503.18172", "authors": ["Zixin Chen", "Sicheng Song", "Kashun Shum", "Yanna Lin", "Rui Sheng", "Huamin Qu"], "title": "Unmasking Deceptive Visuals: Benchmarking Multimodal Large Language Models on Misleading Chart Question Answering", "categories": ["cs.CL", "cs.AI"], "comment": "31 pages in total. Under Review For ARR", "summary": "Misleading chart visualizations, which intentionally manipulate data\nrepresentations to support specific claims, can distort perceptions and lead to\nincorrect conclusions. Despite decades of research, misleading visualizations\nremain a widespread and pressing issue. Recent advances in multimodal large\nlanguage models (MLLMs) have demonstrated strong chart comprehension\ncapabilities, yet no existing work has systematically evaluated their ability\nto detect and interpret misleading charts. This paper introduces the Misleading\nChart Question Answering (Misleading ChartQA) Benchmark, a large-scale\nmultimodal dataset designed to assess MLLMs in identifying and reasoning about\nmisleading charts. It contains over 3,000 curated examples, covering 21 types\nof misleaders and 10 chart types. Each example includes standardized chart\ncode, CSV data, and multiple-choice questions with labeled explanations,\nvalidated through multi-round MLLM checks and exhausted expert human review. We\nbenchmark 16 state-of-the-art MLLMs on our dataset, revealing their limitations\nin identifying visually deceptive practices. We also propose a novel pipeline\nthat detects and localizes misleaders, enhancing MLLMs' accuracy in misleading\nchart interpretation. Our work establishes a foundation for advancing\nMLLM-driven misleading chart comprehension. We publicly release the sample\ndataset to support further research in this critical area."}
{"id": "2503.17750", "pdf": "https://arxiv.org/pdf/2503.17750", "abs": "https://arxiv.org/abs/2503.17750", "authors": ["Houqiang Zhong", "Shaocheng Shen", "Ke Cai", "Zhenglong Wu", "Jiangchao Yao", "Yuan Cheng", "Xuefei Li", "Xiaoyun Zhang", "Li Song", "Qiang Hu"], "title": "Serial Low-rank Adaptation of Vision Transformer", "categories": ["cs.CV", "cs.MM"], "comment": null, "summary": "Fine-tuning large pre-trained vision foundation models in a\nparameter-efficient manner is critical for downstream vision tasks, considering\nthe practical constraints of computational and storage costs. Low-rank\nadaptation (LoRA) is a well-established technique in this domain, achieving\nimpressive efficiency by reducing the parameter space to a low-rank form.\nHowever, developing more advanced low-rank adaptation methods to reduce\nparameters and memory requirements remains a significant challenge in\nresource-constrained application scenarios. In this study, we consider on top\nof the commonly used vision transformer and propose Serial LoRA, a novel LoRA\nvariant that introduces a shared low-rank matrix serially composite with the\nattention mechanism. Such a design extracts the underlying commonality of\nparameters in adaptation, significantly reducing redundancy. Notably, Serial\nLoRA uses only 1/4 parameters of LoRA but achieves comparable performance in\nmost cases. We conduct extensive experiments on a range of vision foundation\nmodels with the transformer structure, and the results confirm consistent\nsuperiority of our method."}
{"id": "2503.18174", "pdf": "https://arxiv.org/pdf/2503.18174", "abs": "https://arxiv.org/abs/2503.18174", "authors": ["Weronika Łajewska", "Krisztian Balog"], "title": "GINGER: Grounded Information Nugget-Based Generation of Responses", "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Retrieval-augmented generation (RAG) faces challenges related to factual\ncorrectness, source attribution, and response completeness. To address them, we\npropose a modular pipeline for grounded response generation that operates on\ninformation nuggets-minimal, atomic units of relevant information extracted\nfrom retrieved documents. The multistage pipeline encompasses nugget detection,\nclustering, ranking, top cluster summarization, and fluency enhancement. It\nguarantees grounding in specific facts, facilitates source attribution, and\nensures maximum information inclusion within length constraints. Extensive\nexperiments on the TREC RAG'24 dataset evaluated with the AutoNuggetizer\nframework demonstrate that GINGER achieves state-of-the-art performance on this\nbenchmark."}
{"id": "2503.17752", "pdf": "https://arxiv.org/pdf/2503.17752", "abs": "https://arxiv.org/abs/2503.17752", "authors": ["R. D. Lin", "Pengcheng Weng", "Yinqiao Wang", "Han Ding", "Jinsong Han", "Fei Wang"], "title": "HiLoTs: High-Low Temporal Sensitive Representation Learning for Semi-Supervised LiDAR Segmentation in Autonomous Driving", "categories": ["cs.CV"], "comment": "accepted by CVPR 2025", "summary": "LiDAR point cloud semantic segmentation plays a crucial role in autonomous\ndriving. In recent years, semi-supervised methods have gained popularity due to\ntheir significant reduction in annotation labor and time costs. Current\nsemi-supervised methods typically focus on point cloud spatial distribution or\nconsider short-term temporal representations, e.g., only two adjacent frames,\noften overlooking the rich long-term temporal properties inherent in autonomous\ndriving scenarios. In driving experience, we observe that nearby objects, such\nas roads and vehicles, remain stable while driving, whereas distant objects\nexhibit greater variability in category and shape. This natural phenomenon is\nalso captured by LiDAR, which reflects lower temporal sensitivity for nearby\nobjects and higher sensitivity for distant ones. To leverage these\ncharacteristics, we propose HiLoTs, which learns high-temporal sensitivity and\nlow-temporal sensitivity representations from continuous LiDAR frames. These\nrepresentations are further enhanced and fused using a cross-attention\nmechanism. Additionally, we employ a teacher-student framework to align the\nrepresentations learned by the labeled and unlabeled branches, effectively\nutilizing the large amounts of unlabeled data. Experimental results on the\nSemanticKITTI and nuScenes datasets demonstrate that our proposed HiLoTs\noutperforms state-of-the-art semi-supervised methods, and achieves performance\nclose to LiDAR+Camera multimodal approaches. Code is available on\nhttps://github.com/rdlin118/HiLoTs"}
{"id": "2503.18182", "pdf": "https://arxiv.org/pdf/2503.18182", "abs": "https://arxiv.org/abs/2503.18182", "authors": ["Divya Patel", "Vansh Parikh", "Om Patel", "Agam Shah", "Bhaskar Chaudhury"], "title": "Exploring Topic Trends in COVID-19 Research Literature using Non-Negative Matrix Factorization", "categories": ["cs.CL"], "comment": null, "summary": "In this work, we apply topic modeling using Non-Negative Matrix Factorization\n(NMF) on the COVID-19 Open Research Dataset (CORD-19) to uncover the underlying\nthematic structure and its evolution within the extensive body of COVID-19\nresearch literature. NMF factorizes the document-term matrix into two\nnon-negative matrices, effectively representing the topics and their\ndistribution across the documents. This helps us see how strongly documents\nrelate to topics and how topics relate to words. We describe the complete\nmethodology which involves a series of rigorous pre-processing steps to\nstandardize the available text data while preserving the context of phrases,\nand subsequently feature extraction using the term frequency-inverse document\nfrequency (tf-idf), which assigns weights to words based on their frequency and\nrarity in the dataset. To ensure the robustness of our topic model, we conduct\na stability analysis. This process assesses the stability scores of the NMF\ntopic model for different numbers of topics, enabling us to select the optimal\nnumber of topics for our analysis. Through our analysis, we track the evolution\nof topics over time within the CORD-19 dataset. Our findings contribute to the\nunderstanding of the knowledge structure of the COVID-19 research landscape,\nproviding a valuable resource for future research in this field."}
{"id": "2503.17760", "pdf": "https://arxiv.org/pdf/2503.17760", "abs": "https://arxiv.org/abs/2503.17760", "authors": ["Zeyu Liu", "Zanlin Ni", "Yeguo Hua", "Xin Deng", "Xiao Ma", "Cheng Zhong", "Gao Huang"], "title": "CODA: Repurposing Continuous VAEs for Discrete Tokenization", "categories": ["cs.CV", "cs.AI"], "comment": "Project page: https://lzy-tony.github.io/coda", "summary": "Discrete visual tokenizers transform images into a sequence of tokens,\nenabling token-based visual generation akin to language models. However, this\nprocess is inherently challenging, as it requires both compressing visual\nsignals into a compact representation and discretizing them into a fixed set of\ncodes. Traditional discrete tokenizers typically learn the two tasks jointly,\noften leading to unstable training, low codebook utilization, and limited\nreconstruction quality. In this paper, we introduce\n\\textbf{CODA}(\\textbf{CO}ntinuous-to-\\textbf{D}iscrete \\textbf{A}daptation), a\nframework that decouples compression and discretization. Instead of training\ndiscrete tokenizers from scratch, CODA adapts off-the-shelf continuous VAEs --\nalready optimized for perceptual compression -- into discrete tokenizers via a\ncarefully designed discretization process. By primarily focusing on\ndiscretization, CODA ensures stable and efficient training while retaining the\nstrong visual fidelity of continuous VAEs. Empirically, with $\\mathbf{6\n\\times}$ less training budget than standard VQGAN, our approach achieves a\nremarkable codebook utilization of 100% and notable reconstruction FID (rFID)\nof $\\mathbf{0.43}$ and $\\mathbf{1.34}$ for $8 \\times$ and $16 \\times$\ncompression on ImageNet 256$\\times$ 256 benchmark."}
{"id": "2503.18212", "pdf": "https://arxiv.org/pdf/2503.18212", "abs": "https://arxiv.org/abs/2503.18212", "authors": ["Kanishka Parankusham", "Rodrigue Rizk", "KC Santosh"], "title": "LakotaBERT: A Transformer-based Model for Low Resource Lakota Language", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Lakota, a critically endangered language of the Sioux people in North\nAmerica, faces significant challenges due to declining fluency among younger\ngenerations. This paper introduces LakotaBERT, the first large language model\n(LLM) tailored for Lakota, aiming to support language revitalization efforts.\nOur research has two primary objectives: (1) to create a comprehensive Lakota\nlanguage corpus and (2) to develop a customized LLM for Lakota. We compiled a\ndiverse corpus of 105K sentences in Lakota, English, and parallel texts from\nvarious sources, such as books and websites, emphasizing the cultural\nsignificance and historical context of the Lakota language. Utilizing the\nRoBERTa architecture, we pre-trained our model and conducted comparative\nevaluations against established models such as RoBERTa, BERT, and multilingual\nBERT. Initial results demonstrate a masked language modeling accuracy of 51%\nwith a single ground truth assumption, showcasing performance comparable to\nthat of English-based models. We also evaluated the model using additional\nmetrics, such as precision and F1 score, to provide a comprehensive assessment\nof its capabilities. By integrating AI and linguistic methodologies, we aspire\nto enhance linguistic diversity and cultural resilience, setting a valuable\nprecedent for leveraging technology in the revitalization of other endangered\nindigenous languages."}
{"id": "2503.17782", "pdf": "https://arxiv.org/pdf/2503.17782", "abs": "https://arxiv.org/abs/2503.17782", "authors": ["Hyungyu Choi", "Young Kyun Jang", "Chanho Eom"], "title": "GOAL: Global-local Object Alignment Learning", "categories": ["cs.CV"], "comment": "16 pages, 5 figures", "summary": "Vision-language models like CLIP have shown impressive capabilities in\naligning images and text, but they often struggle with lengthy and detailed\ntext descriptions because of their training focus on short and concise\ncaptions. We present GOAL (Global-local Object Alignment Learning), a novel\nfine-tuning method that enhances CLIP's ability to handle lengthy text by\nleveraging both global and local semantic alignments between image and lengthy\ntext. Our approach consists of two key components: Local Image-Sentence\nMatching (LISM), which identifies corresponding pairs between image segments\nand descriptive sentences, and Token Similarity-based Learning (TSL), which\nefficiently propagates local element attention through these matched pairs.\nEvaluating GOAL on three new benchmarks for image-lengthy text retrieval, we\ndemonstrate significant improvements over baseline CLIP fine-tuning,\nestablishing a simple yet effective approach for adapting CLIP to detailed\ntextual descriptions. Through extensive experiments, we show that our method's\nfocus on local semantic alignment alongside global context leads to more\nnuanced and representative embeddings, particularly beneficial for tasks\nrequiring fine-grained understanding of lengthy text descriptions."}
{"id": "2503.18226", "pdf": "https://arxiv.org/pdf/2503.18226", "abs": "https://arxiv.org/abs/2503.18226", "authors": ["Venkatesh Bollineni", "Igor Crk", "Eren Gultepe"], "title": "Mapping Hymns and Organizing Concepts in the Rigveda: Quantitatively Connecting the Vedic Suktas", "categories": ["cs.CL"], "comment": "Accepted to NLP4DH 2025 at NAACL 2025", "summary": "Accessing and gaining insight into the Rigveda poses a non-trivial challenge\ndue to its extremely ancient Sanskrit language, poetic structure, and large\nvolume of text. By using NLP techniques, this study identified topics and\nsemantic connections of hymns within the Rigveda that were corroborated by\nseven well-known groupings of hymns. The 1,028 suktas (hymns) from the modern\nEnglish translation of the Rigveda by Jamison and Brereton were preprocessed\nand sukta-level embeddings were obtained using, i) a novel adaptation of LSA,\npresented herein, ii) SBERT, and iii) Doc2Vec embeddings. Following an UMAP\ndimension reduction of the vectors, the network of suktas was formed using\nk-nearest neighbours. Then, community detection of topics in the sukta networks\nwas performed with the Louvain, Leiden, and label propagation methods, whose\nstatistical significance of the formed topics were determined using an\nappropriate null distribution. Only the novel adaptation of LSA using the\nLeiden method, had detected sukta topic networks that were significant (z =\n2.726, p < .01) with a modularity score of 0.944. Of the seven famous sukta\ngroupings analyzed (e.g., creation, funeral, water, etc.) the LSA derived\nnetwork was successful in all seven cases, while Doc2Vec was not significant\nand failed to detect the relevant suktas. SBERT detected four of the famous\nsuktas as separate groups, but mistakenly combined three of them into a single\nmixed group. Also, the SBERT network was not statistically significant."}
{"id": "2503.17788", "pdf": "https://arxiv.org/pdf/2503.17788", "abs": "https://arxiv.org/abs/2503.17788", "authors": ["Gaoge Han", "Yongkang Cheng", "Zhe Chen", "Shaoli Huang", "Tongliang Liu"], "title": "Aligning Foundation Model Priors and Diffusion-Based Hand Interactions for Occlusion-Resistant Two-Hand Reconstruction", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Two-hand reconstruction from monocular images faces persistent challenges due\nto complex and dynamic hand postures and occlusions, causing significant\ndifficulty in achieving plausible interaction alignment. Existing approaches\nstruggle with such alignment issues, often resulting in misalignment and\npenetration artifacts. To tackle this, we propose a novel framework that\nattempts to precisely align hand poses and interactions by synergistically\nintegrating foundation model-driven 2D priors with diffusion-based interaction\nrefinement for occlusion-resistant two-hand reconstruction. First, we introduce\na Fusion Alignment Encoder that learns to align fused multimodal priors\nkeypoints, segmentation maps, and depth cues from foundation models during\ntraining. This provides robust structured guidance, further enabling efficient\ninference without foundation models at test time while maintaining high\nreconstruction accuracy. Second, we employ a two-hand diffusion model\nexplicitly trained to transform interpenetrated poses into plausible,\nnon-penetrated interactions, leveraging gradient-guided denoising to correct\nartifacts and ensure realistic spatial relations. Extensive evaluations\ndemonstrate that our method achieves state-of-the-art performance on\nInterHand2.6M, FreiHAND, and HIC datasets, significantly advancing occlusion\nhandling and interaction robustness."}
{"id": "2503.18242", "pdf": "https://arxiv.org/pdf/2503.18242", "abs": "https://arxiv.org/abs/2503.18242", "authors": ["Aneesh Vathul", "Daniel Lee", "Sheryl Chen", "Arthi Tasmia"], "title": "ShED-HD: A Shannon Entropy Distribution Framework for Lightweight Hallucination Detection on Edge Devices", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities on a\nbroad array of NLP tasks, but their tendency to produce\nhallucinations$\\unicode{x2013}$plausible-sounding but factually incorrect\ncontent$\\unicode{x2013}$poses severe challenges in high-stakes domains.\nExisting hallucination detection methods either bear the computational cost of\nmultiple inference passes or sacrifice accuracy for efficiency with single-pass\napproaches, neither of which is ideal in resource-constrained environments such\nas edge devices. We propose the Shannon Entropy Distribution Hallucination\nDetector (ShED-HD), a novel hallucination detection framework that bridges this\ngap by classifying sequence-level entropy patterns using a lightweight BiLSTM\narchitecture with single-headed attention. In contrast to prior approaches,\nShED-HD efficiently detects distinctive uncertainty patterns across entire\noutput sequences, preserving contextual awareness. Through in-depth evaluation\non three datasets (BioASQ, TriviaQA, and Jeopardy Questions), we show that\nShED-HD significantly outperforms other computationally efficient approaches in\nthe out-of-distribution setting, while achieving comparable performance in the\nin-distribution setting. ShED-HD facilitates hallucination detection that is\nlow-cost, accurate, and generalizable, improving the credibility of content\ngenerated by LLMs in resource-constrained environments where trustworthy AI\nfunctionality is crucial."}
{"id": "2503.17792", "pdf": "https://arxiv.org/pdf/2503.17792", "abs": "https://arxiv.org/abs/2503.17792", "authors": ["Lingyun Deng", "Litong Liu", "Dong Wang", "Xiao-Ping Wang"], "title": "Topology preserving Image segmentation using the iterative convolution-thresholding method", "categories": ["cs.CV"], "comment": "22 pages, 14 figures", "summary": "Variational models are widely used in image segmentation, with various models\ndesigned to address different types of images by optimizing specific objective\nfunctionals. However, traditional segmentation models primarily focus on the\nvisual attributes of the image, often neglecting the topological properties of\nthe target objects. This limitation can lead to segmentation results that\ndeviate from the ground truth, particularly in images with complex topological\nstructures. In this paper, we introduce a topology-preserving constraint into\nthe iterative convolution-thresholding method (ICTM), resulting in the\ntopology-preserving ICTM (TP-ICTM). Extensive experiments demonstrate that, by\nexplicitly preserving the topological properties of target objects-such as\nconnectivity-the proposed algorithm achieves enhanced accuracy and robustness,\nparticularly in images with intricate structures or noise."}
{"id": "2503.18247", "pdf": "https://arxiv.org/pdf/2503.18247", "abs": "https://arxiv.org/abs/2503.18247", "authors": ["Tadesse Destaw Belay", "Israel Abebe Azime", "Ibrahim Said Ahmad", "Idris Abdulmumin", "Abinew Ali Ayele", "Shamsuddeen Hassan Muhammad", "Seid Muhie Yimam"], "title": "AfroXLMR-Social: Adapting Pre-trained Language Models for African Languages Social Media Text", "categories": ["cs.CL"], "comment": null, "summary": "Pretrained Language Models (PLMs) built from various sources are the\nfoundation of today's NLP progress. Language representations learned by such\nmodels achieve strong performance across many tasks with datasets of varying\nsizes drawn from various sources. We explore a thorough analysis of domain and\ntask adaptive continual pretraining approaches for low-resource African\nlanguages and a promising result is shown for the evaluated tasks. We create\nAfriSocial, a corpus designed for domain adaptive finetuning that passes\nthrough quality pre-processing steps. Continual pretraining PLMs using\nAfriSocial as domain adaptive pretraining (DAPT) data, consistently improves\nperformance on fine-grained emotion classification task of 16 targeted\nlanguages from 1% to 28.27% macro F1 score. Likewise, using the task adaptive\npertaining (TAPT) approach, further finetuning with small unlabeled but similar\ntask data shows promising results. For example, unlabeled sentiment data\n(source) for fine-grained emotion classification task (target) improves the\nbase model results by an F1 score ranging from 0.55% to 15.11%. Combining the\ntwo methods, DAPT + TAPT, achieves also better results than base models. All\nthe resources will be available to improve low-resource NLP tasks, generally,\nas well as other similar domain tasks such as hate speech and sentiment tasks."}
{"id": "2503.17794", "pdf": "https://arxiv.org/pdf/2503.17794", "abs": "https://arxiv.org/abs/2503.17794", "authors": ["Ketan Suhaas Saichandran", "Xavier Thomas", "Prakhar Kaushik", "Deepti Ghadiyaram"], "title": "Progressive Prompt Detailing for Improved Alignment in Text-to-Image Generative Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Text-to-image generative models often struggle with long prompts detailing\ncomplex scenes, diverse objects with distinct visual characteristics and\nspatial relationships. In this work, we propose SCoPE (Scheduled interpolation\nof Coarse-to-fine Prompt Embeddings), a training-free method to improve\ntext-to-image alignment by progressively refining the input prompt in a\ncoarse-to-fine-grained manner. Given a detailed input prompt, we first\ndecompose it into multiple sub-prompts which evolve from describing broad scene\nlayout to highly intricate details. During inference, we interpolate between\nthese sub-prompts and thus progressively introduce finer-grained details into\nthe generated image. Our training-free plug-and-play approach significantly\nenhances prompt alignment, achieves an average improvement of up to +4% in\nVisual Question Answering (VQA) scores over the Stable Diffusion baselines on\n85% of the prompts from the GenAI-Bench dataset."}
{"id": "2503.18250", "pdf": "https://arxiv.org/pdf/2503.18250", "abs": "https://arxiv.org/abs/2503.18250", "authors": ["Jong Myoung Kim", "Young-Jun_Lee", "Ho-Jin Choi", "Sangkeun Jung"], "title": "PAD: Towards Efficient Data Generation for Transfer Learning Using Phrase Alignment", "categories": ["cs.CL"], "comment": "Preparing for conference", "summary": "Transfer learning leverages the abundance of English data to address the\nscarcity of resources in modeling non-English languages, such as Korean. In\nthis study, we explore the potential of Phrase Aligned Data (PAD) from\nstandardized Statistical Machine Translation (SMT) to enhance the efficiency of\ntransfer learning. Through extensive experiments, we demonstrate that PAD\nsynergizes effectively with the syntactic characteristics of the Korean\nlanguage, mitigating the weaknesses of SMT and significantly improving model\nperformance. Moreover, we reveal that PAD complements traditional data\nconstruction methods and enhances their effectiveness when combined. This\ninnovative approach not only boosts model performance but also suggests a\ncost-efficient solution for resource-scarce languages."}
{"id": "2503.17798", "pdf": "https://arxiv.org/pdf/2503.17798", "abs": "https://arxiv.org/abs/2503.17798", "authors": ["Zexu Huang", "Min Xu", "Stuart Perry"], "title": "GaussianFocus: Constrained Attention Focus for 3D Gaussian Splatting", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent developments in 3D reconstruction and neural rendering have\nsignificantly propelled the capabilities of photo-realistic 3D scene rendering\nacross various academic and industrial fields. The 3D Gaussian Splatting\ntechnique, alongside its derivatives, integrates the advantages of\nprimitive-based and volumetric representations to deliver top-tier rendering\nquality and efficiency. Despite these advancements, the method tends to\ngenerate excessive redundant noisy Gaussians overfitted to every training view,\nwhich degrades the rendering quality. Additionally, while 3D Gaussian Splatting\nexcels in small-scale and object-centric scenes, its application to larger\nscenes is hindered by constraints such as limited video memory, excessive\noptimization duration, and variable appearance across views. To address these\nchallenges, we introduce GaussianFocus, an innovative approach that\nincorporates a patch attention algorithm to refine rendering quality and\nimplements a Gaussian constraints strategy to minimize redundancy. Moreover, we\npropose a subdivision reconstruction strategy for large-scale scenes, dividing\nthem into smaller, manageable blocks for individual training. Our results\nindicate that GaussianFocus significantly reduces unnecessary Gaussians and\nenhances rendering quality, surpassing existing State-of-The-Art (SoTA)\nmethods. Furthermore, we demonstrate the capability of our approach to\neffectively manage and render large scenes, such as urban environments, whilst\nmaintaining high fidelity in the visual output."}
{"id": "2503.18253", "pdf": "https://arxiv.org/pdf/2503.18253", "abs": "https://arxiv.org/abs/2503.18253", "authors": ["Tadesse Destaw Belay", "Dawit Ketema Gete", "Abinew Ali Ayele", "Olga Kolesnikova", "Grigori Sidorov", "Seid Muhie Yimam"], "title": "Enhancing Multi-Label Emotion Analysis and Corresponding Intensities for Ethiopian Languages", "categories": ["cs.CL"], "comment": null, "summary": "In this digital world, people freely express their emotions using different\nsocial media platforms. As a result, modeling and integrating\nemotion-understanding models are vital for various human-computer interaction\ntasks such as decision-making, product and customer feedback analysis,\npolitical promotions, marketing research, and social media monitoring. As users\nexpress different emotions simultaneously in a single instance, annotating\nemotions in a multilabel setting such as the EthioEmo (Belay et al., 2025)\ndataset effectively captures this dynamic. Additionally, incorporating\nintensity, or the degree of emotion, is crucial, as emotions can significantly\ndiffer in their expressive strength and impact. This intensity is significant\nfor assessing whether further action is necessary in decision-making processes,\nespecially concerning negative emotions in applications such as healthcare and\nmental health studies. To enhance the EthioEmo dataset, we include annotations\nfor the intensity of each labeled emotion. Furthermore, we evaluate various\nstate-of-the-art encoder-only Pretrained Language Models (PLMs) and\ndecoder-only Large Language Models (LLMs) to provide comprehensive\nbenchmarking."}
{"id": "2503.17814", "pdf": "https://arxiv.org/pdf/2503.17814", "abs": "https://arxiv.org/abs/2503.17814", "authors": ["Wen Li", "Chen Liu", "Shangshu Yu", "Dunqiang Liu", "Yin Zhou", "Siqi Shen", "Chenglu Wen", "Cheng Wang"], "title": "LightLoc: Learning Outdoor LiDAR Localization at Light Speed", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "Scene coordinate regression achieves impressive results in outdoor LiDAR\nlocalization but requires days of training. Since training needs to be repeated\nfor each new scene, long training times make these methods impractical for\ntime-sensitive applications, such as autonomous driving, drones, and robotics.\nWe identify large coverage areas and vast data in large-scale outdoor scenes as\nkey challenges that limit fast training. In this paper, we propose LightLoc,\nthe first method capable of efficiently learning localization in a new scene at\nlight speed. LightLoc introduces two novel techniques to address these\nchallenges. First, we introduce sample classification guidance to assist\nregression learning, reducing ambiguity from similar samples and improving\ntraining efficiency. Second, we propose redundant sample downsampling to remove\nwell-learned frames during training, reducing training time without\ncompromising accuracy. Additionally, the fast training and confidence\nestimation capabilities of sample classification enable its integration into\nSLAM, effectively eliminating error accumulation. Extensive experiments on\nlarge-scale outdoor datasets demonstrate that LightLoc achieves\nstate-of-the-art performance with a 50x reduction in training time than\nexisting methods. Our code is available at https://github.com/liw95/LightLoc."}
{"id": "2503.18260", "pdf": "https://arxiv.org/pdf/2503.18260", "abs": "https://arxiv.org/abs/2503.18260", "authors": ["Mahak Shah", "Akaash Vishal Hazarika", "Meetu Malhotra", "Sachin C. Patil", "Joshit Mohanty"], "title": "Bridging Emotions and Architecture: Sentiment Analysis in Modern Distributed Systems", "categories": ["cs.CL", "cs.DC", "cs.LG"], "comment": "IEEE 3rd International Conference on Advancements in Smart, Secure\n  and Intelligent Computing (ASSIC)", "summary": "Sentiment analysis is a field within NLP that has gained importance because\nit is applied in various areas such as; social media surveillance, customer\nfeedback evaluation and market research. At the same time, distributed systems\nallow for effective processing of large amounts of data. Therefore, this paper\nexamines how sentiment analysis converges with distributed systems by\nconcentrating on different approaches, challenges and future investigations.\nFurthermore, we do an extensive experiment where we train sentiment analysis\nmodels using both single node configuration and distributed architecture to\nbring out the benefits and shortcomings of each method in terms of performance\nand accuracy."}
{"id": "2503.17820", "pdf": "https://arxiv.org/pdf/2503.17820", "abs": "https://arxiv.org/abs/2503.17820", "authors": ["Zheng Lin", "Nan Zhou", "Chen-Xi Du", "Deng-Ping Fan", "Shi-Min Hu"], "title": "RefCut: Interactive Segmentation with Reference Guidance", "categories": ["cs.CV"], "comment": null, "summary": "Interactive segmentation aims to segment the specified target on the image\nwith positive and negative clicks from users. Interactive ambiguity is a\ncrucial issue in this field, which refers to the possibility of multiple\ncompliant outcomes with the same clicks, such as selecting a part of an object\nversus the entire object, a single object versus a combination of multiple\nobjects, and so on. The existing methods cannot provide intuitive guidance to\nthe model, which leads to unstable output results and makes it difficult to\nmeet the large-scale and efficient annotation requirements for specific targets\nin some scenarios. To bridge this gap, we introduce RefCut, a reference-based\ninteractive segmentation framework designed to address part ambiguity and\nobject ambiguity in segmenting specific targets. Users only need to provide a\nreference image and corresponding reference masks, and the model will be\noptimized based on them, which greatly reduces the interactive burden on users\nwhen annotating a large number of such targets. In addition, to enrich these\ntwo kinds of ambiguous data, we propose a new Target Disassembly Dataset which\ncontains two subsets of part disassembly and object disassembly for evaluation.\nIn the combination evaluation of multiple datasets, our RefCut achieved\nstate-of-the-art performance. Extensive experiments and visualized results\ndemonstrate that RefCut advances the field of intuitive and controllable\ninteractive segmentation. Our code will be publicly available and the demo\nvideo is in https://www.lin-zheng.com/refcut."}
{"id": "2503.18288", "pdf": "https://arxiv.org/pdf/2503.18288", "abs": "https://arxiv.org/abs/2503.18288", "authors": ["Cheng Huang", "Fan Gao", "Nyima Tashi", "Yutong Liu", "Xiangxiang Wang", "Thupten Tsering", "Ban Ma-bao", "Renzeg Duojie", "Gadeng Luosang", "Rinchen Dongrub", "Dorje Tashi", "Xiao Feng", "Yongbin Yu"], "title": "Sun-Shine: A Large Language Model for Tibetan Culture", "categories": ["cs.CL"], "comment": null, "summary": "Tibetan, a minority language in China, features a highly intricate\ngrammatical structure, characterized by four verb tenses and a tense system\nwith frequent irregularities, contributing to its extensive inflectional\ndiversity. Recently, advances in Large Language Models (LLMs) have transformed\nthe paradigm in many domains. Despite the success in other fields, current LLMs\noften fall short in catering to the needs of domain experts like Tibetans, and\nthe potential of LLMs for Tibetan culture is under-explored. The intrinsic\nreasons are the immense and intricate nature of Tibetan culture as well as the\nnecessity for higher granularity and richness in knowledge. Simultaneously, the\ncomplexity and uniqueness of its grammatical structure, coupled with its status\nas a minority ethnic language, contribute to data scarcity, which remains a\nfundamental challenge. To alleviate these issues, we introduce Llama-Sunshine\n(Sun-Shine), the first large language model for Tibetan culture, which is\nexpert in various Tibetan language processing tasks. Sun-Shine incorporates\nstate-of-the-art model architectures optimized for Tibetan's linguistic\nfeatures. We also propose TIB-STC, a comprehensive dataset comprising diverse\nTibetan texts such as literature, religious scripts, news, and conversational\ndata, which is also the first large-scale dataset for Tibetan culture. Though\ncomprehensive experiments, Sun-Shine not only demonstrates a higher level of\nknowledge expertise for Tibetan culture but also gains preliminary embodied\nintelligence capabilities in Tibetan language processing tasks, like language\nmodeling, text classification, machine translation, and syntactic analysis.\nMoreover, it excels in low-resource scenarios, showcasing strong generalization\ncapabilities."}
{"id": "2503.17825", "pdf": "https://arxiv.org/pdf/2503.17825", "abs": "https://arxiv.org/abs/2503.17825", "authors": ["Yawei Li", "Bin Ren", "Jingyun Liang", "Rakesh Ranjan", "Mengyuan Liu", "Nicu Sebe", "Ming-Hsuan Yang", "Luca Benini"], "title": "Fractal-IR: A Unified Framework for Efficient and Scalable Image Restoration", "categories": ["cs.CV"], "comment": null, "summary": "While vision transformers achieve significant breakthroughs in various image\nrestoration (IR) tasks, it is still challenging to efficiently scale them\nacross multiple types of degradations and resolutions. In this paper, we\npropose Fractal-IR, a fractal-based design that progressively refines degraded\nimages by repeatedly expanding local information into broader regions. This\nfractal architecture naturally captures local details at early stages and\nseamlessly transitions toward global context in deeper fractal stages, removing\nthe need for computationally heavy long-range self-attention mechanisms.\nMoveover, we observe the challenge in scaling up vision transformers for IR\ntasks. Through a series of analyses, we identify a holistic set of strategies\nto effectively guide model scaling. Extensive experimental results show that\nFractal-IR achieves state-of-the-art performance in seven common image\nrestoration tasks, including super-resolution, denoising, JPEG artifact\nremoval, IR in adverse weather conditions, motion deblurring, defocus\ndeblurring, and demosaicking. For $2\\times$ SR on Manga109, Fractal-IR achieves\na 0.21 dB PSNR gain. For grayscale image denoising on Urban100, Fractal-IR\nsurpasses the previous method by 0.2 dB for $\\sigma=50$."}
{"id": "2503.18290", "pdf": "https://arxiv.org/pdf/2503.18290", "abs": "https://arxiv.org/abs/2503.18290", "authors": ["Paul K. Mandal"], "title": "When is dataset cartography ineffective? Using training dynamics does not improve robustness against Adversarial SQuAD", "categories": ["cs.CL", "cs.AI", "I.2.7; I.2.6; I.5.1"], "comment": "5 pages, 3 figures, 4 tables", "summary": "In this paper, I investigate the effectiveness of dataset cartography for\nextractive question answering on the SQuAD dataset. I begin by analyzing\nannotation artifacts in SQuAD and evaluate the impact of two adversarial\ndatasets, AddSent and AddOneSent, on an ELECTRA-small model. Using training\ndynamics, I partition SQuAD into easy-to-learn, ambiguous, and hard-to-learn\nsubsets. I then compare the performance of models trained on these subsets to\nthose trained on randomly selected samples of equal size. Results show that\ntraining on cartography-based subsets does not improve generalization to the\nSQuAD validation set or the AddSent adversarial set. While the hard-to-learn\nsubset yields a slightly higher F1 score on the AddOneSent dataset, the overall\ngains are limited. These findings suggest that dataset cartography provides\nlittle benefit for adversarial robustness in SQuAD-style QA tasks. I conclude\nby comparing these results to prior findings on SNLI and discuss possible\nreasons for the observed differences."}
{"id": "2503.17827", "pdf": "https://arxiv.org/pdf/2503.17827", "abs": "https://arxiv.org/abs/2503.17827", "authors": ["Wenxuan Zhu", "Bing Li", "Cheng Zheng", "Jinjie Mai", "Jun Chen", "Letian Jiang", "Abdullah Hamdi", "Sara Rojas Martinez", "Chia-Wen Lin", "Mohamed Elhoseiny", "Bernard Ghanem"], "title": "4D-Bench: Benchmarking Multi-modal Large Language Models for 4D Object Understanding", "categories": ["cs.CV"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have demonstrated impressive 2D\nimage/video understanding capabilities. However, there are no publicly\nstandardized benchmarks to assess the abilities of MLLMs in understanding the\n4D objects (3D objects with temporal evolution over time). In this paper, we\nintroduce 4D-Bench, the first benchmark to evaluate the capabilities of MLLMs\nin 4D object understanding, featuring tasks in 4D object Question Answering (4D\nobject QA) and 4D object captioning. 4D-Bench provides 4D objects with diverse\ncategories, high-quality annotations, and tasks necessitating multi-view\nspatial-temporal understanding, different from existing 2D image/video-based\nbenchmarks. With 4D-Bench, we evaluate a wide range of open-source and\nclosed-source MLLMs. The results from the 4D object captioning experiment\nindicate that MLLMs generally exhibit weaker temporal understanding compared to\ntheir appearance understanding, notably, while open-source models approach\nclosed-source performance in appearance understanding, they show larger\nperformance gaps in temporal understanding. 4D object QA yields surprising\nfindings: even with simple single-object videos, MLLMs perform poorly, with\nstate-of-the-art GPT-4o achieving only 63\\% accuracy compared to the human\nbaseline of 91\\%. These findings highlight a substantial gap in 4D object\nunderstanding and the need for further advancements in MLLMs."}
{"id": "2503.18293", "pdf": "https://arxiv.org/pdf/2503.18293", "abs": "https://arxiv.org/abs/2503.18293", "authors": ["Jiayi Yao", "Haibo Sun", "Nianwen Xue"], "title": "Fact-checking AI-generated news reports: Can LLMs catch their own lies?", "categories": ["cs.CL"], "comment": null, "summary": "In this paper, we evaluate the ability of Large Language Models (LLMs) to\nassess the veracity of claims in ''news reports'' generated by themselves or\nother LLMs. Our goal is to determine whether LLMs can effectively fact-check\ntheir own content, using methods similar to those used to verify claims made by\nhumans. Our findings indicate that LLMs are more effective at assessing claims\nin national or international news stories than in local news stories, better at\nevaluating static information than dynamic information, and better at verifying\ntrue claims compared to false ones. We hypothesize that this disparity arises\nbecause the former types of claims are better represented in the training data.\nAdditionally, we find that incorporating retrieved results from a search engine\nin a Retrieval-Augmented Generation (RAG) setting significantly reduces the\nnumber of claims an LLM cannot assess. However, this approach also increases\nthe occurrence of incorrect assessments, partly due to irrelevant or\nlow-quality search results. This diagnostic study highlights the need for\nfuture research on fact-checking machine-generated reports to prioritize\nimproving the precision and relevance of retrieved information to better\nsupport fact-checking efforts. Furthermore, claims about dynamic events and\nlocal news may require human-in-the-loop fact-checking systems to ensure\naccuracy and reliability."}
{"id": "2503.17856", "pdf": "https://arxiv.org/pdf/2503.17856", "abs": "https://arxiv.org/abs/2503.17856", "authors": ["Radu Beche", "Sergiu Nedevschi"], "title": "ClaraVid: A Holistic Scene Reconstruction Benchmark From Aerial Perspective With Delentropy-Based Complexity Profiling", "categories": ["cs.CV"], "comment": "Currently under review", "summary": "The development of aerial holistic scene understanding algorithms is hindered\nby the scarcity of comprehensive datasets that enable both semantic and\ngeometric reconstruction. While synthetic datasets offer an alternative,\nexisting options exhibit task-specific limitations, unrealistic scene\ncompositions, and rendering artifacts that compromise real-world applicability.\nWe introduce ClaraVid, a synthetic aerial dataset specifically designed to\novercome these limitations. Comprising 16,917 high-resolution images captured\nat 4032x3024 from multiple viewpoints across diverse landscapes, ClaraVid\nprovides dense depth maps, panoptic segmentation, sparse point clouds, and\ndynamic object masks, while mitigating common rendering artifacts. To further\nadvance neural reconstruction, we introduce the Delentropic Scene Profile\n(DSP), a novel complexity metric derived from differential entropy analysis,\ndesigned to quantitatively assess scene difficulty and inform reconstruction\ntasks. Utilizing DSP, we systematically benchmark neural reconstruction\nmethods, uncovering a consistent, measurable correlation between scene\ncomplexity and reconstruction accuracy. Empirical results indicate that higher\ndelentropy strongly correlates with increased reconstruction errors, validating\nDSP as a reliable complexity prior. Currently under review, upon acceptance the\ndata and code will be available at\n$\\href{https://rdbch.github.io/claravid}{rdbch.github.io/ClaraVid}$."}
{"id": "2503.18296", "pdf": "https://arxiv.org/pdf/2503.18296", "abs": "https://arxiv.org/abs/2503.18296", "authors": ["Mengya Xu", "Zhongzhen Huang", "Jie Zhang", "Xiaofan Zhang", "Qi Dou"], "title": "Surgical Action Planning with Large Language Models", "categories": ["cs.CL"], "comment": "10 pages,4 figures", "summary": "In robot-assisted minimally invasive surgery, we introduce the Surgical\nAction Planning (SAP) task, which generates future action plans from visual\ninputs to address the absence of intraoperative predictive planning in current\nintelligent applications. SAP shows great potential for enhancing\nintraoperative guidance and automating procedures. However, it faces challenges\nsuch as understanding instrument-action relationships and tracking surgical\nprogress. Large Language Models (LLMs) show promise in understanding surgical\nvideo content but remain underexplored for predictive decision-making in SAP,\nas they focus mainly on retrospective analysis. Challenges like data privacy,\ncomputational demands, and modality-specific constraints further highlight\nsignificant research gaps. To tackle these challenges, we introduce LLM-SAP, a\nLarge Language Models-based Surgical Action Planning framework that predicts\nfuture actions and generates text responses by interpreting natural language\nprompts of surgical goals. The text responses potentially support surgical\neducation, intraoperative decision-making, procedure documentation, and skill\nanalysis. LLM-SAP integrates two novel modules: the Near-History Focus Memory\nModule (NHF-MM) for modeling historical states and the prompts factory for\naction planning. We evaluate LLM-SAP on our constructed CholecT50-SAP dataset\nusing models like Qwen2.5 and Qwen2-VL, demonstrating its effectiveness in\nnext-action prediction. Pre-trained LLMs are tested zero-shot, and supervised\nfine-tuning (SFT) with LoRA is implemented to address data privacy concerns.\nOur experiments show that Qwen2.5-72B-SFT surpasses Qwen2.5-72B with a 19.3%\nhigher accuracy."}
{"id": "2503.17862", "pdf": "https://arxiv.org/pdf/2503.17862", "abs": "https://arxiv.org/abs/2503.17862", "authors": ["Li Liu", "Shuzhou Sun", "Shuaifeng Zhi", "Fan Shi", "Zhen Liu", "Janne Heikkilä", "Yongxiang Liu"], "title": "A Causal Adjustment Module for Debiasing Scene Graph Generation", "categories": ["cs.CV", "cs.AI"], "comment": "18 pages, 8 tables, 10 figures", "summary": "While recent debiasing methods for Scene Graph Generation (SGG) have shown\nimpressive performance, these efforts often attribute model bias solely to the\nlong-tail distribution of relationships, overlooking the more profound causes\nstemming from skewed object and object pair distributions. In this paper, we\nemploy causal inference techniques to model the causality among these observed\nskewed distributions. Our insight lies in the ability of causal inference to\ncapture the unobservable causal effects between complex distributions, which is\ncrucial for tracing the roots of model bias. Specifically, we introduce the\nMediator-based Causal Chain Model (MCCM), which, in addition to modeling\ncausality among objects, object pairs, and relationships, incorporates mediator\nvariables, i.e., cooccurrence distribution, for complementing the causality.\nFollowing this, we propose the Causal Adjustment Module (CAModule) to estimate\nthe modeled causal structure, using variables from MCCM as inputs to produce a\nset of adjustment factors aimed at correcting biased model predictions.\nMoreover, our method enables the composition of zero-shot relationships,\nthereby enhancing the model's ability to recognize such relationships.\nExperiments conducted across various SGG backbones and popular benchmarks\ndemonstrate that CAModule achieves state-of-the-art mean recall rates, with\nsignificant improvements also observed on the challenging zero-shot recall rate\nmetric."}
{"id": "2503.18360", "pdf": "https://arxiv.org/pdf/2503.18360", "abs": "https://arxiv.org/abs/2503.18360", "authors": ["Yiran Hu", "Huanghai Liu", "Qingjing Chen", "Ning Zheng", "Chong Wang", "Yun Liu", "Charles L. A. Clarke", "Weixing Shen"], "title": "J&H: Evaluating the Robustness of Large Language Models Under Knowledge-Injection Attacks in Legal Domain", "categories": ["cs.CL"], "comment": "10 pages, 5 figures", "summary": "As the scale and capabilities of Large Language Models (LLMs) increase, their\napplications in knowledge-intensive fields such as legal domain have garnered\nwidespread attention. However, it remains doubtful whether these LLMs make\njudgments based on domain knowledge for reasoning. If LLMs base their judgments\nsolely on specific words or patterns, rather than on the underlying logic of\nthe language, the ''LLM-as-judges'' paradigm poses substantial risks in the\nreal-world applications. To address this question, we propose a method of legal\nknowledge injection attacks for robustness testing, thereby inferring whether\nLLMs have learned legal knowledge and reasoning logic. In this paper, we\npropose J&H: an evaluation framework for detecting the robustness of LLMs under\nknowledge injection attacks in the legal domain. The aim of the framework is to\nexplore whether LLMs perform deductive reasoning when accomplishing legal\ntasks. To further this aim, we have attacked each part of the reasoning logic\nunderlying these tasks (major premise, minor premise, and conclusion\ngeneration). We have collected mistakes that legal experts might make in\njudicial decisions in the real world, such as typos, legal synonyms, inaccurate\nexternal legal statutes retrieval. However, in real legal practice, legal\nexperts tend to overlook these mistakes and make judgments based on logic.\nHowever, when faced with these errors, LLMs are likely to be misled by\ntypographical errors and may not utilize logic in their judgments. We conducted\nknowledge injection attacks on existing general and domain-specific LLMs.\nCurrent LLMs are not robust against the attacks employed in our experiments. In\naddition we propose and compare several methods to enhance the knowledge\nrobustness of LLMs."}
{"id": "2503.17871", "pdf": "https://arxiv.org/pdf/2503.17871", "abs": "https://arxiv.org/abs/2503.17871", "authors": ["Pranavi Kolouju", "Eric Xing", "Robert Pless", "Nathan Jacobs", "Abby Stylianou"], "title": "good4cir: Generating Detailed Synthetic Captions for Composed Image Retrieval", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Composed image retrieval (CIR) enables users to search images using a\nreference image combined with textual modifications. Recent advances in\nvision-language models have improved CIR, but dataset limitations remain a\nbarrier. Existing datasets often rely on simplistic, ambiguous, or insufficient\nmanual annotations, hindering fine-grained retrieval. We introduce good4cir, a\nstructured pipeline leveraging vision-language models to generate high-quality\nsynthetic annotations. Our method involves: (1) extracting fine-grained object\ndescriptions from query images, (2) generating comparable descriptions for\ntarget images, and (3) synthesizing textual instructions capturing meaningful\ntransformations between images. This reduces hallucination, enhances\nmodification diversity, and ensures object-level consistency. Applying our\nmethod improves existing datasets and enables creating new datasets across\ndiverse domains. Results demonstrate improved retrieval accuracy for CIR models\ntrained on our pipeline-generated datasets. We release our dataset construction\nframework to support further research in CIR and multi-modal retrieval."}
{"id": "2503.18432", "pdf": "https://arxiv.org/pdf/2503.18432", "abs": "https://arxiv.org/abs/2503.18432", "authors": ["Junsong Li", "Jie Zhou", "Yutao Yang", "Bihao Zhan", "Qianjun Pan", "Yuyang Ding", "Qin Chen", "Jiang Bo", "Xin Lin", "Liang He"], "title": "Teaching LLMs for Step-Level Automatic Math Correction via Reinforcement Learning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Automatic math correction aims to check students' solutions to mathematical\nproblems via artificial intelligence technologies. Most existing studies focus\non judging the final answer at the problem level, while they ignore detailed\nfeedback on each step in a math problem-solving process, which requires\nabilities of semantic understanding and reasoning. In this paper, we propose a\nreinforcement learning (RL)-based method to boost large language model (LLM)\nfor step-level automatic math correction, named StepAMC. Particularly, we\nconvert the step-level automatic math correction within the text classification\ntask into an RL problem to enhance the reasoning capabilities of LLMs. Then, we\ndesign a space-constrained policy network to improve the stability of RL. Then,\nwe introduce a fine-grained reward network to convert the binary human feedback\ninto a continuous value. We conduct extensive experiments over two benchmark\ndatasets and the results show that our model outperforms the eleven strong\nbaselines."}
{"id": "2503.17877", "pdf": "https://arxiv.org/pdf/2503.17877", "abs": "https://arxiv.org/abs/2503.17877", "authors": ["Samira Alkaee Taleghan", "Andrew P. Barrett", "Walter N. Meier", "Farnoush Banaei-Kashani"], "title": "IceBench: A Benchmark for Deep Learning based Sea Ice Type Classification", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Sea ice plays a critical role in the global climate system and maritime\noperations, making timely and accurate classification essential. However,\ntraditional manual methods are time-consuming, costly, and have inherent\nbiases. Automating sea ice type classification addresses these challenges by\nenabling faster, more consistent, and scalable analysis. While both traditional\nand deep learning approaches have been explored, deep learning models offer a\npromising direction for improving efficiency and consistency in sea ice\nclassification. However, the absence of a standardized benchmark and\ncomparative study prevents a clear consensus on the best-performing models. To\nbridge this gap, we introduce \\textit{IceBench}, a comprehensive benchmarking\nframework for sea ice type classification. Our key contributions are threefold:\nFirst, we establish the IceBench benchmarking framework which leverages the\nexisting AI4Arctic Sea Ice Challenge dataset as a standardized dataset,\nincorporates a comprehensive set of evaluation metrics, and includes\nrepresentative models from the entire spectrum of sea ice type classification\nmethods categorized in two distinct groups, namely, pixel-based classification\nmethods and patch-based classification methods. IceBench is open-source and\nallows for convenient integration and evaluation of other sea ice type\nclassification methods; hence, facilitating comparative evaluation of new\nmethods and improving reproducibility in the field. Second, we conduct an\nin-depth comparative study on representative models to assess their strengths\nand limitations, providing insights for both practitioners and researchers.\nThird, we leverage IceBench for systematic experiments addressing key research\nquestions on model transferability across seasons (time) and locations (space),\ndata downscaling, and preprocessing strategies."}
{"id": "2503.18471", "pdf": "https://arxiv.org/pdf/2503.18471", "abs": "https://arxiv.org/abs/2503.18471", "authors": ["Calvin Bao", "Yow-Ting Shiue", "Marine Carpuat", "Joel Chan"], "title": "Words as Bridges: Exploring Computational Support for Cross-Disciplinary Translation Work", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "26 pages, 8 tables, 6 figures", "summary": "Scholars often explore literature outside of their home community of study.\nThis exploration process is frequently hampered by field-specific jargon. Past\ncomputational work often focuses on supporting translation work by removing\njargon through simplification and summarization; here, we explore a different\napproach that preserves jargon as useful bridges to new conceptual spaces.\nSpecifically, we cast different scholarly domains as different language-using\ncommunities, and explore how to adapt techniques from unsupervised\ncross-lingual alignment of word embeddings to explore conceptual alignments\nbetween domain-specific word embedding spaces.We developed a prototype\ncross-domain search engine that uses aligned domain-specific embeddings to\nsupport conceptual exploration, and tested this prototype in two case studies.\nWe discuss qualitative insights into the promises and pitfalls of this approach\nto translation work, and suggest design insights for future interfaces that\nprovide computational support for cross-domain information seeking."}
{"id": "2503.17899", "pdf": "https://arxiv.org/pdf/2503.17899", "abs": "https://arxiv.org/abs/2503.17899", "authors": ["Dongheng Lin", "Han Hu", "Jianbo Jiao"], "title": "What Time Tells Us? An Explorative Study of Time Awareness Learned from Static Images", "categories": ["cs.CV"], "comment": null, "summary": "Time becomes visible through illumination changes in what we see. Inspired by\nthis, in this paper we explore the potential to learn time awareness from\nstatic images, trying to answer: what time tells us? To this end, we first\nintroduce a Time-Oriented Collection (TOC) dataset, which contains 130,906\nimages with reliable timestamps. Leveraging this dataset, we propose a\nTime-Image Contrastive Learning (TICL) approach to jointly model timestamps and\nrelated visual representations through cross-modal contrastive learning. We\nfound that the proposed TICL, 1) not only achieves state-of-the-art performance\non the timestamp estimation task, over various benchmark metrics, 2) but also,\ninterestingly, though only seeing static images, the time-aware embeddings\nlearned from TICL show strong capability in several time-aware downstream tasks\nsuch as time-based image retrieval, video scene classification, and time-aware\nimage editing. Our findings suggest that time-related visual cues can be\nlearned from static images and are beneficial for various vision tasks, laying\na foundation for future research on understanding time-related visual context.\nProject page:https://rathgrith.github.io/timetells/."}
{"id": "2503.18485", "pdf": "https://arxiv.org/pdf/2503.18485", "abs": "https://arxiv.org/abs/2503.18485", "authors": ["Dawit Ketema Gete", "Bedru Yimam Ahamed", "Tadesse Destaw Belay", "Yohannes Ayana Ejigu", "Sukairaj Hafiz Imam", "Alemu Belay Tessema", "Mohammed Oumer Adem", "Tadesse Amare Belay", "Robert Geislinger", "Umma Aliyu Musa", "Martin Semmann", "Shamsuddeen Hassan Muhammad", "Henning Schreiber", "Seid Muhie Yimam"], "title": "Whispering in Amharic: Fine-tuning Whisper for Low-resource Language", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "This work explores fine-tuning OpenAI's Whisper automatic speech recognition\n(ASR) model for Amharic, a low-resource language, to improve transcription\naccuracy. While the foundational Whisper model struggles with Amharic due to\nlimited representation in its training data, we fine-tune it using datasets\nlike Mozilla Common Voice, FLEURS, and the BDU-speech dataset. The\nbest-performing model, Whispersmall-am, significantly improves when finetuned\non a mix of existing FLEURS data and new, unseen Amharic datasets. Training\nsolely on new data leads to poor performance, but combining it with FLEURS data\nreinforces the model, enabling better specialization in Amharic. We also\ndemonstrate that normalizing Amharic homophones significantly enhances Word\nError Rate (WER) and Bilingual Evaluation Understudy (BLEU) scores. This study\nunderscores the importance of fine-tuning strategies and dataset composition\nfor improving ASR in low-resource languages, providing insights for future\nAmharic speech recognition research."}
{"id": "2503.17907", "pdf": "https://arxiv.org/pdf/2503.17907", "abs": "https://arxiv.org/abs/2503.17907", "authors": ["Takahiro Shindo", "Yui Tatsumi", "Taiju Watanabe", "Hiroshi Watanabe"], "title": "Guided Diffusion for the Extension of Machine Vision to Human Visual Perception", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Image compression technology eliminates redundant information to enable\nefficient transmission and storage of images, serving both machine vision and\nhuman visual perception. For years, image coding focused on human perception\nhas been well-studied, leading to the development of various image compression\nstandards. On the other hand, with the rapid advancements in image recognition\nmodels, image compression for AI tasks, known as Image Coding for Machines\n(ICM), has gained significant importance. Therefore, scalable image coding\ntechniques that address the needs of both machines and humans have become a key\narea of interest. Additionally, there is increasing demand for research\napplying the diffusion model, which can generate human-viewable images from a\nsmall amount of data to image compression methods for human vision. Image\ncompression methods that use diffusion models can partially reconstruct the\ntarget image by guiding the generation process with a small amount of\nconditioning information. Inspired by the diffusion model's potential, we\npropose a method for extending machine vision to human visual perception using\nguided diffusion. Utilizing the diffusion model guided by the output of the ICM\nmethod, we generate images for human perception from random noise. Guided\ndiffusion acts as a bridge between machine vision and human vision, enabling\ntransitions between them without any additional bitrate overhead. The generated\nimages then evaluated based on bitrate and image quality, and we compare their\ncompression performance with other scalable image coding methods for humans and\nmachines."}
{"id": "2503.18491", "pdf": "https://arxiv.org/pdf/2503.18491", "abs": "https://arxiv.org/abs/2503.18491", "authors": ["Shuo Yang", "Siwen Luo", "Soyeon Caren Han", "Eduard Hovy"], "title": "MAGIC-VQA: Multimodal And Grounded Inference with Commonsense Knowledge for Visual Question Answering", "categories": ["cs.CL"], "comment": "8 Pages, 5 figures", "summary": "Visual Question Answering (VQA) requires reasoning across visual and textual\nmodalities, yet Large Vision-Language Models (LVLMs) often lack integrated\ncommonsense knowledge, limiting their robustness in real-world scenarios. To\naddress this, we introduce MAGIC-VQA, a novel framework that enhances VQA by\nsystematically integrating commonsense knowledge with LVLMs. MAGIC-VQA employs\na three-stage process: (1) Explicit Knowledge Integration from external\nsources, (2) By-Type Post-Processing for contextual refinement, and (3)\nImplicit Knowledge Augmentation using a Graph Neural Network (GNN) for\nstructured reasoning. While GNNs bring greater depth to structured inference,\nthey enable superior relational inference beyond LVLMs. MAGIC-VQA bridges a key\ngap by unifying commonsensse knowledge with LVLM-driven reasoning, eliminating\nthe need for extensive pre-training or complex prompt tuning. Our framework\nachieves state-of-the-art performance on benchmark datasets, significantly\nimproving commonsense reasoning in VQA."}
{"id": "2503.17928", "pdf": "https://arxiv.org/pdf/2503.17928", "abs": "https://arxiv.org/abs/2503.17928", "authors": ["Zefeng Zhang", "Hengzhu Tang", "Jiawei Sheng", "Zhenyu Zhang", "Yiming Ren", "Zhenyang Li", "Dawei Yin", "Duohe Ma", "Tingwen Liu"], "title": "Debiasing Multimodal Large Language Models via Noise-Aware Preference Optimization", "categories": ["cs.CV", "cs.CL"], "comment": "CVPR 2025", "summary": "Multimodal Large Language Models excel in various tasks, yet often struggle\nwith modality bias, where the model tends to rely heavily on a single modality\nand overlook critical information in other modalities, which leads to incorrect\nfocus and generating irrelevant responses. In this paper, we propose using the\nparadigm of preference optimization to solve the modality bias problem,\nincluding RLAIFVBias, a debiased preference optimization dataset, and a Noise\nAware Preference Optimization algorithm. Specifically, we first construct the\ndataset by introducing perturbations to reduce the informational content of\ncertain modalities, compelling the model to rely on a specific modality when\ngenerating negative responses. To address the inevitable noise in automatically\nconstructed data, we combine the noise robust Mean Absolute Error with the\nBinary Cross Entropy in Direct Preference Optimization by a negative Box Cox\ntransformation, and dynamically adjust the algorithm noise robustness based on\nthe evaluated noise levels in the data. Extensive experiments validate our\napproach, demonstrating not only its effectiveness in mitigating modality bias\nbut also its significant role in minimizing hallucinations."}
{"id": "2503.18502", "pdf": "https://arxiv.org/pdf/2503.18502", "abs": "https://arxiv.org/abs/2503.18502", "authors": ["Andrés García-Silva", "José Manuel Gómez-Pérez"], "title": "Autoregressive Language Models for Knowledge Base Population: A case study in the space mission domain", "categories": ["cs.CL"], "comment": "Pre-print version", "summary": "Knowledge base population KBP plays a crucial role in populating and\nmaintaining knowledge bases up-to-date in organizations by leveraging domain\ncorpora. Motivated by the increasingly large context windows supported by large\nlanguage models, we propose to fine-tune an autoregressive language model for\nend-toend KPB. Our case study involves the population of a space mission\nknowledge graph. To fine-tune the model we generate a dataset for end-to-end\nKBP tapping into existing domain resources. Our case study shows that\nfine-tuned language models of limited size can achieve competitive and even\nhigher accuracy than larger models in the KBP task. Smaller models specialized\nfor KBP offer affordable deployment and lower-cost inference. Moreover, KBP\nspecialist models do not require the ontology to be included in the prompt,\nallowing for more space in the context for additional input text or output\nserialization."}
{"id": "2503.17934", "pdf": "https://arxiv.org/pdf/2503.17934", "abs": "https://arxiv.org/abs/2503.17934", "authors": ["Xuewei Chen", "Zhimin Chen", "Yiren Song"], "title": "TransAnimate: Taming Layer Diffusion to Generate RGBA Video", "categories": ["cs.CV"], "comment": null, "summary": "Text-to-video generative models have made remarkable advancements in recent\nyears. However, generating RGBA videos with alpha channels for transparency and\nvisual effects remains a significant challenge due to the scarcity of suitable\ndatasets and the complexity of adapting existing models for this purpose. To\naddress these limitations, we present TransAnimate, an innovative framework\nthat integrates RGBA image generation techniques with video generation modules,\nenabling the creation of dynamic and transparent videos. TransAnimate\nefficiently leverages pre-trained text-to-transparent image model weights and\ncombines them with temporal models and controllability plugins trained on RGB\nvideos, adapting them for controllable RGBA video generation tasks.\nAdditionally, we introduce an interactive motion-guided control mechanism,\nwhere directional arrows define movement and colors adjust scaling, offering\nprecise and intuitive control for designing game effects. To further alleviate\ndata scarcity, we have developed a pipeline for creating an RGBA video dataset,\nincorporating high-quality game effect videos, extracted foreground objects,\nand synthetic transparent videos. Comprehensive experiments demonstrate that\nTransAnimate generates high-quality RGBA videos, establishing it as a practical\nand effective tool for applications in gaming and visual effects."}
{"id": "2503.18526", "pdf": "https://arxiv.org/pdf/2503.18526", "abs": "https://arxiv.org/abs/2503.18526", "authors": ["Raúl Ortega", "José Manuel Gómez-Pérez"], "title": "SciClaims: An End-to-End Generative System for Biomedical Claim Analysis", "categories": ["cs.CL", "cs.AI", "cs.DL"], "comment": "Pre-print version", "summary": "Validating key claims in scientific literature, particularly in biomedical\nresearch, is essential for ensuring accuracy and advancing knowledge. This\nprocess is critical in sectors like the pharmaceutical industry, where rapid\nscientific progress requires automation and deep domain expertise. However,\ncurrent solutions have significant limitations. They lack end-to-end pipelines\nencompassing all claim extraction, evidence retrieval, and verification steps;\nrely on complex NLP and information retrieval pipelines prone to multiple\nfailure points; and often fail to provide clear, user-friendly justifications\nfor claim verification outcomes. To address these challenges, we introduce\nSciClaims, an advanced system powered by state-of-the-art large language models\n(LLMs) that seamlessly integrates the entire scientific claim analysis process.\nSciClaims outperforms previous approaches in both claim extraction and\nverification without requiring additional fine-tuning, setting a new benchmark\nfor automated scientific claim analysis."}
{"id": "2503.17937", "pdf": "https://arxiv.org/pdf/2503.17937", "abs": "https://arxiv.org/abs/2503.17937", "authors": ["Zhi Zhang", "Daoyi Chen"], "title": "Cross-Domain Underwater Image Enhancement Guided by No-Reference Image Quality Assessment: A Transfer Learning Approach", "categories": ["cs.CV"], "comment": null, "summary": "Single underwater image enhancement (UIE) is a challenging ill-posed problem,\nbut its development is hindered by two major issues: (1) The labels in\nunderwater reference datasets are pseudo labels, relying on these pseudo ground\ntruths in supervised learning leads to domain discrepancy. (2) Underwater\nreference datasets are scarce, making training on such small datasets prone to\noverfitting and distribution shift. To address these challenges, we propose\nTrans-UIE, a transfer learning-based UIE model that captures the fundamental\nparadigms of UIE through pretraining and utilizes a dataset composed of both\nreference and non-reference datasets for fine-tuning. However, fine-tuning the\nmodel using only reconstruction loss may introduce confirmation bias. To\nmitigate this, our method leverages no-reference image quality assessment\n(NR-IQA) metrics from above-water scenes to guide the transfer learning process\nacross domains while generating enhanced images with the style of the\nabove-water image domain. Additionally, to reduce the risk of overfitting\nduring the pretraining stage, we introduce Pearson correlation loss.\nExperimental results on both full-reference and no-reference underwater\nbenchmark datasets demonstrate that Trans-UIE significantly outperforms\nstate-of-the-art methods."}
{"id": "2503.18539", "pdf": "https://arxiv.org/pdf/2503.18539", "abs": "https://arxiv.org/abs/2503.18539", "authors": ["Ashenafi Zebene Woldaregay", "Jørgen Aarmo Lund", "Phuong Dinh Ngo", "Mariyam Tayefi", "Joel Burman", "Stine Hansen", "Martin Hylleholt Sillesen", "Hercules Dalianis", "Robert Jenssen", "Lindsetmo Rolf Ole", "Karl Øyvind Mikalsen"], "title": "Natural Language Processing for Electronic Health Records in Scandinavian Languages: Norwegian, Swedish, and Danish", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "45 pages including the appendix, 9 figures in the main manuscript and\n  11 figures in the Appendix", "summary": "Background: Clinical natural language processing (NLP) refers to the use of\ncomputational methods for extracting, processing, and analyzing unstructured\nclinical text data, and holds a huge potential to transform healthcare in\nvarious clinical tasks. Objective: The study aims to perform a systematic\nreview to comprehensively assess and analyze the state-of-the-art NLP methods\nfor the mainland Scandinavian clinical text. Method: A literature search was\nconducted in various online databases including PubMed, ScienceDirect, Google\nScholar, ACM digital library, and IEEE Xplore between December 2022 and\nFebruary 2024. Further, relevant references to the included articles were also\nused to solidify our search. The final pool includes articles that conducted\nclinical NLP in the mainland Scandinavian languages and were published in\nEnglish between 2010 and 2024. Results: Out of the 113 articles, 18% (n=21)\nfocus on Norwegian clinical text, 64% (n=72) on Swedish, 10% (n=11) on Danish,\nand 8% (n=9) focus on more than one language. Generally, the review identified\npositive developments across the region despite some observable gaps and\ndisparities between the languages. There are substantial disparities in the\nlevel of adoption of transformer-based models. In essential tasks such as\nde-identification, there is significantly less research activity focusing on\nNorwegian and Danish compared to Swedish text. Further, the review identified a\nlow level of sharing resources such as data, experimentation code, pre-trained\nmodels, and rate of adaptation and transfer learning in the region. Conclusion:\nThe review presented a comprehensive assessment of the state-of-the-art\nClinical NLP for electronic health records (EHR) text in mainland Scandinavian\nlanguages and, highlighted the potential barriers and challenges that hinder\nthe rapid advancement of the field in the region."}
{"id": "2503.17938", "pdf": "https://arxiv.org/pdf/2503.17938", "abs": "https://arxiv.org/abs/2503.17938", "authors": ["Xiang Fang", "Shihua Zhang", "Hao Zhang", "Tao Lu", "Huabing Zhou", "Jiayi Ma"], "title": "Selecting and Pruning: A Differentiable Causal Sequentialized State-Space Model for Two-View Correspondence Learning", "categories": ["cs.CV"], "comment": null, "summary": "Two-view correspondence learning aims to discern true and false\ncorrespondences between image pairs by recognizing their underlying different\ninformation. Previous methods either treat the information equally or require\nthe explicit storage of the entire context, tending to be laborious in\nreal-world scenarios. Inspired by Mamba's inherent selectivity, we propose\n\\textbf{CorrMamba}, a \\textbf{Corr}espondence filter leveraging\n\\textbf{Mamba}'s ability to selectively mine information from true\ncorrespondences while mitigating interference from false ones, thus achieving\nadaptive focus at a lower cost. To prevent Mamba from being potentially\nimpacted by unordered keypoints that obscured its ability to mine spatial\ninformation, we customize a causal sequential learning approach based on the\nGumbel-Softmax technique to establish causal dependencies between features in a\nfully autonomous and differentiable manner. Additionally, a local-context\nenhancement module is designed to capture critical contextual cues essential\nfor correspondence pruning, complementing the core framework. Extensive\nexperiments on relative pose estimation, visual localization, and analysis\ndemonstrate that CorrMamba achieves state-of-the-art performance. Notably, in\noutdoor relative pose estimation, our method surpasses the previous SOTA by\n$2.58$ absolute percentage points in AUC@20\\textdegree, highlighting its\npractical superiority. Our code will be publicly available."}
{"id": "2503.18562", "pdf": "https://arxiv.org/pdf/2503.18562", "abs": "https://arxiv.org/abs/2503.18562", "authors": ["Nariman Naderi", "Seyed Amir Ahmad Safavi-Naini", "Thomas Savage", "Zahra Atf", "Peter Lewis", "Girish Nadkarni", "Ali Soroush"], "title": "Self-Reported Confidence of Large Language Models in Gastroenterology: Analysis of Commercial, Open-Source, and Quantized Models", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "comment": "35 pages, 5 figures, 1 table, 7 supplementary figures", "summary": "This study evaluated self-reported response certainty across several large\nlanguage models (GPT, Claude, Llama, Phi, Mistral, Gemini, Gemma, and Qwen)\nusing 300 gastroenterology board-style questions. The highest-performing models\n(GPT-o1 preview, GPT-4o, and Claude-3.5-Sonnet) achieved Brier scores of\n0.15-0.2 and AUROC of 0.6. Although newer models demonstrated improved\nperformance, all exhibited a consistent tendency towards overconfidence.\nUncertainty estimation presents a significant challenge to the safe use of LLMs\nin healthcare. Keywords: Large Language Models; Confidence Elicitation;\nArtificial Intelligence; Gastroenterology; Uncertainty Quantification"}
{"id": "2503.17940", "pdf": "https://arxiv.org/pdf/2503.17940", "abs": "https://arxiv.org/abs/2503.17940", "authors": ["Dong Zhao", "Jinlong Li", "Shuang Wang", "Mengyao Wu", "Qi Zang", "Nicu Sebe", "Zhun Zhong"], "title": "FisherTune: Fisher-Guided Robust Tuning of Vision Foundation Models for Domain Generalized Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Vision Foundation Models (VFMs) excel in generalization due to large-scale\npretraining, but fine-tuning them for Domain Generalized Semantic Segmentation\n(DGSS) while maintaining this ability remains challenging. Existing approaches\neither selectively fine-tune parameters or freeze the VFMs and update only the\nadapters, both of which may underutilize the VFMs' full potential in DGSS\ntasks. We observe that domain-sensitive parameters in VFMs, arising from task\nand distribution differences, can hinder generalization. To address this, we\npropose \\textbf{FisherTune}, a robust fine-tuning method guided by the\nDomain-Related Fisher Information Matrix (DR-FIM). DR-FIM measures parameter\nsensitivity across tasks and domains, enabling selective updates that preserve\ngeneralization and enhance DGSS adaptability. FisherTune incorporates\nvariational inference to stabilize DR-FIM estimation, treating parameters as\nGaussian-distributed variables and leveraging pre-trained priors. Extensive\nexperiments show that FisherTune achieves superior cross-domain segmentation\nwhile maintaining generalization, outperforming selective-parameter and\nadapter-based methods."}
{"id": "2503.18594", "pdf": "https://arxiv.org/pdf/2503.18594", "abs": "https://arxiv.org/abs/2503.18594", "authors": ["Guillem García Subies", "Álvaro Barbero Jiménez", "Paloma Martínez Fernández"], "title": "ClinText-SP and RigoBERTa Clinical: a new set of open resources for Spanish Clinical NLP", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We present a novel contribution to Spanish clinical natural language\nprocessing by introducing the largest publicly available clinical corpus,\nClinText-SP, along with a state-of-the-art clinical encoder language model,\nRigoBERTa Clinical. Our corpus was meticulously curated from diverse open\nsources, including clinical cases from medical journals and annotated corpora\nfrom shared tasks, providing a rich and diverse dataset that was previously\ndifficult to access. RigoBERTa Clinical, developed through domain-adaptive\npretraining on this comprehensive dataset, significantly outperforms existing\nmodels on multiple clinical NLP benchmarks. By publicly releasing both the\ndataset and the model, we aim to empower the research community with robust\nresources that can drive further advancements in clinical NLP and ultimately\ncontribute to improved healthcare applications."}
{"id": "2503.17966", "pdf": "https://arxiv.org/pdf/2503.17966", "abs": "https://arxiv.org/abs/2503.17966", "authors": ["Zeng-Hui Zhu", "Wei Lu", "Si-Bao Chen", "Chris H. Q. Ding", "Jin Tang", "Bin Luo"], "title": "Real-World Remote Sensing Image Dehazing: Benchmark and Baseline", "categories": ["cs.CV", "eess.IV"], "comment": "11 pages, 9 figures, real-world remote sensing image dehazing dataset", "summary": "Remote Sensing Image Dehazing (RSID) poses significant challenges in\nreal-world scenarios due to the complex atmospheric conditions and severe color\ndistortions that degrade image quality. The scarcity of real-world remote\nsensing hazy image pairs has compelled existing methods to rely primarily on\nsynthetic datasets. However, these methods struggle with real-world\napplications due to the inherent domain gap between synthetic and real data. To\naddress this, we introduce Real-World Remote Sensing Hazy Image Dataset\n(RRSHID), the first large-scale dataset featuring real-world hazy and dehazed\nimage pairs across diverse atmospheric conditions. Based on this, we propose\nMCAF-Net, a novel framework tailored for real-world RSID. Its effectiveness\narises from three innovative components: Multi-branch Feature Integration Block\nAggregator (MFIBA), which enables robust feature extraction through cascaded\nintegration blocks and parallel multi-branch processing; Color-Calibrated\nSelf-Supervised Attention Module (CSAM), which mitigates complex color\ndistortions via self-supervised learning and attention-guided refinement; and\nMulti-Scale Feature Adaptive Fusion Module (MFAFM), which integrates features\neffectively while preserving local details and global context. Extensive\nexperiments validate that MCAF-Net demonstrates state-of-the-art performance in\nreal-world RSID, while maintaining competitive performance on synthetic\ndatasets. The introduction of RRSHID and MCAF-Net sets new benchmarks for\nreal-world RSID research, advancing practical solutions for this complex task.\nThe code and dataset are publicly available at\n\\url{https://github.com/lwCVer/RRSHID}."}
{"id": "2503.18596", "pdf": "https://arxiv.org/pdf/2503.18596", "abs": "https://arxiv.org/abs/2503.18596", "authors": ["Yihan Wang", "Peiyu Liu", "Xin Yang"], "title": "LinkAlign: Scalable Schema Linking for Real-World Large-Scale Multi-Database Text-to-SQL", "categories": ["cs.CL"], "comment": null, "summary": "Schema linking is a critical bottleneck in achieving human-level performance\nin Text-to-SQL tasks, particularly in real-world large-scale multi-database\nscenarios. Addressing schema linking faces two major challenges: (1) Database\nRetrieval: selecting the correct database from a large schema pool in\nmulti-database settings, while filtering out irrelevant ones. (2) Schema Item\nGrounding: accurately identifying the relevant tables and columns from within a\nlarge and redundant schema for SQL generation. To address this, we introduce\nLinkAlign, a novel framework that can effectively adapt existing baselines to\nreal-world environments by systematically addressing schema linking. Our\nframework comprises three key steps: multi-round semantic enhanced retrieval\nand irrelevant information isolation for Challenge 1, and schema extraction\nenhancement for Challenge 2. We evaluate our method performance of schema\nlinking on the SPIDER and BIRD benchmarks, and the ability to adapt existing\nText-to-SQL models to real-world environments on the SPIDER 2.0-lite benchmark.\nExperiments show that LinkAlign outperforms existing baselines in\nmulti-database settings, demonstrating its effectiveness and robustness. On the\nother hand, our method ranks highest among models excluding those using long\nchain-of-thought reasoning LLMs. This work bridges the gap between current\nresearch and real-world scenarios, providing a practical solution for robust\nand scalable schema linking. The codes are available at\nhttps://github.com/Satissss/LinkAlign."}
{"id": "2503.17973", "pdf": "https://arxiv.org/pdf/2503.17973", "abs": "https://arxiv.org/abs/2503.17973", "authors": ["Hanxiao Jiang", "Hao-Yu Hsu", "Kaifeng Zhang", "Hsin-Ni Yu", "Shenlong Wang", "Yunzhu Li"], "title": "PhysTwin: Physics-Informed Reconstruction and Simulation of Deformable Objects from Videos", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": "Project Page: https://jianghanxiao.github.io/phystwin-web/", "summary": "Creating a physical digital twin of a real-world object has immense potential\nin robotics, content creation, and XR. In this paper, we present PhysTwin, a\nnovel framework that uses sparse videos of dynamic objects under interaction to\nproduce a photo- and physically realistic, real-time interactive virtual\nreplica. Our approach centers on two key components: (1) a physics-informed\nrepresentation that combines spring-mass models for realistic physical\nsimulation, generative shape models for geometry, and Gaussian splats for\nrendering; and (2) a novel multi-stage, optimization-based inverse modeling\nframework that reconstructs complete geometry, infers dense physical\nproperties, and replicates realistic appearance from videos. Our method\nintegrates an inverse physics framework with visual perception cues, enabling\nhigh-fidelity reconstruction even from partial, occluded, and limited\nviewpoints. PhysTwin supports modeling various deformable objects, including\nropes, stuffed animals, cloth, and delivery packages. Experiments show that\nPhysTwin outperforms competing methods in reconstruction, rendering, future\nprediction, and simulation under novel interactions. We further demonstrate its\napplications in interactive real-time simulation and model-based robotic motion\nplanning."}
{"id": "2503.18603", "pdf": "https://arxiv.org/pdf/2503.18603", "abs": "https://arxiv.org/abs/2503.18603", "authors": ["Jong Myoung Kim", "Young-Jun Lee", "Ho-Jin Choi", "Sangkeun Jung"], "title": "LANGALIGN: Enhancing Non-English Language Models via Cross-Lingual Embedding Alignment", "categories": ["cs.CL"], "comment": "now preparing", "summary": "While Large Language Models have gained attention, many service developers\nstill rely on embedding-based models due to practical constraints. In such\ncases, the quality of fine-tuning data directly impacts performance, and\nEnglish datasets are often used as seed data for training non-English models.\nIn this study, we propose LANGALIGN, which enhances target language processing\nby aligning English embedding vectors with those of the target language at the\ninterface between the language model and the task header. Experiments on\nKorean, Japanese, and Chinese demonstrate that LANGALIGN significantly improves\nperformance across all three languages. Additionally, we show that LANGALIGN\ncan be applied in reverse to convert target language data into a format that an\nEnglish-based model can process."}
{"id": "2503.17975", "pdf": "https://arxiv.org/pdf/2503.17975", "abs": "https://arxiv.org/abs/2503.17975", "authors": ["Yuzhi Li", "Haojun Xu", "Feng Tian"], "title": "Shot Sequence Ordering for Video Editing: Benchmarks, Metrics, and Cinematology-Inspired Computing Methods", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "With the rising popularity of short video platforms, the demand for video\nproduction has increased substantially. However, high-quality video creation\ncontinues to rely heavily on professional editing skills and a nuanced\nunderstanding of visual language. To address this challenge, the Shot Sequence\nOrdering (SSO) task in AI-assisted video editing has emerged as a pivotal\napproach for enhancing video storytelling and the overall viewing experience.\nNevertheless, the progress in this field has been impeded by a lack of publicly\navailable benchmark datasets. In response, this paper introduces two novel\nbenchmark datasets, AVE-Order and ActivityNet-Order. Additionally, we employ\nthe Kendall Tau distance as an evaluation metric for the SSO task and propose\nthe Kendall Tau Distance-Cross Entropy Loss. We further introduce the concept\nof Cinematology Embedding, which incorporates movie metadata and shot labels as\nprior knowledge into the SSO model, and constructs the AVE-Meta dataset to\nvalidate the method's effectiveness. Experimental results indicate that the\nproposed loss function and method substantially enhance SSO task accuracy. All\ndatasets are publicly accessible at https://github.com/litchiar/ShotSeqBench."}
{"id": "2503.18646", "pdf": "https://arxiv.org/pdf/2503.18646", "abs": "https://arxiv.org/abs/2503.18646", "authors": ["Zhen-Song Chen", "Hong-Wei Ding", "Xian-Jia Wang", "Witold Pedrycz"], "title": "ZeroLM: Data-Free Transformer Architecture Search for Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Neural architecture search (NAS) provides a systematic framework for\nautomating the design of neural network architectures, yet its widespread\nadoption is hindered by prohibitive computational requirements. Existing\nzero-cost proxy methods, while reducing search overhead, demonstrate inadequate\nperformance in architecture ranking tasks, particularly for Transformer-based\nmodels where they often underperform simple parameter counting metrics. Current\nautomated proxy discovery approaches suffer from extended search times,\nsusceptibility to data overfitting, and structural complexity. This paper\nintroduces a novel zero-cost proxy methodology that quantifies model capacity\nthrough efficient weight statistics computation while decomposing Transformer\narchitectures into functionally distinct sub-modules, thereby optimizing the\nbalance of their contributions to overall performance. Our comprehensive\nevaluation demonstrates the superiority of this approach, achieving a\nSpearman's rho of 0.76 and Kendall's tau of 0.53 on the FlexiBERT benchmark.\nThe proposed method exhibits exceptional computational efficiency while\nmaintaining robust performance across diverse NAS benchmark tasks, offering a\npractical solution for large-scale architecture search."}
{"id": "2503.17978", "pdf": "https://arxiv.org/pdf/2503.17978", "abs": "https://arxiv.org/abs/2503.17978", "authors": ["Dominique Nshimyimana", "Vitor Fortes Rey", "Sungho Suh", "Bo Zhou", "Paul Lukowicz"], "title": "PIM: Physics-Informed Multi-task Pre-training for Improving Inertial Sensor-Based Human Activity Recognition", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Human activity recognition (HAR) with deep learning models relies on large\namounts of labeled data, often challenging to obtain due to associated cost,\ntime, and labor. Self-supervised learning (SSL) has emerged as an effective\napproach to leverage unlabeled data through pretext tasks, such as masked\nreconstruction and multitask learning with signal processing-based data\naugmentations, to pre-train encoder models. However, such methods are often\nderived from computer vision approaches that disregard physical mechanisms and\nconstraints that govern wearable sensor data and the phenomena they reflect. In\nthis paper, we propose a physics-informed multi-task pre-training (PIM)\nframework for IMU-based HAR. PIM generates pre-text tasks based on the\nunderstanding of basic physical aspects of human motion: including movement\nspeed, angles of movement, and symmetry between sensor placements. Given a\nsensor signal, we calculate corresponding features using physics-based\nequations and use them as pretext tasks for SSL. This enables the model to\ncapture fundamental physical characteristics of human activities, which is\nespecially relevant for multi-sensor systems. Experimental evaluations on four\nHAR benchmark datasets demonstrate that the proposed method outperforms\nexisting state-of-the-art methods, including data augmentation and masked\nreconstruction, in terms of accuracy and F1 score. We have observed gains of\nalmost 10\\% in macro f1 score and accuracy with only 2 to 8 labeled examples\nper class and up to 3% when there is no reduction in the amount of training\ndata."}
{"id": "2503.18681", "pdf": "https://arxiv.org/pdf/2503.18681", "abs": "https://arxiv.org/abs/2503.18681", "authors": ["Yazhou Zhang", "Chunwang Zou", "Bo Wang", "Jing Qin"], "title": "Commander-GPT: Fully Unleashing the Sarcasm Detection Capability of Multi-Modal Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Sarcasm detection, as a crucial research direction in the field of Natural\nLanguage Processing (NLP), has attracted widespread attention. Traditional\nsarcasm detection tasks have typically focused on single-modal approaches\n(e.g., text), but due to the implicit and subtle nature of sarcasm, such\nmethods often fail to yield satisfactory results. In recent years, researchers\nhave shifted the focus of sarcasm detection to multi-modal approaches. However,\neffectively leveraging multi-modal information to accurately identify sarcastic\ncontent remains a challenge that warrants further exploration. Leveraging the\npowerful integrated processing capabilities of Multi-Modal Large Language\nModels (MLLMs) for various information sources, we propose an innovative\nmulti-modal Commander-GPT framework. Inspired by military strategy, we first\ndecompose the sarcasm detection task into six distinct sub-tasks. A central\ncommander (decision-maker) then assigns the best-suited large language model to\naddress each specific sub-task. Ultimately, the detection results from each\nmodel are aggregated to identify sarcasm. We conducted extensive experiments on\nMMSD and MMSD 2.0, utilizing four multi-modal large language models and six\nprompting strategies. Our experiments demonstrate that our approach achieves\nstate-of-the-art performance, with a 19.3% improvement in F1 score, without\nnecessitating fine-tuning or ground-truth rationales."}
{"id": "2503.17982", "pdf": "https://arxiv.org/pdf/2503.17982", "abs": "https://arxiv.org/abs/2503.17982", "authors": ["Yara AlaaEldin", "Francesca Odone"], "title": "Co-SemDepth: Fast Joint Semantic Segmentation and Depth Estimation on Aerial Images", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Understanding the geometric and semantic properties of the scene is crucial\nin autonomous navigation and particularly challenging in the case of Unmanned\nAerial Vehicle (UAV) navigation. Such information may be by obtained by\nestimating depth and semantic segmentation maps of the surrounding environment\nand for their practical use in autonomous navigation, the procedure must be\nperformed as close to real-time as possible. In this paper, we leverage\nmonocular cameras on aerial robots to predict depth and semantic maps in\nlow-altitude unstructured environments. We propose a joint deep-learning\narchitecture that can perform the two tasks accurately and rapidly, and\nvalidate its effectiveness on MidAir and Aeroscapes benchmark datasets. Our\njoint-architecture proves to be competitive or superior to the other single and\njoint architecture methods while performing its task fast predicting 20.2 FPS\non a single NVIDIA quadro p5000 GPU and it has a low memory footprint. All\ncodes for training and prediction can be found on this link:\nhttps://github.com/Malga-Vision/Co-SemDepth"}
{"id": "2503.18702", "pdf": "https://arxiv.org/pdf/2503.18702", "abs": "https://arxiv.org/abs/2503.18702", "authors": ["David Ph. Shakouri", "Crit Cremers", "Niels O. Schiller"], "title": "Unsupervised Acquisition of Discrete Grammatical Categories", "categories": ["cs.CL", "I.2.6; I.2.7; J.5"], "comment": "34 pages, 3 figures, 7 tables", "summary": "This article presents experiments performed using a computational laboratory\nenvironment for language acquisition experiments. It implements a multi-agent\nsystem consisting of two agents: an adult language model and a daughter\nlanguage model that aims to learn the mother language. Crucially, the daughter\nagent does not have access to the internal knowledge of the mother language\nmodel but only to the language exemplars the mother agent generates. These\nexperiments illustrate how this system can be used to acquire abstract\ngrammatical knowledge. We demonstrate how statistical analyses of patterns in\nthe input data corresponding to grammatical categories yield discrete\ngrammatical rules. These rules are subsequently added to the grammatical\nknowledge of the daughter language model. To this end, hierarchical\nagglomerative cluster analysis was applied to the utterances consecutively\ngenerated by the mother language model. It is argued that this procedure can be\nused to acquire structures resembling grammatical categories proposed by\nlinguists for natural languages. Thus, it is established that non-trivial\ngrammatical knowledge has been acquired. Moreover, the parameter configuration\nof this computational laboratory environment determined using training data\ngenerated by the mother language model is validated in a second experiment with\na test set similarly resulting in the acquisition of non-trivial categories."}
{"id": "2503.17983", "pdf": "https://arxiv.org/pdf/2503.17983", "abs": "https://arxiv.org/abs/2503.17983", "authors": ["Baizhi Wang", "Rui Yan", "Wenxin Ma", "Xu Zhang", "Yuhao Wang", "Xiaolong Li", "Yunjie Gu", "Zihang Jiang", "S. Kevin Zhou"], "title": "Histomorphology-driven multi-instance learning for breast cancer WSI classification", "categories": ["cs.CV"], "comment": "10 pages,5 figures", "summary": "Histomorphology is crucial in breast cancer diagnosis. However, existing\nwhole slide image (WSI) classification methods struggle to effectively\nincorporate histomorphology information, limiting their ability to capture key\nand fine-grained pathological features. To address this limitation, we propose\na novel framework that explicitly incorporates histomorphology (tumor\ncellularity, cellular morphology, and tissue architecture) into WSI\nclassification. Specifically, our approach consists of three key components:\n(1) estimating the importance of tumor-related histomorphology information at\nthe patch level based on medical prior knowledge; (2) generating representative\ncluster-level features through histomorphology-driven cluster pooling; and (3)\nenabling WSI-level classification through histomorphology-driven multi-instance\naggregation. With the incorporation of histomorphological information, our\nframework strengthens the model's ability to capture key and fine-grained\npathological patterns, thereby enhancing WSI classification performance.\nExperimental results demonstrate its effectiveness, achieving high diagnostic\naccuracy for molecular subtyping and cancer subtyping. The code will be made\navailable at https://github.com/Badgewho/HMDMIL."}
{"id": "2503.18730", "pdf": "https://arxiv.org/pdf/2503.18730", "abs": "https://arxiv.org/abs/2503.18730", "authors": ["Hongkuan Zhou", "Stefan Schmid", "Yicong Li", "Lavdim Halilaj", "Xiangtong Yao", "Wei cao"], "title": "Predicting the Road Ahead: A Knowledge Graph based Foundation Model for Scene Understanding in Autonomous Driving", "categories": ["cs.CL"], "comment": null, "summary": "The autonomous driving field has seen remarkable advancements in various\ntopics, such as object recognition, trajectory prediction, and motion planning.\nHowever, current approaches face limitations in effectively comprehending the\ncomplex evolutions of driving scenes over time. This paper proposes FM4SU, a\nnovel methodology for training a symbolic foundation model (FM) for scene\nunderstanding in autonomous driving. It leverages knowledge graphs (KGs) to\ncapture sensory observation along with domain knowledge such as road topology,\ntraffic rules, or complex interactions between traffic participants. A bird's\neye view (BEV) symbolic representation is extracted from the KG for each\ndriving scene, including the spatio-temporal information among the objects\nacross the scenes. The BEV representation is serialized into a sequence of\ntokens and given to pre-trained language models (PLMs) for learning an inherent\nunderstanding of the co-occurrence among driving scene elements and generating\npredictions on the next scenes. We conducted a number of experiments using the\nnuScenes dataset and KG in various scenarios. The results demonstrate that\nfine-tuned models achieve significantly higher accuracy in all tasks. The\nfine-tuned T5 model achieved a next scene prediction accuracy of 86.7%. This\npaper concludes that FM4SU offers a promising foundation for developing more\ncomprehensive models for scene understanding in autonomous driving."}
{"id": "2503.17984", "pdf": "https://arxiv.org/pdf/2503.17984", "abs": "https://arxiv.org/abs/2503.17984", "authors": ["Maochen Yang", "Zekun Li", "Jian Zhang", "Lei Qi", "Yinghuan Shi"], "title": "Taste More, Taste Better: Diverse Data and Strong Model Boost Semi-Supervised Crowd Counting", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by CVPR 2025", "summary": "Semi-supervised crowd counting is crucial for addressing the high annotation\ncosts of densely populated scenes. Although several methods based on\npseudo-labeling have been proposed, it remains challenging to effectively and\naccurately utilize unlabeled data. In this paper, we propose a novel framework\ncalled Taste More Taste Better (TMTB), which emphasizes both data and model\naspects. Firstly, we explore a data augmentation technique well-suited for the\ncrowd counting task. By inpainting the background regions, this technique can\neffectively enhance data diversity while preserving the fidelity of the entire\nscenes. Secondly, we introduce the Visual State Space Model as backbone to\ncapture the global context information from crowd scenes, which is crucial for\nextremely crowded, low-light, and adverse weather scenarios. In addition to the\ntraditional regression head for exact prediction, we employ an Anti-Noise\nclassification head to provide less exact but more accurate supervision, since\nthe regression head is sensitive to noise in manual annotations. We conduct\nextensive experiments on four benchmark datasets and show that our method\noutperforms state-of-the-art methods by a large margin. Code is publicly\navailable on https://github.com/syhien/taste_more_taste_better."}
{"id": "2503.18751", "pdf": "https://arxiv.org/pdf/2503.18751", "abs": "https://arxiv.org/abs/2503.18751", "authors": ["Wesley Scivetti", "Nathan Schneider"], "title": "Construction Identification and Disambiguation Using BERT: A Case Study of NPN", "categories": ["cs.CL", "cs.AI"], "comment": "8 pages, ACL long-paper format (preprint)", "summary": "Construction Grammar hypothesizes that knowledge of a language consists\nchiefly of knowledge of form-meaning pairs (''constructions'') that include\nvocabulary, general grammar rules, and even idiosyncratic patterns. Recent work\nhas shown that transformer language models represent at least some\nconstructional patterns, including ones where the construction is rare overall.\nIn this work, we probe BERT's representation of the form and meaning of a minor\nconstruction of English, the NPN (noun-preposition-noun) construction --\nexhibited in such expressions as face to face and day to day -- which is known\nto be polysemous. We construct a benchmark dataset of semantically annotated\ncorpus instances (including distractors that superficially resemble the\nconstruction). With this dataset, we train and evaluate probing classifiers.\nThey achieve decent discrimination of the construction from distractors, as\nwell as sense disambiguation among true instances of the construction,\nrevealing that BERT embeddings carry indications of the construction's\nsemantics. Moreover, artificially permuting the word order of true construction\ninstances causes them to be rejected, indicating sensitivity to matters of\nform. We conclude that BERT does latently encode at least some knowledge of the\nNPN construction going beyond a surface syntactic pattern and lexical cues."}
{"id": "2503.17992", "pdf": "https://arxiv.org/pdf/2503.17992", "abs": "https://arxiv.org/abs/2503.17992", "authors": ["Xueying Liu", "Lianfang Wang", "Jun Liu", "Yong Wang", "Yuping Duan"], "title": "Geometric Constrained Non-Line-of-Sight Imaging", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Normal reconstruction is crucial in non-line-of-sight (NLOS) imaging, as it\nprovides key geometric and lighting information about hidden objects, which\nsignificantly improves reconstruction accuracy and scene understanding.\nHowever, jointly estimating normals and albedo expands the problem from\nmatrix-valued functions to tensor-valued functions that substantially\nincreasing complexity and computational difficulty. In this paper, we propose a\nnovel joint albedo-surface reconstruction method, which utilizes the Frobenius\nnorm of the shape operator to control the variation rate of the normal field.\nIt is the first attempt to apply regularization methods to the reconstruction\nof surface normals for hidden objects. By improving the accuracy of the normal\nfield, it enhances detail representation and achieves high-precision\nreconstruction of hidden object geometry. The proposed method demonstrates\nrobustness and effectiveness on both synthetic and experimental datasets. On\ntransient data captured within 15 seconds, our surface normal-regularized\nreconstruction model produces more accurate surfaces than recently proposed\nmethods and is 30 times faster than the existing surface reconstruction\napproach."}
{"id": "2503.18760", "pdf": "https://arxiv.org/pdf/2503.18760", "abs": "https://arxiv.org/abs/2503.18760", "authors": ["Nick McKenna", "Xinnuo Xu", "Jack Williams", "Nick Wilson", "Benjamin Van Durme", "Christian Poelitz"], "title": "Synthetic Function Demonstrations Improve Generation in Low-Resource Programming Languages", "categories": ["cs.CL"], "comment": null, "summary": "A key consideration when training an LLM is whether the target language is\nmore or less resourced, whether this is English compared to Welsh, or Python\ncompared to Excel. Typical training data for programming languages consist of\nreal program demonstrations coupled with human-written comments. Here we\npresent novel approaches to the creation of such data for low resource\nprogramming languages. We generate fully-synthetic, textbook-quality\ndemonstrations of common library functions in an example domain of Excel\nformulas, using a teacher model. We then finetune an underperforming student\nmodel, and show improvement on 2 question-answering datasets recast into the\nExcel domain. We show advantages of finetuning over standard, off-the-shelf RAG\napproaches, which can offer only modest improvement due to the unfamiliar\ntarget domain."}
{"id": "2503.18007", "pdf": "https://arxiv.org/pdf/2503.18007", "abs": "https://arxiv.org/abs/2503.18007", "authors": ["Hongyu Yan", "Zijun Li", "Kunming Luo", "Li Lu", "Ping Tan"], "title": "SymmCompletion: High-Fidelity and High-Consistency Point Cloud Completion with Symmetry Guidance", "categories": ["cs.CV"], "comment": "Accepted by AAAI 2025 (Oral presentation), Code:\n  https://github.com/HongyuYann/SymmCompletion", "summary": "Point cloud completion aims to recover a complete point shape from a partial\npoint cloud. Although existing methods can form satisfactory point clouds in\nglobal completeness, they often lose the original geometry details and face the\nproblem of geometric inconsistency between existing point clouds and\nreconstructed missing parts. To tackle this problem, we introduce\nSymmCompletion, a highly effective completion method based on symmetry\nguidance. Our method comprises two primary components: a Local Symmetry\nTransformation Network (LSTNet) and a Symmetry-Guidance Transformer (SGFormer).\nFirst, LSTNet efficiently estimates point-wise local symmetry transformation to\ntransform key geometries of partial inputs into missing regions, thereby\ngenerating geometry-align partial-missing pairs and initial point clouds.\nSecond, SGFormer leverages the geometric features of partial-missing pairs as\nthe explicit symmetric guidance that can constrain the refinement process for\ninitial point clouds. As a result, SGFormer can exploit provided priors to form\nhigh-fidelity and geometry-consistency final point clouds. Qualitative and\nquantitative evaluations on several benchmark datasets demonstrate that our\nmethod outperforms state-of-the-art completion networks."}
{"id": "2503.18769", "pdf": "https://arxiv.org/pdf/2503.18769", "abs": "https://arxiv.org/abs/2503.18769", "authors": ["Alan Dao", "Dinh Bach Vu", "Bui Quang Huy"], "title": "AlphaSpace: Enabling Robotic Actions through Semantic Tokenization and Symbolic Reasoning", "categories": ["cs.CL", "cs.RO"], "comment": null, "summary": "This paper presents AlphaSpace, a novel methodology designed to enhance the\nspatial reasoning capabilities of large language models (LLMs) for 3D Cartesian\nspace navigation. AlphaSpace employs a semantics-based tokenization strategy,\nencoding height information through specialized semantic tokens, and integrates\nprimarily symbolic synthetic reasoning data. This approach enables LLMs to\naccurately manipulate objects by positioning them at specific [x, y, z]\ncoordinates. Experimental results demonstrate that AlphaSpace significantly\noutperforms existing models on manipulation subtasks, achieving a total\naccuracy of 66.67%, compared to 37.5% for GPT-4o and 29.17% for Claude 3.5\nSonnet."}
{"id": "2503.18010", "pdf": "https://arxiv.org/pdf/2503.18010", "abs": "https://arxiv.org/abs/2503.18010", "authors": ["Thomas Dagès", "Simon Weber", "Ya-Wei Eileen Lin", "Ronen Talmon", "Daniel Cremers", "Michael Lindenbaum", "Alfred M. Bruckstein", "Ron Kimmel"], "title": "Finsler Multi-Dimensional Scaling: Manifold Learning for Asymmetric Dimensionality Reduction and Embedding", "categories": ["cs.CV"], "comment": "Accepted for publication at the IEEE/CVF Conference on Computer\n  Vision and Pattern Recognition (CVPR) 2025", "summary": "Dimensionality reduction is a fundamental task that aims to simplify complex\ndata by reducing its feature dimensionality while preserving essential\npatterns, with core applications in data analysis and visualisation. To\npreserve the underlying data structure, multi-dimensional scaling (MDS) methods\nfocus on preserving pairwise dissimilarities, such as distances. They optimise\nthe embedding to have pairwise distances as close as possible to the data\ndissimilarities. However, the current standard is limited to embedding data in\nRiemannian manifolds. Motivated by the lack of asymmetry in the Riemannian\nmetric of the embedding space, this paper extends the MDS problem to a natural\nasymmetric generalisation of Riemannian manifolds called Finsler manifolds.\nInspired by Euclidean space, we define a canonical Finsler space for embedding\nasymmetric data. Due to its simplicity with respect to geodesics, data\nrepresentation in this space is both intuitive and simple to analyse. We\ndemonstrate that our generalisation benefits from the same theoretical\nconvergence guarantees. We reveal the effectiveness of our Finsler embedding\nacross various types of non-symmetric data, highlighting its value in\napplications such as data visualisation, dimensionality reduction, directed\ngraph embedding, and link prediction."}
{"id": "2503.18878", "pdf": "https://arxiv.org/pdf/2503.18878", "abs": "https://arxiv.org/abs/2503.18878", "authors": ["Andrey Galichin", "Alexey Dontsov", "Polina Druzhinina", "Anton Razzhigaev", "Oleg Y. Rogov", "Elena Tutubalina", "Ivan Oseledets"], "title": "I Have Covered All the Bases Here: Interpreting Reasoning Features in Large Language Models via Sparse Autoencoders", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have achieved remarkable success in natural\nlanguage processing. Recent advances have led to the developing of a new class\nof reasoning LLMs; for example, open-source DeepSeek-R1 has achieved\nstate-of-the-art performance by integrating deep thinking and complex\nreasoning. Despite these impressive capabilities, the internal reasoning\nmechanisms of such models remain unexplored. In this work, we employ Sparse\nAutoencoders (SAEs), a method to learn a sparse decomposition of latent\nrepresentations of a neural network into interpretable features, to identify\nfeatures that drive reasoning in the DeepSeek-R1 series of models. First, we\npropose an approach to extract candidate ''reasoning features'' from SAE\nrepresentations. We validate these features through empirical analysis and\ninterpretability methods, demonstrating their direct correlation with the\nmodel's reasoning abilities. Crucially, we demonstrate that steering these\nfeatures systematically enhances reasoning performance, offering the first\nmechanistic account of reasoning in LLMs. Code available at\nhttps://github.com/AIRI-Institute/SAE-Reasoning"}
{"id": "2503.18013", "pdf": "https://arxiv.org/pdf/2503.18013", "abs": "https://arxiv.org/abs/2503.18013", "authors": ["Yufei Zhan", "Yousong Zhu", "Shurong Zheng", "Hongyin Zhao", "Fan Yang", "Ming Tang", "Jinqiao Wang"], "title": "Vision-R1: Evolving Human-Free Alignment in Large Vision-Language Models via Vision-Guided Reinforcement Learning", "categories": ["cs.CV", "cs.AI"], "comment": "Project in development. Github:\n  https://github.com/jefferyZhan/Griffon/tree/master/Vision-R1", "summary": "Large Vision-Language Models (LVLMs) typically follow a two-stage training\nparadigm-pretraining and supervised fine-tuning. Recently, preference\noptimization, derived from the language domain, has emerged as an effective\npost-training reinforcement strategy to enhance capabilities of LVLMs. However,\nconstructing high-quality human-annotated preference data and developing robust\nreward models to mimic these preferences are both costly and challenging.\nMotivated by this observation, we propose Vision-R1, a novel vision-guided\nR1-like reinforcement learning algorithm for LVLMs that rewards models with\ndefinitive vision feedback. It only leverages curated instruction data,\neliminating the need for specialized reward models and handcrafted preference\ndatasets. We incorporate a criterion-driven reward function that further\nintegrates multi-dimensional feedback to evaluate model completions\ncomprehensively based on the vision task logic. Furthermore, we introduce a\nprogressive rule refinement strategy that dynamically adjusts the reward\ncriteria during training, enabling continuous model improvement and mitigating\nreward hacking. Extensive experiments on both in-distribution and\nout-of-distribution benchmarks demonstrate that fine-tuning the 7B LVLMs with\nVision-R1 achieves consistent performance gains, with even up to 50%\nimprovement and surpassing the state-of-the-art 10x size model."}
{"id": "2503.18891", "pdf": "https://arxiv.org/pdf/2503.18891", "abs": "https://arxiv.org/abs/2503.18891", "authors": ["Zhexuan Wang", "Yutong Wang", "Xuebo Liu", "Liang Ding", "Miao Zhang", "Jie Liu", "Min Zhang"], "title": "AgentDropout: Dynamic Agent Elimination for Token-Efficient and High-Performance LLM-Based Multi-Agent Collaboration", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Multi-agent systems (MAS) based on large language models (LLMs) have\ndemonstrated significant potential in collaborative problem-solving. However,\nthey still face substantial challenges of low communication efficiency and\nsuboptimal task performance, making the careful design of the agents'\ncommunication topologies particularly important. Inspired by the management\ntheory that roles in an efficient team are often dynamically adjusted, we\npropose AgentDropout, which identifies redundant agents and communication\nacross different communication rounds by optimizing the adjacency matrices of\nthe communication graphs and eliminates them to enhance both token efficiency\nand task performance. Compared to state-of-the-art methods, AgentDropout\nachieves an average reduction of 21.6% in prompt token consumption and 18.4% in\ncompletion token consumption, along with a performance improvement of 1.14 on\nthe tasks. Furthermore, the extended experiments demonstrate that AgentDropout\nachieves notable domain transferability and structure robustness, revealing its\nreliability and effectiveness. We release our code at\nhttps://github.com/wangzx1219/AgentDropout."}
{"id": "2503.18016", "pdf": "https://arxiv.org/pdf/2503.18016", "abs": "https://arxiv.org/abs/2503.18016", "authors": ["Xu Zheng", "Ziqiao Weng", "Yuanhuiyi Lyu", "Lutao Jiang", "Haiwei Xue", "Bin Ren", "Danda Paudel", "Nicu Sebe", "Luc Van Gool", "Xuming Hu"], "title": "Retrieval Augmented Generation and Understanding in Vision: A Survey and New Outlook", "categories": ["cs.CV"], "comment": "19 pages, 10 figures", "summary": "Retrieval-augmented generation (RAG) has emerged as a pivotal technique in\nartificial intelligence (AI), particularly in enhancing the capabilities of\nlarge language models (LLMs) by enabling access to external, reliable, and\nup-to-date knowledge sources. In the context of AI-Generated Content (AIGC),\nRAG has proven invaluable by augmenting model outputs with supplementary,\nrelevant information, thus improving their quality. Recently, the potential of\nRAG has extended beyond natural language processing, with emerging methods\nintegrating retrieval-augmented strategies into the computer vision (CV)\ndomain. These approaches aim to address the limitations of relying solely on\ninternal model knowledge by incorporating authoritative external knowledge\nbases, thereby improving both the understanding and generation capabilities of\nvision models. This survey provides a comprehensive review of the current state\nof retrieval-augmented techniques in CV, focusing on two main areas: (I) visual\nunderstanding and (II) visual generation. In the realm of visual understanding,\nwe systematically review tasks ranging from basic image recognition to complex\napplications such as medical report generation and multimodal question\nanswering. For visual content generation, we examine the application of RAG in\ntasks related to image, video, and 3D generation. Furthermore, we explore\nrecent advancements in RAG for embodied AI, with a particular focus on\napplications in planning, task execution, multimodal perception, interaction,\nand specialized domains. Given that the integration of retrieval-augmented\ntechniques in CV is still in its early stages, we also highlight the key\nlimitations of current approaches and propose future research directions to\ndrive the development of this promising area."}
{"id": "2503.18893", "pdf": "https://arxiv.org/pdf/2503.18893", "abs": "https://arxiv.org/abs/2503.18893", "authors": ["Chi-Chih Chang", "Chien-Yu Lin", "Yash Akhauri", "Wei-Cheng Lin", "Kai-Chiang Wu", "Luis Ceze", "Mohamed S. Abdelfattah"], "title": "xKV: Cross-Layer SVD for KV-Cache Compression", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) with long context windows enable powerful\napplications but come at the cost of high memory consumption to store the Key\nand Value states (KV-Cache). Recent studies attempted to merge KV-cache from\nmultiple layers into shared representations, yet these approaches either\nrequire expensive pretraining or rely on assumptions of high per-token cosine\nsimilarity across layers which generally does not hold in practice. We find\nthat the dominant singular vectors are remarkably well-aligned across multiple\nlayers of the KV-Cache. Exploiting this insight, we propose xKV, a simple\npost-training method that applies Singular Value Decomposition (SVD) on the\nKV-Cache of grouped layers. xKV consolidates the KV-Cache of multiple layers\ninto a shared low-rank subspace, significantly reducing KV-Cache sizes. Through\nextensive evaluations on the RULER long-context benchmark with widely-used LLMs\n(e.g., Llama-3.1 and Qwen2.5), xKV achieves up to 6.8x higher compression rates\nthan state-of-the-art inter-layer technique while improving accuracy by 2.7%.\nMoreover, xKV is compatible with the emerging Multi-Head Latent Attention (MLA)\n(e.g., DeepSeek-Coder-V2), yielding a notable 3x compression rates on coding\ntasks without performance degradation. These results highlight xKV's strong\ncapability and versatility in addressing memory bottlenecks for long-context\nLLM inference. Our code is publicly available at:\nhttps://github.com/abdelfattah-lab/xKV."}
{"id": "2503.18033", "pdf": "https://arxiv.org/pdf/2503.18033", "abs": "https://arxiv.org/abs/2503.18033", "authors": ["Dvir Samuel", "Matan Levy", "Nir Darshan", "Gal Chechik", "Rami Ben-Ari"], "title": "OmnimatteZero: Training-free Real-time Omnimatte with Pre-trained Video Diffusion Models", "categories": ["cs.CV"], "comment": null, "summary": "Omnimatte aims to decompose a given video into semantically meaningful\nlayers, including the background and individual objects along with their\nassociated effects, such as shadows and reflections. Existing methods often\nrequire extensive training or costly self-supervised optimization. In this\npaper, we present OmnimatteZero, a training-free approach that leverages\noff-the-shelf pre-trained video diffusion models for omnimatte. It can remove\nobjects from videos, extract individual object layers along with their effects,\nand composite those objects onto new videos. We accomplish this by adapting\nzero-shot image inpainting techniques for video object removal, a task they\nfail to handle effectively out-of-the-box. We then show that self-attention\nmaps capture information about the object and its footprints and use them to\ninpaint the object's effects, leaving a clean background. Additionally, through\nsimple latent arithmetic, object layers can be isolated and recombined\nseamlessly with new video layers to produce new videos. Evaluations show that\nOmnimatteZero not only achieves superior performance in terms of background\nreconstruction but also sets a new record for the fastest Omnimatte approach,\nachieving real-time performance with minimal frame runtime."}
{"id": "2503.16586", "pdf": "https://arxiv.org/pdf/2503.16586", "abs": "https://arxiv.org/abs/2503.16586", "authors": ["Yash Vekaria", "Aurelio Loris Canino", "Jonathan Levitsky", "Alex Ciechonski", "Patricia Callejo", "Anna Maria Mandalari", "Zubair Shafiq"], "title": "Big Help or Big Brother? Auditing Tracking, Profiling, and Personalization in Generative AI Assistants", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CR", "cs.CY", "I.2; I.2.1; I.2.7; H.3.4; K.4; K.4.1; H.1; H.1.2; H.5.2; H.4.3"], "comment": null, "summary": "Generative AI (GenAI) browser assistants integrate powerful capabilities of\nGenAI in web browsers to provide rich experiences such as question answering,\ncontent summarization, and agentic navigation. These assistants, available\ntoday as browser extensions, can not only track detailed browsing activity such\nas search and click data, but can also autonomously perform tasks such as\nfilling forms, raising significant privacy concerns. It is crucial to\nunderstand the design and operation of GenAI browser extensions, including how\nthey collect, store, process, and share user data. To this end, we study their\nability to profile users and personalize their responses based on explicit or\ninferred demographic attributes and interests of users. We perform network\ntraffic analysis and use a novel prompting framework to audit tracking,\nprofiling, and personalization by the ten most popular GenAI browser assistant\nextensions. We find that instead of relying on local in-browser models, these\nassistants largely depend on server-side APIs, which can be auto-invoked\nwithout explicit user interaction. When invoked, they collect and share webpage\ncontent, often the full HTML DOM and sometimes even the user's form inputs,\nwith their first-party servers. Some assistants also share identifiers and user\nprompts with third-party trackers such as Google Analytics. The collection and\nsharing continues even if a webpage contains sensitive information such as\nhealth or personal information such as name or SSN entered in a web form. We\nfind that several GenAI browser assistants infer demographic attributes such as\nage, gender, income, and interests and use this profile--which carries across\nbrowsing contexts--to personalize responses. In summary, our work shows that\nGenAI browser assistants can and do collect personal and sensitive information\nfor profiling and personalization with little to no safeguards."}
{"id": "2503.18034", "pdf": "https://arxiv.org/pdf/2503.18034", "abs": "https://arxiv.org/abs/2503.18034", "authors": ["Qiao Liang", "Yanjiang Liu", "Ben He", "Yaojie Lu", "Hongyu Lin", "Jia Zheng", "Xianpei Han", "Le Sun", "Yingfei Sun"], "title": "Expanding the Boundaries of Vision Prior Knowledge in Multi-modal Large Language Models", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Does the prior knowledge of the vision encoder constrain the capability\nboundary of Multi-modal Large Language Models (MLLMs)? While most existing\nresearch treats MLLMs as unified systems optimized through end-to-end training,\nthe impact of vision encoder's prior knowledge is seldom investigated. In this\nwork, we introduce a novel metric, $Rank_e$, to quantify the effect of the\nvision encoder's prior knowledge on MLLM performance. Our analysis reveals a\npositive correlation between prior knowledge and MLLM performance. Moreover, we\nfind that domain-specific fine-tuning using solely end-to-end visual question\nanswering (VQA) data is insufficient--particularly for entities with low\ninherent visual prior knowledge. To address this issue, we propose VisPRE\n(Vision Prior Remediation), a two-stage training framework that explicitly\nincorporates prior knowledge at the vision encoder level. Experimental results\ndemonstrate that augmenting vision encoder's prior knowledge substantially\nboosts the visual understanding capabilities of MLLMs, offering a novel and\neffective strategy for improving performance, especially in scenarios involving\nuncommon visual entities."}
{"id": "2503.17382", "pdf": "https://arxiv.org/pdf/2503.17382", "abs": "https://arxiv.org/abs/2503.17382", "authors": ["Andrew Kiruluta", "Andreas Lemos"], "title": "State Fourier Diffusion Language Model (SFDLM): A Scalable, Novel Iterative Approach to Language Modeling", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "In recent years, diffusion based methods have emerged as a powerful paradigm\nfor generative modeling. Although discrete diffusion for natural language\nprocessing has been explored to a lesser extent, it shows promise for tasks\nrequiring iterative denoising of token based data. In standard approaches to\ntext generation, transformers dominate, but their reliance on self attention\noften incurs high computational costs. This paper introduces a fully diffusion\ndriven discrete text generation model built without any transformer or large\nconvolution modules. Instead, the model integrates structured state space\ndynamics in the time domain with a novel Complex Fourier Multi Layer Perceptron\nmodule that operates in the frequency domain. The forward noising process\nrandomly samples the vocabulary to replace tokens with a controlled\nprobability, while the learned reverse model systematically reverts corrupted\nsequences toward their original states. By composing local state space updates\nwith global Fourier based mixing, the approach effectively captures both short\nand long range dependencies."}
{"id": "2503.18035", "pdf": "https://arxiv.org/pdf/2503.18035", "abs": "https://arxiv.org/abs/2503.18035", "authors": ["Tianyi Shang", "Zhenyu Li", "Pengjie Xu", "Zhaojun Deng", "Ruirui Zhang"], "title": "Text-Driven Cross-Modal Place Recognition Method for Remote Sensing Localization", "categories": ["cs.CV"], "comment": "13 pages", "summary": "Environment description-based localization in large-scale point cloud maps\nconstructed through remote sensing is critically significant for the\nadvancement of large-scale autonomous systems, such as delivery robots\noperating in the last mile. However, current approaches encounter challenges\ndue to the inability of point cloud encoders to effectively capture local\ndetails and long-range spatial relationships, as well as a significant modality\ngap between text and point cloud representations. To address these challenges,\nwe present Des4Pos, a novel two-stage text-driven remote sensing localization\nframework. In the coarse stage, the point-cloud encoder utilizes the\nMulti-scale Fusion Attention Mechanism (MFAM) to enhance local geometric\nfeatures, followed by a bidirectional Long Short-Term Memory (LSTM) module to\nstrengthen global spatial relationships. Concurrently, the Stepped Text Encoder\n(STE) integrates cross-modal prior knowledge from CLIP [1] and aligns text and\npoint-cloud features using this prior knowledge, effectively bridging modality\ndiscrepancies. In the fine stage, we introduce a Cascaded Residual Attention\n(CRA) module to fuse cross-modal features and predict relative localization\noffsets, thereby achieving greater localization precision. Experiments on the\nKITTI360Pose test set demonstrate that Des4Pos achieves state-of-the-art\nperformance in text-to-point-cloud place recognition. Specifically, it attains\na top-1 accuracy of 40% and a top-10 accuracy of 77% under a 5-meter radius\nthreshold, surpassing the best existing methods by 7% and 7%, respectively."}
{"id": "2503.17421", "pdf": "https://arxiv.org/pdf/2503.17421", "abs": "https://arxiv.org/abs/2503.17421", "authors": ["Junwei Kuang", "Liang Yang", "Shaoze Cui", "Weiguo Fan"], "title": "Understanding Social Support Needs in Questions: A Hybrid Approach Integrating Semi-Supervised Learning and LLM-based Data Augmentation", "categories": ["cs.CY", "cs.AI", "cs.CL"], "comment": "55 pages", "summary": "Patients are increasingly turning to online health Q&A communities for social\nsupport to improve their well-being. However, when this support received does\nnot align with their specific needs, it may prove ineffective or even\ndetrimental. This necessitates a model capable of identifying the social\nsupport needs in questions. However, training such a model is challenging due\nto the scarcity and class imbalance issues of labeled data. To overcome these\nchallenges, we follow the computational design science paradigm to develop a\nnovel framework, Hybrid Approach for SOcial Support need classification\n(HA-SOS). HA-SOS integrates an answer-enhanced semi-supervised learning\napproach, a text data augmentation technique leveraging large language models\n(LLMs) with reliability- and diversity-aware sample selection mechanism, and a\nunified training process to automatically label social support needs in\nquestions. Extensive empirical evaluations demonstrate that HA-SOS\nsignificantly outperforms existing question classification models and\nalternative semi-supervised learning approaches. This research contributes to\nthe literature on social support, question classification, semi-supervised\nlearning, and text data augmentation. In practice, our HA-SOS framework\nfacilitates online Q&A platform managers and answerers to better understand\nusers' social support needs, enabling them to provide timely, personalized\nanswers and interventions."}
{"id": "2503.18042", "pdf": "https://arxiv.org/pdf/2503.18042", "abs": "https://arxiv.org/abs/2503.18042", "authors": ["Qiang Wang", "Yuhang He", "SongLin Dong", "Xiang Song", "Jizhou Han", "Haoyu Luo", "Yihong Gong"], "title": "DualCP: Rehearsal-Free Domain-Incremental Learning via Dual-Level Concept Prototype", "categories": ["cs.CV"], "comment": "Accepted at AAAI 2025", "summary": "Domain-Incremental Learning (DIL) enables vision models to adapt to changing\nconditions in real-world environments while maintaining the knowledge acquired\nfrom previous domains. Given privacy concerns and training time, Rehearsal-Free\nDIL (RFDIL) is more practical. Inspired by the incremental cognitive process of\nthe human brain, we design Dual-level Concept Prototypes (DualCP) for each\nclass to address the conflict between learning new knowledge and retaining old\nknowledge in RFDIL. To construct DualCP, we propose a Concept Prototype\nGenerator (CPG) that generates both coarse-grained and fine-grained prototypes\nfor each class. Additionally, we introduce a Coarse-to-Fine calibrator (C2F) to\nalign image features with DualCP. Finally, we propose a Dual Dot-Regression\n(DDR) loss function to optimize our C2F module. Extensive experiments on the\nDomainNet, CDDB, and CORe50 datasets demonstrate the effectiveness of our\nmethod."}
{"id": "2503.17438", "pdf": "https://arxiv.org/pdf/2503.17438", "abs": "https://arxiv.org/abs/2503.17438", "authors": ["Paolo Frazzetto", "Muhammad Uzair Ul Haq", "Flavia Fabris", "Alessandro Sperduti"], "title": "From Text to Talent: A Pipeline for Extracting Insights from Candidate Profiles", "categories": ["cs.CY", "cs.CL", "cs.LG"], "comment": "ITADATA 2024", "summary": "The recruitment process is undergoing a significant transformation with the\nincreasing use of machine learning and natural language processing techniques.\nWhile previous studies have focused on automating candidate selection, the role\nof multiple vacancies in this process remains understudied. This paper\naddresses this gap by proposing a novel pipeline that leverages Large Language\nModels and graph similarity measures to suggest ideal candidates for specific\njob openings. Our approach represents candidate profiles as multimodal\nembeddings, enabling the capture of nuanced relationships between job\nrequirements and candidate attributes. The proposed approach has significant\nimplications for the recruitment industry, enabling companies to streamline\ntheir hiring processes and identify top talent more efficiently. Our work\ncontributes to the growing body of research on the application of machine\nlearning in human resources, highlighting the potential of LLMs and graph-based\nmethods in revolutionizing the recruitment landscape."}
{"id": "2503.18052", "pdf": "https://arxiv.org/pdf/2503.18052", "abs": "https://arxiv.org/abs/2503.18052", "authors": ["Yue Li", "Qi Ma", "Runyi Yang", "Huapeng Li", "Mengjiao Ma", "Bin Ren", "Nikola Popovic", "Nicu Sebe", "Ender Konukoglu", "Theo Gevers", "Luc Van Gool", "Martin R. Oswald", "Danda Pani Paudel"], "title": "SceneSplat: Gaussian Splatting-based Scene Understanding with Vision-Language Pretraining", "categories": ["cs.CV"], "comment": "Our code, model, and dataset will be released at\n  https://github.com/unique1i/SceneSplat", "summary": "Recognizing arbitrary or previously unseen categories is essential for\ncomprehensive real-world 3D scene understanding. Currently, all existing\nmethods rely on 2D or textual modalities during training, or together at\ninference. This highlights a clear absence of a model capable of processing 3D\ndata alone for learning semantics end-to-end, along with the necessary data to\ntrain such a model. Meanwhile, 3D Gaussian Splatting (3DGS) has emerged as the\nde facto standard for 3D scene representation across various vision tasks.\nHowever, effectively integrating semantic reasoning into 3DGS in a\ngeneralizable fashion remains an open challenge. To address these limitations\nwe introduce SceneSplat, to our knowledge the first large-scale 3D indoor scene\nunderstanding approach that operates natively on 3DGS. Furthermore, we propose\na self-supervised learning scheme that unlocks rich 3D feature learning from\nunlabeled scenes. In order to power the proposed methods, we introduce\nSceneSplat-7K, the first large-scale 3DGS dataset for indoor scenes, comprising\nof 6868 scenes derived from 7 established datasets like ScanNet, Matterport3D,\netc. Generating SceneSplat-7K required computational resources equivalent to\n119 GPU-days on an L4 GPU, enabling standardized benchmarking for 3DGS-based\nreasoning for indoor scenes. Our exhaustive experiments on SceneSplat-7K\ndemonstrate the significant benefit of the proposed methods over the\nestablished baselines."}
{"id": "2503.17500", "pdf": "https://arxiv.org/pdf/2503.17500", "abs": "https://arxiv.org/abs/2503.17500", "authors": ["Louis Owen", "Abhay Kumar", "Nilabhra Roy Chowdhury", "Fabian Güra"], "title": "Variance Control via Weight Rescaling in LLM Pre-training", "categories": ["cs.LG", "cs.CL", "stat.ML"], "comment": null, "summary": "The outcome of Large Language Model (LLM) pre-training strongly depends on\nweight initialization and variance control strategies. Although the importance\nof initial variance control has been well documented in neural networks in\ngeneral, the literature on initialization and management of its growth during\nLLM pre-training, specifically, is somewhat sparse. In this paper, we introduce\nthe Layer Index Rescaling (LIR) weight initialization scheme, and the Target\nVariance Rescaling (TVR) variance control strategy. Experiments on a 1B\nparameter LLaMA model demonstrate that better variance management using these\ntechniques yields substantial improvements in downstream task performance (up\nto 4.6% on common pre-training benchmarks) and reduces extreme activation\nvalues, thus mitigating challenges associated with quantization and\nlow-precision training. Our code is available at:\nhttps://github.com/bluorion-com/weight_rescaling."}
{"id": "2503.18055", "pdf": "https://arxiv.org/pdf/2503.18055", "abs": "https://arxiv.org/abs/2503.18055", "authors": ["Mingde Yao", "Menglu Wang", "King-Man Tam", "Lingen Li", "Tianfan Xue", "Jinwei Gu"], "title": "PolarFree: Polarization-based Reflection-free Imaging", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "Reflection removal is challenging due to complex light interactions, where\nreflections obscure important details and hinder scene understanding.\nPolarization naturally provides a powerful cue to distinguish between reflected\nand transmitted light, enabling more accurate reflection removal. However,\nexisting methods often rely on small-scale or synthetic datasets, which fail to\ncapture the diversity and complexity of real-world scenarios. To this end, we\nconstruct a large-scale dataset, PolaRGB, for Polarization-based reflection\nremoval of RGB images, which enables us to train models that generalize\neffectively across a wide range of real-world scenarios. The PolaRGB dataset\ncontains 6,500 well-aligned mixed-transmission image pairs, 8x larger than\nexisting polarization datasets, and is the first to include both RGB and\npolarization images captured across diverse indoor and outdoor environments\nwith varying lighting conditions. Besides, to fully exploit the potential of\npolarization cues for reflection removal, we introduce PolarFree, which\nleverages diffusion process to generate reflection-free cues for accurate\nreflection removal. Extensive experiments show that PolarFree significantly\nenhances image clarity in challenging reflective scenarios, setting a new\nbenchmark for polarized imaging and reflection removal. Code and dataset are\navailable at https://github.com/mdyao/PolarFree."}
{"id": "2503.17502", "pdf": "https://arxiv.org/pdf/2503.17502", "abs": "https://arxiv.org/abs/2503.17502", "authors": ["Hamed Jelodar", "Mohammad Meymani", "Roozbeh Razavi-Far"], "title": "Large Language Models (LLMs) for Source Code Analysis: applications, models and datasets", "categories": ["cs.SE", "cs.AI", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) and transformer-based architectures are\nincreasingly utilized for source code analysis. As software systems grow in\ncomplexity, integrating LLMs into code analysis workflows becomes essential for\nenhancing efficiency, accuracy, and automation. This paper explores the role of\nLLMs for different code analysis tasks, focusing on three key aspects: 1) what\nthey can analyze and their applications, 2) what models are used and 3) what\ndatasets are used, and the challenges they face. Regarding the goal of this\nresearch, we investigate scholarly articles that explore the use of LLMs for\nsource code analysis to uncover research developments, current trends, and the\nintellectual structure of this emerging field. Additionally, we summarize\nlimitations and highlight essential tools, datasets, and key challenges, which\ncould be valuable for future work."}
{"id": "2503.18065", "pdf": "https://arxiv.org/pdf/2503.18065", "abs": "https://arxiv.org/abs/2503.18065", "authors": ["Ziming Wei", "Bingqian Lin", "Yunshuang Nie", "Jiaqi Chen", "Shikui Ma", "Hang Xu", "Xiaodan Liang"], "title": "Unseen from Seen: Rewriting Observation-Instruction Using Foundation Models for Augmenting Vision-Language Navigation", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.RO"], "comment": null, "summary": "Data scarcity is a long-standing challenge in the Vision-Language Navigation\n(VLN) field, which extremely hinders the generalization of agents to unseen\nenvironments. Previous works primarily rely on additional simulator data or\nweb-collected images/videos to improve the generalization. However, the\nsimulator environments still face limited diversity, and the web-collected data\noften requires extensive labor to remove the noise. In this paper, we propose a\nRewriting-driven AugMentation (RAM) paradigm for VLN, which directly creates\nthe unseen observation-instruction pairs via rewriting human-annotated training\ndata. Benefiting from our rewriting mechanism, new observation-instruction can\nbe obtained in both simulator-free and labor-saving manners to promote\ngeneralization. Specifically, we first introduce Object-Enriched Observation\nRewriting, where we combine Vision-Language Models (VLMs) and Large Language\nModels (LLMs) to derive rewritten object-enriched scene descriptions, enabling\nobservation synthesis with diverse objects and spatial layouts via\nText-to-Image Generation Models (T2IMs). Then, we propose Observation-Contrast\nInstruction Rewriting, which generates observation-aligned rewritten\ninstructions by requiring LLMs to reason the difference between original and\nnew observations. We further develop a mixing-then-focusing training strategy\nwith a random observation cropping scheme, effectively enhancing data\ndistribution diversity while suppressing augmentation data noise during\ntraining. Experiments on both the discrete environments (R2R, REVERIE, and R4R\ndatasets) and continuous environments (R2R-CE dataset) show the superior\nperformance and impressive generalization ability of our method. Code is\navailable at https://github.com/SaDil13/VLN-RAM."}
{"id": "2503.17553", "pdf": "https://arxiv.org/pdf/2503.17553", "abs": "https://arxiv.org/abs/2503.17553", "authors": ["Humza Nusrat", "Bing Luo", "Ryan Hall", "Joshua Kim", "Hassan Bagher-Ebadian", "Anthony Doemer", "Benjamin Movsas", "Kundan Thind"], "title": "Autonomous Radiotherapy Treatment Planning Using DOLA: A Privacy-Preserving, LLM-Based Optimization Agent", "categories": ["physics.med-ph", "cs.AI", "cs.CL", "cs.ET", "cs.HC"], "comment": "19 pages, 5 figures, preprint", "summary": "Radiotherapy treatment planning is a complex and time-intensive process,\noften impacted by inter-planner variability and subjective decision-making. To\naddress these challenges, we introduce Dose Optimization Language Agent (DOLA),\nan autonomous large language model (LLM)-based agent designed for optimizing\nradiotherapy treatment plans while rigorously protecting patient privacy. DOLA\nintegrates the LLaMa3.1 LLM directly with a commercial treatment planning\nsystem, utilizing chain-of-thought prompting, retrieval-augmented generation\n(RAG), and reinforcement learning (RL). Operating entirely within secure local\ninfrastructure, this agent eliminates external data sharing. We evaluated DOLA\nusing a retrospective cohort of 18 prostate cancer patients prescribed 60 Gy in\n20 fractions, comparing model sizes (8 billion vs. 70 billion parameters) and\noptimization strategies (No-RAG, RAG, and RAG+RL) over 10 planning iterations.\nThe 70B model demonstrated significantly improved performance, achieving\napproximately 16.4% higher final scores than the 8B model. The RAG approach\noutperformed the No-RAG baseline by 19.8%, and incorporating RL accelerated\nconvergence, highlighting the synergy of retrieval-based memory and\nreinforcement learning. Optimal temperature hyperparameter analysis identified\n0.4 as providing the best balance between exploration and exploitation. This\nproof of concept study represents the first successful deployment of locally\nhosted LLM agents for autonomous optimization of treatment plans within a\ncommercial radiotherapy planning system. By extending human-machine interaction\nthrough interpretable natural language reasoning, DOLA offers a scalable and\nprivacy-conscious framework, with significant potential for clinical\nimplementation and workflow improvement."}
{"id": "2503.18073", "pdf": "https://arxiv.org/pdf/2503.18073", "abs": "https://arxiv.org/abs/2503.18073", "authors": ["Yuxuan Xie", "Xuan Yu", "Changjian Jiang", "Sitong Mao", "Shunbo Zhou", "Rui Fan", "Rong Xiong", "Yue Wang"], "title": "PanopticSplatting: End-to-End Panoptic Gaussian Splatting", "categories": ["cs.CV", "cs.RO"], "comment": "8 pages, 6 figures", "summary": "Open-vocabulary panoptic reconstruction is a challenging task for\nsimultaneous scene reconstruction and understanding. Recently, methods have\nbeen proposed for 3D scene understanding based on Gaussian splatting. However,\nthese methods are multi-staged, suffering from the accumulated errors and the\ndependence of hand-designed components. To streamline the pipeline and achieve\nglobal optimization, we propose PanopticSplatting, an end-to-end system for\nopen-vocabulary panoptic reconstruction. Our method introduces query-guided\nGaussian segmentation with local cross attention, lifting 2D instance masks\nwithout cross-frame association in an end-to-end way. The local cross attention\nwithin view frustum effectively reduces the training memory, making our model\nmore accessible to large scenes with more Gaussians and objects. In addition,\nto address the challenge of noisy labels in 2D pseudo masks, we propose label\nblending to promote consistent 3D segmentation with less noisy floaters, as\nwell as label warping on 2D predictions which enhances multi-view coherence and\nsegmentation accuracy. Our method demonstrates strong performances in 3D scene\npanoptic reconstruction on the ScanNet-V2 and ScanNet++ datasets, compared with\nboth NeRF-based and Gaussian-based panoptic reconstruction methods. Moreover,\nPanopticSplatting can be easily generalized to numerous variants of Gaussian\nsplatting, and we demonstrate its robustness on different Gaussian base models."}
{"id": "2503.17632", "pdf": "https://arxiv.org/pdf/2503.17632", "abs": "https://arxiv.org/abs/2503.17632", "authors": ["Jiali Cheng", "Hadi Amiri"], "title": "FairFlow: Mitigating Dataset Biases through Undecided Learning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "EMNLP 2024", "summary": "Language models are prone to dataset biases, known as shortcuts and spurious\ncorrelations in data, which often result in performance drop on new data. We\npresent a new debiasing framework called ``FairFlow'' that mitigates dataset\nbiases by learning to be undecided in its predictions for data samples or\nrepresentations associated with known or unknown biases. The framework\nintroduces two key components: a suite of data and model perturbation\noperations that generate different biased views of input samples, and a\ncontrastive objective that learns debiased and robust representations from the\nresulting biased views of samples. Experiments show that FairFlow outperforms\nexisting debiasing methods, particularly against out-of-domain and hard test\nsamples without compromising the in-domain performance"}
{"id": "2503.18082", "pdf": "https://arxiv.org/pdf/2503.18082", "abs": "https://arxiv.org/abs/2503.18082", "authors": ["Nachuan Ma", "Zhengfei Song", "Qiang Hu", "Chuang-Wei Liu", "Yu Han", "Yanting Zhang", "Rui Fan", "Lihua Xie"], "title": "Vehicular Road Crack Detection with Deep Learning: A New Online Benchmark for Comprehensive Evaluation of Existing Algorithms", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "In the emerging field of urban digital twins (UDTs), advancing intelligent\nroad inspection (IRI) vehicles with automatic road crack detection systems is\nessential for maintaining civil infrastructure. Over the past decade, deep\nlearning-based road crack detection methods have been developed to detect\ncracks more efficiently, accurately, and objectively, with the goal of\nreplacing manual visual inspection. Nonetheless, there is a lack of systematic\nreviews on state-of-the-art (SoTA) deep learning techniques, especially\ndata-fusion and label-efficient algorithms for this task. This paper thoroughly\nreviews the SoTA deep learning-based algorithms, including (1) supervised, (2)\nunsupervised, (3) semi-supervised, and (4) weakly-supervised methods developed\nfor road crack detection. Also, we create a dataset called UDTIRI-Crack,\ncomprising $2,500$ high-quality images from seven public annotated sources, as\nthe first extensive online benchmark in this field. Comprehensive experiments\nare conducted to compare the detection performance, computational efficiency,\nand generalizability of public SoTA deep learning-based algorithms for road\ncrack detection. In addition, the feasibility of foundation models and large\nlanguage models (LLMs) for road crack detection is explored. Afterwards, the\nexisting challenges and future development trends of deep learning-based road\ncrack detection algorithms are discussed. We believe this review can serve as\npractical guidance for developing intelligent road detection vehicles with the\nnext-generation road condition assessment systems. The released benchmark\nUDTIRI-Crack is available at https://udtiri.com/submission/."}
{"id": "2503.17736", "pdf": "https://arxiv.org/pdf/2503.17736", "abs": "https://arxiv.org/abs/2503.17736", "authors": ["Yiming Zhao", "Yu Zeng", "Yukun Qi", "YaoYang Liu", "Lin Chen", "Zehui Chen", "Xikun Bao", "Jie Zhao", "Feng Zhao"], "title": "V2P-Bench: Evaluating Video-Language Understanding with Visual Prompts for Better Human-Model Interaction", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Large Vision-Language Models (LVLMs) have made significant progress in the\nfield of video understanding recently. However, current benchmarks uniformly\nlean on text prompts for evaluation, which often necessitate complex\nreferential language and fail to provide precise spatial and temporal\nreferences. This limitation diminishes the experience and efficiency of\nhuman-model interaction. To address this limitation, we propose the Video\nVisual Prompt Benchmark(V2P-Bench), a comprehensive benchmark specifically\ndesigned to evaluate LVLMs' video understanding capabilities in multimodal\nhuman-model interaction scenarios. V2P-Bench includes 980 unique videos and\n1,172 QA pairs, covering 5 main tasks and 12 dimensions, facilitating\ninstance-level fine-grained understanding aligned with human cognition.\nBenchmarking results reveal that even the most powerful models perform poorly\non V2P-Bench (65.4% for GPT-4o and 67.9% for Gemini-1.5-Pro), significantly\nlower than the human experts' 88.3%, highlighting the current shortcomings of\nLVLMs in understanding video visual prompts. We hope V2P-Bench will serve as a\nfoundation for advancing multimodal human-model interaction and video\nunderstanding evaluation. Project page:\nhttps://github.com/gaotiexinqu/V2P-Bench."}
{"id": "2503.18083", "pdf": "https://arxiv.org/pdf/2503.18083", "abs": "https://arxiv.org/abs/2503.18083", "authors": ["Tianxin Huang", "Gim Hee Lee"], "title": "Unified Geometry and Color Compression Framework for Point Clouds via Generative Diffusion Priors", "categories": ["cs.CV"], "comment": null, "summary": "With the growth of 3D applications and the rapid increase in sensor-collected\n3D point cloud data, there is a rising demand for efficient compression\nalgorithms. Most existing learning-based compression methods handle geometry\nand color attributes separately, treating them as distinct tasks, making these\nmethods challenging to apply directly to point clouds with colors. Besides, the\nlimited capacities of training datasets also limit their generalizability\nacross points with different distributions. In this work, we introduce a\ntest-time unified geometry and color compression framework of 3D point clouds.\nInstead of training a compression model based on specific datasets, we adapt a\npre-trained generative diffusion model to compress original colored point\nclouds into sparse sets, termed 'seeds', using prompt tuning. Decompression is\nthen achieved through multiple denoising steps with separate sampling\nprocesses. Experiments on objects and indoor scenes demonstrate that our method\nhas superior performances compared to existing baselines for the compression of\ngeometry and color."}
{"id": "2503.17783", "pdf": "https://arxiv.org/pdf/2503.17783", "abs": "https://arxiv.org/abs/2503.17783", "authors": ["Nguyen Phuc Tran", "Brigitte Jaumard", "Oscar Delgado"], "title": "Energy-Aware LLMs: A step towards sustainable AI for downstream applications", "categories": ["cs.PF", "cs.AI", "cs.CL", "cs.LG"], "comment": "This work has been submitted to V. International Conference on\n  Electrical, Computer and Energy Technologies (ICECET 2025) for possible\n  publication", "summary": "Advanced Large Language Models (LLMs) have revolutionized various fields,\nincluding communication networks, sparking an innovation wave that has led to\nnew applications and services, and significantly enhanced solution schemes.\nDespite all these impressive developments, most LLMs typically require huge\ncomputational resources, resulting in terribly high energy consumption. Thus,\nthis research study proposes an end-to-end pipeline that investigates the\ntrade-off between energy efficiency and model performance for an LLM during\nfault ticket analysis in communication networks. It further evaluates the\npipeline performance using two real-world datasets for the tasks of root cause\nanalysis and response feedback in a communication network. Our results show\nthat an appropriate combination of quantization and pruning techniques is able\nto reduce energy consumption while significantly improving model performance."}
{"id": "2503.18094", "pdf": "https://arxiv.org/pdf/2503.18094", "abs": "https://arxiv.org/abs/2503.18094", "authors": ["Fei Li", "Wenxuan Liu", "Jingjing Chen", "Ruixu Zhang", "Yuran Wang", "Xian Zhong", "Zheng Wang"], "title": "Anomize: Better Open Vocabulary Video Anomaly Detection", "categories": ["cs.CV"], "comment": null, "summary": "Open Vocabulary Video Anomaly Detection (OVVAD) seeks to detect and classify\nboth base and novel anomalies. However, existing methods face two specific\nchallenges related to novel anomalies. The first challenge is detection\nambiguity, where the model struggles to assign accurate anomaly scores to\nunfamiliar anomalies. The second challenge is categorization confusion, where\nnovel anomalies are often misclassified as visually similar base instances. To\naddress these challenges, we explore supplementary information from multiple\nsources to mitigate detection ambiguity by leveraging multiple levels of visual\ndata alongside matching textual information. Furthermore, we propose\nincorporating label relations to guide the encoding of new labels, thereby\nimproving alignment between novel videos and their corresponding labels, which\nhelps reduce categorization confusion. The resulting Anomize framework\neffectively tackles these issues, achieving superior performance on UCF-Crime\nand XD-Violence datasets, demonstrating its effectiveness in OVVAD."}
{"id": "2503.17793", "pdf": "https://arxiv.org/pdf/2503.17793", "abs": "https://arxiv.org/abs/2503.17793", "authors": ["Codefuse", "Ling Team", ":", "Wenting Cai", "Yuchen Cao", "Chaoyu Chen", "Chen Chen", "Siba Chen", "Qing Cui", "Peng Di", "Junpeng Fang", "Zi Gong", "Ting Guo", "Zhengyu He", "Yang Huang", "Cong Li", "Jianguo Li", "Zheng Li", "Shijie Lian", "BingChang Liu", "Songshan Luo", "Shuo Mao", "Min Shen", "Jian Wu", "Jiaolong Yang", "Wenjie Yang", "Tong Ye", "Hang Yu", "Wei Zhang", "Zhenduo Zhang", "Hailin Zhao", "Xunjin Zheng", "Jun Zhou"], "title": "Every Sample Matters: Leveraging Mixture-of-Experts and High-Quality Data for Efficient and Accurate Code LLM", "categories": ["cs.LG", "cs.AI", "cs.CL", "I.2.7"], "comment": "20 pages, 6 figures", "summary": "Recent advancements in code large language models (LLMs) have demonstrated\nremarkable capabilities in code generation and understanding. It is still\nchallenging to build a code LLM with comprehensive performance yet ultimate\nefficiency. Many attempts have been released in the open source community to\nbreak the trade-off between performance and efficiency, such as the Qwen Coder\nseries and the DeepSeek Coder series. This paper introduces yet another attempt\nin this area, namely Ling-Coder-Lite. We leverage the efficient\nMixture-of-Experts (MoE) architecture along with a set of high-quality data\ncuration methods (especially those based on program analytics) to build an\nefficient yet powerful code LLM. Ling-Coder-Lite exhibits on-par performance on\n12 representative coding benchmarks compared to state-of-the-art models of\nsimilar size, such as Qwen2.5-Coder-7B and DeepSeek-Coder-V2-Lite, while\noffering competitive latency and throughput. In practice, we achieve a 50\\%\nreduction in deployment resources compared to the similar-sized dense model\nwithout performance loss. To facilitate further research and development in\nthis area, we open-source our models as well as a substantial portion of\nhigh-quality data for the annealing and post-training stages. The models and\ndata can be accessed\nat~\\url{https://huggingface.co/inclusionAI/Ling-Coder-lite}."}
{"id": "2503.18100", "pdf": "https://arxiv.org/pdf/2503.18100", "abs": "https://arxiv.org/abs/2503.18100", "authors": ["Xuesong Chen", "Shaoshuai Shi", "Tao Ma", "Jingqiu Zhou", "Simon See", "Ka Chun Cheung", "Hongsheng Li"], "title": "M3Net: Multimodal Multi-task Learning for 3D Detection, Segmentation, and Occupancy Prediction in Autonomous Driving", "categories": ["cs.CV"], "comment": "Accepted by AAAI 2025", "summary": "The perception system for autonomous driving generally requires to handle\nmultiple diverse sub-tasks. However, current algorithms typically tackle\nindividual sub-tasks separately, which leads to low efficiency when aiming at\nobtaining full-perception results. Some multi-task learning methods try to\nunify multiple tasks with one model, but do not solve the conflicts in\nmulti-task learning. In this paper, we introduce M3Net, a novel multimodal and\nmulti-task network that simultaneously tackles detection, segmentation, and 3D\noccupancy prediction for autonomous driving and achieves superior performance\nthan single task model. M3Net takes multimodal data as input and multiple tasks\nvia query-token interactions. To enhance the integration of multi-modal\nfeatures for multi-task learning, we first propose the Modality-Adaptive\nFeature Integration (MAFI) module, which enables single-modality features to\npredict channel-wise attention weights for their high-performing tasks,\nrespectively. Based on integrated features, we then develop task-specific query\ninitialization strategies to accommodate the needs of detection/segmentation\nand 3D occupancy prediction. Leveraging the properly initialized queries, a\nshared decoder transforms queries and BEV features layer-wise, facilitating\nmulti-task learning. Furthermore, we propose a Task-oriented Channel Scaling\n(TCS) module in the decoder to mitigate conflicts between optimizing for\ndifferent tasks. Additionally, our proposed multi-task querying and TCS module\nsupport both Transformer-based decoder and Mamba-based decoder, demonstrating\nits flexibility to different architectures. M3Net achieves state-of-the-art\nmulti-task learning performance on the nuScenes benchmarks."}
{"id": "2503.17928", "pdf": "https://arxiv.org/pdf/2503.17928", "abs": "https://arxiv.org/abs/2503.17928", "authors": ["Zefeng Zhang", "Hengzhu Tang", "Jiawei Sheng", "Zhenyu Zhang", "Yiming Ren", "Zhenyang Li", "Dawei Yin", "Duohe Ma", "Tingwen Liu"], "title": "Debiasing Multimodal Large Language Models via Noise-Aware Preference Optimization", "categories": ["cs.CV", "cs.CL"], "comment": "CVPR 2025", "summary": "Multimodal Large Language Models excel in various tasks, yet often struggle\nwith modality bias, where the model tends to rely heavily on a single modality\nand overlook critical information in other modalities, which leads to incorrect\nfocus and generating irrelevant responses. In this paper, we propose using the\nparadigm of preference optimization to solve the modality bias problem,\nincluding RLAIFVBias, a debiased preference optimization dataset, and a Noise\nAware Preference Optimization algorithm. Specifically, we first construct the\ndataset by introducing perturbations to reduce the informational content of\ncertain modalities, compelling the model to rely on a specific modality when\ngenerating negative responses. To address the inevitable noise in automatically\nconstructed data, we combine the noise robust Mean Absolute Error with the\nBinary Cross Entropy in Direct Preference Optimization by a negative Box Cox\ntransformation, and dynamically adjust the algorithm noise robustness based on\nthe evaluated noise levels in the data. Extensive experiments validate our\napproach, demonstrating not only its effectiveness in mitigating modality bias\nbut also its significant role in minimizing hallucinations."}
{"id": "2503.18107", "pdf": "https://arxiv.org/pdf/2503.18107", "abs": "https://arxiv.org/abs/2503.18107", "authors": ["Hongjia Zhai", "Hai Li", "Zhenzhe Li", "Xiaokun Pan", "Yijia He", "Guofeng Zhang"], "title": "PanoGS: Gaussian-based Panoptic Segmentation for 3D Open Vocabulary Scene Understanding", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "Recently, 3D Gaussian Splatting (3DGS) has shown encouraging performance for\nopen vocabulary scene understanding tasks. However, previous methods cannot\ndistinguish 3D instance-level information, which usually predicts a heatmap\nbetween the scene feature and text query. In this paper, we propose PanoGS, a\nnovel and effective 3D panoptic open vocabulary scene understanding approach.\nTechnically, to learn accurate 3D language features that can scale to large\nindoor scenarios, we adopt the pyramid tri-plane to model the latent continuous\nparametric feature space and use a 3D feature decoder to regress the multi-view\nfused 2D feature cloud. Besides, we propose language-guided graph cuts that\nsynergistically leverage reconstructed geometry and learned language cues to\ngroup 3D Gaussian primitives into a set of super-primitives. To obtain 3D\nconsistent instance, we perform graph clustering based segmentation with\nSAM-guided edge affinity computation between different super-primitives.\nExtensive experiments on widely used datasets show better or more competitive\nperformance on 3D panoptic open vocabulary scene understanding. Project page:\n\\href{https://zju3dv.github.io/panogs}{https://zju3dv.github.io/panogs}."}
{"id": "2503.17955", "pdf": "https://arxiv.org/pdf/2503.17955", "abs": "https://arxiv.org/abs/2503.17955", "authors": ["Stefan Pasch", "Sun-Young Ha"], "title": "Human-AI Interaction and User Satisfaction: Empirical Evidence from Online Reviews of AI Products", "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": null, "summary": "Human-AI Interaction (HAI) guidelines and design principles have become\nincreasingly important in both industry and academia to guide the development\nof AI systems that align with user needs and expectations. However, large-scale\nempirical evidence on how HAI principles shape user satisfaction in practice\nremains limited. This study addresses that gap by analyzing over 100,000 user\nreviews of AI-related products from G2.com, a leading review platform for\nbusiness software and services. Based on widely adopted industry guidelines, we\nidentify seven core HAI dimensions and examine their coverage and sentiment\nwithin the reviews. We find that the sentiment on four HAI\ndimensions-adaptability, customization, error recovery, and security-is\npositively associated with overall user satisfaction. Moreover, we show that\nengagement with HAI dimensions varies by professional background: Users with\ntechnical job roles are more likely to discuss system-focused aspects, such as\nreliability, while non-technical users emphasize interaction-focused features\nlike customization and feedback. Interestingly, the relationship between HAI\nsentiment and overall satisfaction is not moderated by job role, suggesting\nthat once an HAI dimension has been identified by users, its effect on\nsatisfaction is consistent across job roles."}
{"id": "2503.18123", "pdf": "https://arxiv.org/pdf/2503.18123", "abs": "https://arxiv.org/abs/2503.18123", "authors": ["Alexander Gielisse", "Jan van Gemert"], "title": "End-to-End Implicit Neural Representations for Classification", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025. 8 pages, supplementary material included", "summary": "Implicit neural representations (INRs) such as NeRF and SIREN encode a signal\nin neural network parameters and show excellent results for signal\nreconstruction. Using INRs for downstream tasks, such as classification, is\nhowever not straightforward. Inherent symmetries in the parameters pose\nchallenges and current works primarily focus on designing architectures that\nare equivariant to these symmetries. However, INR-based classification still\nsignificantly under-performs compared to pixel-based methods like CNNs. This\nwork presents an end-to-end strategy for initializing SIRENs together with a\nlearned learning-rate scheme, to yield representations that improve\nclassification accuracy. We show that a simple, straightforward, Transformer\nmodel applied to a meta-learned SIREN, without incorporating explicit symmetry\nequivariances, outperforms the current state-of-the-art. On the CIFAR-10 SIREN\nclassification task, we improve the state-of-the-art without augmentations from\n38.8% to 59.6%, and from 63.4% to 64.7% with augmentations. We demonstrate\nscalability on the high-resolution Imagenette dataset achieving reasonable\nreconstruction quality with a classification accuracy of 60.8% and are the\nfirst to do INR classification on the full ImageNet-1K dataset where we achieve\na SIREN classification performance of 23.6%. To the best of our knowledge, no\nother SIREN classification approach has managed to set a classification\nbaseline for any high-resolution image dataset. Our code is available at\nhttps://github.com/SanderGielisse/MWT"}
{"id": "2503.17979", "pdf": "https://arxiv.org/pdf/2503.17979", "abs": "https://arxiv.org/abs/2503.17979", "authors": ["Weixiang Zhao", "Xingyu Sui", "Jiahe Guo", "Yulin Hu", "Yang Deng", "Yanyan Zhao", "Bing Qin", "Wanxiang Che", "Tat-Seng Chua", "Ting Liu"], "title": "Trade-offs in Large Reasoning Models: An Empirical Analysis of Deliberative and Adaptive Reasoning over Foundational Capabilities", "categories": ["cs.AI", "cs.CL"], "comment": "23 pages. Work in progress", "summary": "Recent advancements in Large Reasoning Models (LRMs), such as OpenAI's o1/o3\nand DeepSeek-R1, have demonstrated remarkable performance in specialized\nreasoning tasks through human-like deliberative thinking and long\nchain-of-thought reasoning. However, our systematic evaluation across various\nmodel families (DeepSeek, Qwen, and LLaMA) and scales (7B to 671B) reveals that\nacquiring these deliberative reasoning capabilities significantly reduces the\nfoundational capabilities of LRMs, including notable declines in helpfulness\nand harmlessness, alongside substantially increased inference costs.\nImportantly, we demonstrate that adaptive reasoning -- employing modes like\nZero-Thinking, Less-Thinking, and Summary-Thinking -- can effectively alleviate\nthese drawbacks. Our empirical insights underline the critical need for\ndeveloping more versatile LRMs capable of dynamically allocating inference-time\ncompute according to specific task characteristics."}
{"id": "2503.18134", "pdf": "https://arxiv.org/pdf/2503.18134", "abs": "https://arxiv.org/abs/2503.18134", "authors": ["Xiaofei Hui", "Haoxuan Qu", "Hossein Rahmani", "Jun Liu"], "title": "An Image-like Diffusion Method for Human-Object Interaction Detection", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "Human-object interaction (HOI) detection often faces high levels of ambiguity\nand indeterminacy, as the same interaction can appear vastly different across\ndifferent human-object pairs. Additionally, the indeterminacy can be further\nexacerbated by issues such as occlusions and cluttered backgrounds. To handle\nsuch a challenging task, in this work, we begin with a key observation: the\noutput of HOI detection for each human-object pair can be recast as an image.\nThus, inspired by the strong image generation capabilities of image diffusion\nmodels, we propose a new framework, HOI-IDiff. In HOI-IDiff, we tackle HOI\ndetection from a novel perspective, using an Image-like Diffusion process to\ngenerate HOI detection outputs as images. Furthermore, recognizing that our\nrecast images differ in certain properties from natural images, we enhance our\nframework with a customized HOI diffusion process and a slice patchification\nmodel architecture, which are specifically tailored to generate our recast\n``HOI images''. Extensive experiments demonstrate the efficacy of our\nframework."}
{"id": "2503.18034", "pdf": "https://arxiv.org/pdf/2503.18034", "abs": "https://arxiv.org/abs/2503.18034", "authors": ["Qiao Liang", "Yanjiang Liu", "Ben He", "Yaojie Lu", "Hongyu Lin", "Jia Zheng", "Xianpei Han", "Le Sun", "Yingfei Sun"], "title": "Expanding the Boundaries of Vision Prior Knowledge in Multi-modal Large Language Models", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Does the prior knowledge of the vision encoder constrain the capability\nboundary of Multi-modal Large Language Models (MLLMs)? While most existing\nresearch treats MLLMs as unified systems optimized through end-to-end training,\nthe impact of vision encoder's prior knowledge is seldom investigated. In this\nwork, we introduce a novel metric, $Rank_e$, to quantify the effect of the\nvision encoder's prior knowledge on MLLM performance. Our analysis reveals a\npositive correlation between prior knowledge and MLLM performance. Moreover, we\nfind that domain-specific fine-tuning using solely end-to-end visual question\nanswering (VQA) data is insufficient--particularly for entities with low\ninherent visual prior knowledge. To address this issue, we propose VisPRE\n(Vision Prior Remediation), a two-stage training framework that explicitly\nincorporates prior knowledge at the vision encoder level. Experimental results\ndemonstrate that augmenting vision encoder's prior knowledge substantially\nboosts the visual understanding capabilities of MLLMs, offering a novel and\neffective strategy for improving performance, especially in scenarios involving\nuncommon visual entities."}
{"id": "2503.18135", "pdf": "https://arxiv.org/pdf/2503.18135", "abs": "https://arxiv.org/abs/2503.18135", "authors": ["Jiaxin Huang", "Runnan Chen", "Ziwen Li", "Zhengqing Gao", "Xiao He", "Yandong Guo", "Mingming Gong", "Tongliang Liu"], "title": "MLLM-For3D: Adapting Multimodal Large Language Model for 3D Reasoning Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Reasoning segmentation aims to segment target objects in complex scenes based\non human intent and spatial reasoning. While recent multimodal large language\nmodels (MLLMs) have demonstrated impressive 2D image reasoning segmentation,\nadapting these capabilities to 3D scenes remains underexplored. In this paper,\nwe introduce MLLM-For3D, a simple yet effective framework that transfers\nknowledge from 2D MLLMs to 3D scene understanding. Specifically, we utilize\nMLLMs to generate multi-view pseudo segmentation masks and corresponding text\nembeddings, then unproject 2D masks into 3D space and align them with the text\nembeddings. The primary challenge lies in the absence of 3D context and spatial\nconsistency across multiple views, causing the model to hallucinate objects\nthat do not exist and fail to target objects consistently. Training the 3D\nmodel with such irrelevant objects leads to performance degradation. To address\nthis, we introduce a spatial consistency strategy to enforce that segmentation\nmasks remain coherent in the 3D space, effectively capturing the geometry of\nthe scene. Moreover, we develop a Token-for-Query approach for multimodal\nsemantic alignment, enabling consistent identification of the same object\nacross different views. Extensive evaluations on various challenging indoor\nscene benchmarks demonstrate that, even without any labeled 3D training data,\nMLLM-For3D outperforms existing 3D reasoning segmentation methods, effectively\ninterpreting user intent, understanding 3D scenes, and reasoning about spatial\nrelationships."}
{"id": "2503.18050", "pdf": "https://arxiv.org/pdf/2503.18050", "abs": "https://arxiv.org/abs/2503.18050", "authors": ["Hanwool Lee"], "title": "(G)I-DLE: Generative Inference via Distribution-preserving Logit Exclusion with KL Divergence Minimization for Constrained Decoding", "categories": ["cs.CE", "cs.CL"], "comment": "preprint", "summary": "We propose (G)I-DLE, a new approach to constrained decoding that leverages KL\ndivergence minimization to preserve the intrinsic conditional probability\ndistribution of autoregressive language models while excluding undesirable\ntokens. Unlike conventional methods that naively set banned tokens' logits to\n$-\\infty$, which can distort the conversion from raw logits to posterior\nprobabilities and increase output variance, (G)I-DLE re-normalizes the allowed\ntoken probabilities to minimize such distortion. We validate our method on the\nK2-Eval dataset, specifically designed to assess Korean language fluency,\nlogical reasoning, and cultural appropriateness. Experimental results on\nQwen2.5 models (ranging from 1.5B to 14B) demonstrate that G-IDLE not only\nboosts mean evaluation scores but also substantially reduces the variance of\noutput quality."}
{"id": "2503.18137", "pdf": "https://arxiv.org/pdf/2503.18137", "abs": "https://arxiv.org/abs/2503.18137", "authors": ["Mingi Kwon", "Shin seong Kim", "Jaeseok Jeong. Yi Ting Hsiao", "Youngjung Uh"], "title": "TCFG: Tangential Damping Classifier-free Guidance", "categories": ["cs.CV"], "comment": "CVPR2025", "summary": "Diffusion models have achieved remarkable success in text-to-image synthesis,\nlargely attributed to the use of classifier-free guidance (CFG), which enables\nhigh-quality, condition-aligned image generation. CFG combines the conditional\nscore (e.g., text-conditioned) with the unconditional score to control the\noutput. However, the unconditional score is in charge of estimating the\ntransition between manifolds of adjacent timesteps from $x_t$ to $x_{t-1}$,\nwhich may inadvertently interfere with the trajectory toward the specific\ncondition. In this work, we introduce a novel approach that leverages a\ngeometric perspective on the unconditional score to enhance CFG performance\nwhen conditional scores are available. Specifically, we propose a method that\nfilters the singular vectors of both conditional and unconditional scores using\nsingular value decomposition. This filtering process aligns the unconditional\nscore with the conditional score, thereby refining the sampling trajectory to\nstay closer to the manifold. Our approach improves image quality with\nnegligible additional computation. We provide deeper insights into the score\nfunction behavior in diffusion models and present a practical technique for\nachieving more accurate and contextually coherent image synthesis."}
{"id": "2503.18065", "pdf": "https://arxiv.org/pdf/2503.18065", "abs": "https://arxiv.org/abs/2503.18065", "authors": ["Ziming Wei", "Bingqian Lin", "Yunshuang Nie", "Jiaqi Chen", "Shikui Ma", "Hang Xu", "Xiaodan Liang"], "title": "Unseen from Seen: Rewriting Observation-Instruction Using Foundation Models for Augmenting Vision-Language Navigation", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.RO"], "comment": null, "summary": "Data scarcity is a long-standing challenge in the Vision-Language Navigation\n(VLN) field, which extremely hinders the generalization of agents to unseen\nenvironments. Previous works primarily rely on additional simulator data or\nweb-collected images/videos to improve the generalization. However, the\nsimulator environments still face limited diversity, and the web-collected data\noften requires extensive labor to remove the noise. In this paper, we propose a\nRewriting-driven AugMentation (RAM) paradigm for VLN, which directly creates\nthe unseen observation-instruction pairs via rewriting human-annotated training\ndata. Benefiting from our rewriting mechanism, new observation-instruction can\nbe obtained in both simulator-free and labor-saving manners to promote\ngeneralization. Specifically, we first introduce Object-Enriched Observation\nRewriting, where we combine Vision-Language Models (VLMs) and Large Language\nModels (LLMs) to derive rewritten object-enriched scene descriptions, enabling\nobservation synthesis with diverse objects and spatial layouts via\nText-to-Image Generation Models (T2IMs). Then, we propose Observation-Contrast\nInstruction Rewriting, which generates observation-aligned rewritten\ninstructions by requiring LLMs to reason the difference between original and\nnew observations. We further develop a mixing-then-focusing training strategy\nwith a random observation cropping scheme, effectively enhancing data\ndistribution diversity while suppressing augmentation data noise during\ntraining. Experiments on both the discrete environments (R2R, REVERIE, and R4R\ndatasets) and continuous environments (R2R-CE dataset) show the superior\nperformance and impressive generalization ability of our method. Code is\navailable at https://github.com/SaDil13/VLN-RAM."}
{"id": "2503.18141", "pdf": "https://arxiv.org/pdf/2503.18141", "abs": "https://arxiv.org/abs/2503.18141", "authors": ["Diwei Wang", "Cédric Bobenrieth", "Hyewon Seo"], "title": "AGIR: Assessing 3D Gait Impairment with Reasoning based on LLMs", "categories": ["cs.CV"], "comment": null, "summary": "Assessing gait impairment plays an important role in early diagnosis, disease\nmonitoring, and treatment evaluation for neurodegenerative diseases. Despite\nits widespread use in clinical practice, it is limited by subjectivity and a\nlack of precision. While recent deep learning-based approaches have\nconsistently improved classification accuracies, they often lack\ninterpretability, hindering their utility in clinical decision-making. To\novercome these challenges, we introduce AGIR, a novel pipeline consisting of a\npre-trained VQ-VAE motion tokenizer and a subsequent Large Language Model (LLM)\nfine-tuned over pairs of motion tokens and Chain-of-Thought (CoT) reasonings.\nTo fine-tune an LLM for pathological gait analysis, we first introduce a\nmultimodal dataset by adding rationales dedicated to MDS-UPDRS gait score\nassessment to an existing PD gait dataset. We then introduce a two-stage\nsupervised fine-tuning (SFT) strategy to enhance the LLM's motion comprehension\nwith pathology-specific knowledge. This strategy includes: 1) a generative\nstage that aligns gait motions with analytic descriptions through bidirectional\nmotion-description generation, 2) a reasoning stage that integrates logical\nChain-of-Thought (CoT) reasoning for impairment assessment with UPDRS gait\nscore. Validation on an existing dataset and comparisons with state-of-the-art\nmethods confirm the robustness and accuracy of our pipeline, demonstrating its\nability to assign gait impairment scores from motion input with clinically\nmeaningful rationales."}
{"id": "2503.18102", "pdf": "https://arxiv.org/pdf/2503.18102", "abs": "https://arxiv.org/abs/2503.18102", "authors": ["Samuel Schmidgall", "Michael Moor"], "title": "AgentRxiv: Towards Collaborative Autonomous Research", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Progress in scientific discovery is rarely the result of a single \"Eureka\"\nmoment, but is rather the product of hundreds of scientists incrementally\nworking together toward a common goal. While existing agent workflows are\ncapable of producing research autonomously, they do so in isolation, without\nthe ability to continuously improve upon prior research results. To address\nthese challenges, we introduce AgentRxiv-a framework that lets LLM agent\nlaboratories upload and retrieve reports from a shared preprint server in order\nto collaborate, share insights, and iteratively build on each other's research.\nWe task agent laboratories to develop new reasoning and prompting techniques\nand find that agents with access to their prior research achieve higher\nperformance improvements compared to agents operating in isolation (11.4%\nrelative improvement over baseline on MATH-500). We find that the best\nperforming strategy generalizes to benchmarks in other domains (improving on\naverage by 3.3%). Multiple agent laboratories sharing research through\nAgentRxiv are able to work together towards a common goal, progressing more\nrapidly than isolated laboratories, achieving higher overall accuracy (13.7%\nrelative improvement over baseline on MATH-500). These findings suggest that\nautonomous agents may play a role in designing future AI systems alongside\nhumans. We hope that AgentRxiv allows agents to collaborate toward research\ngoals and enables researchers to accelerate discovery."}
{"id": "2503.18142", "pdf": "https://arxiv.org/pdf/2503.18142", "abs": "https://arxiv.org/abs/2503.18142", "authors": ["Zhangyu Wang", "Jielu Zhang", "Zhongliang Zhou", "Qian Cao", "Nemin Wu", "Zeping Liu", "Lan Mu", "Yang Song", "Yiqun Xie", "Ni Lao", "Gengchen Mai"], "title": "LocDiffusion: Identifying Locations on Earth by Diffusing in the Hilbert Space", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Image geolocalization is a fundamental yet challenging task, aiming at\ninferring the geolocation on Earth where an image is taken. Existing methods\napproach it either via grid-based classification or via image retrieval. Their\nperformance significantly suffers when the spatial distribution of test images\ndoes not align with such choices. To address these limitations, we propose to\nleverage diffusion as a mechanism for image geolocalization. To avoid the\nproblematic manifold reprojection step in diffusion, we developed a novel\nspherical positional encoding-decoding framework, which encodes points on a\nspherical surface (e.g., geolocations on Earth) into a Hilbert space of\nSpherical Harmonics coefficients and decodes points (geolocations) by\nmode-seeking. We call this type of position encoding Spherical Harmonics Dirac\nDelta (SHDD) Representation. We also propose a novel SirenNet-based\narchitecture called CS-UNet to learn the conditional backward process in the\nlatent SHDD space by minimizing a latent KL-divergence loss. We train a\nconditional latent diffusion model called LocDiffusion that generates\ngeolocations under the guidance of images -- to the best of our knowledge, the\nfirst generative model for image geolocalization by diffusing geolocation\ninformation in a hidden location embedding space. We evaluate our method\nagainst SOTA image geolocalization baselines. LocDiffusion achieves competitive\ngeolocalization performance and demonstrates significantly stronger\ngeneralizability to unseen geolocations."}
{"id": "2503.18225", "pdf": "https://arxiv.org/pdf/2503.18225", "abs": "https://arxiv.org/abs/2503.18225", "authors": ["Massimo Bini", "Leander Girrbach", "Zeynep Akata"], "title": "Decoupling Angles and Strength in Low-rank Adaptation", "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": "ICLR 2025", "summary": "Parameter-Efficient FineTuning (PEFT) methods have recently gained\nsignificant popularity thanks to the widespread availability of large-scale\npretrained models. These methods allow for quick adaptation to downstream tasks\nwith minimal computational cost. However, popular finetuning methods such as\nLoRA exhibit limited robustness when it comes to hyperparameter choices or\nextended training regimes, preventing optimal out-of-the-box performance. In\ncontrast, bounded approaches, such as ETHER, provide greater robustness but are\nlimited to extremely low-rank adaptations and fixed-strength transformations,\nreducing their adaptation expressive power. In this work, we propose Decoupled\nLow-rank Adaptation (DeLoRA), a novel finetuning method that normalizes and\nscales learnable low-rank matrices. By bounding the distance of the\ntransformation, DeLoRA effectively decouples the angular learning from the\nadaptation strength, enhancing robustness without compromising performance.\nThrough evaluations on subject-driven image generation, natural language\nunderstanding, and instruction tuning, we show that DeLoRA matches or surpasses\nperformance of competing PEFT methods, while exhibiting stronger robustness.\nCode is available at https://github.com/ExplainableML/DeLoRA."}
{"id": "2503.18147", "pdf": "https://arxiv.org/pdf/2503.18147", "abs": "https://arxiv.org/abs/2503.18147", "authors": ["Ke Niu", "Yuwen Chen", "Haiyang Yu", "Zhuofan Chen", "Xianghui Que", "Bin Li", "Xiangyang Xue"], "title": "PHT-CAD: Efficient CAD Parametric Primitive Analysis with Progressive Hierarchical Tuning", "categories": ["cs.CV"], "comment": null, "summary": "Computer-Aided Design (CAD) plays a pivotal role in industrial manufacturing,\nyet 2D Parametric Primitive Analysis (PPA) remains underexplored due to two key\nchallenges: structural constraint reasoning and advanced semantic\nunderstanding. To tackle these challenges, we first propose an Efficient Hybrid\nParametrization (EHP) for better representing 2D engineering drawings. EHP\ncontains four types of atomic component i.e., point, line, circle, and arc).\nAdditionally, we propose PHT-CAD, a novel 2D PPA framework that harnesses the\nmodality alignment and reasoning capabilities of Vision-Language Models (VLMs)\nfor precise engineering drawing analysis. In PHT-CAD, we introduce four\ndedicated regression heads to predict corresponding atomic components. To train\nPHT-CAD, a three-stage training paradigm Progressive Hierarchical Tuning (PHT)\nis proposed to progressively enhance PHT-CAD's capability to perceive\nindividual primitives, infer structural constraints, and align annotation\nlayers with their corresponding geometric representations. Considering that\nexisting datasets lack complete annotation layers and real-world engineering\ndrawings, we introduce ParaCAD, the first large-scale benchmark that explicitly\nintegrates both the geometric and annotation layers. ParaCAD comprises over 10\nmillion annotated drawings for training and 3,000 real-world industrial\ndrawings with complex topological structures and physical constraints for test.\nExtensive experiments demonstrate the effectiveness of PHT-CAD and highlight\nthe practical significance of ParaCAD in advancing 2D PPA research."}
{"id": "2503.18320", "pdf": "https://arxiv.org/pdf/2503.18320", "abs": "https://arxiv.org/abs/2503.18320", "authors": ["Dong Jing", "Nanyi Fei", "Zhiwu Lu"], "title": "Bridging Writing Manner Gap in Visual Instruction Tuning by Creating LLM-aligned Instructions", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "In the realm of Large Multi-modal Models (LMMs), the instruction quality\nduring the visual instruction tuning stage significantly influences the\nperformance of modality alignment. In this paper, we assess the instruction\nquality from a unique perspective termed \\textbf{Writing Manner}, which\nencompasses the selection of vocabulary, grammar and sentence structure to\nconvey specific semantics. We argue that there exists a substantial writing\nmanner gap between the visual instructions and the base Large Language Models\n(LLMs) within LMMs. This gap forces the pre-trained base LLMs to deviate from\ntheir original writing styles, leading to capability degradation of both base\nLLMs and LMMs. To bridge the writing manner gap while preserving the original\nsemantics, we propose directly leveraging the base LLM to align the writing\nmanner of soft-format visual instructions with that of the base LLM itself,\nresulting in novel LLM-aligned instructions. The manual writing manner\nevaluation results demonstrate that our approach successfully minimizes the\nwriting manner gap. By utilizing LLM-aligned instructions, the baseline models\nLLaVA-7B and QwenVL demonstrate enhanced resistance to hallucinations and\nnon-trivial comprehensive improvements across all $15$ visual and language\nbenchmarks."}
{"id": "2503.18150", "pdf": "https://arxiv.org/pdf/2503.18150", "abs": "https://arxiv.org/abs/2503.18150", "authors": ["Zhuoling Li", "Hossein Rahmani", "Qiuhong Ke", "Jun Liu"], "title": "LongDiff: Training-Free Long Video Generation in One Go", "categories": ["cs.CV"], "comment": null, "summary": "Video diffusion models have recently achieved remarkable results in video\ngeneration. Despite their encouraging performance, most of these models are\nmainly designed and trained for short video generation, leading to challenges\nin maintaining temporal consistency and visual details in long video\ngeneration. In this paper, we propose LongDiff, a novel training-free method\nconsisting of carefully designed components \\ -- Position Mapping (PM) and\nInformative Frame Selection (IFS) \\ -- to tackle two key challenges that hinder\nshort-to-long video generation generalization: temporal position ambiguity and\ninformation dilution. Our LongDiff unlocks the potential of off-the-shelf video\ndiffusion models to achieve high-quality long video generation in one go.\nExtensive experiments demonstrate the efficacy of our method."}
{"id": "2503.18394", "pdf": "https://arxiv.org/pdf/2503.18394", "abs": "https://arxiv.org/abs/2503.18394", "authors": ["Kun Li", "Xinwei Chen", "Tianyou Song", "Chengrui Zhou", "Zhuoran Liu", "Zhenyan Zhang", "Jiangjian Guo", "Qing Shan"], "title": "Solving Situation Puzzles with Large Language Model and External Reformulation", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "In recent years, large language models (LLMs) have shown an impressive\nability to perform arithmetic and symbolic reasoning tasks. However, we found\nthat LLMs (e.g., ChatGPT) cannot perform well on reasoning that requires\nmultiple rounds of dialogue, especially when solving situation puzzles.\nSpecifically, LLMs intend to ask very detailed questions focusing on a specific\naspect or same/similar questions after several rounds of Q&As. To help LLMs get\nout of the above dilemma, we propose a novel external reformulation\nmethodology, where the situation puzzle will be reformulated after several\nrounds of Q&A or when the LLMs raise an incorrect guess. Experiments show\nsuperior performance (e.g., win rate, number of question/guess attempts) of our\nmethod than directly using LLMs for solving situation puzzles, highlighting the\npotential of strategic problem reformulation to enhance the reasoning\ncapabilities of LLMs in complex interactive scenarios."}
{"id": "2503.18155", "pdf": "https://arxiv.org/pdf/2503.18155", "abs": "https://arxiv.org/abs/2503.18155", "authors": ["Kelly O. Marshall", "Omid Poursaeed", "Sergiu Oprea", "Amit Kumar", "Anushrut Jignasu", "Chinmay Hegde", "Yilei Li", "Rakesh Ranjan"], "title": "Decorum: A Language-Based Approach For Style-Conditioned Synthesis of Indoor 3D Scenes", "categories": ["cs.CV"], "comment": null, "summary": "3D indoor scene generation is an important problem for the design of digital\nand real-world environments. To automate this process, a scene generation model\nshould be able to not only generate plausible scene layouts, but also take into\nconsideration visual features and style preferences. Existing methods for this\ntask exhibit very limited control over these attributes, only allowing text\ninputs in the form of simple object-level descriptions or pairwise spatial\nrelationships. Our proposed method Decorum enables users to control the scene\ngeneration process with natural language by adopting language-based\nrepresentations at each stage. This enables us to harness recent advancements\nin Large Language Models (LLMs) to model language-to-language mappings. In\naddition, we show that using a text-based representation allows us to select\nfurniture for our scenes using a novel object retrieval method based on\nmultimodal LLMs. Evaluations on the benchmark 3D-FRONT dataset show that our\nmethods achieve improvements over existing work in text-conditioned scene\nsynthesis and object retrieval."}
{"id": "2503.18435", "pdf": "https://arxiv.org/pdf/2503.18435", "abs": "https://arxiv.org/abs/2503.18435", "authors": ["Junteng Liu", "Weihao Zeng", "Xiwen Zhang", "Yijun Wang", "Zifei Shan", "Junxian He"], "title": "On the Perception Bottleneck of VLMs for Chart Understanding", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Chart understanding requires models to effectively analyze and reason about\nnumerical data, textual elements, and complex visual components. Our\nobservations reveal that the perception capabilities of existing large\nvision-language models (LVLMs) constitute a critical bottleneck in this\nprocess. In this study, we delve into this perception bottleneck by decomposing\nit into two components: the vision encoder bottleneck, where the visual\nrepresentation may fail to encapsulate the correct information, and the\nextraction bottleneck, where the language model struggles to extract the\nnecessary information from the provided visual representations. Through\ncomprehensive experiments, we find that (1) the information embedded within\nvisual representations is substantially richer than what is typically captured\nby linear extractors, such as the widely used retrieval accuracy metric; (2)\nWhile instruction tuning effectively enhances the extraction capability of\nLVLMs, the vision encoder remains a critical bottleneck, demanding focused\nattention and improvement. Therefore, we further enhance the visual encoder to\nmitigate the vision encoder bottleneck under a contrastive learning framework.\nEmpirical results demonstrate that our approach significantly mitigates the\nperception bottleneck and improves the ability of LVLMs to comprehend charts.\nCode is publicly available at https://github.com/hkust-nlp/Vision4Chart."}
{"id": "2503.18159", "pdf": "https://arxiv.org/pdf/2503.18159", "abs": "https://arxiv.org/abs/2503.18159", "authors": ["Peng Chen", "Xiaobao Wei", "Ming Lu", "Hui Chen", "Feng Tian"], "title": "DiffusionTalker: Efficient and Compact Speech-Driven 3D Talking Head via Personalizer-Guided Distillation", "categories": ["cs.CV", "cs.AI", "cs.SD"], "comment": "Accepted by ICME2025", "summary": "Real-time speech-driven 3D facial animation has been attractive in academia\nand industry. Traditional methods mainly focus on learning a deterministic\nmapping from speech to animation. Recent approaches start to consider the\nnondeterministic fact of speech-driven 3D face animation and employ the\ndiffusion model for the task. Existing diffusion-based methods can improve the\ndiversity of facial animation. However, personalized speaking styles conveying\naccurate lip language is still lacking, besides, efficiency and compactness\nstill need to be improved. In this work, we propose DiffusionTalker to address\nthe above limitations via personalizer-guided distillation. In terms of\npersonalization, we introduce a contrastive personalizer that learns identity\nand emotion embeddings to capture speaking styles from audio. We further\npropose a personalizer enhancer during distillation to enhance the influence of\nembeddings on facial animation. For efficiency, we use iterative distillation\nto reduce the steps required for animation generation and achieve more than 8x\nspeedup in inference. To achieve compactness, we distill the large teacher\nmodel into a smaller student model, reducing our model's storage by 86.4\\%\nwhile minimizing performance loss. After distillation, users can derive their\nidentity and emotion embeddings from audio to quickly create personalized\nanimations that reflect specific speaking styles. Extensive experiments are\nconducted to demonstrate that our method outperforms state-of-the-art methods.\nThe code will be released at: https://github.com/ChenVoid/DiffusionTalker."}
{"id": "2503.18458", "pdf": "https://arxiv.org/pdf/2503.18458", "abs": "https://arxiv.org/abs/2503.18458", "authors": ["Luchao Wang", "Qian Ren", "Kaiming He", "Hua Wang", "Zhi Chen", "Yaohua Tang"], "title": "StableGS: A Floater-Free Framework for 3D Gaussian Splatting", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Recent years have witnessed remarkable success of 3D Gaussian Splatting\n(3DGS) in novel view synthesis, surpassing prior differentiable rendering\nmethods in both quality and efficiency. However, its training process suffers\nfrom coupled opacity-color optimization that frequently converges to local\nminima, producing floater artifacts that degrade visual fidelity. We present\nStableGS, a framework that eliminates floaters through cross-view depth\nconsistency constraints while introducing a dual-opacity GS model to decouple\ngeometry and material properties of translucent objects. To further enhance\nreconstruction quality in weakly-textured regions, we integrate DUSt3R depth\nestimation, significantly improving geometric stability. Our method\nfundamentally addresses 3DGS training instabilities, outperforming existing\nstate-of-the-art methods across open-source datasets."}
{"id": "2503.18160", "pdf": "https://arxiv.org/pdf/2503.18160", "abs": "https://arxiv.org/abs/2503.18160", "authors": ["Haoyang Li", "Siyu Zhou", "Liang Wang", "Guodong Long"], "title": "MAO: Efficient Model-Agnostic Optimization of Prompt Tuning for Vision-Language Models", "categories": ["cs.CV", "cs.MM"], "comment": "Accepted by the IEEE International Conference on Multimedia & Expo\n  2025 (ICME 2025); 12 pages, 6 figures, 8 tables", "summary": "Though CLIP-based prompt tuning significantly enhances pre-trained\nVision-Language Models, existing research focuses on reconstructing the model\narchitecture, e.g., additional loss calculation and meta-networks. These\napproaches generally lead to increased complexity and extended training cost.\nTo maintain the efficiency of the tuning process, we propose plug-and-play\nModel-Agnostic Optimization (MAO) for prompt tuning. Without altering any\ncomponents of the prompt tuning backbone, we introduce a Data-Driven\nEnhancement framework to optimize the distribution of the initial data, and\nincorporate an Alterable Regularization module to boost the task-specific\nfeature processing pipeline, thereby improving overall performance while\nmaintaining low computational cost. Extensive experiments on MAO demonstrate\nits outstanding performance and efficiency. The code of MAO is available at:\nhttps://github.com/JREion/M.A.O ."}
{"id": "2503.18476", "pdf": "https://arxiv.org/pdf/2503.18476", "abs": "https://arxiv.org/abs/2503.18476", "authors": ["Wei Deng", "Mengshi Qi", "Huadong Ma"], "title": "Global-Local Tree Search for Language Guided 3D Scene Generation", "categories": ["cs.CV", "cs.CL"], "comment": "Accepted by CVPR 2025", "summary": "Large Vision-Language Models (VLMs), such as GPT-4, have achieved remarkable\nsuccess across various fields. However, there are few studies on 3D indoor\nscene generation with VLMs. This paper considers this task as a planning\nproblem subject to spatial and layout common sense constraints. To solve the\nproblem with a VLM, we propose a new global-local tree search algorithm.\nGlobally, the method places each object sequentially and explores multiple\nplacements during each placement process, where the problem space is\nrepresented as a tree. To reduce the depth of the tree, we decompose the scene\nstructure hierarchically, i.e. room level, region level, floor object level,\nand supported object level. The algorithm independently generates the floor\nobjects in different regions and supported objects placed on different floor\nobjects. Locally, we also decompose the sub-task, the placement of each object,\ninto multiple steps. The algorithm searches the tree of problem space. To\nleverage the VLM model to produce positions of objects, we discretize the\ntop-down view space as a dense grid and fill each cell with diverse emojis to\nmake to cells distinct. We prompt the VLM with the emoji grid and the VLM\nproduces a reasonable location for the object by describing the position with\nthe name of emojis. The quantitative and qualitative experimental results\nillustrate our approach generates more plausible 3D scenes than\nstate-of-the-art approaches. Our source code is available at\nhttps://github.com/dw-dengwei/TreeSearchGen ."}
{"id": "2503.18170", "pdf": "https://arxiv.org/pdf/2503.18170", "abs": "https://arxiv.org/abs/2503.18170", "authors": ["Abderrachid Hamrani", "Anuradha Godavarty"], "title": "Self-Attention Diffusion Models for Zero-Shot Biomedical Image Segmentation: Unlocking New Frontiers in Medical Imaging", "categories": ["cs.CV", "cs.AI"], "comment": "15 pages, 5 figures", "summary": "Producing high-quality segmentation masks for medical images is a fundamental\nchallenge in biomedical image analysis. Recent research has explored\nlarge-scale supervised training to enable segmentation across various medical\nimaging modalities and unsupervised training to facilitate segmentation without\ndense annotations. However, constructing a model capable of segmenting diverse\nmedical images in a zero-shot manner without any annotations remains a\nsignificant hurdle. This paper introduces the Attention Diffusion Zero-shot\nUnsupervised System (ADZUS), a novel approach that leverages self-attention\ndiffusion models for zero-shot biomedical image segmentation. ADZUS harnesses\nthe intrinsic capabilities of pre-trained diffusion models, utilizing their\ngenerative and discriminative potentials to segment medical images without\nrequiring annotated training data or prior domain-specific knowledge. The ADZUS\narchitecture is detailed, with its integration of self-attention mechanisms\nthat facilitate context-aware and detail-sensitive segmentations being\nhighlighted. Experimental results across various medical imaging datasets,\nincluding skin lesion segmentation, chest X-ray infection segmentation, and\nwhite blood cell segmentation, reveal that ADZUS achieves state-of-the-art\nperformance. Notably, ADZUS reached Dice scores ranging from 88.7\\% to 92.9\\%\nand IoU scores from 66.3\\% to 93.3\\% across different segmentation tasks,\ndemonstrating significant improvements in handling novel, unseen medical\nimagery. It is noteworthy that while ADZUS demonstrates high effectiveness, it\ndemands substantial computational resources and extended processing times. The\nmodel's efficacy in zero-shot settings underscores its potential to reduce\nreliance on costly annotations and seamlessly adapt to new medical imaging\ntasks, thereby expanding the diagnostic capabilities of AI-driven medical\nimaging technologies."}
{"id": "2503.18484", "pdf": "https://arxiv.org/pdf/2503.18484", "abs": "https://arxiv.org/abs/2503.18484", "authors": ["Junyuan Gao", "Jiahe Song", "Jiang Wu", "Runchuan Zhu", "Guanlin Shen", "Shasha Wang", "Xingjian Wei", "Haote Yang", "Songyang Zhang", "Weijia Li", "Bin Wang", "Dahua Lin", "Lijun Wu", "Conghui He"], "title": "PM4Bench: A Parallel Multilingual Multi-Modal Multi-task Benchmark for Large Vision Language Model", "categories": ["cs.CV", "cs.CL"], "comment": "Equal contribution: Junyuan Gao, Jiahe Song, Jiang Wu; Corresponding\n  author: Conghui He", "summary": "Existing multilingual benchmarks for Large Vision Language Models (LVLMs)\nsuffer from limitations including language-specific content biases, disjointed\nmultimodal input formats, and a lack of safety evaluation. To address these\ngaps, we propose PM4Bench, the first Parallel Multilingual Multi-Modal\nMulti-task Benchmark for LVLMs. PM4Bench features a parallel corpus design\nacross 10 languages, enabling fair and accurate cross-lingual comparisons. It\nincludes the vision setting where text and queries are embedded in images,\nrequiring LVLMs to simultaneously \"see\", \"read\", and \"think\", aligning with\nreal-world applications. Additionally, PM\\textsuperscript{4}Bench incorporates\nsafety evaluations, addressing critical oversight in existing multilingual\nbenchmarks. Using PM4Bench, we evaluate 11 mainstream LVLMs, revealing\nsignificant cross-linguistic performance disparities, particularly in vision\nsettings, and identifying OCR capability as a key determinant of these\nimbalances. We will release PM4Bench at https://github.com/opendatalab/PM4Bench ."}
{"id": "2503.18177", "pdf": "https://arxiv.org/pdf/2503.18177", "abs": "https://arxiv.org/abs/2503.18177", "authors": ["Gulnaz Gimaletdinova", "Dim Shaiakhmetov", "Madina Akpaeva", "Mukhammadmuso Abduzhabbarov", "Kadyrmamat Momunov"], "title": "Training A Neural Network For Partially Occluded Road Sign Identification In The Context Of Autonomous Vehicles", "categories": ["cs.CV"], "comment": null, "summary": "The increasing number of autonomous vehicles and the rapid development of\ncomputer vision technologies underscore the particular importance of conducting\nresearch on the accuracy of traffic sign recognition. Numerous studies in this\nfield have already achieved significant results, demonstrating high\neffectiveness in addressing traffic sign recognition tasks. However, the task\nbecomes considerably more complex when a sign is partially obscured by\nsurrounding objects, such as tree branches, billboards, or other elements of\nthe urban environment. In our study, we investigated how partial occlusion of\ntraffic signs affects their recognition. For this purpose, we collected a\ndataset comprising 5,746 images, including both fully visible and partially\noccluded signs, and made it publicly available. Using this dataset, we compared\nthe performance of our custom convolutional neural network (CNN), which\nachieved 96% accuracy, with models trained using transfer learning. The best\nresult was obtained by VGG16 with full layer unfreezing, reaching 99% accuracy.\nAdditional experiments revealed that models trained solely on fully visible\nsigns lose effectiveness when recognizing occluded signs. This highlights the\ncritical importance of incorporating real-world data with partial occlusion\ninto training sets to ensure robust model performance in complex practical\nscenarios and to enhance the safety of autonomous driving."}
{"id": "2503.18492", "pdf": "https://arxiv.org/pdf/2503.18492", "abs": "https://arxiv.org/abs/2503.18492", "authors": ["Jungjae Lee", "Dongjae Lee", "Chihun Choi", "Youngmin Im", "Jaeyoung Wi", "Kihong Heo", "Sangeun Oh", "Sunjae Lee", "Insik Shin"], "title": "Safeguarding Mobile GUI Agent via Logic-based Action Verification", "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": null, "summary": "Large Foundation Models (LFMs) have unlocked new possibilities in\nhuman-computer interaction, particularly with the rise of mobile Graphical User\nInterface (GUI) Agents capable of interpreting GUIs. These agents promise to\nrevolutionize mobile computing by allowing users to automate complex mobile\ntasks through simple natural language instructions. However, the inherent\nprobabilistic nature of LFMs, coupled with the ambiguity and context-dependence\nof mobile tasks, makes LFM-based automation unreliable and prone to errors. To\naddress this critical challenge, we introduce VeriSafe Agent (VSA): a formal\nverification system that serves as a logically grounded safeguard for Mobile\nGUI Agents. VSA is designed to deterministically ensure that an agent's actions\nstrictly align with user intent before conducting an action. At its core, VSA\nintroduces a novel autoformalization technique that translates natural language\nuser instructions into a formally verifiable specification, expressed in our\ndomain-specific language (DSL). This enables runtime, rule-based verification,\nallowing VSA to detect and prevent erroneous actions executing an action,\neither by providing corrective feedback or halting unsafe behavior. To the best\nof our knowledge, VSA is the first attempt to bring the rigor of formal\nverification to GUI agent. effectively bridging the gap between LFM-driven\nautomation and formal software verification. We implement VSA using\noff-the-shelf LLM services (GPT-4o) and evaluate its performance on 300 user\ninstructions across 18 widely used mobile apps. The results demonstrate that\nVSA achieves 94.3%-98.33% accuracy in verifying agent actions, representing a\nsignificant 20.4%-25.6% improvement over existing LLM-based verification\nmethods, and consequently increases the GUI agent's task completion rate by\n90%-130%."}
{"id": "2503.18211", "pdf": "https://arxiv.org/pdf/2503.18211", "abs": "https://arxiv.org/abs/2503.18211", "authors": ["Zhengyuan Li", "Kai Cheng", "Anindita Ghosh", "Uttaran Bhattacharya", "Liangyan Gui", "Aniket Bera"], "title": "SimMotionEdit: Text-Based Human Motion Editing with Motion Similarity Prediction", "categories": ["cs.CV"], "comment": "Project URL: https://github.com/lzhyu/SimMotionEdit", "summary": "Text-based 3D human motion editing is a critical yet challenging task in\ncomputer vision and graphics. While training-free approaches have been\nexplored, the recent release of the MotionFix dataset, which includes\nsource-text-motion triplets, has opened new avenues for training, yielding\npromising results. However, existing methods struggle with precise control,\noften leading to misalignment between motion semantics and language\ninstructions. In this paper, we introduce a related task, motion similarity\nprediction, and propose a multi-task training paradigm, where we train the\nmodel jointly on motion editing and motion similarity prediction to foster the\nlearning of semantically meaningful representations. To complement this task,\nwe design an advanced Diffusion-Transformer-based architecture that separately\nhandles motion similarity prediction and motion editing. Extensive experiments\ndemonstrate the state-of-the-art performance of our approach in both editing\nalignment and fidelity."}
{"id": "2503.18494", "pdf": "https://arxiv.org/pdf/2503.18494", "abs": "https://arxiv.org/abs/2503.18494", "authors": ["Hao-Yuan Chen", "Cheng-Pong Huang", "Jui-Ming Yao"], "title": "Verbal Process Supervision Elicits Better Coding Agents", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "The emergence of large language models and their applications as AI agents\nhave significantly advanced state-of-the-art code generation benchmarks,\ntransforming modern software engineering tasks. However, even with test-time\ncomputed reasoning models, these systems still struggle with complex software\nengineering challenges. This work introduces CURA, a code understanding and\nreasoning agent system enhanced with verbal process supervision (VPS),\nachieving a 3.65\\% improvement over baseline models on challenging benchmarks\nlike BigCodeBench. Furthermore, CURA, when paired with the o3-mini model and\nVPS techniques, attains state-of-the-art performance. This work represents a\nstep forward in integrating reasoning-driven architectures with LLM-based code\ngeneration, enabling agentic reasoning for language models to solve complex\nsoftware engineering tasks."}
{"id": "2503.18223", "pdf": "https://arxiv.org/pdf/2503.18223", "abs": "https://arxiv.org/abs/2503.18223", "authors": ["Valentin Gabeff", "Haozhe Qi", "Brendan Flaherty", "Gencer Sumbül", "Alexander Mathis", "Devis Tuia"], "title": "MammAlps: A multi-view video behavior monitoring dataset of wild mammals in the Swiss Alps", "categories": ["cs.CV", "cs.IR", "q-bio.NC", "q-bio.QM"], "comment": "CVPR 2025; Benchmark and code at:\n  https://github.com/eceo-epfl/MammAlps", "summary": "Monitoring wildlife is essential for ecology and ethology, especially in\nlight of the increasing human impact on ecosystems. Camera traps have emerged\nas habitat-centric sensors enabling the study of wildlife populations at scale\nwith minimal disturbance. However, the lack of annotated video datasets limits\nthe development of powerful video understanding models needed to process the\nvast amount of fieldwork data collected. To advance research in wild animal\nbehavior monitoring we present MammAlps, a multimodal and multi-view dataset of\nwildlife behavior monitoring from 9 camera-traps in the Swiss National Park.\nMammAlps contains over 14 hours of video with audio, 2D segmentation maps and\n8.5 hours of individual tracks densely labeled for species and behavior. Based\non 6135 single animal clips, we propose the first hierarchical and multimodal\nanimal behavior recognition benchmark using audio, video and reference scene\nsegmentation maps as inputs. Furthermore, we also propose a second\necology-oriented benchmark aiming at identifying activities, species, number of\nindividuals and meteorological conditions from 397 multi-view and long-term\necological events, including false positive triggers. We advocate that both\ntasks are complementary and contribute to bridging the gap between machine\nlearning and ecology. Code and data are available at:\nhttps://github.com/eceo-epfl/MammAlps"}
{"id": "2503.18556", "pdf": "https://arxiv.org/pdf/2503.18556", "abs": "https://arxiv.org/abs/2503.18556", "authors": ["Bin Li", "Dehong Gao", "Yeyuan Wang", "Linbo Jin", "Shanqing Yu", "Xiaoyan Cai", "Libin Yang"], "title": "Instruction-Aligned Visual Attention for Mitigating Hallucinations in Large Vision-Language Models", "categories": ["cs.CV", "cs.CL"], "comment": "Accepted by ICME2025", "summary": "Despite the significant success of Large Vision-Language models(LVLMs), these\nmodels still suffer hallucinations when describing images, generating answers\nthat include non-existent objects. It is reported that these models tend to\nover-focus on certain irrelevant image tokens that do not contain critical\ninformation for answering the question and distort the output. To address this,\nwe propose an Instruction-Aligned Visual Attention(IAVA) approach, which\nidentifies irrelevant tokens by comparing changes in attention weights under\ntwo different instructions. By applying contrastive decoding, we dynamically\nadjust the logits generated from original image tokens and irrelevant image\ntokens, reducing the model's over-attention to irrelevant information. The\nexperimental results demonstrate that IAVA consistently outperforms existing\ndecoding techniques on benchmarks such as MME, POPE, and TextVQA in mitigating\nobject hallucinations. Our IAVA approach is available online at\nhttps://github.com/Lee-lab558/IAVA."}
{"id": "2503.18227", "pdf": "https://arxiv.org/pdf/2503.18227", "abs": "https://arxiv.org/abs/2503.18227", "authors": ["Yiheng Zhong", "Zihong Luo", "Chengzhi Liu", "Feilong Tang", "Zelin Peng", "Ming Hu", "Yingzhen Hu", "Jionglong Su", "Zongyuan Geand", "Imran Razzak"], "title": "PG-SAM: Prior-Guided SAM with Medical for Multi-organ Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Segment Anything Model (SAM) demonstrates powerful zero-shot capabilities;\nhowever, its accuracy and robustness significantly decrease when applied to\nmedical image segmentation. Existing methods address this issue through\nmodality fusion, integrating textual and image information to provide more\ndetailed priors. In this study, we argue that the granularity of text and the\ndomain gap affect the accuracy of the priors. Furthermore, the discrepancy\nbetween high-level abstract semantics and pixel-level boundary details in\nimages can introduce noise into the fusion process. To address this, we propose\nPrior-Guided SAM (PG-SAM), which employs a fine-grained modality prior aligner\nto leverage specialized medical knowledge for better modality alignment. The\ncore of our method lies in efficiently addressing the domain gap with\nfine-grained text from a medical LLM. Meanwhile, it also enhances the priors'\nquality after modality alignment, ensuring more accurate segmentation. In\naddition, our decoder enhances the model's expressive capabilities through\nmulti-level feature fusion and iterative mask optimizer operations, supporting\nunprompted learning. We also propose a unified pipeline that effectively\nsupplies high-quality semantic information to SAM. Extensive experiments on the\nSynapse dataset demonstrate that the proposed PG-SAM achieves state-of-the-art\nperformance. Our anonymous code is released at\nhttps://github.com/logan-0623/PG-SAM."}
{"id": "2503.18565", "pdf": "https://arxiv.org/pdf/2503.18565", "abs": "https://arxiv.org/abs/2503.18565", "authors": ["Abdoul Majid O. Thiombiano", "Brahim Hnich", "Ali Ben Mrad", "Mohamed Wiem Mkaouer"], "title": "Distil-xLSTM: Learning Attention Mechanisms through Recurrent Structures", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "The current era of Natural Language Processing (NLP) is dominated by\nTransformer models. However, novel architectures relying on recurrent\nmechanisms, such as xLSTM and Mamba, have been proposed as alternatives to\nattention-based models. Although computation is done differently than with the\nattention mechanism mechanism, these recurrent models yield good results and\nsometimes even outperform state-of-the-art attention-based models. In this\nwork, we propose Distil-xLSTM, an xLSTM-based Small Language Model (SLM)\ntrained by distilling knowledge from a Large Language Model (LLM) that shows\npromising results while being compute and scale efficient. Our Distil-xLSTM\nfocuses on approximating a transformer-based model attention parametrization\nusing its recurrent sequence mixing components and shows good results with\nminimal training."}
{"id": "2503.18244", "pdf": "https://arxiv.org/pdf/2503.18244", "abs": "https://arxiv.org/abs/2503.18244", "authors": ["Jungsoo Lee", "Debasmit Das", "Munawar Hayat", "Sungha Choi", "Kyuwoong Hwang", "Fatih Porikli"], "title": "CustomKD: Customizing Large Vision Foundation for Edge Model Improvement via Knowledge Distillation", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "We propose a novel knowledge distillation approach, CustomKD, that\neffectively leverages large vision foundation models (LVFMs) to enhance the\nperformance of edge models (e.g., MobileNetV3). Despite recent advancements in\nLVFMs, such as DINOv2 and CLIP, their potential in knowledge distillation for\nenhancing edge models remains underexplored. While knowledge distillation is a\npromising approach for improving the performance of edge models, the\ndiscrepancy in model capacities and heterogeneous architectures between LVFMs\nand edge models poses a significant challenge. Our observation indicates that\nalthough utilizing larger backbones (e.g., ViT-S to ViT-L) in teacher models\nimproves their downstream task performances, the knowledge distillation from\nthe large teacher models fails to bring as much performance gain for student\nmodels as for teacher models due to the large model discrepancy. Our simple yet\neffective CustomKD customizes the well-generalized features inherent in LVFMs\nto a given student model in order to reduce model discrepancies. Specifically,\nbeyond providing well-generalized original knowledge from teachers, CustomKD\naligns the features of teachers to those of students, making it easy for\nstudents to understand and overcome the large model discrepancy overall.\nCustomKD significantly improves the performances of edge models in scenarios\nwith unlabeled data such as unsupervised domain adaptation (e.g., OfficeHome\nand DomainNet) and semi-supervised learning (e.g., CIFAR-100 with 400 labeled\nsamples and ImageNet with 1% labeled samples), achieving the new\nstate-of-the-art performances."}
{"id": "2503.18570", "pdf": "https://arxiv.org/pdf/2503.18570", "abs": "https://arxiv.org/abs/2503.18570", "authors": ["Tilahun Yeshambel", "Moncef Garouani", "Serge Molina", "Josiane Mothe"], "title": "Dense Retrieval for Low Resource Languages -- the Case of Amharic Language", "categories": ["cs.IR", "cs.CL"], "comment": "4 pages, 2 figures", "summary": "This paper reports some difficulties and some results when using dense\nretrievers on Amharic, one of the low-resource languages spoken by 120 millions\npopulations. The efforts put and difficulties faced by University Addis Ababa\ntoward Amharic Information Retrieval will be developed during the presentation."}
{"id": "2503.18254", "pdf": "https://arxiv.org/pdf/2503.18254", "abs": "https://arxiv.org/abs/2503.18254", "authors": ["Lukas Uzolas", "Elmar Eisemann", "Petr Kellnhofer"], "title": "Surface-Aware Distilled 3D Semantic Features", "categories": ["cs.CV", "cs.GR"], "comment": null, "summary": "Many 3D tasks such as pose alignment, animation, motion transfer, and 3D\nreconstruction rely on establishing correspondences between 3D shapes. This\nchallenge has recently been approached by matching of semantic features from\npre-trained vision models. However, despite their power, these features\nstruggle to differentiate instances of the same semantic class such as \"left\nhand\" versus \"right hand\" which leads to substantial mapping errors. To solve\nthis, we learn a surface-aware embedding space that is robust to these\nambiguities. Importantly, our approach is self-supervised and requires only a\nsmall number of unpaired training meshes to infer features for new 3D shapes at\ntest time. We achieve this by introducing a contrastive loss that preserves the\nsemantic content of the features distilled from foundational models while\ndisambiguating features located far apart on the shape's surface. We observe\nsuperior performance in correspondence matching benchmarks and enable\ndownstream applications including in-part segmentation, pose alignment, and\nmotion transfer. The project site is available at\nhttps://lukas.uzolas.com/SurfaceAware3DFeaturesSite."}
{"id": "2503.18666", "pdf": "https://arxiv.org/pdf/2503.18666", "abs": "https://arxiv.org/abs/2503.18666", "authors": ["Haoyu Wang", "Christopher M. Poskitt", "Jun Sun"], "title": "AgentSpec: Customizable Runtime Enforcement for Safe and Reliable LLM Agents", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Agents built on LLMs are increasingly deployed across diverse domains,\nautomating complex decision-making and task execution. However, their autonomy\nintroduces safety risks, including security vulnerabilities, legal violations,\nand unintended harmful actions. Existing mitigation methods, such as\nmodel-based safeguards and early enforcement strategies, fall short in\nrobustness, interpretability, and adaptability. To address these challenges, we\npropose AgentSpec, a lightweight domain-specific language for specifying and\nenforcing runtime constraints on LLM agents. With AgentSpec, users define\nstructured rules that incorporate triggers, predicates, and enforcement\nmechanisms, ensuring agents operate within predefined safety boundaries. We\nimplement AgentSpec across multiple domains, including code execution, embodied\nagents, and autonomous driving, demonstrating its adaptability and\neffectiveness. Our evaluation shows that AgentSpec successfully prevents unsafe\nexecutions in over 90% of code agent cases, eliminates all hazardous actions in\nembodied agent tasks, and enforces 100% compliance by autonomous vehicles\n(AVs). Despite its strong safety guarantees, AgentSpec remains computationally\nlightweight, with overheads in milliseconds. By combining interpretability,\nmodularity, and efficiency, AgentSpec provides a practical and scalable\nsolution for enforcing LLM agent safety across diverse applications. We also\nautomate the generation of rules using LLMs and assess their effectiveness. Our\nevaluation shows that the rules generated by OpenAI o1 achieve a precision of\n95.56% and recall of 70.96% for embodied agents, successfully identifying\n87.26% of the risky code, and prevent AVs from breaking laws in 5 out of 8\nscenarios."}
{"id": "2503.18267", "pdf": "https://arxiv.org/pdf/2503.18267", "abs": "https://arxiv.org/abs/2503.18267", "authors": ["Minh-Tuan Tran", "Trung Le", "Xuan-May Le", "Thanh-Toan Do", "Dinh Phung"], "title": "Enhancing Dataset Distillation via Non-Critical Region Refinement", "categories": ["cs.CV"], "comment": "Accepted at CVPR 2025", "summary": "Dataset distillation has become a popular method for compressing large\ndatasets into smaller, more efficient representations while preserving critical\ninformation for model training. Data features are broadly categorized into two\ntypes: instance-specific features, which capture unique, fine-grained details\nof individual examples, and class-general features, which represent shared,\nbroad patterns across a class. However, previous approaches often struggle to\nbalance these features-some focus solely on class-general patterns, neglecting\nfiner instance details, while others prioritize instance-specific features,\noverlooking the shared characteristics essential for class-level understanding.\nIn this paper, we introduce the Non-Critical Region Refinement Dataset\nDistillation (NRR-DD) method, which preserves instance-specific details and\nfine-grained regions in synthetic data while enriching non-critical regions\nwith class-general information. This approach enables models to leverage all\npixel information, capturing both feature types and enhancing overall\nperformance. Additionally, we present Distance-Based Representative (DBR)\nknowledge transfer, which eliminates the need for soft labels in training by\nrelying on the distance between synthetic data predictions and one-hot encoded\nlabels. Experimental results show that NRR-DD achieves state-of-the-art\nperformance on both small- and large-scale datasets. Furthermore, by storing\nonly two distances per instance, our method delivers comparable results across\nvarious settings. The code is available at\nhttps://github.com/tmtuan1307/NRR-DD."}
{"id": "2503.18680", "pdf": "https://arxiv.org/pdf/2503.18680", "abs": "https://arxiv.org/abs/2503.18680", "authors": ["Danrui Li", "Yichao Shi", "Yaluo Wang", "Ziying Shi", "Mubbasir Kapadia"], "title": "ArchSeek: Retrieving Architectural Case Studies Using Vision-Language Models", "categories": ["cs.IR", "cs.CL"], "comment": "15 pages, 8 figures, 3 tables. Accepted by CAAD Futures 2025", "summary": "Efficiently searching for relevant case studies is critical in architectural\ndesign, as designers rely on precedent examples to guide or inspire their\nongoing projects. However, traditional text-based search tools struggle to\ncapture the inherently visual and complex nature of architectural knowledge,\noften leading to time-consuming and imprecise exploration. This paper\nintroduces ArchSeek, an innovative case study search system with recommendation\ncapability, tailored for architecture design professionals. Powered by the\nvisual understanding capabilities from vision-language models and cross-modal\nembeddings, it enables text and image queries with fine-grained control, and\ninteraction-based design case recommendations. It offers architects a more\nefficient, personalized way to discover design inspirations, with potential\napplications across other visually driven design fields. The source code is\navailable at https://github.com/danruili/ArchSeek."}
{"id": "2503.18278", "pdf": "https://arxiv.org/pdf/2503.18278", "abs": "https://arxiv.org/abs/2503.18278", "authors": ["Cheng Yang", "Yang Sui", "Jinqi Xiao", "Lingyi Huang", "Yu Gong", "Chendi Li", "Jinghua Yan", "Yu Bai", "Ponnuswamy Sadayappan", "Xia Hu", "Bo Yuan"], "title": "TopV: Compatible Token Pruning with Inference Time Optimization for Fast and Low-Memory Multimodal Vision Language Model", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by CVPR 2025", "summary": "Vision-Language Models (VLMs) demand substantial computational resources\nduring inference, largely due to the extensive visual input tokens for\nrepresenting visual information. Previous studies have noted that visual tokens\ntend to receive less attention than text tokens, suggesting their lower\nimportance during inference and potential for pruning. However, their methods\nencounter several challenges: reliance on greedy heuristic criteria for token\nimportance and incompatibility with FlashAttention and KV cache. To address\nthese issues, we introduce \\textbf{TopV}, a compatible \\textbf{TO}ken\n\\textbf{P}runing with inference Time Optimization for fast and low-memory\n\\textbf{V}LM, achieving efficient pruning without additional training or\nfine-tuning. Instead of relying on attention scores, we formulate token pruning\nas an optimization problem, accurately identifying important visual tokens\nwhile remaining compatible with FlashAttention. Additionally, since we only\nperform this pruning once during the prefilling stage, it effectively reduces\nKV cache size. Our optimization framework incorporates a visual-aware cost\nfunction considering factors such as Feature Similarity, Relative Spatial\nDistance, and Absolute Central Distance, to measure the importance of each\nsource visual token, enabling effective pruning of low-importance tokens.\nExtensive experiments demonstrate that our method outperforms previous token\npruning methods, validating the effectiveness and efficiency of our approach."}
{"id": "2503.18773", "pdf": "https://arxiv.org/pdf/2503.18773", "abs": "https://arxiv.org/abs/2503.18773", "authors": ["Dayou Du", "Shijie Cao", "Jianyi Cheng", "Ting Cao", "Mao Yang"], "title": "BitDecoding: Unlocking Tensor Cores for Long-Context LLMs Decoding with Low-Bit KV Cache", "categories": ["cs.AR", "cs.AI", "cs.CL", "cs.PF"], "comment": null, "summary": "The growing adoption of long-context Large Language Models (LLMs) has\nintroduced significant memory and computational challenges in autoregressive\ndecoding due to the expanding Key-Value (KV) cache. KV cache quantization has\nemerged as a promising solution, with prior work showing that 4-bit or even\n2-bit quantization can maintain model accuracy while reducing memory costs.\nHowever, despite these benefits, preliminary implementations for the low-bit KV\ncache struggle to deliver the expected speedup due to quantization and\ndequantization overheads and the lack of Tensor Cores utilization. In this\nwork, we propose BitDecoding, a GPU-optimized framework that unlocks Tensor\nCores for efficient decoding with low-bit KV cache. Efficiently leveraging\nTensor Cores for low-bit KV cache is challenging due to the dynamic nature of\nKV cache generation at each decoding step. BitDecoding addresses these\nchallenges with a Tensor Cores-Centric BitFusion Scheme that ensures data\nlayout compatibility to enable high utilization of Tensor Cores. Additionally,\nBitDecoding incorporates a warp-efficient parallel decoding kernel and a\nfine-grained asynchronous pipeline, minimizing dequantization overhead and\nimproving computational efficiency. Experiments show that BitDecoding achieves\nup to 7.5x speedup on RTX 4090, 4.8x on A100, and 8.9x on H100, compared to\nFP16 FlashDecoding-v2. It also outperforms the state-of-the-art low-bit KV\ncache implementation (QServe) by up to 4.3x. On LLaMA-3.1-8B with a 128K\nsequence length, BitDecoding reduces single-batch decoding latency by 3x,\ndemonstrating its effectiveness in long-context generation scenarios. The code\nis available at https://github.com/DD-DuDa/BitDecoding."}
{"id": "2503.18282", "pdf": "https://arxiv.org/pdf/2503.18282", "abs": "https://arxiv.org/abs/2503.18282", "authors": ["Kazuhiro Yamada", "Li Yin", "Qingrui Hu", "Ning Ding", "Shunsuke Iwashita", "Jun Ichikawa", "Kiwamu Kotani", "Calvin Yeung", "Keisuke Fujii"], "title": "TrackID3x3: A Dataset and Algorithm for Multi-Player Tracking with Identification and Pose Estimation in 3x3 Basketball Full-court Videos", "categories": ["cs.CV"], "comment": null, "summary": "Multi-object tracking, player identification, and pose estimation are\nfundamental components of sports analytics, essential for analyzing player\nmovements, performance, and tactical strategies. However, existing datasets and\nmethodologies primarily target mainstream team sports such as soccer and\nconventional 5-on-5 basketball, often overlooking scenarios involving\nfixed-camera setups commonly used at amateur levels, less mainstream sports, or\ndatasets that explicitly incorporate pose annotations. In this paper, we\npropose the TrackID3x3 dataset, the first publicly available comprehensive\ndataset specifically designed for multi-player tracking, player identification,\nand pose estimation in 3x3 basketball scenarios. The dataset comprises three\ndistinct subsets (Indoor fixed-camera, Outdoor fixed-camera, and Drone camera\nfootage), capturing diverse full-court camera perspectives and environments. We\nalso introduce the Track-ID task, a simplified variant of the game state\nreconstruction task that excludes field detection and focuses exclusively on\nfixed-camera scenarios. To evaluate performance, we propose a baseline\nalgorithm called Track-ID algorithm, tailored to assess tracking and\nidentification quality. Furthermore, our benchmark experiments, utilizing\nrecent multi-object tracking algorithms (e.g., BoT-SORT-ReID) and top-down pose\nestimation methods (HRNet, RTMPose, and SwinPose), demonstrate robust results\nand highlight remaining challenges. Our dataset and evaluation benchmarks\nprovide a solid foundation for advancing automated analytics in 3x3 basketball.\nDataset and code will be available at\nhttps://github.com/open-starlab/TrackID3x3."}
{"id": "2503.18792", "pdf": "https://arxiv.org/pdf/2503.18792", "abs": "https://arxiv.org/abs/2503.18792", "authors": ["Jingwen Cheng", "Kshitish Ghate", "Wenyue Hua", "William Yang Wang", "Hong Shen", "Fei Fang"], "title": "REALM: A Dataset of Real-World LLM Use Cases", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY"], "comment": "9 pages, 5 figures", "summary": "Large Language Models, such as the GPT series, have driven significant\nindustrial applications, leading to economic and societal transformations.\nHowever, a comprehensive understanding of their real-world applications remains\nlimited. To address this, we introduce REALM, a dataset of over 94,000 LLM use\ncases collected from Reddit and news articles. REALM captures two key\ndimensions: the diverse applications of LLMs and the demographics of their\nusers. It categorizes LLM applications and explores how users' occupations\nrelate to the types of applications they use. By integrating real-world data,\nREALM offers insights into LLM adoption across different domains, providing a\nfoundation for future research on their evolving societal roles. A dedicated\ndashboard https://realm-e7682.web.app/ presents the data."}
{"id": "2503.18283", "pdf": "https://arxiv.org/pdf/2503.18283", "abs": "https://arxiv.org/abs/2503.18283", "authors": ["Bojun Liu", "Yangzhi Ma", "Ao Luo", "Li Li", "Dong Liu"], "title": "Voxel-based Point Cloud Geometry Compression with Space-to-Channel Context", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Voxel-based methods are among the most efficient for point cloud geometry\ncompression, particularly with dense point clouds. However, they face\nlimitations due to a restricted receptive field, especially when handling\nhigh-bit depth point clouds. To overcome this issue, we introduce a stage-wise\nSpace-to-Channel (S2C) context model for both dense point clouds and low-level\nsparse point clouds. This model utilizes a channel-wise autoregressive strategy\nto effectively integrate neighborhood information at a coarse resolution. For\nhigh-level sparse point clouds, we further propose a level-wise S2C context\nmodel that addresses resolution limitations by incorporating Geometry Residual\nCoding (GRC) for consistent-resolution cross-level prediction. Additionally, we\nuse the spherical coordinate system for its compact representation and enhance\nour GRC approach with a Residual Probability Approximation (RPA) module, which\nfeatures a large kernel size. Experimental results show that our S2C context\nmodel not only achieves bit savings while maintaining or improving\nreconstruction quality but also reduces computational complexity compared to\nstate-of-the-art voxel-based compression methods."}
{"id": "2503.18825", "pdf": "https://arxiv.org/pdf/2503.18825", "abs": "https://arxiv.org/abs/2503.18825", "authors": ["Sara Fish", "Julia Shephard", "Minkai Li", "Ran I. Shorrer", "Yannai A. Gonczarowski"], "title": "EconEvals: Benchmarks and Litmus Tests for LLM Agents in Unknown Environments", "categories": ["cs.AI", "cs.CL", "cs.GT"], "comment": null, "summary": "We develop benchmarks for LLM agents that act in, learn from, and strategize\nin unknown environments, the specifications of which the LLM agent must learn\nover time from deliberate exploration. Our benchmarks consist of\ndecision-making tasks derived from key problems in economics. To forestall\nsaturation, the benchmark tasks are synthetically generated with scalable\ndifficulty levels. Additionally, we propose litmus tests, a new kind of\nquantitative measure for LLMs and LLM agents. Unlike benchmarks, litmus tests\nquantify differences in character, values, and tendencies of LLMs and LLM\nagents, by considering their behavior when faced with tradeoffs (e.g.,\nefficiency versus equality) where there is no objectively right or wrong\nbehavior. Overall, our benchmarks and litmus tests assess the abilities and\ntendencies of LLM agents in tackling complex economic problems in diverse\nsettings spanning procurement, scheduling, task allocation, and pricing --\napplications that should grow in importance as such agents are further\nintegrated into the economy."}
{"id": "2503.18286", "pdf": "https://arxiv.org/pdf/2503.18286", "abs": "https://arxiv.org/abs/2503.18286", "authors": ["Siyuan Cheng", "Lingjuan Lyu", "Zhenting Wang", "Xiangyu Zhang", "Vikash Sehwag"], "title": "CO-SPY: Combining Semantic and Pixel Features to Detect Synthetic Images by AI", "categories": ["cs.CV"], "comment": null, "summary": "With the rapid advancement of generative AI, it is now possible to synthesize\nhigh-quality images in a few seconds. Despite the power of these technologies,\nthey raise significant concerns regarding misuse. Current efforts to\ndistinguish between real and AI-generated images may lack generalization, being\neffective for only certain types of generative models and susceptible to\npost-processing techniques like JPEG compression. To overcome these\nlimitations, we propose a novel framework, Co-Spy, that first enhances existing\nsemantic features (e.g., the number of fingers in a hand) and artifact features\n(e.g., pixel value differences), and then adaptively integrates them to achieve\nmore general and robust synthetic image detection. Additionally, we create\nCo-Spy-Bench, a comprehensive dataset comprising 5 real image datasets and 22\nstate-of-the-art generative models, including the latest models like FLUX. We\nalso collect 50k synthetic images in the wild from the Internet to enable\nevaluation in a more practical setting. Our extensive evaluations demonstrate\nthat our detector outperforms existing methods under identical training\nconditions, achieving an average accuracy improvement of approximately 11% to\n34%. The code is available at https://github.com/Megum1/Co-Spy."}
{"id": "2503.18866", "pdf": "https://arxiv.org/pdf/2503.18866", "abs": "https://arxiv.org/abs/2503.18866", "authors": ["Yangjun Ruan", "Neil Band", "Chris J. Maddison", "Tatsunori Hashimoto"], "title": "Reasoning to Learn from Latent Thoughts", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Compute scaling for language model (LM) pretraining has outpaced the growth\nof human-written texts, leading to concerns that data will become the\nbottleneck to LM scaling. To continue scaling pretraining in this\ndata-constrained regime, we propose that explicitly modeling and inferring the\nlatent thoughts that underlie the text generation process can significantly\nimprove pretraining data efficiency. Intuitively, our approach views web text\nas the compressed final outcome of a verbose human thought process and that the\nlatent thoughts contain important contextual knowledge and reasoning steps that\nare critical to data-efficient learning. We empirically demonstrate the\neffectiveness of our approach through data-constrained continued pretraining\nfor math. We first show that synthetic data approaches to inferring latent\nthoughts significantly improve data efficiency, outperforming training on the\nsame amount of raw data (5.7\\% $\\rightarrow$ 25.4\\% on MATH). Furthermore, we\ndemonstrate latent thought inference without a strong teacher, where an LM\nbootstraps its own performance by using an EM algorithm to iteratively improve\nthe capability of the trained LM and the quality of thought-augmented\npretraining data. We show that a 1B LM can bootstrap its performance across at\nleast three iterations and significantly outperform baselines trained on raw\ndata, with increasing gains from additional inference compute when performing\nthe E-step. The gains from inference scaling and EM iterations suggest new\nopportunities for scaling data-constrained pretraining."}
{"id": "2503.18294", "pdf": "https://arxiv.org/pdf/2503.18294", "abs": "https://arxiv.org/abs/2503.18294", "authors": ["Fiseha B. Tesema", "Alejandro Guerra Manzanares", "Tianxiang Cui", "Qian Zhang", "Moses Solomon", "Sean He"], "title": "LGPS: A Lightweight GAN-Based Approach for Polyp Segmentation in Colonoscopy Images", "categories": ["cs.CV"], "comment": "10 pages, 6 Figures", "summary": "Colorectal cancer (CRC) is a major global cause of cancer-related deaths,\nwith early polyp detection and removal during colonoscopy being crucial for\nprevention. While deep learning methods have shown promise in polyp\nsegmentation, challenges such as high computational costs, difficulty in\nsegmenting small or low-contrast polyps, and limited generalizability across\ndatasets persist. To address these issues, we propose LGPS, a lightweight\nGAN-based framework for polyp segmentation. LGPS incorporates three key\ninnovations: (1) a MobileNetV2 backbone enhanced with modified residual blocks\nand Squeeze-and-Excitation (ResE) modules for efficient feature extraction; (2)\nConvolutional Conditional Random Fields (ConvCRF) for precise boundary\nrefinement; and (3) a hybrid loss function combining Binary Cross-Entropy,\nWeighted IoU Loss, and Dice Loss to address class imbalance and enhance\nsegmentation accuracy. LGPS is validated on five benchmark datasets and\ncompared with state-of-the-art(SOTA) methods. On the largest and challenging\nPolypGen test dataset, LGPS achieves a Dice of 0.7299 and an IoU of 0.7867,\noutperformed all SOTA works and demonstrating robust generalization. With only\n1.07 million parameters, LGPS is 17 times smaller than the smallest existing\nmodel, making it highly suitable for real-time clinical applications. Its\nlightweight design and strong performance underscore its potential for\nimproving early CRC diagnosis. Code is available at\nhttps://github.com/Falmi/LGPS/."}
{"id": "2503.18888", "pdf": "https://arxiv.org/pdf/2503.18888", "abs": "https://arxiv.org/abs/2503.18888", "authors": ["Zhengcong Yin", "Daniel W. Goldberg", "Binbin Lin", "Bing Zhou", "Diya Li", "Andong Ma", "Ziqian Ming", "Heng Cai", "Zhe Zhang", "Shaohua Wang", "Shanzhen Gao", "Joey Ying Lee", "Xiao Li", "Da Huo"], "title": "Toward building next-generation Geocoding systems: a systematic review", "categories": ["cs.SE", "cs.CL", "cs.IR"], "comment": null, "summary": "Geocoding systems are widely used in both scientific research for spatial\nanalysis and everyday life through location-based services. The quality of\ngeocoded data significantly impacts subsequent processes and applications,\nunderscoring the need for next-generation systems. In response to this demand,\nthis review first examines the evolving requirements for geocoding inputs and\noutputs across various scenarios these systems must address. It then provides a\ndetailed analysis of how to construct such systems by breaking them down into\nkey functional components and reviewing a broad spectrum of existing\napproaches, from traditional rule-based methods to advanced techniques in\ninformation retrieval, natural language processing, and large language models.\nFinally, we identify opportunities to improve next-generation geocoding systems\nin light of recent technological advances."}
{"id": "2503.18297", "pdf": "https://arxiv.org/pdf/2503.18297", "abs": "https://arxiv.org/abs/2503.18297", "authors": ["Yishen Liu", "Shengda Liu", "Hudan Pan"], "title": "Image-to-Text for Medical Reports Using Adaptive Co-Attention and Triple-LSTM Module", "categories": ["cs.CV"], "comment": null, "summary": "Medical report generation requires specialized expertise that general large\nmodels often fail to accurately capture. Moreover, the inherent repetition and\nsimilarity in medical data make it difficult for models to extract meaningful\nfeatures, resulting in a tendency to overfit. So in this paper, we propose a\nmultimodal model, Co-Attention Triple-LSTM Network (CA-TriNet), a deep learning\nmodel that combines transformer architectures with a Multi-LSTM network. Its\nCo-Attention module synergistically links a vision transformer with a text\ntransformer to better differentiate medical images with similarities, augmented\nby an adaptive weight operator to catch and amplify image labels with minor\nsimilarities. Furthermore, its Triple-LSTM module refines generated sentences\nusing targeted image objects. Extensive evaluations over three public datasets\nhave demonstrated that CA-TriNet outperforms state-of-the-art models in terms\nof comprehensive ability, even pre-trained large language models on some\nmetrics."}
{"id": "2503.18892", "pdf": "https://arxiv.org/pdf/2503.18892", "abs": "https://arxiv.org/abs/2503.18892", "authors": ["Weihao Zeng", "Yuzhen Huang", "Qian Liu", "Wei Liu", "Keqing He", "Zejun Ma", "Junxian He"], "title": "SimpleRL-Zoo: Investigating and Taming Zero Reinforcement Learning for Open Base Models in the Wild", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "DeepSeek-R1 has shown that long chain-of-thought (CoT) reasoning can\nnaturally emerge through a simple reinforcement learning (RL) framework with\nrule-based rewards, where the training may directly start from the base\nmodels-a paradigm referred to as zero RL training. Most recent efforts to\nreproduce zero RL training have primarily focused on the Qwen2.5 model series,\nwhich may not be representative as we find the base models already exhibit\nstrong instruction-following and self-reflection abilities. In this work, we\ninvestigate zero RL training across 10 diverse base models, spanning different\nfamilies and sizes including LLama3-8B, Mistral-7B/24B, DeepSeek-Math-7B,\nQwen2.5-math-7B, and all Qwen2.5 models from 0.5B to 32B. Leveraging several\nkey design strategies-such as adjusting format reward and controlling query\ndifficulty-we achieve substantial improvements in both reasoning accuracy and\nresponse length across most settings. However, by carefully monitoring the\ntraining dynamics, we observe that different base models exhibit distinct\npatterns during training. For instance, the increased response length does not\nalways correlate with the emergence of certain cognitive behaviors such as\nverification (i.e., the \"aha moment\"). Notably, we observe the \"aha moment\" for\nthe first time in small models not from the Qwen family. We share the key\ndesigns that enable successful zero RL training, along with our findings and\npractices. To facilitate further research, we open-source the code, models, and\nanalysis tools."}
{"id": "2503.18312", "pdf": "https://arxiv.org/pdf/2503.18312", "abs": "https://arxiv.org/abs/2503.18312", "authors": ["Jianlong Jin", "Chenglong Zhao", "Ruixin Zhang", "Sheng Shang", "Jianqing Xu", "Jingyun Zhang", "ShaoMing Wang", "Yang Zhao", "Shouhong Ding", "Wei Jia", "Yunsheng Wu"], "title": "Diff-Palm: Realistic Palmprint Generation with Polynomial Creases and Intra-Class Variation Controllable Diffusion Models", "categories": ["cs.CV"], "comment": "Accepted by CVPR2025", "summary": "Palmprint recognition is significantly limited by the lack of large-scale\npublicly available datasets. Previous methods have adopted B\\'ezier curves to\nsimulate the palm creases, which then serve as input for conditional GANs to\ngenerate realistic palmprints. However, without employing real data\nfine-tuning, the performance of the recognition model trained on these\nsynthetic datasets would drastically decline, indicating a large gap between\ngenerated and real palmprints. This is primarily due to the utilization of an\ninaccurate palm crease representation and challenges in balancing intra-class\nvariation with identity consistency. To address this, we introduce a\npolynomial-based palm crease representation that provides a new palm crease\ngeneration mechanism more closely aligned with the real distribution. We also\npropose the palm creases conditioned diffusion model with a novel intra-class\nvariation control method. By applying our proposed $K$-step noise-sharing\nsampling, we are able to synthesize palmprint datasets with large intra-class\nvariation and high identity consistency. Experimental results show that, for\nthe first time, recognition models trained solely on our synthetic datasets,\nwithout any fine-tuning, outperform those trained on real datasets.\nFurthermore, our approach achieves superior recognition performance as the\nnumber of generated identities increases."}
{"id": "2503.18941", "pdf": "https://arxiv.org/pdf/2503.18941", "abs": "https://arxiv.org/abs/2503.18941", "authors": ["Hongru Cai", "Yongqi Li", "Ruifeng Yuan", "Wenjie Wang", "Zhen Zhang", "Wenjie Li", "Tat-Seng Chua"], "title": "Exploring Training and Inference Scaling Laws in Generative Retrieval", "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "Generative retrieval has emerged as a novel paradigm that leverages large\nlanguage models (LLMs) to autoregressively generate document identifiers.\nAlthough promising, the mechanisms that underpin its performance and\nscalability remain largely unclear. We conduct a systematic investigation of\ntraining and inference scaling laws in generative retrieval, exploring how\nmodel size, training data scale, and inference-time compute jointly influence\nretrieval performance. To address the lack of suitable metrics, we propose a\nnovel evaluation measure inspired by contrastive entropy and generation loss,\nproviding a continuous performance signal that enables robust comparisons\nacross diverse generative retrieval methods. Our experiments show that\nn-gram-based methods demonstrate strong alignment with both training and\ninference scaling laws, especially when paired with larger LLMs. Furthermore,\nincreasing inference computation yields substantial performance gains,\nrevealing that generative retrieval can significantly benefit from higher\ncompute budgets at inference. Across these settings, LLaMA models consistently\noutperform T5 models, suggesting a particular advantage for larger decoder-only\nmodels in generative retrieval. Taken together, our findings underscore that\nmodel sizes, data availability, and inference computation interact to unlock\nthe full potential of generative retrieval, offering new insights for designing\nand optimizing future systems."}
{"id": "2503.18324", "pdf": "https://arxiv.org/pdf/2503.18324", "abs": "https://arxiv.org/abs/2503.18324", "authors": ["Basim Azam", "Naveed Akhtar"], "title": "Plug-and-Play Interpretable Responsible Text-to-Image Generation via Dual-Space Multi-facet Concept Control", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Ethical issues around text-to-image (T2I) models demand a comprehensive\ncontrol over the generative content. Existing techniques addressing these\nissues for responsible T2I models aim for the generated content to be fair and\nsafe (non-violent/explicit). However, these methods remain bounded to handling\nthe facets of responsibility concepts individually, while also lacking in\ninterpretability. Moreover, they often require alteration to the original\nmodel, which compromises the model performance. In this work, we propose a\nunique technique to enable responsible T2I generation by simultaneously\naccounting for an extensive range of concepts for fair and safe content\ngeneration in a scalable manner. The key idea is to distill the target T2I\npipeline with an external plug-and-play mechanism that learns an interpretable\ncomposite responsible space for the desired concepts, conditioned on the target\nT2I pipeline. We use knowledge distillation and concept whitening to enable\nthis. At inference, the learned space is utilized to modulate the generative\ncontent. A typical T2I pipeline presents two plug-in points for our approach,\nnamely; the text embedding space and the diffusion model latent space. We\ndevelop modules for both points and show the effectiveness of our approach with\na range of strong results."}
{"id": "2503.18325", "pdf": "https://arxiv.org/pdf/2503.18325", "abs": "https://arxiv.org/abs/2503.18325", "authors": ["Jinjin Zhang", "Guodong Wang", "Yizhou Jin", "Di Huang"], "title": "Towards Training-free Anomaly Detection with Vision and Language Foundation Models", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "Anomaly detection is valuable for real-world applications, such as industrial\nquality inspection. However, most approaches focus on detecting local\nstructural anomalies while neglecting compositional anomalies incorporating\nlogical constraints. In this paper, we introduce LogSAD, a novel multi-modal\nframework that requires no training for both Logical and Structural Anomaly\nDetection. First, we propose a match-of-thought architecture that employs\nadvanced large multi-modal models (i.e. GPT-4V) to generate matching proposals,\nformulating interests and compositional rules of thought for anomaly detection.\nSecond, we elaborate on multi-granularity anomaly detection, consisting of\npatch tokens, sets of interests, and composition matching with vision and\nlanguage foundation models. Subsequently, we present a calibration module to\nalign anomaly scores from different detectors, followed by integration\nstrategies for the final decision. Consequently, our approach addresses both\nlogical and structural anomaly detection within a unified framework and\nachieves state-of-the-art results without the need for training, even when\ncompared to supervised approaches, highlighting its robustness and\neffectiveness. Code is available at https://github.com/zhang0jhon/LogSAD."}
{"id": "2503.18328", "pdf": "https://arxiv.org/pdf/2503.18328", "abs": "https://arxiv.org/abs/2503.18328", "authors": ["Chun Gu", "Xiaofei Wei", "Li Zhang", "Xiatian Zhu"], "title": "TensoFlow: Tensorial Flow-based Sampler for Inverse Rendering", "categories": ["cs.CV"], "comment": "CVPR 2025. Code: https://github.com/fudan-zvg/tensoflow", "summary": "Inverse rendering aims to recover scene geometry, material properties, and\nlighting from multi-view images. Given the complexity of light-surface\ninteractions, importance sampling is essential for the evaluation of the\nrendering equation, as it reduces variance and enhances the efficiency of Monte\nCarlo sampling. Existing inverse rendering methods typically use pre-defined\nnon-learnable importance samplers in prior manually, struggling to effectively\nmatch the spatially and directionally varied integrand and resulting in high\nvariance and suboptimal performance. To address this limitation, we propose the\nconcept of learning a spatially and directionally aware importance sampler for\nthe rendering equation to accurately and flexibly capture the unconstrained\ncomplexity of a typical scene. We further formulate TensoFlow, a generic\napproach for sampler learning in inverse rendering, enabling to closely match\nthe integrand of the rendering equation spatially and directionally.\nConcretely, our sampler is parameterized by normalizing flows, allowing both\ndirectional sampling of incident light and probability density function (PDF)\ninference. To capture the characteristics of the sampler spatially, we learn a\ntensorial representation of the scene space, which imposes spatial conditions,\ntogether with reflected direction, leading to spatially and directionally aware\nsampling distributions. Our model can be optimized by minimizing the difference\nbetween the integrand and our normalizing flow. Extensive experiments validate\nthe superiority of TensoFlow over prior alternatives on both synthetic and\nreal-world benchmarks."}
{"id": "2503.18334", "pdf": "https://arxiv.org/pdf/2503.18334", "abs": "https://arxiv.org/abs/2503.18334", "authors": ["Haotian Zhai", "Xinyu Chen", "Can Zhang", "Tianming Sha", "Ruirui Li"], "title": "Mitigating Cache Noise in Test-Time Adaptation for Large Vision-Language Models", "categories": ["cs.CV"], "comment": "Accepted by ICME 2025 and ICLR 2025 Workshop on Foundation Models in\n  the Wild", "summary": "Test-time adaptation (TTA) of visual language models has recently attracted\nsignificant attention as a solution to the performance degradation caused by\ndistribution shifts in downstream tasks. However, existing cache-based TTA\nmethods have certain limitations. They mainly rely on the accuracy of cached\nfeature labels, and the presence of noisy pseudo-labels can cause these\nfeatures to deviate from their true distribution. This makes cache retrieval\nmethods based on similarity matching highly sensitive to outliers or extreme\nsamples. Moreover, current methods lack effective mechanisms to model class\ndistributions, which limits their ability to fully exploit the potential of\ncached information. To address these challenges, we introduce a comprehensive\nand reliable caching mechanism and propose a novel zero-shot TTA method called\n``Cache, Residual, Gaussian\" (CRG). This method not only employs learnable\nresidual parameters to better align positive and negative visual prototypes\nwith text prototypes, thereby optimizing the quality of cached features, but\nalso incorporates Gaussian Discriminant Analysis (GDA) to dynamically model\nintra-class feature distributions, further mitigating the impact of noisy\nfeatures. Experimental results on 13 benchmarks demonstrate that CRG\noutperforms state-of-the-art TTA methods, showcasing exceptional robustness and\nadaptability."}
{"id": "2503.18337", "pdf": "https://arxiv.org/pdf/2503.18337", "abs": "https://arxiv.org/abs/2503.18337", "authors": ["Zichen Miao", "Wei Chen", "Qiang Qiu"], "title": "Coeff-Tuning: A Graph Filter Subspace View for Tuning Attention-Based Large Models", "categories": ["cs.CV"], "comment": null, "summary": "Transformer-based large pre-trained models have shown remarkable\ngeneralization ability, and various parameter-efficient fine-tuning (PEFT)\nmethods have been proposed to customize these models on downstream tasks with\nminimal computational and memory budgets. Previous PEFT methods are primarily\ndesigned from a tensor-decomposition perspective that tries to effectively tune\nthe linear transformation by finding the smallest subset of parameters to\ntrain. Our study adopts an orthogonal view by representing the attention\noperation as a graph convolution and formulating the multi-head attention maps\nas a convolutional filter subspace, with each attention map as a subspace\nelement. In this paper, we propose to tune the large pre-trained transformers\nby learning a small set of combination coefficients that construct a more\nexpressive filter subspace from the original multi-head attention maps. We show\nanalytically and experimentally that the tuned filter subspace can effectively\nexpand the feature space of the multi-head attention and further enhance the\ncapacity of transformers. We further stabilize the fine-tuning with a residual\nparameterization of the tunable subspace coefficients, and enhance the\ngeneralization with a regularization design by directly applying dropout on the\ntunable coefficient during training. The tunable coefficients take a tiny\nnumber of parameters and can be combined with previous PEFT methods in a\nplug-and-play manner. Extensive experiments show that our approach achieves\nsuperior performances than PEFT baselines with neglectable additional\nparameters."}
{"id": "2503.18338", "pdf": "https://arxiv.org/pdf/2503.18338", "abs": "https://arxiv.org/abs/2503.18338", "authors": ["Wenrui Cai", "Qingjie Liu", "Yunhong Wang"], "title": "SPMTrack: Spatio-Temporal Parameter-Efficient Fine-Tuning with Mixture of Experts for Scalable Visual Tracking", "categories": ["cs.CV"], "comment": "Accepted by CVPR2025", "summary": "Most state-of-the-art trackers adopt one-stream paradigm, using a single\nVision Transformer for joint feature extraction and relation modeling of\ntemplate and search region images. However, relation modeling between different\nimage patches exhibits significant variations. For instance, background regions\ndominated by target-irrelevant information require reduced attention\nallocation, while foreground, particularly boundary areas, need to be be\nemphasized. A single model may not effectively handle all kinds of relation\nmodeling simultaneously. In this paper, we propose a novel tracker called\nSPMTrack based on mixture-of-experts tailored for visual tracking task (TMoE),\ncombining the capability of multiple experts to handle diverse relation\nmodeling more flexibly. Benefiting from TMoE, we extend relation modeling from\nimage pairs to spatio-temporal context, further improving tracking accuracy\nwith minimal increase in model parameters. Moreover, we employ TMoE as a\nparameter-efficient fine-tuning method, substantially reducing trainable\nparameters, which enables us to train SPMTrack of varying scales efficiently\nand preserve the generalization ability of pretrained models to achieve\nsuperior performance. We conduct experiments on seven datasets, and\nexperimental results demonstrate that our method significantly outperforms\ncurrent state-of-the-art trackers. The source code is available at\nhttps://github.com/WenRuiCai/SPMTrack."}
{"id": "2503.18339", "pdf": "https://arxiv.org/pdf/2503.18339", "abs": "https://arxiv.org/abs/2503.18339", "authors": ["Inpyo Hong", "Youngwan Jo", "Hyojeong Lee", "Sunghyun Ahn", "Sanghyun Park"], "title": "GranQ: Granular Zero-Shot Quantization with Unified Layer-Channel Awareness", "categories": ["cs.CV"], "comment": null, "summary": "Zero-shot quantization (ZSQ) enables neural network compression without\ntraining data, which is crucial in restricted data access environments.\nHowever, existing ZSQ methods suffer from significant activation loss in\nlow-bit environments owing to their coarse-grained scaling strategy. To address\nthis issue, we propose GranQ, a novel ZSQ approach that leverages layer-channel\nawareness to minimize the quantization error. Unlike conventional layer- or\nchannel-wise quantization, GranQ dynamically adjusts quantization granularity\nby considering both layer- and channel-level activation distributions. This\nenables fine-grained quantization while minimizing activation distortion.\nAdditionally, we introduce vectorized activation quantization, which enables\nefficient parallel computation and reduces computational overhead while\npreserving accuracy. GranQ achieves superior performance compared with those of\nstate-of-the-art ZSQ methods that employ quantization-aware training. With\nthese findings, we anticipate that GranQ will inspire novel research directions\nbeyond conventional ZSQ approaches focused on data generation and model\ntraining."}
{"id": "2503.18341", "pdf": "https://arxiv.org/pdf/2503.18341", "abs": "https://arxiv.org/abs/2503.18341", "authors": ["Kazuma Kitazawa", "Takahito Aoto", "Satoshi Ikehata", "Tsuyoshi Takatani"], "title": "PS-EIP: Robust Photometric Stereo Based on Event Interval Profile", "categories": ["cs.CV"], "comment": "CVPR2025", "summary": "Recently, the energy-efficient photometric stereo method using an event\ncamera has been proposed to recover surface normals from events triggered by\nchanges in logarithmic Lambertian reflections under a moving directional light\nsource. However, EventPS treats each event interval independently, making it\nsensitive to noise, shadows, and non-Lambertian reflections. This paper\nproposes Photometric Stereo based on Event Interval Profile (PS-EIP), a robust\nmethod that recovers pixelwise surface normals from a time-series profile of\nevent intervals. By exploiting the continuity of the profile and introducing an\noutlier detection method based on profile shape, our approach enhances\nrobustness against outliers from shadows and specular reflections. Experiments\nusing real event data from 3D-printed objects demonstrate that PS-EIP\nsignificantly improves robustness to outliers compared to EventPS's\ndeep-learning variant, EventPS-FCN, without relying on deep learning."}
{"id": "2503.18349", "pdf": "https://arxiv.org/pdf/2503.18349", "abs": "https://arxiv.org/abs/2503.18349", "authors": ["Zekai Deng", "Ye Shi", "Kaiyang Ji", "Lan Xu", "Shaoli Huang", "Jingya Wang"], "title": "Human-Object Interaction with Vision-Language Model Guided Relative Movement Dynamics", "categories": ["cs.CV"], "comment": null, "summary": "Human-Object Interaction (HOI) is vital for advancing simulation, animation,\nand robotics, enabling the generation of long-term, physically plausible\nmotions in 3D environments. However, existing methods often fall short of\nachieving physics realism and supporting diverse types of interactions. To\naddress these challenges, this paper introduces a unified Human-Object\nInteraction framework that provides unified control over interactions with\nstatic scenes and dynamic objects using language commands. The interactions\nbetween human and object parts can always be described as the continuous stable\nRelative Movement Dynamics (RMD) between human and object parts. By leveraging\nthe world knowledge and scene perception capabilities of Vision-Language Models\n(VLMs), we translate language commands into RMD diagrams, which are used to\nguide goal-conditioned reinforcement learning for sequential interaction with\nobjects. Our framework supports long-horizon interactions among dynamic,\narticulated, and static objects. To support the training and evaluation of our\nframework, we present a new dataset named Interplay, which includes multi-round\ntask plans generated by VLMs, covering both static and dynamic HOI tasks.\nExtensive experiments demonstrate that our proposed framework can effectively\nhandle a wide range of HOI tasks, showcasing its ability to maintain long-term,\nmulti-round transitions. For more details, please refer to our project webpage:\nhttps://rmd-hoi.github.io/."}
{"id": "2503.18352", "pdf": "https://arxiv.org/pdf/2503.18352", "abs": "https://arxiv.org/abs/2503.18352", "authors": ["Jinjin Zhang", "Qiuyu Huang", "Junjie Liu", "Xiefan Guo", "Di Huang"], "title": "Diffusion-4K: Ultra-High-Resolution Image Synthesis with Latent Diffusion Models", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "In this paper, we present Diffusion-4K, a novel framework for direct\nultra-high-resolution image synthesis using text-to-image diffusion models. The\ncore advancements include: (1) Aesthetic-4K Benchmark: addressing the absence\nof a publicly available 4K image synthesis dataset, we construct Aesthetic-4K,\na comprehensive benchmark for ultra-high-resolution image generation. We\ncurated a high-quality 4K dataset with carefully selected images and captions\ngenerated by GPT-4o. Additionally, we introduce GLCM Score and Compression\nRatio metrics to evaluate fine details, combined with holistic measures such as\nFID, Aesthetics and CLIPScore for a comprehensive assessment of\nultra-high-resolution images. (2) Wavelet-based Fine-tuning: we propose a\nwavelet-based fine-tuning approach for direct training with photorealistic 4K\nimages, applicable to various latent diffusion models, demonstrating its\neffectiveness in synthesizing highly detailed 4K images. Consequently,\nDiffusion-4K achieves impressive performance in high-quality image synthesis\nand text prompt adherence, especially when powered by modern large-scale\ndiffusion models (e.g., SD3-2B and Flux-12B). Extensive experimental results\nfrom our benchmark demonstrate the superiority of Diffusion-4K in\nultra-high-resolution image synthesis."}
{"id": "2503.18358", "pdf": "https://arxiv.org/pdf/2503.18358", "abs": "https://arxiv.org/abs/2503.18358", "authors": ["Zhanzhong Pang", "Fadime Sener", "Shrinivas Ramasubramanian", "Angela Yao"], "title": "Cost-Sensitive Learning for Long-Tailed Temporal Action Segmentation", "categories": ["cs.CV"], "comment": "BMCV 2024", "summary": "Temporal action segmentation in untrimmed procedural videos aims to densely\nlabel frames into action classes. These videos inherently exhibit long-tailed\ndistributions, where actions vary widely in frequency and duration. In temporal\naction segmentation approaches, we identified a bi-level learning bias. This\nbias encompasses (1) a class-level bias, stemming from class imbalance favoring\nhead classes, and (2) a transition-level bias arising from variations in\ntransitions, prioritizing commonly observed transitions. As a remedy, we\nintroduce a constrained optimization problem to alleviate both biases. We\ndefine learning states for action classes and their associated transitions and\nintegrate them into the optimization process. We propose a novel cost-sensitive\nloss function formulated as a weighted cross-entropy loss, with weights\nadaptively adjusted based on the learning state of actions and their\ntransitions. Experiments on three challenging temporal segmentation benchmarks\nand various frameworks demonstrate the effectiveness of our approach, resulting\nin significant improvements in both per-class frame-wise and segment-wise\nperformance."}
{"id": "2503.18359", "pdf": "https://arxiv.org/pdf/2503.18359", "abs": "https://arxiv.org/abs/2503.18359", "authors": ["Zhanzhong Pang", "Fadime Sener", "Angela Yao"], "title": "Context-Enhanced Memory-Refined Transformer for Online Action Detection", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025", "summary": "Online Action Detection (OAD) detects actions in streaming videos using past\nobservations. State-of-the-art OAD approaches model past observations and their\ninteractions with an anticipated future. The past is encoded using short- and\nlong-term memories to capture immediate and long-range dependencies, while\nanticipation compensates for missing future context. We identify a\ntraining-inference discrepancy in existing OAD methods that hinders learning\neffectiveness. The training uses varying lengths of short-term memory, while\ninference relies on a full-length short-term memory. As a remedy, we propose a\nContext-enhanced Memory-Refined Transformer (CMeRT). CMeRT introduces a\ncontext-enhanced encoder to improve frame representations using additional\nnear-past context. It also features a memory-refined decoder to leverage\nnear-future generation to enhance performance. CMeRT achieves state-of-the-art\nin online detection and anticipation on THUMOS'14, CrossTask, and\nEPIC-Kitchens-100."}
{"id": "2503.18361", "pdf": "https://arxiv.org/pdf/2503.18361", "abs": "https://arxiv.org/abs/2503.18361", "authors": ["Wenyuan Zhang", "Emily Yue-ting Jia", "Junsheng Zhou", "Baorui Ma", "Kanle Shi", "Yu-Shen Liu"], "title": "NeRFPrior: Learning Neural Radiance Field as a Prior for Indoor Scene Reconstruction", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025. Project page:\n  https://wen-yuan-zhang.github.io/NeRFPrior/", "summary": "Recently, it has shown that priors are vital for neural implicit functions to\nreconstruct high-quality surfaces from multi-view RGB images. However, current\npriors require large-scale pre-training, and merely provide geometric clues\nwithout considering the importance of color. In this paper, we present\nNeRFPrior, which adopts a neural radiance field as a prior to learn signed\ndistance fields using volume rendering for surface reconstruction. Our NeRF\nprior can provide both geometric and color clues, and also get trained fast\nunder the same scene without additional data. Based on the NeRF prior, we are\nenabled to learn a signed distance function (SDF) by explicitly imposing a\nmulti-view consistency constraint on each ray intersection for surface\ninference. Specifically, at each ray intersection, we use the density in the\nprior as a coarse geometry estimation, while using the color near the surface\nas a clue to check its visibility from another view angle. For the textureless\nareas where the multi-view consistency constraint does not work well, we\nfurther introduce a depth consistency loss with confidence weights to infer the\nSDF. Our experimental results outperform the state-of-the-art methods under the\nwidely used benchmarks."}
{"id": "2503.18363", "pdf": "https://arxiv.org/pdf/2503.18363", "abs": "https://arxiv.org/abs/2503.18363", "authors": ["Wenyuan Zhang", "Yixiao Yang", "Han Huang", "Liang Han", "Kanle Shi", "Yu-Shen Liu"], "title": "MonoInstance: Enhancing Monocular Priors via Multi-view Instance Alignment for Neural Rendering and Reconstruction", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025. Project page:\n  https://wen-yuan-zhang.github.io/MonoInstance/", "summary": "Monocular depth priors have been widely adopted by neural rendering in\nmulti-view based tasks such as 3D reconstruction and novel view synthesis.\nHowever, due to the inconsistent prediction on each view, how to more\neffectively leverage monocular cues in a multi-view context remains a\nchallenge. Current methods treat the entire estimated depth map\nindiscriminately, and use it as ground truth supervision, while ignoring the\ninherent inaccuracy and cross-view inconsistency in monocular priors. To\nresolve these issues, we propose MonoInstance, a general approach that explores\nthe uncertainty of monocular depths to provide enhanced geometric priors for\nneural rendering and reconstruction. Our key insight lies in aligning each\nsegmented instance depths from multiple views within a common 3D space, thereby\ncasting the uncertainty estimation of monocular depths into a density measure\nwithin noisy point clouds. For high-uncertainty areas where depth priors are\nunreliable, we further introduce a constraint term that encourages the\nprojected instances to align with corresponding instance masks on nearby views.\nMonoInstance is a versatile strategy which can be seamlessly integrated into\nvarious multi-view neural rendering frameworks. Our experimental results\ndemonstrate that MonoInstance significantly improves the performance in both\nreconstruction and novel view synthesis under various benchmarks."}
{"id": "2503.18364", "pdf": "https://arxiv.org/pdf/2503.18364", "abs": "https://arxiv.org/abs/2503.18364", "authors": ["Chenxi Xie", "Minghan Li", "Hui Zeng", "Jun Luo", "Lei Zhang"], "title": "MaSS13K: A Matting-level Semantic Segmentation Benchmark", "categories": ["cs.CV"], "comment": null, "summary": "High-resolution semantic segmentation is essential for applications such as\nimage editing, bokeh imaging, AR/VR, etc. Unfortunately, existing datasets\noften have limited resolution and lack precise mask details and boundaries. In\nthis work, we build a large-scale, matting-level semantic segmentation dataset,\nnamed MaSS13K, which consists of 13,348 real-world images, all at 4K\nresolution. MaSS13K provides high-quality mask annotations of a number of\nobjects, which are categorized into seven categories: human, vegetation,\nground, sky, water, building, and others. MaSS13K features precise masks, with\nan average mask complexity 20-50 times higher than existing semantic\nsegmentation datasets. We consequently present a method specifically designed\nfor high-resolution semantic segmentation, namely MaSSFormer, which employs an\nefficient pixel decoder that aggregates high-level semantic features and\nlow-level texture features across three stages, aiming to produce\nhigh-resolution masks with minimal computational cost. Finally, we propose a\nnew learning paradigm, which integrates the high-quality masks of the seven\ngiven categories with pseudo labels from new classes, enabling MaSSFormer to\ntransfer its accurate segmentation capability to other classes of objects. Our\nproposed MaSSFormer is comprehensively evaluated on the MaSS13K benchmark\ntogether with 14 representative segmentation models. We expect that our\nmeticulously annotated MaSS13K dataset and the MaSSFormer model can facilitate\nthe research of high-resolution and high-quality semantic segmentation.\nDatasets and codes can be found at https://github.com/xiechenxi99/MaSS13K."}
{"id": "2503.18368", "pdf": "https://arxiv.org/pdf/2503.18368", "abs": "https://arxiv.org/abs/2503.18368", "authors": ["Xu Han", "Yuan Tang", "Jinfeng Xu", "Xianzhi Li"], "title": "MoST: Efficient Monarch Sparse Tuning for 3D Representation Learning", "categories": ["cs.CV", "cs.LG"], "comment": "11 pages, 6 figures, 6 tables. Code and weights are available at\n  https://github.com/xhanxu/MoST", "summary": "We introduce Monarch Sparse Tuning (MoST), the first reparameterization-based\nparameter-efficient fine-tuning (PEFT) method tailored for 3D representation\nlearning. Unlike existing adapter-based and prompt-tuning 3D PEFT methods, MoST\nintroduces no additional inference overhead and is compatible with many 3D\nrepresentation learning backbones. At its core, we present a new family of\nstructured matrices for 3D point clouds, Point Monarch, which can capture local\ngeometric features of irregular points while offering high expressiveness. MoST\nreparameterizes the dense update weight matrices as our sparse Point Monarch\nmatrices, significantly reducing parameters while retaining strong performance.\nExperiments on various backbones show that MoST is simple, effective, and\nhighly generalizable. It captures local features in point clouds, achieving\nstate-of-the-art results on multiple benchmarks, e.g., 97.5% acc. on\nScanObjectNN (PB_50_RS) and 96.2% on ModelNet40 classification, while it can\nalso combine with other matrix decompositions (e.g., Low-rank, Kronecker) to\nfurther reduce parameters."}
{"id": "2503.18370", "pdf": "https://arxiv.org/pdf/2503.18370", "abs": "https://arxiv.org/abs/2503.18370", "authors": ["Raquel Vidaurre", "Elena Garces", "Dan Casas"], "title": "DiffusedWrinkles: A Diffusion-Based Model for Data-Driven Garment Animation", "categories": ["cs.CV"], "comment": "BMVC 2024", "summary": "We present a data-driven method for learning to generate animations of 3D\ngarments using a 2D image diffusion model. In contrast to existing methods,\ntypically based on fully connected networks, graph neural networks, or\ngenerative adversarial networks, which have difficulties to cope with\nparametric garments with fine wrinkle detail, our approach is able to\nsynthesize high-quality 3D animations for a wide variety of garments and body\nshapes, while being agnostic to the garment mesh topology. Our key idea is to\nrepresent 3D garment deformations as a 2D layout-consistent texture that\nencodes 3D offsets with respect to a parametric garment template. Using this\nrepresentation, we encode a large dataset of garments simulated in various\nmotions and shapes and train a novel conditional diffusion model that is able\nto synthesize high-quality pose-shape-and-design dependent 3D garment\ndeformations. Since our model is generative, we can synthesize various\nplausible deformations for a given target pose, shape, and design.\nAdditionally, we show that we can further condition our model using an existing\ngarment state, which enables the generation of temporally coherent sequences."}
{"id": "2503.18371", "pdf": "https://arxiv.org/pdf/2503.18371", "abs": "https://arxiv.org/abs/2503.18371", "authors": ["Hankyul Kang", "Gregor Seifer", "Donghyun Lee", "Jongbin Ryu"], "title": "Do Your Best and Get Enough Rest for Continual Learning", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted by IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition (CVPR) 2025", "summary": "According to the forgetting curve theory, we can enhance memory retention by\nlearning extensive data and taking adequate rest. This means that in order to\neffectively retain new knowledge, it is essential to learn it thoroughly and\nensure sufficient rest so that our brain can memorize without forgetting. The\nmain takeaway from this theory is that learning extensive data at once\nnecessitates sufficient rest before learning the same data again. This aspect\nof human long-term memory retention can be effectively utilized to address the\ncontinual learning of neural networks. Retaining new knowledge for a long\nperiod of time without catastrophic forgetting is the critical problem of\ncontinual learning. Therefore, based on Ebbinghaus' theory, we introduce the\nview-batch model that adjusts the learning schedules to optimize the recall\ninterval between retraining the same samples. The proposed view-batch model\nallows the network to get enough rest to learn extensive knowledge from the\nsame samples with a recall interval of sufficient length. To this end, we\nspecifically present two approaches: 1) a replay method that guarantees the\noptimal recall interval, and 2) a self-supervised learning that acquires\nextensive knowledge from a single training sample at a time. We empirically\nshow that these approaches of our method are aligned with the forgetting curve\ntheory, which can enhance long-term memory. In our experiments, we also\ndemonstrate that our method significantly improves many state-of-the-art\ncontinual learning methods in various protocols and scenarios. We open-source\nthis project at https://github.com/hankyul2/ViewBatchModel."}
{"id": "2503.18378", "pdf": "https://arxiv.org/pdf/2503.18378", "abs": "https://arxiv.org/abs/2503.18378", "authors": ["Tianpei Zhang", "Yiming Zhu", "Jufeng Zhao", "Guangmang Cui", "Yuchen Zheng"], "title": "Exploring State Space Model in Wavelet Domain: An Infrared and Visible Image Fusion Network via Wavelet Transform and State Space Model", "categories": ["cs.CV"], "comment": null, "summary": "Deep learning techniques have revolutionized the infrared and visible image\nfusion (IVIF), showing remarkable efficacy on complex scenarios. However,\ncurrent methods do not fully combine frequency domain features with global\nsemantic information, which will result in suboptimal extraction of global\nfeatures across modalities and insufficient preservation of local texture\ndetails. To address these issues, we propose Wavelet-Mamba (W-Mamba), which\nintegrates wavelet transform with the state-space model (SSM). Specifically, we\nintroduce Wavelet-SSM module, which incorporates wavelet-based frequency domain\nfeature extraction and global information extraction through SSM, thereby\neffectively capturing both global and local features. Additionally, we propose\na cross-modal feature attention modulation, which facilitates efficient\ninteraction and fusion between different modalities. The experimental results\nindicate that our method achieves both visually compelling results and superior\nperformance compared to current state-of-the-art methods. Our code is available\nat https://github.com/Lmmh058/W-Mamba."}
{"id": "2503.18382", "pdf": "https://arxiv.org/pdf/2503.18382", "abs": "https://arxiv.org/abs/2503.18382", "authors": ["Hongen Liu", "Cheng Cui", "Yuning Du", "Yi Liu", "Gang Pan"], "title": "PP-FormulaNet: Bridging Accuracy and Efficiency in Advanced Formula Recognition", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Formula recognition is an important task in document intelligence. It\ninvolves converting mathematical expressions from document images into\nstructured symbolic formats that computers can easily work with. LaTeX is the\nmost common format used for this purpose. In this work, we present\nPP-FormulaNet, a state-of-the-art formula recognition model that excels in both\naccuracy and efficiency. To meet the diverse needs of applications, we have\ndeveloped two specialized models: PP-FormulaNet-L, tailored for high-accuracy\nscenarios, and PP-FormulaNet-S, optimized for high-efficiency contexts. Our\nextensive evaluations reveal that PP-FormulaNet-L attains accuracy levels that\nsurpass those of prominent models such as UniMERNet by a significant 6%.\nConversely, PP-FormulaNet-S operates at speeds that are over 16 times faster.\nThese advancements facilitate seamless integration of PP-FormulaNet into a\nbroad spectrum of document processing environments that involve intricate\nmathematical formulas. Furthermore, we introduce a Formula Mining System, which\nis capable of extracting a vast amount of high-quality formula data. This\nsystem further enhances the robustness and applicability of our formula\nrecognition model. Code and models are publicly available at\nPaddleOCR(https://github.com/PaddlePaddle/PaddleOCR) and\nPaddleX(https://github.com/PaddlePaddle/PaddleX)."}
{"id": "2503.18384", "pdf": "https://arxiv.org/pdf/2503.18384", "abs": "https://arxiv.org/abs/2503.18384", "authors": ["Yuan Gao", "Shaobo Xia", "Pu Wang", "Xiaohuan Xi", "Sheng Nie", "Cheng Wang"], "title": "LiDAR Remote Sensing Meets Weak Supervision: Concepts, Methods, and Perspectives", "categories": ["cs.CV"], "comment": null, "summary": "LiDAR (Light Detection and Ranging) enables rapid and accurate acquisition of\nthree-dimensional spatial data, widely applied in remote sensing areas such as\nsurface mapping, environmental monitoring, urban modeling, and forestry\ninventory. LiDAR remote sensing primarily includes data interpretation and\nLiDAR-based inversion. However, LiDAR interpretation typically relies on dense\nand precise annotations, which are costly and time-consuming. Similarly, LiDAR\ninversion depends on scarce supervisory signals and expensive field surveys for\nannotations. To address this challenge, weakly supervised learning has gained\nsignificant attention in recent years, with many methods emerging to tackle\nLiDAR remote sensing tasks using incomplete, inaccurate, and inexact\nannotations, as well as annotations from other domains. Existing review\narticles treat LiDAR interpretation and inversion as separate tasks. This\nreview, for the first time, adopts a unified weakly supervised learning\nperspective to systematically examine research on both LiDAR interpretation and\ninversion. We summarize the latest advancements, provide a comprehensive review\nof the development and application of weakly supervised techniques in LiDAR\nremote sensing, and discuss potential future research directions in this field."}
{"id": "2503.18386", "pdf": "https://arxiv.org/pdf/2503.18386", "abs": "https://arxiv.org/abs/2503.18386", "authors": ["Sicong Feng", "Jielong Yang", "Li Peng"], "title": "Resource-Efficient Motion Control for Video Generation via Dynamic Mask Guidance", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent advances in diffusion models bring new vitality to visual content\ncreation. However, current text-to-video generation models still face\nsignificant challenges such as high training costs, substantial data\nrequirements, and difficulties in maintaining consistency between given text\nand motion of the foreground object. To address these challenges, we propose\nmask-guided video generation, which can control video generation through mask\nmotion sequences, while requiring limited training data. Our model enhances\nexisting architectures by incorporating foreground masks for precise\ntext-position matching and motion trajectory control. Through mask motion\nsequences, we guide the video generation process to maintain consistent\nforeground objects throughout the sequence. Additionally, through a first-frame\nsharing strategy and autoregressive extension approach, we achieve more stable\nand longer video generation. Extensive qualitative and quantitative experiments\ndemonstrate that this approach excels in various video generation tasks, such\nas video editing and generating artistic videos, outperforming previous methods\nin terms of consistency and quality. Our generated results can be viewed in the\nsupplementary materials."}
{"id": "2503.18393", "pdf": "https://arxiv.org/pdf/2503.18393", "abs": "https://arxiv.org/abs/2503.18393", "authors": ["Xinhua Xu", "Hong Liu", "Jianbing Wu", "Jinfu Liu"], "title": "PDDM: Pseudo Depth Diffusion Model for RGB-PD Semantic Segmentation Based in Complex Indoor Scenes", "categories": ["cs.CV"], "comment": null, "summary": "The integration of RGB and depth modalities significantly enhances the\naccuracy of segmenting complex indoor scenes, with depth data from RGB-D\ncameras playing a crucial role in this improvement. However, collecting an\nRGB-D dataset is more expensive than an RGB dataset due to the need for\nspecialized depth sensors. Aligning depth and RGB images also poses challenges\ndue to sensor positioning and issues like missing data and noise. In contrast,\nPseudo Depth (PD) from high-precision depth estimation algorithms can eliminate\nthe dependence on RGB-D sensors and alignment processes, as well as provide\neffective depth information and show significant potential in semantic\nsegmentation. Therefore, to explore the practicality of utilizing pseudo depth\ninstead of real depth for semantic segmentation, we design an RGB-PD\nsegmentation pipeline to integrate RGB and pseudo depth and propose a Pseudo\nDepth Aggregation Module (PDAM) for fully exploiting the informative clues\nprovided by the diverse pseudo depth maps. The PDAM aggregates multiple pseudo\ndepth maps into a single modality, making it easily adaptable to other RGB-D\nsegmentation methods. In addition, the pre-trained diffusion model serves as a\nstrong feature extractor for RGB segmentation tasks, but multi-modal\ndiffusion-based segmentation methods remain unexplored. Therefore, we present a\nPseudo Depth Diffusion Model (PDDM) that adopts a large-scale text-image\ndiffusion model as a feature extractor and a simple yet effective fusion\nstrategy to integrate pseudo depth. To verify the applicability of pseudo depth\nand our PDDM, we perform extensive experiments on the NYUv2 and SUNRGB-D\ndatasets. The experimental results demonstrate that pseudo depth can\neffectively enhance segmentation performance, and our PDDM achieves\nstate-of-the-art performance, outperforming other methods by +6.98 mIoU on\nNYUv2 and +2.11 mIoU on SUNRGB-D."}
{"id": "2503.18402", "pdf": "https://arxiv.org/pdf/2503.18402", "abs": "https://arxiv.org/abs/2503.18402", "authors": ["Youyu Chen", "Junjun Jiang", "Kui Jiang", "Xiao Tang", "Zhihao Li", "Xianming Liu", "Yinyu Nie"], "title": "DashGaussian: Optimizing 3D Gaussian Splatting in 200 Seconds", "categories": ["cs.CV"], "comment": "Accepted by CVPR2025. Project page: https://dashgaussian.github.io", "summary": "3D Gaussian Splatting (3DGS) renders pixels by rasterizing Gaussian\nprimitives, where the rendering resolution and the primitive number, concluded\nas the optimization complexity, dominate the time cost in primitive\noptimization. In this paper, we propose DashGaussian, a scheduling scheme over\nthe optimization complexity of 3DGS that strips redundant complexity to\naccelerate 3DGS optimization. Specifically, we formulate 3DGS optimization as\nprogressively fitting 3DGS to higher levels of frequency components in the\ntraining views, and propose a dynamic rendering resolution scheme that largely\nreduces the optimization complexity based on this formulation. Besides, we\nargue that a specific rendering resolution should cooperate with a proper\nprimitive number for a better balance between computing redundancy and fitting\nquality, where we schedule the growth of the primitives to synchronize with the\nrendering resolution. Extensive experiments show that our method accelerates\nthe optimization of various 3DGS backbones by 45.7% on average while preserving\nthe rendering quality."}
{"id": "2503.18403", "pdf": "https://arxiv.org/pdf/2503.18403", "abs": "https://arxiv.org/abs/2503.18403", "authors": ["Xusheng Cao", "Haori Lu", "Linlan Huang", "Fei Yang", "Xialei Liu", "Ming-Ming Cheng"], "title": "Knowledge Graph Enhanced Generative Multi-modal Models for Class-Incremental Learning", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Continual learning in computer vision faces the critical challenge of\ncatastrophic forgetting, where models struggle to retain prior knowledge while\nadapting to new tasks. Although recent studies have attempted to leverage the\ngeneralization capabilities of pre-trained models to mitigate overfitting on\ncurrent tasks, models still tend to forget details of previously learned\ncategories as tasks progress, leading to misclassification. To address these\nlimitations, we introduce a novel Knowledge Graph Enhanced Generative\nMulti-modal model (KG-GMM) that builds an evolving knowledge graph throughout\nthe learning process. Our approach utilizes relationships within the knowledge\ngraph to augment the class labels and assigns different relations to similar\ncategories to enhance model differentiation. During testing, we propose a\nKnowledge Graph Augmented Inference method that locates specific categories by\nanalyzing relationships within the generated text, thereby reducing the loss of\ndetailed information about old classes when learning new knowledge and\nalleviating forgetting. Experiments demonstrate that our method effectively\nleverages relational information to help the model correct mispredictions,\nachieving state-of-the-art results in both conventional CIL and few-shot CIL\nsettings, confirming the efficacy of knowledge graphs at preserving knowledge\nin the continual learning scenarios."}
{"id": "2503.18405", "pdf": "https://arxiv.org/pdf/2503.18405", "abs": "https://arxiv.org/abs/2503.18405", "authors": ["Xu Fan", "Yuetan Lin", "Bing Gong", "Hao Li"], "title": "Offline Meteorology-Pollution Coupling Global Air Pollution Forecasting Model with Bilinear Pooling", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Air pollution has become a major threat to human health, making accurate\nforecasting crucial for pollution control. Traditional physics-based models\nforecast global air pollution by coupling meteorology and pollution processes,\nusing either online or offline methods depending on whether fully integrated\nwith meteorological models and run simultaneously. However, the high\ncomputational demands of both methods severely limit real-time prediction\nefficiency. Existing deep learning (DL) solutions employ online coupling\nstrategies for global air pollution forecasting, which finetune pollution\nforecasting based on pretrained atmospheric models, requiring substantial\ntraining resources. This study pioneers a DL-based offline coupling framework\nthat utilizes bilinear pooling to achieve offline coupling between\nmeteorological fields and pollutants. The proposed model requires only 13% of\nthe parameters of DL-based online coupling models while achieving competitive\nperformance. Compared with the state-of-the-art global air pollution\nforecasting model CAMS, our approach demonstrates superiority in 63% variables\nacross all forecast time steps and 85% variables in predictions exceeding 48\nhours. This work pioneers experimental validation of the effectiveness of\nmeteorological fields in DL-based global air pollution forecasting,\ndemonstrating that offline coupling meteorological fields with pollutants can\nachieve a 15% relative reduction in RMSE across all pollution variables. The\nresearch establishes a new paradigm for real-time global air pollution warning\nsystems and delivers critical technical support for developing more efficient\nand comprehensive AI-powered global atmospheric forecasting frameworks."}
{"id": "2503.18406", "pdf": "https://arxiv.org/pdf/2503.18406", "abs": "https://arxiv.org/abs/2503.18406", "authors": ["Sherry X. Chen", "Misha Sra", "Pradeep Sen"], "title": "Instruct-CLIP: Improving Instruction-Guided Image Editing with Automated Data Refinement Using Contrastive Learning", "categories": ["cs.CV"], "comment": "Computer Vision and Pattern Recognition 2025", "summary": "Although natural language instructions offer an intuitive way to guide\nautomated image editing, deep-learning models often struggle to achieve\nhigh-quality results, largely due to challenges in creating large, high-quality\ntraining datasets. Previous work has typically relied on text-toimage (T2I)\ngenerative models to produce pairs of original and edited images that simulate\nthe input/output of an instruction-guided image-editing model. However, these\nimage pairs often fail to align with the specified edit instructions due to the\nlimitations of T2I models, which negatively impacts models trained on such\ndatasets. To address this, we present Instruct-CLIP, a self-supervised method\nthat learns the semantic changes between original and edited images to refine\nand better align the instructions in existing datasets. Furthermore, we adapt\nInstruct-CLIP to handle noisy latent images and diffusion timesteps so that it\ncan be used to train latent diffusion models (LDMs) [19] and efficiently\nenforce alignment between the edit instruction and the image changes in latent\nspace at any step of the diffusion pipeline. We use Instruct-CLIP to correct\nthe InstructPix2Pix dataset and get over 120K refined samples we then use to\nfine-tune their model, guided by our novel Instruct-CLIP-based loss function.\nThe resulting model can produce edits that are more aligned with the given\ninstructions. Our code and dataset are available at\nhttps://github.com/SherryXTChen/Instruct-CLIP.git."}
{"id": "2503.18407", "pdf": "https://arxiv.org/pdf/2503.18407", "abs": "https://arxiv.org/abs/2503.18407", "authors": ["Wencheng Zhu", "Yuexin Wang", "Hongxuan Li", "Pengfei Zhu", "Danqing Song", "Qinghua Hu"], "title": "VTD-CLIP: Video-to-Text Discretization via Prompting CLIP", "categories": ["cs.CV"], "comment": null, "summary": "Vision-language models bridge visual and linguistic understanding and have\nproven to be powerful for video recognition tasks. Existing approaches\nprimarily rely on parameter-efficient fine-tuning of image-text pre-trained\nmodels, yet they often suffer from limited interpretability and poor\ngeneralization due to inadequate temporal modeling. To address these, we\npropose a simple yet effective video-to-text discretization framework. Our\nmethod repurposes the frozen text encoder to construct a visual codebook from\nvideo class labels due to the many-to-one contrastive alignment between visual\nand textual embeddings in multimodal pretraining. This codebook effectively\ntransforms temporal visual data into textual tokens via feature lookups and\noffers interpretable video representations through explicit video modeling.\nThen, to enhance robustness against irrelevant or noisy frames, we introduce a\nconfidence-aware fusion module that dynamically weights keyframes by assessing\ntheir semantic relevance via the codebook. Furthermore, our method incorporates\nlearnable text prompts to conduct adaptive codebook updates. Extensive\nexperiments on HMDB-51, UCF-101, SSv2, and Kinetics-400 have validated the\nsuperiority of our approach, achieving more competitive improvements over\nstate-of-the-art methods. The code will be publicly available at\nhttps://github.com/isxinxin/VTD-CLIP."}
{"id": "2503.18408", "pdf": "https://arxiv.org/pdf/2503.18408", "abs": "https://arxiv.org/abs/2503.18408", "authors": ["Jiacheng Wu", "Ruiqi Zhang", "Jie Chen", "Hui Zhang"], "title": "Fast and Physically-based Neural Explicit Surface for Relightable Human Avatars", "categories": ["cs.CV"], "comment": null, "summary": "Efficiently modeling relightable human avatars from sparse-view videos is\ncrucial for AR/VR applications. Current methods use neural implicit\nrepresentations to capture dynamic geometry and reflectance, which incur high\ncosts due to the need for dense sampling in volume rendering. To overcome these\nchallenges, we introduce Physically-based Neural Explicit Surface (PhyNES),\nwhich employs compact neural material maps based on the Neural Explicit Surface\n(NES) representation. PhyNES organizes human models in a compact 2D space,\nenhancing material disentanglement efficiency. By connecting Signed Distance\nFields to explicit surfaces, PhyNES enables efficient geometry inference around\na parameterized human shape model. This approach models dynamic geometry,\ntexture, and material maps as 2D neural representations, enabling efficient\nrasterization. PhyNES effectively captures physical surface attributes under\nvarying illumination, enabling real-time physically-based rendering.\nExperiments show that PhyNES achieves relighting quality comparable to SOTA\nmethods while significantly improving rendering speed, memory efficiency, and\nreconstruction quality."}
{"id": "2503.18414", "pdf": "https://arxiv.org/pdf/2503.18414", "abs": "https://arxiv.org/abs/2503.18414", "authors": ["Yuchuan Tian", "Hanting Chen", "Mengyu Zheng", "Yuchen Liang", "Chao Xu", "Yunhe Wang"], "title": "U-REPA: Aligning Diffusion U-Nets to ViTs", "categories": ["cs.CV"], "comment": "15 pages, 7 figures", "summary": "Representation Alignment (REPA) that aligns Diffusion Transformer (DiT)\nhidden-states with ViT visual encoders has proven highly effective in DiT\ntraining, demonstrating superior convergence properties, but it has not been\nvalidated on the canonical diffusion U-Net architecture that shows faster\nconvergence compared to DiTs. However, adapting REPA to U-Net architectures\npresents unique challenges: (1) different block functionalities necessitate\nrevised alignment strategies; (2) spatial-dimension inconsistencies emerge from\nU-Net's spatial downsampling operations; (3) space gaps between U-Net and ViT\nhinder the effectiveness of tokenwise alignment. To encounter these challenges,\nwe propose U-REPA, a representation alignment paradigm that bridges U-Net\nhidden states and ViT features as follows: Firstly, we propose via observation\nthat due to skip connection, the middle stage of U-Net is the best alignment\noption. Secondly, we propose upsampling of U-Net features after passing them\nthrough MLPs. Thirdly, we observe difficulty when performing tokenwise\nsimilarity alignment, and further introduces a manifold loss that regularizes\nthe relative similarity between samples. Experiments indicate that the\nresulting U-REPA could achieve excellent generation quality and greatly\naccelerates the convergence speed. With CFG guidance interval, U-REPA could\nreach $FID<1.5$ in 200 epochs or 1M iterations on ImageNet 256 $\\times$ 256,\nand needs only half the total epochs to perform better than REPA. Codes are\navailable at https://github.com/YuchuanTian/U-REPA."}
{"id": "2503.18420", "pdf": "https://arxiv.org/pdf/2503.18420", "abs": "https://arxiv.org/abs/2503.18420", "authors": ["Dian Zheng", "Cheng Zhang", "Xiao-Ming Wu", "Cao Li", "Chengfei Lv", "Jian-Fang Hu", "Wei-Shi Zheng"], "title": "Panorama Generation From NFoV Image Done Right", "categories": ["cs.CV"], "comment": "CVPR2025. Project\n  page:https://isee-laboratory.github.io/PanoDecouple/\n  Code:https://github.com/iSEE-Laboratory/PanoDecouple/", "summary": "Generating 360-degree panoramas from narrow field of view (NFoV) image is a\npromising computer vision task for Virtual Reality (VR) applications. Existing\nmethods mostly assess the generated panoramas with InceptionNet or CLIP based\nmetrics, which tend to perceive the image quality and is \\textbf{not suitable\nfor evaluating the distortion}. In this work, we first propose a\ndistortion-specific CLIP, named Distort-CLIP to accurately evaluate the\npanorama distortion and discover the \\textbf{``visual cheating''} phenomenon in\nprevious works (\\ie, tending to improve the visual results by sacrificing\ndistortion accuracy). This phenomenon arises because prior methods employ a\nsingle network to learn the distinct panorama distortion and content completion\nat once, which leads the model to prioritize optimizing the latter. To address\nthe phenomenon, we propose \\textbf{PanoDecouple}, a decoupled diffusion model\nframework, which decouples the panorama generation into distortion guidance and\ncontent completion, aiming to generate panoramas with both accurate distortion\nand visual appeal. Specifically, we design a DistortNet for distortion guidance\nby imposing panorama-specific distortion prior and a modified condition\nregistration mechanism; and a ContentNet for content completion by imposing\nperspective image information. Additionally, a distortion correction loss\nfunction with Distort-CLIP is introduced to constrain the distortion\nexplicitly. The extensive experiments validate that PanoDecouple surpasses\nexisting methods both in distortion and visual metrics."}
{"id": "2503.18421", "pdf": "https://arxiv.org/pdf/2503.18421", "abs": "https://arxiv.org/abs/2503.18421", "authors": ["Qiang Hu", "Zihan Zheng", "Houqiang Zhong", "Sihua Fu", "Li Song", "XiaoyunZhang", "Guangtao Zhai", "Yanfeng Wang"], "title": "4DGC: Rate-Aware 4D Gaussian Compression for Efficient Streamable Free-Viewpoint Video", "categories": ["cs.CV", "eess.IV"], "comment": "CVPR2025", "summary": "3D Gaussian Splatting (3DGS) has substantial potential for enabling\nphotorealistic Free-Viewpoint Video (FVV) experiences. However, the vast number\nof Gaussians and their associated attributes poses significant challenges for\nstorage and transmission. Existing methods typically handle dynamic 3DGS\nrepresentation and compression separately, neglecting motion information and\nthe rate-distortion (RD) trade-off during training, leading to performance\ndegradation and increased model redundancy. To address this gap, we propose\n4DGC, a novel rate-aware 4D Gaussian compression framework that significantly\nreduces storage size while maintaining superior RD performance for FVV.\nSpecifically, 4DGC introduces a motion-aware dynamic Gaussian representation\nthat utilizes a compact motion grid combined with sparse compensated Gaussians\nto exploit inter-frame similarities. This representation effectively handles\nlarge motions, preserving quality and reducing temporal redundancy.\nFurthermore, we present an end-to-end compression scheme that employs\ndifferentiable quantization and a tiny implicit entropy model to compress the\nmotion grid and compensated Gaussians efficiently. The entire framework is\njointly optimized using a rate-distortion trade-off. Extensive experiments\ndemonstrate that 4DGC supports variable bitrates and consistently outperforms\nexisting methods in RD performance across multiple datasets."}
{"id": "2503.18422", "pdf": "https://arxiv.org/pdf/2503.18422", "abs": "https://arxiv.org/abs/2503.18422", "authors": ["Handong Li", "Yiyuan Zhang", "Longteng Guo", "Xiangyu Yue", "Jing Liu"], "title": "Breaking the Encoder Barrier for Seamless Video-Language Understanding", "categories": ["cs.CV"], "comment": "12 pages", "summary": "Most Video-Large Language Models (Video-LLMs) adopt an encoder-decoder\nframework, where a vision encoder extracts frame-wise features for processing\nby a language model. However, this approach incurs high computational costs,\nintroduces resolution biases, and struggles to capture fine-grained multimodal\ninteractions. To overcome these limitations, we propose ELVA, an encoder-free\nVideo-LLM that directly models nuanced video-language interactions without\nrelying on a vision encoder. ELVA employs token merging to construct a\nbottom-up hierarchical representation and incorporates a video guidance\nsupervisor for direct spatiotemporal representation learning. Additionally, a\nhybrid-resolution mechanism strategically integrates high- and low-resolution\nframes as inputs to achieve an optimal balance between performance and\nefficiency. With only 7M publicly available video-text pairs, ELVA achieves\nperformance on par with encoder-based Video-LLMs while reducing FLOPs by up to\n95\\% and inference latency by 92\\%, offering a scalable and efficient solution\nfor real-time video understanding."}
{"id": "2503.18429", "pdf": "https://arxiv.org/pdf/2503.18429", "abs": "https://arxiv.org/abs/2503.18429", "authors": ["Dingcheng Zhen", "Shunshun Yin", "Shiyang Qin", "Hou Yi", "Ziwei Zhang", "Siyuan Liu", "Gan Qi", "Ming Tao"], "title": "Teller: Real-Time Streaming Audio-Driven Portrait Animation with Autoregressive Motion Generation", "categories": ["cs.CV"], "comment": "Accept in CVPR 2025 Conference Submission", "summary": "In this work, we introduce the first autoregressive framework for real-time,\naudio-driven portrait animation, a.k.a, talking head. Beyond the challenge of\nlengthy animation times, a critical challenge in realistic talking head\ngeneration lies in preserving the natural movement of diverse body parts. To\nthis end, we propose Teller, the first streaming audio-driven protrait\nanimation framework with autoregressive motion generation. Specifically, Teller\nfirst decomposes facial and body detail animation into two components: Facial\nMotion Latent Generation (FMLG) based on an autoregressive transfromer, and\nmovement authenticity refinement using a Efficient Temporal Module\n(ETM).Concretely, FMLG employs a Residual VQ model to map the facial motion\nlatent from the implicit keypoint-based model into discrete motion tokens,\nwhich are then temporally sliced with audio embeddings. This enables the AR\ntranformer to learn real-time, stream-based mappings from audio to motion.\nFurthermore, Teller incorporate ETM to capture finer motion details. This\nmodule ensures the physical consistency of body parts and accessories, such as\nneck muscles and earrings, improving the realism of these movements. Teller is\ndesigned to be efficient, surpassing the inference speed of diffusion-based\nmodels (Hallo 20.93s vs. Teller 0.92s for one second video generation), and\nachieves a real-time streaming performance of up to 25 FPS. Extensive\nexperiments demonstrate that our method outperforms recent audio-driven\nportrait animation models, especially in small movements, as validated by human\nevaluations with a significant margin in quality and realism."}
{"id": "2503.18430", "pdf": "https://arxiv.org/pdf/2503.18430", "abs": "https://arxiv.org/abs/2503.18430", "authors": ["Zhichao Sun", "Huazhang Hu", "Yidong Ma", "Gang Liu", "Nemo Chen", "Xu Tang", "Yongchao Xu"], "title": "CQ-DINO: Mitigating Gradient Dilution via Category Queries for Vast Vocabulary Object Detection", "categories": ["cs.CV"], "comment": null, "summary": "With the exponential growth of data, traditional object detection methods are\nincreasingly struggling to handle vast vocabulary object detection tasks\neffectively. We analyze two key limitations of classification-based detectors:\npositive gradient dilution, where rare positive categories receive insufficient\nlearning signals, and hard negative gradient dilution, where discriminative\ngradients are overwhelmed by numerous easy negatives. To address these\nchallenges, we propose CQ-DINO, a category query-based object detection\nframework that reformulates classification as a contrastive task between object\nqueries and learnable category queries. Our method introduces image-guided\nquery selection, which reduces the negative space by adaptively retrieving\ntop-K relevant categories per image via cross-attention, thereby rebalancing\ngradient distributions and facilitating implicit hard example mining.\nFurthermore, CQ-DINO flexibly integrates explicit hierarchical category\nrelationships in structured datasets (e.g., V3Det) or learns implicit category\ncorrelations via self-attention in generic datasets (e.g., COCO). Experiments\ndemonstrate that CQ-DINO achieves superior performance on the challenging V3Det\nbenchmark (surpassing previous methods by 2.1% AP) while maintaining\ncompetitiveness in COCO. Our work provides a scalable solution for real-world\ndetection systems requiring wide category coverage. The dataset and code will\nbe publicly at https://github.com/RedAIGC/CQ-DINO."}
{"id": "2503.18434", "pdf": "https://arxiv.org/pdf/2503.18434", "abs": "https://arxiv.org/abs/2503.18434", "authors": ["Zhaoqing Zhu", "Chuwei Luo", "Zirui Shao", "Feiyu Gao", "Hangdi Xing", "Qi Zheng", "Ji Zhang"], "title": "A Simple yet Effective Layout Token in Large Language Models for Document Understanding", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "Recent methods that integrate spatial layouts with text for document\nunderstanding in large language models (LLMs) have shown promising results. A\ncommonly used method is to represent layout information as text tokens and\ninterleave them with text content as inputs to the LLMs. However, such a method\nstill demonstrates limitations, as it requires additional position IDs for\ntokens that are used to represent layout information. Due to the constraint on\nmax position IDs, assigning them to layout information reduces those available\nfor text content, reducing the capacity for the model to learn from the text\nduring training, while also introducing a large number of potentially untrained\nposition IDs during long-context inference, which can hinder performance on\ndocument understanding tasks. To address these issues, we propose LayTokenLLM,\na simple yet effective method for document understanding. LayTokenLLM\nrepresents layout information as a single token per text segment and uses a\nspecialized positional encoding scheme. It shares position IDs between text and\nlayout tokens, eliminating the need for additional position IDs. This design\nmaintains the model's capacity to learn from text while mitigating long-context\nissues during inference. Furthermore, a novel pre-training objective called\nNext Interleaved Text and Layout Token Prediction (NTLP) is devised to enhance\ncross-modality learning between text and layout tokens. Extensive experiments\nshow that LayTokenLLM outperforms existing layout-integrated LLMs and MLLMs of\nsimilar scales on multi-page document understanding tasks, as well as most\nsingle-page tasks."}
{"id": "2503.18435", "pdf": "https://arxiv.org/pdf/2503.18435", "abs": "https://arxiv.org/abs/2503.18435", "authors": ["Junteng Liu", "Weihao Zeng", "Xiwen Zhang", "Yijun Wang", "Zifei Shan", "Junxian He"], "title": "On the Perception Bottleneck of VLMs for Chart Understanding", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Chart understanding requires models to effectively analyze and reason about\nnumerical data, textual elements, and complex visual components. Our\nobservations reveal that the perception capabilities of existing large\nvision-language models (LVLMs) constitute a critical bottleneck in this\nprocess. In this study, we delve into this perception bottleneck by decomposing\nit into two components: the vision encoder bottleneck, where the visual\nrepresentation may fail to encapsulate the correct information, and the\nextraction bottleneck, where the language model struggles to extract the\nnecessary information from the provided visual representations. Through\ncomprehensive experiments, we find that (1) the information embedded within\nvisual representations is substantially richer than what is typically captured\nby linear extractors, such as the widely used retrieval accuracy metric; (2)\nWhile instruction tuning effectively enhances the extraction capability of\nLVLMs, the vision encoder remains a critical bottleneck, demanding focused\nattention and improvement. Therefore, we further enhance the visual encoder to\nmitigate the vision encoder bottleneck under a contrastive learning framework.\nEmpirical results demonstrate that our approach significantly mitigates the\nperception bottleneck and improves the ability of LVLMs to comprehend charts.\nCode is publicly available at https://github.com/hkust-nlp/Vision4Chart."}
{"id": "2503.18438", "pdf": "https://arxiv.org/pdf/2503.18438", "abs": "https://arxiv.org/abs/2503.18438", "authors": ["Guosheng Zhao", "Xiaofeng Wang", "Chaojun Ni", "Zheng Zhu", "Wenkang Qin", "Guan Huang", "Xingang Wang"], "title": "ReconDreamer++: Harmonizing Generative and Reconstructive Models for Driving Scene Representation", "categories": ["cs.CV"], "comment": "Project Page: https://recondreamer-plus.github.io/", "summary": "Combining reconstruction models with generative models has emerged as a\npromising paradigm for closed-loop simulation in autonomous driving. For\nexample, ReconDreamer has demonstrated remarkable success in rendering\nlarge-scale maneuvers. However, a significant gap remains between the generated\ndata and real-world sensor observations, particularly in terms of fidelity for\nstructured elements, such as the ground surface. To address these challenges,\nwe propose ReconDreamer++, an enhanced framework that significantly improves\nthe overall rendering quality by mitigating the domain gap and refining the\nrepresentation of the ground surface. Specifically, ReconDreamer++ introduces\nthe Novel Trajectory Deformable Network (NTDNet), which leverages learnable\nspatial deformation mechanisms to bridge the domain gap between synthesized\nnovel views and original sensor observations. Moreover, for structured elements\nsuch as the ground surface, we preserve geometric prior knowledge in 3D\nGaussians, and the optimization process focuses on refining appearance\nattributes while preserving the underlying geometric structure. Experimental\nevaluations conducted on multiple datasets (Waymo, nuScenes, PandaSet, and\nEUVS) confirm the superior performance of ReconDreamer++. Specifically, on\nWaymo, ReconDreamer++ achieves performance comparable to Street Gaussians for\nthe original trajectory while significantly outperforming ReconDreamer on novel\ntrajectories. In particular, it achieves substantial improvements, including a\n6.1% increase in NTA-IoU, a 23. 0% improvement in FID, and a remarkable 4.5%\ngain in the ground surface metric NTL-IoU, highlighting its effectiveness in\naccurately reconstructing structured elements such as the road surface."}
{"id": "2503.18445", "pdf": "https://arxiv.org/pdf/2503.18445", "abs": "https://arxiv.org/abs/2503.18445", "authors": ["Chenfei Liao", "Kaiyu Lei", "Xu Zheng", "Junha Moon", "Zhixiong Wang", "Yixuan Wang", "Danda Pani Paudel", "Luc Van Gool", "Xuming Hu"], "title": "Benchmarking Multi-modal Semantic Segmentation under Sensor Failures: Missing and Noisy Modality Robustness", "categories": ["cs.CV"], "comment": null, "summary": "Multi-modal semantic segmentation (MMSS) addresses the limitations of\nsingle-modality data by integrating complementary information across\nmodalities. Despite notable progress, a significant gap persists between\nresearch and real-world deployment due to variability and uncertainty in\nmulti-modal data quality. Robustness has thus become essential for practical\nMMSS applications. However, the absence of standardized benchmarks for\nevaluating robustness hinders further advancement. To address this, we first\nsurvey existing MMSS literature and categorize representative methods to\nprovide a structured overview. We then introduce a robustness benchmark that\nevaluates MMSS models under three scenarios: Entire-Missing Modality (EMM),\nRandom-Missing Modality (RMM), and Noisy Modality (NM). From a probabilistic\nstandpoint, we model modality failure under two conditions: (1) all damaged\ncombinations are equally probable; (2) each modality fails independently\nfollowing a Bernoulli distribution. Based on these, we propose four\nmetrics-$mIoU^{Avg}_{EMM}$, $mIoU^{E}_{EMM}$, $mIoU^{Avg}_{RMM}$, and\n$mIoU^{E}_{RMM}$-to assess model robustness under EMM and RMM. This work\nprovides the first dedicated benchmark for MMSS robustness, offering new\ninsights and tools to advance the field. Source code is available at\nhttps://github.com/Chenfei-Liao/Multi-Modal-Semantic-Segmentation-Robustness-Benchmark."}
{"id": "2503.18446", "pdf": "https://arxiv.org/pdf/2503.18446", "abs": "https://arxiv.org/abs/2503.18446", "authors": ["Jinho Jeong", "Sangmin Han", "Jinwoo Kim", "Seon Joo Kim"], "title": "Latent Space Super-Resolution for Higher-Resolution Image Generation with Diffusion Models", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025", "summary": "In this paper, we propose LSRNA, a novel framework for higher-resolution\n(exceeding 1K) image generation using diffusion models by leveraging\nsuper-resolution directly in the latent space. Existing diffusion models\nstruggle with scaling beyond their training resolutions, often leading to\nstructural distortions or content repetition. Reference-based methods address\nthe issues by upsampling a low-resolution reference to guide higher-resolution\ngeneration. However, they face significant challenges: upsampling in latent\nspace often causes manifold deviation, which degrades output quality. On the\nother hand, upsampling in RGB space tends to produce overly smoothed outputs.\nTo overcome these limitations, LSRNA combines Latent space Super-Resolution\n(LSR) for manifold alignment and Region-wise Noise Addition (RNA) to enhance\nhigh-frequency details. Our extensive experiments demonstrate that integrating\nLSRNA outperforms state-of-the-art reference-based methods across various\nresolutions and metrics, while showing the critical role of latent space\nupsampling in preserving detail and sharpness. The code is available at\nhttps://github.com/3587jjh/LSRNA."}
{"id": "2503.18454", "pdf": "https://arxiv.org/pdf/2503.18454", "abs": "https://arxiv.org/abs/2503.18454", "authors": ["Yunhong Lu", "Qichao Wang", "Hengyuan Cao", "Xierui Wang", "Xiaoyin Xu", "Min Zhang"], "title": "InPO: Inversion Preference Optimization with Reparametrized DDIM for Efficient Diffusion Model Alignment", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted by CVPR2025", "summary": "Without using explicit reward, direct preference optimization (DPO) employs\npaired human preference data to fine-tune generative models, a method that has\ngarnered considerable attention in large language models (LLMs). However,\nexploration of aligning text-to-image (T2I) diffusion models with human\npreferences remains limited. In comparison to supervised fine-tuning, existing\nmethods that align diffusion model suffer from low training efficiency and\nsubpar generation quality due to the long Markov chain process and the\nintractability of the reverse process. To address these limitations, we\nintroduce DDIM-InPO, an efficient method for direct preference alignment of\ndiffusion models. Our approach conceptualizes diffusion model as a single-step\ngenerative model, allowing us to fine-tune the outputs of specific latent\nvariables selectively. In order to accomplish this objective, we first assign\nimplicit rewards to any latent variable directly via a reparameterization\ntechnique. Then we construct an Inversion technique to estimate appropriate\nlatent variables for preference optimization. This modification process enables\nthe diffusion model to only fine-tune the outputs of latent variables that have\na strong correlation with the preference dataset. Experimental results indicate\nthat our DDIM-InPO achieves state-of-the-art performance with just 400 steps of\nfine-tuning, surpassing all preference aligning baselines for T2I diffusion\nmodels in human preference evaluation tasks."}
{"id": "2503.18458", "pdf": "https://arxiv.org/pdf/2503.18458", "abs": "https://arxiv.org/abs/2503.18458", "authors": ["Luchao Wang", "Qian Ren", "Kaiming He", "Hua Wang", "Zhi Chen", "Yaohua Tang"], "title": "StableGS: A Floater-Free Framework for 3D Gaussian Splatting", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Recent years have witnessed remarkable success of 3D Gaussian Splatting\n(3DGS) in novel view synthesis, surpassing prior differentiable rendering\nmethods in both quality and efficiency. However, its training process suffers\nfrom coupled opacity-color optimization that frequently converges to local\nminima, producing floater artifacts that degrade visual fidelity. We present\nStableGS, a framework that eliminates floaters through cross-view depth\nconsistency constraints while introducing a dual-opacity GS model to decouple\ngeometry and material properties of translucent objects. To further enhance\nreconstruction quality in weakly-textured regions, we integrate DUSt3R depth\nestimation, significantly improving geometric stability. Our method\nfundamentally addresses 3DGS training instabilities, outperforming existing\nstate-of-the-art methods across open-source datasets."}
{"id": "2503.18459", "pdf": "https://arxiv.org/pdf/2503.18459", "abs": "https://arxiv.org/abs/2503.18459", "authors": ["Haoyu Chen", "Yunqiao Yang", "Nan Zhong", "Kede Ma"], "title": "Hiding Images in Diffusion Models by Editing Learned Score Functions", "categories": ["cs.CV"], "comment": null, "summary": "Hiding data using neural networks (i.e., neural steganography) has achieved\nremarkable success across both discriminative classifiers and generative\nadversarial networks. However, the potential of data hiding in diffusion models\nremains relatively unexplored. Current methods exhibit limitations in achieving\nhigh extraction accuracy, model fidelity, and hiding efficiency due primarily\nto the entanglement of the hiding and extraction processes with multiple\ndenoising diffusion steps. To address these, we describe a simple yet effective\napproach that embeds images at specific timesteps in the reverse diffusion\nprocess by editing the learned score functions. Additionally, we introduce a\nparameter-efficient fine-tuning method that combines gradient-based parameter\nselection with low-rank adaptation to enhance model fidelity and hiding\nefficiency. Comprehensive experiments demonstrate that our method extracts\nhigh-quality images at human-indistinguishable levels, replicates the original\nmodel behaviors at both sample and population levels, and embeds images orders\nof magnitude faster than prior methods. Besides, our method naturally supports\nmulti-recipient scenarios through independent extraction channels."}
{"id": "2503.18461", "pdf": "https://arxiv.org/pdf/2503.18461", "abs": "https://arxiv.org/abs/2503.18461", "authors": ["Lingting Zhu", "Jingrui Ye", "Runze Zhang", "Zeyu Hu", "Yingda Yin", "Lanjiong Li", "Jinnan Chen", "Shengju Qian", "Xin Wang", "Qingmin Liao", "Lequan Yu"], "title": "MuMA: 3D PBR Texturing via Multi-Channel Multi-View Generation and Agentic Post-Processing", "categories": ["cs.CV"], "comment": "17 pages, 14 figures", "summary": "Current methods for 3D generation still fall short in physically based\nrendering (PBR) texturing, primarily due to limited data and challenges in\nmodeling multi-channel materials. In this work, we propose MuMA, a method for\n3D PBR texturing through Multi-channel Multi-view generation and Agentic\npost-processing. Our approach features two key innovations: 1) We opt to model\nshaded and albedo appearance channels, where the shaded channels enables the\nintegration intrinsic decomposition modules for material properties. 2)\nLeveraging multimodal large language models, we emulate artists' techniques for\nmaterial assessment and selection. Experiments demonstrate that MuMA achieves\nsuperior results in visual quality and material fidelity compared to existing\nmethods."}
{"id": "2503.18463", "pdf": "https://arxiv.org/pdf/2503.18463", "abs": "https://arxiv.org/abs/2503.18463", "authors": ["Sixian Ding", "Xu Jiang", "Zhongjing Du", "Jiaqi Cui", "Xinyi Zeng", "Yan Wang"], "title": "SIT-FER: Integration of Semantic-, Instance-, Text-level Information for Semi-supervised Facial Expression Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Semi-supervised deep facial expression recognition (SS-DFER) has gained\nincreasingly research interest due to the difficulty in accessing sufficient\nlabeled data in practical settings. However, existing SS-DFER methods mainly\nutilize generated semantic-level pseudo-labels for supervised learning, the\nunreliability of which compromises their performance and undermines the\npractical utility. In this paper, we propose a novel SS-DFER framework that\nsimultaneously incorporates semantic, instance, and text-level information to\ngenerate high-quality pseudo-labels. Specifically, for the unlabeled data,\nconsidering the comprehensive knowledge within the textual descriptions and\ninstance representations, we respectively calculate the similarities between\nthe facial vision features and the corresponding textual and instance features\nto obtain the probabilities at the text- and instance-level. Combining with the\nsemantic-level probability, these three-level probabilities are elaborately\naggregated to gain the final pseudo-labels. Furthermore, to enhance the\nutilization of one-hot labels for the labeled data, we also incorporate text\nembeddings excavated from textual descriptions to co-supervise model training,\nenabling facial visual features to exhibit semantic correlations in the text\nspace. Experiments on three datasets demonstrate that our method significantly\noutperforms current state-of-the-art SS-DFER methods and even exceeds fully\nsupervised baselines. The code will be available at\nhttps://github.com/PatrickStarL/SIT-FER."}
{"id": "2503.18469", "pdf": "https://arxiv.org/pdf/2503.18469", "abs": "https://arxiv.org/abs/2503.18469", "authors": ["Hao Ni", "Lianli Gao", "Pengpeng Zeng", "Heng Tao Shen", "Jingkuan Song"], "title": "CFReID: Continual Few-shot Person Re-Identification", "categories": ["cs.CV"], "comment": "16 pages, 8 figures", "summary": "Real-world surveillance systems are dynamically evolving, requiring a person\nRe-identification model to continuously handle newly incoming data from various\ndomains. To cope with these dynamics, Lifelong ReID (LReID) has been proposed\nto learn and accumulate knowledge across multiple domains incrementally.\nHowever, LReID models need to be trained on large-scale labeled data for each\nunseen domain, which are typically inaccessible due to privacy and cost\nconcerns. In this paper, we propose a new paradigm called Continual Few-shot\nReID (CFReID), which requires models to be incrementally trained using few-shot\ndata and tested on all seen domains. Under few-shot conditions, CFREID faces\ntwo core challenges: 1) learning knowledge from few-shot data of unseen domain,\nand 2) avoiding catastrophic forgetting of seen domains. To tackle these two\nchallenges, we propose a Stable Distribution Alignment (SDA) framework from\nfeature distribution perspective. Specifically, our SDA is composed of two\nmodules, i.e., Meta Distribution Alignment (MDA) and Prototype-based Few-shot\nAdaptation (PFA). To support the study of CFReID, we establish an evaluation\nbenchmark for CFReID on five publicly available ReID datasets. Extensive\nexperiments demonstrate that our SDA can enhance the few-shot learning and\nanti-forgetting capabilities under few-shot conditions. Notably, our approach,\nusing only 5\\% of the data, i.e., 32 IDs, significantly outperforms LReID's\nstate-of-the-art performance, which requires 700 to 1,000 IDs."}
{"id": "2503.18470", "pdf": "https://arxiv.org/pdf/2503.18470", "abs": "https://arxiv.org/abs/2503.18470", "authors": ["Zhenyu Pan", "Han Liu"], "title": "MetaSpatial: Reinforcing 3D Spatial Reasoning in VLMs for the Metaverse", "categories": ["cs.CV", "cs.AI"], "comment": "Working Paper", "summary": "We present MetaSpatial, the first reinforcement learning (RL)-based framework\ndesigned to enhance 3D spatial reasoning in vision-language models (VLMs),\nenabling real-time 3D scene generation without the need for hard-coded\noptimizations. MetaSpatial addresses two core challenges: (i) the lack of\ninternalized 3D spatial reasoning in VLMs, which limits their ability to\ngenerate realistic layouts, and (ii) the inefficiency of traditional supervised\nfine-tuning (SFT) for layout generation tasks, as perfect ground truth\nannotations are unavailable. Our key innovation is a multi-turn RL-based\noptimization mechanism that integrates physics-aware constraints and rendered\nimage evaluations, ensuring generated 3D layouts are coherent, physically\nplausible, and aesthetically consistent. Methodologically, MetaSpatial\nintroduces an adaptive, iterative reasoning process, where the VLM refines\nspatial arrangements over multiple turns by analyzing rendered outputs,\nimproving scene coherence progressively. Empirical evaluations demonstrate that\nMetaSpatial significantly enhances the spatial consistency and formatting\nstability of various scale models. Post-training, object placements are more\nrealistic, aligned, and functionally coherent, validating the effectiveness of\nRL for 3D spatial reasoning in metaverse, AR/VR, digital twins, and game\ndevelopment applications. Our code, data, and training pipeline are publicly\navailable at https://github.com/PzySeere/MetaSpatial."}
{"id": "2503.18476", "pdf": "https://arxiv.org/pdf/2503.18476", "abs": "https://arxiv.org/abs/2503.18476", "authors": ["Wei Deng", "Mengshi Qi", "Huadong Ma"], "title": "Global-Local Tree Search for Language Guided 3D Scene Generation", "categories": ["cs.CV", "cs.CL"], "comment": "Accepted by CVPR 2025", "summary": "Large Vision-Language Models (VLMs), such as GPT-4, have achieved remarkable\nsuccess across various fields. However, there are few studies on 3D indoor\nscene generation with VLMs. This paper considers this task as a planning\nproblem subject to spatial and layout common sense constraints. To solve the\nproblem with a VLM, we propose a new global-local tree search algorithm.\nGlobally, the method places each object sequentially and explores multiple\nplacements during each placement process, where the problem space is\nrepresented as a tree. To reduce the depth of the tree, we decompose the scene\nstructure hierarchically, i.e. room level, region level, floor object level,\nand supported object level. The algorithm independently generates the floor\nobjects in different regions and supported objects placed on different floor\nobjects. Locally, we also decompose the sub-task, the placement of each object,\ninto multiple steps. The algorithm searches the tree of problem space. To\nleverage the VLM model to produce positions of objects, we discretize the\ntop-down view space as a dense grid and fill each cell with diverse emojis to\nmake to cells distinct. We prompt the VLM with the emoji grid and the VLM\nproduces a reasonable location for the object by describing the position with\nthe name of emojis. The quantitative and qualitative experimental results\nillustrate our approach generates more plausible 3D scenes than\nstate-of-the-art approaches. Our source code is available at\nhttps://github.com/dw-dengwei/TreeSearchGen ."}
{"id": "2503.18478", "pdf": "https://arxiv.org/pdf/2503.18478", "abs": "https://arxiv.org/abs/2503.18478", "authors": ["Xiangrui Liu", "Yan Shu", "Zheng Liu", "Ao Li", "Yang Tian", "Bo Zhao"], "title": "Video-XL-Pro: Reconstructive Token Compression for Extremely Long Video Understanding", "categories": ["cs.CV"], "comment": null, "summary": "Despite advanced token compression techniques, existing multimodal large\nlanguage models (MLLMs) still struggle with hour-long video understanding. In\nthis work, we propose Video-XL-Pro, an efficient method for extremely long\nvideo understanding, built upon Reconstructive Compression of Tokens (ReCoT), a\nlearnable module that leverages self-supervised learning to generate\ncomprehensive and compact video tokens. ReCoT introduces two key components:\n(i) Dynamic Token Synthesizer (DTS): DTS generates pseudo-video tokens from\nstatic image tokens by learning intra-token relationships, which are then used\nin masked video modeling. (ii) Semantic-Guided Masking (SGM): SGM adaptively\nmasks redundant visual tokens to facilitate more effective reconstructive\nlearning. To improve training efficiency in MLLMs fine-tuning, we introduce a\nvideo-specific dataset pruning strategy and design a simple yet Query-aware\nSelector that enables the model to precisely locate query-relevant video\ntokens. With only 3B parameters, Video-XL-Pro outperforms most 7B models\ntrained on larger datasets across multiple long video understanding benchmarks.\nMoreover, it can process over 8K frames on a single A100 GPU while maintaining\nhigh-quality performance."}
{"id": "2503.18483", "pdf": "https://arxiv.org/pdf/2503.18483", "abs": "https://arxiv.org/abs/2503.18483", "authors": ["Zequn Zeng", "Yudi Su", "Jianqiao Sun", "Tiansheng Wen", "Hao Zhang", "Zhengjue Wang", "Bo Chen", "Hongwei Liu", "Jiawei Ma"], "title": "Explaining Domain Shifts in Language: Concept erasing for Interpretable Image Classification", "categories": ["cs.CV"], "comment": "Accepted by CVPR2025", "summary": "Concept-based models can map black-box representations to\nhuman-understandable concepts, which makes the decision-making process more\ntransparent and then allows users to understand the reason behind predictions.\nHowever, domain-specific concepts often impact the final predictions, which\nsubsequently undermine the model generalization capabilities, and prevent the\nmodel from being used in high-stake applications. In this paper, we propose a\nnovel Language-guided Concept-Erasing (LanCE) framework. In particular, we\nempirically demonstrate that pre-trained vision-language models (VLMs) can\napproximate distinct visual domain shifts via domain descriptors while\nprompting large Language Models (LLMs) can easily simulate a wide range of\ndescriptors of unseen visual domains. Then, we introduce a novel plug-in domain\ndescriptor orthogonality (DDO) regularizer to mitigate the impact of these\ndomain-specific concepts on the final predictions. Notably, the DDO regularizer\nis agnostic to the design of concept-based models and we integrate it into\nseveral prevailing models. Through evaluation of domain generalization on four\nstandard benchmarks and three newly introduced benchmarks, we demonstrate that\nDDO can significantly improve the out-of-distribution (OOD) generalization over\nthe previous state-of-the-art concept-based models.Our code is available at\nhttps://github.com/joeyz0z/LanCE."}
{"id": "2503.18484", "pdf": "https://arxiv.org/pdf/2503.18484", "abs": "https://arxiv.org/abs/2503.18484", "authors": ["Junyuan Gao", "Jiahe Song", "Jiang Wu", "Runchuan Zhu", "Guanlin Shen", "Shasha Wang", "Xingjian Wei", "Haote Yang", "Songyang Zhang", "Weijia Li", "Bin Wang", "Dahua Lin", "Lijun Wu", "Conghui He"], "title": "PM4Bench: A Parallel Multilingual Multi-Modal Multi-task Benchmark for Large Vision Language Model", "categories": ["cs.CV", "cs.CL"], "comment": "Equal contribution: Junyuan Gao, Jiahe Song, Jiang Wu; Corresponding\n  author: Conghui He", "summary": "Existing multilingual benchmarks for Large Vision Language Models (LVLMs)\nsuffer from limitations including language-specific content biases, disjointed\nmultimodal input formats, and a lack of safety evaluation. To address these\ngaps, we propose PM4Bench, the first Parallel Multilingual Multi-Modal\nMulti-task Benchmark for LVLMs. PM4Bench features a parallel corpus design\nacross 10 languages, enabling fair and accurate cross-lingual comparisons. It\nincludes the vision setting where text and queries are embedded in images,\nrequiring LVLMs to simultaneously \"see\", \"read\", and \"think\", aligning with\nreal-world applications. Additionally, PM\\textsuperscript{4}Bench incorporates\nsafety evaluations, addressing critical oversight in existing multilingual\nbenchmarks. Using PM4Bench, we evaluate 11 mainstream LVLMs, revealing\nsignificant cross-linguistic performance disparities, particularly in vision\nsettings, and identifying OCR capability as a key determinant of these\nimbalances. We will release PM4Bench at https://github.com/opendatalab/PM4Bench ."}
{"id": "2503.18507", "pdf": "https://arxiv.org/pdf/2503.18507", "abs": "https://arxiv.org/abs/2503.18507", "authors": ["Luca Zanella", "Massimiliano Mancini", "Willi Menapace", "Sergey Tulyakov", "Yiming Wang", "Elisa Ricci"], "title": "Can Text-to-Video Generation help Video-Language Alignment?", "categories": ["cs.CV"], "comment": "CVPR 2025. Project website at https://lucazanella.github.io/synvita/", "summary": "Recent video-language alignment models are trained on sets of videos, each\nwith an associated positive caption and a negative caption generated by large\nlanguage models. A problem with this procedure is that negative captions may\nintroduce linguistic biases, i.e., concepts are seen only as negatives and\nnever associated with a video. While a solution would be to collect videos for\nthe negative captions, existing databases lack the fine-grained variations\nneeded to cover all possible negatives. In this work, we study whether\nsynthetic videos can help to overcome this issue. Our preliminary analysis with\nmultiple generators shows that, while promising on some tasks, synthetic videos\nharm the performance of the model on others. We hypothesize this issue is\nlinked to noise (semantic and visual) in the generated videos and develop a\nmethod, SynViTA, that accounts for those. SynViTA dynamically weights the\ncontribution of each synthetic video based on how similar its target caption is\nw.r.t. the real counterpart. Moreover, a semantic consistency loss makes the\nmodel focus on fine-grained differences across captions, rather than\ndifferences in video appearance. Experiments show that, on average, SynViTA\nimproves over existing methods on VideoCon test sets and SSv2-Temporal,\nSSv2-Events, and ATP-Hard benchmarks, being a first promising step for using\nsynthetic videos when learning video-language models."}
{"id": "2503.18512", "pdf": "https://arxiv.org/pdf/2503.18512", "abs": "https://arxiv.org/abs/2503.18512", "authors": ["Leheng Zhang", "Weiyi You", "Kexuan Shi", "Shuhang Gu"], "title": "Uncertainty-guided Perturbation for Image Super-Resolution Diffusion Model", "categories": ["cs.CV", "eess.IV"], "comment": "Accepted to CVPR 2025", "summary": "Diffusion-based image super-resolution methods have demonstrated significant\nadvantages over GAN-based approaches, particularly in terms of perceptual\nquality. Building upon a lengthy Markov chain, diffusion-based methods possess\nremarkable modeling capacity, enabling them to achieve outstanding performance\nin real-world scenarios. Unlike previous methods that focus on modifying the\nnoise schedule or sampling process to enhance performance, our approach\nemphasizes the improved utilization of LR information. We find that different\nregions of the LR image can be viewed as corresponding to different timesteps\nin a diffusion process, where flat areas are closer to the target HR\ndistribution but edge and texture regions are farther away. In these flat\nareas, applying a slight noise is more advantageous for the reconstruction. We\nassociate this characteristic with uncertainty and propose to apply uncertainty\nestimate to guide region-specific noise level control, a technique we refer to\nas Uncertainty-guided Noise Weighting. Pixels with lower uncertainty (i.e.,\nflat regions) receive reduced noise to preserve more LR information, therefore\nimproving performance. Furthermore, we modify the network architecture of\nprevious methods to develop our Uncertainty-guided Perturbation\nSuper-Resolution (UPSR) model. Extensive experimental results demonstrate that,\ndespite reduced model size and training overhead, the proposed UWSR method\noutperforms current state-of-the-art methods across various datasets, both\nquantitatively and qualitatively."}
{"id": "2503.18513", "pdf": "https://arxiv.org/pdf/2503.18513", "abs": "https://arxiv.org/abs/2503.18513", "authors": ["Xiaoyu Zhang", "Weihong Pan", "Chong Bao", "Xiyu Zhang", "Xiaojun Xiang", "Hanqing Jiang", "Hujun Bao"], "title": "LookCloser: Frequency-aware Radiance Field for Tiny-Detail Scene", "categories": ["cs.CV"], "comment": "8 pages, 6 figures. Accepted to CVPR 2025", "summary": "Humans perceive and comprehend their surroundings through information\nspanning multiple frequencies. In immersive scenes, people naturally scan their\nenvironment to grasp its overall structure while examining fine details of\nobjects that capture their attention. However, current NeRF frameworks\nprimarily focus on modeling either high-frequency local views or the broad\nstructure of scenes with low-frequency information, which is limited to\nbalancing both. We introduce FA-NeRF, a novel frequency-aware framework for\nview synthesis that simultaneously captures the overall scene structure and\nhigh-definition details within a single NeRF model. To achieve this, we propose\na 3D frequency quantification method that analyzes the scene's frequency\ndistribution, enabling frequency-aware rendering. Our framework incorporates a\nfrequency grid for fast convergence and querying, a frequency-aware feature\nre-weighting strategy to balance features across different frequency contents.\nExtensive experiments show that our method significantly outperforms existing\napproaches in modeling entire scenes while preserving fine details."}
{"id": "2503.18527", "pdf": "https://arxiv.org/pdf/2503.18527", "abs": "https://arxiv.org/abs/2503.18527", "authors": ["Soulaimene Turki", "Daniel Panangian", "Houda Chaabouni-Chouayakh", "Ksenia Bittner"], "title": "AIM2PC: Aerial Image to 3D Building Point Cloud Reconstruction", "categories": ["cs.CV"], "comment": "Accepted to ISPRS Geospatial Week 2025", "summary": "Three-dimensional urban reconstruction of buildings from single-view images\nhas attracted significant attention over the past two decades. However, recent\nmethods primarily focus on rooftops from aerial images, often overlooking\nessential geometrical details. Additionally, there is a notable lack of\ndatasets containing complete 3D point clouds for entire buildings, along with\nchallenges in obtaining reliable camera pose information for aerial images.\nThis paper addresses these challenges by presenting a novel methodology, AIM2PC\n, which utilizes our generated dataset that includes complete 3D point clouds\nand determined camera poses. Our approach takes features from a single aerial\nimage as input and concatenates them with essential additional conditions, such\nas binary masks and Sobel edge maps, to enable more edge-aware reconstruction.\nBy incorporating a point cloud diffusion model based on Centered denoising\nDiffusion Probabilistic Models (CDPM), we project these concatenated features\nonto the partially denoised point cloud using our camera poses at each\ndiffusion step. The proposed method is able to reconstruct the complete 3D\nbuilding point cloud, including wall information and demonstrates superior\nperformance compared to existing baseline techniques. To allow further\ncomparisons with our methodology the dataset has been made available at\nhttps://github.com/Soulaimene/AIM2PCDataset"}
{"id": "2503.18536", "pdf": "https://arxiv.org/pdf/2503.18536", "abs": "https://arxiv.org/abs/2503.18536", "authors": ["Erjian Guo", "Zhen Zhao", "Zicheng Wang", "Tong Chen", "Yunyi Liu", "Luping Zhou"], "title": "DiN: Diffusion Model for Robust Medical VQA with Semantic Noisy Labels", "categories": ["cs.CV"], "comment": null, "summary": "Medical Visual Question Answering (Med-VQA) systems benefit the\ninterpretation of medical images containing critical clinical information.\nHowever, the challenge of noisy labels and limited high-quality datasets\nremains underexplored. To address this, we establish the first benchmark for\nnoisy labels in Med-VQA by simulating human mislabeling with semantically\ndesigned noise types. More importantly, we introduce the DiN framework, which\nleverages a diffusion model to handle noisy labels in Med-VQA. Unlike the\ndominant classification-based VQA approaches that directly predict answers, our\nAnswer Diffuser (AD) module employs a coarse-to-fine process, refining answer\ncandidates with a diffusion model for improved accuracy. The Answer Condition\nGenerator (ACG) further enhances this process by generating task-specific\nconditional information via integrating answer embeddings with fused\nimage-question features. To address label noise, our Noisy Label\nRefinement(NLR) module introduces a robust loss function and dynamic answer\nadjustment to further boost the performance of the AD module."}
{"id": "2503.18540", "pdf": "https://arxiv.org/pdf/2503.18540", "abs": "https://arxiv.org/abs/2503.18540", "authors": ["Guneet Mutreja", "Philipp Schuegraf", "Ksenia Bittner"], "title": "HiRes-FusedMIM: A High-Resolution RGB-DSM Pre-trained Model for Building-Level Remote Sensing Applications", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent advances in self-supervised learning have led to the development of\nfoundation models that have significantly advanced performance in various\ncomputer vision tasks. However, despite their potential, these models often\noverlook the crucial role of high-resolution digital surface models (DSMs) in\nunderstanding urban environments, particularly for building-level analysis,\nwhich is essential for applications like digital twins. To address this gap, we\nintroduce HiRes-FusedMIM, a novel pre-trained model specifically designed to\nleverage the rich information contained within high-resolution RGB and DSM\ndata. HiRes-FusedMIM utilizes a dual-encoder simple masked image modeling\n(SimMIM) architecture with a multi-objective loss function that combines\nreconstruction and contrastive objectives, enabling it to learn powerful, joint\nrepresentations from both modalities. We conducted a comprehensive evaluation\nof HiRes-FusedMIM on a diverse set of downstream tasks, including\nclassification, semantic segmentation, and instance segmentation. Our results\ndemonstrate that: 1) HiRes-FusedMIM outperforms previous state-of-the-art\ngeospatial methods on several building-related datasets, including WHU Aerial\nand LoveDA, demonstrating its effectiveness in capturing and leveraging\nfine-grained building information; 2) Incorporating DSMs during pre-training\nconsistently improves performance compared to using RGB data alone,\nhighlighting the value of elevation information for building-level analysis; 3)\nThe dual-encoder architecture of HiRes-FusedMIM, with separate encoders for RGB\nand DSM data, significantly outperforms a single-encoder model on the Vaihingen\nsegmentation task, indicating the benefits of learning specialized\nrepresentations for each modality. To facilitate further research and\napplications in this direction, we will publicly release the trained model\nweights."}
{"id": "2503.18541", "pdf": "https://arxiv.org/pdf/2503.18541", "abs": "https://arxiv.org/abs/2503.18541", "authors": ["Kangli Wang", "Wei Gao"], "title": "UniPCGC: Towards Practical Point Cloud Geometry Compression via an Efficient Unified Approach", "categories": ["cs.CV", "cs.AI", "eess.IV"], "comment": "Accepted to AAAI 2025", "summary": "Learning-based point cloud compression methods have made significant progress\nin terms of performance. However, these methods still encounter challenges\nincluding high complexity, limited compression modes, and a lack of support for\nvariable rate, which restrict the practical application of these methods. In\norder to promote the development of practical point cloud compression, we\npropose an efficient unified point cloud geometry compression framework, dubbed\nas UniPCGC. It is a lightweight framework that supports lossy compression,\nlossless compression, variable rate and variable complexity. First, we\nintroduce the Uneven 8-Stage Lossless Coder (UELC) in the lossless mode, which\nallocates more computational complexity to groups with higher coding\ndifficulty, and merges groups with lower coding difficulty. Second, Variable\nRate and Complexity Module (VRCM) is achieved in the lossy mode through joint\nadoption of a rate modulation module and dynamic sparse convolution. Finally,\nthrough the dynamic combination of UELC and VRCM, we achieve lossy compression,\nlossless compression, variable rate and complexity within a unified framework.\nCompared to the previous state-of-the-art method, our method achieves a\ncompression ratio (CR) gain of 8.1\\% on lossless compression, and a Bjontegaard\nDelta Rate (BD-Rate) gain of 14.02\\% on lossy compression, while also\nsupporting variable rate and variable complexity."}
{"id": "2503.18544", "pdf": "https://arxiv.org/pdf/2503.18544", "abs": "https://arxiv.org/abs/2503.18544", "authors": ["Rafia Rahim", "Samuel Woerz", "Andreas Zell"], "title": "Distilling Stereo Networks for Performant and Efficient Leaner Networks", "categories": ["cs.CV"], "comment": "8 pages, 3 figures. Published in: 2023 International Joint Conference\n  on Neural Networks (IJCNN)", "summary": "Knowledge distillation has been quite popular in vision for tasks like\nclassification and segmentation however not much work has been done for\ndistilling state-of-the-art stereo matching methods despite their range of\napplications. One of the reasons for its lack of use in stereo matching\nnetworks is due to the inherent complexity of these networks, where a typical\nnetwork is composed of multiple two- and three-dimensional modules. In this\nwork, we systematically combine the insights from state-of-the-art stereo\nmethods with general knowledge-distillation techniques to develop a joint\nframework for stereo networks distillation with competitive results and faster\ninference. Moreover, we show, via a detailed empirical analysis, that\ndistilling knowledge from the stereo network requires careful design of the\ncomplete distillation pipeline starting from backbone to the right selection of\ndistillation points and corresponding loss functions. This results in the\nstudent networks that are not only leaner and faster but give excellent\nperformance . For instance, our student network while performing better than\nthe performance oriented methods like PSMNet [1], CFNet [2], and LEAStereo [3])\non benchmark SceneFlow dataset, is 8x, 5x, and 8x faster respectively.\nFurthermore, compared to speed oriented methods having inference time less than\n100ms, our student networks perform better than all the tested methods. In\naddition, our student network also shows better generalization capabilities\nwhen tested on unseen datasets like ETH3D and Middlebury."}
{"id": "2503.18548", "pdf": "https://arxiv.org/pdf/2503.18548", "abs": "https://arxiv.org/abs/2503.18548", "authors": ["Lubnaa Abdur Rahman", "Ioannis Papathanail", "Lorenzo Brigato", "Stavroula Mougiakakou"], "title": "Benchmarking Post-Hoc Unknown-Category Detection in Food Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Food recognition models often struggle to distinguish between seen and unseen\nsamples, frequently misclassifying samples from unseen categories by assigning\nthem an in-distribution (ID) label. This misclassification presents significant\nchallenges when deploying these models in real-world applications, particularly\nwithin automatic dietary assessment systems, where incorrect labels can lead to\ncascading errors throughout the system. Ideally, such models should prompt the\nuser when an unknown sample is encountered, allowing for corrective action.\nGiven no prior research exploring food recognition in real-world settings, in\nthis work we conduct an empirical analysis of various post-hoc\nout-of-distribution (OOD) detection methods for fine-grained food recognition.\nOur findings indicate that virtual logit matching (ViM) performed the best\noverall, likely due to its combination of logits and feature-space\nrepresentations. Additionally, our work reinforces prior notions in the OOD\ndomain, noting that models with higher ID accuracy performed better across the\nevaluated OOD detection methods. Furthermore, transformer-based architectures\nconsistently outperformed convolution-based models in detecting OOD samples\nacross various methods."}
{"id": "2503.18552", "pdf": "https://arxiv.org/pdf/2503.18552", "abs": "https://arxiv.org/abs/2503.18552", "authors": ["Qiang Qu", "Ming Li", "Xiaoming Chen", "Tongliang Liu"], "title": "EvAnimate: Event-conditioned Image-to-Video Generation for Human Animation", "categories": ["cs.CV", "cs.AI", "cs.MM"], "comment": null, "summary": "Conditional human animation transforms a static reference image into a\ndynamic sequence by applying motion cues such as poses. These motion cues are\ntypically derived from video data but are susceptible to limitations including\nlow temporal resolution, motion blur, overexposure, and inaccuracies under\nlow-light conditions. In contrast, event cameras provide data streams with\nexceptionally high temporal resolution, a wide dynamic range, and inherent\nresistance to motion blur and exposure issues. In this work, we propose\nEvAnimate, a framework that leverages event streams as motion cues to animate\nstatic human images. Our approach employs a specialized event representation\nthat transforms asynchronous event streams into 3-channel slices with\ncontrollable slicing rates and appropriate slice density, ensuring\ncompatibility with diffusion models. Subsequently, a dual-branch architecture\ngenerates high-quality videos by harnessing the inherent motion dynamics of the\nevent streams, thereby enhancing both video quality and temporal consistency.\nSpecialized data augmentation strategies further enhance cross-person\ngeneralization. Finally, we establish a new benchmarking, including simulated\nevent data for training and validation, and a real-world event dataset\ncapturing human actions under normal and extreme scenarios. The experiment\nresults demonstrate that EvAnimate achieves high temporal fidelity and robust\nperformance in scenarios where traditional video-derived cues fall short."}
{"id": "2503.18553", "pdf": "https://arxiv.org/pdf/2503.18553", "abs": "https://arxiv.org/abs/2503.18553", "authors": ["Zihao Chen", "Hsuanyu Wu", "Chi-Hsi Kung", "Yi-Ting Chen", "Yan-Tsung Peng"], "title": "ATARS: An Aerial Traffic Atomic Activity Recognition and Temporal Segmentation Dataset", "categories": ["cs.CV"], "comment": null, "summary": "Traffic Atomic Activity which describes traffic patterns for topological\nintersection dynamics is a crucial topic for the advancement of intelligent\ndriving systems. However, existing atomic activity datasets are collected from\nan egocentric view, which cannot support the scenarios where traffic activities\nin an entire intersection are required. Moreover, existing datasets only\nprovide video-level atomic activity annotations, which require exhausting\nefforts to manually trim the videos for recognition and limit their\napplications to untrimmed videos. To bridge this gap, we introduce the Aerial\nTraffic Atomic Activity Recognition and Segmentation (ATARS) dataset, the first\naerial dataset designed for multi-label atomic activity analysis. We offer\natomic activity labels for each frame, which accurately record the intervals\nfor traffic activities. Moreover, we propose a novel task, Multi-label Temporal\nAtomic Activity Recognition, enabling the study of accurate temporal\nlocalization for atomic activity and easing the burden of manual video trimming\nfor recognition. We conduct extensive experiments to evaluate existing\nstate-of-the-art models on both atomic activity recognition and temporal atomic\nactivity segmentation. The results highlight the unique challenges of our ATARS\ndataset, such as recognizing extremely small objects' activities. We further\nprovide comprehensive discussion analyzing these challenges and offer valuable\ninsights for future direction to improve recognizing atomic activity in aerial\nview. Our source code and dataset are available at\nhttps://github.com/magecliff96/ATARS/"}
{"id": "2503.18556", "pdf": "https://arxiv.org/pdf/2503.18556", "abs": "https://arxiv.org/abs/2503.18556", "authors": ["Bin Li", "Dehong Gao", "Yeyuan Wang", "Linbo Jin", "Shanqing Yu", "Xiaoyan Cai", "Libin Yang"], "title": "Instruction-Aligned Visual Attention for Mitigating Hallucinations in Large Vision-Language Models", "categories": ["cs.CV", "cs.CL"], "comment": "Accepted by ICME2025", "summary": "Despite the significant success of Large Vision-Language models(LVLMs), these\nmodels still suffer hallucinations when describing images, generating answers\nthat include non-existent objects. It is reported that these models tend to\nover-focus on certain irrelevant image tokens that do not contain critical\ninformation for answering the question and distort the output. To address this,\nwe propose an Instruction-Aligned Visual Attention(IAVA) approach, which\nidentifies irrelevant tokens by comparing changes in attention weights under\ntwo different instructions. By applying contrastive decoding, we dynamically\nadjust the logits generated from original image tokens and irrelevant image\ntokens, reducing the model's over-attention to irrelevant information. The\nexperimental results demonstrate that IAVA consistently outperforms existing\ndecoding techniques on benchmarks such as MME, POPE, and TextVQA in mitigating\nobject hallucinations. Our IAVA approach is available online at\nhttps://github.com/Lee-lab558/IAVA."}
{"id": "2503.18557", "pdf": "https://arxiv.org/pdf/2503.18557", "abs": "https://arxiv.org/abs/2503.18557", "authors": ["Rafia Rahim", "Samuel Woerz", "Andreas Zell"], "title": "LeanStereo: A Leaner Backbone based Stereo Network", "categories": ["cs.CV"], "comment": "8 pages, 4 figures", "summary": "Recently, end-to-end deep networks based stereo matching methods, mainly\nbecause of their performance, have gained popularity. However, this improvement\nin performance comes at the cost of increased computational and memory\nbandwidth requirements, thus necessitating specialized hardware (GPUs); even\nthen, these methods have large inference times compared to classical methods.\nThis limits their applicability in real-world applications. Although we desire\nhigh accuracy stereo methods albeit with reasonable inference time. To this\nend, we propose a fast end-to-end stereo matching method. Majority of this\nspeedup comes from integrating a leaner backbone. To recover the performance\nlost because of a leaner backbone, we propose to use learned attention weights\nbased cost volume combined with LogL1 loss for stereo matching. Using LogL1\nloss not only improves the overall performance of the proposed network but also\nleads to faster convergence. We do a detailed empirical evaluation of different\ndesign choices and show that our method requires 4x less operations and is also\nabout 9 to 14x faster compared to the state of the art methods like ACVNet [1],\nLEAStereo [2] and CFNet [3] while giving comparable performance."}
{"id": "2503.18559", "pdf": "https://arxiv.org/pdf/2503.18559", "abs": "https://arxiv.org/abs/2503.18559", "authors": ["Takashi Isobe", "He Cui", "Dong Zhou", "Mengmeng Ge", "Dong Li", "Emad Barsoum"], "title": "AMD-Hummingbird: Towards an Efficient Text-to-Video Model", "categories": ["cs.CV"], "comment": "Homepage:\n  https://www.amd.com/en/developer/resources/technical-articles/amd-hummingbird-0-9b-text-to-video-diffusion-model-with-4-step-inferencing.html|\n  GitHub: https://github.com/AMD-AIG-AIMA/AMD-Hummingbird-T2V", "summary": "Text-to-Video (T2V) generation has attracted significant attention for its\nability to synthesize realistic videos from textual descriptions. However,\nexisting models struggle to balance computational efficiency and high visual\nquality, particularly on resource-limited devices, e.g.,iGPUs and mobile\nphones. Most prior work prioritizes visual fidelity while overlooking the need\nfor smaller, more efficient models suitable for real-world deployment. To\naddress this challenge, we propose a lightweight T2V framework, termed\nHummingbird, which prunes existing models and enhances visual quality through\nvisual feedback learning. Our approach reduces the size of the U-Net from 1.4\nbillion to 0.7 billion parameters, significantly improving efficiency while\npreserving high-quality video generation. Additionally, we introduce a novel\ndata processing pipeline that leverages Large Language Models (LLMs) and Video\nQuality Assessment (VQA) models to enhance the quality of both text prompts and\nvideo data. To support user-driven training and style customization, we\npublicly release the full training code, including data processing and model\ntraining. Extensive experiments show that our method achieves a 31X speedup\ncompared to state-of-the-art models such as VideoCrafter2, while also attaining\nthe highest overall score on VBench. Moreover, our method supports the\ngeneration of videos with up to 26 frames, addressing the limitations of\nexisting U-Net-based methods in long video generation. Notably, the entire\ntraining process requires only four GPUs, yet delivers performance competitive\nwith existing leading methods. Hummingbird presents a practical and efficient\nsolution for T2V generation, combining high performance, scalability, and\nflexibility for real-world applications."}
{"id": "2503.18567", "pdf": "https://arxiv.org/pdf/2503.18567", "abs": "https://arxiv.org/abs/2503.18567", "authors": ["Biwen Meng", "Xi Long", "Wanrong Yang", "Ruochen Liu", "Yi Tian", "Yalin Zheng", "Jingxin Liu"], "title": "Advancing Cross-Organ Domain Generalization with Test-Time Style Transfer and Diversity Enhancement", "categories": ["cs.CV"], "comment": "2025 IEEE International Symposium on Biomedical Imaging (ISBI)", "summary": "Deep learning has made significant progress in addressing challenges in\nvarious fields including computational pathology (CPath). However, due to the\ncomplexity of the domain shift problem, the performance of existing models will\ndegrade, especially when it comes to multi-domain or cross-domain tasks. In\nthis paper, we propose a Test-time style transfer (T3s) that uses a\nbidirectional mapping mechanism to project the features of the source and\ntarget domains into a unified feature space, enhancing the generalization\nability of the model. To further increase the style expression space, we\nintroduce a Cross-domain style diversification module (CSDM) to ensure the\northogonality between style bases. In addition, data augmentation and low-rank\nadaptation techniques are used to improve feature alignment and sensitivity,\nenabling the model to adapt to multi-domain inputs effectively. Our method has\ndemonstrated effectiveness on three unseen datasets."}
{"id": "2503.18583", "pdf": "https://arxiv.org/pdf/2503.18583", "abs": "https://arxiv.org/abs/2503.18583", "authors": ["Alexander Holmberg", "Nils Mechtel", "Wei Ouyang"], "title": "Adapting Video Diffusion Models for Time-Lapse Microscopy", "categories": ["cs.CV"], "comment": null, "summary": "We present a domain adaptation of video diffusion models to generate highly\nrealistic time-lapse microscopy videos of cell division in HeLa cells. Although\nstate-of-the-art generative video models have advanced significantly for\nnatural videos, they remain underexplored in microscopy domains. To address\nthis gap, we fine-tune a pretrained video diffusion model on\nmicroscopy-specific sequences, exploring three conditioning strategies: (1)\ntext prompts derived from numeric phenotypic measurements (e.g., proliferation\nrates, migration speeds, cell-death frequencies), (2) direct numeric embeddings\nof phenotype scores, and (3) image-conditioned generation, where an initial\nmicroscopy frame is extended into a complete video sequence. Evaluation using\nbiologically meaningful morphological, proliferation, and migration metrics\ndemonstrates that fine-tuning substantially improves realism and accurately\ncaptures critical cellular behaviors such as mitosis and migration. Notably,\nthe fine-tuned model also generalizes beyond the training horizon, generating\ncoherent cell dynamics even in extended sequences. However, precisely\ncontrolling specific phenotypic characteristics remains challenging,\nhighlighting opportunities for future work to enhance conditioning methods. Our\nresults demonstrate the potential for domain-specific fine-tuning of generative\nvideo models to produce biologically plausible synthetic microscopy data,\nsupporting applications such as in-silico hypothesis testing and data\naugmentation."}
{"id": "2503.18589", "pdf": "https://arxiv.org/pdf/2503.18589", "abs": "https://arxiv.org/abs/2503.18589", "authors": ["Guillem Capellera", "Antonio Rubio", "Luis Ferraz", "Antonio Agudo"], "title": "Unified Uncertainty-Aware Diffusion for Multi-Agent Trajectory Modeling", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025 conference", "summary": "Multi-agent trajectory modeling has primarily focused on forecasting future\nstates, often overlooking broader tasks like trajectory completion, which are\ncrucial for real-world applications such as correcting tracking data. Existing\nmethods also generally predict agents' states without offering any state-wise\nmeasure of uncertainty. Moreover, popular multi-modal sampling methods lack any\nerror probability estimates for each generated scene under the same prior\nobservations, making it difficult to rank the predictions during inference\ntime. We introduce U2Diff, a \\textbf{unified} diffusion model designed to\nhandle trajectory completion while providing state-wise \\textbf{uncertainty}\nestimates jointly. This uncertainty estimation is achieved by augmenting the\nsimple denoising loss with the negative log-likelihood of the predicted noise\nand propagating latent space uncertainty to the real state space. Additionally,\nwe incorporate a Rank Neural Network in post-processing to enable \\textbf{error\nprobability} estimation for each generated mode, demonstrating a strong\ncorrelation with the error relative to ground truth. Our method outperforms the\nstate-of-the-art solutions in trajectory completion and forecasting across four\nchallenging sports datasets (NBA, Basketball-U, Football-U, Soccer-U),\nhighlighting the effectiveness of uncertainty and error probability estimation.\nVideo at https://youtu.be/ngw4D4eJToE"}
{"id": "2503.18623", "pdf": "https://arxiv.org/pdf/2503.18623", "abs": "https://arxiv.org/abs/2503.18623", "authors": ["Deepayan Das", "Davide Talon", "Yiming Wang", "Massimiliano Mancini", "Elisa Ricci"], "title": "Training-Free Personalization via Retrieval and Reasoning on Fingerprints", "categories": ["cs.CV"], "comment": null, "summary": "Vision Language Models (VLMs) have lead to major improvements in multimodal\nreasoning, yet they still struggle to understand user-specific concepts.\nExisting personalization methods address this limitation but heavily rely on\ntraining procedures, that can be either costly or unpleasant to individual\nusers. We depart from existing work, and for the first time explore the\ntraining-free setting in the context of personalization. We propose a novel\nmethod, Retrieval and Reasoning for Personalization (R2P), leveraging internal\nknowledge of VLMs. First, we leverage VLMs to extract the concept fingerprint,\ni.e., key attributes uniquely defining the concept within its semantic class.\nWhen a query arrives, the most similar fingerprints are retrieved and scored\nvia chain-of-thought-reasoning. To reduce the risk of hallucinations, the\nscores are validated through cross-modal verification at the attribute level:\nin case of a discrepancy between the scores, R2P refines the concept\nassociation via pairwise multimodal matching, where the retrieved fingerprints\nand their images are directly compared with the query. We validate R2P on two\npublicly available benchmarks and a newly introduced dataset, Personal Concepts\nwith Visual Ambiguity (PerVA), for concept identification highlighting\nchallenges in visual ambiguity. R2P consistently outperforms state-of-the-art\napproaches on various downstream tasks across all benchmarks. Code will be\navailable upon acceptance."}
{"id": "2503.18626", "pdf": "https://arxiv.org/pdf/2503.18626", "abs": "https://arxiv.org/abs/2503.18626", "authors": ["Junqiao Fan", "Yunjiao Zhou", "Min Chang Jordan Ren", "Jianfei Yang"], "title": "Generative Dataset Distillation using Min-Max Diffusion Model", "categories": ["cs.CV"], "comment": "The paper is accepted as the ECCV2024 workshop paper and achieved\n  second place in the generative track of The First Dataset Distillation\n  Challenge of ECCV2024, https://www.dd-challenge.com/#/", "summary": "In this paper, we address the problem of generative dataset distillation that\nutilizes generative models to synthesize images. The generator may produce any\nnumber of images under a preserved evaluation time. In this work, we leverage\nthe popular diffusion model as the generator to compute a surrogate dataset,\nboosted by a min-max loss to control the dataset's diversity and\nrepresentativeness during training. However, the diffusion model is\ntime-consuming when generating images, as it requires an iterative generation\nprocess. We observe a critical trade-off between the number of image samples\nand the image quality controlled by the diffusion steps and propose Diffusion\nStep Reduction to achieve optimal performance. This paper details our\ncomprehensive method and its performance. Our model achieved $2^{nd}$ place in\nthe generative track of \\href{https://www.dd-challenge.com/#/}{The First\nDataset Distillation Challenge of ECCV2024}, demonstrating its superior\nperformance."}
{"id": "2503.18627", "pdf": "https://arxiv.org/pdf/2503.18627", "abs": "https://arxiv.org/abs/2503.18627", "authors": ["Bing Cao", "Baoshuo Cai", "Changqing Zhang", "Qinghua Hu"], "title": "Dig2DIG: Dig into Diffusion Information Gains for Image Fusion", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Image fusion integrates complementary information from multi-source images to\ngenerate more informative results. Recently, the diffusion model, which\ndemonstrates unprecedented generative potential, has been explored in image\nfusion. However, these approaches typically incorporate predefined multimodal\nguidance into diffusion, failing to capture the dynamically changing\nsignificance of each modality, while lacking theoretical guarantees. To address\nthis issue, we reveal a significant spatio-temporal imbalance in image\ndenoising; specifically, the diffusion model produces dynamic information gains\nin different image regions with denoising steps. Based on this observation, we\nDig into the Diffusion Information Gains (Dig2DIG) and theoretically derive a\ndiffusion-based dynamic image fusion framework that provably reduces the upper\nbound of the generalization error. Accordingly, we introduce diffusion\ninformation gains (DIG) to quantify the information contribution of each\nmodality at different denoising steps, thereby providing dynamic guidance\nduring the fusion process. Extensive experiments on multiple fusion scenarios\nconfirm that our method outperforms existing diffusion-based approaches in\nterms of both fusion quality and inference efficiency."}
{"id": "2503.18629", "pdf": "https://arxiv.org/pdf/2503.18629", "abs": "https://arxiv.org/abs/2503.18629", "authors": ["Arne Grobrügge", "Niklas Kühl", "Gerhard Satzger", "Philipp Spitzer"], "title": "Towards Human-Understandable Multi-Dimensional Concept Discovery", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Concept-based eXplainable AI (C-XAI) aims to overcome the limitations of\ntraditional saliency maps by converting pixels into human-understandable\nconcepts that are consistent across an entire dataset. A crucial aspect of\nC-XAI is completeness, which measures how well a set of concepts explains a\nmodel's decisions. Among C-XAI methods, Multi-Dimensional Concept Discovery\n(MCD) effectively improves completeness by breaking down the CNN latent space\ninto distinct and interpretable concept subspaces. However, MCD's explanations\ncan be difficult for humans to understand, raising concerns about their\npractical utility. To address this, we propose Human-Understandable\nMulti-dimensional Concept Discovery (HU-MCD). HU-MCD uses the Segment Anything\nModel for concept identification and implements a CNN-specific input masking\ntechnique to reduce noise introduced by traditional masking methods. These\nchanges to MCD, paired with the completeness relation, enable HU-MCD to enhance\nconcept understandability while maintaining explanation faithfulness. Our\nexperiments, including human subject studies, show that HU-MCD provides more\nprecise and reliable explanations than existing C-XAI methods. The code is\navailable at https://github.com/grobruegge/hu-mcd."}
{"id": "2503.18631", "pdf": "https://arxiv.org/pdf/2503.18631", "abs": "https://arxiv.org/abs/2503.18631", "authors": ["Kunyang Li", "Ming Hou"], "title": "Robust Lane Detection with Wavelet-Enhanced Context Modeling and Adaptive Sampling", "categories": ["cs.CV"], "comment": null, "summary": "Lane detection is critical for autonomous driving and ad-vanced driver\nassistance systems (ADAS). While recent methods like CLRNet achieve strong\nperformance, they struggle under adverse con-ditions such as extreme weather,\nillumination changes, occlusions, and complex curves. We propose a\nWavelet-Enhanced Feature Pyramid Net-work (WE-FPN) to address these challenges.\nA wavelet-based non-local block is integrated before the feature pyramid to\nimprove global context modeling, especially for occluded and curved lanes.\nAdditionally, we de-sign an adaptive preprocessing module to enhance lane\nvisibility under poor lighting. An attention-guided sampling strategy further\nreffnes spa-tial features, boosting accuracy on distant and curved lanes.\nExperiments on CULane and TuSimple demonstrate that our approach signiffcantly\noutperforms baselines in challenging scenarios, achieving better robust-ness\nand accuracy in real-world driving conditions."}
{"id": "2503.18635", "pdf": "https://arxiv.org/pdf/2503.18635", "abs": "https://arxiv.org/abs/2503.18635", "authors": ["Hui Li", "Congcong Bian", "Zeyang Zhang", "Xiaoning Song", "Xi Li", "Xiao-Jun Wu"], "title": "OCCO: LVM-guided Infrared and Visible Image Fusion Framework based on Object-aware and Contextual COntrastive Learning", "categories": ["cs.CV"], "comment": null, "summary": "Image fusion is a crucial technique in the field of computer vision, and its\ngoal is to generate high-quality fused images and improve the performance of\ndownstream tasks. However, existing fusion methods struggle to balance these\ntwo factors. Achieving high quality in fused images may result in lower\nperformance in downstream visual tasks, and vice versa. To address this\ndrawback, a novel LVM (large vision model)-guided fusion framework with\nObject-aware and Contextual COntrastive learning is proposed, termed as OCCO.\nThe pre-trained LVM is utilized to provide semantic guidance, allowing the\nnetwork to focus solely on fusion tasks while emphasizing learning salient\nsemantic features in form of contrastive learning. Additionally, a novel\nfeature interaction fusion network is also designed to resolve information\nconflicts in fusion images caused by modality differences. By learning the\ndistinction between positive samples and negative samples in the latent feature\nspace (contextual space), the integrity of target information in fused image is\nimproved, thereby benefiting downstream performance. Finally, compared with\neight state-of-the-art methods on four datasets, the effectiveness of the\nproposed method is validated, and exceptional performance is also demonstrated\non downstream visual task."}
{"id": "2503.18637", "pdf": "https://arxiv.org/pdf/2503.18637", "abs": "https://arxiv.org/abs/2503.18637", "authors": ["Nina Shvetsova", "Arsha Nagrani", "Bernt Schiele", "Hilde Kuehne", "Christian Rupprecht"], "title": "Unbiasing through Textual Descriptions: Mitigating Representation Bias in Video Benchmarks", "categories": ["cs.CV"], "comment": "To be published at CVPR 2025, project webpage\n  https://utd-project.github.io/", "summary": "We propose a new \"Unbiased through Textual Description (UTD)\" video benchmark\nbased on unbiased subsets of existing video classification and retrieval\ndatasets to enable a more robust assessment of video understanding\ncapabilities. Namely, we tackle the problem that current video benchmarks may\nsuffer from different representation biases, e.g., object bias or single-frame\nbias, where mere recognition of objects or utilization of only a single frame\nis sufficient for correct prediction. We leverage VLMs and LLMs to analyze and\ndebias benchmarks from such representation biases. Specifically, we generate\nframe-wise textual descriptions of videos, filter them for specific information\n(e.g. only objects) and leverage them to examine representation biases across\nthree dimensions: 1) concept bias - determining if a specific concept (e.g.,\nobjects) alone suffice for prediction; 2) temporal bias - assessing if temporal\ninformation contributes to prediction; and 3) common sense vs. dataset bias -\nevaluating whether zero-shot reasoning or dataset correlations contribute to\nprediction. We conduct a systematic analysis of 12 popular video classification\nand retrieval datasets and create new object-debiased test splits for these\ndatasets. Moreover, we benchmark 30 state-of-the-art video models on original\nand debiased splits and analyze biases in the models. To facilitate the future\ndevelopment of more robust video understanding benchmarks and models, we\nrelease: \"UTD-descriptions\", a dataset with our rich structured descriptions\nfor each dataset, and \"UTD-splits\", a dataset of object-debiased test splits."}
{"id": "2503.18640", "pdf": "https://arxiv.org/pdf/2503.18640", "abs": "https://arxiv.org/abs/2503.18640", "authors": ["Haoran Wang", "Jingwei Huang", "Lu Yang", "Tianchen Deng", "Gaojing Zhang", "Mingrui Li"], "title": "LLGS: Unsupervised Gaussian Splatting for Image Enhancement and Reconstruction in Pure Dark Environment", "categories": ["cs.CV"], "comment": null, "summary": "3D Gaussian Splatting has shown remarkable capabilities in novel view\nrendering tasks and exhibits significant potential for multi-view\noptimization.However, the original 3D Gaussian Splatting lacks color\nrepresentation for inputs in low-light environments. Simply using enhanced\nimages as inputs would lead to issues with multi-view consistency, and current\nsingle-view enhancement systems rely on pre-trained data, lacking scene\ngeneralization. These problems limit the application of 3D Gaussian Splatting\nin low-light conditions in the field of robotics, including high-fidelity\nmodeling and feature matching. To address these challenges, we propose an\nunsupervised multi-view stereoscopic system based on Gaussian Splatting, called\nLow-Light Gaussian Splatting (LLGS). This system aims to enhance images in\nlow-light environments while reconstructing the scene. Our method introduces a\ndecomposable Gaussian representation called M-Color, which separately\ncharacterizes color information for targeted enhancement. Furthermore, we\npropose an unsupervised optimization method with zero-knowledge priors, using\ndirection-based enhancement to ensure multi-view consistency. Experiments\nconducted on real-world datasets demonstrate that our system outperforms\nstate-of-the-art methods in both low-light enhancement and 3D Gaussian\nSplatting."}
{"id": "2503.18652", "pdf": "https://arxiv.org/pdf/2503.18652", "abs": "https://arxiv.org/abs/2503.18652", "authors": ["Yaoyao Yun", "Jianwen Xu"], "title": "Robust face recognition based on the wing loss and the $\\ell_1$ regularization", "categories": ["cs.CV"], "comment": "10 pages, 3 figures", "summary": "In recent years, sparse sampling techniques based on regression analysis have\nwitnessed extensive applications in face recognition research. Presently,\nnumerous sparse sampling models based on regression analysis have been explored\nby various researchers. Nevertheless, the recognition rates of the majority of\nthese models would be significantly decreased when confronted with highly\noccluded and highly damaged face images. In this paper, a new wing-constrained\nsparse coding model(WCSC) and its weighted version(WWCSC) are introduced, so as\nto deal with the face recognition problem in complex circumstances, where the\nalternating direction method of multipliers (ADMM) algorithm is employed to\nsolve the corresponding minimization problems. In addition, performances of the\nproposed method are examined based on the four well-known facial databases,\nnamely the ORL facial database, the Yale facial database, the AR facial\ndatabase and the FERET facial database. Also, compared to the other methods in\nthe literatures, the WWCSC has a very high recognition rate even in complex\nsituations where face images have high occlusion or high damage, which\nillustrates the robustness of the WWCSC method in facial recognition."}
{"id": "2503.18658", "pdf": "https://arxiv.org/pdf/2503.18658", "abs": "https://arxiv.org/abs/2503.18658", "authors": ["Christopher Ummerle", "Antonio Giganti", "Sara Mandelli", "Paolo Bestagini", "Stefano Tubaro"], "title": "Leveraging Land Cover Priors for Isoprene Emission Super-Resolution", "categories": ["cs.CV"], "comment": "17 pages, 16 figures, 4 tables", "summary": "Remote sensing plays a crucial role in monitoring Earth's ecosystems, yet\nsatellite-derived data often suffer from limited spatial resolution,\nrestricting their applicability in atmospheric modeling and climate research.\nIn this work, we propose a deep learning-based Super-Resolution (SR) framework\nthat leverages land cover information to enhance the spatial accuracy of\nBiogenic Volatile Organic Compounds (BVOCs) emissions, with a particular focus\non isoprene. Our approach integrates land cover priors as emission drivers,\ncapturing spatial patterns more effectively than traditional methods. We\nevaluate the model's performance across various climate conditions and analyze\nstatistical correlations between isoprene emissions and key environmental\ninformation such as cropland and tree cover data. Additionally, we assess the\ngeneralization capabilities of our SR model by applying it to unseen climate\nzones and geographical regions. Experimental results demonstrate that\nincorporating land cover data significantly improves emission SR accuracy,\nparticularly in heterogeneous landscapes. This study contributes to atmospheric\nchemistry and climate modeling by providing a cost-effective, data-driven\napproach to refining BVOC emission maps. The proposed method enhances the\nusability of satellite-based emissions data, supporting applications in air\nquality forecasting, climate impact assessments, and environmental studies."}
{"id": "2503.18665", "pdf": "https://arxiv.org/pdf/2503.18665", "abs": "https://arxiv.org/abs/2503.18665", "authors": ["Bingchen Miao", "Yang Wu", "Minghe Gao", "Qifan Yu", "Wendong Bu", "Wenqiao Zhang", "Yunfei Li", "Siliang Tang", "Tat-Seng Chua", "Juncheng Li"], "title": "Boosting Virtual Agent Learning and Reasoning: A Step-wise, Multi-dimensional, and Generalist Reward Model with Benchmark", "categories": ["cs.CV"], "comment": null, "summary": "The development of Generalist Virtual Agents (GVAs) powered by Multimodal\nLarge Language Models (MLLMs) has shown significant promise in autonomous task\nexecution. However, current training paradigms face critical limitations,\nincluding reliance on outcome supervision and labor-intensive human\nannotations. To address these challenges, we propose Similar, a Step-wise\nMulti-dimensional Generalist Reward Model, which offers fine-grained signals\nfor agent training and can choose better action for inference-time scaling.\nSpecifically, we begin by systematically defining five dimensions for\nevaluating agent actions. Building on this framework, we design an MCTS-P\nalgorithm to automatically collect and annotate step-wise, five-dimensional\nagent execution data. Using this data, we train Similar with the Triple-M\nstrategy. Furthermore, we introduce the first benchmark in the virtual agent\ndomain for step-wise, multi-dimensional reward model training and evaluation,\nnamed SRM. This benchmark consists of two components: SRMTrain, which serves as\nthe training set for Similar, and SRMEval, a manually selected test set for\nevaluating the reward model. Experimental results demonstrate that Similar,\nthrough its step-wise, multi-dimensional assessment and synergistic gain,\nprovides GVAs with effective intermediate signals during both training and\ninference-time scaling. The code is available at\nhttps://github.com/Galery23/Similar-v1."}
{"id": "2503.18671", "pdf": "https://arxiv.org/pdf/2503.18671", "abs": "https://arxiv.org/abs/2503.18671", "authors": ["Yihan Chen", "Wenfei Yang", "Huan Ren", "Shifeng Zhang", "Tianzhu Zhang", "Feng Wu"], "title": "Structure-Aware Correspondence Learning for Relative Pose Estimation", "categories": ["cs.CV"], "comment": "CVPR2025", "summary": "Relative pose estimation provides a promising way for achieving\nobject-agnostic pose estimation. Despite the success of existing 3D\ncorrespondence-based methods, the reliance on explicit feature matching suffers\nfrom small overlaps in visible regions and unreliable feature estimation for\ninvisible regions. Inspired by humans' ability to assemble two object parts\nthat have small or no overlapping regions by considering object structure, we\npropose a novel Structure-Aware Correspondence Learning method for Relative\nPose Estimation, which consists of two key modules. First, a structure-aware\nkeypoint extraction module is designed to locate a set of kepoints that can\nrepresent the structure of objects with different shapes and appearance, under\nthe guidance of a keypoint based image reconstruction loss. Second, a\nstructure-aware correspondence estimation module is designed to model the\nintra-image and inter-image relationships between keypoints to extract\nstructure-aware features for correspondence estimation. By jointly leveraging\nthese two modules, the proposed method can naturally estimate 3D-3D\ncorrespondences for unseen objects without explicit feature matching for\nprecise relative pose estimation. Experimental results on the CO3D, Objaverse\nand LineMOD datasets demonstrate that the proposed method significantly\noutperforms prior methods, i.e., with 5.7{\\deg}reduction in mean angular error\non the CO3D dataset."}
{"id": "2503.18672", "pdf": "https://arxiv.org/pdf/2503.18672", "abs": "https://arxiv.org/abs/2503.18672", "authors": ["Juncen Guo", "Xiaoguang Zhu", "Lianlong Sun", "Liangyu Teng", "Di Li", "Yang Liu", "Liang Song"], "title": "Feature Calibration enhanced Parameter Synthesis for CLIP-based Class-incremental Learning", "categories": ["cs.CV"], "comment": null, "summary": "Class-incremental Learning (CIL) enables models to continuously learn new\nclass knowledge while memorizing previous classes, facilitating their\nadaptation and evolution in dynamic environments. Traditional CIL methods are\nmainly based on visual features, which limits their ability to handle complex\nscenarios. In contrast, Vision-Language Models (VLMs) show promising potential\nto promote CIL by integrating pretrained knowledge with textual features.\nHowever, previous methods make it difficult to overcome catastrophic forgetting\nwhile preserving the generalization capabilities of VLMs. To tackle these\nchallenges, we propose Feature Calibration enhanced Parameter Synthesis (FCPS)\nin this paper. Specifically, our FCPS employs a specific parameter adjustment\nmechanism to iteratively refine the proportion of original visual features\nparticipating in the final class determination, ensuring the model's\nfoundational generalization capabilities. Meanwhile, parameter integration\nacross different tasks achieves a balance between learning new class knowledge\nand retaining old knowledge. Experimental results on popular benchmarks (e.g.,\nCIFAR100 and ImageNet100) validate the superiority of the proposed method."}
{"id": "2503.18673", "pdf": "https://arxiv.org/pdf/2503.18673", "abs": "https://arxiv.org/abs/2503.18673", "authors": ["Taeyeop Lee", "Bowen Wen", "Minjun Kang", "Gyuree Kang", "In So Kweon", "Kuk-Jin Yoon"], "title": "Any6D: Model-free 6D Pose Estimation of Novel Objects", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": "CVPR 2025, Project Page: https://taeyeop.com/any6d", "summary": "We introduce Any6D, a model-free framework for 6D object pose estimation that\nrequires only a single RGB-D anchor image to estimate both the 6D pose and size\nof unknown objects in novel scenes. Unlike existing methods that rely on\ntextured 3D models or multiple viewpoints, Any6D leverages a joint object\nalignment process to enhance 2D-3D alignment and metric scale estimation for\nimproved pose accuracy. Our approach integrates a render-and-compare strategy\nto generate and refine pose hypotheses, enabling robust performance in\nscenarios with occlusions, non-overlapping views, diverse lighting conditions,\nand large cross-environment variations. We evaluate our method on five\nchallenging datasets: REAL275, Toyota-Light, HO3D, YCBINEOAT, and LM-O,\ndemonstrating its effectiveness in significantly outperforming state-of-the-art\nmethods for novel object pose estimation. Project page:\nhttps://taeyeop.com/any6d"}
{"id": "2503.18674", "pdf": "https://arxiv.org/pdf/2503.18674", "abs": "https://arxiv.org/abs/2503.18674", "authors": ["Edoardo De Matteis", "Matteo Migliarini", "Alessio Sampieri", "Indro Spinelli", "Fabio Galasso"], "title": "Human Motion Unlearning", "categories": ["cs.CV"], "comment": null, "summary": "We introduce the task of human motion unlearning to prevent the synthesis of\ntoxic animations while preserving the general text-to-motion generative\nperformance. Unlearning toxic motions is challenging as those can be generated\nfrom explicit text prompts and from implicit toxic combinations of safe motions\n(e.g., ``kicking\" is ``loading and swinging a leg\"). We propose the first\nmotion unlearning benchmark by filtering toxic motions from the large and\nrecent text-to-motion datasets of HumanML3D and Motion-X. We propose baselines,\nby adapting state-of-the-art image unlearning techniques to process\nspatio-temporal signals. Finally, we propose a novel motion unlearning model\nbased on Latent Code Replacement, which we dub LCR. LCR is training-free and\nsuitable to the discrete latent spaces of state-of-the-art text-to-motion\ndiffusion models. LCR is simple and consistently outperforms baselines\nqualitatively and quantitatively. Project page:\n\\href{https://www.pinlab.org/hmu}{https://www.pinlab.org/hmu}."}
{"id": "2503.18678", "pdf": "https://arxiv.org/pdf/2503.18678", "abs": "https://arxiv.org/abs/2503.18678", "authors": ["Tianyi Wang", "Harry Cheng", "Xiao Zhang", "Yinglong Wang"], "title": "NullSwap: Proactive Identity Cloaking Against Deepfake Face Swapping", "categories": ["cs.CV"], "comment": null, "summary": "Suffering from performance bottlenecks in passively detecting high-quality\nDeepfake images due to the advancement of generative models, proactive\nperturbations offer a promising approach to disabling Deepfake manipulations by\ninserting signals into benign images. However, existing proactive perturbation\napproaches remain unsatisfactory in several aspects: 1) visual degradation due\nto direct element-wise addition; 2) limited effectiveness against face swapping\nmanipulation; 3) unavoidable reliance on white- and grey-box settings to\ninvolve generative models during training. In this study, we analyze the\nessence of Deepfake face swapping and argue the necessity of protecting source\nidentities rather than target images, and we propose NullSwap, a novel\nproactive defense approach that cloaks source image identities and nullifies\nface swapping under a pure black-box scenario. We design an Identity Extraction\nmodule to obtain facial identity features from the source image, while a\nPerturbation Block is then devised to generate identity-guided perturbations\naccordingly. Meanwhile, a Feature Block extracts shallow-level image features,\nwhich are then fused with the perturbation in the Cloaking Block for image\nreconstruction. Furthermore, to ensure adaptability across different identity\nextractors in face swapping algorithms, we propose Dynamic Loss Weighting to\nadaptively balance identity losses. Experiments demonstrate the outstanding\nability of our approach to fool various identity recognition models,\noutperforming state-of-the-art proactive perturbations in preventing face\nswapping models from generating images with correct source identities."}
{"id": "2503.18682", "pdf": "https://arxiv.org/pdf/2503.18682", "abs": "https://arxiv.org/abs/2503.18682", "authors": ["Samuel Rota Bulò", "Nemanja Bartolovic", "Lorenzo Porzi", "Peter Kontschieder"], "title": "Hardware-Rasterized Ray-Based Gaussian Splatting", "categories": ["cs.CV", "cs.GR"], "comment": null, "summary": "We present a novel, hardware rasterized rendering approach for ray-based 3D\nGaussian Splatting (RayGS), obtaining both fast and high-quality results for\nnovel view synthesis. Our work contains a mathematically rigorous and\ngeometrically intuitive derivation about how to efficiently estimate all\nrelevant quantities for rendering RayGS models, structured with respect to\nstandard hardware rasterization shaders. Our solution is the first enabling\nrendering RayGS models at sufficiently high frame rates to support\nquality-sensitive applications like Virtual and Mixed Reality. Our second\ncontribution enables alias-free rendering for RayGS, by addressing MIP-related\nissues arising when rendering diverging scales during training and testing. We\ndemonstrate significant performance gains, across different benchmark scenes,\nwhile retaining state-of-the-art appearance quality of RayGS."}
{"id": "2503.18695", "pdf": "https://arxiv.org/pdf/2503.18695", "abs": "https://arxiv.org/abs/2503.18695", "authors": ["Luyao Tang", "Yuxuan Yuan", "Chaoqi Chen", "Zeyu Zhang", "Yue Huang", "Kun Zhang"], "title": "OCRT: Boosting Foundation Models in the Open World with Object-Concept-Relation Triad", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted by CVPR 2025", "summary": "Although foundation models (FMs) claim to be powerful, their generalization\nability significantly decreases when faced with distribution shifts, weak\nsupervision, or malicious attacks in the open world. On the other hand, most\ndomain generalization or adversarial fine-tuning methods are task-related or\nmodel-specific, ignoring the universality in practical applications and the\ntransferability between FMs. This paper delves into the problem of generalizing\nFMs to the out-of-domain data. We propose a novel framework, the\nObject-Concept-Relation Triad (OCRT), that enables FMs to extract sparse,\nhigh-level concepts and intricate relational structures from raw visual inputs.\nThe key idea is to bind objects in visual scenes and a set of object-centric\nrepresentations through unsupervised decoupling and iterative refinement. To be\nspecific, we project the object-centric representations onto a semantic concept\nspace that the model can readily interpret and estimate their importance to\nfilter out irrelevant elements. Then, a concept-based graph, which has a\nflexible degree, is constructed to incorporate the set of concepts and their\ncorresponding importance, enabling the extraction of high-order factors from\ninformative concepts and facilitating relational reasoning among these\nconcepts. Extensive experiments demonstrate that OCRT can substantially boost\nthe generalizability and robustness of SAM and CLIP across multiple downstream\ntasks."}
{"id": "2503.18703", "pdf": "https://arxiv.org/pdf/2503.18703", "abs": "https://arxiv.org/abs/2503.18703", "authors": ["Guanglu Dong", "Tianheng Zheng", "Yuanzhouhan Cao", "Linbo Qing", "Chao Ren"], "title": "Channel Consistency Prior and Self-Reconstruction Strategy Based Unsupervised Image Deraining", "categories": ["cs.CV"], "comment": "Accepted to CVPR2025", "summary": "Recently, deep image deraining models based on paired datasets have made a\nseries of remarkable progress. However, they cannot be well applied in\nreal-world applications due to the difficulty of obtaining real paired datasets\nand the poor generalization performance. In this paper, we propose a novel\nChannel Consistency Prior and Self-Reconstruction Strategy Based Unsupervised\nImage Deraining framework, CSUD, to tackle the aforementioned challenges.\nDuring training with unpaired data, CSUD is capable of generating high-quality\npseudo clean and rainy image pairs which are used to enhance the performance of\nderaining network. Specifically, to preserve more image background details\nwhile transferring rain streaks from rainy images to the unpaired clean images,\nwe propose a novel Channel Consistency Loss (CCLoss) by introducing the Channel\nConsistency Prior (CCP) of rain streaks into training process, thereby ensuring\nthat the generated pseudo rainy images closely resemble the real ones.\nFurthermore, we propose a novel Self-Reconstruction (SR) strategy to alleviate\nthe redundant information transfer problem of the generator, further improving\nthe deraining performance and the generalization capability of our method.\nExtensive experiments on multiple synthetic and real-world datasets demonstrate\nthat the deraining performance of CSUD surpasses other state-of-the-art\nunsupervised methods and CSUD exhibits superior generalization capability."}
{"id": "2503.18705", "pdf": "https://arxiv.org/pdf/2503.18705", "abs": "https://arxiv.org/abs/2503.18705", "authors": ["Inseung Hwang", "Kiseok Choi", "Hyunho Ha", "Min H. Kim"], "title": "Benchmarking Burst Super-Resolution for Polarization Images: Noise Dataset and Analysis", "categories": ["cs.CV", "cs.GR"], "comment": null, "summary": "Snapshot polarization imaging calculates polarization states from linearly\npolarized subimages. To achieve this, a polarization camera employs a double\nBayer-patterned sensor to capture both color and polarization. It demonstrates\nlow light efficiency and low spatial resolution, resulting in increased noise\nand compromised polarization measurements. Although burst super-resolution\neffectively reduces noise and enhances spatial resolution, applying it to\npolarization imaging poses challenges due to the lack of tailored datasets and\nreliable ground truth noise statistics. To address these issues, we introduce\nPolarNS and PolarBurstSR, two innovative datasets developed specifically for\npolarization imaging. PolarNS provides characterization of polarization noise\nstatistics, facilitating thorough analysis, while PolarBurstSR functions as a\nbenchmark for burst super-resolution in polarization images. These datasets,\ncollected under various real-world conditions, enable comprehensive evaluation.\nAdditionally, we present a model for analyzing polarization noise to quantify\nnoise propagation, tested on a large dataset captured in a darkroom\nenvironment. As part of our application, we compare the latest burst\nsuper-resolution models, highlighting the advantages of training tailored to\npolarization compared to RGB-based methods. This work establishes a benchmark\nfor polarization burst super-resolution and offers critical insights into noise\npropagation, thereby enhancing polarization image reconstruction."}
{"id": "2503.18709", "pdf": "https://arxiv.org/pdf/2503.18709", "abs": "https://arxiv.org/abs/2503.18709", "authors": ["Boqi Chen", "Cédric Vincent-Cuaz", "Lydia A. Schoenpflug", "Manuel Madeira", "Lisa Fournier", "Vaishnavi Subramanian", "Sonali Andani", "Samuel Ruiperez-Campillo", "Julia E. Vogt", "Raphaëlle Luisier", "Dorina Thanou", "Viktor H. Koelzer", "Pascal Frossard", "Gabriele Campanella", "Gunnar Rätsch"], "title": "Revisiting Automatic Data Curation for Vision Foundation Models in Digital Pathology", "categories": ["cs.CV"], "comment": "MICCAI 2025", "summary": "Vision foundation models (FMs) are accelerating the development of digital\npathology algorithms and transforming biomedical research. These models learn,\nin a self-supervised manner, to represent histological features in highly\nheterogeneous tiles extracted from whole-slide images (WSIs) of real-world\npatient samples. The performance of these FMs is significantly influenced by\nthe size, diversity, and balance of the pre-training data. However, data\nselection has been primarily guided by expert knowledge at the WSI level,\nfocusing on factors such as disease classification and tissue types, while\nlargely overlooking the granular details available at the tile level. In this\npaper, we investigate the potential of unsupervised automatic data curation at\nthe tile-level, taking into account 350 million tiles. Specifically, we apply\nhierarchical clustering trees to pre-extracted tile embeddings, allowing us to\nsample balanced datasets uniformly across the embedding space of the pretrained\nFM. We further identify these datasets are subject to a trade-off between size\nand balance, potentially compromising the quality of representations learned by\nFMs, and propose tailored batch sampling strategies to mitigate this effect. We\ndemonstrate the effectiveness of our method through improved performance on a\ndiverse range of clinically relevant downstream tasks."}
{"id": "2503.18711", "pdf": "https://arxiv.org/pdf/2503.18711", "abs": "https://arxiv.org/abs/2503.18711", "authors": ["Thomas Sugg", "Kyle O'Brien", "Lekh Poudel", "Alex Dumouchelle", "Michelle Jou", "Marc Bosch", "Deva Ramanan", "Srinivasa Narasimhan", "Shubham Tulsiani"], "title": "Accenture-NVS1: A Novel View Synthesis Dataset", "categories": ["cs.CV", "cs.LG"], "comment": "6 pages, 7 figures", "summary": "This paper introduces ACC-NVS1, a specialized dataset designed for research\non Novel View Synthesis specifically for airborne and ground imagery. Data for\nACC-NVS1 was collected in Austin, TX and Pittsburgh, PA in 2023 and 2024. The\ncollection encompasses six diverse real-world scenes captured from both\nairborne and ground cameras, resulting in a total of 148,000 images. ACC-NVS1\naddresses challenges such as varying altitudes and transient objects. This\ndataset is intended to supplement existing datasets, providing additional\nresources for comprehensive research, rather than serving as a benchmark."}
{"id": "2503.18712", "pdf": "https://arxiv.org/pdf/2503.18712", "abs": "https://arxiv.org/abs/2503.18712", "authors": ["Shaokai Ye", "Haozhe Qi", "Alexander Mathis", "Mackenzie W. Mathis"], "title": "LLaVAction: evaluating and training multi-modal large language models for action recognition", "categories": ["cs.CV"], "comment": "https://github.com/AdaptiveMotorControlLab/LLaVAction", "summary": "Understanding human behavior requires measuring behavioral actions. Due to\nits complexity, behavior is best mapped onto a rich, semantic structure such as\nlanguage. The recent development of multi-modal large language models (MLLMs)\nis a promising candidate for a wide range of action understanding tasks. In\nthis work, we focus on evaluating and then improving MLLMs to perform action\nrecognition. We reformulate EPIC-KITCHENS-100, one of the largest and most\nchallenging egocentric action datasets, to the form of video multiple question\nanswering (EPIC-KITCHENS-100-MQA). We show that when we sample difficult\nincorrect answers as distractors, leading MLLMs struggle to recognize the\ncorrect actions. We propose a series of methods that greatly improve the MLLMs'\nability to perform action recognition, achieving state-of-the-art on both the\nEPIC-KITCHENS-100 validation set, as well as outperforming GPT-4o by 21 points\nin accuracy on EPIC-KITCHENS-100-MQA. Lastly, we show improvements on other\naction-related video benchmarks such as EgoSchema, PerceptionTest,\nLongVideoBench, VideoMME and MVBench, suggesting that MLLMs are a promising\npath forward for complex action tasks. Code and models are available at:\nhttps://github.com/AdaptiveMotorControlLab/LLaVAction."}
{"id": "2503.18718", "pdf": "https://arxiv.org/pdf/2503.18718", "abs": "https://arxiv.org/abs/2503.18718", "authors": ["Lijiang Li", "Jinglu Wang", "Xiang Ming", "Yan Lu"], "title": "GS-Marker: Generalizable and Robust Watermarking for 3D Gaussian Splatting", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "In the Generative AI era, safeguarding 3D models has become increasingly\nurgent. While invisible watermarking is well-established for 2D images with\nencoder-decoder frameworks, generalizable and robust solutions for 3D remain\nelusive. The main difficulty arises from the renderer between the 3D encoder\nand 2D decoder, which disrupts direct gradient flow and complicates training.\nExisting 3D methods typically rely on per-scene iterative optimization,\nresulting in time inefficiency and limited generalization. In this work, we\npropose a single-pass watermarking approach for 3D Gaussian Splatting (3DGS), a\nwell-known yet underexplored representation for watermarking. We identify two\nmajor challenges: (1) ensuring effective training generalized across diverse 3D\nmodels, and (2) reliably extracting watermarks from free-view renderings, even\nunder distortions. Our framework, named GS-Marker, incorporates a 3D encoder to\nembed messages, distortion layers to enhance resilience against various\ndistortions, and a 2D decoder to extract watermarks from renderings. A key\ninnovation is the Adaptive Marker Control mechanism that adaptively perturbs\nthe initially optimized 3DGS, escaping local minima and improving both training\nstability and convergence. Extensive experiments show that GS-Marker\noutperforms per-scene training approaches in terms of decoding accuracy and\nmodel fidelity, while also significantly reducing computation time."}
{"id": "2503.18719", "pdf": "https://arxiv.org/pdf/2503.18719", "abs": "https://arxiv.org/abs/2503.18719", "authors": ["Cong Liu", "Liang Hou", "Mingwu Zheng", "Xin Tao", "Pengfei Wan", "Di Zhang", "Kun Gai"], "title": "Boosting Resolution Generalization of Diffusion Transformers with Randomized Positional Encodings", "categories": ["cs.CV"], "comment": null, "summary": "Resolution generalization in image generation tasks enables the production of\nhigher-resolution images with lower training resolution overhead. However, a\nsignificant challenge in resolution generalization, particularly in the widely\nused Diffusion Transformers, lies in the mismatch between the positional\nencodings encountered during testing and those used during training. While\nexisting methods have employed techniques such as interpolation, extrapolation,\nor their combinations, none have fully resolved this issue. In this paper, we\npropose a novel two-dimensional randomized positional encodings (RPE-2D)\nframework that focuses on learning positional order of image patches instead of\nthe specific distances between them, enabling seamless high- and low-resolution\nimage generation without requiring high- and low-resolution image training.\nSpecifically, RPE-2D independently selects positions over a broader range along\nboth the horizontal and vertical axes, ensuring that all position encodings are\ntrained during the inference phase, thus improving resolution generalization.\nAdditionally, we propose a random data augmentation technique to enhance the\nmodeling of position order. To address the issue of image cropping caused by\nthe augmentation, we introduce corresponding micro-conditioning to enable the\nmodel to perceive the specific cropping patterns. On the ImageNet dataset, our\nproposed RPE-2D achieves state-of-the-art resolution generalization\nperformance, outperforming existing competitive methods when trained at a\nresolution of $256 \\times 256$ and inferred at $384 \\times 384$ and $512 \\times\n512$, as well as when scaling from $512 \\times 512$ to $768 \\times 768$ and\n$1024 \\times 1024$. And it also exhibits outstanding capabilities in\nlow-resolution image generation, multi-stage training acceleration and\nmulti-resolution inheritance."}
{"id": "2503.18725", "pdf": "https://arxiv.org/pdf/2503.18725", "abs": "https://arxiv.org/abs/2503.18725", "authors": ["Zimin Xia", "Alexandre Alahi"], "title": "FG$^2$: Fine-Grained Cross-View Localization by Fine-Grained Feature Matching", "categories": ["cs.CV"], "comment": null, "summary": "We propose a novel fine-grained cross-view localization method that estimates\nthe 3 Degrees of Freedom pose of a ground-level image in an aerial image of the\nsurroundings by matching fine-grained features between the two images. The pose\nis estimated by aligning a point plane generated from the ground image with a\npoint plane sampled from the aerial image. To generate the ground points, we\nfirst map ground image features to a 3D point cloud. Our method then learns to\nselect features along the height dimension to pool the 3D points to a\nBird's-Eye-View (BEV) plane. This selection enables us to trace which feature\nin the ground image contributes to the BEV representation. Next, we sample a\nset of sparse matches from computed point correspondences between the two point\nplanes and compute their relative pose using Procrustes alignment. Compared to\nthe previous state-of-the-art, our method reduces the mean localization error\nby 28% on the VIGOR cross-area test set. Qualitative results show that our\nmethod learns semantically consistent matches across ground and aerial views\nthrough weakly supervised learning from the camera pose."}
{"id": "2503.18742", "pdf": "https://arxiv.org/pdf/2503.18742", "abs": "https://arxiv.org/abs/2503.18742", "authors": ["Sebastian Tewes", "Yufan Chen", "Omar Moured", "Jiaming Zhang", "Rainer Stiefelhagen"], "title": "SFDLA: Source-Free Document Layout Analysis", "categories": ["cs.CV"], "comment": "The benchmark, models, and code will be publicly available at\n  https://github.com/s3setewe/sfdla-DLAdapter", "summary": "Document Layout Analysis (DLA) is a fundamental task in document\nunderstanding. However, existing DLA and adaptation methods often require\naccess to large-scale source data and target labels. This requirements severely\nlimiting their real-world applicability, particularly in privacy-sensitive and\nresource-constrained domains, such as financial statements, medical records,\nand proprietary business documents. According to our observation, directly\ntransferring source-domain fine-tuned models on target domains often results in\na significant performance drop (Avg. -32.64%). In this work, we introduce\nSource-Free Document Layout Analysis (SFDLA), aiming for adapting a pre-trained\nsource DLA models to an unlabeled target domain, without access to any source\ndata. To address this challenge, we establish the first SFDLA benchmark,\ncovering three major DLA datasets for geometric- and content-aware adaptation.\nFurthermore, we propose Document Layout Analysis Adapter (DLAdapter), a novel\nframework that is designed to improve source-free adaptation across document\ndomains. Our method achieves a +4.21% improvement over the source-only baseline\nand a +2.26% gain over existing source-free methods from PubLayNet to\nDocLayNet. We believe this work will inspire the DLA community to further\ninvestigate source-free document understanding. To support future research of\nthe community, the benchmark, models, and code will be publicly available at\nhttps://github.com/s3setewe/sfdla-DLAdapter."}
{"id": "2503.18746", "pdf": "https://arxiv.org/pdf/2503.18746", "abs": "https://arxiv.org/abs/2503.18746", "authors": ["Yifei Zhang", "Chang Liu", "Jin Wei", "Xiaomeng Yang", "Yu Zhou", "Can Ma", "Xiangyang Ji"], "title": "Linguistics-aware Masked Image Modeling for Self-supervised Scene Text Recognition", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "Text images are unique in their dual nature, encompassing both visual and\nlinguistic information. The visual component encompasses structural and\nappearance-based features, while the linguistic dimension incorporates\ncontextual and semantic elements. In scenarios with degraded visual quality,\nlinguistic patterns serve as crucial supplements for comprehension,\nhighlighting the necessity of integrating both aspects for robust scene text\nrecognition (STR). Contemporary STR approaches often use language models or\nsemantic reasoning modules to capture linguistic features, typically requiring\nlarge-scale annotated datasets. Self-supervised learning, which lacks\nannotations, presents challenges in disentangling linguistic features related\nto the global context. Typically, sequence contrastive learning emphasizes the\nalignment of local features, while masked image modeling (MIM) tends to exploit\nlocal structures to reconstruct visual patterns, resulting in limited\nlinguistic knowledge. In this paper, we propose a Linguistics-aware Masked\nImage Modeling (LMIM) approach, which channels the linguistic information into\nthe decoding process of MIM through a separate branch. Specifically, we design\na linguistics alignment module to extract vision-independent features as\nlinguistic guidance using inputs with different visual appearances. As features\nextend beyond mere visual structures, LMIM must consider the global context to\nachieve reconstruction. Extensive experiments on various benchmarks\nquantitatively demonstrate our state-of-the-art performance, and attention\nvisualizations qualitatively show the simultaneous capture of both visual and\nlinguistic information."}
{"id": "2503.18753", "pdf": "https://arxiv.org/pdf/2503.18753", "abs": "https://arxiv.org/abs/2503.18753", "authors": ["Qin Wang", "Benjamin Bruns", "Hanno Scharr", "Kai Krajsek"], "title": "Self-Supervised Learning based on Transformed Image Reconstruction for Equivariance-Coherent Feature Representation", "categories": ["cs.CV"], "comment": null, "summary": "The equivariant behaviour of features is essential in many computer vision\ntasks, yet popular self-supervised learning (SSL) methods tend to constrain\nequivariance by design. We propose a self-supervised learning approach where\nthe system learns transformations independently by reconstructing images that\nhave undergone previously unseen transformations. Specifically, the model is\ntasked to reconstruct intermediate transformed images, e.g. translated or\nrotated images, without prior knowledge of these transformations. This\nauxiliary task encourages the model to develop equivariance-coherent features\nwithout relying on predefined transformation rules. To this end, we apply\ntransformations to the input image, generating an image pair, and then split\nthe extracted features into two sets per image. One set is used with a usual\nSSL loss encouraging invariance, the other with our loss based on the auxiliary\ntask to reconstruct the intermediate transformed images. Our loss and the SSL\nloss are linearly combined with weighted terms. Evaluating on synthetic tasks\nwith natural images, our proposed method strongly outperforms all competitors,\nregardless of whether they are designed to learn equivariance. Furthermore,\nwhen trained alongside augmentation-based methods as the invariance tasks, such\nas iBOT or DINOv2, we successfully learn a balanced combination of invariant\nand equivariant features. Our approach performs strong on a rich set of\nrealistic computer vision downstream tasks, almost always improving over all\nbaselines."}
{"id": "2503.18755", "pdf": "https://arxiv.org/pdf/2503.18755", "abs": "https://arxiv.org/abs/2503.18755", "authors": ["Nathan Darjana", "Ryo Fujii", "Hideo Saito", "Hiroki Kajita"], "title": "EgoSurgery-HTS: A Dataset for Egocentric Hand-Tool Segmentation in Open Surgery Videos", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Egocentric open-surgery videos capture rich, fine-grained details essential\nfor accurately modeling surgical procedures and human behavior in the operating\nroom. A detailed, pixel-level understanding of hands and surgical tools is\ncrucial for interpreting a surgeon's actions and intentions. We introduce\nEgoSurgery-HTS, a new dataset with pixel-wise annotations and a benchmark suite\nfor segmenting surgical tools, hands, and interacting tools in egocentric\nopen-surgery videos. Specifically, we provide a labeled dataset for (1) tool\ninstance segmentation of 14 distinct surgical tools, (2) hand instance\nsegmentation, and (3) hand-tool segmentation to label hands and the tools they\nmanipulate. Using EgoSurgery-HTS, we conduct extensive evaluations of\nstate-of-the-art segmentation methods and demonstrate significant improvements\nin the accuracy of hand and hand-tool segmentation in egocentric open-surgery\nvideos compared to existing datasets. The dataset will be released at\nhttps://github.com/Fujiry0/EgoSurgery."}
{"id": "2503.18767", "pdf": "https://arxiv.org/pdf/2503.18767", "abs": "https://arxiv.org/abs/2503.18767", "authors": ["Konstantin Pakulev", "Alexander Vakhitov", "Gonzalo Ferrer"], "title": "Good Keypoints for the Two-View Geometry Estimation Problem", "categories": ["cs.CV"], "comment": "Camera-ready version of the CVPR 2025 paper", "summary": "Local features are essential to many modern downstream applications.\nTherefore, it is of interest to determine the properties of local features that\ncontribute to the downstream performance for a better design of feature\ndetectors and descriptors. In our work, we propose a new theoretical model for\nscoring feature points (keypoints) in the context of the two-view geometry\nestimation problem. The model determines two properties that a good keypoint\nfor solving the homography estimation problem should have: be repeatable and\nhave a small expected measurement error. This result provides key insights into\nwhy maximizing the number of correspondences doesn't always lead to better\nhomography estimation accuracy. We use the developed model to design a method\nthat detects keypoints that benefit the homography estimation introducing the\nBounded NeSS-ST (BoNeSS-ST) keypoint detector. The novelty of BoNeSS-ST comes\nfrom strong theoretical foundations, a more accurate keypoint scoring due to\nsubpixel refinement and a cost designed for superior robustness to low saliency\nkeypoints. As a result, BoNeSS-ST outperforms prior self-supervised local\nfeature detectors in both planar homography and epipolar geometry estimation\nproblems."}
{"id": "2503.18783", "pdf": "https://arxiv.org/pdf/2503.18783", "abs": "https://arxiv.org/abs/2503.18783", "authors": ["Linwei Chen", "Lin Gu", "Liang Li", "Chenggang Yan", "Ying Fu"], "title": "Frequency Dynamic Convolution for Dense Image Prediction", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by CVPR 2025", "summary": "While Dynamic Convolution (DY-Conv) has shown promising performance by\nenabling adaptive weight selection through multiple parallel weights combined\nwith an attention mechanism, the frequency response of these weights tends to\nexhibit high similarity, resulting in high parameter costs but limited\nadaptability. In this work, we introduce Frequency Dynamic Convolution\n(FDConv), a novel approach that mitigates these limitations by learning a fixed\nparameter budget in the Fourier domain. FDConv divides this budget into\nfrequency-based groups with disjoint Fourier indices, enabling the construction\nof frequency-diverse weights without increasing the parameter cost. To further\nenhance adaptability, we propose Kernel Spatial Modulation (KSM) and Frequency\nBand Modulation (FBM). KSM dynamically adjusts the frequency response of each\nfilter at the spatial level, while FBM decomposes weights into distinct\nfrequency bands in the frequency domain and modulates them dynamically based on\nlocal content. Extensive experiments on object detection, segmentation, and\nclassification validate the effectiveness of FDConv. We demonstrate that when\napplied to ResNet-50, FDConv achieves superior performance with a modest\nincrease of +3.6M parameters, outperforming previous methods that require\nsubstantial increases in parameter budgets (e.g., CondConv +90M, KW +76.5M).\nMoreover, FDConv seamlessly integrates into a variety of architectures,\nincluding ConvNeXt, Swin-Transformer, offering a flexible and efficient\nsolution for modern vision tasks. The code is made publicly available at\nhttps://github.com/Linwei-Chen/FDConv."}
{"id": "2503.18784", "pdf": "https://arxiv.org/pdf/2503.18784", "abs": "https://arxiv.org/abs/2503.18784", "authors": ["Wenxi Chen", "Raymond A. Yeh", "Shaoshuai Mou", "Yan Gu"], "title": "Leveraging Perturbation Robustness to Enhance Out-of-Distribution Detection", "categories": ["cs.CV"], "comment": null, "summary": "Out-of-distribution (OOD) detection is the task of identifying inputs that\ndeviate from the training data distribution. This capability is essential for\nsafely deploying deep computer vision models in open-world environments. In\nthis work, we propose a post-hoc method, Perturbation-Rectified OOD detection\n(PRO), based on the insight that prediction confidence for OOD inputs is more\nsusceptible to reduction under perturbation than in-distribution (IND) inputs.\nBased on the observation, we propose an adversarial score function that\nsearches for the local minimum scores near the original inputs by applying\ngradient descent. This procedure enhances the separability between IND and OOD\nsamples. Importantly, the approach improves OOD detection performance without\ncomplex modifications to the underlying model architectures. We conduct\nextensive experiments using the OpenOOD benchmark~\\cite{yang2022openood}. Our\napproach further pushes the limit of softmax-based OOD detection and is the\nleading post-hoc method for small-scale models. On a CIFAR-10 model with\nadversarial training, PRO effectively detects near-OOD inputs, achieving a\nreduction of more than 10\\% on FPR@95 compared to state-of-the-art methods."}
{"id": "2503.18785", "pdf": "https://arxiv.org/pdf/2503.18785", "abs": "https://arxiv.org/abs/2503.18785", "authors": ["Zifa Chen"], "title": "LGI-DETR: Local-Global Interaction for UAV Object Detection", "categories": ["cs.CV"], "comment": "14 pages", "summary": "UAV has been widely used in various fields. However, most of the existing\nobject detectors used in drones are not end-to-end and require the design of\nvarious complex components and careful fine-tuning. Most of the existing\nend-to-end object detectors are designed for natural scenes. It is not ideal to\napply them directly to UAV images. In order to solve the above challenges, we\ndesign an local-global information interaction DETR for UAVs, namely LGI-DETR.\nCross-layer bidirectional low-level and high-level feature information\nenhancement, this fusion method is effective especially in the field of small\nobjection detection. At the initial stage of encoder, we propose a local\nspatial enhancement module (LSE), which enhances the low-level rich local\nspatial information into the high-level feature, and reduces the loss of local\ninformation in the transmission process of high-level information. At the final\nstage of the encoder, we propose a novel global information injection module\n(GII) designed to integrate rich high-level global semantic representations\nwith low-level feature maps. This hierarchical fusion mechanism effectively\naddresses the inherent limitations of local receptive fields by propagating\ncontextual information across the feature hierarchy. Experimental results on\ntwo challenging UAV image object detection benchmarks, VisDrone2019 and UAVDT,\nshow that our proposed model outperforms the SOTA model. Compared to the\nbaseline model, AP and AP50 improved by 1.9% and 2.4%, respectively."}
{"id": "2503.18794", "pdf": "https://arxiv.org/pdf/2503.18794", "abs": "https://arxiv.org/abs/2503.18794", "authors": ["Yulong Zheng", "Zicheng Jiang", "Shengfeng He", "Yandu Sun", "Junyu Dong", "Huaidong Zhang", "Yong Du"], "title": "NexusGS: Sparse View Synthesis with Epipolar Depth Priors in 3D Gaussian Splatting", "categories": ["cs.CV"], "comment": "This paper is accepted by CVPR 2025", "summary": "Neural Radiance Field (NeRF) and 3D Gaussian Splatting (3DGS) have noticeably\nadvanced photo-realistic novel view synthesis using images from densely spaced\ncamera viewpoints. However, these methods struggle in few-shot scenarios due to\nlimited supervision. In this paper, we present NexusGS, a 3DGS-based approach\nthat enhances novel view synthesis from sparse-view images by directly\nembedding depth information into point clouds, without relying on complex\nmanual regularizations. Exploiting the inherent epipolar geometry of 3DGS, our\nmethod introduces a novel point cloud densification strategy that initializes\n3DGS with a dense point cloud, reducing randomness in point placement while\npreventing over-smoothing and overfitting. Specifically, NexusGS comprises\nthree key steps: Epipolar Depth Nexus, Flow-Resilient Depth Blending, and\nFlow-Filtered Depth Pruning. These steps leverage optical flow and camera poses\nto compute accurate depth maps, while mitigating the inaccuracies often\nassociated with optical flow. By incorporating epipolar depth priors, NexusGS\nensures reliable dense point cloud coverage and supports stable 3DGS training\nunder sparse-view conditions. Experiments demonstrate that NexusGS\nsignificantly enhances depth accuracy and rendering quality, surpassing\nstate-of-the-art methods by a considerable margin. Furthermore, we validate the\nsuperiority of our generated point clouds by substantially boosting the\nperformance of competing methods. Project page:\nhttps://usmizuki.github.io/NexusGS/."}
{"id": "2503.18803", "pdf": "https://arxiv.org/pdf/2503.18803", "abs": "https://arxiv.org/abs/2503.18803", "authors": ["Duowang Zhu", "Xiaohu Huang", "Haiyan Huang", "Hao Zhou", "Zhenfeng Shao"], "title": "Change3D: Revisiting Change Detection and Captioning from A Video Modeling Perspective", "categories": ["cs.CV"], "comment": "conference paper, accepted by CVPR 2025", "summary": "In this paper, we present Change3D, a framework that reconceptualizes the\nchange detection and captioning tasks through video modeling. Recent methods\nhave achieved remarkable success by regarding each pair of bi-temporal images\nas separate frames. They employ a shared-weight image encoder to extract\nspatial features and then use a change extractor to capture differences between\nthe two images. However, image feature encoding, being a task-agnostic process,\ncannot attend to changed regions effectively. Furthermore, different change\nextractors designed for various change detection and captioning tasks make it\ndifficult to have a unified framework. To tackle these challenges, Change3D\nregards the bi-temporal images as comprising two frames akin to a tiny video.\nBy integrating learnable perception frames between the bi-temporal images, a\nvideo encoder enables the perception frames to interact with the images\ndirectly and perceive their differences. Therefore, we can get rid of the\nintricate change extractors, providing a unified framework for different change\ndetection and captioning tasks. We verify Change3D on multiple tasks,\nencompassing change detection (including binary change detection, semantic\nchange detection, and building damage assessment) and change captioning, across\neight standard benchmarks. Without bells and whistles, this simple yet\neffective framework can achieve superior performance with an ultra-light video\nmodel comprising only ~6%-13% of the parameters and ~8%-34% of the FLOPs\ncompared to state-of-the-art methods. We hope that Change3D could be an\nalternative to 2D-based models and facilitate future research."}
{"id": "2503.18808", "pdf": "https://arxiv.org/pdf/2503.18808", "abs": "https://arxiv.org/abs/2503.18808", "authors": ["Yang Liu", "Hongjin Wang", "Zepu Wang", "Xiaoguang Zhu", "Jing Liu", "Peng Sun", "Rui Tang", "Jianwei Du", "Victor C. M. Leung", "Liang Song"], "title": "CRCL: Causal Representation Consistency Learning for Anomaly Detection in Surveillance Videos", "categories": ["cs.CV"], "comment": "Accepted for publication by IEEE Transactions on Image Processing", "summary": "Video Anomaly Detection (VAD) remains a fundamental yet formidable task in\nthe video understanding community, with promising applications in areas such as\ninformation forensics and public safety protection. Due to the rarity and\ndiversity of anomalies, existing methods only use easily collected regular\nevents to model the inherent normality of normal spatial-temporal patterns in\nan unsupervised manner. Previous studies have shown that existing unsupervised\nVAD models are incapable of label-independent data offsets (e.g., scene\nchanges) in real-world scenarios and may fail to respond to light anomalies due\nto the overgeneralization of deep neural networks. Inspired by causality\nlearning, we argue that there exist causal factors that can adequately\ngeneralize the prototypical patterns of regular events and present significant\ndeviations when anomalous instances occur. In this regard, we propose Causal\nRepresentation Consistency Learning (CRCL) to implicitly mine potential\nscene-robust causal variable in unsupervised video normality learning.\nSpecifically, building on the structural causal models, we propose\nscene-debiasing learning and causality-inspired normality learning to strip\naway entangled scene bias in deep representations and learn causal video\nnormality, respectively. Extensive experiments on benchmarks validate the\nsuperiority of our method over conventional deep representation learning.\nMoreover, ablation studies and extension validation show that the CRCL can cope\nwith label-independent biases in multi-scene settings and maintain stable\nperformance with only limited training data available."}
{"id": "2503.18812", "pdf": "https://arxiv.org/pdf/2503.18812", "abs": "https://arxiv.org/abs/2503.18812", "authors": ["Shrikant Malviya", "Neelanjan Bhowmik", "Stamos Katsigiannis"], "title": "SKDU at De-Factify 4.0: Vision Transformer with Data Augmentation for AI-Generated Image Detection", "categories": ["cs.CV"], "comment": "De-Factify 4.0 workshop at the 39th Annual AAAI Conference on\n  Artificial Intelligence (AAAI 2025)", "summary": "The aim of this work is to explore the potential of pre-trained\nvision-language models, e.g. Vision Transformers (ViT), enhanced with advanced\ndata augmentation strategies for the detection of AI-generated images. Our\napproach leverages a fine-tuned ViT model trained on the Defactify-4.0 dataset,\nwhich includes images generated by state-of-the-art models such as Stable\nDiffusion 2.1, Stable Diffusion XL, Stable Diffusion 3, DALL-E 3, and\nMidJourney. We employ perturbation techniques like flipping, rotation, Gaussian\nnoise injection, and JPEG compression during training to improve model\nrobustness and generalisation. The experimental results demonstrate that our\nViT-based pipeline achieves state-of-the-art performance, significantly\noutperforming competing methods on both validation and test datasets."}
{"id": "2503.18817", "pdf": "https://arxiv.org/pdf/2503.18817", "abs": "https://arxiv.org/abs/2503.18817", "authors": ["Jeonghyeon Kim", "Sangheum Hwang"], "title": "Enhanced OoD Detection through Cross-Modal Alignment of Multi-Modal Representations", "categories": ["cs.CV", "cs.AI"], "comment": "CVPR 2025", "summary": "Prior research on out-of-distribution detection (OoDD) has primarily focused\non single-modality models. Recently, with the advent of large-scale pretrained\nvision-language models such as CLIP, OoDD methods utilizing such multi-modal\nrepresentations through zero-shot and prompt learning strategies have emerged.\nHowever, these methods typically involve either freezing the pretrained weights\nor only partially tuning them, which can be suboptimal for downstream datasets.\nIn this paper, we highlight that multi-modal fine-tuning (MMFT) can achieve\nnotable OoDD performance. Despite some recent works demonstrating the impact of\nfine-tuning methods for OoDD, there remains significant potential for\nperformance improvement. We investigate the limitation of na\\\"ive fine-tuning\nmethods, examining why they fail to fully leverage the pretrained knowledge.\nOur empirical analysis suggests that this issue could stem from the modality\ngap within in-distribution (ID) embeddings. To address this, we propose a\ntraining objective that enhances cross-modal alignment by regularizing the\ndistances between image and text embeddings of ID data. This adjustment helps\nin better utilizing pretrained textual information by aligning similar\nsemantics from different modalities (i.e., text and image) more closely in the\nhyperspherical representation space. We theoretically demonstrate that the\nproposed regularization corresponds to the maximum likelihood estimation of an\nenergy-based model on a hypersphere. Utilizing ImageNet-1k OoD benchmark\ndatasets, we show that our method, combined with post-hoc OoDD approaches\nleveraging pretrained knowledge (e.g., NegLabel), significantly outperforms\nexisting methods, achieving state-of-the-art OoDD performance and leading ID\naccuracy."}
{"id": "2503.18830", "pdf": "https://arxiv.org/pdf/2503.18830", "abs": "https://arxiv.org/abs/2503.18830", "authors": ["Zhengxian Wu", "Chuanrui Zhang", "Hangrui Xu", "Peng Jiao", "Haoqian Wang"], "title": "DAGait: Generalized Skeleton-Guided Data Alignment for Gait Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Gait recognition is emerging as a promising and innovative area within the\nfield of computer vision, widely applied to remote person identification.\nAlthough existing gait recognition methods have achieved substantial success in\ncontrolled laboratory datasets, their performance often declines significantly\nwhen transitioning to wild datasets.We argue that the performance gap can be\nprimarily attributed to the spatio-temporal distribution inconsistencies\npresent in wild datasets, where subjects appear at varying angles, positions,\nand distances across the frames. To achieve accurate gait recognition in the\nwild, we propose a skeleton-guided silhouette alignment strategy, which uses\nprior knowledge of the skeletons to perform affine transformations on the\ncorresponding silhouettes.To the best of our knowledge, this is the first study\nto explore the impact of data alignment on gait recognition. We conducted\nextensive experiments across multiple datasets and network architectures, and\nthe results demonstrate the significant advantages of our proposed alignment\nstrategy.Specifically, on the challenging Gait3D dataset, our method achieved\nan average performance improvement of 7.9% across all evaluated networks.\nFurthermore, our method achieves substantial improvements on cross-domain\ndatasets, with accuracy improvements of up to 24.0%."}
{"id": "2503.18853", "pdf": "https://arxiv.org/pdf/2503.18853", "abs": "https://arxiv.org/abs/2503.18853", "authors": ["Xiao Cao", "Beibei Lin", "Bo Wang", "Zhiyong Huang", "Robby T. Tan"], "title": "3DSwapping: Texture Swapping For 3D Object From Single Reference Image", "categories": ["cs.CV"], "comment": null, "summary": "3D texture swapping allows for the customization of 3D object textures,\nenabling efficient and versatile visual transformations in 3D editing. While no\ndedicated method exists, adapted 2D editing and text-driven 3D editing\napproaches can serve this purpose. However, 2D editing requires frame-by-frame\nmanipulation, causing inconsistencies across views, while text-driven 3D\nediting struggles to preserve texture characteristics from reference images. To\ntackle these challenges, we introduce 3DSwapping, a 3D texture swapping method\nthat integrates: 1) progressive generation, 2) view-consistency gradient\nguidance, and 3) prompt-tuned gradient guidance. To ensure view consistency,\nour progressive generation process starts by editing a single reference image\nand gradually propagates the edits to adjacent views. Our view-consistency\ngradient guidance further reinforces consistency by conditioning the generation\nmodel on feature differences between consistent and inconsistent outputs. To\npreserve texture characteristics, we introduce prompt-tuning-based gradient\nguidance, which learns a token that precisely captures the difference between\nthe reference image and the 3D object. This token then guides the editing\nprocess, ensuring more consistent texture preservation across views. Overall,\n3DSwapping integrates these novel strategies to achieve higher-fidelity texture\ntransfer while preserving structural coherence across multiple viewpoints.\nExtensive qualitative and quantitative evaluations confirm that our three novel\ncomponents enable convincing and effective 2D texture swapping for 3D objects.\nCode will be available upon acceptance."}
{"id": "2503.18854", "pdf": "https://arxiv.org/pdf/2503.18854", "abs": "https://arxiv.org/abs/2503.18854", "authors": ["Ruichuan An", "Sihan Yang", "Ming Lu", "Renrui Zhang", "Kai Zeng", "Yulin Luo", "Jiajun Cao", "Hao Liang", "Ying Chen", "Qi She", "Shanghang Zhang", "Wentao Zhang"], "title": "MC-LLaVA: Multi-Concept Personalized Vision-Language Model", "categories": ["cs.CV", "cs.AI"], "comment": "The code and dataset will be publicly available at\n  $\\href{https://github.com/arctanxarc/MC-LLaVA}{https://github.com/arctanxarc/MC-LLaVA}$", "summary": "Current vision-language models (VLMs) show exceptional abilities across\ndiverse tasks, such as visual question answering. To enhance user experience,\nrecent studies investigate VLM personalization to understand user-provided\nconcepts. However, they mainly focus on single-concept personalization,\nneglecting the existence and interplay of multiple concepts, which limits\nreal-world applicability. This paper proposes the first multi-concept\npersonalization paradigm, MC-LLaVA. Specifically, MC-LLaVA employs a\nmulti-concept instruction tuning strategy, effectively integrating multiple\nconcepts in a single training step. To reduce the costs related to joint\ntraining, we propose a personalized textual prompt that uses visual token\ninformation to initialize concept tokens. Additionally, we introduce a\npersonalized visual prompt during inference, aggregating location confidence\nmaps for enhanced recognition and grounding capabilities. To advance\nmulti-concept personalization research, we further contribute a high-quality\ninstruction tuning dataset. We carefully collect images with multiple\ncharacters and objects from movies and manually generate question-answer\nsamples for multi-concept scenarios, featuring superior diversity.\nComprehensive qualitative and quantitative experiments demonstrate that\nMC-LLaVA can achieve impressive multi-concept personalized responses, paving\nthe way for VLMs to become better user-specific assistants. The code and\ndataset will be publicly available at\n$\\href{https://github.com/arctanxarc/MC-LLaVA}{https://github.com/arctanxarc/MC-LLaVA}$."}
{"id": "2503.18860", "pdf": "https://arxiv.org/pdf/2503.18860", "abs": "https://arxiv.org/abs/2503.18860", "authors": ["Zunnan Xu", "Zhentao Yu", "Zixiang Zhou", "Jun Zhou", "Xiaoyu Jin", "Fa-Ting Hong", "Xiaozhong Ji", "Junwei Zhu", "Chengfei Cai", "Shiyu Tang", "Qin Lin", "Xiu Li", "Qinglin Lu"], "title": "HunyuanPortrait: Implicit Condition Control for Enhanced Portrait Animation", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "We introduce HunyuanPortrait, a diffusion-based condition control method that\nemploys implicit representations for highly controllable and lifelike portrait\nanimation. Given a single portrait image as an appearance reference and video\nclips as driving templates, HunyuanPortrait can animate the character in the\nreference image by the facial expression and head pose of the driving videos.\nIn our framework, we utilize pre-trained encoders to achieve the decoupling of\nportrait motion information and identity in videos. To do so, implicit\nrepresentation is adopted to encode motion information and is employed as\ncontrol signals in the animation phase. By leveraging the power of stable video\ndiffusion as the main building block, we carefully design adapter layers to\ninject control signals into the denoising unet through attention mechanisms.\nThese bring spatial richness of details and temporal consistency.\nHunyuanPortrait also exhibits strong generalization performance, which can\neffectively disentangle appearance and motion under different image styles. Our\nframework outperforms existing methods, demonstrating superior temporal\nconsistency and controllability. Our project is available at\nhttps://kkakkkka.github.io/HunyuanPortrait."}
{"id": "2503.18862", "pdf": "https://arxiv.org/pdf/2503.18862", "abs": "https://arxiv.org/abs/2503.18862", "authors": ["DeShin Hwa", "Tobias Holmes", "Klaus Drechsler"], "title": "Exploring the Integration of Key-Value Attention Into Pure and Hybrid Transformers for Semantic Segmentation", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "6 pages, 3 figures, Preprint. Final version published in:\n  Bildverarbeitung f\\\"ur die Medizin 2025, Springer. DOI:\n  https://doi.org/10.1007/978-3-658-47422-5_71", "summary": "While CNNs were long considered state of the art for image processing, the\nintroduction of Transformer architectures has challenged this position. While\nachieving excellent results in image classification and segmentation,\nTransformers remain inherently reliant on large training datasets and remain\ncomputationally expensive. A newly introduced Transformer derivative named KV\nTransformer shows promising results in synthetic, NLP, and image classification\ntasks, while reducing complexity and memory usage. This is especially conducive\nto use cases where local inference is required, such as medical screening\napplications. We endeavoured to further evaluate the merit of KV Transformers\non semantic segmentation tasks, specifically in the domain of medical imaging.\nBy directly comparing traditional and KV variants of the same base\narchitectures, we provide further insight into the practical tradeoffs of\nreduced model complexity. We observe a notable reduction in parameter count and\nmultiply accumulate operations, while achieving similar performance from most\nof the KV variant models when directly compared to their QKV implementation."}
{"id": "2503.18872", "pdf": "https://arxiv.org/pdf/2503.18872", "abs": "https://arxiv.org/abs/2503.18872", "authors": ["Yanda Chen", "Gongwei Chen", "Miao Zhang", "Weili Guan", "Liqiang Nie"], "title": "Curriculum Coarse-to-Fine Selection for High-IPC Dataset Distillation", "categories": ["cs.CV"], "comment": "Accepted by CVPR2025", "summary": "Dataset distillation (DD) excels in synthesizing a small number of images per\nclass (IPC) but struggles to maintain its effectiveness in high-IPC settings.\nRecent works on dataset distillation demonstrate that combining distilled and\nreal data can mitigate the effectiveness decay. However, our analysis of the\ncombination paradigm reveals that the current one-shot and independent\nselection mechanism induces an incompatibility issue between distilled and real\nimages. To address this issue, we introduce a novel curriculum coarse-to-fine\nselection (CCFS) method for efficient high-IPC dataset distillation. CCFS\nemploys a curriculum selection framework for real data selection, where we\nleverage a coarse-to-fine strategy to select appropriate real data based on the\ncurrent synthetic dataset in each curriculum. Extensive experiments validate\nCCFS, surpassing the state-of-the-art by +6.6\\% on CIFAR-10, +5.8\\% on\nCIFAR-100, and +3.4\\% on Tiny-ImageNet under high-IPC settings. Notably, CCFS\nachieves 60.2\\% test accuracy on ResNet-18 with a 20\\% compression ratio of\nTiny-ImageNet, closely matching full-dataset training with only 0.3\\%\ndegradation. Code: https://github.com/CYDaaa30/CCFS."}
{"id": "2503.18873", "pdf": "https://arxiv.org/pdf/2503.18873", "abs": "https://arxiv.org/abs/2503.18873", "authors": ["Moein Sorkhei", "Emir Konuk", "Jingyu Guo", "Christos Matsoukas", "Kevin Smith"], "title": "Efficient Self-Supervised Adaptation for Medical Image Analysis", "categories": ["cs.CV"], "comment": null, "summary": "Self-supervised adaptation (SSA) improves foundation model transfer to\nmedical domains but is computationally prohibitive. Although parameter\nefficient fine-tuning methods such as LoRA have been explored for supervised\nadaptation, their effectiveness for SSA remains unknown. In this work, we\nintroduce efficient self-supervised adaptation (ESSA), a framework that applies\nparameter-efficient fine-tuning techniques to SSA with the aim of reducing\ncomputational cost and improving adaptation performance. Among the methods\ntested, Attention Projection Layer Adaptation (APLA) sets a new\nstate-of-the-art, consistently surpassing full-parameter SSA and supervised\nfine-tuning across diverse medical tasks, while reducing GPU memory by up to\n40.1% and increasing training throughput by 25.2%, all while maintaining\ninference efficiency."}
{"id": "2503.18880", "pdf": "https://arxiv.org/pdf/2503.18880", "abs": "https://arxiv.org/abs/2503.18880", "authors": ["Hyeonggon Ryu", "Seongyu Kim", "Joon Son Chung", "Arda Senocak"], "title": "Seeing Speech and Sound: Distinguishing and Locating Audios in Visual Scenes", "categories": ["cs.CV", "cs.SD", "eess.AS"], "comment": "CVPR 2025", "summary": "We present a unified model capable of simultaneously grounding both spoken\nlanguage and non-speech sounds within a visual scene, addressing key\nlimitations in current audio-visual grounding models. Existing approaches are\ntypically limited to handling either speech or non-speech sounds independently,\nor at best, together but sequentially without mixing. This limitation prevents\nthem from capturing the complexity of real-world audio sources that are often\nmixed. Our approach introduces a 'mix-and-separate' framework with audio-visual\nalignment objectives that jointly learn correspondence and disentanglement\nusing mixed audio. Through these objectives, our model learns to produce\ndistinct embeddings for each audio type, enabling effective disentanglement and\ngrounding across mixed audio sources. Additionally, we created a new dataset to\nevaluate simultaneous grounding of mixed audio sources, demonstrating that our\nmodel outperforms prior methods. Our approach also achieves comparable or\nbetter performance in standard segmentation and cross-modal retrieval tasks,\nhighlighting the benefits of our mix-and-separate approach."}
{"id": "2503.18883", "pdf": "https://arxiv.org/pdf/2503.18883", "abs": "https://arxiv.org/abs/2503.18883", "authors": ["Savas Ozkan", "Andrea Maracani", "Hyowon Kim", "Sijun Cho", "Eunchung Noh", "Jeongwon Min", "Jung Min Cho", "Mete Ozay"], "title": "Efficient and Accurate Scene Text Recognition with Cascaded-Transformers", "categories": ["cs.CV"], "comment": "Accepted to ACM-MMSys2025", "summary": "In recent years, vision transformers with text decoder have demonstrated\nremarkable performance on Scene Text Recognition (STR) due to their ability to\ncapture long-range dependencies and contextual relationships with high learning\ncapacity. However, the computational and memory demands of these models are\nsignificant, limiting their deployment in resource-constrained applications. To\naddress this challenge, we propose an efficient and accurate STR system.\nSpecifically, we focus on improving the efficiency of encoder models by\nintroducing a cascaded-transformers structure. This structure progressively\nreduces the vision token size during the encoding step, effectively eliminating\nredundant tokens and reducing computational cost. Our experimental results\nconfirm that our STR system achieves comparable performance to state-of-the-art\nbaselines while substantially decreasing computational requirements. In\nparticular, for large-models, the accuracy remains same, 92.77 to 92.68, while\ncomputational complexity is almost halved with our structure."}
{"id": "2503.18886", "pdf": "https://arxiv.org/pdf/2503.18886", "abs": "https://arxiv.org/abs/2503.18886", "authors": ["Weichen Fan", "Amber Yijia Zheng", "Raymond A. Yeh", "Ziwei Liu"], "title": "CFG-Zero*: Improved Classifier-Free Guidance for Flow Matching Models", "categories": ["cs.CV"], "comment": null, "summary": "Classifier-Free Guidance (CFG) is a widely adopted technique in\ndiffusion/flow models to improve image fidelity and controllability. In this\nwork, we first analytically study the effect of CFG on flow matching models\ntrained on Gaussian mixtures where the ground-truth flow can be derived. We\nobserve that in the early stages of training, when the flow estimation is\ninaccurate, CFG directs samples toward incorrect trajectories. Building on this\nobservation, we propose CFG-Zero*, an improved CFG with two contributions: (a)\noptimized scale, where a scalar is optimized to correct for the inaccuracies in\nthe estimated velocity, hence the * in the name; and (b) zero-init, which\ninvolves zeroing out the first few steps of the ODE solver. Experiments on both\ntext-to-image (Lumina-Next, Stable Diffusion 3, and Flux) and text-to-video\n(Wan-2.1) generation demonstrate that CFG-Zero* consistently outperforms CFG,\nhighlighting its effectiveness in guiding Flow Matching models. (Code is\navailable at github.com/WeichenFan/CFG-Zero-star)"}
{"id": "2503.18897", "pdf": "https://arxiv.org/pdf/2503.18897", "abs": "https://arxiv.org/abs/2503.18897", "authors": ["Thomas Chabal", "Shizhe Chen", "Jean Ponce", "Cordelia Schmid"], "title": "Online 3D Scene Reconstruction Using Neural Object Priors", "categories": ["cs.CV", "cs.RO"], "comment": "3DV 2025. Project page:\n  https://www.di.ens.fr/willow/research/online-scene-reconstruction/", "summary": "This paper addresses the problem of reconstructing a scene online at the\nlevel of objects given an RGB-D video sequence. While current object-aware\nneural implicit representations hold promise, they are limited in online\nreconstruction efficiency and shape completion. Our main contributions to\nalleviate the above limitations are twofold. First, we propose a feature grid\ninterpolation mechanism to continuously update grid-based object-centric neural\nimplicit representations as new object parts are revealed. Second, we construct\nan object library with previously mapped objects in advance and leverage the\ncorresponding shape priors to initialize geometric object models in new videos,\nsubsequently completing them with novel views as well as synthesized past views\nto avoid losing original object details. Extensive experiments on synthetic\nenvironments from the Replica dataset, real-world ScanNet sequences and videos\ncaptured in our laboratory demonstrate that our approach outperforms\nstate-of-the-art neural implicit models for this task in terms of\nreconstruction accuracy and completeness."}
{"id": "2503.18903", "pdf": "https://arxiv.org/pdf/2503.18903", "abs": "https://arxiv.org/abs/2503.18903", "authors": ["Moussa Kassem Sbeyti", "Nadja Klein", "Azarm Nowzad", "Fikret Sivrikaya", "Sahin Albayrak"], "title": "Building Blocks for Robust and Effective Semi-Supervised Real-World Object Detection", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted to Transactions on Machine Learning Research (TMLR).\n  OpenReview: https://openreview.net/forum?id=vRYt8QLKqK", "summary": "Semi-supervised object detection (SSOD) based on pseudo-labeling\nsignificantly reduces dependence on large labeled datasets by effectively\nleveraging both labeled and unlabeled data. However, real-world applications of\nSSOD often face critical challenges, including class imbalance, label noise,\nand labeling errors. We present an in-depth analysis of SSOD under real-world\nconditions, uncovering causes of suboptimal pseudo-labeling and key trade-offs\nbetween label quality and quantity. Based on our findings, we propose four\nbuilding blocks that can be seamlessly integrated into an SSOD framework. Rare\nClass Collage (RCC): a data augmentation method that enhances the\nrepresentation of rare classes by creating collages of rare objects. Rare Class\nFocus (RCF): a stratified batch sampling strategy that ensures a more balanced\nrepresentation of all classes during training. Ground Truth Label Correction\n(GLC): a label refinement method that identifies and corrects false, missing,\nand noisy ground truth labels by leveraging the consistency of teacher model\npredictions. Pseudo-Label Selection (PLS): a selection method for removing\nlow-quality pseudo-labeled images, guided by a novel metric estimating the\nmissing detection rate while accounting for class rarity. We validate our\nmethods through comprehensive experiments on autonomous driving datasets,\nresulting in up to 6% increase in SSOD performance. Overall, our investigation\nand novel, data-centric, and broadly applicable building blocks enable robust\nand effective SSOD in complex, real-world scenarios. Code is available at\nhttps://mos-ks.github.io/publications."}
{"id": "2503.18923", "pdf": "https://arxiv.org/pdf/2503.18923", "abs": "https://arxiv.org/abs/2503.18923", "authors": ["Meng Cao", "Pengfei Hu", "Yingyao Wang", "Jihao Gu", "Haoran Tang", "Haoze Zhao", "Jiahua Dong", "Wangbo Yu", "Ge Zhang", "Ian Reid", "Xiaodan Liang"], "title": "Video SimpleQA: Towards Factuality Evaluation in Large Video Language Models", "categories": ["cs.CV"], "comment": "24 pages", "summary": "Recent advancements in Large Video Language Models (LVLMs) have highlighted\ntheir potential for multi-modal understanding, yet evaluating their factual\ngrounding in video contexts remains a critical unsolved challenge. To address\nthis gap, we introduce Video SimpleQA, the first comprehensive benchmark\ntailored for factuality evaluation of LVLMs. Our work distinguishes from\nexisting video benchmarks through the following key features: 1) Knowledge\nrequired: demanding integration of external knowledge beyond the explicit\nnarrative; 2) Fact-seeking question: targeting objective, undisputed events or\nrelationships, avoiding subjective interpretation; 3) Definitive & short-form\nanswer: Answers are crafted as unambiguous and definitively correct in a short\nformat, enabling automated evaluation through LLM-as-a-judge frameworks with\nminimal scoring variance; 4) External-source verified: All annotations undergo\nrigorous validation against authoritative external references to ensure the\nreliability; 5) Temporal reasoning required: The annotated question types\nencompass both static single-frame understanding and dynamic temporal\nreasoning, explicitly evaluating LVLMs factuality under the long-context\ndependencies. We extensively evaluate 41 state-of-the-art LVLMs and summarize\nkey findings as follows: 1) Current LVLMs exhibit notable deficiencies in\nfactual adherence, particularly for open-source models. The best-performing\nmodel Gemini-1.5-Pro achieves merely an F-score of 54.4%; 2) Test-time compute\nparadigms show insignificant performance gains, revealing fundamental\nconstraints for enhancing factuality through post-hoc computation; 3)\nRetrieval-Augmented Generation demonstrates consistent improvements at the cost\nof additional inference time overhead, presenting a critical\nefficiency-performance trade-off."}
{"id": "2503.18931", "pdf": "https://arxiv.org/pdf/2503.18931", "abs": "https://arxiv.org/abs/2503.18931", "authors": ["Yitong Chen", "Lingchen Meng", "Wujian Peng", "Zuxuan Wu", "Yu-Gang Jiang"], "title": "CoMP: Continual Multimodal Pre-training for Vision Foundation Models", "categories": ["cs.CV"], "comment": "Code is available in https://github.com/SliMM-X/CoMP-MM", "summary": "Pre-trained Vision Foundation Models (VFMs) provide strong visual\nrepresentations for a wide range of applications. In this paper, we continually\npre-train prevailing VFMs in a multimodal manner such that they can\neffortlessly process visual inputs of varying sizes and produce visual\nrepresentations that are more aligned with language representations, regardless\nof their original pre-training process. To this end, we introduce CoMP, a\ncarefully designed multimodal pre-training pipeline. CoMP uses a Continual\nRotary Position Embedding to support native resolution continual pre-training,\nand an Alignment Loss between visual and textual features through language\nprototypes to align multimodal representations. By three-stage training, our\nVFMs achieve remarkable improvements not only in multimodal understanding but\nalso in other downstream tasks such as classification and segmentation.\nRemarkably, CoMP-SigLIP achieves scores of 66.7 on ChartQA and 75.9 on DocVQA\nwith a 0.5B LLM, while maintaining an 87.4% accuracy on ImageNet-1K and a 49.5\nmIoU on ADE20K under frozen chunk evaluation."}
{"id": "2503.18933", "pdf": "https://arxiv.org/pdf/2503.18933", "abs": "https://arxiv.org/abs/2503.18933", "authors": ["Enrico Pallotta", "Sina Mokhtarzadeh Azar", "Shuai Li", "Olga Zatsarynna", "Juergen Gall"], "title": "SyncVP: Joint Diffusion for Synchronous Multi-Modal Video Prediction", "categories": ["cs.CV"], "comment": null, "summary": "Predicting future video frames is essential for decision-making systems, yet\nRGB frames alone often lack the information needed to fully capture the\nunderlying complexities of the real world. To address this limitation, we\npropose a multi-modal framework for Synchronous Video Prediction (SyncVP) that\nincorporates complementary data modalities, enhancing the richness and accuracy\nof future predictions. SyncVP builds on pre-trained modality-specific diffusion\nmodels and introduces an efficient spatio-temporal cross-attention module to\nenable effective information sharing across modalities. We evaluate SyncVP on\nstandard benchmark datasets, such as Cityscapes and BAIR, using depth as an\nadditional modality. We furthermore demonstrate its generalization to other\nmodalities on SYNTHIA with semantic information and ERA5-Land with climate\ndata. Notably, SyncVP achieves state-of-the-art performance, even in scenarios\nwhere only one modality is present, demonstrating its robustness and potential\nfor a wide range of applications."}
{"id": "2503.18940", "pdf": "https://arxiv.org/pdf/2503.18940", "abs": "https://arxiv.org/abs/2503.18940", "authors": ["Ye Tian", "Xin Xia", "Yuxi Ren", "Shanchuan Lin", "Xing Wang", "Xuefeng Xiao", "Yunhai Tong", "Ling Yang", "Bin Cui"], "title": "Training-free Diffusion Acceleration with Bottleneck Sampling", "categories": ["cs.CV"], "comment": "Code Repo: https://github.com/tyfeld/Bottleneck-Sampling ,Project\n  Page: https://tyfeld.github.io/BottleneckSampling.github.io/", "summary": "Diffusion models have demonstrated remarkable capabilities in visual content\ngeneration but remain challenging to deploy due to their high computational\ncost during inference. This computational burden primarily arises from the\nquadratic complexity of self-attention with respect to image or video\nresolution. While existing acceleration methods often compromise output quality\nor necessitate costly retraining, we observe that most diffusion models are\npre-trained at lower resolutions, presenting an opportunity to exploit these\nlow-resolution priors for more efficient inference without degrading\nperformance. In this work, we introduce Bottleneck Sampling, a training-free\nframework that leverages low-resolution priors to reduce computational overhead\nwhile preserving output fidelity. Bottleneck Sampling follows a high-low-high\ndenoising workflow: it performs high-resolution denoising in the initial and\nfinal stages while operating at lower resolutions in intermediate steps. To\nmitigate aliasing and blurring artifacts, we further refine the resolution\ntransition points and adaptively shift the denoising timesteps at each stage.\nWe evaluate Bottleneck Sampling on both image and video generation tasks, where\nextensive experiments demonstrate that it accelerates inference by up to\n3$\\times$ for image generation and 2.5$\\times$ for video generation, all while\nmaintaining output quality comparable to the standard full-resolution sampling\nprocess across multiple evaluation metrics. Code is available at:\nhttps://github.com/tyfeld/Bottleneck-Sampling"}
{"id": "2503.18942", "pdf": "https://arxiv.org/pdf/2503.18942", "abs": "https://arxiv.org/abs/2503.18942", "authors": ["Fangfu Liu", "Hanyang Wang", "Yimo Cai", "Kaiyan Zhang", "Xiaohang Zhan", "Yueqi Duan"], "title": "Video-T1: Test-Time Scaling for Video Generation", "categories": ["cs.CV", "cs.AI"], "comment": "Project page: https://liuff19.github.io/Video-T1", "summary": "With the scale capability of increasing training data, model size, and\ncomputational cost, video generation has achieved impressive results in digital\ncreation, enabling users to express creativity across various domains.\nRecently, researchers in Large Language Models (LLMs) have expanded the scaling\nto test-time, which can significantly improve LLM performance by using more\ninference-time computation. Instead of scaling up video foundation models\nthrough expensive training costs, we explore the power of Test-Time Scaling\n(TTS) in video generation, aiming to answer the question: if a video generation\nmodel is allowed to use non-trivial amount of inference-time compute, how much\ncan it improve generation quality given a challenging text prompt. In this\nwork, we reinterpret the test-time scaling of video generation as a searching\nproblem to sample better trajectories from Gaussian noise space to the target\nvideo distribution. Specifically, we build the search space with test-time\nverifiers to provide feedback and heuristic algorithms to guide searching\nprocess. Given a text prompt, we first explore an intuitive linear search\nstrategy by increasing noise candidates at inference time. As full-step\ndenoising all frames simultaneously requires heavy test-time computation costs,\nwe further design a more efficient TTS method for video generation called\nTree-of-Frames (ToF) that adaptively expands and prunes video branches in an\nautoregressive manner. Extensive experiments on text-conditioned video\ngeneration benchmarks demonstrate that increasing test-time compute\nconsistently leads to significant improvements in the quality of videos.\nProject page: https://liuff19.github.io/Video-T1"}
{"id": "2503.18943", "pdf": "https://arxiv.org/pdf/2503.18943", "abs": "https://arxiv.org/abs/2503.18943", "authors": ["Mingze Xu", "Mingfei Gao", "Shiyu Li", "Jiasen Lu", "Zhe Gan", "Zhengfeng Lai", "Meng Cao", "Kai Kang", "Yinfei Yang", "Afshin Dehghan"], "title": "SlowFast-LLaVA-1.5: A Family of Token-Efficient Video Large Language Models for Long-Form Video Understanding", "categories": ["cs.CV"], "comment": "Technical report", "summary": "We introduce SlowFast-LLaVA-1.5 (abbreviated as SF-LLaVA-1.5), a family of\nvideo large language models (LLMs) offering a token-efficient solution for\nlong-form video understanding. This model family employs the two-stream\nSlowFast mechanism, enabling efficient modeling of long-range temporal context\nto meet the demand for lightweight, mobile-friendly Video LLMs. We provide\nmodels ranging from 1B to 7B parameters, optimized through a streamlined\ntraining pipeline and a high-quality data mixture composed of publicly\navailable datasets. Experimental results demonstrate that SF-LLaVA-1.5 achieves\ncompetitive performance on a wide range of video and image benchmarks, with\nrobust results across all model sizes. Notably, SF-LLaVA-1.5 achieves\nstate-of-the-art results in long-form video understanding (e.g., LongVideoBench\nand MLVU) and excels at small scales (1B and 3B) across various video\nbenchmarks."}
{"id": "2503.18944", "pdf": "https://arxiv.org/pdf/2503.18944", "abs": "https://arxiv.org/abs/2503.18944", "authors": ["Karim Abou Zeid", "Kadir Yilmaz", "Daan de Geus", "Alexander Hermans", "David Adrian", "Timm Linder", "Bastian Leibe"], "title": "DINO in the Room: Leveraging 2D Foundation Models for 3D Segmentation", "categories": ["cs.CV"], "comment": "Project page at https://vision.rwth-aachen.de/DITR", "summary": "Vision foundation models (VFMs) trained on large-scale image datasets provide\nhigh-quality features that have significantly advanced 2D visual recognition.\nHowever, their potential in 3D vision remains largely untapped, despite the\ncommon availability of 2D images alongside 3D point cloud datasets. While\nsignificant research has been dedicated to 2D-3D fusion, recent\nstate-of-the-art 3D methods predominantly focus on 3D data, leaving the\nintegration of VFMs into 3D models underexplored. In this work, we challenge\nthis trend by introducing DITR, a simple yet effective approach that extracts\n2D foundation model features, projects them to 3D, and finally injects them\ninto a 3D point cloud segmentation model. DITR achieves state-of-the-art\nresults on both indoor and outdoor 3D semantic segmentation benchmarks. To\nenable the use of VFMs even when images are unavailable during inference, we\nfurther propose to distill 2D foundation models into a 3D backbone as a\npretraining task. By initializing the 3D backbone with knowledge distilled from\n2D VFMs, we create a strong basis for downstream 3D segmentation tasks,\nultimately boosting performance across various datasets."}
{"id": "2503.18945", "pdf": "https://arxiv.org/pdf/2503.18945", "abs": "https://arxiv.org/abs/2503.18945", "authors": ["Aether Team", "Haoyi Zhu", "Yifan Wang", "Jianjun Zhou", "Wenzheng Chang", "Yang Zhou", "Zizun Li", "Junyi Chen", "Chunhua Shen", "Jiangmiao Pang", "Tong He"], "title": "Aether: Geometric-Aware Unified World Modeling", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "comment": "Project Page: https://aether-world.github.io/", "summary": "The integration of geometric reconstruction and generative modeling remains a\ncritical challenge in developing AI systems capable of human-like spatial\nreasoning. This paper proposes Aether, a unified framework that enables\ngeometry-aware reasoning in world models by jointly optimizing three core\ncapabilities: (1) 4D dynamic reconstruction, (2) action-conditioned video\nprediction, and (3) goal-conditioned visual planning. Through task-interleaved\nfeature learning, Aether achieves synergistic knowledge sharing across\nreconstruction, prediction, and planning objectives. Building upon video\ngeneration models, our framework demonstrates unprecedented synthetic-to-real\ngeneralization despite never observing real-world data during training.\nFurthermore, our approach achieves zero-shot generalization in both action\nfollowing and reconstruction tasks, thanks to its intrinsic geometric modeling.\nRemarkably, even without real-world data, its reconstruction performance far\nexceeds that of domain-specific models. Additionally, Aether leverages a\ngeometry-informed action space to seamlessly translate predictions into\nactions, enabling effective autonomous trajectory planning. We hope our work\ninspires the community to explore new frontiers in physically-reasonable world\nmodeling and its applications."}
{"id": "2503.18947", "pdf": "https://arxiv.org/pdf/2503.18947", "abs": "https://arxiv.org/abs/2503.18947", "authors": ["Jae Joong Lee", "Bedrich Benes", "Raymond A. Yeh"], "title": "Tuning-Free Amodal Segmentation via the Occlusion-Free Bias of Inpainting Models", "categories": ["cs.CV"], "comment": null, "summary": "Amodal segmentation aims to predict segmentation masks for both the visible\nand occluded regions of an object. Most existing works formulate this as a\nsupervised learning problem, requiring manually annotated amodal masks or\nsynthetic training data. Consequently, their performance depends on the quality\nof the datasets, which often lack diversity and scale. This work introduces a\ntuning-free approach that repurposes pretrained diffusion-based inpainting\nmodels for amodal segmentation. Our approach is motivated by the\n\"occlusion-free bias\" of inpainting models, i.e., the inpainted objects tend to\nbe complete objects without occlusions. Specifically, we reconstruct the\noccluded regions of an object via inpainting and then apply segmentation, all\nwithout additional training or fine-tuning. Experiments on five datasets\ndemonstrate the generalizability and robustness of our approach. On average,\nour approach achieves 5.3% more accurate masks over the state-of-the-art."}
{"id": "2503.18948", "pdf": "https://arxiv.org/pdf/2503.18948", "abs": "https://arxiv.org/abs/2503.18948", "authors": ["Ruixiao Dong", "Mengde Xu", "Zigang Geng", "Li Li", "Han Hu", "Shuyang Gu"], "title": "Equivariant Image Modeling", "categories": ["cs.CV"], "comment": null, "summary": "Current generative models, such as autoregressive and diffusion approaches,\ndecompose high-dimensional data distribution learning into a series of simpler\nsubtasks. However, inherent conflicts arise during the joint optimization of\nthese subtasks, and existing solutions fail to resolve such conflicts without\nsacrificing efficiency or scalability. We propose a novel equivariant image\nmodeling framework that inherently aligns optimization targets across subtasks\nby leveraging the translation invariance of natural visual signals. Our method\nintroduces (1) column-wise tokenization which enhances translational symmetry\nalong the horizontal axis, and (2) windowed causal attention which enforces\nconsistent contextual relationships across positions. Evaluated on\nclass-conditioned ImageNet generation at 256x256 resolution, our approach\nachieves performance comparable to state-of-the-art AR models while using fewer\ncomputational resources. Systematic analysis demonstrates that enhanced\nequivariance reduces inter-task conflicts, significantly improving zero-shot\ngeneralization and enabling ultra-long image synthesis. This work establishes\nthe first framework for task-aligned decomposition in generative modeling,\noffering insights into efficient parameter sharing and conflict-free\noptimization. The code and models are publicly available at\nhttps://github.com/drx-code/EquivariantModeling."}
{"id": "2503.18950", "pdf": "https://arxiv.org/pdf/2503.18950", "abs": "https://arxiv.org/abs/2503.18950", "authors": ["Taeksoo Kim", "Hanbyul Joo"], "title": "Target-Aware Video Diffusion Models", "categories": ["cs.CV"], "comment": "The project page is available at https://taeksuu.github.io/tavid/", "summary": "We present a target-aware video diffusion model that generates videos from an\ninput image in which an actor interacts with a specified target while\nperforming a desired action. The target is defined by a segmentation mask and\nthe desired action is described via a text prompt. Unlike existing controllable\nimage-to-video diffusion models that often rely on dense structural or motion\ncues to guide the actor's movements toward the target, our target-aware model\nrequires only a simple mask to indicate the target, leveraging the\ngeneralization capabilities of pretrained models to produce plausible actions.\nThis makes our method particularly effective for human-object interaction (HOI)\nscenarios, where providing precise action guidance is challenging, and further\nenables the use of video diffusion models for high-level action planning in\napplications such as robotics. We build our target-aware model by extending a\nbaseline model to incorporate the target mask as an additional input. To\nenforce target awareness, we introduce a special token that encodes the\ntarget's spatial information within the text prompt. We then fine-tune the\nmodel with our curated dataset using a novel cross-attention loss that aligns\nthe cross-attention maps associated with this token with the input target mask.\nTo further improve performance, we selectively apply this loss to the most\nsemantically relevant transformer blocks and attention regions. Experimental\nresults show that our target-aware model outperforms existing solutions in\ngenerating videos where actors interact accurately with the specified targets.\nWe further demonstrate its efficacy in two downstream applications: video\ncontent creation and zero-shot 3D HOI motion synthesis."}
{"id": "2503.17394", "pdf": "https://arxiv.org/pdf/2503.17394", "abs": "https://arxiv.org/abs/2503.17394", "authors": ["Kangrui Du", "Yuhang Wu", "Shikuang Deng", "Shi Gu"], "title": "Temporal Flexibility in Spiking Neural Networks: Towards Generalization Across Time Steps and Deployment Friendliness", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.NE"], "comment": "20 pages, ICLR 2025", "summary": "Spiking Neural Networks (SNNs), models inspired by neural mechanisms in the\nbrain, allow for energy-efficient implementation on neuromorphic hardware.\nHowever, SNNs trained with current direct training approaches are constrained\nto a specific time step. This \"temporal inflexibility\" 1) hinders SNNs'\ndeployment on time-step-free fully event-driven chips and 2) prevents\nenergy-performance balance based on dynamic inference time steps. In this\nstudy, we first explore the feasibility of training SNNs that generalize across\ndifferent time steps. We then introduce Mixed Time-step Training (MTT), a novel\nmethod that improves the temporal flexibility of SNNs, making SNNs adaptive to\ndiverse temporal structures. During each iteration of MTT, random time steps\nare assigned to different SNN stages, with spikes transmitted between stages\nvia communication modules. After training, the weights are deployed and\nevaluated on both time-stepped and fully event-driven platforms. Experimental\nresults show that models trained by MTT gain remarkable temporal flexibility,\nfriendliness for both event-driven and clock-driven deployment (nearly lossless\non N-MNIST and 10.1% higher than standard methods on CIFAR10-DVS), enhanced\nnetwork generalization, and near SOTA performance. To the best of our\nknowledge, this is the first work to report the results of large-scale SNN\ndeployment on fully event-driven scenarios."}
{"id": "2503.17436", "pdf": "https://arxiv.org/pdf/2503.17436", "abs": "https://arxiv.org/abs/2503.17436", "authors": ["Lars Kröger", "Cristian Cioflan", "Victor Kartsch", "Luca Benini"], "title": "On-Device Federated Continual Learning on RISC-V-based Ultra-Low-Power SoC for Intelligent Nano-Drone Swarms", "categories": ["cs.LG", "cs.CV", "cs.MA", "I.2.11; I.2.6; C.5.3; I.4.9"], "comment": "2 pages, 2 tables, 1 figure. Accepted as a poster at the RISC-V\n  Summit Europe 2025", "summary": "RISC-V-based architectures are paving the way for efficient On-Device\nLearning (ODL) in smart edge devices. When applied across multiple nodes, ODL\nenables the creation of intelligent sensor networks that preserve data privacy.\nHowever, developing ODL-capable, battery-operated embedded platforms presents\nsignificant challenges due to constrained computational resources and limited\ndevice lifetime, besides intrinsic learning issues such as catastrophic\nforgetting. We face these challenges by proposing a regularization-based\nOn-Device Federated Continual Learning algorithm tailored for multiple\nnano-drones performing face recognition tasks. We demonstrate our approach on a\nRISC-V-based 10-core ultra-low-power SoC, optimizing the ODL computational\nrequirements. We improve the classification accuracy by 24% over naive\nfine-tuning, requiring 178 ms per local epoch and 10.5 s per global epoch,\ndemonstrating the effectiveness of the architecture for this task."}
{"id": "2503.17477", "pdf": "https://arxiv.org/pdf/2503.17477", "abs": "https://arxiv.org/abs/2503.17477", "authors": ["Miguel López-Pérez", "Marco Miani", "Valery Naranjo", "Søren Hauberg", "Aasa Feragen"], "title": "Bayesian generative models can flag performance loss, bias, and out-of-distribution image content", "categories": ["cs.LG", "cs.CV", "stat.ML"], "comment": "Under review", "summary": "Generative models are popular for medical imaging tasks such as anomaly\ndetection, feature extraction, data visualization, or image generation. Since\nthey are parameterized by deep learning models, they are often sensitive to\ndistribution shifts and unreliable when applied to out-of-distribution data,\ncreating a risk of, e.g. underrepresentation bias. This behavior can be flagged\nusing uncertainty quantification methods for generative models, but their\navailability remains limited. We propose SLUG: A new UQ method for VAEs that\ncombines recent advances in Laplace approximations with stochastic trace\nestimators to scale gracefully with image dimensionality. We show that our UQ\nscore -- unlike the VAE's encoder variances -- correlates strongly with\nreconstruction error and racial underrepresentation bias for dermatological\nimages. We also show how pixel-wise uncertainty can detect out-of-distribution\nimage content such as ink, rulers, and patches, which is known to induce\nlearning shortcuts in predictive models."}
{"id": "2503.17482", "pdf": "https://arxiv.org/pdf/2503.17482", "abs": "https://arxiv.org/abs/2503.17482", "authors": ["Keyon Vafa", "Sarah Bentley", "Jon Kleinberg", "Sendhil Mullainathan"], "title": "What's Producible May Not Be Reachable: Measuring the Steerability of Generative Models", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.HC"], "comment": null, "summary": "How should we evaluate the quality of generative models? Many existing\nmetrics focus on a model's producibility, i.e. the quality and breadth of\noutputs it can generate. However, the actual value from using a generative\nmodel stems not just from what it can produce but whether a user with a\nspecific goal can produce an output that satisfies that goal. We refer to this\nproperty as steerability. In this paper, we first introduce a mathematical\nframework for evaluating steerability independently from producibility.\nSteerability is more challenging to evaluate than producibility because it\nrequires knowing a user's goals. We address this issue by creating a benchmark\ntask that relies on one key idea: sample an output from a generative model and\nask users to reproduce it. We implement this benchmark in a large-scale user\nstudy of text-to-image models and large language models. Despite the ability of\nthese models to produce high-quality outputs, they all perform poorly on\nsteerabilty. This suggests that we need to focus on improving the steerability\nof generative models. We show such improvements are indeed possible: through\nreinforcement learning techniques, we create an alternative steering mechanism\nfor image models that achieves more than 2x improvement on this benchmark."}
{"id": "2503.17489", "pdf": "https://arxiv.org/pdf/2503.17489", "abs": "https://arxiv.org/abs/2503.17489", "authors": ["Shu Pu", "Yaochen Wang", "Dongping Chen", "Yuhang Chen", "Guohao Wang", "Qi Qin", "Zhongyi Zhang", "Zhiyuan Zhang", "Zetong Zhou", "Shuang Gong", "Yi Gui", "Yao Wan", "Philip S. Yu"], "title": "Judge Anything: MLLM as a Judge Across Any Modality", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Evaluating generative foundation models on open-ended multimodal\nunderstanding (MMU) and generation (MMG) tasks across diverse modalities (e.g.,\nimages, audio, video) poses significant challenges due to the complexity of\ncross-modal interactions. To this end, the idea of utilizing Multimodal LLMs\n(MLLMs) as automated judges has emerged, with encouraging results in assessing\nvision-language understanding tasks. Moving further, this paper extends\nMLLM-as-a-Judge across modalities to a unified manner by introducing two\nbenchmarks, TaskAnything and JudgeAnything, to respectively evaluate the\noverall performance and judging capabilities of MLLMs across any-to-any\nmodality tasks. Specifically, TaskAnything evaluates the MMU and MMG\ncapabilities across 15 any-to-any modality categories, employing 1,500 queries\ncurated from well-established benchmarks. Furthermore, JudgeAnything evaluates\nthe judging capabilities of 5 advanced (e.g., GPT-4o and Gemini-2.0-Flash) from\nthe perspectives of Pair Comparison and Score Evaluation, providing a\nstandardized testbed that incorporates human judgments and detailed rubrics.\nOur extensive experiments reveal that while these MLLMs show promise in\nassessing MMU (i.e., achieving an average of 66.55% in Pair Comparison setting\nand 42.79% in Score Evaluation setting), they encounter significant challenges\nwith MMG tasks (i.e., averaging only 53.37% in Pair Comparison setting and\n30.05% in Score Evaluation setting), exposing cross-modality biases and\nhallucination issues. To address this, we present OmniArena, an automated\nplatform for evaluating omni-models and multimodal reward models. Our work\nhighlights the need for fairer evaluation protocols and stronger alignment with\nhuman preferences. The source code and dataset are publicly available at:\nhttps://urrealhero.github.io/judgeanythingweb/."}
{"id": "2503.17491", "pdf": "https://arxiv.org/pdf/2503.17491", "abs": "https://arxiv.org/abs/2503.17491", "authors": ["Emanuele Giacomini", "Luca Di Giammarino", "Lorenzo De Rebotti", "Giorgio Grisetti", "Martin R. Oswald"], "title": "Splat-LOAM: Gaussian Splatting LiDAR Odometry and Mapping", "categories": ["cs.RO", "cs.CV"], "comment": "submitted to ICCV 2025", "summary": "LiDARs provide accurate geometric measurements, making them valuable for\nego-motion estimation and reconstruction tasks. Although its success, managing\nan accurate and lightweight representation of the environment still poses\nchallenges. Both classic and NeRF-based solutions have to trade off accuracy\nover memory and processing times. In this work, we build on recent advancements\nin Gaussian Splatting methods to develop a novel LiDAR odometry and mapping\npipeline that exclusively relies on Gaussian primitives for its scene\nrepresentation. Leveraging spherical projection, we drive the refinement of the\nprimitives uniquely from LiDAR measurements. Experiments show that our approach\nmatches the current registration performance, while achieving SOTA results for\nmapping tasks with minimal GPU requirements. This efficiency makes it a strong\ncandidate for further exploration and potential adoption in real-time robotics\nestimation tasks."}
{"id": "2503.17540", "pdf": "https://arxiv.org/pdf/2503.17540", "abs": "https://arxiv.org/abs/2503.17540", "authors": ["Bin Xie", "Yan Yan", "Gady Agam"], "title": "MM-UNet: Meta Mamba UNet for Medical Image Segmentation", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "State Space Models (SSMs) have recently demonstrated outstanding performance\nin long-sequence modeling, particularly in natural language processing.\nHowever, their direct application to medical image segmentation poses several\nchallenges. SSMs, originally designed for 1D sequences, struggle with 3D\nspatial structures in medical images due to discontinuities introduced by\nflattening. Additionally, SSMs have difficulty fitting high-variance data,\nwhich is common in medical imaging.\n  In this paper, we analyze the intrinsic limitations of SSMs in medical image\nsegmentation and propose a unified U-shaped encoder-decoder architecture, Meta\nMamba UNet (MM-UNet), designed to leverage the advantages of SSMs while\nmitigating their drawbacks. MM-UNet incorporates hybrid modules that integrate\nSSMs within residual connections, reducing variance and improving performance.\nFurthermore, we introduce a novel bi-directional scan order strategy to\nalleviate discontinuities when processing medical images.\n  Extensive experiments on the AMOS2022 and Synapse datasets demonstrate the\nsuperiority of MM-UNet over state-of-the-art methods. MM-UNet achieves a Dice\nscore of 91.0% on AMOS2022, surpassing nnUNet by 3.2%, and a Dice score of\n87.1% on Synapse. These results confirm the effectiveness of integrating SSMs\nin medical image segmentation through architectural design optimizations."}
{"id": "2503.17543", "pdf": "https://arxiv.org/pdf/2503.17543", "abs": "https://arxiv.org/abs/2503.17543", "authors": ["Moein Heidari", "Afshin Bozorgpour", "AmirHossein Zarif-Fakharnia", "Dorit Merhof", "Ilker Hacihaliloglu"], "title": "Echo-E$^3$Net: Efficient Endo-Epi Spatio-Temporal Network for Ejection Fraction Estimation", "categories": ["eess.IV", "cs.CV"], "comment": "Submitted as a conference paper to MICCAI 2025", "summary": "Left ventricular ejection fraction (LVEF) is a critical metric for assessing\ncardiac function, widely used in diagnosing heart failure and guiding clinical\ndecisions. Despite its importance, conventional LVEF estimation remains\ntime-consuming and operator-dependent. Recent deep learning advancements have\nenhanced automation, yet many existing models are computationally demanding,\nhindering their feasibility for real-time clinical applications. Additionally,\nthe interplay between spatial and temporal features is crucial for accurate\nestimation but is often overlooked. In this work, we propose Echo-E$^3$Net, an\nefficient Endo-Epi spatio-temporal network tailored for LVEF estimation. Our\nmethod introduces the Endo-Epi Cardial Border Detector (E$^2$CBD) module, which\nenhances feature extraction by leveraging spatial and temporal landmark cues.\nComplementing this, the Endo-Epi Feature Aggregator (E$^2$FA) distills\nstatistical descriptors from backbone feature maps, refining the final EF\nprediction. These modules, along with a multi-component loss function tailored\nto align with the clinical definition of EF, collectively enhance\nspatial-temporal representation learning, ensuring robust and efficient EF\nestimation. We evaluate Echo-E$^3$Net on the EchoNet-Dynamic dataset, achieving\na RMSE of 5.15 and an R$^2$ score of 0.82, setting a new benchmark in\nefficiency with 6.8 million parameters and only 8.49G Flops. Our model operates\nwithout pre-training, data augmentation, or ensemble methods, making it\nwell-suited for real-time point-of-care ultrasound (PoCUS) applications. Our\nCode is publicly available\non~\\href{https://github.com/moeinheidari7829/Echo-E3Net}{\\textcolor{magenta}{GitHub}}."}
{"id": "2503.17551", "pdf": "https://arxiv.org/pdf/2503.17551", "abs": "https://arxiv.org/abs/2503.17551", "authors": ["Yu Sun", "Yin Li", "Ruixiao Sun", "Chunhui Liu", "Fangming Zhou", "Ze Jin", "Linjie Wang", "Xiang Shen", "Zhuolin Hao", "Hongyu Xiong"], "title": "Audio-Enhanced Vision-Language Modeling with Latent Space Broadening for High Quality Data Expansion", "categories": ["cs.MM", "cs.AI", "cs.CV", "cs.SD", "eess.AS"], "comment": null, "summary": "Transformer-based multimodal models are widely used in industrial-scale\nrecommendation, search, and advertising systems for content understanding and\nrelevance ranking. Enhancing labeled training data quality and cross-modal\nfusion significantly improves model performance, influencing key metrics such\nas quality view rates and ad revenue. High-quality annotations are crucial for\nadvancing content modeling, yet traditional statistical-based active learning\n(AL) methods face limitations: they struggle to detect overconfident\nmisclassifications and are less effective in distinguishing semantically\nsimilar items in deep neural networks. Additionally, audio information plays an\nincreasing role, especially in short-video platforms, yet most pre-trained\nmultimodal architectures primarily focus on text and images. While training\nfrom scratch across all three modalities is possible, it sacrifices the\nbenefits of leveraging existing pre-trained visual-language (VL) and audio\nmodels. To address these challenges, we propose kNN-based Latent Space\nBroadening (LSB) to enhance AL efficiency and Vision-Language Modeling with\nAudio Enhancement (VLMAE), a mid-fusion approach integrating audio into VL\nmodels. This system deployed in production systems, leading to significant\nbusiness gains."}
{"id": "2503.17564", "pdf": "https://arxiv.org/pdf/2503.17564", "abs": "https://arxiv.org/abs/2503.17564", "authors": ["Vishwesh Ramanathan", "Tony Xu", "Pushpak Pati", "Faruk Ahmed", "Maged Goubran", "Anne L. Martel"], "title": "ModalTune: Fine-Tuning Slide-Level Foundation Models with Multi-Modal Information for Multi-task Learning in Digital Pathology", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": null, "summary": "Prediction tasks in digital pathology are challenging due to the massive size\nof whole-slide images (WSIs) and the weak nature of training signals. Advances\nin computing, data availability, and self-supervised learning (SSL) have paved\nthe way for slide-level foundation models (SLFMs) that can improve prediction\ntasks in low-data regimes. However, working with these models is challenging,\nwith issues such as catastrophic forgetting during fine-tuning and\nunder-utilization of shared information between tasks and modalities. To\novercome these two challenges, we propose ModalTune, a novel fine-tuning\nframework which introduces the Modal Adapter to integrate new modalities\nwithout modifying SLFM weights. Additionally, we use large-language models\n(LLMs) to encode labels as text, capturing semantic relationships and enhancing\ngeneralization across multiple tasks and cancer types in a single training\nrecipe. ModalTune achieves state-of-the-art (SOTA) results against both\nuni-modal and multi-modal models across four cancer types, jointly improving\nsurvival and cancer subtype prediction while remaining competitive in\npan-cancer settings. Additionally, we show ModalTune is highly generalizable to\ntwo out-of-distribution (OOD) datasets. To our knowledge, this is the first\nunified fine-tuning framework for multi-modal, multi-task, and pan-cancer\nmodeling in digital pathology."}
{"id": "2503.17646", "pdf": "https://arxiv.org/pdf/2503.17646", "abs": "https://arxiv.org/abs/2503.17646", "authors": ["Yen Cheng Chang", "Jesse Codling", "Yiwen Dong", "Jiale Zhang", "Jiasi Chen", "Hae Young Noh", "Pei Zhang"], "title": "Leveraging Audio Representations for Vibration-Based Crowd Monitoring in Stadiums", "categories": ["cs.SD", "cs.CV"], "comment": null, "summary": "Crowd monitoring in sports stadiums is important to enhance public safety and\nimprove the audience experience. Existing approaches mainly rely on cameras and\nmicrophones, which can cause significant disturbances and often raise privacy\nconcerns. In this paper, we sense floor vibration, which provides a less\ndisruptive and more non-intrusive way of crowd sensing, to predict crowd\nbehavior. However, since the vibration-based crowd monitoring approach is newly\ndeveloped, one main challenge is the lack of training data due to sports\nstadiums being large public spaces with complex physical activities.\n  In this paper, we present ViLA (Vibration Leverage Audio), a vibration-based\nmethod that reduces the dependency on labeled data by pre-training with\nunlabeled cross-modality data. ViLA is first pre-trained on audio data in an\nunsupervised manner and then fine-tuned with a minimal amount of in-domain\nvibration data. By leveraging publicly available audio datasets, ViLA learns\nthe wave behaviors from audio and then adapts the representation to vibration,\nreducing the reliance on domain-specific vibration data. Our real-world\nexperiments demonstrate that pre-training the vibration model using publicly\navailable audio data (YouTube8M) achieved up to a 5.8x error reduction compared\nto the model without audio pre-training."}
{"id": "2503.17733", "pdf": "https://arxiv.org/pdf/2503.17733", "abs": "https://arxiv.org/abs/2503.17733", "authors": ["Bin Fu", "Jialin Li", "Bin Zhang", "Ruiping Wang", "Xilin Chen"], "title": "GS-LTS: 3D Gaussian Splatting-Based Adaptive Modeling for Long-Term Service Robots", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "3D Gaussian Splatting (3DGS) has garnered significant attention in robotics\nfor its explicit, high fidelity dense scene representation, demonstrating\nstrong potential for robotic applications. However, 3DGS-based methods in\nrobotics primarily focus on static scenes, with limited attention to the\ndynamic scene changes essential for long-term service robots. These robots\ndemand sustained task execution and efficient scene updates-challenges current\napproaches fail to meet. To address these limitations, we propose GS-LTS\n(Gaussian Splatting for Long-Term Service), a 3DGS-based system enabling indoor\nrobots to manage diverse tasks in dynamic environments over time. GS-LTS\ndetects scene changes (e.g., object addition or removal) via single-image\nchange detection, employs a rule-based policy to autonomously collect\nmulti-view observations, and efficiently updates the scene representation\nthrough Gaussian editing. Additionally, we propose a simulation-based benchmark\nthat automatically generates scene change data as compact configuration\nscripts, providing a standardized, user-friendly evaluation benchmark.\nExperimental results demonstrate GS-LTS's advantages in reconstruction,\nnavigation, and superior scene updates-faster and higher quality than the image\ntraining baseline-advancing 3DGS for long-term robotic operations. Code and\nbenchmark are available at: https://vipl-vsu.github.io/3DGS-LTS."}
{"id": "2503.17735", "pdf": "https://arxiv.org/pdf/2503.17735", "abs": "https://arxiv.org/abs/2503.17735", "authors": ["Zhiqiang Yuan", "Ting Zhang", "Ying Deng", "Jiapei Zhang", "Yeshuang Zhu", "Zexi Jia", "Jie Zhou", "Jinchao Zhang"], "title": "RDTF: Resource-efficient Dual-mask Training Framework for Multi-frame Animated Sticker Generation", "categories": ["cs.MM", "cs.CV"], "comment": null, "summary": "Recently, great progress has been made in video generation technology,\nattracting the widespread attention of scholars. To apply this technology to\ndownstream applications under resource-constrained conditions, researchers\nusually fine-tune the pre-trained models based on parameter-efficient tuning\nmethods such as Adapter or Lora. Although these methods can transfer the\nknowledge from the source domain to the target domain, fewer training\nparameters lead to poor fitting ability, and the knowledge from the source\ndomain may lead to the inference process deviating from the target domain. In\nthis paper, we argue that under constrained resources, training a smaller video\ngeneration model from scratch using only million-level samples can outperform\nparameter-efficient tuning on larger models in downstream applications: the\ncore lies in the effective utilization of data and curriculum strategy. Take\nanimated sticker generation (ASG) as a case study, we first construct a\ndiscrete frame generation network for stickers with low frame rates, ensuring\nthat its parameters meet the requirements of model training under constrained\nresources. In order to provide data support for models trained from scratch, we\ncome up with a dual-mask based data utilization strategy, which manages to\nimprove the availability and expand the diversity of limited data. To\nfacilitate convergence under dual-mask situation, we propose a\ndifficulty-adaptive curriculum learning method, which decomposes the sample\nentropy into static and adaptive components so as to obtain samples from easy\nto difficult. The experiment demonstrates that our resource-efficient dual-mask\ntraining framework is quantitatively and qualitatively superior to\nefficient-parameter tuning methods such as I2V-Adapter and SimDA, verifying the\nfeasibility of our method on downstream tasks under constrained resources. Code\nwill be available."}
{"id": "2503.17777", "pdf": "https://arxiv.org/pdf/2503.17777", "abs": "https://arxiv.org/abs/2503.17777", "authors": ["Lei Guo", "Wei Chen", "Yuxuan Sun", "Bo Ai", "Nikolaos Pappas", "Tony Quek"], "title": "Hierarchy-Aware and Channel-Adaptive Semantic Communication for Bandwidth-Limited Data Fusion", "categories": ["eess.IV", "cs.CV"], "comment": "Accepted by the WCL", "summary": "Obtaining high-resolution hyperspectral images (HR-HSI) is costly and\ndata-intensive, making it necessary to fuse low-resolution hyperspectral images\n(LR-HSI) with high-resolution RGB images (HR-RGB) for practical applications.\nHowever, traditional fusion techniques, which integrate detailed information\ninto the reconstruction, significantly increase bandwidth consumption compared\nto directly transmitting raw data. To overcome these challenges, we propose a\nhierarchy-aware and channel-adaptive semantic communication approach for\nbandwidth-limited data fusion. A hierarchical correlation module is proposed to\npreserve both the overall structural information and the details of the image\nrequired for super-resolution. This module efficiently combines deep semantic\nand shallow features from LR-HSI and HR-RGB. To further reduce bandwidth usage\nwhile preserving reconstruction quality, a channel-adaptive attention mechanism\nbased on Transformer is proposed to dynamically integrate and transmit the deep\nand shallow features, enabling efficient data transmission and high-quality\nHR-HSI reconstruction. Experimental results on the CAVE and Washington DC Mall\ndatasets demonstrate that our method outperforms single-source transmission,\nachieving up to a 2 dB improvement in peak signal-to-noise ratio (PSNR).\nAdditionally, it reduces bandwidth consumption by two-thirds, confirming its\neffectiveness in bandwidth-constrained environments for HR-HSI reconstruction\ntasks."}
{"id": "2503.17786", "pdf": "https://arxiv.org/pdf/2503.17786", "abs": "https://arxiv.org/abs/2503.17786", "authors": ["Tommaso Di Noto", "Sofyan Jankowski", "Francesco Puccinelli", "Guillaume Marie", "Sebastien Tourbier", "Yasser Aleman-Gomez", "Oscar Esteban", "Ricardo Corredor-Jerez", "Guillaume Saliou", "Patric Hagmann", "Meritxell Bach Cuadra", "Jonas Richiardi"], "title": "Assessing workflow impact and clinical utility of AI-assisted brain aneurysm detection: a multi-reader study", "categories": ["eess.IV", "cs.CV"], "comment": "Paper under review with a Journal in the medical imaging field", "summary": "Despite the plethora of AI-based algorithms developed for anomaly detection\nin radiology, subsequent integration into clinical setting is rarely evaluated.\nIn this work, we assess the applicability and utility of an AI-based model for\nbrain aneurysm detection comparing the performance of two readers with\ndifferent levels of experience (2 and 13 years). We aim to answer the following\nquestions: 1) Do the readers improve their performance when assisted by the AI\nalgorithm? 2) How much does the AI algorithm impact routine clinical workflow?\nWe reuse and enlarge our open-access, Time-Of-Flight Magnetic Resonance\nAngiography dataset (N=460). We use 360 subjects for training/validating our\nalgorithm and 100 as unseen test set for the reading session. Even though our\nmodel reaches state-of-the-art results on the test set (sensitivity=74%, false\npositive rate=1.6), we show that neither the junior nor the senior reader\nsignificantly increase their sensitivity (p=0.59, p=1, respectively). In\naddition, we find that reading time for both readers is significantly higher in\nthe \"AI-assisted\" setting than in the \"Unassisted\" (+15 seconds, on average;\np=3x10^(-4) junior, p=3x10^(-5) senior). The confidence reported by the readers\nis unchanged across the two settings, indicating that the AI assistance does\nnot influence the certainty of the diagnosis. Our findings highlight the\nimportance of clinical validation of AI algorithms in a clinical setting\ninvolving radiologists. This study should serve as a reminder to the community\nto always examine the real-word effectiveness and workflow impact of proposed\nalgorithms."}
{"id": "2503.17804", "pdf": "https://arxiv.org/pdf/2503.17804", "abs": "https://arxiv.org/abs/2503.17804", "authors": ["Xing Xie", "Jiawei Liu", "Huijie Fan", "Zhi Han", "Yandong Tang", "Liangqiong Qu"], "title": "DVG-Diffusion: Dual-View Guided Diffusion Model for CT Reconstruction from X-Rays", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Directly reconstructing 3D CT volume from few-view 2D X-rays using an\nend-to-end deep learning network is a challenging task, as X-ray images are\nmerely projection views of the 3D CT volume. In this work, we facilitate\ncomplex 2D X-ray image to 3D CT mapping by incorporating new view synthesis,\nand reduce the learning difficulty through view-guided feature alignment.\nSpecifically, we propose a dual-view guided diffusion model (DVG-Diffusion),\nwhich couples a real input X-ray view and a synthesized new X-ray view to\njointly guide CT reconstruction. First, a novel view parameter-guided encoder\ncaptures features from X-rays that are spatially aligned with CT. Next, we\nconcatenate the extracted dual-view features as conditions for the latent\ndiffusion model to learn and refine the CT latent representation. Finally, the\nCT latent representation is decoded into a CT volume in pixel space. By\nincorporating view parameter guided encoding and dual-view guided CT\nreconstruction, our DVG-Diffusion can achieve an effective balance between high\nfidelity and perceptual quality for CT reconstruction. Experimental results\ndemonstrate our method outperforms state-of-the-art methods. Based on\nexperiments, the comprehensive analysis and discussions for views and\nreconstruction are also presented."}
{"id": "2503.17831", "pdf": "https://arxiv.org/pdf/2503.17831", "abs": "https://arxiv.org/abs/2503.17831", "authors": ["Qingshan Hou", "Meng Wang", "Peng Cao", "Zou Ke", "Xiaoli Liu", "Huazhu Fu", "Osmar R. Zaiane"], "title": "FundusGAN: A Hierarchical Feature-Aware Generative Framework for High-Fidelity Fundus Image Generation", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Recent advancements in ophthalmology foundation models such as RetFound have\ndemonstrated remarkable diagnostic capabilities but require massive datasets\nfor effective pre-training, creating significant barriers for development and\ndeployment. To address this critical challenge, we propose FundusGAN, a novel\nhierarchical feature-aware generative framework specifically designed for\nhigh-fidelity fundus image synthesis. Our approach leverages a Feature Pyramid\nNetwork within its encoder to comprehensively extract multi-scale information,\ncapturing both large anatomical structures and subtle pathological features.\nThe framework incorporates a modified StyleGAN-based generator with dilated\nconvolutions and strategic upsampling adjustments to preserve critical retinal\nstructures while enhancing pathological detail representation. Comprehensive\nevaluations on the DDR, DRIVE, and IDRiD datasets demonstrate that FundusGAN\nconsistently outperforms state-of-the-art methods across multiple metrics\n(SSIM: 0.8863, FID: 54.2, KID: 0.0436 on DDR). Furthermore, disease\nclassification experiments reveal that augmenting training data with\nFundusGAN-generated images significantly improves diagnostic accuracy across\nmultiple CNN architectures (up to 6.49\\% improvement with ResNet50). These\nresults establish FundusGAN as a valuable foundation model component that\neffectively addresses data scarcity challenges in ophthalmological AI research,\nenabling more robust and generalizable diagnostic systems while reducing\ndependency on large-scale clinical data collection."}
{"id": "2503.17896", "pdf": "https://arxiv.org/pdf/2503.17896", "abs": "https://arxiv.org/abs/2503.17896", "authors": ["Hong Zheng", "Yucheng Chen", "Nan Mu", "Xiaoning Li"], "title": "Multi-Disease-Aware Training Strategy for Cardiac MR Image Segmentation", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Accurate segmentation of the ventricles from cardiac magnetic resonance\nimages (CMRIs) is crucial for enhancing the diagnosis and analysis of heart\nconditions. Deep learning-based segmentation methods have recently garnered\nsignificant attention due to their impressive performance. However, these\nsegmentation methods are typically good at partitioning regularly shaped\norgans, such as the left ventricle (LV) and the myocardium (MYO), whereas they\nperform poorly on irregularly shaped organs, such as the right ventricle (RV).\nIn this study, we argue that this limitation of segmentation models stems from\ntheir insufficient generalization ability to address the distribution shift of\nsegmentation targets across slices, cardiac phases, and disease conditions. To\novercome this issue, we present a Multi-Disease-Aware Training Strategy (MTS)\nand restructure the introduced CMRI datasets into multi-disease datasets.\nAdditionally, we propose a specialized data processing technique for\npreprocessing input images to support the MTS. To validate the effectiveness of\nour method, we performed control group experiments and cross-validation tests.\nThe experimental results show that (1) network models trained using our\nproposed strategy achieved superior segmentation performance, particularly in\nRV segmentation, and (2) these networks exhibited robust performance even when\napplied to data from unknown diseases."}
{"id": "2503.17897", "pdf": "https://arxiv.org/pdf/2503.17897", "abs": "https://arxiv.org/abs/2503.17897", "authors": ["Chenxiao Hu", "Meng Gai", "Guoping Wang", "Sheng Li"], "title": "Real-time Global Illumination for Dynamic 3D Gaussian Scenes", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "We present a real-time global illumination approach along with a pipeline for\ndynamic 3D Gaussian models and meshes. Building on a formulated surface light\ntransport model for 3D Gaussians, we address key performance challenges with a\nfast compound stochastic ray-tracing algorithm and an optimized 3D Gaussian\nrasterizer. Our pipeline integrates multiple real-time techniques to accelerate\nperformance and achieve high-quality lighting effects. Our approach enables\nreal-time rendering of dynamic scenes with interactively editable materials and\ndynamic lighting of diverse multi-lights settings, capturing mutual\nmulti-bounce light transport (indirect illumination) between 3D Gaussians and\nmesh. Additionally, we present a real-time renderer with an interactive user\ninterface, validating our approach and demonstrating its practicality and high\nefficiency with over 40 fps in scenes including both 3D Gaussians and mesh.\nFurthermore, our work highlights the potential of 3D Gaussians in real-time\napplications with dynamic lighting, offering insights into performance and\noptimization."}
{"id": "2503.17914", "pdf": "https://arxiv.org/pdf/2503.17914", "abs": "https://arxiv.org/abs/2503.17914", "authors": ["Jianjian Yin", "Tao Chen", "Gensheng Pei", "Yazhou Yao", "Liqiang Nie", "Xiansheng Hua"], "title": "Semi-supervised Semantic Segmentation with Multi-Constraint Consistency Learning", "categories": ["cs.MM", "cs.CV"], "comment": "accepted by IEEE Transactions on Multimedia", "summary": "Consistency regularization has prevailed in semi-supervised semantic\nsegmentation and achieved promising performance. However, existing methods\ntypically concentrate on enhancing the Image-augmentation based Prediction\nconsistency and optimizing the segmentation network as a whole, resulting in\ninsufficient utilization of potential supervisory information. In this paper,\nwe propose a Multi-Constraint Consistency Learning (MCCL) approach to\nfacilitate the staged enhancement of the encoder and decoder. Specifically, we\nfirst design a feature knowledge alignment (FKA) strategy to promote the\nfeature consistency learning of the encoder from image-augmentation. Our FKA\nencourages the encoder to derive consistent features for strongly and weakly\naugmented views from the perspectives of point-to-point alignment and\nprototype-based intra-class compactness. Moreover, we propose a self-adaptive\nintervention (SAI) module to increase the discrepancy of aligned intermediate\nfeature representations, promoting Feature-perturbation based Prediction\nconsistency learning. Self-adaptive feature masking and noise injection are\ndesigned in an instance-specific manner to perturb the features for robust\nlearning of the decoder. Experimental results on Pascal VOC2012 and Cityscapes\ndatasets demonstrate that our proposed MCCL achieves new state-of-the-art\nperformance. The source code and models are made available at\nhttps://github.com/NUST-Machine-Intelligence-Laboratory/MCCL."}
{"id": "2503.17915", "pdf": "https://arxiv.org/pdf/2503.17915", "abs": "https://arxiv.org/abs/2503.17915", "authors": ["Jiachen Jiang", "Tianyu Ding", "Ke Zhang", "Jinxin Zhou", "Tianyi Chen", "Ilya Zharkov", "Zhihui Zhu", "Luming Liang"], "title": "Cat-AIR: Content and Task-Aware All-in-One Image Restoration", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "All-in-one image restoration seeks to recover high-quality images from\nvarious types of degradation using a single model, without prior knowledge of\nthe corruption source. However, existing methods often struggle to effectively\nand efficiently handle multiple degradation types. We present Cat-AIR, a novel\n\\textbf{C}ontent \\textbf{A}nd \\textbf{T}ask-aware framework for\n\\textbf{A}ll-in-one \\textbf{I}mage \\textbf{R}estoration. Cat-AIR incorporates\nan alternating spatial-channel attention mechanism that adaptively balances the\nlocal and global information for different tasks. Specifically, we introduce\ncross-layer channel attentions and cross-feature spatial attentions that\nallocate computations based on content and task complexity. Furthermore, we\npropose a smooth learning strategy that allows for seamless adaptation to new\nrestoration tasks while maintaining performance on existing ones. Extensive\nexperiments demonstrate that Cat-AIR achieves state-of-the-art results across a\nwide range of restoration tasks, requiring fewer FLOPs than previous methods,\nestablishing new benchmarks for efficient all-in-one image restoration."}
{"id": "2503.17970", "pdf": "https://arxiv.org/pdf/2503.17970", "abs": "https://arxiv.org/abs/2503.17970", "authors": ["Yang Luo", "Shiru Wang", "Jun Liu", "Jiaxuan Xiao", "Rundong Xue", "Zeyu Zhang", "Hao Zhang", "Yu Lu", "Yang Zhao", "Yutong Xie"], "title": "PathoHR: Breast Cancer Survival Prediction on High-Resolution Pathological Images", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Breast cancer survival prediction in computational pathology presents a\nremarkable challenge due to tumor heterogeneity. For instance, different\nregions of the same tumor in the pathology image can show distinct\nmorphological and molecular characteristics. This makes it difficult to extract\nrepresentative features from whole slide images (WSIs) that truly reflect the\ntumor's aggressive potential and likely survival outcomes. In this paper, we\npresent PathoHR, a novel pipeline for accurate breast cancer survival\nprediction that enhances any size of pathological images to enable more\neffective feature learning. Our approach entails (1) the incorporation of a\nplug-and-play high-resolution Vision Transformer (ViT) to enhance patch-wise\nWSI representation, enabling more detailed and comprehensive feature\nextraction, (2) the systematic evaluation of multiple advanced similarity\nmetrics for comparing WSI-extracted features, optimizing the representation\nlearning process to better capture tumor characteristics, (3) the demonstration\nthat smaller image patches enhanced follow the proposed pipeline can achieve\nequivalent or superior prediction accuracy compared to raw larger patches,\nwhile significantly reducing computational overhead. Experimental findings\nvalid that PathoHR provides the potential way of integrating enhanced image\nresolution with optimized feature learning to advance computational pathology,\noffering a promising direction for more accurate and efficient breast cancer\nsurvival prediction. Code will be available at\nhttps://github.com/AIGeeksGroup/PathoHR."}
{"id": "2503.17987", "pdf": "https://arxiv.org/pdf/2503.17987", "abs": "https://arxiv.org/abs/2503.17987", "authors": ["Chenyu Zhang", "Yiwen Ma", "Lanjun Wang", "Wenhui Li", "Yi Tu", "An-An Liu"], "title": "Metaphor-based Jailbreaking Attacks on Text-to-Image Models", "categories": ["cs.CR", "cs.AI", "cs.CV"], "comment": "13 page3, 4 figures. This paper includes model-generated content that\n  may contain offensive or distressing material", "summary": "To mitigate misuse, text-to-image~(T2I) models commonly incorporate safety\nfilters to prevent the generation of sensitive images. Unfortunately, recent\njailbreaking attack methods use LLMs to generate adversarial prompts that\neffectively bypass safety filters while generating sensitive images, revealing\nthe safety vulnerabilities within the T2I model. However, existing LLM-based\nattack methods lack explicit guidance, relying on substantial queries to\nachieve a successful attack, which limits their practicality in real-world\nscenarios. In this work, we introduce \\textbf{MJA}, a \\textbf{m}etaphor-based\n\\textbf{j}ailbreaking \\textbf{a}ttack method inspired by the Taboo game, aiming\nto balance the attack effectiveness and query efficiency by generating\nmetaphor-based adversarial prompts. Specifically, MJA consists of two modules:\nan LLM-based multi-agent generation module~(MLAG) and an adversarial prompt\noptimization module~(APO). MLAG decomposes the generation of metaphor-based\nadversarial prompts into three subtasks: metaphor retrieval, context matching,\nand adversarial prompt generation. Subsequently, MLAG coordinates three\nLLM-based agents to generate diverse adversarial prompts by exploring various\nmetaphors and contexts. To enhance the attack efficiency, APO first trains a\nsurrogate model to predict the attack results of adversarial prompts and then\ndesigns an acquisition strategy to adaptively identify optimal adversarial\nprompts. Experiments demonstrate that MJA achieves better attack effectiveness\nwhile requiring fewer queries compared to baseline methods. Moreover, our\nadversarial prompts exhibit strong transferability across various open-source\nand commercial T2I models. \\textcolor{red}{This paper includes model-generated\ncontent that may contain offensive or distressing material.}"}
{"id": "2503.18032", "pdf": "https://arxiv.org/pdf/2503.18032", "abs": "https://arxiv.org/abs/2503.18032", "authors": ["Emma Coletta", "Davide Salvi", "Viola Negroni", "Daniele Ugo Leonzio", "Paolo Bestagini"], "title": "Anomaly Detection and Localization for Speech Deepfakes via Feature Pyramid Matching", "categories": ["cs.SD", "cs.CV", "cs.MM"], "comment": null, "summary": "The rise of AI-driven generative models has enabled the creation of highly\nrealistic speech deepfakes - synthetic audio signals that can imitate target\nspeakers' voices - raising critical security concerns. Existing methods for\ndetecting speech deepfakes primarily rely on supervised learning, which suffers\nfrom two critical limitations: limited generalization to unseen synthesis\ntechniques and a lack of explainability. In this paper, we address these issues\nby introducing a novel interpretable one-class detection framework, which\nreframes speech deepfake detection as an anomaly detection task. Our model is\ntrained exclusively on real speech to characterize its distribution, enabling\nthe classification of out-of-distribution samples as synthetically generated.\nAdditionally, our framework produces interpretable anomaly maps during\ninference, highlighting anomalous regions across both time and frequency\ndomains. This is done through a Student-Teacher Feature Pyramid Matching\nsystem, enhanced with Discrepancy Scaling to improve generalization\ncapabilities across unseen data distributions. Extensive evaluations\ndemonstrate the superior performance of our approach compared to the considered\nbaselines, validating the effectiveness of framing speech deepfake detection as\nan anomaly detection problem."}
{"id": "2503.18038", "pdf": "https://arxiv.org/pdf/2503.18038", "abs": "https://arxiv.org/abs/2503.18038", "authors": ["Wei-Na Li", "Yi Zhou", "Jiatai Chen", "Hongjie Ou", "XiangSheng Xie"], "title": "Multiple-Particle Autofocusing Algorithm Using Axial Resolution and Morphological Analyses Based on Digital Holography", "categories": ["eess.SP", "cs.CV", "eess.IV"], "comment": null, "summary": "We propose an autofocusing algorithm to obtain, relatively accurately, the 3D\nposition of each particle, particularly its axial location, and particle number\nof a dense transparent particle solution via its hologram. First, morphological\nanalyses and constrained intensity are used on raw reconstructed images to\nobtain information on candidate focused particles. Second, axial resolution is\nused to obtain the real focused particles. Based on the mean intensity and\nequivalent diameter of each candidate focused particle, all focused particles\nare eventually secured. Our proposed method can rapidly provide relatively\naccurate ground-truth axial positions to solve the autofocusing problem that\noccurs with dense particles."}
{"id": "2503.18064", "pdf": "https://arxiv.org/pdf/2503.18064", "abs": "https://arxiv.org/abs/2503.18064", "authors": ["Xiaoming Qi", "Jingyang Zhang", "Huazhu Fu", "Guanyu Yang", "Shuo Li", "Yueming Jin"], "title": "Dynamic Allocation Hypernetwork with Adaptive Model Recalibration for FCL", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Federated continual learning (FCL) offers an emerging pattern to facilitate\nthe applicability of federated learning (FL) in real-world scenarios, where\ntasks evolve dynamically and asynchronously across clients, especially in\nmedical scenario. Existing server-side FCL methods in nature domain construct a\ncontinually learnable server model by client aggregation on all-involved tasks.\nHowever, they are challenged by: (1) Catastrophic forgetting for previously\nlearned tasks, leading to error accumulation in server model, making it\ndifficult to sustain comprehensive knowledge across all tasks. (2) Biased\noptimization due to asynchronous tasks handled across different clients,\nleading to the collision of optimization targets of different clients at the\nsame time steps. In this work, we take the first step to propose a novel\nserver-side FCL pattern in medical domain, Dynamic Allocation Hypernetwork with\nadaptive model recalibration (\\textbf{FedDAH}). It is to facilitate\ncollaborative learning under the distinct and dynamic task streams across\nclients. To alleviate the catastrophic forgetting, we propose a dynamic\nallocation hypernetwork (DAHyper) where a continually updated hypernetwork is\ndesigned to manage the mapping between task identities and their associated\nmodel parameters, enabling the dynamic allocation of the model across clients.\nFor the biased optimization, we introduce a novel adaptive model recalibration\n(AMR) to incorporate the candidate changes of historical models into current\nserver updates, and assign weights to identical tasks across different time\nsteps based on the similarity for continual optimization. Extensive experiments\non the AMOS dataset demonstrate the superiority of our FedDAH to other FCL\nmethods on sites with different task streams. The code is\navailable:https://github.com/jinlab-imvr/FedDAH."}
{"id": "2503.18074", "pdf": "https://arxiv.org/pdf/2503.18074", "abs": "https://arxiv.org/abs/2503.18074", "authors": ["Yu Mao", "Jun Wang", "Nan Guan", "Chun Jason Xue"], "title": "WISE: A Framework for Gigapixel Whole-Slide-Image Lossless Compression", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Whole-Slide Images (WSIs) have revolutionized medical analysis by presenting\nhigh-resolution images of the whole tissue slide. Despite avoiding the physical\nstorage of the slides, WSIs require considerable data volume, which makes the\nstorage and maintenance of WSI records costly and unsustainable. To this end,\nthis work presents the first investigation of lossless compression of WSI\nimages. Interestingly, we find that most existing compression methods fail to\ncompress the WSI images effectively. Furthermore, our analysis reveals that the\nfailure of existing compressors is mainly due to information irregularity in\nWSI images. To resolve this issue, we developed a simple yet effective lossless\ncompressor called WISE, specifically designed for WSI images. WISE employs a\nhierarchical encoding strategy to extract effective bits, reducing the entropy\nof the image and then adopting a dictionary-based method to handle the\nirregular frequency patterns. Through extensive experiments, we show that WISE\ncan effectively compress the gigapixel WSI images to 36 times on average and up\nto 136 times."}
{"id": "2503.18108", "pdf": "https://arxiv.org/pdf/2503.18108", "abs": "https://arxiv.org/abs/2503.18108", "authors": ["Junhao Ge", "Zuhong Liu", "Longteng Fan", "Yifan Jiang", "Jiaqi Su", "Yiming Li", "Zhejun Zhang", "Siheng Chen"], "title": "Unraveling the Effects of Synthetic Data on End-to-End Autonomous Driving", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "End-to-end (E2E) autonomous driving (AD) models require diverse, high-quality\ndata to perform well across various driving scenarios. However, collecting\nlarge-scale real-world data is expensive and time-consuming, making\nhigh-fidelity synthetic data essential for enhancing data diversity and model\nrobustness. Existing driving simulators for synthetic data generation have\nsignificant limitations: game-engine-based simulators struggle to produce\nrealistic sensor data, while NeRF-based and diffusion-based methods face\nefficiency challenges. Additionally, recent simulators designed for closed-loop\nevaluation provide limited interaction with other vehicles, failing to simulate\ncomplex real-world traffic dynamics. To address these issues, we introduce\nSceneCrafter, a realistic, interactive, and efficient AD simulator based on 3D\nGaussian Splatting (3DGS). SceneCrafter not only efficiently generates\nrealistic driving logs across diverse traffic scenarios but also enables robust\nclosed-loop evaluation of end-to-end models. Experimental results demonstrate\nthat SceneCrafter serves as both a reliable evaluation platform and a efficient\ndata generator that significantly improves end-to-end model generalization."}
{"id": "2503.18151", "pdf": "https://arxiv.org/pdf/2503.18151", "abs": "https://arxiv.org/abs/2503.18151", "authors": ["Siwon Kim", "Wooyung Yun", "Jeongbin Oh", "Soomok Lee"], "title": "Efficient Deep Learning Approaches for Processing Ultra-Widefield Retinal Imaging", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Deep learning has emerged as the predominant solution for classifying medical\nimages. We intend to apply these developments to the ultra-widefield (UWF)\nretinal imaging dataset. Since UWF images can accurately diagnose various\nretina diseases, it is very important to clas sify them accurately and prevent\nthem with early treatment. However, processing images manually is\ntime-consuming and labor-intensive, and there are two challenges to automating\nthis process. First, high perfor mance usually requires high computational\nresources. Artificial intelli gence medical technology is better suited for\nplaces with limited medical resources, but using high-performance processing\nunits in such environ ments is challenging. Second, the problem of the accuracy\nof colour fun dus photography (CFP) methods. In general, the UWF method\nprovides more information for retinal diagnosis than the CFP method, but most\nof the research has been conducted based on the CFP method. Thus, we\ndemonstrate that these problems can be efficiently addressed in low performance\nunits using methods such as strategic data augmentation and model ensembles,\nwhich balance performance and computational re sources while utilizing UWF\nimages."}
{"id": "2503.18162", "pdf": "https://arxiv.org/pdf/2503.18162", "abs": "https://arxiv.org/abs/2503.18162", "authors": ["Hui Xue", "Sarah M. Hooper", "Iain Pierce", "Rhodri H. Davies", "John Stairs", "Joseph Naegele", "Adrienne E. Campbell-Washburn", "Charlotte Manisty", "James C. Moon", "Thomas A. Treibel", "Peter Kellman", "Michael S. Hansen"], "title": "SNRAware: Improved Deep Learning MRI Denoising with SNR Unit Training and G-factor Map Augmentation", "categories": ["physics.med-ph", "cs.AI", "cs.CV", "eess.IV"], "comment": null, "summary": "To develop and evaluate a new deep learning MR denoising method that\nleverages quantitative noise distribution information from the reconstruction\nprocess to improve denoising performance and generalization.\n  This retrospective study trained 14 different transformer and convolutional\nmodels with two backbone architectures on a large dataset of 2,885,236 images\nfrom 96,605 cardiac retro-gated cine complex series acquired at 3T. The\nproposed training scheme, termed SNRAware, leverages knowledge of the MRI\nreconstruction process to improve denoising performance by simulating large,\nhigh quality, and diverse synthetic datasets, and providing quantitative\ninformation about the noise distribution to the model. In-distribution testing\nwas performed on a hold-out dataset of 3000 samples with performance measured\nusing PSNR and SSIM, with ablation comparison without the noise augmentation.\nOut-of-distribution tests were conducted on cardiac real-time cine, first-pass\ncardiac perfusion, and neuro and spine MRI, all acquired at 1.5T, to test model\ngeneralization across imaging sequences, dynamically changing contrast,\ndifferent anatomies, and field strengths. The best model found in the\nin-distribution test generalized well to out-of-distribution samples,\ndelivering 6.5x and 2.9x CNR improvement for real-time cine and perfusion\nimaging, respectively. Further, a model trained with 100% cardiac cine data\ngeneralized well to a T1 MPRAGE neuro 3D scan and T2 TSE spine MRI."}
{"id": "2503.18225", "pdf": "https://arxiv.org/pdf/2503.18225", "abs": "https://arxiv.org/abs/2503.18225", "authors": ["Massimo Bini", "Leander Girrbach", "Zeynep Akata"], "title": "Decoupling Angles and Strength in Low-rank Adaptation", "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": "ICLR 2025", "summary": "Parameter-Efficient FineTuning (PEFT) methods have recently gained\nsignificant popularity thanks to the widespread availability of large-scale\npretrained models. These methods allow for quick adaptation to downstream tasks\nwith minimal computational cost. However, popular finetuning methods such as\nLoRA exhibit limited robustness when it comes to hyperparameter choices or\nextended training regimes, preventing optimal out-of-the-box performance. In\ncontrast, bounded approaches, such as ETHER, provide greater robustness but are\nlimited to extremely low-rank adaptations and fixed-strength transformations,\nreducing their adaptation expressive power. In this work, we propose Decoupled\nLow-rank Adaptation (DeLoRA), a novel finetuning method that normalizes and\nscales learnable low-rank matrices. By bounding the distance of the\ntransformation, DeLoRA effectively decouples the angular learning from the\nadaptation strength, enhancing robustness without compromising performance.\nThrough evaluations on subject-driven image generation, natural language\nunderstanding, and instruction tuning, we show that DeLoRA matches or surpasses\nperformance of competing PEFT methods, while exhibiting stronger robustness.\nCode is available at https://github.com/ExplainableML/DeLoRA."}
{"id": "2503.18246", "pdf": "https://arxiv.org/pdf/2503.18246", "abs": "https://arxiv.org/abs/2503.18246", "authors": ["Feiran Wang", "Bin Duan", "Jiachen Tao", "Nikhil Sharma", "Dawen Cai", "Yan Yan"], "title": "ZECO: ZeroFusion Guided 3D MRI Conditional Generation", "categories": ["eess.IV", "cs.CV"], "comment": "Project page: \\url{https://brack-wang.github.io/ZECO_web/}; Github\n  Code: \\url{https://github.com/Brack-Wang/ZECO}", "summary": "Medical image segmentation is crucial for enhancing diagnostic accuracy and\ntreatment planning in Magnetic Resonance Imaging (MRI). However, acquiring\nprecise lesion masks for segmentation model training demands specialized\nexpertise and significant time investment, leading to a small dataset scale in\nclinical practice. In this paper, we present ZECO, a ZeroFusion guided 3D MRI\nconditional generation framework that extracts, compresses, and generates\nhigh-fidelity MRI images with corresponding 3D segmentation masks to mitigate\ndata scarcity. To effectively capture inter-slice relationships within volumes,\nwe introduce a Spatial Transformation Module that encodes MRI images into a\ncompact latent space for the diffusion process. Moving beyond unconditional\ngeneration, our novel ZeroFusion method progressively maps 3D masks to MRI\nimages in latent space, enabling robust training on limited datasets while\navoiding overfitting. ZECO outperforms state-of-the-art models in both\nquantitative and qualitative evaluations on Brain MRI datasets across various\nmodalities, showcasing its exceptional capability in synthesizing high-quality\nMRI images conditioned on segmentation masks."}
{"id": "2503.18275", "pdf": "https://arxiv.org/pdf/2503.18275", "abs": "https://arxiv.org/abs/2503.18275", "authors": ["Xulang Liu", "Ning Tan"], "title": "GI-SLAM: Gaussian-Inertial SLAM", "categories": ["cs.RO", "cs.CV"], "comment": "10 pages, 2 figures, 5 tables", "summary": "3D Gaussian Splatting (3DGS) has recently emerged as a powerful\nrepresentation of geometry and appearance for dense Simultaneous Localization\nand Mapping (SLAM). Through rapid, differentiable rasterization of 3D\nGaussians, many 3DGS SLAM methods achieve near real-time rendering and\naccelerated training. However, these methods largely overlook inertial data,\nwitch is a critical piece of information collected from the inertial\nmeasurement unit (IMU). In this paper, we present GI-SLAM, a novel\ngaussian-inertial SLAM system which consists of an IMU-enhanced camera tracking\nmodule and a realistic 3D Gaussian-based scene representation for mapping. Our\nmethod introduces an IMU loss that seamlessly integrates into the deep learning\nframework underpinning 3D Gaussian Splatting SLAM, effectively enhancing the\naccuracy, robustness and efficiency of camera tracking. Moreover, our SLAM\nsystem supports a wide range of sensor configurations, including monocular,\nstereo, and RGBD cameras, both with and without IMU integration. Our method\nachieves competitive performance compared with existing state-of-the-art\nreal-time methods on the EuRoC and TUM-RGBD datasets."}
{"id": "2503.18462", "pdf": "https://arxiv.org/pdf/2503.18462", "abs": "https://arxiv.org/abs/2503.18462", "authors": ["Tadeusz Dziarmaga", "Marcin Kądziołka", "Artur Kasymov", "Marcin Mazur"], "title": "PALATE: Peculiar Application of the Law of Total Expectation to Enhance the Evaluation of Deep Generative Models", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Deep generative models (DGMs) have caused a paradigm shift in the field of\nmachine learning, yielding noteworthy advancements in domains such as image\nsynthesis, natural language processing, and other related areas. However, a\ncomprehensive evaluation of these models that accounts for the trichotomy\nbetween fidelity, diversity, and novelty in generated samples remains a\nformidable challenge. A recently introduced solution that has emerged as a\npromising approach in this regard is the Feature Likelihood Divergence (FLD), a\nmethod that offers a theoretically motivated practical tool, yet also exhibits\nsome computational challenges. In this paper, we propose PALATE, a novel\nenhancement to the evaluation of DGMs that addresses limitations of existing\nmetrics. Our approach is based on a peculiar application of the law of total\nexpectation to random variables representing accessible real data. When\ncombined with the MMD baseline metric and DINOv2 feature extractor, PALATE\noffers a holistic evaluation framework that matches or surpasses\nstate-of-the-art solutions while providing superior computational efficiency\nand scalability to large-scale datasets. Through a series of experiments, we\ndemonstrate the effectiveness of the PALATE enhancement, contributing a\ncomputationally efficient, holistic evaluation approach that advances the field\nof DGMs assessment, especially in detecting sample memorization and evaluating\ngeneralization capabilities."}
{"id": "2503.18528", "pdf": "https://arxiv.org/pdf/2503.18528", "abs": "https://arxiv.org/abs/2503.18528", "authors": ["Moein Sorkhei", "Christos Matsoukas", "Johan Fredin Haslum", "Kevin Smith"], "title": "k-NN as a Simple and Effective Estimator of Transferability", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "How well can one expect transfer learning to work in a new setting where the\ndomain is shifted, the task is different, and the architecture changes? Many\ntransfer learning metrics have been proposed to answer this question. But how\naccurate are their predictions in a realistic new setting? We conducted an\nextensive evaluation involving over 42,000 experiments comparing 23\ntransferability metrics across 16 different datasets to assess their ability to\npredict transfer performance. Our findings reveal that none of the existing\nmetrics perform well across the board. However, we find that a simple k-nearest\nneighbor evaluation -- as is commonly used to evaluate feature quality for\nself-supervision -- not only surpasses existing metrics, but also offers better\ncomputational efficiency and ease of implementation."}
{"id": "2503.18578", "pdf": "https://arxiv.org/pdf/2503.18578", "abs": "https://arxiv.org/abs/2503.18578", "authors": ["Tianyu Chen", "Xingcheng Fu", "Yisen Gao", "Haodong Qian", "Yuecen Wei", "Kun Yan", "Haoyi Zhou", "Jianxin Li"], "title": "Galaxy Walker: Geometry-aware VLMs For Galaxy-scale Understanding", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Modern vision-language models (VLMs) develop patch embedding and convolution\nbackbone within vector space, especially Euclidean ones, at the very founding.\nWhen expanding VLMs to a galaxy scale for understanding astronomical phenomena,\nthe integration of spherical space for planetary orbits and hyperbolic spaces\nfor black holes raises two formidable challenges. a) The current pre-training\nmodel is confined to Euclidean space rather than a comprehensive geometric\nembedding. b) The predominant architecture lacks suitable backbones for\nanisotropic physical geometries. In this paper, we introduced Galaxy-Walker, a\ngeometry-aware VLM, for the universe-level vision understanding tasks. We\nproposed the geometry prompt that generates geometry tokens by random walks\nacross diverse spaces on a multi-scale physical graph, along with a geometry\nadapter that compresses and reshapes the space anisotropy in a\nmixture-of-experts manner. Extensive experiments demonstrate the effectiveness\nof our approach, with Galaxy-Walker achieving state-of-the-art performance in\nboth galaxy property estimation ($R^2$ scores up to $0.91$) and morphology\nclassification tasks (up to $+0.17$ F1 improvement in challenging features),\nsignificantly outperforming both domain-specific models and general-purpose\nVLMs."}
{"id": "2503.18642", "pdf": "https://arxiv.org/pdf/2503.18642", "abs": "https://arxiv.org/abs/2503.18642", "authors": ["Taejin Jeong", "Joohyeok Kim", "Jaehoon Joo", "Yeonwoo Jung", "Hyeonmin Kim", "Seong Jae Hwang"], "title": "Rethinking Glaucoma Calibration: Voting-Based Binocular and Metadata Integration", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Glaucoma is an incurable ophthalmic disease that damages the optic nerve,\nleads to vision loss, and ranks among the leading causes of blindness\nworldwide. Diagnosing glaucoma typically involves fundus photography, optical\ncoherence tomography (OCT), and visual field testing. However, the high cost of\nOCT often leads to reliance on fundus photography and visual field testing,\nboth of which exhibit inherent inter-observer variability. This stems from\nglaucoma being a multifaceted disease that influenced by various factors. As a\nresult, glaucoma diagnosis is highly subjective, emphasizing the necessity of\ncalibration, which aligns predicted probabilities with actual disease\nlikelihood. Proper calibration is essential to prevent overdiagnosis or\nmisdiagnosis, which are critical concerns for high-risk diseases. Although AI\nhas significantly improved diagnostic accuracy, overconfidence in models have\nworsen calibration performance. Recent study has begun focusing on calibration\nfor glaucoma. Nevertheless, previous study has not fully considered glaucoma's\nsystemic nature and the high subjectivity in its diagnostic process. To\novercome these limitations, we propose V-ViT (Voting-based ViT), a novel\nframework that enhances calibration by incorporating disease-specific\ncharacteristics. V-ViT integrates binocular data and metadata, reflecting the\nmulti-faceted nature of glaucoma diagnosis. Additionally, we introduce a MC\ndropout-based Voting System to address high subjectivity. Our approach achieves\nstate-of-the-art performance across all metrics, including accuracy,\ndemonstrating that our proposed methods are effective in addressing calibration\nissues. We validate our method using a custom dataset including binocular data."}
{"id": "2503.18752", "pdf": "https://arxiv.org/pdf/2503.18752", "abs": "https://arxiv.org/abs/2503.18752", "authors": ["Der-Hau Lee"], "title": "Robust Tube-based Control Strategy for Vision-guided Autonomous Vehicles", "categories": ["eess.SY", "cs.CV", "cs.RO", "cs.SY"], "comment": "13 pages, 14 figures", "summary": "A robust control strategy for autonomous vehicles can improve system\nstability, enhance riding comfort, and prevent driving accidents. This paper\npresents a novel interpolation tube-based constrained iterative linear\nquadratic regulator (itube-CILQR) algorithm for autonomous\ncomputer-vision-based vehicle lane-keeping. The goal of the algorithm is to\nenhance robustness during high-speed cornering on tight turns. The advantages\nof itube-CILQR over the standard tube-approach include reduced system\nconservatism and increased computational speed. Numerical and vision-based\nexperiments were conducted to examine the feasibility of the proposed\nalgorithm. The proposed itube-CILQR algorithm is better suited to vehicle\nlane-keeping than variational CILQR-based methods and model predictive control\n(MPC) approaches using a classical interior-point solver. Specifically, in\nevaluation experiments, itube-CILQR achieved an average runtime of 3.16 ms to\ngenerate a control signal to guide a self-driving vehicle; itube-MPC typically\nrequired a 4.67-times longer computation time to complete the same task.\nMoreover, the influence of conservatism on system behavior was investigated by\nexploring the interpolation variable trajectories derived from the proposed\nitube-CILQR algorithm during lane-keeping maneuvers."}
{"id": "2503.18836", "pdf": "https://arxiv.org/pdf/2503.18836", "abs": "https://arxiv.org/abs/2503.18836", "authors": ["Yuxuan Zhang", "Jinkui Hao", "Bo Zhou"], "title": "Dual-domain Multi-path Self-supervised Diffusion Model for Accelerated MRI Reconstruction", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "10 pages, 8 figures, 5 tables", "summary": "Magnetic resonance imaging (MRI) is a vital diagnostic tool, but its\ninherently long acquisition times reduce clinical efficiency and patient\ncomfort. Recent advancements in deep learning, particularly diffusion models,\nhave improved accelerated MRI reconstruction. However, existing diffusion\nmodels' training often relies on fully sampled data, models incur high\ncomputational costs, and often lack uncertainty estimation, limiting their\nclinical applicability. To overcome these challenges, we propose a novel\nframework, called Dual-domain Multi-path Self-supervised Diffusion Model\n(DMSM), that integrates a self-supervised dual-domain diffusion model training\nscheme, a lightweight hybrid attention network for the reconstruction diffusion\nmodel, and a multi-path inference strategy, to enhance reconstruction accuracy,\nefficiency, and explainability. Unlike traditional diffusion-based models, DMSM\neliminates the dependency on training from fully sampled data, making it more\npractical for real-world clinical settings. We evaluated DMSM on two human MRI\ndatasets, demonstrating that it achieves favorable performance over several\nsupervised and self-supervised baselines, particularly in preserving fine\nanatomical structures and suppressing artifacts under high acceleration\nfactors. Additionally, our model generates uncertainty maps that correlate\nreasonably well with reconstruction errors, offering valuable clinically\ninterpretable guidance and potentially enhancing diagnostic confidence."}
{"id": "2503.18840", "pdf": "https://arxiv.org/pdf/2503.18840", "abs": "https://arxiv.org/abs/2503.18840", "authors": ["Meva Himmetoglu", "Ilja Ciernik", "Ender Konukoglu"], "title": "Learning to segment anatomy and lesions from disparately labeled sources in brain MRI", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Segmenting healthy tissue structures alongside lesions in brain Magnetic\nResonance Images (MRI) remains a challenge for today's algorithms due to\nlesion-caused disruption of the anatomy and lack of jointly labeled training\ndatasets, where both healthy tissues and lesions are labeled on the same\nimages. In this paper, we propose a method that is robust to lesion-caused\ndisruptions and can be trained from disparately labeled training sets, i.e.,\nwithout requiring jointly labeled samples, to automatically segment both. In\ncontrast to prior work, we decouple healthy tissue and lesion segmentation in\ntwo paths to leverage multi-sequence acquisitions and merge information with an\nattention mechanism. During inference, an image-specific adaptation reduces\nadverse influences of lesion regions on healthy tissue predictions. During\ntraining, the adaptation is taken into account through meta-learning and\nco-training is used to learn from disparately labeled training images. Our\nmodel shows an improved performance on several anatomical structures and\nlesions on a publicly available brain glioblastoma dataset compared to the\nstate-of-the-art segmentation methods."}
{"id": "2503.18874", "pdf": "https://arxiv.org/pdf/2503.18874", "abs": "https://arxiv.org/abs/2503.18874", "authors": ["Runze Cheng", "Yao Sun", "Lan Zhang", "Lei Feng", "Lei Zhang", "Muhammad Ali Imran"], "title": "A semantic communication-based workload-adjustable transceiver for wireless AI-generated content (AIGC) delivery", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "With the significant advances in generative AI (GAI) and the proliferation of\nmobile devices, providing high-quality AI-generated content (AIGC) services via\nwireless networks is becoming the future direction. However, the primary\nchallenges of AIGC service delivery in wireless networks lie in unstable\nchannels, limited bandwidth resources, and unevenly distributed computational\nresources. In this paper, we employ semantic communication (SemCom) in\ndiffusion-based GAI models to propose a Resource-aware wOrkload-adjUstable\nTransceivEr (ROUTE) for AIGC delivery in dynamic wireless networks.\nSpecifically, to relieve the communication resource bottleneck, SemCom is\nutilized to prioritize semantic information of the generated content. Then, to\nimprove computational resource utilization in both edge and local and reduce\nAIGC semantic distortion in transmission, modified diffusion-based models are\napplied to adjust the computing workload and semantic density in cooperative\ncontent generation. Simulations verify the superiority of our proposed ROUTE in\nterms of latency and content quality compared to conventional AIGC approaches."}
{"id": "2503.18938", "pdf": "https://arxiv.org/pdf/2503.18938", "abs": "https://arxiv.org/abs/2503.18938", "authors": ["Shenyuan Gao", "Siyuan Zhou", "Yilun Du", "Jun Zhang", "Chuang Gan"], "title": "AdaWorld: Learning Adaptable World Models with Latent Actions", "categories": ["cs.AI", "cs.CV", "cs.LG", "cs.RO"], "comment": "Project page: https://adaptable-world-model.github.io/", "summary": "World models aim to learn action-controlled prediction models and have proven\nessential for the development of intelligent agents. However, most existing\nworld models rely heavily on substantial action-labeled data and costly\ntraining, making it challenging to adapt to novel environments with\nheterogeneous actions through limited interactions. This limitation can hinder\ntheir applicability across broader domains. To overcome this challenge, we\npropose AdaWorld, an innovative world model learning approach that enables\nefficient adaptation. The key idea is to incorporate action information during\nthe pretraining of world models. This is achieved by extracting latent actions\nfrom videos in a self-supervised manner, capturing the most critical\ntransitions between frames. We then develop an autoregressive world model that\nconditions on these latent actions. This learning paradigm enables highly\nadaptable world models, facilitating efficient transfer and learning of new\nactions even with limited interactions and finetuning. Our comprehensive\nexperiments across multiple environments demonstrate that AdaWorld achieves\nsuperior performance in both simulation quality and visual planning."}
