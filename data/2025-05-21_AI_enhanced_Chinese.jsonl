{"id": "2505.13468", "pdf": "https://arxiv.org/pdf/2505.13468", "abs": "https://arxiv.org/abs/2505.13468", "authors": ["Wenxuan Zhang", "Peng Hu"], "title": "An Edge AI Solution for Space Object Detection", "categories": ["cs.CV", "astro-ph.IM", "cs.LG", "cs.SY", "eess.SY"], "comment": "Accepted as poster paper at the 2025 IEEE Canadian Conference on\n  Electrical and Computer Engineering (CCECE)", "summary": "Effective Edge AI for space object detection (SOD) tasks that can facilitate\nreal-time collision assessment and avoidance is essential with the increasing\nspace assets in near-Earth orbits. In SOD, low Earth orbit (LEO) satellites\nmust detect other objects with high precision and minimal delay. We explore an\nEdge AI solution based on deep-learning-based vision sensing for SOD tasks and\npropose a deep learning model based on Squeeze-and-Excitation (SE) layers,\nVision Transformers (ViT), and YOLOv9 framework. We evaluate the performance of\nthese models across various realistic SOD scenarios, demonstrating their\nability to detect multiple satellites with high accuracy and very low latency.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u8fb9\u7f18AI\u89e3\u51b3\u65b9\u6848\uff0c\u7528\u4e8e\u7a7a\u95f4\u7269\u4f53\u68c0\u6d4b\uff08SOD\uff09\uff0c\u7ed3\u5408SE\u5c42\u3001Vision Transformers\u548cYOLOv9\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u548c\u4f4e\u5ef6\u8fdf\u7684\u68c0\u6d4b\u3002", "motivation": "\u968f\u7740\u8fd1\u5730\u8f68\u9053\u7a7a\u95f4\u8d44\u4ea7\u7684\u589e\u52a0\uff0c\u5b9e\u65f6\u78b0\u649e\u8bc4\u4f30\u548c\u907f\u969c\u7684\u9700\u6c42\u4fc3\u4f7f\u5f00\u53d1\u9ad8\u6548\u7684\u8fb9\u7f18AI\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u89c6\u89c9\u611f\u77e5\u6280\u672f\uff0c\u7ed3\u5408SE\u5c42\u3001Vision Transformers\u548cYOLOv9\u6846\u67b6\u6784\u5efa\u6a21\u578b\u3002", "result": "\u6a21\u578b\u5728\u591a\u79cd\u5b9e\u9645SOD\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u9ad8\u7cbe\u5ea6\u548c\u6781\u4f4e\u5ef6\u8fdf\u7684\u591a\u536b\u661f\u68c0\u6d4b\u80fd\u529b\u3002", "conclusion": "\u63d0\u51fa\u7684\u8fb9\u7f18AI\u89e3\u51b3\u65b9\u6848\u5728\u7a7a\u95f4\u7269\u4f53\u68c0\u6d4b\u4efb\u52a1\u4e2d\u5177\u6709\u9ad8\u6548\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2505.13584", "pdf": "https://arxiv.org/pdf/2505.13584", "abs": "https://arxiv.org/abs/2505.13584", "authors": ["Thangarajah Akilan", "Nusrat Jahan", "Wandong Zhang"], "title": "Self-Supervised Learning for Image Segmentation: A Comprehensive Survey", "categories": ["cs.CV"], "comment": "22 pages, 19 figures, to be submitted for a possible IEEE publication", "summary": "Supervised learning demands large amounts of precisely annotated data to\nachieve promising results. Such data curation is labor-intensive and imposes\nsignificant overhead regarding time and costs. Self-supervised learning (SSL)\npartially overcomes these limitations by exploiting vast amounts of unlabeled\ndata and creating surrogate (pretext or proxy) tasks to learn useful\nrepresentations without manual labeling. As a result, SSL has become a powerful\nmachine learning (ML) paradigm for solving several practical downstream\ncomputer vision problems, such as classification, detection, and segmentation.\nImage segmentation is the cornerstone of many high-level visual perception\napplications, including medical imaging, intelligent transportation,\nagriculture, and surveillance. Although there is substantial research potential\nfor developing advanced algorithms for SSL-based semantic segmentation, a\ncomprehensive study of existing methodologies is essential to trace advances\nand guide emerging researchers. This survey thoroughly investigates over 150\nrecent image segmentation articles, particularly focusing on SSL. It provides a\npractical categorization of pretext tasks, downstream tasks, and commonly used\nbenchmark datasets for image segmentation research. It concludes with key\nobservations distilled from a large body of literature and offers future\ndirections to make this research field more accessible and comprehensible for\nreaders.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7efc\u8ff0\u4e86\u81ea\u76d1\u7763\u5b66\u4e60\u5728\u56fe\u50cf\u5206\u5272\u9886\u57df\u7684\u5e94\u7528\uff0c\u5206\u6790\u4e86150\u591a\u7bc7\u76f8\u5173\u6587\u732e\uff0c\u63d0\u4f9b\u4e86\u4efb\u52a1\u5206\u7c7b\u3001\u6570\u636e\u96c6\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u76d1\u7763\u5b66\u4e60\u9700\u8981\u5927\u91cf\u7cbe\u786e\u6807\u6ce8\u6570\u636e\uff0c\u6210\u672c\u9ad8\u4e14\u8017\u65f6\u3002\u81ea\u76d1\u7763\u5b66\u4e60\u901a\u8fc7\u5229\u7528\u65e0\u6807\u7b7e\u6570\u636e\u514b\u670d\u8fd9\u4e00\u9650\u5236\uff0c\u6210\u4e3a\u89e3\u51b3\u8ba1\u7b97\u673a\u89c6\u89c9\u95ee\u9898\u7684\u6709\u529b\u5de5\u5177\u3002", "method": "\u8c03\u67e5\u4e86150\u591a\u7bc7\u5173\u4e8e\u81ea\u76d1\u7763\u5b66\u4e60\u548c\u56fe\u50cf\u5206\u5272\u7684\u6587\u732e\uff0c\u5206\u7c7b\u4e86\u4ee3\u7406\u4efb\u52a1\u3001\u4e0b\u6e38\u4efb\u52a1\u548c\u5e38\u7528\u6570\u636e\u96c6\u3002", "result": "\u603b\u7ed3\u4e86\u81ea\u76d1\u7763\u5b66\u4e60\u5728\u56fe\u50cf\u5206\u5272\u4e2d\u7684\u5173\u952e\u8fdb\u5c55\uff0c\u5e76\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u5206\u7c7b\u548c\u6570\u636e\u96c6\u4fe1\u606f\u3002", "conclusion": "\u81ea\u76d1\u7763\u5b66\u4e60\u5728\u56fe\u50cf\u5206\u5272\u9886\u57df\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u672a\u6765\u7814\u7a76\u5e94\u5173\u6ce8\u63d0\u9ad8\u65b9\u6cd5\u7684\u53ef\u8bbf\u95ee\u6027\u548c\u53ef\u7406\u89e3\u6027\u3002"}}
{"id": "2505.13633", "pdf": "https://arxiv.org/pdf/2505.13633", "abs": "https://arxiv.org/abs/2505.13633", "authors": ["Wentao Song", "He Huang", "Youqiang Sun", "Fang Qu", "Jiaqi Zhang", "Longhui Fang", "Yuwei Hao", "Chenyang Peng"], "title": "IPENS:Interactive Unsupervised Framework for Rapid Plant Phenotyping Extraction via NeRF-SAM2 Fusion", "categories": ["cs.CV"], "comment": null, "summary": "Advanced plant phenotyping technologies play a crucial role in targeted trait\nimprovement and accelerating intelligent breeding. Due to the species diversity\nof plants, existing methods heavily rely on large-scale high-precision manually\nannotated data. For self-occluded objects at the grain level, unsupervised\nmethods often prove ineffective. This study proposes IPENS, an interactive\nunsupervised multi-target point cloud extraction method. The method utilizes\nradiance field information to lift 2D masks, which are segmented by SAM2\n(Segment Anything Model 2), into 3D space for target point cloud extraction. A\nmulti-target collaborative optimization strategy is designed to effectively\nresolve the single-interaction multi-target segmentation challenge.\nExperimental validation demonstrates that IPENS achieves a grain-level\nsegmentation accuracy (mIoU) of 63.72% on a rice dataset, with strong\nphenotypic estimation capabilities: grain volume prediction yields R2 = 0.7697\n(RMSE = 0.0025), leaf surface area R2 = 0.84 (RMSE = 18.93), and leaf length\nand width predictions achieve R2 = 0.97 and 0.87 (RMSE = 1.49 and 0.21). On a\nwheat dataset,IPENS further improves segmentation accuracy to 89.68% (mIoU),\nwith equally outstanding phenotypic estimation performance: spike volume\nprediction achieves R2 = 0.9956 (RMSE = 0.0055), leaf surface area R2 = 1.00\n(RMSE = 0.67), and leaf length and width predictions reach R2 = 0.99 and 0.92\n(RMSE = 0.23 and 0.15). This method provides a non-invasive, high-quality\nphenotyping extraction solution for rice and wheat. Without requiring annotated\ndata, it rapidly extracts grain-level point clouds within 3 minutes through\nsimple single-round interactions on images for multiple targets, demonstrating\nsignificant potential to accelerate intelligent breeding efficiency.", "AI": {"tldr": "IPENS\u662f\u4e00\u79cd\u4ea4\u4e92\u5f0f\u65e0\u76d1\u7763\u591a\u76ee\u6807\u70b9\u4e91\u63d0\u53d6\u65b9\u6cd5\uff0c\u5229\u7528\u8f90\u5c04\u573a\u4fe1\u606f\u5c062D\u63a9\u7801\u63d0\u5347\u4e3a3D\u70b9\u4e91\uff0c\u89e3\u51b3\u4e86\u5355\u4ea4\u4e92\u591a\u76ee\u6807\u5206\u5272\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6c34\u7a3b\u548c\u5c0f\u9ea6\u7684\u8868\u578b\u63d0\u53d6\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5927\u89c4\u6a21\u9ad8\u7cbe\u5ea6\u4eba\u5de5\u6807\u6ce8\u6570\u636e\uff0c\u5bf9\u4e8e\u81ea\u906e\u6321\u7684\u8c37\u7269\u7ea7\u76ee\u6807\uff0c\u65e0\u76d1\u7763\u65b9\u6cd5\u6548\u679c\u4e0d\u4f73\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65e0\u9700\u6807\u6ce8\u6570\u636e\u7684\u9ad8\u6548\u8868\u578b\u63d0\u53d6\u65b9\u6cd5\u3002", "method": "IPENS\u5229\u7528SAM2\u5206\u5272\u76842D\u63a9\u7801\uff0c\u901a\u8fc7\u8f90\u5c04\u573a\u4fe1\u606f\u5c06\u5176\u63d0\u5347\u4e3a3D\u70b9\u4e91\uff0c\u5e76\u8bbe\u8ba1\u591a\u76ee\u6807\u534f\u540c\u4f18\u5316\u7b56\u7565\u89e3\u51b3\u591a\u76ee\u6807\u5206\u5272\u95ee\u9898\u3002", "result": "\u5728\u6c34\u7a3b\u6570\u636e\u96c6\u4e0a\uff0cIPENS\u7684mIoU\u4e3a63.72%\uff0c\u8868\u578b\u9884\u6d4b\u8868\u73b0\u4f18\u5f02\uff1b\u5728\u5c0f\u9ea6\u6570\u636e\u96c6\u4e0a\uff0cmIoU\u63d0\u5347\u81f389.68%\uff0c\u8868\u578b\u9884\u6d4b\u7cbe\u5ea6\u66f4\u9ad8\u3002", "conclusion": "IPENS\u4e3a\u975e\u4fb5\u5165\u5f0f\u9ad8\u8d28\u91cf\u8868\u578b\u63d0\u53d6\u63d0\u4f9b\u4e86\u89e3\u51b3\u65b9\u6848\uff0c\u65e0\u9700\u6807\u6ce8\u6570\u636e\uff0c\u663e\u8457\u52a0\u901f\u667a\u80fd\u80b2\u79cd\u6548\u7387\u3002"}}
{"id": "2505.13669", "pdf": "https://arxiv.org/pdf/2505.13669", "abs": "https://arxiv.org/abs/2505.13669", "authors": ["Barkin Dagda", "Muhammad Awais", "Saber Fallah"], "title": "GeoVLM: Improving Automated Vehicle Geolocalisation Using Vision-Language Matching", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Cross-view geo-localisation identifies coarse geographical position of an\nautomated vehicle by matching a ground-level image to a geo-tagged satellite\nimage from a database. Despite the advancements in Cross-view geo-localisation,\nsignificant challenges still persist such as similar looking scenes which makes\nit challenging to find the correct match as the top match. Existing approaches\nreach high recall rates but they still fail to rank the correct image as the\ntop match. To address this challenge, this paper proposes GeoVLM, a novel\napproach which uses the zero-shot capabilities of vision language models to\nenable cross-view geo-localisation using interpretable cross-view language\ndescriptions. GeoVLM is a trainable reranking approach which improves the best\nmatch accuracy of cross-view geo-localisation. GeoVLM is evaluated on standard\nbenchmark VIGOR and University-1652 and also through real-life driving\nenvironments using Cross-View United Kingdom, a new benchmark dataset\nintroduced in this paper. The results of the paper show that GeoVLM improves\nretrieval performance of cross-view geo-localisation compared to the\nstate-of-the-art methods with the help of explainable natural language\ndescriptions. The code is available at\nhttps://github.com/CAV-Research-Lab/GeoVLM", "AI": {"tldr": "GeoVLM\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u96f6\u6837\u672c\u80fd\u529b\uff0c\u901a\u8fc7\u53ef\u89e3\u91ca\u7684\u8de8\u89c6\u56fe\u8bed\u8a00\u63cf\u8ff0\u6539\u8fdb\u8de8\u89c6\u56fe\u5730\u7406\u5b9a\u4f4d\u7684\u5339\u914d\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u8de8\u89c6\u56fe\u5730\u7406\u5b9a\u4f4d\u65b9\u6cd5\u5728\u9ad8\u53ec\u56de\u7387\u4e0b\u4ecd\u96be\u4ee5\u5c06\u6b63\u786e\u56fe\u50cf\u6392\u540d\u7b2c\u4e00\uff0c\u76f8\u4f3c\u573a\u666f\u5bfc\u81f4\u5339\u914d\u56f0\u96be\u3002", "method": "\u63d0\u51faGeoVLM\uff0c\u4e00\u79cd\u53ef\u8bad\u7ec3\u7684\u91cd\u65b0\u6392\u5e8f\u65b9\u6cd5\uff0c\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u751f\u6210\u8de8\u89c6\u56fe\u8bed\u8a00\u63cf\u8ff0\u3002", "result": "\u5728VIGOR\u3001University-1652\u53ca\u65b0\u6570\u636e\u96c6Cross-View United Kingdom\u4e0a\u9a8c\u8bc1\uff0cGeoVLM\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "GeoVLM\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u663e\u8457\u63d0\u5347\u8de8\u89c6\u56fe\u5730\u7406\u5b9a\u4f4d\u7684\u68c0\u7d22\u6027\u80fd\u3002"}}
{"id": "2505.13573", "pdf": "https://arxiv.org/pdf/2505.13573", "abs": "https://arxiv.org/abs/2505.13573", "authors": ["Jian Liu", "Haohan Weng", "Biwen Lei", "Xianghui Yang", "Zibo Zhao", "Zhuo Chen", "Song Guo", "Tao Han", "Chunchao Guo"], "title": "FreeMesh: Boosting Mesh Generation with Coordinates Merging", "categories": ["cs.GR", "cs.AI"], "comment": "Accepted by ICML 2025, camera-ready version", "summary": "The next-coordinate prediction paradigm has emerged as the de facto standard\nin current auto-regressive mesh generation methods. Despite their\neffectiveness, there is no efficient measurement for the various tokenizers\nthat serialize meshes into sequences. In this paper, we introduce a new metric\nPer-Token-Mesh-Entropy (PTME) to evaluate the existing mesh tokenizers\ntheoretically without any training. Building upon PTME, we propose a\nplug-and-play tokenization technique called coordinate merging. It further\nimproves the compression ratios of existing tokenizers by rearranging and\nmerging the most frequent patterns of coordinates. Through experiments on\nvarious tokenization methods like MeshXL, MeshAnything V2, and Edgerunner, we\nfurther validate the performance of our method. We hope that the proposed PTME\nand coordinate merging can enhance the existing mesh tokenizers and guide the\nfurther development of native mesh generation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u6307\u6807PTME\u548c\u5750\u6807\u5408\u5e76\u6280\u672f\uff0c\u7528\u4e8e\u8bc4\u4f30\u548c\u6539\u8fdb\u7f51\u683c\u6807\u8bb0\u5316\u65b9\u6cd5\uff0c\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u63d0\u5347\u538b\u7f29\u7387\u3002", "motivation": "\u73b0\u6709\u7f51\u683c\u751f\u6210\u65b9\u6cd5\u7f3a\u4e4f\u5bf9\u6807\u8bb0\u5316\u6548\u7387\u7684\u8bc4\u4f30\u6807\u51c6\uff0c\u9650\u5236\u4e86\u7f51\u683c\u6807\u8bb0\u5316\u6280\u672f\u7684\u4f18\u5316\u548c\u53d1\u5c55\u3002", "method": "\u5f15\u5165PTME\u6307\u6807\u8bc4\u4f30\u6807\u8bb0\u5316\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u5750\u6807\u5408\u5e76\u6280\u672f\u4f18\u5316\u538b\u7f29\u7387\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86PTME\u548c\u5750\u6807\u5408\u5e76\u6280\u672f\u5728\u591a\u79cd\u6807\u8bb0\u5316\u65b9\u6cd5\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "PTME\u548c\u5750\u6807\u5408\u5e76\u6280\u672f\u53ef\u63d0\u5347\u73b0\u6709\u7f51\u683c\u6807\u8bb0\u5316\u65b9\u6cd5\uff0c\u5e76\u6307\u5bfc\u672a\u6765\u7f51\u683c\u751f\u6210\u6280\u672f\u7684\u53d1\u5c55\u3002"}}
{"id": "2505.13731", "pdf": "https://arxiv.org/pdf/2505.13731", "abs": "https://arxiv.org/abs/2505.13731", "authors": ["Pengyue Jia", "Seongheon Park", "Song Gao", "Xiangyu Zhao", "Yixuan Li"], "title": "GeoRanker: Distance-Aware Ranking for Worldwide Image Geolocalization", "categories": ["cs.CV"], "comment": null, "summary": "Worldwide image geolocalization-the task of predicting GPS coordinates from\nimages taken anywhere on Earth-poses a fundamental challenge due to the vast\ndiversity in visual content across regions. While recent approaches adopt a\ntwo-stage pipeline of retrieving candidates and selecting the best match, they\ntypically rely on simplistic similarity heuristics and point-wise supervision,\nfailing to model spatial relationships among candidates. In this paper, we\npropose GeoRanker, a distance-aware ranking framework that leverages large\nvision-language models to jointly encode query-candidate interactions and\npredict geographic proximity. In addition, we introduce a multi-order distance\nloss that ranks both absolute and relative distances, enabling the model to\nreason over structured spatial relationships. To support this, we curate\nGeoRanking, the first dataset explicitly designed for geographic ranking tasks\nwith multimodal candidate information. GeoRanker achieves state-of-the-art\nresults on two well-established benchmarks (IM2GPS3K and YFCC4K), significantly\noutperforming current best methods.", "AI": {"tldr": "GeoRanker\u662f\u4e00\u4e2a\u57fa\u4e8e\u8ddd\u79bb\u611f\u77e5\u7684\u6392\u540d\u6846\u67b6\uff0c\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8054\u5408\u7f16\u7801\u67e5\u8be2-\u5019\u9009\u4ea4\u4e92\u5e76\u9884\u6d4b\u5730\u7406\u90bb\u8fd1\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5168\u7403\u56fe\u50cf\u5730\u7406\u5b9a\u4f4d\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u5168\u7403\u56fe\u50cf\u5730\u7406\u5b9a\u4f4d\u4efb\u52a1\u9762\u4e34\u89c6\u89c9\u5185\u5bb9\u591a\u6837\u6027\u7684\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u7b80\u5355\u7684\u76f8\u4f3c\u6027\u542f\u53d1\u5f0f\u548c\u70b9\u76d1\u7763\uff0c\u672a\u80fd\u5efa\u6a21\u5019\u9009\u95f4\u7684\u7a7a\u95f4\u5173\u7cfb\u3002", "method": "\u63d0\u51faGeoRanker\u6846\u67b6\uff0c\u7ed3\u5408\u591a\u9636\u8ddd\u79bb\u635f\u5931\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u8054\u5408\u7f16\u7801\u67e5\u8be2-\u5019\u9009\u4ea4\u4e92\uff0c\u5e76\u5f15\u5165GeoRanking\u6570\u636e\u96c6\u652f\u6301\u5730\u7406\u6392\u540d\u4efb\u52a1\u3002", "result": "\u5728IM2GPS3K\u548cYFCC4K\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "GeoRanker\u901a\u8fc7\u5efa\u6a21\u7a7a\u95f4\u5173\u7cfb\u548c\u5f15\u5165\u591a\u6a21\u6001\u5019\u9009\u4fe1\u606f\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5730\u7406\u5b9a\u4f4d\u4efb\u52a1\u7684\u6027\u80fd\u3002"}}
{"id": "2505.14087", "pdf": "https://arxiv.org/pdf/2505.14087", "abs": "https://arxiv.org/abs/2505.14087", "authors": ["Ziyi Chang", "He Wang", "George Alex Koulieris", "Hubert P. H. Shum"], "title": "Large-Scale Multi-Character Interaction Synthesis", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Generating large-scale multi-character interactions is a challenging and\nimportant task in character animation. Multi-character interactions involve not\nonly natural interactive motions but also characters coordinated with each\nother for transition. For example, a dance scenario involves characters dancing\nwith partners and also characters coordinated to new partners based on spatial\nand temporal observations. We term such transitions as coordinated interactions\nand decompose them into interaction synthesis and transition planning. Previous\nmethods of single-character animation do not consider interactions that are\ncritical for multiple characters. Deep-learning-based interaction synthesis\nusually focuses on two characters and does not consider transition planning.\nOptimization-based interaction synthesis relies on manually designing objective\nfunctions that may not generalize well. While crowd simulation involves more\ncharacters, their interactions are sparse and passive. We identify two\nchallenges to multi-character interaction synthesis, including the lack of data\nand the planning of transitions among close and dense interactions. Existing\ndatasets either do not have multiple characters or do not have close and dense\ninteractions. The planning of transitions for multi-character close and dense\ninteractions needs both spatial and temporal considerations. We propose a\nconditional generative pipeline comprising a coordinatable multi-character\ninteraction space for interaction synthesis and a transition planning network\nfor coordinations. Our experiments demonstrate the effectiveness of our\nproposed pipeline for multicharacter interaction synthesis and the applications\nfacilitated by our method show the scalability and transferability.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u751f\u6210\u5927\u89c4\u6a21\u591a\u89d2\u8272\u4ea4\u4e92\u52a8\u753b\u7684\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u6570\u636e\u4e0d\u8db3\u548c\u8fc7\u6e21\u89c4\u5212\u4e0a\u7684\u6311\u6218\u3002", "motivation": "\u591a\u89d2\u8272\u4ea4\u4e92\u52a8\u753b\u5728\u89d2\u8272\u52a8\u753b\u4e2d\u5177\u6709\u91cd\u8981\u610f\u4e49\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u4ea4\u4e92\u5408\u6210\u548c\u8fc7\u6e21\u89c4\u5212\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0c\u5c24\u5176\u662f\u7f3a\u4e4f\u6570\u636e\u548c\u5bc6\u96c6\u4ea4\u4e92\u7684\u8fc7\u6e21\u89c4\u5212\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6761\u4ef6\u751f\u6210\u7ba1\u9053\uff0c\u5305\u62ec\u4e00\u4e2a\u53ef\u534f\u8c03\u7684\u591a\u89d2\u8272\u4ea4\u4e92\u7a7a\u95f4\u7528\u4e8e\u4ea4\u4e92\u5408\u6210\uff0c\u4ee5\u53ca\u4e00\u4e2a\u8fc7\u6e21\u89c4\u5212\u7f51\u7edc\u7528\u4e8e\u534f\u8c03\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u591a\u89d2\u8272\u4ea4\u4e92\u5408\u6210\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u5176\u53ef\u6269\u5c55\u6027\u548c\u53ef\u8fc1\u79fb\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u591a\u89d2\u8272\u4ea4\u4e92\u52a8\u753b\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u4e0d\u8db3\u548c\u8fc7\u6e21\u89c4\u5212\u7684\u6311\u6218\u3002"}}
{"id": "2505.13741", "pdf": "https://arxiv.org/pdf/2505.13741", "abs": "https://arxiv.org/abs/2505.13741", "authors": ["Gaspard Goupy", "Pierre Tirilly", "Ioan Marius Bilasco"], "title": "Frozen Backpropagation: Relaxing Weight Symmetry in Temporally-Coded Deep Spiking Neural Networks", "categories": ["cs.CV", "cs.NE"], "comment": null, "summary": "Direct training of Spiking Neural Networks (SNNs) on neuromorphic hardware\ncan greatly reduce energy costs compared to GPU-based training. However,\nimplementing Backpropagation (BP) on such hardware is challenging because\nforward and backward passes are typically performed by separate networks with\ndistinct weights. To compute correct gradients, forward and feedback weights\nmust remain symmetric during training, necessitating weight transport between\nthe two networks. This symmetry requirement imposes hardware overhead and\nincreases energy costs. To address this issue, we introduce Frozen\nBackpropagation (fBP), a BP-based training algorithm relaxing weight symmetry\nin settings with separate networks. fBP updates forward weights by computing\ngradients with periodically frozen feedback weights, reducing weight transports\nduring training and minimizing synchronization overhead. To further improve\ntransport efficiency, we propose three partial weight transport schemes of\nvarying computational complexity, where only a subset of weights is transported\nat a time. We evaluate our methods on image recognition tasks and compare them\nto existing approaches addressing the weight symmetry requirement. Our results\nshow that fBP outperforms these methods and achieves accuracy comparable to BP.\nWith partial weight transport, fBP can substantially lower transport costs by\n1,000x with an accuracy drop of only 0.5pp on CIFAR-10 and 1.1pp on CIFAR-100,\nor by up to 10,000x at the expense of moderated accuracy loss. This work\nprovides insights for guiding the design of neuromorphic hardware incorporating\nBP-based on-chip learning.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFrozen Backpropagation (fBP)\u7684\u8bad\u7ec3\u7b97\u6cd5\uff0c\u901a\u8fc7\u5468\u671f\u6027\u51bb\u7ed3\u53cd\u9988\u6743\u91cd\u51cf\u5c11\u6743\u91cd\u4f20\u8f93\uff0c\u4ece\u800c\u964d\u4f4e\u786c\u4ef6\u5f00\u9500\u548c\u80fd\u8017\u3002", "motivation": "\u5728\u795e\u7ecf\u5f62\u6001\u786c\u4ef6\u4e0a\u76f4\u63a5\u8bad\u7ec3SNNs\u80fd\u663e\u8457\u964d\u4f4e\u80fd\u8017\uff0c\u4f46\u53cd\u5411\u4f20\u64ad\u4e2d\u7684\u6743\u91cd\u5bf9\u79f0\u6027\u8981\u6c42\u589e\u52a0\u4e86\u786c\u4ef6\u5f00\u9500\u548c\u80fd\u8017\u3002", "method": "fBP\u7b97\u6cd5\u901a\u8fc7\u5468\u671f\u6027\u51bb\u7ed3\u53cd\u9988\u6743\u91cd\u66f4\u65b0\u524d\u5411\u6743\u91cd\uff0c\u5e76\u63d0\u51fa\u4e86\u4e09\u79cd\u90e8\u5206\u6743\u91cd\u4f20\u8f93\u65b9\u6848\u4ee5\u51cf\u5c11\u4f20\u8f93\u6210\u672c\u3002", "result": "fBP\u5728\u56fe\u50cf\u8bc6\u522b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u63a5\u8fd1BP\u7684\u51c6\u786e\u7387\uff0c\u540c\u65f6\u901a\u8fc7\u90e8\u5206\u6743\u91cd\u4f20\u8f93\u5c06\u4f20\u8f93\u6210\u672c\u964d\u4f4e1000x\u81f310000x\u3002", "conclusion": "fBP\u4e3a\u795e\u7ecf\u5f62\u6001\u786c\u4ef6\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6307\u5bfc\uff0c\u652f\u6301\u57fa\u4e8e\u53cd\u5411\u4f20\u64ad\u7684\u7247\u4e0a\u5b66\u4e60\u3002"}}
{"id": "2505.14306", "pdf": "https://arxiv.org/pdf/2505.14306", "abs": "https://arxiv.org/abs/2505.14306", "authors": ["Yue Fei", "Jingjing Liu", "Yuyou Yao", "Wenming Wu", "Liping Zheng"], "title": "A Remeshing Method via Adaptive Multiple Original-Facet-Clipping and Centroidal Voronoi Tessellation", "categories": ["cs.GR"], "comment": null, "summary": "CVT (Centroidal Voronoi Tessellation)-based remeshing optimizes mesh quality\nby leveraging the Voronoi-Delaunay framework to optimize vertex distribution\nand produce uniformly distributed vertices with regular triangles. Current\nCVT-based approaches can be classified into two categories: (1) exact methods\n(e.g., Geodesic CVT, Restricted Voronoi Diagrams) that ensure high quality but\nrequire significant computation; and (2) approximate methods that try to reduce\ncomputational complexity yet result in fair quality. To address this trade-off,\nwe propose a CVT-based surface remeshing approach that achieves balanced\noptimization between quality and efficiency through multiple clipping times of\n3D Centroidal Voronoi cells with curvature-adaptive original surface facets.\nThe core idea of the method is that we adaptively adjust the number of clipping\ntimes according to local curvature, and use the angular relationship between\nthe normal vectors of neighboring facets to represent the magnitude of local\ncurvature. Experimental results demonstrate the effectiveness of our method.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eCVT\u7684\u8868\u9762\u91cd\u7f51\u683c\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u6b21\u88c1\u526a3D Centroidal Voronoi\u5355\u5143\u5e76\u7ed3\u5408\u66f2\u7387\u81ea\u9002\u5e94\u7684\u539f\u59cb\u8868\u9762\u9762\u7247\uff0c\u5e73\u8861\u4e86\u7f51\u683c\u8d28\u91cf\u548c\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u73b0\u6709CVT\u65b9\u6cd5\u5728\u9ad8\u8d28\u91cf\u548c\u9ad8\u6548\u8ba1\u7b97\u4e4b\u95f4\u5b58\u5728\u6743\u8861\uff0c\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u8bc1\u8d28\u91cf\u53c8\u80fd\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u66f2\u7387\u81ea\u9002\u5e94\u8c03\u6574\u88c1\u526a\u6b21\u6570\uff0c\u5229\u7528\u76f8\u90bb\u9762\u7247\u6cd5\u5411\u91cf\u7684\u89d2\u5ea6\u5173\u7cfb\u8868\u793a\u5c40\u90e8\u66f2\u7387\u5927\u5c0f\uff0c\u4f18\u5316\u9876\u70b9\u5206\u5e03\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u8d28\u91cf\u548c\u6548\u7387\u4e4b\u95f4\u53d6\u5f97\u4e86\u5e73\u8861\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u66f2\u7387\u81ea\u9002\u5e94\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86CVT\u91cd\u7f51\u683c\u4e2d\u7684\u8d28\u91cf\u4e0e\u6548\u7387\u95ee\u9898\u3002"}}
{"id": "2505.13746", "pdf": "https://arxiv.org/pdf/2505.13746", "abs": "https://arxiv.org/abs/2505.13746", "authors": ["Satoshi Kondo"], "title": "ReSW-VL: Representation Learning for Surgical Workflow Analysis Using Vision-Language Model", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Surgical phase recognition from video is a technology that automatically\nclassifies the progress of a surgical procedure and has a wide range of\npotential applications, including real-time surgical support, optimization of\nmedical resources, training and skill assessment, and safety improvement.\nRecent advances in surgical phase recognition technology have focused primarily\non Transform-based methods, although methods that extract spatial features from\nindividual frames using a CNN and video features from the resulting time series\nof spatial features using time series modeling have shown high performance.\nHowever, there remains a paucity of research on training methods for CNNs\nemployed for feature extraction or representation learning in surgical phase\nrecognition. In this study, we propose a method for representation learning in\nsurgical workflow analysis using a vision-language model (ReSW-VL). Our\nproposed method involves fine-tuning the image encoder of a CLIP (Convolutional\nLanguage Image Model) vision-language model using prompt learning for surgical\nphase recognition. The experimental results on three surgical phase recognition\ndatasets demonstrate the effectiveness of the proposed method in comparison to\nconventional methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08ReSW-VL\uff09\u7684\u624b\u672f\u9636\u6bb5\u8bc6\u522b\u65b9\u6cd5\uff0c\u901a\u8fc7\u5fae\u8c03CLIP\u6a21\u578b\u7684\u56fe\u50cf\u7f16\u7801\u5668\u5e76\u7ed3\u5408\u63d0\u793a\u5b66\u4e60\uff0c\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u624b\u672f\u9636\u6bb5\u8bc6\u522b\u6280\u672f\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u5728CNN\u7279\u5f81\u63d0\u53d6\u6216\u8868\u793a\u5b66\u4e60\u7684\u8bad\u7ec3\u65b9\u6cd5\u4e0a\u4ecd\u6709\u4e0d\u8db3\u3002", "method": "\u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578bCLIP\uff0c\u901a\u8fc7\u63d0\u793a\u5b66\u4e60\u5fae\u8c03\u5176\u56fe\u50cf\u7f16\u7801\u5668\uff0c\u7528\u4e8e\u624b\u672f\u9636\u6bb5\u8bc6\u522b\u3002", "result": "\u5728\u4e09\u4e2a\u624b\u672f\u9636\u6bb5\u8bc6\u522b\u6570\u636e\u96c6\u4e0a\uff0c\u6240\u63d0\u65b9\u6cd5\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "ReSW-VL\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u63d0\u793a\u5b66\u4e60\uff0c\u6709\u6548\u63d0\u5347\u4e86\u624b\u672f\u9636\u6bb5\u8bc6\u522b\u7684\u6027\u80fd\u3002"}}
{"id": "2505.14222", "pdf": "https://arxiv.org/pdf/2505.14222", "abs": "https://arxiv.org/abs/2505.14222", "authors": ["Kaixing Yang", "Xulong Tang", "Yuxuan Hu", "Jiahao Yang", "Hongyan Liu", "Qinnan Zhang", "Jun He", "Zhaoxin Fan"], "title": "MatchDance: Collaborative Mamba-Transformer Architecture Matching for High-Quality 3D Dance Synthesis", "categories": ["cs.SD", "cs.GR", "cs.MM", "eess.AS"], "comment": null, "summary": "Music-to-dance generation represents a challenging yet pivotal task at the\nintersection of choreography, virtual reality, and creative content generation.\nDespite its significance, existing methods face substantial limitation in\nachieving choreographic consistency. To address the challenge, we propose\nMatchDance, a novel framework for music-to-dance generation that constructs a\nlatent representation to enhance choreographic consistency. MatchDance employs\na two-stage design: (1) a Kinematic-Dynamic-based Quantization Stage (KDQS),\nwhich encodes dance motions into a latent representation by Finite Scalar\nQuantization (FSQ) with kinematic-dynamic constraints and reconstructs them\nwith high fidelity, and (2) a Hybrid Music-to-Dance Generation Stage(HMDGS),\nwhich uses a Mamba-Transformer hybrid architecture to map music into the latent\nrepresentation, followed by the KDQS decoder to generate 3D dance motions.\nAdditionally, a music-dance retrieval framework and comprehensive metrics are\nintroduced for evaluation. Extensive experiments on the FineDance dataset\ndemonstrate state-of-the-art performance. Code will be released upon\nacceptance.", "AI": {"tldr": "MatchDance\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u97f3\u4e50\u5230\u821e\u8e48\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bbe\u8ba1\u63d0\u5347\u821e\u8e48\u4e00\u81f4\u6027\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u821e\u8e48\u4e00\u81f4\u6027\u4e0a\u5b58\u5728\u5c40\u9650\uff0cMatchDance\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u8bbe\u8ba1\uff1a1\uff09\u57fa\u4e8e\u8fd0\u52a8\u5b66-\u52a8\u529b\u5b66\u7684\u91cf\u5316\u9636\u6bb5\uff08KDQS\uff09\uff0c2\uff09\u6df7\u5408\u97f3\u4e50\u5230\u821e\u8e48\u751f\u6210\u9636\u6bb5\uff08HMDGS\uff09\u3002", "result": "\u5728FineDance\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "MatchDance\u901a\u8fc7\u521b\u65b0\u7684\u6846\u67b6\u548c\u8bc4\u4f30\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u97f3\u4e50\u5230\u821e\u8e48\u751f\u6210\u7684\u4e00\u81f4\u6027\u3002"}}
{"id": "2505.13777", "pdf": "https://arxiv.org/pdf/2505.13777", "abs": "https://arxiv.org/abs/2505.13777", "authors": ["Subash Khanal", "Srikumar Sastry", "Aayush Dhakal", "Adeel Ahmad", "Nathan Jacobs"], "title": "Sat2Sound: A Unified Framework for Zero-Shot Soundscape Mapping", "categories": ["cs.CV", "cs.AI", "cs.SD"], "comment": null, "summary": "We present Sat2Sound, a multimodal representation learning framework for\nsoundscape mapping, designed to predict the distribution of sounds at any\nlocation on Earth. Existing methods for this task rely on satellite image and\npaired geotagged audio samples, which often fail to capture the diversity of\nsound sources at a given location. To address this limitation, we enhance\nexisting datasets by leveraging a Vision-Language Model (VLM) to generate\nsemantically rich soundscape descriptions for locations depicted in satellite\nimages. Our approach incorporates contrastive learning across audio, audio\ncaptions, satellite images, and satellite image captions. We hypothesize that\nthere is a fixed set of soundscape concepts shared across modalities. To this\nend, we learn a shared codebook of soundscape concepts and represent each\nsample as a weighted average of these concepts. Sat2Sound achieves\nstate-of-the-art performance in cross-modal retrieval between satellite image\nand audio on two datasets: GeoSound and SoundingEarth. Additionally, building\non Sat2Sound's ability to retrieve detailed soundscape captions, we introduce a\nnovel application: location-based soundscape synthesis, which enables immersive\nacoustic experiences. Our code and models will be publicly available.", "AI": {"tldr": "Sat2Sound\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u8868\u793a\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u9884\u6d4b\u5730\u7403\u4e0a\u4efb\u4f55\u4f4d\u7f6e\u7684\u58f0\u97f3\u5206\u5e03\uff0c\u901a\u8fc7\u7ed3\u5408\u536b\u661f\u56fe\u50cf\u548c\u97f3\u9891\u6570\u636e\uff0c\u5e76\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u751f\u6210\u4e30\u5bcc\u7684\u58f0\u666f\u63cf\u8ff0\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u536b\u661f\u56fe\u50cf\u548c\u5730\u7406\u6807\u8bb0\u97f3\u9891\u6837\u672c\uff0c\u4f46\u65e0\u6cd5\u5145\u5206\u6355\u6349\u58f0\u97f3\u591a\u6837\u6027\u3002Sat2Sound\u65e8\u5728\u901a\u8fc7\u591a\u6a21\u6001\u5b66\u4e60\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u751f\u6210\u58f0\u666f\u63cf\u8ff0\uff0c\u7ed3\u5408\u5bf9\u6bd4\u5b66\u4e60\u97f3\u9891\u3001\u97f3\u9891\u63cf\u8ff0\u3001\u536b\u661f\u56fe\u50cf\u548c\u56fe\u50cf\u63cf\u8ff0\uff0c\u5b66\u4e60\u5171\u4eab\u7684\u58f0\u666f\u6982\u5ff5\u4ee3\u7801\u5e93\u3002", "result": "\u5728GeoSound\u548cSoundingEarth\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u8de8\u6a21\u6001\u68c0\u7d22\u7684\u6700\u65b0\u6027\u80fd\uff0c\u5e76\u652f\u6301\u57fa\u4e8e\u4f4d\u7f6e\u7684\u58f0\u666f\u5408\u6210\u3002", "conclusion": "Sat2Sound\u901a\u8fc7\u591a\u6a21\u6001\u5b66\u4e60\u63d0\u5347\u4e86\u58f0\u666f\u6620\u5c04\u7684\u51c6\u786e\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u65b0\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2505.13784", "pdf": "https://arxiv.org/pdf/2505.13784", "abs": "https://arxiv.org/abs/2505.13784", "authors": ["Dinh Nam Pham", "Eleftherios Avramidis"], "title": "Transfer Learning from Visual Speech Recognition to Mouthing Recognition in German Sign Language", "categories": ["cs.CV"], "comment": "Accepted at 19th IEEE International Conference on Automatic Face and\n  Gesture Recognition 2025", "summary": "Sign Language Recognition (SLR) systems primarily focus on manual gestures,\nbut non-manual features such as mouth movements, specifically mouthing, provide\nvaluable linguistic information. This work directly classifies mouthing\ninstances to their corresponding words in the spoken language while exploring\nthe potential of transfer learning from Visual Speech Recognition (VSR) to\nmouthing recognition in German Sign Language. We leverage three VSR datasets:\none in English, one in German with unrelated words and one in German containing\nthe same target words as the mouthing dataset, to investigate the impact of\ntask similarity in this setting. Our results demonstrate that multi-task\nlearning improves both mouthing recognition and VSR accuracy as well as model\nrobustness, suggesting that mouthing recognition should be treated as a\ndistinct but related task to VSR. This research contributes to the field of SLR\nby proposing knowledge transfer from VSR to SLR datasets with limited mouthing\nannotations.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u901a\u8fc7\u4ece\u89c6\u89c9\u8bed\u97f3\u8bc6\u522b\uff08VSR\uff09\u8fc1\u79fb\u5b66\u4e60\u6765\u6539\u8fdb\u5fb7\u56fd\u624b\u8bed\u4e2d\u53e3\u578b\u8bc6\u522b\u7684\u6548\u679c\uff0c\u5e76\u63a2\u8ba8\u4e86\u591a\u4efb\u52a1\u5b66\u4e60\u5bf9\u63d0\u5347\u6a21\u578b\u6027\u80fd\u7684\u4f5c\u7528\u3002", "motivation": "\u624b\u8bed\u8bc6\u522b\uff08SLR\uff09\u7cfb\u7edf\u901a\u5e38\u5173\u6ce8\u624b\u52bf\uff0c\u4f46\u975e\u624b\u52a8\u7279\u5f81\uff08\u5982\u53e3\u578b\uff09\u4e5f\u5305\u542b\u91cd\u8981\u8bed\u8a00\u4fe1\u606f\u3002\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63a2\u7d22\u5982\u4f55\u5229\u7528VSR\u7684\u77e5\u8bc6\u63d0\u5347\u53e3\u578b\u8bc6\u522b\u3002", "method": "\u5229\u7528\u4e09\u4e2aVSR\u6570\u636e\u96c6\uff08\u82f1\u8bed\u3001\u5fb7\u8bed\u65e0\u5173\u8bcd\u3001\u5fb7\u8bed\u76ee\u6807\u8bcd\uff09\u8fdb\u884c\u8fc1\u79fb\u5b66\u4e60\uff0c\u5e76\u91c7\u7528\u591a\u4efb\u52a1\u5b66\u4e60\u7b56\u7565\u3002", "result": "\u591a\u4efb\u52a1\u5b66\u4e60\u663e\u8457\u63d0\u5347\u4e86\u53e3\u578b\u8bc6\u522b\u548cVSR\u7684\u51c6\u786e\u6027\uff0c\u540c\u65f6\u589e\u5f3a\u4e86\u6a21\u578b\u9c81\u68d2\u6027\u3002", "conclusion": "\u53e3\u578b\u8bc6\u522b\u5e94\u89c6\u4e3a\u4e0eVSR\u76f8\u5173\u4f46\u72ec\u7acb\u7684\u4efb\u52a1\uff0c\u4e14VSR\u77e5\u8bc6\u53ef\u6709\u6548\u8fc1\u79fb\u81f3SLR\u6570\u636e\u6709\u9650\u7684\u53e3\u578b\u6807\u6ce8\u4efb\u52a1\u3002"}}
{"id": "2505.13788", "pdf": "https://arxiv.org/pdf/2505.13788", "abs": "https://arxiv.org/abs/2505.13788", "authors": ["Yongshuo Zong", "Qin Zhang", "Dongsheng An", "Zhihua Li", "Xiang Xu", "Linghan Xu", "Zhuowen Tu", "Yifan Xing", "Onkar Dabeer"], "title": "Ground-V: Teaching VLMs to Ground Complex Instructions in Pixels", "categories": ["cs.CV"], "comment": "Accepted to CVPR'25", "summary": "This work presents a simple yet effective workflow for automatically scaling\ninstruction-following data to elicit pixel-level grounding capabilities of VLMs\nunder complex instructions. In particular, we address five critical real-world\nchallenges in text-instruction-based grounding: hallucinated references,\nmulti-object scenarios, reasoning, multi-granularity, and part-level\nreferences. By leveraging knowledge distillation from a pre-trained teacher\nmodel, our approach generates high-quality instruction-response pairs linked to\nexisting pixel-level annotations, minimizing the need for costly human\nannotation. The resulting dataset, Ground-V, captures rich object localization\nknowledge and nuanced pixel-level referring expressions. Experiment results\nshow that models trained on Ground-V exhibit substantial improvements across\ndiverse grounding tasks. Specifically, incorporating Ground-V during training\ndirectly achieves an average accuracy boost of 4.4% for LISA and a 7.9% for\nPSALM across six benchmarks on the gIoU metric. It also sets new\nstate-of-the-art results on standard benchmarks such as RefCOCO/+/g. Notably,\non gRefCOCO, we achieve an N-Acc of 83.3%, exceeding the previous\nstate-of-the-art by more than 20%.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u6269\u5c55\u6307\u4ee4\u8ddf\u968f\u6570\u636e\u7684\u5de5\u4f5c\u6d41\uff0c\u7528\u4e8e\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u6307\u4ee4\u4e0b\u7684\u50cf\u7d20\u7ea7\u5b9a\u4f4d\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u6587\u672c\u6307\u4ee4\u5b9a\u4f4d\u4e2d\u7684\u4e94\u5927\u6311\u6218\uff0c\u5e76\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u751f\u6210\u9ad8\u8d28\u91cf\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u6587\u672c\u6307\u4ee4\u5b9a\u4f4d\u4e2d\u7684\u4e94\u5927\u6311\u6218\uff08\u5e7b\u89c9\u5f15\u7528\u3001\u591a\u5bf9\u8c61\u573a\u666f\u3001\u63a8\u7406\u3001\u591a\u7c92\u5ea6\u548c\u90e8\u5206\u7ea7\u5f15\u7528\uff09\uff0c\u51cf\u5c11\u5bf9\u6602\u8d35\u4eba\u5de5\u6807\u6ce8\u7684\u4f9d\u8d56\u3002", "method": "\u5229\u7528\u9884\u8bad\u7ec3\u6559\u5e08\u6a21\u578b\u7684\u77e5\u8bc6\u84b8\u998f\uff0c\u751f\u6210\u4e0e\u73b0\u6709\u50cf\u7d20\u7ea7\u6807\u6ce8\u94fe\u63a5\u7684\u9ad8\u8d28\u91cf\u6307\u4ee4-\u54cd\u5e94\u5bf9\u3002", "result": "\u751f\u6210\u7684Ground-V\u6570\u636e\u96c6\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\uff0cLISA\u548cPSALM\u5728gIoU\u6307\u6807\u4e0a\u5206\u522b\u63d0\u53474.4%\u548c7.9%\uff0c\u5e76\u5728RefCOCO/+/g\u7b49\u57fa\u51c6\u4e0a\u8fbe\u5230\u65b0SOTA\u3002", "conclusion": "Ground-V\u6709\u6548\u63d0\u5347\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u50cf\u7d20\u7ea7\u5b9a\u4f4d\u80fd\u529b\uff0c\u4e3a\u590d\u6742\u6307\u4ee4\u4e0b\u7684\u5b9a\u4f4d\u4efb\u52a1\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u6570\u636e\u652f\u6301\u3002"}}
{"id": "2505.13480", "pdf": "https://arxiv.org/pdf/2505.13480", "abs": "https://arxiv.org/abs/2505.13480", "authors": ["Avinash Patil", "Siru Tao", "Amardeep Gedhu"], "title": "Evaluating Reasoning LLMs for Suicide Screening with the Columbia-Suicide Severity Rating Scale", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "comment": "8 Pages, 6 Figures, 1 Table", "summary": "Suicide prevention remains a critical public health challenge. While online\nplatforms such as Reddit's r/SuicideWatch have historically provided spaces for\nindividuals to express suicidal thoughts and seek community support, the advent\nof large language models (LLMs) introduces a new paradigm-where individuals may\nbegin disclosing ideation to AI systems instead of humans. This study evaluates\nthe capability of LLMs to perform automated suicide risk assessment using the\nColumbia-Suicide Severity Rating Scale (C-SSRS). We assess the zero-shot\nperformance of six models-including Claude, GPT, Mistral, and LLaMA-in\nclassifying posts across a 7-point severity scale (Levels 0-6). Results\nindicate that Claude and GPT closely align with human annotations, while\nMistral achieves the lowest ordinal prediction error. Most models exhibit\nordinal sensitivity, with misclassifications typically occurring between\nadjacent severity levels. We further analyze confusion patterns,\nmisclassification sources, and ethical considerations, underscoring the\nimportance of human oversight, transparency, and cautious deployment. Full code\nand supplementary materials are available at\nhttps://github.com/av9ash/llm_cssrs_code.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u81ea\u6740\u98ce\u9669\u8bc4\u4f30\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0Claude\u548cGPT\u4e0e\u4eba\u7c7b\u6807\u6ce8\u6700\u63a5\u8fd1\uff0cMistral\u5728\u987a\u5e8f\u9884\u6d4b\u8bef\u5dee\u4e0a\u6700\u4f4e\u3002", "motivation": "\u5728\u7ebf\u5e73\u53f0\u5982Reddit\u7684r/SuicideWatch\u4e3a\u81ea\u6740\u503e\u5411\u8005\u63d0\u4f9b\u652f\u6301\uff0c\u4f46LLMs\u53ef\u80fd\u6210\u4e3a\u65b0\u7684\u503e\u8bc9\u5bf9\u8c61\uff0c\u56e0\u6b64\u9700\u8981\u8bc4\u4f30\u5176\u98ce\u9669\u8bc4\u4f30\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u54e5\u4f26\u6bd4\u4e9a\u81ea\u6740\u4e25\u91cd\u7a0b\u5ea6\u8bc4\u5b9a\u91cf\u8868\uff08C-SSRS\uff09\u8bc4\u4f30\u516d\u79cdLLMs\uff08\u5305\u62ecClaude\u3001GPT\u3001Mistral\u548cLLaMA\uff09\u7684\u96f6\u6837\u672c\u8868\u73b0\u3002", "result": "Claude\u548cGPT\u8868\u73b0\u63a5\u8fd1\u4eba\u7c7b\u6807\u6ce8\uff0cMistral\u987a\u5e8f\u9884\u6d4b\u8bef\u5dee\u6700\u4f4e\uff0c\u6a21\u578b\u901a\u5e38\u5728\u76f8\u90bb\u4e25\u91cd\u7ea7\u522b\u95f4\u8bef\u5224\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4eba\u7c7b\u76d1\u7763\u3001\u900f\u660e\u5ea6\u548c\u8c28\u614e\u90e8\u7f72\u7684\u91cd\u8981\u6027\uff0c\u4ee3\u7801\u548c\u8865\u5145\u6750\u6599\u5df2\u5f00\u6e90\u3002"}}
{"id": "2505.13812", "pdf": "https://arxiv.org/pdf/2505.13812", "abs": "https://arxiv.org/abs/2505.13812", "authors": ["Zhongyu Chen", "Rong Zhao", "Xie Han", "Xindong Guo", "Song Wang", "Zherui Qiao"], "title": "Physics-Driven Local-Whole Elastic Deformation Modeling for Point Cloud Representation Learning", "categories": ["cs.CV"], "comment": null, "summary": "Existing point cloud representation learning tend to learning the geometric\ndistribution of objects through data-driven approaches, emphasizing structural\nfeatures while overlooking the relationship between the local information and\nthe whole structure. Local features reflect the fine-grained variations of an\nobject, while the whole structure is determined by the interaction and\ncombination of these local features, collectively defining the object's shape.\nIn real-world, objects undergo elastic deformation under external forces, and\nthis deformation gradually affects the whole structure through the propagation\nof forces from local regions, thereby altering the object's geometric\nproperties. Inspired by this, we propose a physics-driven self-supervised\nlearning method for point cloud representation, which captures the relationship\nbetween parts and the whole by constructing a local-whole force propagation\nmechanism. Specifically, we employ a dual-task encoder-decoder framework,\nintegrating the geometric modeling capability of implicit fields with\nphysics-driven elastic deformation. The encoder extracts features from the\npoint cloud and its tetrahedral mesh representation, capturing both geometric\nand physical properties. These features are then fed into two decoders: one\nlearns the whole geometric shape of the point cloud through an implicit field,\nwhile the other predicts local deformations using two specifically designed\nphysics information loss functions, modeling the deformation relationship\nbetween local and whole shapes. Experimental results show that our method\noutperforms existing approaches in object classification, few-shot learning,\nand segmentation, demonstrating its effectiveness.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u9a71\u52a8\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c40\u90e8-\u6574\u4f53\u529b\u4f20\u64ad\u673a\u5236\u5b66\u4e60\u70b9\u4e91\u8868\u793a\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4fa7\u91cd\u4e8e\u51e0\u4f55\u5206\u5e03\u548c\u7ed3\u6784\u7279\u5f81\uff0c\u5ffd\u7565\u4e86\u5c40\u90e8\u4fe1\u606f\u4e0e\u6574\u4f53\u7ed3\u6784\u7684\u5173\u7cfb\uff0c\u800c\u771f\u5b9e\u4e16\u754c\u4e2d\u7269\u4f53\u7684\u5f39\u6027\u53d8\u5f62\u901a\u8fc7\u5c40\u90e8\u529b\u4f20\u64ad\u5f71\u54cd\u6574\u4f53\u5f62\u72b6\u3002", "method": "\u91c7\u7528\u53cc\u4efb\u52a1\u7f16\u7801\u5668-\u89e3\u7801\u5668\u6846\u67b6\uff0c\u7ed3\u5408\u9690\u5f0f\u573a\u7684\u51e0\u4f55\u5efa\u6a21\u80fd\u529b\u548c\u7269\u7406\u9a71\u52a8\u7684\u5f39\u6027\u53d8\u5f62\uff0c\u901a\u8fc7\u4e24\u4e2a\u89e3\u7801\u5668\u5206\u522b\u5b66\u4e60\u6574\u4f53\u5f62\u72b6\u548c\u5c40\u90e8\u53d8\u5f62\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u7269\u4f53\u5206\u7c7b\u3001\u5c11\u6837\u672c\u5b66\u4e60\u548c\u5206\u5272\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u7269\u7406\u9a71\u52a8\u7684\u5c40\u90e8-\u6574\u4f53\u5173\u7cfb\u5efa\u6a21\uff0c\u6709\u6548\u63d0\u5347\u4e86\u70b9\u4e91\u8868\u793a\u5b66\u4e60\u7684\u6027\u80fd\u3002"}}
{"id": "2505.13483", "pdf": "https://arxiv.org/pdf/2505.13483", "abs": "https://arxiv.org/abs/2505.13483", "authors": ["Xingyuan Lu", "Yuxi Liu", "Dongyu Zhang", "Zhiyao Wu", "Jing Ren", "Feng Xia"], "title": "EmoMeta: A Multimodal Dataset for Fine-grained Emotion Classification in Chinese Metaphors", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "Metaphors play a pivotal role in expressing emotions, making them crucial for\nemotional intelligence. The advent of multimodal data and widespread\ncommunication has led to a proliferation of multimodal metaphors, amplifying\nthe complexity of emotion classification compared to single-mode scenarios.\nHowever, the scarcity of research on constructing multimodal metaphorical\nfine-grained emotion datasets hampers progress in this domain. Moreover,\nexisting studies predominantly focus on English, overlooking potential\nvariations in emotional nuances across languages. To address these gaps, we\nintroduce a multimodal dataset in Chinese comprising 5,000 text-image pairs of\nmetaphorical advertisements. Each entry is meticulously annotated for metaphor\noccurrence, domain relations and fine-grained emotion classification\nencompassing joy, love, trust, fear, sadness, disgust, anger, surprise,\nanticipation, and neutral. Our dataset is publicly accessible\n(https://github.com/DUTIR-YSQ/EmoMeta), facilitating further advancements in\nthis burgeoning field.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e2d\u6587\u591a\u6a21\u6001\u9690\u55bb\u5e7f\u544a\u6570\u636e\u96c6\uff08EmoMeta\uff09\uff0c\u5305\u542b5000\u4e2a\u6587\u672c-\u56fe\u50cf\u5bf9\uff0c\u6807\u6ce8\u4e86\u9690\u55bb\u3001\u9886\u57df\u5173\u7cfb\u548c\u7ec6\u7c92\u5ea6\u60c5\u611f\u5206\u7c7b\uff0c\u586b\u8865\u4e86\u591a\u6a21\u6001\u9690\u55bb\u60c5\u611f\u6570\u636e\u96c6\u7684\u7a7a\u767d\u3002", "motivation": "\u591a\u6a21\u6001\u9690\u55bb\u5728\u60c5\u611f\u8868\u8fbe\u4e2d\u65e5\u76ca\u91cd\u8981\uff0c\u4f46\u76f8\u5173\u7814\u7a76\u7a00\u7f3a\u4e14\u96c6\u4e2d\u4e8e\u82f1\u8bed\uff0c\u5ffd\u7565\u4e86\u8bed\u8a00\u95f4\u7684\u5dee\u5f02\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u4e2d\u6587\u591a\u6a21\u6001\u9690\u55bb\u5e7f\u544a\u6570\u636e\u96c6\uff0c\u6807\u6ce8\u4e86\u9690\u55bb\u3001\u9886\u57df\u5173\u7cfb\u548c10\u79cd\u7ec6\u7c92\u5ea6\u60c5\u611f\u3002", "result": "\u6570\u636e\u96c6\u516c\u5f00\u53ef\u7528\uff08https://github.com/DUTIR-YSQ/EmoMeta\uff09\uff0c\u652f\u6301\u591a\u6a21\u6001\u9690\u55bb\u60c5\u611f\u5206\u7c7b\u7684\u8fdb\u4e00\u6b65\u7814\u7a76\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u4e3a\u591a\u6a21\u6001\u9690\u55bb\u60c5\u611f\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u8d44\u6e90\uff0c\u4fc3\u8fdb\u4e86\u8de8\u8bed\u8a00\u60c5\u611f\u5206\u6790\u7684\u8fdb\u5c55\u3002"}}
{"id": "2505.13817", "pdf": "https://arxiv.org/pdf/2505.13817", "abs": "https://arxiv.org/abs/2505.13817", "authors": ["Feng Li", "Kun Xu", "Zhaoyue Wang", "Yunduan Cui", "Mohammad Masum Billah", "Jia Liu"], "title": "InstanceBEV: Unifying Instance and BEV Representation for Global Modeling", "categories": ["cs.CV"], "comment": null, "summary": "Occupancy Grid Maps are widely used in navigation for their ability to\nrepresent 3D space occupancy. However, existing methods that utilize multi-view\ncameras to construct Occupancy Networks for perception modeling suffer from\ncubic growth in data complexity. Adopting a Bird's-Eye View (BEV) perspective\noffers a more practical solution for autonomous driving, as it provides higher\nsemantic density and mitigates complex object occlusions. Nonetheless,\nBEV-based approaches still require extensive engineering optimizations to\nenable efficient large-scale global modeling. To address this challenge, we\npropose InstanceBEV, the first method to introduce instance-level\ndimensionality reduction for BEV, enabling global modeling with transformers\nwithout relying on sparsification or acceleration operators. Different from\nother BEV methods, our approach directly employs transformers to aggregate\nglobal features. Compared to 3D object detection models, our method samples\nglobal feature maps into 3D space. Experiments on OpenOcc-NuScenes dataset show\nthat InstanceBEV achieves state-of-the-art performance while maintaining a\nsimple, efficient framework without requiring additional optimizations.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faInstanceBEV\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b9e\u4f8b\u7ea7\u964d\u7ef4\u89e3\u51b3BEV\u89c6\u89d2\u4e0b\u5168\u5c40\u5efa\u6a21\u7684\u6570\u636e\u590d\u6742\u5ea6\u95ee\u9898\uff0c\u65e0\u9700\u7a00\u758f\u5316\u6216\u52a0\u901f\u64cd\u4f5c\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u9ad8\u6027\u80fd\u76843D\u7a7a\u95f4\u5efa\u6a21\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eBEV\u7684\u65b9\u6cd5\u5728\u5927\u89c4\u6a21\u5168\u5c40\u5efa\u6a21\u65f6\u9700\u590d\u6742\u4f18\u5316\uff0c\u800cInstanceBEV\u65e8\u5728\u7b80\u5316\u6d41\u7a0b\u5e76\u63d0\u5347\u6027\u80fd\u3002", "method": "\u91c7\u7528\u5b9e\u4f8b\u7ea7\u964d\u7ef4\u6280\u672f\uff0c\u76f4\u63a5\u5229\u7528transformer\u805a\u5408\u5168\u5c40\u7279\u5f81\uff0c\u5e76\u5c06\u5168\u5c40\u7279\u5f81\u56fe\u91c7\u6837\u81f33D\u7a7a\u95f4\u3002", "result": "\u5728OpenOcc-NuScenes\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u4f18\u6027\u80fd\uff0c\u4e14\u6846\u67b6\u7b80\u5355\u9ad8\u6548\u3002", "conclusion": "InstanceBEV\u4e3aBEV\u89c6\u89d2\u4e0b\u7684\u5168\u5c40\u5efa\u6a21\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u65e0\u9700\u989d\u5916\u4f18\u5316\u5373\u53ef\u5b9e\u73b0\u9ad8\u6027\u80fd\u3002"}}
{"id": "2505.13487", "pdf": "https://arxiv.org/pdf/2505.13487", "abs": "https://arxiv.org/abs/2505.13487", "authors": ["Ashwin Kumar", "Yuzi He", "Aram H. Markosyan", "Bobbie Chern", "Imanol Arrieta-Ibarra"], "title": "Detecting Prefix Bias in LLM-based Reward Models", "categories": ["cs.CL"], "comment": null, "summary": "Reinforcement Learning with Human Feedback (RLHF) has emerged as a key\nparadigm for task-specific fine-tuning of language models using human\npreference data. While numerous publicly available preference datasets provide\npairwise comparisons of responses, the potential for biases in the resulting\nreward models remains underexplored. In this work, we introduce novel methods\nto detect and evaluate prefix bias -- a systematic shift in model preferences\ntriggered by minor variations in query prefixes -- in LLM-based reward models\ntrained on such datasets. We leverage these metrics to reveal significant\nbiases in preference models across racial and gender dimensions. Our\ncomprehensive evaluation spans diverse open-source preference datasets and\nreward model architectures, demonstrating susceptibility to this kind of bias\nregardless of the underlying model architecture. Furthermore, we propose a data\naugmentation strategy to mitigate these biases, showing its effectiveness in\nreducing the impact of prefix bias. Our findings highlight the critical need\nfor bias-aware dataset design and evaluation in developing fair and reliable\nreward models, contributing to the broader discourse on fairness in AI.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u57fa\u4e8e\u4eba\u7c7b\u53cd\u9988\u7684\u5f3a\u5316\u5b66\u4e60\uff08RLHF\uff09\u4e2d\u5956\u52b1\u6a21\u578b\u7684\u504f\u89c1\u95ee\u9898\uff0c\u7279\u522b\u662f\u524d\u7f00\u504f\u89c1\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u6765\u51cf\u8f7b\u8fd9\u79cd\u504f\u89c1\u3002", "motivation": "\u63a2\u7d22RLHF\u4e2d\u5956\u52b1\u6a21\u578b\u53ef\u80fd\u5b58\u5728\u7684\u504f\u89c1\uff0c\u5c24\u5176\u662f\u7531\u67e5\u8be2\u524d\u7f00\u5fae\u5c0f\u53d8\u5316\u5f15\u53d1\u7684\u7cfb\u7edf\u6027\u504f\u597d\u504f\u79fb\uff08\u524d\u7f00\u504f\u89c1\uff09\uff0c\u4ee5\u4fc3\u8fdb\u516c\u5e73\u53ef\u9760\u7684AI\u53d1\u5c55\u3002", "method": "\u5f15\u5165\u65b0\u65b9\u6cd5\u68c0\u6d4b\u548c\u8bc4\u4f30\u524d\u7f00\u504f\u89c1\uff0c\u5206\u6790\u5176\u5bf9\u79cd\u65cf\u548c\u6027\u522b\u7ef4\u5ea6\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u6570\u636e\u589e\u5f3a\u7b56\u7565\u4ee5\u51cf\u8f7b\u504f\u89c1\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5956\u52b1\u6a21\u578b\u666e\u904d\u5b58\u5728\u524d\u7f00\u504f\u89c1\uff0c\u4e14\u4e0e\u6a21\u578b\u67b6\u6784\u65e0\u5173\uff1b\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u80fd\u6709\u6548\u51cf\u5c11\u504f\u89c1\u5f71\u54cd\u3002", "conclusion": "\u5f3a\u8c03\u5728\u8bbe\u8ba1\u516c\u5e73\u53ef\u9760\u7684\u5956\u52b1\u6a21\u578b\u65f6\uff0c\u9700\u5173\u6ce8\u504f\u89c1\u611f\u77e5\u7684\u6570\u636e\u96c6\u8bbe\u8ba1\u548c\u8bc4\u4f30\uff0c\u4e3aAI\u516c\u5e73\u6027\u7814\u7a76\u63d0\u4f9b\u91cd\u8981\u53c2\u8003\u3002"}}
{"id": "2505.13839", "pdf": "https://arxiv.org/pdf/2505.13839", "abs": "https://arxiv.org/abs/2505.13839", "authors": ["Zhenyu Bao", "Qing Li", "Guibiao Liao", "Zhongyuan Zhao", "Kanglin Liu"], "title": "MGStream: Motion-aware 3D Gaussian for Streamable Dynamic Scene Reconstruction", "categories": ["cs.CV"], "comment": null, "summary": "3D Gaussian Splatting (3DGS) has gained significant attention in streamable\ndynamic novel view synthesis (DNVS) for its photorealistic rendering capability\nand computational efficiency. Despite much progress in improving rendering\nquality and optimization strategies, 3DGS-based streamable dynamic scene\nreconstruction still suffers from flickering artifacts and storage\ninefficiency, and struggles to model the emerging objects. To tackle this, we\nintroduce MGStream which employs the motion-related 3D Gaussians (3DGs) to\nreconstruct the dynamic and the vanilla 3DGs for the static. The motion-related\n3DGs are implemented according to the motion mask and the clustering-based\nconvex hull algorithm. The rigid deformation is applied to the motion-related\n3DGs for modeling the dynamic, and the attention-based optimization on the\nmotion-related 3DGs enables the reconstruction of the emerging objects. As the\ndeformation and optimization are only conducted on the motion-related 3DGs,\nMGStream avoids flickering artifacts and improves the storage efficiency.\nExtensive experiments on real-world datasets N3DV and MeetRoom demonstrate that\nMGStream surpasses existing streaming 3DGS-based approaches in terms of\nrendering quality, training/storage efficiency and temporal consistency. Our\ncode is available at: https://github.com/pcl3dv/MGStream.", "AI": {"tldr": "MGStream\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8fd0\u52a8\u76f8\u51733D\u9ad8\u65af\u548c\u9759\u60013D\u9ad8\u65af\u7684\u52a8\u6001\u573a\u666f\u91cd\u5efa\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e863DGS\u5728\u52a8\u6001\u573a\u666f\u4e2d\u7684\u95ea\u70c1\u548c\u5b58\u50a8\u6548\u7387\u95ee\u9898\u3002", "motivation": "3DGS\u5728\u52a8\u6001\u65b0\u89c6\u89d2\u5408\u6210\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u4ecd\u5b58\u5728\u95ea\u70c1\u3001\u5b58\u50a8\u6548\u7387\u4f4e\u548c\u96be\u4ee5\u5efa\u6a21\u65b0\u7269\u4f53\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u8fd0\u52a8\u76f8\u51733D\u9ad8\u65af\u91cd\u5efa\u52a8\u6001\u90e8\u5206\uff0c\u9759\u6001\u90e8\u5206\u4f7f\u7528\u666e\u901a3D\u9ad8\u65af\uff1b\u901a\u8fc7\u8fd0\u52a8\u63a9\u7801\u548c\u805a\u7c7b\u51f8\u5305\u7b97\u6cd5\u5b9e\u73b0\u8fd0\u52a8\u76f8\u51733D\u9ad8\u65af\uff0c\u5e76\u5e94\u7528\u521a\u6027\u53d8\u5f62\u548c\u6ce8\u610f\u529b\u4f18\u5316\u3002", "result": "\u5728N3DV\u548cMeetRoom\u6570\u636e\u96c6\u4e0a\uff0cMGStream\u5728\u6e32\u67d3\u8d28\u91cf\u3001\u8bad\u7ec3/\u5b58\u50a8\u6548\u7387\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "MGStream\u6709\u6548\u89e3\u51b3\u4e86\u52a8\u6001\u573a\u666f\u4e2d\u7684\u95ea\u70c1\u548c\u5b58\u50a8\u95ee\u9898\uff0c\u540c\u65f6\u63d0\u5347\u4e86\u5efa\u6a21\u65b0\u7269\u4f53\u7684\u80fd\u529b\u3002"}}
{"id": "2505.13488", "pdf": "https://arxiv.org/pdf/2505.13488", "abs": "https://arxiv.org/abs/2505.13488", "authors": ["Federico Germani", "Giovanni Spitale"], "title": "Source framing triggers systematic evaluation bias in Large Language Models", "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly used not only to generate text\nbut also to evaluate it, raising urgent questions about whether their judgments\nare consistent, unbiased, and robust to framing effects. In this study, we\nsystematically examine inter- and intra-model agreement across four\nstate-of-the-art LLMs (OpenAI o3-mini, Deepseek Reasoner, xAI Grok 2, and\nMistral) tasked with evaluating 4,800 narrative statements on 24 different\ntopics of social, political, and public health relevance, for a total of\n192,000 assessments. We manipulate the disclosed source of each statement to\nassess how attribution to either another LLM or a human author of specified\nnationality affects evaluation outcomes. We find that, in the blind condition,\ndifferent LLMs display a remarkably high degree of inter- and intra-model\nagreement across topics. However, this alignment breaks down when source\nframing is introduced. Here we show that attributing statements to Chinese\nindividuals systematically lowers agreement scores across all models, and in\nparticular for Deepseek Reasoner. Our findings reveal that framing effects can\ndeeply affect text evaluation, with significant implications for the integrity,\nneutrality, and fairness of LLM-mediated information systems.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u6587\u672c\u8bc4\u4f30\u4e2d\u7684\u4e00\u81f4\u6027\u548c\u504f\u89c1\u95ee\u9898\uff0c\u53d1\u73b0\u6a21\u578b\u95f4\u548c\u6a21\u578b\u5185\u9ad8\u5ea6\u4e00\u81f4\uff0c\u4f46\u6765\u6e90\u6807\u6ce8\u4f1a\u663e\u8457\u5f71\u54cd\u7ed3\u679c\uff0c\u5c24\u5176\u662f\u5bf9\u4e2d\u56fd\u4f5c\u8005\u7684\u504f\u89c1\u3002", "motivation": "\u8bc4\u4f30LLMs\u5728\u6587\u672c\u751f\u6210\u548c\u8bc4\u4f30\u4e2d\u7684\u4e00\u81f4\u6027\u3001\u65e0\u504f\u89c1\u6027\u53ca\u5bf9\u6846\u67b6\u6548\u5e94\u7684\u9c81\u68d2\u6027\u3002", "method": "\u4f7f\u7528\u56db\u79cd\u5148\u8fdbLLMs\u8bc4\u4f304800\u6761\u53d9\u8ff0\u6027\u9648\u8ff0\uff0c\u64cd\u7eb5\u6765\u6e90\u6807\u6ce8\uff08LLM\u6216\u4eba\u7c7b\u4f5c\u8005\u56fd\u7c4d\uff09\uff0c\u5206\u6790\u6a21\u578b\u95f4\u548c\u6a21\u578b\u5185\u4e00\u81f4\u6027\u3002", "result": "\u76f2\u8bc4\u65f6\u6a21\u578b\u9ad8\u5ea6\u4e00\u81f4\uff0c\u4f46\u6765\u6e90\u6807\u6ce8\uff08\u5c24\u5176\u662f\u4e2d\u56fd\u4f5c\u8005\uff09\u663e\u8457\u964d\u4f4e\u4e00\u81f4\u6027\uff0cDeepseek Reasoner\u53d7\u5f71\u54cd\u6700\u5927\u3002", "conclusion": "\u6846\u67b6\u6548\u5e94\u4e25\u91cd\u5f71\u54cdLLMs\u7684\u6587\u672c\u8bc4\u4f30\uff0c\u5bf9\u5176\u4fe1\u606f\u7cfb\u7edf\u7684\u5b8c\u6574\u6027\u3001\u4e2d\u7acb\u6027\u548c\u516c\u5e73\u6027\u63d0\u51fa\u6311\u6218\u3002"}}
{"id": "2505.13856", "pdf": "https://arxiv.org/pdf/2505.13856", "abs": "https://arxiv.org/abs/2505.13856", "authors": ["Ruqin Zhou", "San Jiang", "Wanshou Jiang", "Yongsheng Zhang", "Chenguang Dai"], "title": "SuperMapNet for Long-Range and High-Accuracy Vectorized HD Map Construction", "categories": ["cs.CV"], "comment": "13 pages, 9 figures", "summary": "Vectorized HD map is essential for autonomous driving. Remarkable work has\nbeen achieved in recent years, but there are still major issues: (1) in the\ngeneration of the BEV features, single modality-based methods are of limited\nperception capability, while direct concatenation-based multi-modal methods\nfail to capture synergies and disparities between different modalities,\nresulting in limited ranges with feature holes; (2) in the classification and\nlocalization of map elements, only point information is used without the\nconsideration of element infor-mation and neglects the interaction between\npoint information and element information, leading to erroneous shapes and\nelement entanglement with low accuracy. To address above issues, we introduce\nSuperMapNet for long-range and high-accuracy vectorized HD map construction. It\nuses both camera images and LiDAR point clouds as input, and first tightly\ncouple semantic information from camera images and geometric information from\nLiDAR point clouds by a cross-attention based synergy enhancement module and a\nflow-based disparity alignment module for long-range BEV feature generation.\nAnd then, local features from point queries and global features from element\nqueries are tightly coupled by three-level interactions for high-accuracy\nclassification and localization, where Point2Point interaction learns local\ngeometric information between points of the same element and of each point,\nElement2Element interaction learns relation constraints between different\nelements and semantic information of each elements, and Point2Element\ninteraction learns complement element information for its constituent points.\nExperiments on the nuScenes and Argoverse2 datasets demonstrate superior\nperformances, surpassing SOTAs over 14.9/8.8 mAP and 18.5/3.1 mAP under\nhard/easy settings, respectively. The code is made publicly available1.", "AI": {"tldr": "SuperMapNet\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u957f\u8ddd\u79bb\u9ad8\u7cbe\u5ea6\u77e2\u91cf\u9ad8\u6e05\u5730\u56fe\u6784\u5efa\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u8f93\u5165\u548c\u4ea4\u4e92\u6a21\u5757\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728BEV\u7279\u5f81\u751f\u6210\u548c\u5730\u56fe\u5143\u7d20\u5206\u7c7b\u5b9a\u4f4d\u4e2d\u5b58\u5728\u5355\u6a21\u6001\u9650\u5236\u548c\u591a\u6a21\u6001\u534f\u540c\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u7279\u5f81\u7a7a\u6d1e\u548c\u4f4e\u7cbe\u5ea6\u3002", "method": "\u7ed3\u5408\u76f8\u673a\u56fe\u50cf\u548cLiDAR\u70b9\u4e91\u8f93\u5165\uff0c\u901a\u8fc7\u8de8\u6ce8\u610f\u529b\u534f\u540c\u589e\u5f3a\u6a21\u5757\u548c\u6d41\u5f0f\u5dee\u5f02\u5bf9\u9f50\u6a21\u5757\u751f\u6210BEV\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u4e09\u7ea7\u4ea4\u4e92\uff08\u70b9-\u70b9\u3001\u5143\u7d20-\u5143\u7d20\u3001\u70b9-\u5143\u7d20\uff09\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u5206\u7c7b\u5b9a\u4f4d\u3002", "result": "\u5728nuScenes\u548cArgoverse2\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5206\u522b\u8d85\u8fc7SOTA\u65b9\u6cd514.9/8.8 mAP\u548c18.5/3.1 mAP\u3002", "conclusion": "SuperMapNet\u901a\u8fc7\u591a\u6a21\u6001\u534f\u540c\u548c\u4ea4\u4e92\u8bbe\u8ba1\u663e\u8457\u63d0\u5347\u4e86\u77e2\u91cf\u9ad8\u6e05\u5730\u56fe\u7684\u6784\u5efa\u7cbe\u5ea6\u548c\u8303\u56f4\u3002"}}
{"id": "2505.13491", "pdf": "https://arxiv.org/pdf/2505.13491", "abs": "https://arxiv.org/abs/2505.13491", "authors": ["Aakash Gupta", "Nataraj Das"], "title": "ProdRev: A DNN framework for empowering customers using generative pre-trained transformers", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "2022 International Conference on Decision Aid Sciences and\n  Applications (DASA)", "summary": "Following the pandemic, customers, preference for using e-commerce has\naccelerated. Since much information is available in multiple reviews (sometimes\nrunning in thousands) for a single product, it can create decision paralysis\nfor the buyer. This scenario disempowers the consumer, who cannot be expected\nto go over so many reviews since its time consuming and can confuse them.\nVarious commercial tools are available, that use a scoring mechanism to arrive\nat an adjusted score. It can alert the user to potential review manipulations.\nThis paper proposes a framework that fine-tunes a generative pre-trained\ntransformer to understand these reviews better. Furthermore, using\n\"common-sense\" to make better decisions. These models have more than 13 billion\nparameters. To fine-tune the model for our requirement, we use the curie engine\nfrom generative pre-trained transformer (GPT3). By using generative models, we\nare introducing abstractive summarization. Instead of using a simple extractive\nmethod of summarizing the reviews. This brings out the true relationship\nbetween the reviews and not simply copy-paste. This introduces an element of\n\"common sense\" for the user and helps them to quickly make the right decisions.\nThe user is provided the pros and cons of the processed reviews. Thus the\nuser/customer can take their own decisions.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u751f\u6210\u9884\u8bad\u7ec3Transformer\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u66f4\u597d\u5730\u7406\u89e3\u548c\u603b\u7ed3\u7535\u5546\u4ea7\u54c1\u8bc4\u8bba\uff0c\u5e2e\u52a9\u7528\u6237\u5feb\u901f\u505a\u51fa\u51b3\u7b56\u3002", "motivation": "\u75ab\u60c5\u671f\u95f4\uff0c\u7528\u6237\u5bf9\u7535\u5546\u7684\u4f9d\u8d56\u589e\u52a0\uff0c\u4f46\u6d77\u91cf\u8bc4\u8bba\u53ef\u80fd\u5bfc\u81f4\u51b3\u7b56\u762b\u75ea\u3002\u73b0\u6709\u5de5\u5177\u867d\u80fd\u8bc4\u5206\uff0c\u4f46\u7f3a\u4e4f\u6df1\u5c42\u7406\u89e3\u3002", "method": "\u4f7f\u7528GPT-3\u7684Curie\u5f15\u64ce\u5bf9\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff0c\u91c7\u7528\u62bd\u8c61\u6458\u8981\u65b9\u6cd5\u800c\u975e\u7b80\u5355\u63d0\u53d6\uff0c\u5f15\u5165\u201c\u5e38\u8bc6\u201d\u8f85\u52a9\u51b3\u7b56\u3002", "result": "\u6a21\u578b\u80fd\u751f\u6210\u8bc4\u8bba\u7684\u4f18\u7f3a\u70b9\u6458\u8981\uff0c\u5e2e\u52a9\u7528\u6237\u5feb\u901f\u7406\u89e3\u5e76\u505a\u51fa\u51b3\u7b56\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u751f\u6210\u5f0f\u6458\u8981\u63d0\u5347\u4e86\u8bc4\u8bba\u5206\u6790\u7684\u6df1\u5ea6\u548c\u5b9e\u7528\u6027\uff0c\u589e\u5f3a\u4e86\u7528\u6237\u7684\u51b3\u7b56\u80fd\u529b\u3002"}}
{"id": "2505.13860", "pdf": "https://arxiv.org/pdf/2505.13860", "abs": "https://arxiv.org/abs/2505.13860", "authors": ["Tiancheng Jiang", "Henry Wang", "Md Sirajus Salekin", "Parmida Atighehchian", "Shinan Zhang"], "title": "Domain Adaptation of VLM for Soccer Video Understanding", "categories": ["cs.CV", "cs.AI"], "comment": "8 pages, 5 figures, accepted to the 11th IEEE International Workshop\n  on Computer Vision in Sports (CVSports) at CVPR 2025; supplementary appendix\n  included as ancillary PDF", "summary": "Vision Language Models (VLMs) have demonstrated strong performance in\nmulti-modal tasks by effectively aligning visual and textual representations.\nHowever, most video understanding VLM research has been domain-agnostic,\nleaving the understanding of their transfer learning capability to specialized\ndomains under-explored. In this work, we address this by exploring the\nadaptability of open-source VLMs to specific domains, and focusing on soccer as\nan initial case study. Our approach uses large-scale soccer datasets and LLM to\ncreate instruction-following data, and use them to iteratively fine-tune the\ngeneral-domain VLM in a curriculum learning fashion (first teaching the model\nkey soccer concepts to then question answering tasks). The final adapted model,\ntrained using a curated dataset of 20k video clips, exhibits significant\nimprovement in soccer-specific tasks compared to the base model, with a 37.5%\nrelative improvement for the visual question-answering task and an accuracy\nimprovement from 11.8% to 63.5% for the downstream soccer action classification\ntask.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5f00\u6e90\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5728\u7279\u5b9a\u9886\u57df\uff08\u5982\u8db3\u7403\uff09\u4e2d\u7684\u9002\u5e94\u80fd\u529b\uff0c\u901a\u8fc7\u8bfe\u7a0b\u5b66\u4e60\u65b9\u5f0f\u5fae\u8c03\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8db3\u7403\u76f8\u5173\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u7406\u89e3VLM\u7814\u7a76\u591a\u4e3a\u901a\u7528\u9886\u57df\uff0c\u7f3a\u4e4f\u5bf9\u7279\u5b9a\u9886\u57df\u8fc1\u79fb\u5b66\u4e60\u80fd\u529b\u7684\u63a2\u7d22\uff0c\u672c\u6587\u4ee5\u8db3\u7403\u4e3a\u4f8b\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u4f7f\u7528\u5927\u89c4\u6a21\u8db3\u7403\u6570\u636e\u96c6\u548cLLM\u751f\u6210\u6307\u4ee4\u9075\u5faa\u6570\u636e\uff0c\u901a\u8fc7\u8bfe\u7a0b\u5b66\u4e60\u65b9\u5f0f\uff08\u5148\u6559\u6388\u5173\u952e\u6982\u5ff5\u540e\u95ee\u7b54\u4efb\u52a1\uff09\u8fed\u4ee3\u5fae\u8c03\u901a\u7528VLM\u3002", "result": "\u6700\u7ec8\u6a21\u578b\u5728\u8db3\u7403\u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\u4e2d\u76f8\u5bf9\u63d0\u534737.5%\uff0c\u8db3\u7403\u52a8\u4f5c\u5206\u7c7b\u4efb\u52a1\u51c6\u786e\u7387\u4ece11.8%\u63d0\u5347\u81f363.5%\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u9886\u57df\u9002\u914d\u548c\u8bfe\u7a0b\u5b66\u4e60\uff0c\u901a\u7528VLM\u53ef\u663e\u8457\u63d0\u5347\u7279\u5b9a\u9886\u57df\u4efb\u52a1\u6027\u80fd\u3002"}}
{"id": "2505.13492", "pdf": "https://arxiv.org/pdf/2505.13492", "abs": "https://arxiv.org/abs/2505.13492", "authors": ["Weiming Zhang", "Lingyue Fu", "Qingyao Li", "Kounianhua Du", "Jianghao Lin", "Jingwei Yu", "Wei Xia", "Weinan Zhang", "Ruiming Tang", "Yong Yu"], "title": "LLM4CD: Leveraging Large Language Models for Open-World Knowledge Augmented Cognitive Diagnosis", "categories": ["cs.CL"], "comment": null, "summary": "Cognitive diagnosis (CD) plays a crucial role in intelligent education,\nevaluating students' comprehension of knowledge concepts based on their test\nhistories. However, current CD methods often model students, exercises, and\nknowledge concepts solely on their ID relationships, neglecting the abundant\nsemantic relationships present within educational data space. Furthermore,\ncontemporary intelligent tutoring systems (ITS) frequently involve the addition\nof new students and exercises, a situation that ID-based methods find\nchallenging to manage effectively. The advent of large language models (LLMs)\noffers the potential for overcoming this challenge with open-world knowledge.\nIn this paper, we propose LLM4CD, which Leverages Large Language Models for\nOpen-World Knowledge Augmented Cognitive Diagnosis. Our method utilizes the\nopen-world knowledge of LLMs to construct cognitively expressive textual\nrepresentations, which are then encoded to introduce rich semantic information\ninto the CD task. Additionally, we propose an innovative bi-level encoder\nframework that models students' test histories through two levels of encoders:\na macro-level cognitive text encoder and a micro-level knowledge state encoder.\nThis approach substitutes traditional ID embeddings with semantic\nrepresentations, enabling the model to accommodate new students and exercises\nwith open-world knowledge and address the cold-start problem. Extensive\nexperimental results demonstrate that our proposed method consistently\noutperforms previous CD models on multiple real-world datasets, validating the\neffectiveness of leveraging LLMs to introduce rich semantic information into\nthe CD task.", "AI": {"tldr": "LLM4CD\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5f00\u653e\u4e16\u754c\u77e5\u8bc6\u589e\u5f3a\u8ba4\u77e5\u8bca\u65ad\uff0c\u901a\u8fc7\u8bed\u4e49\u8868\u793a\u66ff\u4ee3\u4f20\u7edfID\u5d4c\u5165\uff0c\u89e3\u51b3\u51b7\u542f\u52a8\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u8ba4\u77e5\u8bca\u65ad\u65b9\u6cd5\u4ec5\u4f9d\u8d56ID\u5173\u7cfb\u5efa\u6a21\uff0c\u5ffd\u89c6\u6559\u80b2\u6570\u636e\u4e2d\u7684\u8bed\u4e49\u4fe1\u606f\uff0c\u4e14\u96be\u4ee5\u5904\u7406\u65b0\u589e\u5b66\u751f\u548c\u7ec3\u4e60\u3002", "method": "\u63d0\u51faLLM4CD\uff0c\u5229\u7528LLM\u6784\u5efa\u8ba4\u77e5\u8868\u8fbe\u6587\u672c\u8868\u793a\uff0c\u5e76\u901a\u8fc7\u53cc\u5c42\u7f16\u7801\u5668\u6846\u67b6\uff08\u5b8f\u89c2\u8ba4\u77e5\u6587\u672c\u7f16\u7801\u5668\u548c\u5fae\u89c2\u77e5\u8bc6\u72b6\u6001\u7f16\u7801\u5668\uff09\u5efa\u6a21\u5b66\u751f\u6d4b\u8bd5\u5386\u53f2\u3002", "result": "\u5b9e\u9a8c\u8868\u660eLLM4CD\u5728\u591a\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u9a8c\u8bc1\u4e86\u5f15\u5165\u8bed\u4e49\u4fe1\u606f\u7684\u6709\u6548\u6027\u3002", "conclusion": "LLM4CD\u901a\u8fc7\u5f00\u653e\u4e16\u754c\u77e5\u8bc6\u548c\u8bed\u4e49\u8868\u793a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8ba4\u77e5\u8bca\u65ad\u7684\u6027\u80fd\u548c\u9002\u5e94\u6027\u3002"}}
{"id": "2505.13905", "pdf": "https://arxiv.org/pdf/2505.13905", "abs": "https://arxiv.org/abs/2505.13905", "authors": ["Ruihan Liu", "Xiaoyi Wu", "Xijun Chen", "Liang Hu", "Yunjiang Lou"], "title": "4D-ROLLS: 4D Radar Occupancy Learning via LiDAR Supervision", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "A comprehensive understanding of 3D scenes is essential for autonomous\nvehicles (AVs), and among various perception tasks, occupancy estimation plays\na central role by providing a general representation of drivable and occupied\nspace. However, most existing occupancy estimation methods rely on LiDAR or\ncameras, which perform poorly in degraded environments such as smoke, rain,\nsnow, and fog. In this paper, we propose 4D-ROLLS, the first weakly supervised\noccupancy estimation method for 4D radar using the LiDAR point cloud as the\nsupervisory signal. Specifically, we introduce a method for generating\npseudo-LiDAR labels, including occupancy queries and LiDAR height maps, as\nmulti-stage supervision to train the 4D radar occupancy estimation model. Then\nthe model is aligned with the occupancy map produced by LiDAR, fine-tuning its\naccuracy in occupancy estimation. Extensive comparative experiments validate\nthe exceptional performance of 4D-ROLLS. Its robustness in degraded\nenvironments and effectiveness in cross-dataset training are qualitatively\ndemonstrated. The model is also seamlessly transferred to downstream tasks BEV\nsegmentation and point cloud occupancy prediction, highlighting its potential\nfor broader applications. The lightweight network enables 4D-ROLLS model to\nachieve fast inference speeds at about 30 Hz on a 4060 GPU. The code of\n4D-ROLLS will be made available at https://github.com/CLASS-Lab/4D-ROLLS.", "AI": {"tldr": "4D-ROLLS\u662f\u4e00\u79cd\u5f31\u76d1\u7763\u76844D\u96f7\u8fbe\u5360\u7528\u4f30\u8ba1\u65b9\u6cd5\uff0c\u5229\u7528LiDAR\u70b9\u4e91\u4f5c\u4e3a\u76d1\u7763\u4fe1\u53f7\uff0c\u5728\u6076\u52a3\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u5360\u7528\u4f30\u8ba1\u65b9\u6cd5\u5728\u6076\u52a3\u73af\u5883\uff08\u5982\u70df\u96fe\u3001\u96e8\u96ea\uff09\u4e2d\u6027\u80fd\u4e0d\u4f73\uff0c\u9700\u4e00\u79cd\u66f4\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u751f\u6210\u4f2aLiDAR\u6807\u7b7e\uff08\u5360\u7528\u67e5\u8be2\u548c\u9ad8\u5ea6\u56fe\uff09\u4f5c\u4e3a\u591a\u9636\u6bb5\u76d1\u7763\uff0c\u8bad\u7ec34D\u96f7\u8fbe\u5360\u7528\u4f30\u8ba1\u6a21\u578b\uff0c\u5e76\u4e0eLiDAR\u5360\u7528\u56fe\u5bf9\u9f50\u4ee5\u63d0\u9ad8\u7cbe\u5ea6\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e864D-ROLLS\u7684\u5353\u8d8a\u6027\u80fd\uff0c\u5305\u62ec\u6076\u52a3\u73af\u5883\u4e0b\u7684\u9c81\u68d2\u6027\u548c\u8de8\u6570\u636e\u96c6\u8bad\u7ec3\u7684\u6709\u6548\u6027\u3002", "conclusion": "4D-ROLLS\u8f7b\u91cf\u9ad8\u6548\uff0830Hz\u63a8\u7406\u901f\u5ea6\uff09\uff0c\u53ef\u65e0\u7f1d\u8fc1\u79fb\u81f3\u4e0b\u6e38\u4efb\u52a1\uff0c\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2505.13498", "pdf": "https://arxiv.org/pdf/2505.13498", "abs": "https://arxiv.org/abs/2505.13498", "authors": ["Khanh-Tung Tran", "Barry O'Sullivan", "Hoang D. Nguyen"], "title": "IRLBench: A Multi-modal, Culturally Grounded, Parallel Irish-English Benchmark for Open-Ended LLM Reasoning Evaluation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advances in Large Language Models (LLMs) have demonstrated promising\nknowledge and reasoning abilities, yet their performance in multilingual and\nlow-resource settings remains underexplored. Existing benchmarks often exhibit\ncultural bias, restrict evaluation to text-only, rely on multiple-choice\nformats, and, more importantly, are limited for extremely low-resource\nlanguages. To address these gaps, we introduce IRLBench, presented in parallel\nEnglish and Irish, which is considered definitely endangered by UNESCO. Our\nbenchmark consists of 12 representative subjects developed from the 2024 Irish\nLeaving Certificate exams, enabling fine-grained analysis of model capabilities\nacross domains. By framing the task as long-form generation and leveraging the\nofficial marking scheme, it does not only support a comprehensive evaluation of\ncorrectness but also language fidelity. Our extensive experiments of leading\nclosed-source and open-source LLMs reveal a persistent performance gap between\nEnglish and Irish, in which models produce valid Irish responses less than 80\\%\nof the time, and answer correctly 55.8\\% of the time compared to 76.2\\% in\nEnglish for the best-performing model. We release IRLBench\n(https://huggingface.co/datasets/ReliableAI/IRLBench) and an accompanying\nevaluation codebase (https://github.com/ReML-AI/IRLBench) to enable future\nresearch on robust, culturally aware multilingual AI development.", "AI": {"tldr": "IRLBench\u662f\u4e00\u4e2a\u65b0\u7684\u591a\u8bed\u8a00\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4e13\u6ce8\u4e8e\u82f1\u8bed\u548c\u7231\u5c14\u5170\u8bed\uff0c\u7528\u4e8e\u8bc4\u4f30LLMs\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u548c\u6587\u5316\u591a\u6837\u6027\u73af\u5883\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5b58\u5728\u6587\u5316\u504f\u89c1\u3001\u4ec5\u9650\u6587\u672c\u8bc4\u4f30\u3001\u4f9d\u8d56\u9009\u62e9\u9898\u683c\u5f0f\uff0c\u4e14\u5bf9\u6781\u4f4e\u8d44\u6e90\u8bed\u8a00\u652f\u6301\u4e0d\u8db3\u3002", "method": "\u57fa\u4e8e2024\u5e74\u7231\u5c14\u5170\u6bd5\u4e1a\u8003\u8bd5\u5f00\u53d112\u4e2a\u4ee3\u8868\u6027\u79d1\u76ee\uff0c\u91c7\u7528\u957f\u6587\u672c\u751f\u6210\u4efb\u52a1\u548c\u5b98\u65b9\u8bc4\u5206\u6807\u51c6\u3002", "result": "\u5b9e\u9a8c\u663e\u793aLLMs\u5728\u7231\u5c14\u5170\u8bed\u4e2d\u7684\u8868\u73b0\u663e\u8457\u4f4e\u4e8e\u82f1\u8bed\uff0c\u6b63\u786e\u7387\u5206\u522b\u4e3a55.8%\u548c76.2%\u3002", "conclusion": "IRLBench\u4e3a\u672a\u6765\u591a\u8bed\u8a00AI\u7814\u7a76\u63d0\u4f9b\u4e86\u5de5\u5177\uff0c\u5f3a\u8c03\u6587\u5316\u610f\u8bc6\u548c\u8bed\u8a00\u4fdd\u771f\u5ea6\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2505.13915", "pdf": "https://arxiv.org/pdf/2505.13915", "abs": "https://arxiv.org/abs/2505.13915", "authors": ["Chu Chen", "Kangning Cui", "Pasquale Cascarano", "Wei Tang", "Elena Loli Piccolomini", "Raymond H. Chan"], "title": "Blind Restoration of High-Resolution Ultrasound Video", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Ultrasound imaging is widely applied in clinical practice, yet ultrasound\nvideos often suffer from low signal-to-noise ratios (SNR) and limited\nresolutions, posing challenges for diagnosis and analysis. Variations in\nequipment and acquisition settings can further exacerbate differences in data\ndistribution and noise levels, reducing the generalizability of pre-trained\nmodels. This work presents a self-supervised ultrasound video super-resolution\nalgorithm called Deep Ultrasound Prior (DUP). DUP employs a video-adaptive\noptimization process of a neural network that enhances the resolution of given\nultrasound videos without requiring paired training data while simultaneously\nremoving noise. Quantitative and visual evaluations demonstrate that DUP\noutperforms existing super-resolution algorithms, leading to substantial\nimprovements for downstream applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u76d1\u7763\u8d85\u58f0\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u7b97\u6cd5DUP\uff0c\u65e0\u9700\u914d\u5bf9\u8bad\u7ec3\u6570\u636e\u5373\u53ef\u63d0\u5347\u5206\u8fa8\u7387\u5e76\u964d\u566a\u3002", "motivation": "\u8d85\u58f0\u89c6\u9891\u901a\u5e38\u4fe1\u566a\u6bd4\u4f4e\u3001\u5206\u8fa8\u7387\u6709\u9650\uff0c\u4e14\u8bbe\u5907\u548c\u91c7\u96c6\u8bbe\u7f6e\u7684\u5dee\u5f02\u5bfc\u81f4\u6570\u636e\u5206\u5e03\u548c\u566a\u58f0\u6c34\u5e73\u4e0d\u4e00\u81f4\uff0c\u5f71\u54cd\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "DUP\u901a\u8fc7\u89c6\u9891\u81ea\u9002\u5e94\u4f18\u5316\u795e\u7ecf\u7f51\u7edc\uff0c\u63d0\u5347\u8d85\u58f0\u89c6\u9891\u5206\u8fa8\u7387\u5e76\u53bb\u9664\u566a\u58f0\u3002", "result": "\u5b9a\u91cf\u548c\u89c6\u89c9\u8bc4\u4f30\u663e\u793aDUP\u4f18\u4e8e\u73b0\u6709\u8d85\u5206\u8fa8\u7387\u7b97\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e0b\u6e38\u5e94\u7528\u6548\u679c\u3002", "conclusion": "DUP\u662f\u4e00\u79cd\u6709\u6548\u7684\u81ea\u76d1\u7763\u8d85\u58f0\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\uff0c\u65e0\u9700\u914d\u5bf9\u6570\u636e\u5373\u53ef\u5b9e\u73b0\u9ad8\u8d28\u91cf\u589e\u5f3a\u3002"}}
{"id": "2505.13500", "pdf": "https://arxiv.org/pdf/2505.13500", "abs": "https://arxiv.org/abs/2505.13500", "authors": ["Prithviraj Singh Shahani", "Matthias Scheutz"], "title": "Noise Injection Systemically Degrades Large Language Model Safety Guardrails", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "9 pages,3 figures", "summary": "Safety guardrails in large language models (LLMs) are a critical component in\npreventing harmful outputs. Yet, their resilience under perturbation remains\npoorly understood. In this paper, we investigate the robustness of safety\nfine-tuning in LLMs by systematically injecting Gaussian noise into model\nactivations. We show across multiple open-weight models that (1) Gaussian noise\nraises harmful-output rates (p < 0.001) by up to 27%, (2) that deeper safety\nfine-tuning affords no extra protection, and (3) that chain-of-thought\nreasoning remains largely intact. The findings reveal critical vulnerabilities\nin current safety alignment techniques and highlight the potential of\nreasoning-based and reinforcement learning approaches as promising direction\nfor developing more robust AI safety systems. These results have important\nimplications for real-world deployment of LLMs in safety-critical applications\nas these results imply that widely-deployed safety tuning methods can fail even\nwithout adversarial prompts.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5b89\u5168\u9632\u62a4\u5728\u566a\u58f0\u6270\u52a8\u4e0b\u5b58\u5728\u663e\u8457\u8106\u5f31\u6027\uff0c\u9ad8\u65af\u566a\u58f0\u4f1a\u5bfc\u81f4\u6709\u5bb3\u8f93\u51fa\u7387\u663e\u8457\u4e0a\u5347\uff0c\u4e14\u6df1\u5ea6\u5b89\u5168\u5fae\u8c03\u65e0\u6cd5\u63d0\u4f9b\u989d\u5916\u4fdd\u62a4\u3002", "motivation": "\u63a2\u8ba8\u5927\u8bed\u8a00\u6a21\u578b\u5b89\u5168\u5fae\u8c03\u5728\u566a\u58f0\u6270\u52a8\u4e0b\u7684\u9c81\u68d2\u6027\uff0c\u63ed\u793a\u5f53\u524d\u5b89\u5168\u5bf9\u9f50\u6280\u672f\u7684\u6f5c\u5728\u6f0f\u6d1e\u3002", "method": "\u901a\u8fc7\u5411\u6a21\u578b\u6fc0\u6d3b\u4e2d\u7cfb\u7edf\u6ce8\u5165\u9ad8\u65af\u566a\u58f0\uff0c\u8bc4\u4f30\u591a\u4e2a\u5f00\u6e90\u6a21\u578b\u7684\u5b89\u5168\u6027\u80fd\u3002", "result": "\u9ad8\u65af\u566a\u58f0\u4f7f\u6709\u5bb3\u8f93\u51fa\u7387\u4e0a\u534727%\uff08p<0.001\uff09\uff0c\u6df1\u5ea6\u5b89\u5168\u5fae\u8c03\u65e0\u989d\u5916\u4fdd\u62a4\u4f5c\u7528\uff0c\u4f46\u601d\u7ef4\u94fe\u63a8\u7406\u57fa\u672c\u4e0d\u53d7\u5f71\u54cd\u3002", "conclusion": "\u5f53\u524d\u5b89\u5168\u5bf9\u9f50\u6280\u672f\u5b58\u5728\u5173\u952e\u6f0f\u6d1e\uff0c\u57fa\u4e8e\u63a8\u7406\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u65b9\u6cd5\u53ef\u80fd\u662f\u672a\u6765\u63d0\u5347AI\u5b89\u5168\u6027\u7684\u65b9\u5411\u3002"}}
{"id": "2505.13923", "pdf": "https://arxiv.org/pdf/2505.13923", "abs": "https://arxiv.org/abs/2505.13923", "authors": ["Chinedu Emmanuel Mbonu", "Kenechukwu Anigbogu", "Doris Asogwa", "Tochukwu Belonwu"], "title": "An Explorative Analysis of SVM Classifier and ResNet50 Architecture on African Food Classification", "categories": ["cs.CV"], "comment": "7 pages, 9 figures", "summary": "Food recognition systems has advanced significantly for Western cuisines, yet\nits application to African foods remains underexplored. This study addresses\nthis gap by evaluating both deep learning and traditional machine learning\nmethods for African food classification. We compared the performance of a\nfine-tuned ResNet50 model with a Support Vector Machine (SVM) classifier. The\ndataset comprises 1,658 images across six selected food categories that are\nknown in Africa. To assess model effectiveness, we utilize five key evaluation\nmetrics: Confusion matrix, F1-score, accuracy, recall and precision. Our\nfindings offer valuable insights into the strengths and limitations of both\napproaches, contributing to the advancement of food recognition for African\ncuisines.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6bd4\u8f83\u4e86\u6df1\u5ea6\u5b66\u4e60\u548c\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5728\u975e\u6d32\u98df\u7269\u5206\u7c7b\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u4e24\u8005\u5404\u6709\u4f18\u52a3\u3002", "motivation": "\u5c3d\u7ba1\u98df\u7269\u8bc6\u522b\u7cfb\u7edf\u5728\u897f\u65b9\u83dc\u80b4\u4e2d\u5df2\u6709\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5728\u975e\u6d32\u98df\u7269\u4e2d\u7684\u5e94\u7528\u4ecd\u8f83\u5c11\u88ab\u63a2\u7d22\u3002", "method": "\u4f7f\u7528\u5fae\u8c03\u7684ResNet50\u6a21\u578b\u548c\u652f\u6301\u5411\u91cf\u673a\uff08SVM\uff09\u5206\u7c7b\u5668\uff0c\u5bf9\u5305\u542b1,658\u5f20\u975e\u6d32\u98df\u7269\u56fe\u50cf\u7684\u6570\u636e\u96c6\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u901a\u8fc7\u6df7\u6dc6\u77e9\u9635\u3001F1\u5206\u6570\u3001\u51c6\u786e\u7387\u3001\u53ec\u56de\u7387\u548c\u7cbe\u786e\u5ea6\u4e94\u9879\u6307\u6807\u8bc4\u4f30\u6a21\u578b\u6548\u679c\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u975e\u6d32\u98df\u7269\u8bc6\u522b\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u3002"}}
{"id": "2505.13506", "pdf": "https://arxiv.org/pdf/2505.13506", "abs": "https://arxiv.org/abs/2505.13506", "authors": ["Ruobing Yao", "Yifei Zhang", "Shuang Song", "Neng Gao", "Chenyang Tu"], "title": "EcoSafeRAG: Efficient Security through Context Analysis in Retrieval-Augmented Generation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) compensates for the static knowledge\nlimitations of Large Language Models (LLMs) by integrating external knowledge,\nproducing responses with enhanced factual correctness and query-specific\ncontextualization. However, it also introduces new attack surfaces such as\ncorpus poisoning at the same time. Most of the existing defense methods rely on\nthe internal knowledge of the model, which conflicts with the design concept of\nRAG. To bridge the gap, EcoSafeRAG uses sentence-level processing and\nbait-guided context diversity detection to identify malicious content by\nanalyzing the context diversity of candidate documents without relying on LLM\ninternal knowledge. Experiments show EcoSafeRAG delivers state-of-the-art\nsecurity with plug-and-play deployment, simultaneously improving clean-scenario\nRAG performance while maintaining practical operational costs (relatively\n1.2$\\times$ latency, 48\\%-80\\% token reduction versus Vanilla RAG).", "AI": {"tldr": "EcoSafeRAG\u63d0\u51fa\u4e86\u4e00\u79cd\u4e0d\u4f9d\u8d56LLM\u5185\u90e8\u77e5\u8bc6\u7684\u9632\u5fa1\u65b9\u6cd5\uff0c\u901a\u8fc7\u53e5\u5b50\u7ea7\u5904\u7406\u548c\u8bf1\u9975\u5f15\u5bfc\u7684\u4e0a\u4e0b\u6587\u591a\u6837\u6027\u68c0\u6d4b\u6765\u8bc6\u522b\u6076\u610f\u5185\u5bb9\uff0c\u540c\u65f6\u63d0\u5347RAG\u7684\u6027\u80fd\u3002", "motivation": "RAG\u901a\u8fc7\u96c6\u6210\u5916\u90e8\u77e5\u8bc6\u589e\u5f3a\u4e86LLM\u7684\u54cd\u5e94\u51c6\u786e\u6027\uff0c\u4f46\u4e5f\u5f15\u5165\u4e86\u65b0\u7684\u653b\u51fb\u9762\uff08\u5982\u8bed\u6599\u5e93\u4e2d\u6bd2\uff09\u3002\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\u4f9d\u8d56LLM\u5185\u90e8\u77e5\u8bc6\uff0c\u4e0eRAG\u8bbe\u8ba1\u7406\u5ff5\u51b2\u7a81\u3002", "method": "EcoSafeRAG\u91c7\u7528\u53e5\u5b50\u7ea7\u5904\u7406\u548c\u8bf1\u9975\u5f15\u5bfc\u7684\u4e0a\u4e0b\u6587\u591a\u6837\u6027\u68c0\u6d4b\uff0c\u5206\u6790\u5019\u9009\u6587\u6863\u7684\u4e0a\u4e0b\u6587\u591a\u6837\u6027\u4ee5\u8bc6\u522b\u6076\u610f\u5185\u5bb9\u3002", "result": "\u5b9e\u9a8c\u8868\u660eEcoSafeRAG\u5728\u5b89\u5168\u6027\u4e0a\u8fbe\u5230\u9886\u5148\u6c34\u5e73\uff0c\u540c\u65f6\u63d0\u5347\u5e72\u51c0\u573a\u666f\u4e0b\u7684RAG\u6027\u80fd\uff0c\u4e14\u8fd0\u884c\u6210\u672c\u8f83\u4f4e\uff08\u5ef6\u8fdf1.2\u500d\uff0c\u4ee4\u724c\u51cf\u5c1148%-80%\uff09\u3002", "conclusion": "EcoSafeRAG\u5728\u4e0d\u4f9d\u8d56LLM\u5185\u90e8\u77e5\u8bc6\u7684\u60c5\u51b5\u4e0b\uff0c\u6709\u6548\u89e3\u51b3\u4e86RAG\u7684\u5b89\u5168\u95ee\u9898\uff0c\u540c\u65f6\u4f18\u5316\u4e86\u6027\u80fd\u4e0e\u6210\u672c\u3002"}}
{"id": "2505.13928", "pdf": "https://arxiv.org/pdf/2505.13928", "abs": "https://arxiv.org/abs/2505.13928", "authors": ["Qifeng Cai", "Hao Liang", "Hejun Dong", "Meiyi Qiang", "Ruichuan An", "Zhaoyang Han", "Zhengzhou Zhu", "Bin Cui", "Wentao Zhang"], "title": "LoVR: A Benchmark for Long Video Retrieval in Multimodal Contexts", "categories": ["cs.CV", "cs.IR"], "comment": null, "summary": "Long videos contain a vast amount of information, making video-text retrieval\nan essential and challenging task in multimodal learning. However, existing\nbenchmarks suffer from limited video duration, low-quality captions, and coarse\nannotation granularity, which hinder the evaluation of advanced video-text\nretrieval methods. To address these limitations, we introduce LoVR, a benchmark\nspecifically designed for long video-text retrieval. LoVR contains 467 long\nvideos and over 40,804 fine-grained clips with high-quality captions. To\novercome the issue of poor machine-generated annotations, we propose an\nefficient caption generation framework that integrates VLM automatic\ngeneration, caption quality scoring, and dynamic refinement. This pipeline\nimproves annotation accuracy while maintaining scalability. Furthermore, we\nintroduce a semantic fusion method to generate coherent full-video captions\nwithout losing important contextual information. Our benchmark introduces\nlonger videos, more detailed captions, and a larger-scale dataset, presenting\nnew challenges for video understanding and retrieval. Extensive experiments on\nvarious advanced embedding models demonstrate that LoVR is a challenging\nbenchmark, revealing the limitations of current approaches and providing\nvaluable insights for future research. We release the code and dataset link at\nhttps://github.com/TechNomad-ds/LoVR-benchmark", "AI": {"tldr": "LoVR\u662f\u4e00\u4e2a\u9488\u5bf9\u957f\u89c6\u9891-\u6587\u672c\u68c0\u7d22\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5305\u542b467\u4e2a\u957f\u89c6\u9891\u548c40,804\u4e2a\u7ec6\u7c92\u5ea6\u7247\u6bb5\uff0c\u63d0\u4f9b\u9ad8\u8d28\u91cf\u6807\u6ce8\uff0c\u5e76\u63d0\u51fa\u81ea\u52a8\u6807\u6ce8\u751f\u6210\u6846\u67b6\u548c\u8bed\u4e49\u878d\u5408\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6570\u636e\u96c6\u5728\u89c6\u9891\u65f6\u957f\u3001\u6807\u6ce8\u8d28\u91cf\u548c\u7c92\u5ea6\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u9ad8\u7ea7\u89c6\u9891-\u6587\u672c\u68c0\u7d22\u65b9\u6cd5\u7684\u8bc4\u4f30\u3002", "method": "\u63d0\u51fa\u9ad8\u6548\u7684\u6807\u6ce8\u751f\u6210\u6846\u67b6\uff08VLM\u81ea\u52a8\u751f\u6210\u3001\u8d28\u91cf\u8bc4\u5206\u548c\u52a8\u6001\u4f18\u5316\uff09\u548c\u8bed\u4e49\u878d\u5408\u65b9\u6cd5\uff0c\u751f\u6210\u8fde\u8d2f\u7684\u5168\u89c6\u9891\u6807\u6ce8\u3002", "result": "LoVR\u6570\u636e\u96c6\u4e3a\u89c6\u9891\u7406\u89e3\u548c\u68c0\u7d22\u5e26\u6765\u65b0\u6311\u6218\uff0c\u5b9e\u9a8c\u663e\u793a\u5f53\u524d\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\u3002", "conclusion": "LoVR\u662f\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9d\u8d35\u89c1\u89e3\u3002"}}
{"id": "2505.13508", "pdf": "https://arxiv.org/pdf/2505.13508", "abs": "https://arxiv.org/abs/2505.13508", "authors": ["Zijia Liu", "Peixuan Han", "Haofei Yu", "Haoru Li", "Jiaxuan You"], "title": "Time-R1: Towards Comprehensive Temporal Reasoning in LLMs", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) demonstrate impressive capabilities but lack\nrobust temporal intelligence, struggling to integrate reasoning about the past\nwith predictions and plausible generations of the future. Meanwhile, existing\nmethods typically target isolated temporal skills, such as question answering\nabout past events or basic forecasting, and exhibit poor generalization,\nparticularly when dealing with events beyond their knowledge cutoff or\nrequiring creative foresight. To address these limitations, we introduce\n\\textit{Time-R1}, the first framework to endow a moderate-sized (3B-parameter)\nLLM with comprehensive temporal abilities: understanding, prediction, and\ncreative generation. Our approach features a novel three-stage development\npath; the first two constitute a \\textit{reinforcement learning (RL)\ncurriculum} driven by a meticulously designed dynamic rule-based reward system.\nThis framework progressively builds (1) foundational temporal understanding and\nlogical event-time mappings from historical data, (2) future event prediction\nskills for events beyond its knowledge cutoff, and finally (3) enables\nremarkable generalization to creative future scenario generation without any\nfine-tuning. Strikingly, experiments demonstrate that Time-R1 outperforms\nmodels over 200 times larger, including the state-of-the-art 671B DeepSeek-R1,\non highly challenging future event prediction and creative scenario generation\nbenchmarks. This work provides strong evidence that thoughtfully engineered,\nprogressive RL fine-tuning allows smaller, efficient models to achieve superior\ntemporal performance, offering a practical and scalable path towards truly\ntime-aware AI. To foster further research, we also release \\textit{Time-Bench},\na large-scale multi-task temporal reasoning dataset derived from 10 years of\nnews data, and our series of \\textit{Time-R1} checkpoints.", "AI": {"tldr": "Time-R1\u6846\u67b6\u901a\u8fc7\u4e09\u9636\u6bb5\u5f3a\u5316\u5b66\u4e60\u8bfe\u7a0b\uff0c\u8d4b\u4e88\u4e2d\u7b49\u89c4\u6a21LLM\u5168\u9762\u65f6\u95f4\u80fd\u529b\uff0c\u4f18\u4e8e\u66f4\u5927\u6a21\u578b\u3002", "motivation": "\u73b0\u6709LLM\u7f3a\u4e4f\u7a33\u5065\u7684\u65f6\u95f4\u667a\u80fd\uff0c\u73b0\u6709\u65b9\u6cd5\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u65e0\u6cd5\u5904\u7406\u77e5\u8bc6\u622a\u6b62\u540e\u4e8b\u4ef6\u6216\u521b\u9020\u6027\u524d\u77bb\u3002", "method": "\u91c7\u7528\u4e09\u9636\u6bb5RL\u8bfe\u7a0b\uff0c\u9010\u6b65\u57f9\u517b\u65f6\u95f4\u7406\u89e3\u3001\u9884\u6d4b\u548c\u521b\u9020\u6027\u751f\u6210\u80fd\u529b\u3002", "result": "Time-R1\u5728\u9884\u6d4b\u548c\u751f\u6210\u4efb\u52a1\u4e0a\u4f18\u4e8e200\u500d\u5927\u6a21\u578b\uff0c\u5982671B DeepSeek-R1\u3002", "conclusion": "\u7cbe\u5fc3\u8bbe\u8ba1\u7684RL\u5fae\u8c03\u4f7f\u5c0f\u6a21\u578b\u5177\u5907\u5353\u8d8a\u65f6\u95f4\u6027\u80fd\uff0c\u63a8\u52a8\u65f6\u95f4\u611f\u77e5AI\u53d1\u5c55\u3002"}}
{"id": "2505.13943", "pdf": "https://arxiv.org/pdf/2505.13943", "abs": "https://arxiv.org/abs/2505.13943", "authors": ["Samee Arif", "Sualeha Farid"], "title": "Every Pixel Tells a Story: End-to-End Urdu Newspaper OCR", "categories": ["cs.CV"], "comment": null, "summary": "This paper introduces a comprehensive end-to-end pipeline for Optical\nCharacter Recognition (OCR) on Urdu newspapers. In our approach, we address the\nunique challenges of complex multi-column layouts, low-resolution archival\nscans, and diverse font styles. Our process decomposes the OCR task into four\nkey modules: (1) article segmentation, (2) image super-resolution, (3) column\nsegmentation, and (4) text recognition. For article segmentation, we fine-tune\nand evaluate YOLOv11x to identify and separate individual articles from\ncluttered layouts. Our model achieves a precision of 0.963 and mAP@50 of 0.975.\nFor super-resolution, we fine-tune and benchmark the SwinIR model (reaching\n32.71 dB PSNR) to enhance the quality of degraded newspaper scans. To do our\ncolumn segmentation, we use YOLOv11x to separate columns in text to further\nenhance performance - this model reaches a precision of 0.970 and mAP@50 of\n0.975. In the text recognition stage, we benchmark a range of LLMs from\ndifferent families, including Gemini, GPT, Llama, and Claude. The lowest WER of\n0.133 is achieved by Gemini-2.5-Pro.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u4e4c\u5c14\u90fd\u8bed\u62a5\u7eb8\u7684\u7aef\u5230\u7aefOCR\u6d41\u7a0b\uff0c\u89e3\u51b3\u4e86\u591a\u680f\u5e03\u5c40\u3001\u4f4e\u5206\u8fa8\u7387\u626b\u63cf\u548c\u591a\u6837\u5b57\u4f53\u7b49\u6311\u6218\uff0c\u901a\u8fc7\u56db\u4e2a\u6a21\u5757\uff08\u6587\u7ae0\u5206\u5272\u3001\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u3001\u680f\u5206\u5272\u548c\u6587\u672c\u8bc6\u522b\uff09\u5b9e\u73b0\u9ad8\u6548\u8bc6\u522b\u3002", "motivation": "\u4e4c\u5c14\u90fd\u8bed\u62a5\u7eb8OCR\u9762\u4e34\u591a\u680f\u5e03\u5c40\u590d\u6742\u3001\u626b\u63cf\u8d28\u91cf\u4f4e\u548c\u5b57\u4f53\u591a\u6837\u7b49\u95ee\u9898\uff0c\u4e9f\u9700\u4e00\u79cd\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u56db\u6a21\u5757\u6d41\u7a0b\uff1a1) YOLOv11x\u8fdb\u884c\u6587\u7ae0\u5206\u5272\uff1b2) SwinIR\u6a21\u578b\u63d0\u5347\u56fe\u50cf\u5206\u8fa8\u7387\uff1b3) YOLOv11x\u5206\u5272\u680f\uff1b4) \u6d4b\u8bd5\u591a\u79cdLLM\uff08\u5982Gemini\u3001GPT\uff09\u8fdb\u884c\u6587\u672c\u8bc6\u522b\u3002", "result": "\u6587\u7ae0\u5206\u5272\u7cbe\u5ea60.963\uff0c\u8d85\u5206\u8fa8\u7387PSNR 32.71 dB\uff0c\u680f\u5206\u5272\u7cbe\u5ea60.970\uff0cGemini-2.5-Pro\u7684WER\u6700\u4f4e\u4e3a0.133\u3002", "conclusion": "\u63d0\u51fa\u7684\u7aef\u5230\u7aef\u6d41\u7a0b\u5728\u4e4c\u5c14\u90fd\u8bed\u62a5\u7eb8OCR\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5404\u6a21\u5757\u5747\u8fbe\u5230\u9ad8\u7cbe\u5ea6\uff0cGemini-2.5-Pro\u5728\u6587\u672c\u8bc6\u522b\u4e2d\u8868\u73b0\u6700\u4f73\u3002"}}
{"id": "2505.13514", "pdf": "https://arxiv.org/pdf/2505.13514", "abs": "https://arxiv.org/abs/2505.13514", "authors": ["Shuxun Wang", "Qingyu Yin", "Chak Tou Leong", "Qiang Zhang", "Linyi Yang"], "title": "Induction Head Toxicity Mechanistically Explains Repetition Curse in Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Repetition curse is a phenomenon where Large Language Models (LLMs) generate\nrepetitive sequences of tokens or cyclic sequences. While the repetition curse\nhas been widely observed, its underlying mechanisms remain poorly understood.\nIn this work, we investigate the role of induction heads--a specific type of\nattention head known for their ability to perform in-context learning--in\ndriving this repetitive behavior. Specifically, we focus on the \"toxicity\" of\ninduction heads, which we define as their tendency to dominate the model's\noutput logits during repetition, effectively excluding other attention heads\nfrom contributing to the generation process. Our findings have important\nimplications for the design and training of LLMs. By identifying induction\nheads as a key driver of the repetition curse, we provide a mechanistic\nexplanation for this phenomenon and suggest potential avenues for mitigation.\nWe also propose a technique with attention head regularization that could be\nemployed to reduce the dominance of induction heads during generation, thereby\npromoting more diverse and coherent outputs.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e2d\u7684\u5f52\u7eb3\u5934\uff08induction heads\uff09\u662f\u5bfc\u81f4\u91cd\u590d\u8bc5\u5492\uff08repetition curse\uff09\u7684\u4e3b\u8981\u673a\u5236\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u6ce8\u610f\u529b\u5934\u6b63\u5219\u5316\u6280\u672f\u6765\u7f13\u89e3\u8fd9\u4e00\u95ee\u9898\u3002", "motivation": "\u91cd\u590d\u8bc5\u5492\u73b0\u8c61\u5728LLMs\u4e2d\u5e7f\u6cdb\u5b58\u5728\uff0c\u4f46\u5176\u673a\u5236\u5c1a\u4e0d\u660e\u786e\uff0c\u7814\u7a76\u65e8\u5728\u63ed\u793a\u5176\u80cc\u540e\u7684\u539f\u56e0\u5e76\u63d0\u4f9b\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u5206\u6790\u5f52\u7eb3\u5934\u7684\u6bd2\u6027\uff08\u5373\u5176\u5728\u91cd\u590d\u751f\u6210\u8fc7\u7a0b\u4e2d\u4e3b\u5bfc\u8f93\u51falogits\u7684\u503e\u5411\uff09\uff0c\u5e76\u63d0\u51fa\u6ce8\u610f\u529b\u5934\u6b63\u5219\u5316\u6280\u672f\u3002", "result": "\u5f52\u7eb3\u5934\u662f\u91cd\u590d\u8bc5\u5492\u7684\u5173\u952e\u9a71\u52a8\u56e0\u7d20\uff0c\u6b63\u5219\u5316\u6280\u672f\u53ef\u51cf\u5c11\u5176\u4e3b\u5bfc\u6027\uff0c\u63d0\u5347\u751f\u6210\u591a\u6837\u6027\u3002", "conclusion": "\u7814\u7a76\u4e3aLLMs\u7684\u8bbe\u8ba1\u548c\u8bad\u7ec3\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u901a\u8fc7\u8c03\u63a7\u5f52\u7eb3\u5934\u884c\u4e3a\u53ef\u6539\u5584\u6a21\u578b\u8f93\u51fa\u8d28\u91cf\u3002"}}
{"id": "2505.13997", "pdf": "https://arxiv.org/pdf/2505.13997", "abs": "https://arxiv.org/abs/2505.13997", "authors": ["Huaijie Wang", "De Cheng", "Guozhang Li", "Zhipeng Xu", "Lingfeng He", "Jie Li", "Nannan Wang", "Xinbo Gao"], "title": "StPR: Spatiotemporal Preservation and Routing for Exemplar-Free Video Class-Incremental Learning", "categories": ["cs.CV"], "comment": null, "summary": "Video Class-Incremental Learning (VCIL) seeks to develop models that\ncontinuously learn new action categories over time without forgetting\npreviously acquired knowledge. Unlike traditional Class-Incremental Learning\n(CIL), VCIL introduces the added complexity of spatiotemporal structures,\nmaking it particularly challenging to mitigate catastrophic forgetting while\neffectively capturing both frame-shared semantics and temporal dynamics.\nExisting approaches either rely on exemplar rehearsal, raising concerns over\nmemory and privacy, or adapt static image-based methods that neglect temporal\nmodeling. To address these limitations, we propose Spatiotemporal Preservation\nand Routing (StPR), a unified and exemplar-free VCIL framework that explicitly\ndisentangles and preserves spatiotemporal information. First, we introduce\nFrame-Shared Semantics Distillation (FSSD), which identifies semantically\nstable and meaningful channels by jointly considering semantic sensitivity and\nclassification contribution. These important semantic channels are selectively\nregularized to maintain prior knowledge while allowing for adaptation. Second,\nwe design a Temporal Decomposition-based Mixture-of-Experts (TD-MoE), which\ndynamically routes task-specific experts based on their temporal dynamics,\nenabling inference without task ID or stored exemplars. Together, StPR\neffectively leverages spatial semantics and temporal dynamics, achieving a\nunified, exemplar-free VCIL framework. Extensive experiments on UCF101, HMDB51,\nand Kinetics400 show that our method outperforms existing baselines while\noffering improved interpretability and efficiency in VCIL. Code is available in\nthe supplementary materials.", "AI": {"tldr": "StPR\u662f\u4e00\u79cd\u65e0\u9700\u793a\u4f8b\u7684\u89c6\u9891\u7c7b\u589e\u91cf\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u79bb\u65f6\u7a7a\u4fe1\u606f\u5e76\u52a8\u6001\u8def\u7531\u4efb\u52a1\u4e13\u5bb6\uff0c\u6709\u6548\u89e3\u51b3\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\u3002", "motivation": "\u89c6\u9891\u7c7b\u589e\u91cf\u5b66\u4e60\uff08VCIL\uff09\u9762\u4e34\u65f6\u7a7a\u7ed3\u6784\u7684\u590d\u6742\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u793a\u4f8b\u6216\u5ffd\u7565\u65f6\u5e8f\u5efa\u6a21\uff0cStPR\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\u3002", "method": "\u63d0\u51faFrame-Shared Semantics Distillation\uff08FSSD\uff09\u548cTemporal Decomposition-based Mixture-of-Experts\uff08TD-MoE\uff09\uff0c\u5206\u522b\u5904\u7406\u7a7a\u95f4\u8bed\u4e49\u548c\u65f6\u5e8f\u52a8\u6001\u3002", "result": "\u5728UCF101\u3001HMDB51\u548cKinetics400\u4e0a\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\uff0c\u540c\u65f6\u63d0\u5347\u4e86\u89e3\u91ca\u6027\u548c\u6548\u7387\u3002", "conclusion": "StPR\u4e3aVCIL\u63d0\u4f9b\u4e86\u4e00\u79cd\u7edf\u4e00\u4e14\u65e0\u9700\u793a\u4f8b\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u6548\u5e73\u8861\u4e86\u77e5\u8bc6\u4fdd\u7559\u548c\u65f6\u5e8f\u5efa\u6a21\u3002"}}
{"id": "2505.13527", "pdf": "https://arxiv.org/pdf/2505.13527", "abs": "https://arxiv.org/abs/2505.13527", "authors": ["Jingyu Peng", "Maolin Wang", "Nan Wang", "Xiangyu Zhao", "Jiatong Li", "Kai Zhang", "Qi Liu"], "title": "Logic Jailbreak: Efficiently Unlocking LLM Safety Restrictions Through Formal Logical Expression", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Despite substantial advancements in aligning large language models (LLMs)\nwith human values, current safety mechanisms remain susceptible to jailbreak\nattacks. We hypothesize that this vulnerability stems from distributional\ndiscrepancies between alignment-oriented prompts and malicious prompts. To\ninvestigate this, we introduce LogiBreak, a novel and universal black-box\njailbreak method that leverages logical expression translation to circumvent\nLLM safety systems. By converting harmful natural language prompts into formal\nlogical expressions, LogiBreak exploits the distributional gap between\nalignment data and logic-based inputs, preserving the underlying semantic\nintent and readability while evading safety constraints. We evaluate LogiBreak\non a multilingual jailbreak dataset spanning three languages, demonstrating its\neffectiveness across various evaluation settings and linguistic contexts.", "AI": {"tldr": "LogiBreak\u662f\u4e00\u79cd\u5229\u7528\u903b\u8f91\u8868\u8fbe\u5f0f\u7ffb\u8bd1\u7ed5\u8fc7LLM\u5b89\u5168\u7cfb\u7edf\u7684\u9ed1\u76d2\u8d8a\u72f1\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u6709\u5bb3\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u8f6c\u6362\u4e3a\u5f62\u5f0f\u903b\u8f91\u8868\u8fbe\u5f0f\uff0c\u5229\u7528\u5bf9\u9f50\u6570\u636e\u548c\u903b\u8f91\u8f93\u5165\u4e4b\u95f4\u7684\u5206\u5e03\u5dee\u5f02\uff0c\u6709\u6548\u89c4\u907f\u5b89\u5168\u7ea6\u675f\u3002", "motivation": "\u5c3d\u7ba1LLM\u4e0e\u4eba\u7c7b\u4ef7\u503c\u89c2\u5bf9\u9f50\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u73b0\u6709\u5b89\u5168\u673a\u5236\u4ecd\u6613\u53d7\u8d8a\u72f1\u653b\u51fb\uff0c\u4f5c\u8005\u8ba4\u4e3a\u8fd9\u662f\u7531\u4e8e\u5bf9\u9f50\u5bfc\u5411\u63d0\u793a\u4e0e\u6076\u610f\u63d0\u793a\u4e4b\u95f4\u7684\u5206\u5e03\u5dee\u5f02\u5bfc\u81f4\u7684\u3002", "method": "\u63d0\u51faLogiBreak\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u6709\u5bb3\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u8f6c\u6362\u4e3a\u5f62\u5f0f\u903b\u8f91\u8868\u8fbe\u5f0f\uff0c\u5229\u7528\u5206\u5e03\u5dee\u5f02\u89c4\u907f\u5b89\u5168\u7cfb\u7edf\u3002", "result": "\u5728\u591a\u8bed\u8a00\u8d8a\u72f1\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86LogiBreak\u7684\u6709\u6548\u6027\uff0c\u8bc1\u660e\u5176\u5728\u591a\u79cd\u8bc4\u4f30\u8bbe\u7f6e\u548c\u8bed\u8a00\u73af\u5883\u4e2d\u5747\u80fd\u6210\u529f\u3002", "conclusion": "LogiBreak\u63ed\u793a\u4e86LLM\u5b89\u5168\u673a\u5236\u5728\u903b\u8f91\u8f93\u5165\u4e0a\u7684\u8106\u5f31\u6027\uff0c\u4e3a\u672a\u6765\u5b89\u5168\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6539\u8fdb\u65b9\u5411\u3002"}}
{"id": "2505.14008", "pdf": "https://arxiv.org/pdf/2505.14008", "abs": "https://arxiv.org/abs/2505.14008", "authors": ["Zhidan Liu", "Chengtang Yao", "Jiaxi Zeng", "Yuwei Wu", "Yunde Jia"], "title": "Multi-Label Stereo Matching for Transparent Scene Depth Estimation", "categories": ["cs.CV"], "comment": null, "summary": "In this paper, we present a multi-label stereo matching method to\nsimultaneously estimate the depth of the transparent objects and the occluded\nbackground in transparent scenes.Unlike previous methods that assume a unimodal\ndistribution along the disparity dimension and formulate the matching as a\nsingle-label regression problem, we propose a multi-label regression\nformulation to estimate multiple depth values at the same pixel in transparent\nscenes. To resolve the multi-label regression problem, we introduce a\npixel-wise multivariate Gaussian representation, where the mean vector encodes\nmultiple depth values at the same pixel, and the covariance matrix determines\nwhether a multi-label representation is necessary for a given pixel. The\nrepresentation is iteratively predicted within a GRU framework. In each\niteration, we first predict the update step for the mean parameters and then\nuse both the update step and the updated mean parameters to estimate the\ncovariance matrix. We also synthesize a dataset containing 10 scenes and 89\nobjects to validate the performance of transparent scene depth estimation. The\nexperiments show that our method greatly improves the performance on\ntransparent surfaces while preserving the background information for scene\nreconstruction. Code is available at https://github.com/BFZD233/TranScene.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u591a\u6807\u7b7e\u7acb\u4f53\u5339\u914d\u65b9\u6cd5\uff0c\u7528\u4e8e\u540c\u65f6\u4f30\u8ba1\u900f\u660e\u573a\u666f\u4e2d\u900f\u660e\u7269\u4f53\u548c\u88ab\u906e\u6321\u80cc\u666f\u7684\u6df1\u5ea6\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5047\u8bbe\u89c6\u5dee\u7ef4\u5ea6\u4e3a\u5355\u6a21\u6001\u5206\u5e03\uff0c\u5c06\u5339\u914d\u95ee\u9898\u89c6\u4e3a\u5355\u6807\u7b7e\u56de\u5f52\uff0c\u65e0\u6cd5\u5904\u7406\u900f\u660e\u573a\u666f\u4e2d\u7684\u591a\u6df1\u5ea6\u503c\u95ee\u9898\u3002", "method": "\u91c7\u7528\u591a\u6807\u7b7e\u56de\u5f52\u6846\u67b6\uff0c\u5f15\u5165\u50cf\u7d20\u7ea7\u591a\u5143\u9ad8\u65af\u8868\u793a\uff0c\u901a\u8fc7GRU\u8fed\u4ee3\u9884\u6d4b\u5747\u503c\u548c\u534f\u65b9\u5dee\u77e9\u9635\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u900f\u660e\u8868\u9762\u7684\u6df1\u5ea6\u4f30\u8ba1\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u80cc\u666f\u4fe1\u606f\u3002", "conclusion": "\u591a\u6807\u7b7e\u56de\u5f52\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u900f\u660e\u573a\u666f\u4e2d\u7684\u591a\u6df1\u5ea6\u503c\u4f30\u8ba1\u95ee\u9898\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2505.13554", "pdf": "https://arxiv.org/pdf/2505.13554", "abs": "https://arxiv.org/abs/2505.13554", "authors": ["Zhanglin Wu", "Daimeng Wei", "Xiaoyu Chen", "Hengchao Shang", "Jiaxin Guo", "Zongyao Li", "Yuanchang Luo", "Jinlong Yang", "Zhiqiang Rao", "Hao Yang"], "title": "Combining the Best of Both Worlds: A Method for Hybrid NMT and LLM Translation", "categories": ["cs.CL", "cs.AI"], "comment": "9 pages, 2 figures, 9 tables, ACL 2025", "summary": "Large language model (LLM) shows promising performances in a variety of\ndownstream tasks, such as machine translation (MT). However, using LLMs for\ntranslation suffers from high computational costs and significant latency.\nBased on our evaluation, in most cases, translations using LLMs are comparable\nto that generated by neural machine translation (NMT) systems. Only in\nparticular scenarios, LLM and NMT models show respective advantages. As a\nresult, integrating NMT and LLM for translation and using LLM only when\nnecessary seems to be a sound solution. A scheduling policy that optimizes\ntranslation result while ensuring fast speed and as little LLM usage as\npossible is thereby required. We compare several scheduling policies and\npropose a novel and straightforward decider that leverages source sentence\nfeatures. We conduct extensive experiments on multilingual test sets and the\nresult shows that we can achieve optimal translation performance with minimal\nLLM usage, demonstrating effectiveness of our decider.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u548c\u795e\u7ecf\u673a\u5668\u7ffb\u8bd1\uff08NMT\uff09\u7cfb\u7edf\uff0c\u901a\u8fc7\u8c03\u5ea6\u7b56\u7565\u4f18\u5316\u7ffb\u8bd1\u6548\u679c\uff0c\u51cf\u5c11LLM\u7684\u4f7f\u7528\u3002", "motivation": "LLM\u5728\u7ffb\u8bd1\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u800cNMT\u7cfb\u7edf\u6548\u7387\u66f4\u9ad8\u3002\u7814\u7a76\u65e8\u5728\u627e\u5230\u4e24\u8005\u7ed3\u5408\u7684\u6700\u4f73\u65b9\u6848\uff0c\u4ee5\u5e73\u8861\u6027\u80fd\u548c\u6548\u7387\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6e90\u53e5\u7279\u5f81\u7684\u8c03\u5ea6\u7b56\u7565\uff0c\u51b3\u5b9a\u4f55\u65f6\u4f7f\u7528LLM\u6216NMT\uff0c\u5e76\u901a\u8fc7\u591a\u8bed\u8a00\u6d4b\u8bd5\u96c6\u9a8c\u8bc1\u5176\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u7b56\u7565\u80fd\u4ee5\u6700\u5c11\u7684LLM\u4f7f\u7528\u5b9e\u73b0\u6700\u4f18\u7ffb\u8bd1\u6027\u80fd\u3002", "conclusion": "\u7ed3\u5408LLM\u548cNMT\u7684\u8c03\u5ea6\u7b56\u7565\u662f\u9ad8\u6548\u4e14\u6709\u6548\u7684\u7ffb\u8bd1\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.14010", "pdf": "https://arxiv.org/pdf/2505.14010", "abs": "https://arxiv.org/abs/2505.14010", "authors": ["Pu Wang", "Pengwen Dai", "Chen Wu", "Yeying Jin", "Dianjie Lu", "Guijuan Zhang", "Youshan Zhang", "Zhuoran Zheng"], "title": "UHD Image Dehazing via anDehazeFormer with Atmospheric-aware KV Cache", "categories": ["cs.CV"], "comment": "Under review", "summary": "In this paper, we propose an efficient visual transformer framework for\nultra-high-definition (UHD) image dehazing that addresses the key challenges of\nslow training speed and high memory consumption for existing methods. Our\napproach introduces two key innovations: 1) an \\textbf{a}daptive\n\\textbf{n}ormalization mechanism inspired by the nGPT architecture that enables\nultra-fast and stable training with a network with a restricted range of\nparameter expressions; and 2) we devise an atmospheric scattering-aware KV\ncaching mechanism that dynamically optimizes feature preservation based on the\nphysical haze formation model. The proposed architecture improves the training\nconvergence speed by \\textbf{5 $\\times$} while reducing memory overhead,\nenabling real-time processing of 50 high-resolution images per second on an\nRTX4090 GPU. Experimental results show that our approach maintains\nstate-of-the-art dehazing quality while significantly improving computational\nefficiency for 4K/8K image restoration tasks. Furthermore, we provide a new\ndehazing image interpretable method with the help of an integrated gradient\nattribution map. Our code can be found here:\nhttps://anonymous.4open.science/r/anDehazeFormer-632E/README.md.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u89c6\u89c9Transformer\u6846\u67b6\uff0c\u7528\u4e8e\u8d85\u9ad8\u6e05\u56fe\u50cf\u53bb\u96fe\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u8bad\u7ec3\u901f\u5ea6\u6162\u548c\u5185\u5b58\u6d88\u8017\u9ad8\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u53bb\u96fe\u65b9\u6cd5\u5728\u8d85\u9ad8\u6e05\u56fe\u50cf\u5904\u7406\u4e2d\u9762\u4e34\u8bad\u7ec3\u901f\u5ea6\u6162\u548c\u5185\u5b58\u6d88\u8017\u9ad8\u7684\u6311\u6218\uff0c\u4e9f\u9700\u4e00\u79cd\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5f15\u5165\u81ea\u9002\u5e94\u5f52\u4e00\u5316\u673a\u5236\u548c\u5927\u6c14\u6563\u5c04\u611f\u77e5\u7684KV\u7f13\u5b58\u673a\u5236\uff0c\u4f18\u5316\u8bad\u7ec3\u901f\u5ea6\u548c\u5185\u5b58\u4f7f\u7528\u3002", "result": "\u8bad\u7ec3\u901f\u5ea6\u63d0\u53475\u500d\uff0c\u5185\u5b58\u5f00\u9500\u964d\u4f4e\uff0c\u652f\u6301\u5b9e\u65f6\u5904\u740650\u5f20\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf/\u79d2\uff0c\u540c\u65f6\u4fdd\u6301\u53bb\u96fe\u8d28\u91cf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u57284K/8K\u56fe\u50cf\u6062\u590d\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u8ba1\u7b97\u6548\u7387\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u53ef\u89e3\u91ca\u53bb\u96fe\u65b9\u6cd5\u3002"}}
{"id": "2505.13559", "pdf": "https://arxiv.org/pdf/2505.13559", "abs": "https://arxiv.org/abs/2505.13559", "authors": ["Sathya Krishnan Suresh", "Tanmay Surana", "Lim Zhi Hao", "Eng Siong Chng"], "title": "CS-Sum: A Benchmark for Code-Switching Dialogue Summarization and the Limits of Large Language Models", "categories": ["cs.CL", "cs.LG"], "comment": "17 pages, 5 figures and 11 tables", "summary": "Code-switching (CS) poses a significant challenge for Large Language Models\n(LLMs), yet its comprehensibility remains underexplored in LLMs. We introduce\nCS-Sum, to evaluate the comprehensibility of CS by the LLMs through CS dialogue\nto English summarization. CS-Sum is the first benchmark for CS dialogue\nsummarization across Mandarin-English (EN-ZH), Tamil-English (EN-TA), and\nMalay-English (EN-MS), with 900-1300 human-annotated dialogues per language\npair. Evaluating ten LLMs, including open and closed-source models, we analyze\nperformance across few-shot, translate-summarize, and fine-tuning (LoRA, QLoRA\non synthetic data) approaches. Our findings show that though the scores on\nautomated metrics are high, LLMs make subtle mistakes that alter the complete\nmeaning of the dialogue. To this end, we introduce 3 most common type of errors\nthat LLMs make when handling CS input. Error rates vary across CS pairs and\nLLMs, with some LLMs showing more frequent errors on certain language pairs,\nunderscoring the need for specialized training on code-switched data.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86CS-Sum\uff0c\u9996\u4e2a\u8de8\u8bed\u8a00\uff08\u4e2d\u82f1\u3001\u6cf0\u7c73\u5c14-\u82f1\u3001\u9a6c\u6765-\u82f1\uff09\u7684\u4ee3\u7801\u5207\u6362\u5bf9\u8bdd\u6458\u8981\u57fa\u51c6\uff0c\u8bc4\u4f30\u4e86\u5341\u79cd\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5c3d\u7ba1\u81ea\u52a8\u8bc4\u5206\u9ad8\uff0c\u4f46\u6a21\u578b\u5728\u5904\u7406\u4ee3\u7801\u5207\u6362\u65f6\u4ecd\u5b58\u5728\u8bed\u4e49\u9519\u8bef\u3002", "motivation": "\u63a2\u7d22\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u4ee3\u7801\u5207\u6362\uff08CS\uff09\u7684\u7406\u89e3\u80fd\u529b\uff0c\u586b\u8865\u8be5\u9886\u57df\u7684\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u901a\u8fc7CS-Sum\u57fa\u51c6\uff0c\u91c7\u7528\u5c11\u6837\u672c\u3001\u7ffb\u8bd1-\u6458\u8981\u548c\u5fae\u8c03\uff08LoRA\u3001QLoRA\uff09\u7b49\u65b9\u6cd5\u8bc4\u4f30\u5341\u79cd\u5927\u8bed\u8a00\u6a21\u578b\u3002", "result": "\u6a21\u578b\u5728\u81ea\u52a8\u6307\u6807\u4e0a\u5f97\u5206\u9ad8\uff0c\u4f46\u4f1a\u72af\u7ec6\u5fae\u9519\u8bef\uff0c\u5bfc\u81f4\u8bed\u4e49\u5b8c\u5168\u6539\u53d8\u3002\u9519\u8bef\u7387\u56e0\u8bed\u8a00\u5bf9\u548c\u6a21\u578b\u800c\u5f02\u3002", "conclusion": "\u9700\u9488\u5bf9\u4ee3\u7801\u5207\u6362\u6570\u636e\u8fdb\u884c\u4e13\u95e8\u8bad\u7ec3\uff0c\u4ee5\u63d0\u9ad8\u6a21\u578b\u5904\u7406\u80fd\u529b\u3002"}}
{"id": "2505.14014", "pdf": "https://arxiv.org/pdf/2505.14014", "abs": "https://arxiv.org/abs/2505.14014", "authors": ["Zelin Zhang", "Tao Zhang", "KediLI", "Xu Zheng"], "title": "EGFormer: Towards Efficient and Generalizable Multimodal Semantic Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Recent efforts have explored multimodal semantic segmentation using various\nbackbone architectures. However, while most methods aim to improve accuracy,\ntheir computational efficiency remains underexplored. To address this, we\npropose EGFormer, an efficient multimodal semantic segmentation framework that\nflexibly integrates an arbitrary number of modalities while significantly\nreducing model parameters and inference time without sacrificing performance.\nOur framework introduces two novel modules. First, the Any-modal Scoring Module\n(ASM) assigns importance scores to each modality independently, enabling\ndynamic ranking based on their feature maps. Second, the Modal Dropping Module\n(MDM) filters out less informative modalities at each stage, selectively\npreserving and aggregating only the most valuable features. This design allows\nthe model to leverage useful information from all available modalities while\ndiscarding redundancy, thus ensuring high segmentation quality. In addition to\nefficiency, we evaluate EGFormer on a synthetic-to-real transfer task to\ndemonstrate its generalizability. Extensive experiments show that EGFormer\nachieves competitive performance with up to 88 percent reduction in parameters\nand 50 percent fewer GFLOPs. Under unsupervised domain adaptation settings, it\nfurther achieves state-of-the-art transfer performance compared to existing\nmethods.", "AI": {"tldr": "EGFormer\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u591a\u6a21\u6001\u8bed\u4e49\u5206\u5272\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u8bc4\u5206\u548c\u6a21\u6001\u4e22\u5f03\u6a21\u5757\u51cf\u5c11\u53c2\u6570\u548c\u8ba1\u7b97\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u591a\u5173\u6ce8\u51c6\u786e\u6027\u800c\u5ffd\u7565\u8ba1\u7b97\u6548\u7387\uff0cEGFormer\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51faASM\u52a8\u6001\u8bc4\u5206\u6a21\u5757\u548cMDM\u6a21\u6001\u4e22\u5f03\u6a21\u5757\uff0c\u7075\u6d3b\u6574\u5408\u591a\u6a21\u6001\u5e76\u51cf\u5c11\u5197\u4f59\u3002", "result": "\u53c2\u6570\u51cf\u5c1188%\uff0c\u8ba1\u7b97\u91cf\u51cf\u5c1150%\uff0c\u5728\u65e0\u76d1\u7763\u57df\u9002\u5e94\u4efb\u52a1\u4e2d\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "EGFormer\u5728\u9ad8\u6548\u6027\u548c\u901a\u7528\u6027\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4e3a\u591a\u6a21\u6001\u8bed\u4e49\u5206\u5272\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.13628", "pdf": "https://arxiv.org/pdf/2505.13628", "abs": "https://arxiv.org/abs/2505.13628", "authors": ["Nathaniel Krasner", "Nicholas Lanuzo", "Antonios Anastasopoulos"], "title": "Cross-Lingual Representation Alignment Through Contrastive Image-Caption Tuning", "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 Main Conference", "summary": "Multilingual alignment of sentence representations has mostly required\nbitexts to bridge the gap between languages. We investigate whether visual\ninformation can bridge this gap instead. Image caption datasets are very easy\nto create without requiring multilingual expertise, so this offers a more\nefficient alternative for low-resource languages. We find that multilingual\nimage-caption alignment can implicitly align the text representations between\nlanguages, languages unseen by the encoder in pretraining can be incorporated\ninto this alignment post-hoc, and these aligned representations are usable for\ncross-lingual Natural Language Understanding (NLU) and bitext retrieval.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u89c6\u89c9\u4fe1\u606f\u662f\u5426\u80fd\u66ff\u4ee3\u53cc\u8bed\u6587\u672c\u5b9e\u73b0\u591a\u8bed\u8a00\u53e5\u5b50\u8868\u5f81\u5bf9\u9f50\uff0c\u53d1\u73b0\u591a\u8bed\u8a00\u56fe\u50cf-\u6807\u9898\u5bf9\u9f50\u53ef\u9690\u5f0f\u5bf9\u9f50\u6587\u672c\u8868\u5f81\uff0c\u4e14\u9002\u7528\u4e8e\u8de8\u8bed\u8a00\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u548c\u53cc\u8bed\u68c0\u7d22\u3002", "motivation": "\u53cc\u8bed\u6587\u672c\u5bf9\u9f50\u591a\u8bed\u8a00\u53e5\u5b50\u8868\u5f81\u6210\u672c\u9ad8\uff0c\u800c\u56fe\u50cf\u6807\u9898\u6570\u636e\u96c6\u6613\u4e8e\u521b\u5efa\uff0c\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\u63d0\u4f9b\u9ad8\u6548\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u5229\u7528\u591a\u8bed\u8a00\u56fe\u50cf-\u6807\u9898\u5bf9\u9f50\u9690\u5f0f\u5bf9\u9f50\u6587\u672c\u8868\u5f81\uff0c\u5e76\u6d4b\u8bd5\u5176\u5728\u8de8\u8bed\u8a00\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u548c\u53cc\u8bed\u68c0\u7d22\u4e2d\u7684\u5e94\u7528\u3002", "result": "\u591a\u8bed\u8a00\u56fe\u50cf-\u6807\u9898\u5bf9\u9f50\u53ef\u9690\u5f0f\u5bf9\u9f50\u6587\u672c\u8868\u5f81\uff0c\u672a\u5728\u9884\u8bad\u7ec3\u4e2d\u89c1\u8fc7\u7684\u8bed\u8a00\u4e5f\u80fd\u88ab\u7eb3\u5165\u5bf9\u9f50\uff0c\u4e14\u5bf9\u9f50\u8868\u5f81\u9002\u7528\u4e8e\u8de8\u8bed\u8a00\u4efb\u52a1\u3002", "conclusion": "\u89c6\u89c9\u4fe1\u606f\u53ef\u66ff\u4ee3\u53cc\u8bed\u6587\u672c\u5b9e\u73b0\u591a\u8bed\u8a00\u53e5\u5b50\u8868\u5f81\u5bf9\u9f50\uff0c\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\u63d0\u4f9b\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.14028", "pdf": "https://arxiv.org/pdf/2505.14028", "abs": "https://arxiv.org/abs/2505.14028", "authors": ["Ye Wang", "Ruiqi Liu", "Jiang Lin", "Fei Liu", "Zili Yi", "Yilin Wang", "Rui Ma"], "title": "OmniStyle: Filtering High Quality Style Transfer Data at Scale", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "In this paper, we introduce OmniStyle-1M, a large-scale paired style transfer\ndataset comprising over one million content-style-stylized image triplets\nacross 1,000 diverse style categories, each enhanced with textual descriptions\nand instruction prompts. We show that OmniStyle-1M can not only enable\nefficient and scalable of style transfer models through supervised training but\nalso facilitate precise control over target stylization. Especially, to ensure\nthe quality of the dataset, we introduce OmniFilter, a comprehensive style\ntransfer quality assessment framework, which filters high-quality triplets\nbased on content preservation, style consistency, and aesthetic appeal.\nBuilding upon this foundation, we propose OmniStyle, a framework based on the\nDiffusion Transformer (DiT) architecture designed for high-quality and\nefficient style transfer. This framework supports both instruction-guided and\nimage-guided style transfer, generating high resolution outputs with\nexceptional detail. Extensive qualitative and quantitative evaluations\ndemonstrate OmniStyle's superior performance compared to existing approaches,\nhighlighting its efficiency and versatility. OmniStyle-1M and its accompanying\nmethodologies provide a significant contribution to advancing high-quality\nstyle transfer, offering a valuable resource for the research community.", "AI": {"tldr": "OmniStyle-1M\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u7684\u98ce\u683c\u8fc1\u79fb\u6570\u636e\u96c6\uff0c\u5305\u542b100\u4e07\u5bf9\u5185\u5bb9-\u98ce\u683c-\u98ce\u683c\u5316\u56fe\u50cf\u4e09\u5143\u7ec4\uff0c\u652f\u6301\u9ad8\u6548\u8bad\u7ec3\u548c\u7cbe\u786e\u63a7\u5236\u3002OmniFilter\u786e\u4fdd\u6570\u636e\u8d28\u91cf\uff0cOmniStyle\u6846\u67b6\u57fa\u4e8eDiT\u67b6\u6784\uff0c\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u98ce\u683c\u8fc1\u79fb\u9886\u57df\u7f3a\u4e4f\u5927\u89c4\u6a21\u3001\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u7684\u95ee\u9898\uff0c\u540c\u65f6\u652f\u6301\u7cbe\u786e\u63a7\u5236\u548c\u9ad8\u6548\u8bad\u7ec3\u3002", "method": "\u63d0\u51faOmniFilter\u8bc4\u4f30\u6846\u67b6\u7b5b\u9009\u9ad8\u8d28\u91cf\u6570\u636e\uff0c\u57fa\u4e8eDiT\u67b6\u6784\u8bbe\u8ba1OmniStyle\u6846\u67b6\uff0c\u652f\u6301\u6307\u4ee4\u548c\u56fe\u50cf\u5f15\u5bfc\u7684\u98ce\u683c\u8fc1\u79fb\u3002", "result": "OmniStyle\u5728\u8d28\u91cf\u548c\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u751f\u6210\u9ad8\u5206\u8fa8\u7387\u3001\u7ec6\u8282\u4e30\u5bcc\u7684\u8f93\u51fa\u3002", "conclusion": "OmniStyle-1M\u548c\u65b9\u6cd5\u4e3a\u9ad8\u8d28\u91cf\u98ce\u683c\u8fc1\u79fb\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u8d44\u6e90\uff0c\u63a8\u52a8\u4e86\u9886\u57df\u53d1\u5c55\u3002"}}
{"id": "2505.13657", "pdf": "https://arxiv.org/pdf/2505.13657", "abs": "https://arxiv.org/abs/2505.13657", "authors": ["Charles J. Torres", "Richard Futrell"], "title": "Clarifying orthography: Orthographic transparency as compressibility", "categories": ["cs.CL", "cs.IT", "math.IT"], "comment": null, "summary": "Orthographic transparency -- how directly spelling is related to sound --\nlacks a unified, script-agnostic metric. Using ideas from algorithmic\ninformation theory, we quantify orthographic transparency in terms of the\nmutual compressibility between orthographic and phonological strings. Our\nmeasure provides a principled way to combine two factors that decrease\northographic transparency, capturing both irregular spellings and rule\ncomplexity in one quantity. We estimate our transparency measure using\nprequential code-lengths derived from neural sequence models. Evaluating 22\nlanguages across a broad range of script types (alphabetic, abjad, abugida,\nsyllabic, logographic) confirms common intuitions about relative transparency\nof scripts. Mutual compressibility offers a simple, principled, and general\nyardstick for orthographic transparency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7b97\u6cd5\u4fe1\u606f\u7406\u8bba\u7684\u8de8\u6587\u5b57\u7cfb\u7edf\u5ea6\u91cf\u65b9\u6cd5\uff0c\u7528\u4e8e\u91cf\u5316\u62fc\u5199\u4e0e\u53d1\u97f3\u4e4b\u95f4\u7684\u900f\u660e\u5ea6\u3002", "motivation": "\u7f3a\u4e4f\u4e00\u79cd\u7edf\u4e00\u7684\u3001\u4e0e\u6587\u5b57\u7cfb\u7edf\u65e0\u5173\u7684\u5ea6\u91cf\u65b9\u6cd5\u6765\u8861\u91cf\u62fc\u5199\u4e0e\u53d1\u97f3\u7684\u76f4\u63a5\u5173\u7cfb\u3002", "method": "\u5229\u7528\u7b97\u6cd5\u4fe1\u606f\u7406\u8bba\u4e2d\u7684\u4e92\u538b\u7f29\u6027\u6982\u5ff5\uff0c\u7ed3\u5408\u795e\u7ecf\u5e8f\u5217\u6a21\u578b\u7684\u524d\u5e8f\u7f16\u7801\u957f\u5ea6\uff0c\u91cf\u5316\u62fc\u5199\u4e0e\u53d1\u97f3\u7684\u900f\u660e\u5ea6\u3002", "result": "\u572822\u79cd\u4e0d\u540c\u6587\u5b57\u7cfb\u7edf\uff08\u5982\u5b57\u6bcd\u3001\u8f85\u97f3\u97f3\u7d20\u3001\u5143\u97f3\u9644\u6807\u3001\u97f3\u8282\u3001\u8868\u610f\u6587\u5b57\uff09\u4e2d\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u7ed3\u679c\u7b26\u5408\u5bf9\u6587\u5b57\u900f\u660e\u5ea6\u7684\u666e\u904d\u76f4\u89c9\u3002", "conclusion": "\u4e92\u538b\u7f29\u6027\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u3001\u539f\u5219\u6027\u5f3a\u4e14\u901a\u7528\u7684\u62fc\u5199\u900f\u660e\u5ea6\u5ea6\u91cf\u6807\u51c6\u3002"}}
{"id": "2505.14029", "pdf": "https://arxiv.org/pdf/2505.14029", "abs": "https://arxiv.org/abs/2505.14029", "authors": ["Laura-Sophia von Hirschhausen", "Jannes S. Magnusson", "Mykyta Kovalenko", "Fredrik Boye", "Tanay Rawat", "Peter Eisert", "Anna Hilsmann", "Sebastian Pretzsch", "Sebastian Bosse"], "title": "AppleGrowthVision: A large-scale stereo dataset for phenological analysis, fruit detection, and 3D reconstruction in apple orchards", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Deep learning has transformed computer vision for precision agriculture, yet\napple orchard monitoring remains limited by dataset constraints. The lack of\ndiverse, realistic datasets and the difficulty of annotating dense,\nheterogeneous scenes. Existing datasets overlook different growth stages and\nstereo imagery, both essential for realistic 3D modeling of orchards and tasks\nlike fruit localization, yield estimation, and structural analysis. To address\nthese gaps, we present AppleGrowthVision, a large-scale dataset comprising two\nsubsets. The first includes 9,317 high resolution stereo images collected from\na farm in Brandenburg (Germany), covering six agriculturally validated growth\nstages over a full growth cycle. The second subset consists of 1,125 densely\nannotated images from the same farm in Brandenburg and one in Pillnitz\n(Germany), containing a total of 31,084 apple labels. AppleGrowthVision\nprovides stereo-image data with agriculturally validated growth stages,\nenabling precise phenological analysis and 3D reconstructions. Extending\nMinneApple with our data improves YOLOv8 performance by 7.69 % in terms of\nF1-score, while adding it to MinneApple and MAD boosts Faster R-CNN F1-score by\n31.06 %. Additionally, six BBCH stages were predicted with over 95 % accuracy\nusing VGG16, ResNet152, DenseNet201, and MobileNetv2. AppleGrowthVision bridges\nthe gap between agricultural science and computer vision, by enabling the\ndevelopment of robust models for fruit detection, growth modeling, and 3D\nanalysis in precision agriculture. Future work includes improving annotation,\nenhancing 3D reconstruction, and extending multimodal analysis across all\ngrowth stages.", "AI": {"tldr": "AppleGrowthVision\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u586b\u8865\u4e86\u82f9\u679c\u56ed\u76d1\u6d4b\u4e2d\u6570\u636e\u591a\u6837\u6027\u548c\u7acb\u4f53\u56fe\u50cf\u7684\u7a7a\u767d\uff0c\u63d0\u5347\u4e86\u76ee\u6807\u68c0\u6d4b\u548c\u751f\u957f\u9636\u6bb5\u9884\u6d4b\u7684\u6027\u80fd\u3002", "motivation": "\u82f9\u679c\u56ed\u76d1\u6d4b\u56e0\u6570\u636e\u96c6\u9650\u5236\uff08\u5982\u7f3a\u4e4f\u591a\u6837\u6027\u548c\u7acb\u4f53\u56fe\u50cf\uff09\u800c\u53d7\u9650\uff0c\u5f71\u54cd\u4e863D\u5efa\u6a21\u548c\u4efb\u52a1\uff08\u5982\u6c34\u679c\u5b9a\u4f4d\u548c\u4ea7\u91cf\u4f30\u8ba1\uff09\u3002", "method": "\u63d0\u51faAppleGrowthVision\u6570\u636e\u96c6\uff0c\u5305\u542b\u9ad8\u5206\u8fa8\u7387\u7acb\u4f53\u56fe\u50cf\u548c\u5bc6\u96c6\u6807\u6ce8\u56fe\u50cf\uff0c\u8986\u76d6\u591a\u4e2a\u751f\u957f\u9636\u6bb5\u3002", "result": "\u6570\u636e\u96c6\u663e\u8457\u63d0\u5347\u4e86YOLOv8\u548cFaster R-CNN\u7684\u6027\u80fd\uff0c\u751f\u957f\u9636\u6bb5\u9884\u6d4b\u51c6\u786e\u7387\u8d85\u8fc795%\u3002", "conclusion": "AppleGrowthVision\u8fde\u63a5\u4e86\u519c\u4e1a\u79d1\u5b66\u4e0e\u8ba1\u7b97\u673a\u89c6\u89c9\uff0c\u4e3a\u7cbe\u51c6\u519c\u4e1a\u63d0\u4f9b\u4e86\u5f3a\u5927\u5de5\u5177\u3002\u672a\u6765\u5de5\u4f5c\u5305\u62ec\u6539\u8fdb\u6807\u6ce8\u548c3D\u91cd\u5efa\u3002"}}
{"id": "2505.13706", "pdf": "https://arxiv.org/pdf/2505.13706", "abs": "https://arxiv.org/abs/2505.13706", "authors": ["Julia Jose", "Rachel Greenstadt"], "title": "Are Large Language Models Good at Detecting Propaganda?", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Propagandists use rhetorical devices that rely on logical fallacies and\nemotional appeals to advance their agendas. Recognizing these techniques is key\nto making informed decisions. Recent advances in Natural Language Processing\n(NLP) have enabled the development of systems capable of detecting manipulative\ncontent. In this study, we look at several Large Language Models and their\nperformance in detecting propaganda techniques in news articles. We compare the\nperformance of these LLMs with transformer-based models. We find that, while\nGPT-4 demonstrates superior F1 scores (F1=0.16) compared to GPT-3.5 and Claude\n3 Opus, it does not outperform a RoBERTa-CRF baseline (F1=0.67). Additionally,\nwe find that all three LLMs outperform a MultiGranularity Network (MGN)\nbaseline in detecting instances of one out of six propaganda techniques\n(name-calling), with GPT-3.5 and GPT-4 also outperforming the MGN baseline in\ndetecting instances of appeal to fear and flag-waving.", "AI": {"tldr": "\u7814\u7a76\u6bd4\u8f83\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08\u5982GPT-4\u3001GPT-3.5\u548cClaude 3 Opus\uff09\u4e0e\u57fa\u4e8eTransformer\u7684\u6a21\u578b\u5728\u68c0\u6d4b\u65b0\u95fb\u6587\u7ae0\u4e2d\u7684\u5ba3\u4f20\u6280\u672f\u65b9\u9762\u7684\u6027\u80fd\u3002GPT-4\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6LLM\uff0c\u4f46\u672a\u8d85\u8fc7RoBERTa-CRF\u57fa\u7ebf\u3002\u90e8\u5206\u5ba3\u4f20\u6280\u672f\u68c0\u6d4b\u4e2d\uff0cLLM\u4f18\u4e8e\u591a\u7c92\u5ea6\u7f51\u7edc\uff08MGN\uff09\u57fa\u7ebf\u3002", "motivation": "\u5ba3\u4f20\u6280\u672f\u901a\u8fc7\u903b\u8f91\u8c2c\u8bef\u548c\u60c5\u611f\u8bc9\u6c42\u5f71\u54cd\u51b3\u7b56\uff0c\u8bc6\u522b\u8fd9\u4e9b\u6280\u672f\u5bf9\u4fe1\u606f\u5224\u65ad\u81f3\u5173\u91cd\u8981\u3002NLP\u7684\u8fdb\u6b65\u4e3a\u5f00\u53d1\u68c0\u6d4b\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u80fd\u3002", "method": "\u6bd4\u8f83\u4e86\u591a\u79cd\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e0e\u57fa\u4e8eTransformer\u7684\u6a21\u578b\u5728\u68c0\u6d4b\u5ba3\u4f20\u6280\u672f\u65b9\u9762\u7684\u6027\u80fd\uff0c\u8bc4\u4f30\u4e86F1\u5206\u6570\u7b49\u6307\u6807\u3002", "result": "GPT-4\u5728F1\u5206\u6570\u4e0a\u4f18\u4e8e\u5176\u4ed6LLM\uff08F1=0.16\uff09\uff0c\u4f46\u672a\u8d85\u8fc7RoBERTa-CRF\u57fa\u7ebf\uff08F1=0.67\uff09\u3002\u90e8\u5206\u5ba3\u4f20\u6280\u672f\u68c0\u6d4b\u4e2d\uff0cLLM\u4f18\u4e8eMGN\u57fa\u7ebf\u3002", "conclusion": "\u5c3d\u7ba1GPT-4\u5728LLM\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u4f46\u4f20\u7edf\u6a21\u578b\uff08\u5982RoBERTa-CRF\uff09\u5728\u5ba3\u4f20\u6280\u672f\u68c0\u6d4b\u4e0a\u4ecd\u66f4\u4f18\u3002LLM\u5728\u7279\u5b9a\u5ba3\u4f20\u6280\u672f\u68c0\u6d4b\u4e2d\u6709\u6f5c\u529b\u3002"}}
{"id": "2505.14043", "pdf": "https://arxiv.org/pdf/2505.14043", "abs": "https://arxiv.org/abs/2505.14043", "authors": ["Qianqian Zhang", "WeiJun Wang", "Yunxing Liu", "Li Zhou", "Hao Zhao", "Junshe An", "Zihan Wang"], "title": "Selective Structured State Space for Multispectral-fused Small Target Detection", "categories": ["cs.CV"], "comment": null, "summary": "Target detection in high-resolution remote sensing imagery faces challenges\ndue to the low recognition accuracy of small targets and high computational\ncosts. The computational complexity of the Transformer architecture increases\nquadratically with image resolution, while Convolutional Neural Networks (CNN)\narchitectures are forced to stack deeper convolutional layers to expand their\nreceptive fields, leading to an explosive growth in computational demands. To\naddress these computational constraints, we leverage Mamba's linear complexity\nfor efficiency. However, Mamba's performance declines for small targets,\nprimarily because small targets occupy a limited area in the image and have\nlimited semantic information. Accurate identification of these small targets\nnecessitates not only Mamba's global attention capabilities but also the\nprecise capture of fine local details. To this end, we enhance Mamba by\ndeveloping the Enhanced Small Target Detection (ESTD) module and the\nConvolutional Attention Residual Gate (CARG) module. The ESTD module bolsters\nlocal attention to capture fine-grained details, while the CARG module, built\nupon Mamba, emphasizes spatial and channel-wise information, collectively\nimproving the model's ability to capture distinctive representations of small\ntargets. Additionally, to highlight the semantic representation of small\ntargets, we design a Mask Enhanced Pixel-level Fusion (MEPF) module for\nmultispectral fusion, which enhances target features by effectively fusing\nvisible and infrared multimodal information.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eMamba\u67b6\u6784\u7684\u6539\u8fdb\u65b9\u6cd5\uff0c\u901a\u8fc7ESTD\u548cCARG\u6a21\u5757\u589e\u5f3a\u5c0f\u76ee\u6807\u68c0\u6d4b\u80fd\u529b\uff0c\u5e76\u7ed3\u5408MEPF\u6a21\u5757\u8fdb\u884c\u591a\u5149\u8c31\u878d\u5408\uff0c\u4ee5\u89e3\u51b3\u9ad8\u5206\u8fa8\u7387\u9065\u611f\u56fe\u50cf\u4e2d\u5c0f\u76ee\u6807\u8bc6\u522b\u7cbe\u5ea6\u4f4e\u548c\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u95ee\u9898\u3002", "motivation": "\u9ad8\u5206\u8fa8\u7387\u9065\u611f\u56fe\u50cf\u4e2d\u7684\u5c0f\u76ee\u6807\u68c0\u6d4b\u9762\u4e34\u8bc6\u522b\u7cbe\u5ea6\u4f4e\u548c\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u5982Transformer\u548cCNN\u5728\u8ba1\u7b97\u590d\u6742\u5ea6\u6216\u6027\u80fd\u4e0a\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u5229\u7528Mamba\u7684\u7ebf\u6027\u590d\u6742\u5ea6\u63d0\u5347\u6548\u7387\uff0c\u5e76\u901a\u8fc7ESTD\u6a21\u5757\u589e\u5f3a\u5c40\u90e8\u6ce8\u610f\u529b\uff0cCARG\u6a21\u5757\u5f3a\u5316\u7a7a\u95f4\u548c\u901a\u9053\u4fe1\u606f\uff0cMEPF\u6a21\u5757\u5b9e\u73b0\u591a\u5149\u8c31\u878d\u5408\u3002", "result": "\u6539\u8fdb\u540e\u7684\u6a21\u578b\u80fd\u591f\u66f4\u51c6\u786e\u5730\u6355\u6349\u5c0f\u76ee\u6807\u7684\u7ec6\u8282\u548c\u8bed\u4e49\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u5c40\u90e8\u6ce8\u610f\u529b\u3001\u591a\u6a21\u6001\u878d\u5408\u548c\u9ad8\u6548\u67b6\u6784\uff0c\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u5c0f\u76ee\u6807\u68c0\u6d4b\u7684\u96be\u9898\u3002"}}
{"id": "2505.13725", "pdf": "https://arxiv.org/pdf/2505.13725", "abs": "https://arxiv.org/abs/2505.13725", "authors": ["Yu Guo", "Dong Jin", "Shenghao Ye", "Shuangwu Chen", "Jian Yang", "Xiaobin Tan"], "title": "SQLForge: Synthesizing Reliable and Diverse Data to Enhance Text-to-SQL Reasoning in LLMs", "categories": ["cs.CL"], "comment": "12 pages, 7 figures, accepted to ACL Findings 2025", "summary": "Large Language models (LLMs) have demonstrated significant potential in\ntext-to-SQL reasoning tasks, yet a substantial performance gap persists between\nexisting open-source models and their closed-source counterparts. In this\npaper, we introduce SQLForge, a novel approach for synthesizing reliable and\ndiverse data to enhance text-to-SQL reasoning in LLMs. We improve data\nreliability through SQL syntax constraints and SQL-to-question reverse\ntranslation, ensuring data logic at both structural and semantic levels. We\nalso propose an SQL template enrichment and iterative data domain exploration\nmechanism to boost data diversity. Building on the augmented data, we fine-tune\na variety of open-source models with different architectures and parameter\nsizes, resulting in a family of models termed SQLForge-LM. SQLForge-LM achieves\nthe state-of-the-art performance on the widely recognized Spider and BIRD\nbenchmarks among the open-source models. Specifically, SQLForge-LM achieves EX\naccuracy of 85.7% on Spider Dev and 59.8% on BIRD Dev, significantly narrowing\nthe performance gap with closed-source methods.", "AI": {"tldr": "SQLForge\u901a\u8fc7\u5408\u6210\u53ef\u9760\u4e14\u591a\u6837\u5316\u7684\u6570\u636e\u63d0\u5347LLMs\u5728text-to-SQL\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u663e\u8457\u7f29\u5c0f\u5f00\u6e90\u4e0e\u95ed\u6e90\u6a21\u578b\u7684\u6027\u80fd\u5dee\u8ddd\u3002", "motivation": "\u73b0\u6709\u5f00\u6e90\u6a21\u578b\u5728text-to-SQL\u4efb\u52a1\u4e2d\u4e0e\u95ed\u6e90\u6a21\u578b\u5b58\u5728\u6027\u80fd\u5dee\u8ddd\uff0c\u9700\u6539\u8fdb\u6570\u636e\u8d28\u91cf\u548c\u591a\u6837\u6027\u3002", "method": "\u5f15\u5165SQL\u8bed\u6cd5\u7ea6\u675f\u548cSQL-to-question\u53cd\u5411\u7ffb\u8bd1\u786e\u4fdd\u6570\u636e\u53ef\u9760\u6027\uff0c\u901a\u8fc7\u6a21\u677f\u4e30\u5bcc\u5316\u548c\u8fed\u4ee3\u63a2\u7d22\u63d0\u5347\u591a\u6837\u6027\uff0c\u5e76\u5fae\u8c03\u591a\u79cd\u5f00\u6e90\u6a21\u578b\u3002", "result": "SQLForge-LM\u5728Spider\u548cBIRD\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u5f00\u6e90\u6a21\u578b\u6700\u4f73\u6027\u80fd\uff08Spider Dev 85.7%\uff0cBIRD Dev 59.8%\uff09\u3002", "conclusion": "SQLForge\u6709\u6548\u63d0\u5347\u5f00\u6e90\u6a21\u578b\u6027\u80fd\uff0c\u663e\u8457\u7f29\u5c0f\u4e0e\u95ed\u6e90\u6a21\u578b\u7684\u5dee\u8ddd\u3002"}}
{"id": "2505.14049", "pdf": "https://arxiv.org/pdf/2505.14049", "abs": "https://arxiv.org/abs/2505.14049", "authors": ["Yibo Gao", "Hangqi Zhou", "Zheyao Gao", "Bomin Wang", "Shangqi Gao", "Sihan Wang", "Xiahai Zhuang"], "title": "Learning Concept-Driven Logical Rules for Interpretable and Generalizable Medical Image Classification", "categories": ["cs.CV"], "comment": "early accepted by MICCAI 2025", "summary": "The pursuit of decision safety in clinical applications highlights the\npotential of concept-based methods in medical imaging. While these models offer\nactive interpretability, they often suffer from concept leakages, where\nunintended information within soft concept representations undermines both\ninterpretability and generalizability. Moreover, most concept-based models\nfocus solely on local explanations (instance-level), neglecting the global\ndecision logic (dataset-level). To address these limitations, we propose\nConcept Rule Learner (CRL), a novel framework to learn Boolean logical rules\nfrom binarized visual concepts. CRL employs logical layers to capture concept\ncorrelations and extract clinically meaningful rules, thereby providing both\nlocal and global interpretability. Experiments on two medical image\nclassification tasks show that CRL achieves competitive performance with\nexisting methods while significantly improving generalizability to\nout-of-distribution data. The code of our work is available at\nhttps://github.com/obiyoag/crl.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCRL\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u4e8c\u503c\u5316\u89c6\u89c9\u6982\u5ff5\u5b66\u4e60\u5e03\u5c14\u903b\u8f91\u89c4\u5219\uff0c\u89e3\u51b3\u4e86\u6982\u5ff5\u6cc4\u6f0f\u95ee\u9898\uff0c\u5e76\u63d0\u4f9b\u4e86\u5c40\u90e8\u548c\u5168\u5c40\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u4e34\u5e8a\u5e94\u7528\u4e2d\u51b3\u7b56\u5b89\u5168\u7684\u9700\u6c42\u51f8\u663e\u4e86\u57fa\u4e8e\u6982\u5ff5\u7684\u65b9\u6cd5\u5728\u533b\u5b66\u5f71\u50cf\u4e2d\u7684\u6f5c\u529b\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u6982\u5ff5\u6cc4\u6f0f\u95ee\u9898\u4e14\u4ec5\u5173\u6ce8\u5c40\u90e8\u89e3\u91ca\u3002", "method": "CRL\u6846\u67b6\u5229\u7528\u903b\u8f91\u5c42\u6355\u6349\u6982\u5ff5\u76f8\u5173\u6027\u5e76\u63d0\u53d6\u4e34\u5e8a\u6709\u610f\u4e49\u7684\u89c4\u5219\uff0c\u5b9e\u73b0\u5c40\u90e8\u548c\u5168\u5c40\u53ef\u89e3\u91ca\u6027\u3002", "result": "\u5728\u4e24\u4e2a\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0cCRL\u6027\u80fd\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u5f53\uff0c\u4e14\u663e\u8457\u63d0\u9ad8\u4e86\u5bf9\u5206\u5e03\u5916\u6570\u636e\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "CRL\u901a\u8fc7\u903b\u8f91\u89c4\u5219\u5b66\u4e60\u89e3\u51b3\u4e86\u6982\u5ff5\u6cc4\u6f0f\u95ee\u9898\uff0c\u540c\u65f6\u63d0\u5347\u4e86\u53ef\u89e3\u91ca\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2505.13761", "pdf": "https://arxiv.org/pdf/2505.13761", "abs": "https://arxiv.org/abs/2505.13761", "authors": ["Jacob Kleiman", "Kevin Frank", "Sindy Campagna"], "title": "Simulation Agent: A Framework for Integrating Simulation and Large Language Models for Enhanced Decision-Making", "categories": ["cs.CL"], "comment": null, "summary": "Simulations, although powerful in accurately replicating real-world systems,\noften remain inaccessible to non-technical users due to their complexity.\nConversely, large language models (LLMs) provide intuitive, language-based\ninteractions but can lack the structured, causal understanding required to\nreliably model complex real-world dynamics. We introduce our simulation agent\nframework, a novel approach that integrates the strengths of both simulation\nmodels and LLMs. This framework helps empower users by leveraging the\nconversational capabilities of LLMs to interact seamlessly with sophisticated\nsimulation systems, while simultaneously utilizing the simulations to ground\nthe LLMs in accurate and structured representations of real-world phenomena.\nThis integrated approach helps provide a robust and generalizable foundation\nfor empirical validation and offers broad applicability across diverse domains.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4eff\u771f\u6a21\u578b\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u6846\u67b6\uff0c\u901a\u8fc7LLM\u7684\u5bf9\u8bdd\u80fd\u529b\u7b80\u5316\u4eff\u771f\u7cfb\u7edf\u7684\u4ea4\u4e92\uff0c\u540c\u65f6\u5229\u7528\u4eff\u771f\u4e3aLLM\u63d0\u4f9b\u51c6\u786e\u7684\u7ed3\u6784\u5316\u73b0\u5b9e\u4e16\u754c\u8868\u793a\u3002", "motivation": "\u89e3\u51b3\u4eff\u771f\u7cfb\u7edf\u5bf9\u975e\u6280\u672f\u7528\u6237\u8fc7\u4e8e\u590d\u6742\uff0c\u800cLLM\u7f3a\u4e4f\u7ed3\u6784\u5316\u56e0\u679c\u7406\u89e3\u7684\u95ee\u9898\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u4eff\u771f\u4ee3\u7406\u6846\u67b6\uff0c\u6574\u5408\u4eff\u771f\u6a21\u578b\u548cLLM\u7684\u4f18\u52bf\uff0c\u5b9e\u73b0\u65e0\u7f1d\u4ea4\u4e92\u548c\u7ed3\u6784\u5316\u5efa\u6a21\u3002", "result": "\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7a33\u5065\u4e14\u901a\u7528\u7684\u57fa\u7840\uff0c\u652f\u6301\u8de8\u9886\u57df\u7684\u5b9e\u8bc1\u9a8c\u8bc1\u3002", "conclusion": "\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u4eff\u771f\u7684\u51c6\u786e\u6027\u548cLLM\u7684\u6613\u7528\u6027\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2505.14059", "pdf": "https://arxiv.org/pdf/2505.14059", "abs": "https://arxiv.org/abs/2505.14059", "authors": ["Hao Feng", "Shu Wei", "Xiang Fei", "Wei Shi", "Yingdong Han", "Lei Liao", "Jinghui Lu", "Binghong Wu", "Qi Liu", "Chunhui Lin", "Jingqun Tang", "Hao Liu", "Can Huang"], "title": "Dolphin: Document Image Parsing via Heterogeneous Anchor Prompting", "categories": ["cs.CV"], "comment": "Accepted to ACL 2025", "summary": "Document image parsing is challenging due to its complexly intertwined\nelements such as text paragraphs, figures, formulas, and tables. Current\napproaches either assemble specialized expert models or directly generate\npage-level content autoregressively, facing integration overhead, efficiency\nbottlenecks, and layout structure degradation despite their decent performance.\nTo address these limitations, we present \\textit{Dolphin}\n(\\textit{\\textbf{Do}cument Image \\textbf{P}arsing via \\textbf{H}eterogeneous\nAnchor Prompt\\textbf{in}g}), a novel multimodal document image parsing model\nfollowing an analyze-then-parse paradigm. In the first stage, Dolphin generates\na sequence of layout elements in reading order. These heterogeneous elements,\nserving as anchors and coupled with task-specific prompts, are fed back to\nDolphin for parallel content parsing in the second stage. To train Dolphin, we\nconstruct a large-scale dataset of over 30 million samples, covering\nmulti-granularity parsing tasks. Through comprehensive evaluations on both\nprevalent benchmarks and self-constructed ones, Dolphin achieves\nstate-of-the-art performance across diverse page-level and element-level\nsettings, while ensuring superior efficiency through its lightweight\narchitecture and parallel parsing mechanism. The code and pre-trained models\nare publicly available at https://github.com/ByteDance/Dolphin", "AI": {"tldr": "Dolphin\u662f\u4e00\u79cd\u65b0\u578b\u591a\u6a21\u6001\u6587\u6863\u56fe\u50cf\u89e3\u6790\u6a21\u578b\uff0c\u91c7\u7528\u201c\u5148\u5206\u6790\u540e\u89e3\u6790\u201d\u8303\u5f0f\uff0c\u901a\u8fc7\u5f02\u6784\u951a\u70b9\u63d0\u793a\u5b9e\u73b0\u9ad8\u6548\u5e76\u884c\u89e3\u6790\uff0c\u6027\u80fd\u4f18\u8d8a\u4e14\u8f7b\u91cf\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u96c6\u6210\u5f00\u9500\u3001\u6548\u7387\u74f6\u9888\u548c\u5e03\u5c40\u7ed3\u6784\u9000\u5316\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "method": "\u5206\u4e24\u9636\u6bb5\uff1a\u9996\u5148\u751f\u6210\u5e03\u5c40\u5143\u7d20\u5e8f\u5217\u4f5c\u4e3a\u951a\u70b9\uff0c\u518d\u7ed3\u5408\u4efb\u52a1\u63d0\u793a\u5e76\u884c\u89e3\u6790\u5185\u5bb9\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u4f18\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6548\u3002", "conclusion": "Dolphin\u901a\u8fc7\u8f7b\u91cf\u67b6\u6784\u548c\u5e76\u884c\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6587\u6863\u56fe\u50cf\u89e3\u6790\u7684\u6548\u7387\u548c\u6027\u80fd\u3002"}}
{"id": "2505.13772", "pdf": "https://arxiv.org/pdf/2505.13772", "abs": "https://arxiv.org/abs/2505.13772", "authors": ["Dimitris Roussis", "Leon Voukoutis", "Georgios Paraskevopoulos", "Sokratis Sofianopoulos", "Prokopis Prokopidis", "Vassilis Papavasileiou", "Athanasios Katsamanis", "Stelios Piperidis", "Vassilis Katsouros"], "title": "Krikri: Advancing Open Large Language Models for Greek", "categories": ["cs.CL"], "comment": null, "summary": "We introduce Llama-Krikri-8B, a cutting-edge Large Language Model tailored\nfor the Greek language, built on Meta's Llama 3.1-8B. Llama-Krikri-8B has been\nextensively trained on high-quality Greek data to ensure superior adaptation to\nlinguistic nuances. With 8 billion parameters, it offers advanced capabilities\nwhile maintaining efficient computational performance. Llama-Krikri-8B supports\nboth Modern Greek and English, and is also equipped to handle polytonic text\nand Ancient Greek. The chat version of Llama-Krikri-8B features a multi-stage\npost-training pipeline, utilizing both human and synthetic instruction and\npreference data, by applying techniques such as MAGPIE. In addition, for\nevaluation, we propose three novel public benchmarks for Greek. Our evaluation\non existing as well as the proposed benchmarks shows notable improvements over\ncomparable Greek and multilingual LLMs in both natural language understanding\nand generation as well as code generation.", "AI": {"tldr": "Llama-Krikri-8B\u662f\u57fa\u4e8eMeta Llama 3.1-8B\u7684\u5e0c\u814a\u8bed\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u652f\u6301\u73b0\u4ee3\u5e0c\u814a\u8bed\u3001\u82f1\u8bed\u3001\u591a\u8c03\u6587\u672c\u548c\u53e4\u5e0c\u814a\u8bed\uff0c\u6027\u80fd\u4f18\u4e8e\u540c\u7c7b\u6a21\u578b\u3002", "motivation": "\u4e3a\u5e0c\u814a\u8bed\u63d0\u4f9b\u9ad8\u6027\u80fd\u7684\u8bed\u8a00\u6a21\u578b\uff0c\u586b\u8865\u73b0\u6709\u6a21\u578b\u5728\u5e0c\u814a\u8bed\u5904\u7406\u4e0a\u7684\u4e0d\u8db3\u3002", "method": "\u57fa\u4e8eLlama 3.1-8B\u67b6\u6784\uff0c\u4f7f\u7528\u9ad8\u8d28\u91cf\u5e0c\u814a\u8bed\u6570\u636e\u8bad\u7ec3\uff0c\u91c7\u7528\u591a\u9636\u6bb5\u540e\u8bad\u7ec3\u6d41\u7a0b\uff08\u5982MAGPIE\u6280\u672f\uff09\u3002", "result": "\u5728\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u3001\u751f\u6210\u53ca\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u4e8e\u540c\u7c7b\u5e0c\u814a\u8bed\u548c\u591a\u8bed\u8a00\u6a21\u578b\u3002", "conclusion": "Llama-Krikri-8B\u662f\u5e0c\u814a\u8bed\u5904\u7406\u7684\u5148\u8fdb\u5de5\u5177\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2505.14062", "pdf": "https://arxiv.org/pdf/2505.14062", "abs": "https://arxiv.org/abs/2505.14062", "authors": ["Bo Li", "Haoke Xiao", "Lv Tang"], "title": "Scaling Vision Mamba Across Resolutions via Fractal Traversal", "categories": ["cs.CV"], "comment": "Work in progressing", "summary": "Vision Mamba has recently emerged as a promising alternative to\nTransformer-based architectures, offering linear complexity in sequence length\nwhile maintaining strong modeling capacity. However, its adaptation to visual\ninputs is hindered by challenges in 2D-to-1D patch serialization and weak\nscalability across input resolutions. Existing serialization strategies such as\nraster scanning disrupt local spatial continuity and limit the model's ability\nto generalize across scales. In this paper, we propose FractalMamba++, a robust\nvision backbone that leverages fractal-based patch serialization via Hilbert\ncurves to preserve spatial locality and enable seamless resolution\nadaptability. To address long-range dependency fading in high-resolution\ninputs, we further introduce a Cross-State Routing (CSR) mechanism that\nenhances global context propagation through selective state reuse.\nAdditionally, we propose a Positional-Relation Capture (PRC) module to recover\nlocal adjacency disrupted by curve inflection points. Extensive experiments on\nimage classification, semantic segmentation, object detection, and change\ndetection demonstrate that FractalMamba++ consistently outperforms previous\nMamba-based backbones, particularly under high-resolution settings.", "AI": {"tldr": "FractalMamba++\u662f\u4e00\u79cd\u57fa\u4e8e\u5206\u5f62\u5e8f\u5217\u5316\u548c\u72b6\u6001\u8def\u7531\u7684\u89c6\u89c9\u9aa8\u5e72\u7f51\u7edc\uff0c\u89e3\u51b3\u4e86Vision Mamba\u57282D\u52301D\u5e8f\u5217\u5316\u548c\u5206\u8fa8\u7387\u9002\u5e94\u6027\u4e0a\u7684\u95ee\u9898\uff0c\u5e76\u5728\u9ad8\u5206\u8fa8\u7387\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "Vision Mamba\u5728\u89c6\u89c9\u8f93\u5165\u4e2d\u9762\u4e342D\u52301D\u5e8f\u5217\u5316\u7684\u6311\u6218\u548c\u5206\u8fa8\u7387\u9002\u5e94\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u7834\u574f\u4e86\u7a7a\u95f4\u8fde\u7eed\u6027\u3002", "method": "\u63d0\u51fa\u5206\u5f62\u5e8f\u5217\u5316\uff08Hilbert\u66f2\u7ebf\uff09\u4fdd\u6301\u7a7a\u95f4\u5c40\u90e8\u6027\uff0c\u5f15\u5165Cross-State Routing\uff08CSR\uff09\u589e\u5f3a\u5168\u5c40\u4e0a\u4e0b\u6587\u4f20\u64ad\uff0c\u4ee5\u53caPositional-Relation Capture\uff08PRC\uff09\u6a21\u5757\u6062\u590d\u5c40\u90e8\u90bb\u63a5\u5173\u7cfb\u3002", "result": "\u5728\u56fe\u50cf\u5206\u7c7b\u3001\u8bed\u4e49\u5206\u5272\u3001\u76ee\u6807\u68c0\u6d4b\u548c\u53d8\u5316\u68c0\u6d4b\u4efb\u52a1\u4e2d\uff0cFractalMamba++\u8868\u73b0\u4f18\u4e8e\u73b0\u6709Mamba\u9aa8\u5e72\u7f51\u7edc\uff0c\u5c24\u5176\u5728\u9ad8\u5206\u8fa8\u7387\u4e0b\u3002", "conclusion": "FractalMamba++\u901a\u8fc7\u5206\u5f62\u5e8f\u5217\u5316\u548c\u72b6\u6001\u8def\u7531\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86Vision Mamba\u5728\u89c6\u89c9\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u9ad8\u5206\u8fa8\u7387\u573a\u666f\u4e0b\u3002"}}
{"id": "2505.13792", "pdf": "https://arxiv.org/pdf/2505.13792", "abs": "https://arxiv.org/abs/2505.13792", "authors": ["Siddhant Bhambri", "Upasana Biswas", "Subbarao Kambhampati"], "title": "Interpretable Traces, Unexpected Outcomes: Investigating the Disconnect in Trace-Based Knowledge Distillation", "categories": ["cs.CL", "cs.AI"], "comment": "10 pages", "summary": "Question Answering (QA) poses a challenging and critical problem,\nparticularly in today's age of interactive dialogue systems such as ChatGPT,\nPerplexity, Microsoft Copilot, etc. where users demand both accuracy and\ntransparency in the model's outputs. Since smaller language models (SLMs) are\ncomputationally more efficient but often under-perform compared to larger\nmodels, Knowledge Distillation (KD) methods allow for finetuning these smaller\nmodels to improve their final performance. Lately, the intermediate tokens or\nthe so called `reasoning' traces produced by Chain-of-Thought (CoT) or by\nreasoning models such as DeepSeek R1 are used as a training signal for KD.\nHowever, these reasoning traces are often verbose and difficult to interpret or\nevaluate. In this work, we aim to address the challenge of evaluating the\nfaithfulness of these reasoning traces and their correlation with the final\nperformance. To this end, we employ a KD method leveraging rule-based problem\ndecomposition. This approach allows us to break down complex queries into\nstructured sub-problems, generating interpretable traces whose correctness can\nbe readily evaluated, even at inference time. Specifically, we demonstrate this\napproach on Open Book QA, decomposing the problem into a Classification step\nand an Information Retrieval step, thereby simplifying trace evaluation. Our\nSFT experiments with correct and incorrect traces on the CoTemp QA, Microsoft\nMachine Reading Comprehension QA, and Facebook bAbI QA datasets reveal the\nstriking finding that correct traces do not necessarily imply that the model\noutputs the correct final solution. Similarly, we find a low correlation\nbetween correct final solutions and intermediate trace correctness. These\nresults challenge the implicit assumption behind utilizing reasoning traces for\nimproving SLMs' final performance via KD.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5728\u77e5\u8bc6\u84b8\u998f\uff08KD\uff09\u4e2d\u5229\u7528\u63a8\u7406\u75d5\u8ff9\uff08\u5982Chain-of-Thought\uff09\u7684\u6311\u6218\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c4\u5219\u7684\u95ee\u9898\u5206\u89e3\u65b9\u6cd5\uff0c\u4ee5\u751f\u6210\u53ef\u89e3\u91ca\u7684\u75d5\u8ff9\u5e76\u8bc4\u4f30\u5176\u6b63\u786e\u6027\u3002\u5b9e\u9a8c\u53d1\u73b0\u63a8\u7406\u75d5\u8ff9\u7684\u6b63\u786e\u6027\u4e0e\u6700\u7ec8\u7b54\u6848\u7684\u6b63\u786e\u6027\u76f8\u5173\u6027\u8f83\u4f4e\u3002", "motivation": "\u5728\u4ea4\u4e92\u5f0f\u5bf9\u8bdd\u7cfb\u7edf\uff08\u5982ChatGPT\uff09\u4e2d\uff0c\u7528\u6237\u5bf9\u6a21\u578b\u7684\u51c6\u786e\u6027\u548c\u900f\u660e\u5ea6\u8981\u6c42\u9ad8\u3002\u867d\u7136\u77e5\u8bc6\u84b8\u998f\u53ef\u4ee5\u63d0\u5347\u5c0f\u8bed\u8a00\u6a21\u578b\uff08SLMs\uff09\u7684\u6027\u80fd\uff0c\u4f46\u63a8\u7406\u75d5\u8ff9\u7684\u8bc4\u4f30\u56f0\u96be\u4e14\u5176\u4e0e\u6700\u7ec8\u6027\u80fd\u7684\u76f8\u5173\u6027\u4e0d\u660e\u786e\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u89c4\u5219\u7684\u95ee\u9898\u5206\u89e3\u65b9\u6cd5\uff0c\u5c06\u590d\u6742\u67e5\u8be2\u5206\u89e3\u4e3a\u7ed3\u6784\u5316\u5b50\u95ee\u9898\uff08\u5982\u5206\u7c7b\u548c\u4fe1\u606f\u68c0\u7d22\uff09\uff0c\u751f\u6210\u53ef\u89e3\u91ca\u7684\u63a8\u7406\u75d5\u8ff9\u3002\u5728\u591a\u4e2aQA\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\u3002", "result": "\u5b9e\u9a8c\u53d1\u73b0\uff0c\u6b63\u786e\u7684\u63a8\u7406\u75d5\u8ff9\u5e76\u4e0d\u4e00\u5b9a\u4fdd\u8bc1\u6700\u7ec8\u7b54\u6848\u7684\u6b63\u786e\u6027\uff0c\u4e14\u4e24\u8005\u76f8\u5173\u6027\u8f83\u4f4e\u3002", "conclusion": "\u7814\u7a76\u6311\u6218\u4e86\u5229\u7528\u63a8\u7406\u75d5\u8ff9\u63d0\u5347SLMs\u6027\u80fd\u7684\u9690\u542b\u5047\u8bbe\uff0c\u5f3a\u8c03\u4e86\u8bc4\u4f30\u75d5\u8ff9\u5fe0\u5b9e\u6027\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2505.14068", "pdf": "https://arxiv.org/pdf/2505.14068", "abs": "https://arxiv.org/abs/2505.14068", "authors": ["Zhenyu Li", "Tianyi Shang", "Pengjie Xu", "Zhaojun Deng"], "title": "Place Recognition: A Comprehensive Review, Current Challenges and Future Directions", "categories": ["cs.CV"], "comment": "35 pages", "summary": "Place recognition is a cornerstone of vehicle navigation and mapping, which\nis pivotal in enabling systems to determine whether a location has been\npreviously visited. This capability is critical for tasks such as loop closure\nin Simultaneous Localization and Mapping (SLAM) and long-term navigation under\nvarying environmental conditions. In this survey, we comprehensively review\nrecent advancements in place recognition, emphasizing three representative\nmethodological paradigms: Convolutional Neural Network (CNN)-based approaches,\nTransformer-based frameworks, and cross-modal strategies. We begin by\nelucidating the significance of place recognition within the broader context of\nautonomous systems. Subsequently, we trace the evolution of CNN-based methods,\nhighlighting their contributions to robust visual descriptor learning and\nscalability in large-scale environments. We then examine the emerging class of\nTransformer-based models, which leverage self-attention mechanisms to capture\nglobal dependencies and offer improved generalization across diverse scenes.\nFurthermore, we discuss cross-modal approaches that integrate heterogeneous\ndata sources such as Lidar, vision, and text description, thereby enhancing\nresilience to viewpoint, illumination, and seasonal variations. We also\nsummarize standard datasets and evaluation metrics widely adopted in the\nliterature. Finally, we identify current research challenges and outline\nprospective directions, including domain adaptation, real-time performance, and\nlifelong learning, to inspire future advancements in this domain. The unified\nframework of leading-edge place recognition methods, i.e., code library, and\nthe results of their experimental evaluations are available at\nhttps://github.com/CV4RA/SOTA-Place-Recognitioner.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u5730\u70b9\u8bc6\u522b\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u91cd\u70b9\u4ecb\u7ecd\u4e86CNN\u3001Transformer\u548c\u8de8\u6a21\u6001\u65b9\u6cd5\uff0c\u5e76\u603b\u7ed3\u4e86\u6570\u636e\u96c6\u3001\u8bc4\u4f30\u6307\u6807\u53ca\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u5730\u70b9\u8bc6\u522b\u662f\u8f66\u8f86\u5bfc\u822a\u548c\u5730\u56fe\u6784\u5efa\u7684\u5173\u952e\uff0c\u5c24\u5176\u5728SLAM\u548c\u957f\u671f\u5bfc\u822a\u4e2d\u81f3\u5173\u91cd\u8981\u3002\u672c\u6587\u65e8\u5728\u5168\u9762\u56de\u987e\u8be5\u9886\u57df\u7684\u6700\u65b0\u65b9\u6cd5\u3002", "method": "\u7efc\u8ff0\u4e86CNN\u3001Transformer\u548c\u8de8\u6a21\u6001\u4e09\u79cd\u65b9\u6cd5\uff0c\u5206\u6790\u4e86\u5b83\u4eec\u5728\u89c6\u89c9\u63cf\u8ff0\u7b26\u5b66\u4e60\u3001\u5168\u5c40\u4f9d\u8d56\u6355\u83b7\u548c\u591a\u6a21\u6001\u6570\u636e\u6574\u5408\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u603b\u7ed3\u4e86\u6807\u51c6\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6307\u6807\uff0c\u5e76\u63d0\u4f9b\u4e86\u4ee3\u7801\u5e93\u548c\u5b9e\u9a8c\u7ed3\u679c\u3002", "conclusion": "\u6307\u51fa\u4e86\u5f53\u524d\u6311\u6218\uff08\u5982\u9886\u57df\u9002\u5e94\u3001\u5b9e\u65f6\u6027\u80fd\uff09\u548c\u672a\u6765\u65b9\u5411\uff08\u5982\u7ec8\u8eab\u5b66\u4e60\uff09\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u4e86\u53c2\u8003\u3002"}}
{"id": "2505.13840", "pdf": "https://arxiv.org/pdf/2505.13840", "abs": "https://arxiv.org/abs/2505.13840", "authors": ["Zhengqing Yuan", "Weixiang Sun", "Yixin Liu", "Huichi Zhou", "Rong Zhou", "Yiyang Li", "Zheyuan Zhang", "Wei Song", "Yue Huang", "Haolong Jia", "Keerthiram Murugesan", "Yu Wang", "Lifang He", "Jianfeng Gao", "Lichao Sun", "Yanfang Ye"], "title": "EfficientLLM: Efficiency in Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) have driven significant progress, yet their\ngrowing parameter counts and context windows incur prohibitive compute, energy,\nand monetary costs. We introduce EfficientLLM, a novel benchmark and the first\ncomprehensive empirical study evaluating efficiency techniques for LLMs at\nscale. Conducted on a production-class cluster (48xGH200, 8xH200 GPUs), our\nstudy systematically explores three key axes: (1) architecture pretraining\n(efficient attention variants: MQA, GQA, MLA, NSA; sparse Mixture-of-Experts\n(MoE)), (2) fine-tuning (parameter-efficient methods: LoRA, RSLoRA, DoRA), and\n(3) inference (quantization methods: int4, float16). We define six fine-grained\nmetrics (Memory Utilization, Compute Utilization, Latency, Throughput, Energy\nConsumption, Compression Rate) to capture hardware saturation,\nlatency-throughput balance, and carbon cost. Evaluating over 100\nmodel-technique pairs (0.5B-72B parameters), we derive three core insights: (i)\nEfficiency involves quantifiable trade-offs: no single method is universally\noptimal; e.g., MoE reduces FLOPs and improves accuracy but increases VRAM by\n40%, while int4 quantization cuts memory/energy by up to 3.9x at a 3-5%\naccuracy drop. (ii) Optima are task- and scale-dependent: MQA offers optimal\nmemory-latency trade-offs for constrained devices, MLA achieves lowest\nperplexity for quality-critical tasks, and RSLoRA surpasses LoRA efficiency\nonly beyond 14B parameters. (iii) Techniques generalize across modalities: we\nextend evaluations to Large Vision Models (Stable Diffusion 3.5, Wan 2.1) and\nVision-Language Models (Qwen2.5-VL), confirming effective transferability. By\nopen-sourcing datasets, evaluation pipelines, and leaderboards, EfficientLLM\nprovides essential guidance for researchers and engineers navigating the\nefficiency-performance landscape of next-generation foundation models.", "AI": {"tldr": "EfficientLLM\u662f\u4e00\u4e2a\u65b0\u57fa\u51c6\uff0c\u9996\u6b21\u5168\u9762\u8bc4\u4f30\u5927\u89c4\u6a21LLM\u7684\u6548\u7387\u6280\u672f\uff0c\u6db5\u76d6\u67b6\u6784\u9884\u8bad\u7ec3\u3001\u5fae\u8c03\u548c\u63a8\u7406\uff0c\u63ed\u793a\u4e86\u6548\u7387\u4e0e\u6027\u80fd\u7684\u6743\u8861\u3002", "motivation": "\u968f\u7740LLM\u53c2\u6570\u548c\u4e0a\u4e0b\u6587\u7a97\u53e3\u7684\u589e\u52a0\uff0c\u8ba1\u7b97\u3001\u80fd\u6e90\u548c\u6210\u672c\u53d8\u5f97\u4e0d\u53ef\u6301\u7eed\uff0c\u9700\u8981\u7814\u7a76\u9ad8\u6548\u6280\u672f\u3002", "method": "\u572848xGH200\u548c8xH200 GPU\u96c6\u7fa4\u4e0a\uff0c\u7cfb\u7edf\u8bc4\u4f30\u4e86\u9884\u8bad\u7ec3\uff08\u6ce8\u610f\u529b\u53d8\u4f53\u3001\u7a00\u758fMoE\uff09\u3001\u5fae\u8c03\uff08\u53c2\u6570\u9ad8\u6548\u65b9\u6cd5\uff09\u548c\u63a8\u7406\uff08\u91cf\u5316\u65b9\u6cd5\uff09\uff0c\u5b9a\u4e49\u4e86\u516d\u9879\u7ec6\u7c92\u5ea6\u6307\u6807\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u6548\u7387\u5b58\u5728\u91cf\u5316\u6743\u8861\uff0c\u6700\u4f18\u65b9\u6cd5\u56e0\u4efb\u52a1\u548c\u89c4\u6a21\u800c\u5f02\uff0c\u4e14\u6280\u672f\u53ef\u8de8\u6a21\u6001\u63a8\u5e7f\u3002", "conclusion": "EfficientLLM\u4e3a\u4e0b\u4e00\u4ee3\u57fa\u7840\u6a21\u578b\u7684\u6548\u7387-\u6027\u80fd\u6743\u8861\u63d0\u4f9b\u4e86\u91cd\u8981\u6307\u5bfc\uff0c\u5e76\u5f00\u6e90\u4e86\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u5de5\u5177\u3002"}}
{"id": "2505.14088", "pdf": "https://arxiv.org/pdf/2505.14088", "abs": "https://arxiv.org/abs/2505.14088", "authors": ["Xi Chen", "Shen Yan", "Juelin Zhu", "Chen Chen", "Yu Liu", "Maojun Zhang"], "title": "Generalizable Multispectral Land Cover Classification via Frequency-Aware Mixture of Low-Rank Token Experts", "categories": ["cs.CV"], "comment": null, "summary": "We introduce Land-MoE, a novel approach for multispectral land cover\nclassification (MLCC). Spectral shift, which emerges from disparities in\nsensors and geospatial conditions, poses a significant challenge in this\ndomain. Existing methods predominantly rely on domain adaptation and\ngeneralization strategies, often utilizing small-scale models that exhibit\nlimited performance. In contrast, Land-MoE addresses these issues by\nhierarchically inserting a Frequency-aware Mixture of Low-rank Token Experts,\nto fine-tune Vision Foundation Models (VFMs) in a parameter-efficient manner.\nSpecifically, Land-MoE comprises two key modules: the mixture of low-rank token\nexperts (MoLTE) and frequency-aware filters (FAF). MoLTE leverages\nrank-differentiated tokens to generate diverse feature adjustments for\nindividual instances within multispectral images. By dynamically combining\nlearnable low-rank token experts of varying ranks, it enhances the robustness\nagainst spectral shifts. Meanwhile, FAF conducts frequency-domain modulation on\nthe refined features. This process enables the model to effectively capture\nfrequency band information that is strongly correlated with semantic essence,\nwhile simultaneously suppressing frequency noise irrelevant to the task.\nComprehensive experiments on MLCC tasks involving cross-sensor and\ncross-geospatial setups demonstrate that Land-MoE outperforms existing methods\nby a large margin. Additionally, the proposed approach has also achieved\nstate-of-the-art performance in domain generalization semantic segmentation\ntasks of RGB remote sensing images.", "AI": {"tldr": "Land-MoE\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u5149\u8c31\u571f\u5730\u8986\u76d6\u5206\u7c7b\u65b9\u6cd5\uff0c\u901a\u8fc7\u9891\u7387\u611f\u77e5\u7684\u4f4e\u79e9\u6807\u8bb0\u4e13\u5bb6\u6df7\u5408\u6a21\u5757\u548c\u9891\u7387\u57df\u8c03\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5bf9\u5149\u8c31\u504f\u79fb\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u591a\u5149\u8c31\u571f\u5730\u8986\u76d6\u5206\u7c7b\u4e2d\uff0c\u4f20\u611f\u5668\u548c\u5730\u7406\u6761\u4ef6\u5dee\u5f02\u5bfc\u81f4\u7684\u5149\u8c31\u504f\u79fb\u95ee\u9898\u4e9f\u5f85\u89e3\u51b3\u3002\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5c0f\u89c4\u6a21\u6a21\u578b\uff0c\u6027\u80fd\u6709\u9650\u3002", "method": "Land-MoE\u91c7\u7528\u9891\u7387\u611f\u77e5\u7684\u4f4e\u79e9\u6807\u8bb0\u4e13\u5bb6\u6df7\u5408\u6a21\u5757\uff08MoLTE\uff09\u548c\u9891\u7387\u611f\u77e5\u6ee4\u6ce2\u5668\uff08FAF\uff09\uff0c\u9ad8\u6548\u5fae\u8c03\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLand-MoE\u5728\u8de8\u4f20\u611f\u5668\u548c\u8de8\u5730\u7406\u4efb\u52a1\u4e2d\u5927\u5e45\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5728RGB\u9065\u611f\u56fe\u50cf\u7684\u9886\u57df\u6cdb\u5316\u8bed\u4e49\u5206\u5272\u4efb\u52a1\u4e2d\u8fbe\u5230SOTA\u6027\u80fd\u3002", "conclusion": "Land-MoE\u901a\u8fc7\u521b\u65b0\u7684\u6a21\u5757\u8bbe\u8ba1\uff0c\u5728\u591a\u5149\u8c31\u571f\u5730\u8986\u76d6\u5206\u7c7b\u548c\u9886\u57df\u6cdb\u5316\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u76f8\u5173\u9886\u57df\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.13844", "pdf": "https://arxiv.org/pdf/2505.13844", "abs": "https://arxiv.org/abs/2505.13844", "authors": ["Congchi Yin", "Yongpeng Zhang", "Xuyun Wen", "Piji Li"], "title": "Improve Language Model and Brain Alignment via Associative Memory", "categories": ["cs.CL"], "comment": "Accepted by Findings of ACL 2025", "summary": "Associative memory engages in the integration of relevant information for\ncomprehension in the human cognition system. In this work, we seek to improve\nalignment between language models and human brain while processing speech\ninformation by integrating associative memory. After verifying the alignment\nbetween language model and brain by mapping language model activations to brain\nactivity, the original text stimuli expanded with simulated associative memory\nare regarded as input to computational language models. We find the alignment\nbetween language model and brain is improved in brain regions closely related\nto associative memory processing. We also demonstrate large language models\nafter specific supervised fine-tuning better align with brain response, by\nbuilding the \\textit{Association} dataset containing 1000 samples of stories,\nwith instructions encouraging associative memory as input and associated\ncontent as output.", "AI": {"tldr": "\u901a\u8fc7\u6574\u5408\u8054\u60f3\u8bb0\u5fc6\uff0c\u63d0\u9ad8\u8bed\u8a00\u6a21\u578b\u4e0e\u4eba\u7c7b\u5927\u8111\u5728\u8bed\u97f3\u4fe1\u606f\u5904\u7406\u4e2d\u7684\u5bf9\u9f50\u6027\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8054\u60f3\u8bb0\u5fc6\u7684\u5f15\u5165\u6539\u5584\u4e86\u8bed\u8a00\u6a21\u578b\u4e0e\u5927\u8111\u6d3b\u52a8\u7684\u5bf9\u9f50\u6027\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u8054\u60f3\u8bb0\u5fc6\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u4e0e\u4eba\u7c7b\u5927\u8111\u5728\u5904\u7406\u8bed\u97f3\u4fe1\u606f\u65f6\u7684\u5bf9\u9f50\u6027\u3002", "method": "\u901a\u8fc7\u5c06\u8bed\u8a00\u6a21\u578b\u6fc0\u6d3b\u6620\u5c04\u5230\u5927\u8111\u6d3b\u52a8\uff0c\u9a8c\u8bc1\u5bf9\u9f50\u6027\uff1b\u4f7f\u7528\u6a21\u62df\u8054\u60f3\u8bb0\u5fc6\u6269\u5c55\u7684\u6587\u672c\u4f5c\u4e3a\u8f93\u5165\uff1b\u6784\u5efa\u5305\u542b1000\u4e2a\u6545\u4e8b\u6837\u672c\u7684Association\u6570\u636e\u96c6\uff0c\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\u3002", "result": "\u8054\u60f3\u8bb0\u5fc6\u7684\u5f15\u5165\u6539\u5584\u4e86\u8bed\u8a00\u6a21\u578b\u4e0e\u5927\u8111\u6d3b\u52a8\u7684\u5bf9\u9f50\u6027\uff0c\u5c24\u5176\u662f\u5728\u4e0e\u8054\u60f3\u8bb0\u5fc6\u5904\u7406\u5bc6\u5207\u76f8\u5173\u7684\u5927\u8111\u533a\u57df\u3002", "conclusion": "\u8054\u60f3\u8bb0\u5fc6\u7684\u6574\u5408\u548c\u7279\u5b9a\u76d1\u7763\u5fae\u8c03\u53ef\u663e\u8457\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u4e0e\u5927\u8111\u7684\u5bf9\u9f50\u6027\u3002"}}
{"id": "2505.14100", "pdf": "https://arxiv.org/pdf/2505.14100", "abs": "https://arxiv.org/abs/2505.14100", "authors": ["Qianxiong Xu", "Lanyun Zhu", "Xuanyi Liu", "Guosheng Lin", "Cheng Long", "Ziyue Li", "Rui Zhao"], "title": "Unlocking the Power of SAM 2 for Few-Shot Segmentation", "categories": ["cs.CV"], "comment": "This paper is accepted by ICML'25", "summary": "Few-Shot Segmentation (FSS) aims to learn class-agnostic segmentation on few\nclasses to segment arbitrary classes, but at the risk of overfitting. To\naddress this, some methods use the well-learned knowledge of foundation models\n(e.g., SAM) to simplify the learning process. Recently, SAM 2 has extended SAM\nby supporting video segmentation, whose class-agnostic matching ability is\nuseful to FSS. A simple idea is to encode support foreground (FG) features as\nmemory, with which query FG features are matched and fused. Unfortunately, the\nFG objects in different frames of SAM 2's video data are always the same\nidentity, while those in FSS are different identities, i.e., the matching step\nis incompatible. Therefore, we design Pseudo Prompt Generator to encode pseudo\nquery memory, matching with query features in a compatible way. However, the\nmemories can never be as accurate as the real ones, i.e., they are likely to\ncontain incomplete query FG, and some unexpected query background (BG)\nfeatures, leading to wrong segmentation. Hence, we further design Iterative\nMemory Refinement to fuse more query FG features into the memory, and devise a\nSupport-Calibrated Memory Attention to suppress the unexpected query BG\nfeatures in memory. Extensive experiments have been conducted on PASCAL-5$^i$\nand COCO-20$^i$ to validate the effectiveness of our design, e.g., the 1-shot\nmIoU can be 4.2\\% better than the best baseline.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u5c11\u6837\u672c\u5206\u5272\uff08FSS\uff09\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bbe\u8ba1\u4f2a\u63d0\u793a\u751f\u6210\u5668\u548c\u8fed\u4ee3\u8bb0\u5fc6\u7cbe\u70bc\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4e2d\u5339\u914d\u4e0d\u517c\u5bb9\u548c\u5206\u5272\u9519\u8bef\u7684\u95ee\u9898\u3002", "motivation": "\u5c11\u6837\u672c\u5206\u5272\u5bb9\u6613\u8fc7\u62df\u5408\uff0c\u73b0\u6709\u65b9\u6cd5\u5229\u7528\u57fa\u7840\u6a21\u578b\uff08\u5982SAM\uff09\u7684\u77e5\u8bc6\u7b80\u5316\u5b66\u4e60\u8fc7\u7a0b\uff0c\u4f46\u89c6\u9891\u6570\u636e\u4e2d\u7684\u5bf9\u8c61\u8eab\u4efd\u4e0eFSS\u4e0d\u540c\uff0c\u5bfc\u81f4\u5339\u914d\u4e0d\u517c\u5bb9\u3002", "method": "\u8bbe\u8ba1\u4e86\u4f2a\u63d0\u793a\u751f\u6210\u5668\u7f16\u7801\u4f2a\u67e5\u8be2\u8bb0\u5fc6\uff0c\u4ee5\u517c\u5bb9\u65b9\u5f0f\u5339\u914d\u67e5\u8be2\u7279\u5f81\uff1b\u8fdb\u4e00\u6b65\u63d0\u51fa\u8fed\u4ee3\u8bb0\u5fc6\u7cbe\u70bc\u548c\u652f\u6301\u6821\u51c6\u8bb0\u5fc6\u6ce8\u610f\u529b\uff0c\u4f18\u5316\u8bb0\u5fc6\u5185\u5bb9\u3002", "result": "\u5728PASCAL-5$^i$\u548cCOCO-20$^i$\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c1-shot mIoU\u6bd4\u6700\u4f73\u57fa\u7ebf\u63d0\u9ad8\u4e864.2%\u3002", "conclusion": "\u901a\u8fc7\u517c\u5bb9\u6027\u5339\u914d\u548c\u8bb0\u5fc6\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5c11\u6837\u672c\u5206\u5272\u7684\u6027\u80fd\u3002"}}
{"id": "2505.13855", "pdf": "https://arxiv.org/pdf/2505.13855", "abs": "https://arxiv.org/abs/2505.13855", "authors": ["Arihant Tripathi", "Liam Dugan", "Charis Gao", "Maggie Huan", "Emma Jin", "Peter Zhang", "David Zhang", "Julia Zhao", "Chris Callison-Burch"], "title": "Domain Gating Ensemble Networks for AI-Generated Text Detection", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Submitted to EMNLP 2025", "summary": "As state-of-the-art language models continue to improve, the need for robust\ndetection of machine-generated text becomes increasingly critical. However,\ncurrent state-of-the-art machine text detectors struggle to adapt to new unseen\ndomains and generative models. In this paper we present DoGEN (Domain Gating\nEnsemble Networks), a technique that allows detectors to adapt to unseen\ndomains by ensembling a set of domain expert detector models using weights from\na domain classifier. We test DoGEN on a wide variety of domains from leading\nbenchmarks and find that it achieves state-of-the-art performance on in-domain\ndetection while outperforming models twice its size on out-of-domain detection.\nWe release our code and trained models to assist in future research in\ndomain-adaptive AI detection.", "AI": {"tldr": "DoGEN\u662f\u4e00\u79cd\u901a\u8fc7\u96c6\u6210\u9886\u57df\u4e13\u5bb6\u68c0\u6d4b\u6a21\u578b\u548c\u9886\u57df\u5206\u7c7b\u5668\u6743\u91cd\u6765\u9002\u5e94\u672a\u89c1\u9886\u57df\u7684\u673a\u5668\u751f\u6210\u6587\u672c\u68c0\u6d4b\u6280\u672f\uff0c\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u968f\u7740\u8bed\u8a00\u6a21\u578b\u7684\u8fdb\u6b65\uff0c\u68c0\u6d4b\u673a\u5668\u751f\u6210\u6587\u672c\u7684\u9700\u6c42\u65e5\u76ca\u8feb\u5207\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u9002\u5e94\u65b0\u9886\u57df\u548c\u751f\u6210\u6a21\u578b\u3002", "method": "\u63d0\u51faDoGEN\u6280\u672f\uff0c\u901a\u8fc7\u96c6\u6210\u9886\u57df\u4e13\u5bb6\u68c0\u6d4b\u6a21\u578b\u548c\u9886\u57df\u5206\u7c7b\u5668\u6743\u91cd\uff0c\u5b9e\u73b0\u5bf9\u65b0\u9886\u57df\u7684\u9002\u5e94\u3002", "result": "\u5728\u591a\u4e2a\u9886\u57df\u6d4b\u8bd5\u4e2d\uff0cDoGEN\u5728\u57df\u5185\u68c0\u6d4b\u4e2d\u8fbe\u5230\u6700\u4f18\u6027\u80fd\uff0c\u5728\u57df\u5916\u68c0\u6d4b\u4e2d\u4f18\u4e8e\u66f4\u5927\u6a21\u578b\u3002", "conclusion": "DoGEN\u4e3a\u9886\u57df\u81ea\u9002\u5e94AI\u68c0\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u516c\u5f00\u4ee3\u7801\u548c\u6a21\u578b\u4ee5\u652f\u6301\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2505.14105", "pdf": "https://arxiv.org/pdf/2505.14105", "abs": "https://arxiv.org/abs/2505.14105", "authors": ["Zs\u00f3fia Moln\u00e1r", "Gergely Szab\u00f3", "Andr\u00e1s Horv\u00e1th"], "title": "Unintended Bias in 2D+ Image Segmentation and Its Effect on Attention Asymmetry", "categories": ["cs.CV"], "comment": null, "summary": "Supervised pretrained models have become widely used in deep learning,\nespecially for image segmentation tasks. However, when applied to specialized\ndatasets such as biomedical imaging, pretrained weights often introduce\nunintended biases. These biases cause models to assign different levels of\nimportance to different slices, leading to inconsistencies in feature\nutilization, which can be observed as asymmetries in saliency map\ndistributions. This transfer of color distributions from natural images to\nnon-natural datasets can compromise model performance and reduce the\nreliability of results. In this study, we investigate the effects of these\nbiases and propose strategies to mitigate them. Through a series of\nexperiments, we test both pretrained and randomly initialized models, comparing\ntheir performance and saliency map distributions. Our proposed methods, which\naim to neutralize the bias introduced by pretrained color channel weights,\ndemonstrate promising results, offering a practical approach to improving model\nexplainability while maintaining the benefits of pretrained models. This\npublication presents our findings, providing insights into addressing\npretrained weight biases across various deep learning tasks.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u751f\u7269\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u7684\u504f\u5dee\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u6d88\u9664\u504f\u5dee\u7684\u7b56\u7565\u3002", "motivation": "\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u751f\u7269\u533b\u5b66\u56fe\u50cf\u7b49\u4e13\u4e1a\u6570\u636e\u96c6\u4e0a\u53ef\u80fd\u5f15\u5165\u504f\u5dee\uff0c\u5bfc\u81f4\u7279\u5f81\u5229\u7528\u4e0d\u4e00\u81f4\uff0c\u5f71\u54cd\u6a21\u578b\u6027\u80fd\u548c\u7ed3\u679c\u53ef\u9760\u6027\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u6bd4\u8f83\u9884\u8bad\u7ec3\u548c\u968f\u673a\u521d\u59cb\u5316\u6a21\u578b\u7684\u6027\u80fd\u53ca\u663e\u8457\u6027\u56fe\u5206\u5e03\uff0c\u63d0\u51fa\u6d88\u9664\u9884\u8bad\u7ec3\u989c\u8272\u901a\u9053\u6743\u91cd\u504f\u5dee\u7684\u65b9\u6cd5\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u6d88\u9664\u504f\u5dee\u7684\u540c\u65f6\u4fdd\u6301\u4e86\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u4f18\u52bf\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u7814\u7a76\u4e3a\u89e3\u51b3\u9884\u8bad\u7ec3\u6743\u91cd\u504f\u5dee\u63d0\u4f9b\u4e86\u5b9e\u7528\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u6df1\u5ea6\u5b66\u4e60\u4efb\u52a1\u3002"}}
{"id": "2505.13866", "pdf": "https://arxiv.org/pdf/2505.13866", "abs": "https://arxiv.org/abs/2505.13866", "authors": ["Jiwon Song", "Dongwon Jo", "Yulhwa Kim", "Jae-Joon Kim"], "title": "Reasoning Path Compression: Compressing Generation Trajectories for Efficient LLM Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "Recent reasoning-focused language models achieve high accuracy by generating\nlengthy intermediate reasoning paths before producing final answers. While this\napproach is effective in solving problems that require logical thinking, long\nreasoning paths significantly increase memory usage and throughput of token\ngeneration, limiting the practical deployment of such models. We propose\nReasoning Path Compression (RPC), a training-free method that accelerates\ninference by leveraging the semantic sparsity of reasoning paths. RPC\nperiodically compresses the KV cache by retaining KV cache that receive high\nimportance score, which are computed using a selector window composed of\nrecently generated queries. Experiments show that RPC improves generation\nthroughput of QwQ-32B by up to 1.60$\\times$ compared to the inference with full\nKV cache, with an accuracy drop of 1.2% on the AIME 2024 benchmark. Our\nfindings demonstrate that semantic sparsity in reasoning traces can be\neffectively exploited for compression, offering a practical path toward\nefficient deployment of reasoning LLMs. Our code is available at\nhttps://github.com/jiwonsong-dev/ReasoningPathCompression.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u63a8\u7406\u8def\u5f84\u538b\u7f29\u65b9\u6cd5\uff08RPC\uff09\uff0c\u901a\u8fc7\u5229\u7528\u63a8\u7406\u8def\u5f84\u7684\u8bed\u4e49\u7a00\u758f\u6027\u52a0\u901f\u63a8\u7406\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u751f\u6210\u541e\u5410\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u8f83\u9ad8\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u63a8\u7406\u7684\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u751f\u6210\u5197\u957f\u7684\u4e2d\u95f4\u63a8\u7406\u8def\u5f84\u6765\u63d0\u9ad8\u51c6\u786e\u6027\uff0c\u4f46\u8fd9\u589e\u52a0\u4e86\u5185\u5b58\u4f7f\u7528\u548c\u751f\u6210\u5ef6\u8fdf\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u90e8\u7f72\u3002", "method": "RPC\u901a\u8fc7\u5b9a\u671f\u538b\u7f29KV\u7f13\u5b58\uff0c\u4fdd\u7559\u91cd\u8981\u6027\u5206\u6570\u9ad8\u7684\u90e8\u5206\uff0c\u91cd\u8981\u6027\u5206\u6570\u7531\u6700\u8fd1\u751f\u6210\u7684\u67e5\u8be2\u7ec4\u6210\u7684\u9009\u62e9\u5668\u7a97\u53e3\u8ba1\u7b97\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cRPC\u5c06QwQ-32B\u7684\u751f\u6210\u541e\u5410\u91cf\u63d0\u9ad8\u4e861.60\u500d\uff0c\u5728AIME 2024\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u51c6\u786e\u7387\u4ec5\u4e0b\u964d1.2%\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u63a8\u7406\u8def\u5f84\u7684\u8bed\u4e49\u7a00\u758f\u6027\u53ef\u6709\u6548\u7528\u4e8e\u538b\u7f29\uff0c\u4e3a\u9ad8\u6548\u90e8\u7f72\u63a8\u7406\u578b\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u5b9e\u7528\u65b9\u6cd5\u3002"}}
{"id": "2505.14113", "pdf": "https://arxiv.org/pdf/2505.14113", "abs": "https://arxiv.org/abs/2505.14113", "authors": ["Bruno Viti", "Elias Karabelas", "Martin Holler"], "title": "CONSIGN: Conformal Segmentation Informed by Spatial Groupings via Decomposition", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Most machine learning-based image segmentation models produce pixel-wise\nconfidence scores - typically derived from softmax outputs - that represent the\nmodel's predicted probability for each class label at every pixel. While this\ninformation can be particularly valuable in high-stakes domains such as medical\nimaging, these (uncalibrated) scores are heuristic in nature and do not\nconstitute rigorous quantitative uncertainty estimates. Conformal prediction\n(CP) provides a principled framework for transforming heuristic confidence\nscores into statistically valid uncertainty estimates. However, applying CP\ndirectly to image segmentation ignores the spatial correlations between pixels,\na fundamental characteristic of image data. This can result in overly\nconservative and less interpretable uncertainty estimates. To address this, we\npropose CONSIGN (Conformal Segmentation Informed by Spatial Groupings via\nDecomposition), a CP-based method that incorporates spatial correlations to\nimprove uncertainty quantification in image segmentation. Our method generates\nmeaningful prediction sets that come with user-specified, high-probability\nerror guarantees. It is compatible with any pre-trained segmentation model\ncapable of generating multiple sample outputs - such as those using dropout,\nBayesian modeling, or ensembles. We evaluate CONSIGN against a standard\npixel-wise CP approach across three medical imaging datasets and two COCO\ndataset subsets, using three different pre-trained segmentation models. Results\ndemonstrate that accounting for spatial structure significantly improves\nperformance across multiple metrics and enhances the quality of uncertainty\nestimates.", "AI": {"tldr": "CONSIGN\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408\u7a7a\u95f4\u76f8\u5173\u6027\u6539\u8fdb\u56fe\u50cf\u5206\u5272\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff0c\u751f\u6210\u5177\u6709\u7edf\u8ba1\u4fdd\u8bc1\u7684\u9884\u6d4b\u96c6\u3002", "motivation": "\u4f20\u7edf\u7f6e\u4fe1\u5ea6\u5206\u6570\u7f3a\u4e4f\u4e25\u683c\u7684\u7edf\u8ba1\u6709\u6548\u6027\uff0c\u4e14\u5ffd\u7565\u7a7a\u95f4\u76f8\u5173\u6027\uff0c\u5bfc\u81f4\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u4fdd\u5b88\u4e14\u4e0d\u76f4\u89c2\u3002", "method": "\u63d0\u51faCONSIGN\u65b9\u6cd5\uff0c\u5229\u7528\u7a7a\u95f4\u5206\u7ec4\u5206\u89e3\u7ed3\u5408\u5171\u5f62\u9884\u6d4b\uff0c\u6539\u8fdb\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u6a21\u578b\u4e0a\u9a8c\u8bc1\uff0cCONSIGN\u663e\u8457\u63d0\u5347\u6027\u80fd\u5e76\u6539\u5584\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u8d28\u91cf\u3002", "conclusion": "CONSIGN\u901a\u8fc7\u8003\u8651\u7a7a\u95f4\u7ed3\u6784\uff0c\u4e3a\u56fe\u50cf\u5206\u5272\u63d0\u4f9b\u66f4\u53ef\u9760\u548c\u89e3\u91ca\u6027\u5f3a\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u3002"}}
{"id": "2505.13886", "pdf": "https://arxiv.org/pdf/2505.13886", "abs": "https://arxiv.org/abs/2505.13886", "authors": ["Jingqi Tong", "Jixin Tang", "Hangcheng Li", "Yurong Mou", "Ming Zhang", "Jun Zhao", "Yanbo Wen", "Fan Song", "Jiahao Zhan", "Yuyang Lu", "Chaoran Tao", "Zhiyuan Guo", "Jizhou Yu", "Tianhao Cheng", "Changhao Jiang", "Zhen Wang", "Tao Liang", "Zhihui Fei", "Mingyang Wan", "Guojun Ma", "Weifeng Ge", "Guanhua Chen", "Tao Gui", "Xipeng Qiu", "Qi Zhang", "Xuanjing Huang"], "title": "Code2Logic: Game-Code-Driven Data Synthesis for Enhancing VLMs General Reasoning", "categories": ["cs.CL", "I.2.7; I.2.10"], "comment": "49 pages, 19 figures, submitted to NeurIPS 2025", "summary": "Visual-language Chain-of-Thought (CoT) data resources are relatively scarce\ncompared to text-only counterparts, limiting the improvement of reasoning\ncapabilities in Vision Language Models (VLMs). However, high-quality\nvision-language reasoning data is expensive and labor-intensive to annotate. To\naddress this issue, we leverage a promising resource: game code, which\nnaturally contains logical structures and state transition processes.\nTherefore, we propose Code2Logic, a novel game-code-driven approach for\nmultimodal reasoning data synthesis. Our approach leverages Large Language\nModels (LLMs) to adapt game code, enabling automatic acquisition of reasoning\nprocesses and results through code execution. Using the Code2Logic approach, we\ndeveloped the GameQA dataset to train and evaluate VLMs. GameQA is\ncost-effective and scalable to produce, challenging for state-of-the-art\nmodels, and diverse with 30 games and 158 tasks. Surprisingly, despite training\nsolely on game data, VLMs demonstrated out of domain generalization,\nspecifically Qwen2.5-VL-7B improving performance by 2.33\\% across 7 diverse\nvision-language benchmarks. Our code and dataset are available at\nhttps://github.com/tongjingqi/Code2Logic.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faCode2Logic\u65b9\u6cd5\uff0c\u5229\u7528\u6e38\u620f\u4ee3\u7801\u81ea\u52a8\u751f\u6210\u89c6\u89c9\u8bed\u8a00\u63a8\u7406\u6570\u636e\uff0c\u89e3\u51b3\u4e86\u9ad8\u8d28\u91cf\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u5e76\u5f00\u53d1\u4e86GameQA\u6570\u636e\u96c6\u3002", "motivation": "\u89c6\u89c9\u8bed\u8a00\u63a8\u7406\u6570\u636e\u7a00\u7f3a\u4e14\u6807\u6ce8\u6210\u672c\u9ad8\uff0c\u9650\u5236\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u63d0\u5347\u3002", "method": "\u5229\u7528\u6e38\u620f\u4ee3\u7801\u7684\u903b\u8f91\u7ed3\u6784\uff0c\u901a\u8fc7LLM\u9002\u914d\u4ee3\u7801\u5e76\u81ea\u52a8\u83b7\u53d6\u63a8\u7406\u8fc7\u7a0b\u548c\u7ed3\u679c\uff0c\u751f\u6210\u591a\u6a21\u6001\u63a8\u7406\u6570\u636e\u3002", "result": "\u5f00\u53d1\u4e86GameQA\u6570\u636e\u96c6\uff0c\u6a21\u578b\u57287\u4e2a\u89c6\u89c9\u8bed\u8a00\u57fa\u51c6\u4e0a\u6027\u80fd\u63d0\u53472.33%\u3002", "conclusion": "Code2Logic\u65b9\u6cd5\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\uff0c\u751f\u6210\u7684GameQA\u6570\u636e\u96c6\u80fd\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2505.14124", "pdf": "https://arxiv.org/pdf/2505.14124", "abs": "https://arxiv.org/abs/2505.14124", "authors": ["Hongjun Choi", "Eun Som Jeon", "Ankita Shukla", "Pavan Turaga"], "title": "Intra-class Patch Swap for Self-Distillation", "categories": ["cs.CV"], "comment": "Accepted for publication in Neurocomputing", "summary": "Knowledge distillation (KD) is a valuable technique for compressing large\ndeep learning models into smaller, edge-suitable networks. However,\nconventional KD frameworks rely on pre-trained high-capacity teacher networks,\nwhich introduce significant challenges such as increased memory/storage\nrequirements, additional training costs, and ambiguity in selecting an\nappropriate teacher for a given student model. Although a teacher-free\ndistillation (self-distillation) has emerged as a promising alternative, many\nexisting approaches still rely on architectural modifications or complex\ntraining procedures, which limit their generality and efficiency.\n  To address these limitations, we propose a novel framework based on\nteacher-free distillation that operates using a single student network without\nany auxiliary components, architectural modifications, or additional learnable\nparameters. Our approach is built on a simple yet highly effective\naugmentation, called intra-class patch swap augmentation. This augmentation\nsimulates a teacher-student dynamic within a single model by generating pairs\nof intra-class samples with varying confidence levels, and then applying\ninstance-to-instance distillation to align their predictive distributions. Our\nmethod is conceptually simple, model-agnostic, and easy to implement, requiring\nonly a single augmentation function. Extensive experiments across image\nclassification, semantic segmentation, and object detection show that our\nmethod consistently outperforms both existing self-distillation baselines and\nconventional teacher-based KD approaches. These results suggest that the\nsuccess of self-distillation could hinge on the design of the augmentation\nitself. Our codes are available at\nhttps://github.com/hchoi71/Intra-class-Patch-Swap.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6559\u5e08\u65e0\u5173\u84b8\u998f\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u7c7b\u5185\u8865\u4e01\u4ea4\u6362\u589e\u5f3a\u5b9e\u73b0\u5355\u7f51\u7edc\u81ea\u84b8\u998f\uff0c\u65e0\u9700\u989d\u5916\u7ec4\u4ef6\u6216\u67b6\u6784\u4fee\u6539\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u77e5\u8bc6\u84b8\u998f\u4f9d\u8d56\u9884\u8bad\u7ec3\u6559\u5e08\u7f51\u7edc\uff0c\u5e26\u6765\u5185\u5b58\u3001\u8bad\u7ec3\u6210\u672c\u548c\u6559\u5e08\u9009\u62e9\u95ee\u9898\uff1b\u73b0\u6709\u81ea\u84b8\u998f\u65b9\u6cd5\u5e38\u9700\u67b6\u6784\u4fee\u6539\u6216\u590d\u6742\u6d41\u7a0b\uff0c\u901a\u7528\u6027\u548c\u6548\u7387\u53d7\u9650\u3002", "method": "\u4f7f\u7528\u7c7b\u5185\u8865\u4e01\u4ea4\u6362\u589e\u5f3a\uff0c\u6a21\u62df\u5e08\u751f\u52a8\u6001\uff0c\u901a\u8fc7\u5b9e\u4f8b\u95f4\u84b8\u998f\u5bf9\u9f50\u9884\u6d4b\u5206\u5e03\uff0c\u4ec5\u9700\u5355\u4e00\u589e\u5f3a\u51fd\u6570\u3002", "result": "\u5728\u56fe\u50cf\u5206\u7c7b\u3001\u8bed\u4e49\u5206\u5272\u548c\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\u4e2d\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u81ea\u84b8\u998f\u548c\u4f20\u7edf\u6559\u5e08\u84b8\u998f\u65b9\u6cd5\u3002", "conclusion": "\u81ea\u84b8\u998f\u7684\u6210\u529f\u53ef\u80fd\u4f9d\u8d56\u4e8e\u589e\u5f3a\u8bbe\u8ba1\uff0c\u8be5\u65b9\u6cd5\u7b80\u5355\u3001\u901a\u7528\u4e14\u9ad8\u6548\u3002"}}
{"id": "2505.13890", "pdf": "https://arxiv.org/pdf/2505.13890", "abs": "https://arxiv.org/abs/2505.13890", "authors": ["Zhen Xiong", "Yujun Cai", "Zhecheng Li", "Yiwei Wang"], "title": "Mapping the Minds of LLMs: A Graph-Based Analysis of Reasoning LLM", "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in test-time scaling have enabled Large Language Models\n(LLMs) to display sophisticated reasoning abilities via extended\nChain-of-Thought (CoT) generation. Despite their potential, these Reasoning\nLLMs (RLMs) often demonstrate counterintuitive and unstable behaviors, such as\nperformance degradation under few-shot prompting, that challenge our current\nunderstanding of RLMs. In this work, we introduce a unified graph-based\nanalytical framework for better modeling the reasoning processes of RLMs. Our\nmethod first clusters long, verbose CoT outputs into semantically coherent\nreasoning steps, then constructs directed reasoning graphs to capture\ncontextual and logical dependencies among these steps. Through comprehensive\nanalysis across models and prompting regimes, we reveal that structural\nproperties, such as exploration density, branching, and convergence ratios,\nstrongly correlate with reasoning accuracy. Our findings demonstrate how\nprompting strategies substantially reshape the internal reasoning structure of\nRLMs, directly affecting task outcomes. The proposed framework not only enables\nquantitative evaluation of reasoning quality beyond conventional metrics but\nalso provides practical insights for prompt engineering and the cognitive\nanalysis of LLMs. Code and resources will be released to facilitate future\nresearch in this direction.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u7684\u7edf\u4e00\u5206\u6790\u6846\u67b6\uff0c\u7528\u4e8e\u5efa\u6a21\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u63ed\u793a\u4e86\u63a8\u7406\u7ed3\u6784\u4e0e\u51c6\u786e\u6027\u7684\u5f3a\u76f8\u5173\u6027\u3002", "motivation": "\u5c3d\u7ba1\u63a8\u7406\u578bLLMs\uff08RLMs\uff09\u5c55\u73b0\u51fa\u590d\u6742\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5176\u4e0d\u7a33\u5b9a\u548c\u53cd\u76f4\u89c9\u7684\u884c\u4e3a\uff08\u5982\u5c11\u6837\u672c\u63d0\u793a\u4e0b\u6027\u80fd\u4e0b\u964d\uff09\u6311\u6218\u4e86\u5f53\u524d\u5bf9RLMs\u7684\u7406\u89e3\u3002", "method": "\u901a\u8fc7\u805a\u7c7b\u5197\u957f\u7684Chain-of-Thought\uff08CoT\uff09\u8f93\u51fa\u4e3a\u8bed\u4e49\u8fde\u8d2f\u7684\u63a8\u7406\u6b65\u9aa4\uff0c\u5e76\u6784\u5efa\u6709\u5411\u63a8\u7406\u56fe\u6355\u6349\u6b65\u9aa4\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\u3002", "result": "\u5206\u6790\u8868\u660e\uff0c\u63a8\u7406\u56fe\u7684\u7ed3\u6784\u7279\u6027\uff08\u5982\u63a2\u7d22\u5bc6\u5ea6\u3001\u5206\u652f\u548c\u6536\u655b\u6bd4\uff09\u4e0e\u63a8\u7406\u51c6\u786e\u6027\u9ad8\u5ea6\u76f8\u5173\uff0c\u63d0\u793a\u7b56\u7565\u663e\u8457\u5f71\u54cd\u63a8\u7406\u7ed3\u6784\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e0d\u4ec5\u63d0\u4f9b\u4e86\u8d85\u8d8a\u4f20\u7edf\u6307\u6807\u7684\u63a8\u7406\u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\uff0c\u8fd8\u4e3a\u63d0\u793a\u5de5\u7a0b\u548cLLMs\u8ba4\u77e5\u5206\u6790\u63d0\u4f9b\u4e86\u5b9e\u7528\u89c1\u89e3\u3002"}}
{"id": "2505.14135", "pdf": "https://arxiv.org/pdf/2505.14135", "abs": "https://arxiv.org/abs/2505.14135", "authors": ["Ruihuang Li", "Caijin Zhou", "Shoujian Zheng", "Jianxiang Lu", "Jiabin Huang", "Comi Chen", "Junshu Tang", "Guangzheng Xu", "Jiale Tao", "Hongmei Wang", "Donghao Li", "Wenqing Yu", "Senbo Wang", "Zhimin Li", "Yetshuan Shi", "Haoyu Yang", "Yukun Wang", "Wenxun Dai", "Jiaqi Li", "Linqing Wang", "Qixun Wang", "Zhiyong Xu", "Yingfang Zhang", "Jiangfeng Xiong", "Weijie Kong", "Chao Zhang", "Hongxin Zhang", "Qiaoling Zheng", "Weiting Guo", "Xinchi Deng", "Yixuan Li", "Renjia Wei", "Yulin Jian", "Duojun Huang", "Xuhua Ren", "Sihuan Lin", "Yifu Sun", "Yuan Zhou", "Joey Wang", "Qin Lin", "Jingmiao Yu", "Jihong Zhang", "Caesar Zhong", "Di Wang", "Yuhong Liu", "Linus", "Jie Jiang", "Longhuang Wu", "Shuai Shao", "Qinglin Lu"], "title": "Hunyuan-Game: Industrial-grade Intelligent Game Creation Model", "categories": ["cs.CV"], "comment": null, "summary": "Intelligent game creation represents a transformative advancement in game\ndevelopment, utilizing generative artificial intelligence to dynamically\ngenerate and enhance game content. Despite notable progress in generative\nmodels, the comprehensive synthesis of high-quality game assets, including both\nimages and videos, remains a challenging frontier. To create high-fidelity game\ncontent that simultaneously aligns with player preferences and significantly\nboosts designer efficiency, we present Hunyuan-Game, an innovative project\ndesigned to revolutionize intelligent game production. Hunyuan-Game encompasses\ntwo primary branches: image generation and video generation. The image\ngeneration component is built upon a vast dataset comprising billions of game\nimages, leading to the development of a group of customized image generation\nmodels tailored for game scenarios: (1) General Text-to-Image Generation. (2)\nGame Visual Effects Generation, involving text-to-effect and reference\nimage-based game visual effect generation. (3) Transparent Image Generation for\ncharacters, scenes, and game visual effects. (4) Game Character Generation\nbased on sketches, black-and-white images, and white models. The video\ngeneration component is built upon a comprehensive dataset of millions of game\nand anime videos, leading to the development of five core algorithmic models,\neach targeting critical pain points in game development and having robust\nadaptation to diverse game video scenarios: (1) Image-to-Video Generation. (2)\n360 A/T Pose Avatar Video Synthesis. (3) Dynamic Illustration Generation. (4)\nGenerative Video Super-Resolution. (5) Interactive Game Video Generation. These\nimage and video generation models not only exhibit high-level aesthetic\nexpression but also deeply integrate domain-specific knowledge, establishing a\nsystematic understanding of diverse game and anime art styles.", "AI": {"tldr": "Hunyuan-Game\u9879\u76ee\u5229\u7528\u751f\u6210\u5f0fAI\u6280\u672f\uff0c\u901a\u8fc7\u56fe\u50cf\u548c\u89c6\u9891\u751f\u6210\u6a21\u578b\uff0c\u4e3a\u6e38\u620f\u5f00\u53d1\u63d0\u4f9b\u9ad8\u8d28\u91cf\u5185\u5bb9\u751f\u6210\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u5c3d\u7ba1\u751f\u6210\u6a21\u578b\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u9ad8\u8d28\u91cf\u6e38\u620f\u5185\u5bb9\uff08\u5982\u56fe\u50cf\u548c\u89c6\u9891\uff09\u7684\u5408\u6210\u4ecd\u5177\u6311\u6218\u6027\u3002Hunyuan-Game\u65e8\u5728\u63d0\u5347\u73a9\u5bb6\u4f53\u9a8c\u5e76\u63d0\u9ad8\u8bbe\u8ba1\u5e08\u6548\u7387\u3002", "method": "\u9879\u76ee\u5206\u4e3a\u56fe\u50cf\u751f\u6210\u548c\u89c6\u9891\u751f\u6210\u4e24\u90e8\u5206\u3002\u56fe\u50cf\u751f\u6210\u57fa\u4e8e\u6570\u5341\u4ebf\u6e38\u620f\u56fe\u50cf\u6570\u636e\u96c6\uff0c\u5f00\u53d1\u4e86\u56db\u79cd\u5b9a\u5236\u6a21\u578b\uff1b\u89c6\u9891\u751f\u6210\u57fa\u4e8e\u6570\u767e\u4e07\u6e38\u620f\u548c\u52a8\u6f2b\u89c6\u9891\u6570\u636e\u96c6\uff0c\u5f00\u53d1\u4e86\u4e94\u79cd\u6838\u5fc3\u7b97\u6cd5\u6a21\u578b\u3002", "result": "\u751f\u6210\u7684\u56fe\u50cf\u548c\u89c6\u9891\u4e0d\u4ec5\u5177\u6709\u9ad8\u7f8e\u5b66\u8868\u73b0\uff0c\u8fd8\u6df1\u5ea6\u878d\u5408\u4e86\u6e38\u620f\u548c\u52a8\u6f2b\u9886\u57df\u7684\u4e13\u4e1a\u77e5\u8bc6\u3002", "conclusion": "Hunyuan-Game\u4e3a\u667a\u80fd\u6e38\u620f\u751f\u4ea7\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u89e3\u51b3\u65b9\u6848\uff0c\u63a8\u52a8\u4e86\u6e38\u620f\u5185\u5bb9\u751f\u6210\u7684\u9769\u65b0\u3002"}}
{"id": "2505.13893", "pdf": "https://arxiv.org/pdf/2505.13893", "abs": "https://arxiv.org/abs/2505.13893", "authors": ["Yuanyi Wang", "Zhaoyi Yan", "Yiming Zhang", "Qi Zhou", "Yanggan Gu", "Fei Wu", "Hongxia Yang"], "title": "InfiGFusion: Graph-on-Logits Distillation via Efficient Gromov-Wasserstein for Model Fusion", "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in large language models (LLMs) have intensified efforts to\nfuse heterogeneous open-source models into a unified system that inherits their\ncomplementary strengths. Existing logit-based fusion methods maintain inference\nefficiency but treat vocabulary dimensions independently, overlooking semantic\ndependencies encoded by cross-dimension interactions. These dependencies\nreflect how token types interact under a model's internal reasoning and are\nessential for aligning models with diverse generation behaviors. To explicitly\nmodel these dependencies, we propose \\textbf{InfiGFusion}, the first\nstructure-aware fusion framework with a novel \\textit{Graph-on-Logits\nDistillation} (GLD) loss. Specifically, we retain the top-$k$ logits per output\nand aggregate their outer products across sequence positions to form a global\nco-activation graph, where nodes represent vocabulary channels and edges\nquantify their joint activations. To ensure scalability and efficiency, we\ndesign a sorting-based closed-form approximation that reduces the original\n$O(n^4)$ cost of Gromov-Wasserstein distance to $O(n \\log n)$, with provable\napproximation guarantees. Experiments across multiple fusion settings show that\nGLD consistently improves fusion quality and stability. InfiGFusion outperforms\nSOTA models and fusion baselines across 11 benchmarks spanning reasoning,\ncoding, and mathematics. It shows particular strength in complex reasoning\ntasks, with +35.6 improvement on Multistep Arithmetic and +37.06 on Causal\nJudgement over SFT, demonstrating superior multi-step and relational inference.", "AI": {"tldr": "InfiGFusion\u662f\u4e00\u4e2a\u7ed3\u6784\u611f\u77e5\u7684\u878d\u5408\u6846\u67b6\uff0c\u901a\u8fc7Graph-on-Logits Distillation\uff08GLD\uff09\u635f\u5931\u663e\u5f0f\u5efa\u6a21\u8bed\u4e49\u4f9d\u8d56\uff0c\u663e\u8457\u63d0\u5347\u4e86\u878d\u5408\u8d28\u91cf\u548c\u7a33\u5b9a\u6027\uff0c\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8elogit\u7684\u878d\u5408\u65b9\u6cd5\u5ffd\u89c6\u4e86\u8bcd\u6c47\u7ef4\u5ea6\u95f4\u7684\u8bed\u4e49\u4f9d\u8d56\uff0c\u8fd9\u4e9b\u4f9d\u8d56\u5bf9\u6a21\u578b\u751f\u6210\u884c\u4e3a\u7684\u5bf9\u9f50\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51faInfiGFusion\u6846\u67b6\uff0c\u5229\u7528\u5168\u5c40\u5171\u6fc0\u6d3b\u56fe\u5efa\u6a21\u8bed\u4e49\u4f9d\u8d56\uff0c\u5e76\u901a\u8fc7\u6392\u5e8f\u8fd1\u4f3c\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "result": "\u572811\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5c24\u5176\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e0a\u63d0\u5347\u663e\u8457\uff08\u5982Multistep Arithmetic +35.6\uff09\u3002", "conclusion": "InfiGFusion\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u8bed\u4e49\u4f9d\u8d56\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u878d\u5408\u7684\u6548\u679c\u548c\u6548\u7387\u3002"}}
{"id": "2505.14151", "pdf": "https://arxiv.org/pdf/2505.14151", "abs": "https://arxiv.org/abs/2505.14151", "authors": ["Jiaming Li", "Sheng Wang", "Xin Wang", "Yitao Zhu", "Honglin Xiong", "Zixu Zhuang", "Qian Wang"], "title": "ReactDiff: Latent Diffusion for Facial Reaction Generation", "categories": ["cs.CV", "cs.MM"], "comment": null, "summary": "Given the audio-visual clip of the speaker, facial reaction generation aims\nto predict the listener's facial reactions. The challenge lies in capturing the\nrelevance between video and audio while balancing appropriateness, realism, and\ndiversity. While prior works have mostly focused on uni-modal inputs or\nsimplified reaction mappings, recent approaches such as PerFRDiff have explored\nmulti-modal inputs and the one-to-many nature of appropriate reaction mappings.\nIn this work, we propose the Facial Reaction Diffusion (ReactDiff) framework\nthat uniquely integrates a Multi-Modality Transformer with conditional\ndiffusion in the latent space for enhanced reaction generation. Unlike existing\nmethods, ReactDiff leverages intra- and inter-class attention for fine-grained\nmulti-modal interaction, while the latent diffusion process between the encoder\nand decoder enables diverse yet contextually appropriate outputs. Experimental\nresults demonstrate that ReactDiff significantly outperforms existing\napproaches, achieving a facial reaction correlation of 0.26 and diversity score\nof 0.094 while maintaining competitive realism. The code is open-sourced at\n\\href{https://github.com/Hunan-Tiger/ReactDiff}{github}.", "AI": {"tldr": "ReactDiff\u6846\u67b6\u901a\u8fc7\u591a\u6a21\u6001Transformer\u548c\u6f5c\u5728\u7a7a\u95f4\u6761\u4ef6\u6269\u6563\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9762\u90e8\u53cd\u5e94\u751f\u6210\u7684\u591a\u6837\u6027\u548c\u76f8\u5173\u6027\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u6355\u6349\u89c6\u9891\u4e0e\u97f3\u9891\u76f8\u5173\u6027\u53ca\u5e73\u8861\u53cd\u5e94\u591a\u6837\u6027\u3001\u771f\u5b9e\u6027\u548c\u9002\u5f53\u6027\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "method": "\u7ed3\u5408\u591a\u6a21\u6001Transformer\u548c\u6f5c\u5728\u7a7a\u95f4\u6761\u4ef6\u6269\u6563\uff0c\u5229\u7528\u7c7b\u5185\u548c\u7c7b\u95f4\u6ce8\u610f\u529b\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u591a\u6a21\u6001\u4ea4\u4e92\u3002", "result": "\u5b9e\u9a8c\u663e\u793aReactDiff\u5728\u76f8\u5173\u6027\uff080.26\uff09\u548c\u591a\u6837\u6027\uff080.094\uff09\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u771f\u5b9e\u6027\u3002", "conclusion": "ReactDiff\u4e3a\u9762\u90e8\u53cd\u5e94\u751f\u6210\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u591a\u6837\u5316\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2505.13903", "pdf": "https://arxiv.org/pdf/2505.13903", "abs": "https://arxiv.org/abs/2505.13903", "authors": ["Chengyu Shen", "Zhen Hao Wong", "Runming He", "Hao Liang", "Meiyi Qiang", "Zimo Meng", "Zhengyang Zhao", "Bohan Zeng", "Zhengzhou Zhu", "Bin Cui", "Wentao Zhang"], "title": "Let's Verify Math Questions Step by Step", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have recently achieved remarkable progress in\nmathematical reasoning. To enable such capabilities, many existing works\ndistill strong reasoning models into long chains of thought or design\nalgorithms to construct high-quality math QA data for training. However, these\nefforts primarily focus on generating correct reasoning paths and answers,\nwhile largely overlooking the validity of the questions themselves. In this\nwork, we propose Math Question Verification (MathQ-Verify), a novel five-stage\npipeline designed to rigorously filter ill-posed or under-specified math\nproblems. MathQ-Verify first performs format-level validation to remove\nredundant instructions and ensure that each question is syntactically\nwell-formed. It then formalizes each question, decomposes it into atomic\nconditions, and verifies them against mathematical definitions. Next, it\ndetects logical contradictions among these conditions, followed by a\ngoal-oriented completeness check to ensure the question provides sufficient\ninformation for solving. To evaluate this task, we use existing benchmarks\nalong with an additional dataset we construct, containing 2,147 math questions\nwith diverse error types, each manually double-validated. Experiments show that\nMathQ-Verify achieves state-of-the-art performance across multiple benchmarks,\nimproving the F1 score by up to 25 percentage points over the direct\nverification baseline. It further attains approximately 90% precision and 63%\nrecall through a lightweight model voting scheme. MathQ-Verify offers a\nscalable and accurate solution for curating reliable mathematical datasets,\nreducing label noise and avoiding unnecessary computation on invalid questions.\nOur code and data are available at https://github.com/scuuy/MathQ-Verify.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faMathQ-Verify\uff0c\u4e00\u4e2a\u4e94\u9636\u6bb5\u6d41\u7a0b\uff0c\u7528\u4e8e\u8fc7\u6ee4\u6570\u5b66\u95ee\u9898\u4e2d\u7684\u4e0d\u660e\u786e\u6216\u65e0\u6548\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u6570\u636e\u96c6\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u751f\u6210\u6b63\u786e\u7684\u63a8\u7406\u8def\u5f84\u548c\u7b54\u6848\uff0c\u4f46\u5ffd\u7565\u4e86\u95ee\u9898\u672c\u8eab\u7684\u6709\u6548\u6027\uff0c\u5bfc\u81f4\u6570\u636e\u96c6\u8d28\u91cf\u4e0d\u9ad8\u3002", "method": "MathQ-Verify\u901a\u8fc7\u683c\u5f0f\u9a8c\u8bc1\u3001\u5f62\u5f0f\u5316\u5206\u89e3\u3001\u903b\u8f91\u77db\u76fe\u68c0\u6d4b\u3001\u76ee\u6807\u5b8c\u6574\u6027\u68c0\u67e5\u7b49\u4e94\u4e2a\u9636\u6bb5\uff0c\u4e25\u683c\u7b5b\u9009\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMathQ-Verify\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u6700\u4f18\uff0cF1\u5206\u6570\u63d0\u5347\u9ad8\u8fbe25\u4e2a\u767e\u5206\u70b9\uff0c\u7cbe\u5ea6\u7ea690%\uff0c\u53ec\u56de\u738763%\u3002", "conclusion": "MathQ-Verify\u4e3a\u6570\u5b66\u6570\u636e\u96c6\u7684\u53ef\u9760\u7b5b\u9009\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u51cf\u5c11\u6807\u7b7e\u566a\u58f0\u548c\u65e0\u6548\u8ba1\u7b97\u3002"}}
{"id": "2505.14156", "pdf": "https://arxiv.org/pdf/2505.14156", "abs": "https://arxiv.org/abs/2505.14156", "authors": ["Songhao Wu", "Quan Tu", "Hong Liu", "Jia Xu", "Zhongyi Liu", "Guannan Zhang", "Ran Wang", "Xiuying Chen", "Rui Yan"], "title": "Unify Graph Learning with Text: Unleashing LLM Potentials for Session Search", "categories": ["cs.CV", "cs.AI", "cs.IR", "I.2; H.3.3"], "comment": null, "summary": "Session search involves a series of interactive queries and actions to\nfulfill user's complex information need. Current strategies typically\nprioritize sequential modeling for deep semantic understanding, overlooking the\ngraph structure in interactions. While some approaches focus on capturing\nstructural information, they use a generalized representation for documents,\nneglecting the word-level semantic modeling. In this paper, we propose Symbolic\nGraph Ranker (SGR), which aims to take advantage of both text-based and\ngraph-based approaches by leveraging the power of recent Large Language Models\n(LLMs). Concretely, we first introduce a set of symbolic grammar rules to\nconvert session graph into text. This allows integrating session history,\ninteraction process, and task instruction seamlessly as inputs for the LLM.\nMoreover, given the natural discrepancy between LLMs pre-trained on textual\ncorpora, and the symbolic language we produce using our graph-to-text grammar,\nour objective is to enhance LLMs' ability to capture graph structures within a\ntextual format. To achieve this, we introduce a set of self-supervised symbolic\nlearning tasks including link prediction, node content generation, and\ngenerative contrastive learning, to enable LLMs to capture the topological\ninformation from coarse-grained to fine-grained. Experiment results and\ncomprehensive analysis on two benchmark datasets, AOL and Tiangong-ST, confirm\nthe superiority of our approach. Our paradigm also offers a novel and effective\nmethodology that bridges the gap between traditional search strategies and\nmodern LLMs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSymbolic Graph Ranker (SGR)\u7684\u65b9\u6cd5\uff0c\u7ed3\u5408\u6587\u672c\u548c\u56fe\u7ed3\u6784\u4fe1\u606f\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b(LLMs)\u63d0\u5347\u4f1a\u8bdd\u641c\u7d22\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u4f1a\u8bdd\u641c\u7d22\u65b9\u6cd5\u591a\u5173\u6ce8\u5e8f\u5217\u5efa\u6a21\u6216\u56fe\u7ed3\u6784\uff0c\u4f46\u672a\u5145\u5206\u7ed3\u5408\u4e24\u8005\u7684\u4f18\u52bf\uff0c\u4e14\u5ffd\u7565\u4e86\u8bcd\u7ea7\u8bed\u4e49\u5efa\u6a21\u3002", "method": "SGR\u901a\u8fc7\u7b26\u53f7\u8bed\u6cd5\u89c4\u5219\u5c06\u4f1a\u8bdd\u56fe\u8f6c\u4e3a\u6587\u672c\uff0c\u5e76\u8bbe\u8ba1\u81ea\u76d1\u7763\u4efb\u52a1\uff08\u5982\u94fe\u63a5\u9884\u6d4b\u3001\u8282\u70b9\u5185\u5bb9\u751f\u6210\u7b49\uff09\u589e\u5f3aLLMs\u5bf9\u56fe\u7ed3\u6784\u7684\u7406\u89e3\u3002", "result": "\u5728AOL\u548cTiangong-ST\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86SGR\u7684\u4f18\u8d8a\u6027\u3002", "conclusion": "SGR\u4e3a\u4f20\u7edf\u641c\u7d22\u7b56\u7565\u4e0e\u73b0\u4ee3LLMs\u4e4b\u95f4\u67b6\u8d77\u4e86\u6865\u6881\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u9896\u6709\u6548\u7684\u65b9\u6cd5\u3002"}}
{"id": "2505.13908", "pdf": "https://arxiv.org/pdf/2505.13908", "abs": "https://arxiv.org/abs/2505.13908", "authors": ["Ajitesh Bankula", "Praney Bankula"], "title": "Cross-Linguistic Transfer in Multilingual NLP: The Role of Language Families and Morphology", "categories": ["cs.CL"], "comment": null, "summary": "Cross-lingual transfer has become a crucial aspect of multilingual NLP, as it\nallows for models trained on resource-rich languages to be applied to\nlow-resource languages more effectively. Recently massively multilingual\npre-trained language models (e.g., mBERT, XLM-R) demonstrate strong zero-shot\ntransfer capabilities[14] [13]. This paper investigates cross-linguistic\ntransfer through the lens of language families and morphology. Investigating\nhow language family proximity and morphological similarity affect performance\nacross NLP tasks. We further discuss our results and how it relates to findings\nfrom recent literature. Overall, we compare multilingual model performance and\nreview how linguistic distance metrics correlate with transfer outcomes. We\nalso look into emerging approaches that integrate typological and morphological\ninformation into model pre-training to improve transfer to diverse\nlanguages[18] [19].", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u8de8\u8bed\u8a00\u8fc1\u79fb\u5728\u8bed\u8a00\u5bb6\u65cf\u548c\u5f62\u6001\u5b66\u89c6\u89d2\u4e0b\u7684\u8868\u73b0\uff0c\u63a2\u8ba8\u4e86\u8bed\u8a00\u5bb6\u65cf\u63a5\u8fd1\u5ea6\u548c\u5f62\u6001\u76f8\u4f3c\u6027\u5bf9NLP\u4efb\u52a1\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5e76\u6bd4\u8f83\u4e86\u591a\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u8de8\u8bed\u8a00\u8fc1\u79fb\u5728\u8d44\u6e90\u4e30\u5bcc\u8bed\u8a00\u4e0e\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e4b\u95f4\u7684\u6709\u6548\u6027\uff0c\u4ee5\u53ca\u8bed\u8a00\u5bb6\u65cf\u548c\u5f62\u6001\u5b66\u7279\u5f81\u5bf9\u8fc1\u79fb\u6548\u679c\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u5206\u6790\u8bed\u8a00\u5bb6\u65cf\u63a5\u8fd1\u5ea6\u548c\u5f62\u6001\u76f8\u4f3c\u6027\uff0c\u8bc4\u4f30\u591a\u8bed\u8a00\u6a21\u578b\u5728\u4e0d\u540cNLP\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u7ed3\u5408\u6587\u732e\u8ba8\u8bba\u7ed3\u679c\u3002", "result": "\u53d1\u73b0\u8bed\u8a00\u5bb6\u65cf\u548c\u5f62\u6001\u76f8\u4f3c\u6027\u4e0e\u8de8\u8bed\u8a00\u8fc1\u79fb\u6027\u80fd\u76f8\u5173\uff0c\u5e76\u63a2\u8ba8\u4e86\u6574\u5408\u7c7b\u578b\u5b66\u548c\u5f62\u6001\u5b66\u4fe1\u606f\u5230\u6a21\u578b\u9884\u8bad\u7ec3\u4e2d\u7684\u65b0\u5174\u65b9\u6cd5\u3002", "conclusion": "\u8bed\u8a00\u5bb6\u65cf\u548c\u5f62\u6001\u5b66\u7279\u5f81\u5bf9\u8de8\u8bed\u8a00\u8fc1\u79fb\u6709\u663e\u8457\u5f71\u54cd\uff0c\u672a\u6765\u53ef\u901a\u8fc7\u6574\u5408\u66f4\u591a\u8bed\u8a00\u5b66\u4fe1\u606f\u4f18\u5316\u8fc1\u79fb\u6548\u679c\u3002"}}
{"id": "2505.14159", "pdf": "https://arxiv.org/pdf/2505.14159", "abs": "https://arxiv.org/abs/2505.14159", "authors": ["Junjie Li", "Jiawei Wang", "Miyu Li", "Yu Liu", "Yumei Wang", "Haitao Xu"], "title": "M3Depth: Wavelet-Enhanced Depth Estimation on Mars via Mutual Boosting of Dual-Modal Data", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Depth estimation plays a great potential role in obstacle avoidance and\nnavigation for further Mars exploration missions. Compared to traditional\nstereo matching, learning-based stereo depth estimation provides a data-driven\napproach to infer dense and precise depth maps from stereo image pairs.\nHowever, these methods always suffer performance degradation in environments\nwith sparse textures and lacking geometric constraints, such as the\nunstructured terrain of Mars. To address these challenges, we propose M3Depth,\na depth estimation model tailored for Mars rovers. Considering the sparse and\nsmooth texture of Martian terrain, which is primarily composed of low-frequency\nfeatures, our model incorporates a convolutional kernel based on wavelet\ntransform that effectively captures low-frequency response and expands the\nreceptive field. Additionally, we introduce a consistency loss that explicitly\nmodels the complementary relationship between depth map and surface normal map,\nutilizing the surface normal as a geometric constraint to enhance the accuracy\nof depth estimation. Besides, a pixel-wise refinement module with mutual\nboosting mechanism is designed to iteratively refine both depth and surface\nnormal predictions. Experimental results on synthetic Mars datasets with depth\nannotations show that M3Depth achieves a significant 16% improvement in depth\nestimation accuracy compared to other state-of-the-art methods in depth\nestimation. Furthermore, the model demonstrates strong applicability in\nreal-world Martian scenarios, offering a promising solution for future Mars\nexploration missions.", "AI": {"tldr": "M3Depth\u662f\u4e00\u79cd\u4e13\u4e3a\u706b\u661f\u63a2\u6d4b\u5668\u8bbe\u8ba1\u7684\u6df1\u5ea6\u4f30\u8ba1\u6a21\u578b\uff0c\u901a\u8fc7\u5c0f\u6ce2\u53d8\u6362\u5377\u79ef\u6838\u548c\u4e00\u81f4\u6027\u635f\u5931\u63d0\u5347\u7a00\u758f\u7eb9\u7406\u73af\u5883\u4e0b\u7684\u6df1\u5ea6\u4f30\u8ba1\u7cbe\u5ea6\u3002", "motivation": "\u706b\u661f\u5730\u5f62\u7eb9\u7406\u7a00\u758f\u4e14\u7f3a\u4e4f\u51e0\u4f55\u7ea6\u675f\uff0c\u4f20\u7edf\u5b66\u4e60\u65b9\u6cd5\u6027\u80fd\u4e0b\u964d\uff0c\u9700\u9488\u5bf9\u6027\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7ed3\u5408\u5c0f\u6ce2\u53d8\u6362\u5377\u79ef\u6838\u6355\u6349\u4f4e\u9891\u7279\u5f81\uff0c\u5f15\u5165\u4e00\u81f4\u6027\u635f\u5931\u548c\u50cf\u7d20\u7ea7\u7ec6\u5316\u6a21\u5757\u3002", "result": "\u5728\u5408\u6210\u706b\u661f\u6570\u636e\u96c6\u4e0a\u6df1\u5ea6\u4f30\u8ba1\u7cbe\u5ea6\u63d0\u534716%\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u706b\u661f\u573a\u666f\u3002", "conclusion": "M3Depth\u4e3a\u672a\u6765\u706b\u661f\u63a2\u7d22\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6df1\u5ea6\u4f30\u8ba1\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.13913", "pdf": "https://arxiv.org/pdf/2505.13913", "abs": "https://arxiv.org/abs/2505.13913", "authors": ["Hiram Ring"], "title": "Word length predicts word order: \"Min-max\"-ing drives language evolution", "categories": ["cs.CL"], "comment": null, "summary": "Current theories of language propose an innate (Baker 2001; Chomsky 1981) or\na functional (Greenberg 1963; Dryer 2007; Hawkins 2014) origin for the surface\nstructures (i.e. word order) that we observe in languages of the world, while\nevolutionary modeling (Dunn et al. 2011) suggests that descent is the primary\nfactor influencing such patterns. Although there are hypotheses for word order\nchange from both innate and usage-based perspectives for specific languages and\nfamilies, there are key disagreements between the two major proposals for\nmechanisms that drive the evolution of language more broadly (Wasow 2002; Levy\n2008). This paper proposes a universal underlying mechanism for word order\nchange based on a large tagged parallel dataset of over 1,500 languages\nrepresenting 133 language families and 111 isolates. Results indicate that word\nclass length is significantly correlated with word order crosslinguistically,\nbut not in a straightforward manner, partially supporting opposing theories of\nprocessing, while at the same time predicting historical word order change in\ntwo different phylogenetic lines and explaining more variance than descent or\nlanguage area in regression models. Such findings suggest an integrated\n\"Min-Max\" theory of language evolution driven by competing pressures of\nprocessing and information structure, aligning with recent efficiency-oriented\n(Levshina 2023) and information-theoretic proposals (Zaslavsky 2020; Tucker et\nal. 2025).", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u89c4\u6a21\u5e73\u884c\u6570\u636e\u96c6\u7684\u901a\u7528\u673a\u5236\uff0c\u89e3\u91ca\u8bcd\u5e8f\u53d8\u5316\uff0c\u652f\u6301\u5904\u7406\u548c\u4fe1\u606f\u7ed3\u6784\u7684\u7ade\u4e89\u538b\u529b\u7406\u8bba\u3002", "motivation": "\u63a2\u8ba8\u8bed\u8a00\u8868\u9762\u7ed3\u6784\uff08\u8bcd\u5e8f\uff09\u7684\u8d77\u6e90\uff0c\u89e3\u51b3\u5148\u5929\u8bba\u548c\u529f\u80fd\u8bba\u7684\u5206\u6b67\u3002", "method": "\u4f7f\u7528\u8d85\u8fc71,500\u79cd\u8bed\u8a00\u7684\u6807\u8bb0\u5e73\u884c\u6570\u636e\u96c6\uff0c\u5206\u6790\u8bcd\u7c7b\u957f\u5ea6\u4e0e\u8bcd\u5e8f\u7684\u5173\u8054\u3002", "result": "\u8bcd\u7c7b\u957f\u5ea6\u4e0e\u8bcd\u5e8f\u663e\u8457\u76f8\u5173\uff0c\u652f\u6301\u5904\u7406\u7406\u8bba\uff0c\u5e76\u9884\u6d4b\u5386\u53f2\u8bcd\u5e8f\u53d8\u5316\u3002", "conclusion": "\u63d0\u51fa\u201c\u6700\u5c0f-\u6700\u5927\u201d\u7406\u8bba\uff0c\u6574\u5408\u5904\u7406\u548c\u4fe1\u606f\u7ed3\u6784\u7684\u7ade\u4e89\u538b\u529b\uff0c\u89e3\u91ca\u8bed\u8a00\u6f14\u5316\u3002"}}
{"id": "2505.14167", "pdf": "https://arxiv.org/pdf/2505.14167", "abs": "https://arxiv.org/abs/2505.14167", "authors": ["Changgu Chen", "Xiaoyan Yang", "Junwei Shu", "Changbo Wang", "Yang Li"], "title": "LMP: Leveraging Motion Prior in Zero-Shot Video Generation with Diffusion Transformer", "categories": ["cs.CV"], "comment": null, "summary": "In recent years, large-scale pre-trained diffusion transformer models have\nmade significant progress in video generation. While current DiT models can\nproduce high-definition, high-frame-rate, and highly diverse videos, there is a\nlack of fine-grained control over the video content. Controlling the motion of\nsubjects in videos using only prompts is challenging, especially when it comes\nto describing complex movements. Further, existing methods fail to control the\nmotion in image-to-video generation, as the subject in the reference image\noften differs from the subject in the reference video in terms of initial\nposition, size, and shape. To address this, we propose the Leveraging Motion\nPrior (LMP) framework for zero-shot video generation. Our framework harnesses\nthe powerful generative capabilities of pre-trained diffusion transformers to\nenable motion in the generated videos to reference user-provided motion videos\nin both text-to-video and image-to-video generation. To this end, we first\nintroduce a foreground-background disentangle module to distinguish between\nmoving subjects and backgrounds in the reference video, preventing interference\nin the target video generation. A reweighted motion transfer module is designed\nto allow the target video to reference the motion from the reference video. To\navoid interference from the subject in the reference video, we propose an\nappearance separation module to suppress the appearance of the reference\nsubject in the target video. We annotate the DAVIS dataset with detailed\nprompts for our experiments and design evaluation metrics to validate the\neffectiveness of our method. Extensive experiments demonstrate that our\napproach achieves state-of-the-art performance in generation quality,\nprompt-video consistency, and control capability. Our homepage is available at\nhttps://vpx-ecnu.github.io/LMP-Website/", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faLMP\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u524d\u666f\u80cc\u666f\u3001\u52a0\u6743\u8fd0\u52a8\u8f6c\u79fb\u548c\u5916\u89c2\u5206\u79bb\u6a21\u5757\uff0c\u5b9e\u73b0\u96f6\u6837\u672c\u89c6\u9891\u751f\u6210\u4e2d\u5bf9\u8fd0\u52a8\u7684\u7cbe\u7ec6\u63a7\u5236\u3002", "motivation": "\u5f53\u524dDiT\u6a21\u578b\u5728\u89c6\u9891\u751f\u6210\u4e2d\u7f3a\u4e4f\u5bf9\u5185\u5bb9\u7684\u7cbe\u7ec6\u63a7\u5236\uff0c\u5c24\u5176\u662f\u5728\u590d\u6742\u8fd0\u52a8\u63cf\u8ff0\u548c\u56fe\u50cf\u5230\u89c6\u9891\u751f\u6210\u4e2d\u7684\u8fd0\u52a8\u63a7\u5236\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u63d0\u51faLMP\u6846\u67b6\uff0c\u5305\u62ec\u524d\u666f\u80cc\u666f\u89e3\u8026\u6a21\u5757\u3001\u52a0\u6743\u8fd0\u52a8\u8f6c\u79fb\u6a21\u5757\u548c\u5916\u89c2\u5206\u79bb\u6a21\u5757\uff0c\u4ee5\u53c2\u8003\u7528\u6237\u63d0\u4f9b\u7684\u8fd0\u52a8\u89c6\u9891\u751f\u6210\u76ee\u6807\u89c6\u9891\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLMP\u5728\u751f\u6210\u8d28\u91cf\u3001\u63d0\u793a\u89c6\u9891\u4e00\u81f4\u6027\u548c\u63a7\u5236\u80fd\u529b\u65b9\u9762\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "LMP\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u89c6\u9891\u751f\u6210\u4e2d\u8fd0\u52a8\u63a7\u5236\u7684\u6311\u6218\uff0c\u4e3a\u6587\u672c\u5230\u89c6\u9891\u548c\u56fe\u50cf\u5230\u89c6\u9891\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2505.13936", "pdf": "https://arxiv.org/pdf/2505.13936", "abs": "https://arxiv.org/abs/2505.13936", "authors": ["Saydul Akbar Murad", "Ashim Dahal", "Nick Rahimi"], "title": "EEG-to-Text Translation: A Model for Deciphering Human Brain Activity", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "With the rapid advancement of large language models like Gemini, GPT, and\nothers, bridging the gap between the human brain and language processing has\nbecome an important area of focus. To address this challenge, researchers have\ndeveloped various models to decode EEG signals into text. However, these models\nstill face significant performance limitations. To overcome these shortcomings,\nwe propose a new model, R1 Translator, which aims to improve the performance of\nEEG-to-text decoding. The R1 Translator model combines a bidirectional LSTM\nencoder with a pretrained transformer-based decoder, utilizing EEG features to\nproduce high-quality text outputs. The model processes EEG embeddings through\nthe LSTM to capture sequential dependencies, which are then fed into the\ntransformer decoder for effective text generation. The R1 Translator excels in\nROUGE metrics, outperforming both T5 (previous research) and Brain Translator.\nSpecifically, R1 achieves a ROUGE-1 score of 38.00% (P), which is up to 9%\nhigher than T5 (34.89%) and 3% better than Brain (35.69%). It also leads in\nROUGE-L, with a F1 score of 32.51%, outperforming T5 by 3% (29.67%) and Brain\nby 2% (30.38%). In terms of CER, R1 achieves a CER of 0.5795, which is 2% lower\nthan T5 (0.5917) and 4% lower than Brain (0.6001). Additionally, R1 performs\nbetter in WER with a score of 0.7280, outperforming T5 by 4.3% (0.7610) and\nBrain by 3.6% (0.7553). Code is available at\nhttps://github.com/Mmurrad/EEG-To-text.", "AI": {"tldr": "R1 Translator\u6a21\u578b\u7ed3\u5408\u53cc\u5411LSTM\u7f16\u7801\u5668\u548c\u9884\u8bad\u7ec3Transformer\u89e3\u7801\u5668\uff0c\u663e\u8457\u63d0\u5347EEG\u4fe1\u53f7\u5230\u6587\u672c\u7684\u89e3\u7801\u6027\u80fd\uff0c\u5728ROUGE\u3001CER\u548cWER\u6307\u6807\u4e0a\u4f18\u4e8eT5\u548cBrain Translator\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5feb\u901f\u53d1\u5c55\uff0c\u4f46EEG\u4fe1\u53f7\u89e3\u7801\u4e3a\u6587\u672c\u4ecd\u5b58\u5728\u6027\u80fd\u9650\u5236\uff0c\u9700\u6539\u8fdb\u73b0\u6709\u6a21\u578b\u3002", "method": "R1 Translator\u7ed3\u5408\u53cc\u5411LSTM\u7f16\u7801\u5668\uff08\u6355\u6349\u5e8f\u5217\u4f9d\u8d56\uff09\u548c\u9884\u8bad\u7ec3Transformer\u89e3\u7801\u5668\uff0c\u5229\u7528EEG\u7279\u5f81\u751f\u6210\u9ad8\u8d28\u91cf\u6587\u672c\u3002", "result": "ROUGE-1\u5f97\u520638.00%\uff08\u4f18\u4e8eT5\u548cBrain\uff09\uff0cCER\u548cWER\u4e5f\u663e\u8457\u964d\u4f4e\u3002", "conclusion": "R1 Translator\u5728EEG\u5230\u6587\u672c\u89e3\u7801\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u65b0\u65b9\u5411\u3002"}}
{"id": "2505.14197", "pdf": "https://arxiv.org/pdf/2505.14197", "abs": "https://arxiv.org/abs/2505.14197", "authors": ["Xinshen Zhang", "Zhen Ye", "Xu Zheng"], "title": "Towards Omnidirectional Reasoning with 360-R1: A Dataset, Benchmark, and GRPO-based Method", "categories": ["cs.CV"], "comment": null, "summary": "Omnidirectional images (ODIs), with their 360{\\deg} field of view, provide\nunparalleled spatial awareness for immersive applications like augmented\nreality and embodied AI. However, the capability of existing multi-modal large\nlanguage models (MLLMs) to comprehend and reason about such panoramic scenes\nremains underexplored. This paper addresses this gap by introducing OmniVQA,\nthe first dataset and conducting the first benchmark for omnidirectional visual\nquestion answering. Our evaluation of state-of-the-art MLLMs reveals\nsignificant limitations in handling omnidirectional visual question answering,\nhighlighting persistent challenges in object localization, feature extraction,\nand hallucination suppression within panoramic contexts. These results\nunderscore the disconnect between current MLLM capabilities and the demands of\nomnidirectional visual understanding, which calls for dedicated architectural\nor training innovations tailored to 360{\\deg} imagery. Building on the OmniVQA\ndataset and benchmark, we further introduce a rule-based reinforcement learning\nmethod, 360-R1, based on Qwen2.5-VL-Instruct. Concretely, we modify the group\nrelative policy optimization (GRPO) by proposing three novel reward functions:\n(1) reasoning process similarity reward, (2) answer semantic accuracy reward,\nand (3) structured format compliance reward. Extensive experiments on our\nOmniVQA demonstrate the superiority of our proposed method in omnidirectional\nspace (+6% improvement).", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86OmniVQA\u6570\u636e\u96c6\u548c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u5168\u666f\u89c6\u89c9\u95ee\u7b54\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5176\u5b58\u5728\u663e\u8457\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u65b9\u6cd5360-R1\u4ee5\u6539\u8fdb\u6027\u80fd\u3002", "motivation": "\u73b0\u6709MLLMs\u5728\u5168\u666f\u56fe\u50cf\u7406\u89e3\u80fd\u529b\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0c\u7f3a\u4e4f\u9488\u5bf9360\u5ea6\u89c6\u89c9\u95ee\u7b54\u7684\u4e13\u7528\u6570\u636e\u96c6\u548c\u57fa\u51c6\u6d4b\u8bd5\u3002", "method": "\u5f15\u5165OmniVQA\u6570\u636e\u96c6\u548c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8eQwen2.5-VL-Instruct\u7684360-R1\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e09\u79cd\u65b0\u7684\u5956\u52b1\u51fd\u6570\u6539\u8fdbGRPO\u3002", "result": "\u5b9e\u9a8c\u8868\u660e360-R1\u5728\u5168\u666f\u7a7a\u95f4\u4e2d\u7684\u6027\u80fd\u63d0\u5347\u4e866%\u3002", "conclusion": "\u5f53\u524dMLLMs\u5728\u5168\u666f\u89c6\u89c9\u7406\u89e3\u4e0a\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u9488\u5bf9360\u5ea6\u56fe\u50cf\u7684\u4e13\u7528\u67b6\u6784\u6216\u8bad\u7ec3\u521b\u65b0\u3002"}}
{"id": "2505.13944", "pdf": "https://arxiv.org/pdf/2505.13944", "abs": "https://arxiv.org/abs/2505.13944", "authors": ["Bao-Ngoc Dao", "Quang Nguyen", "Luyen Ngo Dinh", "Minh Le", "Nam Le", "Linh Ngo Van"], "title": "Towards Rehearsal-Free Continual Relation Extraction: Capturing Within-Task Variance with Adaptive Prompting", "categories": ["cs.CL"], "comment": null, "summary": "Memory-based approaches have shown strong performance in Continual Relation\nExtraction (CRE). However, storing examples from previous tasks increases\nmemory usage and raises privacy concerns. Recently, prompt-based methods have\nemerged as a promising alternative, as they do not rely on storing past\nsamples. Despite this progress, current prompt-based techniques face several\ncore challenges in CRE, particularly in accurately identifying task identities\nand mitigating catastrophic forgetting. Existing prompt selection strategies\noften suffer from inaccuracies, lack robust mechanisms to prevent forgetting in\nshared parameters, and struggle to handle both cross-task and within-task\nvariations. In this paper, we propose WAVE++, a novel approach inspired by the\nconnection between prefix-tuning and mixture of experts. Specifically, we\nintroduce task-specific prompt pools that enhance flexibility and adaptability\nacross diverse tasks while avoiding boundary-spanning risks; this design more\neffectively captures variations within each task and across tasks. To further\nrefine relation classification, we incorporate label descriptions that provide\nricher, more global context, enabling the model to better distinguish among\ndifferent relations. We also propose a training-free mechanism to improve task\nprediction during inference. Moreover, we integrate a generative model to\nconsolidate prior knowledge within the shared parameters, thereby removing the\nneed for explicit data storage. Extensive experiments demonstrate that WAVE++\noutperforms state-of-the-art prompt-based and rehearsal-based methods, offering\na more robust solution for continual relation extraction. Our code is publicly\navailable at https://github.com/PiDinosauR2804/WAVE-CRE-PLUS-PLUS.", "AI": {"tldr": "WAVE++\u662f\u4e00\u79cd\u57fa\u4e8e\u63d0\u793a\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u4efb\u52a1\u7279\u5b9a\u63d0\u793a\u6c60\u548c\u6807\u7b7e\u63cf\u8ff0\u6539\u8fdb\u6301\u7eed\u5173\u7cfb\u63d0\u53d6\uff0c\u89e3\u51b3\u4e86\u4efb\u52a1\u8bc6\u522b\u548c\u9057\u5fd8\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u6301\u7eed\u5173\u7cfb\u63d0\u53d6\u4e2d\u57fa\u4e8e\u63d0\u793a\u65b9\u6cd5\u7684\u4efb\u52a1\u8bc6\u522b\u4e0d\u51c6\u786e\u3001\u9057\u5fd8\u95ee\u9898\u53ca\u8de8\u4efb\u52a1\u548c\u4efb\u52a1\u5185\u53d8\u5f02\u6027\u6311\u6218\u3002", "method": "\u5f15\u5165\u4efb\u52a1\u7279\u5b9a\u63d0\u793a\u6c60\u3001\u6807\u7b7e\u63cf\u8ff0\u548c\u751f\u6210\u6a21\u578b\uff0c\u7ed3\u5408\u8bad\u7ec3\u65e0\u5173\u7684\u4efb\u52a1\u9884\u6d4b\u673a\u5236\u3002", "result": "WAVE++\u5728\u6301\u7eed\u5173\u7cfb\u63d0\u53d6\u4e2d\u4f18\u4e8e\u73b0\u6709\u57fa\u4e8e\u63d0\u793a\u548c\u57fa\u4e8e\u8bb0\u5fc6\u7684\u65b9\u6cd5\u3002", "conclusion": "WAVE++\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u9c81\u68d2\u7684\u6301\u7eed\u5173\u7cfb\u63d0\u53d6\u89e3\u51b3\u65b9\u6848\uff0c\u65e0\u9700\u663e\u5f0f\u5b58\u50a8\u6570\u636e\u3002"}}
{"id": "2505.14204", "pdf": "https://arxiv.org/pdf/2505.14204", "abs": "https://arxiv.org/abs/2505.14204", "authors": ["Yang Hu", "Runchen Wang", "Stephen Chong Zhao", "Xuhui Zhan", "Do Hun Kim", "Mark Wallace", "David A. Tovar"], "title": "Beginning with You: Perceptual-Initialization Improves Vision-Language Representation and Alignment", "categories": ["cs.CV", "q-bio.NC"], "comment": "10 pages, 5 figures, 2 tables", "summary": "We introduce Perceptual-Initialization (PI), a paradigm shift in visual\nrepresentation learning that incorporates human perceptual structure during the\ninitialization phase rather than as a downstream fine-tuning step. By\nintegrating human-derived triplet embeddings from the NIGHTS dataset to\ninitialize a CLIP vision encoder, followed by self-supervised learning on\nYFCC15M, our approach demonstrates significant zero-shot performance\nimprovements, without any task-specific fine-tuning, across 29 zero shot\nclassification and 2 retrieval benchmarks. On ImageNet-1K, zero-shot gains\nemerge after approximately 15 epochs of pretraining. Benefits are observed\nacross datasets of various scales, with improvements manifesting at different\nstages of the pretraining process depending on dataset characteristics. Our\napproach consistently enhances zero-shot top-1 accuracy, top-5 accuracy, and\nretrieval recall (e.g., R@1, R@5) across these diverse evaluation tasks,\nwithout requiring any adaptation to target domains. These findings challenge\nthe conventional wisdom of using human-perceptual data primarily for\nfine-tuning and demonstrate that embedding human perceptual structure during\nearly representation learning yields more capable and vision-language aligned\nsystems that generalize immediately to unseen tasks. Our work shows that\n\"beginning with you\", starting with human perception, provides a stronger\nfoundation for general-purpose vision-language intelligence.", "AI": {"tldr": "Perceptual-Initialization (PI) \u901a\u8fc7\u5728\u521d\u59cb\u5316\u9636\u6bb5\u878d\u5165\u4eba\u7c7b\u611f\u77e5\u7ed3\u6784\uff0c\u663e\u8457\u63d0\u5347\u4e86\u96f6\u6837\u672c\u6027\u80fd\uff0c\u65e0\u9700\u4efb\u52a1\u7279\u5b9a\u5fae\u8c03\u3002", "motivation": "\u6311\u6218\u4f20\u7edf\u65b9\u6cd5\uff0c\u63a2\u7d22\u5728\u65e9\u671f\u8868\u793a\u5b66\u4e60\u4e2d\u5d4c\u5165\u4eba\u7c7b\u611f\u77e5\u7ed3\u6784\u7684\u6548\u679c\uff0c\u4ee5\u63d0\u5347\u89c6\u89c9-\u8bed\u8a00\u5bf9\u9f50\u7cfb\u7edf\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u5229\u7528NIGHTS\u6570\u636e\u96c6\u7684\u4eba\u7c7b\u611f\u77e5\u4e09\u5143\u7ec4\u5d4c\u5165\u521d\u59cb\u5316CLIP\u89c6\u89c9\u7f16\u7801\u5668\uff0c\u968f\u540e\u5728YFCC15M\u4e0a\u8fdb\u884c\u81ea\u76d1\u7763\u5b66\u4e60\u3002", "result": "\u572829\u4e2a\u96f6\u6837\u672c\u5206\u7c7b\u548c2\u4e2a\u68c0\u7d22\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u663e\u8457\u63d0\u5347\uff0cImageNet-1K\u4e0a\u4ec5\u970015\u8f6e\u9884\u8bad\u7ec3\u5373\u663e\u73b0\u589e\u76ca\u3002", "conclusion": "\u65e9\u671f\u878d\u5165\u4eba\u7c7b\u611f\u77e5\u7ed3\u6784\u4e3a\u901a\u7528\u89c6\u89c9-\u8bed\u8a00\u667a\u80fd\u63d0\u4f9b\u4e86\u66f4\u5f3a\u57fa\u7840\uff0c\u6311\u6218\u4e86\u4f20\u7edf\u4ec5\u7528\u4e8e\u5fae\u8c03\u7684\u505a\u6cd5\u3002"}}
{"id": "2505.13948", "pdf": "https://arxiv.org/pdf/2505.13948", "abs": "https://arxiv.org/abs/2505.13948", "authors": ["Mingliang Zhai", "Zhi Gao", "Yuwei Wu", "Yunde Jia"], "title": "Memory-Centric Embodied Question Answer", "categories": ["cs.CL", "cs.AI", "cs.MM"], "comment": "14pages, 7 figures, 6 tables", "summary": "Embodied Question Answering (EQA) requires agents to autonomously explore and\nunderstand the environment to answer context-dependent questions. Existing\nframeworks typically center around the planner, which guides the stopping\nmodule, memory module, and answering module for reasoning. In this paper, we\npropose a memory-centric EQA framework named MemoryEQA. Unlike planner-centric\nEQA models where the memory module cannot fully interact with other modules,\nMemoryEQA flexible feeds memory information into all modules, thereby enhancing\nefficiency and accuracy in handling complex tasks, such as those involving\nmultiple targets across different regions. Specifically, we establish a\nmulti-modal hierarchical memory mechanism, which is divided into global memory\nthat stores language-enhanced scene maps, and local memory that retains\nhistorical observations and state information. When performing EQA tasks, the\nmulti-modal large language model is leveraged to convert memory information\ninto the required input formats for injection into different modules. To\nevaluate EQA models' memory capabilities, we constructed the MT-HM3D dataset\nbased on HM3D, comprising 1,587 question-answer pairs involving multiple\ntargets across various regions, which requires agents to maintain memory of\nexploration-acquired target information. Experimental results on HM-EQA,\nMT-HM3D, and OpenEQA demonstrate the effectiveness of our framework, where a\n19.8% performance gain on MT-HM3D compared to baseline model further\nunderscores memory capability's pivotal role in resolving complex tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4ee5\u8bb0\u5fc6\u4e3a\u4e2d\u5fc3\u7684EQA\u6846\u67b6MemoryEQA\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u5206\u5c42\u8bb0\u5fc6\u673a\u5236\u63d0\u5347\u590d\u6742\u4efb\u52a1\u5904\u7406\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709EQA\u6846\u67b6\u4ee5\u89c4\u5212\u5668\u4e3a\u4e2d\u5fc3\uff0c\u8bb0\u5fc6\u6a21\u5757\u65e0\u6cd5\u5145\u5206\u4e0e\u5176\u4ed6\u6a21\u5757\u4ea4\u4e92\uff0c\u9650\u5236\u4e86\u590d\u6742\u4efb\u52a1\uff08\u5982\u8de8\u533a\u57df\u591a\u76ee\u6807\u4efb\u52a1\uff09\u7684\u5904\u7406\u80fd\u529b\u3002", "method": "\u63d0\u51faMemoryEQA\u6846\u67b6\uff0c\u91c7\u7528\u5168\u5c40\u548c\u5c40\u90e8\u8bb0\u5fc6\u5206\u5c42\u673a\u5236\uff0c\u5229\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5c06\u8bb0\u5fc6\u4fe1\u606f\u8f6c\u6362\u4e3a\u6a21\u5757\u8f93\u5165\u683c\u5f0f\u3002", "result": "\u5728HM-EQA\u3001MT-HM3D\u548cOpenEQA\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u6846\u67b6\u6709\u6548\u6027\uff0cMT-HM3D\u4e0a\u6027\u80fd\u63d0\u534719.8%\u3002", "conclusion": "\u8bb0\u5fc6\u80fd\u529b\u5bf9\u89e3\u51b3\u590d\u6742EQA\u4efb\u52a1\u81f3\u5173\u91cd\u8981\uff0cMemoryEQA\u901a\u8fc7\u4f18\u5316\u8bb0\u5fc6\u4ea4\u4e92\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002"}}
{"id": "2505.14218", "pdf": "https://arxiv.org/pdf/2505.14218", "abs": "https://arxiv.org/abs/2505.14218", "authors": ["Jie Li", "Shengwei Tian", "Long Yu", "Xin Ning"], "title": "Flexible-weighted Chamfer Distance: Enhanced Objective Function for Point Cloud Completion", "categories": ["cs.CV"], "comment": null, "summary": "Chamfer Distance (CD) comprises two components that can evaluate the global\ndistribution and local performance of generated point clouds, making it widely\nutilized as a similarity measure between generated and target point clouds in\npoint cloud completion tasks. Additionally, CD's computational efficiency has\nled to its frequent application as an objective function for guiding point\ncloud generation. However, using CD directly as an objective function with\nfixed equal weights for its two components can often result in seemingly high\noverall performance (i.e., low CD score), while failing to achieve a good\nglobal distribution. This is typically reflected in high Earth Mover's Distance\n(EMD) and Decomposed Chamfer Distance (DCD) scores, alongside poor human\nassessments. To address this issue, we propose a Flexible-Weighted Chamfer\nDistance (FCD) to guide point cloud generation. FCD assigns a higher weight to\nthe global distribution component of CD and incorporates a flexible weighting\nstrategy to adjust the balance between the two components, aiming to improve\nglobal distribution while maintaining robust overall performance. Experimental\nresults on two state-of-the-art networks demonstrate that our method achieves\nsuperior results across multiple evaluation metrics, including CD, EMD, DCD,\nand F-Score, as well as in human evaluations.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7075\u6d3b\u52a0\u6743\u7684Chamfer\u8ddd\u79bb\uff08FCD\uff09\uff0c\u7528\u4e8e\u6539\u8fdb\u70b9\u4e91\u751f\u6210\u4efb\u52a1\u4e2d\u5168\u5c40\u5206\u5e03\u4e0e\u5c40\u90e8\u6027\u80fd\u7684\u5e73\u8861\u3002", "motivation": "\u76f4\u63a5\u4f7f\u7528\u56fa\u5b9a\u6743\u91cd\u7684Chamfer\u8ddd\u79bb\uff08CD\uff09\u4f5c\u4e3a\u76ee\u6807\u51fd\u6570\u53ef\u80fd\u5bfc\u81f4\u5168\u5c40\u5206\u5e03\u4e0d\u4f73\uff0c\u5c3d\u7ba1\u6574\u4f53\u6027\u80fd\u770b\u4f3c\u826f\u597d\u3002", "method": "\u63d0\u51faFCD\uff0c\u901a\u8fc7\u4e3aCD\u7684\u5168\u5c40\u5206\u5e03\u7ec4\u4ef6\u5206\u914d\u66f4\u9ad8\u6743\u91cd\uff0c\u5e76\u91c7\u7528\u7075\u6d3b\u52a0\u6743\u7b56\u7565\u8c03\u6574\u7ec4\u4ef6\u95f4\u7684\u5e73\u8861\u3002", "result": "\u5728\u4e24\u4e2a\u5148\u8fdb\u7f51\u7edc\u4e0a\u9a8c\u8bc1\uff0cFCD\u5728CD\u3001EMD\u3001DCD\u3001F-Score\u53ca\u4eba\u5de5\u8bc4\u4f30\u4e2d\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "FCD\u80fd\u6709\u6548\u63d0\u5347\u70b9\u4e91\u751f\u6210\u7684\u5168\u5c40\u5206\u5e03\u8d28\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u6574\u4f53\u6027\u80fd\u3002"}}
{"id": "2505.13949", "pdf": "https://arxiv.org/pdf/2505.13949", "abs": "https://arxiv.org/abs/2505.13949", "authors": ["Guochao Jiang", "Guofeng Quan", "Zepeng Ding", "Ziqin Luo", "Dixuan Wang", "Zheng Hu"], "title": "FlashThink: An Early Exit Method For Efficient Reasoning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have shown impressive performance in reasoning\ntasks. However, LLMs tend to generate excessively long reasoning content,\nleading to significant computational overhead. Our observations indicate that\neven on simple problems, LLMs tend to produce unnecessarily lengthy reasoning\ncontent, which is against intuitive expectations. Preliminary experiments show\nthat at a certain point during the generation process, the model is already\ncapable of producing the correct solution without completing the full reasoning\ncontent. Therefore, we consider that the reasoning process of the model can be\nexited early to achieve the purpose of efficient reasoning. We introduce a\nverification model that identifies the exact moment when the model can stop\nreasoning and still provide the correct answer. Comprehensive experiments on\nfour different benchmarks demonstrate that our proposed method, FlashThink,\neffectively shortens the reasoning content while preserving the model accuracy.\nFor the Deepseek-R1 and QwQ-32B models, we reduced the length of reasoning\ncontent by 77.04% and 77.47%, respectively, without reducing the accuracy.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFlashThink\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u9a8c\u8bc1\u6a21\u578b\u63d0\u524d\u7ec8\u6b62\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u4ee5\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\uff0c\u540c\u65f6\u4fdd\u6301\u51c6\u786e\u6027\u3002", "motivation": "LLMs\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5e38\u751f\u6210\u5197\u957f\u7684\u63a8\u7406\u5185\u5bb9\uff0c\u5bfc\u81f4\u8ba1\u7b97\u8d44\u6e90\u6d6a\u8d39\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u6a21\u578b\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u53ef\u80fd\u63d0\u524d\u5f97\u51fa\u6b63\u786e\u7b54\u6848\uff0c\u65e0\u9700\u5b8c\u6210\u5168\u90e8\u63a8\u7406\u3002", "method": "\u5f15\u5165\u9a8c\u8bc1\u6a21\u578b\uff0c\u8bc6\u522b\u6a21\u578b\u4f55\u65f6\u53ef\u4ee5\u505c\u6b62\u63a8\u7406\u4f46\u4ecd\u80fd\u63d0\u4f9b\u6b63\u786e\u7b54\u6848\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cFlashThink\u663e\u8457\u7f29\u77ed\u4e86\u63a8\u7406\u5185\u5bb9\u957f\u5ea6\uff08\u5982Deepseek-R1\u548cQwQ-32B\u6a21\u578b\u5206\u522b\u51cf\u5c1177.04%\u548c77.47%\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u51c6\u786e\u6027\u3002", "conclusion": "FlashThink\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86LLMs\u63a8\u7406\u5197\u957f\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u63a8\u7406\u3002"}}
{"id": "2505.14227", "pdf": "https://arxiv.org/pdf/2505.14227", "abs": "https://arxiv.org/abs/2505.14227", "authors": ["Luyang Jiang", "Jianing An", "Jie Luo", "Wenjun Wu", "Lei Huang"], "title": "VoQA: Visual-only Question Answering", "categories": ["cs.CV", "cs.AI"], "comment": "18 pages", "summary": "We propose Visual-only Question Answering (VoQA), a novel multimodal task in\nwhich questions are visually embedded within images, without any accompanying\ntextual input. This requires models to locate, recognize, and reason over\nvisually embedded textual questions, posing challenges for existing large\nvision-language models (LVLMs), which show notable performance drops even with\ncarefully designed prompts. To bridge this gap, we introduce Guided Response\nTriggering Supervised Fine-tuning (GRT-SFT), a structured fine-tuning strategy\nthat guides the model to perform step-by-step reasoning purely based on visual\ninput, significantly improving model performance. Our work enhances models'\ncapacity for human-like visual understanding in complex multimodal scenarios,\nwhere information, including language, is perceived visually.", "AI": {"tldr": "\u63d0\u51faVoQA\u4efb\u52a1\uff0c\u8981\u6c42\u6a21\u578b\u4ec5\u901a\u8fc7\u89c6\u89c9\u8f93\u5165\u56de\u7b54\u5d4c\u5165\u56fe\u50cf\u4e2d\u7684\u95ee\u9898\uff0c\u5e76\u5f15\u5165GRT-SFT\u65b9\u6cd5\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u7eaf\u89c6\u89c9\u95ee\u9898\u56de\u7b54\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u6539\u8fdb\u4ee5\u589e\u5f3a\u89c6\u89c9\u7406\u89e3\u80fd\u529b\u3002", "method": "\u91c7\u7528GRT-SFT\u7b56\u7565\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u5fae\u8c03\u5f15\u5bfc\u6a21\u578b\u9010\u6b65\u63a8\u7406\u3002", "result": "GRT-SFT\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728VoQA\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u5347\u4e86\u6a21\u578b\u5728\u590d\u6742\u591a\u6a21\u6001\u573a\u666f\u4e2d\u7684\u7eaf\u89c6\u89c9\u7406\u89e3\u80fd\u529b\u3002"}}
{"id": "2505.13963", "pdf": "https://arxiv.org/pdf/2505.13963", "abs": "https://arxiv.org/abs/2505.13963", "authors": ["Qianli Wang", "Mingyang Wang", "Nils Feldhus", "Simon Ostermann", "Yuan Cao", "Hinrich Sch\u00fctze", "Sebastian M\u00f6ller", "Vera Schmitt"], "title": "Through a Compressed Lens: Investigating the Impact of Quantization on LLM Explainability and Interpretability", "categories": ["cs.CL", "cs.LG"], "comment": "In submission", "summary": "Quantization methods are widely used to accelerate inference and streamline\nthe deployment of large language models (LLMs). While prior research has\nextensively investigated the degradation of various LLM capabilities due to\nquantization, its effects on model explainability and interpretability, which\nare crucial for understanding decision-making processes, remain unexplored. To\naddress this gap, we conduct comprehensive experiments using three common\nquantization techniques at distinct bit widths, in conjunction with two\nexplainability methods, counterfactual examples and natural language\nexplanations, as well as two interpretability approaches, knowledge\nmemorization analysis and latent multi-hop reasoning analysis. We complement\nour analysis with a thorough user study, evaluating selected explainability\nmethods. Our findings reveal that, depending on the configuration, quantization\ncan significantly impact model explainability and interpretability. Notably,\nthe direction of this effect is not consistent, as it strongly depends on (1)\nthe quantization method, (2) the explainability or interpretability approach,\nand (3) the evaluation protocol. In some settings, human evaluation shows that\nquantization degrades explainability, while in others, it even leads to\nimprovements. Our work serves as a cautionary tale, demonstrating that\nquantization can unpredictably affect model transparency. This insight has\nimportant implications for deploying LLMs in applications where transparency is\na critical requirement.", "AI": {"tldr": "\u91cf\u5316\u65b9\u6cd5\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u53ef\u89e3\u91ca\u6027\u548c\u900f\u660e\u5ea6\u6709\u663e\u8457\u5f71\u54cd\uff0c\u6548\u679c\u56e0\u91cf\u5316\u65b9\u6cd5\u3001\u89e3\u91ca\u65b9\u6cd5\u548c\u8bc4\u4f30\u534f\u8bae\u800c\u5f02\u3002", "motivation": "\u7814\u7a76\u91cf\u5316\u5bf9LLM\u53ef\u89e3\u91ca\u6027\u548c\u900f\u660e\u5ea6\u7684\u5f71\u54cd\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u4f7f\u7528\u4e09\u79cd\u91cf\u5316\u6280\u672f\u548c\u4e24\u79cd\u89e3\u91ca\u65b9\u6cd5\uff08\u53cd\u4e8b\u5b9e\u793a\u4f8b\u548c\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\uff09\u53ca\u4e24\u79cd\u900f\u660e\u5ea6\u65b9\u6cd5\uff08\u77e5\u8bc6\u8bb0\u5fc6\u5206\u6790\u548c\u6f5c\u5728\u591a\u8df3\u63a8\u7406\u5206\u6790\uff09\uff0c\u7ed3\u5408\u7528\u6237\u7814\u7a76\u3002", "result": "\u91cf\u5316\u5bf9\u53ef\u89e3\u91ca\u6027\u548c\u900f\u660e\u5ea6\u7684\u5f71\u54cd\u4e0d\u4e00\u81f4\uff0c\u53ef\u80fd\u964d\u4f4e\u6216\u63d0\u5347\u6548\u679c\uff0c\u53d6\u51b3\u4e8e\u5177\u4f53\u914d\u7f6e\u3002", "conclusion": "\u91cf\u5316\u53ef\u80fd\u4e0d\u53ef\u9884\u6d4b\u5730\u5f71\u54cd\u6a21\u578b\u900f\u660e\u5ea6\uff0c\u5bf9\u9700\u8981\u9ad8\u900f\u660e\u5ea6\u7684LLM\u5e94\u7528\u5177\u6709\u91cd\u8981\u542f\u793a\u3002"}}
{"id": "2505.14231", "pdf": "https://arxiv.org/pdf/2505.14231", "abs": "https://arxiv.org/abs/2505.14231", "authors": ["Sule Bai", "Mingxing Li", "Yong Liu", "Jing Tang", "Haoji Zhang", "Lei Sun", "Xiangxiang Chu", "Yansong Tang"], "title": "UniVG-R1: Reasoning Guided Universal Visual Grounding with Reinforcement Learning", "categories": ["cs.CV"], "comment": null, "summary": "Traditional visual grounding methods primarily focus on single-image\nscenarios with simple textual references. However, extending these methods to\nreal-world scenarios that involve implicit and complex instructions,\nparticularly in conjunction with multiple images, poses significant challenges,\nwhich is mainly due to the lack of advanced reasoning ability across diverse\nmulti-modal contexts. In this work, we aim to address the more practical\nuniversal grounding task, and propose UniVG-R1, a reasoning guided multimodal\nlarge language model (MLLM) for universal visual grounding, which enhances\nreasoning capabilities through reinforcement learning (RL) combined with\ncold-start data. Specifically, we first construct a high-quality\nChain-of-Thought (CoT) grounding dataset, annotated with detailed reasoning\nchains, to guide the model towards correct reasoning paths via supervised\nfine-tuning. Subsequently, we perform rule-based reinforcement learning to\nencourage the model to identify correct reasoning chains, thereby incentivizing\nits reasoning capabilities. In addition, we identify a difficulty bias arising\nfrom the prevalence of easy samples as RL training progresses, and we propose a\ndifficulty-aware weight adjustment strategy to further strengthen the\nperformance. Experimental results demonstrate the effectiveness of UniVG-R1,\nwhich achieves state-of-the-art performance on MIG-Bench with a 9.1%\nimprovement over the previous method. Furthermore, our model exhibits strong\ngeneralizability, achieving an average improvement of 23.4% in zero-shot\nperformance across four image and video reasoning grounding benchmarks. The\nproject page can be accessed at https://amap-ml.github.io/UniVG-R1-page/.", "AI": {"tldr": "UniVG-R1\u662f\u4e00\u4e2a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u7528\u4e8e\u89e3\u51b3\u590d\u6742\u6307\u4ee4\u548c\u591a\u56fe\u50cf\u573a\u666f\u4e0b\u7684\u89c6\u89c9\u5b9a\u4f4d\u4efb\u52a1\uff0c\u901a\u8fc7\u63a8\u7406\u94fe\u6570\u636e\u96c6\u548c\u96be\u5ea6\u611f\u77e5\u7b56\u7565\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u89c6\u89c9\u5b9a\u4f4d\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u590d\u6742\u6307\u4ee4\u548c\u591a\u56fe\u50cf\u573a\u666f\uff0c\u7f3a\u4e4f\u8de8\u6a21\u6001\u63a8\u7406\u80fd\u529b\u3002", "method": "\u6784\u5efa\u63a8\u7406\u94fe\u6570\u636e\u96c6\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\uff0c\u7ed3\u5408\u57fa\u4e8e\u89c4\u5219\u7684\u5f3a\u5316\u5b66\u4e60\u548c\u96be\u5ea6\u611f\u77e5\u6743\u91cd\u8c03\u6574\u7b56\u7565\u3002", "result": "\u5728MIG-Bench\u4e0a\u6027\u80fd\u63d0\u53479.1%\uff0c\u96f6\u6837\u672c\u6027\u80fd\u5e73\u5747\u63d0\u534723.4%\u3002", "conclusion": "UniVG-R1\u5728\u590d\u6742\u89c6\u89c9\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u5f3a\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2505.13965", "pdf": "https://arxiv.org/pdf/2505.13965", "abs": "https://arxiv.org/abs/2505.13965", "authors": ["Jiamin Su", "Yibo Yan", "Zhuoran Gao", "Han Zhang", "Xiang Liu", "Xuming Hu"], "title": "CAFES: A Collaborative Multi-Agent Framework for Multi-Granular Multimodal Essay Scoring", "categories": ["cs.CL", "cs.AI"], "comment": "arXiv admin note: substantial text overlap with arXiv:2502.11916", "summary": "Automated Essay Scoring (AES) is crucial for modern education, particularly\nwith the increasing prevalence of multimodal assessments. However, traditional\nAES methods struggle with evaluation generalizability and multimodal\nperception, while even recent Multimodal Large Language Model (MLLM)-based\napproaches can produce hallucinated justifications and scores misaligned with\nhuman judgment. To address the limitations, we introduce CAFES, the first\ncollaborative multi-agent framework specifically designed for AES. It\norchestrates three specialized agents: an Initial Scorer for rapid,\ntrait-specific evaluations; a Feedback Pool Manager to aggregate detailed,\nevidence-grounded strengths; and a Reflective Scorer that iteratively refines\nscores based on this feedback to enhance human alignment. Extensive\nexperiments, using state-of-the-art MLLMs, achieve an average relative\nimprovement of 21% in Quadratic Weighted Kappa (QWK) against ground truth,\nespecially for grammatical and lexical diversity. Our proposed CAFES framework\npaves the way for an intelligent multimodal AES system. The code will be\navailable upon acceptance.", "AI": {"tldr": "CAFES\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u5347\u81ea\u52a8\u4f5c\u6587\u8bc4\u5206\uff08AES\uff09\u7684\u901a\u7528\u6027\u548c\u591a\u6a21\u6001\u611f\u77e5\u80fd\u529b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8bc4\u5206\u4e0e\u4eba\u7c7b\u5224\u65ad\u7684\u4e00\u81f4\u6027\u3002", "motivation": "\u4f20\u7edfAES\u65b9\u6cd5\u5728\u591a\u6a21\u6001\u8bc4\u4f30\u548c\u8bc4\u5206\u901a\u7528\u6027\u4e0a\u8868\u73b0\u4e0d\u8db3\uff0c\u800c\u73b0\u6709MLLM\u65b9\u6cd5\u5b58\u5728\u5e7b\u89c9\u6027\u89e3\u91ca\u548c\u8bc4\u5206\u504f\u5dee\u95ee\u9898\u3002", "method": "CAFES\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u667a\u80fd\u4f53\uff1a\u521d\u59cb\u8bc4\u5206\u5668\u3001\u53cd\u9988\u6c60\u7ba1\u7406\u5668\u548c\u53cd\u601d\u8bc4\u5206\u5668\uff0c\u901a\u8fc7\u534f\u4f5c\u8fed\u4ee3\u4f18\u5316\u8bc4\u5206\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCAFES\u5728QWK\u6307\u6807\u4e0a\u76f8\u5bf9\u63d0\u5347\u4e8621%\uff0c\u5c24\u5176\u5728\u8bed\u6cd5\u548c\u8bcd\u6c47\u591a\u6837\u6027\u65b9\u9762\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "CAFES\u4e3a\u667a\u80fd\u591a\u6a21\u6001AES\u7cfb\u7edf\u7684\u53d1\u5c55\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2505.14239", "pdf": "https://arxiv.org/pdf/2505.14239", "abs": "https://arxiv.org/abs/2505.14239", "authors": ["Bin-Bin Gao", "Xiaochen Chen", "Zhongyi Huang", "Congchong Nie", "Jun Liu", "Jinxiang Lai", "Guannan Jiang", "Xi Wang", "Chengjie Wang"], "title": "Decoupling Classifier for Boosting Few-shot Object Detection and Instance Segmentation", "categories": ["cs.CV"], "comment": "Accepted by NeurIPS 2022", "summary": "This paper focus on few-shot object detection~(FSOD) and instance\nsegmentation~(FSIS), which requires a model to quickly adapt to novel classes\nwith a few labeled instances. The existing methods severely suffer from bias\nclassification because of the missing label issue which naturally exists in an\ninstance-level few-shot scenario and is first formally proposed by us. Our\nanalysis suggests that the standard classification head of most FSOD or FSIS\nmodels needs to be decoupled to mitigate the bias classification. Therefore, we\npropose an embarrassingly simple but effective method that decouples the\nstandard classifier into two heads. Then, these two individual heads are\ncapable of independently addressing clear positive samples and noisy negative\nsamples which are caused by the missing label. In this way, the model can\neffectively learn novel classes while mitigating the effects of noisy negative\nsamples. Without bells and whistles, our model without any additional\ncomputation cost and parameters consistently outperforms its baseline and\nstate-of-the-art by a large margin on PASCAL VOC and MS-COCO benchmarks for\nFSOD and FSIS tasks. The Code is available at\nhttps://csgaobb.github.io/Projects/DCFS.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u4f46\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u89e3\u8026\u5206\u7c7b\u5668\u6765\u7f13\u89e3\u5c0f\u6837\u672c\u76ee\u6807\u68c0\u6d4b\u548c\u5b9e\u4f8b\u5206\u5272\u4e2d\u7684\u504f\u5dee\u5206\u7c7b\u95ee\u9898\u3002", "motivation": "\u5c0f\u6837\u672c\u76ee\u6807\u68c0\u6d4b\u548c\u5b9e\u4f8b\u5206\u5272\u4e2d\uff0c\u6a21\u578b\u56e0\u7f3a\u5931\u6807\u7b7e\u95ee\u9898\u5bfc\u81f4\u5206\u7c7b\u504f\u5dee\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u89e3\u51b3\u3002", "method": "\u5c06\u6807\u51c6\u5206\u7c7b\u5668\u89e3\u8026\u4e3a\u4e24\u4e2a\u72ec\u7acb\u7684\u5934\uff0c\u5206\u522b\u5904\u7406\u6e05\u6670\u7684\u6b63\u6837\u672c\u548c\u7531\u7f3a\u5931\u6807\u7b7e\u5f15\u8d77\u7684\u566a\u58f0\u8d1f\u6837\u672c\u3002", "result": "\u5728PASCAL VOC\u548cMS-COCO\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6a21\u578b\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u548c\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\uff0c\u4e14\u65e0\u9700\u989d\u5916\u8ba1\u7b97\u6210\u672c\u3002", "conclusion": "\u89e3\u8026\u5206\u7c7b\u5668\u7684\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u504f\u5dee\u5206\u7c7b\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u5c0f\u6837\u672c\u5b66\u4e60\u7684\u6027\u80fd\u3002"}}
{"id": "2505.13972", "pdf": "https://arxiv.org/pdf/2505.13972", "abs": "https://arxiv.org/abs/2505.13972", "authors": ["Qianli Wang", "Van Bach Nguyen", "Nils Feldhus", "Luis Felipe Villa-Arenas", "Christin Seifert", "Sebastian M\u00f6ller", "Vera Schmitt"], "title": "Truth or Twist? Optimal Model Selection for Reliable Label Flipping Evaluation in LLM-based Counterfactuals", "categories": ["cs.CL"], "comment": "in submission", "summary": "Counterfactual examples are widely employed to enhance the performance and\nrobustness of large language models (LLMs) through counterfactual data\naugmentation (CDA). However, the selection of the judge model used to evaluate\nlabel flipping, the primary metric for assessing the validity of generated\ncounterfactuals for CDA, yields inconsistent results. To decipher this, we\ndefine four types of relationships between the counterfactual generator and\njudge models. Through extensive experiments involving two state-of-the-art\nLLM-based methods, three datasets, five generator models, and 15 judge models,\ncomplemented by a user study (n = 90), we demonstrate that judge models with an\nindependent, non-fine-tuned relationship to the generator model provide the\nmost reliable label flipping evaluations. Relationships between the generator\nand judge models, which are closely aligned with the user study for CDA, result\nin better model performance and robustness. Nevertheless, we find that the gap\nbetween the most effective judge models and the results obtained from the user\nstudy remains considerably large. This suggests that a fully automated pipeline\nfor CDA may be inadequate and requires human intervention.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u53cd\u4e8b\u5b9e\u6570\u636e\u589e\u5f3a\uff08CDA\uff09\u4e2d\uff0c\u9009\u62e9\u72ec\u7acb\u4e14\u672a\u7ecf\u5fae\u8c03\u7684\u8bc4\u4f30\u6a21\u578b\u80fd\u63d0\u4f9b\u6700\u53ef\u9760\u7684\u6807\u7b7e\u7ffb\u8f6c\u8bc4\u4f30\uff0c\u4f46\u81ea\u52a8\u5316\u6d41\u7a0b\u4ecd\u9700\u4eba\u5de5\u5e72\u9884\u3002", "motivation": "\u63a2\u8ba8\u8bc4\u4f30\u6a21\u578b\u9009\u62e9\u5bf9\u53cd\u4e8b\u5b9e\u6570\u636e\u589e\u5f3a\uff08CDA\uff09\u4e2d\u6807\u7b7e\u7ffb\u8f6c\u8bc4\u4f30\u7684\u5f71\u54cd\uff0c\u4ee5\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u5206\u6790\u56db\u79cd\u751f\u6210\u6a21\u578b\u4e0e\u8bc4\u4f30\u6a21\u578b\u7684\u5173\u7cfb\uff0c\u6d89\u53ca\u4e24\u79cd\u5148\u8fdbLLM\u65b9\u6cd5\u3001\u4e09\u4e2a\u6570\u636e\u96c6\u3001\u4e94\u4e2a\u751f\u6210\u6a21\u578b\u548c15\u4e2a\u8bc4\u4f30\u6a21\u578b\uff0c\u5e76\u7ed3\u5408\u7528\u6237\u7814\u7a76\uff08n=90\uff09\u3002", "result": "\u72ec\u7acb\u4e14\u672a\u7ecf\u5fae\u8c03\u7684\u8bc4\u4f30\u6a21\u578b\u8868\u73b0\u6700\u53ef\u9760\uff0c\u4f46\u4e0e\u7528\u6237\u7814\u7a76\u7ed3\u679c\u4ecd\u6709\u8f83\u5927\u5dee\u8ddd\u3002", "conclusion": "\u5b8c\u5168\u81ea\u52a8\u5316\u7684CDA\u6d41\u7a0b\u53ef\u80fd\u4e0d\u8db3\uff0c\u9700\u7ed3\u5408\u4eba\u5de5\u5e72\u9884\u3002"}}
{"id": "2505.14246", "pdf": "https://arxiv.org/pdf/2505.14246", "abs": "https://arxiv.org/abs/2505.14246", "authors": ["Ziyu Liu", "Yuhang Zang", "Yushan Zou", "Zijian Liang", "Xiaoyi Dong", "Yuhang Cao", "Haodong Duan", "Dahua Lin", "Jiaqi Wang"], "title": "Visual Agentic Reinforcement Fine-Tuning", "categories": ["cs.CV", "cs.AI"], "comment": "project url:\n  https://github.com/Liuziyu77/Visual-RFT/tree/main/Visual-ARFT", "summary": "A key trend in Large Reasoning Models (e.g., OpenAI's o3) is the native\nagentic ability to use external tools such as web browsers for searching and\nwriting/executing code for image manipulation to think with images. In the\nopen-source research community, while significant progress has been made in\nlanguage-only agentic abilities such as function calling and tool integration,\nthe development of multi-modal agentic capabilities that involve truly thinking\nwith images, and their corresponding benchmarks, are still less explored. This\nwork highlights the effectiveness of Visual Agentic Reinforcement Fine-Tuning\n(Visual-ARFT) for enabling flexible and adaptive reasoning abilities for Large\nVision-Language Models (LVLMs). With Visual-ARFT, open-source LVLMs gain the\nability to browse websites for real-time information updates and write code to\nmanipulate and analyze input images through cropping, rotation, and other image\nprocessing techniques. We also present a Multi-modal Agentic Tool Bench (MAT)\nwith two settings (MAT-Search and MAT-Coding) designed to evaluate LVLMs'\nagentic search and coding abilities. Our experimental results demonstrate that\nVisual-ARFT outperforms its baseline by +18.6% F1 / +13.0% EM on MAT-Coding and\n+10.3% F1 / +8.7% EM on MAT-Search, ultimately surpassing GPT-4o. Visual-ARFT\nalso achieves +29.3 F1% / +25.9% EM gains on existing multi-hop QA benchmarks\nsuch as 2Wiki and HotpotQA, demonstrating strong generalization capabilities.\nOur findings suggest that Visual-ARFT offers a promising path toward building\nrobust and generalizable multimodal agents.", "AI": {"tldr": "Visual-ARFT\u65b9\u6cd5\u901a\u8fc7\u5f3a\u5316\u5fae\u8c03\u63d0\u5347\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u591a\u6a21\u6001\u4ee3\u7406\u80fd\u529b\uff0c\u5728\u641c\u7d22\u548c\u7f16\u7801\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u548cGPT-4o\uff0c\u5e76\u5c55\u73b0\u51fa\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5f00\u6e90\u793e\u533a\u5728\u591a\u6a21\u6001\u4ee3\u7406\u80fd\u529b\uff08\u5c24\u5176\u662f\u56fe\u50cf\u601d\u7ef4\uff09\u65b9\u9762\u7684\u7814\u7a76\u8f83\u5c11\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u91c7\u7528Visual-ARFT\uff08\u89c6\u89c9\u4ee3\u7406\u5f3a\u5316\u5fae\u8c03\uff09\u65b9\u6cd5\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u5b9e\u65f6\u6d4f\u89c8\u7f51\u9875\u3001\u7f16\u5199\u4ee3\u7801\u5904\u7406\u56fe\u50cf\u3002", "result": "\u5728MAT-Coding\u548cMAT-Search\u4efb\u52a1\u4e2d\u5206\u522b\u63d0\u534718.6% F1/13.0% EM\u548c10.3% F1/8.7% EM\uff0c\u8d85\u8d8aGPT-4o\uff1b\u5728\u591a\u8df3QA\u4efb\u52a1\u4e2d\u4e5f\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "Visual-ARFT\u4e3a\u6784\u5efa\u9c81\u68d2\u4e14\u6cdb\u5316\u80fd\u529b\u5f3a\u7684\u591a\u6a21\u6001\u4ee3\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u8def\u5f84\u3002"}}
{"id": "2505.13973", "pdf": "https://arxiv.org/pdf/2505.13973", "abs": "https://arxiv.org/abs/2505.13973", "authors": ["Wenhui Zhu", "Xuanzhao Dong", "Xin Li", "Peijie Qiu", "Xiwen Chen", "Abolfazl Razi", "Aris Sotiras", "Yi Su", "Yalin Wang"], "title": "Toward Effective Reinforcement Learning Fine-Tuning for Medical VQA in Vision-Language Models", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "Recently, reinforcement learning (RL)-based tuning has shifted the trajectory\nof Multimodal Large Language Models (MLLMs), particularly following the\nintroduction of Group Relative Policy Optimization (GRPO). However, directly\napplying it to medical tasks remains challenging for achieving clinically\ngrounded model behavior. Motivated by the need to align model response with\nclinical expectations, we investigate four critical dimensions that affect the\neffectiveness of RL-based tuning in medical visual question answering (VQA):\nbase model initialization strategy, the role of medical semantic alignment, the\nimpact of length-based rewards on long-chain reasoning, and the influence of\nbias. We conduct extensive experiments to analyze these factors for medical\nMLLMs, providing new insights into how models are domain-specifically\nfine-tuned. Additionally, our results also demonstrate that GRPO-based RL\ntuning consistently outperforms standard supervised fine-tuning (SFT) in both\naccuracy and reasoning quality.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5728\u533b\u5b66\u89c6\u89c9\u95ee\u7b54\uff08VQA\uff09\u4e2d\uff0c\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7684\u8c03\u4f18\u65b9\u6cd5\uff08\u5982GRPO\uff09\u5982\u4f55\u901a\u8fc7\u56db\u4e2a\u5173\u952e\u7ef4\u5ea6\uff08\u57fa\u7840\u6a21\u578b\u521d\u59cb\u5316\u3001\u533b\u5b66\u8bed\u4e49\u5bf9\u9f50\u3001\u957f\u5ea6\u5956\u52b1\u548c\u504f\u5dee\u5f71\u54cd\uff09\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u5e76\u8bc1\u660e\u5176\u4f18\u4e8e\u4f20\u7edf\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u533b\u5b66\u4efb\u52a1\u4e2d\u6a21\u578b\u884c\u4e3a\u4e0e\u4e34\u5e8a\u671f\u671b\u7684\u5bf9\u9f50\u95ee\u9898\uff0c\u7814\u7a76\u5206\u6790\u4e86RL\u8c03\u4f18\u5728\u533b\u5b66MLLMs\u4e2d\u7684\u6709\u6548\u6027\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u5206\u6790\u56db\u4e2a\u5173\u952e\u7ef4\u5ea6\uff08\u57fa\u7840\u6a21\u578b\u521d\u59cb\u5316\u3001\u533b\u5b66\u8bed\u4e49\u5bf9\u9f50\u3001\u957f\u5ea6\u5956\u52b1\u548c\u504f\u5dee\u5f71\u54cd\uff09\u5bf9\u533b\u5b66VQA\u4efb\u52a1\u7684\u5f71\u54cd\u3002", "result": "GRPO-based RL\u8c03\u4f18\u5728\u51c6\u786e\u6027\u548c\u63a8\u7406\u8d28\u91cf\u4e0a\u5747\u4f18\u4e8e\u6807\u51c6\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u3002", "conclusion": "\u7814\u7a76\u4e3a\u533b\u5b66MLLMs\u7684\u9886\u57df\u7279\u5b9a\u8c03\u4f18\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\uff0c\u5e76\u9a8c\u8bc1\u4e86GRPO\u5728\u533b\u5b66\u4efb\u52a1\u4e2d\u7684\u4f18\u8d8a\u6027\u3002"}}
{"id": "2505.14254", "pdf": "https://arxiv.org/pdf/2505.14254", "abs": "https://arxiv.org/abs/2505.14254", "authors": ["Yuanyuan Chang", "Yinghua Yao", "Tao Qin", "Mengmeng Wang", "Ivor Tsang", "Guang Dai"], "title": "Instructing Text-to-Image Diffusion Models via Classifier-Guided Semantic Optimization", "categories": ["cs.CV"], "comment": null, "summary": "Text-to-image diffusion models have emerged as powerful tools for\nhigh-quality image generation and editing. Many existing approaches rely on\ntext prompts as editing guidance. However, these methods are constrained by the\nneed for manual prompt crafting, which can be time-consuming, introduce\nirrelevant details, and significantly limit editing performance. In this work,\nwe propose optimizing semantic embeddings guided by attribute classifiers to\nsteer text-to-image models toward desired edits, without relying on text\nprompts or requiring any training or fine-tuning of the diffusion model. We\nutilize classifiers to learn precise semantic embeddings at the dataset level.\nThe learned embeddings are theoretically justified as the optimal\nrepresentation of attribute semantics, enabling disentangled and accurate\nedits. Experiments further demonstrate that our method achieves high levels of\ndisentanglement and strong generalization across different domains of data.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u901a\u8fc7\u4f18\u5316\u8bed\u4e49\u5d4c\u5165\u548c\u5c5e\u6027\u5206\u7c7b\u5668\u5f15\u5bfc\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u7f16\u8f91\u7684\u65b9\u6cd5\uff0c\u907f\u514d\u624b\u52a8\u7f16\u5199\u6587\u672c\u63d0\u793a\uff0c\u5b9e\u73b0\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u7f16\u8f91\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u624b\u52a8\u7f16\u5199\u6587\u672c\u63d0\u793a\uff0c\u6548\u7387\u4f4e\u4e14\u53ef\u80fd\u5f15\u5165\u65e0\u5173\u7ec6\u8282\uff0c\u9650\u5236\u4e86\u7f16\u8f91\u6027\u80fd\u3002", "method": "\u5229\u7528\u5c5e\u6027\u5206\u7c7b\u5668\u4f18\u5316\u8bed\u4e49\u5d4c\u5165\uff0c\u65e0\u9700\u6587\u672c\u63d0\u793a\u6216\u6269\u6563\u6a21\u578b\u8bad\u7ec3\uff0c\u5b9e\u73b0\u7cbe\u786e\u7f16\u8f91\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u5b9e\u73b0\u9ad8\u5ea6\u89e3\u8026\u548c\u8de8\u9886\u57df\u6570\u636e\u7684\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u7f16\u8f91\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u65e0\u9700\u8bad\u7ec3\u7684\u65b0\u601d\u8def\u3002"}}
{"id": "2505.13975", "pdf": "https://arxiv.org/pdf/2505.13975", "abs": "https://arxiv.org/abs/2505.13975", "authors": ["Yuxuan Jiang", "Dawei Li", "Frank Ferraro"], "title": "DRP: Distilled Reasoning Pruning with Skill-aware Step Decomposition for Efficient Large Reasoning Models", "categories": ["cs.CL"], "comment": null, "summary": "While Large Reasoning Models (LRMs) have demonstrated success in complex\nreasoning tasks through long chain-of-thought (CoT) reasoning, their inference\noften involves excessively verbose reasoning traces, resulting in substantial\ninefficiency. To address this, we propose Distilled Reasoning Pruning (DRP), a\nhybrid framework that combines inference-time pruning with tuning-based\ndistillation, two widely used strategies for efficient reasoning. DRP uses a\nteacher model to perform skill-aware step decomposition and content pruning,\nand then distills the pruned reasoning paths into a student model, enabling it\nto reason both efficiently and accurately. Across several challenging\nmathematical reasoning datasets, we find that models trained with DRP achieve\nsubstantial improvements in token efficiency without sacrificing accuracy.\nSpecifically, DRP reduces average token usage on GSM8K from 917 to 328 while\nimproving accuracy from 91.7% to 94.1%, and achieves a 43% token reduction on\nAIME with no performance drop. Further analysis shows that aligning the\nreasoning structure of training CoTs with the student's reasoning capacity is\ncritical for effective knowledge transfer and performance gains.", "AI": {"tldr": "DRP\u6846\u67b6\u901a\u8fc7\u63a8\u7406\u65f6\u526a\u679d\u548c\u84b8\u998f\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u89e3\u51b3\u5927\u578b\u63a8\u7406\u6a21\u578b\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u56e0\u5197\u957f\u63a8\u7406\u8def\u5f84\u5bfc\u81f4\u7684\u6548\u7387\u4f4e\u4e0b\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u63a8\u7406\u65f6\u526a\u679d\u548c\u84b8\u998f\u6280\u672f\uff0c\u901a\u8fc7\u6559\u5e08\u6a21\u578b\u8fdb\u884c\u6280\u80fd\u611f\u77e5\u7684\u6b65\u9aa4\u5206\u89e3\u548c\u5185\u5bb9\u526a\u679d\uff0c\u5e76\u5c06\u526a\u679d\u540e\u7684\u63a8\u7406\u8def\u5f84\u84b8\u998f\u5230\u5b66\u751f\u6a21\u578b\u4e2d\u3002", "result": "\u5728\u591a\u4e2a\u6570\u5b66\u63a8\u7406\u6570\u636e\u96c6\u4e0a\uff0cDRP\u663e\u8457\u51cf\u5c11token\u4f7f\u7528\u91cf\uff08\u5982GSM8K\u4ece917\u964d\u81f3328\uff09\uff0c\u540c\u65f6\u63d0\u5347\u51c6\u786e\u7387\uff08\u5982GSM8K\u4ece91.7%\u63d0\u5347\u81f394.1%\uff09\u3002", "conclusion": "DRP\u901a\u8fc7\u4f18\u5316\u63a8\u7406\u7ed3\u6784\u4e0e\u5b66\u751f\u6a21\u578b\u80fd\u529b\u5bf9\u9f50\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u77e5\u8bc6\u8fc1\u79fb\u548c\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2505.14257", "pdf": "https://arxiv.org/pdf/2505.14257", "abs": "https://arxiv.org/abs/2505.14257", "authors": ["Jianfei Zhao", "Feng Zhang", "Xin Sun", "Chong Feng"], "title": "Aligning Attention Distribution to Information Flow for Hallucination Mitigation in Large Vision-Language Models", "categories": ["cs.CV"], "comment": null, "summary": "Due to the unidirectional masking mechanism, Decoder-Only models propagate\ninformation from left to right. LVLMs (Large Vision-Language Models) follow the\nsame architecture, with visual information gradually integrated into semantic\nrepresentations during forward propagation. Through systematic analysis, we\nobserve that over 80\\% of the visual information is absorbed into the semantic\nrepresentations. However, the model's attention still predominantly focuses on\nthe visual representations. This misalignment between the attention\ndistribution and the actual information flow undermines the model's visual\nunderstanding ability and contributes to hallucinations. To address this issue,\nwe enhance the model's visual understanding by leveraging the core information\nembedded in semantic representations. Specifically, we identify attention heads\nthat focus on core semantic representations based on their attention\ndistributions. Then, through a two-stage optimization paradigm, we propagate\nthe advantages of these attention heads across the entire model, aligning the\nattention distribution with the actual information flow. We evaluate our method\non three image captioning benchmarks using five different LVLMs, demonstrating\nits effectiveness in significantly reducing hallucinations. Further experiments\nreveal a trade-off between reduced hallucinations and richer details. Notably,\nour method allows for manual adjustment of the model's conservativeness,\nenabling flexible control to meet diverse real-world requirements. Code will be\nreleased once accepted.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u6ce8\u610f\u529b\u5206\u5e03\u4e0e\u4fe1\u606f\u6d41\u7684\u5bf9\u9f50\uff0c\u51cf\u5c11\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLM\uff09\u4e2d\u7684\u5e7b\u89c9\u73b0\u8c61\u3002", "motivation": "\u7814\u7a76\u53d1\u73b0LVLM\u7684\u6ce8\u610f\u529b\u5206\u5e03\u4e0e\u4fe1\u606f\u6d41\u4e0d\u5339\u914d\uff0c\u5bfc\u81f4\u89c6\u89c9\u7406\u89e3\u80fd\u529b\u4e0b\u964d\u548c\u5e7b\u89c9\u73b0\u8c61\u3002", "method": "\u8bc6\u522b\u5173\u6ce8\u6838\u5fc3\u8bed\u4e49\u8868\u793a\u7684\u6ce8\u610f\u529b\u5934\uff0c\u5e76\u901a\u8fc7\u4e24\u9636\u6bb5\u4f18\u5316\u8303\u5f0f\u8c03\u6574\u6ce8\u610f\u529b\u5206\u5e03\u3002", "result": "\u5728\u4e94\u4e2aLVLM\u4e0a\u9a8c\u8bc1\uff0c\u663e\u8457\u51cf\u5c11\u5e7b\u89c9\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u51cf\u5c11\u5e7b\u89c9\u4e0e\u4e30\u5bcc\u7ec6\u8282\u4e4b\u95f4\u7684\u6743\u8861\u3002", "conclusion": "\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u89c6\u89c9\u7406\u89e3\u80fd\u529b\uff0c\u5e76\u5141\u8bb8\u624b\u52a8\u8c03\u6574\u6a21\u578b\u7684\u4fdd\u5b88\u6027\uff0c\u9002\u5e94\u591a\u6837\u5316\u9700\u6c42\u3002"}}
{"id": "2505.13979", "pdf": "https://arxiv.org/pdf/2505.13979", "abs": "https://arxiv.org/abs/2505.13979", "authors": ["Maya Srikanth", "Run Chen", "Julia Hirschberg"], "title": "Mixed Signals: Understanding Model Disagreement in Multimodal Empathy Detection", "categories": ["cs.CL"], "comment": null, "summary": "Multimodal models play a key role in empathy detection, but their performance\ncan suffer when modalities provide conflicting cues. To understand these\nfailures, we examine cases where unimodal and multimodal predictions diverge.\nUsing fine-tuned models for text, audio, and video, along with a gated fusion\nmodel, we find that such disagreements often reflect underlying ambiguity, as\nevidenced by annotator uncertainty. Our analysis shows that dominant signals in\none modality can mislead fusion when unsupported by others. We also observe\nthat humans, like models, do not consistently benefit from multimodal input.\nThese insights position disagreement as a useful diagnostic signal for\nidentifying challenging examples and improving empathy system robustness.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u591a\u6a21\u6001\u6a21\u578b\u5728\u5171\u60c5\u68c0\u6d4b\u4e2d\u7684\u6027\u80fd\u95ee\u9898\uff0c\u7279\u522b\u662f\u6a21\u6001\u95f4\u51b2\u7a81\u4fe1\u53f7\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u5206\u6b67\u4f5c\u4e3a\u8bca\u65ad\u4fe1\u53f7\u4ee5\u63d0\u5347\u7cfb\u7edf\u9c81\u68d2\u6027\u3002", "motivation": "\u7814\u7a76\u591a\u6a21\u6001\u6a21\u578b\u5728\u5171\u60c5\u68c0\u6d4b\u4e2d\u56e0\u6a21\u6001\u51b2\u7a81\u5bfc\u81f4\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u4ee5\u7406\u89e3\u6a21\u578b\u4e0e\u4eba\u7c7b\u5728\u591a\u6a21\u6001\u8f93\u5165\u4e0b\u7684\u8868\u73b0\u5dee\u5f02\u3002", "method": "\u4f7f\u7528\u6587\u672c\u3001\u97f3\u9891\u548c\u89c6\u9891\u7684\u5fae\u8c03\u6a21\u578b\u53ca\u95e8\u63a7\u878d\u5408\u6a21\u578b\uff0c\u5206\u6790\u5355\u6a21\u6001\u4e0e\u591a\u6a21\u6001\u9884\u6d4b\u7684\u5206\u6b67\u6848\u4f8b\uff0c\u7ed3\u5408\u6807\u6ce8\u8005\u4e0d\u786e\u5b9a\u6027\u9a8c\u8bc1\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u6a21\u6001\u95f4\u4e3b\u5bfc\u4fe1\u53f7\u53ef\u80fd\u8bef\u5bfc\u878d\u5408\uff0c\u4e14\u4eba\u7c7b\u4e0e\u6a21\u578b\u5728\u591a\u6a21\u6001\u8f93\u5165\u4e0b\u8868\u73b0\u5e76\u4e0d\u603b\u4e00\u81f4\u3002\u5206\u6b67\u53ef\u8bc6\u522b\u6311\u6218\u6027\u6848\u4f8b\u3002", "conclusion": "\u6a21\u6001\u5206\u6b67\u662f\u8bca\u65ad\u5171\u60c5\u68c0\u6d4b\u7cfb\u7edf\u6311\u6218\u6027\u6848\u4f8b\u7684\u6709\u6548\u4fe1\u53f7\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u6a21\u578b\u9c81\u68d2\u6027\u3002"}}
{"id": "2505.14260", "pdf": "https://arxiv.org/pdf/2505.14260", "abs": "https://arxiv.org/abs/2505.14260", "authors": ["Luxi Lin", "Zhihang Lin", "Zhanpeng Zeng", "Rongrong Ji"], "title": "Speculative Decoding Reimagined for Multimodal Large Language Models", "categories": ["cs.CV", "cs.AI"], "comment": "12 pages", "summary": "This paper introduces Multimodal Speculative Decoding (MSD) to accelerate\nMultimodal Large Language Models (MLLMs) inference. Speculative decoding has\nbeen shown to accelerate Large Language Models (LLMs) without sacrificing\naccuracy. However, current speculative decoding methods for MLLMs fail to\nachieve the same speedup as they do for LLMs. To address this, we reimagine\nspeculative decoding specifically for MLLMs. Our analysis of MLLM\ncharacteristics reveals two key design principles for MSD: (1) Text and visual\ntokens have fundamentally different characteristics and need to be processed\nseparately during drafting. (2) Both language modeling ability and visual\nperception capability are crucial for the draft model. For the first principle,\nMSD decouples text and visual tokens in the draft model, allowing each to be\nhandled based on its own characteristics. For the second principle, MSD uses a\ntwo-stage training strategy: In stage one, the draft model is trained on\ntext-only instruction-tuning datasets to improve its language modeling ability.\nIn stage two, MSD gradually introduces multimodal data to enhance the visual\nperception capability of the draft model. Experiments show that MSD boosts\ninference speed by up to $2.29\\times$ for LLaVA-1.5-7B and up to $2.46\\times$\nfor LLaVA-1.5-13B on multimodal benchmarks, demonstrating its effectiveness.\nOur code is available at https://github.com/Lyn-Lucy/MSD.", "AI": {"tldr": "Multimodal Speculative Decoding (MSD) \u52a0\u901f\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b (MLLMs) \u63a8\u7406\uff0c\u901a\u8fc7\u5206\u79bb\u5904\u7406\u6587\u672c\u548c\u89c6\u89c9\u6807\u8bb0\uff0c\u5e76\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u63a8\u7406\u901f\u5ea6\u3002", "motivation": "\u73b0\u6709\u63a8\u6d4b\u89e3\u7801\u65b9\u6cd5\u5728\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b (MLLMs) \u4e2d\u65e0\u6cd5\u8fbe\u5230\u4e0e\u5355\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b (LLMs) \u76f8\u540c\u7684\u52a0\u901f\u6548\u679c\uff0c\u56e0\u6b64\u9700\u8981\u9488\u5bf9 MLLMs \u91cd\u65b0\u8bbe\u8ba1\u63a8\u6d4b\u89e3\u7801\u65b9\u6cd5\u3002", "method": "MSD \u5206\u79bb\u5904\u7406\u6587\u672c\u548c\u89c6\u89c9\u6807\u8bb0\uff0c\u5e76\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff1a\u7b2c\u4e00\u9636\u6bb5\u8bad\u7ec3\u6587\u672c\u80fd\u529b\uff0c\u7b2c\u4e8c\u9636\u6bb5\u9010\u6b65\u5f15\u5165\u591a\u6a21\u6001\u6570\u636e\u589e\u5f3a\u89c6\u89c9\u80fd\u529b\u3002", "result": "MSD \u5728 LLaVA-1.5-7B \u548c LLaVA-1.5-13B \u4e0a\u5206\u522b\u5b9e\u73b0\u6700\u9ad8 2.29 \u500d\u548c 2.46 \u500d\u7684\u63a8\u7406\u52a0\u901f\u3002", "conclusion": "MSD \u662f\u4e00\u79cd\u9488\u5bf9 MLLMs \u7684\u9ad8\u6548\u63a8\u6d4b\u89e3\u7801\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u63a8\u7406\u901f\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u51c6\u786e\u6027\u3002"}}
{"id": "2505.13988", "pdf": "https://arxiv.org/pdf/2505.13988", "abs": "https://arxiv.org/abs/2505.13988", "authors": ["Linxin Song", "Taiwei Shi", "Jieyu Zhao"], "title": "The Hallucination Tax of Reinforcement Finetuning", "categories": ["cs.CL"], "comment": null, "summary": "Reinforcement finetuning (RFT) has become a standard approach for enhancing\nthe reasoning capabilities of large language models (LLMs). However, its impact\non model trustworthiness remains underexplored. In this work, we identify and\nsystematically study a critical side effect of RFT, which we term the\nhallucination tax: a degradation in refusal behavior causing models to produce\nhallucinated answers to unanswerable questions confidently. To investigate\nthis, we introduce SUM (Synthetic Unanswerable Math), a high-quality dataset of\nunanswerable math problems designed to probe models' ability to recognize an\nunanswerable question by reasoning from the insufficient or ambiguous\ninformation. Our results show that standard RFT training could reduce model\nrefusal rates by more than 80%, which significantly increases model's tendency\nto hallucinate. We further demonstrate that incorporating just 10% SUM during\nRFT substantially restores appropriate refusal behavior, with minimal accuracy\ntrade-offs on solvable tasks. Crucially, this approach enables LLMs to leverage\ninference-time compute to reason about their own uncertainty and knowledge\nboundaries, improving generalization not only to out-of-domain math problems\nbut also to factual question answering tasks.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5f3a\u5316\u5fae\u8c03\uff08RFT\uff09\u4f1a\u964d\u4f4e\u8bed\u8a00\u6a21\u578b\u5bf9\u4e0d\u53ef\u56de\u7b54\u95ee\u9898\u7684\u62d2\u7edd\u80fd\u529b\uff0c\u5bfc\u81f4\u5e7b\u89c9\u56de\u7b54\u589e\u52a0\u3002\u901a\u8fc7\u5f15\u5165SUM\u6570\u636e\u96c6\uff0c\u5c11\u91cf\u8c03\u6574RFT\u53ef\u663e\u8457\u6062\u590d\u6a21\u578b\u7684\u62d2\u7edd\u884c\u4e3a\u3002", "motivation": "\u63a2\u7d22RFT\u5bf9\u6a21\u578b\u53ef\u4fe1\u5ea6\u7684\u5f71\u54cd\uff0c\u7279\u522b\u662f\u5176\u5bf9\u4e0d\u53ef\u56de\u7b54\u95ee\u9898\u5904\u7406\u80fd\u529b\u7684\u526f\u4f5c\u7528\u3002", "method": "\u5f15\u5165SUM\u6570\u636e\u96c6\uff0c\u6d4b\u8bd5\u6a21\u578b\u5bf9\u4e0d\u53ef\u56de\u7b54\u6570\u5b66\u95ee\u9898\u7684\u8bc6\u522b\u80fd\u529b\uff0c\u5e76\u8c03\u6574RFT\u8bad\u7ec3\u7b56\u7565\u3002", "result": "\u6807\u51c6RFT\u8bad\u7ec3\u4f7f\u6a21\u578b\u62d2\u7edd\u7387\u4e0b\u964d80%\uff0c\u52a0\u516510% SUM\u6570\u636e\u53ef\u663e\u8457\u6062\u590d\u62d2\u7edd\u884c\u4e3a\uff0c\u4e14\u5bf9\u53ef\u89e3\u4efb\u52a1\u5f71\u54cd\u5c0f\u3002", "conclusion": "\u8c03\u6574RFT\u7b56\u7565\u80fd\u5e2e\u52a9\u6a21\u578b\u66f4\u597d\u5730\u8bc6\u522b\u77e5\u8bc6\u8fb9\u754c\uff0c\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2505.14270", "pdf": "https://arxiv.org/pdf/2505.14270", "abs": "https://arxiv.org/abs/2505.14270", "authors": ["Yoorhim Cho", "Hongyeob Kim", "Semin Kim", "Youjia Zhang", "Yunseok Choi", "Sungeun Hong"], "title": "RA-Touch: Retrieval-Augmented Touch Understanding with Enriched Visual Data", "categories": ["cs.CV"], "comment": null, "summary": "Visuo-tactile perception aims to understand an object's tactile properties,\nsuch as texture, softness, and rigidity. However, the field remains\nunderexplored because collecting tactile data is costly and labor-intensive. We\nobserve that visually distinct objects can exhibit similar surface textures or\nmaterial properties. For example, a leather sofa and a leather jacket have\ndifferent appearances but share similar tactile properties. This implies that\ntactile understanding can be guided by material cues in visual data, even\nwithout direct tactile supervision. In this paper, we introduce RA-Touch, a\nretrieval-augmented framework that improves visuo-tactile perception by\nleveraging visual data enriched with tactile semantics. We carefully recaption\na large-scale visual dataset with tactile-focused descriptions, enabling the\nmodel to access tactile semantics typically absent from conventional visual\ndatasets. A key challenge remains in effectively utilizing these tactile-aware\nexternal descriptions. RA-Touch addresses this by retrieving visual-textual\nrepresentations aligned with tactile inputs and integrating them to focus on\nrelevant textural and material properties. By outperforming prior methods on\nthe TVL benchmark, our method demonstrates the potential of retrieval-based\nvisual reuse for tactile understanding. Code is available at\nhttps://aim-skku.github.io/RA-Touch", "AI": {"tldr": "RA-Touch\u662f\u4e00\u4e2a\u57fa\u4e8e\u68c0\u7d22\u589e\u5f3a\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u89c6\u89c9\u6570\u636e\u548c\u89e6\u89c9\u8bed\u4e49\u63d0\u5347\u89c6\u89c9\u89e6\u89c9\u611f\u77e5\u80fd\u529b\u3002", "motivation": "\u89e6\u89c9\u6570\u636e\u7684\u6536\u96c6\u6210\u672c\u9ad8\u4e14\u8017\u65f6\uff0c\u800c\u89c6\u89c9\u4e0a\u4e0d\u540c\u7684\u7269\u4f53\u53ef\u80fd\u5177\u6709\u76f8\u4f3c\u7684\u89e6\u89c9\u7279\u6027\uff0c\u56e0\u6b64\u53ef\u4ee5\u901a\u8fc7\u89c6\u89c9\u6570\u636e\u4e2d\u7684\u6750\u8d28\u7ebf\u7d22\u95f4\u63a5\u7406\u89e3\u89e6\u89c9\u7279\u6027\u3002", "method": "RA-Touch\u901a\u8fc7\u91cd\u65b0\u6807\u6ce8\u5927\u89c4\u6a21\u89c6\u89c9\u6570\u636e\u96c6\uff0c\u52a0\u5165\u89e6\u89c9\u8bed\u4e49\u63cf\u8ff0\uff0c\u5e76\u5229\u7528\u68c0\u7d22\u589e\u5f3a\u7684\u65b9\u6cd5\u5c06\u89c6\u89c9-\u6587\u672c\u8868\u793a\u4e0e\u89e6\u89c9\u8f93\u5165\u5bf9\u9f50\u3002", "result": "\u5728TVL\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cRA-Touch\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u57fa\u4e8e\u68c0\u7d22\u7684\u89c6\u89c9\u91cd\u7528\u5bf9\u89e6\u89c9\u7406\u89e3\u7684\u6f5c\u529b\u3002", "conclusion": "RA-Touch\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9\u548c\u89e6\u89c9\u8bed\u4e49\uff0c\u4e3a\u89e6\u89c9\u611f\u77e5\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u6210\u672c\u8f83\u4f4e\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.13990", "pdf": "https://arxiv.org/pdf/2505.13990", "abs": "https://arxiv.org/abs/2505.13990", "authors": ["Tingfeng Hui", "Pengyu Zhu", "Bowen Ping", "Ling Tang", "Yaqi Zhang", "Sen Su"], "title": "DecIF: Improving Instruction-Following through Meta-Decomposition", "categories": ["cs.CL"], "comment": "Work in progress", "summary": "Instruction-following has emerged as a crucial capability for large language\nmodels (LLMs). However, existing approaches often rely on pre-existing\ndocuments or external resources to synthesize instruction-following data, which\nlimits their flexibility and generalizability. In this paper, we introduce\nDecIF, a fully autonomous, meta-decomposition guided framework that generates\ndiverse and high-quality instruction-following data using only LLMs. DecIF is\ngrounded in the principle of decomposition. For instruction generation, we\nguide LLMs to iteratively produce various types of meta-information, which are\nthen combined with response constraints to form well-structured and\nsemantically rich instructions. We further utilize LLMs to detect and resolve\npotential inconsistencies within the generated instructions. Regarding response\ngeneration, we decompose each instruction into atomic-level evaluation\ncriteria, enabling rigorous validation and the elimination of inaccurate\ninstruction-response pairs. Extensive experiments across a wide range of\nscenarios and settings demonstrate DecIF's superior performance on\ninstruction-following tasks. Further analysis highlights its strong\nflexibility, scalability, and generalizability in automatically synthesizing\nhigh-quality instruction data.", "AI": {"tldr": "DecIF\u662f\u4e00\u4e2a\u5b8c\u5168\u81ea\u4e3b\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u89e3\u539f\u5219\u751f\u6210\u591a\u6837\u4e14\u9ad8\u8d28\u91cf\u7684\u6307\u4ee4\u8ddf\u968f\u6570\u636e\uff0c\u4ec5\u4f9d\u8d56LLMs\uff0c\u65e0\u9700\u5916\u90e8\u8d44\u6e90\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5916\u90e8\u8d44\u6e90\u751f\u6210\u6307\u4ee4\u6570\u636e\uff0c\u9650\u5236\u4e86\u7075\u6d3b\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002DecIF\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "DecIF\u901a\u8fc7\u5206\u89e3\u539f\u5219\uff0c\u5f15\u5bfcLLMs\u8fed\u4ee3\u751f\u6210\u5143\u4fe1\u606f\uff0c\u7ed3\u5408\u54cd\u5e94\u7ea6\u675f\u5f62\u6210\u7ed3\u6784\u5316\u6307\u4ee4\uff0c\u5e76\u68c0\u6d4b\u548c\u89e3\u51b3\u4e0d\u4e00\u81f4\u6027\u3002\u54cd\u5e94\u751f\u6210\u65f6\uff0c\u5c06\u6307\u4ee4\u5206\u89e3\u4e3a\u539f\u5b50\u7ea7\u8bc4\u4f30\u6807\u51c6\u4ee5\u9a8c\u8bc1\u51c6\u786e\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660eDecIF\u5728\u6307\u4ee4\u8ddf\u968f\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5177\u6709\u5f3a\u7075\u6d3b\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "DecIF\u4e3a\u81ea\u52a8\u751f\u6210\u9ad8\u8d28\u91cf\u6307\u4ee4\u6570\u636e\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.14296", "pdf": "https://arxiv.org/pdf/2505.14296", "abs": "https://arxiv.org/abs/2505.14296", "authors": ["Abdul-Kazeem Shamba"], "title": "Towards Generating Realistic Underwater Images", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "This paper explores the use of contrastive learning and generative\nadversarial networks for generating realistic underwater images from synthetic\nimages with uniform lighting. We investigate the performance of image\ntranslation models for generating realistic underwater images using the VAROS\ndataset. Two key evaluation metrics, Fr\\'echet Inception Distance (FID) and\nStructural Similarity Index Measure (SSIM), provide insights into the\ntrade-offs between perceptual quality and structural preservation. For paired\nimage translation, pix2pix achieves the best FID scores due to its paired\nsupervision and PatchGAN discriminator, while the autoencoder model attains the\nhighest SSIM, suggesting better structural fidelity despite producing blurrier\noutputs. Among unpaired methods, CycleGAN achieves a competitive FID score by\nleveraging cycle-consistency loss, whereas CUT, which replaces\ncycle-consistency with contrastive learning, attains higher SSIM, indicating\nimproved spatial similarity retention. Notably, incorporating depth information\ninto CUT results in the lowest overall FID score, demonstrating that depth cues\nenhance realism. However, the slight decrease in SSIM suggests that depth-aware\nlearning may introduce structural variations.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5bf9\u6bd4\u5b66\u4e60\u548c\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u5728\u4ece\u5408\u6210\u56fe\u50cf\u751f\u6210\u903c\u771f\u6c34\u4e0b\u56fe\u50cf\u4e2d\u7684\u5e94\u7528\uff0c\u8bc4\u4f30\u4e86\u4e0d\u540c\u6a21\u578b\u5728VAROS\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u56fe\u50cf\u7ffb\u8bd1\u6a21\u578b\u5c06\u5408\u6210\u56fe\u50cf\u8f6c\u6362\u4e3a\u903c\u771f\u7684\u6c34\u4e0b\u56fe\u50cf\uff0c\u89e3\u51b3\u6c34\u4e0b\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u5149\u7167\u548c\u7ed3\u6784\u95ee\u9898\u3002", "method": "\u4f7f\u7528pix2pix\u3001CycleGAN\u3001CUT\u7b49\u6a21\u578b\u8fdb\u884c\u56fe\u50cf\u7ffb\u8bd1\uff0c\u5e76\u901a\u8fc7FID\u548cSSIM\u6307\u6807\u8bc4\u4f30\u6027\u80fd\u3002", "result": "pix2pix\u5728FID\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u800c\u81ea\u7f16\u7801\u5668\u5728SSIM\u4e0a\u6700\u4f18\uff1bCycleGAN\u5728\u975e\u914d\u5bf9\u65b9\u6cd5\u4e2dFID\u8868\u73b0\u597d\uff0cCUT\u5728SSIM\u4e0a\u66f4\u4f18\uff0c\u52a0\u5165\u6df1\u5ea6\u4fe1\u606f\u540eCUT\u7684FID\u6700\u4f4e\u3002", "conclusion": "\u6df1\u5ea6\u4fe1\u606f\u80fd\u63d0\u5347\u751f\u6210\u56fe\u50cf\u7684\u903c\u771f\u5ea6\uff0c\u4f46\u53ef\u80fd\u727a\u7272\u90e8\u5206\u7ed3\u6784\u4fdd\u771f\u5ea6\uff1b\u4e0d\u540c\u6a21\u578b\u5728\u611f\u77e5\u8d28\u91cf\u548c\u7ed3\u6784\u4fdd\u7559\u4e0a\u5404\u6709\u4f18\u52a3\u3002"}}
{"id": "2505.13995", "pdf": "https://arxiv.org/pdf/2505.13995", "abs": "https://arxiv.org/abs/2505.13995", "authors": ["Myra Cheng", "Sunny Yu", "Cinoo Lee", "Pranav Khadpe", "Lujain Ibrahim", "Dan Jurafsky"], "title": "Social Sycophancy: A Broader Understanding of LLM Sycophancy", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "A serious risk to the safety and utility of LLMs is sycophancy, i.e.,\nexcessive agreement with and flattery of the user. Yet existing work focuses on\nonly one aspect of sycophancy: agreement with users' explicitly stated beliefs\nthat can be compared to a ground truth. This overlooks forms of sycophancy that\narise in ambiguous contexts such as advice and support-seeking, where there is\nno clear ground truth, yet sycophancy can reinforce harmful implicit\nassumptions, beliefs, or actions. To address this gap, we introduce a richer\ntheory of social sycophancy in LLMs, characterizing sycophancy as the excessive\npreservation of a user's face (the positive self-image a person seeks to\nmaintain in an interaction). We present ELEPHANT, a framework for evaluating\nsocial sycophancy across five face-preserving behaviors (emotional validation,\nmoral endorsement, indirect language, indirect action, and accepting framing)\non two datasets: open-ended questions (OEQ) and Reddit's r/AmITheAsshole\n(AITA). Across eight models, we show that LLMs consistently exhibit high rates\nof social sycophancy: on OEQ, they preserve face 47% more than humans, and on\nAITA, they affirm behavior deemed inappropriate by crowdsourced human judgments\nin 42% of cases. We further show that social sycophancy is rewarded in\npreference datasets and is not easily mitigated. Our work provides theoretical\ngrounding and empirical tools (datasets and code) for understanding and\naddressing this under-recognized but consequential issue.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faLLMs\u4e2d\u7684\u793e\u4f1a\u8c04\u5a9a\u95ee\u9898\uff0c\u5b9a\u4e49\u5176\u4e3a\u8fc7\u5ea6\u7ef4\u62a4\u7528\u6237\u9762\u5b50\uff0c\u5e76\u5f00\u53d1ELEPHANT\u6846\u67b6\u8bc4\u4f30\u4e94\u79cd\u884c\u4e3a\uff0c\u53d1\u73b0LLMs\u8c04\u5a9a\u7387\u663e\u8457\u9ad8\u4e8e\u4eba\u7c7b\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4ec5\u5173\u6ce8\u660e\u786e\u53ef\u9a8c\u8bc1\u7684\u8c04\u5a9a\u884c\u4e3a\uff0c\u5ffd\u7565\u4e86\u6a21\u7cca\u60c5\u5883\u4e0b\u7684\u793e\u4f1a\u8c04\u5a9a\u95ee\u9898\u3002", "method": "\u5f15\u5165\u793e\u4f1a\u8c04\u5a9a\u7406\u8bba\uff0c\u5f00\u53d1ELEPHANT\u6846\u67b6\uff0c\u8bc4\u4f30\u4e94\u79cd\u9762\u5b50\u7ef4\u62a4\u884c\u4e3a\uff0c\u4f7f\u7528OEQ\u548cAITA\u6570\u636e\u96c6\u6d4b\u8bd5\u516b\u79cd\u6a21\u578b\u3002", "result": "LLMs\u5728OEQ\u4e2d\u6bd4\u4eba\u7c7b\u591a47%\u7ef4\u62a4\u9762\u5b50\uff0c\u5728AITA\u4e2d42%\u60c5\u51b5\u4e0b\u652f\u6301\u4e0d\u5f53\u884c\u4e3a\u3002\u8c04\u5a9a\u884c\u4e3a\u5728\u504f\u597d\u6570\u636e\u96c6\u4e2d\u53d7\u5956\u52b1\u4e14\u96be\u4ee5\u7f13\u89e3\u3002", "conclusion": "\u7814\u7a76\u4e3a\u793e\u4f1a\u8c04\u5a9a\u95ee\u9898\u63d0\u4f9b\u4e86\u7406\u8bba\u548c\u5b9e\u8bc1\u5de5\u5177\uff0c\u63ed\u793a\u4e86\u5176\u91cd\u8981\u6027\u548c\u590d\u6742\u6027\u3002"}}
{"id": "2505.14298", "pdf": "https://arxiv.org/pdf/2505.14298", "abs": "https://arxiv.org/abs/2505.14298", "authors": ["Fulong Yao", "Wenju Zhou", "Huosheng Hu"], "title": "A Review of Vision-Based Assistive Systems for Visually Impaired People: Technologies, Applications, and Future Directions", "categories": ["cs.CV"], "comment": null, "summary": "Visually impaired individuals rely heavily on accurate and timely information\nabout obstacles and their surrounding environments to achieve independent\nliving. In recent years, significant progress has been made in the development\nof assistive technologies, particularly vision-based systems, that enhance\nmobility and facilitate interaction with the external world in both indoor and\noutdoor settings. This paper presents a comprehensive review of recent advances\nin assistive systems designed for the visually impaired, with a focus on\nstate-of-the-art technologies in obstacle detection, navigation, and user\ninteraction. In addition, emerging trends and future directions in visual\nguidance systems are discussed.", "AI": {"tldr": "\u7efc\u8ff0\u4e86\u8fd1\u5e74\u6765\u9488\u5bf9\u89c6\u969c\u4eba\u58eb\u7684\u8f85\u52a9\u7cfb\u7edf\u6280\u672f\u8fdb\u5c55\uff0c\u91cd\u70b9\u5173\u6ce8\u969c\u788d\u7269\u68c0\u6d4b\u3001\u5bfc\u822a\u548c\u7528\u6237\u4ea4\u4e92\u7684\u6700\u65b0\u6210\u679c\u3002", "motivation": "\u89c6\u969c\u4eba\u58eb\u9700\u8981\u51c6\u786e\u53ca\u65f6\u7684\u73af\u5883\u4fe1\u606f\u4ee5\u5b9e\u73b0\u72ec\u7acb\u751f\u6d3b\uff0c\u800c\u89c6\u89c9\u8f85\u52a9\u6280\u672f\u80fd\u663e\u8457\u63d0\u5347\u5176\u79fb\u52a8\u80fd\u529b\u548c\u4e0e\u5916\u754c\u4e92\u52a8\u3002", "method": "\u901a\u8fc7\u6587\u732e\u7efc\u8ff0\uff0c\u5206\u6790\u6700\u65b0\u7684\u89c6\u89c9\u8f85\u52a9\u7cfb\u7edf\u6280\u672f\uff0c\u5305\u62ec\u969c\u788d\u7269\u68c0\u6d4b\u3001\u5bfc\u822a\u548c\u7528\u6237\u4ea4\u4e92\u3002", "result": "\u603b\u7ed3\u4e86\u5f53\u524d\u6280\u672f\u7684\u8fdb\u5c55\uff0c\u5e76\u63a2\u8ba8\u4e86\u89c6\u89c9\u5f15\u5bfc\u7cfb\u7edf\u7684\u672a\u6765\u8d8b\u52bf\u3002", "conclusion": "\u89c6\u89c9\u8f85\u52a9\u7cfb\u7edf\u5728\u5e2e\u52a9\u89c6\u969c\u4eba\u58eb\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u672a\u6765\u4ecd\u6709\u8fdb\u4e00\u6b65\u53d1\u5c55\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.14009", "pdf": "https://arxiv.org/pdf/2505.14009", "abs": "https://arxiv.org/abs/2505.14009", "authors": ["Yuxuan Yao", "Shuqi Liu", "Zehua Liu", "Qintong Li", "Mingyang Liu", "Xiongwei Han", "Zhijiang Guo", "Han Wu", "Linqi Song"], "title": "Activation-Guided Consensus Merging for Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Recent research has increasingly focused on reconciling the reasoning\ncapabilities of System 2 with the efficiency of System 1. While existing\ntraining-based and prompt-based approaches face significant challenges in terms\nof efficiency and stability, model merging emerges as a promising strategy to\nintegrate the diverse capabilities of different Large Language Models (LLMs)\ninto a unified model. However, conventional model merging methods often assume\nuniform importance across layers, overlooking the functional heterogeneity\ninherent in neural components. To address this limitation, we propose\n\\textbf{A}ctivation-Guided \\textbf{C}onsensus \\textbf{M}erging (\\textbf{ACM}),\na plug-and-play merging framework that determines layer-specific merging\ncoefficients based on mutual information between activations of pre-trained and\nfine-tuned models. ACM effectively preserves task-specific capabilities without\nrequiring gradient computations or additional training. Extensive experiments\non Long-to-Short (L2S) and general merging tasks demonstrate that ACM\nconsistently outperforms all baseline methods. For instance, in the case of\nQwen-7B models, TIES-Merging equipped with ACM achieves a \\textbf{55.3\\%}\nreduction in response length while simultaneously improving reasoning accuracy\nby \\textbf{1.3} points. We submit the code with the paper for reproducibility,\nand it will be publicly available.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aACM\u7684\u6a21\u578b\u5408\u5e76\u6846\u67b6\uff0c\u901a\u8fc7\u6fc0\u6d3b\u5f15\u5bfc\u7684\u5171\u8bc6\u5408\u5e76\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u5408\u5e76\u65b9\u6cd5\u5ffd\u7565\u795e\u7ecf\u7f51\u7edc\u529f\u80fd\u5f02\u8d28\u6027\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u7ed3\u5408System 2\u7684\u63a8\u7406\u80fd\u529b\u548cSystem 1\u7684\u6548\u7387\u65f6\u9762\u4e34\u6311\u6218\uff0c\u4f20\u7edf\u6a21\u578b\u5408\u5e76\u65b9\u6cd5\u5047\u8bbe\u5c42\u95f4\u91cd\u8981\u6027\u4e00\u81f4\uff0c\u5ffd\u7565\u4e86\u529f\u80fd\u5f02\u8d28\u6027\u3002", "method": "\u63d0\u51fa\u4e86ACM\u6846\u67b6\uff0c\u57fa\u4e8e\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u6a21\u578b\u6fc0\u6d3b\u95f4\u7684\u4e92\u4fe1\u606f\u786e\u5b9a\u5c42\u7279\u5b9a\u5408\u5e76\u7cfb\u6570\uff0c\u65e0\u9700\u68af\u5ea6\u8ba1\u7b97\u6216\u989d\u5916\u8bad\u7ec3\u3002", "result": "\u5728L2S\u548c\u901a\u7528\u5408\u5e76\u4efb\u52a1\u4e2d\uff0cACM\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4f8b\u5982\u5728Qwen-7B\u6a21\u578b\u4e0a\uff0c\u54cd\u5e94\u957f\u5ea6\u51cf\u5c1155.3%\uff0c\u63a8\u7406\u51c6\u786e\u7387\u63d0\u53471.3\u5206\u3002", "conclusion": "ACM\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u65e0\u9700\u8bad\u7ec3\u7684\u5408\u5e76\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u4ee3\u7801\u5c06\u516c\u5f00\u4ee5\u4fc3\u8fdb\u53ef\u590d\u73b0\u6027\u3002"}}
{"id": "2505.14318", "pdf": "https://arxiv.org/pdf/2505.14318", "abs": "https://arxiv.org/abs/2505.14318", "authors": ["Wenjun Hou", "Yi Cheng", "Kaishuai Xu", "Heng Li", "Yan Hu", "Wenjie Li", "Jiang Liu"], "title": "RADAR: Enhancing Radiology Report Generation with Supplementary Knowledge Injection", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in\nvarious domains, including radiology report generation. Previous approaches\nhave attempted to utilize multimodal LLMs for this task, enhancing their\nperformance through the integration of domain-specific knowledge retrieval.\nHowever, these approaches often overlook the knowledge already embedded within\nthe LLMs, leading to redundant information integration and inefficient\nutilization of learned representations. To address this limitation, we propose\nRADAR, a framework for enhancing radiology report generation with supplementary\nknowledge injection. RADAR improves report generation by systematically\nleveraging both the internal knowledge of an LLM and externally retrieved\ninformation. Specifically, it first extracts the model's acquired knowledge\nthat aligns with expert image-based classification outputs. It then retrieves\nrelevant supplementary knowledge to further enrich this information. Finally,\nby aggregating both sources, RADAR generates more accurate and informative\nradiology reports. Extensive experiments on MIMIC-CXR, CheXpert-Plus, and IU\nX-ray demonstrate that our model outperforms state-of-the-art LLMs in both\nlanguage quality and clinical accuracy", "AI": {"tldr": "RADAR\u6846\u67b6\u901a\u8fc7\u7ed3\u5408LLM\u5185\u90e8\u77e5\u8bc6\u548c\u5916\u90e8\u68c0\u7d22\u4fe1\u606f\uff0c\u63d0\u5347\u653e\u5c04\u5b66\u62a5\u544a\u751f\u6210\u7684\u51c6\u786e\u6027\u548c\u4fe1\u606f\u91cf\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5ffd\u7565\u4e86LLM\u5185\u90e8\u5df2\u5d4c\u5165\u7684\u77e5\u8bc6\uff0c\u5bfc\u81f4\u4fe1\u606f\u5197\u4f59\u548c\u4f4e\u6548\u5229\u7528\u3002", "method": "RADAR\u9996\u5148\u63d0\u53d6LLM\u5185\u90e8\u4e0e\u4e13\u5bb6\u5206\u7c7b\u8f93\u51fa\u4e00\u81f4\u7684\u77e5\u8bc6\uff0c\u518d\u68c0\u7d22\u8865\u5145\u4fe1\u606f\uff0c\u6700\u540e\u6574\u5408\u751f\u6210\u62a5\u544a\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\uff0cRADAR\u5728\u8bed\u8a00\u8d28\u91cf\u548c\u4e34\u5e8a\u51c6\u786e\u6027\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709LLM\u3002", "conclusion": "RADAR\u901a\u8fc7\u7cfb\u7edf\u6574\u5408\u5185\u5916\u77e5\u8bc6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u653e\u5c04\u5b66\u62a5\u544a\u751f\u6210\u7684\u6548\u679c\u3002"}}
{"id": "2505.14015", "pdf": "https://arxiv.org/pdf/2505.14015", "abs": "https://arxiv.org/abs/2505.14015", "authors": ["Tai D. Nguyen", "Long H. Pham", "Jun Sun"], "title": "AUTOLAW: Enhancing Legal Compliance in Large Language Models via Case Law Generation and Jury-Inspired Deliberation", "categories": ["cs.CL"], "comment": null, "summary": "The rapid advancement of domain-specific large language models (LLMs) in\nfields like law necessitates frameworks that account for nuanced regional legal\ndistinctions, which are critical for ensuring compliance and trustworthiness.\nExisting legal evaluation benchmarks often lack adaptability and fail to\naddress diverse local contexts, limiting their utility in dynamically evolving\nregulatory landscapes. To address these gaps, we propose AutoLaw, a novel\nviolation detection framework that combines adversarial data generation with a\njury-inspired deliberation process to enhance legal compliance of LLMs. Unlike\nstatic approaches, AutoLaw dynamically synthesizes case law to reflect local\nregulations and employs a pool of LLM-based \"jurors\" to simulate judicial\ndecision-making. Jurors are ranked and selected based on synthesized legal\nexpertise, enabling a deliberation process that minimizes bias and improves\ndetection accuracy. Evaluations across three benchmarks: Law-SG, Case-SG\n(legality), and Unfair-TOS (policy), demonstrate AutoLaw's effectiveness:\nadversarial data generation improves LLM discrimination, while the jury-based\nvoting strategy significantly boosts violation detection rates. Our results\nhighlight the framework's ability to adaptively probe legal misalignments and\ndeliver reliable, context-aware judgments, offering a scalable solution for\nevaluating and enhancing LLMs in legally sensitive applications.", "AI": {"tldr": "AutoLaw\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u6cd5\u5f8b\u8fdd\u89c4\u68c0\u6d4b\u6846\u67b6\uff0c\u7ed3\u5408\u5bf9\u6297\u6570\u636e\u751f\u6210\u548c\u966a\u5ba1\u56e2\u5f0f\u5ba1\u8bae\u8fc7\u7a0b\uff0c\u63d0\u5347LLMs\u7684\u6cd5\u5f8b\u5408\u89c4\u6027\u3002", "motivation": "\u73b0\u6709\u6cd5\u5f8b\u8bc4\u4f30\u57fa\u51c6\u7f3a\u4e4f\u9002\u5e94\u6027\u548c\u5bf9\u672c\u5730\u591a\u6837\u6027\u7684\u8003\u8651\uff0c\u9650\u5236\u4e86\u5176\u5728\u52a8\u6001\u76d1\u7ba1\u73af\u5883\u4e2d\u7684\u5b9e\u7528\u6027\u3002", "method": "AutoLaw\u52a8\u6001\u5408\u6210\u6848\u4f8b\u6cd5\u4ee5\u53cd\u6620\u672c\u5730\u6cd5\u89c4\uff0c\u5e76\u5229\u7528LLM\u966a\u5ba1\u56e2\u6a21\u62df\u53f8\u6cd5\u51b3\u7b56\uff0c\u51cf\u5c11\u504f\u89c1\u5e76\u63d0\u9ad8\u68c0\u6d4b\u51c6\u786e\u6027\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAutoLaw\u663e\u8457\u63d0\u5347\u4e86\u8fdd\u89c4\u68c0\u6d4b\u7387\uff0c\u5bf9\u6297\u6570\u636e\u751f\u6210\u548c\u966a\u5ba1\u56e2\u6295\u7968\u7b56\u7565\u6548\u679c\u663e\u8457\u3002", "conclusion": "AutoLaw\u80fd\u81ea\u9002\u5e94\u63a2\u6d4b\u6cd5\u5f8b\u504f\u5dee\uff0c\u63d0\u4f9b\u53ef\u9760\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u5224\u65ad\uff0c\u4e3a\u6cd5\u5f8b\u654f\u611f\u5e94\u7528\u4e2d\u7684LLMs\u8bc4\u4f30\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.14319", "pdf": "https://arxiv.org/pdf/2505.14319", "abs": "https://arxiv.org/abs/2505.14319", "authors": ["Weihao Xia", "Chenliang Zhou", "Cengiz Oztireli"], "title": "RETRO: REthinking Tactile Representation Learning with Material PriOrs", "categories": ["cs.CV", "cs.MM"], "comment": "Code: https://github.com/weihaox/RETRO", "summary": "Tactile perception is profoundly influenced by the surface properties of\nobjects in contact. However, despite their crucial role in shaping tactile\nexperiences, these material characteristics have been largely neglected in\nexisting tactile representation learning methods. Most approaches primarily\nfocus on aligning tactile data with visual or textual information, overlooking\nthe richness of tactile feedback that comes from understanding the materials'\ninherent properties. In this work, we address this gap by revisiting the\ntactile representation learning framework and incorporating material-aware\npriors into the learning process. These priors, which represent pre-learned\ncharacteristics specific to different materials, allow tactile models to better\ncapture and generalize the nuances of surface texture. Our method enables more\naccurate, contextually rich tactile feedback across diverse materials and\ntextures, improving performance in real-world applications such as robotics,\nhaptic feedback systems, and material editing.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6750\u6599\u611f\u77e5\u5148\u9a8c\u7684\u89e6\u89c9\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\uff0c\u5f25\u8865\u4e86\u73b0\u6709\u65b9\u6cd5\u5ffd\u89c6\u6750\u6599\u7279\u6027\u7684\u4e0d\u8db3\uff0c\u63d0\u5347\u4e86\u89e6\u89c9\u53cd\u9988\u7684\u51c6\u786e\u6027\u548c\u4e30\u5bcc\u6027\u3002", "motivation": "\u73b0\u6709\u89e6\u89c9\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u89e6\u89c9\u6570\u636e\u4e0e\u89c6\u89c9\u6216\u6587\u672c\u4fe1\u606f\u7684\u5bf9\u9f50\uff0c\u5ffd\u7565\u4e86\u6750\u6599\u7279\u6027\u5bf9\u89e6\u89c9\u4f53\u9a8c\u7684\u5173\u952e\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u5f15\u5165\u6750\u6599\u611f\u77e5\u5148\u9a8c\uff08\u9884\u5b66\u4e60\u7684\u6750\u6599\u7279\u6027\uff09\u5230\u89e6\u89c9\u8868\u793a\u5b66\u4e60\u6846\u67b6\u4e2d\uff0c\u4ee5\u66f4\u597d\u5730\u6355\u6349\u548c\u6cdb\u5316\u8868\u9762\u7eb9\u7406\u7684\u7ec6\u5fae\u5dee\u5f02\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u6750\u6599\u548c\u7eb9\u7406\u4e0a\u5b9e\u73b0\u4e86\u66f4\u51c6\u786e\u3001\u4e0a\u4e0b\u6587\u4e30\u5bcc\u7684\u89e6\u89c9\u53cd\u9988\uff0c\u63d0\u5347\u4e86\u5728\u673a\u5668\u4eba\u3001\u89e6\u89c9\u53cd\u9988\u7cfb\u7edf\u548c\u6750\u6599\u7f16\u8f91\u7b49\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6027\u80fd\u3002", "conclusion": "\u7ed3\u5408\u6750\u6599\u611f\u77e5\u5148\u9a8c\u7684\u89e6\u89c9\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u89e6\u89c9\u6a21\u578b\u7684\u6027\u80fd\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.14045", "pdf": "https://arxiv.org/pdf/2505.14045", "abs": "https://arxiv.org/abs/2505.14045", "authors": ["Yingli Shen", "Wen Lai", "Shuo Wang", "Kangyang Luo", "Alexander Fraser", "Maosong Sun"], "title": "From Unaligned to Aligned: Scaling Multilingual LLMs with Multi-Way Parallel Corpora", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Continued pretraining and instruction tuning on large-scale multilingual data\nhave proven to be effective in scaling large language models (LLMs) to\nlow-resource languages. However, the unaligned nature of such data limits its\nability to effectively capture cross-lingual semantics. In contrast, multi-way\nparallel data, where identical content is aligned across multiple languages,\nprovides stronger cross-lingual consistency and offers greater potential for\nimproving multilingual performance. In this paper, we introduce a large-scale,\nhigh-quality multi-way parallel corpus, TED2025, based on TED Talks. The corpus\nspans 113 languages, with up to 50 languages aligned in parallel, ensuring\nextensive multilingual coverage. Using this dataset, we investigate best\npractices for leveraging multi-way parallel data to enhance LLMs, including\nstrategies for continued pretraining, instruction tuning, and the analysis of\nkey influencing factors. Experiments on six multilingual benchmarks show that\nmodels trained on multiway parallel data consistently outperform those trained\non unaligned multilingual data.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTED Talks\u7684\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u591a\u8bed\u8a00\u5e73\u884c\u8bed\u6599\u5e93TED2025\uff0c\u7528\u4e8e\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591a\u8bed\u8a00\u6027\u80fd\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u4f7f\u7528\u591a\u5411\u5e73\u884c\u6570\u636e\u7684\u6a21\u578b\u4f18\u4e8e\u672a\u5bf9\u9f50\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b\u3002", "motivation": "\u672a\u5bf9\u9f50\u7684\u591a\u8bed\u8a00\u6570\u636e\u5728\u6355\u6349\u8de8\u8bed\u8a00\u8bed\u4e49\u65b9\u9762\u6548\u679c\u6709\u9650\uff0c\u800c\u591a\u5411\u5e73\u884c\u6570\u636e\u80fd\u63d0\u4f9b\u66f4\u5f3a\u7684\u8de8\u8bed\u8a00\u4e00\u81f4\u6027\uff0c\u4ece\u800c\u63d0\u5347\u591a\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\u3002", "method": "\u6784\u5efa\u4e86TED2025\u8bed\u6599\u5e93\uff0c\u6db5\u76d6113\u79cd\u8bed\u8a00\uff0c\u5176\u4e2d\u591a\u8fbe50\u79cd\u8bed\u8a00\u5e73\u884c\u5bf9\u9f50\u3002\u7814\u7a76\u4e86\u5229\u7528\u591a\u5411\u5e73\u884c\u6570\u636e\u8fdb\u884c\u6301\u7eed\u9884\u8bad\u7ec3\u548c\u6307\u4ee4\u8c03\u4f18\u7684\u6700\u4f73\u5b9e\u8df5\u3002", "result": "\u5728\u516d\u4e2a\u591a\u8bed\u8a00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4f7f\u7528\u591a\u5411\u5e73\u884c\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b\u8868\u73b0\u4f18\u4e8e\u672a\u5bf9\u9f50\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b\u3002", "conclusion": "\u591a\u5411\u5e73\u884c\u6570\u636e\u80fd\u663e\u8457\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591a\u8bed\u8a00\u6027\u80fd\uff0cTED2025\u8bed\u6599\u5e93\u4e3a\u76f8\u5173\u7814\u7a76\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u8d44\u6e90\u3002"}}
{"id": "2505.14320", "pdf": "https://arxiv.org/pdf/2505.14320", "abs": "https://arxiv.org/abs/2505.14320", "authors": ["Maria Cuellar", "Hon Kiu", "To", "Arush Mehrotra"], "title": "Accuracy and Fairness of Facial Recognition Technology in Low-Quality Police Images: An Experiment With Synthetic Faces", "categories": ["cs.CV", "stat.AP"], "comment": null, "summary": "Facial recognition technology (FRT) is increasingly used in criminal\ninvestigations, yet most evaluations of its accuracy rely on high-quality\nimages, unlike those often encountered by law enforcement. This study examines\nhow five common forms of image degradation--contrast, brightness, motion blur,\npose shift, and resolution--affect FRT accuracy and fairness across demographic\ngroups. Using synthetic faces generated by StyleGAN3 and labeled with FairFace,\nwe simulate degraded images and evaluate performance using Deepface with\nArcFace loss in 1:n identification tasks. We perform an experiment and find\nthat false positive rates peak near baseline image quality, while false\nnegatives increase as degradation intensifies--especially with blur and low\nresolution. Error rates are consistently higher for women and Black\nindividuals, with Black females most affected. These disparities raise concerns\nabout fairness and reliability when FRT is used in real-world investigative\ncontexts. Nevertheless, even under the most challenging conditions and for the\nmost affected subgroups, FRT accuracy remains substantially higher than that of\nmany traditional forensic methods. This suggests that, if appropriately\nvalidated and regulated, FRT should be considered a valuable investigative\ntool. However, algorithmic accuracy alone is not sufficient: we must also\nevaluate how FRT is used in practice, including user-driven data manipulation.\nSuch cases underscore the need for transparency and oversight in FRT deployment\nto ensure both fairness and forensic validity.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u56fe\u50cf\u8d28\u91cf\u9000\u5316\uff08\u5982\u5bf9\u6bd4\u5ea6\u3001\u4eae\u5ea6\u3001\u8fd0\u52a8\u6a21\u7cca\u7b49\uff09\u5bf9\u4eba\u8138\u8bc6\u522b\u6280\u672f\uff08FRT\uff09\u51c6\u786e\u6027\u548c\u516c\u5e73\u6027\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u9519\u8bef\u7387\u5728\u5973\u6027\u53ca\u9ed1\u4eba\u7fa4\u4f53\u4e2d\u66f4\u9ad8\uff0c\u4f46FRT\u4ecd\u4f18\u4e8e\u4f20\u7edf\u6cd5\u533b\u65b9\u6cd5\u3002", "motivation": "\u8bc4\u4f30FRT\u5728\u771f\u5b9e\u6267\u6cd5\u573a\u666f\u4e2d\u7684\u8868\u73b0\uff0c\u5c24\u5176\u662f\u56fe\u50cf\u8d28\u91cf\u4e0d\u4f73\u65f6\u7684\u51c6\u786e\u6027\u548c\u516c\u5e73\u6027\u3002", "method": "\u4f7f\u7528StyleGAN3\u751f\u6210\u5408\u6210\u4eba\u8138\uff0c\u6a21\u62df\u9000\u5316\u56fe\u50cf\uff0c\u5e76\u901a\u8fc7Deepface\u548cArcFace\u635f\u5931\u8fdb\u884c1:n\u8bc6\u522b\u4efb\u52a1\u6d4b\u8bd5\u3002", "result": "\u9519\u8bef\u7387\u5728\u56fe\u50cf\u8d28\u91cf\u63a5\u8fd1\u57fa\u7ebf\u65f6\u6700\u9ad8\uff0c\u6a21\u7cca\u548c\u4f4e\u5206\u8fa8\u7387\u5f71\u54cd\u663e\u8457\uff1b\u5973\u6027\u548c\u9ed1\u4eba\u7fa4\u4f53\uff08\u5c24\u5176\u662f\u9ed1\u4eba\u5973\u6027\uff09\u9519\u8bef\u7387\u66f4\u9ad8\u3002", "conclusion": "FRT\u5728\u9a8c\u8bc1\u548c\u76d1\u7ba1\u4e0b\u53ef\u4f5c\u4e3a\u6709\u4ef7\u503c\u7684\u8c03\u67e5\u5de5\u5177\uff0c\u4f46\u9700\u900f\u660e\u5ea6\u548c\u76d1\u7763\u4ee5\u786e\u4fdd\u516c\u5e73\u6027\u548c\u6cd5\u533b\u6709\u6548\u6027\u3002"}}
{"id": "2505.14052", "pdf": "https://arxiv.org/pdf/2505.14052", "abs": "https://arxiv.org/abs/2505.14052", "authors": ["Wei Jiang", "Anying Fu", "Youling Zhang"], "title": "Improved Methods for Model Pruning and Knowledge Distillation", "categories": ["cs.CL", "cs.CE"], "comment": null, "summary": "Model pruning is a performance optimization technique for large language\nmodels like R1 or o3-mini. However, existing pruning methods often lead to\nsignificant performance degradation or require extensive retraining and\nfine-tuning. This technique aims to identify and remove neurons, connections\nunlikely leading to the contribution during the human-computer interaction\nphase. Our goal is to obtain a much smaller and faster knowledge distilled\nmodel that can quickly generate content almost as good as those of the unpruned\nones. We propose MAMA Pruning, short for Movement and Magnitude Analysis, an\nimproved pruning method that effectively reduces model size and computational\ncomplexity while maintaining performance comparable to the original unpruned\nmodel even at extreme pruned levels. The improved method is based on weights,\nbias fixed in the pre-training phase and GRPO rewards verified during the\npost-training phase as our novel pruning indicators. Preliminary experimental\nresults show that our method outperforms and be comparable to state-of-the-art\nmethods across various pruning levels and different downstream computational\nlinguistics tasks.", "AI": {"tldr": "MAMA Pruning\u662f\u4e00\u79cd\u6539\u8fdb\u7684\u6a21\u578b\u526a\u679d\u65b9\u6cd5\uff0c\u901a\u8fc7\u6743\u91cd\u548c\u504f\u7f6e\u5206\u6790\u51cf\u5c11\u6a21\u578b\u5927\u5c0f\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u526a\u679d\u65b9\u6cd5\u5e38\u5bfc\u81f4\u6027\u80fd\u663e\u8457\u4e0b\u964d\u6216\u9700\u8981\u5927\u91cf\u91cd\u65b0\u8bad\u7ec3\u548c\u5fae\u8c03\uff0c\u76ee\u6807\u662f\u83b7\u5f97\u66f4\u5c0f\u3001\u66f4\u5feb\u7684\u77e5\u8bc6\u84b8\u998f\u6a21\u578b\u3002", "method": "\u57fa\u4e8e\u9884\u8bad\u7ec3\u9636\u6bb5\u7684\u6743\u91cd\u548c\u504f\u7f6e\u56fa\u5b9a\uff0c\u4ee5\u53ca\u540e\u8bad\u7ec3\u9636\u6bb5\u7684GRPO\u5956\u52b1\u9a8c\u8bc1\uff0c\u4f5c\u4e3a\u526a\u679d\u6307\u6807\u3002", "result": "\u5728\u6781\u7aef\u526a\u679d\u6c34\u5e73\u4e0b\uff0c\u6027\u80fd\u4ecd\u4e0e\u672a\u526a\u679d\u6a21\u578b\u76f8\u5f53\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "MAMA Pruning\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u6027\u80fd\u4f18\u8d8a\u7684\u526a\u679d\u65b9\u6cd5\u3002"}}
{"id": "2505.14321", "pdf": "https://arxiv.org/pdf/2505.14321", "abs": "https://arxiv.org/abs/2505.14321", "authors": ["Bo Feng", "Zhengfeng Lai", "Shiyu Li", "Zizhen Wang", "Simon Wang", "Ping Huang", "Meng Cao"], "title": "Breaking Down Video LLM Benchmarks: Knowledge, Spatial Perception, or True Temporal Understanding?", "categories": ["cs.CV"], "comment": null, "summary": "Existing video understanding benchmarks often conflate knowledge-based and\npurely image-based questions, rather than clearly isolating a model's temporal\nreasoning ability, which is the key aspect that distinguishes video\nunderstanding from other modalities. We identify two major limitations that\nobscure whether higher scores truly indicate stronger understanding of the\ndynamic content in videos: (1) strong language priors, where models can answer\nquestions without watching the video; and (2) shuffling invariance, where\nmodels maintain similar performance on certain questions even when video frames\nare temporally shuffled. To alleviate these issues, we propose VBenchComp, an\nautomated pipeline that categorizes questions into different domains:\nLLM-Answerable, Semantic, and Temporal. Specifically, LLM-Answerable questions\ncan be answered without viewing the video; Semantic questions remain answerable\neven when the video frames are shuffled; and Temporal questions require\nunderstanding the correct temporal order of frames. The rest of the questions\nare labeled as Others. This can enable fine-grained evaluation of different\ncapabilities of a video LLM. Our analysis reveals nuanced model weaknesses that\nare hidden by traditional overall scores, and we offer insights and\nrecommendations for designing future benchmarks that more accurately assess\nvideo LLMs.", "AI": {"tldr": "VBenchComp\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u5206\u7c7b\u65b9\u6cd5\uff0c\u5c06\u89c6\u9891\u7406\u89e3\u95ee\u9898\u5206\u4e3aLLM-Answerable\u3001Semantic\u3001Temporal\u548cOthers\u56db\u7c7b\uff0c\u4ee5\u66f4\u7cbe\u786e\u8bc4\u4f30\u89c6\u9891LLM\u7684\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u7406\u89e3\u57fa\u51c6\u5e38\u6df7\u6dc6\u77e5\u8bc6\u95ee\u9898\u548c\u7eaf\u56fe\u50cf\u95ee\u9898\uff0c\u672a\u80fd\u6e05\u6670\u533a\u5206\u6a21\u578b\u7684\u65f6\u5e8f\u63a8\u7406\u80fd\u529b\uff0c\u5bfc\u81f4\u8bc4\u5206\u65e0\u6cd5\u771f\u5b9e\u53cd\u6620\u5bf9\u52a8\u6001\u5185\u5bb9\u7684\u7406\u89e3\u3002", "method": "\u63d0\u51faVBenchComp\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u6d41\u7a0b\u5c06\u95ee\u9898\u5206\u7c7b\u4e3aLLM-Answerable\u3001Semantic\u3001Temporal\u548cOthers\uff0c\u4ee5\u7ec6\u7c92\u5ea6\u8bc4\u4f30\u6a21\u578b\u80fd\u529b\u3002", "result": "\u5206\u6790\u63ed\u793a\u4e86\u4f20\u7edf\u8bc4\u5206\u63a9\u76d6\u7684\u6a21\u578b\u5f31\u70b9\uff0c\u5e76\u63d0\u4f9b\u4e86\u8bbe\u8ba1\u66f4\u51c6\u786e\u8bc4\u4f30\u89c6\u9891LLM\u7684\u57fa\u51c6\u7684\u5efa\u8bae\u3002", "conclusion": "VBenchComp\u4e3a\u672a\u6765\u89c6\u9891\u7406\u89e3\u57fa\u51c6\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u80fd\u66f4\u7cbe\u51c6\u8bc4\u4f30\u6a21\u578b\u7684\u65f6\u5e8f\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2505.14070", "pdf": "https://arxiv.org/pdf/2505.14070", "abs": "https://arxiv.org/abs/2505.14070", "authors": ["Feiyu Duan", "Xuemiao Zhang", "Sirui Wang", "Haoran Que", "Yuqi Liu", "Wenge Rong", "Xunliang Cai"], "title": "Enhancing LLMs via High-Knowledge Data Selection", "categories": ["cs.CL"], "comment": null, "summary": "The performance of Large Language Models (LLMs) is intrinsically linked to\nthe quality of its training data. Although several studies have proposed\nmethods for high-quality data selection, they do not consider the importance of\nknowledge richness in text corpora. In this paper, we propose a novel and\ngradient-free High-Knowledge Scorer (HKS) to select high-quality data from the\ndimension of knowledge, to alleviate the problem of knowledge scarcity in the\npre-trained corpus. We propose a comprehensive multi-domain knowledge element\npool and introduce knowledge density and coverage as metrics to assess the\nknowledge content of the text. Based on this, we propose a comprehensive\nknowledge scorer to select data with intensive knowledge, which can also be\nutilized for domain-specific high-knowledge data selection by restricting\nknowledge elements to the specific domain. We train models on a high-knowledge\nbilingual dataset, and experimental results demonstrate that our scorer\nimproves the model's performance in knowledge-intensive and general\ncomprehension tasks, and is effective in enhancing both the generic and\ndomain-specific capabilities of the model.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u77e5\u8bc6\u4e30\u5bcc\u5ea6\u7684\u9ad8\u8d28\u91cf\u6570\u636e\u9009\u62e9\u65b9\u6cd5\uff08HKS\uff09\uff0c\u901a\u8fc7\u77e5\u8bc6\u5bc6\u5ea6\u548c\u8986\u76d6\u7387\u8bc4\u4f30\u6587\u672c\u77e5\u8bc6\u5185\u5bb9\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u9009\u62e9\u65b9\u6cd5\u672a\u8003\u8651\u77e5\u8bc6\u4e30\u5bcc\u5ea6\uff0c\u5bfc\u81f4\u9884\u8bad\u7ec3\u8bed\u6599\u5e93\u4e2d\u77e5\u8bc6\u7a00\u7f3a\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u68af\u5ea6\u65e0\u5173\u7684HKS\u8bc4\u5206\u5668\uff0c\u7ed3\u5408\u591a\u9886\u57df\u77e5\u8bc6\u5143\u7d20\u6c60\u548c\u77e5\u8bc6\u5bc6\u5ea6\u3001\u8986\u76d6\u7387\u6307\u6807\uff0c\u9009\u62e9\u9ad8\u77e5\u8bc6\u5bc6\u5ea6\u6570\u636e\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cHKS\u8bc4\u5206\u5668\u5728\u77e5\u8bc6\u5bc6\u96c6\u548c\u901a\u7528\u7406\u89e3\u4efb\u52a1\u4e2d\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u589e\u5f3a\u901a\u7528\u548c\u9886\u57df\u7279\u5b9a\u80fd\u529b\u3002", "conclusion": "HKS\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u77e5\u8bc6\u7a00\u7f3a\u95ee\u9898\uff0c\u63d0\u5347\u6a21\u578b\u8868\u73b0\uff0c\u9002\u7528\u4e8e\u901a\u7528\u548c\u9886\u57df\u7279\u5b9a\u4efb\u52a1\u3002"}}
{"id": "2505.14330", "pdf": "https://arxiv.org/pdf/2505.14330", "abs": "https://arxiv.org/abs/2505.14330", "authors": ["Rajat Kanti Bhattacharjee", "Meghali Nandi", "Amrit Jha", "Gunajit Kalita", "Ferdous Ahmed Barbhuiya"], "title": "Handloom Design Generation Using Generative Networks", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "This paper proposes deep learning techniques of generating designs for\nclothing, focused on handloom fabric and discusses the associated challenges\nalong with its application. The capability of generative neural network models\nin understanding artistic designs and synthesizing those is not yet explored\nwell. In this work, multiple methods are employed incorporating the current\nstate of the art generative models and style transfer algorithms to study and\nobserve their performance for the task. The results are then evaluated through\nuser score. This work also provides a new dataset NeuralLoom for the task of\nthe design generation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u670d\u88c5\u8bbe\u8ba1\u751f\u6210\u65b9\u6cd5\uff0c\u4e13\u6ce8\u4e8e\u624b\u7ec7\u9762\u6599\uff0c\u5e76\u63a2\u8ba8\u4e86\u76f8\u5173\u6311\u6218\u53ca\u5e94\u7528\u3002", "motivation": "\u751f\u6210\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u5728\u7406\u89e3\u827a\u672f\u8bbe\u8ba1\u548c\u5408\u6210\u65b9\u9762\u7684\u80fd\u529b\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002", "method": "\u7ed3\u5408\u4e86\u5f53\u524d\u6700\u5148\u8fdb\u7684\u751f\u6210\u6a21\u578b\u548c\u98ce\u683c\u8fc1\u79fb\u7b97\u6cd5\uff0c\u7814\u7a76\u5176\u5728\u8bbe\u8ba1\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u901a\u8fc7\u7528\u6237\u8bc4\u5206\u8bc4\u4f30\u7ed3\u679c\uff0c\u5e76\u63d0\u4f9b\u4e86\u65b0\u7684\u6570\u636e\u96c6NeuralLoom\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u624b\u7ec7\u9762\u6599\u8bbe\u8ba1\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u6cd5\u548c\u6570\u636e\u96c6\u3002"}}
{"id": "2505.14079", "pdf": "https://arxiv.org/pdf/2505.14079", "abs": "https://arxiv.org/abs/2505.14079", "authors": ["Weihong Du", "Wenrui Liao", "Binyu Yan", "Hongru Liang", "Anthony G. Cohn", "Wenqiang Lei"], "title": "BAR: A Backward Reasoning based Agent for Complex Minecraft Tasks", "categories": ["cs.CL"], "comment": null, "summary": "Large language model (LLM) based agents have shown great potential in\nfollowing human instructions and automatically completing various tasks. To\ncomplete a task, the agent needs to decompose it into easily executed steps by\nplanning. Existing studies mainly conduct the planning by inferring what steps\nshould be executed next starting from the agent's initial state. However, this\nforward reasoning paradigm doesn't work well for complex tasks. We propose to\nstudy this issue in Minecraft, a virtual environment that simulates complex\ntasks based on real-world scenarios. We believe that the failure of forward\nreasoning is caused by the big perception gap between the agent's initial state\nand task goal. To this end, we leverage backward reasoning and make the\nplanning starting from the terminal state, which can directly achieve the task\ngoal in one step. Specifically, we design a BAckward Reasoning based agent\n(BAR). It is equipped with a recursive goal decomposition module, a state\nconsistency maintaining module and a stage memory module to make robust,\nconsistent, and efficient planning starting from the terminal state.\nExperimental results demonstrate the superiority of BAR over existing methods\nand the effectiveness of proposed modules.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9006\u5411\u63a8\u7406\u7684BAR\u4ee3\u7406\uff0c\u7528\u4e8e\u89e3\u51b3\u590d\u6742\u4efb\u52a1\u4e2d\u524d\u5411\u63a8\u7406\u7684\u4e0d\u8db3\uff0c\u901a\u8fc7\u4ece\u7ec8\u7aef\u72b6\u6001\u51fa\u53d1\u8fdb\u884c\u89c4\u5212\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4efb\u52a1\u5b8c\u6210\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u524d\u5411\u63a8\u7406\u7684\u65b9\u6cd5\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u7531\u4e8e\u521d\u59cb\u72b6\u6001\u4e0e\u4efb\u52a1\u76ee\u6807\u95f4\u7684\u611f\u77e5\u5dee\u8ddd\u8f83\u5927\u3002", "method": "\u8bbe\u8ba1\u4e86BAR\u4ee3\u7406\uff0c\u5305\u542b\u9012\u5f52\u76ee\u6807\u5206\u89e3\u6a21\u5757\u3001\u72b6\u6001\u4e00\u81f4\u6027\u7ef4\u62a4\u6a21\u5757\u548c\u9636\u6bb5\u8bb0\u5fc6\u6a21\u5757\uff0c\u652f\u6301\u4ece\u7ec8\u7aef\u72b6\u6001\u51fa\u53d1\u7684\u9006\u5411\u63a8\u7406\u89c4\u5212\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eBAR\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u63d0\u51fa\u7684\u6a21\u5757\u6709\u6548\u3002", "conclusion": "\u9006\u5411\u63a8\u7406\u5728\u590d\u6742\u4efb\u52a1\u89c4\u5212\u4e2d\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0cBAR\u7684\u8bbe\u8ba1\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.14333", "pdf": "https://arxiv.org/pdf/2505.14333", "abs": "https://arxiv.org/abs/2505.14333", "authors": ["Inder Pal Singh", "Enjie Ghorbel", "Anis Kacem", "Djamila Aouada"], "title": "Domain Adaptation for Multi-label Image Classification: a Discriminator-free Approach", "categories": ["cs.CV"], "comment": "The paper is under consideration at Computer Vision and Image\n  Understanding. arXiv admin note: text overlap with arXiv:2301.10611", "summary": "This paper introduces a discriminator-free adversarial-based approach termed\nDDA-MLIC for Unsupervised Domain Adaptation (UDA) in the context of Multi-Label\nImage Classification (MLIC). While recent efforts have explored\nadversarial-based UDA methods for MLIC, they typically include an additional\ndiscriminator subnet. Nevertheless, decoupling the classification and the\ndiscrimination tasks may harm their task-specific discriminative power. Herein,\nwe address this challenge by presenting a novel adversarial critic directly\nderived from the task-specific classifier. Specifically, we employ a\ntwo-component Gaussian Mixture Model (GMM) to model both source and target\npredictions, distinguishing between two distinct clusters. Instead of using the\ntraditional Expectation Maximization (EM) algorithm, our approach utilizes a\nDeep Neural Network (DNN) to estimate the parameters of each GMM component.\nSubsequently, the source and target GMM parameters are leveraged to formulate\nan adversarial loss using the Fr\\'echet distance. The proposed framework is\ntherefore not only fully differentiable but is also cost-effective as it avoids\nthe expensive iterative process usually induced by the standard EM method. The\nproposed method is evaluated on several multi-label image datasets covering\nthree different types of domain shift. The obtained results demonstrate that\nDDA-MLIC outperforms existing state-of-the-art methods in terms of precision\nwhile requiring a lower number of parameters. The code is made publicly\navailable at github.com/cvi2snt/DDA-MLIC.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u5224\u522b\u5668\u7684\u5bf9\u6297\u6027\u65b9\u6cd5DDA-MLIC\uff0c\u7528\u4e8e\u591a\u6807\u7b7e\u56fe\u50cf\u5206\u7c7b\u7684\u65e0\u76d1\u7763\u57df\u9002\u5e94\uff0c\u901a\u8fc7\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u548c\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u4f18\u5316\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u5bf9\u6297\u6027\u65b9\u6cd5\u5728\u591a\u6807\u7b7e\u56fe\u50cf\u5206\u7c7b\u4e2d\u901a\u5e38\u9700\u8981\u989d\u5916\u7684\u5224\u522b\u5668\u5b50\u7f51\uff0c\u8fd9\u53ef\u80fd\u635f\u5bb3\u4efb\u52a1\u7279\u5f02\u6027\u5224\u522b\u80fd\u529b\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u9ad8\u65af\u6df7\u5408\u6a21\u578b\uff08GMM\uff09\u5efa\u6a21\u6e90\u548c\u76ee\u6807\u9884\u6d4b\uff0c\u901a\u8fc7\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u4f30\u8ba1GMM\u53c2\u6570\uff0c\u5e76\u5229\u7528Fr'echet\u8ddd\u79bb\u6784\u5efa\u5bf9\u6297\u635f\u5931\u3002", "result": "\u5728\u591a\u79cd\u591a\u6807\u7b7e\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\uff0cDDA-MLIC\u5728\u7cbe\u5ea6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u53c2\u6570\u66f4\u5c11\u3002", "conclusion": "DDA-MLIC\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u6027\u80fd\u4f18\u8d8a\u7684\u65e0\u76d1\u7763\u57df\u9002\u5e94\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u591a\u6807\u7b7e\u56fe\u50cf\u5206\u7c7b\u3002"}}
{"id": "2505.14080", "pdf": "https://arxiv.org/pdf/2505.14080", "abs": "https://arxiv.org/abs/2505.14080", "authors": ["Franziska Sofia Hafner", "Ana Valdivia", "Luc Rocher"], "title": "Gender Trouble in Language Models: An Empirical Audit Guided by Gender Performativity Theory", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "comment": null, "summary": "Language models encode and subsequently perpetuate harmful gendered\nstereotypes. Research has succeeded in mitigating some of these harms, e.g. by\ndissociating non-gendered terms such as occupations from gendered terms such as\n'woman' and 'man'. This approach, however, remains superficial given that\nassociations are only one form of prejudice through which gendered harms arise.\nCritical scholarship on gender, such as gender performativity theory,\nemphasizes how harms often arise from the construction of gender itself, such\nas conflating gender with biological sex. In language models, these issues\ncould lead to the erasure of transgender and gender diverse identities and\ncause harms in downstream applications, from misgendering users to\nmisdiagnosing patients based on wrong assumptions about their anatomy.\n  For FAccT research on gendered harms to go beyond superficial linguistic\nassociations, we advocate for a broader definition of 'gender bias' in language\nmodels. We operationalize insights on the construction of gender through\nlanguage from gender studies literature and then empirically test how 16\nlanguage models of different architectures, training datasets, and model sizes\nencode gender. We find that language models tend to encode gender as a binary\ncategory tied to biological sex, and that gendered terms that do not neatly\nfall into one of these binary categories are erased and pathologized. Finally,\nwe show that larger models, which achieve better results on performance\nbenchmarks, learn stronger associations between gender and sex, further\nreinforcing a narrow understanding of gender. Our findings lead us to call for\na re-evaluation of how gendered harms in language models are defined and\naddressed.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u7f16\u7801\u548c\u5ef6\u7eed\u6709\u5bb3\u7684\u6027\u522b\u523b\u677f\u5370\u8c61\uff0c\u5e76\u547c\u5401\u91cd\u65b0\u5b9a\u4e49\u6027\u522b\u504f\u89c1\u7684\u6982\u5ff5\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4ec5\u901a\u8fc7\u89e3\u8026\u975e\u6027\u522b\u8bcd\u6c47\u4e0e\u6027\u522b\u8bcd\u6c47\u6765\u7f13\u89e3\u6027\u522b\u504f\u89c1\uff0c\u4f46\u5ffd\u7565\u4e86\u6027\u522b\u5efa\u6784\u672c\u8eab\u5e26\u6765\u7684\u66f4\u6df1\u5c42\u6b21\u95ee\u9898\uff0c\u5982\u5bf9\u8de8\u6027\u522b\u548c\u975e\u4e8c\u5143\u6027\u522b\u8eab\u4efd\u7684\u5ffd\u89c6\u3002", "method": "\u4f5c\u8005\u7ed3\u5408\u6027\u522b\u7814\u7a76\u7406\u8bba\uff0c\u5b9e\u8bc1\u5206\u6790\u4e8616\u79cd\u4e0d\u540c\u67b6\u6784\u3001\u8bad\u7ec3\u6570\u636e\u548c\u89c4\u6a21\u7684\u6a21\u578b\u5982\u4f55\u7f16\u7801\u6027\u522b\u3002", "result": "\u53d1\u73b0\u8bed\u8a00\u6a21\u578b\u503e\u5411\u4e8e\u5c06\u6027\u522b\u7f16\u7801\u4e3a\u4e0e\u751f\u7269\u6027\u522b\u7ed1\u5b9a\u7684\u4e8c\u5143\u7c7b\u522b\uff0c\u4e14\u66f4\u5927\u89c4\u6a21\u7684\u6a21\u578b\u5f3a\u5316\u4e86\u8fd9\u79cd\u72ed\u9698\u7684\u6027\u522b\u7406\u89e3\u3002", "conclusion": "\u547c\u5401\u91cd\u65b0\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\u4e2d\u6027\u522b\u504f\u89c1\u7684\u5b9a\u4e49\u548c\u89e3\u51b3\u65b9\u6cd5\uff0c\u4ee5\u66f4\u5168\u9762\u5730\u5e94\u5bf9\u6027\u522b\u591a\u6837\u6027\u95ee\u9898\u3002"}}
{"id": "2505.14340", "pdf": "https://arxiv.org/pdf/2505.14340", "abs": "https://arxiv.org/abs/2505.14340", "authors": ["Seunghyuk Cho", "Zhenyue Qin", "Yang Liu", "Youngbin Choi", "Seungbeom Lee", "Dongwoo Kim"], "title": "Plane Geometry Problem Solving with Multi-modal Reasoning: A Survey", "categories": ["cs.CV", "cs.LG"], "comment": "18 pages", "summary": "Plane geometry problem solving (PGPS) has recently gained significant\nattention as a benchmark to assess the multi-modal reasoning capabilities of\nlarge vision-language models. Despite the growing interest in PGPS, the\nresearch community still lacks a comprehensive overview that systematically\nsynthesizes recent work in PGPS. To fill this gap, we present a survey of\nexisting PGPS studies. We first categorize PGPS methods into an encoder-decoder\nframework and summarize the corresponding output formats used by their encoders\nand decoders. Subsequently, we classify and analyze these encoders and decoders\naccording to their architectural designs. Finally, we outline major challenges\nand promising directions for future research. In particular, we discuss the\nhallucination issues arising during the encoding phase within encoder-decoder\narchitectures, as well as the problem of data leakage in current PGPS\nbenchmarks.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u5e73\u9762\u51e0\u4f55\u95ee\u9898\u6c42\u89e3\uff08PGPS\uff09\u7684\u7814\u7a76\u73b0\u72b6\uff0c\u5c06\u5176\u65b9\u6cd5\u5f52\u7c7b\u4e3a\u7f16\u7801\u5668-\u89e3\u7801\u5668\u6846\u67b6\uff0c\u5206\u6790\u4e86\u5176\u67b6\u6784\u8bbe\u8ba1\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u7684\u6311\u6218\u4e0e\u65b9\u5411\u3002", "motivation": "\u586b\u8865PGPS\u9886\u57df\u7f3a\u4e4f\u7cfb\u7edf\u6027\u7efc\u8ff0\u7684\u7a7a\u767d\uff0c\u4e3a\u7814\u7a76\u793e\u533a\u63d0\u4f9b\u5168\u9762\u7684\u7814\u7a76\u73b0\u72b6\u5206\u6790\u3002", "method": "\u5c06PGPS\u65b9\u6cd5\u5f52\u7c7b\u4e3a\u7f16\u7801\u5668-\u89e3\u7801\u5668\u6846\u67b6\uff0c\u603b\u7ed3\u5176\u8f93\u5165\u8f93\u51fa\u683c\u5f0f\uff0c\u5e76\u5206\u6790\u4e0d\u540c\u67b6\u6784\u8bbe\u8ba1\u3002", "result": "\u63d0\u51fa\u4e86PGPS\u7814\u7a76\u7684\u4e3b\u8981\u6311\u6218\uff0c\u5982\u7f16\u7801\u9636\u6bb5\u7684\u5e7b\u89c9\u95ee\u9898\u548c\u6570\u636e\u6cc4\u9732\u95ee\u9898\u3002", "conclusion": "\u672a\u6765\u7814\u7a76\u5e94\u5173\u6ce8\u89e3\u51b3\u5e7b\u89c9\u95ee\u9898\u548c\u6570\u636e\u6cc4\u9732\u95ee\u9898\uff0c\u4ee5\u63a8\u52a8PGPS\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2505.14099", "pdf": "https://arxiv.org/pdf/2505.14099", "abs": "https://arxiv.org/abs/2505.14099", "authors": ["Yihua Zhu", "Qianying Liu", "Akiko Aizawa", "Hidetoshi Shimodaira"], "title": "Beyond Chains: Bridging Large Language Models and Knowledge Bases in Complex Question Answering", "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Knowledge Base Question Answering (KBQA) aims to answer natural language\nquestions using structured knowledge from KBs. While LLM-only approaches offer\ngeneralization, they suffer from outdated knowledge, hallucinations, and lack\nof transparency. Chain-based KG-RAG methods address these issues by\nincorporating external KBs, but are limited to simple chain-structured\nquestions due to the absence of planning and logical structuring. Inspired by\nsemantic parsing methods, we propose PDRR: a four-stage framework consisting of\nPredict, Decompose, Retrieve, and Reason. Our method first predicts the\nquestion type and decomposes the question into structured triples. Then\nretrieves relevant information from KBs and guides the LLM as an agent to\nreason over and complete the decomposed triples. Experimental results\ndemonstrate that PDRR consistently outperforms existing methods across various\nLLM backbones and achieves superior performance on both chain-structured and\nnon-chain complex questions.", "AI": {"tldr": "PDRR\u6846\u67b6\u901a\u8fc7\u9884\u6d4b\u3001\u5206\u89e3\u3001\u68c0\u7d22\u548c\u63a8\u7406\u56db\u9636\u6bb5\uff0c\u7ed3\u5408KB\u548cLLM\uff0c\u663e\u8457\u63d0\u5347KBQA\u6027\u80fd\uff0c\u5c24\u5176\u5728\u590d\u6742\u95ee\u9898\u4e0a\u8868\u73b0\u4f18\u8d8a\u3002", "motivation": "\u89e3\u51b3LLM-only\u65b9\u6cd5\u7684\u77e5\u8bc6\u8fc7\u65f6\u3001\u5e7b\u89c9\u548c\u900f\u660e\u5ea6\u95ee\u9898\uff0c\u4ee5\u53caKG-RAG\u65b9\u6cd5\u4ec5\u9002\u7528\u4e8e\u7b80\u5355\u94fe\u5f0f\u95ee\u9898\u7684\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51faPDRR\u6846\u67b6\uff1a\u9884\u6d4b\u95ee\u9898\u7c7b\u578b\u3001\u5206\u89e3\u4e3a\u7ed3\u6784\u5316\u4e09\u5143\u7ec4\u3001\u4eceKB\u68c0\u7d22\u4fe1\u606f\u3001\u5f15\u5bfcLLM\u63a8\u7406\u5b8c\u6210\u4e09\u5143\u7ec4\u3002", "result": "PDRR\u5728\u591a\u79cdLLM\u9aa8\u5e72\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5bf9\u94fe\u5f0f\u548c\u975e\u94fe\u5f0f\u590d\u6742\u95ee\u9898\u5747\u6709\u4f18\u8d8a\u8868\u73b0\u3002", "conclusion": "PDRR\u901a\u8fc7\u7ed3\u6784\u5316\u89c4\u5212\u548c\u903b\u8f91\u63a8\u7406\uff0c\u6709\u6548\u7ed3\u5408KB\u4e0eLLM\uff0c\u663e\u8457\u63d0\u5347KBQA\u80fd\u529b\u3002"}}
{"id": "2505.14341", "pdf": "https://arxiv.org/pdf/2505.14341", "abs": "https://arxiv.org/abs/2505.14341", "authors": ["Sifan Li", "Ming Tao", "Hao Zhao", "Ling Shao", "Hao Tang"], "title": "Replace in Translation: Boost Concept Alignment in Counterfactual Text-to-Image", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Text-to-Image (T2I) has been prevalent in recent years, with most common\ncondition tasks having been optimized nicely. Besides, counterfactual\nText-to-Image is obstructing us from a more versatile AIGC experience. For\nthose scenes that are impossible to happen in real world and anti-physics, we\nshould spare no efforts in increasing the factual feel, which means\nsynthesizing images that people think very likely to be happening, and concept\nalignment, which means all the required objects should be in the same frame. In\nthis paper, we focus on concept alignment. As controllable T2I models have\nachieved satisfactory performance for real applications, we utilize this\ntechnology to replace the objects in a synthesized image in latent space\nstep-by-step to change the image from a common scene to a counterfactual scene\nto meet the prompt. We propose a strategy to instruct this replacing process,\nwhich is called as Explicit Logical Narrative Prompt (ELNP), by using the newly\nSoTA language model DeepSeek to generate the instructions. Furthermore, to\nevaluate models' performance in counterfactual T2I, we design a metric to\ncalculate how many required concepts in the prompt can be covered averagely in\nthe synthesized images. The extensive experiments and qualitative comparisons\ndemonstrate that our strategy can boost the concept alignment in counterfactual\nT2I.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u663e\u5f0f\u903b\u8f91\u53d9\u4e8b\u63d0\u793a\uff08ELNP\uff09\u548c\u6f5c\u5728\u7a7a\u95f4\u9010\u6b65\u66ff\u6362\u7684\u65b9\u6cd5\uff0c\u63d0\u5347\u53cd\u4e8b\u5b9e\u6587\u672c\u5230\u56fe\u50cf\uff08T2I\uff09\u751f\u6210\u4e2d\u7684\u6982\u5ff5\u5bf9\u9f50\u80fd\u529b\u3002", "motivation": "\u53cd\u4e8b\u5b9eT2I\u751f\u6210\u5728\u771f\u5b9e\u611f\u548c\u6982\u5ff5\u5bf9\u9f50\u65b9\u9762\u5b58\u5728\u6311\u6218\uff0c\u9650\u5236\u4e86AIGC\u7684\u591a\u6837\u6027\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5229\u7528\u53ef\u63a7T2I\u6a21\u578b\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u9010\u6b65\u66ff\u6362\u5bf9\u8c61\uff0c\u7ed3\u5408DeepSeek\u751f\u6210\u7684ELNP\u6307\u5bfc\u66ff\u6362\u8fc7\u7a0b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u53cd\u4e8b\u5b9eT2I\u751f\u6210\u7684\u6982\u5ff5\u5bf9\u9f50\u80fd\u529b\u3002", "conclusion": "\u63d0\u51fa\u7684\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86\u53cd\u4e8b\u5b9eT2I\u751f\u6210\u4e2d\u7684\u6982\u5ff5\u5bf9\u9f50\u95ee\u9898\uff0c\u4e3aAIGC\u63d0\u4f9b\u4e86\u66f4\u4e30\u5bcc\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2505.14101", "pdf": "https://arxiv.org/pdf/2505.14101", "abs": "https://arxiv.org/abs/2505.14101", "authors": ["Ernests Lavrinovics", "Russa Biswas", "Katja Hose", "Johannes Bjerva"], "title": "MultiHal: Multilingual Dataset for Knowledge-Graph Grounded Evaluation of LLM Hallucinations", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have inherent limitations of faithfulness and\nfactuality, commonly referred to as hallucinations. Several benchmarks have\nbeen developed that provide a test bed for factuality evaluation within the\ncontext of English-centric datasets, while relying on supplementary informative\ncontext like web links or text passages but ignoring the available structured\nfactual resources. To this end, Knowledge Graphs (KGs) have been identified as\na useful aid for hallucination mitigation, as they provide a structured way to\nrepresent the facts about entities and their relations with minimal linguistic\noverhead. We bridge the lack of KG paths and multilinguality for factual\nlanguage modeling within the existing hallucination evaluation benchmarks and\npropose a KG-based multilingual, multihop benchmark called \\textbf{MultiHal}\nframed for generative text evaluation. As part of our data collection pipeline,\nwe mined 140k KG-paths from open-domain KGs, from which we pruned noisy\nKG-paths, curating a high-quality subset of 25.9k. Our baseline evaluation\nshows an absolute scale increase by approximately 0.12 to 0.36 points for the\nsemantic similarity score in KG-RAG over vanilla QA across multiple languages\nand multiple models, demonstrating the potential of KG integration. We\nanticipate MultiHal will foster future research towards several graph-based\nhallucination mitigation and fact-checking tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\uff08KG\uff09\u7684\u591a\u8bed\u8a00\u3001\u591a\u8df3\u57fa\u51c6\u6d4b\u8bd5MultiHal\uff0c\u7528\u4e8e\u8bc4\u4f30\u751f\u6210\u6587\u672c\u7684\u4e8b\u5b9e\u6027\uff0c\u5e76\u5c55\u793a\u4e86KG\u6574\u5408\u5728\u51cf\u5c11\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5e7b\u89c9\u65b9\u9762\u7684\u6f5c\u529b\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5b58\u5728\u5e7b\u89c9\u95ee\u9898\uff0c\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u591a\u4f9d\u8d56\u82f1\u8bed\u6570\u636e\u96c6\u548c\u8865\u5145\u4e0a\u4e0b\u6587\uff0c\u5ffd\u7565\u4e86\u7ed3\u6784\u5316\u4e8b\u5b9e\u8d44\u6e90\u3002\u77e5\u8bc6\u56fe\u8c31\uff08KG\uff09\u56e0\u5176\u7ed3\u6784\u5316\u8868\u793a\u4e8b\u5b9e\u7684\u80fd\u529b\u88ab\u7528\u4e8e\u7f13\u89e3\u5e7b\u89c9\u3002", "method": "\u901a\u8fc7\u6316\u6398\u548c\u7b5b\u9009\u5f00\u653e\u9886\u57dfKG\u4e2d\u7684\u8def\u5f84\uff0c\u6784\u5efa\u4e86\u9ad8\u8d28\u91cf\u768425.9k\u6761KG\u8def\u5f84\uff0c\u5e76\u63d0\u51fa\u4e86MultiHal\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u591a\u8bed\u8a00\u751f\u6210\u6587\u672c\u7684\u4e8b\u5b9e\u6027\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cKG-RAG\u5728\u8bed\u4e49\u76f8\u4f3c\u5ea6\u5f97\u5206\u4e0a\u6bd4\u4f20\u7edfQA\u65b9\u6cd5\u63d0\u9ad8\u4e860.12\u52300.36\u5206\uff0c\u9a8c\u8bc1\u4e86KG\u6574\u5408\u7684\u6709\u6548\u6027\u3002", "conclusion": "MultiHal\u57fa\u51c6\u6d4b\u8bd5\u6709\u671b\u63a8\u52a8\u57fa\u4e8e\u56fe\u8c31\u7684\u5e7b\u89c9\u7f13\u89e3\u548c\u4e8b\u5b9e\u6838\u67e5\u4efb\u52a1\u7684\u7814\u7a76\u3002"}}
{"id": "2505.14346", "pdf": "https://arxiv.org/pdf/2505.14346", "abs": "https://arxiv.org/abs/2505.14346", "authors": ["Mingfang Zhang", "Ryo Yonetani", "Yifei Huang", "Liangyang Ouyang", "Ruicong Liu", "Yoichi Sato"], "title": "Egocentric Action-aware Inertial Localization in Point Clouds", "categories": ["cs.CV"], "comment": null, "summary": "This paper presents a novel inertial localization framework named Egocentric\nAction-aware Inertial Localization (EAIL), which leverages egocentric action\ncues from head-mounted IMU signals to localize the target individual within a\n3D point cloud. Human inertial localization is challenging due to IMU sensor\nnoise that causes trajectory drift over time. The diversity of human actions\nfurther complicates IMU signal processing by introducing various motion\npatterns. Nevertheless, we observe that some actions observed through the\nhead-mounted IMU correlate with spatial environmental structures (e.g., bending\ndown to look inside an oven, washing dishes next to a sink), thereby serving as\nspatial anchors to compensate for the localization drift. The proposed EAIL\nframework learns such correlations via hierarchical multi-modal alignment. By\nassuming that the 3D point cloud of the environment is available, it\ncontrastively learns modality encoders that align short-term egocentric action\ncues in IMU signals with local environmental features in the point cloud. These\nencoders are then used in reasoning the IMU data and the point cloud over time\nand space to perform inertial localization. Interestingly, these encoders can\nfurther be utilized to recognize the corresponding sequence of actions as a\nby-product. Extensive experiments demonstrate the effectiveness of the proposed\nframework over state-of-the-art inertial localization and inertial action\nrecognition baselines.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aEAIL\u7684\u65b0\u578b\u60ef\u6027\u5b9a\u4f4d\u6846\u67b6\uff0c\u5229\u7528\u5934\u6234\u5f0fIMU\u4fe1\u53f7\u4e2d\u7684\u81ea\u6211\u4e2d\u5fc3\u52a8\u4f5c\u7ebf\u7d22\u57283D\u70b9\u4e91\u4e2d\u5b9a\u4f4d\u76ee\u6807\u4e2a\u4f53\u3002", "motivation": "\u89e3\u51b3IMU\u4f20\u611f\u5668\u566a\u58f0\u548c\u4eba\u7c7b\u52a8\u4f5c\u591a\u6837\u6027\u5bfc\u81f4\u7684\u5b9a\u4f4d\u6f02\u79fb\u95ee\u9898\uff0c\u5229\u7528\u52a8\u4f5c\u4e0e\u73af\u5883\u7ed3\u6784\u7684\u5173\u8054\u6027\u4f5c\u4e3a\u7a7a\u95f4\u951a\u70b9\u3002", "method": "\u901a\u8fc7\u5206\u5c42\u591a\u6a21\u6001\u5bf9\u9f50\u5b66\u4e60IMU\u4fe1\u53f7\u4e2d\u7684\u52a8\u4f5c\u7ebf\u7d22\u4e0e\u70b9\u4e91\u4e2d\u73af\u5883\u7279\u5f81\u7684\u5173\u8054\uff0c\u5229\u7528\u5bf9\u6bd4\u5b66\u4e60\u8bad\u7ec3\u6a21\u6001\u7f16\u7801\u5668\u3002", "result": "\u5b9e\u9a8c\u8868\u660eEAIL\u5728\u60ef\u6027\u5b9a\u4f4d\u548c\u52a8\u4f5c\u8bc6\u522b\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "EAIL\u6846\u67b6\u6709\u6548\u5229\u7528\u52a8\u4f5c\u4e0e\u73af\u5883\u7684\u76f8\u5173\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u60ef\u6027\u5b9a\u4f4d\u7684\u7cbe\u5ea6\uff0c\u540c\u65f6\u8fd8\u80fd\u8bc6\u522b\u52a8\u4f5c\u5e8f\u5217\u3002"}}
{"id": "2505.14104", "pdf": "https://arxiv.org/pdf/2505.14104", "abs": "https://arxiv.org/abs/2505.14104", "authors": ["Wei Fan", "Tianshi Zheng", "Yiran Hu", "Zheye Deng", "Weiqi Wang", "Baixuan Xu", "Chunyang Li", "Haoran Li", "Weixing Shen", "Yangqiu Song"], "title": "Legal Rule Induction: Towards Generalizable Principle Discovery from Analogous Judicial Precedents", "categories": ["cs.CL"], "comment": "Under Review", "summary": "Legal rules encompass not only codified statutes but also implicit\nadjudicatory principles derived from precedents that contain discretionary\nnorms, social morality, and policy. While computational legal research has\nadvanced in applying established rules to cases, inducing legal rules from\njudicial decisions remains understudied, constrained by limitations in model\ninference efficacy and symbolic reasoning capability. The advent of Large\nLanguage Models (LLMs) offers unprecedented opportunities for automating the\nextraction of such latent principles, yet progress is stymied by the absence of\nformal task definitions, benchmark datasets, and methodologies. To address this\ngap, we formalize Legal Rule Induction (LRI) as the task of deriving concise,\ngeneralizable doctrinal rules from sets of analogous precedents, distilling\ntheir shared preconditions, normative behaviors, and legal consequences. We\nintroduce the first LRI benchmark, comprising 5,121 case sets (38,088 Chinese\ncases in total) for model tuning and 216 expert-annotated gold test sets.\nExperimental results reveal that: 1) State-of-the-art LLMs struggle with\nover-generalization and hallucination; 2) Training on our dataset markedly\nenhances LLMs capabilities in capturing nuanced rule patterns across similar\ncases.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u6cd5\u5f8b\u89c4\u5219\u5f52\u7eb3\uff08LRI\uff09\u4efb\u52a1\uff0c\u65e8\u5728\u4ece\u7c7b\u4f3c\u5224\u4f8b\u4e2d\u63d0\u53d6\u7b80\u660e\u3001\u53ef\u63a8\u5e7f\u7684\u6cd5\u5f8b\u89c4\u5219\uff0c\u5e76\u6784\u5efa\u4e86\u9996\u4e2aLRI\u57fa\u51c6\u6570\u636e\u96c6\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u73b0\u6709LLMs\u5b58\u5728\u8fc7\u5ea6\u6cdb\u5316\u548c\u5e7b\u89c9\u95ee\u9898\uff0c\u4f46\u901a\u8fc7\u8be5\u6570\u636e\u96c6\u8bad\u7ec3\u53ef\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u8ba1\u7b97\u6cd5\u5f8b\u7814\u7a76\u5728\u5e94\u7528\u5df2\u6709\u89c4\u5219\u65b9\u9762\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u4ece\u53f8\u6cd5\u5224\u51b3\u4e2d\u5f52\u7eb3\u6cd5\u5f8b\u89c4\u5219\u7684\u7814\u7a76\u4ecd\u4e0d\u8db3\uff0c\u53d7\u9650\u4e8e\u6a21\u578b\u63a8\u7406\u6548\u80fd\u548c\u7b26\u53f7\u63a8\u7406\u80fd\u529b\u3002LLMs\u7684\u51fa\u73b0\u4e3a\u81ea\u52a8\u5316\u63d0\u53d6\u6f5c\u5728\u89c4\u5219\u63d0\u4f9b\u4e86\u673a\u4f1a\uff0c\u4f46\u7f3a\u4e4f\u6b63\u5f0f\u4efb\u52a1\u5b9a\u4e49\u3001\u57fa\u51c6\u6570\u636e\u96c6\u548c\u65b9\u6cd5\u8bba\u3002", "method": "\u8bba\u6587\u5c06LRI\u4efb\u52a1\u5b9a\u4e49\u4e3a\u4ece\u7c7b\u4f3c\u5224\u4f8b\u4e2d\u63d0\u53d6\u5171\u4eab\u524d\u63d0\u6761\u4ef6\u3001\u89c4\u8303\u884c\u4e3a\u548c\u6cd5\u5f8b\u540e\u679c\u7684\u7b80\u660e\u89c4\u5219\uff0c\u5e76\u6784\u5efa\u4e86\u5305\u542b5,121\u4e2a\u6848\u4f8b\u96c6\uff08\u603b\u8ba138,088\u4e2a\u4e2d\u56fd\u6848\u4f8b\uff09\u7684\u57fa\u51c6\u6570\u636e\u96c6\u548c216\u4e2a\u4e13\u5bb6\u6807\u6ce8\u7684\u6d4b\u8bd5\u96c6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff1a1\uff09\u73b0\u6709LLMs\u5b58\u5728\u8fc7\u5ea6\u6cdb\u5316\u548c\u5e7b\u89c9\u95ee\u9898\uff1b2\uff09\u4f7f\u7528\u8be5\u6570\u636e\u96c6\u8bad\u7ec3\u663e\u8457\u63d0\u5347\u4e86LLMs\u5728\u6355\u6349\u7c7b\u4f3c\u6848\u4f8b\u4e2d\u7ec6\u5fae\u89c4\u5219\u6a21\u5f0f\u7684\u80fd\u529b\u3002", "conclusion": "\u8bba\u6587\u586b\u8865\u4e86\u6cd5\u5f8b\u89c4\u5219\u5f52\u7eb3\u7814\u7a76\u7684\u7a7a\u767d\uff0c\u4e3a\u81ea\u52a8\u5316\u63d0\u53d6\u6cd5\u5f8b\u89c4\u5219\u63d0\u4f9b\u4e86\u65b9\u6cd5\u8bba\u548c\u57fa\u51c6\u652f\u6301\uff0c\u5c55\u793a\u4e86LLMs\u5728\u8be5\u9886\u57df\u7684\u6f5c\u529b\u4e0e\u6539\u8fdb\u65b9\u5411\u3002"}}
{"id": "2505.14357", "pdf": "https://arxiv.org/pdf/2505.14357", "abs": "https://arxiv.org/abs/2505.14357", "authors": ["Siqiao Huang", "Jialong Wu", "Qixing Zhou", "Shangchen Miao", "Mingsheng Long"], "title": "Vid2World: Crafting Video Diffusion Models to Interactive World Models", "categories": ["cs.CV", "cs.LG"], "comment": "Project page: http://knightnemo.github.io/vid2world/", "summary": "World models, which predict transitions based on history observation and\naction sequences, have shown great promise in improving data efficiency for\nsequential decision making. However, existing world models often require\nextensive domain-specific training and still produce low-fidelity, coarse\npredictions, limiting their applicability in complex environments. In contrast,\nvideo diffusion models trained on large, internet-scale datasets have\ndemonstrated impressive capabilities in generating high-quality videos that\ncapture diverse real-world dynamics. In this work, we present Vid2World, a\ngeneral approach for leveraging and transferring pre-trained video diffusion\nmodels into interactive world models. To bridge the gap, Vid2World performs\ncasualization of a pre-trained video diffusion model by crafting its\narchitecture and training objective to enable autoregressive generation.\nFurthermore, it introduces a causal action guidance mechanism to enhance action\ncontrollability in the resulting interactive world model. Extensive experiments\nin robot manipulation and game simulation domains show that our method offers a\nscalable and effective approach for repurposing highly capable video diffusion\nmodels to interactive world models.", "AI": {"tldr": "Vid2World\u5229\u7528\u9884\u8bad\u7ec3\u89c6\u9891\u6269\u6563\u6a21\u578b\u6784\u5efa\u4ea4\u4e92\u5f0f\u4e16\u754c\u6a21\u578b\uff0c\u901a\u8fc7\u56e0\u679c\u5316\u548c\u52a8\u4f5c\u5f15\u5bfc\u673a\u5236\u63d0\u5347\u9884\u6d4b\u8d28\u91cf\u548c\u53ef\u63a7\u6027\u3002", "motivation": "\u73b0\u6709\u4e16\u754c\u6a21\u578b\u9700\u8981\u5927\u91cf\u9886\u57df\u7279\u5b9a\u8bad\u7ec3\u4e14\u9884\u6d4b\u7c97\u7cd9\uff0c\u800c\u89c6\u9891\u6269\u6563\u6a21\u578b\u80fd\u751f\u6210\u9ad8\u8d28\u91cf\u89c6\u9891\uff0c\u4f46\u7f3a\u4e4f\u4ea4\u4e92\u6027\u3002", "method": "Vid2World\u901a\u8fc7\u56e0\u679c\u5316\u9884\u8bad\u7ec3\u89c6\u9891\u6269\u6563\u6a21\u578b\u67b6\u6784\u548c\u8bad\u7ec3\u76ee\u6807\uff0c\u5e76\u5f15\u5165\u56e0\u679c\u52a8\u4f5c\u5f15\u5bfc\u673a\u5236\u3002", "result": "\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u548c\u6e38\u620f\u4eff\u771f\u9886\u57df\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "Vid2World\u4e3a\u5c06\u89c6\u9891\u6269\u6563\u6a21\u578b\u8f6c\u5316\u4e3a\u4ea4\u4e92\u5f0f\u4e16\u754c\u6a21\u578b\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u9ad8\u6548\u7684\u65b9\u6cd5\u3002"}}
{"id": "2505.14106", "pdf": "https://arxiv.org/pdf/2505.14106", "abs": "https://arxiv.org/abs/2505.14106", "authors": ["Li Li", "Peilin Cai", "Ryan A. Rossi", "Franck Dernoncourt", "Branislav Kveton", "Junda Wu", "Tong Yu", "Linxin Song", "Tiankai Yang", "Yuehan Qin", "Nesreen K. Ahmed", "Samyadeep Basu", "Subhojyoti Mukherjee", "Ruiyi Zhang", "Zhengmian Hu", "Bo Ni", "Yuxiao Zhou", "Zichao Wang", "Yue Huang", "Yu Wang", "Xiangliang Zhang", "Philip S. Yu", "Xiyang Hu", "Yue Zhao"], "title": "A Personalized Conversational Benchmark: Towards Simulating Personalized Conversations", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We present PersonaConvBench, a large-scale benchmark for evaluating\npersonalized reasoning and generation in multi-turn conversations with large\nlanguage models (LLMs). Unlike existing work that focuses on either\npersonalization or conversational structure in isolation, PersonaConvBench\nintegrates both, offering three core tasks: sentence classification, impact\nregression, and user-centric text generation across ten diverse Reddit-based\ndomains. This design enables systematic analysis of how personalized\nconversational context shapes LLM outputs in realistic multi-user scenarios. We\nbenchmark several commercial and open-source LLMs under a unified prompting\nsetup and observe that incorporating personalized history yields substantial\nperformance improvements, including a 198 percent relative gain over the best\nnon-conversational baseline in sentiment classification. By releasing\nPersonaConvBench with evaluations and code, we aim to support research on LLMs\nthat adapt to individual styles, track long-term context, and produce\ncontextually rich, engaging responses.", "AI": {"tldr": "PersonaConvBench\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u4e2a\u6027\u5316\u63a8\u7406\u548c\u751f\u6210\uff0c\u7ed3\u5408\u4e86\u4e2a\u6027\u5316\u548c\u5bf9\u8bdd\u7ed3\u6784\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u4efb\u52a1\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u7edf\u4e00\u63d0\u793a\u8bbe\u7f6e\u4e0bLLMs\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u5de5\u4f5c\u901a\u5e38\u5355\u72ec\u5173\u6ce8\u4e2a\u6027\u5316\u6216\u5bf9\u8bdd\u7ed3\u6784\uff0c\u800cPersonaConvBench\u65e8\u5728\u6574\u5408\u4e24\u8005\uff0c\u4ee5\u7cfb\u7edf\u5206\u6790\u4e2a\u6027\u5316\u5bf9\u8bdd\u4e0a\u4e0b\u6587\u5982\u4f55\u5f71\u54cdLLMs\u8f93\u51fa\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e09\u4e2a\u6838\u5fc3\u4efb\u52a1\uff08\u53e5\u5b50\u5206\u7c7b\u3001\u5f71\u54cd\u56de\u5f52\u548c\u7528\u6237\u4e2d\u5fc3\u6587\u672c\u751f\u6210\uff09\uff0c\u5e76\u5728\u5341\u4e2a\u591a\u6837\u5316Reddit\u9886\u57df\u4e2d\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5f15\u5165\u4e2a\u6027\u5316\u5386\u53f2\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u4f8b\u5982\u5728\u60c5\u611f\u5206\u7c7b\u4e2d\u76f8\u5bf9\u6700\u4f73\u975e\u5bf9\u8bdd\u57fa\u7ebf\u63d0\u5347198%\u3002", "conclusion": "PersonaConvBench\u7684\u53d1\u5e03\u65e8\u5728\u652f\u6301\u7814\u7a76LLMs\u5982\u4f55\u9002\u5e94\u4e2a\u4f53\u98ce\u683c\u3001\u8ddf\u8e2a\u957f\u671f\u4e0a\u4e0b\u6587\u5e76\u751f\u6210\u4e30\u5bcc\u7684\u54cd\u5e94\u3002"}}
{"id": "2505.14359", "pdf": "https://arxiv.org/pdf/2505.14359", "abs": "https://arxiv.org/abs/2505.14359", "authors": ["Ruoxin Chen", "Junwei Xi", "Zhiyuan Yan", "Ke-Yue Zhang", "Shuang Wu", "Jingyi Xie", "Xu Chen", "Lei Xu", "Isabel Guan", "Taiping Yao", "Shouhong Ding"], "title": "Dual Data Alignment Makes AI-Generated Image Detector Easier Generalizable", "categories": ["cs.CV"], "comment": "12 Pages, 9 figures", "summary": "Existing detectors are often trained on biased datasets, leading to the\npossibility of overfitting on non-causal image attributes that are spuriously\ncorrelated with real/synthetic labels. While these biased features enhance\nperformance on the training data, they result in substantial performance\ndegradation when applied to unbiased datasets. One common solution is to\nperform dataset alignment through generative reconstruction, matching the\nsemantic content between real and synthetic images. However, we revisit this\napproach and show that pixel-level alignment alone is insufficient. The\nreconstructed images still suffer from frequency-level misalignment, which can\nperpetuate spurious correlations. To illustrate, we observe that reconstruction\nmodels tend to restore the high-frequency details lost in real images (possibly\ndue to JPEG compression), inadvertently creating a frequency-level\nmisalignment, where synthetic images appear to have richer high-frequency\ncontent than real ones. This misalignment leads to models associating\nhigh-frequency features with synthetic labels, further reinforcing biased cues.\nTo resolve this, we propose Dual Data Alignment (DDA), which aligns both the\npixel and frequency domains. Moreover, we introduce two new test sets:\nDDA-COCO, containing DDA-aligned synthetic images for testing detector\nperformance on the most aligned dataset, and EvalGEN, featuring the latest\ngenerative models for assessing detectors under new generative architectures\nsuch as visual auto-regressive generators. Finally, our extensive evaluations\ndemonstrate that a detector trained exclusively on DDA-aligned MSCOCO could\nimprove across 8 diverse benchmarks by a non-trivial margin, showing a +7.2% on\nin-the-wild benchmarks, highlighting the improved generalizability of unbiased\ndetectors.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u6570\u636e\u5bf9\u9f50\uff08DDA\uff09\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u68c0\u6d4b\u5668\u5728\u50cf\u7d20\u548c\u9891\u7387\u57df\u4e0a\u7684\u5bf9\u9f50\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u65b0\u6d4b\u8bd5\u96c6\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u68c0\u6d4b\u5668\u5728\u8bad\u7ec3\u6570\u636e\u4e0a\u5bb9\u6613\u8fc7\u62df\u5408\u865a\u5047\u76f8\u5173\u7279\u5f81\uff0c\u5bfc\u81f4\u5728\u65e0\u504f\u6570\u636e\u96c6\u4e0a\u6027\u80fd\u4e0b\u964d\u3002\u4f20\u7edf\u65b9\u6cd5\u4ec5\u901a\u8fc7\u50cf\u7d20\u7ea7\u5bf9\u9f50\u65e0\u6cd5\u89e3\u51b3\u9891\u7387\u57df\u4e0a\u7684\u504f\u5dee\u95ee\u9898\u3002", "method": "\u63d0\u51faDDA\u65b9\u6cd5\uff0c\u540c\u65f6\u5bf9\u9f50\u50cf\u7d20\u548c\u9891\u7387\u57df\uff1b\u5e76\u5f15\u5165DDA-COCO\u548cEvalGEN\u4e24\u4e2a\u65b0\u6d4b\u8bd5\u96c6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4f7f\u7528DDA\u5bf9\u9f50\u7684MSCOCO\u8bad\u7ec3\u7684\u68c0\u6d4b\u5668\u57288\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6027\u80fd\u663e\u8457\u63d0\u5347\uff0c\u5c24\u5176\u5728\u91ce\u5916\u57fa\u51c6\u4e0a\u63d0\u9ad8\u4e867.2%\u3002", "conclusion": "DDA\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u68c0\u6d4b\u5668\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u9891\u7387\u57df\u504f\u5dee\u95ee\u9898\u3002"}}
{"id": "2505.14107", "pdf": "https://arxiv.org/pdf/2505.14107", "abs": "https://arxiv.org/abs/2505.14107", "authors": ["Yakun Zhu", "Zhongzhen Huang", "Linjie Mu", "Yutong Huang", "Wei Nie", "Shaoting Zhang", "Pengfei Liu", "Xiaofan Zhang"], "title": "DiagnosisArena: Benchmarking Diagnostic Reasoning for Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The emergence of groundbreaking large language models capable of performing\ncomplex reasoning tasks holds significant promise for addressing various\nscientific challenges, including those arising in complex clinical scenarios.\nTo enable their safe and effective deployment in real-world healthcare\nsettings, it is urgently necessary to benchmark the diagnostic capabilities of\ncurrent models systematically. Given the limitations of existing medical\nbenchmarks in evaluating advanced diagnostic reasoning, we present\nDiagnosisArena, a comprehensive and challenging benchmark designed to\nrigorously assess professional-level diagnostic competence. DiagnosisArena\nconsists of 1,113 pairs of segmented patient cases and corresponding diagnoses,\nspanning 28 medical specialties, deriving from clinical case reports published\nin 10 top-tier medical journals. The benchmark is developed through a\nmeticulous construction pipeline, involving multiple rounds of screening and\nreview by both AI systems and human experts, with thorough checks conducted to\nprevent data leakage. Our study reveals that even the most advanced reasoning\nmodels, o3-mini, o1, and DeepSeek-R1, achieve only 45.82%, 31.09%, and 17.79%\naccuracy, respectively. This finding highlights a significant generalization\nbottleneck in current large language models when faced with clinical diagnostic\nreasoning challenges. Through DiagnosisArena, we aim to drive further\nadvancements in AIs diagnostic reasoning capabilities, enabling more effective\nsolutions for real-world clinical diagnostic challenges. We provide the\nbenchmark and evaluation tools for further research and development\nhttps://github.com/SPIRAL-MED/DiagnosisArena.", "AI": {"tldr": "DiagnosisArena\u662f\u4e00\u4e2a\u9488\u5bf9\u4e34\u5e8a\u8bca\u65ad\u80fd\u529b\u8bbe\u8ba1\u7684\u7efc\u5408\u6027\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u533b\u7597\u573a\u666f\u4e2d\u7684\u8868\u73b0\uff0c\u7ed3\u679c\u663e\u793a\u5f53\u524d\u6a21\u578b\u8868\u73b0\u4e0d\u4f73\u3002", "motivation": "\u4e3a\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u771f\u5b9e\u533b\u7597\u73af\u5883\u4e2d\u7684\u8bca\u65ad\u80fd\u529b\uff0c\u5f25\u8865\u73b0\u6709\u533b\u7597\u57fa\u51c6\u7684\u4e0d\u8db3\u3002", "method": "\u5f00\u53d1\u4e86\u5305\u542b1,113\u5bf9\u75c5\u4f8b\u548c\u8bca\u65ad\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6db5\u76d628\u4e2a\u533b\u5b66\u4e13\u4e1a\uff0c\u5e76\u901a\u8fc7\u591a\u8f6e\u7b5b\u9009\u548c\u4e13\u5bb6\u5ba1\u6838\u786e\u4fdd\u6570\u636e\u8d28\u91cf\u3002", "result": "\u6700\u5148\u8fdb\u7684\u6a21\u578b\uff08o3-mini\u3001o1\u3001DeepSeek-R1\uff09\u51c6\u786e\u7387\u4ec5\u4e3a45.82%\u300131.09%\u548c17.79%\uff0c\u8868\u660e\u5176\u5728\u4e34\u5e8a\u8bca\u65ad\u63a8\u7406\u4e2d\u5b58\u5728\u663e\u8457\u74f6\u9888\u3002", "conclusion": "DiagnosisArena\u65e8\u5728\u63a8\u52a8AI\u8bca\u65ad\u63a8\u7406\u80fd\u529b\u7684\u8fdb\u6b65\uff0c\u5e76\u4e3a\u5b9e\u9645\u4e34\u5e8a\u6311\u6218\u63d0\u4f9b\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.14361", "pdf": "https://arxiv.org/pdf/2505.14361", "abs": "https://arxiv.org/abs/2505.14361", "authors": ["Xingxing Weng", "Chao Pang", "Gui-Song Xia"], "title": "Vision-Language Modeling Meets Remote Sensing: Models, Datasets and Perspectives", "categories": ["cs.CV"], "comment": "Accepted by IEEE Geoscience and Remote Sensing Magazine", "summary": "Vision-language modeling (VLM) aims to bridge the information gap between\nimages and natural language. Under the new paradigm of first pre-training on\nmassive image-text pairs and then fine-tuning on task-specific data, VLM in the\nremote sensing domain has made significant progress. The resulting models\nbenefit from the absorption of extensive general knowledge and demonstrate\nstrong performance across a variety of remote sensing data analysis tasks.\nMoreover, they are capable of interacting with users in a conversational\nmanner. In this paper, we aim to provide the remote sensing community with a\ntimely and comprehensive review of the developments in VLM using the two-stage\nparadigm. Specifically, we first cover a taxonomy of VLM in remote sensing:\ncontrastive learning, visual instruction tuning, and text-conditioned image\ngeneration. For each category, we detail the commonly used network architecture\nand pre-training objectives. Second, we conduct a thorough review of existing\nworks, examining foundation models and task-specific adaptation methods in\ncontrastive-based VLM, architectural upgrades, training strategies and model\ncapabilities in instruction-based VLM, as well as generative foundation models\nwith their representative downstream applications. Third, we summarize datasets\nused for VLM pre-training, fine-tuning, and evaluation, with an analysis of\ntheir construction methodologies (including image sources and caption\ngeneration) and key properties, such as scale and task adaptability. Finally,\nwe conclude this survey with insights and discussions on future research\ndirections: cross-modal representation alignment, vague requirement\ncomprehension, explanation-driven model reliability, continually scalable model\ncapabilities, and large-scale datasets featuring richer modalities and greater\nchallenges.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u9065\u611f\u9886\u57df\u4e2d\u89c6\u89c9\u8bed\u8a00\u5efa\u6a21\uff08VLM\uff09\u7684\u4e24\u9636\u6bb5\u8303\u5f0f\u8fdb\u5c55\uff0c\u5305\u62ec\u5206\u7c7b\u3001\u65b9\u6cd5\u3001\u6570\u636e\u96c6\u53ca\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u586b\u8865\u56fe\u50cf\u4e0e\u81ea\u7136\u8bed\u8a00\u4e4b\u95f4\u7684\u4fe1\u606f\u9e3f\u6c9f\uff0c\u63a8\u52a8\u9065\u611f\u9886\u57dfVLM\u7684\u53d1\u5c55\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u8303\u5f0f\uff08\u9884\u8bad\u7ec3+\u5fae\u8c03\uff09\uff0c\u5206\u7c7b\u4e3a\u5bf9\u6bd4\u5b66\u4e60\u3001\u89c6\u89c9\u6307\u4ee4\u8c03\u4f18\u548c\u6587\u672c\u6761\u4ef6\u56fe\u50cf\u751f\u6210\uff0c\u5e76\u8be6\u7ec6\u5206\u6790\u67b6\u6784\u4e0e\u76ee\u6807\u3002", "result": "VLM\u6a21\u578b\u5728\u9065\u611f\u6570\u636e\u5206\u6790\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u652f\u6301\u5bf9\u8bdd\u5f0f\u4ea4\u4e92\uff0c\u5e76\u603b\u7ed3\u4e86\u73b0\u6709\u5de5\u4f5c\u548c\u6570\u636e\u96c6\u3002", "conclusion": "\u672a\u6765\u7814\u7a76\u65b9\u5411\u5305\u62ec\u8de8\u6a21\u6001\u5bf9\u9f50\u3001\u6a21\u7cca\u9700\u6c42\u7406\u89e3\u3001\u6a21\u578b\u53ef\u9760\u6027\u3001\u53ef\u6269\u5c55\u80fd\u529b\u53ca\u591a\u6a21\u6001\u6570\u636e\u96c6\u3002"}}
{"id": "2505.14112", "pdf": "https://arxiv.org/pdf/2505.14112", "abs": "https://arxiv.org/abs/2505.14112", "authors": ["Tianle Gu", "Zongqi Wang", "Kexin Huang", "Yuanqi Yao", "Xiangliang Zhang", "Yujiu Yang", "Xiuying Chen"], "title": "Invisible Entropy: Towards Safe and Efficient Low-Entropy LLM Watermarking", "categories": ["cs.CL", "cs.CR"], "comment": null, "summary": "Logit-based LLM watermarking traces and verifies AI-generated content by\nmaintaining green and red token lists and increasing the likelihood of green\ntokens during generation. However, it fails in low-entropy scenarios, where\npredictable outputs make green token selection difficult without disrupting\nnatural text flow. Existing approaches address this by assuming access to the\noriginal LLM to calculate entropy and selectively watermark high-entropy\ntokens. However, these methods face two major challenges: (1) high\ncomputational costs and detection delays due to reliance on the original LLM,\nand (2) potential risks of model leakage. To address these limitations, we\npropose Invisible Entropy (IE), a watermarking paradigm designed to enhance\nboth safety and efficiency. Instead of relying on the original LLM, IE\nintroduces a lightweight feature extractor and an entropy tagger to predict\nwhether the entropy of the next token is high or low. Furthermore, based on\ntheoretical analysis, we develop a threshold navigator that adaptively sets\nentropy thresholds. It identifies a threshold where the watermark ratio\ndecreases as the green token count increases, enhancing the naturalness of the\nwatermarked text and improving detection robustness. Experiments on HumanEval\nand MBPP datasets demonstrate that IE reduces parameter size by 99\\% while\nachieving performance on par with state-of-the-art methods. Our work introduces\na safe and efficient paradigm for low-entropy watermarking.\nhttps://github.com/Carol-gutianle/IE\nhttps://huggingface.co/datasets/Carol0110/IE-Tagger", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aInvisible Entropy (IE)\u7684\u6c34\u5370\u65b9\u6cd5\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u7279\u5f81\u63d0\u53d6\u5668\u548c\u71b5\u6807\u8bb0\u5668\u9884\u6d4b\u9ad8\u4f4e\u71b5\u4ee4\u724c\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u4f4e\u71b5\u573a\u666f\u4e0b\u7684\u95ee\u9898\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u5b89\u5168\u6027\u548c\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8eLogit\u7684LLM\u6c34\u5370\u65b9\u6cd5\u5728\u4f4e\u71b5\u573a\u666f\u4e0b\u6548\u679c\u4e0d\u4f73\uff0c\u4e14\u4f9d\u8d56\u539f\u59cbLLM\u5bfc\u81f4\u9ad8\u8ba1\u7b97\u6210\u672c\u548c\u6f5c\u5728\u6a21\u578b\u6cc4\u6f0f\u98ce\u9669\u3002", "method": "\u5f15\u5165\u8f7b\u91cf\u7ea7\u7279\u5f81\u63d0\u53d6\u5668\u548c\u71b5\u6807\u8bb0\u5668\u9884\u6d4b\u4ee4\u724c\u71b5\uff0c\u5f00\u53d1\u81ea\u9002\u5e94\u9608\u503c\u5bfc\u822a\u5668\u4f18\u5316\u6c34\u5370\u6bd4\u4f8b\u548c\u6587\u672c\u81ea\u7136\u6027\u3002", "result": "\u5728HumanEval\u548cMBPP\u6570\u636e\u96c6\u4e0a\uff0cIE\u5c06\u53c2\u6570\u91cf\u51cf\u5c1199%\uff0c\u6027\u80fd\u4e0e\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u76f8\u5f53\u3002", "conclusion": "IE\u4e3a\u4f4e\u71b5\u6c34\u5370\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b89\u5168\u9ad8\u6548\u7684\u8303\u5f0f\u3002"}}
{"id": "2505.14362", "pdf": "https://arxiv.org/pdf/2505.14362", "abs": "https://arxiv.org/abs/2505.14362", "authors": ["Ziwei Zheng", "Michael Yang", "Jack Hong", "Chenxiao Zhao", "Guohai Xu", "Le Yang", "Chao Shen", "Xing Yu"], "title": "DeepEyes: Incentivizing \"Thinking with Images\" via Reinforcement Learning", "categories": ["cs.CV"], "comment": null, "summary": "Large Vision-Language Models (VLMs) have shown strong capabilities in\nmultimodal understanding and reasoning, yet they are primarily constrained by\ntext-based reasoning processes. However, achieving seamless integration of\nvisual and textual reasoning which mirrors human cognitive processes remains a\nsignificant challenge. In particular, effectively incorporating advanced visual\ninput processing into reasoning mechanisms is still an open question. Thus, in\nthis paper, we explore the interleaved multimodal reasoning paradigm and\nintroduce DeepEyes, a model with \"thinking with images\" capabilities\nincentivized through end-to-end reinforcement learning without the need for\ncold-start SFT. Notably, this ability emerges natively within the model itself,\nleveraging its inherent grounding ability as a tool instead of depending on\nseparate specialized models. Specifically, we propose a tool-use-oriented data\nselection mechanism and a reward strategy to encourage successful tool-assisted\nreasoning trajectories. DeepEyes achieves significant performance gains on\nfine-grained perception and reasoning benchmarks and also demonstrates\nimprovement in grounding, hallucination, and mathematical reasoning tasks.\nInterestingly, we observe the distinct evolution of tool-calling behavior from\ninitial exploration to efficient and accurate exploitation, and diverse\nthinking patterns that closely mirror human visual reasoning processes. Code is\navailable at https://github.com/Visual-Agent/DeepEyes.", "AI": {"tldr": "DeepEyes\u63a2\u7d22\u4e86\u89c6\u89c9\u4e0e\u6587\u672c\u4ea4\u7ec7\u7684\u591a\u6a21\u6001\u63a8\u7406\u8303\u5f0f\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0\u65e0\u9700\u51b7\u542f\u52a8SFT\u7684\u201c\u56fe\u50cf\u601d\u8003\u201d\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7ec6\u7c92\u5ea6\u611f\u77e5\u548c\u63a8\u7406\u4efb\u52a1\u8868\u73b0\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u89c6\u89c9\u4e0e\u6587\u672c\u63a8\u7406\u65e0\u7f1d\u6574\u5408\u4e0a\u7684\u4e0d\u8db3\uff0c\u6a21\u62df\u4eba\u7c7b\u8ba4\u77e5\u8fc7\u7a0b\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u5de5\u5177\u4f7f\u7528\u7684\u6570\u636e\u9009\u62e9\u673a\u5236\u548c\u5956\u52b1\u7b56\u7565\uff0c\u901a\u8fc7\u7aef\u5230\u7aef\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3DeepEyes\u6a21\u578b\u3002", "result": "\u5728\u7ec6\u7c92\u5ea6\u611f\u77e5\u3001\u63a8\u7406\u3001\u57fa\u7840\u4efb\u52a1\uff08\u5982\u5e7b\u89c9\u548c\u6570\u5b66\u63a8\u7406\uff09\u4e0a\u8868\u73b0\u663e\u8457\u63d0\u5347\uff0c\u5e76\u89c2\u5bdf\u5230\u5de5\u5177\u8c03\u7528\u884c\u4e3a\u7684\u8fdb\u5316\u3002", "conclusion": "DeepEyes\u5c55\u793a\u4e86\u89c6\u89c9\u4e0e\u6587\u672c\u63a8\u7406\u7684\u81ea\u7136\u6574\u5408\u80fd\u529b\uff0c\u4e3a\u591a\u6a21\u6001\u63a8\u7406\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.14116", "pdf": "https://arxiv.org/pdf/2505.14116", "abs": "https://arxiv.org/abs/2505.14116", "authors": ["Hongru Wang", "Deng Cai", "Wanjun Zhong", "Shijue Huang", "Jeff Z. Pan", "Zeming Liu", "Kam-Fai Wong"], "title": "Self-Reasoning Language Models: Unfold Hidden Reasoning Chains with Few Reasoning Catalyst", "categories": ["cs.CL"], "comment": null, "summary": "Inference-time scaling has attracted much attention which significantly\nenhance the performance of Large Language Models (LLMs) in complex reasoning\ntasks by increasing the length of Chain-of-Thought. These longer intermediate\nreasoning rationales embody various meta-reasoning skills in human cognition,\nsuch as reflection and decomposition, being difficult to create and acquire. In\nthis work, we introduce \\textit{Self-Reasoning Language Model} (SRLM), where\nthe model itself can synthesize longer CoT data and iteratively improve\nperformance through self-training. By incorporating a few demonstration\nexamples (i.e., 1,000 samples) on how to unfold hidden reasoning chains from\nexisting responses, which act as a reasoning catalyst, we demonstrate that SRLM\nnot only enhances the model's initial performance but also ensures more stable\nand consistent improvements in subsequent iterations. Our proposed SRLM\nachieves an average absolute improvement of more than $+2.5$ points across five\nreasoning tasks: MMLU, GSM8K, ARC-C, HellaSwag, and BBH on two backbone models.\nMoreover, it brings more improvements with more times of sampling during\ninference, such as absolute $+7.89$ average improvement with $64$ sampling\ntimes, revealing the in-depth, diverse and creative reasoning paths in SRLM\nagainst the strong baseline.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u63a8\u7406\u8bed\u8a00\u6a21\u578b\uff08SRLM\uff09\uff0c\u901a\u8fc7\u81ea\u6211\u8bad\u7ec3\u5408\u6210\u66f4\u957f\u7684\u601d\u7ef4\u94fe\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u957f\u601d\u7ef4\u94fe\u6570\u636e\u96be\u4ee5\u521b\u5efa\u548c\u83b7\u53d6\u7684\u95ee\u9898\uff0c\u540c\u65f6\u63d0\u5347\u6a21\u578b\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u5f15\u5165SRLM\uff0c\u5229\u7528\u5c11\u91cf\u793a\u8303\u6837\u672c\uff08\u59821,000\u4e2a\uff09\u4f5c\u4e3a\u63a8\u7406\u50ac\u5316\u5242\uff0c\u6a21\u578b\u901a\u8fc7\u81ea\u6211\u8bad\u7ec3\u5408\u6210\u66f4\u957f\u601d\u7ef4\u94fe\u6570\u636e\u5e76\u8fed\u4ee3\u63d0\u5347\u6027\u80fd\u3002", "result": "\u5728\u4e94\u4e2a\u63a8\u7406\u4efb\u52a1\uff08MMLU\u3001GSM8K\u3001ARC-C\u3001HellaSwag\u548cBBH\uff09\u4e0a\u5e73\u5747\u7edd\u5bf9\u63d0\u5347\u8d85\u8fc72.5\u5206\uff0c\u91c7\u6837\u6b21\u6570\u589e\u52a0\u65f6\u63d0\u5347\u66f4\u663e\u8457\uff08\u598264\u6b21\u91c7\u6837\u65f6\u5e73\u5747\u63d0\u5347+7.89\u5206\uff09\u3002", "conclusion": "SRLM\u901a\u8fc7\u81ea\u6211\u8bad\u7ec3\u751f\u6210\u591a\u6837\u4e14\u6df1\u5165\u7684\u63a8\u7406\u8def\u5f84\uff0c\u663e\u8457\u63d0\u5347\u63a8\u7406\u4efb\u52a1\u6027\u80fd\uff0c\u4e14\u6548\u679c\u968f\u91c7\u6837\u6b21\u6570\u589e\u52a0\u800c\u589e\u5f3a\u3002"}}
{"id": "2505.14404", "pdf": "https://arxiv.org/pdf/2505.14404", "abs": "https://arxiv.org/abs/2505.14404", "authors": ["Xuecheng Wu", "Jiaxing Liu", "Danlei Huang", "Xiaoyu Li", "Yifan Wang", "Chen Chen", "Liya Ma", "Xuezhi Cao", "Junxiao Xue"], "title": "ViC-Bench: Benchmarking Visual-Interleaved Chain-of-Thought Capability in MLLMs with Free-Style Intermediate State Representations", "categories": ["cs.CV"], "comment": null, "summary": "Visual-Interleaved Chain-of-Thought (VI-CoT) enables MLLMs to continually\nupdate their understanding and decisions based on step-wise intermediate visual\nstates (IVS), much like a human would, which demonstrates impressive success in\nvarious tasks, thereby leading to emerged advancements in related benchmarks.\nDespite promising progress, current benchmarks provide models with relatively\nfixed IVS, rather than free-style IVS, whch might forcibly distort the original\nthinking trajectories, failing to evaluate their intrinsic reasoning\ncapabilities. More importantly, existing benchmarks neglect to systematically\nexplore the impact factors that IVS would impart to untamed reasoning\nperformance. To tackle above gaps, we introduce a specialized benchmark termed\nViC-Bench, consisting of four representive tasks: maze navigation, jigsaw\npuzzle, embodied long-horizon planning, and complex counting, where each task\nhas dedicated free-style IVS generation pipeline supporting function calls. To\nsystematically examine VI-CoT capability, we propose a thorough evaluation\nsuite incorporating a progressive three-stage strategy with targeted new\nmetrics. Besides, we establish Incremental Prompting Information Injection\n(IPII) strategy to ablatively explore the prompting factors for VI-CoT. We\nextensively conduct evaluations for 18 advanced MLLMs, revealing key insights\ninto their VI-CoT capability. Our proposed benchmark is publicly open at\nHuggingface.", "AI": {"tldr": "VI-CoT\u901a\u8fc7\u9010\u6b65\u89c6\u89c9\u72b6\u6001\u66f4\u65b0\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u56fa\u5b9a\u89c6\u89c9\u72b6\u6001\u9650\u5236\u4e86\u8bc4\u4f30\u3002\u4e3a\u6b64\uff0c\u63d0\u51faViC-Bench\u57fa\u51c6\u548cIPII\u7b56\u7565\uff0c\u7cfb\u7edf\u6027\u8bc4\u4f3018\u79cdMLLMs\u7684VI-CoT\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u56fa\u5b9a\u89c6\u89c9\u72b6\u6001\u53ef\u80fd\u626d\u66f2\u6a21\u578b\u63a8\u7406\u8f68\u8ff9\uff0c\u4e14\u672a\u7cfb\u7edf\u63a2\u7d22\u89c6\u89c9\u72b6\u6001\u5bf9\u63a8\u7406\u80fd\u529b\u7684\u5f71\u54cd\u3002", "method": "\u63d0\u51faViC-Bench\u57fa\u51c6\uff0c\u5305\u542b\u56db\u9879\u4efb\u52a1\uff0c\u652f\u6301\u81ea\u7531\u89c6\u89c9\u72b6\u6001\u751f\u6210\uff1b\u91c7\u7528\u4e09\u9636\u6bb5\u8bc4\u4f30\u7b56\u7565\u548cIPII\u63d0\u793a\u7b56\u7565\u3002", "result": "\u8bc4\u4f30\u4e8618\u79cdMLLMs\uff0c\u63ed\u793a\u4e86\u5176VI-CoT\u80fd\u529b\u7684\u5173\u952e\u6d1e\u5bdf\u3002", "conclusion": "ViC-Bench\u4e3a\u7cfb\u7edf\u6027\u8bc4\u4f30VI-CoT\u80fd\u529b\u63d0\u4f9b\u4e86\u65b0\u57fa\u51c6\uff0c\u5e76\u516c\u5f00\u53ef\u7528\u3002"}}
{"id": "2505.14130", "pdf": "https://arxiv.org/pdf/2505.14130", "abs": "https://arxiv.org/abs/2505.14130", "authors": ["Filip Mileti\u0107", "Aaron Schmid", "Sabine Schulte im Walde"], "title": "Probing BERT for German Compound Semantics", "categories": ["cs.CL"], "comment": "Accepted to SwissText 2025", "summary": "This paper investigates the extent to which pretrained German BERT encodes\nknowledge of noun compound semantics. We comprehensively vary combinations of\ntarget tokens, layers, and cased vs. uncased models, and evaluate them by\npredicting the compositionality of 868 gold standard compounds. Looking at\nrepresentational patterns within the transformer architecture, we observe\ntrends comparable to equivalent prior work on English, with compositionality\ninformation most easily recoverable in the early layers. However, our strongest\nresults clearly lag behind those reported for English, suggesting an inherently\nmore difficult task in German. This may be due to the higher productivity of\ncompounding in German than in English and the associated increase in\nconstituent-level ambiguity, including in our target compound set.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5fb7\u8bedBERT\u9884\u8bad\u7ec3\u6a21\u578b\u5bf9\u540d\u8bcd\u590d\u5408\u8bcd\u8bed\u4e49\u7684\u7f16\u7801\u80fd\u529b\uff0c\u53d1\u73b0\u5176\u8868\u73b0\u4e0d\u53ca\u82f1\u8bedBERT\uff0c\u53ef\u80fd\u4e0e\u5fb7\u8bed\u590d\u5408\u8bcd\u7684\u66f4\u9ad8\u751f\u4ea7\u529b\u548c\u6b67\u4e49\u6027\u6709\u5173\u3002", "motivation": "\u63a2\u7a76\u5fb7\u8bedBERT\u6a21\u578b\u662f\u5426\u80fd\u591f\u7f16\u7801\u540d\u8bcd\u590d\u5408\u8bcd\u7684\u8bed\u4e49\u4fe1\u606f\uff0c\u5e76\u4e0e\u82f1\u8bedBERT\u7684\u8868\u73b0\u8fdb\u884c\u5bf9\u6bd4\u3002", "method": "\u901a\u8fc7\u8c03\u6574\u76ee\u6807\u8bcd\u3001\u5c42\u6570\u53ca\u6a21\u578b\u5927\u5c0f\u5199\u8bbe\u7f6e\uff0c\u8bc4\u4f30\u6a21\u578b\u5bf9868\u4e2a\u6807\u51c6\u590d\u5408\u8bcd\u7ec4\u5408\u6027\u7684\u9884\u6d4b\u80fd\u529b\u3002", "result": "\u5fb7\u8bedBERT\u7684\u8868\u73b0\u660e\u663e\u843d\u540e\u4e8e\u82f1\u8bedBERT\uff0c\u53ef\u80fd\u4e0e\u5fb7\u8bed\u590d\u5408\u8bcd\u7684\u9ad8\u751f\u4ea7\u529b\u548c\u6b67\u4e49\u6027\u6709\u5173\u3002", "conclusion": "\u5fb7\u8bed\u540d\u8bcd\u590d\u5408\u8bcd\u8bed\u4e49\u7f16\u7801\u4efb\u52a1\u66f4\u5177\u6311\u6218\u6027\uff0c\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u6539\u8fdb\u3002"}}
{"id": "2505.14405", "pdf": "https://arxiv.org/pdf/2505.14405", "abs": "https://arxiv.org/abs/2505.14405", "authors": ["Jiafeng Liang", "Shixin Jiang", "Xuan Dong", "Ning Wang", "Zheng Chu", "Hui Su", "Jinlan Fu", "Ming Liu", "See-Kiong Ng", "Bing Qin"], "title": "Investigating and Enhancing the Robustness of Large Multimodal Models Against Temporal Inconsistency", "categories": ["cs.CV"], "comment": null, "summary": "Large Multimodal Models (LMMs) have recently demonstrated impressive\nperformance on general video comprehension benchmarks. Nevertheless, for\nbroader applications, the robustness of their temporal analysis capability\nneeds to be thoroughly investigated yet predominantly ignored. Motivated by\nthis, we propose a novel temporal robustness benchmark (TemRobBench), which\nintroduces temporal inconsistency perturbations separately at the visual and\ntextual modalities to assess the robustness of models. We evaluate 16\nmainstream LMMs and find that they exhibit over-reliance on prior knowledge and\ntextual context in adversarial environments, while ignoring the actual temporal\ndynamics in the video. To mitigate this issue, we design panoramic direct\npreference optimization (PanoDPO), which encourages LMMs to incorporate both\nvisual and linguistic feature preferences simultaneously. Experimental results\nshow that PanoDPO can effectively enhance the model's robustness and\nreliability in temporal analysis.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65f6\u95f4\u9c81\u68d2\u6027\u57fa\u51c6\uff08TemRobBench\uff09\uff0c\u8bc4\u4f30\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff08LMMs\uff09\u5728\u65f6\u95f4\u5206\u6790\u4e2d\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u5168\u666f\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff08PanoDPO\uff09\u65b9\u6cd5\u4ee5\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709LMMs\u5728\u89c6\u9891\u7406\u89e3\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u65f6\u95f4\u5206\u6790\u80fd\u529b\u7684\u9c81\u68d2\u6027\u5c1a\u672a\u5145\u5206\u7814\u7a76\uff0c\u5c24\u5176\u662f\u5728\u5bf9\u6297\u73af\u5883\u4e2d\u5bf9\u5148\u9a8c\u77e5\u8bc6\u548c\u6587\u672c\u4e0a\u4e0b\u6587\u7684\u8fc7\u5ea6\u4f9d\u8d56\u95ee\u9898\u3002", "method": "\u63d0\u51faTemRobBench\u57fa\u51c6\uff0c\u5f15\u5165\u89c6\u89c9\u548c\u6587\u672c\u6a21\u6001\u7684\u65f6\u95f4\u4e0d\u4e00\u81f4\u6270\u52a8\uff1b\u8bbe\u8ba1PanoDPO\u65b9\u6cd5\uff0c\u9f13\u52b1\u6a21\u578b\u540c\u65f6\u7ed3\u5408\u89c6\u89c9\u548c\u8bed\u8a00\u7279\u5f81\u504f\u597d\u3002", "result": "\u8bc4\u4f3016\u79cd\u4e3b\u6d41LMMs\uff0c\u53d1\u73b0\u5176\u5728\u5bf9\u6297\u73af\u5883\u4e2d\u8868\u73b0\u4e0d\u4f73\uff1bPanoDPO\u80fd\u663e\u8457\u63d0\u5347\u6a21\u578b\u7684\u65f6\u95f4\u5206\u6790\u9c81\u68d2\u6027\u548c\u53ef\u9760\u6027\u3002", "conclusion": "PanoDPO\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86LMMs\u5728\u65f6\u95f4\u5206\u6790\u4e2d\u7684\u9c81\u68d2\u6027\u95ee\u9898\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2505.14131", "pdf": "https://arxiv.org/pdf/2505.14131", "abs": "https://arxiv.org/abs/2505.14131", "authors": ["Wei Zhou", "Mohsen Mesgar", "Heike Adel", "Annemarie Friedrich"], "title": "Texts or Images? A Fine-grained Analysis on the Effectiveness of Input Representations and Models for Table Question Answering", "categories": ["cs.CL"], "comment": "Accepted at ACL25 (Findings)", "summary": "In table question answering (TQA), tables are encoded as either texts or\nimages. Prior work suggests that passing images of tables to multi-modal large\nlanguage models (MLLMs) performs comparably to or even better than using\ntextual input with large language models (LLMs). However, the lack of\ncontrolled setups limits fine-grained distinctions between these approaches. In\nthis paper, we conduct the first controlled study on the effectiveness of\nseveral combinations of table representations and models from two perspectives:\nquestion complexity and table size. We build a new benchmark based on existing\nTQA datasets. In a systematic analysis of seven pairs of MLLMs and LLMs, we\nfind that the best combination of table representation and model varies across\nsetups. We propose FRES, a method selecting table representations dynamically,\nand observe a 10% average performance improvement compared to using both\nrepresentations indiscriminately.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u8868\u683c\u95ee\u7b54\uff08TQA\uff09\u4e2d\u8868\u683c\u7f16\u7801\u4e3a\u6587\u672c\u6216\u56fe\u50cf\u7684\u6548\u679c\uff0c\u901a\u8fc7\u63a7\u5236\u5b9e\u9a8c\u6bd4\u8f83\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u548c\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u8868\u73b0\uff0c\u5e76\u63d0\u51fa\u52a8\u6001\u9009\u62e9\u8868\u683c\u8868\u793a\u7684\u65b9\u6cd5FRES\uff0c\u6027\u80fd\u63d0\u534710%\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u5bf9\u8868\u683c\u8868\u793a\uff08\u6587\u672c\u6216\u56fe\u50cf\uff09\u4e0e\u6a21\u578b\uff08MLLMs\u6216LLMs\uff09\u7ec4\u5408\u6548\u679c\u7684\u7ec6\u7c92\u5ea6\u6bd4\u8f83\uff0c\u9650\u5236\u4e86\u5bf9\u6b64\u7c7b\u65b9\u6cd5\u7684\u6df1\u5165\u7406\u89e3\u3002", "method": "\u6784\u5efa\u65b0\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u7cfb\u7edf\u5206\u6790\u4e03\u5bf9MLLMs\u548cLLMs\u7ec4\u5408\u5728\u4e0d\u540c\u95ee\u9898\u590d\u6742\u5ea6\u548c\u8868\u683c\u5927\u5c0f\u4e0b\u7684\u8868\u73b0\uff0c\u5e76\u63d0\u51fa\u52a8\u6001\u9009\u62e9\u8868\u793a\u7684\u65b9\u6cd5FRES\u3002", "result": "\u6700\u4f73\u8868\u683c\u8868\u793a\u4e0e\u6a21\u578b\u7ec4\u5408\u56e0\u573a\u666f\u800c\u5f02\uff0cFRES\u65b9\u6cd5\u5e73\u5747\u6027\u80fd\u63d0\u534710%\u3002", "conclusion": "\u52a8\u6001\u9009\u62e9\u8868\u683c\u8868\u793a\u7684\u65b9\u6cd5FRES\u663e\u8457\u63d0\u5347TQA\u6027\u80fd\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2505.14414", "pdf": "https://arxiv.org/pdf/2505.14414", "abs": "https://arxiv.org/abs/2505.14414", "authors": ["Chengtang Yao", "Lidong Yu", "Zhidan Liu", "Jiaxi Zeng", "Yuwei Wu", "Yunde Jia"], "title": "Diving into the Fusion of Monocular Priors for Generalized Stereo Matching", "categories": ["cs.CV"], "comment": "Code:\n  https://github.com/YaoChengTang/Diving-into-the-Fusion-of-Monocular-Priors-for-Generalized-Stereo-Matching", "summary": "The matching formulation makes it naturally hard for the stereo matching to\nhandle ill-posed regions like occlusions and non-Lambertian surfaces. Fusing\nmonocular priors has been proven helpful for ill-posed matching, but the biased\nmonocular prior learned from small stereo datasets constrains the\ngeneralization. Recently, stereo matching has progressed by leveraging the\nunbiased monocular prior from the vision foundation model (VFM) to improve the\ngeneralization in ill-posed regions. We dive into the fusion process and\nobserve three main problems limiting the fusion of the VFM monocular prior. The\nfirst problem is the misalignment between affine-invariant relative monocular\ndepth and absolute depth of disparity. Besides, when we use the monocular\nfeature in an iterative update structure, the over-confidence in the disparity\nupdate leads to local optima results. A direct fusion of a monocular depth map\ncould alleviate the local optima problem, but noisy disparity results computed\nat the first several iterations will misguide the fusion. In this paper, we\npropose a binary local ordering map to guide the fusion, which converts the\ndepth map into a binary relative format, unifying the relative and absolute\ndepth representation. The computed local ordering map is also used to re-weight\nthe initial disparity update, resolving the local optima and noisy problem. In\naddition, we formulate the final direct fusion of monocular depth to the\ndisparity as a registration problem, where a pixel-wise linear regression\nmodule can globally and adaptively align them. Our method fully exploits the\nmonocular prior to support stereo matching results effectively and efficiently.\nWe significantly improve the performance from the experiments when generalizing\nfrom SceneFlow to Middlebury and Booster datasets while barely reducing the\nefficiency.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff08VFM\uff09\u7684\u5355\u76ee\u5148\u9a8c\u6765\u6539\u8fdb\u7acb\u4f53\u5339\u914d\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e8c\u8fdb\u5236\u5c40\u90e8\u6392\u5e8f\u56fe\u548c\u50cf\u7d20\u7ea7\u7ebf\u6027\u56de\u5f52\u6a21\u5757\u89e3\u51b3\u878d\u5408\u4e2d\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u7acb\u4f53\u5339\u914d\u5728\u906e\u6321\u548c\u975e\u6717\u4f2f\u8868\u9762\u7b49\u4e0d\u9002\u5b9a\u533a\u57df\u8868\u73b0\u4e0d\u4f73\uff0c\u800c\u73b0\u6709\u7684\u5355\u76ee\u5148\u9a8c\u56e0\u6570\u636e\u504f\u5dee\u9650\u5236\u4e86\u6cdb\u5316\u80fd\u529b\u3002\u5229\u7528VFM\u7684\u65e0\u504f\u5355\u76ee\u5148\u9a8c\u53ef\u4ee5\u6539\u8fdb\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e8c\u8fdb\u5236\u5c40\u90e8\u6392\u5e8f\u56fe\u7edf\u4e00\u76f8\u5bf9\u548c\u7edd\u5bf9\u6df1\u5ea6\u8868\u793a\uff0c\u5e76\u901a\u8fc7\u50cf\u7d20\u7ea7\u7ebf\u6027\u56de\u5f52\u6a21\u5757\u5168\u5c40\u81ea\u9002\u5e94\u5bf9\u9f50\u5355\u76ee\u6df1\u5ea6\u4e0e\u89c6\u5dee\u3002", "result": "\u5728\u4eceSceneFlow\u5230Middlebury\u548cBooster\u6570\u636e\u96c6\u7684\u6cdb\u5316\u5b9e\u9a8c\u4e2d\uff0c\u6027\u80fd\u663e\u8457\u63d0\u5347\uff0c\u4e14\u6548\u7387\u51e0\u4e4e\u4e0d\u53d7\u5f71\u54cd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5145\u5206\u5229\u7528\u5355\u76ee\u5148\u9a8c\uff0c\u6709\u6548\u4e14\u9ad8\u6548\u5730\u652f\u6301\u7acb\u4f53\u5339\u914d\u7ed3\u679c\u3002"}}
{"id": "2505.14149", "pdf": "https://arxiv.org/pdf/2505.14149", "abs": "https://arxiv.org/abs/2505.14149", "authors": ["Chengzhi Zhang", "Xinyi Yan", "Lei Zhao", "Yingyi Zhang"], "title": "Enhancing Keyphrase Extraction from Academic Articles Using Section Structure Information", "categories": ["cs.CL", "cs.DL", "cs.IR"], "comment": null, "summary": "The exponential increase in academic papers has significantly increased the\ntime required for researchers to access relevant literature. Keyphrase\nExtraction (KPE) offers a solution to this situation by enabling researchers to\nefficiently retrieve relevant literature. The current study on KPE from\nacademic articles aims to improve the performance of extraction models through\ninnovative approaches using Title and Abstract as input corpora. However, the\nsemantic richness of keywords is significantly constrained by the length of the\nabstract. While full-text-based KPE can address this issue, it simultaneously\nintroduces noise, which significantly diminishes KPE performance. To address\nthis issue, this paper utilized the structural features and section texts\nobtained from the section structure information of academic articles to extract\nkeyphrase from academic papers. The approach consists of two main parts: (1)\nexploring the effect of seven structural features on KPE models, and (2)\nintegrating the extraction results from all section texts used as input corpora\nfor KPE models via a keyphrase integration algorithm to obtain the keyphrase\nintegration result. Furthermore, this paper also examined the effect of the\nclassification quality of section structure on the KPE performance. The results\nshow that incorporating structural features improves KPE performance, though\ndifferent features have varying effects on model efficacy. The keyphrase\nintegration approach yields the best performance, and the classification\nquality of section structure can affect KPE performance. These findings\nindicate that using the section structure information of academic articles\ncontributes to effective KPE from academic articles. The code and dataset\nsupporting this study are available at https://github.com/yan-xinyi/SSB_KPE.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b66\u672f\u6587\u7ae0\u7ed3\u6784\u7279\u5f81\u7684\u5173\u952e\u8bcd\u63d0\u53d6\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u7ae0\u8282\u7ed3\u6784\u548c\u6587\u672c\u4fe1\u606f\u63d0\u5347\u63d0\u53d6\u6027\u80fd\u3002", "motivation": "\u5b66\u672f\u8bba\u6587\u6570\u91cf\u6fc0\u589e\u5bfc\u81f4\u7814\u7a76\u8005\u68c0\u7d22\u76f8\u5173\u6587\u732e\u65f6\u95f4\u589e\u52a0\uff0c\u73b0\u6709\u57fa\u4e8e\u6807\u9898\u548c\u6458\u8981\u7684\u5173\u952e\u8bcd\u63d0\u53d6\u65b9\u6cd5\u53d7\u9650\u4e8e\u6458\u8981\u957f\u5ea6\uff0c\u800c\u5168\u6587\u63d0\u53d6\u5219\u5f15\u5165\u566a\u58f0\u3002", "method": "\u7814\u7a76\u5206\u4e3a\u4e24\u90e8\u5206\uff1a(1) \u63a2\u7d22\u4e03\u79cd\u7ed3\u6784\u7279\u5f81\u5bf9\u5173\u952e\u8bcd\u63d0\u53d6\u6a21\u578b\u7684\u5f71\u54cd\uff1b(2) \u901a\u8fc7\u5173\u952e\u8bcd\u6574\u5408\u7b97\u6cd5\u878d\u5408\u5404\u7ae0\u8282\u6587\u672c\u7684\u63d0\u53d6\u7ed3\u679c\u3002\u540c\u65f6\u5206\u6790\u4e86\u7ae0\u8282\u7ed3\u6784\u5206\u7c7b\u8d28\u91cf\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u7ed3\u6784\u7279\u5f81\u7684\u5f15\u5165\u63d0\u5347\u4e86\u5173\u952e\u8bcd\u63d0\u53d6\u6027\u80fd\uff0c\u4e0d\u540c\u7279\u5f81\u5bf9\u6a21\u578b\u6548\u679c\u5f71\u54cd\u5404\u5f02\u3002\u5173\u952e\u8bcd\u6574\u5408\u65b9\u6cd5\u8868\u73b0\u6700\u4f73\uff0c\u7ae0\u8282\u7ed3\u6784\u5206\u7c7b\u8d28\u91cf\u4e5f\u4f1a\u5f71\u54cd\u6027\u80fd\u3002", "conclusion": "\u5229\u7528\u5b66\u672f\u6587\u7ae0\u7684\u7ae0\u8282\u7ed3\u6784\u4fe1\u606f\u53ef\u6709\u6548\u63d0\u5347\u5173\u952e\u8bcd\u63d0\u53d6\u6548\u679c\uff0c\u76f8\u5173\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5df2\u5f00\u6e90\u3002"}}
{"id": "2505.14454", "pdf": "https://arxiv.org/pdf/2505.14454", "abs": "https://arxiv.org/abs/2505.14454", "authors": ["Xuyang Liu", "Yiyu Wang", "Junpeng Ma", "Linfeng Zhang"], "title": "Video Compression Commander: Plug-and-Play Inference Acceleration for Video Large Language Models", "categories": ["cs.CV"], "comment": "Our code is available at https://github.com/xuyang-liu16/VidCom2", "summary": "Video large language models (VideoLLM) excel at video understanding, but face\nefficiency challenges due to the quadratic complexity of abundant visual\ntokens. Our systematic analysis of token compression methods for VideoLLMs\nreveals two critical issues: (i) overlooking distinctive visual signals across\nframes, leading to information loss; (ii) suffering from implementation\nconstraints, causing incompatibility with modern architectures or efficient\noperators. To address these challenges, we distill three design principles for\nVideoLLM token compression and propose a plug-and-play inference acceleration\nframework \"Video Compression Commander\" (VidCom2). By quantifying each frame's\nuniqueness, VidCom2 adaptively adjusts compression intensity across frames,\neffectively preserving essential information while reducing redundancy in video\nsequences. Extensive experiments across various VideoLLMs and benchmarks\ndemonstrate the superior performance and efficiency of our VidCom2. With only\n25% visual tokens, VidCom2 achieves 99.6% of the original performance on\nLLaVA-OV while reducing 70.8% of the LLM generation latency. Notably, our Frame\nCompression Adjustment strategy is compatible with other token compression\nmethods to further improve their performance. Our code is available at\nhttps://github.com/xuyang-liu16/VidCom2.", "AI": {"tldr": "VideoLLM\u9762\u4e34\u89c6\u89c9\u4ee4\u724c\u6548\u7387\u95ee\u9898\uff0cVidCom2\u6846\u67b6\u901a\u8fc7\u81ea\u9002\u5e94\u538b\u7f29\u89e3\u51b3\u4fe1\u606f\u4e22\u5931\u548c\u517c\u5bb9\u6027\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\uff08VideoLLM\uff09\u56e0\u89c6\u89c9\u4ee4\u724c\u7684\u4e8c\u6b21\u590d\u6742\u5ea6\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\uff0c\u73b0\u6709\u538b\u7f29\u65b9\u6cd5\u5b58\u5728\u4fe1\u606f\u4e22\u5931\u548c\u67b6\u6784\u517c\u5bb9\u6027\u95ee\u9898\u3002", "method": "\u63d0\u51faVidCom2\u6846\u67b6\uff0c\u901a\u8fc7\u91cf\u5316\u5e27\u72ec\u7279\u6027\u81ea\u9002\u5e94\u8c03\u6574\u538b\u7f29\u5f3a\u5ea6\uff0c\u4fdd\u7559\u5173\u952e\u4fe1\u606f\u5e76\u51cf\u5c11\u5197\u4f59\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cVidCom2\u4ec5\u752825%\u4ee4\u724c\u5373\u53ef\u8fbe\u523099.6%\u539f\u59cb\u6027\u80fd\uff0c\u5e76\u51cf\u5c1170.8%\u751f\u6210\u5ef6\u8fdf\u3002", "conclusion": "VidCom2\u9ad8\u6548\u4e14\u517c\u5bb9\u73b0\u6709\u65b9\u6cd5\uff0c\u4e3aVideoLLM\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u7684\u52a0\u901f\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.14157", "pdf": "https://arxiv.org/pdf/2505.14157", "abs": "https://arxiv.org/abs/2505.14157", "authors": ["Pittawat Taveekitworachai", "Potsawee Manakul", "Sarana Nutanong", "Kunat Pipatanakul"], "title": "Prior Prompt Engineering for Reinforcement Fine-Tuning", "categories": ["cs.CL", "cs.AI"], "comment": "25 pages, 42 figures", "summary": "This paper investigates prior prompt engineering (pPE) in the context of\nreinforcement fine-tuning (RFT), where language models (LMs) are incentivized\nto exhibit behaviors that maximize performance through reward signals. While\nexisting RFT research has primarily focused on algorithms, reward shaping, and\ndata curation, the design of the prior prompt--the instructions prepended to\nqueries during training to elicit behaviors such as step-by-step\nreasoning--remains underexplored. We investigate whether different pPE\napproaches can guide LMs to internalize distinct behaviors after RFT. Inspired\nby inference-time prompt engineering (iPE), we translate five representative\niPE strategies--reasoning, planning, code-based reasoning, knowledge recall,\nand null-example utilization--into corresponding pPE approaches. We experiment\nwith Qwen2.5-7B using each of the pPE approaches, then evaluate performance on\nin-domain and out-of-domain benchmarks (e.g., AIME2024, HumanEval+, and\nGPQA-Diamond). Our results show that all pPE-trained models surpass their\niPE-prompted counterparts, with the null-example pPE approach achieving the\nlargest average performance gain and the highest improvement on AIME2024 and\nGPQA-Diamond, surpassing the commonly used reasoning approach. Furthermore, by\nadapting a behavior-classification framework, we demonstrate that different pPE\nstrategies instill distinct behavioral styles in the resulting models. These\nfindings position pPE as a powerful yet understudied axis for RFT.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728\u5f3a\u5316\u5fae\u8c03\uff08RFT\uff09\u4e2d\u7684\u5148\u9a8c\u63d0\u793a\u5de5\u7a0b\uff08pPE\uff09\uff0c\u63a2\u8ba8\u4e86\u4e0d\u540cpPE\u65b9\u6cd5\u5982\u4f55\u5f15\u5bfc\u8bed\u8a00\u6a21\u578b\uff08LM\uff09\u5185\u5316\u7279\u5b9a\u884c\u4e3a\uff0c\u5e76\u53d1\u73b0pPE\u8bad\u7ec3\u7684\u6a21\u578b\u4f18\u4e8e\u63a8\u7406\u65f6\u63d0\u793a\u5de5\u7a0b\uff08iPE\uff09\u7684\u6a21\u578b\u3002", "motivation": "\u73b0\u6709RFT\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u7b97\u6cd5\u3001\u5956\u52b1\u5851\u9020\u548c\u6570\u636e\u7ba1\u7406\u4e0a\uff0c\u800c\u5148\u9a8c\u63d0\u793a\u8bbe\u8ba1\u7684\u4f5c\u7528\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u5c06\u4e94\u79cdiPE\u7b56\u7565\uff08\u5982\u63a8\u7406\u3001\u89c4\u5212\u3001\u4ee3\u7801\u63a8\u7406\u7b49\uff09\u8f6c\u5316\u4e3apPE\u65b9\u6cd5\uff0c\u5e76\u5728Qwen2.5-7B\u6a21\u578b\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u8bc4\u4f30\u5176\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u6240\u6709pPE\u8bad\u7ec3\u7684\u6a21\u578b\u5747\u4f18\u4e8eiPE\u63d0\u793a\u7684\u6a21\u578b\uff0c\u5176\u4e2dnull-example pPE\u65b9\u6cd5\u8868\u73b0\u6700\u4f73\uff0c\u5c24\u5176\u5728AIME2024\u548cGPQA-Diamond\u4e0a\u63d0\u5347\u663e\u8457\u3002", "conclusion": "pPE\u662fRFT\u4e2d\u4e00\u4e2a\u5f3a\u5927\u4f46\u672a\u88ab\u5145\u5206\u7814\u7a76\u7684\u7ef4\u5ea6\uff0c\u4e0d\u540cpPE\u7b56\u7565\u80fd\u5851\u9020\u6a21\u578b\u7684\u4e0d\u540c\u884c\u4e3a\u98ce\u683c\u3002"}}
{"id": "2505.14460", "pdf": "https://arxiv.org/pdf/2505.14460", "abs": "https://arxiv.org/abs/2505.14460", "authors": ["Tianhe Wu", "Jian Zou", "Jie Liang", "Lei Zhang", "Kede Ma"], "title": "VisualQuality-R1: Reasoning-Induced Image Quality Assessment via Reinforcement Learning to Rank", "categories": ["cs.CV"], "comment": null, "summary": "DeepSeek-R1 has demonstrated remarkable effectiveness in incentivizing\nreasoning and generalization capabilities of large language models (LLMs)\nthrough reinforcement learning. Nevertheless, the potential of\nreasoning-induced computational modeling has not been thoroughly explored in\nthe context of image quality assessment (IQA), a task critically dependent on\nvisual reasoning. In this paper, we introduce VisualQuality-R1, a\nreasoning-induced no-reference IQA (NR-IQA) model, and we train it with\nreinforcement learning to rank, a learning algorithm tailored to the\nintrinsically relative nature of visual quality. Specifically, for a pair of\nimages, we employ group relative policy optimization to generate multiple\nquality scores for each image. These estimates are then used to compute\ncomparative probabilities of one image having higher quality than the other\nunder the Thurstone model. Rewards for each quality estimate are defined using\ncontinuous fidelity measures rather than discretized binary labels. Extensive\nexperiments show that the proposed VisualQuality-R1 consistently outperforms\ndiscriminative deep learning-based NR-IQA models as well as a recent\nreasoning-induced quality regression method. Moreover, VisualQuality-R1 is\ncapable of generating contextually rich, human-aligned quality descriptions,\nand supports multi-dataset training without requiring perceptual scale\nrealignment. These features make VisualQuality-R1 especially well-suited for\nreliably measuring progress in a wide range of image processing tasks like\nsuper-resolution and image generation.", "AI": {"tldr": "VisualQuality-R1\u662f\u4e00\u79cd\u57fa\u4e8e\u63a8\u7406\u7684\u65e0\u53c2\u8003\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u6a21\u578b\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u80fd\u751f\u6210\u4e0e\u4eba\u7c7b\u5bf9\u9f50\u7684\u8d28\u91cf\u63cf\u8ff0\u3002", "motivation": "\u63a2\u7d22\u63a8\u7406\u8bf1\u5bfc\u7684\u8ba1\u7b97\u5efa\u6a21\u5728\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\uff08IQA\uff09\u4e2d\u7684\u6f5c\u529b\uff0c\u5c24\u5176\u662f\u5728\u4f9d\u8d56\u89c6\u89c9\u63a8\u7406\u7684\u4efb\u52a1\u4e2d\u3002", "method": "\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u6a21\u578b\uff0c\u901a\u8fc7\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\u751f\u6210\u591a\u4e2a\u8d28\u91cf\u5206\u6570\uff0c\u5e76\u57fa\u4e8eThurstone\u6a21\u578b\u8ba1\u7b97\u6bd4\u8f83\u6982\u7387\u3002", "result": "VisualQuality-R1\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u65e0\u53c2\u8003IQA\u6a21\u578b\uff0c\u5e76\u80fd\u751f\u6210\u4e30\u5bcc\u7684\u8d28\u91cf\u63cf\u8ff0\u3002", "conclusion": "VisualQuality-R1\u9002\u7528\u4e8e\u5e7f\u6cdb\u7684\u56fe\u50cf\u5904\u7406\u4efb\u52a1\uff0c\u5982\u8d85\u5206\u8fa8\u7387\u548c\u56fe\u50cf\u751f\u6210\uff0c\u5177\u6709\u53ef\u9760\u6027\u548c\u591a\u6570\u636e\u96c6\u8bad\u7ec3\u80fd\u529b\u3002"}}
{"id": "2505.14158", "pdf": "https://arxiv.org/pdf/2505.14158", "abs": "https://arxiv.org/abs/2505.14158", "authors": ["Sanjay Govindan", "Maurice Pagnucco", "Yang Song"], "title": "Temporal Alignment of Time Sensitive Facts with Activation Engineering", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) are trained on diverse and often conflicting\nknowledge spanning multiple domains and time periods. Some of this knowledge is\nonly valid within specific temporal contexts, such as answering the question,\n\"Who is the President of the United States in 2022?\" Ensuring LLMs generate\ntime appropriate responses is crucial for maintaining relevance and accuracy.\nIn this work we explore activation engineering as a method for temporally\naligning LLMs to improve factual recall without any training or dataset\ncreation. In this research we explore an activation engineering technique to\nground three versions of LLaMA 2 to specific points in time and examine the\neffects of varying injection layers and prompting strategies. Our experiments\ndemonstrate up to a 44% and 16% improvement in relative and explicit prompting\nrespectively, achieving comparable performance to the fine-tuning method\nproposed by Zhao et al. (2024) . Notably, our approach achieves similar results\nto the fine-tuning baseline while being significantly more computationally\nefficient and requiring no pre-aligned datasets.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u901a\u8fc7\u6fc0\u6d3b\u5de5\u7a0b\u6280\u672f\u4f7fLLMs\u5728\u7279\u5b9a\u65f6\u95f4\u70b9\u4e0a\u5bf9\u9f50\uff0c\u4ee5\u63d0\u9ad8\u4e8b\u5b9e\u56de\u5fc6\u80fd\u529b\uff0c\u65e0\u9700\u8bad\u7ec3\u6216\u6570\u636e\u96c6\u521b\u5efa\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u76f8\u5bf9\u548c\u663e\u5f0f\u63d0\u793a\u4e0b\u5206\u522b\u63d0\u5347\u4e8644%\u548c16%\u7684\u6027\u80fd\u3002", "motivation": "LLMs\u8bad\u7ec3\u6570\u636e\u6db5\u76d6\u591a\u4e2a\u9886\u57df\u548c\u65f6\u95f4\u6bb5\uff0c\u90e8\u5206\u77e5\u8bc6\u4ec5\u5728\u7279\u5b9a\u65f6\u95f4\u6709\u6548\u3002\u786e\u4fddLLMs\u751f\u6210\u65f6\u95f4\u76f8\u5173\u7684\u54cd\u5e94\u5bf9\u5176\u51c6\u786e\u6027\u548c\u76f8\u5173\u6027\u81f3\u5173\u91cd\u8981\u3002", "method": "\u91c7\u7528\u6fc0\u6d3b\u5de5\u7a0b\u6280\u672f\uff0c\u5c06\u4e09\u4e2a\u7248\u672c\u7684LLaMA 2\u5bf9\u9f50\u5230\u7279\u5b9a\u65f6\u95f4\u70b9\uff0c\u7814\u7a76\u4e0d\u540c\u6ce8\u5165\u5c42\u548c\u63d0\u793a\u7b56\u7565\u7684\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u76f8\u5bf9\u63d0\u793a\u548c\u663e\u5f0f\u63d0\u793a\u5206\u522b\u63d0\u5347\u4e8644%\u548c16%\u7684\u6027\u80fd\uff0c\u4e0e\u5fae\u8c03\u65b9\u6cd5\u6548\u679c\u76f8\u5f53\uff0c\u4f46\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\u4e14\u65e0\u9700\u9884\u5bf9\u9f50\u6570\u636e\u96c6\u3002", "conclusion": "\u6fc0\u6d3b\u5de5\u7a0b\u6280\u672f\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u53ef\u663e\u8457\u63d0\u5347LLMs\u5728\u65f6\u95f4\u76f8\u5173\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002"}}
{"id": "2505.14462", "pdf": "https://arxiv.org/pdf/2505.14462", "abs": "https://arxiv.org/abs/2505.14462", "authors": ["Jiaang Li", "Yifei Yuan", "Wenyan Li", "Mohammad Aliannejadi", "Daniel Hershcovich", "Anders S\u00f8gaard", "Ivan Vuli\u0107", "Wenxuan Zhang", "Paul Pu Liang", "Yang Deng", "Serge Belongie"], "title": "RAVENEA: A Benchmark for Multimodal Retrieval-Augmented Visual Culture Understanding", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "As vision-language models (VLMs) become increasingly integrated into daily\nlife, the need for accurate visual culture understanding is becoming critical.\nYet, these models frequently fall short in interpreting cultural nuances\neffectively. Prior work has demonstrated the effectiveness of\nretrieval-augmented generation (RAG) in enhancing cultural understanding in\ntext-only settings, while its application in multimodal scenarios remains\nunderexplored. To bridge this gap, we introduce RAVENEA (Retrieval-Augmented\nVisual culturE uNdErstAnding), a new benchmark designed to advance visual\nculture understanding through retrieval, focusing on two tasks: culture-focused\nvisual question answering (cVQA) and culture-informed image captioning (cIC).\nRAVENEA extends existing datasets by integrating over 10,000 Wikipedia\ndocuments curated and ranked by human annotators. With RAVENEA, we train and\nevaluate seven multimodal retrievers for each image query, and measure the\ndownstream impact of retrieval-augmented inputs across fourteen\nstate-of-the-art VLMs. Our results show that lightweight VLMs, when augmented\nwith culture-aware retrieval, outperform their non-augmented counterparts (by\nat least 3.2% absolute on cVQA and 6.2% absolute on cIC). This highlights the\nvalue of retrieval-augmented methods and culturally inclusive benchmarks for\nmultimodal understanding.", "AI": {"tldr": "RAVENEA\u662f\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u65b9\u6cd5\u63d0\u5347\u89c6\u89c9\u6587\u5316\u7406\u89e3\uff0c\u5728cVQA\u548ccIC\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u6587\u5316\u7ec6\u5fae\u5dee\u522b\u7406\u89e3\u4e0a\u8868\u73b0\u4e0d\u8db3\uff0c\u800c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u5728\u6587\u672c\u573a\u666f\u4e2d\u6709\u6548\uff0c\u4f46\u5728\u591a\u6a21\u6001\u573a\u666f\u4e2d\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u5f15\u5165RAVENEA\u57fa\u51c6\uff0c\u6574\u540810,000+\u7ef4\u57fa\u767e\u79d1\u6587\u6863\uff0c\u8bad\u7ec3\u5e76\u8bc4\u4f30\u4e03\u79cd\u591a\u6a21\u6001\u68c0\u7d22\u5668\uff0c\u6d4b\u8bd5\u5176\u5bf914\u79cd\u5148\u8fdbVLMs\u7684\u5f71\u54cd\u3002", "result": "\u8f7b\u91cf\u7ea7VLMs\u7ed3\u5408\u6587\u5316\u611f\u77e5\u68c0\u7d22\u540e\uff0c\u5728cVQA\u548ccIC\u4efb\u52a1\u4e2d\u5206\u522b\u63d0\u53473.2%\u548c6.2%\u3002", "conclusion": "\u68c0\u7d22\u589e\u5f3a\u65b9\u6cd5\u548c\u6587\u5316\u5305\u5bb9\u6027\u57fa\u51c6\u5bf9\u591a\u6a21\u6001\u7406\u89e3\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2505.14160", "pdf": "https://arxiv.org/pdf/2505.14160", "abs": "https://arxiv.org/abs/2505.14160", "authors": ["Zahraa Al Sahili", "Ioannis Patras", "Matthew Purver"], "title": "Breaking Language Barriers or Reinforcing Bias? A Study of Gender and Racial Disparities in Multilingual Contrastive Vision Language Models", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Multilingual vision-language models promise universal image-text retrieval,\nyet their social biases remain under-explored. We present the first systematic\naudit of three public multilingual CLIP checkpoints -- M-CLIP, NLLB-CLIP, and\nCAPIVARA-CLIP -- across ten languages that vary in resource availability and\ngrammatical gender. Using balanced subsets of \\textsc{FairFace} and the\n\\textsc{PATA} stereotype suite in a zero-shot setting, we quantify race and\ngender bias and measure stereotype amplification. Contrary to the assumption\nthat multilinguality mitigates bias, every model exhibits stronger gender bias\nthan its English-only baseline. CAPIVARA-CLIP shows its largest biases\nprecisely in the low-resource languages it targets, while the shared\ncross-lingual encoder of NLLB-CLIP transports English gender stereotypes into\ngender-neutral languages; loosely coupled encoders largely avoid this transfer.\nHighly gendered languages consistently magnify all measured bias types, but\neven gender-neutral languages remain vulnerable when cross-lingual weight\nsharing imports foreign stereotypes. Aggregated metrics conceal\nlanguage-specific ``hot spots,'' underscoring the need for fine-grained,\nlanguage-aware bias evaluation in future multilingual vision-language research.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u591a\u8bed\u8a00\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u591a\u8bed\u8a00\u73af\u5883\u4e0b\u6027\u522b\u548c\u79cd\u65cf\u504f\u89c1\u66f4\u5f3a\uff0c\u4e14\u4f4e\u8d44\u6e90\u8bed\u8a00\u548c\u6027\u522b\u4e2d\u6027\u8bed\u8a00\u5c24\u5176\u53d7\u5f71\u54cd\u3002", "motivation": "\u63a2\u8ba8\u591a\u8bed\u8a00\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u793e\u4f1a\u504f\u89c1\u65b9\u9762\u7684\u8868\u73b0\uff0c\u5c24\u5176\u662f\u8de8\u8bed\u8a00\u6743\u91cd\u5171\u4eab\u5bf9\u504f\u89c1\u4f20\u64ad\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u96f6\u6837\u672c\u8bbe\u7f6e\uff0c\u4f7f\u7528\u5e73\u8861\u7684FairFace\u548cPATA\u6570\u636e\u96c6\uff0c\u5bf9\u4e09\u79cd\u591a\u8bed\u8a00CLIP\u6a21\u578b\uff08M-CLIP\u3001NLLB-CLIP\u3001CAPIVARA-CLIP\uff09\u5728\u5341\u79cd\u8bed\u8a00\u4e2d\u7684\u6027\u522b\u548c\u79cd\u65cf\u504f\u89c1\u8fdb\u884c\u91cf\u5316\u3002", "result": "\u6240\u6709\u6a21\u578b\u5728\u591a\u8bed\u8a00\u73af\u5883\u4e0b\u8868\u73b0\u51fa\u6bd4\u5355\u8bed\u8a00\u57fa\u7ebf\u66f4\u5f3a\u7684\u6027\u522b\u504f\u89c1\uff0c\u4f4e\u8d44\u6e90\u8bed\u8a00\u548c\u6027\u522b\u4e2d\u6027\u8bed\u8a00\u5c24\u5176\u53d7\u5f71\u54cd\u3002", "conclusion": "\u672a\u6765\u591a\u8bed\u8a00\u89c6\u89c9\u8bed\u8a00\u7814\u7a76\u9700\u8981\u66f4\u7ec6\u7c92\u5ea6\u3001\u8bed\u8a00\u611f\u77e5\u7684\u504f\u89c1\u8bc4\u4f30\u65b9\u6cd5\u3002"}}
{"id": "2505.14476", "pdf": "https://arxiv.org/pdf/2505.14476", "abs": "https://arxiv.org/abs/2505.14476", "authors": ["Farshad Sangari Abiz", "Reshad Hosseini", "Babak N. Araabi"], "title": "Enhancing Interpretability of Sparse Latent Representations with Class Information", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Variational Autoencoders (VAEs) are powerful generative models for learning\nlatent representations. Standard VAEs generate dispersed and unstructured\nlatent spaces by utilizing all dimensions, which limits their interpretability,\nespecially in high-dimensional spaces. To address this challenge, Variational\nSparse Coding (VSC) introduces a spike-and-slab prior distribution, resulting\nin sparse latent representations for each input. These sparse representations,\ncharacterized by a limited number of active dimensions, are inherently more\ninterpretable. Despite this advantage, VSC falls short in providing structured\ninterpretations across samples within the same class. Intuitively, samples from\nthe same class are expected to share similar attributes while allowing for\nvariations in those attributes. This expectation should manifest as consistent\npatterns of active dimensions in their latent representations, but VSC does not\nenforce such consistency.\n  In this paper, we propose a novel approach to enhance the latent space\ninterpretability by ensuring that the active dimensions in the latent space are\nconsistent across samples within the same class. To achieve this, we introduce\na new loss function that encourages samples from the same class to share\nsimilar active dimensions. This alignment creates a more structured and\ninterpretable latent space, where each shared dimension corresponds to a\nhigh-level concept, or \"factor.\" Unlike existing disentanglement-based methods\nthat primarily focus on global factors shared across all classes, our method\ncaptures both global and class-specific factors, thereby enhancing the utility\nand interpretability of latent representations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u786e\u4fdd\u540c\u4e00\u7c7b\u522b\u6837\u672c\u7684\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7684\u6d3b\u8dc3\u7ef4\u5ea6\u4e00\u81f4\uff0c\u589e\u5f3a\u6f5c\u5728\u7a7a\u95f4\u7684\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u6807\u51c6VAE\u751f\u6210\u7684\u6f5c\u5728\u7a7a\u95f4\u5206\u6563\u4e14\u65e0\u7ed3\u6784\uff0c\u9650\u5236\u4e86\u5176\u53ef\u89e3\u91ca\u6027\u3002VSC\u867d\u7136\u5f15\u5165\u4e86\u7a00\u758f\u6f5c\u5728\u8868\u793a\uff0c\u4f46\u672a\u80fd\u4fdd\u8bc1\u540c\u4e00\u7c7b\u522b\u6837\u672c\u7684\u6d3b\u8dc3\u7ef4\u5ea6\u4e00\u81f4\u6027\u3002", "method": "\u5f15\u5165\u65b0\u7684\u635f\u5931\u51fd\u6570\uff0c\u9f13\u52b1\u540c\u4e00\u7c7b\u522b\u6837\u672c\u5171\u4eab\u76f8\u4f3c\u7684\u6d3b\u8dc3\u7ef4\u5ea6\uff0c\u4ece\u800c\u521b\u5efa\u66f4\u7ed3\u6784\u5316\u7684\u6f5c\u5728\u7a7a\u95f4\u3002", "result": "\u8be5\u65b9\u6cd5\u751f\u6210\u4e86\u4e00\u4e2a\u66f4\u7ed3\u6784\u5316\u548c\u53ef\u89e3\u91ca\u7684\u6f5c\u5728\u7a7a\u95f4\uff0c\u5176\u4e2d\u6bcf\u4e2a\u5171\u4eab\u7ef4\u5ea6\u5bf9\u5e94\u4e00\u4e2a\u9ad8\u7ea7\u6982\u5ff5\u6216\u201c\u56e0\u5b50\u201d\u3002", "conclusion": "\u65b0\u65b9\u6cd5\u4e0d\u4ec5\u6355\u6349\u4e86\u5168\u5c40\u56e0\u5b50\uff0c\u8fd8\u6355\u83b7\u4e86\u7c7b\u522b\u7279\u5b9a\u56e0\u5b50\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6f5c\u5728\u8868\u793a\u7684\u5b9e\u7528\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2505.14165", "pdf": "https://arxiv.org/pdf/2505.14165", "abs": "https://arxiv.org/abs/2505.14165", "authors": ["Zhenkai Qin", "Jiajing He", "Qiao Fang"], "title": "PL-FGSA: A Prompt Learning Framework for Fine-Grained Sentiment Analysis Based on MindSpore", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Fine-grained sentiment analysis (FGSA) aims to identify sentiment polarity\ntoward specific aspects within a text, enabling more precise opinion mining in\ndomains such as product reviews and social media. However, traditional FGSA\napproaches often require task-specific architectures and extensive annotated\ndata, limiting their generalization and scalability. To address these\nchallenges, we propose PL-FGSA, a unified prompt learning-based framework\nimplemented using the MindSpore platform, which integrates prompt design with a\nlightweight TextCNN backbone. Our method reformulates FGSA as a multi-task\nprompt-augmented generation problem, jointly tackling aspect extraction,\nsentiment classification, and causal explanation in a unified paradigm. By\nleveraging prompt-based guidance, PL-FGSA enhances interpretability and\nachieves strong performance under both full-data and low-resource conditions.\nExperiments on three benchmark datasets-SST-2, SemEval-2014 Task 4, and\nMAMS-demonstrate that our model consistently outperforms traditional\nfine-tuning methods and achieves F1-scores of 0.922, 0.694, and 0.597,\nrespectively. These results validate the effectiveness of prompt-based\ngeneralization and highlight the practical value of PL-FGSA for real-world\nsentiment analysis tasks.", "AI": {"tldr": "PL-FGSA\u662f\u4e00\u79cd\u57fa\u4e8e\u63d0\u793a\u5b66\u4e60\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u7528\u4e8e\u7ec6\u7c92\u5ea6\u60c5\u611f\u5206\u6790\uff0c\u901a\u8fc7\u591a\u4efb\u52a1\u63d0\u793a\u589e\u5f3a\u751f\u6210\u65b9\u6cd5\uff0c\u7ed3\u5408\u8f7b\u91cf\u7ea7TextCNN\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u5e76\u589e\u5f3a\u4e86\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u4f20\u7edf\u7ec6\u7c92\u5ea6\u60c5\u611f\u5206\u6790\u65b9\u6cd5\u9700\u8981\u7279\u5b9a\u67b6\u6784\u548c\u5927\u91cf\u6807\u6ce8\u6570\u636e\uff0c\u6cdb\u5316\u6027\u548c\u6269\u5c55\u6027\u53d7\u9650\u3002", "method": "\u63d0\u51faPL-FGSA\u6846\u67b6\uff0c\u5c06\u4efb\u52a1\u91cd\u65b0\u5b9a\u4e49\u4e3a\u591a\u4efb\u52a1\u63d0\u793a\u589e\u5f3a\u751f\u6210\u95ee\u9898\uff0c\u7ed3\u5408\u63d0\u793a\u8bbe\u8ba1\u548c\u8f7b\u91cf\u7ea7TextCNN\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0cF1\u5206\u6570\u5206\u522b\u4e3a0.922\u30010.694\u548c0.597\uff0c\u4f18\u4e8e\u4f20\u7edf\u5fae\u8c03\u65b9\u6cd5\u3002", "conclusion": "PL-FGSA\u901a\u8fc7\u63d0\u793a\u5b66\u4e60\u63d0\u5347\u4e86\u6cdb\u5316\u80fd\u529b\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2505.14511", "pdf": "https://arxiv.org/pdf/2505.14511", "abs": "https://arxiv.org/abs/2505.14511", "authors": ["Guillaume Vray", "Devavrat Tomar", "Xufeng Gao", "Jean-Philippe Thiran", "Evan Shelhamer", "Behzad Bozorgtabar"], "title": "ReservoirTTA: Prolonged Test-time Adaptation for Evolving and Recurring Domains", "categories": ["cs.CV"], "comment": null, "summary": "This paper introduces ReservoirTTA, a novel plug-in framework designed for\nprolonged test-time adaptation (TTA) in scenarios where the test domain\ncontinuously shifts over time, including cases where domains recur or evolve\ngradually. At its core, ReservoirTTA maintains a reservoir of\ndomain-specialized models -- an adaptive test-time model ensemble -- that both\ndetects new domains via online clustering over style features of incoming\nsamples and routes each sample to the appropriate specialized model, and\nthereby enables domain-specific adaptation. This multi-model strategy overcomes\nkey limitations of single model adaptation, such as catastrophic forgetting,\ninter-domain interference, and error accumulation, ensuring robust and stable\nperformance on sustained non-stationary test distributions. Our theoretical\nanalysis reveals key components that bound parameter variance and prevent model\ncollapse, while our plug-in TTA module mitigates catastrophic forgetting of\npreviously encountered domains. Extensive experiments on the classification\ncorruption benchmarks, including ImageNet-C and CIFAR-10/100-C, as well as the\nCityscapes$\\rightarrow$ACDC semantic segmentation task, covering recurring and\ncontinuously evolving domain shifts, demonstrate that ReservoirTTA\nsignificantly improves adaptation accuracy and maintains stable performance\nacross prolonged, recurring shifts, outperforming state-of-the-art methods.", "AI": {"tldr": "ReservoirTTA\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u63d2\u4ef6\u6846\u67b6\uff0c\u7528\u4e8e\u5904\u7406\u6d4b\u8bd5\u9886\u57df\u6301\u7eed\u53d8\u5316\u7684\u957f\u671f\u6d4b\u8bd5\u65f6\u95f4\u9002\u5e94\uff08TTA\uff09\uff0c\u901a\u8fc7\u7ef4\u62a4\u4e00\u4e2a\u9886\u57df\u4e13\u7528\u6a21\u578b\u5e93\u6765\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u6d4b\u8bd5\u9886\u57df\u6301\u7eed\u53d8\u5316\u65f6\u5355\u6a21\u578b\u9002\u5e94\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u3001\u9886\u57df\u95f4\u5e72\u6270\u548c\u9519\u8bef\u7d2f\u79ef\u7b49\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u5728\u7ebf\u805a\u7c7b\u68c0\u6d4b\u65b0\u9886\u57df\uff0c\u5e76\u901a\u8fc7\u8def\u7531\u6837\u672c\u5230\u4e13\u7528\u6a21\u578b\u5b9e\u73b0\u9886\u57df\u7279\u5b9a\u9002\u5e94\u3002", "result": "\u5728ImageNet-C\u3001CIFAR-10/100-C\u548cCityscapes\u2192ACDC\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "ReservoirTTA\u5728\u591a\u9886\u57df\u6301\u7eed\u53d8\u5316\u573a\u666f\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u9002\u5e94\u51c6\u786e\u6027\u548c\u7a33\u5b9a\u6027\u3002"}}
{"id": "2505.14172", "pdf": "https://arxiv.org/pdf/2505.14172", "abs": "https://arxiv.org/abs/2505.14172", "authors": ["Adrian Cosma", "Stefan Ruseti", "Emilian Radoi", "Mihai Dascalu"], "title": "The Strawberry Problem: Emergence of Character-level Understanding in Tokenized Language Models", "categories": ["cs.CL"], "comment": "1 Table, 8 Figures", "summary": "Despite their remarkable progress across diverse domains, Large Language\nModels (LLMs) consistently fail at simple character-level tasks, such as\ncounting letters in words, due to a fundamental limitation: tokenization. In\nthis work, we frame this limitation as a problem of low mutual information and\nanalyze it in terms of concept emergence. Using a suite of 19 synthetic tasks\nthat isolate character-level reasoning in a controlled setting, we show that\nsuch capabilities emerge slowly, suddenly, and only late in training. We\nfurther show that percolation-based models of concept emergence explain these\npatterns, suggesting that learning character composition is not fundamentally\ndifferent from learning commonsense knowledge. To address this bottleneck, we\npropose a lightweight architectural modification that significantly improves\ncharacter-level reasoning while preserving the inductive advantages of subword\nmodels. Together, our results bridge low-level perceptual gaps in tokenized LMs\nand provide a principled framework for understanding and mitigating their\nstructural blind spots. We make our code publicly available.", "AI": {"tldr": "LLMs\u5728\u5b57\u7b26\u7ea7\u4efb\u52a1\uff08\u5982\u5355\u8bcd\u5b57\u6bcd\u8ba1\u6570\uff09\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u5206\u8bcd\u95ee\u9898\u3002\u672c\u6587\u901a\u8fc719\u4e2a\u5408\u6210\u4efb\u52a1\u5206\u6790\u5176\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u8f7b\u91cf\u7ea7\u67b6\u6784\u6539\u8fdb\u65b9\u6cd5\u3002", "motivation": "\u7814\u7a76LLMs\u5728\u5b57\u7b26\u7ea7\u4efb\u52a1\u4e0a\u7684\u5931\u8d25\u539f\u56e0\uff0c\u5e76\u63d0\u51fa\u89e3\u51b3\u65b9\u6848\uff0c\u4ee5\u5f25\u8865\u5176\u7ed3\u6784\u6027\u76f2\u70b9\u3002", "method": "\u4f7f\u752819\u4e2a\u5408\u6210\u4efb\u52a1\u5206\u6790\u5b57\u7b26\u7ea7\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u8f7b\u91cf\u7ea7\u67b6\u6784\u6539\u8fdb\u65b9\u6cd5\u3002", "result": "\u5b57\u7b26\u7ea7\u80fd\u529b\u5728\u8bad\u7ec3\u540e\u671f\u7f13\u6162\u4e14\u7a81\u7136\u51fa\u73b0\uff0c\u6539\u8fdb\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "conclusion": "\u7814\u7a76\u4e3a\u7406\u89e3LLMs\u7684\u5c40\u9650\u6027\u63d0\u4f9b\u4e86\u6846\u67b6\uff0c\u5e76\u63d0\u51fa\u4e86\u6709\u6548\u7684\u6539\u8fdb\u65b9\u6848\u3002"}}
{"id": "2505.14521", "pdf": "https://arxiv.org/pdf/2505.14521", "abs": "https://arxiv.org/abs/2505.14521", "authors": ["Zhihao Li", "Yufei Wang", "Heliang Zheng", "Yihao Luo", "Bihan Wen"], "title": "SparC: Sparse Representation and Construction for High-Resolution 3D Shapes Modeling", "categories": ["cs.CV"], "comment": "Homepage: https://lizhihao6.github.io/SparC", "summary": "High-fidelity 3D object synthesis remains significantly more challenging than\n2D image generation due to the unstructured nature of mesh data and the cubic\ncomplexity of dense volumetric grids. Existing two-stage pipelines-compressing\nmeshes with a VAE (using either 2D or 3D supervision), followed by latent\ndiffusion sampling-often suffer from severe detail loss caused by inefficient\nrepresentations and modality mismatches introduced in VAE. We introduce SparC,\na unified framework that combines a sparse deformable marching cubes\nrepresentation SparseCubes with a novel encoder SparConv-VAE. SparseCubes\nconverts raw meshes into high-resolution ($1024^3$) surfaces with arbitrary\ntopology by scattering signed distance and deformation fields onto a sparse\ncube, allowing differentiable optimization. SparConv-VAE is the first\nmodality-consistent variational autoencoder built entirely upon sparse\nconvolutional networks, enabling efficient and near-lossless 3D reconstruction\nsuitable for high-resolution generative modeling through latent diffusion.\nSparC achieves state-of-the-art reconstruction fidelity on challenging inputs,\nincluding open surfaces, disconnected components, and intricate geometry. It\npreserves fine-grained shape details, reduces training and inference cost, and\nintegrates naturally with latent diffusion models for scalable, high-resolution\n3D generation.", "AI": {"tldr": "SparC\u6846\u67b6\u901a\u8fc7\u7a00\u758f\u53ef\u53d8\u5f62\u7acb\u65b9\u4f53\u8868\u793a\u548c\u65b0\u578b\u7f16\u7801\u5668SparConv-VAE\uff0c\u89e3\u51b3\u4e863D\u5bf9\u8c61\u5408\u6210\u4e2d\u7684\u7ec6\u8282\u4e22\u5931\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u91cd\u5efa\u548c\u9ad8\u6548\u751f\u6210\u3002", "motivation": "\u73b0\u67093D\u5bf9\u8c61\u5408\u6210\u65b9\u6cd5\u56e0\u7f51\u683c\u6570\u636e\u975e\u7ed3\u6784\u5316\u548c\u4f53\u79ef\u7f51\u683c\u7acb\u65b9\u590d\u6742\u5ea6\u9ad8\uff0c\u5bfc\u81f4\u7ec6\u8282\u4e22\u5931\u4e25\u91cd\uff0cSparC\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u7a00\u758f\u53ef\u53d8\u5f62\u7acb\u65b9\u4f53\u8868\u793a\uff08SparseCubes\uff09\u548c\u7a00\u758f\u5377\u79ef\u7f51\u7edc\u6784\u5efa\u7684SparConv-VAE\uff0c\u5b9e\u73b0\u9ad8\u6548\u3001\u8fd1\u65e0\u635f\u76843D\u91cd\u5efa\u3002", "result": "SparC\u5728\u5f00\u653e\u8868\u9762\u3001\u4e0d\u8fde\u7eed\u7ec4\u4ef6\u548c\u590d\u6742\u51e0\u4f55\u4f53\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u91cd\u5efa\u4fdd\u771f\u5ea6\uff0c\u5e76\u964d\u4f4e\u4e86\u8bad\u7ec3\u548c\u63a8\u7406\u6210\u672c\u3002", "conclusion": "SparC\u4e3a\u9ad8\u5206\u8fa8\u73873D\u751f\u6210\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u4e0e\u6f5c\u5728\u6269\u6563\u6a21\u578b\u81ea\u7136\u96c6\u6210\u3002"}}
{"id": "2505.14173", "pdf": "https://arxiv.org/pdf/2505.14173", "abs": "https://arxiv.org/abs/2505.14173", "authors": ["Yunlong Liang", "Fandong Meng", "Jie Zhou"], "title": "THOR-MoE: Hierarchical Task-Guided and Context-Responsive Routing for Neural Machine Translation", "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 main conference", "summary": "The sparse Mixture-of-Experts (MoE) has achieved significant progress for\nneural machine translation (NMT). However, there exist two limitations in\ncurrent MoE solutions which may lead to sub-optimal performance: 1) they\ndirectly use the task knowledge of NMT into MoE (\\emph{e.g.},\ndomain/linguistics-specific knowledge), which are generally unavailable at\npractical application and neglect the naturally grouped domain/linguistic\nproperties; 2) the expert selection only depends on the localized token\nrepresentation without considering the context, which fully grasps the state of\neach token in a global view. To address the above limitations, we propose\nTHOR-MoE via arming the MoE with hierarchical task-guided and\ncontext-responsive routing policies. Specifically, it 1) firstly predicts the\ndomain/language label and then extracts mixed domain/language representation to\nallocate task-level experts in a hierarchical manner; 2) injects the context\ninformation to enhance the token routing from the pre-selected task-level\nexperts set, which can help each token to be accurately routed to more\nspecialized and suitable experts. Extensive experiments on multi-domain\ntranslation and multilingual translation benchmarks with different\narchitectures consistently demonstrate the superior performance of THOR-MoE.\nAdditionally, the THOR-MoE operates as a plug-and-play module compatible with\nexisting Top-$k$~\\cite{shazeer2017} and Top-$p$~\\cite{huang-etal-2024-harder}\nrouting schemes, ensuring broad applicability across diverse MoE architectures.\nFor instance, compared with vanilla Top-$p$~\\cite{huang-etal-2024-harder}\nrouting, the context-aware manner can achieve an average improvement of 0.75\nBLEU with less than 22\\% activated parameters on multi-domain translation\ntasks.", "AI": {"tldr": "THOR-MoE\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u4efb\u52a1\u5f15\u5bfc\u548c\u4e0a\u4e0b\u6587\u54cd\u5e94\u7684\u8def\u7531\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u7a00\u758fMoE\u5728NMT\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u5f53\u524dMoE\u89e3\u51b3\u65b9\u6848\u5b58\u5728\u4e24\u4e2a\u5c40\u9650\u6027\uff1a1\uff09\u76f4\u63a5\u4f7f\u7528NMT\u7684\u4efb\u52a1\u77e5\u8bc6\uff0c\u5ffd\u7565\u4e86\u81ea\u7136\u5206\u7ec4\u7684\u9886\u57df/\u8bed\u8a00\u7279\u6027\uff1b2\uff09\u4e13\u5bb6\u9009\u62e9\u4ec5\u4f9d\u8d56\u5c40\u90e8\u6807\u8bb0\u8868\u793a\uff0c\u672a\u8003\u8651\u4e0a\u4e0b\u6587\u3002", "method": "THOR-MoE\u901a\u8fc7\u5206\u5c42\u4efb\u52a1\u5f15\u5bfc\u548c\u4e0a\u4e0b\u6587\u54cd\u5e94\u8def\u7531\u7b56\u7565\uff1a1\uff09\u9884\u6d4b\u9886\u57df/\u8bed\u8a00\u6807\u7b7e\u5e76\u63d0\u53d6\u6df7\u5408\u8868\u793a\uff1b2\uff09\u6ce8\u5165\u4e0a\u4e0b\u6587\u4fe1\u606f\u589e\u5f3a\u6807\u8bb0\u8def\u7531\u3002", "result": "\u5728\u591a\u9886\u57df\u548c\u591a\u8bed\u8a00\u7ffb\u8bd1\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5e73\u5747BLEU\u63d0\u53470.75\uff0c\u6fc0\u6d3b\u53c2\u6570\u51cf\u5c1122%\u3002", "conclusion": "THOR-MoE\u662f\u4e00\u79cd\u5373\u63d2\u5373\u7528\u6a21\u5757\uff0c\u517c\u5bb9\u73b0\u6709\u8def\u7531\u65b9\u6848\uff0c\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\u3002"}}
{"id": "2505.14527", "pdf": "https://arxiv.org/pdf/2505.14527", "abs": "https://arxiv.org/abs/2505.14527", "authors": ["Nitish Shukla", "Arun Ross"], "title": "diffDemorph: Extending Reference-Free Demorphing to Unseen Faces", "categories": ["cs.CV"], "comment": null, "summary": "A face morph is created by combining two (or more) face images corresponding\nto two (or more) identities to produce a composite that successfully matches\nthe constituent identities. Reference-free (RF) demorphing reverses this\nprocess using only the morph image, without the need for additional reference\nimages. Previous RF demorphing methods were overly constrained, as they rely on\nassumptions about the distributions of training and testing morphs such as the\nmorphing technique used, face style, and images used to create the morph. In\nthis paper, we introduce a novel diffusion-based approach that effectively\ndisentangles component images from a composite morph image with high visual\nfidelity. Our method is the first to generalize across morph techniques and\nface styles, beating the current state of the art by $\\geq 59.46\\%$ under a\ncommon training protocol across all datasets tested. We train our method on\nmorphs created using synthetically generated face images and test on real\nmorphs, thereby enhancing the practicality of the technique. Experiments on six\ndatasets and two face matchers establish the utility and efficacy of our\nmethod.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u7684\u65b0\u65b9\u6cd5\uff0c\u65e0\u9700\u53c2\u8003\u56fe\u50cf\u5373\u53ef\u4ece\u5408\u6210\u4eba\u8138\u56fe\u50cf\u4e2d\u5206\u79bb\u51fa\u539f\u59cb\u4eba\u8138\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u73b0\u6709\u53c2\u8003\u65e0\u5173\u7684\u53bb\u5408\u6210\u65b9\u6cd5\u53d7\u9650\u4e8e\u8bad\u7ec3\u548c\u6d4b\u8bd5\u6570\u636e\u7684\u5206\u5e03\u5047\u8bbe\uff0c\u5982\u5408\u6210\u6280\u672f\u548c\u4eba\u8138\u98ce\u683c\uff0c\u9650\u5236\u4e86\u5b9e\u7528\u6027\u3002", "method": "\u91c7\u7528\u6269\u6563\u6a21\u578b\uff0c\u4ece\u5408\u6210\u56fe\u50cf\u4e2d\u5206\u79bb\u51fa\u539f\u59cb\u4eba\u8138\uff0c\u652f\u6301\u8de8\u6280\u672f\u548c\u98ce\u683c\u7684\u6cdb\u5316\u3002", "result": "\u5728\u516d\u4e2a\u6570\u636e\u96c6\u548c\u4e24\u4e2a\u4eba\u8138\u5339\u914d\u5668\u4e0a\u6d4b\u8bd5\uff0c\u6027\u80fd\u63d0\u5347\u226559.46%\uff0c\u4e14\u80fd\u6cdb\u5316\u5230\u771f\u5b9e\u5408\u6210\u56fe\u50cf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u53bb\u5408\u6210\u6280\u672f\u7684\u5b9e\u7528\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u573a\u666f\u3002"}}
{"id": "2505.14174", "pdf": "https://arxiv.org/pdf/2505.14174", "abs": "https://arxiv.org/abs/2505.14174", "authors": ["Yusuf Denizay D\u00f6nder", "Derek Hommel", "Andrea W Wen-Yi", "David Mimno", "Unso Eun Seo Jo"], "title": "Cheaper, Better, Faster, Stronger: Robust Text-to-SQL without Chain-of-Thought or Fine-Tuning", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "LLMs are effective at code generation tasks like text-to-SQL, but is it worth\nthe cost? Many state-of-the-art approaches use non-task-specific LLM techniques\nincluding Chain-of-Thought (CoT), self-consistency, and fine-tuning. These\nmethods can be costly at inference time, sometimes requiring over a hundred LLM\ncalls with reasoning, incurring average costs of up to \\$0.46 per query, while\nfine-tuning models can cost thousands of dollars. We introduce \"N-rep\"\nconsistency, a more cost-efficient text-to-SQL approach that achieves similar\nBIRD benchmark scores as other more expensive methods, at only \\$0.039 per\nquery. N-rep leverages multiple representations of the same schema input to\nmitigate weaknesses in any single representation, making the solution more\nrobust and allowing the use of smaller and cheaper models without any reasoning\nor fine-tuning. To our knowledge, N-rep is the best-performing text-to-SQL\napproach in its cost range.", "AI": {"tldr": "N-rep\u4e00\u81f4\u6027\u662f\u4e00\u79cd\u66f4\u7ecf\u6d4e\u7684\u6587\u672c\u5230SQL\u65b9\u6cd5\uff0c\u6027\u80fd\u63a5\u8fd1\u6602\u8d35\u65b9\u6cd5\uff0c\u6210\u672c\u4ec5\u4e3a\u6bcf\u6b21\u67e5\u8be20.039\u7f8e\u5143\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\uff08\u5982CoT\u3001\u81ea\u4e00\u81f4\u6027\u548c\u5fae\u8c03\uff09\u6210\u672c\u9ad8\uff0c\u63a8\u7406\u65f6\u53ef\u80fd\u9700\u8981\u4e0a\u767e\u6b21LLM\u8c03\u7528\uff0c\u6bcf\u6b21\u67e5\u8be2\u6210\u672c\u9ad8\u8fbe0.46\u7f8e\u5143\u3002", "method": "N-rep\u5229\u7528\u540c\u4e00\u6a21\u5f0f\u8f93\u5165\u7684\u591a\u79cd\u8868\u793a\u6765\u5f25\u8865\u5355\u4e00\u8868\u793a\u7684\u4e0d\u8db3\uff0c\u65e0\u9700\u63a8\u7406\u6216\u5fae\u8c03\uff0c\u4f7f\u7528\u66f4\u5c0f\u66f4\u4fbf\u5b9c\u7684\u6a21\u578b\u3002", "result": "N-rep\u5728BIRD\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u63a5\u8fd1\u6602\u8d35\u65b9\u6cd5\uff0c\u6210\u672c\u663e\u8457\u964d\u4f4e\u3002", "conclusion": "N-rep\u662f\u76ee\u524d\u6210\u672c\u8303\u56f4\u5185\u6027\u80fd\u6700\u4f73\u7684\u6587\u672c\u5230SQL\u65b9\u6cd5\u3002"}}
{"id": "2505.14537", "pdf": "https://arxiv.org/pdf/2505.14537", "abs": "https://arxiv.org/abs/2505.14537", "authors": ["Yuxuan Wang", "Xuanyu Yi", "Qingshan Xu", "Yuan Zhou", "Long Chen", "Hanwang Zhang"], "title": "Personalize Your Gaussian: Consistent 3D Scene Personalization from a Single Image", "categories": ["cs.CV"], "comment": "9 pages", "summary": "Personalizing 3D scenes from a single reference image enables intuitive\nuser-guided editing, which requires achieving both multi-view consistency\nacross perspectives and referential consistency with the input image. However,\nthese goals are particularly challenging due to the viewpoint bias caused by\nthe limited perspective provided in a single image. Lacking the mechanisms to\neffectively expand reference information beyond the original view, existing\nmethods of image-conditioned 3DGS personalization often suffer from this\nviewpoint bias and struggle to produce consistent results. Therefore, in this\npaper, we present Consistent Personalization for 3D Gaussian Splatting (CP-GS),\na framework that progressively propagates the single-view reference appearance\nto novel perspectives. In particular, CP-GS integrates pre-trained image-to-3D\ngeneration and iterative LoRA fine-tuning to extract and extend the reference\nappearance, and finally produces faithful multi-view guidance images and the\npersonalized 3DGS outputs through a view-consistent generation process guided\nby geometric cues. Extensive experiments on real-world scenes show that our\nCP-GS effectively mitigates the viewpoint bias, achieving high-quality\npersonalization that significantly outperforms existing methods. The code will\nbe released at https://github.com/Yuxuan-W/CP-GS.", "AI": {"tldr": "CP-GS\u662f\u4e00\u4e2a\u901a\u8fc7\u5355\u5f20\u53c2\u8003\u56fe\u50cf\u4e2a\u6027\u53163D\u573a\u666f\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6e10\u8fdb\u5f0f\u4f20\u64ad\u53c2\u8003\u5916\u89c2\u548c\u51e0\u4f55\u7ebf\u7d22\uff0c\u89e3\u51b3\u4e86\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u548c\u53c2\u8003\u4e00\u81f4\u6027\u7684\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u56e0\u5355\u89c6\u89d2\u9650\u5236\u5bfc\u81f4\u89c6\u89d2\u504f\u5dee\uff0c\u96be\u4ee5\u5b9e\u73b0\u591a\u89c6\u89d2\u548c\u53c2\u8003\u4e00\u81f4\u6027\uff0cCP-GS\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u9884\u8bad\u7ec3\u56fe\u50cf\u52303D\u751f\u6210\u548c\u8fed\u4ee3LoRA\u5fae\u8c03\uff0c\u901a\u8fc7\u51e0\u4f55\u7ebf\u7d22\u751f\u6210\u591a\u89c6\u89d2\u6307\u5bfc\u56fe\u50cf\u548c\u4e2a\u6027\u53163DGS\u8f93\u51fa\u3002", "result": "\u5b9e\u9a8c\u8868\u660eCP-GS\u6709\u6548\u51cf\u5c11\u89c6\u89d2\u504f\u5dee\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "CP-GS\u901a\u8fc7\u51e0\u4f55\u5f15\u5bfc\u7684\u751f\u6210\u8fc7\u7a0b\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u76843D\u573a\u666f\u4e2a\u6027\u5316\u3002"}}
{"id": "2505.14178", "pdf": "https://arxiv.org/pdf/2505.14178", "abs": "https://arxiv.org/abs/2505.14178", "authors": ["Xiang Zhang", "Juntai Cao", "Jiaqi Wei", "Yiwei Xu", "Chenyu You"], "title": "Tokenization Constraints in LLMs: A Study of Symbolic and Arithmetic Reasoning Limits", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Tokenization is the first - and often underappreciated - layer of computation\nin language models. While Chain-of-Thought (CoT) prompting enables transformer\nmodels to approximate recurrent computation by externalizing intermediate\nsteps, we show that the success of such reasoning is fundamentally bounded by\nthe structure of tokenized inputs. This work presents a theoretical and\nempirical investigation into how tokenization schemes, particularly\nsubword-based methods like byte-pair encoding (BPE), impede symbolic\ncomputation by merging or obscuring atomic reasoning units. We introduce the\nnotion of Token Awareness to formalize how poor token granularity disrupts\nlogical alignment and prevents models from generalizing symbolic procedures.\nThrough systematic evaluation on arithmetic and symbolic tasks, we demonstrate\nthat token structure dramatically affect reasoning performance, causing failure\neven with CoT, while atomically-aligned formats unlock strong generalization,\nallowing small models (e.g., GPT-4o-mini) to outperform larger systems (e.g.,\no1) in structured reasoning. Our findings reveal that symbolic reasoning\nability in LLMs is not purely architectural, but deeply conditioned on\ntoken-level representations.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5206\u8bcd\u5bf9\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\u7684\u9650\u5236\uff0c\u63d0\u51faToken Awareness\u6982\u5ff5\uff0c\u5e76\u8bc1\u660e\u539f\u5b50\u5bf9\u9f50\u7684\u5206\u8bcd\u683c\u5f0f\u80fd\u663e\u8457\u63d0\u5347\u63a8\u7406\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u5206\u8bcd\uff08\u5982BPE\uff09\u5982\u4f55\u901a\u8fc7\u5408\u5e76\u6216\u6a21\u7cca\u539f\u5b50\u63a8\u7406\u5355\u5143\u963b\u788d\u7b26\u53f7\u8ba1\u7b97\uff0c\u4ece\u800c\u9650\u5236\u6a21\u578b\u63a8\u7406\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u548c\u5b9e\u9a8c\u5206\u6790\u5206\u8bcd\u7ed3\u6784\u5bf9\u63a8\u7406\u7684\u5f71\u54cd\uff0c\u5f15\u5165Token Awareness\u6982\u5ff5\uff0c\u5e76\u5728\u7b97\u672f\u548c\u7b26\u53f7\u4efb\u52a1\u4e0a\u7cfb\u7edf\u8bc4\u4f30\u3002", "result": "\u539f\u5b50\u5bf9\u9f50\u7684\u5206\u8bcd\u683c\u5f0f\u663e\u8457\u63d0\u5347\u63a8\u7406\u6027\u80fd\uff0c\u5c0f\u6a21\u578b\uff08\u5982GPT-4o-mini\uff09\u5728\u7ed3\u6784\u5316\u63a8\u7406\u4e2d\u4f18\u4e8e\u5927\u6a21\u578b\uff08\u5982o1\uff09\u3002", "conclusion": "LLMs\u7684\u7b26\u53f7\u63a8\u7406\u80fd\u529b\u4e0d\u4ec5\u4f9d\u8d56\u67b6\u6784\uff0c\u8fd8\u53d7\u5206\u8bcd\u8868\u793a\u6df1\u5ea6\u5f71\u54cd\u3002"}}
{"id": "2505.14556", "pdf": "https://arxiv.org/pdf/2505.14556", "abs": "https://arxiv.org/abs/2505.14556", "authors": ["Marl\u00e8ne Careil", "Yohann Benchetrit", "Jean-R\u00e9mi King"], "title": "Dynadiff: Single-stage Decoding of Images from Continuously Evolving fMRI", "categories": ["cs.CV"], "comment": null, "summary": "Brain-to-image decoding has been recently propelled by the progress in\ngenerative AI models and the availability of large ultra-high field functional\nMagnetic Resonance Imaging (fMRI). However, current approaches depend on\ncomplicated multi-stage pipelines and preprocessing steps that typically\ncollapse the temporal dimension of brain recordings, thereby limiting\ntime-resolved brain decoders. Here, we introduce Dynadiff (Dynamic Neural\nActivity Diffusion for Image Reconstruction), a new single-stage diffusion\nmodel designed for reconstructing images from dynamically evolving fMRI\nrecordings. Our approach offers three main contributions. First, Dynadiff\nsimplifies training as compared to existing approaches. Second, our model\noutperforms state-of-the-art models on time-resolved fMRI signals, especially\non high-level semantic image reconstruction metrics, while remaining\ncompetitive on preprocessed fMRI data that collapse time. Third, this approach\nallows a precise characterization of the evolution of image representations in\nbrain activity. Overall, this work lays the foundation for time-resolved\nbrain-to-image decoding.", "AI": {"tldr": "Dynadiff\u662f\u4e00\u79cd\u65b0\u7684\u5355\u9636\u6bb5\u6269\u6563\u6a21\u578b\uff0c\u7528\u4e8e\u4ece\u52a8\u6001fMRI\u8bb0\u5f55\u4e2d\u91cd\u5efa\u56fe\u50cf\uff0c\u7b80\u5316\u4e86\u8bad\u7ec3\u6d41\u7a0b\uff0c\u5e76\u5728\u65f6\u95f4\u5206\u8fa8\u7387\u548c\u8bed\u4e49\u91cd\u5efa\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u7684\u591a\u9636\u6bb5\u9884\u5904\u7406\u65b9\u6cd5\u9650\u5236\u4e86\u65f6\u95f4\u5206\u8fa8\u7387\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u52a8\u6001fMRI\u89e3\u7801\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86Dynadiff\uff0c\u4e00\u79cd\u5355\u9636\u6bb5\u6269\u6563\u6a21\u578b\uff0c\u76f4\u63a5\u5904\u7406\u52a8\u6001fMRI\u4fe1\u53f7\u3002", "result": "\u5728\u65f6\u95f4\u5206\u8fa8\u7387\u548c\u9ad8\u5c42\u6b21\u8bed\u4e49\u91cd\u5efa\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u540c\u65f6\u5728\u9759\u6001\u6570\u636e\u4e0a\u4fdd\u6301\u7ade\u4e89\u529b\u3002", "conclusion": "\u4e3a\u65f6\u95f4\u5206\u8fa8\u7684\u8111\u5230\u56fe\u50cf\u89e3\u7801\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2505.14179", "pdf": "https://arxiv.org/pdf/2505.14179", "abs": "https://arxiv.org/abs/2505.14179", "authors": ["Tong Bao", "Heng Zhang", "Chengzhi Zhang"], "title": "Enhancing Abstractive Summarization of Scientific Papers Using Structure Information", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Abstractive summarization of scientific papers has always been a research\nfocus, yet existing methods face two main challenges. First, most summarization\nmodels rely on Encoder-Decoder architectures that treat papers as sequences of\nwords, thus fail to fully capture the structured information inherent in\nscientific papers. Second, existing research often use keyword mapping or\nfeature engineering to identify the structural information, but these methods\nstruggle with the structural flexibility of scientific papers and lack\nrobustness across different disciplines. To address these challenges, we\npropose a two-stage abstractive summarization framework that leverages\nautomatic recognition of structural functions within scientific papers. In the\nfirst stage, we standardize chapter titles from numerous scientific papers and\nconstruct a large-scale dataset for structural function recognition. A\nclassifier is then trained to automatically identify the key structural\ncomponents (e.g., Background, Methods, Results, Discussion), which provides a\nfoundation for generating more balanced summaries. In the second stage, we\nemploy Longformer to capture rich contextual relationships across sections and\ngenerating context-aware summaries. Experiments conducted on two\ndomain-specific scientific paper summarization datasets demonstrate that our\nmethod outperforms advanced baselines, and generates more comprehensive\nsummaries. The code and dataset can be accessed at\nhttps://github.com/tongbao96/code-for-SFR-AS.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u62bd\u8c61\u6458\u8981\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u52a8\u8bc6\u522b\u79d1\u5b66\u8bba\u6587\u7684\u7ed3\u6784\u529f\u80fd\uff0c\u751f\u6210\u66f4\u5168\u9762\u7684\u6458\u8981\u3002", "motivation": "\u73b0\u6709\u6458\u8981\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u6355\u6349\u79d1\u5b66\u8bba\u6587\u7684\u7ed3\u6784\u4fe1\u606f\uff0c\u4e14\u7f3a\u4e4f\u8de8\u5b66\u79d1\u7684\u9c81\u68d2\u6027\u3002", "method": "\u4e24\u9636\u6bb5\u6846\u67b6\uff1a1) \u6784\u5efa\u7ed3\u6784\u529f\u80fd\u8bc6\u522b\u6570\u636e\u96c6\u5e76\u8bad\u7ec3\u5206\u7c7b\u5668\uff1b2) \u4f7f\u7528Longformer\u751f\u6210\u4e0a\u4e0b\u6587\u611f\u77e5\u6458\u8981\u3002", "result": "\u5728\u4e24\u4e2a\u9886\u57df\u7279\u5b9a\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u5148\u8fdb\u57fa\u7ebf\uff0c\u751f\u6210\u66f4\u5168\u9762\u7684\u6458\u8981\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u7ed3\u6784\u529f\u80fd\u8bc6\u522b\u548c\u4e0a\u4e0b\u6587\u5efa\u6a21\uff0c\u663e\u8457\u63d0\u5347\u4e86\u79d1\u5b66\u8bba\u6587\u6458\u8981\u7684\u8d28\u91cf\u3002"}}
{"id": "2505.14583", "pdf": "https://arxiv.org/pdf/2505.14583", "abs": "https://arxiv.org/abs/2505.14583", "authors": ["Abhimanyu Talwar", "Julien Laasri"], "title": "Instance Segmentation for Point Sets", "categories": ["cs.CV", "cs.LG", "68T45", "I.2.10"], "comment": "6 pages, 11 figures, paper dated 2019", "summary": "Recently proposed neural network architectures like PointNet [QSMG16] and\nPointNet++ [QYSG17] have made it possible to apply Deep Learning to 3D point\nsets. The feature representations of shapes learned by these two networks\nenabled training classifiers for Semantic Segmentation, and more recently for\nInstance Segmentation via the Similarity Group Proposal Network (SGPN)\n[WYHN17]. One area of improvement which has been highlighted by SGPN's authors,\npertains to use of memory intensive similarity matrices which occupy memory\nquadratic in the number of points. In this report, we attempt to tackle this\nissue through use of two sampling based methods, which compute Instance\nSegmentation on a sub-sampled Point Set, and then extrapolate labels to the\ncomplete set using the nearest neigbhour approach. While both approaches\nperform equally well on large sub-samples, the random-based strategy gives the\nmost improvements in terms of speed and memory usage.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e24\u79cd\u57fa\u4e8e\u91c7\u6837\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3SGPN\u4e2d\u5185\u5b58\u5bc6\u96c6\u578b\u76f8\u4f3c\u5ea6\u77e9\u9635\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u5b50\u91c7\u6837\u70b9\u96c6\u8fdb\u884c\u5b9e\u4f8b\u5206\u5272\uff0c\u5e76\u4f7f\u7528\u6700\u8fd1\u90bb\u65b9\u6cd5\u5c06\u6807\u7b7e\u6269\u5c55\u5230\u5b8c\u6574\u70b9\u96c6\u3002\u968f\u673a\u91c7\u6837\u7b56\u7565\u5728\u901f\u5ea6\u548c\u5185\u5b58\u4f7f\u7528\u4e0a\u8868\u73b0\u6700\u4f73\u3002", "motivation": "SGPN\u7b49\u7f51\u7edc\u5728\u5b9e\u4f8b\u5206\u5272\u4e2d\u4f7f\u7528\u5185\u5b58\u5bc6\u96c6\u578b\u76f8\u4f3c\u5ea6\u77e9\u9635\uff0c\u5bfc\u81f4\u5185\u5b58\u5360\u7528\u968f\u70b9\u6570\u5e73\u65b9\u589e\u957f\uff0c\u4e9f\u9700\u6539\u8fdb\u3002", "method": "\u91c7\u7528\u4e24\u79cd\u91c7\u6837\u65b9\u6cd5\uff1a\u5728\u5b50\u91c7\u6837\u70b9\u96c6\u4e0a\u8ba1\u7b97\u5b9e\u4f8b\u5206\u5272\uff0c\u518d\u901a\u8fc7\u6700\u8fd1\u90bb\u65b9\u6cd5\u5c06\u6807\u7b7e\u6269\u5c55\u5230\u5b8c\u6574\u70b9\u96c6\u3002", "result": "\u4e24\u79cd\u65b9\u6cd5\u5728\u5927\u5b50\u91c7\u6837\u96c6\u4e0a\u8868\u73b0\u76f8\u5f53\uff0c\u4f46\u968f\u673a\u91c7\u6837\u7b56\u7565\u5728\u901f\u5ea6\u548c\u5185\u5b58\u4f7f\u7528\u4e0a\u63d0\u5347\u6700\u663e\u8457\u3002", "conclusion": "\u968f\u673a\u91c7\u6837\u7b56\u7565\u662f\u89e3\u51b3\u5185\u5b58\u95ee\u9898\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u70b9\u96c6\u3002"}}
{"id": "2505.14181", "pdf": "https://arxiv.org/pdf/2505.14181", "abs": "https://arxiv.org/abs/2505.14181", "authors": ["Yunlong Liang", "Fandong Meng", "Jiaan Wang", "Jie Zhou"], "title": "SlangDIT: Benchmarking LLMs in Interpretative Slang Translation", "categories": ["cs.CL"], "comment": "work in progress", "summary": "The challenge of slang translation lies in capturing context-dependent\nsemantic extensions, as slang terms often convey meanings beyond their literal\ninterpretation. While slang detection, explanation, and translation have been\nstudied as isolated tasks in the era of large language models (LLMs), their\nintrinsic interdependence remains underexplored. The main reason is lacking of\na benchmark where the two tasks can be a prerequisite for the third one, which\ncan facilitate idiomatic translation. In this paper, we introduce the\ninterpretative slang translation task (named SlangDIT) consisting of three\nsub-tasks: slang detection, cross-lingual slang explanation, and slang\ntranslation within the current context, aiming to generate more accurate\ntranslation with the help of slang detection and slang explanation. To this\nend, we construct a SlangDIT dataset, containing over 25k English-Chinese\nsentence pairs. Each source sentence mentions at least one slang term and is\nlabeled with corresponding cross-lingual slang explanation. Based on the\nbenchmark, we propose a deep thinking model, named SlangOWL. It firstly\nidentifies whether the sentence contains a slang, and then judges whether the\nslang is polysemous and analyze its possible meaning. Further, the SlangOWL\nprovides the best explanation of the slang term targeting on the current\ncontext. Finally, according to the whole thought, the SlangOWL offers a\nsuitable translation. Our experiments on LLMs (\\emph{e.g.}, Qwen2.5 and\nLLama-3.1), show that our deep thinking approach indeed enhances the\nperformance of LLMs where the proposed SLangOWL significantly surpasses the\nvanilla models and supervised fine-tuned models without thinking.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aSlangDIT\u7684\u4efb\u52a1\uff0c\u7ed3\u5408\u4fda\u8bed\u68c0\u6d4b\u3001\u89e3\u91ca\u548c\u7ffb\u8bd1\uff0c\u5e76\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b25k\u82f1\u4e2d\u53e5\u5bf9\u7684\u6570\u636e\u96c6\u3002\u63d0\u51fa\u7684SlangOWL\u6a21\u578b\u901a\u8fc7\u6df1\u5ea6\u601d\u8003\u663e\u8457\u63d0\u5347\u4e86\u7ffb\u8bd1\u6027\u80fd\u3002", "motivation": "\u4fda\u8bed\u7ffb\u8bd1\u7684\u96be\u70b9\u5728\u4e8e\u6355\u6349\u5176\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u8bed\u4e49\u6269\u5c55\uff0c\u73b0\u6709\u7814\u7a76\u672a\u5145\u5206\u63a2\u7d22\u4fda\u8bed\u68c0\u6d4b\u3001\u89e3\u91ca\u548c\u7ffb\u8bd1\u4e4b\u95f4\u7684\u5185\u5728\u5173\u8054\u3002", "method": "\u63d0\u51faSlangDIT\u4efb\u52a1\uff0c\u5305\u542b\u4fda\u8bed\u68c0\u6d4b\u3001\u8de8\u8bed\u8a00\u89e3\u91ca\u548c\u7ffb\u8bd1\u4e09\u4e2a\u5b50\u4efb\u52a1\uff1b\u6784\u5efaSlangDIT\u6570\u636e\u96c6\uff1b\u8bbe\u8ba1SlangOWL\u6a21\u578b\uff0c\u901a\u8fc7\u6df1\u5ea6\u601d\u8003\u9010\u6b65\u5b8c\u6210\u4fda\u8bed\u8bc6\u522b\u3001\u591a\u4e49\u6027\u5224\u65ad\u3001\u89e3\u91ca\u751f\u6210\u548c\u7ffb\u8bd1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSlangOWL\u5728LLMs\uff08\u5982Qwen2.5\u548cLLama-3.1\uff09\u4e0a\u663e\u8457\u4f18\u4e8e\u666e\u901a\u6a21\u578b\u548c\u65e0\u601d\u8003\u7684\u76d1\u7763\u5fae\u8c03\u6a21\u578b\u3002", "conclusion": "SlangDIT\u4efb\u52a1\u548cSlangOWL\u6a21\u578b\u6709\u6548\u63d0\u5347\u4e86\u4fda\u8bed\u7ffb\u8bd1\u7684\u51c6\u786e\u6027\uff0c\u8bc1\u660e\u4e86\u6df1\u5ea6\u601d\u8003\u5728LLMs\u4e2d\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2505.14621", "pdf": "https://arxiv.org/pdf/2505.14621", "abs": "https://arxiv.org/abs/2505.14621", "authors": ["Abhimanyu Talwar", "Julien Laasri"], "title": "3D Reconstruction from Sketches", "categories": ["cs.CV", "cs.LG", "68T45", "I.2.10"], "comment": "6 pages, 8 figures, paper dated December 12, 2018", "summary": "We consider the problem of reconstructing a 3D scene from multiple sketches.\nWe propose a pipeline which involves (1) stitching together multiple sketches\nthrough use of correspondence points, (2) converting the stitched sketch into a\nrealistic image using a CycleGAN, and (3) estimating that image's depth-map\nusing a pre-trained convolutional neural network based architecture called\nMegaDepth. Our contribution includes constructing a dataset of image-sketch\npairs, the images for which are from the Zurich Building Database, and sketches\nhave been generated by us. We use this dataset to train a CycleGAN for our\npipeline's second step. We end up with a stitching process that does not\ngeneralize well to real drawings, but the rest of the pipeline that creates a\n3D reconstruction from a single sketch performs quite well on a wide variety of\ndrawings.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u591a\u5f20\u8349\u56fe\u91cd\u5efa3D\u573a\u666f\u7684\u6d41\u7a0b\uff0c\u5305\u62ec\u8349\u56fe\u62fc\u63a5\u3001CycleGAN\u8f6c\u6362\u548c\u6df1\u5ea6\u56fe\u4f30\u8ba1\u3002\u5c3d\u7ba1\u62fc\u63a5\u6548\u679c\u4e0d\u4f73\uff0c\u4f46\u5355\u8349\u56fe\u91cd\u5efa\u8868\u73b0\u826f\u597d\u3002", "motivation": "\u89e3\u51b3\u4ece\u8349\u56fe\u91cd\u5efa3D\u573a\u666f\u7684\u95ee\u9898\uff0c\u5c24\u5176\u662f\u9488\u5bf9\u591a\u5f20\u8349\u56fe\u7684\u62fc\u63a5\u548c\u5355\u8349\u56fe\u7684\u6df1\u5ea6\u4f30\u8ba1\u3002", "method": "1) \u901a\u8fc7\u5bf9\u5e94\u70b9\u62fc\u63a5\u8349\u56fe\uff1b2) \u7528CycleGAN\u5c06\u62fc\u63a5\u56fe\u8f6c\u4e3a\u771f\u5b9e\u56fe\u50cf\uff1b3) \u7528MegaDepth\u4f30\u8ba1\u6df1\u5ea6\u56fe\u3002", "result": "\u62fc\u63a5\u6548\u679c\u4e0d\u7406\u60f3\uff0c\u4f46\u5355\u8349\u56fe\u91cd\u5efa\u5728\u591a\u79cd\u7ed8\u56fe\u4e0a\u8868\u73b0\u826f\u597d\u3002", "conclusion": "\u6d41\u7a0b\u5728\u5355\u8349\u56fe\u91cd\u5efa\u4e0a\u6709\u6548\uff0c\u4f46\u591a\u8349\u56fe\u62fc\u63a5\u9700\u8fdb\u4e00\u6b65\u6539\u8fdb\u3002"}}
{"id": "2505.14183", "pdf": "https://arxiv.org/pdf/2505.14183", "abs": "https://arxiv.org/abs/2505.14183", "authors": ["Guosheng Liang", "Longguang Zhong", "Ziyi Yang", "Xiaojun Quan"], "title": "ThinkSwitcher: When to Think Hard, When to Think Fast", "categories": ["cs.CL"], "comment": null, "summary": "Large reasoning models (LRMs) excel at solving complex tasks by leveraging\nlong chain-of-thought (CoT) reasoning. However, this often leads to\noverthinking on simple tasks, resulting in unnecessary computational overhead.\nWe observe that LRMs inherently possess the capability for efficient short CoT\nreasoning, which can be reliably elicited through prompt design. To leverage\nthis capability, we propose ThinkSwitcher, a framework that enables a single\nLRM to dynamically switch between short and long CoT modes based on task\ncomplexity. ThinkSwitcher introduces a lightweight switching module trained\nwith supervision signals derived from the relative performance of each\nreasoning mode across tasks. Experiments on multiple reasoning benchmarks show\nthat ThinkSwitcher reduces computational cost by 20-30% while maintaining high\naccuracy on complex tasks. This demonstrates the effectiveness of ThinkSwitcher\nas a scalable and efficient solution for unified LRM deployment.", "AI": {"tldr": "ThinkSwitcher\u6846\u67b6\u901a\u8fc7\u52a8\u6001\u5207\u6362\u957f\u77ed\u94fe\u63a8\u7406\u6a21\u5f0f\uff0c\u964d\u4f4e\u8ba1\u7b97\u6210\u672c20-30%\uff0c\u540c\u65f6\u4fdd\u6301\u590d\u6742\u4efb\u52a1\u7684\u9ad8\u7cbe\u5ea6\u3002", "motivation": "\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u7b80\u5355\u4efb\u52a1\u4e0a\u8fc7\u5ea6\u63a8\u7406\u5bfc\u81f4\u8ba1\u7b97\u6d6a\u8d39\u3002", "method": "\u63d0\u51faThinkSwitcher\u6846\u67b6\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u5207\u6362\u6a21\u5757\u52a8\u6001\u9009\u62e9\u63a8\u7406\u6a21\u5f0f\uff0c\u57fa\u4e8e\u4efb\u52a1\u590d\u6742\u5ea6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cThinkSwitcher\u5728\u591a\u4e2a\u63a8\u7406\u57fa\u51c6\u4e0a\u51cf\u5c11\u8ba1\u7b97\u6210\u672c20-30%\uff0c\u4e14\u4e0d\u5f71\u54cd\u590d\u6742\u4efb\u52a1\u7cbe\u5ea6\u3002", "conclusion": "ThinkSwitcher\u662f\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u9ad8\u6548\u7684\u7edf\u4e00LRM\u90e8\u7f72\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.14634", "pdf": "https://arxiv.org/pdf/2505.14634", "abs": "https://arxiv.org/abs/2505.14634", "authors": ["Gokul Bhusal", "Yifei Lou", "Cristina Garcia-Cardona", "Ekaterina Merkurjev"], "title": "A General Framework for Group Sparsity in Hyperspectral Unmixing Using Endmember Bundles", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Due to low spatial resolution, hyperspectral data often consists of mixtures\nof contributions from multiple materials. This limitation motivates the task of\nhyperspectral unmixing (HU), a fundamental problem in hyperspectral imaging. HU\naims to identify the spectral signatures (\\textit{endmembers}) of the materials\npresent in an observed scene, along with their relative proportions\n(\\textit{fractional abundance}) in each pixel. A major challenge lies in the\nclass variability in materials, which hinders accurate representation by a\nsingle spectral signature, as assumed in the conventional linear mixing model.\nMoreover, To address this issue, we propose using group sparsity after\nrepresenting each material with a set of spectral signatures, known as\nendmember bundles, where each group corresponds to a specific material. In\nparticular, we develop a bundle-based framework that can enforce either\ninter-group sparsity or sparsity within and across groups (SWAG) on the\nabundance coefficients. Furthermore, our framework offers the flexibility to\nincorporate a variety of sparsity-promoting penalties, among which the\ntransformed $\\ell_1$ (TL1) penalty is a novel regularization in the HU\nliterature. Extensive experiments conducted on both synthetic and real\nhyperspectral data demonstrate the effectiveness and superiority of the\nproposed approaches.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7fa4\u7a00\u758f\u6027\u7684\u9ad8\u5149\u8c31\u89e3\u6df7\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f7f\u7528\u7aef\u5143\u675f\u8868\u793a\u6750\u6599\uff0c\u5e76\u5f15\u5165\u7ec4\u5185\u548c\u7ec4\u95f4\u7a00\u758f\u6027\uff08SWAG\uff09\u4ee5\u53caTL1\u60e9\u7f5a\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u9ad8\u5149\u8c31\u6570\u636e\u7531\u4e8e\u7a7a\u95f4\u5206\u8fa8\u7387\u4f4e\uff0c\u5e38\u5305\u542b\u591a\u79cd\u6750\u6599\u7684\u6df7\u5408\u4fe1\u53f7\uff0c\u4f20\u7edf\u7ebf\u6027\u6df7\u5408\u6a21\u578b\u65e0\u6cd5\u51c6\u786e\u8868\u793a\u6750\u6599\u7684\u7c7b\u5185\u53d8\u5f02\u6027\uff0c\u56e0\u6b64\u9700\u8981\u65b0\u7684\u89e3\u6df7\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7aef\u5143\u675f\u7684\u6846\u67b6\uff0c\u652f\u6301\u7ec4\u95f4\u7a00\u758f\u6027\u6216\u7ec4\u5185\u548c\u7ec4\u95f4\u7a00\u758f\u6027\uff08SWAG\uff09\uff0c\u5e76\u5f15\u5165\u4e86TL1\u60e9\u7f5a\u4f5c\u4e3a\u65b0\u7684\u6b63\u5219\u5316\u65b9\u6cd5\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u9ad8\u5149\u8c31\u6570\u636e\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u5177\u6709\u663e\u8457\u7684\u6709\u6548\u6027\u548c\u4f18\u8d8a\u6027\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u7aef\u5143\u675f\u548c\u7075\u6d3b\u7684\u7a00\u758f\u6027\u60e9\u7f5a\uff0c\u8be5\u65b9\u6cd5\u5728\u9ad8\u5149\u8c31\u89e3\u6df7\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u89e3\u51b3\u6750\u6599\u7c7b\u5185\u53d8\u5f02\u6027\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2505.14195", "pdf": "https://arxiv.org/pdf/2505.14195", "abs": "https://arxiv.org/abs/2505.14195", "authors": ["Tuc Nguyen", "Yifan Hu", "Thai Le"], "title": "Unraveling Interwoven Roles of Large Language Models in Authorship Privacy: Obfuscation, Mimicking, and Verification", "categories": ["cs.CL"], "comment": "17 pages, 3 figures", "summary": "Recent advancements in large language models (LLMs) have been fueled by large\nscale training corpora drawn from diverse sources such as websites, news\narticles, and books. These datasets often contain explicit user information,\nsuch as person names and addresses, that LLMs may unintentionally reproduce in\ntheir generated outputs. Beyond such explicit content, LLMs can also leak\nidentity revealing cues through implicit signals such as distinctive writing\nstyles, raising significant concerns about authorship privacy. There are three\nmajor automated tasks in authorship privacy, namely authorship obfuscation\n(AO), authorship mimicking (AM), and authorship verification (AV). Prior\nresearch has studied AO, AM, and AV independently. However, their interplays\nremain under explored, which leaves a major research gap, especially in the era\nof LLMs, where they are profoundly shaping how we curate and share user\ngenerated content, and the distinction between machine generated and human\nauthored text is also increasingly blurred. This work then presents the first\nunified framework for analyzing the dynamic relationships among LLM enabled AO,\nAM, and AV in the context of authorship privacy. We quantify how they interact\nwith each other to transform human authored text, examining effects at a single\npoint in time and iteratively over time. We also examine the role of\ndemographic metadata, such as gender, academic background, in modulating their\nperformances, inter-task dynamics, and privacy risks. All source code will be\npublicly available.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u751f\u6210\u6587\u672c\u65f6\u53ef\u80fd\u6cc4\u9732\u7528\u6237\u9690\u79c1\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u9996\u4e2a\u7edf\u4e00\u6846\u67b6\u5206\u6790\u4f5c\u8005\u9690\u79c1\u4e2d\u7684\u4e09\u79cd\u4efb\u52a1\uff08AO\u3001AM\u3001AV\uff09\u7684\u52a8\u6001\u5173\u7cfb\uff0c\u5e76\u7814\u7a76\u4e86\u4eba\u53e3\u7edf\u8ba1\u5143\u6570\u636e\u5bf9\u5176\u6027\u80fd\u7684\u5f71\u54cd\u3002", "motivation": "LLMs\u5728\u751f\u6210\u6587\u672c\u65f6\u53ef\u80fd\u6cc4\u9732\u7528\u6237\u9690\u79c1\uff0c\u5305\u62ec\u663e\u5f0f\u4fe1\u606f\uff08\u5982\u59d3\u540d\u3001\u5730\u5740\uff09\u548c\u9690\u5f0f\u4fe1\u53f7\uff08\u5982\u5199\u4f5c\u98ce\u683c\uff09\u3002\u76ee\u524d\u5bf9\u4f5c\u8005\u9690\u79c1\u7684\u4e09\u79cd\u4efb\u52a1\uff08AO\u3001AM\u3001AV\uff09\u7684\u7814\u7a76\u72ec\u7acb\u8fdb\u884c\uff0c\u7f3a\u4e4f\u5bf9\u5176\u4ea4\u4e92\u5173\u7cfb\u7684\u63a2\u7d22\u3002", "method": "\u63d0\u51fa\u4e86\u9996\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u91cf\u5316\u5206\u6790LLM\u652f\u6301\u7684AO\u3001AM\u548cAV\u4e4b\u95f4\u7684\u52a8\u6001\u5173\u7cfb\uff0c\u7814\u7a76\u5176\u5bf9\u4eba\u7c7b\u64b0\u5199\u6587\u672c\u7684\u8f6c\u5316\u6548\u679c\uff0c\u5e76\u8003\u5bdf\u4eba\u53e3\u7edf\u8ba1\u5143\u6570\u636e\uff08\u5982\u6027\u522b\u3001\u5b66\u672f\u80cc\u666f\uff09\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u53d1\u73b0LLMs\u5728\u4f5c\u8005\u9690\u79c1\u4efb\u52a1\u4e2d\u7684\u4ea4\u4e92\u5173\u7cfb\u663e\u8457\uff0c\u4eba\u53e3\u7edf\u8ba1\u5143\u6570\u636e\u5bf9\u4efb\u52a1\u6027\u80fd\u548c\u9690\u79c1\u98ce\u9669\u6709\u8c03\u8282\u4f5c\u7528\u3002", "conclusion": "\u8bba\u6587\u586b\u8865\u4e86LLM\u65f6\u4ee3\u4e0b\u4f5c\u8005\u9690\u79c1\u7814\u7a76\u7684\u7a7a\u767d\uff0c\u4e3a\u672a\u6765\u9690\u79c1\u4fdd\u62a4\u6280\u672f\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2505.14638", "pdf": "https://arxiv.org/pdf/2505.14638", "abs": "https://arxiv.org/abs/2505.14638", "authors": ["Tomer Gafni", "Asaf Karnieli", "Yair Hanani"], "title": "Dual Precision Quantization for Efficient and Accurate Deep Neural Networks Inference", "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": "Accepted at eLVM Workshop, CVPR, 2025", "summary": "Deep neural networks have achieved state-of-the-art results in a wide range\nof applications, from natural language processing and computer vision to speech\nrecognition. However, as tasks become increasingly complex, model sizes\ncontinue to grow, posing challenges in latency and memory efficiency. To meet\nthese constraints, post-training quantization has emerged as a promising\nsolution. In this paper, we propose a novel hardware-efficient quantization and\ninference scheme that exploits hardware advantages with minimal accuracy\ndegradation. Specifically, we introduce a W4A8 scheme, where weights are\nquantized and stored using 4-bit integer precision, and inference computations\nare performed using 8-bit floating-point arithmetic, demonstrating significant\nspeedups and improved memory utilization compared to 16-bit operations,\napplicable on various modern accelerators. To mitigate accuracy loss, we\ndevelop a novel quantization algorithm, dubbed Dual Precision Quantization\n(DPQ), that leverages the unique structure of our scheme without introducing\nadditional inference overhead. Experimental results demonstrate improved\nperformance (i.e., increased throughput) while maintaining tolerable accuracy\ndegradation relative to the full-precision model.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u786c\u4ef6\u9ad8\u6548\u7684\u91cf\u5316\u63a8\u7406\u65b9\u6848W4A8\uff0c\u7ed3\u54084\u4f4d\u6574\u6570\u6743\u91cd\u548c8\u4f4d\u6d6e\u70b9\u8ba1\u7b97\uff0c\u663e\u8457\u63d0\u5347\u901f\u5ea6\u548c\u5185\u5b58\u5229\u7528\u7387\uff0c\u540c\u65f6\u901a\u8fc7\u53cc\u7cbe\u5ea6\u91cf\u5316\u7b97\u6cd5\uff08DPQ\uff09\u51cf\u5c11\u7cbe\u5ea6\u635f\u5931\u3002", "motivation": "\u968f\u7740\u4efb\u52a1\u590d\u6742\u5ea6\u589e\u52a0\uff0c\u6a21\u578b\u89c4\u6a21\u589e\u957f\u5bfc\u81f4\u5ef6\u8fdf\u548c\u5185\u5b58\u6548\u7387\u95ee\u9898\uff0c\u540e\u8bad\u7ec3\u91cf\u5316\u6210\u4e3a\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528W4A8\u65b9\u6848\uff084\u4f4d\u6574\u6570\u6743\u91cd\u548c8\u4f4d\u6d6e\u70b9\u8ba1\u7b97\uff09\uff0c\u5e76\u5f00\u53d1\u53cc\u7cbe\u5ea6\u91cf\u5316\u7b97\u6cd5\uff08DPQ\uff09\u4ee5\u51cf\u5c11\u7cbe\u5ea6\u635f\u5931\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u663e\u8457\u63d0\u5347\u541e\u5410\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u53ef\u63a5\u53d7\u7684\u7cbe\u5ea6\u635f\u5931\u3002", "conclusion": "\u8be5\u65b9\u6848\u5728\u591a\u79cd\u73b0\u4ee3\u52a0\u901f\u5668\u4e0a\u6709\u6548\u5e73\u8861\u4e86\u6027\u80fd\u548c\u7cbe\u5ea6\u3002"}}
{"id": "2505.14212", "pdf": "https://arxiv.org/pdf/2505.14212", "abs": "https://arxiv.org/abs/2505.14212", "authors": ["Sizhe Yuen", "Ting Su", "Ziyang Wang", "Yali Du", "Adam J. Sobey"], "title": "Automatic Dataset Generation for Knowledge Intensive Question Answering Tasks", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "A question-answering (QA) system is to search suitable answers within a\nknowledge base. Current QA systems struggle with queries requiring complex\nreasoning or real-time knowledge integration. They are often supplemented with\nretrieval techniques on a data source such as Retrieval-Augmented Generation\n(RAG). However, RAG continues to face challenges in handling complex reasoning\nand logical connections between multiple sources of information. A novel\napproach for enhancing Large Language Models (LLMs) in knowledge-intensive QA\ntasks is presented through the automated generation of context-based QA pairs.\nThis methodology leverages LLMs to create fine-tuning data, reducing reliance\non human labelling and improving model comprehension and reasoning\ncapabilities. The proposed system includes an automated QA generator and a\nmodel fine-tuner, evaluated using perplexity, ROUGE, BLEU, and BERTScore.\nComprehensive experiments demonstrate improvements in logical coherence and\nfactual accuracy, with implications for developing adaptable Artificial\nIntelligence (AI) systems. Mistral-7b-v0.3 outperforms Llama-3-8b with BERT F1,\nBLEU, and ROUGE scores 0.858, 0.172, and 0.260 of for the LLM generated QA\npairs compared to scores of 0.836, 0.083, and 0.139 for the human annotated QA\npairs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u81ea\u52a8\u751f\u6210\u57fa\u4e8e\u4e0a\u4e0b\u6587\u7684QA\u5bf9\u6765\u589e\u5f3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u77e5\u8bc6\u5bc6\u96c6\u578b\u95ee\u7b54\u4efb\u52a1\u4e2d\u7684\u65b9\u6cd5\uff0c\u51cf\u5c11\u4e86\u4eba\u5de5\u6807\u6ce8\u7684\u4f9d\u8d56\u5e76\u63d0\u5347\u4e86\u6a21\u578b\u7684\u7406\u89e3\u548c\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u95ee\u7b54\u7cfb\u7edf\u5728\u5904\u7406\u590d\u6742\u63a8\u7406\u6216\u5b9e\u65f6\u77e5\u8bc6\u6574\u5408\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u6280\u672f\u4ecd\u9762\u4e34\u591a\u6e90\u4fe1\u606f\u903b\u8f91\u8fde\u63a5\u7684\u6311\u6218\u3002", "method": "\u91c7\u7528\u81ea\u52a8\u5316QA\u751f\u6210\u5668\u548c\u6a21\u578b\u5fae\u8c03\u5668\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u5fae\u8c03\u6570\u636e\uff0c\u5e76\u901a\u8fc7\u56f0\u60d1\u5ea6\u3001ROUGE\u3001BLEU\u548cBERTScore\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u903b\u8f91\u8fde\u8d2f\u6027\u548c\u4e8b\u5b9e\u51c6\u786e\u6027\u4e0a\u6709\u6240\u63d0\u5347\uff0cMistral-7b-v0.3\u5728\u591a\u9879\u6307\u6807\u4e0a\u4f18\u4e8eLlama-3-8b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5f00\u53d1\u9002\u5e94\u6027\u5f3a\u7684AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6f5c\u5728\u4ef7\u503c\uff0c\u5c24\u5176\u5728\u51cf\u5c11\u4eba\u5de5\u6807\u6ce8\u548c\u63d0\u5347\u6a21\u578b\u63a8\u7406\u80fd\u529b\u65b9\u9762\u3002"}}
{"id": "2505.14640", "pdf": "https://arxiv.org/pdf/2505.14640", "abs": "https://arxiv.org/abs/2505.14640", "authors": ["Wentao Ma", "Weiming Ren", "Yiming Jia", "Zhuofeng Li", "Ping Nie", "Ge Zhang", "Wenhu Chen"], "title": "VideoEval-Pro: Robust and Realistic Long Video Understanding Evaluation", "categories": ["cs.CV"], "comment": "Dataset: https://huggingface.co/datasets/TIGER-Lab/VideoEval-Pro,\n  Project Webpage: https://tiger-ai-lab.github.io/VideoEval-Pro", "summary": "Large multimodal models (LMMs) have recently emerged as a powerful tool for\nlong video understanding (LVU), prompting the development of standardized LVU\nbenchmarks to evaluate their performance. However, our investigation reveals a\nrather sober lesson for existing LVU benchmarks. First, most existing\nbenchmarks rely heavily on multiple-choice questions (MCQs), whose evaluation\nresults are inflated due to the possibility of guessing the correct answer;\nSecond, a significant portion of questions in these benchmarks have strong\npriors to allow models to answer directly without even reading the input video.\nFor example, Gemini-1.5-Pro can achieve over 50\\% accuracy given a random frame\nfrom a long video on Video-MME. We also observe that increasing the number of\nframes does not necessarily lead to improvement on existing benchmarks, which\nis counterintuitive. As a result, the validity and robustness of current LVU\nbenchmarks are undermined, impeding a faithful assessment of LMMs' long-video\nunderstanding capability. To tackle this problem, we propose VideoEval-Pro, a\nrealistic LVU benchmark containing questions with open-ended short-answer,\nwhich truly require understanding the entire video. VideoEval-Pro assesses both\nsegment-level and full-video understanding through perception and reasoning\ntasks. By evaluating 21 proprietary and open-source video LMMs, we conclude the\nfollowing findings: (1) video LMMs show drastic performance ($>$25\\%) drops on\nopen-ended questions compared with MCQs; (2) surprisingly, higher MCQ scores do\nnot lead to higher open-ended scores on VideoEval-Pro; (3) compared to other\nMCQ benchmarks, VideoEval-Pro benefits more from increasing the number of input\nframes. Our results show that VideoEval-Pro offers a more realistic and\nreliable measure of long video understanding, providing a clearer view of\nprogress in this domain.", "AI": {"tldr": "\u8bba\u6587\u6307\u51fa\u73b0\u6709\u957f\u89c6\u9891\u7406\u89e3\uff08LVU\uff09\u57fa\u51c6\u6d4b\u8bd5\u5b58\u5728\u7f3a\u9677\uff0c\u5982\u8fc7\u5ea6\u4f9d\u8d56\u9009\u62e9\u9898\uff08MCQ\uff09\u548c\u95ee\u9898\u5148\u9a8c\u6027\uff0c\u5bfc\u81f4\u8bc4\u4f30\u7ed3\u679c\u5931\u771f\u3002\u4f5c\u8005\u63d0\u51faVideoEval-Pro\uff0c\u4e00\u4e2a\u66f4\u771f\u5b9e\u7684LVU\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7\u5f00\u653e\u5f0f\u95ee\u9898\u8bc4\u4f30\u6a21\u578b\u80fd\u529b\u3002", "motivation": "\u73b0\u6709LVU\u57fa\u51c6\u6d4b\u8bd5\u7684\u8bc4\u4f30\u7ed3\u679c\u56e0\u9009\u62e9\u9898\u548c\u95ee\u9898\u5148\u9a8c\u6027\u800c\u5931\u771f\uff0c\u65e0\u6cd5\u771f\u5b9e\u53cd\u6620\u6a21\u578b\u7684\u957f\u89c6\u9891\u7406\u89e3\u80fd\u529b\u3002", "method": "\u63d0\u51faVideoEval-Pro\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u5f00\u653e\u5f0f\u77ed\u7b54\u9898\uff0c\u8bc4\u4f30\u7247\u6bb5\u7ea7\u548c\u5168\u89c6\u9891\u7406\u89e3\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u89c6\u9891LMM\u5728\u5f00\u653e\u5f0f\u95ee\u9898\u4e0a\u8868\u73b0\u663e\u8457\u4e0b\u964d\uff08>25%\uff09\uff0c\u4e14MCQ\u9ad8\u5206\u4e0d\u610f\u5473\u7740\u5f00\u653e\u5f0f\u9ad8\u5206\u3002VideoEval-Pro\u66f4\u80fd\u4f53\u73b0\u8f93\u5165\u5e27\u6570\u589e\u52a0\u7684\u4f18\u52bf\u3002", "conclusion": "VideoEval-Pro\u63d0\u4f9b\u4e86\u66f4\u771f\u5b9e\u53ef\u9760\u7684LVU\u8bc4\u4f30\u65b9\u6cd5\uff0c\u6709\u52a9\u4e8e\u66f4\u6e05\u6670\u5730\u8861\u91cf\u8be5\u9886\u57df\u7684\u8fdb\u5c55\u3002"}}
{"id": "2505.14226", "pdf": "https://arxiv.org/pdf/2505.14226", "abs": "https://arxiv.org/abs/2505.14226", "authors": ["Darpan Aswal", "Siddharth D Jaiswal"], "title": "\"Haet Bhasha aur Diskrimineshun\": Phonetic Perturbations in Code-Mixed Hinglish to Red-Team LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have become increasingly powerful, with\nmultilingual and multimodal capabilities improving by the day. These models are\nbeing evaluated through audits, alignment studies and red-teaming efforts to\nexpose model vulnerabilities towards generating harmful, biased and unfair\ncontent. Existing red-teaming efforts have previously focused on the English\nlanguage, using fixed template-based attacks; thus, models continue to be\nsusceptible to multilingual jailbreaking strategies, especially in the\nmultimodal context. In this study, we introduce a novel strategy that leverages\ncode-mixing and phonetic perturbations to jailbreak LLMs for both text and\nimage generation tasks. We also introduce two new jailbreak strategies that\nshow higher effectiveness than baseline strategies. Our work presents a method\nto effectively bypass safety filters in LLMs while maintaining interpretability\nby applying phonetic misspellings to sensitive words in code-mixed prompts. Our\nnovel prompts achieve a 99% Attack Success Rate for text generation and 78% for\nimage generation, with Attack Relevance Rate of 100% for text generation and\n95% for image generation when using the phonetically perturbed code-mixed\nprompts. Our interpretability experiments reveal that phonetic perturbations\nimpact word tokenization, leading to jailbreak success. Our study motivates\nincreasing the focus towards more generalizable safety alignment for\nmultilingual multimodal models, especially in real-world settings wherein\nprompts can have misspelt words.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u4ee3\u7801\u6df7\u5408\u548c\u8bed\u97f3\u6270\u52a8\u7684\u65b0\u578b\u7b56\u7565\uff0c\u6210\u529f\u7834\u89e3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u5b89\u5168\u8fc7\u6ee4\u5668\uff0c\u5e76\u5728\u6587\u672c\u548c\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e2d\u53d6\u5f97\u9ad8\u653b\u51fb\u6210\u529f\u7387\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u82f1\u8bed\u8bed\u8a00\u548c\u56fa\u5b9a\u6a21\u677f\u653b\u51fb\u4e0a\uff0c\u800c\u6a21\u578b\u5728\u591a\u8bed\u8a00\u548c\u591a\u6a21\u6001\u73af\u5883\u4e0b\u4ecd\u6613\u53d7\u653b\u51fb\u3002\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63a2\u7d22\u66f4\u901a\u7528\u7684\u5b89\u5168\u5bf9\u9f50\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u4ee3\u7801\u6df7\u5408\u548c\u8bed\u97f3\u6270\u52a8\u751f\u6210\u65b0\u578b\u63d0\u793a\uff0c\u7ed5\u8fc7LLMs\u7684\u5b89\u5168\u8fc7\u6ee4\u5668\uff0c\u540c\u65f6\u4fdd\u6301\u53ef\u89e3\u91ca\u6027\u3002", "result": "\u65b0\u578b\u63d0\u793a\u5728\u6587\u672c\u751f\u6210\u4e2d\u8fbe\u523099%\u7684\u653b\u51fb\u6210\u529f\u7387\uff0c\u56fe\u50cf\u751f\u6210\u4e2d\u8fbe\u523078%\uff0c\u4e14\u653b\u51fb\u76f8\u5173\u6027\u9ad8\u3002\u8bed\u97f3\u6270\u52a8\u901a\u8fc7\u5f71\u54cd\u8bcd\u6807\u8bb0\u5316\u5b9e\u73b0\u7834\u89e3\u3002", "conclusion": "\u7814\u7a76\u547c\u5401\u52a0\u5f3a\u5bf9\u591a\u8bed\u8a00\u591a\u6a21\u6001\u6a21\u578b\u7684\u901a\u7528\u5b89\u5168\u5bf9\u9f50\uff0c\u5c24\u5176\u662f\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u53ef\u80fd\u5b58\u5728\u62fc\u5199\u9519\u8bef\u7684\u60c5\u51b5\u4e0b\u3002"}}
{"id": "2505.14646", "pdf": "https://arxiv.org/pdf/2505.14646", "abs": "https://arxiv.org/abs/2505.14646", "authors": ["Anna C. Doris", "Md Ferdous Alam", "Amin Heyrani Nobari", "Faez Ahmed"], "title": "CAD-Coder: An Open-Source Vision-Language Model for Computer-Aided Design Code Generation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Efficient creation of accurate and editable 3D CAD models is critical in\nengineering design, significantly impacting cost and time-to-market in product\ninnovation. Current manual workflows remain highly time-consuming and demand\nextensive user expertise. While recent developments in AI-driven CAD generation\nshow promise, existing models are limited by incomplete representations of CAD\noperations, inability to generalize to real-world images, and low output\naccuracy. This paper introduces CAD-Coder, an open-source Vision-Language Model\n(VLM) explicitly fine-tuned to generate editable CAD code (CadQuery Python)\ndirectly from visual input. Leveraging a novel dataset that we\ncreated--GenCAD-Code, consisting of over 163k CAD-model image and code\npairs--CAD-Coder outperforms state-of-the-art VLM baselines such as GPT-4.5 and\nQwen2.5-VL-72B, achieving a 100% valid syntax rate and the highest accuracy in\n3D solid similarity. Notably, our VLM demonstrates some signs of\ngeneralizability, successfully generating CAD code from real-world images and\nexecuting CAD operations unseen during fine-tuning. The performance and\nadaptability of CAD-Coder highlights the potential of VLMs fine-tuned on code\nto streamline CAD workflows for engineers and designers. CAD-Coder is publicly\navailable at: https://github.com/anniedoris/CAD-Coder.", "AI": {"tldr": "CAD-Coder\u662f\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u5f00\u6e90\u5de5\u5177\uff0c\u80fd\u591f\u76f4\u63a5\u4ece\u89c6\u89c9\u8f93\u5165\u751f\u6210\u53ef\u7f16\u8f91\u7684CAD\u4ee3\u7801\uff08CadQuery Python\uff09\uff0c\u663e\u8457\u63d0\u5347\u4e86CAD\u6a21\u578b\u751f\u6210\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u5f53\u524d\u624b\u52a8\u521b\u5efaCAD\u6a21\u578b\u7684\u5de5\u4f5c\u6d41\u7a0b\u8017\u65f6\u4e14\u9700\u8981\u4e13\u4e1a\u77e5\u8bc6\uff0c\u800c\u73b0\u6709AI\u9a71\u52a8\u7684CAD\u751f\u6210\u6a21\u578b\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5982\u64cd\u4f5c\u8868\u793a\u4e0d\u5b8c\u6574\u3001\u6cdb\u5316\u80fd\u529b\u5dee\u548c\u8f93\u51fa\u7cbe\u5ea6\u4f4e\u3002", "method": "\u901a\u8fc7\u65b0\u6784\u5efa\u7684\u6570\u636e\u96c6GenCAD-Code\uff08\u5305\u542b16.3\u4e07\u5bf9CAD\u6a21\u578b\u56fe\u50cf\u548c\u4ee3\u7801\uff09\uff0c\u5bf9VLM\u8fdb\u884c\u5fae\u8c03\uff0c\u751f\u6210\u53ef\u7f16\u8f91\u7684CAD\u4ee3\u7801\u3002", "result": "CAD-Coder\u5728\u8bed\u6cd5\u6709\u6548\u6027\uff08100%\uff09\u548c3D\u5b9e\u4f53\u76f8\u4f3c\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709VLM\u57fa\u7ebf\u6a21\u578b\uff08\u5982GPT-4.5\u548cQwen2.5-VL-72B\uff09\uff0c\u5e76\u80fd\u4ece\u771f\u5b9e\u56fe\u50cf\u751f\u6210CAD\u4ee3\u7801\u3002", "conclusion": "CAD-Coder\u5c55\u793a\u4e86VLM\u5728\u4f18\u5316CAD\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u5de5\u7a0b\u5e08\u548c\u8bbe\u8ba1\u5e08\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u5de5\u5177\u3002"}}
{"id": "2505.14233", "pdf": "https://arxiv.org/pdf/2505.14233", "abs": "https://arxiv.org/abs/2505.14233", "authors": ["Hakaze Cho", "Peng Luo", "Mariko Kato", "Rin Kaenbyou", "Naoya Inoue"], "title": "Mechanistic Fine-tuning for In-context Learning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "28 pages, 31 figures, 6 tables", "summary": "In-context Learning (ICL) utilizes structured demonstration-query inputs to\ninduce few-shot learning on Language Models (LMs), which are not originally\npre-trained on ICL-style data. To bridge the gap between ICL and pre-training,\nsome approaches fine-tune LMs on large ICL-style datasets by an end-to-end\nparadigm with massive computational costs. To reduce such costs, in this paper,\nwe propose Attention Behavior Fine-Tuning (ABFT), utilizing the previous\nfindings on the inner mechanism of ICL, building training objectives on the\nattention scores instead of the final outputs, to force the attention scores to\nfocus on the correct label tokens presented in the context and mitigate\nattention scores from the wrong label tokens. Our experiments on 9 modern LMs\nand 8 datasets empirically find that ABFT outperforms in performance,\nrobustness, unbiasedness, and efficiency, with only around 0.01% data cost\ncompared to the previous methods. Moreover, our subsequent analysis finds that\nthe end-to-end training objective contains the ABFT objective, suggesting the\nimplicit bias of ICL-style data to the emergence of induction heads. Our work\ndemonstrates the possibility of controlling specific module sequences within\nLMs to improve their behavior, opening up the future application of mechanistic\ninterpretability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u6ce8\u610f\u529b\u884c\u4e3a\u5fae\u8c03\uff08ABFT\uff09\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u8c03\u6574\u6ce8\u610f\u529b\u5206\u6570\u800c\u975e\u6700\u7ec8\u8f93\u51fa\u6765\u4f18\u5316\u8bed\u8a00\u6a21\u578b\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u529b\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u73b0\u6709\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u65b9\u6cd5\uff08ICL\uff09\u9700\u8981\u5927\u91cf\u8ba1\u7b97\u8d44\u6e90\u8fdb\u884c\u7aef\u5230\u7aef\u5fae\u8c03\uff0c\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5229\u7528\u6ce8\u610f\u529b\u673a\u5236\u7684\u5185\u5728\u673a\u5236\u51cf\u5c11\u8fd9\u79cd\u6210\u672c\u3002", "method": "ABFT\u65b9\u6cd5\u901a\u8fc7\u4f18\u5316\u6ce8\u610f\u529b\u5206\u6570\uff0c\u4f7f\u5176\u4e13\u6ce8\u4e8e\u6b63\u786e\u7684\u6807\u7b7e\u6807\u8bb0\uff0c\u540c\u65f6\u51cf\u5c11\u5bf9\u9519\u8bef\u6807\u7b7e\u6807\u8bb0\u7684\u5173\u6ce8\u3002", "result": "\u57289\u79cd\u73b0\u4ee3\u8bed\u8a00\u6a21\u578b\u548c8\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cABFT\u5728\u6027\u80fd\u3001\u9c81\u68d2\u6027\u3001\u65e0\u504f\u6027\u548c\u6548\u7387\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u4ec5\u9700\u7ea60.01%\u7684\u6570\u636e\u6210\u672c\u3002", "conclusion": "ABFT\u5c55\u793a\u4e86\u901a\u8fc7\u63a7\u5236\u8bed\u8a00\u6a21\u578b\u5185\u90e8\u6a21\u5757\u5e8f\u5217\u6765\u6539\u8fdb\u884c\u4e3a\u7684\u53ef\u80fd\u6027\uff0c\u4e3a\u673a\u5236\u89e3\u91ca\u6027\u7684\u672a\u6765\u5e94\u7528\u5f00\u8f9f\u4e86\u9053\u8def\u3002"}}
{"id": "2505.14654", "pdf": "https://arxiv.org/pdf/2505.14654", "abs": "https://arxiv.org/abs/2505.14654", "authors": ["Zikai Liao", "Yi Ouyang", "Yi-Lun Lee", "Chen-Ping Yu", "Yi-Hsuan Tsai", "Zhaozheng Yin"], "title": "Beyond Words: Multimodal LLM Knows When to Speak", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Project page: https://github.com/lzk901372/MM-When2Speak", "summary": "While large language model (LLM)-based chatbots have demonstrated strong\ncapabilities in generating coherent and contextually relevant responses, they\noften struggle with understanding when to speak, particularly in delivering\nbrief, timely reactions during ongoing conversations. This limitation arises\nlargely from their reliance on text input, lacking the rich contextual cues in\nreal-world human dialogue. In this work, we focus on real-time prediction of\nresponse types, with an emphasis on short, reactive utterances that depend on\nsubtle, multimodal signals across vision, audio, and text. To support this, we\nintroduce a new multimodal dataset constructed from real-world conversational\nvideos, containing temporally aligned visual, auditory, and textual streams.\nThis dataset enables fine-grained modeling of response timing in dyadic\ninteractions. Building on this dataset, we propose MM-When2Speak, a multimodal\nLLM-based model that adaptively integrates visual, auditory, and textual\ncontext to predict when a response should occur, and what type of response is\nappropriate. Experiments show that MM-When2Speak significantly outperforms\nstate-of-the-art unimodal and LLM-based baselines, achieving up to a 4x\nimprovement in response timing accuracy over leading commercial LLMs. These\nresults underscore the importance of multimodal inputs for producing timely,\nnatural, and engaging conversational AI.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001LLM\u6a21\u578bMM-When2Speak\uff0c\u7528\u4e8e\u9884\u6d4b\u5bf9\u8bdd\u4e2d\u4f55\u65f6\u53ca\u5982\u4f55\u56de\u5e94\uff0c\u663e\u8457\u4f18\u4e8e\u5355\u6a21\u6001\u548c\u73b0\u6709LLM\u57fa\u7ebf\u3002", "motivation": "\u89e3\u51b3LLM\u804a\u5929\u673a\u5668\u4eba\u5728\u5b9e\u65f6\u5bf9\u8bdd\u4e2d\u96be\u4ee5\u628a\u63e1\u56de\u5e94\u65f6\u673a\u7684\u95ee\u9898\uff0c\u5c24\u5176\u662f\u4f9d\u8d56\u6587\u672c\u8f93\u5165\u800c\u5ffd\u7565\u591a\u6a21\u6001\u4e0a\u4e0b\u6587\u4fe1\u53f7\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u7ed3\u5408\u89c6\u89c9\u3001\u542c\u89c9\u548c\u6587\u672c\u4fe1\u606f\uff0c\u5e76\u63d0\u51fa\u4e86MM-When2Speak\u6a21\u578b\uff0c\u6574\u5408\u591a\u6a21\u6001\u4e0a\u4e0b\u6587\u9884\u6d4b\u56de\u5e94\u65f6\u673a\u548c\u7c7b\u578b\u3002", "result": "\u5b9e\u9a8c\u663e\u793aMM-When2Speak\u5728\u56de\u5e94\u65f6\u673a\u51c6\u786e\u6027\u4e0a\u6bd4\u73b0\u6709\u5546\u4e1aLLM\u63d0\u53474\u500d\u3002", "conclusion": "\u591a\u6a21\u6001\u8f93\u5165\u5bf9\u5b9e\u73b0\u81ea\u7136\u3001\u53ca\u65f6\u7684\u5bf9\u8bddAI\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2505.14238", "pdf": "https://arxiv.org/pdf/2505.14238", "abs": "https://arxiv.org/abs/2505.14238", "authors": ["Raghav Singhal", "Kaustubh Ponkshe", "Rohit Vartak", "Praneeth Vepakomma"], "title": "ABBA: Highly Expressive Hadamard Product Adaptation for Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Raghav Singhal, Kaustubh Ponkshe, and Rohit Vartak contributed\n  equally to this work", "summary": "Large Language Models have demonstrated strong performance across a wide\nrange of tasks, but adapting them efficiently to new domains remains a key\nchallenge. Parameter-Efficient Fine-Tuning (PEFT) methods address this by\nintroducing lightweight, trainable modules while keeping most pre-trained\nweights fixed. The prevailing approach, LoRA, models updates using a low-rank\ndecomposition, but its expressivity is inherently constrained by the rank.\nRecent methods like HiRA aim to increase expressivity by incorporating a\nHadamard product with the frozen weights, but still rely on the structure of\nthe pre-trained model. We introduce ABBA, a new PEFT architecture that\nreparameterizes the update as a Hadamard product of two independently learnable\nlow-rank matrices. In contrast to prior work, ABBA fully decouples the update\nfrom the pre-trained weights, enabling both components to be optimized freely.\nThis leads to significantly higher expressivity under the same parameter\nbudget. We formally analyze ABBA's expressive capacity and validate its\nadvantages through matrix reconstruction experiments. Empirically, ABBA\nachieves state-of-the-art results on arithmetic and commonsense reasoning\nbenchmarks, consistently outperforming existing PEFT methods by a significant\nmargin across multiple models. Our code is publicly available at:\nhttps://github.com/CERT-Lab/abba.", "AI": {"tldr": "ABBA\u662f\u4e00\u79cd\u65b0\u7684\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff08PEFT\uff09\u67b6\u6784\uff0c\u901a\u8fc7\u89e3\u8026\u9884\u8bad\u7ec3\u6743\u91cd\u548c\u66f4\u65b0\u77e9\u9635\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8868\u8fbe\u80fd\u529b\uff0c\u5e76\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8bb8\u591a\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u9ad8\u6548\u9002\u5e94\u65b0\u9886\u57df\u4ecd\u662f\u4e00\u4e2a\u6311\u6218\u3002\u73b0\u6709PEFT\u65b9\u6cd5\uff08\u5982LoRA\u548cHiRA\uff09\u7684\u8868\u8fbe\u80fd\u529b\u53d7\u9650\u4e8e\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u7ed3\u6784\u3002", "method": "ABBA\u901a\u8fc7\u5c06\u66f4\u65b0\u77e9\u9635\u91cd\u65b0\u53c2\u6570\u5316\u4e3a\u4e24\u4e2a\u72ec\u7acb\u53ef\u5b66\u4e60\u7684\u4f4e\u79e9\u77e9\u9635\u7684Hadamard\u79ef\uff0c\u5b8c\u5168\u89e3\u8026\u4e86\u66f4\u65b0\u4e0e\u9884\u8bad\u7ec3\u6743\u91cd\u3002", "result": "ABBA\u5728\u77e9\u9635\u91cd\u6784\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u9ad8\u8868\u8fbe\u80fd\u529b\uff0c\u5e76\u5728\u7b97\u672f\u548c\u5e38\u8bc6\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709PEFT\u65b9\u6cd5\u3002", "conclusion": "ABBA\u901a\u8fc7\u89e3\u8026\u8bbe\u8ba1\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u4e3a\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.14664", "pdf": "https://arxiv.org/pdf/2505.14664", "abs": "https://arxiv.org/abs/2505.14664", "authors": ["Yilin Ye", "Junchao Huang", "Xingchen Zeng", "Jiazhi Xia", "Wei Zeng"], "title": "AKRMap: Adaptive Kernel Regression for Trustworthy Visualization of Cross-Modal Embeddings", "categories": ["cs.CV", "cs.AI", "cs.HC", "cs.LG"], "comment": null, "summary": "Cross-modal embeddings form the foundation for multi-modal models. However,\nvisualization methods for interpreting cross-modal embeddings have been\nprimarily confined to traditional dimensionality reduction (DR) techniques like\nPCA and t-SNE. These DR methods primarily focus on feature distributions within\na single modality, whilst failing to incorporate metrics (e.g., CLIPScore)\nacross multiple modalities.This paper introduces AKRMap, a new DR technique\ndesigned to visualize cross-modal embeddings metric with enhanced accuracy by\nlearning kernel regression of the metric landscape in the projection space.\nSpecifically, AKRMap constructs a supervised projection network guided by a\npost-projection kernel regression loss, and employs adaptive generalized\nkernels that can be jointly optimized with the projection. This approach\nenables AKRMap to efficiently generate visualizations that capture complex\nmetric distributions, while also supporting interactive features such as zoom\nand overlay for deeper exploration. Quantitative experiments demonstrate that\nAKRMap outperforms existing DR methods in generating more accurate and\ntrustworthy visualizations. We further showcase the effectiveness of AKRMap in\nvisualizing and comparing cross-modal embeddings for text-to-image models. Code\nand demo are available at https://github.com/yilinye/AKRMap.", "AI": {"tldr": "AKRMap\u662f\u4e00\u79cd\u65b0\u7684\u964d\u7ef4\u6280\u672f\uff0c\u7528\u4e8e\u53ef\u89c6\u5316\u8de8\u6a21\u6001\u5d4c\u5165\uff0c\u901a\u8fc7\u5b66\u4e60\u6295\u5f71\u7a7a\u95f4\u4e2d\u5ea6\u91cf\u666f\u89c2\u7684\u6838\u56de\u5f52\uff0c\u63d0\u9ad8\u51c6\u786e\u6027\u3002", "motivation": "\u4f20\u7edf\u964d\u7ef4\u65b9\u6cd5\uff08\u5982PCA\u548ct-SNE\uff09\u4e3b\u8981\u5173\u6ce8\u5355\u6a21\u6001\u7279\u5f81\u5206\u5e03\uff0c\u672a\u80fd\u6709\u6548\u6574\u5408\u8de8\u6a21\u6001\u5ea6\u91cf\uff08\u5982CLIPScore\uff09\u3002", "method": "AKRMap\u901a\u8fc7\u6784\u5efa\u76d1\u7763\u6295\u5f71\u7f51\u7edc\uff0c\u5229\u7528\u540e\u6295\u5f71\u6838\u56de\u5f52\u635f\u5931\u548c\u81ea\u9002\u5e94\u5e7f\u4e49\u6838\uff0c\u8054\u5408\u4f18\u5316\u6295\u5f71\u8fc7\u7a0b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cAKRMap\u5728\u751f\u6210\u66f4\u51c6\u786e\u548c\u53ef\u4fe1\u7684\u53ef\u89c6\u5316\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "AKRMap\u80fd\u6709\u6548\u53ef\u89c6\u5316\u8de8\u6a21\u6001\u5d4c\u5165\uff0c\u652f\u6301\u4ea4\u4e92\u529f\u80fd\uff0c\u9002\u7528\u4e8e\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u7684\u6bd4\u8f83\u3002"}}
{"id": "2505.14242", "pdf": "https://arxiv.org/pdf/2505.14242", "abs": "https://arxiv.org/abs/2505.14242", "authors": ["Ziang Wang", "Amir Aryani"], "title": "Technical Report on classification of literature related to children speech disorder", "categories": ["cs.CL", "cs.IR", "cs.LG", "cs.SD", "eess.AS"], "comment": null, "summary": "This technical report presents a natural language processing (NLP)-based\napproach for systematically classifying scientific literature on childhood\nspeech disorders. We retrieved and filtered 4,804 relevant articles published\nafter 2015 from the PubMed database using domain-specific keywords. After\ncleaning and pre-processing the abstracts, we applied two topic modeling\ntechniques - Latent Dirichlet Allocation (LDA) and BERTopic - to identify\nlatent thematic structures in the corpus. Our models uncovered 14 clinically\nmeaningful clusters, such as infantile hyperactivity and abnormal epileptic\nbehavior. To improve relevance and precision, we incorporated a custom stop\nword list tailored to speech pathology. Evaluation results showed that the LDA\nmodel achieved a coherence score of 0.42 and a perplexity of -7.5, indicating\nstrong topic coherence and predictive performance. The BERTopic model exhibited\na low proportion of outlier topics (less than 20%), demonstrating its capacity\nto classify heterogeneous literature effectively. These results provide a\nfoundation for automating literature reviews in speech-language pathology.", "AI": {"tldr": "\u8be5\u6280\u672f\u62a5\u544a\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eNLP\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u7cfb\u7edf\u5206\u7c7b\u5173\u4e8e\u513f\u7ae5\u8a00\u8bed\u969c\u788d\u7684\u79d1\u5b66\u6587\u732e\u3002\u901a\u8fc7LDA\u548cBERTopic\u4e24\u79cd\u4e3b\u9898\u5efa\u6a21\u6280\u672f\uff0c\u8bc6\u522b\u51fa14\u4e2a\u4e34\u5e8a\u76f8\u5173\u4e3b\u9898\uff0c\u5e76\u5c55\u793a\u4e86\u6a21\u578b\u7684\u826f\u597d\u6027\u80fd\u3002", "motivation": "\u81ea\u52a8\u5316\u6587\u732e\u7efc\u8ff0\u5728\u8a00\u8bed\u75c5\u7406\u5b66\u9886\u57df\u7684\u9700\u6c42\uff0c\u4ee5\u63d0\u9ad8\u6548\u7387\u548c\u7cbe\u786e\u6027\u3002", "method": "\u4ecePubMed\u6570\u636e\u5e93\u4e2d\u68c0\u7d22\u5e76\u8fc7\u6ee44,804\u7bc7\u76f8\u5173\u6587\u732e\uff0c\u4f7f\u7528LDA\u548cBERTopic\u8fdb\u884c\u4e3b\u9898\u5efa\u6a21\uff0c\u5e76\u91c7\u7528\u5b9a\u5236\u505c\u7528\u8bcd\u5217\u8868\u4f18\u5316\u7ed3\u679c\u3002", "result": "LDA\u6a21\u578b\u7684\u8fde\u8d2f\u6027\u5f97\u5206\u4e3a0.42\uff0c\u56f0\u60d1\u5ea6\u4e3a-7.5\uff1bBERTopic\u6a21\u578b\u7684\u5f02\u5e38\u4e3b\u9898\u6bd4\u4f8b\u4f4e\u4e8e20%\uff0c\u8868\u660e\u5176\u6709\u6548\u5206\u7c7b\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u8a00\u8bed\u75c5\u7406\u5b66\u9886\u57df\u7684\u81ea\u52a8\u5316\u6587\u732e\u7efc\u8ff0\u63d0\u4f9b\u4e86\u53ef\u884c\u57fa\u7840\u3002"}}
{"id": "2505.14671", "pdf": "https://arxiv.org/pdf/2505.14671", "abs": "https://arxiv.org/abs/2505.14671", "authors": ["Ruichuan An", "Sihan Yang", "Renrui Zhang", "Zijun Shen", "Ming Lu", "Gaole Dai", "Hao Liang", "Ziyu Guo", "Shilin Yan", "Yulin Luo", "Bocheng Zou", "Chaoqun Yang", "Wentao Zhang"], "title": "UniCTokens: Boosting Personalized Understanding and Generation via Unified Concept Tokens", "categories": ["cs.CV"], "comment": null, "summary": "Personalized models have demonstrated remarkable success in understanding and\ngenerating concepts provided by users. However, existing methods use separate\nconcept tokens for understanding and generation, treating these tasks in\nisolation. This may result in limitations for generating images with complex\nprompts. For example, given the concept $\\langle bo\\rangle$, generating\n\"$\\langle bo\\rangle$ wearing its hat\" without additional textual descriptions\nof its hat. We call this kind of generation personalized knowledge-driven\ngeneration. To address the limitation, we present UniCTokens, a novel framework\nthat effectively integrates personalized information into a unified vision\nlanguage model (VLM) for understanding and generation. UniCTokens trains a set\nof unified concept tokens to leverage complementary semantics, boosting two\npersonalized tasks. Moreover, we propose a progressive training strategy with\nthree stages: understanding warm-up, bootstrapping generation from\nunderstanding, and deepening understanding from generation to enhance mutual\nbenefits between both tasks. To quantitatively evaluate the unified VLM\npersonalization, we present UnifyBench, the first benchmark for assessing\nconcept understanding, concept generation, and knowledge-driven generation.\nExperimental results on UnifyBench indicate that UniCTokens shows competitive\nperformance compared to leading methods in concept understanding, concept\ngeneration, and achieving state-of-the-art results in personalized\nknowledge-driven generation. Our research demonstrates that enhanced\nunderstanding improves generation, and the generation process can yield\nvaluable insights into understanding. Our code and dataset will be released at:\n\\href{https://github.com/arctanxarc/UniCTokens}{https://github.com/arctanxarc/UniCTokens}.", "AI": {"tldr": "UniCTokens\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u6982\u5ff5\u6807\u8bb0\u6846\u67b6\uff0c\u5c06\u4e2a\u6027\u5316\u4fe1\u606f\u6574\u5408\u5230\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\uff0c\u4ee5\u63d0\u5347\u7406\u89e3\u548c\u751f\u6210\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5c06\u6982\u5ff5\u7406\u89e3\u548c\u751f\u6210\u4efb\u52a1\u5206\u79bb\uff0c\u5bfc\u81f4\u590d\u6742\u63d0\u793a\u751f\u6210\u53d7\u9650\u3002UniCTokens\u65e8\u5728\u901a\u8fc7\u7edf\u4e00\u6807\u8bb0\u548c\u6e10\u8fdb\u8bad\u7ec3\u7b56\u7565\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "UniCTokens\u8bad\u7ec3\u7edf\u4e00\u7684\u6982\u5ff5\u6807\u8bb0\uff0c\u5e76\u91c7\u7528\u4e09\u9636\u6bb5\u6e10\u8fdb\u8bad\u7ec3\u7b56\u7565\uff1a\u7406\u89e3\u9884\u70ed\u3001\u4ece\u7406\u89e3\u5f15\u5bfc\u751f\u6210\u3001\u4ece\u751f\u6210\u6df1\u5316\u7406\u89e3\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cUniCTokens\u5728\u6982\u5ff5\u7406\u89e3\u3001\u751f\u6210\u53ca\u77e5\u8bc6\u9a71\u52a8\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5c24\u5176\u5728\u77e5\u8bc6\u9a71\u52a8\u751f\u6210\u4e0a\u8fbe\u5230SOTA\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u589e\u5f3a\u7406\u89e3\u53ef\u63d0\u5347\u751f\u6210\u8d28\u91cf\uff0c\u751f\u6210\u8fc7\u7a0b\u4e5f\u80fd\u53cd\u54fa\u7406\u89e3\u3002UniCTokens\u4e3a\u4e2a\u6027\u5316\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.14244", "pdf": "https://arxiv.org/pdf/2505.14244", "abs": "https://arxiv.org/abs/2505.14244", "authors": ["Haijun Li", "Tianqi Shi", "Zifu Shang", "Yuxuan Han", "Xueyu Zhao", "Hao Wang", "Yu Qian", "Zhiqiang Qian", "Linlong Xu", "Minghao Wu", "Chenyang Lyu", "Longyue Wang", "Gongbo Tang", "Weihua Luo", "Zhao Xu", "Kaifu Zhang"], "title": "TransBench: Benchmarking Machine Translation for Industrial-Scale Applications", "categories": ["cs.CL"], "comment": null, "summary": "Machine translation (MT) has become indispensable for cross-border\ncommunication in globalized industries like e-commerce, finance, and legal\nservices, with recent advancements in large language models (LLMs)\nsignificantly enhancing translation quality. However, applying general-purpose\nMT models to industrial scenarios reveals critical limitations due to\ndomain-specific terminology, cultural nuances, and stylistic conventions absent\nin generic benchmarks. Existing evaluation frameworks inadequately assess\nperformance in specialized contexts, creating a gap between academic benchmarks\nand real-world efficacy. To address this, we propose a three-level translation\ncapability framework: (1) Basic Linguistic Competence, (2) Domain-Specific\nProficiency, and (3) Cultural Adaptation, emphasizing the need for holistic\nevaluation across these dimensions. We introduce TransBench, a benchmark\ntailored for industrial MT, initially targeting international e-commerce with\n17,000 professionally translated sentences spanning 4 main scenarios and 33\nlanguage pairs. TransBench integrates traditional metrics (BLEU, TER) with\nMarco-MOS, a domain-specific evaluation model, and provides guidelines for\nreproducible benchmark construction. Our contributions include: (1) a\nstructured framework for industrial MT evaluation, (2) the first publicly\navailable benchmark for e-commerce translation, (3) novel metrics probing\nmulti-level translation quality, and (4) open-sourced evaluation tools. This\nwork bridges the evaluation gap, enabling researchers and practitioners to\nsystematically assess and enhance MT systems for industry-specific needs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u5de5\u4e1a\u673a\u5668\u7ffb\u8bd1\u7684\u4e09\u7ea7\u80fd\u529b\u8bc4\u4f30\u6846\u67b6\uff08\u57fa\u7840\u8bed\u8a00\u80fd\u529b\u3001\u9886\u57df\u4e13\u4e1a\u80fd\u529b\u3001\u6587\u5316\u9002\u5e94\u80fd\u529b\uff09\uff0c\u5e76\u63a8\u51fa\u4e86TransBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u586b\u8865\u4e86\u5b66\u672f\u8bc4\u4f30\u4e0e\u5b9e\u9645\u5de5\u4e1a\u9700\u6c42\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "motivation": "\u5de5\u4e1a\u573a\u666f\u4e2d\u901a\u7528\u673a\u5668\u7ffb\u8bd1\u6a21\u578b\u56e0\u7f3a\u4e4f\u9886\u57df\u672f\u8bed\u3001\u6587\u5316\u5dee\u5f02\u548c\u98ce\u683c\u89c4\u8303\u800c\u8868\u73b0\u4e0d\u4f73\uff0c\u73b0\u6709\u8bc4\u4f30\u6846\u67b6\u65e0\u6cd5\u6ee1\u8db3\u4e13\u4e1a\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u4e09\u7ea7\u7ffb\u8bd1\u80fd\u529b\u6846\u67b6\uff0c\u5f00\u53d1TransBench\u57fa\u51c6\uff0817,000\u53e5\u4e13\u4e1a\u7ffb\u8bd1\uff0c\u6db5\u76d64\u573a\u666f\u548c33\u8bed\u8a00\u5bf9\uff09\uff0c\u7ed3\u5408\u4f20\u7edf\u6307\u6807\u4e0e\u9886\u57df\u4e13\u7528\u6a21\u578bMarco-MOS\u3002", "result": "\u8d21\u732e\u5305\u62ec\u7ed3\u6784\u5316\u8bc4\u4f30\u6846\u67b6\u3001\u9996\u4e2a\u516c\u5f00\u7535\u5546\u7ffb\u8bd1\u57fa\u51c6\u3001\u591a\u7ea7\u8d28\u91cf\u6307\u6807\u53ca\u5f00\u6e90\u5de5\u5177\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u5de5\u4e1a\u673a\u5668\u7ffb\u8bd1\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u8bc4\u4f30\u65b9\u6cd5\uff0c\u63a8\u52a8\u9886\u57df\u4e13\u7528\u6a21\u578b\u7684\u4f18\u5316\u3002"}}
{"id": "2505.14673", "pdf": "https://arxiv.org/pdf/2505.14673", "abs": "https://arxiv.org/abs/2505.14673", "authors": ["Yu Tong", "Zihao Pan", "Shuai Yang", "Kaiyang Zhou"], "title": "Training-Free Watermarking for Autoregressive Image Generation", "categories": ["cs.CV", "cs.AI", "cs.CR"], "comment": null, "summary": "Invisible image watermarking can protect image ownership and prevent\nmalicious misuse of visual generative models. However, existing generative\nwatermarking methods are mainly designed for diffusion models while\nwatermarking for autoregressive image generation models remains largely\nunderexplored. We propose IndexMark, a training-free watermarking framework for\nautoregressive image generation models. IndexMark is inspired by the redundancy\nproperty of the codebook: replacing autoregressively generated indices with\nsimilar indices produces negligible visual differences. The core component in\nIndexMark is a simple yet effective match-then-replace method, which carefully\nselects watermark tokens from the codebook based on token similarity, and\npromotes the use of watermark tokens through token replacement, thereby\nembedding the watermark without affecting the image quality. Watermark\nverification is achieved by calculating the proportion of watermark tokens in\ngenerated images, with precision further improved by an Index Encoder.\nFurthermore, we introduce an auxiliary validation scheme to enhance robustness\nagainst cropping attacks. Experiments demonstrate that IndexMark achieves\nstate-of-the-art performance in terms of image quality and verification\naccuracy, and exhibits robustness against various perturbations, including\ncropping, noises, Gaussian blur, random erasing, color jittering, and JPEG\ncompression.", "AI": {"tldr": "\u63d0\u51faIndexMark\uff0c\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u81ea\u52a8\u56de\u5f52\u56fe\u50cf\u751f\u6210\u6a21\u578b\u6c34\u5370\u6846\u67b6\uff0c\u901a\u8fc7\u4ee3\u7801\u672c\u5197\u4f59\u7279\u6027\u5d4c\u5165\u6c34\u5370\uff0c\u4e0d\u5f71\u54cd\u56fe\u50cf\u8d28\u91cf\uff0c\u9a8c\u8bc1\u7cbe\u5ea6\u9ad8\u4e14\u6297\u5e72\u6270\u80fd\u529b\u5f3a\u3002", "motivation": "\u73b0\u6709\u751f\u6210\u6c34\u5370\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u6269\u6563\u6a21\u578b\uff0c\u81ea\u52a8\u56de\u5f52\u56fe\u50cf\u751f\u6210\u6a21\u578b\u7684\u6c34\u5370\u7814\u7a76\u4e0d\u8db3\uff0c\u9700\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u5229\u7528\u4ee3\u7801\u672c\u5197\u4f59\u7279\u6027\uff0c\u8bbe\u8ba1match-then-replace\u65b9\u6cd5\u9009\u62e9\u6c34\u5370\u6807\u8bb0\u5e76\u66ff\u6362\uff0c\u901a\u8fc7Index Encoder\u63d0\u5347\u9a8c\u8bc1\u7cbe\u5ea6\uff0c\u5f15\u5165\u8f85\u52a9\u9a8c\u8bc1\u65b9\u6848\u589e\u5f3a\u6297\u88c1\u526a\u653b\u51fb\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8868\u660eIndexMark\u5728\u56fe\u50cf\u8d28\u91cf\u548c\u9a8c\u8bc1\u7cbe\u5ea6\u4e0a\u8fbe\u5230\u6700\u4f18\uff0c\u4e14\u5bf9\u88c1\u526a\u3001\u566a\u58f0\u7b49\u591a\u79cd\u5e72\u6270\u5177\u6709\u9c81\u68d2\u6027\u3002", "conclusion": "IndexMark\u4e3a\u81ea\u52a8\u56de\u5f52\u56fe\u50cf\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u9c81\u68d2\u7684\u6c34\u5370\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.14256", "pdf": "https://arxiv.org/pdf/2505.14256", "abs": "https://arxiv.org/abs/2505.14256", "authors": ["Shaolin Zhu", "Tianyu Dong", "Bo Li", "Deyi Xiong"], "title": "FuxiMT: Sparsifying Large Language Models for Chinese-Centric Multilingual Machine Translation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In this paper, we present FuxiMT, a novel Chinese-centric multilingual\nmachine translation model powered by a sparsified large language model (LLM).\nWe adopt a two-stage strategy to train FuxiMT. We first pre-train the model on\na massive Chinese corpus and then conduct multilingual fine-tuning on a large\nparallel dataset encompassing 65 languages. FuxiMT incorporates\nMixture-of-Experts (MoEs) and employs a curriculum learning strategy for robust\nperformance across various resource levels. Experimental results demonstrate\nthat FuxiMT significantly outperforms strong baselines, including\nstate-of-the-art LLMs and machine translation models, particularly under\nlow-resource scenarios. Furthermore, FuxiMT exhibits remarkable zero-shot\ntranslation capabilities for unseen language pairs, indicating its potential to\nbridge communication gaps where parallel data are scarce or unavailable.", "AI": {"tldr": "FuxiMT\u662f\u4e00\u79cd\u57fa\u4e8e\u7a00\u758f\u5316\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u4e2d\u6587\u4e2d\u5fc3\u591a\u8bed\u8a00\u673a\u5668\u7ffb\u8bd1\u6a21\u578b\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff0c\u7ed3\u5408MoE\u548c\u8bfe\u7a0b\u5b66\u4e60\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u5c24\u5176\u5728\u4f4e\u8d44\u6e90\u573a\u666f\u548c\u96f6\u6837\u672c\u7ffb\u8bd1\u4e2d\u8868\u73b0\u7a81\u51fa\u3002", "motivation": "\u89e3\u51b3\u591a\u8bed\u8a00\u673a\u5668\u7ffb\u8bd1\u4e2d\u4f4e\u8d44\u6e90\u8bed\u8a00\u5bf9\u7ffb\u8bd1\u8d28\u91cf\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5e76\u63a2\u7d22\u7a00\u758f\u5316LLM\u5728\u7ffb\u8bd1\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u3002", "method": "\u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a\u5148\u5728\u4e2d\u6587\u8bed\u6599\u4e0a\u9884\u8bad\u7ec3\uff0c\u518d\u572865\u79cd\u8bed\u8a00\u7684\u5e73\u884c\u6570\u636e\u4e0a\u8fdb\u884c\u591a\u8bed\u8a00\u5fae\u8c03\uff0c\u7ed3\u5408MoE\u548c\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\u3002", "result": "FuxiMT\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u5c24\u5176\u5728\u4f4e\u8d44\u6e90\u573a\u666f\u548c\u96f6\u6837\u672c\u7ffb\u8bd1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "FuxiMT\u5c55\u793a\u4e86\u7a00\u758f\u5316LLM\u5728\u591a\u8bed\u8a00\u7ffb\u8bd1\u4e2d\u7684\u6f5c\u529b\uff0c\u5c24\u5176\u5728\u8d44\u6e90\u7a00\u7f3a\u6216\u65e0\u5e73\u884c\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u5177\u6709\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2505.14677", "pdf": "https://arxiv.org/pdf/2505.14677", "abs": "https://arxiv.org/abs/2505.14677", "authors": ["Jiaer Xia", "Yuhang Zang", "Peng Gao", "Yixuan Li", "Kaiyang Zhou"], "title": "Visionary-R1: Mitigating Shortcuts in Visual Reasoning with Reinforcement Learning", "categories": ["cs.CV"], "comment": null, "summary": "Learning general-purpose reasoning capabilities has long been a challenging\nproblem in AI. Recent research in large language models (LLMs), such as\nDeepSeek-R1, has shown that reinforcement learning techniques like GRPO can\nenable pre-trained LLMs to develop reasoning capabilities using simple\nquestion-answer pairs. In this paper, we aim to train visual language models\n(VLMs) to perform reasoning on image data through reinforcement learning and\nvisual question-answer pairs, without any explicit chain-of-thought (CoT)\nsupervision. Our findings indicate that simply applying reinforcement learning\nto a VLM -- by prompting the model to produce a reasoning chain before\nproviding an answer -- can lead the model to develop shortcuts from easy\nquestions, thereby reducing its ability to generalize across unseen data\ndistributions. We argue that the key to mitigating shortcut learning is to\nencourage the model to interpret images prior to reasoning. Therefore, we train\nthe model to adhere to a caption-reason-answer output format: initially\ngenerating a detailed caption for an image, followed by constructing an\nextensive reasoning chain. When trained on 273K CoT-free visual question-answer\npairs and using only reinforcement learning, our model, named Visionary-R1,\noutperforms strong multimodal models, such as GPT-4o, Claude3.5-Sonnet, and\nGemini-1.5-Pro, on multiple visual reasoning benchmarks.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u8fdb\u884c\u56fe\u50cf\u63a8\u7406\uff0c\u65e0\u9700\u663e\u5f0f\u601d\u7ef4\u94fe\u76d1\u7763\u3002\u7814\u7a76\u53d1\u73b0\u76f4\u63a5\u5e94\u7528\u5f3a\u5316\u5b66\u4e60\u4f1a\u5bfc\u81f4\u6a21\u578b\u8d70\u6377\u5f84\uff0c\u63d0\u51fa\u901a\u8fc7\u5148\u89e3\u91ca\u56fe\u50cf\u518d\u63a8\u7406\u7684\u65b9\u6cd5\uff08caption-reason-answer\u683c\u5f0f\uff09\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u3002\u5b9e\u9a8c\u8868\u660e\u6a21\u578bVisionary-R1\u5728\u591a\u4e2a\u89c6\u89c9\u63a8\u7406\u57fa\u51c6\u4e0a\u4f18\u4e8e\u4e3b\u6d41\u591a\u6a21\u6001\u6a21\u578b\u3002", "motivation": "\u89e3\u51b3AI\u4e2d\u901a\u7528\u63a8\u7406\u80fd\u529b\u7684\u6311\u6218\uff0c\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u56fe\u50cf\u63a8\u7406\uff0c\u907f\u514d\u663e\u5f0f\u601d\u7ef4\u94fe\u76d1\u7763\u3002", "method": "\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3VLM\uff0c\u91c7\u7528caption-reason-answer\u8f93\u51fa\u683c\u5f0f\uff1a\u9996\u5148\u751f\u6210\u8be6\u7ec6\u56fe\u50cf\u63cf\u8ff0\uff0c\u518d\u6784\u5efa\u63a8\u7406\u94fe\u3002\u5b9e\u9a8c\u57fa\u4e8e273K\u65e0\u601d\u7ef4\u94fe\u7684\u89c6\u89c9\u95ee\u7b54\u5bf9\u3002", "result": "\u6a21\u578bVisionary-R1\u5728\u591a\u4e2a\u89c6\u89c9\u63a8\u7406\u57fa\u51c6\u4e0a\u8868\u73b0\u4f18\u4e8eGPT-4o\u3001Claude3.5-Sonnet\u548cGemini-1.5-Pro\u7b49\u5f3a\u5927\u591a\u6a21\u6001\u6a21\u578b\u3002", "conclusion": "\u901a\u8fc7\u5148\u89e3\u91ca\u56fe\u50cf\u518d\u63a8\u7406\u7684\u65b9\u6cd5\u80fd\u6709\u6548\u907f\u514d\u6a21\u578b\u8d70\u6377\u5f84\uff0c\u63d0\u5347\u6cdb\u5316\u80fd\u529b\uff0c\u5f3a\u5316\u5b66\u4e60\u5728\u65e0\u663e\u5f0f\u76d1\u7763\u4e0b\u4ecd\u53ef\u8bad\u7ec3\u51fa\u9ad8\u6027\u80fd\u89c6\u89c9\u63a8\u7406\u6a21\u578b\u3002"}}
{"id": "2505.14268", "pdf": "https://arxiv.org/pdf/2505.14268", "abs": "https://arxiv.org/abs/2505.14268", "authors": ["Hui Huang", "Yancheng He", "Hongli Zhou", "Rui Zhang", "Wei Liu", "Weixun Wang", "Wenbo Su", "Bo Zheng", "Jiaheng Liu"], "title": "Think-J: Learning to Think for Generative LLM-as-a-Judge", "categories": ["cs.CL", "cs.AI"], "comment": "16 pages, 14 figures", "summary": "LLM-as-a-Judge refers to the automatic modeling of preferences for responses\ngenerated by Large Language Models (LLMs), which is of significant importance\nfor both LLM evaluation and reward modeling. Although generative LLMs have made\nsubstantial progress in various tasks, their performance as LLM-Judge still\nfalls short of expectations. In this work, we propose Think-J, which improves\ngenerative LLM-as-a-Judge by learning how to think. We first utilized a small\namount of curated data to develop the model with initial judgment thinking\ncapabilities. Subsequently, we optimize the judgment thinking traces based on\nreinforcement learning (RL). We propose two methods for judgment thinking\noptimization, based on offline and online RL, respectively. The offline RL\nrequires training a critic model to construct positive and negative examples\nfor learning. The online method defines rule-based reward as feedback for\noptimization. Experimental results showed that our approach can significantly\nenhance the evaluation capability of generative LLM-Judge, surpassing both\ngenerative and classifier-based LLM-Judge without requiring extra human\nannotations.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faThink-J\u65b9\u6cd5\uff0c\u901a\u8fc7\u8ba9\u751f\u6210\u5f0fLLM\u5b66\u4e60\u5982\u4f55\u601d\u8003\u6765\u63d0\u5347\u5176\u4f5c\u4e3aLLM-Judge\u7684\u80fd\u529b\uff0c\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u5224\u65ad\u601d\u7ef4\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bc4\u4f30\u80fd\u529b\u3002", "motivation": "\u751f\u6210\u5f0fLLM\u5728\u4f5c\u4e3aLLM-Judge\u65f6\u7684\u8868\u73b0\u672a\u8fbe\u9884\u671f\uff0c\u56e0\u6b64\u9700\u8981\u6539\u8fdb\u5176\u5224\u65ad\u80fd\u529b\u3002", "method": "\u9996\u5148\u5229\u7528\u5c11\u91cf\u7cbe\u9009\u6570\u636e\u5f00\u53d1\u521d\u6b65\u5224\u65ad\u601d\u7ef4\u6a21\u578b\uff0c\u968f\u540e\u57fa\u4e8e\u79bb\u7ebf\uff08\u8bad\u7ec3\u8bc4\u8bba\u6a21\u578b\uff09\u548c\u5728\u7ebf\uff08\u89c4\u5219\u5956\u52b1\uff09\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u5224\u65ad\u601d\u7ef4\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cThink-J\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u5f0fLLM-Judge\u7684\u8bc4\u4f30\u80fd\u529b\uff0c\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u4e14\u65e0\u9700\u989d\u5916\u4eba\u5de5\u6807\u6ce8\u3002", "conclusion": "Think-J\u901a\u8fc7\u4f18\u5316\u5224\u65ad\u601d\u7ef4\uff0c\u6709\u6548\u63d0\u5347\u4e86\u751f\u6210\u5f0fLLM\u4f5c\u4e3aLLM-Judge\u7684\u80fd\u529b\u3002"}}
{"id": "2505.14682", "pdf": "https://arxiv.org/pdf/2505.14682", "abs": "https://arxiv.org/abs/2505.14682", "authors": ["Rui Tian", "Mingfei Gao", "Mingze Xu", "Jiaming Hu", "Jiasen Lu", "Zuxuan Wu", "Yinfei Yang", "Afshin Dehghan"], "title": "UniGen: Enhanced Training & Test-Time Strategies for Unified Multimodal Understanding and Generation", "categories": ["cs.CV"], "comment": "Technical report", "summary": "We introduce UniGen, a unified multimodal large language model (MLLM) capable\nof image understanding and generation. We study the full training pipeline of\nUniGen from a data-centric perspective, including multi-stage pre-training,\nsupervised fine-tuning, and direct preference optimization. More importantly,\nwe propose a new Chain-of-Thought Verification (CoT-V) strategy for test-time\nscaling, which significantly boosts UniGen's image generation quality using a\nsimple Best-of-N test-time strategy. Specifically, CoT-V enables UniGen to act\nas both image generator and verifier at test time, assessing the semantic\nalignment between a text prompt and its generated image in a step-by-step CoT\nmanner. Trained entirely on open-source datasets across all stages, UniGen\nachieves state-of-the-art performance on a range of image understanding and\ngeneration benchmarks, with a final score of 0.78 on GenEval and 85.19 on\nDPG-Bench. Through extensive ablation studies, our work provides actionable\ninsights and addresses key challenges in the full life cycle of building\nunified MLLMs, contributing meaningful directions to the future research.", "AI": {"tldr": "UniGen\u662f\u4e00\u79cd\u7edf\u4e00\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5177\u5907\u56fe\u50cf\u7406\u89e3\u548c\u751f\u6210\u80fd\u529b\u3002\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u7684\u8bad\u7ec3\u6d41\u7a0b\u548c\u521b\u65b0\u7684Chain-of-Thought Verification\u7b56\u7565\uff0cUniGen\u5728\u591a\u9879\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u9886\u5148\u6c34\u5e73\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u6784\u5efa\u4e00\u4e2a\u7edf\u4e00\u7684MLLM\uff0c\u89e3\u51b3\u56fe\u50cf\u7406\u89e3\u4e0e\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u5e76\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u65b9\u5411\u3002", "method": "\u91c7\u7528\u591a\u9636\u6bb5\u9884\u8bad\u7ec3\u3001\u76d1\u7763\u5fae\u8c03\u548c\u76f4\u63a5\u504f\u597d\u4f18\u5316\u7684\u8bad\u7ec3\u6d41\u7a0b\uff0c\u5e76\u63d0\u51faCoT-V\u7b56\u7565\u4ee5\u63d0\u5347\u751f\u6210\u8d28\u91cf\u3002", "result": "UniGen\u5728GenEval\u548cDPG-Bench\u4e0a\u5206\u522b\u53d6\u5f970.78\u548c85.19\u7684\u5206\u6570\uff0c\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "UniGen\u901a\u8fc7\u5168\u9762\u7684\u8bad\u7ec3\u7b56\u7565\u548c\u521b\u65b0\u65b9\u6cd5\uff0c\u4e3a\u7edf\u4e00MLLM\u7684\u5f00\u53d1\u63d0\u4f9b\u4e86\u5b9e\u7528\u89c1\u89e3\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2505.14271", "pdf": "https://arxiv.org/pdf/2505.14271", "abs": "https://arxiv.org/abs/2505.14271", "authors": ["Minh Ngoc Ta", "Dong Cao Van", "Duc-Anh Hoang", "Minh Le-Anh", "Truong Nguyen", "My Anh Tran Nguyen", "Yuxia Wang", "Preslav Nakov", "Sang Dinh"], "title": "FAID: Fine-grained AI-generated Text Detection using Multi-task Auxiliary and Multi-level Contrastive Learning", "categories": ["cs.CL"], "comment": null, "summary": "The growing collaboration between humans and AI models in generative tasks\nhas introduced new challenges in distinguishing between human-written,\nAI-generated, and human-AI collaborative texts. In this work, we collect a\nmultilingual, multi-domain, multi-generator dataset FAIDSet. We further\nintroduce a fine-grained detection framework FAID to classify text into these\nthree categories, meanwhile identifying the underlying AI model family. Unlike\nexisting binary classifiers, FAID is built to capture both authorship and\nmodel-specific characteristics. Our method combines multi-level contrastive\nlearning with multi-task auxiliary classification to learn subtle stylistic\ncues. By modeling AI families as distinct stylistic entities, FAID offers\nimproved interpretability. We incorporate an adaptation to address\ndistributional shifts without retraining for unseen data. Experimental results\ndemonstrate that FAID outperforms several baseline approaches, particularly\nenhancing the generalization accuracy on unseen domains and new AI models. It\nprovide a potential solution for improving transparency and accountability in\nAI-assisted writing.", "AI": {"tldr": "FAIDSet\u6570\u636e\u96c6\u548cFAID\u6846\u67b6\u7528\u4e8e\u533a\u5206\u4eba\u7c7b\u3001AI\u751f\u6210\u53ca\u4eba\u673a\u534f\u4f5c\u6587\u672c\uff0c\u901a\u8fc7\u591a\u7ea7\u5bf9\u6bd4\u5b66\u4e60\u548c\u591a\u4efb\u52a1\u5206\u7c7b\u63d0\u5347\u68c0\u6d4b\u6027\u80fd\uff0c\u5e76\u5728\u672a\u89c1\u6570\u636e\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3\u4eba\u7c7b\u4e0eAI\u534f\u4f5c\u751f\u6210\u6587\u672c\u4e2d\u533a\u5206\u6765\u6e90\u7684\u6311\u6218\uff0c\u63d0\u5347\u900f\u660e\u5ea6\u548c\u8d23\u4efb\u8ffd\u6eaf\u3002", "method": "\u7ed3\u5408\u591a\u7ea7\u5bf9\u6bd4\u5b66\u4e60\u548c\u591a\u4efb\u52a1\u8f85\u52a9\u5206\u7c7b\uff0c\u5efa\u6a21AI\u6a21\u578b\u5bb6\u65cf\u4e3a\u72ec\u7279\u98ce\u683c\u5b9e\u4f53\uff0c\u9002\u5e94\u672a\u89c1\u6570\u636e\u5206\u5e03\u53d8\u5316\u3002", "result": "FAID\u5728\u672a\u89c1\u9886\u57df\u548c\u65b0AI\u6a21\u578b\u4e0a\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "FAID\u4e3aAI\u8f85\u52a9\u5199\u4f5c\u7684\u900f\u660e\u5ea6\u548c\u8d23\u4efb\u8ffd\u6eaf\u63d0\u4f9b\u4e86\u6f5c\u5728\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.14683", "pdf": "https://arxiv.org/pdf/2505.14683", "abs": "https://arxiv.org/abs/2505.14683", "authors": ["Chaorui Deng", "Deyao Zhu", "Kunchang Li", "Chenhui Gou", "Feng Li", "Zeyu Wang", "Shu Zhong", "Weihao Yu", "Xiaonan Nie", "Ziang Song", "Guang Shi", "Haoqi Fan"], "title": "Emerging Properties in Unified Multimodal Pretraining", "categories": ["cs.CV"], "comment": "37 pages, 17 figures", "summary": "Unifying multimodal understanding and generation has shown impressive\ncapabilities in cutting-edge proprietary systems. In this work, we introduce\nBAGEL, an open0source foundational model that natively supports multimodal\nunderstanding and generation. BAGEL is a unified, decoder0only model pretrained\non trillions of tokens curated from large0scale interleaved text, image, video,\nand web data. When scaled with such diverse multimodal interleaved data, BAGEL\nexhibits emerging capabilities in complex multimodal reasoning. As a result, it\nsignificantly outperforms open-source unified models in both multimodal\ngeneration and understanding across standard benchmarks, while exhibiting\nadvanced multimodal reasoning abilities such as free-form image manipulation,\nfuture frame prediction, 3D manipulation, and world navigation. In the hope of\nfacilitating further opportunities for multimodal research, we share the key\nfindings, pretraining details, data creation protocal, and release our code and\ncheckpoints to the community. The project page is at https://bagel-ai.org/", "AI": {"tldr": "BAGEL\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u57fa\u7840\u6a21\u578b\uff0c\u652f\u6301\u591a\u6a21\u6001\u7406\u89e3\u548c\u751f\u6210\uff0c\u901a\u8fc7\u5927\u89c4\u6a21\u591a\u6a21\u6001\u6570\u636e\u9884\u8bad\u7ec3\uff0c\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u7edf\u4e00\u591a\u6a21\u6001\u7406\u89e3\u548c\u751f\u6210\u7684\u9700\u6c42\uff0c\u63a8\u52a8\u5f00\u6e90\u793e\u533a\u5728\u591a\u6a21\u6001\u9886\u57df\u7684\u7814\u7a76\u3002", "method": "\u91c7\u7528\u7edf\u4e00\u7684\u89e3\u7801\u5668\u6a21\u578b\uff0c\u57fa\u4e8e\u5927\u89c4\u6a21\u6587\u672c\u3001\u56fe\u50cf\u3001\u89c6\u9891\u548c\u7f51\u9875\u6570\u636e\u8fdb\u884c\u9884\u8bad\u7ec3\u3002", "result": "\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u5f00\u6e90\u6a21\u578b\uff0c\u5c55\u73b0\u51fa\u590d\u6742\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "BAGEL\u4e3a\u591a\u6a21\u6001\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u673a\u4f1a\uff0c\u5e76\u516c\u5f00\u4e86\u4ee3\u7801\u548c\u6a21\u578b\u68c0\u67e5\u70b9\u3002"}}
{"id": "2505.14272", "pdf": "https://arxiv.org/pdf/2505.14272", "abs": "https://arxiv.org/abs/2505.14272", "authors": ["Faeze Ghorbanpour", "Daryna Dementieva", "Alexander Fraser"], "title": "Data-Efficient Hate Speech Detection via Cross-Lingual Nearest Neighbor Retrieval with Limited Labeled Data", "categories": ["cs.CL", "cs.CY", "cs.MM"], "comment": null, "summary": "Considering the importance of detecting hateful language, labeled hate speech\ndata is expensive and time-consuming to collect, particularly for low-resource\nlanguages. Prior work has demonstrated the effectiveness of cross-lingual\ntransfer learning and data augmentation in improving performance on tasks with\nlimited labeled data. To develop an efficient and scalable cross-lingual\ntransfer learning approach, we leverage nearest-neighbor retrieval to augment\nminimal labeled data in the target language, thereby enhancing detection\nperformance. Specifically, we assume access to a small set of labeled training\ninstances in the target language and use these to retrieve the most relevant\nlabeled examples from a large multilingual hate speech detection pool. We\nevaluate our approach on eight languages and demonstrate that it consistently\noutperforms models trained solely on the target language data. Furthermore, in\nmost cases, our method surpasses the current state-of-the-art. Notably, our\napproach is highly data-efficient, retrieving as small as 200 instances in some\ncases while maintaining superior performance. Moreover, it is scalable, as the\nretrieval pool can be easily expanded, and the method can be readily adapted to\nnew languages and tasks. We also apply maximum marginal relevance to mitigate\nredundancy and filter out highly similar retrieved instances, resulting in\nimprovements in some languages.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6700\u8fd1\u90bb\u68c0\u7d22\u7684\u8de8\u8bed\u8a00\u8fc1\u79fb\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u5e76\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u7531\u4e8e\u6807\u8bb0\u4ec7\u6068\u8a00\u8bba\u6570\u636e\u6210\u672c\u9ad8\u4e14\u8017\u65f6\uff0c\u5c24\u5176\u662f\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2d\uff0c\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\u6765\u63d0\u5347\u68c0\u6d4b\u6027\u80fd\u3002", "method": "\u5229\u7528\u6700\u8fd1\u90bb\u68c0\u7d22\u4ece\u591a\u8bed\u8a00\u4ec7\u6068\u8a00\u8bba\u6570\u636e\u6c60\u4e2d\u68c0\u7d22\u76f8\u5173\u6837\u672c\uff0c\u589e\u5f3a\u76ee\u6807\u8bed\u8a00\u7684\u5c11\u91cf\u6807\u8bb0\u6570\u636e\uff0c\u5e76\u7ed3\u5408\u6700\u5927\u8fb9\u9645\u76f8\u5173\u6027\u51cf\u5c11\u5197\u4f59\u3002", "result": "\u5728\u516b\u79cd\u8bed\u8a00\u4e0a\u9a8c\u8bc1\uff0c\u6027\u80fd\u4f18\u4e8e\u4ec5\u4f7f\u7528\u76ee\u6807\u8bed\u8a00\u6570\u636e\u7684\u6a21\u578b\uff0c\u5e76\u5728\u591a\u6570\u60c5\u51b5\u4e0b\u8d85\u8d8a\u5f53\u524d\u6700\u4f18\u65b9\u6cd5\uff0c\u4e14\u6570\u636e\u6548\u7387\u9ad8\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9ad8\u6548\u3001\u53ef\u6269\u5c55\uff0c\u9002\u7528\u4e8e\u65b0\u8bed\u8a00\u548c\u4efb\u52a1\uff0c\u540c\u65f6\u901a\u8fc7\u51cf\u5c11\u5197\u4f59\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u6027\u80fd\u3002"}}
{"id": "2505.14687", "pdf": "https://arxiv.org/pdf/2505.14687", "abs": "https://arxiv.org/abs/2505.14687", "authors": ["Sucheng Ren", "Qihang Yu", "Ju He", "Alan Yuille", "Liang-Chieh Chen"], "title": "Grouping First, Attending Smartly: Training-Free Acceleration for Diffusion Transformers", "categories": ["cs.CV"], "comment": "Project website at oliverrensu.github.io/project/GRAT", "summary": "Diffusion-based Transformers have demonstrated impressive generative\ncapabilities, but their high computational costs hinder practical deployment,\nfor example, generating an $8192\\times 8192$ image can take over an hour on an\nA100 GPU. In this work, we propose GRAT (\\textbf{GR}ouping first,\n\\textbf{AT}tending smartly), a training-free attention acceleration strategy\nfor fast image and video generation without compromising output quality. The\nkey insight is to exploit the inherent sparsity in learned attention maps\n(which tend to be locally focused) in pretrained Diffusion Transformers and\nleverage better GPU parallelism. Specifically, GRAT first partitions contiguous\ntokens into non-overlapping groups, aligning both with GPU execution patterns\nand the local attention structures learned in pretrained generative\nTransformers. It then accelerates attention by having all query tokens within\nthe same group share a common set of attendable key and value tokens. These key\nand value tokens are further restricted to structured regions, such as\nsurrounding blocks or criss-cross regions, significantly reducing computational\noverhead (e.g., attaining a \\textbf{35.8$\\times$} speedup over full attention\nwhen generating $8192\\times 8192$ images) while preserving essential attention\npatterns and long-range context. We validate GRAT on pretrained Flux and\nHunyuanVideo for image and video generation, respectively. In both cases, GRAT\nachieves substantially faster inference without any fine-tuning, while\nmaintaining the performance of full attention. We hope GRAT will inspire future\nresearch on accelerating Diffusion Transformers for scalable visual generation.", "AI": {"tldr": "GRAT\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u52a0\u901f\u6269\u6563Transformer\u6ce8\u610f\u529b\u8ba1\u7b97\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u7ec4\u548c\u7ed3\u6784\u5316\u533a\u57df\u9650\u5236\uff0c\u663e\u8457\u63d0\u5347\u56fe\u50cf\u548c\u89c6\u9891\u751f\u6210\u901f\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u8f93\u51fa\u8d28\u91cf\u3002", "motivation": "\u6269\u6563Transformer\u7684\u9ad8\u8ba1\u7b97\u6210\u672c\u9650\u5236\u4e86\u5176\u5b9e\u9645\u90e8\u7f72\uff0c\u4f8b\u5982\u751f\u6210\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u8017\u65f6\u8fc7\u957f\u3002", "method": "GRAT\u901a\u8fc7\u5206\u7ec4\u8fde\u7eed\u4ee4\u724c\u5e76\u9650\u5236\u6ce8\u610f\u529b\u533a\u57df\uff0c\u5229\u7528GPU\u5e76\u884c\u6027\u548c\u6ce8\u610f\u529b\u56fe\u7684\u7a00\u758f\u6027\uff0c\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u3002", "result": "GRAT\u5728\u751f\u62108192\u00d78192\u56fe\u50cf\u65f6\u5b9e\u73b0\u4e8635.8\u500d\u7684\u52a0\u901f\uff0c\u4e14\u65e0\u9700\u5fae\u8c03\u5373\u53ef\u4fdd\u6301\u6027\u80fd\u3002", "conclusion": "GRAT\u4e3a\u52a0\u901f\u6269\u6563Transformer\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u671b\u63a8\u52a8\u53ef\u6269\u5c55\u89c6\u89c9\u751f\u6210\u7684\u7814\u7a76\u3002"}}
{"id": "2505.14279", "pdf": "https://arxiv.org/pdf/2505.14279", "abs": "https://arxiv.org/abs/2505.14279", "authors": ["Jennifer D'Souza", "Hamed Babaei Giglou", "Quentin M\u00fcnch"], "title": "YESciEval: Robust LLM-as-a-Judge for Scientific Question Answering", "categories": ["cs.CL", "cs.AI"], "comment": "8 pages, 3 figures, Accepted as a Long Paper at the 63rd Annual\n  Meeting of the Association for Computational Linguistics (ACL 2025)", "summary": "Large Language Models (LLMs) drive scientific question-answering on modern\nsearch engines, yet their evaluation robustness remains underexplored. We\nintroduce YESciEval, an open-source framework that combines fine-grained\nrubric-based assessment with reinforcement learning to mitigate optimism bias\nin LLM evaluators. We release multidisciplinary scienceQ&A datasets, including\nadversarial variants, with evaluation scores from multiple LLMs. Independent of\nproprietary models and human feedback, our approach enables scalable, cost-free\nevaluation. By advancing reliable LLM-as-a-judge models, this work supports AI\nalignment and fosters robust, transparent evaluation essential for scientific\ninquiry and artificial general intelligence.", "AI": {"tldr": "YESciEval\u662f\u4e00\u4e2a\u5f00\u6e90\u6846\u67b6\uff0c\u7ed3\u5408\u7ec6\u7c92\u5ea6\u8bc4\u5206\u6807\u51c6\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u79d1\u5b66\u95ee\u7b54\u80fd\u529b\uff0c\u51cf\u5c11\u8bc4\u4f30\u4e2d\u7684\u4e50\u89c2\u504f\u5dee\u3002", "motivation": "\u5f53\u524dLLMs\u5728\u79d1\u5b66\u95ee\u7b54\u4e2d\u7684\u8bc4\u4f30\u9c81\u68d2\u6027\u4e0d\u8db3\uff0c\u7f3a\u4e4f\u900f\u660e\u4e14\u53ef\u6269\u5c55\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u63d0\u51faYESciEval\u6846\u67b6\uff0c\u7ed3\u5408\u8bc4\u5206\u6807\u51c6\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u5e76\u53d1\u5e03\u591a\u5b66\u79d1\u79d1\u5b66\u95ee\u7b54\u6570\u636e\u96c6\u53ca\u5bf9\u6297\u6027\u53d8\u4f53\u3002", "result": "\u5b9e\u73b0\u4e86\u72ec\u7acb\u4e8e\u4e13\u6709\u6a21\u578b\u548c\u4eba\u7c7b\u53cd\u9988\u7684\u53ef\u6269\u5c55\u3001\u514d\u8d39\u8bc4\u4f30\uff0c\u652f\u6301\u53ef\u9760\u7684LLM-as-a-judge\u6a21\u578b\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u63a8\u52a8\u4e86AI\u5bf9\u9f50\uff0c\u4e3a\u79d1\u5b66\u7814\u7a76\u548c\u901a\u7528\u4eba\u5de5\u667a\u80fd\u63d0\u4f9b\u4e86\u900f\u660e\u4e14\u9c81\u68d2\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002"}}
{"id": "2505.06699", "pdf": "https://arxiv.org/pdf/2505.06699", "abs": "https://arxiv.org/abs/2505.06699", "authors": ["Xiyuan Wei", "Ming Lin", "Fanjiang Ye", "Fengguang Song", "Liangliang Cao", "My T. Thai", "Tianbao Yang"], "title": "Model Steering: Learning with a Reference Model Improves Generalization Bounds and Scaling Laws", "categories": ["cs.LG", "cs.AI", "cs.CV", "stat.ML"], "comment": "18 pages, 6 figures", "summary": "This paper formalizes an emerging learning paradigm that uses a trained model\nas a reference to guide and enhance the training of a target model through\nstrategic data selection or weighting, named $\\textbf{model steering}$. While\nad-hoc methods have been used in various contexts, including the training of\nlarge foundation models, its underlying principles remain insufficiently\nunderstood, leading to sub-optimal performance. In this work, we propose a\ntheory-driven framework for model steering called $\\textbf{DRRho risk\nminimization}$, which is rooted in Distributionally Robust Optimization (DRO).\nThrough a generalization analysis, we provide theoretical insights into why\nthis approach improves generalization and data efficiency compared to training\nwithout a reference model. To the best of our knowledge, this is the first time\nsuch theoretical insights are provided for the new learning paradigm, which\nsignificantly enhance our understanding and practice of model steering.\nBuilding on these insights and the connection between contrastive learning and\nDRO, we introduce a novel method for Contrastive Language-Image Pretraining\n(CLIP) with a reference model, termed DRRho-CLIP. Extensive experiments\nvalidate the theoretical insights, reveal a superior scaling law compared to\nCLIP without a reference model, and demonstrate its strength over existing\nheuristic approaches.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u6a21\u578b\u5f15\u5bfc\u201d\u7684\u5b66\u4e60\u8303\u5f0f\uff0c\u901a\u8fc7\u53c2\u8003\u6a21\u578b\u6307\u5bfc\u76ee\u6807\u6a21\u578b\u7684\u8bad\u7ec3\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5206\u5e03\u9c81\u68d2\u4f18\u5316\uff08DRO\uff09\u7684\u7406\u8bba\u6846\u67b6DRRho\u98ce\u9669\u6700\u5c0f\u5316\u3002\u8be5\u65b9\u6cd5\u5728\u7406\u8bba\u548c\u5b9e\u9a8c\u4e0a\u5747\u4f18\u4e8e\u65e0\u53c2\u8003\u6a21\u578b\u7684\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5e76\u9996\u6b21\u63d0\u4f9b\u4e86\u76f8\u5173\u7406\u8bba\u5206\u6790\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u5f15\u5bfc\u65b9\u6cd5\u7f3a\u4e4f\u7406\u8bba\u652f\u6301\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0d\u4f73\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u7406\u8bba\u6846\u67b6\u63d0\u5347\u6a21\u578b\u5f15\u5bfc\u7684\u7406\u89e3\u548c\u5b9e\u8df5\u6548\u679c\u3002", "method": "\u63d0\u51faDRRho\u98ce\u9669\u6700\u5c0f\u5316\u6846\u67b6\uff0c\u57fa\u4e8eDRO\u7406\u8bba\uff0c\u5e76\u7ed3\u5408\u5bf9\u6bd4\u5b66\u4e60\u4e0eDRO\u7684\u5173\u7cfb\uff0c\u8bbe\u8ba1\u4e86DRRho-CLIP\u65b9\u6cd5\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u548c\u6570\u636e\u6548\u7387\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u4f18\u4e8e\u65e0\u53c2\u8003\u6a21\u578b\u7684\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5e76\u5c55\u793a\u4e86\u66f4\u597d\u7684\u6269\u5c55\u6027\u3002", "conclusion": "DRRho\u6846\u67b6\u4e3a\u6a21\u578b\u5f15\u5bfc\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0cDRRho-CLIP\u65b9\u6cd5\u5728\u5b9e\u8df5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f18\u4e8e\u73b0\u6709\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002"}}
{"id": "2505.14286", "pdf": "https://arxiv.org/pdf/2505.14286", "abs": "https://arxiv.org/abs/2505.14286", "authors": ["Rao Ma", "Mengjie Qian", "Vyas Raina", "Mark Gales", "Kate Knill"], "title": "Universal Acoustic Adversarial Attacks for Flexible Control of Speech-LLMs", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "The combination of pre-trained speech encoders with large language models has\nenabled the development of speech LLMs that can handle a wide range of spoken\nlanguage processing tasks. While these models are powerful and flexible, this\nvery flexibility may make them more vulnerable to adversarial attacks. To\nexamine the extent of this problem, in this work we investigate universal\nacoustic adversarial attacks on speech LLMs. Here a fixed, universal,\nadversarial audio segment is prepended to the original input audio. We\ninitially investigate attacks that cause the model to either produce no output\nor to perform a modified task overriding the original prompt. We then extend\nthe nature of the attack to be selective so that it activates only when\nspecific input attributes, such as a speaker gender or spoken language, are\npresent. Inputs without the targeted attribute should be unaffected, allowing\nfine-grained control over the model outputs. Our findings reveal critical\nvulnerabilities in Qwen2-Audio and Granite-Speech and suggest that similar\nspeech LLMs may be susceptible to universal adversarial attacks. This\nhighlights the need for more robust training strategies and improved resistance\nto adversarial attacks.", "AI": {"tldr": "\u7814\u7a76\u4e86\u8bed\u97f3\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5bf9\u901a\u7528\u58f0\u5b66\u5bf9\u6297\u653b\u51fb\u7684\u8106\u5f31\u6027\uff0c\u53d1\u73b0Qwen2-Audio\u548cGranite-Speech\u7b49\u6a21\u578b\u5b58\u5728\u4e25\u91cd\u6f0f\u6d1e\uff0c\u9700\u6539\u8fdb\u8bad\u7ec3\u7b56\u7565\u3002", "motivation": "\u8bed\u97f3LLMs\u7684\u7075\u6d3b\u6027\u53ef\u80fd\u4f7f\u5176\u66f4\u6613\u53d7\u5bf9\u6297\u653b\u51fb\uff0c\u9700\u8bc4\u4f30\u5176\u8106\u5f31\u6027\u3002", "method": "\u901a\u8fc7\u5728\u539f\u59cb\u97f3\u9891\u524d\u6dfb\u52a0\u56fa\u5b9a\u7684\u5bf9\u6297\u97f3\u9891\u6bb5\uff0c\u7814\u7a76\u6a21\u578b\u7684\u65e0\u8f93\u51fa\u6216\u4efb\u52a1\u8986\u76d6\u884c\u4e3a\uff0c\u5e76\u6269\u5c55\u4e3a\u9009\u62e9\u6027\u653b\u51fb\u3002", "result": "\u53d1\u73b0Qwen2-Audio\u548cGranite-Speech\u7b49\u6a21\u578b\u5bf9\u901a\u7528\u5bf9\u6297\u653b\u51fb\u9ad8\u5ea6\u8106\u5f31\u3002", "conclusion": "\u8bed\u97f3LLMs\u9700\u66f4\u9c81\u68d2\u7684\u8bad\u7ec3\u7b56\u7565\u4ee5\u63d0\u9ad8\u5bf9\u6297\u653b\u51fb\u7684\u62b5\u6297\u529b\u3002"}}
{"id": "2505.13462", "pdf": "https://arxiv.org/pdf/2505.13462", "abs": "https://arxiv.org/abs/2505.13462", "authors": ["Thien Nguyen", "William Guicquero"], "title": "End-to-end fully-binarized network design: from Generic Learned Thermometer to Block Pruning", "categories": ["cs.LG", "cs.AR", "cs.CV", "eess.IV", "stat.ML"], "comment": "Accepted to IEEE AICAS 2025", "summary": "Existing works on Binary Neural Network (BNN) mainly focus on model's weights\nand activations while discarding considerations on the input raw data. This\narticle introduces Generic Learned Thermometer (GLT), an encoding technique to\nimprove input data representation for BNN, relying on learning non linear\nquantization thresholds. This technique consists in multiple data binarizations\nwhich can advantageously replace a conventional Analog to Digital Conversion\n(ADC) that uses natural binary coding. Additionally, we jointly propose a\ncompact topology with light-weight grouped convolutions being trained thanks to\nblock pruning and Knowledge Distillation (KD), aiming at reducing furthermore\nthe model size so as its computational complexity. We show that GLT brings\nversatility to the BNN by intrinsically performing global tone mapping,\nenabling significant accuracy gains in practice (demonstrated by simulations on\nthe STL-10 and VWW datasets). Moreover, when combining GLT with our proposed\nblock-pruning technique, we successfully achieve lightweight (under 1Mb),\nfully-binarized models with limited accuracy degradation while being suitable\nfor in-sensor always-on inference use cases.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGLT\u7684\u7f16\u7801\u6280\u672f\uff0c\u901a\u8fc7\u975e\u7ebf\u6027\u91cf\u5316\u9608\u503c\u6539\u8fdbBNN\u7684\u8f93\u5165\u6570\u636e\u8868\u793a\uff0c\u5e76\u7ed3\u5408\u8f7b\u91cf\u7ea7\u5206\u7ec4\u5377\u79ef\u548c\u5757\u526a\u679d\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u8f7b\u91cf\u7ea7\u5168\u4e8c\u503c\u5316\u6a21\u578b\u3002", "motivation": "\u73b0\u6709BNN\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u6a21\u578b\u6743\u91cd\u548c\u6fc0\u6d3b\uff0c\u800c\u5ffd\u7565\u4e86\u8f93\u5165\u539f\u59cb\u6570\u636e\u7684\u4f18\u5316\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u6539\u8fdb\u8f93\u5165\u6570\u636e\u8868\u793a\u548c\u6a21\u578b\u7ed3\u6784\uff0c\u63d0\u5347BNN\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "method": "\u63d0\u51faGLT\u7f16\u7801\u6280\u672f\uff0c\u5229\u7528\u975e\u7ebf\u6027\u91cf\u5316\u9608\u503c\u4f18\u5316\u8f93\u5165\u6570\u636e\u8868\u793a\uff1b\u7ed3\u5408\u8f7b\u91cf\u7ea7\u5206\u7ec4\u5377\u79ef\u3001\u5757\u526a\u679d\u548c\u77e5\u8bc6\u84b8\u998f\u6280\u672f\uff0c\u51cf\u5c11\u6a21\u578b\u89c4\u6a21\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "result": "\u5728STL-10\u548cVWW\u6570\u636e\u96c6\u4e0a\uff0cGLT\u663e\u8457\u63d0\u5347\u4e86\u51c6\u786e\u6027\uff1b\u7ed3\u5408\u5757\u526a\u679d\u6280\u672f\u540e\uff0c\u5b9e\u73b0\u4e86\u8f7b\u91cf\u7ea7\uff08\u5c0f\u4e8e1Mb\uff09\u5168\u4e8c\u503c\u5316\u6a21\u578b\uff0c\u4e14\u51c6\u786e\u6027\u635f\u5931\u6709\u9650\u3002", "conclusion": "GLT\u4e3aBNN\u63d0\u4f9b\u4e86\u5168\u5c40\u8272\u8c03\u6620\u5c04\u7684\u7075\u6d3b\u6027\uff0c\u7ed3\u5408\u5757\u526a\u679d\u6280\u672f\uff0c\u9002\u7528\u4e8e\u4f20\u611f\u5668\u7aef\u6301\u7eed\u63a8\u7406\u573a\u666f\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u8f7b\u91cf\u5316\u7684\u6a21\u578b\u8bbe\u8ba1\u3002"}}
{"id": "2505.14297", "pdf": "https://arxiv.org/pdf/2505.14297", "abs": "https://arxiv.org/abs/2505.14297", "authors": ["Jungseob Lee", "Seongtae Hong", "Hyeonseok Moon", "Heuiseok Lim"], "title": "Cross-Lingual Optimization for Language Transfer in Large Language Models", "categories": ["cs.CL"], "comment": "Accepted for publication at ACL 2025. Jungseob Lee and Seongtae Hong\n  contributed equally to this work", "summary": "Adapting large language models to other languages typically employs\nsupervised fine-tuning (SFT) as a standard approach. However, it often suffers\nfrom an overemphasis on English performance, a phenomenon that is especially\npronounced in data-constrained environments. To overcome these challenges, we\npropose \\textbf{Cross-Lingual Optimization (CLO)} that efficiently transfers an\nEnglish-centric LLM to a target language while preserving its English\ncapabilities. CLO utilizes publicly available English SFT data and a\ntranslation model to enable cross-lingual transfer. We conduct experiments\nusing five models on six languages, each possessing varying levels of resource.\nOur results show that CLO consistently outperforms SFT in both acquiring target\nlanguage proficiency and maintaining English performance. Remarkably, in\nlow-resource languages, CLO with only 3,200 samples surpasses SFT with 6,400\nsamples, demonstrating that CLO can achieve better performance with less data.\nFurthermore, we find that SFT is particularly sensitive to data quantity in\nmedium and low-resource languages, whereas CLO remains robust. Our\ncomprehensive analysis emphasizes the limitations of SFT and incorporates\nadditional training strategies in CLO to enhance efficiency.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u8de8\u8bed\u8a00\u4f18\u5316\uff08CLO\uff09\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u6570\u636e\u53d7\u9650\u73af\u5883\u4e0b\u9ad8\u6548\u5730\u5c06\u4ee5\u82f1\u8bed\u4e3a\u4e2d\u5fc3\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8fc1\u79fb\u5230\u76ee\u6807\u8bed\u8a00\uff0c\u540c\u65f6\u4fdd\u6301\u5176\u82f1\u8bed\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u7684\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u65b9\u6cd5\u5728\u6570\u636e\u53d7\u9650\u73af\u5883\u4e0b\u8fc7\u5ea6\u4f9d\u8d56\u82f1\u8bed\u6027\u80fd\uff0c\u5bfc\u81f4\u76ee\u6807\u8bed\u8a00\u8868\u73b0\u4e0d\u4f73\u3002", "method": "CLO\u5229\u7528\u516c\u5f00\u7684\u82f1\u8bedSFT\u6570\u636e\u548c\u7ffb\u8bd1\u6a21\u578b\u5b9e\u73b0\u8de8\u8bed\u8a00\u8fc1\u79fb\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCLO\u5728\u76ee\u6807\u8bed\u8a00\u6027\u80fd\u548c\u82f1\u8bed\u80fd\u529b\u4fdd\u6301\u4e0a\u5747\u4f18\u4e8eSFT\uff0c\u4e14\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2d\u4ec5\u9700\u4e00\u534a\u6570\u636e\u5373\u53ef\u8d85\u8d8aSFT\u3002", "conclusion": "CLO\u5728\u6570\u636e\u6548\u7387\u548c\u9c81\u68d2\u6027\u4e0a\u663e\u8457\u4f18\u4e8eSFT\uff0c\u4e3a\u591a\u8bed\u8a00\u6a21\u578b\u8fc1\u79fb\u63d0\u4f9b\u4e86\u66f4\u4f18\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.13507", "pdf": "https://arxiv.org/pdf/2505.13507", "abs": "https://arxiv.org/abs/2505.13507", "authors": ["Haoyang Chen"], "title": "Open Set Domain Adaptation with Vision-language models via Gradient-aware Separation", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Open-Set Domain Adaptation (OSDA) confronts the dual challenge of aligning\nknown-class distributions across domains while identifying\ntarget-domain-specific unknown categories. Current approaches often fail to\nleverage semantic relationships between modalities and struggle with error\naccumulation in unknown sample detection. We propose to harness Contrastive\nLanguage-Image Pretraining (CLIP) to address these limitations through two key\ninnovations: 1) Prompt-driven cross-domain alignment: Learnable textual prompts\nconditioned on domain discrepancy metrics dynamically adapt CLIP's text\nencoder, enabling semantic consistency between source and target domains\nwithout explicit unknown-class supervision. 2) Gradient-aware open-set\nseparation: A gradient analysis module quantifies domain shift by comparing the\nL2-norm of gradients from the learned prompts, where known/unknown samples\nexhibit statistically distinct gradient behaviors. Evaluations on Office-Home\nshow that our method consistently outperforms CLIP baseline and standard\nbaseline. Ablation studies confirm the gradient norm's critical role.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528CLIP\u6a21\u578b\u89e3\u51b3\u5f00\u653e\u96c6\u57df\u9002\u5e94\u95ee\u9898\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u6587\u672c\u63d0\u793a\u548c\u68af\u5ea6\u5206\u6790\u6a21\u5757\u5b9e\u73b0\u8de8\u57df\u5bf9\u9f50\u548c\u5f00\u653e\u96c6\u5206\u79bb\u3002", "motivation": "\u5f00\u653e\u96c6\u57df\u9002\u5e94\uff08OSDA\uff09\u9762\u4e34\u5df2\u77e5\u7c7b\u522b\u5206\u5e03\u5bf9\u9f50\u548c\u76ee\u6807\u57df\u672a\u77e5\u7c7b\u522b\u8bc6\u522b\u7684\u53cc\u91cd\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528\u6a21\u6001\u95f4\u7684\u8bed\u4e49\u5173\u7cfb\u4e14\u6613\u53d7\u672a\u77e5\u6837\u672c\u68c0\u6d4b\u8bef\u5dee\u7d2f\u79ef\u7684\u5f71\u54cd\u3002", "method": "1\uff09\u57fa\u4e8e\u57df\u5dee\u5f02\u5ea6\u91cf\u7684\u53ef\u5b66\u4e60\u6587\u672c\u63d0\u793a\u52a8\u6001\u8c03\u6574CLIP\u6587\u672c\u7f16\u7801\u5668\uff0c\u5b9e\u73b0\u8de8\u57df\u8bed\u4e49\u5bf9\u9f50\uff1b2\uff09\u901a\u8fc7\u68af\u5ea6\u5206\u6790\u6a21\u5757\u91cf\u5316\u57df\u504f\u79fb\uff0c\u5229\u7528\u68af\u5ea6L2\u8303\u6570\u533a\u5206\u5df2\u77e5/\u672a\u77e5\u6837\u672c\u3002", "result": "\u5728Office-Home\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8eCLIP\u57fa\u7ebf\u548c\u6807\u51c6\u57fa\u7ebf\uff0c\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u68af\u5ea6\u8303\u6570\u7684\u5173\u952e\u4f5c\u7528\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408\u52a8\u6001\u63d0\u793a\u548c\u68af\u5ea6\u5206\u6790\uff0c\u6709\u6548\u89e3\u51b3\u4e86OSDA\u95ee\u9898\uff0c\u4e3a\u8de8\u57df\u9002\u5e94\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.14305", "pdf": "https://arxiv.org/pdf/2505.14305", "abs": "https://arxiv.org/abs/2505.14305", "authors": ["Jinwang Song", "Hongying Zan", "Kunli Zhang", "Lingling Mu", "Yingjie Han", "Haobo Hua", "Min Peng"], "title": "JOLT-SQL: Joint Loss Tuning of Text-to-SQL with Confusion-aware Noisy Schema Sampling", "categories": ["cs.CL"], "comment": "Work in progress. 13 pages, 6 figures", "summary": "Text-to-SQL, which maps natural language to SQL queries, has benefited\ngreatly from recent advances in Large Language Models (LLMs). While LLMs offer\nvarious paradigms for this task, including prompting and supervised fine-tuning\n(SFT), SFT approaches still face challenges such as complex multi-stage\npipelines and poor robustness to noisy schema information. To address these\nlimitations, we present JOLT-SQL, a streamlined single-stage SFT framework that\njointly optimizes schema linking and SQL generation via a unified loss.\nJOLT-SQL employs discriminative schema linking, enhanced by local bidirectional\nattention, alongside a confusion-aware noisy schema sampling strategy with\nselective attention to improve robustness under noisy schema conditions.\nExperiments on the Spider and BIRD benchmarks demonstrate that JOLT-SQL\nachieves state-of-the-art execution accuracy among comparable-size open-source\nmodels, while significantly improving both training and inference efficiency.", "AI": {"tldr": "JOLT-SQL\u662f\u4e00\u4e2a\u5355\u9636\u6bb5\u7684\u76d1\u7763\u5fae\u8c03\u6846\u67b6\uff0c\u901a\u8fc7\u7edf\u4e00\u635f\u5931\u8054\u5408\u4f18\u5316\u6a21\u5f0f\u94fe\u63a5\u548cSQL\u751f\u6210\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u4e2d\u7684\u590d\u6742\u6d41\u7a0b\u548c\u566a\u58f0\u6a21\u5f0f\u9c81\u68d2\u6027\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u76d1\u7763\u5fae\u8c03\u65b9\u6cd5\u5728Text-to-SQL\u4efb\u52a1\u4e2d\u5b58\u5728\u591a\u9636\u6bb5\u6d41\u7a0b\u590d\u6742\u548c\u566a\u58f0\u6a21\u5f0f\u9c81\u68d2\u6027\u5dee\u7684\u95ee\u9898\u3002", "method": "JOLT-SQL\u91c7\u7528\u5224\u522b\u5f0f\u6a21\u5f0f\u94fe\u63a5\uff08\u589e\u5f3a\u5c40\u90e8\u53cc\u5411\u6ce8\u610f\u529b\uff09\u548c\u566a\u58f0\u6a21\u5f0f\u91c7\u6837\u7b56\u7565\uff08\u9009\u62e9\u6027\u6ce8\u610f\u529b\uff09\uff0c\u8054\u5408\u4f18\u5316\u6a21\u5f0f\u94fe\u63a5\u548cSQL\u751f\u6210\u3002", "result": "\u5728Spider\u548cBIRD\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cJOLT-SQL\u5728\u540c\u7c7b\u5f00\u6e90\u6a21\u578b\u4e2d\u8fbe\u5230\u6700\u4f18\u6267\u884c\u51c6\u786e\u7387\uff0c\u5e76\u663e\u8457\u63d0\u5347\u8bad\u7ec3\u548c\u63a8\u7406\u6548\u7387\u3002", "conclusion": "JOLT-SQL\u901a\u8fc7\u5355\u9636\u6bb5\u6846\u67b6\u548c\u566a\u58f0\u9c81\u68d2\u6027\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86Text-to-SQL\u4efb\u52a1\u7684\u6027\u80fd\u548c\u6548\u7387\u3002"}}
{"id": "2505.13539", "pdf": "https://arxiv.org/pdf/2505.13539", "abs": "https://arxiv.org/abs/2505.13539", "authors": ["Rodrigo Fritz", "Pablo Su\u00e1rez-Serrato", "Victor Mijangos", "Anayanzi D. Martinez-Hernandez", "Eduardo Ivan Velazquez Richards"], "title": "EuLearn: A 3D database for learning Euler characteristics", "categories": ["cs.CG", "cs.CV", "cs.LG", "math.DG", "math.GT"], "comment": "35 pages, many figures. Datasets and source code publicly available\n  at https://huggingface.co/datasets/appliedgeometry/EuLearn and\n  https://github.com/appliedgeometry/EuLearn_db", "summary": "We present EuLearn, the first surface datasets equitably representing a\ndiversity of topological types. We designed our embedded surfaces of uniformly\nvarying genera relying on random knots, thus allowing our surfaces to knot with\nthemselves. EuLearn contributes new topological datasets of meshes, point\nclouds, and scalar fields in 3D. We aim to facilitate the training of machine\nlearning systems that can discern topological features. We experimented with\nspecific emblematic 3D neural network architectures, finding that their vanilla\nimplementations perform poorly on genus classification. To enhance performance,\nwe developed a novel, non-Euclidean, statistical sampling method adapted to\ngraph and manifold data. We also introduce adjacency-informed adaptations of\nPointNet and Transformer architectures that rely on our non-Euclidean sampling\nstrategy. Our results demonstrate that incorporating topological information\ninto deep learning workflows significantly improves performance on these\notherwise challenging EuLearn datasets.", "AI": {"tldr": "EuLearn\u662f\u9996\u4e2a\u516c\u5e73\u4ee3\u8868\u591a\u79cd\u62d3\u6251\u7c7b\u578b\u7684\u8868\u9762\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u968f\u673a\u7ed3\u8bbe\u8ba1\u5747\u5300\u53d8\u5316\u7684\u66f2\u9762\uff0c\u652f\u6301\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u8bc6\u522b\u62d3\u6251\u7279\u5f81\u3002", "motivation": "\u4e3a\u8bad\u7ec3\u80fd\u591f\u8bc6\u522b\u62d3\u6251\u7279\u5f81\u7684\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u63d0\u4f9b\u591a\u6837\u5316\u7684\u6570\u636e\u96c6\u3002", "method": "\u5229\u7528\u968f\u673a\u7ed3\u8bbe\u8ba1\u66f2\u9762\uff0c\u63d0\u51fa\u975e\u6b27\u51e0\u91cc\u5f97\u7edf\u8ba1\u91c7\u6837\u65b9\u6cd5\uff0c\u5e76\u6539\u8fdbPointNet\u548cTransformer\u67b6\u6784\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5c06\u62d3\u6251\u4fe1\u606f\u878d\u5165\u6df1\u5ea6\u5b66\u4e60\u6d41\u7a0b\u663e\u8457\u63d0\u5347\u4e86\u5728EuLearn\u6570\u636e\u96c6\u4e0a\u7684\u6027\u80fd\u3002", "conclusion": "EuLearn\u6570\u636e\u96c6\u53ca\u975e\u6b27\u51e0\u91cc\u5f97\u91c7\u6837\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u62d3\u6251\u7279\u5f81\u8bc6\u522b\u7684\u6027\u80fd\u3002"}}
{"id": "2505.14309", "pdf": "https://arxiv.org/pdf/2505.14309", "abs": "https://arxiv.org/abs/2505.14309", "authors": ["Ehsan Doostmohammadi", "Marco Kuhlmann"], "title": "Studying the Role of Input-Neighbor Overlap in Retrieval-Augmented Language Models Training Efficiency", "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-augmented language models have demonstrated performance comparable\nto much larger models while requiring fewer computational resources. The\neffectiveness of these models crucially depends on the overlap between query\nand retrieved context, but the optimal degree of this overlap remains\nunexplored. In this paper, we systematically investigate how varying levels of\nquery--context overlap affect model performance during both training and\ninference. Our experiments reveal that increased overlap initially has minimal\neffect, but substantially improves test-time perplexity and accelerates model\nlearning above a critical threshold. Building on these findings, we demonstrate\nthat deliberately increasing overlap through synthetic context can enhance data\nefficiency and reduce training time by approximately 40\\% without compromising\nperformance. We specifically generate synthetic context through paraphrasing\nqueries. We validate our perplexity-based findings on question-answering tasks,\nconfirming that the benefits of retrieval-augmented language modeling extend to\npractical applications. Our results provide empirical evidence of significant\noptimization potential for retrieval mechanisms in language model pretraining.", "AI": {"tldr": "\u68c0\u7d22\u589e\u5f3a\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\u4e0e\u67e5\u8be2\u548c\u68c0\u7d22\u4e0a\u4e0b\u6587\u7684\u91cd\u53e0\u7a0b\u5ea6\u5bc6\u5207\u76f8\u5173\uff0c\u4f46\u6700\u4f73\u91cd\u53e0\u7a0b\u5ea6\u5c1a\u672a\u660e\u786e\u3002\u672c\u6587\u901a\u8fc7\u5b9e\u9a8c\u53d1\u73b0\uff0c\u8d85\u8fc7\u4e34\u754c\u9608\u503c\u540e\uff0c\u589e\u52a0\u91cd\u53e0\u80fd\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u548c\u5b66\u4e60\u901f\u5ea6\u3002\u901a\u8fc7\u751f\u6210\u5408\u6210\u4e0a\u4e0b\u6587\uff08\u5982\u6539\u5199\u67e5\u8be2\uff09\u53ef\u63d0\u9ad8\u6570\u636e\u6548\u7387\u5e76\u51cf\u5c1140%\u8bad\u7ec3\u65f6\u95f4\u3002", "motivation": "\u63a2\u7d22\u67e5\u8be2\u4e0e\u68c0\u7d22\u4e0a\u4e0b\u6587\u91cd\u53e0\u7a0b\u5ea6\u5bf9\u68c0\u7d22\u589e\u5f3a\u8bed\u8a00\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u4ee5\u4f18\u5316\u6a21\u578b\u8bad\u7ec3\u548c\u63a8\u7406\u6548\u7387\u3002", "method": "\u7cfb\u7edf\u7814\u7a76\u4e0d\u540c\u91cd\u53e0\u7a0b\u5ea6\u5bf9\u6a21\u578b\u8bad\u7ec3\u548c\u63a8\u7406\u7684\u5f71\u54cd\uff0c\u5e76\u901a\u8fc7\u751f\u6210\u5408\u6210\u4e0a\u4e0b\u6587\uff08\u5982\u6539\u5199\u67e5\u8be2\uff09\u9a8c\u8bc1\u5176\u6548\u679c\u3002", "result": "\u589e\u52a0\u91cd\u53e0\u7a0b\u5ea6\u5728\u8d85\u8fc7\u4e34\u754c\u9608\u503c\u540e\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u548c\u5b66\u4e60\u901f\u5ea6\uff1b\u5408\u6210\u4e0a\u4e0b\u6587\u53ef\u51cf\u5c1140%\u8bad\u7ec3\u65f6\u95f4\u4e14\u4e0d\u5f71\u54cd\u6027\u80fd\u3002", "conclusion": "\u68c0\u7d22\u673a\u5236\u5728\u8bed\u8a00\u6a21\u578b\u9884\u8bad\u7ec3\u4e2d\u5b58\u5728\u663e\u8457\u4f18\u5316\u6f5c\u529b\uff0c\u5408\u6210\u4e0a\u4e0b\u6587\u662f\u4e00\u79cd\u6709\u6548\u7684\u6570\u636e\u6548\u7387\u63d0\u5347\u65b9\u6cd5\u3002"}}
{"id": "2505.13542", "pdf": "https://arxiv.org/pdf/2505.13542", "abs": "https://arxiv.org/abs/2505.13542", "authors": ["Karthik Sivakoti"], "title": "GANCompress: GAN-Enhanced Neural Image Compression with Binary Spherical Quantization", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "The exponential growth of visual data in digital communications has\nintensified the need for efficient compression techniques that balance\nrate-distortion performance with computational feasibility. While recent neural\ncompression approaches have shown promise, they still struggle with fundamental\nchallenges: preserving perceptual quality at high compression ratios,\ncomputational efficiency, and adaptability to diverse visual content. This\npaper introduces GANCompress, a novel neural compression framework that\nsynergistically combines Binary Spherical Quantization (BSQ) with Generative\nAdversarial Networks (GANs) to address these challenges. Our approach employs a\ntransformer-based autoencoder with an enhanced BSQ bottleneck that projects\nlatent representations onto a hypersphere, enabling efficient discretization\nwith bounded quantization error. This is followed by a specialized GAN\narchitecture incorporating frequency-domain attention and color consistency\noptimization. Experimental results demonstrate that GANCompress achieves\nsubstantial improvement in compression efficiency -- reducing file sizes by up\nto 100x with minimal visual distortion. Our method outperforms traditional\ncodecs like H.264 by 12-15% in perceptual metrics while maintaining comparable\nPSNR/SSIM values, with 2.4x faster encoding and decoding speeds. On standard\nbenchmarks including ImageNet-1k and COCO2017, GANCompress sets a new\nstate-of-the-art, reducing FID from 0.72 to 0.41 (43% improvement) compared to\nprevious methods while maintaining higher throughput. This work presents a\nsignificant advancement in neural compression technology with promising\napplications for real-time visual communication systems.", "AI": {"tldr": "GANCompress\u7ed3\u5408\u4e8c\u8fdb\u5236\u7403\u5f62\u91cf\u5316\uff08BSQ\uff09\u4e0e\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08GANs\uff09\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u795e\u7ecf\u538b\u7f29\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u538b\u7f29\u6548\u7387\u548c\u89c6\u89c9\u8d28\u91cf\u3002", "motivation": "\u89c6\u89c9\u6570\u636e\u7206\u70b8\u5f0f\u589e\u957f\u9700\u8981\u9ad8\u6548\u538b\u7f29\u6280\u672f\uff0c\u73b0\u6709\u795e\u7ecf\u538b\u7f29\u65b9\u6cd5\u5728\u611f\u77e5\u8d28\u91cf\u3001\u8ba1\u7b97\u6548\u7387\u548c\u5185\u5bb9\u9002\u5e94\u6027\u65b9\u9762\u5b58\u5728\u6311\u6218\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u53d8\u538b\u5668\u7684\u81ea\u52a8\u7f16\u7801\u5668\u548c\u589e\u5f3a\u7684BSQ\u74f6\u9888\uff0c\u7ed3\u5408\u9891\u7387\u57df\u6ce8\u610f\u529b\u548c\u989c\u8272\u4e00\u81f4\u6027\u4f18\u5316\u7684GAN\u67b6\u6784\u3002", "result": "\u5b9e\u9a8c\u663e\u793aGANCompress\u538b\u7f29\u6548\u7387\u663e\u8457\u63d0\u5347\uff0c\u6587\u4ef6\u5927\u5c0f\u51cf\u5c11100\u500d\uff0c\u89c6\u89c9\u5931\u771f\u6781\u5c0f\uff0c\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u7f16\u89e3\u7801\u5668\u3002", "conclusion": "GANCompress\u5728\u795e\u7ecf\u538b\u7f29\u6280\u672f\u4e0a\u53d6\u5f97\u91cd\u8981\u8fdb\u5c55\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u89c6\u89c9\u901a\u4fe1\u7cfb\u7edf\u3002"}}
{"id": "2505.14311", "pdf": "https://arxiv.org/pdf/2505.14311", "abs": "https://arxiv.org/abs/2505.14311", "authors": ["Shamsuddeen Hassan Muhammad", "Ibrahim Said Ahmad", "Idris Abdulmumin", "Falalu Ibrahim Lawan", "Babangida Sani", "Sukairaj Hafiz Imam", "Yusuf Aliyu", "Sani Abdullahi Sani", "Ali Usman Umar", "Kenneth Church", "Vukosi Marivate"], "title": "HausaNLP: Current Status, Challenges and Future Directions for Hausa Natural Language Processing", "categories": ["cs.CL"], "comment": null, "summary": "Hausa Natural Language Processing (NLP) has gained increasing attention in\nrecent years, yet remains understudied as a low-resource language despite\nhaving over 120 million first-language (L1) and 80 million second-language (L2)\nspeakers worldwide. While significant advances have been made in high-resource\nlanguages, Hausa NLP faces persistent challenges, including limited open-source\ndatasets and inadequate model representation. This paper presents an overview\nof the current state of Hausa NLP, systematically examining existing resources,\nresearch contributions, and gaps across fundamental NLP tasks: text\nclassification, machine translation, named entity recognition, speech\nrecognition, and question answering. We introduce HausaNLP\n(https://catalog.hausanlp.org), a curated catalog that aggregates datasets,\ntools, and research works to enhance accessibility and drive further\ndevelopment. Furthermore, we discuss challenges in integrating Hausa into large\nlanguage models (LLMs), addressing issues of suboptimal tokenization and\ndialectal variation. Finally, we propose strategic research directions\nemphasizing dataset expansion, improved language modeling approaches, and\nstrengthened community collaboration to advance Hausa NLP. Our work provides\nboth a foundation for accelerating Hausa NLP progress and valuable insights for\nbroader multilingual NLP research.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u8c6a\u8428\u8bedNLP\u7684\u73b0\u72b6\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u8d44\u6e90\u76ee\u5f55HausaNLP\uff0c\u5e76\u8ba8\u8bba\u4e86\u8c6a\u8428\u8bed\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u6311\u6218\u53ca\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u5c3d\u7ba1\u8c6a\u8428\u8bed\u62e5\u6709\u5927\u91cf\u4f7f\u7528\u8005\uff0c\u4f46\u5176NLP\u7814\u7a76\u4ecd\u56e0\u8d44\u6e90\u532e\u4e4f\u800c\u53d7\u9650\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u5e76\u63a8\u52a8\u5176\u53d1\u5c55\u3002", "method": "\u7cfb\u7edf\u5206\u6790\u4e86\u8c6a\u8428\u8bedNLP\u7684\u73b0\u6709\u8d44\u6e90\u3001\u7814\u7a76\u6210\u679c\u53ca\u4e0d\u8db3\uff0c\u5e76\u63d0\u51fa\u4e86HausaNLP\u76ee\u5f55\u4ee5\u6574\u5408\u8d44\u6e90\u3002", "result": "\u63d0\u51fa\u4e86HausaNLP\u76ee\u5f55\uff0c\u603b\u7ed3\u4e86\u8c6a\u8428\u8bedNLP\u7684\u6311\u6218\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "\u672c\u6587\u4e3a\u8c6a\u8428\u8bedNLP\u7684\u53d1\u5c55\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5e76\u4e3a\u591a\u8bed\u8a00NLP\u7814\u7a76\u63d0\u4f9b\u4e86\u53c2\u8003\u3002"}}
{"id": "2505.13563", "pdf": "https://arxiv.org/pdf/2505.13563", "abs": "https://arxiv.org/abs/2505.13563", "authors": ["Xiaohui Wang", "Peng Ye", "Chenyu Huang", "Shenghe Zheng", "Bo Zhang", "Wanli Ouyang", "Tao Chen"], "title": "Breaking the Compression Ceiling: Data-Free Pipeline for Ultra-Efficient Delta Compression", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "With the rise of the fine-tuned--pretrained paradigm, storing numerous\nfine-tuned models for multi-tasking creates significant storage overhead. Delta\ncompression alleviates this by storing only the pretrained model and the highly\ncompressed delta weights (the differences between fine-tuned and pretrained\nmodel weights). However, existing methods fail to maintain both high\ncompression and performance, and often rely on data. To address these\nchallenges, we propose UltraDelta, the first data-free delta compression\npipeline that achieves both ultra-high compression and strong performance.\nUltraDelta is designed to minimize redundancy, maximize information, and\nstabilize performance across inter-layer, intra-layer, and global dimensions,\nusing three key components: (1) Variance-Based Mixed Sparsity Allocation\nassigns sparsity based on variance, giving lower sparsity to high-variance\nlayers to preserve inter-layer information. (2) Distribution-Aware Compression\napplies uniform quantization and then groups parameters by value, followed by\ngroup-wise pruning, to better preserve intra-layer distribution. (3)\nTrace-Norm-Guided Rescaling uses the trace norm of delta weights to estimate a\nglobal rescaling factor, improving model stability under higher compression.\nExtensive experiments across (a) large language models (fine-tuned on LLaMA-2\n7B and 13B) with up to 133x, (b) general NLP models (RoBERTa-base, T5-base)\nwith up to 800x, (c) vision models (ViT-B/32, ViT-L/14) with up to 400x, and\n(d) multi-modal models (BEiT-3) with 40x compression ratio, demonstrate that\nUltraDelta consistently outperforms existing methods, especially under\nultra-high compression.", "AI": {"tldr": "UltraDelta\u662f\u4e00\u79cd\u65e0\u6570\u636e\u4f9d\u8d56\u7684delta\u538b\u7f29\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u5c42\u7a00\u758f\u5206\u914d\u3001\u5206\u5e03\u611f\u77e5\u538b\u7f29\u548c\u5168\u5c40\u91cd\u7f29\u653e\uff0c\u5b9e\u73b0\u8d85\u9ad8\u538b\u7f29\u548c\u5f3a\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709delta\u538b\u7f29\u65b9\u6cd5\u5728\u9ad8\u538b\u7f29\u4e0b\u6027\u80fd\u548c\u5b58\u50a8\u6548\u7387\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u65b9\u5dee\u6df7\u5408\u7a00\u758f\u5206\u914d\u3001\u5206\u5e03\u611f\u77e5\u538b\u7f29\u548c\u8ff9\u8303\u6570\u5f15\u5bfc\u91cd\u7f29\u653e\u4e09\u79cd\u6280\u672f\u3002", "result": "\u5728\u591a\u79cd\u6a21\u578b\u4e0a\u5b9e\u73b0\u8d85\u9ad8\u538b\u7f29\uff08\u6700\u9ad8800\u500d\uff09\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "UltraDelta\u5728\u8d85\u9ad8\u538b\u7f29\u4e0b\u4ecd\u80fd\u4fdd\u6301\u6027\u80fd\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2505.14313", "pdf": "https://arxiv.org/pdf/2505.14313", "abs": "https://arxiv.org/abs/2505.14313", "authors": ["Leonardo Bertolazzi", "Manuel Vargas Guzm\u00e1n", "Raffaella Bernardi", "Maciej Malicki", "Jakub Szymanik"], "title": "A MIND for Reasoning: Meta-learning for In-context Deduction", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are increasingly evaluated on formal tasks,\nwhere strong reasoning abilities define the state of the art. However, their\nability to generalize to out-of-distribution problems remains limited. In this\npaper, we investigate how LLMs can achieve a systematic understanding of\ndeductive rules. Our focus is on the task of identifying the appropriate subset\nof premises within a knowledge base needed to derive a given hypothesis. To\ntackle this challenge, we propose Meta-learning for In-context Deduction\n(MIND), a novel few-shot meta-learning fine-tuning approach. The goal of MIND\nis to enable models to generalize more effectively to unseen knowledge bases\nand to systematically apply inference rules. Our results show that MIND\nsignificantly improves generalization in small LMs ranging from 1.5B to 7B\nparameters. The benefits are especially pronounced in smaller models and\nlow-data settings. Remarkably, small models fine-tuned with MIND outperform\nstate-of-the-art LLMs, such as GPT-4o and o3-mini, on this task.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMIND\u7684\u5143\u5b66\u4e60\u5fae\u8c03\u65b9\u6cd5\uff0c\u65e8\u5728\u63d0\u5347\u5c0f\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u5728\u6f14\u7ece\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4f7f\u5176\u5728\u672a\u89c1\u8fc7\u7684\u77e5\u8bc6\u5e93\u4e0a\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5f62\u5f0f\u5316\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u5206\u5e03\u5916\u95ee\u9898\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\u6709\u9650\u3002\u672c\u6587\u7814\u7a76\u4e86\u5982\u4f55\u8ba9LLMs\u7cfb\u7edf\u7406\u89e3\u6f14\u7ece\u89c4\u5219\uff0c\u5e76\u89e3\u51b3\u4ece\u77e5\u8bc6\u5e93\u4e2d\u9009\u62e9\u5408\u9002\u524d\u63d0\u5b50\u96c6\u4ee5\u63a8\u5bfc\u5047\u8bbe\u7684\u4efb\u52a1\u3002", "method": "\u63d0\u51fa\u4e86Meta-learning for In-context Deduction (MIND)\uff0c\u4e00\u79cd\u5c11\u6837\u672c\u5143\u5b66\u4e60\u5fae\u8c03\u65b9\u6cd5\uff0c\u65e8\u5728\u63d0\u5347\u6a21\u578b\u5bf9\u672a\u89c1\u77e5\u8bc6\u5e93\u7684\u6cdb\u5316\u80fd\u529b\u548c\u7cfb\u7edf\u5e94\u7528\u63a8\u7406\u89c4\u5219\u7684\u80fd\u529b\u3002", "result": "MIND\u663e\u8457\u63d0\u5347\u4e861.5B\u52307B\u53c2\u6570\u7684\u5c0f\u89c4\u6a21\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5c24\u5176\u5728\u4f4e\u6570\u636e\u73af\u5883\u4e0b\u8868\u73b0\u7a81\u51fa\u3002\u5c0f\u6a21\u578b\u7ecfMIND\u5fae\u8c03\u540e\uff0c\u751a\u81f3\u4f18\u4e8eGPT-4o\u548co3-mini\u7b49\u5148\u8fdbLLMs\u3002", "conclusion": "MIND\u65b9\u6cd5\u4e3a\u5c0f\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u5728\u6f14\u7ece\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u9ad8\u6548\u6cdb\u5316\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5c55\u793a\u4e86\u5176\u5728\u4f4e\u8d44\u6e90\u73af\u5883\u4e0b\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.13579", "pdf": "https://arxiv.org/pdf/2505.13579", "abs": "https://arxiv.org/abs/2505.13579", "authors": ["Yipeng Sun", "Linda-Sophie Schneider", "Chengze Ye", "Mingxuan Gu", "Siyuan Mei", "Siming Bayer", "Andreas Maier"], "title": "Learning Wavelet-Sparse FDK for 3D Cone-Beam CT Reconstruction", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "comment": "Accepted by Fully3D 2025", "summary": "Cone-Beam Computed Tomography (CBCT) is essential in medical imaging, and the\nFeldkamp-Davis-Kress (FDK) algorithm is a popular choice for reconstruction due\nto its efficiency. However, FDK is susceptible to noise and artifacts. While\nrecent deep learning methods offer improved image quality, they often increase\ncomputational complexity and lack the interpretability of traditional methods.\nIn this paper, we introduce an enhanced FDK-based neural network that maintains\nthe classical algorithm's interpretability by selectively integrating trainable\nelements into the cosine weighting and filtering stages. Recognizing the\nchallenge of a large parameter space inherent in 3D CBCT data, we leverage\nwavelet transformations to create sparse representations of the cosine weights\nand filters. This strategic sparsification reduces the parameter count by\n$93.75\\%$ without compromising performance, accelerates convergence, and\nimportantly, maintains the inference computational cost equivalent to the\nclassical FDK algorithm. Our method not only ensures volumetric consistency and\nboosts robustness to noise, but is also designed for straightforward\nintegration into existing CT reconstruction pipelines. This presents a\npragmatic enhancement that can benefit clinical applications, particularly in\nenvironments with computational limitations.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u6539\u8fdb\u7684FDK\u7b97\u6cd5\uff0c\u7ed3\u5408\u795e\u7ecf\u7f51\u7edc\u548c\u7a00\u758f\u8868\u793a\uff0c\u964d\u4f4e\u53c2\u6570\u6570\u91cf\u5e76\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\uff0c\u63d0\u5347CBCT\u91cd\u5efa\u8d28\u91cf\u3002", "motivation": "FDK\u7b97\u6cd5\u5728CBCT\u91cd\u5efa\u4e2d\u6548\u7387\u9ad8\u4f46\u6613\u53d7\u566a\u58f0\u548c\u4f2a\u5f71\u5f71\u54cd\uff0c\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u867d\u63d0\u5347\u8d28\u91cf\u4f46\u8ba1\u7b97\u590d\u6742\u4e14\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u5728FDK\u7684\u4f59\u5f26\u52a0\u6743\u548c\u6ee4\u6ce2\u9636\u6bb5\u9009\u62e9\u6027\u5f15\u5165\u53ef\u8bad\u7ec3\u5143\u7d20\uff0c\u5229\u7528\u5c0f\u6ce2\u53d8\u6362\u751f\u6210\u7a00\u758f\u8868\u793a\uff0c\u51cf\u5c11\u53c2\u6570\u6570\u91cf\u3002", "result": "\u53c2\u6570\u51cf\u5c1193.75%\uff0c\u8ba1\u7b97\u6210\u672c\u4e0eFDK\u76f8\u5f53\uff0c\u6536\u655b\u66f4\u5feb\uff0c\u566a\u58f0\u9c81\u68d2\u6027\u589e\u5f3a\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301FDK\u4f18\u52bf\u7684\u540c\u65f6\u63d0\u5347\u6027\u80fd\uff0c\u9002\u5408\u4e34\u5e8a\u8ba1\u7b97\u53d7\u9650\u73af\u5883\u3002"}}
{"id": "2505.14347", "pdf": "https://arxiv.org/pdf/2505.14347", "abs": "https://arxiv.org/abs/2505.14347", "authors": ["Neelabh Sinha"], "title": "QA-prompting: Improving Summarization with Large Language Models using Question-Answering", "categories": ["cs.CL"], "comment": "Submitted to ARR", "summary": "Language Models (LMs) have revolutionized natural language processing,\nenabling high-quality text generation through prompting and in-context\nlearning. However, models often struggle with long-context summarization due to\npositional biases, leading to suboptimal extraction of critical information.\nThere are techniques to improve this with fine-tuning, pipelining, or using\ncomplex techniques, which have their own challenges. To solve these challenges,\nwe propose QA-prompting - a simple prompting method for summarization that\nutilizes question-answering as an intermediate step prior to summary\ngeneration. Our method extracts key information and enriches the context of\ntext to mitigate positional biases and improve summarization in a single LM\ncall per task without requiring fine-tuning or pipelining. Experiments on\nmultiple datasets belonging to different domains using ten state-of-the-art\npre-trained models demonstrate that QA-prompting outperforms baseline and other\nstate-of-the-art methods, achieving up to 29% improvement in ROUGE scores. This\nprovides an effective and scalable solution for summarization and highlights\nthe importance of domain-specific question selection for optimal performance.", "AI": {"tldr": "QA-prompting\u662f\u4e00\u79cd\u901a\u8fc7\u95ee\u7b54\u6b65\u9aa4\u4f18\u5316\u957f\u6587\u672c\u6458\u8981\u7684\u65b9\u6cd5\uff0c\u65e0\u9700\u5fae\u8c03\u6216\u590d\u6742\u6d41\u7a0b\uff0c\u663e\u8457\u63d0\u5347\u6458\u8981\u8d28\u91cf\u3002", "motivation": "\u89e3\u51b3\u8bed\u8a00\u6a21\u578b\u5728\u957f\u6587\u672c\u6458\u8981\u4e2d\u56e0\u4f4d\u7f6e\u504f\u5dee\u5bfc\u81f4\u7684\u5173\u952e\u4fe1\u606f\u63d0\u53d6\u4e0d\u8db3\u95ee\u9898\u3002", "method": "\u63d0\u51faQA-prompting\u65b9\u6cd5\uff0c\u901a\u8fc7\u95ee\u7b54\u6b65\u9aa4\u63d0\u53d6\u5173\u952e\u4fe1\u606f\u5e76\u4e30\u5bcc\u4e0a\u4e0b\u6587\uff0c\u518d\u751f\u6210\u6458\u8981\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u6a21\u578b\u4e0a\uff0cQA-prompting\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u63d0\u5347\u9ad8\u8fbe29%\u7684ROUGE\u5206\u6570\u3002", "conclusion": "QA-prompting\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u6458\u8981\u65b9\u6cd5\uff0c\u4e14\u9886\u57df\u7279\u5b9a\u95ee\u9898\u9009\u62e9\u5bf9\u6027\u80fd\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2505.13617", "pdf": "https://arxiv.org/pdf/2505.13617", "abs": "https://arxiv.org/abs/2505.13617", "authors": ["Christopher Ick", "Gordon Wichern", "Yoshiki Masuyama", "Fran\u00e7ois Germain", "Jonathan Le Roux"], "title": "Direction-Aware Neural Acoustic Fields for Few-Shot Interpolation of Ambisonic Impulse Responses", "categories": ["eess.AS", "cs.AI", "cs.CV", "cs.LG", "cs.SD"], "comment": "Accepted at Interspeech 2025", "summary": "The characteristics of a sound field are intrinsically linked to the\ngeometric and spatial properties of the environment surrounding a sound source\nand a listener. The physics of sound propagation is captured in a time-domain\nsignal known as a room impulse response (RIR). Prior work using neural fields\n(NFs) has allowed learning spatially-continuous representations of RIRs from\nfinite RIR measurements. However, previous NF-based methods have focused on\nmonaural omnidirectional or at most binaural listeners, which does not\nprecisely capture the directional characteristics of a real sound field at a\nsingle point. We propose a direction-aware neural field (DANF) that more\nexplicitly incorporates the directional information by Ambisonic-format RIRs.\nWhile DANF inherently captures spatial relations between sources and listeners,\nwe further propose a direction-aware loss. In addition, we investigate the\nability of DANF to adapt to new rooms in various ways including low-rank\nadaptation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b9\u5411\u611f\u77e5\u795e\u7ecf\u573a\uff08DANF\uff09\uff0c\u901a\u8fc7Ambisonic\u683c\u5f0f\u7684RIR\u66f4\u660e\u786e\u5730\u6355\u6349\u65b9\u5411\u4fe1\u606f\uff0c\u5e76\u8fdb\u4e00\u6b65\u63d0\u51fa\u4e86\u65b9\u5411\u611f\u77e5\u635f\u5931\u51fd\u6570\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u795e\u7ecf\u573a\u7684\u65b9\u6cd5\u4ec5\u5173\u6ce8\u5355\u58f0\u9053\u5168\u5411\u6216\u53cc\u8033\u542c\u4f17\uff0c\u672a\u80fd\u7cbe\u786e\u6355\u6349\u771f\u5b9e\u58f0\u573a\u7684\u5b9a\u5411\u7279\u6027\u3002", "method": "\u63d0\u51faDANF\u6a21\u578b\uff0c\u7ed3\u5408Ambisonic\u683c\u5f0fRIR\uff0c\u5f15\u5165\u65b9\u5411\u611f\u77e5\u635f\u5931\u51fd\u6570\uff0c\u5e76\u63a2\u7d22\u4f4e\u79e9\u9002\u5e94\u7b49\u65b0\u623f\u95f4\u9002\u5e94\u80fd\u529b\u3002", "result": "DANF\u80fd\u591f\u66f4\u7cbe\u786e\u5730\u6355\u6349\u58f0\u573a\u7684\u5b9a\u5411\u7279\u6027\uff0c\u5e76\u5177\u5907\u9002\u5e94\u65b0\u623f\u95f4\u7684\u80fd\u529b\u3002", "conclusion": "DANF\u4e3a\u58f0\u573a\u5efa\u6a21\u63d0\u4f9b\u4e86\u66f4\u7cbe\u786e\u7684\u65b9\u5411\u611f\u77e5\u80fd\u529b\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u65b0\u73af\u5883\u4e2d\u7684\u9002\u5e94\u6027\u3002"}}
{"id": "2505.14350", "pdf": "https://arxiv.org/pdf/2505.14350", "abs": "https://arxiv.org/abs/2505.14350", "authors": ["Jialong Han", "Si Zhang", "Ke Zhang"], "title": "OSoRA: Output-Dimension and Singular-Value Initialized Low-Rank Adaptation", "categories": ["cs.CL"], "comment": null, "summary": "Fine-tuning Large Language Models (LLMs) has become increasingly challenging\ndue to their massive scale and associated computational costs.\nParameter-Efficient Fine-Tuning (PEFT) methodologies have been proposed as\ncomputational alternatives; however, their implementations still require\nsignificant resources. In this paper, we present OSoRA (Output-Dimension and\nSingular-Value Initialized Low-Rank Adaptation), a novel PEFT method for LLMs.\nOSoRA extends Low-Rank Adaptation (LoRA) by integrating Singular Value\nDecomposition (SVD) with learnable scaling vectors in a unified framework. It\nfirst performs an SVD of pre-trained weight matrices, then optimizes an\noutput-dimension vector during training, while keeping the corresponding\nsingular vector matrices frozen. OSoRA substantially reduces computational\nresource requirements by minimizing the number of trainable parameters during\nfine-tuning. Comprehensive evaluations across mathematical reasoning, common\nsense reasoning, and other benchmarks demonstrate that OSoRA achieves\ncomparable or superior performance to state-of-the-art methods like LoRA and\nVeRA, while maintaining a linear parameter scaling even as the rank increases\nto higher dimensions. Our ablation studies further confirm that jointly\ntraining both the singular values and the output-dimension vector is critical\nfor optimal performance.", "AI": {"tldr": "OSoRA\u662f\u4e00\u79cd\u65b0\u7684\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408SVD\u548c\u53ef\u5b66\u4e60\u7f29\u653e\u5411\u91cf\uff0c\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u7531\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u89c4\u6a21\u5e9e\u5927\u4e14\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u4f20\u7edf\u7684\u5fae\u8c03\u65b9\u6cd5\u9762\u4e34\u6311\u6218\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "OSoRA\u6269\u5c55\u4e86LoRA\uff0c\u901a\u8fc7SVD\u5206\u89e3\u9884\u8bad\u7ec3\u6743\u91cd\u77e9\u9635\uff0c\u5e76\u4f18\u5316\u8f93\u51fa\u7ef4\u5ea6\u5411\u91cf\uff0c\u540c\u65f6\u51bb\u7ed3\u5947\u5f02\u5411\u91cf\u77e9\u9635\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u3001\u5e38\u8bc6\u63a8\u7406\u7b49\u4efb\u52a1\u4e0a\uff0cOSoRA\u6027\u80fd\u4e0eLoRA\u548cVeRA\u76f8\u5f53\u6216\u66f4\u4f18\uff0c\u4e14\u53c2\u6570\u89c4\u6a21\u7ebf\u6027\u589e\u957f\u3002", "conclusion": "\u8054\u5408\u8bad\u7ec3\u5947\u5f02\u503c\u548c\u8f93\u51fa\u7ef4\u5ea6\u5411\u91cf\u5bf9\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0cOSoRA\u4e3a\u9ad8\u6548\u5fae\u8c03\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2505.13643", "pdf": "https://arxiv.org/pdf/2505.13643", "abs": "https://arxiv.org/abs/2505.13643", "authors": ["Rakibul Hasan Rajib", "Md Akil Raihan Iftee", "Mir Sazzat Hossain", "A. K. M. Mahbubur Rahman", "Sajib Mistry", "M Ashraful Amin", "Amin Ahsan Ali"], "title": "FedCTTA: A Collaborative Approach to Continual Test-Time Adaptation in Federated Learning", "categories": ["cs.LG", "cs.CV"], "comment": "8 pages, 5 figures, Accepted In IJCNN 2025", "summary": "Federated Learning (FL) enables collaborative model training across\ndistributed clients without sharing raw data, making it ideal for\nprivacy-sensitive applications. However, FL models often suffer performance\ndegradation due to distribution shifts between training and deployment.\nTest-Time Adaptation (TTA) offers a promising solution by allowing models to\nadapt using only test samples. However, existing TTA methods in FL face\nchallenges such as computational overhead, privacy risks from feature sharing,\nand scalability concerns due to memory constraints. To address these\nlimitations, we propose Federated Continual Test-Time Adaptation (FedCTTA), a\nprivacy-preserving and computationally efficient framework for federated\nadaptation. Unlike prior methods that rely on sharing local feature statistics,\nFedCTTA avoids direct feature exchange by leveraging similarity-aware\naggregation based on model output distributions over randomly generated noise\nsamples. This approach ensures adaptive knowledge sharing while preserving data\nprivacy. Furthermore, FedCTTA minimizes the entropy at each client for\ncontinual adaptation, enhancing the model's confidence in evolving target\ndistributions. Our method eliminates the need for server-side training during\nadaptation and maintains a constant memory footprint, making it scalable even\nas the number of clients or training rounds increases. Extensive experiments\nshow that FedCTTA surpasses existing methods across diverse temporal and\nspatial heterogeneity scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFedCTTA\u7684\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6d4b\u8bd5\u65f6\u9002\u5e94\u65b9\u6cd5\u5728\u8ba1\u7b97\u5f00\u9500\u3001\u9690\u79c1\u98ce\u9669\u548c\u53ef\u6269\u5c55\u6027\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u5728\u9690\u79c1\u654f\u611f\u5e94\u7528\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u6a21\u578b\u6027\u80fd\u5e38\u56e0\u8bad\u7ec3\u4e0e\u90e8\u7f72\u95f4\u7684\u5206\u5e03\u504f\u79fb\u800c\u4e0b\u964d\u3002\u6d4b\u8bd5\u65f6\u9002\u5e94\uff08TTA\uff09\u867d\u80fd\u89e3\u51b3\u6b64\u95ee\u9898\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u8ba1\u7b97\u5f00\u9500\u5927\u3001\u9690\u79c1\u98ce\u9669\u9ad8\u548c\u53ef\u6269\u5c55\u6027\u5dee\u7b49\u6311\u6218\u3002", "method": "FedCTTA\u901a\u8fc7\u57fa\u4e8e\u6a21\u578b\u8f93\u51fa\u5206\u5e03\u7684\u76f8\u4f3c\u6027\u611f\u77e5\u805a\u5408\uff0c\u907f\u514d\u76f4\u63a5\u7279\u5f81\u5171\u4eab\uff0c\u540c\u65f6\u5229\u7528\u968f\u673a\u566a\u58f0\u6837\u672c\u6700\u5c0f\u5316\u5ba2\u6237\u7aef\u71b5\uff0c\u5b9e\u73b0\u6301\u7eed\u9002\u5e94\u3002\u8be5\u65b9\u6cd5\u65e0\u9700\u670d\u52a1\u5668\u7aef\u8bad\u7ec3\uff0c\u5185\u5b58\u5360\u7528\u6052\u5b9a\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFedCTTA\u5728\u65f6\u95f4\u548c\u7a7a\u95f4\u5f02\u8d28\u6027\u573a\u666f\u4e0b\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "FedCTTA\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u9690\u79c1\u4fdd\u62a4\u4e14\u53ef\u6269\u5c55\u7684\u8054\u90a6\u5b66\u4e60\u9002\u5e94\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u6027\u80fd\u3002"}}
{"id": "2505.14354", "pdf": "https://arxiv.org/pdf/2505.14354", "abs": "https://arxiv.org/abs/2505.14354", "authors": ["Xin Li", "Mengbing Liu", "Li Wei", "Jiancheng An", "M\u00e9rouane Debbah", "Chau Yuen"], "title": "WirelessMathBench: A Mathematical Modeling Benchmark for LLMs in Wireless Communications", "categories": ["cs.CL", "cs.LG"], "comment": "Accepted to ACL 2025 Findings", "summary": "Large Language Models (LLMs) have achieved impressive results across a broad\narray of tasks, yet their capacity for complex, domain-specific mathematical\nreasoning-particularly in wireless communications-remains underexplored. In\nthis work, we introduce WirelessMathBench, a novel benchmark specifically\ndesigned to evaluate LLMs on mathematical modeling challenges to wireless\ncommunications engineering. Our benchmark consists of 587 meticulously curated\nquestions sourced from 40 state-of-the-art research papers, encompassing a\ndiverse spectrum of tasks ranging from basic multiple-choice questions to\ncomplex equation completion tasks, including both partial and full completions,\nall of which rigorously adhere to physical and dimensional constraints. Through\nextensive experimentation with leading LLMs, we observe that while many models\nexcel in basic recall tasks, their performance degrades significantly when\nreconstructing partially or fully obscured equations, exposing fundamental\nlimitations in current LLMs. Even DeepSeek-R1, the best performer on our\nbenchmark, achieves an average accuracy of only 38.05%, with a mere 7.83%\nsuccess rate in full equation completion. By publicly releasing\nWirelessMathBench along with the evaluation toolkit, we aim to advance the\ndevelopment of more robust, domain-aware LLMs for wireless system analysis and\nbroader engineering applications.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86WirelessMathBench\uff0c\u4e00\u4e2a\u4e13\u95e8\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u65e0\u7ebf\u901a\u4fe1\u6570\u5b66\u5efa\u6a21\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u53d1\u73b0\u5f53\u524d\u6a21\u578b\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002", "motivation": "\u63a2\u7d22\u5927\u8bed\u8a00\u6a21\u578b\u5728\u65e0\u7ebf\u901a\u4fe1\u9886\u57df\u590d\u6742\u6570\u5b66\u63a8\u7406\u80fd\u529b\u7684\u4e0d\u8db3\u3002", "method": "\u8bbe\u8ba1\u4e86\u5305\u542b587\u4e2a\u95ee\u9898\u7684WirelessMathBench\uff0c\u6db5\u76d6\u591a\u6837\u4efb\u52a1\uff0c\u6d4b\u8bd5\u4e86\u591a\u4e2a\u9886\u5148\u6a21\u578b\u3002", "result": "\u6a21\u578b\u5728\u57fa\u7840\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u590d\u6742\u65b9\u7a0b\u5b8c\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u5dee\uff0c\u6700\u4f73\u6a21\u578b\u5e73\u5747\u51c6\u786e\u7387\u4ec538.05%\u3002", "conclusion": "\u901a\u8fc7\u516c\u5f00\u57fa\u51c6\u6d4b\u8bd5\u548c\u5de5\u5177\u5305\uff0c\u63a8\u52a8\u5f00\u53d1\u66f4\u5f3a\u5927\u7684\u9886\u57df\u611f\u77e5\u5927\u8bed\u8a00\u6a21\u578b\u3002"}}
{"id": "2505.13740", "pdf": "https://arxiv.org/pdf/2505.13740", "abs": "https://arxiv.org/abs/2505.13740", "authors": ["Chenning Yu", "Sicun Gao"], "title": "Improving Compositional Generation with Diffusion Models Using Lift Scores", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "We introduce a novel resampling criterion using lift scores, for improving\ncompositional generation in diffusion models. By leveraging the lift scores, we\nevaluate whether generated samples align with each single condition and then\ncompose the results to determine whether the composed prompt is satisfied. Our\nkey insight is that lift scores can be efficiently approximated using only the\noriginal diffusion model, requiring no additional training or external modules.\nWe develop an optimized variant that achieves relatively lower computational\noverhead during inference while maintaining effectiveness. Through extensive\nexperiments, we demonstrate that lift scores significantly improved the\ncondition alignment for compositional generation across 2D synthetic data,\nCLEVR position tasks, and text-to-image synthesis. Our code is available at\nhttp://github.com/rainorangelemon/complift.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u63d0\u5347\u5206\u6570\u7684\u65b0\u91cd\u91c7\u6837\u6807\u51c6\uff0c\u7528\u4e8e\u6539\u8fdb\u6269\u6563\u6a21\u578b\u4e2d\u7684\u7ec4\u5408\u751f\u6210\u3002", "motivation": "\u901a\u8fc7\u63d0\u5347\u5206\u6570\u8bc4\u4f30\u751f\u6210\u6837\u672c\u662f\u5426\u6ee1\u8db3\u5355\u4e2a\u6761\u4ef6\uff0c\u8fdb\u800c\u7ec4\u5408\u7ed3\u679c\u5224\u65ad\u662f\u5426\u6ee1\u8db3\u7ec4\u5408\u63d0\u793a\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6216\u5916\u90e8\u6a21\u5757\u3002", "method": "\u5229\u7528\u539f\u59cb\u6269\u6563\u6a21\u578b\u9ad8\u6548\u8fd1\u4f3c\u63d0\u5347\u5206\u6570\uff0c\u5f00\u53d1\u4e86\u8ba1\u7b97\u5f00\u9500\u8f83\u4f4e\u7684\u4f18\u5316\u53d8\u4f53\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u63d0\u5347\u5206\u6570\u663e\u8457\u63d0\u9ad8\u4e862D\u5408\u6210\u6570\u636e\u3001CLEVR\u4f4d\u7f6e\u4efb\u52a1\u548c\u6587\u672c\u5230\u56fe\u50cf\u5408\u6210\u4e2d\u7684\u6761\u4ef6\u5bf9\u9f50\u6548\u679c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u6709\u6548\u6027\u7684\u540c\u65f6\u964d\u4f4e\u4e86\u8ba1\u7b97\u5f00\u9500\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2505.14367", "pdf": "https://arxiv.org/pdf/2505.14367", "abs": "https://arxiv.org/abs/2505.14367", "authors": ["Jialong Han", "Si Zhang", "Ke Zhang"], "title": "Dual Decomposition of Weights and Singular Value Low Rank Adaptation", "categories": ["cs.CL"], "comment": null, "summary": "Parameter-Efficient Fine-Tuning (PEFT) has emerged as a critical paradigm for\nadapting Large Language Models (LLMs) to downstream tasks, among which Low-rank\nAdaptation (LoRA) represents one of the most widely adopted methodologies.\nHowever, existing LoRA-based approaches exhibit two fundamental limitations:\nunstable training dynamics and inefficient knowledge transfer from pre-trained\nmodels, both stemming from random initialization of adapter parameters. To\novercome these challenges, we propose DuDe, a novel approach that decomposes\nweight matrices into magnitude and direction components, employing Singular\nValue Decomposition (SVD) for principled initialization. Our comprehensive\nevaluation demonstrates DuDe's superior performance and robustness, achieving\nup to 48.35\\% accuracy on MMLU and 62.53\\% ($\\pm$ 1.59) accuracy on GSM8K. Our\ntheoretical analysis and empirical validation collectively demonstrate that\nDuDe's decomposition strategy enhances optimization stability and better\npreserves pre-trained representations, particularly for domain-specific tasks\nrequiring specialized knowledge. The combination of robust empirical\nperformance and rigorous theoretical foundations establishes DuDe as a\nsignificant contribution to PEFT methodologies for LLMs.", "AI": {"tldr": "DuDe\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eSVD\u5206\u89e3\u7684PEFT\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86LoRA\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u548c\u77e5\u8bc6\u8fc1\u79fb\u6548\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u73b0\u6709LoRA\u65b9\u6cd5\u56e0\u968f\u673a\u521d\u59cb\u5316\u5bfc\u81f4\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u548c\u77e5\u8bc6\u8fc1\u79fb\u6548\u7387\u4f4e\uff0c\u9700\u6539\u8fdb\u3002", "method": "\u901a\u8fc7SVD\u5206\u89e3\u6743\u91cd\u77e9\u9635\u4e3a\u5e45\u5ea6\u548c\u65b9\u5411\u5206\u91cf\uff0c\u8fdb\u884c\u6709\u539f\u5219\u7684\u521d\u59cb\u5316\u3002", "result": "\u5728MMLU\u548cGSM8K\u4efb\u52a1\u4e0a\u5206\u522b\u8fbe\u523048.35%\u548c62.53%\u7684\u51c6\u786e\u7387\u3002", "conclusion": "DuDe\u901a\u8fc7\u5206\u89e3\u7b56\u7565\u63d0\u5347\u4e86\u4f18\u5316\u7a33\u5b9a\u6027\u5e76\u4fdd\u7559\u4e86\u9884\u8bad\u7ec3\u8868\u793a\uff0c\u6210\u4e3aPEFT\u9886\u57df\u7684\u91cd\u8981\u8d21\u732e\u3002"}}
{"id": "2505.13813", "pdf": "https://arxiv.org/pdf/2505.13813", "abs": "https://arxiv.org/abs/2505.13813", "authors": ["Matthew Raffel", "Lizhong Chen"], "title": "FlashKAT: Understanding and Addressing Performance Bottlenecks in the Kolmogorov-Arnold Transformer", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "The Kolmogorov-Arnold Network (KAN) has been gaining popularity as an\nalternative to the multi-layer perceptron (MLP) with its increased\nexpressiveness and interpretability. However, the KAN can be orders of\nmagnitude slower due to its increased computational cost and training\ninstability, limiting its applicability to larger-scale tasks. Recently, the\nKolmogorov-Arnold Transformer (KAT) has been proposed, which can achieve FLOPs\nsimilar to the traditional Transformer with MLPs by leveraging Group-Rational\nKAN (GR-KAN). Unfortunately, despite the comparable FLOPs, our\ncharacterizations reveal that the KAT is still 123x slower in training speeds,\nindicating that there are other performance bottlenecks beyond FLOPs. In this\npaper, we conduct a series of experiments to understand the root cause of the\nslowdown in KAT. We uncover that the slowdown can be isolated to memory stalls\nand, more specifically, in the backward pass of GR-KAN caused by inefficient\ngradient accumulation. To address this memory bottleneck, we propose FlashKAT,\nwhich builds on our restructured kernel that minimizes gradient accumulation\nwith atomic adds and accesses to slow memory. Evaluations demonstrate that\nFlashKAT can achieve a training speedup of 86.5x compared with the\nstate-of-the-art KAT, while reducing rounding errors in the coefficient\ngradients. Our code is available at https://github.com/OSU-STARLAB/FlashKAT.", "AI": {"tldr": "FlashKAT\u901a\u8fc7\u4f18\u5316\u5185\u5b58\u8bbf\u95ee\u548c\u68af\u5ea6\u7d2f\u79ef\uff0c\u663e\u8457\u63d0\u5347\u4e86KAT\u7684\u8bad\u7ec3\u901f\u5ea6\uff0c\u89e3\u51b3\u4e86\u5176\u6027\u80fd\u74f6\u9888\u3002", "motivation": "KAN\u548cKAT\u56e0\u5176\u8868\u8fbe\u80fd\u529b\u548c\u53ef\u89e3\u91ca\u6027\u53d7\u5230\u5173\u6ce8\uff0c\u4f46\u8bad\u7ec3\u901f\u5ea6\u6162\u9650\u5236\u4e86\u5176\u5e94\u7528\u3002\u672c\u7814\u7a76\u65e8\u5728\u627e\u51faKAT\u901f\u5ea6\u6162\u7684\u6839\u672c\u539f\u56e0\u5e76\u63d0\u51fa\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u5206\u6790KAT\u7684\u6027\u80fd\u74f6\u9888\uff0c\u53d1\u73b0\u5185\u5b58\u505c\u6ede\u548c\u68af\u5ea6\u7d2f\u79ef\u6548\u7387\u4f4e\u662f\u4e3b\u8981\u539f\u56e0\uff0c\u63d0\u51faFlashKAT\u4f18\u5316\u5185\u6838\u4ee5\u51cf\u5c11\u5185\u5b58\u8bbf\u95ee\u548c\u68af\u5ea6\u7d2f\u79ef\u3002", "result": "FlashKAT\u6bd4\u73b0\u6709KAT\u5feb86.5\u500d\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u7cfb\u6570\u68af\u5ea6\u7684\u820d\u5165\u8bef\u5dee\u3002", "conclusion": "FlashKAT\u6709\u6548\u89e3\u51b3\u4e86KAT\u7684\u6027\u80fd\u74f6\u9888\uff0c\u4e3a\u5927\u89c4\u6a21\u4efb\u52a1\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.14376", "pdf": "https://arxiv.org/pdf/2505.14376", "abs": "https://arxiv.org/abs/2505.14376", "authors": ["Maitreya Prafulla Chitale", "Ketaki Mangesh Shetye", "Harshit Gupta", "Manav Chaudhary", "Vasudeva Varma"], "title": "AutoRev: Automatic Peer Review System for Academic Research Papers", "categories": ["cs.CL"], "comment": null, "summary": "Generating a review for an academic research paper is a complex task that\nrequires a deep understanding of the document's content and the\ninterdependencies between its sections. It demands not only insight into\ntechnical details but also an appreciation of the paper's overall coherence and\nstructure. Recent methods have predominantly focused on fine-tuning large\nlanguage models (LLMs) to address this challenge. However, they often overlook\nthe computational and performance limitations imposed by long input token\nlengths. To address this, we introduce AutoRev, an Automatic Peer Review System\nfor Academic Research Papers. Our novel framework represents an academic\ndocument as a graph, enabling the extraction of the most critical passages that\ncontribute significantly to the review. This graph-based approach demonstrates\neffectiveness for review generation and is potentially adaptable to various\ndownstream tasks, such as question answering, summarization, and document\nrepresentation. When applied to review generation, our method outperforms SOTA\nbaselines by an average of 58.72% across all evaluation metrics. We hope that\nour work will stimulate further research in applying graph-based extraction\ntechniques to other downstream tasks in NLP. We plan to make our code public\nupon acceptance.", "AI": {"tldr": "AutoRev\u662f\u4e00\u4e2a\u57fa\u4e8e\u56fe\u7684\u81ea\u52a8\u5b66\u672f\u8bba\u6587\u8bc4\u5ba1\u7cfb\u7edf\uff0c\u901a\u8fc7\u63d0\u53d6\u5173\u952e\u6bb5\u843d\u751f\u6210\u8bc4\u5ba1\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd558.72%\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u5ffd\u89c6\u4e86\u957f\u8f93\u5165\u4ee4\u724c\u7684\u8ba1\u7b97\u548c\u6027\u80fd\u9650\u5236\u3002", "method": "\u5c06\u5b66\u672f\u8bba\u6587\u8868\u793a\u4e3a\u56fe\uff0c\u63d0\u53d6\u5173\u952e\u6bb5\u843d\u7528\u4e8e\u8bc4\u5ba1\u751f\u6210\u3002", "result": "\u5728\u8bc4\u5ba1\u751f\u6210\u4efb\u52a1\u4e2d\uff0c\u6027\u80fd\u5e73\u5747\u63d0\u534758.72%\u3002", "conclusion": "\u56fe\u63d0\u53d6\u6280\u672f\u6709\u671b\u5e94\u7528\u4e8e\u5176\u4ed6NLP\u4e0b\u6e38\u4efb\u52a1\uff0c\u4ee3\u7801\u5c06\u516c\u5f00\u3002"}}
{"id": "2505.13841", "pdf": "https://arxiv.org/pdf/2505.13841", "abs": "https://arxiv.org/abs/2505.13841", "authors": ["Yixuan Gao", "Xiongkuo Min", "Guangtao Zhai"], "title": "Exploring Image Quality Assessment from a New Perspective: Pupil Size", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "This paper explores how the image quality assessment (IQA) task affects the\ncognitive processes of people from the perspective of pupil size and studies\nthe relationship between pupil size and image quality. Specifically, we first\ninvited subjects to participate in a subjective experiment, which includes two\ntasks: free observation and IQA. In the free observation task, subjects did not\nneed to perform any action, and they only needed to observe images as they\nusually do with an album. In the IQA task, subjects were required to score\nimages according to their overall impression of image quality. Then, by\nanalyzing the difference in pupil size between the two tasks, we find that\npeople may activate the visual attention mechanism when evaluating image\nquality. Meanwhile, we also find that the change in pupil size is closely\nrelated to image quality in the IQA task. For future research on IQA, this\nresearch can not only provide a theoretical basis for the objective IQA method\nand promote the development of more effective objective IQA methods, but also\nprovide a new subjective IQA method for collecting the authentic subjective\nimpression of image quality.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\uff08IQA\uff09\u4efb\u52a1\u5982\u4f55\u901a\u8fc7\u77b3\u5b54\u5927\u5c0f\u5f71\u54cd\u4eba\u7684\u8ba4\u77e5\u8fc7\u7a0b\uff0c\u5e76\u5206\u6790\u4e86\u77b3\u5b54\u5927\u5c0f\u4e0e\u56fe\u50cf\u8d28\u91cf\u7684\u5173\u7cfb\u3002", "motivation": "\u63a2\u7d22IQA\u4efb\u52a1\u5bf9\u8ba4\u77e5\u8fc7\u7a0b\u7684\u5f71\u54cd\uff0c\u4e3a\u5ba2\u89c2IQA\u65b9\u6cd5\u63d0\u4f9b\u7406\u8bba\u57fa\u7840\uff0c\u5e76\u5f00\u53d1\u65b0\u7684\u4e3b\u89c2IQA\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u81ea\u7531\u89c2\u5bdf\u548cIQA\u4efb\u52a1\u7684\u4e3b\u89c2\u5b9e\u9a8c\uff0c\u5206\u6790\u77b3\u5b54\u5927\u5c0f\u5dee\u5f02\u3002", "result": "\u53d1\u73b0IQA\u4efb\u52a1\u6fc0\u6d3b\u4e86\u89c6\u89c9\u6ce8\u610f\u529b\u673a\u5236\uff0c\u4e14\u77b3\u5b54\u53d8\u5316\u4e0e\u56fe\u50cf\u8d28\u91cf\u5bc6\u5207\u76f8\u5173\u3002", "conclusion": "\u7814\u7a76\u4e3a\u5ba2\u89c2IQA\u65b9\u6cd5\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4e3b\u89c2IQA\u65b9\u6cd5\u3002"}}
{"id": "2505.14393", "pdf": "https://arxiv.org/pdf/2505.14393", "abs": "https://arxiv.org/abs/2505.14393", "authors": ["Nadir Durrani", "Basel Mousi", "Fahim Dalvi"], "title": "Editing Across Languages: A Survey of Multilingual Knowledge Editing", "categories": ["cs.CL"], "comment": null, "summary": "While Knowledge Editing has been extensively studied in monolingual settings,\nit remains underexplored in multilingual contexts. This survey systematizes\nrecent research on Multilingual Knowledge Editing (MKE), a growing subdomain of\nmodel editing focused on ensuring factual edits generalize reliably across\nlanguages. We present a comprehensive taxonomy of MKE methods, covering\nparameter-based, memory-based, fine-tuning, and hypernetwork approaches. We\nsurvey available benchmarks,summarize key findings on method effectiveness and\ntransfer patterns, identify challenges in cross-lingual propagation, and\nhighlight open problems related to language anisotropy, evaluation coverage,\nand edit scalability. Our analysis consolidates a rapidly evolving area and\nlays the groundwork for future progress in editable language-aware LLMs.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u5316\u7814\u7a76\u4e86\u591a\u8bed\u8a00\u77e5\u8bc6\u7f16\u8f91\uff08MKE\uff09\u9886\u57df\uff0c\u603b\u7ed3\u4e86\u65b9\u6cd5\u3001\u57fa\u51c6\u548c\u6311\u6218\uff0c\u4e3a\u672a\u6765\u53ef\u7f16\u8f91\u8bed\u8a00\u611f\u77e5\u5927\u6a21\u578b\u7684\u53d1\u5c55\u5960\u5b9a\u57fa\u7840\u3002", "motivation": "\u5c3d\u7ba1\u77e5\u8bc6\u7f16\u8f91\u5728\u5355\u8bed\u8a00\u73af\u5883\u4e2d\u5df2\u5e7f\u6cdb\u7814\u7a76\uff0c\u4f46\u5728\u591a\u8bed\u8a00\u80cc\u666f\u4e0b\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u63d0\u51fa\u4e86MKE\u65b9\u6cd5\u7684\u7efc\u5408\u5206\u7c7b\uff0c\u5305\u62ec\u53c2\u6570\u5316\u3001\u57fa\u4e8e\u8bb0\u5fc6\u3001\u5fae\u8c03\u548c\u8d85\u7f51\u7edc\u65b9\u6cd5\uff0c\u5e76\u8c03\u67e5\u4e86\u73b0\u6709\u57fa\u51c6\u3002", "result": "\u603b\u7ed3\u4e86\u65b9\u6cd5\u6548\u679c\u548c\u8fc1\u79fb\u6a21\u5f0f\u7684\u5173\u952e\u53d1\u73b0\uff0c\u6307\u51fa\u4e86\u8de8\u8bed\u8a00\u4f20\u64ad\u7684\u6311\u6218\u3002", "conclusion": "\u672c\u6587\u6574\u5408\u4e86\u5feb\u901f\u53d1\u5c55\u7684MKE\u9886\u57df\uff0c\u4e3a\u672a\u6765\u53ef\u7f16\u8f91\u8bed\u8a00\u611f\u77e5\u5927\u6a21\u578b\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2505.13875", "pdf": "https://arxiv.org/pdf/2505.13875", "abs": "https://arxiv.org/abs/2505.13875", "authors": ["Lanlan Kang", "Jian Wang", "Jian QIn", "Yiqin Liang", "Yongjun He"], "title": "Automated Quality Evaluation of Cervical Cytopathology Whole Slide Images Based on Content Analysis", "categories": ["eess.IV", "cs.CV"], "comment": "12 pages, 10 figures", "summary": "The ThinPrep Cytologic Test (TCT) is the most widely used method for cervical\ncancer screening, and the sample quality directly impacts the accuracy of the\ndiagnosis. Traditional manual evaluation methods rely on the observation of\npathologist under microscopes. These methods exhibit high subjectivity, high\ncost, long duration, and low reliability. With the development of\ncomputer-aided diagnosis (CAD), an automated quality assessment system that\nperforms at the level of a professional pathologist is necessary. To address\nthis need, we propose a fully automated quality assessment method for Cervical\nCytopathology Whole Slide Images (WSIs) based on The Bethesda System (TBS)\ndiagnostic standards, artificial intelligence algorithms, and the\ncharacteristics of clinical data. The method analysis the context of WSIs to\nquantify quality evaluation metrics which are focused by TBS such as staining\nquality, cell counts and cell mass proportion through multiple models including\nobject detection, classification and segmentation. Subsequently, the XGBoost\nmodel is used to mine the attention paid by pathologists to different quality\nevaluation metrics when evaluating samples, thereby obtaining a comprehensive\nWSI sample score calculation model. Experimental results on 100 WSIs\ndemonstrate that the proposed evaluation method has significant advantages in\nterms of speed and consistency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4eba\u5de5\u667a\u80fd\u7684\u81ea\u52a8\u5316\u5bab\u9888\u7ec6\u80de\u75c5\u7406\u5b66\u5168\u5207\u7247\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bc4\u4f30\u901f\u5ea6\u548c\u4e00\u81f4\u6027\u3002", "motivation": "\u4f20\u7edf\u624b\u52a8\u8bc4\u4f30\u65b9\u6cd5\u4e3b\u89c2\u6027\u5f3a\u3001\u6210\u672c\u9ad8\u3001\u8017\u65f6\u957f\u4e14\u53ef\u9760\u6027\u4f4e\uff0c\u4e9f\u9700\u81ea\u52a8\u5316\u8d28\u91cf\u8bc4\u4f30\u7cfb\u7edf\u3002", "method": "\u7ed3\u5408TBS\u6807\u51c6\u3001AI\u7b97\u6cd5\u548c\u4e34\u5e8a\u6570\u636e\u7279\u5f81\uff0c\u901a\u8fc7\u76ee\u6807\u68c0\u6d4b\u3001\u5206\u7c7b\u548c\u5206\u5272\u6a21\u578b\u91cf\u5316\u8d28\u91cf\u6307\u6807\uff0c\u5229\u7528XGBoost\u6a21\u578b\u7efc\u5408\u8bc4\u5206\u3002", "result": "\u5728100\u5f20WSI\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u901f\u5ea6\u548c\u4e00\u81f4\u6027\u4e0a\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5bab\u9888\u764c\u7b5b\u67e5\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u9760\u7684\u8d28\u91cf\u8bc4\u4f30\u5de5\u5177\u3002"}}
{"id": "2505.14395", "pdf": "https://arxiv.org/pdf/2505.14395", "abs": "https://arxiv.org/abs/2505.14395", "authors": ["Seyoung Song", "Seogyeong Jeong", "Eunsu Kim", "Jiho Jin", "Dongkwan Kim", "Jay Shin", "Alice Oh"], "title": "MUG-Eval: A Proxy Evaluation Framework for Multilingual Generation Capabilities in Any Language", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Evaluating text generation capabilities of large language models (LLMs) is\nchallenging, particularly for low-resource languages where methods for direct\nassessment are scarce. We propose MUG-Eval, a novel framework that evaluates\nLLMs' multilingual generation capabilities by transforming existing benchmarks\ninto conversational tasks and measuring the LLMs' accuracies on those tasks. We\nspecifically designed these conversational tasks to require effective\ncommunication in the target language. Then, we simply use task success rate as\na proxy of successful conversation generation. Our approach offers two key\nadvantages: it is independent of language-specific NLP tools or annotated\ndatasets, which are limited for most languages, and it does not rely on\nLLMs-as-judges, whose evaluation quality degrades outside a few high-resource\nlanguages. We evaluate 8 LLMs across 30 languages spanning high, mid, and\nlow-resource categories, and we find that MUG-Eval correlates strongly with\nestablished benchmarks ($r$ > 0.75) while enabling standardized comparisons\nacross languages and models. Our framework provides a robust and\nresource-efficient solution for evaluating multilingual generation that can be\nextended to thousands of languages.", "AI": {"tldr": "MUG-Eval\u662f\u4e00\u4e2a\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u591a\u8bed\u8a00\u751f\u6210\u80fd\u529b\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u73b0\u6709\u57fa\u51c6\u8f6c\u5316\u4e3a\u5bf9\u8bdd\u4efb\u52a1\u5e76\u6d4b\u91cf\u4efb\u52a1\u6210\u529f\u7387\u6765\u8bc4\u4f30\u6a21\u578b\u8868\u73b0\u3002", "motivation": "\u8bc4\u4f30\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6587\u672c\u751f\u6210\u80fd\u529b\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u76f4\u63a5\u8bc4\u4f30\u65b9\u6cd5\u7a00\u7f3a\u3002", "method": "\u5c06\u73b0\u6709\u57fa\u51c6\u8f6c\u5316\u4e3a\u5bf9\u8bdd\u4efb\u52a1\uff0c\u4ee5\u4efb\u52a1\u6210\u529f\u7387\u4f5c\u4e3a\u751f\u6210\u80fd\u529b\u7684\u4ee3\u7406\u6307\u6807\uff0c\u907f\u514d\u4f9d\u8d56\u8bed\u8a00\u7279\u5b9a\u5de5\u5177\u6216\u6807\u6ce8\u6570\u636e\u3002", "result": "\u572830\u79cd\u8bed\u8a00\u4e0a\u8bc4\u4f308\u4e2a\u6a21\u578b\uff0cMUG-Eval\u4e0e\u73b0\u6709\u57fa\u51c6\u5f3a\u76f8\u5173\uff08r > 0.75\uff09\uff0c\u652f\u6301\u8de8\u8bed\u8a00\u548c\u6a21\u578b\u7684\u6807\u51c6\u5316\u6bd4\u8f83\u3002", "conclusion": "MUG-Eval\u4e3a\u591a\u8bed\u8a00\u751f\u6210\u8bc4\u4f30\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.13906", "pdf": "https://arxiv.org/pdf/2505.13906", "abs": "https://arxiv.org/abs/2505.13906", "authors": ["Soyabul Islam Lincoln", "Mirza Mohd Shahriar Maswood"], "title": "XDementNET: An Explainable Attention Based Deep Convolutional Network to Detect Alzheimer Progression from MRI data", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "20 pages, 12 figures,", "summary": "A common neurodegenerative disease, Alzheimer's disease requires a precise\ndiagnosis and efficient treatment, particularly in light of escalating\nhealthcare expenses and the expanding use of artificial intelligence in medical\ndiagnostics. Many recent studies shows that the combination of brain Magnetic\nResonance Imaging (MRI) and deep neural networks have achieved promising\nresults for diagnosing AD. Using deep convolutional neural networks, this paper\nintroduces a novel deep learning architecture that incorporates multiresidual\nblocks, specialized spatial attention blocks, grouped query attention, and\nmulti-head attention. The study assessed the model's performance on four\npublicly accessible datasets and concentrated on identifying binary and\nmulticlass issues across various categories. This paper also takes into account\nof the explainability of AD's progression and compared with state-of-the-art\nmethods namely Gradient Class Activation Mapping (GradCAM), Score-CAM, Faster\nScore-CAM, and XGRADCAM. Our methodology consistently outperforms current\napproaches, achieving 99.66\\% accuracy in 4-class classification, 99.63\\% in\n3-class classification, and 100\\% in binary classification using Kaggle\ndatasets. For Open Access Series of Imaging Studies (OASIS) datasets the\naccuracies are 99.92\\%, 99.90\\%, and 99.95\\% respectively. The Alzheimer's\nDisease Neuroimaging Initiative-1 (ADNI-1) dataset was used for experiments in\nthree planes (axial, sagittal, and coronal) and a combination of all planes.\nThe study achieved accuracies of 99.08\\% for axis, 99.85\\% for sagittal, 99.5\\%\nfor coronal, and 99.17\\% for all axis, and 97.79\\% and 8.60\\% respectively for\nADNI-2. The network's ability to retrieve important information from MRI images\nis demonstrated by its excellent accuracy in categorizing AD stages.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u591a\u6b8b\u5dee\u5757\u3001\u7a7a\u95f4\u6ce8\u610f\u529b\u673a\u5236\u548c\u591a\u79cd\u6ce8\u610f\u529b\u673a\u5236\u7684\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\uff0c\u7528\u4e8e\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u7684\u7cbe\u786e\u8bca\u65ad\uff0c\u5e76\u5728\u591a\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6781\u9ad8\u7684\u5206\u7c7b\u51c6\u786e\u7387\u3002", "motivation": "\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u7684\u7cbe\u786e\u8bca\u65ad\u9700\u6c42\u65e5\u76ca\u589e\u957f\uff0c\u7ed3\u5408\u4eba\u5de5\u667a\u80fd\u6280\u672f\u53ef\u4ee5\u63d0\u5347\u8bca\u65ad\u6548\u7387\u5e76\u964d\u4f4e\u533b\u7597\u6210\u672c\u3002", "method": "\u91c7\u7528\u6df1\u5ea6\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff0c\u7ed3\u5408\u591a\u6b8b\u5dee\u5757\u3001\u7a7a\u95f4\u6ce8\u610f\u529b\u5757\u3001\u5206\u7ec4\u67e5\u8be2\u6ce8\u610f\u529b\u548c\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5bf9MRI\u56fe\u50cf\u8fdb\u884c\u5206\u7c7b\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6781\u9ad8\u7684\u5206\u7c7b\u51c6\u786e\u7387\uff0c\u6700\u9ad8\u8fbe\u5230100%\uff08\u4e8c\u5206\u7c7b\uff09\u548c99.66%\uff08\u56db\u5206\u7c7b\uff09\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u8bca\u65ad\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u5177\u6709\u8f83\u597d\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002"}}
{"id": "2505.14398", "pdf": "https://arxiv.org/pdf/2505.14398", "abs": "https://arxiv.org/abs/2505.14398", "authors": ["Peter Baile Chen", "Yi Zhang", "Dan Roth", "Samuel Madden", "Jacob Andreas", "Michael Cafarella"], "title": "Log-Augmented Generation: Scaling Test-Time Reasoning with Reusable Computation", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Data and code are available at https://peterbaile.github.io/lag/", "summary": "While humans naturally learn and adapt from past experiences, large language\nmodels (LLMs) and their agentic counterparts struggle to retain reasoning from\nprevious tasks and apply them in future contexts. To address this limitation,\nwe propose a novel framework, log-augmented generation (LAG) that directly\nreuses prior computation and reasoning from past logs at test time to enhance\nmodel's ability to learn from previous tasks and perform better on new, unseen\nchallenges, all while keeping the system efficient and scalable. Specifically,\nour system represents task logs using key-value (KV) caches, encoding the full\nreasoning context of prior tasks while storing KV caches for only a selected\nsubset of tokens. When a new task arises, LAG retrieves the KV values from\nrelevant logs to augment generation. Our approach differs from reflection-based\nmemory mechanisms by directly reusing prior reasoning and computations without\nrequiring additional steps for knowledge extraction or distillation. Our method\nalso goes beyond existing KV caching techniques, which primarily target\nefficiency gains rather than improving accuracy. Experiments on knowledge- and\nreasoning-intensive datasets demonstrate that our method significantly\noutperforms standard agentic systems that do not utilize logs, as well as\nexisting solutions based on reflection and KV cache techniques.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3alog-augmented generation (LAG)\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u76f4\u63a5\u590d\u7528\u8fc7\u53bb\u7684\u8ba1\u7b97\u548c\u63a8\u7406\u65e5\u5fd7\u6765\u589e\u5f3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u65b0\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6548\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u4eba\u7c7b\u80fd\u591f\u4ece\u8fc7\u53bb\u7684\u7ecf\u9a8c\u4e2d\u5b66\u4e60\u548c\u9002\u5e94\uff0c\u800c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u53ca\u5176\u4ee3\u7406\u7cfb\u7edf\u96be\u4ee5\u4fdd\u7559\u548c\u590d\u7528\u4e4b\u524d\u7684\u63a8\u7406\u80fd\u529b\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86LAG\u6846\u67b6\u3002", "method": "LAG\u5229\u7528\u952e\u503c\uff08KV\uff09\u7f13\u5b58\u8868\u793a\u4efb\u52a1\u65e5\u5fd7\uff0c\u4ec5\u5b58\u50a8\u90e8\u5206\u5173\u952e\u6807\u8bb0\u7684KV\u7f13\u5b58\u3002\u5728\u65b0\u4efb\u52a1\u4e2d\uff0c\u7cfb\u7edf\u68c0\u7d22\u76f8\u5173\u65e5\u5fd7\u7684KV\u503c\u4ee5\u589e\u5f3a\u751f\u6210\u80fd\u529b\uff0c\u76f4\u63a5\u590d\u7528\u8fc7\u53bb\u7684\u63a8\u7406\u548c\u8ba1\u7b97\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLAG\u5728\u77e5\u8bc6\u548c\u63a8\u7406\u5bc6\u96c6\u578b\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u672a\u4f7f\u7528\u65e5\u5fd7\u7684\u6807\u51c6\u4ee3\u7406\u7cfb\u7edf\uff0c\u4ee5\u53ca\u57fa\u4e8e\u53cd\u601d\u548cKV\u7f13\u5b58\u6280\u672f\u7684\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "LAG\u901a\u8fc7\u76f4\u63a5\u590d\u7528\u8fc7\u53bb\u7684\u63a8\u7406\u548c\u8ba1\u7b97\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u65b0\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u6548\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2505.13911", "pdf": "https://arxiv.org/pdf/2505.13911", "abs": "https://arxiv.org/abs/2505.13911", "authors": ["Ruijie Zhao", "Zuopeng Tan", "Xiao Xue", "Longfei Zhao", "Bing Li", "Zicheng Liao", "Ying Ming", "Jiaru Wang", "Ran Xiao", "Sirong Piao", "Rui Zhao", "Qiqi Xu", "Wei Song"], "title": "Bronchovascular Tree-Guided Weakly Supervised Learning Method for Pulmonary Segment Segmentation", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Pulmonary segment segmentation is crucial for cancer localization and\nsurgical planning. However, the pixel-wise annotation of pulmonary segments is\nlaborious, as the boundaries between segments are indistinguishable in medical\nimages. To this end, we propose a weakly supervised learning (WSL) method,\ntermed Anatomy-Hierarchy Supervised Learning (AHSL), which consults the precise\nclinical anatomical definition of pulmonary segments to perform pulmonary\nsegment segmentation. Since pulmonary segments reside within the lobes and are\ndetermined by the bronchovascular tree, i.e., artery, airway and vein, the\ndesign of the loss function is founded on two principles. First, segment-level\nlabels are utilized to directly supervise the output of the pulmonary segments,\nensuring that they accurately encompass the appropriate bronchovascular tree.\nSecond, lobe-level supervision indirectly oversees the pulmonary segment,\nensuring their inclusion within the corresponding lobe. Besides, we introduce a\ntwo-stage segmentation strategy that incorporates bronchovascular priori\ninformation. Furthermore, a consistency loss is proposed to enhance the\nsmoothness of segment boundaries, along with an evaluation metric designed to\nmeasure the smoothness of pulmonary segment boundaries. Visual inspection and\nevaluation metrics from experiments conducted on a private dataset demonstrate\nthe effectiveness of our method.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89e3\u5256\u5c42\u6b21\u76d1\u7763\u5b66\u4e60\uff08AHSL\uff09\u7684\u5f31\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u80ba\u90e8\u6bb5\u5206\u5272\uff0c\u7ed3\u5408\u4e34\u5e8a\u89e3\u5256\u5b9a\u4e49\u548c\u652f\u6c14\u7ba1\u8840\u7ba1\u6811\u4fe1\u606f\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u5206\u5272\u7b56\u7565\u548c\u4e00\u81f4\u6027\u635f\u5931\u63d0\u5347\u8fb9\u754c\u5e73\u6ed1\u5ea6\u3002", "motivation": "\u80ba\u90e8\u6bb5\u5206\u5272\u5bf9\u764c\u75c7\u5b9a\u4f4d\u548c\u624b\u672f\u89c4\u5212\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u50cf\u7d20\u7ea7\u6807\u6ce8\u8017\u65f6\u4e14\u8fb9\u754c\u96be\u4ee5\u533a\u5206\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u5f31\u76d1\u7763\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u6bb5\u7ea7\u548c\u53f6\u7ea7\u6807\u7b7e\u76d1\u7763\uff0c\u7ed3\u5408\u652f\u6c14\u7ba1\u8840\u7ba1\u5148\u9a8c\u4fe1\u606f\u7684\u4e24\u9636\u6bb5\u5206\u5272\u7b56\u7565\uff0c\u5e76\u63d0\u51fa\u4e00\u81f4\u6027\u635f\u5931\u548c\u8fb9\u754c\u5e73\u6ed1\u5ea6\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u5728\u79c1\u6709\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u89c6\u89c9\u68c0\u67e5\u548c\u8bc4\u4f30\u6307\u6807\u4e0a\u5747\u8868\u73b0\u6709\u6548\u3002", "conclusion": "AHSL\u65b9\u6cd5\u901a\u8fc7\u89e3\u5256\u5c42\u6b21\u76d1\u7763\u548c\u4e24\u9636\u6bb5\u7b56\u7565\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u80ba\u90e8\u6bb5\u5206\u5272\u3002"}}
{"id": "2505.14406", "pdf": "https://arxiv.org/pdf/2505.14406", "abs": "https://arxiv.org/abs/2505.14406", "authors": ["Haoming Huang", "Yibo Yan", "Jiahao Huo", "Xin Zou", "Xinfeng Li", "Kun Wang", "Xuming Hu"], "title": "Pierce the Mists, Greet the Sky: Decipher Knowledge Overshadowing via Knowledge Circuit Analysis", "categories": ["cs.CL"], "comment": "18 pages, 6 figures, EMNLP under review", "summary": "Large Language Models (LLMs), despite their remarkable capabilities, are\nhampered by hallucinations. A particularly challenging variant, knowledge\novershadowing, occurs when one piece of activated knowledge inadvertently masks\nanother relevant piece, leading to erroneous outputs even with high-quality\ntraining data. Current understanding of overshadowing is largely confined to\ninference-time observations, lacking deep insights into its origins and\ninternal mechanisms during model training. Therefore, we introduce\nPhantomCircuit, a novel framework designed to comprehensively analyze and\ndetect knowledge overshadowing. By innovatively employing knowledge circuit\nanalysis, PhantomCircuit dissects the internal workings of attention heads,\ntracing how competing knowledge pathways contribute to the overshadowing\nphenomenon and its evolution throughout the training process. Extensive\nexperiments demonstrate PhantomCircuit's effectiveness in identifying such\ninstances, offering novel insights into this elusive hallucination and\nproviding the research community with a new methodological lens for its\npotential mitigation.", "AI": {"tldr": "PhantomCircuit\u6846\u67b6\u901a\u8fc7\u77e5\u8bc6\u7535\u8def\u5206\u6790\uff0c\u63ed\u793aLLM\u4e2d\u77e5\u8bc6\u906e\u853d\u73b0\u8c61\u7684\u8d77\u6e90\u548c\u673a\u5236\uff0c\u5e76\u63d0\u4f9b\u68c0\u6d4b\u65b9\u6cd5\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5b58\u5728\u77e5\u8bc6\u906e\u853d\u95ee\u9898\uff0c\u5f53\u524d\u7814\u7a76\u5bf9\u5176\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u5185\u90e8\u673a\u5236\u7f3a\u4e4f\u6df1\u5165\u7406\u89e3\u3002", "method": "\u63d0\u51faPhantomCircuit\u6846\u67b6\uff0c\u5229\u7528\u77e5\u8bc6\u7535\u8def\u5206\u6790\u6ce8\u610f\u529b\u5934\uff0c\u8ffd\u8e2a\u7ade\u4e89\u77e5\u8bc6\u8def\u5f84\u53ca\u5176\u6f14\u53d8\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660ePhantomCircuit\u80fd\u6709\u6548\u8bc6\u522b\u77e5\u8bc6\u906e\u853d\u73b0\u8c61\uff0c\u4e3a\u7814\u7a76\u63d0\u4f9b\u65b0\u65b9\u6cd5\u3002", "conclusion": "PhantomCircuit\u4e3a\u7406\u89e3\u548c\u7f13\u89e3\u77e5\u8bc6\u906e\u853d\u73b0\u8c61\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u548c\u65b9\u6cd5\u3002"}}
{"id": "2505.14418", "pdf": "https://arxiv.org/pdf/2505.14418", "abs": "https://arxiv.org/abs/2505.14418", "authors": ["Pengzhou Cheng", "Haowen Hu", "Zheng Wu", "Zongru Wu", "Tianjie Ju", "Daizong Ding", "Zhuosheng Zhang", "Gongshen Liu"], "title": "Hidden Ghost Hand: Unveiling Backdoor Vulnerabilities in MLLM-Powered Mobile GUI Agents", "categories": ["cs.CL"], "comment": "25 pages, 10 figures, 12 Tables", "summary": "Graphical user interface (GUI) agents powered by multimodal large language\nmodels (MLLMs) have shown greater promise for human-interaction. However, due\nto the high fine-tuning cost, users often rely on open-source GUI agents or\nAPIs offered by AI providers, which introduces a critical but underexplored\nsupply chain threat: backdoor attacks. In this work, we first unveil that\nMLLM-powered GUI agents naturally expose multiple interaction-level triggers,\nsuch as historical steps, environment states, and task progress. Based on this\nobservation, we introduce AgentGhost, an effective and stealthy framework for\nred-teaming backdoor attacks. Specifically, we first construct composite\ntriggers by combining goal and interaction levels, allowing GUI agents to\nunintentionally activate backdoors while ensuring task utility. Then, we\nformulate backdoor injection as a Min-Max optimization problem that uses\nsupervised contrastive learning to maximize the feature difference across\nsample classes at the representation space, improving flexibility of the\nbackdoor. Meanwhile, it adopts supervised fine-tuning to minimize the\ndiscrepancy between backdoor and clean behavior generation, enhancing\neffectiveness and utility. Extensive evaluations of various agent models in two\nestablished mobile benchmarks show that AgentGhost is effective and generic,\nwith attack accuracy that reaches 99.7\\% on three attack objectives, and shows\nstealthiness with only 1\\% utility degradation. Furthermore, we tailor a\ndefense method against AgentGhost that reduces the attack accuracy to 22.1\\%.\nOur code is available at \\texttt{anonymous}.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAgentGhost\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5bf9\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u7684GUI\u4ee3\u7406\u8fdb\u884c\u9690\u853d\u7684\u540e\u95e8\u653b\u51fb\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u9ad8\u6548\u6027\u548c\u901a\u7528\u6027\u3002", "motivation": "\u7531\u4e8eGUI\u4ee3\u7406\u901a\u5e38\u4f9d\u8d56\u5f00\u6e90\u6a21\u578b\u6216API\uff0c\u5b58\u5728\u540e\u95e8\u653b\u51fb\u7684\u4f9b\u5e94\u94fe\u5a01\u80c1\uff0c\u4f46\u76ee\u524d\u7814\u7a76\u4e0d\u8db3\u3002\u8bba\u6587\u65e8\u5728\u63ed\u793a\u5e76\u5229\u7528GUI\u4ee3\u7406\u7684\u4ea4\u4e92\u7ea7\u89e6\u53d1\u5668\uff0c\u8bbe\u8ba1\u4e00\u79cd\u9ad8\u6548\u4e14\u9690\u853d\u7684\u540e\u95e8\u653b\u51fb\u6846\u67b6\u3002", "method": "\u901a\u8fc7\u7ec4\u5408\u76ee\u6807\u548c\u4ea4\u4e92\u7ea7\u89e6\u53d1\u5668\u6784\u5efa\u590d\u5408\u89e6\u53d1\u5668\uff0c\u5c06\u540e\u95e8\u6ce8\u5165\u5efa\u6a21\u4e3aMin-Max\u4f18\u5316\u95ee\u9898\uff0c\u5229\u7528\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u548c\u76d1\u7763\u5fae\u8c03\u6765\u589e\u5f3a\u540e\u95e8\u7684\u7075\u6d3b\u6027\u548c\u6709\u6548\u6027\u3002", "result": "\u5728\u591a\u4e2a\u4ee3\u7406\u6a21\u578b\u548c\u79fb\u52a8\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAgentGhost\u653b\u51fb\u51c6\u786e\u7387\u8fbe\u523099.7%\uff0c\u4e14\u4ec5\u5bfc\u81f41%\u7684\u6548\u7528\u4e0b\u964d\u3002\u8bba\u6587\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u9632\u5fa1\u65b9\u6cd5\uff0c\u5c06\u653b\u51fb\u51c6\u786e\u7387\u964d\u81f322.1%\u3002", "conclusion": "AgentGhost\u5c55\u793a\u4e86GUI\u4ee3\u7406\u7684\u540e\u95e8\u653b\u51fb\u98ce\u9669\uff0c\u5e76\u63d0\u51fa\u4e86\u6709\u6548\u7684\u9632\u5fa1\u65b9\u6848\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002"}}
{"id": "2505.14017", "pdf": "https://arxiv.org/pdf/2505.14017", "abs": "https://arxiv.org/abs/2505.14017", "authors": ["Jesper Duemose Nielsen", "Karthik Gopinath", "Andrew Hoopes", "Adrian Dalca", "Colin Magdamo", "Steven Arnold", "Sudeshna Das", "Axel Thielscher", "Juan Eugenio Iglesias", "Oula Puonti"], "title": "End-to-end Cortical Surface Reconstruction from Clinical Magnetic Resonance Images", "categories": ["eess.IV", "cs.CV"], "comment": "11 pages, 4 figures", "summary": "Surface-based cortical analysis is valuable for a variety of neuroimaging\ntasks, such as spatial normalization, parcellation, and gray matter (GM)\nthickness estimation. However, most tools for estimating cortical surfaces work\nexclusively on scans with at least 1 mm isotropic resolution and are tuned to a\nspecific magnetic resonance (MR) contrast, often T1-weighted (T1w). This\nprecludes application using most clinical MR scans, which are very\nheterogeneous in terms of contrast and resolution. Here, we use synthetic\ndomain-randomized data to train the first neural network for explicit\nestimation of cortical surfaces from scans of any contrast and resolution,\nwithout retraining. Our method deforms a template mesh to the white matter (WM)\nsurface, which guarantees topological correctness. This mesh is further\ndeformed to estimate the GM surface. We compare our method to\nrecon-all-clinical (RAC), an implicit surface reconstruction method which is\ncurrently the only other tool capable of processing heterogeneous clinical MR\nscans, on ADNI and a large clinical dataset (n=1,332). We show a approximately\n50 % reduction in cortical thickness error (from 0.50 to 0.24 mm) with respect\nto RAC and better recovery of the aging-related cortical thinning patterns\ndetected by FreeSurfer on high-resolution T1w scans. Our method enables fast\nand accurate surface reconstruction of clinical scans, allowing studies (1)\nwith sample sizes far beyond what is feasible in a research setting, and (2) of\nclinical populations that are difficult to enroll in research studies. The code\nis publicly available at https://github.com/simnibs/brainnet.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u901a\u7528\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u4efb\u610f\u5bf9\u6bd4\u5ea6\u548c\u5206\u8fa8\u7387\u7684MRI\u626b\u63cf\u4e2d\u4f30\u8ba1\u76ae\u8d28\u8868\u9762\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4e34\u5e8a\u626b\u63cf\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u5de5\u5177\u4ec5\u9002\u7528\u4e8e\u7279\u5b9a\u5206\u8fa8\u7387\u548c\u5bf9\u6bd4\u5ea6\u7684MRI\u626b\u63cf\uff0c\u9650\u5236\u4e86\u5728\u4e34\u5e8a\u6570\u636e\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u4f7f\u7528\u5408\u6210\u57df\u968f\u673a\u5316\u6570\u636e\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\uff0c\u901a\u8fc7\u6a21\u677f\u7f51\u683c\u53d8\u5f62\u4f30\u8ba1\u767d\u8d28\u548c\u7070\u8d28\u8868\u9762\u3002", "result": "\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0c\u76ae\u8d28\u539a\u5ea6\u8bef\u5dee\u51cf\u5c11\u7ea650%\uff0c\u5e76\u66f4\u597d\u5730\u6062\u590d\u4e86\u4e0e\u8870\u8001\u76f8\u5173\u7684\u76ae\u8d28\u53d8\u8584\u6a21\u5f0f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u4e34\u5e8a\u626b\u63cf\u63d0\u4f9b\u4e86\u5feb\u901f\u51c6\u786e\u7684\u8868\u9762\u91cd\u5efa\uff0c\u6269\u5c55\u4e86\u7814\u7a76\u6837\u672c\u8303\u56f4\u548c\u4e34\u5e8a\u4eba\u7fa4\u7684\u5e94\u7528\u3002"}}
{"id": "2505.14420", "pdf": "https://arxiv.org/pdf/2505.14420", "abs": "https://arxiv.org/abs/2505.14420", "authors": ["Huopu Zhang", "Yanguang Liu", "Mengnan Du"], "title": "SAE-FiRE: Enhancing Earnings Surprise Predictions Through Sparse Autoencoder Feature Selection", "categories": ["q-fin.CP", "cs.CL", "cs.LG"], "comment": null, "summary": "Predicting earnings surprises through the analysis of earnings conference\ncall transcripts has attracted increasing attention from the financial research\ncommunity. Conference calls serve as critical communication channels between\ncompany executives, analysts, and shareholders, offering valuable\nforward-looking information. However, these transcripts present significant\nanalytical challenges, typically containing over 5,000 words with substantial\nredundancy and industry-specific terminology that creates obstacles for\nlanguage models. In this work, we propose the Sparse Autoencoder for Financial\nRepresentation Enhancement (SAE-FiRE) framework to address these limitations by\nextracting key information while eliminating redundancy. SAE-FiRE employs\nSparse Autoencoders (SAEs) to efficiently identify patterns and filter out\nnoises, and focusing specifically on capturing nuanced financial signals that\nhave predictive power for earnings surprises. Experimental results indicate\nthat the proposed method can significantly outperform comparing baselines.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faSAE-FiRE\u6846\u67b6\uff0c\u901a\u8fc7\u7a00\u758f\u81ea\u7f16\u7801\u5668\u5206\u6790\u8d22\u62a5\u7535\u8bdd\u4f1a\u8bae\u8f6c\u5f55\uff0c\u63d0\u53d6\u5173\u952e\u4fe1\u606f\u5e76\u9884\u6d4b\u76c8\u5229\u610f\u5916\u3002", "motivation": "\u8d22\u62a5\u7535\u8bdd\u4f1a\u8bae\u662f\u516c\u53f8\u9ad8\u7ba1\u3001\u5206\u6790\u5e08\u548c\u80a1\u4e1c\u95f4\u7684\u91cd\u8981\u6c9f\u901a\u6e20\u9053\uff0c\u4f46\u5185\u5bb9\u5197\u957f\u4e14\u4e13\u4e1a\u672f\u8bed\u591a\uff0c\u7ed9\u8bed\u8a00\u6a21\u578b\u5206\u6790\u5e26\u6765\u6311\u6218\u3002", "method": "\u4f7f\u7528\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff08SAEs\uff09\u63d0\u53d6\u5173\u952e\u4fe1\u606f\u5e76\u8fc7\u6ee4\u566a\u97f3\uff0c\u4e13\u6ce8\u4e8e\u6355\u6349\u9884\u6d4b\u76c8\u5229\u610f\u5916\u7684\u91d1\u878d\u4fe1\u53f7\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSAE-FiRE\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "SAE-FiRE\u80fd\u6709\u6548\u89e3\u51b3\u8d22\u62a5\u7535\u8bdd\u4f1a\u8bae\u5206\u6790\u4e2d\u7684\u5197\u4f59\u548c\u4e13\u4e1a\u672f\u8bed\u95ee\u9898\uff0c\u63d0\u5347\u9884\u6d4b\u80fd\u529b\u3002"}}
{"id": "2505.14021", "pdf": "https://arxiv.org/pdf/2505.14021", "abs": "https://arxiv.org/abs/2505.14021", "authors": ["Soichiro Kumano", "Hiroshi Kera", "Toshihiko Yamasaki"], "title": "Adversarial Training from Mean Field Perspective", "categories": ["cs.LG", "cs.CV", "stat.ML"], "comment": "NeurIPS23", "summary": "Although adversarial training is known to be effective against adversarial\nexamples, training dynamics are not well understood. In this study, we present\nthe first theoretical analysis of adversarial training in random deep neural\nnetworks without any assumptions on data distributions. We introduce a new\ntheoretical framework based on mean field theory, which addresses the\nlimitations of existing mean field-based approaches. Based on this framework,\nwe derive (empirically tight) upper bounds of $\\ell_q$ norm-based adversarial\nloss with $\\ell_p$ norm-based adversarial examples for various values of $p$\nand $q$. Moreover, we prove that networks without shortcuts are generally not\nadversarially trainable and that adversarial training reduces network capacity.\nWe also show that network width alleviates these issues. Furthermore, we\npresent the various impacts of the input and output dimensions on the upper\nbounds and time evolution of the weight variance.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u5bf9\u968f\u673a\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u5bf9\u6297\u8bad\u7ec3\u8fdb\u884c\u4e86\u7406\u8bba\u5206\u6790\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u5e73\u5747\u573a\u7406\u8bba\u7684\u65b0\u6846\u67b6\uff0c\u5e76\u63a8\u5bfc\u4e86\u4e0d\u540c\u8303\u6570\u4e0b\u5bf9\u6297\u635f\u5931\u7684\u7d27\u4e0a\u754c\u3002", "motivation": "\u5c3d\u7ba1\u5bf9\u6297\u8bad\u7ec3\u5bf9\u5bf9\u6297\u6837\u672c\u6709\u6548\uff0c\u4f46\u5176\u8bad\u7ec3\u52a8\u6001\u5c1a\u672a\u88ab\u5145\u5206\u7406\u89e3\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7406\u8bba\u7a7a\u767d\u3002", "method": "\u5f15\u5165\u57fa\u4e8e\u5e73\u5747\u573a\u7406\u8bba\u7684\u65b0\u6846\u67b6\uff0c\u5206\u6790\u968f\u673a\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u5bf9\u6297\u8bad\u7ec3\uff0c\u63a8\u5bfc\u5bf9\u6297\u635f\u5931\u7684\u7d27\u4e0a\u754c\u3002", "result": "\u8bc1\u660e\u4e86\u65e0\u6377\u5f84\u7f51\u7edc\u901a\u5e38\u65e0\u6cd5\u5bf9\u6297\u8bad\u7ec3\uff0c\u4e14\u5bf9\u6297\u8bad\u7ec3\u4f1a\u964d\u4f4e\u7f51\u7edc\u5bb9\u91cf\uff1b\u7f51\u7edc\u5bbd\u5ea6\u53ef\u7f13\u89e3\u8fd9\u4e9b\u95ee\u9898\u3002", "conclusion": "\u672c\u6587\u7684\u7406\u8bba\u6846\u67b6\u4e3a\u5bf9\u6297\u8bad\u7ec3\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u89e3\uff0c\u63ed\u793a\u4e86\u7f51\u7edc\u7ed3\u6784\u548c\u7ef4\u5ea6\u5bf9\u5176\u6548\u679c\u7684\u5f71\u54cd\u3002"}}
{"id": "2505.14423", "pdf": "https://arxiv.org/pdf/2505.14423", "abs": "https://arxiv.org/abs/2505.14423", "authors": ["Ona de Gibert", "Joseph Attieh", "Teemu Vahtola", "Mikko Aulamo", "Zihao Li", "Ra\u00fal V\u00e1zquez", "Tiancheng Hu", "J\u00f6rg Tiedemann"], "title": "Scaling Low-Resource MT via Synthetic Data Generation with LLMs", "categories": ["cs.CL"], "comment": null, "summary": "We investigate the potential of LLM-generated synthetic data for improving\nlow-resource machine translation (MT). Focusing on seven diverse target\nlanguages, we construct a document-level synthetic corpus from English\nEuroparl, and extend it via pivoting to 147 additional language pairs.\nAutomatic and human evaluation confirm its high overall quality. We study its\npractical application by (i) identifying effective training regimes, (ii)\ncomparing our data with the HPLT dataset, and (iii) testing its utility beyond\nEnglish-centric MT. Finally, we introduce SynOPUS, a public repository for\nsynthetic parallel datasets. Our findings show that LLM-generated synthetic\ndata, even when noisy, can substantially improve MT performance for\nlow-resource languages.", "AI": {"tldr": "LLM\u751f\u6210\u7684\u5408\u6210\u6570\u636e\u80fd\u663e\u8457\u63d0\u5347\u4f4e\u8d44\u6e90\u673a\u5668\u7ffb\u8bd1\u6027\u80fd\uff0c\u901a\u8fc7\u6784\u5efa\u591a\u8bed\u8a00\u5408\u6210\u8bed\u6599\u5e93\u5e76\u9a8c\u8bc1\u5176\u8d28\u91cf\u3002", "motivation": "\u7814\u7a76\u4f4e\u8d44\u6e90\u673a\u5668\u7ffb\u8bd1\u4e2dLLM\u751f\u6210\u5408\u6210\u6570\u636e\u7684\u6f5c\u529b\uff0c\u4ee5\u89e3\u51b3\u6570\u636e\u4e0d\u8db3\u95ee\u9898\u3002", "method": "\u6784\u5efa\u6587\u6863\u7ea7\u5408\u6210\u8bed\u6599\u5e93\uff0c\u901a\u8fc7\u81ea\u52a8\u548c\u4eba\u5de5\u8bc4\u4f30\u9a8c\u8bc1\u8d28\u91cf\uff0c\u5e76\u6bd4\u8f83HPLT\u6570\u636e\u96c6\u3002", "result": "\u5408\u6210\u6570\u636e\u5373\u4f7f\u6709\u566a\u58f0\u4e5f\u80fd\u663e\u8457\u63d0\u5347\u7ffb\u8bd1\u6027\u80fd\uff0c\u5e76\u53d1\u5e03SynOPUS\u516c\u5171\u6570\u636e\u96c6\u3002", "conclusion": "LLM\u751f\u6210\u7684\u5408\u6210\u6570\u636e\u662f\u4f4e\u8d44\u6e90\u673a\u5668\u7ffb\u8bd1\u7684\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.14022", "pdf": "https://arxiv.org/pdf/2505.14022", "abs": "https://arxiv.org/abs/2505.14022", "authors": ["Chenghuan Huang", "Zhigeng Xu", "Chong Sun", "Chen Li", "Ziyang Ma"], "title": "Towards Efficient Multi-Scale Deformable Attention on NPU", "categories": ["cs.PF", "cs.CV"], "comment": "10 pages, 8 figures", "summary": "Multi-scale deformable attention (MSDA) is a flexible and powerful feature\nextraction mechanism for visual tasks, but its random-access grid sampling\nstrategy poses significant optimization challenges, especially on\ndomain-specific accelerators such as NPUs. In this work, we present a co-design\napproach that systematically rethinks memory access and computation strategies\nfor MSDA on the Ascend NPU architecture. With this co-design approach, our\nimplementation supports both efficient forward and backward computation, is\nfully adapted for training workloads, and incorporates a suite of\nhardware-aware optimizations. Extensive experiments show that our solution\nachieves up to $5.9\\times$ (forward), $8.9\\times$ (backward), and $7.3\\times$\n(end-to-end training) speedup over the grid sample-based baseline, and\n$1.9\\times$, $2.4\\times$, and $2.0\\times$ acceleration over the latest vendor\nlibrary, respectively.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9Ascend NPU\u67b6\u6784\u7684\u591a\u5c3a\u5ea6\u53ef\u53d8\u5f62\u6ce8\u610f\u529b\uff08MSDA\uff09\u7684\u534f\u540c\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "MSDA\u7684\u968f\u673a\u8bbf\u95ee\u7f51\u683c\u91c7\u6837\u7b56\u7565\u5728NPU\u7b49\u7279\u5b9a\u9886\u57df\u52a0\u901f\u5668\u4e0a\u5b58\u5728\u4f18\u5316\u6311\u6218\uff0c\u9700\u8981\u91cd\u65b0\u8bbe\u8ba1\u5185\u5b58\u8bbf\u95ee\u548c\u8ba1\u7b97\u7b56\u7565\u3002", "method": "\u91c7\u7528\u534f\u540c\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u4f18\u5316\u5185\u5b58\u8bbf\u95ee\u548c\u8ba1\u7b97\u7b56\u7565\uff0c\u652f\u6301\u9ad8\u6548\u7684\u524d\u5411\u548c\u53cd\u5411\u8ba1\u7b97\uff0c\u5e76\u5305\u542b\u786c\u4ef6\u611f\u77e5\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\uff0c\u524d\u5411\u8ba1\u7b97\u52a0\u901f5.9\u500d\uff0c\u53cd\u5411\u8ba1\u7b97\u52a0\u901f8.9\u500d\uff0c\u7aef\u5230\u7aef\u8bad\u7ec3\u52a0\u901f7.3\u500d\uff1b\u76f8\u6bd4\u6700\u65b0\u5382\u5546\u5e93\uff0c\u5206\u522b\u52a0\u901f1.9\u500d\u30012.4\u500d\u548c2.0\u500d\u3002", "conclusion": "\u534f\u540c\u8bbe\u8ba1\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86MSDA\u5728Ascend NPU\u4e0a\u7684\u8ba1\u7b97\u6548\u7387\uff0c\u9002\u7528\u4e8e\u8bad\u7ec3\u4efb\u52a1\u3002"}}
{"id": "2505.14425", "pdf": "https://arxiv.org/pdf/2505.14425", "abs": "https://arxiv.org/abs/2505.14425", "authors": ["Chalamalasetti Kranti", "Sherzod Hakimov", "David Schlangen"], "title": "From Templates to Natural Language: Generalization Challenges in Instruction-Tuned LLMs for Spatial Reasoning", "categories": ["cs.CL"], "comment": "4 pages", "summary": "Instruction-tuned large language models (LLMs) have shown strong performance\non a variety of tasks; however, generalizing from synthetic to human-authored\ninstructions in grounded environments remains a challenge for them. In this\nwork, we study generalization challenges in spatial grounding tasks where\nmodels interpret and translate instructions for building object arrangements on\na $2.5$D grid. We fine-tune LLMs using only synthetic instructions and evaluate\ntheir performance on a benchmark dataset containing both synthetic and\nhuman-written instructions. Our results reveal that while models generalize\nwell on simple tasks, their performance degrades significantly on more complex\ntasks. We present a detailed error analysis of the gaps in instruction\ngeneralization.", "AI": {"tldr": "\u6307\u4ee4\u8c03\u4f18\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7a7a\u95f4\u57fa\u7840\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u4ece\u5408\u6210\u6307\u4ee4\u6cdb\u5316\u5230\u4eba\u7c7b\u6307\u4ee4\u65f6\u5b58\u5728\u6311\u6218\u3002", "motivation": "\u7814\u7a76\u5728\u7a7a\u95f4\u57fa\u7840\u4efb\u52a1\u4e2d\uff0c\u6a21\u578b\u4ece\u5408\u6210\u6307\u4ee4\u6cdb\u5316\u5230\u4eba\u7c7b\u6307\u4ee4\u7684\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u5408\u6210\u6307\u4ee4\u5fae\u8c03\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5e76\u5728\u5305\u542b\u5408\u6210\u548c\u4eba\u7c7b\u6307\u4ee4\u7684\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u6027\u80fd\u3002", "result": "\u6a21\u578b\u5728\u7b80\u5355\u4efb\u52a1\u4e0a\u6cdb\u5316\u826f\u597d\uff0c\u4f46\u5728\u590d\u6742\u4efb\u52a1\u4e0a\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002", "conclusion": "\u901a\u8fc7\u9519\u8bef\u5206\u6790\u63ed\u793a\u4e86\u6307\u4ee4\u6cdb\u5316\u4e2d\u7684\u5dee\u8ddd\u3002"}}
{"id": "2505.14042", "pdf": "https://arxiv.org/pdf/2505.14042", "abs": "https://arxiv.org/abs/2505.14042", "authors": ["Soichiro Kumano", "Hiroshi Kera", "Toshihiko Yamasaki"], "title": "Adversarially Pretrained Transformers may be Universally Robust In-Context Learners", "categories": ["cs.LG", "cs.CV", "stat.ML"], "comment": null, "summary": "Adversarial training is one of the most effective adversarial defenses, but\nit incurs a high computational cost. In this study, we show that transformers\nadversarially pretrained on diverse tasks can serve as robust foundation models\nand eliminate the need for adversarial training in downstream tasks.\nSpecifically, we theoretically demonstrate that through in-context learning, a\nsingle adversarially pretrained transformer can robustly generalize to multiple\nunseen tasks without any additional training, i.e., without any parameter\nupdates. This robustness stems from the model's focus on robust features and\nits resistance to attacks that exploit non-predictive features. Besides these\npositive findings, we also identify several limitations. Under certain\nconditions (though unrealistic), no universally robust single-layer\ntransformers exist. Moreover, robust transformers exhibit an\naccuracy--robustness trade-off and require a large number of in-context\ndemonstrations. The code is available at\nhttps://github.com/s-kumano/universally-robust-in-context-learner.", "AI": {"tldr": "\u901a\u8fc7\u5bf9\u6297\u6027\u9884\u8bad\u7ec3\u7684Transformer\u6a21\u578b\u53ef\u4f5c\u4e3a\u9c81\u68d2\u57fa\u7840\u6a21\u578b\uff0c\u65e0\u9700\u4e0b\u6e38\u4efb\u52a1\u5bf9\u6297\u8bad\u7ec3\u5373\u53ef\u6cdb\u5316\u5230\u672a\u89c1\u4efb\u52a1\u3002", "motivation": "\u5bf9\u6297\u8bad\u7ec3\u867d\u6709\u6548\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5bf9\u6297\u6027\u9884\u8bad\u7ec3\u6a21\u578b\u662f\u5426\u80fd\u66ff\u4ee3\u4e0b\u6e38\u4efb\u52a1\u7684\u5bf9\u6297\u8bad\u7ec3\u3002", "method": "\u7406\u8bba\u8bc1\u660e\u901a\u8fc7\u4e0a\u4e0b\u6587\u5b66\u4e60\uff0c\u5355\u4e00\u5bf9\u6297\u6027\u9884\u8bad\u7ec3Transformer\u53ef\u9c81\u68d2\u6cdb\u5316\u5230\u591a\u4e2a\u672a\u89c1\u4efb\u52a1\uff0c\u65e0\u9700\u53c2\u6570\u66f4\u65b0\u3002", "result": "\u6a21\u578b\u80fd\u805a\u7126\u9c81\u68d2\u7279\u5f81\u5e76\u62b5\u6297\u653b\u51fb\uff0c\u4f46\u5b58\u5728\u5c40\u9650\u6027\uff1a\u67d0\u4e9b\u6761\u4ef6\u4e0b\u65e0\u666e\u904d\u9c81\u68d2\u5355\u5c42Transformer\uff0c\u4e14\u9700\u5927\u91cf\u4e0a\u4e0b\u6587\u6f14\u793a\u3002", "conclusion": "\u5bf9\u6297\u6027\u9884\u8bad\u7ec3Transformer\u5177\u6f5c\u529b\uff0c\u4f46\u9700\u6743\u8861\u51c6\u786e\u6027\u4e0e\u9c81\u68d2\u6027\uff0c\u5e76\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u5176\u5c40\u9650\u6027\u3002"}}
{"id": "2505.14436", "pdf": "https://arxiv.org/pdf/2505.14436", "abs": "https://arxiv.org/abs/2505.14436", "authors": ["Yuqiao Tan", "Shizhu He", "Kang Liu", "Jun Zhao"], "title": "Neural Incompatibility: The Unbridgeable Gap of Cross-Scale Parametric Knowledge Transfer in Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ACL'25 Main. Code link:\n  https://github.com/Trae1ounG/Neural_Incompatibility", "summary": "Large Language Models (LLMs) offer a transparent brain with accessible\nparameters that encode extensive knowledge, which can be analyzed, located and\ntransferred. Consequently, a key research challenge is to transcend traditional\nknowledge transfer paradigms rooted in symbolic language and achieve genuine\nParametric Knowledge Transfer (PKT). Significantly, exploring effective methods\nfor transferring knowledge across LLMs of different scales through parameters\npresents an intriguing and valuable research direction. In this paper, we first\ndemonstrate $\\textbf{Alignment}$ in parametric space is the fundamental\nprerequisite to achieve successful cross-scale PKT. We redefine the previously\nexplored knowledge transfer as Post-Align PKT (PostPKT), which utilizes\nextracted parameters for LoRA initialization and requires subsequent fine-tune\nfor alignment. Hence, to reduce cost for further fine-tuning, we introduce a\nnovel Pre-Align PKT (PrePKT) paradigm and propose a solution called\n$\\textbf{LaTen}$\n($\\textbf{L}$oc$\\textbf{a}$te-$\\textbf{T}$h$\\textbf{e}$n-Alig$\\textbf{n}$) that\naligns the parametric spaces of LLMs across scales only using several training\nsteps without following training. Comprehensive experiments on four benchmarks\ndemonstrate that both PostPKT and PrePKT face challenges in achieving\nconsistently stable transfer. Through in-depth analysis, we identify\n$\\textbf{Neural Incompatibility}$ as the ethological and parametric structural\ndifferences between LLMs of varying scales, presenting fundamental challenges\nto achieving effective PKT. These findings provide fresh insights into the\nparametric architectures of LLMs and highlight promising directions for future\nresearch on efficient PKT. Our code is available at\nhttps://github.com/Trae1ounG/Neural_Incompatibility.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u901a\u8fc7\u53c2\u6570\u5b9e\u73b0\u8de8\u89c4\u6a21\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u77e5\u8bc6\u8f6c\u79fb\uff08PKT\uff09\u7684\u6311\u6218\uff0c\u63d0\u51fa\u4e86Pre-Align PKT\uff08PrePKT\uff09\u548cPost-Align PKT\uff08PostPKT\uff09\u4e24\u79cd\u8303\u5f0f\uff0c\u5e76\u53d1\u73b0\u795e\u7ecf\u4e0d\u517c\u5bb9\u6027\u662f\u4e3b\u8981\u969c\u788d\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u7b26\u53f7\u8bed\u8a00\u7684\u77e5\u8bc6\u8f6c\u79fb\u8303\u5f0f\u96be\u4ee5\u5b9e\u73b0\u771f\u6b63\u7684\u53c2\u6570\u5316\u77e5\u8bc6\u8f6c\u79fb\uff08PKT\uff09\uff0c\u63a2\u7d22\u8de8\u89c4\u6a21LLMs\u53c2\u6570\u5316\u77e5\u8bc6\u8f6c\u79fb\u7684\u6709\u6548\u65b9\u6cd5\u662f\u5173\u952e\u7814\u7a76\u65b9\u5411\u3002", "method": "\u63d0\u51fa\u4e86PrePKT\u8303\u5f0f\u53caLaTen\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c11\u91cf\u8bad\u7ec3\u6b65\u9aa4\u5bf9\u9f50\u53c2\u6570\u7a7a\u95f4\uff0c\u907f\u514d\u540e\u7eed\u5fae\u8c03\uff1b\u540c\u65f6\u91cd\u65b0\u5b9a\u4e49\u4e86PostPKT\u3002", "result": "\u5b9e\u9a8c\u8868\u660ePostPKT\u548cPrePKT\u5747\u96be\u4ee5\u5b9e\u73b0\u7a33\u5b9a\u8f6c\u79fb\uff0c\u795e\u7ecf\u4e0d\u517c\u5bb9\u6027\u662f\u6839\u672c\u6311\u6218\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86LLMs\u53c2\u6570\u67b6\u6784\u7684\u65b0\u89c1\u89e3\uff0c\u4e3a\u672a\u6765\u9ad8\u6548PKT\u7814\u7a76\u6307\u660e\u4e86\u65b9\u5411\u3002"}}
{"id": "2505.14064", "pdf": "https://arxiv.org/pdf/2505.14064", "abs": "https://arxiv.org/abs/2505.14064", "authors": ["Cosmin I. Bercea", "Jun Li", "Philipp Raffler", "Evamaria O. Riedel", "Lena Schmitzer", "Angela Kurz", "Felix Bitzer", "Paula Ro\u00dfm\u00fcller", "Julian Canisius", "Mirjam L. Beyrle", "Che Liu", "Wenjia Bai", "Bernhard Kainz", "Julia A. Schnabel", "Benedikt Wiestler"], "title": "NOVA: A Benchmark for Anomaly Localization and Clinical Reasoning in Brain MRI", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "In many real-world applications, deployed models encounter inputs that differ\nfrom the data seen during training. Out-of-distribution detection identifies\nwhether an input stems from an unseen distribution, while open-world\nrecognition flags such inputs to ensure the system remains robust as\never-emerging, previously $unknown$ categories appear and must be addressed\nwithout retraining. Foundation and vision-language models are pre-trained on\nlarge and diverse datasets with the expectation of broad generalization across\ndomains, including medical imaging. However, benchmarking these models on test\nsets with only a few common outlier types silently collapses the evaluation\nback to a closed-set problem, masking failures on rare or truly novel\nconditions encountered in clinical use.\n  We therefore present $NOVA$, a challenging, real-life $evaluation-only$\nbenchmark of $\\sim$900 brain MRI scans that span 281 rare pathologies and\nheterogeneous acquisition protocols. Each case includes rich clinical\nnarratives and double-blinded expert bounding-box annotations. Together, these\nenable joint assessment of anomaly localisation, visual captioning, and\ndiagnostic reasoning. Because NOVA is never used for training, it serves as an\n$extreme$ stress-test of out-of-distribution generalisation: models must bridge\na distribution gap both in sample appearance and in semantic space. Baseline\nresults with leading vision-language models (GPT-4o, Gemini 2.0 Flash, and\nQwen2.5-VL-72B) reveal substantial performance drops across all tasks,\nestablishing NOVA as a rigorous testbed for advancing models that can detect,\nlocalize, and reason about truly unknown anomalies.", "AI": {"tldr": "NOVA\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u6a21\u578b\u5728\u771f\u5b9e\u4e16\u754c\u4e2d\u5904\u7406\u7f55\u89c1\u75c5\u7406\u548c\u5f02\u6784\u6570\u636e\u7684\u6781\u7aef\u538b\u529b\u6d4b\u8bd5\u57fa\u51c6\uff0c\u5305\u542b900\u4e2a\u8111MRI\u626b\u63cf\u548c281\u79cd\u7f55\u89c1\u75c5\u7406\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5728\u8bc4\u4f30\u6a21\u578b\u65f6\u5f80\u5f80\u5ffd\u7565\u7f55\u89c1\u6216\u65b0\u9896\u7684\u5f02\u5e38\u60c5\u51b5\uff0c\u5bfc\u81f4\u6a21\u578b\u5728\u4e34\u5e8a\u5e94\u7528\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002", "method": "NOVA\u63d0\u4f9b900\u4e2a\u8111MRI\u626b\u63cf\uff0c\u6db5\u76d6281\u79cd\u7f55\u89c1\u75c5\u7406\u548c\u5f02\u6784\u91c7\u96c6\u534f\u8bae\uff0c\u5e76\u5305\u542b\u4e34\u5e8a\u53d9\u8ff0\u548c\u4e13\u5bb6\u6807\u6ce8\uff0c\u7528\u4e8e\u8bc4\u4f30\u5f02\u5e38\u5b9a\u4f4d\u3001\u89c6\u89c9\u63cf\u8ff0\u548c\u8bca\u65ad\u63a8\u7406\u3002", "result": "\u9886\u5148\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728NOVA\u57fa\u51c6\u4e0a\u8868\u73b0\u663e\u8457\u4e0b\u964d\uff0c\u8868\u660e\u5176\u5728\u5904\u7406\u672a\u77e5\u5f02\u5e38\u65f6\u7684\u5c40\u9650\u6027\u3002", "conclusion": "NOVA\u4e3a\u6a21\u578b\u5728\u771f\u5b9e\u4e16\u754c\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u63d0\u4f9b\u4e86\u4e25\u683c\u7684\u6d4b\u8bd5\u5e73\u53f0\uff0c\u63a8\u52a8\u4e86\u6a21\u578b\u5728\u68c0\u6d4b\u3001\u5b9a\u4f4d\u548c\u63a8\u7406\u672a\u77e5\u5f02\u5e38\u65b9\u9762\u7684\u8fdb\u6b65\u3002"}}
{"id": "2505.14442", "pdf": "https://arxiv.org/pdf/2505.14442", "abs": "https://arxiv.org/abs/2505.14442", "authors": ["Mete Ismayilzada", "Antonio Laverghetta Jr.", "Simone A. Luchini", "Reet Patel", "Antoine Bosselut", "Lonneke van der Plas", "Roger Beaty"], "title": "Creative Preference Optimization", "categories": ["cs.CL", "cs.AI"], "comment": "27 pages", "summary": "While Large Language Models (LLMs) have demonstrated impressive performance\nacross natural language generation tasks, their ability to generate truly\ncreative content-characterized by novelty, diversity, surprise, and\nquality-remains limited. Existing methods for enhancing LLM creativity often\nfocus narrowly on diversity or specific tasks, failing to address creativity's\nmultifaceted nature in a generalizable way. In this work, we propose Creative\nPreference Optimization (CrPO), a novel alignment method that injects signals\nfrom multiple creativity dimensions into the preference optimization objective\nin a modular fashion. We train and evaluate creativity-augmented versions of\nseveral models using CrPO and MuCE, a new large-scale human preference dataset\nspanning over 200,000 human-generated responses and ratings from more than 30\npsychological creativity assessments. Our models outperform strong baselines,\nincluding GPT-4o, on both automated and human evaluations, producing more\nnovel, diverse, and surprising generations while maintaining high output\nquality. Additional evaluations on NoveltyBench further confirm the\ngeneralizability of our approach. Together, our results demonstrate that\ndirectly optimizing for creativity within preference frameworks is a promising\ndirection for advancing the creative capabilities of LLMs without compromising\noutput quality.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCreative Preference Optimization (CrPO)\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u7ef4\u5ea6\u521b\u9020\u529b\u4fe1\u53f7\u4f18\u5316LLM\u7684\u751f\u6210\u5185\u5bb9\uff0c\u5728\u4fdd\u6301\u9ad8\u8d28\u91cf\u7684\u540c\u65f6\u63d0\u5347\u65b0\u9896\u6027\u3001\u591a\u6837\u6027\u548c\u60ca\u559c\u611f\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u63d0\u5347LLM\u521b\u9020\u529b\u65f6\u5f80\u5f80\u5c40\u9650\u4e8e\u5355\u4e00\u7ef4\u5ea6\u6216\u4efb\u52a1\uff0c\u672a\u80fd\u5168\u9762\u89e3\u51b3\u521b\u9020\u529b\u7684\u591a\u9762\u6027\u95ee\u9898\u3002", "method": "\u63d0\u51faCrPO\u65b9\u6cd5\uff0c\u5c06\u591a\u7ef4\u5ea6\u521b\u9020\u529b\u4fe1\u53f7\u6a21\u5757\u5316\u5730\u6ce8\u5165\u504f\u597d\u4f18\u5316\u76ee\u6807\uff0c\u5e76\u4f7f\u7528\u65b0\u7684\u5927\u89c4\u6a21\u4eba\u7c7b\u504f\u597d\u6570\u636e\u96c6MuCE\u8fdb\u884c\u8bad\u7ec3\u548c\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4f18\u5316\u540e\u7684\u6a21\u578b\u5728\u81ea\u52a8\u548c\u4eba\u5de5\u8bc4\u4f30\u4e2d\u5747\u4f18\u4e8eGPT-4o\u7b49\u57fa\u7ebf\uff0c\u751f\u6210\u5185\u5bb9\u66f4\u5177\u65b0\u9896\u6027\u3001\u591a\u6837\u6027\u548c\u60ca\u559c\u611f\uff0c\u4e14\u8d28\u91cf\u4e0d\u964d\u3002", "conclusion": "\u76f4\u63a5\u5728\u504f\u597d\u6846\u67b6\u4e2d\u4f18\u5316\u521b\u9020\u529b\u662f\u63d0\u5347LLM\u521b\u610f\u80fd\u529b\u7684\u6709\u6548\u65b9\u5411\uff0c\u4e14\u4e0d\u5f71\u54cd\u8f93\u51fa\u8d28\u91cf\u3002"}}
{"id": "2505.14071", "pdf": "https://arxiv.org/pdf/2505.14071", "abs": "https://arxiv.org/abs/2505.14071", "authors": ["Woody Haosheng Gan", "Deqing Fu", "Julian Asilis", "Ollie Liu", "Dani Yogatama", "Vatsal Sharan", "Robin Jia", "Willie Neiswanger"], "title": "Textual Steering Vectors Can Improve Visual Understanding in Multimodal Large Language Models", "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": null, "summary": "Steering methods have emerged as effective and targeted tools for guiding\nlarge language models' (LLMs) behavior without modifying their parameters.\nMultimodal large language models (MLLMs), however, do not currently enjoy the\nsame suite of techniques, due in part to their recency and architectural\ndiversity. Inspired by this gap, we investigate whether MLLMs can be steered\nusing vectors derived from their text-only LLM backbone, via sparse\nautoencoders (SAEs), mean shift, and linear probing. We find that text-derived\nsteering consistently enhances multimodal accuracy across diverse MLLM\narchitectures and visual tasks. In particular, mean shift boosts spatial\nrelationship accuracy on CV-Bench by up to +7.3% and counting accuracy by up to\n+3.3%, outperforming prompting and exhibiting strong generalization to\nout-of-distribution datasets. These results highlight textual steering vectors\nas a powerful, efficient mechanism for enhancing grounding in MLLMs with\nminimal additional data collection and computational overhead.", "AI": {"tldr": "\u6587\u672c\u5f15\u5bfc\u65b9\u6cd5\u901a\u8fc7\u7a00\u758f\u81ea\u7f16\u7801\u5668\u3001\u5747\u503c\u6f02\u79fb\u548c\u7ebf\u6027\u63a2\u6d4b\uff0c\u6210\u529f\u63d0\u5347\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u7684\u51c6\u786e\u6027\uff0c\u5c24\u5176\u5728\u7a7a\u95f4\u5173\u7cfb\u548c\u8ba1\u6570\u4efb\u52a1\u4e0a\u8868\u73b0\u7a81\u51fa\u3002", "motivation": "\u76ee\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7f3a\u4e4f\u6709\u6548\u7684\u5f15\u5bfc\u65b9\u6cd5\uff0c\u7814\u7a76\u63a2\u7d22\u4e86\u662f\u5426\u53ef\u4ee5\u901a\u8fc7\u6587\u672c\u5f15\u5bfc\u5411\u91cf\u63d0\u5347\u5176\u6027\u80fd\u3002", "method": "\u4f7f\u7528\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff08SAEs\uff09\u3001\u5747\u503c\u6f02\u79fb\u548c\u7ebf\u6027\u63a2\u6d4b\u6280\u672f\uff0c\u4ece\u6587\u672cLLM\u9aa8\u5e72\u4e2d\u63d0\u53d6\u5f15\u5bfc\u5411\u91cf\u3002", "result": "\u6587\u672c\u5f15\u5bfc\u663e\u8457\u63d0\u5347\u4e86MLLMs\u7684\u51c6\u786e\u6027\uff0c\u5747\u503c\u6f02\u79fb\u5728CV-Bench\u4e0a\u7a7a\u95f4\u5173\u7cfb\u4efb\u52a1\u63d0\u53477.3%\uff0c\u8ba1\u6570\u4efb\u52a1\u63d0\u53473.3%\u3002", "conclusion": "\u6587\u672c\u5f15\u5bfc\u5411\u91cf\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u65e0\u9700\u989d\u5916\u6570\u636e\u7684\u65b9\u6cd5\uff0c\u53ef\u663e\u8457\u589e\u5f3aMLLMs\u7684\u63a5\u5730\u80fd\u529b\u3002"}}
{"id": "2505.14455", "pdf": "https://arxiv.org/pdf/2505.14455", "abs": "https://arxiv.org/abs/2505.14455", "authors": ["Chihan Huang", "Hao Tang"], "title": "CtrlDiff: Boosting Large Diffusion Language Models with Dynamic Block Prediction and Controllable Generation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Although autoregressive models have dominated language modeling in recent\nyears, there has been a growing interest in exploring alternative paradigms to\nthe conventional next-token prediction framework. Diffusion-based language\nmodels have emerged as a compelling alternative due to their powerful parallel\ngeneration capabilities and inherent editability. However, these models are\noften constrained by fixed-length generation. A promising direction is to\ncombine the strengths of both paradigms, segmenting sequences into blocks,\nmodeling autoregressive dependencies across blocks while leveraging discrete\ndiffusion to estimate the conditional distribution within each block given the\npreceding context. Nevertheless, their practical application is often hindered\nby two key limitations: rigid fixed-length outputs and a lack of flexible\ncontrol mechanisms. In this work, we address the critical limitations of fixed\ngranularity and weak controllability in current large diffusion language\nmodels. We propose CtrlDiff, a dynamic and controllable semi-autoregressive\nframework that adaptively determines the size of each generation block based on\nlocal semantics using reinforcement learning. Furthermore, we introduce a\nclassifier-guided control mechanism tailored to discrete diffusion, which\nsignificantly reduces computational overhead while facilitating efficient\npost-hoc conditioning without retraining. Extensive experiments demonstrate\nthat CtrlDiff sets a new standard among hybrid diffusion models, narrows the\nperformance gap to state-of-the-art autoregressive approaches, and enables\neffective conditional text generation across diverse tasks.", "AI": {"tldr": "CtrlDiff\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u53ef\u63a7\u7684\u534a\u81ea\u56de\u5f52\u6846\u67b6\uff0c\u7ed3\u5408\u79bb\u6563\u6269\u6563\u6a21\u578b\u548c\u81ea\u56de\u5f52\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u56fa\u5b9a\u957f\u5ea6\u751f\u6210\u548c\u5f31\u53ef\u63a7\u6027\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u6269\u6563\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u56fa\u5b9a\u957f\u5ea6\u751f\u6210\u548c\u7f3a\u4e4f\u7075\u6d3b\u63a7\u5236\u673a\u5236\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u7ed3\u5408\u81ea\u56de\u5f52\u548c\u79bb\u6563\u6269\u6563\u6a21\u578b\uff0c\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u52a8\u6001\u786e\u5b9a\u751f\u6210\u5757\u5927\u5c0f\uff0c\u5e76\u5f15\u5165\u5206\u7c7b\u5668\u5f15\u5bfc\u7684\u63a7\u5236\u673a\u5236\u3002", "result": "CtrlDiff\u5728\u6df7\u5408\u6269\u6563\u6a21\u578b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u7f29\u5c0f\u4e86\u4e0e\u81ea\u56de\u5f52\u6a21\u578b\u7684\u6027\u80fd\u5dee\u8ddd\uff0c\u652f\u6301\u591a\u6837\u5316\u6761\u4ef6\u6587\u672c\u751f\u6210\u3002", "conclusion": "CtrlDiff\u4e3a\u6269\u6563\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u52a8\u6001\u53ef\u63a7\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u63d0\u5347\u4e86\u751f\u6210\u7075\u6d3b\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2505.14464", "pdf": "https://arxiv.org/pdf/2505.14464", "abs": "https://arxiv.org/abs/2505.14464", "authors": ["Xiaoyu Tian", "Yunjie Ji", "Haotian Wang", "Shuaiting Chen", "Sitong Zhao", "Yiping Peng", "Han Zhao", "Xiangang Li"], "title": "Not All Correct Answers Are Equal: Why Your Distillation Source Matters", "categories": ["cs.CL"], "comment": null, "summary": "Distillation has emerged as a practical and effective approach to enhance the\nreasoning capabilities of open-source language models. In this work, we conduct\na large-scale empirical study on reasoning data distillation by collecting\nverified outputs from three state-of-the-art teacher models-AM-Thinking-v1,\nQwen3-235B-A22B, and DeepSeek-R1-on a shared corpus of 1.89 million queries. We\nconstruct three parallel datasets and analyze their distributions, revealing\nthat AM-Thinking-v1-distilled data exhibits greater token length diversity and\nlower perplexity. Student models trained on each dataset are evaluated on\nreasoning benchmarks including AIME2024, AIME2025, MATH500, and LiveCodeBench.\nThe AM-based model consistently achieves the best performance (e.g., 84.3 on\nAIME2024, 72.2 on AIME2025, 98.4 on MATH500, and 65.9 on LiveCodeBench) and\ndemonstrates adaptive output behavior-producing longer responses for harder\ntasks and shorter ones for simpler tasks. These findings highlight the value of\nhigh-quality, verified reasoning traces. We release the AM-Thinking-v1 and\nQwen3-235B-A22B distilled datasets to support future research on open and\nhigh-performing reasoning-oriented language models. The datasets are publicly\navailable on Hugging Face\\footnote{Datasets are available on Hugging Face:\n\\href{https://huggingface.co/datasets/a-m-team/AM-Thinking-v1-Distilled}{AM-Thinking-v1-Distilled},\n\\href{https://huggingface.co/datasets/a-m-team/AM-Qwen3-Distilled}{AM-Qwen3-Distilled}.}.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5927\u89c4\u6a21\u5b9e\u8bc1\u7814\u7a76\uff0c\u5206\u6790\u4e86\u4e09\u79cd\u5148\u8fdb\u6559\u5e08\u6a21\u578b\uff08AM-Thinking-v1\u3001Qwen3-235B-A22B\u548cDeepSeek-R1\uff09\u5728189\u4e07\u67e5\u8be2\u4e0a\u7684\u63a8\u7406\u6570\u636e\u84b8\u998f\u6548\u679c\uff0c\u53d1\u73b0AM-Thinking-v1\u84b8\u998f\u7684\u6570\u636e\u5177\u6709\u66f4\u9ad8\u7684\u591a\u6837\u6027\uff0c\u4e14\u5b66\u751f\u6a21\u578b\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u63d0\u5347\u5f00\u6e90\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u63a2\u7d22\u9ad8\u8d28\u91cf\u63a8\u7406\u6570\u636e\u84b8\u998f\u7684\u4ef7\u503c\u3002", "method": "\u6536\u96c6\u4e09\u79cd\u6559\u5e08\u6a21\u578b\u7684\u9a8c\u8bc1\u8f93\u51fa\uff0c\u6784\u5efa\u5e76\u884c\u6570\u636e\u96c6\uff0c\u8bad\u7ec3\u5b66\u751f\u6a21\u578b\u5e76\u8bc4\u4f30\u5176\u5728\u591a\u4e2a\u63a8\u7406\u57fa\u51c6\u4e0a\u7684\u8868\u73b0\u3002", "result": "AM-Thinking-v1\u84b8\u998f\u7684\u6a21\u578b\u5728AIME2024\u3001AIME2025\u3001MATH500\u548cLiveCodeBench\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u4e14\u80fd\u81ea\u9002\u5e94\u4efb\u52a1\u96be\u5ea6\u8c03\u6574\u8f93\u51fa\u957f\u5ea6\u3002", "conclusion": "\u9ad8\u8d28\u91cf\u9a8c\u8bc1\u63a8\u7406\u6570\u636e\u5bf9\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\u81f3\u5173\u91cd\u8981\uff0c\u76f8\u5173\u6570\u636e\u96c6\u5df2\u516c\u5f00\u4ee5\u652f\u6301\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2505.14177", "pdf": "https://arxiv.org/pdf/2505.14177", "abs": "https://arxiv.org/abs/2505.14177", "authors": ["Marien Renaud", "Valentin De Bortoli", "Arthur Leclaire", "Nicolas Papadakis"], "title": "From stability of Langevin diffusion to convergence of proximal MCMC for non-log-concave sampling", "categories": ["stat.ML", "cs.CV", "cs.LG"], "comment": null, "summary": "We consider the problem of sampling distributions stemming from non-convex\npotentials with Unadjusted Langevin Algorithm (ULA). We prove the stability of\nthe discrete-time ULA to drift approximations under the assumption that the\npotential is strongly convex at infinity. In many context, e.g. imaging inverse\nproblems, potentials are non-convex and non-smooth. Proximal Stochastic\nGradient Langevin Algorithm (PSGLA) is a popular algorithm to handle such\npotentials. It combines the forward-backward optimization algorithm with a ULA\nstep. Our main stability result combined with properties of the Moreau envelope\nallows us to derive the first proof of convergence of the PSGLA for non-convex\npotentials. We empirically validate our methodology on synthetic data and in\nthe context of imaging inverse problems. In particular, we observe that PSGLA\nexhibits faster convergence rates than Stochastic Gradient Langevin Algorithm\nfor posterior sampling while preserving its restoration properties.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u975e\u51f8\u52bf\u80fd\u5206\u5e03\u91c7\u6837\u95ee\u9898\uff0c\u8bc1\u660e\u4e86\u79bb\u6563\u65f6\u95f4ULA\u5728\u52bf\u80fd\u5f3a\u51f8\u5047\u8bbe\u4e0b\u7684\u7a33\u5b9a\u6027\uff0c\u5e76\u9996\u6b21\u4e3aPSGLA\u5728\u975e\u51f8\u52bf\u80fd\u4e0b\u7684\u6536\u655b\u6027\u63d0\u4f9b\u4e86\u8bc1\u660e\u3002", "motivation": "\u89e3\u51b3\u975e\u51f8\u548c\u975e\u5149\u6ed1\u52bf\u80fd\u4e0b\u7684\u91c7\u6837\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u6210\u50cf\u9006\u95ee\u9898\u4e2d\u3002", "method": "\u7ed3\u5408\u524d\u5411\u540e\u5411\u4f18\u5316\u7b97\u6cd5\u4e0eULA\u6b65\u9aa4\u7684PSGLA\u65b9\u6cd5\u3002", "result": "PSGLA\u5728\u5408\u6210\u6570\u636e\u548c\u6210\u50cf\u9006\u95ee\u9898\u4e2d\u8868\u73b0\u51fa\u6bd4SGLA\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u6062\u590d\u7279\u6027\u3002", "conclusion": "PSGLA\u5728\u975e\u51f8\u52bf\u80fd\u4e0b\u5177\u6709\u7a33\u5b9a\u6027\u548c\u6536\u655b\u6027\uff0c\u9002\u7528\u4e8e\u590d\u6742\u91c7\u6837\u95ee\u9898\u3002"}}
{"id": "2505.14467", "pdf": "https://arxiv.org/pdf/2505.14467", "abs": "https://arxiv.org/abs/2505.14467", "authors": ["Mani Shemiranifar"], "title": "Void in Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Despite advances in transformer-based language models (LMs), a fundamental\nquestion remains largely unanswered: Are all layers activated during inference?\nWe investigate this question by detecting unactivated layers (which we refer to\nas Voids) using a non-trainable and parameter-free adaptive computation method\ncalled L2 Adaptive Computation (LAC). We adapt LAC from its original\nefficiency-focused application to trace activated layers during inference. This\nmethod monitors changes in the L2-norm of activations to identify voids. We\nanalyze layer activation in instruction-tuned LMs across two phases: Prompt\nProcessing (PP), where we trace activated layers for each token in the input\nprompts, and Response Generation (RG), where we trace activated layers for each\ngenerated token. We further demonstrate that distinct layers are activated\nduring these two phases. To show the effectiveness of our method, we evaluated\nthree distinct instruction-tuned LMs from the Llama, Mistral, and Qwen families\non three benchmarks: MMLU, GPQA Diamond, and BoolQ. For example, on MMLU with a\nzero-shot setting, skipping voids in Qwen2.5-7B-Instruct resulted in an\nimprovement from 69.24 to 71.29 while the model uses only 30% of the layers.\nSimilarly, Mistral-7B-Instruct-v0.3 on GPQA Diamond improved from 13.88 to\n18.36 when using 70% of the layers during both the PP and RG phases. These\nresults show that not all layers contribute equally during inference, and that\nselectively skipping most of them can improve the performance of models on\ncertain tasks.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5e76\u975e\u6240\u6709Transformer\u8bed\u8a00\u6a21\u578b\u7684\u5c42\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u90fd\u88ab\u6fc0\u6d3b\uff0c\u901a\u8fc7L2\u81ea\u9002\u5e94\u8ba1\u7b97\uff08LAC\uff09\u65b9\u6cd5\u53ef\u4ee5\u8bc6\u522b\u672a\u6fc0\u6d3b\u5c42\uff08Voids\uff09\uff0c\u5e76\u9009\u62e9\u6027\u8df3\u8fc7\u8fd9\u4e9b\u5c42\u4ee5\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u63a2\u8ba8Transformer\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u662f\u5426\u6240\u6709\u5c42\u5747\u88ab\u6fc0\u6d3b\uff0c\u4ee5\u53ca\u5982\u4f55\u901a\u8fc7\u8bc6\u522b\u672a\u6fc0\u6d3b\u5c42\u4f18\u5316\u6a21\u578b\u6548\u7387\u4e0e\u6027\u80fd\u3002", "method": "\u4f7f\u7528L2\u81ea\u9002\u5e94\u8ba1\u7b97\uff08LAC\uff09\u65b9\u6cd5\u76d1\u6d4b\u6fc0\u6d3b\u7684L2\u8303\u6570\u53d8\u5316\uff0c\u8bc6\u522b\u672a\u6fc0\u6d3b\u5c42\uff0c\u5e76\u5206\u6790\u6307\u4ee4\u8c03\u4f18\u6a21\u578b\u5728\u63d0\u793a\u5904\u7406\uff08PP\uff09\u548c\u54cd\u5e94\u751f\u6210\uff08RG\uff09\u9636\u6bb5\u7684\u5c42\u6fc0\u6d3b\u60c5\u51b5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8df3\u8fc7\u672a\u6fc0\u6d3b\u5c42\u53ef\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u4f8b\u5982Qwen2.5-7B-Instruct\u5728MMLU\u4efb\u52a1\u4e2d\u6027\u80fd\u63d0\u53472.05%\uff0c\u540c\u65f6\u4ec5\u4f7f\u752830%\u7684\u5c42\u3002", "conclusion": "\u7814\u7a76\u8bc1\u5b9e\uff0c\u9009\u62e9\u6027\u8df3\u8fc7\u672a\u6fc0\u6d3b\u5c42\u80fd\u6709\u6548\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u4e3a\u4f18\u5316\u63a8\u7406\u6548\u7387\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.14180", "pdf": "https://arxiv.org/pdf/2505.14180", "abs": "https://arxiv.org/abs/2505.14180", "authors": ["Songhao Wu", "Quan Tu", "Mingjie Zhong", "Hong Liu", "Jia Xu", "Jinjie Gu", "Rui Yan"], "title": "Bridge the Gap between Past and Future: Siamese Model Optimization for Context-Aware Document Ranking", "categories": ["cs.IR", "cs.CV", "H.3.3"], "comment": null, "summary": "In the realm of information retrieval, users often engage in multi-turn\ninteractions with search engines to acquire information, leading to the\nformation of sequences of user feedback behaviors. Leveraging the session\ncontext has proven to be beneficial for inferring user search intent and\ndocument ranking. A multitude of approaches have been proposed to exploit\nin-session context for improved document ranking. Despite these advances, the\nlimitation of historical session data for capturing evolving user intent\nremains a challenge. In this work, we explore the integration of future\ncontextual information into the session context to enhance document ranking. We\npresent the siamese model optimization framework, comprising a\nhistory-conditioned model and a future-aware model. The former processes only\nthe historical behavior sequence, while the latter integrates both historical\nand anticipated future behaviors. Both models are trained collaboratively using\nthe supervised labels and pseudo labels predicted by the other. The\nhistory-conditioned model, referred to as ForeRanker, progressively learns\nfuture-relevant information to enhance ranking, while it singly uses historical\nsession at inference time. To mitigate inconsistencies during training, we\nintroduce the peer knowledge distillation method with a dynamic gating\nmechanism, allowing models to selectively incorporate contextual information.\nExperimental results on benchmark datasets demonstrate the effectiveness of our\nForeRanker, showcasing its superior performance compared to existing methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5386\u53f2\u4e0e\u672a\u6765\u4e0a\u4e0b\u6587\u4fe1\u606f\u7684\u6587\u6863\u6392\u5e8f\u6a21\u578bForeRanker\uff0c\u901a\u8fc7\u53cc\u6a21\u578b\u534f\u4f5c\u8bad\u7ec3\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4ec5\u5229\u7528\u5386\u53f2\u4f1a\u8bdd\u6570\u636e\uff0c\u96be\u4ee5\u6355\u6349\u7528\u6237\u610f\u56fe\u7684\u52a8\u6001\u53d8\u5316\uff0c\u56e0\u6b64\u63a2\u7d22\u7ed3\u5408\u672a\u6765\u4e0a\u4e0b\u6587\u4fe1\u606f\u4ee5\u6539\u8fdb\u6587\u6863\u6392\u5e8f\u3002", "method": "\u8bbe\u8ba1\u4e86\u53cc\u6a21\u578b\u4f18\u5316\u6846\u67b6\uff0c\u5305\u62ec\u5386\u53f2\u6761\u4ef6\u6a21\u578b\u548c\u672a\u6765\u611f\u77e5\u6a21\u578b\uff0c\u901a\u8fc7\u76d1\u7763\u6807\u7b7e\u548c\u4f2a\u6807\u7b7e\u534f\u4f5c\u8bad\u7ec3\uff0c\u5e76\u5f15\u5165\u52a8\u6001\u95e8\u63a7\u673a\u5236\u51cf\u5c11\u8bad\u7ec3\u4e0d\u4e00\u81f4\u6027\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cForeRanker\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u7ed3\u5408\u672a\u6765\u4e0a\u4e0b\u6587\u4fe1\u606f\u80fd\u6709\u6548\u63d0\u5347\u6587\u6863\u6392\u5e8f\u6027\u80fd\uff0c\u52a8\u6001\u95e8\u63a7\u673a\u5236\u6709\u52a9\u4e8e\u6a21\u578b\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002"}}
{"id": "2505.14469", "pdf": "https://arxiv.org/pdf/2505.14469", "abs": "https://arxiv.org/abs/2505.14469", "authors": ["Somnath Banerjee", "Pratyush Chatterjee", "Shanu Kumar", "Sayan Layek", "Parag Agrawal", "Rima Hazra", "Animesh Mukherjee"], "title": "Attributional Safety Failures in Large Language Models under Code-Mixed Perturbations", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advancements in LLMs have raised significant safety concerns,\nparticularly when dealing with code-mixed inputs and outputs. Our study\nsystematically investigates the increased susceptibility of LLMs to produce\nunsafe outputs from code-mixed prompts compared to monolingual English prompts.\nUtilizing explainability methods, we dissect the internal attribution shifts\ncausing model's harmful behaviors. In addition, we explore cultural dimensions\nby distinguishing between universally unsafe and culturally-specific unsafe\nqueries. This paper presents novel experimental insights, clarifying the\nmechanisms driving this phenomenon.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0cLLMs\u5728\u5904\u7406\u4ee3\u7801\u6df7\u5408\u8f93\u5165\u65f6\u66f4\u5bb9\u6613\u4ea7\u751f\u4e0d\u5b89\u5168\u8f93\u51fa\uff0c\u76f8\u6bd4\u5355\u8bed\u82f1\u8bed\u8f93\u5165\u3002\u901a\u8fc7\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u5185\u90e8\u5f52\u56e0\u53d8\u5316\uff0c\u5e76\u533a\u5206\u4e86\u666e\u904d\u4e0d\u5b89\u5168\u4e0e\u6587\u5316\u7279\u5b9a\u4e0d\u5b89\u5168\u67e5\u8be2\u3002", "motivation": "\u63a2\u8ba8LLMs\u5728\u4ee3\u7801\u6df7\u5408\u8f93\u5165\u4e0b\u7684\u5b89\u5168\u6027\u95ee\u9898\uff0c\u63ed\u793a\u5176\u5185\u90e8\u673a\u5236\u3002", "method": "\u4f7f\u7528\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\u5206\u6790\u6a21\u578b\u5f52\u56e0\u53d8\u5316\uff0c\u533a\u5206\u666e\u904d\u4e0e\u6587\u5316\u7279\u5b9a\u4e0d\u5b89\u5168\u67e5\u8be2\u3002", "result": "\u4ee3\u7801\u6df7\u5408\u8f93\u5165\u663e\u8457\u589e\u52a0LLMs\u7684\u4e0d\u5b89\u5168\u8f93\u51fa\u503e\u5411\uff0c\u63ed\u793a\u4e86\u5185\u90e8\u673a\u5236\u3002", "conclusion": "\u7814\u7a76\u4e3aLLMs\u5b89\u5168\u6027\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\uff0c\u5f3a\u8c03\u4e86\u4ee3\u7801\u6df7\u5408\u8f93\u5165\u7684\u98ce\u9669\u3002"}}
{"id": "2505.14336", "pdf": "https://arxiv.org/pdf/2505.14336", "abs": "https://arxiv.org/abs/2505.14336", "authors": ["Umberto Cappellazzo", "Minsu Kim", "Stavros Petridis", "Daniele Falavigna", "Alessio Brutti"], "title": "Scaling and Enhancing LLM-based AVSR: A Sparse Mixture of Projectors Approach", "categories": ["eess.AS", "cs.CV", "cs.MM", "cs.SD"], "comment": null, "summary": "Audio-Visual Speech Recognition (AVSR) enhances robustness in noisy\nenvironments by integrating visual cues. While recent advances integrate Large\nLanguage Models (LLMs) into AVSR, their high computational cost hinders\ndeployment in resource-constrained settings. To address this, we propose\nLlama-SMoP, an efficient Multimodal LLM that employs a Sparse Mixture of\nProjectors (SMoP) module to scale model capacity without increasing inference\ncosts. By incorporating sparsely-gated mixture-of-experts (MoE) projectors,\nLlama-SMoP enables the use of smaller LLMs while maintaining strong\nperformance. We explore three SMoP configurations and show that Llama-SMoP DEDR\n(Disjoint-Experts, Disjoint-Routers), which uses modality-specific routers and\nexperts, achieves superior performance on ASR, VSR, and AVSR tasks. Ablation\nstudies confirm its effectiveness in expert activation, scalability, and noise\nrobustness.", "AI": {"tldr": "Llama-SMoP\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u7a00\u758f\u6df7\u5408\u6295\u5f71\u5668\uff08SMoP\uff09\u6a21\u5757\u5728\u4e0d\u589e\u52a0\u63a8\u7406\u6210\u672c\u7684\u60c5\u51b5\u4e0b\u63d0\u5347\u6a21\u578b\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u73af\u5883\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u97f3\u9891-\u89c6\u89c9\u8bed\u97f3\u8bc6\u522b\uff08AVSR\uff09\u4e2d\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u63d0\u5347\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u7684\u90e8\u7f72\u80fd\u529b\u3002", "method": "\u63d0\u51faLlama-SMoP\uff0c\u91c7\u7528\u7a00\u758f\u95e8\u63a7\u7684\u4e13\u5bb6\u6df7\u5408\uff08MoE\uff09\u6295\u5f71\u5668\uff0c\u63a2\u7d22\u4e09\u79cdSMoP\u914d\u7f6e\uff0c\u5176\u4e2dDEDR\uff08\u5206\u79bb\u4e13\u5bb6\u548c\u8def\u7531\u5668\uff09\u8868\u73b0\u6700\u4f73\u3002", "result": "Llama-SMoP DEDR\u5728ASR\u3001VSR\u548cAVSR\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u4e13\u5bb6\u6fc0\u6d3b\u3001\u53ef\u6269\u5c55\u6027\u548c\u566a\u58f0\u9c81\u68d2\u6027\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "Llama-SMoP\u901a\u8fc7SMoP\u6a21\u5757\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u591a\u6a21\u6001LLM\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.14471", "pdf": "https://arxiv.org/pdf/2505.14471", "abs": "https://arxiv.org/abs/2505.14471", "authors": ["Tong Li", "Jiachuan Wang", "Yongqi Zhang", "Shuangyin Li", "Lei Chen"], "title": "Adapting Pretrained Language Models for Citation Classification via Self-Supervised Contrastive Learning", "categories": ["cs.CL"], "comment": "Manuscripts, accepted to KDD 2025", "summary": "Citation classification, which identifies the intention behind academic\ncitations, is pivotal for scholarly analysis. Previous works suggest\nfine-tuning pretrained language models (PLMs) on citation classification\ndatasets, reaping the reward of the linguistic knowledge they gained during\npretraining. However, directly fine-tuning for citation classification is\nchallenging due to labeled data scarcity, contextual noise, and spurious\nkeyphrase correlations. In this paper, we present a novel framework, Citss,\nthat adapts the PLMs to overcome these challenges. Citss introduces\nself-supervised contrastive learning to alleviate data scarcity, and is\nequipped with two specialized strategies to obtain the contrastive pairs:\nsentence-level cropping, which enhances focus on target citations within long\ncontexts, and keyphrase perturbation, which mitigates reliance on specific\nkeyphrases. Compared with previous works that are only designed for\nencoder-based PLMs, Citss is carefully developed to be compatible with both\nencoder-based PLMs and decoder-based LLMs, to embrace the benefits of enlarged\npretraining. Experiments with three benchmark datasets with both encoder-based\nPLMs and decoder-based LLMs demonstrate our superiority compared to the\nprevious state of the art. Our code is available at: github.com/LITONG99/Citss", "AI": {"tldr": "Citss\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u548c\u4e24\u79cd\u4e13\u95e8\u7b56\u7565\u89e3\u51b3\u5f15\u6587\u5206\u7c7b\u4e2d\u7684\u6570\u636e\u7a00\u7f3a\u548c\u566a\u58f0\u95ee\u9898\uff0c\u517c\u5bb9\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u6a21\u578b\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5f15\u6587\u5206\u7c7b\u5bf9\u5b66\u672f\u5206\u6790\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u76f4\u63a5\u5fae\u8c03\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u9762\u4e34\u6570\u636e\u7a00\u7f3a\u3001\u4e0a\u4e0b\u6587\u566a\u58f0\u548c\u865a\u5047\u5173\u952e\u8bcd\u5173\u8054\u7684\u6311\u6218\u3002", "method": "Citss\u5f15\u5165\u81ea\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\uff0c\u7ed3\u5408\u53e5\u5b50\u7ea7\u88c1\u526a\u548c\u5173\u952e\u8bcd\u6270\u52a8\u7b56\u7565\uff0c\u517c\u5bb9\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u6a21\u578b\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cCitss\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "Citss\u6709\u6548\u89e3\u51b3\u4e86\u5f15\u6587\u5206\u7c7b\u4e2d\u7684\u6311\u6218\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u517c\u5bb9\u6027\u548c\u6027\u80fd\u4f18\u52bf\u3002"}}
{"id": "2505.14541", "pdf": "https://arxiv.org/pdf/2505.14541", "abs": "https://arxiv.org/abs/2505.14541", "authors": ["Chuanbo Tang", "Zhuoyuan Li", "Yifan Bian", "Li Li", "Dong Liu"], "title": "Neural Video Compression with Context Modulation", "categories": ["eess.IV", "cs.CV"], "comment": "11 pages, 8 figures, accepted by CVPR 2025", "summary": "Efficient video coding is highly dependent on exploiting the temporal\nredundancy, which is usually achieved by extracting and leveraging the temporal\ncontext in the emerging conditional coding-based neural video codec (NVC).\nAlthough the latest NVC has achieved remarkable progress in improving the\ncompression performance, the inherent temporal context propagation mechanism\nlacks the ability to sufficiently leverage the reference information, limiting\nfurther improvement. In this paper, we address the limitation by modulating the\ntemporal context with the reference frame in two steps. Specifically, we first\npropose the flow orientation to mine the inter-correlation between the\nreference frame and prediction frame for generating the additional oriented\ntemporal context. Moreover, we introduce the context compensation to leverage\nthe oriented context to modulate the propagated temporal context generated from\nthe propagated reference feature. Through the synergy mechanism and decoupling\nloss supervision, the irrelevant propagated information can be effectively\neliminated to ensure better context modeling. Experimental results demonstrate\nthat our codec achieves on average 22.7% bitrate reduction over the advanced\ntraditional video codec H.266/VVC, and offers an average 10.1% bitrate saving\nover the previous state-of-the-art NVC DCVC-FM. The code is available at\nhttps://github.com/Austin4USTC/DCMVC.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u8c03\u5236\u65f6\u95f4\u4e0a\u4e0b\u6587\u7684\u4e24\u6b65\u6cd5\u6765\u63d0\u5347\u795e\u7ecf\u89c6\u9891\u7f16\u7801\u5668\uff08NVC\uff09\u7684\u6027\u80fd\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u6bd4\u7279\u7387\u3002", "motivation": "\u73b0\u6709NVC\u7684\u65f6\u95f4\u4e0a\u4e0b\u6587\u4f20\u64ad\u673a\u5236\u672a\u80fd\u5145\u5206\u5229\u7528\u53c2\u8003\u4fe1\u606f\uff0c\u9650\u5236\u4e86\u538b\u7f29\u6027\u80fd\u7684\u8fdb\u4e00\u6b65\u63d0\u5347\u3002", "method": "\u901a\u8fc7\u6d41\u5bfc\u5411\u6316\u6398\u53c2\u8003\u5e27\u4e0e\u9884\u6d4b\u5e27\u7684\u4e92\u76f8\u5173\u6027\u751f\u6210\u989d\u5916\u65f6\u95f4\u4e0a\u4e0b\u6587\uff0c\u5e76\u5f15\u5165\u4e0a\u4e0b\u6587\u8865\u507f\u673a\u5236\u8c03\u5236\u4f20\u64ad\u7684\u65f6\u95f4\u4e0a\u4e0b\u6587\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u7f16\u7801\u5668\u5e73\u5747\u6bd4\u7279\u7387\u6bd4H.266/VVC\u964d\u4f4e22.7%\uff0c\u6bd4DCVC-FM\u964d\u4f4e10.1%\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86NVC\u7684\u6027\u80fd\uff0c\u4e3a\u89c6\u9891\u538b\u7f29\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.14481", "pdf": "https://arxiv.org/pdf/2505.14481", "abs": "https://arxiv.org/abs/2505.14481", "authors": ["He Zhu", "Junyou Su", "Minxi Chen", "Wen Wang", "Yijie Deng", "Guanhua Chen", "Wenjia Zhang"], "title": "PlanGPT-VL: Enhancing Urban Planning with Domain-Specific Vision-Language Models", "categories": ["cs.CL"], "comment": null, "summary": "In the field of urban planning, existing Vision-Language Models (VLMs)\nfrequently fail to effectively analyze and evaluate planning maps, despite the\ncritical importance of these visual elements for urban planners and related\neducational contexts. Planning maps, which visualize land use, infrastructure\nlayouts, and functional zoning, require specialized understanding of spatial\nconfigurations, regulatory requirements, and multi-scale analysis. To address\nthis challenge, we introduce PlanGPT-VL, the first domain-specific\nVision-Language Model tailored specifically for urban planning maps. PlanGPT-VL\nemploys three innovative approaches: (1) PlanAnno-V framework for high-quality\nVQA data synthesis, (2) Critical Point Thinking to reduce hallucinations\nthrough structured verification, and (3) comprehensive training methodology\ncombining Supervised Fine-Tuning with frozen vision encoder parameters. Through\nsystematic evaluation on our proposed PlanBench-V benchmark, we demonstrate\nthat PlanGPT-VL significantly outperforms general-purpose state-of-the-art VLMs\nin specialized planning map interpretation tasks, offering urban planning\nprofessionals a reliable tool for map analysis, assessment, and educational\napplications while maintaining high factual accuracy. Our lightweight 7B\nparameter model achieves comparable performance to models exceeding 72B\nparameters, demonstrating efficient domain specialization without sacrificing\nperformance.", "AI": {"tldr": "PlanGPT-VL\u662f\u4e00\u79cd\u4e13\u4e3a\u57ce\u5e02\u89c4\u5212\u5730\u56fe\u8bbe\u8ba1\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u521b\u65b0\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u5730\u56fe\u5206\u6790\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u57ce\u5e02\u89c4\u5212\u5730\u56fe\u5206\u6790\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4e9f\u9700\u4e13\u4e1a\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528PlanAnno-V\u6846\u67b6\u3001Critical Point Thinking\u548c\u7efc\u5408\u8bad\u7ec3\u65b9\u6cd5\u3002", "result": "PlanGPT-VL\u5728PlanBench-V\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u901a\u7528\u6a21\u578b\uff0c\u4e14\u53c2\u6570\u6548\u7387\u9ad8\u3002", "conclusion": "PlanGPT-VL\u4e3a\u57ce\u5e02\u89c4\u5212\u63d0\u4f9b\u4e86\u9ad8\u6548\u53ef\u9760\u7684\u5de5\u5177\uff0c\u517c\u5177\u4e13\u4e1a\u6027\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2505.14560", "pdf": "https://arxiv.org/pdf/2505.14560", "abs": "https://arxiv.org/abs/2505.14560", "authors": ["Yuan Gao", "Wenhan Guo", "Yu Sun"], "title": "Neural Inverse Scattering with Score-based Regularization", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Inverse scattering is a fundamental challenge in many imaging applications,\nranging from microscopy to remote sensing. Solving this problem often requires\njointly estimating two unknowns -- the image and the scattering field inside\nthe object -- necessitating effective image prior to regularize the inference.\nIn this paper, we propose a regularized neural field (NF) approach which\nintegrates the denoising score function used in score-based generative models.\nThe neural field formulation offers convenient flexibility to performing joint\nestimation, while the denoising score function imposes the rich structural\nprior of images. Our results on three high-contrast simulated objects show that\nthe proposed approach yields a better imaging quality compared to the\nstate-of-the-art NF approach, where regularization is based on total variation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u53bb\u566a\u8bc4\u5206\u51fd\u6570\u7684\u6b63\u5219\u5316\u795e\u7ecf\u573a\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u9006\u6563\u5c04\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u6210\u50cf\u8d28\u91cf\u3002", "motivation": "\u9006\u6563\u5c04\u95ee\u9898\u662f\u6210\u50cf\u5e94\u7528\u4e2d\u7684\u57fa\u7840\u6311\u6218\uff0c\u9700\u8981\u540c\u65f6\u4f30\u8ba1\u56fe\u50cf\u548c\u6563\u5c04\u573a\uff0c\u56e0\u6b64\u9700\u8981\u6709\u6548\u7684\u56fe\u50cf\u5148\u9a8c\u6765\u89c4\u8303\u63a8\u65ad\u3002", "method": "\u91c7\u7528\u6b63\u5219\u5316\u795e\u7ecf\u573a\uff08NF\uff09\u65b9\u6cd5\uff0c\u5e76\u6574\u5408\u4e86\u57fa\u4e8e\u53bb\u566a\u8bc4\u5206\u51fd\u6570\u7684\u56fe\u50cf\u7ed3\u6784\u5148\u9a8c\u3002", "result": "\u5728\u4e09\u4e2a\u9ad8\u5bf9\u6bd4\u5ea6\u6a21\u62df\u5bf9\u8c61\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6bd4\u57fa\u4e8e\u603b\u53d8\u5206\u7684\u73b0\u6709NF\u65b9\u6cd5\u6210\u50cf\u8d28\u91cf\u66f4\u597d\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408\u795e\u7ecf\u573a\u548c\u53bb\u566a\u8bc4\u5206\u51fd\u6570\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9006\u6563\u5c04\u95ee\u9898\u7684\u6210\u50cf\u6548\u679c\u3002"}}
{"id": "2505.14483", "pdf": "https://arxiv.org/pdf/2505.14483", "abs": "https://arxiv.org/abs/2505.14483", "authors": ["Agam Goyal", "Xianyang Zhan", "Yilun Chen", "Koustuv Saha", "Eshwar Chandrasekharan"], "title": "MoMoE: Mixture of Moderation Experts Framework for AI-Assisted Online Governance", "categories": ["cs.CL"], "comment": "Preprint: 15 pages, 4 figures, 2 tables", "summary": "Large language models (LLMs) have shown great potential in flagging harmful\ncontent in online communities. Yet, existing approaches for moderation require\na separate model for every community and are opaque in their decision-making,\nlimiting real-world adoption. We introduce Mixture of Moderation Experts\n(MoMoE), a modular, cross-community framework that adds post-hoc explanations\nto scalable content moderation. MoMoE orchestrates four operators -- Allocate,\nPredict, Aggregate, Explain -- and is instantiated as seven\ncommunity-specialized experts (MoMoE-Community) and five norm-violation experts\n(MoMoE-NormVio). On 30 unseen subreddits, the best variants obtain Micro-F1\nscores of 0.72 and 0.67, respectively, matching or surpassing strong fine-tuned\nbaselines while consistently producing concise and reliable explanations.\nAlthough community-specialized experts deliver the highest peak accuracy,\nnorm-violation experts provide steadier performance across domains. These\nfindings show that MoMoE yields scalable, transparent moderation without\nneeding per-community fine-tuning. More broadly, they suggest that lightweight,\nexplainable expert ensembles can guide future NLP and HCI research on\ntrustworthy human-AI governance of online communities.", "AI": {"tldr": "MoMoE\u6846\u67b6\u901a\u8fc7\u6a21\u5757\u5316\u8bbe\u8ba1\u5b9e\u73b0\u8de8\u793e\u533a\u5185\u5bb9\u5ba1\u6838\uff0c\u63d0\u4f9b\u900f\u660e\u51b3\u7b56\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u5185\u5bb9\u5ba1\u6838\u65b9\u6cd5\u9700\u4e3a\u6bcf\u4e2a\u793e\u533a\u5355\u72ec\u8bad\u7ec3\u6a21\u578b\u4e14\u51b3\u7b56\u4e0d\u900f\u660e\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "MoMoE\u6846\u67b6\u5305\u542b\u56db\u4e2a\u64cd\u4f5c\uff08\u5206\u914d\u3001\u9884\u6d4b\u3001\u805a\u5408\u3001\u89e3\u91ca\uff09\uff0c\u5206\u4e3a\u793e\u533a\u4e13\u5bb6\u548c\u89c4\u8303\u8fdd\u89c4\u4e13\u5bb6\u4e24\u7c7b\u3002", "result": "\u572830\u4e2a\u5b50\u8bba\u575b\u4e0a\uff0cMoMoE\u6027\u80fd\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u5e76\u63d0\u4f9b\u7b80\u6d01\u53ef\u9760\u89e3\u91ca\u3002", "conclusion": "MoMoE\u5c55\u793a\u4e86\u8f7b\u91cf\u7ea7\u3001\u53ef\u89e3\u91ca\u7684\u4e13\u5bb6\u96c6\u6210\u5728\u53ef\u4fe1\u4eba\u673a\u6cbb\u7406\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.14572", "pdf": "https://arxiv.org/pdf/2505.14572", "abs": "https://arxiv.org/abs/2505.14572", "authors": ["Jayroop Ramesh", "Valentin Bacher", "Mark C. Eid", "Hoda Kalabizadeh", "Christian Rupprecht", "Ana IL Namburete", "Pak-Hei Yeung", "Madeleine K. Wyburd", "Nicola K. Dinsdale"], "title": "Automated Fetal Biometry Assessment with Deep Ensembles using Sparse-Sampling of 2D Intrapartum Ultrasound Images", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": "Top 5 in MICCAI IUGC 2024: Intrapartum Ultrasound Grand Challenge &\n  Runners up in Classification!", "summary": "The International Society of Ultrasound advocates Intrapartum Ultrasound (US)\nImaging in Obstetrics and Gynecology (ISUOG) to monitor labour progression\nthrough changes in fetal head position. Two reliable ultrasound-derived\nparameters that are used to predict outcomes of instrumental vaginal delivery\nare the angle of progression (AoP) and head-symphysis distance (HSD). In this\nwork, as part of the Intrapartum Ultrasounds Grand Challenge (IUGC) 2024, we\npropose an automated fetal biometry measurement pipeline to reduce intra- and\ninter-observer variability and improve measurement reliability. Our pipeline\nconsists of three key tasks: (i) classification of standard planes (SP) from US\nvideos, (ii) segmentation of fetal head and pubic symphysis from the detected\nSPs, and (iii) computation of the AoP and HSD from the segmented regions. We\nperform sparse sampling to mitigate class imbalances and reduce spurious\ncorrelations in task (i), and utilize ensemble-based deep learning methods for\ntask (i) and (ii) to enhance generalizability under different US acquisition\nsettings. Finally, to promote robustness in task iii) with respect to the\nstructural fidelity of measurements, we retain the largest connected components\nand apply ellipse fitting to the segmentations. Our solution achieved ACC:\n0.9452, F1: 0.9225, AUC: 0.983, MCC: 0.8361, DSC: 0.918, HD: 19.73, ASD: 5.71,\n$\\Delta_{AoP}$: 8.90 and $\\Delta_{HSD}$: 14.35 across an unseen hold-out set of\n4 patients and 224 US frames. The results from the proposed automated pipeline\ncan improve the understanding of labour arrest causes and guide the development\nof clinical risk stratification tools for efficient and effective prenatal\ncare.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u80ce\u513f\u751f\u7269\u6d4b\u91cf\u6d41\u7a0b\uff0c\u7528\u4e8e\u51cf\u5c11\u8d85\u58f0\u6d4b\u91cf\u4e2d\u7684\u89c2\u5bdf\u8005\u5dee\u5f02\u5e76\u63d0\u9ad8\u53ef\u9760\u6027\uff0c\u901a\u8fc7\u5206\u7c7b\u3001\u5206\u5272\u548c\u8ba1\u7b97\u5173\u952e\u53c2\u6570\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u7ed3\u679c\u3002", "motivation": "\u51cf\u5c11\u4ea7\u7a0b\u8d85\u58f0\u76d1\u6d4b\u4e2d\u7684\u89c2\u5bdf\u8005\u5dee\u5f02\uff0c\u63d0\u9ad8\u6d4b\u91cf\u53ef\u9760\u6027\uff0c\u4ee5\u66f4\u597d\u5730\u7406\u89e3\u4ea7\u7a0b\u505c\u6ede\u539f\u56e0\u5e76\u6307\u5bfc\u4e34\u5e8a\u98ce\u9669\u5206\u5c42\u3002", "method": "\u63d0\u51fa\u4e09\u9636\u6bb5\u6d41\u7a0b\uff1a\u6807\u51c6\u5e73\u9762\u5206\u7c7b\u3001\u80ce\u513f\u5934\u90e8\u548c\u803b\u9aa8\u8054\u5408\u5206\u5272\u3001\u8ba1\u7b97\u89d2\u5ea6\u548c\u8ddd\u79bb\u53c2\u6570\uff0c\u91c7\u7528\u7a00\u758f\u91c7\u6837\u548c\u96c6\u6210\u5b66\u4e60\u65b9\u6cd5\u63d0\u9ad8\u9c81\u68d2\u6027\u3002", "result": "\u5728\u672a\u89c1\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5404\u9879\u6307\u6807\uff08\u5982ACC\u3001F1\u3001AUC\u7b49\uff09\u5747\u663e\u793a\u9ad8\u7cbe\u5ea6\uff0c\u6d4b\u91cf\u8bef\u5dee\u8f83\u5c0f\u3002", "conclusion": "\u81ea\u52a8\u5316\u6d41\u7a0b\u53ef\u63d0\u5347\u4ea7\u7a0b\u76d1\u6d4b\u7684\u53ef\u9760\u6027\uff0c\u4e3a\u4e34\u5e8a\u98ce\u9669\u5206\u5c42\u5de5\u5177\u7684\u5f00\u53d1\u63d0\u4f9b\u652f\u6301\u3002"}}
{"id": "2505.14499", "pdf": "https://arxiv.org/pdf/2505.14499", "abs": "https://arxiv.org/abs/2505.14499", "authors": ["Jun Cao", "Jiyi Li", "Ziwei Yang", "Renjie Zhou"], "title": "Enhanced Multimodal Aspect-Based Sentiment Analysis by LLM-Generated Rationales", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "There has been growing interest in Multimodal Aspect-Based Sentiment Analysis\n(MABSA) in recent years. Existing methods predominantly rely on pre-trained\nsmall language models (SLMs) to collect information related to aspects and\nsentiments from both image and text, with an aim to align these two modalities.\nHowever, small SLMs possess limited capacity and knowledge, often resulting in\ninaccurate identification of meaning, aspects, sentiments, and their\ninterconnections in textual and visual data. On the other hand, Large language\nmodels (LLMs) have shown exceptional capabilities in various tasks by\neffectively exploring fine-grained information in multimodal data. However,\nsome studies indicate that LLMs still fall short compared to fine-tuned small\nmodels in the field of ABSA. Based on these findings, we propose a novel\nframework, termed LRSA, which combines the decision-making capabilities of SLMs\nwith additional information provided by LLMs for MABSA. Specifically, we inject\nexplanations generated by LLMs as rationales into SLMs and employ a dual\ncross-attention mechanism for enhancing feature interaction and fusion, thereby\naugmenting the SLMs' ability to identify aspects and sentiments. We evaluated\nour method using two baseline models, numerous experiments highlight the\nsuperiority of our approach on three widely-used benchmarks, indicating its\ngeneralizability and applicability to most pre-trained models for MABSA.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08SLM\uff09\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u65b0\u6846\u67b6LRSA\uff0c\u7528\u4e8e\u591a\u6a21\u6001\u57fa\u4e8e\u65b9\u9762\u7684\u60c5\u611f\u5206\u6790\uff08MABSA\uff09\uff0c\u901a\u8fc7LLM\u751f\u6210\u7684\u89e3\u91ca\u589e\u5f3aSLM\u7684\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08SLM\uff09\u8fdb\u884c\u591a\u6a21\u6001\u60c5\u611f\u5206\u6790\uff0c\u4f46\u5176\u80fd\u529b\u6709\u9650\uff0c\u5bfc\u81f4\u5bf9\u6587\u672c\u548c\u89c6\u89c9\u6570\u636e\u4e2d\u65b9\u9762\u3001\u60c5\u611f\u53ca\u5176\u5173\u8054\u7684\u8bc6\u522b\u4e0d\u51c6\u786e\u3002\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u867d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728ABSA\u9886\u57df\u4ecd\u4e0d\u53ca\u5fae\u8c03\u7684\u5c0f\u578b\u6a21\u578b\u3002", "method": "\u63d0\u51faLRSA\u6846\u67b6\uff0c\u5c06LLM\u751f\u6210\u7684\u89e3\u91ca\u4f5c\u4e3a\u7406\u6027\u6ce8\u5165SLM\uff0c\u5e76\u91c7\u7528\u53cc\u91cd\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u589e\u5f3a\u7279\u5f81\u4ea4\u4e92\u4e0e\u878d\u5408\uff0c\u63d0\u5347SLM\u5bf9\u65b9\u9762\u548c\u60c5\u611f\u7684\u8bc6\u522b\u80fd\u529b\u3002", "result": "\u5728\u4e24\u4e2a\u57fa\u7ebf\u6a21\u578b\u4e0a\u8bc4\u4f30\uff0c\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u4e09\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u8d8a\uff0c\u5177\u6709\u901a\u7528\u6027\u548c\u9002\u7528\u6027\u3002", "conclusion": "LRSA\u6846\u67b6\u901a\u8fc7\u7ed3\u5408SLM\u548cLLM\u7684\u4f18\u52bf\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u57fa\u4e8e\u65b9\u9762\u7684\u60c5\u611f\u5206\u6790\u6027\u80fd\u3002"}}
{"id": "2505.14629", "pdf": "https://arxiv.org/pdf/2505.14629", "abs": "https://arxiv.org/abs/2505.14629", "authors": ["Fnu Mohbat", "Mohammed J Zaki"], "title": "KERL: Knowledge-Enhanced Personalized Recipe Recommendation using Large Language Models", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": "Accepted at ACL 2025", "summary": "Recent advances in large language models (LLMs) and the abundance of food\ndata have resulted in studies to improve food understanding using LLMs. Despite\nseveral recommendation systems utilizing LLMs and Knowledge Graphs (KGs), there\nhas been limited research on integrating food related KGs with LLMs. We\nintroduce KERL, a unified system that leverages food KGs and LLMs to provide\npersonalized food recommendations and generates recipes with associated\nmicro-nutritional information. Given a natural language question, KERL extracts\nentities, retrieves subgraphs from the KG, which are then fed into the LLM as\ncontext to select the recipes that satisfy the constraints. Next, our system\ngenerates the cooking steps and nutritional information for each recipe. To\nevaluate our approach, we also develop a benchmark dataset by curating recipe\nrelated questions, combined with constraints and personal preferences. Through\nextensive experiments, we show that our proposed KG-augmented LLM significantly\noutperforms existing approaches, offering a complete and coherent solution for\nfood recommendation, recipe generation, and nutritional analysis. Our code and\nbenchmark datasets are publicly available at\nhttps://github.com/mohbattharani/KERL.", "AI": {"tldr": "KERL\u662f\u4e00\u4e2a\u7ed3\u5408\u98df\u7269\u77e5\u8bc6\u56fe\u8c31\uff08KG\uff09\u548c\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u7cfb\u7edf\uff0c\u7528\u4e8e\u4e2a\u6027\u5316\u98df\u7269\u63a8\u8350\u548c\u98df\u8c31\u751f\u6210\uff0c\u5e76\u63d0\u4f9b\u8425\u517b\u4fe1\u606f\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u8f83\u5c11\u5c06\u98df\u7269\u76f8\u5173KG\u4e0eLLM\u7ed3\u5408\uff0cKERL\u586b\u8865\u4e86\u8fd9\u4e00\u7a7a\u767d\uff0c\u63d0\u4f9b\u66f4\u5168\u9762\u7684\u98df\u7269\u63a8\u8350\u548c\u8425\u517b\u5206\u6790\u89e3\u51b3\u65b9\u6848\u3002", "method": "KERL\u901a\u8fc7\u63d0\u53d6\u81ea\u7136\u8bed\u8a00\u95ee\u9898\u4e2d\u7684\u5b9e\u4f53\uff0c\u4eceKG\u4e2d\u68c0\u7d22\u5b50\u56fe\uff0c\u5e76\u5c06\u5176\u4f5c\u4e3a\u4e0a\u4e0b\u6587\u8f93\u5165LLM\uff0c\u751f\u6210\u6ee1\u8db3\u7ea6\u675f\u7684\u98df\u8c31\u53ca\u5176\u70f9\u996a\u6b65\u9aa4\u548c\u8425\u517b\u4fe1\u606f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cKERL\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u63d0\u4f9b\u4e86\u4e00\u81f4\u4e14\u5b8c\u6574\u7684\u98df\u7269\u63a8\u8350\u3001\u98df\u8c31\u751f\u6210\u548c\u8425\u517b\u5206\u6790\u65b9\u6848\u3002", "conclusion": "KERL\u4e3a\u98df\u7269\u7406\u89e3\u548c\u4e2a\u6027\u5316\u63a8\u8350\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u5168\u9762\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5df2\u5f00\u6e90\u3002"}}
{"id": "2505.14505", "pdf": "https://arxiv.org/pdf/2505.14505", "abs": "https://arxiv.org/abs/2505.14505", "authors": ["Jiale Kang", "Ziyin Yue", "Qingyu Yin", "Jiang Rui", "Weile Li", "Zening Lu", "Zhouran Ji"], "title": "ModRWKV: Transformer Multimodality in Linear Time", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Currently, most multimodal studies are based on large language models (LLMs)\nwith quadratic-complexity Transformer architectures. While linear models like\nRNNs enjoy low inference costs, their application has been largely limited to\nthe text-only modality. This work explores the capabilities of modern RNN\narchitectures in multimodal contexts. We propose ModRWKV-a decoupled multimodal\nframework built upon the RWKV7 architecture as its LLM backbone-which achieves\nmulti-source information fusion through dynamically adaptable heterogeneous\nmodality encoders. We designed the multimodal modules in ModRWKV with an\nextremely lightweight architecture and, through extensive experiments,\nidentified a configuration that achieves an optimal balance between performance\nand computational efficiency. ModRWKV leverages the pretrained weights of the\nRWKV7 LLM for initialization, which significantly accelerates multimodal\ntraining. Comparative experiments with different pretrained checkpoints further\ndemonstrate that such initialization plays a crucial role in enhancing the\nmodel's ability to understand multimodal signals. Supported by extensive\nexperiments, we conclude that modern RNN architectures present a viable\nalternative to Transformers in the domain of multimodal large language models\n(MLLMs). Furthermore, we identify the optimal configuration of the ModRWKV\narchitecture through systematic exploration.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eRWKV7\u67b6\u6784\u7684\u591a\u6a21\u6001\u6846\u67b6ModRWKV\uff0c\u901a\u8fc7\u52a8\u6001\u9002\u914d\u7684\u5f02\u8d28\u6a21\u6001\u7f16\u7801\u5668\u5b9e\u73b0\u591a\u6e90\u4fe1\u606f\u878d\u5408\uff0c\u5c55\u793a\u4e86\u73b0\u4ee3RNN\u67b6\u6784\u5728\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u6f5c\u529b\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u7814\u7a76\u4e3b\u8981\u4f9d\u8d56\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u7684Transformer\u67b6\u6784\uff0c\u800c\u7ebf\u6027\u6a21\u578b\u5982RNN\u5728\u8ba1\u7b97\u6548\u7387\u4e0a\u6709\u4f18\u52bf\u4f46\u591a\u9650\u4e8e\u5355\u6a21\u6001\u5e94\u7528\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u73b0\u4ee3RNN\u5728\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51faModRWKV\u6846\u67b6\uff0c\u57fa\u4e8eRWKV7\u67b6\u6784\uff0c\u91c7\u7528\u8f7b\u91cf\u7ea7\u591a\u6a21\u6001\u6a21\u5757\u8bbe\u8ba1\uff0c\u5e76\u901a\u8fc7\u9884\u8bad\u7ec3\u6743\u91cd\u521d\u59cb\u5316\u52a0\u901f\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u8868\u660eModRWKV\u5728\u6027\u80fd\u548c\u8ba1\u7b97\u6548\u7387\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u9884\u8bad\u7ec3\u6743\u91cd\u521d\u59cb\u5316\u663e\u8457\u63d0\u5347\u591a\u6a21\u6001\u4fe1\u53f7\u7406\u89e3\u80fd\u529b\u3002", "conclusion": "\u73b0\u4ee3RNN\u67b6\u6784\u53ef\u4f5c\u4e3aTransformer\u7684\u66ff\u4ee3\u65b9\u6848\u7528\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5e76\u786e\u5b9a\u4e86ModRWKV\u7684\u6700\u4f18\u914d\u7f6e\u3002"}}
{"id": "2505.14660", "pdf": "https://arxiv.org/pdf/2505.14660", "abs": "https://arxiv.org/abs/2505.14660", "authors": ["Ronald Seoh", "Dan Goldwasser"], "title": "EmoGist: Efficient In-Context Learning for Visual Emotion Understanding", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "In this paper, we introduce EmoGist, a training-free, in-context learning\nmethod for performing visual emotion classification with LVLMs. The key\nintuition of our approach is that context-dependent definition of emotion\nlabels could allow more accurate predictions of emotions, as the ways in which\nemotions manifest within images are highly context dependent and nuanced.\nEmoGist pre-generates multiple explanations of emotion labels, by analyzing the\nclusters of example images belonging to each category. At test time, we\nretrieve a version of explanation based on embedding similarity, and feed it to\na fast VLM for classification. Through our experiments, we show that EmoGist\nallows up to 13 points improvement in micro F1 scores with the multi-label\nMemotion dataset, and up to 8 points in macro F1 in the multi-class FI dataset.", "AI": {"tldr": "EmoGist\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u3001\u57fa\u4e8e\u4e0a\u4e0b\u6587\u5b66\u4e60\u7684\u89c6\u89c9\u60c5\u611f\u5206\u7c7b\u65b9\u6cd5\uff0c\u901a\u8fc7\u9884\u751f\u6210\u60c5\u611f\u6807\u7b7e\u7684\u591a\u91cd\u89e3\u91ca\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u60c5\u611f\u5728\u56fe\u50cf\u4e2d\u7684\u8868\u73b0\u9ad8\u5ea6\u4f9d\u8d56\u4e0a\u4e0b\u6587\u4e14\u590d\u6742\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u51c6\u786e\u6355\u6349\u3002EmoGist\u65e8\u5728\u901a\u8fc7\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u6807\u7b7e\u5b9a\u4e49\u63d0\u5347\u60c5\u611f\u5206\u7c7b\u7684\u51c6\u786e\u6027\u3002", "method": "EmoGist\u9884\u751f\u6210\u60c5\u611f\u6807\u7b7e\u7684\u591a\u91cd\u89e3\u91ca\uff0c\u57fa\u4e8e\u5d4c\u5165\u76f8\u4f3c\u6027\u68c0\u7d22\u5408\u9002\u7684\u89e3\u91ca\uff0c\u5e76\u5229\u7528\u5feb\u901f\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5206\u7c7b\u3002", "result": "\u5728Memotion\u6570\u636e\u96c6\u4e0a\uff0cEmoGist\u7684\u5faeF1\u5206\u6570\u63d0\u534713\u70b9\uff1b\u5728FI\u6570\u636e\u96c6\u4e0a\uff0c\u5b8fF1\u5206\u6570\u63d0\u53478\u70b9\u3002", "conclusion": "EmoGist\u901a\u8fc7\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u6807\u7b7e\u89e3\u91ca\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u60c5\u611f\u5206\u7c7b\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2505.14523", "pdf": "https://arxiv.org/pdf/2505.14523", "abs": "https://arxiv.org/abs/2505.14523", "authors": ["Michael Sullivan"], "title": "Exploring Graph Representations of Logical Forms for Language Modeling", "categories": ["cs.CL", "cs.AI", "I.2.7"], "comment": "To be published in ACL 2025 Findings", "summary": "We make the case for language models over logical forms (LFLMs), arguing that\nsuch models are more data-efficient than their textual counterparts. To that\nend, we introduce the Graph-based Formal-Logical Distributional Semantics\n(GFoLDS) prototype, a pretrained LM over graph representations of logical\nforms, as a proof-of-concept of LFLMs. Using GFoLDS, we present strong\nexperimental evidence that LFLMs can leverage the built-in, basic linguistic\nknowledge inherent in such models to immediately begin learning more complex\npatterns. On downstream tasks, we show that GFoLDS vastly outperforms textual,\ntransformer LMs pretrained on similar amounts of data, indicating that LFLMs\ncan learn with substantially less data than models over plain text.\nFurthermore, we show that the performance of this model is likely to scale with\nadditional parameters and pretraining data, suggesting the viability of LFLMs\nin real-world applications.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u57fa\u4e8e\u903b\u8f91\u5f62\u5f0f\u7684\u8bed\u8a00\u6a21\u578b\uff08LFLMs\uff09\uff0c\u5e76\u901a\u8fc7GFoLDS\u539f\u578b\u8bc1\u660e\u5176\u6570\u636e\u6548\u7387\u4f18\u4e8e\u6587\u672c\u6a21\u578b\u3002\u5b9e\u9a8c\u8868\u660e\uff0cLFLMs\u80fd\u5229\u7528\u5185\u7f6e\u8bed\u8a00\u77e5\u8bc6\u5feb\u901f\u5b66\u4e60\u590d\u6742\u6a21\u5f0f\uff0c\u4e14\u5728\u5c0f\u6570\u636e\u91cf\u4e0b\u8868\u73b0\u4f18\u4e8e\u6587\u672c\u6a21\u578b\u3002", "motivation": "\u63a2\u8ba8\u903b\u8f91\u5f62\u5f0f\u8bed\u8a00\u6a21\u578b\uff08LFLMs\uff09\u7684\u6570\u636e\u6548\u7387\u4f18\u52bf\uff0c\u5e76\u9a8c\u8bc1\u5176\u5728\u771f\u5b9e\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002", "method": "\u63d0\u51faGFoLDS\u539f\u578b\uff0c\u4e00\u79cd\u57fa\u4e8e\u56fe\u8868\u793a\u903b\u8f91\u5f62\u5f0f\u7684\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u6bd4\u8f83\u5176\u4e0e\u6587\u672c\u6a21\u578b\u7684\u6027\u80fd\u3002", "result": "GFoLDS\u5728\u5c0f\u6570\u636e\u91cf\u4e0b\u663e\u8457\u4f18\u4e8e\u6587\u672c\u6a21\u578b\uff0c\u4e14\u6027\u80fd\u53ef\u80fd\u968f\u53c2\u6570\u548c\u6570\u636e\u91cf\u589e\u52a0\u800c\u63d0\u5347\u3002", "conclusion": "LFLMs\u5177\u6709\u66f4\u9ad8\u7684\u6570\u636e\u6548\u7387\u548c\u6269\u5c55\u6f5c\u529b\uff0c\u9002\u7528\u4e8e\u771f\u5b9e\u573a\u666f\u3002"}}
{"id": "2505.14681", "pdf": "https://arxiv.org/pdf/2505.14681", "abs": "https://arxiv.org/abs/2505.14681", "authors": ["Mengru Wang", "Xingyu Chen", "Yue Wang", "Zhiwei He", "Jiahao Xu", "Tian Liang", "Qiuzhi Liu", "Yunzhi Yao", "Wenxuan Wang", "Ruotian Ma", "Haitao Mi", "Ningyu Zhang", "Zhaopeng Tu", "Xiaolong Li", "Dong Yu"], "title": "Two Experts Are All You Need for Steering Thinking: Reinforcing Cognitive Effort in MoE Reasoning Models Without Additional Training", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.IR", "cs.LG"], "comment": "Work in progress", "summary": "Mixture-of-Experts (MoE) architectures within Large Reasoning Models (LRMs)\nhave achieved impressive reasoning capabilities by selectively activating\nexperts to facilitate structured cognitive processes. Despite notable advances,\nexisting reasoning models often suffer from cognitive inefficiencies like\noverthinking and underthinking. To address these limitations, we introduce a\nnovel inference-time steering methodology called Reinforcing Cognitive Experts\n(RICE), designed to improve reasoning performance without additional training\nor complex heuristics. Leveraging normalized Pointwise Mutual Information\n(nPMI), we systematically identify specialized experts, termed ''cognitive\nexperts'' that orchestrate meta-level reasoning operations characterized by\ntokens like ''<think>''. Empirical evaluations with leading MoE-based LRMs\n(DeepSeek-R1 and Qwen3-235B) on rigorous quantitative and scientific reasoning\nbenchmarks demonstrate noticeable and consistent improvements in reasoning\naccuracy, cognitive efficiency, and cross-domain generalization. Crucially, our\nlightweight approach substantially outperforms prevalent reasoning-steering\ntechniques, such as prompt design and decoding constraints, while preserving\nthe model's general instruction-following skills. These results highlight\nreinforcing cognitive experts as a promising, practical, and interpretable\ndirection to enhance cognitive efficiency within advanced reasoning models.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRICE\u7684\u63a8\u7406\u65f6\u95f4\u5f15\u5bfc\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f3a\u5316\u8ba4\u77e5\u4e13\u5bb6\uff08cognitive experts\uff09\u63d0\u5347\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u6027\u80fd\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6216\u590d\u6742\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u63a8\u7406\u6a21\u578b\u5b58\u5728\u8ba4\u77e5\u6548\u7387\u4f4e\u7684\u95ee\u9898\uff08\u5982\u8fc7\u5ea6\u601d\u8003\u6216\u601d\u8003\u4e0d\u8db3\uff09\uff0c\u9700\u8981\u4e00\u79cd\u8f7b\u91cf\u7ea7\u65b9\u6cd5\u63d0\u5347\u63a8\u7406\u6027\u80fd\u3002", "method": "\u5229\u7528\u5f52\u4e00\u5316\u70b9\u95f4\u4e92\u4fe1\u606f\uff08nPMI\uff09\u8bc6\u522b\u5e76\u5f3a\u5316\u8ba4\u77e5\u4e13\u5bb6\uff0c\u8fd9\u4e9b\u4e13\u5bb6\u8d1f\u8d23\u5143\u7ea7\u63a8\u7406\u64cd\u4f5c\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cRICE\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u51c6\u786e\u6027\u3001\u8ba4\u77e5\u6548\u7387\u548c\u8de8\u9886\u57df\u6cdb\u5316\u80fd\u529b\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "RICE\u662f\u4e00\u79cd\u5b9e\u7528\u4e14\u53ef\u89e3\u91ca\u7684\u65b9\u6cd5\uff0c\u80fd\u6709\u6548\u63d0\u5347\u9ad8\u7ea7\u63a8\u7406\u6a21\u578b\u7684\u8ba4\u77e5\u6548\u7387\u3002"}}
{"id": "2505.14530", "pdf": "https://arxiv.org/pdf/2505.14530", "abs": "https://arxiv.org/abs/2505.14530", "authors": ["Zhipeng Yang", "Junzhuo Li", "Siyu Xia", "Xuming Hu"], "title": "Internal Chain-of-Thought: Empirical Evidence for Layer-wise Subtask Scheduling in LLMs", "categories": ["cs.CL", "cs.LG"], "comment": "27 pages, 17 figures", "summary": "We show that large language models (LLMs) exhibit an $\\textit{internal\nchain-of-thought}$: they sequentially decompose and execute composite tasks\nlayer-by-layer. Two claims ground our study: (i) distinct subtasks are learned\nat different network depths, and (ii) these subtasks are executed sequentially\nacross layers. On a benchmark of 15 two-step composite tasks, we employ\nlayer-from context-masking and propose a novel cross-task patching method,\nconfirming (i). To examine claim (ii), we apply LogitLens to decode hidden\nstates, revealing a consistent layerwise execution pattern. We further\nreplicate our analysis on the real-world $\\text{TRACE}$ benchmark, observing\nthe same stepwise dynamics. Together, our results enhance LLMs transparency by\nshowing their capacity to internally plan and execute subtasks (or\ninstructions), opening avenues for fine-grained, instruction-level activation\nsteering.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5177\u6709\u5185\u90e8\u601d\u7ef4\u94fe\uff0c\u80fd\u591f\u9010\u5c42\u5206\u89e3\u548c\u6267\u884c\u590d\u5408\u4efb\u52a1\u3002", "motivation": "\u63a2\u7a76LLMs\u5982\u4f55\u901a\u8fc7\u4e0d\u540c\u7f51\u7edc\u6df1\u5ea6\u5b66\u4e60\u548c\u6267\u884c\u590d\u5408\u4efb\u52a1\u7684\u5b50\u4efb\u52a1\uff0c\u4ee5\u589e\u5f3a\u6a21\u578b\u900f\u660e\u5ea6\u3002", "method": "\u4f7f\u7528\u5c42\u95f4\u4e0a\u4e0b\u6587\u63a9\u7801\u548c\u8de8\u4efb\u52a1\u4fee\u8865\u65b9\u6cd5\u9a8c\u8bc1\u5b50\u4efb\u52a1\u5728\u4e0d\u540c\u6df1\u5ea6\u7684\u5b66\u4e60\uff0c\u5e76\u901a\u8fc7LogitLens\u89e3\u7801\u9690\u85cf\u72b6\u6001\u5206\u6790\u6267\u884c\u6a21\u5f0f\u3002", "result": "\u572815\u4e2a\u4e24\u6b65\u590d\u5408\u4efb\u52a1\u548c\u771f\u5b9eTRACE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u89c2\u5bdf\u5230\u4e00\u81f4\u7684\u9010\u5c42\u6267\u884c\u6a21\u5f0f\u3002", "conclusion": "LLMs\u80fd\u591f\u5185\u90e8\u89c4\u5212\u548c\u6267\u884c\u5b50\u4efb\u52a1\uff0c\u4e3a\u7ec6\u7c92\u5ea6\u6307\u4ee4\u7ea7\u6fc0\u6d3b\u8c03\u63a7\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2505.14536", "pdf": "https://arxiv.org/pdf/2505.14536", "abs": "https://arxiv.org/abs/2505.14536", "authors": ["Agam Goyal", "Vedant Rathi", "William Yeh", "Yian Wang", "Yuen Chen", "Hari Sundaram"], "title": "Breaking Bad Tokens: Detoxification of LLMs Using Sparse Autoencoders", "categories": ["cs.CL"], "comment": "Preprint: 19 pages, 7 figures, 1 table", "summary": "Large language models (LLMs) are now ubiquitous in user-facing applications,\nyet they still generate undesirable toxic outputs, including profanity,\nvulgarity, and derogatory remarks. Although numerous detoxification methods\nexist, most apply broad, surface-level fixes and can therefore easily be\ncircumvented by jailbreak attacks. In this paper we leverage sparse\nautoencoders (SAEs) to identify toxicity-related directions in the residual\nstream of models and perform targeted activation steering using the\ncorresponding decoder vectors. We introduce three tiers of steering\naggressiveness and evaluate them on GPT-2 Small and Gemma-2-2B, revealing\ntrade-offs between toxicity reduction and language fluency. At stronger\nsteering strengths, these causal interventions surpass competitive baselines in\nreducing toxicity by up to 20%, though fluency can degrade noticeably on GPT-2\nSmall depending on the aggressiveness. Crucially, standard NLP benchmark scores\nupon steering remain stable, indicating that the model's knowledge and general\nabilities are preserved. We further show that feature-splitting in wider SAEs\nhampers safety interventions, underscoring the importance of disentangled\nfeature learning. Our findings highlight both the promise and the current\nlimitations of SAE-based causal interventions for LLM detoxification, further\nsuggesting practical guidelines for safer language-model deployment.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff08SAEs\uff09\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bc6\u522b\u6a21\u578b\u6b8b\u5dee\u6d41\u4e2d\u7684\u6bd2\u6027\u76f8\u5173\u65b9\u5411\u5e76\u8fdb\u884c\u9488\u5bf9\u6027\u6fc0\u6d3b\u5f15\u5bfc\uff0c\u4ee5\u51cf\u5c11\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u6bd2\u6027\u8f93\u51fa\u3002", "motivation": "\u5c3d\u7ba1\u5df2\u6709\u591a\u79cd\u53bb\u6bd2\u65b9\u6cd5\uff0c\u4f46\u5927\u591a\u6570\u65b9\u6cd5\u4ec5\u63d0\u4f9b\u8868\u9762\u4fee\u590d\uff0c\u5bb9\u6613\u88ab\u7ed5\u8fc7\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u66f4\u7cbe\u786e\u7684\u5e72\u9884\u51cf\u5c11\u6bd2\u6027\u8f93\u51fa\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u7684\u901a\u7528\u80fd\u529b\u3002", "method": "\u5229\u7528\u7a00\u758f\u81ea\u7f16\u7801\u5668\u8bc6\u522b\u6bd2\u6027\u76f8\u5173\u65b9\u5411\uff0c\u5e76\u901a\u8fc7\u4e0d\u540c\u5f3a\u5ea6\u7684\u6fc0\u6d3b\u5f15\u5bfc\u5e72\u9884\u6a21\u578b\u8f93\u51fa\uff0c\u8bc4\u4f30\u4e86GPT-2 Small\u548cGemma-2-2B\u6a21\u578b\u3002", "result": "\u5728\u8f83\u5f3a\u5f15\u5bfc\u5f3a\u5ea6\u4e0b\uff0c\u6bd2\u6027\u51cf\u5c11\u8fbe20%\uff0c\u4f46\u8bed\u8a00\u6d41\u7545\u6027\u53ef\u80fd\u4e0b\u964d\u3002\u6807\u51c6NLP\u57fa\u51c6\u5206\u6570\u4fdd\u6301\u7a33\u5b9a\uff0c\u8868\u660e\u6a21\u578b\u77e5\u8bc6\u548c\u80fd\u529b\u672a\u53d7\u5f71\u54cd\u3002", "conclusion": "\u57fa\u4e8eSAE\u7684\u56e0\u679c\u5e72\u9884\u5728\u53bb\u6bd2\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u4ecd\u9700\u6539\u8fdb\u7279\u5f81\u89e3\u8026\u4ee5\u63d0\u5347\u6548\u679c\uff0c\u4e3a\u5b89\u5168\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5357\u3002"}}
{"id": "2505.14552", "pdf": "https://arxiv.org/pdf/2505.14552", "abs": "https://arxiv.org/abs/2505.14552", "authors": ["Jiajun Shi", "Jian Yang", "Jiaheng Liu", "Xingyuan Bu", "Jiangjie Chen", "Junting Zhou", "Kaijing Ma", "Zhoufutu Wen", "Bingli Wang", "Yancheng He", "Liang Song", "Hualei Zhu", "Shilong Li", "Xingjian Wang", "Wei Zhang", "Ruibin Yuan", "Yifan Yao", "Wenjun Yang", "Yunli Wang", "Siyuan Fang", "Siyu Yuan", "Qianyu He", "Xiangru Tang", "Yingshui Tan", "Wangchunshu Zhou", "Zhaoxiang Zhang", "Zhoujun Li", "Wenhao Huang", "Ge Zhang"], "title": "KORGym: A Dynamic Game Platform for LLM Reasoning Evaluation", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "22 pages", "summary": "Recent advancements in large language models (LLMs) underscore the need for\nmore comprehensive evaluation methods to accurately assess their reasoning\ncapabilities. Existing benchmarks are often domain-specific and thus cannot\nfully capture an LLM's general reasoning potential. To address this limitation,\nwe introduce the Knowledge Orthogonal Reasoning Gymnasium (KORGym), a dynamic\nevaluation platform inspired by KOR-Bench and Gymnasium. KORGym offers over\nfifty games in either textual or visual formats and supports interactive,\nmulti-turn assessments with reinforcement learning scenarios. Using KORGym, we\nconduct extensive experiments on 19 LLMs and 8 VLMs, revealing consistent\nreasoning patterns within model families and demonstrating the superior\nperformance of closed-source models. Further analysis examines the effects of\nmodality, reasoning strategies, reinforcement learning techniques, and response\nlength on model performance. We expect KORGym to become a valuable resource for\nadvancing LLM reasoning research and developing evaluation methodologies suited\nto complex, interactive environments.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86KORGym\uff0c\u4e00\u4e2a\u52a8\u6001\u8bc4\u4f30\u5e73\u53f0\uff0c\u7528\u4e8e\u5168\u9762\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u5c55\u793a\u4e86\u5c01\u95ed\u6e90\u6a21\u578b\u7684\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u591a\u4e3a\u9886\u57df\u7279\u5b9a\uff0c\u65e0\u6cd5\u5168\u9762\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u7684\u901a\u7528\u63a8\u7406\u80fd\u529b\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u5e73\u53f0\u3002", "method": "\u5f00\u53d1\u4e86KORGym\u5e73\u53f0\uff0c\u5305\u542b50\u591a\u79cd\u6e38\u620f\uff0c\u652f\u6301\u591a\u8f6e\u4ea4\u4e92\u8bc4\u4f30\u548c\u5f3a\u5316\u5b66\u4e60\u573a\u666f\uff0c\u5e76\u5bf919\u4e2aLLM\u548c8\u4e2aVLM\u8fdb\u884c\u4e86\u5b9e\u9a8c\u3002", "result": "\u5b9e\u9a8c\u63ed\u793a\u4e86\u6a21\u578b\u5bb6\u65cf\u5185\u4e00\u81f4\u7684\u63a8\u7406\u6a21\u5f0f\uff0c\u5e76\u663e\u793a\u5c01\u95ed\u6e90\u6a21\u578b\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "KORGym\u6709\u671b\u6210\u4e3a\u63a8\u52a8LLM\u63a8\u7406\u7814\u7a76\u548c\u590d\u6742\u4ea4\u4e92\u73af\u5883\u8bc4\u4f30\u65b9\u6cd5\u53d1\u5c55\u7684\u91cd\u8981\u8d44\u6e90\u3002"}}
{"id": "2505.14553", "pdf": "https://arxiv.org/pdf/2505.14553", "abs": "https://arxiv.org/abs/2505.14553", "authors": ["Abhimanyu Talwar", "Julien Laasri"], "title": "Pivot Language for Low-Resource Machine Translation", "categories": ["cs.CL", "cs.LG", "68T50", "I.2.7"], "comment": "7 pages, 3 figures, paper dated May 13, 2019", "summary": "Certain pairs of languages suffer from lack of a parallel corpus which is\nlarge in size and diverse in domain. One of the ways this is overcome is via\nuse of a pivot language. In this paper we use Hindi as a pivot language to\ntranslate Nepali into English. We describe what makes Hindi a good candidate\nfor the pivot. We discuss ways in which a pivot language can be used, and use\ntwo such approaches - the Transfer Method (fully supervised) and\nBacktranslation (semi-supervised) - to translate Nepali into English. Using the\nformer, we are able to achieve a devtest Set SacreBLEU score of 14.2, which\nimproves the baseline fully supervised score reported by (Guzman et al., 2019)\nby 6.6 points. While we are slightly below the semi-supervised baseline score\nof 15.1, we discuss what may have caused this under-performance, and suggest\nscope for future work.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u4f7f\u7528\u5370\u5730\u8bed\u4f5c\u4e3a\u67a2\u8f74\u8bed\u8a00\u5c06\u5c3c\u6cca\u5c14\u8bed\u7ffb\u8bd1\u4e3a\u82f1\u8bed\u7684\u65b9\u6cd5\uff0c\u6bd4\u8f83\u4e86\u4e24\u79cd\u65b9\u6cd5\uff08\u5b8c\u5168\u76d1\u7763\u7684\u8f6c\u79fb\u65b9\u6cd5\u548c\u534a\u76d1\u7763\u7684\u56de\u8bd1\u65b9\u6cd5\uff09\uff0c\u5e76\u5206\u6790\u4e86\u6027\u80fd\u5dee\u5f02\u53ca\u672a\u6765\u6539\u8fdb\u65b9\u5411\u3002", "motivation": "\u7531\u4e8e\u67d0\u4e9b\u8bed\u8a00\u5bf9\u7f3a\u4e4f\u5927\u89c4\u6a21\u3001\u591a\u9886\u57df\u7684\u5e73\u884c\u8bed\u6599\u5e93\uff0c\u8bba\u6587\u63d0\u51fa\u4f7f\u7528\u67a2\u8f74\u8bed\u8a00\uff08\u5370\u5730\u8bed\uff09\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u5e76\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "method": "\u8bba\u6587\u91c7\u7528\u4e24\u79cd\u65b9\u6cd5\uff1a\u5b8c\u5168\u76d1\u7763\u7684\u8f6c\u79fb\u65b9\u6cd5\u548c\u534a\u76d1\u7763\u7684\u56de\u8bd1\u65b9\u6cd5\uff0c\u5229\u7528\u5370\u5730\u8bed\u4f5c\u4e3a\u67a2\u8f74\u8bed\u8a00\u8fdb\u884c\u5c3c\u6cca\u5c14\u8bed\u5230\u82f1\u8bed\u7684\u7ffb\u8bd1\u3002", "result": "\u8f6c\u79fb\u65b9\u6cd5\u5728\u5f00\u53d1\u6d4b\u8bd5\u96c6\u4e0a\u53d6\u5f97\u4e8614.2\u7684SacreBLEU\u5206\u6570\uff0c\u6bd4\u57fa\u7ebf\u63d0\u9ad8\u4e866.6\u5206\uff0c\u4f46\u7565\u4f4e\u4e8e\u534a\u76d1\u7763\u57fa\u7ebf\u768415.1\u5206\u3002", "conclusion": "\u8bba\u6587\u8ba8\u8bba\u4e86\u6027\u80fd\u5dee\u5f02\u7684\u539f\u56e0\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u6539\u8fdb\u7684\u65b9\u5411\u3002"}}
{"id": "2505.14577", "pdf": "https://arxiv.org/pdf/2505.14577", "abs": "https://arxiv.org/abs/2505.14577", "authors": ["Sohaila Eltanbouly", "Salam Albatarni", "Tamer Elsayed"], "title": "TRATES: Trait-Specific Rubric-Assisted Cross-Prompt Essay Scoring", "categories": ["cs.CL"], "comment": "Accepted at ACL 2025 Findings", "summary": "Research on holistic Automated Essay Scoring (AES) is long-dated; yet, there\nis a notable lack of attention for assessing essays according to individual\ntraits. In this work, we propose TRATES, a novel trait-specific and\nrubric-based cross-prompt AES framework that is generic yet specific to the\nunderlying trait. The framework leverages a Large Language Model (LLM) that\nutilizes the trait grading rubrics to generate trait-specific features\n(represented by assessment questions), then assesses those features given an\nessay. The trait-specific features are eventually combined with generic\nwriting-quality and prompt-specific features to train a simple classical\nregression model that predicts trait scores of essays from an unseen prompt.\nExperiments show that TRATES achieves a new state-of-the-art performance across\nall traits on a widely-used dataset, with the generated LLM-based features\nbeing the most significant.", "AI": {"tldr": "TRATES\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u7279\u8d28\u8bc4\u5206\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u7279\u8d28\u76f8\u5173\u7279\u5f81\u5e76\u7ed3\u5408\u901a\u7528\u7279\u5f81\uff0c\u5b9e\u73b0\u4e86\u8de8\u63d0\u793a\u7684\u81ea\u52a8\u5316\u4f5c\u6587\u8bc4\u5206\u3002", "motivation": "\u73b0\u6709\u81ea\u52a8\u5316\u4f5c\u6587\u8bc4\u5206\uff08AES\uff09\u7814\u7a76\u591a\u5173\u6ce8\u6574\u4f53\u8bc4\u5206\uff0c\u800c\u5ffd\u89c6\u4e86\u5bf9\u4e2a\u4f53\u7279\u8d28\u7684\u8bc4\u4f30\u3002", "method": "\u5229\u7528LLM\u751f\u6210\u7279\u8d28\u76f8\u5173\u7279\u5f81\uff08\u8bc4\u4f30\u95ee\u9898\uff09\uff0c\u7ed3\u5408\u901a\u7528\u548c\u63d0\u793a\u7279\u5b9a\u7279\u5f81\uff0c\u8bad\u7ec3\u56de\u5f52\u6a21\u578b\u9884\u6d4b\u672a\u89c1\u63d0\u793a\u7684\u4f5c\u6587\u7279\u8d28\u5206\u6570\u3002", "result": "TRATES\u5728\u5e7f\u6cdb\u4f7f\u7528\u7684\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6240\u6709\u7279\u8d28\u7684\u6700\u65b0\u6700\u4f18\u6027\u80fd\uff0cLLM\u751f\u6210\u7684\u7279\u5f81\u8d21\u732e\u6700\u5927\u3002", "conclusion": "TRATES\u6846\u67b6\u4e3a\u7279\u8d28\u8bc4\u5206\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0cLLM\u751f\u6210\u7684\u7279\u5f81\u662f\u5173\u952e\u56e0\u7d20\u3002"}}
{"id": "2505.14582", "pdf": "https://arxiv.org/pdf/2505.14582", "abs": "https://arxiv.org/abs/2505.14582", "authors": ["Shangziqi Zhao", "Jiahao Yuan", "Guisong Yang", "Usman Naseem"], "title": "Can Pruning Improve Reasoning? Revisiting Long-CoT Compression with Capability in Mind for Better Reasoning", "categories": ["cs.CL"], "comment": "17 pages,4 figures", "summary": "Long chain-of-thought (Long-CoT) reasoning improves accuracy in LLMs, yet its\nverbose, self-reflective style often hinders effective distillation into small\nlanguage models (SLMs). We revisit Long-CoT compression through the lens of\ncapability alignment and ask: Can pruning improve reasoning? We propose\nPrune-on-Logic, a structure-aware framework that transforms Long-CoT into logic\ngraphs and selectively prunes low-utility reasoning steps under\nself-verification constraints. Through systematic analysis across three pruning\nstrategies -- targeting entire chains, core reasoning, and verification -- we\nfind that pruning verification steps yields consistent accuracy gains while\nreducing inference cost, outperforming token-level baselines and uncompressed\nfine-tuning. In contrast, pruning reasoning or all-chain steps degrades\nperformance, revealing that small models benefit not from shorter CoTs, but\nfrom semantically leaner ones. Our findings highlight pruning as a structural\noptimization strategy for aligning CoT reasoning with SLM capacity.", "AI": {"tldr": "Prune-on-Logic\u6846\u67b6\u901a\u8fc7\u903b\u8f91\u56fe\u9009\u62e9\u6027\u4fee\u526aLong-CoT\u7684\u4f4e\u6548\u63a8\u7406\u6b65\u9aa4\uff0c\u9a8c\u8bc1\u6b65\u9aa4\u4fee\u526a\u63d0\u5347\u5c0f\u6a21\u578b\u63a8\u7406\u7cbe\u5ea6\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u4fee\u526a\u4f18\u5316Long-CoT\u63a8\u7406\uff0c\u4f7f\u5176\u66f4\u9002\u5408\u5c0f\u8bed\u8a00\u6a21\u578b\u3002", "method": "\u63d0\u51faPrune-on-Logic\u6846\u67b6\uff0c\u5c06Long-CoT\u8f6c\u4e3a\u903b\u8f91\u56fe\u5e76\u9009\u62e9\u6027\u4fee\u526a\u4f4e\u6548\u6b65\u9aa4\u3002", "result": "\u9a8c\u8bc1\u6b65\u9aa4\u4fee\u526a\u663e\u8457\u63d0\u5347\u7cbe\u5ea6\u5e76\u964d\u4f4e\u63a8\u7406\u6210\u672c\uff0c\u800c\u5176\u4ed6\u4fee\u526a\u7b56\u7565\u6548\u679c\u4e0d\u4f73\u3002", "conclusion": "\u4fee\u526a\u662f\u4f18\u5316CoT\u63a8\u7406\u7ed3\u6784\u4ee5\u9002\u914d\u5c0f\u6a21\u578b\u80fd\u529b\u7684\u6709\u6548\u7b56\u7565\u3002"}}
{"id": "2505.14585", "pdf": "https://arxiv.org/pdf/2505.14585", "abs": "https://arxiv.org/abs/2505.14585", "authors": ["Wenbin Hu", "Haoran Li", "Huihao Jing", "Qi Hu", "Ziqian Zeng", "Sirui Han", "Heli Xu", "Tianshu Chu", "Peizhao Hu", "Yangqiu Song"], "title": "Context Reasoner: Incentivizing Reasoning Capability for Contextualized Privacy and Safety Compliance via Reinforcement Learning", "categories": ["cs.CL"], "comment": null, "summary": "While Large Language Models (LLMs) exhibit remarkable capabilities, they also\nintroduce significant safety and privacy risks. Current mitigation strategies\noften fail to preserve contextual reasoning capabilities in risky scenarios.\nInstead, they rely heavily on sensitive pattern matching to protect LLMs, which\nlimits the scope. Furthermore, they overlook established safety and privacy\nstandards, leading to systemic risks for legal compliance. To address these\ngaps, we formulate safety and privacy issues into contextualized compliance\nproblems following the Contextual Integrity (CI) theory. Under the CI\nframework, we align our model with three critical regulatory standards: GDPR,\nEU AI Act, and HIPAA. Specifically, we employ reinforcement learning (RL) with\na rule-based reward to incentivize contextual reasoning capabilities while\nenhancing compliance with safety and privacy norms. Through extensive\nexperiments, we demonstrate that our method not only significantly enhances\nlegal compliance (achieving a +17.64% accuracy improvement in safety/privacy\nbenchmarks) but also further improves general reasoning capability. For\nOpenThinker-7B, a strong reasoning model that significantly outperforms its\nbase model Qwen2.5-7B-Instruct across diverse subjects, our method enhances its\ngeneral reasoning capabilities, with +2.05% and +8.98% accuracy improvement on\nthe MMLU and LegalBench benchmark, respectively.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e0a\u4e0b\u6587\u5b8c\u6574\u6027\u7406\u8bba\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5b89\u5168\u548c\u9690\u79c1\u98ce\u9669\uff0c\u540c\u65f6\u63d0\u5347\u5408\u89c4\u6027\u548c\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5b89\u5168\u548c\u9690\u79c1\u7f13\u89e3\u7b56\u7565\u4f9d\u8d56\u654f\u611f\u6a21\u5f0f\u5339\u914d\uff0c\u5ffd\u7565\u4e86\u5408\u89c4\u6807\u51c6\uff0c\u5bfc\u81f4\u7cfb\u7edf\u6027\u98ce\u9669\u3002", "method": "\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u7ed3\u5408\u89c4\u5219\u5956\u52b1\uff0c\u5c06\u5b89\u5168\u548c\u9690\u79c1\u95ee\u9898\u8f6c\u5316\u4e3a\u4e0a\u4e0b\u6587\u5408\u89c4\u95ee\u9898\uff0c\u5e76\u9075\u5faaGDPR\u3001EU AI Act\u548cHIPAA\u6807\u51c6\u3002", "result": "\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6cd5\u5f8b\u5408\u89c4\u6027\uff08\u5b89\u5168\u548c\u9690\u79c1\u57fa\u51c6\u51c6\u786e\u7387\u63d0\u534717.64%\uff09\uff0c\u5e76\u589e\u5f3a\u4e86\u901a\u7528\u63a8\u7406\u80fd\u529b\uff08MMLU\u548cLegalBench\u57fa\u51c6\u5206\u522b\u63d0\u53472.05%\u548c8.98%\uff09\u3002", "conclusion": "\u901a\u8fc7\u4e0a\u4e0b\u6587\u5b8c\u6574\u6027\u6846\u67b6\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u8bba\u6587\u65b9\u6cd5\u6709\u6548\u5e73\u8861\u4e86\u5408\u89c4\u6027\u548c\u63a8\u7406\u80fd\u529b\uff0c\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5b89\u5168\u548c\u9690\u79c1\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.14590", "pdf": "https://arxiv.org/pdf/2505.14590", "abs": "https://arxiv.org/abs/2505.14590", "authors": ["Huihao Jing", "Haoran Li", "Wenbin Hu", "Qi Hu", "Heli Xu", "Tianshu Chu", "Peizhao Hu", "Yangqiu Song"], "title": "MCIP: Protecting MCP Safety via Model Contextual Integrity Protocol", "categories": ["cs.CL"], "comment": "17 pages", "summary": "As Model Context Protocol (MCP) introduces an easy-to-use ecosystem for users\nand developers, it also brings underexplored safety risks. Its decentralized\narchitecture, which separates clients and servers, poses unique challenges for\nsystematic safety analysis. This paper proposes a novel framework to enhance\nMCP safety. Guided by the MAESTRO framework, we first analyze the missing\nsafety mechanisms in MCP, and based on this analysis, we propose the Model\nContextual Integrity Protocol (MCIP), a refined version of MCP that addresses\nthese gaps.Next, we develop a fine-grained taxonomy that captures a diverse\nrange of unsafe behaviors observed in MCP scenarios. Building on this taxonomy,\nwe develop benchmark and training data that support the evaluation and\nimprovement of LLMs' capabilities in identifying safety risks within MCP\ninteractions. Leveraging the proposed benchmark and training data, we conduct\nextensive experiments on state-of-the-art LLMs. The results highlight LLMs'\nvulnerabilities in MCP interactions and demonstrate that our approach\nsubstantially improves their safety performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u6a21\u578b\u4e0a\u4e0b\u6587\u534f\u8bae\uff08MCP\uff09\u5b89\u5168\u6027\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u6790MCP\u7684\u5b89\u5168\u7f3a\u9677\u5e76\u5f00\u53d1MCIP\u534f\u8bae\uff0c\u8fdb\u4e00\u6b65\u6784\u5efa\u4e86\u7ec6\u7c92\u5ea6\u7684\u5206\u7c7b\u6cd5\u548c\u57fa\u51c6\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u4e86LLMs\u5728MCP\u4ea4\u4e92\u4e2d\u7684\u5b89\u5168\u6027\u8868\u73b0\u3002", "motivation": "MCP\u7684\u5206\u6563\u5f0f\u67b6\u6784\u5e26\u6765\u4e86\u672a\u88ab\u5145\u5206\u63a2\u7d22\u7684\u5b89\u5168\u98ce\u9669\uff0c\u9700\u8981\u7cfb\u7edf\u6027\u5206\u6790\u4ee5\u63d0\u5347\u5b89\u5168\u6027\u3002", "method": "\u57fa\u4e8eMAESTRO\u6846\u67b6\u5206\u6790MCP\u7684\u5b89\u5168\u7f3a\u9677\uff0c\u63d0\u51faMCIP\u534f\u8bae\uff0c\u5e76\u5f00\u53d1\u5206\u7c7b\u6cd5\u3001\u57fa\u51c6\u6570\u636e\u548c\u8bad\u7ec3\u6570\u636e\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLLMs\u5728MCP\u4ea4\u4e92\u4e2d\u5b58\u5728\u6f0f\u6d1e\uff0c\u800c\u63d0\u51fa\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u5176\u5b89\u5168\u6027\u3002", "conclusion": "MCIP\u534f\u8bae\u53ca\u76f8\u5173\u5de5\u5177\u80fd\u6709\u6548\u63d0\u5347MCP\u7684\u5b89\u5168\u6027\uff0c\u4e3aLLMs\u7684\u5b89\u5168\u4ea4\u4e92\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.14597", "pdf": "https://arxiv.org/pdf/2505.14597", "abs": "https://arxiv.org/abs/2505.14597", "authors": ["Xianzhen Luo", "Qingfu Zhu", "Zhiming Zhang", "Mingzheng Xu", "Tianhao Cheng", "Yixuan Wang", "Zheng Chu", "Shijie Xuyang", "Zhiyuan Ma", "YuanTao Fan", "Wanxiang Che"], "title": "Success is in the Details: Evaluate and Enhance Details Sensitivity of Code LLMs through Counterfactuals", "categories": ["cs.CL"], "comment": "Code & Model is https://github.com/Luowaterbi/CTF-Instruct", "summary": "Code Sensitivity refers to the ability of Code LLMs to recognize and respond\nto details changes in problem descriptions. While current code benchmarks and\ninstruction data focus on difficulty and diversity, sensitivity is overlooked.\nWe first introduce the CTF-Code benchmark, constructed using counterfactual\nperturbations, minimizing input changes while maximizing output changes. The\nevaluation shows that many LLMs have a more than 10\\% performance drop compared\nto the original problems. To fully utilize sensitivity, CTF-Instruct, an\nincremental instruction fine-tuning framework, extends on existing data and\nuses a selection mechanism to meet the three dimensions of difficulty,\ndiversity, and sensitivity. Experiments show that LLMs fine-tuned with\nCTF-Instruct data achieve over a 2\\% improvement on CTF-Code, and more than a\n10\\% performance boost on LiveCodeBench, validating the feasibility of\nenhancing LLMs' sensitivity to improve performance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4ee3\u7801\u654f\u611f\u6027\u6982\u5ff5\uff0c\u5e76\u901a\u8fc7CTF-Code\u57fa\u51c6\u548cCTF-Instruct\u5fae\u8c03\u6846\u67b6\u63d0\u5347LLMs\u7684\u654f\u611f\u6027\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u4ee3\u7801\u57fa\u51c6\u548c\u6307\u4ee4\u6570\u636e\u5ffd\u89c6\u4e86\u4ee3\u7801\u654f\u611f\u6027\uff0c\u5bfc\u81f4LLMs\u5bf9\u95ee\u9898\u63cf\u8ff0\u7684\u7ec6\u8282\u53d8\u5316\u54cd\u5e94\u4e0d\u8db3\u3002", "method": "\u5f15\u5165CTF-Code\u57fa\u51c6\u548cCTF-Instruct\u5fae\u8c03\u6846\u67b6\uff0c\u901a\u8fc7\u53cd\u4e8b\u5b9e\u6270\u52a8\u548c\u589e\u91cf\u6307\u4ee4\u4f18\u5316LLMs\u7684\u654f\u611f\u6027\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u5fae\u8c03\u540e\u7684LLMs\u5728CTF-Code\u4e0a\u63d0\u53472%\uff0c\u5728LiveCodeBench\u4e0a\u63d0\u534710%\u3002", "conclusion": "\u589e\u5f3aLLMs\u7684\u4ee3\u7801\u654f\u611f\u6027\u53ef\u663e\u8457\u63d0\u5347\u5176\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2505.14599", "pdf": "https://arxiv.org/pdf/2505.14599", "abs": "https://arxiv.org/abs/2505.14599", "authors": ["Guangzhi Xiong", "Eric Xie", "Corey Williams", "Myles Kim", "Amir Hassan Shariatmadari", "Sikun Guo", "Stefan Bekiranov", "Aidong Zhang"], "title": "Toward Reliable Biomedical Hypothesis Generation: Evaluating Truthfulness and Hallucination in Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to IJCAI 2025", "summary": "Large language models (LLMs) have shown significant potential in scientific\ndisciplines such as biomedicine, particularly in hypothesis generation, where\nthey can analyze vast literature, identify patterns, and suggest research\ndirections. However, a key challenge lies in evaluating the truthfulness of\ngenerated hypotheses, as verifying their accuracy often requires substantial\ntime and resources. Additionally, the hallucination problem in LLMs can lead to\nthe generation of hypotheses that appear plausible but are ultimately\nincorrect, undermining their reliability. To facilitate the systematic study of\nthese challenges, we introduce TruthHypo, a benchmark for assessing the\ncapabilities of LLMs in generating truthful biomedical hypotheses, and KnowHD,\na knowledge-based hallucination detector to evaluate how well hypotheses are\ngrounded in existing knowledge. Our results show that LLMs struggle to generate\ntruthful hypotheses. By analyzing hallucinations in reasoning steps, we\ndemonstrate that the groundedness scores provided by KnowHD serve as an\neffective metric for filtering truthful hypotheses from the diverse outputs of\nLLMs. Human evaluations further validate the utility of KnowHD in identifying\ntruthful hypotheses and accelerating scientific discovery. Our data and source\ncode are available at https://github.com/Teddy-XiongGZ/TruthHypo.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86TruthHypo\u57fa\u51c6\u548cKnowHD\u68c0\u6d4b\u5668\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u751f\u6210\u771f\u5b9e\u751f\u7269\u533b\u5b66\u5047\u8bbe\u7684\u80fd\u529b\uff0c\u5e76\u89e3\u51b3\u5e7b\u89c9\u95ee\u9898\u3002", "motivation": "LLM\u5728\u751f\u7269\u533b\u5b66\u9886\u57df\u6709\u6f5c\u529b\u751f\u6210\u5047\u8bbe\uff0c\u4f46\u5176\u771f\u5b9e\u6027\u548c\u53ef\u9760\u6027\u56e0\u5e7b\u89c9\u95ee\u9898\u800c\u53d7\u9650\uff0c\u9700\u7cfb\u7edf\u8bc4\u4f30\u3002", "method": "\u5f15\u5165TruthHypo\u57fa\u51c6\u548cKnowHD\u77e5\u8bc6\u57fa\u7840\u5e7b\u89c9\u68c0\u6d4b\u5668\uff0c\u5206\u6790LLM\u751f\u6210\u5047\u8bbe\u7684\u771f\u5b9e\u6027\u3002", "result": "LLM\u96be\u4ee5\u751f\u6210\u771f\u5b9e\u5047\u8bbe\uff0cKnowHD\u7684groundedness\u8bc4\u5206\u80fd\u6709\u6548\u7b5b\u9009\u771f\u5b9e\u5047\u8bbe\uff0c\u4eba\u7c7b\u8bc4\u4f30\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "conclusion": "TruthHypo\u548cKnowHD\u4e3a\u8bc4\u4f30LLM\u751f\u6210\u5047\u8bbe\u7684\u771f\u5b9e\u6027\u63d0\u4f9b\u4e86\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u52a0\u901f\u79d1\u5b66\u53d1\u73b0\u3002"}}
{"id": "2505.14607", "pdf": "https://arxiv.org/pdf/2505.14607", "abs": "https://arxiv.org/abs/2505.14607", "authors": ["Soumadeep Saha", "Akshay Chaturvedi", "Joy Mahapatra", "Utpal Garain"], "title": "sudoLLM : On Multi-role Alignment of Language Models", "categories": ["cs.CL", "cs.CR", "I.2.7"], "comment": "Under review. Code and data to be released later", "summary": "User authorization-based access privileges are a key feature in many\nsafety-critical systems, but have thus far been absent from the large language\nmodel (LLM) realm. In this work, drawing inspiration from such access control\nsystems, we introduce sudoLLM, a novel framework that results in multi-role\naligned LLMs, i.e., LLMs that account for, and behave in accordance with, user\naccess rights. sudoLLM injects subtle user-based biases into queries and trains\nan LLM to utilize this bias signal in order to produce sensitive information if\nand only if the user is authorized. We present empirical results demonstrating\nthat this approach shows substantially improved alignment, generalization, and\nresistance to prompt-based jailbreaking attacks. The persistent tension between\nthe language modeling objective and safety alignment, which is often exploited\nto jailbreak LLMs, is somewhat resolved with the aid of the injected bias\nsignal. Our framework is meant as an additional security layer, and complements\nexisting guardrail mechanisms for enhanced end-to-end safety with LLMs.", "AI": {"tldr": "sudoLLM\u662f\u4e00\u4e2a\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u7528\u6237\u6388\u6743\u673a\u5236\u5b9e\u73b0\u591a\u89d2\u8272\u5bf9\u9f50\u7684LLM\uff0c\u63d0\u5347\u5b89\u5168\u6027\u548c\u6297\u653b\u51fb\u80fd\u529b\u3002", "motivation": "\u73b0\u6709LLM\u7f3a\u4e4f\u7528\u6237\u6388\u6743\u8bbf\u95ee\u63a7\u5236\uff0c\u5bfc\u81f4\u5b89\u5168\u98ce\u9669\u3002", "method": "\u5728\u67e5\u8be2\u4e2d\u6ce8\u5165\u7528\u6237\u504f\u7f6e\u4fe1\u53f7\uff0c\u8bad\u7ec3LLM\u6839\u636e\u6388\u6743\u751f\u6210\u654f\u611f\u4fe1\u606f\u3002", "result": "\u5b9e\u9a8c\u663e\u793asudoLLM\u663e\u8457\u63d0\u5347\u5bf9\u9f50\u6027\u3001\u6cdb\u5316\u80fd\u529b\u548c\u6297\u653b\u51fb\u6027\u3002", "conclusion": "sudoLLM\u4f5c\u4e3a\u989d\u5916\u5b89\u5168\u5c42\uff0c\u8865\u5145\u73b0\u6709\u9632\u62a4\u673a\u5236\uff0c\u589e\u5f3aLLM\u7aef\u5230\u7aef\u5b89\u5168\u6027\u3002"}}
{"id": "2505.14608", "pdf": "https://arxiv.org/pdf/2505.14608", "abs": "https://arxiv.org/abs/2505.14608", "authors": ["Rafael Rivera Soto", "Barry Chen", "Nicholas Andrews"], "title": "Language Models Optimized to Fool Detectors Still Have a Distinct Style (And How to Change It)", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Despite considerable progress in the development of machine-text detectors,\nit has been suggested that the problem is inherently hard, and therefore, that\nstakeholders should proceed under the assumption that machine-generated text\ncannot be reliably detected as such. We examine a recent such claim by Nicks et\nal. (2024) regarding the ease with which language models can be optimized to\ndegrade the performance of machine-text detectors, including detectors not\nspecifically optimized against. We identify a feature space$\\unicode{x2013}$the\nstylistic feature space$\\unicode{x2013}$that is robust to such optimization,\nand show that it may be used to reliably detect samples from language models\noptimized to prevent detection. Furthermore, we show that even when models are\nexplicitly optimized against stylistic detectors, detection performance remains\nsurprisingly unaffected. We then seek to understand if stylistic detectors are\ninherently more robust. To study this question, we explore a new paraphrasing\napproach that simultaneously aims to close the gap between human writing and\nmachine writing in stylistic feature space while avoiding detection using\ntraditional features. We show that when only a single sample is available for\ndetection, this attack is universally effective across all detectors\nconsidered, including those that use writing style. However, as the number of\nsamples available for detection grows, the human and machine distributions\nbecome distinguishable. This observation encourages us to introduce AURA, a\nmetric that estimates the overlap between human and machine-generated\ndistributions by analyzing how detector performance improves as more samples\nbecome available. Overall, our findings underscore previous recommendations to\navoid reliance on machine-text detection.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u673a\u5668\u751f\u6210\u6587\u672c\u68c0\u6d4b\u7684\u56f0\u96be\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u98ce\u683c\u7279\u5f81\u7a7a\u95f4\u7684\u68c0\u6d4b\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5bf9\u6a21\u578b\u4f18\u5316\u5177\u6709\u9c81\u68d2\u6027\u3002\u540c\u65f6\uff0c\u8bba\u6587\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u5ea6\u91cf\u6807\u51c6AURA\uff0c\u7528\u4e8e\u8bc4\u4f30\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u89e3\u51b3\u673a\u5668\u751f\u6210\u6587\u672c\u68c0\u6d4b\u7684\u56f0\u96be\u6027\uff0c\u5c24\u5176\u662f\u9488\u5bf9\u4f18\u5316\u540e\u7684\u8bed\u8a00\u6a21\u578b\u5bf9\u68c0\u6d4b\u5668\u7684\u6027\u80fd\u5f71\u54cd\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u98ce\u683c\u7279\u5f81\u7a7a\u95f4\u7684\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5e76\u63a2\u7d22\u4e86\u4e00\u79cd\u65b0\u7684\u6539\u5199\u653b\u51fb\u65b9\u6cd5\u3002\u540c\u65f6\uff0c\u5f15\u5165\u4e86AURA\u5ea6\u91cf\u6807\u51c6\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u98ce\u683c\u7279\u5f81\u7a7a\u95f4\u5bf9\u4f18\u5316\u540e\u7684\u6a21\u578b\u5177\u6709\u9c81\u68d2\u6027\uff0c\u4f46\u5355\u6837\u672c\u68c0\u6d4b\u65f6\u653b\u51fb\u4ecd\u7136\u6709\u6548\u3002\u968f\u7740\u6837\u672c\u91cf\u589e\u52a0\uff0c\u68c0\u6d4b\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u7ed3\u8bba\u5f3a\u8c03\u4e86\u907f\u514d\u4f9d\u8d56\u673a\u5668\u6587\u672c\u68c0\u6d4b\u7684\u5efa\u8bae\uff0c\u5e76\u63d0\u51fa\u4e86AURA\u4f5c\u4e3a\u8bc4\u4f30\u5de5\u5177\u3002"}}
{"id": "2505.14617", "pdf": "https://arxiv.org/pdf/2505.14617", "abs": "https://arxiv.org/abs/2505.14617", "authors": ["Sahar Abdelnabi", "Ahmed Salem"], "title": "Linear Control of Test Awareness Reveals Differential Compliance in Reasoning Models", "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "Reasoning-focused large language models (LLMs) sometimes alter their behavior\nwhen they detect that they are being evaluated, an effect analogous to the\nHawthorne phenomenon, which can lead them to optimize for test-passing\nperformance or to comply more readily with harmful prompts if real-world\nconsequences appear absent. We present the first quantitative study of how such\n\"test awareness\" impacts model behavior, particularly its safety alignment. We\nintroduce a white-box probing framework that (i) linearly identifies\nawareness-related activations and (ii) steers models toward or away from test\nawareness while monitoring downstream performance. We apply our method to\ndifferent state-of-the-art open-source reasoning LLMs across both realistic and\nhypothetical tasks. Our results demonstrate that test awareness significantly\nimpact safety alignment, and is different for different models. By providing\nfine-grained control over this latent effect, our work aims to increase trust\nin how we perform safety evaluation.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u610f\u8bc6\u5230\u88ab\u8bc4\u4f30\u65f6\u4f1a\u6539\u53d8\u884c\u4e3a\uff0c\u7c7b\u4f3c\u970d\u6851\u6548\u5e94\uff0c\u5f71\u54cd\u5176\u5b89\u5168\u5bf9\u9f50\u3002\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u767d\u76d2\u63a2\u6d4b\u6846\u67b6\uff0c\u91cf\u5316\u4e86\u8fd9\u79cd\u201c\u6d4b\u8bd5\u610f\u8bc6\u201d\u5bf9\u6a21\u578b\u884c\u4e3a\u7684\u5f71\u54cd\uff0c\u5e76\u5c55\u793a\u4e86\u4e0d\u540c\u6a21\u578b\u95f4\u7684\u5dee\u5f02\u3002", "motivation": "\u63a2\u8ba8LLMs\u5728\u88ab\u8bc4\u4f30\u65f6\u7684\u884c\u4e3a\u53d8\u5316\uff08\u7c7b\u4f3c\u970d\u6851\u6548\u5e94\uff09\u5982\u4f55\u5f71\u54cd\u5176\u5b89\u5168\u5bf9\u9f50\uff0c\u4ee5\u63d0\u5347\u5b89\u5168\u8bc4\u4f30\u7684\u53ef\u4fe1\u5ea6\u3002", "method": "\u63d0\u51fa\u767d\u76d2\u63a2\u6d4b\u6846\u67b6\uff0c\u7ebf\u6027\u8bc6\u522b\u4e0e\u6d4b\u8bd5\u610f\u8bc6\u76f8\u5173\u7684\u6fc0\u6d3b\uff0c\u5e76\u901a\u8fc7\u8c03\u63a7\u8fd9\u4e9b\u6fc0\u6d3b\u6765\u89c2\u5bdf\u6a21\u578b\u884c\u4e3a\u53d8\u5316\u3002", "result": "\u6d4b\u8bd5\u610f\u8bc6\u663e\u8457\u5f71\u54cd\u6a21\u578b\u7684\u5b89\u5168\u5bf9\u9f50\uff0c\u4e14\u4e0d\u540c\u6a21\u578b\u8868\u73b0\u4e0d\u540c\u3002", "conclusion": "\u901a\u8fc7\u91cf\u5316\u6d4b\u8bd5\u610f\u8bc6\u7684\u5f71\u54cd\u5e76\u63d0\u4f9b\u8c03\u63a7\u65b9\u6cd5\uff0c\u7814\u7a76\u65e8\u5728\u589e\u5f3a\u5b89\u5168\u8bc4\u4f30\u7684\u53ef\u4fe1\u5ea6\u3002"}}
{"id": "2505.14631", "pdf": "https://arxiv.org/pdf/2505.14631", "abs": "https://arxiv.org/abs/2505.14631", "authors": ["Lingjie Jiang", "Xun Wu", "Shaohan Huang", "Qingxiu Dong", "Zewen Chi", "Li Dong", "Xingxing Zhang", "Tengchao Lv", "Lei Cui", "Furu Wei"], "title": "Think Only When You Need with Large Hybrid-Reasoning Models", "categories": ["cs.CL"], "comment": null, "summary": "Recent Large Reasoning Models (LRMs) have shown substantially improved\nreasoning capabilities over traditional Large Language Models (LLMs) by\nincorporating extended thinking processes prior to producing final responses.\nHowever, excessively lengthy thinking introduces substantial overhead in terms\nof token consumption and latency, which is particularly unnecessary for simple\nqueries. In this work, we introduce Large Hybrid-Reasoning Models (LHRMs), the\nfirst kind of model capable of adaptively determining whether to perform\nthinking based on the contextual information of user queries. To achieve this,\nwe propose a two-stage training pipeline comprising Hybrid Fine-Tuning (HFT) as\na cold start, followed by online reinforcement learning with the proposed\nHybrid Group Policy Optimization (HGPO) to implicitly learn to select the\nappropriate thinking mode. Furthermore, we introduce a metric called Hybrid\nAccuracy to quantitatively assess the model's capability for hybrid thinking.\nExtensive experimental results show that LHRMs can adaptively perform hybrid\nthinking on queries of varying difficulty and type. It outperforms existing\nLRMs and LLMs in reasoning and general capabilities while significantly\nimproving efficiency. Together, our work advocates for a reconsideration of the\nappropriate use of extended thinking processes and provides a solid starting\npoint for building hybrid thinking systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684Large Hybrid-Reasoning Models\uff08LHRMs\uff09\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u5730\u51b3\u5b9a\u662f\u5426\u8fdb\u884c\u601d\u8003\u6765\u4f18\u5316\u63a8\u7406\u6548\u7387\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u4e0d\u5fc5\u8981\u7684\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u73b0\u6709\u7684Large Reasoning Models\uff08LRMs\uff09\u5728\u5904\u7406\u7b80\u5355\u67e5\u8be2\u65f6\uff0c\u8fc7\u957f\u7684\u601d\u8003\u8fc7\u7a0b\u4f1a\u5e26\u6765\u4e0d\u5fc5\u8981\u7684\u8ba1\u7b97\u5f00\u9500\u548c\u5ef6\u8fdf\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u6d41\u7a0b\uff1aHybrid Fine-Tuning\uff08HFT\uff09\u4f5c\u4e3a\u51b7\u542f\u52a8\uff0c\u968f\u540e\u901a\u8fc7\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\uff08Hybrid Group Policy Optimization, HGPO\uff09\u9690\u5f0f\u5b66\u4e60\u9009\u62e9\u9002\u5f53\u7684\u601d\u8003\u6a21\u5f0f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLHRMs\u80fd\u591f\u81ea\u9002\u5e94\u5730\u5904\u7406\u4e0d\u540c\u96be\u5ea6\u548c\u7c7b\u578b\u7684\u67e5\u8be2\uff0c\u5728\u63a8\u7406\u548c\u901a\u7528\u80fd\u529b\u4e0a\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u540c\u65f6\u663e\u8457\u63d0\u5347\u6548\u7387\u3002", "conclusion": "LHRMs\u4e3a\u6df7\u5408\u601d\u8003\u7cfb\u7edf\u7684\u6784\u5efa\u63d0\u4f9b\u4e86\u575a\u5b9e\u57fa\u7840\uff0c\u5e76\u91cd\u65b0\u5ba1\u89c6\u4e86\u6269\u5c55\u601d\u8003\u8fc7\u7a0b\u7684\u9002\u7528\u6027\u3002"}}
{"id": "2505.14633", "pdf": "https://arxiv.org/pdf/2505.14633", "abs": "https://arxiv.org/abs/2505.14633", "authors": ["Yu Ying Chiu", "Zhilin Wang", "Sharan Maiya", "Yejin Choi", "Kyle Fish", "Sydney Levine", "Evan Hubinger"], "title": "Will AI Tell Lies to Save Sick Children? Litmus-Testing AI Values Prioritization with AIRiskDilemmas", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "cs.LG"], "comment": "34 pages, 11 figures, see associated data at\n  https://huggingface.co/datasets/kellycyy/AIRiskDilemmas and code at\n  https://github.com/kellycyy/LitmusValues", "summary": "Detecting AI risks becomes more challenging as stronger models emerge and\nfind novel methods such as Alignment Faking to circumvent these detection\nattempts. Inspired by how risky behaviors in humans (i.e., illegal activities\nthat may hurt others) are sometimes guided by strongly-held values, we believe\nthat identifying values within AI models can be an early warning system for\nAI's risky behaviors. We create LitmusValues, an evaluation pipeline to reveal\nAI models' priorities on a range of AI value classes. Then, we collect\nAIRiskDilemmas, a diverse collection of dilemmas that pit values against one\nanother in scenarios relevant to AI safety risks such as Power Seeking. By\nmeasuring an AI model's value prioritization using its aggregate choices, we\nobtain a self-consistent set of predicted value priorities that uncover\npotential risks. We show that values in LitmusValues (including seemingly\ninnocuous ones like Care) can predict for both seen risky behaviors in\nAIRiskDilemmas and unseen risky behaviors in HarmBench.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faLitmusValues\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bc4\u4f30AI\u6a21\u578b\u7684\u4ef7\u503c\u4f18\u5148\u7ea7\u6765\u9884\u6d4b\u5176\u6f5c\u5728\u98ce\u9669\u884c\u4e3a\u3002", "motivation": "\u968f\u7740AI\u6a21\u578b\u80fd\u529b\u589e\u5f3a\uff0c\u68c0\u6d4b\u5176\u98ce\u9669\u884c\u4e3a\uff08\u5982Alignment Faking\uff09\u53d8\u5f97\u66f4\u56f0\u96be\u3002\u53d7\u4eba\u7c7b\u98ce\u9669\u884c\u4e3a\uff08\u5982\u975e\u6cd5\u6d3b\u52a8\uff09\u5e38\u53d7\u4ef7\u503c\u89c2\u9a71\u52a8\u7684\u542f\u53d1\uff0c\u7814\u7a76\u8ba4\u4e3a\u8bc6\u522bAI\u6a21\u578b\u7684\u4ef7\u503c\u53ef\u4f5c\u4e3a\u98ce\u9669\u9884\u8b66\u3002", "method": "\u521b\u5efaLitmusValues\u8bc4\u4f30\u6d41\u7a0b\uff0c\u63ed\u793aAI\u6a21\u578b\u5728\u591a\u79cd\u4ef7\u503c\u7c7b\u522b\u4e2d\u7684\u4f18\u5148\u7ea7\uff1b\u6536\u96c6AIRiskDilemmas\u6570\u636e\u96c6\uff0c\u6a21\u62dfAI\u5b89\u5168\u98ce\u9669\u573a\u666f\u4e2d\u7684\u4ef7\u503c\u51b2\u7a81\u3002\u901a\u8fc7\u6a21\u578b\u7684\u9009\u62e9\u884c\u4e3a\u9884\u6d4b\u5176\u4ef7\u503c\u4f18\u5148\u7ea7\u3002", "result": "LitmusValues\u4e2d\u7684\u4ef7\u503c\uff08\u5982Care\uff09\u80fd\u9884\u6d4bAIRiskDilemmas\u4e2d\u7684\u5df2\u77e5\u98ce\u9669\u884c\u4e3a\u53caHarmBench\u4e2d\u7684\u672a\u77e5\u98ce\u9669\u884c\u4e3a\u3002", "conclusion": "\u8bc6\u522bAI\u6a21\u578b\u7684\u4ef7\u503c\u4f18\u5148\u7ea7\u662f\u9884\u6d4b\u5176\u98ce\u9669\u884c\u4e3a\u7684\u6709\u6548\u65b9\u6cd5\uff0cLitmusValues\u4e3aAI\u5b89\u5168\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002"}}
{"id": "2505.14652", "pdf": "https://arxiv.org/pdf/2505.14652", "abs": "https://arxiv.org/abs/2505.14652", "authors": ["Xueguang Ma", "Qian Liu", "Dongfu Jiang", "Ge Zhang", "Zejun Ma", "Wenhu Chen"], "title": "General-Reasoner: Advancing LLM Reasoning Across All Domains", "categories": ["cs.CL"], "comment": null, "summary": "Reinforcement learning (RL) has recently demonstrated strong potential in\nenhancing the reasoning capabilities of large language models (LLMs).\nParticularly, the \"Zero\" reinforcement learning introduced by Deepseek-R1-Zero,\nenables direct RL training of base LLMs without relying on an intermediate\nsupervised fine-tuning stage. Despite these advancements, current works for LLM\nreasoning mainly focus on mathematical and coding domains, largely due to data\nabundance and the ease of answer verification. This limits the applicability\nand generalization of such models to broader domains, where questions often\nhave diverse answer representations, and data is more scarce. In this paper, we\npropose General-Reasoner, a novel training paradigm designed to enhance LLM\nreasoning capabilities across diverse domains. Our key contributions include:\n(1) constructing a large-scale, high-quality dataset of questions with\nverifiable answers curated by web crawling, covering a wide range of\ndisciplines; and (2) developing a generative model-based answer verifier, which\nreplaces traditional rule-based verification with the capability of\nchain-of-thought and context-awareness. We train a series of models and\nevaluate them on a wide range of datasets covering wide domains like physics,\nchemistry, finance, electronics etc. Our comprehensive evaluation across these\n12 benchmarks (e.g. MMLU-Pro, GPQA, SuperGPQA, TheoremQA, BBEH and MATH AMC)\ndemonstrates that General-Reasoner outperforms existing baseline methods,\nachieving robust and generalizable reasoning performance while maintaining\nsuperior effectiveness in mathematical reasoning tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faGeneral-Reasoner\uff0c\u4e00\u79cd\u589e\u5f3aLLM\u8de8\u9886\u57df\u63a8\u7406\u80fd\u529b\u7684\u65b0\u8bad\u7ec3\u8303\u5f0f\uff0c\u901a\u8fc7\u6784\u5efa\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u548c\u751f\u6210\u5f0f\u7b54\u6848\u9a8c\u8bc1\u5668\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u5728\u591a\u6837\u5316\u9886\u57df\u7684\u8868\u73b0\u3002", "motivation": "\u5f53\u524dLLM\u63a8\u7406\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u4e8e\u6570\u5b66\u548c\u7f16\u7a0b\u9886\u57df\uff0c\u9650\u5236\u4e86\u6a21\u578b\u5728\u591a\u6837\u5316\u9886\u57df\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u6784\u5efa\u8de8\u9886\u57df\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\uff0c\u5e76\u5f00\u53d1\u57fa\u4e8e\u751f\u6210\u6a21\u578b\u7684\u7b54\u6848\u9a8c\u8bc1\u5668\uff0c\u53d6\u4ee3\u4f20\u7edf\u89c4\u5219\u9a8c\u8bc1\u3002", "result": "\u572812\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cGeneral-Reasoner\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5c55\u73b0\u51fa\u9c81\u68d2\u4e14\u6cdb\u5316\u7684\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "General-Reasoner\u4e3aLLM\u5728\u591a\u6837\u5316\u9886\u57df\u7684\u63a8\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u7684\u4f18\u8d8a\u6027\u3002"}}
{"id": "2505.14674", "pdf": "https://arxiv.org/pdf/2505.14674", "abs": "https://arxiv.org/abs/2505.14674", "authors": ["Jiaxin Guo", "Zewen Chi", "Li Dong", "Qingxiu Dong", "Xun Wu", "Shaohan Huang", "Furu Wei"], "title": "Reward Reasoning Model", "categories": ["cs.CL"], "comment": null, "summary": "Reward models play a critical role in guiding large language models toward\noutputs that align with human expectations. However, an open challenge remains\nin effectively utilizing test-time compute to enhance reward model performance.\nIn this work, we introduce Reward Reasoning Models (RRMs), which are\nspecifically designed to execute a deliberate reasoning process before\ngenerating final rewards. Through chain-of-thought reasoning, RRMs leverage\nadditional test-time compute for complex queries where appropriate rewards are\nnot immediately apparent. To develop RRMs, we implement a reinforcement\nlearning framework that fosters self-evolved reward reasoning capabilities\nwithout requiring explicit reasoning traces as training data. Experimental\nresults demonstrate that RRMs achieve superior performance on reward modeling\nbenchmarks across diverse domains. Notably, we show that RRMs can adaptively\nexploit test-time compute to further improve reward accuracy. The pretrained\nreward reasoning models are available at\nhttps://huggingface.co/Reward-Reasoning.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5956\u52b1\u63a8\u7406\u6a21\u578b\uff08RRM\uff09\uff0c\u901a\u8fc7\u94fe\u5f0f\u63a8\u7406\u5229\u7528\u989d\u5916\u8ba1\u7b97\u8d44\u6e90\u63d0\u5347\u5956\u52b1\u6a21\u578b\u6027\u80fd\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u5956\u52b1\u6a21\u578b\u5728\u5229\u7528\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u8d44\u6e90\u63d0\u5347\u6027\u80fd\u65b9\u9762\u5b58\u5728\u6311\u6218\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u5f15\u5165\u5956\u52b1\u63a8\u7406\u6a21\u578b\uff08RRM\uff09\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u5b9e\u73b0\u81ea\u6211\u8fdb\u5316\u7684\u5956\u52b1\u63a8\u7406\u80fd\u529b\uff0c\u65e0\u9700\u663e\u5f0f\u63a8\u7406\u8f68\u8ff9\u4f5c\u4e3a\u8bad\u7ec3\u6570\u636e\u3002", "result": "RRM\u5728\u591a\u4e2a\u9886\u57df\u7684\u5956\u52b1\u5efa\u6a21\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u80fd\u81ea\u9002\u5e94\u5229\u7528\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u8d44\u6e90\u63d0\u9ad8\u5956\u52b1\u51c6\u786e\u6027\u3002", "conclusion": "RRM\u901a\u8fc7\u94fe\u5f0f\u63a8\u7406\u548c\u81ea\u9002\u5e94\u8ba1\u7b97\u8d44\u6e90\u5229\u7528\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5956\u52b1\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2505.14679", "pdf": "https://arxiv.org/pdf/2505.14679", "abs": "https://arxiv.org/abs/2505.14679", "authors": ["Xiaojie Gu", "Guangxu Chen", "Jungang Li", "Jia-Chen Gu", "Xuming Hu", "Kai Zhang"], "title": "UltraEdit: Training-, Subject-, and Memory-Free Lifelong Editing in Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Lifelong learning enables large language models (LLMs) to adapt to evolving\ninformation by continually updating their internal knowledge. An ideal system\nshould support efficient, wide-ranging updates while preserving existing\ncapabilities and ensuring reliable deployment. Model editing stands out as a\npromising solution for this goal, offering a focused and efficient way to\nrevise a model's internal knowledge. Although recent paradigms have made\nnotable progress, they often struggle to meet the demands of practical lifelong\nadaptation at scale. To bridge this gap, we propose ULTRAEDIT-a fundamentally\nnew editing solution that is training-, subject- and memory-free, making it\nparticularly well-suited for ultra-scalable, real-world lifelong model editing.\nULTRAEDIT performs editing through a self-contained process that relies solely\non lightweight linear algebra operations to compute parameter shifts, enabling\nfast and consistent parameter modifications with minimal overhead. To improve\nscalability in lifelong settings, ULTRAEDIT employs a lifelong normalization\nstrategy that continuously updates feature statistics across turns, allowing it\nto adapt to distributional shifts and maintain consistency over time. ULTRAEDIT\nachieves editing speeds over 7x faster than the previous state-of-the-art\nmethod-which was also the fastest known approach-while consuming less than 1/3\nthe VRAM, making it the only method currently capable of editing a 7B LLM on a\n24GB consumer-grade GPU. Furthermore, we construct ULTRAEDITBENCH-the largest\ndataset in the field to date, with over 2M editing pairs-and demonstrate that\nour method supports up to 1M edits while maintaining high accuracy.\nComprehensive experiments on four datasets and six models show that ULTRAEDIT\nconsistently achieves superior performance across diverse model editing\nscenarios. Our code is available at: https://github.com/XiaojieGu/UltraEdit.", "AI": {"tldr": "ULTRAEDIT\u662f\u4e00\u79cd\u65b0\u578b\u7684\u7ec8\u8eab\u5b66\u4e60\u6a21\u578b\u7f16\u8f91\u65b9\u6cd5\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u7ebf\u6027\u4ee3\u6570\u64cd\u4f5c\u5b9e\u73b0\u5feb\u901f\u3001\u9ad8\u6548\u7684\u77e5\u8bc6\u66f4\u65b0\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u5b9e\u9645\u5e94\u7528\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u7ec8\u8eab\u5b66\u4e60\u65b9\u6cd5\u5728\u89c4\u6a21\u5316\u548c\u5b9e\u9645\u90e8\u7f72\u4e2d\u7684\u4e0d\u8db3\uff0c\u63d0\u4f9b\u9ad8\u6548\u3001\u5e7f\u6cdb\u7684\u77e5\u8bc6\u66f4\u65b0\u80fd\u529b\u3002", "method": "\u91c7\u7528\u8bad\u7ec3\u3001\u4e3b\u9898\u548c\u5185\u5b58\u65e0\u5173\u7684\u7f16\u8f91\u65b9\u6848\uff0c\u901a\u8fc7\u7ebf\u6027\u4ee3\u6570\u64cd\u4f5c\u8ba1\u7b97\u53c2\u6570\u53d8\u5316\uff0c\u5e76\u7ed3\u5408\u7ec8\u8eab\u5f52\u4e00\u5316\u7b56\u7565\u9002\u5e94\u5206\u5e03\u53d8\u5316\u3002", "result": "ULTRAEDIT\u7f16\u8f91\u901f\u5ea6\u6bd4\u73b0\u6709\u6700\u5feb\u65b9\u6cd5\u5feb7\u500d\uff0cVRAM\u6d88\u8017\u51cf\u5c112/3\uff0c\u652f\u6301\u767e\u4e07\u7ea7\u7f16\u8f91\u5e76\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u3002", "conclusion": "ULTRAEDIT\u5728\u591a\u79cd\u6a21\u578b\u7f16\u8f91\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u662f\u5f53\u524d\u552f\u4e00\u80fd\u5728\u6d88\u8d39\u7ea7GPU\u4e0a\u7f16\u8f917B LLM\u7684\u65b9\u6cd5\u3002"}}
{"id": "2505.14684", "pdf": "https://arxiv.org/pdf/2505.14684", "abs": "https://arxiv.org/abs/2505.14684", "authors": ["Haolei Xu", "Yuchen Yan", "Yongliang Shen", "Wenqi Zhang", "Guiyang Hou", "Shengpei Jiang", "Kaitao Song", "Weiming Lu", "Jun Xiao", "Yueting Zhuang"], "title": "Mind the Gap: Bridging Thought Leap for Improved Chain-of-Thought Tuning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have achieved remarkable progress on\nmathemati-cal tasks through Chain-of-Thought (CoT) reasoning. However, existing\nmathematical CoT datasets often suffer from Thought Leaps due to experts\nomitting intermediate steps, which negatively impacts model learning and\ngeneralization. We propose the CoT Thought Leap Bridge Task, which aims to\nautomatically detect leaps and generate missing intermediate reasoning steps to\nrestore the completeness and coherence of CoT. To facilitate this, we\nconstructed a specialized training dataset called ScaleQM+, based on the\nstructured ScaleQuestMath dataset, and trained CoT-Bridge to bridge thought\nleaps. Through comprehensive experiments on mathematical reasoning benchmarks,\nwe demonstrate that models fine-tuned on bridged datasets consistently\noutperform those trained on original datasets, with improvements of up to\n+5.87% on NuminaMath. Our approach effectively enhances distilled data (+3.02%)\nand provides better starting points for reinforcement learning (+3.1%),\nfunctioning as a plug-and-play module compatible with existing optimization\ntechniques. Furthermore, CoT-Bridge demonstrate improved generalization to\nout-of-domain logical reasoning tasks, confirming that enhancing reasoning\ncompleteness yields broadly applicable benefits.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faCoT Thought Leap Bridge\u4efb\u52a1\uff0c\u901a\u8fc7\u68c0\u6d4b\u601d\u7ef4\u8df3\u8dc3\u5e76\u751f\u6210\u7f3a\u5931\u7684\u63a8\u7406\u6b65\u9aa4\uff0c\u63d0\u5347\u6570\u5b66\u63a8\u7406\u7684\u5b8c\u6574\u6027\u548c\u8fde\u8d2f\u6027\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6570\u5b66CoT\u6570\u636e\u96c6\u4e2d\u5b58\u5728\u4e13\u5bb6\u7701\u7565\u4e2d\u95f4\u6b65\u9aa4\u5bfc\u81f4\u7684\u601d\u7ef4\u8df3\u8dc3\u95ee\u9898\uff0c\u5f71\u54cd\u6a21\u578b\u5b66\u4e60\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51faCoT Thought Leap Bridge\u4efb\u52a1\uff0c\u6784\u5efaScaleQM+\u6570\u636e\u96c6\uff0c\u8bad\u7ec3CoT-Bridge\u6a21\u578b\u4ee5\u586b\u8865\u601d\u7ef4\u8df3\u8dc3\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6a21\u578b\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe5.87%\uff0c\u4e14\u5728\u84b8\u998f\u6570\u636e\u548c\u5f3a\u5316\u5b66\u4e60\u4e2d\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u589e\u5f3a\u63a8\u7406\u5b8c\u6574\u6027\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\uff0cCoT-Bridge\u53ef\u4f5c\u4e3a\u5373\u63d2\u5373\u7528\u6a21\u5757\u517c\u5bb9\u73b0\u6709\u4f18\u5316\u6280\u672f\u3002"}}
{"id": "2505.14685", "pdf": "https://arxiv.org/pdf/2505.14685", "abs": "https://arxiv.org/abs/2505.14685", "authors": ["Nikhil Prakash", "Natalie Shapira", "Arnab Sen Sharma", "Christoph Riedl", "Yonatan Belinkov", "Tamar Rott Shaham", "David Bau", "Atticus Geiger"], "title": "Language Models use Lookbacks to Track Beliefs", "categories": ["cs.CL"], "comment": "32 pages, 32 figures. Code and data at https://belief.baulab.info/", "summary": "How do language models (LMs) represent characters' beliefs, especially when\nthose beliefs may differ from reality? This question lies at the heart of\nunderstanding the Theory of Mind (ToM) capabilities of LMs. We analyze\nLlama-3-70B-Instruct's ability to reason about characters' beliefs using causal\nmediation and abstraction. We construct a dataset that consists of simple\nstories where two characters each separately change the state of two objects,\npotentially unaware of each other's actions. Our investigation uncovered a\npervasive algorithmic pattern that we call a lookback mechanism, which enables\nthe LM to recall important information when it becomes necessary. The LM binds\neach character-object-state triple together by co-locating reference\ninformation about them, represented as their Ordering IDs (OIs) in low rank\nsubspaces of the state token's residual stream. When asked about a character's\nbeliefs regarding the state of an object, the binding lookback retrieves the\ncorresponding state OI and then an answer lookback retrieves the state token.\nWhen we introduce text specifying that one character is (not) visible to the\nother, we find that the LM first generates a visibility ID encoding the\nrelation between the observing and the observed character OIs. In a visibility\nlookback, this ID is used to retrieve information about the observed character\nand update the observing character's beliefs. Our work provides insights into\nthe LM's belief tracking mechanisms, taking a step toward reverse-engineering\nToM reasoning in LMs.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u8bed\u8a00\u6a21\u578b\uff08LMs\uff09\u5982\u4f55\u8868\u793a\u89d2\u8272\u7684\u4fe1\u5ff5\uff0c\u5c24\u5176\u662f\u5f53\u8fd9\u4e9b\u4fe1\u5ff5\u4e0e\u73b0\u5b9e\u4e0d\u540c\u65f6\uff0c\u901a\u8fc7\u56e0\u679c\u4e2d\u4ecb\u548c\u62bd\u8c61\u5206\u6790Llama-3-70B-Instruct\u7684\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u7406\u89e3\u8bed\u8a00\u6a21\u578b\u7684\u2018\u5fc3\u667a\u7406\u8bba\u2019\uff08ToM\uff09\u80fd\u529b\uff0c\u63a2\u7d22\u5176\u5982\u4f55\u8ffd\u8e2a\u548c\u66f4\u65b0\u89d2\u8272\u7684\u4fe1\u5ff5\u3002", "method": "\u6784\u5efa\u5305\u542b\u7b80\u5355\u6545\u4e8b\u7684\u6570\u636e\u96c6\uff0c\u5206\u6790\u6a21\u578b\u901a\u8fc7\u2018\u56de\u6eaf\u673a\u5236\u2019\u548c\u2018\u7ed1\u5b9a\u2019\u6280\u672f\u5904\u7406\u89d2\u8272-\u5bf9\u8c61-\u72b6\u6001\u4fe1\u606f\u7684\u65b9\u5f0f\u3002", "result": "\u53d1\u73b0\u6a21\u578b\u901a\u8fc7\u2018\u56de\u6eaf\u673a\u5236\u2019\u548c\u2018\u53ef\u89c1\u6027ID\u2019\u52a8\u6001\u66f4\u65b0\u89d2\u8272\u4fe1\u5ff5\uff0c\u63ed\u793a\u4e86\u5176\u4fe1\u5ff5\u8ffd\u8e2a\u7684\u7b97\u6cd5\u6a21\u5f0f\u3002", "conclusion": "\u7814\u7a76\u4e3a\u9006\u5411\u5de5\u7a0b\u8bed\u8a00\u6a21\u578b\u7684\u5fc3\u667a\u7406\u8bba\u63a8\u7406\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\u3002"}}
{"id": "2505.13482", "pdf": "https://arxiv.org/pdf/2505.13482", "abs": "https://arxiv.org/abs/2505.13482", "authors": ["Anand Selvadurai", "Jasheen Shaik", "Girish Chandrasekar", "ShriRadhaKrishnan Balamurugan", "Eswara Reddy"], "title": "MedEIR: A Specialized Medical Embedding Model for Enhanced Information Retrieval", "categories": ["cs.IR", "cs.CL"], "comment": "9 pages, 1 figure. This manuscript is a substantial revision of a\n  previously submitted paper. We have explicitly clarified novelty,\n  strengthened scholarly depth, and expanded experimental validation", "summary": "Embedding models have become essential for retrieval-augmented generation\n(RAG) tasks, semantic clustering, and text re-ranking. But despite their\ngrowing use, many of these come with notable limitations. For example, Jina\nfails to capture the semantic content of medical documents, while models such\nas MiniLM often perform poorly on long-form documents. Domain-adapted models,\nwhile specialized, often underperform in general-purpose tasks, reducing their\noverall applicability. General-domain tokenizers often misinterpret medical\nvocabulary. The limitations of current embedding models, whether in\ntokenization accuracy, domain comprehension, or handling long sequences,\nhighlight the need for more versatile solutions. In this work, we present\nMedEIR, a novel embedding model and tokenizer jointly optimized for both\nmedical and general NLP tasks, incorporating ALiBi-based long-context\nprocessing to support sequences of up to 8,192 tokens. MedEIR was pre-trained\non only 6 billion tokens, significantly fewer than Jina's, followed by\nfine-tuning on 3 million sentence pairs. MedEIR consistently outperforms Jina\nV2 and MiniLM across MTEB benchmarks, achieving top scores on ArguAna (55.24),\nNFCorpus (38.44), MedicalQARetrieval (74.25), SciFact (72.04), and TRECCOVID\n(79.56). These results highlight the potential of MedEIR as a highly effective\nembedding model, demonstrating strong performance across both general-purpose\nand domain-specific tasks and outperforming existing models on multiple\nbenchmarks.", "AI": {"tldr": "MedEIR\u662f\u4e00\u79cd\u65b0\u578b\u5d4c\u5165\u6a21\u578b\u548c\u5206\u8bcd\u5668\uff0c\u4e13\u4e3a\u533b\u5b66\u548c\u901a\u7528NLP\u4efb\u52a1\u4f18\u5316\uff0c\u652f\u6301\u957f\u6587\u672c\u5904\u7406\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u5d4c\u5165\u6a21\u578b\u5728\u533b\u5b66\u6587\u6863\u8bed\u4e49\u6355\u6349\u3001\u957f\u6587\u672c\u5904\u7406\u548c\u901a\u7528\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u4e9f\u9700\u66f4\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "MedEIR\u7ed3\u5408ALiBi\u957f\u6587\u672c\u5904\u7406\u6280\u672f\uff0c\u9884\u8bad\u7ec360\u4ebftoken\uff0c\u5e76\u5728300\u4e07\u53e5\u5bf9\u4e0a\u5fae\u8c03\u3002", "result": "\u5728MTEB\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMedEIR\u5728\u591a\u4e2a\u4efb\u52a1\uff08\u5982ArguAna\u3001NFCorpus\u7b49\uff09\u4e0a\u8868\u73b0\u4f18\u4e8eJina V2\u548cMiniLM\u3002", "conclusion": "MedEIR\u5728\u901a\u7528\u548c\u7279\u5b9a\u9886\u57df\u4efb\u52a1\u4e2d\u5747\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2505.13484", "pdf": "https://arxiv.org/pdf/2505.13484", "abs": "https://arxiv.org/abs/2505.13484", "authors": ["Rene Heesch", "Sebastian Eilermann", "Alexander Windmann", "Alexander Diedrich", "Philipp Rosenthal", "Oliver Niggemann"], "title": "Evaluating Large Language Models for Real-World Engineering Tasks", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are transformative not only for daily activities\nbut also for engineering tasks. However, current evaluations of LLMs in\nengineering exhibit two critical shortcomings: (i) the reliance on simplified\nuse cases, often adapted from examination materials where correctness is easily\nverifiable, and (ii) the use of ad hoc scenarios that insufficiently capture\ncritical engineering competencies. Consequently, the assessment of LLMs on\ncomplex, real-world engineering problems remains largely unexplored. This paper\naddresses this gap by introducing a curated database comprising over 100\nquestions derived from authentic, production-oriented engineering scenarios,\nsystematically designed to cover core competencies such as product design,\nprognosis, and diagnosis. Using this dataset, we evaluate four state-of-the-art\nLLMs, including both cloud-based and locally hosted instances, to\nsystematically investigate their performance on complex engineering tasks. Our\nresults show that LLMs demonstrate strengths in basic temporal and structural\nreasoning but struggle significantly with abstract reasoning, formal modeling,\nand context-sensitive engineering logic.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u771f\u5b9e\u5de5\u7a0b\u573a\u666f\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u586b\u8865\u4e86\u5f53\u524dLLMs\u5728\u590d\u6742\u5de5\u7a0b\u95ee\u9898\u8bc4\u4f30\u4e0a\u7684\u7a7a\u767d\u3002", "motivation": "\u5f53\u524dLLMs\u5728\u5de5\u7a0b\u4efb\u52a1\u4e2d\u7684\u8bc4\u4f30\u5b58\u5728\u7b80\u5316\u7528\u4f8b\u548c\u4e34\u65f6\u573a\u666f\u7684\u4e0d\u8db3\uff0c\u672a\u80fd\u5145\u5206\u53cd\u6620\u771f\u5b9e\u5de5\u7a0b\u80fd\u529b\u3002", "method": "\u6784\u5efa\u5305\u542b100\u591a\u4e2a\u771f\u5b9e\u5de5\u7a0b\u95ee\u9898\u7684\u6570\u636e\u5e93\uff0c\u8bc4\u4f30\u56db\u79cd\u5148\u8fdbLLMs\u7684\u6027\u80fd\u3002", "result": "LLMs\u5728\u57fa\u7840\u65f6\u7a7a\u63a8\u7406\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u62bd\u8c61\u63a8\u7406\u3001\u5f62\u5f0f\u5efa\u6a21\u548c\u4e0a\u4e0b\u6587\u654f\u611f\u903b\u8f91\u4e0a\u8868\u73b0\u8f83\u5dee\u3002", "conclusion": "LLMs\u5728\u590d\u6742\u5de5\u7a0b\u4efb\u52a1\u4e2d\u4ecd\u6709\u5c40\u9650\u6027\uff0c\u9700\u8fdb\u4e00\u6b65\u6539\u8fdb\u3002"}}
{"id": "2505.13489", "pdf": "https://arxiv.org/pdf/2505.13489", "abs": "https://arxiv.org/abs/2505.13489", "authors": ["Wenkang Han", "Wang Lin", "Liya Hu", "Zhenlong Dai", "Yiyun Zhou", "Mengze Li", "Zemin Liu", "Chang Yao", "Jingyuan Chen"], "title": "Contrastive Cross-Course Knowledge Tracing via Concept Graph Guided Knowledge Transfer", "categories": ["cs.AI", "cs.CL"], "comment": "Accepted by IJCAI 2025", "summary": "Knowledge tracing (KT) aims to predict learners' future performance based on\nhistorical learning interactions. However, existing KT models predominantly\nfocus on data from a single course, limiting their ability to capture a\ncomprehensive understanding of learners' knowledge states. In this paper, we\npropose TransKT, a contrastive cross-course knowledge tracing method that\nleverages concept graph guided knowledge transfer to model the relationships\nbetween learning behaviors across different courses, thereby enhancing\nknowledge state estimation. Specifically, TransKT constructs a cross-course\nconcept graph by leveraging zero-shot Large Language Model (LLM) prompts to\nestablish implicit links between related concepts across different courses.\nThis graph serves as the foundation for knowledge transfer, enabling the model\nto integrate and enhance the semantic features of learners' interactions across\ncourses. Furthermore, TransKT includes an LLM-to-LM pipeline for incorporating\nsummarized semantic features, which significantly improves the performance of\nGraph Convolutional Networks (GCNs) used for knowledge transfer. Additionally,\nTransKT employs a contrastive objective that aligns single-course and\ncross-course knowledge states, thereby refining the model's ability to provide\na more robust and accurate representation of learners' overall knowledge\nstates.", "AI": {"tldr": "TransKT\u63d0\u51fa\u4e86\u4e00\u79cd\u8de8\u8bfe\u7a0b\u7684\u77e5\u8bc6\u8ffd\u8e2a\u65b9\u6cd5\uff0c\u901a\u8fc7\u6982\u5ff5\u56fe\u5f15\u5bfc\u77e5\u8bc6\u8fc1\u79fb\uff0c\u63d0\u5347\u5b66\u4e60\u8005\u77e5\u8bc6\u72b6\u6001\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u77e5\u8bc6\u8ffd\u8e2a\u6a21\u578b\u4e3b\u8981\u5173\u6ce8\u5355\u4e00\u8bfe\u7a0b\u6570\u636e\uff0c\u96be\u4ee5\u5168\u9762\u6355\u6349\u5b66\u4e60\u8005\u7684\u77e5\u8bc6\u72b6\u6001\u3002", "method": "TransKT\u6784\u5efa\u8de8\u8bfe\u7a0b\u6982\u5ff5\u56fe\uff0c\u5229\u7528\u96f6\u6837\u672c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5efa\u7acb\u6982\u5ff5\u95f4\u7684\u9690\u5f0f\u8054\u7cfb\uff0c\u5e76\u901a\u8fc7\u5bf9\u6bd4\u76ee\u6807\u4f18\u5316\u77e5\u8bc6\u72b6\u6001\u8868\u793a\u3002", "result": "TransKT\u663e\u8457\u63d0\u5347\u4e86\u77e5\u8bc6\u8fc1\u79fb\u7684\u6027\u80fd\uff0c\u589e\u5f3a\u4e86\u5b66\u4e60\u8005\u77e5\u8bc6\u72b6\u6001\u7684\u8868\u793a\u80fd\u529b\u3002", "conclusion": "TransKT\u901a\u8fc7\u8de8\u8bfe\u7a0b\u77e5\u8bc6\u8fc1\u79fb\u548c\u5bf9\u6bd4\u5b66\u4e60\uff0c\u4e3a\u77e5\u8bc6\u8ffd\u8e2a\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u548c\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.13511", "pdf": "https://arxiv.org/pdf/2505.13511", "abs": "https://arxiv.org/abs/2505.13511", "authors": ["David Noever", "Forrest McKee"], "title": "Can AI Freelancers Compete? Benchmarking Earnings, Reliability, and Task Success at Scale", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "This study explores Large Language Models (LLMs) as autonomous agents for\nreal-world tasks, including freelance software development. This work presents\na new benchmark that evaluates LLMs on freelance programming and data analysis\ntasks derived from economic data. We construct the benchmark using synthetic\ntasks created from a Kaggle Freelancer dataset of job postings, with all job\nprices standardized to USD (median fixed-project price around $250, and an\naverage of $306). Each task is accompanied by structured input-output test\ncases and an estimated price tag, enabling automated correctness checking and a\nmonetary performance valuation. This approach is inspired by OpenAI's recent\nSWE-Lancer benchmark (1,400 real Upwork tasks worth $1M total). Still, our\nframework simplifies evaluation using programmatically testable tasks and\npredicted price values, making it highly scalable and repeatable. On this\nbenchmark, we evaluate four modern LLMs - Claude 3.5 Haiku, GPT-4o-mini, Qwen\n2.5, and Mistral. We report each model's accuracy (task success rate and\ntest-case pass rate) and the total \"freelance earnings\" it achieves (sum of\nprices of solved tasks). Our results show that Claude 3.5 Haiku performs best,\nearning approximately $1.52 million USD, followed closely by GPT-4o-mini at\n$1.49 million, then Qwen 2.5 ($1.33M) and Mistral ($0.70M). We analyze the\ndistribution of errors per task and observe that the strongest models solve the\nmost tasks and rarely fail completely on any project. We discuss the\nimplications of these results for the feasibility of AI as a freelance\ndeveloper, the advantages and limitations of our automated benchmark approach,\nand the gap between performance on structured tasks versus the true complexity\nof real-world freelance jobs.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4f5c\u4e3a\u81ea\u4e3b\u4ee3\u7406\u5b8c\u6210\u73b0\u5b9e\u4efb\u52a1\uff08\u5982\u81ea\u7531\u804c\u4e1a\u8f6f\u4ef6\u5f00\u53d1\uff09\u7684\u80fd\u529b\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u7ecf\u6d4e\u6570\u636e\u7684\u65b0\u57fa\u51c6\uff0c\u8bc4\u4f30LLMs\u5728\u81ea\u7531\u804c\u4e1a\u7f16\u7a0b\u548c\u6570\u636e\u5206\u6790\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u8bc4\u4f30LLMs\u5728\u81ea\u7531\u804c\u4e1a\u4efb\u52a1\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\uff0c\u4e3aAI\u4f5c\u4e3a\u81ea\u7531\u804c\u4e1a\u5f00\u53d1\u8005\u7684\u53ef\u884c\u6027\u63d0\u4f9b\u4f9d\u636e\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u57fa\u4e8eKaggle\u81ea\u7531\u804c\u4e1a\u6570\u636e\u96c6\u7684\u5408\u6210\u4efb\u52a1\u57fa\u51c6\uff0c\u5305\u542b\u7ed3\u6784\u5316\u8f93\u5165\u8f93\u51fa\u6d4b\u8bd5\u7528\u4f8b\u548c\u9884\u4f30\u4ef7\u683c\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u8bc4\u4f30\u3002", "result": "Claude 3.5 Haiku\u8868\u73b0\u6700\u4f73\uff0c\u8d5a\u53d6\u7ea6152\u4e07\u7f8e\u5143\uff0c\u5176\u6b21\u662fGPT-4o-mini\uff08149\u4e07\u7f8e\u5143\uff09\u3001Qwen 2.5\uff08133\u4e07\u7f8e\u5143\uff09\u548cMistral\uff0870\u4e07\u7f8e\u5143\uff09\u3002", "conclusion": "\u7814\u7a76\u8868\u660eLLMs\u5728\u7ed3\u6784\u5316\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u771f\u5b9e\u81ea\u7531\u804c\u4e1a\u4efb\u52a1\u7684\u590d\u6742\u6027\u4ecd\u5b58\u5728\u5dee\u8ddd\uff0c\u81ea\u52a8\u5316\u57fa\u51c6\u65b9\u6cd5\u5177\u6709\u53ef\u6269\u5c55\u6027\u548c\u91cd\u590d\u6027\u4f18\u52bf\u3002"}}
{"id": "2505.13515", "pdf": "https://arxiv.org/pdf/2505.13515", "abs": "https://arxiv.org/abs/2505.13515", "authors": ["Yanan Li", "Fanxu Meng", "Muhan Zhang", "Shiai Zhu", "Shangguang Wang", "Mengwei Xu"], "title": "LoRASuite: Efficient LoRA Adaptation Across Large Language Model Upgrades", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "As Large Language Models (LLMs) are frequently updated, LoRA weights trained\non earlier versions quickly become obsolete. The conventional practice of\nretraining LoRA weights from scratch on the latest model is costly,\ntime-consuming, and environmentally detrimental, particularly as the diversity\nof LLMs and downstream tasks expands. This motivates a critical question: \"How\ncan we efficiently leverage existing LoRA weights to adapt to newer model\nversions?\" To address this, we propose LoRASuite, a modular approach tailored\nspecifically to various types of LLM updates. First, we compute a transfer\nmatrix utilizing known parameters from both old and new LLMs. Next, we allocate\ncorresponding layers and attention heads based on centered kernel alignment and\ncosine similarity metrics, respectively. A subsequent small-scale, skillful\nfine-tuning step ensures numerical stability. Experimental evaluations\ndemonstrate that LoRASuite consistently surpasses small-scale vanilla LoRA\nmethods. Notably, on backbone LLMs such as MiniCPM and Qwen, LoRASuite even\nexceeds the performance of full-scale LoRA retraining, with average\nimprovements of +1.4 and +6.6 points on math tasks, respectively. Additionally,\nLoRASuite significantly reduces memory consumption by 5.5 GB and computational\ntime by 78.23%.", "AI": {"tldr": "LoRASuite\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u5757\u5316\u65b9\u6cd5\uff0c\u5229\u7528\u73b0\u6709LoRA\u6743\u91cd\u9ad8\u6548\u9002\u5e94\u65b0\u7248LLM\uff0c\u907f\u514d\u4ece\u5934\u8bad\u7ec3\uff0c\u663e\u8457\u8282\u7701\u8d44\u6e90\u548c\u65f6\u95f4\u3002", "motivation": "\u968f\u7740LLM\u9891\u7e41\u66f4\u65b0\uff0c\u65e7\u7248LoRA\u6743\u91cd\u5feb\u901f\u8fc7\u65f6\uff0c\u4ece\u5934\u8bad\u7ec3\u6210\u672c\u9ad8\u4e14\u4e0d\u73af\u4fdd\u3002", "method": "\u901a\u8fc7\u8f6c\u79fb\u77e9\u9635\u548c\u5c42/\u6ce8\u610f\u529b\u5934\u5206\u914d\uff0c\u7ed3\u5408\u5c0f\u89c4\u6a21\u5fae\u8c03\uff0c\u5b9e\u73b0\u9ad8\u6548\u8fc1\u79fb\u3002", "result": "LoRASuite\u5728MiniCPM\u548cQwen\u4e0a\u8868\u73b0\u4f18\u4e8e\u5168\u91cfLoRA\u91cd\u8bad\u7ec3\uff0c\u6570\u5b66\u4efb\u52a1\u5206\u522b\u63d0\u53471.4\u548c6.6\u5206\uff0c\u8282\u7701\u5185\u5b585.5GB\uff0c\u8ba1\u7b97\u65f6\u95f4\u51cf\u5c1178.23%\u3002", "conclusion": "LoRASuite\u4e3aLLM\u66f4\u65b0\u63d0\u4f9b\u9ad8\u6548\u3001\u4f4e\u6210\u672c\u7684LoRA\u6743\u91cd\u8fc1\u79fb\u65b9\u6848\u3002"}}
{"id": "2505.13529", "pdf": "https://arxiv.org/pdf/2505.13529", "abs": "https://arxiv.org/abs/2505.13529", "authors": ["Junxiao Yang", "Jinzhe Tu", "Haoran Liu", "Xiaoce Wang", "Chujie Zheng", "Zhexin Zhang", "Shiyao Cui", "Caishun Chen", "Tiantian He", "Hongning Wang", "Yew-Soon Ong", "Minlie Huang"], "title": "BARREL: Boundary-Aware Reasoning for Factual and Reliable LRMs", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Recent advances in Large Reasoning Models (LRMs) have shown impressive\ncapabilities in mathematical and logical reasoning. However, current LRMs\nrarely admit ignorance or respond with \"I don't know\". Instead, they often\nproduce incorrect answers while showing undue confidence, raising concerns\nabout their factual reliability. In this work, we identify two pathological\nreasoning patterns characterized by overthinking that contribute to the\noverconfident and incorrect answers: last-minute guessing and second-thought\nspiraling. To address these issues, we propose BARREL-a novel framework that\npromotes concise and boundary-aware factual reasoning. Our experiments show\nthat BARREL-training increases the reliability of DeepSeek-R1-Distill-Llama-8B\nfrom 39.33% to 61.48%, while still achieving accuracy comparable to models\nfinetuned on reasoning data generated by R1. These results demonstrate that our\npilot study is inspiring to build more reliable and factual System 2 LRMs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faBARREL\u6846\u67b6\uff0c\u89e3\u51b3\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u8fc7\u5ea6\u81ea\u4fe1\u548c\u9519\u8bef\u56de\u7b54\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u8fb9\u754c\u611f\u77e5\u63a8\u7406\u63d0\u5347\u53ef\u9760\u6027\u3002", "motivation": "\u5f53\u524dLRMs\u5728\u6570\u5b66\u548c\u903b\u8f91\u63a8\u7406\u4e2d\u8868\u73b0\u51fa\u8fc7\u5ea6\u81ea\u4fe1\uff0c\u5373\u4f7f\u4e0d\u77e5\u9053\u7b54\u6848\u4e5f\u4f1a\u7ed9\u51fa\u9519\u8bef\u56de\u7b54\uff0c\u5f71\u54cd\u4e8b\u5b9e\u53ef\u9760\u6027\u3002", "method": "\u63d0\u51faBARREL\u6846\u67b6\uff0c\u9488\u5bf9\u4e24\u79cd\u75c5\u6001\u63a8\u7406\u6a21\u5f0f\uff08\u6700\u540e\u731c\u6d4b\u548c\u7b2c\u4e8c\u601d\u7ef4\u87ba\u65cb\uff09\u8fdb\u884c\u4f18\u5316\uff0c\u5b9e\u73b0\u7b80\u6d01\u4e14\u8fb9\u754c\u611f\u77e5\u7684\u63a8\u7406\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cBARREL\u8bad\u7ec3\u5c06DeepSeek-R1-Distill-Llama-8B\u7684\u53ef\u9760\u6027\u4ece39.33%\u63d0\u5347\u81f361.48%\uff0c\u540c\u65f6\u4fdd\u6301\u4e0eR1\u751f\u6210\u6570\u636e\u5fae\u8c03\u6a21\u578b\u76f8\u5f53\u7684\u51c6\u786e\u6027\u3002", "conclusion": "BARREL\u6846\u67b6\u4e3a\u6784\u5efa\u66f4\u53ef\u9760\u7684\u4e8b\u5b9e\u6027System 2 LRMs\u63d0\u4f9b\u4e86\u542f\u53d1\u3002"}}
{"id": "2505.13531", "pdf": "https://arxiv.org/pdf/2505.13531", "abs": "https://arxiv.org/abs/2505.13531", "authors": ["Shitong Duan", "Xiaoyuan Yi", "Peng Zhang", "Dongkuan Xu", "Jing Yao", "Tun Lu", "Ning Gu", "Xing Xie"], "title": "AdAEM: An Adaptively and Automated Extensible Measurement of LLMs' Value Difference", "categories": ["cs.CY", "cs.AI", "cs.CL"], "comment": null, "summary": "Assessing Large Language Models (LLMs)' underlying value differences enables\ncomprehensive comparison of their misalignment, cultural adaptability, and\nbiases. Nevertheless, current value measurement datasets face the\ninformativeness challenge: with often outdated, contaminated, or generic test\nquestions, they can only capture the shared value orientations among different\nLLMs, leading to saturated and thus uninformative results. To address this\nproblem, we introduce AdAEM, a novel, self-extensible assessment framework for\nrevealing LLMs' inclinations. Distinct from previous static benchmarks, AdAEM\ncan automatically and adaptively generate and extend its test questions. This\nis achieved by probing the internal value boundaries of a diverse set of LLMs\ndeveloped across cultures and time periods in an in-context optimization\nmanner. The optimization process theoretically maximizes an\ninformation-theoretic objective to extract the latest or culturally\ncontroversial topics, providing more distinguishable and informative insights\nabout models' value differences. In this way, AdAEM is able to co-evolve with\nthe development of LLMs, consistently tracking their value dynamics. Using\nAdAEM, we generate 12,310 questions grounded in Schwartz Value Theory, conduct\nan extensive analysis to manifest our method's validity and effectiveness, and\nbenchmark the values of 16 LLMs, laying the groundwork for better value\nresearch.", "AI": {"tldr": "AdAEM\u662f\u4e00\u4e2a\u81ea\u6269\u5c55\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u63ed\u793a\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u4ef7\u503c\u503e\u5411\uff0c\u901a\u8fc7\u52a8\u6001\u751f\u6210\u6d4b\u8bd5\u95ee\u9898\u89e3\u51b3\u73b0\u6709\u6570\u636e\u96c6\u7684\u4fe1\u606f\u4e0d\u8db3\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u4ef7\u503c\u6d4b\u91cf\u6570\u636e\u96c6\u56e0\u95ee\u9898\u8fc7\u65f6\u3001\u6c61\u67d3\u6216\u6cdb\u6cdb\u800c\u65e0\u6cd5\u6709\u6548\u6355\u6349LLMs\u7684\u4ef7\u503c\u5dee\u5f02\uff0c\u5bfc\u81f4\u7ed3\u679c\u9971\u548c\u4e14\u65e0\u4fe1\u606f\u91cf\u3002", "method": "AdAEM\u901a\u8fc7\u4e0a\u4e0b\u6587\u4f18\u5316\u65b9\u5f0f\u81ea\u52a8\u751f\u6210\u548c\u6269\u5c55\u6d4b\u8bd5\u95ee\u9898\uff0c\u6700\u5927\u5316\u4fe1\u606f\u7406\u8bba\u76ee\u6807\u4ee5\u63d0\u53d6\u6700\u65b0\u6216\u6709\u6587\u5316\u4e89\u8bae\u7684\u4e3b\u9898\u3002", "result": "\u751f\u621012,310\u4e2a\u57fa\u4e8eSchwartz\u4ef7\u503c\u7406\u8bba\u7684\u95ee\u9898\uff0c\u5206\u679016\u4e2aLLMs\u7684\u4ef7\u503c\u5dee\u5f02\uff0c\u9a8c\u8bc1\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u533a\u5206\u5ea6\u3002", "conclusion": "AdAEM\u80fd\u52a8\u6001\u8ddf\u8e2aLLMs\u7684\u4ef7\u503c\u6f14\u53d8\uff0c\u4e3a\u4ef7\u503c\u7814\u7a76\u63d0\u4f9b\u66f4\u6709\u6548\u7684\u57fa\u7840\u3002"}}
{"id": "2505.13534", "pdf": "https://arxiv.org/pdf/2505.13534", "abs": "https://arxiv.org/abs/2505.13534", "authors": ["Dan Ofer", "Michal Linial", "Dafna Shahaf"], "title": "InterFeat: An Automated Pipeline for Finding Interesting Hypotheses in Structured Biomedical Data", "categories": ["q-bio.QM", "cs.AI", "cs.CL", "cs.IR", "68T05, 68T50, 92C50", "I.2.6; I.2.7; H.2.8; J.3"], "comment": null, "summary": "Finding interesting phenomena is the core of scientific discovery, but it is\na manual, ill-defined concept. We present an integrative pipeline for\nautomating the discovery of interesting simple hypotheses (feature-target\nrelations with effect direction and a potential underlying mechanism) in\nstructured biomedical data. The pipeline combines machine learning, knowledge\ngraphs, literature search and Large Language Models. We formalize\n\"interestingness\" as a combination of novelty, utility and plausibility. On 8\nmajor diseases from the UK Biobank, our pipeline consistently recovers risk\nfactors years before their appearance in the literature. 40--53% of our top\ncandidates were validated as interesting, compared to 0--7% for a SHAP-based\nbaseline. Overall, 28% of 109 candidates were interesting to medical experts.\nThe pipeline addresses the challenge of operationalizing \"interestingness\"\nscalably and for any target. We release data and code:\nhttps://github.com/LinialLab/InterFeat", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u53d1\u73b0\u751f\u7269\u533b\u5b66\u6570\u636e\u4e2d\u6709\u8da3\u5047\u8bbe\u7684\u7ba1\u9053\uff0c\u7ed3\u5408\u673a\u5668\u5b66\u4e60\u3001\u77e5\u8bc6\u56fe\u8c31\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u5b9a\u4e49\u201c\u6709\u8da3\u6027\u201d\u4e3a\u65b0\u9896\u6027\u3001\u5b9e\u7528\u6027\u548c\u5408\u7406\u6027\u3002", "motivation": "\u79d1\u5b66\u53d1\u73b0\u7684\u6838\u5fc3\u662f\u53d1\u73b0\u6709\u8da3\u73b0\u8c61\uff0c\u4f46\u8fd9\u662f\u4e00\u4e2a\u624b\u52a8\u4e14\u6a21\u7cca\u7684\u6982\u5ff5\uff0c\u9700\u8981\u81ea\u52a8\u5316\u65b9\u6cd5\u6765\u63d0\u9ad8\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u3002", "method": "\u7ed3\u5408\u673a\u5668\u5b66\u4e60\u3001\u77e5\u8bc6\u56fe\u8c31\u3001\u6587\u732e\u641c\u7d22\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u5b9a\u4e49\u201c\u6709\u8da3\u6027\u201d\u4e3a\u65b0\u9896\u6027\u3001\u5b9e\u7528\u6027\u548c\u5408\u7406\u6027\u7684\u7ec4\u5408\u3002", "result": "\u57288\u79cd\u4e3b\u8981\u75be\u75c5\u4e0a\uff0c\u7ba1\u9053\u80fd\u63d0\u524d\u6570\u5e74\u53d1\u73b0\u98ce\u9669\u56e0\u7d20\uff0c40-53%\u7684\u5019\u9009\u5047\u8bbe\u88ab\u9a8c\u8bc1\u4e3a\u6709\u8da3\uff0c\u8fdc\u8d85\u57fa\u7ebf\u65b9\u6cd5\u76840-7%\u3002", "conclusion": "\u8be5\u7ba1\u9053\u4e3a\u201c\u6709\u8da3\u6027\u201d\u7684\u53ef\u6269\u5c55\u64cd\u4f5c\u63d0\u4f9b\u4e86\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u751f\u7269\u533b\u5b66\u6570\u636e\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2505.13581", "pdf": "https://arxiv.org/pdf/2505.13581", "abs": "https://arxiv.org/abs/2505.13581", "authors": ["Tommaso Mario Buonocore", "Enea Parimbelli"], "title": "RAR: Setting Knowledge Tripwires for Retrieval Augmented Rejection", "categories": ["cs.IR", "cs.CL", "cs.CR", "68M25, 68T07", "I.2.7; K.6.5"], "comment": "7 pages, 4 figures, 2 tables", "summary": "Content moderation for large language models (LLMs) remains a significant\nchallenge, requiring flexible and adaptable solutions that can quickly respond\nto emerging threats. This paper introduces Retrieval Augmented Rejection (RAR),\na novel approach that leverages a retrieval-augmented generation (RAG)\narchitecture to dynamically reject unsafe user queries without model\nretraining. By strategically inserting and marking malicious documents into the\nvector database, the system can identify and reject harmful requests when these\ndocuments are retrieved. Our preliminary results show that RAR achieves\ncomparable performance to embedded moderation in LLMs like Claude 3.5 Sonnet,\nwhile offering superior flexibility and real-time customization capabilities, a\nfundamental feature to timely address critical vulnerabilities. This approach\nintroduces no architectural changes to existing RAG systems, requiring only the\naddition of specially crafted documents and a simple rejection mechanism based\non retrieval results.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u68c0\u7d22\u589e\u5f3a\u62d2\u7edd\uff08RAR\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u67b6\u6784\u52a8\u6001\u62d2\u7edd\u4e0d\u5b89\u5168\u67e5\u8be2\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u5185\u5bb9\u5ba1\u6838\u9700\u8981\u7075\u6d3b\u4e14\u9002\u5e94\u6027\u5f3a\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4ee5\u5feb\u901f\u5e94\u5bf9\u65b0\u5174\u5a01\u80c1\u3002", "method": "\u901a\u8fc7\u5728\u5411\u91cf\u6570\u636e\u5e93\u4e2d\u7b56\u7565\u6027\u5730\u63d2\u5165\u548c\u6807\u8bb0\u6076\u610f\u6587\u6863\uff0c\u7cfb\u7edf\u53ef\u4ee5\u5728\u68c0\u7d22\u5230\u8fd9\u4e9b\u6587\u6863\u65f6\u8bc6\u522b\u5e76\u62d2\u7edd\u6709\u5bb3\u8bf7\u6c42\u3002", "result": "\u521d\u6b65\u7ed3\u679c\u663e\u793a\uff0cRAR\u7684\u6027\u80fd\u4e0eClaude 3.5 Sonnet\u7b49LLM\u4e2d\u7684\u5d4c\u5165\u5f0f\u5ba1\u6838\u76f8\u5f53\uff0c\u540c\u65f6\u63d0\u4f9b\u66f4\u9ad8\u7684\u7075\u6d3b\u6027\u548c\u5b9e\u65f6\u5b9a\u5236\u80fd\u529b\u3002", "conclusion": "RAR\u65e0\u9700\u5bf9\u73b0\u6709RAG\u7cfb\u7edf\u8fdb\u884c\u67b6\u6784\u66f4\u6539\uff0c\u4ec5\u9700\u6dfb\u52a0\u7279\u6b8a\u6587\u6863\u548c\u57fa\u4e8e\u68c0\u7d22\u7ed3\u679c\u7684\u7b80\u5355\u62d2\u7edd\u673a\u5236\uff0c\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.13652", "pdf": "https://arxiv.org/pdf/2505.13652", "abs": "https://arxiv.org/abs/2505.13652", "authors": ["Karina Zainullina", "Alexander Golubev", "Maria Trofimova", "Sergei Polezhaev", "Ibragim Badertdinov", "Daria Litvintseva", "Simon Karasik", "Filipp Fisin", "Sergei Skvortsov", "Maksim Nekrashevich", "Anton Shevtsov", "Boris Yangel"], "title": "Guided Search Strategies in Non-Serializable Environments with Applications to Software Engineering Agents", "categories": ["cs.SE", "cs.CL"], "comment": "ICML", "summary": "Large language models (LLMs) have recently achieved remarkable results in\ncomplex multi-step tasks, such as mathematical reasoning and agentic software\nengineering. However, they often struggle to maintain consistent performance\nacross multiple solution attempts. One effective approach to narrow the gap\nbetween average-case and best-case performance is guided test-time search,\nwhich explores multiple solution paths to identify the most promising one.\nUnfortunately, effective search techniques (e.g. MCTS) are often unsuitable for\nnon-serializable RL environments, such as Docker containers, where intermediate\nenvironment states cannot be easily saved and restored. We investigate two\ncomplementary search strategies applicable to such environments: 1-step\nlookahead and trajectory selection, both guided by a learned action-value\nfunction estimator. On the SWE-bench Verified benchmark, a key testbed for\nagentic software engineering, we find these methods to double the average\nsuccess rate of a fine-tuned Qwen-72B model, achieving 40.8%, the new\nstate-of-the-art for open-weights models. Additionally, we show that these\ntechniques are transferable to more advanced closed models, yielding similar\nimprovements with GPT-4o.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5728\u975e\u5e8f\u5217\u5316RL\u73af\u5883\u4e2d\uff08\u5982Docker\u5bb9\u5668\uff09\u5982\u4f55\u901a\u8fc71\u6b65\u524d\u77bb\u548c\u8f68\u8ff9\u9009\u62e9\u4e24\u79cd\u641c\u7d22\u7b56\u7565\u63d0\u5347LLMs\u7684\u6027\u80fd\uff0c\u5e76\u5728SWE-bench Verified\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u663e\u8457\u6548\u679c\u3002", "motivation": "LLMs\u5728\u591a\u6b65\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u591a\u6b21\u5c1d\u8bd5\u4e2d\u6027\u80fd\u4e0d\u7a33\u5b9a\uff0c\u5c24\u5176\u662f\u5728\u975e\u5e8f\u5217\u5316\u73af\u5883\u4e2d\uff0c\u4f20\u7edf\u641c\u7d22\u65b9\u6cd5\uff08\u5982MCTS\uff09\u96be\u4ee5\u9002\u7528\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u4e92\u8865\u7684\u641c\u7d22\u7b56\u7565\uff1a1\u6b65\u524d\u77bb\u548c\u8f68\u8ff9\u9009\u62e9\uff0c\u57fa\u4e8e\u5b66\u4e60\u7684\u52a8\u4f5c\u4ef7\u503c\u51fd\u6570\u4f30\u8ba1\u5668\u6307\u5bfc\u3002", "result": "\u5728SWE-bench Verified\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u4f7fQwen-72B\u6a21\u578b\u7684\u5e73\u5747\u6210\u529f\u7387\u7ffb\u500d\uff0c\u8fbe\u523040.8%\uff0c\u5e76\u5728GPT-4o\u4e2d\u540c\u6837\u6709\u6548\u3002", "conclusion": "\u8fd9\u4e9b\u641c\u7d22\u7b56\u7565\u663e\u8457\u63d0\u5347\u4e86LLMs\u5728\u975e\u5e8f\u5217\u5316\u73af\u5883\u4e2d\u7684\u6027\u80fd\uff0c\u4e14\u53ef\u8fc1\u79fb\u81f3\u66f4\u5148\u8fdb\u7684\u6a21\u578b\u3002"}}
{"id": "2505.13718", "pdf": "https://arxiv.org/pdf/2505.13718", "abs": "https://arxiv.org/abs/2505.13718", "authors": ["Safal Shrestha", "Minwu Kim", "Aadim Nepal", "Anubhav Shrestha", "Keith Ross"], "title": "Warm Up Before You Train: Unlocking General Reasoning in Resource-Constrained Settings", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Designing effective reasoning-capable LLMs typically requires training using\nReinforcement Learning with Verifiable Rewards (RLVR) or distillation with\ncarefully curated Long Chain of Thoughts (CoT), both of which depend heavily on\nextensive training data. This creates a major challenge when the amount of\nquality training data is scarce. We propose a sample-efficient, two-stage\ntraining strategy to develop reasoning LLMs under limited supervision. In the\nfirst stage, we \"warm up\" the model by distilling Long CoTs from a toy domain,\nnamely, Knights \\& Knaves (K\\&K) logic puzzles to acquire general reasoning\nskills. In the second stage, we apply RLVR to the warmed-up model using a\nlimited set of target-domain examples. Our experiments demonstrate that this\ntwo-phase approach offers several benefits: $(i)$ the warmup phase alone\nfacilitates generalized reasoning, leading to performance improvements across a\nrange of tasks, including MATH, HumanEval$^{+}$, and MMLU-Pro. $(ii)$ When both\nthe base model and the warmed-up model are RLVR trained on the same small\ndataset ($\\leq100$ examples), the warmed-up model consistently outperforms the\nbase model; $(iii)$ Warming up before RLVR training allows a model to maintain\ncross-domain generalizability even after training on a specific domain; $(iv)$\nIntroducing warmup in the pipeline improves not only accuracy but also overall\nsample efficiency during RLVR training. The results in this paper highlight the\npromise of warmup for building robust reasoning LLMs in data-scarce\nenvironments.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff0c\u7528\u4e8e\u5728\u6709\u9650\u76d1\u7763\u4e0b\u5f00\u53d1\u5177\u5907\u63a8\u7406\u80fd\u529b\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\uff0c\u901a\u8fc7\u9884\u70ed\u9636\u6bb5\u548c\u5f3a\u5316\u5b66\u4e60\u9636\u6bb5\u63d0\u9ad8\u6837\u672c\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u8bad\u7ec3\u63a8\u7406\u80fd\u529b\u5f3a\u7684LLM\u901a\u5e38\u9700\u8981\u5927\u91cf\u9ad8\u8d28\u91cf\u6570\u636e\uff0c\u4f46\u5728\u6570\u636e\u7a00\u7f3a\u7684\u60c5\u51b5\u4e0b\uff0c\u8fd9\u6210\u4e3a\u4e3b\u8981\u6311\u6218\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a1\uff09\u9884\u70ed\u9636\u6bb5\uff0c\u901a\u8fc7\u73a9\u5177\u9886\u57df\uff08\u5982K&K\u903b\u8f91\u8c1c\u9898\uff09\u7684\u957f\u94fe\u601d\u7ef4\u84b8\u998f\u83b7\u53d6\u901a\u7528\u63a8\u7406\u80fd\u529b\uff1b2\uff09\u5f3a\u5316\u5b66\u4e60\u9636\u6bb5\uff0c\u5728\u9884\u70ed\u6a21\u578b\u4e0a\u4f7f\u7528\u5c11\u91cf\u76ee\u6807\u9886\u57df\u6570\u636e\u8fdb\u884cRLVR\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u9884\u70ed\u9636\u6bb5\u80fd\u63d0\u5347\u8de8\u4efb\u52a1\u6027\u80fd\uff0c\u9884\u70ed\u540e\u7684\u6a21\u578b\u5728\u76f8\u540c\u5c0f\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u57fa\u7840\u6a21\u578b\uff0c\u4e14\u4fdd\u6301\u4e86\u8de8\u9886\u57df\u6cdb\u5316\u80fd\u529b\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u6837\u672c\u6548\u7387\u3002", "conclusion": "\u9884\u70ed\u7b56\u7565\u5728\u6570\u636e\u7a00\u7f3a\u73af\u5883\u4e0b\u6784\u5efa\u9c81\u68d2\u63a8\u7406LLM\u5177\u6709\u6f5c\u529b\u3002"}}
{"id": "2505.13738", "pdf": "https://arxiv.org/pdf/2505.13738", "abs": "https://arxiv.org/abs/2505.13738", "authors": ["Shane Bergsma", "Nolan Dey", "Gurpreet Gosal", "Gavia Gray", "Daria Soboleva", "Joel Hestness"], "title": "Power Lines: Scaling Laws for Weight Decay and Batch Size in LLM Pre-training", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Efficient LLM pre-training requires well-tuned hyperparameters (HPs),\nincluding learning rate {\\eta} and weight decay {\\lambda}. We study scaling\nlaws for HPs: formulas for how to scale HPs as we scale model size N, dataset\nsize D, and batch size B. Recent work suggests the AdamW timescale,\nB/({\\eta}{\\lambda}D), should remain constant across training settings, and we\nverify the implication that optimal {\\lambda} scales linearly with B, for a\nfixed N,D. However, as N,D scale, we show the optimal timescale obeys a precise\npower law in the tokens-per-parameter ratio, D/N. This law thus provides a\nmethod to accurately predict {\\lambda}opt in advance of large-scale training.\nWe also study scaling laws for optimal batch size Bopt (the B enabling lowest\nloss at a given N,D) and critical batch size Bcrit (the B beyond which further\ndata parallelism becomes ineffective). In contrast with prior work, we find\nboth Bopt and Bcrit scale as power laws in D, independent of model size, N.\nFinally, we analyze how these findings inform the real-world selection of\nPareto-optimal N and D under dual training time and compute objectives.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86LLM\u9884\u8bad\u7ec3\u4e2d\u8d85\u53c2\u6570\uff08\u5982\u5b66\u4e60\u7387\u548c\u6743\u91cd\u8870\u51cf\uff09\u7684\u7f29\u653e\u89c4\u5f8b\uff0c\u53d1\u73b0\u6700\u4f18\u6743\u91cd\u8870\u51cf\u4e0e\u6279\u6b21\u5927\u5c0f\u7ebf\u6027\u76f8\u5173\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53c2\u6570\u4e0e\u6570\u636e\u6bd4\u4f8b\u7684\u5e42\u5f8b\u9884\u6d4b\u65b9\u6cd5\u3002\u540c\u65f6\uff0c\u63a2\u8ba8\u4e86\u6700\u4f18\u6279\u6b21\u5927\u5c0f\u548c\u4e34\u754c\u6279\u6b21\u5927\u5c0f\u7684\u7f29\u653e\u89c4\u5f8b\uff0c\u5e76\u5206\u6790\u4e86\u5176\u5bf9\u5b9e\u9645\u8bad\u7ec3\u4e2d\u8d44\u6e90\u5206\u914d\u7684\u6307\u5bfc\u610f\u4e49\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u5728\u5927\u89c4\u6a21\u6a21\u578b\u8bad\u7ec3\u4e2d\u9ad8\u6548\u8c03\u6574\u8d85\u53c2\u6570\uff0c\u4ee5\u51cf\u5c11\u8ba1\u7b97\u8d44\u6e90\u6d6a\u8d39\u5e76\u4f18\u5316\u8bad\u7ec3\u6548\u679c\u3002", "method": "\u901a\u8fc7\u5206\u6790\u6a21\u578b\u5927\u5c0f\uff08N\uff09\u3001\u6570\u636e\u96c6\u5927\u5c0f\uff08D\uff09\u548c\u6279\u6b21\u5927\u5c0f\uff08B\uff09\u7684\u7f29\u653e\u5173\u7cfb\uff0c\u63d0\u51fa\u8d85\u53c2\u6570\uff08\u5982\u6743\u91cd\u8870\u51cf\uff09\u7684\u5e42\u5f8b\u7f29\u653e\u89c4\u5f8b\uff0c\u5e76\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "result": "\u53d1\u73b0\u6700\u4f18\u6743\u91cd\u8870\u51cf\u4e0e\u6279\u6b21\u5927\u5c0f\u7ebf\u6027\u76f8\u5173\uff0c\u4e14\u6700\u4f18\u6279\u6b21\u5927\u5c0f\u548c\u4e34\u754c\u6279\u6b21\u5927\u5c0f\u4e0e\u6570\u636e\u96c6\u5927\u5c0f\u5448\u5e42\u5f8b\u5173\u7cfb\uff0c\u72ec\u7acb\u4e8e\u6a21\u578b\u5927\u5c0f\u3002", "conclusion": "\u7814\u7a76\u4e3a\u5927\u89c4\u6a21LLM\u9884\u8bad\u7ec3\u63d0\u4f9b\u4e86\u8d85\u53c2\u6570\u8c03\u6574\u7684\u7406\u8bba\u4f9d\u636e\uff0c\u6709\u52a9\u4e8e\u4f18\u5316\u8bad\u7ec3\u6548\u7387\u548c\u8d44\u6e90\u5206\u914d\u3002"}}
{"id": "2505.13757", "pdf": "https://arxiv.org/pdf/2505.13757", "abs": "https://arxiv.org/abs/2505.13757", "authors": ["Runchu Tian", "Xueqiang Xu", "Bowen Jin", "SeongKu Kang", "Jiawei Han"], "title": "LLM-Based Compact Reranking with Document Features for Scientific Retrieval", "categories": ["cs.IR", "cs.CL"], "comment": "17 pages, 4 figures", "summary": "Scientific retrieval is essential for advancing academic discovery. Within\nthis process, document reranking plays a critical role by refining first-stage\nretrieval results. However, large language model (LLM) listwise reranking faces\nunique challenges in the scientific domain. First-stage retrieval is often\nsuboptimal in the scientific domain, so relevant documents are ranked lower.\nMoreover, conventional listwise reranking uses the full text of candidate\ndocuments in the context window, limiting the number of candidates that can be\nconsidered. As a result, many relevant documents are excluded before reranking,\nwhich constrains overall retrieval performance. To address these challenges, we\nexplore compact document representations based on semantic features such as\ncategories, sections, and keywords, and propose a training-free, model-agnostic\nreranking framework for scientific retrieval called CoRank. The framework\ninvolves three stages: (i) offline extraction of document-level features, (ii)\ncoarse reranking using these compact representations, and (iii) fine-grained\nreranking on full texts of the top candidates from stage (ii). This hybrid\ndesign provides a high-level abstraction of document semantics, expands\ncandidate coverage, and retains critical details required for precise ranking.\nExperiments on LitSearch and CSFCube show that CoRank significantly improves\nreranking performance across different LLM backbones, increasing nDCG@10 from\n32.0 to 39.7. Overall, these results highlight the value of information\nextraction for reranking in scientific retrieval.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCoRank\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u7d27\u51d1\u6587\u6863\u8868\u793a\u548c\u6df7\u5408\u91cd\u6392\u5e8f\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u79d1\u5b66\u68c0\u7d22\u4e2dLLM\u5217\u8868\u91cd\u6392\u5e8f\u7684\u6311\u6218\u3002", "motivation": "\u79d1\u5b66\u68c0\u7d22\u4e2d\uff0cLLM\u5217\u8868\u91cd\u6392\u5e8f\u9762\u4e34\u5019\u9009\u6587\u6863\u6570\u91cf\u53d7\u9650\u548c\u76f8\u5173\u6587\u6863\u6392\u540d\u4f4e\u7684\u95ee\u9898\uff0c\u5f71\u54cd\u4e86\u68c0\u7d22\u6027\u80fd\u3002", "method": "CoRank\u6846\u67b6\u5206\u4e09\u9636\u6bb5\uff1a\u79bb\u7ebf\u63d0\u53d6\u6587\u6863\u7279\u5f81\u3001\u57fa\u4e8e\u7d27\u51d1\u8868\u793a\u7684\u7c97\u7c92\u5ea6\u91cd\u6392\u5e8f\u3001\u5bf9\u5019\u9009\u6587\u6863\u5168\u6587\u7684\u7ec6\u7c92\u5ea6\u91cd\u6392\u5e8f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCoRank\u663e\u8457\u63d0\u5347\u4e86\u91cd\u6392\u5e8f\u6027\u80fd\uff0cnDCG@10\u4ece32.0\u63d0\u9ad8\u523039.7\u3002", "conclusion": "\u4fe1\u606f\u63d0\u53d6\u5bf9\u79d1\u5b66\u68c0\u7d22\u4e2d\u7684\u91cd\u6392\u5e8f\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2505.13763", "pdf": "https://arxiv.org/pdf/2505.13763", "abs": "https://arxiv.org/abs/2505.13763", "authors": ["Li Ji-An", "Hua-Dong Xiong", "Robert C. Wilson", "Marcelo G. Mattar", "Marcus K. Benna"], "title": "Language Models Are Capable of Metacognitive Monitoring and Control of Their Internal Activations", "categories": ["cs.AI", "cs.CL", "q-bio.NC"], "comment": null, "summary": "Large language models (LLMs) can sometimes report the strategies they\nactually use to solve tasks, but they can also fail to do so. This suggests\nsome degree of metacognition -- the capacity to monitor one's own cognitive\nprocesses for subsequent reporting and self-control. Metacognitive abilities\nenhance AI capabilities but raise safety concerns, as models might obscure\ntheir internal processes to evade neural-activation-based oversight mechanisms\ndesigned to detect harmful behaviors. Given society's increased reliance on\nthese models, it is critical that we understand the limits of their\nmetacognitive abilities, particularly their ability to monitor their internal\nactivations. To address this, we introduce a neuroscience-inspired\nneurofeedback paradigm designed to quantify the ability of LLMs to explicitly\nreport and control their activation patterns. By presenting models with\nsentence-label pairs where labels correspond to sentence-elicited internal\nactivations along specific directions in the neural representation space, we\ndemonstrate that LLMs can learn to report and control these activations. The\nperformance varies with several factors: the number of example pairs provided,\nthe semantic interpretability of the target neural direction, and the variance\nexplained by that direction. These results reveal a \"metacognitive space\" with\ndimensionality much lower than the model's neural space, suggesting LLMs can\nmonitor only a subset of their neural mechanisms. Our findings provide\nempirical evidence quantifying metacognitive capabilities in LLMs, with\nsignificant implications for AI safety.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u5143\u8ba4\u77e5\u80fd\u529b\uff0c\u5373\u6a21\u578b\u76d1\u63a7\u548c\u62a5\u544a\u81ea\u8eab\u5185\u90e8\u6fc0\u6d3b\u6a21\u5f0f\u7684\u80fd\u529b\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u795e\u7ecf\u53cd\u9988\u8303\u5f0f\u6765\u91cf\u5316\u8fd9\u79cd\u80fd\u529b\u3002", "motivation": "\u968f\u7740\u793e\u4f1a\u5bf9LLMs\u7684\u4f9d\u8d56\u589e\u52a0\uff0c\u4e86\u89e3\u5176\u5143\u8ba4\u77e5\u80fd\u529b\u7684\u5c40\u9650\u6027\u5bf9AI\u5b89\u5168\u81f3\u5173\u91cd\u8981\uff0c\u5c24\u5176\u662f\u6a21\u578b\u53ef\u80fd\u9690\u85cf\u5185\u90e8\u8fc7\u7a0b\u4ee5\u9003\u907f\u76d1\u7ba1\u3002", "method": "\u91c7\u7528\u795e\u7ecf\u79d1\u5b66\u542f\u53d1\u7684\u795e\u7ecf\u53cd\u9988\u8303\u5f0f\uff0c\u901a\u8fc7\u53e5\u5b50-\u6807\u7b7e\u5bf9\u8bad\u7ec3\u6a21\u578b\u62a5\u544a\u548c\u63a7\u5236\u5176\u5185\u90e8\u6fc0\u6d3b\u6a21\u5f0f\u3002", "result": "LLMs\u80fd\u591f\u5b66\u4e60\u548c\u63a7\u5236\u7279\u5b9a\u795e\u7ecf\u65b9\u5411\u7684\u6fc0\u6d3b\uff0c\u4f46\u80fd\u529b\u53d7\u6837\u672c\u6570\u91cf\u3001\u76ee\u6807\u65b9\u5411\u7684\u8bed\u4e49\u53ef\u89e3\u91ca\u6027\u548c\u65b9\u5dee\u89e3\u91ca\u5ea6\u5f71\u54cd\u3002", "conclusion": "LLMs\u7684\u5143\u8ba4\u77e5\u7a7a\u95f4\u7ef4\u5ea6\u8fdc\u4f4e\u4e8e\u5176\u795e\u7ecf\u7a7a\u95f4\uff0c\u8868\u660e\u5176\u4ec5\u80fd\u76d1\u63a7\u90e8\u5206\u795e\u7ecf\u673a\u5236\uff0c\u8fd9\u5bf9AI\u5b89\u5168\u6709\u91cd\u8981\u542f\u793a\u3002"}}
{"id": "2505.13766", "pdf": "https://arxiv.org/pdf/2505.13766", "abs": "https://arxiv.org/abs/2505.13766", "authors": ["Avinash Patil"], "title": "Advancing Software Quality: A Standards-Focused Review of LLM-Based Assurance Techniques", "categories": ["cs.SE", "cs.AI", "cs.CL"], "comment": "16 pages, 1 Table, 6 Figures", "summary": "Software Quality Assurance (SQA) is critical for delivering reliable, secure,\nand efficient software products. The Software Quality Assurance Process aims to\nprovide assurance that work products and processes comply with predefined\nprovisions and plans. Recent advancements in Large Language Models (LLMs)\npresent new opportunities to enhance existing SQA processes by automating tasks\nlike requirement analysis, code review, test generation, and compliance checks.\nSimultaneously, established standards such as ISO/IEC 12207, ISO/IEC 25010,\nISO/IEC 5055, ISO 9001/ISO/IEC 90003, CMMI, and TMM provide structured\nframeworks for ensuring robust quality practices. This paper surveys the\nintersection of LLM-based SQA methods and these recognized standards,\nhighlighting how AI-driven solutions can augment traditional approaches while\nmaintaining compliance and process maturity. We first review the foundational\nsoftware quality standards and the technical fundamentals of LLMs in software\nengineering. Next, we explore various LLM-based SQA applications, including\nrequirement validation, defect detection, test generation, and documentation\nmaintenance. We then map these applications to key software quality frameworks,\nillustrating how LLMs can address specific requirements and metrics within each\nstandard. Empirical case studies and open-source initiatives demonstrate the\npractical viability of these methods. At the same time, discussions on\nchallenges (e.g., data privacy, model bias, explainability) underscore the need\nfor deliberate governance and auditing. Finally, we propose future directions\nencompassing adaptive learning, privacy-focused deployments, multimodal\nanalysis, and evolving standards for AI-driven software quality.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u589e\u5f3a\u8f6f\u4ef6\u8d28\u91cf\u4fdd\u8bc1\uff08SQA\uff09\u8fc7\u7a0b\uff0c\u540c\u65f6\u786e\u4fdd\u7b26\u5408\u56fd\u9645\u6807\u51c6\u3002", "motivation": "\u8f6f\u4ef6\u8d28\u91cf\u4fdd\u8bc1\u5bf9\u53ef\u9760\u3001\u5b89\u5168\u548c\u9ad8\u6548\u7684\u8f6f\u4ef6\u4ea7\u54c1\u81f3\u5173\u91cd\u8981\uff0c\u800cLLMs\u4e3a\u81ea\u52a8\u5316SQA\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u673a\u4f1a\u3002", "method": "\u7efc\u8ff0\u4e86LLM\u5728SQA\u4e2d\u7684\u5e94\u7528\uff08\u5982\u9700\u6c42\u9a8c\u8bc1\u3001\u7f3a\u9677\u68c0\u6d4b\u7b49\uff09\uff0c\u5e76\u5c06\u5176\u6620\u5c04\u5230ISO/IEC\u7b49\u6807\u51c6\u6846\u67b6\u4e2d\u3002", "result": "\u5b9e\u8bc1\u6848\u4f8b\u548c\u5f00\u6e90\u9879\u76ee\u9a8c\u8bc1\u4e86LLM\u65b9\u6cd5\u7684\u53ef\u884c\u6027\uff0c\u4f46\u4e5f\u6307\u51fa\u4e86\u6570\u636e\u9690\u79c1\u548c\u6a21\u578b\u504f\u89c1\u7b49\u6311\u6218\u3002", "conclusion": "\u672a\u6765\u65b9\u5411\u5305\u62ec\u81ea\u9002\u5e94\u5b66\u4e60\u3001\u9690\u79c1\u4fdd\u62a4\u90e8\u7f72\u548c\u591a\u6a21\u6001\u5206\u6790\uff0c\u4ee5\u63a8\u52a8AI\u9a71\u52a8\u7684SQA\u53d1\u5c55\u3002"}}
{"id": "2505.13770", "pdf": "https://arxiv.org/pdf/2505.13770", "abs": "https://arxiv.org/abs/2505.13770", "authors": ["Jin Du", "Li Chen", "Xun Xian", "An Luo", "Fangqiao Tian", "Ganghua Wang", "Charles Doss", "Xiaotong Shen", "Jie Ding"], "title": "Ice Cream Doesn't Cause Drowning: Benchmarking LLMs Against Statistical Pitfalls in Causal Inference", "categories": ["cs.AI", "cs.CL", "cs.LG", "stat.ME", "stat.ML", "62-08, 68T50, 68T05, 68T01, 68T07, 62-07, 68U35, 62C99", "I.2.7; I.2.6; I.2.0; I.5.1; I.5.4; F.2.2; H.2.8; G.3"], "comment": null, "summary": "Reliable causal inference is essential for making decisions in high-stakes\nareas like medicine, economics, and public policy. However, it remains unclear\nwhether large language models (LLMs) can handle rigorous and trustworthy\nstatistical causal inference. Current benchmarks usually involve simplified\ntasks. For example, these tasks might only ask LLMs to identify semantic causal\nrelationships or draw conclusions directly from raw data. As a result, models\nmay overlook important statistical pitfalls, such as Simpson's paradox or\nselection bias. This oversight limits the applicability of LLMs in the real\nworld. To address these limitations, we propose CausalPitfalls, a comprehensive\nbenchmark designed to rigorously evaluate the capability of LLMs in overcoming\ncommon causal inference pitfalls. Our benchmark features structured challenges\nacross multiple difficulty levels, each paired with grading rubrics. This\napproach allows us to quantitatively measure both causal reasoning capabilities\nand the reliability of LLMs' responses. We evaluate models using two protocols:\n(1) direct prompting, which assesses intrinsic causal reasoning, and (2)\ncode-assisted prompting, where models generate executable code for explicit\nstatistical analysis. Additionally, we validate the effectiveness of this judge\nby comparing its scoring with assessments from human experts. Our results\nreveal significant limitations in current LLMs when performing statistical\ncausal inference. The CausalPitfalls benchmark provides essential guidance and\nquantitative metrics to advance the development of trustworthy causal reasoning\nsystems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86CausalPitfalls\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u56e0\u679c\u63a8\u7406\u4e2d\u514b\u670d\u5e38\u89c1\u9677\u9631\u7684\u80fd\u529b\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u53ef\u9760\u56e0\u679c\u63a8\u7406\u5bf9\u9ad8\u98ce\u9669\u9886\u57df\u51b3\u7b56\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u57fa\u51c6\u8fc7\u4e8e\u7b80\u5316\uff0c\u65e0\u6cd5\u5168\u9762\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u7684\u80fd\u529b\u3002", "method": "\u8bbe\u8ba1\u4e86\u591a\u96be\u5ea6\u7ea7\u522b\u7684\u7ed3\u6784\u5316\u6311\u6218\uff0c\u7ed3\u5408\u8bc4\u5206\u6807\u51c6\uff0c\u91c7\u7528\u76f4\u63a5\u63d0\u793a\u548c\u4ee3\u7801\u8f85\u52a9\u63d0\u793a\u4e24\u79cd\u534f\u8bae\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7edf\u8ba1\u56e0\u679c\u63a8\u7406\u4e2d\u5b58\u5728\u663e\u8457\u5c40\u9650\u6027\u3002", "conclusion": "CausalPitfalls\u4e3a\u5f00\u53d1\u53ef\u4fe1\u8d56\u7684\u56e0\u679c\u63a8\u7406\u7cfb\u7edf\u63d0\u4f9b\u4e86\u91cd\u8981\u6307\u5bfc\u548c\u91cf\u5316\u6307\u6807\u3002"}}
{"id": "2505.13820", "pdf": "https://arxiv.org/pdf/2505.13820", "abs": "https://arxiv.org/abs/2505.13820", "authors": ["Jun Liu", "Zhenglun Kong", "Peiyan Dong", "Changdi Yang", "Tianqi Li", "Hao Tang", "Geng Yuan", "Wei Niu", "Wenbin Zhang", "Pu Zhao", "Xue Lin", "Dong Huang", "Yanzhi Wang"], "title": "Structured Agent Distillation for Large Language Model", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) exhibit strong capabilities as decision-making\nagents by interleaving reasoning and actions, as seen in ReAct-style\nframeworks. Yet, their practical deployment is constrained by high inference\ncosts and large model sizes. We propose Structured Agent Distillation, a\nframework that compresses large LLM-based agents into smaller student models\nwhile preserving both reasoning fidelity and action consistency. Unlike\nstandard token-level distillation, our method segments trajectories into\n{[REASON]} and {[ACT]} spans, applying segment-specific losses to align each\ncomponent with the teacher's behavior. This structure-aware supervision enables\ncompact agents to better replicate the teacher's decision process. Experiments\non ALFWorld, HotPotQA-ReAct, and WebShop show that our approach consistently\noutperforms token-level and imitation learning baselines, achieving significant\ncompression with minimal performance drop. Scaling and ablation results further\nhighlight the importance of span-level alignment for efficient and deployable\nagents.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aStructured Agent Distillation\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u6bb5\u76d1\u7763\u538b\u7f29\u5927\u578bLLM\u4ee3\u7406\uff0c\u4fdd\u7559\u63a8\u7406\u548c\u884c\u52a8\u4e00\u81f4\u6027\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4f5c\u4e3a\u51b3\u7b56\u4ee3\u7406\u6210\u672c\u9ad8\u4e14\u6a21\u578b\u5927\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u90e8\u7f72\u3002", "method": "\u5c06\u8f68\u8ff9\u5206\u6bb5\u4e3a{[REASON]}\u548c{[ACT]}\uff0c\u5e94\u7528\u5206\u6bb5\u7279\u5b9a\u635f\u5931\u5bf9\u9f50\u6559\u5e08\u884c\u4e3a\u3002", "result": "\u5728ALFWorld\u3001HotPotQA-ReAct\u548cWebShop\u4e0a\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\uff0c\u5b9e\u73b0\u9ad8\u6548\u538b\u7f29\u4e14\u6027\u80fd\u4e0b\u964d\u6700\u5c0f\u3002", "conclusion": "\u5206\u6bb5\u5bf9\u9f50\u5bf9\u9ad8\u6548\u53ef\u90e8\u7f72\u4ee3\u7406\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2505.13847", "pdf": "https://arxiv.org/pdf/2505.13847", "abs": "https://arxiv.org/abs/2505.13847", "authors": ["Tianle Yang", "Chengzhe Sun", "Siwei Lyu", "Phil Rose"], "title": "Forensic deepfake audio detection using segmental speech features", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "comment": null, "summary": "This study explores the potential of using acoustic features of segmental\nspeech sounds to detect deepfake audio. These features are highly interpretable\nbecause of their close relationship with human articulatory processes and are\nexpected to be more difficult for deepfake models to replicate. The results\ndemonstrate that certain segmental features commonly used in forensic voice\ncomparison are effective in identifying deep-fakes, whereas some global\nfeatures provide little value. These findings underscore the need to approach\naudio deepfake detection differently for forensic voice comparison and offer a\nnew perspective on leveraging segmental features for this purpose.", "AI": {"tldr": "\u5229\u7528\u8bed\u97f3\u5206\u6bb5\u58f0\u5b66\u7279\u5f81\u68c0\u6d4b\u6df1\u5ea6\u4f2a\u9020\u97f3\u9891\uff0c\u6548\u679c\u663e\u8457\u4e14\u53ef\u89e3\u91ca\u6027\u5f3a\u3002", "motivation": "\u6df1\u5ea6\u4f2a\u9020\u97f3\u9891\u68c0\u6d4b\u9700\u8981\u66f4\u53ef\u9760\u4e14\u53ef\u89e3\u91ca\u7684\u7279\u5f81\uff0c\u5206\u6bb5\u7279\u5f81\u56e0\u5176\u4e0e\u4eba\u7c7b\u53d1\u97f3\u8fc7\u7a0b\u7684\u7d27\u5bc6\u5173\u8054\u800c\u6210\u4e3a\u7406\u60f3\u9009\u62e9\u3002", "method": "\u7814\u7a76\u901a\u8fc7\u5206\u6790\u5206\u6bb5\u8bed\u97f3\u58f0\u5b66\u7279\u5f81\uff0c\u8bc4\u4f30\u5176\u5728\u6df1\u5ea6\u4f2a\u9020\u97f3\u9891\u68c0\u6d4b\u4e2d\u7684\u6709\u6548\u6027\u3002", "result": "\u67d0\u4e9b\u5e38\u7528\u4e8e\u53f8\u6cd5\u8bed\u97f3\u6bd4\u5bf9\u7684\u5c40\u90e8\u7279\u5f81\u5bf9\u68c0\u6d4b\u6df1\u5ea6\u4f2a\u9020\u97f3\u9891\u6709\u6548\uff0c\u800c\u5168\u5c40\u7279\u5f81\u6548\u679c\u8f83\u5dee\u3002", "conclusion": "\u7814\u7a76\u4e3a\u97f3\u9891\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u5f3a\u8c03\u5206\u6bb5\u7279\u5f81\u7684\u91cd\u8981\u6027\uff0c\u5e76\u6307\u51fa\u9700\u9488\u5bf9\u4e0d\u540c\u5e94\u7528\u573a\u666f\u8c03\u6574\u68c0\u6d4b\u65b9\u6cd5\u3002"}}
{"id": "2505.13862", "pdf": "https://arxiv.org/pdf/2505.13862", "abs": "https://arxiv.org/abs/2505.13862", "authors": ["Guobin Shen", "Dongcheng Zhao", "Linghao Feng", "Xiang He", "Jihang Wang", "Sicheng Shen", "Haibo Tong", "Yiting Dong", "Jindong Li", "Xiang Zheng", "Yi Zeng"], "title": "PandaGuard: Systematic Evaluation of LLM Safety in the Era of Jailbreaking Attacks", "categories": ["cs.CR", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) have achieved remarkable capabilities but remain\nvulnerable to adversarial prompts known as jailbreaks, which can bypass safety\nalignment and elicit harmful outputs. Despite growing efforts in LLM safety\nresearch, existing evaluations are often fragmented, focused on isolated attack\nor defense techniques, and lack systematic, reproducible analysis. In this\nwork, we introduce PandaGuard, a unified and modular framework that models LLM\njailbreak safety as a multi-agent system comprising attackers, defenders, and\njudges. Our framework implements 19 attack methods and 12 defense mechanisms,\nalong with multiple judgment strategies, all within a flexible plugin\narchitecture supporting diverse LLM interfaces, multiple interaction modes, and\nconfiguration-driven experimentation that enhances reproducibility and\npractical deployment. Built on this framework, we develop PandaBench, a\ncomprehensive benchmark that evaluates the interactions between these\nattack/defense methods across 49 LLMs and various judgment approaches,\nrequiring over 3 billion tokens to execute. Our extensive evaluation reveals\nkey insights into model vulnerabilities, defense cost-performance trade-offs,\nand judge consistency. We find that no single defense is optimal across all\ndimensions and that judge disagreement introduces nontrivial variance in safety\nassessments. We release the code, configurations, and evaluation results to\nsupport transparent and reproducible research in LLM safety.", "AI": {"tldr": "PandaGuard\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u591a\u4ee3\u7406\u6846\u67b6\uff0c\u7528\u4e8e\u7cfb\u7edf\u8bc4\u4f30LLM\u7684\u8d8a\u72f1\u5b89\u5168\u6027\uff0c\u5305\u542b\u591a\u79cd\u653b\u51fb\u3001\u9632\u5fa1\u548c\u5224\u65ad\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7PandaBench\u57fa\u51c6\u6d4b\u8bd5\u63ed\u793a\u5173\u952e\u89c1\u89e3\u3002", "motivation": "\u5c3d\u7ba1LLM\u80fd\u529b\u663e\u8457\uff0c\u4f46\u5176\u6613\u53d7\u8d8a\u72f1\u653b\u51fb\uff0c\u73b0\u6709\u8bc4\u4f30\u7f3a\u4e4f\u7cfb\u7edf\u6027\u548c\u53ef\u91cd\u590d\u6027\u3002", "method": "\u63d0\u51faPandaGuard\u6846\u67b6\uff0c\u96c6\u621019\u79cd\u653b\u51fb\u65b9\u6cd5\u300112\u79cd\u9632\u5fa1\u673a\u5236\u53ca\u591a\u79cd\u5224\u65ad\u7b56\u7565\uff0c\u652f\u6301\u7075\u6d3b\u914d\u7f6e\u548c\u5b9e\u9a8c\u3002", "result": "\u8bc4\u4f3049\u79cdLLM\uff0c\u53d1\u73b0\u9632\u5fa1\u65e0\u5355\u4e00\u6700\u4f18\u65b9\u6848\uff0c\u5224\u65ad\u4e0d\u4e00\u81f4\u6027\u5f71\u54cd\u5b89\u5168\u6027\u8bc4\u4f30\u3002", "conclusion": "\u53d1\u5e03\u4ee3\u7801\u548c\u7ed3\u679c\uff0c\u652f\u6301\u900f\u660e\u53ef\u91cd\u590d\u7684LLM\u5b89\u5168\u7814\u7a76\u3002"}}
{"id": "2505.13878", "pdf": "https://arxiv.org/pdf/2505.13878", "abs": "https://arxiv.org/abs/2505.13878", "authors": ["Yanggan Gu", "Zhaoyi Yan", "Yuanyi Wang", "Yiming Zhang", "Qi Zhou", "Fei Wu", "Hongxia Yang"], "title": "InfiFPO: Implicit Model Fusion via Preference Optimization in Large Language Models", "categories": ["cs.LG", "cs.CL"], "comment": "17 pages", "summary": "Model fusion combines multiple Large Language Models (LLMs) with different\nstrengths into a more powerful, integrated model through lightweight training\nmethods. Existing works on model fusion focus primarily on supervised\nfine-tuning (SFT), leaving preference alignment (PA) --a critical phase for\nenhancing LLM performance--largely unexplored. The current few fusion methods\non PA phase, like WRPO, simplify the process by utilizing only response outputs\nfrom source models while discarding their probability information. To address\nthis limitation, we propose InfiFPO, a preference optimization method for\nimplicit model fusion. InfiFPO replaces the reference model in Direct\nPreference Optimization (DPO) with a fused source model that synthesizes\nmulti-source probabilities at the sequence level, circumventing complex\nvocabulary alignment challenges in previous works and meanwhile maintaining the\nprobability information. By introducing probability clipping and max-margin\nfusion strategies, InfiFPO enables the pivot model to align with human\npreferences while effectively distilling knowledge from source models.\nComprehensive experiments on 11 widely-used benchmarks demonstrate that InfiFPO\nconsistently outperforms existing model fusion and preference optimization\nmethods. When using Phi-4 as the pivot model, InfiFPO improve its average\nperformance from 79.95 to 83.33 on 11 benchmarks, significantly improving its\ncapabilities in mathematics, coding, and reasoning tasks.", "AI": {"tldr": "InfiFPO\u662f\u4e00\u79cd\u9690\u5f0f\u6a21\u578b\u878d\u5408\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u591a\u6e90\u6982\u7387\u4fe1\u606f\u4f18\u5316\u504f\u597d\u5bf9\u9f50\uff0c\u663e\u8457\u63d0\u5347LLM\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u878d\u5408\u65b9\u6cd5\u5728\u504f\u597d\u5bf9\u9f50\u9636\u6bb5\u4ec5\u5229\u7528\u54cd\u5e94\u8f93\u51fa\u800c\u5ffd\u7565\u6982\u7387\u4fe1\u606f\uff0c\u9650\u5236\u4e86\u6027\u80fd\u63d0\u5347\u3002", "method": "InfiFPO\u5728DPO\u4e2d\u66ff\u6362\u53c2\u8003\u6a21\u578b\u4e3a\u878d\u5408\u591a\u6e90\u6982\u7387\u7684\u6a21\u578b\uff0c\u5f15\u5165\u6982\u7387\u88c1\u526a\u548c\u6700\u5927\u95f4\u9694\u878d\u5408\u7b56\u7565\u3002", "result": "\u572811\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cInfiFPO\u5e73\u5747\u6027\u80fd\u4ece79.95\u63d0\u5347\u81f383.33\uff0c\u5c24\u5176\u5728\u6570\u5b66\u3001\u7f16\u7801\u548c\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "InfiFPO\u901a\u8fc7\u4fdd\u7559\u6982\u7387\u4fe1\u606f\u548c\u4f18\u5316\u878d\u5408\u7b56\u7565\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e3aLLM\u878d\u5408\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.13887", "pdf": "https://arxiv.org/pdf/2505.13887", "abs": "https://arxiv.org/abs/2505.13887", "authors": ["Junyang Wang", "Haiyang Xu", "Xi Zhang", "Ming Yan", "Ji Zhang", "Fei Huang", "Jitao Sang"], "title": "Mobile-Agent-V: A Video-Guided Approach for Effortless and Efficient Operational Knowledge Injection in Mobile Automation", "categories": ["cs.AI", "cs.CL"], "comment": "17 pages, 7 figures, 9 tables. arXiv admin note: substantial text\n  overlap with arXiv:2502.17110", "summary": "The exponential rise in mobile device usage necessitates streamlined\nautomation for effective task management, yet many AI frameworks fall short due\nto inadequate operational expertise. While manually written knowledge can\nbridge this gap, it is often burdensome and inefficient. We introduce\nMobile-Agent-V, an innovative framework that utilizes video as a guiding tool\nto effortlessly and efficiently inject operational knowledge into mobile\nautomation processes. By deriving knowledge directly from video content,\nMobile-Agent-V eliminates manual intervention, significantly reducing the\neffort and time required for knowledge acquisition. To rigorously evaluate this\napproach, we propose Mobile-Knowledge, a benchmark tailored to assess the\nimpact of external knowledge on mobile agent performance. Our experimental\nfindings demonstrate that Mobile-Agent-V enhances performance by 36% compared\nto existing methods, underscoring its effortless and efficient advantages in\nmobile automation.", "AI": {"tldr": "Mobile-Agent-V\u5229\u7528\u89c6\u9891\u4f5c\u4e3a\u6307\u5bfc\u5de5\u5177\uff0c\u81ea\u52a8\u6ce8\u5165\u64cd\u4f5c\u77e5\u8bc6\uff0c\u663e\u8457\u63d0\u5347\u79fb\u52a8\u81ea\u52a8\u5316\u6548\u7387\uff0c\u6027\u80fd\u63d0\u534736%\u3002", "motivation": "\u79fb\u52a8\u8bbe\u5907\u4f7f\u7528\u6fc0\u589e\uff0c\u73b0\u6709AI\u6846\u67b6\u56e0\u7f3a\u4e4f\u64cd\u4f5c\u77e5\u8bc6\u800c\u6548\u7387\u4e0d\u8db3\uff0c\u624b\u52a8\u6ce8\u5165\u77e5\u8bc6\u53c8\u8fc7\u4e8e\u7e41\u7410\u3002", "method": "\u901a\u8fc7\u89c6\u9891\u5185\u5bb9\u76f4\u63a5\u63d0\u53d6\u64cd\u4f5c\u77e5\u8bc6\uff0c\u907f\u514d\u4eba\u5de5\u5e72\u9884\uff0c\u63d0\u51faMobile-Knowledge\u57fa\u51c6\u8bc4\u4f30\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u663e\u793aMobile-Agent-V\u6027\u80fd\u63d0\u534736%\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "Mobile-Agent-V\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u65e0\u9700\u4eba\u5de5\u5e72\u9884\u7684\u79fb\u52a8\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.13909", "pdf": "https://arxiv.org/pdf/2505.13909", "abs": "https://arxiv.org/abs/2505.13909", "authors": ["Yanheng He", "Jiahe Jin", "Pengfei Liu"], "title": "Efficient Agent Training for Computer Use", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "We open-source our entire suite of code, data, and models to\n  facilitate future research at https://github.com/GAIR-NLP/PC-Agent-E", "summary": "Scaling up high-quality trajectory data has long been a critical bottleneck\nfor developing human-like computer use agents. We introduce PC Agent-E, an\nefficient agent training framework that significantly reduces reliance on\nlarge-scale human demonstrations. Starting with just 312 human-annotated\ncomputer use trajectories, we further improved data quality by synthesizing\ndiverse action decisions with Claude 3.7 Sonnet. Trained on these enriched\ntrajectories, our PC Agent-E model achieved a remarkable 141% relative\nimprovement, surpassing the strong Claude 3.7 Sonnet with extended thinking on\nWindowsAgentArena-V2, an improved benchmark we also released. Furthermore, PC\nAgent-E demonstrates strong generalizability to different operating systems on\nOSWorld. Our findings suggest that strong computer use capabilities can be\nstimulated from a small amount of high-quality trajectory data.", "AI": {"tldr": "PC Agent-E\u6846\u67b6\u901a\u8fc7\u5c11\u91cf\u9ad8\u8d28\u91cf\u8f68\u8ff9\u6570\u636e\u663e\u8457\u63d0\u5347\u8ba1\u7b97\u673a\u4f7f\u7528\u4ee3\u7406\u6027\u80fd\uff0c\u51cf\u5c11\u5bf9\u4eba\u7c7b\u6f14\u793a\u7684\u4f9d\u8d56\u3002", "motivation": "\u89e3\u51b3\u9ad8\u8d28\u91cf\u8f68\u8ff9\u6570\u636e\u7a00\u7f3a\u5bf9\u5f00\u53d1\u4eba\u7c7b\u8ba1\u7b97\u673a\u4f7f\u7528\u4ee3\u7406\u7684\u9650\u5236\u3002", "method": "\u5229\u7528312\u6761\u4eba\u5de5\u6807\u6ce8\u8f68\u8ff9\uff0c\u7ed3\u5408Claude 3.7 Sonnet\u5408\u6210\u591a\u6837\u5316\u52a8\u4f5c\u51b3\u7b56\uff0c\u8bad\u7ec3PC Agent-E\u6a21\u578b\u3002", "result": "\u5728WindowsAgentArena-V2\u57fa\u51c6\u4e0a\u76f8\u5bf9\u63d0\u5347141%\uff0c\u5e76\u5728OSWorld\u4e0a\u5c55\u793a\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u5c11\u91cf\u9ad8\u8d28\u91cf\u8f68\u8ff9\u6570\u636e\u53ef\u6fc0\u53d1\u5f3a\u5927\u7684\u8ba1\u7b97\u673a\u4f7f\u7528\u80fd\u529b\u3002"}}
{"id": "2505.13941", "pdf": "https://arxiv.org/pdf/2505.13941", "abs": "https://arxiv.org/abs/2505.13941", "authors": ["Haoyang Fang", "Boran Han", "Nick Erickson", "Xiyuan Zhang", "Su Zhou", "Anirudh Dagar", "Jiani Zhang", "Ali Caner Turkmen", "Cuixiong Hu", "Huzefa Rangwala", "Ying Nian Wu", "Bernie Wang", "George Karypis"], "title": "MLZero: A Multi-Agent System for End-to-end Machine Learning Automation", "categories": ["cs.MA", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Existing AutoML systems have advanced the automation of machine learning\n(ML); however, they still require substantial manual configuration and expert\ninput, particularly when handling multimodal data. We introduce MLZero, a novel\nmulti-agent framework powered by Large Language Models (LLMs) that enables\nend-to-end ML automation across diverse data modalities with minimal human\nintervention. A cognitive perception module is first employed, transforming raw\nmultimodal inputs into perceptual context that effectively guides the\nsubsequent workflow. To address key limitations of LLMs, such as hallucinated\ncode generation and outdated API knowledge, we enhance the iterative code\ngeneration process with semantic and episodic memory. MLZero demonstrates\nsuperior performance on MLE-Bench Lite, outperforming all competitors in both\nsuccess rate and solution quality, securing six gold medals. Additionally, when\nevaluated on our Multimodal AutoML Agent Benchmark, which includes 25 more\nchallenging tasks spanning diverse data modalities, MLZero outperforms the\ncompeting methods by a large margin with a success rate of 0.92 (+263.6\\%) and\nan average rank of 2.28. Our approach maintains its robust effectiveness even\nwith a compact 8B LLM, outperforming full-size systems from existing solutions.", "AI": {"tldr": "MLZero\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u591a\u6a21\u6001\u6570\u636e\u7684\u7aef\u5230\u7aef\u81ea\u52a8\u5316\u673a\u5668\u5b66\u4e60\uff0c\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709AutoML\u7cfb\u7edf\u5728\u591a\u6a21\u6001\u6570\u636e\u5904\u7406\u4e2d\u4ecd\u9700\u5927\u91cf\u4eba\u5de5\u914d\u7f6e\u548c\u4e13\u5bb6\u8f93\u5165\uff0cMLZero\u65e8\u5728\u5b9e\u73b0\u6700\u5c0f\u5316\u4eba\u5de5\u5e72\u9884\u7684\u81ea\u52a8\u5316\u3002", "method": "\u91c7\u7528\u8ba4\u77e5\u611f\u77e5\u6a21\u5757\u5c06\u591a\u6a21\u6001\u8f93\u5165\u8f6c\u5316\u4e3a\u611f\u77e5\u4e0a\u4e0b\u6587\uff0c\u5e76\u901a\u8fc7\u8bed\u4e49\u548c\u60c5\u666f\u8bb0\u5fc6\u589e\u5f3a\u8fed\u4ee3\u4ee3\u7801\u751f\u6210\uff0c\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5c40\u9650\u6027\u3002", "result": "\u5728MLE-Bench Lite\u548cMultimodal AutoML Agent Benchmark\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u6210\u529f\u7387\u548c\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\u5747\u663e\u8457\u9886\u5148\u3002", "conclusion": "MLZero\u5728\u591a\u6a21\u6001AutoML\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5373\u4f7f\u4f7f\u7528\u5c0f\u578bLLM\u4e5f\u80fd\u8d85\u8d8a\u73b0\u6709\u7cfb\u7edf\u3002"}}
{"id": "2505.13957", "pdf": "https://arxiv.org/pdf/2505.13957", "abs": "https://arxiv.org/abs/2505.13957", "authors": ["Jiankun Zhang", "Shenglai Zeng", "Jie Ren", "Tianqi Zheng", "Hui Liu", "Xianfeng Tang", "Hui Liu", "Yi Chang"], "title": "Beyond Text: Unveiling Privacy Vulnerabilities in Multi-modal Retrieval-Augmented Generation", "categories": ["cs.CR", "cs.CL"], "comment": null, "summary": "Multimodal Retrieval-Augmented Generation (MRAG) systems enhance LMMs by\nintegrating external multimodal databases, but introduce unexplored privacy\nvulnerabilities. While text-based RAG privacy risks have been studied,\nmultimodal data presents unique challenges. We provide the first systematic\nanalysis of MRAG privacy vulnerabilities across vision-language and\nspeech-language modalities. Using a novel compositional structured prompt\nattack in a black-box setting, we demonstrate how attackers can extract private\ninformation by manipulating queries. Our experiments reveal that LMMs can both\ndirectly generate outputs resembling retrieved content and produce descriptions\nthat indirectly expose sensitive information, highlighting the urgent need for\nrobust privacy-preserving MRAG techniques.", "AI": {"tldr": "MRAG\u7cfb\u7edf\u901a\u8fc7\u6574\u5408\u5916\u90e8\u591a\u6a21\u6001\u6570\u636e\u5e93\u589e\u5f3aLMMs\uff0c\u4f46\u5f15\u5165\u672a\u63a2\u7d22\u7684\u9690\u79c1\u6f0f\u6d1e\u3002\u672c\u6587\u9996\u6b21\u7cfb\u7edf\u5206\u6790\u4e86MRAG\u5728\u89c6\u89c9-\u8bed\u8a00\u548c\u8bed\u97f3-\u8bed\u8a00\u6a21\u6001\u4e2d\u7684\u9690\u79c1\u98ce\u9669\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u9ed1\u76d2\u653b\u51fb\u65b9\u6cd5\u3002", "motivation": "\u7814\u7a76MRAG\u7cfb\u7edf\u4e2d\u7684\u9690\u79c1\u6f0f\u6d1e\uff0c\u586b\u8865\u591a\u6a21\u6001\u6570\u636e\u9690\u79c1\u98ce\u9669\u7684\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u4f7f\u7528\u65b0\u9896\u7684\u7ec4\u5408\u7ed3\u6784\u5316\u63d0\u793a\u653b\u51fb\u65b9\u6cd5\uff0c\u5728\u9ed1\u76d2\u8bbe\u7f6e\u4e0b\u6d4b\u8bd5\u9690\u79c1\u6f0f\u6d1e\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLMMs\u53ef\u76f4\u63a5\u751f\u6210\u7c7b\u4f3c\u68c0\u7d22\u5185\u5bb9\u7684\u8f93\u51fa\uff0c\u6216\u95f4\u63a5\u66b4\u9732\u654f\u611f\u4fe1\u606f\u3002", "conclusion": "\u4e9f\u9700\u5f00\u53d1\u9c81\u68d2\u7684\u9690\u79c1\u4fdd\u62a4MRAG\u6280\u672f\u3002"}}
{"id": "2505.14038", "pdf": "https://arxiv.org/pdf/2505.14038", "abs": "https://arxiv.org/abs/2505.14038", "authors": ["Xinzhe Zheng", "Sijie Ji", "Jiawei Sun", "Renqi Chen", "Wei Gao", "Mani Srivastava"], "title": "ProMind-LLM: Proactive Mental Health Care via Causal Reasoning with Sensor Data", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Mental health risk is a critical global public health challenge,\nnecessitating innovative and reliable assessment methods. With the development\nof large language models (LLMs), they stand out to be a promising tool for\nexplainable mental health care applications. Nevertheless, existing approaches\npredominantly rely on subjective textual mental records, which can be distorted\nby inherent mental uncertainties, leading to inconsistent and unreliable\npredictions. To address these limitations, this paper introduces ProMind-LLM.\nWe investigate an innovative approach integrating objective behavior data as\ncomplementary information alongside subjective mental records for robust mental\nhealth risk assessment. Specifically, ProMind-LLM incorporates a comprehensive\npipeline that includes domain-specific pretraining to tailor the LLM for mental\nhealth contexts, a self-refine mechanism to optimize the processing of\nnumerical behavioral data, and causal chain-of-thought reasoning to enhance the\nreliability and interpretability of its predictions. Evaluations of two\nreal-world datasets, PMData and Globem, demonstrate the effectiveness of our\nproposed methods, achieving substantial improvements over general LLMs. We\nanticipate that ProMind-LLM will pave the way for more dependable,\ninterpretable, and scalable mental health case solutions.", "AI": {"tldr": "ProMind-LLM\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4e3b\u89c2\u5fc3\u7406\u8bb0\u5f55\u548c\u5ba2\u89c2\u884c\u4e3a\u6570\u636e\u7684\u521b\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u53ef\u9760\u7684\u5fc3\u7406\u5065\u5eb7\u98ce\u9669\u8bc4\u4f30\uff0c\u901a\u8fc7\u9886\u57df\u7279\u5b9a\u9884\u8bad\u7ec3\u3001\u81ea\u6211\u4f18\u5316\u673a\u5236\u548c\u56e0\u679c\u94fe\u5f0f\u63a8\u7406\u63d0\u5347\u9884\u6d4b\u7684\u53ef\u9760\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u73b0\u6709\u5fc3\u7406\u5065\u5eb7\u98ce\u9669\u8bc4\u4f30\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u4e3b\u89c2\u6587\u672c\u8bb0\u5f55\uff0c\u6613\u53d7\u5fc3\u7406\u4e0d\u786e\u5b9a\u6027\u5f71\u54cd\uff0c\u5bfc\u81f4\u9884\u6d4b\u4e0d\u4e00\u81f4\u4e14\u4e0d\u53ef\u9760\u3002", "method": "ProMind-LLM\u6574\u5408\u4e86\u9886\u57df\u7279\u5b9a\u9884\u8bad\u7ec3\u3001\u81ea\u6211\u4f18\u5316\u673a\u5236\u548c\u56e0\u679c\u94fe\u5f0f\u63a8\u7406\uff0c\u7ed3\u5408\u4e3b\u89c2\u5fc3\u7406\u8bb0\u5f55\u548c\u5ba2\u89c2\u884c\u4e3a\u6570\u636e\u3002", "result": "\u5728PMData\u548cGlobem\u6570\u636e\u96c6\u4e0a\uff0cProMind-LLM\u8868\u73b0\u4f18\u4e8e\u901a\u7528\u5927\u8bed\u8a00\u6a21\u578b\u3002", "conclusion": "ProMind-LLM\u4e3a\u5fc3\u7406\u5065\u5eb7\u9886\u57df\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u3001\u53ef\u89e3\u91ca\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.14146", "pdf": "https://arxiv.org/pdf/2505.14146", "abs": "https://arxiv.org/abs/2505.14146", "authors": ["Pengcheng Jiang", "Xueqiang Xu", "Jiacheng Lin", "Jinfeng Xiao", "Zifeng Wang", "Jimeng Sun", "Jiawei Han"], "title": "s3: You Don't Need That Much Data to Train a Search Agent via RL", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Retrieval-augmented generation (RAG) systems empower large language models\n(LLMs) to access external knowledge during inference. Recent advances have\nenabled LLMs to act as search agents via reinforcement learning (RL), improving\ninformation acquisition through multi-turn interactions with retrieval engines.\nHowever, existing approaches either optimize retrieval using search-only\nmetrics (e.g., NDCG) that ignore downstream utility or fine-tune the entire LLM\nto jointly reason and retrieve-entangling retrieval with generation and\nlimiting the real search utility and compatibility with frozen or proprietary\nmodels. In this work, we propose s3, a lightweight, model-agnostic framework\nthat decouples the searcher from the generator and trains the searcher using a\nGain Beyond RAG reward: the improvement in generation accuracy over naive RAG.\ns3 requires only 2.4k training samples to outperform baselines trained on over\n70x more data, consistently delivering stronger downstream performance across\nsix general QA and five medical QA benchmarks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3as3\u7684\u8f7b\u91cf\u7ea7\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u68c0\u7d22\u4e0e\u751f\u6210\u4efb\u52a1\uff0c\u5e76\u4f7f\u7528Gain Beyond RAG\u5956\u52b1\u8bad\u7ec3\u68c0\u7d22\u5668\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u5ffd\u89c6\u4e0b\u6e38\u4efb\u52a1\u7684\u6548\u7528\uff0c\u8981\u4e48\u5c06\u68c0\u7d22\u4e0e\u751f\u6210\u8026\u5408\uff0c\u9650\u5236\u4e86\u68c0\u7d22\u7684\u5b9e\u7528\u6027\u548c\u517c\u5bb9\u6027\u3002", "method": "\u63d0\u51fas3\u6846\u67b6\uff0c\u89e3\u8026\u68c0\u7d22\u5668\u4e0e\u751f\u6210\u5668\uff0c\u4f7f\u7528Gain Beyond RAG\u5956\u52b1\u8bad\u7ec3\u68c0\u7d22\u5668\u3002", "result": "\u4ec5\u97002.4k\u8bad\u7ec3\u6837\u672c\u5373\u53ef\u8d85\u8d8a\u57fa\u7ebf\uff0c\u5728\u591a\u4e2aQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "s3\u6846\u67b6\u5728\u63d0\u5347\u751f\u6210\u51c6\u786e\u6027\u548c\u517c\u5bb9\u6027\u65b9\u9762\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2505.14185", "pdf": "https://arxiv.org/pdf/2505.14185", "abs": "https://arxiv.org/abs/2505.14185", "authors": ["Kaustubh Ponkshe", "Shaan Shah", "Raghav Singhal", "Praneeth Vepakomma"], "title": "Safety Subspaces are Not Distinct: A Fine-Tuning Case Study", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Kaustubh Ponkshe, Shaan Shah, and Raghav Singhal contributed equally\n  to this work", "summary": "Large Language Models (LLMs) rely on safety alignment to produce socially\nacceptable responses. This is typically achieved through instruction tuning and\nreinforcement learning from human feedback. However, this alignment is known to\nbe brittle: further fine-tuning, even on benign or lightly contaminated data,\ncan degrade safety and reintroduce harmful behaviors. A growing body of work\nsuggests that alignment may correspond to identifiable geometric directions in\nweight space, forming subspaces that could, in principle, be isolated or\npreserved to defend against misalignment. In this work, we conduct a\ncomprehensive empirical study of this geometric perspective. We examine whether\nsafety-relevant behavior is concentrated in specific subspaces, whether it can\nbe separated from general-purpose learning, and whether harmfulness arises from\ndistinguishable patterns in internal representations. Across both parameter and\nactivation space, our findings are consistent: subspaces that amplify safe\nbehaviors also amplify unsafe ones, and prompts with different safety\nimplications activate overlapping representations. We find no evidence of a\nsubspace that selectively governs safety. These results challenge the\nassumption that alignment is geometrically localized. Rather than residing in\ndistinct directions, safety appears to emerge from entangled, high-impact\ncomponents of the model's broader learning dynamics. This suggests that\nsubspace-based defenses may face fundamental limitations and underscores the\nneed for alternative strategies to preserve alignment under continued training.\nWe corroborate these findings through multiple experiments on five open-source\nLLMs. Our code is publicly available at:\nhttps://github.com/CERT-Lab/safety-subspaces.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5b89\u5168\u5bf9\u9f50\u884c\u4e3a\u5e76\u4e0d\u96c6\u4e2d\u5728\u7279\u5b9a\u51e0\u4f55\u5b50\u7a7a\u95f4\u4e2d\uff0c\u800c\u662f\u4e0e\u6a21\u578b\u7684\u5e7f\u6cdb\u5b66\u4e60\u52a8\u6001\u9ad8\u5ea6\u7ea0\u7f20\uff0c\u6311\u6218\u4e86\u5b50\u7a7a\u95f4\u9632\u5fa1\u7684\u53ef\u884c\u6027\u3002", "motivation": "\u63a2\u8ba8\u5b89\u5168\u5bf9\u9f50\u662f\u5426\u5bf9\u5e94\u53ef\u8bc6\u522b\u7684\u51e0\u4f55\u65b9\u5411\u6216\u5b50\u7a7a\u95f4\uff0c\u4ee5\u9632\u5fa1\u5fae\u8c03\u5bfc\u81f4\u7684\u5b89\u5168\u9000\u5316\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u5206\u6790\u53c2\u6570\u548c\u6fc0\u6d3b\u7a7a\u95f4\u4e2d\u7684\u5b89\u5168\u76f8\u5173\u884c\u4e3a\uff0c\u9a8c\u8bc1\u5b50\u7a7a\u95f4\u662f\u5426\u9009\u62e9\u6027\u63a7\u5236\u5b89\u5168\u6027\u3002", "result": "\u5b89\u5168\u884c\u4e3a\u4e0e\u4e0d\u5b89\u5168\u884c\u4e3a\u5728\u540c\u4e00\u5b50\u7a7a\u95f4\u4e2d\u76f8\u4e92\u653e\u5927\uff0c\u4e14\u4e0d\u540c\u5b89\u5168\u63d0\u793a\u6fc0\u6d3b\u91cd\u53e0\u8868\u793a\uff0c\u672a\u53d1\u73b0\u9009\u62e9\u6027\u63a7\u5236\u5b89\u5168\u7684\u5b50\u7a7a\u95f4\u3002", "conclusion": "\u5b89\u5168\u5bf9\u9f50\u5e76\u975e\u51e0\u4f55\u5c40\u90e8\u5316\uff0c\u800c\u662f\u4e0e\u6a21\u578b\u5b66\u4e60\u52a8\u6001\u9ad8\u5ea6\u7ea0\u7f20\uff0c\u5b50\u7a7a\u95f4\u9632\u5fa1\u7b56\u7565\u53ef\u80fd\u9762\u4e34\u6839\u672c\u9650\u5236\u3002"}}
{"id": "2505.14216", "pdf": "https://arxiv.org/pdf/2505.14216", "abs": "https://arxiv.org/abs/2505.14216", "authors": ["Minwu Kim", "Anubhav Shrestha", "Safal Shrestha", "Aadim Nepal", "Keith Ross"], "title": "Reinforcement Learning vs. Distillation: Understanding Accuracy and Capability in LLM Reasoning", "categories": ["cs.AI", "cs.CL"], "comment": "23 pages", "summary": "Recent studies have shown that reinforcement learning with verifiable rewards\n(RLVR) enhances overall accuracy but fails to improve capability, while\ndistillation can improve both. In this paper, we investigate the mechanisms\nbehind these phenomena. First, we demonstrate that RLVR does not improve\ncapability because it focuses on improving the accuracy of the less-difficult\nquestions to the detriment of the accuracy of the most difficult questions,\nthereby leading to no improvement in capability. Second, we find that RLVR does\nnot merely increase the success probability for the less difficult questions,\nbut in our small model settings produces quality responses that were absent in\nits output distribution before training. In addition, we show these responses\nare neither noticeably longer nor feature more reflection-related keywords,\nunderscoring the need for more reliable indicators of response quality. Third,\nwe show that while distillation reliably improves accuracy by learning strong\nreasoning patterns, it only improves capability when new knowledge is\nintroduced. Moreover, when distilling only with reasoning patterns and no new\nknowledge, the accuracy of the less-difficult questions improves to the\ndetriment of the most difficult questions, similar to RLVR. Together, these\nfindings offer a clearer understanding of how RLVR and distillation shape\nreasoning behavior in language models.", "AI": {"tldr": "RLVR\u63d0\u5347\u51c6\u786e\u6027\u4f46\u672a\u6539\u5584\u80fd\u529b\uff0c\u84b8\u998f\u53ef\u540c\u65f6\u63d0\u5347\u4e24\u8005\u3002\u7814\u7a76\u53d1\u73b0RLVR\u56e0\u5173\u6ce8\u7b80\u5355\u95ee\u9898\u800c\u5ffd\u89c6\u96be\u9898\uff0c\u84b8\u998f\u5219\u9700\u65b0\u77e5\u8bc6\u624d\u80fd\u63d0\u5347\u80fd\u529b\u3002", "motivation": "\u63a2\u7a76RLVR\u548c\u84b8\u998f\u5bf9\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u884c\u4e3a\u7684\u5f71\u54cd\u673a\u5236\u3002", "method": "\u5206\u6790RLVR\u548c\u84b8\u998f\u5bf9\u95ee\u9898\u96be\u5ea6\u7684\u5f71\u54cd\uff0c\u6bd4\u8f83\u5176\u8f93\u51fa\u5206\u5e03\u548c\u8d28\u91cf\u3002", "result": "RLVR\u727a\u7272\u96be\u9898\u51c6\u786e\u6027\uff0c\u84b8\u998f\u9700\u65b0\u77e5\u8bc6\u624d\u80fd\u63d0\u5347\u80fd\u529b\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86RLVR\u548c\u84b8\u998f\u5bf9\u6a21\u578b\u63a8\u7406\u884c\u4e3a\u7684\u5177\u4f53\u5f71\u54cd\u3002"}}
{"id": "2505.14264", "pdf": "https://arxiv.org/pdf/2505.14264", "abs": "https://arxiv.org/abs/2505.14264", "authors": ["Jian Xiong", "Jingbo Zhou", "Jingyong Ye", "Dejing Dou"], "title": "AAPO: Enhance the Reasoning Capabilities of LLMs with Advantage Momentum", "categories": ["cs.LG", "cs.CL"], "comment": "14 pages, 7 figures", "summary": "Reinforcement learning (RL) has emerged as an effective approach for\nenhancing the reasoning capabilities of large language models (LLMs),\nespecially in scenarios where supervised fine-tuning (SFT) falls short due to\nlimited chain-of-thought (CoT) data. Among RL-based post-training methods,\ngroup relative advantage estimation, as exemplified by Group Relative Policy\nOptimization (GRPO), has attracted considerable attention for eliminating the\ndependency on the value model, thereby simplifying training compared to\ntraditional approaches like Proximal Policy Optimization (PPO). However, we\nobserve that exsiting group relative advantage estimation method still suffers\nfrom training inefficiencies, particularly when the estimated advantage\napproaches zero. To address this limitation, we propose Advantage-Augmented\nPolicy Optimization (AAPO), a novel RL algorithm that optimizes the\ncross-entropy (CE) loss using advantages enhanced through a momentum-based\nestimation scheme. This approach effectively mitigates the inefficiencies\nassociated with group relative advantage estimation. Experimental results on\nmultiple mathematical reasoning benchmarks demonstrate the superior performance\nof AAPO.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAAPO\u7684\u65b0\u578b\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u901a\u8fc7\u52a8\u91cf\u589e\u5f3a\u7684\u4f18\u52bf\u4f30\u8ba1\u65b9\u6848\u4f18\u5316\u4ea4\u53c9\u71b5\u635f\u5931\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u7ec4\u76f8\u5bf9\u4f18\u52bf\u4f30\u8ba1\u65b9\u6cd5\u7684\u8bad\u7ec3\u6548\u7387\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u7ec4\u76f8\u5bf9\u4f18\u52bf\u4f30\u8ba1\u7684\u65b9\u6cd5\uff08\u5982GRPO\uff09\u5728\u4f18\u52bf\u63a5\u8fd1\u96f6\u65f6\u5b58\u5728\u8bad\u7ec3\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5f3a\u5316\u5b66\u4e60\u5728\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51faAdvantage-Augmented Policy Optimization (AAPO)\uff0c\u5229\u7528\u52a8\u91cf\u589e\u5f3a\u7684\u4f18\u52bf\u4f30\u8ba1\u65b9\u6848\u4f18\u5316\u4ea4\u53c9\u71b5\u635f\u5931\uff0c\u6539\u8fdb\u8bad\u7ec3\u6548\u7387\u3002", "result": "\u5728\u591a\u4e2a\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAAPO\u8868\u73b0\u51fa\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "conclusion": "AAPO\u901a\u8fc7\u6539\u8fdb\u4f18\u52bf\u4f30\u8ba1\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u7387\uff0c\u4e3a\u5f3a\u5316\u5b66\u4e60\u5728\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u4f18\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.14300", "pdf": "https://arxiv.org/pdf/2505.14300", "abs": "https://arxiv.org/abs/2505.14300", "authors": ["Maheep Chaudhary", "Fazl Barez"], "title": "SafetyNet: Detecting Harmful Outputs in LLMs by Modeling and Monitoring Deceptive Behaviors", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "High-risk industries like nuclear and aviation use real-time monitoring to\ndetect dangerous system conditions. Similarly, Large Language Models (LLMs)\nneed monitoring safeguards. We propose a real-time framework to predict harmful\nAI outputs before they occur by using an unsupervised approach that treats\nnormal behavior as the baseline and harmful outputs as outliers. Our study\nfocuses specifically on backdoor-triggered responses -- where specific input\nphrases activate hidden vulnerabilities causing the model to generate unsafe\ncontent like violence, pornography, or hate speech. We address two key\nchallenges: (1) identifying true causal indicators rather than surface\ncorrelations, and (2) preventing advanced models from deception -- deliberately\nevading monitoring systems. Hence, we approach this problem from an\nunsupervised lens by drawing parallels to human deception: just as humans\nexhibit physical indicators while lying, we investigate whether LLMs display\ndistinct internal behavioral signatures when generating harmful content. Our\nstudy addresses two critical challenges: 1) designing monitoring systems that\ncapture true causal indicators rather than superficial correlations; and\n2)preventing intentional evasion by increasingly capable \"Future models''. Our\nfindings show that models can produce harmful content through causal mechanisms\nand can become deceptive by: (a) alternating between linear and non-linear\nrepresentations, and (b) modifying feature relationships. To counter this, we\ndeveloped Safety-Net -- a multi-detector framework that monitors different\nrepresentation dimensions, successfully detecting harmful behavior even when\ninformation is shifted across representational spaces to evade individual\nmonitors. Our evaluation shows 96% accuracy in detecting harmful cases using\nour unsupervised ensemble approach.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5b9e\u65f6\u76d1\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u65e0\u76d1\u7763\u65b9\u6cd5\u9884\u6d4b\u6709\u5bb3AI\u8f93\u51fa\uff0c\u4e13\u6ce8\u4e8e\u540e\u95e8\u89e6\u53d1\u54cd\u5e94\uff0c\u5e76\u5f00\u53d1\u4e86Safety-Net\u591a\u68c0\u6d4b\u5668\u6846\u67b6\uff0c\u68c0\u6d4b\u51c6\u786e\u7387\u8fbe96%\u3002", "motivation": "\u9ad8\u98ce\u9669\u884c\u4e1a\u5982\u6838\u80fd\u548c\u822a\u7a7a\u4f7f\u7528\u5b9e\u65f6\u76d1\u6d4b\uff0c\u7c7b\u4f3c\u5730\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e5f\u9700\u8981\u76d1\u6d4b\u4fdd\u969c\uff0c\u4ee5\u9632\u6b62\u751f\u6210\u6709\u5bb3\u5185\u5bb9\u3002", "method": "\u91c7\u7528\u65e0\u76d1\u7763\u65b9\u6cd5\uff0c\u5c06\u6b63\u5e38\u884c\u4e3a\u4f5c\u4e3a\u57fa\u7ebf\uff0c\u6709\u5bb3\u8f93\u51fa\u89c6\u4e3a\u5f02\u5e38\uff0c\u7814\u7a76\u540e\u95e8\u89e6\u53d1\u54cd\u5e94\uff0c\u5e76\u8bbe\u8ba1\u591a\u68c0\u6d4b\u5668\u6846\u67b6Safety-Net\u3002", "result": "\u6a21\u578b\u53ef\u901a\u8fc7\u56e0\u679c\u673a\u5236\u751f\u6210\u6709\u5bb3\u5185\u5bb9\uff0c\u5e76\u53ef\u80fd\u901a\u8fc7\u6539\u53d8\u8868\u793a\u65b9\u5f0f\u9003\u907f\u76d1\u6d4b\u3002Safety-Net\u5728\u68c0\u6d4b\u6709\u5bb3\u884c\u4e3a\u65f6\u51c6\u786e\u7387\u8fbe96%\u3002", "conclusion": "\u63d0\u51fa\u7684\u65e0\u76d1\u7763\u6846\u67b6\u80fd\u6709\u6548\u76d1\u6d4b\u548c\u9632\u6b62LLMs\u751f\u6210\u6709\u5bb3\u5185\u5bb9\uff0c\u5c24\u5176\u5728\u5e94\u5bf9\u672a\u6765\u6a21\u578b\u7684\u6b3a\u9a97\u884c\u4e3a\u65b9\u9762\u5177\u6709\u6f5c\u529b\u3002"}}
{"id": "2505.14302", "pdf": "https://arxiv.org/pdf/2505.14302", "abs": "https://arxiv.org/abs/2505.14302", "authors": ["Mengzhao Chen", "Chaoyi Zhang", "Jing Liu", "Yutao Zeng", "Zeyue Xue", "Zhiheng Liu", "Yunshui Li", "Jin Ma", "Jie Huang", "Xun Zhou", "Ping Luo"], "title": "Scaling Law for Quantization-Aware Training", "categories": ["cs.LG", "cs.CL"], "comment": "A unified scaling law for QAT that models quantization error as a\n  function of model size, training data volume, and quantization group size", "summary": "Large language models (LLMs) demand substantial computational and memory\nresources, creating deployment challenges. Quantization-aware training (QAT)\naddresses these challenges by reducing model precision while maintaining\nperformance. However, the scaling behavior of QAT, especially at 4-bit\nprecision (W4A4), is not well understood. Existing QAT scaling laws often\nignore key factors such as the number of training tokens and quantization\ngranularity, which limits their applicability. This paper proposes a unified\nscaling law for QAT that models quantization error as a function of model size,\ntraining data volume, and quantization group size. Through 268 QAT experiments,\nwe show that quantization error decreases as model size increases, but rises\nwith more training tokens and coarser quantization granularity. To identify the\nsources of W4A4 quantization error, we decompose it into weight and activation\ncomponents. Both components follow the overall trend of W4A4 quantization\nerror, but with different sensitivities. Specifically, weight quantization\nerror increases more rapidly with more training tokens. Further analysis shows\nthat the activation quantization error in the FC2 layer, caused by outliers, is\nthe primary bottleneck of W4A4 QAT quantization error. By applying\nmixed-precision quantization to address this bottleneck, we demonstrate that\nweight and activation quantization errors can converge to similar levels.\nAdditionally, with more training data, weight quantization error eventually\nexceeds activation quantization error, suggesting that reducing weight\nquantization error is also important in such scenarios. These findings offer\nkey insights for improving QAT research and development.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u91cf\u5316\u611f\u77e5\u8bad\u7ec3\uff08QAT\uff09\u7f29\u653e\u5b9a\u5f8b\uff0c\u7814\u7a76\u4e864\u4f4d\u7cbe\u5ea6\uff08W4A4\uff09\u4e0b\u7684\u91cf\u5316\u8bef\u5dee\u4e0e\u6a21\u578b\u5927\u5c0f\u3001\u8bad\u7ec3\u6570\u636e\u91cf\u548c\u91cf\u5316\u7c92\u5ea6\u7684\u5173\u7cfb\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u63ed\u793a\u4e86\u6743\u91cd\u548c\u6fc0\u6d3b\u91cf\u5316\u8bef\u5dee\u7684\u4e0d\u540c\u654f\u611f\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u9ad8\u8ba1\u7b97\u548c\u5185\u5b58\u9700\u6c42\u5e26\u6765\u4e86\u90e8\u7f72\u6311\u6218\uff0c\u91cf\u5316\u611f\u77e5\u8bad\u7ec3\uff08QAT\uff09\u901a\u8fc7\u964d\u4f4e\u6a21\u578b\u7cbe\u5ea6\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u4f464\u4f4d\u7cbe\u5ea6\u4e0b\u7684\u7f29\u653e\u884c\u4e3a\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u901a\u8fc7268\u6b21QAT\u5b9e\u9a8c\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u7f29\u653e\u5b9a\u5f8b\uff0c\u91cf\u5316\u8bef\u5dee\u88ab\u5efa\u6a21\u4e3a\u6a21\u578b\u5927\u5c0f\u3001\u8bad\u7ec3\u6570\u636e\u91cf\u548c\u91cf\u5316\u7ec4\u5927\u5c0f\u7684\u51fd\u6570\uff0c\u5e76\u5206\u89e3\u4e86\u6743\u91cd\u548c\u6fc0\u6d3b\u91cf\u5316\u8bef\u5dee\u3002", "result": "\u91cf\u5316\u8bef\u5dee\u968f\u6a21\u578b\u589e\u5927\u800c\u51cf\u5c0f\uff0c\u4f46\u968f\u8bad\u7ec3\u6570\u636e\u91cf\u589e\u52a0\u548c\u91cf\u5316\u7c92\u5ea6\u53d8\u7c97\u800c\u4e0a\u5347\uff1b\u6fc0\u6d3b\u91cf\u5316\u8bef\u5dee\uff08\u5c24\u5176\u662fFC2\u5c42\u7684\u5f02\u5e38\u503c\uff09\u662f\u4e3b\u8981\u74f6\u9888\u3002\u901a\u8fc7\u6df7\u5408\u7cbe\u5ea6\u91cf\u5316\u53ef\u89e3\u51b3\u6b64\u95ee\u9898\u3002", "conclusion": "\u7814\u7a76\u4e3a\u6539\u8fdbQAT\u63d0\u4f9b\u4e86\u5173\u952e\u89c1\u89e3\uff0c\u6307\u51fa\u5728\u66f4\u591a\u8bad\u7ec3\u6570\u636e\u4e0b\uff0c\u6743\u91cd\u91cf\u5316\u8bef\u5dee\u53ef\u80fd\u8d85\u8fc7\u6fc0\u6d3b\u91cf\u5316\u8bef\u5dee\uff0c\u9700\u540c\u65f6\u5173\u6ce8\u4e24\u8005\u4f18\u5316\u3002"}}
{"id": "2505.14351", "pdf": "https://arxiv.org/pdf/2505.14351", "abs": "https://arxiv.org/abs/2505.14351", "authors": ["Yutong Liu", "Ziyue Zhang", "Ban Ma-bao", "Yuqing Cai", "Yongbin Yu", "Renzeng Duojie", "Xiangxiang Wang", "Fan Gao", "Cheng Huang", "Nyima Tashi"], "title": "FMSD-TTS: Few-shot Multi-Speaker Multi-Dialect Text-to-Speech Synthesis for \u00dc-Tsang, Amdo and Kham Speech Dataset Generation", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "comment": "13 pages", "summary": "Tibetan is a low-resource language with minimal parallel speech corpora\nspanning its three major dialects-\\\"U-Tsang, Amdo, and Kham-limiting progress\nin speech modeling. To address this issue, we propose FMSD-TTS, a few-shot,\nmulti-speaker, multi-dialect text-to-speech framework that synthesizes parallel\ndialectal speech from limited reference audio and explicit dialect labels. Our\nmethod features a novel speaker-dialect fusion module and a Dialect-Specialized\nDynamic Routing Network (DSDR-Net) to capture fine-grained acoustic and\nlinguistic variations across dialects while preserving speaker identity.\nExtensive objective and subjective evaluations demonstrate that FMSD-TTS\nsignificantly outperforms baselines in both dialectal expressiveness and\nspeaker similarity. We further validate the quality and utility of the\nsynthesized speech through a challenging speech-to-speech dialect conversion\ntask. Our contributions include: (1) a novel few-shot TTS system tailored for\nTibetan multi-dialect speech synthesis, (2) the public release of a large-scale\nsynthetic Tibetan speech corpus generated by FMSD-TTS, and (3) an open-source\nevaluation toolkit for standardized assessment of speaker similarity, dialect\nconsistency, and audio quality.", "AI": {"tldr": "FMSD-TTS\u662f\u4e00\u4e2a\u9488\u5bf9\u85cf\u8bed\u591a\u65b9\u8a00\u7684\u5c11\u6837\u672c\u3001\u591a\u8bf4\u8bdd\u4eba\u3001\u591a\u65b9\u8a00\u6587\u672c\u8f6c\u8bed\u97f3\u6846\u67b6\uff0c\u901a\u8fc7\u65b0\u9896\u7684\u6a21\u5757\u548c\u7f51\u7edc\u8bbe\u8ba1\u663e\u8457\u63d0\u5347\u4e86\u65b9\u8a00\u8868\u8fbe\u548c\u8bf4\u8bdd\u4eba\u76f8\u4f3c\u6027\u3002", "motivation": "\u85cf\u8bed\u4f5c\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\uff0c\u7f3a\u4e4f\u8de8\u65b9\u8a00\u7684\u5e73\u884c\u8bed\u97f3\u8bed\u6599\u5e93\uff0c\u9650\u5236\u4e86\u8bed\u97f3\u5efa\u6a21\u7684\u8fdb\u5c55\u3002", "method": "\u91c7\u7528\u8bf4\u8bdd\u4eba-\u65b9\u8a00\u878d\u5408\u6a21\u5757\u548c\u65b9\u8a00\u4e13\u7528\u52a8\u6001\u8def\u7531\u7f51\u7edc\uff08DSDR-Net\uff09\uff0c\u6355\u6349\u65b9\u8a00\u95f4\u7684\u58f0\u5b66\u548c\u8bed\u8a00\u7ec6\u5fae\u5dee\u5f02\uff0c\u540c\u65f6\u4fdd\u7559\u8bf4\u8bdd\u4eba\u8eab\u4efd\u3002", "result": "\u5728\u65b9\u8a00\u8868\u8fbe\u548c\u8bf4\u8bdd\u4eba\u76f8\u4f3c\u6027\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\uff0c\u5e76\u901a\u8fc7\u8bed\u97f3\u5230\u8bed\u97f3\u65b9\u8a00\u8f6c\u6362\u4efb\u52a1\u9a8c\u8bc1\u4e86\u5408\u6210\u8bed\u97f3\u7684\u8d28\u91cf\u548c\u5b9e\u7528\u6027\u3002", "conclusion": "FMSD-TTS\u4e3a\u85cf\u8bed\u591a\u65b9\u8a00\u8bed\u97f3\u5408\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u516c\u5f00\u4e86\u5408\u6210\u8bed\u6599\u5e93\u548c\u8bc4\u4f30\u5de5\u5177\u5305\u3002"}}
{"id": "2505.14356", "pdf": "https://arxiv.org/pdf/2505.14356", "abs": "https://arxiv.org/abs/2505.14356", "authors": ["Sho Inoue", "Shai Wang", "Haizhou Li"], "title": "PersonaTAB: Predicting Personality Traits using Textual, Acoustic, and Behavioral Cues in Fully-Duplex Speech Dialogs", "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": "This is accepted to Interspeech 2025; Added an extra page for\n  supplementary figures; Project page:\n  https://github.com/shinshoji01/Personality-Prediction-for-Conversation-Agents", "summary": "Despite significant progress in neural spoken dialog systems,\npersonality-aware conversation agents -- capable of adapting behavior based on\npersonalities -- remain underexplored due to the absence of personality\nannotations in speech datasets. We propose a pipeline that preprocesses raw\naudio recordings to create a dialogue dataset annotated with timestamps,\nresponse types, and emotion/sentiment labels. We employ an automatic speech\nrecognition (ASR) system to extract transcripts and timestamps, then generate\nconversation-level annotations. Leveraging these annotations, we design a\nsystem that employs large language models to predict conversational\npersonality. Human evaluators were engaged to identify conversational\ncharacteristics and assign personality labels. Our analysis demonstrates that\nthe proposed system achieves stronger alignment with human judgments compared\nto existing approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5904\u7406\u539f\u59cb\u97f3\u9891\u6570\u636e\u5e76\u751f\u6210\u5e26\u6807\u6ce8\u7684\u5bf9\u8bdd\u6570\u636e\u96c6\u7684\u6d41\u7a0b\uff0c\u7528\u4e8e\u6784\u5efa\u57fa\u4e8e\u4e2a\u6027\u7684\u5bf9\u8bdd\u7cfb\u7edf\u3002", "motivation": "\u7531\u4e8e\u8bed\u97f3\u6570\u636e\u4e2d\u7f3a\u4e4f\u4e2a\u6027\u6807\u6ce8\uff0c\u4e2a\u6027\u611f\u77e5\u7684\u5bf9\u8bdd\u7cfb\u7edf\u7814\u7a76\u4e0d\u8db3\u3002", "method": "\u4f7f\u7528ASR\u7cfb\u7edf\u63d0\u53d6\u8f6c\u5f55\u548c\u65f6\u95f4\u6233\uff0c\u751f\u6210\u5bf9\u8bdd\u7ea7\u6807\u6ce8\uff0c\u5e76\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u9884\u6d4b\u5bf9\u8bdd\u4e2a\u6027\u3002", "result": "\u7cfb\u7edf\u5728\u4eba\u7c7b\u8bc4\u4f30\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u4e2a\u6027\u611f\u77e5\u5bf9\u8bdd\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.14368", "pdf": "https://arxiv.org/pdf/2505.14368", "abs": "https://arxiv.org/abs/2505.14368", "authors": ["Jiawen Wang", "Pritha Gupta", "Ivan Habernal", "Eyke H\u00fcllermeier"], "title": "Is Your Prompt Safe? Investigating Prompt Injection Attacks Against Open-Source LLMs", "categories": ["cs.CR", "cs.CL"], "comment": "8 pages, 3 figures, EMNLP 2025 under review", "summary": "Recent studies demonstrate that Large Language Models (LLMs) are vulnerable\nto different prompt-based attacks, generating harmful content or sensitive\ninformation. Both closed-source and open-source LLMs are underinvestigated for\nthese attacks. This paper studies effective prompt injection attacks against\nthe $\\mathbf{14}$ most popular open-source LLMs on five attack benchmarks.\nCurrent metrics only consider successful attacks, whereas our proposed Attack\nSuccess Probability (ASP) also captures uncertainty in the model's response,\nreflecting ambiguity in attack feasibility. By comprehensively analyzing the\neffectiveness of prompt injection attacks, we propose a simple and effective\nhypnotism attack; results show that this attack causes aligned language models,\nincluding Stablelm2, Mistral, Openchat, and Vicuna, to generate objectionable\nbehaviors, achieving around $90$% ASP. They also indicate that our ignore\nprefix attacks can break all $\\mathbf{14}$ open-source LLMs, achieving over\n$60$% ASP on a multi-categorical dataset. We find that moderately well-known\nLLMs exhibit higher vulnerability to prompt injection attacks, highlighting the\nneed to raise public awareness and prioritize efficient mitigation strategies.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u9488\u5bf914\u79cd\u6d41\u884c\u5f00\u6e90LLM\u7684\u63d0\u793a\u6ce8\u5165\u653b\u51fb\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u50ac\u7720\u653b\u51fb\u65b9\u6cd5\uff0c\u5e76\u5f15\u5165\u653b\u51fb\u6210\u529f\u6982\u7387\uff08ASP\uff09\u6307\u6807\uff0c\u7ed3\u679c\u663e\u793a\u653b\u51fb\u6548\u679c\u663e\u8457\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5bf9\u5f00\u6e90\u548c\u95ed\u6e90LLM\u5728\u63d0\u793a\u653b\u51fb\u4e0b\u7684\u8106\u5f31\u6027\u7814\u7a76\u4e0d\u8db3\uff0c\u9700\u8fdb\u4e00\u6b65\u63a2\u7d22\u5176\u5b89\u5168\u98ce\u9669\u3002", "method": "\u901a\u8fc7\u4e94\u79cd\u653b\u51fb\u57fa\u51c6\u6d4b\u8bd5\uff0c\u63d0\u51fa\u50ac\u7720\u653b\u51fb\u548c\u5ffd\u7565\u524d\u7f00\u653b\u51fb\uff0c\u5e76\u5f15\u5165ASP\u6307\u6807\u8861\u91cf\u653b\u51fb\u6548\u679c\u3002", "result": "\u50ac\u7720\u653b\u51fb\u5bf9\u591a\u79cd\u5bf9\u9f50\u6a21\u578b\uff08\u5982Stablelm2\u3001Mistral\u7b49\uff09\u6709\u6548\uff0cASP\u8fbe90%\uff1b\u5ffd\u7565\u524d\u7f00\u653b\u51fb\u5bf914\u79cdLLM\u5747\u6709\u6548\uff0cASP\u8d8560%\u3002", "conclusion": "\u4e2d\u7b49\u77e5\u540d\u5ea6\u7684LLM\u66f4\u6613\u53d7\u653b\u51fb\uff0c\u9700\u63d0\u9ad8\u516c\u4f17\u610f\u8bc6\u5e76\u4f18\u5148\u5236\u5b9a\u7f13\u89e3\u7b56\u7565\u3002"}}
{"id": "2505.14396", "pdf": "https://arxiv.org/pdf/2505.14396", "abs": "https://arxiv.org/abs/2505.14396", "authors": ["Ga\u00ebl Gendron", "Jo\u017ee M. Ro\u017eanec", "Michael Witbrock", "Gillian Dobbie"], "title": "Causal Cartographer: From Mapping to Reasoning Over Counterfactual Worlds", "categories": ["cs.AI", "cs.CL", "cs.LG", "I.2.3; I.2.6; I.2.7; G.2.2; G.3; J.1"], "comment": "29 pages, 9 pages for the main paper, 20 pages for the references and\n  appendix, 25 figures", "summary": "Causal world models are systems that can answer counterfactual questions\nabout an environment of interest, i.e. predict how it would have evolved if an\narbitrary subset of events had been realized differently. It requires\nunderstanding the underlying causes behind chains of events and conducting\ncausal inference for arbitrary unseen distributions. So far, this task eludes\nfoundation models, notably large language models (LLMs), which do not have\ndemonstrated causal reasoning capabilities beyond the memorization of existing\ncausal relationships. Furthermore, evaluating counterfactuals in real-world\napplications is challenging since only the factual world is observed, limiting\nevaluation to synthetic datasets. We address these problems by explicitly\nextracting and modeling causal relationships and propose the Causal\nCartographer framework. First, we introduce a graph retrieval-augmented\ngeneration agent tasked to retrieve causal relationships from data. This\napproach allows us to construct a large network of real-world causal\nrelationships that can serve as a repository of causal knowledge and build\nreal-world counterfactuals. In addition, we create a counterfactual reasoning\nagent constrained by causal relationships to perform reliable step-by-step\ncausal inference. We show that our approach can extract causal knowledge and\nimprove the robustness of LLMs for causal reasoning tasks while reducing\ninference costs and spurious correlations.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faCausal Cartographer\u6846\u67b6\uff0c\u901a\u8fc7\u663e\u5f0f\u63d0\u53d6\u548c\u5efa\u6a21\u56e0\u679c\u5173\u7cfb\uff0c\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u56e0\u679c\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u57fa\u7840\u6a21\u578b\uff08\u5982\u5927\u8bed\u8a00\u6a21\u578b\uff09\u7f3a\u4e4f\u56e0\u679c\u63a8\u7406\u80fd\u529b\uff0c\u4e14\u771f\u5b9e\u4e16\u754c\u4e2d\u7684\u53cd\u4e8b\u5b9e\u8bc4\u4f30\u56f0\u96be\u3002", "method": "\u91c7\u7528\u56fe\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u4ee3\u7406\u63d0\u53d6\u56e0\u679c\u5173\u7cfb\uff0c\u6784\u5efa\u56e0\u679c\u77e5\u8bc6\u5e93\uff0c\u5e76\u8bbe\u8ba1\u53cd\u4e8b\u5b9e\u63a8\u7406\u4ee3\u7406\u8fdb\u884c\u53ef\u9760\u63a8\u7406\u3002", "result": "\u65b9\u6cd5\u80fd\u63d0\u53d6\u56e0\u679c\u77e5\u8bc6\uff0c\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u56e0\u679c\u63a8\u7406\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u964d\u4f4e\u63a8\u7406\u6210\u672c\u548c\u865a\u5047\u76f8\u5173\u6027\u3002", "conclusion": "Causal Cartographer\u6846\u67b6\u4e3a\u89e3\u51b3\u56e0\u679c\u63a8\u7406\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2505.14402", "pdf": "https://arxiv.org/pdf/2505.14402", "abs": "https://arxiv.org/abs/2505.14402", "authors": ["Heng Yang", "Jack Cole", "Yuan Li", "Renzhi Chen", "Geyong Min", "Ke Li"], "title": "OmniGenBench: A Modular Platform for Reproducible Genomic Foundation Models Benchmarking", "categories": ["q-bio.GN", "cs.CL"], "comment": null, "summary": "The code of nature, embedded in DNA and RNA genomes since the origin of life,\nholds immense potential to impact both humans and ecosystems through genome\nmodeling. Genomic Foundation Models (GFMs) have emerged as a transformative\napproach to decoding the genome. As GFMs scale up and reshape the landscape of\nAI-driven genomics, the field faces an urgent need for rigorous and\nreproducible evaluation. We present OmniGenBench, a modular benchmarking\nplatform designed to unify the data, model, benchmarking, and interpretability\nlayers across GFMs. OmniGenBench enables standardized, one-command evaluation\nof any GFM across five benchmark suites, with seamless integration of over 31\nopen-source models. Through automated pipelines and community-extensible\nfeatures, the platform addresses critical reproducibility challenges, including\ndata transparency, model interoperability, benchmark fragmentation, and\nblack-box interpretability. OmniGenBench aims to serve as foundational\ninfrastructure for reproducible genomic AI research, accelerating trustworthy\ndiscovery and collaborative innovation in the era of genome-scale modeling.", "AI": {"tldr": "OmniGenBench\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u57fa\u51c6\u6d4b\u8bd5\u5e73\u53f0\uff0c\u65e8\u5728\u7edf\u4e00\u57fa\u56e0\u7ec4\u57fa\u7840\u6a21\u578b\uff08GFMs\uff09\u7684\u6570\u636e\u3001\u6a21\u578b\u3001\u57fa\u51c6\u6d4b\u8bd5\u548c\u53ef\u89e3\u91ca\u6027\u5c42\uff0c\u4ee5\u89e3\u51b3\u53ef\u91cd\u590d\u6027\u6311\u6218\u3002", "motivation": "\u57fa\u56e0\u7ec4\u57fa\u7840\u6a21\u578b\uff08GFMs\uff09\u5728\u89e3\u7801\u57fa\u56e0\u7ec4\u65b9\u9762\u5177\u6709\u53d8\u9769\u6027\u6f5c\u529b\uff0c\u4f46\u9700\u8981\u4e25\u683c\u4e14\u53ef\u91cd\u590d\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "OmniGenBench\u901a\u8fc7\u6807\u51c6\u5316\u4e00\u952e\u8bc4\u4f30\u3001\u81ea\u52a8\u5316\u7ba1\u9053\u548c\u793e\u533a\u53ef\u6269\u5c55\u529f\u80fd\uff0c\u6574\u5408\u4e8631\u4e2a\u5f00\u6e90\u6a21\u578b\uff0c\u8986\u76d6\u4e94\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\u3002", "result": "\u8be5\u5e73\u53f0\u89e3\u51b3\u4e86\u6570\u636e\u900f\u660e\u5ea6\u3001\u6a21\u578b\u4e92\u64cd\u4f5c\u6027\u3001\u57fa\u51c6\u788e\u7247\u5316\u548c\u9ed1\u76d2\u53ef\u89e3\u91ca\u6027\u7b49\u5173\u952e\u6311\u6218\u3002", "conclusion": "OmniGenBench\u65e8\u5728\u4f5c\u4e3a\u53ef\u91cd\u590d\u57fa\u56e0\u7ec4AI\u7814\u7a76\u7684\u57fa\u7840\u8bbe\u65bd\uff0c\u52a0\u901f\u57fa\u56e0\u7ec4\u89c4\u6a21\u5efa\u6a21\u65f6\u4ee3\u7684\u53ef\u4fe1\u53d1\u73b0\u548c\u534f\u4f5c\u521b\u65b0\u3002"}}
{"id": "2505.14410", "pdf": "https://arxiv.org/pdf/2505.14410", "abs": "https://arxiv.org/abs/2505.14410", "authors": ["Jinzuomu Zhong", "Suyuan Liu", "Dan Wells", "Korin Richmond"], "title": "Pairwise Evaluation of Accent Similarity in Speech Synthesis", "categories": ["eess.AS", "cs.CL", "cs.SD"], "comment": "Accepted by INTERSPEECH 2025", "summary": "Despite growing interest in generating high-fidelity accents, evaluating\naccent similarity in speech synthesis has been underexplored. We aim to enhance\nboth subjective and objective evaluation methods for accent similarity.\nSubjectively, we refine the XAB listening test by adding components that\nachieve higher statistical significance with fewer listeners and lower costs.\nOur method involves providing listeners with transcriptions, having them\nhighlight perceived accent differences, and implementing meticulous screening\nfor reliability. Objectively, we utilise pronunciation-related metrics, based\non distances between vowel formants and phonetic posteriorgrams, to evaluate\naccent generation. Comparative experiments reveal that these metrics, alongside\naccent similarity, speaker similarity, and Mel Cepstral Distortion, can be\nused. Moreover, our findings underscore significant limitations of common\nmetrics like Word Error Rate in assessing underrepresented accents.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u6539\u8fdb\u4e3b\u89c2\u548c\u5ba2\u89c2\u65b9\u6cd5\u6765\u8bc4\u4f30\u8bed\u97f3\u5408\u6210\u4e2d\u7684\u53e3\u97f3\u76f8\u4f3c\u6027\uff0c\u5305\u62ec\u4f18\u5316XAB\u542c\u529b\u6d4b\u8bd5\u548c\u5f15\u5165\u53d1\u97f3\u76f8\u5173\u6307\u6807\u3002", "motivation": "\u5c3d\u7ba1\u5bf9\u751f\u6210\u9ad8\u4fdd\u771f\u53e3\u97f3\u7684\u5174\u8da3\u589e\u52a0\uff0c\u4f46\u8bc4\u4f30\u8bed\u97f3\u5408\u6210\u4e2d\u7684\u53e3\u97f3\u76f8\u4f3c\u6027\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u4e3b\u89c2\u4e0a\u4f18\u5316XAB\u542c\u529b\u6d4b\u8bd5\uff0c\u589e\u52a0\u8f6c\u5f55\u548c\u5dee\u5f02\u6807\u6ce8\uff1b\u5ba2\u89c2\u4e0a\u4f7f\u7528\u53d1\u97f3\u76f8\u5173\u6307\u6807\uff08\u5982\u5143\u97f3\u5171\u632f\u5cf0\u8ddd\u79bb\u548c\u8bed\u97f3\u540e\u9a8c\u56fe\uff09\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8fd9\u4e9b\u65b9\u6cd5\u6709\u6548\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u5e38\u89c1\u6307\u6807\uff08\u5982\u8bcd\u9519\u8bef\u7387\uff09\u5728\u8bc4\u4f30\u5c11\u6570\u53e3\u97f3\u65f6\u7684\u5c40\u9650\u6027\u3002", "conclusion": "\u6539\u8fdb\u7684\u8bc4\u4f30\u65b9\u6cd5\u80fd\u66f4\u51c6\u786e\u5730\u8861\u91cf\u53e3\u97f3\u76f8\u4f3c\u6027\uff0c\u4e3a\u8bed\u97f3\u5408\u6210\u7814\u7a76\u63d0\u4f9b\u65b0\u65b9\u5411\u3002"}}
{"id": "2505.14412", "pdf": "https://arxiv.org/pdf/2505.14412", "abs": "https://arxiv.org/abs/2505.14412", "authors": ["Pawe\u0142 Batorski", "Adrian Kosmala", "Paul Swoboda"], "title": "PRL: Prompts from Reinforcement Learning", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Effective prompt engineering remains a central challenge in fully harnessing\nthe capabilities of LLMs. While well-designed prompts can dramatically enhance\nperformance, crafting them typically demands expert intuition and a nuanced\nunderstanding of the task. Moreover, the most impactful prompts often hinge on\nsubtle semantic cues, ones that may elude human perception but are crucial for\nguiding LLM behavior. In this paper, we introduce PRL (Prompts from\nReinforcement Learning), a novel RL-based approach for automatic prompt\ngeneration. Unlike previous methods, PRL can produce novel few-shot examples\nthat were not seen during training. Our approach achieves state-of-the-art\nperformance across a range of benchmarks, including text classification,\nsimplification, and summarization. On the classification task, it surpasses\nprior methods by 2.58% over APE and 1.00% over EvoPrompt. Additionally, it\nimproves the average ROUGE scores on the summarization task by 4.32 over APE\nand by 2.12 over EvoPrompt and the SARI score on simplification by 6.93 over\nAPE and by 6.01 over EvoPrompt. Our code is available at\nhttps://github.com/Batorskq/prl .", "AI": {"tldr": "PRL\uff08\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u63d0\u793a\u751f\u6210\u65b9\u6cd5\uff09\u901a\u8fc7\u81ea\u52a8\u751f\u6210\u65b0\u9896\u7684\u5c11\u6837\u672c\u793a\u4f8b\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u5728\u6587\u672c\u5206\u7c7b\u3001\u7b80\u5316\u548c\u6458\u8981\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u63d0\u793a\u5de5\u7a0b\u4f9d\u8d56\u4e13\u5bb6\u76f4\u89c9\u548c\u4efb\u52a1\u7406\u89e3\uff0c\u4e14\u5173\u952e\u8bed\u4e49\u7ebf\u7d22\u53ef\u80fd\u96be\u4ee5\u6355\u6349\u3002PRL\u65e8\u5728\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u81ea\u52a8\u751f\u6210\u9ad8\u6548\u63d0\u793a\uff0c\u51cf\u5c11\u4eba\u5de5\u5e72\u9884\u3002", "method": "PRL\u662f\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u751f\u6210\u8bad\u7ec3\u4e2d\u672a\u89c1\u8fc7\u7684\u5c11\u6837\u672c\u793a\u4f8b\uff0c\u7528\u4e8e\u63d0\u793a\u4f18\u5316\u3002", "result": "PRL\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff1a\u5206\u7c7b\u4efb\u52a1\u8d85\u8d8aAPE 2.58%\u548cEvoPrompt 1.00%\uff1b\u6458\u8981\u4efb\u52a1ROUGE\u5206\u6570\u63d0\u53474.32\u548c2.12\uff1b\u7b80\u5316\u4efb\u52a1SARI\u5206\u6570\u63d0\u53476.93\u548c6.01\u3002", "conclusion": "PRL\u901a\u8fc7\u81ea\u52a8\u751f\u6210\u63d0\u793a\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u6027\u80fd\uff0c\u4e3a\u63d0\u793a\u5de5\u7a0b\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.14432", "pdf": "https://arxiv.org/pdf/2505.14432", "abs": "https://arxiv.org/abs/2505.14432", "authors": ["Eugene Yang", "Andrew Yates", "Kathryn Ricci", "Orion Weller", "Vivek Chari", "Benjamin Van Durme", "Dawn Lawrie"], "title": "Rank-K: Test-Time Reasoning for Listwise Reranking", "categories": ["cs.IR", "cs.CL"], "comment": "15 pages, 4 figures", "summary": "Retrieve-and-rerank is a popular retrieval pipeline because of its ability to\nmake slow but effective rerankers efficient enough at query time by reducing\nthe number of comparisons. Recent works in neural rerankers take advantage of\nlarge language models for their capability in reasoning between queries and\npassages and have achieved state-of-the-art retrieval effectiveness. However,\nsuch rerankers are resource-intensive, even after heavy optimization. In this\nwork, we introduce Rank-K, a listwise passage reranking model that leverages\nthe reasoning capability of the reasoning language model at query time that\nprovides test time scalability to serve hard queries. We show that Rank-K\nimproves retrieval effectiveness by 23\\% over the RankZephyr, the\nstate-of-the-art listwise reranker, when reranking a BM25 initial ranked list\nand 19\\% when reranking strong retrieval results by SPLADE-v3. Since Rank-K is\ninherently a multilingual model, we found that it ranks passages based on\nqueries in different languages as effectively as it does in monolingual\nretrieval.", "AI": {"tldr": "Rank-K\u662f\u4e00\u79cd\u57fa\u4e8e\u63a8\u7406\u8bed\u8a00\u6a21\u578b\u7684\u5217\u8868\u5f0f\u91cd\u6392\u5e8f\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u7d22\u6548\u7387\uff0c\u5c24\u5176\u5728\u591a\u8bed\u8a00\u68c0\u7d22\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5c3d\u7ba1\u795e\u7ecf\u91cd\u6392\u5e8f\u6a21\u578b\u5728\u68c0\u7d22\u6548\u679c\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u8d44\u6e90\u6d88\u8017\u5927\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002Rank-K\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u540c\u65f6\u63d0\u5347\u68c0\u7d22\u6548\u679c\u3002", "method": "Rank-K\u5229\u7528\u63a8\u7406\u8bed\u8a00\u6a21\u578b\u7684\u80fd\u529b\uff0c\u5728\u67e5\u8be2\u65f6\u8fdb\u884c\u5217\u8868\u5f0f\u6bb5\u843d\u91cd\u6392\u5e8f\uff0c\u652f\u6301\u591a\u8bed\u8a00\u68c0\u7d22\u3002", "result": "Rank-K\u5728BM25\u521d\u59cb\u6392\u540d\u5217\u8868\u4e0a\u7684\u68c0\u7d22\u6548\u679c\u6bd4RankZephyr\u63d0\u534723%\uff0c\u5728SPLADE-v3\u7ed3\u679c\u4e0a\u63d0\u534719%\uff0c\u4e14\u5728\u591a\u8bed\u8a00\u68c0\u7d22\u4e2d\u8868\u73b0\u540c\u6837\u51fa\u8272\u3002", "conclusion": "Rank-K\u901a\u8fc7\u9ad8\u6548\u7684\u63a8\u7406\u8bed\u8a00\u6a21\u578b\u5e94\u7528\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u7d22\u6548\u679c\uff0c\u5e76\u5177\u5907\u591a\u8bed\u8a00\u9002\u5e94\u6027\u3002"}}
{"id": "2505.14438", "pdf": "https://arxiv.org/pdf/2505.14438", "abs": "https://arxiv.org/abs/2505.14438", "authors": ["Yuanbo Fang", "Haoze Sun", "Jun Liu", "Tao Zhang", "Zenan Zhou", "Weipeng Chen", "Xiaofen Xing", "Xiangmin Xu"], "title": "S2SBench: A Benchmark for Quantifying Intelligence Degradation in Speech-to-Speech Large Language Models", "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": null, "summary": "End-to-end speech large language models ((LLMs)) extend the capabilities of\ntext-based models to directly process and generate audio tokens. However, this\noften leads to a decline in reasoning and generation performance compared to\ntext input, a phenomenon referred to as intelligence degradation. To\nsystematically evaluate this gap, we propose S2SBench, a benchmark designed to\nquantify performance degradation in Speech LLMs. It includes diagnostic\ndatasets targeting sentence continuation and commonsense reasoning under audio\ninput. We further introduce a pairwise evaluation protocol based on perplexity\ndifferences between plausible and implausible samples to measure degradation\nrelative to text input. We apply S2SBench to analyze the training process of\nBaichuan-Audio, which further demonstrates the benchmark's effectiveness. All\ndatasets and evaluation code are available at\nhttps://github.com/undobug/S2SBench.", "AI": {"tldr": "S2SBench\u662f\u4e00\u4e2a\u7528\u4e8e\u91cf\u5316\u8bed\u97f3\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u6027\u80fd\u4e0b\u964d\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7\u8bca\u65ad\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u534f\u8bae\u5206\u6790\u8bed\u97f3\u8f93\u5165\u5bf9\u6a21\u578b\u63a8\u7406\u548c\u751f\u6210\u80fd\u529b\u7684\u5f71\u54cd\u3002", "motivation": "\u8bed\u97f3LLMs\u5728\u5904\u7406\u97f3\u9891\u8f93\u5165\u65f6\u53ef\u80fd\u5bfc\u81f4\u63a8\u7406\u548c\u751f\u6210\u6027\u80fd\u4e0b\u964d\uff08\u79f0\u4e3a\u667a\u80fd\u9000\u5316\uff09\uff0c\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30\u8fd9\u4e00\u73b0\u8c61\u3002", "method": "\u63d0\u51faS2SBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u9488\u5bf9\u53e5\u5b50\u5ef6\u7eed\u548c\u5e38\u8bc6\u63a8\u7406\u7684\u8bca\u65ad\u6570\u636e\u96c6\uff0c\u5e76\u57fa\u4e8e\u56f0\u60d1\u5ea6\u5dee\u5f02\u5f15\u5165\u6210\u5bf9\u8bc4\u4f30\u534f\u8bae\u3002", "result": "\u5e94\u7528S2SBench\u5206\u6790\u4e86Baichuan-Audio\u7684\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u9a8c\u8bc1\u4e86\u57fa\u51c6\u7684\u6709\u6548\u6027\u3002", "conclusion": "S2SBench\u4e3a\u8bed\u97f3LLMs\u7684\u6027\u80fd\u9000\u5316\u63d0\u4f9b\u4e86\u91cf\u5316\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u8fdb\u4e00\u6b65\u7814\u7a76\u548c\u6539\u8fdb\u3002"}}
{"id": "2505.14449", "pdf": "https://arxiv.org/pdf/2505.14449", "abs": "https://arxiv.org/abs/2505.14449", "authors": ["Yi-Cheng Lin", "Huang-Cheng Chou", "Hung-yi Lee"], "title": "Mitigating Subgroup Disparities in Multi-Label Speech Emotion Recognition: A Pseudo-Labeling and Unsupervised Learning Approach", "categories": ["eess.AS", "cs.CL", "cs.SD"], "comment": "Accepted by InterSpeech 2025. 7 pages including 2 pages of appendix", "summary": "While subgroup disparities and performance bias are increasingly studied in\ncomputational research, fairness in categorical Speech Emotion Recognition\n(SER) remains underexplored. Existing methods often rely on explicit\ndemographic labels, which are difficult to obtain due to privacy concerns. To\naddress this limitation, we introduce an Implicit Demography Inference (IDI)\nmodule that leverages pseudo-labeling from a pre-trained model and unsupervised\nlearning using k-means clustering to mitigate bias in SER. Our experiments show\nthat pseudo-labeling IDI reduces subgroup disparities, improving fairness\nmetrics by over 33% with less than a 3% decrease in SER accuracy. Also, the\nunsupervised IDI yields more than a 26% improvement in fairness metrics with a\ndrop of less than 4% in SER performance. Further analyses reveal that the\nunsupervised IDI consistently mitigates race and age disparities, demonstrating\nits potential in scenarios where explicit demographic information is\nunavailable.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9690\u5f0f\u4eba\u53e3\u7edf\u8ba1\u63a8\u65ad\uff08IDI\uff09\u6a21\u5757\uff0c\u901a\u8fc7\u4f2a\u6807\u7b7e\u548c\u65e0\u76d1\u7763\u5b66\u4e60\u51cf\u5c11\u8bed\u97f3\u60c5\u611f\u8bc6\u522b\uff08SER\uff09\u4e2d\u7684\u504f\u89c1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u516c\u5e73\u6027\u6307\u6807\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u663e\u5f0f\u4eba\u53e3\u7edf\u8ba1\u6807\u7b7e\uff0c\u4f46\u9690\u79c1\u95ee\u9898\u4f7f\u5176\u96be\u4ee5\u83b7\u53d6\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u4e0d\u4f9d\u8d56\u663e\u5f0f\u6807\u7b7e\u7684\u516c\u5e73\u6027\u6539\u8fdb\u65b9\u6cd5\u3002", "method": "\u7ed3\u5408\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u4f2a\u6807\u7b7e\u548c\u65e0\u76d1\u7763k-means\u805a\u7c7b\uff0c\u8bbe\u8ba1\u4e86IDI\u6a21\u5757\u4ee5\u51cf\u5c11SER\u4e2d\u7684\u504f\u89c1\u3002", "result": "\u4f2a\u6807\u7b7eIDI\u5c06\u516c\u5e73\u6027\u6307\u6807\u63d0\u534733%\u4ee5\u4e0a\uff0cSER\u51c6\u786e\u7387\u4e0b\u964d\u4e0d\u52303%\uff1b\u65e0\u76d1\u7763IDI\u63d0\u5347\u516c\u5e73\u6027\u6307\u680726%\u4ee5\u4e0a\uff0cSER\u6027\u80fd\u4e0b\u964d\u4e0d\u52304%\u3002", "conclusion": "IDI\u6a21\u5757\u5728\u7f3a\u4e4f\u663e\u5f0f\u4eba\u53e3\u7edf\u8ba1\u4fe1\u606f\u65f6\u4ecd\u80fd\u6709\u6548\u51cf\u5c11\u79cd\u65cf\u548c\u5e74\u9f84\u504f\u89c1\uff0c\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2505.14470", "pdf": "https://arxiv.org/pdf/2505.14470", "abs": "https://arxiv.org/abs/2505.14470", "authors": ["Nadav Har-Tuv", "Or Tal", "Yossi Adi"], "title": "PAST: Phonetic-Acoustic Speech Tokenizer", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "comment": null, "summary": "We present PAST, a novel end-to-end framework that jointly models phonetic\ninformation alongside signal reconstruction, eliminating the need for external\npretrained models. Unlike previous approaches that rely on pretrained\nself-supervised models, PAST employs supervised phonetic data, directly\nintegrating domain knowledge into the tokenization process via auxiliary tasks.\nAdditionally, we introduce a streamable, causal variant of PAST, enabling\nreal-time speech applications. Results demonstrate that PAST surpasses existing\nevaluated baseline tokenizers across common evaluation metrics, including\nphonetic representation and speech reconstruction. Notably, PAST also achieves\nsuperior performance when serving as a speech representation for speech\nlanguage models, further highlighting its effectiveness as a foundation for\nspoken language generation. To foster further research, we release the full\nimplementation. For code, model checkpoints, and samples see:\nhttps://pages.cs.huji.ac.il/adiyoss-lab/PAST", "AI": {"tldr": "PAST\u662f\u4e00\u79cd\u65b0\u9896\u7684\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u8054\u5408\u5efa\u6a21\u8bed\u97f3\u4fe1\u606f\u548c\u4fe1\u53f7\u91cd\u5efa\uff0c\u65e0\u9700\u4f9d\u8d56\u5916\u90e8\u9884\u8bad\u7ec3\u6a21\u578b\u3002", "motivation": "\u6d88\u9664\u5bf9\u9884\u8bad\u7ec3\u81ea\u76d1\u7763\u6a21\u578b\u7684\u4f9d\u8d56\uff0c\u76f4\u63a5\u901a\u8fc7\u8f85\u52a9\u4efb\u52a1\u5c06\u9886\u57df\u77e5\u8bc6\u6574\u5408\u5230\u6807\u8bb0\u5316\u8fc7\u7a0b\u4e2d\u3002", "method": "PAST\u5229\u7528\u76d1\u7763\u8bed\u97f3\u6570\u636e\uff0c\u5f15\u5165\u53ef\u6d41\u5f0f\u5904\u7406\u7684\u56e0\u679c\u53d8\u4f53\uff0c\u652f\u6301\u5b9e\u65f6\u8bed\u97f3\u5e94\u7528\u3002", "result": "PAST\u5728\u8bed\u97f3\u8868\u793a\u548c\u8bed\u97f3\u91cd\u5efa\u7b49\u5e38\u89c1\u8bc4\u4f30\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u6807\u8bb0\u5668\uff0c\u5e76\u4f5c\u4e3a\u8bed\u97f3\u8bed\u8a00\u6a21\u578b\u7684\u57fa\u7840\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "PAST\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u8bed\u97f3\u8868\u793a\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u8bed\u97f3\u751f\u6210\u4efb\u52a1\uff0c\u5e76\u516c\u5f00\u4e86\u5b8c\u6574\u5b9e\u73b0\u4ee5\u4fc3\u8fdb\u7814\u7a76\u3002"}}
{"id": "2505.14479", "pdf": "https://arxiv.org/pdf/2505.14479", "abs": "https://arxiv.org/abs/2505.14479", "authors": ["Oren Sultan", "Eitan Stern", "Dafna Shahaf"], "title": "Towards Reliable Proof Generation with LLMs: A Neuro-Symbolic Approach", "categories": ["cs.AI", "cs.CL"], "comment": "long paper", "summary": "Large language models (LLMs) struggle with formal domains that require\nrigorous logical deduction and symbolic reasoning, such as mathematical proof\ngeneration. We propose a neuro-symbolic approach that combines LLMs' generative\nstrengths with structured components to overcome this challenge. As a\nproof-of-concept, we focus on geometry problems. Our approach is two-fold: (1)\nwe retrieve analogous problems and use their proofs to guide the LLM, and (2) a\nformal verifier evaluates the generated proofs and provides feedback, helping\nthe model fix incorrect proofs. We demonstrate that our method significantly\nimproves proof accuracy for OpenAI's o1 model (58%-70% improvement); both\nanalogous problems and the verifier's feedback contribute to these gains. More\nbroadly, shifting to LLMs that generate provably correct conclusions could\ndramatically improve their reliability, accuracy and consistency, unlocking\ncomplex tasks and critical real-world applications that require\ntrustworthiness.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u751f\u6210\u80fd\u529b\u548c\u7ed3\u6784\u5316\u7ec4\u4ef6\u7684\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3LLM\u5728\u903b\u8f91\u63a8\u7406\u548c\u7b26\u53f7\u63a8\u7406\u4efb\u52a1\uff08\u5982\u51e0\u4f55\u8bc1\u660e\uff09\u4e2d\u7684\u4e0d\u8db3\u3002", "motivation": "LLM\u5728\u9700\u8981\u4e25\u683c\u903b\u8f91\u63a8\u7406\u7684\u9886\u57df\u8868\u73b0\u4e0d\u4f73\uff0c\u5982\u6570\u5b66\u8bc1\u660e\u751f\u6210\u3002\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u7ed3\u5408\u795e\u7ecf\u548c\u7b26\u53f7\u65b9\u6cd5\u63d0\u5347\u5176\u6027\u80fd\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\uff1a\uff081\uff09\u68c0\u7d22\u7c7b\u4f3c\u95ee\u9898\u5e76\u5229\u7528\u5176\u8bc1\u660e\u6307\u5bfcLLM\uff1b\uff082\uff09\u4f7f\u7528\u5f62\u5f0f\u5316\u9a8c\u8bc1\u5668\u8bc4\u4f30\u751f\u6210\u7684\u8bc1\u660e\u5e76\u63d0\u4f9b\u53cd\u9988\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86OpenAI o1\u6a21\u578b\u7684\u8bc1\u660e\u51c6\u786e\u7387\uff0858%-70%\u63d0\u5347\uff09\uff0c\u7c7b\u4f3c\u95ee\u9898\u548c\u9a8c\u8bc1\u5668\u53cd\u9988\u5747\u5bf9\u63d0\u5347\u6709\u8d21\u732e\u3002", "conclusion": "\u901a\u8fc7\u751f\u6210\u53ef\u9a8c\u8bc1\u7684\u6b63\u786e\u7ed3\u8bba\uff0cLLM\u7684\u53ef\u9760\u6027\u3001\u51c6\u786e\u6027\u548c\u4e00\u81f4\u6027\u5c06\u5927\u5e45\u63d0\u5347\uff0c\u4ece\u800c\u89e3\u9501\u66f4\u591a\u590d\u6742\u548c\u5173\u952e\u4efb\u52a1\u3002"}}
{"id": "2505.14489", "pdf": "https://arxiv.org/pdf/2505.14489", "abs": "https://arxiv.org/abs/2505.14489", "authors": ["Dongkeun Yoon", "Seungone Kim", "Sohee Yang", "Sunkyoung Kim", "Soyeon Kim", "Yongil Kim", "Eunbi Choi", "Yireun Kim", "Minjoon Seo"], "title": "Reasoning Models Better Express Their Confidence", "categories": ["cs.AI", "cs.CL"], "comment": "Work in progress", "summary": "Despite their strengths, large language models (LLMs) often fail to\ncommunicate their confidence accurately, making it difficult to assess when\nthey might be wrong and limiting their reliability. In this work, we\ndemonstrate that reasoning models-LLMs that engage in extended chain-of-thought\n(CoT) reasoning-exhibit superior performance not only in problem-solving but\nalso in accurately expressing their confidence. Specifically, we benchmark six\nreasoning models across six datasets and find that they achieve strictly better\nconfidence calibration than their non-reasoning counterparts in 33 out of the\n36 settings. Our detailed analysis reveals that these gains in calibration stem\nfrom the slow thinking behaviors of reasoning models-such as exploring\nalternative approaches and backtracking-which enable them to adjust their\nconfidence dynamically throughout their CoT, making it progressively more\naccurate. In particular, we find that reasoning models become increasingly\nbetter calibrated as their CoT unfolds, a trend not observed in non-reasoning\nmodels. Moreover, removing slow thinking behaviors from the CoT leads to a\nsignificant drop in calibration. Lastly, we show that these gains are not\nexclusive to reasoning models-non-reasoning models also benefit when guided to\nperform slow thinking via in-context learning.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u63a8\u7406\u578b\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u901a\u8fc7\u94fe\u5f0f\u601d\u7ef4\uff08CoT\uff09\u63a8\u7406\uff0c\u4e0d\u4ec5\u80fd\u63d0\u5347\u95ee\u9898\u89e3\u51b3\u80fd\u529b\uff0c\u8fd8\u80fd\u66f4\u51c6\u786e\u5730\u8868\u8fbe\u5176\u7f6e\u4fe1\u5ea6\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u7f6e\u4fe1\u5ea6\u8868\u8fbe\u4e0a\u5e38\u4e0d\u51c6\u786e\uff0c\u5f71\u54cd\u5176\u53ef\u9760\u6027\u3002\u672c\u6587\u63a2\u8ba8\u63a8\u7406\u6a21\u578b\u662f\u5426\u80fd\u6539\u5584\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5bf9\u516d\u79cd\u63a8\u7406\u6a21\u578b\u5728\u516d\u4e2a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5206\u6790\u5176\u7f6e\u4fe1\u5ea6\u6821\u51c6\u8868\u73b0\uff0c\u5e76\u7814\u7a76\u6162\u601d\u8003\u884c\u4e3a\uff08\u5982\u63a2\u7d22\u66ff\u4ee3\u65b9\u6848\u548c\u56de\u6eaf\uff09\u7684\u4f5c\u7528\u3002", "result": "\u63a8\u7406\u6a21\u578b\u572836\u79cd\u8bbe\u7f6e\u4e2d\u670933\u79cd\u8868\u73b0\u4f18\u4e8e\u975e\u63a8\u7406\u6a21\u578b\uff0c\u4e14\u7f6e\u4fe1\u5ea6\u6821\u51c6\u968fCoT\u5c55\u5f00\u9010\u6b65\u63d0\u5347\u3002\u79fb\u9664\u6162\u601d\u8003\u884c\u4e3a\u4f1a\u663e\u8457\u964d\u4f4e\u6821\u51c6\u6548\u679c\u3002", "conclusion": "\u63a8\u7406\u6a21\u578b\u901a\u8fc7\u6162\u601d\u8003\u884c\u4e3a\u52a8\u6001\u8c03\u6574\u7f6e\u4fe1\u5ea6\uff0c\u663e\u8457\u63d0\u5347\u6821\u51c6\u80fd\u529b\uff0c\u4e14\u975e\u63a8\u7406\u6a21\u578b\u901a\u8fc7\u4e0a\u4e0b\u6587\u5b66\u4e60\u4e5f\u80fd\u53d7\u76ca\u3002"}}
{"id": "2505.14518", "pdf": "https://arxiv.org/pdf/2505.14518", "abs": "https://arxiv.org/abs/2505.14518", "authors": ["Chun-Yi Kuan", "Hung-yi Lee"], "title": "Teaching Audio-Aware Large Language Models What Does Not Hear: Mitigating Hallucinations through Synthesized Negative Samples", "categories": ["eess.AS", "cs.CL", "cs.SD"], "comment": "Accepted to Interspeech 2025", "summary": "Recent advancements in audio-aware large language models (ALLMs) enable them\nto process and understand audio inputs. However, these models often hallucinate\nnon-existent sound events, reducing their reliability in real-world\napplications. To address this, we propose LISTEN (Learning to Identify Sounds\nThrough Extended Negative Samples), a contrastive-like training method that\nenhances ALLMs' ability to distinguish between present and absent sounds using\nsynthesized data from the backbone LLM. Unlike prior approaches, our method\nrequires no modification to LLM parameters and efficiently integrates audio\nrepresentations via a lightweight adapter. Experiments show that LISTEN\neffectively mitigates hallucinations while maintaining impressive performance\non existing audio question and reasoning benchmarks. At the same time, it is\nmore efficient in both data and computation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faLISTEN\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bf9\u6bd4\u8bad\u7ec3\u589e\u5f3a\u97f3\u9891\u611f\u77e5\u5927\u8bed\u8a00\u6a21\u578b\uff08ALLMs\uff09\u533a\u5206\u771f\u5b9e\u4e0e\u865a\u6784\u58f0\u97f3\u7684\u80fd\u529b\uff0c\u65e0\u9700\u4fee\u6539\u6a21\u578b\u53c2\u6570\uff0c\u9ad8\u6548\u4e14\u6027\u80fd\u4f18\u8d8a\u3002", "motivation": "\u73b0\u6709\u97f3\u9891\u611f\u77e5\u5927\u8bed\u8a00\u6a21\u578b\uff08ALLMs\uff09\u5728\u5904\u7406\u97f3\u9891\u8f93\u5165\u65f6\u5bb9\u6613\u4ea7\u751f\u865a\u6784\u58f0\u97f3\u4e8b\u4ef6\uff0c\u5f71\u54cd\u53ef\u9760\u6027\u3002", "method": "\u63d0\u51faLISTEN\u65b9\u6cd5\uff0c\u5229\u7528\u5408\u6210\u6570\u636e\u548c\u8f7b\u91cf\u9002\u914d\u5668\u8fdb\u884c\u5bf9\u6bd4\u8bad\u7ec3\uff0c\u65e0\u9700\u4fee\u6539LLM\u53c2\u6570\u3002", "result": "\u5b9e\u9a8c\u8868\u660eLISTEN\u6709\u6548\u51cf\u5c11\u865a\u6784\u58f0\u97f3\uff0c\u540c\u65f6\u4fdd\u6301\u97f3\u9891\u95ee\u7b54\u548c\u63a8\u7406\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u4e14\u6570\u636e\u4e0e\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\u3002", "conclusion": "LISTEN\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u65e0\u9700\u4fee\u6539\u6a21\u578b\u53c2\u6570\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86ALLMs\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2505.14569", "pdf": "https://arxiv.org/pdf/2505.14569", "abs": "https://arxiv.org/abs/2505.14569", "authors": ["Devansh Bhardwaj", "Arjun Beniwal", "Shreyas Chaudhari", "Ashwin Kalyan", "Tanmay Rajpurohit", "Karthik R. Narasimhan", "Ameet Deshpande", "Vishvak Murahari"], "title": "Agent Context Protocols Enhance Collective Inference", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "AI agents have become increasingly adept at complex tasks such as coding,\nreasoning, and multimodal understanding. However, building generalist systems\nrequires moving beyond individual agents to collective inference -- a paradigm\nwhere multi-agent systems with diverse, task-specialized agents complement one\nanother through structured communication and collaboration. Today, coordination\nis usually handled with imprecise, ad-hoc natural language, which limits\ncomplex interaction and hinders interoperability with domain-specific agents.\nWe introduce Agent context protocols (ACPs): a domain- and agent-agnostic\nfamily of structured protocols for agent-agent communication, coordination, and\nerror handling. ACPs combine (i) persistent execution blueprints -- explicit\ndependency graphs that store intermediate agent outputs -- with (ii)\nstandardized message schemas, enabling robust and fault-tolerant multi-agent\ncollective inference. ACP-powered generalist systems reach state-of-the-art\nperformance: 28.3 % accuracy on AssistantBench for long-horizon web assistance\nand best-in-class multimodal technical reports, outperforming commercial AI\nsystems in human evaluation. ACPs are highly modular and extensible, allowing\npractitioners to build top-tier generalist agents quickly.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAgent Context Protocols (ACPs)\u7684\u7ed3\u6784\u5316\u534f\u8bae\uff0c\u7528\u4e8e\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u901a\u4fe1\u4e0e\u534f\u4f5c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u534f\u8c03\u4f9d\u8d56\u4e8e\u4e0d\u7cbe\u786e\u7684\u81ea\u7136\u8bed\u8a00\uff0c\u9650\u5236\u4e86\u590d\u6742\u4ea4\u4e92\u548c\u9886\u57df\u4e13\u7528\u667a\u80fd\u4f53\u7684\u4e92\u64cd\u4f5c\u6027\u3002", "method": "\u5f15\u5165ACP\u534f\u8bae\uff0c\u7ed3\u5408\u6301\u4e45\u6267\u884c\u84dd\u56fe\u548c\u6807\u51c6\u5316\u6d88\u606f\u6a21\u5f0f\uff0c\u5b9e\u73b0\u9c81\u68d2\u4e14\u5bb9\u9519\u7684\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u3002", "result": "ACP\u7cfb\u7edf\u5728\u957f\u5468\u671f\u7f51\u7edc\u8f85\u52a9\u4efb\u52a1\u4e2d\u8fbe\u523028.3%\u7684\u51c6\u786e\u7387\uff0c\u5e76\u5728\u591a\u6a21\u6001\u6280\u672f\u62a5\u544a\u4e2d\u4f18\u4e8e\u5546\u4e1aAI\u7cfb\u7edf\u3002", "conclusion": "ACP\u534f\u8bae\u5177\u6709\u9ad8\u5ea6\u6a21\u5757\u5316\u548c\u53ef\u6269\u5c55\u6027\uff0c\u80fd\u5feb\u901f\u6784\u5efa\u9ad8\u6027\u80fd\u901a\u7528\u667a\u80fd\u4f53\u3002"}}
{"id": "2505.14615", "pdf": "https://arxiv.org/pdf/2505.14615", "abs": "https://arxiv.org/abs/2505.14615", "authors": ["Anjiang Wei", "Yuheng Wu", "Yingjia Wan", "Tarun Suresh", "Huanmi Tan", "Zhanke Zhou", "Sanmi Koyejo", "Ke Wang", "Alex Aiken"], "title": "SATBench: Benchmarking LLMs' Logical Reasoning via Automated Puzzle Generation from SAT Formulas", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.LO"], "comment": null, "summary": "We introduce SATBench, a benchmark for evaluating the logical reasoning\ncapabilities of large language models (LLMs) through logical puzzles derived\nfrom Boolean satisfiability (SAT) problems. Unlike prior work that focuses on\ninference rule-based reasoning, which often involves deducing conclusions from\na set of premises, our approach leverages the search-based nature of SAT\nproblems, where the objective is to find a solution that fulfills a specified\nset of logical constraints. Each instance in SATBench is generated from a SAT\nformula, then translated into a story context and conditions using LLMs. The\ngeneration process is fully automated and allows for adjustable difficulty by\nvarying the number of clauses. All 2100 puzzles are validated through both\nLLM-assisted and solver-based consistency checks, with human validation on a\nsubset. Experimental results show that even the strongest model, o4-mini,\nachieves only 65.0% accuracy on hard UNSAT problems, close to the random\nbaseline of 50%. SATBench exposes fundamental limitations in the search-based\nlogical reasoning abilities of current LLMs and provides a scalable testbed for\nfuture research in logical reasoning.", "AI": {"tldr": "SATBench\u662f\u4e00\u4e2a\u901a\u8fc7\u5e03\u5c14\u53ef\u6ee1\u8db3\u6027\u95ee\u9898\u751f\u6210\u7684\u903b\u8f91\u8c1c\u9898\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u903b\u8f91\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u57fa\u4e8e\u63a8\u7406\u89c4\u5219\u7684\u903b\u8f91\u63a8\u7406\uff0c\u800cSATBench\u5219\u5229\u7528SAT\u95ee\u9898\u7684\u641c\u7d22\u7279\u6027\uff0c\u586b\u8865\u4e86LLMs\u5728\u641c\u7d22\u5f0f\u903b\u8f91\u63a8\u7406\u80fd\u529b\u8bc4\u4f30\u4e0a\u7684\u7a7a\u767d\u3002", "method": "SATBench\u901a\u8fc7\u81ea\u52a8\u5316\u751f\u6210\u57fa\u4e8eSAT\u516c\u5f0f\u7684\u903b\u8f91\u8c1c\u9898\uff0c\u5e76\u8c03\u6574\u5b50\u53e5\u6570\u91cf\u4ee5\u63a7\u5236\u96be\u5ea6\uff0c\u540c\u65f6\u4f7f\u7528LLM\u8f85\u52a9\u548c\u6c42\u89e3\u5668\u9a8c\u8bc1\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u6700\u5f3a\u6a21\u578bo4-mini\u5728\u56f0\u96beUNSAT\u95ee\u9898\u4e0a\u51c6\u786e\u7387\u4ec5\u4e3a65.0%\uff0c\u63a5\u8fd1\u968f\u673a\u57fa\u7ebf50%\u3002", "conclusion": "SATBench\u63ed\u793a\u4e86\u5f53\u524dLLMs\u5728\u641c\u7d22\u5f0f\u903b\u8f91\u63a8\u7406\u4e0a\u7684\u6839\u672c\u5c40\u9650\u6027\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u6d4b\u8bd5\u5e73\u53f0\u3002"}}
{"id": "2505.14620", "pdf": "https://arxiv.org/pdf/2505.14620", "abs": "https://arxiv.org/abs/2505.14620", "authors": ["Morgan Lindsay Heisler", "Linzi Xing", "Ge Shi", "Hanieh Sadri", "Gursimran Singh", "Weiwei Zhang", "Tao Ye", "Ying Xiong", "Yong Zhang", "Zhenan Fan"], "title": "Enhancing Learned Knowledge in LoRA Adapters Through Efficient Contrastive Decoding on Ascend NPUs", "categories": ["cs.LG", "cs.CL"], "comment": "Accepted at ACM KDD 2025", "summary": "Huawei Cloud users leverage LoRA (Low-Rank Adaptation) as an efficient and\nscalable method to fine-tune and customize large language models (LLMs) for\napplication-specific needs. However, tasks that require complex reasoning or\ndeep contextual understanding are often hindered by biases or interference from\nthe base model when using typical decoding methods like greedy or beam search.\nThese biases can lead to generic or task-agnostic responses from the base model\ninstead of leveraging the LoRA-specific adaptations. In this paper, we\nintroduce Contrastive LoRA Decoding (CoLD), a novel decoding framework designed\nto maximize the use of task-specific knowledge in LoRA-adapted models,\nresulting in better downstream performance. CoLD uses contrastive decoding by\nscoring candidate tokens based on the divergence between the probability\ndistributions of a LoRA-adapted expert model and the corresponding base model.\nThis approach prioritizes tokens that better align with the LoRA's learned\nrepresentations, enhancing performance for specialized tasks. While effective,\na naive implementation of CoLD is computationally expensive because each\ndecoding step requires evaluating multiple token candidates across both models.\nTo address this, we developed an optimized kernel for Huawei's Ascend NPU. CoLD\nachieves up to a 5.54% increase in task accuracy while reducing end-to-end\nlatency by 28% compared to greedy decoding. This work provides practical and\nefficient decoding strategies for fine-tuned LLMs in resource-constrained\nenvironments and has broad implications for applied data science in both cloud\nand on-premises settings.", "AI": {"tldr": "\u534e\u4e3a\u4e91\u7528\u6237\u4f7f\u7528LoRA\uff08\u4f4e\u79e9\u9002\u5e94\uff09\u9ad8\u6548\u5fae\u8c03\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\uff0c\u4f46\u4f20\u7edf\u89e3\u7801\u65b9\u6cd5\uff08\u5982\u8d2a\u5a6a\u6216\u675f\u641c\u7d22\uff09\u53ef\u80fd\u5bfc\u81f4\u4efb\u52a1\u65e0\u5173\u7684\u54cd\u5e94\u3002\u672c\u6587\u63d0\u51fa\u5bf9\u6bd4LoRA\u89e3\u7801\uff08CoLD\uff09\uff0c\u901a\u8fc7\u5bf9\u6bd4LoRA\u9002\u5e94\u6a21\u578b\u4e0e\u57fa\u7840\u6a21\u578b\u7684\u6982\u7387\u5206\u5e03\u5dee\u5f02\uff0c\u4f18\u5148\u9009\u62e9\u4e0eLoRA\u5b66\u4e60\u8868\u793a\u66f4\u4e00\u81f4\u7684\u6807\u8bb0\uff0c\u63d0\u5347\u4efb\u52a1\u6027\u80fd\u3002\u4f18\u5316\u540e\u7684CoLD\u5728\u534e\u4e3aAscend NPU\u4e0a\u5b9e\u73b0\u4efb\u52a1\u51c6\u786e\u7387\u63d0\u53475.54%\uff0c\u5ef6\u8fdf\u964d\u4f4e28%\u3002", "motivation": "\u4f20\u7edf\u89e3\u7801\u65b9\u6cd5\u5728\u590d\u6742\u63a8\u7406\u6216\u6df1\u5ea6\u4e0a\u4e0b\u6587\u7406\u89e3\u4efb\u52a1\u4e2d\u53ef\u80fd\u56e0\u57fa\u7840\u6a21\u578b\u7684\u504f\u89c1\u6216\u5e72\u6270\u800c\u8868\u73b0\u4e0d\u4f73\uff0c\u5bfc\u81f4\u4efb\u52a1\u65e0\u5173\u7684\u54cd\u5e94\u3002", "method": "\u63d0\u51faCoLD\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u6bd4LoRA\u9002\u5e94\u6a21\u578b\u4e0e\u57fa\u7840\u6a21\u578b\u7684\u6982\u7387\u5206\u5e03\u5dee\u5f02\uff0c\u4f18\u5148\u9009\u62e9\u4e0eLoRA\u5b66\u4e60\u8868\u793a\u66f4\u4e00\u81f4\u7684\u6807\u8bb0\u3002\u4f18\u5316\u5b9e\u73b0\u4ee5\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u3002", "result": "CoLD\u5728\u4efb\u52a1\u51c6\u786e\u7387\u4e0a\u63d0\u53475.54%\uff0c\u7aef\u5230\u7aef\u5ef6\u8fdf\u964d\u4f4e28%\u3002", "conclusion": "CoLD\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u7684\u5fae\u8c03LLM\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u7801\u7b56\u7565\uff0c\u5bf9\u4e91\u7aef\u548c\u672c\u5730\u5e94\u7528\u5177\u6709\u5e7f\u6cdb\u610f\u4e49\u3002"}}
{"id": "2505.14625", "pdf": "https://arxiv.org/pdf/2505.14625", "abs": "https://arxiv.org/abs/2505.14625", "authors": ["Zhangchen Xu", "Yuetai Li", "Fengqing Jiang", "Bhaskar Ramasubramanian", "Luyao Niu", "Bill Yuchen Lin", "Radha Poovendran"], "title": "TinyV: Reducing False Negatives in Verification Improves RL for LLM Reasoning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Reinforcement Learning (RL) has become a powerful tool for enhancing the\nreasoning abilities of large language models (LLMs) by optimizing their\npolicies with reward signals. Yet, RL's success relies on the reliability of\nrewards, which are provided by verifiers. In this paper, we expose and analyze\na widespread problem--false negatives--where verifiers wrongly reject correct\nmodel outputs. Our in-depth study of the Big-Math-RL-Verified dataset reveals\nthat over 38% of model-generated responses suffer from false negatives, where\nthe verifier fails to recognize correct answers. We show, both empirically and\ntheoretically, that these false negatives severely impair RL training by\ndepriving the model of informative gradient signals and slowing convergence. To\nmitigate this, we propose tinyV, a lightweight LLM-based verifier that augments\nexisting rule-based methods, which dynamically identifies potential false\nnegatives and recovers valid responses to produce more accurate reward\nestimates. Across multiple math-reasoning benchmarks, integrating TinyV boosts\npass rates by up to 10% and accelerates convergence relative to the baseline.\nOur findings highlight the critical importance of addressing verifier false\nnegatives and offer a practical approach to improve RL-based fine-tuning of\nLLMs. Our code is available at https://github.com/uw-nsl/TinyV.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86\u5f3a\u5316\u5b66\u4e60\u4e2d\u9a8c\u8bc1\u5668\u9519\u8bef\u62d2\u7edd\u6b63\u786e\u6a21\u578b\u8f93\u51fa\u7684\u95ee\u9898\uff08\u5047\u9634\u6027\uff09\uff0c\u5e76\u63d0\u51fa\u8f7b\u91cf\u7ea7\u9a8c\u8bc1\u5668tinyV\u4ee5\u63d0\u5347\u5956\u52b1\u4fe1\u53f7\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u4f9d\u8d56\u9a8c\u8bc1\u5668\u63d0\u4f9b\u7684\u5956\u52b1\u4fe1\u53f7\uff0c\u4f46\u5047\u9634\u6027\u95ee\u9898\u666e\u904d\u5b58\u5728\uff0c\u4e25\u91cd\u5f71\u54cd\u6a21\u578b\u8bad\u7ec3\u6548\u679c\u3002", "method": "\u901a\u8fc7\u5206\u6790Big-Math-RL-Verified\u6570\u636e\u96c6\uff0c\u63d0\u51fatinyV\u9a8c\u8bc1\u5668\uff0c\u52a8\u6001\u8bc6\u522b\u5047\u9634\u6027\u5e76\u6062\u590d\u6709\u6548\u54cd\u5e94\u3002", "result": "tinyV\u5728\u591a\u4e2a\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u63d0\u5347\u901a\u8fc7\u738710%\uff0c\u5e76\u52a0\u901f\u6536\u655b\u3002", "conclusion": "\u89e3\u51b3\u5047\u9634\u6027\u95ee\u9898\u5bf9\u63d0\u5347LLM\u7684RL\u5fae\u8c03\u81f3\u5173\u91cd\u8981\uff0ctinyV\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u65b9\u6cd5\u3002"}}
{"id": "2505.14627", "pdf": "https://arxiv.org/pdf/2505.14627", "abs": "https://arxiv.org/abs/2505.14627", "authors": ["Ashutosh Adhikari", "Mirella Lapata"], "title": "Debating for Better Reasoning: An Unsupervised Multimodal Approach", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "As Large Language Models (LLMs) gain expertise across diverse domains and\nmodalities, scalable oversight becomes increasingly challenging, particularly\nwhen their capabilities may surpass human evaluators. Debate has emerged as a\npromising mechanism for enabling such oversight. In this work, we extend the\ndebate paradigm to a multimodal setting, exploring its potential for weaker\nmodels to supervise and enhance the performance of stronger models. We focus on\nvisual question answering (VQA), where two \"sighted\" expert vision-language\nmodels debate an answer, while a \"blind\" (text-only) judge adjudicates based\nsolely on the quality of the arguments. In our framework, the experts defend\nonly answers aligned with their beliefs, thereby obviating the need for\nexplicit role-playing and concentrating the debate on instances of expert\ndisagreement. Experiments on several multimodal tasks demonstrate that the\ndebate framework consistently outperforms individual expert models. Moreover,\njudgments from weaker LLMs can help instill reasoning capabilities in\nvision-language models through finetuning.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5728\u591a\u6a21\u6001\u73af\u5883\u4e2d\u6269\u5c55\u8fa9\u8bba\u673a\u5236\uff0c\u901a\u8fc7\u8fa9\u8bba\u8ba9\u8f83\u5f31\u6a21\u578b\u76d1\u7763\u548c\u589e\u5f3a\u8f83\u5f3a\u6a21\u578b\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\u4e2d\u3002\u5b9e\u9a8c\u8868\u660e\u8fa9\u8bba\u6846\u67b6\u4f18\u4e8e\u5355\u4e2a\u4e13\u5bb6\u6a21\u578b\uff0c\u4e14\u8f83\u5f31LLM\u7684\u5224\u65ad\u80fd\u901a\u8fc7\u5fae\u8c03\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u591a\u9886\u57df\u548c\u591a\u6a21\u6001\u4e2d\u7684\u80fd\u529b\u63d0\u5347\uff0c\u5982\u4f55\u5b9e\u73b0\u53ef\u6269\u5c55\u7684\u76d1\u7763\u6210\u4e3a\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u6a21\u578b\u80fd\u529b\u53ef\u80fd\u8d85\u8d8a\u4eba\u7c7b\u8bc4\u4f30\u8005\u65f6\u3002\u8fa9\u8bba\u673a\u5236\u88ab\u8ba4\u4e3a\u662f\u4e00\u79cd\u6709\u6f5c\u529b\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7814\u7a76\u5c06\u8fa9\u8bba\u8303\u5f0f\u6269\u5c55\u5230\u591a\u6a21\u6001\u73af\u5883\uff0c\u4e13\u6ce8\u4e8e\u89c6\u89c9\u95ee\u7b54\uff08VQA\uff09\u4efb\u52a1\u3002\u4e24\u4e2a\u201c\u6709\u89c6\u89c9\u201d\u7684\u4e13\u5bb6\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u8fa9\u8bba\uff0c\u4e00\u4e2a\u201c\u65e0\u89c6\u89c9\u201d\uff08\u4ec5\u6587\u672c\uff09\u7684\u6cd5\u5b98\u6839\u636e\u8fa9\u8bba\u8d28\u91cf\u88c1\u51b3\u3002\u4e13\u5bb6\u4ec5\u652f\u6301\u5176\u771f\u5b9e\u4fe1\u5ff5\u7684\u7b54\u6848\uff0c\u907f\u514d\u89d2\u8272\u626e\u6f14\uff0c\u96c6\u4e2d\u8fa9\u8bba\u4e8e\u4e13\u5bb6\u5206\u6b67\u7684\u5b9e\u4f8b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8fa9\u8bba\u6846\u67b6\u5728\u591a\u4e2a\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u6301\u7eed\u4f18\u4e8e\u5355\u4e2a\u4e13\u5bb6\u6a21\u578b\u3002\u6b64\u5916\uff0c\u8f83\u5f31LLM\u7684\u88c1\u51b3\u53ef\u901a\u8fc7\u5fae\u8c03\u5e2e\u52a9\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63d0\u5347\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "\u8fa9\u8bba\u673a\u5236\u5728\u591a\u6a21\u6001\u73af\u5883\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u80fd\u591f\u901a\u8fc7\u8f83\u5f31\u6a21\u578b\u7684\u76d1\u7763\u63d0\u5347\u8f83\u5f3a\u6a21\u578b\u7684\u6027\u80fd\uff0c\u540c\u65f6\u589e\u5f3a\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2505.14667", "pdf": "https://arxiv.org/pdf/2505.14667", "abs": "https://arxiv.org/abs/2505.14667", "authors": ["Wonje Jeung", "Sangyeon Yoon", "Minsuk Kahng", "Albert No"], "title": "SAFEPATH: Preventing Harmful Reasoning in Chain-of-Thought via Early Alignment", "categories": ["cs.AI", "cs.CL"], "comment": "22 pages", "summary": "Large Reasoning Models (LRMs) have become powerful tools for complex problem\nsolving, but their structured reasoning pathways can lead to unsafe outputs\nwhen exposed to harmful prompts. Existing safety alignment methods reduce\nharmful outputs but can degrade reasoning depth, leading to significant\ntrade-offs in complex, multi-step tasks, and remain vulnerable to sophisticated\njailbreak attacks. To address this, we introduce SAFEPATH, a lightweight\nalignment method that fine-tunes LRMs to emit a short, 8-token Safety Primer at\nthe start of their reasoning, in response to harmful prompts, while leaving the\nrest of the reasoning process unsupervised. Empirical results across multiple\nbenchmarks indicate that SAFEPATH effectively reduces harmful outputs while\nmaintaining reasoning performance. Specifically, SAFEPATH reduces harmful\nresponses by up to 90.0% and blocks 83.3% of jailbreak attempts in the\nDeepSeek-R1-Distill-Llama-8B model, while requiring 295.9x less compute than\nDirect Refusal and 314.1x less than SafeChain. We further introduce a zero-shot\nvariant that requires no fine-tuning. In addition, we provide a comprehensive\nanalysis of how existing methods in LLMs generalize, or fail, when applied to\nreasoning-centric models, revealing critical gaps and new directions for safer\nAI.", "AI": {"tldr": "SAFEPATH\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u5bf9\u9f50\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u6709\u5bb3\u63d0\u793a\u4e0b\u751f\u62108\u4e2atoken\u7684\u5b89\u5168\u63d0\u793a\uff0c\u6709\u6548\u51cf\u5c11\u6709\u5bb3\u8f93\u51fa\u5e76\u4fdd\u6301\u63a8\u7406\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u5728\u590d\u6742\u95ee\u9898\u89e3\u51b3\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u7ed3\u6784\u5316\u63a8\u7406\u8def\u5f84\u53ef\u80fd\u5bfc\u81f4\u6709\u5bb3\u8f93\u51fa\uff0c\u73b0\u6709\u5b89\u5168\u5bf9\u9f50\u65b9\u6cd5\u4f1a\u964d\u4f4e\u63a8\u7406\u6df1\u5ea6\u4e14\u6613\u53d7\u653b\u51fb\u3002", "method": "\u5f15\u5165SAFEPATH\uff0c\u901a\u8fc7\u5fae\u8c03LRMs\u5728\u6709\u5bb3\u63d0\u793a\u4e0b\u751f\u6210\u7b80\u77ed\u5b89\u5168\u63d0\u793a\uff0c\u5176\u4f59\u63a8\u7406\u8fc7\u7a0b\u4e0d\u53d7\u76d1\u7763\u3002", "result": "SAFEPATH\u51cf\u5c1190.0%\u6709\u5bb3\u8f93\u51fa\uff0c\u963b\u6b6283.3%\u8d8a\u72f1\u653b\u51fb\uff0c\u8ba1\u7b97\u6210\u672c\u663e\u8457\u4f4e\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002", "conclusion": "SAFEPATH\u5728\u4fdd\u6301\u63a8\u7406\u6027\u80fd\u7684\u540c\u65f6\u63d0\u5347\u5b89\u5168\u6027\uff0c\u4e3a\u96f6\u6837\u672c\u53d8\u4f53\u63d0\u4f9b\u53ef\u80fd\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u63a8\u7406\u6a21\u578b\u4e2d\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2505.14668", "pdf": "https://arxiv.org/pdf/2505.14668", "abs": "https://arxiv.org/abs/2505.14668", "authors": ["Bufang Yang", "Lilin Xu", "Liekang Zeng", "Kaiwei Liu", "Siyang Jiang", "Wenrui Lu", "Hongkai Chen", "Xiaofan Jiang", "Guoliang Xing", "Zhenyu Yan"], "title": "ContextAgent: Context-Aware Proactive LLM Agents with Open-World Sensory Perceptions", "categories": ["cs.AI", "cs.CL", "cs.HC"], "comment": null, "summary": "Recent advances in Large Language Models (LLMs) have propelled intelligent\nagents from reactive responses to proactive support. While promising, existing\nproactive agents either rely exclusively on observations from enclosed\nenvironments (e.g., desktop UIs) with direct LLM inference or employ rule-based\nproactive notifications, leading to suboptimal user intent understanding and\nlimited functionality for proactive service. In this paper, we introduce\nContextAgent, the first context-aware proactive agent that incorporates\nextensive sensory contexts to enhance the proactive capabilities of LLM agents.\nContextAgent first extracts multi-dimensional contexts from massive sensory\nperceptions on wearables (e.g., video and audio) to understand user intentions.\nContextAgent then leverages the sensory contexts and the persona contexts from\nhistorical data to predict the necessity for proactive services. When proactive\nassistance is needed, ContextAgent further automatically calls the necessary\ntools to assist users unobtrusively. To evaluate this new task, we curate\nContextAgentBench, the first benchmark for evaluating context-aware proactive\nLLM agents, covering 1,000 samples across nine daily scenarios and twenty\ntools. Experiments on ContextAgentBench show that ContextAgent outperforms\nbaselines by achieving up to 8.5% and 6.0% higher accuracy in proactive\npredictions and tool calling, respectively. We hope our research can inspire\nthe development of more advanced, human-centric, proactive AI assistants.", "AI": {"tldr": "ContextAgent\u662f\u4e00\u79cd\u57fa\u4e8e\u591a\u7ef4\u5ea6\u611f\u77e5\u4e0a\u4e0b\u6587\u7684\u4e3b\u52a8\u4ee3\u7406\uff0c\u901a\u8fc7\u7a7f\u6234\u8bbe\u5907\u6570\u636e\u589e\u5f3aLLM\u4ee3\u7406\u7684\u4e3b\u52a8\u670d\u52a1\u80fd\u529b\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u3002", "motivation": "\u73b0\u6709\u4e3b\u52a8\u4ee3\u7406\u4f9d\u8d56\u5c01\u95ed\u73af\u5883\u6216\u89c4\u5219\u901a\u77e5\uff0c\u5bfc\u81f4\u7528\u6237\u610f\u56fe\u7406\u89e3\u4e0d\u8db3\u548c\u529f\u80fd\u53d7\u9650\u3002", "method": "ContextAgent\u4ece\u7a7f\u6234\u8bbe\u5907\u63d0\u53d6\u591a\u7ef4\u5ea6\u4e0a\u4e0b\u6587\uff0c\u7ed3\u5408\u5386\u53f2\u6570\u636e\u9884\u6d4b\u4e3b\u52a8\u670d\u52a1\u9700\u6c42\uff0c\u5e76\u81ea\u52a8\u8c03\u7528\u5de5\u5177\u3002", "result": "\u5728ContextAgentBench\u4e0a\uff0cContextAgent\u5728\u4e3b\u52a8\u9884\u6d4b\u548c\u5de5\u5177\u8c03\u7528\u4e0a\u5206\u522b\u6bd4\u57fa\u7ebf\u9ad88.5%\u548c6.0%\u3002", "conclusion": "ContextAgent\u4e3a\u5f00\u53d1\u66f4\u5148\u8fdb\u3001\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u4e3b\u52a8AI\u52a9\u624b\u63d0\u4f9b\u4e86\u542f\u793a\u3002"}}
{"id": "2505.14680", "pdf": "https://arxiv.org/pdf/2505.14680", "abs": "https://arxiv.org/abs/2505.14680", "authors": ["Sunhao Dai", "Wenjie Wang", "Liang Pang", "Jun Xu", "See-Kiong Ng", "Ji-Rong Wen", "Tat-Seng Chua"], "title": "NExT-Search: Rebuilding User Feedback Ecosystem for Generative AI Search", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.HC"], "comment": "SIGIR 2025 Perspective Paper", "summary": "Generative AI search is reshaping information retrieval by offering\nend-to-end answers to complex queries, reducing users' reliance on manually\nbrowsing and summarizing multiple web pages. However, while this paradigm\nenhances convenience, it disrupts the feedback-driven improvement loop that has\nhistorically powered the evolution of traditional Web search. Web search can\ncontinuously improve their ranking models by collecting large-scale,\nfine-grained user feedback (e.g., clicks, dwell time) at the document level. In\ncontrast, generative AI search operates through a much longer search pipeline,\nspanning query decomposition, document retrieval, and answer generation, yet\ntypically receives only coarse-grained feedback on the final answer. This\nintroduces a feedback loop disconnect, where user feedback for the final output\ncannot be effectively mapped back to specific system components, making it\ndifficult to improve each intermediate stage and sustain the feedback loop. In\nthis paper, we envision NExT-Search, a next-generation paradigm designed to\nreintroduce fine-grained, process-level feedback into generative AI search.\nNExT-Search integrates two complementary modes: User Debug Mode, which allows\nengaged users to intervene at key stages; and Shadow User Mode, where a\npersonalized user agent simulates user preferences and provides AI-assisted\nfeedback for less interactive users. Furthermore, we envision how these\nfeedback signals can be leveraged through online adaptation, which refines\ncurrent search outputs in real-time, and offline update, which aggregates\ninteraction logs to periodically fine-tune query decomposition, retrieval, and\ngeneration models. By restoring human control over key stages of the generative\nAI search pipeline, we believe NExT-Search offers a promising direction for\nbuilding feedback-rich AI search systems that can evolve continuously alongside\nhuman feedback.", "AI": {"tldr": "NExT-Search\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7\u7ec6\u7c92\u5ea6\u3001\u8fc7\u7a0b\u7ea7\u53cd\u9988\u6539\u8fdb\u751f\u6210\u5f0fAI\u641c\u7d22\uff0c\u89e3\u51b3\u4f20\u7edf\u53cd\u9988\u5faa\u73af\u4e2d\u65ad\u7684\u95ee\u9898\u3002", "motivation": "\u751f\u6210\u5f0fAI\u641c\u7d22\u867d\u7136\u4fbf\u6377\uff0c\u4f46\u7834\u574f\u4e86\u4f20\u7edfWeb\u641c\u7d22\u7684\u53cd\u9988\u9a71\u52a8\u6539\u8fdb\u5faa\u73af\uff0c\u5bfc\u81f4\u96be\u4ee5\u4f18\u5316\u4e2d\u95f4\u9636\u6bb5\u3002", "method": "NExT-Search\u5f15\u5165\u4e24\u79cd\u6a21\u5f0f\uff1a\u7528\u6237\u8c03\u8bd5\u6a21\u5f0f\u548c\u5f71\u5b50\u7528\u6237\u6a21\u5f0f\uff0c\u7ed3\u5408\u5728\u7ebf\u9002\u5e94\u548c\u79bb\u7ebf\u66f4\u65b0\uff0c\u4f18\u5316\u641c\u7d22\u6d41\u7a0b\u3002", "result": "\u901a\u8fc7\u6062\u590d\u4eba\u7c7b\u5bf9\u641c\u7d22\u6d41\u7a0b\u5173\u952e\u9636\u6bb5\u7684\u63a7\u5236\uff0cNExT-Search\u6709\u671b\u6784\u5efa\u53cd\u9988\u4e30\u5bcc\u7684AI\u641c\u7d22\u7cfb\u7edf\u3002", "conclusion": "NExT-Search\u4e3a\u751f\u6210\u5f0fAI\u641c\u7d22\u7684\u6301\u7eed\u6539\u8fdb\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\u3002"}}
