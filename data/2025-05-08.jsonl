{"id": "2505.04002", "pdf": "https://arxiv.org/pdf/2505.04002", "abs": "https://arxiv.org/abs/2505.04002", "authors": ["Michael Xu", "Yi Shi", "KangKang Yin", "Xue Bin Peng"], "title": "PARC: Physics-based Augmentation with Reinforcement Learning for Character Controllers", "categories": ["cs.GR", "cs.AI", "cs.LG", "cs.RO"], "comment": "SIGGRAPH Conference Papers 2025", "summary": "Humans excel in navigating diverse, complex environments with agile motor\nskills, exemplified by parkour practitioners performing dynamic maneuvers, such\nas climbing up walls and jumping across gaps. Reproducing these agile movements\nwith simulated characters remains challenging, in part due to the scarcity of\nmotion capture data for agile terrain traversal behaviors and the high cost of\nacquiring such data. In this work, we introduce PARC (Physics-based\nAugmentation with Reinforcement Learning for Character Controllers), a\nframework that leverages machine learning and physics-based simulation to\niteratively augment motion datasets and expand the capabilities of terrain\ntraversal controllers. PARC begins by training a motion generator on a small\ndataset consisting of core terrain traversal skills. The motion generator is\nthen used to produce synthetic data for traversing new terrains. However, these\ngenerated motions often exhibit artifacts, such as incorrect contacts or\ndiscontinuities. To correct these artifacts, we train a physics-based tracking\ncontroller to imitate the motions in simulation. The corrected motions are then\nadded to the dataset, which is used to continue training the motion generator\nin the next iteration. PARC's iterative process jointly expands the\ncapabilities of the motion generator and tracker, creating agile and versatile\nmodels for interacting with complex environments. PARC provides an effective\napproach to develop controllers for agile terrain traversal, which bridges the\ngap between the scarcity of motion data and the need for versatile character\ncontrollers."}
{"id": "2505.04050", "pdf": "https://arxiv.org/pdf/2505.04050", "abs": "https://arxiv.org/abs/2505.04050", "authors": ["Kazuki Higo", "Toshiki Kanai", "Yuki Endo", "Yoshihiro Kanamori"], "title": "TerraFusion: Joint Generation of Terrain Geometry and Texture Using Latent Diffusion Models", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "3D terrain models are essential in fields such as video game development and\nfilm production. Since surface color often correlates with terrain geometry,\ncapturing this relationship is crucial to achieving realism. However, most\nexisting methods generate either a heightmap or a texture, without sufficiently\naccounting for the inherent correlation. In this paper, we propose a method\nthat jointly generates terrain heightmaps and textures using a latent diffusion\nmodel. First, we train the model in an unsupervised manner to randomly generate\npaired heightmaps and textures. Then, we perform supervised learning of an\nexternal adapter to enable user control via hand-drawn sketches. Experiments\nshow that our approach allows intuitive terrain generation while preserving the\ncorrelation between heightmaps and textures."}
{"id": "2505.04051", "pdf": "https://arxiv.org/pdf/2505.04051", "abs": "https://arxiv.org/abs/2505.04051", "authors": ["Junming Huang", "Chi Wang", "Letian Li", "Changxin Huang", "Qiang Dai", "Weiwei Xu"], "title": "BuildingBlock: A Hybrid Approach for Structured Building Generation", "categories": ["cs.GR"], "comment": "SIGGRAPH 2025 (Conference Track)", "summary": "Three-dimensional building generation is vital for applications in gaming,\nvirtual reality, and digital twins, yet current methods face challenges in\nproducing diverse, structured, and hierarchically coherent buildings. We\npropose BuildingBlock, a hybrid approach that integrates generative models,\nprocedural content generation (PCG), and large language models (LLMs) to\naddress these limitations. Specifically, our method introduces a two-phase\npipeline: the Layout Generation Phase (LGP) and the Building Construction Phase\n(BCP).\n  LGP reframes box-based layout generation as a point-cloud generation task,\nutilizing a newly constructed architectural dataset and a Transformer-based\ndiffusion model to create globally consistent layouts. With LLMs, these layouts\nare extended into rule-based hierarchical designs, seamlessly incorporating\ncomponent styles and spatial structures.\n  The BCP leverages these layouts to guide PCG, enabling local-customizable,\nhigh-quality structured building generation. Experimental results demonstrate\nBuildingBlock's effectiveness in generating diverse and hierarchically\nstructured buildings, achieving state-of-the-art results on multiple\nbenchmarks, and paving the way for scalable and intuitive architectural\nworkflows."}
{"id": "2505.03821", "pdf": "https://arxiv.org/pdf/2505.03821", "abs": "https://arxiv.org/abs/2505.03821", "authors": ["Gracjan Góral", "Alicja Ziarko", "Piotr Miłoś", "Michał Nauman", "Maciej Wołczyk", "Michał Kosiński"], "title": "Beyond Recognition: Evaluating Visual Perspective Taking in Vision Language Models", "categories": ["cs.CV", "cs.AI"], "comment": "Dataset:\n  https://huggingface.co/datasets/Gracjan/Isle/viewer/Isle-Brick-V2", "summary": "We investigate the ability of Vision Language Models (VLMs) to perform visual\nperspective taking using a novel set of visual tasks inspired by established\nhuman tests. Our approach leverages carefully controlled scenes, in which a\nsingle humanoid minifigure is paired with a single object. By systematically\nvarying spatial configurations - such as object position relative to the\nhumanoid minifigure and the humanoid minifigure's orientation - and using both\nbird's-eye and surface-level views, we created 144 unique visual tasks. Each\nvisual task is paired with a series of 7 diagnostic questions designed to\nassess three levels of visual cognition: scene understanding, spatial\nreasoning, and visual perspective taking. Our evaluation of several\nstate-of-the-art models, including GPT-4-Turbo, GPT-4o,\nLlama-3.2-11B-Vision-Instruct, and variants of Claude Sonnet, reveals that\nwhile they excel in scene understanding, the performance declines significantly\non spatial reasoning and further deteriorates on perspective-taking. Our\nanalysis suggests a gap between surface-level object recognition and the deeper\nspatial and perspective reasoning required for complex visual tasks, pointing\nto the need for integrating explicit geometric representations and tailored\ntraining protocols in future VLM development."}
{"id": "2505.04052", "pdf": "https://arxiv.org/pdf/2505.04052", "abs": "https://arxiv.org/abs/2505.04052", "authors": ["Shun Masuda", "Yuki Endo", "Yoshihiro Kanamori"], "title": "Person-In-Situ: Scene-Consistent Human Image Insertion with Occlusion-Aware Pose Control", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Compositing human figures into scene images has broad applications in areas\nsuch as entertainment and advertising. However, existing methods often cannot\nhandle occlusion of the inserted person by foreground objects and unnaturally\nplace the person in the frontmost layer. Moreover, they offer limited control\nover the inserted person's pose. To address these challenges, we propose two\nmethods. Both allow explicit pose control via a 3D body model and leverage\nlatent diffusion models to synthesize the person at a contextually appropriate\ndepth, naturally handling occlusions without requiring occlusion masks. The\nfirst is a two-stage approach: the model first learns a depth map of the scene\nwith the person through supervised learning, and then synthesizes the person\naccordingly. The second method learns occlusion implicitly and synthesizes the\nperson directly from input data without explicit depth supervision.\nQuantitative and qualitative evaluations show that both methods outperform\nexisting approaches by better preserving scene consistency while accurately\nreflecting occlusions and user-specified poses."}
{"id": "2505.03826", "pdf": "https://arxiv.org/pdf/2505.03826", "abs": "https://arxiv.org/abs/2505.03826", "authors": ["Minji Kang", "Seongho Kim", "Eunseo Go", "Donghyeon Paek", "Geon Lim", "Muyoung Kim", "Soyeun Kim", "Sung Kyu Jang", "Min Sup Choi", "Woo Seok Kang", "Jaehyun Kim", "Jaekwang Kim", "Hyeong-U Kim"], "title": "In-situ and Non-contact Etch Depth Prediction in Plasma Etching via Machine Learning (ANN & BNN) and Digital Image Colorimetry", "categories": ["cs.CV", "cs.AI"], "comment": "20 pages", "summary": "Precise monitoring of etch depth and the thickness of insulating materials,\nsuch as Silicon dioxide and silicon nitride, is critical to ensuring device\nperformance and yield in semiconductor manufacturing. While conventional\nex-situ analysis methods are accurate, they are constrained by time delays and\ncontamination risks. To address these limitations, this study proposes a\nnon-contact, in-situ etch depth prediction framework based on machine learning\n(ML) techniques. Two scenarios are explored. In the first scenario, an\nartificial neural network (ANN) is trained to predict average etch depth from\nprocess parameters, achieving a significantly lower mean squared error (MSE)\ncompared to a linear baseline model. The approach is then extended to\nincorporate variability from repeated measurements using a Bayesian Neural\nNetwork (BNN) to capture both aleatoric and epistemic uncertainty. Coverage\nanalysis confirms the BNN's capability to provide reliable uncertainty\nestimates. In the second scenario, we demonstrate the feasibility of using RGB\ndata from digital image colorimetry (DIC) as input for etch depth prediction,\nachieving strong performance even in the absence of explicit process\nparameters. These results suggest that the integration of DIC and ML offers a\nviable, cost-effective alternative for real-time, in-situ, and non-invasive\nmonitoring in plasma etching processes, contributing to enhanced process\nstability, and manufacturing efficiency."}
{"id": "2505.03788", "pdf": "https://arxiv.org/pdf/2505.03788", "abs": "https://arxiv.org/abs/2505.03788", "authors": ["Trilok Padhi", "Ramneet Kaur", "Adam D. Cobb", "Manoj Acharya", "Anirban Roy", "Colin Samplawski", "Brian Matejek", "Alexander M. Berenbeim", "Nathaniel D. Bastian", "Susmit Jha"], "title": "Calibrating Uncertainty Quantification of Multi-Modal LLMs using Grounding", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "We introduce a novel approach for calibrating uncertainty quantification (UQ)\ntailored for multi-modal large language models (LLMs). Existing\nstate-of-the-art UQ methods rely on consistency among multiple responses\ngenerated by the LLM on an input query under diverse settings. However, these\napproaches often report higher confidence in scenarios where the LLM is\nconsistently incorrect. This leads to a poorly calibrated confidence with\nrespect to accuracy. To address this, we leverage cross-modal consistency in\naddition to self-consistency to improve the calibration of the multi-modal\nmodels. Specifically, we ground the textual responses to the visual inputs. The\nconfidence from the grounding model is used to calibrate the overall\nconfidence. Given that using a grounding model adds its own uncertainty in the\npipeline, we apply temperature scaling - a widely accepted parametric\ncalibration technique - to calibrate the grounding model's confidence in the\naccuracy of generated responses. We evaluate the proposed approach across\nmultiple multi-modal tasks, such as medical question answering (Slake) and\nvisual question answering (VQAv2), considering multi-modal models such as\nLLaVA-Med and LLaVA. The experiments demonstrate that the proposed framework\nachieves significantly improved calibration on both tasks."}
{"id": "2505.04203", "pdf": "https://arxiv.org/pdf/2505.04203", "abs": "https://arxiv.org/abs/2505.04203", "authors": ["Zhiping Qiu", "Yitong Jin", "Yuan Wang", "Yi Shi", "Chongwu Wang", "Chao Tan", "Xiaobing Li", "Feng Yu", "Tao Yu", "Qionghai Dai"], "title": "ELGAR: Expressive Cello Performance Motion Generation for Audio Rendition", "categories": ["cs.GR", "cs.SD", "eess.AS"], "comment": null, "summary": "The art of instrument performance stands as a vivid manifestation of human\ncreativity and emotion. Nonetheless, generating instrument performance motions\nis a highly challenging task, as it requires not only capturing intricate\nmovements but also reconstructing the complex dynamics of the\nperformer-instrument interaction. While existing works primarily focus on\nmodeling partial body motions, we propose Expressive ceLlo performance motion\nGeneration for Audio Rendition (ELGAR), a state-of-the-art diffusion-based\nframework for whole-body fine-grained instrument performance motion generation\nsolely from audio. To emphasize the interactive nature of the instrument\nperformance, we introduce Hand Interactive Contact Loss (HICL) and Bow\nInteractive Contact Loss (BICL), which effectively guarantee the authenticity\nof the interplay. Moreover, to better evaluate whether the generated motions\nalign with the semantic context of the music audio, we design novel metrics\nspecifically for string instrument performance motion generation, including\nfinger-contact distance, bow-string distance, and bowing score. Extensive\nevaluations and ablation studies are conducted to validate the efficacy of the\nproposed methods. In addition, we put forward a motion generation dataset\nSPD-GEN, collated and normalized from the MoCap dataset SPD. As demonstrated,\nELGAR has shown great potential in generating instrument performance motions\nwith complicated and fast interactions, which will promote further development\nin areas such as animation, music education, interactive art creation, etc."}
{"id": "2505.03829", "pdf": "https://arxiv.org/pdf/2505.03829", "abs": "https://arxiv.org/abs/2505.03829", "authors": ["Yogesh Kumar"], "title": "VideoLLM Benchmarks and Evaluation: A Survey", "categories": ["cs.CV", "cs.AI"], "comment": "12 pages, 2 Tables", "summary": "The rapid development of Large Language Models (LLMs) has catalyzed\nsignificant advancements in video understanding technologies. This survey\nprovides a comprehensive analysis of benchmarks and evaluation methodologies\nspecifically designed or used for Video Large Language Models (VideoLLMs). We\nexamine the current landscape of video understanding benchmarks, discussing\ntheir characteristics, evaluation protocols, and limitations. The paper\nanalyzes various evaluation methodologies, including closed-set, open-set, and\nspecialized evaluations for temporal and spatiotemporal understanding tasks. We\nhighlight the performance trends of state-of-the-art VideoLLMs across these\nbenchmarks and identify key challenges in current evaluation frameworks.\nAdditionally, we propose future research directions to enhance benchmark\ndesign, evaluation metrics, and protocols, including the need for more diverse,\nmultimodal, and interpretability-focused benchmarks. This survey aims to equip\nresearchers with a structured understanding of how to effectively evaluate\nVideoLLMs and identify promising avenues for advancing the field of video\nunderstanding with large language models."}
{"id": "2505.03910", "pdf": "https://arxiv.org/pdf/2505.03910", "abs": "https://arxiv.org/abs/2505.03910", "authors": ["Gianluca Manzo", "Julia Ive"], "title": "Hesitation is defeat? Connecting Linguistic and Predictive Uncertainty", "categories": ["cs.CL"], "comment": null, "summary": "Automating chest radiograph interpretation using Deep Learning (DL) models\nhas the potential to significantly improve clinical workflows, decision-making,\nand large-scale health screening. However, in medical settings, merely\noptimising predictive performance is insufficient, as the quantification of\nuncertainty is equally crucial. This paper investigates the relationship\nbetween predictive uncertainty, derived from Bayesian Deep Learning\napproximations, and human/linguistic uncertainty, as estimated from free-text\nradiology reports labelled by rule-based labellers. Utilising BERT as the model\nof choice, this study evaluates different binarisation methods for uncertainty\nlabels and explores the efficacy of Monte Carlo Dropout and Deep Ensembles in\nestimating predictive uncertainty. The results demonstrate good model\nperformance, but also a modest correlation between predictive and linguistic\nuncertainty, highlighting the challenges in aligning machine uncertainty with\nhuman interpretation nuances. Our findings suggest that while Bayesian\napproximations provide valuable uncertainty estimates, further refinement is\nnecessary to fully capture and utilise the subtleties of human uncertainty in\nclinical applications."}
{"id": "2505.04387", "pdf": "https://arxiv.org/pdf/2505.04387", "abs": "https://arxiv.org/abs/2505.04387", "authors": ["Amin Fadaeinejad", "Abdallah Dib", "Luiz Gustavo Hafemann", "Emeline Got", "Trevor Anderson", "Amaury Depierre", "Nikolaus F. Troje", "Marcus A. Brubaker", "Marc-André Carbonneau"], "title": "Geometry-Aware Texture Generation for 3D Head Modeling with Artist-driven Control", "categories": ["cs.GR", "cs.CV"], "comment": "11 pages, 9 figures, AI for Creative Visual Content Generation\n  Editing and Understanding (CVEU), CVPRW 2025", "summary": "Creating realistic 3D head assets for virtual characters that match a precise\nartistic vision remains labor-intensive. We present a novel framework that\nstreamlines this process by providing artists with intuitive control over\ngenerated 3D heads. Our approach uses a geometry-aware texture synthesis\npipeline that learns correlations between head geometry and skin texture maps\nacross different demographics. The framework offers three levels of artistic\ncontrol: manipulation of overall head geometry, adjustment of skin tone while\npreserving facial characteristics, and fine-grained editing of details such as\nwrinkles or facial hair. Our pipeline allows artists to make edits to a single\ntexture map using familiar tools, with our system automatically propagating\nthese changes coherently across the remaining texture maps needed for realistic\nrendering. Experiments demonstrate that our method produces diverse results\nwith clean geometries. We showcase practical applications focusing on intuitive\ncontrol for artists, including skin tone adjustments and simplified editing\nworkflows for adding age-related details or removing unwanted features from\nscanned models. This integrated approach aims to streamline the artistic\nworkflow in virtual character creation."}
{"id": "2505.03832", "pdf": "https://arxiv.org/pdf/2505.03832", "abs": "https://arxiv.org/abs/2505.03832", "authors": ["Noor B. Tayfor", "Tarik A. Rashid", "Shko M. Qader", "Bryar A. Hassan", "Mohammed H. Abdalla", "Jafar Majidpour", "Aram M. Ahmed", "Hussein M. Ali", "Aso M. Aladdin", "Abdulhady A. Abdullah", "Ahmed S. Shamsaldin", "Haval M. Sidqi", "Abdulrahman Salih", "Zaher M. Yaseen", "Azad A. Ameen", "Janmenjoy Nayak", "Mahmood Yashar Hamza"], "title": "Video Forgery Detection for Surveillance Cameras: A Review", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The widespread availability of video recording through smartphones and\ndigital devices has made video-based evidence more accessible than ever.\nSurveillance footage plays a crucial role in security, law enforcement, and\njudicial processes. However, with the rise of advanced video editing tools,\ntampering with digital recordings has become increasingly easy, raising\nconcerns about their authenticity. Ensuring the integrity of surveillance\nvideos is essential, as manipulated footage can lead to misinformation and\nundermine judicial decisions. This paper provides a comprehensive review of\nexisting forensic techniques used to detect video forgery, focusing on their\neffectiveness in verifying the authenticity of surveillance recordings. Various\nmethods, including compression-based analysis, frame duplication detection, and\nmachine learning-based approaches, are explored. The findings highlight the\ngrowing necessity for more robust forensic techniques to counteract evolving\nforgery methods. Strengthening video forensic capabilities will ensure that\nsurveillance recordings remain credible and admissible as legal evidence."}
{"id": "2505.03970", "pdf": "https://arxiv.org/pdf/2505.03970", "abs": "https://arxiv.org/abs/2505.03970", "authors": ["Lucia Zheng", "Neel Guha", "Javokhir Arifov", "Sarah Zhang", "Michal Skreta", "Christopher D. Manning", "Peter Henderson", "Daniel E. Ho"], "title": "A Reasoning-Focused Legal Retrieval Benchmark", "categories": ["cs.CL"], "comment": "CS&Law 2025. For data, see\n  https://reglab.github.io/legal-rag-benchmarks/", "summary": "As the legal community increasingly examines the use of large language models\n(LLMs) for various legal applications, legal AI developers have turned to\nretrieval-augmented LLMs (\"RAG\" systems) to improve system performance and\nrobustness. An obstacle to the development of specialized RAG systems is the\nlack of realistic legal RAG benchmarks which capture the complexity of both\nlegal retrieval and downstream legal question-answering. To address this, we\nintroduce two novel legal RAG benchmarks: Bar Exam QA and Housing Statute QA.\nOur tasks correspond to real-world legal research tasks, and were produced\nthrough annotation processes which resemble legal research. We describe the\nconstruction of these benchmarks and the performance of existing retriever\npipelines. Our results suggest that legal RAG remains a challenging\napplication, thus motivating future research."}
{"id": "2505.04590", "pdf": "https://arxiv.org/pdf/2505.04590", "abs": "https://arxiv.org/abs/2505.04590", "authors": ["Alexandre Binninger", "Ruben Wiersma", "Philipp Herholz", "Olga Sorkine-Hornung"], "title": "TetWeave: Isosurface Extraction using On-The-Fly Delaunay Tetrahedral Grids for Gradient-Based Mesh Optimization", "categories": ["cs.GR", "cs.CV", "I.3.5"], "comment": "ACM Trans. Graph. 44, 4. SIGGRAPH 2025. 19 pages, 21 figures", "summary": "We introduce TetWeave, a novel isosurface representation for gradient-based\nmesh optimization that jointly optimizes the placement of a tetrahedral grid\nused for Marching Tetrahedra and a novel directional signed distance at each\npoint. TetWeave constructs tetrahedral grids on-the-fly via Delaunay\ntriangulation, enabling increased flexibility compared to predefined grids. The\nextracted meshes are guaranteed to be watertight, two-manifold and\nintersection-free. The flexibility of TetWeave enables a resampling strategy\nthat places new points where reconstruction error is high and allows to\nencourage mesh fairness without compromising on reconstruction error. This\nleads to high-quality, adaptive meshes that require minimal memory usage and\nfew parameters to optimize. Consequently, TetWeave exhibits near-linear memory\nscaling relative to the vertex count of the output mesh - a substantial\nimprovement over predefined grids. We demonstrate the applicability of TetWeave\nto a broad range of challenging tasks in computer graphics and vision, such as\nmulti-view 3D reconstruction, mesh compression and geometric texture\ngeneration."}
{"id": "2505.03833", "pdf": "https://arxiv.org/pdf/2505.03833", "abs": "https://arxiv.org/abs/2505.03833", "authors": ["Xuechao Wang", "Sven Nomm", "Junqing Huang", "Kadri Medijainen", "Aaro Toomela", "Michael Ruzhansky"], "title": "PointExplainer: Towards Transparent Parkinson's Disease Diagnosis", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Deep neural networks have shown potential in analyzing digitized hand-drawn\nsignals for early diagnosis of Parkinson's disease. However, the lack of clear\ninterpretability in existing diagnostic methods presents a challenge to\nclinical trust. In this paper, we propose PointExplainer, an explainable\ndiagnostic strategy to identify hand-drawn regions that drive model diagnosis.\nSpecifically, PointExplainer assigns discrete attribution values to hand-drawn\nsegments, explicitly quantifying their relative contributions to the model's\ndecision. Its key components include: (i) a diagnosis module, which encodes\nhand-drawn signals into 3D point clouds to represent hand-drawn trajectories,\nand (ii) an explanation module, which trains an interpretable surrogate model\nto approximate the local behavior of the black-box diagnostic model. We also\nintroduce consistency measures to further address the issue of faithfulness in\nexplanations. Extensive experiments on two benchmark datasets and a newly\nconstructed dataset show that PointExplainer can provide intuitive explanations\nwith no diagnostic performance degradation. The source code is available at\nhttps://github.com/chaoxuewang/PointExplainer."}
{"id": "2505.03973", "pdf": "https://arxiv.org/pdf/2505.03973", "abs": "https://arxiv.org/abs/2505.03973", "authors": ["Jiale Liu", "Yifan Zeng", "Shaokun Zhang", "Chi Zhang", "Malte Højmark-Bertelsen", "Marie Normann Gadeberg", "Huazheng Wang", "Qingyun Wu"], "title": "Divide, Optimize, Merge: Fine-Grained LLM Agent Optimization at Scale", "categories": ["cs.CL"], "comment": null, "summary": "LLM-based optimization has shown remarkable potential in enhancing agentic\nsystems. However, the conventional approach of prompting LLM optimizer with the\nwhole training trajectories on training dataset in a single pass becomes\nuntenable as datasets grow, leading to context window overflow and degraded\npattern recognition. To address these challenges, we propose Fine-Grained\nOptimization (FGO), a scalable framework that divides large optimization tasks\ninto manageable subsets, performs targeted optimizations, and systematically\ncombines optimized components through progressive merging. Evaluation across\nALFWorld, LogisticsQA, and GAIA benchmarks demonstrate that FGO outperforms\nexisting approaches by 1.6-8.6% while reducing average prompt token consumption\nby 56.3%. Our framework provides a practical solution for scaling up LLM-based\noptimization of increasingly sophisticated agent systems. Further analysis\ndemonstrates that FGO achieves the most consistent performance gain in all\ntraining dataset sizes, showcasing its scalability and efficiency."}
{"id": "2505.04622", "pdf": "https://arxiv.org/pdf/2505.04622", "abs": "https://arxiv.org/abs/2505.04622", "authors": ["Jingwen Ye", "Yuze He", "Yanning Zhou", "Yiqin Zhu", "Kaiwen Xiao", "Yong-Jin Liu", "Wei Yang", "Xiao Han"], "title": "PrimitiveAnything: Human-Crafted 3D Primitive Assembly Generation with Auto-Regressive Transformer", "categories": ["cs.GR", "cs.CV"], "comment": "SIGGRAPH 2025. 14 pages, 15 figures", "summary": "Shape primitive abstraction, which decomposes complex 3D shapes into simple\ngeometric elements, plays a crucial role in human visual cognition and has\nbroad applications in computer vision and graphics. While recent advances in 3D\ncontent generation have shown remarkable progress, existing primitive\nabstraction methods either rely on geometric optimization with limited semantic\nunderstanding or learn from small-scale, category-specific datasets, struggling\nto generalize across diverse shape categories. We present PrimitiveAnything, a\nnovel framework that reformulates shape primitive abstraction as a primitive\nassembly generation task. PrimitiveAnything includes a shape-conditioned\nprimitive transformer for auto-regressive generation and an ambiguity-free\nparameterization scheme to represent multiple types of primitives in a unified\nmanner. The proposed framework directly learns the process of primitive\nassembly from large-scale human-crafted abstractions, enabling it to capture\nhow humans decompose complex shapes into primitive elements. Through extensive\nexperiments, we demonstrate that PrimitiveAnything can generate high-quality\nprimitive assemblies that better align with human perception while maintaining\ngeometric fidelity across diverse shape categories. It benefits various 3D\napplications and shows potential for enabling primitive-based user-generated\ncontent (UGC) in games. Project page: https://primitiveanything.github.io"}
{"id": "2505.03837", "pdf": "https://arxiv.org/pdf/2505.03837", "abs": "https://arxiv.org/abs/2505.03837", "authors": ["Rashik Shadman", "Daqing Hou", "Faraz Hussain", "M G Sarwar Murshed"], "title": "Explainable Face Recognition via Improved Localization", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Biometric authentication has become one of the most widely used tools in the\ncurrent technological era to authenticate users and to distinguish between\ngenuine users and imposters. Face is the most common form of biometric modality\nthat has proven effective. Deep learning-based face recognition systems are now\ncommonly used across different domains. However, these systems usually operate\nlike black-box models that do not provide necessary explanations or\njustifications for their decisions. This is a major disadvantage because users\ncannot trust such artificial intelligence-based biometric systems and may not\nfeel comfortable using them when clear explanations or justifications are not\nprovided. This paper addresses this problem by applying an efficient method for\nexplainable face recognition systems. We use a Class Activation Mapping\n(CAM)-based discriminative localization (very narrow/specific localization)\ntechnique called Scaled Directed Divergence (SDD) to visually explain the\nresults of deep learning-based face recognition systems. We perform fine\nlocalization of the face features relevant to the deep learning model for its\nprediction/decision. Our experiments show that the SDD Class Activation Map\n(CAM) highlights the relevant face features very specifically compared to the\ntraditional CAM and very accurately. The provided visual explanations with\nnarrow localization of relevant features can ensure much-needed transparency\nand trust for deep learning-based face recognition systems."}
{"id": "2505.03981", "pdf": "https://arxiv.org/pdf/2505.03981", "abs": "https://arxiv.org/abs/2505.03981", "authors": ["Qianchu Liu", "Sheng Zhang", "Guanghui Qin", "Timothy Ossowski", "Yu Gu", "Ying Jin", "Sid Kiblawi", "Sam Preston", "Mu Wei", "Paul Vozila", "Tristan Naumann", "Hoifung Poon"], "title": "X-Reasoner: Towards Generalizable Reasoning Across Modalities and Domains", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent proprietary models (e.g., o3) have begun to demonstrate strong\nmultimodal reasoning capabilities. Yet, most existing open-source research\nconcentrates on training text-only reasoning models, with evaluations limited\nto mainly mathematical and general-domain tasks. Therefore, it remains unclear\nhow to effectively extend reasoning capabilities beyond text input and general\ndomains. This paper explores a fundamental research question: Is reasoning\ngeneralizable across modalities and domains? Our findings support an\naffirmative answer: General-domain text-based post-training can enable such\nstrong generalizable reasoning. Leveraging this finding, we introduce\nX-Reasoner, a vision-language model post-trained solely on general-domain text\nfor generalizable reasoning, using a two-stage approach: an initial supervised\nfine-tuning phase with distilled long chain-of-thoughts, followed by\nreinforcement learning with verifiable rewards. Experiments show that\nX-Reasoner successfully transfers reasoning capabilities to both multimodal and\nout-of-domain settings, outperforming existing state-of-the-art models trained\nwith in-domain and multimodal data across various general and medical\nbenchmarks (Figure 1). Additionally, we find that X-Reasoner's performance in\nspecialized domains can be further enhanced through continued training on\ndomain-specific text-only data. Building upon this, we introduce\nX-Reasoner-Med, a medical-specialized variant that achieves new state of the\nart on numerous text-only and multimodal medical benchmarks."}
{"id": "2505.03846", "pdf": "https://arxiv.org/pdf/2505.03846", "abs": "https://arxiv.org/abs/2505.03846", "authors": ["Kangsheng Wang", "Yuhang Li", "Chengwei Ye", "Yufei Lin", "Huanzhen Zhang", "Bohan Hu", "Linuo Xu", "Shuyan Liu"], "title": "GAME: Learning Multimodal Interactions via Graph Structures for Personality Trait Estimation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Apparent personality analysis from short videos poses significant chal-lenges\ndue to the complex interplay of visual, auditory, and textual cues. In this\npaper, we propose GAME, a Graph-Augmented Multimodal Encoder designed to\nrobustly model and fuse multi-source features for automatic personality\nprediction. For the visual stream, we construct a facial graph and introduce a\ndual-branch Geo Two-Stream Network, which combines Graph Convolutional Networks\n(GCNs) and Convolutional Neural Net-works (CNNs) with attention mechanisms to\ncapture both structural and appearance-based facial cues. Complementing this,\nglobal context and iden-tity features are extracted using pretrained ResNet18\nand VGGFace back-bones. To capture temporal dynamics, frame-level features are\nprocessed by a BiGRU enhanced with temporal attention modules. Meanwhile, audio\nrepresentations are derived from the VGGish network, and linguistic se-mantics\nare captured via the XLM-Roberta transformer. To achieve effective multimodal\nintegration, we propose a Channel Attention-based Fusion module, followed by a\nMulti-Layer Perceptron (MLP) regression head for predicting personality traits.\nExtensive experiments show that GAME con-sistently outperforms existing methods\nacross multiple benchmarks, vali-dating its effectiveness and generalizability."}
{"id": "2505.04016", "pdf": "https://arxiv.org/pdf/2505.04016", "abs": "https://arxiv.org/abs/2505.04016", "authors": ["Darren Yow-Bang Wang", "Zhengyuan Shen", "Soumya Smruti Mishra", "Zhichao Xu", "Yifei Teng", "Haibo Ding"], "title": "SLOT: Structuring the Output of Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Structured outputs are essential for large language models (LLMs) in critical\napplications like agents and information extraction. Despite their\ncapabilities, LLMs often generate outputs that deviate from predefined schemas,\nsignificantly hampering reliable application development. We present SLOT\n(Structured LLM Output Transformer), a model-agnostic approach that transforms\nunstructured LLM outputs into precise structured formats. While existing\nsolutions predominantly rely on constrained decoding techniques or are tightly\ncoupled with specific models, SLOT employs a fine-tuned lightweight language\nmodel as a post-processing layer, achieving flexibility across various LLMs and\nschema specifications. We introduce a systematic pipeline for data curation and\nsynthesis alongside a formal evaluation methodology that quantifies both schema\naccuracy and content fidelity. Our results demonstrate that fine-tuned\nMistral-7B model with constrained decoding achieves near perfect schema\naccuracy (99.5%) and content similarity (94.0%), outperforming\nClaude-3.5-Sonnet by substantial margins (+25 and +20 percentage points,\nrespectively). Notably, even compact models like Llama-3.2-1B can match or\nexceed the structured output capabilities of much larger proprietary models\nwhen equipped with SLOT, enabling reliable structured generation in\nresource-constrained environments."}
{"id": "2505.03848", "pdf": "https://arxiv.org/pdf/2505.03848", "abs": "https://arxiv.org/abs/2505.03848", "authors": ["Janhavi Giri", "Attila Lengyel", "Don Kent", "Edward Kibardin"], "title": "Advanced Clustering Framework for Semiconductor Image Analytics Integrating Deep TDA with Self-Supervised and Transfer Learning Techniques", "categories": ["cs.CV", "cs.AI", "cs.ET", "cs.LG"], "comment": "46 pages, 22 figures, 5 tables", "summary": "Semiconductor manufacturing generates vast amounts of image data, crucial for\ndefect identification and yield optimization, yet often exceeds manual\ninspection capabilities. Traditional clustering techniques struggle with\nhigh-dimensional, unlabeled data, limiting their effectiveness in capturing\nnuanced patterns. This paper introduces an advanced clustering framework that\nintegrates deep Topological Data Analysis (TDA) with self-supervised and\ntransfer learning techniques, offering a novel approach to unsupervised image\nclustering. TDA captures intrinsic topological features, while self-supervised\nlearning extracts meaningful representations from unlabeled data, reducing\nreliance on labeled datasets. Transfer learning enhances the framework's\nadaptability and scalability, allowing fine-tuning to new datasets without\nretraining from scratch. Validated on synthetic and open-source semiconductor\nimage datasets, the framework successfully identifies clusters aligned with\ndefect patterns and process variations. This study highlights the\ntransformative potential of combining TDA, self-supervised learning, and\ntransfer learning, providing a scalable solution for proactive process\nmonitoring and quality control in semiconductor manufacturing and other domains\nwith large-scale image datasets."}
{"id": "2505.04072", "pdf": "https://arxiv.org/pdf/2505.04072", "abs": "https://arxiv.org/abs/2505.04072", "authors": ["Xu Huang", "Yuefeng Huang", "Weiwen Liu", "Xingshan Zeng", "Yasheng Wang", "Ruiming Tang", "Hong Xie", "Defu Lian"], "title": "Advancing and Benchmarking Personalized Tool Invocation for LLMs", "categories": ["cs.CL", "cs.AI"], "comment": "14 pages, 7 figures, 5 tables", "summary": "Tool invocation is a crucial mechanism for extending the capabilities of\nLarge Language Models (LLMs) and has recently garnered significant attention.\nIt enables LLMs to solve complex problems through tool calls while accessing\nup-to-date world knowledge. However, existing work primarily focuses on the\nfundamental ability of LLMs to invoke tools for problem-solving, without\nconsidering personalized constraints in tool invocation. In this work, we\nintroduce the concept of Personalized Tool Invocation and define two key tasks:\nTool Preference and Profile-dependent Query. Tool Preference addresses user\npreferences when selecting among functionally similar tools, while\nProfile-dependent Query considers cases where a user query lacks certain tool\nparameters, requiring the model to infer them from the user profile. To tackle\nthese challenges, we propose PTool, a data synthesis framework designed for\npersonalized tool invocation. Additionally, we construct \\textbf{PTBench}, the\nfirst benchmark for evaluating personalized tool invocation. We then fine-tune\nvarious open-source models, demonstrating the effectiveness of our framework\nand providing valuable insights. Our benchmark is public at\nhttps://github.com/hyfshadow/PTBench."}
{"id": "2505.03856", "pdf": "https://arxiv.org/pdf/2505.03856", "abs": "https://arxiv.org/abs/2505.03856", "authors": ["Tin Mišić", "Karlo Koledić", "Fabio Bonsignorio", "Ivan Petrović", "Ivan Marković"], "title": "An Active Inference Model of Covert and Overt Visual Attention", "categories": ["cs.CV", "cs.AI", "I.2.6; I.2.10"], "comment": "7 pages, 7 figures. Code available at\n  https://github.com/unizgfer-lamor/ainf-visual-attention", "summary": "The ability to selectively attend to relevant stimuli while filtering out\ndistractions is essential for agents that process complex, high-dimensional\nsensory input. This paper introduces a model of covert and overt visual\nattention through the framework of active inference, utilizing dynamic\noptimization of sensory precisions to minimize free-energy. The model\ndetermines visual sensory precisions based on both current environmental\nbeliefs and sensory input, influencing attentional allocation in both covert\nand overt modalities. To test the effectiveness of the model, we analyze its\nbehavior in the Posner cueing task and a simple target focus task using\ntwo-dimensional(2D) visual data. Reaction times are measured to investigate the\ninterplay between exogenous and endogenous attention, as well as valid and\ninvalid cueing. The results show that exogenous and valid cues generally lead\nto faster reaction times compared to endogenous and invalid cues. Furthermore,\nthe model exhibits behavior similar to inhibition of return, where previously\nattended locations become suppressed after a specific cue-target onset\nasynchrony interval. Lastly, we investigate different aspects of overt\nattention and show that involuntary, reflexive saccades occur faster than\nintentional ones, but at the expense of adaptability."}
{"id": "2505.04073", "pdf": "https://arxiv.org/pdf/2505.04073", "abs": "https://arxiv.org/abs/2505.04073", "authors": ["Mengxian Lyu", "Xiaohan Li", "Ziyi Chen", "Jinqian Pan", "Cheng Peng", "Sankalp Talankar", "Yonghui Wu"], "title": "Natural Language Generation in Healthcare: A Review of Methods and Applications", "categories": ["cs.CL"], "comment": null, "summary": "Natural language generation (NLG) is the key technology to achieve generative\nartificial intelligence (AI). With the breakthroughs in large language models\n(LLMs), NLG has been widely used in various medical applications, demonstrating\nthe potential to enhance clinical workflows, support clinical decision-making,\nand improve clinical documentation. Heterogeneous and diverse medical data\nmodalities, such as medical text, images, and knowledge bases, are utilized in\nNLG. Researchers have proposed many generative models and applied them in a\nnumber of healthcare applications. There is a need for a comprehensive review\nof NLG methods and applications in the medical domain. In this study, we\nsystematically reviewed 113 scientific publications from a total of 3,988\nNLG-related articles identified using a literature search, focusing on data\nmodality, model architecture, clinical applications, and evaluation methods.\nFollowing PRISMA (Preferred Reporting Items for Systematic reviews and\nMeta-Analyses) guidelines, we categorize key methods, identify clinical\napplications, and assess their capabilities, limitations, and emerging\nchallenges. This timely review covers the key NLG technologies and medical\napplications and provides valuable insights for future studies to leverage NLG\nto transform medical discovery and healthcare."}
{"id": "2505.03896", "pdf": "https://arxiv.org/pdf/2505.03896", "abs": "https://arxiv.org/abs/2505.03896", "authors": ["Shuang Zeng", "Chee Hong Lee", "Micky C Nnamdi", "Wenqi Shi", "J Ben Tamo", "Lei Zhu", "Hangzhou He", "Xinliang Zhang", "Qian Chen", "May D. Wang", "Yanye Lu", "Qiushi Ren"], "title": "Novel Extraction of Discriminative Fine-Grained Feature to Improve Retinal Vessel Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Retinal vessel segmentation is a vital early detection method for several\nsevere ocular diseases. Despite significant progress in retinal vessel\nsegmentation with the advancement of Neural Networks, there are still\nchallenges to overcome. Specifically, retinal vessel segmentation aims to\npredict the class label for every pixel within a fundus image, with a primary\nfocus on intra-image discrimination, making it vital for models to extract more\ndiscriminative features. Nevertheless, existing methods primarily focus on\nminimizing the difference between the output from the decoder and the label,\nbut ignore fully using feature-level fine-grained representations from the\nencoder. To address these issues, we propose a novel Attention U-shaped\nKolmogorov-Arnold Network named AttUKAN along with a novel Label-guided\nPixel-wise Contrastive Loss for retinal vessel segmentation. Specifically, we\nimplement Attention Gates into Kolmogorov-Arnold Networks to enhance model\nsensitivity by suppressing irrelevant feature activations and model\ninterpretability by non-linear modeling of KAN blocks. Additionally, we also\ndesign a novel Label-guided Pixel-wise Contrastive Loss to supervise our\nproposed AttUKAN to extract more discriminative features by distinguishing\nbetween foreground vessel-pixel pairs and background pairs. Experiments are\nconducted across four public datasets including DRIVE, STARE, CHASE_DB1, HRF\nand our private dataset. AttUKAN achieves F1 scores of 82.50%, 81.14%, 81.34%,\n80.21% and 80.09%, along with MIoU scores of 70.24%, 68.64%, 68.59%, 67.21% and\n66.94% in the above datasets, which are the highest compared to 11 networks for\nretinal vessel segmentation. Quantitative and qualitative results show that our\nAttUKAN achieves state-of-the-art performance and outperforms existing retinal\nvessel segmentation methods. Our code will be available at\nhttps://github.com/stevezs315/AttUKAN."}
{"id": "2505.04132", "pdf": "https://arxiv.org/pdf/2505.04132", "abs": "https://arxiv.org/abs/2505.04132", "authors": ["Mingruo Yuan", "Ben Kao", "Tien-Hsuan Wu", "Michael M. K. Cheung", "Henry W. H. Chan", "Anne S. Y. Cheung", "Felix W. H. Chan", "Yongxi Chen"], "title": "Bringing legal knowledge to the public by constructing a legal question bank using large-scale pre-trained language model", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Access to legal information is fundamental to access to justice. Yet\naccessibility refers not only to making legal documents available to the\npublic, but also rendering legal information comprehensible to them. A vexing\nproblem in bringing legal information to the public is how to turn formal legal\ndocuments such as legislation and judgments, which are often highly technical,\nto easily navigable and comprehensible knowledge to those without legal\neducation. In this study, we formulate a three-step approach for bringing legal\nknowledge to laypersons, tackling the issues of navigability and\ncomprehensibility. First, we translate selected sections of the law into\nsnippets (called CLIC-pages), each being a small piece of article that focuses\non explaining certain technical legal concept in layperson's terms. Second, we\nconstruct a Legal Question Bank (LQB), which is a collection of legal questions\nwhose answers can be found in the CLIC-pages. Third, we design an interactive\nCLIC Recommender (CRec). Given a user's verbal description of a legal situation\nthat requires a legal solution, CRec interprets the user's input and shortlists\nquestions from the question bank that are most likely relevant to the given\nlegal situation and recommends their corresponding CLIC pages where relevant\nlegal knowledge can be found. In this paper we focus on the technical aspects\nof creating an LQB. We show how large-scale pre-trained language models, such\nas GPT-3, can be used to generate legal questions. We compare machine-generated\nquestions (MGQs) against human-composed questions (HCQs) and find that MGQs are\nmore scalable, cost-effective, and more diversified, while HCQs are more\nprecise. We also show a prototype of CRec and illustrate through an example how\nour 3-step approach effectively brings relevant legal knowledge to the public."}
{"id": "2505.03974", "pdf": "https://arxiv.org/pdf/2505.03974", "abs": "https://arxiv.org/abs/2505.03974", "authors": ["Nikhil M. Pawar", "Jorge A. Prozzi", "Feng Hong", "Surya Sarat Chandra Congress"], "title": "Deep Learning Framework for Infrastructure Maintenance: Crack Detection and High-Resolution Imaging of Infrastructure Surfaces", "categories": ["cs.CV", "cs.AI"], "comment": "Presented :Transportation Research Board 104th Annual Meeting,\n  Washington, D.C", "summary": "Recently, there has been an impetus for the application of cutting-edge data\ncollection platforms such as drones mounted with camera sensors for\ninfrastructure asset management. However, the sensor characteristics, proximity\nto the structure, hard-to-reach access, and environmental conditions often\nlimit the resolution of the datasets. A few studies used super-resolution\ntechniques to address the problem of low-resolution images. Nevertheless, these\ntechniques were observed to increase computational cost and false alarms of\ndistress detection due to the consideration of all the infrastructure images\ni.e., positive and negative distress classes. In order to address the\npre-processing of false alarm and achieve efficient super-resolution, this\nstudy developed a framework consisting of convolutional neural network (CNN)\nand efficient sub-pixel convolutional neural network (ESPCNN). CNN accurately\nclassified both the classes. ESPCNN, which is the lightweight super-resolution\ntechnique, generated high-resolution infrastructure image of positive distress\nobtained from CNN. The ESPCNN outperformed bicubic interpolation in all the\nevaluation metrics for super-resolution. Based on the performance metrics, the\ncombination of CNN and ESPCNN was observed to be effective in preprocessing the\ninfrastructure images with negative distress, reducing the computational cost\nand false alarms in the next step of super-resolution. The visual inspection\nshowed that EPSCNN is able to capture crack propagation, complex geometry of\neven minor cracks. The proposed framework is expected to help the highway\nagencies in accurately performing distress detection and assist in efficient\nasset management practices."}
{"id": "2505.04135", "pdf": "https://arxiv.org/pdf/2505.04135", "abs": "https://arxiv.org/abs/2505.04135", "authors": ["Vihaan Miriyala", "Smrithi Bukkapatnam", "Lavanya Prahallad"], "title": "Enhancing Granular Sentiment Classification with Chain-of-Thought Prompting in Large Language Models", "categories": ["cs.CL", "cs.LG"], "comment": "5 pages", "summary": "We explore the use of Chain-of-Thought (CoT) prompting with large language\nmodels (LLMs) to improve the accuracy of granular sentiment categorization in\napp store reviews. Traditional numeric and polarity-based ratings often fail to\ncapture the nuanced sentiment embedded in user feedback. We evaluated the\neffectiveness of CoT prompting versus simple prompting on 2000 Amazon app\nreviews by comparing each method's predictions to human judgements. CoT\nprompting improved classification accuracy from 84% to 93% highlighting the\nbenefit of explicit reasoning in enhancing sentiment analysis performance."}
{"id": "2505.03991", "pdf": "https://arxiv.org/pdf/2505.03991", "abs": "https://arxiv.org/abs/2505.03991", "authors": ["Hao Xu", "Arbind Agrahari Baniya", "Sam Well", "Mohamed Reda Bouadjenek", "Richard Dazeley", "Sunil Aryal"], "title": "Action Spotting and Precise Event Detection in Sports: Datasets, Methods, and Challenges", "categories": ["cs.CV"], "comment": "13 pages, 4 figures, 2 tables", "summary": "Video event detection has become an essential component of sports analytics,\nenabling automated identification of key moments and enhancing performance\nanalysis, viewer engagement, and broadcast efficiency. Recent advancements in\ndeep learning, particularly Convolutional Neural Networks (CNNs) and\nTransformers, have significantly improved accuracy and efficiency in Temporal\nAction Localization (TAL), Action Spotting (AS), and Precise Event Spotting\n(PES). This survey provides a comprehensive overview of these three key tasks,\nemphasizing their differences, applications, and the evolution of\nmethodological approaches. We thoroughly review and categorize existing\ndatasets and evaluation metrics specifically tailored for sports contexts,\nhighlighting the strengths and limitations of each. Furthermore, we analyze\nstate-of-the-art techniques, including multi-modal approaches that integrate\naudio and visual information, methods utilizing self-supervised learning and\nknowledge distillation, and approaches aimed at generalizing across multiple\nsports. Finally, we discuss critical open challenges and outline promising\nresearch directions toward developing more generalized, efficient, and robust\nevent detection frameworks applicable to diverse sports. This survey serves as\na foundation for future research on efficient, generalizable, and multi-modal\nsports event detection."}
{"id": "2505.04146", "pdf": "https://arxiv.org/pdf/2505.04146", "abs": "https://arxiv.org/abs/2505.04146", "authors": ["Variath Madhupal Gautham Nair", "Vishal Varma Dantuluri"], "title": "Unmasking the Canvas: A Dynamic Benchmark for Image Generation Jailbreaking and LLM Content Safety", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "Existing large language models (LLMs) are advancing rapidly and produce\noutstanding results in image generation tasks, yet their content safety checks\nremain vulnerable to prompt-based jailbreaks. Through preliminary testing on\nplatforms such as ChatGPT, MetaAI, and Grok, we observed that even short,\nnatural prompts could lead to the generation of compromising images ranging\nfrom realistic depictions of forged documents to manipulated images of public\nfigures.\n  We introduce Unmasking the Canvas (UTC Benchmark; UTCB), a dynamic and\nscalable benchmark dataset to evaluate LLM vulnerability in image generation.\nOur methodology combines structured prompt engineering, multilingual\nobfuscation (e.g., Zulu, Gaelic, Base64), and evaluation using Groq-hosted\nLLaMA-3. The pipeline supports both zero-shot and fallback prompting\nstrategies, risk scoring, and automated tagging. All generations are stored\nwith rich metadata and curated into Bronze (non-verified), Silver (LLM-aided\nverification), and Gold (manually verified) tiers. UTCB is designed to evolve\nover time with new data sources, prompt templates, and model behaviors.\n  Warning: This paper includes visual examples of adversarial inputs designed\nto test model safety. All outputs have been redacted to ensure responsible\ndisclosure."}
{"id": "2505.04006", "pdf": "https://arxiv.org/pdf/2505.04006", "abs": "https://arxiv.org/abs/2505.04006", "authors": ["Inamullah", "Imran Razzak", "Shoaib Jameel"], "title": "The Eye as a Window to Systemic Health: A Survey of Retinal Imaging from Classical Techniques to Oculomics", "categories": ["cs.CV"], "comment": null, "summary": "The unique vascularized anatomy of the human eye, encased in the retina,\nprovides an opportunity to act as a window for human health. The retinal\nstructure assists in assessing the early detection, monitoring of disease\nprogression and intervention for both ocular and non-ocular diseases. The\nadvancement in imaging technology leveraging Artificial Intelligence has seized\nthis opportunity to bridge the gap between the eye and human health. This track\npaves the way for unveiling systemic health insight from the ocular system and\nsurrogating non-invasive markers for timely intervention and identification.\nThe new frontiers of oculomics in ophthalmology cover both ocular and systemic\ndiseases, and getting more attention to explore them. In this survey paper, we\nexplore the evolution of retinal imaging techniques, the dire need for the\nintegration of AI-driven analysis, and the shift of retinal imaging from\nclassical techniques to oculomics. We also discuss some hurdles that may be\nfaced in the progression of oculomics, highlighting the research gaps and\nfuture directions."}
{"id": "2505.04152", "pdf": "https://arxiv.org/pdf/2505.04152", "abs": "https://arxiv.org/abs/2505.04152", "authors": ["Manas Satish Bedmutha", "Feng Chen", "Andrea Hartzler", "Trevor Cohen", "Nadir Weibel"], "title": "Can Language Models Understand Social Behavior in Clinical Conversations?", "categories": ["cs.CL", "cs.CY", "cs.HC", "H.5.2; H.1.2; I.2.7; I.2.m; J.3"], "comment": null, "summary": "Effective communication between providers and their patients influences\nhealth and care outcomes. The effectiveness of such conversations has been\nlinked not only to the exchange of clinical information, but also to a range of\ninterpersonal behaviors; commonly referred to as social signals, which are\noften conveyed through non-verbal cues and shape the quality of the\npatient-provider relationship. Recent advances in large language models (LLMs)\nhave demonstrated an increasing ability to infer emotional and social behaviors\neven when analyzing only textual information. As automation increases also in\nclinical settings, such as for transcription of patient-provider conversations,\nthere is growing potential for LLMs to automatically analyze and extract social\nbehaviors from these interactions. To explore the foundational capabilities of\nLLMs in tracking social signals in clinical dialogue, we designed task-specific\nprompts and evaluated model performance across multiple architectures and\nprompting styles using a highly imbalanced, annotated dataset spanning 20\ndistinct social signals such as provider dominance, patient warmth, etc. We\npresent the first system capable of tracking all these 20 coded signals, and\nuncover patterns in LLM behavior. Further analysis of model configurations and\nclinical context provides insights for enhancing LLM performance on social\nsignal processing tasks in healthcare settings."}
{"id": "2505.04055", "pdf": "https://arxiv.org/pdf/2505.04055", "abs": "https://arxiv.org/abs/2505.04055", "authors": ["Ervin Wang", "Yuhao Chen"], "title": "FoodTrack: Estimating Handheld Food Portions with Egocentric Video", "categories": ["cs.CV"], "comment": "Accepted as extended abstract at CVPR 2025 Metafood workshop", "summary": "Accurately tracking food consumption is crucial for nutrition and health\nmonitoring. Traditional approaches typically require specific camera angles,\nnon-occluded images, or rely on gesture recognition to estimate intake, making\nassumptions about bite size rather than directly measuring food volume. We\npropose the FoodTrack framework for tracking and measuring the volume of\nhand-held food items using egocentric video which is robust to hand occlusions\nand flexible with varying camera and object poses. FoodTrack estimates food\nvolume directly, without relying on intake gestures or fixed assumptions about\nbite size, offering a more accurate and adaptable solution for tracking food\nconsumption. We achieve absolute percentage loss of approximately 7.01% on a\nhandheld food object, improving upon a previous approach that achieved a 16.40%\nmean absolute percentage error in its best case, under less flexible\nconditions."}
{"id": "2505.04253", "pdf": "https://arxiv.org/pdf/2505.04253", "abs": "https://arxiv.org/abs/2505.04253", "authors": ["Maria Marina", "Nikolay Ivanov", "Sergey Pletenev", "Mikhail Salnikov", "Daria Galimzianova", "Nikita Krayko", "Vasily Konovalov", "Alexander Panchenko", "Viktor Moskvoretskii"], "title": "LLM-Independent Adaptive RAG: Let the Question Speak for Itself", "categories": ["cs.CL", "cs.LG"], "comment": "11 pages, 5 figures, 2 tables", "summary": "Large Language Models~(LLMs) are prone to hallucinations, and\nRetrieval-Augmented Generation (RAG) helps mitigate this, but at a high\ncomputational cost while risking misinformation. Adaptive retrieval aims to\nretrieve only when necessary, but existing approaches rely on LLM-based\nuncertainty estimation, which remain inefficient and impractical. In this\nstudy, we introduce lightweight LLM-independent adaptive retrieval methods\nbased on external information. We investigated 27 features, organized into 7\ngroups, and their hybrid combinations. We evaluated these methods on 6 QA\ndatasets, assessing the QA performance and efficiency. The results show that\nour approach matches the performance of complex LLM-based methods while\nachieving significant efficiency gains, demonstrating the potential of external\ninformation for adaptive retrieval."}
{"id": "2505.04058", "pdf": "https://arxiv.org/pdf/2505.04058", "abs": "https://arxiv.org/abs/2505.04058", "authors": ["Feng Xiao", "Hongbin Xu", "Guocan Zhao", "Wenxiong Kang"], "title": "AS3D: 2D-Assisted Cross-Modal Understanding with Semantic-Spatial Scene Graphs for 3D Visual Grounding", "categories": ["cs.CV"], "comment": null, "summary": "3D visual grounding aims to localize the unique target described by natural\nlanguages in 3D scenes. The significant gap between 3D and language modalities\nmakes it a notable challenge to distinguish multiple similar objects through\nthe described spatial relationships. Current methods attempt to achieve\ncross-modal understanding in complex scenes via a target-centered learning\nmechanism, ignoring the perception of referred objects. We propose a novel\n2D-assisted 3D visual grounding framework that constructs semantic-spatial\nscene graphs with referred object discrimination for relationship perception.\nThe framework incorporates a dual-branch visual encoder that utilizes 2D\npre-trained attributes to guide the multi-modal object encoding. Furthermore,\nour cross-modal interaction module uses graph attention to facilitate\nrelationship-oriented information fusion. The enhanced object representation\nand iterative relational learning enable the model to establish effective\nalignment between 3D vision and referential descriptions. Experimental results\non the popular benchmarks demonstrate our superior performance compared to\nstate-of-the-art methods, especially in addressing the challenges of multiple\nsimilar distractors."}
{"id": "2505.04284", "pdf": "https://arxiv.org/pdf/2505.04284", "abs": "https://arxiv.org/abs/2505.04284", "authors": ["Sofia Jamil", "Aryan Dabad", "Bollampalli Areen Reddy", "Sriparna Saha", "Rajiv Misra", "Adil A. Shakur"], "title": "GASCADE: Grouped Summarization of Adverse Drug Event for Enhanced Cancer Pharmacovigilance", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In the realm of cancer treatment, summarizing adverse drug events (ADEs)\nreported by patients using prescribed drugs is crucial for enhancing\npharmacovigilance practices and improving drug-related decision-making. While\nthe volume and complexity of pharmacovigilance data have increased, existing\nresearch in this field has predominantly focused on general diseases rather\nthan specifically addressing cancer. This work introduces the task of grouped\nsummarization of adverse drug events reported by multiple patients using the\nsame drug for cancer treatment. To address the challenge of limited resources\nin cancer pharmacovigilance, we present the MultiLabeled Cancer Adverse Drug\nReaction and Summarization (MCADRS) dataset. This dataset includes\npharmacovigilance posts detailing patient concerns regarding drug efficacy and\nadverse effects, along with extracted labels for drug names, adverse drug\nevents, severity, and adversity of reactions, as well as summaries of ADEs for\neach drug. Additionally, we propose the Grouping and Abstractive Summarization\nof Cancer Adverse Drug events (GASCADE) framework, a novel pipeline that\ncombines the information extraction capabilities of Large Language Models\n(LLMs) with the summarization power of the encoder-decoder T5 model. Our work\nis the first to apply alignment techniques, including advanced algorithms like\nDirect Preference Optimization, to encoder-decoder models using synthetic\ndatasets for summarization tasks. Through extensive experiments, we demonstrate\nthe superior performance of GASCADE across various metrics, validated through\nboth automated assessments and human evaluations. This multitasking approach\nenhances drug-related decision-making and fosters a deeper understanding of\npatient concerns, paving the way for advancements in personalized and\nresponsive cancer care. The code and dataset used in this work are publicly\navailable."}
{"id": "2505.04087", "pdf": "https://arxiv.org/pdf/2505.04087", "abs": "https://arxiv.org/abs/2505.04087", "authors": ["Zixuan Hu", "Yichun Hu", "Ling-Yu Duan"], "title": "SEVA: Leveraging Single-Step Ensemble of Vicinal Augmentations for Test-Time Adaptation", "categories": ["cs.CV"], "comment": null, "summary": "Test-Time adaptation (TTA) aims to enhance model robustness against\ndistribution shifts through rapid model adaptation during inference. While\nexisting TTA methods often rely on entropy-based unsupervised training and\nachieve promising results, the common practice of a single round of entropy\ntraining is typically unable to adequately utilize reliable samples, hindering\nadaptation efficiency. In this paper, we discover augmentation strategies can\neffectively unleash the potential of reliable samples, but the rapidly growing\ncomputational cost impedes their real-time application. To address this\nlimitation, we propose a novel TTA approach named Single-step Ensemble of\nVicinal Augmentations (SEVA), which can take advantage of data augmentations\nwithout increasing the computational burden. Specifically, instead of\nexplicitly utilizing the augmentation strategy to generate new data, SEVA\ndevelops a theoretical framework to explore the impacts of multiple\naugmentations on model adaptation and proposes to optimize an upper bound of\nthe entropy loss to integrate the effects of multiple rounds of augmentation\ntraining into a single step. Furthermore, we discover and verify that using the\nupper bound as the loss is more conducive to the selection mechanism, as it can\neffectively filter out harmful samples that confuse the model. Combining these\ntwo key advantages, the proposed efficient loss and a complementary selection\nstrategy can simultaneously boost the potential of reliable samples and meet\nthe stringent time requirements of TTA. The comprehensive experiments on\nvarious network architectures across challenging testing scenarios demonstrate\nimpressive performances and the broad adaptability of SEVA. The code will be\npublicly available."}
{"id": "2505.04388", "pdf": "https://arxiv.org/pdf/2505.04388", "abs": "https://arxiv.org/abs/2505.04388", "authors": ["Dario Garcia-Gasulla", "Jordi Bayarri-Planas", "Ashwin Kumar Gururajan", "Enrique Lopez-Cuena", "Adrian Tormos", "Daniel Hinjos", "Pablo Bernabeu-Perez", "Anna Arias-Duart", "Pablo Agustin Martin-Torres", "Marta Gonzalez-Mallo", "Sergio Alvarez-Napagao", "Eduard Ayguadé-Parra", "Ulises Cortés"], "title": "The Aloe Family Recipe for Open and Specialized Healthcare LLMs", "categories": ["cs.CL", "cs.AI"], "comment": "arXiv admin note: substantial text overlap with arXiv:2405.01886", "summary": "Purpose: With advancements in Large Language Models (LLMs) for healthcare,\nthe need arises for competitive open-source models to protect the public\ninterest. This work contributes to the field of open medical LLMs by optimizing\nkey stages of data preprocessing and training, while showing how to improve\nmodel safety (through DPO) and efficacy (through RAG). The evaluation\nmethodology used, which includes four different types of tests, defines a new\nstandard for the field. The resultant models, shown to be competitive with the\nbest private alternatives, are released with a permisive license.\n  Methods: Building on top of strong base models like Llama 3.1 and Qwen 2.5,\nAloe Beta uses a custom dataset to enhance public data with synthetic Chain of\nThought examples. The models undergo alignment with Direct Preference\nOptimization, emphasizing ethical and policy-aligned performance in the\npresence of jailbreaking attacks. Evaluation includes close-ended, open-ended,\nsafety and human assessments, to maximize the reliability of results.\n  Results: Recommendations are made across the entire pipeline, backed by the\nsolid performance of the Aloe Family. These models deliver competitive\nperformance across healthcare benchmarks and medical fields, and are often\npreferred by healthcare professionals. On bias and toxicity, the Aloe Beta\nmodels significantly improve safety, showing resilience to unseen jailbreaking\nattacks. For a responsible release, a detailed risk assessment specific to\nhealthcare is attached to the Aloe Family models.\n  Conclusion: The Aloe Beta models, and the recipe that leads to them, are a\nsignificant contribution to the open-source medical LLM field, offering\ntop-of-the-line performance while maintaining high ethical requirements. This\nwork sets a new standard for developing and reporting aligned LLMs in\nhealthcare."}
{"id": "2505.04088", "pdf": "https://arxiv.org/pdf/2505.04088", "abs": "https://arxiv.org/abs/2505.04088", "authors": ["Shang Zhang", "Huanbin Zhang", "Dali Feng", "Yujie Cui", "Ruoyan Xiong", "Cen He"], "title": "SMMT: Siamese Motion Mamba with Self-attention for Thermal Infrared Target Tracking", "categories": ["cs.CV"], "comment": null, "summary": "Thermal infrared (TIR) object tracking often suffers from challenges such as\ntarget occlusion, motion blur, and background clutter, which significantly\ndegrade the performance of trackers. To address these issues, this paper\npro-poses a novel Siamese Motion Mamba Tracker (SMMT), which integrates a\nbidirectional state-space model and a self-attention mechanism. Specifically,\nwe introduce the Motion Mamba module into the Siamese architecture to ex-tract\nmotion features and recover overlooked edge details using bidirectional\nmodeling and self-attention. We propose a Siamese parameter-sharing strate-gy\nthat allows certain convolutional layers to share weights. This approach\nreduces computational redundancy while preserving strong feature\nrepresen-tation. In addition, we design a motion edge-aware regression loss to\nimprove tracking accuracy, especially for motion-blurred targets. Extensive\nexperi-ments are conducted on four TIR tracking benchmarks, including\nLSOTB-TIR, PTB-TIR, VOT-TIR2015, and VOT-TIR 2017. The results show that SMMT\nachieves superior performance in TIR target tracking."}
{"id": "2505.04393", "pdf": "https://arxiv.org/pdf/2505.04393", "abs": "https://arxiv.org/abs/2505.04393", "authors": ["David Exler", "Mark Schutera", "Markus Reischl", "Luca Rettenberger"], "title": "Large Means Left: Political Bias in Large Language Models Increases with Their Number of Parameters", "categories": ["cs.CL"], "comment": null, "summary": "With the increasing prevalence of artificial intelligence, careful evaluation\nof inherent biases needs to be conducted to form the basis for alleviating the\neffects these predispositions can have on users. Large language models (LLMs)\nare predominantly used by many as a primary source of information for various\ntopics. LLMs frequently make factual errors, fabricate data (hallucinations),\nor present biases, exposing users to misinformation and influencing opinions.\nEducating users on their risks is key to responsible use, as bias, unlike\nhallucinations, cannot be caught through data verification. We quantify the\npolitical bias of popular LLMs in the context of the recent vote of the German\nBundestag using the score produced by the Wahl-O-Mat. This metric measures the\nalignment between an individual's political views and the positions of German\npolitical parties. We compare the models' alignment scores to identify factors\ninfluencing their political preferences. Doing so, we discover a bias toward\nleft-leaning parties, most dominant in larger LLMs. Also, we find that the\nlanguage we use to communicate with the models affects their political views.\nAdditionally, we analyze the influence of a model's origin and release date and\ncompare the results to the outcome of the recent vote of the Bundestag. Our\nresults imply that LLMs are prone to exhibiting political bias. Large\ncorporations with the necessary means to develop LLMs, thus, knowingly or\nunknowingly, have a responsibility to contain these biases, as they can\ninfluence each voter's decision-making process and inform public opinion in\ngeneral and at scale."}
{"id": "2505.04105", "pdf": "https://arxiv.org/pdf/2505.04105", "abs": "https://arxiv.org/abs/2505.04105", "authors": ["Andrew Zhang", "Hao Wang", "Shuchang Ye", "Michael Fulham", "Jinman Kim"], "title": "MAISY: Motion-Aware Image SYnthesis for MedicalImage Motion Correction", "categories": ["cs.CV"], "comment": null, "summary": "Patient motion during medical image acquisition causes blurring, ghosting,\nand distorts organs, which makes image interpretation challenging.Current\nstate-of-the-art algorithms using Generative Adversarial Network (GAN)-based\nmethods with their ability to learn the mappings between corrupted images and\ntheir ground truth via Structural Similarity Index Measure (SSIM) loss\neffectively generate motion-free images. However, we identified the following\nlimitations: (i) they mainly focus on global structural characteristics and\ntherefore overlook localized features that often carry critical pathological\ninformation, and (ii) the SSIM loss function struggles to handle images with\nvarying pixel intensities, luminance factors, and variance. In this study, we\npropose Motion-Aware Image SYnthesis (MAISY) which initially characterize\nmotion and then uses it for correction by: (a) leveraging the foundation model\nSegment Anything Model (SAM), to dynamically learn spatial patterns along\nanatomical boundaries where motion artifacts are most pronounced and, (b)\nintroducing the Variance-Selective SSIM (VS-SSIM) loss which adaptively\nemphasizes spatial regions with high pixel variance to preserve essential\nanatomical details during artifact correction. Experiments on chest and head CT\ndatasets demonstrate that our model outperformed the state-of-the-art\ncounterparts, with Peak Signal-to-Noise Ratio (PSNR) increasing by 40%, SSIM by\n10%, and Dice by 16%."}
{"id": "2505.04406", "pdf": "https://arxiv.org/pdf/2505.04406", "abs": "https://arxiv.org/abs/2505.04406", "authors": ["Aidar Valeev", "Roman Garaev", "Vadim Lomshakov", "Irina Piontkovskaya", "Vladimir Ivanov", "Israel Adewuyi"], "title": "YABLoCo: Yet Another Benchmark for Long Context Code Generation", "categories": ["cs.CL", "cs.AI", "cs.SE"], "comment": "Presented at LLM4Code 2025 Workshop co-located wtih ICSE 2025", "summary": "Large Language Models demonstrate the ability to solve various programming\ntasks, including code generation. Typically, the performance of LLMs is\nmeasured on benchmarks with small or medium-sized context windows of thousands\nof lines of code. At the same time, in real-world software projects,\nrepositories can span up to millions of LoC. This paper closes this gap by\ncontributing to the long context code generation benchmark (YABLoCo). The\nbenchmark featured a test set of 215 functions selected from four large\nrepositories with thousands of functions. The dataset contained metadata of\nfunctions, contexts of the functions with different levels of dependencies,\ndocstrings, functions bodies, and call graphs for each repository. This paper\npresents three key aspects of the contribution. First, the benchmark aims at\nfunction body generation in large repositories in C and C++, two languages not\ncovered by previous benchmarks. Second, the benchmark contains large\nrepositories from 200K to 2,000K LoC. Third, we contribute a scalable\nevaluation pipeline for efficient computing of the target metrics and a tool\nfor visual analysis of generated code. Overall, these three aspects allow for\nevaluating code generation in large repositories in C and C++."}
{"id": "2505.04109", "pdf": "https://arxiv.org/pdf/2505.04109", "abs": "https://arxiv.org/abs/2505.04109", "authors": ["Mengya Liu", "Siyuan Li", "Ajad Chhatkuli", "Prune Truong", "Luc Van Gool", "Federico Tombari"], "title": "One2Any: One-Reference 6D Pose Estimation for Any Object", "categories": ["cs.CV"], "comment": "accepted by CVPR 2025", "summary": "6D object pose estimation remains challenging for many applications due to\ndependencies on complete 3D models, multi-view images, or training limited to\nspecific object categories. These requirements make generalization to novel\nobjects difficult for which neither 3D models nor multi-view images may be\navailable. To address this, we propose a novel method One2Any that estimates\nthe relative 6-degrees of freedom (DOF) object pose using only a single\nreference-single query RGB-D image, without prior knowledge of its 3D model,\nmulti-view data, or category constraints. We treat object pose estimation as an\nencoding-decoding process, first, we obtain a comprehensive Reference Object\nPose Embedding (ROPE) that encodes an object shape, orientation, and texture\nfrom a single reference view. Using this embedding, a U-Net-based pose decoding\nmodule produces Reference Object Coordinate (ROC) for new views, enabling fast\nand accurate pose estimation. This simple encoding-decoding framework allows\nour model to be trained on any pair-wise pose data, enabling large-scale\ntraining and demonstrating great scalability. Experiments on multiple benchmark\ndatasets demonstrate that our model generalizes well to novel objects,\nachieving state-of-the-art accuracy and robustness even rivaling methods that\nrequire multi-view or CAD inputs, at a fraction of compute."}
{"id": "2505.04416", "pdf": "https://arxiv.org/pdf/2505.04416", "abs": "https://arxiv.org/abs/2505.04416", "authors": ["Xiaoyu Xu", "Minxin Du", "Qingqing Ye", "Haibo Hu"], "title": "OBLIVIATE: Robust and Practical Machine Unlearning for Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "comment": "18 pages, 2 figures", "summary": "Large language models (LLMs) trained over extensive corpora risk memorizing\nsensitive, copyrighted, or toxic content. To address this, we propose\nOBLIVIATE, a robust unlearning framework that removes targeted data while\npreserving model utility. The framework follows a structured process:\nextracting target tokens, building retain sets, and fine-tuning with a tailored\nloss function comprising three components -- masking, distillation, and world\nfact. Using low-rank adapters (LoRA), it ensures efficiency without\ncompromising unlearning quality. We conduct experiments on multiple datasets,\nincluding the Harry Potter series, WMDP, and TOFU, using a comprehensive suite\nof metrics: forget quality (new document-level memorization score), model\nutility, and fluency. Results demonstrate its effectiveness in resisting\nmembership inference attacks, minimizing the impact on retained data, and\nmaintaining robustness across diverse scenarios."}
{"id": "2505.04119", "pdf": "https://arxiv.org/pdf/2505.04119", "abs": "https://arxiv.org/abs/2505.04119", "authors": ["Zixiang Ai", "Zichen Liu", "Yuanhang Lei", "Zhenyu Cui", "Xu Zou", "Jiahuan Zhou"], "title": "GAPrompt: Geometry-Aware Point Cloud Prompt for 3D Vision Model", "categories": ["cs.CV"], "comment": "Accepted by ICML 2025", "summary": "Pre-trained 3D vision models have gained significant attention for their\npromising performance on point cloud data. However, fully fine-tuning these\nmodels for downstream tasks is computationally expensive and storage-intensive.\nExisting parameter-efficient fine-tuning (PEFT) approaches, which focus\nprimarily on input token prompting, struggle to achieve competitive performance\ndue to their limited ability to capture the geometric information inherent in\npoint clouds. To address this challenge, we propose a novel Geometry-Aware\nPoint Cloud Prompt (GAPrompt) that leverages geometric cues to enhance the\nadaptability of 3D vision models. First, we introduce a Point Prompt that\nserves as an auxiliary input alongside the original point cloud, explicitly\nguiding the model to capture fine-grained geometric details. Additionally, we\npresent a Point Shift Prompter designed to extract global shape information\nfrom the point cloud, enabling instance-specific geometric adjustments at the\ninput level. Moreover, our proposed Prompt Propagation mechanism incorporates\nthe shape information into the model's feature extraction process, further\nstrengthening its ability to capture essential geometric characteristics.\nExtensive experiments demonstrate that GAPrompt significantly outperforms\nstate-of-the-art PEFT methods and achieves competitive results compared to full\nfine-tuning on various benchmarks, while utilizing only 2.19% of trainable\nparameters. Our code is available at\nhttps://github.com/zhoujiahuan1991/ICML2025-VGP."}
{"id": "2505.04507", "pdf": "https://arxiv.org/pdf/2505.04507", "abs": "https://arxiv.org/abs/2505.04507", "authors": ["Ilya Koziev"], "title": "Detecting Spelling and Grammatical Anomalies in Russian Poetry Texts", "categories": ["cs.CL"], "comment": null, "summary": "The quality of natural language texts in fine-tuning datasets plays a\ncritical role in the performance of generative models, particularly in\ncomputational creativity tasks such as poem or song lyric generation. Fluency\ndefects in generated poems significantly reduce their value. However, training\ntexts are often sourced from internet-based platforms without stringent quality\ncontrol, posing a challenge for data engineers to manage defect levels\neffectively.\n  To address this issue, we propose the use of automated linguistic anomaly\ndetection to identify and filter out low-quality texts from training datasets\nfor creative models. In this paper, we present a comprehensive comparison of\nunsupervised and supervised text anomaly detection approaches, utilizing both\nsynthetic and human-labeled datasets. We also introduce the RUPOR dataset, a\ncollection of Russian-language human-labeled poems designed for cross-sentence\ngrammatical error detection, and provide the full evaluation code. Our work\naims to empower the community with tools and insights to improve the quality of\ntraining datasets for generative models in creative domains."}
{"id": "2505.04121", "pdf": "https://arxiv.org/pdf/2505.04121", "abs": "https://arxiv.org/abs/2505.04121", "authors": ["Zixiang Ai", "Zichen Liu", "Jiahuan Zhou"], "title": "Vision Graph Prompting via Semantic Low-Rank Decomposition", "categories": ["cs.CV"], "comment": "Accepted by ICML 2025", "summary": "Vision GNN (ViG) demonstrates superior performance by representing images as\ngraph structures, providing a more natural way to capture irregular semantic\npatterns beyond traditional grid or sequence-based representations. To\nefficiently adapt ViG to downstream tasks, parameter-efficient fine-tuning\ntechniques like visual prompting become increasingly essential. However,\nexisting prompting methods are primarily designed for Transformer-based models,\nneglecting the rich topological relationships among nodes and edges in\ngraph-based representations, limiting their capacity to model complex\nsemantics. In this paper, we propose Vision Graph Prompting (VGP), a novel\nframework tailored for vision graph structures. Our core insight reveals that\nsemantically connected components in the graph exhibit low-rank properties.\nBuilding on this observation, we introduce a semantic low-rank prompting method\nthat decomposes low-rank semantic features and integrates them with prompts on\nvision graph topologies, capturing both global structural patterns and\nfine-grained semantic dependencies. Extensive experiments demonstrate our\nmethod significantly improves ViG's transfer performance on diverse downstream\ntasks, achieving results comparable to full fine-tuning while maintaining\nparameter efficiency. Our code is available at\nhttps://github.com/zhoujiahuan1991/ICML2025-VGP."}
{"id": "2505.04519", "pdf": "https://arxiv.org/pdf/2505.04519", "abs": "https://arxiv.org/abs/2505.04519", "authors": ["Yehui Tang", "Yichun Yin", "Yaoyuan Wang", "Hang Zhou", "Yu Pan", "Wei Guo", "Ziyang Zhang", "Miao Rang", "Fangcheng Liu", "Naifu Zhang", "Binghan Li", "Yonghan Dong", "Xiaojun Meng", "Yasheng Wang", "Dong Li", "Yin Li", "Dandan Tu", "Can Chen", "Youliang Yan", "Fisher Yu", "Ruiming Tang", "Yunhe Wang", "Botian Huang", "Bo Wang", "Boxiao Liu", "Changzheng Zhang", "Da Kuang", "Fei Liu", "Gang Huang", "Jiansheng Wei", "Jiarui Qin", "Jie Ran", "Jinpeng Li", "Jun Zhao", "Liang Dai", "Lin Li", "Liqun Deng", "Peifeng Qin", "Pengyuan Zeng", "Qiang Gu", "Shaohua Tang", "Shengjun Cheng", "Tao Gao", "Tao Yu", "Tianshu Li", "Tianyu Bi", "Wei He", "Weikai Mao", "Wenyong Huang", "Wulong Liu", "Xiabing Li", "Xianzhi Yu", "Xueyu Wu", "Xu He", "Yangkai Du", "Yan Xu", "Ye Tian", "Yimeng Wu", "Yongbing Huang", "Yong Tian", "Yong Zhu", "Yue Li", "Yufei Wang", "Yuhang Gai", "Yujun Li", "Yu Luo", "Yunsheng Ni", "Yusen Sun", "Zelin Chen", "Zhe Liu", "Zhicheng Liu", "Zhipeng Tu", "Zilin Ding", "Zongyuan Zhan"], "title": "Pangu Ultra MoE: How to Train Your Big MoE on Ascend NPUs", "categories": ["cs.CL"], "comment": null, "summary": "Sparse large language models (LLMs) with Mixture of Experts (MoE) and close\nto a trillion parameters are dominating the realm of most capable language\nmodels. However, the massive model scale poses significant challenges for the\nunderlying software and hardware systems. In this paper, we aim to uncover a\nrecipe to harness such scale on Ascend NPUs. The key goals are better usage of\nthe computing resources under the dynamic sparse model structures and\nmaterializing the expected performance gain on the actual hardware. To select\nmodel configurations suitable for Ascend NPUs without repeatedly running the\nexpensive experiments, we leverage simulation to compare the trade-off of\nvarious model hyperparameters. This study led to Pangu Ultra MoE, a sparse LLM\nwith 718 billion parameters, and we conducted experiments on the model to\nverify the simulation results. On the system side, we dig into Expert\nParallelism to optimize the communication between NPU devices to reduce the\nsynchronization overhead. We also optimize the memory efficiency within the\ndevices to further reduce the parameter and activation management overhead. In\nthe end, we achieve an MFU of 30.0% when training Pangu Ultra MoE, with\nperformance comparable to that of DeepSeek R1, on 6K Ascend NPUs, and\ndemonstrate that the Ascend system is capable of harnessing all the training\nstages of the state-of-the-art language models. Extensive experiments indicate\nthat our recipe can lead to efficient training of large-scale sparse language\nmodels with MoE. We also study the behaviors of such models for future\nreference."}
{"id": "2505.04147", "pdf": "https://arxiv.org/pdf/2505.04147", "abs": "https://arxiv.org/abs/2505.04147", "authors": ["Lixing Niu", "Jiapeng Li", "Xingping Yu", "Shu Wang", "Ruining Feng", "Bo Wu", "Ping Wei", "Yisen Wang", "Lifeng Fan"], "title": "R^3-VQA: \"Read the Room\" by Video Social Reasoning", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "\"Read the room\" is a significant social reasoning capability in human daily\nlife. Humans can infer others' mental states from subtle social cues. Previous\nsocial reasoning tasks and datasets lack complexity (e.g., simple scenes, basic\ninteractions, incomplete mental state variables, single-step reasoning, etc.)\nand fall far short of the challenges present in real-life social interactions.\nIn this paper, we contribute a valuable, high-quality, and comprehensive video\ndataset named R^3-VQA with precise and fine-grained annotations of social\nevents and mental states (i.e., belief, intent, desire, and emotion) as well as\ncorresponding social causal chains in complex social scenarios. Moreover, we\ninclude human-annotated and model-generated QAs. Our task R^3-VQA includes\nthree aspects: Social Event Understanding, Mental State Estimation, and Social\nCausal Reasoning. As a benchmark, we comprehensively evaluate the social\nreasoning capabilities and consistencies of current state-of-the-art large\nvision-language models (LVLMs). Comprehensive experiments show that (i) LVLMs\nare still far from human-level consistent social reasoning in complex social\nscenarios; (ii) Theory of Mind (ToM) prompting can help LVLMs perform better on\nsocial reasoning tasks. We provide some of our dataset and codes in\nsupplementary material and will release our full dataset and codes upon\nacceptance."}
{"id": "2505.04531", "pdf": "https://arxiv.org/pdf/2505.04531", "abs": "https://arxiv.org/abs/2505.04531", "authors": ["Josh McGiff", "Nikola S. Nikolov"], "title": "Overcoming Data Scarcity in Generative Language Modelling for Low-Resource Languages: A Systematic Review", "categories": ["cs.CL", "cs.AI"], "comment": "This work is currently under review. Please do not cite without\n  permission", "summary": "Generative language modelling has surged in popularity with the emergence of\nservices such as ChatGPT and Google Gemini. While these models have\ndemonstrated transformative potential in productivity and communication, they\noverwhelmingly cater to high-resource languages like English. This has\namplified concerns over linguistic inequality in natural language processing\n(NLP). This paper presents the first systematic review focused specifically on\nstrategies to address data scarcity in generative language modelling for\nlow-resource languages (LRL). Drawing from 54 studies, we identify, categorise\nand evaluate technical approaches, including monolingual data augmentation,\nback-translation, multilingual training, and prompt engineering, across\ngenerative tasks. We also analyse trends in architecture choices, language\nfamily representation, and evaluation methods. Our findings highlight a strong\nreliance on transformer-based models, a concentration on a small subset of\nLRLs, and a lack of consistent evaluation across studies. We conclude with\nrecommendations for extending these methods to a wider range of LRLs and\noutline open challenges in building equitable generative language systems.\nUltimately, this review aims to support researchers and developers in building\ninclusive AI tools for underrepresented languages, a necessary step toward\nempowering LRL speakers and the preservation of linguistic diversity in a world\nincreasingly shaped by large-scale language technologies."}
{"id": "2505.04150", "pdf": "https://arxiv.org/pdf/2505.04150", "abs": "https://arxiv.org/abs/2505.04150", "authors": ["Yu Yamaoka or Weng Ian Chan", "Shigeto Seno", "Soichiro Fukada", "Hideo Matsuda"], "title": "Learning from Similarity Proportion Loss for Classifying Skeletal Muscle Recovery Stages", "categories": ["cs.CV", "cs.LG"], "comment": "MICCAI2024 workshop ADSMI in Morocco (oral) [Peer-reviewed]", "summary": "Evaluating the regeneration process of damaged muscle tissue is a fundamental\nanalysis in muscle research to measure experimental effect sizes and uncover\nmechanisms behind muscle weakness due to aging and disease. The conventional\napproach to assessing muscle tissue regeneration involves whole-slide imaging\nand expert visual inspection of the recovery stages based on the morphological\ninformation of cells and fibers. There is a need to replace these tasks with\nautomated methods incorporating machine learning techniques to ensure a\nquantitative and objective analysis. Given the limited availability of fully\nlabeled data, a possible approach is Learning from Label Proportions (LLP), a\nweakly supervised learning method using class label proportions. However,\ncurrent LLP methods have two limitations: (1) they cannot adapt the feature\nextractor for muscle tissues, and (2) they treat the classes representing\nrecovery stages and cell morphological changes as nominal, resulting in the\nloss of ordinal information. To address these issues, we propose Ordinal Scale\nLearning from Similarity Proportion (OSLSP), which uses a similarity proportion\nloss derived from two bag combinations. OSLSP can update the feature extractor\nby using class proportion attention to the ordinal scale of the class. Our\nmodel with OSLSP outperforms large-scale pre-trained and fine-tuning models in\nclassification tasks of skeletal muscle recovery stages."}
{"id": "2505.04588", "pdf": "https://arxiv.org/pdf/2505.04588", "abs": "https://arxiv.org/abs/2505.04588", "authors": ["Hao Sun", "Zile Qiao", "Jiayan Guo", "Xuanbo Fan", "Yingyan Hou", "Yong Jiang", "Pengjun Xie", "Fei Huang", "Yan Zhang"], "title": "ZeroSearch: Incentivize the Search Capability of LLMs without Searching", "categories": ["cs.CL"], "comment": null, "summary": "Effective information searching is essential for enhancing the reasoning and\ngeneration capabilities of large language models (LLMs). Recent research has\nexplored using reinforcement learning (RL) to improve LLMs' search capabilities\nby interacting with live search engines in real-world environments. While these\napproaches show promising results, they face two major challenges: (1)\nUncontrolled Document Quality: The quality of documents returned by search\nengines is often unpredictable, introducing noise and instability into the\ntraining process. (2) Prohibitively High API Costs: RL training requires\nfrequent rollouts, potentially involving hundreds of thousands of search\nrequests, which incur substantial API expenses and severely constrain\nscalability. To address these challenges, we introduce ZeroSearch, a\nreinforcement learning framework that incentivizes the search capabilities of\nLLMs without interacting with real search engines. Our approach begins with\nlightweight supervised fine-tuning to transform the LLM into a retrieval module\ncapable of generating both relevant and noisy documents in response to a query.\nDuring RL training, we employ a curriculum-based rollout strategy that\nincrementally degrades the quality of generated documents, progressively\neliciting the model's reasoning ability by exposing it to increasingly\nchallenging retrieval scenarios. Extensive experiments demonstrate that\nZeroSearch effectively incentivizes the search capabilities of LLMs using a 3B\nLLM as the retrieval module. Remarkably, a 7B retrieval module achieves\ncomparable performance to the real search engine, while a 14B retrieval module\neven surpasses it. Furthermore, it generalizes well across both base and\ninstruction-tuned models of various parameter sizes and is compatible with a\nwide range of RL algorithms."}
{"id": "2505.04175", "pdf": "https://arxiv.org/pdf/2505.04175", "abs": "https://arxiv.org/abs/2505.04175", "authors": ["Naphat Nithisopa", "Teerapong Panboonyuen"], "title": "DOTA: Deformable Optimized Transformer Architecture for End-to-End Text Recognition with Retrieval-Augmented Generation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Text recognition in natural images remains a challenging yet essential task,\nwith broad applications spanning computer vision and natural language\nprocessing. This paper introduces a novel end-to-end framework that combines\nResNet and Vision Transformer backbones with advanced methodologies, including\nDeformable Convolutions, Retrieval-Augmented Generation, and Conditional Random\nFields (CRF). These innovations collectively enhance feature representation and\nimprove Optical Character Recognition (OCR) performance. Specifically, the\nframework substitutes standard convolution layers in the third and fourth\nblocks with Deformable Convolutions, leverages adaptive dropout for\nregularization, and incorporates CRF for more refined sequence modeling.\nExtensive experiments conducted on six benchmark datasets IC13, IC15, SVT,\nIIIT5K, SVTP, and CUTE80 validate the proposed method's efficacy, achieving\nnotable accuracies: 97.32% on IC13, 58.26% on IC15, 88.10% on SVT, 74.13% on\nIIIT5K, 82.17% on SVTP, and 66.67% on CUTE80, resulting in an average accuracy\nof 77.77%. These results establish a new state-of-the-art for text recognition,\ndemonstrating the robustness of the approach across diverse and challenging\ndatasets."}
{"id": "2505.03786", "pdf": "https://arxiv.org/pdf/2505.03786", "abs": "https://arxiv.org/abs/2505.03786", "authors": ["Md Fahim Anjum"], "title": "When Reasoning Beats Scale: A 1.5B Reasoning Model Outranks 13B LLMs as Discriminator", "categories": ["cs.LG", "cs.CL"], "comment": "12 pages, 5 figures. Code available at:\n  https://github.com/MDFahimAnjum/llm-planning-with-reasoning", "summary": "Large Language Models (LLM) with reasoning capabilities offer a promising\npath for improving candidate evaluation in planning frameworks, but their\nrelative performance against traditional non-reasoning models remains largely\nunderexplored. In this study, we benchmark a distilled 1.5B parameter reasoning\nmodel (DeepSeek-R1) against several state-of-the-art non-reasoning LLMs within\na generator-discriminator LLM planning framework for the text-to-SQL task. For\nthis, we introduce a novel method for extracting soft scores from the\nchain-of-thought (CoT) outputs from reasoning that enables fine-grained ranking\nof candidates. Our central hypothesis is that reasoning models are more\neffective discriminators than non-reasoning LLMs. Our results show that\ndistilled DeepSeek-R1-1.5B achieves up to $87\\%$ higher F1 and $3.7\\%$ better\ndiscrimination accuracy than CodeLlama-7B, as well as $3.7\\%$ higher execution\naccuracy than CodeLlama-13B, despite having significantly fewer parameters.\nFurthermore, we find that there is a limit to the logical capabilities of\nreasoning models, and only providing more context or allowing more compute\nbudget for reasoning is not enough to improve their discrimination performance.\nFinally, we demonstrate that, unlike non-reasoning LLMs, reasoning models find\ngeneration more challenging than discrimination and may underperform as\ngenerators compared to smaller non-reasoning LLMs. Our work highlights the\npotential of reasoning models as discriminators in agentic frameworks, far\noutweighing their capabilities as generators, offering insights into their\noptimal role within LLM planning infrastructures."}
{"id": "2505.04185", "pdf": "https://arxiv.org/pdf/2505.04185", "abs": "https://arxiv.org/abs/2505.04185", "authors": ["Hail Song", "Wonsik Shin", "Naeun Lee", "Soomin Chung", "Nojun Kwak", "Woontack Woo"], "title": "S3D: Sketch-Driven 3D Model Generation", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted as a short paper to the GMCV Workshop at CVPR'25", "summary": "Generating high-quality 3D models from 2D sketches is a challenging task due\nto the inherent ambiguity and sparsity of sketch data. In this paper, we\npresent S3D, a novel framework that converts simple hand-drawn sketches into\ndetailed 3D models. Our method utilizes a U-Net-based encoder-decoder\narchitecture to convert sketches into face segmentation masks, which are then\nused to generate a 3D representation that can be rendered from novel views. To\nensure robust consistency between the sketch domain and the 3D output, we\nintroduce a novel style-alignment loss that aligns the U-Net bottleneck\nfeatures with the initial encoder outputs of the 3D generation module,\nsignificantly enhancing reconstruction fidelity. To further enhance the\nnetwork's robustness, we apply augmentation techniques to the sketch dataset.\nThis streamlined framework demonstrates the effectiveness of S3D in generating\nhigh-quality 3D models from sketch inputs. The source code for this project is\npublicly available at https://github.com/hailsong/S3D."}
{"id": "2505.03799", "pdf": "https://arxiv.org/pdf/2505.03799", "abs": "https://arxiv.org/abs/2505.03799", "authors": ["Hyun Lee", "Chris Yi", "Maminur Islam", "B. D. S. Aritra"], "title": "Scalability Matters: Overcoming Challenges in InstructGLM with Similarity-Degree-Based Sampling", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "To be published in International Joint Conference on Neural Networks\n  (IJCNN), 2025", "summary": "Large Language Models (LLMs) have demonstrated strong capabilities in various\nnatural language processing tasks; however, their application to graph-related\nproblems remains limited, primarily due to scalability constraints and the\nabsence of dedicated mechanisms for processing graph structures. Existing\napproaches predominantly integrate LLMs with Graph Neural Networks (GNNs),\nusing GNNs as feature encoders or auxiliary components. However, directly\nencoding graph structures within LLMs has been underexplored, particularly in\nthe context of large-scale graphs where token limitations hinder effective\nrepresentation. To address these challenges, we propose SDM-InstructGLM, a\nnovel instruction-tuned Graph Language Model (InstructGLM) framework that\nenhances scalability and efficiency without relying on GNNs. Our method\nintroduces a similarity-degree-based biased random walk mechanism, which\nselectively samples and encodes graph information based on node-feature\nsimilarity and degree centrality, ensuring an adaptive and structured\nrepresentation within the LLM. This approach significantly improves token\nefficiency, mitigates information loss due to random sampling, and enhances\nperformance on graph-based tasks such as node classification and link\nprediction. Furthermore, our results demonstrate the feasibility of LLM-only\ngraph processing, enabling scalable and interpretable Graph Language Models\n(GLMs) optimized through instruction-based fine-tuning. This work paves the way\nfor GNN-free approaches to graph learning, leveraging LLMs as standalone graph\nreasoning models. Our source code is available on GitHub."}
{"id": "2505.04192", "pdf": "https://arxiv.org/pdf/2505.04192", "abs": "https://arxiv.org/abs/2505.04192", "authors": ["Trinh T. L. Vuong", "Jin Tae Kwak"], "title": "VideoPath-LLaVA: Pathology Diagnostic Reasoning Through Video Instruction Tuning", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "We present VideoPath-LLaVA, the first large multimodal model (LMM) in\ncomputational pathology that integrates three distinct image scenarios, single\npatch images, automatically keyframe-extracted clips, and manually segmented\nvideo pathology images, to mimic the natural diagnostic process of\npathologists. By generating detailed histological descriptions and culminating\nin a definitive sign-out diagnosis, VideoPath-LLaVA bridges visual narratives\nwith diagnostic reasoning.\n  Central to our approach is the VideoPath-Instruct dataset, comprising 4278\nvideo and diagnosis-specific chain-of-thought instructional pairs sourced from\neducational histopathology videos on YouTube. Although high-quality data is\ncritical for enhancing diagnostic reasoning, its creation is time-intensive and\nlimited in volume. To overcome this challenge, we transfer knowledge from\nexisting single-image instruction datasets to train on weakly annotated,\nkeyframe-extracted clips, followed by fine-tuning on manually segmented videos.\nVideoPath-LLaVA establishes a new benchmark in pathology video analysis and\noffers a promising foundation for future AI systems that support clinical\ndecision-making through integrated visual and diagnostic reasoning. Our code,\ndata, and model are publicly available at\nhttps://github.com/trinhvg/VideoPath-LLaVA."}
{"id": "2505.03810", "pdf": "https://arxiv.org/pdf/2505.03810", "abs": "https://arxiv.org/abs/2505.03810", "authors": ["Euntae Choi", "Sumin Song", "Woosang Lim", "Sungjoo Yoo"], "title": "Grouped Sequency-arranged Rotation: Optimizing Rotation Transformation for Quantization for Free", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "7 pages", "summary": "Large Language Models (LLMs) face deployment challenges due to high\ncomputational costs, and while Post-Training Quantization (PTQ) offers a\nsolution, existing rotation-based methods struggle at very low bit-widths like\n2-bit. We introduce a novel, training-free approach to construct an improved\nrotation matrix, addressing the limitations of current methods. The key\ncontributions include leveraging the Walsh-Hadamard transform with sequency\nordering, which clusters similar frequency components to reduce quantization\nerror compared to standard Hadamard matrices, significantly improving\nperformance. Furthermore, we propose a Grouped Sequency-arranged Rotation (GSR)\nusing block-diagonal matrices with smaller Walsh blocks, effectively isolating\noutlier impacts and achieving performance comparable to optimization-based\nmethods without requiring any training. Our method demonstrates robust\nperformance on reasoning tasks and Perplexity (PPL) score on WikiText-2. Our\nmethod also enhances results even when applied over existing learned rotation\ntechniques."}
{"id": "2505.04201", "pdf": "https://arxiv.org/pdf/2505.04201", "abs": "https://arxiv.org/abs/2505.04201", "authors": ["Ning Cheng", "Jinan Xu", "Jialing Chen", "Wenjuan Han"], "title": "SToLa: Self-Adaptive Touch-Language Framework with Tactile Commonsense Reasoning in Open-Ended Scenarios", "categories": ["cs.CV"], "comment": null, "summary": "This paper explores the challenges of integrating tactile sensing into\nintelligent systems for multimodal reasoning, particularly in enabling\ncommonsense reasoning about the open-ended physical world. We identify two key\nchallenges: modality discrepancy, where existing large touch-language models\noften treat touch as a mere sub-modality of language, and open-ended tactile\ndata scarcity, where current datasets lack the diversity, open-endness and\ncomplexity needed for reasoning. To overcome these challenges, we introduce\nSToLa, a Self-Adaptive Touch-Language framework. SToLa utilizes Mixture of\nExperts (MoE) to dynamically process, unify, and manage tactile and language\nmodalities, capturing their unique characteristics. Crucially, we also present\na comprehensive tactile commonsense reasoning dataset and benchmark featuring\nfree-form questions and responses, 8 physical properties, 4 interactive\ncharacteristics, and diverse commonsense knowledge. Experiments show SToLa\nexhibits competitive performance compared to existing models on the PhysiCLeAR\nbenchmark and self-constructed datasets, proving the effectiveness of the\nMixture of Experts architecture in multimodal management and the performance\nadvantages for open-scenario tactile commonsense reasoning tasks."}
{"id": "2505.03814", "pdf": "https://arxiv.org/pdf/2505.03814", "abs": "https://arxiv.org/abs/2505.03814", "authors": ["Ganghua Wang", "Zhaorun Chen", "Bo Li", "Haifeng Xu"], "title": "Cer-Eval: Certifiable and Cost-Efficient Evaluation Framework for LLMs", "categories": ["stat.ML", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "As foundation models continue to scale, the size of trained models grows\nexponentially, presenting significant challenges for their evaluation. Current\nevaluation practices involve curating increasingly large datasets to assess the\nperformance of large language models (LLMs). However, there is a lack of\nsystematic analysis and guidance on determining the sufficiency of test data or\nselecting informative samples for evaluation. This paper introduces a\ncertifiable and cost-efficient evaluation framework for LLMs. Our framework\nadapts to different evaluation objectives and outputs confidence intervals that\ncontain true values with high probability. We use ``test sample complexity'' to\nquantify the number of test points needed for a certifiable evaluation and\nderive tight bounds on test sample complexity. Based on the developed theory,\nwe develop a partition-based algorithm, named Cer-Eval, that adaptively selects\ntest points to minimize the cost of LLM evaluation. Real-world experiments\ndemonstrate that Cer-Eval can save 20% to 40% test points across various\nbenchmarks, while maintaining an estimation error level comparable to the\ncurrent evaluation process and providing a 95% confidence guarantee."}
{"id": "2505.04207", "pdf": "https://arxiv.org/pdf/2505.04207", "abs": "https://arxiv.org/abs/2505.04207", "authors": ["Mustafa Yurdakul", "Şakir Tasdemir"], "title": "An Enhanced YOLOv8 Model for Real-Time and Accurate Pothole Detection and Measurement", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Potholes cause vehicle damage and traffic accidents, creating serious safety\nand economic problems. Therefore, early and accurate detection of potholes is\ncrucial. Existing detection methods are usually only based on 2D RGB images and\ncannot accurately analyze the physical characteristics of potholes. In this\npaper, a publicly available dataset of RGB-D images (PothRGBD) is created and\nan improved YOLOv8-based model is proposed for both pothole detection and\npothole physical features analysis. The Intel RealSense D415 depth camera was\nused to collect RGB and depth data from the road surfaces, resulting in a\nPothRGBD dataset of 1000 images. The data was labeled in YOLO format suitable\nfor segmentation. A novel YOLO model is proposed based on the YOLOv8n-seg\narchitecture, which is structurally improved with Dynamic Snake Convolution\n(DSConv), Simple Attention Module (SimAM) and Gaussian Error Linear Unit\n(GELU). The proposed model segmented potholes with irregular edge structure\nmore accurately, and performed perimeter and depth measurements on depth maps\nwith high accuracy. The standard YOLOv8n-seg model achieved 91.9% precision,\n85.2% recall and 91.9% mAP@50. With the proposed model, the values increased to\n93.7%, 90.4% and 93.8% respectively. Thus, an improvement of 1.96% in\nprecision, 6.13% in recall and 2.07% in mAP was achieved. The proposed model\nperforms pothole detection as well as perimeter and depth measurement with high\naccuracy and is suitable for real-time applications due to its low model\ncomplexity. In this way, a lightweight and effective model that can be used in\ndeep learning-based intelligent transportation solutions has been acquired."}
{"id": "2505.03828", "pdf": "https://arxiv.org/pdf/2505.03828", "abs": "https://arxiv.org/abs/2505.03828", "authors": ["Yogesh Gajula"], "title": "Sentiment-Aware Recommendation Systems in E-Commerce: A Review from a Natural Language Processing Perspective", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "comment": "12 pages, 2 tables, 2 figures", "summary": "E-commerce platforms generate vast volumes of user feedback, such as star\nratings, written reviews, and comments. However, most recommendation engines\nrely primarily on numerical scores, often overlooking the nuanced opinions\nembedded in free text. This paper comprehensively reviews sentiment-aware\nrecommendation systems from a natural language processing perspective, covering\nadvancements from 2023 to early 2025. It highlights the benefits of integrating\nsentiment analysis into e-commerce recommenders to enhance prediction accuracy\nand explainability through detailed opinion extraction. Our survey categorizes\nrecent work into four main approaches: deep learning classifiers that combine\nsentiment embeddings with user item interactions, transformer based methods for\nnuanced feature extraction, graph neural networks that propagate sentiment\nsignals, and conversational recommenders that adapt in real time to user\nfeedback. We summarize model architectures and demonstrate how sentiment flows\nthrough recommendation pipelines, impacting dialogue-based suggestions. Key\nchallenges include handling noisy or sarcastic text, dynamic user preferences,\nand bias mitigation. Finally, we outline research gaps and provide a roadmap\nfor developing smarter, fairer, and more user-centric recommendation tools."}
{"id": "2505.04214", "pdf": "https://arxiv.org/pdf/2505.04214", "abs": "https://arxiv.org/abs/2505.04214", "authors": ["Fabian Wolf", "Oliver Tüselmann", "Arthur Matei", "Lukas Hennies", "Christoph Rass", "Gernot A. Fink"], "title": "CM1 -- A Dataset for Evaluating Few-Shot Information Extraction with Large Vision Language Models", "categories": ["cs.CV"], "comment": null, "summary": "The automatic extraction of key-value information from handwritten documents\nis a key challenge in document analysis. A reliable extraction is a\nprerequisite for the mass digitization efforts of many archives. Large Vision\nLanguage Models (LVLM) are a promising technology to tackle this problem\nespecially in scenarios where little annotated training data is available. In\nthis work, we present a novel dataset specifically designed to evaluate the\nfew-shot capabilities of LVLMs. The CM1 documents are a historic collection of\nforms with handwritten entries created in Europe to administer the Care and\nMaintenance program after World War Two. The dataset establishes three\nbenchmarks on extracting name and birthdate information and, furthermore,\nconsiders different training set sizes. We provide baseline results for two\ndifferent LVLMs and compare performances to an established full-page extraction\nmodel. While the traditional full-page model achieves highly competitive\nperformances, our experiments show that when only a few training samples are\navailable the considered LVLMs benefit from their size and heavy pretraining\nand outperform the classical approach."}
{"id": "2505.03961", "pdf": "https://arxiv.org/pdf/2505.03961", "abs": "https://arxiv.org/abs/2505.03961", "authors": ["Gerrit Großmann", "Larisa Ivanova", "Sai Leela Poduru", "Mohaddeseh Tabrizian", "Islam Mesabah", "David A. Selby", "Sebastian J. Vollmer"], "title": "The Power of Stories: Narrative Priming Shapes How LLM Agents Collaborate and Compete", "categories": ["cs.AI", "cs.CL", "cs.MA", "I.2.11; I.2.7; I.6; J.4"], "comment": "16 pages, 8 figures. Code available at\n  https://github.com/storyagents25/story-agents", "summary": "According to Yuval Noah Harari, large-scale human cooperation is driven by\nshared narratives that encode common beliefs and values. This study explores\nwhether such narratives can similarly nudge LLM agents toward collaboration. We\nuse a finitely repeated public goods game in which LLM agents choose either\ncooperative or egoistic spending strategies. We prime agents with stories\nhighlighting teamwork to different degrees and test how this influences\nnegotiation outcomes. Our experiments explore four questions:(1) How do\nnarratives influence negotiation behavior? (2) What differs when agents share\nthe same story versus different ones? (3) What happens when the agent numbers\ngrow? (4) Are agents resilient against self-serving negotiators? We find that\nstory-based priming significantly affects negotiation strategies and success\nrates. Common stories improve collaboration, benefiting each agent. By\ncontrast, priming agents with different stories reverses this effect, and those\nagents primed toward self-interest prevail. We hypothesize that these results\ncarry implications for multi-agent system design and AI alignment."}
{"id": "2505.04229", "pdf": "https://arxiv.org/pdf/2505.04229", "abs": "https://arxiv.org/abs/2505.04229", "authors": ["Theophilus Aidoo", "Till Koebe", "Akansh Maurya", "Hewan Shrestha", "Ingmar Weber"], "title": "A Weak Supervision Learning Approach Towards an Equitable Parking Lot Occupancy Estimation", "categories": ["cs.CV", "cs.CY"], "comment": null, "summary": "The scarcity and high cost of labeled high-resolution imagery have long\nchallenged remote sensing applications, particularly in low-income regions\nwhere high-resolution data are scarce. In this study, we propose a weak\nsupervision framework that estimates parking lot occupancy using 3m resolution\nsatellite imagery. By leveraging coarse temporal labels -- based on the\nassumption that parking lots of major supermarkets and hardware stores in\nGermany are typically full on Saturdays and empty on Sundays -- we train a\npairwise comparison model that achieves an AUC of 0.92 on large parking lots.\nThe proposed approach minimizes the reliance on expensive high-resolution\nimages and holds promise for scalable urban mobility analysis. Moreover, the\nmethod can be adapted to assess transit patterns and resource allocation in\nvulnerable communities, providing a data-driven basis to improve the well-being\nof those most in need."}
{"id": "2505.03997", "pdf": "https://arxiv.org/pdf/2505.03997", "abs": "https://arxiv.org/abs/2505.03997", "authors": ["Prudhviraj Naidu", "Zixian Wang", "Leon Bergen", "Ramamohan Paturi"], "title": "Quiet Feature Learning in Algorithmic Tasks", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "We train Transformer-based language models on ten foundational algorithmic\ntasks and observe pronounced phase transitions in their loss curves that\ndeviate from established power-law scaling trends. Over large ranges of\ncompute, the validation loss barely improves, then abruptly decreases. Probing\nthe models' internal representations reveals the learning of quiet features\nduring the stagnant phase, followed by sudden acquisition of loud features that\ncoincide with the sharp drop in loss. Our ablation experiments show that\ndisrupting a single learned feature can dramatically degrade performance,\nproviding evidence of their causal role in task performance. These findings\nchallenge the prevailing assumption that next-token predictive loss reliably\ntracks incremental progress; instead, key internal features may be developing\nbelow the surface until they coalesce, triggering a rapid performance gain."}
{"id": "2505.04262", "pdf": "https://arxiv.org/pdf/2505.04262", "abs": "https://arxiv.org/abs/2505.04262", "authors": ["Feng Yang", "Wenliang Qian", "Wangmeng Zuo", "Hui Li"], "title": "Bridging Geometry-Coherent Text-to-3D Generation with Multi-View Diffusion Priors and Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "Score Distillation Sampling (SDS) leverages pretrained 2D diffusion models to\nadvance text-to-3D generation but neglects multi-view correlations, being prone\nto geometric inconsistencies and multi-face artifacts in the generated 3D\ncontent. In this work, we propose Coupled Score Distillation (CSD), a framework\nthat couples multi-view joint distribution priors to ensure geometrically\nconsistent 3D generation while enabling the stable and direct optimization of\n3D Gaussian Splatting. Specifically, by reformulating the optimization as a\nmulti-view joint optimization problem, we derive an effective optimization rule\nthat effectively couples multi-view priors to guide optimization across\ndifferent viewpoints while preserving the diversity of generated 3D assets.\nAdditionally, we propose a framework that directly optimizes 3D Gaussian\nSplatting (3D-GS) with random initialization to generate geometrically\nconsistent 3D content. We further employ a deformable tetrahedral grid,\ninitialized from 3D-GS and refined through CSD, to produce high-quality,\nrefined meshes. Quantitative and qualitative experimental results demonstrate\nthe efficiency and competitive quality of our approach."}
{"id": "2505.04066", "pdf": "https://arxiv.org/pdf/2505.04066", "abs": "https://arxiv.org/abs/2505.04066", "authors": ["Tuochao Chen", "Nicholas Batchelder", "Alisa Liu", "Noah Smith", "Shyamnath Gollakota"], "title": "LLAMAPIE: Proactive In-Ear Conversation Assistants", "categories": ["cs.LG", "cs.CL", "eess.AS"], "comment": null, "summary": "We introduce LlamaPIE, the first real-time proactive assistant designed to\nenhance human conversations through discreet, concise guidance delivered via\nhearable devices. Unlike traditional language models that require explicit user\ninvocation, this assistant operates in the background, anticipating user needs\nwithout interrupting conversations. We address several challenges, including\ndetermining when to respond, crafting concise responses that enhance\nconversations, leveraging knowledge of the user for context-aware assistance,\nand real-time, on-device processing. To achieve this, we construct a\nsemi-synthetic dialogue dataset and propose a two-model pipeline: a small model\nthat decides when to respond and a larger model that generates the response. We\nevaluate our approach on real-world datasets, demonstrating its effectiveness\nin providing helpful, unobtrusive assistance. User studies with our assistant,\nimplemented on Apple Silicon M2 hardware, show a strong preference for the\nproactive assistant over both a baseline with no assistance and a reactive\nmodel, highlighting the potential of LlamaPie to enhance live conversations."}
{"id": "2505.04270", "pdf": "https://arxiv.org/pdf/2505.04270", "abs": "https://arxiv.org/abs/2505.04270", "authors": ["Yisen Feng", "Haoyu Zhang", "Meng Liu", "Weili Guan", "Liqiang Nie"], "title": "Object-Shot Enhanced Grounding Network for Egocentric Video", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by CVPR 2025", "summary": "Egocentric video grounding is a crucial task for embodied intelligence\napplications, distinct from exocentric video moment localization. Existing\nmethods primarily focus on the distributional differences between egocentric\nand exocentric videos but often neglect key characteristics of egocentric\nvideos and the fine-grained information emphasized by question-type queries. To\naddress these limitations, we propose OSGNet, an Object-Shot enhanced Grounding\nNetwork for egocentric video. Specifically, we extract object information from\nvideos to enrich video representation, particularly for objects highlighted in\nthe textual query but not directly captured in the video features.\nAdditionally, we analyze the frequent shot movements inherent to egocentric\nvideos, leveraging these features to extract the wearer's attention\ninformation, which enhances the model's ability to perform modality alignment.\nExperiments conducted on three datasets demonstrate that OSGNet achieves\nstate-of-the-art performance, validating the effectiveness of our approach. Our\ncode can be found at https://github.com/Yisen-Feng/OSGNet."}
{"id": "2505.04171", "pdf": "https://arxiv.org/pdf/2505.04171", "abs": "https://arxiv.org/abs/2505.04171", "authors": ["Nouar Aldahoul", "Hazem Ibrahim", "Matteo Varvello", "Aaron Kaufman", "Talal Rahwan", "Yasir Zaki"], "title": "Large Language Models are often politically extreme, usually ideologically inconsistent, and persuasive even in informational contexts", "categories": ["cs.CY", "cs.CL"], "comment": "61 pages, 29 figures", "summary": "Large Language Models (LLMs) are a transformational technology, fundamentally\nchanging how people obtain information and interact with the world. As people\nbecome increasingly reliant on them for an enormous variety of tasks, a body of\nacademic research has developed to examine these models for inherent biases,\nespecially political biases, often finding them small. We challenge this\nprevailing wisdom. First, by comparing 31 LLMs to legislators, judges, and a\nnationally representative sample of U.S. voters, we show that LLMs' apparently\nsmall overall partisan preference is the net result of offsetting extreme views\non specific topics, much like moderate voters. Second, in a randomized\nexperiment, we show that LLMs can promulgate their preferences into political\npersuasiveness even in information-seeking contexts: voters randomized to\ndiscuss political issues with an LLM chatbot are as much as 5 percentage points\nmore likely to express the same preferences as that chatbot. Contrary to\nexpectations, these persuasive effects are not moderated by familiarity with\nLLMs, news consumption, or interest in politics. LLMs, especially those\ncontrolled by private companies or governments, may become a powerful and\ntargeted vector for political influence."}
{"id": "2505.04276", "pdf": "https://arxiv.org/pdf/2505.04276", "abs": "https://arxiv.org/abs/2505.04276", "authors": ["Yajie Fu", "Chaorui Huang", "Junwei Li", "Hui Kong", "Yibin Tian", "Huakang Li", "Zhiyuan Zhang"], "title": "HDiffTG: A Lightweight Hybrid Diffusion-Transformer-GCN Architecture for 3D Human Pose Estimation", "categories": ["cs.CV", "cs.MM"], "comment": "8 pages, 4 figures, International Joint Conference on Neural Networks\n  (IJCNN)", "summary": "We propose HDiffTG, a novel 3D Human Pose Estimation (3DHPE) method that\nintegrates Transformer, Graph Convolutional Network (GCN), and diffusion model\ninto a unified framework. HDiffTG leverages the strengths of these techniques\nto significantly improve pose estimation accuracy and robustness while\nmaintaining a lightweight design. The Transformer captures global\nspatiotemporal dependencies, the GCN models local skeletal structures, and the\ndiffusion model provides step-by-step optimization for fine-tuning, achieving a\ncomplementary balance between global and local features. This integration\nenhances the model's ability to handle pose estimation under occlusions and in\ncomplex scenarios. Furthermore, we introduce lightweight optimizations to the\nintegrated model and refine the objective function design to reduce\ncomputational overhead without compromising performance. Evaluation results on\nthe Human3.6M and MPI-INF-3DHP datasets demonstrate that HDiffTG achieves\nstate-of-the-art (SOTA) performance on the MPI-INF-3DHP dataset while excelling\nin both accuracy and computational efficiency. Additionally, the model exhibits\nexceptional robustness in noisy and occluded environments. Source codes and\nmodels are available at https://github.com/CirceJie/HDiffTG"}
{"id": "2505.04192", "pdf": "https://arxiv.org/pdf/2505.04192", "abs": "https://arxiv.org/abs/2505.04192", "authors": ["Trinh T. L. Vuong", "Jin Tae Kwak"], "title": "VideoPath-LLaVA: Pathology Diagnostic Reasoning Through Video Instruction Tuning", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "We present VideoPath-LLaVA, the first large multimodal model (LMM) in\ncomputational pathology that integrates three distinct image scenarios, single\npatch images, automatically keyframe-extracted clips, and manually segmented\nvideo pathology images, to mimic the natural diagnostic process of\npathologists. By generating detailed histological descriptions and culminating\nin a definitive sign-out diagnosis, VideoPath-LLaVA bridges visual narratives\nwith diagnostic reasoning.\n  Central to our approach is the VideoPath-Instruct dataset, comprising 4278\nvideo and diagnosis-specific chain-of-thought instructional pairs sourced from\neducational histopathology videos on YouTube. Although high-quality data is\ncritical for enhancing diagnostic reasoning, its creation is time-intensive and\nlimited in volume. To overcome this challenge, we transfer knowledge from\nexisting single-image instruction datasets to train on weakly annotated,\nkeyframe-extracted clips, followed by fine-tuning on manually segmented videos.\nVideoPath-LLaVA establishes a new benchmark in pathology video analysis and\noffers a promising foundation for future AI systems that support clinical\ndecision-making through integrated visual and diagnostic reasoning. Our code,\ndata, and model are publicly available at\nhttps://github.com/trinhvg/VideoPath-LLaVA."}
{"id": "2505.04281", "pdf": "https://arxiv.org/pdf/2505.04281", "abs": "https://arxiv.org/abs/2505.04281", "authors": ["Yi Li", "Zhiyuan Zhang", "Jiangnan Xia", "Jianghan Cheng", "Qilong Wu", "Junwei Li", "Yibin Tian", "Hui Kong"], "title": "TS-Diff: Two-Stage Diffusion Model for Low-Light RAW Image Enhancement", "categories": ["cs.CV"], "comment": "International Joint Conference on Neural Networks (IJCNN)", "summary": "This paper presents a novel Two-Stage Diffusion Model (TS-Diff) for enhancing\nextremely low-light RAW images. In the pre-training stage, TS-Diff synthesizes\nnoisy images by constructing multiple virtual cameras based on a noise space.\nCamera Feature Integration (CFI) modules are then designed to enable the model\nto learn generalizable features across diverse virtual cameras. During the\naligning stage, CFIs are averaged to create a target-specific CFI$^T$, which is\nfine-tuned using a small amount of real RAW data to adapt to the noise\ncharacteristics of specific cameras. A structural reparameterization technique\nfurther simplifies CFI$^T$ for efficient deployment. To address color shifts\nduring the diffusion process, a color corrector is introduced to ensure color\nconsistency by dynamically adjusting global color distributions. Additionally,\na novel dataset, QID, is constructed, featuring quantifiable illumination\nlevels and a wide dynamic range, providing a comprehensive benchmark for\ntraining and evaluation under extreme low-light conditions. Experimental\nresults demonstrate that TS-Diff achieves state-of-the-art performance on\nmultiple datasets, including QID, SID, and ELD, excelling in denoising,\ngeneralization, and color consistency across various cameras and illumination\nlevels. These findings highlight the robustness and versatility of TS-Diff,\nmaking it a practical solution for low-light imaging applications. Source codes\nand models are available at https://github.com/CircccleK/TS-Diff"}
{"id": "2505.04364", "pdf": "https://arxiv.org/pdf/2505.04364", "abs": "https://arxiv.org/abs/2505.04364", "authors": ["Kai Ruan", "Mowen Huang", "Ji-Rong Wen", "Hao Sun"], "title": "Benchmarking LLMs' Swarm intelligence", "categories": ["cs.MA", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) show potential for complex reasoning, yet their\ncapacity for emergent coordination in Multi-Agent Systems (MAS) when operating\nunder strict constraints-such as limited local perception and communication,\ncharacteristic of natural swarms-remains largely unexplored, particularly\nconcerning the nuances of swarm intelligence. Existing benchmarks often do not\nfully capture the unique challenges of decentralized coordination that arise\nwhen agents operate with incomplete spatio-temporal information. To bridge this\ngap, we introduce SwarmBench, a novel benchmark designed to systematically\nevaluate the swarm intelligence capabilities of LLMs acting as decentralized\nagents. SwarmBench features five foundational MAS coordination tasks within a\nconfigurable 2D grid environment, forcing agents to rely primarily on local\nsensory input (k x k view) and local communication. We propose metrics for\ncoordination effectiveness and analyze emergent group dynamics. Evaluating\nseveral leading LLMs in a zero-shot setting, we find significant performance\nvariations across tasks, highlighting the difficulties posed by local\ninformation constraints. While some coordination emerges, results indicate\nlimitations in robust planning and strategy formation under uncertainty in\nthese decentralized scenarios. Assessing LLMs under swarm-like conditions is\ncrucial for realizing their potential in future decentralized systems. We\nrelease SwarmBench as an open, extensible toolkit-built upon a customizable and\nscalable physical system with defined mechanical properties. It provides\nenvironments, prompts, evaluation scripts, and the comprehensive experimental\ndatasets generated, aiming to foster reproducible research into LLM-based MAS\ncoordination and the theoretical underpinnings of Embodied MAS. Our code\nrepository is available at https://github.com/x66ccff/swarmbench."}
{"id": "2505.04306", "pdf": "https://arxiv.org/pdf/2505.04306", "abs": "https://arxiv.org/abs/2505.04306", "authors": ["Qiannan Fan", "Zhuoyang Li", "Jitong Li", "Chenyang Cao"], "title": "MoDE: Mixture of Diffusion Experts for Any Occluded Face Recognition", "categories": ["cs.CV", "I.4.8; I.5.4; I.2.10"], "comment": "8 pages,7 figures", "summary": "With the continuous impact of epidemics, people have become accustomed to\nwearing masks. However, most current occluded face recognition (OFR) algorithms\nlack prior knowledge of occlusions, resulting in poor performance when dealing\nwith occluded faces of varying types and severity in reality. Recognizing\noccluded faces is still a significant challenge, which greatly affects the\nconvenience of people's daily lives. In this paper, we propose an\nidentity-gated mixture of diffusion experts (MoDE) for OFR. Each\ndiffusion-based generative expert estimates one possible complete image for\noccluded faces. Considering the random sampling process of the diffusion model,\nwhich introduces inevitable differences and variations between the inpainted\nfaces and the real ones. To ensemble effective information from\nmulti-reconstructed faces, we introduce an identity-gating network to evaluate\nthe contribution of each reconstructed face to the identity and adaptively\nintegrate the predictions in the decision space. Moreover, our MoDE is a\nplug-and-play module for most existing face recognition models. Extensive\nexperiments on three public face datasets and two datasets in the wild validate\nour advanced performance for various occlusions in comparison with the\ncompeting methods."}
{"id": "2505.04457", "pdf": "https://arxiv.org/pdf/2505.04457", "abs": "https://arxiv.org/abs/2505.04457", "authors": ["Shigeki Karita", "Yuma Koizumi", "Heiga Zen", "Haruko Ishikawa", "Robin Scheibler", "Michiel Bacchiani"], "title": "Miipher-2: A Universal Speech Restoration Model for Million-Hour Scale Data Restoration", "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": null, "summary": "Training data cleaning is a new application for generative model-based speech\nrestoration (SR). This paper introduces Miipher-2, an SR model designed for\nmillion-hour scale data, for training data cleaning for large-scale generative\nmodels like large language models. Key challenges addressed include\ngeneralization to unseen languages, operation without explicit conditioning\n(e.g., text, speaker ID), and computational efficiency. Miipher-2 utilizes a\nfrozen, pre-trained Universal Speech Model (USM), supporting over 300\nlanguages, as a robust, conditioning-free feature extractor. To optimize\nefficiency and minimize memory, Miipher-2 incorporates parallel adapters for\npredicting clean USM features from noisy inputs and employs the WaneFit neural\nvocoder for waveform synthesis. These components were trained on 3,000 hours of\nmulti-lingual, studio-quality recordings with augmented degradations, while USM\nparameters remained fixed. Experimental results demonstrate Miipher-2's\nsuperior or comparable performance to conventional SR models in\nword-error-rate, speaker similarity, and both objective and subjective sound\nquality scores across all tested languages. Miipher-2 operates efficiently on\nconsumer-grade accelerators, achieving a real-time factor of 0.0078, enabling\nthe processing of a million-hour speech dataset in approximately three days\nusing only 100 such accelerators."}
{"id": "2505.04320", "pdf": "https://arxiv.org/pdf/2505.04320", "abs": "https://arxiv.org/abs/2505.04320", "authors": ["Zijun Zhou", "Yingying Deng", "Xiangyu He", "Weiming Dong", "Fan Tang"], "title": "Multi-turn Consistent Image Editing", "categories": ["cs.CV"], "comment": null, "summary": "Many real-world applications, such as interactive photo retouching, artistic\ncontent creation, and product design, require flexible and iterative image\nediting. However, existing image editing methods primarily focus on achieving\nthe desired modifications in a single step, which often struggles with\nambiguous user intent, complex transformations, or the need for progressive\nrefinements. As a result, these methods frequently produce inconsistent\noutcomes or fail to meet user expectations. To address these challenges, we\npropose a multi-turn image editing framework that enables users to iteratively\nrefine their edits, progressively achieving more satisfactory results. Our\napproach leverages flow matching for accurate image inversion and a\ndual-objective Linear Quadratic Regulators (LQR) for stable sampling,\neffectively mitigating error accumulation. Additionally, by analyzing the\nlayer-wise roles of transformers, we introduce a adaptive attention\nhighlighting method that enhances editability while preserving multi-turn\ncoherence. Extensive experiments demonstrate that our framework significantly\nimproves edit success rates and visual fidelity compared to existing methods."}
{"id": "2505.04528", "pdf": "https://arxiv.org/pdf/2505.04528", "abs": "https://arxiv.org/abs/2505.04528", "authors": ["Qi Liu", "Xinhao Zheng", "Renqiu Xia", "Xingzhi Qi", "Qinxiang Cao", "Junchi Yan"], "title": "Beyond Theorem Proving: Formulation, Framework and Benchmark for Formal Problem-Solving", "categories": ["cs.AI", "cs.CL", "cs.LO"], "comment": "42 pages, 3 figures", "summary": "As a seemingly self-explanatory task, problem-solving has been a significant\ncomponent of science and engineering. However, a general yet concrete\nformulation of problem-solving itself is missing. With the recent development\nof AI-based problem-solving agents, the demand for process-level verifiability\nis rapidly increasing yet underexplored. To fill these gaps, we present a\nprincipled formulation of problem-solving as a deterministic Markov decision\nprocess; a novel framework, FPS (Formal Problem-Solving), which utilizes\nexisting FTP (formal theorem proving) environments to perform process-verified\nproblem-solving; and D-FPS (Deductive FPS), decoupling solving and answer\nverification for better human-alignment. The expressiveness, soundness and\ncompleteness of the frameworks are proven. We construct three benchmarks on\nproblem-solving: FormalMath500, a formalization of a subset of the MATH500\nbenchmark; MiniF2F-Solving and PutnamBench-Solving, adaptations of FTP\nbenchmarks MiniF2F and PutnamBench. For faithful, interpretable, and\nhuman-aligned evaluation, we propose RPE (Restricted Propositional\nEquivalence), a symbolic approach to determine the correctness of answers by\nformal verification. We evaluate four prevalent FTP models and two prompting\nmethods as baselines, solving at most 23.77% of FormalMath500, 27.47% of\nMiniF2F-Solving, and 0.31% of PutnamBench-Solving."}
{"id": "2505.04347", "pdf": "https://arxiv.org/pdf/2505.04347", "abs": "https://arxiv.org/abs/2505.04347", "authors": ["Yanyu Li", "Pencheng Wan", "Liang Han", "Yaowei Wang", "Liqiang Nie", "Min Zhang"], "title": "CountDiffusion: Text-to-Image Synthesis with Training-Free Counting-Guidance Diffusion", "categories": ["cs.CV"], "comment": "8 pages, 9 figures, 3 tables", "summary": "Stable Diffusion has advanced text-to-image synthesis, but training models to\ngenerate images with accurate object quantity is still difficult due to the\nhigh computational cost and the challenge of teaching models the abstract\nconcept of quantity. In this paper, we propose CountDiffusion, a training-free\nframework aiming at generating images with correct object quantity from textual\ndescriptions. CountDiffusion consists of two stages. In the first stage, an\nintermediate denoising result is generated by the diffusion model to predict\nthe final synthesized image with one-step denoising, and a counting model is\nused to count the number of objects in this image. In the second stage, a\ncorrection module is used to correct the object quantity by changing the\nattention map of the object with universal guidance. The proposed\nCountDiffusion can be plugged into any diffusion-based text-to-image (T2I)\ngeneration models without further training. Experiment results demonstrate the\nsuperiority of our proposed CountDiffusion, which improves the accurate object\nquantity generation ability of T2I models by a large margin."}
{"id": "2505.04369", "pdf": "https://arxiv.org/pdf/2505.04369", "abs": "https://arxiv.org/abs/2505.04369", "authors": ["Jie Sun", "Heng Liu", "Yongzhen Wang", "Xiao-Ping Zhang", "Mingqiang Wei"], "title": "WDMamba: When Wavelet Degradation Prior Meets Vision Mamba for Image Dehazing", "categories": ["cs.CV"], "comment": null, "summary": "In this paper, we reveal a novel haze-specific wavelet degradation prior\nobserved through wavelet transform analysis, which shows that haze-related\ninformation predominantly resides in low-frequency components. Exploiting this\ninsight, we propose a novel dehazing framework, WDMamba, which decomposes the\nimage dehazing task into two sequential stages: low-frequency restoration\nfollowed by detail enhancement. This coarse-to-fine strategy enables WDMamba to\neffectively capture features specific to each stage of the dehazing process,\nresulting in high-quality restored images. Specifically, in the low-frequency\nrestoration stage, we integrate Mamba blocks to reconstruct global structures\nwith linear complexity, efficiently removing overall haze and producing a\ncoarse restored image. Thereafter, the detail enhancement stage reinstates\nfine-grained information that may have been overlooked during the previous\nphase, culminating in the final dehazed output. Furthermore, to enhance detail\nretention and achieve more natural dehazing, we introduce a self-guided\ncontrastive regularization during network training. By utilizing the coarse\nrestored output as a hard negative example, our model learns more\ndiscriminative representations, substantially boosting the overall dehazing\nperformance. Extensive evaluations on public dehazing benchmarks demonstrate\nthat our method surpasses state-of-the-art approaches both qualitatively and\nquantitatively. Code is available at https://github.com/SunJ000/WDMamba."}
{"id": "2505.04375", "pdf": "https://arxiv.org/pdf/2505.04375", "abs": "https://arxiv.org/abs/2505.04375", "authors": ["Moseli Mots'oehli", "Hope Mogale", "Kyungim Baek"], "title": "Balancing Accuracy, Calibration, and Efficiency in Active Learning with Vision Transformers Under Label Noise", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Fine-tuning pre-trained convolutional neural networks on ImageNet for\ndownstream tasks is well-established. Still, the impact of model size on the\nperformance of vision transformers in similar scenarios, particularly under\nlabel noise, remains largely unexplored. Given the utility and versatility of\ntransformer architectures, this study investigates their practicality under\nlow-budget constraints and noisy labels. We explore how classification accuracy\nand calibration are affected by symmetric label noise in active learning\nsettings, evaluating four vision transformer configurations (Base and Large\nwith 16x16 and 32x32 patch sizes) and three Swin Transformer configurations\n(Tiny, Small, and Base) on CIFAR10 and CIFAR100 datasets, under varying label\nnoise rates. Our findings show that larger ViT models (ViTl32 in particular)\nconsistently outperform their smaller counterparts in both accuracy and\ncalibration, even under moderate to high label noise, while Swin Transformers\nexhibit weaker robustness across all noise levels. We find that smaller patch\nsizes do not always lead to better performance, as ViTl16 performs consistently\nworse than ViTl32 while incurring a higher computational cost. We also find\nthat information-based Active Learning strategies only provide meaningful\naccuracy improvements at moderate label noise rates, but they result in poorer\ncalibration compared to models trained on randomly acquired labels, especially\nat high label noise rates. We hope these insights provide actionable guidance\nfor practitioners looking to deploy vision transformers in resource-constrained\nenvironments, where balancing model complexity, label noise, and compute\nefficiency is critical in model fine-tuning or distillation."}
{"id": "2505.04376", "pdf": "https://arxiv.org/pdf/2505.04376", "abs": "https://arxiv.org/abs/2505.04376", "authors": ["Zili Zhang", "Ziting Wen", "Yiheng Qiang", "Hongzhou Dong", "Wenle Dong", "Xinyang Li", "Xiaofan Wang", "Xiaoqiang Ren"], "title": "Label-efficient Single Photon Images Classification via Active Learning", "categories": ["cs.CV"], "comment": null, "summary": "Single-photon LiDAR achieves high-precision 3D imaging in extreme\nenvironments through quantum-level photon detection technology. Current\nresearch primarily focuses on reconstructing 3D scenes from sparse photon\nevents, whereas the semantic interpretation of single-photon images remains\nunderexplored, due to high annotation costs and inefficient labeling\nstrategies. This paper presents the first active learning framework for\nsingle-photon image classification. The core contribution is an imaging\ncondition-aware sampling strategy that integrates synthetic augmentation to\nmodel variability across imaging conditions. By identifying samples where the\nmodel is both uncertain and sensitive to these conditions, the proposed method\nselectively annotates only the most informative examples. Experiments on both\nsynthetic and real-world datasets show that our approach outperforms all\nbaselines and achieves high classification accuracy with significantly fewer\nlabeled samples. Specifically, our approach achieves 97% accuracy on synthetic\nsingle-photon data using only 1.5% labeled samples. On real-world data, we\nmaintain 90.63% accuracy with just 8% labeled samples, which is 4.51% higher\nthan the best-performing baseline. This illustrates that active learning\nenables the same level of classification performance on single-photon images as\non classical images, opening doors to large-scale integration of single-photon\ndata in real-world applications."}
{"id": "2505.04380", "pdf": "https://arxiv.org/pdf/2505.04380", "abs": "https://arxiv.org/abs/2505.04380", "authors": ["Jinhai Xiang", "Shuai Guo", "Qianru Han", "Dantong Shi", "Xinwei He", "Xiang Bai"], "title": "Tetrahedron-Net for Medical Image Registration", "categories": ["cs.CV", "cs.IR"], "comment": null, "summary": "Medical image registration plays a vital role in medical image processing.\nExtracting expressive representations for medical images is crucial for\nimproving the registration quality. One common practice for this end is\nconstructing a convolutional backbone to enable interactions with skip\nconnections among feature extraction layers. The de facto structure, U-Net-like\nnetworks, has attempted to design skip connections such as nested or full-scale\nones to connect one single encoder and one single decoder to improve its\nrepresentation capacity. Despite being effective, it still does not fully\nexplore interactions with a single encoder and decoder architectures. In this\npaper, we embrace this observation and introduce a simple yet effective\nalternative strategy to enhance the representations for registrations by\nappending one additional decoder. The new decoder is designed to interact with\nboth the original encoder and decoder. In this way, it not only reuses feature\npresentation from corresponding layers in the encoder but also interacts with\nthe original decoder to corporately give more accurate registration results.\nThe new architecture is concise yet generalized, with only one encoder and two\ndecoders forming a ``Tetrahedron'' structure, thereby dubbed Tetrahedron-Net.\nThree instantiations of Tetrahedron-Net are further constructed regarding the\ndifferent structures of the appended decoder. Our extensive experiments prove\nthat superior performance can be obtained on several representative benchmarks\nof medical image registration. Finally, such a ``Tetrahedron'' design can also\nbe easily integrated into popular U-Net-like architectures including\nVoxelMorph, ViT-V-Net, and TransMorph, leading to consistent performance gains."}
{"id": "2505.04384", "pdf": "https://arxiv.org/pdf/2505.04384", "abs": "https://arxiv.org/abs/2505.04384", "authors": ["Ming-Hui Liu", "Xiao-Qian Liu", "Xin Luo", "Xin-Shun Xu"], "title": "DATA: Multi-Disentanglement based Contrastive Learning for Open-World Semi-Supervised Deepfake Attribution", "categories": ["cs.CV"], "comment": "Accepted by IEEE TMM on 17-Jan-2025; Submitted to IEEE TMM on\n  11-Jul-2024", "summary": "Deepfake attribution (DFA) aims to perform multiclassification on different\nfacial manipulation techniques, thereby mitigating the detrimental effects of\nforgery content on the social order and personal reputations. However, previous\nmethods focus only on method-specific clues, which easily lead to overfitting,\nwhile overlooking the crucial role of common forgery features. Additionally,\nthey struggle to distinguish between uncertain novel classes in more practical\nopen-world scenarios. To address these issues, in this paper we propose an\ninnovative multi-DisentAnglement based conTrastive leArning framework, DATA, to\nenhance the generalization ability on novel classes for the open-world\nsemi-supervised deepfake attribution (OSS-DFA) task. Specifically, since all\ngeneration techniques can be abstracted into a similar architecture, DATA\ndefines the concept of 'Orthonormal Deepfake Basis' for the first time and\nutilizes it to disentangle method-specific features, thereby reducing the\noverfitting on forgery-irrelevant information. Furthermore, an augmented-memory\nmechanism is designed to assist in novel class discovery and contrastive\nlearning, which aims to obtain clear class boundaries for the novel classes\nthrough instance-level disentanglements. Additionally, to enhance the\nstandardization and discrimination of features, DATA uses bases contrastive\nloss and center contrastive loss as auxiliaries for the aforementioned modules.\nExtensive experimental evaluations show that DATA achieves state-of-the-art\nperformance on the OSS-DFA benchmark, e.g., there are notable accuracy\nimprovements in 2.55% / 5.7% under different settings, compared with the\nexisting methods."}
{"id": "2505.04392", "pdf": "https://arxiv.org/pdf/2505.04392", "abs": "https://arxiv.org/abs/2505.04392", "authors": ["Petr Jahoda", "Jan Cech"], "title": "Predicting Road Surface Anomalies by Visual Tracking of a Preceding Vehicle", "categories": ["cs.CV"], "comment": "Accepted to the IEEE Intelligent Vehicles Symposium (IV), 2025", "summary": "A novel approach to detect road surface anomalies by visual tracking of a\npreceding vehicle is proposed. The method is versatile, predicting any kind of\nroad anomalies, such as potholes, bumps, debris, etc., unlike direct\nobservation methods that rely on training visual detectors of those cases. The\nmethod operates in low visibility conditions or in dense traffic where the\nanomaly is occluded by a preceding vehicle. Anomalies are detected\npredictively, i.e., before a vehicle encounters them, which allows to\npre-configure low-level vehicle systems (such as chassis) or to plan an\navoidance maneuver in case of autonomous driving. A challenge is that the\nsignal coming from camera-based tracking of a preceding vehicle may be weak and\ndisturbed by camera ego motion due to vibrations affecting the ego vehicle.\nTherefore, we propose an efficient method to compensate camera pitch rotation\nby an iterative robust estimator. Our experiments on both controlled setup and\nnormal traffic conditions show that road anomalies can be detected reliably at\na distance even in challenging cases where the ego vehicle traverses imperfect\nroad surfaces. The method is effective and performs in real time on standard\nconsumer hardware."}
{"id": "2505.04394", "pdf": "https://arxiv.org/pdf/2505.04394", "abs": "https://arxiv.org/abs/2505.04394", "authors": ["Young-Hu Park", "Rae-Hong Park", "Hyung-Min Park"], "title": "SwinLip: An Efficient Visual Speech Encoder for Lip Reading Using Swin Transformer", "categories": ["cs.CV", "eess.AS"], "comment": null, "summary": "This paper presents an efficient visual speech encoder for lip reading. While\nmost recent lip reading studies have been based on the ResNet architecture and\nhave achieved significant success, they are not sufficiently suitable for\nefficiently capturing lip reading features due to high computational complexity\nin modeling spatio-temporal information. Additionally, using a complex visual\nmodel not only increases the complexity of lip reading models but also induces\ndelays in the overall network for multi-modal studies (e.g., audio-visual\nspeech recognition, speech enhancement, and speech separation). To overcome the\nlimitations of Convolutional Neural Network (CNN)-based models, we apply the\nhierarchical structure and window self-attention of the Swin Transformer to lip\nreading. We configure a new lightweight scale of the Swin Transformer suitable\nfor processing lip reading data and present the SwinLip visual speech encoder,\nwhich efficiently reduces computational load by integrating modified\nConvolution-augmented Transformer (Conformer) temporal embeddings with\nconventional spatial embeddings in the hierarchical structure. Through\nextensive experiments, we have validated that our SwinLip successfully improves\nthe performance and inference speed of the lip reading network when applied to\nvarious backbones for word and sentence recognition, reducing computational\nload. In particular, our SwinLip demonstrated robust performance in both\nEnglish LRW and Mandarin LRW-1000 datasets and achieved state-of-the-art\nperformance on the Mandarin LRW-1000 dataset with less computation compared to\nthe existing state-of-the-art model."}
{"id": "2505.04397", "pdf": "https://arxiv.org/pdf/2505.04397", "abs": "https://arxiv.org/abs/2505.04397", "authors": ["Ziyuan Li", "Uwe Jaekel", "Babette Dellen"], "title": "Deep residual learning with product units", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "We propose a deep product-unit residual neural network (PURe) that integrates\nproduct units into residual blocks to improve the expressiveness and parameter\nefficiency of deep convolutional networks. Unlike standard summation neurons,\nproduct units enable multiplicative feature interactions, potentially offering\na more powerful representation of complex patterns. PURe replaces conventional\nconvolutional layers with 2D product units in the second layer of each residual\nblock, eliminating nonlinear activation functions to preserve structural\ninformation. We validate PURe on three benchmark datasets. On Galaxy10 DECaLS,\nPURe34 achieves the highest test accuracy of 84.89%, surpassing the much deeper\nResNet152, while converging nearly five times faster and demonstrating strong\nrobustness to Poisson noise. On ImageNet, PURe architectures outperform\nstandard ResNet models at similar depths, with PURe34 achieving a top-1\naccuracy of 80.27% and top-5 accuracy of 95.78%, surpassing deeper ResNet\nvariants (ResNet50, ResNet101) while utilizing significantly fewer parameters\nand computational resources. On CIFAR-10, PURe consistently outperforms ResNet\nvariants across varying depths, with PURe272 reaching 95.01% test accuracy,\ncomparable to ResNet1001 but at less than half the model size. These results\ndemonstrate that PURe achieves a favorable balance between accuracy,\nefficiency, and robustness. Compared to traditional residual networks, PURe not\nonly achieves competitive classification performance with faster convergence\nand fewer parameters, but also demonstrates greater robustness to noise. Its\neffectiveness across diverse datasets highlights the potential of\nproduct-unit-based architectures for scalable and reliable deep learning in\ncomputer vision."}
{"id": "2505.04408", "pdf": "https://arxiv.org/pdf/2505.04408", "abs": "https://arxiv.org/abs/2505.04408", "authors": ["Chengjie Huang", "Krzysztof Czarnecki"], "title": "MFSeg: Efficient Multi-frame 3D Semantic Segmentation", "categories": ["cs.CV"], "comment": "ICRA 2025", "summary": "We propose MFSeg, an efficient multi-frame 3D semantic segmentation\nframework. By aggregating point cloud sequences at the feature level and\nregularizing the feature extraction and aggregation process, MFSeg reduces\ncomputational overhead while maintaining high accuracy. Moreover, by employing\na lightweight MLP-based point decoder, our method eliminates the need to\nupsample redundant points from past frames. Experiments on the nuScenes and\nWaymo datasets show that MFSeg outperforms existing methods, demonstrating its\neffectiveness and efficiency."}
{"id": "2505.04410", "pdf": "https://arxiv.org/pdf/2505.04410", "abs": "https://arxiv.org/abs/2505.04410", "authors": ["Junjie Wang", "Bin Chen", "Yulin Li", "Bin Kang", "Yichi Chen", "Zhuotao Tian"], "title": "DeCLIP: Decoupled Learning for Open-Vocabulary Dense Perception", "categories": ["cs.CV"], "comment": null, "summary": "Dense visual prediction tasks have been constrained by their reliance on\npredefined categories, limiting their applicability in real-world scenarios\nwhere visual concepts are unbounded. While Vision-Language Models (VLMs) like\nCLIP have shown promise in open-vocabulary tasks, their direct application to\ndense prediction often leads to suboptimal performance due to limitations in\nlocal feature representation. In this work, we present our observation that\nCLIP's image tokens struggle to effectively aggregate information from\nspatially or semantically related regions, resulting in features that lack\nlocal discriminability and spatial consistency. To address this issue, we\npropose DeCLIP, a novel framework that enhances CLIP by decoupling the\nself-attention module to obtain ``content'' and ``context'' features\nrespectively. The ``content'' features are aligned with image crop\nrepresentations to improve local discriminability, while ``context'' features\nlearn to retain the spatial correlations under the guidance of vision\nfoundation models, such as DINO. Extensive experiments demonstrate that DeCLIP\nsignificantly outperforms existing methods across multiple open-vocabulary\ndense prediction tasks, including object detection and semantic segmentation.\nCode is available at \\textcolor{magenta}{https://github.com/xiaomoguhz/DeCLIP}."}
{"id": "2505.04424", "pdf": "https://arxiv.org/pdf/2505.04424", "abs": "https://arxiv.org/abs/2505.04424", "authors": ["Jing Hu", "Chengming Feng", "Shu Hu", "Ming-Ching Chang", "Xin Li", "Xi Wu", "Xin Wang"], "title": "RLMiniStyler: Light-weight RL Style Agent for Arbitrary Sequential Neural Style Generation", "categories": ["cs.CV"], "comment": "IJCAI2025", "summary": "Arbitrary style transfer aims to apply the style of any given artistic image\nto another content image. Still, existing deep learning-based methods often\nrequire significant computational costs to generate diverse stylized results.\nMotivated by this, we propose a novel reinforcement learning-based framework\nfor arbitrary style transfer RLMiniStyler. This framework leverages a unified\nreinforcement learning policy to iteratively guide the style transfer process\nby exploring and exploiting stylization feedback, generating smooth sequences\nof stylized results while achieving model lightweight. Furthermore, we\nintroduce an uncertainty-aware multi-task learning strategy that automatically\nadjusts loss weights to adapt to the content and style balance requirements at\ndifferent training stages, thereby accelerating model convergence. Through a\nseries of experiments across image various resolutions, we have validated the\nadvantages of RLMiniStyler over other state-of-the-art methods in generating\nhigh-quality, diverse artistic image sequences at a lower cost. Codes are\navailable at https://github.com/fengxiaoming520/RLMiniStyler."}
{"id": "2505.04460", "pdf": "https://arxiv.org/pdf/2505.04460", "abs": "https://arxiv.org/abs/2505.04460", "authors": ["Ming-Hui Liu", "Harry Cheng", "Tianyi Wang", "Xin Luo", "Xin-Shun Xu"], "title": "Learning Real Facial Concepts for Independent Deepfake Detection", "categories": ["cs.CV"], "comment": "Accepted by IJCAI 2025", "summary": "Deepfake detection models often struggle with generalization to unseen\ndatasets, manifesting as misclassifying real instances as fake in target\ndomains. This is primarily due to an overreliance on forgery artifacts and a\nlimited understanding of real faces. To address this challenge, we propose a\nnovel approach RealID to enhance generalization by learning a comprehensive\nconcept of real faces while assessing the probabilities of belonging to the\nreal and fake classes independently. RealID comprises two key modules: the Real\nConcept Capture Module (RealC2) and the Independent Dual-Decision Classifier\n(IDC). With the assistance of a MultiReal Memory, RealC2 maintains various\nprototypes for real faces, allowing the model to capture a comprehensive\nconcept of real class. Meanwhile, IDC redefines the classification strategy by\nmaking independent decisions based on the concept of the real class and the\npresence of forgery artifacts. Through the combined effect of the above\nmodules, the influence of forgery-irrelevant patterns is alleviated, and\nextensive experiments on five widely used datasets demonstrate that RealID\nsignificantly outperforms existing state-of-the-art methods, achieving a 1.74%\nimprovement in average accuracy."}
{"id": "2505.04481", "pdf": "https://arxiv.org/pdf/2505.04481", "abs": "https://arxiv.org/abs/2505.04481", "authors": ["Jiahao Li", "Weijian Ma", "Xueyang Li", "Yunzhong Lou", "Guichun Zhou", "Xiangdong Zhou"], "title": "CAD-Llama: Leveraging Large Language Models for Computer-Aided Design Parametric 3D Model Generation", "categories": ["cs.CV"], "comment": null, "summary": "Recently, Large Language Models (LLMs) have achieved significant success,\nprompting increased interest in expanding their generative capabilities beyond\ngeneral text into domain-specific areas. This study investigates the generation\nof parametric sequences for computer-aided design (CAD) models using LLMs. This\nendeavor represents an initial step towards creating parametric 3D shapes with\nLLMs, as CAD model parameters directly correlate with shapes in\nthree-dimensional space. Despite the formidable generative capacities of LLMs,\nthis task remains challenging, as these models neither encounter parametric\nsequences during their pretraining phase nor possess direct awareness of 3D\nstructures. To address this, we present CAD-Llama, a framework designed to\nenhance pretrained LLMs for generating parametric 3D CAD models. Specifically,\nwe develop a hierarchical annotation pipeline and a code-like format to\ntranslate parametric 3D CAD command sequences into Structured Parametric CAD\nCode (SPCC), incorporating hierarchical semantic descriptions. Furthermore, we\npropose an adaptive pretraining approach utilizing SPCC, followed by an\ninstruction tuning process aligned with CAD-specific guidelines. This\nmethodology aims to equip LLMs with the spatial knowledge inherent in\nparametric sequences. Experimental results demonstrate that our framework\nsignificantly outperforms prior autoregressive methods and existing LLM\nbaselines."}
{"id": "2505.04485", "pdf": "https://arxiv.org/pdf/2505.04485", "abs": "https://arxiv.org/abs/2505.04485", "authors": ["Ali Alawieh", "Alexandru P. Condurache"], "title": "FA-KPConv: Introducing Euclidean Symmetries to KPConv via Frame Averaging", "categories": ["cs.CV"], "comment": "8 pages, 2 figures, accepted at IJCNN 2025", "summary": "We present Frame-Averaging Kernel-Point Convolution (FA-KPConv), a neural\nnetwork architecture built on top of the well-known KPConv, a widely adopted\nbackbone for 3D point cloud analysis. Even though invariance and/or\nequivariance to Euclidean transformations are required for many common tasks,\nKPConv-based networks can only approximately achieve such properties when\ntraining on large datasets or with significant data augmentations. Using Frame\nAveraging, we allow to flexibly customize point cloud neural networks built\nwith KPConv layers, by making them exactly invariant and/or equivariant to\ntranslations, rotations and/or reflections of the input point clouds. By simply\nwrapping around an existing KPConv-based network, FA-KPConv embeds geometrical\nprior knowledge into it while preserving the number of learnable parameters and\nnot compromising any input information. We showcase the benefit of such an\nintroduced bias for point cloud classification and point cloud registration,\nespecially in challenging cases such as scarce training data or randomly\nrotated test data."}
{"id": "2505.04486", "pdf": "https://arxiv.org/pdf/2505.04486", "abs": "https://arxiv.org/abs/2505.04486", "authors": ["Anirban Samaddar", "Yixuan Sun", "Viktor Nilsson", "Sandeep Madireddy"], "title": "Efficient Flow Matching using Latent Variables", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Flow matching models have shown great potential in image generation tasks\namong probabilistic generative models. Building upon the ideas of continuous\nnormalizing flows, flow matching models generalize the transport path of the\ndiffusion models from a simple prior distribution to the data. Most flow\nmatching models in the literature do not explicitly model the underlying\nstructure/manifold in the target data when learning the flow from a simple\nsource distribution like the standard Gaussian. This leads to inefficient\nlearning, especially for many high-dimensional real-world datasets, which often\nreside in a low-dimensional manifold. Existing strategies of incorporating\nmanifolds, including data with underlying multi-modal distribution, often\nrequire expensive training and hence frequently lead to suboptimal performance.\nTo this end, we present \\texttt{Latent-CFM}, which provides simplified\ntraining/inference strategies to incorporate multi-modal data structures using\npretrained deep latent variable models. Through experiments on multi-modal\nsynthetic data and widely used image benchmark datasets, we show that\n\\texttt{Latent-CFM} exhibits improved generation quality with significantly\nless training ($\\sim 50\\%$ less in some cases) and computation than\nstate-of-the-art flow matching models. Using a 2d Darcy flow dataset, we\ndemonstrate that our approach generates more physically accurate samples than\ncompetitive approaches. In addition, through latent space analysis, we\ndemonstrate that our approach can be used for conditional image generation\nconditioned on latent features."}
{"id": "2505.04488", "pdf": "https://arxiv.org/pdf/2505.04488", "abs": "https://arxiv.org/abs/2505.04488", "authors": ["Ziyi Zhang", "Zhen Sun", "Zongmin Zhang", "Zifan Peng", "Yuemeng Zhao", "Zichun Wang", "Zeren Luo", "Ruiting Zuo", "Xinlei He"], "title": "\"I Can See Forever!\": Evaluating Real-time VideoLLMs for Assisting Individuals with Visual Impairments", "categories": ["cs.CV", "cs.AI", "cs.HC", "cs.MM"], "comment": "12 pages, 6 figures", "summary": "The visually impaired population, especially the severely visually impaired,\nis currently large in scale, and daily activities pose significant challenges\nfor them. Although many studies use large language and vision-language models\nto assist the blind, most focus on static content and fail to meet real-time\nperception needs in dynamic and complex environments, such as daily activities.\nTo provide them with more effective intelligent assistance, it is imperative to\nincorporate advanced visual understanding technologies. Although real-time\nvision and speech interaction VideoLLMs demonstrate strong real-time visual\nunderstanding, no prior work has systematically evaluated their effectiveness\nin assisting visually impaired individuals. In this work, we conduct the first\nsuch evaluation. First, we construct a benchmark dataset (VisAssistDaily),\ncovering three categories of assistive tasks for visually impaired individuals:\nBasic Skills, Home Life Tasks, and Social Life Tasks. The results show that\nGPT-4o achieves the highest task success rate. Next, we conduct a user study to\nevaluate the models in both closed-world and open-world scenarios, further\nexploring the practical challenges of applying VideoLLMs in assistive contexts.\nOne key issue we identify is the difficulty current models face in perceiving\npotential hazards in dynamic environments. To address this, we build an\nenvironment-awareness dataset named SafeVid and introduce a polling mechanism\nthat enables the model to proactively detect environmental risks. We hope this\nwork provides valuable insights and inspiration for future research in this\nfield."}
{"id": "2505.04497", "pdf": "https://arxiv.org/pdf/2505.04497", "abs": "https://arxiv.org/abs/2505.04497", "authors": ["Aditi Ramaswamy"], "title": "Defining and Quantifying Creative Behavior in Popular Image Generators", "categories": ["cs.CV", "cs.AI", "I.4.m; I.2.m"], "comment": null, "summary": "Creativity of generative AI models has been a subject of scientific debate in\nthe last years, without a conclusive answer. In this paper, we study creativity\nfrom a practical perspective and introduce quantitative measures that help the\nuser to choose a suitable AI model for a given task. We evaluated our measures\non a number of popular image-to-image generation models, and the results of\nthis suggest that our measures conform to human intuition."}
{"id": "2505.04502", "pdf": "https://arxiv.org/pdf/2505.04502", "abs": "https://arxiv.org/abs/2505.04502", "authors": ["Asma Baobaid", "Mahmoud Meribout"], "title": "Leveraging Simultaneous Usage of Edge GPU Hardware Engines for Video Face Detection and Recognition", "categories": ["cs.CV", "cs.AR", "eess.IV"], "comment": "10 pages, 11 figures", "summary": "Video face detection and recognition in public places at the edge is required\nin several applications, such as security reinforcement and contactless access\nto authorized venues. This paper aims to maximize the simultaneous usage of\nhardware engines available in edge GPUs nowadays by leveraging the concurrency\nand pipelining of tasks required for face detection and recognition. This also\nincludes the video decoding task, which is required in most face monitoring\napplications as the video streams are usually carried via Gbps Ethernet\nnetwork. This constitutes an improvement over previous works where the tasks\nare usually allocated to a single engine due to the lack of a unified and\nautomated framework that simultaneously explores all hardware engines. In\naddition, previously, the input faces were usually embedded in still images or\nwithin raw video streams that overlook the burst delay caused by the decoding\nstage. The results on real-life video streams suggest that simultaneously using\nall the hardware engines available in the recent NVIDIA edge Orin GPU, higher\nthroughput, and a slight saving of power consumption of around 300 mW,\naccounting for around 5%, have been achieved while satisfying the real-time\nperformance constraint. The performance gets even higher by considering several\nvideo streams simultaneously. Further performance improvement could have been\nobtained if the number of shuffle layers that were created by the tensor RT\nframework for the face recognition task was lower. Thus, the paper suggests\nsome hardware improvements to the existing edge GPU processors to enhance their\nperformance even higher."}
{"id": "2505.04512", "pdf": "https://arxiv.org/pdf/2505.04512", "abs": "https://arxiv.org/abs/2505.04512", "authors": ["Teng Hu", "Zhentao Yu", "Zhengguang Zhou", "Sen Liang", "Yuan Zhou", "Qin Lin", "Qinglin Lu"], "title": "HunyuanCustom: A Multimodal-Driven Architecture for Customized Video Generation", "categories": ["cs.CV"], "comment": null, "summary": "Customized video generation aims to produce videos featuring specific\nsubjects under flexible user-defined conditions, yet existing methods often\nstruggle with identity consistency and limited input modalities. In this paper,\nwe propose HunyuanCustom, a multi-modal customized video generation framework\nthat emphasizes subject consistency while supporting image, audio, video, and\ntext conditions. Built upon HunyuanVideo, our model first addresses the\nimage-text conditioned generation task by introducing a text-image fusion\nmodule based on LLaVA for enhanced multi-modal understanding, along with an\nimage ID enhancement module that leverages temporal concatenation to reinforce\nidentity features across frames. To enable audio- and video-conditioned\ngeneration, we further propose modality-specific condition injection\nmechanisms: an AudioNet module that achieves hierarchical alignment via spatial\ncross-attention, and a video-driven injection module that integrates\nlatent-compressed conditional video through a patchify-based feature-alignment\nnetwork. Extensive experiments on single- and multi-subject scenarios\ndemonstrate that HunyuanCustom significantly outperforms state-of-the-art open-\nand closed-source methods in terms of ID consistency, realism, and text-video\nalignment. Moreover, we validate its robustness across downstream tasks,\nincluding audio and video-driven customized video generation. Our results\nhighlight the effectiveness of multi-modal conditioning and identity-preserving\nstrategies in advancing controllable video generation. All the code and models\nare available at https://hunyuancustom.github.io."}
{"id": "2505.04522", "pdf": "https://arxiv.org/pdf/2505.04522", "abs": "https://arxiv.org/abs/2505.04522", "authors": ["Pengfei Guo", "Can Zhao", "Dong Yang", "Yufan He", "Vishwesh Nath", "Ziyue Xu", "Pedro R. A. S. Bassi", "Zongwei Zhou", "Benjamin D. Simon", "Stephanie Anne Harmon", "Baris Turkbey", "Daguang Xu"], "title": "Text2CT: Towards 3D CT Volume Generation from Free-text Descriptions Using Diffusion Model", "categories": ["cs.CV"], "comment": null, "summary": "Generating 3D CT volumes from descriptive free-text inputs presents a\ntransformative opportunity in diagnostics and research. In this paper, we\nintroduce Text2CT, a novel approach for synthesizing 3D CT volumes from textual\ndescriptions using the diffusion model. Unlike previous methods that rely on\nfixed-format text input, Text2CT employs a novel prompt formulation that\nenables generation from diverse, free-text descriptions. The proposed framework\nencodes medical text into latent representations and decodes them into\nhigh-resolution 3D CT scans, effectively bridging the gap between semantic text\ninputs and detailed volumetric representations in a unified 3D framework. Our\nmethod demonstrates superior performance in preserving anatomical fidelity and\ncapturing intricate structures as described in the input text. Extensive\nevaluations show that our approach achieves state-of-the-art results, offering\npromising potential applications in diagnostics, and data augmentation."}
{"id": "2505.04524", "pdf": "https://arxiv.org/pdf/2505.04524", "abs": "https://arxiv.org/abs/2505.04524", "authors": ["Asma Baobaid", "Mahmoud Meribout"], "title": "Edge-GPU Based Face Tracking for Face Detection and Recognition Acceleration", "categories": ["cs.CV", "cs.AR", "cs.LG", "eess.IV"], "comment": "10 pages, 12 figures", "summary": "Cost-effective machine vision systems dedicated to real-time and accurate\nface detection and recognition in public places are crucial for many modern\napplications. However, despite their high performance, which could be reached\nusing specialized edge or cloud AI hardware accelerators, there is still room\nfor improvement in throughput and power consumption. This paper aims to suggest\na combined hardware-software approach that optimizes face detection and\nrecognition systems on one of the latest edge GPUs, namely NVIDIA Jetson AGX\nOrin. First, it leverages the simultaneous usage of all its hardware engines to\nimprove processing time. This offers an improvement over previous works where\nthese tasks were mainly allocated automatically and exclusively to the CPU or,\nto a higher extent, to the GPU core. Additionally, the paper suggests\nintegrating a face tracker module to avoid redundantly running the face\nrecognition algorithm for every frame but only when a new face appears in the\nscene. The results of extended experiments suggest that simultaneous usage of\nall the hardware engines that are available in the Orin GPU and tracker\nintegration into the pipeline yield an impressive throughput of 290 FPS (frames\nper second) on 1920 x 1080 input size frames containing in average of 6\nfaces/frame. Additionally, a substantial saving of power consumption of around\n800 mW was achieved when compared to running the task on the CPU/GPU engines\nonly and without integrating a tracker into the Orin GPU\\'92s pipeline. This\nhardware-codesign approach can pave the way to design high-performance machine\nvision systems at the edge, critically needed in video monitoring in public\nplaces where several nearby cameras are usually deployed for a same scene."}
{"id": "2505.04526", "pdf": "https://arxiv.org/pdf/2505.04526", "abs": "https://arxiv.org/abs/2505.04526", "authors": ["Qi Zhou", "Yukai Shi", "Xiaojun Yang", "Xiaoyu Xian", "Lunjia Liao", "Ruimao Zhang", "Liang Lin"], "title": "DFVO: Learning Darkness-free Visible and Infrared Image Disentanglement and Fusion All at Once", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Visible and infrared image fusion is one of the most crucial tasks in the\nfield of image fusion, aiming to generate fused images with clear structural\ninformation and high-quality texture features for high-level vision tasks.\nHowever, when faced with severe illumination degradation in visible images, the\nfusion results of existing image fusion methods often exhibit blurry and dim\nvisual effects, posing major challenges for autonomous driving. To this end, a\nDarkness-Free network is proposed to handle Visible and infrared image\ndisentanglement and fusion all at Once (DFVO), which employs a cascaded\nmulti-task approach to replace the traditional two-stage cascaded training\n(enhancement and fusion), addressing the issue of information entropy loss\ncaused by hierarchical data transmission. Specifically, we construct a\nlatent-common feature extractor (LCFE) to obtain latent features for the\ncascaded tasks strategy. Firstly, a details-extraction module (DEM) is devised\nto acquire high-frequency semantic information. Secondly, we design a hyper\ncross-attention module (HCAM) to extract low-frequency information and preserve\ntexture features from source images. Finally, a relevant loss function is\ndesigned to guide the holistic network learning, thereby achieving better image\nfusion. Extensive experiments demonstrate that our proposed approach\noutperforms state-of-the-art alternatives in terms of qualitative and\nquantitative evaluations. Particularly, DFVO can generate clearer, more\ninformative, and more evenly illuminated fusion results in the dark\nenvironments, achieving best performance on the LLVIP dataset with 63.258 dB\nPSNR and 0.724 CC, providing more effective information for high-level vision\ntasks. Our code is publicly accessible at https://github.com/DaVin-Qi530/DFVO."}
{"id": "2505.04529", "pdf": "https://arxiv.org/pdf/2505.04529", "abs": "https://arxiv.org/abs/2505.04529", "authors": ["Edward Humes", "Xiaomin Lin", "Uttej Kallakuri", "Tinoosh Mohsenin"], "title": "RAFT: Robust Augmentation of FeaTures for Image Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Image segmentation is a powerful computer vision technique for scene\nunderstanding. However, real-world deployment is stymied by the need for\nhigh-quality, meticulously labeled datasets. Synthetic data provides\nhigh-quality labels while reducing the need for manual data collection and\nannotation. However, deep neural networks trained on synthetic data often face\nthe Syn2Real problem, leading to poor performance in real-world deployments.\n  To mitigate the aforementioned gap in image segmentation, we propose RAFT, a\nnovel framework for adapting image segmentation models using minimal labeled\nreal-world data through data and feature augmentations, as well as active\nlearning. To validate RAFT, we perform experiments on the synthetic-to-real\n\"SYNTHIA->Cityscapes\" and \"GTAV->Cityscapes\" benchmarks. We managed to surpass\nthe previous state of the art, HALO. SYNTHIA->Cityscapes experiences an\nimprovement in mIoU* upon domain adaptation of 2.1%/79.9%, and GTAV->Cityscapes\nexperiences a 0.4%/78.2% improvement in mIoU. Furthermore, we test our approach\non the real-to-real benchmark of \"Cityscapes->ACDC\", and again surpass HALO,\nwith a gain in mIoU upon adaptation of 1.3%/73.2%. Finally, we examine the\neffect of the allocated annotation budget and various components of RAFT upon\nthe final transfer mIoU."}
{"id": "2505.04540", "pdf": "https://arxiv.org/pdf/2505.04540", "abs": "https://arxiv.org/abs/2505.04540", "authors": ["Ashutosh Singandhupe", "Sanket Lokhande", "Hung Manh La"], "title": "Registration of 3D Point Sets Using Exponential-based Similarity Matrix", "categories": ["cs.CV"], "comment": null, "summary": "Point cloud registration is a fundamental problem in computer vision and\nrobotics, involving the alignment of 3D point sets captured from varying\nviewpoints using depth sensors such as LiDAR or structured light. In modern\nrobotic systems, especially those focused on mapping, it is essential to merge\nmultiple views of the same environment accurately. However, state-of-the-art\nregistration techniques often struggle when large rotational differences exist\nbetween point sets or when the data is significantly corrupted by sensor noise.\nThese challenges can lead to misalignments and, consequently, to inaccurate or\ndistorted 3D reconstructions. In this work, we address both these limitations\nby proposing a robust modification to the classic Iterative Closest Point (ICP)\nalgorithm. Our method, termed Exponential Similarity Matrix ICP (ESM-ICP),\nintegrates a Gaussian-inspired exponential weighting scheme to construct a\nsimilarity matrix that dynamically adapts across iterations. This matrix\nfacilitates improved estimation of both rotational and translational components\nduring alignment. We demonstrate the robustness of ESM-ICP in two challenging\nscenarios: (i) large rotational discrepancies between the source and target\npoint clouds, and (ii) data corrupted by non-Gaussian noise. Our results show\nthat ESM-ICP outperforms traditional geometric registration techniques as well\nas several recent learning-based methods. To encourage reproducibility and\ncommunity engagement, our full implementation is made publicly available on\nGitHub. https://github.com/aralab-unr/ESM_ICP"}
{"id": "2505.04575", "pdf": "https://arxiv.org/pdf/2505.04575", "abs": "https://arxiv.org/abs/2505.04575", "authors": ["Kunlun Xu", "Xu Zou", "Gang Hua", "Jiahuan Zhou"], "title": "Componential Prompt-Knowledge Alignment for Domain Incremental Learning", "categories": ["cs.CV", "cs.LG"], "comment": "Accpted by ICML2025", "summary": "Domain Incremental Learning (DIL) aims to learn from non-stationary data\nstreams across domains while retaining and utilizing past knowledge. Although\nprompt-based methods effectively store multi-domain knowledge in prompt\nparameters and obtain advanced performance through cross-domain prompt fusion,\nwe reveal an intrinsic limitation: component-wise misalignment between\ndomain-specific prompts leads to conflicting knowledge integration and degraded\npredictions. This arises from the random positioning of knowledge components\nwithin prompts, where irrelevant component fusion introduces interference.To\naddress this, we propose Componential Prompt-Knowledge Alignment (KA-Prompt), a\nnovel prompt-based DIL method that introduces component-aware prompt-knowledge\nalignment during training, significantly improving both the learning and\ninference capacity of the model. KA-Prompt operates in two phases: (1) Initial\nComponential Structure Configuring, where a set of old prompts containing\nknowledge relevant to the new domain are mined via greedy search, which is then\nexploited to initialize new prompts to achieve reusable knowledge transfer and\nestablish intrinsic alignment between new and old prompts. (2) Online Alignment\nPreservation, which dynamically identifies the target old prompts and applies\nadaptive componential consistency constraints as new prompts evolve. Extensive\nexperiments on DIL benchmarks demonstrate the effectiveness of our KA-Prompt.\nOur source code is available at\nhttps://github.com/zhoujiahuan1991/ICML2025-KA-Prompt"}
{"id": "2505.04586", "pdf": "https://arxiv.org/pdf/2505.04586", "abs": "https://arxiv.org/abs/2505.04586", "authors": ["Yuning Du", "Jingshuai Liu", "Rohan Dharmakumar", "Sotirios A. Tsaftaris"], "title": "Active Sampling for MRI-based Sequential Decision Making", "categories": ["cs.CV", "cs.LG"], "comment": "Under Review", "summary": "Despite the superior diagnostic capability of Magnetic Resonance Imaging\n(MRI), its use as a Point-of-Care (PoC) device remains limited by high cost and\ncomplexity. To enable such a future by reducing the magnetic field strength,\none key approach will be to improve sampling strategies. Previous work has\nshown that it is possible to make diagnostic decisions directly from k-space\nwith fewer samples. Such work shows that single diagnostic decisions can be\nmade, but if we aspire to see MRI as a true PoC, multiple and sequential\ndecisions are necessary while minimizing the number of samples acquired. We\npresent a novel multi-objective reinforcement learning framework enabling\ncomprehensive, sequential, diagnostic evaluation from undersampled k-space\ndata. Our approach during inference actively adapts to sequential decisions to\noptimally sample. To achieve this, we introduce a training methodology that\nidentifies the samples that contribute the best to each diagnostic objective\nusing a step-wise weighting reward function. We evaluate our approach in two\nsequential knee pathology assessment tasks: ACL sprain detection and cartilage\nthickness loss assessment. Our framework achieves diagnostic performance\ncompetitive with various policy-based benchmarks on disease detection, severity\nquantification, and overall sequential diagnosis, while substantially saving\nk-space samples. Our approach paves the way for the future of MRI as a\ncomprehensive and affordable PoC device. Our code is publicly available at\nhttps://github.com/vios-s/MRI_Sequential_Active_Sampling"}
{"id": "2505.04594", "pdf": "https://arxiv.org/pdf/2505.04594", "abs": "https://arxiv.org/abs/2505.04594", "authors": ["Zhihao Zhang", "Abhinav Kumar", "Girish Chandar Ganesan", "Xiaoming Liu"], "title": "MonoCoP: Chain-of-Prediction for Monocular 3D Object Detection", "categories": ["cs.CV"], "comment": null, "summary": "Accurately predicting 3D attributes is crucial for monocular 3D object\ndetection (Mono3D), with depth estimation posing the greatest challenge due to\nthe inherent ambiguity in mapping 2D images to 3D space. While existing methods\nleverage multiple depth cues (e.g., estimating depth uncertainty, modeling\ndepth error) to improve depth accuracy, they overlook that accurate depth\nprediction requires conditioning on other 3D attributes, as these attributes\nare intrinsically inter-correlated through the 3D to 2D projection, which\nultimately limits overall accuracy and stability. Inspired by Chain-of-Thought\n(CoT) in large language models (LLMs), this paper proposes MonoCoP, which\nleverages a Chain-of-Prediction (CoP) to predict attributes sequentially and\nconditionally via three key designs. First, it employs a lightweight\nAttributeNet (AN) for each 3D attribute to learn attribute-specific features.\nNext, MonoCoP constructs an explicit chain to propagate these learned features\nfrom one attribute to the next. Finally, MonoCoP uses a residual connection to\naggregate features for each attribute along the chain, ensuring that later\nattribute predictions are conditioned on all previously processed attributes\nwithout forgetting the features of earlier ones. Experimental results show that\nour MonoCoP achieves state-of-the-art (SoTA) performance on the KITTI\nleaderboard without requiring additional data and further surpasses existing\nmethods on the Waymo and nuScenes frontal datasets."}
{"id": "2505.04601", "pdf": "https://arxiv.org/pdf/2505.04601", "abs": "https://arxiv.org/abs/2505.04601", "authors": ["Xianhang Li", "Yanqing Liu", "Haoqin Tu", "Hongru Zhu", "Cihang Xie"], "title": "OpenVision: A Fully-Open, Cost-Effective Family of Advanced Vision Encoders for Multimodal Learning", "categories": ["cs.CV"], "comment": null, "summary": "OpenAI's CLIP, released in early 2021, have long been the go-to choice of\nvision encoder for building multimodal foundation models. Although recent\nalternatives such as SigLIP have begun to challenge this status quo, to our\nknowledge none are fully open: their training data remains proprietary and/or\ntheir training recipes are not released. This paper fills this gap with\nOpenVision, a fully-open, cost-effective family of vision encoders that match\nor surpass the performance of OpenAI's CLIP when integrated into multimodal\nframeworks like LLaVA. OpenVision builds on existing works -- e.g., CLIPS for\ntraining framework and Recap-DataComp-1B for training data -- while revealing\nmultiple key insights in enhancing encoder quality and showcasing practical\nbenefits in advancing multimodal models. By releasing vision encoders spanning\nfrom 5.9M to 632.1M parameters, OpenVision offers practitioners a flexible\ntrade-off between capacity and efficiency in building multimodal models: larger\nmodels deliver enhanced multimodal performance, while smaller versions enable\nlightweight, edge-ready multimodal deployments."}
{"id": "2505.04612", "pdf": "https://arxiv.org/pdf/2505.04612", "abs": "https://arxiv.org/abs/2505.04612", "authors": ["Jiahao Li", "Haochen Wang", "Muhammad Zubair Irshad", "Igor Vasiljevic", "Matthew R. Walter", "Vitor Campagnolo Guizilini", "Greg Shakhnarovich"], "title": "FastMap: Revisiting Dense and Scalable Structure from Motion", "categories": ["cs.CV"], "comment": "Project webpage: https://jiahao.ai/fastmap", "summary": "We propose FastMap, a new global structure from motion method focused on\nspeed and simplicity. Previous methods like COLMAP and GLOMAP are able to\nestimate high-precision camera poses, but suffer from poor scalability when the\nnumber of matched keypoint pairs becomes large. We identify two key factors\nleading to this problem: poor parallelization and computationally expensive\noptimization steps. To overcome these issues, we design an SfM framework that\nrelies entirely on GPU-friendly operations, making it easily parallelizable.\nMoreover, each optimization step runs in time linear to the number of image\npairs, independent of keypoint pairs or 3D points. Through extensive\nexperiments, we show that FastMap is one to two orders of magnitude faster than\nCOLMAP and GLOMAP on large-scale scenes with comparable pose accuracy."}
{"id": "2505.04616", "pdf": "https://arxiv.org/pdf/2505.04616", "abs": "https://arxiv.org/abs/2505.04616", "authors": ["Feng Liu", "Nicholas Chimitt", "Lanqing Guo", "Jitesh Jain", "Aditya Kane", "Minchul Kim", "Wes Robbins", "Yiyang Su", "Dingqiang Ye", "Xingguang Zhang", "Jie Zhu", "Siddharth Satyakam", "Christopher Perry", "Stanley H. Chan", "Arun Ross", "Humphrey Shi", "Zhangyang Wang", "Anil Jain", "Xiaoming Liu"], "title": "Person Recognition at Altitude and Range: Fusion of Face, Body Shape and Gait", "categories": ["cs.CV"], "comment": "18 pages, 12 figures", "summary": "We address the problem of whole-body person recognition in unconstrained\nenvironments. This problem arises in surveillance scenarios such as those in\nthe IARPA Biometric Recognition and Identification at Altitude and Range\n(BRIAR) program, where biometric data is captured at long standoff distances,\nelevated viewing angles, and under adverse atmospheric conditions (e.g.,\nturbulence and high wind velocity). To this end, we propose FarSight, a unified\nend-to-end system for person recognition that integrates complementary\nbiometric cues across face, gait, and body shape modalities. FarSight\nincorporates novel algorithms across four core modules: multi-subject detection\nand tracking, recognition-aware video restoration, modality-specific biometric\nfeature encoding, and quality-guided multi-modal fusion. These components are\ndesigned to work cohesively under degraded image conditions, large pose and\nscale variations, and cross-domain gaps. Extensive experiments on the BRIAR\ndataset, one of the most comprehensive benchmarks for long-range, multi-modal\nbiometric recognition, demonstrate the effectiveness of FarSight. Compared to\nour preliminary system, this system achieves a 34.1% absolute gain in 1:1\nverification accuracy (TAR@0.1% FAR), a 17.8% increase in closed-set\nidentification (Rank-20), and a 34.3% reduction in open-set identification\nerrors (FNIR@1% FPIR). Furthermore, FarSight was evaluated in the 2025 NIST RTE\nFace in Video Evaluation (FIVE), which conducts standardized face recognition\ntesting on the BRIAR dataset. These results establish FarSight as a\nstate-of-the-art solution for operational biometric recognition in challenging\nreal-world conditions."}
{"id": "2505.04620", "pdf": "https://arxiv.org/pdf/2505.04620", "abs": "https://arxiv.org/abs/2505.04620", "authors": ["Hao Fei", "Yuan Zhou", "Juncheng Li", "Xiangtai Li", "Qingshan Xu", "Bobo Li", "Shengqiong Wu", "Yaoting Wang", "Junbao Zhou", "Jiahao Meng", "Qingyu Shi", "Zhiyuan Zhou", "Liangtao Shi", "Minghe Gao", "Daoan Zhang", "Zhiqi Ge", "Weiming Wu", "Siliang Tang", "Kaihang Pan", "Yaobo Ye", "Haobo Yuan", "Tao Zhang", "Tianjie Ju", "Zixiang Meng", "Shilin Xu", "Liyu Jia", "Wentao Hu", "Meng Luo", "Jiebo Luo", "Tat-Seng Chua", "Shuicheng Yan", "Hanwang Zhang"], "title": "On Path to Multimodal Generalist: General-Level and General-Bench", "categories": ["cs.CV"], "comment": "ICML'25, 305 pages, 115 tables, 177 figures, project page:\n  https://generalist.top/", "summary": "The Multimodal Large Language Model (MLLM) is currently experiencing rapid\ngrowth, driven by the advanced capabilities of LLMs. Unlike earlier\nspecialists, existing MLLMs are evolving towards a Multimodal Generalist\nparadigm. Initially limited to understanding multiple modalities, these models\nhave advanced to not only comprehend but also generate across modalities. Their\ncapabilities have expanded from coarse-grained to fine-grained multimodal\nunderstanding and from supporting limited modalities to arbitrary ones. While\nmany benchmarks exist to assess MLLMs, a critical question arises: Can we\nsimply assume that higher performance across tasks indicates a stronger MLLM\ncapability, bringing us closer to human-level AI? We argue that the answer is\nnot as straightforward as it seems. This project introduces General-Level, an\nevaluation framework that defines 5-scale levels of MLLM performance and\ngenerality, offering a methodology to compare MLLMs and gauge the progress of\nexisting systems towards more robust multimodal generalists and, ultimately,\ntowards AGI. At the core of the framework is the concept of Synergy, which\nmeasures whether models maintain consistent capabilities across comprehension\nand generation, and across multiple modalities. To support this evaluation, we\npresent General-Bench, which encompasses a broader spectrum of skills,\nmodalities, formats, and capabilities, including over 700 tasks and 325,800\ninstances. The evaluation results that involve over 100 existing\nstate-of-the-art MLLMs uncover the capability rankings of generalists,\nhighlighting the challenges in reaching genuine AI. We expect this project to\npave the way for future research on next-generation multimodal foundation\nmodels, providing a robust infrastructure to accelerate the realization of AGI.\nProject page: https://generalist.top/"}
{"id": "2505.03757", "pdf": "https://arxiv.org/pdf/2505.03757", "abs": "https://arxiv.org/abs/2505.03757", "authors": ["Vinicius Francisco Rofatto", "Luiz Felipe Rodrigues de Almeida", "Marcelo Tomio Matsuoka", "Ivandro Klein", "Mauricio Roberto Veronez", "Luiz Gonzaga Da Silveira Junior"], "title": "On the Residual-based Neural Network for Unmodeled Distortions in Coordinate Transformation", "categories": ["physics.geo-ph", "cs.CV", "cs.LG", "stat.AP", "stat.ML"], "comment": null, "summary": "Coordinate transformation models often fail to account for nonlinear and\nspatially dependent distortions, leading to significant residual errors in\ngeospatial applications. Here we propose a residual-based neural correction\nstrategy, in which a neural network learns to model only the systematic\ndistortions left by an initial geometric transformation. By focusing solely on\nresidual patterns, the proposed method reduces model complexity and improves\nperformance, particularly in scenarios with sparse or structured control point\nconfigurations. We evaluate the method using both simulated datasets with\nvarying distortion intensities and sampling strategies, as well as under the\nreal-world image georeferencing tasks. Compared with direct neural network\ncoordinate converter and classical transformation models, the residual-based\nneural correction delivers more accurate and stable results under challenging\nconditions, while maintaining comparable performance in ideal cases. These\nfindings demonstrate the effectiveness of residual modelling as a lightweight\nand robust alternative for improving coordinate transformation accuracy."}
{"id": "2505.03788", "pdf": "https://arxiv.org/pdf/2505.03788", "abs": "https://arxiv.org/abs/2505.03788", "authors": ["Trilok Padhi", "Ramneet Kaur", "Adam D. Cobb", "Manoj Acharya", "Anirban Roy", "Colin Samplawski", "Brian Matejek", "Alexander M. Berenbeim", "Nathaniel D. Bastian", "Susmit Jha"], "title": "Calibrating Uncertainty Quantification of Multi-Modal LLMs using Grounding", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "We introduce a novel approach for calibrating uncertainty quantification (UQ)\ntailored for multi-modal large language models (LLMs). Existing\nstate-of-the-art UQ methods rely on consistency among multiple responses\ngenerated by the LLM on an input query under diverse settings. However, these\napproaches often report higher confidence in scenarios where the LLM is\nconsistently incorrect. This leads to a poorly calibrated confidence with\nrespect to accuracy. To address this, we leverage cross-modal consistency in\naddition to self-consistency to improve the calibration of the multi-modal\nmodels. Specifically, we ground the textual responses to the visual inputs. The\nconfidence from the grounding model is used to calibrate the overall\nconfidence. Given that using a grounding model adds its own uncertainty in the\npipeline, we apply temperature scaling - a widely accepted parametric\ncalibration technique - to calibrate the grounding model's confidence in the\naccuracy of generated responses. We evaluate the proposed approach across\nmultiple multi-modal tasks, such as medical question answering (Slake) and\nvisual question answering (VQAv2), considering multi-modal models such as\nLLaVA-Med and LLaVA. The experiments demonstrate that the proposed framework\nachieves significantly improved calibration on both tasks."}
{"id": "2505.03800", "pdf": "https://arxiv.org/pdf/2505.03800", "abs": "https://arxiv.org/abs/2505.03800", "authors": ["TianYi Yu"], "title": "Design description of Wisdom Computing Persperctive", "categories": ["cs.AI", "cs.CV"], "comment": null, "summary": "This course design aims to develop and research a handwriting matrix\nrecognition and step-by-step visual calculation process display system,\naddressing the issue of abstract formulas and complex calculation steps that\nstudents find difficult to understand when learning mathematics. By integrating\nartificial intelligence with visualization animation technology, the system\nenhances precise recognition of handwritten matrix content through the\nintroduction of Mamba backbone networks, completes digital extraction and\nmatrix reconstruction using the YOLO model, and simultaneously combines\nCoordAttention coordinate attention mechanisms to improve the accurate grasp of\ncharacter spatial positions. The calculation process is demonstrated frame by\nframe through the Manim animation engine, vividly showcasing each mathematical\ncalculation step, helping students intuitively understand the intrinsic logic\nof mathematical operations. Through dynamically generating animation processes\nfor different computational tasks, the system exhibits high modularity and\nflexibility, capable of generating various mathematical operation examples in\nreal-time according to student needs. By innovating human-computer interaction\nmethods, it brings mathematical calculation processes to life, helping students\nbridge the gap between knowledge and understanding on a deeper level,\nultimately achieving a learning experience where \"every step is understood.\"\nThe system's scalability and interactivity make it an intuitive, user-friendly,\nand efficient auxiliary tool in education."}
{"id": "2505.03807", "pdf": "https://arxiv.org/pdf/2505.03807", "abs": "https://arxiv.org/abs/2505.03807", "authors": ["Yiwen Zhang", "Jianing Hao", "Zhan Wang", "Hongling Sheng", "Wei Zeng"], "title": "Facilitating Video Story Interaction with Multi-Agent Collaborative System", "categories": ["cs.HC", "cs.AI", "cs.CV", "cs.MA"], "comment": "Prepared and submitted in 2024", "summary": "Video story interaction enables viewers to engage with and explore narrative\ncontent for personalized experiences. However, existing methods are limited to\nuser selection, specially designed narratives, and lack customization. To\naddress this, we propose an interactive system based on user intent. Our system\nuses a Vision Language Model (VLM) to enable machines to understand video\nstories, combining Retrieval-Augmented Generation (RAG) and a Multi-Agent\nSystem (MAS) to create evolving characters and scene experiences. It includes\nthree stages: 1) Video story processing, utilizing VLM and prior knowledge to\nsimulate human understanding of stories across three modalities. 2) Multi-space\nchat, creating growth-oriented characters through MAS interactions based on\nuser queries and story stages. 3) Scene customization, expanding and\nvisualizing various story scenes mentioned in dialogue. Applied to the Harry\nPotter series, our study shows the system effectively portrays emergent\ncharacter social behavior and growth, enhancing the interactive experience in\nthe video story world."}
{"id": "2505.03808", "pdf": "https://arxiv.org/pdf/2505.03808", "abs": "https://arxiv.org/abs/2505.03808", "authors": ["Ioannis Nasios"], "title": "AI-driven multi-source data fusion for algal bloom severity classification in small inland water bodies: Leveraging Sentinel-2, DEM, and NOAA climate data", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Harmful algal blooms are a growing threat to inland water quality and public\nhealth worldwide, creating an urgent need for efficient, accurate, and\ncost-effective detection methods. This research introduces a high-performing\nmethodology that integrates multiple open-source remote sensing data with\nadvanced artificial intelligence models. Key data sources include Copernicus\nSentinel-2 optical imagery, the Copernicus Digital Elevation Model (DEM), and\nNOAA's High-Resolution Rapid Refresh (HRRR) climate data, all efficiently\nretrieved using platforms like Google Earth Engine (GEE) and Microsoft\nPlanetary Computer (MPC). The NIR and two SWIR bands from Sentinel-2, the\naltitude from the elevation model, the temperature and wind from NOAA as well\nas the longitude and latitude were the most important features. The approach\ncombines two types of machine learning models, tree-based models and a neural\nnetwork, into an ensemble for classifying algal bloom severity. While the tree\nmodels performed strongly on their own, incorporating a neural network added\nrobustness and demonstrated how deep learning models can effectively use\ndiverse remote sensing inputs. The method leverages high-resolution satellite\nimagery and AI-driven analysis to monitor algal blooms dynamically, and\nalthough initially developed for a NASA competition in the U.S., it shows\npotential for global application. The complete code is available for further\nadaptation and practical implementation, illustrating the convergence of remote\nsensing data and AI to address critical environmental challenges\n(https://github.com/IoannisNasios/HarmfulAlgalBloomDetection)."}
{"id": "2505.03809", "pdf": "https://arxiv.org/pdf/2505.03809", "abs": "https://arxiv.org/abs/2505.03809", "authors": ["Suorong Yang", "Peng Ye", "Furao Shen", "Dongzhan Zhou"], "title": "When Dynamic Data Selection Meets Data Augmentation", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Dynamic data selection aims to accelerate training with lossless performance.\nHowever, reducing training data inherently limits data diversity, potentially\nhindering generalization. While data augmentation is widely used to enhance\ndiversity, it is typically not optimized in conjunction with selection. As a\nresult, directly combining these techniques fails to fully exploit their\nsynergies. To tackle the challenge, we propose a novel online data training\nframework that, for the first time, unifies dynamic data selection and\naugmentation, achieving both training efficiency and enhanced performance. Our\nmethod estimates each sample's joint distribution of local density and\nmultimodal semantic consistency, allowing for the targeted selection of\naugmentation-suitable samples while suppressing the inclusion of noisy or\nambiguous data. This enables a more significant reduction in dataset size\nwithout sacrificing model generalization. Experimental results demonstrate that\nour method outperforms existing state-of-the-art approaches on various\nbenchmark datasets and architectures, e.g., reducing 50\\% training costs on\nImageNet-1k with lossless performance. Furthermore, our approach enhances noise\nresistance and improves model robustness, reinforcing its practical utility in\nreal-world scenarios."}
{"id": "2505.03836", "pdf": "https://arxiv.org/pdf/2505.03836", "abs": "https://arxiv.org/abs/2505.03836", "authors": ["Chongsheng Zhang", "Shuwen Wu", "Yingqi Chen", "Matthias Aßenmacher", "Christian Heumann", "Yi Men", "Gaojuan Fan", "João Gama"], "title": "OBD-Finder: Explainable Coarse-to-Fine Text-Centric Oracle Bone Duplicates Discovery", "categories": ["cs.IR", "cs.AI", "cs.CV"], "comment": "This is the long version of our OBD-Finder paper for AI-enabled\n  Oracle Bone Duplicates Discovery (currently under review at the ECML PKDD\n  2025 Demo Track). The models, video illustration and demonstration of this\n  paper are available at: https://github.com/cszhangLMU/OBD-Finder/.\n  Illustration video: https://www.youtube.com/watch?v=5QT4f0YIo0Q", "summary": "Oracle Bone Inscription (OBI) is the earliest systematic writing system in\nChina, while the identification of Oracle Bone (OB) duplicates is a fundamental\nissue in OBI research. In this work, we design a progressive OB duplicate\ndiscovery framework that combines unsupervised low-level keypoints matching\nwith high-level text-centric content-based matching to refine and rank the\ncandidate OB duplicates with semantic awareness and interpretability. We\ncompare our approach with state-of-the-art content-based image retrieval and\nimage matching methods, showing that our approach yields comparable recall\nperformance and the highest simplified mean reciprocal rank scores for both\nTop-5 and Top-15 retrieval results, and with significantly accelerated\ncomputation efficiency. We have discovered over 60 pairs of new OB duplicates\nin real-world deployment, which were missed by OBI researchers for decades. The\nmodels, video illustration and demonstration of this work are available at:\nhttps://github.com/cszhangLMU/OBD-Finder/."}
{"id": "2505.03838", "pdf": "https://arxiv.org/pdf/2505.03838", "abs": "https://arxiv.org/abs/2505.03838", "authors": ["Ting Yu Tsai", "An Yu", "Meghana Spurthi Maadugundu", "Ishrat Jahan Mohima", "Umme Habiba Barsha", "Mei-Hwa F. Chen", "Balakrishnan Prabhakaran", "Ming-Ching Chang"], "title": "IntelliCardiac: An Intelligent Platform for Cardiac Image Segmentation and Classification", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Precise and effective processing of cardiac imaging data is critical for the\nidentification and management of the cardiovascular diseases. We introduce\nIntelliCardiac, a comprehensive, web-based medical image processing platform\nfor the automatic segmentation of 4D cardiac images and disease classification,\nutilizing an AI model trained on the publicly accessible ACDC dataset. The\nsystem, intended for patients, cardiologists, and healthcare professionals,\noffers an intuitive interface and uses deep learning models to identify\nessential heart structures and categorize cardiac diseases. The system supports\nanalysis of both the right and left ventricles as well as myocardium, and then\nclassifies patient's cardiac images into five diagnostic categories: dilated\ncardiomyopathy, myocardial infarction, hypertrophic cardiomyopathy, right\nventricular abnormality, and no disease. IntelliCardiac combines a deep\nlearning-based segmentation model with a two-step classification pipeline. The\nsegmentation module gains an overall accuracy of 92.6\\%. The classification\nmodule, trained on characteristics taken from segmented heart structures,\nachieves 98\\% accuracy in five categories. These results exceed the performance\nof the existing state-of-the-art methods that integrate both segmentation and\nclassification models. IntelliCardiac, which supports real-time visualization,\nworkflow integration, and AI-assisted diagnostics, has great potential as a\nscalable, accurate tool for clinical decision assistance in cardiac imaging and\ndiagnosis."}
{"id": "2505.03842", "pdf": "https://arxiv.org/pdf/2505.03842", "abs": "https://arxiv.org/abs/2505.03842", "authors": ["Vadim Musienko", "Axel Jacquet", "Ingmar Weber", "Till Koebe"], "title": "Coverage Biases in High-Resolution Satellite Imagery", "categories": ["cs.CY", "astro-ph.EP", "cs.CV"], "comment": null, "summary": "Satellite imagery is increasingly used to complement traditional data\ncollection approaches such as surveys and censuses across scientific\ndisciplines. However, we ask: Do all places on earth benefit equally from this\nnew wealth of information? In this study, we investigate coverage bias of major\nsatellite constellations that provide optical satellite imagery with a ground\nsampling distance below 10 meters, evaluating both the future on-demand tasking\nopportunities as well as the availability of historic images across the globe.\nSpecifically, forward-looking, we estimate how often different places are\nrevisited during a window of 30 days based on the satellites' orbital paths,\nthus investigating potential coverage biases caused by physical factors. We\nfind that locations farther away from the equator are generally revisited more\nfrequently by the constellations under study. Backward-looking, we show that\nhistoric satellite image availability -- based on metadata collected from major\nsatellite imagery providers -- is influenced by socio-economic factors on the\nground: less developed, less populated places have less satellite images\navailable. Furthermore, in three small case studies on recent conflict regions\nin this world, namely Gaza, Sudan and Ukraine, we show that also geopolitical\nevents play an important role in satellite image availability, hinting at\nunderlying business model decisions. These insights lay bare that the digital\ndividend yielded by satellite imagery is not equally distributed across our\nplanet."}
{"id": "2505.03844", "pdf": "https://arxiv.org/pdf/2505.03844", "abs": "https://arxiv.org/abs/2505.03844", "authors": ["Solène Debuysère", "Nicolas Trouvé", "Nathan Letheule", "Olivier Lévêque", "Elise Colin"], "title": "From Spaceborn to Airborn: SAR Image Synthesis Using Foundation Models for Multi-Scale Adaptation", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "The availability of Synthetic Aperture Radar (SAR) satellite imagery has\nincreased considerably in recent years, with datasets commercially available.\nHowever, the acquisition of high-resolution SAR images in airborne\nconfigurations, remains costly and limited. Thus, the lack of open source,\nwell-labeled, or easily exploitable SAR text-image datasets is a barrier to the\nuse of existing foundation models in remote sensing applications. In this\ncontext, synthetic image generation is a promising solution to augment this\nscarce data, enabling a broader range of applications. Leveraging over 15 years\nof ONERA's extensive archival airborn data from acquisition campaigns, we\ncreated a comprehensive training dataset of 110 thousands SAR images to exploit\na 3.5 billion parameters pre-trained latent diffusion model. In this work, we\npresent a novel approach utilizing spatial conditioning techniques within a\nfoundation model to transform satellite SAR imagery into airborne SAR\nrepresentations. Additionally, we demonstrate that our pipeline is effective\nfor bridging the realism of simulated images generated by ONERA's physics-based\nsimulator EMPRISE. Our method explores a key application of AI in advancing SAR\nimaging technology. To the best of our knowledge, we are the first to introduce\nthis approach in the literature."}
{"id": "2505.03845", "pdf": "https://arxiv.org/pdf/2505.03845", "abs": "https://arxiv.org/abs/2505.03845", "authors": ["Ioannis Kyprakis", "Vasileios Skaramagkas", "Iro Boura", "Georgios Karamanis", "Dimitrios I. Fotiadis", "Zinovia Kefalopoulou", "Cleanthe Spanaki", "Manolis Tsiknakis"], "title": "A Deep Learning approach for Depressive Symptoms assessment in Parkinson's disease patients using facial videos", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Parkinson's disease (PD) is a neurodegenerative disorder, manifesting with\nmotor and non-motor symptoms. Depressive symptoms are prevalent in PD,\naffecting up to 45% of patients. They are often underdiagnosed due to\noverlapping motor features, such as hypomimia. This study explores deep\nlearning (DL) models-ViViT, Video Swin Tiny, and 3D CNN-LSTM with attention\nlayers-to assess the presence and severity of depressive symptoms, as detected\nby the Geriatric Depression Scale (GDS), in PD patients through facial video\nanalysis. The same parameters were assessed in a secondary analysis taking into\naccount whether patients were one hour after (ON-medication state) or 12 hours\nwithout (OFF-medication state) dopaminergic medication. Using a dataset of\n1,875 videos from 178 patients, the Video Swin Tiny model achieved the highest\nperformance, with up to 94% accuracy and 93.7% F1-score in binary\nclassification (presence of absence of depressive symptoms), and 87.1% accuracy\nwith an 85.4% F1-score in multiclass tasks (absence or mild or severe\ndepressive symptoms)."}
{"id": "2505.03859", "pdf": "https://arxiv.org/pdf/2505.03859", "abs": "https://arxiv.org/abs/2505.03859", "authors": ["Will Hawkins", "Chris Russell", "Brent Mittelstadt"], "title": "Deepfakes on Demand: the rise of accessible non-consensual deepfake image generators", "categories": ["cs.CY", "cs.AI", "cs.CV", "68T01"], "comment": "13 pages", "summary": "Advances in multimodal machine learning have made text-to-image (T2I) models\nincreasingly accessible and popular. However, T2I models introduce risks such\nas the generation of non-consensual depictions of identifiable individuals,\notherwise known as deepfakes. This paper presents an empirical study exploring\nthe accessibility of deepfake model variants online. Through a metadata\nanalysis of thousands of publicly downloadable model variants on two popular\nrepositories, Hugging Face and Civitai, we demonstrate a huge rise in easily\naccessible deepfake models. Almost 35,000 examples of publicly downloadable\ndeepfake model variants are identified, primarily hosted on Civitai. These\ndeepfake models have been downloaded almost 15 million times since November\n2022, with the models targeting a range of individuals from global celebrities\nto Instagram users with under 10,000 followers. Both Stable Diffusion and Flux\nmodels are used for the creation of deepfake models, with 96% of these\ntargeting women and many signalling intent to generate non-consensual intimate\nimagery (NCII). Deepfake model variants are often created via the\nparameter-efficient fine-tuning technique known as low rank adaptation (LoRA),\nrequiring as few as 20 images, 24GB VRAM, and 15 minutes of time, making this\nprocess widely accessible via consumer-grade computers. Despite these models\nviolating the Terms of Service of hosting platforms, and regulation seeking to\nprevent dissemination, these results emphasise the pressing need for greater\naction to be taken against the creation of deepfakes and NCII."}
{"id": "2505.03912", "pdf": "https://arxiv.org/pdf/2505.03912", "abs": "https://arxiv.org/abs/2505.03912", "authors": ["Can Cui", "Pengxiang Ding", "Wenxuan Song", "Shuanghao Bai", "Xinyang Tong", "Zirui Ge", "Runze Suo", "Wanqi Zhou", "Yang Liu", "Bofang Jia", "Han Zhao", "Siteng Huang", "Donglin Wang"], "title": "OpenHelix: A Short Survey, Empirical Analysis, and Open-Source Dual-System VLA Model for Robotic Manipulation", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Dual-system VLA (Vision-Language-Action) architectures have become a hot\ntopic in embodied intelligence research, but there is a lack of sufficient\nopen-source work for further performance analysis and optimization. To address\nthis problem, this paper will summarize and compare the structural designs of\nexisting dual-system architectures, and conduct systematic empirical\nevaluations on the core design elements of existing dual-system architectures.\nUltimately, it will provide a low-cost open-source model for further\nexploration. Of course, this project will continue to update with more\nexperimental conclusions and open-source models with improved performance for\neveryone to choose from. Project page: https://openhelix-robot.github.io/."}
{"id": "2505.04003", "pdf": "https://arxiv.org/pdf/2505.04003", "abs": "https://arxiv.org/abs/2505.04003", "authors": ["Feng Gao", "Sheng Liu", "Chuanzheng Gong", "Xiaowei Zhou", "Jiayi Wang", "Junyu Dong", "Qian Du"], "title": "Prototype-Based Information Compensation Network for Multi-Source Remote Sensing Data Classification", "categories": ["eess.IV", "cs.CV"], "comment": "Accepted by IEEE TGRS 2025", "summary": "Multi-source remote sensing data joint classification aims to provide\naccuracy and reliability of land cover classification by leveraging the\ncomplementary information from multiple data sources. Existing methods confront\ntwo challenges: inter-frequency multi-source feature coupling and inconsistency\nof complementary information exploration. To solve these issues, we present a\nPrototype-based Information Compensation Network (PICNet) for land cover\nclassification based on HSI and SAR/LiDAR data. Specifically, we first design a\nfrequency interaction module to enhance the inter-frequency coupling in\nmulti-source feature extraction. The multi-source features are first decoupled\ninto high- and low-frequency components. Then, these features are recoupled to\nachieve efficient inter-frequency communication. Afterward, we design a\nprototype-based information compensation module to model the global\nmulti-source complementary information. Two sets of learnable modality\nprototypes are introduced to represent the global modality information of\nmulti-source data. Subsequently, cross-modal feature integration and alignment\nare achieved through cross-attention computation between the modality-specific\nprototype vectors and the raw feature representations. Extensive experiments on\nthree public datasets demonstrate the significant superiority of our PICNet\nover state-of-the-art methods. The codes are available at\nhttps://github.com/oucailab/PICNet."}
{"id": "2505.04050", "pdf": "https://arxiv.org/pdf/2505.04050", "abs": "https://arxiv.org/abs/2505.04050", "authors": ["Kazuki Higo", "Toshiki Kanai", "Yuki Endo", "Yoshihiro Kanamori"], "title": "TerraFusion: Joint Generation of Terrain Geometry and Texture Using Latent Diffusion Models", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "3D terrain models are essential in fields such as video game development and\nfilm production. Since surface color often correlates with terrain geometry,\ncapturing this relationship is crucial to achieving realism. However, most\nexisting methods generate either a heightmap or a texture, without sufficiently\naccounting for the inherent correlation. In this paper, we propose a method\nthat jointly generates terrain heightmaps and textures using a latent diffusion\nmodel. First, we train the model in an unsupervised manner to randomly generate\npaired heightmaps and textures. Then, we perform supervised learning of an\nexternal adapter to enable user control via hand-drawn sketches. Experiments\nshow that our approach allows intuitive terrain generation while preserving the\ncorrelation between heightmaps and textures."}
{"id": "2505.04052", "pdf": "https://arxiv.org/pdf/2505.04052", "abs": "https://arxiv.org/abs/2505.04052", "authors": ["Shun Masuda", "Yuki Endo", "Yoshihiro Kanamori"], "title": "Person-In-Situ: Scene-Consistent Human Image Insertion with Occlusion-Aware Pose Control", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Compositing human figures into scene images has broad applications in areas\nsuch as entertainment and advertising. However, existing methods often cannot\nhandle occlusion of the inserted person by foreground objects and unnaturally\nplace the person in the frontmost layer. Moreover, they offer limited control\nover the inserted person's pose. To address these challenges, we propose two\nmethods. Both allow explicit pose control via a 3D body model and leverage\nlatent diffusion models to synthesize the person at a contextually appropriate\ndepth, naturally handling occlusions without requiring occlusion masks. The\nfirst is a two-stage approach: the model first learns a depth map of the scene\nwith the person through supervised learning, and then synthesizes the person\naccordingly. The second method learns occlusion implicitly and synthesizes the\nperson directly from input data without explicit depth supervision.\nQuantitative and qualitative evaluations show that both methods outperform\nexisting approaches by better preserving scene consistency while accurately\nreflecting occlusions and user-specified poses."}
{"id": "2505.04095", "pdf": "https://arxiv.org/pdf/2505.04095", "abs": "https://arxiv.org/abs/2505.04095", "authors": ["Shuo Wen", "Edwin Meriaux", "Mariana Sosa Guzmán", "Charlotte Morissette", "Chloe Si", "Bobak Baghi", "Gregory Dudek"], "title": "Scalable Aerial GNSS Localization for Marine Robots", "categories": ["cs.RO", "cs.CV"], "comment": "International Conference on Robotics and Automation 2025 Workshop\n  Robots in the Wild", "summary": "Accurate localization is crucial for water robotics, yet traditional onboard\nGlobal Navigation Satellite System (GNSS) approaches are difficult or\nineffective due to signal reflection on the water's surface and its high cost\nof aquatic GNSS receivers. Existing approaches, such as inertial navigation,\nDoppler Velocity Loggers (DVL), SLAM, and acoustic-based methods, face\nchallenges like error accumulation and high computational complexity.\nTherefore, a more efficient and scalable solution remains necessary. This paper\nproposes an alternative approach that leverages an aerial drone equipped with\nGNSS localization to track and localize a marine robot once it is near the\nsurface of the water. Our results show that this novel adaptation enables\naccurate single and multi-robot marine robot localization."}
{"id": "2505.04097", "pdf": "https://arxiv.org/pdf/2505.04097", "abs": "https://arxiv.org/abs/2505.04097", "authors": ["Thien Nhan Vo", "Bac Nam Ho", "Thanh Xuan Truong"], "title": "3D Brain MRI Classification for Alzheimer Diagnosis Using CNN with Data Augmentation", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": null, "summary": "A three-dimensional convolutional neural network was developed to classify\nT1-weighted brain MRI scans as healthy or Alzheimer. The network comprises 3D\nconvolution, pooling, batch normalization, dense ReLU layers, and a sigmoid\noutput. Using stochastic noise injection and five-fold cross-validation, the\nmodel achieved test set accuracy of 0.912 and area under the ROC curve of\n0.961, an improvement of approximately 0.027 over resizing alone. Sensitivity\nand specificity both exceeded 0.90. These results align with prior work\nreporting up to 0.10 gain via synthetic augmentation. The findings demonstrate\nthe effectiveness of simple augmentation for 3D MRI classification and motivate\nfuture exploration of advanced augmentation methods and architectures such as\n3D U-Net and vision transformers."}
{"id": "2505.04258", "pdf": "https://arxiv.org/pdf/2505.04258", "abs": "https://arxiv.org/abs/2505.04258", "authors": ["Pietro Bonazzi", "Christian Vogt", "Michael Jost", "Haotong Qin", "Lyes Khacef", "Federico Paredes-Valles", "Michele Magno"], "title": "RGB-Event Fusion with Self-Attention for Collision Prediction", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Ensuring robust and real-time obstacle avoidance is critical for the safe\noperation of autonomous robots in dynamic, real-world environments. This paper\nproposes a neural network framework for predicting the time and collision\nposition of an unmanned aerial vehicle with a dynamic object, using RGB and\nevent-based vision sensors. The proposed architecture consists of two separate\nencoder branches, one for each modality, followed by fusion by self-attention\nto improve prediction accuracy. To facilitate benchmarking, we leverage the\nABCD [8] dataset collected that enables detailed comparisons of single-modality\nand fusion-based approaches. At the same prediction throughput of 50Hz, the\nexperimental results show that the fusion-based model offers an improvement in\nprediction accuracy over single-modality approaches of 1% on average and 10%\nfor distances beyond 0.5m, but comes at the cost of +71% in memory and + 105%\nin FLOPs. Notably, the event-based model outperforms the RGB model by 4% for\nposition and 26% for time error at a similar computational cost, making it a\ncompetitive alternative. Additionally, we evaluate quantized versions of the\nevent-based models, applying 1- to 8-bit quantization to assess the trade-offs\nbetween predictive performance and computational efficiency. These findings\nhighlight the trade-offs of multi-modal perception using RGB and event-based\ncameras in robotic applications."}
{"id": "2505.04387", "pdf": "https://arxiv.org/pdf/2505.04387", "abs": "https://arxiv.org/abs/2505.04387", "authors": ["Amin Fadaeinejad", "Abdallah Dib", "Luiz Gustavo Hafemann", "Emeline Got", "Trevor Anderson", "Amaury Depierre", "Nikolaus F. Troje", "Marcus A. Brubaker", "Marc-André Carbonneau"], "title": "Geometry-Aware Texture Generation for 3D Head Modeling with Artist-driven Control", "categories": ["cs.GR", "cs.CV"], "comment": "11 pages, 9 figures, AI for Creative Visual Content Generation\n  Editing and Understanding (CVEU), CVPRW 2025", "summary": "Creating realistic 3D head assets for virtual characters that match a precise\nartistic vision remains labor-intensive. We present a novel framework that\nstreamlines this process by providing artists with intuitive control over\ngenerated 3D heads. Our approach uses a geometry-aware texture synthesis\npipeline that learns correlations between head geometry and skin texture maps\nacross different demographics. The framework offers three levels of artistic\ncontrol: manipulation of overall head geometry, adjustment of skin tone while\npreserving facial characteristics, and fine-grained editing of details such as\nwrinkles or facial hair. Our pipeline allows artists to make edits to a single\ntexture map using familiar tools, with our system automatically propagating\nthese changes coherently across the remaining texture maps needed for realistic\nrendering. Experiments demonstrate that our method produces diverse results\nwith clean geometries. We showcase practical applications focusing on intuitive\ncontrol for artists, including skin tone adjustments and simplified editing\nworkflows for adding age-related details or removing unwanted features from\nscanned models. This integrated approach aims to streamline the artistic\nworkflow in virtual character creation."}
{"id": "2505.04590", "pdf": "https://arxiv.org/pdf/2505.04590", "abs": "https://arxiv.org/abs/2505.04590", "authors": ["Alexandre Binninger", "Ruben Wiersma", "Philipp Herholz", "Olga Sorkine-Hornung"], "title": "TetWeave: Isosurface Extraction using On-The-Fly Delaunay Tetrahedral Grids for Gradient-Based Mesh Optimization", "categories": ["cs.GR", "cs.CV", "I.3.5"], "comment": "ACM Trans. Graph. 44, 4. SIGGRAPH 2025. 19 pages, 21 figures", "summary": "We introduce TetWeave, a novel isosurface representation for gradient-based\nmesh optimization that jointly optimizes the placement of a tetrahedral grid\nused for Marching Tetrahedra and a novel directional signed distance at each\npoint. TetWeave constructs tetrahedral grids on-the-fly via Delaunay\ntriangulation, enabling increased flexibility compared to predefined grids. The\nextracted meshes are guaranteed to be watertight, two-manifold and\nintersection-free. The flexibility of TetWeave enables a resampling strategy\nthat places new points where reconstruction error is high and allows to\nencourage mesh fairness without compromising on reconstruction error. This\nleads to high-quality, adaptive meshes that require minimal memory usage and\nfew parameters to optimize. Consequently, TetWeave exhibits near-linear memory\nscaling relative to the vertex count of the output mesh - a substantial\nimprovement over predefined grids. We demonstrate the applicability of TetWeave\nto a broad range of challenging tasks in computer graphics and vision, such as\nmulti-view 3D reconstruction, mesh compression and geometric texture\ngeneration."}
{"id": "2505.04596", "pdf": "https://arxiv.org/pdf/2505.04596", "abs": "https://arxiv.org/abs/2505.04596", "authors": ["Mohammad Merati", "David Castañón"], "title": "Dynamic Network Flow Optimization for Task Scheduling in PTZ Camera Surveillance Systems", "categories": ["math.OC", "cs.CV", "cs.SY", "eess.SY"], "comment": "7 pages, 3 Figures, Accepted at AIRC 2025", "summary": "This paper presents a novel approach for optimizing the scheduling and\ncontrol of Pan-Tilt-Zoom (PTZ) cameras in dynamic surveillance environments.\nThe proposed method integrates Kalman filters for motion prediction with a\ndynamic network flow model to enhance real-time video capture efficiency. By\nassigning Kalman filters to tracked objects, the system predicts future\nlocations, enabling precise scheduling of camera tasks. This prediction-driven\napproach is formulated as a network flow optimization, ensuring scalability and\nadaptability to various surveillance scenarios. To further reduce redundant\nmonitoring, we also incorporate group-tracking nodes, allowing multiple objects\nto be captured within a single camera focus when appropriate. In addition, a\nvalue-based system is introduced to prioritize camera actions, focusing on the\ntimely capture of critical events. By adjusting the decay rates of these values\nover time, the system ensures prompt responses to tasks with imminent\ndeadlines. Extensive simulations demonstrate that this approach improves\ncoverage, reduces average wait times, and minimizes missed events compared to\ntraditional master-slave camera systems. Overall, our method significantly\nenhances the efficiency, scalability, and effectiveness of surveillance\nsystems, particularly in dynamic and crowded environments."}
{"id": "2505.04619", "pdf": "https://arxiv.org/pdf/2505.04619", "abs": "https://arxiv.org/abs/2505.04619", "authors": ["Abdulaziz Almuzairee", "Rohan Patil", "Dwait Bhatt", "Henrik I. Christensen"], "title": "Merging and Disentangling Views in Visual Reinforcement Learning for Robotic Manipulation", "categories": ["cs.LG", "cs.CV", "cs.RO"], "comment": "For project website and code, see https://aalmuzairee.github.io/mad", "summary": "Vision is well-known for its use in manipulation, especially using visual\nservoing. To make it robust, multiple cameras are needed to expand the field of\nview. That is computationally challenging. Merging multiple views and using\nQ-learning allows the design of more effective representations and optimization\nof sample efficiency. Such a solution might be expensive to deploy. To mitigate\nthis, we introduce a Merge And Disentanglement (MAD) algorithm that efficiently\nmerges views to increase sample efficiency while augmenting with single-view\nfeatures to allow lightweight deployment and ensure robust policies. We\ndemonstrate the efficiency and robustness of our approach using Meta-World and\nManiSkill3. For project website and code, see https://aalmuzairee.github.io/mad"}
{"id": "2505.04622", "pdf": "https://arxiv.org/pdf/2505.04622", "abs": "https://arxiv.org/abs/2505.04622", "authors": ["Jingwen Ye", "Yuze He", "Yanning Zhou", "Yiqin Zhu", "Kaiwen Xiao", "Yong-Jin Liu", "Wei Yang", "Xiao Han"], "title": "PrimitiveAnything: Human-Crafted 3D Primitive Assembly Generation with Auto-Regressive Transformer", "categories": ["cs.GR", "cs.CV"], "comment": "SIGGRAPH 2025. 14 pages, 15 figures", "summary": "Shape primitive abstraction, which decomposes complex 3D shapes into simple\ngeometric elements, plays a crucial role in human visual cognition and has\nbroad applications in computer vision and graphics. While recent advances in 3D\ncontent generation have shown remarkable progress, existing primitive\nabstraction methods either rely on geometric optimization with limited semantic\nunderstanding or learn from small-scale, category-specific datasets, struggling\nto generalize across diverse shape categories. We present PrimitiveAnything, a\nnovel framework that reformulates shape primitive abstraction as a primitive\nassembly generation task. PrimitiveAnything includes a shape-conditioned\nprimitive transformer for auto-regressive generation and an ambiguity-free\nparameterization scheme to represent multiple types of primitives in a unified\nmanner. The proposed framework directly learns the process of primitive\nassembly from large-scale human-crafted abstractions, enabling it to capture\nhow humans decompose complex shapes into primitive elements. Through extensive\nexperiments, we demonstrate that PrimitiveAnything can generate high-quality\nprimitive assemblies that better align with human perception while maintaining\ngeometric fidelity across diverse shape categories. It benefits various 3D\napplications and shows potential for enabling primitive-based user-generated\ncontent (UGC) in games. Project page: https://primitiveanything.github.io"}
{"id": "2505.04623", "pdf": "https://arxiv.org/pdf/2505.04623", "abs": "https://arxiv.org/abs/2505.04623", "authors": ["Zhenghao Xing", "Xiaowei Hu", "Chi-Wing Fu", "Wenhai Wang", "Jifeng Dai", "Pheng-Ann Heng"], "title": "EchoInk-R1: Exploring Audio-Visual Reasoning in Multimodal LLMs via Reinforcement Learning", "categories": ["eess.AS", "cs.AI", "cs.CV", "cs.MM", "cs.SD"], "comment": null, "summary": "Multimodal large language models (MLLMs) have advanced perception across\ntext, vision, and audio, yet they often struggle with structured cross-modal\nreasoning, particularly when integrating audio and visual signals. We introduce\nEchoInk-R1, a reinforcement learning framework that enhances such reasoning in\nMLLMs. Built upon the Qwen2.5-Omni-7B foundation and optimized with Group\nRelative Policy Optimization (GRPO), EchoInk-R1 tackles multiple-choice\nquestion answering over synchronized audio-image pairs. To enable this, we\ncurate AVQA-R1-6K, a dataset pairing such audio-image inputs with\nmultiple-choice questions derived from OmniInstruct-v1. EchoInk-R1-7B achieves\n85.77% accuracy on the validation set, outperforming the base model, which\nscores 80.53%, using only 562 reinforcement learning steps. Beyond accuracy,\nEchoInk-R1 demonstrates reflective reasoning by revisiting initial\ninterpretations and refining responses when facing ambiguous multimodal inputs.\nThese results suggest that lightweight reinforcement learning fine-tuning\nenhances cross-modal reasoning in MLLMs. EchoInk-R1 is the first framework to\nunify audio, visual, and textual modalities for general open-world reasoning\nvia reinforcement learning. Code and data are publicly released to facilitate\nfurther research."}
