{"id": "2505.02867", "pdf": "https://arxiv.org/pdf/2505.02867", "abs": "https://arxiv.org/abs/2505.02867", "authors": ["Ruiqi Wang", "Hao Zhang"], "title": "RESAnything: Attribute Prompting for Arbitrary Referring Segmentation", "categories": ["cs.CV"], "comment": "42 pages, 31 figures. For more details:\n  https://suikei-wang.github.io/RESAnything/", "summary": "We present an open-vocabulary and zero-shot method for arbitrary referring\nexpression segmentation (RES), targeting input expressions that are more\ngeneral than what prior works were designed to handle. Specifically, our inputs\nencompass both object- and part-level labels as well as implicit references\npointing to properties or qualities of object/part function, design, style,\nmaterial, etc. Our model, coined RESAnything, leverages Chain-of-Thoughts (CoT)\nreasoning, where the key idea is attribute prompting. We generate detailed\ndescriptions of object/part attributes including shape, color, and location for\npotential segment proposals through systematic prompting of a large language\nmodel (LLM), where the proposals are produced by a foundational image\nsegmentation model. Our approach encourages deep reasoning about object or part\nattributes related to function, style, design, etc., enabling the system to\nhandle implicit queries without any part annotations for training or\nfine-tuning. As the first zero-shot and LLM-based RES method, RESAnything\nachieves clearly superior performance among zero-shot methods on traditional\nRES benchmarks and significantly outperforms existing methods on challenging\nscenarios involving implicit queries and complex part-level relations. Finally,\nwe contribute a new benchmark dataset to offer ~3K carefully curated RES\ninstances to assess part-level, arbitrary RES solutions.", "AI": {"tldr": "RESAnything\u662f\u4e00\u79cd\u5f00\u653e\u8bcd\u6c47\u3001\u96f6\u6837\u672c\u7684\u4efb\u610f\u53c2\u8003\u8868\u8fbe\u5206\u5272\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c5e\u6027\u63d0\u793a\u548cChain-of-Thoughts\u63a8\u7406\u5904\u7406\u5bf9\u8c61\u548c\u90e8\u5206\u7ea7\u522b\u7684\u6807\u7b7e\u53ca\u9690\u5f0f\u53c2\u8003\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u5904\u7406\u7684\u5e7f\u4e49\u8f93\u5165\u8868\u8fbe\uff0c\u5305\u62ec\u5bf9\u8c61/\u90e8\u5206\u7ea7\u522b\u6807\u7b7e\u548c\u9690\u5f0f\u53c2\u8003\uff08\u5982\u529f\u80fd\u3001\u8bbe\u8ba1\u3001\u98ce\u683c\u7b49\uff09\u3002", "method": "\u5229\u7528Chain-of-Thoughts\u63a8\u7406\u548c\u5c5e\u6027\u63d0\u793a\uff0c\u901a\u8fc7\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u8be6\u7ec6\u5c5e\u6027\u63cf\u8ff0\uff0c\u7ed3\u5408\u57fa\u7840\u56fe\u50cf\u5206\u5272\u6a21\u578b\u751f\u6210\u63d0\u6848\u3002", "result": "\u5728\u4f20\u7edfRES\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5c24\u5176\u5728\u9690\u5f0f\u67e5\u8be2\u548c\u590d\u6742\u90e8\u5206\u5173\u7cfb\u573a\u666f\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "RESAnything\u662f\u9996\u4e2a\u96f6\u6837\u672c\u3001\u57fa\u4e8eLLM\u7684RES\u65b9\u6cd5\uff0c\u63d0\u51fa\u65b0\u57fa\u51c6\u6570\u636e\u96c6\u4ee5\u8bc4\u4f30\u90e8\u5206\u7ea7\u522b\u548c\u4efb\u610fRES\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.03154", "pdf": "https://arxiv.org/pdf/2505.03154", "abs": "https://arxiv.org/abs/2505.03154", "authors": ["Yuxuan Mu", "Hung Yu Ling", "Yi Shi", "Ismael Baira Ojeda", "Pengcheng Xi", "Chang Shu", "Fabio Zinno", "Xue Bin Peng"], "title": "StableMotion: Training Motion Cleanup Models with Unpaired Corrupted Data", "categories": ["cs.CV", "cs.AI", "cs.GR"], "comment": "17 pages, 13 figures", "summary": "Motion capture (mocap) data often exhibits visually jarring artifacts due to\ninaccurate sensors and post-processing. Cleaning this corrupted data can\nrequire substantial manual effort from human experts, which can be a costly and\ntime-consuming process. Previous data-driven motion cleanup methods offer the\npromise of automating this cleanup process, but often require in-domain paired\ncorrupted-to-clean training data. Constructing such paired datasets requires\naccess to high-quality, relatively artifact-free motion clips, which often\nnecessitates laborious manual cleanup. In this work, we present StableMotion, a\nsimple yet effective method for training motion cleanup models directly from\nunpaired corrupted datasets that need cleanup. The core component of our method\nis the introduction of motion quality indicators, which can be easily annotated\nthrough manual labeling or heuristic algorithms and enable training of\nquality-aware motion generation models on raw motion data with mixed quality.\nAt test time, the model can be prompted to generate high-quality motions using\nthe quality indicators. Our method can be implemented through a simple\ndiffusion-based framework, leading to a unified motion generate-discriminate\nmodel, which can be used to both identify and fix corrupted frames. We\ndemonstrate that our proposed method is effective for training motion cleanup\nmodels on raw mocap data in production scenarios by applying StableMotion to\nSoccerMocap, a 245-hour soccer mocap dataset containing real-world motion\nartifacts. The trained model effectively corrects a wide range of motion\nartifacts, reducing motion pops and frozen frames by 68% and 81%, respectively.\nSee https://youtu.be/3Y7MMAH02B4 for more results.", "AI": {"tldr": "StableMotion\u63d0\u51fa\u4e86\u4e00\u79cd\u76f4\u63a5\u4ece\u65e0\u914d\u5bf9\u6570\u636e\u4e2d\u8bad\u7ec3\u8fd0\u52a8\u6e05\u7406\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u8fd0\u52a8\u8d28\u91cf\u6307\u6807\uff0c\u5b9e\u73b0\u4e86\u5bf9\u6df7\u5408\u8d28\u91cf\u6570\u636e\u7684\u5904\u7406\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u8fd0\u52a8\u4f2a\u5f71\u3002", "motivation": "\u8fd0\u52a8\u6355\u6349\u6570\u636e\u5e38\u56e0\u4f20\u611f\u5668\u548c\u540e\u671f\u5904\u7406\u4e0d\u51c6\u786e\u800c\u4ea7\u751f\u89c6\u89c9\u4f2a\u5f71\uff0c\u6e05\u7406\u8fd9\u4e9b\u6570\u636e\u901a\u5e38\u9700\u8981\u5927\u91cf\u4eba\u5de5\uff0c\u6210\u672c\u9ad8\u4e14\u8017\u65f6\u3002\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u914d\u5bf9\u6570\u636e\uff0c\u4f46\u83b7\u53d6\u9ad8\u8d28\u91cf\u914d\u5bf9\u6570\u636e\u540c\u6837\u8d39\u65f6\u8d39\u529b\u3002", "method": "\u5f15\u5165\u8fd0\u52a8\u8d28\u91cf\u6307\u6807\uff0c\u901a\u8fc7\u624b\u52a8\u6807\u6ce8\u6216\u542f\u53d1\u5f0f\u7b97\u6cd5\u6807\u6ce8\uff0c\u8bad\u7ec3\u8d28\u91cf\u611f\u77e5\u7684\u8fd0\u52a8\u751f\u6210\u6a21\u578b\u3002\u91c7\u7528\u6269\u6563\u6846\u67b6\u5b9e\u73b0\u7edf\u4e00\u7684\u751f\u6210-\u5224\u522b\u6a21\u578b\uff0c\u65e2\u80fd\u8bc6\u522b\u4e5f\u80fd\u4fee\u590d\u4f2a\u5f71\u5e27\u3002", "result": "\u5728SoccerMocap\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\uff0c\u6a21\u578b\u6709\u6548\u51cf\u5c11\u4e8668%\u7684\u8fd0\u52a8\u7a81\u53d8\u548c81%\u7684\u51bb\u7ed3\u5e27\u3002", "conclusion": "StableMotion\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u76f4\u63a5\u4ece\u65e0\u914d\u5bf9\u6570\u636e\u4e2d\u8bad\u7ec3\u8fd0\u52a8\u6e05\u7406\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8fd0\u52a8\u6570\u636e\u7684\u8d28\u91cf\u3002"}}
{"id": "2505.02949", "pdf": "https://arxiv.org/pdf/2505.02949", "abs": "https://arxiv.org/abs/2505.02949", "authors": ["Tian Qiu", "Arjun Nichani", "Rasta Tadayontahmasebi", "Haewon Jeong"], "title": "Gone With the Bits: Revealing Racial Bias in Low-Rate Neural Compression for Facial Images", "categories": ["cs.CV"], "comment": "Accepted at ACM FAccT '25", "summary": "Neural compression methods are gaining popularity due to their superior\nrate-distortion performance over traditional methods, even at extremely low\nbitrates below 0.1 bpp. As deep learning architectures, these models are prone\nto bias during the training process, potentially leading to unfair outcomes for\nindividuals in different groups. In this paper, we present a general,\nstructured, scalable framework for evaluating bias in neural image compression\nmodels. Using this framework, we investigate racial bias in neural compression\nalgorithms by analyzing nine popular models and their variants. Through this\ninvestigation, we first demonstrate that traditional distortion metrics are\nineffective in capturing bias in neural compression models. Next, we highlight\nthat racial bias is present in all neural compression models and can be\ncaptured by examining facial phenotype degradation in image reconstructions. We\nthen examine the relationship between bias and realism in the decoded images\nand demonstrate a trade-off across models. Finally, we show that utilizing a\nracially balanced training set can reduce bias but is not a sufficient bias\nmitigation strategy. We additionally show the bias can be attributed to\ncompression model bias and classification model bias. We believe that this work\nis a first step towards evaluating and eliminating bias in neural image\ncompression models.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8bc4\u4f30\u795e\u7ecf\u56fe\u50cf\u538b\u7f29\u6a21\u578b\u4e2d\u504f\u89c1\u7684\u6846\u67b6\uff0c\u53d1\u73b0\u4f20\u7edf\u5931\u771f\u6307\u6807\u65e0\u6cd5\u6355\u6349\u504f\u89c1\uff0c\u4e14\u6240\u6709\u6a21\u578b\u5747\u5b58\u5728\u79cd\u65cf\u504f\u89c1\uff0c\u5e76\u63d0\u51fa\u5e73\u8861\u8bad\u7ec3\u96c6\u53ef\u90e8\u5206\u7f13\u89e3\u504f\u89c1\u3002", "motivation": "\u795e\u7ecf\u538b\u7f29\u65b9\u6cd5\u5728\u6781\u4f4e\u6bd4\u7279\u7387\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u53ef\u80fd\u56e0\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u504f\u89c1\u5bfc\u81f4\u4e0d\u516c\u5e73\u7ed3\u679c\uff0c\u9700\u7cfb\u7edf\u8bc4\u4f30\u548c\u89e3\u51b3\u3002", "method": "\u63d0\u51fa\u901a\u7528\u3001\u7ed3\u6784\u5316\u3001\u53ef\u6269\u5c55\u7684\u6846\u67b6\uff0c\u5206\u6790\u4e5d\u79cd\u6d41\u884c\u6a21\u578b\u53ca\u5176\u53d8\u79cd\uff0c\u901a\u8fc7\u9762\u90e8\u8868\u578b\u9000\u5316\u8bc4\u4f30\u79cd\u65cf\u504f\u89c1\u3002", "result": "\u53d1\u73b0\u6240\u6709\u6a21\u578b\u5747\u5b58\u5728\u79cd\u65cf\u504f\u89c1\uff0c\u4f20\u7edf\u5931\u771f\u6307\u6807\u65e0\u6548\uff1b\u5e73\u8861\u8bad\u7ec3\u96c6\u53ef\u51cf\u5c11\u504f\u89c1\u4f46\u4e0d\u8db3\uff1b\u504f\u89c1\u6e90\u4e8e\u538b\u7f29\u548c\u5206\u7c7b\u6a21\u578b\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u8bc4\u4f30\u548c\u6d88\u9664\u795e\u7ecf\u56fe\u50cf\u538b\u7f29\u6a21\u578b\u4e2d\u7684\u504f\u89c1\u8fc8\u51fa\u4e86\u7b2c\u4e00\u6b65\u3002"}}
{"id": "2505.03682", "pdf": "https://arxiv.org/pdf/2505.03682", "abs": "https://arxiv.org/abs/2505.03682", "authors": ["Christopher Fl\u00f6ter", "Sergej Geringer", "Guido Reina", "Daniel Weiskopf", "Timo Ropinski"], "title": "Evaluating Foveated Frame Rate Reduction in Virtual Reality for Head-Mounted Displays", "categories": ["cs.HC", "cs.GR", "I.3.6; I.3.7"], "comment": "Temporal resolution reduction, frame rate reduction, foveated\n  rendering, virtual reality", "summary": "Foveated rendering methods usually reduce spatial resolution in the periphery\nof the users' view. However, using foveated rendering to reduce temporal\nresolution, i.e., rendering frame rate, seems less explored. In this work, we\npresent the results of a user study investigating the perceptual effects of\nfoveated temporal resolution reduction, where only the temporal resolution\n(frame rate) is reduced in the periphery without affecting spatial quality\n(pixel density). In particular, we investigated the perception of temporal\nresolution artifacts caused by reducing the frame rate dependent on the\neccentricity of the user's gaze. Our user study with 15 participants was\nconducted in a virtual reality setting using a head-mounted display. Our\nresults indicate that it was possible to reduce average rendering costs, i.e.,\nthe number of rendered pixels, to a large degree before participants\nconsistently reported perceiving temporal artifacts.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5728\u865a\u62df\u73b0\u5b9e\u4e2d\u901a\u8fc7\u964d\u4f4e\u5916\u56f4\u533a\u57df\u7684\u5e27\u7387\uff08\u65f6\u95f4\u5206\u8fa8\u7387\uff09\u800c\u975e\u7a7a\u95f4\u5206\u8fa8\u7387\u6765\u51cf\u5c11\u6e32\u67d3\u6210\u672c\u7684\u6548\u679c\u3002", "motivation": "\u63a2\u7d22\u5728\u7528\u6237\u89c6\u91ce\u5916\u56f4\u964d\u4f4e\u65f6\u95f4\u5206\u8fa8\u7387\uff08\u5e27\u7387\uff09\u800c\u975e\u7a7a\u95f4\u5206\u8fa8\u7387\u7684\u611f\u77e5\u6548\u679c\uff0c\u4ee5\u4f18\u5316\u6e32\u67d3\u6548\u7387\u3002", "method": "\u5728\u865a\u62df\u73b0\u5b9e\u73af\u5883\u4e2d\u5bf915\u540d\u53c2\u4e0e\u8005\u8fdb\u884c\u7528\u6237\u7814\u7a76\uff0c\u6d4b\u8bd5\u4e0d\u540c\u504f\u5fc3\u5ea6\u4e0b\u5e27\u7387\u964d\u4f4e\u7684\u611f\u77e5\u6548\u679c\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u5728\u53c2\u4e0e\u8005\u4e00\u81f4\u611f\u77e5\u5230\u65f6\u95f4\u4f2a\u5f71\u4e4b\u524d\uff0c\u53ef\u4ee5\u5927\u5e45\u964d\u4f4e\u5e73\u5747\u6e32\u67d3\u6210\u672c\uff08\u6e32\u67d3\u50cf\u7d20\u6570\uff09\u3002", "conclusion": "\u5916\u56f4\u533a\u57df\u7684\u65f6\u95f4\u5206\u8fa8\u7387\u964d\u4f4e\u662f\u4e00\u79cd\u53ef\u884c\u7684\u6e32\u67d3\u4f18\u5316\u65b9\u6cd5\uff0c\u53ef\u5728\u4e0d\u660e\u663e\u5f71\u54cd\u7528\u6237\u4f53\u9a8c\u7684\u60c5\u51b5\u4e0b\u51cf\u5c11\u8ba1\u7b97\u8d1f\u62c5\u3002"}}
{"id": "2505.02966", "pdf": "https://arxiv.org/pdf/2505.02966", "abs": "https://arxiv.org/abs/2505.02966", "authors": ["Alexander Holmberg"], "title": "Generating Narrated Lecture Videos from Slides with Synchronized Highlights", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Turning static slides into engaging video lectures takes considerable time\nand effort, requiring presenters to record explanations and visually guide\ntheir audience through the material. We introduce an end-to-end system designed\nto automate this process entirely. Given a slide deck, this system synthesizes\na video lecture featuring AI-generated narration synchronized precisely with\ndynamic visual highlights. These highlights automatically draw attention to the\nspecific concept being discussed, much like an effective presenter would. The\ncore technical contribution is a novel highlight alignment module. This module\naccurately maps spoken phrases to locations on a given slide using diverse\nstrategies (e.g., Levenshtein distance, LLM-based semantic analysis) at\nselectable granularities (line or word level) and utilizes timestamp-providing\nText-to-Speech (TTS) for timing synchronization. We demonstrate the system's\neffectiveness through a technical evaluation using a manually annotated slide\ndataset with 1000 samples, finding that LLM-based alignment achieves high\nlocation accuracy (F1 > 92%), significantly outperforming simpler methods,\nespecially on complex, math-heavy content. Furthermore, the calculated\ngeneration cost averages under $1 per hour of video, offering potential savings\nof two orders of magnitude compared to conservative estimates of manual\nproduction costs. This combination of high accuracy and extremely low cost\npositions this approach as a practical and scalable tool for transforming\nstatic slides into effective, visually-guided video lectures.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7aef\u5230\u7aef\u7cfb\u7edf\uff0c\u5c06\u9759\u6001\u5e7b\u706f\u7247\u81ea\u52a8\u8f6c\u5316\u4e3a\u89c6\u9891\u8bb2\u5ea7\uff0c\u901a\u8fc7AI\u751f\u6210\u65c1\u767d\u548c\u52a8\u6001\u89c6\u89c9\u9ad8\u4eae\uff0c\u663e\u8457\u8282\u7701\u65f6\u95f4\u548c\u6210\u672c\u3002", "motivation": "\u5c06\u9759\u6001\u5e7b\u706f\u7247\u8f6c\u5316\u4e3a\u89c6\u9891\u8bb2\u5ea7\u901a\u5e38\u8017\u65f6\u8017\u529b\uff0c\u9700\u8981\u4eba\u5de5\u5f55\u5236\u548c\u89c6\u89c9\u5f15\u5bfc\u3002", "method": "\u7cfb\u7edf\u91c7\u7528\u65b0\u9896\u7684\u9ad8\u4eae\u5bf9\u9f50\u6a21\u5757\uff0c\u7ed3\u5408\u591a\u79cd\u7b56\u7565\uff08\u5982Levenshtein\u8ddd\u79bb\u3001LLM\u8bed\u4e49\u5206\u6790\uff09\u548cTTS\u6280\u672f\uff0c\u5b9e\u73b0\u8bed\u97f3\u4e0e\u5e7b\u706f\u7247\u5185\u5bb9\u7684\u7cbe\u786e\u540c\u6b65\u3002", "result": "\u57281000\u4e2a\u6837\u672c\u7684\u6570\u636e\u96c6\u4e0a\uff0cLLM\u5bf9\u9f50\u65b9\u6cd5\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\uff08F1>92%\uff09\uff0c\u751f\u6210\u6210\u672c\u4f4e\u4e8e\u6bcf\u5c0f\u65f61\u7f8e\u5143\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u9ad8\u6548\u3001\u4f4e\u6210\u672c\uff0c\u4e3a\u9759\u6001\u5e7b\u706f\u7247\u8f6c\u5316\u4e3a\u89c6\u9891\u8bb2\u5ea7\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.02971", "pdf": "https://arxiv.org/pdf/2505.02971", "abs": "https://arxiv.org/abs/2505.02971", "authors": ["Anjila Budathoki", "Manish Dhakal"], "title": "Adversarial Robustness Analysis of Vision-Language Models in Medical Image Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Adversarial attacks have been fairly explored for computer vision and\nvision-language models. However, the avenue of adversarial attack for the\nvision language segmentation models (VLSMs) is still under-explored, especially\nfor medical image analysis.\n  Thus, we have investigated the robustness of VLSMs against adversarial\nattacks for 2D medical images with different modalities with radiology,\nphotography, and endoscopy. The main idea of this project was to assess the\nrobustness of the fine-tuned VLSMs specially in the medical domain setting to\naddress the high risk scenario.\n  First, we have fine-tuned pre-trained VLSMs for medical image segmentation\nwith adapters.\n  Then, we have employed adversarial attacks -- projected gradient descent\n(PGD) and fast gradient sign method (FGSM) -- on that fine-tuned model to\ndetermine its robustness against adversaries.\n  We have reported models' performance decline to analyze the adversaries'\nimpact.\n  The results exhibit significant drops in the DSC and IoU scores after the\nintroduction of these adversaries. Furthermore, we also explored universal\nperturbation but were not able to find for the medical images.\n  \\footnote{https://github.com/anjilab/secure-private-ai}", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u89c6\u89c9\u8bed\u8a00\u5206\u5272\u6a21\u578b\uff08VLSMs\uff09\u5728\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4e2d\u5bf9\u5bf9\u6297\u653b\u51fb\u7684\u9c81\u68d2\u6027\uff0c\u901a\u8fc7\u5fae\u8c03\u6a21\u578b\u5e76\u5e94\u7528PGD\u548cFGSM\u653b\u51fb\uff0c\u53d1\u73b0\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002", "motivation": "\u5bf9\u6297\u653b\u51fb\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u5df2\u6709\u7814\u7a76\uff0c\u4f46\u5728\u533b\u5b66\u56fe\u50cf\u5206\u6790\u7684VLSMs\u4e2d\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u5fae\u8c03\u9884\u8bad\u7ec3\u7684VLSMs\uff0c\u5e76\u5e94\u7528PGD\u548cFGSM\u5bf9\u6297\u653b\u51fb\u8bc4\u4f30\u5176\u9c81\u68d2\u6027\u3002", "result": "\u5bf9\u6297\u653b\u51fb\u5bfc\u81f4DSC\u548cIoU\u5206\u6570\u663e\u8457\u4e0b\u964d\uff0c\u4f46\u672a\u627e\u5230\u9002\u7528\u4e8e\u533b\u5b66\u56fe\u50cf\u7684\u901a\u7528\u6270\u52a8\u3002", "conclusion": "VLSMs\u5728\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4e2d\u5bf9\u5bf9\u6297\u653b\u51fb\u8868\u73b0\u8106\u5f31\uff0c\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u63d0\u5347\u9c81\u68d2\u6027\u3002"}}
{"id": "2505.02847", "pdf": "https://arxiv.org/pdf/2505.02847", "abs": "https://arxiv.org/abs/2505.02847", "authors": ["Bang Zhang", "Ruotian Ma", "Qingxuan Jiang", "Peisong Wang", "Jiaqi Chen", "Zheng Xie", "Xingyu Chen", "Yue Wang", "Fanghua Ye", "Jian Li", "Yifan Yang", "Zhaopeng Tu", "Xiaolong Li"], "title": "Sentient Agent as a Judge: Evaluating Higher-Order Social Cognition in Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "Assessing how well a large language model (LLM) understands human, rather\nthan merely text, remains an open challenge. To bridge the gap, we introduce\nSentient Agent as a Judge (SAGE), an automated evaluation framework that\nmeasures an LLM's higher-order social cognition. SAGE instantiates a Sentient\nAgent that simulates human-like emotional changes and inner thoughts during\ninteraction, providing a more realistic evaluation of the tested model in\nmulti-turn conversations. At every turn, the agent reasons about (i) how its\nemotion changes, (ii) how it feels, and (iii) how it should reply, yielding a\nnumerical emotion trajectory and interpretable inner thoughts. Experiments on\n100 supportive-dialogue scenarios show that the final Sentient emotion score\ncorrelates strongly with Barrett-Lennard Relationship Inventory (BLRI) ratings\nand utterance-level empathy metrics, validating psychological fidelity. We also\nbuild a public Sentient Leaderboard covering 18 commercial and open-source\nmodels that uncovers substantial gaps (up to 4x) between frontier systems\n(GPT-4o-Latest, Gemini2.5-Pro) and earlier baselines, gaps not reflected in\nconventional leaderboards (e.g., Arena). SAGE thus provides a principled,\nscalable and interpretable tool for tracking progress toward genuinely\nempathetic and socially adept language agents.", "AI": {"tldr": "SAGE\u662f\u4e00\u4e2a\u81ea\u52a8\u5316\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u62df\u4eba\u7c7b\u60c5\u611f\u53d8\u5316\u548c\u5185\u5fc3\u601d\u8003\uff0c\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u793e\u4f1a\u8ba4\u77e5\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u8bc4\u4f30LLM\u5bf9\u4eba\u7c7b\u60c5\u611f\u7684\u7406\u89e3\uff0cSAGE\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "SAGE\u901a\u8fc7\u6a21\u62df\u60c5\u611f\u53d8\u5316\u3001\u5185\u5fc3\u601d\u8003\u548c\u56de\u590d\u884c\u4e3a\uff0c\u751f\u6210\u60c5\u611f\u8f68\u8ff9\u548c\u53ef\u89e3\u91ca\u7684\u5185\u5fc3\u6d3b\u52a8\u3002", "result": "\u5b9e\u9a8c\u663e\u793aSAGE\u7684\u60c5\u611f\u8bc4\u5206\u4e0e\u5fc3\u7406\u5b66\u6307\u6807\u9ad8\u5ea6\u76f8\u5173\uff0c\u5e76\u63ed\u793a\u4e86\u524d\u6cbf\u6a21\u578b\u4e0e\u57fa\u7ebf\u6a21\u578b\u7684\u663e\u8457\u5dee\u8ddd\u3002", "conclusion": "SAGE\u4e3a\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\u7684\u793e\u4f1a\u8ba4\u77e5\u80fd\u529b\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u53ef\u89e3\u91ca\u7684\u5de5\u5177\u3002"}}
{"id": "2505.02980", "pdf": "https://arxiv.org/pdf/2505.02980", "abs": "https://arxiv.org/abs/2505.02980", "authors": ["Daniela Ruiz", "Paula Cardenas", "Leonardo Manrique", "Daniela Vega", "Gabriel Mejia", "Pablo Arbelaez"], "title": "Completing Spatial Transcriptomics Data for Gene Expression Prediction Benchmarking", "categories": ["cs.CV"], "comment": "arXiv admin note: substantial text overlap with arXiv:2407.13027", "summary": "Spatial Transcriptomics is a groundbreaking technology that integrates\nhistology images with spatially resolved gene expression profiles. Among the\nvarious Spatial Transcriptomics techniques available, Visium has emerged as the\nmost widely adopted. However, its accessibility is limited by high costs, the\nneed for specialized expertise, and slow clinical integration. Additionally,\ngene capture inefficiencies lead to significant dropout, corrupting acquired\ndata. To address these challenges, the deep learning community has explored the\ngene expression prediction task directly from histology images. Yet,\ninconsistencies in datasets, preprocessing, and training protocols hinder fair\ncomparisons between models. To bridge this gap, we introduce SpaRED, a\nsystematically curated database comprising 26 public datasets, providing a\nstandardized resource for model evaluation. We further propose SpaCKLE, a\nstate-of-the-art transformer-based gene expression completion model that\nreduces mean squared error by over 82.5% compared to existing approaches.\nFinally, we establish the SpaRED benchmark, evaluating eight state-of-the-art\nprediction models on both raw and SpaCKLE-completed data, demonstrating SpaCKLE\nsubstantially improves the results across all the gene expression prediction\nmodels. Altogether, our contributions constitute the most comprehensive\nbenchmark of gene expression prediction from histology images to date and a\nstepping stone for future research on Spatial Transcriptomics.", "AI": {"tldr": "SpaRED\u548cSpaCKLE\u4e3a\u7a7a\u95f4\u8f6c\u5f55\u7ec4\u5b66\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u6570\u636e\u5e93\u548c\u5148\u8fdb\u7684\u57fa\u56e0\u8868\u8fbe\u9884\u6d4b\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u51c6\u786e\u6027\u3002", "motivation": "Visium\u6280\u672f\u6210\u672c\u9ad8\u3001\u6548\u7387\u4f4e\uff0c\u4e14\u6570\u636e\u5b58\u5728\u4e22\u5931\u95ee\u9898\uff0c\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u56e0\u6570\u636e\u4e0d\u4e00\u81f4\u96be\u4ee5\u516c\u5e73\u6bd4\u8f83\u3002", "method": "\u63d0\u51faSpaRED\u6807\u51c6\u5316\u6570\u636e\u5e93\u548cSpaCKLE\u57fa\u4e8eTransformer\u7684\u57fa\u56e0\u8868\u8fbe\u8865\u5168\u6a21\u578b\u3002", "result": "SpaCKLE\u5c06\u5747\u65b9\u8bef\u5dee\u964d\u4f4e82.5%\uff0c\u5e76\u5728SpaRED\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u5347\u6240\u6709\u6a21\u578b\u7684\u9884\u6d4b\u6548\u679c\u3002", "conclusion": "SpaRED\u548cSpaCKLE\u4e3a\u7a7a\u95f4\u8f6c\u5f55\u7ec4\u5b66\u7814\u7a76\u63d0\u4f9b\u4e86\u5168\u9762\u57fa\u51c6\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2505.02850", "pdf": "https://arxiv.org/pdf/2505.02850", "abs": "https://arxiv.org/abs/2505.02850", "authors": ["Nicy Scaria", "Silvester John Joseph Kennedy", "Diksha Seth", "Ananya Thakur", "Deepak Subramani"], "title": "Harnessing Structured Knowledge: A Concept Map-Based Approach for High-Quality Multiple Choice Question Generation with Effective Distractors", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.DB"], "comment": null, "summary": "Generating high-quality MCQs, especially those targeting diverse cognitive\nlevels and incorporating common misconceptions into distractor design, is\ntime-consuming and expertise-intensive, making manual creation impractical at\nscale. Current automated approaches typically generate questions at lower\ncognitive levels and fail to incorporate domain-specific misconceptions. This\npaper presents a hierarchical concept map-based framework that provides\nstructured knowledge to guide LLMs in generating MCQs with distractors. We\nchose high-school physics as our test domain and began by developing a\nhierarchical concept map covering major Physics topics and their\ninterconnections with an efficient database design. Next, through an automated\npipeline, topic-relevant sections of these concept maps are retrieved to serve\nas a structured context for the LLM to generate questions and distractors that\nspecifically target common misconceptions. Lastly, an automated validation is\ncompleted to ensure that the generated MCQs meet the requirements provided. We\nevaluate our framework against two baseline approaches: a base LLM and a\nRAG-based generation. We conducted expert evaluations and student assessments\nof the generated MCQs. Expert evaluation shows that our method significantly\noutperforms the baseline approaches, achieving a success rate of 75.20% in\nmeeting all quality criteria compared to approximately 37% for both baseline\nmethods. Student assessment data reveal that our concept map-driven approach\nachieved a significantly lower guess success rate of 28.05% compared to 37.10%\nfor the baselines, indicating a more effective assessment of conceptual\nunderstanding. The results demonstrate that our concept map-based approach\nenables robust assessment across cognitive levels and instant identification of\nconceptual gaps, facilitating faster feedback loops and targeted interventions\nat scale.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5206\u5c42\u6982\u5ff5\u56fe\u7684\u6846\u67b6\uff0c\u5229\u7528LLM\u751f\u6210\u9ad8\u8d28\u91cfMCQ\uff0c\u9488\u5bf9\u9ad8\u4e2d\u7269\u7406\u9886\u57df\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u624b\u52a8\u751f\u6210\u9ad8\u8d28\u91cfMCQ\u8017\u65f6\u4e14\u4f9d\u8d56\u4e13\u5bb6\u77e5\u8bc6\uff0c\u73b0\u6709\u81ea\u52a8\u5316\u65b9\u6cd5\u65e0\u6cd5\u6ee1\u8db3\u9ad8\u8ba4\u77e5\u6c34\u5e73\u548c\u9886\u57df\u7279\u5b9a\u8bef\u89e3\u7684\u9700\u6c42\u3002", "method": "\u5f00\u53d1\u5206\u5c42\u6982\u5ff5\u56fe\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u6d41\u7a0b\u63d0\u53d6\u76f8\u5173\u5185\u5bb9\u6307\u5bfcLLM\u751f\u6210MCQ\u548c\u5e72\u6270\u9879\uff0c\u5e76\u8fdb\u884c\u81ea\u52a8\u9a8c\u8bc1\u3002", "result": "\u4e13\u5bb6\u8bc4\u4f30\u663e\u793a\u6210\u529f\u7387\u8fbe75.20%\uff0c\u5b66\u751f\u6d4b\u8bd5\u4e2d\u731c\u6d4b\u6210\u529f\u7387\u663e\u8457\u964d\u4f4e\u81f328.05%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u8bc4\u4f30\u8ba4\u77e5\u6c34\u5e73\u5e76\u5feb\u901f\u8bc6\u522b\u6982\u5ff5\u5dee\u8ddd\uff0c\u652f\u6301\u89c4\u6a21\u5316\u53cd\u9988\u548c\u5e72\u9884\u3002"}}
{"id": "2505.03007", "pdf": "https://arxiv.org/pdf/2505.03007", "abs": "https://arxiv.org/abs/2505.03007", "authors": ["Nikolay Safonov", "Alexey Bryncev", "Andrey Moskalenko", "Dmitry Kulikov", "Dmitry Vatolin", "Radu Timofte", "Haibo Lei", "Qifan Gao", "Qing Luo", "Yaqing Li", "Jie Song", "Shaozhe Hao", "Meisong Zheng", "Jingyi Xu", "Chengbin Wu", "Jiahui Liu", "Ying Chen", "Xin Deng", "Mai Xu", "Peipei Liang", "Jie Ma", "Junjie Jin", "Yingxue Pang", "Fangzhou Luo", "Kai Chen", "Shijie Zhao", "Mingyang Wu", "Renjie Li", "Yushen Zuo", "Shengyun Zhong", "Zhengzhong Tu"], "title": "NTIRE 2025 Challenge on UGC Video Enhancement: Methods and Results", "categories": ["cs.CV"], "comment": null, "summary": "This paper presents an overview of the NTIRE 2025 Challenge on UGC Video\nEnhancement. The challenge constructed a set of 150 user-generated content\nvideos without reference ground truth, which suffer from real-world\ndegradations such as noise, blur, faded colors, compression artifacts, etc. The\ngoal of the participants was to develop an algorithm capable of improving the\nvisual quality of such videos. Given the widespread use of UGC on short-form\nvideo platforms, this task holds substantial practical importance. The\nevaluation was based on subjective quality assessment in crowdsourcing,\nobtaining votes from over 8000 assessors. The challenge attracted more than 25\nteams submitting solutions, 7 of which passed the final phase with source code\nverification. The outcomes may provide insights into the state-of-the-art in\nUGC video enhancement and highlight emerging trends and effective strategies in\nthis evolving research area. All data, including the processed videos and\nsubjective comparison votes and scores, is made publicly available at\nhttps://github.com/msu-video-group/NTIRE25_UGC_Video_Enhancement.", "AI": {"tldr": "NTIRE 2025\u6311\u6218\u8d5b\u805a\u7126\u7528\u6237\u751f\u6210\u5185\u5bb9\uff08UGC\uff09\u89c6\u9891\u589e\u5f3a\uff0c\u65e8\u5728\u63d0\u5347\u65e0\u53c2\u8003\u89c6\u9891\u7684\u89c6\u89c9\u8d28\u91cf\uff0c\u5438\u5f15\u4e8625\u652f\u56e2\u961f\u53c2\u4e0e\uff0c\u6700\u7ec87\u652f\u901a\u8fc7\u9a8c\u8bc1\u3002", "motivation": "UGC\u89c6\u9891\u5728\u77ed\u89c6\u9891\u5e73\u53f0\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4f46\u5176\u8d28\u91cf\u5e38\u53d7\u566a\u58f0\u3001\u6a21\u7cca\u7b49\u95ee\u9898\u5f71\u54cd\uff0c\u4e9f\u9700\u6709\u6548\u589e\u5f3a\u65b9\u6cd5\u3002", "method": "\u6311\u6218\u8d5b\u63d0\u4f9b150\u4e2a\u65e0\u53c2\u8003UGC\u89c6\u9891\uff0c\u53c2\u8d5b\u56e2\u961f\u5f00\u53d1\u7b97\u6cd5\u63d0\u5347\u5176\u8d28\u91cf\uff0c\u8bc4\u4f30\u57fa\u4e8e8000\u591a\u540d\u4f17\u5305\u8bc4\u5206\u8005\u7684\u4e3b\u89c2\u8bc4\u5206\u3002", "result": "7\u652f\u56e2\u961f\u901a\u8fc7\u6700\u7ec8\u9a8c\u8bc1\uff0c\u6210\u679c\u63ed\u793a\u4e86UGC\u89c6\u9891\u589e\u5f3a\u7684\u6700\u65b0\u8fdb\u5c55\u548c\u6709\u6548\u7b56\u7565\u3002", "conclusion": "\u6311\u6218\u8d5b\u6210\u679c\u4e3aUGC\u89c6\u9891\u589e\u5f3a\u9886\u57df\u63d0\u4f9b\u4e86\u5b9d\u8d35\u6570\u636e\u548c\u524d\u6cbf\u89c1\u89e3\uff0c\u76f8\u5173\u8d44\u6e90\u5df2\u516c\u5f00\u3002"}}
{"id": "2505.02851", "pdf": "https://arxiv.org/pdf/2505.02851", "abs": "https://arxiv.org/abs/2505.02851", "authors": ["Franklin Zhang", "Sonya Zhang", "Alon Halevy"], "title": "30DayGen: Leveraging LLMs to Create a Content Corpus for Habit Formation", "categories": ["cs.CL", "cs.AI", "cs.CY", "I.2.7; H.3.1; H.3.3"], "comment": "8 pages (main content), 4 figures. Submitted to ACL BEA2025", "summary": "In this paper, we present 30 Day Me, a habit formation application that\nleverages Large Language Models (LLMs) to help users break down their goals\ninto manageable, actionable steps and track their progress. Central to the app\nis the 30DAYGEN system, which generates 3,531 unique 30-day challenges sourced\nfrom over 15K webpages, and enables runtime search of challenge ideas aligned\nwith user-defined goals. We showcase how LLMs can be harnessed to rapidly\nconstruct domain specific content corpora for behavioral and educational\npurposes, and propose a practical pipeline that incorporates effective LLM\nenhanced approaches for content generation and semantic deduplication.", "AI": {"tldr": "30 Day Me\u662f\u4e00\u6b3e\u5229\u7528LLM\u5e2e\u52a9\u7528\u6237\u5206\u89e3\u76ee\u6807\u5e76\u8ddf\u8e2a\u8fdb\u5ea6\u7684\u4e60\u60ef\u517b\u6210\u5e94\u7528\uff0c\u6838\u5fc3\u662f30DAYGEN\u7cfb\u7edf\uff0c\u80fd\u751f\u62103,531\u79cd30\u5929\u6311\u6218\u3002", "motivation": "\u5229\u7528LLM\u5feb\u901f\u6784\u5efa\u9886\u57df\u7279\u5b9a\u5185\u5bb9\u5e93\uff0c\u652f\u6301\u884c\u4e3a\u548c\u6559\u80b2\u76ee\u6807\u3002", "method": "\u901a\u8fc7LLM\u751f\u6210\u5185\u5bb9\u5e76\u8bed\u4e49\u53bb\u91cd\uff0c\u6784\u5efa30DAYGEN\u7cfb\u7edf\u3002", "result": "\u751f\u6210\u4e863,531\u79cd\u72ec\u7279\u768430\u5929\u6311\u6218\uff0c\u652f\u6301\u7528\u6237\u76ee\u6807\u5bf9\u9f50\u3002", "conclusion": "\u5c55\u793a\u4e86LLM\u5728\u5185\u5bb9\u751f\u6210\u548c\u53bb\u91cd\u4e2d\u7684\u5b9e\u7528\u6027\uff0c\u4e3a\u884c\u4e3a\u548c\u6559\u80b2\u5e94\u7528\u63d0\u4f9b\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2505.03012", "pdf": "https://arxiv.org/pdf/2505.03012", "abs": "https://arxiv.org/abs/2505.03012", "authors": ["Saeed Ebrahimi", "Sahar Rahimi", "Ali Dabouei", "Srinjoy Das", "Jeremy M. Dawson", "Nasser M. Nasrabadi"], "title": "GIF: Generative Inspiration for Face Recognition at Scale", "categories": ["cs.CV"], "comment": null, "summary": "Aiming to reduce the computational cost of Softmax in massive label space of\nFace Recognition (FR) benchmarks, recent studies estimate the output using a\nsubset of identities. Although promising, the association between the\ncomputation cost and the number of identities in the dataset remains linear\nonly with a reduced ratio. A shared characteristic among available FR methods\nis the employment of atomic scalar labels during training. Consequently, the\ninput to label matching is through a dot product between the feature vector of\nthe input and the Softmax centroids. Inspired by generative modeling, we\npresent a simple yet effective method that substitutes scalar labels with\nstructured identity code, i.e., a sequence of integers. Specifically, we\npropose a tokenization scheme that transforms atomic scalar labels into\nstructured identity codes. Then, we train an FR backbone to predict the code\nfor each input instead of its scalar label. As a result, the associated\ncomputational cost becomes logarithmic w.r.t. number of identities. We\ndemonstrate the benefits of the proposed method by conducting experiments. In\nparticular, our method outperforms its competitors by 1.52%, and 0.6% at\nTAR@FAR$=1e-4$ on IJB-B and IJB-C, respectively, while transforming the\nassociation between computational cost and the number of identities from linear\nto logarithmic. See code at https://github.com/msed-Ebrahimi/GIF", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u7ed3\u6784\u5316\u8eab\u4efd\u7801\u66ff\u4ee3\u6807\u91cf\u6807\u7b7e\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u4eba\u8138\u8bc6\u522b\u4e2dSoftmax\u7684\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u51cf\u5c11\u5927\u89c4\u6a21\u6807\u7b7e\u7a7a\u95f4\u4e2dSoftmax\u7684\u8ba1\u7b97\u6210\u672c\uff0c\u73b0\u6709\u65b9\u6cd5\u4ec5\u80fd\u7ebf\u6027\u964d\u4f4e\u8ba1\u7b97\u91cf\u3002", "method": "\u5c06\u6807\u91cf\u6807\u7b7e\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u8eab\u4efd\u7801\uff0c\u8bad\u7ec3\u6a21\u578b\u9884\u6d4b\u4ee3\u7801\u800c\u975e\u6807\u91cf\u6807\u7b7e\uff0c\u4f7f\u8ba1\u7b97\u6210\u672c\u4e0e\u8eab\u4efd\u6570\u5448\u5bf9\u6570\u5173\u7cfb\u3002", "result": "\u5728IJB-B\u548cIJB-C\u4e0a\u5206\u522b\u63d0\u53471.52%\u548c0.6%\u7684TAR@FAR=1e-4\u6027\u80fd\uff0c\u8ba1\u7b97\u6210\u672c\u4ece\u7ebf\u6027\u964d\u81f3\u5bf9\u6570\u3002", "conclusion": "\u7ed3\u6784\u5316\u8eab\u4efd\u7801\u65b9\u6cd5\u6709\u6548\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u5e76\u63d0\u5347\u4e86\u6027\u80fd\u3002"}}
{"id": "2505.02854", "pdf": "https://arxiv.org/pdf/2505.02854", "abs": "https://arxiv.org/abs/2505.02854", "authors": ["Masumi Morishige", "Ryo Koshihara"], "title": "Ensuring Reproducibility in Generative AI Systems for General Use Cases: A Framework for Regression Testing and Open Datasets", "categories": ["cs.CL", "cs.AI"], "comment": "15 pages, 10 figures", "summary": "Reproducibility and reliability remain pressing challenges for generative AI\nsystems whose behavior can drift with each model update or prompt revision. We\nintroduce GPR-bench, a lightweight, extensible benchmark that operationalizes\nregression testing for general purpose use cases. GPR-bench couples an open,\nbilingual (English and Japanese) dataset covering eight task categories (e.g.,\ntext generation, code generation, and information retrieval) and 10 scenarios\nin each task categories (80 total test cases for each language) with an\nautomated evaluation pipeline that employs \"LLM-as-a-Judge\" scoring of\ncorrectness and conciseness. Experiments across three recent model versions -\ngpt-4o-mini, o3-mini, and o4-mini - and two prompt configurations (default\nversus concise-writing instruction) reveal heterogeneous quality. Our results\nshow that newer models generally improve correctness, but the differences are\nmodest and not statistically significant, suggesting that GPR-bench may not be\nsufficiently challenging to differentiate between recent model versions. In\ncontrast, the concise-writing instruction significantly enhances conciseness\n(+12.37 pp, Mann-Whitney U test: p < 0.001, effect size r = 0.2995) with\nminimal degradations on accuracy (-1.7 pp), demonstrating the effectiveness of\nprompt engineering. Released under the MIT License, GPR- bench lowers the\nbarrier to initiating reproducibility monitoring and provides a foundation for\ncommunity-driven extensions, while also raising important considerations about\nbenchmark design for rapidly evolving language models.", "AI": {"tldr": "GPR-bench\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u3001\u53ef\u6269\u5c55\u7684\u57fa\u51c6\u6d4b\u8bd5\u5de5\u5177\uff0c\u7528\u4e8e\u8bc4\u4f30\u751f\u6210\u5f0fAI\u7cfb\u7edf\u7684\u53ef\u91cd\u590d\u6027\u548c\u53ef\u9760\u6027\uff0c\u8986\u76d6\u591a\u4efb\u52a1\u548c\u591a\u8bed\u8a00\u573a\u666f\u3002", "motivation": "\u89e3\u51b3\u751f\u6210\u5f0fAI\u7cfb\u7edf\u56e0\u6a21\u578b\u66f4\u65b0\u6216\u63d0\u793a\u8c03\u6574\u5bfc\u81f4\u884c\u4e3a\u6f02\u79fb\u7684\u53ef\u91cd\u590d\u6027\u548c\u53ef\u9760\u6027\u95ee\u9898\u3002", "method": "\u5f00\u53d1GPR-bench\uff0c\u5305\u542b\u53cc\u8bed\u6570\u636e\u96c6\uff08\u82f1\u8bed\u548c\u65e5\u8bed\uff09\u548c\u81ea\u52a8\u5316\u8bc4\u4f30\u6d41\u7a0b\uff0c\u4f7f\u7528\u201cLLM-as-a-Judge\u201d\u8bc4\u5206\u3002", "result": "\u65b0\u6a21\u578b\u5728\u6b63\u786e\u6027\u4e0a\u7565\u6709\u63d0\u5347\u4f46\u5dee\u5f02\u4e0d\u663e\u8457\uff0c\u7b80\u6d01\u5199\u4f5c\u63d0\u793a\u663e\u8457\u63d0\u9ad8\u7b80\u6d01\u6027\u3002", "conclusion": "GPR-bench\u4e3a\u793e\u533a\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u57fa\u51c6\u6d4b\u8bd5\u5de5\u5177\uff0c\u4f46\u9700\u6ce8\u610f\u5176\u6311\u6218\u6027\u4e0d\u8db3\u7684\u95ee\u9898\u3002"}}
{"id": "2505.03018", "pdf": "https://arxiv.org/pdf/2505.03018", "abs": "https://arxiv.org/abs/2505.03018", "authors": ["Aurora Rofena", "Arianna Manchia", "Claudia Lucia Piccolo", "Bruno Beomonte Zobel", "Paolo Soda", "Valerio Guarrasi"], "title": "Lesion-Aware Generative Artificial Intelligence for Virtual Contrast-Enhanced Mammography in Breast Cancer", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Contrast-Enhanced Spectral Mammography (CESM) is a dual-energy mammographic\ntechnique that improves lesion visibility through the administration of an\niodinated contrast agent. It acquires both a low-energy image, comparable to\nstandard mammography, and a high-energy image, which are then combined to\nproduce a dual-energy subtracted image highlighting lesion contrast\nenhancement. While CESM offers superior diagnostic accuracy compared to\nstandard mammography, its use entails higher radiation exposure and potential\nside effects associated with the contrast medium. To address these limitations,\nwe propose Seg-CycleGAN, a generative deep learning framework for Virtual\nContrast Enhancement in CESM. The model synthesizes high-fidelity dual-energy\nsubtracted images from low-energy images, leveraging lesion segmentation maps\nto guide the generative process and improve lesion reconstruction. Building\nupon the standard CycleGAN architecture, Seg-CycleGAN introduces localized loss\nterms focused on lesion areas, enhancing the synthesis of diagnostically\nrelevant regions. Experiments on the CESM@UCBM dataset demonstrate that\nSeg-CycleGAN outperforms the baseline in terms of PSNR and SSIM, while\nmaintaining competitive MSE and VIF. Qualitative evaluations further confirm\nimproved lesion fidelity in the generated images. These results suggest that\nsegmentation-aware generative models offer a viable pathway toward\ncontrast-free CESM alternatives.", "AI": {"tldr": "Seg-CycleGAN\u662f\u4e00\u79cd\u751f\u6210\u6027\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u5bf9\u6bd4\u589e\u5f3a\u5149\u8c31\u4e73\u817a\u6444\u5f71\uff08CESM\uff09\u4e2d\u5b9e\u73b0\u865a\u62df\u5bf9\u6bd4\u589e\u5f3a\uff0c\u901a\u8fc7\u5408\u6210\u9ad8\u8d28\u91cf\u7684\u53cc\u80fd\u91cf\u51cf\u5f71\u56fe\u50cf\uff0c\u51cf\u5c11\u8f90\u5c04\u548c\u5bf9\u6bd4\u5242\u7684\u4f7f\u7528\u3002", "motivation": "CESM\u867d\u7136\u8bca\u65ad\u51c6\u786e\u6027\u9ad8\uff0c\u4f46\u5b58\u5728\u8f90\u5c04\u548c\u5bf9\u6bd4\u5242\u526f\u4f5c\u7528\u7684\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u65e0\u5bf9\u6bd4\u5242\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u63d0\u51faSeg-CycleGAN\uff0c\u57fa\u4e8eCycleGAN\u67b6\u6784\uff0c\u5f15\u5165\u75c5\u7076\u5206\u5272\u56fe\u6307\u5bfc\u751f\u6210\u8fc7\u7a0b\uff0c\u5e76\u589e\u52a0\u5c40\u90e8\u635f\u5931\u9879\u4ee5\u4f18\u5316\u75c5\u7076\u533a\u57df\u7684\u91cd\u5efa\u3002", "result": "\u5728CESM@UCBM\u6570\u636e\u96c6\u4e0a\uff0cSeg-CycleGAN\u5728PSNR\u548cSSIM\u4e0a\u4f18\u4e8e\u57fa\u7ebf\uff0c\u540c\u65f6\u4fdd\u6301\u7ade\u4e89\u529b\u7684MSE\u548cVIF\uff0c\u5b9a\u6027\u8bc4\u4f30\u4e5f\u663e\u793a\u75c5\u7076\u4fdd\u771f\u5ea6\u63d0\u9ad8\u3002", "conclusion": "Seg-CycleGAN\u4e3a\u65e0\u5bf9\u6bd4\u5242\u7684CESM\u66ff\u4ee3\u65b9\u6848\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\u3002"}}
{"id": "2505.02858", "pdf": "https://arxiv.org/pdf/2505.02858", "abs": "https://arxiv.org/abs/2505.02858", "authors": ["Henry Tari", "Nojus Sereiva", "Rishabh Kaushal", "Thales Bertaglia", "Adriana Iamnitchi"], "title": "Towards High-Fidelity Synthetic Multi-platform Social Media Datasets via Large Language Models", "categories": ["cs.CL", "cs.CY"], "comment": "arXiv admin note: text overlap with arXiv:2407.08323", "summary": "Social media datasets are essential for research on a variety of topics, such\nas disinformation, influence operations, hate speech detection, or influencer\nmarketing practices. However, access to social media datasets is often\nconstrained due to costs and platform restrictions. Acquiring datasets that\nspan multiple platforms, which is crucial for understanding the digital\necosystem, is particularly challenging. This paper explores the potential of\nlarge language models to create lexically and semantically relevant social\nmedia datasets across multiple platforms, aiming to match the quality of real\ndata. We propose multi-platform topic-based prompting and employ various\nlanguage models to generate synthetic data from two real datasets, each\nconsisting of posts from three different social media platforms. We assess the\nlexical and semantic properties of the synthetic data and compare them with\nthose of the real data. Our empirical findings show that using large language\nmodels to generate synthetic multi-platform social media data is promising,\ndifferent language models perform differently in terms of fidelity, and a\npost-processing approach might be needed for generating high-fidelity synthetic\ndatasets for research. In addition to the empirical evaluation of three state\nof the art large language models, our contributions include new fidelity\nmetrics specific to multi-platform social media datasets.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u591a\u5e73\u53f0\u793e\u4ea4\u5a92\u4f53\u5408\u6210\u6570\u636e\u7684\u6f5c\u529b\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e3b\u9898\u7684\u63d0\u793a\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u53ef\u884c\u6027\u3002", "motivation": "\u7531\u4e8e\u6210\u672c\u548c\u5e73\u53f0\u9650\u5236\uff0c\u83b7\u53d6\u591a\u5e73\u53f0\u793e\u4ea4\u5a92\u4f53\u6570\u636e\u96c6\u56f0\u96be\uff0c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u5408\u6210\u6570\u636e\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u91c7\u7528\u591a\u5e73\u53f0\u4e3b\u9898\u63d0\u793a\u65b9\u6cd5\uff0c\u5229\u7528\u4e0d\u540c\u8bed\u8a00\u6a21\u578b\u751f\u6210\u5408\u6210\u6570\u636e\uff0c\u5e76\u4e0e\u771f\u5b9e\u6570\u636e\u5bf9\u6bd4\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u591a\u5e73\u53f0\u5408\u6210\u6570\u636e\u5177\u6709\u6f5c\u529b\uff0c\u4e0d\u540c\u6a21\u578b\u8868\u73b0\u5404\u5f02\uff0c\u540e\u5904\u7406\u53ef\u80fd\u63d0\u5347\u6570\u636e\u4fdd\u771f\u5ea6\u3002", "conclusion": "\u7814\u7a76\u4e3a\u591a\u5e73\u53f0\u793e\u4ea4\u5a92\u4f53\u6570\u636e\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86\u9488\u5bf9\u6027\u7684\u4fdd\u771f\u5ea6\u8bc4\u4f30\u6307\u6807\u3002"}}
{"id": "2505.03039", "pdf": "https://arxiv.org/pdf/2505.03039", "abs": "https://arxiv.org/abs/2505.03039", "authors": ["Yuezhou Zhang", "Amos A. Folarin", "Callum Stewart", "Heet Sankesara", "Yatharth Ranjan", "Pauline Conde", "Akash Roy Choudhury", "Shaoxiong Sun", "Zulqarnain Rashid", "Richard J. B. Dobson"], "title": "An Explainable Anomaly Detection Framework for Monitoring Depression and Anxiety Using Consumer Wearable Devices", "categories": ["cs.CV", "stat.AP"], "comment": null, "summary": "Continuous monitoring of behavior and physiology via wearable devices offers\na novel, objective method for the early detection of worsening depression and\nanxiety. In this study, we present an explainable anomaly detection framework\nthat identifies clinically meaningful increases in symptom severity using\nconsumer-grade wearable data. Leveraging data from 2,023 participants with\ndefined healthy baselines, our LSTM autoencoder model learned normal health\npatterns of sleep duration, step count, and resting heart rate. Anomalies were\nflagged when self-reported depression or anxiety scores increased by >=5 points\n(a threshold considered clinically significant). The model achieved an adjusted\nF1-score of 0.80 (precision = 0.73, recall = 0.88) in detecting 393\nsymptom-worsening episodes across 341 participants, with higher performance\nobserved for episodes involving concurrent depression and anxiety escalation\n(F1 = 0.84) and for more pronounced symptom changes (>=10-point increases, F1 =\n0.85). Model interpretability was supported by SHAP-based analysis, which\nidentified resting heart rate as the most influential feature in 71.4\npercentage of detected anomalies, followed by physical activity and sleep.\nTogether, our findings highlight the potential of explainable anomaly detection\nto enable personalized, scalable, and proactive mental health monitoring in\nreal-world settings.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53ef\u7a7f\u6234\u8bbe\u5907\u7684\u53ef\u89e3\u91ca\u5f02\u5e38\u68c0\u6d4b\u6846\u67b6\uff0c\u7528\u4e8e\u65e9\u671f\u68c0\u6d4b\u6291\u90c1\u548c\u7126\u8651\u75c7\u72b6\u7684\u6076\u5316\u3002", "motivation": "\u901a\u8fc7\u53ef\u7a7f\u6234\u8bbe\u5907\u8fde\u7eed\u76d1\u6d4b\u884c\u4e3a\u548c\u751f\u7406\u6570\u636e\uff0c\u4e3a\u65e9\u671f\u53d1\u73b0\u6291\u90c1\u548c\u7126\u8651\u75c7\u72b6\u6076\u5316\u63d0\u4f9b\u5ba2\u89c2\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528LSTM\u81ea\u7f16\u7801\u5668\u6a21\u578b\uff0c\u57fa\u4e8e2,023\u540d\u53c2\u4e0e\u8005\u7684\u5065\u5eb7\u57fa\u7ebf\u6570\u636e\uff0c\u5b66\u4e60\u7761\u7720\u65f6\u957f\u3001\u6b65\u6570\u548c\u9759\u606f\u5fc3\u7387\u7684\u6b63\u5e38\u6a21\u5f0f\uff0c\u5e76\u5728\u75c7\u72b6\u8bc4\u5206\u589e\u52a0\u22655\u5206\u65f6\u6807\u8bb0\u5f02\u5e38\u3002", "result": "\u6a21\u578b\u5728\u68c0\u6d4b\u75c7\u72b6\u6076\u5316\u65f6\u7684\u8c03\u6574F1\u5206\u6570\u4e3a0.80\uff0c\u9759\u606f\u5fc3\u7387\u662f\u6700\u5177\u5f71\u54cd\u529b\u7684\u7279\u5f81\u3002", "conclusion": "\u7814\u7a76\u5c55\u793a\u4e86\u53ef\u89e3\u91ca\u5f02\u5e38\u68c0\u6d4b\u5728\u4e2a\u6027\u5316\u3001\u53ef\u6269\u5c55\u548c\u4e3b\u52a8\u5fc3\u7406\u5065\u5eb7\u76d1\u6d4b\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.02859", "pdf": "https://arxiv.org/pdf/2505.02859", "abs": "https://arxiv.org/abs/2505.02859", "authors": ["Jonas Bokstaller", "Julia Altheimer", "Julian Dormehl", "Alina Buss", "Jasper Wiltfang", "Johannes Schneider", "Maximilian R\u00f6glinger"], "title": "Enhancing ML Model Interpretability: Leveraging Fine-Tuned Large Language Models for Better Understanding of AI", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Across various sectors applications of eXplainableAI (XAI) gained momentum as\nthe increasing black-boxedness of prevailing Machine Learning (ML) models\nbecame apparent. In parallel, Large Language Models (LLMs) significantly\ndeveloped in their abilities to understand human language and complex patterns.\nBy combining both, this paper presents a novel reference architecture for the\ninterpretation of XAI through an interactive chatbot powered by a fine-tuned\nLLM. We instantiate the reference architecture in the context of\nState-of-Health (SoH) prediction for batteries and validate its design in\nmultiple evaluation and demonstration rounds. The evaluation indicates that the\nimplemented prototype enhances the human interpretability of ML, especially for\nusers with less experience with XAI.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5fae\u8c03LLM\u7684\u4ea4\u4e92\u5f0f\u804a\u5929\u673a\u5668\u4eba\u67b6\u6784\uff0c\u7528\u4e8e\u89e3\u91caXAI\uff0c\u5e76\u5728\u7535\u6c60\u5065\u5eb7\u72b6\u6001\u9884\u6d4b\u4e2d\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u968f\u7740\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u9ed1\u7bb1\u7279\u6027\u65e5\u76ca\u660e\u663e\uff0cXAI\u7684\u5e94\u7528\u9700\u6c42\u589e\u52a0\uff0c\u540c\u65f6LLM\u5728\u7406\u89e3\u4eba\u7c7b\u8bed\u8a00\u548c\u590d\u6742\u6a21\u5f0f\u65b9\u9762\u53d6\u5f97\u8fdb\u5c55\uff0c\u7ed3\u5408\u4e24\u8005\u63d0\u5347XAI\u7684\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u53c2\u8003\u67b6\u6784\uff0c\u901a\u8fc7\u5fae\u8c03\u7684LLM\u9a71\u52a8\u7684\u4ea4\u4e92\u5f0f\u804a\u5929\u673a\u5668\u4eba\u89e3\u91caXAI\uff0c\u5e76\u5728\u7535\u6c60SoH\u9884\u6d4b\u4e2d\u5b9e\u4f8b\u5316\u8be5\u67b6\u6784\u3002", "result": "\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u539f\u578b\u663e\u8457\u63d0\u5347\u4e86ML\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u5c24\u5176\u5bf9XAI\u7ecf\u9a8c\u8f83\u5c11\u7684\u7528\u6237\u3002", "conclusion": "\u7ed3\u5408LLM\u7684\u4ea4\u4e92\u5f0fXAI\u89e3\u91ca\u67b6\u6784\u6709\u6548\u63d0\u5347\u4e86\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u5c24\u5176\u5728\u7279\u5b9a\u9886\u57df\uff08\u5982\u7535\u6c60\u5065\u5eb7\u9884\u6d4b\uff09\u4e2d\u8868\u73b0\u826f\u597d\u3002"}}
{"id": "2505.03093", "pdf": "https://arxiv.org/pdf/2505.03093", "abs": "https://arxiv.org/abs/2505.03093", "authors": ["Siming He", "Zachary Osman", "Fernando Cladera", "Dexter Ong", "Nitant Rai", "Patrick Corey Green", "Vijay Kumar", "Pratik Chaudhari"], "title": "Estimating the Diameter at Breast Height of Trees in a Forest With a Single 360 Camera", "categories": ["cs.CV"], "comment": null, "summary": "Forest inventories rely on accurate measurements of the diameter at breast\nheight (DBH) for ecological monitoring, resource management, and carbon\naccounting. While LiDAR-based techniques can achieve centimeter-level\nprecision, they are cost-prohibitive and operationally complex. We present a\nlow-cost alternative that only needs a consumer-grade 360 video camera. Our\nsemi-automated pipeline comprises of (i) a dense point cloud reconstruction\nusing Structure from Motion (SfM) photogrammetry software called Agisoft\nMetashape, (ii) semantic trunk segmentation by projecting Grounded Segment\nAnything (SAM) masks onto the 3D cloud, and (iii) a robust RANSAC-based\ntechnique to estimate cross section shape and DBH. We introduce an interactive\nvisualization tool for inspecting segmented trees and their estimated DBH. On\n61 acquisitions of 43 trees under a variety of conditions, our method attains\nmedian absolute relative errors of 5-9% with respect to \"ground-truth\" manual\nmeasurements. This is only 2-4% higher than LiDAR-based estimates, while\nemploying a single 360 camera that costs orders of magnitude less, requires\nminimal setup, and is widely available.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u4f4e\u6210\u672c\u3001\u57fa\u4e8e360\u76f8\u673a\u7684\u534a\u81ea\u52a8\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u6d4b\u91cf\u6811\u6728\u80f8\u5f84\uff08DBH\uff09\uff0c\u7cbe\u5ea6\u63a5\u8fd1LiDAR\u6280\u672f\u3002", "motivation": "LiDAR\u6280\u672f\u867d\u7136\u7cbe\u5ea6\u9ad8\uff0c\u4f46\u6210\u672c\u6602\u8d35\u4e14\u64cd\u4f5c\u590d\u6742\uff0c\u9700\u8981\u4e00\u79cd\u4f4e\u6210\u672c\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u4f7f\u7528Agisoft Metashape\u8fdb\u884c\u70b9\u4e91\u91cd\u5efa\uff0c\u7ed3\u5408Grounded SAM\u8fdb\u884c\u6811\u5e72\u5206\u5272\uff0c\u5e76\u901a\u8fc7RANSAC\u6280\u672f\u4f30\u8ba1DBH\u3002", "result": "\u572861\u6b21\u6d4b\u91cf\u4e2d\uff0c\u76f8\u5bf9\u8bef\u5dee\u4e3a5-9%\uff0c\u4ec5\u6bd4LiDAR\u9ad82-4%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u672c\u4f4e\u3001\u64cd\u4f5c\u7b80\u5355\uff0c\u9002\u5408\u5e7f\u6cdb\u4f7f\u7528\u3002"}}
{"id": "2505.02862", "pdf": "https://arxiv.org/pdf/2505.02862", "abs": "https://arxiv.org/abs/2505.02862", "authors": ["Haoming Yang", "Ke Ma", "Xiaojun Jia", "Yingfei Sun", "Qianqian Xu", "Qingming Huang"], "title": "Cannot See the Forest for the Trees: Invoking Heuristics and Biases to Elicit Irrational Choices of LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Despite the remarkable performance of Large Language Models (LLMs), they\nremain vulnerable to jailbreak attacks, which can compromise their safety\nmechanisms. Existing studies often rely on brute-force optimization or manual\ndesign, failing to uncover potential risks in real-world scenarios. To address\nthis, we propose a novel jailbreak attack framework, ICRT, inspired by\nheuristics and biases in human cognition. Leveraging the simplicity effect, we\nemploy cognitive decomposition to reduce the complexity of malicious prompts.\nSimultaneously, relevance bias is utilized to reorganize prompts, enhancing\nsemantic alignment and inducing harmful outputs effectively. Furthermore, we\nintroduce a ranking-based harmfulness evaluation metric that surpasses the\ntraditional binary success-or-failure paradigm by employing ranking aggregation\nmethods such as Elo, HodgeRank, and Rank Centrality to comprehensively quantify\nthe harmfulness of generated content. Experimental results show that our\napproach consistently bypasses mainstream LLMs' safety mechanisms and generates\nhigh-risk content, providing insights into jailbreak attack risks and\ncontributing to stronger defense strategies.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4eba\u7c7b\u8ba4\u77e5\u542f\u53d1\u7684\u65b0\u6846\u67b6ICRT\uff0c\u7528\u4e8e\u7ed5\u8fc7\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5b89\u5168\u673a\u5236\uff0c\u901a\u8fc7\u8ba4\u77e5\u5206\u89e3\u548c\u76f8\u5173\u6027\u504f\u89c1\u4f18\u5316\u6076\u610f\u63d0\u793a\uff0c\u5e76\u5f15\u5165\u6392\u540d\u8bc4\u4f30\u6307\u6807\u91cf\u5316\u5371\u5bb3\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4f9d\u8d56\u66b4\u529b\u4f18\u5316\u6216\u624b\u52a8\u8bbe\u8ba1\uff0c\u672a\u80fd\u63ed\u793a\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u6f5c\u5728\u98ce\u9669\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u6765\u8bc4\u4f30\u548c\u653b\u51fb\u6a21\u578b\u7684\u5b89\u5168\u673a\u5236\u3002", "method": "\u63d0\u51faICRT\u6846\u67b6\uff0c\u5229\u7528\u8ba4\u77e5\u5206\u89e3\u7b80\u5316\u6076\u610f\u63d0\u793a\uff0c\u901a\u8fc7\u76f8\u5173\u6027\u504f\u89c1\u91cd\u7ec4\u63d0\u793a\u4ee5\u589e\u5f3a\u8bed\u4e49\u5bf9\u9f50\uff0c\u5e76\u91c7\u7528\u6392\u540d\u805a\u5408\u65b9\u6cd5\uff08\u5982Elo\u3001HodgeRank\u7b49\uff09\u8bc4\u4f30\u5371\u5bb3\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cICRT\u80fd\u6709\u6548\u7ed5\u8fc7\u4e3b\u6d41LLM\u7684\u5b89\u5168\u673a\u5236\uff0c\u751f\u6210\u9ad8\u98ce\u9669\u5185\u5bb9\uff0c\u63ed\u793a\u4e86\u653b\u51fb\u98ce\u9669\u5e76\u4e3a\u9632\u5fa1\u7b56\u7565\u63d0\u4f9b\u53c2\u8003\u3002", "conclusion": "ICRT\u6846\u67b6\u4e3a\u7406\u89e3jailbreak\u653b\u51fb\u98ce\u9669\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u6709\u52a9\u4e8e\u5f00\u53d1\u66f4\u5f3a\u5927\u7684\u9632\u5fa1\u673a\u5236\u3002"}}
{"id": "2505.03097", "pdf": "https://arxiv.org/pdf/2505.03097", "abs": "https://arxiv.org/abs/2505.03097", "authors": ["Lei Wang", "Senmao Li", "Fei Yang", "Jianye Wang", "Ziheng Zhang", "Yuhan Liu", "Yaxing Wang", "Jian Yang"], "title": "Not All Parameters Matter: Masking Diffusion Models for Enhancing Generation Ability", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "The diffusion models, in early stages focus on constructing basic image\nstructures, while the refined details, including local features and textures,\nare generated in later stages. Thus the same network layers are forced to learn\nboth structural and textural information simultaneously, significantly\ndiffering from the traditional deep learning architectures (e.g., ResNet or\nGANs) which captures or generates the image semantic information at different\nlayers. This difference inspires us to explore the time-wise diffusion models.\nWe initially investigate the key contributions of the U-Net parameters to the\ndenoising process and identify that properly zeroing out certain parameters\n(including large parameters) contributes to denoising, substantially improving\nthe generation quality on the fly. Capitalizing on this discovery, we propose a\nsimple yet effective method-termed ``MaskUNet''- that enhances generation\nquality with negligible parameter numbers. Our method fully leverages timestep-\nand sample-dependent effective U-Net parameters. To optimize MaskUNet, we offer\ntwo fine-tuning strategies: a training-based approach and a training-free\napproach, including tailored networks and optimization functions. In zero-shot\ninference on the COCO dataset, MaskUNet achieves the best FID score and further\ndemonstrates its effectiveness in downstream task evaluations. Project page:\nhttps://gudaochangsheng.github.io/MaskUnet-Page/", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMaskUNet\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574U-Net\u53c2\u6570\uff08\u5305\u62ec\u5f52\u96f6\u67d0\u4e9b\u53c2\u6570\uff09\u6765\u63d0\u5347\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u8d28\u91cf\uff0c\u65e0\u9700\u589e\u52a0\u989d\u5916\u53c2\u6570\u3002", "motivation": "\u4f20\u7edf\u6269\u6563\u6a21\u578b\u5728\u540c\u4e00\u7f51\u7edc\u5c42\u4e2d\u540c\u65f6\u5b66\u4e60\u7ed3\u6784\u548c\u7eb9\u7406\u4fe1\u606f\uff0c\u8fd9\u4e0e\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\uff08\u5982ResNet\u6216GANs\uff09\u4e0d\u540c\uff0c\u56e0\u6b64\u63a2\u7d22\u65f6\u95f4\u76f8\u5173\u7684\u6269\u6563\u6a21\u578b\u3002", "method": "\u63d0\u51faMaskUNet\u65b9\u6cd5\uff0c\u5229\u7528\u65f6\u95f4\u6b65\u548c\u6837\u672c\u4f9d\u8d56\u7684\u6709\u6548U-Net\u53c2\u6570\uff0c\u63d0\u4f9b\u4e24\u79cd\u5fae\u8c03\u7b56\u7565\uff1a\u57fa\u4e8e\u8bad\u7ec3\u7684\u65b9\u6cd5\u548c\u65e0\u8bad\u7ec3\u65b9\u6cd5\u3002", "result": "\u5728COCO\u6570\u636e\u96c6\u4e0a\u96f6\u6837\u672c\u63a8\u7406\u4e2d\uff0cMaskUNet\u53d6\u5f97\u4e86\u6700\u4f73FID\u5206\u6570\uff0c\u5e76\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "MaskUNet\u901a\u8fc7\u4f18\u5316U-Net\u53c2\u6570\u52a8\u6001\u8c03\u6574\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u8d28\u91cf\uff0c\u4e14\u65b9\u6cd5\u7b80\u5355\u9ad8\u6548\u3002"}}
{"id": "2505.02865", "pdf": "https://arxiv.org/pdf/2505.02865", "abs": "https://arxiv.org/abs/2505.02865", "authors": ["Zhihai Wang", "Jie Wang", "Jilai Pan", "Xilin Xia", "Huiling Zhen", "Mingxuan Yuan", "Jianye Hao", "Feng Wu"], "title": "Accelerating Large Language Model Reasoning via Speculative Search", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ICML2025", "summary": "Tree-search-based reasoning methods have significantly enhanced the reasoning\ncapability of large language models (LLMs) by facilitating the exploration of\nmultiple intermediate reasoning steps, i.e., thoughts. However, these methods\nsuffer from substantial inference latency, as they have to generate numerous\nreasoning thoughts, severely limiting LLM applicability. To address this\nchallenge, we propose a novel Speculative Search (SpecSearch) framework that\nsignificantly accelerates LLM reasoning by optimizing thought generation.\nSpecifically, SpecSearch utilizes a small model to strategically collaborate\nwith a large model at both thought and token levels, efficiently generating\nhigh-quality reasoning thoughts. The major pillar of SpecSearch is a novel\nquality-preserving rejection mechanism, which effectively filters out thoughts\nwhose quality falls below that of the large model's outputs. Moreover, we show\nthat SpecSearch preserves comparable reasoning quality to the large model.\nExperiments on both the Qwen and Llama models demonstrate that SpecSearch\nsignificantly outperforms state-of-the-art approaches, achieving up to\n2.12$\\times$ speedup with comparable reasoning quality.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSpecSearch\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u4f18\u5316\u601d\u7ef4\u751f\u6210\u663e\u8457\u52a0\u901fLLM\u63a8\u7406\uff0c\u540c\u65f6\u4fdd\u6301\u63a8\u7406\u8d28\u91cf\u3002", "motivation": "\u89e3\u51b3\u57fa\u4e8e\u6811\u641c\u7d22\u7684\u63a8\u7406\u65b9\u6cd5\u56e0\u751f\u6210\u5927\u91cf\u63a8\u7406\u601d\u7ef4\u5bfc\u81f4\u7684\u9ad8\u5ef6\u8fdf\u95ee\u9898\uff0c\u9650\u5236LLM\u7684\u9002\u7528\u6027\u3002", "method": "\u5229\u7528\u5c0f\u6a21\u578b\u4e0e\u5927\u6a21\u578b\u5728\u601d\u7ef4\u548c\u6807\u8bb0\u7ea7\u522b\u6218\u7565\u534f\u4f5c\uff0c\u7ed3\u5408\u8d28\u91cf\u4fdd\u6301\u62d2\u7edd\u673a\u5236\u8fc7\u6ee4\u4f4e\u8d28\u91cf\u601d\u7ef4\u3002", "result": "\u5728Qwen\u548cLlama\u6a21\u578b\u4e0a\u5b9e\u9a8c\uff0cSpecSearch\u6bd4\u73b0\u6709\u65b9\u6cd5\u5feb2.12\u500d\uff0c\u4e14\u63a8\u7406\u8d28\u91cf\u76f8\u5f53\u3002", "conclusion": "SpecSearch\u6709\u6548\u52a0\u901fLLM\u63a8\u7406\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u8d28\u91cf\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2505.03113", "pdf": "https://arxiv.org/pdf/2505.03113", "abs": "https://arxiv.org/abs/2505.03113", "authors": ["Zherui Zhang", "Rongtao Xu", "Jie Zhou", "Changwei Wang", "Xingtian Pei", "Wenhao Xu", "Jiguang Zhang", "Li Guo", "Longxiang Gao", "Wenbo Xu", "Shibiao Xu"], "title": "Image Recognition with Online Lightweight Vision Transformer: A Survey", "categories": ["cs.CV"], "comment": null, "summary": "The Transformer architecture has achieved significant success in natural\nlanguage processing, motivating its adaptation to computer vision tasks. Unlike\nconvolutional neural networks, vision transformers inherently capture\nlong-range dependencies and enable parallel processing, yet lack inductive\nbiases and efficiency benefits, facing significant computational and memory\nchallenges that limit its real-world applicability. This paper surveys various\nonline strategies for generating lightweight vision transformers for image\nrecognition, focusing on three key areas: Efficient Component Design, Dynamic\nNetwork, and Knowledge Distillation. We evaluate the relevant exploration for\neach topic on the ImageNet-1K benchmark, analyzing trade-offs among precision,\nparameters, throughput, and more to highlight their respective advantages,\ndisadvantages, and flexibility. Finally, we propose future research directions\nand potential challenges in the lightweighting of vision transformers with the\naim of inspiring further exploration and providing practical guidance for the\ncommunity. Project Page: https://github.com/ajxklo/Lightweight-VIT", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u8f7b\u91cf\u5316\u89c6\u89c9Transformer\u7684\u5728\u7ebf\u7b56\u7565\uff0c\u805a\u7126\u9ad8\u6548\u7ec4\u4ef6\u8bbe\u8ba1\u3001\u52a8\u6001\u7f51\u7edc\u548c\u77e5\u8bc6\u84b8\u998f\uff0c\u8bc4\u4f30\u4e86\u5176\u5728ImageNet-1K\u4e0a\u7684\u8868\u73b0\uff0c\u5e76\u63a2\u8ba8\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "Transformer\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4e2d\u7684\u6210\u529f\u6fc0\u53d1\u4e86\u5176\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\uff0c\u4f46\u5176\u7f3a\u4e4f\u5f52\u7eb3\u504f\u7f6e\u548c\u6548\u7387\u4f18\u52bf\uff0c\u9762\u4e34\u8ba1\u7b97\u548c\u5185\u5b58\u6311\u6218\u3002", "method": "\u7814\u7a76\u4e86\u4e09\u79cd\u8f7b\u91cf\u5316\u7b56\u7565\uff1a\u9ad8\u6548\u7ec4\u4ef6\u8bbe\u8ba1\u3001\u52a8\u6001\u7f51\u7edc\u548c\u77e5\u8bc6\u84b8\u998f\uff0c\u5e76\u5728ImageNet-1K\u4e0a\u8bc4\u4f30\u5176\u6027\u80fd\u3002", "result": "\u5206\u6790\u4e86\u6bcf\u79cd\u7b56\u7565\u5728\u7cbe\u5ea6\u3001\u53c2\u6570\u91cf\u3001\u541e\u5410\u91cf\u7b49\u65b9\u9762\u7684\u6743\u8861\uff0c\u603b\u7ed3\u4e86\u5404\u81ea\u7684\u4f18\u7f3a\u70b9\u548c\u7075\u6d3b\u6027\u3002", "conclusion": "\u63d0\u51fa\u4e86\u672a\u6765\u8f7b\u91cf\u5316\u89c6\u89c9Transformer\u7684\u7814\u7a76\u65b9\u5411\u548c\u6f5c\u5728\u6311\u6218\uff0c\u65e8\u5728\u4e3a\u793e\u533a\u63d0\u4f9b\u5b9e\u8df5\u6307\u5bfc\u548c\u542f\u53d1\u3002"}}
{"id": "2505.02872", "pdf": "https://arxiv.org/pdf/2505.02872", "abs": "https://arxiv.org/abs/2505.02872", "authors": ["Cfir Avraham Hadar", "Omer Shubi", "Yoav Meiri", "Yevgeni Berzak"], "title": "Decoding Open-Ended Information Seeking Goals from Eye Movements in Reading", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "When reading, we often have specific information that interests us in a text.\nFor example, you might be reading this paper because you are curious about LLMs\nfor eye movements in reading, the experimental design, or perhaps you only care\nabout the question ``but does it work?''. More broadly, in daily life, people\napproach texts with any number of text-specific goals that guide their reading\nbehavior. In this work, we ask, for the first time, whether open-ended reading\ngoals can be automatically decoded from eye movements in reading. To address\nthis question, we introduce goal classification and goal reconstruction tasks\nand evaluation frameworks, and use large-scale eye tracking for reading data in\nEnglish with hundreds of text-specific information seeking tasks. We develop\nand compare several discriminative and generative multimodal LLMs that combine\neye movements and text for goal classification and goal reconstruction. Our\nexperiments show considerable success on both tasks, suggesting that LLMs can\nextract valuable information about the readers' text-specific goals from eye\nmovements.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u662f\u5426\u53ef\u4ee5\u901a\u8fc7\u773c\u52a8\u6570\u636e\u81ea\u52a8\u89e3\u7801\u8bfb\u8005\u7684\u5f00\u653e\u5f0f\u9605\u8bfb\u76ee\u6807\uff0c\u63d0\u51fa\u4e86\u76ee\u6807\u5206\u7c7b\u548c\u91cd\u6784\u4efb\u52a1\uff0c\u5e76\u5f00\u53d1\u4e86\u591a\u6a21\u6001LLM\u6a21\u578b\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u6709\u6548\u6027\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u63a2\u7d22\u773c\u52a8\u6570\u636e\u662f\u5426\u80fd\u53cd\u6620\u8bfb\u8005\u7684\u6587\u672c\u7279\u5b9a\u76ee\u6807\uff0c\u4ece\u800c\u4e3a\u7406\u89e3\u9605\u8bfb\u884c\u4e3a\u63d0\u4f9b\u65b0\u89c6\u89d2\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u8bbe\u8ba1\u76ee\u6807\u5206\u7c7b\u548c\u91cd\u6784\u4efb\u52a1\uff0c\u5229\u7528\u5927\u89c4\u6a21\u82f1\u8bed\u9605\u8bfb\u773c\u52a8\u6570\u636e\uff0c\u5f00\u53d1\u591a\u6a21\u6001LLM\u6a21\u578b\uff08\u7ed3\u5408\u773c\u52a8\u548c\u6587\u672c\uff09\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6a21\u578b\u5728\u76ee\u6807\u5206\u7c7b\u548c\u91cd\u6784\u4efb\u52a1\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u8bc1\u660e\u773c\u52a8\u6570\u636e\u80fd\u6709\u6548\u53cd\u6620\u9605\u8bfb\u76ee\u6807\u3002", "conclusion": "\u7ed3\u8bba\u662fLLM\u53ef\u4ee5\u4ece\u773c\u52a8\u6570\u636e\u4e2d\u63d0\u53d6\u8bfb\u8005\u76ee\u6807\u4fe1\u606f\uff0c\u4e3a\u9605\u8bfb\u884c\u4e3a\u7814\u7a76\u63d0\u4f9b\u65b0\u5de5\u5177\u3002"}}
{"id": "2505.03114", "pdf": "https://arxiv.org/pdf/2505.03114", "abs": "https://arxiv.org/abs/2505.03114", "authors": ["Teng Zhou", "Jax Luo", "Yuping Sun", "Yiheng Tan", "Shun Yao", "Nazim Haouchine", "Scott Raymond"], "title": "Path and Bone-Contour Regularized Unpaired MRI-to-CT Translation", "categories": ["cs.CV"], "comment": null, "summary": "Accurate MRI-to-CT translation promises the integration of complementary\nimaging information without the need for additional imaging sessions. Given the\npractical challenges associated with acquiring paired MRI and CT scans, the\ndevelopment of robust methods capable of leveraging unpaired datasets is\nessential for advancing the MRI-to-CT translation. Current unpaired MRI-to-CT\ntranslation methods, which predominantly rely on cycle consistency and\ncontrastive learning frameworks, frequently encounter challenges in accurately\ntranslating anatomical features that are highly discernible on CT but less\ndistinguishable on MRI, such as bone structures. This limitation renders these\napproaches less suitable for applications in radiation therapy, where precise\nbone representation is essential for accurate treatment planning. To address\nthis challenge, we propose a path- and bone-contour regularized approach for\nunpaired MRI-to-CT translation. In our method, MRI and CT images are projected\nto a shared latent space, where the MRI-to-CT mapping is modeled as a\ncontinuous flow governed by neural ordinary differential equations. The optimal\nmapping is obtained by minimizing the transition path length of the flow. To\nenhance the accuracy of translated bone structures, we introduce a trainable\nneural network to generate bone contours from MRI and implement mechanisms to\ndirectly and indirectly encourage the model to focus on bone contours and their\nadjacent regions. Evaluations conducted on three datasets demonstrate that our\nmethod outperforms existing unpaired MRI-to-CT translation approaches,\nachieving lower overall error rates. Moreover, in a downstream bone\nsegmentation task, our approach exhibits superior performance in preserving the\nfidelity of bone structures. Our code is available at:\nhttps://github.com/kennysyp/PaBoT.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8def\u5f84\u548c\u9aa8\u8f6e\u5ed3\u6b63\u5219\u5316\u7684\u65e0\u914d\u5bf9MRI\u5230CT\u8f6c\u6362\u65b9\u6cd5\uff0c\u901a\u8fc7\u795e\u7ecfODE\u5efa\u6a21\u8fde\u7eed\u6d41\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9aa8\u7ed3\u6784\u7684\u8f6c\u6362\u7cbe\u5ea6\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65e0\u914d\u5bf9MRI\u5230CT\u8f6c\u6362\u65b9\u6cd5\u5728\u9aa8\u7ed3\u6784\u7b49\u89e3\u5256\u7279\u5f81\u8f6c\u6362\u4e0a\u7684\u4e0d\u8db3\uff0c\u6ee1\u8db3\u653e\u5c04\u6cbb\u7597\u4e2d\u5bf9\u7cbe\u786e\u9aa8\u7ed3\u6784\u7684\u9700\u6c42\u3002", "method": "\u5c06MRI\u548cCT\u6295\u5f71\u5230\u5171\u4eab\u6f5c\u5728\u7a7a\u95f4\uff0c\u901a\u8fc7\u795e\u7ecfODE\u5efa\u6a21\u8fde\u7eed\u6d41\uff0c\u6700\u5c0f\u5316\u8def\u5f84\u957f\u5ea6\uff0c\u5e76\u5f15\u5165\u53ef\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u751f\u6210\u9aa8\u8f6e\u5ed3\u3002", "result": "\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u6574\u4f53\u8bef\u5dee\u66f4\u4f4e\uff0c\u4e14\u5728\u9aa8\u5206\u5272\u4efb\u52a1\u4e2d\u9aa8\u7ed3\u6784\u4fdd\u771f\u5ea6\u66f4\u9ad8\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u65e0\u914d\u5bf9MRI\u5230CT\u8f6c\u6362\u7684\u7cbe\u5ea6\uff0c\u5c24\u5176\u5728\u9aa8\u7ed3\u6784\u8f6c\u6362\u4e0a\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2505.02983", "pdf": "https://arxiv.org/pdf/2505.02983", "abs": "https://arxiv.org/abs/2505.02983", "authors": ["Wenjie Hua", "Shenghan Xu"], "title": "Logits-Constrained Framework with RoBERTa for Ancient Chinese NER", "categories": ["cs.CL", "68T50", "I.2.7; I.5.1; I.5.4"], "comment": "5 pages, 2 figures, 6 tables. Accepted to EvaHan 2025 shared task on\n  Ancient Chinese NLP", "summary": "This paper presents a Logits-Constrained (LC) framework for Ancient Chinese\nNamed Entity Recognition (NER), evaluated on the EvaHan 2025 benchmark. Our\ntwo-stage model integrates GujiRoBERTa for contextual encoding and a\ndifferentiable decoding mechanism to enforce valid BMES label transitions.\nExperiments demonstrate that LC improves performance over traditional CRF and\nBiLSTM-based approaches, especially in high-label or large-data settings. We\nalso propose a model selection criterion balancing label complexity and dataset\nsize, providing practical guidance for real-world Ancient Chinese NLP tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cdLogits-Constrained\uff08LC\uff09\u6846\u67b6\u7528\u4e8e\u53e4\u6c49\u8bed\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\uff08NER\uff09\uff0c\u5728EvaHan 2025\u57fa\u51c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u53e4\u6c49\u8bedNER\u4efb\u52a1\u4e2d\u9ad8\u6807\u7b7e\u6216\u5927\u6570\u636e\u573a\u666f\u4e0b\u7684\u6027\u80fd\u95ee\u9898\u3002", "method": "\u4e24\u9636\u6bb5\u6a21\u578b\uff0c\u7ed3\u5408GujiRoBERTa\u8fdb\u884c\u4e0a\u4e0b\u6587\u7f16\u7801\u548c\u53ef\u5fae\u5206\u89e3\u7801\u673a\u5236\u4ee5\u7ea6\u675fBMES\u6807\u7b7e\u8f6c\u79fb\u3002", "result": "LC\u6846\u67b6\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u4f20\u7edfCRF\u548cBiLSTM\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u6a21\u578b\u9009\u62e9\u6807\u51c6\u4e3a\u5b9e\u9645\u53e4\u6c49\u8bedNLP\u4efb\u52a1\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\u3002"}}
{"id": "2505.03116", "pdf": "https://arxiv.org/pdf/2505.03116", "abs": "https://arxiv.org/abs/2505.03116", "authors": ["Haoyue Liu", "Jinghan Xu", "Yi Chang", "Hanyu Zhou", "Haozhi Zhao", "Lin Wang", "Luxin Yan"], "title": "TimeTracker: Event-based Continuous Point Tracking for Video Frame Interpolation with Non-linear Motion", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025", "summary": "Video frame interpolation (VFI) that leverages the bio-inspired event cameras\nas guidance has recently shown better performance and memory efficiency than\nthe frame-based methods, thanks to the event cameras' advantages, such as high\ntemporal resolution. A hurdle for event-based VFI is how to effectively deal\nwith non-linear motion, caused by the dynamic changes in motion direction and\nspeed within the scene. Existing methods either use events to estimate sparse\noptical flow or fuse events with image features to estimate dense optical flow.\nUnfortunately, motion errors often degrade the VFI quality as the continuous\nmotion cues from events do not align with the dense spatial information of\nimages in the temporal dimension. In this paper, we find that object motion is\ncontinuous in space, tracking local regions over continuous time enables more\naccurate identification of spatiotemporal feature correlations. In light of\nthis, we propose a novel continuous point tracking-based VFI framework, named\nTimeTracker. Specifically, we first design a Scene-Aware Region Segmentation\n(SARS) module to divide the scene into similar patches. Then, a Continuous\nTrajectory guided Motion Estimation (CTME) module is proposed to track the\ncontinuous motion trajectory of each patch through events. Finally,\nintermediate frames at any given time are generated through global motion\noptimization and frame refinement. Moreover, we collect a real-world dataset\nthat features fast non-linear motion. Extensive experiments show that our\nmethod outperforms prior arts in both motion estimation and frame interpolation\nquality.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8fde\u7eed\u70b9\u8ddf\u8e2a\u7684\u89c6\u9891\u5e27\u63d2\u503c\u6846\u67b6TimeTracker\uff0c\u901a\u8fc7\u4e8b\u4ef6\u76f8\u673a\u5904\u7406\u975e\u7ebf\u6027\u8fd0\u52a8\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63d2\u5e27\u8d28\u91cf\u3002", "motivation": "\u4e8b\u4ef6\u76f8\u673a\u7684\u9ad8\u65f6\u95f4\u5206\u8fa8\u7387\u4f18\u52bf\u672a\u88ab\u5145\u5206\u5229\u7528\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u975e\u7ebf\u6027\u8fd0\u52a8\u65f6\u5b58\u5728\u8fd0\u52a8\u8bef\u5dee\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u4e86\u573a\u666f\u611f\u77e5\u533a\u57df\u5206\u5272\u6a21\u5757\uff08SARS\uff09\u548c\u8fde\u7eed\u8f68\u8ff9\u5f15\u5bfc\u7684\u8fd0\u52a8\u4f30\u8ba1\u6a21\u5757\uff08CTME\uff09\uff0c\u901a\u8fc7\u8ddf\u8e2a\u5c40\u90e8\u533a\u57df\u7684\u8fde\u7eed\u8fd0\u52a8\u8f68\u8ff9\u751f\u6210\u4e2d\u95f4\u5e27\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u8fd0\u52a8\u4f30\u8ba1\u548c\u5e27\u63d2\u503c\u8d28\u91cf\u4e0a\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "TimeTracker\u6846\u67b6\u901a\u8fc7\u8fde\u7eed\u70b9\u8ddf\u8e2a\u6709\u6548\u89e3\u51b3\u4e86\u975e\u7ebf\u6027\u8fd0\u52a8\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u89c6\u9891\u5e27\u63d2\u503c\u7684\u6027\u80fd\u3002"}}
{"id": "2505.03005", "pdf": "https://arxiv.org/pdf/2505.03005", "abs": "https://arxiv.org/abs/2505.03005", "authors": ["Daniel Goldstein", "Eric Alcaide", "Janna Lu", "Eugene Cheah"], "title": "RADLADS: Rapid Attention Distillation to Linear Attention Decoders at Scale", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7"], "comment": null, "summary": "We present Rapid Attention Distillation to Linear Attention Decoders at Scale\n(RADLADS), a protocol for rapidly converting softmax attention transformers\ninto linear attention decoder models, along with two new RWKV-variant\narchitectures, and models converted from popular Qwen2.5 open source models in\n7B, 32B, and 72B sizes. Our conversion process requires only 350-700M tokens,\nless than 0.005% of the token count used to train the original teacher models.\nConverting to our 72B linear attention model costs less than \\$2,000 USD at\ntoday's prices, yet quality at inference remains close to the original\ntransformer. These models achieve state-of-the-art downstream performance\nacross a set of standard benchmarks for linear attention models of their size.\nWe release all our models on HuggingFace under the Apache 2.0 license, with the\nexception of our 72B models which are also governed by the Qwen License\nAgreement.\n  Models at\nhttps://huggingface.co/collections/recursal/radlads-6818ee69e99e729ba8a87102\nTraining Code at https://github.com/recursal/RADLADS-paper", "AI": {"tldr": "RADLADS\u63d0\u51fa\u4e86\u4e00\u79cd\u5feb\u901f\u5c06softmax\u6ce8\u610f\u529bTransformer\u8f6c\u6362\u4e3a\u7ebf\u6027\u6ce8\u610f\u529b\u89e3\u7801\u5668\u6a21\u578b\u7684\u534f\u8bae\uff0c\u5e76\u5c55\u793a\u4e86\u4e24\u79cd\u65b0\u7684RWKV\u53d8\u4f53\u67b6\u6784\u3002\u8f6c\u6362\u8fc7\u7a0b\u4ec5\u9700350-700M tokens\uff0c\u6210\u672c\u4f4e\u81f32000\u7f8e\u5143\uff0c\u4e14\u6027\u80fd\u63a5\u8fd1\u539f\u59cb\u6a21\u578b\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edfTransformer\u6a21\u578b\u7684\u9ad8\u8ba1\u7b97\u6210\u672c\u548c\u8d44\u6e90\u6d88\u8017\u95ee\u9898\uff0c\u63d0\u4f9b\u4e00\u79cd\u9ad8\u6548\u4e14\u7ecf\u6d4e\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u901a\u8fc7RADLADS\u534f\u8bae\u5feb\u901f\u8f6c\u6362softmax\u6ce8\u610f\u529bTransformer\u4e3a\u7ebf\u6027\u6ce8\u610f\u529b\u89e3\u7801\u5668\u6a21\u578b\uff0c\u5e76\u5f00\u53d1\u4e86\u65b0\u7684RWKV\u53d8\u4f53\u67b6\u6784\u3002", "result": "\u8f6c\u6362\u540e\u7684\u6a21\u578b\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u6027\u80fd\u63a5\u8fd1\u539f\u59cbTransformer\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u3002", "conclusion": "RADLADS\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u4f4e\u6210\u672c\u7684\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u6a21\u578b\u7684\u8f6c\u6362\uff0c\u4e14\u6027\u80fd\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2505.03132", "pdf": "https://arxiv.org/pdf/2505.03132", "abs": "https://arxiv.org/abs/2505.03132", "authors": ["Xinyuan Yan", "Xiwei Xuan", "Jorge Piazentin Ono", "Jiajing Guo", "Vikram Mohanty", "Shekar Arvind Kumar", "Liang Gou", "Bei Wang", "Liu Ren"], "title": "VISLIX: An XAI Framework for Validating Vision Models with Slice Discovery and Analysis", "categories": ["cs.CV", "cs.AI", "cs.HC"], "comment": null, "summary": "Real-world machine learning models require rigorous evaluation before\ndeployment, especially in safety-critical domains like autonomous driving and\nsurveillance. The evaluation of machine learning models often focuses on data\nslices, which are subsets of the data that share a set of characteristics. Data\nslice finding automatically identifies conditions or data subgroups where\nmodels underperform, aiding developers in mitigating performance issues.\nDespite its popularity and effectiveness, data slicing for vision model\nvalidation faces several challenges. First, data slicing often needs additional\nimage metadata or visual concepts, and falls short in certain computer vision\ntasks, such as object detection. Second, understanding data slices is a\nlabor-intensive and mentally demanding process that heavily relies on the\nexpert's domain knowledge. Third, data slicing lacks a human-in-the-loop\nsolution that allows experts to form hypothesis and test them interactively. To\novercome these limitations and better support the machine learning operations\nlifecycle, we introduce VISLIX, a novel visual analytics framework that employs\nstate-of-the-art foundation models to help domain experts analyze slices in\ncomputer vision models. Our approach does not require image metadata or visual\nconcepts, automatically generates natural language insights, and allows users\nto test data slice hypothesis interactively. We evaluate VISLIX with an expert\nstudy and three use cases, that demonstrate the effectiveness of our tool in\nproviding comprehensive insights for validating object detection models.", "AI": {"tldr": "VISLIX\u662f\u4e00\u4e2a\u65b0\u7684\u89c6\u89c9\u5206\u6790\u6846\u67b6\uff0c\u5229\u7528\u57fa\u7840\u6a21\u578b\u5e2e\u52a9\u4e13\u5bb6\u5206\u6790\u8ba1\u7b97\u673a\u89c6\u89c9\u6a21\u578b\u7684\u6570\u636e\u5207\u7247\uff0c\u65e0\u9700\u989d\u5916\u5143\u6570\u636e\uff0c\u5e76\u63d0\u4f9b\u4ea4\u4e92\u5f0f\u6d4b\u8bd5\u529f\u80fd\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff08\u5982\u81ea\u52a8\u9a7e\u9a76\u548c\u76d1\u63a7\uff09\u9700\u8981\u4e25\u683c\u8bc4\u4f30\uff0c\u4f46\u73b0\u6709\u6570\u636e\u5207\u7247\u65b9\u6cd5\u9762\u4e34\u4f9d\u8d56\u5143\u6570\u636e\u3001\u4efb\u52a1\u5c40\u9650\u6027\u9ad8\u548c\u7f3a\u4e4f\u4ea4\u4e92\u6027\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faVISLIX\u6846\u67b6\uff0c\u5229\u7528\u57fa\u7840\u6a21\u578b\u81ea\u52a8\u751f\u6210\u81ea\u7136\u8bed\u8a00\u6d1e\u5bdf\uff0c\u652f\u6301\u4ea4\u4e92\u5f0f\u6570\u636e\u5207\u7247\u5047\u8bbe\u6d4b\u8bd5\u3002", "result": "\u901a\u8fc7\u4e13\u5bb6\u7814\u7a76\u548c\u4e09\u4e2a\u7528\u4f8b\u9a8c\u8bc1\u4e86VISLIX\u5728\u5bf9\u8c61\u68c0\u6d4b\u6a21\u578b\u9a8c\u8bc1\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "VISLIX\u514b\u670d\u4e86\u73b0\u6709\u6570\u636e\u5207\u7247\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u8ba1\u7b97\u673a\u89c6\u89c9\u6a21\u578b\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u5de5\u5177\u3002"}}
{"id": "2505.03019", "pdf": "https://arxiv.org/pdf/2505.03019", "abs": "https://arxiv.org/abs/2505.03019", "authors": ["Alb\u00e9rick Euraste Djir\u00e9", "Abdoul Kader Kabor\u00e9", "Earl T. Barr", "Jacques Klein", "Tegawend\u00e9 F. Bissyand\u00e9"], "title": "Memorization or Interpolation ? Detecting LLM Memorization through Input Perturbation Analysis", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "While Large Language Models (LLMs) achieve remarkable performance through\ntraining on massive datasets, they can exhibit concerning behaviors such as\nverbatim reproduction of training data rather than true generalization. This\nmemorization phenomenon raises significant concerns about data privacy,\nintellectual property rights, and the reliability of model evaluations. This\npaper introduces PEARL, a novel approach for detecting memorization in LLMs.\nPEARL assesses how sensitive an LLM's performance is to input perturbations,\nenabling memorization detection without requiring access to the model's\ninternals. We investigate how input perturbations affect the consistency of\noutputs, enabling us to distinguish between true generalization and\nmemorization. Our findings, following extensive experiments on the Pythia open\nmodel, provide a robust framework for identifying when the model simply\nregurgitates learned information. Applied on the GPT 4o models, the PEARL\nframework not only identified cases of memorization of classic texts from the\nBible or common code from HumanEval but also demonstrated that it can provide\nsupporting evidence that some data, such as from the New York Times news\narticles, were likely part of the training data of a given model.", "AI": {"tldr": "PEARL\u662f\u4e00\u79cd\u68c0\u6d4b\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8bb0\u5fc6\u73b0\u8c61\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u8f93\u5165\u6270\u52a8\u8bc4\u4f30\u6a21\u578b\u8f93\u51fa\u7684\u654f\u611f\u6027\uff0c\u65e0\u9700\u8bbf\u95ee\u6a21\u578b\u5185\u90e8\u5373\u53ef\u533a\u5206\u771f\u5b9e\u6cdb\u5316\u4e0e\u8bb0\u5fc6\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8bad\u7ec3\u4e2d\u53ef\u80fd\u8bb0\u5fc6\u800c\u975e\u6cdb\u5316\u6570\u636e\uff0c\u5f15\u53d1\u9690\u79c1\u3001\u77e5\u8bc6\u4ea7\u6743\u548c\u8bc4\u4f30\u53ef\u9760\u6027\u95ee\u9898\u3002", "method": "PEARL\u901a\u8fc7\u8f93\u5165\u6270\u52a8\u5206\u6790\u6a21\u578b\u8f93\u51fa\u7684\u4e00\u81f4\u6027\uff0c\u68c0\u6d4b\u8bb0\u5fc6\u73b0\u8c61\u3002", "result": "\u5728Pythia\u548cGPT-4o\u6a21\u578b\u4e0a\u9a8c\u8bc1\uff0cPEARL\u6210\u529f\u8bc6\u522b\u4e86\u7ecf\u5178\u6587\u672c\u548c\u4ee3\u7801\u7684\u8bb0\u5fc6\u73b0\u8c61\uff0c\u5e76\u63a8\u6d4b\u4e86\u8bad\u7ec3\u6570\u636e\u6765\u6e90\u3002", "conclusion": "PEARL\u4e3a\u68c0\u6d4bLLM\u8bb0\u5fc6\u63d0\u4f9b\u4e86\u6709\u6548\u6846\u67b6\uff0c\u6709\u52a9\u4e8e\u89e3\u51b3\u9690\u79c1\u548c\u8bc4\u4f30\u95ee\u9898\u3002"}}
{"id": "2505.03134", "pdf": "https://arxiv.org/pdf/2505.03134", "abs": "https://arxiv.org/abs/2505.03134", "authors": ["Sajjad Rezvani Boroujeni", "Hossein Abedi", "Tom Bush"], "title": "Enhancing Glass Defect Detection with Diffusion Models: Addressing Imbalanced Datasets in Manufacturing Quality Control", "categories": ["cs.CV", "cs.LG"], "comment": "12 pages, 7 figures, submitted to Computer and Decision Making An\n  International Journal (COMDEM)", "summary": "Visual defect detection in industrial glass manufacturing remains a critical\nchallenge due to the low frequency of defective products, leading to imbalanced\ndatasets that limit the performance of deep learning models and computer vision\nsystems. This paper presents a novel approach using Denoising Diffusion\nProbabilistic Models (DDPMs) to generate synthetic defective glass product\nimages for data augmentation, effectively addressing class imbalance issues in\nmanufacturing quality control and automated visual inspection. The methodology\nsignificantly enhances image classification performance of standard CNN\narchitectures (ResNet50V2, EfficientNetB0, and MobileNetV2) in detecting\nanomalies by increasing the minority class representation. Experimental results\ndemonstrate substantial improvements in key machine learning metrics,\nparticularly in recall for defective samples across all tested deep neural\nnetwork architectures while maintaining perfect precision. The most dramatic\nimprovement was observed in ResNet50V2's overall classification accuracy, which\nincreased from 78 percent to 93 percent when trained with the augmented data.\nThis work provides a scalable, cost-effective approach to enhancing automated\ndefect detection in glass manufacturing that can potentially be extended to\nother industrial quality assurance systems and industries with similar class\nimbalance challenges.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eDDPMs\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u5408\u6210\u7f3a\u9677\u73bb\u7483\u56fe\u50cf\u89e3\u51b3\u5de5\u4e1a\u73bb\u7483\u5236\u9020\u4e2d\u6570\u636e\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86CNN\u6a21\u578b\u7684\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u5de5\u4e1a\u73bb\u7483\u5236\u9020\u4e2d\u89c6\u89c9\u7f3a\u9677\u68c0\u6d4b\u56e0\u7f3a\u9677\u4ea7\u54c1\u9891\u7387\u4f4e\u5bfc\u81f4\u6570\u636e\u4e0d\u5e73\u8861\uff0c\u9650\u5236\u4e86\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u6027\u80fd\u3002", "method": "\u4f7f\u7528Denoising Diffusion Probabilistic Models (DDPMs)\u751f\u6210\u5408\u6210\u7f3a\u9677\u56fe\u50cf\u8fdb\u884c\u6570\u636e\u589e\u5f3a\uff0c\u63d0\u5347\u5c11\u6570\u7c7b\u6837\u672c\u7684\u8868\u793a\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86CNN\u6a21\u578b\uff08\u5982ResNet50V2\uff09\u7684\u5206\u7c7b\u51c6\u786e\u7387\uff0c\u4ece78%\u63d0\u5347\u81f393%\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u73bb\u7483\u5236\u9020\u4e2d\u7684\u7f3a\u9677\u68c0\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u3001\u7ecf\u6d4e\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u53ef\u80fd\u9002\u7528\u4e8e\u5176\u4ed6\u7c7b\u4f3c\u6570\u636e\u4e0d\u5e73\u8861\u95ee\u9898\u7684\u884c\u4e1a\u3002"}}
{"id": "2505.03025", "pdf": "https://arxiv.org/pdf/2505.03025", "abs": "https://arxiv.org/abs/2505.03025", "authors": ["Steven Bedrick", "A. Seza Do\u011fru\u00f6z", "Sergiu Nisioi"], "title": "A Typology of Synthetic Datasets for Dialogue Processing in Clinical Contexts", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Synthetic data sets are used across linguistic domains and NLP tasks,\nparticularly in scenarios where authentic data is limited (or even\nnon-existent). One such domain is that of clinical (healthcare) contexts, where\nthere exist significant and long-standing challenges (e.g., privacy,\nanonymization, and data governance) which have led to the development of an\nincreasing number of synthetic datasets. One increasingly important category of\nclinical dataset is that of clinical dialogues which are especially sensitive\nand difficult to collect, and as such are commonly synthesized.\n  While such synthetic datasets have been shown to be sufficient in some\nsituations, little theory exists to inform how they may be best used and\ngeneralized to new applications. In this paper, we provide an overview of how\nsynthetic datasets are created, evaluated and being used for dialogue related\ntasks in the medical domain. Additionally, we propose a novel typology for use\nin classifying types and degrees of data synthesis, to facilitate comparison\nand evaluation.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u533b\u7597\u9886\u57df\u4e2d\u5408\u6210\u6570\u636e\u96c6\u7684\u521b\u5efa\u3001\u8bc4\u4f30\u548c\u5e94\u7528\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5206\u7c7b\u6cd5\u4ee5\u6bd4\u8f83\u548c\u8bc4\u4f30\u6570\u636e\u5408\u6210\u7c7b\u578b\u548c\u7a0b\u5ea6\u3002", "motivation": "\u4e34\u5e8a\u5bf9\u8bdd\u6570\u636e\u56e0\u9690\u79c1\u548c\u6cbb\u7406\u95ee\u9898\u96be\u4ee5\u83b7\u53d6\uff0c\u5408\u6210\u6570\u636e\u96c6\u6210\u4e3a\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u7f3a\u4e4f\u7406\u8bba\u6307\u5bfc\u5176\u6700\u4f73\u4f7f\u7528\u548c\u6cdb\u5316\u3002", "method": "\u7efc\u8ff0\u5408\u6210\u6570\u636e\u96c6\u7684\u521b\u5efa\u4e0e\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u65b0\u7684\u6570\u636e\u5408\u6210\u5206\u7c7b\u6cd5\u3002", "result": "\u5408\u6210\u6570\u636e\u96c6\u5728\u533b\u7597\u5bf9\u8bdd\u4efb\u52a1\u4e2d\u6709\u4e00\u5b9a\u6548\u679c\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u7406\u8bba\u652f\u6301\u3002", "conclusion": "\u63d0\u51fa\u5206\u7c7b\u6cd5\u6709\u52a9\u4e8e\u5408\u6210\u6570\u636e\u96c6\u7684\u6bd4\u8f83\u4e0e\u8bc4\u4f30\uff0c\u672a\u6765\u9700\u66f4\u591a\u7814\u7a76\u652f\u6301\u5176\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2505.03149", "pdf": "https://arxiv.org/pdf/2505.03149", "abs": "https://arxiv.org/abs/2505.03149", "authors": ["Joseph William Kettelkamp", "Ludovica Romanin", "Sarv Priya", "Mathews Jacob"], "title": "Motion-compensated cardiac MRI using low-rank diffeomorphic flow (DMoCo)", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "We introduce an unsupervised motion-compensated image reconstruction\nalgorithm for free-breathing and ungated 3D cardiac magnetic resonance imaging\n(MRI). We express the image volume corresponding to each specific motion phase\nas the deformation of a single static image template. The main contribution of\nthe work is the low-rank model for the compact joint representation of the\nfamily of diffeomorphisms, parameterized by the motion phases. The\ndiffeomorphism at a specific motion phase is obtained by integrating a\nparametric velocity field along a path connecting the reference template phase\nto the motion phase. The velocity field at different phases is represented\nusing a low-rank model. The static template and the low-rank motion model\nparameters are learned directly from the k-space data in an unsupervised\nfashion. The more constrained motion model is observed to offer improved\nrecovery compared to current motion-resolved and motion-compensated algorithms\nfor free-breathing 3D cine MRI.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u76d1\u7763\u8fd0\u52a8\u8865\u507f\u56fe\u50cf\u91cd\u5efa\u7b97\u6cd5\uff0c\u7528\u4e8e\u81ea\u7531\u547c\u5438\u548c\u975e\u95e8\u63a73D\u5fc3\u810fMRI\uff0c\u901a\u8fc7\u4f4e\u79e9\u6a21\u578b\u8868\u793a\u8fd0\u52a8\u76f8\u4f4d\u95f4\u7684\u53d8\u5f62\u3002", "motivation": "\u89e3\u51b3\u81ea\u7531\u547c\u5438\u548c\u975e\u95e8\u63a73D\u5fc3\u810fMRI\u4e2d\u7684\u8fd0\u52a8\u4f2a\u5f71\u95ee\u9898\uff0c\u63d0\u9ad8\u56fe\u50cf\u91cd\u5efa\u8d28\u91cf\u3002", "method": "\u5c06\u6bcf\u4e2a\u8fd0\u52a8\u76f8\u4f4d\u7684\u56fe\u50cf\u4f53\u79ef\u8868\u793a\u4e3a\u9759\u6001\u6a21\u677f\u7684\u53d8\u5f62\uff0c\u4f7f\u7528\u4f4e\u79e9\u6a21\u578b\u8868\u793a\u53d8\u5f62\u5bb6\u65cf\uff0c\u5e76\u901a\u8fc7\u53c2\u6570\u5316\u901f\u5ea6\u573a\u79ef\u5206\u83b7\u5f97\u7279\u5b9a\u76f8\u4f4d\u7684\u53d8\u5f62\u3002", "result": "\u76f8\u6bd4\u73b0\u6709\u8fd0\u52a8\u5206\u8fa8\u548c\u8fd0\u52a8\u8865\u507f\u7b97\u6cd5\uff0c\u63d0\u51fa\u7684\u4f4e\u79e9\u8fd0\u52a8\u6a21\u578b\u5728\u81ea\u7531\u547c\u54383D cine MRI\u4e2d\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u901a\u8fc7\u65e0\u76d1\u7763\u5b66\u4e60\u9759\u6001\u6a21\u677f\u548c\u4f4e\u79e9\u8fd0\u52a8\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u91cd\u5efa\u6548\u679c\u3002"}}
{"id": "2505.03030", "pdf": "https://arxiv.org/pdf/2505.03030", "abs": "https://arxiv.org/abs/2505.03030", "authors": ["Sicong Huang", "Jincheng He", "Shiyuan Huang", "Karthik Raja Anandan", "Arkajyoti Chakraborty", "Ian Lane"], "title": "UCSC at SemEval-2025 Task 3: Context, Models and Prompt Optimization for Automated Hallucination Detection in LLM Output", "categories": ["cs.CL"], "comment": "6 pages, 1 figure", "summary": "Hallucinations pose a significant challenge for large language models when\nanswering knowledge-intensive queries. As LLMs become more widely adopted, it\nis crucial not only to detect if hallucinations occur but also to pinpoint\nexactly where in the LLM output they occur. SemEval 2025 Task 3, Mu-SHROOM:\nMultilingual Shared-task on Hallucinations and Related Observable\nOvergeneration Mistakes, is a recent effort in this direction. This paper\ndescribes the UCSC system submission to the shared Mu-SHROOM task. We introduce\na framework that first retrieves relevant context, next identifies false\ncontent from the answer, and finally maps them back to spans in the LLM output.\nThe process is further enhanced by automatically optimizing prompts. Our system\nachieves the highest overall performance, ranking #1 in average position across\nall languages. We release our code and experiment results.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86UCSC\u56e2\u961f\u5728Mu-SHROOM\u4efb\u52a1\u4e2d\u7684\u7cfb\u7edf\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u68c0\u6d4b\u548c\u5b9a\u4f4d\u5927\u8bed\u8a00\u6a21\u578b\u5e7b\u89c9\u7684\u6846\u67b6\uff0c\u53d6\u5f97\u4e86\u6700\u4f73\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u77e5\u8bc6\u5bc6\u96c6\u578b\u67e5\u8be2\u4e2d\u4ea7\u751f\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u5e76\u7cbe\u786e\u5b9a\u4f4d\u5e7b\u89c9\u53d1\u751f\u7684\u4f4d\u7f6e\u3002", "method": "\u901a\u8fc7\u68c0\u7d22\u76f8\u5173\u4e0a\u4e0b\u6587\u3001\u8bc6\u522b\u7b54\u6848\u4e2d\u7684\u9519\u8bef\u5185\u5bb9\uff0c\u5e76\u5c06\u5176\u6620\u5c04\u56deLLM\u8f93\u51fa\u4e2d\u7684\u5177\u4f53\u7247\u6bb5\uff0c\u540c\u65f6\u4f18\u5316\u63d0\u793a\u3002", "result": "\u7cfb\u7edf\u5728\u6240\u6709\u8bed\u8a00\u4e2d\u5e73\u5747\u6392\u540d\u7b2c\u4e00\uff0c\u6027\u80fd\u6700\u4f18\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u5e7b\u89c9\u68c0\u6d4b\u548c\u5b9a\u4f4d\u95ee\u9898\uff0c\u4ee3\u7801\u548c\u5b9e\u9a8c\u7ed3\u679c\u5df2\u516c\u5f00\u3002"}}
{"id": "2505.03153", "pdf": "https://arxiv.org/pdf/2505.03153", "abs": "https://arxiv.org/abs/2505.03153", "authors": ["Sparsh Bansal", "Mingyang Wu", "Xin Wang", "Shu Hu"], "title": "Robust Fairness Vision-Language Learning for Medical Image Analysis", "categories": ["cs.CV"], "comment": null, "summary": "The advent of Vision-Language Models (VLMs) in medical image analysis has the\npotential to help process multimodal inputs and increase performance over\ntraditional inference methods. However, when considering the domain in which\nthese models will be implemented, fairness and robustness are important to\nensure the model stays true for any patient. In this paper, we introduce a\nframework for ensuring robustness and fairness of VLM models. This framework\nmodifies the loss function at training by identifying and adjusting faulty\nimage-text pairs through a Dynamic Bad Pair Mining algorithm and also utilizing\nSinkhorn distance to ensure the loss distributions of protected groups do not\ndeviate from the total loss. Experimental testing of our framework shows up to\na 8.6\\% improvement when looking at equity-scaled AUC.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u786e\u4fdd\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5728\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4e2d\u516c\u5e73\u6027\u548c\u9c81\u68d2\u6027\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u574f\u5bf9\u6316\u6398\u7b97\u6cd5\u548cSinkhorn\u8ddd\u79bb\u4f18\u5316\u635f\u5931\u51fd\u6570\uff0c\u5b9e\u9a8c\u663e\u793a\u516c\u5e73\u6027AUC\u63d0\u53478.6%\u3002", "motivation": "\u5728\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4e2d\uff0c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u516c\u5e73\u6027\u548c\u9c81\u68d2\u6027\u81f3\u5173\u91cd\u8981\uff0c\u4ee5\u786e\u4fdd\u6a21\u578b\u5bf9\u6240\u6709\u60a3\u8005\u5747\u9002\u7528\u3002", "method": "\u63d0\u51fa\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u574f\u5bf9\u6316\u6398\u7b97\u6cd5\u8c03\u6574\u635f\u5931\u51fd\u6570\uff0c\u5e76\u4f7f\u7528Sinkhorn\u8ddd\u79bb\u786e\u4fdd\u4fdd\u62a4\u7ec4\u7684\u635f\u5931\u5206\u5e03\u4e0e\u603b\u4f53\u4e00\u81f4\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u516c\u5e73\u6027AUC\u6307\u6807\u63d0\u5347\u4e868.6%\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86VLM\u5728\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4e2d\u7684\u516c\u5e73\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2505.03052", "pdf": "https://arxiv.org/pdf/2505.03052", "abs": "https://arxiv.org/abs/2505.03052", "authors": ["Ryan Wang", "Matthew Finlayson", "Luca Soldaini", "Swabha Swayamdipta", "Robin Jia"], "title": "Teaching Models to Understand (but not Generate) High-risk Data", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Language model developers typically filter out high-risk content -- such as\ntoxic or copyrighted text -- from their pre-training data to prevent models\nfrom generating similar outputs. However, removing such data altogether limits\nmodels' ability to recognize and appropriately respond to harmful or sensitive\ncontent. In this paper, we introduce Selective Loss to Understand but Not\nGenerate (SLUNG), a pre-training paradigm through which models learn to\nunderstand high-risk data without learning to generate it. Instead of uniformly\napplying the next-token prediction loss, SLUNG selectively avoids incentivizing\nthe generation of high-risk tokens while ensuring they remain within the\nmodel's context window. As the model learns to predict low-risk tokens that\nfollow high-risk ones, it is forced to understand the high-risk content.\nThrough our experiments, we show that SLUNG consistently improves models'\nunderstanding of high-risk data (e.g., ability to recognize toxic content)\nwithout increasing its generation (e.g., toxicity of model responses). Overall,\nour SLUNG paradigm enables models to benefit from high-risk text that would\notherwise be filtered out.", "AI": {"tldr": "SLUNG\u662f\u4e00\u79cd\u9884\u8bad\u7ec3\u8303\u5f0f\uff0c\u65e8\u5728\u8ba9\u6a21\u578b\u7406\u89e3\u9ad8\u98ce\u9669\u5185\u5bb9\u4f46\u4e0d\u751f\u6210\u5b83\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u635f\u5931\u907f\u514d\u9ad8\u98ce\u9669\u6807\u8bb0\u7684\u751f\u6210\uff0c\u540c\u65f6\u4fdd\u7559\u5176\u7406\u89e3\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u8fc7\u6ee4\u9ad8\u98ce\u9669\u5185\u5bb9\u4f1a\u9650\u5236\u6a21\u578b\u5bf9\u6709\u5bb3\u6216\u654f\u611f\u5185\u5bb9\u7684\u8bc6\u522b\u548c\u54cd\u5e94\u80fd\u529b\uff0cSLUNG\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "SLUNG\u5728\u9884\u8bad\u7ec3\u4e2d\u9009\u62e9\u6027\u907f\u514d\u6fc0\u52b1\u9ad8\u98ce\u9669\u6807\u8bb0\u7684\u751f\u6210\uff0c\u4f46\u4ecd\u4fdd\u7559\u5176\u5728\u4e0a\u4e0b\u6587\u4e2d\u7684\u7406\u89e3\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSLUNG\u63d0\u9ad8\u4e86\u6a21\u578b\u5bf9\u9ad8\u98ce\u9669\u5185\u5bb9\u7684\u7406\u89e3\u80fd\u529b\uff0c\u540c\u65f6\u672a\u589e\u52a0\u5176\u751f\u6210\u9ad8\u98ce\u9669\u5185\u5bb9\u7684\u6982\u7387\u3002", "conclusion": "SLUNG\u4f7f\u6a21\u578b\u80fd\u591f\u4ece\u672a\u88ab\u8fc7\u6ee4\u7684\u9ad8\u98ce\u9669\u6587\u672c\u4e2d\u53d7\u76ca\uff0c\u540c\u65f6\u907f\u514d\u751f\u6210\u6709\u5bb3\u5185\u5bb9\u3002"}}
{"id": "2505.03053", "pdf": "https://arxiv.org/pdf/2505.03053", "abs": "https://arxiv.org/abs/2505.03053", "authors": ["Jennifer Healey", "Laurie Byrum", "Md Nadeem Akhtar", "Surabhi Bhargava", "Moumita Sinha"], "title": "Developing A Framework to Support Human Evaluation of Bias in Generated Free Response Text", "categories": ["cs.CL", "cs.AI"], "comment": "6 pages, no figures, presented at CHI 2025 workshop for Human\n  Evaluation and Auditing of Language Models", "summary": "LLM evaluation is challenging even the case of base models. In real world\ndeployments, evaluation is further complicated by the interplay of task\nspecific prompts and experiential context. At scale, bias evaluation is often\nbased on short context, fixed choice benchmarks that can be rapidly evaluated,\nhowever, these can lose validity when the LLMs' deployed context differs. Large\nscale human evaluation is often seen as too intractable and costly. Here we\npresent our journey towards developing a semi-automated bias evaluation\nframework for free text responses that has human insights at its core. We\ndiscuss how we developed an operational definition of bias that helped us\nautomate our pipeline and a methodology for classifying bias beyond multiple\nchoice. We additionally comment on how human evaluation helped us uncover\nproblematic templates in a bias benchmark.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u534a\u81ea\u52a8\u5316\u7684\u504f\u89c1\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u7531\u6587\u672c\u54cd\u5e94\uff0c\u7ed3\u5408\u4eba\u7c7b\u6d1e\u5bdf\u529b\u89e3\u51b3LLM\u8bc4\u4f30\u4e2d\u7684\u6311\u6218\u3002", "motivation": "LLM\u8bc4\u4f30\u5728\u73b0\u5b9e\u90e8\u7f72\u4e2d\u56e0\u4efb\u52a1\u7279\u5b9a\u63d0\u793a\u548c\u4e0a\u4e0b\u6587\u4ea4\u4e92\u800c\u590d\u6742\u5316\uff0c\u4f20\u7edf\u77ed\u4e0a\u4e0b\u6587\u57fa\u51c6\u53ef\u80fd\u5931\u6548\uff0c\u5927\u89c4\u6a21\u4eba\u7c7b\u8bc4\u4f30\u6210\u672c\u9ad8\u4e14\u96be\u4ee5\u5b9e\u65bd\u3002", "method": "\u5f00\u53d1\u4e86\u504f\u89c1\u7684\u64cd\u4f5c\u5b9a\u4e49\u4ee5\u81ea\u52a8\u5316\u6d41\u7a0b\uff0c\u5e76\u63d0\u51fa\u8d85\u8d8a\u591a\u9009\u9898\u7684\u504f\u89c1\u5206\u7c7b\u65b9\u6cd5\uff0c\u7ed3\u5408\u4eba\u7c7b\u8bc4\u4f30\u53d1\u73b0\u504f\u89c1\u57fa\u51c6\u4e2d\u7684\u95ee\u9898\u6a21\u677f\u3002", "result": "\u6846\u67b6\u6210\u529f\u7ed3\u5408\u81ea\u52a8\u5316\u4e0e\u4eba\u7c7b\u6d1e\u5bdf\u529b\uff0c\u63d0\u5347\u4e86\u504f\u89c1\u8bc4\u4f30\u7684\u6709\u6548\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\u534a\u81ea\u52a8\u5316\u6846\u67b6\u4e3aLLM\u504f\u89c1\u8bc4\u4f30\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\uff0c\u4eba\u7c7b\u53c2\u4e0e\u5bf9\u53d1\u73b0\u6f5c\u5728\u95ee\u9898\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2505.03173", "pdf": "https://arxiv.org/pdf/2505.03173", "abs": "https://arxiv.org/abs/2505.03173", "authors": ["Sameer Malik", "Moyuru Yamada", "Ayush Singh", "Dishank Aggarwal"], "title": "RAVU: Retrieval Augmented Video Understanding with Compositional Reasoning over Graph", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Comprehending long videos remains a significant challenge for Large\nMulti-modal Models (LMMs). Current LMMs struggle to process even minutes to\nhours videos due to their lack of explicit memory and retrieval mechanisms. To\naddress this limitation, we propose RAVU (Retrieval Augmented Video\nUnderstanding), a novel framework for video understanding enhanced by retrieval\nwith compositional reasoning over a spatio-temporal graph. We construct a graph\nrepresentation of the video, capturing both spatial and temporal relationships\nbetween entities. This graph serves as a long-term memory, allowing us to track\nobjects and their actions across time. To answer complex queries, we decompose\nthe queries into a sequence of reasoning steps and execute these steps on the\ngraph, retrieving relevant key information. Our approach enables more accurate\nunderstanding of long videos, particularly for queries that require multi-hop\nreasoning and tracking objects across frames. Our approach demonstrate superior\nperformances with limited retrieved frames (5-10) compared with other SOTA\nmethods and baselines on two major video QA datasets, NExT-QA and EgoSchema.", "AI": {"tldr": "RAVU\u6846\u67b6\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u548c\u65f6\u7a7a\u56fe\u63a8\u7406\uff0c\u89e3\u51b3\u4e86LMMs\u5904\u7406\u957f\u89c6\u9891\u7684\u96be\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u95ee\u7b54\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524dLMMs\u56e0\u7f3a\u4e4f\u663e\u5f0f\u8bb0\u5fc6\u548c\u68c0\u7d22\u673a\u5236\uff0c\u96be\u4ee5\u5904\u7406\u957f\u89c6\u9891\uff0c\u9650\u5236\u4e86\u5176\u5728\u590d\u6742\u89c6\u9891\u7406\u89e3\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u6784\u5efa\u89c6\u9891\u7684\u65f6\u7a7a\u56fe\u8868\u793a\uff0c\u4f5c\u4e3a\u957f\u671f\u8bb0\u5fc6\uff0c\u5e76\u901a\u8fc7\u5206\u89e3\u67e5\u8be2\u4e3a\u591a\u6b65\u63a8\u7406\u5728\u56fe\u4e0a\u6267\u884c\u68c0\u7d22\u3002", "result": "\u5728NExT-QA\u548cEgoSchema\u6570\u636e\u96c6\u4e0a\uff0cRAVU\u4ec5\u97005-10\u5e27\u68c0\u7d22\u5373\u4f18\u4e8e\u5176\u4ed6SOTA\u65b9\u6cd5\u3002", "conclusion": "RAVU\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u548c\u65f6\u7a7a\u56fe\u63a8\u7406\uff0c\u663e\u8457\u63d0\u5347\u4e86\u957f\u89c6\u9891\u7406\u89e3\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2505.03059", "pdf": "https://arxiv.org/pdf/2505.03059", "abs": "https://arxiv.org/abs/2505.03059", "authors": ["Junlin Wang", "Roy Xie", "Shang Zhu", "Jue Wang", "Ben Athiwaratkun", "Bhuwan Dhingra", "Shuaiwen Leon Song", "Ce Zhang", "James Zou"], "title": "Improving Model Alignment Through Collective Intelligence of Open-Source LLMS", "categories": ["cs.CL"], "comment": "ICML 2025", "summary": "Building helpful and harmless large language models (LLMs) requires effective\nmodel alignment approach based on human instructions and feedback, which\nnecessitates high-quality human-labeled data. Constructing such datasets is\noften expensive and hard to scale, and may face potential limitations on\ndiversity and generalization. To address these challenges, we introduce Mixture\nof Agents Alignment (MoAA), that leverages the collective strengths of various\nlanguage models to provide high-quality data for model alignment. By employing\nMoAA, we enhance both supervised fine-tuning and preference optimization,\nleading to improved performance compared to using a single model alone to\ngenerate alignment data (e.g. using GPT-4o alone). Evaluation results show that\nour approach can improve win rate of LLaMA-3.1-8B-Instruct from 19.5 to 48.3 on\nArena-Hard and from 22.33 to 57.23 on AlpacaEval2, highlighting a promising\ndirection for model alignment through this new scalable and diverse synthetic\ndata recipe. Furthermore, we demonstrate that MoAA enables a self-improvement\npipeline, where models finetuned on MoA-generated data surpass their own\ninitial capabilities, providing evidence that our approach can push the\nfrontier of open-source LLMs without reliance on stronger external supervision.\nData and code will be released.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMoAA\uff08Mixture of Agents Alignment\uff09\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u591a\u79cd\u8bed\u8a00\u6a21\u578b\u7684\u96c6\u4f53\u4f18\u52bf\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u5bf9\u9f50\u6570\u636e\uff0c\u4ece\u800c\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u6784\u5efa\u9ad8\u8d28\u91cf\u7684\u4eba\u7c7b\u6807\u6ce8\u6570\u636e\u6210\u672c\u9ad8\u4e14\u96be\u4ee5\u6269\u5c55\uff0c\u4e14\u53ef\u80fd\u7f3a\u4e4f\u591a\u6837\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002MoAA\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u91c7\u7528MoAA\u65b9\u6cd5\uff0c\u7ed3\u5408\u591a\u79cd\u8bed\u8a00\u6a21\u578b\u7684\u4f18\u52bf\u751f\u6210\u5bf9\u9f50\u6570\u636e\uff0c\u7528\u4e8e\u76d1\u7763\u5fae\u8c03\u548c\u504f\u597d\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMoAA\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\uff08\u5982LLaMA-3.1-8B-Instruct\u5728Arena-Hard\u548cAlpacaEval2\u4e0a\u7684\u80dc\u7387\u5927\u5e45\u63d0\u9ad8\uff09\u3002", "conclusion": "MoAA\u4e3a\u6a21\u578b\u5bf9\u9f50\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u591a\u6837\u5316\u7684\u5408\u6210\u6570\u636e\u65b9\u6848\uff0c\u5e76\u80fd\u901a\u8fc7\u81ea\u6539\u8fdb\u6d41\u7a0b\u63d0\u5347\u5f00\u6e90LLM\u7684\u6027\u80fd\u3002"}}
{"id": "2505.03176", "pdf": "https://arxiv.org/pdf/2505.03176", "abs": "https://arxiv.org/abs/2505.03176", "authors": ["Hafez Ghaemi", "Eilif Muller", "Shahab Bakhtiari"], "title": "seq-JEPA: Autoregressive Predictive Learning of Invariant-Equivariant World Models", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Current self-supervised algorithms mostly rely on transformations such as\ndata augmentation and masking to learn visual representations. This is achieved\nby inducing invariance or equivariance with respect to these transformations\nafter encoding two views of an image. This dominant two-view paradigm can limit\nthe flexibility of learned representations for downstream adaptation by\ncreating performance trade-offs between invariance-related tasks such as image\nclassification and more fine-grained equivariance-related tasks. In this work,\nwe introduce \\emph{seq-JEPA}, a world modeling paradigm based on\njoint-embedding predictive architecture that leverages architectural inductive\nbiases to resolve this trade-off. Without requiring an additional equivariance\npredictor or loss term, seq-JEPA simultaneously learns two architecturally\nsegregated representations: one equivariant to the specified transformations\nand another invariant to them and suited for tasks such as classification. To\ndo so, our model processes a short sequence of different views (observations)\nof an input image. Each encoded view is concatenated with embeddings\ncorresponding to the relative transformation (action) producing the next\nobservation in the sequence. A transformer encoder outputs an aggregate\nrepresentation of this sequence, which is subsequently conditioned on the\naction leading to the next observation to predict its representation.\nEmpirically, seq-JEPA achieves strong performance on equivariant benchmarks and\nimage classification without sacrificing one for the other. Additionally, our\nframework excels at tasks that inherently require aggregating a sequence of\nobservations, such as path integration across actions and predictive learning\nacross eye movements.", "AI": {"tldr": "seq-JEPA\u662f\u4e00\u79cd\u57fa\u4e8e\u8054\u5408\u5d4c\u5165\u9884\u6d4b\u67b6\u6784\u7684\u4e16\u754c\u5efa\u6a21\u8303\u5f0f\uff0c\u901a\u8fc7\u5904\u7406\u56fe\u50cf\u7684\u4e0d\u540c\u89c6\u56fe\u5e8f\u5217\uff0c\u540c\u65f6\u5b66\u4e60\u4e0d\u53d8\u548c\u7b49\u53d8\u8868\u793a\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u4e24\u89c6\u56fe\u8303\u5f0f\u5728\u4efb\u52a1\u9002\u5e94\u6027\u4e0a\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5f53\u524d\u81ea\u76d1\u7763\u7b97\u6cd5\u4e3b\u8981\u4f9d\u8d56\u6570\u636e\u589e\u5f3a\u548c\u63a9\u7801\u7b49\u53d8\u6362\u5b66\u4e60\u89c6\u89c9\u8868\u793a\uff0c\u4f46\u4e24\u89c6\u56fe\u8303\u5f0f\u9650\u5236\u4e86\u8868\u793a\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u7075\u6d3b\u6027\uff0c\u5bfc\u81f4\u4e0d\u53d8\u6027\u548c\u7b49\u53d8\u6027\u4efb\u52a1\u4e4b\u95f4\u7684\u6027\u80fd\u6743\u8861\u3002", "method": "seq-JEPA\u901a\u8fc7\u5904\u7406\u8f93\u5165\u56fe\u50cf\u7684\u4e0d\u540c\u89c6\u56fe\u5e8f\u5217\uff0c\u7ed3\u5408\u76f8\u5bf9\u53d8\u6362\u7684\u5d4c\u5165\uff0c\u4f7f\u7528Transformer\u7f16\u7801\u5668\u751f\u6210\u5e8f\u5217\u7684\u805a\u5408\u8868\u793a\uff0c\u5e76\u9884\u6d4b\u4e0b\u4e00\u4e2a\u89c6\u56fe\u7684\u8868\u793a\uff0c\u65e0\u9700\u989d\u5916\u7684\u7b49\u53d8\u9884\u6d4b\u5668\u6216\u635f\u5931\u9879\u3002", "result": "seq-JEPA\u5728\u7b49\u53d8\u57fa\u51c6\u548c\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u65e0\u9700\u727a\u7272\u4efb\u4e00\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u540c\u65f6\u5728\u9700\u8981\u5e8f\u5217\u805a\u5408\u7684\u4efb\u52a1\uff08\u5982\u8def\u5f84\u6574\u5408\u548c\u9884\u6d4b\u5b66\u4e60\uff09\u4e2d\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "seq-JEPA\u901a\u8fc7\u8054\u5408\u5d4c\u5165\u9884\u6d4b\u67b6\u6784\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4e0d\u53d8\u6027\u548c\u7b49\u53d8\u6027\u4efb\u52a1\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\uff0c\u4e3a\u89c6\u89c9\u8868\u793a\u5b66\u4e60\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.03229", "pdf": "https://arxiv.org/pdf/2505.03229", "abs": "https://arxiv.org/abs/2505.03229", "authors": ["Behrooz Mansouri"], "title": "Survey of Abstract Meaning Representation: Then, Now, Future", "categories": ["cs.CL"], "comment": null, "summary": "This paper presents a survey of Abstract Meaning Representation (AMR), a\nsemantic representation framework that captures the meaning of sentences\nthrough a graph-based structure. AMR represents sentences as rooted, directed\nacyclic graphs, where nodes correspond to concepts and edges denote\nrelationships, effectively encoding the meaning of complex sentences. This\nsurvey investigates AMR and its extensions, focusing on AMR capabilities. It\nthen explores the parsing (text-to-AMR) and generation (AMR-to-text) tasks by\nshowing traditional, current, and possible futures approaches. It also reviews\nvarious applications of AMR including text generation, text classification, and\ninformation extraction and information seeking. By analyzing recent\ndevelopments and challenges in the field, this survey provides insights into\nfuture directions for research and the potential impact of AMR on enhancing\nmachine understanding of human language.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u62bd\u8c61\u610f\u4e49\u8868\u793a\uff08AMR\uff09\u53ca\u5176\u6269\u5c55\uff0c\u63a2\u8ba8\u4e86\u5176\u89e3\u6790\u548c\u751f\u6210\u4efb\u52a1\uff0c\u5e76\u56de\u987e\u4e86AMR\u5728\u6587\u672c\u751f\u6210\u3001\u5206\u7c7b\u548c\u4fe1\u606f\u63d0\u53d6\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u7814\u7a76AMR\u7684\u76ee\u7684\u662f\u4e3a\u4e86\u589e\u5f3a\u673a\u5668\u5bf9\u4eba\u7c7b\u8bed\u8a00\u7684\u7406\u89e3\u80fd\u529b\uff0c\u901a\u8fc7\u56fe\u7ed3\u6784\u6355\u6349\u53e5\u5b50\u8bed\u4e49\u3002", "method": "\u901a\u8fc7\u8c03\u67e5AMR\u53ca\u5176\u6269\u5c55\uff0c\u5206\u6790\u89e3\u6790\u548c\u751f\u6210\u4efb\u52a1\u7684\u4f20\u7edf\u3001\u5f53\u524d\u548c\u672a\u6765\u65b9\u6cd5\uff0c\u5e76\u603b\u7ed3AMR\u7684\u5e94\u7528\u3002", "result": "\u7efc\u8ff0\u63ed\u793a\u4e86AMR\u5728\u8bed\u4e49\u8868\u793a\u548c\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u53ca\u5176\u9762\u4e34\u7684\u6311\u6218\u3002", "conclusion": "AMR\u4e3a\u673a\u5668\u7406\u89e3\u4eba\u7c7b\u8bed\u8a00\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\uff0c\u672a\u6765\u7814\u7a76\u9700\u89e3\u51b3\u73b0\u6709\u6311\u6218\u4ee5\u8fdb\u4e00\u6b65\u53d1\u6325\u5176\u6f5c\u529b\u3002"}}
{"id": "2505.03184", "pdf": "https://arxiv.org/pdf/2505.03184", "abs": "https://arxiv.org/abs/2505.03184", "authors": ["Xiang Xu", "Ruotong Li", "Mengjun Yi", "Baile XU", "Furao Shen", "Jian Zhao"], "title": "Interactive Instance Annotation with Siamese Networks", "categories": ["cs.CV"], "comment": null, "summary": "Annotating instance masks is time-consuming and labor-intensive. A promising\nsolution is to predict contours using a deep learning model and then allow\nusers to refine them. However, most existing methods focus on in-domain\nscenarios, limiting their effectiveness for cross-domain annotation tasks. In\nthis paper, we propose SiamAnno, a framework inspired by the use of Siamese\nnetworks in object tracking. SiamAnno leverages one-shot learning to annotate\npreviously unseen objects by taking a bounding box as input and predicting\nobject boundaries, which can then be adjusted by annotators. Trained on one\ndataset and tested on another without fine-tuning, SiamAnno achieves\nstate-of-the-art (SOTA) performance across multiple datasets, demonstrating its\nability to handle domain and environment shifts in cross-domain tasks. We also\nprovide more comprehensive results compared to previous work, establishing a\nstrong baseline for future research. To our knowledge, SiamAnno is the first\nmodel to explore Siamese architecture for instance annotation.", "AI": {"tldr": "SiamAnno\u662f\u4e00\u4e2a\u57fa\u4e8eSiamese\u7f51\u7edc\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u8de8\u57df\u5b9e\u4f8b\u6807\u6ce8\u4efb\u52a1\uff0c\u901a\u8fc7\u4e00\u6b21\u6027\u5b66\u4e60\u9884\u6d4b\u5bf9\u8c61\u8fb9\u754c\uff0c\u65e0\u9700\u5fae\u8c03\u5373\u53ef\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0SOTA\u6027\u80fd\u3002", "motivation": "\u5b9e\u4f8b\u6807\u6ce8\u8017\u65f6\u8017\u529b\uff0c\u73b0\u6709\u65b9\u6cd5\u591a\u5c40\u9650\u4e8e\u540c\u57df\u573a\u666f\uff0c\u96be\u4ee5\u5e94\u5bf9\u8de8\u57df\u4efb\u52a1\u3002", "method": "\u5229\u7528Siamese\u7f51\u7edc\u548c\u4e00\u6b21\u6027\u5b66\u4e60\uff0c\u901a\u8fc7\u8f93\u5165\u8fb9\u754c\u6846\u9884\u6d4b\u5bf9\u8c61\u8fb9\u754c\uff0c\u652f\u6301\u7528\u6237\u8c03\u6574\u3002", "result": "\u5728\u672a\u5fae\u8c03\u7684\u60c5\u51b5\u4e0b\uff0cSiamAnno\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "SiamAnno\u662f\u9996\u4e2a\u63a2\u7d22Siamese\u67b6\u6784\u7528\u4e8e\u5b9e\u4f8b\u6807\u6ce8\u7684\u6a21\u578b\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u5f3a\u57fa\u7ebf\u3002"}}
{"id": "2505.03293", "pdf": "https://arxiv.org/pdf/2505.03293", "abs": "https://arxiv.org/abs/2505.03293", "authors": ["Shijing Zhu", "Zhuang Chen", "Guanqun Bi", "Binghang Li", "Yaxi Deng", "Dazhen Wan", "Libiao Peng", "Xiyao Xiao", "Rongsheng Zhang", "Tangjie Lv", "Zhipeng Hu", "FangFang Li", "Minlie Huang"], "title": "\u03a8-Arena: Interactive Assessment and Optimization of LLM-based Psychological Counselors with Tripartite Feedback", "categories": ["cs.CL"], "comment": "in progress", "summary": "Large language models (LLMs) have shown promise in providing scalable mental\nhealth support, while evaluating their counseling capability remains crucial to\nensure both efficacy and safety. Existing evaluations are limited by the static\nassessment that focuses on knowledge tests, the single perspective that centers\non user experience, and the open-loop framework that lacks actionable feedback.\nTo address these issues, we propose {\\Psi}-Arena, an interactive framework for\ncomprehensive assessment and optimization of LLM-based counselors, featuring\nthree key characteristics: (1) Realistic arena interactions that simulate\nreal-world counseling through multi-stage dialogues with psychologically\nprofiled NPC clients, (2) Tripartite evaluation that integrates assessments\nfrom the client, counselor, and supervisor perspectives, and (3) Closed-loop\noptimization that iteratively improves LLM counselors using diagnostic\nfeedback. Experiments across eight state-of-the-art LLMs show significant\nperformance variations in different real-world scenarios and evaluation\nperspectives. Moreover, reflection-based optimization results in up to a 141%\nimprovement in counseling performance. We hope PsychoArena provides a\nfoundational resource for advancing reliable and human-aligned LLM applications\nin mental healthcare.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86Psi-Arena\u6846\u67b6\uff0c\u7528\u4e8e\u5168\u9762\u8bc4\u4f30\u548c\u4f18\u5316\u57fa\u4e8eLLM\u7684\u5fc3\u7406\u54a8\u8be2\u5e08\uff0c\u901a\u8fc7\u591a\u9636\u6bb5\u5bf9\u8bdd\u3001\u4e09\u65b9\u8bc4\u4f30\u548c\u95ed\u73af\u4f18\u5316\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u5728\u9759\u6001\u6d4b\u8bd5\u3001\u5355\u4e00\u89c6\u89d2\u548c\u5f00\u73af\u6846\u67b6\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u65e0\u6cd5\u5168\u9762\u8bc4\u4f30LLM\u5fc3\u7406\u54a8\u8be2\u5e08\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51faPsi-Arena\u6846\u67b6\uff0c\u5305\u62ec\u6a21\u62df\u771f\u5b9e\u54a8\u8be2\u7684\u591a\u9636\u6bb5\u5bf9\u8bdd\u3001\u4e09\u65b9\u8bc4\u4f30\uff08\u5ba2\u6237\u3001\u54a8\u8be2\u5e08\u3001\u7763\u5bfc\uff09\u548c\u95ed\u73af\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u4e0d\u540cLLM\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u8868\u73b0\u5dee\u5f02\u663e\u8457\uff0c\u4f18\u5316\u540e\u54a8\u8be2\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe141%\u3002", "conclusion": "Psi-Arena\u4e3a\u5fc3\u7406\u4fdd\u5065\u9886\u57df\u53ef\u9760\u4e14\u4eba\u6027\u5316\u7684LLM\u5e94\u7528\u63d0\u4f9b\u4e86\u57fa\u7840\u8d44\u6e90\u3002"}}
{"id": "2505.03203", "pdf": "https://arxiv.org/pdf/2505.03203", "abs": "https://arxiv.org/abs/2505.03203", "authors": ["Chang Xie", "Chenyi Zhuang", "Pan Gao"], "title": "PiCo: Enhancing Text-Image Alignment with Improved Noise Selection and Precise Mask Control in Diffusion Models", "categories": ["cs.CV"], "comment": null, "summary": "Advanced diffusion models have made notable progress in text-to-image\ncompositional generation. However, it is still a challenge for existing models\nto achieve text-image alignment when confronted with complex text prompts. In\nthis work, we highlight two factors that affect this alignment: the quality of\nthe randomly initialized noise and the reliability of the generated controlling\nmask. We then propose PiCo (Pick-and-Control), a novel training-free approach\nwith two key components to tackle these two factors. First, we develop a noise\nselection module to assess the quality of the random noise and determine\nwhether the noise is suitable for the target text. A fast sampling strategy is\nutilized to ensure efficiency in the noise selection stage. Second, we\nintroduce a referring mask module to generate pixel-level masks and to\nprecisely modulate the cross-attention maps. The referring mask is applied to\nthe standard diffusion process to guide the reasonable interaction between text\nand image features. Extensive experiments have been conducted to verify the\neffectiveness of PiCo in liberating users from the tedious process of random\ngeneration and in enhancing the text-image alignment for diverse text\ndescriptions.", "AI": {"tldr": "PiCo\uff08Pick-and-Control\uff09\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u566a\u58f0\u9009\u62e9\u6a21\u5757\u548c\u53c2\u8003\u63a9\u7801\u6a21\u5757\u63d0\u5347\u6587\u672c-\u56fe\u50cf\u5bf9\u9f50\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u6269\u6563\u6a21\u578b\u5728\u590d\u6742\u6587\u672c\u63d0\u793a\u4e0b\u96be\u4ee5\u5b9e\u73b0\u6587\u672c-\u56fe\u50cf\u5bf9\u9f50\uff0c\u4e3b\u8981\u53d7\u968f\u673a\u521d\u59cb\u5316\u566a\u58f0\u8d28\u91cf\u548c\u751f\u6210\u63a7\u5236\u63a9\u7801\u53ef\u9760\u6027\u7684\u5f71\u54cd\u3002", "method": "PiCo\u5305\u542b\u566a\u58f0\u9009\u62e9\u6a21\u5757\uff08\u8bc4\u4f30\u566a\u58f0\u8d28\u91cf\u5e76\u5feb\u901f\u91c7\u6837\uff09\u548c\u53c2\u8003\u63a9\u7801\u6a21\u5757\uff08\u751f\u6210\u50cf\u7d20\u7ea7\u63a9\u7801\u5e76\u8c03\u5236\u4ea4\u53c9\u6ce8\u610f\u529b\u56fe\uff09\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660ePiCo\u80fd\u6709\u6548\u51cf\u5c11\u968f\u673a\u751f\u6210\u7684\u7e41\u7410\u6027\uff0c\u5e76\u663e\u8457\u63d0\u5347\u591a\u6837\u6587\u672c\u63cf\u8ff0\u7684\u6587\u672c-\u56fe\u50cf\u5bf9\u9f50\u6548\u679c\u3002", "conclusion": "PiCo\u901a\u8fc7\u566a\u58f0\u9009\u62e9\u548c\u63a9\u7801\u8c03\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u6587\u672c\u63d0\u793a\u4e0b\u7684\u751f\u6210\u8d28\u91cf\u548c\u5bf9\u9f50\u6548\u679c\u3002"}}
{"id": "2505.03320", "pdf": "https://arxiv.org/pdf/2505.03320", "abs": "https://arxiv.org/abs/2505.03320", "authors": ["Junyu Ma", "Tianqing Fang", "Zhisong Zhang", "Hongming Zhang", "Haitao Mi", "Dong Yu"], "title": "Recall with Reasoning: Chain-of-Thought Distillation for Mamba's Long-Context Memory and Extrapolation", "categories": ["cs.CL"], "comment": null, "summary": "Mamba's theoretical infinite-context potential is limited in practice when\nsequences far exceed training lengths. This work explores unlocking Mamba's\nlong-context memory ability by a simple-yet-effective method, Recall with\nReasoning (RwR), by distilling chain-of-thought (CoT) summarization from a\nteacher model. Specifically, RwR prepends these summarization as CoT prompts\nduring fine-tuning, teaching Mamba to actively recall and reason over long\ncontexts. Experiments on LONGMEMEVAL and HELMET show RwR boosts Mamba's\nlong-context performance against comparable Transformer/hybrid baselines under\nsimilar pretraining conditions, while preserving short-context capabilities,\nall without architectural changes.", "AI": {"tldr": "\u901a\u8fc7Recall with Reasoning (RwR)\u65b9\u6cd5\uff0c\u63d0\u5347Mamba\u6a21\u578b\u7684\u957f\u4e0a\u4e0b\u6587\u8bb0\u5fc6\u80fd\u529b\uff0c\u65e0\u9700\u67b6\u6784\u6539\u52a8\u3002", "motivation": "Mamba\u6a21\u578b\u5728\u7406\u8bba\u4e0a\u5177\u6709\u65e0\u9650\u4e0a\u4e0b\u6587\u6f5c\u529b\uff0c\u4f46\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u5f53\u5e8f\u5217\u8fdc\u8d85\u8bad\u7ec3\u957f\u5ea6\u65f6\u8868\u73b0\u53d7\u9650\u3002", "method": "\u63d0\u51faRwR\u65b9\u6cd5\uff0c\u901a\u8fc7\u4ece\u6559\u5e08\u6a21\u578b\u4e2d\u63d0\u53d6\u94fe\u5f0f\u601d\u7ef4\uff08CoT\uff09\u6458\u8981\uff0c\u5e76\u5728\u5fae\u8c03\u65f6\u5c06\u5176\u4f5c\u4e3aCoT\u63d0\u793a\u524d\u7f6e\uff0c\u6559\u4f1aMamba\u4e3b\u52a8\u56de\u5fc6\u548c\u63a8\u7406\u957f\u4e0a\u4e0b\u6587\u3002", "result": "\u5728LONGMEMEVAL\u548cHELMET\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cRwR\u5728\u76f8\u4f3c\u9884\u8bad\u7ec3\u6761\u4ef6\u4e0b\u4f18\u4e8eTransformer/\u6df7\u5408\u57fa\u7ebf\uff0c\u540c\u65f6\u4fdd\u6301\u77ed\u4e0a\u4e0b\u6587\u80fd\u529b\u3002", "conclusion": "RwR\u662f\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u53ef\u663e\u8457\u63d0\u5347Mamba\u7684\u957f\u4e0a\u4e0b\u6587\u6027\u80fd\uff0c\u4e14\u4e0d\u5f71\u54cd\u5176\u77ed\u4e0a\u4e0b\u6587\u8868\u73b0\u3002"}}
{"id": "2505.03204", "pdf": "https://arxiv.org/pdf/2505.03204", "abs": "https://arxiv.org/abs/2505.03204", "authors": ["Liu Suxing", "Byungwon Min"], "title": "DCS-ST for Classification of Breast Cancer Histopathology Images with Limited Annotations", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Deep learning methods have shown promise in classifying breast cancer\nhistopathology images, but their performance often declines with limited\nannotated data, a critical challenge in medical imaging due to the high cost\nand expertise required for annotations.", "AI": {"tldr": "\u6df1\u5ea6\u5b66\u4e60\u5728\u4e73\u817a\u764c\u75c5\u7406\u56fe\u50cf\u5206\u7c7b\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u6807\u6ce8\u6570\u636e\u6709\u9650\u65f6\u6027\u80fd\u4e0b\u964d\u3002", "motivation": "\u89e3\u51b3\u533b\u5b66\u5f71\u50cf\u4e2d\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u4e14\u6602\u8d35\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u3002", "result": "\u5728\u6807\u6ce8\u6570\u636e\u6709\u9650\u65f6\u6027\u80fd\u4e0b\u964d\u3002", "conclusion": "\u9700\u8981\u6539\u8fdb\u65b9\u6cd5\u4ee5\u5e94\u5bf9\u6570\u636e\u7a00\u7f3a\u7684\u6311\u6218\u3002"}}
{"id": "2505.03406", "pdf": "https://arxiv.org/pdf/2505.03406", "abs": "https://arxiv.org/abs/2505.03406", "authors": ["Mohammad Shoaib Ansari", "Mohd Sohail Ali Khan", "Shubham Revankar", "Aditya Varma", "Anil S. Mokhade"], "title": "Lightweight Clinical Decision Support System using QLoRA-Fine-Tuned LLMs and Retrieval-Augmented Generation", "categories": ["cs.CL", "cs.AI"], "comment": "12 pages", "summary": "This research paper investigates the application of Large Language Models\n(LLMs) in healthcare, specifically focusing on enhancing medical decision\nsupport through Retrieval-Augmented Generation (RAG) integrated with\nhospital-specific data and fine-tuning using Quantized Low-Rank Adaptation\n(QLoRA). The system utilizes Llama 3.2-3B-Instruct as its foundation model. By\nembedding and retrieving context-relevant healthcare information, the system\nsignificantly improves response accuracy. QLoRA facilitates notable parameter\nefficiency and memory optimization, preserving the integrity of medical\ninformation through specialized quantization techniques. Our research also\nshows that our model performs relatively well on various medical benchmarks,\nindicating that it can be used to make basic medical suggestions. This paper\ndetails the system's technical components, including its architecture,\nquantization methods, and key healthcare applications such as enhanced disease\nprediction from patient symptoms and medical history, treatment suggestions,\nand efficient summarization of complex medical reports. We touch on the ethical\nconsiderations-patient privacy, data security, and the need for rigorous\nclinical validation-as well as the practical challenges of integrating such\nsystems into real-world healthcare workflows. Furthermore, the lightweight\nquantized weights ensure scalability and ease of deployment even in\nlow-resource hospital environments. Finally, the paper concludes with an\nanalysis of the broader impact of LLMs on healthcare and outlines future\ndirections for LLMs in medical settings.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u533b\u7597\u9886\u57df\u7684\u5e94\u7528\uff0c\u901a\u8fc7\u7ed3\u5408\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u548c\u91cf\u5316\u4f4e\u79e9\u9002\u5e94\uff08QLoRA\uff09\u6280\u672f\uff0c\u63d0\u5347\u533b\u7597\u51b3\u7b56\u652f\u6301\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "motivation": "\u65e8\u5728\u89e3\u51b3\u533b\u7597\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u4e2d\u4fe1\u606f\u51c6\u786e\u6027\u548c\u6548\u7387\u7684\u95ee\u9898\uff0c\u540c\u65f6\u4f18\u5316\u8d44\u6e90\u5229\u7528\u3002", "method": "\u91c7\u7528Llama 3.2-3B-Instruct\u4f5c\u4e3a\u57fa\u7840\u6a21\u578b\uff0c\u7ed3\u5408RAG\u548cQLoRA\u6280\u672f\uff0c\u5d4c\u5165\u548c\u68c0\u7d22\u533b\u7597\u6570\u636e\u3002", "result": "\u7cfb\u7edf\u663e\u8457\u63d0\u9ad8\u4e86\u54cd\u5e94\u51c6\u786e\u6027\uff0c\u5e76\u5728\u591a\u4e2a\u533b\u7597\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u9002\u7528\u4e8e\u57fa\u7840\u533b\u7597\u5efa\u8bae\u3002", "conclusion": "LLMs\u5728\u533b\u7597\u9886\u57df\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u9700\u5173\u6ce8\u4f26\u7406\u95ee\u9898\u548c\u5b9e\u9645\u90e8\u7f72\u6311\u6218\uff0c\u672a\u6765\u53ef\u8fdb\u4e00\u6b65\u4f18\u5316\u548c\u6269\u5c55\u3002"}}
{"id": "2505.03220", "pdf": "https://arxiv.org/pdf/2505.03220", "abs": "https://arxiv.org/abs/2505.03220", "authors": ["Shaheer Mohamed", "Tharindu Fernando", "Sridha Sridharan", "Peyman Moghadam", "Clinton Fookes"], "title": "Dual-Domain Masked Image Modeling: A Self-Supervised Pretraining Strategy Using Spatial and Frequency Domain Masking for Hyperspectral Data", "categories": ["cs.CV"], "comment": "Preprint to appear in IEEE IGARSS 2025", "summary": "Hyperspectral images (HSIs) capture rich spectral signatures that reveal\nvital material properties, offering broad applicability across various domains.\nHowever, the scarcity of labeled HSI data limits the full potential of deep\nlearning, especially for transformer-based architectures that require\nlarge-scale training. To address this constraint, we propose Spatial-Frequency\nMasked Image Modeling (SFMIM), a self-supervised pretraining strategy for\nhyperspectral data that utilizes the large portion of unlabeled data. Our\nmethod introduces a novel dual-domain masking mechanism that operates in both\nspatial and frequency domains. The input HSI cube is initially divided into\nnon-overlapping patches along the spatial dimension, with each patch comprising\nthe entire spectrum of its corresponding spatial location. In spatial masking,\nwe randomly mask selected patches and train the model to reconstruct the masked\ninputs using the visible patches. Concurrently, in frequency masking, we remove\nportions of the frequency components of the input spectra and predict the\nmissing frequencies. By learning to reconstruct these masked components, the\ntransformer-based encoder captures higher-order spectral-spatial correlations.\nWe evaluate our approach on three publicly available HSI classification\nbenchmarks and demonstrate that it achieves state-of-the-art performance.\nNotably, our model shows rapid convergence during fine-tuning, highlighting the\nefficiency of our pretraining strategy.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSFMIM\u7684\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u7a7a\u95f4\u548c\u9891\u7387\u53cc\u57df\u63a9\u7801\u673a\u5236\uff0c\u5229\u7528\u672a\u6807\u8bb0\u7684HSI\u6570\u636e\u63d0\u5347\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u6807\u8bb0HSI\u6570\u636e\u7a00\u7f3a\u9650\u5236\u4e86\u6df1\u5ea6\u5b66\u4e60\u6f5c\u529b\uff0c\u5c24\u5176\u662f\u9700\u8981\u5927\u89c4\u6a21\u8bad\u7ec3\u7684Transformer\u67b6\u6784\u3002", "method": "\u63d0\u51faSFMIM\u65b9\u6cd5\uff0c\u7ed3\u5408\u7a7a\u95f4\u548c\u9891\u7387\u63a9\u7801\u673a\u5236\uff0c\u901a\u8fc7\u91cd\u5efa\u63a9\u7801\u8f93\u5165\u5b66\u4e60\u9ad8\u9636\u8c31\u7a7a\u76f8\u5173\u6027\u3002", "result": "\u5728\u4e09\u4e2a\u516c\u5f00HSI\u5206\u7c7b\u57fa\u51c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\uff0c\u4e14\u5fae\u8c03\u65f6\u6536\u655b\u901f\u5ea6\u5feb\u3002", "conclusion": "SFMIM\u65b9\u6cd5\u6709\u6548\u5229\u7528\u4e86\u672a\u6807\u8bb0\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u4e86HSI\u5206\u7c7b\u6027\u80fd\u3002"}}
{"id": "2505.03427", "pdf": "https://arxiv.org/pdf/2505.03427", "abs": "https://arxiv.org/abs/2505.03427", "authors": ["Mouath Abu Daoud", "Chaimae Abouzahir", "Leen Kharouf", "Walid Al-Eisawi", "Nizar Habash", "Farah E. Shamout"], "title": "MedArabiQ: Benchmarking Large Language Models on Arabic Medical Tasks", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "21 pages", "summary": "Large Language Models (LLMs) have demonstrated significant promise for\nvarious applications in healthcare. However, their efficacy in the Arabic\nmedical domain remains unexplored due to the lack of high-quality\ndomain-specific datasets and benchmarks. This study introduces MedArabiQ, a\nnovel benchmark dataset consisting of seven Arabic medical tasks, covering\nmultiple specialties and including multiple choice questions,\nfill-in-the-blank, and patient-doctor question answering. We first constructed\nthe dataset using past medical exams and publicly available datasets. We then\nintroduced different modifications to evaluate various LLM capabilities,\nincluding bias mitigation. We conducted an extensive evaluation with five\nstate-of-the-art open-source and proprietary LLMs, including GPT-4o, Claude\n3.5-Sonnet, and Gemini 1.5. Our findings highlight the need for the creation of\nnew high-quality benchmarks that span different languages to ensure fair\ndeployment and scalability of LLMs in healthcare. By establishing this\nbenchmark and releasing the dataset, we provide a foundation for future\nresearch aimed at evaluating and enhancing the multilingual capabilities of\nLLMs for the equitable use of generative AI in healthcare.", "AI": {"tldr": "\u7814\u7a76\u4ecb\u7ecd\u4e86MedArabiQ\uff0c\u4e00\u4e2a\u963f\u62c9\u4f2f\u8bed\u533b\u7597\u9886\u57df\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u6027\u80fd\uff0c\u5e76\u5f3a\u8c03\u591a\u8bed\u8a00\u9ad8\u8d28\u91cf\u57fa\u51c6\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u963f\u62c9\u4f2f\u8bed\u533b\u7597\u9886\u57df\u4e2d\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u7684\u6570\u636e\u96c6\u548c\u57fa\u51c6\uff0c\u9650\u5236\u4e86LLMs\u5728\u8be5\u9886\u57df\u7684\u5e94\u7528\u548c\u8bc4\u4f30\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b\u4e03\u9879\u963f\u62c9\u4f2f\u8bed\u533b\u7597\u4efb\u52a1\u7684MedArabiQ\u6570\u636e\u96c6\uff0c\u5e76\u8bc4\u4f30\u4e86\u4e94\u79cd\u5148\u8fdbLLMs\u7684\u6027\u80fd\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u73b0\u6709LLMs\u5728\u963f\u62c9\u4f2f\u8bed\u533b\u7597\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4e00\uff0c\u51f8\u663e\u4e86\u591a\u8bed\u8a00\u57fa\u51c6\u7684\u5fc5\u8981\u6027\u3002", "conclusion": "\u901a\u8fc7MedArabiQ\u6570\u636e\u96c6\uff0c\u4e3a\u672a\u6765\u591a\u8bed\u8a00LLMs\u7684\u7814\u7a76\u548c\u516c\u5e73\u5e94\u7528\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2505.03242", "pdf": "https://arxiv.org/pdf/2505.03242", "abs": "https://arxiv.org/abs/2505.03242", "authors": ["Davide Talon", "Federico Girella", "Ziyue Liu", "Marco Cristani", "Yiming Wang"], "title": "Seeing the Abstract: Translating the Abstract Language for Vision Language Models", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted to CVPR25. Project page:\n  https://davidetalon.github.io/fashionact-page/", "summary": "Natural language goes beyond dryly describing visual content. It contains\nrich abstract concepts to express feeling, creativity and properties that\ncannot be directly perceived. Yet, current research in Vision Language Models\n(VLMs) has not shed light on abstract-oriented language. Our research breaks\nnew ground by uncovering its wide presence and under-estimated value, with\nextensive analysis. Particularly, we focus our investigation on the fashion\ndomain, a highly-representative field with abstract expressions. By analyzing\nrecent large-scale multimodal fashion datasets, we find that abstract terms\nhave a dominant presence, rivaling the concrete ones, providing novel\ninformation, and being useful in the retrieval task. However, a critical\nchallenge emerges: current general-purpose or fashion-specific VLMs are\npre-trained with databases that lack sufficient abstract words in their text\ncorpora, thus hindering their ability to effectively represent\nabstract-oriented language. We propose a training-free and model-agnostic\nmethod, Abstract-to-Concrete Translator (ACT), to shift abstract\nrepresentations towards well-represented concrete ones in the VLM latent space,\nusing pre-trained models and existing multimodal databases. On the\ntext-to-image retrieval task, despite being training-free, ACT outperforms the\nfine-tuned VLMs in both same- and cross-dataset settings, exhibiting its\neffectiveness with a strong generalization capability. Moreover, the\nimprovement introduced by ACT is consistent with various VLMs, making it a\nplug-and-play solution.", "AI": {"tldr": "\u8bba\u6587\u63ed\u793a\u4e86\u62bd\u8c61\u8bed\u8a00\u5728\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u6a21\u578b\u65e0\u5173\u65b9\u6cd5\uff08ACT\uff09\uff0c\u5728\u68c0\u7d22\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u5fae\u8c03\u7684VLM\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7814\u7a76\u5ffd\u89c6\u4e86\u62bd\u8c61\u8bed\u8a00\u7684\u4ef7\u503c\uff0c\u800c\u62bd\u8c61\u8bed\u8a00\u5728\u65f6\u5c1a\u7b49\u9886\u57df\u5177\u6709\u5e7f\u6cdb\u5b58\u5728\u548c\u91cd\u8981\u4fe1\u606f\u3002", "method": "\u63d0\u51faAbstract-to-Concrete Translator\uff08ACT\uff09\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u548c\u591a\u6a21\u6001\u6570\u636e\u5e93\uff0c\u5c06\u62bd\u8c61\u8868\u793a\u6620\u5c04\u5230VLM\u6f5c\u5728\u7a7a\u95f4\u4e2d\u5df2\u5145\u5206\u8868\u793a\u7684\u5177\u8c61\u8868\u793a\u3002", "result": "ACT\u5728\u6587\u672c\u5230\u56fe\u50cf\u68c0\u7d22\u4efb\u52a1\u4e2d\u4f18\u4e8e\u5fae\u8c03\u7684VLM\uff0c\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4e14\u9002\u7528\u4e8e\u591a\u79cdVLM\u3002", "conclusion": "ACT\u662f\u4e00\u79cd\u5373\u63d2\u5373\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86VLM\u5904\u7406\u62bd\u8c61\u8bed\u8a00\u7684\u80fd\u529b\u3002"}}
{"id": "2505.03452", "pdf": "https://arxiv.org/pdf/2505.03452", "abs": "https://arxiv.org/abs/2505.03452", "authors": ["Matan Orbach", "Ohad Eytan", "Benjamin Sznajder", "Ariel Gera", "Odellia Boni", "Yoav Kantor", "Gal Bloch", "Omri Levy", "Hadas Abraham", "Nitzan Barzilay", "Eyal Shnarch", "Michael E. Factor", "Shila Ofek-Koifman", "Paula Ta-Shma", "Assaf Toledo"], "title": "An Analysis of Hyper-Parameter Optimization Methods for Retrieval Augmented Generation", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Finding the optimal Retrieval-Augmented Generation (RAG) configuration for a\ngiven use case can be complex and expensive. Motivated by this challenge,\nframeworks for RAG hyper-parameter optimization (HPO) have recently emerged,\nyet their effectiveness has not been rigorously benchmarked. To address this\ngap, we present a comprehensive study involving 5 HPO algorithms over 5\ndatasets from diverse domains, including a new one collected for this work on\nreal-world product documentation. Our study explores the largest HPO search\nspace considered to date, with two optimized evaluation metrics. Analysis of\nthe results shows that RAG HPO can be done efficiently, either greedily or with\niterative random search, and that it significantly boosts RAG performance for\nall datasets. For greedy HPO approaches, we show that optimizing models first\nis preferable to the prevalent practice of optimizing sequentially according to\nthe RAG pipeline order.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7684\u8d85\u53c2\u6570\u4f18\u5316\uff08HPO\uff09\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u901a\u8fc75\u79cdHPO\u7b97\u6cd5\u548c5\u4e2a\u6570\u636e\u96c6\u8fdb\u884c\u4e86\u5168\u9762\u8bc4\u4f30\uff0c\u53d1\u73b0\u8d2a\u5a6a\u6216\u8fed\u4ee3\u968f\u673a\u641c\u7d22\u80fd\u9ad8\u6548\u63d0\u5347RAG\u6027\u80fd\u3002", "motivation": "\u7531\u4e8e\u4e3a\u7279\u5b9a\u7528\u4f8b\u627e\u5230\u6700\u4f18RAG\u914d\u7f6e\u590d\u6742\u4e14\u6602\u8d35\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u73b0\u6709HPO\u6846\u67b6\u7f3a\u4e4f\u4e25\u683c\u57fa\u51c6\u6d4b\u8bd5\u7684\u7a7a\u767d\u3002", "method": "\u7814\u7a76\u91c7\u7528\u4e865\u79cdHPO\u7b97\u6cd5\uff0c\u57285\u4e2a\u591a\u6837\u5316\u6570\u636e\u96c6\uff08\u5305\u62ec\u4e00\u4e2a\u65b0\u6536\u96c6\u7684\u771f\u5b9e\u4ea7\u54c1\u6587\u6863\u6570\u636e\u96c6\uff09\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u63a2\u7d22\u4e86\u8fc4\u4eca\u6700\u5927\u7684HPO\u641c\u7d22\u7a7a\u95f4\uff0c\u5e76\u4f7f\u7528\u4e24\u79cd\u4f18\u5316\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u8d2a\u5a6a\u6216\u8fed\u4ee3\u968f\u673a\u641c\u7d22\u80fd\u9ad8\u6548\u5b8c\u6210RAG HPO\uff0c\u5e76\u663e\u8457\u63d0\u5347\u6240\u6709\u6570\u636e\u96c6\u7684\u6027\u80fd\u3002\u8d2a\u5a6a\u65b9\u6cd5\u4e2d\uff0c\u4f18\u5148\u4f18\u5316\u6a21\u578b\u6bd4\u6309RAG\u6d41\u7a0b\u987a\u5e8f\u4f18\u5316\u66f4\u6709\u6548\u3002", "conclusion": "RAG HPO\u53ef\u901a\u8fc7\u8d2a\u5a6a\u6216\u968f\u673a\u641c\u7d22\u9ad8\u6548\u5b9e\u73b0\uff0c\u4e14\u4f18\u5148\u4f18\u5316\u6a21\u578b\u662f\u66f4\u4f18\u7b56\u7565\u3002"}}
{"id": "2505.03254", "pdf": "https://arxiv.org/pdf/2505.03254", "abs": "https://arxiv.org/abs/2505.03254", "authors": ["Lukas Meiner", "Jens Mehnert", "Alexandru Paul Condurache"], "title": "PROM: Prioritize Reduction of Multiplications Over Lower Bit-Widths for Efficient CNNs", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Convolutional neural networks (CNNs) are crucial for computer vision tasks on\nresource-constrained devices. Quantization effectively compresses these models,\nreducing storage size and energy cost. However, in modern depthwise-separable\narchitectures, the computational cost is distributed unevenly across its\ncomponents, with pointwise operations being the most expensive. By applying a\ngeneral quantization scheme to this imbalanced cost distribution, existing\nquantization approaches fail to fully exploit potential efficiency gains. To\nthis end, we introduce PROM, a straightforward approach for quantizing modern\ndepthwise-separable convolutional networks by selectively using two distinct\nbit-widths. Specifically, pointwise convolutions are quantized to ternary\nweights, while the remaining modules use 8-bit weights, which is achieved\nthrough a simple quantization-aware training procedure. Additionally, by\nquantizing activations to 8-bit, our method transforms pointwise convolutions\nwith ternary weights into int8 additions, which enjoy broad support across\nhardware platforms and effectively eliminates the need for expensive\nmultiplications. Applying PROM to MobileNetV2 reduces the model's energy cost\nby more than an order of magnitude (23.9x) and its storage size by 2.7x\ncompared to the float16 baseline while retaining similar classification\nperformance on ImageNet. Our method advances the Pareto frontier for energy\nconsumption vs. top-1 accuracy for quantized convolutional models on ImageNet.\nPROM addresses the challenges of quantizing depthwise-separable convolutional\nnetworks to both ternary and 8-bit weights, offering a simple way to reduce\nenergy cost and storage size.", "AI": {"tldr": "PROM\u662f\u4e00\u79cd\u9488\u5bf9\u6df1\u5ea6\u53ef\u5206\u79bb\u5377\u79ef\u7f51\u7edc\u7684\u91cf\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u4f7f\u7528\u4e09\u5143\u548c8\u4f4d\u6743\u91cd\uff0c\u663e\u8457\u964d\u4f4e\u80fd\u8017\u548c\u5b58\u50a8\u9700\u6c42\u3002", "motivation": "\u73b0\u4ee3\u6df1\u5ea6\u53ef\u5206\u79bb\u5377\u79ef\u7f51\u7edc\u4e2d\uff0c\u70b9\u5377\u79ef\u64cd\u4f5c\u7684\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u5206\u5e03\u4e0d\u5747\uff0c\u73b0\u6709\u91cf\u5316\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528\u6548\u7387\u6f5c\u529b\u3002", "method": "PROM\u91c7\u7528\u4e24\u79cd\u4f4d\u5bbd\u91cf\u5316\uff1a\u70b9\u5377\u79ef\u4f7f\u7528\u4e09\u5143\u6743\u91cd\uff0c\u5176\u4f59\u6a21\u5757\u4f7f\u75288\u4f4d\u6743\u91cd\uff0c\u5e76\u901a\u8fc7\u91cf\u5316\u611f\u77e5\u8bad\u7ec3\u5b9e\u73b0\u3002\u6fc0\u6d3b\u91cf\u5316\u4e3a8\u4f4d\uff0c\u5c06\u70b9\u5377\u79ef\u8f6c\u6362\u4e3aint8\u52a0\u6cd5\u3002", "result": "\u5728MobileNetV2\u4e0a\uff0cPROM\u5c06\u80fd\u8017\u964d\u4f4e23.9\u500d\uff0c\u5b58\u50a8\u9700\u6c42\u51cf\u5c112.7\u500d\uff0c\u540c\u65f6\u4fdd\u6301ImageNet\u5206\u7c7b\u6027\u80fd\u3002", "conclusion": "PROM\u901a\u8fc7\u7b80\u5355\u65b9\u6cd5\u89e3\u51b3\u4e86\u6df1\u5ea6\u53ef\u5206\u79bb\u7f51\u7edc\u7684\u91cf\u5316\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86\u80fd\u6548\u548c\u5b58\u50a8\u6548\u7387\u3002"}}
{"id": "2505.03467", "pdf": "https://arxiv.org/pdf/2505.03467", "abs": "https://arxiv.org/abs/2505.03467", "authors": ["Shuang Zhou", "Jiashuo Wang", "Zidu Xu", "Song Wang", "David Brauer", "Lindsay Welton", "Jacob Cogan", "Yuen-Hei Chung", "Lei Tian", "Zaifu Zhan", "Yu Hou", "Mingquan Lin", "Genevieve B. Melton", "Rui Zhang"], "title": "Uncertainty-Aware Large Language Models for Explainable Disease Diagnosis", "categories": ["cs.CL"], "comment": "22 pages, 8 figures", "summary": "Explainable disease diagnosis, which leverages patient information (e.g.,\nsigns and symptoms) and computational models to generate probable diagnoses and\nreasonings, offers clear clinical values. However, when clinical notes\nencompass insufficient evidence for a definite diagnosis, such as the absence\nof definitive symptoms, diagnostic uncertainty usually arises, increasing the\nrisk of misdiagnosis and adverse outcomes. Although explicitly identifying and\nexplaining diagnostic uncertainties is essential for trustworthy diagnostic\nsystems, it remains under-explored. To fill this gap, we introduce ConfiDx, an\nuncertainty-aware large language model (LLM) created by fine-tuning open-source\nLLMs with diagnostic criteria. We formalized the task and assembled richly\nannotated datasets that capture varying degrees of diagnostic ambiguity.\nEvaluating ConfiDx on real-world datasets demonstrated that it excelled in\nidentifying diagnostic uncertainties, achieving superior diagnostic\nperformance, and generating trustworthy explanations for diagnoses and\nuncertainties. To our knowledge, this is the first study to jointly address\ndiagnostic uncertainty recognition and explanation, substantially enhancing the\nreliability of automatic diagnostic systems.", "AI": {"tldr": "ConfiDx\u662f\u4e00\u4e2a\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\uff0c\u901a\u8fc7\u5fae\u8c03\u5f00\u6e90LLM\u5e76\u7ed3\u5408\u8bca\u65ad\u6807\u51c6\uff0c\u89e3\u51b3\u4e86\u8bca\u65ad\u4e0d\u786e\u5b9a\u6027\u8bc6\u522b\u548c\u89e3\u91ca\u7684\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u81ea\u52a8\u8bca\u65ad\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u3002", "motivation": "\u4e34\u5e8a\u7b14\u8bb0\u4e2d\u8bc1\u636e\u4e0d\u8db3\u65f6\uff0c\u8bca\u65ad\u4e0d\u786e\u5b9a\u6027\u4f1a\u589e\u52a0\u8bef\u8bca\u98ce\u9669\uff0c\u4f46\u76ee\u524d\u5bf9\u8bca\u65ad\u4e0d\u786e\u5b9a\u6027\u7684\u8bc6\u522b\u548c\u89e3\u91ca\u7814\u7a76\u4e0d\u8db3\u3002", "method": "\u901a\u8fc7\u5fae\u8c03\u5f00\u6e90LLM\u5e76\u6784\u5efa\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u5f00\u53d1\u4e86ConfiDx\u6a21\u578b\uff0c\u7528\u4e8e\u8bc6\u522b\u548c\u89e3\u91ca\u8bca\u65ad\u4e0d\u786e\u5b9a\u6027\u3002", "result": "ConfiDx\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u8bc6\u522b\u8bca\u65ad\u4e0d\u786e\u5b9a\u6027\u5e76\u63d0\u4f9b\u53ef\u4fe1\u7684\u89e3\u91ca\uff0c\u8bca\u65ad\u6027\u80fd\u4f18\u8d8a\u3002", "conclusion": "ConfiDx\u9996\u6b21\u8054\u5408\u89e3\u51b3\u8bca\u65ad\u4e0d\u786e\u5b9a\u6027\u7684\u8bc6\u522b\u548c\u89e3\u91ca\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u8bca\u65ad\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2505.03261", "pdf": "https://arxiv.org/pdf/2505.03261", "abs": "https://arxiv.org/abs/2505.03261", "authors": ["Wei-Ting Chen", "Yu-Jiet Vong", "Yi-Tsung Lee", "Sy-Yen Kuo", "Qiang Gao", "Sizhuo Ma", "Jian Wang"], "title": "DiffVQA: Video Quality Assessment Using Diffusion Feature Extractor", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Video Quality Assessment (VQA) aims to evaluate video quality based on\nperceptual distortions and human preferences. Despite the promising performance\nof existing methods using Convolutional Neural Networks (CNNs) and Vision\nTransformers (ViTs), they often struggle to align closely with human\nperceptions, particularly in diverse real-world scenarios. This challenge is\nexacerbated by the limited scale and diversity of available datasets. To\naddress this limitation, we introduce a novel VQA framework, DiffVQA, which\nharnesses the robust generalization capabilities of diffusion models\npre-trained on extensive datasets. Our framework adapts these models to\nreconstruct identical input frames through a control module. The adapted\ndiffusion model is then used to extract semantic and distortion features from a\nresizing branch and a cropping branch, respectively. To enhance the model's\nability to handle long-term temporal dynamics, a parallel Mamba module is\nintroduced, which extracts temporal coherence augmented features that are\nmerged with the diffusion features to predict the final score. Experiments\nacross multiple datasets demonstrate DiffVQA's superior performance on\nintra-dataset evaluations and its exceptional generalization across datasets.\nThese results confirm that leveraging a diffusion model as a feature extractor\ncan offer enhanced VQA performance compared to CNN and ViT backbones.", "AI": {"tldr": "DiffVQA\u662f\u4e00\u79cd\u65b0\u7684\u89c6\u9891\u8d28\u91cf\u8bc4\u4f30\u6846\u67b6\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u7684\u6269\u6563\u6a21\u578b\u63d0\u53d6\u7279\u5f81\uff0c\u7ed3\u5408Mamba\u6a21\u5757\u5904\u7406\u65f6\u95f4\u52a8\u6001\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bc4\u4f30\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eCNN\u548cViT\u7684\u89c6\u9891\u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\u96be\u4ee5\u4e0e\u4eba\u7c7b\u611f\u77e5\u5bf9\u9f50\uff0c\u4e14\u53d7\u9650\u4e8e\u6570\u636e\u96c6\u7684\u89c4\u6a21\u548c\u591a\u6837\u6027\u3002", "method": "DiffVQA\u901a\u8fc7\u63a7\u5236\u6a21\u5757\u9002\u5e94\u6269\u6563\u6a21\u578b\uff0c\u63d0\u53d6\u8bed\u4e49\u548c\u5931\u771f\u7279\u5f81\uff0c\u5e76\u5f15\u5165Mamba\u6a21\u5757\u5904\u7406\u65f6\u95f4\u52a8\u6001\uff0c\u5408\u5e76\u7279\u5f81\u9884\u6d4b\u5206\u6570\u3002", "result": "\u5b9e\u9a8c\u8868\u660eDiffVQA\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u6cdb\u5316\u80fd\u529b\u5f3a\u3002", "conclusion": "\u6269\u6563\u6a21\u578b\u4f5c\u4e3a\u7279\u5f81\u63d0\u53d6\u5668\u6bd4CNN\u548cViT\u66f4\u6709\u6548\uff0c\u63d0\u5347\u4e86\u89c6\u9891\u8d28\u91cf\u8bc4\u4f30\u6027\u80fd\u3002"}}
{"id": "2505.03469", "pdf": "https://arxiv.org/pdf/2505.03469", "abs": "https://arxiv.org/abs/2505.03469", "authors": ["Bin Yu", "Hang Yuan", "Yuliang Wei", "Bailing Wang", "Weizhen Qi", "Kai Chen"], "title": "Long-Short Chain-of-Thought Mixture Supervised Fine-Tuning Eliciting Efficient Reasoning in Large Language Models", "categories": ["cs.CL"], "comment": "11 pages, 2 figures", "summary": "Recent advances in large language models have demonstrated that Supervised\nFine-Tuning (SFT) with Chain-of-Thought (CoT) reasoning data distilled from\nlarge reasoning models (e.g., DeepSeek R1) can effectively transfer reasoning\ncapabilities to non-reasoning models. However, models fine-tuned with this\napproach inherit the \"overthinking\" problem from teacher models, producing\nverbose and redundant reasoning chains during inference. To address this\nchallenge, we propose \\textbf{L}ong-\\textbf{S}hort Chain-of-Thought\n\\textbf{Mixture} \\textbf{S}upervised \\textbf{F}ine-\\textbf{T}uning\n(\\textbf{LS-Mixture SFT}), which combines long CoT reasoning dataset with their\nshort counterparts obtained through structure-preserved rewriting. Our\nexperiments demonstrate that models trained using the LS-Mixture SFT method,\ncompared to those trained with direct SFT, achieved an average accuracy\nimprovement of 2.3\\% across various benchmarks while substantially reducing\nmodel response length by approximately 47.61\\%. This work offers an approach to\nendow non-reasoning models with reasoning capabilities through supervised\nfine-tuning while avoiding the inherent overthinking problems inherited from\nteacher models, thereby enabling efficient reasoning in the fine-tuned models.", "AI": {"tldr": "LS-Mixture SFT\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408\u957f\u94fe\u548c\u77ed\u94fe\u63a8\u7406\u6570\u636e\uff0c\u89e3\u51b3\u4e86\u76d1\u7763\u5fae\u8c03\u4e2d\u7684\u201c\u8fc7\u5ea6\u601d\u8003\u201d\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u51c6\u786e\u7387\u5e76\u51cf\u5c11\u4e86\u54cd\u5e94\u957f\u5ea6\u3002", "motivation": "\u73b0\u6709\u76d1\u7763\u5fae\u8c03\u65b9\u6cd5\uff08SFT\uff09\u4ece\u5927\u578b\u63a8\u7406\u6a21\u578b\u84b8\u998f\u7684CoT\u6570\u636e\u4e2d\u7ee7\u627f\u4e86\u201c\u8fc7\u5ea6\u601d\u8003\u201d\u95ee\u9898\uff0c\u5bfc\u81f4\u63a8\u7406\u94fe\u5197\u957f\u5197\u4f59\u3002", "method": "\u63d0\u51faLS-Mixture SFT\uff0c\u7ed3\u5408\u957f\u94feCoT\u6570\u636e\u53ca\u5176\u901a\u8fc7\u7ed3\u6784\u4fdd\u7559\u91cd\u5199\u5f97\u5230\u7684\u77ed\u94fe\u6570\u636e\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cLS-Mixture SFT\u5e73\u5747\u51c6\u786e\u7387\u63d0\u53472.3%\uff0c\u54cd\u5e94\u957f\u5ea6\u51cf\u5c11\u7ea647.61%\u3002", "conclusion": "LS-Mixture SFT\u6709\u6548\u8d4b\u4e88\u975e\u63a8\u7406\u6a21\u578b\u63a8\u7406\u80fd\u529b\uff0c\u540c\u65f6\u907f\u514d\u201c\u8fc7\u5ea6\u601d\u8003\u201d\u95ee\u9898\uff0c\u5b9e\u73b0\u9ad8\u6548\u63a8\u7406\u3002"}}
{"id": "2505.03284", "pdf": "https://arxiv.org/pdf/2505.03284", "abs": "https://arxiv.org/abs/2505.03284", "authors": ["Zhenxing Ming", "Julie Stephany Berrio", "Mao Shan", "Yaoqi Huang", "Hongyu Lyu", "Nguyen Hoang Khoi Tran", "Tzu-Yun Tseng", "Stewart Worrall"], "title": "OccCylindrical: Multi-Modal Fusion with Cylindrical Representation for 3D Semantic Occupancy Prediction", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "The safe operation of autonomous vehicles (AVs) is highly dependent on their\nunderstanding of the surroundings. For this, the task of 3D semantic occupancy\nprediction divides the space around the sensors into voxels, and labels each\nvoxel with both occupancy and semantic information. Recent perception models\nhave used multisensor fusion to perform this task. However, existing\nmultisensor fusion-based approaches focus mainly on using sensor information in\nthe Cartesian coordinate system. This ignores the distribution of the sensor\nreadings, leading to a loss of fine-grained details and performance\ndegradation. In this paper, we propose OccCylindrical that merges and refines\nthe different modality features under cylindrical coordinates. Our method\npreserves more fine-grained geometry detail that leads to better performance.\nExtensive experiments conducted on the nuScenes dataset, including challenging\nrainy and nighttime scenarios, confirm our approach's effectiveness and\nstate-of-the-art performance. The code will be available at:\nhttps://github.com/DanielMing123/OccCylindrical", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aOccCylindrical\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u5706\u67f1\u5750\u6807\u7cfb\u4e0b\u878d\u5408\u548c\u4f18\u5316\u591a\u6a21\u6001\u7279\u5f81\uff0c\u63d0\u5347\u4e863D\u8bed\u4e49\u5360\u7528\u9884\u6d4b\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u591a\u4f20\u611f\u5668\u878d\u5408\u7684\u65b9\u6cd5\u4e3b\u8981\u4f7f\u7528\u7b1b\u5361\u5c14\u5750\u6807\u7cfb\uff0c\u5ffd\u7565\u4e86\u4f20\u611f\u5668\u8bfb\u6570\u7684\u5206\u5e03\uff0c\u5bfc\u81f4\u7ec6\u8282\u4e22\u5931\u548c\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u63d0\u51faOccCylindrical\u65b9\u6cd5\uff0c\u5728\u5706\u67f1\u5750\u6807\u7cfb\u4e0b\u878d\u5408\u548c\u4f18\u5316\u591a\u6a21\u6001\u7279\u5f81\uff0c\u4fdd\u7559\u66f4\u591a\u51e0\u4f55\u7ec6\u8282\u3002", "result": "\u5728nuScenes\u6570\u636e\u96c6\uff08\u5305\u62ec\u96e8\u5929\u548c\u591c\u95f4\u573a\u666f\uff09\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "OccCylindrical\u65b9\u6cd5\u901a\u8fc7\u5706\u67f1\u5750\u6807\u7cfb\u4f18\u5316\u7279\u5f81\u878d\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e863D\u8bed\u4e49\u5360\u7528\u9884\u6d4b\u7684\u7cbe\u5ea6\u3002"}}
{"id": "2505.03473", "pdf": "https://arxiv.org/pdf/2505.03473", "abs": "https://arxiv.org/abs/2505.03473", "authors": ["Marta Boscariol", "Luana Bulla", "Lia Draetta", "Beatrice Fiuman\u00f2", "Emanuele Lenzi", "Leonardo Piano"], "title": "Evaluation of LLMs on Long-tail Entity Linking in Historical Documents", "categories": ["cs.CL"], "comment": null, "summary": "Entity Linking (EL) plays a crucial role in Natural Language Processing (NLP)\napplications, enabling the disambiguation of entity mentions by linking them to\ntheir corresponding entries in a reference knowledge base (KB). Thanks to their\ndeep contextual understanding capabilities, LLMs offer a new perspective to\ntackle EL, promising better results than traditional methods. Despite the\nimpressive generalization capabilities of LLMs, linking less popular, long-tail\nentities remains challenging as these entities are often underrepresented in\ntraining data and knowledge bases. Furthermore, the long-tail EL task is an\nunderstudied problem, and limited studies address it with LLMs. In the present\nwork, we assess the performance of two popular LLMs, GPT and LLama3, in a\nlong-tail entity linking scenario. Using MHERCL v0.1, a manually annotated\nbenchmark of sentences from domain-specific historical texts, we quantitatively\ncompare the performance of LLMs in identifying and linking entities to their\ncorresponding Wikidata entries against that of ReLiK, a state-of-the-art Entity\nLinking and Relation Extraction framework. Our preliminary experiments reveal\nthat LLMs perform encouragingly well in long-tail EL, indicating that this\ntechnology can be a valuable adjunct in filling the gap between head and\nlong-tail EL.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86LLMs\u5728\u957f\u5c3e\u5b9e\u4f53\u94fe\u63a5\uff08EL\uff09\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5176\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u4f46\u4ecd\u9700\u6539\u8fdb\u3002", "motivation": "\u957f\u5c3e\u5b9e\u4f53\u5728\u8bad\u7ec3\u6570\u636e\u548c\u77e5\u8bc6\u5e93\u4e2d\u4ee3\u8868\u6027\u4e0d\u8db3\uff0cLLMs\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002", "method": "\u4f7f\u7528GPT\u548cLLama3\u4e24\u79cdLLMs\uff0c\u5728MHERCL v0.1\u6570\u636e\u96c6\u4e0a\u4e0eReLiK\u6846\u67b6\u8fdb\u884c\u6027\u80fd\u5bf9\u6bd4\u3002", "result": "LLMs\u5728\u957f\u5c3eEL\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u663e\u793a\u51fa\u6f5c\u529b\u3002", "conclusion": "LLMs\u53ef\u4f5c\u4e3a\u8865\u5145\u6280\u672f\uff0c\u5f25\u5408\u4e3b\u6d41\u4e0e\u957f\u5c3e\u5b9e\u4f53\u94fe\u63a5\u7684\u5dee\u8ddd\u3002"}}
{"id": "2505.03286", "pdf": "https://arxiv.org/pdf/2505.03286", "abs": "https://arxiv.org/abs/2505.03286", "authors": ["Zhihao Gong", "Lian Wu", "Yong Xu"], "title": "Base-Detail Feature Learning Framework for Visible-Infrared Person Re-Identification", "categories": ["cs.CV"], "comment": "9 pages, 5 figures, 2025 34th International Joint Conference on\n  Artificial Intelligence (IJCAI 2025)", "summary": "Visible-infrared person re-identification (VIReID) provides a solution for\nReID tasks in 24-hour scenarios; however, significant challenges persist in\nachieving satisfactory performance due to the substantial discrepancies between\nvisible (VIS) and infrared (IR) modalities. Existing methods inadequately\nleverage information from different modalities, primarily focusing on digging\ndistinguishing features from modality-shared information while neglecting\nmodality-specific details. To fully utilize differentiated minutiae, we propose\na Base-Detail Feature Learning Framework (BDLF) that enhances the learning of\nboth base and detail knowledge, thereby capitalizing on both modality-shared\nand modality-specific information. Specifically, the proposed BDLF mines detail\nand base features through a lossless detail feature extraction module and a\ncomplementary base embedding generation mechanism, respectively, supported by a\nnovel correlation restriction method that ensures the features gained by BDLF\nenrich both detail and base knowledge across VIS and IR features. Comprehensive\nexperiments conducted on the SYSU-MM01, RegDB, and LLCM datasets validate the\neffectiveness of BDLF.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cdBase-Detail Feature Learning Framework (BDLF)\uff0c\u901a\u8fc7\u540c\u65f6\u5229\u7528\u6a21\u6001\u5171\u4eab\u548c\u6a21\u6001\u7279\u5b9a\u7684\u4fe1\u606f\uff0c\u63d0\u5347\u53ef\u89c1\u5149-\u7ea2\u5916\u884c\u4eba\u91cd\u8bc6\u522b\uff08VIReID\uff09\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528\u4e0d\u540c\u6a21\u6001\u7684\u4fe1\u606f\uff0c\u4e3b\u8981\u5173\u6ce8\u6a21\u6001\u5171\u4eab\u7279\u5f81\u800c\u5ffd\u7565\u6a21\u6001\u7279\u5b9a\u7ec6\u8282\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0d\u4f73\u3002", "method": "BDLF\u901a\u8fc7\u65e0\u635f\u7ec6\u8282\u7279\u5f81\u63d0\u53d6\u6a21\u5757\u548c\u4e92\u8865\u57fa\u7840\u5d4c\u5165\u751f\u6210\u673a\u5236\uff0c\u5206\u522b\u6316\u6398\u7ec6\u8282\u548c\u57fa\u7840\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u76f8\u5173\u6027\u9650\u5236\u65b9\u6cd5\u786e\u4fdd\u7279\u5f81\u4e30\u5bcc\u3002", "result": "\u5728SYSU-MM01\u3001RegDB\u548cLLCM\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86BDLF\u7684\u6709\u6548\u6027\u3002", "conclusion": "BDLF\u901a\u8fc7\u540c\u65f6\u5b66\u4e60\u57fa\u7840\u548c\u7ec6\u8282\u7279\u5f81\uff0c\u663e\u8457\u63d0\u5347\u4e86VIReID\u4efb\u52a1\u7684\u6027\u80fd\u3002"}}
{"id": "2505.03481", "pdf": "https://arxiv.org/pdf/2505.03481", "abs": "https://arxiv.org/abs/2505.03481", "authors": ["Maciej Zembrzuski", "Saad Mahamood"], "title": "Sentence Embeddings as an intermediate target in end-to-end summarisation", "categories": ["cs.CL"], "comment": "10 pages, 1 figure, Year: 2019", "summary": "Current neural network-based methods to the problem of document summarisation\nstruggle when applied to datasets containing large inputs. In this paper we\npropose a new approach to the challenge of content-selection when dealing with\nend-to-end summarisation of user reviews of accommodations. We show that by\ncombining an extractive approach with externally pre-trained sentence level\nembeddings in an addition to an abstractive summarisation model we can\noutperform existing methods when this is applied to the task of summarising a\nlarge input dataset. We also prove that predicting sentence level embedding of\na summary increases the quality of an end-to-end system for loosely aligned\nsource to target corpora, than compared to commonly predicting probability\ndistributions of sentence selection.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u62bd\u53d6\u5f0f\u548c\u751f\u6210\u5f0f\u65b9\u6cd5\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u5904\u7406\u5927\u89c4\u6a21\u7528\u6237\u8bc4\u8bba\u7684\u6458\u8981\u4efb\u52a1\uff0c\u5e76\u901a\u8fc7\u9884\u8bad\u7ec3\u53e5\u5b50\u5d4c\u5165\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\u5728\u5904\u7406\u5927\u89c4\u6a21\u8f93\u5165\u6570\u636e\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u6539\u8fdb\u5185\u5bb9\u9009\u62e9\u7b56\u7565\u3002", "method": "\u7ed3\u5408\u62bd\u53d6\u5f0f\u65b9\u6cd5\u548c\u9884\u8bad\u7ec3\u53e5\u5b50\u5d4c\u5165\uff0c\u518d\u4e0e\u751f\u6210\u5f0f\u6a21\u578b\u7ed3\u5408\uff0c\u9884\u6d4b\u53e5\u5b50\u7ea7\u5d4c\u5165\u800c\u975e\u4f20\u7edf\u6982\u7387\u5206\u5e03\u3002", "result": "\u65b0\u65b9\u6cd5\u5728\u5927\u89c4\u6a21\u8f93\u5165\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u9884\u6d4b\u53e5\u5b50\u5d4c\u5165\u63d0\u5347\u4e86\u7aef\u5230\u7aef\u7cfb\u7edf\u7684\u8d28\u91cf\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u62bd\u53d6\u5f0f\u4e0e\u751f\u6210\u5f0f\u65b9\u6cd5\u5e76\u5229\u7528\u53e5\u5b50\u5d4c\u5165\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5927\u89c4\u6a21\u7528\u6237\u8bc4\u8bba\u6458\u8981\u7684\u6027\u80fd\u3002"}}
{"id": "2505.03299", "pdf": "https://arxiv.org/pdf/2505.03299", "abs": "https://arxiv.org/abs/2505.03299", "authors": ["Pierre Adorni", "Minh-Tan Pham", "St\u00e9phane May", "S\u00e9bastien Lef\u00e8vre"], "title": "Towards Efficient Benchmarking of Foundation Models in Remote Sensing: A Capabilities Encoding Approach", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted at the MORSE workshop of CVPR 2025", "summary": "Foundation models constitute a significant advancement in computer vision:\nafter a single, albeit costly, training phase, they can address a wide array of\ntasks. In the field of Earth observation, over 75 remote sensing vision\nfoundation models have been developed in the past four years. However, none has\nconsistently outperformed the others across all available downstream tasks. To\nfacilitate their comparison, we propose a cost-effective method for predicting\na model's performance on multiple downstream tasks without the need for\nfine-tuning on each one. This method is based on what we call \"capabilities\nencoding.\" The utility of this novel approach is twofold: we demonstrate its\npotential to simplify the selection of a foundation model for a given new task,\nand we employ it to offer a fresh perspective on the existing literature,\nsuggesting avenues for future research. Codes are available at\nhttps://github.com/pierreadorni/capabilities-encoding.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u201c\u80fd\u529b\u7f16\u7801\u201d\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u9884\u6d4b\u57fa\u7840\u6a21\u578b\u5728\u591a\u4e2a\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u65e0\u9700\u5fae\u8c03\uff0c\u7b80\u5316\u6a21\u578b\u9009\u62e9\u5e76\u63d0\u4f9b\u7814\u7a76\u65b0\u89c6\u89d2\u3002", "motivation": "\u5c3d\u7ba1\u5df2\u670975\u591a\u4e2a\u9065\u611f\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff0c\u4f46\u5c1a\u65e0\u6a21\u578b\u5728\u6240\u6709\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8868\u73b0\u4e00\u81f4\u6700\u4f18\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u6bd4\u8f83\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u201c\u80fd\u529b\u7f16\u7801\u201d\u9884\u6d4b\u6a21\u578b\u8868\u73b0\uff0c\u907f\u514d\u5bf9\u6bcf\u4e2a\u4efb\u52a1\u8fdb\u884c\u5fae\u8c03\uff0c\u6210\u672c\u66f4\u4f4e\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u7b80\u5316\u57fa\u7840\u6a21\u578b\u9009\u62e9\uff0c\u5e76\u4e3a\u73b0\u6709\u6587\u732e\u63d0\u4f9b\u65b0\u89c6\u89d2\u3002", "conclusion": "\u80fd\u529b\u7f16\u7801\u65b9\u6cd5\u4e3a\u6a21\u578b\u6bd4\u8f83\u548c\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2505.03531", "pdf": "https://arxiv.org/pdf/2505.03531", "abs": "https://arxiv.org/abs/2505.03531", "authors": ["Haoqi Yang", "Luohe Shi", "Qiwei Li", "Zuchao Li", "Ping Wang", "Bo Du", "Mengjia Shen", "Hai Zhao"], "title": "Faster MoE LLM Inference for Extremely Large Models", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Sparse Mixture of Experts (MoE) large language models (LLMs) are gradually\nbecoming the mainstream approach for ultra-large-scale models. Existing\noptimization efforts for MoE models have focused primarily on coarse-grained\nMoE architectures. With the emergence of DeepSeek Models, fine-grained MoE\nmodels are gaining popularity, yet research on them remains limited. Therefore,\nwe want to discuss the efficiency dynamic under different service loads.\nAdditionally, fine-grained models allow deployers to reduce the number of\nrouted experts, both activated counts and total counts, raising the question of\nhow this reduction affects the trade-off between MoE efficiency and\nperformance. Our findings indicate that while deploying MoE models presents\ngreater challenges, it also offers significant optimization opportunities.\nReducing the number of activated experts can lead to substantial efficiency\nimprovements in certain scenarios, with only minor performance degradation.\nReducing the total number of experts provides limited efficiency gains but\nresults in severe performance degradation. Our method can increase throughput\nby at least 10\\% without any performance degradation. Overall, we conclude that\nMoE inference optimization remains an area with substantial potential for\nexploration and improvement.", "AI": {"tldr": "\u7a00\u758f\u4e13\u5bb6\u6df7\u5408\uff08MoE\uff09\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6b63\u6210\u4e3a\u8d85\u5927\u89c4\u6a21\u6a21\u578b\u7684\u4e3b\u6d41\u65b9\u6cd5\u3002\u672c\u6587\u63a2\u8ba8\u4e86\u7ec6\u7c92\u5ea6MoE\u6a21\u578b\u5728\u4e0d\u540c\u670d\u52a1\u8d1f\u8f7d\u4e0b\u7684\u6548\u7387\u52a8\u6001\uff0c\u5e76\u7814\u7a76\u4e86\u51cf\u5c11\u8def\u7531\u4e13\u5bb6\u6570\u91cf\u5bf9\u6548\u7387\u4e0e\u6027\u80fd\u6743\u8861\u7684\u5f71\u54cd\u3002", "motivation": "\u968f\u7740DeepSeek\u7b49\u7ec6\u7c92\u5ea6MoE\u6a21\u578b\u7684\u5174\u8d77\uff0c\u76f8\u5173\u7814\u7a76\u4ecd\u6709\u9650\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63a2\u8ba8\u5176\u4f18\u5316\u6f5c\u529b\u3002", "method": "\u901a\u8fc7\u5206\u6790\u51cf\u5c11\u6fc0\u6d3b\u4e13\u5bb6\u548c\u603b\u4e13\u5bb6\u6570\u91cf\u5bf9\u6548\u7387\u548c\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u4f18\u5316\u65b9\u6cd5\u3002", "result": "\u51cf\u5c11\u6fc0\u6d3b\u4e13\u5bb6\u6570\u91cf\u53ef\u663e\u8457\u63d0\u5347\u6548\u7387\u4e14\u6027\u80fd\u635f\u5931\u5c0f\uff1b\u51cf\u5c11\u603b\u4e13\u5bb6\u6570\u91cf\u6548\u7387\u63d0\u5347\u6709\u9650\u4f46\u6027\u80fd\u635f\u5931\u4e25\u91cd\u3002\u65b9\u6cd5\u53ef\u5b9e\u73b0\u81f3\u5c1110%\u7684\u541e\u5410\u91cf\u63d0\u5347\u4e14\u65e0\u6027\u80fd\u635f\u5931\u3002", "conclusion": "MoE\u63a8\u7406\u4f18\u5316\u4ecd\u6709\u5de8\u5927\u63a2\u7d22\u548c\u6539\u8fdb\u7a7a\u95f4\u3002"}}
{"id": "2505.03300", "pdf": "https://arxiv.org/pdf/2505.03300", "abs": "https://arxiv.org/abs/2505.03300", "authors": ["Andrew Caunes", "Thierry Chateau", "Vincent Fr\u00e9mont"], "title": "3D Can Be Explored In 2D: Pseudo-Label Generation for LiDAR Point Clouds Using Sensor-Intensity-Based 2D Semantic Segmentation", "categories": ["cs.CV"], "comment": "Accepted to IV2024", "summary": "Semantic segmentation of 3D LiDAR point clouds, essential for autonomous\ndriving and infrastructure management, is best achieved by supervised learning,\nwhich demands extensive annotated datasets and faces the problem of domain\nshifts. We introduce a new 3D semantic segmentation pipeline that leverages\naligned scenes and state-of-the-art 2D segmentation methods, avoiding the need\nfor direct 3D annotation or reliance on additional modalities such as camera\nimages at inference time. Our approach generates 2D views from LiDAR scans\ncolored by sensor intensity and applies 2D semantic segmentation to these views\nusing a camera-domain pretrained model. The segmented 2D outputs are then\nback-projected onto the 3D points, with a simple voting-based estimator that\nmerges the labels associated to each 3D point. Our main contribution is a\nglobal pipeline for 3D semantic segmentation requiring no prior 3D annotation\nand not other modality for inference, which can be used for pseudo-label\ngeneration. We conduct a thorough ablation study and demonstrate the potential\nof the generated pseudo-labels for the Unsupervised Domain Adaptation task.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u97003D\u6807\u6ce8\u76843D\u8bed\u4e49\u5206\u5272\u65b9\u6cd5\uff0c\u901a\u8fc72D\u5206\u5272\u6a21\u578b\u548c\u6295\u7968\u673a\u5236\u5b9e\u73b0\u3002", "motivation": "\u89e3\u51b33D\u70b9\u4e91\u8bed\u4e49\u5206\u5272\u4e2d\u6807\u6ce8\u6210\u672c\u9ad8\u548c\u9886\u57df\u504f\u79fb\u95ee\u9898\u3002", "method": "\u5229\u7528LiDAR\u626b\u63cf\u751f\u62102D\u89c6\u56fe\uff0c\u5e94\u75282D\u5206\u5272\u6a21\u578b\uff0c\u518d\u901a\u8fc7\u6295\u7968\u673a\u5236\u5c06\u7ed3\u679c\u6620\u5c04\u56de3D\u70b9\u4e91\u3002", "result": "\u65b9\u6cd5\u5728\u65e03D\u6807\u6ce8\u7684\u60c5\u51b5\u4e0b\u6709\u6548\uff0c\u53ef\u7528\u4e8e\u4f2a\u6807\u7b7e\u751f\u6210\uff0c\u652f\u6301\u65e0\u76d1\u7763\u9886\u57df\u9002\u5e94\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a3D\u8bed\u4e49\u5206\u5272\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u4f4e\u6210\u672c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.03563", "pdf": "https://arxiv.org/pdf/2505.03563", "abs": "https://arxiv.org/abs/2505.03563", "authors": ["Cl\u00e9a Chataigner", "Rebecca Ma", "Prakhar Ganesh", "Afaf Ta\u00efk", "Elliot Creager", "Golnoosh Farnadi"], "title": "Say It Another Way: A Framework for User-Grounded Paraphrasing", "categories": ["cs.CL"], "comment": null, "summary": "Small changes in how a prompt is worded can lead to meaningful differences in\nthe behavior of large language models (LLMs), raising concerns about the\nstability and reliability of their evaluations. While prior work has explored\nsimple formatting changes, these rarely capture the kinds of natural variation\nseen in real-world language use. We propose a controlled paraphrasing framework\nbased on a taxonomy of minimal linguistic transformations to systematically\ngenerate natural prompt variations. Using the BBQ dataset, we validate our\nmethod with both human annotations and automated checks, then use it to study\nhow LLMs respond to paraphrased prompts in stereotype evaluation tasks. Our\nanalysis shows that even subtle prompt modifications can lead to substantial\nchanges in model behavior. These results highlight the need for robust,\nparaphrase-aware evaluation protocols.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u63d0\u793a\u8bcd\u5fae\u5c0f\u53d8\u5316\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u884c\u4e3a\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u8bed\u8a00\u8f6c\u6362\u7684\u6846\u67b6\u6765\u751f\u6210\u81ea\u7136\u63d0\u793a\u53d8\u4f53\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u5bf9\u6a21\u578b\u8bc4\u4f30\u7684\u5f71\u54cd\u3002", "motivation": "\u7814\u7a76\u63d0\u793a\u8bcd\u5fae\u5c0f\u53d8\u5316\u5bf9LLM\u884c\u4e3a\u7684\u5f71\u54cd\uff0c\u4ee5\u89e3\u51b3\u8bc4\u4f30\u7684\u7a33\u5b9a\u6027\u548c\u53ef\u9760\u6027\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u8bed\u8a00\u8f6c\u6362\u5206\u7c7b\u7684\u53d7\u63a7\u8f6c\u8ff0\u6846\u67b6\uff0c\u751f\u6210\u81ea\u7136\u63d0\u793a\u53d8\u4f53\uff0c\u5e76\u5728BBQ\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u3002", "result": "\u5373\u4f7f\u7ec6\u5fae\u7684\u63d0\u793a\u4fee\u6539\u4e5f\u4f1a\u5bfc\u81f4\u6a21\u578b\u884c\u4e3a\u7684\u663e\u8457\u53d8\u5316\u3002", "conclusion": "\u9700\u8981\u5f00\u53d1\u5bf9\u8f6c\u8ff0\u654f\u611f\u7684\u7a33\u5065\u8bc4\u4f30\u534f\u8bae\u3002"}}
{"id": "2505.03303", "pdf": "https://arxiv.org/pdf/2505.03303", "abs": "https://arxiv.org/abs/2505.03303", "authors": ["Tasnim Shahriar"], "title": "Comparative Analysis of Lightweight Deep Learning Models for Memory-Constrained Devices", "categories": ["cs.CV", "cs.AI", "68-XX (Primary) 68Txx, 68T07 (Secondary)"], "comment": "22 pages, 10 figures, 4 tables, submitted to Springer - Pattern\n  Recognition and Image Analysis", "summary": "This paper presents a comprehensive evaluation of lightweight deep learning\nmodels for image classification, emphasizing their suitability for deployment\nin resource-constrained environments such as low-memory devices. Five\nstate-of-the-art architectures - MobileNetV3 Small, ResNet18, SqueezeNet,\nEfficientNetV2-S, and ShuffleNetV2 - are benchmarked across three diverse\ndatasets: CIFAR-10, CIFAR-100, and Tiny ImageNet. The models are assessed using\nfour key performance metrics: classification accuracy, inference time,\nfloating-point operations (FLOPs), and model size. Additionally, we investigate\nthe impact of hyperparameter tuning, data augmentation, and training paradigms\nby comparing pretrained models with scratch-trained counterparts, focusing on\nMobileNetV3 Small. Our findings reveal that transfer learning significantly\nenhances model accuracy and computational efficiency, particularly for complex\ndatasets like Tiny ImageNet. EfficientNetV2 consistently achieves the highest\naccuracy, while MobileNetV3 offers the best balance between accuracy and\nefficiency, and SqueezeNet excels in inference speed and compactness. This\nstudy highlights critical trade-offs between accuracy and efficiency, offering\nactionable insights for deploying lightweight models in real-world applications\nwhere computational resources are limited. By addressing these challenges, this\nresearch contributes to optimizing deep learning systems for edge computing and\nmobile platforms.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86\u4e94\u79cd\u8f7b\u91cf\u7ea7\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0EfficientNetV2-S\u51c6\u786e\u7387\u6700\u9ad8\uff0cMobileNetV3\u5728\u51c6\u786e\u7387\u548c\u6548\u7387\u95f4\u5e73\u8861\u6700\u4f73\uff0cSqueezeNet\u63a8\u7406\u901f\u5ea6\u6700\u5feb\u3002", "motivation": "\u7814\u7a76\u8f7b\u91cf\u7ea7\u6a21\u578b\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u7684\u9002\u7528\u6027\uff0c\u4ee5\u4f18\u5316\u8fb9\u7f18\u8ba1\u7b97\u548c\u79fb\u52a8\u5e73\u53f0\u7684\u6df1\u5ea6\u5b66\u4e60\u7cfb\u7edf\u3002", "method": "\u5bf9\u4e94\u79cd\u6a21\u578b\uff08MobileNetV3 Small\u3001ResNet18\u7b49\uff09\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u5206\u7c7b\u51c6\u786e\u7387\u3001\u63a8\u7406\u65f6\u95f4\u3001FLOPs\u548c\u6a21\u578b\u5927\u5c0f\uff0c\u5e76\u6bd4\u8f83\u9884\u8bad\u7ec3\u4e0e\u4ece\u5934\u8bad\u7ec3\u7684\u5dee\u5f02\u3002", "result": "\u8fc1\u79fb\u5b66\u4e60\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0cEfficientNetV2-S\u51c6\u786e\u7387\u6700\u9ad8\uff0cMobileNetV3\u5e73\u8861\u6700\u4f73\uff0cSqueezeNet\u6700\u7d27\u51d1\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u51c6\u786e\u7387\u4e0e\u6548\u7387\u7684\u6743\u8861\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u573a\u666f\u4e0b\u7684\u6a21\u578b\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\u3002"}}
{"id": "2505.03675", "pdf": "https://arxiv.org/pdf/2505.03675", "abs": "https://arxiv.org/abs/2505.03675", "authors": ["Anuja Tayal", "Devika Salunke", "Barbara Di Eugenio", "Paula G Allen-Meares", "Eulalia P Abril", "Olga Garcia-Bedoya", "Carolyn A Dickens", "Andrew D. Boyd"], "title": "Towards conversational assistants for health applications: using ChatGPT to generate conversations about heart failure", "categories": ["cs.CL"], "comment": null, "summary": "We explore the potential of ChatGPT (3.5-turbo and 4) to generate\nconversations focused on self-care strategies for African-American heart\nfailure patients -- a domain with limited specialized datasets. To simulate\npatient-health educator dialogues, we employed four prompting strategies:\ndomain, African American Vernacular English (AAVE), Social Determinants of\nHealth (SDOH), and SDOH-informed reasoning. Conversations were generated across\nkey self-care domains of food, exercise, and fluid intake, with varying turn\nlengths (5, 10, 15) and incorporated patient-specific SDOH attributes such as\nage, gender, neighborhood, and socioeconomic status. Our findings show that\neffective prompt design is essential. While incorporating SDOH and reasoning\nimproves dialogue quality, ChatGPT still lacks the empathy and engagement\nneeded for meaningful healthcare communication.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86ChatGPT\uff083.5-turbo\u548c4\uff09\u4e3a\u975e\u6d32\u88d4\u7f8e\u56fd\u5fc3\u8870\u60a3\u8005\u751f\u6210\u81ea\u6211\u62a4\u7406\u5bf9\u8bdd\u7684\u6f5c\u529b\uff0c\u53d1\u73b0\u6709\u6548\u63d0\u793a\u8bbe\u8ba1\u662f\u5173\u952e\uff0c\u4f46ChatGPT\u4ecd\u7f3a\u4e4f\u533b\u7597\u6c9f\u901a\u6240\u9700\u7684\u540c\u7406\u5fc3\u548c\u53c2\u4e0e\u5ea6\u3002", "motivation": "\u9488\u5bf9\u975e\u6d32\u88d4\u7f8e\u56fd\u5fc3\u8870\u60a3\u8005\u81ea\u6211\u62a4\u7406\u9886\u57df\u7f3a\u4e4f\u4e13\u95e8\u6570\u636e\u96c6\u7684\u95ee\u9898\uff0c\u63a2\u7d22ChatGPT\u751f\u6210\u76f8\u5173\u5bf9\u8bdd\u7684\u80fd\u529b\u3002", "method": "\u91c7\u7528\u56db\u79cd\u63d0\u793a\u7b56\u7565\uff08\u9886\u57df\u3001AAVE\u3001SDOH\u3001SDOH\u63a8\u7406\uff09\u751f\u6210\u5bf9\u8bdd\uff0c\u6db5\u76d6\u996e\u98df\u3001\u8fd0\u52a8\u548c\u6db2\u4f53\u6444\u5165\u7b49\u5173\u952e\u9886\u57df\uff0c\u5e76\u7ed3\u5408\u60a3\u8005\u7279\u5b9aSDOH\u5c5e\u6027\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u7ed3\u5408SDOH\u548c\u63a8\u7406\u80fd\u63d0\u5347\u5bf9\u8bdd\u8d28\u91cf\uff0c\u4f46ChatGPT\u5728\u540c\u7406\u5fc3\u548c\u53c2\u4e0e\u5ea6\u65b9\u9762\u4ecd\u6709\u4e0d\u8db3\u3002", "conclusion": "\u63d0\u793a\u8bbe\u8ba1\u5bf9\u751f\u6210\u9ad8\u8d28\u91cf\u5bf9\u8bdd\u81f3\u5173\u91cd\u8981\uff0c\u4f46ChatGPT\u9700\u8fdb\u4e00\u6b65\u6539\u8fdb\u4ee5\u66f4\u597d\u5730\u652f\u6301\u533b\u7597\u6c9f\u901a\u3002"}}
{"id": "2505.03310", "pdf": "https://arxiv.org/pdf/2505.03310", "abs": "https://arxiv.org/abs/2505.03310", "authors": ["Lei Liu", "Zhenghao Chen", "Dong Xu"], "title": "3D Gaussian Splatting Data Compression with Mixture of Priors", "categories": ["cs.CV"], "comment": null, "summary": "3D Gaussian Splatting (3DGS) data compression is crucial for enabling\nefficient storage and transmission in 3D scene modeling. However, its\ndevelopment remains limited due to inadequate entropy models and suboptimal\nquantization strategies for both lossless and lossy compression scenarios,\nwhere existing methods have yet to 1) fully leverage hyperprior information to\nconstruct robust conditional entropy models, and 2) apply fine-grained,\nelement-wise quantization strategies for improved compression granularity. In\nthis work, we propose a novel Mixture of Priors (MoP) strategy to\nsimultaneously address these two challenges. Specifically, inspired by the\nMixture-of-Experts (MoE) paradigm, our MoP approach processes hyperprior\ninformation through multiple lightweight MLPs to generate diverse prior\nfeatures, which are subsequently integrated into the MoP feature via a gating\nmechanism. To enhance lossless compression, the resulting MoP feature is\nutilized as a hyperprior to improve conditional entropy modeling. Meanwhile,\nfor lossy compression, we employ the MoP feature as guidance information in an\nelement-wise quantization procedure, leveraging a prior-guided Coarse-to-Fine\nQuantization (C2FQ) strategy with a predefined quantization step value.\nSpecifically, we expand the quantization step value into a matrix and\nadaptively refine it from coarse to fine granularity, guided by the MoP\nfeature, thereby obtaining a quantization step matrix that facilitates\nelement-wise quantization. Extensive experiments demonstrate that our proposed\n3DGS data compression framework achieves state-of-the-art performance across\nmultiple benchmarks, including Mip-NeRF360, BungeeNeRF, DeepBlending, and\nTank&Temples.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df7\u5408\u5148\u9a8c\uff08MoP\uff09\u7b56\u7565\u76843D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u6570\u636e\u538b\u7f29\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u71b5\u6a21\u578b\u548c\u91cf\u5316\u7b56\u7565\u4e0a\u7684\u4e0d\u8db3\uff0c\u663e\u8457\u63d0\u5347\u4e86\u538b\u7f29\u6027\u80fd\u3002", "motivation": "3DGS\u6570\u636e\u538b\u7f29\u5728\u9ad8\u6548\u5b58\u50a8\u548c\u4f20\u8f93\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u71b5\u6a21\u578b\u548c\u91cf\u5316\u7b56\u7565\u4e0a\u8868\u73b0\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u5176\u53d1\u5c55\u3002", "method": "\u91c7\u7528\u6df7\u5408\u5148\u9a8c\uff08MoP\uff09\u7b56\u7565\uff0c\u901a\u8fc7\u591a\u4e2a\u8f7b\u91cf\u7ea7MLP\u751f\u6210\u591a\u6837\u5316\u7684\u5148\u9a8c\u7279\u5f81\uff0c\u5e76\u7ed3\u5408\u95e8\u63a7\u673a\u5236\u6574\u5408\u4e3aMoP\u7279\u5f81\u3002MoP\u7279\u5f81\u7528\u4e8e\u6539\u8fdb\u6761\u4ef6\u71b5\u6a21\u578b\uff08\u65e0\u635f\u538b\u7f29\uff09\u548c\u6307\u5bfc\u5143\u7d20\u7ea7\u91cf\u5316\uff08\u6709\u635f\u538b\u7f29\uff09\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\uff08\u5982Mip-NeRF360\u3001BungeeNeRF\u7b49\uff09\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "MoP\u7b56\u7565\u6709\u6548\u63d0\u5347\u4e863DGS\u6570\u636e\u538b\u7f29\u7684\u6027\u80fd\uff0c\u4e3a\u9ad8\u6548\u5b58\u50a8\u548c\u4f20\u8f93\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.03688", "pdf": "https://arxiv.org/pdf/2505.03688", "abs": "https://arxiv.org/abs/2505.03688", "authors": ["Sharvi Endait", "Ruturaj Ghatage", "Aditya Kulkarni", "Rajlaxmi Patil", "Raviraj Joshi"], "title": "IndicSQuAD: A Comprehensive Multilingual Question Answering Dataset for Indic Languages", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "The rapid progress in question-answering (QA) systems has predominantly\nbenefited high-resource languages, leaving Indic languages largely\nunderrepresented despite their vast native speaker base. In this paper, we\npresent IndicSQuAD, a comprehensive multi-lingual extractive QA dataset\ncovering nine major Indic languages, systematically derived from the SQuAD\ndataset. Building on previous work with MahaSQuAD for Marathi, our approach\nadapts and extends translation techniques to maintain high linguistic fidelity\nand accurate answer-span alignment across diverse languages. IndicSQuAD\ncomprises extensive training, validation, and test sets for each language,\nproviding a robust foundation for model development. We evaluate baseline\nperformances using language-specific monolingual BERT models and the\nmultilingual MuRIL-BERT. The results indicate some challenges inherent in\nlow-resource settings. Moreover, our experiments suggest potential directions\nfor future work, including expanding to additional languages, developing\ndomain-specific datasets, and incorporating multimodal data. The dataset and\nmodels are publicly shared at https://github.com/l3cube-pune/indic-nlp", "AI": {"tldr": "IndicSQuAD\u662f\u4e00\u4e2a\u8986\u76d6\u4e5d\u79cd\u4e3b\u8981\u5370\u5ea6\u8bed\u8a00\u7684\u591a\u8bed\u8a00\u62bd\u53d6\u5f0fQA\u6570\u636e\u96c6\uff0c\u57fa\u4e8eSQuAD\u6570\u636e\u96c6\u6784\u5efa\uff0c\u65e8\u5728\u89e3\u51b3\u5370\u5ea6\u8bed\u8a00\u5728QA\u7cfb\u7edf\u4e2d\u7684\u8d44\u6e90\u4e0d\u8db3\u95ee\u9898\u3002", "motivation": "\u9ad8\u8d44\u6e90\u8bed\u8a00\u5728QA\u7cfb\u7edf\u4e2d\u8fdb\u5c55\u8fc5\u901f\uff0c\u800c\u5370\u5ea6\u8bed\u8a00\u5c3d\u7ba1\u62e5\u6709\u5927\u91cf\u6bcd\u8bed\u4f7f\u7528\u8005\uff0c\u5374\u8d44\u6e90\u532e\u4e4f\u3002IndicSQuAD\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u7ffb\u8bd1\u6280\u672f\u4eceSQuAD\u6570\u636e\u96c6\u6269\u5c55\uff0c\u4fdd\u6301\u8bed\u8a00\u4fdd\u771f\u5ea6\u548c\u7b54\u6848\u8de8\u5ea6\u5bf9\u9f50\uff0c\u6784\u5efa\u8bad\u7ec3\u3001\u9a8c\u8bc1\u548c\u6d4b\u8bd5\u96c6\u3002", "result": "\u4f7f\u7528\u5355\u8bed\u548c\u591a\u8bedBERT\u6a21\u578b\u8bc4\u4f30\uff0c\u7ed3\u679c\u663e\u793a\u4f4e\u8d44\u6e90\u73af\u5883\u4e0b\u7684\u6311\u6218\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "IndicSQuAD\u4e3a\u5370\u5ea6\u8bed\u8a00QA\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u672a\u6765\u53ef\u6269\u5c55\u81f3\u66f4\u591a\u8bed\u8a00\u548c\u591a\u6a21\u6001\u6570\u636e\u3002"}}
{"id": "2505.03318", "pdf": "https://arxiv.org/pdf/2505.03318", "abs": "https://arxiv.org/abs/2505.03318", "authors": ["Yibin Wang", "Zhimin Li", "Yuhang Zang", "Chunyu Wang", "Qinglin Lu", "Cheng Jin", "Jiaqi Wang"], "title": "Unified Multimodal Chain-of-Thought Reward Model through Reinforcement Fine-Tuning", "categories": ["cs.CV"], "comment": "project page: https://codegoat24.github.io/UnifiedReward/think", "summary": "Recent advances in multimodal Reward Models (RMs) have shown significant\npromise in delivering reward signals to align vision models with human\npreferences. However, current RMs are generally restricted to providing direct\nresponses or engaging in shallow reasoning processes with limited depth, often\nleading to inaccurate reward signals. We posit that incorporating explicit long\nchains of thought (CoT) into the reward reasoning process can significantly\nstrengthen their reliability and robustness. Furthermore, we believe that once\nRMs internalize CoT reasoning, their direct response accuracy can also be\nimproved through implicit reasoning capabilities. To this end, this paper\nproposes UnifiedReward-Think, the first unified multimodal CoT-based reward\nmodel, capable of multi-dimensional, step-by-step long-chain reasoning for both\nvisual understanding and generation reward tasks. Specifically, we adopt an\nexploration-driven reinforcement fine-tuning approach to elicit and incentivize\nthe model's latent complex reasoning ability: (1) We first use a small amount\nof image generation preference data to distill the reasoning process of GPT-4o,\nwhich is then used for the model's cold start to learn the format and structure\nof CoT reasoning. (2) Subsequently, by leveraging the model's prior knowledge\nand generalization capabilities, we prepare large-scale unified multimodal\npreference data to elicit the model's reasoning process across various vision\ntasks. During this phase, correct reasoning outputs are retained for rejection\nsampling to refine the model (3) while incorrect predicted samples are finally\nused for Group Relative Policy Optimization (GRPO) based reinforcement\nfine-tuning, enabling the model to explore diverse reasoning paths and optimize\nfor correct and robust solutions. Extensive experiments across various vision\nreward tasks demonstrate the superiority of our model.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u94fe\u5f0f\u601d\u7ef4\uff08CoT\uff09\u7684\u591a\u6a21\u6001\u5956\u52b1\u6a21\u578bUnifiedReward-Think\uff0c\u901a\u8fc7\u957f\u94fe\u63a8\u7406\u63d0\u5347\u5956\u52b1\u4fe1\u53f7\u7684\u53ef\u9760\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u5956\u52b1\u6a21\u578b\uff08RMs\uff09\u901a\u5e38\u4ec5\u63d0\u4f9b\u76f4\u63a5\u54cd\u5e94\u6216\u6d45\u5c42\u63a8\u7406\uff0c\u5bfc\u81f4\u5956\u52b1\u4fe1\u53f7\u4e0d\u51c6\u786e\u3002\u901a\u8fc7\u5f15\u5165\u663e\u5f0f\u957f\u94fe\u63a8\u7406\uff0c\u53ef\u4ee5\u63d0\u5347\u6a21\u578b\u7684\u53ef\u9760\u6027\u548c\u9c81\u68d2\u6027\u3002", "method": "\u91c7\u7528\u63a2\u7d22\u9a71\u52a8\u7684\u5f3a\u5316\u5fae\u8c03\u65b9\u6cd5\uff1a1) \u5229\u7528\u5c11\u91cf\u56fe\u50cf\u751f\u6210\u504f\u597d\u6570\u636e\u84b8\u998fGPT-4o\u7684\u63a8\u7406\u8fc7\u7a0b\uff1b2) \u5229\u7528\u5927\u89c4\u6a21\u591a\u6a21\u6001\u504f\u597d\u6570\u636e\u6fc0\u53d1\u6a21\u578b\u63a8\u7406\u80fd\u529b\uff1b3) \u901a\u8fc7GRPO\u5f3a\u5316\u5fae\u8c03\u4f18\u5316\u6a21\u578b\u3002", "result": "\u5728\u591a\u79cd\u89c6\u89c9\u5956\u52b1\u4efb\u52a1\u4e2d\uff0c\u6a21\u578b\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "UnifiedReward-Think\u901a\u8fc7\u957f\u94fe\u63a8\u7406\u663e\u8457\u63d0\u5347\u4e86\u5956\u52b1\u6a21\u578b\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2505.03711", "pdf": "https://arxiv.org/pdf/2505.03711", "abs": "https://arxiv.org/abs/2505.03711", "authors": ["Baharul Islam", "Nasim Ahmad", "Ferdous Ahmed Barbhuiya", "Kuntal Dey"], "title": "NBF at SemEval-2025 Task 5: Light-Burst Attention Enhanced System for Multilingual Subject Recommendation", "categories": ["cs.CL"], "comment": null, "summary": "We present our system submission for SemEval 2025 Task 5, which focuses on\ncross-lingual subject classification in the English and German academic\ndomains. Our approach leverages bilingual data during training, employing\nnegative sampling and a margin-based retrieval objective. We demonstrate that a\ndimension-as-token self-attention mechanism designed with significantly reduced\ninternal dimensions can effectively encode sentence embeddings for subject\nretrieval. In quantitative evaluation, our system achieved an average recall\nrate of 32.24% in the general quantitative setting (all subjects), 43.16% and\n31.53% of the general qualitative evaluation methods with minimal GPU usage,\nhighlighting their competitive performance. Our results demonstrate that our\napproach is effective in capturing relevant subject information under resource\nconstraints, although there is still room for improvement.", "AI": {"tldr": "\u8be5\u7cfb\u7edf\u5728SemEval 2025\u4efb\u52a15\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u5229\u7528\u53cc\u8bed\u6570\u636e\u548c\u8d1f\u91c7\u6837\u6280\u672f\uff0c\u901a\u8fc7\u4f4e\u7ef4\u81ea\u6ce8\u610f\u529b\u673a\u5236\u5b9e\u73b0\u9ad8\u6548\u4e3b\u9898\u68c0\u7d22\u3002", "motivation": "\u7814\u7a76\u8de8\u8bed\u8a00\u4e3b\u9898\u5206\u7c7b\u95ee\u9898\uff0c\u65e8\u5728\u5728\u8d44\u6e90\u53d7\u9650\u6761\u4ef6\u4e0b\u9ad8\u6548\u6355\u6349\u4e3b\u9898\u4fe1\u606f\u3002", "method": "\u91c7\u7528\u53cc\u8bed\u6570\u636e\u8bad\u7ec3\uff0c\u7ed3\u5408\u8d1f\u91c7\u6837\u548c\u57fa\u4e8e\u8fb9\u754c\u7684\u68c0\u7d22\u76ee\u6807\uff0c\u8bbe\u8ba1\u4f4e\u7ef4\u81ea\u6ce8\u610f\u529b\u673a\u5236\u7f16\u7801\u53e5\u5b50\u5d4c\u5165\u3002", "result": "\u5e73\u5747\u53ec\u56de\u7387\u4e3a32.24%\uff08\u6240\u6709\u4e3b\u9898\uff09\uff0c\u5b9a\u6027\u8bc4\u4f30\u4e2d\u5206\u522b\u4e3a43.16%\u548c31.53%\uff0cGPU\u4f7f\u7528\u7387\u4f4e\u3002", "conclusion": "\u65b9\u6cd5\u5728\u8d44\u6e90\u53d7\u9650\u6761\u4ef6\u4e0b\u6709\u6548\uff0c\u4f46\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\u3002"}}
{"id": "2505.03319", "pdf": "https://arxiv.org/pdf/2505.03319", "abs": "https://arxiv.org/abs/2505.03319", "authors": ["Manolis Mylonas", "Evlampios Apostolidis", "Vasileios Mezaris"], "title": "SD-VSum: A Method and Dataset for Script-Driven Video Summarization", "categories": ["cs.CV", "cs.AI", "cs.MM"], "comment": "Under review", "summary": "In this work, we introduce the task of script-driven video summarization,\nwhich aims to produce a summary of the full-length video by selecting the parts\nthat are most relevant to a user-provided script outlining the visual content\nof the desired summary. Following, we extend a recently-introduced large-scale\ndataset for generic video summarization (VideoXum) by producing natural\nlanguage descriptions of the different human-annotated summaries that are\navailable per video. In this way we make it compatible with the introduced\ntask, since the available triplets of ``video, summary and summary\ndescription'' can be used for training a method that is able to produce\ndifferent summaries for a given video, driven by the provided script about the\ncontent of each summary. Finally, we develop a new network architecture for\nscript-driven video summarization (SD-VSum), that relies on the use of a\ncross-modal attention mechanism for aligning and fusing information from the\nvisual and text modalities. Our experimental evaluations demonstrate the\nadvanced performance of SD-VSum against state-of-the-art approaches for\nquery-driven and generic (unimodal and multimodal) summarization from the\nliterature, and document its capacity to produce video summaries that are\nadapted to each user's needs about their content.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u811a\u672c\u7684\u89c6\u9891\u6458\u8981\u4efb\u52a1\uff08SD-VSum\uff09\uff0c\u901a\u8fc7\u7528\u6237\u63d0\u4f9b\u7684\u811a\u672c\u9009\u62e9\u89c6\u9891\u4e2d\u6700\u76f8\u5173\u7684\u90e8\u5206\u751f\u6210\u6458\u8981\uff0c\u5e76\u6269\u5c55\u4e86VideoXum\u6570\u636e\u96c6\u4ee5\u652f\u6301\u591a\u6a21\u6001\u8bad\u7ec3\u3002\u5b9e\u9a8c\u8868\u660e\uff0cSD-VSum\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u89c6\u9891\u6458\u8981\u4efb\u52a1\u65e0\u6cd5\u6839\u636e\u7528\u6237\u9700\u6c42\u751f\u6210\u591a\u6837\u5316\u6458\u8981\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u57fa\u4e8e\u811a\u672c\u9a71\u52a8\u7684\u89c6\u9891\u6458\u8981\u4efb\u52a1\u3002", "method": "\u6269\u5c55VideoXum\u6570\u636e\u96c6\uff0c\u52a0\u5165\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\uff1b\u8bbe\u8ba1\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u673a\u5236\u7f51\u7edc\uff08SD-VSum\uff09\uff0c\u878d\u5408\u89c6\u89c9\u4e0e\u6587\u672c\u4fe1\u606f\u3002", "result": "SD-VSum\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u7684\u67e5\u8be2\u9a71\u52a8\u548c\u901a\u7528\u6458\u8981\u65b9\u6cd5\uff0c\u80fd\u6839\u636e\u7528\u6237\u9700\u6c42\u751f\u6210\u9002\u914d\u7684\u6458\u8981\u3002", "conclusion": "SD-VSum\u4e3a\u89c6\u9891\u6458\u8981\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u7684\u7528\u6237\u9a71\u52a8\u89e3\u51b3\u65b9\u6848\uff0c\u6027\u80fd\u4f18\u8d8a\u3002"}}
{"id": "2505.03733", "pdf": "https://arxiv.org/pdf/2505.03733", "abs": "https://arxiv.org/abs/2505.03733", "authors": ["Zimu Lu", "Yunqiao Yang", "Houxing Ren", "Haotian Hou", "Han Xiao", "Ke Wang", "Weikang Shi", "Aojun Zhou", "Mingjie Zhan", "Hongsheng Li"], "title": "WebGen-Bench: Evaluating LLMs on Generating Interactive and Functional Websites from Scratch", "categories": ["cs.CL"], "comment": null, "summary": "LLM-based agents have demonstrated great potential in generating and managing\ncode within complex codebases. In this paper, we introduce WebGen-Bench, a\nnovel benchmark designed to measure an LLM-based agent's ability to create\nmulti-file website codebases from scratch. It contains diverse instructions for\nwebsite generation, created through the combined efforts of human annotators\nand GPT-4o. These instructions span three major categories and thirteen minor\ncategories, encompassing nearly all important types of web applications. To\nassess the quality of the generated websites, we use GPT-4o to generate test\ncases targeting each functionality described in the instructions, and then\nmanually filter, adjust, and organize them to ensure accuracy, resulting in 647\ntest cases. Each test case specifies an operation to be performed on the\nwebsite and the expected result after the operation. To automate testing and\nimprove reproducibility, we employ a powerful web-navigation agent to execute\ntests on the generated websites and determine whether the observed responses\nalign with the expected results. We evaluate three high-performance code-agent\nframeworks, Bolt.diy, OpenHands, and Aider, using multiple proprietary and\nopen-source LLMs as engines. The best-performing combination, Bolt.diy powered\nby DeepSeek-R1, achieves only 27.8\\% accuracy on the test cases, highlighting\nthe challenging nature of our benchmark. Additionally, we construct\nWebGen-Instruct, a training set consisting of 6,667 website-generation\ninstructions. Training Qwen2.5-Coder-32B-Instruct on Bolt.diy trajectories\ngenerated from a subset of this training set achieves an accuracy of 38.2\\%,\nsurpassing the performance of the best proprietary model.", "AI": {"tldr": "WebGen-Bench\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30LLM\u4ee3\u7406\u751f\u6210\u591a\u6587\u4ef6\u7f51\u7ad9\u4ee3\u7801\u80fd\u529b\u7684\u57fa\u51c6\uff0c\u5305\u542b\u591a\u6837\u5316\u7684\u6307\u4ee4\u548c647\u4e2a\u6d4b\u8bd5\u7528\u4f8b\u3002\u6d4b\u8bd5\u7ed3\u679c\u663e\u793a\u73b0\u6709\u6a21\u578b\u7684\u6027\u80fd\u4ecd\u6709\u63d0\u5347\u7a7a\u95f4\u3002", "motivation": "\u8bc4\u4f30LLM\u4ee3\u7406\u5728\u751f\u6210\u590d\u6742\u7f51\u7ad9\u4ee3\u7801\u65b9\u9762\u7684\u80fd\u529b\uff0c\u63a8\u52a8\u76f8\u5173\u6280\u672f\u7684\u53d1\u5c55\u3002", "method": "\u901a\u8fc7\u4eba\u7c7b\u548cGPT-4o\u5408\u4f5c\u751f\u6210\u591a\u6837\u5316\u6307\u4ee4\uff0c\u4f7f\u7528GPT-4o\u751f\u6210\u6d4b\u8bd5\u7528\u4f8b\u5e76\u624b\u52a8\u4f18\u5316\uff0c\u5229\u7528\u7f51\u9875\u5bfc\u822a\u4ee3\u7406\u81ea\u52a8\u5316\u6d4b\u8bd5\u3002", "result": "\u6700\u4f73\u7ec4\u5408\uff08Bolt.diy + DeepSeek-R1\uff09\u7684\u51c6\u786e\u7387\u4ec5\u4e3a27.8%\uff0c\u8bad\u7ec3\u540e\u7684Qwen2.5-Coder-32B-Instruct\u8fbe\u523038.2%\u3002", "conclusion": "WebGen-Bench\u5177\u6709\u6311\u6218\u6027\uff0c\u73b0\u6709\u6a21\u578b\u8868\u73b0\u6709\u9650\uff0c\u4f46\u901a\u8fc7\u8bad\u7ec3\u53ef\u63d0\u5347\u6027\u80fd\u3002"}}
{"id": "2505.03327", "pdf": "https://arxiv.org/pdf/2505.03327", "abs": "https://arxiv.org/abs/2505.03327", "authors": ["Jos\u00e9-Luis Bueso-Bello", "Benjamin Chauvel", "Daniel Carcereri", "Philipp Posovszky", "Pietro Milillo", "Jennifer Ruiz", "Juan-Carlos Fern\u00e1ndez-Diaz", "Carolina Gonz\u00e1lez", "Michele Martone", "Ronny H\u00e4nsch", "Paola Rizzoli"], "title": "Very High-Resolution Forest Mapping with TanDEM-X InSAR Data and Self-Supervised Learning", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.IV"], "comment": "Preprint submitted to Remote Sensing of Environment", "summary": "Deep learning models have shown encouraging capabilities for mapping\naccurately forests at medium resolution with TanDEM-X interferometric SAR data.\nSuch models, as most of current state-of-the-art deep learning techniques in\nremote sensing, are trained in a fully-supervised way, which requires a large\namount of labeled data for training and validation. In this work, our aim is to\nexploit the high-resolution capabilities of the TanDEM-X mission to map forests\nat 6 m. The goal is to overcome the intrinsic limitations posed by\nmidresolution products, which affect, e.g., the detection of narrow roads\nwithin vegetated areas and the precise delineation of forested regions\ncontours. To cope with the lack of extended reliable reference datasets at such\na high resolution, we investigate self-supervised learning techniques for\nextracting highly informative representations from the input features, followed\nby a supervised training step with a significantly smaller number of reliable\nlabels. A 1 m resolution forest/non-forest reference map over Pennsylvania,\nUSA, allows for comparing different training approaches for the development of\nan effective forest mapping framework with limited labeled samples. We select\nthe best-performing approach over this test region and apply it in a real-case\nforest mapping scenario over the Amazon rainforest, where only very few labeled\ndata at high resolution are available. In this challenging scenario, the\nproposed self-supervised framework significantly enhances the classification\naccuracy with respect to fully-supervised methods, trained using the same\namount of labeled data, representing an extremely promising starting point for\nlarge-scale, very high-resolution forest mapping with TanDEM-X data.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u81ea\u76d1\u7763\u5b66\u4e60\u548c\u76d1\u7763\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u5229\u7528TanDEM-X\u6570\u636e\u5b9e\u73b0\u9ad8\u5206\u8fa8\u7387\u68ee\u6797\u5236\u56fe\uff0c\u89e3\u51b3\u4e86\u6807\u8bb0\u6570\u636e\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u5229\u7528TanDEM-X\u4efb\u52a1\u7684\u9ad8\u5206\u8fa8\u7387\u80fd\u529b\uff0c\u514b\u670d\u4e2d\u5206\u8fa8\u7387\u4ea7\u54c1\u5728\u68ee\u6797\u5236\u56fe\u4e2d\u7684\u5c40\u9650\u6027\uff08\u5982\u7a84\u9053\u8def\u68c0\u6d4b\u548c\u68ee\u6797\u8fb9\u754c\u7cbe\u786e\u5212\u5206\uff09\u3002", "method": "\u91c7\u7528\u81ea\u76d1\u7763\u5b66\u4e60\u4ece\u8f93\u5165\u7279\u5f81\u4e2d\u63d0\u53d6\u4fe1\u606f\u8868\u793a\uff0c\u968f\u540e\u7528\u5c11\u91cf\u53ef\u9760\u6807\u8bb0\u6570\u636e\u8fdb\u884c\u76d1\u7763\u8bad\u7ec3\u3002", "result": "\u5728\u4e9a\u9a6c\u900a\u96e8\u6797\u7684\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u5206\u7c7b\u7cbe\u5ea6\uff0c\u4f18\u4e8e\u5168\u76d1\u7763\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5927\u89c4\u6a21\u9ad8\u5206\u8fa8\u7387\u68ee\u6797\u5236\u56fe\u63d0\u4f9b\u4e86\u6781\u5177\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.03739", "pdf": "https://arxiv.org/pdf/2505.03739", "abs": "https://arxiv.org/abs/2505.03739", "authors": ["Zuwei Long", "Yunhang Shen", "Chaoyou Fu", "Heting Gao", "Lijiang Li", "Peixian Chen", "Mengdan Zhang", "Hang Shao", "Jian Li", "Jinlong Peng", "Haoyu Cao", "Ke Li", "Rongrong Ji", "Xing Sun"], "title": "VITA-Audio: Fast Interleaved Cross-Modal Token Generation for Efficient Large Speech-Language Model", "categories": ["cs.CL", "cs.AI"], "comment": "Training and Inference Codes: https://github.com/VITA-MLLM/VITA-Audio", "summary": "With the growing requirement for natural human-computer interaction,\nspeech-based systems receive increasing attention as speech is one of the most\ncommon forms of daily communication. However, the existing speech models still\nexperience high latency when generating the first audio token during streaming,\nwhich poses a significant bottleneck for deployment. To address this issue, we\npropose VITA-Audio, an end-to-end large speech model with fast audio-text token\ngeneration. Specifically, we introduce a lightweight Multiple Cross-modal Token\nPrediction (MCTP) module that efficiently generates multiple audio tokens\nwithin a single model forward pass, which not only accelerates the inference\nbut also significantly reduces the latency for generating the first audio in\nstreaming scenarios. In addition, a four-stage progressive training strategy is\nexplored to achieve model acceleration with minimal loss of speech quality. To\nour knowledge, VITA-Audio is the first multi-modal large language model capable\nof generating audio output during the first forward pass, enabling real-time\nconversational capabilities with minimal latency. VITA-Audio is fully\nreproducible and is trained on open-source data only. Experimental results\ndemonstrate that our model achieves an inference speedup of 3~5x at the 7B\nparameter scale, but also significantly outperforms open-source models of\nsimilar model size on multiple benchmarks for automatic speech recognition\n(ASR), text-to-speech (TTS), and spoken question answering (SQA) tasks.", "AI": {"tldr": "VITA-Audio\u662f\u4e00\u79cd\u7aef\u5230\u7aef\u7684\u5927\u578b\u8bed\u97f3\u6a21\u578b\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u591a\u6a21\u6001\u4ee4\u724c\u9884\u6d4b\u6a21\u5757\u548c\u56db\u9636\u6bb5\u6e10\u8fdb\u8bad\u7ec3\u7b56\u7565\uff0c\u663e\u8457\u964d\u4f4e\u6d41\u5f0f\u573a\u666f\u4e2d\u7684\u9996\u97f3\u9891\u751f\u6210\u5ef6\u8fdf\uff0c\u5b9e\u73b03~5\u500d\u7684\u63a8\u7406\u52a0\u901f\u3002", "motivation": "\u73b0\u6709\u8bed\u97f3\u6a21\u578b\u5728\u6d41\u5f0f\u573a\u666f\u4e2d\u751f\u6210\u9996\u97f3\u9891\u4ee4\u724c\u65f6\u5b58\u5728\u9ad8\u5ef6\u8fdf\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u90e8\u7f72\u3002", "method": "\u63d0\u51fa\u8f7b\u91cf\u7ea7\u591a\u6a21\u6001\u4ee4\u724c\u9884\u6d4b\u6a21\u5757\uff08MCTP\uff09\u548c\u56db\u9636\u6bb5\u6e10\u8fdb\u8bad\u7ec3\u7b56\u7565\uff0c\u4ee5\u9ad8\u6548\u751f\u6210\u591a\u97f3\u9891\u4ee4\u724c\u5e76\u52a0\u901f\u63a8\u7406\u3002", "result": "\u57287B\u53c2\u6570\u89c4\u6a21\u4e0b\u5b9e\u73b03~5\u500d\u63a8\u7406\u52a0\u901f\uff0c\u5e76\u5728ASR\u3001TTS\u548cSQA\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u540c\u7c7b\u5f00\u6e90\u6a21\u578b\u3002", "conclusion": "VITA-Audio\u662f\u9996\u4e2a\u80fd\u5728\u9996\u8f6e\u524d\u5411\u4f20\u9012\u4e2d\u751f\u6210\u97f3\u9891\u7684\u591a\u6a21\u6001\u5927\u6a21\u578b\uff0c\u4e3a\u5b9e\u65f6\u5bf9\u8bdd\u63d0\u4f9b\u4e86\u4f4e\u5ef6\u8fdf\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.03329", "pdf": "https://arxiv.org/pdf/2505.03329", "abs": "https://arxiv.org/abs/2505.03329", "authors": ["Rui Lan", "Yancheng Bai", "Xu Duan", "Mingxing Li", "Lei Sun", "Xiangxiang Chu"], "title": "FLUX-Text: A Simple and Advanced Diffusion Transformer Baseline for Scene Text Editing", "categories": ["cs.CV"], "comment": "9 pages, 4 figures", "summary": "The task of scene text editing is to modify or add texts on images while\nmaintaining the fidelity of newly generated text and visual coherence with the\nbackground. Recent works based on latent diffusion models (LDM) show improved\ntext editing results, yet still face challenges and often generate inaccurate\nor unrecognizable characters, especially for non-Latin ones (\\eg, Chinese),\nwhich have complex glyph structures. To address these issues, we present\nFLUX-Text, a simple and advanced multilingual scene text editing framework\nbased on FLUX-Fill. Specifically, we carefully investigate glyph conditioning,\nconsidering both visual and textual modalities. To retain the original\ngenerative capabilities of FLUX-Fill while enhancing its understanding and\ngeneration of glyphs, we propose lightweight glyph and text embedding modules.\nOwning to the lightweight design, FLUX-Text is trained only with $100K$\ntraining examples compared to current popular methods trained with 2.9M ones.\nWith no bells and whistles, our method achieves state-of-the-art performance on\ntext editing tasks. Qualitative and quantitative experiments on the public\ndatasets demonstrate that our method surpasses previous works in text fidelity.", "AI": {"tldr": "FLUX-Text\u662f\u4e00\u4e2a\u57fa\u4e8eFLUX-Fill\u7684\u591a\u8bed\u8a00\u573a\u666f\u6587\u672c\u7f16\u8f91\u6846\u67b6\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u5b57\u5f62\u548c\u6587\u672c\u5d4c\u5165\u6a21\u5757\u63d0\u5347\u6587\u672c\u7f16\u8f91\u6548\u679c\uff0c\u5c24\u5176\u5728\u975e\u62c9\u4e01\u5b57\u7b26\uff08\u5982\u4e2d\u6587\uff09\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff08LDM\uff09\u7684\u6587\u672c\u7f16\u8f91\u65b9\u6cd5\u5728\u975e\u62c9\u4e01\u5b57\u7b26\u4e0a\u751f\u6210\u6548\u679c\u4e0d\u4f73\uff0c\u5b57\u5f62\u590d\u6742\u65f6\u6613\u4ea7\u751f\u4e0d\u51c6\u786e\u6216\u4e0d\u53ef\u8bc6\u522b\u7684\u5b57\u7b26\u3002", "method": "\u63d0\u51faFLUX-Text\u6846\u67b6\uff0c\u7ed3\u5408\u89c6\u89c9\u548c\u6587\u672c\u6a21\u6001\u7684\u5b57\u5f62\u6761\u4ef6\uff0c\u8bbe\u8ba1\u8f7b\u91cf\u7ea7\u5b57\u5f62\u548c\u6587\u672c\u5d4c\u5165\u6a21\u5757\uff0c\u4ec5\u970010\u4e07\u8bad\u7ec3\u6837\u672c\u3002", "result": "\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\uff0cFLUX-Text\u5728\u6587\u672c\u4fdd\u771f\u5ea6\u4e0a\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "FLUX-Text\u901a\u8fc7\u8f7b\u91cf\u7ea7\u8bbe\u8ba1\u6709\u6548\u89e3\u51b3\u4e86\u590d\u6742\u5b57\u5f62\u6587\u672c\u7f16\u8f91\u95ee\u9898\uff0c\u4e14\u8bad\u7ec3\u6548\u7387\u9ad8\u3002"}}
{"id": "2505.02848", "pdf": "https://arxiv.org/pdf/2505.02848", "abs": "https://arxiv.org/abs/2505.02848", "authors": ["Kexin Ding", "Mu Zhou", "Akshay Chaudhari", "Shaoting Zhang", "Dimitris N. Metaxas"], "title": "Aligning Large Language Models with Healthcare Stakeholders: A Pathway to Trustworthy AI Integration", "categories": ["cs.CY", "cs.AI", "cs.CL"], "comment": null, "summary": "The wide exploration of large language models (LLMs) raises the awareness of\nalignment between healthcare stakeholder preferences and model outputs. This\nalignment becomes a crucial foundation to empower the healthcare workflow\neffectively, safely, and responsibly. Yet the varying behaviors of LLMs may not\nalways match with healthcare stakeholders' knowledge, demands, and values. To\nenable a human-AI alignment, healthcare stakeholders will need to perform\nessential roles in guiding and enhancing the performance of LLMs. Human\nprofessionals must participate in the entire life cycle of adopting LLM in\nhealthcare, including training data curation, model training, and inference. In\nthis review, we discuss the approaches, tools, and applications of alignments\nbetween healthcare stakeholders and LLMs. We demonstrate that LLMs can better\nfollow human values by properly enhancing healthcare knowledge integration,\ntask understanding, and human guidance. We provide outlooks on enhancing the\nalignment between humans and LLMs to build trustworthy real-world healthcare\napplications.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u533b\u7597\u9886\u57df\u4e2d\u7684\u5bf9\u9f50\u95ee\u9898\uff0c\u5f3a\u8c03\u533b\u7597\u5229\u76ca\u76f8\u5173\u8005\u9700\u53c2\u4e0e\u6a21\u578b\u5168\u751f\u547d\u5468\u671f\u4ee5\u786e\u4fdd\u5176\u8f93\u51fa\u7b26\u5408\u9700\u6c42\u4e0e\u4ef7\u503c\u89c2\u3002", "motivation": "LLMs\u5728\u533b\u7597\u9886\u57df\u7684\u5e7f\u6cdb\u5e94\u7528\u9700\u8981\u5176\u8f93\u51fa\u4e0e\u533b\u7597\u5229\u76ca\u76f8\u5173\u8005\u7684\u77e5\u8bc6\u3001\u9700\u6c42\u548c\u4ef7\u503c\u89c2\u5bf9\u9f50\uff0c\u4ee5\u786e\u4fdd\u5de5\u4f5c\u6d41\u7a0b\u7684\u6709\u6548\u6027\u3001\u5b89\u5168\u6027\u548c\u8d23\u4efb\u6027\u3002", "method": "\u901a\u8fc7\u533b\u7597\u5229\u76ca\u76f8\u5173\u8005\u53c2\u4e0eLLMs\u7684\u8bad\u7ec3\u6570\u636e\u6574\u7406\u3001\u6a21\u578b\u8bad\u7ec3\u548c\u63a8\u7406\u7b49\u5168\u751f\u547d\u5468\u671f\uff0c\u7ed3\u5408\u77e5\u8bc6\u6574\u5408\u3001\u4efb\u52a1\u7406\u89e3\u548c\u4eba\u5de5\u6307\u5bfc\uff0c\u5b9e\u73b0\u5bf9\u9f50\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u4e0a\u8ff0\u65b9\u6cd5\uff0cLLMs\u80fd\u66f4\u597d\u5730\u9075\u5faa\u4eba\u7c7b\u4ef7\u503c\u89c2\uff0c\u63d0\u5347\u533b\u7597\u5e94\u7528\u7684\u4fe1\u4efb\u5ea6\u3002", "conclusion": "\u672a\u6765\u9700\u8fdb\u4e00\u6b65\u5f3a\u5316\u4eba\u7c7b\u4e0eLLMs\u7684\u5bf9\u9f50\uff0c\u4ee5\u6784\u5efa\u53ef\u4fe1\u8d56\u7684\u533b\u7597\u5e94\u7528\u3002"}}
{"id": "2505.03334", "pdf": "https://arxiv.org/pdf/2505.03334", "abs": "https://arxiv.org/abs/2505.03334", "authors": ["Guoting Wei", "Yu Liu", "Xia Yuan", "Xizhe Xue", "Linlin Guo", "Yifan Yang", "Chunxia Zhao", "Zongwen Bai", "Haokui Zhang", "Rong Xiao"], "title": "From Word to Sentence: A Large-Scale Multi-Instance Dataset for Open-Set Aerial Detection", "categories": ["cs.CV", "cs.DB"], "comment": null, "summary": "In recent years, language-guided open-world aerial object detection has\ngained significant attention due to its better alignment with real-world\napplication needs. However, due to limited datasets, most existing\nlanguage-guided methods primarily focus on vocabulary, which fails to meet the\ndemands of more fine-grained open-world detection. To address this limitation,\nwe propose constructing a large-scale language-guided open-set aerial detection\ndataset, encompassing three levels of language guidance: from words to phrases,\nand ultimately to sentences. Centered around an open-source large\nvision-language model and integrating image-operation-based preprocessing with\nBERT-based postprocessing, we present the OS-W2S Label Engine, an automatic\nannotation pipeline capable of handling diverse scene annotations for aerial\nimages. Using this label engine, we expand existing aerial detection datasets\nwith rich textual annotations and construct a novel benchmark dataset, called\nMulti-instance Open-set Aerial Dataset (MI-OAD), addressing the limitations of\ncurrent remote sensing grounding data and enabling effective open-set aerial\ndetection. Specifically, MI-OAD contains 163,023 images and 2 million\nimage-caption pairs, approximately 40 times larger than comparable datasets. We\nalso employ state-of-the-art open-set methods from the natural image domain,\ntrained on our proposed dataset, to validate the model's open-set detection\ncapabilities. For instance, when trained on our dataset, Grounding DINO\nachieves improvements of 29.5 AP_{50} and 33.7 Recall@10 for sentence inputs\nunder zero-shot transfer conditions. Both the dataset and the label engine will\nbe released publicly.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5927\u89c4\u6a21\u8bed\u8a00\u5f15\u5bfc\u7684\u5f00\u96c6\u822a\u7a7a\u68c0\u6d4b\u6570\u636e\u96c6MI-OAD\uff0c\u5e76\u901a\u8fc7OS-W2S\u6807\u6ce8\u5f15\u64ce\u81ea\u52a8\u751f\u6210\u4e30\u5bcc\u6587\u672c\u6807\u6ce8\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5f00\u96c6\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u8bed\u8a00\u5f15\u5bfc\u65b9\u6cd5\u56e0\u6570\u636e\u96c6\u6709\u9650\uff0c\u96be\u4ee5\u6ee1\u8db3\u7ec6\u7c92\u5ea6\u5f00\u96c6\u68c0\u6d4b\u9700\u6c42\uff0c\u9700\u6784\u5efa\u66f4\u5927\u89c4\u6a21\u7684\u6570\u636e\u96c6\u3002", "method": "\u6784\u5efaMI-OAD\u6570\u636e\u96c6\uff0c\u5305\u542b163,023\u5f20\u56fe\u50cf\u548c200\u4e07\u56fe\u50cf-\u6587\u672c\u5bf9\uff0c\u91c7\u7528OS-W2S\u6807\u6ce8\u5f15\u64ce\u81ea\u52a8\u6807\u6ce8\u3002", "result": "\u5728\u96f6\u6837\u672c\u6761\u4ef6\u4e0b\uff0cGrounding DINO\u6a21\u578b\u5728\u53e5\u5b50\u8f93\u5165\u4e0a\u7684AP_{50}\u548cRecall@10\u5206\u522b\u63d0\u534729.5\u548c33.7\u3002", "conclusion": "MI-OAD\u548cOS-W2S\u6807\u6ce8\u5f15\u64ce\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u6570\u636e\u96c6\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5f00\u96c6\u822a\u7a7a\u68c0\u6d4b\u80fd\u529b\u3002"}}
{"id": "2505.02888", "pdf": "https://arxiv.org/pdf/2505.02888", "abs": "https://arxiv.org/abs/2505.02888", "authors": ["Rintaro Ando"], "title": "When Your Own Output Becomes Your Training Data: Noise-to-Meaning Loops and a Formal RSI Trigger", "categories": ["cs.LG", "cs.AI", "cs.CL", "68T05, 68Q85", "I.2.0; I.2.3; I.2.6"], "comment": "20 pages, 4 figures, 3 tables. Code:\n  github.com/rintaro-ando-tech/n2m-rsi-demo (v1.0)", "summary": "We present Noise-to-Meaning Recursive Self-Improvement (N2M-RSI), a minimal\nformal model showing that once an AI agent feeds its own outputs back as inputs\nand crosses an explicit information-integration threshold, its internal\ncomplexity will grow without bound under our assumptions. The framework unifies\nearlier ideas on self-prompting large language models, G\\\"odelian\nself-reference, and AutoML, yet remains implementation-agnostic. The model\nfurthermore scales naturally to interacting swarms of agents, hinting at\nsuper-linear effects once communication among instances is permitted. For\nsafety reasons, we omit system-specific implementation details and release only\na brief, model-agnostic toy prototype in Appendix C.", "AI": {"tldr": "N2M-RSI\u662f\u4e00\u4e2a\u5f62\u5f0f\u5316\u6a21\u578b\uff0c\u5c55\u793aAI\u4ee3\u7406\u5728\u53cd\u9988\u81ea\u8eab\u8f93\u51fa\u5e76\u8de8\u8d8a\u4fe1\u606f\u6574\u5408\u9608\u503c\u540e\uff0c\u5176\u5185\u90e8\u590d\u6742\u6027\u5c06\u65e0\u9650\u589e\u957f\u3002", "motivation": "\u7edf\u4e00\u81ea\u6211\u63d0\u793a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3001\u54e5\u5fb7\u5c14\u81ea\u6307\u548cAutoML\u7b49\u65e9\u671f\u601d\u60f3\uff0c\u63a2\u7d22AI\u81ea\u6211\u6539\u8fdb\u7684\u6f5c\u529b\u3002", "method": "\u901a\u8fc7\u9012\u5f52\u53cd\u9988\u548c\u8de8\u8d8a\u4fe1\u606f\u6574\u5408\u9608\u503c\uff0c\u6a21\u578b\u5b9e\u73b0\u81ea\u6211\u6539\u8fdb\uff0c\u5e76\u53ef\u6269\u5c55\u5230\u591a\u4ee3\u7406\u4ea4\u4e92\u3002", "result": "\u6a21\u578b\u663e\u793a\u5185\u90e8\u590d\u6742\u6027\u65e0\u9650\u589e\u957f\uff0c\u591a\u4ee3\u7406\u4ea4\u4e92\u53ef\u80fd\u4ea7\u751f\u8d85\u7ebf\u6027\u6548\u5e94\u3002", "conclusion": "N2M-RSI\u4e3aAI\u81ea\u6211\u6539\u8fdb\u63d0\u4f9b\u4e86\u7406\u8bba\u6846\u67b6\uff0c\u4f46\u51fa\u4e8e\u5b89\u5168\u8003\u8651\u672a\u516c\u5f00\u5177\u4f53\u5b9e\u73b0\u7ec6\u8282\u3002"}}
{"id": "2505.03350", "pdf": "https://arxiv.org/pdf/2505.03350", "abs": "https://arxiv.org/abs/2505.03350", "authors": ["Song Jian", "Hu Yuchang", "Wang Hui", "Chen Yen-Wei"], "title": "A Vision-Language Model for Focal Liver Lesion Classification", "categories": ["cs.CV", "I.2.10; I.4.8"], "comment": "9 pages,4 figures, 4 tables,Innovation in Medicine and Healthcare\n  Proceedings of 13th KES-InMed 2025", "summary": "Accurate classification of focal liver lesions is crucial for diagnosis and\ntreatment in hepatology. However, traditional supervised deep learning models\ndepend on large-scale annotated datasets, which are often limited in medical\nimaging. Recently, Vision-Language models (VLMs) such as Contrastive\nLanguage-Image Pre-training model (CLIP) has been applied to image\nclassifications. Compared to the conventional convolutional neural network\n(CNN), which classifiers image based on visual information only, VLM leverages\nmultimodal learning with text and images, allowing it to learn effectively even\nwith a limited amount of labeled data. Inspired by CLIP, we pro-pose a\nLiver-VLM, a model specifically designed for focal liver lesions (FLLs)\nclassification. First, Liver-VLM incorporates class information into the text\nencoder without introducing additional inference overhead. Second, by\ncalculating the pairwise cosine similarities between image and text embeddings\nand optimizing the model with a cross-entropy loss, Liver-VLM ef-fectively\naligns image features with class-level text features. Experimental results on\nMPCT-FLLs dataset demonstrate that the Liver-VLM model out-performs both the\nstandard CLIP and MedCLIP models in terms of accuracy and Area Under the Curve\n(AUC). Further analysis shows that using a lightweight ResNet18 backbone\nenhances classification performance, particularly under data-constrained\nconditions.", "AI": {"tldr": "Liver-VLM\u6a21\u578b\u5229\u7528\u89c6\u89c9-\u8bed\u8a00\u591a\u6a21\u6001\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86\u809d\u810f\u75c5\u7076\u5206\u7c7b\u7684\u51c6\u786e\u6027\uff0c\u5c24\u5176\u5728\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u4f9d\u8d56\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\uff0c\u800c\u533b\u5b66\u5f71\u50cf\u6570\u636e\u5f80\u5f80\u6709\u9650\u3002\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08\u5982CLIP\uff09\u901a\u8fc7\u591a\u6a21\u6001\u5b66\u4e60\u89e3\u51b3\u4e86\u8fd9\u4e00\u95ee\u9898\u3002", "method": "Liver-VLM\u5c06\u7c7b\u522b\u4fe1\u606f\u878d\u5165\u6587\u672c\u7f16\u7801\u5668\uff0c\u901a\u8fc7\u8ba1\u7b97\u56fe\u50cf\u4e0e\u6587\u672c\u5d4c\u5165\u7684\u4f59\u5f26\u76f8\u4f3c\u5ea6\uff0c\u5e76\u4f18\u5316\u4ea4\u53c9\u71b5\u635f\u5931\uff0c\u5b9e\u73b0\u56fe\u50cf\u4e0e\u6587\u672c\u7279\u5f81\u7684\u5bf9\u9f50\u3002", "result": "\u5728MPCT-FLLs\u6570\u636e\u96c6\u4e0a\uff0cLiver-VLM\u5728\u51c6\u786e\u7387\u548cAUC\u4e0a\u4f18\u4e8e\u6807\u51c6CLIP\u548cMedCLIP\u6a21\u578b\uff0c\u8f7b\u91cf\u7ea7ResNet18\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u6027\u80fd\u3002", "conclusion": "Liver-VLM\u4e3a\u809d\u810f\u75c5\u7076\u5206\u7c7b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u6570\u636e\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u6807\u6ce8\u6570\u636e\u6709\u9650\u7684\u573a\u666f\u3002"}}
{"id": "2505.02931", "pdf": "https://arxiv.org/pdf/2505.02931", "abs": "https://arxiv.org/abs/2505.02931", "authors": ["Fernando Vallecillos Ruiz", "Max Hort", "Leon Moonen"], "title": "The Art of Repair: Optimizing Iterative Program Repair with Instruction-Tuned Models", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG"], "comment": "Accepted for publication in the research track of the 29th\n  International Conference on Evaluation and Assessment in Software Engineering\n  (EASE), 17-20 June 2025, Istanbul, T\\\"urkiye", "summary": "Automatic program repair (APR) aims to reduce the manual efforts required to\nidentify and fix errors in source code. Before the rise of LLM-based agents, a\ncommon strategy was to increase the number of generated patches, sometimes to\nthe thousands, to achieve better repair results on benchmarks. More recently,\nself-iterative capabilities enabled LLMs to refine patches over multiple rounds\nguided by feedback. However, literature often focuses on many iterations and\ndisregards different numbers of outputs.\n  We investigate an APR pipeline that balances these two approaches, the\ngeneration of multiple outputs and multiple rounds of iteration, while imposing\na limit of 10 total patches per bug. We apply three SOTA instruction-tuned LLMs\n- DeepSeekCoder-Instruct, Codellama-Instruct, Llama3.1-Instruct - to the APR\ntask. We further fine-tune each model on an APR dataset with three sizes (1K,\n30K, 65K) and two techniques (Full Fine-Tuning and LoRA), allowing us to assess\ntheir repair capabilities on two APR benchmarks: HumanEval-Java and Defects4J.\n  Our results show that by using only a fraction (<1%) of the fine-tuning\ndataset, we can achieve improvements of up to 78% in the number of plausible\npatches generated, challenging prior studies that reported limited gains using\nFull Fine-Tuning. However, we find that exceeding certain thresholds leads to\ndiminishing outcomes, likely due to overfitting. Moreover, we show that base\nmodels greatly benefit from creating patches in an iterative fashion rather\nthan generating them all at once. In addition, the benefit of iterative\nstrategies becomes more pronounced in complex benchmarks. Even fine-tuned\nmodels, while benefiting less from iterations, still gain advantages,\nparticularly on complex benchmarks. The research underscores the need for\nbalanced APR strategies that combine multi-output generation and iterative\nrefinement.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u81ea\u52a8\u7a0b\u5e8f\u4fee\u590d\uff08APR\uff09\u4e2d\u591a\u8f93\u51fa\u751f\u6210\u4e0e\u591a\u8f6e\u8fed\u4ee3\u7684\u5e73\u8861\u7b56\u7565\uff0c\u901a\u8fc7\u9650\u5236\u6bcf\u4e2a\u9519\u8bef\u7684\u603b\u8865\u4e01\u6570\u4e3a10\uff0c\u8bc4\u4f30\u4e86\u4e09\u79cd\u6307\u4ee4\u8c03\u4f18LLM\u7684\u6027\u80fd\uff0c\u5e76\u53d1\u73b0\u5c0f\u89c4\u6a21\u5fae\u8c03\u6570\u636e\u96c6\u80fd\u663e\u8457\u63d0\u5347\u4fee\u590d\u6548\u679c\u3002", "motivation": "\u51cf\u5c11\u624b\u52a8\u4fee\u590d\u4ee3\u7801\u9519\u8bef\u7684\u5de5\u4f5c\u91cf\uff0c\u540c\u65f6\u63a2\u7d22LLM\u5728APR\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\uff0c\u5e73\u8861\u591a\u8f93\u51fa\u751f\u6210\u4e0e\u8fed\u4ee3\u4f18\u5316\u7684\u7b56\u7565\u3002", "method": "\u4f7f\u7528\u4e09\u79cd\u6307\u4ee4\u8c03\u4f18LLM\uff08DeepSeekCoder-Instruct\u3001Codellama-Instruct\u3001Llama3.1-Instruct\uff09\uff0c\u5728\u4e0d\u540c\u89c4\u6a21\u7684\u5fae\u8c03\u6570\u636e\u96c6\uff081K\u300130K\u300165K\uff09\u548c\u4e24\u79cd\u5fae\u8c03\u6280\u672f\uff08Full Fine-Tuning\u548cLoRA\uff09\u4e0b\uff0c\u8bc4\u4f30\u5176\u5728HumanEval-Java\u548cDefects4J\u57fa\u51c6\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u5c0f\u89c4\u6a21\u5fae\u8c03\u6570\u636e\u96c6\uff08<1%\uff09\u80fd\u63d0\u534778%\u7684\u5408\u7406\u8865\u4e01\u751f\u6210\u91cf\uff0c\u4f46\u8fc7\u5ea6\u5fae\u8c03\u4f1a\u5bfc\u81f4\u6548\u679c\u4e0b\u964d\uff1b\u8fed\u4ee3\u7b56\u7565\u5bf9\u57fa\u7840\u6a21\u578b\u6548\u679c\u663e\u8457\uff0c\u590d\u6742\u57fa\u51c6\u4e2d\u4f18\u52bf\u66f4\u660e\u663e\u3002", "conclusion": "\u5e73\u8861\u591a\u8f93\u51fa\u751f\u6210\u4e0e\u8fed\u4ee3\u4f18\u5316\u7684\u7b56\u7565\u5728APR\u4efb\u52a1\u4e2d\u66f4\u4e3a\u6709\u6548\uff0c\u5c24\u5176\u662f\u5728\u590d\u6742\u57fa\u51c6\u4e0a\uff0c\u540c\u65f6\u9700\u907f\u514d\u8fc7\u5ea6\u5fae\u8c03\u5bfc\u81f4\u7684\u8fc7\u62df\u5408\u3002"}}
{"id": "2505.03351", "pdf": "https://arxiv.org/pdf/2505.03351", "abs": "https://arxiv.org/abs/2505.03351", "authors": ["Dongbin Zhang", "Yunfei Liu", "Lijian Lin", "Ye Zhu", "Yang Li", "Minghan Qin", "Yu Li", "Haoqian Wang"], "title": "GUAVA: Generalizable Upper Body 3D Gaussian Avatar", "categories": ["cs.CV"], "comment": "Project page: https://eastbeanzhang.github.io/GUAVA/", "summary": "Reconstructing a high-quality, animatable 3D human avatar with expressive\nfacial and hand motions from a single image has gained significant attention\ndue to its broad application potential. 3D human avatar reconstruction\ntypically requires multi-view or monocular videos and training on individual\nIDs, which is both complex and time-consuming. Furthermore, limited by SMPLX's\nexpressiveness, these methods often focus on body motion but struggle with\nfacial expressions. To address these challenges, we first introduce an\nexpressive human model (EHM) to enhance facial expression capabilities and\ndevelop an accurate tracking method. Based on this template model, we propose\nGUAVA, the first framework for fast animatable upper-body 3D Gaussian avatar\nreconstruction. We leverage inverse texture mapping and projection sampling\ntechniques to infer Ubody (upper-body) Gaussians from a single image. The\nrendered images are refined through a neural refiner. Experimental results\ndemonstrate that GUAVA significantly outperforms previous methods in rendering\nquality and offers significant speed improvements, with reconstruction times in\nthe sub-second range (0.1s), and supports real-time animation and rendering.", "AI": {"tldr": "GUAVA\u6846\u67b6\u901a\u8fc7\u5355\u56fe\u50cf\u5feb\u901f\u91cd\u5efa\u9ad8\u8d28\u91cf\u3001\u53ef\u52a8\u753b\u7684\u4e0a\u534a\u8eab3D\u9ad8\u65af\u5316\u8eab\uff0c\u663e\u8457\u63d0\u5347\u6e32\u67d3\u8d28\u91cf\u548c\u901f\u5ea6\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u591a\u89c6\u56fe\u6216\u89c6\u9891\uff0c\u4e14\u53d7\u9650\u4e8eSMPLX\u6a21\u578b\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u96be\u4ee5\u5904\u7406\u9762\u90e8\u8868\u60c5\u3002", "method": "\u5f15\u5165\u8868\u8fbe\u6027\u4eba\u4f53\u6a21\u578b\uff08EHM\uff09\u548c\u6539\u8fdb\u7684\u8ddf\u8e2a\u65b9\u6cd5\uff0c\u57fa\u4e8e\u9ad8\u65af\u6295\u5f71\u548c\u9006\u5411\u7eb9\u7406\u6620\u5c04\u6280\u672f\u3002", "result": "GUAVA\u5728\u6e32\u67d3\u8d28\u91cf\u548c\u901f\u5ea6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u91cd\u5efa\u65f6\u95f4\u4ec50.1\u79d2\uff0c\u652f\u6301\u5b9e\u65f6\u52a8\u753b\u3002", "conclusion": "GUAVA\u4e3a\u5355\u56fe\u50cf\u91cd\u5efa\u53ef\u52a8\u753b3D\u5316\u8eab\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.02952", "pdf": "https://arxiv.org/pdf/2505.02952", "abs": "https://arxiv.org/abs/2505.02952", "authors": ["Fabrizio Marozzo"], "title": "Iterative Resolution of Prompt Ambiguities Using a Progressive Cutting-Search Approach", "categories": ["cs.AI", "cs.CL", "cs.ET", "cs.IR", "cs.LG"], "comment": null, "summary": "Generative AI systems have revolutionized human interaction by enabling\nnatural language-based coding and problem solving. However, the inherent\nambiguity of natural language often leads to imprecise instructions, forcing\nusers to iteratively test, correct, and resubmit their prompts. We propose an\niterative approach that systematically narrows down these ambiguities through a\nstructured series of clarification questions and alternative solution\nproposals, illustrated with input/output examples as well. Once every\nuncertainty is resolved, a final, precise solution is generated. Evaluated on a\ndiverse dataset spanning coding, data analysis, and creative writing, our\nmethod demonstrates superior accuracy, competitive resolution times, and higher\nuser satisfaction compared to conventional one-shot solutions, which typically\nrequire multiple manual iterations to achieve a correct output.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8fed\u4ee3\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u95ee\u9898\u548c\u793a\u4f8b\u89e3\u51b3\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u7684\u6a21\u7cca\u6027\uff0c\u751f\u6210\u7cbe\u786e\u89e3\u51b3\u65b9\u6848\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u66f4\u9ad8\u6548\u4e14\u7528\u6237\u6ee1\u610f\u5ea6\u66f4\u9ad8\u3002", "motivation": "\u81ea\u7136\u8bed\u8a00\u7684\u6a21\u7cca\u6027\u5bfc\u81f4\u751f\u6210\u5f0fAI\u7cfb\u7edf\u9700\u8981\u591a\u6b21\u8fed\u4ee3\u4fee\u6b63\uff0c\u6548\u7387\u4f4e\u4e0b\uff0c\u7528\u6237\u4f53\u9a8c\u4e0d\u4f73\u3002", "method": "\u91c7\u7528\u8fed\u4ee3\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u95ee\u9898\u548c\u66ff\u4ee3\u65b9\u6848\u9010\u6b65\u6d88\u9664\u6307\u4ee4\u6a21\u7cca\u6027\uff0c\u7ed3\u5408\u8f93\u5165/\u8f93\u51fa\u793a\u4f8b\u751f\u6210\u6700\u7ec8\u7cbe\u786e\u89e3\u3002", "result": "\u5728\u7f16\u7801\u3001\u6570\u636e\u5206\u6790\u548c\u521b\u610f\u5199\u4f5c\u7b49\u591a\u6837\u5316\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u3001\u7ade\u4e89\u6027\u7684\u89e3\u51b3\u65f6\u95f4\u548c\u7528\u6237\u6ee1\u610f\u5ea6\u3002", "conclusion": "\u8fed\u4ee3\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u5f0fAI\u7cfb\u7edf\u7684\u6548\u7387\u548c\u7528\u6237\u4f53\u9a8c\uff0c\u4f18\u4e8e\u4f20\u7edf\u4e00\u6b21\u6027\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.03361", "pdf": "https://arxiv.org/pdf/2505.03361", "abs": "https://arxiv.org/abs/2505.03361", "authors": ["Zihan Ye", "Shreyank N Gowda", "Shiming Chen", "Yaochu Jin", "Kaizhu Huang", "Xiaobo Jin"], "title": "Interpretable Zero-shot Learning with Infinite Class Concepts", "categories": ["cs.CV"], "comment": null, "summary": "Zero-shot learning (ZSL) aims to recognize unseen classes by aligning images\nwith intermediate class semantics, like human-annotated concepts or class\ndefinitions. An emerging alternative leverages Large-scale Language Models\n(LLMs) to automatically generate class documents. However, these methods often\nface challenges with transparency in the classification process and may suffer\nfrom the notorious hallucination problem in LLMs, resulting in non-visual class\nsemantics. This paper redefines class semantics in ZSL with a focus on\ntransferability and discriminability, introducing a novel framework called\nZero-shot Learning with Infinite Class Concepts (InfZSL). Our approach\nleverages the powerful capabilities of LLMs to dynamically generate an\nunlimited array of phrase-level class concepts. To address the hallucination\nchallenge, we introduce an entropy-based scoring process that incorporates a\n``goodness\" concept selection mechanism, ensuring that only the most\ntransferable and discriminative concepts are selected. Our InfZSL framework not\nonly demonstrates significant improvements on three popular benchmark datasets\nbut also generates highly interpretable, image-grounded concepts. Code will be\nreleased upon acceptance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aInfZSL\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u751f\u6210\u65e0\u9650\u77ed\u8bed\u7ea7\u7c7b\u522b\u6982\u5ff5\u6765\u6539\u8fdb\u96f6\u6837\u672c\u5b66\u4e60\uff0c\u89e3\u51b3\u4e86LLMs\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u5e76\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u96f6\u6837\u672c\u5b66\u4e60\uff08ZSL\uff09\u4f9d\u8d56\u7c7b\u522b\u8bed\u4e49\u5bf9\u9f50\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u900f\u660e\u5ea6\u4e0d\u8db3\u548cLLMs\u5e7b\u89c9\u95ee\u9898\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u91cd\u65b0\u5b9a\u4e49\u7c7b\u522b\u8bed\u4e49\u7684\u8fc1\u79fb\u6027\u548c\u533a\u5206\u6027\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u63d0\u51faInfZSL\u6846\u67b6\uff0c\u5229\u7528LLMs\u52a8\u6001\u751f\u6210\u77ed\u8bed\u7ea7\u7c7b\u522b\u6982\u5ff5\uff0c\u5e76\u901a\u8fc7\u57fa\u4e8e\u71b5\u7684\u8bc4\u5206\u548c\u201c\u4f18\u8d28\u201d\u6982\u5ff5\u9009\u62e9\u673a\u5236\u7b5b\u9009\u6700\u5177\u8fc1\u79fb\u6027\u548c\u533a\u5206\u6027\u7684\u6982\u5ff5\u3002", "result": "\u5728\u4e09\u4e2a\u6d41\u884c\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u663e\u8457\u63d0\u5347\uff0c\u5e76\u751f\u6210\u9ad8\u5ea6\u53ef\u89e3\u91ca\u7684\u56fe\u50cf\u57fa\u7840\u6982\u5ff5\u3002", "conclusion": "InfZSL\u6846\u67b6\u901a\u8fc7\u52a8\u6001\u751f\u6210\u548c\u7b5b\u9009\u7c7b\u522b\u6982\u5ff5\uff0c\u6709\u6548\u89e3\u51b3\u4e86ZSL\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u540c\u65f6\u63d0\u5347\u4e86\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2505.03031", "pdf": "https://arxiv.org/pdf/2505.03031", "abs": "https://arxiv.org/abs/2505.03031", "authors": ["Sean I. Young"], "title": "Radio: Rate-Distortion Optimization for Large Language Model Compression", "categories": ["cs.LG", "cs.CL"], "comment": "Accepted to ICML 2025", "summary": "In recent years, the compression of large language models (LLMs) has emerged\nas a key problem in facilitating LLM deployment on resource-limited devices,\nreducing compute costs, and mitigating the environmental footprint due to\nlarge-scale AI infrastructure. Here, we establish the foundations of LLM\nquantization from a rate-distortion theory perspective and propose a\nquantization technique based on simple rate-distortion optimization. Our\ntechnique scales to models containing hundreds of billions of weight parameters\nand offers users the flexibility to compress models, post-training, to a model\nsize or accuracy specified by the user.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7387\u5931\u771f\u7406\u8bba\u7684\u5927\u8bed\u8a00\u6a21\u578b\u91cf\u5316\u65b9\u6cd5\uff0c\u652f\u6301\u7528\u6237\u6309\u9700\u538b\u7f29\u6a21\u578b\u3002", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u7684\u90e8\u7f72\u95ee\u9898\uff0c\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u548c\u73af\u5883\u5f71\u54cd\u3002", "method": "\u4ece\u7387\u5931\u771f\u7406\u8bba\u51fa\u53d1\uff0c\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u7b80\u5355\u7387\u5931\u771f\u4f18\u5316\u7684\u91cf\u5316\u6280\u672f\u3002", "result": "\u6280\u672f\u53ef\u6269\u5c55\u5230\u5305\u542b\u6570\u5343\u4ebf\u53c2\u6570\u7684\u6a21\u578b\uff0c\u652f\u6301\u7528\u6237\u6309\u6a21\u578b\u5927\u5c0f\u6216\u7cbe\u5ea6\u9700\u6c42\u538b\u7f29\u6a21\u578b\u3002", "conclusion": "\u8be5\u91cf\u5316\u65b9\u6cd5\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u7684\u538b\u7f29\u63d0\u4f9b\u4e86\u7075\u6d3b\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.03362", "pdf": "https://arxiv.org/pdf/2505.03362", "abs": "https://arxiv.org/abs/2505.03362", "authors": ["Shikun Zhang", "Yiqun Wang", "Cunjian Chen", "Yong Li", "Qiuhong Ke"], "title": "3D Surface Reconstruction with Enhanced High-Frequency Details", "categories": ["cs.CV"], "comment": "Accepted by Journal of Visual Communication and Image Representation", "summary": "Neural implicit 3D reconstruction can reproduce shapes without 3D\nsupervision, and it learns the 3D scene through volume rendering methods and\nneural implicit representations. Current neural surface reconstruction methods\ntend to randomly sample the entire image, making it difficult to learn\nhigh-frequency details on the surface, and thus the reconstruction results tend\nto be too smooth. We designed a method (FreNeuS) based on high-frequency\ninformation to solve the problem of insufficient surface detail. Specifically,\nFreNeuS uses pixel gradient changes to easily acquire high-frequency regions in\nan image and uses the obtained high-frequency information to guide surface\ndetail reconstruction. High-frequency information is first used to guide the\ndynamic sampling of rays, applying different sampling strategies according to\nvariations in high-frequency regions. To further enhance the focus on surface\ndetails, we have designed a high-frequency weighting method that constrains the\nrepresentation of high-frequency details during the reconstruction process.\nQualitative and quantitative results show that our method can reconstruct fine\nsurface details and obtain better surface reconstruction quality compared to\nexisting methods. In addition, our method is more applicable and can be\ngeneralized to any NeuS-based work.", "AI": {"tldr": "FreNeuS\u5229\u7528\u9ad8\u9891\u4fe1\u606f\u6539\u8fdb\u795e\u7ecf\u9690\u5f0f3D\u91cd\u5efa\uff0c\u901a\u8fc7\u52a8\u6001\u91c7\u6837\u548c\u9ad8\u9891\u52a0\u6743\u65b9\u6cd5\u63d0\u5347\u8868\u9762\u7ec6\u8282\u91cd\u5efa\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u795e\u7ecf\u8868\u9762\u91cd\u5efa\u65b9\u6cd5\u56e0\u968f\u673a\u91c7\u6837\u5bfc\u81f4\u9ad8\u9891\u7ec6\u8282\u5b66\u4e60\u4e0d\u8db3\uff0c\u91cd\u5efa\u7ed3\u679c\u8fc7\u4e8e\u5e73\u6ed1\u3002", "method": "FreNeuS\u5229\u7528\u50cf\u7d20\u68af\u5ea6\u53d8\u5316\u83b7\u53d6\u9ad8\u9891\u533a\u57df\uff0c\u52a8\u6001\u8c03\u6574\u91c7\u6837\u7b56\u7565\uff0c\u5e76\u8bbe\u8ba1\u9ad8\u9891\u52a0\u6743\u65b9\u6cd5\u589e\u5f3a\u7ec6\u8282\u91cd\u5efa\u3002", "result": "\u5b9e\u9a8c\u8868\u660eFreNeuS\u80fd\u91cd\u5efa\u7cbe\u7ec6\u8868\u9762\u7ec6\u8282\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u9002\u7528\u4e8e\u57fa\u4e8eNeuS\u7684\u5de5\u4f5c\u3002", "conclusion": "FreNeuS\u901a\u8fc7\u9ad8\u9891\u4fe1\u606f\u5f15\u5bfc\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8868\u9762\u7ec6\u8282\u91cd\u5efa\u7684\u8d28\u91cf\u548c\u9002\u7528\u6027\u3002"}}
{"id": "2505.03054", "pdf": "https://arxiv.org/pdf/2505.03054", "abs": "https://arxiv.org/abs/2505.03054", "authors": ["Orevaoghene Ahia", "Martijn Bartelds", "Kabir Ahuja", "Hila Gonen", "Valentin Hofmann", "Siddhant Arora", "Shuyue Stella Li", "Vishal Puttagunta", "Mofetoluwa Adeyemi", "Charishma Buchireddy", "Ben Walls", "Noah Bennett", "Shinji Watanabe", "Noah A. Smith", "Yulia Tsvetkov", "Sachin Kumar"], "title": "BLAB: Brutally Long Audio Bench", "categories": ["cs.AI", "cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "Developing large audio language models (LMs) capable of understanding diverse\nspoken interactions is essential for accommodating the multimodal nature of\nhuman communication and can increase the accessibility of language technologies\nacross different user populations. Recent work on audio LMs has primarily\nevaluated their performance on short audio segments, typically under 30\nseconds, with limited exploration of long-form conversational speech segments\nthat more closely reflect natural user interactions with these models. We\nintroduce Brutally Long Audio Bench (BLAB), a challenging long-form audio\nbenchmark that evaluates audio LMs on localization, duration estimation,\nemotion, and counting tasks using audio segments averaging 51 minutes in\nlength. BLAB consists of 833+ hours of diverse, full-length audio clips, each\npaired with human-annotated, text-based natural language questions and answers.\nOur audio data were collected from permissively licensed sources and underwent\na human-assisted filtering process to ensure task compliance. We evaluate six\nopen-source and proprietary audio LMs on BLAB and find that all of them,\nincluding advanced models such as Gemini 2.0 Pro and GPT-4o, struggle with the\ntasks in BLAB. Our comprehensive analysis reveals key insights into the\ntrade-offs between task difficulty and audio duration. In general, we find that\naudio LMs struggle with long-form speech, with performance declining as\nduration increases. They perform poorly on localization, temporal reasoning,\ncounting, and struggle to understand non-phonemic information, relying more on\nprompts than audio content. BLAB serves as a challenging evaluation framework\nto develop audio LMs with robust long-form audio understanding capabilities.", "AI": {"tldr": "BLAB\u662f\u4e00\u4e2a\u9488\u5bf9\u957f\u97f3\u9891\u8bed\u8a00\u6a21\u578b\u7684\u6311\u6218\u6027\u57fa\u51c6\u6d4b\u8bd5\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u6a21\u578b\u5728\u957f\u97f3\u9891\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u4e0d\u4f73\u3002", "motivation": "\u5f00\u53d1\u80fd\u591f\u7406\u89e3\u591a\u6837\u5316\u8bed\u97f3\u4ea4\u4e92\u7684\u5927\u578b\u97f3\u9891\u8bed\u8a00\u6a21\u578b\uff0c\u4ee5\u63d0\u5347\u8bed\u8a00\u6280\u672f\u7684\u53ef\u8bbf\u95ee\u6027\u3002", "method": "\u5f15\u5165BLAB\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b833+\u5c0f\u65f6\u7684\u957f\u97f3\u9891\u7247\u6bb5\u548c\u4eba\u5de5\u6807\u6ce8\u7684\u95ee\u9898\u4e0e\u7b54\u6848\uff0c\u8bc4\u4f30\u6a21\u578b\u5728\u5b9a\u4f4d\u3001\u65f6\u957f\u4f30\u8ba1\u3001\u60c5\u611f\u548c\u8ba1\u6570\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u73b0\u6709\u97f3\u9891\u8bed\u8a00\u6a21\u578b\u5728BLAB\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u6027\u80fd\u968f\u97f3\u9891\u65f6\u957f\u589e\u52a0\u800c\u4e0b\u964d\u3002", "conclusion": "BLAB\u4e3a\u5f00\u53d1\u5177\u6709\u5f3a\u5927\u957f\u97f3\u9891\u7406\u89e3\u80fd\u529b\u7684\u6a21\u578b\u63d0\u4f9b\u4e86\u6311\u6218\u6027\u8bc4\u4f30\u6846\u67b6\u3002"}}
{"id": "2505.03374", "pdf": "https://arxiv.org/pdf/2505.03374", "abs": "https://arxiv.org/abs/2505.03374", "authors": ["Abram Schonfeldt", "Benjamin Maylor", "Xiaofang Chen", "Ronald Clark", "Aiden Doherty"], "title": "Reducing Annotation Burden in Physical Activity Research Using Vision-Language Models", "categories": ["cs.CV"], "comment": null, "summary": "Introduction: Data from wearable devices collected in free-living settings,\nand labelled with physical activity behaviours compatible with health research,\nare essential for both validating existing wearable-based measurement\napproaches and developing novel machine learning approaches. One common way of\nobtaining these labels relies on laborious annotation of sequences of images\ncaptured by cameras worn by participants through the course of a day. Methods:\nWe compare the performance of three vision language models and two\ndiscriminative models on two free-living validation studies with 161 and 111\nparticipants, collected in Oxfordshire, United Kingdom and Sichuan, China,\nrespectively, using the Autographer (OMG Life, defunct) wearable camera.\nResults: We found that the best open-source vision-language model (VLM) and\nfine-tuned discriminative model (DM) achieved comparable performance when\npredicting sedentary behaviour from single images on unseen participants in the\nOxfordshire study; median F1-scores: VLM = 0.89 (0.84, 0.92), DM = 0.91 (0.86,\n0.95). Performance declined for light (VLM = 0.60 (0.56,0.67), DM = 0.70 (0.63,\n0.79)), and moderate-to-vigorous intensity physical activity (VLM = 0.66 (0.53,\n0.85); DM = 0.72 (0.58, 0.84)). When applied to the external Sichuan study,\nperformance fell across all intensity categories, with median Cohen's\nkappa-scores falling from 0.54 (0.49, 0.64) to 0.26 (0.15, 0.37) for the VLM,\nand from 0.67 (0.60, 0.74) to 0.19 (0.10, 0.30) for the DM. Conclusion: Freely\navailable computer vision models could help annotate sedentary behaviour,\ntypically the most prevalent activity of daily living, from wearable camera\nimages within similar populations to seen data, reducing the annotation burden.", "AI": {"tldr": "\u8bba\u6587\u6bd4\u8f83\u4e86\u4e09\u79cd\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u4e24\u79cd\u5224\u522b\u6a21\u578b\u5728\u81ea\u7531\u751f\u6d3b\u573a\u666f\u4e0b\u9884\u6d4b\u8eab\u4f53\u6d3b\u52a8\u884c\u4e3a\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5f00\u6e90\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u9884\u6d4b\u4e45\u5750\u884c\u4e3a\u65f6\u8868\u73b0\u63a5\u8fd1\u5224\u522b\u6a21\u578b\uff0c\u4f46\u5728\u5176\u4ed6\u5f3a\u5ea6\u6d3b\u52a8\u4e0a\u8868\u73b0\u4e0b\u964d\uff0c\u4e14\u8de8\u6570\u636e\u96c6\u6027\u80fd\u663e\u8457\u964d\u4f4e\u3002", "motivation": "\u9a8c\u8bc1\u548c\u5f00\u53d1\u57fa\u4e8e\u53ef\u7a7f\u6234\u8bbe\u5907\u7684\u8eab\u4f53\u6d3b\u52a8\u6d4b\u91cf\u65b9\u6cd5\uff0c\u51cf\u5c11\u4eba\u5de5\u6807\u6ce8\u8d1f\u62c5\u3002", "method": "\u6bd4\u8f83\u4e09\u79cd\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u4e24\u79cd\u5224\u522b\u6a21\u578b\u5728\u4e24\u4e2a\u81ea\u7531\u751f\u6d3b\u9a8c\u8bc1\u7814\u7a76\u4e2d\u7684\u8868\u73b0\uff0c\u4f7f\u7528\u53ef\u7a7f\u6234\u76f8\u673a\u6570\u636e\u3002", "result": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u5224\u522b\u6a21\u578b\u5728\u9884\u6d4b\u4e45\u5750\u884c\u4e3a\u65f6\u8868\u73b0\u63a5\u8fd1\uff0c\u4f46\u5728\u5176\u4ed6\u5f3a\u5ea6\u6d3b\u52a8\u4e0a\u8868\u73b0\u8f83\u5dee\uff0c\u8de8\u6570\u636e\u96c6\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002", "conclusion": "\u5f00\u6e90\u8ba1\u7b97\u673a\u89c6\u89c9\u6a21\u578b\u53ef\u7528\u4e8e\u51cf\u5c11\u76f8\u4f3c\u4eba\u7fa4\u7684\u4e45\u5750\u884c\u4e3a\u6807\u6ce8\u8d1f\u62c5\u3002"}}
{"id": "2505.03273", "pdf": "https://arxiv.org/pdf/2505.03273", "abs": "https://arxiv.org/abs/2505.03273", "authors": ["Zhaoxi Mu", "Xinyu Yang", "Gang Wang"], "title": "SepALM: Audio Language Models Are Error Correctors for Robust Speech Separation", "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": "Appears in IJCAI 2025", "summary": "While contemporary speech separation technologies adeptly process lengthy\nmixed audio waveforms, they are frequently challenged by the intricacies of\nreal-world environments, including noisy and reverberant settings, which can\nresult in artifacts or distortions in the separated speech. To overcome these\nlimitations, we introduce SepALM, a pioneering approach that employs audio\nlanguage models (ALMs) to rectify and re-synthesize speech within the text\ndomain following preliminary separation. SepALM comprises four core components:\na separator, a corrector, a synthesizer, and an aligner. By integrating an\nALM-based end-to-end error correction mechanism, we mitigate the risk of error\naccumulation and circumvent the optimization hurdles typically encountered in\nconventional methods that amalgamate automatic speech recognition (ASR) with\nlarge language models (LLMs). Additionally, we have developed Chain-of-Thought\n(CoT) prompting and knowledge distillation techniques to facilitate the\nreasoning and training processes of the ALM. Our experiments substantiate that\nSepALM not only elevates the precision of speech separation but also markedly\nbolsters adaptability in novel acoustic environments.", "AI": {"tldr": "SepALM\u662f\u4e00\u79cd\u5229\u7528\u97f3\u9891\u8bed\u8a00\u6a21\u578b\uff08ALM\uff09\u5728\u6587\u672c\u57df\u4e2d\u6821\u6b63\u548c\u91cd\u65b0\u5408\u6210\u8bed\u97f3\u7684\u521b\u65b0\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u8bed\u97f3\u5206\u79bb\u6280\u672f\u5728\u566a\u58f0\u548c\u6df7\u54cd\u73af\u5883\u4e2d\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u5b9e\u73af\u5883\u4e2d\u7684\u566a\u58f0\u548c\u6df7\u54cd\u4f1a\u5bfc\u81f4\u8bed\u97f3\u5206\u79bb\u4ea7\u751f\u4f2a\u5f71\u6216\u5931\u771f\uff0c\u4f20\u7edf\u65b9\u6cd5\u7ed3\u5408ASR\u548cLLM\u5b58\u5728\u8bef\u5dee\u7d2f\u79ef\u548c\u4f18\u5316\u96be\u9898\u3002", "method": "SepALM\u5305\u542b\u56db\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u5206\u79bb\u5668\u3001\u6821\u6b63\u5668\u3001\u5408\u6210\u5668\u548c\u5bf9\u9f50\u5668\uff0c\u5e76\u91c7\u7528ALM\u7aef\u5230\u7aef\u7ea0\u9519\u673a\u5236\uff0c\u7ed3\u5408Chain-of-Thought\u63d0\u793a\u548c\u77e5\u8bc6\u84b8\u998f\u6280\u672f\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eSepALM\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u8bed\u97f3\u5206\u79bb\u7684\u7cbe\u5ea6\uff0c\u8fd8\u663e\u8457\u589e\u5f3a\u4e86\u5728\u65b0\u58f0\u5b66\u73af\u5883\u4e2d\u7684\u9002\u5e94\u6027\u3002", "conclusion": "SepALM\u901a\u8fc7ALM\u7684\u6587\u672c\u57df\u5904\u7406\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u590d\u6742\u73af\u5883\u4e0b\u7684\u8bed\u97f3\u5206\u79bb\u63d0\u4f9b\u4e86\u66f4\u4f18\u65b9\u6848\u3002"}}
{"id": "2505.03380", "pdf": "https://arxiv.org/pdf/2505.03380", "abs": "https://arxiv.org/abs/2505.03380", "authors": ["Haonan Wang", "Jiaji Mao", "Lehan Wang", "Qixiang Zhang", "Marawan Elbatel", "Yi Qin", "Huijun Hu", "Baoxun Li", "Wenhui Deng", "Weifeng Qin", "Hongrui Li", "Jialin Liang", "Jun Shen", "Xiaomeng Li"], "title": "Reinforced Correlation Between Vision and Language for Precise Medical AI Assistant", "categories": ["cs.CV", "cs.AI", "eess.IV"], "comment": null, "summary": "Medical AI assistants support doctors in disease diagnosis, medical image\nanalysis, and report generation. However, they still face significant\nchallenges in clinical use, including limited accuracy with multimodal content\nand insufficient validation in real-world settings. We propose RCMed, a\nfull-stack AI assistant that improves multimodal alignment in both input and\noutput, enabling precise anatomical delineation, accurate localization, and\nreliable diagnosis through hierarchical vision-language grounding. A\nself-reinforcing correlation mechanism allows visual features to inform\nlanguage context, while language semantics guide pixel-wise attention, forming\na closed loop that refines both modalities. This correlation is enhanced by a\ncolor region description strategy, translating anatomical structures into\nsemantically rich text to learn shape-location-text relationships across\nscales. Trained on 20 million image-mask-description triplets, RCMed achieves\nstate-of-the-art precision in contextualizing irregular lesions and subtle\nanatomical boundaries, excelling in 165 clinical tasks across 9 modalities. It\nachieved a 23.5% relative improvement in cell segmentation from microscopy\nimages over prior methods. RCMed's strong vision-language alignment enables\nexceptional generalization, with state-of-the-art performance in external\nvalidation across 20 clinically significant cancer types, including novel\ntasks. This work demonstrates how integrated multimodal models capture\nfine-grained patterns, enabling human-level interpretation in complex scenarios\nand advancing human-centric AI healthcare.", "AI": {"tldr": "RCMed\u662f\u4e00\u79cd\u5168\u6808AI\u52a9\u624b\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u5bf9\u9f50\u548c\u5206\u5c42\u89c6\u89c9-\u8bed\u8a00\u57fa\u7840\uff0c\u63d0\u5347\u533b\u5b66\u56fe\u50cf\u5206\u6790\u548c\u8bca\u65ad\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u533b\u5b66AI\u52a9\u624b\u5728\u4e34\u5e8a\u5e94\u7528\u4e2d\u9762\u4e34\u591a\u6a21\u6001\u5185\u5bb9\u51c6\u786e\u6027\u4e0d\u8db3\u548c\u771f\u5b9e\u573a\u666f\u9a8c\u8bc1\u4e0d\u8db3\u7684\u6311\u6218\u3002", "method": "\u91c7\u7528\u81ea\u589e\u5f3a\u76f8\u5173\u673a\u5236\u548c\u989c\u8272\u533a\u57df\u63cf\u8ff0\u7b56\u7565\uff0c\u901a\u8fc7\u89c6\u89c9\u7279\u5f81\u4e0e\u8bed\u8a00\u8bed\u4e49\u7684\u95ed\u73af\u4ea4\u4e92\uff0c\u5b66\u4e60\u5f62\u72b6-\u4f4d\u7f6e-\u6587\u672c\u5173\u7cfb\u3002", "result": "\u5728165\u4e2a\u4e34\u5e8a\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u7ec6\u80de\u5206\u5272\u7cbe\u5ea6\u76f8\u5bf9\u63d0\u534723.5%\uff0c\u5e76\u572820\u79cd\u764c\u75c7\u7c7b\u578b\u7684\u5916\u90e8\u9a8c\u8bc1\u4e2d\u8fbe\u5230\u6700\u4f18\u6027\u80fd\u3002", "conclusion": "RCMed\u5c55\u793a\u4e86\u591a\u6a21\u6001\u6a21\u578b\u5728\u590d\u6742\u573a\u666f\u4e2d\u5b9e\u73b0\u4eba\u7c7b\u6c34\u5e73\u89e3\u91ca\u7684\u6f5c\u529b\uff0c\u63a8\u52a8\u4e86\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684AI\u533b\u7597\u53d1\u5c55\u3002"}}
{"id": "2505.03335", "pdf": "https://arxiv.org/pdf/2505.03335", "abs": "https://arxiv.org/abs/2505.03335", "authors": ["Andrew Zhao", "Yiran Wu", "Yang Yue", "Tong Wu", "Quentin Xu", "Yang Yue", "Matthieu Lin", "Shenzhi Wang", "Qingyun Wu", "Zilong Zheng", "Gao Huang"], "title": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Reinforcement learning with verifiable rewards (RLVR) has shown promise in\nenhancing the reasoning capabilities of large language models by learning\ndirectly from outcome-based rewards. Recent RLVR works that operate under the\nzero setting avoid supervision in labeling the reasoning process, but still\ndepend on manually curated collections of questions and answers for training.\nThe scarcity of high-quality, human-produced examples raises concerns about the\nlong-term scalability of relying on human supervision, a challenge already\nevident in the domain of language model pretraining. Furthermore, in a\nhypothetical future where AI surpasses human intelligence, tasks provided by\nhumans may offer limited learning potential for a superintelligent system. To\naddress these concerns, we propose a new RLVR paradigm called Absolute Zero, in\nwhich a single model learns to propose tasks that maximize its own learning\nprogress and improves reasoning by solving them, without relying on any\nexternal data. Under this paradigm, we introduce the Absolute Zero Reasoner\n(AZR), a system that self-evolves its training curriculum and reasoning ability\nby using a code executor to both validate proposed code reasoning tasks and\nverify answers, serving as an unified source of verifiable reward to guide\nopen-ended yet grounded learning. Despite being trained entirely without\nexternal data, AZR achieves overall SOTA performance on coding and mathematical\nreasoning tasks, outperforming existing zero-setting models that rely on tens\nof thousands of in-domain human-curated examples. Furthermore, we demonstrate\nthat AZR can be effectively applied across different model scales and is\ncompatible with various model classes.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAbsolute Zero\u7684\u65b0RLVR\u8303\u5f0f\uff0c\u901a\u8fc7\u6a21\u578b\u81ea\u4e3b\u751f\u6210\u4efb\u52a1\u5e76\u9a8c\u8bc1\u7b54\u6848\uff0c\u65e0\u9700\u5916\u90e8\u6570\u636e\uff0c\u5b9e\u73b0\u4e86\u5728\u7f16\u7801\u548c\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u7684SOTA\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709RLVR\u65b9\u6cd5\u4f9d\u8d56\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u7684\u95ee\u9898\uff0c\u4ee5\u53ca\u5728AI\u8d85\u8d8a\u4eba\u7c7b\u667a\u80fd\u540e\u4eba\u7c7b\u4efb\u52a1\u53ef\u80fd\u9650\u5236\u5b66\u4e60\u6f5c\u529b\u7684\u6311\u6218\u3002", "method": "\u63d0\u51faAbsolute Zero\u8303\u5f0f\uff0c\u5f15\u5165Absolute Zero Reasoner (AZR)\uff0c\u901a\u8fc7\u4ee3\u7801\u6267\u884c\u5668\u81ea\u4e3b\u751f\u6210\u548c\u9a8c\u8bc1\u4efb\u52a1\uff0c\u5b9e\u73b0\u81ea\u6211\u8fdb\u5316\u3002", "result": "AZR\u5728\u7f16\u7801\u548c\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u4e8e\u4f9d\u8d56\u5927\u91cf\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u7684\u73b0\u6709\u6a21\u578b\u3002", "conclusion": "Absolute Zero\u8303\u5f0f\u5c55\u793a\u4e86\u65e0\u9700\u5916\u90e8\u6570\u636e\u7684\u81ea\u6211\u8fdb\u5316\u6f5c\u529b\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u6a21\u578b\u89c4\u6a21\u548c\u7c7b\u578b\u3002"}}
{"id": "2505.03383", "pdf": "https://arxiv.org/pdf/2505.03383", "abs": "https://arxiv.org/abs/2505.03383", "authors": ["Jian-Wei Li", "Wen-Ze Shao"], "title": "Attention-aggregated Attack for Boosting the Transferability of Facial Adversarial Examples", "categories": ["cs.CV"], "comment": null, "summary": "Adversarial examples have revealed the vulnerability of deep learning models\nand raised serious concerns about information security. The transfer-based\nattack is a hot topic in black-box attacks that are practical to real-world\nscenarios where the training datasets, parameters, and structure of the target\nmodel are unknown to the attacker. However, few methods consider the\nparticularity of class-specific deep models for fine-grained vision tasks, such\nas face recognition (FR), giving rise to unsatisfactory attacking performance.\nIn this work, we first investigate what in a face exactly contributes to the\nembedding learning of FR models and find that both decisive and auxiliary\nfacial features are specific to each FR model, which is quite different from\nthe biological mechanism of human visual system. Accordingly we then propose a\nnovel attack method named Attention-aggregated Attack (AAA) to enhance the\ntransferability of adversarial examples against FR, which is inspired by the\nattention divergence and aims to destroy the facial features that are critical\nfor the decision-making of other FR models by imitating their attentions on the\nclean face images. Extensive experiments conducted on various FR models\nvalidate the superiority and robust effectiveness of the proposed method over\nexisting methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u6ce8\u610f\u529b\u805a\u5408\u653b\u51fb\uff08AAA\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u6a21\u4eff\u5176\u4ed6FR\u6a21\u578b\u5bf9\u5e72\u51c0\u4eba\u8138\u56fe\u50cf\u7684\u6ce8\u610f\u529b\uff0c\u589e\u5f3a\u5bf9\u6297\u6837\u672c\u7684\u53ef\u8fc1\u79fb\u6027\uff0c\u4ece\u800c\u63d0\u5347\u5bf9\u4eba\u8138\u8bc6\u522b\uff08FR\uff09\u6a21\u578b\u7684\u653b\u51fb\u6548\u679c\u3002", "motivation": "\u5bf9\u6297\u6837\u672c\u63ed\u793a\u4e86\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u8106\u5f31\u6027\uff0c\u4f46\u5728\u7ec6\u7c92\u5ea6\u89c6\u89c9\u4efb\u52a1\uff08\u5982\u4eba\u8138\u8bc6\u522b\uff09\u4e2d\uff0c\u73b0\u6709\u65b9\u6cd5\u672a\u5145\u5206\u8003\u8651\u6a21\u578b\u7279\u5f02\u6027\uff0c\u5bfc\u81f4\u653b\u51fb\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u7814\u7a76FR\u6a21\u578b\u4e2d\u54ea\u4e9b\u9762\u90e8\u7279\u5f81\u5bf9\u5d4c\u5165\u5b66\u4e60\u6709\u8d21\u732e\uff0c\u63d0\u51faAAA\u65b9\u6cd5\uff0c\u901a\u8fc7\u6a21\u4eff\u5176\u4ed6FR\u6a21\u578b\u7684\u6ce8\u610f\u529b\u6765\u7834\u574f\u5173\u952e\u9762\u90e8\u7279\u5f81\u3002", "result": "\u5728\u591a\u79cdFR\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86AAA\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "AAA\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u5bf9\u6297\u6837\u672c\u7684\u53ef\u8fc1\u79fb\u6027\uff0c\u4e3aFR\u6a21\u578b\u7684\u653b\u51fb\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.03414", "pdf": "https://arxiv.org/pdf/2505.03414", "abs": "https://arxiv.org/abs/2505.03414", "authors": ["Fangming Cui", "Yonggang Zhang", "Xuan Wang", "Xinmei Tian", "Jun Yu"], "title": "Enhancing Target-unspecific Tasks through a Features Matrix", "categories": ["cs.CV", "cs.CL"], "comment": "ICML 2025", "summary": "Recent developments in prompt learning of large vision-language models have\nsignificantly improved performance in target-specific tasks. However, these\nprompt optimizing methods often struggle to tackle the target-unspecific or\ngeneralizable tasks effectively. It may be attributed to the fact that\noverfitting training causes the model to forget its general knowledge having\nstrong promotion on target-unspecific tasks. To alleviate this issue, we\npropose a novel Features Matrix (FM) regularization approach designed to\nenhance these models on target-unspecific tasks. Our method extracts and\nleverages general knowledge, shaping a Features Matrix (FM). Specifically, the\nFM captures the semantics of diverse inputs from a deep and fine perspective,\npreserving essential general knowledge, which mitigates the risk of\noverfitting. Representative evaluations demonstrate that: 1) the FM is\ncompatible with existing frameworks as a generic and flexible module, and 2)\nthe FM significantly showcases its effectiveness in enhancing target-unspecific\ntasks, achieving state-of-the-art performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7279\u5f81\u77e9\u9635\uff08FM\uff09\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u589e\u5f3a\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u76ee\u6807\u975e\u7279\u5b9a\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u907f\u514d\u8fc7\u62df\u5408\u3002", "motivation": "\u73b0\u6709\u63d0\u793a\u5b66\u4e60\u65b9\u6cd5\u5728\u76ee\u6807\u975e\u7279\u5b9a\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u53ef\u80fd\u56e0\u8fc7\u62df\u5408\u5bfc\u81f4\u6a21\u578b\u9057\u5fd8\u901a\u7528\u77e5\u8bc6\u3002", "method": "\u901a\u8fc7\u63d0\u53d6\u548c\u5229\u7528\u901a\u7528\u77e5\u8bc6\u6784\u5efa\u7279\u5f81\u77e9\u9635\uff08FM\uff09\uff0c\u4ece\u6df1\u5ea6\u548c\u7ec6\u7c92\u5ea6\u89d2\u5ea6\u6355\u6349\u8f93\u5165\u8bed\u4e49\uff0c\u4fdd\u7559\u901a\u7528\u77e5\u8bc6\u3002", "result": "FM\u517c\u5bb9\u73b0\u6709\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u76ee\u6807\u975e\u7279\u5b9a\u4efb\u52a1\u6027\u80fd\uff0c\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "FM\u662f\u4e00\u79cd\u901a\u7528\u4e14\u7075\u6d3b\u7684\u6a21\u5757\uff0c\u80fd\u6709\u6548\u7f13\u89e3\u8fc7\u62df\u5408\u95ee\u9898\uff0c\u63d0\u5347\u6a21\u578b\u901a\u7528\u6027\u3002"}}
{"id": "2505.03394", "pdf": "https://arxiv.org/pdf/2505.03394", "abs": "https://arxiv.org/abs/2505.03394", "authors": ["Sarthak Mehrotra", "Rishabh Jain", "Mayur Hemani", "Balaji Krishnamurthy", "Mausoom Sarkar"], "title": "EOPose : Exemplar-based object reposing using Generalized Pose Correspondences", "categories": ["cs.CV"], "comment": "Accepted in CVPR 2025 AI4CC workshop", "summary": "Reposing objects in images has a myriad of applications, especially for\ne-commerce where several variants of product images need to be produced\nquickly. In this work, we leverage the recent advances in unsupervised keypoint\ncorrespondence detection between different object images of the same class to\npropose an end-to-end framework for generic object reposing. Our method,\nEOPose, takes a target pose-guidance image as input and uses its keypoint\ncorrespondence with the source object image to warp and re-render the latter\ninto the target pose using a novel three-step approach. Unlike generative\napproaches, our method also preserves the fine-grained details of the object\nsuch as its exact colors, textures, and brand marks. We also prepare a new\ndataset of paired objects based on the Objaverse dataset to train and test our\nnetwork. EOPose produces high-quality reposing output as evidenced by different\nimage quality metrics (PSNR, SSIM and FID). Besides a description of the method\nand the dataset, the paper also includes detailed ablation and user studies to\nindicate the efficacy of the proposed method", "AI": {"tldr": "EOPose\u662f\u4e00\u79cd\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u5229\u7528\u65e0\u76d1\u7763\u5173\u952e\u70b9\u68c0\u6d4b\u6280\u672f\u5b9e\u73b0\u7269\u4f53\u5728\u56fe\u50cf\u4e2d\u7684\u91cd\u65b0\u59ff\u6001\u8c03\u6574\uff0c\u9002\u7528\u4e8e\u7535\u5b50\u5546\u52a1\u7b49\u9886\u57df\u3002", "motivation": "\u7535\u5b50\u5546\u52a1\u9700\u8981\u5feb\u901f\u751f\u6210\u591a\u79cd\u4ea7\u54c1\u56fe\u50cf\u53d8\u4f53\uff0c\u73b0\u6709\u751f\u6210\u65b9\u6cd5\u96be\u4ee5\u4fdd\u7559\u7269\u4f53\u7684\u7cbe\u7ec6\u7ec6\u8282\uff08\u5982\u989c\u8272\u3001\u7eb9\u7406\u548c\u54c1\u724c\u6807\u5fd7\uff09\u3002", "method": "EOPose\u91c7\u7528\u4e09\u6b65\u6cd5\uff0c\u901a\u8fc7\u76ee\u6807\u59ff\u6001\u5f15\u5bfc\u56fe\u50cf\u7684\u5173\u952e\u70b9\u5bf9\u5e94\u5173\u7cfb\uff0c\u5bf9\u6e90\u56fe\u50cf\u8fdb\u884c\u53d8\u5f62\u548c\u91cd\u65b0\u6e32\u67d3\u3002", "result": "EOPose\u5728PSNR\u3001SSIM\u548cFID\u7b49\u56fe\u50cf\u8d28\u91cf\u6307\u6807\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u4fdd\u7559\u4e86\u7269\u4f53\u7684\u7cbe\u7ec6\u7ec6\u8282\u3002", "conclusion": "EOPose\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u9ad8\u8d28\u91cf\u7684\u7269\u4f53\u91cd\u65b0\u59ff\u6001\u8c03\u6574\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2505.03443", "pdf": "https://arxiv.org/pdf/2505.03443", "abs": "https://arxiv.org/abs/2505.03443", "authors": ["Valerio Bellandi"], "title": "Elevating Semantic Exploration: A Novel Approach Utilizing Distributed Repositories", "categories": ["cs.DC", "cs.AI", "cs.CL"], "comment": "This paper has been accepted at the 6th International Conference on\n  Recent Trends and Applications in Computer Science. It will appear in the\n  proceedings", "summary": "Centralized and distributed systems are two main approaches to organizing ICT\ninfrastructure, each with its pros and cons. Centralized systems concentrate\nresources in one location, making management easier but creating single points\nof failure. Distributed systems, on the other hand, spread resources across\nmultiple nodes, offering better scalability and fault tolerance, but requiring\nmore complex management. The choice between them depends on factors like\napplication needs, scalability, and data sensitivity. Centralized systems suit\napplications with limited scalability and centralized control, while\ndistributed systems excel in large-scale environments requiring high\navailability and performance. This paper explores a distributed document\nrepository system developed for the Italian Ministry of Justice, using edge\nrepositories to analyze textual data and metadata, enhancing semantic\nexploration capabilities.", "AI": {"tldr": "\u672c\u6587\u6bd4\u8f83\u4e86\u96c6\u4e2d\u5f0f\u548c\u5206\u5e03\u5f0f\u7cfb\u7edf\u7684\u4f18\u7f3a\u70b9\uff0c\u5e76\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u4e3a\u610f\u5927\u5229\u53f8\u6cd5\u90e8\u5f00\u53d1\u7684\u5206\u5e03\u5f0f\u6587\u6863\u5b58\u50a8\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u5229\u7528\u8fb9\u7f18\u5b58\u50a8\u5e93\u589e\u5f3a\u8bed\u4e49\u63a2\u7d22\u80fd\u529b\u3002", "motivation": "\u63a2\u8ba8\u96c6\u4e2d\u5f0f\u4e0e\u5206\u5e03\u5f0f\u7cfb\u7edf\u7684\u9002\u7528\u573a\u666f\uff0c\u5e76\u5f00\u53d1\u4e00\u4e2a\u5206\u5e03\u5f0f\u6587\u6863\u5b58\u50a8\u7cfb\u7edf\u4ee5\u6ee1\u8db3\u5927\u89c4\u6a21\u73af\u5883\u4e2d\u7684\u9ad8\u53ef\u7528\u6027\u548c\u6027\u80fd\u9700\u6c42\u3002", "method": "\u91c7\u7528\u5206\u5e03\u5f0f\u67b6\u6784\uff0c\u5229\u7528\u8fb9\u7f18\u5b58\u50a8\u5e93\u5206\u6790\u6587\u672c\u6570\u636e\u548c\u5143\u6570\u636e\uff0c\u4ee5\u589e\u5f3a\u8bed\u4e49\u63a2\u7d22\u529f\u80fd\u3002", "result": "\u6210\u529f\u5f00\u53d1\u4e86\u4e00\u4e2a\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u73af\u5883\u7684\u5206\u5e03\u5f0f\u6587\u6863\u5b58\u50a8\u7cfb\u7edf\uff0c\u63d0\u9ad8\u4e86\u8bed\u4e49\u63a2\u7d22\u80fd\u529b\u3002", "conclusion": "\u5206\u5e03\u5f0f\u7cfb\u7edf\u5728\u9700\u8981\u9ad8\u53ef\u7528\u6027\u548c\u6027\u80fd\u7684\u5927\u89c4\u6a21\u73af\u5883\u4e2d\u66f4\u5177\u4f18\u52bf\uff0c\u800c\u96c6\u4e2d\u5f0f\u7cfb\u7edf\u9002\u7528\u4e8e\u6709\u9650\u6269\u5c55\u6027\u548c\u96c6\u4e2d\u63a7\u5236\u7684\u5e94\u7528\u3002"}}
{"id": "2505.03401", "pdf": "https://arxiv.org/pdf/2505.03401", "abs": "https://arxiv.org/abs/2505.03401", "authors": ["Shanshan Song", "Hui Tang", "Honglong Yang", "Xiaomeng Li"], "title": "DDaTR: Dynamic Difference-aware Temporal Residual Network for Longitudinal Radiology Report Generation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Radiology Report Generation (RRG) automates the creation of radiology reports\nfrom medical imaging, enhancing the efficiency of the reporting process.\nLongitudinal Radiology Report Generation (LRRG) extends RRG by incorporating\nthe ability to compare current and prior exams, facilitating the tracking of\ntemporal changes in clinical findings. Existing LRRG approaches only extract\nfeatures from prior and current images using a visual pre-trained encoder,\nwhich are then concatenated to generate the final report. However, these\nmethods struggle to effectively capture both spatial and temporal correlations\nduring the feature extraction process. Consequently, the extracted features\ninadequately capture the information of difference across exams and thus\nunderrepresent the expected progressions, leading to sub-optimal performance in\nLRRG. To address this, we develop a novel dynamic difference-aware temporal\nresidual network (DDaTR). In DDaTR, we introduce two modules at each stage of\nthe visual encoder to capture multi-level spatial correlations. The Dynamic\nFeature Alignment Module (DFAM) is designed to align prior features across\nmodalities for the integrity of prior clinical information. Prompted by the\nenriched prior features, the dynamic difference-aware module (DDAM) captures\nfavorable difference information by identifying relationships across exams.\nFurthermore, our DDaTR employs the dynamic residual network to unidirectionally\ntransmit longitudinal information, effectively modelling temporal correlations.\nExtensive experiments demonstrated superior performance over existing methods\non three benchmarks, proving its efficacy in both RRG and LRRG tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u5dee\u5f02\u611f\u77e5\u65f6\u95f4\u6b8b\u5dee\u7f51\u7edc\uff08DDaTR\uff09\uff0c\u7528\u4e8e\u63d0\u5347\u7eb5\u5411\u653e\u5c04\u5b66\u62a5\u544a\u751f\u6210\uff08LRRG\uff09\u7684\u6027\u80fd\uff0c\u901a\u8fc7\u6355\u6349\u591a\u7ea7\u7a7a\u95f4\u548c\u65f6\u95f4\u76f8\u5173\u6027\u3002", "motivation": "\u73b0\u6709LRRG\u65b9\u6cd5\u5728\u7279\u5f81\u63d0\u53d6\u8fc7\u7a0b\u4e2d\u672a\u80fd\u6709\u6548\u6355\u6349\u7a7a\u95f4\u548c\u65f6\u95f4\u76f8\u5173\u6027\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0d\u4f73\u3002", "method": "DDaTR\u5305\u542b\u52a8\u6001\u7279\u5f81\u5bf9\u9f50\u6a21\u5757\uff08DFAM\uff09\u548c\u52a8\u6001\u5dee\u5f02\u611f\u77e5\u6a21\u5757\uff08DDAM\uff09\uff0c\u5206\u522b\u7528\u4e8e\u5bf9\u9f50\u5148\u9a8c\u7279\u5f81\u548c\u6355\u6349\u5dee\u5f02\u4fe1\u606f\uff0c\u5e76\u901a\u8fc7\u52a8\u6001\u6b8b\u5dee\u7f51\u7edc\u5efa\u6a21\u65f6\u95f4\u76f8\u5173\u6027\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDDaTR\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u5176\u5728RRG\u548cLRRG\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "DDaTR\u901a\u8fc7\u6539\u8fdb\u7279\u5f81\u63d0\u53d6\u548c\u65f6\u95f4\u5efa\u6a21\uff0c\u663e\u8457\u63d0\u5347\u4e86LRRG\u7684\u6027\u80fd\u3002"}}
{"id": "2505.03501", "pdf": "https://arxiv.org/pdf/2505.03501", "abs": "https://arxiv.org/abs/2505.03501", "authors": ["Zihan Wang", "Hongwei Li", "Rui Zhang", "Wenbo Jiang", "Kangjie Chen", "Tianwei Zhang", "Qingchuan Zhao", "Guowen Xu"], "title": "BadLingual: A Novel Lingual-Backdoor Attack against Large Language Models", "categories": ["cs.CR", "cs.CL"], "comment": null, "summary": "In this paper, we present a new form of backdoor attack against Large\nLanguage Models (LLMs): lingual-backdoor attacks. The key novelty of\nlingual-backdoor attacks is that the language itself serves as the trigger to\nhijack the infected LLMs to generate inflammatory speech. They enable the\nprecise targeting of a specific language-speaking group, exacerbating racial\ndiscrimination by malicious entities. We first implement a baseline\nlingual-backdoor attack, which is carried out by poisoning a set of training\ndata for specific downstream tasks through translation into the trigger\nlanguage. However, this baseline attack suffers from poor task generalization\nand is impractical in real-world settings. To address this challenge, we design\nBadLingual, a novel task-agnostic lingual-backdoor, capable of triggering any\ndownstream tasks within the chat LLMs, regardless of the specific questions of\nthese tasks. We design a new approach using PPL-constrained Greedy Coordinate\nGradient-based Search (PGCG) based adversarial training to expand the decision\nboundary of lingual-backdoor, thereby enhancing the generalization ability of\nlingual-backdoor across various tasks. We perform extensive experiments to\nvalidate the effectiveness of our proposed attacks. Specifically, the baseline\nattack achieves an ASR of over 90% on the specified tasks. However, its ASR\nreaches only 37.61% across six tasks in the task-agnostic scenario. In\ncontrast, BadLingual brings up to 37.35% improvement over the baseline. Our\nstudy sheds light on a new perspective of vulnerabilities in LLMs with\nmultilingual capabilities and is expected to promote future research on the\npotential defenses to enhance the LLMs' robustness", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u65b0\u578b\u540e\u95e8\u653b\u51fb\u2014\u2014\u8bed\u8a00\u540e\u95e8\u653b\u51fb\uff08lingual-backdoor attacks\uff09\uff0c\u5229\u7528\u8bed\u8a00\u672c\u8eab\u4f5c\u4e3a\u89e6\u53d1\u5668\uff0c\u8bf1\u5bfc\u6a21\u578b\u751f\u6210\u717d\u52a8\u6027\u8a00\u8bba\u3002\u901a\u8fc7\u6539\u8fdb\u4efb\u52a1\u65e0\u5173\u7684\u653b\u51fb\u65b9\u6cd5\uff08BadLingual\uff09\uff0c\u663e\u8457\u63d0\u5347\u4e86\u653b\u51fb\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u63ed\u793a\u591a\u8bed\u8a00LLMs\u7684\u6f5c\u5728\u6f0f\u6d1e\uff0c\u5c24\u5176\u662f\u8bed\u8a00\u4f5c\u4e3a\u540e\u95e8\u89e6\u53d1\u5668\u7684\u653b\u51fb\u65b9\u5f0f\uff0c\u4ee5\u4fc3\u8fdb\u672a\u6765\u9632\u5fa1\u7814\u7a76\u3002", "method": "1. \u57fa\u7ebf\u653b\u51fb\uff1a\u901a\u8fc7\u7ffb\u8bd1\u89e6\u53d1\u8bed\u8a00\u6c61\u67d3\u8bad\u7ec3\u6570\u636e\uff1b2. BadLingual\uff1a\u57fa\u4e8ePGCG\u5bf9\u6297\u8bad\u7ec3\u7684\u4efb\u52a1\u65e0\u5173\u653b\u51fb\u65b9\u6cd5\u3002", "result": "\u57fa\u7ebf\u653b\u51fb\u5728\u7279\u5b9a\u4efb\u52a1\u4e0aASR\u8fbe90%\uff0c\u4f46\u5728\u4efb\u52a1\u65e0\u5173\u573a\u666f\u4e2d\u4ec537.61%\uff1bBadLingual\u5c06ASR\u63d0\u534737.35%\u3002", "conclusion": "\u8bed\u8a00\u540e\u95e8\u653b\u51fb\u66b4\u9732\u4e86LLMs\u7684\u591a\u8bed\u8a00\u80fd\u529b\u6f0f\u6d1e\uff0c\u9700\u52a0\u5f3a\u9632\u5fa1\u7814\u7a76\u4ee5\u63d0\u5347\u6a21\u578b\u9c81\u68d2\u6027\u3002"}}
{"id": "2505.03412", "pdf": "https://arxiv.org/pdf/2505.03412", "abs": "https://arxiv.org/abs/2505.03412", "authors": ["Haoyu Bai", "Jie Wang", "Gaomin Li", "Xuan Li", "Xiaohu Zhang", "Xia Yang"], "title": "CXR-AD: Component X-ray Image Dataset for Industrial Anomaly Detection", "categories": ["cs.CV"], "comment": null, "summary": "Internal defect detection constitutes a critical process in ensuring\ncomponent quality, for which anomaly detection serves as an effective solution.\nHowever, existing anomaly detection datasets predominantly focus on surface\ndefects in visible-light images, lacking publicly available X-ray datasets\ntargeting internal defects in components. To address this gap, we construct the\nfirst publicly accessible component X-ray anomaly detection (CXR-AD) dataset,\ncomprising real-world X-ray images. The dataset covers five industrial\ncomponent categories, including 653 normal samples and 561 defect samples with\nprecise pixel-level mask annotations. We systematically analyze the dataset\ncharacteristics and identify three major technical challenges: (1) strong\ncoupling between complex internal structures and defect regions, (2) inherent\nlow contrast and high noise interference in X-ray imaging, and (3) significant\nvariations in defect scales and morphologies. To evaluate dataset complexity,\nwe benchmark three state-of-the-art anomaly detection frameworks\n(feature-based, reconstruction-based, and zero-shot learning methods).\nExperimental results demonstrate a 29.78% average performance degradation on\nCXR-AD compared to MVTec AD, highlighting the limitations of current algorithms\nin handling internal defect detection tasks. To the best of our knowledge,\nCXR-AD represents the first publicly available X-ray dataset for component\nanomaly detection, providing a real-world industrial benchmark to advance\nalgorithm development and enhance precision in internal defect inspection\ntechnologies.", "AI": {"tldr": "\u8bba\u6587\u6784\u5efa\u4e86\u9996\u4e2a\u516c\u5f00\u7684X\u5c04\u7ebf\u7ec4\u4ef6\u5f02\u5e38\u68c0\u6d4b\u6570\u636e\u96c6CXR-AD\uff0c\u586b\u8865\u4e86\u5185\u90e8\u7f3a\u9677\u68c0\u6d4b\u6570\u636e\u96c6\u7684\u7a7a\u767d\uff0c\u5e76\u5206\u6790\u4e86\u5176\u6280\u672f\u6311\u6218\u548c\u73b0\u6709\u7b97\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u5f02\u5e38\u68c0\u6d4b\u6570\u636e\u96c6\u4e3b\u8981\u5173\u6ce8\u53ef\u89c1\u5149\u56fe\u50cf\u4e2d\u7684\u8868\u9762\u7f3a\u9677\uff0c\u7f3a\u4e4f\u9488\u5bf9\u7ec4\u4ef6\u5185\u90e8\u7f3a\u9677\u7684\u516c\u5f00X\u5c04\u7ebf\u6570\u636e\u96c6\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b653\u4e2a\u6b63\u5e38\u6837\u672c\u548c561\u4e2a\u7f3a\u9677\u6837\u672c\u7684CXR-AD\u6570\u636e\u96c6\uff0c\u5e76\u5206\u6790\u4e86\u5176\u7279\u6027\u53ca\u4e09\u5927\u6280\u672f\u6311\u6218\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u73b0\u6709\u7b97\u6cd5\u5728CXR-AD\u4e0a\u7684\u6027\u80fd\u5e73\u5747\u4e0b\u964d29.78%\uff0c\u63ed\u793a\u4e86\u5176\u5728\u5185\u90e8\u7f3a\u9677\u68c0\u6d4b\u4efb\u52a1\u4e2d\u7684\u5c40\u9650\u6027\u3002", "conclusion": "CXR-AD\u4e3a\u5185\u90e8\u7f3a\u9677\u68c0\u6d4b\u63d0\u4f9b\u4e86\u9996\u4e2a\u516c\u5f00\u57fa\u51c6\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u7b97\u6cd5\u5f00\u53d1\u548c\u63d0\u5347\u68c0\u6d4b\u6280\u672f\u7cbe\u5ea6\u3002"}}
{"id": "2505.03676", "pdf": "https://arxiv.org/pdf/2505.03676", "abs": "https://arxiv.org/abs/2505.03676", "authors": ["Arthur Satouf", "Gabriel Ben Zenou", "Benjamin Piwowarski", "Habiboulaye Amadou Boubacar", "Pablo Piantanida"], "title": "Rational Retrieval Acts: Leveraging Pragmatic Reasoning to Improve Sparse Retrieval", "categories": ["cs.IR", "cs.CL", "68P20, 68T50", "H.3"], "comment": "6 pages - 2 figures - conference: accepted at SIGIR 2025", "summary": "Current sparse neural information retrieval (IR) methods, and to a lesser\nextent more traditional models such as BM25, do not take into account the\ndocument collection and the complex interplay between different term weights\nwhen representing a single document. In this paper, we show how the Rational\nSpeech Acts (RSA), a linguistics framework used to minimize the number of\nfeatures to be communicated when identifying an object in a set, can be adapted\nto the IR case -- and in particular to the high number of potential features\n(here, tokens). RSA dynamically modulates token-document interactions by\nconsidering the influence of other documents in the dataset, better contrasting\ndocument representations. Experiments show that incorporating RSA consistently\nimproves multiple sparse retrieval models and achieves state-of-the-art\nperformance on out-of-domain datasets from the BEIR benchmark.\nhttps://github.com/arthur-75/Rational-Retrieval-Acts", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7406\u6027\u8a00\u8bed\u884c\u4e3a\uff08RSA\uff09\u6846\u67b6\u7684\u7a00\u758f\u795e\u7ecf\u4fe1\u606f\u68c0\u7d22\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u8bcd\u9879\u4e0e\u6587\u6863\u7684\u4ea4\u4e92\uff0c\u63d0\u5347\u68c0\u7d22\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7a00\u758f\u795e\u7ecf\u4fe1\u606f\u68c0\u7d22\u65b9\u6cd5\u53ca\u4f20\u7edf\u6a21\u578b\uff08\u5982BM25\uff09\u672a\u5145\u5206\u8003\u8651\u6587\u6863\u96c6\u5408\u53ca\u8bcd\u9879\u6743\u91cd\u95f4\u7684\u590d\u6742\u5173\u7cfb\u3002", "method": "\u5c06RSA\u6846\u67b6\u5e94\u7528\u4e8e\u4fe1\u606f\u68c0\u7d22\uff0c\u52a8\u6001\u8c03\u6574\u8bcd\u9879\u4e0e\u6587\u6863\u7684\u4ea4\u4e92\uff0c\u8003\u8651\u6570\u636e\u96c6\u4e2d\u6587\u6863\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cRSA\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u591a\u79cd\u7a00\u758f\u68c0\u7d22\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5e76\u5728BEIR\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u4f18\u8868\u73b0\u3002", "conclusion": "RSA\u6846\u67b6\u80fd\u6709\u6548\u6539\u8fdb\u7a00\u758f\u68c0\u7d22\u6a21\u578b\uff0c\u63d0\u5347\u8de8\u9886\u57df\u6570\u636e\u96c6\u4e0a\u7684\u68c0\u7d22\u6027\u80fd\u3002"}}
{"id": "2505.03422", "pdf": "https://arxiv.org/pdf/2505.03422", "abs": "https://arxiv.org/abs/2505.03422", "authors": ["Yepeng Liu", "Wenpeng Lai", "Zhou Zhao", "Yuxuan Xiong", "Jinchi Zhu", "Jun Cheng", "Yongchao Xu"], "title": "LiftFeat: 3D Geometry-Aware Local Feature Matching", "categories": ["cs.CV", "cs.RO"], "comment": "Accepted at ICRA 2025", "summary": "Robust and efficient local feature matching plays a crucial role in\napplications such as SLAM and visual localization for robotics. Despite great\nprogress, it is still very challenging to extract robust and discriminative\nvisual features in scenarios with drastic lighting changes, low texture areas,\nor repetitive patterns. In this paper, we propose a new lightweight network\ncalled \\textit{LiftFeat}, which lifts the robustness of raw descriptor by\naggregating 3D geometric feature. Specifically, we first adopt a pre-trained\nmonocular depth estimation model to generate pseudo surface normal label,\nsupervising the extraction of 3D geometric feature in terms of predicted\nsurface normal. We then design a 3D geometry-aware feature lifting module to\nfuse surface normal feature with raw 2D descriptor feature. Integrating such 3D\ngeometric feature enhances the discriminative ability of 2D feature description\nin extreme conditions. Extensive experimental results on relative pose\nestimation, homography estimation, and visual localization tasks, demonstrate\nthat our LiftFeat outperforms some lightweight state-of-the-art methods. Code\nwill be released at : https://github.com/lyp-deeplearning/LiftFeat.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLiftFeat\u7684\u8f7b\u91cf\u7ea7\u7f51\u7edc\uff0c\u901a\u8fc7\u805a\u54083D\u51e0\u4f55\u7279\u5f81\u63d0\u5347\u539f\u59cb\u63cf\u8ff0\u7b26\u7684\u9c81\u68d2\u6027\uff0c\u9002\u7528\u4e8eSLAM\u548c\u89c6\u89c9\u5b9a\u4f4d\u7b49\u4efb\u52a1\u3002", "motivation": "\u5728\u5149\u7167\u53d8\u5316\u5267\u70c8\u3001\u4f4e\u7eb9\u7406\u533a\u57df\u6216\u91cd\u590d\u6a21\u5f0f\u573a\u666f\u4e2d\uff0c\u63d0\u53d6\u9c81\u68d2\u4e14\u5177\u6709\u533a\u5206\u6027\u7684\u89c6\u89c9\u7279\u5f81\u4ecd\u5177\u6311\u6218\u6027\u3002", "method": "\u91c7\u7528\u9884\u8bad\u7ec3\u7684\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u6a21\u578b\u751f\u6210\u4f2a\u8868\u9762\u6cd5\u7ebf\u6807\u7b7e\uff0c\u76d1\u77633D\u51e0\u4f55\u7279\u5f81\u7684\u63d0\u53d6\uff0c\u5e76\u8bbe\u8ba13D\u51e0\u4f55\u611f\u77e5\u7684\u7279\u5f81\u63d0\u5347\u6a21\u5757\u878d\u5408\u8868\u9762\u6cd5\u7ebf\u7279\u5f81\u4e0e\u539f\u59cb2D\u63cf\u8ff0\u7b26\u7279\u5f81\u3002", "result": "\u5728\u76f8\u5bf9\u4f4d\u59ff\u4f30\u8ba1\u3001\u5355\u5e94\u6027\u4f30\u8ba1\u548c\u89c6\u89c9\u5b9a\u4f4d\u4efb\u52a1\u4e2d\uff0cLiftFeat\u4f18\u4e8e\u5176\u4ed6\u8f7b\u91cf\u7ea7\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "LiftFeat\u901a\u8fc7\u5f15\u51653D\u51e0\u4f55\u7279\u5f81\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6781\u7aef\u6761\u4ef6\u4e0b\u7684\u7279\u5f81\u63cf\u8ff0\u80fd\u529b\u3002"}}
{"id": "2505.03426", "pdf": "https://arxiv.org/pdf/2505.03426", "abs": "https://arxiv.org/abs/2505.03426", "authors": ["Ziyu Li", "Yujian Hu", "Zhengyao Ding", "Yiheng Mao", "Haitao Li", "Fan Yi", "Hongkun Zhang", "Zhengxing Huang"], "title": "Phenotype-Guided Generative Model for High-Fidelity Cardiac MRI Synthesis: Advancing Pretraining and Clinical Applications", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Cardiac Magnetic Resonance (CMR) imaging is a vital non-invasive tool for\ndiagnosing heart diseases and evaluating cardiac health. However, the limited\navailability of large-scale, high-quality CMR datasets poses a major challenge\nto the effective application of artificial intelligence (AI) in this domain.\nEven the amount of unlabeled data and the health status it covers are difficult\nto meet the needs of model pretraining, which hinders the performance of AI\nmodels on downstream tasks. In this study, we present Cardiac Phenotype-Guided\nCMR Generation (CPGG), a novel approach for generating diverse CMR data that\ncovers a wide spectrum of cardiac health status. The CPGG framework consists of\ntwo stages: in the first stage, a generative model is trained using cardiac\nphenotypes derived from CMR data; in the second stage, a masked autoregressive\ndiffusion model, conditioned on these phenotypes, generates high-fidelity CMR\ncine sequences that capture both structural and functional features of the\nheart in a fine-grained manner. We synthesized a massive amount of CMR to\nexpand the pretraining data. Experimental results show that CPGG generates\nhigh-quality synthetic CMR data, significantly improving performance on various\ndownstream tasks, including diagnosis and cardiac phenotypes prediction. These\ngains are demonstrated across both public and private datasets, highlighting\nthe effectiveness of our approach. Code is availabel at\nhttps://anonymous.4open.science/r/CPGG.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCPGG\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u591a\u6837\u5316\u7684CMR\u6570\u636e\u6765\u5f25\u8865\u6570\u636e\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86AI\u6a21\u578b\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u7531\u4e8e\u5927\u89c4\u6a21\u9ad8\u8d28\u91cfCMR\u6570\u636e\u7684\u7a00\u7f3a\u6027\uff0c\u9650\u5236\u4e86AI\u5728\u5fc3\u810f\u75be\u75c5\u8bca\u65ad\u4e2d\u7684\u5e94\u7528\uff0cCPGG\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "CPGG\u5206\u4e3a\u4e24\u9636\u6bb5\uff1a\u9996\u5148\u751f\u6210\u6a21\u578b\u57fa\u4e8eCMR\u6570\u636e\u7684\u5fc3\u810f\u8868\u578b\u8bad\u7ec3\uff0c\u968f\u540e\u901a\u8fc7\u63a9\u7801\u81ea\u56de\u5f52\u6269\u6563\u6a21\u578b\u751f\u6210\u9ad8\u8d28\u91cfCMR\u5e8f\u5217\u3002", "result": "\u751f\u6210\u7684\u5408\u6210CMR\u6570\u636e\u8d28\u91cf\u9ad8\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bca\u65ad\u548c\u8868\u578b\u9884\u6d4b\u7b49\u4e0b\u6e38\u4efb\u52a1\u7684\u6027\u80fd\u3002", "conclusion": "CPGG\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86CMR\u6570\u636e\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u4e3aAI\u5728\u5fc3\u810f\u5065\u5eb7\u9886\u57df\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.03431", "pdf": "https://arxiv.org/pdf/2505.03431", "abs": "https://arxiv.org/abs/2505.03431", "authors": ["Usman Muhammad", "Jorma Laaksonen"], "title": "A Fusion-Guided Inception Network for Hyperspectral Image Super-Resolution", "categories": ["cs.CV"], "comment": null, "summary": "The fusion of low-spatial-resolution hyperspectral images (HSIs) with\nhigh-spatial-resolution conventional images (e.g., panchromatic or RGB) has\nplayed a significant role in recent advancements in HSI super-resolution.\nHowever, this fusion process relies on the availability of precise alignment\nbetween image pairs, which is often challenging in real-world scenarios. To\nmitigate this limitation, we propose a single-image super-resolution model\ncalled the Fusion-Guided Inception Network (FGIN). Specifically, we first\nemploy a spectral-spatial fusion module to effectively integrate spectral and\nspatial information at an early stage. Next, an Inception-like hierarchical\nfeature extraction strategy is used to capture multiscale spatial dependencies,\nfollowed by a dedicated multi-scale fusion block. To further enhance\nreconstruction quality, we incorporate an optimized upsampling module that\ncombines bilinear interpolation with depthwise separable convolutions.\nExperimental evaluations on two publicly available hyperspectral datasets\ndemonstrate the competitive performance of our method.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFGIN\u7684\u5355\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u6a21\u578b\uff0c\u901a\u8fc7\u878d\u5408\u6a21\u5757\u548c\u591a\u5c3a\u5ea6\u7279\u5f81\u63d0\u53d6\u63d0\u5347HSI\u8d85\u5206\u8fa8\u7387\u6027\u80fd\u3002", "motivation": "\u73b0\u6709HSI\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u4f9d\u8d56\u7cbe\u786e\u5bf9\u9f50\u7684\u56fe\u50cf\u5bf9\uff0c\u4f46\u5b9e\u9645\u573a\u666f\u4e2d\u96be\u4ee5\u5b9e\u73b0\uff0c\u56e0\u6b64\u63d0\u51fa\u5355\u56fe\u50cf\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u5149\u8c31-\u7a7a\u95f4\u878d\u5408\u6a21\u5757\u3001Inception-like\u591a\u5c3a\u5ea6\u7279\u5f81\u63d0\u53d6\u3001\u591a\u5c3a\u5ea6\u878d\u5408\u5757\u4ee5\u53ca\u4f18\u5316\u7684\u4e0a\u91c7\u6837\u6a21\u5757\u3002", "result": "\u5728\u4e24\u4e2a\u516c\u5f00HSI\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u7ade\u4e89\u529b\u3002", "conclusion": "FGIN\u6a21\u578b\u6709\u6548\u89e3\u51b3\u4e86\u56fe\u50cf\u5bf9\u9f50\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u8d85\u5206\u8fa8\u7387\u6027\u80fd\u3002"}}
{"id": "2505.03435", "pdf": "https://arxiv.org/pdf/2505.03435", "abs": "https://arxiv.org/abs/2505.03435", "authors": ["Sun Haoxuan", "Hong Yan", "Zhan Jiahui", "Chen Haoxing", "Lan Jun", "Zhu Huijia", "Wang Weiqiang", "Zhang Liqing", "Zhang Jianfu"], "title": "Robustness in AI-Generated Detection: Enhancing Resistance to Adversarial Attacks", "categories": ["cs.CV"], "comment": null, "summary": "The rapid advancement of generative image technology has introduced\nsignificant security concerns, particularly in the domain of face generation\ndetection. This paper investigates the vulnerabilities of current AI-generated\nface detection systems. Our study reveals that while existing detection methods\noften achieve high accuracy under standard conditions, they exhibit limited\nrobustness against adversarial attacks. To address these challenges, we propose\nan approach that integrates adversarial training to mitigate the impact of\nadversarial examples. Furthermore, we utilize diffusion inversion and\nreconstruction to further enhance detection robustness. Experimental results\ndemonstrate that minor adversarial perturbations can easily bypass existing\ndetection systems, but our method significantly improves the robustness of\nthese systems. Additionally, we provide an in-depth analysis of adversarial and\nbenign examples, offering insights into the intrinsic characteristics of\nAI-generated content. All associated code will be made publicly available in a\ndedicated repository to facilitate further research and verification.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86AI\u751f\u6210\u4eba\u8138\u68c0\u6d4b\u7cfb\u7edf\u7684\u8106\u5f31\u6027\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5bf9\u6297\u8bad\u7ec3\u548c\u6269\u6563\u53cd\u6f14\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u751f\u6210\u56fe\u50cf\u6280\u672f\u7684\u5feb\u901f\u53d1\u5c55\u5e26\u6765\u4e86\u5b89\u5168\u98ce\u9669\uff0c\u5c24\u5176\u662f\u4eba\u8138\u751f\u6210\u68c0\u6d4b\u9886\u57df\u3002\u73b0\u6709\u68c0\u6d4b\u65b9\u6cd5\u5728\u6807\u51c6\u6761\u4ef6\u4e0b\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5bf9\u5bf9\u6297\u653b\u51fb\u7684\u9c81\u68d2\u6027\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5bf9\u6297\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u5e76\u5229\u7528\u6269\u6563\u53cd\u6f14\u548c\u91cd\u5efa\u6280\u672f\u589e\u5f3a\u68c0\u6d4b\u9c81\u68d2\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u73b0\u6709\u68c0\u6d4b\u7cfb\u7edf\u6613\u53d7\u5bf9\u6297\u6270\u52a8\u5f71\u54cd\uff0c\u800c\u672c\u6587\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u9c81\u68d2\u6027\u3002", "conclusion": "\u672c\u6587\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86AI\u751f\u6210\u4eba\u8138\u68c0\u6d4b\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u516c\u5f00\u4e86\u4ee3\u7801\u4ee5\u4fc3\u8fdb\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2505.03445", "pdf": "https://arxiv.org/pdf/2505.03445", "abs": "https://arxiv.org/abs/2505.03445", "authors": ["Qi Gan", "Sao Mai Nguyen", "Eric Fenaux", "Stephan Cl\u00e9men\u00e7on", "Moun\u00eem El Yacoubi"], "title": "Polar Coordinate-Based 2D Pose Prior with Neural Distance Field", "categories": ["cs.CV"], "comment": "This paper is accepted by CVPRW 2025", "summary": "Human pose capture is essential for sports analysis, enabling precise\nevaluation of athletes' movements. While deep learning-based human pose\nestimation (HPE) models from RGB videos have achieved impressive performance on\npublic datasets, their effectiveness in real-world sports scenarios is often\nhindered by motion blur, occlusions, and domain shifts across different pose\nrepresentations. Fine-tuning these models can partially alleviate such\nchallenges but typically requires large-scale annotated data and still\nstruggles to generalize across diverse sports environments. To address these\nlimitations, we propose a 2D pose prior-guided refinement approach based on\nNeural Distance Fields (NDF). Unlike existing approaches that rely solely on\nangular representations of human poses, we introduce a polar coordinate-based\nrepresentation that explicitly incorporates joint connection lengths, enabling\na more accurate correction of erroneous pose estimations. Additionally, we\ndefine a novel non-geodesic distance metric that separates angular and radial\ndiscrepancies, which we demonstrate is better suited for polar representations\nthan traditional geodesic distances. To mitigate data scarcity, we develop a\ngradient-based batch-projection augmentation strategy, which synthesizes\nrealistic pose samples through iterative refinement. Our method is evaluated on\na long jump dataset, demonstrating its ability to improve 2D pose estimation\nacross multiple pose representations, making it robust across different\ndomains. Experimental results show that our approach enhances pose plausibility\nwhile requiring only limited training data. Code is available at:\nhttps://github.com/QGAN2019/polar-NDF.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u8ddd\u79bb\u573a\uff08NDF\uff09\u76842D\u59ff\u6001\u5148\u9a8c\u5f15\u5bfc\u7ec6\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u6781\u5750\u6807\u8868\u793a\u548c\u65b0\u7684\u975e\u6d4b\u5730\u8ddd\u79bb\u5ea6\u91cf\uff0c\u63d0\u9ad8\u4e86\u8fd0\u52a8\u573a\u666f\u4e2d\u59ff\u6001\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eRGB\u89c6\u9891\u7684\u6df1\u5ea6\u5b66\u4e60\u59ff\u6001\u4f30\u8ba1\u6a21\u578b\u5728\u771f\u5b9e\u8fd0\u52a8\u573a\u666f\u4e2d\u56e0\u8fd0\u52a8\u6a21\u7cca\u3001\u906e\u6321\u548c\u9886\u57df\u504f\u79fb\u7b49\u95ee\u9898\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\u4e14\u6cdb\u5316\u80fd\u529b\u6709\u9650\u3002", "method": "\u91c7\u7528\u6781\u5750\u6807\u8868\u793a\u7ed3\u5408\u5173\u8282\u8fde\u63a5\u957f\u5ea6\uff0c\u5b9a\u4e49\u975e\u6d4b\u5730\u8ddd\u79bb\u5ea6\u91cf\uff0c\u5e76\u63d0\u51fa\u68af\u5ea6\u6279\u91cf\u6295\u5f71\u589e\u5f3a\u7b56\u7565\u4ee5\u7f13\u89e3\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u3002", "result": "\u5728\u8df3\u8fdc\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u8de8\u591a\u79cd\u59ff\u6001\u8868\u793a\u63d0\u53472D\u59ff\u6001\u4f30\u8ba1\u7684\u9c81\u68d2\u6027\uff0c\u4e14\u4ec5\u9700\u5c11\u91cf\u8bad\u7ec3\u6570\u636e\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u6781\u5750\u6807\u8868\u793a\u548c\u65b0\u8ddd\u79bb\u5ea6\u91cf\u663e\u8457\u63d0\u5347\u4e86\u59ff\u6001\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u6570\u636e\u7a00\u7f3a\u7684\u8fd0\u52a8\u573a\u666f\u3002"}}
{"id": "2505.03463", "pdf": "https://arxiv.org/pdf/2505.03463", "abs": "https://arxiv.org/abs/2505.03463", "authors": ["Muge Du", "Zhuozhao Zheng", "Wenying Wang", "Guotao Quan", "Wuliang Shi", "Le Shen", "Li Zhang", "Liang Li", "Yinong Liu", "Yuxiang Xing"], "title": "Nonperiodic dynamic CT reconstruction using backward-warping INR with regularization of diffeomorphism (BIRD)", "categories": ["cs.CV", "physics.med-ph"], "comment": null, "summary": "Dynamic computed tomography (CT) reconstruction faces significant challenges\nin addressing motion artifacts, particularly for nonperiodic rapid movements\nsuch as cardiac imaging with fast heart rates. Traditional methods struggle\nwith the extreme limited-angle problems inherent in nonperiodic cases. Deep\nlearning methods have improved performance but face generalization challenges.\nRecent implicit neural representation (INR) techniques show promise through\nself-supervised deep learning, but have critical limitations: computational\ninefficiency due to forward-warping modeling, difficulty balancing DVF\ncomplexity with anatomical plausibility, and challenges in preserving fine\ndetails without additional patient-specific pre-scans. This paper presents a\nnovel INR-based framework, BIRD, for nonperiodic dynamic CT reconstruction. It\naddresses these challenges through four key contributions: (1) backward-warping\ndeformation that enables direct computation of each dynamic voxel with\nsignificantly reduced computational cost, (2) diffeomorphism-based DVF\nregularization that ensures anatomically plausible deformations while\nmaintaining representational capacity, (3) motion-compensated analytical\nreconstruction that enhances fine details without requiring additional\npre-scans, and (4) dimensional-reduction design for efficient 4D coordinate\nencoding. Through various simulations and practical studies, including digital\nand physical phantoms and retrospective patient data, we demonstrate the\neffectiveness of our approach for nonperiodic dynamic CT reconstruction with\nenhanced details and reduced motion artifacts. The proposed framework enables\nmore accurate dynamic CT reconstruction with potential clinical applications,\nsuch as one-beat cardiac reconstruction, cinematic image sequences for\nfunctional imaging, and motion artifact reduction in conventional CT scans.", "AI": {"tldr": "BIRD\u6846\u67b6\u901a\u8fc7\u53cd\u5411\u53d8\u5f62\u3001\u5fae\u5206\u540c\u80da\u6b63\u5219\u5316\u3001\u8fd0\u52a8\u8865\u507f\u91cd\u5efa\u548c\u964d\u7ef4\u8bbe\u8ba1\uff0c\u89e3\u51b3\u4e86\u975e\u5468\u671f\u6027\u52a8\u6001CT\u91cd\u5efa\u4e2d\u7684\u8ba1\u7b97\u6548\u7387\u3001\u89e3\u5256\u5408\u7406\u6027\u548c\u7ec6\u8282\u4fdd\u7559\u95ee\u9898\u3002", "motivation": "\u975e\u5468\u671f\u6027\u5feb\u901f\u8fd0\u52a8\uff08\u5982\u5fc3\u810f\u6210\u50cf\uff09\u5728\u52a8\u6001CT\u91cd\u5efa\u4e2d\u9762\u4e34\u8fd0\u52a8\u4f2a\u5f71\u548c\u6709\u9650\u89d2\u5ea6\u95ee\u9898\u7684\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u8ba1\u7b97\u6548\u7387\u548c\u89e3\u5256\u5408\u7406\u6027\u4e0a\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u63d0\u51faBIRD\u6846\u67b6\uff0c\u91c7\u7528\u53cd\u5411\u53d8\u5f62\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u5fae\u5206\u540c\u80da\u6b63\u5219\u5316\u786e\u4fdd\u89e3\u5256\u5408\u7406\u6027\uff0c\u8fd0\u52a8\u8865\u507f\u91cd\u5efa\u589e\u5f3a\u7ec6\u8282\uff0c\u964d\u7ef4\u8bbe\u8ba1\u63d0\u9ad84D\u7f16\u7801\u6548\u7387\u3002", "result": "\u901a\u8fc7\u6a21\u62df\u548c\u5b9e\u9645\u6570\u636e\u9a8c\u8bc1\uff0cBIRD\u5728\u51cf\u5c11\u8fd0\u52a8\u4f2a\u5f71\u548c\u589e\u5f3a\u7ec6\u8282\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u9002\u7528\u4e8e\u4e34\u5e8a\u5982\u5355\u5fc3\u8df3\u5fc3\u810f\u91cd\u5efa\u3002", "conclusion": "BIRD\u6846\u67b6\u4e3a\u975e\u5468\u671f\u6027\u52a8\u6001CT\u91cd\u5efa\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u4e34\u5e8a\u6f5c\u529b\u3002"}}
{"id": "2505.03470", "pdf": "https://arxiv.org/pdf/2505.03470", "abs": "https://arxiv.org/abs/2505.03470", "authors": ["Vibhas Vats", "Md. Alimoor Reza", "David Crandall", "Soon-heung Jung"], "title": "Blending 3D Geometry and Machine Learning for Multi-View Stereopsis", "categories": ["cs.CV", "cs.AI", "cs.CG", "cs.LG"], "comment": "A pre-print -- paper under-review. arXiv admin note: substantial text\n  overlap with arXiv:2310.19583", "summary": "Traditional multi-view stereo (MVS) methods primarily depend on photometric\nand geometric consistency constraints. In contrast, modern learning-based\nalgorithms often rely on the plane sweep algorithm to infer 3D geometry,\napplying explicit geometric consistency (GC) checks only as a post-processing\nstep, with no impact on the learning process itself. In this work, we introduce\nGC MVSNet plus plus, a novel approach that actively enforces geometric\nconsistency of reference view depth maps across multiple source views (multi\nview) and at various scales (multi scale) during the learning phase (see Fig.\n1). This integrated GC check significantly accelerates the learning process by\ndirectly penalizing geometrically inconsistent pixels, effectively halving the\nnumber of training iterations compared to other MVS methods. Furthermore, we\nintroduce a densely connected cost regularization network with two distinct\nblock designs simple and feature dense optimized to harness dense feature\nconnections for enhanced regularization. Extensive experiments demonstrate that\nour approach achieves a new state of the art on the DTU and BlendedMVS datasets\nand secures second place on the Tanks and Temples benchmark. To our knowledge,\nGC MVSNet plus plus is the first method to enforce multi-view, multi-scale\nsupervised geometric consistency during learning. Our code is available.", "AI": {"tldr": "GC MVSNet++\u901a\u8fc7\u5728\u5b66\u4e60\u9636\u6bb5\u4e3b\u52a8\u5b9e\u65bd\u591a\u89c6\u89d2\u3001\u591a\u5c3a\u5ea6\u7684\u51e0\u4f55\u4e00\u81f4\u6027\u68c0\u67e5\uff0c\u663e\u8457\u52a0\u901f\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u65b0\u6700\u4f18\u3002", "motivation": "\u4f20\u7edfMVS\u65b9\u6cd5\u4f9d\u8d56\u5149\u5ea6\u4e00\u81f4\u6027\uff0c\u800c\u73b0\u4ee3\u5b66\u4e60\u65b9\u6cd5\u4ec5\u5728\u540e\u5904\u7406\u4e2d\u5e94\u7528\u51e0\u4f55\u4e00\u81f4\u6027\u68c0\u67e5\uff0c\u672a\u5f71\u54cd\u5b66\u4e60\u8fc7\u7a0b\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5728\u5b66\u4e60\u9636\u6bb5\u96c6\u6210\u51e0\u4f55\u4e00\u81f4\u6027\u68c0\u67e5\uff0c\u63d0\u5347\u6027\u80fd\u548c\u6548\u7387\u3002", "method": "\u63d0\u51faGC MVSNet++\uff0c\u5728\u5b66\u4e60\u9636\u6bb5\u5b9e\u65bd\u591a\u89c6\u89d2\u3001\u591a\u5c3a\u5ea6\u7684\u51e0\u4f55\u4e00\u81f4\u6027\u68c0\u67e5\uff0c\u5e76\u5f15\u5165\u5bc6\u96c6\u8fde\u63a5\u7684\u6210\u672c\u6b63\u5219\u5316\u7f51\u7edc\u3002", "result": "\u5728DTU\u548cBlendedMVS\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u65b0\u6700\u4f18\uff0c\u5728Tanks and Temples\u57fa\u51c6\u4e0a\u6392\u540d\u7b2c\u4e8c\u3002", "conclusion": "GC MVSNet++\u662f\u9996\u4e2a\u5728\u5b66\u4e60\u9636\u6bb5\u5b9e\u65bd\u591a\u89c6\u89d2\u3001\u591a\u5c3a\u5ea6\u51e0\u4f55\u4e00\u81f4\u6027\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u548c\u8bad\u7ec3\u6548\u7387\u3002"}}
{"id": "2505.03494", "pdf": "https://arxiv.org/pdf/2505.03494", "abs": "https://arxiv.org/abs/2505.03494", "authors": ["Zhanyuan Jia", "Ni Yao", "Danyang Sun", "Chuang Han", "Yanting Li", "Jiaofen Nan", "Fubao Zhu", "Chen Zhao", "Weihua Zhou"], "title": "UPMAD-Net: A Brain Tumor Segmentation Network with Uncertainty Guidance and Adaptive Multimodal Feature Fusion", "categories": ["cs.CV"], "comment": "21 pages, 7 figures", "summary": "Background: Brain tumor segmentation has a significant impact on the\ndiagnosis and treatment of brain tumors. Accurate brain tumor segmentation\nremains challenging due to their irregular shapes, vague boundaries, and high\nvariability. Objective: We propose a brain tumor segmentation method that\ncombines deep learning with prior knowledge derived from a region-growing\nalgorithm. Methods: The proposed method utilizes a multi-scale feature fusion\n(MSFF) module and adaptive attention mechanisms (AAM) to extract multi-scale\nfeatures and capture global contextual information. To enhance the model's\nrobustness in low-confidence regions, the Monte Carlo Dropout (MC Dropout)\nstrategy is employed for uncertainty estimation. Results: Extensive experiments\ndemonstrate that the proposed method achieves superior performance on Brain\nTumor Segmentation (BraTS) datasets, significantly outperforming various\nstate-of-the-art methods. On the BraTS2021 dataset, the test Dice scores are\n89.18% for Enhancing Tumor (ET) segmentation, 93.67% for Whole Tumor (WT)\nsegmentation, and 91.23% for Tumor Core (TC) segmentation. On the BraTS2019\nvalidation set, the validation Dice scores are 87.43%, 90.92%, and 90.40% for\nET, WT, and TC segmentation, respectively. Ablation studies further confirmed\nthe contribution of each module to segmentation accuracy, indicating that each\ncomponent played a vital role in overall performance improvement. Conclusion:\nThis study proposed a novel 3D brain tumor segmentation network based on the\nU-Net architecture. By incorporating the prior knowledge and employing the\nuncertainty estimation method, the robustness and performance were improved.\nThe code for the proposed method is available at\nhttps://github.com/chenzhao2023/UPMAD_Net_BrainSeg.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u548c\u533a\u57df\u751f\u957f\u7b97\u6cd5\u5148\u9a8c\u77e5\u8bc6\u7684\u8111\u80bf\u7624\u5206\u5272\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408\u548c\u81ea\u9002\u5e94\u6ce8\u610f\u529b\u673a\u5236\u63d0\u5347\u6027\u80fd\uff0c\u5e76\u5728BraTS\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u8111\u80bf\u7624\u5206\u5272\u5bf9\u8bca\u65ad\u548c\u6cbb\u7597\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u80bf\u7624\u5f62\u72b6\u4e0d\u89c4\u5219\u3001\u8fb9\u754c\u6a21\u7cca\u548c\u53d8\u5f02\u6027\u9ad8\uff0c\u51c6\u786e\u5206\u5272\u4ecd\u5177\u6311\u6218\u6027\u3002", "method": "\u91c7\u7528\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408\u6a21\u5757\u548c\u81ea\u9002\u5e94\u6ce8\u610f\u529b\u673a\u5236\u63d0\u53d6\u7279\u5f81\uff0c\u7ed3\u5408\u8499\u7279\u5361\u6d1bDropout\u7b56\u7565\u8fdb\u884c\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u3002", "result": "\u5728BraTS2021\u548cBraTS2019\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0cDice\u5206\u6570\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eU-Net\u67b6\u6784\u7684\u65b0\u578b3D\u8111\u80bf\u7624\u5206\u5272\u7f51\u7edc\uff0c\u901a\u8fc7\u5148\u9a8c\u77e5\u8bc6\u548c\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u63d0\u5347\u4e86\u9c81\u68d2\u6027\u548c\u6027\u80fd\u3002"}}
{"id": "2505.03498", "pdf": "https://arxiv.org/pdf/2505.03498", "abs": "https://arxiv.org/abs/2505.03498", "authors": ["Mojtaba Safari", "Shansong Wang", "Qiang Li", "Zach Eidex", "Richard L. J. Qiu", "Chih-Wei Chang", "Hui Mao", "Xiaofeng Yang"], "title": "MRI motion correction via efficient residual-guided denoising diffusion probabilistic models", "categories": ["cs.CV", "physics.med-ph"], "comment": null, "summary": "Purpose: Motion artifacts in magnetic resonance imaging (MRI) significantly\ndegrade image quality and impair quantitative analysis. Conventional mitigation\nstrategies, such as repeated acquisitions or motion tracking, are costly and\nworkflow-intensive. This study introduces Res-MoCoDiff, an efficient denoising\ndiffusion probabilistic model tailored for MRI motion artifact correction.\nMethods: Res-MoCoDiff incorporates a novel residual error shifting mechanism in\nthe forward diffusion process, aligning the noise distribution with\nmotion-corrupted data and enabling an efficient four-step reverse diffusion. A\nU-net backbone enhanced with Swin-Transformer blocks conventional attention\nlayers, improving adaptability across resolutions. Training employs a combined\nl1+l2 loss, which promotes image sharpness and reduces pixel-level errors.\nRes-MoCoDiff was evaluated on synthetic dataset generated using a realistic\nmotion simulation framework and on an in-vivo dataset. Comparative analyses\nwere conducted against established methods, including CycleGAN, Pix2pix, and\nMT-DDPM using quantitative metrics such as peak signal-to-noise ratio (PSNR),\nstructural similarity index measure (SSIM), and normalized mean squared error\n(NMSE). Results: The proposed method demonstrated superior performance in\nremoving motion artifacts across all motion severity levels. Res-MoCoDiff\nconsistently achieved the highest SSIM and the lowest NMSE values, with a PSNR\nof up to 41.91+-2.94 dB for minor distortions. Notably, the average sampling\ntime was reduced to 0.37 seconds per batch of two image slices, compared with\n101.74 seconds for conventional approaches.", "AI": {"tldr": "Res-MoCoDiff\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u6269\u6563\u6982\u7387\u6a21\u578b\uff0c\u7528\u4e8eMRI\u8fd0\u52a8\u4f2a\u5f71\u6821\u6b63\uff0c\u901a\u8fc7\u6b8b\u5dee\u8bef\u5dee\u79fb\u4f4d\u673a\u5236\u548c\u56db\u6b65\u53cd\u5411\u6269\u6563\u663e\u8457\u63d0\u5347\u56fe\u50cf\u8d28\u91cf\uff0c\u5e76\u5728\u901f\u5ea6\u548c\u6027\u80fd\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "MRI\u4e2d\u7684\u8fd0\u52a8\u4f2a\u5f71\u4e25\u91cd\u5f71\u54cd\u56fe\u50cf\u8d28\u91cf\u548c\u5b9a\u91cf\u5206\u6790\uff0c\u4f20\u7edf\u65b9\u6cd5\u6210\u672c\u9ad8\u4e14\u6d41\u7a0b\u7e41\u7410\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u4e14\u6027\u80fd\u4f18\u8d8a\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "Res-MoCoDiff\u91c7\u7528\u6b8b\u5dee\u8bef\u5dee\u79fb\u4f4d\u673a\u5236\u548c\u56db\u6b65\u53cd\u5411\u6269\u6563\uff0c\u7ed3\u5408U-net\u548cSwin-Transformer\u5757\uff0c\u4f7f\u7528l1+l2\u635f\u5931\u51fd\u6570\u8bad\u7ec3\uff0c\u5e76\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u3002", "result": "\u5728\u6240\u6709\u8fd0\u52a8\u4e25\u91cd\u7a0b\u5ea6\u4e0b\uff0cRes-MoCoDiff\u5747\u8868\u73b0\u51fa\u6700\u4f73\u6027\u80fd\uff0cSSIM\u6700\u9ad8\uff0cNMSE\u6700\u4f4e\uff0cPSNR\u8fbe41.91\u00b12.94 dB\uff0c\u91c7\u6837\u65f6\u95f4\u663e\u8457\u7f29\u77ed\u81f30.37\u79d2/\u6279\u6b21\u3002", "conclusion": "Res-MoCoDiff\u5728MRI\u8fd0\u52a8\u4f2a\u5f71\u6821\u6b63\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u548c\u4f18\u8d8a\u6027\u80fd\uff0c\u4e3a\u4e34\u5e8a\u548c\u79d1\u7814\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2505.03507", "pdf": "https://arxiv.org/pdf/2505.03507", "abs": "https://arxiv.org/abs/2505.03507", "authors": ["Shenglan Li", "Rui Yao", "Yong Zhou", "Hancheng Zhu", "Kunyang Sun", "Bing Liu", "Zhiwen Shao", "Jiaqi Zhao"], "title": "Modality-Guided Dynamic Graph Fusion and Temporal Diffusion for Self-Supervised RGB-T Tracking", "categories": ["cs.CV"], "comment": "Accepted by the 34th International Joint Conference on Artificial\n  Intelligence (IJCAI 2025)", "summary": "To reduce the reliance on large-scale annotations, self-supervised RGB-T\ntracking approaches have garnered significant attention. However, the omission\nof the object region by erroneous pseudo-label or the introduction of\nbackground noise affects the efficiency of modality fusion, while pseudo-label\nnoise triggered by similar object noise can further affect the tracking\nperformance. In this paper, we propose GDSTrack, a novel approach that\nintroduces dynamic graph fusion and temporal diffusion to address the above\nchallenges in self-supervised RGB-T tracking. GDSTrack dynamically fuses the\nmodalities of neighboring frames, treats them as distractor noise, and\nleverages the denoising capability of a generative model. Specifically, by\nconstructing an adjacency matrix via an Adjacency Matrix Generator (AMG), the\nproposed Modality-guided Dynamic Graph Fusion (MDGF) module uses a dynamic\nadjacency matrix to guide graph attention, focusing on and fusing the object's\ncoherent regions. Temporal Graph-Informed Diffusion (TGID) models MDGF features\nfrom neighboring frames as interference, and thus improving robustness against\nsimilar-object noise. Extensive experiments conducted on four public RGB-T\ntracking datasets demonstrate that GDSTrack outperforms the existing\nstate-of-the-art methods. The source code is available at\nhttps://github.com/LiShenglana/GDSTrack.", "AI": {"tldr": "GDSTrack\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u56fe\u878d\u5408\u548c\u65f6\u5e8f\u6269\u6563\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u81ea\u76d1\u7763RGB-T\u8ddf\u8e2a\u4e2d\u7684\u4f2a\u6807\u7b7e\u566a\u58f0\u548c\u80cc\u666f\u5e72\u6270\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u51cf\u5c11\u5bf9\u5927\u89c4\u6a21\u6807\u6ce8\u7684\u4f9d\u8d56\uff0c\u540c\u65f6\u89e3\u51b3\u81ea\u76d1\u7763RGB-T\u8ddf\u8e2a\u4e2d\u4f2a\u6807\u7b7e\u9519\u8bef\u548c\u80cc\u666f\u566a\u58f0\u5bf9\u6a21\u6001\u878d\u5408\u6548\u7387\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u52a8\u6001\u56fe\u878d\u5408\uff08MDGF\uff09\u6a21\u5757\u548c\u65f6\u5e8f\u56fe\u6269\u6563\uff08TGID\uff09\u6a21\u5757\uff0c\u52a8\u6001\u878d\u5408\u90bb\u8fd1\u5e27\u6a21\u6001\u5e76\u5229\u7528\u751f\u6210\u6a21\u578b\u7684\u53bb\u566a\u80fd\u529b\u3002", "result": "\u5728\u56db\u4e2a\u516c\u5f00RGB-T\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cGDSTrack\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "GDSTrack\u901a\u8fc7\u52a8\u6001\u56fe\u878d\u5408\u548c\u65f6\u5e8f\u6269\u6563\u6709\u6548\u63d0\u5347\u4e86\u81ea\u76d1\u7763RGB-T\u8ddf\u8e2a\u7684\u9c81\u68d2\u6027\u548c\u6027\u80fd\u3002"}}
{"id": "2505.03522", "pdf": "https://arxiv.org/pdf/2505.03522", "abs": "https://arxiv.org/abs/2505.03522", "authors": ["Haotong Cheng", "Zhiqi Zhang", "Hao Li", "Xinshang Zhang"], "title": "Optimization of Module Transferability in Single Image Super-Resolution: Universality Assessment and Cycle Residual Blocks", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Deep learning has substantially advanced the Single Image Super-Resolution\n(SISR). However, existing researches have predominantly focused on raw\nperformance gains, with little attention paid to quantifying the\ntransferability of architectural components. In this paper, we introduce the\nconcept of \"Universality\" and its associated definitions which extend the\ntraditional notion of \"Generalization\" to encompass the modules' ease of\ntransferability, thus revealing the relationships between module universality\nand model generalizability. Then we propose the Universality Assessment\nEquation (UAE), a metric for quantifying how readily a given module could be\ntransplanted across models. Guided by the UAE results of standard residual\nblocks and other plug-and-play modules, we further design two optimized\nmodules, Cycle Residual Block (CRB) and Depth-Wise Cycle Residual Block (DCRB).\nThrough comprehensive experiments on natural-scene benchmarks, remote-sensing\ndatasets, extreme-industrial imagery and on-device deployments, we demonstrate\nthat networks embedded with the proposed plug-and-play modules outperform\nseveral state-of-the-arts, reaching a PSNR enhancement of up to 0.83dB or\nenabling a 71.3% reduction in parameters with negligible loss in reconstruction\nfidelity.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u201c\u901a\u7528\u6027\u201d\u6982\u5ff5\u53ca\u8bc4\u4f30\u65b9\u7a0b\uff08UAE\uff09\uff0c\u7528\u4e8e\u91cf\u5316\u6a21\u5757\u7684\u53ef\u79fb\u690d\u6027\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e24\u79cd\u4f18\u5316\u6a21\u5757\uff08CRB\u548cDCRB\uff09\uff0c\u5728\u591a\u79cd\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8\u6027\u80fd\u63d0\u5347\uff0c\u800c\u5ffd\u7565\u4e86\u6a21\u5757\u7684\u53ef\u79fb\u690d\u6027\u91cf\u5316\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63a2\u7d22\u6a21\u5757\u901a\u7528\u6027\u4e0e\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u7684\u5173\u7cfb\u3002", "method": "\u5f15\u5165\u201c\u901a\u7528\u6027\u201d\u6982\u5ff5\u53caUAE\u8bc4\u4f30\u65b9\u7a0b\uff0c\u57fa\u4e8e\u8bc4\u4f30\u7ed3\u679c\u8bbe\u8ba1\u4f18\u5316\u6a21\u5757CRB\u548cDCRB\u3002", "result": "\u5728\u591a\u79cd\u6570\u636e\u96c6\u4e0a\uff0c\u5d4c\u5165\u4f18\u5316\u6a21\u5757\u7684\u7f51\u7edc\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0cPSNR\u63d0\u5347\u8fbe0.83dB\uff0c\u6216\u53c2\u6570\u51cf\u5c1171.3%\u4e14\u91cd\u5efa\u8d28\u91cf\u51e0\u4e4e\u65e0\u635f\u3002", "conclusion": "\u901a\u8fc7\u91cf\u5316\u6a21\u5757\u901a\u7528\u6027\u5e76\u4f18\u5316\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u6027\u80fd\u548c\u53ef\u79fb\u690d\u6027\u3002"}}
{"id": "2505.03528", "pdf": "https://arxiv.org/pdf/2505.03528", "abs": "https://arxiv.org/abs/2505.03528", "authors": ["Chenguang Liu", "Jianjun Chen", "Yunfei Chen", "Yubei He", "Zhuangkun Wei", "Hongjian Sun", "Haiyan Lu", "Qi Hao"], "title": "Coop-WD: Cooperative Perception with Weighting and Denoising for Robust V2V Communication", "categories": ["cs.CV"], "comment": null, "summary": "Cooperative perception, leveraging shared information from multiple vehicles\nvia vehicle-to-vehicle (V2V) communication, plays a vital role in autonomous\ndriving to alleviate the limitation of single-vehicle perception. Existing\nworks have explored the effects of V2V communication impairments on perception\nprecision, but they lack generalization to different levels of impairments. In\nthis work, we propose a joint weighting and denoising framework, Coop-WD, to\nenhance cooperative perception subject to V2V channel impairments. In this\nframework, the self-supervised contrastive model and the conditional diffusion\nprobabilistic model are adopted hierarchically for vehicle-level and\npixel-level feature enhancement. An efficient variant model, Coop-WD-eco, is\nproposed to selectively deactivate denoising to reduce processing overhead.\nRician fading, non-stationarity, and time-varying distortion are considered.\nSimulation results demonstrate that the proposed Coop-WD outperforms\nconventional benchmarks in all types of channels. Qualitative analysis with\nvisual examples further proves the superiority of our proposed method. The\nproposed Coop-WD-eco achieves up to 50% reduction in computational cost under\nsevere distortion while maintaining comparable accuracy as channel conditions\nimprove.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCoop-WD\u7684\u8054\u5408\u52a0\u6743\u548c\u53bb\u566a\u6846\u67b6\uff0c\u7528\u4e8e\u589e\u5f3aV2V\u901a\u4fe1\u53d7\u635f\u4e0b\u7684\u534f\u540c\u611f\u77e5\u6027\u80fd\uff0c\u5e76\u63d0\u51fa\u4e86\u9ad8\u6548\u53d8\u4f53Coop-WD-eco\u4ee5\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u5bf9\u4e0d\u540c\u7ea7\u522bV2V\u901a\u4fe1\u635f\u4f24\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u9650\u5236\u4e86\u534f\u540c\u611f\u77e5\u7684\u6027\u80fd\u63d0\u5347\u3002", "method": "\u91c7\u7528\u81ea\u76d1\u7763\u5bf9\u6bd4\u6a21\u578b\u548c\u6761\u4ef6\u6269\u6563\u6982\u7387\u6a21\u578b\u8fdb\u884c\u8f66\u8f86\u7ea7\u548c\u50cf\u7d20\u7ea7\u7279\u5f81\u589e\u5f3a\uff0c\u5e76\u63d0\u51faCoop-WD-eco\u9009\u62e9\u6027\u53bb\u6fc0\u6d3b\u53bb\u566a\u4ee5\u964d\u4f4e\u5f00\u9500\u3002", "result": "Coop-WD\u5728\u6240\u6709\u7c7b\u578b\u4fe1\u9053\u4e2d\u5747\u4f18\u4e8e\u4f20\u7edf\u57fa\u51c6\uff0cCoop-WD-eco\u5728\u4e25\u91cd\u5931\u771f\u4e0b\u53ef\u51cf\u5c1150%\u8ba1\u7b97\u6210\u672c\u4e14\u4fdd\u6301\u7cbe\u5ea6\u3002", "conclusion": "Coop-WD\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u534f\u540c\u611f\u77e5\u6027\u80fd\uff0cCoop-WD-eco\u5728\u8ba1\u7b97\u6548\u7387\u548c\u7cbe\u5ea6\u95f4\u53d6\u5f97\u4e86\u5e73\u8861\u3002"}}
{"id": "2505.03538", "pdf": "https://arxiv.org/pdf/2505.03538", "abs": "https://arxiv.org/abs/2505.03538", "authors": ["Chuyu Zhao", "Hao Huang", "Jiashuo Guo", "Ziyu Shen", "Zhongwei Zhou", "Jie Liu", "Zekuan Yu"], "title": "RAIL: Region-Aware Instructive Learning for Semi-Supervised Tooth Segmentation in CBCT", "categories": ["cs.CV"], "comment": null, "summary": "Semi-supervised learning has become a compelling approach for 3D tooth\nsegmentation from CBCT scans, where labeled data is minimal. However, existing\nmethods still face two persistent challenges: limited corrective supervision in\nstructurally ambiguous or mislabeled regions during supervised training and\nperformance degradation caused by unreliable pseudo-labels on unlabeled data.\nTo address these problems, we propose Region-Aware Instructive Learning (RAIL),\na dual-group dual-student, semi-supervised framework. Each group contains two\nstudent models guided by a shared teacher network. By alternating training\nbetween the two groups, RAIL promotes intergroup knowledge transfer and\ncollaborative region-aware instruction while reducing overfitting to the\ncharacteristics of any single model. Specifically, RAIL introduces two\ninstructive mechanisms. Disagreement-Focused Supervision (DFS) Controller\nimproves supervised learning by instructing predictions only within areas where\nstudent outputs diverge from both ground truth and the best student, thereby\nconcentrating supervision on structurally ambiguous or mislabeled areas. In the\nunsupervised phase, Confidence-Aware Learning (CAL) Modulator reinforces\nagreement in regions with high model certainty while reducing the effect of\nlow-confidence predictions during training. This helps prevent our model from\nlearning unstable patterns and improves the overall reliability of\npseudo-labels. Extensive experiments on four CBCT tooth segmentation datasets\nshow that RAIL surpasses state-of-the-art methods under limited annotation. Our\ncode will be available at https://github.com/Tournesol-Saturday/RAIL.", "AI": {"tldr": "RAIL\u662f\u4e00\u79cd\u53cc\u7ec4\u53cc\u5b66\u751f\u7684\u534a\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u533a\u57df\u611f\u77e5\u6307\u5bfc\u5b66\u4e60\u89e3\u51b3CBCT\u7259\u9f7f\u5206\u5272\u4e2d\u76d1\u7763\u4e0d\u8db3\u548c\u4f2a\u6807\u7b7e\u4e0d\u53ef\u9760\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u534a\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u5728CBCT\u7259\u9f7f\u5206\u5272\u4e2d\u5b58\u5728\u76d1\u7763\u4e0d\u8db3\u548c\u4f2a\u6807\u7b7e\u4e0d\u53ef\u9760\u7684\u95ee\u9898\uff0cRAIL\u65e8\u5728\u901a\u8fc7\u53cc\u7ec4\u53cc\u5b66\u751f\u6846\u67b6\u548c\u533a\u57df\u611f\u77e5\u673a\u5236\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "RAIL\u91c7\u7528\u53cc\u7ec4\u53cc\u5b66\u751f\u6846\u67b6\uff0c\u901a\u8fc7\u4ea4\u66ff\u8bad\u7ec3\u4fc3\u8fdb\u7ec4\u95f4\u77e5\u8bc6\u8f6c\u79fb\u3002\u5f15\u5165DFS\u63a7\u5236\u5668\u548cCAL\u8c03\u5236\u5668\uff0c\u5206\u522b\u4f18\u5316\u76d1\u7763\u5b66\u4e60\u548c\u65e0\u76d1\u7763\u5b66\u4e60\u3002", "result": "\u5728\u56db\u4e2aCBCT\u7259\u9f7f\u5206\u5272\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cRAIL\u5728\u6709\u9650\u6807\u6ce8\u4e0b\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "RAIL\u901a\u8fc7\u533a\u57df\u611f\u77e5\u6307\u5bfc\u548c\u53cc\u7ec4\u6846\u67b6\uff0c\u6709\u6548\u63d0\u5347\u4e86\u534a\u76d1\u7763\u5b66\u4e60\u5728CBCT\u7259\u9f7f\u5206\u5272\u4e2d\u7684\u6027\u80fd\u3002"}}
{"id": "2505.03539", "pdf": "https://arxiv.org/pdf/2505.03539", "abs": "https://arxiv.org/abs/2505.03539", "authors": ["Mengfei Duan", "Kailun Yang", "Yuheng Zhang", "Yihong Cao", "Fei Teng", "Kai Luo", "Jiaming Zhang", "Zhiyong Li", "Shutao Li"], "title": "Panoramic Out-of-Distribution Segmentation", "categories": ["cs.CV", "cs.RO", "eess.IV"], "comment": "Code and datasets will be available at\n  https://github.com/MengfeiD/PanOoS", "summary": "Panoramic imaging enables capturing 360{\\deg} images with an ultra-wide\nField-of-View (FoV) for dense omnidirectional perception. However, current\npanoramic semantic segmentation methods fail to identify outliers, and pinhole\nOut-of-distribution Segmentation (OoS) models perform unsatisfactorily in the\npanoramic domain due to background clutter and pixel distortions. To address\nthese issues, we introduce a new task, Panoramic Out-of-distribution\nSegmentation (PanOoS), achieving OoS for panoramas. Furthermore, we propose the\nfirst solution, POS, which adapts to the characteristics of panoramic images\nthrough text-guided prompt distribution learning. Specifically, POS integrates\na disentanglement strategy designed to materialize the cross-domain\ngeneralization capability of CLIP. The proposed Prompt-based Restoration\nAttention (PRA) optimizes semantic decoding by prompt guidance and\nself-adaptive correction, while Bilevel Prompt Distribution Learning (BPDL)\nrefines the manifold of per-pixel mask embeddings via semantic prototype\nsupervision. Besides, to compensate for the scarcity of PanOoS datasets, we\nestablish two benchmarks: DenseOoS, which features diverse outliers in complex\nenvironments, and QuadOoS, captured by a quadruped robot with a panoramic\nannular lens system. Extensive experiments demonstrate superior performance of\nPOS, with AuPRC improving by 34.25% and FPR95 decreasing by 21.42% on DenseOoS,\noutperforming state-of-the-art pinhole-OoS methods. Moreover, POS achieves\nleading closed-set segmentation capabilities. Code and datasets will be\navailable at https://github.com/MengfeiD/PanOoS.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u5168\u666f\u56fe\u50cf\u4e2d\u7684\u79bb\u7fa4\u70b9\u5206\u5272\u4efb\u52a1\uff08PanOoS\uff09\uff0c\u5e76\u9996\u6b21\u63d0\u51fa\u89e3\u51b3\u65b9\u6848POS\uff0c\u901a\u8fc7\u6587\u672c\u5f15\u5bfc\u7684\u63d0\u793a\u5206\u5e03\u5b66\u4e60\u9002\u5e94\u5168\u666f\u56fe\u50cf\u7279\u6027\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u5168\u666f\u8bed\u4e49\u5206\u5272\u65b9\u6cd5\u65e0\u6cd5\u8bc6\u522b\u79bb\u7fa4\u70b9\uff0c\u800c\u4f20\u7edf\u79bb\u7fa4\u70b9\u5206\u5272\u6a21\u578b\u5728\u5168\u666f\u57df\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u89e3\u51b3\u80cc\u666f\u5e72\u6270\u548c\u50cf\u7d20\u5931\u771f\u95ee\u9898\u3002", "method": "\u63d0\u51faPOS\u65b9\u6cd5\uff0c\u7ed3\u5408\u89e3\u8026\u7b56\u7565\u3001\u63d0\u793a\u4fee\u590d\u6ce8\u610f\u529b\uff08PRA\uff09\u548c\u53cc\u5c42\u63d0\u793a\u5206\u5e03\u5b66\u4e60\uff08BPDL\uff09\uff0c\u4f18\u5316\u8bed\u4e49\u89e3\u7801\u548c\u5d4c\u5165\u5206\u5e03\u3002", "result": "POS\u5728DenseOoS\u6570\u636e\u96c6\u4e0aAuPRC\u63d0\u534734.25%\uff0cFPR95\u964d\u4f4e21.42%\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u540c\u65f6\u5177\u5907\u9886\u5148\u7684\u5c01\u95ed\u96c6\u5206\u5272\u80fd\u529b\u3002", "conclusion": "POS\u4e3a\u5168\u666f\u79bb\u7fa4\u70b9\u5206\u5272\u63d0\u4f9b\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u901a\u8fc7\u65b0\u6570\u636e\u96c6DenseOoS\u548cQuadOoS\u586b\u8865\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u3002"}}
{"id": "2505.03554", "pdf": "https://arxiv.org/pdf/2505.03554", "abs": "https://arxiv.org/abs/2505.03554", "authors": ["Jo\u00e3o Alves", "Pia Haubro Andersen", "Rikke Gade"], "title": "Read My Ears! Horse Ear Movement Detection for Equine Affective State Assessment", "categories": ["cs.CV"], "comment": null, "summary": "The Equine Facial Action Coding System (EquiFACS) enables the systematic\nannotation of facial movements through distinct Action Units (AUs). It serves\nas a crucial tool for assessing affective states in horses by identifying\nsubtle facial expressions associated with discomfort. However, the field of\nhorse affective state assessment is constrained by the scarcity of annotated\ndata, as manually labelling facial AUs is both time-consuming and costly. To\naddress this challenge, automated annotation systems are essential for\nleveraging existing datasets and improving affective states detection tools. In\nthis work, we study different methods for specific ear AU detection and\nlocalization from horse videos. We leverage past works on deep learning-based\nvideo feature extraction combined with recurrent neural networks for the video\nclassification task, as well as a classic optical flow based approach. We\nachieve 87.5% classification accuracy of ear movement presence on a public\nhorse video dataset, demonstrating the potential of our approach. We discuss\nfuture directions to develop these systems, with the aim of bridging the gap\nbetween automated AU detection and practical applications in equine welfare and\nveterinary diagnostics. Our code will be made publicly available at\nhttps://github.com/jmalves5/read-my-ears.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u548c\u5149\u6d41\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u81ea\u52a8\u68c0\u6d4b\u548c\u5b9a\u4f4d\u9a6c\u5339\u89c6\u9891\u4e2d\u7684\u7279\u5b9a\u8033\u6735\u52a8\u4f5c\u5355\u5143\uff08AU\uff09\uff0c\u65e8\u5728\u89e3\u51b3\u624b\u52a8\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\u3002", "motivation": "\u7531\u4e8e\u624b\u52a8\u6807\u6ce8\u9a6c\u5339\u9762\u90e8\u52a8\u4f5c\u5355\u5143\uff08AUs\uff09\u8017\u65f6\u4e14\u6602\u8d35\uff0c\u73b0\u6709\u6570\u636e\u7a00\u7f3a\uff0c\u9650\u5236\u4e86\u9a6c\u5339\u60c5\u611f\u72b6\u6001\u8bc4\u4f30\u7684\u53d1\u5c55\u3002\u56e0\u6b64\uff0c\u9700\u8981\u81ea\u52a8\u5316\u6807\u6ce8\u7cfb\u7edf\u6765\u5229\u7528\u73b0\u6709\u6570\u636e\u5e76\u6539\u8fdb\u68c0\u6d4b\u5de5\u5177\u3002", "method": "\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u89c6\u9891\u7279\u5f81\u63d0\u53d6\u548c\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u89c6\u9891\u5206\u7c7b\u4efb\u52a1\uff0c\u540c\u65f6\u91c7\u7528\u7ecf\u5178\u7684\u5149\u6d41\u65b9\u6cd5\uff0c\u7528\u4e8e\u68c0\u6d4b\u548c\u5b9a\u4f4d\u9a6c\u5339\u89c6\u9891\u4e2d\u7684\u8033\u6735AU\u3002", "result": "\u5728\u516c\u5f00\u7684\u9a6c\u5339\u89c6\u9891\u6570\u636e\u96c6\u4e0a\uff0c\u5b9e\u73b0\u4e8687.5%\u7684\u8033\u6735\u52a8\u4f5c\u5b58\u5728\u5206\u7c7b\u51c6\u786e\u7387\uff0c\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u6f5c\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u81ea\u52a8\u5316AU\u68c0\u6d4b\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\uff0c\u672a\u6765\u53ef\u8fdb\u4e00\u6b65\u5f00\u53d1\u4ee5\u5e94\u7528\u4e8e\u9a6c\u5339\u798f\u5229\u548c\u517d\u533b\u8bca\u65ad\u5b9e\u8df5\u3002\u4ee3\u7801\u5c06\u516c\u5f00\u3002"}}
{"id": "2505.03557", "pdf": "https://arxiv.org/pdf/2505.03557", "abs": "https://arxiv.org/abs/2505.03557", "authors": ["Koray Ulusan", "Benjamin Kiefer"], "title": "Generating Synthetic Data via Augmentations for Improved Facial Resemblance in DreamBooth and InstantID", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted to CVPR 2025 Workshop \"Synthetic Data for Computer Vision\n  Workshop\", https://syndata4cv.github.io/", "summary": "The personalization of Stable Diffusion for generating professional portraits\nfrom amateur photographs is a burgeoning area, with applications in various\ndownstream contexts. This paper investigates the impact of augmentations on\nimproving facial resemblance when using two prominent personalization\ntechniques: DreamBooth and InstantID. Through a series of experiments with\ndiverse subject datasets, we assessed the effectiveness of various augmentation\nstrategies on the generated headshots' fidelity to the original subject. We\nintroduce FaceDistance, a wrapper around FaceNet, to rank the generations based\non facial similarity, which aided in our assessment. Ultimately, this research\nprovides insights into the role of augmentations in enhancing facial\nresemblance in SDXL-generated portraits, informing strategies for their\neffective deployment in downstream applications.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5982\u4f55\u901a\u8fc7\u589e\u5f3a\u6280\u672f\u63d0\u5347Stable Diffusion\u751f\u6210\u8096\u50cf\u7684\u9762\u90e8\u76f8\u4f3c\u6027\uff0c\u6bd4\u8f83\u4e86DreamBooth\u548cInstantID\u4e24\u79cd\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86FaceDistance\u8bc4\u4f30\u5de5\u5177\u3002", "motivation": "\u63a2\u7d22\u589e\u5f3a\u6280\u672f\u5bf9\u63d0\u5347\u4e1a\u4f59\u7167\u7247\u751f\u6210\u4e13\u4e1a\u8096\u50cf\u7684\u9762\u90e8\u76f8\u4f3c\u6027\u7684\u5f71\u54cd\uff0c\u4e3a\u4e0b\u6e38\u5e94\u7528\u63d0\u4f9b\u4f18\u5316\u7b56\u7565\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u8bc4\u4f30\u591a\u79cd\u589e\u5f3a\u7b56\u7565\u5bf9DreamBooth\u548cInstantID\u751f\u6210\u8096\u50cf\u7684\u5f71\u54cd\uff0c\u5e76\u5f15\u5165FaceDistance\u5de5\u5177\u91cf\u5316\u9762\u90e8\u76f8\u4f3c\u6027\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u589e\u5f3a\u6280\u672f\u80fd\u663e\u8457\u63d0\u5347\u751f\u6210\u8096\u50cf\u7684\u9762\u90e8\u76f8\u4f3c\u6027\uff0cFaceDistance\u5de5\u5177\u6709\u6548\u8f85\u52a9\u8bc4\u4f30\u3002", "conclusion": "\u589e\u5f3a\u6280\u672f\u5728\u63d0\u5347Stable Diffusion\u751f\u6210\u8096\u50cf\u7684\u9762\u90e8\u76f8\u4f3c\u6027\u4e2d\u8d77\u5173\u952e\u4f5c\u7528\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u4f18\u5316\u65b9\u5411\u3002"}}
{"id": "2505.03562", "pdf": "https://arxiv.org/pdf/2505.03562", "abs": "https://arxiv.org/abs/2505.03562", "authors": ["Jiwoo Jeong", "Kirok Kim", "Wooju Kim", "Nam-Joon Kim"], "title": "Real-Time Person Image Synthesis Using a Flow Matching Model", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Pose-Guided Person Image Synthesis (PGPIS) generates realistic person images\nconditioned on a target pose and a source image. This task plays a key role in\nvarious real-world applications, such as sign language video generation, AR/VR,\ngaming, and live streaming. In these scenarios, real-time PGPIS is critical for\nproviding immediate visual feedback and maintaining user immersion.However,\nachieving real-time performance remains a significant challenge due to the\ncomplexity of synthesizing high-fidelity images from diverse and dynamic human\nposes. Recent diffusion-based methods have shown impressive image quality in\nPGPIS, but their slow sampling speeds hinder deployment in time-sensitive\napplications. This latency is particularly problematic in tasks like generating\nsign language videos during live broadcasts, where rapid image updates are\nrequired. Therefore, developing a fast and reliable PGPIS model is a crucial\nstep toward enabling real-time interactive systems. To address this challenge,\nwe propose a generative model based on flow matching (FM). Our approach enables\nfaster, more stable, and more efficient training and sampling. Furthermore, the\nproposed model supports conditional generation and can operate in latent space,\nmaking it especially suitable for real-time PGPIS applications where both speed\nand quality are critical. We evaluate our proposed method, Real-Time Person\nImage Synthesis Using a Flow Matching Model (RPFM), on the widely used\nDeepFashion dataset for PGPIS tasks. Our results show that RPFM achieves\nnear-real-time sampling speeds while maintaining performance comparable to the\nstate-of-the-art models. Our methodology trades off a slight, acceptable\ndecrease in generated-image accuracy for over a twofold increase in generation\nspeed, thereby ensuring real-time performance.", "AI": {"tldr": "PGPIS\u4efb\u52a1\u901a\u8fc7\u76ee\u6807\u59ff\u6001\u548c\u6e90\u56fe\u50cf\u751f\u6210\u903c\u771f\u7684\u4eba\u4f53\u56fe\u50cf\uff0c\u4f46\u5b9e\u65f6\u6027\u80fd\u662f\u6311\u6218\u3002\u57fa\u4e8e\u6d41\u5339\u914d\u7684\u6a21\u578bRPFM\u5728\u901f\u5ea6\u548c\u6027\u80fd\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u5b9e\u73b0\u4e86\u8fd1\u5b9e\u65f6\u751f\u6210\u3002", "motivation": "\u5b9e\u65f6PGPIS\u5728AR/VR\u3001\u76f4\u64ad\u7b49\u573a\u666f\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u6269\u6563\u6a21\u578b\u901f\u5ea6\u6162\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5b9e\u65f6\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u6d41\u5339\u914d\uff08FM\uff09\u7684\u751f\u6210\u6a21\u578bRPFM\uff0c\u652f\u6301\u6761\u4ef6\u751f\u6210\u548c\u6f5c\u5728\u7a7a\u95f4\u64cd\u4f5c\uff0c\u63d0\u5347\u8bad\u7ec3\u548c\u91c7\u6837\u6548\u7387\u3002", "result": "RPFM\u5728DeepFashion\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u8fd1\u5b9e\u65f6\u901f\u5ea6\uff0c\u6027\u80fd\u4e0eSOTA\u6a21\u578b\u76f8\u5f53\uff0c\u901f\u5ea6\u63d0\u5347\u4e24\u500d\u4ee5\u4e0a\u3002", "conclusion": "RPFM\u901a\u8fc7\u727a\u7272\u5c11\u91cf\u751f\u6210\u7cbe\u5ea6\u6362\u53d6\u901f\u5ea6\uff0c\u4e3a\u5b9e\u65f6\u4ea4\u4e92\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u884c\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.03567", "pdf": "https://arxiv.org/pdf/2505.03567", "abs": "https://arxiv.org/abs/2505.03567", "authors": ["Zengli Luo", "Canlong Zhang", "Xiaochun Lu", "Zhixin Li", "Zhiwen Wang"], "title": "Uncertainty-Aware Prototype Semantic Decoupling for Text-Based Person Search in Full Images", "categories": ["cs.CV"], "comment": "9pages,5figures", "summary": "Text-based pedestrian search (TBPS) in full images aims to locate a target\npedestrian in untrimmed images using natural language descriptions. However, in\ncomplex scenes with multiple pedestrians, existing methods are limited by\nuncertainties in detection and matching, leading to degraded performance. To\naddress this, we propose UPD-TBPS, a novel framework comprising three modules:\nMulti-granularity Uncertainty Estimation (MUE), Prototype-based Uncertainty\nDecoupling (PUD), and Cross-modal Re-identification (ReID). MUE conducts\nmulti-granularity queries to identify potential targets and assigns confidence\nscores to reduce early-stage uncertainty. PUD leverages visual context\ndecoupling and prototype mining to extract features of the target pedestrian\ndescribed in the query. It separates and learns pedestrian prototype\nrepresentations at both the coarse-grained cluster level and the fine-grained\nindividual level, thereby reducing matching uncertainty. ReID evaluates\ncandidates with varying confidence levels, improving detection and retrieval\naccuracy. Experiments on CUHK-SYSU-TBPS and PRW-TBPS datasets validate the\neffectiveness of our framework.", "AI": {"tldr": "UPD-TBPS\u6846\u67b6\u901a\u8fc7\u591a\u7c92\u5ea6\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u3001\u539f\u578b\u4e0d\u786e\u5b9a\u6027\u89e3\u8026\u548c\u8de8\u6a21\u6001\u91cd\u8bc6\u522b\u4e09\u4e2a\u6a21\u5757\uff0c\u63d0\u5347\u6587\u672c\u884c\u4eba\u641c\u7d22\u5728\u590d\u6742\u573a\u666f\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u590d\u6742\u573a\u666f\u4e2d\u56e0\u68c0\u6d4b\u548c\u5339\u914d\u7684\u4e0d\u786e\u5b9a\u6027\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\uff0c\u9700\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51faUPD-TBPS\u6846\u67b6\uff0c\u5305\u542bMUE\uff08\u591a\u7c92\u5ea6\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff09\u3001PUD\uff08\u539f\u578b\u4e0d\u786e\u5b9a\u6027\u89e3\u8026\uff09\u548cReID\uff08\u8de8\u6a21\u6001\u91cd\u8bc6\u522b\uff09\u6a21\u5757\u3002", "result": "\u5728CUHK-SYSU-TBPS\u548cPRW-TBPS\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "conclusion": "UPD-TBPS\u901a\u8fc7\u51cf\u5c11\u4e0d\u786e\u5b9a\u6027\u63d0\u5347\u4e86\u6587\u672c\u884c\u4eba\u641c\u7d22\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2505.03569", "pdf": "https://arxiv.org/pdf/2505.03569", "abs": "https://arxiv.org/abs/2505.03569", "authors": ["Mishal Fatima", "Steffen Jung", "Margret Keuper"], "title": "Corner Cases: How Size and Position of Objects Challenge ImageNet-Trained Models", "categories": ["cs.CV"], "comment": null, "summary": "Backgrounds in images play a major role in contributing to spurious\ncorrelations among different data points. Owing to aesthetic preferences of\nhumans capturing the images, datasets can exhibit positional (location of the\nobject within a given frame) and size (region-of-interest to image ratio)\nbiases for different classes. In this paper, we show that these biases can\nimpact how much a model relies on spurious features in the background to make\nits predictions. To better illustrate our findings, we propose a synthetic\ndataset derived from ImageNet1k, Hard-Spurious-ImageNet, which contains images\nwith various backgrounds, object positions, and object sizes. By evaluating the\ndataset on different pretrained models, we find that most models rely heavily\non spurious features in the background when the region-of-interest (ROI) to\nimage ratio is small and the object is far from the center of the image.\nMoreover, we also show that current methods that aim to mitigate harmful\nspurious features, do not take into account these factors, hence fail to\nachieve considerable performance gains for worst-group accuracies when the size\nand location of core features in an image change.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u56fe\u50cf\u80cc\u666f\u5bf9\u6a21\u578b\u9884\u6d4b\u4e2d\u865a\u5047\u76f8\u5173\u6027\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u5408\u6210\u6570\u636e\u96c6Hard-Spurious-ImageNet\uff0c\u5e76\u53d1\u73b0\u6a21\u578b\u5728\u76ee\u6807\u533a\u57df\u5360\u6bd4\u8f83\u5c0f\u6216\u504f\u79bb\u4e2d\u5fc3\u65f6\u66f4\u4f9d\u8d56\u80cc\u666f\u7279\u5f81\u3002", "motivation": "\u63a2\u8ba8\u56fe\u50cf\u6570\u636e\u96c6\u4e2d\u7684\u4f4d\u7f6e\u548c\u5927\u5c0f\u504f\u5dee\u5982\u4f55\u5bfc\u81f4\u6a21\u578b\u4f9d\u8d56\u865a\u5047\u80cc\u666f\u7279\u5f81\uff0c\u8fdb\u800c\u5f71\u54cd\u9884\u6d4b\u51c6\u786e\u6027\u3002", "method": "\u63d0\u51fa\u5408\u6210\u6570\u636e\u96c6Hard-Spurious-ImageNet\uff0c\u8bc4\u4f30\u4e0d\u540c\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u80cc\u666f\u3001\u4f4d\u7f6e\u548c\u5927\u5c0f\u53d8\u5316\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u53d1\u73b0\u6a21\u578b\u5728\u76ee\u6807\u533a\u57df\u5c0f\u6216\u504f\u79bb\u4e2d\u5fc3\u65f6\u66f4\u4f9d\u8d56\u80cc\u666f\u7279\u5f81\uff0c\u4e14\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u663e\u8457\u6539\u5584\u6700\u5dee\u7ec4\u6027\u80fd\u3002", "conclusion": "\u80cc\u666f\u7279\u5f81\u5bf9\u6a21\u578b\u9884\u6d4b\u6709\u663e\u8457\u5f71\u54cd\uff0c\u73b0\u6709\u65b9\u6cd5\u9700\u8fdb\u4e00\u6b65\u6539\u8fdb\u4ee5\u5e94\u5bf9\u76ee\u6807\u533a\u57df\u53d8\u5316\u3002"}}
{"id": "2505.03575", "pdf": "https://arxiv.org/pdf/2505.03575", "abs": "https://arxiv.org/abs/2505.03575", "authors": ["Maria Kainz", "Johannes K. Krondorfer", "Malte Jaschik", "Maria Jernej", "Harald Ganster"], "title": "Supervised and Unsupervised Textile Classification via Near-Infrared Hyperspectral Imaging and Deep Learning", "categories": ["cs.CV", "physics.app-ph"], "comment": "Accepted at: Proceedings of OCM 2025 - 7th International Conference\n  on Optical Characterization of Materials, March 26-27, 2025, Karlsruhe,\n  Germany, pp. 319-328", "summary": "Recycling textile fibers is critical to reducing the environmental impact of\nthe textile industry. Hyperspectral near-infrared (NIR) imaging combined with\nadvanced deep learning algorithms offers a promising solution for efficient\nfiber classification and sorting. In this study, we investigate supervised and\nunsupervised deep learning models and test their generalization capabilities on\ndifferent textile structures. We show that optimized convolutional neural\nnetworks (CNNs) and autoencoder networks achieve robust generalization under\nvarying conditions. These results highlight the potential of hyperspectral\nimaging and deep learning to advance sustainable textile recycling through\naccurate and robust classification.", "AI": {"tldr": "\u5229\u7528\u9ad8\u5149\u8c31\u8fd1\u7ea2\u5916\u6210\u50cf\u548c\u6df1\u5ea6\u5b66\u4e60\u7b97\u6cd5\u4f18\u5316\u7eba\u7ec7\u7ea4\u7ef4\u5206\u7c7b\uff0c\u63d0\u5347\u53ef\u6301\u7eed\u56de\u6536\u6548\u7387\u3002", "motivation": "\u7eba\u7ec7\u7ea4\u7ef4\u56de\u6536\u5bf9\u51cf\u5c11\u73af\u5883\u5f71\u54cd\u81f3\u5173\u91cd\u8981\uff0c\u9700\u9ad8\u6548\u5206\u7c7b\u65b9\u6cd5\u3002", "method": "\u7814\u7a76\u76d1\u7763\u4e0e\u975e\u76d1\u7763\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u6d4b\u8bd5\u5176\u5728\u4e0d\u540c\u7eba\u7ec7\u7ed3\u6784\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u4f18\u5316\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u548c\u81ea\u7f16\u7801\u5668\u7f51\u7edc\u5728\u591a\u53d8\u6761\u4ef6\u4e0b\u8868\u73b0\u7a33\u5065\u3002", "conclusion": "\u9ad8\u5149\u8c31\u6210\u50cf\u4e0e\u6df1\u5ea6\u5b66\u4e60\u7ed3\u5408\u53ef\u63a8\u52a8\u7eba\u7ec7\u56de\u6536\u7684\u51c6\u786e\u6027\u548c\u7a33\u5065\u6027\u3002"}}
{"id": "2505.03581", "pdf": "https://arxiv.org/pdf/2505.03581", "abs": "https://arxiv.org/abs/2505.03581", "authors": ["Sergey Linok", "Vadim Semenov", "Anastasia Trunova", "Oleg Bulichev", "Dmitry Yudin"], "title": "DyGEnc: Encoding a Sequence of Textual Scene Graphs to Reason and Answer Questions in Dynamic Scenes", "categories": ["cs.CV"], "comment": "8 pages, 5 figures, 6 tables", "summary": "The analysis of events in dynamic environments poses a fundamental challenge\nin the development of intelligent agents and robots capable of interacting with\nhumans. Current approaches predominantly utilize visual models. However, these\nmethods often capture information implicitly from images, lacking interpretable\nspatial-temporal object representations. To address this issue we introduce\nDyGEnc - a novel method for Encoding a Dynamic Graph. This method integrates\ncompressed spatial-temporal structural observation representation with the\ncognitive capabilities of large language models. The purpose of this\nintegration is to enable advanced question answering based on a sequence of\ntextual scene graphs. Extended evaluations on the STAR and AGQA datasets\nindicate that DyGEnc outperforms existing visual methods by a large margin of\n15-25% in addressing queries regarding the history of human-to-object\ninteractions. Furthermore, the proposed method can be seamlessly extended to\nprocess raw input images utilizing foundational models for extracting explicit\ntextual scene graphs, as substantiated by the results of a robotic experiment\nconducted with a wheeled manipulator platform. We hope that these findings will\ncontribute to the implementation of robust and compressed graph-based robotic\nmemory for long-horizon reasoning. Code is available at\ngithub.com/linukc/DyGEnc.", "AI": {"tldr": "DyGEnc\u662f\u4e00\u79cd\u52a8\u6001\u56fe\u7f16\u7801\u65b9\u6cd5\uff0c\u7ed3\u5408\u7a7a\u95f4-\u65f6\u95f4\u7ed3\u6784\u8868\u793a\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u52a8\u6001\u73af\u5883\u4e2d\u4e8b\u4ef6\u5206\u6790\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u89c6\u89c9\u6a21\u578b\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7f3a\u4e4f\u53ef\u89e3\u91ca\u7a7a\u95f4-\u65f6\u95f4\u5bf9\u8c61\u8868\u793a\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faDyGEnc\u65b9\u6cd5\uff0c\u6574\u5408\u538b\u7f29\u7a7a\u95f4-\u65f6\u95f4\u7ed3\u6784\u8868\u793a\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u652f\u6301\u57fa\u4e8e\u6587\u672c\u573a\u666f\u56fe\u7684\u9ad8\u7ea7\u95ee\u7b54\u3002", "result": "\u5728STAR\u548cAGQA\u6570\u636e\u96c6\u4e0a\uff0cDyGEnc\u6bd4\u73b0\u6709\u89c6\u89c9\u65b9\u6cd5\u6027\u80fd\u63d0\u534715-25%\u3002", "conclusion": "DyGEnc\u4e3a\u57fa\u4e8e\u56fe\u7684\u673a\u5668\u4eba\u8bb0\u5fc6\u548c\u957f\u671f\u63a8\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.03597", "pdf": "https://arxiv.org/pdf/2505.03597", "abs": "https://arxiv.org/abs/2505.03597", "authors": ["Zhiyu Pan", "Xiongjun Guan", "Yongjie Duan", "Jianjiang Feng", "Jie Zhou"], "title": "Fixed-Length Dense Fingerprint Representation", "categories": ["cs.CV"], "comment": "Under review at IEEE Transactions on Information Forensics and\n  Security (TIFS)", "summary": "Fixed-length fingerprint representations, which map each fingerprint to a\ncompact and fixed-size feature vector, are computationally efficient and\nwell-suited for large-scale matching. However, designing a robust\nrepresentation that effectively handles diverse fingerprint modalities, pose\nvariations, and noise interference remains a significant challenge. In this\nwork, we propose a fixed-length dense descriptor of fingerprints, and introduce\nFLARE-a fingerprint matching framework that integrates the Fixed-Length dense\ndescriptor with pose-based Alignment and Robust Enhancement. This fixed-length\nrepresentation employs a three-dimensional dense descriptor to effectively\ncapture spatial relationships among fingerprint ridge structures, enabling\nrobust and locally discriminative representations. To ensure consistency within\nthis dense feature space, FLARE incorporates pose-based alignment using\ncomplementary estimation methods, along with dual enhancement strategies that\nrefine ridge clarity while preserving the original fingerprint modality. The\nproposed dense descriptor supports fixed-length representation while\nmaintaining spatial correspondence, enabling fast and accurate similarity\ncomputation. Extensive experiments demonstrate that FLARE achieves superior\nperformance across rolled, plain, latent, and contactless fingerprints,\nsignificantly outperforming existing methods in cross-modality and low-quality\nscenarios. Further analysis validates the effectiveness of the dense descriptor\ndesign, as well as the impact of alignment and enhancement modules on the\naccuracy of dense descriptor matching. Experimental results highlight the\neffectiveness and generalizability of FLARE as a unified and scalable solution\nfor robust fingerprint representation and matching. The implementation and code\nwill be publicly available at https://github.com/Yu-Yy/FLARE.", "AI": {"tldr": "FLARE\u63d0\u51fa\u4e86\u4e00\u79cd\u56fa\u5b9a\u957f\u5ea6\u7684\u6307\u7eb9\u5bc6\u96c6\u63cf\u8ff0\u7b26\uff0c\u7ed3\u5408\u59ff\u6001\u5bf9\u9f50\u548c\u9c81\u68d2\u589e\u5f3a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8de8\u6a21\u6001\u548c\u4f4e\u8d28\u91cf\u6307\u7eb9\u7684\u5339\u914d\u6027\u80fd\u3002", "motivation": "\u56fa\u5b9a\u957f\u5ea6\u6307\u7eb9\u8868\u793a\u5728\u5904\u7406\u591a\u6837\u6a21\u6001\u3001\u59ff\u6001\u53d8\u5316\u548c\u566a\u58f0\u5e72\u6270\u65f6\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u9700\u8981\u66f4\u9c81\u68d2\u548c\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u4e09\u7ef4\u5bc6\u96c6\u63cf\u8ff0\u7b26\u6355\u6349\u6307\u7eb9\u810a\u7ed3\u6784\u7684\u7a7a\u95f4\u5173\u7cfb\uff0c\u7ed3\u5408\u59ff\u6001\u5bf9\u9f50\u548c\u53cc\u589e\u5f3a\u7b56\u7565\u4f18\u5316\u7279\u5f81\u8868\u793a\u3002", "result": "FLARE\u5728\u591a\u79cd\u6307\u7eb9\u7c7b\u578b\u548c\u4f4e\u8d28\u91cf\u573a\u666f\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "FLARE\u662f\u4e00\u79cd\u7edf\u4e00\u4e14\u53ef\u6269\u5c55\u7684\u6307\u7eb9\u8868\u793a\u4e0e\u5339\u914d\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u9ad8\u6548\u6027\u548c\u901a\u7528\u6027\u3002"}}
{"id": "2505.03599", "pdf": "https://arxiv.org/pdf/2505.03599", "abs": "https://arxiv.org/abs/2505.03599", "authors": ["Fengming Lin", "Arezoo Zakeri", "Yidan Xue", "Michael MacRaild", "Haoran Dou", "Zherui Zhou", "Ziwei Zou", "Ali Sarrami-Foroushani", "Jinming Duan", "Alejandro F. Frangi"], "title": "From Pixels to Polygons: A Survey of Deep Learning Approaches for Medical Image-to-Mesh Reconstruction", "categories": ["cs.CV"], "comment": null, "summary": "Deep learning-based medical image-to-mesh reconstruction has rapidly evolved,\nenabling the transformation of medical imaging data into three-dimensional mesh\nmodels that are critical in computational medicine and in silico trials for\nadvancing our understanding of disease mechanisms, and diagnostic and\ntherapeutic techniques in modern medicine. This survey systematically\ncategorizes existing approaches into four main categories: template models,\nstatistical models, generative models, and implicit models. Each category is\nanalysed in detail, examining their methodological foundations, strengths,\nlimitations, and applicability to different anatomical structures and imaging\nmodalities. We provide an extensive evaluation of these methods across various\nanatomical applications, from cardiac imaging to neurological studies,\nsupported by quantitative comparisons using standard metrics. Additionally, we\ncompile and analyze major public datasets available for medical mesh\nreconstruction tasks and discuss commonly used evaluation metrics and loss\nfunctions. The survey identifies current challenges in the field, including\nrequirements for topological correctness, geometric accuracy, and\nmulti-modality integration. Finally, we present promising future research\ndirections in this domain. This systematic review aims to serve as a\ncomprehensive reference for researchers and practitioners in medical image\nanalysis and computational medicine.", "AI": {"tldr": "\u8fd9\u7bc7\u7efc\u8ff0\u7cfb\u7edf\u5206\u7c7b\u4e86\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u533b\u5b66\u56fe\u50cf\u5230\u7f51\u683c\u91cd\u5efa\u65b9\u6cd5\uff0c\u5206\u6790\u4e86\u56db\u7c7b\u4e3b\u8981\u6a21\u578b\uff0c\u8bc4\u4f30\u4e86\u5176\u5e94\u7528\u548c\u6027\u80fd\uff0c\u5e76\u8ba8\u8bba\u4e86\u5f53\u524d\u6311\u6218\u4e0e\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u63a8\u52a8\u533b\u5b66\u56fe\u50cf\u5206\u6790\u9886\u57df\u7684\u53d1\u5c55\uff0c\u4e3a\u8ba1\u7b97\u533b\u5b66\u548c\u865a\u62df\u8bd5\u9a8c\u63d0\u4f9b\u66f4\u7cbe\u786e\u7684\u4e09\u7ef4\u7f51\u683c\u6a21\u578b\uff0c\u4ee5\u652f\u6301\u75be\u75c5\u673a\u5236\u7814\u7a76\u548c\u8bca\u7597\u6280\u672f\u8fdb\u6b65\u3002", "method": "\u5c06\u73b0\u6709\u65b9\u6cd5\u5206\u4e3a\u6a21\u677f\u6a21\u578b\u3001\u7edf\u8ba1\u6a21\u578b\u3001\u751f\u6210\u6a21\u578b\u548c\u9690\u5f0f\u6a21\u578b\u56db\u7c7b\uff0c\u8be6\u7ec6\u5206\u6790\u5176\u65b9\u6cd5\u57fa\u7840\u3001\u4f18\u7f3a\u70b9\u53ca\u9002\u7528\u6027\uff0c\u5e76\u901a\u8fc7\u5b9a\u91cf\u6bd4\u8f83\u548c\u516c\u5f00\u6570\u636e\u96c6\u8bc4\u4f30\u6027\u80fd\u3002", "result": "\u603b\u7ed3\u4e86\u5404\u7c7b\u65b9\u6cd5\u7684\u6027\u80fd\u8868\u73b0\uff0c\u6307\u51fa\u4e86\u62d3\u6251\u6b63\u786e\u6027\u3001\u51e0\u4f55\u7cbe\u5ea6\u548c\u591a\u6a21\u6001\u96c6\u6210\u7b49\u6311\u6218\u3002", "conclusion": "\u4e3a\u533b\u5b66\u56fe\u50cf\u5206\u6790\u548c\u8ba1\u7b97\u533b\u5b66\u9886\u57df\u7684\u7814\u7a76\u8005\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u53c2\u8003\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2505.03603", "pdf": "https://arxiv.org/pdf/2505.03603", "abs": "https://arxiv.org/abs/2505.03603", "authors": ["Y. B. Wang", "S. Z. Zhou", "J. F. Wu", "T. Hu", "J. N. Zhang", "Y. Liu"], "title": "PAHA: Parts-Aware Audio-Driven Human Animation with Diffusion Model", "categories": ["cs.CV", "cs.MM"], "comment": null, "summary": "Audio-driven human animation technology is widely used in human-computer\ninteraction, and the emergence of diffusion models has further advanced its\ndevelopment. Currently, most methods rely on multi-stage generation and\nintermediate representations, resulting in long inference time and issues with\ngeneration quality in specific foreground regions and audio-motion consistency.\nThese shortcomings are primarily due to the lack of localized fine-grained\nsupervised guidance. To address above challenges, we propose PAHA, an\nend-to-end audio-driven upper-body human animation framework with diffusion\nmodel. We introduce two key methods: Parts-Aware Re-weighting (PAR) and Parts\nConsistency Enhancement (PCE). PAR dynamically adjusts regional training loss\nweights based on pose confidence scores, effectively improving visual quality.\nPCE constructs and trains diffusion-based regional audio-visual classifiers to\nimprove the consistency of motion and co-speech audio. Afterwards, we design\ntwo novel inference guidance methods for the foregoing classifiers, Sequential\nGuidance (SG) and Differential Guidance (DG), to balance efficiency and quality\nrespectively. Additionally, we build CNAS, the first public Chinese News Anchor\nSpeech dataset, to advance research and validation in this field. Extensive\nexperimental results and user studies demonstrate that PAHA significantly\noutperforms existing methods in audio-motion alignment and video-related\nevaluations. The codes and CNAS dataset will be released upon acceptance.", "AI": {"tldr": "PAHA\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u7aef\u5230\u7aef\u97f3\u9891\u9a71\u52a8\u4e0a\u534a\u8eab\u4eba\u4f53\u52a8\u753b\u6846\u67b6\uff0c\u901a\u8fc7PAR\u548cPCE\u65b9\u6cd5\u63d0\u5347\u751f\u6210\u8d28\u91cf\u4e0e\u97f3\u9891-\u52a8\u4f5c\u4e00\u81f4\u6027\uff0c\u5e76\u8bbe\u8ba1\u4e86SG\u548cDG\u63a8\u7406\u6307\u5bfc\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u591a\u4f9d\u8d56\u591a\u9636\u6bb5\u751f\u6210\u548c\u4e2d\u95f4\u8868\u793a\uff0c\u5bfc\u81f4\u63a8\u7406\u65f6\u95f4\u957f\u4e14\u751f\u6210\u8d28\u91cf\u4e0e\u97f3\u9891-\u52a8\u4f5c\u4e00\u81f4\u6027\u4e0d\u8db3\u3002", "method": "\u63d0\u51faPAR\u52a8\u6001\u8c03\u6574\u533a\u57df\u8bad\u7ec3\u635f\u5931\u6743\u91cd\uff0cPCE\u6784\u5efa\u57fa\u4e8e\u6269\u6563\u7684\u533a\u57df\u97f3\u9891-\u89c6\u89c9\u5206\u7c7b\u5668\uff0c\u5e76\u8bbe\u8ba1SG\u548cDG\u63a8\u7406\u6307\u5bfc\u65b9\u6cd5\u3002", "result": "PAHA\u5728\u97f3\u9891-\u52a8\u4f5c\u5bf9\u9f50\u548c\u89c6\u9891\u76f8\u5173\u8bc4\u4f30\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "PAHA\u901a\u8fc7\u5c40\u90e8\u7ec6\u7c92\u5ea6\u76d1\u7763\u6307\u5bfc\u89e3\u51b3\u4e86\u73b0\u6709\u95ee\u9898\uff0c\u5e76\u53d1\u5e03\u4e86\u9996\u4e2a\u4e2d\u6587\u65b0\u95fb\u4e3b\u64ad\u8bed\u97f3\u6570\u636e\u96c6CNAS\u3002"}}
{"id": "2505.03610", "pdf": "https://arxiv.org/pdf/2505.03610", "abs": "https://arxiv.org/abs/2505.03610", "authors": ["Fangling Jiang", "Qi Li", "Bing Liu", "Weining Wang", "Caifeng Shan", "Zhenan Sun", "Ming-Hsuan Yang"], "title": "Learning Knowledge-based Prompts for Robust 3D Mask Presentation Attack Detection", "categories": ["cs.CV"], "comment": null, "summary": "3D mask presentation attack detection is crucial for protecting face\nrecognition systems against the rising threat of 3D mask attacks. While most\nexisting methods utilize multimodal features or remote photoplethysmography\n(rPPG) signals to distinguish between real faces and 3D masks, they face\nsignificant challenges, such as the high costs associated with multimodal\nsensors and limited generalization ability. Detection-related text descriptions\noffer concise, universal information and are cost-effective to obtain. However,\nthe potential of vision-language multimodal features for 3D mask presentation\nattack detection remains unexplored. In this paper, we propose a novel\nknowledge-based prompt learning framework to explore the strong generalization\ncapability of vision-language models for 3D mask presentation attack detection.\nSpecifically, our approach incorporates entities and triples from knowledge\ngraphs into the prompt learning process, generating fine-grained, task-specific\nexplicit prompts that effectively harness the knowledge embedded in pre-trained\nvision-language models. Furthermore, considering different input images may\nemphasize distinct knowledge graph elements, we introduce a visual-specific\nknowledge filter based on an attention mechanism to refine relevant elements\naccording to the visual context. Additionally, we leverage causal graph theory\ninsights into the prompt learning process to further enhance the generalization\nability of our method. During training, a spurious correlation elimination\nparadigm is employed, which removes category-irrelevant local image patches\nusing guidance from knowledge-based text features, fostering the learning of\ngeneralized causal prompts that align with category-relevant local patches.\nExperimental results demonstrate that the proposed method achieves\nstate-of-the-art intra- and cross-scenario detection performance on benchmark\ndatasets.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u63d0\u793a\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u7528\u4e8e3D\u9762\u5177\u653b\u51fb\u68c0\u6d4b\uff0c\u7ed3\u5408\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u548c\u56e0\u679c\u56fe\u7406\u8bba\uff0c\u63d0\u5347\u4e86\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u6210\u672c\u9ad8\u4e14\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff0c\u800c\u89c6\u89c9-\u8bed\u8a00\u591a\u6a21\u6001\u7279\u5f81\u7684\u6f5c\u529b\u5c1a\u672a\u88ab\u63a2\u7d22\u3002", "method": "\u7ed3\u5408\u77e5\u8bc6\u56fe\u8c31\u5b9e\u4f53\u548c\u4e09\u91cd\u4fe1\u606f\u751f\u6210\u4efb\u52a1\u7279\u5b9a\u63d0\u793a\uff0c\u5e76\u5f15\u5165\u89c6\u89c9\u7279\u5b9a\u77e5\u8bc6\u8fc7\u6ee4\u5668\u548c\u56e0\u679c\u56fe\u7406\u8bba\u4f18\u5316\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u4f18\u7684\u8de8\u573a\u666f\u68c0\u6d4b\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u77e5\u8bc6\u9a71\u52a8\u548c\u56e0\u679c\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e863D\u9762\u5177\u653b\u51fb\u68c0\u6d4b\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2505.03611", "pdf": "https://arxiv.org/pdf/2505.03611", "abs": "https://arxiv.org/abs/2505.03611", "authors": ["Fangling Jiang", "Qi Li", "Weining Wang", "Wei Shen", "Bing Liu", "Zhenan Sun"], "title": "Learning Unknown Spoof Prompts for Generalized Face Anti-Spoofing Using Only Real Face Images", "categories": ["cs.CV"], "comment": null, "summary": "Face anti-spoofing is a critical technology for ensuring the security of face\nrecognition systems. However, its ability to generalize across diverse\nscenarios remains a significant challenge. In this paper, we attribute the\nlimited generalization ability to two key factors: covariate shift, which\narises from external data collection variations, and semantic shift, which\nresults from substantial differences in emerging attack types. To address both\nchallenges, we propose a novel approach for learning unknown spoof prompts,\nrelying solely on real face images from a single source domain. Our method\ngenerates textual prompts for real faces and potential unknown spoof attacks by\nleveraging the general knowledge embedded in vision-language models, thereby\nenhancing the model's ability to generalize to unseen target domains.\nSpecifically, we introduce a diverse spoof prompt optimization framework to\nlearn effective prompts. This framework constrains unknown spoof prompts within\na relaxed prior knowledge space while maximizing their distance from real face\nimages. Moreover, it enforces semantic independence among different spoof\nprompts to capture a broad range of spoof patterns. Experimental results on\nnine datasets demonstrate that the learned prompts effectively transfer the\nknowledge of vision-language models, enabling state-of-the-art generalization\nability against diverse unknown attack types across unseen target domains\nwithout using any spoof face images.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u6587\u672c\u63d0\u793a\u6765\u63d0\u5347\u4eba\u8138\u9632\u4f2a\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u534f\u53d8\u91cf\u504f\u79fb\u548c\u8bed\u4e49\u504f\u79fb\u95ee\u9898\u3002", "motivation": "\u4eba\u8138\u9632\u4f2a\u6280\u672f\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u4e3b\u8981\u53d7\u534f\u53d8\u91cf\u504f\u79fb\u548c\u8bed\u4e49\u504f\u79fb\u7684\u5f71\u54cd\u3002", "method": "\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u751f\u6210\u771f\u5b9e\u4eba\u8138\u548c\u6f5c\u5728\u672a\u77e5\u653b\u51fb\u7684\u6587\u672c\u63d0\u793a\uff0c\u901a\u8fc7\u591a\u6837\u5316\u63d0\u793a\u4f18\u5316\u6846\u67b6\u5b66\u4e60\u6709\u6548\u63d0\u793a\u3002", "result": "\u5728\u4e5d\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u65e0\u9700\u4f7f\u7528\u4f2a\u9020\u4eba\u8138\u56fe\u50cf\u5373\u53ef\u5b9e\u73b0\u4f18\u5f02\u7684\u6cdb\u5316\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u6587\u672c\u63d0\u793a\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4eba\u8138\u9632\u4f2a\u7cfb\u7edf\u5bf9\u672a\u77e5\u653b\u51fb\u7c7b\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2505.03621", "pdf": "https://arxiv.org/pdf/2505.03621", "abs": "https://arxiv.org/abs/2505.03621", "authors": ["Yiping Xie", "Bo Zhao", "Mingtong Dai", "Jian-Ping Zhou", "Yue Sun", "Tao Tan", "Weicheng Xie", "Linlin Shen", "Zitong Yu"], "title": "PhysLLM: Harnessing Large Language Models for Cross-Modal Remote Physiological Sensing", "categories": ["cs.CV"], "comment": null, "summary": "Remote photoplethysmography (rPPG) enables non-contact physiological\nmeasurement but remains highly susceptible to illumination changes, motion\nartifacts, and limited temporal modeling. Large Language Models (LLMs) excel at\ncapturing long-range dependencies, offering a potential solution but struggle\nwith the continuous, noise-sensitive nature of rPPG signals due to their\ntext-centric design. To bridge this gap, we introduce PhysLLM, a collaborative\noptimization framework that synergizes LLMs with domain-specific rPPG\ncomponents. Specifically, the Text Prototype Guidance (TPG) strategy is\nproposed to establish cross-modal alignment by projecting hemodynamic features\ninto LLM-interpretable semantic space, effectively bridging the\nrepresentational gap between physiological signals and linguistic tokens.\nBesides, a novel Dual-Domain Stationary (DDS) Algorithm is proposed for\nresolving signal instability through adaptive time-frequency domain feature\nre-weighting. Finally, rPPG task-specific cues systematically inject\nphysiological priors through physiological statistics, environmental contextual\nanswering, and task description, leveraging cross-modal learning to integrate\nboth visual and textual information, enabling dynamic adaptation to challenging\nscenarios like variable illumination and subject movements. Evaluation on four\nbenchmark datasets, PhysLLM achieves state-of-the-art accuracy and robustness,\ndemonstrating superior generalization across lighting variations and motion\nscenarios.", "AI": {"tldr": "PhysLLM\u6846\u67b6\u7ed3\u5408LLMs\u4e0erPPG\u7ec4\u4ef6\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u5bf9\u9f50\u548c\u53cc\u57df\u7279\u5f81\u91cd\u52a0\u6743\uff0c\u63d0\u5347\u4e86\u975e\u63a5\u89e6\u5f0f\u751f\u7406\u6d4b\u91cf\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "rPPG\u6280\u672f\u6613\u53d7\u5149\u7167\u53d8\u5316\u548c\u8fd0\u52a8\u4f2a\u5f71\u5f71\u54cd\uff0cLLMs\u867d\u64c5\u957f\u957f\u7a0b\u4f9d\u8d56\u5efa\u6a21\uff0c\u4f46\u96be\u4ee5\u76f4\u63a5\u5904\u7406\u566a\u58f0\u654f\u611f\u7684rPPG\u4fe1\u53f7\u3002", "method": "\u63d0\u51faText Prototype Guidance\uff08TPG\uff09\u7b56\u7565\u5b9e\u73b0\u8de8\u6a21\u6001\u5bf9\u9f50\uff0c\u5e76\u8bbe\u8ba1Dual-Domain Stationary\uff08DDS\uff09\u7b97\u6cd5\u8fdb\u884c\u65f6\u9891\u57df\u7279\u5f81\u91cd\u52a0\u6743\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cPhysLLM\u5b9e\u73b0\u4e86\u6700\u4f18\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u5c24\u5176\u5728\u5149\u7167\u53d8\u5316\u548c\u8fd0\u52a8\u573a\u666f\u4e2d\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "PhysLLM\u901a\u8fc7\u7ed3\u5408LLMs\u4e0e\u9886\u57df\u77e5\u8bc6\uff0c\u663e\u8457\u63d0\u5347\u4e86rPPG\u7684\u6027\u80fd\uff0c\u4e3a\u52a8\u6001\u73af\u5883\u4e0b\u7684\u751f\u7406\u76d1\u6d4b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.03623", "pdf": "https://arxiv.org/pdf/2505.03623", "abs": "https://arxiv.org/abs/2505.03623", "authors": ["Alessandro Simoni", "Francesco Pelosin"], "title": "Bounding Box-Guided Diffusion for Synthesizing Industrial Images and Segmentation Map", "categories": ["cs.CV"], "comment": "Accepted at Synthetic Data for Computer Vision Workshop - CVPR 2025", "summary": "Synthetic dataset generation in Computer Vision, particularly for industrial\napplications, is still underexplored. Industrial defect segmentation, for\ninstance, requires highly accurate labels, yet acquiring such data is costly\nand time-consuming. To address this challenge, we propose a novel\ndiffusion-based pipeline for generating high-fidelity industrial datasets with\nminimal supervision. Our approach conditions the diffusion model on enriched\nbounding box representations to produce precise segmentation masks, ensuring\nrealistic and accurately localized defect synthesis. Compared to existing\nlayout-conditioned generative methods, our approach improves defect consistency\nand spatial accuracy. We introduce two quantitative metrics to evaluate the\neffectiveness of our method and assess its impact on a downstream segmentation\ntask trained on real and synthetic data. Our results demonstrate that\ndiffusion-based synthesis can bridge the gap between artificial and real-world\nindustrial data, fostering more reliable and cost-efficient segmentation\nmodels. The code is publicly available at\nhttps://github.com/covisionlab/diffusion_labeling.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u751f\u6210\u9ad8\u4fdd\u771f\u5de5\u4e1a\u7f3a\u9677\u6570\u636e\u96c6\uff0c\u51cf\u5c11\u6807\u6ce8\u6210\u672c\u3002", "motivation": "\u5de5\u4e1a\u7f3a\u9677\u5206\u5272\u9700\u8981\u9ad8\u7cbe\u5ea6\u6807\u6ce8\u6570\u636e\uff0c\u4f46\u83b7\u53d6\u6210\u672c\u9ad8\u4e14\u8017\u65f6\u3002", "method": "\u5229\u7528\u6269\u6563\u6a21\u578b\u7ed3\u5408\u8fb9\u754c\u6846\u8868\u793a\u751f\u6210\u7cbe\u786e\u5206\u5272\u63a9\u7801\u3002", "result": "\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\uff0c\u63d0\u5347\u4e86\u7f3a\u9677\u4e00\u81f4\u6027\u548c\u7a7a\u95f4\u51c6\u786e\u6027\uff0c\u5e76\u901a\u8fc7\u4e0b\u6e38\u4efb\u52a1\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "conclusion": "\u6269\u6563\u5408\u6210\u65b9\u6cd5\u80fd\u7f29\u5c0f\u4eba\u5de5\u6570\u636e\u4e0e\u771f\u5b9e\u5de5\u4e1a\u6570\u636e\u7684\u5dee\u8ddd\uff0c\u63d0\u5347\u5206\u5272\u6a21\u578b\u7684\u53ef\u9760\u6027\u548c\u6210\u672c\u6548\u76ca\u3002"}}
{"id": "2505.03631", "pdf": "https://arxiv.org/pdf/2505.03631", "abs": "https://arxiv.org/abs/2505.03631", "authors": ["Linhan Cao", "Wei Sun", "Kaiwei Zhang", "Yicong Peng", "Guangtao Zhai", "Xiongkuo Min"], "title": "Breaking Annotation Barriers: Generalized Video Quality Assessment via Ranking-based Self-Supervision", "categories": ["cs.CV"], "comment": null, "summary": "Video quality assessment (VQA) is essential for quantifying perceptual\nquality in various video processing workflows, spanning from camera capture\nsystems to over-the-top streaming platforms. While recent supervised VQA models\nhave made substantial progress, the reliance on manually annotated datasets --\na process that is labor-intensive, costly, and difficult to scale up -- has\nhindered further optimization of their generalization to unseen video content\nand distortions. To bridge this gap, we introduce a self-supervised learning\nframework for VQA to learn quality assessment capabilities from large-scale,\nunlabeled web videos. Our approach leverages a \\textbf{learning-to-rank}\nparadigm to train a large multimodal model (LMM) on video pairs automatically\nlabeled via two manners, including quality pseudo-labeling by existing VQA\nmodels and relative quality ranking based on synthetic distortion simulations.\nFurthermore, we introduce a novel \\textbf{iterative self-improvement training\nstrategy}, where the trained model acts an improved annotator to iteratively\nrefine the annotation quality of training data. By training on a dataset\n$10\\times$ larger than the existing VQA benchmarks, our model: (1) achieves\nzero-shot performance on in-domain VQA benchmarks that matches or surpasses\nsupervised models; (2) demonstrates superior out-of-distribution (OOD)\ngeneralization across diverse video content and distortions; and (3) sets a new\nstate-of-the-art when fine-tuned on human-labeled datasets. Extensive\nexperimental results validate the effectiveness of our self-supervised approach\nin training generalized VQA models. The datasets and code will be publicly\nreleased to facilitate future research.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u89c6\u9891\u8d28\u91cf\u8bc4\u4f30\uff08VQA\uff09\uff0c\u901a\u8fc7\u5927\u89c4\u6a21\u65e0\u6807\u7b7e\u7f51\u7edc\u89c6\u9891\u5b66\u4e60\u8d28\u91cf\u8bc4\u4f30\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u76d1\u7763VQA\u6a21\u578b\u4f9d\u8d56\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u6210\u672c\u9ad8\u4e14\u96be\u4ee5\u6269\u5c55\uff0c\u9650\u5236\u4e86\u6a21\u578b\u5bf9\u672a\u77e5\u89c6\u9891\u5185\u5bb9\u548c\u5931\u771f\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u91c7\u7528\u5b66\u4e60\u6392\u5e8f\u8303\u5f0f\u8bad\u7ec3\u591a\u6a21\u6001\u6a21\u578b\uff0c\u5229\u7528\u73b0\u6709VQA\u6a21\u578b\u751f\u6210\u7684\u4f2a\u6807\u7b7e\u548c\u57fa\u4e8e\u5408\u6210\u5931\u771f\u6a21\u62df\u7684\u76f8\u5bf9\u8d28\u91cf\u6392\u5e8f\u8fdb\u884c\u81ea\u52a8\u6807\u6ce8\uff0c\u5e76\u5f15\u5165\u8fed\u4ee3\u81ea\u6539\u8fdb\u8bad\u7ec3\u7b56\u7565\u3002", "result": "\u6a21\u578b\u5728\u96f6\u6837\u672c\u60c5\u51b5\u4e0b\u6027\u80fd\u5339\u914d\u6216\u8d85\u8d8a\u76d1\u7763\u6a21\u578b\uff0c\u5c55\u793a\u4e86\u4f18\u8d8a\u7684\u5206\u5e03\u5916\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u5728\u5fae\u8c03\u540e\u8fbe\u5230\u65b0SOTA\u3002", "conclusion": "\u81ea\u76d1\u7763\u65b9\u6cd5\u6709\u6548\u8bad\u7ec3\u4e86\u6cdb\u5316\u80fd\u529b\u5f3a\u7684VQA\u6a21\u578b\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u6570\u636e\u96c6\u548c\u4ee3\u7801\u652f\u6301\u3002"}}
{"id": "2505.03638", "pdf": "https://arxiv.org/pdf/2505.03638", "abs": "https://arxiv.org/abs/2505.03638", "authors": ["Jiawan Li", "Fei Zhou", "Zhipeng Zhong", "Jiongzhi Lin", "Guoping Qiu"], "title": "Towards Smart Point-and-Shoot Photography", "categories": ["cs.CV"], "comment": "CVPR2025 Accepted", "summary": "Hundreds of millions of people routinely take photos using their smartphones\nas point and shoot (PAS) cameras, yet very few would have the photography\nskills to compose a good shot of a scene. While traditional PAS cameras have\nbuilt-in functions to ensure a photo is well focused and has the right\nbrightness, they cannot tell the users how to compose the best shot of a scene.\nIn this paper, we present a first of its kind smart point and shoot (SPAS)\nsystem to help users to take good photos. Our SPAS proposes to help users to\ncompose a good shot of a scene by automatically guiding the users to adjust the\ncamera pose live on the scene. We first constructed a large dataset containing\n320K images with camera pose information from 4000 scenes. We then developed an\ninnovative CLIP-based Composition Quality Assessment (CCQA) model to assign\npseudo labels to these images. The CCQA introduces a unique learnable text\nembedding technique to learn continuous word embeddings capable of discerning\nsubtle visual quality differences in the range covered by five levels of\nquality description words {bad, poor, fair, good, perfect}. And finally we have\ndeveloped a camera pose adjustment model (CPAM) which first determines if the\ncurrent view can be further improved and if so it outputs the adjust suggestion\nin the form of two camera pose adjustment angles. The two tasks of CPAM make\ndecisions in a sequential manner and each involves different sets of training\nsamples, we have developed a mixture-of-experts model with a gated loss\nfunction to train the CPAM in an end-to-end manner. We will present extensive\nresults to demonstrate the performances of our SPAS system using publicly\navailable image composition datasets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u667a\u80fd\u70b9\u62cd\uff08SPAS\uff09\u7cfb\u7edf\uff0c\u901a\u8fc7\u5b9e\u65f6\u8c03\u6574\u76f8\u673a\u59ff\u6001\u5e2e\u52a9\u7528\u6237\u62cd\u6444\u66f4\u597d\u7684\u7167\u7247\u3002\u7cfb\u7edf\u5305\u62ec\u4e00\u4e2a\u57fa\u4e8eCLIP\u7684\u6784\u56fe\u8d28\u91cf\u8bc4\u4f30\u6a21\u578b\uff08CCQA\uff09\u548c\u4e00\u4e2a\u76f8\u673a\u59ff\u6001\u8c03\u6574\u6a21\u578b\uff08CPAM\uff09\u3002", "motivation": "\u4f20\u7edf\u70b9\u62cd\u76f8\u673a\u65e0\u6cd5\u6307\u5bfc\u7528\u6237\u5982\u4f55\u6784\u56fe\uff0c\u800c\u667a\u80fd\u624b\u673a\u7528\u6237\u666e\u904d\u7f3a\u4e4f\u6444\u5f71\u6280\u5de7\u3002\u56e0\u6b64\uff0c\u5f00\u53d1\u4e00\u4e2a\u80fd\u5b9e\u65f6\u6307\u5bfc\u7528\u6237\u8c03\u6574\u76f8\u673a\u59ff\u6001\u7684\u7cfb\u7edf\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002", "method": "1. \u6784\u5efa\u5305\u542b32\u4e07\u5f20\u56fe\u50cf\u7684\u6570\u636e\u96c6\uff1b2. \u5f00\u53d1CCQA\u6a21\u578b\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u6587\u672c\u5d4c\u5165\u6280\u672f\u8bc4\u4f30\u6784\u56fe\u8d28\u91cf\uff1b3. \u5f00\u53d1CPAM\u6a21\u578b\uff0c\u91c7\u7528\u4e13\u5bb6\u6df7\u5408\u6a21\u578b\u548c\u95e8\u63a7\u635f\u5931\u51fd\u6570\uff0c\u8f93\u51fa\u76f8\u673a\u59ff\u6001\u8c03\u6574\u5efa\u8bae\u3002", "result": "\u7cfb\u7edf\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u5c55\u793a\u4e86\u826f\u597d\u7684\u6027\u80fd\uff0c\u80fd\u591f\u6709\u6548\u6307\u5bfc\u7528\u6237\u8c03\u6574\u76f8\u673a\u59ff\u6001\u4ee5\u6539\u5584\u6784\u56fe\u3002", "conclusion": "SPAS\u7cfb\u7edf\u901a\u8fc7\u521b\u65b0\u7684CCQA\u548cCPAM\u6a21\u578b\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u5b9e\u65f6\u6784\u56fe\u6307\u5bfc\uff0c\u4e3a\u666e\u901a\u7528\u6237\u63d0\u4f9b\u4e86\u4e13\u4e1a\u7ea7\u7684\u6444\u5f71\u8f85\u52a9\u3002"}}
{"id": "2505.03654", "pdf": "https://arxiv.org/pdf/2505.03654", "abs": "https://arxiv.org/abs/2505.03654", "authors": ["Yifan Xiang", "Zhenxi Zhang", "Bin Li", "Yixuan Weng", "Shoujun Zhou", "Yangfan He", "Keqin Li"], "title": "ReGraP-LLaVA: Reasoning enabled Graph-based Personalized Large Language and Vision Assistant", "categories": ["cs.CV", "cs.AI"], "comment": "Work in progress", "summary": "Recent advances in personalized MLLMs enable effective capture of\nuser-specific concepts, supporting both recognition of personalized concepts\nand contextual captioning. However, humans typically explore and reason over\nrelations among objects and individuals, transcending surface-level information\nto achieve more personalized and contextual understanding. To this end,\nexisting methods may face three main limitations: Their training data lacks\nmulti-object sets in which relations among objects are learnable. Building on\nthe limited training data, their models overlook the relations between\ndifferent personalized concepts and fail to reason over them. Their experiments\nmainly focus on a single personalized concept, where evaluations are limited to\nrecognition and captioning tasks. To address the limitations, we present a new\ndataset named ReGraP, consisting of 120 sets of personalized knowledge. Each\nset includes images, KGs, and CoT QA pairs derived from the KGs, enabling more\nstructured and sophisticated reasoning pathways. We propose ReGraP-LLaVA, an\nMLLM trained with the corresponding KGs and CoT QA pairs, where soft and hard\ngraph prompting methods are designed to align KGs within the model's semantic\nspace. We establish the ReGraP Benchmark, which contains diverse task types:\nmultiple-choice, fill-in-the-blank, True/False, and descriptive questions in\nboth open- and closed-ended settings. The proposed benchmark is designed to\nevaluate the relational reasoning and knowledge-connection capability of\npersonalized MLLMs. We conduct experiments on the proposed ReGraP-LLaVA and\nother competitive MLLMs. Results show that the proposed model not only learns\npersonalized knowledge but also performs relational reasoning in responses,\nachieving the SoTA performance compared with the competitive methods. All the\ncodes and datasets are released at: https://github.com/xyfyyds/ReGraP.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faReGraP\u6570\u636e\u96c6\u548cReGraP-LLaVA\u6a21\u578b\uff0c\u89e3\u51b3\u73b0\u6709\u4e2a\u6027\u5316MLLMs\u5728\u5173\u7cfb\u63a8\u7406\u548c\u591a\u6982\u5ff5\u5b66\u4e60\u4e0a\u7684\u4e0d\u8db3\uff0c\u901a\u8fc7\u56fe\u63d0\u793a\u65b9\u6cd5\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u4e2a\u6027\u5316\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u4e2d\u7f3a\u4e4f\u5bf9\u591a\u5bf9\u8c61\u5173\u7cfb\u7684\u5b66\u4e60\u548c\u63a8\u7406\u80fd\u529b\uff0c\u4e14\u5b9e\u9a8c\u5c40\u9650\u4e8e\u5355\u4e00\u6982\u5ff5\u4efb\u52a1\u3002", "method": "\u63d0\u51faReGraP\u6570\u636e\u96c6\uff08\u542b\u56fe\u50cf\u3001\u77e5\u8bc6\u56fe\u8c31\u548c\u63a8\u7406\u95ee\u7b54\u5bf9\uff09\uff0c\u8bbe\u8ba1ReGraP-LLaVA\u6a21\u578b\uff0c\u91c7\u7528\u8f6f\u786c\u56fe\u63d0\u793a\u65b9\u6cd5\u5bf9\u9f50\u77e5\u8bc6\u56fe\u8c31\u3002", "result": "ReGraP-LLaVA\u5728\u5173\u7cfb\u63a8\u7406\u548c\u77e5\u8bc6\u8fde\u63a5\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8fbe\u5230\u5f53\u524d\u6700\u4f18\u6027\u80fd\u3002", "conclusion": "ReGraP\u6570\u636e\u96c6\u548c\u6a21\u578b\u6709\u6548\u63d0\u5347\u4e86MLLMs\u5728\u591a\u6982\u5ff5\u5173\u7cfb\u63a8\u7406\u4e0a\u7684\u80fd\u529b\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2505.03662", "pdf": "https://arxiv.org/pdf/2505.03662", "abs": "https://arxiv.org/abs/2505.03662", "authors": ["Xin Du", "Francesca M. Cozzi", "Rajesh Jena"], "title": "Revolutionizing Brain Tumor Imaging: Generating Synthetic 3D FA Maps from T1-Weighted MRI using CycleGAN Models", "categories": ["cs.CV", "cs.AI", "68U10"], "comment": "9 pages", "summary": "Fractional anisotropy (FA) and directionally encoded colour (DEC) maps are\nessential for evaluating white matter integrity and structural connectivity in\nneuroimaging. However, the spatial misalignment between FA maps and\ntractography atlases hinders their effective integration into predictive\nmodels. To address this issue, we propose a CycleGAN based approach for\ngenerating FA maps directly from T1-weighted MRI scans, representing the first\napplication of this technique to both healthy and tumour-affected tissues. Our\nmodel, trained on unpaired data, produces high fidelity maps, which have been\nrigorously evaluated using Structural Similarity Index (SSIM) and Peak\nSignal-to-Noise Ratio (PSNR), demonstrating particularly robust performance in\ntumour regions. Radiological assessments further underscore the model's\npotential to enhance clinical workflows by providing an AI-driven alternative\nthat reduces the necessity for additional scans.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eCycleGAN\u7684\u65b9\u6cd5\uff0c\u76f4\u63a5\u4eceT1\u52a0\u6743MRI\u626b\u63cf\u751f\u6210FA\u56fe\uff0c\u89e3\u51b3\u4e86FA\u56fe\u4e0e\u7ea4\u7ef4\u675f\u8ffd\u8e2a\u56fe\u8c31\u7684\u7a7a\u95f4\u4e0d\u5bf9\u9f50\u95ee\u9898\u3002", "motivation": "FA\u548cDEC\u56fe\u5bf9\u8bc4\u4f30\u767d\u8d28\u5b8c\u6574\u6027\u81f3\u5173\u91cd\u8981\uff0c\u4f46FA\u56fe\u4e0e\u7ea4\u7ef4\u675f\u8ffd\u8e2a\u56fe\u8c31\u7684\u7a7a\u95f4\u4e0d\u5bf9\u9f50\u963b\u788d\u4e86\u5176\u5728\u9884\u6d4b\u6a21\u578b\u4e2d\u7684\u6709\u6548\u6574\u5408\u3002", "method": "\u4f7f\u7528CycleGAN\u65b9\u6cd5\uff0c\u57fa\u4e8e\u672a\u914d\u5bf9\u6570\u636e\u8bad\u7ec3\u6a21\u578b\uff0c\u76f4\u63a5\u4eceT1\u52a0\u6743MRI\u751f\u6210FA\u56fe\u3002", "result": "\u6a21\u578b\u751f\u6210\u7684FA\u56fe\u5177\u6709\u9ad8\u4fdd\u771f\u5ea6\uff0c\u5c24\u5176\u5728\u80bf\u7624\u533a\u57df\u8868\u73b0\u4f18\u5f02\uff0cSSIM\u548cPSNR\u8bc4\u4f30\u7ed3\u679c\u826f\u597d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u4e34\u5e8a\u5de5\u4f5c\u6d41\u63d0\u4f9b\u4e86AI\u9a71\u52a8\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u51cf\u5c11\u4e86\u989d\u5916\u626b\u63cf\u7684\u9700\u6c42\uff0c\u5177\u6709\u6f5c\u5728\u4e34\u5e8a\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2505.03667", "pdf": "https://arxiv.org/pdf/2505.03667", "abs": "https://arxiv.org/abs/2505.03667", "authors": ["Fu Feng", "Yucheng Xie", "Xu Yang", "Jing Wang", "Xin Geng"], "title": "Distribution-Conditional Generation: From Class Distribution to Creative Generation", "categories": ["cs.CV"], "comment": null, "summary": "Text-to-image (T2I) diffusion models are effective at producing semantically\naligned images, but their reliance on training data distributions limits their\nability to synthesize truly novel, out-of-distribution concepts. Existing\nmethods typically enhance creativity by combining pairs of known concepts,\nyielding compositions that, while out-of-distribution, remain linguistically\ndescribable and bounded within the existing semantic space. Inspired by the\nsoft probabilistic outputs of classifiers on ambiguous inputs, we propose\nDistribution-Conditional Generation, a novel formulation that models creativity\nas image synthesis conditioned on class distributions, enabling semantically\nunconstrained creative generation. Building on this, we propose DisTok, an\nencoder-decoder framework that maps class distributions into a latent space and\ndecodes them into tokens of creative concept. DisTok maintains a dynamic\nconcept pool and iteratively sampling and fusing concept pairs, enabling the\ngeneration of tokens aligned with increasingly complex class distributions. To\nenforce distributional consistency, latent vectors sampled from a Gaussian\nprior are decoded into tokens and rendered into images, whose class\ndistributions-predicted by a vision-language model-supervise the alignment\nbetween input distributions and the visual semantics of generated tokens. The\nresulting tokens are added to the concept pool for subsequent composition.\nExtensive experiments demonstrate that DisTok, by unifying\ndistribution-conditioned fusion and sampling-based synthesis, enables efficient\nand flexible token-level generation, achieving state-of-the-art performance\nwith superior text-image alignment and human preference scores.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDisTok\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u5e03\u6761\u4ef6\u751f\u6210\u548c\u52a8\u6001\u6982\u5ff5\u6c60\u5b9e\u73b0\u521b\u610f\u56fe\u50cf\u5408\u6210\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u7684\u9650\u5236\u3002", "motivation": "\u4f20\u7edf\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u4f9d\u8d56\u8bad\u7ec3\u6570\u636e\u5206\u5e03\uff0c\u65e0\u6cd5\u751f\u6210\u771f\u6b63\u65b0\u9896\u7684\u3001\u8d85\u51fa\u5206\u5e03\u7684\u6982\u5ff5\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u901a\u8fc7\u7ec4\u5408\u5df2\u77e5\u6982\u5ff5\u6765\u589e\u5f3a\u521b\u610f\uff0c\u4f46\u4ecd\u53d7\u9650\u4e8e\u8bed\u4e49\u7a7a\u95f4\u3002", "method": "\u63d0\u51faDistribution-Conditional Generation\uff0c\u5c06\u521b\u610f\u5efa\u6a21\u4e3a\u57fa\u4e8e\u7c7b\u522b\u5206\u5e03\u7684\u56fe\u50cf\u5408\u6210\u3002DisTok\u6846\u67b6\u901a\u8fc7\u7f16\u7801-\u89e3\u7801\u5c06\u7c7b\u522b\u5206\u5e03\u6620\u5c04\u5230\u6f5c\u5728\u7a7a\u95f4\uff0c\u5e76\u52a8\u6001\u8fed\u4ee3\u751f\u6210\u521b\u610f\u6982\u5ff5\u3002", "result": "DisTok\u5728\u6587\u672c-\u56fe\u50cf\u5bf9\u9f50\u548c\u4eba\u7c7b\u504f\u597d\u8bc4\u5206\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u4ee4\u724c\u7ea7\u751f\u6210\u3002", "conclusion": "DisTok\u901a\u8fc7\u5206\u5e03\u6761\u4ef6\u878d\u5408\u548c\u57fa\u4e8e\u91c7\u6837\u7684\u5408\u6210\uff0c\u5b9e\u73b0\u4e86\u7075\u6d3b\u4e14\u9ad8\u6548\u7684\u521b\u610f\u56fe\u50cf\u751f\u6210\uff0c\u6027\u80fd\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002"}}
{"id": "2505.03679", "pdf": "https://arxiv.org/pdf/2505.03679", "abs": "https://arxiv.org/abs/2505.03679", "authors": ["Huawei Sun", "Bora Kunter Sahin", "Georg Stettinger", "Maximilian Bernhard", "Matthias Schubert", "Robert Wille"], "title": "CaRaFFusion: Improving 2D Semantic Segmentation with Camera-Radar Point Cloud Fusion and Zero-Shot Image Inpainting", "categories": ["cs.CV"], "comment": "Accepted at RA-L 2025", "summary": "Segmenting objects in an environment is a crucial task for autonomous driving\nand robotics, as it enables a better understanding of the surroundings of each\nagent. Although camera sensors provide rich visual details, they are vulnerable\nto adverse weather conditions. In contrast, radar sensors remain robust under\nsuch conditions, but often produce sparse and noisy data. Therefore, a\npromising approach is to fuse information from both sensors. In this work, we\npropose a novel framework to enhance camera-only baselines by integrating a\ndiffusion model into a camera-radar fusion architecture. We leverage radar\npoint features to create pseudo-masks using the Segment-Anything model,\ntreating the projected radar points as point prompts. Additionally, we propose\na noise reduction unit to denoise these pseudo-masks, which are further used to\ngenerate inpainted images that complete the missing information in the original\nimages. Our method improves the camera-only segmentation baseline by 2.63% in\nmIoU and enhances our camera-radar fusion architecture by 1.48% in mIoU on the\nWaterscenes dataset. This demonstrates the effectiveness of our approach for\nsemantic segmentation using camera-radar fusion under adverse weather\nconditions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u76f8\u673a-\u96f7\u8fbe\u878d\u5408\u6846\u67b6\uff0c\u901a\u8fc7\u6269\u6563\u6a21\u578b\u548c\u4f2a\u63a9\u7801\u751f\u6210\u6280\u672f\uff0c\u63d0\u5347\u4e86\u6076\u52a3\u5929\u6c14\u6761\u4ef6\u4e0b\u7684\u8bed\u4e49\u5206\u5272\u6027\u80fd\u3002", "motivation": "\u76f8\u673a\u4f20\u611f\u5668\u5728\u6076\u52a3\u5929\u6c14\u4e0b\u6027\u80fd\u4e0b\u964d\uff0c\u96f7\u8fbe\u4f20\u611f\u5668\u6570\u636e\u7a00\u758f\u4e14\u566a\u58f0\u5927\uff0c\u878d\u5408\u4e24\u8005\u4fe1\u606f\u53ef\u63d0\u5347\u73af\u5883\u611f\u77e5\u80fd\u529b\u3002", "method": "\u5229\u7528\u96f7\u8fbe\u70b9\u7279\u5f81\u751f\u6210\u4f2a\u63a9\u7801\uff0c\u901a\u8fc7\u566a\u58f0\u6291\u5236\u5355\u5143\u4f18\u5316\uff0c\u5e76\u751f\u6210\u4fee\u590d\u56fe\u50cf\u4ee5\u8865\u5145\u539f\u59cb\u56fe\u50cf\u7f3a\u5931\u4fe1\u606f\u3002", "result": "\u5728Waterscenes\u6570\u636e\u96c6\u4e0a\uff0c\u76f8\u673a\u57fa\u7ebf\u5206\u5272\u6027\u80fd\u63d0\u53472.63%\uff0c\u76f8\u673a-\u96f7\u8fbe\u878d\u5408\u67b6\u6784\u6027\u80fd\u63d0\u53471.48%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u6076\u52a3\u5929\u6c14\u6761\u4ef6\u4e0b\u6709\u6548\u63d0\u5347\u4e86\u8bed\u4e49\u5206\u5272\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u76f8\u673a-\u96f7\u8fbe\u878d\u5408\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.03692", "pdf": "https://arxiv.org/pdf/2505.03692", "abs": "https://arxiv.org/abs/2505.03692", "authors": ["Shiqi Li", "Jihua Zhu", "Yifan Xie", "Naiwen Hu", "Di Wang"], "title": "Matching Distance and Geometric Distribution Aided Learning Multiview Point Cloud Registration", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Multiview point cloud registration plays a crucial role in robotics,\nautomation, and computer vision fields. This paper concentrates on pose graph\nconstruction and motion synchronization within multiview registration. Previous\nmethods for pose graph construction often pruned fully connected graphs or\nconstructed sparse graph using global feature aggregated from local\ndescriptors, which may not consistently yield reliable results. To identify\ndependable pairs for pose graph construction, we design a network model that\nextracts information from the matching distance between point cloud pairs. For\nmotion synchronization, we propose another neural network model to calculate\nthe absolute pose in a data-driven manner, rather than optimizing inaccurate\nhandcrafted loss functions. Our model takes into account geometric distribution\ninformation and employs a modified attention mechanism to facilitate flexible\nand reliable feature interaction. Experimental results on diverse indoor and\noutdoor datasets confirm the effectiveness and generalizability of our\napproach. The source code is available at https://github.com/Shi-Qi-Li/MDGD.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7f51\u7edc\u6a21\u578b\u7684\u591a\u89c6\u89d2\u70b9\u4e91\u914d\u51c6\u65b9\u6cd5\uff0c\u901a\u8fc7\u5339\u914d\u8ddd\u79bb\u63d0\u53d6\u53ef\u9760\u5bf9\u6784\u5efa\u4f4d\u59ff\u56fe\uff0c\u5e76\u5229\u7528\u6570\u636e\u9a71\u52a8\u65b9\u5f0f\u8ba1\u7b97\u7edd\u5bf9\u4f4d\u59ff\u3002", "motivation": "\u591a\u89c6\u89d2\u70b9\u4e91\u914d\u51c6\u5728\u673a\u5668\u4eba\u3001\u81ea\u52a8\u5316\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u6784\u5efa\u4f4d\u59ff\u56fe\u548c\u8fd0\u52a8\u540c\u6b65\u65f6\u5b58\u5728\u53ef\u9760\u6027\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e24\u4e2a\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff1a\u4e00\u4e2a\u7528\u4e8e\u63d0\u53d6\u70b9\u4e91\u5bf9\u5339\u914d\u8ddd\u79bb\u4fe1\u606f\u4ee5\u6784\u5efa\u53ef\u9760\u4f4d\u59ff\u56fe\uff0c\u53e6\u4e00\u4e2a\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u65b9\u5f0f\u8ba1\u7b97\u7edd\u5bf9\u4f4d\u59ff\uff0c\u7ed3\u5408\u51e0\u4f55\u5206\u5e03\u4fe1\u606f\u548c\u6539\u8fdb\u7684\u6ce8\u610f\u529b\u673a\u5236\u3002", "result": "\u5728\u591a\u79cd\u5ba4\u5185\u5916\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u591a\u89c6\u89d2\u70b9\u4e91\u914d\u51c6\u7684\u53ef\u9760\u6027\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2505.03703", "pdf": "https://arxiv.org/pdf/2505.03703", "abs": "https://arxiv.org/abs/2505.03703", "authors": ["Fran\u00e7ois Role", "S\u00e9bastien Meyer", "Victor Amblard"], "title": "Fill the Gap: Quantifying and Reducing the Modality Gap in Image-Text Representation Learning", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Vision-language models (VLMs) allow to embed texts and images in a shared\nrepresentation space. However, it has been shown that these models are subject\nto a modality gap phenomenon meaning there exists a clear separation between\nthe embeddings from one modality and another in the embedding space. While this\nmisalignment is detrimental for downstream tasks such as multimodal retrieval,\nmultimodal clustering or zero-shot classification, etc. no generic and\npractical methods have so far been proposed to assess it precisely and even\nreduce it. We therefore propose novel measures and effective techniques\n(spectral- and optimal transport-based methods) to achieve this goal. Extensive\nexperiments conducted on several image-text datasets and models demonstrate\ntheir effectiveness and beneficial effects on downstream tasks. Our code is\navailable at the URL provided in the paper's abstract.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u65b0\u65b9\u6cd5\u548c\u5ea6\u91cf\u6807\u51c6\uff08\u57fa\u4e8e\u8c31\u65b9\u6cd5\u548c\u6700\u4f18\u4f20\u8f93\uff09\u6765\u8bc4\u4f30\u548c\u51cf\u5c11\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u6a21\u6001\u95f4\u9699\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u6709\u6548\u6027\u3002", "motivation": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u6a21\u6001\u95f4\u9699\u95ee\u9898\uff0c\u5f71\u54cd\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\uff0c\u76ee\u524d\u7f3a\u4e4f\u901a\u7528\u4e14\u5b9e\u7528\u7684\u8bc4\u4f30\u548c\u89e3\u51b3\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u8c31\u65b9\u6cd5\u548c\u6700\u4f18\u4f20\u8f93\u7684\u6280\u672f\uff0c\u7528\u4e8e\u8bc4\u4f30\u548c\u51cf\u5c11\u6a21\u6001\u95f4\u9699\u3002", "result": "\u5728\u591a\u4e2a\u56fe\u50cf-\u6587\u672c\u6570\u636e\u96c6\u548c\u6a21\u578b\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e76\u6539\u5584\u4e86\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u3002", "conclusion": "\u65b0\u65b9\u6cd5\u80fd\u6709\u6548\u89e3\u51b3\u6a21\u6001\u95f4\u9699\u95ee\u9898\uff0c\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\u3002"}}
{"id": "2505.03715", "pdf": "https://arxiv.org/pdf/2505.03715", "abs": "https://arxiv.org/abs/2505.03715", "authors": ["Luca Caldera", "Lara Cavinato", "Alessio Cirone", "Isabella Cama", "Sara Garbarino", "Raffaele Lodi", "Fabrizio Tagliavini", "Anna Nigri", "Silvia De Francesco", "Andrea Cappozzo", "Michele Piana", "Francesca Ieva"], "title": "DISARM++: Beyond scanner-free harmonization", "categories": ["cs.CV"], "comment": null, "summary": "Harmonization of T1-weighted MR images across different scanners is crucial\nfor ensuring consistency in neuroimaging studies. This study introduces a novel\napproach to direct image harmonization, moving beyond feature standardization\nto ensure that extracted features remain inherently reliable for downstream\nanalysis. Our method enables image transfer in two ways: (1) mapping images to\na scanner-free space for uniform appearance across all scanners, and (2)\ntransforming images into the domain of a specific scanner used in model\ntraining, embedding its unique characteristics. Our approach presents strong\ngeneralization capability, even for unseen scanners not included in the\ntraining phase. We validated our method using MR images from diverse cohorts,\nincluding healthy controls, traveling subjects, and individuals with\nAlzheimer's disease (AD). The model's effectiveness is tested in multiple\napplications, such as brain age prediction (R2 = 0.60 \\pm 0.05), biomarker\nextraction, AD classification (Test Accuracy = 0.86 \\pm 0.03), and diagnosis\nprediction (AUC = 0.95). In all cases, our harmonization technique outperforms\nstate-of-the-art methods, showing improvements in both reliability and\npredictive accuracy. Moreover, our approach eliminates the need for extensive\npreprocessing steps, such as skull-stripping, which can introduce errors by\nmisclassifying brain and non-brain structures. This makes our method\nparticularly suitable for applications that require full-head analysis,\nincluding research on head trauma and cranial deformities. Additionally, our\nharmonization model does not require retraining for new datasets, allowing\nsmooth integration into various neuroimaging workflows. By ensuring\nscanner-invariant image quality, our approach provides a robust and efficient\nsolution for improving neuroimaging studies across diverse settings. The code\nis available at this link.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684T1\u52a0\u6743MR\u56fe\u50cf\u8de8\u626b\u63cf\u4eea\u76f4\u63a5\u534f\u8c03\u65b9\u6cd5\uff0c\u786e\u4fdd\u63d0\u53d6\u7684\u7279\u5f81\u5728\u4e0b\u6e38\u5206\u6790\u4e2d\u4fdd\u6301\u53ef\u9760\uff0c\u5e76\u5728\u591a\u9879\u5e94\u7528\u4e2d\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u89e3\u51b3\u4e0d\u540c\u626b\u63cf\u4eea\u83b7\u53d6\u7684T1\u52a0\u6743MR\u56fe\u50cf\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u786e\u4fdd\u795e\u7ecf\u5f71\u50cf\u7814\u7a76\u7684\u53ef\u9760\u6027\u548c\u4e00\u81f4\u6027\u3002", "method": "\u901a\u8fc7\u4e24\u79cd\u65b9\u5f0f\u5b9e\u73b0\u56fe\u50cf\u8f6c\u6362\uff1a(1)\u6620\u5c04\u5230\u65e0\u626b\u63cf\u4eea\u7a7a\u95f4\uff0c(2)\u8f6c\u6362\u5230\u7279\u5b9a\u626b\u63cf\u4eea\u57df\u3002\u65b9\u6cd5\u5177\u6709\u5f3a\u6cdb\u5316\u80fd\u529b\uff0c\u65e0\u9700\u9884\u5904\u7406\u6b65\u9aa4\u3002", "result": "\u5728\u8111\u9f84\u9884\u6d4b\uff08R2=0.60\uff09\u3001AD\u5206\u7c7b\uff08\u51c6\u786e\u73870.86\uff09\u7b49\u5e94\u7528\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u65b0\u6570\u636e\u96c6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u795e\u7ecf\u5f71\u50cf\u7814\u7a76\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u53ef\u9760\u7684\u8de8\u626b\u63cf\u5668\u534f\u8c03\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2505.03730", "pdf": "https://arxiv.org/pdf/2505.03730", "abs": "https://arxiv.org/abs/2505.03730", "authors": ["Shiyi Zhang", "Junhao Zhuang", "Zhaoyang Zhang", "Ying Shan", "Yansong Tang"], "title": "FlexiAct: Towards Flexible Action Control in Heterogeneous Scenarios", "categories": ["cs.CV", "cs.AI", "cs.MM"], "comment": "Accepted by Siggraph2025, Project Page:\n  https://shiyi-zh0408.github.io/projectpages/FlexiAct/", "summary": "Action customization involves generating videos where the subject performs\nactions dictated by input control signals. Current methods use pose-guided or\nglobal motion customization but are limited by strict constraints on spatial\nstructure, such as layout, skeleton, and viewpoint consistency, reducing\nadaptability across diverse subjects and scenarios. To overcome these\nlimitations, we propose FlexiAct, which transfers actions from a reference\nvideo to an arbitrary target image. Unlike existing methods, FlexiAct allows\nfor variations in layout, viewpoint, and skeletal structure between the subject\nof the reference video and the target image, while maintaining identity\nconsistency. Achieving this requires precise action control, spatial structure\nadaptation, and consistency preservation. To this end, we introduce RefAdapter,\na lightweight image-conditioned adapter that excels in spatial adaptation and\nconsistency preservation, surpassing existing methods in balancing appearance\nconsistency and structural flexibility. Additionally, based on our\nobservations, the denoising process exhibits varying levels of attention to\nmotion (low frequency) and appearance details (high frequency) at different\ntimesteps. So we propose FAE (Frequency-aware Action Extraction), which, unlike\nexisting methods that rely on separate spatial-temporal architectures, directly\nachieves action extraction during the denoising process. Experiments\ndemonstrate that our method effectively transfers actions to subjects with\ndiverse layouts, skeletons, and viewpoints. We release our code and model\nweights to support further research at\nhttps://shiyi-zh0408.github.io/projectpages/FlexiAct/", "AI": {"tldr": "FlexiAct\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u52a8\u4f5c\u5b9a\u5236\u65b9\u6cd5\uff0c\u901a\u8fc7RefAdapter\u548cFAE\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u5728\u591a\u6837\u5e03\u5c40\u3001\u9aa8\u67b6\u548c\u89c6\u89d2\u4e0b\u7684\u52a8\u4f5c\u8fc1\u79fb\uff0c\u540c\u65f6\u4fdd\u6301\u8eab\u4efd\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u52a8\u4f5c\u5b9a\u5236\u4e2d\u53d7\u9650\u4e8e\u4e25\u683c\u7684\u7a7a\u95f4\u7ed3\u6784\u7ea6\u675f\uff08\u5982\u5e03\u5c40\u3001\u9aa8\u67b6\u548c\u89c6\u89d2\u4e00\u81f4\u6027\uff09\uff0c\u9650\u5236\u4e86\u5176\u9002\u5e94\u6027\u548c\u7075\u6d3b\u6027\u3002", "method": "FlexiAct\u7ed3\u5408\u4e86RefAdapter\uff08\u8f7b\u91cf\u7ea7\u56fe\u50cf\u6761\u4ef6\u9002\u914d\u5668\uff09\u548cFAE\uff08\u9891\u7387\u611f\u77e5\u52a8\u4f5c\u63d0\u53d6\uff09\uff0c\u5728\u53bb\u566a\u8fc7\u7a0b\u4e2d\u76f4\u63a5\u63d0\u53d6\u52a8\u4f5c\uff0c\u65e0\u9700\u5206\u79bb\u7684\u65f6\u7a7a\u67b6\u6784\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFlexiAct\u80fd\u6709\u6548\u5c06\u52a8\u4f5c\u8fc1\u79fb\u5230\u5177\u6709\u4e0d\u540c\u5e03\u5c40\u3001\u9aa8\u67b6\u548c\u89c6\u89d2\u7684\u76ee\u6807\u56fe\u50cf\u4e0a\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "FlexiAct\u901a\u8fc7\u521b\u65b0\u7684\u7a7a\u95f4\u9002\u5e94\u548c\u4e00\u81f4\u6027\u4fdd\u6301\u6280\u672f\uff0c\u4e3a\u52a8\u4f5c\u5b9a\u5236\u63d0\u4f9b\u4e86\u66f4\u9ad8\u7684\u7075\u6d3b\u6027\u548c\u9002\u5e94\u6027\uff0c\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2505.03735", "pdf": "https://arxiv.org/pdf/2505.03735", "abs": "https://arxiv.org/abs/2505.03735", "authors": ["Jiayuan Rao", "Zifeng Li", "Haoning Wu", "Ya Zhang", "Yanfeng Wang", "Weidi Xie"], "title": "Multi-Agent System for Comprehensive Soccer Understanding", "categories": ["cs.CV"], "comment": "Technical Report; Project Page: https://jyrao.github.io/SoccerAgent/", "summary": "Recent advancements in AI-driven soccer understanding have demonstrated rapid\nprogress, yet existing research predominantly focuses on isolated or narrow\ntasks. To bridge this gap, we propose a comprehensive framework for holistic\nsoccer understanding. Specifically, we make the following contributions in this\npaper: (i) we construct SoccerWiki, the first large-scale multimodal soccer\nknowledge base, integrating rich domain knowledge about players, teams,\nreferees, and venues to enable knowledge-driven reasoning; (ii) we present\nSoccerBench, the largest and most comprehensive soccer-specific benchmark,\nfeaturing around 10K standardized multimodal (text, image, video) multi-choice\nQA pairs across 13 distinct understanding tasks, curated through automated\npipelines and manual verification; (iii) we introduce SoccerAgent, a novel\nmulti-agent system that decomposes complex soccer questions via collaborative\nreasoning, leveraging domain expertise from SoccerWiki and achieving robust\nperformance; (iv) extensive evaluations and ablations that benchmark\nstate-of-the-art MLLMs on SoccerBench, highlighting the superiority of our\nproposed agentic system. All data and code are publicly available at:\nhttps://jyrao.github.io/SoccerAgent/.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u5168\u9762\u7684\u8db3\u7403\u7406\u89e3\u6846\u67b6\uff0c\u5305\u62ec\u6784\u5efa\u591a\u6a21\u6001\u77e5\u8bc6\u5e93SoccerWiki\u3001\u521b\u5efa\u5927\u89c4\u6a21\u57fa\u51c6SoccerBench\u3001\u8bbe\u8ba1\u591a\u667a\u80fd\u4f53\u7cfb\u7edfSoccerAgent\uff0c\u5e76\u8fdb\u884c\u5e7f\u6cdb\u8bc4\u4f30\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u96c6\u4e2d\u4e8e\u5b64\u7acb\u6216\u72ed\u7a84\u4efb\u52a1\uff0c\u7f3a\u4e4f\u5bf9\u8db3\u7403\u7406\u89e3\u7684\u5168\u9762\u8986\u76d6\u3002", "method": "\u6784\u5efaSoccerWiki\u77e5\u8bc6\u5e93\u3001SoccerBench\u57fa\u51c6\uff0c\u8bbe\u8ba1SoccerAgent\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u3002", "result": "SoccerAgent\u5728SoccerBench\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f18\u4e8e\u73b0\u6709MLLMs\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u4e3a\u8db3\u7403\u7406\u89e3\u63d0\u4f9b\u4e86\u5168\u9762\u89e3\u51b3\u65b9\u6848\uff0c\u6570\u636e\u4e0e\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2505.02843", "pdf": "https://arxiv.org/pdf/2505.02843", "abs": "https://arxiv.org/abs/2505.02843", "authors": ["Miriam Cobo", "David Corral Fontecha", "Wilson Silva", "Lara Lloret Iglesias"], "title": "Physical foundations for trustworthy medical imaging: a review for artificial intelligence researchers", "categories": ["eess.IV", "cs.AI", "cs.CV", "physics.med-ph"], "comment": "17 pages, 2 figures", "summary": "Artificial intelligence in medical imaging has seen unprecedented growth in\nthe last years, due to rapid advances in deep learning and computing resources.\nApplications cover the full range of existing medical imaging modalities, with\nunique characteristics driven by the physics of each technique. Yet, artificial\nintelligence professionals entering the field, and even experienced developers,\noften lack a comprehensive understanding of the physical principles underlying\nmedical image acquisition, which hinders their ability to fully leverage its\npotential. The integration of physics knowledge into artificial intelligence\nalgorithms enhances their trustworthiness and robustness in medical imaging,\nespecially in scenarios with limited data availability. In this work, we review\nthe fundamentals of physics in medical images and their impact on the latest\nadvances in artificial intelligence, particularly, in generative models and\nreconstruction algorithms. Finally, we explore the integration of physics\nknowledge into physics-inspired machine learning models, which leverage\nphysics-based constraints to enhance the learning of medical imaging features.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u533b\u5b66\u5f71\u50cf\u4e2d\u4eba\u5de5\u667a\u80fd\u7684\u53d1\u5c55\uff0c\u5f3a\u8c03\u4e86\u7269\u7406\u77e5\u8bc6\u5728\u63d0\u5347AI\u7b97\u6cd5\u53ef\u4fe1\u5ea6\u548c\u9c81\u68d2\u6027\u4e2d\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u7531\u4e8eAI\u5f00\u53d1\u8005\u5bf9\u533b\u5b66\u5f71\u50cf\u7269\u7406\u539f\u7406\u7684\u7406\u89e3\u4e0d\u8db3\uff0c\u9650\u5236\u4e86AI\u5728\u533b\u5b66\u5f71\u50cf\u4e2d\u7684\u6f5c\u529b\u53d1\u6325\u3002", "method": "\u56de\u987e\u533b\u5b66\u5f71\u50cf\u7684\u7269\u7406\u57fa\u7840\u53ca\u5176\u5bf9AI\uff08\u5982\u751f\u6210\u6a21\u578b\u548c\u91cd\u5efa\u7b97\u6cd5\uff09\u7684\u5f71\u54cd\uff0c\u5e76\u63a2\u7d22\u7269\u7406\u77e5\u8bc6\u5728\u673a\u5668\u5b66\u4e60\u6a21\u578b\u4e2d\u7684\u6574\u5408\u3002", "result": "\u7269\u7406\u77e5\u8bc6\u7684\u6574\u5408\u589e\u5f3a\u4e86AI\u7b97\u6cd5\u5728\u533b\u5b66\u5f71\u50cf\u4e2d\u7684\u8868\u73b0\uff0c\u5c24\u5176\u5728\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u3002", "conclusion": "\u7269\u7406\u542f\u53d1\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u901a\u8fc7\u7ed3\u5408\u7269\u7406\u7ea6\u675f\uff0c\u80fd\u66f4\u597d\u5730\u5b66\u4e60\u533b\u5b66\u5f71\u50cf\u7279\u5f81\u3002"}}
{"id": "2505.02845", "pdf": "https://arxiv.org/pdf/2505.02845", "abs": "https://arxiv.org/abs/2505.02845", "authors": ["Jeremias Gerner", "Klaus Bogenberger", "Stefanie Schmidtner"], "title": "Floating Car Observers in Intelligent Transportation Systems: Detection Modeling and Temporal Insights", "categories": ["physics.soc-ph", "cs.CV", "cs.SY", "eess.SY"], "comment": null, "summary": "Floating Car Observers (FCOs) extend traditional Floating Car Data (FCD) by\nintegrating onboard sensors to detect and localize other traffic participants,\nproviding richer and more detailed traffic data. In this work, we explore\nvarious modeling approaches for FCO detections within microscopic traffic\nsimulations to evaluate their potential for Intelligent Transportation System\n(ITS) applications. These approaches range from 2D raytracing to high-fidelity\nco-simulations that emulate real-world sensors and integrate 3D object\ndetection algorithms to closely replicate FCO detections. Additionally, we\nintroduce a neural network-based emulation technique that effectively\napproximates the results of high-fidelity co-simulations. This approach\ncaptures the unique characteristics of FCO detections while offering a fast and\nscalable solution for modeling. Using this emulation method, we investigate the\nimpact of FCO data in a digital twin of a traffic network modeled in SUMO.\nResults demonstrate that even at a 20% penetration rate, FCOs using LiDAR-based\ndetections can identify 65% of vehicles across various intersections and\ntraffic demand scenarios. Further potential emerges when temporal insights are\nintegrated, enabling the recovery of previously detected but currently unseen\nvehicles. By employing data-driven methods, we recover over 80% of these\nvehicles with minimal positional deviations. These findings underscore the\npotential of FCOs for ITS, particularly in enhancing traffic state estimation\nand monitoring under varying penetration rates and traffic conditions.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86Floating Car Observers (FCOs)\u5728\u5fae\u89c2\u4ea4\u901a\u6a21\u62df\u4e2d\u7684\u5efa\u6a21\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u5176\u5728\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u4e2d\u7684\u6f5c\u529b\u3002", "motivation": "\u4f20\u7edfFloating Car Data (FCD)\u6570\u636e\u6709\u9650\uff0cFCOs\u901a\u8fc7\u96c6\u6210\u8f66\u8f7d\u4f20\u611f\u5668\u63d0\u4f9b\u66f4\u4e30\u5bcc\u7684\u4ea4\u901a\u6570\u636e\uff0c\u4ee5\u652f\u6301\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u5e94\u7528\u3002", "method": "\u91c7\u7528\u591a\u79cd\u5efa\u6a21\u65b9\u6cd5\uff0c\u5305\u62ec2D\u5149\u7ebf\u8ffd\u8e2a\u3001\u9ad8\u4fdd\u771f\u534f\u540c\u6a21\u62df\u548c\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u4eff\u771f\u6280\u672f\uff0c\u8bc4\u4f30FCOs\u5728SUMO\u4ea4\u901a\u7f51\u7edc\u6570\u5b57\u5b6a\u751f\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5373\u4f7f\u572820%\u7684\u6e17\u900f\u7387\u4e0b\uff0c\u57fa\u4e8eLiDAR\u7684FCOs\u80fd\u8bc6\u522b65%\u7684\u8f66\u8f86\uff1b\u7ed3\u5408\u65f6\u95f4\u4fe1\u606f\u540e\uff0c\u53ef\u6062\u590d80%\u4ee5\u4e0a\u8f66\u8f86\uff0c\u4f4d\u7f6e\u504f\u5dee\u6781\u5c0f\u3002", "conclusion": "FCOs\u5728\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u4e2d\u5177\u6709\u663e\u8457\u6f5c\u529b\uff0c\u5c24\u5176\u5728\u4ea4\u901a\u72b6\u6001\u4f30\u8ba1\u548c\u76d1\u63a7\u65b9\u9762\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u6e17\u900f\u7387\u548c\u4ea4\u901a\u6761\u4ef6\u3002"}}
{"id": "2505.02877", "pdf": "https://arxiv.org/pdf/2505.02877", "abs": "https://arxiv.org/abs/2505.02877", "authors": ["Hele Zhu", "Xinyi Huang", "Haojia Gao", "Mengfei Jiang", "Haohua Que", "Lei Mu"], "title": "A Wireless Collaborated Inference Acceleration Framework for Plant Disease Recognition", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Plant disease is a critical factor affecting agricultural production.\nTraditional manual recognition methods face significant drawbacks, including\nlow accuracy, high costs, and inefficiency. Deep learning techniques have\ndemonstrated significant benefits in identifying plant diseases, but they still\nface challenges such as inference delays and high energy consumption. Deep\nlearning algorithms are difficult to run on resource-limited embedded devices.\nOffloading these models to cloud servers is confronted with the restriction of\ncommunication bandwidth, and all of these factors will influence the\ninference's efficiency. We propose a collaborative inference framework for\nrecognizing plant diseases between edge devices and cloud servers to enhance\ninference speed. The DNN model for plant disease recognition is pruned through\ndeep reinforcement learning to improve the inference speed and reduce energy\nconsumption. Then the optimal split point is determined by a greedy strategy to\nachieve the best collaborated inference acceleration. Finally, the system for\ncollaborative inference acceleration in plant disease recognition has been\nimplemented using Gradio to facilitate friendly human-machine interaction.\nExperiments indicate that the proposed collaborative inference framework\nsignificantly increases inference speed while maintaining acceptable\nrecognition accuracy, offering a novel solution for rapidly diagnosing and\npreventing plant diseases.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8fb9\u7f18\u8bbe\u5907\u4e0e\u4e91\u670d\u52a1\u5668\u534f\u4f5c\u7684\u690d\u7269\u75c5\u5bb3\u8bc6\u522b\u6846\u67b6\uff0c\u901a\u8fc7\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u526a\u679d\u6a21\u578b\u5e76\u4f18\u5316\u5206\u5272\u70b9\uff0c\u663e\u8457\u63d0\u5347\u63a8\u7406\u901f\u5ea6\u3002", "motivation": "\u4f20\u7edf\u624b\u52a8\u8bc6\u522b\u65b9\u6cd5\u6548\u7387\u4f4e\uff0c\u6df1\u5ea6\u5b66\u4e60\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u8fd0\u884c\u56f0\u96be\uff0c\u901a\u4fe1\u5e26\u5bbd\u9650\u5236\u5f71\u54cd\u4e91\u670d\u52a1\u5668\u63a8\u7406\u6548\u7387\u3002", "method": "\u4f7f\u7528\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u526a\u679dDNN\u6a21\u578b\uff0c\u8d2a\u5a6a\u7b56\u7565\u786e\u5b9a\u6700\u4f18\u5206\u5272\u70b9\uff0c\u5b9e\u73b0\u534f\u4f5c\u63a8\u7406\u52a0\u901f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u6846\u67b6\u663e\u8457\u63d0\u5347\u63a8\u7406\u901f\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u53ef\u63a5\u53d7\u7684\u8bc6\u522b\u7cbe\u5ea6\u3002", "conclusion": "\u4e3a\u5feb\u901f\u8bca\u65ad\u548c\u9884\u9632\u690d\u7269\u75c5\u5bb3\u63d0\u4f9b\u4e86\u65b0\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.03037", "pdf": "https://arxiv.org/pdf/2505.03037", "abs": "https://arxiv.org/abs/2505.03037", "authors": ["Xiaofeng Liu", "Yongsong Huang", "Thibault Marin", "Samira Vafay Eslahi", "Tiss Amal", "Yanis Chemli", "Keith Johnson", "Georges El Fakhri", "Jinsong Ouyang"], "title": "Dual Prompting for Diverse Count-level PET Denoising", "categories": ["eess.IV", "cs.CV", "physics.med-ph"], "comment": "Published in IEEE International Symposium on Biomedical Imaging\n  (ISBI) 2025", "summary": "The to-be-denoised positron emission tomography (PET) volumes are inherent\nwith diverse count levels, which imposes challenges for a unified model to\ntackle varied cases. In this work, we resort to the recently flourished prompt\nlearning to achieve generalizable PET denoising with different count levels.\nSpecifically, we propose dual prompts to guide the PET denoising in a\ndivide-and-conquer manner, i.e., an explicitly count-level prompt to provide\nthe specific prior information and an implicitly general denoising prompt to\nencode the essential PET denoising knowledge. Then, a novel prompt fusion\nmodule is developed to unify the heterogeneous prompts, followed by a\nprompt-feature interaction module to inject prompts into the features. The\nprompts are able to dynamically guide the noise-conditioned denoising process.\nTherefore, we are able to efficiently train a unified denoising model for\nvarious count levels, and deploy it to different cases with personalized\nprompts. We evaluated on 1940 low-count PET 3D volumes with uniformly randomly\nselected 13-22\\% fractions of events from 97 $^{18}$F-MK6240 tau PET studies.\nIt shows our dual prompting can largely improve the performance with informed\ncount-level and outperform the count-conditional model.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u63d0\u793a\u5b66\u4e60\u7684PET\u53bb\u566a\u65b9\u6cd5\uff0c\u901a\u8fc7\u53cc\u63d0\u793a\u673a\u5236\uff08\u663e\u5f0f\u8ba1\u6570\u7ea7\u63d0\u793a\u548c\u9690\u5f0f\u901a\u7528\u53bb\u566a\u63d0\u793a\uff09\u52a8\u6001\u6307\u5bfc\u4e0d\u540c\u8ba1\u6570\u6c34\u5e73\u7684\u53bb\u566a\u8fc7\u7a0b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "PET\u56fe\u50cf\u53bb\u566a\u9762\u4e34\u4e0d\u540c\u8ba1\u6570\u6c34\u5e73\u7684\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u7edf\u4e00\u4e14\u901a\u7528\u7684\u6a21\u578b\u6765\u5904\u7406\u591a\u6837\u5316\u7684\u6848\u4f8b\u3002", "method": "\u91c7\u7528\u53cc\u63d0\u793a\u673a\u5236\uff08\u663e\u5f0f\u8ba1\u6570\u7ea7\u63d0\u793a\u548c\u9690\u5f0f\u901a\u7528\u53bb\u566a\u63d0\u793a\uff09\uff0c\u7ed3\u5408\u63d0\u793a\u878d\u5408\u6a21\u5757\u548c\u63d0\u793a-\u7279\u5f81\u4ea4\u4e92\u6a21\u5757\uff0c\u52a8\u6001\u6307\u5bfc\u53bb\u566a\u8fc7\u7a0b\u3002", "result": "\u57281940\u4e2a\u4f4e\u8ba1\u6570PET 3D\u4f53\u79ef\u4e0a\u8bc4\u4f30\uff0c\u53cc\u63d0\u793a\u673a\u5236\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u4f18\u4e8e\u8ba1\u6570\u6761\u4ef6\u6a21\u578b\u3002", "conclusion": "\u53cc\u63d0\u793a\u673a\u5236\u80fd\u591f\u9ad8\u6548\u8bad\u7ec3\u7edf\u4e00\u7684\u53bb\u566a\u6a21\u578b\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u8ba1\u6570\u6c34\u5e73\u7684\u4e2a\u6027\u5316\u53bb\u566a\u4efb\u52a1\u3002"}}
{"id": "2505.03046", "pdf": "https://arxiv.org/pdf/2505.03046", "abs": "https://arxiv.org/abs/2505.03046", "authors": ["Pau Amargant", "Peter H\u00f6nig", "Markus Vincze"], "title": "Sim2Real Transfer for Vision-Based Grasp Verification", "categories": ["cs.RO", "cs.CV"], "comment": "Accepted at Austrian Robotics Workshop 2025", "summary": "The verification of successful grasps is a crucial aspect of robot\nmanipulation, particularly when handling deformable objects. Traditional\nmethods relying on force and tactile sensors often struggle with deformable and\nnon-rigid objects. In this work, we present a vision-based approach for grasp\nverification to determine whether the robotic gripper has successfully grasped\nan object. Our method employs a two-stage architecture; first YOLO-based object\ndetection model to detect and locate the robot's gripper and then a\nResNet-based classifier determines the presence of an object. To address the\nlimitations of real-world data capture, we introduce HSR-GraspSynth, a\nsynthetic dataset designed to simulate diverse grasping scenarios. Furthermore,\nwe explore the use of Visual Question Answering capabilities as a zero-shot\nbaseline to which we compare our model. Experimental results demonstrate that\nour approach achieves high accuracy in real-world environments, with potential\nfor integration into grasping pipelines. Code and datasets are publicly\navailable at https://github.com/pauamargant/HSR-GraspSynth .", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u7684\u6293\u53d6\u9a8c\u8bc1\u65b9\u6cd5\uff0c\u7528\u4e8e\u5224\u65ad\u673a\u5668\u4eba\u5939\u722a\u662f\u5426\u6210\u529f\u6293\u53d6\u7269\u4f53\uff0c\u91c7\u7528YOLO\u548cResNet\u7684\u4e24\u9636\u6bb5\u67b6\u6784\uff0c\u5e76\u5f15\u5165\u5408\u6210\u6570\u636e\u96c6HSR-GraspSynth\u3002\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u529b\u548c\u89e6\u89c9\u4f20\u611f\u5668\u7684\u65b9\u6cd5\u5728\u5904\u7406\u53ef\u53d8\u5f62\u548c\u975e\u521a\u6027\u7269\u4f53\u65f6\u6548\u679c\u4e0d\u4f73\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u53ef\u9760\u7684\u89c6\u89c9\u9a8c\u8bc1\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528YOLO\u68c0\u6d4b\u5939\u722a\u4f4d\u7f6e\uff0cResNet\u5206\u7c7b\u5668\u5224\u65ad\u7269\u4f53\u5b58\u5728\uff0c\u5e76\u5229\u7528\u5408\u6210\u6570\u636e\u96c6HSR-GraspSynth\u8865\u5145\u771f\u5b9e\u6570\u636e\u4e0d\u8db3\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u8be5\u65b9\u6cd5\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u5177\u6709\u9ad8\u51c6\u786e\u6027\uff0c\u5e76\u53ef\u96c6\u6210\u5230\u6293\u53d6\u6d41\u7a0b\u4e2d\u3002", "conclusion": "\u63d0\u51fa\u7684\u89c6\u89c9\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u53ef\u53d8\u5f62\u7269\u4f53\u6293\u53d6\u9a8c\u8bc1\u7684\u6311\u6218\uff0c\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5df2\u5f00\u6e90\u3002"}}
{"id": "2505.03123", "pdf": "https://arxiv.org/pdf/2505.03123", "abs": "https://arxiv.org/abs/2505.03123", "authors": ["Yiran Zhu", "Wei Yang", "Yan su", "Zesheng Li", "Chengchang Pan", "Honggang Qi"], "title": "STG: Spatiotemporal Graph Neural Network with Fusion and Spatiotemporal Decoupling Learning for Prognostic Prediction of Colorectal Cancer Liver Metastasis", "categories": ["eess.IV", "cs.CV", "cs.MM"], "comment": "9 pages, 4 figures, 5 tables", "summary": "We propose a multimodal spatiotemporal graph neural network (STG) framework\nto predict colorectal cancer liver metastasis (CRLM) progression. Current\nclinical models do not effectively integrate the tumor's spatial heterogeneity,\ndynamic evolution, and complex multimodal data relationships, limiting their\npredictive accuracy. Our STG framework combines preoperative CT imaging and\nclinical data into a heterogeneous graph structure, enabling joint modeling of\ntumor distribution and temporal evolution through spatial topology and\ncross-modal edges. The framework uses GraphSAGE to aggregate spatiotemporal\nneighborhood information and leverages supervised and contrastive learning\nstrategies to enhance the model's ability to capture temporal features and\nimprove robustness. A lightweight version of the model reduces parameter count\nby 78.55%, maintaining near-state-of-the-art performance. The model jointly\noptimizes recurrence risk regression and survival analysis tasks, with\ncontrastive loss improving feature representational discriminability and\ncross-modal consistency. Experimental results on the MSKCC CRLM dataset show a\ntime-adjacent accuracy of 85% and a mean absolute error of 1.1005,\nsignificantly outperforming existing methods. The innovative heterogeneous\ngraph construction and spatiotemporal decoupling mechanism effectively uncover\nthe associations between dynamic tumor microenvironment changes and prognosis,\nproviding reliable quantitative support for personalized treatment decisions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u65f6\u7a7a\u56fe\u795e\u7ecf\u7f51\u7edc\uff08STG\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u9884\u6d4b\u7ed3\u76f4\u80a0\u764c\u809d\u8f6c\u79fb\uff08CRLM\uff09\u7684\u8fdb\u5c55\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u4e34\u5e8a\u6a21\u578b\u672a\u80fd\u6709\u6548\u6574\u5408\u80bf\u7624\u7684\u7a7a\u95f4\u5f02\u8d28\u6027\u3001\u52a8\u6001\u6f14\u5316\u548c\u591a\u6a21\u6001\u6570\u636e\u5173\u7cfb\uff0c\u9650\u5236\u4e86\u9884\u6d4b\u51c6\u786e\u6027\u3002", "method": "\u7ed3\u5408\u672f\u524dCT\u5f71\u50cf\u548c\u4e34\u5e8a\u6570\u636e\u6784\u5efa\u5f02\u6784\u56fe\u7ed3\u6784\uff0c\u901a\u8fc7\u7a7a\u95f4\u62d3\u6251\u548c\u8de8\u6a21\u6001\u8fb9\u8054\u5408\u5efa\u6a21\u80bf\u7624\u5206\u5e03\u4e0e\u65f6\u95f4\u6f14\u5316\uff0c\u4f7f\u7528GraphSAGE\u805a\u5408\u65f6\u7a7a\u90bb\u57df\u4fe1\u606f\uff0c\u5e76\u91c7\u7528\u76d1\u7763\u4e0e\u5bf9\u6bd4\u5b66\u4e60\u7b56\u7565\u3002", "result": "\u5728MSKCC CRLM\u6570\u636e\u96c6\u4e0a\uff0c\u65f6\u95f4\u90bb\u8fd1\u51c6\u786e\u7387\u8fbe85%\uff0c\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u4e3a1.1005\uff0c\u53c2\u6570\u6570\u91cf\u51cf\u5c1178.55%\u3002", "conclusion": "\u8be5\u6846\u67b6\u63ed\u793a\u4e86\u52a8\u6001\u80bf\u7624\u5fae\u73af\u5883\u53d8\u5316\u4e0e\u9884\u540e\u7684\u5173\u8054\uff0c\u4e3a\u4e2a\u6027\u5316\u6cbb\u7597\u51b3\u7b56\u63d0\u4f9b\u4e86\u53ef\u9760\u652f\u6301\u3002"}}
{"id": "2505.03174", "pdf": "https://arxiv.org/pdf/2505.03174", "abs": "https://arxiv.org/abs/2505.03174", "authors": ["Guillermo Roque", "Erika Maquiling", "Jose Giovanni Tapia Lopez", "Ross Greer"], "title": "Automated Data Curation Using GPS & NLP to Generate Instruction-Action Pairs for Autonomous Vehicle Vision-Language Navigation Datasets", "categories": ["cs.RO", "cs.CV", "cs.LG"], "comment": null, "summary": "Instruction-Action (IA) data pairs are valuable for training robotic systems,\nespecially autonomous vehicles (AVs), but having humans manually annotate this\ndata is costly and time-inefficient. This paper explores the potential of using\nmobile application Global Positioning System (GPS) references and Natural\nLanguage Processing (NLP) to automatically generate large volumes of IA\ncommands and responses without having a human generate or retroactively tag the\ndata. In our pilot data collection, by driving to various destinations and\ncollecting voice instructions from GPS applications, we demonstrate a means to\ncollect and categorize the diverse sets of instructions, further accompanied by\nvideo data to form complete vision-language-action triads. We provide details\non our completely automated data collection prototype system, ADVLAT-Engine. We\ncharacterize collected GPS voice instructions into eight different\nclassifications, highlighting the breadth of commands and referentialities\navailable for curation from freely available mobile applications. Through\nresearch and exploration into the automation of IA data pairs using GPS\nreferences, the potential to increase the speed and volume at which\nhigh-quality IA datasets are created, while minimizing cost, can pave the way\nfor robust vision-language-action (VLA) models to serve tasks in\nvision-language navigation (VLN) and human-interactive autonomous systems.", "AI": {"tldr": "\u5229\u7528GPS\u548cNLP\u81ea\u52a8\u751f\u6210\u6307\u4ee4-\u52a8\u4f5c\u6570\u636e\u5bf9\uff0c\u51cf\u5c11\u4eba\u5de5\u6807\u6ce8\u6210\u672c\uff0c\u63d0\u5347\u6570\u636e\u96c6\u751f\u6210\u6548\u7387\u3002", "motivation": "\u4eba\u5de5\u6807\u6ce8\u6307\u4ee4-\u52a8\u4f5c\u6570\u636e\u5bf9\u6210\u672c\u9ad8\u4e14\u6548\u7387\u4f4e\uff0c\u9700\u63a2\u7d22\u81ea\u52a8\u5316\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7GPS\u5e94\u7528\u6536\u96c6\u8bed\u97f3\u6307\u4ee4\uff0c\u7ed3\u5408\u89c6\u9891\u6570\u636e\u5f62\u6210\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u4e09\u5143\u7ec4\uff0c\u5f00\u53d1\u81ea\u52a8\u5316\u6570\u636e\u6536\u96c6\u7cfb\u7edfADVLAT-Engine\u3002", "result": "\u6210\u529f\u5c06GPS\u8bed\u97f3\u6307\u4ee4\u5206\u7c7b\u4e3a\u516b\u79cd\u7c7b\u578b\uff0c\u5c55\u793a\u4e86\u81ea\u52a8\u5316\u751f\u6210\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u7684\u6f5c\u529b\u3002", "conclusion": "\u81ea\u52a8\u5316\u751f\u6210\u6307\u4ee4-\u52a8\u4f5c\u6570\u636e\u5bf9\u53ef\u52a0\u901f\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u7684\u521b\u5efa\uff0c\u4e3a\u89c6\u89c9-\u8bed\u8a00\u5bfc\u822a\u548c\u4ea4\u4e92\u5f0f\u81ea\u4e3b\u7cfb\u7edf\u63d0\u4f9b\u652f\u6301\u3002"}}
{"id": "2505.03186", "pdf": "https://arxiv.org/pdf/2505.03186", "abs": "https://arxiv.org/abs/2505.03186", "authors": ["Detao Bai", "Zhiheng Ma", "Xihan Wei", "Liefeng Bo"], "title": "CoGenAV: Versatile Audio-Visual Representation Learning via Contrastive-Generative Synchronization", "categories": ["cs.SD", "cs.CV", "eess.AS"], "comment": null, "summary": "The inherent synchronization between a speaker's lip movements, voice, and\nthe underlying linguistic content offers a rich source of information for\nimproving speech processing tasks, especially in challenging conditions where\ntraditional audio-only systems falter. We introduce CoGenAV, a powerful and\ndata-efficient model designed to learn versatile audio-visual representations\napplicable across a wide range of speech and audio-visual tasks. CoGenAV is\ntrained by optimizing a dual objective derived from natural audio-visual\nsynchrony, contrastive feature alignment and generative text prediction, using\nonly 223 hours of labeled data from the LRS2 dataset. This\ncontrastive-generative synchronization strategy effectively captures\nfundamental cross-modal correlations. We showcase the effectiveness and\nversatility of the learned CoGenAV representations on multiple benchmarks. When\nutilized for Audio-Visual Speech Recognition (AVSR) on LRS2, these\nrepresentations contribute to achieving a state-of-the-art Word Error Rate\n(WER) of 1.27. They also enable strong performance in Visual Speech Recognition\n(VSR) with a WER of 22.0 on LRS2, and significantly improve performance in\nnoisy environments by over 70%. Furthermore, CoGenAV representations benefit\nspeech reconstruction tasks, boosting performance in Speech Enhancement and\nSeparation, and achieve competitive results in audio-visual synchronization\ntasks like Active Speaker Detection (ASD). Our model will be open-sourced to\nfacilitate further development and collaboration within both academia and\nindustry.", "AI": {"tldr": "CoGenAV\u6a21\u578b\u901a\u8fc7\u7ed3\u5408\u5bf9\u6bd4\u7279\u5f81\u5bf9\u9f50\u548c\u751f\u6210\u6587\u672c\u9884\u6d4b\u7684\u53cc\u76ee\u6807\u4f18\u5316\uff0c\u5229\u7528\u5c11\u91cf\u6807\u6ce8\u6570\u636e\u5b66\u4e60\u591a\u4efb\u52a1\u9002\u7528\u7684\u97f3\u9891-\u89c6\u89c9\u8868\u793a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bed\u97f3\u8bc6\u522b\u548c\u566a\u58f0\u73af\u5883\u4e0b\u7684\u6027\u80fd\u3002", "motivation": "\u5229\u7528\u8bf4\u8bdd\u8005\u7684\u5507\u90e8\u52a8\u4f5c\u3001\u58f0\u97f3\u548c\u8bed\u8a00\u5185\u5bb9\u4e4b\u95f4\u7684\u540c\u6b65\u6027\uff0c\u6539\u8fdb\u4f20\u7edf\u97f3\u9891\u7cfb\u7edf\u5728\u590d\u6742\u6761\u4ef6\u4e0b\u7684\u8868\u73b0\u3002", "method": "CoGenAV\u6a21\u578b\u901a\u8fc7\u5bf9\u6bd4-\u751f\u6210\u540c\u6b65\u7b56\u7565\u4f18\u5316\u53cc\u76ee\u6807\uff08\u5bf9\u6bd4\u7279\u5f81\u5bf9\u9f50\u548c\u751f\u6210\u6587\u672c\u9884\u6d4b\uff09\uff0c\u4ec5\u4f7f\u7528223\u5c0f\u65f6LRS2\u6570\u636e\u96c6\u6807\u6ce8\u6570\u636e\u3002", "result": "\u5728LRS2\u6570\u636e\u96c6\u4e0a\uff0cAVSR\u7684WER\u4e3a1.27\uff0cVSR\u7684WER\u4e3a22.0\uff0c\u566a\u58f0\u73af\u5883\u4e0b\u6027\u80fd\u63d0\u534770%\u4ee5\u4e0a\uff0c\u5e76\u5728\u8bed\u97f3\u91cd\u5efa\u548c\u540c\u6b65\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "CoGenAV\u5c55\u793a\u4e86\u97f3\u9891-\u89c6\u89c9\u8868\u793a\u5728\u591a\u4efb\u52a1\u4e2d\u7684\u5f3a\u5927\u6f5c\u529b\uff0c\u672a\u6765\u5c06\u5f00\u6e90\u4ee5\u4fc3\u8fdb\u5b66\u672f\u548c\u5de5\u4e1a\u754c\u7684\u5408\u4f5c\u3002"}}
{"id": "2505.03420", "pdf": "https://arxiv.org/pdf/2505.03420", "abs": "https://arxiv.org/abs/2505.03420", "authors": ["Fei Zhao", "Chengcui Zhang", "Runlin Zhang", "Tianyang Wang", "Xi Li"], "title": "Mitigating Image Captioning Hallucinations in Vision-Language Models", "categories": ["cs.MM", "cs.CV"], "comment": null, "summary": "Hallucinations in vision-language models (VLMs) hinder reliability and\nreal-world applicability, usually stemming from distribution shifts between\npretraining data and test samples. Existing solutions, such as retraining or\nfine-tuning on additional data, demand significant computational resources and\nlabor-intensive data collection, while ensemble-based methods incur additional\ncosts by introducing auxiliary VLMs. To address these challenges, we propose a\nnovel test-time adaptation framework using reinforcement learning to mitigate\nhallucinations during inference without retraining or any auxiliary VLMs. By\nupdating only the learnable parameters in the layer normalization of the\nlanguage model (approximately 0.003% of the model parameters), our method\nreduces distribution shifts between test samples and pretraining samples. A\nCLIP-based hallucination evaluation model is proposed to provide dual rewards\nto VLMs. Experimental results demonstrate a 15.4% and 17.3% reduction in\nhallucination rates on LLaVA and InstructBLIP, respectively. Our approach\noutperforms state-of-the-art baselines with a 68.3% improvement in\nhallucination mitigation, demonstrating its effectiveness.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6d4b\u8bd5\u65f6\u9002\u5e94\u6846\u67b6\uff0c\u901a\u8fc7\u66f4\u65b0\u8bed\u8a00\u6a21\u578b\u4e2d\u53ef\u5b66\u4e60\u7684\u53c2\u6570\u6765\u51cf\u5c11\u5e7b\u89c9\u73b0\u8c61\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6216\u8f85\u52a9\u6a21\u578b\u3002", "motivation": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\u964d\u4f4e\u4e86\u53ef\u9760\u6027\u548c\u5b9e\u9645\u5e94\u7528\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u6570\u636e\u6536\u96c6\u7e41\u7410\u3002", "method": "\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u4ec5\u66f4\u65b0\u8bed\u8a00\u6a21\u578b\u5c42\u5f52\u4e00\u5316\u4e2d\u7684\u53ef\u5b66\u4e60\u53c2\u6570\uff08\u7ea60.003%\uff09\uff0c\u5e76\u57fa\u4e8eCLIP\u7684\u5e7b\u89c9\u8bc4\u4f30\u6a21\u578b\u63d0\u4f9b\u53cc\u91cd\u5956\u52b1\u3002", "result": "\u5728LLaVA\u548cInstructBLIP\u4e0a\u5206\u522b\u51cf\u5c1115.4%\u548c17.3%\u7684\u5e7b\u89c9\u7387\uff0c\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf68.3%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u51cf\u5c11\u4e86\u5e7b\u89c9\u73b0\u8c61\uff0c\u4e14\u8ba1\u7b97\u6210\u672c\u4f4e\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2505.03510", "pdf": "https://arxiv.org/pdf/2505.03510", "abs": "https://arxiv.org/abs/2505.03510", "authors": ["Ludovico Iannello", "Luca Ciampi", "Gabriele Lagani", "Fabrizio Tonelli", "Eleonora Crocco", "Lucio Maria Calcagnile", "Angelo Di Garbo", "Federico Cremisi", "Giuseppe Amato"], "title": "From Neurons to Computation: Biological Reservoir Computing for Pattern Recognition", "categories": ["cs.NE", "cs.AI", "cs.CV"], "comment": null, "summary": "In this paper, we introduce a novel paradigm for reservoir computing (RC)\nthat leverages a pool of cultured biological neurons as the reservoir\nsubstrate, creating a biological reservoir computing (BRC). This system\noperates similarly to an echo state network (ESN), with the key distinction\nthat the neural activity is generated by a network of cultured neurons, rather\nthan being modeled by traditional artificial computational units. The neuronal\nactivity is recorded using a multi-electrode array (MEA), which enables\nhigh-throughput recording of neural signals. In our approach, inputs are\nintroduced into the network through a subset of the MEA electrodes, while the\nremaining electrodes capture the resulting neural activity. This generates a\nnonlinear mapping of the input data to a high-dimensional biological feature\nspace, where distinguishing between data becomes more efficient and\nstraightforward, allowing a simple linear classifier to perform pattern\nrecognition tasks effectively. To evaluate the performance of our proposed\nsystem, we present an experimental study that includes various input patterns,\nsuch as positional codes, bars with different orientations, and a digit\nrecognition task. The results demonstrate the feasibility of using biological\nneural networks to perform tasks traditionally handled by artificial neural\nnetworks, paving the way for further exploration of biologically-inspired\ncomputing systems, with potential applications in neuromorphic engineering and\nbio-hybrid computing.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u57f9\u517b\u751f\u7269\u795e\u7ecf\u5143\u7684\u65b0\u578b\u50a8\u5c42\u8ba1\u7b97\u8303\u5f0f\uff08BRC\uff09\uff0c\u5229\u7528\u591a\u7535\u6781\u9635\u5217\u8bb0\u5f55\u795e\u7ecf\u6d3b\u52a8\uff0c\u901a\u8fc7\u975e\u7ebf\u6027\u6620\u5c04\u5b9e\u73b0\u9ad8\u6548\u6a21\u5f0f\u8bc6\u522b\u3002", "motivation": "\u63a2\u7d22\u751f\u7269\u795e\u7ecf\u7f51\u7edc\u5728\u4f20\u7edf\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u63a8\u52a8\u751f\u7269\u542f\u53d1\u8ba1\u7b97\u7cfb\u7edf\u7684\u53d1\u5c55\u3002", "method": "\u4f7f\u7528\u57f9\u517b\u795e\u7ecf\u5143\u4f5c\u4e3a\u50a8\u5c42\u57fa\u8d28\uff0c\u901a\u8fc7\u591a\u7535\u6781\u9635\u5217\u8f93\u5165\u548c\u8bb0\u5f55\u795e\u7ecf\u6d3b\u52a8\uff0c\u751f\u6210\u9ad8\u7ef4\u751f\u7269\u7279\u5f81\u7a7a\u95f4\u3002", "result": "\u5b9e\u9a8c\u8868\u660eBRC\u80fd\u6709\u6548\u5904\u7406\u6a21\u5f0f\u8bc6\u522b\u4efb\u52a1\uff0c\u5982\u4f4d\u7f6e\u7f16\u7801\u3001\u65b9\u5411\u6761\u548c\u6570\u5b57\u8bc6\u522b\u3002", "conclusion": "BRC\u5c55\u793a\u4e86\u751f\u7269\u795e\u7ecf\u7f51\u7edc\u5728\u8ba1\u7b97\u4efb\u52a1\u4e2d\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u795e\u7ecf\u5f62\u6001\u5de5\u7a0b\u548c\u751f\u7269\u6df7\u5408\u8ba1\u7b97\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2505.03646", "pdf": "https://arxiv.org/pdf/2505.03646", "abs": "https://arxiv.org/abs/2505.03646", "authors": ["Chethan Krishnamurthy Ramanaik", "Arjun Roy", "Eirini Ntoutsi"], "title": "ALMA: Aggregated Lipschitz Maximization Attack on Auto-encoders", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Despite the extensive use of deep autoencoders (AEs) in critical\napplications, their adversarial robustness remains relatively underexplored\ncompared to classification models. AE robustness is characterized by the\nLipschitz bounds of its components. Existing robustness evaluation frameworks\nbased on white-box attacks do not fully exploit the vulnerabilities of\nintermediate ill-conditioned layers in AEs. In the context of optimizing\nimperceptible norm-bounded additive perturbations to maximize output damage,\nexisting methods struggle to effectively propagate adversarial loss gradients\nthroughout the network, often converging to less effective perturbations. To\naddress this, we propose a novel layer-conditioning-based adversarial\noptimization objective that effectively guides the adversarial map toward\nregions of local Lipschitz bounds by enhancing loss gradient information\npropagation during attack optimization. We demonstrate through extensive\nexperiments on state-of-the-art AEs that our adversarial objective results in\nstronger attacks, outperforming existing methods in both universal and\nsample-specific scenarios. As a defense method against this attack, we\nintroduce an inference-time adversarially trained defense plugin that mitigates\nthe effects of adversarial examples.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5c42\u6761\u4ef6\u7684\u5bf9\u6297\u4f18\u5316\u76ee\u6807\uff0c\u7528\u4e8e\u589e\u5f3a\u6df1\u5ea6\u81ea\u7f16\u7801\u5668\u7684\u5bf9\u6297\u9c81\u68d2\u6027\u8bc4\u4f30\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u6df1\u5ea6\u81ea\u7f16\u7801\u5668\u7684\u5bf9\u6297\u9c81\u68d2\u6027\u7814\u7a76\u76f8\u5bf9\u4e0d\u8db3\uff0c\u73b0\u6709\u767d\u76d2\u653b\u51fb\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528\u5176\u4e2d\u95f4\u5c42\u7684\u8106\u5f31\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5c42\u6761\u4ef6\u5bf9\u6297\u4f18\u5316\u76ee\u6807\uff0c\u901a\u8fc7\u589e\u5f3a\u635f\u5931\u68af\u5ea6\u4fe1\u606f\u4f20\u64ad\u6765\u4f18\u5316\u5bf9\u6297\u6270\u52a8\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u901a\u7528\u548c\u6837\u672c\u7279\u5b9a\u573a\u666f\u4e0b\u5747\u4f18\u4e8e\u73b0\u6709\u653b\u51fb\u65b9\u6cd5\u3002", "conclusion": "\u8bba\u6587\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u9632\u5fa1\u63d2\u4ef6\uff0c\u7528\u4e8e\u51cf\u8f7b\u5bf9\u6297\u6837\u672c\u7684\u5f71\u54cd\u3002"}}
{"id": "2505.03702", "pdf": "https://arxiv.org/pdf/2505.03702", "abs": "https://arxiv.org/abs/2505.03702", "authors": ["Srecharan Selvam", "Abhishesh Silwal", "George Kanter"], "title": "Self-Supervised Learning for Robotic Leaf Manipulation: A Hybrid Geometric-Neural Approach", "categories": ["cs.RO", "cs.CV", "cs.LG", "I.2.10"], "comment": "13 pages, 9 figures", "summary": "Automating leaf manipulation in agricultural settings faces significant\nchallenges, including the variability of plant morphologies and deformable\nleaves. We propose a novel hybrid geometric-neural approach for autonomous leaf\ngrasping that combines traditional computer vision with neural networks through\nself-supervised learning. Our method integrates YOLOv8 for instance\nsegmentation and RAFT-Stereo for 3D depth estimation to build rich leaf\nrepresentations, which feed into both a geometric feature scoring pipeline and\na neural refinement module (GraspPointCNN). The key innovation is our\nconfidence-weighted fusion mechanism that dynamically balances the contribution\nof each approach based on prediction certainty. Our self-supervised framework\nuses the geometric pipeline as an expert teacher to automatically generate\ntraining data. Experiments demonstrate that our approach achieves an 88.0%\nsuccess rate in controlled environments and 84.7% in real greenhouse\nconditions, significantly outperforming both purely geometric (75.3%) and\nneural (60.2%) methods. This work establishes a new paradigm for agricultural\nrobotics where domain expertise is seamlessly integrated with machine learning\ncapabilities, providing a foundation for fully automated crop monitoring\nsystems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u51e0\u4f55\u4e0e\u795e\u7ecf\u7f51\u7edc\u7684\u81ea\u4e3b\u53f6\u7247\u6293\u53d6\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\u5b9e\u73b0\u9ad8\u6210\u529f\u7387\u3002", "motivation": "\u519c\u4e1a\u73af\u5883\u4e2d\u53f6\u7247\u64cd\u4f5c\u7684\u81ea\u52a8\u5316\u9762\u4e34\u690d\u7269\u5f62\u6001\u591a\u53d8\u548c\u53f6\u7247\u53ef\u53d8\u5f62\u7b49\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u65b9\u6cd5\u3002", "method": "\u7ed3\u5408YOLOv8\u5b9e\u4f8b\u5206\u5272\u548cRAFT-Stereo 3D\u6df1\u5ea6\u4f30\u8ba1\uff0c\u901a\u8fc7\u51e0\u4f55\u7279\u5f81\u8bc4\u5206\u548c\u795e\u7ecf\u7ec6\u5316\u6a21\u5757\uff08GraspPointCNN\uff09\u52a8\u6001\u878d\u5408\u3002", "result": "\u5728\u63a7\u5236\u73af\u5883\u4e2d\u6210\u529f\u738788.0%\uff0c\u771f\u5b9e\u6e29\u5ba4\u4e2d84.7%\uff0c\u663e\u8457\u4f18\u4e8e\u7eaf\u51e0\u4f55\uff0875.3%\uff09\u548c\u7eaf\u795e\u7ecf\u7f51\u7edc\uff0860.2%\uff09\u65b9\u6cd5\u3002", "conclusion": "\u4e3a\u519c\u4e1a\u673a\u5668\u4eba\u9886\u57df\u63d0\u4f9b\u4e86\u4e00\u79cd\u7ed3\u5408\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\u548c\u673a\u5668\u5b66\u4e60\u7684\u65b0\u8303\u5f0f\uff0c\u652f\u6301\u5168\u81ea\u52a8\u4f5c\u7269\u76d1\u6d4b\u7cfb\u7edf\u3002"}}
{"id": "2505.03729", "pdf": "https://arxiv.org/pdf/2505.03729", "abs": "https://arxiv.org/abs/2505.03729", "authors": ["Arthur Allshire", "Hongsuk Choi", "Junyi Zhang", "David McAllister", "Anthony Zhang", "Chung Min Kim", "Trevor Darrell", "Pieter Abbeel", "Jitendra Malik", "Angjoo Kanazawa"], "title": "Visual Imitation Enables Contextual Humanoid Control", "categories": ["cs.RO", "cs.CV"], "comment": "Project website: https://www.videomimic.net/", "summary": "How can we teach humanoids to climb staircases and sit on chairs using the\nsurrounding environment context? Arguably, the simplest way is to just show\nthem-casually capture a human motion video and feed it to humanoids. We\nintroduce VIDEOMIMIC, a real-to-sim-to-real pipeline that mines everyday\nvideos, jointly reconstructs the humans and the environment, and produces\nwhole-body control policies for humanoid robots that perform the corresponding\nskills. We demonstrate the results of our pipeline on real humanoid robots,\nshowing robust, repeatable contextual control such as staircase ascents and\ndescents, sitting and standing from chairs and benches, as well as other\ndynamic whole-body skills-all from a single policy, conditioned on the\nenvironment and global root commands. VIDEOMIMIC offers a scalable path towards\nteaching humanoids to operate in diverse real-world environments.", "AI": {"tldr": "VIDEOMIMIC\u662f\u4e00\u79cd\u4ece\u65e5\u5e38\u89c6\u9891\u4e2d\u63d0\u53d6\u4eba\u7c7b\u52a8\u4f5c\u548c\u73af\u5883\u4fe1\u606f\uff0c\u5e76\u901a\u8fc7\u4eff\u771f\u751f\u6210\u4eba\u5f62\u673a\u5668\u4eba\u63a7\u5236\u7b56\u7565\u7684\u6d41\u7a0b\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u901a\u8fc7\u7b80\u5355\u7684\u4eba\u7c7b\u52a8\u4f5c\u89c6\u9891\u6559\u6388\u4eba\u5f62\u673a\u5668\u4eba\u590d\u6742\u6280\u80fd\uff08\u5982\u722c\u697c\u68af\u3001\u5750\u6905\u5b50\uff09\uff0c\u5229\u7528\u73af\u5883\u4e0a\u4e0b\u6587\u5b9e\u73b0\u9ad8\u6548\u5b66\u4e60\u3002", "method": "\u63d0\u51faVIDEOMIMIC\u6d41\u7a0b\uff0c\u4ece\u89c6\u9891\u4e2d\u91cd\u5efa\u4eba\u7c7b\u4e0e\u73af\u5883\u4fe1\u606f\uff0c\u751f\u6210\u673a\u5668\u4eba\u63a7\u5236\u7b56\u7565\uff0c\u5e76\u901a\u8fc7\u4eff\u771f\u9a8c\u8bc1\u3002", "result": "\u5728\u771f\u5b9e\u4eba\u5f62\u673a\u5668\u4eba\u4e0a\u5c55\u793a\u4e86\u7a33\u5065\u3001\u53ef\u91cd\u590d\u7684\u4e0a\u4e0b\u6587\u63a7\u5236\u80fd\u529b\uff0c\u5982\u722c\u697c\u68af\u3001\u5750\u6905\u5b50\u7b49\u52a8\u6001\u5168\u8eab\u6280\u80fd\u3002", "conclusion": "VIDEOMIMIC\u4e3a\u6559\u6388\u4eba\u5f62\u673a\u5668\u4eba\u5728\u591a\u6837\u5316\u73af\u5883\u4e2d\u64cd\u4f5c\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
